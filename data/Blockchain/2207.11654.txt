2
2
0
2

l
u
J

7
2

]
I

N
.
s
c
[

2
v
4
5
6
1
1
.
7
0
2
2
:
v
i
X
r
a

1

BPFISH: Blockchain and Privacy-preserving FL
Inspired Smart Healthcare

Moirangthem Biken Singh, and Ajay Pratap, Member, IEEE

Abstract—This paper proposes Federated Learning (FL) based smart healthcare system where Medical Centers (MCs) train the local
model using the data collected from patients and send the model weights to the miners in a blockchain-based robust framework without
sharing raw data, keeping privacy preservation into deliberation. We formulate an optimization problem by maximizing the utility and
minimizing the loss function considering energy consumption and FL process delay of MCs for learning effective models on distributed
healthcare data underlying a blockchain-based framework. We propose a solution in two stages- ﬁrst, offer a stable matching-based
association algorithm to maximize the utility of both miners and MCs and then solve loss minimization using Stochastic Gradient
Descent (SGD) algorithm employing FL under Differential Privacy (DP) and blockchain technology. Moreover, we incorporate
blockchain technology to provide tempered resistant and decentralized model weight sharing in the proposed FL-based framework.
The effectiveness of the proposed model is shown through simulation on real-world healthcare data comparing other state-of-the-art
techniques.

Index Terms—Federated Learning, Blockchain, Stable Matching, Differential Privacy, Smart Healthcare.

(cid:70)

1 INTRODUCTION

Smart healthcare system is likely to play a crucial role in
our society. It can provide remote medical services, helps
medical diagnosis and protect patients against dangerous
infectious disease such as COVID-19 during personal visits
to hospitals. Smart healthcare solutions are extremely help-
ful during the recent COVID-19 pandemic [1], [2]. However,
there are still challenges in smart healthcare such as data
unavailability due to privacy concerns of patients [3].

Federated Learning (FL) [4], an emerging framework is
used to address the unavailability and privacy risks of sensi-
tive healthcare data [5]–[7]. Since training data is not leaving
the Medical Centres (MCs), this framework ensures privacy
protection for all involved centres by sharing models instead
of sharing sensitive raw healthcare data. However, there are
still challenges in FL based healthcare architecture such as
privacy leakage and single point of failure [3], [8]. Firstly,
healthcare data is highly privacy-sensitive, thus the leakage
of this sensitive information could destroy the reputation
and ﬁnances of the patient. Secondly, the existing FL based
healthcare architectures are suffering from a single point of
failure risks due to the aggregation of the model in a central
server and the unwillingness of MCs to participate in the FL
process due to the lack of incentives.

To overcome the above-mentioned issues, we consider a
blockchain and FL based smart healthcare framework (see
Fig. 1) in which MCs train a model locally using the data
collected from patients and forward the model weights to a
miner in a blockchain to build a robust model without shar-
ing raw data. However, it is necessary to optimize both the
utilities of miners as well as MCs and the FL loss function

• M. B. Singh and A. Pratap are with the Department of Computer Science
and Engineering, Indian Institute of Technology (Banaras Hindu Uni-
versity) Varanasi 221005 India. E-mail: {moirangthembsingh.rs.cse21,
ajay.cse}@iitbhu.ac.in.

simultaneously to increase the accuracy and effectiveness of
smart healthcare system while keeping privacy risks into
consideration. Therefore, we propose a joint optimization
problem by maximizing the utility and minimizing the loss
function together, considering energy consumption and the
model training delay at MCs for learning effective models.
However, in blockchain-based FL, there is a need for a
proper association between miners and MCs in order to
increase accuracy and effectiveness of smart healthcare sys-
tem. Thus, we offer a stable association algorithm between
miners and MCs to maximize the utility of both miners
as well as MCs in polynomial time and then solve the FL
loss minimization using Stochastic Gradient Descent (SGD)
algorithm employing FL under Differential Privacy (DP)
and blockchain technology.

In this paper, we focus on developing a blockchain-
based privacy-preserving FL framework for collaborative
training across multiple MCs under differential privacy to
solve privacy leakage problems in Healthcare Domain (HD).
An adversary can use an inference attack to recreate the
training data from the shared model. Therefore, we incor-
porate DP [9] to prevent privacy leakage while transmitting
model weights during FL process. DP is a privacy protection
technique that has been widely used in the ﬁeld of privacy
protection in deep learning models [9]–[11]. We also use
blockchain as a way for decentralized architecture in model
weights sharing. Blockchain provides a tempered resistant
framework where each device veriﬁes the transactions in
the network and the miners perform Proof-of-work (PoW)
to add new blocks. PoW is a proof of doing computation by
the miner to add a new block to the blockchain. Therefore,
we formulate a blockchain and FL based privacy-preserving
for smart healthcare framework by maximizing the utility
and minimizing the loss function considering energy con-
sumption and latency of MCs for learning effective models.
Furthermore, to improve the utility and accuracy of the

 
 
 
 
 
 
FL model, we offer a stable and computationally efﬁcient
association algorithm among miners and MCs. We also
proposed an incentive mechanism to promote the MCs for
participation in the FL process. Speciﬁcally, the contribu-
tions of this paper are summarized as follows:

•

•

Formulate an optimization problem by maximizing
the utility and minimizing the loss function consid-
ering energy consumption and FL process delay of
MCs for collaborative learning on distributed health-
care data.
Stable Miner-MC Association (MMA) algorithm is
proposed between miners and MCs to maximize the
utility with computational complexity of O(N 2S),
where N and S represent the number of MCs and
the number of miners, respectively.

• Blockchain-based privacy-preserving FL framework
that guarantees DP for decentralized collaborative
learning from data stored across MCs is proposed,
with communication complexity of O(T |K|2|w|),
where T , |K| and |w| represent the number of global
iteration, the number of participated MCs in the FL
process and the number of model weights, respec-
tively. An Incentive Mechanism (IM) is also proposed
to encourage miners for verifying and adding the
blocks, and MCs for participating as proportional to
their data sample size.

• Through rigorous evaluations on Chest X-ray Images
(Pneumonia) dataset we verify the effectiveness of
the proposed model on various privacy parame-
ters. Performance study demonstrates that proposed
BPFISH framework surpass state-of-the-art schemes,
achieving 11.18% better outcome on an average.

The rest of the paper is organized as follows: The rele-
vant work is reviewed in Section 2. The system model and
the problem formulation are discussed in Section 3. Miner-
MC association and FL process are given in Section 4 and
Section 5, respectively. Experiment results and analysis are
given in Section 6. Finally, Section 7 provides the conclusion.

2 RELATED WORK

In this section, we present the related works for FL with DP,
blockchain, association and their combination in HD along
with a comparative analysis in Table 1.

2.1 FL with DP in Smart Healthcare

Recently, NVIDIA introduced Clara FL [5] for distributed,
collaborative AI model training across multiple hospitals to
develop a more accurate global model while maintaining
patient privacy. In [12] and [13], authors developed a FL
model for IoT based smart healthcare system. Silva et al.
[14] also proposed an FL framework for securely accessing
and analysing medical data stored at several institutions.
These works consider central server for model aggregation
in FL process but this causes inaccurate global model update
if the server is malfunctioned or attacked. The adaptation of
privacy-preserving techniques to conserve patient privacy
with the use of FL has been demonstrated in clinical and
epidemiological research [15]. In [16] and [17], FL based

TABLE 1: Summary of existing works

Problem Focus

FL DP

Block
-chain

Associa
-tion

IM HD

2

Collaborative
AI Model [5]
Smart Health-
care [12], [13]
Meta-Analysis
of Brain
Data [14]
Health
Research [15]
Privacy
Preserving [16]
Multi-site
fMRI
Analysis [17]
Prevent
Information
Leakage [18]
Access
Management [19]
EMRs Access
Control [20], [21]
EMRs
Management [22]
Resource off-
loading [23], [24]
Blockchain
based Crowd-
sourcing [25], [26]
Patient-
Physician
Matching [27]
Privacy
Preserving
FL [28]
Blockchain
for Privacy
Preserving [29]
Privacy
Preserving
IIoT [30]
Blockchained
On-Device
FL [8]
Decentralized
aggregator
free FL [31]
BPFISH
(Proposed)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

×

×

(cid:88)

×

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

×

×

(cid:88)

(cid:88)

(cid:88)

×

×

(cid:88)

×

×

×

×

×

×

×

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

×

×

×

×

×

×

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

×

(cid:88)

×

×

×

×

×

×

×

×

×

×

×

×

×

×

×

×

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

×

(cid:88)

(cid:88)

(cid:88)

×

×

×

×

×

×

×

×

(cid:88)

privacy-preserving method has been used for smart health-
care systems such as detection of Alzheimer’s disease and
multi-site fMRI data analysis. In [18], K. Wei et al. proposed
a framework under DP by adding Gaussian noises to the
locally trained weights before sending it to the server. How-
ever, these works mainly focused on optimizing FL under
DP by considering centralized model aggregation that is
vulnerable to the single point of failure and did not consider
incentives for participating in the FL process.

2.2 Blockchain in Smart Healthcare

In recent years, many research have proven that blockchain
is a promising solution for ensuring the conﬁdentiality, pri-
vacy preserving and distributed sharing of sensitive health
information. Oscar Novo [19] focused on distributed access
management based on blockchain in IoT. However, the
work does not consider the privacy of Electronic Medical

Records (EMRs) of the patients. In [20], [21], blockchain-
based EMRs access control model provided different access
levels to different types of users, which had been controlled
by the hospitals in traditional EMRs access control system.
Li Chaoyang et al. [22] proposed healthchain, a peer-to-peer
EMRs management and trading system based on a consor-
tium blockchain. Healthchain system allows the access of
patient EMRs in different institutions, and the EMRs can
be traded between different patients. These works mainly
focused on healthcare record management but did not con-
sider FL optimization considering privacy preservation.

2.3 Association in Smart Healthcare

Many studies had been done for user association in re-
source ofﬂoading considering UAVs and IoTs enabled edge
computing framework [23], [24]. Smart user matching inter-
section with blockchain for stable matching in ultra-dense
wireless networks had been proposed for computation of-
ﬂoading in [23]. M. Kadadha et al. [25] and J. An et al.
[26] explored Gale-Shapley based matching algorithm for
node selection in blockchain-based crowdsourcing model.
R. Chen et al. [27] presented a matching algorithm by con-
sidering the preferences from both patients and physicians
to reduce the waiting time of patient. However, the above
stated works cannot be directly applied to the proposed
framework due to ill posed nature of miners and MCs in
blockchain and FL based healthcare architecture.

2.4 FL, DP and Blockchain for Smart Healthcare

C. Li et al. [28] and K. Gai et al. [29] proposed privacy-
preserving data sharing architecture based on blockchain
for industrial Internet of Things by integrating FL and
blockchain technology. In [30], privacy protection schemes
for data in blockchain have been applied using data per-
turbation techniques like DP. However, these works do not
consider incentives and the privacy preserving technique in
smart healthcare domain. In [8] and [31], blockchain-based
FL for privacy-preserving had been applied to eliminate the
centralized global model aggregation.

The above existing works mainly focused on improving
the performance of learning algorithms in FL and they did
not provide incentives to the MCs participated in the FL
process. However, it is necessary to provide incentive since
participating institutes in FL requires data collection and
model training on the device itself. Users are unlikely to
participate in FL activities unless they are rewarded because
model training consumes energy and requires a constant
network connection. Moreover, none of the existing ap-
proaches has considered privacy-preserving decentralized
FL with association and incentive mechanisms altogether as
shown in Table 1. In this paper, we proposed a blockchain-
based privacy-preserving FL combined with incentive and
association mechanisms for smart healthcare applications.

3 SYSTEM MODEL AND PROBLEM FORMULATION
As shown in Fig. 1, we consider a smart healthcare sce-
nario consisting of N MCs and S miners represented by
C = {C1, · · · , Cn, · · · , CN } and M = {M1, · · · , Ms, · · · MS},
n, · · · , xDn
respectively. Furthermore, let Xn = {x1
n }

n, · · · , xd

3

Fig. 1: System model of the proposed framework.

be the set of Dn data samples available at MC Cn ∈ C,
collected from different patients admitted in the MC. MCs
collaboratively train a shared model using their own data
without sharing it to a central server. Speciﬁcally, each MC
trains a model locally using their data and then send it to
the miners for distributed aggregation [30], resulting in a
global model. An MC is characterized by a tuple <Xn, fn>,
where Xn and fn represent the available amount of data
samples for local model training and, the available number
of CPU cycles at MC Cn ∈ C, respectively. The above-stated
scenario needs two-stage categorization as described in the
following:

Stage 1: MMA: In this stage, each MC ﬁnds an associa-
tion with a miner. The association is formed by considering
the utilities of both the MCs and miners using deferred
acceptance algorithm (discussed in Section 4).

Stage 2: Blockchain-based privacy-preserving FL: MCs
train local model and send model weights to the associated
miner without sharing the healthcare data. Thus, hereby
preserving the privacy of the patient’s healthcare data. Min-
ers add the models to the blockchain network. Blockchain
provides distributed model weights sharing framework that
is robust to the single point of failure (discussed in Section
5).

To set up the association between MCs and miners (Stage
1), there is a need for preference order of one over the
other, depending on maximum utilities; described in the
following.

3.1 Utility of Miner

The blockchain network provides mining rewards for veri-
ﬁcation and adding blocks to the blockchain. When a miner
Ms adds a block, its mining reward is provided from the
blockchain network, as does in the traditional blockchain
network [32]. Each miner adds the local models from MCs to
the blockchain. However, miner Ms adds local model from
Cn as a block to the blockchain if and only if Ms and Cn

ECGMRIWerable DevicesMedicalDevicesMedicalCentersMinersMRIBlockchained MinersFederated Learningmodel downloadmodel uploadmodel uploadmodel uploadmodel uploadmodel downloadmodel downloadmodel downloadare associated with each other. Speciﬁcally, the association
between miner and MC is given as follows:

(cid:40)

yn,s =

1,
0, otherwise.

if MC Cn gets associated with miner Ms,

(1)

Let Rs be the revenue of the miner Ms in the FL process
and R be the mining reward for adding a block to the
blockchain [32], which is the same for all the miners in the
network. Each miner solves the PoW multiple times i.e., in
every iteration. Therefore, the total revenue for a miner Ms,
is represented as follows:

Rs = T · h(R),

(2)

where h(·) is a monotonically increasing function for R
and T is the number of global iterations. For simplicity, we
consider following function to deﬁne h(R):

h(R) = R

(cid:88)

Cn∈C

yn,s,

(3)

where (cid:80)
Cn∈C yn,s is the number of blocks added to the
blockchain by the miner Ms in each iteration which is
equivalent to the number of MCs associated with the miner
Ms. Therefore, the total revenue of a miner can be rewritten
as follows:

Rs = T R

yn,s.

(4)

(cid:88)

Cn∈C

The reward is an essential component of an FL based
framework for the MC to participate in the FL process. The
reward of MC is related to the number of data samples
available for training at MC, Cn. We consider that the re-
ward is linearly dependent on the number of data samples.
Therefore, the reward offer to MC, Cn from miner Ms for
training local model on data sample Dn, is calculated as
follows [8]:

Rn,s = Rs

(cid:80)

.

(5)

Dn
Cn∈C Dn

TABLE 2: Symbol description

4

Symbol
C
N
M
w(n)
Xn
Dn
l(w(n), xd
n)
α, B
T
In
G, G(cid:48)
G(cid:48)(cid:48)
A
N , σ
(cid:15), δ
w(n,t)
W(t)
K
|K|
yn,s
R, Rn,s
ϕ
U M in
fn, βn
In
µn
ET rans
n,s
EComp
n
τ T rans
n,s
τ Comp
n
τ th
Υn
∆s
ρ, η
As, Bs
Pn
Ps

n,s , U M C
n,s

Description
Set of MCs
Number of MCs
Set of Miners
Model weights at MC Cn
Dataset at MC Cn
Number of data samples at Cn
Loss per data sample
Learning Rate and Batch Size
Number of global iteration
Number of local iteration of MC, Cn
Computed Gradient and Clipped Gradient
Gradient after adding noise
Gradient bound
Gaussian Noise and Noise scale
Privacy parameters
Weights at MC, Cn at global iteration t
Aggregate Weights at global iteration t
Set of MCs associated with miners
Number of MCs participated in FL process
Association between MC and miner
Mining reward and miner revenue
Cost per unit energy
Utility of miner and MC
CPU cycles/second, CPU cycles/sample
Number of local iteration
Transmit power
Energy consumption in model upload
Energy consumption in model computation
Model computation time at MC
Transmission time from MC to miner
Threshold
Utility list of Cn in increasing order
Utility list of Ms in increasing order
Weights assigned to utility and FL loss
Feasible association candidates of Ms and Cn
Preference list of MC Cn
Preference list of miner Ms

total energy consumption in local model training is given
by:

Ecomp
n

= InEn = κInβnDnf 2
n.

(8)

We assume that the utility and the total revenue of a
miner are linearly dependent. Therefore, the utility of a
miner is deﬁned as follows:

U M in

n,s = Rs − Rn,s.

(6)

3.2 Utility of Medical Center

We use the utility as a criterion to determine the preferences
of the MCs and miners for the association. Since MC trains
model in the FL process with the consumption of energy, we
consider computation energy while deﬁning the utility of an
MC. Let fn be the number of CPU cycles per second (com-
putation capacity) of MC Cn ∈ C. Let βn (cycles/sample)
be the number of CPU cycles needed for computing one
sample data xd
n ∈ Xn at MC, Cn. The energy consumption
to calculate the total number of βnDn CPU cycles at MC, Cn
can be derived as [33]:

En = κβnDnf 2
n,

(7)

where κ is a coefﬁcient that depends on the chip architec-
ture. In the FL process, an MC, Cn requires to compute
βnDn CPU cycles in each In local iterations. Therefore the

n

Moreover, the computation time taken by an MC, Cn to train
local model can be deﬁned as follows: τ comp
= InβnDn/fn.
Following the local model training, model is sent from
the MC to the associated miner. For simplicity, we deﬁne
the achievable data rate between MC, Cn and miner Ms as:
υn,s = QV log2 (1 + SIN Rn,s). Here, V is the total number
of allocated PRBs1 between MC and miner. SIN Rn,s and Q
are the signal-to-interference-plus-noise ratio (SINR) 2 and
the bandwidth of PRB allocated between MC and miner,
respectively. The total transmission time3 [35] for an MC in
uploading the model weights of size H to the miner during
the FL process is as follows: τ trans
n,s = T H/υn,s. Given the
same model across all MCs, the size of the local model
remains constant, independent of the number of iterations
or the amount of data available. The energy consumption in

1. PRB is the smallest unit that can be assigned to a device in the 5G

network [34].

2. For simplicity, we assume that the channel exhibits ﬂat fading.
However, this can be easily extended to frequency-selective fading
channels as well [34].

3. We assume that model download time between miners and MCs
is negligible compared with the transmission time as usually the
downlink bandwidth is signiﬁcantly larger than the uplink bandwidth
[35].

the transmission of the trained local model can be deﬁned
as follows [24]:

where α is the learning rate of the model. Moreover, G(cid:48)(cid:48) is
calculated using Eq. (15):

5

ζn,s = Etrans

n,s = τ trans

n,s µn.

(9)

where, µn is the transmit power of MC, Cn.

Therefore, the utility, U M C

n,s of an MC, Cn can be deﬁned

as follows:

U M C

n,s = Rn,s − ϕ(Ecomp

n

= Rn,s − ϕ(κInβnDnf 2

+ Etrans
),
n,s
n + ζn,s),

G(cid:48)(cid:48) = G(cid:48) + N (0, σ2A2I),

(15)
where N (0, σ2A2I) is the Gaussian noise, I is the identity
matrix and G(cid:48) is the clipped gradient using L2 norm of the
gradient calculated as follows:
(cid:32)

(cid:33)

(10)

G(cid:48) = ∆Ln,s(w(n))/max

1,

||∆Ln,s(w(n))||2
A

,

(16)

where ϕ represents the cost per unit energy.

During the FL process, each MC has to upload the local
model within a speciﬁed time. Particularly, to conduct FL
process reliably and to ﬁnd the feasible association candi-
dates between miners and MCs, the time constraint needs
to satisfy:

τ comp
n

+ τ trans

n,s ≤ τth, ∀Cn ∈ C, ∀Ms ∈ M,

(11)

where τth is the predeﬁned threshold.

3.3 Federated Learning
We deﬁne a vector w(n) as weights related to the FL model
for MC, Cn. We introduce the loss function l(w(n), xd
n) of
the model, which indicates the FL performance over an
input sample. The loss function varies depending on the
learning problem. Different learning problems use different
loss functions. In smart healthcare, loss deﬁnes how far
the model’s prediction or the outcomes is from the actual
outcomes of the health condition [36]. The FL local model
training problem at MC Cn ∈ C for miner Ms, using its
own data Xn can be formulated as follows [37]:

min
w(n)

Ln,s(w(n)) min
w(n)

1
Dn

(cid:88)

xd

n∈Xn

l(w(n), xd

n).

(12)

In order to get better privacy, we integrate DP in the
local model training. DP ensures that the output of a differ-
entially private algorithm gives the same output with (cid:15) error
whether or not a local data sample is included in the input
of the algorithm. (cid:15) is the privacy budget that is bound on
the loss of the privacy of the algorithm. We use the variant
of DP deﬁnition introduced in [10], that satisﬁes ((cid:15), δ)-DP,
where δ deﬁnes the bound that the privacy guarantee does
not hold (which is preferably very small positive number).

Deﬁnition 1. A randomized algorithm Z : X → Y with
domain X and range Y satisﬁes ((cid:15), δ)-DP if for any two ad-
jacent datasets D’ and D” that differ by one data record and
for any subset O ⊆ Y, the following probability condition
holds:

P r[Z(D(cid:48)) ∈ O] ≤ e(cid:15)P r[Z(D(cid:48)(cid:48)) ∈ O] + δ.

(13)

Privacy can be achieved in the algorithm by adding ran-
dom noise to the gradient computation so that the resultant
model is noisy. Therefore, we update the weights at i-th
(i ∈ [1, In]) local iteration for optimizing the loss function
as follows [10]:

(cid:113)

2 log 1.25

for

δ /(cid:15)

(cid:15) > 0 and A is
where σ =
the gradient bound. DP requires bounding the im-
pact of each data sample on G(cid:48)(cid:48). So, each gradient is
i.e., the gradient ∆Ln,s(w(n))
clipped in the L2 norm,
1, ||∆Ln,s(w(n))||2
is replaced by ∆Ln,s(w(n))/max
, for
a gradient bound A. This clipping ensures
if
||∆Ln,s(w(n))||2≤ A,
then ∆Ln,s(w(n)) is preserved,
whereas if ||∆Ln,s(w(n))||2> A, gradient gets scaled down
to be of norm A. This gives the global minimization problem
over the collection of all the loss functions from MCs. This
global minimization problem is minimized by ﬁnding the
optimal weights, WT and is deﬁned as follows:

that

(cid:16)

(cid:17)

A

WT = min
W
where J(W) is the total loss over the collection of all the
MCs, given as follows:

J(W),

(17)

J(W) =

(cid:80)

Cn∈C

1
(cid:80)
Ms∈M yn,s

(cid:88)

(cid:88)

Cn∈C

Ms∈M

yn,sLn,s(w(n)),

where W is
(cid:80)

(18)
of
global model.
the weights
total number of MCs
the
participated in the FL process. Moreover the descriptions of
used symbols are given in Table 2.

Cn∈C,Ms∈M yn,s

gives

the

3.4 Problem Formulation

We formulate a joint optimization problem having goal to
maximize the utility of both the miner and MC and min-
imize the FL loss function while satisfying the FL process
delay requirement. Utility of MC involves determining the
miner associated with, amount of data present at MC for
local model training, and the uplink transmit power of each
MC for model update transmission to the miner. The utility
of miner involves mining rewards for adding blocks to the
blockchain. Therefore, the total utilities of miners and MCs
is given by:

(cid:88)

(cid:88)

U =

yn,s(U M in

n,s + U M C

n,s ),

(19)

Cn∈C
n,s and U M C

where U M in
Eq. (10), respectively.

Ms∈M

n,s are deﬁned in the above Eq. (6) and

The minimization problem of Eq. (17) is equivalent to the

following maximization problem:

WT = max
W

{−J(W)}.

(20)

As a result, we combine both the factors i.e., utilities of
miners and MCs, and FL process loss function as follows:

w(n,i+1) = w(n,i) − αG(cid:48)(cid:48),

(14)

F = ρU + η{−J(W)},

(21)

where ρ and η are the weights assigned to the utility and
the FL loss function and ρ + η = 1.

Therefore, the optimization problem of the system model

can be formulated as follows:

Algorithm 1 Proposed MMA Algorithm
Input: Set of MCs C, set of miners M, number of data sam-
ples Dn, CPU cycles fn, cycles per sample βn, threshold
τth.

6

max
yn,s,W

F

S. T.

1.1. U M C

n,s > 0, U M in
(cid:88)
(cid:88)

1.2.

yn,s > 0,

n,s > 0, ∀Cn ∈ C, ∀Ms ∈ M,

Cn∈C

Ms∈M

1.3. Eqs. (1) and (11),

(P1)
∀Cn ∈ C, ∀Ms ∈ M. Constraint 1.1 tells that utility of MCs
and miners should be greater than zero. Constraint 1.2 tells
that at least one association between miners and MCs is
possible in the FL process. Constraint 1.3 is described in the
above Eq. (1) and Eq. (11), respectively.

The problem given in P1 is challenging since it involves
the optimization of utilities as well as FL loss values. We
decouple this problem into the optimization of utility and
optimization of FL loss and solve them separately. Firstly,
the optimization of utility is solved for the Miner-MC Asso-
ciation (MMA) by applying Algorithm 1. Then the FL loss
optimization is solved using Stochastic Gradient Descent
(SGD) algorithm with DP and blockchain technology as
described in Algorithm 2.

4 MINER-MC ASSOCIATION
Associations between miners and MCs are formed in coor-
dination with a trusted entity as shown in Fig. 2. To perform
the association, MCs and miners submit their computational
power and data size, and amount of mining reward to
the trusted entity, respectively. Here, computational power
refers to the CPU cycles of each MC. Associations between
miners and MCs are formally deﬁned as follows:

Deﬁnition 2. MMA: An association between miner Ms ∈
M and MC Cn ∈ C is a mapping g : C → M, such that
g(Cn) = Ms, ∀Cn ∈ C, ∀Ms ∈ M.

However, performing MMA in practice is subject to
some constraints. To perform the FL process reliably, all the
constraints 1.1, 1.2 and 1.3 need to satisfy. Thus, we deﬁne a
feasible MMA as follows:

n,s and U M C

n,s , ∀Cn ∈ C, ∀Ms ∈ M

1: Initialization: U M in
2: while (|C|(cid:54)= 0) do
3:

n

4:

) ≤ τth, ∀Cn ∈ C} and

+τ trans
n,s

Find As = {Cn|(τ comp
Bn = {Ms|U M in
n,s > 0, ∀Ms ∈ M}
Arrange the utility lists in non increasing order
n,s |∀Ms ∈ Bn} and
Υn = desc{U M in
n,s |∀Cn ∈ As},
∆s = desc{U M C
Create preference lists Pn and Ps using Υn and ∆s
5:
6: Ws ← φ, Rejs ← φ, Rejn ← φ, ∀Cn ∈ C, ∀Ms ∈ M
7:
8:

Find the miner Ms in Pn with the highest utility,
such that the utility of which is U M in
n,s∗
if Ms∗ = 0 then

for all Cn ∈ C do

yn,s = 0, ∀Ms ∈ Pn

else

Cn applies to Ms
Ms removes Cn from Pn and add it into Ws

end if

end for

for all Ms ∈ M do

Find the MC, Cn∗ in Ws according to Ps with the
highest utility
g(Cn∗) = Ms, yn∗,s = 1, Y ← Y ∪ {yn∗,s}
yn,s = 0, ∀Cn ∈ Ws, Cn (cid:54)= Cn∗
Remove MCs in Ws (except Cn∗) from Ps into Rejs
Rejected MCs in Rejs adds Ms from Pn into Rejn
K ← K ∪ {Cn∗}, C ← C − {Cn∗}

end for

9:
10:
11:
12:
13:
14:
15:

16:
17:

18:
19:
20:
21:
22:
23:

if No MCs are rejected then

Break

24:
25:
26:
27: end while

end if

Output: Association between miners and MCs i.e., Y
and successfully associated MCs i.e., K.

Deﬁnition 3. Feasible MMA: An MMA is feasible, if:

4.1 Proposed MMA Algorithm

•

•

∀Cn ∈ C, each of its association with miner, g(Cn) =
Ms should satisfy Eq. (11)
∀Cn ∈ C, there exists at most one association with
miner, i.e., |{g(Cn)}|≤ 1.

The above Deﬁnition 2 implies that g is a many-to-one
association i.e., g(Cn) is not unique. The interpretation of
g(Cn) = φ implies that for some Cn ∈ C, there is no
association due to non satisfactory of constraints. The result
of the function determines the successfully associated MC
and the association between the miner and MC, e.g., K and
g ≡ Y, where

K = {Cn|yn,s = 1, ∀Cn ∈ C, ∀Ms ∈ M}

and

Y = {yn,s|yn,s = 1, ∀Cn ∈ C, ∀Ms ∈ M}.

(22)

(23)

The proposed MMA is based on the deferred acceptance al-
gorithm [38]. We modify the deferred acceptance algorithm
in particular due to its feasibility and stability, allowing the
proposed MMA algorithm to ﬁnd a stable association result
as described in the following.

n

+τ trans
n,s

In the beginning, miners and MCs select feasible associa-
tion candidates that meet the criteria, i.e., (τ comp
) ≤
τth as deﬁned in Deﬁnition 3 and U M in
n,s > 0, respectively.
Miner Ms’s and MC Cn’s feasible association candidates are
denoted by As and Bn, respectively (line 3). We arrange the
utility lists of MC, Cn and miner, Ms in non increasing
n,s |∀Ms ∈ Bn} and
order, denoted by Υn = desc{U M in
n,s |∀Cn ∈ As}, respectively (line 4), where
∆s = desc{U M C
desc{·} is to represent the ordered list in non-increasing
order. Miners and MCs then construct their preference lists
by arranging the utilities in non-increasing order based on

the feasible candidate list (line 5). The preference lists of
miners and MCs present in Υn and ∆s are given by Pn and
Ps, ∀Cn ∈ C, ∀Ms ∈ M. At the start of each association
iteration, all waiting lists, Ws and miner Ms’s and MC, Cn’s
rejection lists denoted by Rejs and Rejn, respectively are
initialized as empty lists (line 6) i.e., Ws ← φ, Rejs ← φ,
and Rejn ← φ,∀Ms ∈ M, ∀Cn ∈ C.

Once the preference lists of miners and MCs are set
up, the main matching algorithm between miners and MCs
begin as shown in Algorithm 1. At each iteration, Ps, Pn,
Rejs, Rejn and yn,s, ∀Cn ∈ C, ∀Ms ∈ M, are updated.
Furthermore, MC, Cn selects its preferred Ms according
to the preference list (line 8). Then MC Cn applies to its
preferred miner Mi∗ in Ps for association, i.e., U M in
n,s∗ >
n,s , ∀Ms ∈ Pn, Ms (cid:54)= Ms∗ (line 12). Every miner Ms ∈ M
U M in
receives a set of request for association from MCs. These
requests are added to Ws, called a waiting list, ∀Ms ∈ M
(line 13). If MC, Cn has the greatest utility in waiting list
Ws of miner Ms, miner Ms accepts the MC and rejects the
other MCs in Ws (line 17). The association between miner
and MC is updated as yn,s (line 18). Miners remove all the
MCs except for Cn∗ from its preference list and add it into
the rejected list (line 20). The rejected MCs in Rejs remove
the miners who reject them from their preference lists and
add them to their rejected lists (line 21). All MCs that are
associated with a miner are added to set K and removed
from the set of MCs, C (line 22). If no MCs are rejected
i.e., all the MCs are associated with one of the miners, then
this completes MMA algorithm (lines 24-26). Otherwise, the
above-mentioned procedure is repeated until no more MCs
are rejected (line 2-27).

4.2 Theoretical Analysis of MMA Algorithm

The stability of the association algorithm implies the robust-
ness to change in the association that increases the utilities
of miners and MCs. If the association is unstable, an MC is
willing to change its associated miner if favourable to the
MC. Such a network with unstable associations eventually
results in an unsatisfactory and unreliable association. For a
stable MMA, the condition of the nonexistence of blocking
pairs must satisfy. We formally deﬁne the blocking pairs as
follows.

Deﬁnition 4. Blocking Pair: For every MC, Cn, a pair
(Cn, Ms) is deﬁned as blocking pair if all the following
conditions are satisfy:

• Cn associated with miner Ms∗ ∈ M.
• There exists another pair (Cn, Ms), such that U M C

n,s >

U M C

n,s∗ , Ms (cid:54)= Ms∗, Ms ∈ M.

The higher utility can be obtained by blocking pairs.
This indicates that the MC has a strong desire to change the
association, implying that the association is unstable. Based
on the deﬁnition of blocking pair, we deﬁne the stability of
the MMA in the following.

Deﬁnition 5. Stability: An MMA is stable if it satisﬁes the
condition of nonexistence of blocking pairs.

As a result, we can make Lemma 1 concerning the

stability of the MMA formed by applying Algorithm 1.

Lemma 1. Algorithm 1 gives stable association.

7

n,q > U M C

Proof. To prove the stability of the MMA algorithm, we
show that there exists no blocking pair. Let there be a
blocking pair (Cn, Mq) for MC, Cn after associated with
miner Mr, ∀Cn ∈ C, Mq, Mr ∈ M. According to Deﬁnition
4, (Cn, Mq) satisﬁes U M C
n,r . According to Algorithm
1, if MC, Cn is associated with miner Mr, then Cn associated
with Mr should have a greater utility than that the utility
obtained by association with other miners. However, Cn
fails to form association with Mq if and only if the utility
of Cn is higher if associated with miner Mr than the
utility obtained from the association with miner Mq. This
contradicts with (Cn, Mq), i.e., U M C
n,r . As a result,
there exists no blocking pair after forming the association
between MCs and miners by applying Algorithm 1, which
thus proves the Lemma.

n,q > U M C

Theorem 1. Time
O(N 2S log N S).

complexity

of MMA algorithm is

Proof. Algorithm 1 iteratively formed the associations be-
tween miners and MCs. We can see from the algorithm that
the number of iterations depends on the number of MCs
to be associated with miners. Let consider the worst-case
scenario where an MC is associated with a miner after both
MC and miner search all their association candidates. The
total association candidate in As and Bn could be 2N S.
Therefore, line 3 takes O(N S) time. Construction of utility
lists Υn and ∆s (line 4) take O(N S log S) and O(N S log N )
using Heap sort algorithm, respectively. Creation of Pn and
Ps (line 5) take O(N S). Selection of candidate miners for
each MC (lines 7-15) takes N times. Thus, the worst-case
computational complexity of lines 7-15 is O(N ). Similarly,
the worst-case computational complexity of lines 16-23 is
O(S). Lines 2-27 iterates over the number of MCs i.e., N .
Therefore, the worst-case computational time complexity of
MMA algorithm is O(N (N S + N S log S + N S log N + N +
S)) i.e., O(N 2S log N S).

This completes Stage 1 i.e., the association between
miners and MCs. After the association, the actual privacy-
preserving FL process begins. In the next section, Stage 2
i.e., Blockchain and Privacy-preserving FL Inspired Smart
Healthcare (BPFISH) is discussed in detail.

5 PROPOSED BPFISH ALGORITHM

FL allows the collaborative learning of multiple MCs with-
out sharing their local data. An adversary can use an infer-
ence attack to recreate the training data from the shared
model. Therefore, we incorporate the concept of DP for
privacy-preserving in model training. This can be achieved
in the algorithm by adding random noise to the gradi-
ent computation. Moreover, we leverage the advantages
of blockchain in this work. Blockchain provides an im-
mutable and decentralized framework for model weights
sharing. The proposed algorithmic steps are shown in the
Fig. 2. Speciﬁcally, FL process consists of two components:
Blockchain for distributed local model weights sharing and
local model training by MCs using their private data. Each
MC trains the local model using their local data and up-
dates the model weights to the miners. The FL local model

Algorithm 2 Blockchain-based Privacy-preserving FL
Input: Training samples Xn, learning rate α, batch size
B, loss function Ln,s(w(n)) = 1
n),
Dn
number of global iteration T , noise scale σ, Gradient
bound A.

l(w(n), xd

n∈Xn

(cid:80)

xd

1: Initialized weights w(0), w(n) ← w(0)
2: for each global iteration t = 1, 2, · · · , T do

Local Model Training
for each Cn ∈ K do

\\Perform in parallel

for each local iteration i = 1, 2, · · · , In do

for each batch b ∈ Xn do

for each sample x ∈ b do

Compute Loss Ln,s(w(n,t))
Compute gradient G = ∆Ln,s(w(n,t))
1, ||G||2
Gradient clipped G(cid:48) = G/max
A

(cid:16)

(cid:17)

end for
Add noise to the gradient G(cid:48)(cid:48) = 1
+ N (0, σ2A2I))
Updates local weights w(n,t) = w(n,t) − αG(cid:48)(cid:48)
Upload w(n,t) to associated miner

x∈b G(cid:48)

B ((cid:80)

end for

end for

end for

Blockchained Miners
for each Miner Ms ∈ M do

Verify received w(n,t) and broadcast to all miners
Add w(n,t) to candidate block
Performs PoW and adds candidate block to
blockchain
Broadcast the new block to all miners

end for

3:
4:
5:
6:
7:
8:

9:

10:
11:

12:
13:
14:
15:
16:

17:
18:
19:
20:

21:
22:

23:

Model Aggregation
Download local model weights from the associated
miner for t-th global iteration
24: MCs Update the global weights as

W(t+1) = (cid:80)|K|
Initialize local weights as w(n,t+1) ← W(t+1)

n=1 pnw(n,t)

25:
26: end for
Output: Optimal model weights, WT .

training problem is to optimize the loss as given in Eq.
(12). The FL loss optimization is solved using Stochastic
Gradient Descent (SGD) algorithm under DP as described
in Algorithm 2.

In the beginning, blockchain adds the model that will be
utilised by all MCs, as a genesis block on the blockchain.
At the initialization step, each MC downloads the model
weights present in the genesis block from its associated
miner and initialized the weights for local model training
(line 1). At each local iteration of the SGD, we computed
the gradient G for a batch of data samples and clipped
each computed gradient (lines 6-10). Then add Gaussian
noise drawn from Gaussian distribution to the average
gradient and upload the updated weights to the associated
miner (lines 11-13). This process repeats for each batch in

8

Fig. 2: The proposed BPFISH framework.

every local iteration (lines 4-15). Each MC in the FL process
performs lines 3-16 in parallel for every global iteration.

Miners add the local models from the associated MCs
to the blockchain for distributed model weights sharing
and provide an immutable framework. Miners verify the
received weights and add them to the candidate block (lines
18-19). Miners perform PoW to add candidate block in the
blockchain and broadcast the new block to all miners (lines
20-21). For model aggregation, MCs download local model
from the associated miners and aggregate the model weights
(lines 23-25) on-device in every global iteration. Each MC
aggregates |K| local weights w(n,t) downloaded from its
associated miner at t-th global iteration as follows:

W(t+1) =

|K|
(cid:88)

n=1

pnw(n,t),

(24)

where pn = Dn
D is the weightage given to each MC, Cn.
w(n,t) is the local model weights at MC, Cn at global
iteration t. Here D = (cid:80)|K|
n=1 Dn, and |K| is the number of
MCs in the set K i.e., number of MCs participated in FL
process.

After global model aggregation, each MC updates local
weights using the global model for the next iteration (line
25). MC continues training using the updated local model
weights and uploads them again to the associated miners
for the next iteration. This process repeats in every global it-
eration (line 2-26). Finally, we obtained the optimal weights,
WT after the FL process that gives the minimum value of
loss function as given in Eq. (20).

MinerMedical CentersBlockchained MinersBlockFederated LeariningBlock4. Add Block1. Association1. AssociationTrustedEntity2. FL ProcessMedical CentersBlockBlockBlockBlockMinerBlockchained Miners2. Model downloadLocal ModelMC 1Local Data2. Model download3. Model Upload3. Model Upload5. Block PropagationModelAggregationLocal ModelMC 2Local DataModelAggregationTABLE 3: System parameters

Parameters
S
N
Iterations
Cycle rate of MC fn
Cycles per sample βn [39]
Bandwidth Q
Number of allocated PRBs V [34]
SIN Rn,s
Transmit powers µn [39]
κ [39]
Batch size
R
ϕ
Model weight size H
Weight parameters
Threshold τ th
Initial learning rate α

Details
5
10-100
T = 15, In = 10
1-2.6 GHz
[1, 3] x 104
20 MHz
1-10
13-20 dB
1-10 dB
10−28
32
10 units
1 units
3.776 Kbits
ρ = 0.5, η = 0.5
24 mins
0.01

Theorem 2. Communication complexity of Algorithm 2 is
O(T (|K|2|w|).

Proof. We consider the number of model weights sent by ev-
ery MC for the local model training to analyze the commu-
nication complexity of Algorithm 2. Let |w| be the number
of weights in a local model at each MC. In FL, the commu-
nication of weights during upload (line 13) and download
of local model weights (line 23) from the blockchain is |w|
and |K||w|, respectively [40]. Therefore, communication per
MC in each global iteration is |w|+|K||w| i.e., (|K|+1)|w|.
Since |K| number of MCs participated in the FL process, the
communication required for MCs in each global iteration is
|K|(|K|+1)|w|. Similarly, miners in the blockchain broadcast
|K| local models (line 21) and at least (S −1)|w| communica-
tion is required to broadcast a local model. Thus, communi-
cation per miner in each global iteration is |K|(S − 1)|w|.
Lines 2-26 iterates over the total number of global itera-
tions i.e., T . Therefore, overall communication required in
T global iteration is T (|K|(|K|+1)|w|+|K|(S − 1)|w|) i.e.,
T (|K|2|w|+|K||w|S). Generally, the number of MCs partici-
pated in the FL process is more than the number of miners
i.e., |K|≥ S. As a result, the communication complexity of
the Algorithm 2 is O(T (|K|2|w|).

6 PERFORMANCE ANALYSIS

In this section, we present the performance of our proposed
BPFISH framework through simulation analysis. The pro-
posed framework is simulated using Python 3.9 and Tensor-
ﬂow 2.0 on Windows 10 Home PC with Intel(R) Core(TM)
i7-10750H @ 2.60 GHz processor and 16 GB memory.

We evaluate the performance of the proposed BPFISH
framework on the Chest X-Ray Images (Pneumonia) dataset
consisting of 5856 Chest X-Ray images [41]. The dataset is
divided into 5270 training samples and 586 testing samples.
Training data samples are partitioned into 5270/N equal
parts, with one part given to each MC. The images in the
dataset have different resolutions, while our model needs a
ﬁxed dimension. Therefore, the images are down-sampled
to a ﬁxed resolution of 150 × 150 images. Our model uses
a neural network that contains fourteen layers — the ﬁrst
ten layers are convolutional layers and the last four are

9

Fig. 3: Analysis of the value of F.

fully connected layers. We use ReLU activation function
in each layer except sigmoid activation function in output
layer. The ﬁrst convolutional layer uses 16 ﬁlters of size 3
x 3 with a stride of 1 pixel. The number of ﬁlters doubles
every two convolution layers. Every two convolution layers
are followed by batch normalization and max-pooling layer.
The fully-connected layers have 512, 128, 68 and 1 neurons.
The ’dropout’ [42] rates of 0.7, 0.5 and 0.3 are used in the
ﬁrst three fully-connected layers to prevent overﬁtting. For
the optimization of the convolutional neural network, we
set the initial learning rate to 0.01 and reduced it by a factor
of 0.3 once the loss stopped decreasing for two iterations.

In this experiment, cycle rate of each MC is set between
1 and 2.6 GHz, and cycles per sample βn are chosen uni-
formly from [1, 3] x 104 [39]. Unless otherwise stated, we
set coefﬁcient κ to 10−28 [39], transmit powers µn from 1 to
10 dB [39], model size H = 3.776 Kbits, bandwidth Q = 20
MHz and batch size B = 32. Number of allocated PRBs V
are chosen from 1 to 10 and SIN Rn,s are chosen from 13 to
20 dB. The common parameters used in the experiment are
shown in Table 3.

6.1 Objective Function Analysis

We plot the value of F as a function of global iteration,
shown in Fig. 3. We conduct the experiments by setting
N = 500, S = 50, T = 15, In = 10, δ = 10−5, ρ = 0.5
and η = 0.5. To get a fair comparison, we apply random
association for BlockFL [8] framework in which MCs select
miners randomly. From the result, we can conclude that
the proposed BPFISH is better compared to BlockFL and
almost equal to non-private framework where non-private
is the BPFISH without DP i.e., without adding noise to
the gradient. The proposed BPFISH achieves 11.18% better
results on an average. The reason for the better performance
of BPFISH compared to BlockFL is that associations between
miners and MCs in BPFISH are done more accurately using
MMA algorithm in the proposed framework. The result of
BPFISH is almost equal to that of non-private because both
BPFISH and non-private framework used MMA algorithm
and the difference in the value of loss function is small.

6.2 Accuracy and Loss Comparison

Fig. 4 compares the test accuracy and the value of loss func-
tion for 10 MCs with BlockFL and non-private framework
using noise level σ = 0.25 and gradient bound A = 8. As we
can see from Fig. 4a, the test accuracy increases as compared
to BlockFL and the decrease in accuracy for private BPFISH
as compared to the non-private framework is small and has

 90000 95000 100000 105000 110000 2 4 6 8 10 12 14Value of FNumber of Global IterationsBlockFLBPFISHNon-PrivateTABLE 4: Accuracy for different privacy levels

10

((cid:15), δ)
None
(185, 10−5)
(8, 10−5)
(1.89, 10−5)

Noise level Accuracy

σ = 0.00
σ = 0.25
σ = 0.60
σ = 1.00

0.8993
0.8819
0.8454
0.7309

of BPFISH is higher as compared to BlockFL because of the
association formed by the MMA algorithm and the addition
of noise in gradient calculation during local model training
to ensure DP. However, the time taken of BPFISH is almost
equal to the time taken of non-private because the addition
of noise takes less time.

6.4 Effects of Different Privacy Parameters
In Fig. 6, we set N = 10, S = 5, T = 15, In = 10 and
δ = 10−5. We take various noise levels such as σ = 0.25, σ =
0.6 and σ = 1.0 to illustrate the test accuracy and the value
of loss function using gradient bound A = 8 as shown in
Figs. 6a and 6b. We plot the test accuracy on various noise
levels as a function of global iteration as shown in Fig 6a.
As seen from the ﬁgure, the accuracy increases as the noise
level decreases and attained the highest accuracy when the
noise level is set to σ = 0.25. Fig. 6b compares the value of
test set loss on various noise levels as a function of global
iteration. We can see from the ﬁgure that the value of the loss
function decreases as the noise level decreases and attained
the best value when the noise level is set to σ = 0.25.

In Figs. 6c and 6d, we set different gradient bounds
such as A = 1, A = 4 and A = 8 to illustrate the results
of the test accuracy and the value of loss function using
noise level σ = 0.25. Limiting the gradient bound destroys
the true gradient value. Gradient bound destroys the true
direction of gradient estimate if gradient bound is too
small whereas a large gradient bound does not destroy true
gradient. Therefore the clipped gradient becomes closer to
true gradient estimates. As we can see from Fig. 6c, the test
accuracy increases when the gradient bound increases from
1 to 8 and attained the best accuracy value when A = 8.
Fig. 6d shows that the value of the test set loss function
decreases when gradient bound increases from 1 to 8 and
convergence performance of the proposed BPFISH obtained
the best value when A = 8. We cannot compare BlockFL with
BPFISH on various privacy parameters because BlockFL did
not consider privacy parameters in their model.

7 CONCLUSION

In this paper, we have focused on decentralized privacy-
preserving FL framework for learning effective models on
healthcare data stored at different MCs. We have proposed
a joint optimization problem as maximization of utility and
minimization of FL loss function altogether in smart health-
care domain. We introduced a stable association algorithm
to maximize the utility of miners and MCs in polynomial
time complexity. Moreover, we leveraged blockchain tech-
nology to enable tempered resistant and decentralized local
model weights sharing. Through simulation analysis using
Chest X-Ray Images (Pneumonia) dataset, we have veriﬁed

(a) Test accuracy comparison.

(b) Value of loss function comparison.

Fig. 4: Test accuracy and loss analysis.

Fig. 5: Analysis of computation time.

little effect on the accuracy. In comparison to BlockFL, the
proposed BPFISH obtains 10% higher accuracy on average.
As we can see from Fig. 4b, the value of the loss function
decrease as compared to BlockFL but is higher than the non-
private. The increase in the value of loss for private BPFISH
as compared to the non-private framework is small and has
little effect on the value of the loss.

Table 4 shows the best test accuracy obtained when
different noise levels are applied. As observed from the
table, the proposed BPFISH achieved accuracy comparable
to that of non-private collaborative learning framework
when we added small noise. However, large noise affects
test accuracy severely. Therefore, it is important to ﬁnd
an acceptable noise level to achieve strong DP without
compromising accuracy.

6.3 Computation Time Analysis

Fig. 5 shows computational time complexity analysis of the
proposed BPFISH in comparison with BlockFL. For compar-
ing computational time complexity, we set the number of
MCs from 10 to 100 and the number of miners to 5. As we
can conclude from the ﬁgure, the time taken by the BPFISH
is increasing with the increased number of MCs. The reason
is that the number of local models added to the blockchain
increases as the number of MCs increases. The time taken

 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14AccuracyNumber of Global IterationsNon-PrivateBPFISHBlockFL 0.2 0.3 0.4 0.5 0.6 0.7 0.8 2 4 6 8 10 12 14Value of Loss FunctionNumber of Global IterationsNon-PrivateBPFISHBlockFL 600 800 1000 1200 1400 10 20 30 40 50 60 70 80 90 100Time (s)Number of Medical CentersBPFISHNon-PrivateBlockFL11

(a)

(b)

(c)

(d)

Fig. 6: a) Effect of noise levels on test accuracy, b) Effect of noise levels on value of loss function, c) Effect of gradient bound
on test accuracy, d) Effect of gradient bound on value of loss function.

the effectiveness of the proposed BPFISH framework on
various privacy parameters. The simulation results demon-
strated that BPFISH achieves high accuracy under a strong
privacy protection level. Performance study shows that our
proposed BPFISH framework outperforms state-of-the-art
methods, achieving 11.18% better results on an average.

In the future, we would like to consider other privacy-
preserving techniques such as secure multi-party compu-
tation or homomorphic encryption. We would also like to
work on energy efﬁcient blockchain-based FL framework
to reduce energy consumption during the FL process. In
addition, we will consider state-of-the-art neural network
architectures and different data augmentation techniques to
observe the effects of privacy parameters on accuracy.
Acknowledgments: This work is supported by the Science
and Engineering Research Board (SERB), Government of
India under Grant SRG/2020/000318.

REFERENCES

[1] R. P. Singh, M. Javaid, A. Haleem, and R. Suman, “Internet of
things (IoT) applications to ﬁght against covid-19 pandemic,”
Diabetes & Metabolic Syndrome: Clinical Research & Reviews, vol. 14,
no. 4, pp. 521–524, 2020.

[2] B. A. Jnr, “Use of telemedicine and virtual care for remote
treatment in response to covid-19 pandemic,” Journal of Medical
Systems, vol. 44, no. 7, pp. 1–9, 2020.

[3] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine
learning: Concept and applications,” ACM Trans. Intell. Syst.
Technol., vol. 10, no. 2,
[Online]. Available: https:
jan 2019.
//doi.org/10.1145/3298981

[4] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efﬁcient learning of deep networks from
decentralized data,” in Artiﬁcial intelligence and statistics. PMLR,
2017, pp. 1273–1282.

[5] K. Powell. NVIDIA clara federated learning to deliver AI
to hospitals while protecting patient data:
Intelligent edge
computing platform streamlines deep learning for radiology.
[Online]. Available: https://blogs.nvidia.com/blog/2019/12/01/
clara-federated-learning/

[6] A. I. Newaz, A. K. Sikder, M. A. Rahman, and A. S. Uluagac,
“Healthguard: A machine learning-based security framework for
smart healthcare systems,” in 2019 Sixth International Conference on
Social Networks Analysis, Management and Security (SNAMS), 2019,
pp. 389–396.

[7] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated learning:
Challenges, methods, and future directions,” IEEE Signal Process-
ing Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[8] H. Kim, J. Park, M. Bennis, and S.-L. Kim, “Blockchained on-device
federated learning,” IEEE Communications Letters, vol. 24, no. 6, pp.
1279–1283, 2020.

[9] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating
noise to sensitivity in private data analysis,” in Theory of Cryptog-
raphy, S. Halevi and T. Rabin, Eds. Berlin, Heidelberg: Springer
Berlin Heidelberg, 2006, pp. 265–284.

[10] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential
privacy,” Proceedings of
the 2016 ACM SIGSAC Conference
on Computer and Communications Security, Oct 2016. [Online].
Available: http://dx.doi.org/10.1145/2976749.2978318

[11] N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, and
G. Yu, “Collecting and analyzing multidimensional data with local
differential privacy,” in 2019 IEEE 35th International Conference on
Data Engineering (ICDE).

IEEE, 2019, pp. 638–649.

[12] S. Tuli, N. Basumatary, S. S. Gill, M. Kahani, R. C. Arya, G. S.
Wander, and R. Buyya, “Healthfog: An ensemble deep learning
based smart healthcare system for automatic diagnosis of heart
diseases in integrated IoT and fog computing environments,”
Future Generation Computer Systems, vol. 104, pp. 187–200, 2020.
[13] Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A
federated transfer learning framework for wearable healthcare,”
IEEE Intelligent Systems, vol. 35, no. 4, pp. 83–93, 2020.

[14] S. Silva, B. A. Gutman, E. Romero, P. M. Thompson, A. Alt-
mann, and M. Lorenzi, “Federated learning in distributed medical
databases: Meta-analysis of large-scale subcortical brain data,” in
2019 IEEE 16th international symposium on biomedical imaging (ISBI
2019).

IEEE, 2019, pp. 270–274.

[15] A. Sadilek, L. Liu, D. Nguyen, M. Kamruzzaman, S. Serghiou,
B. Rader, A. Ingerman, S. Mellem, P. Kairouz, E. O. Nsoesie et al.,
“Privacy-ﬁrst health research with federated learning,” NPJ digital
medicine, vol. 4, no. 1, pp. 1–8, 2021.

[16] J. Li, Y. Meng, L. Ma, S. Du, H. Zhu, Q. Pei, and S. Shen,
“A federated learning based privacy-preserving smart healthcare
system,” IEEE Transactions on Industrial Informatics, pp. 1–1, 2021.
[17] X. Li, Y. Gu, N. Dvornek, L. H. Staib, P. Ventola, and J. S. Duncan,
“Multi-site fmri analysis using privacy-preserving federated
learning and domain adaptation: Abide results,” Medical Image
Analysis, vol. 65, p. 101765, 2020. [Online]. Available: https://
www.sciencedirect.com/science/article/pii/S1361841520301298

[18] K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin,
T. Q. S. Quek, and H. V. Poor, “Federated learning with differential
privacy: Algorithms and performance analysis,” IEEE Transactions
on Information Forensics and Security, vol. 15, pp. 3454–3469, 2020.

[19] O. Novo, “Blockchain meets IoT: An architecture for scalable
access management in IoT,” IEEE Internet of Things Journal, vol. 5,
no. 2, pp. 1184–1195, 2018.

[20] X. Zhang and S. Poslad, “Blockchain support for ﬂexible queries
with granular access control to electronic medical records (emr),”
in 2018 IEEE International conference on communications (ICC).
IEEE, 2018, pp. 1–6.

[21] G. G. Dagher, J. Mohler, M. Milojkovic, and P. B. Marella, “Ancile:
Privacy-preserving framework for access control and interoper-
ability of electronic health records using blockchain technology,”
Sustainable cities and society, vol. 39, pp. 283–297, 2018.

[22] C. Li, M. Dong, J. Li, G. Xu, X. Chen, and K. Ota, “Healthchain:
Secure emrs management and trading in distributed healthcare
service system,” IEEE Internet of Things Journal, vol. 8, no. 9, pp.
7192–7202, 2021.

[23] S. Seng, C. Luo, X. Li, H. Zhang, and H. Ji, “User matching
on blockchain for computation ofﬂoading in ultra-dense wireless
networks,” IEEE Transactions on Network Science and Engineering,
vol. 8, no. 2, pp. 1167–1177, 2021.

[24] W. Y. B. Lim, J. Huang, Z. Xiong, J. Kang, D. Niyato, X.-S.
Hua, C. Leung, and C. Miao, “Towards federated learning in
uav-enabled internet of vehicles: A multi-dimensional contract-
matching approach,” IEEE Transactions on Intelligent Transportation
Systems, 2021.

 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14AccuracyNumber of Global Iterationsnoise = 0.25noise = 0.6noise = 1.0 0.2 0.3 0.4 0.5 0.6 0.7 0.8 2 4 6 8 10 12 14Value of Loss FunctionNumber of Global Iterationsnoise = 0.25noise = 0.6noise = 1.0 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14AccuracyNumber of Global IterationsA = 1.0A = 4.0A = 8.0 0.2 0.3 0.4 0.5 0.6 0.7 0.8 2 4 6 8 10 12 14Value of Loss FunctionNumber of Global IterationsA = 1.0A = 4.0A = 8.012

Moirangthem Biken Singh completed the
B.Tech degree in Computer Science and En-
gineering from the National Institute of Tech-
nology Manipur, India, in 2018 and the M.Tech
degree from National Institute of Technology Ku-
rukshetra, India, in 2021. He is currently pursu-
ing Ph.D. degree in Computer Science and En-
gineering, Indian Institute of Technology (BHU)
Varanasi,
India. His current research interest
include AI, machine learning and FL in Smart
Healthcare.

Ajay Pratap is an Assistant Professor with the
Department of Computer Science and Engi-
neering, Indian Institute of Technology (BHU)
Varanasi, India. Before joining IIT (BHU), he was
associated with the Department of Computer
Science and Engineering, National Institute of
Technology Karnataka (NITK) Surathkal, India,
as an Assistant Professor from December 2019
to May 2020. He worked as a Postdoctoral Re-
searcher in the Department of Computer Sci-
ence at Missouri University of Science and Tech-
nology, USA, from August 2018 to December 2019. He completed his
Ph.D. degree in Computer Science and Engineering from the Indian
Institute of Technology Patna, India, in July 2018. His research inter-
ests include Cyber-Physical Systems, IoT-enabled Smart Environments,
Mobile Computing and Networking, Statistical Learning, Algorithm De-
sign for Next-generation Advanced Wireless Networks, Applied Graph
Theory, and Game Theory. His current work is related to HetNet, Small
Cells, Fog Computing, IoT, and D2D communication underlaying cellular
5G and beyond. His papers appeared in several international journals
and conferences including IEEE Transactions on Mobile Computing,
IEEE Transactions on Parallel and Distributed Systems, and IEEE LCN,
etc. He has received several awards including the Best Paper Candidate
Award and NSF travel grant for IEEE Smartcom’19 in the USA.

[25] M. Kadadha, H. Otrok, S. Singh, R. Mizouni, and A. Ouali, “Two-
sided preferences task matching mechanisms for blockchain-based
crowdsourcing,” Journal of Network and Computer Applications,
vol. 191, p. 103155, 2021.
[Online]. Available: https://www.
sciencedirect.com/science/article/pii/S1084804521001697

[26] J. An, H. Yang, X. Gui, W. Zhang, R. Gui, and J. Kang, “TCNS:
Node selection with privacy protection in crowdsensing based on
twice consensuses of blockchain,” IEEE Transactions on Network and
Service Management, vol. 16, no. 3, pp. 1255–1267, 2019.

[27] R. Chen, M. Chen, and H. Yang, “Dynamic physician-patient
matching in the healthcare system,” in 2020 42nd Annual Interna-
tional Conference of the IEEE Engineering in Medicine Biology Society
(EMBC), 2020, pp. 5868–5871.

[28] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and federated learning for privacy-preserved data sharing in
industrial IoT,” IEEE Transactions on Industrial Informatics, vol. 16,
no. 6, pp. 4177–4186, 2019.

[29] K. Gai, Y. Wu, L. Zhu, Z. Zhang, and M. Qiu, “Differential
privacy-based blockchain for industrial internet-of-things,” IEEE
Transactions on Industrial Informatics, vol. 16, no. 6, pp. 4156–4165,
2020.

[30] B. Jia, X. Zhang, J. Liu, Y. Zhang, K. Huang, and Y. Liang,
“Blockchain-enabled federated learning data protection aggrega-
tion scheme with differential privacy and homomorphic encryp-
tion in IIoT,” IEEE Transactions on Industrial Informatics, 2021.
[31] P. Ramanan and K. Nakayama, “BAFFLE: Blockchain based aggre-
gator free federated learning,” in 2020 IEEE International Conference
on Blockchain (Blockchain).

IEEE, 2020, pp. 72–81.

[32] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,”

Cryptography Mailing list at https://metzdowd.com, 03 2009.

[33] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei,
“Energy efﬁcient federated learning over wireless communication
networks,” IEEE Transactions on Wireless Communications, vol. 20,
no. 3, pp. 1935–1949, 2021.

[34] A. Pratap and S. K. Das, “Stable matching based resource alloca-
tion for service provider’s revenue maximization in 5G networks,”
IEEE Transactions on Mobile Computing, pp. 1–1, 2021.

[35] J. Kang, Z. Xiong, D. Niyato, S. Xie, and J. Zhang, “Incentive
mechanism for reliable federated learning: A joint optimization
approach to combining reputation and contract theory,” IEEE
Internet of Things Journal, vol. 6, no. 6, pp. 10 700–10 714, 2019.
[36] C. K. Leung, D. L. X. Fung, S. B. Mushtaq, O. T. Leduchowski,
R. L. Bouchard, H.
Jin, A. Cuzzocrea, and C. Y. Zhang,
“Data science for healthcare predictive analytics,” in Proceedings
the 24th Symposium on International Database Engineering
of
’I&’ Applications,
IDEAS ’20. New York, NY, USA:
Association for Computing Machinery, 2020. [Online]. Available:
https://doi.org/10.1145/3410566.3410598
[37] A. Reisizadeh, A. Mokhtari, H. Hassani, A.

Jadbabaie, and
R. Pedarsani, “Fedpaq: A communication-efﬁcient federated learn-
ing method with periodic averaging and quantization,” in Interna-
tional Conference on Artiﬁcial Intelligence and Statistics. PMLR, 2020,
pp. 2021–2031.

ser.

[38] D. Gale and L. S. Shapley, “College admissions and the stability of
marriage,” The American Mathematical Monthly, vol. 69, no. 1, pp.
9–15, 1962.

[39] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei,
“Energy efﬁcient federated learning over wireless communication
networks,” IEEE Transactions on Wireless Communications, vol. 20,
no. 3, pp. 1935–1949, 2020.

[40] A. Singh, P. Vepakomma, O. Gupta, and R. Raskar, “Detailed
comparison of communication efﬁciency of split learning and
federated learning,” arXiv preprint arXiv:1909.09145, 2019.

[41] P. Mooney. Chest x-ray images (pneumonia), version 2. [On-
line]. Available: https://www.kaggle.com/paultimothymooney/
chest-xray-pneumonia

[42] N. Srivastava, G. Hinton, A. Krizhevsky,

I. Sutskever, and
R. Salakhutdinov, “Dropout: A simple way to prevent neural
networks from overﬁtting,” J. Mach. Learn. Res., vol. 15, no. 1, p.
1929–1958, jan 2014.

