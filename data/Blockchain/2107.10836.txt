Qanaat: A Scalable Multi-Enterprise Permissioned
Blockchain System with Confidentiality
Guarantees

2
2
0
2

l
u
J

7
1

]

B
D
.
s
c
[

2
v
6
3
8
0
1
.
7
0
1
2
:
v
i
X
r
a

Mohammad Javad Amiri1 Boon Thau Loo1 Divyakant Agrawal2 Amr El Abbadi2
1University of Pennsylvania, 2University of California Santa Barbara
1{mjamiri, boonloo}@seas.upenn.edu 2{agrawal, amr}@cs.ucsb.edu

Abstract

Today’s large-scale data management systems need to address distributed applications’ con-
ﬁdentiality and scalability requirements among a set of collaborative enterprises. This paper
presents Qanaat, a scalable multi-enterprise permissioned blockchain system that guarantees
the conﬁdentiality of enterprises in collaboration workﬂows. Qanaat presents data collec-
tions that enable any subset of enterprises involved in a collaboration workﬂow to keep
their collaboration private from other enterprises. A transaction ordering scheme is also
presented to enforce only the necessary and suﬃcient constraints on transaction order to
guarantee data consistency. Furthermore, Qanaat supports data consistency across collab-
oration workﬂows where an enterprise can participate in diﬀerent collaboration workﬂows
with diﬀerent sets of enterprises. Finally, Qanaat presents a suite of consensus protocols to
support intra-shard and cross-shard transactions within or across enterprises.

1

Introduction

Emerging multi-enterprise applications, e.g., supply chain management [61], multi-platform crowdworking
[10], and healthcare [15], require extensive use of collaboration workﬂows where multiple mutually distrustful
distributed enterprises collaboratively process a mix of public and private transactions. Despite years of
research in distributed transaction processing, data management systems today have not yet achieved a
good balance among the conﬁdentiality and scalability requirements of these applications.
One of the main requirements of multi-enterprise applications is conﬁdentiality with respect to data sharing
and data leakage. First, while public collaboration among all enterprises is visible to everyone, speciﬁc
subsets of the data may need to be shared only with speciﬁc subsets of involved enterprises. For example,
in a product trading, the distributor may want to keep collaboration with a farmer and a shipper involving
terms of the trades conﬁdential from the wholesaler and the retailer, so as not to expose the premium they
are charging [54]. Second, the enterprises must prevent malicious nodes from leaking conﬁdential data, e.g.,
requests, replies, and stored data.

Multi-enterprise applications also need to scalably process a large number of transactions within or across
enterprises. Although sharding has been used in single-enterprise applications to address scalability, it is
challenging to use sharding in multi-enterprise applications where conﬁdentiality of data is paramount.
In multi-enterprise applications, furthermore, data provenance is needed to provide a detailed picture of
how the data was collected, where it was stored, and how it was used. The information should be stored
in a transparent and immutable manner which is veriﬁable by all participants, e.g., transparent end-to-end
tracking of goods to detect possible fraud in a supply chain collaboration workﬂow.

The decentralized nature of blockchain and its unique features such as provenance, immutability, and tamper-
resistant, make it appealing to a wide range of applications, e.g., supply chain management [32, 97], crowd-
sourcing [51, 10], contact tracing [81] and federated learning [82].

 
 
 
 
 
 
Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

In recent years, various permissioned blockchain systems have been proposed to address the conﬁdentiality
and/or scalability of multi-enterprise applications. Hyperledger Fabric [12] and its variants [47, 46, 91, 87]
supports multi-enterprise applications and provides scalability using channels [13] managed by subsets of
enterprises. However, all transactions of a channel are sequentially ordered, resulting in reduced perform-
ance. Fabric also uses private data collections [54] to manage conﬁdential collaboration among a subset of
enterprises. However, appending a hash of all private transactions to a global ledger replicated on every
enterprise increases computational overhead and hence reduces throughput. Furthermore, Fabric does not
address conﬁdential data leakage by malicious nodes.

Caper [6] supports collaborative enterprises and provides conﬁdentiality by maintaining a partial view of
the global ledger on each enterprise. Caper, however, does not support: (1) conﬁdential collaboration
among subsets of enterprises (i.e., it supports internal or public transactions), (2) data consistency across
collaboration workﬂows that an enterprise is involved in, (3) data conﬁdentiality in the presence of malicious
nodes, and (4) multi-shard enterprises.

Scalability has also been studied in the context of applications that are used within a single enterprise, e.g.,
AHL [36], SharPer [9] and ResilientDB [49]. However, since these systems are restricted to single-enterprise
applications, they do not address the conﬁdentiality requirement of multi-enterprise applications.

To address the above scalability and conﬁdentiality challenges of multi-enterprise applications, we present
Qanaat1, a permissioned blockchain system that supports collaboration workﬂows across enterprises.
In
Qanaat, each enterprise partitions its data into multiple shards to improve scalability. Each shard’s trans-
actions are then processed by a disjoint cluster of nodes.

To support conﬁdential collaboration (i.e., data sharing), in addition to public transactions among all enter-
prises and internal transactions within each enterprise, any subset of enterprises might process transactions
conﬁdentially from other enterprises. As a result, Qanaat uses a novel hierarchical data model consisting of
a set of data collections where each transaction is executed on a data collection. The root data collection
consists of the public records from executing public transactions replicated on all enterprises. Each local
data collection includes the private records of a single enterprise. Intermediate data collections, which are
optional and are needed in case of conﬁdential collaborations among subsets of enterprises, maintain private
records shared among subsets of collaborating enterprises. Qanaat replicates such shared data collections
on the involved enterprises to facilitate the use of shared data by enterprises in their internal transactions.
For example, a supplier in a supply chain collaboration workﬂow requires the order data stored in the shared
data collection with a consumer to perform its internal transactions that produce the products.

To support this collaborative yet conﬁdential data model, Qanaat proposes a transaction ordering scheme to
preserve the data consistency of transactions. In Qanaat, an enterprise might be involved in multiple data
collections. Hence, the traditional transaction ordering schemes used in fault-tolerant protocols where each
cluster orders transactions on a single data store do not work. In fact, while ordering transactions, Qanaat
needs to capture the state of all data collections that might aﬀect the execution of a transaction.

Finally, to prevent conﬁdential data leakage despite the Byzantine failure of nodes, Qanaat utilizes the
privacy ﬁrewall technique [103, 40] and (1) separates ordering nodes that agree on the order of transactions
from execution nodes that execute transactions and maintain the ledger, and (2) uses a privacy ﬁrewall
between execution nodes and ordering nodes. The privacy ﬁrewall restricts communication from execution
nodes in order to ﬁlter out incorrect messages, possibly including conﬁdential data.

The main contributions of this paper are:

• An infrastructure consisting of ordering nodes, execution nodes, and a privacy ﬁrewall and a data
model consisting of data collections to preserve conﬁdentiality, followed by a transaction ordering
scheme that enforces only the necessary and suﬃcient constraints to guarantee data consistency,
• A suite of consensus protocols to process transactions that accesses single or multiple data shards

within or across enterprises in a scalable manner, and

• Qanaat, a scalable multi-enterprise permissioned blockchain system that guarantees conﬁdentiality.

The rest of this paper is organized as follows. Section 2 motivates Qanaat by discussing the vaccine supply
chain. The Qanaat model is introduced in Section 3. Section 4 presents transaction processing in Qanaat.
Section 5 evaluates the performance of Qanaat. Section 6 discusses related work, and Section 7 concludes
the paper.

1Qanaat is a scalable underground network consisting of private channels to transport water from an aquifer to

the surface (i.e., an underground aqueduct).

2

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 1: A vaccine supply chain collaboration workﬂow

2 Motivation

Qanaat is designed to support multi-enterprise applications such as supply chain management [61], multi-
platform crowdworking [10], and healthcare [15] in a scalable and conﬁdentiality-preserving manner. To
motivate Qanaat, we consider an example application based on the COVID-19 vaccine supply chain, given
that it is a demanding application. The COVID-19 outbreak has demonstrated diverse challenges that supply
chains face [4]. While the vaccine supply chain seems to work ﬁne on the surface, it suﬀers from several
crucial challenges. First, current vaccine supply chains are subjected to diﬀerent types of vulnerabilities, e.g.,
attacks on 44 companies involved in the COVID-19 vaccine distribution in 14 countries [79], rapidly growth
of the black market for COVID-19 vaccines [94], issuing fake vaccine cards [84], and loss of 400 millions of
vaccine doses [25]. Moreover, while vaccines are well distributed in most developed countries, getting them
to developing countries and accounting for them remains a challenge, e.g., fake vaccines [1, 34] and falsiﬁed
CoviShield vaccines [75]. Currently, the COVID-19 vaccine supply chains have been over-stretched given the
global supply chain challenges and the possibility of abuse and fraud stated above. At the same time, other
supply chains are now under unprecedented pressure [89, 19]. Even in a developed country, supply chains
can be subjected to attacks that can potentially disrupt the distribution [79]. Vaccines are vulnerable to
counterfeiting [17, 5], tampering and theft [77] especially since products and components often pass through
multiple locations and even countries and it has become diﬃcult to trace the source of vaccines, creating
space for substandard and falsiﬁed vaccines [76].

Multiple enterprises collaborate in a vaccine supply chain to get vaccines from manufacturers to citizens.
Enterprises involved in the vaccine ecosystem need to collaborate and share data based on agreed-upon
service level agreements. Vaccines, on one hand, need to be tracked and monitored throughout the process
to ensure that they are stable and not tampered with, and on the other hand, this data should be accessible
to the public to increase their trust in the vaccines. While distributed databases can be used to address these
challenges, they lack mechanisms to ﬁrst prevent any malicious entity from altering data and second enable
enterprises to verify the data and the state of collaboration. Qanaat uses blockchains to provide veriﬁability,
provenance, transparency, and assurances to end-users of the vaccines’ safety and eﬃcacy. In particular,
blockchains enable pharmaceutical manufacturers to easily track on-time shipment and delivery of vaccines,
provide eﬃcient delivery tracking for transportation companies, and help hospitals manage their stocks and
mitigate supply and demand constraints.

Figure 1 shows a simpliﬁed vaccine supply chain collaboration workﬂow consisting of the pharmaceutical
manufacturer (M), a supplier (S), a logistics provider (L), a transportation company (T), and hospitals (H).
Using Qanaat, the public transactions of the collaboration workﬂow (T1 to T8) are executed on data collection
dM SLT H maintained by all enterprises. In particular, the pharmaceutical manufacturer places orders (T1 and
T2) of materials via the supplier and logistics provider. The logistics provider arranges shipment (T3) by the
transportation company, Once the materials are ready, the supplier informs (T4) the transportation company
to pick order (T5) and deliver order (T6) to the manufacturer. Once the vaccines become available, the
transportation company pick vaccines (T7) and deliver (T8) them to hospitals. These transactions are public
and executed on data collection dM SLT H .

In addition, the collaboration workﬂow includes the internal transactions of each enterprise. Internal trans-
actions are executed on the enterprise’s private data collection and might reﬂect patented and copyrighted
formulas and information. For example, producing vaccines in a pharmaceutical manufacturer consists of
material reception (TM 1) (some vaccines requiring ∼160 consumables), ingredient manufacturing (TM 2),
coupling (TM 3), formulation (TM 4), filling (TM 5), and packaging (TM 6) [88, 99]. The pharmaceutical
manufacturer executes these transactions on its private data collection dM . The supplier, logistics provider,

3

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

transportation company, and hospitals also execute their internal transactions on private data collections dS,
dL, dT , and dH respectively.

A blockchain-enabled multi-enterprise application, e.g., vaccine supply chain, needs to support the following
requirements:

R1. Conﬁdential collaborations across enterprises. Any subset of enterprises involved in a collab-
oration workﬂow might want to keep their collaboration private from other enterprises. For example, the
supplier might want to make private transactions with the pharmaceutical manufacturer to keep some terms
of trade conﬁdential from other enterprises, e.g., price quotation (TM S1) transaction. Such private trans-
actions need to be executed on a data collection shared between only the involved enterprises, e.g., dM S
between the pharmaceutical manufacturer and the supplier.

R2. Consistency across collaboration workﬂows. An enterprise might be involved in multiple collab-
oration workﬂows with diﬀerent sets of enterprises. For example, a transportation company that distributes
both Pﬁzer and Moderna vaccines needs to assign trucks based on the total number of vaccine packages and
deal with interleaving transactions. In case of data dependency among transactions that span collaboration
workﬂows, data integrity and consistency must be maintained.

R3. Conﬁdential data leakage prevention. An enterprise needs to ensure that even if its infrastructure
includes malicious nodes (i.e., an attacker manages to compromise some nodes), the malicious nodes cannot
leak any conﬁdential data, e.g., requests, replies, processed data and stored data.

R4. Scaling multi-shard enterprises. Every day, millions of people get vaccinated, thousands of ship-
ments take place and many other transactions are executed. All these activities need to be immediately
processed by the system and speciﬁcally by all involved enterprises. A transportation company that distrib-
utes vaccines across the world might maintain its data in multiple data shards. This highlights the need
for a system that can scale as demand increases and eﬃciently process distributed transactions that access
multiple data shards across multiple enterprises.

Existing blockchain solutions, however, are not able to meet all requirements of multi-enterprise collaboration
workﬂows. While Caper [6] enables enterprises to keep their local data conﬁdential, it does not address
any of the R1 to R4 requirements. Fabric [12] addresses conﬁdential collaboration using private data
collection (although with a high overhead) and scalability using channels. However, Fabric does not support
requirements R2 and R3. Several variants of Fabric, such as Fast Fabric [47], Fabric++ [91], FabricSharp
[87], and XOX Fabric [46], try to address the performance shortcomings of Fabric, especially when dealing
with contentious workloads. However, these permissioned blockchain systems, similar to Fabric, suﬀer from
the overhead of conﬁdential collaboration and also do not support requirements R2 and R3.

3 Qanaat Model

Qanaat is a permissioned blockchain system designed to support multi-enterprise applications. This sec-
tion introduces Qanaat model and demonstrates how it supports diﬀerent requirements of multi-enterprise
applications.

3.1 The Model Assumptions

Qanaat consists of a set of collaborative enterprises. Each enterprise owns a set of nodes (i.e., servers) that
are grouped into diﬀerent clusters. Each enterprise further partitions its data into multiple data shards.
Each cluster of nodes maintains a shard of the enterprise data and processes transactions on that data shard.

Qanaat assumes the partially synchronous communication model. In the partially synchrony model, there
exists an unknown global stabilization time (GST), after which all messages between correct nodes are
received within some bound ∆. Qanaat inherits the standard assumptions of previous permissioned block-
chain systems, including a strong computationally-bounded adversary that can coordinate malicious nodes
and delay communication to compromise service an unreliable network that connects nodes and might drop,
corrupt, or delay messages, and the existence of pairwise authenticated communication channels, standard
digital signatures and public-key infrastructure (PKI). A message m signed by node i is denoted as hmiσi .
Qanaat further uses threshold signatures where each node i holds a distinct private key that is used to create
a signature share σhmii for message m. A node can generate a valid threshold signature σhmi for m given
n − f signature shares from distinct nodes. A collision-resistant hash function D(.) is also used to map a
message m to a constant-sized digest D(m).

4

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 2: (a) A data model for a 4-enterprise collaboration workﬂow.
Intermediate data collections are
optional and needed only if there is a collaboration among a speciﬁc subset of enterprises, (b) data maintained
by enterprise A, (c) data models for 3-enterprise workﬂows.

We next present the data model (Section 3.2) and the blockchain ledger (Section 3.3) of Qanaat, which to-
gether address collaboration conﬁdentiality (R1) and data consistency (R2) requirements and then demon-
strate the Qanaat infrastructure (Section 3.4) that addresses the conﬁdential data leakage prevention re-
quirement (R3). For simplicity of presentation, we ﬁrst consider single-shard enterprises and then show how
the model and the ledger are extended to support multi-shard enterprises (Section 3.6), which addresses the
scalability requirement (R4).

3.2 Data Model

Qanaat constructs a hierarchical data model consisting of a set of data collections for each collaboration
workﬂow. Each data collection can be seen as a separate datastore where the execution of transactions
updates its data. Each data collection further has its own (business) logic to execute transactions. Public
transactions of a collaboration workﬂow, e.g., place order or arrange shipment in the vaccine supply chain,
are executed on the root data collection. All enterprises maintain the root data collection. This is needed
because enterprises use the data maintained in the root data collection in other transactions. For example,
the order data stored in the root data collection is used by the supplier in its private transactions to provide
raw materials.
Internal private transactions of each enterprise, on the other hand, are executed on its local data collection.
Internal transactions are performed within an enterprise following the logic of the enterprise, e.g., develop
formulation, and package vaccines take place within the pharmaceutical manufacturer.

Any subset of enterprises might also execute private transactions among themselves and conﬁdentially from
other enterprises. Such transactions are executed on a data collection shared among and maintained by only
the involved enterprises. For example, the supplier executes private transactions with the pharmaceutical com-
pany on the material demand on a data collection that is maintained only by the supplier and pharmaceutical
company.

It should be noted that a data collection is not a physically separated datastore, rather a logical partition
of the data records of an enterprise that might be shared with a set of enterprises. Hence, creating a data
collection causes no overhead, e.g., conﬁguration cost.

Figure 2(a) presents a data model for a collaboration workﬂow with enterprises A, B, C, and D. At the top
level, a public data collection dABCD is stored on all four enterprises. At the bottom level, there are four
private data collections dA, dB, dC, and dD stored on the corresponding enterprises. The ﬁgure also includes
all possible private data collections with diﬀerent subsets of enterprises at the intermediate levels, e.g., dAB
or dACD. When a subset of enterprises, e.g., A and B, creates a data collection, e.g., dAB, to execute private
transactions, the shared data collection maintains the execution results of transactions that are executed on
the data collection. Such data records are diﬀerent from the records maintained in the data collection of
each individual enterprise, e.g., dA and dB. In contrast to the root and the local data collections that are
needed in every collaboration workﬂow, intermediate data collections are optional and needed only if there
is a collaboration among a speciﬁc subset of enterprises.

Qanaat deﬁnes operational primitives necessarily to capture data consistency and data dependency across
data collections.

5

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Write. The execution results of transactions executed on a data collection are written on the records of
the same data collection. This is needed to guarantee collaboration conﬁdentiality as (intermediate) data
collections are created for private collaboration, e.g., private lobbying, among a subset of enterprises while
preventing other enterprises from accessing the data. In particular, if enterprises A and B make a conﬁdential
collaboration, the resulting data should not be written on a data collection that is either shared with some
other enterprise C or not shared with one of the involved enterprises, e.g., dA. Note that dAB is maintained
by both enterprises A and B and they have access to its record. This captures the conﬁdential collaborations
in real-world cross-enterprise applications.

Read. When a transaction is being executed on a data collection dX , it might read records of a data
collection dY if enterprises sharing dX are a subset of the enterprises sharing dY . For example, the internal
transactions of the supplier can read the records of all data collections that the supplier is involved in with
other enterprises. This is needed because those records might aﬀect the internal transactions of the supplier,
e.g., the number of vaccines that the supplier supplies depends on the orders it receives from other enterprises.

More formally, given a data collection dX where X is the set of enterprises sharing dX . For each data
collection dY where X ⊆ Y , we deﬁne dX as order-dependent of dY . We say transactions executed on dX
can read the records of dY if data collection dX is order-dependent on dY . The dashed lines between data
collections in Figure 2(a) present order-dependency among data collections; the transactions executed on
the lower-level data collections can read the records of the higher-level data collections, e.g., dAB can read
dABC, dABD and dABCD.

In some applications, transactions that are executed on data collection dX might also need to verify the
records of another data collection dY in a privacy-preserving manner (i.e., without reading the exact records)
revealing any information about the content of the records if enterprises sharing dX are a superset of the
enterprises sharing dY , i.e., Y ⊂ X.

In particular, for intangible assets, e.g., cryptocurrencies, if enterprise A initiates a transaction in data
collection dAB that consumes some coins, enterprise B needs to verify the existence of the coins in data
collection dA. In supply chain workﬂows, however, since transactions are usually placed as a result of physical
actions, verifying the records of order-dependent data collections is unnecessary. Qanaat can be extended
to support privacy-preserving veriﬁability using advanced cryptographic primitives like secure multiparty
computation [42, 14] and zero-knowledge proofs [63, 69, 44].

Every enterprise maintains all data collections that the enterprise is involved in, i.e., the root, a local, and
perhaps several intermediate data collections. Qanaat replicates shared data collections on all involved
enterprises to facilitate the use of shared data by the transactions on order-dependent data collections. Note
that such shared data collections can be maintained by a third party, e.g., a cloud provider, trusted by all
enterprises. However, this is a centralized solution that contradicts the decentralized nature of blockchains.
Moreover, the data maintained in a shared data collection might be used (i.e., read) in transactions on all
order-dependent data collections. Hence, enterprises need to query the cloud for every simple read operation
to access the data.

As shown in Figure 2(b), enterprise A maintains local data collection dA, root data collection dABCD, and
all intermediate data collections (if exist), e.g., dAB and dACD.

A data model represents data processed by each collaboration workﬂow among a set of enterprises. However,
an enterprise (or a group of enterprises) might be involved in multiple collaboration workﬂows (instances of
Qanaat) with diﬀerent sets of enterprises. The transactions of the same enterprise in diﬀerent collaboration
workﬂows might be data-dependent. Creating an independent data collection for each enterprise in each
collaboration workﬂow might result in data consistency issues. For example, a supplier that provides raw
materials for both Pﬁzer and Moderna vaccines in two diﬀerent supply chain collaboration workﬂows needs to
know the total number of the requested materials in order to provision for them correctly. As a result, Qanaat
creates a single data collection for each enterprise and if an enterprise is involved in multiple possibly data-
dependent collaboration workﬂows, the transactions of the enterprise in diﬀerent collaboration workﬂows are
executed on the same data collection. An enterprise might also collaborate with enterprises outside Qanaat
and require to access its data. Such data operations can be performed on the enterprise’s local data collection
without violating data consistency enabling the enterprise to use such data in its collaboration workﬂows
within Qanaat.

Figure 2(c) presents two data models for two collaboration workﬂows where enterprises K, L, and M are
involved in the ﬁrst and enterprises L, M , and N are involved in the second collaboration workﬂow. Since

6

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

L and M are involved in both workﬂows, the local data collections dL and dM and the intermediate data
collection dLM are shared in both collaboration workﬂows.

3.3 Blockchain Ledger

The blockchain ledger is an append-only data structure to maintain transaction records. The blockchain
ledger provides immutability and veriﬁability that prevents any malicious enterprise from altering data and
enables enterprises to verify the state of the data. When several enterprises execute transactions across
diﬀerent data collections, maintaining consistency and preserving conﬁdentiality in a scalable manner is
challenging. Before describing the Qanaat ledger, we discuss three possible solutions for ordering transactions
in cross-enterprise collaborations.

1. A single, global ledger. One possibility is to construct a linear blockchain ledger for each collaboration
workﬂow where all transactions on all data collections are totally ordered and appended to a single global
ledger. To preserve the conﬁdentiality of private transactions, the cryptographic hash of such transactions
(instead of the actual transaction) can be maintained in the blockchain ledger. This technique, which has
been used in Hyperledger Fabric [12], suﬀers from two main shortcomings. First, forcing all transactions
into a single, sequential order unnecessarily degrades performance, especially in the large-scale collaborations
targeted by Qanaat. To see this, note that while total ordering is needed for data-dependent transactions,
e.g., transactions executed on the same data collection, to preserve data consistency, total ordering among
transactions of independent data collections, e.g., dA and dB, is clearly not needed. Second, since this
solution requires a single, global blockchain ledger to be maintained by every enterprise, each enterprise needs
to maintain (the hash of) transactions that the enterprise was not involved in, e.g., internal transactions of
other enterprises, which increases bandwidth and storage costs.

2. One ledger for each data collection. A second possibility is to maintain a separate transaction
ordering (i.e., linear blockchain ledger) for each data collection. The blockchain ledger of each enterprise
then consists of multiple parallel linear ledgers (one per each data collection that the enterprise is involved
in). This solution, however, does not consider dependencies across order-dependent data collections where
the execution of a transaction might require reading records from other data collections. Such dependencies
need to be captured during the ordering phase to ensure that all replicas read the same state in the execution
phase. Note that the read-set and write-set of transactions might not be known before execution. As a result,
writing the read values into the block is not possible in the ordering phase.

For example, consider an internal transaction of the supplier that reads the order record of the root data
collection and based on that, computes and stores the results. In the meantime (during the period from the
initiation to the execution of the transaction), the value of the order record might change. As a result, if the
state of the root data collection has not been captured, diﬀerent nodes (i.e., replicas) of the supplier, might
read diﬀerent values (old or new) of the order in the execution phase resulting in inconsistency.

3. One ledger for each enterprise. A third solution is to maintain a total order among all transactions
in which an enterprise is involved. This solution, in contrast to the second solution, guarantees data con-
sistency. However, similar to the ﬁrst solution, this solution prevents order-independent transactions from
being appended to the ledger in parallel. For example, transactions on data collections dAB have no data
dependency with transactions on dAC and can append to the ledger of enterprise A in parallel.

Qanaat addresses the shortcomings of the existing solutions in guaranteeing data consistency and preserving
collaboration conﬁdentiality while creating a ledger in an eﬃcient and scalable manner.
In Qanaat, the
blockchain ledger guarantees two main properties:

• Local consistency. A total order on the transactions of each data collection is enforced due to

data dependency among transactions executed on a data collection, and

• Global consistency. The order of the transactions of data collection dX with respect to the
transactions of all data collections that dX is order-dependent on, is determined by capturing the
state of such data collections. Moreover, transactions are ordered across the enterprises consistently.

Qanaat constructs a single DAG-structured ledger for each enterprise consisting of transaction records of
the data collections that are maintained by the enterprise. The ﬁrst rule guarantees data consistency within
each single data collection and the second rule ensures global data consistency across all order-dependent
data collections maintained by an enterprise. Given the above rules, each transaction t initiated on data
collection dX has an identiﬁer, ID = hα, γi, composed of a local part α and possibly a global part γ where
the ID is assigned during the ordering phase. The local part α is [X:n] where X is a label representing the

7

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 3: (a)-(d) The blockchain ledger for enterprises A, B, C, and D in a collaboration workﬂow

involved enterprises in dX and n is a sequence number representing the order of transaction t on dX . In the
global part γ, for every data collection dY where dX is order-dependent on dY , Y :m is added to the γ of t to
capture the state of data collection dY . Here, Y :m is the local part of the ID of the last transaction that is
committed on data collection dY . When t is executed, it might read the state of a subset of data collections
that are captured in the global part of its ID.
Given two transactions t and t0 on data collection dX . Let ID(t) = hα, γi where α = [X:n], γ =
[..., Yp:mp, Yq:mq, ...] and ID(t0) = hα0, γ0i where α0 = [X:n0], and γ = [..., Yq:m0
r, ...]. If t is ordered
before t0 (t → t0) then:

q, Yr:m0

• n < n0 (local consistency), and
• ∀ data collection dYq ∈ γ ∩ γ0, mq ≤ m0

q (global consistency).

Figure 3(a)-(d) shows the blockchain ledger of four enterprises A, B, C, and D created in the Qanaat
model (following Figure 2(a)) for a collaboration workﬂow. The collaboration workﬂow includes two public
transactions on root data collection dABCD, several internal transactions on local data collections, and
multiple transactions among subsets of enterprises. In this ﬁgure, h[A:1], ∅i is an internal transaction on
data collections dA (with γ = ∅ because no transactions has been processed yet). h[ABCD:1], ∅i is a public
transaction on dABCD (with γ = ∅ because the root data collection is not order-dependent on any data
collections). As shown, two transactions h[ABC:1], [ABCD:1]i (on dABC) and h[BCD:1], [ABCD:1]i (on
dBCD) are appended to the ledger of enterprise B (as well as C) in parallel because dABC and dBCD are
not order-dependent. Both transactions include global part γ = [ABCD:1] because both dABC and dBCD
are order-dependent on dABCD. Transactions of Each non-local data collection, e.g., h[ABC:1], [ABCD:1]i,
which is shared between multiple enterprises, e.g., A, B, and C, is replicated on all involved enterprises is the
same order. h[BC:1], [ABC:1, BCD:1]i (on dBC) has γ = [ABC:1, BCD:1] as the global part of its ID. Note
that dBC in addition to dABC and dBCD, is order-dependent on dABCD. However, since the state of dABCD
is captured in the global part of transaction h[ABC:1], [ABCD:1]i (and h[BCD:1], [ABCD:1]i) and it has
not been changed, there is no need to add ABCD:1 to the global part of h[BC:1], [ABC:1, BCD:1]i. Dashed
lines, e.g., between h[B:1], ∅i and h[ABCD:1], ∅i, are only used to show the order of appending entries
to the ledger, i.e., does not imply any data or ordering dependencies. Note that while in this example,
most transactions are cross-enterprise (to help illustrate how a blockchain ledger is created), in practical
collaboration workﬂows, a signiﬁcant percentage of transactions are internal.

3.4 Qanaat Infrastructure

In Qanaat, nodes follow either the crash failure model or the Byzantine failure model. To guarantee fault
tolerance, clusters with crash-only nodes include 2f +1 nodes. In the presence of Byzantine nodes, 3f +1 nodes
are needed to provide fault tolerance [23]. The malicious nodes, however, can violate data conﬁdentiality by
leaking requests, replies, and data stored and processed at the nodes. To prevent conﬁdential data leakage
(known as intrusion tolerance [58, 98]), Qanaat can use either of the following two techniques: (1) restricting

8

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

[

: non-faulty,

: malicious,

: crashed, and : clients]

Figure 4: The ﬂow of reply messages in an instance of Qanaat with (a) 2f + 1 crash-only nodes, (b) 3f + 1
Byzantine ordering nodes and g + 1 crash-only execution nodes, (c) 3f + 1 Byzantine ordering and 2g + 1
Byzantine execution nodes, and a privacy ﬁrewall consisting of h + 1 crash-only ﬁlter nodes, and (d) 3f + 1
Byzantine ordering nodes, 2g + 1 Byzantine execution nodes and h + 1 rows of h + 1 Byzantine ﬁlter nodes.

the data that nodes can access [78, 98, 21, 70, 20] using secret sharing schemes, or (2) adding a privacy
ﬁrewall between the ordering nodes and the execution nodes [103, 40].

In the secret sharing scheme, clients encode data using an (f + 1, n)-threshold secret sharing scheme, where
f + 1 shares out of n total shares are needed to reconstruct the conﬁdential data [58]. Secret sharing schemes
only perform basic store and retrieve operations (Belisarius [78] also supports addition) and do not support
general transactions that require nodes to manipulate the contents of stored data. As a result, this technique
is not suitable for blockchain systems that are supposed to support complex transactions.

In the privacy ﬁrewall mechanism [103], the infrastructure consists of 3f + 1 ordering nodes (where f is the
maximum number of malicious ordering nodes) that run a BFT protocol to order client requests, 2g + 1
execution nodes (where g is the maximum number of malicious execution nodes) that maintain data and
deterministically execute arbitrary transactions following the order assigned by ordering nodes, and a privacy
ﬁrewall consisting of a set of h + 1 rows of h + 1 ﬁlters (where h is the maximum number of malicious ﬁlter
nodes) between the ordering nodes and execution nodes. The privacy ﬁrewall architecture assumes a network
conﬁguration that physically restricts communication paths between ordering nodes, ﬁlters, and execution
nodes, i.e., clients can only communicate with ordering (and not execution) nodes and each ﬁlter has a
physical network connection only to all (ﬁlter) nodes in the rows above and below. As a result, a malicious
node can either access conﬁdential data (an execution node or a ﬁlter) or communicate freely with clients
(an ordering node) but not both. The h + 1 rows of h + 1 ﬁlters guarantee that ﬁrst, there is at least one path
between execution nodes and ordering nodes which only consists of non-faulty ﬁlters (liveness) and second,
since there are h + 1 rows and maximum h malicious ﬁlters, there exist a row consisting of only non-faulty
ﬁlters that ﬁlters any malicious message (possibly including conﬁdential data). As a result, the rows below
have no access to any conﬁdential data leaked by malicious execution nodes. Note that if h ≤ 3f , ordering
In most
nodes can be merged with the bottom row of ﬁlters by placing a ﬁlter on each ordering node.
applications, request and reply bodies must also be encrypted, thus, ordering nodes cannot read them (while
clients and execution nodes can).

By separating ordering nodes from execution nodes, a simple majority of non-faulty nodes is suﬃcient to
mask Byzantine failure among execution nodes, i.e., 2g + 1 execution nodes can tolerate g Byzantine faults.
This is important because, compared to ordering, executing transactions, and maintaining the application
logic (e.g., smart contracts) and data require more powerful computation, storage, and I/O resources. The
overhead of the privacy ﬁrewall mechanism can be reduced by adding h ﬁlters to each row while providing
a higher degree of conﬁdentiality [40].

Separating ordering nodes from execution nodes and using a privacy ﬁrewall comes with extra resource
costs because ordering nodes and execution nodes need to be physically separated. However, if a cluster is
deployed in a cloud platform, ordering nodes and execution nodes can match the control layer and computing
nodes, respectively. Similarly, tools such as internal authorization services, node auditors, and load balancers
deployed in existing cloud platforms can be used as privacy ﬁrewalls [40].

Qanaat prevents conﬁdential data leakage despite Byzantine failure using the privacy ﬁrewall mechanism
presented in [103].

9

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 4 presents a cluster in Qanaat and shows the ﬂow of reply messages in diﬀerent settings where diﬀerent
types of nodes follow diﬀerent failure models. When nodes are crash only, as shown in Figure 4(a), a cluster
consisting of 2f + 1 nodes can order and execute transaction while conﬁdential data leakage is prevented.
If ordering nodes follow the Byzantine failure model, the ordering nodes and execution nodes need to be
separated to prevent data leakage. However, if execution nodes are crash-only, as shown in Figure 4(b),
there is no need to add a privacy ﬁrewall and execution nodes can directly send the reply to the client and
inform ordering nodes about execution. Since execution nodes are crash-only, g + 1 nodes are suﬃcient to
execute transactions. When both ordering and execution nodes might behave maliciously, a privacy ﬁrewall
consisting of a set of ﬁlters is needed. The ﬁrewall physically restricts communication from execution nodes,
hence, a malicious execution node cannot send conﬁdential data to a malicious ordering node that might
share the data with clients. If ﬁlters are crash-only (Figure 4(c)), a row of h + 1 ﬁlters is suﬃcient to ﬁlter
incorrect messages (data leakage) where h is the maximum number of crashed ﬁlters.

The general case, as shown in Figure 4(d), is when all ordering nodes, execution nodes and ﬁlters follow the
Byzantine failure model In this case, a privacy ﬁrewall with h + 1 rows of h + 1 ﬁlters per row is needed.
Communication is restricted so that each ﬁlter has a physical network connection only to all (ﬁlter) nodes
in the rows above and below.

3.5 Conﬁdentiality Guarantees

Qanaat provides collaboration conﬁdentiality through its hierarchical data model, which segregates data
into “data collections” that are only maintained by authorized enterprises. Qanaat also prevents conﬁdential
data leakage by separating ordering nodes from execution nodes and using a privacy ﬁrewall in between.
Conﬁdentiality is mainly preserved based on three main rules.

1. Data collections are completely separated. For example, dAC captures transactions that are executed
in a shared collaboration between A and C. This is diﬀerent from the data records of dA or dC (internal
transactions of A or C). Moreover, each data collection is maintained only by its involved enterprises.
Similarly, each transaction record in the blockchain ledger, as shown in Figure 3, is only maintained by the
involved enterprises.
2. Transactions of a data collection dX can read transactions of another data collection dY if and only if
X ⊆ Y . For example, transactions of dAB can read records of dABC because both A and B are involved in
dABC. However, transactions of dABC can not read records of dAB because enterprise C is not involved in
dAB.
3. A malicious node can either access conﬁdential data (an execution node or a ﬁlter) or communicate freely
with clients (an ordering node) but not both. This is guaranteed by separating ordering nodes from execution
nodes and using a privacy ﬁrewall.

Note that a compromised client may leak its own state or updates. Qanaat, similar to all other conﬁdential
fault-tolerant systems (that we are aware of) does not prevent this. Moreover, an enterprise might share the
data resulting from its cross-enterprise collaborations with some external entity or even grant its access to
the external entity. Qanaat cannot restrict such external data leakage.

3.6 Multi-Shard Enterprises

We now show how the data model and the blockchain ledger can be extended to support multi-shard enter-
prises. With single-shard enterprises, every execution node of each enterprise maintains all data collections
that the enterprise is involved in. Enterprises, however, partition their data into diﬀerent shards. Each
data shard is then assigned to a cluster of nodes where ordering nodes of the cluster order the transactions
and execution nodes maintain the data shard and execute transactions on the data shard. A shard of the
enterprise data might not include a shard of every data collection that the enterprise is involved in. For
example, consider a private collaboration between the logistic provider and the pharmaceutical manufacturer
maintained on a shared data collection. While the logistic provider might partition its data into multiple
shards, it maintains this shared data collection in only one of its data shards on a cluster that is placed in
Michigan (close to the Pﬁzer manufacturing site), e.g., a local collaboration.

We assume that enterprises use the same sharding schema for each shared data collection to facilitate
transaction processing across diﬀerent enterprises. The schema is agreed upon by all involved enterprises
when a data collection is created, i.e., the sharding schema is part of the conﬁguration metadata. Using the
same sharding schema leads to a more eﬃcient ordering phase for cross-enterprise transactions because there

10

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Table 1: Processing transactions in Qanaat

Transaction Type
Shard Enterprise
intra
intra
cross
cross

intra
cross
intra
cross

Example
Clusters- Shards
A1 - d1
A
C3, D3 - d3
A2, A3 - d2
B1,C1,B2,C2- d1

CD
A, d3
A
BC ,d2

BC

Consensus Protocol
Coordinator-based Flattened
Pluggable
Fig. 6(a)
Fig. 6(b)
Fig. 6(c)

Pluggable
Fig. 5(a)
Fig. 5(b)
Fig. 5(c)

is no need for all involved enterprises to run a consensus protocol to agree on the order of every transaction.
For each cross-enterprise transaction, one enterprise orders the transaction and other involved enterprises
only validate the order (details in Sections 4.3 and 4.4). Furthermore, diﬀerent data shards of an enterprise
are processed by diﬀerent clusters. Using the same sharding schema, enterprises can easily communicate
with the right cluster that processes the data records of transactions. If enterprises use diﬀerent sharding
schemas for a shared data collection, Qanaat could still process cross-enterprise transactions but with the
overhead required to execute a consensus protocol with every transaction on each cluster.

The blockchain ledger of a single-shard enterprise maintains all transactions that are executed on the enter-
prise data. In a multi-shard enterprise, the enterprise data is partitioned into diﬀerent shards where each
shard is replicated on a cluster of execution nodes. Since each cluster maintains a separate data shard, it
executes a diﬀerent set of transactions. As a result, each cluster in a multi-shard enterprise needs to maintain
a diﬀerent ledger. The ledger of each cluster of a multi-shard enterprise, however, is constructed in the same
way as a single-shard enterprise. Moreover, the notion of global consistency is extended to guarantee that
each cross-shard transaction is ordered across participating shards consistently.

4 Transaction Processing in Qanaat

Processing transactions requires establishing consensus on a unique order of requests. Fault-tolerant protocols
use the State Machine Replication (SMR) technique [65, 90] to assign each client request an order in the
global service history. In an SMR fault-tolerant protocol, all non-faulty nodes execute the same requests in
the same order (safety) and all correct client requests are eventually executed (liveness). Qanaat guarantees
safety in an asynchronous network, however, it considers a synchrony assumption to ensure liveness (due to
FLP results [43]).

A transaction might be executed on a single shard or on multiple shards of a data collection where the data
collection belongs to a single enterprise (i.e., a local data collection) or is shared among multiple enterprises
(i.e., a root or an intermediate data collection). As a result, as shown in Table 1, Qanaat needs to support
four diﬀerent types of transactions. In Table 1, the network consists of four enterprises where each enterprise
ψ∈{A, B, C, D} owns three clusters of nodes, e.g., ψ1, ψ2 and ψ3 and processes shard di
ψ by nodes of cluster
ψi.
An intra-shard transaction is executed on the same shard of a data collection either intra-enterprise, e.g., on
shard d1
CD of a shared data
collection dCD by clusters C3 and D3. A cross-shard transaction is executed on multiple shards of a data
collection either intra-enterprise, e.g., on shards d2
A and d3
A of a data collection dA by clusters A2, A3, or
cross-enterprise, e.g., on shards d1
BC of a shared data collection dBC by clusters B1, C1, B2, and
C2.

A of a local data collection dA by cluster A1, or cross-enterprise, e.g., on shard d3

BC and d2

A transaction can be executed on multiple shards of the same data collection (i.e., a cross-shard transaction)
or the same data shard that is maintained by multiple enterprises (i.e., a cross-enterprise transaction).
However, as mentioned earlier, a transaction can not be executed or write data records on multiple data
collections for two main reasons. First, the data of each (non-root) data collection is conﬁdentially shared
with only its involved enterprises. For example, transactions of dAB should not write data on dAC as C is not
involved in dAB. Note that transactions among enterprises A, B, and C are executed on dABC. Moreover,
there is no need to write the transaction results of dAB on dA or dB as both enterprises A and B store dAB
(see Figure 2(b)) and can read its data record in their local data collections dA and dB. Second, each data
collection might have its own business logic to execute transactions.

In Qanaat, cross-shard intra-enterprise transactions and intra-shard cross-enterprise transactions are handled
diﬀerently. In a cross-shard intra-enterprise transaction, diﬀerent clusters maintain separate data shards.
As a result, each cluster needs to separately reach agreement on the transaction order among transactions

11

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

of its shard. However, in an intra-shard cross-enterprise transaction, since the involved enterprises use the
same sharding schema for each shared data collection, the cluster that initiates the transaction and all the
involved clusters process the transaction on the same shard of data. Hence, the order of transactions on
diﬀerent clusters is the same. As a result, it is suﬃcient that one (initiator) cluster determines the order and
other clusters only validate the suggested order. Note that while the involved clusters belong to diﬀerent
distrustful enterprises, all clusters can detect any malicious behavior of the initiator cluster. We will discuss
how Qanaat addresses all possible malicious behaviors, e.g., assigning invalid ID (order) or even not sending
the transaction to other clusters. In cross-enterprise collaboration, enterprises cannot trust that the other
enterprise nodes might not be compromised. As a result, independent of the declared failure model of nodes,
Qanaat uses a BFT protocol to order cross-enterprise transactions. Furthermore, as it is suggested in Caper
[6], we can assume that less than a one-third of enterprises might be malicious. Hence, by a similar argument
as in PBFT [28], with agreement from at least two-thirds of the enterprises, a cross-enterprise transaction can
be committed. For cross-shard intra-enterprise transactions, on the other hand, the involved clusters belong
to the same enterprise and trust each other. As a result, if all nodes are crash-only, cross-shard consensus
can be achieved using a crash fault-tolerant protocol.
To process transactions across clusters coordinator-based and ﬂattened approaches are used.
In the
coordinator-based approach, inspired by existing permissioned blockchains such as AHL [36], Saguaro [11],
and Blockplane [73], a cluster coordinates transaction ordering, whereas, in the ﬂattened approach, transac-
tions are ordered across the clusters without requiring a coordinator.

Nodes in Qanaat follow diﬀerent failure models, i.e., crash or Byzantine. We mainly focus on two common
cases. First, when nodes follow the crash failure, a cluster contains 2f + 1 nodes that perform both ordering
and execution. Second, when nodes follow the Byzantine failure and each cluster includes 3f + 1 ordering
nodes, 2g + 1 execution nodes, and a privacy ﬁrewall consisting of h + 1 rows of h + 1 ﬁlters. The number of
required matching votes to ensure that a quorum of ordering nodes within a cluster agrees with the order of
a transaction is diﬀerent in diﬀerent settings. We deﬁne local-majority as the number of required matching
votes from a cluster. For crash-only clusters, local-majority is f + 1 (from 2f + 1 nodes), and for clusters
with Byzantine ordering nodes, local-majority is 2f + 1 (from 3f + 1 ordering nodes).

4.1

Intra-Cluster Consensus

In the internal (intra-cluster) consensus protocol, ordering nodes of a cluster, independently of other clusters,
agree on the order of a transaction. The internal consensus protocol is pluggable and depending on the failure
model of nodes of the cluster a crash fault-tolerant (CFT) protocol, e.g., (Multi-)Paxos [66] or a Byzantine
fault-tolerant (BFT) protocol, e.g., PBFT [28], can be used. If nodes follow the Byzantine failure model,
as discussed before, Qanaat separates ordering nodes from execution nodes and uses a privacy ﬁrewall to
prevent conﬁdential data leakage.
The protocol is initiated by a pre-elected ordering node of the cluster, called the primary. When the primary
node π(Pi) of cluster Pi receives a valid signed request message hREQUEST, op, tc, ciσc from an authorized
client c (with timestamp tc) to execute (encrypted) operation op on local data collection dPi, it initiates the
protocol by multicasting a message, e.g., accept message in Multi-Paxos (assuming the primary is elected) or
pre-prepare message in PBFT, including the req and its digest to other ordering nodes of the cluster.

To provide a total order among transaction blocks and preserve data consistency, the primary also assigns an
ID, as discussed in Section 3.3, to the request. The ID consists of a local part, e.g., [Pi:n], and a global part
including the state of all data collections that dPi is order-dependent on. the global part includes the state
of all data collections that dPi is order-dependent on because the read-set of the transaction is not known
(will be known during the execution time). The primary includes the current state (i.e., the local sequence
number of the last committed transaction) of such data collections in the global part. The ordering nodes
of the cluster then agree on the order of the transaction using the utilized protocol.

4.2 Transaction Execution Routine

The next step after ordering is to execute a transaction and inform the client. If nodes follow the Byzantine
failure model, ordering and execution are performed by distinct sets of nodes that are separated by a
privacy ﬁrewall. Once a transaction is ordered, the ordering nodes generate a commit certiﬁcate consisting
of signatures from 2f + 1 diﬀerent nodes and multicast both the request and the commit certiﬁcate to the
bottom row of ﬁlters. The ﬁlters check the request and the commit certiﬁcate to be valid and multicast them

12

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 5: (a) intra-shard cross-enterprise, (b) cross-shard intra-enterprise, and (c) cross-shard cross-enterprise
coordinator-based consensus protocols

to the next row above. Filters also track the requests that they have seen in a commit or a reply certiﬁcate
and their IDs.

Upon receiving both a valid request and a valid commit certiﬁcate, execution nodes append the transaction
and the corresponding commit certiﬁcate to the ledger. Note that the execution nodes store the last reply
certiﬁcate sent to client c to check if the current request is new (i.e., has a larger timestamp) that needs to
be executed and committed to the ledger or an old request (i.e., has the same or smaller timestamp) where
nodes re-send the reply messages. If all transactions accessing on dPi with lower local sequence numbers (< n)
have been executed, nodes execute the transaction on data collection dPi following its application logic. If,
during the execution, the execution nodes need to read values from some other data collection dX where dPi
is order-dependent on dX (i.e., Pi ⊆ X), the nodes check the public part of the transaction ID to ensure that
they read the correct state of dX . Data collections store data in multi-versioned datastores to enable nodes
to read the version they need to. This ensures that all execution nodes execute requests in the same order
and on the same state.

The commit certiﬁcate are appended to the ledger to guarantee immutability. Since commit messages include
the digest of transaction blocks, by appending them to the ledger, any attempt to alter the block data can
easily be detected.

Execution nodes then multicast a signed reply message including the (encrypted) results to the top row of
ﬁlters. When a ﬁlter node in the top row receives g + 1 valid matching reply messages, it generates a reply
certiﬁcate authenticated by g + 1 signature shares from distinct execution nodes and multicasts it to the
row below. Each ﬁlter then multicasts the reply certiﬁcate to the row of ﬁlter nodes or ordering nodes below.
Finally, client c accepts the result once it receives a valid reply certiﬁcate from ordering nodes.

If nodes follow the crash failure model, ordering and execution are performed by the same set of nodes. In
this case, nodes append the transaction to the ledger and execute it as explained before and then the primary
node sends a reply message to client c.

4.3 Coordinator-based Consensus

The coordinator-based approach has been used in distributed databases to process cross-shard transactions,
e.g., two-phase commit. Coordinator-based protocols have also been used in several permissioned blockchain
systems, e.g., AHL [36], Blockplane [73], and Saguaro [11]. This section brieﬂy demonstrates how Qanaat can
leverage the coordinator-based approach to process cross-cluster transactions. In contrast to the existing
coordinator-based databases and blockchains that address multi-shard single-enterprise contexts, Qanaat
deals with multi-shard multi-enterprise environments. Moreover, since each transaction might read data
from multiple data collections, ordering transactions is Qanaat is more complex. All three coordinator-based
protocols (i.e., intra-shard cross-enterprise, cross-shard intra-enterprise, and cross-shard cross-enterprise),
as shown in Figure 5, consist of prepare, prepared and commit phases. We brieﬂy explain the normal case
operation of the protocols.

13

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Prepare: Upon receiving a transaction, ordering nodes of the coordinator cluster establish (internal)
consensus on the order of the transaction among the transactions of their shard. The primary then sends
a signed prepare message to the ordering nodes of all involved clusters.

Prepared: This phase is handled diﬀerently in diﬀerent protocols.
Intra-shard cross-enterprise transactions: All involved clusters, e.g., C3 and D3 in Figure 5(a), maintain
the same data shard. As a result, there is no need to run consensus in non-coordinator clusters, e.g., D3.
Upon receiving a prepare message from the primary of the coordinator cluster, e.g., C3, each ordering node
of a non-coordinator cluster, e.g., D3, validates the order and sends a prepared message to the primary of
the coordinator cluster.
Cross-shard intra-enterprise transactions: The involved clusters, e.g., A2 and A3, maintain diﬀerent shards
of a local data collection, e.g., dA. Upon receiving a prepare message from the coordinator cluster, e.g., A2,
each non-coordinator cluster separately establishes (internal) consensus on the order of the transaction
in its data shard. The primary of each non-coordinator cluster, e.g., A3, then sends a prepared message
to the ordering nodes of the coordinator cluster, e.g., A2.
Cross-shard cross-enterprise transactions: The involved clusters of the initiator enterprise, e.g., B1 and
B2 of enterprise B, maintain diﬀerent shards of a shared data collection, e.g., dBC, while each shard is
replicated on clusters of diﬀerent enterprises, e.g., d1
BC is maintained by B1 and C1. Upon receiving a
prepare message from the coordinator cluster, e.g., B1, if the cluster belongs to the initiator enterprise,
e.g., B2, it establishes (internal) consensus on the transaction order. Otherwise, the cluster waits for a
message from the cluster of the initiator enterprise that maintains the same data shard as they do, e.g.,
C1 waits for B1 and C2 waits for B2. Each node then validates the received message and sends a prepared
message to the primary of the coordinator cluster.

Commit: When the primary of the coordinator cluster receives valid prepared messages from every
involved cluster, it establishes internal consensus within the coordinator cluster and multicasts a signed
commit message to ordering node of all involved clusters.

4.3.1

Intra-Shard Cross-Enterprise Consensus

In the intra-shard cross-enterprise protocol, multiple clusters (one from each enterprise) process a transaction
on the same shard of a non-local data collection shared between enterprises, e.g., clusters C3 and D3 on shard
d3
CD of data collection dCD (Figure 5(a)).
Upon receiving a valid transaction m, the primary of the coordinator cluster Pc, i.e., the cluster that receives
the transaction, assigns an ID to the transaction (as discussed in Section 3.3) and initiates internal consensus
on the order of m in the coordinator cluster.

Once consensus is achieved, the primary sends a signed prepare message hPREPARE, ID, d, miσPc
to the nodes
of all involved clusters where d = D(m) is the digest of request m. The prepare messages are signed by
local-majority of the clusters (denoted by σPc ). Upon receiving a prepare message with valid ID (following
the local and global consistency rules discussed in Section 3.3), digest, and signature, each node r sends a
signed prepared message hPREPARED, ID, d, riσr to the primary node of the coordinator cluster.
Upon receiving valid prepared messages from the local-majority of every involved cluster, the primary estab-
lishes internal consensus within the coordinator cluster and sends a commit message signed by local-majority
of the cluster to every node of all involved clusters. The commit message includes the transaction ID, its
digest, and also prepared messages received from the local-majority of every involved cluster as evidence of
message validity. When a node receives a valid commit message, the node appends the transaction to the
ledger and executes the transaction.

If a node of an involved cluster waits for a message from another cluster, e.g., a node in D3 does not receive a
commit message from the coordinator cluster C3, it sends a query message to all nodes of that cluster resulting
in either receiving the message or replacing the faulty primary. The routine is explained in Section 4.3.4.

4.3.2 Cross-Shard Intra-Enterprise Consensus

The cross-shard intra-enterprise consensus protocol, inspired by permissioned blockchains AHL [36] and
Saguaro [11], processes transactions on diﬀerent shards of a local data collection within an enterprise, e.g., a
transaction on data shards d2
A of data collection dA processed by clusters A2 and A3 (Figure 5(b)).

A and d3

14

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

After receiving transaction m, the primary of the coordinator cluster Pc assigns an ID to the transaction
and establishes internal consensus on the order of m in the coordinator cluster. Pc then multicasts a signed
prepare message hPREPARE, IDc, d, miσPc
to the nodes of all involved clusters. Upon receiving a valid prepare
message, the primary of each involved cluster Pi assigns IDi to m and establishes consensus on request m
in cluster Pi. IDc and IDi represent the order of transaction m in clusters Pc and Pi respectively. Note that
the primary of the coordinator and all involved clusters processes the request only if there is no concurrent
(uncommitted) request m0 where m and m0 intersect in at least 2 shards. This is needed to ensure that
requests are committed in diﬀerent shards in the same order (global consistency).
The primary of each involved cluster Pi then multicasts a signed prepared message hPREPARED, IDc, IDi, diσPi
to all nodes of the coordinator cluster Pc. Otherwise (if the consensus is not achieved), it multicasts a
signed abort message. As before, each message is signed by a local-majority of a cluster. When the primary
node of the coordinator cluster receives valid prepared messages from every involved cluster, e.g., Pi, Pj,
..., it initiates internal consensus within the coordinator cluster and multicasts a signed commit message
hCOMMIT, IDc, IDi, IDj, ..., diσPc
Otherwise (if some cluster has aborted the transaction), the primary multicasts a signed abort message.
The ID of the commit messages is a concatenation of the received IDs from all involved clusters. Since
the transaction takes place within an enterprise, in contrast to the previous protocol, the primary of the
coordinator cluster does not need to include the received prepared messages in its commit message. Upon
receiving a valid commit message, each node appends the transaction to the ledger and executes it.

to every node of all involved clusters.

4.3.3 Cross-shard Cross-Enterprise Consensus

BC of shared data collection dBC where d1

The cross-shard cross-enterprise consensus protocol is needed when a transaction is executed on diﬀerent
shards of a non-local data collection shared between diﬀerent enterprises, e.g., a transaction that is executed
on data shards d1
BC and d2
is maintained by B2, and C2 (Figure 5(c)).
In this case, diﬀerent clusters of each enterprise maintain diﬀerent data shards, e.g., B1 and B2 maintain d1
and d2
BC, however, each data shard is replicated on only one cluster of each involved enterprise, e.g., d1
BC
is replicated on B1 and C1. In Qanaat, in order to improve performance, only the clusters of the initiator
enterprise, e.g., B1 and B2, establish consensus on the transaction order and clusters of other enterprises,
e.g., C1 and C2, validate the order.

BC is maintained by B1 and C1 and d2

BC

BC

Upon receiving a valid transaction m, the primary node of the coordinator cluster Pc establishes internal
consensus on the order of m in the coordinator cluster and then multicasts a signed prepare message
hPREPARE, IDc, d, miσPc
to the nodes of all involved clusters. When the primary node of an involved cluster Pi
of the initiator enterprise receives a valid prepare message, it assigns IDi to m and initiates internal consensus
on request m in cluster Pi. Same as cross-shard intra-enterprise consensus, the primary of Pc and every Pi
processes the request only if there is no concurrent request m0 with shared data shards. Once consensus is
achieved, the primary of the involved cluster Pi multicasts a signed prepared message hPREPARED, IDc, IDi, diσPi
to every node of Pc and also any other clusters that maintain the same data shard as cluster Pi. Otherwise,
it multicasts an abort message to those clusters.

A node in a cluster of a non-initiator enterprise either (1) maintains the same shard as the coordinator cluster
and receives a valid prepare message (with IDc) from primary of coordinator cluster Pc , or (2) maintains
some other shard and receives a valid prepared message (with IDi) from primary of cluster Pi. In either case,
the node sends a signed prepared message including IDc (and IDi) and message digest to the primary of the
coordinator cluster.

The primary node of the coordinator cluster waits for (1) signed valid prepared messages from the primary
node of all involved clusters of its enterprise, and (2) signed valid prepared messages from a local-majority
of every involved cluster of other enterprises. The primary then initiates internal consensus within the
coordinator cluster and multicasts a signed commit (or abort) message to every node of all involved clusters.

4.3.4 Primary Failure Handling

We use the retransmission routine presented in [103] to handle unreliable communication between ordering
nodes and execution nodes. In this section, we focus on the failure of the primary ordering node. If the
timer of node r expires before it receives a commit certiﬁcate, it suspects that the primary might be faulty.

15

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

The timeout mechanism prevents the protocol from blocking and waiting forever by ensuring that eventually
(after GST), communication between non-faulty ordering nodes is timely and reliable. Ordering nodes
use diﬀerent timers for intra-cluster and cross-cluster transactions because processing transactions across
clusters usually takes more time. The timeout value mainly depends on the network latency (within or
across clusters).
It must be long enough to allow nodes to communicate with each other and establish
consensus, i.e., at least 3 times the WAN round-trip for cross-cluster transactions to allow a view-change
routine to complete without replacing the primary node again. The timeouts are adjusted to ensure that
this period increases exponentially until some transaction is committed, e.g., in case of consecutive primary
failure, the timeout is doubled as in PBFT [28].

When the primary node of a cluster fails, the primary failure handling routine of the internal consensus
protocol, e.g., view change in PBFT, is triggered by timeouts to elect a new primary. For transactions
across clusters, if a node of an involved cluster does not receive a commit message from the primary of the
coordinator cluster for a prepared request and its timer expires, the node multicasts a signed commit-query
message to all nodes of the coordinator cluster including the request ID and its digest. As a result, if the
(malicious) primary of the coordinator cluster maliciously has not sent commit messages to other clusters
(while the transaction is committed in the coordinator cluster), other nodes of the coordinator cluster will
be informed (to prevent any inconsistency between clusters). Similarly, if a node in the coordinator cluster
does not receive prepared messages from an involved cluster (either from the primary node when it requires
to run consensus or from a local-majority of nodes), it multicasts a prepared-query to all nodes of the involved
cluster. It might also happen between two involved clusters in cross-shard cross-enterprise transactions where
a cluster is waiting for consensus results, e.g., C2 and B2 in Figure 5(c).

In all such cases, if the message has already been processed, the nodes simply re-send the corresponding
response. Nodes also log the query messages to detect denial-of-service attacks initiated by malicious nodes.
If the query message is received from a local-majority of a cluster, the primary will be suspected to be faulty,
triggering the execution of the failure handling routine. Moreover, since all messages from a primary of a
cluster, e.g., prepare, prepared, or commit, are multicasts to every node of the receiver cluster, if the primary
of the receiver cluster does not initiate consensus on the message among the nodes of its cluster, it will
eventually be suspected to be faulty by the nodes. Finally, if a client does not receive a reply soon enough,
it multicasts the request to all nodes of the cluster that it has already sent its request. If the request has
already been processed, the nodes simply send the execution result back to the client. Otherwise, if the node
is not the primary, it relays the request to the primary. If the nodes do not receive prepare messages, the
primary will be suspected to be faulty.

4.3.5 Correctness Argument

We brieﬂy analyze the safety (agreement, validity and consistency) and liveness properties of the coordinator-
based protocols.

Proposition 4.1 (Agreement) If node r commits request m with local ID α in cluster P , no other non-faulty
node commits request m0 (m 6= m0) with the same local ID α in P .

l, ID0

k, ID0

Proof: We assume that the internal consensus protocols, e.g., Paxos and PBFT, are correct and ensure
safety. Let m and m0 (m 6= m0) be two committed requests where ID(m) = [IDi, IDj, IDk, ...] and ID(m0) =
[ID0
m, ..] respectively. Note that in intra-shard transactions, the ID includes a single part. Given
an involved cluster Pk in the intersection of m and m0 where IDk(m) = hα, γi and IDk(m0) = hα0, γ0i. To
commit m and m0, a local-majority Q agreed with m and local-majority Q0 agreed with m0 (If nodes of
Pk are crash only, local-majority is f + 1 out of 2f + 1 and if nodes might be Byzantine, local-majority is
2f + 1 out of 3f + 1). Since any two local-majority Q and Q0 intersect on at least one non-faulty node and
non-faulty nodes do not behave maliciously, if m 6= m0 then α 6= α0, hence agreement is guaranteed.

Proposition 4.2 (Validity) If a non-faulty node r commits m, then m must have been proposed by some
node π.

Proof: For crash-only clusters validity is guaranteed as crash-only nodes do not send ﬁctitious messages.
Since the adversary cannot subvert standard cryptographic assumptions (as explained in Section 3), validity
is guaranteed for Byzantine clusters as well. All messages are signed and include the request or its digest
(to prevent changes and alterations to any part of the message). As a result, if request m is committed by
non-faulty node r, the same request must have been proposed earlier by some node π.

16

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Figure 6: (a) intra-shard cross-enterprise, (b) cross-shard intra-enterprise, and (c) cross-shard cross-enterprise
ﬂattened consensus protocols

Proposition 4.3 (Consistency) Let P(m) denote the set of involved clusters in a request m. For any two
committed requests m and m0 and any two nodes r1 and r2 such that r1 ∈ Pi, r2 ∈ Pj, and {Pi, Pj} ∈
P(m) ∩ P(m0), if m is committed after m0 on r1, then m is committed after m0 on r2.

Proof: For intra-shard cross-enterprise transactions, consistency is guaranteed as all clusters in P(m) main-
tain the same data shard. For cross-shard transactions, as discussed in Sections 4.3.2 and 4.3.3, the primary
of the coordinator as well as all involved clusters processes the request only if there is no concurrent (uncom-
mitted) request m0 where m and m0 intersect in at least 2 shards. Committing request m requires suﬃcient
prepared messages from every involved cluster, hence, m cannot be committed in any cluster until m0 is
committed.

Proposition 4.4 (Liveness) A request m issued by a correct client eventually completes.

Proof: Qanaat guarantees liveness only during periods of synchrony (FLP result [43]). If the primary of
the coordinator cluster is faulty, e.g., does not multicast valid prepare or commit messages, or the primary
of an involved cluster that needs to run consensus is faulty and does not multicast valid prepared messages,
as explained in Section 4.3.4, its failure will be detected using timeouts. As explained, nodes use diﬀerent
timers for intra-cluster and cross-cluster transactions where the timer for cross-cluster transactions is long
enough to allow nodes to communicate with each other and establish consensus, i.e. at least 3 times the
WAN round-trip for cross-cluster transactions to allow a view-change routine to complete without replacing
the primary node again. If a newly elected leader is not able to change the view, the timeout is doubled (as
in PBFT [28]). This adjustment is needed to ensure that some transactions will be committed, i.e., nodes
are synchronized. Qanaat uses the primary failure handling routine of the internal consensus protocol, e.g.,
view-change in PBFT, to elect a new primary.

Two coordinator clusters might initiate concurrent transactions on the same data shard of the same data
collection with inconsistent IDs, e.g., both have the same local ID, and the involved clusters receive these
transactions in a diﬀerent order. In this case, neither of the concurrent transactions receive prepared messages
from all involved clusters resulting in a deadlock situation. In such a situation, once the timer of a coordinator
cluster for its transaction expires, the coordinator cluster sends a new prepare message to the involved
clusters to resolve the deadlock. Qanaat assigns diﬀerent timers to diﬀerent clusters to prevent parallel
and consecutive deadlock situations. To prevent deadlock situations, Qanaat can consider a designated
coordinator cluster for each (shard of a) data collection. An enterprise, however, might not want to consume
its resources to coordinate transactions that are initiated by other enterprises.

4.4 Flattened Consensus

The ﬂattened consensus protocols of Qanaat, in contrast to the ﬂattened cross-shard consensus protocols, e.g.,
SharPer [9], which are designed for single-enterprise environments, support multiple multi-shard collaborative
enterprises.

Propose: Upon receiving a transaction, the primary of the coordinator cluster sends a signed propose
message to the nodes of all involved clusters.

17

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Accept: This phase is handled diﬀerently in diﬀerent protocols.
Intra-shard cross-enterprise transactions: All involved clusters, e.g., C3 and D3 in Figure 6(a), maintain
the same data shard. Upon receiving a propose message from the primary of the coordinator cluster, each
node (in the coordinator, e.g., C3 or another involved cluster, e.g., D3) validates the message and its ID
and multicasts an accept message to all nodes of every involved cluster.
Cross-shard intra-enterprise transactions: The involved clusters, e.g., A2 and A3, maintain diﬀerent shards
of a local data collection, e.g., dA. Upon receiving a propose message including IDi from the coordinator
cluster, e.g., A2, the primary of each non-coordinator cluster, e.g., A3, assigns an IDj and sends an accept
message (including both IDs) to the nodes of its clusters. Each node then multicasts an accept message
to all nodes of every involved cluster.
Cross-shard cross-enterprise transactions: While diﬀerent involved clusters of each enterprise, e.g., B1
and B2 of enterprise B, maintain diﬀerent shards, e.g., d1
BC, of a shared data collection, e.g.,
dBC, each data shard is replicated on one cluster of each involved enterprise, e.g., d2
BC is maintained by
B2, and C2. Upon receiving a valid propose message from the coordinator cluster, e.g., B1, if the cluster
belongs to the initiator enterprise, e.g., B2, its primary sends an accept message to the nodes of clusters
that maintain the same shard, e.g., B2 and C2. Each node then sends an accept message to all nodes of
every involved cluster.

BC and d2

Commit: Upon receiving valid matching accept messages from a local-majority of every involved cluster,
each node multicasts a commit message to every node of all involved clusters. Once a node receives
valid matching commit messages from the local-majority of every involved clusters, the node commits the
transaction.

The ﬂattened protocols of Qanaat enable clusters to commit a transaction in a smaller number of communic-
ation phases. For instance, a cross-shard intra-enterprise transaction can be committed using the ﬂattened
protocol in 3 cross-cluster communication steps. In contrast, using the coordinator-based protocol, the same
transaction requires 3 cross-cluster and 9 intra-cluster communication steps (3 runs of PBFT). It also reduces
the load on the coordinator cluster. We now discuss ﬂattened consensus protocols in detail.

4.4.1

Intra-Shard Cross-Enterprise Consensus

In the intra-shard cross-enterprise protocol, multiple clusters from diﬀerent enterprises process a transaction
on the same data shard of a shared data collection, e.g., clusters C3 and D3 on shard d3
CD of shared data
collection dCD (Figure 6(a)). Upon receiving a valid request message m, the primary ordering node π(Pi) of
the initiator cluster Pi assigns transaction ID (as discussed in Section 3.3) and multicasts a signed propose
message hPROPOSE, ID, d, miσπ(Pi) to the nodes of all involved clusters where d = D(m) is the digest of request
m. Upon receiving a propose message, each node r validates the digest, signature and ID, and multicasts
a signed accept message hACCEPT, ID, diσr to all nodes of every involved cluster. Each node r waits for valid
matching accept messages from a local-majority of every involved cluster and then multicasts a commit message
including the transaction ID and its digest to every node of all involved clusters. The propose and accept
phases of the protocol, similar to pre-prepare and prepare phases of PBFT, guarantee that non-faulty nodes
agree on an order for the transactions. Finally, once an ordering node receives valid commit messages from
the local-majority of all involved clusters that match its commit message, it generates a commit certiﬁcate
by merging messages from all involved clusters. Each cluster then uses the transaction execution routine to
execute the transaction and send reply back to the client.

4.4.2 Cross-shard Intra-Enterprise consensus

The cross-shard intra-enterprise consensus protocol of Qanaat is inspired by the ﬂattened cross-shard con-
sensus protocols of SharPer [9].
In the cross-shard intra-enterprise consensus protocol, as shown in Fig-
ure 6(b), the involved clusters, e.g., A2 and A3, maintain diﬀerent shards of a local data collection, e.g.,
dA. Upon receiving a valid request message m, the primary π(Pi) of the initiator cluster Pi assigns IDi to
the transaction and multicasts a propose message µ = hPROPOSE, IDi, d, miσπ(Pi) to all nodes of every involved
cluster. Upon receiving a valid propose message, each primary node π(Pj) of an involved cluster Pj multicasts
hACCEPT, IDi, IDj, d, dµiσπ(Pj ) to the nodes of its cluster where IDj represents the order of m on data shard of
cluster Pj. The digest of the propose message, dµ = D(µ), is added to link the accept message with the
corresponding propose message enabling nodes to detect changes and alterations to any part of the message.
The primary of the initiator and all involved clusters process the request only if there is no (concurrent)

18

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

uncommitted request m0 where m and m0 intersect in at least 2 shards. This is needed to ensure that requests
are committed in diﬀerent shards in the same order (global consistency) [9].

Each node r (including the primary) of an involved cluster Pj then multicasts a signed accept mes-
sage hACCEPT, IDi, IDj, d, riσr to all nodes of every involved cluster. Upon receiving valid matching ac-
cept messages from the local-majority of all involved clusters Pi, Pj, ..., Pk, each node r multicasts a
hCOMMIT, IDi, IDj, ..., IDk, d, riσr message to every node of all involved clusters.
Upon receiving valid matching commit messages from the local-majority of every involved cluster, the node
generates a commit certiﬁcate. The transaction execution routine then executes the transaction and appends
the transaction to the ledger.

In this protocol, since all clusters belong to the same enterprise, if nodes of all involved clusters are crash-only,
Qanaat processes transactions more eﬃciently. In that case, and upon receiving hPROPOSE, IDi, d, miσπ(Pi) from
the initiator primary π(Pi), the primary node of each involved cluster Pj assigns IDj to the transaction and
multicasts an accept message to the ordering nodes of its cluster. Nodes of each involved cluster then send
an accept message to only the initiator primary (in contrast to the previous protocol where nodes multicast
accept messages to all nodes of every involved cluster). The initiator primary waits for f + 1 valid matching
accept messages from diﬀerent nodes of every involved cluster and multicasts a commit message to every node
of all involved clusters. Once an ordering node receives a valid commit message from the initiator primary,
considers the transaction as committed.

4.4.3 Cross-Shard Cross-Enterprise consensus

BC of shared data collection dBC where d1

The cross-shard cross-enterprise consensus protocol is needed when a transaction is executed on diﬀerent
shards of a non-local data collection shared between diﬀerent enterprises, e.g., a transaction that is executed
BC and d2
on data shards d1
is maintained by B2, and C2 (Figure 6(c)).
In this case, diﬀerent clusters of each enterprise maintain diﬀerent data shards, e.g., B1 and B2 maintain d1
BC, however, each data shard is replicated on only one cluster of each involved enterprise, e.g., d1
and d2
BC
is replicated on B1 and C1. In Qanaat, in order to improve performance, only the clusters of the initiator
enterprise, e.g., B1 and B2, establish consensus on the transaction order and clusters of other enterprises,
e.g., C1 and C2, validate the order.

BC is maintained by B1 and C1 and d2

BC

BC

The primary node π(Pi) of
the initiator cluster Pi multicasts a signed propose message µ =
hPROPOSE, IDi, d, miσπ(Pi) to the nodes of all involved clusters. Upon receiving a valid propose message, each
primary node π(Pj) of an involved cluster Pj belonging to the initiator enterprise assigns an IDj to the
transaction and multicasts a signed hACCEPT, IDi, IDj, d, dµiσπ(Pj ) message to all the nodes of its cluster and
all other clusters that maintain the same data shard. As before, primary nodes ensure consistency (i.e., no
concurrent request with more than one shared shard) before processing a request. The accept and commit
phases process the transaction in the same way as cross-shard intra-enterprise protocol.

4.4.4 Primary Failure Handling

We use the retransmission routine presented in [103] to handle unreliable communication between ordering
nodes and execution nodes. In this section, we focus on the failure of the primary ordering node. If the timer
of node r expires before it receives a commit certiﬁcate, it suspects that the primary might be faulty. When
the primary node of a cluster fails, the primary failure handling routine of the internal consensus protocol,
e.g., view-change in PBFT, is triggered by timeouts to elect a new primary.

While in the intra-shard cross-enterprise consensus, only the failure of the initiator primary node needs to
be handled, in the second and third protocols, the primary node of any of the involved clusters that assign
an ID, i.e., belongs to the initiator enterprise, might fail. In the intra-shard cross-enterprise consensus, if
the timer of node r expires and r belongs to the initiator cluster, it initiates the failure handling routine of
the internal consensus protocol, e.g., view-change in PBFT. Otherwise, if the node is in some other involved
cluster, it multicasts a commit-query message including the request ID and its digest to the nodes of the
initiator cluster. If nodes of the initiator cluster receive commit-query from the local-majority of an involved
cluster for a request, they suspect their primary is faulty and did not send consistent propose messages. Nodes
also log the query messages to detect denial-of-service attacks initiated by malicious nodes.

19

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Crd-B Crd-B(PF)

Flt-B Flt-B(PF) Crd-C Flt-C Fabric

Fabric++ FastFabric

]
s

m

[

y
c
n
e
t
a
L

75

50

25

0

40

20

60
80 100
Throughput [ktrans/sec]

300

200

100

0

20

10

30
Throughput [Ktrans/sec]

40

50

600

400

200

5

10

15
Throughput [Ktrans/sec]

25

20

Figure 7: Workloads with (a) 10%, (b) 50%, (c) 90% intra-shard cross-enterprise transactions

In the second and third protocols, each transaction needs to be ordered by the initiator primary as well as
the primary nodes of all clusters belonging to the initiator enterprise (i.e., which maintain diﬀerent data
shards). Suppose a malicious primary node (from the initiator or an involved cluster) sends a request with
inconsistent ID to diﬀerent nodes. In that case, nodes detect the failure in all to all communication phases
of the protocol triggering the primary failure handling routine within the faulty primary cluster. Note that
since propose and accept messages need to be multicast to all nodes, and all nodes of all involved clusters
multicast accept and commit messages to each other, even if a malicious primary decides not to multicast a
request to a group of nodes, it will be easily detected.

Finally, if a client does not receive a reply soon enough, it multicasts its request message to all ordering nodes
of the cluster that it has already sent its request. If an ordering node has both the commit certiﬁcate (i.e.,
the request has already been ordered) and the reply certiﬁcate (i.e., the request has already been executed),
the node simply sends the reply certiﬁcate to the client. If the node has not received the reply certiﬁcate, it
re-sends the commit certiﬁcate to the ﬁlter nodes. Otherwise, if the node is not the primary, it relays the
request to the primary. If the nodes do not receive propose messages, the primary will be suspected to be
faulty.

4.4.5 Correctness Argument

We brieﬂy analyze the agreement and liveness properties of the ﬂattened protocols. Validity, and consistency
are proven in the same way as coordinator-based protocols.

Lemma 4.5 (Agreement) If node r commits request m with local ID α in cluster P , no other non-faulty
node commits request m0 (m 6= m0) with the same local ID α in P .

Proof: The propose and accept phases of each presented consensus protocol guarantee that non-faulty nodes
agree on the order of a transaction. To commit a request, matching messages from the local-majority of
every involved cluster are needed. Given two requests m with local ID α and m0 with local ID α0 where
local-majority Q agreed with m and local-majority Q0 agreed with m0. Let m and m0 (m 6= m0) be two
committed requests where ID(m) = [IDi, IDj, IDk, ...] and ID(m0) = [ID0
m, ..] respectively. Note that
in intra-shard transactions, the ID includes a single part. Given an involved cluster Pk in the intersection of
m and m0 where IDk(m) = hα, γi and IDk(m0) = hα0, γ0i. To commit m and m0, a local-majority Q agreed
with m and local-majority Q0 agreed with m0. Since Q and Q0 intersect on at least one non-faulty node and
non-faulty nodes do not behave maliciously, if m 6= m0 then α 6= α0, hence agreement is guaranteed.

k, ID0

l, ID0

Proposition 4.6 (Liveness) A request m issued by a correct client eventually completes.

Proof: Qanaat guarantees liveness only during periods of synchrony. If the primary of a cluster that needs
to order the request and assign ID is faulty, as explained in Section 4.4.4, its failure will be detected using
timers and Qanaat uses the primary failure handling routine of the internal consensus protocol to elect a
new primary. As explained for the coordinator-based protocols, nodes use diﬀerent timers for intra-cluster
and cross-cluster transactions where the timer for cross-cluster transactions is long enough to allow nodes to
communicate with each other and establish consensus, i.e. at least 3 times the WAN round-trip for cross-
cluster transactions to allow a view-change routine to complete without replacing the primary node again.
If a newly elected leader is not able to change the view, the timeout is doubled (as in PBFT [28]). This

20

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Crd-B Crd-B(PF)

Flt-B Flt-B(PF) Crd-C Flt-C Fabric

Fabric++ FastFabric

]
s

m

[

y
c
n
e
t
a
L

75

50

25

0

20

40

60
80 100
Throughput [ktrans/sec]

300

200

100

600

400

200

0

20

40

60

0

10

20

30

Throughput [Ktrans/sec]

Throughput [Ktrans/sec]

Figure 8: Workloads with (a) 10%, (b) 50%, (c) 90% cross-shard intra-enterprise transactions

adjustment is needed to ensure that some transactions will be committed, i.e., nodes are synchronized. The
deadlock situation is addressed in the same way as the coordinator-based protocols.

5 Experimental Evaluation

Our evaluation has three goals: (1) comparing the performance of the coordinator-based and ﬂattened con-
sensus protocols in diﬀerent workloads with diﬀerent types of transactions; (2) demonstrating the overhead
of using the privacy ﬁrewall mechanism [103] to provide conﬁdentiality despite Byzantine faults; (3) com-
paring the performance of Qanaat with Hyperledger Fabric [12] and two of its variants, FastFabric [48] and
Fabric++ [91], to analyze the impact of sharding and the performance trade-oﬀ between a higher number
of shards versus a higher percentage of cross-shard transactions.

We analyze the impact of (1) the failure model of nodes, (2) the percentage of cross-enterprise transactions,
(3) the percentage of cross-shard transactions, (4) the geo-distribution of clusters, (5) the number of involved
enterprises, (6) the failure of nodes, and (7) the degrees of contention on the performance of these protocols
and systems.

We have implemented a prototype of Qanaat using Java and deployed a simple asset management collab-
oration workﬂow. We use SmallBank benchmarks (modiﬁed it to be able to control cross-shard and cross-
enterprise transactions). The workloads are write-heavy (sendPayment transactions are used to perform read
and write in a single or multiple shards of data collection). The key selection distribution in each data
collection is uniform with s-value = 0.

Other than Section 5.5 where we vary the number of enterprises, we consider an infrastructure with 4
enterprises. Each enterprise partitions its data into 4 shards. Each crash-only clusters includes 2f + 1 nodes
that order and execute transactions while each Byzantine cluster includes 3f + 1 ordering nodes, 2g + 1
execution nodes and h + 1 rows of h + 1 ﬁlter nodes. To demonstrate the overhead of the privacy ﬁrewall
mechanism, we also measure the performance of Byzantine clusters with only 3f + 1 nodes that order and
execute transactions. In all experiments f = g = h = 1. We use Paxos and PBFT as the internal consensus
protocols.

We consider a single-channel Fabric deployment (v2.2) where Raft [74] is its consensus protocol. We deploy
four enterprises on the channel where enterprises can form public or private collaboration. Sharding was not
possible in a single-channel Fabric deployment, however, we deﬁned four endorsers to execute transactions of
enterprises in parallel. We use a similar setting for FastFabric and Fabric++. FastFabric re-architects Fabric
and provides eﬃcient optimizations such as separating endorsers from storage nodes and sending transaction
hashes to orderers. Fabric++, on the other hand, presents transaction reordering and early abort mechanisms
to improve the performance of Fabric, especially in contentious workloads. We do not compare Qanaat with
Caper [6] because Caper supports neither cross-enterprise transactions among a subset of enterprises nor
sharding. Similarly, sharded permissioned blockchains like AHL [36] and SharPer [9] can only be compared
to cross-shard intra-enterprise transactions as they do not support multi-enterprise environments.

The experiments were conducted on the Amazon EC2 platform on multiple VM instances. Other than the
fourth set of experiments where clusters are distributed over 4 diﬀerent AWS regions, in all experiments,
clusters are placed in the same data center (California) with < 1 ms ping latency. Each VM is a c4.2xlarge
instance with 8 vCPUs and 15GB RAM, Intel Xeon E5-2666 v3 processor clocked at 3.50 GHz. The results
reﬂect end-to-end measurements from the clients. Each experiment is run for 90 seconds (30s warmup/cool-

21

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Crd-B Crd-B(PF)

Flt-B Flt-B(PF) Crd-C Flt-C Fabric

Fabric++ FastFabric

90

60

30

]
s

m

[

y
c
n
e
t
a
L

0

40

20

60
80 100
Throughput [ktrans/sec]

450

300

150

0

10

20

50
Throughput [Ktrans/sec]

40

30

1,200

800

400

5

10

20
Throughput [Ktrans/sec]

15

Figure 9: Workloads with (a) 10%, (b) 50%, (c) 90% cross-shard cross-enterprise transactions

down). The reported results are the average of ﬁve runs. we use an increasing number of requests, until the
end-to-end throughput is saturated, and state the throughput and latency just below saturation.

5.1

Intra-Shard Cross-Enterprise Transactions

In the ﬁrst set of experiments, we measure the performance of all protocols with diﬀerent percentages, i.e.,
10%, 50%, and 90%, of intra-shard cross-enterprise transactions. Each transaction is randomly initiated on a
single data shard of a data collection shared among multiple enterprises. The number of involved enterprises
depends on the data collection. Figure 7(a) demonstrate the results for the workload with 10% intra-
shard cross-enterprise (and 90% internal) transactions. In this scenario and with crash-only nodes, Qanaat
processes more than 110 ktps with 38 ms latency using the ﬂattened protocol (Flt-C) and 103 ktps with 36
ms latency using the coordinator-based protocol (Crd-C) before the end-to-end throughput is saturated.

Fabric processes only 9.7 ktps (8% of the throughput of Flt-C) with 37 ms latency. While diﬀerent endorsers
of diﬀerent enterprises execute their transactions in parallel, ordering the transactions of all enterprises by a
single set of orderers becomes a bottleneck. This clearly demonstrates the impact of parallel ordering (due to
sharding) in Qanaat, where diﬀerent clusters order and execute their transactions independently. Fabric++
demonstrates only 3% higher throughput compared to Fabric with the same latency as it is able to reorder
and early abort invalidated transactions. FastFabric, however, demonstrates 189% throughput improvement
compared Fabric due to its optimized architecture. However, its throughput is still 26% of the throughput
of Flt-C with the same latency.

With Byzantine nodes, the performance of the ﬂattened protocol (Flt-B) and the coordinator-based protocol
(Crd-B) is reduced, which is expected due to the higher complexity of BFT protocols. Using the privacy
ﬁrewall mechanism results in 8% and 6% lower throughput in the coordinator-based (Crd-B(PF)) and ﬂattened
(Flt-B(PF)) protocols respectively before the end-to-end throughput is saturated. This slight throughput
reduction is the result of two infrastructural changes. On one hand, using the privacy ﬁrewall mechanism,
request and reply messages go through ﬁlters resulting in lower performance. On the other hand, separating
ordering from execution reduces the performance overhead by decreasing the load on ordering nodes. The
privacy ﬁrewall mechanism also increases the latency of transaction processing in diﬀerent workloads by a
constant coeﬃcient. This is expected because the increased latency comes from sending messages through
the ﬁlters, which is separated from the consensus routine; the bottleneck in heavy workloads. While this
latency is considerable in light workloads, it becomes much lower in heavy workloads, e.g., 166% vs. 25%
higher latency in Crd-B(PF) protocol.

Increasing the percentage of cross-enterprise transactions to 50%, as shown in Figure 7(b), reduces the
throughput of all protocols. This is expected because a higher percentage of transactions requires cross-
enterprise consensus. In this experiment, Flt-B processes 52 ktps with 230 ms latency while Crd-B processes
43 ktps (18% lower) with 130 ms latency (44% lower). This shows a trade-oﬀ between the number of
communication phases and the quorum size. While the coordinator-based approach requires more phases of
message passing (resulting in lower throughput), the quorum size of the ﬂattened approach is larger, i.e., all
nodes communicate with each other (resulting in higher latency). Using the privacy ﬁrewall mechanism, the
throughput of Crd-B(PF) and Flt-B(PF) is increased by 5% and 7% respectively and their latency is increased
by only 9% and 6% (compared to Crd-B and Flt-B) before the end-to-end throughput is saturated. These
results demonstrate that as the ordering routine becomes heavily loaded, the overhead of using the privacy
ﬁrewall mechanism is alleviated.

22

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Crd-B Crd-B(PF)

Flt-B Flt-B(PF) Crd-C Flt-C

90

60

30

150

100

50

20

40
Throughput [ktrans/sec]

60

0

20

40

60

80

0

20

40

60

Throughput [ktrans/sec]

Throughput [ktrans/sec]

]
s

m

[

y
c
n
e
t
a
L

90

60

30

0

Figure 10: Scalability over spatial domains with (a) 10% intra-shard cross-enterprise, (b) 10% cross-shard
intra-enterprise, (c) 10% cross-shard cross-enterprise transactions

The performance of Crd-C is signiﬁcantly better than Crd-B (i.e., 23% higher throughput, 39% lower latency)
in this scenario. This diﬀerence, however, is not remarkable in the ﬂattened protocols. The reason is that
in the coordinator-based protocol, consensus takes place within each cluster using the internal consensus
protocols (Paxos in this case). However, in the ﬂattened protocol, as shown in Figure 6(a), there is no
internal consensus within clusters and a BFT protocol across enterprises establishes agreement.

The performance of Fabric is also aﬀected by increasing the percentage of cross-enterprise transactions due
to (1) the overhead of hashing techniques and (2) conﬂicting transactions [7, 47, 46, 91]. Interestingly, the
throughput gap between Fabric and Fabric++ is increased to 18% (from 3%) in this scenario due to early
abort and reordering mechanisms presented in Fabric++.

With 90% cross-enterprise transactions, as shown in Figure 7(c), the latency of Flt-B becomes very high
(680 ms) due to its O(n2) message communication. The performance of Flt-C and Flt-B becomes close to
each other since in both cases, a BFT protocol is used for cross-enterprise consensus. In this experiment,
FastFabric demonstrates the lowest latency (35% lower than Crd-C with 18 ktps) as it does not suﬀer from
the overhead of message communication across clusters.

5.2 Cross-Shard Intra-Enterprise Transactions

In the second set of experiments, we measure the performance of coordinator-based and ﬂattened cross-
shard intra-enterprise protocols. Compared to the ﬁrst set, these two protocols, in general, demonstrate
lower throughput and higher latency due to the need for establishing consensus in multiple data shards.
With Byzantine nodes and with 10% cross-shard transactions, as shown in Figure 8(a), the performance of
Crd-B is still close to Flt-B. However, by increasing the percentage of cross-shard transactions to 50%, Flt-B
shows 20% higher throughput.
In this set of experiments, since all shards belong to a single enterprise,
as explained in Section 4.4.2, Flt-C is implemented as a CFT protocol, and, as shown in Figure 8(a)-(c),
has the best performance in all three workloads. Similar to the previous section, the overhead of using
the privacy ﬁrewall mechanism is alleviated when the ordering nodes become highly loaded, e.g., the gap
between the latency of Flt-B(PF) and Flt-B is reduced from 25% to 4% by increasing the percentage of
cross-shard transactions from 10% to 90%. Since enterprises maintain their data on a single data shard,
Fabric demonstrates the same performance in all three workloads, which is signiﬁcantly worse than Qanaat.
However, with 90% cross-shard transactions, similar to cross-enterprise transactions, FastFabric shows the
lowest latency because only one cluster orders all transactions.

5.3 Cross-Shard Cross-Enterprise Transactions

In workloads consisting of cross-shard cross-enterprise transactions, as shown in Figure 9(a)-(c), the
coordinator-based protocol shows better performance, especially with a higher percentage of cross-cluster
transactions. In particular, with 90% cross-cluster transactions, Flt-B demonstrates 24% lower throughput
and 93% higher latency compared to Crd-B. This is expected because cross-shard cross-enterprise transac-
tions, as shown in Figures 5(c) and 6(c), require consensus among multiple clusters of multiple enterprises.
As a result, the all to all communication phases of the ﬂattened protocol result in high latency. Note that
since clusters belong to diﬀerent enterprises, even if nodes follow the crash failure model, a BFT across enter-
prises needs to establish agreement, hence, the performance is not signiﬁcantly improved, i.e., Flt-C processes

23

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Table 2: Performance with diﬀerent number of Enterprises

4 Enterprises

6 Enterprises

Protocols
Crd-B
Crd-B(PF)
Flt-B
Flt-B(PF)
Crd-C
Flt-C

2 Enterprises
T[tps] L[ms] T[tps]
79213
45
42023
72629
65
34011
81111
58
44209
73530
70
36091
104511
33
53581
108192
31
59302

L[ms] T[tps]
108672
99810
116559
105023
144311
161864

52
68
60
74
39
36

L[ms] T[tps]
141502
129402
154951
143509
189311
214729

8 Enterprises
L[ms]
56
75
64
79
41
36

53
70
61
77
41
37

Table 3: Performance with faulty nodes

Crd-B

79213
69541
52
56

Crd-B
(PF)
72629
64032
68
74

Flt-B

81111
67023
60
68

Flt-B
(PF)
73530
61672
74
79

Crd-C

Flt-C

Fabric

104511
98032
39
40

108192
97034
36
38

9743
9211
37
38

Fabric
++
10212
9661
40
42

Fast
Fabric
28142
26392
35
38

.
u
o
r
h
T

.
n
e
t
a
L

) no fail
s
p
1 fail
t
(
) no fail
1 fail
m

s

(

only 13.4 k transaction with 920 ms latency. Since cross-shard transactions do not aﬀect the performance of
Fabric, it demonstrates similar performance as intra-shard cross-enterprise transactions.

5.4 Scalability Across Spatial Domains

In the next set of experiments, we measure the impact of network distance on the performance of the
protocols. Clusters of diﬀerent enterprises are distributed over four diﬀerent AWS regions, i.e., Tokyo (TY),
Seoul (SU), Virginia (VA), and California (CA) with the average Round-Trip Time (RTT): TY (cid:10) SU: 33
ms, TY (cid:10) VA: 148 ms, TY (cid:10) CA: 107 ms, SU (cid:10) VA: 175 ms, SU (cid:10) CA: 135 ms, and VA (cid:10) CA: 62
ms. We consider workloads with 90% internal transactions (the typical setting in partitioned databases
[95]) and repeat the previous experiments, i.e., intra-shard cross-enterprise transactions (Figure 7(a)), cross-
shard intra-enterprise transactions (Figure 8(a)), and cross-shard cross-enterprise transactions (Figure 9(a)).
Since in Fabric, Fabric++, and FastFabric, all transactions are ordered by the same set of orderers, placing
endorser nodes that execute transactions of diﬀerent enterprises and orderer nodes in diﬀerent locations far
from each other is not plausible. Hence, we do not perform this set of experiments for Fabric and its variants.

With 10% intra-shard cross-enterprise transactions (Figure 10(a)), Flt-B demonstrates higher latency due to
the message complexity of the protocol that requires all nodes to multicast messages to each other over a
wide area network. With 10% cross-shard intra-enterprise transactions (Figure 10(b)), Flt-C demonstrates
the best performance because clusters belong to the same enterprise and Qanaat processes transactions using
a CFT protocol. Finally, With 10% cross-shard cross-enterprise transactions (Figure 10(c)), the coordinator-
based protocols show better performance because of the two all to all communication phases of the ﬂattened
protocols that take place among distant clusters. Compared to the single datacenter setting, the overhead
of using the privacy ﬁrewall mechanism is also reduced. With 10% intra-shard cross-enterprise transactions,
Crd-B(PF) shows 3% lower throughput and 10% higher latency Compared to Crd-B while with the same
workload and in the single datacenter setting, Crd-B(PF) demonstrates 6% lower throughput and 20% higher
latency.

5.5 Varying the number of Enterprises

We next measure the performance of diﬀerent protocols by varying the number of enterprises from 2 to 8.
The workloads include 90% internal and 10% cross-cluster transactions (typical workload). We do not report
the results for Fabric and its variants because the ordering routine in those systems is the bottleneck, and
increasing the number of enterprises does not signiﬁcantly aﬀect their performance. As shown in Table 2, by
increasing the number of enterprises (clusters) the throughput of all protocols is increased almost linearly.
This is expected because 90% of transactions are internal where for such transactions, the throughput of the
entire system will increase linearly with the increasing number of clusters. In addition, since cross-cluster
transactions access two clusters, by increasing the number of clusters, the chance of parallel processing of
such transactions is increased. With 8 enterprises, Flt-C processes 214 ktps with 36 ms latency.

24

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Crd-B Crd-B(PF) Flt-B Flt-B(PF) Crd-C Flt-C Fabric Fabric++ FastFabric

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

100

80

60

40

20

0

0

1
Zipﬁan skewness

2

]
s

m

[

y
c
n
e
t
a
L

100

80

60

40

20

0

0

1
Zipﬁan skewness

2

Figure 11: Performance with diﬀerent Zipﬁan skewness

5.6 Performance with Faulty Nodes

In this set of experiments, we force a non-primary ordering node to fail (f = 1) and repeat the ﬁrst set of
experiments. In case of Crd-B(PF) and Flt-B(PF), one ordering node, one execution node and one ﬁlter are
failed. As can be seen in Table 3, since all protocols are pessimistic and quorums can be constructed even if
f nodes (in our experiments, 1) fail, the failure of a single node does not reduce the performance of diﬀerent
systems signiﬁcantly. For example, Crd-B(PF) incurs 12% throughput reduction and 9% higher latency.

5.7 Varying the Degree of Contention

We next measure the impact of conﬂicting transactions on the performance of diﬀerent systems. We use
three workloads with Zipﬁan skewness equal to 0 (uniform distribution) 1 and 2 (high contention). The
workloads include 90% internal and 10% cross-cluster transactions.
Increasing the degree of contention
slightly aﬀects the performance of diﬀerent protocols in Qanaat. This is because transactions are ﬁrst ordered
and then executed and the execution is performed sequentially. The performance of Fabric, however, reduces
signiﬁcantly (i.e., 91% throughput reduction) by increasing the workload contention. This is because in
Fabric, transactions are simulated on the same state and if two transactions write the same record, one of
them needs to be invalidated during the validation phase. FastFabric incurs a similar throughput reduction
(92%) as it follows the workﬂow of Fabric. Fabric++, however, reorders transactions to resolve w-r conﬂicts
and early aborts transactions with w-w conﬂicts. As a result, Fabric++ incurs 58% throughput reduction
by increasing the skewness from 0 to 2.

6 Related Work

A permissioned blockchain system, e.g., [64, 31, 12, 72, 47, 46, 91, 55, 87, 2, 83] consists of a set of known,
identiﬁed but possibly untrusted participants (permissioned blockchains are analyzed in several surveys and
empirical studies [38, 27, 30, 86, 39, 92, 92, 8]). Several blockchains support conﬁdential transactions in both
the permissioned [29, 71] and permissionless [53, 26] settings. These systems, however, are not scalable and
only support simple, ﬁnancial transfers, not more complex database updates. Hyperledger Fabric [12] stores
conﬁdential data in private data collections [54] replicated on authorized enterprises. The hash of all private
transactions, however, is appended to the single global ledger of every node resulting in low performance.
Several variants of Fabric, e.g., Fast Fabric [47], XOX Fabric [46], Fabric++ [91], and FabricSharp [87], have
been presented to improve its performance (as demonstrated in Section 5). Such systems, however, do not
address the conﬁdential data leakage and consistency challenges of Fabric. Fabric also can be combined
with secure multiparty computation [18] but this does not address performance issues. Caper [6] supports
private internal and public global transactions of collaborative enterprises. Caper, however, does not address
transactions among a subset of enterprises, consistency across collaboration workﬂows, conﬁdential data
leakage prevention, and multi-shard enterprises.

The zero-knowledge proof techniques used in Quorum [31] to support private transactions have also con-
siderable overhead [12] especially in an environment where most transactions might be local. Enterprises
can also maintain their independent blockchains and use cross-chain transactions [52, 104] for conﬁdential

25

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

cross-enterprise collaboration. Such techniques, however, are often costly, complex, and mainly designed for
permissionless blockchains. Hawk [62], Arbitrum [57], Ekiden [33], Zkay [93] and zexe [22] are other block-
chains that support conﬁdential transactions at the application layer. These systems do not address the
consensus and data layers that are the focus of Qanaat. To support conﬁdential transactions, e.g. Hawk [62]
(which requires a semi-trusted manager), Arbitrum [57] (which allows users to delegate managers, who can
execute contracts in an oﬀ-chain Virtual Machine), Ekiden [33] (which relies on trusted hardware) or Zkay
[93] and zexe [22] (which use Zero-Knowledge proofs) have been proposed. These systems focus on the
application layer and do not attempt to address the consensus and data layers that are the focus of Qanaat.
Hence, these systems are complementary to Qanaat, and some of these tools could be adapted to provide
private contracting capabilities on top of the Qanaat ledger. Related to blockchain conﬁdentiality, tracking
handover of calls [68], secure data provenance [85], veriﬁable data sharing [41], tamper evidence storage [100],
global asset management [105], veriﬁable query processing [101, 80, 107], on-chain secret management [59]
and private membership attestation [102] have been studied.

Data sharding techniques are used in distributed databases [56, 96, 35, 45, 16, 37, 24, 95] with crash-only
nodes and in permissioned blockchain systems, e.g., AHL [36], Blockplane [73], chainspace [3], SharPer [9],
and Saguaro [11] with Byzantine nodes.

AHL [36] uses a similar technique as permissionless blockchains Elastico [67], OmniLedger [60], and Rapid-
chain [106], provides probabilistic safety guarantees, and processes cross-shard transactions in a centralized
manner. SharPer [9] in contrast to AHL, guarantees deterministic safety and processes transactions in
a ﬂattened manner. Scalability over spatial domains has also been studied in ResilientDB [49, 50], and
Saguaro [11]. Qanaat, in contrast to all these systems, supports multi-enterprise environments and guaran-
tees conﬁdentiality.

7 Conclusion

In this paper, we proposed Qanaat, a permissioned blockchain system to support the scalability and conﬁd-
entiality requirements of multi-enterprise applications. To guarantee collaboration conﬁdentiality, Qanaat
constructs a hierarchical data model consisting of a set of data collections for each collaboration workﬂow.
Every subset of enterprises is able to form a conﬁdential collaboration private from other enterprises and
execute transactions on a private data collection shared between only the involved enterprises. To prevent
conﬁdential data leakage, Qanaat utilizes the privacy ﬁrewall mechanism [103]. To support scalability, each
enterprise partitions its data into diﬀerent data shards. Qanaat further presents a transaction ordering
scheme that enforces only the necessary and suﬃcient constraints to guarantee data consistency. Finally, a
suite of consensus protocols is presented to process diﬀerent types of intra-shard and cross-shard transac-
tions within and across enterprises. Our experimental results clearly demonstrate the scalability of Qanaat
in comparison to Fabric and its variants. Moreover, while coordinator-based protocols demonstrate better
performance in cross-enterprise transactions, ﬂattened protocols show higher performance in cross-shard
transactions.

Acknowledgement

This work is funded by NSF grants CNS-1703560, CNS-1815733, and CNS-2104882 and by ONR grant
130.1303.4.573135.3000.2000.0292.

References

[1] A. T. Aborode, W. A. Awuah, S. Talukder, A. A. Oyeyemi, E. P. Nansubuga, P. Machai, H. Tillewein,
and C. I. Oko. Fake covid-19 vaccinations in africa. Postgraduate medical journal, 98(1159):317–318,
2022.

[2] R. R. Agarwal, D. Kumar, L. Golab, and S. Keshav. Consentio: Managing consent to data access
using permissioned blockchains. In Int. Conf. on Blockchain and Cryptocurrency (ICBC), pages 1–9.
IEEE, 2020.

[3] M. Al-Bassam, A. Sonnino, S. Bano, D. Hrycyszyn, and G. Danezis. Chainspace: A sharded smart

contracts platform. In Network and Distributed System Security Symposium (NDSS), 2018.

26

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

[4] S. T. Alam, S. Ahmed, S. M. Ali, S. Sarker, G. Kabir, et al. Challenges to covid-19 vaccine supply
chain: Implications for sustainable development goals. International Journal of Production Economics,
239:108193, 2021.

[5] J. Amankwah-Amoah. Covid-19 and counterfeit vaccines: Global implications, new challenges and

opportunities. Health Policy and Technology, page 100630, 2022.

[6] M. J. Amiri, D. Agrawal, and A. El Abbadi. Caper: a cross-application permissioned blockchain. Proc.

of the VLDB Endowment, 12(11):1385–1398, 2019.

[7] M. J. Amiri, D. Agrawal, and A. El Abbadi. Parblockchain: Leveraging transaction parallelism in
permissioned blockchain systems. In Int. Conf. on Distributed Computing Systems (ICDCS), pages
1337–1347. IEEE, 2019.

[8] M. J. Amiri, D. Agrawal, and A. El Abbadi. Permissioned blockchains: Properties, techniques and

applications. In SIGMOD Int. Conf. on Management of Data, pages 2813–2820, 2021.

[9] M. J. Amiri, D. Agrawal, and A. El Abbadi. Sharper: Sharding permissioned blockchains over network

clusters. In SIGMOD Int. Conf. on Management of Data, pages 76–88. ACM, 2021.

[10] M. J. Amiri, J. Duguépéroux, T. Allard, D. Agrawal, and A. El Abbadi. Separ: Separ: Towards
regulating future of work multi-platform crowdworking environments with privacy guarantees.
In
Proceedings of The Web Conf. (WWW), pages 1891–1903, 2021.

[11] M. J. Amiri, Z. Lai, L. Patel, B. Thau Loo, E. Loo, and W. Zhou. Saguaro: Eﬃcient processing
of transactions in wide area networks using a hierarchical permissioned blockchain. arXiv preprint
arXiv:2101.08819, 2021.

[12] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin, et al. Hyperledger fabric: a distributed operating
system for permissioned blockchains. In European Conf. on Computer Systems (EuroSys), page 30.
ACM, 2018.

[13] E. Androulaki, C. Cachin, A. De Caro, and E. Kokoris-Kogias. Channels: Horizontal scaling and
conﬁdentiality on permissioned blockchains. In European Symposium on Research in Computer Security
(ESORICS), pages 111–131. Springer, 2018.

[14] D. W. Archer, D. Bogdanov, Y. Lindell, L. Kamm, K. Nielsen, J. I. Pagter, N. P. Smart, and R. N.
Wright. From keys to databases—real-world applications of secure multi-party computation. The
Computer Journal, 61(12):1749–1771, 2018.

[15] A. Azaria, A. Ekblaw, T. Vieira, and A. Lippman. Medrec: Using blockchain for medical data access
and permission management. In Int. Conf. on Open and Big Data (OBD), pages 25–30. IEEE, 2016.
[16] J. Baker, C. Bond, J. C. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd,
and V. Yushprakh. Megastore: Providing scalable, highly available storage for interactive services. In
Conf. on Innovative Data Systems Research (CIDR), 2011.

[17] BBC News. Coronavirus: Pﬁzer conﬁrms fake versions of vaccine in poland and mexico. https:

//www.bbc.com/news/world-56844149, April 2021.

[18] F. Benhamouda, S. Halevi, and T. Halevi. Supporting private data on hyperledger fabric with secure

multiparty computation. IBM Journal of Research and Development, 63(2/3):3–1, 2019.

[19] G. Benigno, J. di Giovanni, J. J. Groen, and A. Noble.

supply chain pres-
https://libertystreeteconomics.newyorkfed.org/2022/05/

Global

sure index: May 2022 update.
global-supply-chain-pressure-index-may-2022-update/, 2022.

[20] A. Bessani, M. Correia, B. Quaresma, F. André, and P. Sousa. Depsky: dependable and secure storage

in a cloud-of-clouds. Transactions on Storage (TOS), 9(4):12, 2013.

[21] A. N. Bessani, E. P. Alchieri, M. Correia, and J. S. Fraga. Depspace: a byzantine fault-tolerant
coordination service. In ACM SIGOPS/EuroSys European Conference on Computer Systems, pages
163–176, 2008.

[22] S. Bowe, A. Chiesa, M. Green, I. Miers, P. Mishra, and H. Wu. Zexe: Enabling decentralized private

computation. In Symposium on Security and Privacy (SP), pages 947–964. IEEE, 2020.

[23] G. Bracha and S. Toueg. Asynchronous consensus and broadcast protocols. Journal of the ACM

(JACM), 32(4):824–840, 1985.

[24] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, P. Dimov, H. Ding, J. Ferris, A. Giardullo, S. Kulkarni,
H. Li, et al. Tao: Facebook’s distributed data store for the social graph. In Annual Technical Conf.
(ATC), pages 49–60. USENIX Association, 2013.

27

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

Bruggeman
400 million

[25] L.
for
probe
emergent-biosolutions-discarded-ingredients-400-million-covid-19/story?id=84604285,
2022.

ingredients
https://abcnews.go.com/US/

S.
covid-19

biosolutions

Emergent

discarded

vaccines,

Pezenik.

ﬁnds.

and

[26] B. Bünz, S. Agrawal, M. Zamani, and D. Boneh. Zether: Towards privacy in a smart contract world.

In Int. Conf. on Financial Cryptography and Data Security, pages 423–443. Springer, 2020.

[27] C. Cachin and M. Vukolić. Blockchain consensus protocols in the wild. In Int. Symposium on Distributed

Computing (DISC), pages 1–16, 2017.

[28] M. Castro, B. Liskov, et al. Practical byzantine fault tolerance. In Symposium on Operating systems

design and implementation (OSDI), volume 99, pages 173–186. USENIX Association, 1999.

[29] E. Cecchetti, F. Zhang, Y. Ji, A. Kosba, A. Juels, and E. Shi. Solidus: Conﬁdential distributed ledger
transactions via pvorm. In SIGSAC Conf. on Computer and Communications Security, pages 701–717.
ACM, 2017.

[30] J. A. Chacko, R. Mayer, and H.-A. Jacobsen. Why do my blockchain transactions fail? a study of
hyperledger fabric. In SIGMOD Int. Conf. on Management of Data, pages 221–234. ACM, 2021.

[31] J. M. Chase. Quorum white paper, 2016.

[32] J. Chen, T. Cai, W. He, L. Chen, G. Zhao, W. Zou, and L. Guo. A blockchain-driven supply chain

ﬁnance application for auto retail industry. Entropy, 22(1):95, 2020.

[33] R. Cheng, F. Zhang, J. Kos, W. He, N. Hynes, N. Johnson, A. Juels, A. Miller, and D. Song. Ekiden:
A platform for conﬁdentiality-preserving, trustworthy, and performant smart contracts. In European
Symposium on Security and Privacy (EuroS&P), pages 185–200. IEEE, 2019.

[34] O. P. Choudhary, Priyanka, I. Singh, T. A. Mohammed, and A. J. Rodriguez-Morales. Fake covid-
19 vaccines: scams hampering the vaccination drive in india and possibly other countries. Human
Vaccines & Immunotherapeutics, pages 1–2, 2021.

[35] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, et al. Spanner: Google’s globally distributed database.

Transactions on Computer Systems (TOCS), 31(3):8, 2013.

[36] H. Dang, T. T. A. Dinh, D. Loghin, E.-C. Chang, Q. Lin, and B. C. Ooi. Towards scaling blockchain

systems via sharding. In SIGMOD Int. Conf. on Management of Data. ACM, 2019.

[37] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian,
P. Vosshall, and W. Vogels. Dynamo: amazon’s highly available key-value store. In Operating Systems
Review (OSR), volume 41, pages 205–220. ACM SIGOPS, 2007.

[38] T. T. A. Dinh, R. Liu, M. Zhang, G. Chen, B. C. Ooi, and J. Wang. Untangling blockchain: A
data processing view of blockchain systems. IEEE transactions on knowledge and data engineering,
30(7):1366–1385, 2018.

[39] T. T. A. Dinh, J. Wang, G. Chen, R. Liu, B. C. Ooi, and K.-L. Tan. Blockbench: A framework
for analyzing private blockchains. In SIGMOD Int. Conf. on Management of Data, pages 1085–1100.
ACM, 2017.

[40] S. Duan and H. Zhang. Practical state machine replication with conﬁdentiality. In Symposium on

Reliable Distributed Systems (SRDS), pages 187–196. IEEE, 2016.

[41] M. El-Hindi, C. Binnig, A. Arasu, D. Kossmann, and R. Ramamurthy. Blockchaindb: A shared

database on blockchains. Proceedings of the VLDB Endowment, 12(11):1597–1609, 2019.

[42] D. Evans, V. Kolesnikov, and M. Rosulek. A pragmatic introduction to secure multi-party computation.

Foundations and Trends® in Privacy and Security, 2(2-3), 2017.

[43] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty

process. Journal of the ACM (JACM), 32(2):374–382, 1985.

[44] A. Gabizon and Z. J. Williamson. Plonk: Permutations over lagrange-bases for oecumenical noninter-

active arguments of knowledge. 2019.

[45] L. Glendenning, I. Beschastnikh, A. Krishnamurthy, and T. Anderson. Scalable consistency in scatter.

In Symposium on Operating Systems Principles (SOSP), pages 15–28. ACM, 2011.

[46] C. Gorenﬂo, L. Golab, and S. Keshav. Xox fabric: A hybrid approach to transaction execution. In

Int. Conf. on Blockchain and Cryptocurrency (ICBC), pages 1–9. IEEE, 2020.

28

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

[47] C. Gorenﬂo, S. Lee, L. Golab, and S. Keshav. Fastfabric: Scaling hyperledger fabric to 20,000 trans-
actions per second. In Int. Conf. on Blockchain and Cryptocurrency (ICBC), pages 455–463. IEEE,
2019.

[48] C. Gorenﬂo, S. Lee, L. Golab, and S. Keshav. Fastfabric: Scaling hyperledger fabric to 20,000 trans-

actions per second. arXiv preprint arXiv:1901.00910, 2019.

[49] S. Gupta, S. Rahnama, J. Hellings, and M. Sadoghi. Resilientdb: Global scale resilient blockchain

fabric. Proceedings of the VLDB Endowment, 13(6):868–883, 2020.

[50] S. Gupta, S. Rahnama, and M. Sadoghi. Permissioned blockchain through the looking glass: Architec-
tural and implementation lessons learned. In Int. Conf. on Distributed Computing Systems (ICDCS),
pages 754–764. IEEE, 2020.

[51] S. Han, Z. Xu, Y. Zeng, and L. Chen. Fluid: A blockchain based framework for crowdsourcing. In

SIGMOD Int. Conf. on Management of Data, pages 1921–1924. ACM, 2019.

[52] M. Herlihy. Atomic cross-chain swaps. In Symposium on Principles of Distributed Computing (PODC),

pages 245–254. ACM, 2018.

[53] D. Hopwood, S. Bowe, T. Hornby, and N. Wilcox. Zcash protocol speciﬁcation. GitHub: San Francisco,

CA, USA, 2016.

[54] Hyperledger.

Private

data

collections:

A

high-level

overview.

https://www.hyperledger.org/blog/2018/10/23/private-data-collections-a-high-level-overview.

[55] Z. István, A. Sorniotti, and M. Vukolić. Streamchain: Do blockchains need blocks? In Workshop on
Scalable and Resilient Infrastructures for Distributed Ledgers (SERIAL), pages 1–6. ACM, 2018.
[56] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, A. Rasin, S. Zdonik, E. P. Jones, S. Madden, M. Stone-
braker, Y. Zhang, et al. H-store: a high-performance, distributed main memory transaction processing
system. Proc. of the VLDB Endowment, 1(2):1496–1499, 2008.

[57] H. Kalodner, S. Goldfeder, X. Chen, S. M. Weinberg, and E. W. Felten. Arbitrum: Scalable, private

smart contracts. In USENIX Security Symposium), pages 1353–1370, 2018.

[58] M. Khan and A. Babay. Toward intrusion tolerance as a service: Conﬁdentiality in partially cloud-
based bft systems. In Int. Conf. on Dependable Systems and Networks (DSN), pages 14–25. IEEE,
2021.

[59] E. Kokoris-Kogias, E. C. Alp, L. Gasser, P. Jovanovic, E. Syta, and B. Ford. Calypso: Private data
management for decentralized ledgers. Proceedings of the VLDB Endowment, 14(4):586–599, 2020.
[60] E. Kokoris-Kogias, P. Jovanovic, L. Gasser, N. Gailly, E. Syta, and B. Ford. Omniledger: A secure,
scale-out, decentralized ledger via sharding. In Symposium on Security and Privacy (SP), pages 583–
598. IEEE, 2018.

[61] K. Korpela, J. Hallikas, and T. Dahlberg. Digital supply chain transformation toward blockchain

integration. In Hawaii Int. Conf. on system sciences (HICSS), 2017.

[62] A. Kosba, A. Miller, E. Shi, Z. Wen, and C. Papamanthou. Hawk: The blockchain model of cryp-
tography and privacy-preserving smart contracts. In Symposium on security and privacy (SP), pages
839–858. IEEE, 2016.

[63] A. Kosba, C. Papamanthou, and E. Shi. xjsnark: A framework for eﬃcient veriﬁable computation. In

2018 IEEE Symposium on Security and Privacy (SP), pages 944–961. IEEE, 2018.

[64] J. Kwon. Tendermint: Consensus without mining. 2014.
[65] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the

ACM, 21(7):558–565, 1978.

[66] L. Lamport. Paxos made simple. ACM Sigact News, 32(4):18–25, 2001.
[67] L. Luu, V. Narayanan, C. Zheng, K. Baweja, S. Gilbert, and P. Saxena. A secure sharding protocol for
open blockchains. In SIGSAC Conf. on Computer and Communications Security (CCS), pages 17–30.
ACM, 2016.

[68] S. Ma, T. Dasu, Y. Kanza, D. Srivastava, and L. Xiong. Fraud buster: Tracking irsf using blockchain

while protecting business conﬁdentiality. In CIDR, 2021.

[69] M. Maller, S. Bowe, M. Kohlweiss, and S. Meiklejohn. Sonic: Zero-knowledge snarks from linear-
size universal and updatable structured reference strings. In Proceedings of the 2019 ACM SIGSAC
Conference on Computer and Communications Security, pages 2111–2128, 2019.

29

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

[70] M. A. Marsh and F. B. Schneider. Codex: A robust and secure secret distribution system. Transactions

on Dependable and secure Computing, 1(1):34–47, 2004.

[71] N. Narula, W. Vasquez, and M. Virza. zkledger: Privacy-preserving auditing for distributed ledgers. In
15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18), pages
65–80, 2018.

[72] S. Nathan, C. Govindarajan, A. Saraf, M. Sethi, and P. Jayachandran. Blockchain meets database:
design and implementation of a blockchain relational database. Proceedings of the VLDB Endowment,
12(11):1539–1552, 2019.

[73] F. Nawab and M. Sadoghi. Blockplane: A global-scale byzantizing middleware. In 2019 IEEE 35th

Int. Conf. on Data Engineering (ICDE), pages 124–135. IEEE, 2019.

[74] D. Ongaro and J. K. Ousterhout. In search of an understandable consensus algorithm. In Annual

Technical Conf. (ATC), pages 305–319. USENIX Association, 2014.

[75] W. H. Organization. Medical product alert n°5/2021: Falsiﬁed covishield vaccine. https://www.who.
int/news/item/31-08-2021-medical-product-alert-n-5-2021-falsified-covishield-vaccine,
2021.

[76] W. H. Organization et al. A study on the public health and socioeconomic impact of substandard and

falsiﬁed medical products. 2017.

[77] P.

Owen.

Vaccine

supply

under

threat

from

theft

and

counterfeits.

https://www.ttclub.com/news-and-resources/news/press-releases/2021/
vaccine-supply-under-threat-from-theft-and-counterfeits/, March 2021.

[78] R. Padilha and F. Pedone. Belisarius: Bft storage with conﬁdentiality. In 2011 IEEE 10th International

Symposium on Network Computing and Applications, pages 9–16. IEEE, 2011.

[79] D. Patterson. Hackers are attacking the covid-19 vaccine supply chain. https://www.cbsnews.com/

news/covid-19-vaccine-hackers-supply-chain/, 2021.

[80] Y. Peng, M. Du, F. Li, R. Cheng, and D. Song. Falcondb: Blockchain-based collaborative database.

In SIGMOD Int. Conf. on Management of Data, pages 637–652, 2020.

[81] Z. Peng, C. Xu, H. Wang, J. Huang, J. Xu, and X. Chu. P2b-trace: Privacy-preserving blockchain-
based contact tracing to combat pandemics. In SIGMOD Int. Conf. on Management of Data, pages
2389–2393, 2021.

[82] Z. Peng, J. Xu, X. Chu, S. Gao, Y. Yao, R. Gu, and Y. Tang. Vfchain: Enabling veriﬁable and auditable
federated learning via blockchain systems. IEEE Transactions on Network Science and Engineering,
2021.

[83] J. Qi, X. Chen, Y. Jiang, J. Jiang, T. Shen, S. Zhao, S. Wang, G. Zhang, L. Chen, M. H. Au, et al.
Bidl: A high-throughput, low-latency permissioned blockchain framework for datacenter networks. In
Symposium on Operating Systems Principles (SOSP), pages 18–34. ACM SIGOPS, 2021.

[84] S. Reilly,
where.
fake-vaccine-cards-are-everywhere-its-a-public-health-nightmare/, 2022.

J. Lambert,
it’s a public health nightmare.

and M. Stiles.
every-
Fake vaccine
https://www.grid.news/story/science/2022/01/25/

J. Paladino,

cards are

[85] P. Ruan, G. Chen, T. T. A. Dinh, Q. Lin, B. C. Ooi, and M. Zhang. Fine-grained, secure and eﬃcient
data provenance on blockchain systems. Proceedings of the VLDB Endowment, 12(9):975–988, 2019.
[86] P. Ruan, T. T. A. Dinh, D. Loghin, M. Zhang, G. Chen, Q. Lin, and B. C. Ooi. Blockchains vs.
distributed databases: Dichotomy and fusion. In SIGMOD Int. Conf. on Management of Data, pages
1504–1517, 2021.

[87] P. Ruan, D. Loghin, Q.-T. Ta, M. Zhang, G. Chen, and B. C. Ooi. A transactional perspective on
execute-order-validate blockchains. In SIGMOD Int. Conf. on Management of Data, pages 543–557.
ACM, 2020.

[88] Sanoﬁ. Journey of vaccine: a complex manufacturing process. https://www.sanoﬁ.com/en/your-

health/vaccines/production, 2019.

[89] S. Schiﬄing and N. Valantasis Kanellos.

mal, but supply chains will get worse before they get better.
shanghai-worlds-biggest-port-is-returning-to-normal-but-supply-chains-will-get-worse/
before-they-get-better-182720, 2022.

Shanghai: world’s biggest port is returning to nor-
https://theconversation.com/

30

Qanaat: A Scalable Multi-Enterprise Permissioned Blockchain System

[90] F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial.

Computing Surveys (CSUR), 22(4):299–319, 1990.

[91] A. Sharma, F. M. Schuhknecht, D. Agrawal, and J. Dittrich. Blurring the lines between blockchains
and database systems: the case of hyperledger fabric. In SIGMOD Int. Conf. on Management of Data,
pages 105–122. ACM, 2019.

[92] M.-K. Sit, M. Bravo, and Z. István. An experimental framework for improving the performance of
In Proceedings of the 15th ACM Int. Conf. on

bft consensus for future permissioned blockchains.
Distributed and Event-based Systems, pages 55–65, 2021.

[93] S. Steﬀen, B. Bichsel, M. Gersbach, N. Melchior, P. Tsankov, and M. Vechev. zkay: Specifying and
enforcing data privacy in smart contracts. In ACM SIGSAC Conf. on Computer and Communications
Security (CCS), pages 1759–1776, 2019.
counterfeit

vaccination

covid-19

en-
https://www.forbes.com/sites/judystone/2021/03/31/

danger
how-counterfeit-covid-19-vaccines-and-vaccination-cards-endanger-us-all/?sh=
eaddb0e36495, 2021.

How
all.

vaccines

[94] J.

Stone.

cards

and

us

[95] R. Taft, E. Mansour, M. Seraﬁni, J. Duggan, A. J. Elmore, A. Aboulnaga, A. Pavlo, and M. Stoneb-
raker. E-store: Fine-grained elastic partitioning for distributed transaction processing systems. Proc.
of the VLDB Endowment, 8(3):245–256, 2014.

[96] A. Thomson, T. Diamond, S.-C. Weng, K. Ren, P. Shao, and D. J. Abadi. Calvin: fast distributed
transactions for partitioned database systems. In SIGMOD Int. Conf. on Management of Data, pages
1–12. ACM, 2012.

[97] F. Tian. A supply chain traceability system for food safety based on haccp, blockchain & internet of
things. In Int. Conf. on service systems and service management (ICSSSM), pages 1–6. IEEE, 2017.
[98] R. Vassantlal, E. Alchieri, B. Ferreira, and A. Bessani. Cobra: Dynamic proactive secret sharing for
conﬁdential bft services. In Symposium on Security and Privacy (SP), pages 1528–1528. IEEE, 2022.
produced?
manufacturing.

Vaccines

vaccines

[99] V.

(VE).

how

are

E.

https://www.vaccineseurope.eu/about-vaccines/how-are-vaccines-produced, 2016.

[100] S. Wang, T. T. A. Dinh, Q. Lin, Z. Xie, M. Zhang, Q. Cai, G. Chen, B. C. Ooi, and P. Ruan.
Forkbase: An eﬃcient storage engine for blockchain and forkable applications. Proceedings of the
VLDB Endowment, 11(10):1137–1150, 2018.

[101] C. Xu, C. Zhang, and J. Xu. vchain: Enabling veriﬁable boolean range queries over blockchain

databases. In SIGMOD Int. Conf. on Management of Data, pages 141–158, 2019.

[102] Z. Xu and L. Chen. Div: Resolving the dynamic issues of zero-knowledge set membership proof in the

blockchain. In SIGMOD Int. Conf. on Management of Data, pages 2036–2048. ACM, 2021.

[103] J. Yin, J.-P. Martin, A. Venkataramani, L. Alvisi, and M. Dahlin. Separating agreement from execution

for byzantine fault tolerant services. Operating Systems Review (OSR), 37(5):253–267, 2003.

[104] V. Zakhary, D. Agrawal, and A. El Abbadi. Atomic commitment across blockchains. Proc. of the

VLDB Endowment, 13:1319–1331, 2020.

[105] V. Zakhary, M. J. Amiri, S. Maiyya, D. Agrawal, and A. El Abbadi. Towards global asset management

in blockchain systems. arXiv preprint arXiv:1905.09359, 2019.

[106] M. Zamani, M. Movahedi, and M. Raykova. Rapidchain: Scaling blockchain via full sharding.

SIGSAC Conf. on Computer and Communications Security, pages 931–948. ACM, 2018.

In

[107] C. Zhang, C. Xu, H. Wang, J. Xu, and B. Choi. Authenticated keyword search in scalable hybrid-
storage blockchains. In Int. Conf. on Data Engineering (ICDE), pages 996–1007. IEEE, 2021.

31

