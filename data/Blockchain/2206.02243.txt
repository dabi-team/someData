2
2
0
2

n
u
J

5

]

C
D
.
s
c
[

1
v
3
4
2
2
0
.
6
0
2
2
:
v
i
X
r
a

1

Resource Optimization for Blockchain-based
Federated Learning in Mobile Edge Computing

Zhilin Wang, Qin Hu, Zehui Xiong

Abstract—With the development of mobile edge computing (MEC) and blockchain-based federated learning (BCFL), a number of
studies suggest deploying BCFL on edge servers. In this case, resource-limited edge servers need to serve both mobile devices for
their ofﬂoading tasks and the BCFL system for model training and blockchain consensus in a cost-efﬁcient manner without sacriﬁcing
the service quality to any side. To address this challenge, this paper proposes a resource allocation scheme for edge servers, aiming to
provide the optimal services with the minimum cost. Speciﬁcally, we ﬁrst analyze the energy consumed by the MEC and BCFL tasks,
and then use the completion time of each task as the service quality constraint. Then, we model the resource allocation challenge into
a multivariate, multi-constraint, and convex optimization problem. To solve the problem in a progressive manner, we design two
algorithms based on the alternating direction method of multipliers (ADMM) in both the homogeneous and heterogeneous situations
with equal and on-demand resource distribution strategies, respectively. The validity of our proposed algorithms is proved via rigorous
theoretical analysis. Through extensive experiments, the convergence and efﬁciency of our proposed resource allocation schemes are
evaluated. To the best of our knowledge, this is the ﬁrst work to investigate the resource allocation dilemma of edge servers for BCFL in
MEC.

Index Terms—Blockchain, federated learning, resource allocation, mobile edge computing, ADMM

(cid:70)

1 INTRODUCTION

A S embedded sensors are widely deployed on mobile

devices, such as smartphones and smart vehicles, they
can pervasively perceive the physical world and collect an
extensive amount of data. With the advances of hardware
technology, it becomes promising for devices to process the
collected data locally, such as training machine learning
models. However, as the resources of mobile devices are
usually inadequate, they may experience difﬁculty ﬁnishing
computing-intensive tasks, which drives the emergence of
mobile edge computing (MEC). Its basic idea is to facilitate
mobile devices ofﬂoading computing tasks to their nearby
edge servers with sufﬁcient resources and then obtain the
calculated results with communication-efﬁciency in close
proximity [1], [2]. MEC has been applied to many ﬁelds,
such as the Internet of Things (IoT) [3], [4], smart healthcare
[5], [6], and smart transportation [7], [8].

To address the main challenges of federated learning
(FL) [9], [10], such as the single point of failure and the
privacy protection of model updates, blockchain has been
extensively used to assist in achieving full decentralization
with security [11], [12], which is termed as Blockchain-based
FL (BCFL). This new framework connects participants in FL,
i.e., clients, through blockchain network and requires them
to complete both FL and blockchain related operations, such

This work is partly supported by the US NSF under grant CNS-2105004.

• Zhilin Wang and Qin Hu are with the Department of Computer &
Information Science, Indiana University-Purdue University Indianapolis,
is Qin Hu. Email: {wangzhil,
USA. The
qinhu}@iu.edu

corresponding author

• Zehui Xiong is with the Pillar of Information Systems Technology &
Design, Singapore University of Technology Design, Singapore. Email:
zehui xiong@sutd.edu.sg

as data collection, model training, and block generation [13].
As for a client in BCFL, a large amount of resources will be
consumed in completing the BCFL task, however, making
it an impractical job for mobile devices with constrained re-
sources. To address this issue, researchers advocate deploy-
ing BCFL at the edge given that edge servers usually have
strong computing, communication, and storage capabilities
for FL model training and blockchain consensus [14], [15].

In this case, the MEC servers are responsible for com-
pleting both the BCFL and MEC tasks. For the MEC task,
the edge server is usually required to allocate the commu-
nication resource (e.g., bandwidth) for mobile devices to
transfer data, the storage resource for data caching, and
the computing resources (e.g., CPU cycle frequency) for
computing based on the received data. Similarly, for the
BCFL task, the edge server needs to distribute the band-
width resource for sharing model updates and reaching
consensus among blockchain nodes, the storage resource
for saving the copy of blockchain data and local training
data, and the CPU resource for FL model training and
updating, as well as the generation of new blocks. Since both
the MEC and BCFL tasks are time-sensitive and could be
performed simultaneously at the MEC servers, the servers
have to deal with the challenge of serving both the lower-
layer mobile edge devices and the upper-layer BCFL system
without signiﬁcant delay, which makes it a necessity to
design reasonable resource allocation schemes for them.

The existing resource allocation mechanisms for BCFL
and MEC can be classiﬁed into two main categories, where
the ﬁrst type of studies focus on allocating resources to
each device for the requested MEC task [16]–[18], and the
other is to allocate resources for model training and block
generation in the BCFL task [19], [20]. Although the state-
of-the-art studies can help edge servers allocate resources to

 
 
 
 
 
 
handling the MEC or BCFL tasks, these mechanisms have
never considered the resource conﬂicts when both tasks are
running on servers at the same time.

To ﬁll the gap, we design a resource allocation scheme
that allows the edge server to ﬁnish both the MEC and BCFL
tasks simultaneously and timely. Speciﬁcally, we deﬁne
the cost as the total energy consumed by the edge server
in completing both the MEC and BCFL tasks, and then
use the corresponding time requirements as the constraints
on the quality of services provided by the edge server.
We can transform the resource allocation problem into
a multivariate, multi-constraint, and convex optimization
problem. However, solving this optimization problem faces
the following challenges: 1) there are multiple variables
since assigning resources to the MEC task means making
decisions on resource allocation to each device, where the
number of variables increases with the device quantity; and
2) there are too many constraints, making the solution non-
trivial.

These two challenges result in the invalidity of tradi-
tional optimization methods for multiple variable calcula-
tion. Therefore, we design a scheme based on a distributed
optimization algorithm, named the alternating direction
method of multipliers (ADMM), which is the combination
of dual ascent and dual decomposition, and can determine
multiple variables by iterations in a distributed manner [21].
We adopt the ADMM-based algorithm to solve our pro-
posed resource allocation problem in two scenarios progres-
sively for easy understanding. Speciﬁcally, we ﬁrst apply
the modiﬁed general ADMM (MG-ADMM) algorithm for the
homogeneous scenario, which distributes resources to each
local device equally; then we propose the modiﬁed consensus
ADMM (MC-ADMM) based algorithm to assign resources
to devices on demand in the heterogeneous scenario. Be-
sides, by adding additional regularization terms during
the variable iteration process, our proposed algorithms can
converge in the case of more than two variables, which
is conﬁrmed by theoretical analysis. Finally, we conduct
extensive experiments to testify the convergence and the
effectiveness of our proposed resource allocation schemes.

To the best of our knowledge, we are the ﬁrst to tackle
the challenge of resource allocation for edge servers in the
deployment of BCFL at edge. And our contributions can be
summarized as follows:

• We formulate the resource allocation problem of
BCFL in MEC as a multivariate and multi-constraint
optimization problem, where the solution is the re-
source allocation scheme for edge servers to simul-
taneously handle both the MEC and BCFL tasks
without delay.

• To solve the above optimization problem progres-
sively, we design two algorithms based on MG-
ADMM and MC-ADMM for both the homogeneous
and heterogeneous scenarios.

• To ensure the convergence of algorithms for more
than two variables, we add regularization terms in
our proposed algorithms based on MG-ADMM and
MC-ADMM, and prove the validity with solid theo-
retical analysis.

• We conduct numerous experimental evaluations to

2

prove that the optimization solutions are valid and
our proposed resource allocation schemes are effec-
tive.

The rest of this paper is organized as follows. We
introduce the system model and problem formulation in
Section 2. The MG-ADMM based algorithm to solve the
optimization problem in the homogeneous scenario and the
MC-ADMM based algorithm for the heterogeneous scenario
are displayed in Sections 3 and 4, respectively. Experimental
evaluations are presented in Section 5. We discuss the re-
lated work in Section 6. Finally, we conclude this paper in
Section 7.

2 SYSTEM MODEL AND PROBLEM FORMULATION

In this section, we will discuss the system model from a
general perspective, and then we explore both communica-
tion models and computing models of our proposed system
respectively. We also analyze the cost model of our proposed
resource allocation scheme.

2.1 System Model Overview
We consider an edge server S connected with N local mobile
devices, denoted as i ∈ {1, 2, 3, · · · , N }. Local devices are
usually lacking of resources, so they may choose to ofﬂoad
their computing tasks to its nearby edge server S. In this
way, S can provide necessary resources to help local devices
ﬁnish their ofﬂoading tasks in the MEC scenario. At the
same time, there are multiple edge servers connecting via
blockchain network, where they conduct federated learning
(FL) to form a blockchain-based FL (BCFL) service provider.
In other words, edge servers will be responsible for not
only providing computing services to local devices, but
also maintaining the BCFL system at the same time. The
topology of our considered system is shown in Fig. 1.

Fig. 1: The topology of BCFL in MEC.

In the MEC, local device i ﬁrst transmits an ofﬂoading
request Ri(Di, Ti) to server S, where Di is the data size
of its task and Ti is the time constraint of this task to

BCFL nodeBlockchain-based federated learning systemMobile edge computing systemEdge serverBCFL nodeBCFL nodeEdge serverEdge serverbe ﬁnished. Once S accepts tasks, local devices transmit
their data to S. As for the BCFL system, according to [13],
we consider a fully coupled BCFL which runs FL on a
consortium blockchain. First, the clients of FL, i.e., edge
servers, train the local models using local data which may be
collected from other devices or by themselves, and then they
also work as blockchain nodes to generate new blocks which
contain the local model updates and the newly updated
global model of FL. For simplicity, we treat the FL and
the blockchain jobs together as the BCFL task, consuming
computing, communication, and storage resources.

Generally, S has limited computing capacity and com-
munication bandwidth, which can be denoted as F and B,
respectively. Given that the tasks in both the MEC and BCFL
are usually time-sensitive, simultaneously computing the
ofﬂoading tasks for lower-layer mobile devices and main-
taining the upper-layer BCFL system without any delay
require rigorous and optimal resource allocation at edge
servers.

2.2 Communication Models

In this subsection, we would like to model the communica-
tion resource consumption for ﬁnishing the MEC and BCFL
tasks, respectively.

2.2.1 MEC Task

Communications between any device i and the server S
include sending ofﬂoading request, sending original data
and returning computing results. Since the sizes of the of-
ﬂoading request and computing results are smaller than that
of the data, we only consider the transmission of original
data from devices to the server.

According to Shannon Bound, the data transmission rate

from local device i to edge server S is deﬁned as

rcomm
i

(αi) = αiB log 2(1 +

PiGi
δ2 ),

where αi ∈ (0, 1) represents the percentage of bandwidth
allocated to local devices i; B is the maximum bandwidth
of server S; Pi and Gi are the transmission power and gain
from i to S, respectively; and δ is the Gaussian noise during
the transmission.

Then, we can calculate the time cost of data transmission

from device i to server S as

T comm
i

(αi) =

Di
rcomm
i

(αi)

,

which indicates that the transmission time cost is correlated
to the data size of MEC task.

Also, the data transmission will cost a certain amount of

energy, which can be calculated by

Ecomm

i

(αi) = PiT comm

i

(αi),

and the total consumption of transmitting the data from all
the local devices to the server is calculated as below:

3

2.2.2 BCFL task

The communications during the BCFL task are composed
by sharing updates in the blockchain network and conduct-
ing blockchain consensus. For simplicity, here we treat the
communication in BCFL as a general work process. Let αbcf l
denote the percentage of bandwidth distributed to the BCFL
task, and let Pbcf l and Gbcf l represent the transmission
power and gain of the BCFL task respectively. Then, we can
calculate the data transmission rate in the BCFL task by

rcomm
bcf l

(αbcf l) = αbcf lB log 2(1 +

Pbcf lGbcf l
δ2

).

And the time cost of transmission in the BCFL task is

(cid:92)Dbcf l

,

T comm
bcf l

(αbcf l) =

(αbcf l)

rcomm
bcf l
(cid:92)Dbcf l is the size of required transmission data in the
where
BCFL task, which is smaller than the size of the training and
mining data for the BCFL task, denoted as Dbcf l, at server
S.

The energy consumption of the server for conducting the

BCFL task can be calculated as
Ecomm
bcf l

(αbcf l) = Pbcf lT comm

bcf l

(αbcf l).

2.3 Computing Models

In this part, we describe the time and energy consumed
by the MEC server to process the MEC and BCFL tasks,
respectively.

2.3.1 MEC Task
Let fi ∈ (0, F ) be the CPU cycle frequency allocated to the
task of device i. First, we deﬁne the total CPU cycles used
for the task of device i as µi, and it can be calculated as
µi = Didi with di denoting the unit CPU cycle frequency
required to process one data sample of the MEC task from
device i. Then, the computing time can be calculated by

T comp
i

(fi) =

µi
fi

.

According to [22], the energy cost of computing one

single task of device i is

Ecomp
i

(fi) = γµif 2
i ,

where γ is the parameter correlated to the architecture of the
CPU. Thus, the total energy consumption of computing the
MEC tasks for all devices is calculated by

Ecomp

total (fi) =

N
(cid:88)

i=1

Ecomp
i

(fi).

2.3.2 BCFL Task
Similarly, we deﬁne fbcf l ∈ (0, F ) as the CPU cycle fre-
quency allocated to the BCFL task. Let µbcf l = Dbcf ldbcf l
denote the total CPU cycles for processing the BCFL task,
where dbcf l means the unit CPU cycle used to process one
BCFL data sample.

Then, we can have the time cost of computing the BCFL

Ecomm

total (αi) =

N
(cid:88)

i=1

Ecomm

i

(αi).

task:

T comp
bcf l (fbcf l) =

µbcf l
fbcf l

.

In this way, the energy cost of computing the BCFL task

The eigenvalues of matrix H1 are:

is calculated as:

Ecomp

bcf l (fbcf l) = γµbcf lf 2

bcf l.

2.4 Cost Model

We have discussed the energy consumed by the commu-
nication and computation of the MEC and the BCFL tasks.
Now we can deﬁne the cost model of our proposed resource
allocation scheme. Denoting the total energy cost as U ,
based on the above analysis, we know that U is composed
of the transmission cost and the computing cost. Then, we
have

U (αi, αbcf l, fi, fbcf l) =Ecomm

total (αi) + Ecomm
total (fi) + Ecomp
+ Ecomp

(αbcf l)
bcf l (fbcf l).

bcf l

(1)

2.5 Problem Formulation

The purpose of our resource allocation mechanism is to al-
low the edge server to handle both the MEC and BCFL tasks
satisfying resource and time constraints with the minimum
cost. The edge server should make the decisions about how
many CPU cycle frequencies and how much bandwidth
should be allocated to each task. Technically, the optimal
resource allocation decisions need to consider minimizing
the total energy consumption of the edge server. Thus, we
can formulate the decision making challenge of resource
allocation into an optimization problem as below:

P1 :

arg min
αi,αbcf l,fi,fbcf l

: U

s.t. : C1 : T comm
C2 : T comm
i

bcf l + T comp
+ T comp
i
N
(cid:88)

C3 : αbcf l +

αi ≤ 1,

bcf l ≤ Tbcf l,
≤ Ti,

C4 : fbcf l +

i=1
N
(cid:88)

i=1

fi ≤ F,

C5 : Dbcf l + (cid:92)Dbcf l +

N
(cid:88)

i=1

Di ≤ D,

C6 : fi, fbcf l ∈ (0, F ), αi, αbcf l ∈ (0, 1),

i ∈ {1, 2, · · · , N } ,

where C1 and C2 guarantee that the server can ﬁnish the
BCFL task and MEC task on time; C3 and C4 ensure that the
communication and computing resources allocated to each
task are not out of the maximum capacities of the server;
C5 means that the total data size of all the tasks running
on the server cannot exceed its maximum storage capacity,
denoted as D; C6 clariﬁes the ranges of all variables.

Theorem 2.1. The
U (αi, αbcf l, fi, fbcf l) is convex.

optimization

objective

function

Proof. The Hessian Matrix of U respect to αi, αbcf l, fi, fbcf l
is given by:


















V1 =

2γµbcf l
2N γµi

2Dbcf lpbcf llog(2)
Gbcf l Pbcf l
bcf l ln(
δ2

Bα3

+1)

2Dipilog(2)
i ln( GiPi

δ2 +1)

Bα3

4


















.

It can be seen that all elements in vector V1 are positive.
So matrix H1 is a positive deﬁnite matrix, and thus we can
prove that the optimization objective function U is convex.

However, it is still hard to solve P1 even though the
objective function is convex due to the following reasons:
1) there are multiple variables required to be optimized,
and they are not fully correlated; 2) there are multiple
constraints, making it harder to ﬁnd the optimal solutions.
Hence, we need to design solutions for P1, which will be
introduced in the following sections.

3 MG-ADMM BASED SOLUTION IN THE HOMOGE-
NEOUS SITUATION

To present our resource allocation scheme in a progressive
way, we will give a benchmark solution of P1 in the homoge-
neous situation where all MEC tasks have the same data size
and time requirement. In this case, we start from a simple
case of P1 in this section, where an equal distribution strat-
egy is considered to allocate resources to all local devices,
including both the bandwidth and CPU cycle frequencies.
The equal distribution strategy means that the edge server
distributes the communication and computing resources to
each local device for MEC tasks in an equal way, that is, αi
and fi are the same for any arbitrary device i.

According to Boyd et al. [21], the alternating direction
method of multipliers (ADMM), combining dual ascend and
dual decomposition, is designed to solve problems which
are multivariate, separable and convex. We will solve P1
with the equal distribution strategy based on the modiﬁed
general ADMM (MG-ADMM) method, which is derived
from the basic form of ADMM. In the following, we ﬁrst
introduce MG-ADMM and reformulate the problem based
on the MG-ADMM algorithm, and then we explain how we
solve P1.

3.1 Brief Introduction to MG-ADMM

First, we introduce G-ADMM as the basis of MG-ADMM.
According to Boyd et al. [21], G-ADMM tries to solve the
following problem:










H1 =

Bα3

2Dipilog(2)
i ln( GiPi
0

δ2 +1)

0
0

2Dbcf lpbcf llog(2)
Gbcf l Pbcf l
bcf l ln(
δ2

Bα3

+1)

0

0
0

0

0

0

0

2N γµi
0

0
2γµbcf l










.

f (x) + g(z)

arg min
x,y
s.t : Ax + Bz = c,

where x ∈ Rn, z ∈ Rm, A ∈ Rp∗n, B ∈ Rp∗m, and c ∈ Rp.
Functions f (x) and g(z) are convex, and x and z are two

parameters. The objective of G-ADMM is to ﬁnd the optimal
value p∗:

p∗ = inf {f (x) + g(z)|Ax + Bz = c} .

Then, we can form the augmented Lagrangian as below:

Lρ(x, z, y) =f (x) + g(z) + yT (Ax + Bz − c)

+

ρ
2

(cid:107)Ax + Bz − c(cid:107)2
2 .

where y is the Lagrange multiplier, and ρ > 0 is the penalty
parameter.

We assume that k ∈ {1, 2, · · · , K} iterations are required
to ﬁnd the optimal value, and the updates of the iterations
are:

xk+1 := arg min Lρ(x, zk, yk),

zk+1 := arg min Lρ(xk+1, z, yk),

yk+1 := yk + ρ(Axk+1 + Bzk+1 − c).

It has been proved that when the following two con-
ditions are satisﬁed, the ADMM algorithm can converge:
1) The functions f : Rn → R ∪ (+∞) and g : Rm →
R ∪ (+∞) are closed, proper, and convex; 2) the augmented
Lagrangian Lρ(x, z, y) has a saddle point.

The above G-ADMM algorithm is the basic form, which
is effective to solve the problem which has 2-block (i.e., two
separable functions). However, when we need to solve the
problem with more than two separable functions, imple-
menting G-ADMM directly can’t guarantee convergence.

To handle this issue, He et al. [23] propose a novel
operator splitting method. In this paper, we term it as MG-
ADMM. Let’s take 3-block separable minimization problem
as the example to describe MG-ADMM when the number of
blocks is more then 2.

The form of 3-block separable minimization problem is:

min {f (x) + g(z) + h(y)|Ax + Bz + Ch = b} .

Then the Lagrangian function is:

Lρ(x, z, y, λ) =f (x) + g(z) + h(y)

5

3.2 Problem Reformulation based on MG-ADMM

In the homogeneous scenario, the edge server distributes
the same amount of resources, denoted as α∗ and f ∗, to
each local device. As for the energy cost of computing, it is
the sum of all devices’ costs:

Ecomp

total (f ∗) =

N
(cid:88)

i=1

γµif 2

i = N γµif ∗2.

And the communication cost of the MEC tasks is

Ecomm

total (α∗) =

N
(cid:88)

i=1

PiT comm
i

= N Pi

Di
α∗B log 2(1 + PiGi
δ2 )

.

Thus, we can rewrite U as:

U (cid:48)(α∗, αbcf l, f ∗, fbcf l) = Ecomm
+ Ecomp

total (α∗) + Ecomm
total (f ∗) + Ecomp

bcf l
bcf l (fbcf l).

(αbcf l)

(2)

Besides, the ofﬂoading time costs of communication and

computing are:

(cid:92)
T comp
i

(f ∗) =

µi
f ∗ ,

(cid:92)T comm
i

(α∗) =

Di
α∗B log 2(1 + PiGi
δ2 )

.

Based on the above analysis, in the case of homoge-
neous situation, we need to determine four variables, i.e.,
α∗, αbcf l, f ∗ and fbcf l. We can easily prove that U (cid:48) is convex
based on Theorem 2.1. So we apply MG-ADMM to optimize
U (cid:48) and derive the optimal variables. In this way, we can
reformulate P1 as below:

+ λT (Ax + Bz + Ch − b)
+ (cid:107)Ax + Bz + Cy − b(cid:107)2
2 .

P2:

arg min
α∗,αbcf l,f ∗,fbcf l

: U (cid:48)

And the updates of iterations are:

xk+1 := arg min

(cid:110)

(cid:111)
ρ (x, zk, yk, λk)

Lβ

,

zk+1 := arg min

(cid:26)

Lβ

ρ (xk+1, z, yk, λk) +

yk+1 := arg min

(cid:26)

Lβ

ρ (xk+1, zk, y, λk) +

ρ
2

ρ
2

β

(cid:13)
(cid:13)
2
(cid:13)B(z − zk)
(cid:13)
(cid:13)
(cid:13)
2

β

(cid:13)
(cid:13)
2
(cid:13)C(y − yk)
(cid:13)
(cid:13)
(cid:13)
2

(cid:27)

(cid:27)

,

,

≤ Ti,

i

+

(cid:92)
T comp
i

s.t. : C1, C5 in P1,
C2 : (cid:92)T comm
C3 : αbcf l + N α∗ ≤ 1,
C4 : fbcf l + N f ∗ ≤ F,
C6 : f ∗, fbcf l ∈ (0, F ),
α∗, αbcf l ∈ (0, 1),
i ∈ {1, 2, · · · , N } .

λk+1 := λk − β(Axk+1 + Byk+1 + Czk+1 − b),

3.3 Solution based on MG-ADMM

where β > 1 is the penalty parameter.

First, we form the augmented Lagrangian of P2 as follows:

bcf l, f ∗, f k
f ∗k+1 := arg min(L(α∗k+1, αk
bcf l,
(cid:13)
(cid:13)
ρ
2
(cid:13)N (f ∗ − f ∗k)
(cid:13)
(cid:13)
(cid:13)
2
2

1, λk
λk

4, λk

3, λk

2, λk

5) +

β

6

),

(6)

f k+1
bcf l, f ∗k, fbcf l,
bcf l := arg min(L(α∗k+1, αk
(cid:13)
ρ
2, λk
(cid:13)fbcf l − f k
(cid:13)
2

1, λk
λk

3, λk

4, λk

5) +

β

bcf l

(cid:13)
2
(cid:13)
(cid:13)
2

),

(7)

Di − D)

where β > 0 is the penalty parameter.

The updates of augmented Lagrange multipliers are:

i

(cid:13)
2
(cid:13)
(cid:13)
2

λk+1
1

λk+1
2

:= λk

1 − β(T comm

bcf l

(αk+1

bcf l) + T comp

bcf l (f k+1

bcf l ) − Tbcf l),

(8)

(α∗k+1) +

(cid:92)
T comp
i

(f ∗k+1) − Ti),

:= λk

2 − β( (cid:92)T comm

i

L1 = L(α∗, αbcf l, f ∗, fbcf l, λ1, λ2, λ3, λ4, λ5)
bcf l + T comp
bcf l − Tbcf l)
(cid:92)
T comp
− Ti)
+
i

i

= U (cid:48) + λ1(T comm
+ λ2( (cid:92)T comm
+ λ3(αbcf l + N α∗ − 1)
+ λ4(fbcf l + N f ∗ − F )
N
(cid:88)

+ λ5(Dbcf l + (cid:92)Dbcf l +

+

+

+

+

+

ρ
2
ρ
2
ρ
2
ρ
2

ρ
2

+

− Ti

bcf l + T comp
bcf l − Tbcf l
(cid:13)
2
(cid:92)
T comp
(cid:13)
(cid:13)
i
2

(cid:13)
(cid:13)T comm
(cid:13)
(cid:13)
(cid:92)T comm
(cid:13)
(cid:13)
i
(cid:107)αbcf l + N α∗ − 1(cid:107)2
2
(cid:107)fbcf l + N f ∗ − 1(cid:107)2
2
(cid:13)
(cid:13)
Dbcf l + (cid:92)Dbcf l +
(cid:13)
(cid:13)
(cid:13)

N
(cid:88)

i

Di − D

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

,

where λm > 0 with m ∈ {1, 2, 3, 4, 5} is the augmented
Lagrange multiplier, and ρ > 0 is the penalty parameter.

Theorem 3.1. The augmented Lagrangian of P2, i.e., L1, has a
saddle point.

Proof. The Hessian matrix of L1 is shown in (3).

Then we calculate the eigenvalues of matrix H2 as












V2 =

Di log(2)(2λ2+2Pi+ρ)
i B ln(1+ PiGi
α3
δ2 )
Dbcf l log(2)(2λ1+2Pbcf l+ρ)

−

α3

bcf lB ln(1+

Pbcf lGbcf l
)
δ2
2γµbcf l
2N γµi

N 2ρ
1−N αi−αbcf l

−

3ρ
8(1−αbcf l−N αi)












.

)

3ρ

−

and

α3

bcf lB ln(1+

In vector V2,
it
are positive. As for

is clear that 2γµbcf l and 2N γµi
Di log(2)(2λ2+2Pi+ρ)
i B ln(1+ PiGi
α3
δ2 )
Dbcf l log(2)(2λ1+2Pbcf l+ρ)

N 2ρ
1−N αi−αbcf l

−

Pbcf lGbcf l
δ2
they are non-negative.

8(1−αbcf l−N αi) , we can-
If we let
then we have

−

< 0,

not know whether
N 2ρ
Di log(2)(2λ2+2Pi+ρ)
i B ln(1+ PiGi
1−N αi−αbcf l
α3
δ2 )
N 2ρ
> Di log(2)(2λ2+2Pi+ρ)
1−N αi−αbcf l
above condition is satisﬁed, then we can say that at least
one of the elements in vector V2 is negative. In this way,
matrix H2 is a positive semi-deﬁnite matrix. Thus, L1 has a
saddle point.

. In other words, if the

i B ln(1+ PiGi
α3
δ2 )

Let k ∈ {1, 2, · · · , K} be the iteration round, and the

updates of variables can be expressed as:

α∗k+1 := arg min L(α∗, αk

bcf l, f ∗k, f k

bcf l, λk

1, λk

2, λk

3, λk

4, λk

5),
(4)

αk+1
bcf l := arg min(L(α∗k+1, αbcf l, f ∗k, f k
3, λk

bcf l,
(cid:13)
(cid:13)αbcf l − αk
(cid:13)

1, λk
λk

2, λk

4, λk

5) +

β

ρ
2

(cid:13)
2
(cid:13)
(cid:13)
2

),

(5)

bcf l

λk+1
3

:= λk

3 − β(αbcf l + N α∗ − 1),

λk+1
4

:= λk

4 − β(fbcf l + N f ∗ − F ),

(9)

(10)

(11)

λk+1
5

:= λk

5 − β(Dbcf l + (cid:92)Dbcf l +

N
(cid:88)

i=1

Di − D).

(12)

Then, we can set the stopping criteria for above itera-

tions:

(cid:13)
(cid:13)

(cid:13)α∗k+1 − α∗k(cid:13)

2
(cid:13)
(cid:13)
2

≤ ψ,

(cid:13)
(cid:13)

(cid:13)f ∗k+1 − f ∗k(cid:13)

2
(cid:13)
(cid:13)
2

≤ ψ,

(13)

(cid:13)
(cid:13)αk+1
(cid:13)

bcf l − αk

bcf l

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψ,

(cid:13)
(cid:13)αk+1
(cid:13)

bcf l − αk

bcf l

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψ,

(14)

where ψ is the predeﬁned threshold [24].

Note that (4) to (7) are quadratic optimization problems
and can be solved easily. Due to the space limitation, we
omit the detailed calculations.

It has been proved that when the following two condi-
tions are satisﬁed, the MG-ADMM algorithm can converge:
1) the objective function is closed, proper, and convex; and
2) the augmented Lagrangian has a saddle point. We have
proved that the objective function is convex, and it is also
closed and proper. Besides, we have proved that L1 has a
saddle point in Theorem 3.1. Thus, the convergence of P2 is
guaranteed.

We summarize our proposed solution based on MG-
ADMM in Algorithm 1. First, we initialize four variables
and ﬁve augmented Lagrangian multipliers (Line 1), and
then we update the variables and Lagrange multipliers in an
iterative process (Lines 2-17). Speciﬁcally, we update vari-
ables and Lagrange multipliers (Lines 3-11), and calculate
the stopping criteria (Line 12). If the termination condition
is satisﬁed, then the objective function is converged (Lines
13-15). In the end, we calculate the optimal value of the
objective function, and then all the optimal decisions and
the optimal total energy cost are returned (Lines 18-19).










H2 =

N 2ρ
1−N αi−αbcf l

+ Di log(2)(2λ2+2Pi+ρ)
i B ln(1+ PiGi
α3
δ2 )

0

0
0

3ρ

0
8 (1−αbcfl−N αi) + Dbcf l log(2)(2λ1+2Pbcf l+ρ)
α3
bcf lB ln(1+
0
0

Pbcf lGbcf l
δ2

)

7

(3)

0

0

0

0

2N γµi
0

0
2γµbcf l










Algorithm 1 Solution of P2 based on MG-ADMM Algorithm
Require: Pi, Di, N , Gi, Gbcf l, δ, ψ, Gbcf l, F , γ, dbcf l, ρ,

Pbcf l, Dbcf l, k, Ti, Tbcf l, β, λ1, λ2, λ3, λ4, λ5

Ensure: α∗, αbcf l, f ∗, fbcf l, U (cid:48)
1: Initialize α∗, αbcf l, f ∗, fbcf l, λ1, λ2, λ3, λ4, λ5
2: while Convergence (cid:54)= True do
3:
4:

5:
6:

α∗k+1 ← ﬁnd the optimal value of (4)
αk+1
bcf l ← ﬁnd the optimal value of (5)
f ∗k+1 ← ﬁnd the optimal value of (6)
f k+1
bcf l ← ﬁnd the optimal value of (7)
λk+1
1 ← update (8)
λk+1
2 ← update (9)
λk+1
3 ← update (10)
λk+1
4 ← update (11)
λk+1
5 ← update (12)
Calculate (13) and (14)
if (13) and (14) are satisﬁed then

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end while
18: Calculate U (cid:48) via (2)
19: return α∗, αbcf l, f ∗, fbcf l, U (cid:48)

Convergence = True

end if
k ← k + 1

4 MC-ADMM BASED SOLUTION IN THE HETERO-
GENEOUS SCENARIO

In this section, we consider the heterogeneous scenario with
diverse MEC requests from local devices. To this end, we
need to apply an on-demand resource allocation strategy.
That is to say, we have to determine the resource alloca-
tion decisions for each MEC task, which is more realistic
compared to the equal distribution strategy in the homo-
geneous scenario. Speciﬁcally, we calculate αi and fi for
i ∈ {1, 2, · · · , N }, as well as αbcf l and fbcf l. Thus, the
optimization problem in this scenario is more practical and
complicated. In the following, we ﬁrst introduce modiﬁed
consensus ADMM (MC-ADMM), which is another form of
ADMM. Then we formulate P1 based on the MC-ADMM
algorithm, and the basic idea is to separate the whole opti-
mization task into multiple subtasks which can be resolved
in a distributed manner.

4.1 Brief Introduction to MC-ADMM

At the beginning, we introduce the C-ADMM, which is one
of the ADMM forms. It is designed to solve the following
problem:

arg min
x

N
(cid:88)

i=1

fi(x),

where x ∈ Rn and fi
convex.

: Rn → R ∪ {+∞} are assumed

The basic idea of C-ADMM is dividing a large scale
optimization problem into N subproblems which can be
solved in a distributed manner. So, for (cid:80)N
i=1 fi(x), we can
rewrite it as:

arg min
x

N
(cid:88)

fi(xi)

i=1
s.t. xi − z = 0.

where z ∈ Rn is called as auxiliary variable or global
variable.

The augmented Lagrangian is:

L(x1, x2, · · · , xn, z, y) =

N
(cid:88)

(fi(xi) + yT

i (xi − z)

i=1
ρ
2

+

(cid:107)xi − z(cid:107)2

2),

where (x1, x2, · · · , xn) ∈ RnN .

The updates of parameters are as:

xk+1
i

:= arg min

(cid:110)

L(fi(xi), zk, yk
i )

(cid:111)

,

zk+1 :=

yk+1
i

:= yk

N
(cid:88)

(xk+1

i +

1
N

1
ρ

yk
i ),

i=1
i + ρ(xk+1

i − zk+1).

Similar to the MG-ADMM built upon G-ADMM , MC-
ADMM is based on C-ADMM by adding regularization
terms to the Augmented Lagrangian formula and the vari-
able iteration formulas. Therefore, we omit the detailed
formulas of MC-ADMM for brevity.

4.2 Problem Reformulation based on MC-ADMM

In the heterogeneous scenario, we have to distribute re-
sources to each MEC task and the BCFL task, so there are
2N + 2 variables in total. Directly applying the previous
MG-ADMM algorithm in this case is not practical since
the resource distribution in the heterogeneous situation is
much more complicated than the optimization in the ho-
mogeneous scenario. Besides, the convergence for 2N + 2
variables in the MG-ADMM algorithm is not guaranteed.
Therefore, we resort to the MC-ADMM algorithm, which
can solve the large-scale optimization problem in a dis-
tributed way.

Intuitively, allocating the resources to each device is to
divide the bandwidth and CPU cycle frequency into N + 1
parts to ﬁnd the best decision separately. To calculate αi
and fi for each i ∈ {1, · · · , N }, we ﬁrst deﬁne ˆα and ˆf

as global variables, also called auxiliary variables, to assist
the distributed optimization. Besides, we have to consider
the constraints of P1. For simplicity, we denote the space
formed by the constraints related to αi and fi (i.e., C2-C4 of
P1) as Ω, which is the feasible set of local variables αi and
fi. While the other constrains not related to αi and fi in P1
need to be kept unchanged because they will inﬂuence the
rest two variables, i.e., αbcf l and fbcf l. Then we can have the
reformulated problem as:

P3:

arg min
αi,αbcf l,fi,fbcf l

: U

s.t. : C1 : αi = ˆα,
C2 : fi = ˆf ,
C3 : T comm

bcf l + T comp

bcf l ≤ Tbcf l,
N
(cid:88)

C4 : Dbcf l + (cid:92)Dbcf l +

Di ≤ D,

C5 : (αi, fi) ∈ Ω, αbcf l ∈ (0, 1),

i=1

fbcf l ∈ (0, F ),
ˆα ∈ (0, 1), ˆf ∈ (0, F ),
i ∈ {1, 2, · · · , N } .

4.3 Solution based on MC-ADMM

Here we detail the solution based on MC-ADMM. First, the
augmented Lagrangian form of P3 is:

L2 =L(αi, αbcf l, fi, fbcf l, θi, (cid:15)i, η1, η2, ˆα, ˆf )

= U +

N
(cid:88)

i=1

θi(αi − ˆα) +

N
(cid:88)

i=1

(cid:15)i(fi − ˆf )

+ η1(T comm

bcf l + T comp

bcf l − Tbcf l)
N
(cid:88)

+ η2(Dbcf l + (cid:92)Dbcf l +

Di − D) +

+

+

+

ρ
2

(cid:13)
(cid:13)
2
(cid:13)fi − ˆf
(cid:13)
(cid:13)
(cid:13)
2

ρ
2
(cid:13)
(cid:13)
Dbcf l + (cid:92)Dbcf l +
(cid:13)
(cid:13)
(cid:13)

i=1

(cid:13)
(cid:13)T comm
(cid:13)

bcf l + T comp

bcf l − Tbcf l
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

,

N
(cid:88)

i=1

Di − D

where θi, (cid:15)i, η1, η2 > 0 are augmented Lagrange multipliers.

Theorem 4.1. The augmented Lagrangian of P3, i.e., L2, has a
saddle point.

Proof. The Hessian matrix of L2 is shown in (15).

Then we calculate the eigenvalues of matrix H3 as

0. So H3 is a semi-deﬁnite matrix, and L2 has a saddle point.

8

By applying the method proposed in [23], the updates of

local variables (i.e., αi and fi) are:
(cid:111)

(cid:110)

αk+1
i

, f k+1
i

:= arg min L(αi, αk

bcf l,

bcf l, fi, f k
1 , ηk
i , ηk

2 , (cid:99)αk, (cid:99)f k),

(16)

i , (cid:15)k
θk
The updates of αbcf l and fbcf l are:
bcf l := arg min(L(αk+1
αk+1
, αbcf l, f k+1
i
1 , ηk
i , ηk
i , (cid:15)k
θk
(cid:13)
ρ
(cid:13)αbcf l − αk
(cid:13)
2

β

i

bcf l

, f k+1
bcf l ,
2 , (cid:99)αk, (cid:99)f k)+

(cid:13)
2
(cid:13)
(cid:13)
2

),

(17)

bcf l := arg min(L(αk+1
f k+1
bcf l, f k+1
, αk+1
i
1 , ηk
i , ηk
i , (cid:15)k
θk
(cid:13)
ρ
(cid:13)fbcf l − f k
(cid:13)
2

2 , (cid:99)αk, (cid:99)f k)
(cid:13)
2
(cid:13)
(cid:13)
2

bcf l

+

β

i

, fbcf l,

where ρ, β > 0 are penalty parameters.

And the updates of global variables are:

(cid:91)
αk+1 :=

1
N

N
(cid:88)

i=1

(αk+1

i +

(cid:91)
f k+1 :=

1
N

N
(cid:88)

i=1

(f k+1

i +

ρ
2

ρ
2

θk
i ),

(cid:15)k
i ),

),

(18)

(19)

(20)

Besides, the updates of augmented Lagrange multipliers

are:

ρ
2

(cid:107)αi − ˆα(cid:107)2
2

θk+1
i

(cid:15)k+1
i

:= θk

i + ρ(αk+1

i −

:= (cid:15)k

i + ρ(f k+1

i −

(cid:91)
αk+1),

(cid:91)
f k+1),

(21)

(22)

(cid:13)
2
(cid:13)
(cid:13)
2

ηk+1
1

:= ηk

1 − β(T comm

bcf l

(αk+1

bcf l) + T comp

bcf l (f k+1

bcf l ) − Tbcf l),

ηk+1
2

:= ηk

2 − β(Dbcf l + (cid:92)Dbcf l +

(23)

N
(cid:88)

i=1

Di − D).

(24)

Lastly, the stopping criteria can be set as:

(cid:13)
(cid:13)αk+1
(cid:13)

i −

(cid:91)
αk+1

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψprim,

(cid:13)
(cid:13)f k+1
(cid:13)

i −

(cid:91)
f k+1

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψprim, (25)

V3 =















bcf lγµbcf l + 2N γµi + µiρ
f 2
f 3
i
4γµbcf l

bcf l

f 3
− Dilog(2)(ρ+2N Pi)
i ln( GiPi
δ2 +1)

Bα3

− 2Dbcf llog(2)(Pbcf l+η1)
+1)

Gbcf l Pbcf l
δ2

bcf l ln(

Bα3















.

Clearly, f 2

bcf lγµbcf l + 2N γµi + µiρ
f 3
i

> 0 and

4γµbcf l

f 3
bcf l

while − Dilog(2)(ρ+2N Pi)
Bα3
δ2 +1)

i ln( GiPi

< 0 and − 2Dbcf llog(2)(Pbcf l+η1)
bcf l ln(
+1)

Gbcf l Pbcf l
δ2

Bα3

(cid:13)
(cid:91)
(cid:13)
αk+1 − (cid:99)αk
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψdual,

(cid:13)
(cid:91)
(cid:13)
f k+1 − (cid:99)f k
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)
2

≤ ψdual,

(26)

where ψprim and ψdual are the predeﬁned thresholds [24].
Besides, (14) should also be included as a stopping criteria.
Even though the forms of P2 and P3 are different, the
proof of the convergence is similar. According to Theorem
2.1, we know that U is convex, and it’s clear that U is closed
and proper. In addition, the augmented Lagrangian L2 has
a saddle point. So the convergence of P3 is guaranteed with
the MC-ADMM algorithm.

> 0,

<

For reference, we generalize the solution based on MC-
ADMM in Algorithm 2. We ﬁrst initialize local variables,











H3 =

Dilog(2)(ρ+2N Pi)
i log( GiPi
Bα3
δ2 +1)
0

0

0

0

0

2Dbcf llog(2)(Pbcf l+η1)

Bα3

bcf l log(

Gbcf l Pbcf l
δ2

0

+1)

0

0
bcf lγµbcf l + 2N γµi + µiρ
f 2
f 3
i
0











0

0

0
4γµbcf l

f 3

bcf l

9

(15)

global variables and augmented Lagrangian multipliers
(Line 1), and then we calculate the optimal decisions for
each MEC task (Lines 2-19). In detail, we keep updating
parameters until the objective function is converged (Lines
3-18). Then, we can calculate the optimal total energy cost
and return the optimal decisions (Lines 20-21).

Algorithm 2 Solution of P3 based on MC-ADMM Algorithm
Require: Pi, Di, N , Gi, Gbcf l, δ, ψ, Gbcf l, F , γ, dbcf l, ρ,

Pbcf l, Dbcf l, k, Ti, Tbcf l, β, θi, (cid:15)i, η1, η2

Ensure: αi, αbcf l, fi, fbcf l, U
1: Initialize αi, αbcf l, fi, fbcf l, θi, (cid:15)i, η1, η2, ˆα, ˆf
2: for i ∈ {1, 2, · · · , N } do
3: while Convergence (cid:54)= True do
4:

i ← ﬁnd the optimal values of (16)

5:

6:
7:

8:

, f k+1

αk+1
i
(cid:91)
αk+1 ← ﬁnd the optimal value of (19)
(cid:91)
f k+1 ← ﬁnd the optimal value of (20)
αk+1
bcf l ← ﬁnd the optimal value of (17)
f k+1
bcf l ← ﬁnd the optimal value of (18)
θk+1
i ← update (21)
(cid:15)k+1
i ← update (22)
ηk+1
1 ← update (23)
ηk+1
2 ← update (24)
Calculate (14), (25) and (26)
if (14), (25) and (26) are satisﬁed then

9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19: end for
20: Calculate U via (1)
21: return αi, αbcf l, fi, fbcf l, U

Convergence = True

end if
k ← k + 1

end while

5 EXPERIMENTAL EVALUATION

In this section, we design experiments to test the validity
and efﬁciency of our proposed algorithms. We ﬁrst provide
the parameter setting for experiments, then we present and
analyze the experimental results. We conduct the experi-
ments using Python 3.8.5 in macOS 11.6 running on Intel
i7 processor with 32 GB RAM and 1 TB SSD.

5.1 Basic Experimental Setting

We consider a mobile edge computing scenario with one
edge server and 10 local devices. For brevity, we provide
Table 1 to detail the basic parameter settings in our exper-
iments. As for the settings of some certain experiments,
we will clarify them later. For the augmented Lagrange
multipliers, we set them as 1.0 at the beginning.

TABLE 1: Basic Experimental Setting

Gi = 10

β = 0.5
N = 10
Pi = 2 Gbcf l = 10
k = 100
ρ = 0.5 Pbcf l = 2 F = 1000
Ti = 10 Tbcf l = 50 ψ = 10−3 ψprim = 10−3 ψdual = 10−3

Di = 10
Dbcf l = 10
γ = 0.001

di = 2
dbcf l = 2
δ = 0.1

Note that we have also tested our proposed algorithms
with other parameter settings, while it can be found that the
values of data and time related parameters would not affect
the changing trends of the experimental results. So, we only
report the results for the above parameter settings. Besides,
to avoid the statistical bias, we report the averaged results
for ten rounds of repeated experiments.

5.2 Experimental Results

We design two parts of the experiments: the evaluation
of the MG-ADMM based algorithm and the evaluation of
the MC-ADMM based algorithm. These two algorithms
are designed for different scenarios, i.e., homogeneous and
heterogeneous. In the homogeneous scenario, we assume
that all the parameters of each MEC task are the same,
while in the heterogeneous scenario, we treat each MEC task
individually. Due to the limitation of space, we only present
partial experimental results with importance in this section.

5.2.1 The Evaluation of the MG-ADMM based Algorithm

We ﬁrst evaluate the MG-ADMM based algorithm solving
P2 in the homogeneous scenario, and then we analyze the
impacts of the data sizes of both the MEC and the BCFL
tasks on the optimal decisions in our resource allocation
scheme.

For comparison, we design a random allocation strategy,
which assigns the bandwidth and CPU cycle frequencies to
the MEC and the BCFL tasks in a random way. And we
also consider a ﬁxed allocation strategy, which determines
the resource allocation with ﬁxed values at the beginning.
Besides, we use the G-ADMM algorithm by setting αbcf l
and fbcf l to ﬁxed values as another benchmark solution
since setting other variables as constants cannot return
converged results. Via comparing the proposed MG-ADMM
based algorithm with these three solutions, we plot the
experimental results in Fig. 2(a). We can see that the MG-
ADMM based algorithm can converge after about 80 rounds
of iteration, while random strategy cannot converge. In
addition, the random and ﬁxed strategies and G-ADMM can
only get the total energy cost larger than that of the MG-
ADMM algorithm. The results show that the MG-ADMM
based algorithm outperforms the other three strategies.

As the penalty parameters ρ and β will inﬂuence the
convergence speed of the MG-ADMM based algorithm, we
set the values of ρ as {0.1, 0.5, 1.0}, and maintain other

10

optimization problem will be more difﬁcult, so more time
will be cost to converge.

In the homogeneous scenario, both the bandwidth and
CPU cycle frequencies assigned to each local device are
the same, so we only need to calculate four variables,
i.e., α∗, αbcf l, f ∗, fbcf l for the optimal allocation decisions.
In Fig. 3, for different data sizes of the MEC tasks (Di)
and the BCFL task (Dbcf l), the results show that the data
sizes of tasks signiﬁcantly inﬂuence the resource allocation
decisions. In Figs. 3(a) and (b), it can be seen that the larger
the data size of each MEC task, the more communication
and computing resources allocated to devices and the fewer
resources allocated to the BCFL task. Similarly, we can
conclude from Figs. 3(c) and (d) that more resources will
be distributed to the BCFL task and fewer resources will be
assigned to the MEC tasks if the data size of the BCFL task
is larger. The results match the intuition that the larger data
size of a task requires more resources in communication and
computing.

(a) Scheme comparison.

(b) Penalty parameter ρ.

(c) Penalty parameter β.

(d) The number of devices N .

Fig. 2: The convergence of the MMG-ADMM based algo-
rithm.

(a) Bandwidth.

(b) CPU cycle frequencies.

(a) Strategies comparison.

(b) The penalty parameter ρ.

(c) The penalty parameter β.

(d) The number of devices N .

(c) Bandwidth.

(d) CPU cycle frequencies.

Fig. 4: The convergence of the MC-ADMM based algorithm.

Fig. 3: The optimal resource allocation decisions based on
the MG-ADMM algorithm.

parameters unchanged. The results in Fig. 2(b) show that
the faster convergence speed comes for a larger ρ. Similarly,
we can see from Fig. 2(c) that the convergence speed will
be faster when β is larger. The reason is that the penalty
parameters control the length of the step in each iteration
and larger penalty parameters will lead to the greater length
of each step, so the convergence speed will be faster.

To testify the impact of the number of local devices
on the convergence of MG-ADMM, we plot experimental
results in Fig. 2(d). We can see that the convergence speed
will be slower and the optimal value will be larger when
the number of local devices increases, which indicates that
it will inﬂuence not only the convergence speed but also
the optimal value of the total energy cost. This is because
with more devices involved in the MEC tasks, the edge
server will cost more energy to work for the tasks, and the

5.2.2 The Evaluation of MC-ADMM based Algorithm

In this part, the experiments are designed to evaluate the
optimization objective of P3 from the perspective of con-
vergence and reveal the relationship between the data sizes
of tasks and the optimal resource allocation decisions, i.e.,
the optimization variables in P3. The parameter setting
is Di ∈ {1, 2, 3, · · · , 10} and Ti ∈ {1, 2, 3, · · · , 10} with
N = 10, ρ = 0.5 and β = 1.0, while others are the same
with the above experiments.

First, we compare our proposed MC-ADMM based algo-
rithm with the above-mentioned random allocation strategy
and ﬁxed allocation strategy and C-ADMM in terms of
checking the convergence speed of each strategy. Similar
to the setting mentioned above regarding G-ADMM, C-
ADMM is implemented by setting αbcf l and fbcf l as con-
stants. The results are reported in Fig. 4(a), which shows
that our proposed algorithm performs well in solving P3
since it can converge and achieve a lower stable value of the
total energy cost than the other three strategies.

020406080100Iterations200250300350400450Total Energy CostRandomFixedG-ADMMMG-ADMM0255075100125150175200Iterations210220230240250260Total Energy Cost=0.1=0.5=1.0020406080100120140Iterations220240260280300Total Energy Cost=0.1=0.5=1.0020406080100Iterations180200220240260Total Energy CostN=5N=10N=15246810The Data Size of MEC Task9.0%9.3%9.5%9.8%10.0%10.2%10.5%10.8%11.0%Bandwidth*bcfl246810The Data Size of MEC Task859095100105110CPU Cycle Frequencyf*fbcfl246810The Data Size of BCFL Task8.8%9.0%9.2%9.4%9.6%9.8%Bandwidth*bcfl246810The Data Size of BCFL Task9092949698100CPU Cycle Frequencyf*fbcfl01020304050Iterations500050010001500200025003000Total Energy CostRandomFixedC-ADMMMC-ADMM01020304050Iterations02000400060008000100001200014000Total Energy Cost=0.10=0.45=0.5001020304050Iterations050010001500200025003000Total Energy Cost=0.10=0.50=1.0001020304050Iterations050010001500200025003000Total Energy CostN=8N=9N=10ρ

Then, we

test how penalty parameters

∈
{0.10, 0.45, 0.50} and β ∈ {0.10, 0.50, 1.00} inﬂuence the
convergence speed. From Figs. 4(b) and (c), we can know
that the larger penalties will cause faster convergence speed.
What’s more, We ﬁnd that the value of ρ cannot be too
large, or the algorithm would not converge. We also test
the inﬂuence of the number of local devices (N ∈ {8, 9, 10})
with the results in Fig. 4(d) showing that more local devices
will lead to more cost and slower convergence speed.

By comparing Figs. 2 and 4, it can be seen that MG-
ADMM requires about 80 rounds to converge, while MC-
ADMM only needs less than 50 rounds to reach the stable
value, which indicates that the distributed algorithm is more
effective.

11

(a) Latency changes with the data
size of MEC task.

(b) Latency changes with the data
size of BCFL task.

Fig. 6: The Latency based on the MG-ADMM algorithm.

(a) Bandwidth.

(b) CPU cycle frequencies.

Fig. 7: The Latency based on the MC-ADMM algorithm.

(a) Latency changes with the data
size of MEC task.

(b) Latency changes with the data
size of BCFL task.

i

= T comm
i

First, we let T mec

be the total time
consumed by the MEC server in processing the MEC task
submitted by user i according to the optimal decisions.
Similarly, we can deﬁne T bcf l = T comm
bcf l as the time
consumption for processing the BCFL task.

bcf l + T comp

+ T comp
i

(c) Bandwidth.

(d) CPU cycle frequencies.

Fig. 5: The optimal resource allocation decisions based on
the MC-ADMM algorithm.

In P3, we have to determine αi and fi for each i ∈
{1, 2, 3, · · · , N }, as well as αbcf l and fbcf l. Thus, we need to
calculate 2N +2 variables. Here, we set N = 5, and we want
to know how the increase and decrease in the sizes of data
for the MEC and BCFL tasks affect the optimal decisions. We
ﬁrst let Di decrease by 10% and 20%, and then increase it by
10% and 20%. The changes of the percentage are expressed
as {−0.2, −0.1, 0, 0.1, 0.2} in Fig. 5, where 0 refers to the
original data size. From the results in Figs. 5(a) and (b),
we can see that more resources are allocated to the MEC
tasks and fewer resources are distributed to the BCFL task
when Di increases. Conversely, the results in Figs. 5(c) and
(d) show that more resources are assigned to the BCFL task
when Dbcf l is larger. This is consistent with the changing
trends in the homogeneous scenario and can be explained
by the same reason that more resources are needed to ﬁnish
tasks with larger data sizes.

5.2.3 The Evaluation of Latency
In an ideal scenario, the MEC server would devote the
appropriate resources to task processing based on the de-
cisions obtained by the algorithms we designed. In this
part, experiments are conducted to evaluate the latency
of processing the MEC and BCFL tasks according to the
decisions obtained from our algorithms.

Based on the same experimental settings as in Fig. 3,
we calculate the latency of completing both MEC and BCFL
tasks. The results based on MG-ADMM are shown in Fig. 6.
In Fig. 6(a), we can see that T bcf l increases slightly and T mec
increases signiﬁcantly when Di increases. This is because
when the data size of MEC task is larger, more time will be
required to complete this task. While less resources will be
allocated to process the BCFL task, T bcf l will be also larger.
Similarly, we can see the results with the change of Dbcf l in
Fig. 6(b).

i

Then, we analyze the latency of the MC-ADMM algo-
rithm with the same settings as in Fig. 5. The results are
shown in Fig. 7. It is clear that when the data sizes of the
MEC and BCFL tasks required to be processed increase, the
time spent by the server to complete the tasks also increases.

6 RELATED WORK
Recently, there are many studies focusing on deploying
BCFL on the edge servers. In [14], the authors design a
BCFL system running on at the edge with edge servers being
responsible for collecting and training the local models,
where a device selection mechanism and incentive scheme
are proposed to facilitate the performance of the crowdsens-
ing. Rehman et al. [25] devise a blockchain-based reputation-
aware ﬁne-gained FL system to enhance the trustworthiness
of devices in the MEC system. The work in [26] tries to
address the privacy protection issue for BCFL in MEC via
resisting a novel property inference attack, which attempts
to cause the unintended property leakage. Hu et al. [15]
deploy a BCFL framework on the MEC edge servers to

-0.2-0.100.10.2The Data Size of MEC Task0.00.20.40.60.81.0Bandwidth12345bcfl-0.2-0.100.10.2The Data Size of MEC Task02004006008001000CPU Cycle Frequencyf1f2f3f4f5fbcfl-0.2-0.100.10.2The Data Size of BCFL Task0.00.20.40.60.8Bandwidth12345bcfl-0.2-0.100.10.2The Data Size of BCFL Task02004006008001000CPU Cycle Frequencyf1f2f3f4f5fbcfl246810The Data Size of MEC Task0.51.01.52.02.5Latency/sTbcflTmeci246810The Data Size of BCFL Task0.51.01.52.02.5Latency/sTbcflTmeci-0.2-0.100.10.2The Data Size of MEC Task0.00.20.40.60.81.01.21.41.6Latency/sTmec1Tmec2Tmec3Tmec4Tmec5Tbcfl-0.2-0.100.10.2The Data Size of BCFL Task0.00.10.20.30.40.50.60.70.80.9Latency/sTmec1Tmec2Tmec3Tmec4Tmec5Tbcflfacilitate ﬁnishing mobile crowdsensing tasks, which aims
to achieve privacy preservation and incentive rationality at
the same time. Qu et al. [27] provide a simulation platform
for BCFL in the MEC environment to measure the quality
of local updates and conﬁgurations of IoT devices. From
these studies, it can be concluded that the development of
BCFL in MEC is promising, even though there are still some
challenges that should be tackled.

Speciﬁcally, resource allocation is one of the crucial but
open challenges. Since the resources of edge servers are
usually limited, it is essential to design a resource allocation
scheme for edge servers to provide satisfactory services for
both the MEC and the BCFL tasks with the minimum cost.
Wang et al. [28] design a joint resource allocation mechanism
in BCFL, which assists the participants in deciding the
proper resources for completing training and mining tasks.
In [29], a hybrid blockchain-assisted resource trading system
is designed to achieve the decentralization and efﬁciency
for FL in MEC. Li et al. [20] propose a BCFL framework to
tackle the security and privacy challenges of FL, where a
computing resource allocation mechanism for training and
mining is also designed by optimizing the upper bound
of the global loss function. One main vulnerability of this
scheme is that all participants are assumed to be homoge-
neous, which is clearly impractical in the mobile scenario.

In summary, none of the existing studies related to the
implementation of BCFL in MEC has ever addressed the re-
source allocation challenge between the MEC tasks and the
BCFL task. Because of the dual roles of edge servers in BCFL
and MEC, they have to simultaneously ﬁnish the upper-
layer BCFL task and provide MEC services for the lower-
layer mobile devices. To ﬁll this gap, we devise resource
allocation schemes for edge servers in the deployment of
BCFL at edge to guarantee the service quality to both sides
with the minimum cost.

7 CONCLUSION
In this paper, we are the ﬁrst to address the resource allo-
cation challenge for edge servers when they are required
to handle both the BCFL and MEC tasks. We formulate
the design of the resource allocation scheme into a convex,
multivariate optimization problem with multiple inequity
constraints, and then we design two algorithms based on
ADMM to solve it in both the homogeneous and hetero-
geneous scenarios. Solid theoretic analysis is conducted to
prove the validity of our proposed solutions, and numerous
experiments are carried out to evaluate the correctness and
effectiveness of the algorithms.

REFERENCES

[1] N. Abbas, Y. Zhang, A. Taherkordi, and T. Skeie, “Mobile edge
computing: A survey,” IEEE Internet of Things Journal, vol. 5, no. 1,
pp. 450–465, 2017.

[2] B. Liang, V. Wong, R. Schober, D. Ng, and L. Wang, “Mobile edge
computing,” Key technologies for 5G wireless systems, vol. 16, no. 3,
pp. 1397–1411, 2017.

[3] X. Sun and N. Ansari, “Edgeiot: Mobile edge computing for the
internet of things,” IEEE Communications Magazine, vol. 54, no. 12,
pp. 22–29, 2016.

[4] D. Sabella, A. Vaillant, P. Kuure, U. Rauschenbach, and F. Giust,
“Mobile-edge computing architecture: The role of mec in the
internet of things,” IEEE Consumer Electronics Magazine, vol. 5,
no. 4, pp. 84–91, 2016.

12

[5] A. H. Sodhro, Z. Luo, A. K. Sangaiah, and S. W. Baik, “Mobile
edge computing based qos optimization in medical healthcare ap-
plications,” International Journal of Information Management, vol. 45,
pp. 308–318, 2019.

[6] X. Li, X. Huang, C. Li, R. Yu, and L. Shu, “Edgecare: leveraging
edge computing for collaborative data management in mobile
healthcare systems,” IEEE Access, vol. 7, pp. 22 011–22 025, 2019.

[7] Y. Chen, Y. Zhang, S. Maharjan, M. Alam, and T. Wu, “Deep
learning for secure mobile edge computing in cyber-physical
transportation systems,” IEEE Network, vol. 33, no. 4, pp. 36–41,
2019.
J. Zhou, H.-N. Dai, and H. Wang, “Lightweight convolution neural
networks for mobile edge computing in transportation cyber
physical systems,” ACM Transactions on Intelligent Systems and
Technology (TIST), vol. 10, no. 6, pp. 1–20, 2019.

[8]

[9] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Ar-
cas, “Communication-efﬁcient learning of deep networks from
decentralized data,” in Artiﬁcial intelligence and statistics. PMLR,
2017, pp. 1273–1282.

[10] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman,
V. Ivanov, C. Kiddon, J. Koneˇcn `y, S. Mazzocchi, B. McMahan et al.,
“Towards federated learning at scale: System design,” Proceedings
of Machine Learning and Systems, vol. 1, pp. 374–388, 2019.

[11] Y. Zhao, J. Zhao, L. Jiang, R. Tan, D. Niyato, Z. Li, L. Lyu, and
Y. Liu, “Privacy-preserving blockchain-based federated learning
for iot devices,” IEEE Internet of Things Journal, vol. 8, no. 3, pp.
1817–1829, 2020.

[12] S. R. Pokhrel and J. Choi, “Federated learning with blockchain
for autonomous vehicles: Analysis and design challenges,” IEEE
Transactions on Communications, vol. 68, no. 8, pp. 4734–4746, 2020.
[13] Z. Wang and Q. Hu, “Blockchain-based federated learning: A
comprehensive survey,” arXiv preprint arXiv:2110.02182, 2021.
[14] Y. Zhao, J. Zhao, L. Jiang, R. Tan, and D. Niyato, “Mobile
edge computing, blockchain and reputation-based crowdsourc-
ing iot federated learning: A secure, decentralized and privacy-
preserving system,” 2020.

[15] Q. Hu, Z. Wang, M. Xu, and X. Cheng, “Blockchain and federated
edge learning for privacy-preserving mobile crowdsensing,” IEEE
Internet of Things Journal, 2021.

[16] S. Sardellitti, G. Scutari, and S. Barbarossa, “Joint optimization
of radio and computational resources for multicell mobile-edge
computing,” IEEE Transactions on Signal and Information Processing
over Networks, vol. 1, no. 2, pp. 89–103, 2015.

[17] J. Wang, L. Zhao, J. Liu, and N. Kato, “Smart resource allocation
for mobile edge computing: A deep reinforcement learning ap-
proach,” IEEE Transactions on emerging topics in computing, 2019.

[18] L. Wan, L. Sun, X. Kong, Y. Yuan, K. Sun, and F. Xia, “Task-
driven resource assignment in mobile edge computing exploiting
evolutionary computation,” IEEE Wireless Communications, vol. 26,
no. 6, pp. 94–101, 2019.

[19] N. Q. Hieu, T. T. Anh, N. C. Luong, D. Niyato, D. I. Kim, and
E. Elmroth, “Resource management for blockchain-enabled fed-
erated learning: A deep reinforcement learning approach,” arXiv
preprint arXiv:2004.04104, 2020.

[20] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and
H. V. Poor, “Blockchain assisted decentralized federated learning
(blade-ﬂ): Performance analysis and resource allocation,” arXiv
preprint arXiv:2101.06905, 2021.

[21] S. Boyd, N. Parikh, and E. Chu, Distributed optimization and statis-
tical learning via the alternating direction method of multipliers. Now
Publishers Inc, 2011.

[22] T. D. Burd and R. W. Brodersen, “Processor design for portable
systems,” Journal of VLSI signal processing systems for signal, image
and video technology, vol. 13, no. 2, pp. 203–221, 1996.

[23] B. He, M. Tao, and X. Yuan, “A splitting method for separable
convex programming,” IMA Journal of Numerical Analysis, vol. 35,
no. 1, pp. 394–426, 2015.

[24] Z. Xiong,

J. Kang, D. Niyato, P. Wang, and H. V. Poor,
“Cloud/edge computing service management in blockchain net-
works: Multi-leader multi-follower game-based admm for pric-
ing,” IEEE Transactions on Services computing, vol. 13, no. 2, pp.
356–367, 2019.

[25] M. H. ur Rehman, K. Salah, E. Damiani, and D. Svetinovic, “To-
wards blockchain-based reputation-aware federated learning,” in
IEEE INFOCOM 2020-IEEE Conference on Computer Communications
Workshops (INFOCOM WKSHPS).

IEEE, 2020, pp. 183–188.

13

[26] M. Shen, H. Wang, B. Zhang, L. Zhu, K. Xu, Q. Li, and X. Du,
“Exploiting unintended property leakage in blockchain-assisted
federated learning for intelligent edge computing,” IEEE Internet
of Things Journal, vol. 8, no. 4, pp. 2265–2275, 2020.

[27] G. Qu, N. Cui, H. Wu, R. Li, and Y. M. Ding, “Chainﬂ: A
simulation platform for joint federated learning and blockchain
in edge/cloud computing environments,” IEEE Transactions on
Industrial Informatics, 2021.

[28] Z. Wang, Q. Hu, R. Li, M. Xu, and Z. Xiong, “Incentive mechanism
design for joint resource allocation in blockchain-based federated
learning,” arXiv preprint arXiv:2202.10938, 2022.

[29] S. Fan, H. Zhang, Y. Zeng, and W. Cai, “Hybrid blockchain-
based resource trading system for federated learning in edge
computing,” IEEE Internet of Things Journal, vol. 8, no. 4, pp. 2252–
2264, 2020.

Zhilin Wang received his B.S. from Nanchang
University in 2020. He is currently pursuing his
Ph.D. degree of Computer and Information Sci-
ence In Indiana University-Purdue University In-
dianapolis (IUPUI). He is a Research Assistant
with IUPUI, and he is also a reviewer of 2022
IEEE International Conference on Communica-
tions (ICC) and IEEE Access. His research in-
terests include blockchain, federated learning,
edge computing, and Internet of Things (IoT).

Qin Hu received her Ph.D. degree in Computer
Science from the George Washington University
in 2019. She is currently an Assistant Professor
with the Department of Computer and Informa-
tion Science, Indiana University-Purdue Univer-
sity Indianapolis (IUPUI). She has served on the
Editorial Board of two journals, the Guest Editor
for two journals, the TPC/Publicity Co-chair for
several workshops, and the TPC Member for
several international conferences. Her research
interests include wireless and mobile security,

edge computing, blockchain, and crowdsensing.

Zehui Xiong is currently an Assistant Professor
in the Pillar of Information Systems Technology
and Design, Singapore University of Technol-
ogy and Design. He received the PhD degree
in Nanyang Technological University, Singapore.
His research interests include wireless com-
munications, network games and economics,
blockchain, and edge intelligence. He has pub-
lished more than 140 research papers in leading
journals and ﬂagship conferences and many of
them are ESI Highly Cited Papers. He has won
over 10 Best Paper Awards in international conferences and is listed in
the World’s Top 2% Scientists identiﬁed by Stanford University. He is now
serving as the editor or guest editor for many leading journals including
IEEE JSAC, TVT, IoTJ, TCCN, TNSE, ISJ, JAS.

