Gromit: Benchmarking the Performance and
Scalability of Blockchain Systems

Bulat Nasrulin, Martijn De Vos, Georgy Ishmaev, Johan Pouwelse
Delft University of Technology
{b.nasrulin, m.a.devos-1, g.ishmaev, j.a.pouwelse,}@tudelft.nl

2
2
0
2

g
u
A
4
2

]

C
D
.
s
c
[

1
v
4
5
2
1
1
.
8
0
2
2
:
v
i
X
r
a

of

Abstract—The

implementations

growing number

of
blockchain systems stands in stark contrast with still limited
research on a systematic comparison of performance character-
istics of these solutions. Such research is crucial for evaluating
fundamental trade-offs introduced by novel consensus protocols
and their implementations. These performance limitations are
commonly analyzed with ad-hoc benchmarking frameworks fo-
cused on the consensus algorithm of blockchain systems. How-
ever, comparative evaluations of design choices require macro-
benchmarks for uniform and comprehensive performance evalu-
ations of blockchains at the system level rather than performance
metrics of isolated components. To address this research gap, we
implement Gromit, a generic framework for analyzing blockchain
systems. Gromit treats each system under test as a transaction
fabric where clients issue transactions to validators. We use
Gromit to conduct the largest blockchain study to date, involving
seven representative systems with varying consensus models. We
determine the peak performance of these systems with a synthetic
workload in terms of transaction throughput and scalability and
show that transaction throughput does not scale with the number
of validators. We explore how robust the subjected systems
are against network delays and reveal that the performance of
permissoned blockchain is highly sensitive to network conditions.

Index Terms—benchmark, blockchain performance, repro-

ducibility, stress testing

I. INTRODUCTION

The rapid growth in the number of blockchain protocols in
the past few years has been boosted by the interest in crypto-
currencies, decentralized ﬁnance, and identity systems. The
solutions are mostly empirically driven, with direct economic
incentives stimulating engineering experiments. To date, there
are more than 700 different blockchain and distributed ledger
platforms offering native digital assets and products.Many of
these solutions deploy original families of consensus proto-
cols or signiﬁcant modiﬁcations of popular protocols, with
variations in scalability, performance, and decentralization
guarantees. These developments outpace systematization and
research on inherent trade-offs of different design choices [1].
The absence of benchmarking solutions for comprehensive
comparative analysis of various protocols is a particularly
problematic omission, given the cumulative marketcap of
1,468 trillion $ for these projects. More fundamentally, this
systematization gap hampers our ability to tackle the increas-
ing complexity of blockchain systems and make conscious
design choices in blockchain engineering. Developers of these

This work was funded by NWO/TKI grant BLOCK.2019.004

protocols often provide performance metrics of blockchain
solutions as declarative whitepapers that do not pass the
standards of peer review and reproducibility, calling into
question the reliability and objectivity of these measurements.
For instance, it is a common practice to provide performance
metrics of an isolated component and report them as a system-
wide performance metric [2]–[4]. This practice often leads to
false impressions of the end-to-end system performance.

Few available benchmarking solutions, such as Block-
bench [5] and Hyperledger Caliper [6] focus on narrow sets
of permissioned consensus protocols or DAG-based proto-
cols as DAGBENCH [7]. There is also a noticeable deﬁcit
of academic research in macro benchmarks for blockchain
systems. Existing studies are rather limited in scope either
focusing on speciﬁc protocols such as Hyperledger [8], [9] or
Ripple [10], or reusing Hyperldeger Caliper [11] and Block-
bench [5]. BCTMark is one of the few comparative benchmark
studies which compares three different protocols, including
permissionless Ethereum blockchain [12]. The authors in this
study highlight the necessity to extend the comparison set
and include more metrics such as partition tolerance. The
most recent systematic survey on performance evaluation of
blockchain systems demonstrates that available comparative
studies are rather limited in scope both in terms of compared
systems and depth of analysis, focusing on isolated layers
of blockchain systems [1]. To address this research gap, we
design a benchmark that is comprehensive in scope, allowing
us to stress-test the system under different network conditions.
We introduce Gromit, a generic benchmarking framework
that allows a performance evaluation of any blockchain so-
lution. Gromit treats each system under test as a transaction
fabric, which means a transaction processing system where a
group of peers continuously reach a consensus on transactions
submitted by clients. Gromit analyses performance metrics re-
lated to transactional data, speciﬁcally throughput and latency.
As we will show, this data alone can reveal the limitations of
various aspects of the system.

the
We show the applicability of Gromit and conduct
largest blockchain benchmarking study to date. Our bench-
mark involves seven major blockchain solutions with different
consensus algorithms. We determine the peak performance
of these blockchain systems for different numbers of peers
and without any modiﬁcations to the source code. A key
ﬁnding is that the performance for most evaluated blockchain
systems degrades when the number of peers increases. We also

 
 
 
 
 
 
Fig. 1: Our abstraction model of blockchain system as a transaction fabric, comprising four stages. The dotted line highlights
the focus of most whitepaper benchmarks.

reveal the effect of network delays and an increase in system
load on the distribution of end-to-end transaction latencies,
yielding valuable insights into the real-life bottlenecks of
underlying consensus mechanisms. While most performance
bottleneck research focuses on the consensus layer, a system-
wide stress test can reveal bottlenecks in other layers, e.g., in
the persistence layer.

The contribution of this work is two-fold:
1) We design and implement Gromit, a benchmarking
framework that enables an analysis of any blockchain
system (Section III).

2) We conduct

the largest blockchain benchmark to
date, measuring the performance of seven prominent
blockchain systems (Section V).

II. BLOCKCHAIN AS A TRANSACTION FABRIC
In this work, we view a blockchain system as a transaction
fabric. Figure 1 visualizes this abstraction model, which
illustrates a typical transaction life cycle.

In our model, we distinguish between clients and peers.
Clients are instances that create transactions and send them to
peers. An example of a client is a light wallet or lightweight
nodes in a Bitcoin network. Peers in our model are respon-
sible for processing and validating transactions in a shared,
decentralized network. Thus, we call them validator peers. In
some blockchain systems, they are also referred to as miners.

A. Transaction Life Cycle Model

1) Transaction Creation: A transaction contains logic that
modiﬁes the system state, e.g., by transferring an asset to
another account. Each transaction is cryptographically signed
with the private key of the issuing client to ensure authenticity.
Blockchain solutions usually provide Wallet API’s for clients
to submit their transactions, e.g., with an RPC endpoint or
REST endpoint.

2) Transaction Sharing: Blockchain systems

employ
complex transaction sharing mechanisms. Permissionless
blockchains typically use a global gossip protocol to share
transactions over a structured or unstructured overlay. Permis-
soned blockchains are deployed in a more controlled network
environment and, as a result, might share transactions using a
broadcast algorithm.

The transactions are stored in a datastore, often called a
transaction pool (TxPool). The transaction pool is a temporary
store used to queue or preprocess transactions before the
network validates them.

3) Transaction Settlement: A consensus algorithm is a
crucial part of the blockchain system as a mechanism for
achieving security and liveness [13]. The intermediate outcome
of the consensus process in blockchain systems is a set of
valid transactions. Valid transactions are stored in a tamper-
proof distributed ledger, a replicated data structure maintained
by validators. The system discards invalid transactions.

Blockchain solutions typically bundle valid transactions in
blocks,
interlinked in a hash chain, and stored in a local
database (Block DB). Each block in a hash chain contains the
cryptographic hash of the previous block, making illegitimate
modiﬁcations of the chain detectable. Some blockchain-like
systems adopt a different organization of the distributed ledger,
e.g., by maintaining a Directed Acyclic Graph (DAG) [7].

4) Transaction Status Response: After the transaction is
settled the client waits for a transaction approval (or rejection),
received from validator peers. Some blockchains also include a
proof in the response that proves that a transaction is ﬁnalized.

B. Transaction Performance Indicators

Our approach is to analyze the performance of current
blockchain solutions through transaction benchmarking. The
speed at which a blockchain system processes transactions is
a deﬁning metric for blockchains. High transaction latencies
directly impact end-users experience, e.g., the relatively high
ﬁnalization times of Bitcoin transactions (10 minutes) make
it unsuitable for interactive trade [14]. We include all stages
of the transaction life cycle in our measurements, rather than
focusing on the performance of the consensus layer only.

We obtain insights into the limitation of blockchain systems
by measuring peak performance and associated transaction la-
tencies. We also measure performance under different network
conditions, such as changes in the geographical distribution of
the overlay network. Our experiments in Section V highlight
that these metrics can reveal performance bottlenecks and act
as a guideline for optimization efforts.

Fig. 2: Architectural overview and process ﬂow of Gromit, our benchmarking framework for blockchain systems.

III. GROMIT: A GENERIC FRAMEWORK FOR BLOCKCHAIN
BENCHMARKING

We design and implement Gromit, the ﬁrst generic frame-
work for blockchain benchmarking. Gromit is designed for
quick experiment iterations, utilizing a domain-speciﬁc lan-
guage to devise experiments. Our framework, implemented
in Python, is based on the abstraction model discussed in
Section II. To encourage the adoption of Gromit, its source
code is published on Github.1 Gromit provides developers
and researchers with the necessary tools to design benchmarks
targeting blockchain performance. Gromit spawns client and
validator processes on remote servers and coordinates interac-
tions between running processes, e.g., transaction issuing.

A. Experiment Flow

Figure 2 shows the architecture of the framework and the

ﬂow of an experiment. This diagram is described next.

Experiment Setup. Before an experiment starts, it spawns
a dedicated process, called the orchestrator, that setups the
environment on remote servers and handles cross-server com-
munication. The orchestrator reads the conﬁguration ﬁle as-
sociated with the experiment. This ﬁle speciﬁes the network
addresses of remote servers and the number of clients and
validators that should be started (step 1 ). The orchestrator
then copies the source code and all necessary experiment ﬁles
to the remote servers using rsync. Next, the coordinator
setups the environment on the speciﬁed servers over an SSH
connection (step 2 ). This step includes the installation of
required system packages and the generation of a genesis ﬁle.
This ﬁle describes the initial state of the blockchain system and
can pre-load speciﬁc accounts with assets. The orchestrator
then starts a instance for each client or validator on the remote
server and assigns an identiﬁer to each running instance. The
scenario manager, part of the logic of a Gromit instance, parses
a provided scenario ﬁle and starts the experiment (step 3 ).
Scenario Files. A user describes the actions performed dur-
ing an experiment with a scenario ﬁle. The scenario manager
parses this ﬁle and schedules actions using the asyncio
library.

1See https://github.com/grimadas/gromit

i n i t b l o c k c h a i n c o n f i g {1 −10}
@8
s t a r t v a l i d a t o r {1 −10}
@12
s t a r t
@20
@80
s t a r t
@100 s t o p

c l i e n t {11 −20}
c r e a t i n g t r a n s a c t i o n s {11 −20}

Listing 1: An example of a scenario ﬁle in Gromit, describing
an experiment with 10 clients and validators.

An example scenario ﬁle associated with a simple
blockchain benchmark is given in listing 1. Actions and their
timestamps are explicitly denoted. A user can schedule an
action to execute on a subset of all validators. Scenario ﬁles
can be machine-generated and are a ﬂexible approach to
devising and conducting experiments.

Experiment Runner. The experiment runner spawns val-
idators and clients as a subprocess during each experiment run
(step 4 ). Clients interact with validators through the exposed
API endpoint. We simulate a client in Gromit as a proce-
dure that issues transactions. During an experiment, Gromit
instances can share data over a TCP connection through a
message broker. We use the message broker functionality to
share the credentials of pre-deﬁned blockchain accounts with
clients. Gromit contains tools to gracefully terminate a running
experiment, e.g., when a particular condition is not met.

Gromit also provides utilities to track system resource
usage. Gromit instances monitor CPU, memory, disk, and
network usage using the procfs library (step 5 ). These
metrics allow us to estimate the system resource usage of
blockchain systems. Developers can easily extend Gromit to
monitor speciﬁc metrics, for example, the number of inbound
network messages for a particular blockchain system.

Collecting Experiment Results. When the experiment
ends, the orchestrator copies all generated artifacts from the
remote nodes using rsync (step 6 ). These artifacts include
the data generated by the blockchain systems and the data
output by the Gromit instances, e.g., monitoring statistics. This
data is parsed by the orchestrator (step 7 ) and generates
human-readable graphs. Finally, the data is stored in a database
(step 8 ) and is ready for analysis by the user (step 9 ).

B. Integrating Blockchain Systems into Gromit

is modular,

To show practicality of Gromit, we have integrated seven
prominent blockchain systems into it. The integration requires
no change in the source code of the blockchain systems. This
allows to benchmark the blockchain system as close to the
deployed systems as possible.
The design of Gromit

and developers
can implement modules that enrich an experiment with
for example, bandwidth monitoring.
more functionality,
Integration of
a devel-
oper to subclass the BlockchainModule and to imple-
the init_configuration, start_validator,
ment
stop_validator, and parse_ledger methods. Gromit
enables developers to specify custom network topologies to
connect validators among each other. We refer readers who
wish to integrate a particular blockchain system to the Gromit
documentation.

a blockchain system requires

Besides supporting blockchain systems, Gromit has support
for generic experiments with distributed systems. For example,
we have used Gromit to conduct experiments with peer-to-peer
protocols on custom infrastructure.

C. Transaction Analysis

A workload during a Gromit benchmark consists of trans-
actions issued by one or more clients. All transactions can
be submitted to one validator peer or can be evenly spread
among the peers. In line with related work, we mainly
gauge blockchain performance using two metrics, transaction
throughput, and transaction latency. Speciﬁcally, we consider
the peak transaction throughput, which is the maximum rate
at which the system can process transactions before getting
congested. Second, we analyze the latency of transactions,
which is the time between submitting a transaction and its
irreversible inclusion in the distributed ledger.

temporal

We store the timestamp at which a client has submitted
the transaction to a validator to determine transaction latency.
However, determining the ﬁnalization time of transactions can
be complicated since some blockchain systems do not expose
granular,
information. We use the following two
approaches to determine the latency of individual transactions.
Our ﬁrst approach is to inspect the resulting distributed ledger
after all
transactions have been issued (this logic should
be part of the parse_ledger method). We then compute
transaction latency based on timestamps included in ledger
data elements (e.g., blocks). However, this approach is useful
only if the blockchain system annotates data elements in the
distributed ledger with a timestamp. The second approach
suggests clients periodically poll the blockchain system to
determine if a transaction has been conﬁrmed.

IV. BLOCKCHAIN CONSENSUS MODEL

Since the introduction of Bitcoin in 2008, there have been
many proposals for new blockchains and consensus mech-
anisms. Some proposals have materialized into operational
systems, whereas other ones are only theoretically analyzed.

The proliferation of blockchain solutions makes it infeasible
to conduct a benchmarking study with all available systems.
Based on taxonomy of different consensus mechanisms used
in blockchain systems [13] we consider seven prominent con-
sensus models and select a representative blockchain system
for each consensus model. Our selection process is based on
the economic magnitude, adoption, maturity of the system,
and the protocol’s academic signiﬁcance. Table I lists the
representative system for each considered consensus model.
For each system, we also show the evaluated version of the
software, the principle underpinning each consensus model,
how the group of validators is formed, and the year in which
the ﬁrst software commit has been made (as an indicator
of matureness). We are aware of state-of-the-art blockchains
that use techniques such as layer-one scaling to improve
throughput, e.g., OmniLedger [22] and RapidChain [23], or
layer-two scaling solutions [24]. However, the primary focus of
our benchmarking study is on deployed layer-one blockchain
systems. We now elaborate on each consensus model and refer
the reader to [13] for a more extensive overview of blockchain
consensus models.

Proof-of-Work (PoW) is the oldest consensus mechanisms
designed explicitly for blockchain systems. PoW is used in
blockchains such as Bitcoin [25] and Ethereum [15] and to
date remains a standard approach to build open blockchain
systems. PoW scales well in the number of participants: the
Bitcoin network has over 11’000 operational miners. However,
PoW is known to consume large amounts of energy, and the
achievable transaction throughput is theoretically limited to
around 60 transactions per second [26].

Proof-of-Stake (PoS), initially proposed by the Peercoin
cryptocurrency [27], is an alternative consensus mechanism
that addresses the excessive resource usage by PoW. PoS is
typically based on a random, periodic selection of a leader.
This selection process is weighted by the participants’ stake
in the system, e.g., by the number of assets owned, or by
the age of possessed assets. Algorand is one of the most
prominent PoS-based blockchains and leverages Veriﬁable
Random Functions (VRFs) for the leader election process [16].
Delegated Proof-of-Stake (dPoS) is a consensus mecha-
nism where stakeholders elect a group of delegates through
voting. Voting decision can, for example, be based on com-
munity engagement. Since the set of validators participating
in consensus remains relatively small compared to PoS-based
solutions, dPoS-based consensus has in theory a better poten-
tial to scale. BitShares is one of the most mature blockchains
using dPoS consensus [17].

Practical Byzantine Fault Tolerance (PBFT) is a consen-
sus algorithm introduced in 1999 [28]. PBFT is speciﬁcally de-
signed for networks with static and pre-approved membership
and has been revised for adoption in blockchain environments.
Recent advancements have resulted in HotStuff [19], a con-
sensus protocol based on PBFT that reduces communication
overhead and increases throughput. HotStuff is at the core
of Diem, a permissioned distributed ledger maintained by a
consortium led by Facebook [18].

Blockchain System

Consensus Model

Consensus Principle

Ethereum
(v1.9.24) [15]
Algorand (v2.3.0) [16]

Proof of Stake (PoS)

Proof of Work (PoW)

Resource-based lottery

Validator Group
Formation
Resource mining

First
(year)
2013

commit

BitShares (v5.0.0) [17] Delegated Proof of Stake (dPoS)

Diem (v1.1.0) [18]

Stellar (v15.1.0) [3]

Practical Byzantine Fault Tolerance
(PBFT), based on HotStuff [19]
Federated
(FBA)
Crash-tolerant
based on Raft [21]

Agreement

Byzantine

consensus

(CFT),

Hyperledger
(v1.4.9) [20]
Avalanche (v1.1.1) [2] Meta-Stable Consensus (MSC)

Fabric

Random selection of
leaders
Rotating leader

Leader-based

Quorum Intersection

Leader-based

Network subsampling

Stake-based enrol-
ment
Election by stake-
holders
Enrolment by an
authority
User-deﬁned quo-
rums
Enrolment by an
authority
Stake-based enrol-
ment

2019

2015

2019

2014

2016

2020

TABLE I: The seven selected blockchain systems and consensus models analysed in this work.

Federated BFT (FBFT) is a consensus model that dis-
tinguishes itself from the approaches mentioned earlier by
having validators explicitly specifying trust relations. Stellar
is one of the ﬁrst systems to adopt FBFT consensus [3]. The
Stellar Consensus Protocol (SCP) leverages a federated voting
approach in which each validator votes on statements while
ensuring that no two members of an overlapping quorum can
conﬁrm contradicting statements.

Crash-tolerant Consensus (CFT) is a consensus approach
widely used to achieve fault
tolerance, e.g., by Apache
Kafka [29]. Unlike PBFT, CFT is not resistant against arbitrary
(Byzantine) behaviour but can withstand crash-stop failures of
participants. Notable algorithms achieving CFT are Paxos [30]
and Raft [21]. Hyperledger Fabric, one of the most prominent
industrial blockchains, is currently using Raft [20].

Metastable Consensus (MSC) is a family of consensus
algorithms that leverage network subsampling techniques to
determine the validity of a transaction. The idea is to repeat-
edly sample random validators in the network and to steer
correct nodes to a common decision. Avalanche is one of the
most mature blockchain solutions to leverage MSC consensus
and maintains a DAG data structure to store transactions [2].

V. BLOCKCHAIN PERFORMANCE EVALUATION

We conduct a diverse set of experiments with Gromit to
reveal the performance characteristics and limitations of seven
blockchain solutions. To the best of our knowledge, We are
the ﬁrst to perform a large-scale performance benchmark of
blockchain solutions.

Our experiments answer the following questions: How does
the increase in the number of validators affect the system per-
formance? What is the peak performance of each system under
a heavy system load? What is the impact of a network delay
on systems’ performance? How consistent are our performance
results compared to previously reported values?

Throughout the section we use two variables for our exper-
iments: n indicates the number of validators and λ signiﬁes
the transaction throughput.

A. Setup and Transaction Workload

All experiments are conducted on four HPE DL385 Gen10
located within the same data centre and inter-
servers,
connected with 10GB Ethernet links. Each server is equipped
with 128 AMD EPYC 7452 CPUs, has 512GB of DDR4
memory, and runs Debian 10. During our experiments, we
deploy each blockchain system with its source code unmod-
iﬁed. For each system, we use the default settings provided
by the systems. Each experiment starts with only the genesis
block included in the blockchain. We use a random network
topology where each validator is connected to 10 other random
validators.

We use Gromit

to subject each system to a synthetic
transaction workload, issued by up to 64 clients. To ensure
an equal load on each validator, a client submits transactions
to the validator with ID i ≡ c (mod n) where c is the ID of the
client and n the total number of validators. Each transaction is
submitted to exactly one validator. Transactions are submitted
during a two-minute period, after which we wait an additional
minute for all transaction to be ﬁnalized.

We use simple asset transfers as a performance baseline.
In our workload, a transaction issued by a client involves an
asset transfer of a small, ﬁxed amount to another account;
the client counterparty is ﬁxed throughout the experiment.
We ensure that each client has sufﬁcient funds to spend
during the experiment. For Ethereum and Hyperledger Fabric,
transactions involve the transfer of an ERC20 token.

B. Determining Peak Transaction Throughput

To determine peak transaction throughput, we gradually
increase the system transaction rate in steps of 100 transactions
per second (tx/s). Based on the reported statistics by Gromit,
we estimate the peak transaction load that each system is still
able to process during a sustained period. If the system has
any unconﬁrmed transactions after our two-minute period,
we consider the system as “saturated”. We evaluate the peak
throughput of each system with an increasing number of
validators (n). We provide each system with an equal amount
of resources and ensure that the resource usage of evaluated

(a) Peak transaction throughput

(b) Transaction latency

Fig. 3: The peak throughput and transaction latencies of evaluated blockchain systems with the number of validators.

systems (CPU power, disk space, and memory) does not
exceed the available resources. Due to excessive resource
usage of the Stellar software, we are only able to run Stellar
with up to 64 validators. We run each experiment at least
ﬁve times and average all results. Appropriate graphs are
annotated with 95% conﬁdence interval markers.

Finding 1. Adding validators does not have a signiﬁcant
positive effect on the achievable peak transaction throughput
of the evaluated systems.

Figure 3 shows the result of our scalability experiment as n
grows, in terms of peak transaction throughput and transaction
latency. Figure 3a shows the peak transaction throughput of
evaluated systems (with a horizontal and vertical log-axis).
We notice that none of the evaluated systems can process over
1’000 tx/s with n = 128. In general, the transaction throughput
of most of the systems is capped between 500 and 1’500 tx/s.
Except for Ethereum, the peak transaction of all blockchains
is decreasing as n increases. Speciﬁcally, Hyperledger Fabric
shows a severe degradation in performance when n > 8, and
is just capable of processing 2 to 4 tx/s with n = 128. We
believe that this is caused by the underlying consensus model
of Hyperledger Fabric, Raft, which does not scale well with
the number of validators [21]. Of all systems, Ethereum has the
lowest transaction throughput (around 10-20 tx/s), yet manages
to keep stable throughput with the increase in the number
of validators. Peak throughput of Avalanche increases up to
n = 64, but then degrades for n = 128.

Finding 2. For Avalanche, BitShares and Hyperledger
Fabric, we observe signiﬁcant discrepancies between the peak
throughput found by us and previously reported values.

The previous experiment estimates the peak throughput of
blockchain systems, without modiﬁcation to the source code
and under default settings. We now compare the performance
results with values reported by other literature. The perfor-
mance, i.e. peak throughput, is typically determined through
an evaluation by system developers themselves. For Stellar,
we could not ﬁnd reliable benchmarking results, making this
work the ﬁrst benchmarking study of Stellar. We are interested
to see if there are signiﬁcant inconsistencies with performance
metrics reported by these studies.

Our performance results are comparable with results re-
ported for Ethereum (4-40 tx/s [31]), Algorand (880 tx/s [16])
and Diem (200-1’000 tx/s [32]). However, we notice that
previously reported values for some systems are signiﬁcantly
higher than our ﬁndings, speciﬁcally for Avalanche (7’000 tx/s,
26x higher [2]), BitShares (3’300 tx/s, 3x higher [33]) and
Hyperledger Fabric (3’500 tx/s, 2 times higher [20]). For each
system, we now explain these inconsistencies with additional
experiments and analysis.

Avalanche. The relative low throughput of Avalanche sur-
prises us and warrants further performance analysis. Since
we noticed that each validator node is fully utilizing a CPU
core, even with n = 4 and 32 tx/s, we perform a CPU
analysis of deployed validators using the pprof proﬁler.
We ﬁnd that around 60% of CPU time is spent on hash
computations using the argon2 algorithm [34]. This CPU
consumption originates from the API provided by Avalanche
validators. Speciﬁcally, each validator maintains a keystore
with credentials that is managed by end users; interactions
with that keystore, e.g., accessing a private key, requires the
user to include the password hash in the request. Consequently,
many parallel requests to the API by clients cause severe
performance degradations.

To analyse the impact of password hashing, we recompile

48163264128Number of validators100101102103Peak Throughput(tx/s)48163264128Number of validators02040Transaction Latency (s)Fig. 4: The distribution of transaction latencies for systems, under peak load (λ tx/s) and moderate load ( λ

2 tx/s) (n = 64).

Avalanche with this hash veriﬁcation disabled and re-run our
experiment. We do not observe a signiﬁcant increase in trans-
action throughput. However, this reveals another performance
bottleneck, originating from the veriﬁcation of transactions,
consuming around 90% of CPU time. Since Avalanche trans-
actions are linked in a DAG structure,
incoming transac-
tions require the validation of parent transactions, which is
a resource-intensive, recursive operation. We believe this can
be addressed with further engineering efforts, e.g., queueing
the veriﬁcation of transactions.

BitShares. We further analyse the reported throughput of
BitShares and found that
the peak transaction throughput
(3’300 tx/s) is not an accurate performance indicator since
the sustained throughput throughout the experiment is only
around 450 tx/s. Speciﬁcally, the achievable throughput of
the BitShares consensus algorithm seems to be predicated by
the speed of the slowest consensus participants in terms of
connectivity and CPU resources. When operating BitShares
in a heterogeneous environment, this can result in signiﬁcant
deviations in transaction throughput.

Hyperledger Fabric. We further analyse the results re-
ported in the work of Androulaki et al. [20] and Block-
bench [5]. We ﬁnd that their work evaluates an early imple-
mentation of Hyperledger Fabric using a different consensus
model (Zookeeper or PBFT). As such, these results are not
directly comparable.

We also present the following two reasons to explain the
discrepancies in reported and observed throughput numbers:

1) Experimental Software vs Production. Many of the
throughput numbers reported by system developers are
extracted using a premature or even incomplete software
implementation. As such, we argue that the values found
by our experiments are a more accurate reﬂection of
the achievable throughput in a production environment.
Additionally, we noticed that some solutions (Diem and
Stellar) have built-in measures that artiﬁcially lower the
achievable throughput, likely to ensure safety properties
or to prevent attacks in a production environment. A

similar insights were observed in [35].

2) Client-Validator Interaction. In our experiments, clients
submit transactions to the API exposed by the system,
whereas other studies might directly inject transactions
in the validator process. API-based interaction adds
additional overhead as the request needs to be pro-
cessed, and this approach therefore is likely to lower the
peak throughput of the system. However, this approach
resembles how users interact with validators when a
blockchain system is Internet-deployed.

C. Transaction Latency

Finding 3. For all evaluated systems,

the average
transaction latency under peak load is largely independent of
the number of validators.

Figure 3b shows the average transaction latency under
peak load, as n increases. Except for Ethereum, the average
transaction latency of evaluated systems is around or below
ten seconds. The variance between different runs is relatively
low, except for Ethereum. In general, the average transaction
latency increases as the number of validators grows. The
consensus algorithm underlying BitShares, Algorand and
Stellar progresses in ﬁve-second rounds, theoretically resulting
in an average transaction latency of 2.5 seconds. We see that
the transaction latency of Algorand increases from 5 seconds
for n = 4 to 15 seconds with n = 128, suggesting that
consensus rounds take longer to complete. For BitShares and
Stellar, this increase is less pronounced.

transaction latencies

Finding 4. The variance of
for
Algorand and Diem increases signiﬁcantly under peak load,
compared to a moderate load. However,
the transaction
latencies of BitShares and Ethereum are largerly indifferent
towards the system load.

We visualize the distribution of transaction latencies for
each system to explore further the effects of increasing the
system load on the transaction latency. We consider both

Fig. 5: Transaction latencies without network modiﬁcations and in a geo-distributed setting (λ = 64 tx/s, n = 32). The
transaction latencies of Ethereum are shown in the right plot.

peak and moderate loads, the latter being deﬁned as half the
determined peak load. Figure 4 shows this distribution in a
violin plot. We observe that Algorand, BitShares, Hyperledger
Fabric, and Stellar transaction latencies are roughly uniformly
distributed under moderate load. These systems adopt a round-
based consensus approach, with a target of around ﬁve sec-
onds for Algorand, BitShares, and Stellar, and one second
for Hyperledger Fabric. For these systems, most transactions
are usually conﬁrmed within the current or next consensus
round relative to transaction submission. The distribution of
transaction latencies transforms as the system is subjected to
a peak load. Figure 4 shows that the ﬁnalization of Algorand
transactions is being deferred to later rounds: 6% of Algorand
transactions have a transaction latency above 20 seconds. This
effect is less pronounced for BitShares and Stellar.

Increasing the system load impacts the latency distribution
of Diem transactions. Further investigation reveals that the
round duration in Diem adjusts to the system load. As more
validators join the network and as more transactions are
submitted to Diem validators, the round duration increases
to ensure transactions can be processed on time. Nonetheless,
30% of all issued transactions in Diem are conﬁrmed only after
10 seconds under peak load, whereas the system can handle all
submitted transactions within 7 seconds under moderate load.

D. Impact of Network Delays

Finding 5. Adding network delays has a minimal effect on
the transaction latencies of Algorand and Stellar. However,
Avalanche and Diem are extremely sensitive to network delays.

Finally, we modify the network settings using the netem
Linux kernel module2 and measure the impact of a network
delay on the transaction latency for integrated blockchain
systems. This experiment reveals how different blockchain
systems are sensitive to deployment in geo-distributed settings.
To replicate realistic network delays, we extract ping statistics
for 32 cities worldwide from WonderNetwork [36] and assign
each validator to a city in a round-robin fashion.

2TC documentation: https://www.linux.org/docs/man8/tc-netem.html

Figure 5 shows the distribution of transaction latencies of
systems deployed in one data center with unmodiﬁed network
settings and when deployed with geo-distributed settings.
For each experiment run, we use 32 validators and ﬁx the
transaction load to 64 tx/s, which all evaluated systems can
handle without congestion. Except for Algorand and Stellar,
network delays visibly impact transaction latencies.

The effect is the most pronounced for the permissoned
systems Diem and Hyperledger Fabric. This suggests that
these systems can only operate in controlled/closed network
environments, as any change in the network signiﬁcantly
affects the performance. We also observe a signiﬁcant impact
of network conditions on Avalance. This is due to the poll-
based nature of metastable consensus. The poll rounds are
visible in Figure 5.

For BitShares, we observe that validators occasionally fail
to produce a block with higher network latency. For Diem,
consensus progress stales a few seconds after starting the
experiment, and rounds are timing out without conﬁrming any
transaction, violating system liveness.

E. Network and CPU Utilization

Finding 6. Algorand, Stellar, Diem show high network
utilization under idle load. Ethereum, Stellar and Diem
consume signiﬁcant CPU resources under idle load.

We track the total network usage (inbound and outbound
trafﬁc) and average CPU utilization for each system while
increasing λ and ﬁxing n to 32. To obtain insights into
the system under idle load, we also run blockchain systems
with λ = 0 tx/s. Figure 6a shows that the network usage
per validator quickly grows for Avalanche and Stellar as the
transaction load increases. For λ = 256 tx/s, both Avalanche
and Stellar use over 800 MB of network trafﬁc per validator
process. BitShares is the most network-efﬁcient, using only
80 MB per validator for λ = 256 tx/s. We also observe
that Algorand, Diem, and Stellar show 10x to 100x more
bandwidth consumption under no transaction load compared
to other systems.

(a) Network usage

(b) CPU utilization

Fig. 6: The resource utilization of the evaluated systems with increase in transaction load (n = 32).

(a) Network usage

(b) CPU utilization

Fig. 7: The network and CPU usage for evaluated systems, when increasing the number of validators (λ = 32 tx/s).

Figure 6b shows the average CPU utilization when in-
creasing λ. The mining process of Ethereum is continuously
utilizing a single CPU. Under λ = 256 tx/s, BitShares and
Hyperledger Fabric are the most CPU-efﬁcient compared to
other systems. Our synthetic workload results are consistent
with real-world observations of liveness issues for Avalanche
and Stellar [37], [38].

Figure 7a shows that Diem is using signiﬁcant network
resources for n = 4: 970 MB per validator process. This
number decreases quickly when n increases. We explain
this behavior by the self-adjusting round times of the Diem
consensus mechanism: as n increases, rounds take longer to
complete, lowering the bandwidth usage. For Stellar, we see
the opposite effect: network usage becomes signiﬁcant for
n = 128. Although Stellar cannot process transactions for
n = 128, we report its resource usage nonetheless. Inspection
of Stellar logs reveals that validators lose track of consensus,
clogging the network with resynchronization messages.

Figure 7b shows how CPU utilization behaves when increas-
ing n. BitShares is the most CPU-efﬁcient for all evaluated val-
ues of n. The CPU load of validators decreases for Avalanche
and Diem as n increases. Since we ﬁx the transaction load,
adding more validators decreases the individual load.

VI. RELATED WORK

Blockchain benchmarking and its associated challenges has
received attention from other researchers. Fan et al. present an
extensive survey outlining methods for evaluating blockchain
performance [1]. The work of Wang and Ye describes bench-
marking tools and consensus mechanisms and outlines tech-
niques to improve the throughput of blockchains [39]. The
authors of these studies summarize work on blockchain perfor-
mance but do not conduct benchmarking studies themselves.
Popular blockchain benchmarking tools are Blockbench [5],
Hyperledger Caliper [6], and DAGBench [7]. Blockbench,
introduced in 2017, is the earliest benchmarking framework
for blockchain and is speciﬁcally designed to evaluate permis-
sioned blockchains [5]. Blockbench measures the performance
of components commonly found in blockchains, e.g., the trans-
action execution engine). The authors of Blockbench evaluate
the performance of Hyperledger Fabric, Ethereum, and Parity.
Hyperledger Caliper is a benchmarking tool for the perfor-
mance evaluation of speciﬁc systems with a set of pre-deﬁned
use cases [6]. Hyperledger Caliper primarily supports projects
by the Hyperledger Foundation. DAGBench is a benchmarking
tool for DAG-based blockchains [7]. The authors evaluate
three popular DAG-based blockchain implementations [40]:
IOTA, Nano, and Byteball. However, DAGBench does not

032256Transaction load (tx/s)0.00.51.01.52.02.53.03.5CPU Utilization AlgorandAvalancheBitsharesDiemEthereumHyperledger FabricStellarSystem032256Transaction load (tx/s)05001000Network Usage per  Validator (MB)032256Transaction load (tx/s)0123CPU Utilization 432128Number of validators05001000Network Usage per  Validator (MB)432128Number of validators0.00.51.01.5CPU Utilization [23] M. Zamani, M. Movahedi, and M. Raykova, “Rapidchain: Scaling
blockchain via full sharding,” in Proceedings of the 2018 ACM SIGSAC
Conference on Computer and Communications Security, 2018, pp. 931–
948.

[24] L. Gudgeon et al., “Sok: Layer-two blockchain protocols,” in Inter-
national Conference on Financial Cryptography and Data Security.
Springer, 2020, pp. 201–226.

[25] S. Nakamoto, “Bitcoin: A peer-to-peer electronic cash system,”

Manubot, Tech. Rep., 2019.

[26] A. Gervais et al., “On the security and performance of proof of work

blockchains,” in SIGSAC, 2016.

[27] S. King and S. Nadal, “Peercoin–secure & sustainable cryptocoin,” Aug-
2012 [Online]. Available: https://peercoin. net/whitepaper (), 2012.
[28] M. Castro, B. Liskov et al., “Practical byzantine fault tolerance,” in

OSDI, 1999.

[29] “Apache Kafka,” https://kafka.apache.org, 2017.
[30] L. Lamport et al., “Paxos made simple,” ACM Sigact News, 2001.
[31] S. Pongnumkul et al., “Performance analysis of private blockchain

platforms in varying workloads,” in ICCCN.

IEEE, 2017.

[32] J. Zhang et al., “Performance analysis of the libra blockchain: An

experimental study,” in HotICN.

IEEE, 2019.

[33] “BitShares Industrial Performance and Scalability,” https://bitshareshub.

io/industrial-performance-and-scalability/, 2017.

for password hashing and other applications,” in EuroS&P.

[34] A. Biryukov et al., “Argon2: new generation of memory-hard functions
IEEE, 2016.
[35] J. A. Chacko, R. Mayer, and H.-A. Jacobsen, “Why do my blockchain
transactions fail? a study of hyperledger fabric,” in SIGMOD’21, 2021.
[36] WonderNetwork, “Global ping statistics,” https://wondernetwork.com/

pings, accessed: 2022-05-12.

[37] P. O’Grady. Preliminary analysis of the invalid minting bug. Accessed:
[Online]. Available: https://medium.com/avalancheavax/

2022-05-12.
preliminary-analysis-of-the-invalid-minting-bug-bee940cbd9e9

[38] M. McSweeney, “Stellar hit by transaction issues, developers say
’network is still online’ despite node outage,” The Block, 2021,
accessed: 2022-05-12. [Online]. Available: https://www.theblockcrypto.
com/linked/100649/stellar-network-stoppage-developers-investigation

[39] R. Wang et al., “Performance benchmarking and optimization for

blockchain systems: A survey,” in ICBC. Springer, 2019.

[40] H. Pervez et al., “A comparative analysis of dag-based blockchain

architectures,” in ICOSST.

IEEE, 2018, pp. 27–34.

allow for a comparison of other blockchains. The authors of
BCTMark [12]. present an Ethereum performance evaluation.

VII. CONCLUSION

We have presented Gromit, a generic benchmarking frame-
work for blockchain solutions. By treating each blockchain
system as a transaction fabric, our framework enables any
blockchain system’s integration, benchmarking, and perfor-
mance analysis. We leverage the functionalities of Gromit and
conduct the largest blockchain benchmark to date, involving
seven prominent blockchain systems. Our main ﬁnding is that
none of the evaluated solutions can handle beyond 1’000
transactions per second as the number of validators increases.
Yet, they show relatively low transaction latencies on average.
We also ﬁnd that synthetic workloads can provide accurate
predictions of performance limitations, as our ﬁndings are con-
sistent with real-world observations on performance incidents
on Avalanche and Stellar networks.

REFERENCES

[1] C. Fan et al., “Performance evaluation of blockchain systems: A

systematic survey,” Access, 2020.

[2] T. Rocket et al., “Scalable and probabilistic leaderless bft consensus

through metastability,” arXiv preprint arXiv:1906.08936, 2019.

[3] M. Lokhava et al., “Fast and secure global payments with stellar,” in

SOSP. ACM, 2019.

[4] B. Cao et al., “Performance analysis and comparison of pow, pos and
dag based blockchains,” Digital Communications and Networks, 2020.
[5] T. T. A. Dinh et al., “Blockbench: A framework for analyzing private

blockchains,” in ICMD, 2017.

[6] “Hyperledger Caliper,” https://github.com/hyperledger/caliper, 2018.
[7] Z. Dong et al., “Dagbench: A performance evaluation framework for

dag distributed ledgers,” in CLOUD.

IEEE, 2019.

[8] H. Sukhwani et al., “Performance modeling of hyperledger fabric

(permissioned blockchain network),” in NCA.

IEEE, 2018.

[9] M. Kuzlu et al., “Performance analysis of a hyperledger

fabric
blockchain framework: throughput, latency and scalability,” in IEEE
international conference on blockchain (Blockchain), 2019.

[10] M. Touloupou et al., “Towards a framework for understanding the

performance of blockchains,” in BRAINS.

IEEE, 2021, pp. 47–48.

[11] M. Dabbagh et al., “Performance analysis of blockchain platforms:
Empirical evaluation of hyperledger fabric and ethereum,” in IEEE
IICAIET, 2020, pp. 1–6.

[12] D. Saingre et al., “Bctmark: a framework for benchmarking blockchain

technologies,” in AICCSA.

IEEE, 2020.

[13] S. Bano et al., “Sok: Consensus in the age of blockchains,” in AFT,

2019.

[14] T. Bamert et al., “Have a snack, pay with bitcoins,” in P2P.

IEEE,

2013.

[15] G. Wood et al., “Ethereum: A secure decentralised generalised transac-

tion ledger,” Ethereum project yellow paper, 2014.

[16] Y. Gilad et al., “Algorand: Scaling byzantine agreements for cryptocur-

rencies,” in SOSP. ACM, 2017.

[17] F. Schuh and D. Larimer, “Bitshares 2.0: General overview,” 2017.
[18] “Libra white paper,” https://libra.org/en-US/white-paper/, 2019.
[19] M. Yin et al., “Hotstuff: Bft consensus with linearity and responsive-

ness,” in SPDC, 2019.

[20] E. Androulaki et al., “Hyperledger fabric: a distributed operating system

for permissioned blockchains,” in EuroSys. ACM, 2018.

[21] D. Ongaro and J. Ousterhout, “In search of an understandable consensus

algorithm,” in USENIX ATC, 2014.

[22] E. Kokoris-Kogias, P. Jovanovic, L. Gasser, N. Gailly, E. Syta, and
B. Ford, “Omniledger: A secure, scale-out, decentralized ledger via
sharding,” in 2018 IEEE Symposium on Security and Privacy (SP).
IEEE, 2018, pp. 583–598.

