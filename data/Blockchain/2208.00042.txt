GoodFATR: A Platform for Automated Threat Report
Collection and IOC Extraction
Gibran Gomez
gibran.gomez@imdea.org
IMDEA Software Institute
Madrid, Spain
Universidad Politécnica de Madrid
Madrid, Spain

Juan Caballero
juan.caballero@imdea.org
IMDEA Software Institute
Madrid, Spain

Srdjan Matic
srdjan.matic@imdea.org
IMDEA Software Institute
Madrid, Spain

2
2
0
2

l
u
J

9
2

]

R
C
.
s
c
[

1
v
2
4
0
0
0
.
8
0
2
2
:
v
i
X
r
a

Gustavo Sánchez
gustavo.sanchez@imdea.org
IMDEA Software Institute
Madrid, Spain

Silvia Sebastián
silvia.sebastian@imdea.org
IMDEA Software Institute
Madrid, Spain
Universidad Politécnica de Madrid
Madrid, Spain

Arturo Villacañas
arturo.villacanas@imdea.org
IMDEA Software Institute
Madrid, Spain
Universidad Politécnica de Madrid
Madrid, Spain

ABSTRACT
To adapt to a constantly evolving landscape of cyber threats, organi-
zations actively need to collect Indicators of Compromise (IOCs),
i.e., forensic artifacts that signal that a host or network might have
been compromised. IOCs can be collected through open-source and
commercial structured IOC feeds. But, they can also be extracted
from a myriad of unstructured threat reports written in natural lan-
guage and distributed using a wide array of sources such as blogs
and social media. This work presents GoodFATR an automated
platform for collecting threat reports from a wealth of sources and
extracting IOCs from them. GoodFATR supports 6 sources: RSS,
Twitter, Telegram, Malpedia, APTnotes, and ChainSmith. Good-
FATR continuously monitors the sources, downloads new threat
reports, extracts 41 indicator types from the collected reports, and
filters generic indicators to output the IOCs. We propose a novel
majority-vote methodology for evaluating the accuracy of indica-
tor extraction tools, and apply it to compare 7 popular tools with
GoodFATR’s indicator extraction module. We run GoodFATR over
15 months to collect 472,891 reports from the 6 sources; extract
1,043,932 indicators from the reports; and identify 655,971 IOCs.
We analyze the collected data to identify the top IOC contributors
and the IOC class distribution. Finally, we present a case study on
how GoodFATR can assist in tracking cybercrime relations on the
Bitcoin blockchain.

KEYWORDS
Indicators of Compromise, IOC, Cyber Threat Intelligence, RSS,
Twitter, Telegram

INTRODUCTION

1
Cyber Threat Intelligence (CTI) provides information on attacker
behavior that allows to gain visibility into the fast-evolving threat
landscape; to understand the techniques, tactics, and procedures
(TTPs) attackers use; and to timely identify and contain attacks.
CTI is a multi-billion dollar industry, expected to keep growing
to more than 16 billion USD by 2026 [30]. An essential piece of
CTI is extracting and sharing indicators of compromise (IOCs),

1

forensic artifacts that when observed on a device or network indicate
it may have been compromised, e.g., malicious IPs, domains, and
file hashes. IOCs are an actionable piece of CTI, as they can be fed
to security systems (e.g., NIDS, firewall, HIDS, blacklists) to detect
and block attacks.

IOCs are generally distributed through open and commercial IOC
feeds [6, 27]. Such feeds provide structured IOC data, following
standardized formats (e.g., STIX [36], OpenIOC [14]). However,
much CTI is distributed through unstructured threat reports, written
in natural language and published through a wealth of security blogs
and social media platforms. For example, Twitter has become widely
used for exchanging and spreading cybersecurity information, not
only by cybersecurity companies, but also by experts that sometimes
rush to share their discoveries [3, 47]. Natural language reports
often contain IOCs and typically provide more detailed contextual
descriptions about the IOCs compared to IOC feeds. In addition,
while IOCs feeds usually focus on a small set of indicator types (i.e.,
IP addresses, domain names, file hashes), unstructured threat reports
frequently provide more varied indicator types such as vulnerability
identifiers, email addresses, blockchain addresses, fuzzy hashes (e.g.,
SSDeep [25]), MAC addresses, and target countries.

In a large and fast-evolving landscape of threats, coverage is diffi-
cult to achieve by any single entity, as shown by different sources
having little IOC overlap [6, 27]. Thus, it is not sufficient to rely
on a single source, or a small set of sources, to build an accurate,
comprehensive, and up-to-date IOC list. To overcome this limitation,
organizations and security analysts can resort to collecting and ex-
tracting IOCs from multiple sources. However, there exist a myriad
of sources through which threat reports are disseminated including
hundreds of blogs, Twitter accounts, and Telegram channels.

To address this challenge, we present GoodFATR, an automated
platform for collecting threat reports from a wealth of sources and
extracting their IOCs. GoodFATR has a modular design that allows
treating 6 diverse sources in an unified manner: RSS, Twitter, Tele-
gram, and three report datasets Malpedia [41], APTnotes [5], and
ChainSmith [58]. For RSS, Twitter, and Telegram it takes as input a
list of origins (RSS feeds, Twitter accounts, Telegram channels) to
monitor. It periodically visits those origins to identify new entries

 
 
 
 
 
 
Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

(blog posts, tweets, messages). For entries that contain a URL to
a report, it downloads the report’s document. The selected sources
are complementary. RSS allows collecting reports from hundreds of
blogs from cybersecurity companies and individual experts, while
Twitter and Telegram cover social media distribution. In addition,
crowd-sourced datasets such as Malpedia and APTnotes allow iden-
tifying previously unknown blogs and Twitter accounts that should
be monitored, as well as collecting reports from blogs without an
RSS feed. GoodFATR also has an indicator extraction tool called
SEARCHER that takes as input a document in HTML, PDF, or plain
text format; applies regular expressions to extract 41 indicator types;
and filters generic (i.e., benign) indicators to produce a list of IOCs.
Prior approaches for extracting indicators from unstructured threat
reports most often leverage regular expressions. Even recent works
that apply natural language processing (NLP) techniques to analyze
threat reports still use regular expressions for identifying indicators
as part of their pipeline [20, 28, 48, 58]. Regular expressions for
extracting indicators can easily become complex, making it difficult
to understand what they match. Also, small differences between
regular expressions for the same indicator type may significantly
affect the extraction results. Furthermore, regular expressions can
be affected by catastrophic backtracking introducing ReDoS vul-
nerabilities [11]. Despite the popularity of regular expression based
indicator extraction tools [7, 21, 35, 45, 46, 51] no previous work
has systematically evaluated them to understand which tool is more
accurate for each indicator type and how efficient their extraction
is. This work addresses this gap by applying a novel majority-vote
methodology to compare the accuracy of 7 popular indicator extrac-
tion tools [7, 17, 21, 35, 45, 46, 51], as well as GoodFATR’s own
SEARCHER tool, on 4,420 reports. The results show SEARCHER is
the most accurate tool on 11 of the 13 indicator types supported by
multiple tools. We also identify ReDos vulnerabilities in two of the
tools, one new and one previously reported, but still unfixed.

We have used GoodFATR to collect 472,891 reports over 15
months from 3,226 origins distributed through 6 sources; extract
1,043,932 indicators from the reports; and identify 655,971 IOCs. We
analyze the collected data to identify the top IOC contributors and
the IOC class distribution. Finally, we detail how we have leveraged
GoodFATR on a novel approach for tracking cybercrime relations
on the Bitcoin blockchain [15].

This work provides the following contributions:

• We present GoodFATR, an automated platform to collect
threat reports from a variety of sources, extract the indicators
in the collected reports, and filter generic indicators to output
IOCs.

• We implement a modular report collection module that sup-
ports 6 sources: RSS, Twitter, Telegram, Malpedia, APTnotes,
and ChainSmith.

• We develop SEARCHER, an indicator extraction tool that given
an HTML, PDF, or plain text document, applies regular ex-
pressions to extract 41 indicator types.

• We present a novel majority-vote methodology for evaluat-
ing the accuracy of indicator extraction tools. We apply our
methodology to compare the accuracy of 7 popular tools and
our own SEARCHER over 4,420 reports. The results show
SEARCHER is the most accurate tool on 11 of the 13 indicator
types supported by multiple tools.

Figure 1: The architecture of GoodFATR.

• We use GoodFATR to collect 472,891 reports over 15 months
from the 6 sources; extract 1,043,932 indicators from the re-
ports; and identify 655,971 IOCs. We analyze the collected
data to identify the top IOC contributors and IOC class distri-
bution.

• We demonstrate GoodFATR in a case study where we apply it
to extract previously unknown Bitcoin addresses from threat
reports. Those addressed are tagged and used as input to a
novel automated Bitcoin transaction tracing approach [15].

2 OVERVIEW
An indicator represents an artifact such as an IP address, a do-
main name, a URL, a file, a registry key, a blockchain address, or
a company name. An indicator is typically represented as a pair
of a type (e.g., email) and a string value (e.g., contact@test.com).
An indicator of compromise (IOC) is a malicious indicator whose
presence on a device or network indicates the device might have
been compromised.

This paper presents GoodFATR, a platform for collecting threat
reports from a wealth of sources and extracting IOCs from the
collected reports.

Figure 1 depicts the architecture of GoodFATR. It consists of
four modules: Report Collection, Downloader, Searcher, and Fil-
tering. The Report Collection gathers documents from a wealth of
sources including RSS feeds, Telegram channels, Twitter accounts,
and external lists of reports like Malpedia [41], APTnotes [5], and
ChainSmith [58]. The collection outputs a list of entries each cor-
responding to the observation of a report in a source. Each entry
contains the URL of the report, or the content of a tweet or Telegram
message. In addition, it may contain optional metadata about the
report (e.g., publication date, author, language) and the source where
it was observed (e.g., RSS feed, Twitter account). The Downloader
leverages an instrumented Chrome browser (and Python’s requests
library as backup) to fetch the document pointed by an entry’s URL
and store it to file together with download metadata (e.g., download
date, HTTP status code, redirection chain followed). For Twitter and
Telegram the Downloader is not used, as their entries already contain
the content of the posts. The Searcher takes as input a document
in HTML, PDF, or plain text format, extracts its text (for HTML
and PDF documents), and applies regular expressions to identify
41 indicators. We choose regular expressions because they are an

2

RSS feedsTelegramTwitterReportCollectionSearcherFilteringRegex FileIoCreportAPTnotesChainSmithMalpediaDownloaderGoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

efficient technique for identifying, in a given string, indicators with
some intrinsic structure such as email addresses or URLs. To ex-
tract indicators, the Searcher applies a match and validate approach.
Matching applies each of the regular expressions to identify candi-
date indicators in the input string. Validation uses a function specific
to each indicator type to ensure the candidate is indeed an indica-
tor. Separating matching from validation allows the validation to
perform computations that are not possible within regular expres-
sions such as checking a checksum embedded in an indicator value
(e.g., blockchain addresses, bank account numbers). It also prevents
regular expressions from becoming too complex. While validation
minimizes the set of incorrect indicators output, it is possible that
indicators present in the text are not IOCs, but rather correspond
to benign indicators (e.g., the email of the report’s author). Thus,
before generating the final list of IOCs, the Filtering removes generic
indicators that are not IOCs.

Throughout its pipeline, GoodFATR maintains traceability. From
an IOC an analyst can identify the documents that contained it, from
a document the entries from where it was collected, and from an
entry the sources where it was observed.

3 THREAT REPORT COLLECTION
Our threat report collection module has been designed to support a
variety of threat report sources. We collect information from RSS,
Twitter, Telegram, and publicly available report datasets such as the
Malpedia malware encyclopedia [41], the APTnotes repository [5],
and the ChainSmith database [58].

To handle diverse sources in a unified manner, the collection is

structured around three concepts: origin, entry, and document.

An origin captures the distribution vector through which a report
is disseminated, at a finer granularity than a source. Each source
typically has many origins; for RSS, an origin corresponds to the
feed from a specific blog, for Twitter to a user account, and for
Telegram to a channel. For report datasets (Malpedia, APTnotes,
ChainSmith) we use as origin the organization that authored the
report (e.g., Norton, TrendMicro). The same threat report may be
distributed through multiple origins within the same source, as well
as through different sources. An example is the report about a new
APT threat, authored by analysts of a security company. The report is
initially published on the blog of the security company, and thus will
appear in the company’s RSS feed. In addition, the security company
also advertises the report through its official Twitter account. After
its publication, external third-party Twitter accounts and Telegram
channels may advertise the report as well, and the report may end up
being indexed by datasets such as APTnotes and Malpedia. In this
case, the report has been disseminated through multiple origins. For
each report, in addition to the source we include also the origin in its
identifier in the form source:origin. For example, rss:nakedsecurity
and twitter:nakedsecurity correspond to the RSS feed of the Naked
Security blog by Sophos, and its Twitter account @NakedSecurity,
respectively. The main use of origins is to produce fine-grained
statistics about report distribution, e.g., to measure which Twitter
accounts, Telegram channels, and RSS feeds provide more threat
reports.

An entry captures a specific mention of a threat report through
an origin. It corresponds to a post in a Twitter account, Telegram

Table 1: Sources used for collecting threat reports.

s
C
O
I

t
u
p
n
I

s
n
i
g
i
r
O

e
v
i
t
a
l

u
m
u
C

t
n
e
t
n
o
C

L
R
U
d
a
o
l
n
w
o
D
✗ ✓ ✓ ✓ ✗
✗ ✓ ✓ ✗ ✓
✗
✗ ✓ ✓ ✗
✓
✗
✗ ✓ ✗
✓ ✓ ✗ ✓ ✗
✓ ✓ ✗ ✓ ✗

Source
aptnotes
chainsmith
malpedia
rss
telegram
twitter

Origin
organization
organization
organization
feed
channel
account

channel, or RSS feed. For report datasets an entry is a record in the
report metadata, namely a line in the report index file of APTnotes,
a row in the database of ChainSmith, and a BibTex entry in the
Malpedia bibliography. Each entry has an origin and it contains
either a download URL pointing to the report, or in the case of
social media, a string with the post’s text. In addition, an entry may
contain additional report metadata as provided by the source: the
report author, its original publication timestamp, and the last update
timestamp.

A document is identified by its SHA256 hash and it captures
the HTML, PDF, or text content of a report. Multiple documents
may correspond to the same report. For example, due to webpage
non-determinism a report distributed as HTML might produce a new
content (i.e., different SHA256) every time the webpage is down-
loaded. Similarly, an APT report originally distributed as a webpage
in the company’s blog, may be stored by APTnotes as a PDF doc-
ument in a third-party storage service. When collecting the same
report from the company’s RSS feed and APTnotes, each source
produces a different document. Every time GoodFATR downloads
an entry, it links the entry information to the download information,
e.g., the hash of the document downloaded from the entry’s URL
and the download timestamp. This information allows us to trace
each document to the entries, origins, and sources from where it has
been disseminated.

3.1 Threat Report Sources
Table 1 summarizes the six report sources currently supported by
GoodFATR. Each source has a dedicated collection submodule. The
RSS, Twitter, and Telegram submodules take as input a file that
specifies the origins (i.e., feeds, accounts, channels) that should be
monitored. The report datasets do not require an input origin list as
the whole dataset is downloaded. We manually created our initial ori-
gin lists for RSS, Twitter, and Telegram by including resources from
prominent companies, cybersecurity news websites, and well-known
security experts. We continuously add new origins as we identify
interesting RSS feeds, Twitter accounts, and Telegram channels.
Adding new origins is currently the only manual step in GoodFATR.
Table 4 captures the total number of origins in each source at the
time of writing: 300 RSS feeds, 383 Twitter accounts, 9 Telegram
channels, 2,368 organizations in Malpedia, 155 organizations in
APTnotes, and 11 blogs in ChainSmith.

3

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

For each source, Table 1 shows how the origin is defined and
five properties: whether it requires an input list of origins; whether
the reports are cumulative (i.e., old entries are always available) or
entries expire after some time; whether it provides a URL for each
entry from where the report needs to be downloaded; whether it
includes the actual report or only the URL to the report content; and
whether it includes IOCs extracted from the content.

RSS. We use RSS feeds to collect reports from 300 security blogs.
Each RSS feed is uniquely identified by a URL pointing to the
feed’s XML file. At a given moment, an RSS feed provides only
a configurable number of entries, e.g., the 10 most recent ones. To
minimize the risk of missing entries, the RSS module queries each
feed on a daily basis. Between two consecutive visits, we collect
data using an incremental approach to avoid storing the same entry
multiple times, in case a feed was not updated. After fetching a
feed, the module extracts the unique URLs from the new entries and
passes them to the Downloader. URLs that were previously crawled
are ignored.

Previous work that collected threat reports from security blogs
built dedicated crawlers for each blog of interest (e.g., [28, 58]).
The advantage of dedicated crawlers is that they can be designed to
collect the historical archive of reports available in a blog’s website.
One disadvantage is that a different crawler needs to be developed
for each blog, which limits the number of monitored blogs. For
example, Liao et al. [28] collected reports from 45 blogs and Zhu
and Dumitras from 10 blogs [58]. In contrast, we collect reports
from 300 blogs. Moreover, as websites evolve, the crawlers may
break and need to be updated. For example, ChainSmith originally
supported 10 blogs in 2018, but collection from different blogs
gradually decreased over time, possibly due to crawlers breaking
due to website updates. In mid-2020 only two blogs were still being
crawled, and since mid-2021 ChainSmith does not collect any new
blog posts. In contrast, our RSS feed collection still collects entries
from the same set of blog supported by ChainSmith, showing that
those origins are still active. It is worth highlighting that our RSS
feed collection may also require updates (e.g., if the feed’s XML
URL changes), but updating a feed’s URL is significantly easier than
building a new dedicated crawler.

Telegram & Twitter. The Telegram and Twitter modules leverage
the official APIs to query each service. Using these APIs they can
fetch not just the most recent messages, but any message that was
ever posted on an account or channel. Each module takes as input a
list of origins that should be monitored, and connects to each origin
on a daily basis to fetch all new posts. Using the API, our modules
collect each post from an account, or channel, only once. However,
it is possible to obtain posts with the same content from multiple
origins, e.g., if different users re-tweet the same original message.

Malpedia. Malpedia offers a manually-curated BibTex file with
reports related to malware. BibTex entries for new reports are added
on a nearly daily basis [41]. The BibTex file is generated dynamically
upon request and includes all reports in the database in a cumulative
manner. Each bibliography entry contains the following report fields:
author, title, publication date, organization (which we use as origin),
URL, language, and the date when the report was added to Malpedia.
The reports’ document are not provided, so the URLs are passed to
the Downloader to collect them.

4

APTnotes. APTnotes is a GitHub repository with manually-curated
APT reports [5]. It provides an index file with report metadata con-
taining the report’s title, source (i.e., author organization which we
use as origin), publication date, the filename and file hash of the
report document, and the URL where the document can be obtained.
Reports in webpages are converted from HTML to PDF. All reports
are stored in the box.com service as PDF files.

ChainSmith. The ChainSmith project [58] makes available an SQLite3
database with the reports it has downloaded, their metadata, and the
IOCs it has extracted from the reports. The database has been up-
dated on a weekly basis since 2018. It contains one table where each
row corresponds to an IOC extracted from a report including the
report URL, the source (i.e., blog identifier we use as origin), the
report title, and the publication date. The reports’ documents are
not provided, so we extract the document URLs and pass them to
Downloader.

3.2 Downloader
The Downloader takes as input an entry and tries to download the
content pointed to by the entry’s URL. It uses Selenium, a popular
framework used for testing Web applications [53]. We instrument
Selenium to render URLs using a fully fledged instance of Google
Chrome. The Downloader is able to follow redirects, it supports
dynamic content executed with JavaScript, and, in addition to HTML
pages, it can also download plain text documents as well as other
MIME types such as PDFs. In case our instrumented browser did
not succeed in retrieving the content, the Downloader makes an
additional attempt with Python’s requests library [43]. For each
successfully downloaded URL, the Downloader stores the document
to a file and it updates the entry with the download information
including the download timestamp, the document hash, the HTTP
status code, and the redirection chain followed. In the final step,
the downloaded documents are filtered to remove resources with
HTTP status code errors and HTML content where the title states
the webpage was not found.

4

INDICATOR EXTRACTION: A
COMPARATIVE STUDY

In this section, we perform a survey and functionality comparison
of indicator extraction tools including our own. We further quantita-
tively evaluate the tools in Section 6.

To identify the tools, we search for open-source projects for
extracting indicators. For this, we query GitHub for projects related
to the ioc keyword. Then, we manually examine each matching
project to select those that correspond to tools that extract indicators.
We keep only popular tools with at least 30 stars, which helps avoid
forks of more popular projects. If an identified tool references any
other indicator extraction tools, we also include those in our search.
This process identifies 7 popular open-source tools for extracting
indicators. We also include our own SEARCHER tool, for a total of 8
tools in Table 2.

The 8 tools follow the same model comprising of three steps, two
of which are optional. The first optional step is text extraction, which
given an HTML or PDF document, extracts its text. The second
step, present in all tools, is indicator extraction, which extracts
the indicators present in an input string by applying a number of

GoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

Table 2: Comparison of indicator extraction tools.

Tool
JAGER [45]
IOC_PARSER [7]
CACADOR [46]
CYOBSTRACT [51]
IOC-FINDER [17]
IOCEXTRACT [21]
IOC-EXTRACTOR [35]
SEARCHER

Language
Python
Python
Go
Python
Python
Python
JavaScript
Python

Indicator Extraction
Stars Text HTML PDF Rearm Validate Dedup.

Inputs

69
378
119
66
96
356
35
-

(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)

(cid:32)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)

(cid:32)
(cid:32)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)

(cid:35)
(cid:71)(cid:35)
(cid:71)(cid:35)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)

(cid:35)
(cid:35)
(cid:35)
(cid:71)(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)

(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:35)
(cid:71)(cid:35)
(cid:35)
(cid:32)
(cid:32)

IOCs
11
11
12
25
25
8
18
41

Filtering

(cid:35)
(cid:32)
(cid:71)(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:35)
(cid:32)

regular expressions, grammars, and rules. Finally, some tools apply
an optional filtering step whose goal is to remove indicators that
are benign (e.g., domains of security vendors), and thus cannot be
considered IOCs. The dominant language for implementing the tools
is Python used by 6 tools including the two most popular tools
(IOC_PARSER and IOCEXTRACT) and our own SEARCHER tool.
CACADOR is written in Go and IOC-EXTRACTOR in JavaScript.

analysis showing wide variability among the produced text. They
concluded that the best performing HTML text extraction libraries
for privacy policies were Boilerpipe [24] and Readability.js [33],
while the worst-performing one was BeatifulSoup [44]. BoilerPipe
is written in Java and Readability.js in JavaScript, but both have
Python wrappers available.

In Table 2, a solid circle (

circle (
support. Next, we detail each of the three steps.

) partial or optional support, and an empty circle (

(cid:32)

(cid:71)(cid:35)

(cid:35)

) indicates full support, a half-filled
) no

4.1 Text Extraction
Since most security reports are distributed as HTML and PDF docu-
ments, text extraction is an important step in the complete indicator
extraction process. However, it can be performed as a separate pre-
processing step that takes as input an HTML or PDF document and
outputs its content as text. Content extraction is a an independent
process from identifying indicators, and this is likely the reason why
5 of the tools do not support it and they assume that the input is a
string, e.g., containing the full text of a report. In summary, all 8 tools
can take an input string, or read text from standard input or from file,
and extract indicators in the string. In addition, three tools including
our own, accept directly input HTML and PDF files, including the
text extracting from the input document, before extracting indicators.
This is convenient for the user since it does not need to write its own
text extraction code and allows the tool developer to customize the
text extraction.

Text extraction is a common step in many Natural Language
Processing (NLP) pipelines such as those used to analyze privacy
policies (e.g., [4, 16, 52, 59, 60]). Text extraction can have a sig-
nificant impact on the extracted indicators. For example, the text
extraction process may pre-pend or append text to the indicator
value, making the extraction to miss the indicator or to output an
incorrect indicator with extraneous characters. It is also possible for
the extracted text to split indicators across multiple lines, causing
them to be missed. For example, PDF text extraction libraries often-
times split long URLs appearing in footnotes across multiple lines
in the output text (e.g., URLs that do not fit the length of a column).
All three tools that support HTML and PDF files are written in
Python. They all use the pdfminer.six [50] library for PDF text ex-
traction. JAGER and IOC_PARSER use the BeautifulSoup [44] library
for HTML text extraction, while our SEARCHER tool supports both
BeautifulSoup and Readability.js [33]. Recently, Hosseini et al. [18]
analyzed 7 HTML text extraction approaches used in privacy policy

5

Indicator Extraction

4.2
Given an input string, indicator extraction identifies indicators present
in the string. Seven of the tools use regular expressions for the ex-
traction, while IOC-FINDER uses grammars instead. The tools using
regular expressions process the text in a loop, each time applying
one regular expression at a time to the whole text. Each regular
expression is associated to one indicator type that is assigned to each
of the regular expression matches. Most tools have one regular ex-
pression for each supported indicator type, although some tools (e.g.,
IOCEXTRACT, SEARCHER) support multiple regular expressions for
the same indicator.

The middle part of Table 2 captures four properties of the in-
dicator extraction step: whether defanged indicators are supported
and rearmed, whether tools split the extraction process into match-
ing and validation, whether the indicators are deduplicated, and the
supported indicator types. We describe each property next.

Defanged and rearmed indicators. It is common for security re-
ports to defang malicious indicators, (i.e., IOCs) in case a user
inadvertently clicks on them in a navigation tool like a browser.
For example, IP address 9.9.9.9 may appear in a security report as
9[.]9[.]9[.]9, while URL http://malicious.com/badfile may appear as
hxxp://malicious(.)com/badfile. Such indicators are often called de-
fanged to indicate that they have been converted in less harmful, and
they are not dangerous anymore. These transformations are similar to
the ones applied to email addresses to limit automated collection by
spammers, e.g., “contact_at_somewhere[.]com”. Of the 8 tools, only
JAGER does not identify defanged indicators. The other tools support
a subset of the following defang transformations: (I) replacing the
dot in IPv4 addresses, domain names, URLs, emails, and filenames,
(II) replacing the @ sign in emails, (III) replacing the scheme in
URLs (e.g., using hxxp:// instead of http://), and (IV) replacing the
backslash in URLs. Two tools are marked as having partial support:
IOC_PARSER only supports the dot in URLs and domain names,
while CACADOR only supports the dot in IPv4 addresses. The other
tools are marked as full support, however they may not exactly sup-
port the same transformations, e.g., only IOC-EXTRACTOR supports

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

the replacement of the backslash in URLs. The space of defang
transformations that users can apply is very large and the extraction
tools only support the limited set presented above, which is clearly
incomplete. Intuitively, the defang transformations most important
to support are those most often used by security report authors (or
by users when analyzing other sources like webpages). However, it
is hard to know the most popular transformations a priori, so it is
typical to add support for new defang transformations as they are
observed in the wild.

The tools use two approaches to handle defanged indicators. The
most popular approach, used by six tools, is broadening the regular
expressions used to identify the indicators to cover common defang
operations. For example, the regular expression should support that
an IP address optionally contains brackets or parenthesis around
the dots. Once the defanged indicators have been matched by the
regular expression, they can be rearmed (or refanged) to output the
original indicator values. Four tools rearm the defanged indicators
by default, while another two (IOC_PARSER, IOCEXTRACT) allow
the user to if defanged or rearmed values should be returned. The
alternative approach used by IOC-FINDER and IOC-EXTRACTOR is
to first apply a rearm transformation to the raw text before applying
the regular expressions. This approach does not require broadening
the regular expressions to handle different defang transformations.
On the other hand, blindly rearming the text before applying the
regular expressions could incorrectly modify the text (e.g., rearming
some text that is not part of an indicator). It also prevents returning
the defanged value as it appears in the text since it has been rewritten.

Match and validate. One goal of indicator extraction is to minimize
false positives (FPs), i.e., avoid to output indicators that are not real
indicators, such as a two tokens concatenated with a period, that
is not a fully qualified domain name. This requires making the
regular expressions as narrow as possible, without introducing false
negatives. For example, it is typical that a regular expression for
domain names will check that the top-level domain (TLD) is one of
the IANA-approved TLDs. For this, it is common for tools to include
a long list of valid TLDs inside the regular expression. Unfortunately,
the IANA list already contains over 1,500 TLDs and may continue
to grow over time making the regular expression cumbersome. An
alternative approach is to split the indicator extraction process into
two steps: regular expression matching and validation. In this model,
the regular expression can be a bit wider (i.e., produce more matches)
because the validation, which is specific to each indicator type, which
will discard incorrect matches. This match and validate process was
first used by CYOBSTRACT, with basic checks such as validating
that ASN numbers are in hard-coded ranges or that the length of a
domain name is below 160 characters (which is not entirely correct
as the maximum length is 255 octets).

Our SEARCHER tool implements a more complete match and
validate process where each indicator type has an optional validation
function that checks the returned regular expression matches. For
example, in SEARCHER the fqdn regular expression does not need to
include the list of valid TLDs because there is a fqdn validation func-
tion that ensures the matched valued indeed contains a valid TLD.
The advantage is that the list of IANA-approved TLDs can be kept in
a separate file, which can be updated without modifying the regular

expression. More importantly, the match and validate approach al-
lows to perform Turing-complete processing on the matches such as
validating an embedded checksum in a bank account or a Bitcoin ad-
dress. Such validation is not possible using only a regular expression,
and it can significantly reduce FPs. For example, it is common for
MD5 hashes to match a Bitcoin regular expression, but it is highly
unlikely those spurious matches will pass the checksum validation.
It is worth mentioning, that the implementation in our SEARCHER
tool performs validation after the indicator has been rearmed, so that
the validation does not need to handle defang transformations.

Deduplication. It is possible for the same indicator to appear mul-
tiple times in the same string at different locations. We say that a
tool deduplicates if it removes duplicated indicators from its output.
For example, if the same URL appears twice in the input string, the
output only contains the url indicator once. Tools can be classified
), those that do not deduplicate
into those that always deduplicate (
(
). None of the
open-source tools return the position (i.e., start offset) at which the
(cid:35)
indicator is matched. Thus, the main value of not deduplicating is to
know how many times an indicator appeared in the input string. The
most flexible approach is to make deduplication optional, as done
by IOC_PARSER and IOC-FINDER.

), and those where deduplication is optional (

(cid:71)(cid:35)

(cid:32)

Our SEARCHER tool offers two different APIs. The raw API does
not deduplicate. It returns all indicators identified, with its indicator
type, starting offset, raw value, and rearmed value. In contrast, the
deduplicated API first invokes the raw API, and then it deduplicates
the received values by removing the starting offset and the raw
values, i.e., returns only deduplicated rearmed indicators. Providing
all matches with their starting offset allows the raw API to be used
in additional scenarios. For example, Gao et al. [13] propose IOC
protection to handle IOCs that contain dots (e.g., URLs, domains,
IPs) in NLP pipelines. Such indicators negatively impact sentence
tokenization in NLP libraries that leverage dots to identify the end of
sentences. IOC protection first identifies the indicators (e.g., using
regular expressions), replaces their value in the text with a keyword
that does not contain dots, tokenizes the text into sentences, and
finally replaces back the keyword with the original indicator value.
IOC protection requires the starting offset and the raw value of the
indicator in the input string, precisely what our raw API provides.

Indicators. One key difference between indicator extraction tools
is the set of indicator types they support, i.e., the set of indicators
for which they have regular expressions, grammars, or extraction
rules. Overall, we have identified 63 indicators that the tools extract,
as summarized in Table 3. We group indicator types into 16 classes:
network (13 indicators), social (12), blockchain (9), cryptographic
hashes (6), analytics (3), attack (3), contact (2), file (2), intellectual
property (2), organization (2), payment (2), vulnerability (2), fuzzy
hash (1), information sharing (1), location (1), registry (1), and yara
(1).

The largest class corresponds to network-related artifacts such as
domain names (fqdn), URLs (url), IP addresses (ip4, ip6), IP sub-
nets in CIDR form (ip4cidr, ip6cidr), IP address ranges (ipv4range,
ipv6range), AS numbers (asn), MAC addresses (macaddress), Tor
onion addresses used to access hidden services (onionAddress), In-
ternet Content Provider numbers that uniquely identify the owner

6

GoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

Table 3: Indicators extracted by different tools.

R
E
S
R
A
P
_
C
O
I

R
O
D
A
C
A
C

R
E
G
A
J

T
C
A
R
T
X
E
C
O
I

R
O
T
C
A
R
T
X
E
-
C
O
I
✓

R
E
H
C
R
A
E
S

T
C
R
A
E
R
D
T
N
S
I
F
B
-
O
C
Y
O
C
I
✓ ✓
✓
✓

✓
✓

✓

✓

✓
✓

✓

✓

✓
✓
✓

✓

✓ ✓
✓
✓

✓ ✓ ✓ ✓ ✓

✓ ✓
✓
✓
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓ ✓
✓

✓ ✓ ✓ ✓

✓

✓ ✓
✓ ✓ ✓ ✓ ✓

✓ ✓
✓
✓ ✓
✓ ✓
✓
✓
✓

✓
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓ ✓
✓
✓

✓ ✓ ✓ ✓ ✓

✓

✓

✓

✓

✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓ ✓
✓
✓
✓
✓
✓ ✓
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓
✓

✓
✓ ✓ ✓

✓ ✓
✓

✓

✓

✓
✓
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
✓ ✓

✓

✓

✓
✓

✓
✓

✓
✓

✓
✓
✓

s
l
o
o
T
m
u
N
3
1
1
1
1
1
3
1
1
1
7
1
1
8
2
1
4
3
7
1
3
3
1
1
1
1
1
1
8
3
1
5
1
1
1
1
1
2
8
3
1
2
1
4
8
8
4
5
1
1
1
1
1
8
2
1
1
1
1
1
1
1

Indicator
asn
asnOwner
attackType
attCk
authentihash
avLabel
bitcoin
bitcoincash
copyright
country
cve
dashcoin
dogecoin
email
ethereum
facebookHandle
filename
filepath
fqdn
githubHandle
googleAdsense
googleAnalytics
googleTagManager
iban
icp
importHash
incident
instagramHandle
ip4
ip4cidr
ip4range
ip6
ip6cidr
ip6range
isp
linkedinHandle
litecoin
macAddress
md5
monero
onionAddress
phoneNumber
pinterestHandle
regKey
sha1
sha256
sha512
ssdeep
telegramHandle
tezos
tlpLabel
trademark
twitterHandle
url
userAgent
webmoney
whatsappHandle
xmppHandle
yara
youTubeChannel
youtubeHandle
zcash

of a Chinese website (icp), and HTTP User-Agent strings (user-
Agent). The social category comprises of user handles for social
networks (Facebook, Instagram, LinkedIn, Pinterest, Twitter), open
source repositories (GitHub), Instant messaging tools (Jabber, Tele-
gram, WhatsApp), as well as YouTube usernames and channel iden-
tifiers. The blockchain category comprises of addresses for 9 popular
blockchains (Bitcoin, BitcoinCash, DashCoin, DogeCoin, Ethereum,
LiteCoin, Monero, Tezos, ZCash). Cryptographic hashes include
MD5, SHA1, SHA256, and SHA512, as well as their application
on specific parts of a Windows executable such as the import table
(importHash) and the whole executable without Authenticode code-
signing fields (authentihash). There is also a fuzzy hash (ssdeep)
used to identify files with similar content [25]. The analytics category

7

includes three Google identifiers: Google Adsense, Google Analyt-
ics, and Google Tag Manager. The attack category contains MITRE
ATT&CK techniques, tactics, and procedures, a list of attack-related
keywords (e.g., DoS, spam), and antivirus labels (avLabel). The
payment category includes IBAN bank account numbers and Web-
Money addresses [56]. The vulnerability category includes CVE
identifiers and also a variety of identifiers for other vulnerability
sources (e.g., BugTrack, Microsoft Bulletins) grouped into a generic
incident indicator by CYOBSTRACT. The information sharing cat-
egory comprises of a single indicator for the traffic light protocol
(tlpLabel) that controls dissemination of information [12]. Indicators
in the remaining categories are straightforward and capture: contact
(email, phoneNumber), files (filename, filepath), intellectual prop-
erty (copyright, trademark), organization names (asnOwner, isp),
locations (country), Windows registry keys (registryKey), and Yara
rules (yara).

There are only 6 indicators that are extracted by all 8 tools: Emails,
IPv4 addresses (ip4), hashes (md5, sha1, sha256), and URLs. An-
other two indicators are extracted by 7 tools: CVE vulnerability
identifiers (cve) and domain names (fqdn). Next come two indicators
extracted by 5 tools: IPv6 addresses (ip6) and SSDeep fuzzy hashes.
However, the majority corresponds to 38 (61%) indicators extracted
by a single tool.

Among the indicators in Table 3 there are some that deserve dis-
cussion. There are three indicators (country, tlpLabel, attackType)
that are identified using regular expressions that are a disjunction
of keywords. For example TLP labels can have only four values:
TLP:white, TLP:green, TLP:amber, TLP:red. Since the set of pos-
sible TLP values is finite, the regular expression can identify all
possible indicator values. Similarly, there is only a finite number
of recognized countries. However, attackType includes a number
of attack-related keywords that CYOBSTRACT identifies. Such list
only includes attack topics of interest for the tool authors and may
not include attack topics other users are interested in. The approach
of building a regular expression from a set of keywords can be
applied to any taxonomy of terms (e.g., family names, exploit kit
names). CYOBSTRACT provides support for building such regular
expressions, how, the main limitation of this technique is that it
cannot identify new terms (e.g., new family names or exploit kits).
CYOBSTRACT extracts two indicators that aim to capture organiza-
tion names (asnOwner, isp). However, organization names do not
have a clear structure save when using company-related suffixes
(e.g., Ltd., Inc.) and thus are typically extracted using Named Entity
Recognition (NER) techniques based on machine learning classi-
fiers. Furthermore, trying to separate whether an organization name
corresponds to an ISP or ASN owner is a challenging problem that
is better suited for NLP techniques. There are also some indicators
that one could argue should be split into multiple indicators. For ex-
ample, the regular expression for the incident indicator extracted by
CYOBSTRACT captures a disjunction of regular expressions, each for
a different vulnerability report identifier (e.g., BugTrack, Microsoft
Bulletins). Such disjunction is more efficient than having a separate
regular expression for each identifier, but does not allow users to
extract only those identifiers they are interested in. Another interest-
ing case are cryptographic hashes like authentihash and importHash
that are really subtypes of other cryptographic hashes. For example,
importHash is the MD5 of the import table of a PE executable [29],

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

while authentihash is the SHA256 of a PE executable excluding Au-
thenticode code signing fields. Thus, it is not possible for a regular
expression to differentiate an importHash from a md5 or an authen-
tihash from a sha256 except if the subtype indicators appear with
some specific keywords before or after. Such regular expressions
are very specific, they rarely produce FPs but they will introduce
FNs when the expected keywords do not appear before or following
the hashes. On the other hand, regular expressions for the parent
types will also identify indicators subtypes (e.g., the md5 regular
expression will also identify importHash indicators), but classifying
an MD5 into what the MD5 captures (e.g., a file hash, a certificate
hash, an import table hash) is a challenging problem that is may
be easier to tackle through NLP. There are also two indicators that
correspond to ranges of IP addresses (ip4range, ip6range). Similar
range indicators could be defined for any integer indicators, e.g.,
ASN numbers. The last interesting case are Yara rules extracted by
IOCEXTRACT. Yara rules can be long and have internal structure
with mandatory and optional fields. Thus, for some readers they may
not fit the concept of indicator.

4.3 Filtering.
The extracted indicators may contain benign indicators that are not
IOCS. The goal of filtering is to remove such generic indicators
leaving only the IOCs. There are three tools that support filtering:
IOC_PARSER, CACADOR, and our SEARCHER. IOC_PARSER and
CACADOR both make use of hand-crafted filter lists to avoid includ-
ing some indicator values in the results. IOC_PARSER uses 11 filter
lists, one per indicator type it supports. However, only 4 of those
filter lists (email, fqdn, ip4, url) have entries. The email filter list
contains 6 contact addresses for large cybersecurity companies. The
ip4 filter list contains 6 reserved IP address ranges. The fqdn filter
list contains 143 manually-curated domains. Finally, the url filter list
contain 79 entries for popular cybersecurity blogs. CACADOR uses
a hard-coded filter list of 15 domains that belong to popular Web
services and cybersecurity companies.

All indicators SEARCHER extracts are passed to the filtering mod-
ule, which applies different rules depending on the indicator type.
First, the filtering treats as generic any indicator that embeds a do-
main name that appears in the list of URLs provided as input to the
Report Collector and the Downloader. An analogous filtering rule
is applied to the domains in the Tranco top-100K list [42]. Next,
the module excludes IP addresses and network ranges of private or
local networks (e.g., “192.168.1.0/24”). To avoid including contact
emails that often appear in security reports, the filtering module
does not consider as IOCs email addresses of known providers1 that
appeared more than 10 times in the same origin. Finally, for handles
from the social category, the filtering module excludes identifiers
whose name usually refers to web resources on social networks (e.g.,
“profile.php”, “media”, “photo”, etc).

5 EVALUATION
We have applied GoodFATR to collect reports from the six sources
over the last 15 months. We started our collection with RSS on April
1st, 2021. We then added Telegram in September 9th, 2021 and

1Our manually curated list includes: Gmail, Protonmail, Yahoo, AOL, Tutanota, Hotmail,
QQ, Mail.ru, Outlook, Yandex

Table 4: Data collection summary.

Source
aptnotes
chainsmith
malpedia
rss
telegram
twitter
ALL

Orig.
155
11
2,368
300
9
383
3,226

Start Date
2022/07/18
2022/07/18
2022/07/18
2021/04/01
2021/09/09
2021/10/28
2021/04/01

Entries
629
3,792
11,363
97,882
61,892
397,333
472,891

Indicators

Extracted
44,619
49,769
282,450
368,211
44,534
337,663
1,043,932

IOCs
41,530
42,820
253,337
231,937
33,200
125,733
655,971

Twitter on October 10th, 2021. On July 18th, 2022, we added the
three report datasets (APTnotes, ChainSmith, Malpedia), but these
datasets are cumulative so GoodFATR can also process their past
entries.

Table 4 summarizes the data collection and IOC extraction results
for each source. It captures the number of origins in the source at
the end of the collection, the collection start date, the number of
collected entries, as well as the number of extracted indicators (be-
fore filtering) and IOCs (after filtering). The most diverse source by
number of origins is Malpedia with reports from 2,368 organizations,
followed by Twitter with 383 accounts, RSS with 300 feeds, and
APTnotes with reports from 155 organizations. In contrast, Chain-
Smith has reports from only 11 blogs and GoodFATR tracks only 9
Telegram channels.

Twitter is the source with the largest number of collected entries
despite being introduced later than RSS and Telegram, but its entries
are tweets and thus short compared to full reports collected from
RSS and the three datasets. RSS is the most stable contributor with
an average of 6.1K new report URLs per month. Among the three
report datasets, Malpedia provides the most reports (11,363), with
ChainSmith providing one third (3,792), and APTnotes only 629
reports. The relative small size of APTnotes is due to its more
focused goal of only collecting reports for APTs.

IOC extraction. The lasts two columns in Table 4 show the indica-
tors extracted by SEARCHER (before filtering) and the final IOCs
(after filtering). Overall, GoodFATR extracted over 1M indicators of
which 656K (63%) are IOCs. Thus, filtering is extremely important
for discarding over one third of indicators that are generic and thus
are not IOCs. Filtering affects each source differently. In the extreme
case of Twitter, two thirds of the indicators are filtered. Across all
sources, domain names and IPv4 addresses account for 99% of the
generic indicators. Filtered domains names are those matching the
domain from where the report was collected and those included in
the Tranco top-100K list. Filtered IPv4 addresses are those of private
and local networks,

The average number of IOCs per report is highest for the report
datasets: APTnotes (66), Malpedia (22), and ChainSmith (11). Of
those, APTnotes and Malpedia are manually-curated, which prevents
the collection of non-technical reports. The collection in ChainSmith
is automated, but it leverages a small number of blogs from large
security companies known for their high quality technical content.
In contrast, reports from RSS have a lower ratio of IOCs (2.2), likely
due to the variety of feeds and reports they disseminate. For example,
some feeds may focus on technology news for the wider public
(and thus provide less IOCs), while blogs from security companies
may mix technical reports with less technical reports that focus on
the virtues of their products. Still, the large number of RSS feeds

8

GoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

Figure 2: Distribution of IOC classes grouped per source.

GoodFATR monitors compensates for the lower ratio, making RSS a
consistent IOC contributor. Finally, Telegram (0.5) and Twitter (0.3)
have the lowest ratios, which is expected due to the limited content
in each entry.

IOC class distribution. To understand what type of indicators each
source distributes, we group indicators using the categories intro-
duced in Section 4.2. Figure 2 reports the filtered IOCs split by class
and source. The plots include only the six most popular classes, the
rest are aggregated in the other category. Across all sources, network
indicators dominate. Malpedia, Telegram and Twitter are the biggest
contributors to the network category, with more than 110K IOCs
of this category per source. The hash category ranks second with
most indicators in this category coming from Malpedia (82K file
hash IOCs). In third place we find contact and social indicators, both
having similar distributions with the vast majority coming from RSS
and Malpedia. These two sources are also the highest contributors
of IOCs that survive the filtering and exhibit the highest variety in
terms of indicator types. For example, RSS and Malpedia provide
94% of the blockchain IOCs and over 61K email addresses, phone
numbers and social network handles. Telegram and Twitter show
significantly smaller variety of IOC types. For example, the union
of the contact and social classes represent only 1.6% of the IOCs
in Telegram, and they only account for 92 of the IOCs extracted
from Twitter. One possible reason is that Twitter accounts are more
specialized with some accounts focusing only on certain indicator
types such as malicious file hashes or vulnerability identifiers.

Top origins. Table 5 shows the top-10 origins for each source,
ranked by the percentage of IOCs the origin contributes to the source.
Overall, we notice a long tail distribution, with the top-10 origins gen-
erating 50% of all IOCs. This pattern is consistent across all sources
and IOC classes. All three report datasets are dominated by large
security companies, with the exception of the Citizen Lab at Univer-
sity of Toronto (rank 8 in APTnotes) and three personal blogs (ranks
8–10 in ChainSmith). A surprising contributor is Price Waterhouse

9

Coopers (rank 10 in APTnotes), one of the largest financial account-
ing companies, but less known for their security services. In general,
personal blogs exhibit less activity compared to company blogs with
7% of all IOCs coming from personal blogs. RSS has more variety
in the origins. Surprisingly, the top contributor is the personal blog
from Dancho Danchev, followed by two Medium blogs that aggre-
gate blockchain and cybersecurity news. The RSS top-10 is rounded
by the research labs of three large companies (Cisco, F5, Malware-
bytes), two other personal blogs (Bruce Schneier, contagiodump),
and two magazines (Cointelegraph.com and BleepingComputer).
Interestingly, two of the RSS top 10 origins focus on blockchain. We
added blockchain-specific sources motivated by the case study in
Section 7. Telegram is largely dominated by the cybsecurity channel,
which provides 76% of all Telegram IOCs. Similarly, Twitter also
has one dominant account @ecarlesi with more than 140K tweets
and responsible for 76% of the Twitter IOCs. Second, but far be-
hind with 4.7% of IOCs, comes @threatmeter, an automated bot
that publicizes vulnerabilities and accounts for 90% of vulnerability
IOCs.

6 EVALUATION OF IOC EXTRACTION

TOOLS

To evaluate the indicator extraction tools, we design a majority-
vote methodology that runs the 8 tools in Table 2 on the text of
the same set of threat reports. Then, we compare the indicators
extracted by the different tools on the same report, assuming that
the correct indicators are those extracted by a majority of tools.
The advantage of such evaluation is that it can be run on a large
set of reports bypassing the challenge of building a ground truth
that is representative of the wealth of indicators the tools extract.
The disadvantage is that this approach works only when indicators
are extracted by multiple tools, and that in some occasions it is
possible that the minority of tools is actually correct. We detail our
methodology and the obtained results next.

We first build a document dataset by extracting the text using
GoodFATR from all reports in APTnotes and ChainSmith. We
choose these two sources because they cover different report file
types (APTnotes has PDF reports and ChainSmith mostly HTML
reports) and because both sources have a high average of IOCs per
report, as shown in Section 5. In four PDF reports the text extraction
failed: two were Excel spreadsheets that our text extraction does
not support and the other two were corrupted. We evaluate the tools
using the 4,420 successfully extracted text documents.

Methodology. We run all tools on each document, saving the in-
dicators each tool extracts to a separate file. To compare all tools
in a similar setting, we disable filtering for tools that support it
(IOC_PARSER, CACADOR, SEARCHER) and deduplicate indicators.
Since each tool may assign slightly different names to indicator
types (e.g. ipv4addr, ip, and ipv4 for IPv4 addresses), we normalize
them to match the names in Table 3. Additionally, some tools trans-
form case-insensitive indicator values (e.g., domain names, email
addresses) to lower case or upper case, while others output them
as they appear in the text. To address such differences, we also
normalize indicator values. In particular, we lower case the follow-
ing indicator values: hashes (md5, sha1, sha256, sha512, ssdeep),
regkey, ip6, fqdn, and email. Finally, we normalize AS numbers to

IOC classes:socialhashescontactCVEsblockchainnetworkothersAPTnotesChainsmithMalpediaRSSTelegramTwitterTable 5: Top-10 origins that contribute with the highest number of IOCs, grouped per source. In brackets we report the percentage
of IOCs associated to each origin.

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

ChainSmith

APTnotes
Rank
#1
Kaspersky (13.11%)
#2 Palo Alto Networks ( 7.18%)
#3
#4
#5
#6
#7
#8
#9
#10

Sucuri (22.07%)
Norman ( 6.11%) Malwarebytes (15.01%)
Virus Bulletin (14.12%)
Sophos ( 4.88%)
Forcepoint ( 4.60%)
ESET ( 4.34%)
TaoSecurity ( 4.15%)
Hexacorn ( 1.70%)
PwC UK Blogs ( 1.88%) Roger McClinton ( 0.90%)

Symantec ( 5.24%)
ClearSky ( 4.74%)
FireEye ( 4.24%)
ESET ( 3.72%)
Trend Micro ( 2.67%)
Citizen Lab ( 2.40%)

RSS
Malpedia
Webroot (27.92%) Palo Alto Networks ( 4.16%) Dancho Danchev’s Blog (26.21%)
Blockchain on Medium ( 9.06%)

Trend Micro ( 4.11%)

ESET ( 2.82%)

Telegram
cybsecurity (76.70%)
VulnerabilityNews ( 8.28%)

Twitter
ecarlesi (76.42%)
threatmeter ( 4.73%)
Kaspersky ( 2.92%) Cybersecurity on Medium ( 8.63%) Cyber_Security_Channel ( 7.33%) malwrhunterteam ( 2.54%)
cKure ( 4.01%)
bgpstream ( 2.02%)
malwr ( 2.18%) YourAnonRiots ( 1.54%)
cryptolaemus1 ( 1.44%)
ActorExpose ( 1.32%)
canyoupwnme ( 0.16%) MalwarePatrol ( 0.98%)
1zrr4h ( 0.97%)
dubstard ( 0.74%)

Cisco Talos ( 6.57%)
Proofpoint ( 1.68%) BleepingComputer News ( 2.44%)
Cisco Talos ( 1.66%) Cointelegraph.com News ( 2.33%)
F5 Labs ( 1.28%)
Schneier on Security ( 1.26%)
Malwarebytes Labs ( 1.12%)
contagiodump ( 1.08%)

FireEye ( 1.65%)
BitDefender ( 1.44%)
360netlab ( 1.39%)
Microsoft ( 1.34%)

androidMalware ( 0.84%)
ckuRED ( 0.48%)

itsecalert ( 0.03%)
-

Table 6: Precision (P), recall (R), and F1 score achieved by each indicator extraction tool on the comparative evaluation over 4,420
documents. Only indicator types extracted by at least two tools are included.

Indicator
asn
bitcoin
cve
email
filename
filepath
fqdn
googleAdsense
googleAnalytics
ip4
ip4cidr
ip6
macAddress
md5
monero
regkey
sha1
sha256
sha512
ssdeep
url
All

Count
202
2,698
2,135
1,947
17,082
1,551
41,360
4
3
9,479
287
967
64
14,635
2
608
4,150
5,336
1
74
14,818
117,403

JAGER
R
-
-
0.99
0.85
0.97
-
0.06
-
-
0.87
-
-
-
0.98
-
-
1.00
1.00
1.00
0.28
0.56
0.57

F1
-
-
0.98
0.70
0.98
-
0.10
-
-
0.92
-
-
-
0.99
-
-
1.00
1.00
1.00
0.44
0.59
0.69

P
-
-
0.96
0.60
0.98
-
0.48
-
-
0.99
-
-
-
1.00
-
-
1.00
1.00
1.00
1.00
0.61
0.87

IOC_PARSER
R
P
-
-
-
-
1.00
1.00
0.75
0.89
0.71
0.97
0.66
0.73
0.91
0.99
-
-
-
-
0.92
0.98
-
-
-
-
-
-
1.00
1.00
-
-
0.72
0.96
0.99
1.00
1.00
1.00
-
-
-
-
0.56
0.53
0.85
0.91

F1
-
-
1.00
0.81
0.82
0.69
0.94
-
-
0.95
-
-
-
1.00
-
0.82
0.99
1.00
-
-
0.54
0.88

CACADOR
R
-
-
-
0.73
0.97
-
0.39
-
-
1.00
-
0.91
-
1.00
-
-
1.00
1.00
1.00
0.30
0.52
0.70

F1
-
-
-
0.66
0.92
-
0.46
-
-
0.98
-
0.65
-
0.62
-
-
0.58
0.99
0.12
0.21
0.52
0.65

P
-
-
-
0.60
0.87
-
0.56
-
-
0.97
-
0.50
-
0.45
-
-
0.41
0.97
0.07
0.16
0.51
0.61

CYOBSTRACT
R
P
0.95
0.93
-
-
0.97
0.98
0.98
0.99
0.87
0.78
0.97
0.29
0.96
0.97
-
-
-
-
0.98
1.00
0.99
1.00
0.66
0.87
-
-
0.97
1.00
-
-
0.90
0.80
0.99
1.00
0.94
1.00
-
-
0.88
0.81
0.96
0.76
0.96
0.92

F1
0.94
-
0.97
0.98
0.82
0.45
0.97
-
-
0.99
0.99
0.75
-
0.98
-
0.85
0.99
0.97
-
0.84
0.85
0.94

IOC-FINDER
R
1.00
1.00
1.00
1.00
-
0.76
0.99
1.00
1.00
0.99
1.00
0.12
1.00
1.00
1.00
0.73
1.00
1.00
1.00
0.91
0.83
0.96

F1
0.99
0.87
0.99
0.96
-
0.37
0.97
1.00
1.00
0.99
0.99
0.21
1.00
1.00
1.00
0.71
1.00
1.00
1.00
0.69
0.71
0.91

P
0.99
0.77
0.98
0.93
-
0.25
0.95
1.00
1.00
0.99
0.97
0.91
1.00
1.00
1.00
0.69
1.00
1.00
1.00
0.55
0.62
0.87

IOCEXTRACT
R
P
-
-
-
-
-
-
0.97
0.75
-
-
-
-
-
-
-
-
-
-
0.97
0.98
-
-
0.80
0.15
-
-
1.00
1.00
-
-
-
-
1.00
1.00
1.00
1.00
1.00
1.00
-
-
0.99
0.60
0.99
0.77

F1
-
-
-
0.85
-
-
-
-
-
0.98
-
0.25
-
1.00
-
-
1.00
1.00
1.00
-
0.75
0.87

IOC-EXTRACTOR
F1
R
0.81
0.97
1.00
1.00
0.98
1.00
0.98
0.99
-
-
-
-
0.95
0.99
1.00
1.00
0.86
1.00
0.99
1.00
-
-
0.66
0.97
1.00
1.00
1.00
1.00
1.00
1.00
-
-
1.00
1.00
1.00
1.00
1.00
1.00
0.64
1.00
0.74
0.80
0.93
0.96

P
0.69
1.00
0.96
0.97
-
-
0.92
1.00
0.75
0.98
-
0.51
1.00
1.00
1.00
-
1.00
1.00
1.00
0.47
0.70
0.90

SEARCHER
R
-
0.01
1.00
1.00
-
-
1.00
0.75
1.00
1.00
0.99
-
-
1.00
1.00
-
1.00
1.00
-
-
1.00
0.97

F1
-
0.03
1.00
1.00
-
-
0.99
0.86
1.00
1.00
0.99
-
-
1.00
1.00
-
1.00
1.00
-
-
0.87
0.96

P
-
1.00
1.00
0.99
-
-
0.98
1.00
1.00
1.00
1.00
-
-
1.00
1.00
-
1.00
1.00
-
-
0.77
0.95

the format AS1234 (e.g. from asn1234 to AS1234) and URLs by
prepending “http://” when no scheme is present.

The evaluation keeps for each tool counters for true positives
(TPs), false positives (FPs), false negatives (FNs), and true negatives
(TNs). The evaluation processes one document at a time, updating
the counters with the document results. For each document, it exam-
ines all indicators extracted from the document by at least one tool.
For each indicator, it generates three sets: the found set captures the
tools that identified the indicator in the document, the missed set
captures the tools that support the indicator type but did not identify
it, and the unsupported set captures the tools that do not support the
indicator type. If the size of the found set is larger than the size of the
missed set, then the evaluation assumes the majority is correct and
therefore the indicator was indeed present in the document. Thus,
it adds a TP for each tool in the found set and a FN for each tool
in the missed set. If the size of the missed set is larger than the size
of the found set, then the evaluation assumes the majority is correct
and therefore the indicator was not present in the document. Thus,
it adds a FP for each tool in the found set and a TN for each tool in
the missed set. If the size of the found and missed sets is the same,
then there is no majority. In such cases we skip the indicator and do
not update the counters. In future work, we plan to investigate how
to break such ties. After processing all documents, we compute the
precision, recall, and F1 score for each tool across all indicators, as
well as separately for each indicator type.

Handling errors. When running all tools on the 4,420 documents
we observed that IOCEXTRACT, the second most popular tool, needed
hours to process some documents, compared to seconds or a few
minutes for other tools. We suspected that one of their regular expres-
sion had a ReDoS vulnerability that caused a worst case triggered
only in some documents [11]. We found an open issue in the IO-
CEXTRACT repository regarding catastrophic backtracking in the
regular expression used to identify defanged URLs that modify the
backslash [10]. To be able to complete the evaluation in reasonable
time, we configured IOCEXTRACT to avoid using the problematic
regular expression. This change does not affect the accuracy results,
as IOCEXTRACT is the only tool supporting that defang transforma-
tion. Similarly, there are two documents where IOC-EXTRACTOR
does not terminate. We identify the root of this issue in the regular
expressions used to extract domain names. Of the four domain regu-
lar expressions used, only one avoids the problem, while the others
require hours to process the document. For these two documents,
we added IOC-EXTRACTOR to the missed set for all indicators in
the document. These results show that ReDoS vulnerabilities are a
serious problem for indicator extraction tools, but oftentimes these
issues occur only when examining large volume of documents.

There are also a few documents where a tool throws an exception
and thus extracts no indicators. This happens for CYOBSTRACT in
41 documents and for JAGER in 20. For CYOBSTRACT, since each
indicator is processed individually, only the indicators of the type

10

GoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

causing the exception are missed. We included JAGER in the missed
set for all indicators in those documents.

Results. Table 6 summarizes the results for the 21 indicator types
that are extracted by more than one tool. The columns show the
indicator type, the number of indicators of that type considered TPs,
as well as the precision (P), recall (R) and F1 score for each tool.
We do not include indicators extracted only by a single tool, as there
is no concept of majority for those.

Overall, SEARCHER is the tool that achieves both best precision
(0.95) and F1 score (0.96), while IOCEXTRACT achieves best recall
(0.99). The highest overall F1 scores are for SEARCHER (0.96),
CYOBSTRACT (0.94), IOC-EXTRACTOR (0.93), and IOC-FINDER
(0.91). The lowest F1 scores are for CACADOR (0.65) and JAGER
(0.69). These results seem to indicate that recent tools (SEARCHER,
IOC-EXTRACTOR, IOC-FINDER) perform better than those released
earlier (JAGER, CACADOR), possibly because newer tools could use
older ones for comparison.

The table also presents the accuracy results for each indicator
type. We observe perfect agreement for MAC address extraction
(although only 64 MAC addresses are found by two tools) and nearly
perfect agreement on the hashes, with the exception of CACADOR
that has low F1 scores for sha512 (0.12), sha1 (0.58), and md5 (0.62).
SEARCHER performs best in 11 of the 13 (85%) indicator types it
supports in the table, IOC-EXTRACTOR in 8 out of 17 (47%), and
IOC-FINDER in 9 out of 20 (45%). CYOBSTRACT is the best tool for
extracting registry keys (0.85) and SSDeep hashes (0.84); JAGER for
extracting filenames (0.98), IOC_PARSER for filepaths (0.69), and
SEARCHER for emails (1.0), domain names (0.99), IPv4 addresses
(1.0), and URLs (0.87). These results are useful for future work
in this area, to identify which prior tool may have the best regular
expression for an indicator type. For example, if we were to add
registry key support to SEARCHER we would start by looking at
CYOBSTRACT and for filenames to JAGER.

The indicator types with lowest agreement are IPv6 addresses
with F1 score ranging 0.21–0.75; filepaths (0.37–0.69); URLs (0.52–
0.87); and registry keys (0.71–0.85). These are arguably the indicator
types for which regular expressions are harder to build. We observe
that most IPv6 addresses extracted are actually FPs caused by a
regular expressions that matches serial numbers in certificates and
certificate fingerprints.

One surprising result is that SEARCHER has an F1 score of 0.03
on Bitcoin addresses, while it achieves best F1 score on most other
indicators it extracts. We manually checked the extracted Bitcoin
addresses and observed that SEARCHER is actually always correct
in identifying Bitcoin addresses. Both IOC-EXTRACTOR and IOC-
FINDER return hashes that are erroneously included as Bitcoin ad-
dresses. SEARCHER avoids those FPs by examining the checksum
in the regular expression matches, which do not validate. This is
an example of the minority of tools being correct and the majority
being wrong. In general, the more tools support an indicator, the
higher confidence we have on the majority results. An alternative
approach for evaluation tools is to use a ground truth (GT) dataset.
However, the manual effort required to build a GT with thousands
of documents would be very large, and it would be difficult to cover
different indicator types. For this reason regular expression eval-
uations tend to be built leveraging synthetic examples (e.g., [31]).

Table 7: Top-10 indicator types extracted by all tools.

Indicator
fqdn
attackType
country
filename
url
md5
ip4
sha256
avLabel
sha1

Count
41,360
26,109
17,654
17,082
14,818
14,635
9,479
5,336
4,660
4,150

Tools
7
1
1
4
8
8
8
8
1
8

Table 8: Indicator extraction runtime (in seconds) across all
APTnotes and average by indicator type.

Tool
CACADOR
IOCEXTRACT
IOC_PARSER
IOC-EXTRACTOR
JAGER
SEARCHER
CYOBSTRACT
IOC-FINDER

Runtime (sec)
489.23
1350.69
1563.33
1978.51
2360.02
2847.98
6945.03
96596.23

Ind.
12
8
11
18
11
41
25
25

Avg. per Ind. (sec)
40.76
168.83
142.12
109.91
214.54
69.46
277.80
3863.84

Unfortunately, synthetic examples may not represent results on real
documents.

Table 7 shows the Top 10 most popular indicator types across
the 4,420 documents. The count column represents the number of
TPs, and the last column reports the number of tools that extract
each indicator. Among those, 7 correspond to indicators supported
by most tools, namely domain names, file names, URLs, IPv4 ad-
dresses, and hashes (md5, sha256, sha1). On the other hand, the
other three indicators are extracted only by CYOBSTRACT and corre-
spond to attack-type keywords, countries, and AV labels. A total of
16 indicator types are not found in the 4,420 reports. These include
attribution, groupName, authentihash, ip6range, useragent, iban,
icp, 7 blockchain addresses (dashcoin, dogecoin, ethereum, litecoin,
tezos, webmoney, zcash), and two social handles (telegramHandle,
whatsappHandle). These correspond to less popular indicators in
APTnotes and ChainSmith reports. However, other sources may dif-
fer. For example, RSS collection includes blockchain-related blogs,
which leads to SEARCHER extracting indicators for all blockchain
addresses in Section 5 (e.g., 726 ethereum addresses).

Runtime. Table 8 shows the total time in seconds each tool took
to extract indicators on the 4,420 text documents, the number of
indicator types supported, and the average time by indicator type.
In our tests the fastest tool is CACADOR, likely due to being im-
plemented in Go and compiled to native code. The slowest tool is
IOC-FINDER, which is 14 times slower than the second slowest tool
(CYOBSTRACT). This is likely due to IOC-FINDER being the only
tool that uses grammars for the extraction, which highlights the effi-
ciency of using regular expressions to extract indicators. All tools
perform one pass on the text for each regular expression and they
typically use one regular expression for each indicator type. Thus,
the runtime is highly influenced by the number of indicator types
extracted. When normalizing the total runtime by the number of
extracted indicator types, CACADOR is still fastest with 40.8 seconds,
followed by SEARCHER (69.5), and IOC-EXTRACTOR (109.9).

11

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

Table 9: Social indicators validation.

Indicator
facebookHandle
gitHubHandle
instagramHandle
pinterestHandle
youTubeHandle
youTubeChannel
twitterHandle
All

Total
150
258
10
1
24
6
516
965

Validated
124
242
8
1
24
6
431
836

Social handle validation. Our majority-based evaluation cannot be
applied to the 38 indicators only extracted by one tool. Among those,
there are 11 social indicators only extracted by SEARCHER, of which
9 appear in APTnotes and ChainSmith reports (no Telegram and
WhatsApp handles are found in these sources). For social network
handles we perform an alternative validation, which checks whether
there currently exists an account in the social network for that han-
dle. For this, we use the Blackbird [37] open-source tool, which
given a username checks if there currently exists an account with
that handle on 143 different social networks. Blackbird supports 6
social networks SEARCHER extracts handles for (Facebook, GitHub,
Instagram, Pinterest, Telegram and YouTube). In addition, we ex-
panded Blackbird to also support youtubeChannel indicators. For
Twitter handles, we leverage the official Twitter API for the same
purpose [55]. LinkedIn does not allow searching for account names.
Table 9 summarizes the results. Our automated approach success-
fully validated 87% of the social indicators. We manually inspect
the 102 indicators that we could not validate with Blackbird, using a
web browser to search for a particular handle on each social network.
For Twitter, 15 of of the 85 usernames that did not validate, were
linked to suspended accounts (e.g., @MalwareSigs). In all of the
remaining cases the social networks returned a page to inform us
that the particular account does not exist. For 75 of the 87 accounts
for which we could not find the user profile page, the identifier con-
tained a meaningful sequence of characters (e.g., “sucuri_security”,
“avast_antivirus”), suggesting that the identifier could belong to an
old account that was closed. Overall, this evaluation suggests that
over 90% of the social identifiers SEARCHER extracted are likely
true positives. For the rest we cannot determine if they are false
positives or correspond to accounts that have been closed since they
appeared in the sources.

7 CASE STUDY: BLOCKCHAIN ADDRESS

TAGGING

Cryptocurrencies such as Bitcoin are frequently abused by cybercrim-
inals. A key property of Bitcoin is that its transaction ledger is public,
which allows to inspect and analyze all transactions. Related work
has leveraged this visibility on transactions for tracking payments
from victims of ransomware [19, 38] and scams [32, 39]; buying and
selling of illegal items in underground marketplaces [9, 26], and iden-
tifying malware C&C channels hosted on the blockchain [40, 54].
Bitcoin addresses are pseudo-anonymous, but in some cases it is
possible to identify a set of addresses owned by an entity, e.g., by
transacting with a service [32] or by analyzing malware samples [23].
Related work that analyzed malicious Bitcoin transactions leveraged
Bitcoin tagging databases. These databases contain Bitcoin addresses
together with information on whether the address is malicious or

benign and who the owner is. Tagging databases are a cornerstone
of many commercial blockchain analysis companies [1, 2].

Building and updating tagging databases is currently a largely
manual process, and analysts are in desperate need of tools to assist
them. This case study shows how we leveraged GoodFATR to assist
us in building and maintaining a Bitcoin tagging database. We used
this database in a research project related to identifying relationships
between cybercriminals that abuse Bitcoin [15].

To generate the tagging database we included blockchain-related
blogs into the list of RSS origins input to GoodFATR. Every day,
GoodFATR collects new RSS entries, downloads the documents
using the entry URLs, and extracts the indicators. On a weekly basis,
a cron job selects the Bitcoin addresses identified over the week,
tags them with the URLs of the reports from where the addresses
have been extracted, filters those already present in our tag database,
and sends an email with the untagged addresses to an analyst.

It is worth noting we do not know if addresses reported in the
email were used only in legitimate transactions, as our Bitcoin ad-
dress validation step only discards indicators with an invalid check-
sum, without further filtering. We are interested in collecting both
malicious and benign addresses, as cybercriminals often abuse ad-
dresses of legitimate services (e.g., cryptocurrency exchanges). After
receiving the list of new Bitcoin addresses, the analyst visits the re-
port URLs (or examines the documents downloaded by GoodFATR)
and manually tags each address. To this end, the analysis searches
for the address in the report text, reads the part of the report in which
the address is mentioned, and tags the address accordingly.

Using this approach, GoodFATR is able to gather a continuous
feed of fresh Bitcoin addresses, which can be tagged within a few
days since they were first published in one of our monitored origins.
The tagged addresses are added to the database used by our auto-
mated system [15]. Malicious addresses can also be used as input
to the automated system, so that they their transactions are traced.
In a period of 1.3 years (68 weeks), this approach automatically
identified 854 new Bitcoin addresses, for an average of 12.6 Bitcoin
addresses per week, a median of one address, a maximum of 497,
minimum of zero, and a standard deviation of 60.4. In future work,
we plan to investigate NLP techniques to automate the tagging of
Bitcoin address with context information obtained from the threat
report, thus avoiding any manual steps by the analyst.

8 RELATED WORK
Over time, several threat intelligence tools and platforms have been
developed [7, 8, 20–22, 28, 57, 58]. These platforms focus on the au-
tomatic extraction of IOCs from different sources including technical
articles [20, 28, 57, 58], social networks [34, 49], and public/private
IOC feeds [27] . Regular expressions are a common choice for IOC
extraction, as most IOCs follow a well defined structure that can be
matched, e.g., IP addresses are formatted as a set of four base-10
written hexadecimal bytes separated by dots. Commonly, such plat-
forms focus on a limited set of IOCs (URLs, IP addresses, and file
hashes). Our approach also relies on regular expressions to detect
IOCs, but we cover a significantly larger number of IOC classes
(i.e., network and social handles, contact information, CVEs), and
support a variety of document formats (HTML, PDF, plain text).

12

GoodFATR: A Platform for Automated Threat Report Collection and IOC Extraction

Moreover, GoodFATR uses a real web browser to download docu-
ments, allowing us to follow complex redirection chains and render
dynamic content. A common challenge is to select the best sources
to track. Niakanlahiji et al. [34] combine graph theory, machine
learning (ML), and text mining to build reputation models to identify
valuable Twitter users to follow, based on their posts. Li et al. [27]
define a special set of metrics that can be used to measure the quality
of an IOC feed. We start with a manually generated list of origins,
and then perform the analysis in Section 5 to identify the best and
worst contributors. This is possible because GoodFATR can back-
track each IOC to its origin, and can aggregate them to identify the
most active sources, as well as sources that specialize on particular
indicator types. We can easily include new sources to our platform,
and remove those that are less active, or that do not generate IOCs.
In addition, the validation of the IOCs found is considered an open
problem. For example, in [8] authors propose to identify web-IOCs
by searching for external resources added to a web application once
it gets compromised. Still, it is extremely challenging to figure out
that an external resource (e.g., a URL or a JavaScript snippet) is
indeed an indicator of the attack, and not just a generic indicator that
appears also in non-compromised web sites. Our platform leverages
the insight that some IOCs can be validated, significantly lowering
generic indicators, e.g., Bitcoin addresses can be easily validated us-
ing their embedded checksum. In addition, GoodFATR also includes
filtering to remove generic IOCs, e.g., a URL whose domain is the
same as an origin we track.

9 CONCLUSIONS
We have presented GoodFATR, an automated platform for collecting
threat reports from 6 sources and extracting IOCs from the collected
reports. GoodFATR continuously monitor the sources, downloads
new threat reports, extracts indicators from the reports, and filters
generic indicators to produce a list of IOCs. GoodFATR includes
the SEARCHER tool for extracting 41 indicator types from HTML,
PDF, and text files using regular expressions. We have compared 7
popular indicator extraction tools with SEARCHER and compared
their accuracy using a novel majority-vote methodology. We have run
our platform for over 15 months to collect 472,891 reports from the
6 sources; extract 1,043,932 indicators from the reports; and identify
655,971 IOCs. We have analyzed the collected data to identify the
top IOC contributors and the IOC class distribution. Finally, we have
presented a case study on how GoodFATR can assist in tracking
cybercrime relations on the Bitcoin blockchain.

ACKNOWLEDGMENTS
This work has been partially supported by the Madrid regional gov-
ernment as part of the program S2018/TCS-4339 (BLOQUES-CM),
co-funded by EIE Funds of the European Union, and by Atrac-
ción de Talento grant (Ref. 2020-T2/TIC-20184). This work has
been partially supported by the SCUM Project (RTI2018-102043-
B-I00) MCIN/AEI/10.13039/501100011033/ERDF and by grant
PRE2019-088472/MCIN/AEI /10.13039/501100011033, ESF In-
vesting in your future. Partial funding was also provided by Ministe-
rio de Ciencia, Innovación y Universidades FPU grant FPU18/06416.

13

Any opinions, findings, and conclusions or recommendations ex-
pressed in this material are those of the authors or originators, and
do not necessarily reflect the views of the sponsors.

REFERENCES
[1] 2022. Chainalysis. https://www.chainalysis.com/.
[2] 2022. Elliptic. https://www.elliptic.co/.
[3] Fernando Alves, Ambrose Andongabo, Ilir Gashi, Pedro M. Ferreira, and Alysson
Bessani. 2020. Follow the Blue Bird: A Study on Threat Data Published on Twitter.
In ESORICS.

[4] Benjamin Andow, Samin Yaseer Mahmud, Wenyu Wang, Justin Whitaker, William
Enck, Bradley Reaves, Kapil Singh, and Tao Xie. 2019. PolicyLint: Investigating
Internal Privacy Policy Contradictions on Google Play. In USENIX Security.
[5] Kiran Bandla and Santiago Castro. 2022. APTnotes. https://github.com/aptnotes/

data.

[6] Xander Bouwman, Harm Griffioen, Jelle Egbers, Christian Doerr, Bram Klievink,
and Michel Van Eeten. 2020. A different cup of TI? The added value of commercial
threat intelligence. In USENIX Security Symposium.

[7] Armin Buescher. 2017. https://github.com/armbues/ioc_parser/.
[8] Onur Catakoglu, Marco Balduzzi, and Davide Balzarotti. 2016. Automatic Ex-

traction of Indicators of Compromise for Web Applications. In WWW.

[9] Nicolas Christin. 2013. Traveling the Silk Road: A measurement analysis of a
large anonymous online marketplace. In The World Wide Web Conference.
[10] DaveCrim. 2021. Catastrophic backtracking in BACKSLASH_URL_RE. https:

//github.com/InQuest/python-iocextract/issues/52.

[11] James C Davis, Christy A Coghlan, Francisco Servant, and Dongyoon Lee. 2018.
The Impact of Regular Expression Denial of Service (ReDoS) in Practice: An
Empirical Study at the Ecosystem Scale. In ACM Joint Meeting on European
Software Engineering Conference and Symposium on the Foundations of Software
Engineering.

[12] FIRST. 2016. Traffic Light Protocol (TLP). FIRST Standards Definitions and

Usage Guidance - Version 1.0. https://www.first.org/tlp/docs/tlp-v1.pdf.

[13] Peng Gao, Fei Shao, Xiaoyuan Liu, Xusheng Xiao, Zheng Qin, Fengyuan Xu,
Prateek Mittal, Sanjeev R Kulkarni, and Dawn Song. 2021. Enabling Efficient
Cyber Threat Hunting With Cyber Threat Intelligence. In IEEE International
Conference on Data Engineering.

[14] Will Gibb and Devon Kerr. 2013. OpenIOC: Back to the Basics. https://www.ma

ndiant.com/resources/openioc-basics.

[15] Gibran Gomez, Pedro Moreno-Sanchez, and Juan Caballero. 2022. Detecting
Cybercriminal Bitcoin Relationships through Backwards Exploration. (2022).
https://doi.org/10.48550/arXiv.2206.00375

[16] Hamza Harkous, Kassem Fawaz, Rémi Lebret, Florian Schaub, Kang G. Shin,
and Karl Aberer. 2018. Polisis: Automated Analysis and Presentation of Privacy
Policies Using Deep Learning. In USENIX Security.

[17] Floyd Hightower. 2018. IOC Finder. https://github.com/fhightower/ioc-finder.
[18] Henry Hosseini, Martin Degeling, Christine Utz, and Thomas Hupperich. 2021.

Unifying Privacy Policy Detection. In PoPETs.

[19] D. Y. Huang, M. M. Aliapoulios, V. G. Li, L. Invernizzi, E. Bursztein, K.
McRoberts, J. Levin, K. Levchenko, A. C. Snoeren, and D. McCoy. 2018. Track-
ing Ransomware End-to-end. In IEEE Symposium on Security and Privacy.
https://doi.org/10.1109/SP.2018.00047

[20] Ghaith Husari, Ehab Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. 2017.
TTPDrill: Automatic and Accurate Extraction of Threat Actionsfrom Unstructured
Text of CTI Sources. In ACSAC.

[21] InQuest. 2019. iocextract. https://github.com/InQuest/python-iocextract.
[22] InQuest. 2020. ThreatIngestor. https://github.com/InQuest/ThreatIngestor.
[23] Kobi Eisenkraft and Arie Olshtein. 2022. Pony’s C&C servers hidden inside the
Bitcoin blockchain. https://research.checkpoint.com/2019/ponys-cc-servers-
hidden-inside-the-bitcoin-blockchain/.

[24] Christian Kohlschütter. 2022. boilerpipe: Boilerplate Removal and Fulltext Ex-

traction from HTML pages. https://github.com/kohlschutter/boilerpipe.

[25] Jesse Kornblum. 2016. ssdeep Project. https://ssdeep-project.github.io/ssdeep/ind

ex.html.

[26] Seunghyeon Lee, Changhoon Yoon, Heedo Kang, Yeonkeun Kim, Yongdae Kim,
Dongsu Han, Sooel Son, and Seungwon Shin. 2019. Cybercriminal Minds: An
Investigative Study of Cryptocurrency Abuses in the Dark Web. In Network and
Distributed Systems Security Symposium.

[27] Vector Guo Li, Matthew Dunn, Paul Pearce, Damon McCoy, Geoffrey M. Voelker,
and Stefan Savage. 2019. Reading the Tea leaves: A Comparative Analysis of
Threat Intelligence. In USENIX Security.

[28] Xiaojing Liao, Kan Yuan, XiaoFeng Wang, Zhou Li, Luyi Xing, and Raheem
Beyah. 2016. Acing the IOC Game: Toward Automatic Discovery and Analysis
of Open-Source Cyber Threat Intelligence. In CCS.

[29] Mandiant. 2014. Tracking Malware with Import Hashing. https://www.mandiant

.com/resources/tracking-malware-import-hashing.

Juan Caballero, Gibran Gomez, Srdjan Matic, Gustavo Sánchez, Silvia Sebastián, and Arturo Villacañas

[30] MarketWatch. 2022. Threat Intelligence Market 2022 Report. https://www.mark
etwatch.com/press-release/threat-intelligence-market-2022-report-examines-
latest-trends-and-key-drivers-supporting-growth-till-2030-2022-07-27.

[31] Mathias Bynens. 2021. In search of the perfect URL validation regex. https:

//mathiasbynens.be/demo/url-regex.

[32] Sarah Meiklejohn, Marjori Pomarole, Grant Jordan, Kirill Levchenko, Damon
McCoy, Geoffrey M. Voelker, and Stefan Savage. 2013. A Fistful of Bitcoins:
Characterizing Payments among Men with No Names. In Internet Measurement
Conference.

[46] Scott J. Roberts. 2016. Cacador. https://github.com/sroberts/cacador.
[47] Carl Sabottke, Octavian Suciu, and Tudor Dumitras. 2015. Vulnerability Disclo-
sure in the Age of Social Media: Exploiting Twitter for Predicting Real-World
Exploits. In USENIX Security.

[48] Kiavash Satvat, Rigel Gjomemo, and VN Venkatakrishnan. 2021. Extractor:
Extracting Attack Behavior from Threat Reports. In IEEE European Symposium
on Security and Privacy.

[49] Hyejin Shin, WooChul Shim, Saebom Kim, Sol Lee, Yong Goo Kang, and Yong Ho

Hwang. 2021. Twiti: Social Listening for Threat Intelligence. In WWW.

[33] Mozilla. 2022. Mozilla Readability.js library. https://github.com/mozilla/readabi

[50] Yusuke Shinyama, Philippe Guglielmetti, and Pieter Marsman. 2019. pdminer.six.

lity.

[34] Amirreza Niakanlahiji, Lida Safarnejad, Reginald Harper, and Bei-Tseng Chu.
2019. IoCMiner: Automatic Extraction of Indicators of Compromise from Twitter.
In IEEE Big Data.

[35] Manabu Niseki. 2019. https://github.com/ninoseki/ioc-extractor.
[36] OASIS Open. 2022. STIX: A structured language for cyber threat intelligence.

https://oasis-open.github.io/cti-documentation/.

[37] p1ngul1n0. 2022. Blackbird. https://github.com/p1ngul1n0/blackbird.
[38] Masarah Paquet-Clouston, Bernhard Haslhofer, and Benoit Dupont. 2019. Ran-
somware Payments in the Bitcoin Ecosystem. Journal of Cybersecurity 5, 1
(2019).

[39] Masarah Paquet-Clouston, Matteo Romiti, Bernhard Haslhofer, and Thomas Char-
vat. 2019. Spams Meet Cryptocurrencies: Sextortion in the Bitcoin Ecosystem. In
ACM Conference on Advances in Financial Technologies.

[40] S. Pletinckx, C. Trap, and C. Doerr. 2018. Malware Coordination using the
Blockchain: An Analysis of the Cerber Ransomware. In IEEE Conference on
Communications and Network Security.

[41] Daniel Plohmann, Martin Clauß, Steffen Enders, and Elmar Padilla. 2017. Malpe-
dia: A Collaborative Effort to Inventorize the Malware Landscape. The Journal
on Cybercrime & Digital Investigations 3, 1 (2017).

[42] Victor Le Pochat, Tom Van Goethem, Samaneh Tajalizadehkhoob, Maciej Ko-
rczy´nski, and Wouter Joosen. 2019. Tranco: A Research-Oriented Top Sites
Ranking Hardened Against Manipulation. In NDSS.

[43] Python Software Foundation. 2022. Requests. https://github.com/psf/requests.
[44] Leonard Richardson. 2022. Beautiful Soup. https://beautiful-soup-4.readthedocs.

io/en/latest/.

[45] Scott J. Roberts. 2015. jager. https://github.com/sroberts/jager.

https://github.com/pdfminer/pdfminer.six.

[51] Matt Sisk, Robin Ruefle, and Sam Perl. 2018. Harvesting Artifacts: Improving
Useful Data Extraction from Cybersecurity Incident Reports. https://github.com/c
mu-sei/cyobstract.

[52] Rocky Slavin, Xiaoyin Wang, Mitra Bokaei Hosseini, James Hester, Ram Krishnan,
Jaspreet Bhatia, Travis D. Breaux, and Jianwei Niu. 2016. Toward a Framework for
Detecting Privacy Policy Violations in Android Application Code. In International
Conference on Software Engineering.

[53] Software Freedom Conservancy. 2022. Selenium. https://www.selenium.dev/.
[54] Tsuyoshi Taniguchi, Harm Griffioen, and Christian Doerr. 2021. Analysis and
Takeover of the Bitcoin-Coordinated Pony Malware. In ACM ASIA Conference on
Computer and Communications Security.

[55] Twitter. 2022. Twitter API. https://developer.twitter.com/en/docs/twitter-api.
[56] WebMoney. 2022. WebMoney - Universal Payment System. https://www.wmtran

sfer.com/.

[57] Jun Zhao, Qiben Yan, Jianxin Li, Minglai Shao, Zuti He, and Bo Li. 2020. TIMiner:
Automatically extracting and analyzing categorized cyber threat intelligence from
social data. Computers & Security (2020).

[58] Ziyun Zhu and Tudor Dumitras. 2018. ChainSmith: Automatically Learning the
Semantics of Malicious Campaigns by Mining Threat Intelligence Reports. In
Euro S&P.

[59] Sebastian Zimmeck and Steven M Bellovin. 2014. Privee: An Architecture for

Automatically Analyzing Web Privacy Policies. In USENIX Security.

[60] Sebastian Zimmeck, Ziqi Wang, Lieyong Zou, Roger Iyengar, Bin Liu, Florian
Schaub, Shomir Wilson, Norman M. Sadeh M, Steven M. Bellovin, and Joel R.
Reidenberg. 2017. Automated Analysis of Privacy Requirements for Mobile Apps.
In NDSS.

14

