ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

1

Latency Optimization for Blockchain-Empowered Federated
Learning in Multi-Server Edge Computing

Dinh C. Nguyen, Member, IEEE, Seyyedali Hosseinalipour, Member, IEEE, David J. Love, Fellow, IEEE,
Pubudu N. Pathirana, Senior Member, IEEE, and Christopher G. Brinton, Senior Member, IEEE

2
2
0
2

l
u
J

4

]

G
L
.
s
c
[

2
v
0
7
6
9
0
.
3
0
2
2
:
v
i
X
r
a

Abstract—In this paper, we study a new latency optimization
problem for blockchain-based federated learning (BFL) in multi-
server edge computing. In this system model, distributed mobile
devices (MDs) communicate with a set of edge servers (ESs) to
handle both machine learning (ML) model training and block
mining simultaneously. To assist the ML model training for
resource-constrained MDs, we develop an ofﬂoading strategy that
enables MDs to transmit their data to one of the associated ESs.
We then propose a new decentralized ML model aggregation
solution at the edge layer based on a consensus mechanism to
build a global ML model via peer-to-peer (P2P)-based blockchain
communications. Blockchain builds trust among MDs and ESs to
facilitate reliable ML model sharing and cooperative consensus
formation, and enables rapid elimination of manipulated models
caused by poisoning attacks. We formulate latency-aware BFL as
an optimization aiming to minimize the system latency via joint
consideration of the data ofﬂoading decisions, MDs’ transmit
power, channel bandwidth allocation for MDs’ data ofﬂoading,
MDs’ computational allocation, and hash power allocation. Given
the mixed action space of discrete ofﬂoading and continuous
allocation variables, we propose a novel deep reinforcement
learning scheme with a parameterized advantage actor critic
algorithm. We theoretically characterize the convergence prop-
erties of BFL in terms of the aggregation delay, mini-batch
size, and number of P2P communication rounds. Our numerical
evaluation demonstrates the superiority of our proposed scheme
over baselines in terms of model training efﬁciency, convergence
rate, system latency, and robustness against model poisoning
attacks.

Index Terms—Federated learning, Blockchain, edge comput-

ing, actor-critic learning, network optimization.

I. INTRODUCTION

In recent years, the demand for deploying machine learning
(ML) in wireless networks and Internet of Things (IoT) appli-
cations has increased dramatically. However, due to growing
concerns associated with data privacy, it is infeasible to trans-
mit all collected data from IoT edge devices to a central loca-
tion (e.g., a datacenter) for model training. Federated learning
(FL) has emerged as a popular approach for distributed ML
which allows for model training without requiring data sharing
[1], [2]. Under FL, devices operate as workers to train local
ML models using their own datasets, and exchange their model
updates with an aggregator, e.g., an edge server (ES), over
multiple communication rounds to converge on a global model.
While FL distributes the data processing step across devices,

Dinh C. Nguyen, Seyyedali Hosseinalipour, David J. Love, and Christopher
G. Brinton are with the Elmore Family School of Electrical and Computer
Engineering, Purdue University, USA (e-mails: {nguye772, hosseina, djlove,
cgb}@purdue.edu)
Pubudu N.

Pathirana

is with

School

the

of

Ponds, VIC 3216, Australia

Engineering,
(e-mail:

Deakin University, Waurn
pubudu.pathirana@deakin.edu.au).

the model aggregation step is still often carried out at a single
location, which imposes security issues including single-point-
of-failure and server malfunction. Additionally,
this poses
scalability restrictions for ML training processes, especially as
the number of IoT devices involved and their geographic reach
continue to expand [3]. Therefore, it is desirable to develop
a more decentralized FL architecture for realizing scalable
model training while preserving security in next-generation
intelligent networks.

In this context, blockchain is a promising technology for
enabling reliable decentralized FL via its peer-to-peer (P2P)
networking topology empowered by an immutable, transparent
and tamper-proof data ledger [4]. The use of blockchain in
FL can mitigate the single-point-of-failure issue, and builds
trust between devices and multiple servers for secure ML
model
training [5]. Given these beneﬁts, blockchain-based
federated learning (BFL) has been recently investigated in
different domains, such as vehicular communications [6] and
mobile crowdsensing [7]. To implement BFL systems, devices
need to interface with decentralized servers in settings where
communication and computation resources are limited, which
will impact the ML model training quality. Moreover, each
device exhibits interest in participating in block mining to
further gain blockchain rewards, e.g., cryptocurrency tokens,
which in turn enhances the reliability and security for FL. This
leads to the concept of mobile mining which has been adopted
in practical BFL environments [8], [9]. The concurrence of
both ML model training and block mining introduces new
challenges to network service latency and resource man-
agement, motivating a holistic optimization architecture for
efﬁcient BFL in wireless networks.

A. Related Works

We summarize related works in latency optimization and

resource allocation for standard FL and decentralized BFL.

1) Standard FL: Latency optimization has recently received
signiﬁcant attention in FL research. The work in [10] proposed
a joint device scheduling and bandwidth allocation frame-
work for wireless FL to improve the convergence rate of
ML model training. Another study in [11] investigated semi-
asynchronous FL, focusing on the convergence analysis of
model training under edge heterogeneity and non-independent
and identically distributed (non-IID) data distributions across
the edge devices. An asynchronous FL scheme was considered
in [12] for unmanned aerial vehicles (UAVs)-assisted networks
to minimize the model exchange latency and ML training
learning (DRL). To mitigate
loss via deep reinforcement

 
 
 
 
 
 
ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

2

Table I: Comparison of methodology design features between
our paper and related works in latency optimization and
resource allocation for FL.

Features

[10]

[13]

consensus-based

ofﬂoading-assisted

FL design in multi-edge
servers
Data
FL
P2P
model aggregation
Blockchain-based FL de-
sign
DRL-based resource allo-
cation
Parameterized A2C design
FL latency optimization

(cid:88)

(cid:88)

(cid:88)

Our
work

(cid:88)

(cid:88)

(cid:88)

(cid:88)

[22]

[25]

[26]

(cid:88) (cid:88)

[27],
[28]

(cid:88) (cid:88)

[18],
[19]

(cid:88)

(cid:88)

(cid:88)

(cid:88) (cid:88)

(cid:88)
(cid:88)

(cid:88)

straggler effects caused by resource-limited clients, the authors
in [13] presented a partial ofﬂoading-assisted FL scheme using
game theory. A partial ofﬂoading-based FL solution was also
proposed in [14] for edge computing, focusing on the delay
analysis of the data ofﬂoading and model update. Similarly, a
convex optimization approach was applied in [15] to minimize
the energy consumption of model updating and sharing.

2) Decentralized and Blockchain-based FL: Several recent
works have considered techniques for decentralizing FL aggre-
gation schemes. The authors in [16] introduced a decentralized
FL scheme based on model segmentation with a gossip pro-
tocol for client sampling in each model aggregation round.
In [17], a decentralized FL solution was proposed using the
device-to-device (D2D) concept in serverless edge networks,
with a graph-coloring based scheduling policy to characterize
the data communications between two devices. Our recent
works in [18], [19] developed D2D-based semi-decentralized
and hybrid FL frameworks, where we jointly modeled com-
munication efﬁciency and statistical heterogeneity of data and
obtained new convergence bounds for distributed ML.

Recently, the integration of blockchain into FL has been
investigated. The study in [20] analyzed end-to-end latency
for ML model training, update transmission and block mining
in a BFL system. [21] analyzed the communication latency and
consensus delays for BFL-based vehicular networks, deriving
an optimal block arrival rate for vehicles based on system
dynamics. The authors in [22] focused on developing a band-
width allocation and device scheduling solution for digital
twin-enabled BFL. Moreover,
the work in [23] proposed
a BFL-based privacy-preserving UAV network to optimize
the energy consumption of UAVs, vehicular device service
coverage and a composite service hit ratio. In recent work
[24], a dynamic resource allocation framework for BFL was
proposed with a focus on maximizing the training data size
with respect to energy usage constraints.

B. Motivations and Key Contributions

Despite such research efforts, several limitations still exist

in current BFL works, which are highlighted below:

• Most current standard FL [10], [11], [14] and decen-
tralized FL frameworks [18] still rely on a single ES
to coordinate model aggregations. In these architectures,

single-point-of-failure bottlenecks may disrupt the entire
FL system if the server is attacked. Only the work in [22]
has considered a multi-server edge computing model for
BFL, but its model aggregation still follows traditional
FL.

• End-to-end latency optimization,

i.e., for both model
training and block mining, remains understudied in cur-
rent BFL systems [20]–[22]. Existing works mostly aim
to characterize, rather than optimize, BFL latency. More-
over, the beneﬁts of blockchain to support robust BFL
training against model attacks have not been yet investi-
gated.

• The potential beneﬁts of edge computing have not been
well exploited in existing BFL schemes. Most works
[12], [15], [24] have not considered practical resource
constraints of mobile IoT devices and the potential for
ESs to mitigate resulting straggler effects. Only [13], [14]
have considered such a scenario, with the focus instead
on partial ofﬂoading for traditional FL.

• None of the existing works have analyzed the conver-
gence properties of BFL in a multi-server system. A
comprehensive theoretical analysis will provide insights
into BFL operations, leading to potential optimization
techniques.

Motivated by the aforementioned limitations, we propose a
novel consensus-based BFL model for efﬁcient and robust ML
model training via blockchain. Speciﬁcally, we develop a new
cooperative ofﬂoading-assisted model learning and resource
trading-assisted block mining framework for FL. We then
propose a partial model aggregation solution for facilitating
global model aggregations at the edge layer using blockchain-
enabled P2P communications. Blockchain is important for
our methodology in two key ways: (i) it builds trust among
MDs and ESs to facilitate reliable ML model sharing and
cooperative consensus formation for our federated learning ap-
proach; and (ii) it allows for rapid elimination of manipulated
models from compromised ESs caused by poisoning attacks,
thereby enhancing the robustness of global model training.
The system latency is subsequently formulated by considering
both model learning latency and block mining latency, which is
then optimized by a parameterized actor-critic algorithm. The
comparison of our paper with related works in terms of several
key design features is summarized in Table I. In summary, the
unique contributions of this paper are:

1) We propose a multi-server-assisted BFL architecture,
where geo-distributed mobile devices (MDs) commu-
nicate with a set of ESs for ML model training and
block mining simultaneously. To mitigate straggler ef-
fects caused by resource-constrained MDs, an ofﬂoading
strategy is proposed that enables MD data transmission
to an ES for ML model training. Moreover, we develop a
resource trading strategy to alleviate block mining latency
from resource-limited MDs.

2) We provide a holistic convergence analysis of BFL. In
doing so, we consider a new partial model aggregation
solution for facilitating global model aggregations at the
edge layer via P2P-based blockchain communications.

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

3

Our resulting bound reveals the impact of the aggregation
delay, mini-batch size, and number of P2P communica-
tion rounds on the convergence rate.

3) We formulate a new system latency minimization prob-
taking into account both ofﬂoading-assisted ML
lem,
model
training latency, model consensus latency and
block mining latency. This optimization couples data
ofﬂoading decisions, MD transmit powers, and the allo-
cation of channel bandwidth, MD computation, and hash
power resources to minimize latency with a model quality
constraint.

4) To solve the resulting optimization over a mixed dis-
crete and continuous solution space, we propose a novel
DRL method based on a parameterized advantage actor
critic (A2C) algorithm. We provide a holistic design of
the actor,
including ofﬂoading and allocation policies
empowered by trust region policy optimization (TRPO),
along with a critic for the state-value training.

5) We

experiments

conduct numerical

for both our
consensus-based BFL and parameterized A2C schemes.
The results reveal
that our BFL scheme outperforms
existing FL approaches in terms of model loss and accu-
racy convergence in both IID and non-IID data settings.
Our proposed parameterized A2C scheme also helps
lower the system latency by up to 38% compared with
state-of-the-art DRL schemes. Moreover, our blockchain-
empowered BFL scheme shows high robustness against
model poisoning attacks.

C. Paper Organization

The remainder of this paper is organized as follows. Sec-
tion II presents the BFL architecture and its different compo-
nents including the ML training and model aggregation pro-
cedures. Moreover, we conduct an analysis of the BFL model
training to characterize the impact of different system parame-
ters on learning convergence. In Section III, we formulate the
corresponding system latency minimization problem, taking
into account ofﬂoading-assisted ML model training latency,
model consensus latency, and block mining latency. To solve
the formulated problem, we propose a DRL method based
on a parameterized advantage actor critic (A2C) algorithm
in Section IV, where the utility measure captures the latency
objective and the action space is restricted by the learning
and resource constraints. We present experiments comparing
latency and accuracy obtained by our methodology against
several baselines in Section V. Finally, Section VI concludes
the paper. The key acronyms and notations used in this paper
are summarized in Table II and Table III, respectively.

II. SYSTEM MODEL
In this section, we describe our BFL system and detail the
BFL task model, and then analyze its convergence properties.

A. Overall System Architecture

Our proposed BFL architecture is illustrated in Fig. 1. Each
ES is located at a base station (BS) to provide computation

Table II: List of key acronyms.

Acronym Deﬁnition
FL
BFL

Acronym Deﬁnition
ML
MD

Machine learning
Mobile device

Federated learning
Blockchain-based
federated learning
Edge server
Unmanned aerial vehicle

Deep reinforcement learn-
ing
Trust region policy opti-
mization
Deep neural network

P2P
IID

A2C

SGD

DDPG

ES
UAV

DRL

TRPO

DNN

MSPBE

Mean Squared Projected
Bellman Error

KL

Peer-to-peer
Independent and identi-
cally distributed
Advantage actor critic

Stochastic gradient de-
scent
Deep deterministic policy
gradient
Kullback–Leibler

Table III: List of key notations.

Notation
M
K
f (cid:96)
n
pn
ϑ
¯h
pm(cid:48)
xn,m,g
w
T oﬀ,(k)
n,m
T loc,(k)

n

T update,(k)

m

T cons,(k)

Deﬁnition
Number of ESs
Number of sub-channels
MD’s CPU workload
MDs’ transmit power
MD’s model size
Hash amount of a block
ES’s transmit power
MD’s ofﬂoading decision
ML model parameter
MD’s ofﬂoading latency
MD’s local data process-
ing latency
ES’s model updating la-
tency
Total model consensus la-
tency

Notation
N
Dn
fm
bn,g
Ψn
bm(cid:48)
g
k
xm
T exe,(k)
m
T up,(k)
n,m

T learn,(k)

T mine,(k)

n

Deﬁnition
Number of MDs
MD’s Data size
ES’s CPU workload
MD’s bandwidth
MD’s hash rate
ES’s bandwidth
Wireless channel
Global aggregation round
ES’s gradient
ES’s execution latency
MD’s model uploading la-
tency
Total learning latency

Total mining latency

services for multiple MDs concurrently. We consider N MDs
gathered in the set N , N = |N |, which are connected to
M ESs collected in the set M, M = |M|, in a multi-server
edge computing network. The goal of the system is to train
an ML model, with training proceeding through a series of
global aggregation rounds collected via set K. In each round
k ∈ K, each MD n possesses a dataset D(k)
n , which may vary
n = |D(k)
from one round to the next, with size D(k)
n |. Each
dataset D(k)
contains multiple data samples, each consisting
n
of an input feature vector and (e.g., supervised learning) a
label. The MDs employ these datasets for local training in
round k, formalized in Section II-B. The total dataset is given
n∈N D(k)
by the set D(k) = ∪n∈N D(k)
n .
The interactions between MDs, ESs, and blockchain com-

n with size D(k) = (cid:80)

ponents of our BFL system are summarized as follows:

Fig. 1: Our proposed BFL architecture in multi-server edge
computing.

Edge Server (ESs)Mobile Device (MDs)SGD local trainingMining𝐷!𝐷"𝐷#𝐷$𝐷%…𝑤%𝑤!𝑤"𝑤&…Data offloadingEdge trainingP2P communicationBlockchainBlockchainBSDataset𝑤!𝑤"𝑤#𝑤$Share block to MDs for miningBlockPerform consensus among ESs over !▽𝐹!to build final gradient ▽𝐹!Compute accumulated gradient !▽𝐹!from 𝑤!and local model 𝑤"Compute the global model and add to block: 𝑤#=𝑤(#%&)-𝜂#%&.▽𝐹!!%&(#%&)ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

4

g in round k, and x(k)
n,m,g = 0 otherwise. Each dataset can
be either trained locally at the MD or ofﬂoaded to the ES
under a feasible ofﬂoading policy X (k) = {x(k)
n,m,g ∈
{0, 1}, ∀n ∈ N , m ∈ M, g ∈ G}. In each training round, the
BFL operation consists of four key steps, as depicted in Fig. 1:

n,m,g|x(k)

1) Data Ofﬂoading and Processing: Depending on its avail-
able resource, each MD can choose to ofﬂoad its data
to one of the nearby ESs for edge learning, or learn the
model locally via local data processing. We assume that
the model learning process begins once the ofﬂoading
phase is completed [14]. This assumption is realistic in
practical scenarios, where each MD needs to obtain an
ofﬂoading policy on whether it should ofﬂoad the data to
an ES or not, before it allows the associated ES to train
its data. For both training cases, MDs and ESs conduct
stochastic gradient descent (SGD) iterations and synchro-
nize their model parameters through block mining. After
model training, a MD uploads its ML model parameters
to its nearby ES via blockchain. Considering an ES, if
it receives data from MDs, it allocates its resources to
conduct ML model training.

2) Partial Model Aggregation and Consensus Update: Each
ES combines its computed ML model with models re-
ceived from its associated MDs to perform a partial
model aggregation. Then, ESs will
join a consensus
update process in which they conduct multiple rounds of
blockchain-enabled peer-to-peer (P2P) communications
to exchange their models. After the consensus update,
an ES will be randomly sampled to work as a leader
and build an unveriﬁed block that contains the aggregated
model for mining.

3) Block Mining: The leader ES broadcasts the block via the
blockchain to the connected ESs and MDs for mining. In
this work, we are mostly interested in the mining latency
from the users’ perspective, and thus we analyze the
mining process at the MDs. Given resource constraints
at
the MDs, we develop a resource trading strategy
where MDs can purchase hash power from the edge/cloud
service provider (ESP) (e.g., Amazon cloud services) to
run the mining task [29].

4) Block Generation: After receipt of the unveriﬁed block,
the MD that is the ﬁrst to successfully verify the block
will append it to the blockchain and receive a reward.
Then, each MD downloads the veriﬁed block via its
blockchain account [29] that contains the global model,
and uses this for local model synchronization to begin
the next round of model training.

B. Federated Learning Model

Let f (w, i) ∈ R denote the loss function of the ML model
(e.g., neural network) associated with data point i, where
w ∈ Rd is the parameter vector (e.g., weights on neurons).
In this work, we consider a scenario of full ML training at
MDs, i.e., each MD either keeps its entire data locally or
ofﬂoads it completely to an ES. When the MD keeps its data
local, it ofﬂoads the entire trained ML model to the ES. We

Fig. 2: A block architecture in our blockchain for global model
sharing, where a global model w(k) is embedded into the
transaction of a block. The transaction is also used to transfer
the local gradient ∇F C,(k)
m of
ESs in the model uploading and model consensus processes,
respectively.

of MDs and edge gradient x(l)

n

• MDs participate in ML model training in a federated
manner to serve their intelligence applications (e.g.,
object detection). Moreover,
they work as blockchain
nodes to mine blocks containing global models in each
communication round to support BFL model sharing.
• ESs assist in MD model training by providing computa-
tion resources through the ofﬂoading process. They also
coordinate the model aggregation process to build the
global model that is shared with MDs via blockchain.
• Blockchain allows MDs to securely transmit their com-
puted local model to ESs via blockchain. It also facil-
itates the model consensus process between ESs with
a traceable data ledger. Moreover, blockchain enables
secure global model sharing from ESs to MDs. After the
model aggregation process completes, the global model
is added to the blockchain, where mining is executed
for secure model sharing. Speciﬁcally, an ES will be
randomly sampled to function as a leader node and build
an unveriﬁed block which contains the global model.
This block is then shared with other ESs and MDs for
mining. Subsequently, each MD downloads the veriﬁed
block from the blockchain to extract the global model
which is used for the next training round. As illustrated
in Fig. 2, each block includes (i) a header, with a hash and
cryptographic nonce, and (ii) the data part. To construct a
block, an ES generates a transaction using its aggregated
model and hashes it, resulting in an output of a ﬁxed
length. The block is then shared with the MDs for model
training.

We let G, G = |G|, denote the set of available sub-
channels for communication at a BS. We assume that uplink
communications between MDs and ESs follow an OFDMA-
based protocol, where each MD is assigned to a sub-channel
g ∈ G to ofﬂoad its data in each time slot, and thus each
ES can serve at most G MDs in every ofﬂoading period. We
deﬁne the data ofﬂoading policy, which incorporates the uplink
sub-channel scheduling,
n,m,g,
(n ∈ N , m ∈ M, g ∈ G), where x(k)
n,m,g = 1 indicates that
data D(k)
from MD n is ofﬂoaded to ES m via sub-channel
n

through a binary variable x(k)

…Previous hashNonceTime stampBlock headerData hashBlock data𝑤("#$)TransactionBlock i-1Global model at round (k-1)…Previous hashNonceTime stampBlock headerData hashBlock data𝑤(")TransactionBlock iGlobal model at round (k)Previous hashNonceTime stampBlock headerData hashBlock data𝑤("&$)TransactionBlock i+1Global model at round (k+1)ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

5

measure the online loss function of each MD n at each global
aggregation round k as

the number of data points) of its cumulative gradient and that
of its associated MDs, as

F (k)

n (w) =

1
D(k)
n

(cid:88)

i∈D(k)

n

f (w, i),

(1)

∇F A,(k)

m =

(cid:88)

n∈N loc,(k)
m

D(k)
n
D(k)e(k)
n

∇F C,(k)

n +

D(k)
m
D(k)e(k)
m

∇F C,(k)

m . (8)

and subsequently deﬁne the online global loss as

F (k)(w) =

1
D(k)

(cid:88)

n∈N

D(k)

n F (k)

n (w), D(k) =

(cid:88)

n∈N

D(k)

n . (2)

In our BFL system, each ES processes datasets ofﬂoaded
from a portion of MDs, and also receives the computed ML
models uploaded from another portion of nearby MDs which
had chosen local processing. We denote N oﬀ,(k)
and N loc,(k)
m
as the sets of MDs engaged in data ofﬂoading and model
uploading with ES m, respectively (N oﬀ,(k)
⊆ N
and N oﬀ,(k)
D(k)
m
n
denote ES m’s dataset received from MDs, with size D(k)
m ,
the loss at ES m is given as

, N loc,(k)
m
m
= ∅). Letting D(k)
m = ∪n∈N oﬀ,(k)

∩ N loc,(k)
m

m

m

F (k)

m (w) =

1
D(k)
m

(cid:88)

i∈D(k)
m

f (w, i).

(3)

We now formalize the ML model training procedure at
ESs and MDs. Each model training round k starts with the
broadcast of a global model, w(k), from one of the ESs. During
round k, each ES m performs e(k)
m iterations of SGD over
its local/ofﬂoaded dataset, which may vary from one ES to
another, where the evolution of its local model parameters is
given by

w(k),e

m = w(k),e−1

m

−

ηk
B(k)
m

(cid:88)

∇f (w(k),e−1
m

, d),

(4)

d∈B(k),e
m

m

where ηk > 0 is the step-size and e ∈ {1, · · · , e(k)
n } is the
= w(k). In (4), B(k),e
index of local iteration with w(k),0
denotes the set of data points sampled at the e-th iteration from
the local dataset of the respective MD to perform mini-batch
SGD. We assume that the mini-batch size B(k)
m |, ∀e,
is ﬁxed during each local model training round k for each ES
m. The local model training at each MD n is also similar to
(4), where each MD n performs e(k)
iterations of SGD with
n
local updates as

m = |B(k),e

m

w(k),e = w(k),e−1

n

−

ηk
B(k)
n

(cid:88)

d∈B(k),e
n

∇fn(w(k),e−1
n

, d)

(5)

After model training, each ES m computes its cumulative
gradient:

∇F C,(k)

m = (w(k) − w(k),e(k)

m

m

)/ηk .

Similarly, each MD n also obtains its gradient:

∇F C,(k)
n

= (w(k) − w(k),e(k)

n

n

)/ηk .

(6)

(7)

Subsequently, the MDs ofﬂoad their cumulative gradients
to their associated ES via blockchain. Each ES m ∈ M
subsequently acquires its aggregated gradient, which is a
scaled sum (with respect to the number of SGD iterations and

The ESs then engage in P2P communications for cooper-
ative consensus formation among their aggregated gradients.
For this purpose, we assume that they exploit linear distributed
consensus iterations [30], where during training round k, each
ES m ∈ M conducts φ(k) ∈ N consensus rounds of P2P
communications with its neighboring ESs. During each round
l = 0, · · · , φ(k) − 1 of P2P communications, the evolution of
the local gradient of ES m ∈ M can be expressed as:

m = λ(k)
x(l+1)

m,mx(l)

m +

(cid:88)

m,m(cid:48)x(l)
λ(k)
m(cid:48),

(9)

m(cid:48)∈(cid:37)(m)

m
(φ(k))
m

m = ∇F A,(k)

is ES m’s initial local aggregated

where x(0)
gradient, and x
denotes the local gradient after the
consensus process concludes. In (9), (cid:37)(m) ⊆ M denotes the
set of ESs in the neighborhood of ES m, and λ(k)
m,m(cid:48) ∈ [0, 1],
m(cid:48) ∈ {m} ∪ (cid:37)(m) are the consensus weights employed at m.

(φ(k))
m

Let ∇F L,(k)

m = x

denote the ﬁnal local gradient at ES
m after the P2P communication process for training round k
concludes, which can be expressed as

∇F L,(k)

m =

(cid:88)

∇F A,(k)
m

+c(k)
m ,

(10)

m∈M
(cid:124)

(cid:123)(cid:122)
(a)

(cid:125)

where term (a) is the perfect average of the local aggregated
gradients and c(k)
m denotes the error of consensus caused by
ﬁnite P2P rounds. The selected ES at aggregation round k,
denoted by mk ∈ M, then adds a boosting coefﬁcient to its
local gradient ∇F L,(k)
mk , forming the vector
n e(k)
n
D(k)

∇F L,(k)
mk

D(k)

(k)
mk

(11)

∇F

(cid:88)

(cid:88)

=

,

m∈M

n∈N (k)
m

and updates the global model parameter as follows:

w(k+1) = w(k) − ηk∇F

(k)
mk

.

(12)

w(k+1) is then broadcast across the ESs and MDs via block
mining to begin the next round of local model training. The
training procedure of our BFL scheme is summarized in Algo-
rithm 1. At the beginning of each global communication round,
each MD makes a training decision: edge model
training
at the ESs or local model training, which is an output of
Algorithm 2 (line 5) that will be developed in Section IV. In
the case of edge model training, the MD ofﬂoads its data to its
associated ES (line 7); otherwise, the MD trains its data locally
(lines 9-14). After receiving all models from local MDs, each
training (lines 17-21). Once the
ES performs edge model
local training and edge training processes are completed, ESs
collaborate to perform P2P-based consensus on aggregated
gradients to update the global model (lines 23-35). Finally,
the mining is executed, where an ES leader adds the global
model parameter to an unveriﬁed block and broadcasts it to

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

6

other ESs and MDs. In particular, each MD performs resource
trading to purchase hash power from the ESP to run the Proof-
of-Work-based mining. The conﬁrmed block is then appended
into the blockchain for global model sharing, where each MD
downloads the block for the next round of training (lines 36-
40).

C. Convergence Analysis of ML Model Training

We now study the convergence of our BFL scheme. We
ﬁrst make a few assumptions and deﬁne some quantities of
interest. To obtain convergence guarantees for the distributed
consensus process, we make the following assumptions on the
consensus weights in (9):

Assumption 1 (Conditions on Consensus Weights [30], [31]).
The consensus matrix Λ(k) = [λ(k)
m,m(cid:48)]m,m(cid:48)∈M(k) satisﬁes the
following properties: (i) λ(k)
m,m(cid:48) = 0 if ESs m and m(cid:48) are
not connected, (ii) Λ(k)1 = 1, (iii) Λ(k) = Λ(k),(cid:62), and
≤ λ(k) < 1, where 1 represents the
(iv) ρ
1s’s vector and ρ(A) deﬁnes A’s spectral radius.

Λ(k) − 11(cid:62)
|M(k)|

(cid:16)

(cid:17)

If λ(k)

m,m = 1 − d|(cid:37)(m)| and λ(k)

m,m(cid:48) = d, m (cid:54)= m(cid:48), 0 < d <
1/M [30], the conditions in Assumption 1 hold. These weights
can be distributedly obtained at the ESs given a predeﬁned d.

Deﬁnition 1 (Gradient Divergence). The divergence of local
aggregated gradients across the ESs at global aggregation
round k, denoted by Ξ(k), is deﬁned as follows:

(cid:13)
(cid:13)∇F A,(k)
(cid:13)

m − ∇F A,(k)

m(cid:48)

(cid:13)
(cid:13) ≤ Ξ(k), ∀m, m(cid:48) ∈ M.
(cid:13)

(14)

Assumption 2 (Smoothness of the Loss Functions [32], [33]).
For each MD n that conducts local model training during
aggregation round k, the local loss function F (k)
is β-smooth:

n

(cid:107)∇F (k)

n (w) − ∇F (k)

n (w(cid:48))(cid:107)≤ β(cid:107)w − w(cid:48)(cid:107), ∀w, w(cid:48).

(15)

Also, for each ES m that conducts model training, the loss
function F (k)
m is assumed to be β-smooth, which veriﬁes that
the global loss function F (k) achieves β-smoothness.

m

m = N loc,(k)

Let y ∈ N ∪ M denote the index of an arbitrary MD/ES.
Also, let N (k)
∪ {m} denote a set containing the
devices which conduct local model training and ofﬂoad their
cumulative gradients to ES m as well as ES m itself. We
measure the heterogeneity of data across the MDs/ESs via the
following assumption [33]:

Assumption 3 (Bounded Dissimilarity of Local Loss Func-
tions). The ﬁnite constants ζ1 ≥ 1, ζ2 ≥ 0 exist for which the
following inequality deﬁned on the gradient of the local loss
in (2) and (3) holds for any set of coefﬁcients {ay ≥ 0} where
(cid:80)

(cid:80)

m∈M

y∈N (k)
m

ay = 1:

(cid:88)

(cid:88)

ay(cid:107)∇F (k)

y

(w)(cid:107)2

m∈M

y∈N (k)
m
(cid:13)
(cid:13)
(cid:13)

≤ ζ1

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

ay∇F (k)

y

(w)

(cid:13)
2
(cid:13)
(cid:13)

+ ζ2, ∀k.

(17)

As can be seen, ζ1 = 1 and ζ2 = 0 in (17) correspond to a
scenario in which the data across the MDs/ESs is completely
homogeneous, and ζ1 and ζ2 increase as the heterogeneity
across the datasets increases. We next quantify the hetero-
geneity of data inside each local dataset:

Deﬁnition 2 (Local Data Variability). The local data vari-
ability at each MD/ES y is denoted by Θy ≥ 0, which ∀w, k
satisﬁes
(cid:107)∇fy(w, d) − ∇fy(w, d(cid:48))(cid:107)≤ Θy(cid:107)d − d(cid:48)(cid:107), ∀d, d(cid:48) ∈D(k)

n .(18)

We further deﬁne Θ = maxy∈N ∪M{Θy}.

Additionally, we let (σ(k)
y )2 denote the variance across the
feature vectors of dataset D(k)
at ES/MD y which conducts
local model training. We further quantify the dynamics of local
datasets via their impact on the ML performance [26]:

y

Deﬁnition 3 (Model/Concept Drift). For each MD/ES y, we
calculate the model/concept drift between two FL rounds
k − 1 and k, ∆(k)
y ∈ R, which characterizes the local loss’s
variation induced by data arrival/departure at the MDs and
data collection at the ESs, as follows:

F (k)
y

D(k)
y
D(k)

D(k−1)
y
D(k−1)
We next present one of our main results,

(w) ≤ ∆(k)

F (k−1)

(w) −

y

y , ∀w.

(19)

the general

convergence behavior of BFL:

let e(k)

avg = (cid:80)

Theorem 1 (Convergence Characteristics). Let N (k), N (k) =
|∪m∈MN (k)
m | denote the set of all MDs and ESs en-
gaged in model training during the k-th local model train-
y }, e(k)
max = maxy∈N (k){e(k)
ing round. Also,
avg =
(cid:80)
y e(k)
y∈N (k) e(k)
y∈N (k) D(k)
y /D(k).
avg ≤ ˆe(k)
avg ≤ ˆemax
Assume that emin
avg ,
∀k, where emin
avg are four ﬁnite positive
∆(k)
y . In
constants. Further,
conducting K global aggregation rounds,
the step size
satisﬁes ηk =
, where α is chosen such that ηk ≤
(cid:110) 1
2β

y /N (k), and ˆe(k)
avg ≤ e(k)
avg , ˆemin

avg ≤ emax
avg and ˆemax
let ∆(k) = (cid:80)

avg and ˆemin

y∈N (k)
m
if

avg , emax

, then the

min

(cid:17)(cid:17) ,

m∈N

(cid:80)

(cid:114)

(cid:111)

(cid:113)

(cid:16)

(cid:16)

α
Ke(k)
avg
1
e(k)
max

D(k)
y∈N D(k)

y e(k)
y

e(k)
max−1

(4ζ1+1)

2β (cid:80)

convergence characteristics of the global loss function under
BFL follow the bound in (13).

Proof. See Appendix A.

The bound in (13) reveals the impact of different de-
vice/network conﬁgurations on the ML model performance.
In particular, the impact of model drift is captured in term
(a). Also, the divergence of the global model caused by bias
of the local models due to the heterogeneity of data across
the MDs/ESs is captured via term (c) which encapsulates ζ2.
Larger daatset heterogeneity captured via ζ1 imposes a stricter
condition on the step size ηk described in the statement of
the theorem as well. Terms (b) and (e) in (13) capture the
impact of local data heterogeneity ({Θy}) and mini-batch sizes
({B(k)
y }) on the performance of the model. Finally, term (d)
captures the impact of imperfect local aggregations caused
by ﬁnite P2P rounds (φ(k)) and divergence of gradients Ξ(k),

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

7

1
K

K−1
(cid:88)

k=0

(cid:104)
(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)

E

≤

8(cid:112)emax
avg
√
αˆemin
avg

K

(cid:16)

F (0)(w(0)) − F (k)(cid:63) (cid:17)

+

8(cid:112)emax
avg
√
αˆemin
avg
(cid:124)

K

K−1
(cid:88)

k=1

(cid:123)(cid:122)
(a)

∆(k)

(cid:125)

+

80β2α2
K2emin
avg

K−1
(cid:88)

(cid:88)

(cid:88)

k=0

m∈M

y∈N

(k)
m

(cid:16)
e(k)
y − 1

(cid:17)

(cid:32)

1 −

D(k)
y
D(k)

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

K−1
(cid:88)

k=0

80β2α2
K2emin
avg
(cid:124)

Θ2
y

+

(cid:125)

(cid:123)(cid:122)
(c)

(cid:16)
e(k)
max

ζ2

(cid:17) (cid:16)

e(k)
max − 1

(cid:17)

(cid:124)

1
K

(cid:124)

+

K−1
(cid:88)

k=0

(cid:123)(cid:122)
(b)

24M (λ(k))2ϕ(k) (cid:16)

Ξ(k)(cid:17)2

+

(cid:123)(cid:122)
(d)

(cid:125)

(cid:124)

16βαˆemax
avg
√
(cid:113)

K

K

emin
avg

K−1
(cid:88)

(cid:88)

(cid:88)

k=0

m∈M

y∈N

(k)
m






D(k)
y
(cid:113)

e(k)
y

D(k)
(cid:123)(cid:122)
(e)



2

(cid:32)




1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

1
K

K−1
(cid:88)

k=0

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)
(cid:104)

E

≤

+

80β2α2
Kemin
avg

K

8(cid:112)emax
avg
√
αˆemin
avg
80β2α2
Kemin
avg

(emax − 1) ϑ +

ζ2 (emax) (emax − 1) +

(cid:16)

F (0)(w(0)) − F (k)(cid:63) (cid:17)

+

8Υ(cid:112)emax
avg
√
αˆemin
K
avg
16βαˆemax
avg
√
(cid:113)

+

K

emin
avg

ϑ

24ξ
√

K

(cid:125)

Θ2
y

.

(cid:125)

(13)

(16)

where the consensus matrix’s spectral radius across the ESs
(λ(k) < 1) deﬁned in Assumption 1 determines the rate under
which the consensus error decays to zero with respect to the
number of P2P rounds φ(k).

We derive Theorem 1 to obtain conditions for the online

global gradient under BFL converges:

(cid:18)

1 −

B(k)
y
D(k)
y

Corollary 1 (Convergence under Proper Choice of Mini-batch
Size and P2P Communication Rounds). Besides conditions
in Theorem 1, we assume that (i) the model/concept drift is
small enough such that ∆(k) ≤ Υ
K , ∀k, for some positive
constant Υ, (ii) the choice of mini-batch size B(k)
at each
y
(cid:19) (σ(k)
y )2
B(k)
y

y ≤
ϑ, ∀k, where ϑ is a ﬁnite positive constant, and (iii)
e(k)
the
max ≤ emax, ∀k for some positive constant emax. If
number of P2P communications among the ESs at each
i.e., φ(k), satisﬁes ϕ(k) ≥
global aggregation round k,
(cid:19)(cid:21)+

MD/ES y ensures a uniﬁed bound

ξ
K(Ξ(k))2

for some positive constant ξ,

1
2
where λ(k) is deﬁned in Assumption 1, the gradient of the
global loss under BFL satisﬁes the upper bound in (16), which
E (cid:2)(cid:107)∇F (k)(w(k))(cid:107)2(cid:3) = O
implies 1
, and thus
K
E (cid:2)(cid:107)∇F (k)(w(k))(cid:107)2(cid:3) → 0.
limK→∞

(cid:80)K−1
k=0
(cid:80)K−1
k=0

(cid:20)
logλ(k)

(cid:16) 1√

Θ2

m(k)

(cid:18)

(cid:17)

K

√

1
K

Proof. See Appendix B.

III. SYSTEM LATENCY MODEL

In this section, we analyze the latencies of the model
training and block mining processes in detail, and present our
latency optimization problem.

A. Latency of Model Training in BFL

In our BFL system, an MD can ofﬂoad its data to the ES
for edge learning or choose to train the ML model locally
in each global aggregation k ∈ K. In the case of ofﬂoading
(x(k)
n,m,g = 1), the latency for model training consists of data
communication latency and execution latency at the ES. The

Algorithm 1 Proposed BFL algorithm

1: Input: Global communication rounds K, local training round en, ∀n ∈
N , edge training round em, ∀m ∈ M, number of MDs N , number of
ESs M

Determine each training decision xn,m,g via Algorithm 2
if xn,m,g = 1 then

for each MD n ∈ N do

Ofﬂoad dataset D(k)

2: Initialization: Initialize global model w(0)
3: for each global communication round k ∈ K do
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Perform local model training on local dataset D(k)
for each local training epoch e ∈ en do
Update local parameters w(k),e

end for
Compute the cumulative gradient via (7)
Add the gradient
framework deﬁned in Fig. 2 to transfer to an ES

n to ES m

via (5)

else

n

n

to a transaction based on the blockchain

15:
16:
17:
18:
19:
20:
21:
22:
23:

24:
25:
26:
27:
28:

29:
30:

31:
32:
33:
34:
35:
36:

37:
38:

39:
40:

end if

end for
for each ES m ∈ M do

for each edge training epoch e ∈ em do
Update local parameters w(k),e

m via (4)

end for
Compute its cumulative gradient via (6)

end for
Each ES m ∈ M computes an aggregated gradient: ∇F A,(k)
∇F C,(k)
m
m
Set x(0)
m = ∇F A,(k)
for each P2P consensus round l ∈ φ(k) do

and ∇F C,(k)
m

via (8)

m

using

for each ES m ∈ M do

for each neighboring ES m(cid:48) ∈ (cid:37)(m) of ES m do

Transmit the gradient x(l)
blockchain framework deﬁned in Fig. 2 to ES m

m(cid:48) via a transaction based on the

end for
ES m downloads the block of all transactions to obtain neigh-
boring ESs’ gradients and computes its gradient x(l+1)
via (9)

m

end for

(φ(k))
m

m = x
(k)
mk

end for
Obtain the ﬁnal local gradient at ES m: ∇F L,(k)
Add a boosting coefﬁcient to ∇F L,(k)
m to form ∇F
Update the global model parameter w(k+1) via 12
Add the global model parameter to an unveriﬁed block B by the ES
leader and broadcast it to other ESs and MDs for mining
Each MD trades hash resource Ψ(k)
n from the ESP to mine the block
The fastest MD propagates the veriﬁed block to ESs and other MDs
for conﬁrmation
Add the block to the blockchain for global model sharing
Each MD downloads the block to obtain the global model for next
round of training

via (11)

41: end for
42: Output: Final global model w(K)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

8

data communication latency of MD n when ofﬂoading its data
D(k)
n

to ES m is given by

ES via blockchain. The latency of this model uploading can
be given as

T oﬀ,(k)
n,m =

x(k)
n,m,g

(cid:88)

g∈G

D(k)
n
R(k)
n,m

, ∀n ∈ N ,

(20)

T up,(k)
n,m =

1 −





x(k)
n,m,g



(cid:88)

g∈G

ϑ
R(k)
n,m

, ∀n ∈ N , m ∈ M, (23)

(cid:17)

,

1 +

g∈G x(k)

n,m,gb(k)

n,g log2

n h(k)
p(k)
n,m,g
(cid:16)
j,m,gp(k)
x(k)
j h(k)
(cid:17)
j h(k)

n,m is

where R(k)
from MD n to ES m, which is given by R(k)
(cid:80)

transmission

rate

the

(in

bits/s)
n,m =
(cid:19)

(cid:18)

j,m,g

n,g|0 < b(k)

σ2+(cid:80)
j∈N \N oﬀ
m
(cid:16)
j,m,gp(k)
x(k)
j∈N \N oﬀ
m
(N \ N oﬀ
m )

j,m,g
interference (cid:80)
caused
under
by a group of MDs
that are associated
with other ESs on sub-channel g. Here, b(k)
is
the
n,g
bandwidth (in Hz) allocated to channel g under
the
policy B(k) = {b(k)
n,g ≤ W, ∀n ∈ N , g ∈ G, k ∈ K},
where W is the maximum system bandwidth constraint.
Further, p(k)
n is transmission power (in Watts) of MD n in the
ofﬂoading subject to the power constraint Pn under a policy
P (k) = {p(k)
n ≤ Pn, n ∈ N oﬀ
m , ∀m ∈ M}, and
h(k)
n,m,g is the gain of wireless channel between the MD m and
ES n on sub-channel g. Accordingly, the energy consumption
for data ofﬂoading at aggregation round k at MD n is given
n T oﬀ,(k)
as Eoﬀ(k)
n,m .
After the ofﬂoading process, ES m receives a combined
dataset D(k)
m =
|D(k)
m | from MDs engaged in data ofﬂoading. Then, the ES
allocates its computational resource to perform the SGD-based
model training. The data processing latency at ES m is thus
given by

m (as deﬁned in Section II-B) with size D(k)

n |0 < p(k)

n,m = (cid:80)

g∈G x(k)

n,m,gp(k)

T exe,(k)
m

=

x(k)
n,m,g

(cid:88)

g∈G

m e(k)
m

m (cid:37)(k)
CmD(k)
fm

, m ∈ M,

(21)

where Cm represents how many CPU cycles is required to
calculate the gradient per data point at ES m, and (cid:37)(k)
m ∈
(0, 1] is the mini-batch size ratio, from which the size of SGD
mini-batches can be written as B(k)
m . Moreover,
e(k)
m is the number of SGD iterations, and fm is the ﬁxed
computational capability of ES m (in CPU cycles/s).

m = (cid:37)(k)

m D(k)

On the other hand, each MD n can also choose to locally
process its data, implying x(k)
n,m,g = 0, ∀m, g. We denote f (cid:96),(k)
as the computational capability of MD n (in CPU cycles/s)
allocated to train the data given maximum capacity Fn, which
is represented via a policy: F (k) = {f (cid:96),(k)
≤
n
Fn, ∀n ∈ N }. The latency of model training over e(k)
n SGD
iterations at MD n is given by

|0 < f (cid:96),(k)

n

n



T loc,(k)
n

=

1 −



x(k)
n,m,g



(cid:88)

g∈G

CnD(k)

n e(k)
n

n (cid:37)(k)
f (cid:96),(k)
n

,

(22)

where (cid:37)(k)
n ∈ (0, 1] is the SGD mini-batch ratio, and Cn is the
number of CPU cycles required to compute the gradient per
data point at MD n. Also, the local energy consumption of MD
1 − (cid:80)
n is given by Eloc,(k)
)2Cn,
where κ is the energy coefﬁcient depending on the chip
architecture. After completing the local model training, MD n
then transmits the computed model ∇F A,(k)
to its associated

g∈G x(k)

κ(f (cid:96),(k)
n

n,m,g

=

(cid:16)

(cid:17)

n

n

where ϑ is the gradient/model size (in bits) that is the same
across the MDs. Based on above formulations, the total latency
of model training of the BFL system at each global aggregation
k is

T learn,(k) =

(cid:88)

(cid:88)

T oﬀ,(k)
n,m +

(cid:88)

T exe,(k)
m

n∈N
(cid:88)

m∈M
T loc,(k)
n

+

+

(cid:88)

m∈M
(cid:88)

T up,(k)
n,m , ∀k ∈ K.

(24)

n∈N

n∈N

m∈M

After the model training process, the ESs in the edge layer
perform the P2P-based consensus on the computed model.
An ES performs the model updating where the neighbours
exchange their aggregated gradients deﬁned in (8) via P2P
communications. The latency of parameter updating at ES m
in each P2P communication round l is determined by

T update,(k)
m

(l) = max

m(cid:48)∈Ξ(m)

ϑ
R(k)
m,m(cid:48)(l)

,

(25)

m,m(cid:48)(l) is the transmission rate from ES m(cid:48)

where R(k)
to
ES m via wired communications. Thus the updating latency
of each ES m over ϕ consensus rounds is T cons,(k)
=
(cid:80)ϕ
(l). Finally, the latency of the model consen-

l=1 T update,(k)

m

m

sus is determined by the slowest ES as below:

T cons,(k) = max
m∈M

T cons,(k)
m

= max
m∈M

(cid:35)

T update,(k)
m

(l)

. (26)

(cid:34) ϕ
(cid:88)

l=1

B. Latency of Block Mining in BFL

After model consensus among ESs, an ES will be selected
to work as a leader that builds an unveriﬁed block B and
broadcasts it to all connected ESs and participating MDs for
universal block mining. This selection can be based on the
ESs’ reputation in the previous aggregation round from the
P2P collaboration process, where the reputation evaluation
methodology can be used to quantify each ES’s contribution
to block generation [34]. Inspired by our previous work [35],
we adopt mining latency as the reputation metric: an ES which
generates the fastest block in the previous mining round will
have the highest reputation and is selected as the leader for
mining coordination in the current mining round.

After leader selection,

the mining process is executed.
Similar to existing works [8], [9], we focus on mobile mining
analysis, where the mining latency at MDs is considered, and
thus the mining analysis at ESs is ignored. We adopt the
popular Proof-of-Work mining mechanism [6], [7] for our
BFL, where MDs compete to mine the block. The adoption of
other mining mechanisms in BFL will be considered in future
works. Conceptually, the mining latency at each MD n mainly
consists of block generation latency and block propagation

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

latency [24]. Due to resource constraints, we allow MDs to
implement resource trading to buy hash power from the ESP to
assist their mining. Accordingly, MDs compete with each other
to gain the maximum hash power allocation from the ESP’s
hash resource pool, aiming to increase the probability of be-
coming the mining winner for gaining rewards. In every global
aggregation round k, each MD n speciﬁes its mining demand
to trade hash resource, denoted as Ψ(k)
n (Hash/sec) subject to
the constraint of the total hash power of the BFL system Ψmax
under a policy Ψ(k) = {Ψ(k)
n ≤ Ψmax, ∀n ∈ N }.
Let ¯h(k) denote the hash amount (in hash) required to mine
the block B (which also represents the size) in aggregation
round k, the block generation latency at MD n can be speciﬁed
as T gen,(k)
. Thus, the energy consumption for block
n
generation at MD n can be given as Egen,(k)
= Ξn¯h(k), where
Ξn is the power efﬁciency of the mining rig of MD n (J/hash).

n |0 < Ψ(k)

= ¯h(k)
Ψ(k)
n

n

After generating a block, MD n will propagate it
to
ESs and other MDs for conﬁrmation. The latency of this
block propagation process can be determined as T prop,(k)
=
ξ(B|LM +N −1|)(k), where ξ is a parameter to quantify the ef-
ﬁciency of block veriﬁcation. Further, B|LM +N −1| represents
the average delay of the repeated veriﬁcation on the block B
of all entities except MD n [35]. Therefore, the mining latency
at an MD can be given as T mine,(k)

+ T prop,(k)
n

= T gen,(k)
n

n

n

.

However, in the block mining process, there is the pos-
sibility that a MU n generates the block and propagates
it slower than other miners which will discard this block
from blockchain. This issue is called forking and such a
block is called an orphaned one. The forking probability can
be determined as Pfork = 1 − e−ιφ(sn), where ι is set to
ι = 1/600(sec) [29]. Moreover, sn represents how many
transactions are included in the block mined by MD n, and
φ(sn) is a function of block size. Therefore, the mining latency
at MD n can be rewritten as

T mine,(k)
n

= ζ(T gen,(k)
n

+ T prop,(k)
n

), ∀k ∈ K,

(27)

where ζ is the number of forking occurrences in each global
training round. Therefore, the total mining latency of BFL
system can be given as T mine,(k) = (cid:80)

n∈N T mine,(k)

n

.

C. Formulation of System Latency Problem

Our objective is to optimize the total latency of the BFL
system from the user perspective as the sum of model training
latency, model consensus latency and block mining latency at
a certain aggregation round k:

minimize
X,P ,B,F ,Ψ

1
K

K−1
(cid:88)

k=0

(cid:16)

T learn,(k) + T cons,(k) + T mine,(k)(cid:17)

(28a)

s.t.

x(k)
n,m,g ∈ {0, 1}, ∀n ∈ N , m ∈ M, g ∈ G,
(cid:88)
x(k)
n,m,g ≤ 1, n ∈ N ,

(cid:88)

g∈G
m∈M
0 < p(k)
n ≤ Pn, ∀n ∈ Nn,
0 < b(k)
n,g ≤ W, ∀n ∈ N , g ∈ G,
0 < f (cid:96),(k)
≤ Fn, ∀n ∈ N ,
n

(28b)

(28c)

(28d)

(28e)

(28f)

9

(28g)

, ∀n ∈ Nn, (28h)

0 < Ψ(k)
0 < Elearn,(k)
n
K−1
(cid:88)

n ≤ H, ∀n ∈ Nn,
+ Egen,(k)
n
(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)
(cid:104)

E

1
K

≤ Emax,(k)
n

≤ (cid:15),

(28i)

k=0
Elearn,(k)
(cid:17)

(cid:80)

n

+

n,m

the

n,m,g

g∈G x(k)

=
Eloc,(k)
n

g∈G x(k)
n,m,gEoﬀ,(k)
where
(cid:16)
1 − (cid:80)
energy consumption
is
for model training. Here, constraints (28b) and (28c) imply
that each dataset can be either trained locally or ofﬂoaded
to at most one ES via a sub-channel. (28d) ensures the
transmit power constraint of each MD. Constraint
(28e)
guarantees that each MD n is allocated a feasible bandwidth
resource for data ofﬂoading. The MD also allocates a positive
computational resource to train its ML model with respect
to the maximum CPU capability Fn, as indicated in (28f).
Constraint (28g) guarantees that
the hash power allocated
to each MD is limited by the total system hash resource.
Further, constraint (28h) implies that the energy consumption
of MD n for model training and blockchain mining is limited
by its battery energy level. Finally, the global loss function
should be less than a desirable value (cid:15) to ensure the required
training quality, as indicated in constraint (28i).

The optimization problem in (28) is non-convex with respect
to the mixed discrete ofﬂoading and continuous allocation
variables. Due to the time-varying nature of system states,
such as channel condition and computational availability, it
is challenging to directly solve the formulated problem via
conventional optimization approaches such as Lyapunov opti-
mization [24]. Therefore, we will propose to use a learning-
based approach, where a new DRL algorithm is developed to
well capture the dynamics of system and integrate them into
the solution design.

IV. DRL DESIGN WITH PARAMETERIZED A2C FOR BFL
Different from the existing DRL algorithms which consider
either purely discrete [22], [36], [27] or purely continuous
actions [12], [37], [28], we here study a more practical DRL
setting with a hybrid discrete-continuous action for improving
the training performance. Even though such a hybrid action
setting has been previously mentioned in a few related works
such as [38], a holistic investigation on the sampling of
discrete and continuous actions has not been given. Therefore,
we propose a parameterized advantage actor critic (A2C) algo-
rithm to optimize the system latency, as illustrated in Fig. 3.
We consider a hybrid discrete and continuous action space,
where the resource allocation variables in (28) are continuous,
while the ofﬂoading decision variables are discrete. The actor
is designed to train both ofﬂoading and resource allocation
policies, which we demonstrate in Section IV-C. The critic is
then designed to evaluate the efﬁciency of the actor policy
training, which we present in Section IV-D. We also include
the advantage function in the critic design as it has been shown
to reduce the variance in actor policy training compared with
conventional actor-critic approaches [28], [39]. Finally, we
develop a training procedure to optimize the long-term system
utility of our developed A2C algorithm (Section IV-E), which
corresponds to minimizing the long-term BFL system latency.

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

10

Fig. 3: The proposed parameterized A2C architecture for our BFL environment.

A. DRL Formulation

We consider a single-agent DRL problem setting [12],
[28], where a virtual centralized agent
interacts with the
BFL environment induced by the interaction between MDs
and ESs during model training and block mining. Our DRL
scheme aims to optimize the total system latency in (28a)
consisting of model training latency, consensus latency and
block mining latency. To handle the formulated optimization
problem, we build a centralized agent which deﬁnes its reward
based on the total utility, and restricts its action space based
on the resource allocation and learning constraints in (28).
With a comprehensive view of the model training and block
mining processes, the agent can obtain the system state and
employ it to take efﬁcient actions via well-trained data of-
ﬂoading and resource allocation policies that are observed to
minimize the system latency. We consider a parameterized
Markov Decision process characterized by M =< S, A, r >.
Here, S = {s1, ..., sN } and A = {a1..., aN } are the ﬁnite
sets of state and parameterized action, respectively. Further,
r(s, a) : S × A → [−Ro, Ro], (Ro ∈ R>0) denotes the
bounded system reward. We also denote P (s(cid:48)|s, a) as the
probability of transition executed by action a of the agent
from s to s(cid:48).

In our BFL latency optimization problem, the ofﬂoading
decisions for ML model training at MDs are closely associated
with resource allocation variables. For instance, to ofﬂoad the
data to an ES, an MD needs to tune its transmit power and
determine channel availability; or to execute data locally, it
should determine its computational utilization. We consider a
parameterized action space: a ﬁnite set of discrete ofﬂoading
action a ∈ Aa = {a1, a2, ..., an} deﬁned via a discrete-
action policy πd(a|s) and each an has a set of continuous
parameters {c ∈ Ac} deﬁned via an action-parameter policy
πc(c|s, x). Thus,
the joint action is given by conditional
probability (a, c) ∼ πθ(a, c|s) = πd
(c|s, a), where
θd
θ is the parameter of the overall action policy, and θd and
θc are parameters of discrete action policy and parameter
policy, respectively, with θ = [θd, θc]. To simplify the notation,
πθ(a, c|s) is expressed by πθ(a, s) in the rest of the paper.

(a|s)πc
θc

At each time step t, the agent at state st takes an action

at to transition to the next state st+1 and observe a reward
rt+1 ← r(st, at, st+1). As a result, the training data for DRL
is produced in a form of trajectory where each data point on
the trajectory can be represented by tuple {st, at, rt, st+1}.
Given a policy π, the long-term discounted system reward is
characterized by the state-value function V πθ (s) : A → R
and the action-value function Qπθ (s, a) : S × A → R, which
are V πθ (s) = Eπθ [(cid:80)∞
t=0 γtrt+1|st = s] and Qπθ (s, a) =
Eπθ [(cid:80)∞
t=0 γtrt+1|st = s, at = a], where γt ∈ [0, 1] is the dis-
count value and Eπθ [.] represents expectation of the executed
reward function under policy π. By using the Bellman optimal-
ity equation, the Q-value associated with a state-action pair can
also be expressed by Qπθ (st, at) = Eπθ [rt+1 + γV πθ (st+1)].
In this paper, we focus on the A2C model
that can be
characterized by the advantage function Aπθ (s, a)

Aπθ (st, at) = Qπθ (st, at) − V πθ (st)

= rt+1 + γV πθ (st+1) − V πθ (st), ∀s ∈ S, a ∈ A.

(29)

It is worth noting that the proposed A2C algorithm training
is performed in a certain FL aggregation round k to allow
the agent to obtain an optimal latency-aware ofﬂoading and
allocation strategy for MDs. Based on the well trained A2C
model, we then deploy it into the BFL environment to guide
the FL training across aggregation rounds. Therefore, the index
k is dropped for the simplicity of notations in our DRL
formulation. In the following, we deﬁne state, action and
reward for our DRL algorithm.

1) State:

In our BFL environment,

the system state
consists of ﬁve components: data state Sdata(t), channel
state Schannel(t), bandwidth state Sband(t), computation state
Scomp(t), and hash power state Shash(t). Therefore, the system
state is deﬁned as:

s(t) = {Sdata(t), Schannel(t), Sband(t), Scomp(t), Shash(t)}.

(30)
Here, Sdata(t) is deﬁned as Sdata(t) = {Dn(t)}n∈N . Fur-
ther, Schannel(t) = {qn,g(t)}n∈N ,g∈G indicates whether the
sub-channel g is used by MD n at
time slot t. If yes,
qn,g(t) = 1, otherwise qn,g(t) = 0. The bandwidth state
Sband(t) can be given Sband(t) = {bn,g(t)}n∈N ,g∈G under

BFL Environment =𝒓𝒕+ γV(𝒔𝒕"𝟏) -V(𝒔𝒕)  Critic updateCriticActor𝑠$𝑟%𝑠$,𝑠$𝜋%(x|s)𝜋&(c|s,x)𝝅(x,c|s)=     *Action samplingV(s)Action sampling𝑎%Discrete offloading action policyContinuous allocation parameter policyE(ω) = 𝟏𝟐||V(ω)−ΛB.V(ω)||𝟐MSPBE LossGradient descent-based minimizationPolicy objective =    *Trust region optimizationSurrogate objectiveActor updateAdvantageBacktracking line searchTransformxAction-parameter policyACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

11

the total radio system bandwidth W . The computation state
Scomp(t) contains the information of current computational
resource Scomp(t) = {f (cid:96)
n(t)}n∈N . Lastly, the hash power
state Shash(t) presents the current hash power Ψn of all MDs:
Shash(t) = {Ψ1(t), Ψ2(t), ..., ΨN (t)}.

2) Action: Our BFL system features a parameterized action
space with ofﬂoading or local execution, as elaborated below:

• Ofﬂoading (transmit power, channel bandwidth allo-
cation, hash power allocation): When MD n chooses
the ofﬂoading mode xn,m,g = 1,
it must determine
relevant parameters, i.e., transmit power pn and channel
bandwidth bn,g that are needed for ofﬂoading. Also,
MDs perform mining regardless of their learning sta-
tus, and thus we also involve a hash power allocation
parameter Ψn. Therefore, the joint action on each MD
can be expressed by an = {xn,m,g, pn, bn,g, Ψn} and
the complete system action in this mode is a(t) =
xn,m,g(t), pn(t), bn,g(t), Ψn(t), ∀n ∈ N , m ∈ M, g ∈
G.

• Local Execution (computational allocation, hash power
allocation): When MD n chooses the local execution
mode xn,m,g = 0,
it must specify its necessary pa-
rameters to execute the model training, e.g., f (cid:96)
n. More-
over, similar
the parameter
to the ofﬂoading mode,
of hash power allocation is also involved to support
the block mining task executed after the model learn-
ing. Therefore, the joint action on each MD is an =
{xn,m,g, f (cid:96)
n, Ψn} and the complete action for the BFL
system in this local execution mode is given by a(t) =
xn,m,g(t), f (cid:96)

n(t), Ψn(t), ∀n ∈ N , m ∈ M, g ∈ G.

3) System Reward Function: The reward in our BFL system
comes from the joint model learning and block mining, by
maximizing the system returns in the long run. However,
in the optimization problem (28), our objective is to min-
imize the system latency in a certain aggregation round,
which requires a negative multiplication before being used
i.e., we deﬁne the reward as r(st, at) =
as the reward,
− (cid:0)T learn + T cons + T mine(cid:1). For better presentation, we trans-
form the latency minimization problem into a system utility
optimization problem by using a simple exponential equation:

(cid:32)



U =

e

(T learn+T cons+T mine)
τ

1−

(cid:33)



− 1

 ,

(31)

where τ denotes an upper bound of the system latency. (31)
implies that
the lower system latency results in a higher
system utility. Therefore, instead of minimizing the latency,
we are keen on maximining the system utility which better
characterizes the efﬁciency of our algorithm. Accordingly, we
transform (28) to the following optimization problem:

maximize
X,P ,B,F ,Ψ
s.t.

U

28b − 28i.

(32a)

(32b)

B. Policy Gradient Update for A2C

We ﬁrst analyze the policy gradient update necessary for the
design of actor and critic components that will be elaborated
later. In the BFL environment, the agent tries to search among
the set of parameterized ofﬂoading policies to obtain the
optimal policy π∗
θ (a, s) that can return the maximum reward,
i.e., system utility. However, in practice the search space may
be very large and the agent may not be able to ﬁnd the optimal
policy. Thus, we restrict the policy set by a vector θ ∈ Rz for
some integer z > 0 and perform the optimization over the
group of the parameterized policies πθ(a, s). To facilitate our
analysis, the following assumptions are introduced.

Assumption 4. The policy πθ and P(s(cid:48)|s, a) guarantee an ir-
reducible and aperiodic Markov chain described by P πθ (s(cid:48)|s),
∀θ. Therefore, there exists a stationary distribution deﬁned as
dπθ (s) on policy θ.

Assumption 4 is a common assumption for actor-critic
algorithms [28], [39]. Accordingly, we deﬁne J(πθ) = r(πθ),
where r is the system reward deﬁned in Section IV-A3, as the
performance function with respect to the policy parameterized
by θ. Accordingly, the objective function with linear Q-value
function approximation is given by

J(πθ) =

(cid:88)

s∈S

dπθ (s)Qπθ (s, a).

(33)

Next, similar to [39], we make an assumption on the policy
πθ(a, s).

Assumption 5. The following assumptions are made on the
policy function:

• Positive policy function: πθ(a|s) > 0, ∀θ ∈ Rd
• Bounded policy gradient:

||∇ log πθ(a|s)||2< Gπ, ∀θ, ∀s, ∀a, Gπ > 0

• (cid:96)-Lipschitz policy gradient:

||∇ log πθ1 − ∇ log πθ2||2≤ (cid:96)||θ1 − θ2||2, ∀θ1, ∀θ2

Here, the regularity conditions in Assumption 5 can be
satisﬁed by using the Gibbs softmax distribution network, e.g.,
in deep neutral networks, for action selection in the actor.
Under this assumption, the policy gradient πθ can be updated
as ∇J(πθ) = Es∼dπθ (.),a∼πθ(.|s) [Qπθ (s, a)∇ log πθ(a|s)].

C. Actor Design

In our A2C-based DRL algorithm, the actor aims to update
the parameter θ over time-step iterations to ﬁnd the optimal
policy π∗
θ that characterizes the best trajectory for our system
utility optimization problem. In other words,
the actor is
expected to make optimal model learning and block mining
decisions in a fashion that the long-term reward (i.e., system
utility) is maximized. In doing so, the actor needs to use the
gradient ∇J(πθ) to optimize its policy

J(πθ) = Es∼dπθ (.) [πθ(a|s)Aπθ (s, a)] ,

(34)

max
θ∈Rd

Thus, we re-deﬁne the system reward as a result of executing
the action with given states as r(st, at) = U (t).

the policy is optimized via a vanilla policy
Traditionally,
gradient algorithm by direct policy search over the entire

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

12

exploration space which is known to be inefﬁcient. Instead,
we use trust region policy optimization (TRPO) to improve
policy optimization by maximizing a surrogate objective over
a trust-region [40]. Accordingly, (34) can be re-written

max
θ∈Rd

J(πθ) = Eπθ

(cid:20) πθnew (a|s)
πθ(a|s)

Aπθ (s, a)

(cid:21)

,

(35)

subject to Es [KL (πθ(.|s)||πθnew (.|s))] ≤ (cid:15)KL,

for some (cid:15)KL > 0. In (35), Es represents the state visitation
distribution induced by πθ, and θ and θnew are the vectors of
policy parameters before and after each update, respectively.
By enforcing a Kullback-Leibler (KL)-divergence constraint
KL, the probability distributions of the policy before and
after the update will be kept closely in the parameter space
to avoid divergence in the gradient update. Considering our
parameterized action space including the ofﬂoading actions
and their parameters, the optimization objective function can
(a|s)πc
θnew
be expressed as Eπθ
c
(a|s)πc
θc
To solve (35), we construct a closed form solution for
computing the KL-divergence between the distributions of
the discrete ofﬂoading action policy and the action-parameter
policy for our BFL problem as follows:

(c|s,a) Aπθ(cid:48) (s, a)

(cid:20) πd
θnew
d
πd
θd

(c|s,a)

(cid:21)
.

Es[KL(πθ(a,c|s)||πθnew (a,c|s))]
(cid:17)

(cid:16)

(cid:104)

=Es

KL

=Es

(cid:104)

KL

(cid:16)

πd
θnew
d
πd
θnew
d
EsE

(a|s)||πd
θd

(a|s)

+KL

(cid:16)

πc
θnew
c

(c|s,a)||πc
θc

(cid:17)(cid:105)

(c|s,a)

(a|s)||πd
θd

(a|s)

(cid:17)(cid:105)

+

a∼πd
θnew
d

(a|s)

(cid:104)

KL

(cid:16)

πc
θnew
c

(c|s,a)||πc
θc

(c|s,a)

(cid:17)(cid:105)
.

(36)

By using the analytical form of the discrete ofﬂoading action
policy πd
d (a|s) via its trajectory probability, we can further
θnew
reduce the variance of the KL-divergence in the last term
of (36) as

Es[KL(πθ(a,c|s)||πθnew (a,c|s))]≈Es
(cid:16)

(cid:104)(cid:16)

(cid:17)

+Es

− log(πd
θnew
d

(a|s))

KL

πc
θnew
c

(a|s)||πd
θd

πd
θnew
d
(c|s,a)||πc
θc

(cid:104)

KL

(cid:16)

(cid:17)(cid:105)

(a|s)

(c|s,a)

(cid:17)(cid:105)
.

(37)

Based on the above approximation steps, the optimization of
the original policy in (34) is transformed into a conjugate gra-
dient form which allows for estimating the expectations of the
policy objective in (35) with policy improvement guarantees.
In this regard, the update direction can be approximated by
ν ≈ ˆH −1∇θJ(πθ), where ˆH −1 is the Hessian-vector product
of sampled KL-divergence. Finally, the actor parameter is
updated via backtracking line search [41] subject to the KL
constraint as follows:

θt+1 = θt + γa
t

(cid:114) 2(cid:15)KL
ν(cid:62) ˆHν

ν,

(38)

where γa
t ∈ (0, 1) is the backtracking step-size parameter
which controls the line search for guaranteeing the conjugate
gradient improvement given the KL divergence constraint. It
is desirable to set up a fairly large step size γa
t to initialize the
line search space on the policy, and gradually shrink γa
t until
a Armijo-Goldstein condition [41] is satisﬁed where a critical
(optimal) point is obtained.

D. Critic Design

The role of the critic is to estimate the state-value function
V πθ (s) to guide the update of the actor by approximating its
state-value function. Speciﬁcally, a feature function φ : S (cid:55)→
Rn is created as a full-ranked matrix with I dimensions to
create i-dimensional features (i ≤ I) for any state s ∈ S, i.e.,
φ(s) = (cid:0)φ1(s), ..., φi(s)(cid:1)(cid:62)
. Given a state s, the state-value
function is thus approximated by a linear function Vω(s) ≈
ωφ(s)(cid:62), where ω ∈ Rm is a parameter vector used to update
the state-value function. By function approximation, the critic
provides an inexact temporal difference (TD) solution to the
value function V πθ (s) under policy πθ. This naturally results
in the minimization of TD error as a loss function, i.e., the
Mean Squared Projected Bellman Error (MSPBE) deﬁned by

Eθ(ω) =

1
2

||Vω − ΛBθVω||2,

(39)

where Λ = φ(cid:62)(φM φ(cid:62))−1φH is the projector with H ∈
R|S|×|S| being a diagonal matrix whose elements are within
the stationary state distribution dπθ generated according to the
policy πθ when the entire state space is irreducible. Also,
Bθ denotes the Bellman operator implied by BθV (s) ←
r(s, a) + γP θ(s, s(cid:48), a)V (s), where V (s) is the state value,
r(s, a) is reward with discount γ ∈ (0, 1), and P θ(s, s(cid:48), a) is
the transition probability as deﬁned in IV-A. During the value
function evaluation process, the critic aims to minimize the
loss function in (39) to obtain a ﬁxed point of the projected
Bellman operator by gradient TD learning, where the Lipschitz
continuity property of the MSPBE loss function in (39) is
signiﬁcant to guarantee a successful policy-parameter update.
Note that the loss function in is quadratic and thus convex
with respect to θ.

1. Given

Lemma
(cid:0)φ1(s), ..., φi(s)(cid:1)(cid:62)
with (cid:96) = (1 + γ)2 maxi||φi||2

feature
the Eθ(ω)’s gradient
2, where i ∈ I.

the

,

vector φ(s)

=
is (cid:96)-Lipschitz

Proof. See Appendix C.

Based on the stochastic gradient descent on Eθ(ω) with
respect to parameter ω, the critic update can be achieved by

ωt+1 = ωt + γc

t Aπθ (st, at)∇ωVω(st),

(40)

where γc

t ∈ (0, 1) is a step-size parameter.

E. Training Procedure of Parametrized A2C Algorithm in BFL

The training procedure of our A2C algorithm is summarized
in Algorithm 2. To prepare for the training, we build a BFL
environment where multiple MDs participate in the FL training
and block mining with a set of ESs connected via wireless
links. The objective function in the system utility optimization
problem built in (32) is selected as a system DRL reward that
is obtained via iterative training with parameterized actions
and system states for an optimal ofﬂoading policy π∗
θ to
maximize the reward R in the long run with a initial system
state (line 5). We build an actor that consists of a deep neural
network (DNN) for ofﬂoading decision sampling and another
DNN for parameter sampling. At each timestep, we randomly
sample a set of discrete ofﬂoading actions for all MDs via

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

13

Algorithm 2 Training procedure of our A2C algorithm
1: Input: Time budget T , discount factor γ, BFL environment env
2: Output: Optimal parameterized action policy π∗

θ and maximum reward

R

3: Initialization: Initialize ofﬂoading policy parameters θd, parameterized
allocation policy parameter θc, critic parameter ω, actor’s learning rate
γa, critic’s learning rate γc, discount parameter γ, initial reward R = 0

Set up initial state s0
for timestep t = 1, 2, ..., T do

4: for each episode do
5:
6:
7:
8:
9:

Perform action sampling using πd(.|s) and πc(.|s)
for each MD n ∈ N do

Sample the discrete ofﬂoading action with policy xn ∼
πd
nθd (.|s)
Sample continuous allocation parameters for the selected of-
ﬂoading action xn
(cid:40)

(pn, bn,g, Ψn) ∼ πc
θc
(f (cid:96)
(.|s, xn),

n, Ψn) ∼ πc
θc

(.|s, xn),

if xn,m,g = 1
if xn,m,g = 0

cn =

end for
Execute the parameterized action at ← (xn,m,g,t, cn,t), ∀n ∈ N
in the BFL environment
Obtain reward rt and next state st+1: (rt, st+1) ← env.step(at)

for

the policy gradient

Calculate the accumulated reward Rt+1 ← rt + γt ∗ Rt
Compute the advantage function Aπθ (s, a) using (29) based on rt
and V πθ (s)
Estimate
Es∼dπθ (.),a∼πθ (.|s) [Aπθ (s, a)∇π log πθ(a|s)]
Optimize the actor policy via TRPO in (35) with computed ad-
vantage value and update its parameter with KL constraint in (38):
(cid:113) 2(cid:15)KL
θt+1 = θt + γa
t
ν(cid:62) ˆHν
Calculate the MSPBE loss for critic via (39)
Optimize Vω for the critic and update its policy in (40) via TD
error σt: ωt+1 = ωt + γc

t Aπθ (st, at).∇ωVω(st)

actor: ∇J(πθ) =

the

ν

10:

11:
12:

13:

14:
15:

16:

17:

18:
19:

end for

20:
21: end for

a DNN-empowered policy πd(.|s). Then we also sample a
set of continuous allocation parameters based on the sampled
ofﬂoading decision using another policy πc
(.|s, xn) as deﬁned
θc
in IV-A (lines 7-11). With the result of the action sampling
step, we generate a complete set of parameterized actions for
all MDs which is ready to be executed in the BFL environment
(line 12). This allows us to obtain the reward, i.e., system
utility, to calculate the long-term return and the agent moves
to the next state needed for the following step of training
(lines 13-14). Then, the actor and critic updates their policy
(lines 15-19): (i) the actor computes the policy gradient via its
advantage function and TRPO to optimize its policy, and (ii)
the critic computes the MSPBE loss function and updates its
gradient via TD error learning.

V. SIMULATIONS AND PERFORMANCE EVALUATION

A. Parameter Settings

We conduct numerical experiments to verify our method
under various parameter settings. Inspired by related works
[12], [14], [15], [24], [29], [35], we set up all necessary
parameters for our BFL environment as listed in Table IV.

We consider a BFL system with 3 ESs and 10 MDs
which aim to collaboratively train two popular image datasets:
SVHN1
(including 73,257 training instances and testing

Table IV: Simulation parameters.

Parameter
Number of ESs M
Number of MDs N
Number of sub-channels at each ES K
Data size Dn
CPU workload of MDs and ESs
MDs’ transmit power pn
MD’s computational capability f (cid:96)
n
ES’s computational capability fm
Background noise variance
Maximum system bandwidth W
MD’s model size ϑ
MD’s energy coefﬁcient κ
MD’s hash rate Ψn
MD’s mining power efﬁciency Ξn
Hash amount of a block ¯h
Block broadcasting rate ξ
Number of forking occurrences ζ
MD’s maximum latency τn
Noise power spectral density N0
ES’s bandwidth bm(cid:48)
ES’s transmit power pm(cid:48)

Value
5
[20-100]
5
[0.5-2] MB
[0.7-1.1] Gcyles
[10-30] dBm
[0,2-2] GHz
5 GHz
-100 dBm
20 MHz
5 KB
5 ∗ 10−27
[100-1000] GHash/s
5 ∗ 10−8 J/hash
50 GHash
0.005
3
3 sec
-174 dBm/Hz
5 MHz
[100-120] dBm

26,032 instances) and Fashion-MNIST2 (including 60,000
training instances and testing 10,000 instances), where each
dataset contains 10 labels/classes. For the SVHN dataset, we
deploy a convolutional neural network (CNN) with two 2-
D convolutional layers followed by two hidden layers (the
ﬁrst with 256 units and the second with 72 units) with ReLU
activation. The CNN architecture used for the Fashion-MNIST
dataset is similar, with the ﬁrst hidden layer with 320 units and
the second hidden layer with 50 units. We investigate the FL
performance under both IID and non-IID data settings. In IID
data setting, each MD possesses datapoints from all the 10
labels, while in non-IID data setting, each MD contains data
samples from three of 10 labels for the SVHN dataset and
two of 10 labels for the Fashion-MNIST dataset. We employ
the Adam optimizer with mini-batch size of 25 and 10 SGD
iterations.

In our A2C algorithm, the actor has two DNNs, one for
the discrete ofﬂoading policy with two hidden neural layers
{ 64 and 32 in sizes} and one for the continuous allocation
parameter policy with two layers {128 and 64 in sizes}. For
the output layers, we used Softmax to generate ofﬂoading
decisions for MDs and adopted Tanh to produce allocation
parameters given the discrete ofﬂoading policy. The KL diver-
gence constraint (cid:15)KL is set to 0.01 for the TRPO-based actor
policy optimizer [40]. The critic was also built by a DNN
that contains two hidden layers of sizes {200, 100} to train
the state-value function of our A2C scheme with the Adam
optimizer. To implement our parameterized A2C algorithm,
we set up a virtual agent that is allowed to interact with a pre-
deﬁned BFL environment to learn the parameterized ofﬂoading
policy for all MDs and observe the return, i.e., system utility,
after each iteration. We trained the agent over 10000 episodes
with 100 timesteps per each episode. All numerical results
are averaged over 10 independent simulation runs.

B. Evaluation of FL Performance

We evaluate the classiﬁcation accuracy and loss perfor-
mance of our proposed BFL scheme (with 5 consensus rounds)

1http://uﬂdl.stanford.edu/housenumbers/

2https://github.com/zalandoresearch/fashion-mnist

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

14

(a) Accuracy under IID setting.

(b) Accuracy under non-IID set-
ting.

(a) Accuracy under IID setting.

(b) Accuracy under non-IID set-
ting.

(c) Loss under IID setting.

(d) Loss under non-IID setting.

(c) Loss under IID setting.

(d) Loss under non-IID setting.

Fig. 4: Comparison of different FL approaches on SVHN
dataset.

Fig. 6: Comparison of BFL performance with different con-
sensus rounds on Fashion-MNIST dataset.

(a) Accuracy under IID setting.

(b) Accuracy under non-IID set-
ting.

(a) System rewards with different
actor learning rates.

(b) System rewards with different
critic learning rates.

(c) Loss under IID setting.

(d) Loss under non-IID setting.

Fig. 5: Comparison of different FL approaches on Fashion-
MNIST dataset.

and compare it with two related schemes. The ﬁrst one is
a traditional FL scheme [1], where ESs work independently
and each only aggregates the models of its associated MDs
and then broadcasts the resulting model across devices. The
second one is a traditional BFL without P2P consensus [4], in
which each ES runs an averaging algorithm by collaborating
with its MDs to build its aggregated model, and then a
random ES is selected as a leader that builds a global model
based on its local aggregated model without conducting P2P
consensus. Fig. 4 illustrates the performance when training
the SVHN dataset, showing the considerable improvements in
terms of higher accuracy and lower loss compared with the
counterparts. Although the performance degrades when the
dataset becomes non-IID, our consensus-based BFL scheme
still outperforms other algorithms. The BFL scheme without
consensus achieves a better training performance than the

Fig. 7: Evaluation of the training performance.

traditional FL scheme since its randomized leader ES selection
avoid local model bias across the clients. Moreover,
the
performance gap between our scheme and the others becomes
larger in the non-IID case which demonstrates the beneﬁt of
consensus-based model aggregation over existing approaches.
The advantages of our scheme are also veriﬁed on the Fashion-
MNIST dataset in both IID and non-IID settings, as indicated
in Fig. 5. For example, in the IID setting, our consensus-
based BFL scheme improves the accuracy rate by 8% and
14% in comparison with the traditional BFL scheme without
consensus and the traditional FL scheme, respectively.

Fig. 6 investigates the impact of P2P consensus rounds
(i.e., 5, 10, and 15 rounds) on the learning performance. We
can see that the increase of consensus rounds signiﬁcantly
improves the performance. Under the non-IID setting, the role
of consensus on the model training becomes more signiﬁcant
with a larger performance gap, for example, between 5 rounds
and 15 rounds, which shows the efﬁciency of our consensus-
based BFL design for federated model training.

C. Evaluation of DRL Training Performance

We ﬁrst investigate the training system reward (i.e., sys-
tem utility as deﬁned in (32)) performance by changing the
learning rates at both actor and critic, which is important to
determine a reliable parameter set for our later simulations,
as shown in Fig. 7. If the learning rate is too small, the

020406080100Communication rounds0.750.800.850.900.95Test AccuracyProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.250.300.350.400.450.500.550.60Test AccuracyProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.00.10.20.30.4LossProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.10.20.30.40.50.60.7LossProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.40.50.60.70.8Test AccuracyProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.10.20.30.40.50.60.7Test AccuracyProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.40.60.81.01.21.4LossProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.60.81.01.21.41.61.8LossProposed consensus-based BFLTraditional BFL without consensusTraditional FL020406080100Communication rounds0.600.650.700.750.800.85Test AccuracyBFL with 15 consensus roundsBFL with 10 consensus roundsBFL with 5 consensus rounds020406080100Communication rounds0.30.40.50.60.7Test AccuracyBFL with 15 consensus roundsBFL with 10 consensus roundsBFL with 5 consensus rounds020406080100Communication rounds0.60.81.01.2LossBFL with 15 consensus roundsBFL with 10 consensus roundsBFL with 5 consensus rounds020406080100Communication rounds0.30.40.50.60.70.80.91.0LossBFL with 15 consensus roundsBFL with 10 consensus roundsBFL with 5 consensus rounds020406080100Training episodes (x102)0.500.751.001.251.501.752.00System rewardActor learning rate = 0.001Actor learning rate = 0.003Actor learning rate = 0.005020406080100Training episodes (x102)1.21.41.61.82.0System rewardCritic learning rate = 0.01Critic learning rate = 0.02Critic learning rate = 0.03ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

15

transmit power variable of MD n is discretized into Z levels as
(cid:17) 1

Pn =

(cid:20)
0, pmin, pmin.

(cid:16) pmax
pmin

(cid:21)
, where pn = 0

|Z|−2 , ..., pmax

(a) System reward with different
action spaces.

(b) System reward with different
learning schemes.

Fig. 8: Comparison of system reward via learning curves.

(a) System latency with different
numbers of MDs.

(b) System latency versus consen-
sus rounds.

Fig. 9: Comparison of system latency between different
schemes.

policy training probably requires long time to achieve an
optimal solution. However, if we set a large learning rate,
the training may become unstable and possibly diverge. In
A2C algorithms, since the actor updates slower than the
critic with a timestep, we set up the actor with a learning
rate smaller than the critic’s learning rate. We ﬁrst evaluate
the impacts of the actor learning rate, i.e., the backtracking
step-size parameter γa
t which controls the policy update in
the trust region in each iteration of TRPO under the KL
divergence constraint. Fig. 7(a) reveals that the learning rate
of 0.003 exhibits the highest system reward with the fastest
convergence, compared to the case of γa
t = 0.001 which shows
the slowest convergence rate. However, when the learning rate
is relatively high (γa
t = 0.005), the learning process becomes
unstable and diverges. Similar to the actor part, we also set
up a learning procedure by considering various critic learning
rates. As indicated in Fig. 7(b), the learning rate γc
t = 0.02 has
the most stable training performance with a quick convergence,
compared to other learning settings. Thus, we will use learning
rates γa
t = 0.02 for the following simulations.
Next, we compare the reward performance obtained by
our proposed parameterized A2C algorithm against two base-
line schemes: A2C with relaxed action space [35] and A2C
with approximated action space [36]. For the ﬁrst baseline
scheme, we ﬁrst relax the discrete ofﬂoading vector into
a continuous set by deﬁning a relaxed space as Aa =
{f (x1, ), f (x2), ..., f (xN )}, where f (.) is a probability soft-
max function, and then re-normalize them to approximate
discrete ofﬂoading vectors for execution. Compared with our
proposed parameterized scheme, this method signiﬁcantly in-
creases the sampling complexity on the joint action space. For
the second baseline scheme, we discretize each of continuous
allocation vectors into a discrete subset. For example, the

t = 0.003 and γc

implies the local execution mode. This discretization process
creates a large number of quantization levels for convenient
action sampling but also results in quantization noise.

From Fig. 8(a), our proposed scheme can achieve the
best reward performance compared with baseline approaches,
thanks to a ﬂexible parameterized action sampling solution
where both discrete ofﬂoading decisions and allocation vari-
ables are directly trained without relaxation or approximation.
Meanwhile, the A2C scheme with relaxed action space suffers
a reward decrease with high training variance since the ofﬂoad-
ing action selection must be converted into a continuous space
of allocation vectors, leading to an extremely high complexity
in the action sampling and thus making the training inefﬁcient.
Moreover, its complex action sampling requires longer time
to reach convergence, i.e., after 4000 episodes compared to
2500 episodes in our proposed scheme. The lowest reward
the approximated scheme, where the
gain is observed at
approximation of action space introduces the quantization
error which degrades the policy training.

We then compare our scheme with state-of-the-art DRL
approaches: (i) A2C [12], as in our method but only using the
vanilla gradient update at the actor without policy optimization
improvement; (ii) deep deterministic policy gradient (DDPG),
using actor-critic with deterministic policy training over the
continuous action space [35]; (iii) standard actor-critic [28],
i.e., without using advantage function in the policy estimation;
and (iv) deep Q-network [22], which uses action sampling
approximation. From Fig. 8(b), we ﬁnd that our proposed
parameterized A2C method is better than baselines in terms
of system reward and convergence rate and stability. This
is due to the efﬁcient action sampling and improved policy
optimization based on trust region enforcement which helps
avoiding possible gradient divergence. Thus, a faster and more
reliable policy search is achieved toward an optimal solution.
The DDPG scheme has a lower reward convergence speed
due to the relaxation of the discrete ofﬂoading action space.
This increases action sampling complexity and thus makes the
policy training less efﬁcient compared with our parameterized
A2C. On the other hand, we see that DDPG outperforms
standard A2C, which demonstrates the importance of the
policy optimization improvement step employed in our scheme
over a hybrid continuous and discrete action space. Although
the standard A2C scheme achieves a relatively stable reward
performance, its reward gain is lower than that of our approach
due to the natural gradient descent with greedy policy update.

D. Evaluation of System Latency Performance

In this subsection, we evaluate the system latency per-
formance under networking scenarios. Based on the system
reward (utility) computed via the above training process, it
is straightforward to calculate the system latency via their
mathematical relation as mentioned in (31) and (32).

We investigate the latency performance when varying the
numbers of MDs from 20 to 100 in Fig. 9(a). The system
latency obtained by our proposed parameterized A2C scheme

020406080100Training episodes (x102)0.60.81.01.21.41.61.82.02.2System rewardProposed A2C with parameterized action spaceA2C with approximated action spaceA2C with relaxed action space020406080100Training episodes (x102)0.81.01.21.41.61.82.0System rewardProposed parameterized A2CStandard A2C DDPGStandard actor-criticDeep Q-network2030405060708090100Number of MDs6080100120140160180200220System latency (second)Proposed parameterized A2C schemeA2C schemeDDPG schemeStandard actor-critic schemeDeep Q network0.02.55.07.510.012.515.017.520.0Number of consensus rounds5075100125150175200225250System latency (second)Proposed parameterized A2C schemeA2C schemeDDPG schemeStandard actor-critic schemeDeep Q networkACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

16

(a) System latency versus transmit power
allocation.

(b) System latency versus bandwidth alloca-
tion.

(c) System latency versus mining hash allo-
cation.

Fig. 10: Comparison of system latency with different resource allocation settings.

is the lowest across the considered methods. As expected, the
latency of each method increases with the number of MDs,
due to a higher ofﬂoading and mining latency caused by a
higher competition on bandwidth and hash allocation among
MDs, our proposed scheme still achieves the best latency
performance when the number of MDs increases. For instance,
with 100 MDs, the system latency of our scheme is 11%, 13%,
25% and 38% lower than that of the DDPG, A2C, actor-critic,
and deep Q-network schemes, respectively.

the edge layer enhances the model

We then investigate the impact of P2P rounds on the system
latency, as indicated in Fig. 9(b). We change the number of
P2P rounds from 0 to 20, where 0 implies the traditional BFL
scheme without consensus. Although the use of consensus
training
procedure at
performance, it potentially increases the system latency due
to the additional delay of P2P model aggregation among ESs.
However, with our advanced DRL design, the proposed param-
eterized A2C scheme achieves the minimal latency compared
with other baselines. The increase of P2P rounds requires more
time for model aggregation, leading to a higher system latency,
but our approach still has the best performance.

We also investigate the latency performance under different
resource allocation scenarios in Fig. 10 in a BFL environment
with 5 ESs and 30 MDs. We ﬁrst vary the maximum transmit
power Pn at each MD n between 10 and 30 dBm and then
measure the latency of ofﬂoading and mining. As shown in
Fig. 10(a), the latency of all schemes decreases with respect
to the increasing amount of allocated transmit power. This is
because a larger transmit power improves the data transmission
rate that helps reduce the data ofﬂoading latency. Notably,
compared with other schemes, our method achieves the most
signiﬁcant and stable latency decrease due to our efﬁcient
and robust parameterized policy training to obtain the better
ofﬂoading trajectory. Another interesting observation is that
when the maximum transmit power exceeds 20 dBm, the
system latency reduces at a slower rate. This is due to the
less impact of such a large power allocation on the ofﬂoading
latency savings given a certain number of MDs and data sizes.

Moreover, we investigate the latency trends of different
algorithms when the maximum system bandwidth W varies
from 20 to 30 MHz in Fig. 10(b). Similar to the power
allocation scenario, more bandwidth would help mitigate the
ofﬂoading latency. In our simulation setting, the impact of
bandwidth on the system latency is signiﬁcant before the

threshold of 25 MHz, where our scheme shows the best latency
savings. Compared with DDPG, A2C and actor-critic schemes,
our parameterized A2C scheme can reduce the latency by 7%,
8% and 17%, respectively, and achieves a 60% lower latency
compared with the deep Q-network baseline.

Finally, we compare the latency performance with respect to
the changes of mining hash power capability Ψmax. A higher
hash allocation would help reduce the block generation latency
for a lower mining and system latency. By increasing the hash
budget, each MD has a higher chance to obtain sufﬁcient hash
power to run the block mining. As can be seen in Fig. 10(c),
our proposed scheme outperforms other baselines in terms of
system latency in each hash allocation setting. The simulation
results thus demonstrate the efﬁciency of our proposed A2C
algorithm design in the system latency minimization.

E. Attack Evaluation

1) Attack Model and Security Analysis with Blockchain:
A major concern in conventional FL with a single centralized
aggregation server is that the server may become compromised
via model poisoning attacks launched by adversaries. In this
section, we investigate how blockchain can help mitigate
impact of model poisoning attacks on the ML model training
performance, which as discussed in Section I-B is one of
our motivations for integrating blockchain with FL. Model
poisoning attacks mainly consist of untargeted model poison-
ing attacks and targeted model poisoning attacks [42]. The
former category aims to degrade the accuracy of the global
model training, whereas the latter aims to control the model
deviation towards their target. Here, we focus on untargeted
model poisoning attacks on our BFL system.

We assume that at a certain global FL round k, an ES can
be compromised by a model poisoning attack following two
steps. First, the attacker injects certain random noise v(k) to the
aggregated global model w(k) obtained via 12 to manipulate
the global model update w(k) ← w(k) + v(k). Here we adopt
Gaussian noise, i.e., v(k) = −(cid:36)q(k), where (cid:36) is a scaling
factor which characterizes the magnitude of the compromised
model, and q(k) is a random vector sampled from the Gaussian
distribution N (0, I). Next, the attacked ES broadcasts the
compromised global model to local MDs.

In our BFL system, blockchain replaces the centralized
authority in FL with a decentralized tamper-proof data ledger

10.012.515.017.520.022.525.027.530.0Maximum transmit power (dBm)304050607080System latency (second)Proposed parameterized A2C schemeA2C schemeDDPG schemeStandard actor-critic schemeDeep Q network202224262830Maximum system bandwidth (MHz)1020304050607080System latency (second)Proposed parameterized A2C schemeA2C schemeDDPG schemeStandard actor-critic schemeDeep Q network2004006008001000Mining hash power capability (GHash/s)1020304050607080System latency (second)Proposed parameterized A2C schemeA2C schemeDDPG schemeStandard actor-critic schemeDeep Q networkACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

17

to monitor the model consensus process as well as mitigate
single-point-of-failure. By deploying a blockchain over the
edge layer, any model update event over consensus rounds
at a certain ES is automatically traced by other ESs. If a
poisoning attack takes place at an ES, other ESs can detect
this behavior via transaction logs. Here, we adopt the attack
detection score metric [43], which characterizes the abnormal
gradient deviation caused by the poisoning attacker at a certain
FL round, to detect the occurrence of a model attack at a
certain ES. Since all gradient update information including
the compromised gradient is recorded on the digital ledger,
blockchain can identify the compromised ES and temporarily
disregard it from the ES network, while the global model
aggregation process continues.

2) Attack Simulation: We investigate the training perfor-
mance of different FL methods under model poisoning attacks
on the two considered datasets. We consider an attack scenario
where an adversary compromizes an ES and deploys model
poisoning attacks by injecting random noise at the global
rounds of 20 and 60 during the model consensus process
among ESs. Our proposed consensus-based BFL scheme with
blockchain is compared with other three baselines: consensus-
based FL without blockchain, BFL without consensus, and
traditional FL. For the consensus-based schemes, we consider
that there are 5 consensus rounds and the attacker poisons the
model at round 3. For the BFL scheme without consensus, the
attacker poisons one of the ESs during the global model ag-
gregation process. For the traditional FL scheme, the attacker
poisons the centralized aggregator.

As illustrated in Fig. 11 for the SVHN dataset, the ac-
curacy performance of all schemes is dropped when the
attack occurs (i.e., at aggregation rounds 20 and 60). How-
ever, our consensus-based BFL scheme achieves the best
accuracy rate and highest robustness against the poisoning
attack in both IID and non-IID data distribution settings.
For instance, from Fig. 11(a), we see that the accuracy of
our scheme at the onset of an attack only drops by 3.7%,
as opposed to 7.6%, 16.9%, and 32.1% in the consensus-
based FL without blockchain, BFL without consensus, and
traditional FL schemes, respectively. The traceable data ledger
of blockchain records all the global updating behaviors of ESs
across consensus rounds, which allows blockchain to rapidly
detect the poisoning attack. Since all ESs are connected on
the shared ledger, the blockchain disregards this ES from the
consensus process to protect model training. The consensus-
based FL scheme without blockchain has the second highest
performance. The poisoned model is involved in the consensus
process, which degrades the overall model aggregation, though
the impact is dampened since the poisoned model is only
one participating in the consensus process. The other two
non-consensus-based methods show lower accuracy and less
robustness against the attack. The BFL scheme without con-
sensus shows lower training robustness since the attack at one
of the ESs signiﬁcantly affects the global model aggregation at
that particular server and its associated MDs. In the traditional
FL scheme with a centralized server, the model training is
degraded after the attack since the attacker directly poisons
these experiments
the only model aggregator. We repeat

(a) Accuracy under IID setting.

(b) Accuracy under non-IID setting.

Fig. 11: Comparison of different FL approaches under model
poisoning attacks (SVHN dataset).

(a) Accuracy under IID setting.

(b) Accuracy under non-IID setting.

Fig. 12: Comparison of different FL approaches under model
poisoning attacks (Fashion-MNIST dataset).

on the Fashion-MNIST dataset in Fig. 12 which indicates
qualitatively similar results. For example, in Fig. 12(a), the
accuracy rate of our BFL scheme with blockchain only reduces
by 2.67%, compared with 5.6% in the consensus-based FL
without blockchain, 6.8% in the BFL without consensus, and
16.7% in the traditional FL scheme at the onset of poisoning
attack.

VI. CONCLUSION

This paper studied a decentralized BFL system in multi-
server edge computing with a holistic design of both
ofﬂoading-assisted ML model training and mobile block min-
ing schemes. A model aggregation solution has been developed
via P2P-based consensus among ESs to build a global model
that is shared with MDs via Blockchain for reliable model
learning empowered by block mining. We aimed to minimize
the system latency by a parameterized A2C algorithm with a
careful design of actor and critic. A comprehensive analysis
of the convergence properties of our proposed BFL model
was given. Numerical simulations veriﬁed the superior per-
formance of our proposed consensus-based BFL scheme over
state-of-the-art schemes in terms of higher accuracy and lower
loss. The proposed parameterized A2C algorithm exhibited the
faster convergence rate and lower system latency, compared
with the existing DRL schemes. Our blockchain-empowered
BFL scheme also achieved high robustness against model
poisoning attacks.

REFERENCES

[1] S. Wang et al., “Adaptive federated learning in resource constrained edge
computing systems,” IEEE J. Sel. Areas in Commun., vol. 37, no. 6, pp.
1205–1221, 2019.

[2] D. C. Nguyen et al., “Federated learning for Internet of Things: A
comprehensive survey,” IEEE Commun. Surveys & Tutorials, vol. 23,
no. 3, pp. 1622–1658, 2021.

020406080100Communication rounds0.450.500.550.600.650.700.750.800.85Test AccuracyProposed consensus-based BFL with blockchainConsensus-based FL without blockchainBFL without consensusTraditional FL020406080100Communication rounds0.20.30.40.50.60.70.8Test AccuracyProposed consensus-based BFL with blockchainConsensus-based FL without blockchainBFL without consensusTraditional FL020406080100Communication rounds0.450.500.550.600.650.700.750.80Test AccuracyProposed consensus-based BFL with blockchainConsensus-based FL without blockchainBFL without consensusTraditional FL020406080100Communication rounds0.350.400.450.500.550.600.650.70Test AccuracyProposed consensus-based BFL with blockchainConsensus-based FL without blockchainBFL without consensusTraditional FLACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

18

[3] C. Ma, J. Li, M. Ding, H. H. Yang, F. Shu, T. Q. Quek, and H. V. Poor,
“On safeguarding privacy and security in the framework of federated
learning,” IEEE Net., vol. 34, no. 4, pp. 242–248, 2020.

[4] F. Ayaz, Z. Sheng, D. Tian, and Y. L. Guan, “A blockchain based
federated learning for message dissemination in vehicular networks,”
IEEE Trans. Vehicular Techno., 2021.

[5] V. Mothukuri et al., “Fabricﬂ: Blockchain-in-the-loop federated learning

for trusted decentralized systems,” IEEE Systems J., 2021.

[6] Y. He, K. Huang, G. Zhang, F. R. Yu, J. Chen, and J. Li, “Bift:
A blockchain-based federated learning system for connected and au-
tonomous vehicles,” IEEE Internet of Things J., 2021.

[7] Q. Hu et al., “Blockchain and federated edge learning for privacy-
preserving mobile crowdsensing,” IEEE Internet of Things J., 2021.
[8] Y. Zhao, J. Zhao, L. Jiang, R. Tan, D. Niyato, Z. Li, L. Lyu, and
Y. Liu, “Privacy-preserving blockchain-based federated learning for IoT
devices,” IEEE Internet of Things J., vol. 8, no. 3, pp. 1817–1829, 2020.
[9] D. C. Nguyen et al., “Federated learning meets blockchain in edge
computing: Opportunities and challenges,” IEEE Inter. Things J., 2021.
[10] W. Shi et al., “Joint device scheduling and resource allocation for
latency constrained wireless federated learning,” IEEE Trans. Wireless
Commun., vol. 20, no. 1, pp. 453–467, 2020.

[11] Q. Ma et al., “FedSA: A semi-asynchronous federated learning mecha-
nism in heterogeneous edge computing,” IEEE J. Sel. Areas in Commun.,
vol. 39, no. 12, pp. 3654–3672, 2021.

[12] H. Yang et al., “Privacy-preserving federated learning for UAV-enabled
networks: Learning-based joint scheduling and resource management,”
IEEE J. Sel. Areas in Commun., 2021.

[13] C. W. Zaw, S. R. Pandey, K. Kim, and C. S. Hong, “Energy-aware
resource management for federated learning in multi-access edge com-
puting systems,” IEEE Access, vol. 9, pp. 34 938–34 950, 2021.
[14] Z. Ji, L. Chen, N. Zhao, Y. Chen, G. Wei, and F. R. Yu, “Computation
ofﬂoading for edge-assisted federated learning,” IEEE Trans. Vehicular
Techno., vol. 70, no. 9, pp. 9330–9344, 2021.

[15] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy
efﬁcient federated learning over wireless communication networks,”
IEEE Trans. Wireless Commun., vol. 20, no. 3, pp. 1935–1949, 2020.

[16] C. Hu, J. Jiang, and Z. Wang, “Decentralized federated learning: A

segmented gossip approach,” arXiv:1908.07782, 2019.

[17] H. Xing, O. Simeone, and S. Bi, “Decentralized federated learning via
SGD over wireless D2D networks,” in Proc. IEEE 21st Inter. Workshop
on Signal Processing Advances in Wireless Commun., 2020, pp. 1–5.

[18] F. P.-C. Lin et al., “Semi-decentralized federated learning with cooper-
ative D2D local model aggregations,” IEEE J. Sel. Areas in Commun.,
vol. 39, no. 12, pp. 3851–3869, 2021.

[19] S. Hosseinalipour et al., “Multi-stage hybrid federated learning over
large-scale D2D-enabled fog networks,” IEEE/ACM Trans. Net., 2022.
[20] H. Kim et al., “Blockchained on-device federated learning,” IEEE

Commun. Lett., vol. 24, no. 6, pp. 1279–1283, 2019.

[21] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for
autonomous vehicles: Analysis and design challenges,” IEEE Trans.
Commun., vol. 68, no. 8, pp. 4734–4746, 2020.
[22] Y. Lu, X. Huang, K. Zhang, S. Maharjan,

and Y. Zhang,
“Communication-efﬁcient
permissioned
blockchain for digital twin edge networks,” IEEE Internet of Things J.,
vol. 8, no. 4, pp. 2276–2288, 2020.

federated

learning

and

[23] M. Aloqaily, I. Al Ridhawi, and M. Guizani, “Energy-aware blockchain
and federated learning-supported vehicular networks,” IEEE Trans.
Intell. Transpor. Systems, 2021.

[24] X. Deng et al., “On dynamic resource allocation for blockchain assisted

federated learning over wireless channels,” arXiv:2105.14708, 2021.

[25] B. Ganguly et al., “Multi-edge server-assisted dynamic federated learn-
ing with an optimized ﬂoating aggregation point,” arXiv preprint
arXiv:2203.13950, 2022.

[26] S. Hosseinalipour et al., “Parallel successive learning for dynamic
training over heterogeneous wireless networks,”

distributed model
arXiv:2202.02947, 2022.

[27] X. Xiong, K. Zheng, L. Lei, and L. Hou, “Resource allocation based
on deep reinforcement learning in IoT edge computing,” IEEE J. Sel.
Areas in Commun., vol. 38, no. 6, pp. 1133–1146, 2020.

[28] S. Chen et al., “Multi-agent deep reinforcement learning-based coop-
erative edge caching for ultra-dense next-generation networks,” IEEE
Trans. Commun., vol. 69, no. 4, pp. 2441–2456, 2020.

[29] D. C. Nguyen et al., “Privacy-preserved task ofﬂoading in mobile
blockchain with deep reinforcement learning,” IEEE Trans. Net. and
Service Manag., vol. 17, no. 4, pp. 2536–2549, 2020.

[30] L. Xiao and S. Boyd, “Fast linear iterations for distributed averaging,”

Systems & Control Lett., vol. 53, no. 1, pp. 65–78, 2004.

[31] L. Xiao, S. Boyd, and S.-J. Kim, “Distributed average consensus
with least-mean-square deviation,” J. Parallel and Distributed Comput.,
vol. 67, no. 1, pp. 33–46, 2007.

[32] C. T. Dinh, N. H. Tran, M. N. Nguyen, C. S. Hong, W. Bao, A. Y.
Zomaya, and V. Gramoli, “Federated learning over wireless networks:
Convergence analysis and resource allocation,” IEEE/ACM Trans. Net.,
vol. 29, no. 1, pp. 398–409, 2020.

[33] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the
objective inconsistency problem in heterogeneous federated optimiza-
tion,” Proc. Advances in Neural Infor. Processing Systems, vol. 33, pp.
7611–7623, 2020.

[34] C. Huang et al., “Zkrep: A privacy-preserving scheme for reputation-
based blockchain system,” IEEE Internet of Things J., vol. 9, no. 6, pp.
4330–4342, 2021.

[35] D. C. Nguyen et al., “Cooperative task ofﬂoading and block mining in
blockchain-based edge computing with multi-agent deep reinforcement
learning,” IEEE Trans. Mobile Comput., 2021.

[36] F. Meng, P. Chen, L. Wu, and J. Cheng, “Power allocation in multi-
user cellular networks: Deep reinforcement learning approaches,” IEEE
Trans. Wireless Commun., vol. 19, no. 10, pp. 6255–6267, 2020.
[37] H. Lu et al., “Edge QoE: Computation ofﬂoading with deep reinforce-
ment learning for internet of things,” IEEE Internet of Things J., vol. 7,
no. 10, pp. 9255–9265, 2020.

[38] M. Akbari et al., “Age of information aware VNF scheduling in
industrial IoT using deep reinforcement learning,” IEEE J. Sel. Areas in
Commun., 2021.

[39] S. Qiu et al., “On ﬁnite-time convergence of actor-critic algorithm,”

IEEE J. Sel. Areas in Info. Theory, vol. 2, no. 2, pp. 652–664, 2021.

[40] L. Shani et al., “Adaptive trust region policy optimization: Global
convergence and faster rates for regularized MDPS,” in Proc. AAAI Conf.
Artiﬁcial Intell., vol. 34, no. 04, 2020, pp. 5668–5675.

[41] S. Vaswani et al., “Painless stochastic gradient: Interpolation,

line-
search, and convergence rates,” Proc. Advances in Neural Infor. Pro-
cessing Systems, vol. 32, pp. 3732–3745, 2019.

[42] M. Fang et al., “Local model poisoning attacks to Byzantine-robust
federated learning,” in Proc. 29th USENIX Security Symposium, 2020,
pp. 1605–1622.

[43] A. Mondal, H. Virk, and D. Gupta, “Beas: Blockchain enabled asyn-
chronous & secure federated machine learning,” Proc. Third AAAI
WRKSH Privacy-Preserving Artif. Intel., 2022.

[44] H.-T. Wai et al., “Provably efﬁcient neural GTD for off-policy learning,”
Proc. Advances in Neural Infor. Processing Systems, vol. 33, pp. 10 431–
10 442, 2020.

[45] S. L. Lohr, Sampling: Design and Analysis: Design And Analysis. CRC

Press, 2019.

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

19

A. Brief Recap of ML Model Training

APPENDIX A
PROOF OF THEOREM 1

Let set N (k)

m contain the devices associated with ES m during global aggregation k and also the ES m itself. Also, let y
denote the index of an arbitrary ES/MD. Each ES m at global aggregation k acquires its local aggregated gradient, which
is then normalized by the total number of data points D(k) to form ∇F A,(k)
according to (8), which can be written in the
compact form as

m

Then, the ESs engage in consensus process, where the ﬁnal parameter at each ES m can be expressed as

∇F A,(k)

m =

(cid:88)

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,k
y

.

∇F L,(k)

m =

(cid:88)

m∈M

∇F A,(k)

m + c(k)

m =

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+ c(k)
m ,

(41)

(42)

m denotes the error of consensus, which is caused by ﬁnite rounds of P2P communications. The selected ES at

where c(k)
aggregation round k, denoted by mk, then adds a boosting coefﬁcient (cid:80)
the following vector:

(cid:80)

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

m∈M

to its local gradient to form

∇F

(k)
mk

=

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

∇F L,(k)
mk

=

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)

y +

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

The aggregator ES mk then updates the global model parameter as follows:

w(k+1) = w(k) − ηk∇F

(k)
mk

.

B. Convergence Analysis

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

c(k)
mk

.

(43)

(44)

Using the β-smoothness of the global loss function (Assumption 2), we have:
w(k+1) − w(k)(cid:17)

F (k)(w(k+1)) ≤ F (k)(w(k)) + ∇F (k)(w(k))

(cid:62) (cid:16)

+

β
2

(cid:13)
(cid:13)

(cid:13)w(k+1) − w(k)(cid:13)

2
(cid:13)
(cid:13)

.

(45)

Using the updating rule of w(k+1) and taking the conditional expectation (with respect to data sampling conducted via

mini-batch SGD at the last aggregation instance), we have

(cid:104)

Ek

F (k)(w(k+1))

(cid:105)



≤ F (k)(w(k))−

∇F (k)(w(k))

(cid:62)Ek

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+ ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)



c(k)
mk



+

β
2

Ek






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+ ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

c(k)
mk

2


 .

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Since ∇F C,(k)

y

= −

(cid:18)

(k),e(k)
w
y

y

− w(k)

(cid:19) (cid:46)

ηk, we have

∇F C,(k)
y

=

1
B(k)
y

e(k)
y
(cid:88)

(cid:88)

∇f (w(k),e−1
y

, d),

e=1

d∈B(k),e
y

(46)

(47)

y

where B(k),e
the mini-batch size is ﬁxed during local SGD iterations at each global aggregation round, we deﬁned B(k)
Noting that SGD is unbiased it can be easily shown that

denotes the set of data points selected to form the mini-batch at the e-th local SGD iteration at ES/MD y. Since
|, ∀e.

y = |B(k),e

y

(cid:104)

Ek

∇F C,(k)
y

(cid:105)

=

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

).

(48)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

20

c(k)
mk

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(49)


 .

c(k)
mk

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(50)


 .

Replacing the above result in (46), we get

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤F (k)(w(k)) − ∇F (k)(w(k))

(cid:104)

(cid:62)Ek

ηk

(cid:88)

(cid:88)

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:105)

c(k)
mk

(cid:88)

(cid:88)

+ ηk

y(cid:48)∈N (k)
m

m∈M


+

β
2

Ek




ηk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+ ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

Using the linearity of expectation and inner product, we can simplify the above expression as follows:

(cid:104)

(cid:105)
F (k)(w(k+1))

Ek

≤F (k)(w(k)) − ∇F (k)(w(k))



(cid:62)Ek

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1



∇F (k)
y

(w(k),e−1
y

)



D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

∇F (k)(w(k))

(cid:104)

(cid:62)Ek

−c(k)
mk

(cid:105)

(cid:88)

(cid:88)

+ ηk

y(cid:48)∈N (k)
m

m∈M


+

β
2

Ek




ηk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+ ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

Using Cauchy-Schwartz and Young’s inequalities, we have

a(cid:62)b ≤

α
2

(cid:107)a(cid:107)2+

1
2α

(cid:107)b(cid:107)2, α ∈ R++

(51)

for two real valued vectors a and b, implying a(cid:62)b ≤ (cid:107)a(cid:107)2+ 1

4 (cid:107)b(cid:107)2. Using this inequality in (50) yields

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤F (k)(w(k)) − ∇F (k)(w(k))



(cid:62)Ek

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1



∇F (k)
y

(w(k),e−1
y

)



+

ηk
4

(cid:88)

(cid:88)

y(cid:48)∈N (k)
m

m∈M


D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

+ ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

y∈N (k)
m
D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(52)
Further, for any two real valued vectors a and b, we have: 2a(cid:62)b = (cid:107)a(cid:107)2+(cid:107)b(cid:107)2−(cid:107)a − b(cid:107)2. Using this equality in (52) results

D(k)
y
D(k)e(k)

y(cid:48) e(k)
y(cid:48)
D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

∇F C,(k)
y

y(cid:48)∈N (k)
m

y(cid:48)∈N (k)
m

y∈N (k)
m

D(k)

D(k)

c(k)
mk

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ ηk

m∈M

m∈M

m∈M

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

(cid:88)

Ek

β
2




ηk

+

y


 .

2

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

21

in the following bound:

(cid:104)

(cid:105)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −

ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

Ek







(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

2 
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





+

ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+

Ek

(cid:88)

(cid:88)

+ ηk

y(cid:48)∈N (k)
m

m∈M


βη2
k
2

Ek




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

+

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

c(k)
mk

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)


 .

(53)

Applying the Cauchy-Schwarz inequality (i.e., (cid:107)a + b(cid:107)2≤ 2(cid:107)a(cid:107)2+2(cid:107)b(cid:107)2) on the last term of the above bound yields

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −

ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

Ek







(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2 
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)





+

ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

(cid:88)

(cid:88)

+ ηk

m∈M
y(cid:48)∈N (k)
m

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

Ek

m∈M




+ βη2
k

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

y(cid:48)∈N (k)
m

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

(cid:124)

(cid:123)(cid:122)
(a)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2




(cid:125)



+βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)



2

Ek



(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

.

(54)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

22

Focusing on term (a) in (54), using (47), we upper bound it as follows:






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ek

(cid:88)

(cid:88)

(cid:88)

(cid:88)

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

m∈M

y(cid:48)∈N (k)
m

m∈M



=





=





=



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



Ek

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



Ek

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



Ek



≤ 2



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

y∈N (k)
m

D(k)
y
D(k)e(k)

y

(cid:88)

y∈N (k)
m

D(k)
y
D(k)e(k)

y

(cid:88)

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

e(k)
y
(cid:88)

e=1

e(k)
y
(cid:88)

e=1

(cid:88)

m∈M

(cid:88)

m∈M






y∈N (k)
m
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)










(cid:88)

m∈M

(cid:88)

d∈B(k),e
y


(cid:88)









d∈B(k),e
y

(cid:88)

d∈B(k),e
y


(cid:88)

∇f (w(k),e−1
y
B(k)
y

, d)

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

∇f (w(k),e−1
y
B(k)
y

, d)

∇f (w(k),e−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e−1
y

) + ∇F (k)

y

(w(k),e−1
y

− ∇F (k)

y

(w(k),e−1
y

) + ∇F (k)

y

(w(k),e−1
y




)





)


2




2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)


2



Ek

(cid:124)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)









(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1




d∈B(k),e
y

∇f (w(k),e−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e−1
y

)






2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)




(cid:125)



+ 2



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



Ek

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

(cid:123)(cid:122)
(a)

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2


 .

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

(55)
where the last inequality is obtained via Cauchy-Schwarz inequality. Note that (cid:80)
) in
the above expression corresponds to the noise of SGD at local iteration i at device y, which is zero mean and iid across the
devices and the iterations. Using these facts in term (a) in (55) implies

∇f (w(k),e−1
y
B(k)
y

(w(k),e−1
y

− ∇F (k)

d∈B(k),e
y

,d)

y






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ek

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



=



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)



2



(cid:88)

(cid:88)

m∈M

y∈N (k)
m

(cid:33)2

(cid:32)

D(k)
y
D(k)e(k)

y

Ek

(cid:124)






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(k)
y
(cid:88)

e=1






(cid:88)

d∈B(k),e
y

∇f (w(k),e−1
y
B(k)
y

, d)

(cid:123)(cid:122)
(a)

− ∇F (k)

y

(w(k),e−1
y

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)






2



+



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)



2



Ek






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

2


 .

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Using Lemma 2 (Appendix D) and the fact that SGD noise is zero mean, expanding term (a) in (56) yields




(cid:125)

(56)

(cid:88)

(cid:88)

Ek

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

∇F C,(k)
y

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)


(cid:88)

(cid:88)

≤



m∈M

y(cid:48)∈N (k)
m



+



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m


2




2

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:88)

(cid:88)

m∈M


y∈N (k)
m
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m∈M

(cid:88)



Ek




(cid:32)

D(k)
y
D(k)e(k)

y

(cid:33)2 e(k)
(cid:88)

y

(cid:32)

2

1 −

e=1

(cid:33)

B(k)
y
D(k)
y

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

(cid:88)

y∈N (k)
m

∇F (k)
y

(w(k),e−1
y

(σ(k)
y )2
B(k)
y
2

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)


 .

Θ2
y

(57)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

23

Replacing the above result back in (54), we get

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −

ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

Ek







(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

+

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

−

(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2 
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)





+

ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

(cid:88)

(cid:88)

+ ηk

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek



+ βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



(cid:88)

(cid:88)

m∈M

(cid:32)

D(k)
y
D(k)e(k)

y

(cid:33)2 e(k)
y
(cid:88)

(cid:32)

2

1 −

e=1

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y



+ βη2
k





+ βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



Ek






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2

Ek



(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

.

With rearranging the terms in the above bound, we get

y∈N (k)
m
2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)




(w(k),e−1
y

(58)

(59)

(cid:104)

Ek

F (k)(w(k+1))

(cid:105)

≤ F (k)(w(k)) −





ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)





(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)





βηk








(cid:124)

+ ηk





(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



−

1
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)








(cid:125)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)



+



ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

(cid:123)(cid:122)
(a)

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)





(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)



1 + βηk

(cid:88)

(cid:88)

(cid:88)

(cid:88)

+ ηk

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)



+ βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2



m∈M

y(cid:48)∈N (k)
m
(cid:32)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

D(k)

y∈N (k)
m

 Ek

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

D(k)
y
D(k)e(k)

y

(cid:33)2 e(k)
y
(cid:88)

(cid:32)

2

1 −

e=1

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

Assuming ηk ≤

D(k)

2β (cid:80)

m∈M

(cid:80)

y(cid:48)∈N

(k)
m

D(k)

y(cid:48) e(k)
y(cid:48)

makes term (a) in the above expression negative and also implies 1 +

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

24

βηk

(cid:80)

m∈M

(cid:80)

y(cid:48)∈N (k)
m

D(k)
y(cid:48) e(k)
D(k) ≤ 3
y(cid:48)

2 . Replacing these results in the above bound gives us

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −





ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)





(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)



+



ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)





(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)
(cid:124)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

e(k)
y
(cid:88)

e=1

D(k)
y
D(k)e(k)
(cid:123)(cid:122)
(a)

y

∇F (k)
y

(w(k),e−1
y

)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:125)

+

3
2

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek



+ 2βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

We next aim to bound term (a) in (60) as follows:

(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

(60)

=

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

∇F (k)
y

(w(k)) −

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

2
(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

≤

≤

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

Ek






(cid:13)
(cid:13)
(cid:13)
∇F (k)
(cid:13)
y
(cid:13)
(cid:13)

(w(k)) −

1
e(k)
y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)
(cid:13)

2




(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

(cid:20)(cid:13)
(cid:13)∇F (k)
(cid:13)

y

Ek

(w(k)) − ∇F (k)

y

(w(k),e−1
y

2(cid:21)

(cid:13)
(cid:13)
)
(cid:13)

≤ β2 (cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

,

Ek

(61)

where we used Jensens’s inequality and β-smoothness of the loss functions.

We then bound the last term in the above bound as follows:

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

=η2
k

Ek






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

(cid:88)

e(cid:48)=1

d∈B(k),e(cid:48)

y

∇f (w(k),e(cid:48)−1
y
B(k)
y

, d)

2




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)







(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

(cid:16) (cid:88)

e(cid:48)=1

d∈B(k),e(cid:48)

y

∇f (w(k),e(cid:48)−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e(cid:48)−1

y

) + ∇F (k)

y

(w(k),e(cid:48)−1

y







(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)
)

= η2
k

Ek

≤2η2
k

Ek






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

(cid:88)

d∈B(k),e(cid:48)

y

e(cid:48)=1


∇f (w(k),e(cid:48)−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e(cid:48)−1

y

)

2
(cid:13)
(cid:13)
 + 2η2

(cid:13)
(cid:13)
(cid:13)

k

Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

e(cid:48)=1

∇F (k)
y

(w(k),e(cid:48)−1

y

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)

2



=2η2
k

e−1
(cid:88)

e(cid:48)=1

Ek




(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

d∈B(k),e(cid:48)

y

∇f (w(k),e(cid:48)−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e(cid:48)−1

y

2
(cid:13)
(cid:13)
 + 2η2

(cid:13)
)
(cid:13)
(cid:13)

k

Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

e(cid:48)=1

∇F (k)
y

(w(k),e(cid:48)−1

y

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)

2

(cid:32)

≤ 4(e − 1)η2
k

1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y + 2η2
k

Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

e(cid:48)=1

∇F (k)
y

(w(k),e(cid:48)−1

y

(cid:124)

(cid:123)(cid:122)
(a)

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)

2

,


(cid:125)



(62)

where we used Cauchy–Schwarz inequality, the fact that noise of SGD is zero mean, and Lemma 2. We then bound term (a)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

25

in (62) as follows:

2η2
k

Ek





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e−1
(cid:88)

e(cid:48)=1

∇F (k)
y

(w(k),e(cid:48)−1

y

(cid:13)
(cid:13)
(cid:13)
)
(cid:13)
(cid:13)

2
 ≤ 2η2

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

Ek

(w(k),e(cid:48)−1

y

) − ∇F (k)

y

(w(k)) + ∇F (k)

y

≤4η2

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

Ek

≤ 4η2

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

Ek







(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)


(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)


(w(k),e(cid:48)−1

y

) − ∇F (k)

y

(w(k))

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(w(k))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(w(k),e(cid:48)−1

y

) − ∇F (k)

y

2
 + 4η2

(cid:13)
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

≤ 4η2

kβ2(e − 1)

e−1
(cid:88)

e(cid:48)=1

Ek



(cid:13)
(cid:13)
w(k),e(cid:48)−1
(cid:13)
(cid:13)
(cid:13)

y

− w(k)

2
 + 4η2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

,





(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)
2
 + 4η2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)



(63)

where we used Cauchy-Schwarz inequality repeatedly and applied the β-smoothness of the loss function. Replacing the above
expression in (62) yields

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

(cid:32)

≤4(e − 1)η2
k

1 −

(cid:33)

B(k)
y
D(k)
y

+ 4η2

kβ2(e − 1)

e−1
(cid:88)

e(cid:48)=1

Ek

which implies:

Θ2
y

(σ(k)
y )2
B(k)
y
(cid:13)
(cid:13)
w(k),e(cid:48)−1
(cid:13)
(cid:13)
(cid:13)

y





− w(k)

2
 + 4η2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

k(e − 1)

e−1
(cid:88)

e(cid:48)=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

,

(64)

e(k)
y
(cid:88)

e=1

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

≤ 4η2
k

(cid:32)

(e − 1)

1 −

e(k)
y
(cid:88)

e=1

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y

+ 4η2

kβ2

e(k)
y
(cid:88)

(e − 1)

e=1

e−1
(cid:88)

e(cid:48)=1

Ek





(cid:13)
(cid:13)
w(k),e(cid:48)−1
(cid:13)
(cid:13)
(cid:13)

y

− w(k)

2
 + 4η2
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(k)
y
(cid:88)

(e − 1)

e=1

e−1
(cid:88)

e(cid:48)=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

≤ 4η2
k

(cid:16)

e(k)
y

(cid:17) (cid:16)

e(k)
y − 1

(cid:17)

+ 4η2

kβ2 (cid:16)

e(k)
y

(cid:17) (cid:16)

e(k)
y − 1

(cid:17)

(cid:33)

(σ(k)
y )2
B(k)
y

Θ2
y

(cid:32)

B(k)
y
D(k)
y


Ek



1 −

e(k)
y
(cid:88)

e=1

(cid:13)
(cid:13)
w(k),e−1
(cid:13)
(cid:13)
(cid:13)

y

− w(k)

2
 + 4η2
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:16)

e(k)
y

(cid:17) (cid:16)

e(k)
y − 1

(cid:17)

e(k)
y
(cid:88)

e=1

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(w(k))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

(65)

Assuming ηk ≤

2β

(cid:18)

(cid:113)

y (e(k)
e(k)

y − 1)

(cid:19)−1

, ∀n, we get

e(k)
y
(cid:88)

e=1

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

≤

Ek

(cid:16)

e(k)
y

4η2
k

(cid:17) (cid:16)

e(k)
y − 1

(cid:17) (cid:18)

1 −

B(k)
y
D(k)
y

(cid:19) (σ(k)
y )2
B(k)
y

Θ2
y

1 − 4η2

kβ2e(k)

y

(cid:16)

(cid:17)

e(k)
y − 1
(cid:17)

(cid:16)

e(k)
y

4η2
k

(cid:17)2 (cid:16)

1 − 4η2

kβ2e(k)

y

+

e(k)
y − 1
(cid:16)

e(k)
y − 1

(cid:17)

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)

.

(66)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

26

Replacing the above expression in (61), we get:

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

∇F (k)
y

(w(k),e−1
y

)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ β2 (cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

e(k)
y
(cid:88)

e=1

(cid:20)(cid:13)
(cid:13)w(k) − w(k),e−1
(cid:13)

y

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek

(cid:13)
(cid:13)
(cid:13)
∇F (k)(w(k)) −
(cid:13)
(cid:13)
(cid:13)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m


≤ β2 (cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y





(cid:17) (cid:16)

(cid:16)

e(k)
y

e(k)
y − 1

4η2
k

(cid:17) (cid:18)

1 −

B(k)
y
D(k)
y

(cid:19) (σ(k)
y )2
B(k)
y

Θ2
y

1 − 4η2

kβ2e(k)

y

(cid:16)

(cid:17)

e(k)
y − 1

(cid:16)

e(k)
y

4η2
k

(cid:17)2 (cid:16)

1 − 4η2

kβ2e(k)

y

(cid:17)

e(k)
y − 1
(cid:16)

e(k)
y − 1

(cid:17)

(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

(cid:13)
2
(cid:13)
(w(k))
(cid:13)
(cid:13)
(cid:13)







4β2η2

ke(k)

y

(cid:16)

(cid:17) (cid:18)

e(k)
y − 1

1 −

B(k)
y
D(k)
y

(cid:19) (σ(k)
y )2
B(k)
y

Θ2
y

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)e(k)

y

1 − 4η2
(cid:16)

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

4β2η2

y

ke(k)
kβ2e(k)

y

e(k)
y − 1
(cid:16)
e(k)
y − 1

1 − 4η2

4β2η2

ke(k)

y

(cid:16)

e(k)
y − 1

1 − 4η2

(cid:88)

(cid:88)

(cid:17)

m∈M

y∈N (k)
m
(cid:17) (cid:18)

(cid:16)

4β2η2
k

e(k)
y − 1

1 − 4η2

kβ2e(k)

y

(cid:88)

(cid:88)

m∈M

y∈N (k)
m
ke(k)
kβ2e(k)

max

max

4β2η2

1 − 4η2

D(k)
y
D(k)e(k)
(cid:16)

y

(cid:17)

e(k)
max − 1
(cid:16)

e(k)
max − 1

(cid:88)

(cid:88)

D(k)
y
D(k)

m∈M

(cid:16)

y∈N (k)
m
ke(k)
kβ2e(k)

max

max

4β2η2

1 − 4η2

(cid:17)

e(k)
max − 1
(cid:16)

e(k)
max − 1

(cid:16)
e(k)
y − 1

(cid:17)

y

kβ2e(k)
(cid:17)
(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)
(cid:17) (cid:18)

(cid:17)

(w(k))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:19) (σ(k)
y )2
B(k)
y

Θ2
y

1 −

B(k)
y
D(k)
y

(cid:16)
e(k)
y − 1

(cid:17)

y

kβ2e(k)
(cid:13)
(cid:13)
∇F (k)
(cid:13)
(cid:13)
y
(cid:13)

D(k)
y
D(k)

(w(k))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1 −

(cid:19) (σ(k)
y )2
B(k)
y
(cid:17)

B(k)
y
D(k)
y
(cid:16)
e(k)
y − 1

Θ2
y

(cid:18)

ζ1

(cid:17)

(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

(cid:19)

+ ζ2

,

+

=

+

≤

+

≤

+

where we used e(k)
expression in (60) gives us

max = maxn∈N {e(k)

(67)
y }, and the bounded dissimilarity of local gradients (Assumption 3). Replacing the above

(cid:104)

(cid:105)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −





ηk
4

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)





(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)



+



ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)











(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

(cid:16)

e(k)
y − 1

(cid:17) (cid:18)

1 −

4β2η2
k

1 − 4η2

kβ2e(k)

y

(cid:16)

(cid:19) (σ(k)
y )2
B(k)
y
(cid:17)

B(k)
y
D(k)
y
e(k)
y − 1

Θ2
y

4β2η2

+

1 − 4η2

(cid:16)

max

ke(k)
kβ2e(k)

max

(cid:17)

(cid:18)

e(k)
max − 1
(cid:16)

e(k)
max − 1

(cid:17)

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

ζ1

+ ζ2

(cid:19)







+

3
2

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek



+ 2βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y,

(68)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

27

which implies

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) +

ηk
2

(cid:88)

n∈N

D(k)

y e(k)
y
D(k)





+

ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

e(k)
max

4η2

kβ2 (cid:16)
1 − 4η2

max

kβ2e(k)
(cid:17) (cid:18)

1 −

(cid:17) (cid:16)

(cid:17)

e(k)
max − 1
(cid:16)

e(k)
max − 1
(cid:19) (σ(k)
y )2
B(k)
y
(cid:17)

B(k)
y
D(k)
y
e(k)
max − 1

1 − 4η2

kβ2e(k)

max

(cid:16)

(cid:16)

4β2η2
k

e(k)
y − 1





1
2

(cid:13)
(cid:13)
2
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

(cid:17) ζ1 −

Θ2
y

+

4ζ2η2

kβ2 (cid:16)
1 − 4η2

(cid:17) (cid:16)

e(k)
max

(cid:17)

e(k)
max − 1
(cid:17)

e(k)
max − 1

(cid:16)

kβ2e(k)

max

+

3
2

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek



+ 2βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

B(k)
y
D(k)
y







(cid:33)

(σ(k)
y )2
B(k)
y

Θ2
y.

(69)

Assuming

4η2

1−4η2

kβ2(e(k)
max)(e(k)
max−1)
(cid:16)
kβ2e(k)
e(k)
max−1
max
(cid:114)

size choice of ηk ≤ 1
2β

(cid:16)

(4ζ1+1)

1
e(k)
max

(cid:16)

e(k)
max−1

(cid:17)(cid:17) , we get

(cid:17) ζ1 ≤ 1

4 , implying

1
kβ2e(k)

max

1−4η2

(cid:16)

e(k)
max−1

(cid:17) ≤ 1+4ζ1

ζ1

≤ 5, which can be obtained under the step

(cid:105)
(cid:104)
F (k)(w(k+1))

Ek

≤ F (k)(w(k)) −

1
8

ηk

(cid:13)
(cid:13)
2 (cid:88)
(cid:13)∇F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

n∈N

D(k)

y e(k)
y
D(k)

+

ηk
2

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
D(k)

20β2η2
k

(cid:16)

e(k)
y − 1

(cid:17)

(cid:32)

1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y + 20ζ2η2

kβ2 (cid:16)

e(k)
max

(cid:17) (cid:16)

e(k)
max − 1

(cid:17)







+

3
2

ηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

Ek



+ 2βη2
k



(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)


2







(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

(70)

Rearranging the terms of the above bound gives the following bound on the norm of gradient:

(cid:13)
(cid:13)∇F (k)(w(k))
(cid:13)

(cid:13)
2
(cid:13)
(cid:13)

≤

ηk

F (k)(w(k)) − Ek
(cid:80)
(cid:80)

m∈M

(cid:88)

(cid:88)

+

m∈M

y∈N (k)
m

D(k)
y
D(k)

80β2η2
k

(cid:16)

(cid:17)

e(k)
y − 1

(cid:2)F (k)(w(k+1))(cid:3)
y(cid:48) e(k)
D(k)
y(cid:48) /(8D(k))
(cid:33)
(σ(k)
B(k)
y )2
y
B(k)
D(k)
y
y

Θ2

y(cid:48)∈N (k)
m

(cid:32)

1 −

y + 80ζ2η2

kβ2 (cid:16)

e(k)
max

(cid:17) (cid:16)

e(k)
max − 1

(cid:17)

+ 24Ek

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+ 16βηk





(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)









(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

(71)

Note that F (k)(w(k)) in the ﬁrst term on the right hand side of the above bound is in fact the loss under which the global
aggregation started from, i.e., F (k)(w(k)|, D(k)) and F (k)(w(k+1)) is the loss under which the global aggregation concludes,
i.e., F (k)(w(k+1)|D(k)). We next ﬁnd the connection between F (k)(w(k)|D(k)) and the actual loss under which iteration k − 1
has been concluded, i.e., F (k−1)(w(k)|D(k−1)). Using the deﬁnition of model/concept drift in Deﬁnition 3, since the global
loss is deﬁned as the loss per data point we have:

F (k)(w(k)|D(k)) =

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

(cid:88)

(cid:88)

m∈M
(cid:88)

y∈N (k)
m
(cid:88)

m∈M

y∈N (k)
m

≤

=

(cid:34)

D(k)
y
D(k)

F (k)
y

(w(k)|D(k)

y ) −

D(k−1)
y
D(k−1)

F (k−1)

y

(w(k)|D(k−1)

y

) +

D(k−1)
y
D(k−1)

F (k−1)

y

(w(k)|D(k−1)

y

)

(cid:35)

∆(k)

y +

(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k−1)
y
D(k−1)

F (k−1)

y

(w(k)|D(k−1)

y

)

∆(k)

y + F (k−1)(w(k)|D(k−1)) = ∆(k) + F (k−1)(w(k)), k ≥ 1.

(72)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

28

Taking total expectation from both hand sides of (71) and taking the summation over global aggregation index implies

1
K

K−1
(cid:88)

k=0

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)
(cid:104)

E

≤

1
K

F (0)(w(0)) − F (0)(w(1))

ηk

(cid:80)

m∈M

(cid:80)

y(cid:48)∈N (k)
m

∆(k)

D(k)

y(cid:48) e(k)

y(cid:48) /(8D(k))

+

1
K

K−1
(cid:88)

k=1

F (k−1)(w(k)) − F (k)(w(k+1))

(cid:80)

ηk

m∈M

(cid:80)

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)

y(cid:48) /(8D(k))

K−1
(cid:88)

+

1
K

k=1

ηk

(cid:80)

m∈M

(cid:80)

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)

+

1
K

K−1
(cid:88)

(cid:88)

(cid:88)

k=0

m∈M

y∈N (k)
m

D(k)
y
D(k)

(cid:16)

80β2η2
k

y(cid:48) /(8D(k))
(cid:32)

(cid:17)

e(k)
y − 1

1 −

B(k)
y
D(k)
y

(cid:33)

(σ(k)
y )2
B(k)
y

Θ2

y +

1
K

K−1
(cid:88)

k=0

80ζ2η2

kβ2 (cid:16)

e(k)
max

(cid:17) (cid:16)

e(k)
max − 1

(cid:17)

+

1
K

K−1
(cid:88)

k=0

24Ek

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)

mk

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+

1
K

K−1
(cid:88)

k=0





16βηk

(cid:88)

(cid:88)

m∈M

y(cid:48)∈N (k)
m

D(k)

y(cid:48) e(k)
y(cid:48)
D(k)









(cid:88)

(cid:88)

m∈M

y∈N (k)
m

D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y.(73)

Assuming the choice of step size ηk =

(cid:113)

α
Ke(k)
avg

devices, we get:

, where e(k)

avg denotes the average number of local iterations across the participating

1
K

K−1
(cid:88)

k=0

(cid:104)

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)

E

≤

+

80β2α2
K 2emin
avg

K−1
(cid:88)

(cid:88)

k=0

m∈M

(cid:88)

y∈N (k)
m

8(cid:112)emax
avg
√
αˆemin
K
avg
D(k)
y
D(k)

(cid:16)

(cid:16)

F (0)(w(0)) − F (k)(cid:63) (cid:17)

+

8(cid:112)emax
avg
√
αˆemin
K
avg

(cid:17)

e(k)
y − 1

(cid:32)

1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y +

K−1
(cid:88)

∆(k)

k=1
80β2α2
K 2emin
avg

K−1
(cid:88)

k=0

(cid:16)

ζ2

e(k)
max

(cid:17) (cid:16)

(cid:17)

e(k)
max − 1

+

1
K

K−1
(cid:88)

k=0

24 Ek

(cid:124)

mk

(cid:20)(cid:13)
(cid:13)c(k)
(cid:13)
(cid:123)(cid:122)
(a)

2(cid:21)

(cid:13)
(cid:13)
(cid:13)

+

(cid:125)

16βαˆemax
avg
√
(cid:113)

K

K

emin
avg

K−1
(cid:88)

(cid:88)

(cid:88)

k=0

m∈M

y∈N (k)
m





D(k)
y
(cid:113)

D(k)

e(k)
y



2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

(74)

We next aim to bound term (a) in (74). Let (cid:103)∇F

gradient and the gradient of its associated users, and (cid:100)∇F
communications concludes.

(k)
m denote the initial local aggregated gradient at ES m, containing its own
(k)
m denote the gradient at ES m after the consensus through P2P

As explained in (42), we have

∇F L,(k)

m =

(cid:88)

m∈M

∇F A,(k)

m + e(k)
m .

On the other hand, the evolution of the local gradient during the consensus process can be described as

∇F L,(k) =

(cid:16)

Λ(k)(cid:17)ϕ(k)

∇F A,(k),

(75)

(76)

where ∇F L,(k) ∈ Rd×d denotes a matrix,3 with its row m containing the ﬁnal vector of gradient at ES m (i.e., ∇F L,(k)
and ∇F A,(k) ∈ Rd×d denotes a matrix, with its row m containing the initial gradient vector at ES m (i.e., ∇F A,(k)
Λ(k) = [λ(k)
aggregation round k.

m ),
m ). In (76),
i,j ]i,j∈M is the consensus matrix across the ES network and ϕ(k) denotes the rounds of P2P communications at the

Let matrix ∇F P,(k) denote the matrix of true/perfect average of the gradient vectors across the ESs, which is given by

∇F P,(k) =

1M 1(cid:62)

M ∇F A,(k)

m

M

.

(77)

Also, let ∇F P,(k)
1(cid:62) (cid:16)

∇F L,(k) − ∇F A,(k)(cid:17)

m denote the m-th row of ∇F P,(k). Since the consensus error is zero mean [30] across the ESs, we have

= 0 implying that (11(cid:62))

(cid:16)

∇F L,(k) − ∇F A,(k)(cid:17)

= 0. Thus, we get

∇F L,(k) − ∇F P,(k) =

=

=

(cid:19) (cid:16)

(cid:18)

(cid:18)

I −

I −

11(cid:62)
M
11(cid:62)
M
Λ(k)(cid:17)ϕ(k)

(cid:18)(cid:16)

−

(cid:19) (cid:18)(cid:16)

Λ(k)(cid:17)ϕ(k)
(cid:19) (cid:16)

11(cid:62)
M

3d denotes the size/length of the gradient vector.

∇F A,(k) − ∇F P,(k)(cid:17)

,

∇F L,(k) − ∇F P,(k)(cid:17)

=

(cid:18)

I −

11(cid:62)
M
Λ(k)(cid:17)ϕ(k)

(cid:19) (cid:18)(cid:16)

Λ(k)(cid:17)ϕ(k)
(cid:19)

∇F A,(k) − ∇F P,(k)

(cid:19)

(78)

∇F P,(k)

∇F A,(k) −

(cid:16)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

29

where the above derivations are obtained based on the following properties: (i) (cid:0)Λ(k)(cid:1)ϕ(k)
11(cid:62)
M

(cid:0)Λ(k)(cid:1)ϕ(k)

= 11(cid:62)

M . We ﬁnally get
(cid:32)

(cid:107)c(k)

m (cid:107)2 ≤ trace

(cid:16)

∇F A,(k) − ∇F P,(k)(cid:17)(cid:62) (cid:18)(cid:16)

Λ(k)(cid:17)ϕ(k)

−

(cid:19)2

11(cid:62)
M

(cid:16)

∇F A,(k) − ∇F P,(k)(cid:17)

(cid:33)

∇F P,(k) = ∇F P,(k), and (ii)

(cid:32)

(cid:16)

= trace

∇F A,(k) − ∇F P,(k)(cid:17)(cid:62) (cid:18)(cid:16)

Λ(k)(cid:17)ϕ(k) (cid:20)

I −

(cid:21)(cid:19)2

11(cid:62)
M

(cid:16)

∇F A,(k) − ∇F P,(k)(cid:17)

(cid:33)



(cid:16)




= trace

∇F A,(k) − ∇F P,(k)(cid:17)(cid:62)





(cid:16)

Λ(k)(cid:17)ϕ(k) (cid:20)

I −

2

(cid:21)ϕ(k)


(cid:16)

11(cid:62)
M



∇F A,(k) − ∇F P,(k)(cid:17)




(79)

≤ (λ(k))2ϕ(k) (cid:88)

(cid:107)∇F A,(k)

m(cid:48) − ∇F P,(k)

m(cid:48)

(cid:107)2

≤

1
M

m(cid:48)∈M

(λ(k))2ϕ(k) (cid:88)

m(cid:48),m(cid:48)(cid:48)∈M

(cid:107)∇F A,(k)

m(cid:48) − ∇F A,(k)

m(cid:48)(cid:48) (cid:107)2

≤ M (λ(k))2ϕ(k)

max
m(cid:48),m(cid:48)(cid:48)∈M

(cid:107)∇F A,(k)

m(cid:48) − ∇F A,(k)

m(cid:48)(cid:48) (cid:107)2 ≤ M (λ(k))2ϕ(k) (cid:16)

Ξ(k)(cid:17)2

.

Replacing the above result back in (74), we get the ﬁnal result

1
K

K−1
(cid:88)

k=0

(cid:104)

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)

E

≤

+

80β2α2
K 2emin
avg

K−1
(cid:88)

(cid:88)

k=0

m∈M

(cid:88)

y∈N (k)
m

8(cid:112)emax
avg
√
αˆemin
K
avg
D(k)
y
D(k)

(cid:16)

(cid:16)

F (0)(w(0)) − F (k)(cid:63) (cid:17)

+

8(cid:112)emax
avg
√
αˆemin
K
avg

(cid:17)

e(k)
y − 1

(cid:32)

1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y +

K−1
(cid:88)

∆(k)

k=1
80β2α2
K 2emin
avg

+

1
K

K−1
(cid:88)

k=0

24M (λ(k))2ϕ(k) (cid:16)

Ξ(k)(cid:17)2

+

16βαˆemax
avg
√
(cid:113)

K

K

emin
avg

K−1
(cid:88)

(cid:88)

(cid:88)

k=0

m∈M

y∈N (k)
m





D(k)
y
(cid:113)

D(k)

e(k)
y

K−1
(cid:88)

(cid:16)

ζ2

e(k)
max

(cid:17) (cid:16)

(cid:17)

e(k)
max − 1

k=0


2 (cid:32)



1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2
y.

(80)

Assuming ∆(k) ≤ Υ

K and considering the assumption

y ≤ ϑ simpliﬁes the bound in (13) as follows:

APPENDIX B
PROOF OF COROLLARY 1
B(k)
y
D(k)
y

(cid:19) (σ(k)
y )2
B(k)
y

1 −

(cid:18)

Θ2

1
K

K−1
(cid:88)

k=0

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)
(cid:104)

E

≤

F (0)(w(0)) − F (k)(cid:63) (cid:17)
(cid:16)

+

8Υ(cid:112)emax
avg
√
αˆemin
K
avg

+

K (emax − 1) ϑ +

24M (λ(k))2ϕ(k) (cid:16)

Ξ(k)(cid:17)2

+

80β2α2
K 2emin
avg
K−1
(cid:88)

k=0

+

1
K

Kζ2 (emax) (emax − 1)

16βαˆemax
avg
√
(cid:113)

K

K

emin
avg

Kϑ.

8(cid:112)emax
avg
√
αˆemin
K
avg
80β2α2
K 2emin
avg

Assuming ϕ(k) ≥ 1
2

(cid:18)

(cid:20)
logλ(k)

√

ξ

K(Ξ(k))2

M (k)

(cid:19)(cid:21)+

implies that (note that λ(k) < 1)

(cid:32)

2ϕ(k) ≥ logλ(k)

√

ξ

(cid:33)

K (cid:0)Ξ(k)(cid:1)2

M (k)

⇒ (λ(k))2ϕ(k)

≤

√

ξ

⇒ M (k)(λ(k))2ϕ(k) (cid:16)

K (cid:0)Ξ(k)(cid:1)2
Ξ(k)(cid:17)2

M (k)
√

≤ ξ/

K.

Replacing this result back in (81) yields

1
K

K−1
(cid:88)

k=0

(cid:104)

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)

E

≤

8(cid:112)emax
avg
√
αˆemin
K
avg

(cid:16)

F (0)(w(0)) − F (k)(cid:63) (cid:17)

+

8Υ(cid:112)emax
avg
√
αˆemin
K
avg

(81)

(82)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

30

+

80β2α2
Kemin
avg

(emax − 1) ϑ +

80β2α2
Kemin
avg

ζ2 (emax) (emax − 1) + 24

ξ
√
K

+

16βαˆemax
avg
√
(cid:113)

K

emin
avg

ϑ,

and thus

1
K

K−1
(cid:88)

k=0

(cid:104)

(cid:107)∇F (k)(w(k))(cid:107)2(cid:105)

E

= O

+ O

(cid:18) β2α2
Kemin
avg

ζ2 (emax) (emax − 1)

(cid:16)

√

1/

(cid:17)

,

K

= O

which concludes the proof.

(cid:32) (cid:112)emax
avg
√
αˆemin
K
avg
(cid:18) ξ
√
K

+ O

(cid:19)

(cid:33)

+ O

(cid:33)

(cid:32) Υ(cid:112)emax
avg
√
αˆemin
K
avg

+ O

(cid:18) β2α2
Kemin
avg

(cid:19)

(emax − 1) ϑ

(cid:19)

+ O





βαˆemax
avg
(cid:113)

√

K

emin
avg



ϑ



(83)

APPENDIX C
CHARACTERIZATION OF THE GRADIENT OF THE CRITIC’S LOSS

Inspired by [44], our proof for the Lipschitz continuity property of the MSPBE loss function’s gradient is presented as

below. First, the loss function in (39) can be re-written as

Eθ(ω) =

1
2

||Vω − Λ(BθVω)||2=

1
2

||Λ(Vω − BθVω)||2, (since Vω = ΛVω),

which is upper-bounded by its expectation value given by the loss across the entire system state vector H:

Eθ(ω) ≤

1
2

||Λ(Vω − BθVω)||2

H ,

(84)

(85)

where H is the diagonal matrix whose elements are within the stationary system state distribution dπθ(s), ∀s ∈ S as deﬁned
in section IV-D. Under this stationary condition, (85) can be further analyzed as:

Eθ(ω) =

1
2

[Λ(Vω − BθVω)](cid:62) H [Λ(Vω − BθVω)]

=

=

1
2
1
2

(Vω − BθVω)(cid:62)Λ(cid:62)HΛ(Vω − BθVω)

√

||¯Λ

H(Vω − BθVω)||2
2,

√

where ¯Λ =
of the loss function is given by

Hφ(cid:62)(φHφ(cid:62))−1φ

√

H is an orthogonal projector of a column element of

H(ω) = φ(I − γP )(cid:62)

√

√

H ¯Λ

H(I − γP )φ(cid:62),

(86)

(87)

(88)

√

Hφ(cid:62). Then, the Hessian product

(89)

where I ∈ R|S|×|S| is the identity matrix, and P is the transition probability as deﬁned in Section IV-A. Since Eθ(ω) is
convex, its gradient is (cid:96)-Lipschitz if

holds. Indeed, we have

ω(cid:62)H(ω)ω ≤ l||ω||2
2

√

√

H ¯Λ

H(I − γP )φ(cid:62)ω

ω(cid:62)H(ω)ω = ω(cid:62)φ(I − γP )(cid:62)
= ||¯Λ
√
= ||

√

H(I − γP )φ(cid:62)ω||2
2

√

√

2 − 2γω(cid:62)φHP φ(cid:62)ω + γ2||¯Λ
√
2 − 2γω(cid:62)φHP φ(cid:62)ω + γ2||

Hφω||2
Hφω||2
H(I − γP )φ(cid:62)ω||2
2.

≤ ||

= ||

√

HP φ(cid:62)ω||2
2

HP φ(cid:62)ω||2
2

Using the Cauchy-Schwartz inequality, we have

ω(cid:62)H(ω)ω ≤

≤

(cid:16)

(cid:16)

√

||

√

||

Hφ(cid:62)ω||2 + γ||

√

(cid:17)2

HP φ(cid:62)ω||2
(cid:17)2

Hφ(cid:62)ω||2

√

Hφ(cid:62)ω||2
√

2 + γ||

= (1 + γ)2||

Hφ(cid:62)ω||2
2.

(90)

(91)

(92)

ACCEPTED TO IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS

Here, we have

√

||

Hφ(cid:62)ω||2

2 = ||φ(cid:62)ω||2

P =

(cid:88)

di (cid:16)

(cid:62)

(φi)

ω

(cid:17)2

(cid:88)

≤

i∈|S|

di||ωi||2

2||θi||2

2 ≤

i∈|S|

(cid:16)

max
i

||φi||2
2

(cid:17)

||θi||2
2.

Thus, we get

ω(cid:62)H(ω)ω ≤ (1 + γ)2 (cid:16)

max
i

||φi||2
2

(cid:17)

||θi||2
2,

which leads to 90, and completes our proof.

31

(93)

(94)

APPENDIX D
MINI-BATCH SGD NOISE CHARACTERIZATION
Lemma 2. During each aggregation round t, for each MD/ES y ∈ N ∪ M, the variance of stochastic gradient during
mini-batch gradient descent iteration k, i.e., (cid:101)∇F (t)
y (x) is upper-bounded by

E






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

d∈B(k),e
y

∇f (w(k),e−1
y
B(k)
y

, d)

− ∇F (k)

y

(w(k),e−1
y

)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:32)


 ≤ 2

1 −

(cid:33)

B(k)
y
D(k)
y

(σ(k)
y )2
B(k)
y

Θ2

y, ∀e,

(95)

where σ(k)

y

denotes the sampled variance of data at the respective node.

Proof. The proof can be easily carried out using the result on the variance of of simple random sampling in [45] and our
deﬁnition of the data variability in Deﬁnition 2, and thus omitted for brevity.

