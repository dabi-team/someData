JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

1

AUDITEM: Toward an Automated and Efﬁcient
Data Integrity Veriﬁcation Model Using Blockchain
Zeshun Shi∗, Jeroen Bergers∗, Ken Korsmit†, and Zhiming Zhao∗
∗Informatics Institute, University of Amsterdam, Amsterdam, 1098 XH, the Netherlands
†Spatial Eye B.V., Culemborg, 4105 JH, the Netherlands
Email: z.shi2@uva.nl, jeroen.bergers@hetnet.nl, ken.korsmit@spatial-eye.com, z.zhao@uva.nl

2
2
0
2

l
u
J

1

]

R
C
.
s
c
[

1
v
0
7
3
0
0
.
7
0
2
2
:
v
i
X
r
a

Abstract—Data tampering is often considered a severe problem
in industrial applications as it can lead to inaccurate ﬁnancial re-
ports or even a corporate security crisis. A correct representation
of data is essential for companies’ core business processes and
is demanded by investors and customers. Traditional data audits
are performed through third-party auditing services; however,
these services are expensive and can be untrustworthy in some
cases. Blockchain and smart contracts provide a decentralized
mechanism to achieve secure and trustworthy data integrity
veriﬁcation; however, existing solutions present challenges in
terms of scalability, privacy protection, and compliance with
data regulations. In this paper, we propose the AUtomated
and Decentralized InTegrity vEriﬁcation Model (AUDITEM)
to assist business stakeholders in verifying data integrity in a
trustworthy and automated manner. To address the challenges in
existing integrity veriﬁcation processes, our model uses carefully
designed smart contracts and a distributed ﬁle system to store
integrity veriﬁcation attributes and uses blockchain to enhance
the authenticity of data certiﬁcates. A sub-module called Data
Integrity Veriﬁcation Tool (DIVT) is also developed to support
easy-to-use interfaces and customizable veriﬁcation operations.
This paper presents a detailed implementation and designs
experiments to verify the proposed model. The experimental and
analytical results demonstrate that our model is feasible and
efﬁcient to meet various business requirements for data integrity
veriﬁcation.

Index Terms—Data management, Integrity veriﬁcation, Decen-

tralization, Blockchain

I. INTRODUCTION

Information and data are the most valuable assets of indus-
trial companies and need to be handled properly to maximize
their business potential [1]. With the rapid development of
Internet technology, more and more data is being produced
every day. Accordingly, various data storage and management
tools have emerged to support different business operations,
including databases, data warehouses, and data lakes. These
data management solutions have different architectures and
application scenarios. For example, traditional databases are
suitable for basic, day-to-day transaction processing. In con-
trast, data warehouses help decision-makers with real-time
decision support by online analytical processing through a
unique data storage architecture. Data lakes, on the other hand,
can store data in any format and analyze it, often through
machine learning or data mining techniques to achieve greater
value from the data [2]. In business operation practices, the
joint utilization of these tools can provide enterprises with
more options and deliver more beneﬁts. However, despite the

advantages of using these data management tools, data stored
in the source repository can be edited by internal or external
people in the system. This leads to the possibility of people
tampering with data to cause huge business losses.

Data integrity ensures the correctness and consistency of
data throughout its life cycle. It, therefore, plays a vital role
in the design, implementation, and utilization of any data
management system [3]. The current solution for data integrity
veriﬁcation is through third-party auditing service providers
such as Spectra [4]. However, these centralized services are
not immune to malicious auditors and therefore suffer from
a single point of failure (SPOF) [5]. Besides, the expensive
commission fees also discourage companies from using these
services due to increased operating costs. According to a
report from Audit Analytics, a typical company with revenues
of C25 billion would expect audit fees of around C12.3
million [6]. Recently, blockchain and smart contracts have
brought promising hints to address the challenges in the data
integrity veriﬁcation process. In general, a blockchain is a
decentralized ledger technology with a “chained blocks” data
structure. Blocks are linked to each other using a cryptographic
algorithm, and every block header includes the root of a
Merkle tree, a timestamp, and a hash of the previous block
[7]. When the data in any block is changed, the hash value
is changed accordingly, resulting in rejection by the network.
Consequently, once data is committed to the blockchain, it is
immutable, transparent, and decentralized. Combining these
properties with a data veriﬁcation model can achieve the
following features:

• Immutability of the veriﬁcation certiﬁcates. Any data
immutable. Therefore,
stored in the blockchain is
blockchain can create a tamper-proof environment and
enhance the trustworthiness of the data veriﬁcation result
[8].

• Decentralization of the veriﬁcation process. There is no
need to use third-party auditing services, which avoids
cheating auditors and saves a lot of commission fees and
operation costs.

• Automation of the veriﬁcation process. Smart contracts
can be used as self-executing and automated validation
control systems, allowing different stakeholders to easily
verify data integrity without tedious manual operations.

Correct implementation of blockchain and data management
tools can reliably verify data integrity. This can replace the

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

2

current expensive and untrustworthy third-party audit process
and save time and costs [9], [10]. The potential of using
blockchain as data management and veriﬁcation systems has
been investigated in recent studies [11], [12], [13], [14] This
paper builds on top of the previous research and leads to cre-
ating the AUtomated and Decentralized InTegrity vEriﬁcation
Model (AUDITEM). Unlike existing models, AUDITEM is
designed to handle large-scale data stored in classical data
repositories while providing integrity veriﬁcation capabilities.
AUDITEM prevents malicious behavior not only from third
parties but also from users within the enterprise organization.

A. Motivating Scenario

One of the possible use cases for AUDITEM is the utility
and telecommunication company. This kind of company usu-
ally has numerous ﬁxed assets in the form of utility lines and
pipes that must be properly stored and managed [15]. A widely
used solution is the spatial data warehouse, as it provides
various functions to track data changes and obtain daily
asset information. For example, one of these functions is the
ﬁnancial reporting of assets, which requires that the integrity
of the data is not compromised. Unfortunately, this is not
always the case; there are many examples of integrity issues
in reporting data assets. The two most famous incidents are
the WorldCom and Enron scandals, both leading to millions
of debt because of poor ﬁnancial reporting [9]. These events
led to the introduction of the Sarbanes-Oxley Act (SOX), a
U.S. law to protect investors. It proposes that public compa-
nies must have a strong external audit of their bookkeeping.
Currently, utility companies usually store sensitive asset data
the
on their private servers. This does not guarantee that
data is not tampered with, requiring comprehensive auditing.
However, the auditing task is expensive, and the auditor can be
biased. The proposed solution uses blockchain as a trustworthy
auditing device. A combination of the data warehouse with the
blockchain could lead to the required prevention of data fraud
expected by the government and investors [9].

B. Problem Statement

the
The main objective of AUDITEM is to prove that
data in the data repository is veriﬁable and authentic. This
is necessary for companies and researchers where data assets
are crucial for their core business. One of the current solutions
is to use a third-party audit, but this is expensive and cannot
be fully trusted. Blockchain can help as it is decentralized
and tamper-resistant. However, it also faces challenges such
as traditional blockchain technology cannot provide sufﬁcient
transaction throughput. Besides, the system must prevent data
leakage to outside parties as the data can contain private
company information. Finally, we must ﬁnd a solution when
information is stored in the blockchain. This is
personal
because personal data regulations, e.g., the right to be forgot-
ten in General Data Protection Regulation (GDPR), conﬂict
with the blockchain where data is immutable and cannot be
deleted. These considerations promote the following research
questions:

• How to design and implement an automated and efﬁcient

data integrity veriﬁcation model using blockchain?

To answer this main research question, we further deﬁne

the following sub-questions:

1) What blockchain technologies are suitable for data ver-
iﬁcation, and how scalable are these technologies when
more data need to be validated?

2) How to prevent blockchain from sharing private com-
pany data with unauthorized parties while still ensuring
data integrity?

3) How to comply with national personal data regulations
when personal data is stored in blockchain for data
veriﬁcation?

C. Contributions

The main contribution of this paper is a novel decen-
tralized model called AUDITEM to verify data integrity in
a trustworthy and efﬁcient manner. We further introduce a
second component called the Data Integrity Veriﬁcation Tool
(DIVT). This tool is an addition to the traditional warehouse
by automating the entire validation process. In brief, the main
contributions of this paper can be summarized as follows:

• A blockchain-based decentralized model for data integrity
veriﬁcation, where users can customize the degree of
traceability based on the importance of the data.

• The DIVT protocol, which is a ﬂexible addition on top
of the traditional data management system that combines
the model, functions, and algorithms into a single tool
with user interfaces.

• A methodology to verify data integrity via blockchain
when privacy regulations (e.g., GDPR) require the data
to be deleted.

• A prototype system based on Hyperledger Fabric is fully
developed and tested. The experimental results demon-
strate that AUDITEM is feasible and effective in verifying
data integrity.

The remainder of this paper is organized as follows: In
Section II, the research background is investigated, including
a detailed summary of the related work and research gaps.
In Section III, the model overview is presented. The section
starts with an analysis of the requirements and actors. Then,
the system components, process ﬂows, and privacy regulations
are described. Next, Section IV introduces the implementation
details of the AUDITEM prototype. In Section V, our model
is evaluated with detailed experiments to investigate whether it
meets the requirements. Section VI discusses the advantages
as well as limitations. Finally, in Section VII, the paper is
concluded, and the future work is presented.

II. RELATED WORK

This section ﬁrst reviews the state-of-the-art data integrity
veriﬁcation models related to AUDITEM. Then, existing re-
search gaps are summarized, and the position of this study is
discussed.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

3

TABLE I
COMPARISON OF EXISTING DATA VERIFICATION MODELS AND THEIR LIMITATIONS.

Ref.

Topic

Limitations

Chen et al. [16]
Gan et al. [17]
Lu et al. [5]
Hao et al. [18]
Wang et al. [19]
Li et al. [20]
Zyskind et al. [21]
Sun et al. [22]
Kalis and Belloum [8]

Dynamic data auditing
Outsourced big data auditing
Outsourced data auditing
Data integrity veriﬁcation
Provable data possession
IoT data storage and protection
Personal data management
Medical records storage and retrieval
Data integrity veriﬁcation

Still rely on trusted third parties to conﬁrm data integrity.
Still rely on trusted third parties to conﬁrm data integrity.
Still rely on trusted third parties to conﬁrm data integrity.
PoW-based permissionless blockchain leads to high costs and limited scalability.
Only works with simple data types. High computation latency for large data ﬁles.
Require radical changes to the existing data management system.
Only designed for personal proﬁles. Lack of implementation of the model.
Only designed for health care data records.
Only high-level tamper-proof traceability. PoW causes high costs and low scalability.

A. Data Integrity Veriﬁcation Models

Existing related research is in multiple directions. Tra-
ditional data integrity veriﬁcation studies mainly focus on
innovating new veriﬁcation algorithms to support diverse ver-
iﬁcation needs. In this respect, Chen et al. [16] and Gan et
al. [17] proposed two efﬁcient and secure auditing schemes
for dynamic data auditing operations. On the other hand,
blockchain has recently been proposed as a trustworthy data
auditing device. Lu et al. [5] tried to improve the traditional
audit model by combining third-party auditing services with a
blockchain system. Their model constructs a trusted system
by randomly assigning a user to multiple untrusted third-
party auditors through a selection algorithm. Hao et al. [18]
proposed a system where the blockchain is leveraged as a data
auditing device. The system works by creating two functions:
WriteBlock and Checkblock. In the WriteBlock function, the
encrypted data is outsourced to an untrusted external data
source, and a blockchain with a PoW consensus is used to
create and conﬁrm the data signature. In the CheckBlock
function, the state of the blockchain is conﬁrmed with the
data on the external data source using the signature. This
is to prove that the data exist and have not been changed.
Similarly, Wang et al. [19] create a blockchain scheme that
focuses more on efﬁcient cryptographic methods. To eliminate
data storage problems in traditional centralized servers, Li et
al. [20] proposed a distributed IoT data storage scheme using
blockchain and cetriﬁcateless cryptography. Zyskind et al. [21]
created a personal data management tool using an off-chain
storage solution with a focus on privacy protection. In their
model, the data owner controls the permissions of other parties
to view the data. Sun et al. [22] introduced a system to store
and retrieve medical records via IPFS and blockchain securely.
The focus of this work is to encrypt the ﬁles stored in IFPS and
request the decryption key using speciﬁc keywords. Kalis and
Belloum [8] presented a more simple algorithm. They decide
only to store a hash and a unique identiﬁer on the blockchain.
New versions of the data are stored under a new identiﬁer. To
verify the integrity, the hash is recalculated and matched with
the hash on the blockchain. In this model, the data owner is
considered untrusted, and the audit trail covers all forms of
malicious actors.

party to conﬁrm the data integrity, not addressing the issue of
untrustworthy auditors and certiﬁcates. The model in [18] uses
a permissionless blockchain with the PoW consensus, which
leads to high costs and limited scalability. Similarly, the model
proposed by [19] may suffer from high computation latency
when larger ﬁles need to be encrypted and stored [23]. Some
of the blockchain-based techniques require a complete change
of the current data management tools [20]. Besides, they are
often designed for small ﬁles such as personal proﬁles [21]
or special data formats such as health care records [22], and
are not suitable for large tables stored in a data warehouse
or data lake. The technique presented by [8] only stores a
periodic hash on a public blockchain. The storage of more
hashes results in better traceability of the tampering location
but increases the storage burden. Furthermore, an increase in
information storage would also lead to an increase in cost,
limiting the scalability of this technique. In summary, the
topics and limitations of the discussed techniques are shown
in Table I.

B. Existing Research Gaps

The ﬁrst gap that needs to be addressed is the scalability of
the blockchain when building a data veriﬁcation model. The
consensus algorithm is the main factor limiting the scalability
of current blockchain-based systems [23]. Many presented
models work with permissionless blockchains that require a
compute-intensive PoW consensus; however, this is not feasi-
ble for large-scale data storage and veriﬁcation. On the other
hand, even permissioned blockchains may have the scalability
issue, as transaction throughput may not be sufﬁcient in some
cases [24]. Besides, there are considerable differences in the
discussed literature regarding how data manipulation should
be located and recovered. For example, the identiﬁcation of
tampering is at the table level in [8], while in [18] a small
part of a ﬁle is checked to verify the authenticity. The amount
of data stored in a tamper-proof location determines how
many tampered details can be located and recovered. However,
keeping more data on the blockchain requires greater scala-
bility and storage space. This results in a constant trade-off
between traceability/recoverability and scalability/redundancy
when using blockchain for data auditing.

It should be noted that each of the discussed models has
some limitations when combined with a real-life data reposi-
tory. The models in [17], [5], [16] all rely on a trusted third

Another gap is how to prevent the blockchain from sharing
private data with unauthorized parties while still ensuring the
integrity of the data. Considering the transparent nature of the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

4

blockchain, most researchers do not recommend storing the
actual data on the blockchain. As a result, two main techniques
can be adopted for protecting data privacy. The ﬁrst approach
is to extract only the hash of the data ﬁle and store it on the
blockchain, as illustrated in [8], [21]. This approach allows
for fast and secure veriﬁcation but does not provide sufﬁcient
traceability and recoverability. In fact, comparing ﬁle hashes
only tells the auditor whether the ﬁle has been changed, not
where it has been modiﬁed. Another approach is to store the
encrypted data in an off-chain distributed ﬁle system for data
comparison. For instance, the authors in [25], [22] use the
attribute-based encryption (ABE) protocol for encryption and
use IPFS to store encrypted data. This asymmetric encryption
provides good security but may not be suitable for large-scale
data integrity veriﬁcation tasks due to the high computational
overhead [26].

Finally, there is a gap in compliance with national data
regulations when personal data is stored in blockchain for
data veriﬁcation. Regardless of whether the discussed models
are based on permissionless or permissioned blockchain, it is
always difﬁcult to modify and delete data once it is uploaded.
This can be a huge problem when it comes to personal data
regulations like GDPR, which allow users to delete personal
private data [24]. A frequently used solution is to store
personal data off-chain so that it can be deleted or edited at
any time. However, this is not a solution for checking the
integrity of data repositories that contain personal information
(e.g., an address register). For example, if one address needs
to be deleted, the ﬁle’s hash value will change and result in
an integrity violation. In addition, malicious actors may also
use this feature to compromise the data integrity veriﬁcation
system.

To summarize, the following research gaps are identiﬁed in

the related studies:

1) Some data veriﬁcation techniques

radical
changes to the core system, and they are not suitable
for complex data types, e.g., geometric data.

require

2) Some data veriﬁcation models still require one or more
trusted third parties to conﬁrm the integrity. However,
this can cause a SPOF.

3) Existing blockchain-based data veriﬁcation models do
not meet the required scalability. They have high trans-
action costs due to the use of PoW or have high
computational overhead due to the need to send too
many transactions.

4) Existing blockchain-based integrity veriﬁcation tools do
not have the option to handle GDPR because they
assume that the user is the data owner or that the data
does not need to be changed.

This paper builds on top of our previous conference paper
[27]. We have extensively extended the model, protocol,
evaluation, and discussion, with more than 60% of the content
newly added. Speciﬁcally, In Section I, we extended the
research background and added the motivating scenario and
problem statement subsections. In Section II, we added a
detailed review and comparison of the related studies. In
Section III, we updated the model to include introductions

on schemes/algorithms and privacy regulations. In Section IV,
we updated the implementation details with the chaincode
demonstration and the explanation of the encryption algorithm.
In Section V, we added a new functional experiment sub-
section and extended another four benchmarks for blockchain
scalability experiments. New experimental results and ﬁndings
are presented. In Section VI, we added a detailed discussion
about the advantages and limitations of the model, which is
missing in the conference manuscript. Finally, the conclusion
was updated in Section VII.

III. MODEL DESIGN

In this section, we introduce the model details of AU-
DITEM. The section starts with an analysis of the require-
ments and actors. Then, the system components, process ﬂows,
and privacy regulations are introduced in detail.

A. Requirement Analysis

The requirements are derived in compliance with Spatial
Eye1, a utility company in the Netherlands that creates data
warehouse applications. However, the possibility of model
usage is not limited to utility and telecommunication compa-
nies. The input of AUDITEM is an existing data management
tool such as a database, data warehouse, or data lake. In this
section, we use a data warehouse as a demonstration example.
The output of AUDITEM is a certiﬁcate that proves the
integrity of the data. It is stored on the blockchain and provides
a trustworthy endorsement of the source data. In summary, the
following requirements are identiﬁed for AUDITEM:

• Decentralization: The model should be decentralized and

not rely on trusted third parties to avoid a SPOF.

• Traceability: The location of a possible manipulation
must be determined at least on the table’s column level
and can be customized to be more precise if necessary.
• Security: Users’ personal data must be encrypted and
times when stored outside of the data

secured at all
storage products.

• Data Types: The model should be able to handle various

data types for data integrity veriﬁcation.

• Regulations: The model should comply with the GDPR’s
right to be forgotten, i.e., personal data can be deleted
after the end of the service.

• Overhead: The computational overhead should be kept to
a few minutes or less when a regular data batch is added
to the data repository.

• Scalability: It should be possible to allow at least a few
dozen companies in a consortium to use the system and
submit blockchain transactions simultaneously.

B. Actor Identiﬁcation

The actors and stakeholders that interact with AUDITEM
are human actors, external systems, or devices that exchange
information with the model. Taking this into account, the
following actors are identiﬁed:

1https://www.spatial-eye.com/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

5

Fig. 1. Consortium overview of the AUDITEM system.

1) Regular User: The users in the proposed use case are
usually analysts who use the geographic information
system (GIS). The GIS extracts and uploads data to
the data warehouse. Regular users do not want their
the system to
workﬂow to be changed;
work fully automatic and without performing different
operations to verify integrity.

they expect

2) Internal Auditor: The internal auditor makes the ﬁnan-
cial reports and expects that the asset data retrieved from
the data warehouse is accurate. They do not have much
IT knowledge, so the auditing process should be easy to
set up. In addition, they expect AUDITEM to be precise,
so data mistakes or manipulations can be detected and
recovered on time.

3) External Auditor: External auditors can be investors, au-
diting companies, or government agencies. They expect
a company to have perfect bookkeeping. They like to
see a certiﬁcate to show that everything about the data
is correct. It is essential that the system is explainable
for external auditors, as they need to trust the certiﬁcate.

C. System Components

AUDITEM follows a modular design approach and consists
of four main components. A consortium overview of the
AUDITEM system is shown in Fig. 1.

1) Data Warehouse: The data warehouse is the data source
of the system. It takes care of the retrieval of data stored
in the various connected databases. Data is added to the
data warehouse in batches on a regular basis.

2) Data Integrity Veriﬁcation Tool (DIVT): This tool is the
communication interface between the system and users.
It takes care of basic operations such as encryption, de-
cryption, authentication, and veriﬁcation and automates
the whole process. To provide a scalable solution for

large-scale data veriﬁcation, we design a scheme that
combines the Advanced Encryption Standard (AES) and
column-level hashes. Besides, two veriﬁcation methods
are designed to support diverse veriﬁcation require-
ments. The schemes and algorithms used in the DIVT
protocol are summarized in Fig. 2. The symbolic expres-
sions used in this section are illustrated in Table II.
3) Blockchain: A consortium permissioned blockchain is
leveraged to build trust among different business orga-
nizations. The immutable and decentralized nature of
the blockchain makes it suitable for storing and sharing
integrity veriﬁcation hashes and identiﬁcation attributes.
As more parties join the blockchain network, the system
becomes more decentralized with enhanced network
protection.

4) Distributed File System: A distributed ﬁle system is used
to take care of the storage of additional veriﬁcation ﬁles
because of the limited storage capability of blockchain.
The distributed ﬁle system should permanently store ﬁles
and have a precise storage location, preferably based on
ﬁle content. Options are the InterPlanetary File System
(IPFS), Hadoop distributed File System (HDFS), and
blockchain-based ﬁle systems like StorJ or bigchanDB
[28].

TABLE II
LIST OF SYMBOLS USED IN THIS SECTION.

Symbol

Description

by
si
sv
hv
r
ps
CT
hl

The batch subset y of the data warehouse, added or updated at the same time.
Identiﬁcation attribute information where a batch can be identiﬁed from.
Veriﬁcation attribute information that can help with the integrity veriﬁcation.
The SHA-256 hash of the veriﬁcation attribute.
The veriﬁcation record that contains identiﬁcation and veriﬁcation attributes.
The private key used to encrypt and decrypt the veriﬁcation record r.
The encrypted ciphertext of the veriﬁcation record r.
The location hash of the veriﬁcation record from the IPFS.

Distributed File SystemBlockchainSmart ContractsDataWarehouseInvokeUploadDIVTOrg1UserInternalAuditorDataWarehouseDIVTUserInternalAuditorOrg2DIVTDataWarehouseUserInternal  AuditorOrgNIdentification Attribute Generator Verification Attribute Generator Record Generator EncryptVerify IDecryptVerify IIUpdateSearchExternal AuditorExternal AuditorExternal AuditorCheckDIVTJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

6

The DIVT protocol is used to assist data users and internal/external auditors to perform automated data integrity veriﬁcation tasks.
Input:

• A batch subset of the data warehouse by. A private key ps for the encryption/decryption of veriﬁcation records. A blockchain

network with conﬁgured smart contracts. A distributed ﬁle storage system.

Protocol: DIVT

Output:

• The on-chain certiﬁcate indicating whether the data batch holds the integrity.

Functions:

• IDAttGen. (by) −→ (si): Identiﬁcation attribute generator extracts the identiﬁcation keyword set si from the data batch.
• VrfcAttGen. (by) −→ (sv + hv): The veriﬁcation attribute generator extracts the integrity veriﬁcation attribute sv from the data

batch and generate its hash hv.

• RecGen. (si + sv) −→ (r): The record generator is to create a record ﬁle r in searchable format based on the veriﬁcation

attribute sv and identiﬁcation attribute si.

• Encrypt. (r + ps) −→ (CT + hl): The inputs of the encryption function are the veriﬁcation record r and the private key ps.

The created ciphertext CT is stored on the distributed ﬁle system, and the location hash hl is stored on the blockchain.

• Search. (si) −→ (h(cid:48)

v + hl): The identiﬁcation attribute si is used to search the blockchain network via the smart contract to

get the new veriﬁcation hash h(cid:48)

v and location hash hl.

• Verify I. (hv + h(cid:48)

v): The hash hv created from the data batch is compared with the hash h(cid:48)

v retrieved from the blockchain. If

hv = h(cid:48)

v, it shows that the data is authentic and has not been tampered with.

• Decrypt. (hl + CT + ps) −→ (r): The auditor receives the ciphertext CT from the decentralized storage system using hl,

and decrypts it with the private key ps to receive r.

• Verify II. (sv + s(cid:48)

• Update. (r) −→ (r(cid:48)): Compare the old veriﬁcation attribute sv with the new veriﬁcation attribute s(cid:48)

v): The veriﬁcation attribute sv from the data batch is compared with s(cid:48)

v received from the distributed ﬁle
vx , then it can be concluded that the data batch is authentic. Otherwise the data is reported as tampered.
v; if changes are allowed,

system. If ∀x, svx = s(cid:48)

the new veriﬁcation record r(cid:48) is created.

Fig. 2. The schemes/algorithms used in the DIVT protocol.

Fig. 3. Sequence diagram of the AUDITEM process ﬂows.

D. Process Flows

The process ﬂow of AUDITEM can be divided into two
general steps: 1) upload new data to the data warehouse and
generate attributes, and 2) check data integrity by internal and
external auditors.

1) Upload New Data: In the Spatial Eye use case, a new
network cable is buried in the ground, and information about
this cable is sent to the network architect, who later adds it to
the GIS along with other changes in the electricity network.
Then, the GIS uploads the network changes as batch number
100 to the data warehouse at the end of the day. The DIVT
builds on top of the data warehouse extracts identiﬁcation
and veriﬁcation attributes from this batch. The identiﬁcation
attribute si is the keyword set to identify the batch, in this
case it can be: si = {T ableId : LowV oltage,
BatchId : 100, T imestamp : 2017-12-09, ..., ...}. The veri-
ﬁcation attribute sv is to verify the integrity of the warehouse
batch and can include the vertical column hashes of the table.
In practice, sv can vary between data tables based on the
importance of the data. It is recommended to have at least
one SHA-256 hash for each column in sv to support good
traceability. Nevertheless, it should at least consist of a hash
of the subset and a description of what other attributes are
included. Both si and sv are added together into a veriﬁcation
record r, namely r = sv + si. r is encrypted using a
private key ps, and the returned cyphertext is stored on the
distributed ﬁle system for later veriﬁcation. A location hash
hl is returned from the distributed ﬁle system; this location
hash is stored together with veriﬁcation hash hv and the

AuditorSmart ContractConfirmation9 Request10 Verification Hash15 Certificate19 Certificate20 optPhase 1: Upload New DataPhase 2: Data Integrity Check[Until all data are loop    4 AES Encryption  6 Location Hash  8 Transaction ID    11 Batch Query     User Data WarehouseDIVTDistributed File Systemuploaded]        2 Identification Attribute3 Verification Attribute         12 Identification Attribute13 Verification Attribute        16 LocationHash  [info] 17 Location Hash   1 Data Batch  5 Verification Record                        7 Identification Attribute, Verification & Location Hash14 Identification Attribute  18 Verification RecordJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

7

identiﬁcation attribute si by broadcasting a transaction to the
blockchain smart contract. The private key ps is stored on a
private ledger with the identiﬁcation attribute. An overview of
the process can be seen in the ﬁrst phase of Fig. 3.

v and hl. h(cid:48)

2) Data Integrity Check: When internal auditors request a
data veriﬁcation task, the DIVT protocol helps them automate
this process. An overview of the process ﬂow can be seen
in the second phase of Fig. 3. First, the auditor selects the
batch of data to be validated. For this example, the LowVoltage
table is used. DIVT then extracts the identiﬁcation attribute
si and veriﬁcation attribute sv based on the batch number
from the data source. Next, the identiﬁcation attribute si is
used to search the blockchain network via the smart contract
to get h(cid:48)
v retrieved from the blockchain can be
compared to hv received from the batch data with sv. If
both are the same value, we can conﬁrm the integrity of
this speciﬁc batch. However, if the hashes are different, a
more advanced veriﬁcation method is invoked to perform a
deep comparison. At this time, DIVT will extract r from the
distributed ﬁle system and private key ps from the private
blockchain ledger. s(cid:48)
v obtained from r decryption can be
compared with sv extracted from the LowVoltage table to ﬁnd
the differences. A certiﬁcate containing the date, auditor, and
result is broadcast to the blockchain if the veriﬁcation of all
columns is successful. Otherwise, the modiﬁed columns will
be reported. External auditors can conﬁrm a company’s data
records by checking the certiﬁcates on the blockchain when
they have the proper permission.

E. Privacy Regulations

There should be a method to take care of personal data
stored in the veriﬁcation process. An example of this use
case could be that
in the previously discussed batch 100
of the LowVoltage table, some records contain information
about the endpoint users. After a year, the service is ﬁnished,
and the company decides that this data is no longer needed,
so it should be deleted. However,
if this data is deleted
from the warehouse, it will no longer correspond with the
blockchain records, resulting in a suggested loss of integrity.
A reasonable solution is to create a new veriﬁcation record
and update hl and hv on the blockchain. However, this could
be a potential vulnerability for the system, i.e., a malicious
actor can also delete the data and update the veriﬁcation
records. To prevent the risks associated with this function,
certain countermeasures need to be designed: 1) Only GDPR-
sensitive data is allowed to be edited, and these data should
not be stored on the blockchain; 2) The changes have to be
logged on the blockchain so modiﬁcations can be audited and
traced; and 3) It has to be easy or automatic to delete the
personal data when there is a reason.

To fulﬁll the above requirements, the proposed method lets
the smart contract compare the old veriﬁcation attribute sv
with the new veriﬁcation attribute s(cid:48)
v. In the record description,
r is speciﬁed which attributes are essential and which are not.
For example, the length of a low voltage cable can never
be erased, so slenght must always be the same as s(cid:48)
lenght.
However, s(cid:48)
EndP oint is allowed to replace sEndP oint if there

is a reason, and the user is allowed to perform this action. As
a ﬁnal precaution, a unique client can be speciﬁed within the
smart contract. This client can control a GDPR cleanup script
that automatically removes GDPR-sensitive data and updates
related attributes.

IV. IMPLEMENTATION

This section describes the implementation and technical
details of AUDITEM. AUDITEM can be developed using a
variety of existing technologies. Our implementation is open-
sourced in the Github repository2.

A. Design Choices

The design choices are made in compliance with the utility
company Spatial Eye. However, there could be more suitable
technologies available for other use cases.

1) Spatial Data Warehouse: The data warehouse product
used in this implementation is the Spatial Warehouse3.
This tool is used to work with numeric and alphabetic
data. It can also handle advanced data structures, such
as geographic data. The advantage of this tool is that it
can track changes in data over time and facilitate data
analysis.

2) DIVT: The DIVT protocol is programmed using the
Python Jupyter notebook. Jupyter notebook is an open-
source, web-based interactive computational environ-
ment4. It is often used by data scientists working in
Python or R. The decision to use Jupyter notebooks is
based on the fact that it provides an easy-to-use interface
and interactive data science environment for users.
3) IPFS: For the distributed ﬁle system, IPFS is chosen in
this implementation5. IFPS meets all our requirements
because it is decentralized and allows for faster and
secure storage at a lower cost. Besides, it uses a hash
over the ﬁle’s content as a ﬁle location. This gives the
advantage that data cannot be stored twice as this will
result in the same identiﬁcation hash, saving redundant
storage space [22].

4) Hyperledger Fabric: Hyperledger Fabric is an open-
source permissioned blockchain platform. The reason for
choosing a permissioned blockchain is that companies
work together in a consortium in the proposed model. In
addition, the efﬁcient consensus mechanism used in the
permissioned blockchain can improve scalability and re-
duce energy waste. AUDITEM is built with Hyperledger
Fabric v2.2, where the recommended consensus protocol
is Raft. This consensus mechanism is a Byzantine fault-
tolerant mechanism that determines new transactions by
a selected leader node [29].

2https://github.com/ZeshunShi/AUDITEM
3https://www.spatial-eye.com/product-spatial-warehouse.html
4https://jupyter.org/
5https://ipfs.io/

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

8

Listing 1 Integrity veriﬁcation chaincode.

Listing 2 Private storage chaincode.

// Data structure of the verification evidence
type VerificationEvidence struct {

Organisation string `json:"Organisation"`
Table_name string `json:"Table_name"`
Batch_ID string `json:"Batch_ID"`
Verification_Hash string `json:"Verification_Hash"`

}
// To create the verification evidence in the blockchain
func (s *SmartContract) createEvidence(APIstub
(cid:44)→

shim.ChaincodeStubInterface, args []string)
sc.Response {
var rec = VerificationEvidence{Organisation: args[1],
(cid:44)→

(cid:44)→

Table_name: args[2], Batch_ID: args[3],
Verification_Hash: args[4]}

(cid:44)→
RecordAsBytes, _ := json.Marshal(rec)
APIstub.PutState(args[0], RecordAsBytes)
compositeKey, err :=
(cid:44)→

APIstub.CreateCompositeKey("owner_key",
[]string{rec.Organisation, args[0]})

(cid:44)→
if err != nil {

return shim.Error(err.Error())

}
value := []byte{0x00}
APIstub.PutState(compositeKey, value)
return shim.Success(RecordAsBytes)

}
// To query the verification evidence
func (s *SmartContract) queryEvidence(APIstub
(cid:44)→

shim.ChaincodeStubInterface, args []string)
sc.Response {
RecordAsBytes, _ := APIstub.GetState(args[0])
return shim.Success(RecordAsBytes)

(cid:44)→

}

B. Chaincodes Implementation

The smart contract in Hyperledger Fabric is called chain-
code and can be written in programming languages such as
Go, Node.js, and java. In order to achieve the model function,
the following three chaincodes are designed in AUDITEM.

1) Integrity Veriﬁcation: The integrity veriﬁcation chain-
code is the main chaincode to create veriﬁcation evi-
dence on the blockchain and query them when needed.
includes two main functions: createEvidence and
It
queryEvidence. The demonstration code is shown in
Listing 1. First of all, the data structure of veriﬁcation
evidence contains four string ﬁelds, namely organisation,
table name, Batch ID, and veriﬁcation hash. In the
Hyperledger Fabric blcockchain, write and read opera-
tions are performed using the PutState() and GetState()
APIs. createEvidence function ﬁrst reads the veriﬁcation
evidence and encodes the data into a JSON object.
Next, the createCompositeKey() API is used to create
a composite key for the putState() function. Finally the
record and composite key index are stored separately
using the putState() API. In the queryEvidence function,
the record is queried by the GetState() API. If the
operation is successful, the function returns no error.
2) Private Storage: Hyperledger Fabric offers the ability
to create private data collections to store the company’s
private data, which cannot be viewed by other partic-
ipants in the blockchain network without permission.
In AUDITEM, private data collection is used to store
the keys necessary to decrypt the integrity veriﬁcation
ﬁles in IPFS. With private data collection, keys can be

// To create the private key
func(s * SmartContract) createPrivateKey(APIstub
(cid:44)→

shim.ChaincodeStubInterface, args[] string)
sc.Response {

(cid:44)→

type TransientInput struct {

SecretKey string `json:"secretKey"`
Nonce string `json:"nonce"`
Key string `json:"key"`

}
transMap, err := APIstub.GetTransient()
if err != nil {

return shim.Error(err.Error())

}
privateDataAsBytes, errr := transMap["keys"]
if !errr {

return shim.Error("")

}
var Input TransientInput
err = json.Unmarshal(privateDataAsBytes, &Input)
if len(Input.SecretKey) == 0 {

return shim.Error("")

}
if len(Input.Nonce) == 0 {
return shim.Error("")

}
PrivateDetails := &PrivateDetails {

SecretKey: Input.SecretKey,
Nonce: Input.Nonce

json.Marshal(PrivateDetails)

}
PrivateDetailsAsBytes, err :=
(cid:44)→
err = APIstub.PutPrivateData("collectionPrivateDetails",
Input.Key, PrivateDetailsAsBytes)
(cid:44)→
return shim.Success(PrivateDetailsAsBytes)

}
// To query the private key
func(s * SmartContract) queryPrivateKey(APIstub
shim.ChaincodeStubInterface, args[] string)
(cid:44)→
sc.Response {

(cid:44)→

keyAsBytes, err := APIstub.GetPrivateData(args[0],
args[1])
(cid:44)→
if err != nil {

jsonResp := "{\"Error\":\"" + args[1] + err.Error() +
(cid:44)→
return shim.Error(jsonResp)
} else if keyAsBytes == nil {

"\"}"

jsonResp := "{\"Error\":\"" + args[1] + "\"}"
return shim.Error(jsonResp)

}
return shim.Success(keyAsBytes)

}

easily shared with external auditors when needed. The
private storage chaincode contains a createPrivateKey
and a queryPrivateKey function, as shown in Listing 2.
The createPrivateKey function ﬁrst checks if the input
parameters are correct, and then the secret key and
nonce are saved using the PutPrivateData() API. Cor-
respondingly, queryPrivateKey function uses the Get-
PrivateData() API to retrieve the data. In Hyperledger
Fabric, these two APIs allow smart contracts to store and
retrieve private data collections in a secure and privacy-
preserving manner.

3) Integrity Certiﬁcates: The integrity certiﬁcate chaincode
is responsible for the storage of the certiﬁcate that shows
the authenticity of the data. It includes a createCertiﬁ-
cate function, which is invoked when data is identiﬁed
as authentic, and a queryCertiﬁcate, which is called by
external auditors to query the certiﬁcate. The certiﬁcate
attributes of this chaincode also contain user and time

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

9

SDK uses the JWT token to determine if the user has
the proper authorization. The payload of the transaction
is a JSON string; for private data, this is transient.
3) Query: The query operation happens via the same logic
as the invoke operation but is used when a Get request
is received. Their difference is that the invoke operation
is used for transactions that change the blockchain
state. Instead, the query operation only queries the state
database and does not create a new transaction or block.

D. DIVT Protocol

The DIVT protocol is the automated interface for users to
interact with the data warehouse, blockchain, and IPFS. In ad-
dition, it is responsible for creating and comparing veriﬁcation
attributes and sending requests to the blockchain to generate
certiﬁcates. We have identiﬁed three types of AUDITEM
users in Section III-B. Each type of user corresponds to a
different system function and requires a unique user interface.
Therefore, three notebook interfaces are created in the DIVT
protocol: one for uploading data to the warehouse by regular
users, one for checking integrity by internal auditors, and one
for checking integrity certiﬁcates by external auditors. In the
following text, we introduce the key techniques used in the
DIVT protocol.

1) Veriﬁcation Options: When the database is loaded, in-
dividual veriﬁcation ﬁles can be created based on the
number of batches and tables in the database. Veriﬁca-
tion attributes are JSON ﬁles containing different ﬁelds
that can be customized according to the importance of
the data. Storing more veriﬁcation attributes increases
the traceability and recoverability of the repository. It
is important to declare in the JSON ﬁle what the prop-
erties are and how they are used because the attributes
must be recreated for the veriﬁcation tasks. In addition,
for GDPR compliance, it is necessary to know which
columns are allowed to be changed. Taking this into
consideration, we propose three different veriﬁcation
options in Listing 3. The three options from top to
bottom have sequentially higher traceability, but at the
same time,
lead to greater computational overhead.
Other conﬁgurations can be easily extended to support
more complex data types such as images.

2) Veriﬁcation Functions: We developed two functions to
address different veriﬁcation tasks. Verify I is supposed
to be a faster veriﬁcation method. It uses the veriﬁcation
hash from the blockchain and compares it with the
veriﬁcation hash created from the data batch. If the two
hashes are the same, it means that the data has not been
changed, and there is no need for deeper investigation.
By contrast, Verify II is a deeper comparison, as it
compares the two integrity ﬁles. This takes more time as
the ﬁle needs to be retrieved from IPFS and compared
with the current database state. Therefore, Verify II is
only used when Verify I results that there is something
wrong.

Fig. 4. Overview of the components and the internal communications.

information to facilitate validation.

C. Communications

The communication between different components is per-
formed through the REST protocol. This allows using HTTP
operations such as GET and POST for component interaction.
An overview of the communication within the network can be
found in Fig. 4. A blockchain manager is needed to conﬁgure
the blockchain parameters in advance. CouchDB is a state
database in the Hyperledger Fabric blockchain that models
the ledger data in JSON format. Both CouchDB and IPFS
support the possibility of sending HTTP requests via their
APIs, which means that operations can be sent using the
same logic. The Hyperledger Fabric SDK is used to interact
with the blockchain and the DIVT protocol. Especially, it is
responsible for checking authorizations, invoking transactions,
and querying states.

1) Authorization: In Hyperledger Fabric, users are assigned
through a certiﬁcate authority (CA). The CA distributes
certiﬁcates to users that can be used to sign and au-
thenticate transactions. For this implementation, we give
each company a CA and use the default elliptic curve
digital signature algorithm (ECDSA) [30] to generate
user certiﬁcates. Besides, the JSON web token (JWT) is
used to sign REST API requests and invoke transactions.
For example, a new user can be created using a Post
request on the registered URL. The system returns a
JWT token that can be used as a bearer token for the
organization’s transactions.
In order

to invoke a transaction to the
blockchain, a Post request needs to be made to the SDK.
The Post request must mention the chaincode and name
of the chaincode function to invoke a transaction. The

2) Invoke:

States:5984 - Post:5001 - Add/Cat CouchDBSqlite Connect()Invoke:4000 - Post/GetData WarehouseInternal/External  AuditorIPFSDIVTBlockchainSDKUserConfiguration FilesBlockchain  ManagerClientsAUDITEMJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

10

Listing 3 JSON data structure of three veriﬁcation options.

{

}

{

}

{

}

"h_v":"SHA-256 over subset",
"traceability":"1",
"cols":"all column names",
"rows":"number of rows",
"gdpr":"column names containing GDPR data",
"gdprHash":"SHA-256 over the subset without GDPR columns",
"colHash":"vertical calculated SHA-256 for each column"

"h_v":"SHA-256 over subset",
"traceability":"2",
"cols":"all column names",
"rows":"number of rows",
"gdpr":"column names containing GDPR data",
"gdprHash":"SHA-256 over the subset without GDPR columns",
"colHash":"vertical calculated SHA-256 for each column",
"rowHash":"horizontal calculated SHA-256 for each column"

"h_v":"SHA-256 over subset",
"traceability":"3",
"cols":"all column names",
"rows":"number of rows",
"gdpr":"column names containing GDPR data",
"gdprHash":"SHA-256 over the subset without GDPR columns",
"data":"all data inside subset"

AES Round Function

M × C

S-box

*

x

x

x

* x
x
x
x

s2

x

s1
s0
AddRoundKey

SubBytes

ShiftRows

s3

s4
MixColumns

Fig. 5. Overview of the AES encryption used in DIVT.

3) Encryption and Decryption: Veriﬁcation records must be
encrypted before uploading to the IPFS. The AES is used
in this implementation for encryption and decryption
operations in the DIVT protocol. It is a standard chosen
by the U.S. government to protect classiﬁed information
and has been widely used for electronic data protection
due to its security and efﬁcient properties [31]. The AES
encryption algorithm requires multiple runs of the round
function. In each round there are generally four oper-
ations: AddRoundKey, SubBytes, ShiftRows, and Mix-
Columns, as shown in Fig. 5. In the AddRoundKey step,
each byte in the data matrix will perform an XOR
operation with the round key derived from the master
key. Next, the main purpose of the SubBytes step is
to complete the mapping of one byte to another via a
substitution box (S-box). ShiftRows is the permutation
between bytes within a matrix and is used to provide
diffusivity to the algorithm. MixColumns is achieved by
multiplying the state matrix with a ﬁxed matrix to reach
a diffusion over the columns. It should be noted that the
AES algorithm is a symmetric-key algorithm, meaning
that the same key is used for encryption and decryption.

During the encryption, the ﬁle needs to be encoded
into bytes at ﬁrst. Then, the ﬁle is encrypted using a
secret key and a randomly created nonce. Both the secret
key and the nonce is stored in the private ledger of
the Hyperledger Fabric blockchain. The encrypted data
message is then sent to the IPFS for later validation. For
decryption, the nonce and secret key can be extracted
from the private ledger if the user is from a licensed
organization.

V. EXPERIMENTAL EVALUATION

This section conducts extensive experiments to validate the
proposed model. Our tests are divided into functional and non-
functional tests. For functional testing, AUDITEM is evaluated
to see whether it can accomplish veriﬁcation tasks. Non-
functional tests are further divided into computational over-
head and blockchain scalability experiments to test whether
AUDITEM is efﬁcient enough for verifying the integrity.

A. Functional Evaluation

To evaluate the veriﬁcation capabilities of AUDITEM, two
scenarios are designed in which AUDITEM should identify
different data manipulation activities. These scenarios use real
datasets and have been validated by company experts.

• Scenario A: Electron, an electricity company, has some
bad times due to disappointing sales and aged assets that
needed to be replaced. Bob, the CEO of Electron, is
afraid that the presentation of this bad news will inﬂuence
the stock price. To handle this, he asks one of the IT
interns, Alice, to make some of the cable data “newer”
by changing the date they went into the ground. This
will increase the apparent value of the company as the
new cable is worth more and presents better results. The
changes that Alice made in the data warehouse can be
found in Fig. 6. What Bob does not know is that the

Fig. 6. Data manipulation in scenario A: the above is the ground truth data,
and the below is the modiﬁed data.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

11

Fig. 7. Data manipulation in scenario B: the ﬁrst 10 records are repeated.

company recently started using AUDITEM to validate the
data warehouse.

• Scenario B: Alice, an intern at Electron, is working on
the efﬁcient storage of spatial data in Electron’s data
warehouse. She needs a subset of the data and stores it in
a separate test environment for her research. However, she
forgets to change the output location; as a result, the sub-
set is duplicated within the data warehouse. Because she
knows about AUDITEM and does not have permission
to delete her mess-up, she decides to re-upload the right
batch to AUDITEM so that no one else would notice this
mistake. In Fig. 7 the mistakes from Alice can be seen.
Results: If we reproduce scenario A in AUDITEM, we will
get the prompt message presented in Fig. 8. This indicates that
when the cable data record is changed, the integrity is reported
broken because the hash of the “begindate” column is changed.
With this result, we can conﬁrm that AUDITEM can detect
this kind of fraud, i.e., Alice could not easily manipulate the
data, even though she is instructed to do so by the CEO. If
we reproduce scenario B in AUDITEM, we have two prompt
messages, as shown in Fig. 9. Firstly, the uploading of the new
batch is rejected because the batch already exists. Secondly,
the message shows that all the columns have been changed
inside batch 1, but the integrity of non duplicated batches
(batch 2 and batch 5) is still correct. This demonstrates that
AUDITEM can detect this fraud, i.e., Alice can’t hide her
mistake by re-uploading the data batch. It should be noted
that nothing prevents Alice from changing the original data in
the above scenarios. That is why we designed two veriﬁcation
methods in AUDITEM. By comparing the integrity report with
the backups in IPFS, problematic data ﬁelds can be identiﬁed,
and errors can be ﬁxed.

Fig. 8. Outcome of AUDITEM in Scenario A.

Fig. 9. Outcome of AUDITEM in scenario B: the ﬁrst message is on the left,
and the second is on the right.

B. Computational Overhead

To verify whether AUDITEM can handle large and com-
plex data warehouses, we conducted performance tests using
multiple test cases. Especially, we repeatedly run AUDITEM
with different database settings to ﬁnd out how long it takes
to perform the functions. The database varies in the number
of batches and the number of records per batch. For the
evaluation, an SQLite database containing information about
properties in the Netherlands is chosen to simulate a realistic
scenario. A subset table containing 28 columns, including
two columns of complex geometries, is used to perform the
benchmark. The subsets are generated by randomly selecting
1 000, 10 000, 100 000, 1 000 000, or 10 000 000 records.
Then, each subset is duplicated to create 10 subsets. Half of
the subsets contain one batch, while the other half contains one
batch per 1 000 records. An overview of the subsets can be
seen in Table III. The subset containing 10 000 000 records
is only tested once. All other benchmarks are performed in
triplicate. For the benchmark, there are three points of interest:
1) the time for uploading subsets to the network, 2) the time
for performing integrity check with Verify I and Verify II; and
3) the time of the individual functions in the DIVT protocol.

TABLE III
OVERVIEW OF VARIATIONS IN BENCHMARK DATA SUBSETS.

Experiment

1

2

3

4

5

Benchmark I

Benchmark II

Records:
Batches:
Records:
Batches:

1 000
1
1 000
1

10 000
10
10 000
1

100 000
100
100 000
1

1 000 000
1 000
1 000 000
1

10 000 000
-
10 000 000
1

Fig. 10(a) shows the overhead variation with different
number of batches. The result indicates that the execution
time to upload one batch is consistent per experiment. As the
number of batches increases from 1, 10, 100 to 1 000, the
execution time of Upload and Verify II also increases with
a factor of 10. The only exception where the linear increase
is not consistent is Verify I with 1 000 batches. This higher
increase could be explained by a higher blockchain query time.
For the next benchmark, the performance is evaluated with a
different number of records per batch, as shown in Fig. 10(b).
It can be seen that the execution time of Verify I is signiﬁcantly
lower when the batch size is small, but increases to the same
level as Upload and Verify II as the batch size increases. Also,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

12

it is worth noting that the execution time of the three functions
tends to increase exponentially when the number of records
increases. We can conclude that uploading and verifying a
batch containing 1 000 000 records is an efﬁcient choice since
it takes only 100 seconds. When the batch size is increased
by a further 10 times, the execution time increases by a factor
of 100. Besides, it is not efﬁcient to upload small batches of
1 000 records as this takes almost the same time as 10 000
records.

To investigate this further, the execution time of individual
functions with different batch sizes is tested and plotted in
Fig. 10(c). It is possible to see which functions cause the
system delay in the plot. First, retrieving identiﬁcations is
the least time-consuming function and remains stable as the
number of records increases. When having 1 000 records per
batch, the dominant factors are the two functions that perform
API requests and wait for responses: send to IPFS and send
to blockchain. When more records are added to the batch, the
execution time of these two functions remains stable. After 100
000 records per batch, retrieving the data from the database
and creating the veriﬁcation attributes are the most dominant
factors. This is reasonable since 10 000 000 geographic records
are almost 4 GB in size to be transferred to DIVT. In summary,
we can conclude that AUDITEM is effective because a large
database of 10 000 000 geographical records can be validated
within 3 hours.

C. Blockchain Scalability

The limited scalability of blockchain networks often leads
to high latency or transaction failures [32]. For this reason,
AUDITEM aims to limit the number of transactions done
with the blockchain. Nevertheless, the blockchain network
needs to process transactions from multiple users simultane-
ously, making a performance evaluation necessary. For the
evaluation, a benchmark tool developed by the Hyperledger
Foundation called Caliper [33] is used. Caliper is installed
as a docker image and run inside a docker container. The
performance benchmarks are operated based on the following
conﬁgurations:

• In addition to the Hyperledger Fabric mentioned in
Section IV, four other popular permissioned blockchain

platforms were identiﬁed as benchmarks for testing the
AUDITEM system. These platforms include the Hyper-
ledger community’s Iroha, Besu, and Sawtooth and the
Ethereum permissioned (private) blockchain. The com-
parison of these platforms is summarized in Table IV.
We leave the testing of more platforms for our future
work.

• The scalability of AUDITEM’s underlying blockchain is
tested by submitting different numbers of transactions per
second (1, 2, 4, 8, 16, 32, 64, 128, 256) to the platform
with a ﬁxed workload. Each experiment is repeated ﬁve
times to obtain the average throughput or latency values.
• Two basic smart contract operations are measured: 1)
Write operations are those related to the creation of
attributes required for data integrity veriﬁcation; 2) Read
operations are those related to the query of attributes and
certiﬁcates in smart contracts.

• Although different blockchain platforms have different
conﬁgurations, consensus algorithms, and other factors
that may affect performance, we believe these impacts are
limited by the platform itself [32]. Therefore, we focus
on comparing the scalability of these platforms under the
benchmark conﬁguration from a platform level.

• The performance of the blockchain is measured by the
metrics of throughput,
latency, and success ratio. In
particular, throughput is a measure of how many trans-
actions are completed within a given time frame, latency
is the time for transaction execution, and success ratio
is the ratio of successful transactions to all submitted
transactions [34].

Fig. 11 contains ten plots comparing the ﬁve blockchain
platforms’ throughput and latency for write/read operations
with 1 worker. First, Beau, Ethereum, and Iroha have similar
performance in the read throughput testing, with a bottleneck
of around 200 transactions per second (TPS). Fabric holds a
slightly lower read throughput of around 130 TPS. Sawtooth
offers the worst performance, with a bottleneck of only 30
TPS. In terms of write performance, Ethereum and Iroha
have similar write throughput bottlenecks of about 13 TPS. In
contrast, Besu has about twice as much throughput when input
transaction rates are very high. Fabric holds a much higher

Fig. 10. Execution time of the DIVT protocol with (a) different batch numbers for upload and verify functions; (b) different batch size for upload and verify
functions; and (c) different batch size for individual component functions.

100101102103Number of batches(a)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(b)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(c)10-210-1100101102103104Execution Time (s)Retrieve from DBCreate verificationsSent to IPFSSent to blockchainRetrieve identifications100101102103Number of batches(a)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(b)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(c)10-210-1100101102103104Execution Time (s)Retrieve from DBCreate verificationsSent to IPFSSent to blockchainRetrieve identifications100101102103Number of batches(a)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(b)10-1100101102103104Execution Time (s)UploadVerify IVerify II103104105106107Number of records per batch(c)10-210-1100101102103104Execution Time (s)Retrieve from DBCreate verificationsSent to IPFSSent to blockchainRetrieve identificationsJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

13

TABLE IV
COMPARISON OF FIVE PERMISSIONED BLOCKCHAIN PLATFORMS.

Blockchain

Introduction

Hyperledger
Besu

Ethereum
(private)

Hyperledger
Fabric

Hyperledger
Iroha

Hyperledger
Sawtooth

An Ethereum client that can run on public
networks, test networks, and private permis-
sioned networks.
An open-source blockchain platform with
smart contract
functionality that handles
transactions via the EVM.

An open-source permissioned blockchain
with a highly ﬂexible and adaptable design
for enterprise usage.

A framework for incorporating blockchain
into infrastructures or IoT projects that are
easy and quick to implement.
A secure and modularity-based architecture
for creating enterprise-level permissioned
blockchains.

Consensus
Algorithms

Smart Contract
Protocol

Key Features

PoW and PoA Smart Contract

- Support for multiple types of Ethereum
networks

PoA

Smart Contract

- Mature blockchain and DApp system
- Customized for enterprise applications

and

Raft
Kafka
(deprecated
in v2.x)

YAC

Chaincode

- Multi-ledger structure
- Private data storage in channels

Command (built
in contracts)

- Easy to use
- Built-in smart contract functionality

PoET, Raft,
and PBFT

Transaction Pro-
cessor

- Parallel transaction execution
- Customizable transaction processors

Fig. 11. Comparison of throughput and latency of write/read operations for ﬁve chosen blockchain platforms. The plots (a) to (e) plots show the throughput
results, while the plots (f) to (j) show the latency results.

Fig. 12. Performance of Hyperledger Fabric in the AUDITEM system at different input transaction rates and the number of workers. The plots (a) and (b)
show the results of write operations, while the plots (c) and (d) show the results of read operations.

write throughput than other platforms, with a bottleneck of
around 70 TPS. Secondly, the ﬁve subplots below show that
Fabric has signiﬁcantly lower latency for write operations than

the other four blockchain platforms. A detailed comparison
of the maximum, minimum, and average latency of the ﬁve
blockchain platforms can be seen in Table V. The result further

1  2  4  8  16 32 64 128256Input transaction rate (tps)(a)10-1100101102103Throughput (tps)WriteReadEthereum1  2  4  8  16 32 64 128256Input transaction rate (tps)(b)10-1100101102103Throughput (tps)WriteReadHyperledger Fabric1  2  4  8  16 32 64 128256Input transaction rate (tps)(c)10-1100101102103Throughput (tps)WriteReadHyperledger Iroha1  2  4  8  16 32 64 128256Input transaction rate (tps)(d)10-1100101102103Throughput (tps)WriteReadHyperledger Sawtooth1  2  4  8  16 32 64 128256Input transaction rate (tps)(e)10-1100101102103Throughput (tps)WriteRead1  2  4  8  16 32 64 128256Input transaction rate (tps)(f)10-2100102Latency (s)WriteReadEthereum1  2  4  8  16 32 64 128256Input transaction rate (tps)(g)10-2100102Latency (s)WriteReadHyperledger Fabric1  2  4  8  16 32 64 128256Input transaction rate (tps)(h)10-2100102Latency (s)WriteReadHyperledger Iroha1  2  4  8  16 32 64 128256Input transaction rate (tps)(i)10-2100102Latency (s)WriteReadHyperledger Sawtooth1  2  4  8  16 32 64 128256Input transaction rate (tps)(j)10-2100102Latency (s)WriteReadHyperledger BesuHyperledger Besu1  2  4  8  16 32 64 128256Input transaction rate (tps)(a)050100150Write throughput (tps)0.80.850.90.9510.8Latency (s)Throughput (10workers)Throughput (20workers)Latency (10workers)Latency (20workers)1  2  4  8  16 32 64 128256Input transaction rate (tps)(b)050100150Write throughput (tps)0.80.850.90.951Success ratio (%)Throughput (10workers)Throughput (20workers)Success ratio (10workers)Success ratio (20workers)1  2  4  8  16 32 64 128256Input transaction rate (tps)(c)050100150Read throughput (tps)0.80.850.90.9510.8Latency (s)Throughput (10workers)Throughput (20workers)Latency (10workers)Latency (20workers)1  2  4  8  16 32 64 128256Input transaction rate (tps)(d)050100150Read throughput (tps)0.80.850.90.951Success ratio (%)Throughput (10workers)Throughput (20workers)Success ratio (10workers)Success ratio (20workers)JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

14

TABLE V
EXECUTION LATENCY OF FIVE BLOCKCHAIN PLATFORMS.

Blockchain

Send Rate (TPS)

Write Operation
Max Latency (s) Min Latency (s)

Read Operation
Avg Latency (s) Max Latency (s) Min Latency (s)

Avg Latency (s)

Hyperledger Beau

Ethereum (private)

Hyperledger Fabric

Hyperledger Iroha

Hyperledger Sawtooth

1
2
4
8
16
32
64
128
256

1
2
4
8
16
32
64
128
256

1
2
4
8
16
32
64
128
256

1
2
4
8
16
32
64
128
256

1
2
4
8
16
32
64
128
256

5.116 ± 0.009
5.288 ± 0.443
5.476 ± 0.533
5.656 ± 0.540
5.860 ± 0.447
5.402 ± 0.608
4.082 ± 0.008
3.340 ± 0.119
3.336 ± 0.088

7.072 ± 0.015
7.070 ± 0.012
7.056 ± 0.009
7.060 ± 0.000
7.074 ± 0.005
7.062 ± 0.004
7.086 ± 0.005
7.184 ± 0.055
7.468 ± 0.182

0.360 ± 0.101
0.694 ± 0.518
0.518 ± 0.448
0.528 ± 0.454
0.342 ± 0.018
0.388 ± 0.029
0.726 ± 0.446
1.164 ± 0.153
1.238 ± 0.099

1.036 ± 0.005
1.030 ± 0.000
1.030 ± 0.000
1.028 ± 0.004
1.028 ± 0.004
1.226 ± 0.449
2.046 ± 0.009
2.154 ± 0.133
2.098 ± 0.035

3.020 ± 0.000
3.020 ± 0.000
3.020 ± 0.000
2.822 ± 0.443
3.024 ± 0.005
4.034 ± 0.005
6.022 ± 0.004
7.036 ± 0.005
4.060 ± 2.748

0.288 ± 0.004
0.274 ± 0.036
0.040 ± 0.000
0.048 ± 0.004
0.052 ± 0.004
0.072 ± 0.011
0.132 ± 0.029
0.324 ± 0.113
0.532 ± 0.023

3.056 ± 0.005
3.056 ± 0.031
3.108 ± 0.091
2.912 ± 0.051
3.022 ± 0.013
2.850 ± 0.788
3.266 ± 0.045
2.796 ± 0.044
2.844 ± 0.068

5.034 ± 0.005
5.032 ± 0.004
5.030 ± 0.000
5.250 ± 0.022
5.390 ± 0.025
5.920 ± 0.019
6.624 ± 0.005
7.084 ± 0.017
7.258 ± 0.112

0.298 ± 0.008
0.304 ± 0.005
0.214 ± 0.017
0.214 ± 0.031
0.204 ± 0.005
0.226 ± 0.005
0.386 ± 0.214
0.810 ± 0.132
0.946 ± 0.038

5.202 ± 1.894
5.760 ± 0.372
8.602 ± 2.327
6.350 ± 0.587
9.746 ± 3.528
9.188 ± 3.140
15.542 ± 9.606
14.048 ± 10.550
7.532 ± 1.267

56.242 ± 30.050
38.082 ± 11.623
35.822 ± 10.701
28.694 ± 9.826
29.628 ± 23.472
29.944 ± 18.929
35.388 ± 14.294
19.414 ± 4.372
41.724 ± 15.870

0.106 ± 0.015
0.168 ± 0.018
0.164 ± 0.011
0.166 ± 0.054
2.256 ± 0.942
4.506 ± 1.388
10.172 ± 4.662
9.152 ± 4.815
5.618 ± 0.224

0.552 ± 0.180
1.124 ± 0.475
13.658 ± 11.498
3.676 ± 6.420
23.430 ± 23.532
26.612 ± 19.225
24.278 ± 15.852
15.428 ± 7.824
41.260 ± 15.857

1.518 ± 0.078
1.786 ± 0.174
2.394 ± 0.573
2.346 ± 0.111
5.890 ± 1.966
7.416 ± 2.729
12.906 ± 6.709
11.530 ± 6.690
7.018 ± 1.116

19.552 ± 14.957
17.746 ± 4.899
23.844 ± 12.153
22.568 ± 9.709
26.646 ± 23.373
28.322 ± 19.015
34.504 ± 14.149
18.946 ± 4.363
41.486 ± 15.867

0.032 ± 0.011
0.032 ± 0.022
0.022 ± 0.004
0.024 ± 0.005
0.020 ± 0.000
0.020 ± 0.000
0.026 ± 0.005
0.050 ± 0.029
0.190 ± 0.030

0.020 ± 0.000
0.020 ± 0.000
0.020 ± 0.000
0.022 ± 0.004
0.030 ± 0.022
0.020 ± 0.000
0.028 ± 0.008
0.038 ± 0.008
0.162 ± 0.025

0.022 ± 0.004
0.192 ± 0.373
0.206 ± 0.416
0.028 ± 0.018
0.224 ± 0.456
0.036 ± 0.005
0.052 ± 0.022
0.698 ± 0.178
0.760 ± 0.034

0.024 ± 0.005
0.018 ± 0.004
0.018 ± 0.004
0.026 ± 0.019
0.018 ± 0.004
0.020 ± 0.000
0.020 ± 0.012
0.026 ± 0.005
0.378 ± 0.107

0.062 ± 0.013
0.456 ± 0.189
0.454 ± 0.075
0.570 ± 0.099
0.680 ± 0.137
1.270 ± 0.249
2.672 ± 0.227
2.860 ± 0.321
3.062 ± 0.263

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.016 ± 0.005

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.012 ± 0.004

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.026 ± 0.009
0.042 ± 0.004

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.016 ± 0.005

0.030 ± 0.000
0.030 ± 0.000
0.030 ± 0.000
0.030 ± 0.000
0.030 ± 0.000
0.050 ± 0.012
0.066 ± 0.015
0.188 ± 0.097
0.634 ± 0.071

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.022 ± 0.004
0.092 ± 0.019

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.020 ± 0.000
0.070 ± 0.007

0.016 ± 0.005
0.018 ± 0.008
0.020 ± 0.012
0.020 ± 0.000
0.038 ± 0.040
0.020 ± 0.000
0.020 ± 0.000
0.308 ± 0.120
0.416 ± 0.011

0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.186 ± 0.069

0.038 ± 0.004
0.042 ± 0.004
0.044 ± 0.005
0.054 ± 0.005
0.108 ± 0.028
0.926 ± 0.219
2.010 ± 0.244
2.320 ± 0.263
2.504 ± 0.194

TABLE VI
EXECUTION LATENCY OF HYPERLEDGER FABRIC WITH DIFFERENT WORKER NUMBERS.

Blockchain

Send Rate (TPS)

Write Operation
Max Latency (s) Min Latency (s)

Read Operation
Avg Latency (s) Max Latency (s) Min Latency (s)

Avg Latency (s)

Hyperledger Fabric
(10 workers)

Hyperledger Fabric
(20 workers)

1
2
4
8
16
32
64
128
256

1
2
4
8
16
32
64
128
256

0.684 ± 0.390
0.342 ± 0.049
0.370 ± 0.025
0.364 ± 0.130
0.336 ± 0.044
0.558 ± 0.566
0.802 ± 0.130
1.206 ± 0.042
1.236 ± 0.009

0.892 ± 0.442
0.604 ± 0.090
1.430 ± 1.743
0.485 ± 0.062
0.444 ± 0.055
0.594 ± 0.203
0.860 ± 0.121
1.175 ± 0.114
1.390 ± 0.054

0.124 ± 0.023
0.070 ± 0.000
0.082 ± 0.013
0.070 ± 0.007
0.076 ± 0.013
0.070 ± 0.010
0.216 ± 0.057
0.610 ± 0.046
0.678 ± 0.049

0.136 ± 0.061
0.120 ± 0.026
0.133 ± 0.036
0.108 ± 0.025
0.124 ± 0.034
0.120 ± 0.027
0.230 ± 0.092
0.683 ± 0.047
0.818 ± 0.061

0.310 ± 0.057
0.202 ± 0.013
0.222 ± 0.026
0.188 ± 0.022
0.186 ± 0.009
0.244 ± 0.160
0.560 ± 0.079
0.910 ± 0.032
0.972 ± 0.018

0.418 ± 0.104
0.330 ± 0.033
0.333 ± 0.043
0.265 ± 0.026
0.266 ± 0.021
0.316 ± 0.086
0.547 ± 0.111
0.945 ± 0.068
1.060 ± 0.029

0.074 ± 0.022
0.086 ± 0.030
0.056 ± 0.009
0.074 ± 0.018
0.062 ± 0.023
0.126 ± 0.128
0.108 ± 0.024
0.274 ± 0.032
0.548 ± 0.031

0.134 ± 0.054
0.144 ± 0.092
0.200 ± 0.175
0.100 ± 0.031
0.248 ± 0.224
0.124 ± 0.106
0.264 ± 0.328
0.320 ± 0.289
0.490 ± 0.047

0.010 ± 0.000
0.012 ± 0.004
0.010 ± 0.000
0.010 ± 0.000
0.010 ± 0.000
0.012 ± 0.004
0.010 ± 0.000
0.012 ± 0.004
0.012 ± 0.004

0.012 ± 0.004
0.014 ± 0.005
0.012 ± 0.004
0.014 ± 0.005
0.016 ± 0.009
0.012 ± 0.004
0.012 ± 0.004
0.020 ± 0.007
0.018 ± 0.004

0.026 ± 0.005
0.028 ± 0.008
0.024 ± 0.005
0.028 ± 0.004
0.026 ± 0.005
0.026 ± 0.013
0.030 ± 0.000
0.104 ± 0.015
0.328 ± 0.019

0.044 ± 0.026
0.040 ± 0.014
0.054 ± 0.043
0.038 ± 0.004
0.082 ± 0.105
0.052 ± 0.050
0.072 ± 0.069
0.160 ± 0.174
0.280 ± 0.029

validates our conclusion that although Fabric’s read perfor-
mance (throughput and latency) is not the best, it has the best
performance for write operations and acceptable performance
for read operations. In fact, write performance is particularly
critical in our model because more transactions submitted to
the blockchain imply a higher level of security for integrity
veriﬁcation. In contrast, read performance is not as critical
since data auditing is not required at high frequency. Based
on this experiment, we conclude that Fabric is the blockchain

platform that best meets our scalability requirement.

Fig. 12 further interprets the performance of the Fabric
under different worker numbers. At this time, different num-
bers of workers (10 and 20) are leveraged to diversify clients
connected to the blockchain network. As can be seen from the
plot, the highest write/read throughput can be found for both
numbers of workers at an input transaction rate of 256 TPS.
When the number of workers is doubled from 10 to 20, the
measurement’s throughput and stability are slightly decreased.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

15

However, the overall performance bottleneck is maintained at a
high level (write and read throughput are 60 TPS and 120 TPS
for 20 workers). The performance is also compared with the
transaction success ratio, as shown in Fig. 12(b) and Fig. 12(d).
It can be seen that when the number of workers is 10, all
transactions can be submitted successfully. However, when
worker numbers increase to 20, some of the write operations
may fail when the input transaction rate is high. Nevertheless,
the success ratio is maintained above 95% in all experiments,
which demonstrates the good scalability of the model. It is
also evident from the latency experiments in Fig. 12(a) and
Fig. 12(c) that the latency increases in different degrees when
both the number of workers and the input transaction rate
increase. However, all the latency values are maintained within
1.5 seconds. This low latency of the Fabric blockchain ensures
that the entire model can work efﬁciently.

In Table VI, we show the latency result of Fabric with
different worker numbers. It can be seen that despite some
ﬂuctuations, the maximum, minimum, and average latency
increases for both worker numbers when increasing the input
transaction rate from 1 to 256 TPS. If we take the example of
1 000 veriﬁcation transactions, in the worst case (i.e., an input
rate of 256), 10 and 20 workers can still complete the workload
in an average of 16 and 18 minutes. Latency only increased
by 9% when the number of workers doubled. Compared to
write operations, read operations have very low latency, with
1 000 queries costing only about 30 seconds to complete.
These experiments demonstrate that the latency satisﬁes the
scalability requirements of our model.

VI. DISCUSSION

This section discusses the advantages and limitations of the

proposed model.

A. Advantages

We ﬁrst discuss the advantages of AUDITEM based on the

research questions proposed in Section I-B.

RQ 1. What blockchain technologies are suitable for data
veriﬁcation, and how scalable are these technologies when
more data need to be validated?

We have identiﬁed that scalability is one of the core
challenges of combining blockchain with large-scale data veri-
ﬁcation tasks. It is well known that public blockchain networks
such as Bitcoin and Ethereum suffer from low throughput and
high transaction costs. Therefore, a permissioned blockchain
with better scalability and privacy properties is decided as
the underlying blockchain of AUDITEM for data veriﬁcation.
It should be noted that the scalability issues may also arise
in permissioned blockchains, which is why we conducted
various experiments in Section V-C. The experimental results
demonstrated that Hyperledger Fabric is the one that best
meets our scalability requirement compared to the other four
benchmark blockchain platforms. Besides, AUDITEM tries
to minimize the transactions submitted to the blockchain,
as this is often the most limiting factor for scalability. A
distributed ﬁle system is used in AUDITEM to store so-called

veriﬁcation attributes to validate or recover data repositories.
These attributes allow ﬂexibility in choosing the degree of
recoverability and traceability when more data need to be
validated.

RQ 2. How to prevent blockchain from sharing private com-
pany data with unauthorized parties while still ensuring data
integrity?

In AUDITEM, a series of approaches are taken to prevent
private data from being shared with unauthorized parties.
These methods include the careful selection of what is stored
on the blockchain, the encryption of the veriﬁcation attributes,
and the use of a private storage chaincode to store private keys.
In order to protect private data, only non-personally identiﬁca-
tion attributes are stored on the blockchain in AUDITEM. In
addition, to ensure privacy and traceability, AUDITEM utilizes
both hashing and AES encryption to prevent unauthorized
access to the company’s private data. Hashing the data table
identiﬁes the location of possible tampering while protecting
data privacy very well. All veriﬁcation records are encrypted
using AES and stored in a decentralized ﬁle system for further
validation. Finally, we take advantage of Hyperledger Fabric’s
ability to create private data collections and use a private
storage chaincode to generate and query private keys. These
private keys are stored on private channels in Hyperledger Fab-
ric and are only accessible to authorized organizations/peers.
With this chaincode, private keys can be securely shared with
external auditors when needed.

RQ 3. How to comply with national personal data regulations
when personal data is stored in blockchain for data veriﬁca-
tion?

One of the core features of blockchain is the immutability
of the data. This is by nature conﬂicting with data regulations,
e.g., the right to be forgotten in GDPR. Therefore, a solution
is not as straightforward as expected. Storing personal data or
references in immutable integrity ﬁles gives them a trustworthy
status. If AUDITEM allows integrity ﬁles to be updated when
personal data is changed or deleted, the malicious actor can
use this function for their beneﬁt. To solve this, we designed an
update function that can only be invoked when personal data is
edited. The smart contract conﬁrms whether only the personal
data is edited and the non-personal data is authentic before
updating the data. The user and the reason for calling the
function are kept as logs in the blockchain for future auditing.
With this solution, personal data can be updated when needed,
but it is difﬁcult to change non-personal data and perform
malicious actions. For further research, it would be beneﬁcial
to implement AUTITEM and conﬁrm its compatibility with
other data regulations.

Next, we discuss how AUDITEM ﬁlls the research gaps
summarized in Section II-B. AUDITEM is compatible with
most existing data management tools without changing the
core system. For regular users,
the activity of adding or
querying data within the data repository does not change.
Besides, the workﬂow is totally automated and optimized for
data veriﬁcation processes; users and internal/external auditors
have a new interface to upload veriﬁcation attributes and check

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

16

the data integrity. To validate how AUDITEM works with
a complex data type, the geometric data has been tested in
Section V with good performance results obtained. Secondly,
AUDITEM is decentralized and does not require a trusted
third party to perform integrity veriﬁcation. It leverages a
blockchain to provide immutable and trustworthy certiﬁcates
the storage of additional data. All
and an IPFS to assist
veriﬁcation tasks are performed automatically based on self-
executing smart contracts and the DIVT protocol. Regarding
scalability, AUDITEM works with batch validation to reduce
costs. This allows validating a database on-demand at low
overhead. In addition, AUDITEM is developed based on the
permissioned blockchain, and experiments have shown that
the current implementation of Hyperledger Fabric has good
to update or delete
scalability. Finally, companies expect
personal data in a timely manner. AUDITEM does not change
the current way GDPR is handled. It still gives the possibility
to delete or edit personal information without altering the
blockchain. So companies that are currently compliant with
the GDPR can expect to remain compliant with the GDPR
after implementing AUDITEM.

B. Limitations

The limitations of AUDITEM can be discussed in terms of

scalability and recoverability concerns and security issues.

1) Scalability and Recoverability Concerns: We observed
that when performing upload and veriﬁcation operations, it is
not recommended to use data batches with a large number
of records because of the long processing time. A solution to
this problem could be concurrent processing, where multiple
small data batches are uploaded simultaneously; however, this
also increases the transaction input rate to the blockchain and
can lead to high latency. In the current implementation of AU-
DITEM, we observed that the Hyperledger Fabric blockchain
could not handle write operations with more than 70 TPS.
This can lead to problems when companies upload their initial
database or use cases where uploads are more frequent. In this
regard, more advanced blockchain scalability solutions such as
sharding and DAG blockchains [35] should be investigated in
the future.

The next concern is the degree of traceability of mali-
cious behaviors and the recoverability of tampered data, as
AUDITEM does not prevent tampering. Storing more data
in integrity ﬁles increases traceability and recoverability; this
does not increase the workload of the blockchain but does
result in increased storage in the IPFS. The question of how
IPFS handles large-scale ﬁle storage has not been investigated.
To completely prevent the possibility of tampering, the data
repository can be replaced with a distributed ﬁle system. As
an additional beneﬁt, this makes it easier to recover the data.
This, however, would lead to a complete change of the core
product/system, something that is not desired.

2) Security Issues: Analyzing the AUDITEM’s security is
complex as it contains many components, and the implemen-
tation can be different. We can look at the security from
the core functions of the veriﬁcation task: immutable storage
and data encryption. The immutable storage of the records

and certiﬁcates is dependent on the choice of the underlying
blockchain. In our implementation, the RAFT consensus pro-
tocol used in Hyperledger Fabric can tolerate f number of
crash nodes in a network with a total number of n = 2f + 1
nodes [29]. This means that an organization can control the
blockchain if it holds more than half of the nodes, which,
of course, should be prevented at all times. More companies
joining the consortium network reduces this probability. On the
other hand, Hyperledger Fabric can hand out user certiﬁcates
to prevent adversaries from misleading the network. However,
the network can be compromised by an internal adversary.
For example, an internal attacker sitting between DIVT and
the database can send fake data to DIVT, causing the model
to use the fake database to verify integrity. An accountability
mechanism can partially prevent this issue.

For the comparison of the veriﬁcation records, the SHA-
256 hash function is used and can be considered secure [24].
Besides, the identiﬁcation attributes are stored publicly in our
model; this is an intentional design since we assume those
attributes do not contain sensitive information. If for some
reason, the identiﬁcation attributes contain information that
cannot be shared with the outside world, the decision could
be made to use cryptographic hashes. In this case, users should
be careful of dictionary attacks as identiﬁcation attributes are
often short identiﬁcation tags [36]. To prevent the unauthorized
share of data, the integrity veriﬁcation records are protected
using the AES encryption algorithm. The encryption ensures
that information is only visible to designated partners who
obtained the private keys. Of course, anyone can retrieve the
encrypted integrity records, but the data is safe as long as the
secret keys remain secret and the AES algorithm stays safe.
This is based on the fact that in a properly implemented AES,
there is currently no feasible attack that would allow someone
without a key to access encrypted data [37]. However, various
implementations of AES have been reported to be potentially
vulnerable to side-channel attacks and social engineering at-
tacks [38]. The use of the differential privacy technique can
be effective in migrating side-channel attacks [39], but this
inevitably reduces the data utility due to the introduction of
data noise. Therefore, in this respect, the comparison and
implementation of other alternative encryption algorithms are
left for our future work.

VII. CONCLUSION

This paper proposes a novel decentralized data integrity ver-
iﬁcation model called AUDITEM. It ﬁlls the gaps in the exist-
ing literature by processing large-scale complex data stored in
traditional data repositories without changing the system’s core
components. AUDITEM aims to replace the current expensive
and untrustworthy third-party auditing services. In addition to
decentralization, it is designed to achieve three objectives:
scalability, private data protection, and GDPR compliance.
A sub-module called DIVT is also developed to support
automated operations and customizable veriﬁcation operations.
Experimental and analytical results show that AUDITEM is
feasible and effective in meeting various business requirements
for data integrity veriﬁcation.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

17

The future work can be directed in two main directions:
blockchain platforms and cryptographic algorithms. In terms
of the blockchain, we will continue to investigate other scal-
able and secure blockchain platforms for possible integration
with AUDIEEM. Regarding cryptographic algorithms,
the
comparison and implementation of other alternative crypto-
graphic algorithms will be studied in the future.

ACKNOWLEDGMENT

This research is funded by the European Union’s Horizon
2020 research and innovation program under grant agreements
825134 (ARTICONF project), 862409 (BlueCloud project)
and 824068 (ENVRI-FAIR project). The research is also sup-
ported by the Chinese Scholarship Council, and EU LifeWatch
ERIC.

REFERENCES

[1] R. Mork, P. Martin, and Z. Zhao, “Contemporary challenges for data-
intensive scientiﬁc workﬂow management systems,” in Workﬂows in
Support of Large-Scale Science Workshop (WORKS).
ACM, 2015,
pp. 1–11.
data
[2] Conﬂuent,
warehouses
[On-
line]. Available: https://www.conﬂuent.io/learn/databases-data-lakes-
and-data-warehouses-compared/

Inc.
explained. Accessed

(2021) Databases,

29 December

and
2021.

lakes,

data

[3] J. E. Boritz, “IS practitioners’ views on core concepts of information

integrity,” Int. J. Account. Inf. Syst., vol. 6, no. 4, pp. 260–279, 2005.

[4] California Spectra

Instrumentation,

Inc.

veriﬁcation. Accessed 25 December 2021.
https://spectralogic.com/features/data-integrity-veriﬁcation/

(2021) Data

integrity
[Online]. Available:

[5] N. Lu, Y. Zhang, W. Shi, S. Kumari, and K.-K. R. Choo, “A secure and
scalable data integrity auditing scheme based on hyperledger fabric,”
Comput. Secur. J., vol. 92, p. 101741, 2020.

[6] Audit Analytics. (2022) Trends in the european audit market. Accessed
18 January 2022. [Online]. Available: https://www.auditanalytics.com/
doc/Audit-Analytics-Trends-in-the-European-Audit-Market.pdf

[7] M. Belotti, N. Boži´c, G. Pujolle, and S. Secci, “A vademecum on
blockchain technologies: When, which, and how,” IEEE Commun. Surv.
Tutor., vol. 21, no. 4, pp. 3796–3838, 2019.

[8] R. Kalis and A. Belloum, “Validating data integrity with blockchain,”
in IEEE International Conference on Cloud Computing Technology and
Science (CloudCom).
IEEE, 2018, pp. 272–277.

[9] D. Brandon, “The blockchain: The future of business information
systems,” Int. J. Acad. Bus. World, vol. 10, no. 2, pp. 33–40, 2016.
[10] H. Zhou, C. de Laat, and Z. Zhao, “Trustworthy cloud service level
agreement enforcement with blockchain based smart contract,” in IEEE
International Conference on Cloud Computing Technology and Science
(CloudCom).

IEEE, 2018, pp. 255–260.

[11] L. Liu, J. Feng, Q. Pei, C. Chen, Y. Ming, B. Shang, and M. Dong,
“Blockchain-enabled secure data sharing scheme in mobile-edge com-
puting: An asynchronous advantage actor–critic learning approach,”
IEEE Internet Things J., vol. 8, no. 4, pp. 2342–2353, 2021.

[12] H. Yin, Z. Zhang, J. He, L. Ma, L. Zhu, M. Li, and B. Khoussainov,
“Proof of continuous work for reliable data storage over permissionless
blockchain,” IEEE Internet Things J., pp. 1–1, 2021.

[13] T. Li, H. Wang, D. He, and J. Yu, “Blockchain-based privacy-preserving
and rewarding private data sharing for iot,” IEEE Internet Things J., pp.
1–1, 2022.

[14] M. Zhaofeng, W. Lingyun, W. Xiaochang, W. Zhen, and Z. Weizhe,
“Blockchain-enabled decentralized trust management and secure usage
control of iot big data,” IEEE Internet Things J., vol. 7, no. 5, pp. 4000–
4015, 2020.

[15] N. Metje, A. Hojjati, A. Beck, and C. D. Rogers, “Improved underground
utilities asset management–assessing the impact of the UK utility survey
standard (PAS128),” Proc. Inst. Civil Eng.-Munic. Eng., vol. 173, pp.
218–236, 2020.

[16] X. Chen, T. Shang, F. Zhang, J. Liu, and Z. Guan, “Dynamic data
auditing scheme for big data storage,” Front. Comput. Sci., vol. 14,
no. 1, pp. 219–229, 2020.

[17] Q. Gan, X. Wang, and X. Fang, “Efﬁcient and secure auditing scheme
for outsourced big data with dynamicity in cloud,” Sci. China Inf. Sci.,
vol. 61, no. 12, pp. 1–15, 2018.

[18] K. Hao, J. Xin, Z. Wang, Z. Jiang, and G. Wang, “Decentralized data
integrity veriﬁcation model in untrusted environment,” in Asia-Paciﬁc
Web (APWeb) and Web-Age Information Management (WAIM) Joint
International Conference on Web and Big Data.
Springer, 2018, pp.
410–424.

[19] H. Wang, Q. Wang, and D. He, “Blockchain-based private provable data
possession,” IEEE Trans. Dependable Secur. Comput., vol. 18, no. 5, pp.
2379–2389, 2021.

[20] R. Li, T. Song, B. Mei, H. Li, X. Cheng, and L. Sun, “Blockchain for
large-scale internet of things data storage and protection,” IEEE Trans.
Serv. Comput., vol. 12, no. 5, pp. 762–771, 2018.

[21] G. Zyskind, O. Nathan et al., “Decentralizing privacy: Using blockchain
to protect personal data,” in IEEE Security and Privacy Workshops
(SPW).

IEEE, 2015, pp. 180–184.

[22] J. Sun, X. Yao, S. Wang, and Y. Wu, “Blockchain-based secure storage
and access scheme for electronic medical records in IPFS,” IEEE Access,
vol. 8, pp. 59 389–59 401, 2020.

[23] K. Gangadevi and R. R. Devi, “A survey on data integrity veriﬁcation
schemes using blockchain technology in cloud computing environment,”
in IOP Conference Series: Materials Science and Engineering.
IOP
Publishing, 2021, p. 012011.

[24] N. B. Truong, K. Sun, G. M. Lee, and Y. Guo, “GDPR-compliant
personal data management: A blockchain-based solution,” IEEE Trans.
Inf. Forensic Secur., vol. 15, pp. 1746–1761, 2020.

[25] S. Wang, Y. Zhang, and Y. Zhang, “A blockchain-based framework for
data sharing with ﬁne-grained access control in decentralized storage
systems,” IEEE Access, vol. 6, pp. 38 437–38 450, 2018.

[26] B. S. Varsha and P. Suryateja, “Using attribute-based encryption with
advanced encryption standard for secure and scalable sharing of personal
health records in cloud,” Int. J. Comput. Sci. Inf. Technol., vol. 5, no. 5,
pp. 6395–6399, 2014.

[27] J. Bergers, Z. Shi, K. Korsmit, and Z. Zhao, “DWH-DIM: A blockchain
based decentralized integrity veriﬁcation model for data warehouses,”
in 2021 IEEE International Conference on Blockchain (Blockchain).
IEEE, 2021, pp. 221–228.

[28] H. Huang, J. Lin, B. Zheng, Z. Zheng, and J. Bian, “When blockchain
meets distributed ﬁle systems: An overview, challenges, and open
issues,” IEEE Access, vol. 8, pp. 50 574–50 586, 2020.

[29] Hyperledger Foundation. (2022) Hyperledger fabric documentation.
Accessed 15 January 2022. [Online]. Available: https://hyperledger-
fabric.readthedocs.io/en/release-2.2/

[30] Hyperledger Foundation. (2021) Fabric CA user’s guide. Accessed
[Online]. Available: https://hyperledger-fabric-

20 December 2021.
ca.readthedocs.io/en/release-1.4/users-guide.html

[31] W. E. Burr, “Selecting the advanced encryption standard,” IEEE Secur.

Priv., vol. 1, no. 2, pp. 43–52, 2003.

[32] Z. Shi, H. Zhou, Y. Hu, S. Jayachander, C. de Laat, and Z. Zhao,
“Operating permissioned blockchain in clouds: A performance study
of hyperledger sawtooth,” in 2019 18th International Symposium on
Parallel and Distributed Computing (ISPDC).
IEEE, 2019, pp. 50–57.
(2021) Hyperledger caliper. Accessed 21
[Online]. Available: https://hyperledger.github.io/

[33] Hyperledger Foundation.
December 2021.
caliper/
[34] Hyperledger

per-
Foundation.
formance metrics white
2022.
[Online]. Available: https://hyperledger-fabric.readthedocs.io/en/release-
2.2/chaincode_lifecycle.html

(2022) Hyperledger
paper. Accessed

blockchain
January

17

[35] X. Li, P. Jiang, T. Chen, X. Luo, and Q. Wen, “A survey on the security
of blockchain systems,” Futur. Gener. Comp. Syst., vol. 107, pp. 841–
853, 2020.

[36] B. Pinkas and T. Sander, “Securing passwords against dictionary at-
tacks,” in ACM Conference on Computer and Communications Security
(CCS). ACM, 2002, pp. 161–170.

[37] A. Biryukov and J. Großschädl, “Cryptanalysis of the full AES using
GPU-like special-purpose hardware,” Fundam. Inform., vol. 114, no. 3-
4, pp. 221–237, 2012.

[38] D. A. Osvik, A. Shamir, and E. Tromer, “Cache attacks and countermea-
sures: the case of AES,” in Cryptographers’ Track at the RSA Conference
(CT-RSA). Springer, 2006, pp. 1–20.

[39] M. Xu, A. Papadimitriou, A. Feldman, and A. Haeberlen, “Using
differential privacy to efﬁciently mitigate side channels in distributed an-
alytics,” in European Workshop on Systems Security (EuroSec). ACM,
2018, pp. 1–6.

