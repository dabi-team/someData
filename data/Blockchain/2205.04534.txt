THE BEDROCK OF BYZANTINE FAULT TOLERANCE: A UNIFIED
PLATFORM FOR BFT PROTOCOL DESIGN AND IMPLEMENTATION

2
2
0
2

g
u
A
3

]

C
D
.
s
c
[

2
v
4
3
5
4
0
.
5
0
2
2
:
v
i
X
r
a

Mohammad Javad Amiri1 Chenyuan Wu1 Divyakant Agrawal2
Amr El Abbadi2 Boon Thau Loo1 Mohammad Sadoghi3
1Department of Computer and Information Science, University of Pennsylvania
2Department of Computer Science, University of California Santa Barbara
3Department of Computer Science, University of California Davis
1{mjamiri, wucy, boonloo}@seas.upenn.edu, 2{agrawal, amr}@cs.ucsb.edu, 3 msadoghi@ucdavis.edu

ABSTRACT

Byzantine Fault-Tolerant (BFT) protocols have recently been extensively used by decentralized data
management systems with non-trustworthy infrastructures, e.g., permissioned blockchains. BFT pro-
tocols cover a broad spectrum of design dimensions from infrastructure settings such as the commu-
nication topology, to more technical features such as commitment strategy and even fundamental so-
cial choice properties like order-fairness. The proliferation of diï¬€erent BFT protocols has rendered
it diï¬ƒcult to navigate the BFT landscape, let alone determine the protocol that best meets applic-
ation needs. This paper presents BEDROCK, a uniï¬ed platform for BFT protocols design, analysis,
implementation, and experiments. BEDROCK proposes a design space consisting of a set of design
choices capturing the trade-oï¬€s between diï¬€erent design space dimensions and providing funda-
mentally new insights into the strengths and weaknesses of BFT protocols. BEDROCK enables users
to analyze and experiment with BFT protocols within the space of plausible choices, evolve current
protocols to design new ones, and even uncover previously unknown protocols. Our experimental
results demonstrate the capability of BEDROCK to uniformly evaluate BFT protocols in new ways that
were not possible before due to the diverse assumptions made by these protocols. The results validate
BEDROCKâ€™s ability to analyze and derive BFT protocols.

1

Introduction

Large-scale data management systems rely on fault-tolerant protocols to provide robustness and high availability
[41,54,60,82,94,132,181]. While cloud systems, e.g., Googleâ€™s Spanner [82], Amazonâ€™s Dynamo [94], and Facebookâ€™s
Tao [60], rely on crash fault-tolerant protocols, e.g., Paxos [150], to establish consensus, a Byzantine fault-tolerant
(BFT) protocol is a key ingredient in decentralized data management systems with non-trustworthy infrastructures.
In particular, a BFT protocol is the core component of the most recent large-scale data management system, permis-
sioned blockchains [1â€“3, 24, 27, 29, 30, 43, 64, 78, 116â€“118, 125, 148, 197, 204, 207]. BFT protocols have also been used
in permissionless blockchains [61, 140, 142, 168, 231], distributed ï¬le systems [13, 70, 80], locking service [81], ï¬re-
walls [52, 112, 113, 203, 214, 229], certiï¬cate authority systems [235], SCADA systems [39, 139, 189, 234], key-value
datastores [50, 98, 115, 128, 203], and key management [171].

BFT protocols use the State Machine Replication (SMR) technique [149,205] to ensure that non-faulty replicas execute
client requests in the same order despite the concurrent failure of ğ‘“ Byzantine replicas. BFT SMR protocols are diï¬€erent
along several dimensions, including the number of replicas, processing strategy (i.e., optimistic, pessimistic, or robust),
supporting load balancing, etc. While dependencies and trade-oï¬€s among these dimensions lead to several design
choices, there is currently no unifying tool that provides the foundations for studying and analyzing BFT protocolsâ€™
design dimensions and their trade-oï¬€s. We envision that such a unifying foundation will be based on an in-depth
understanding of existing BFT protocols and trade-oï¬€s among dimensions; and include an API that allows protocol
designers to choose among several dimensions, and ï¬nd a protocol that best ï¬ts the characteristics of their applications.

This paper presents BEDROCK, a uniï¬ed platform that enables us to design, analyze, discover, implement, and exper-
iment with partially asynchronous SMR BFT protocols within the design space of possible variants. It provides an

 
 
 
 
 
 
API that enables BFT protocol designers to analyze and experiment with BFT protocols and their trade-oï¬€s and even
derive new protocols. Application developers also can query their required BFT protocol characteristics where the
platform responds with a list of candidate BFT protocols that match the given query and enables them to choose the
BFT protocol that best ï¬ts the characteristics of their applications.

BEDROCK presents a design space to characterize BFT protocols based on diï¬€erent dimensions that capture the environ-
mental settings, protocol structure, QoS features, and performance optimizations. Each protocol is a plausible point in
the design space. Within the design space, BEDROCK deï¬nes a set of design choices that demonstrate trade-oï¬€s between
diï¬€erent dimensions. For example, the communication complexity can be reduced by increasing the number of com-
munication phases or the number of phases can be reduced by adding more replicas. Each design choice expresses a
one-to-one transformation function to map a plausible input point (i.e., a BFT protocol) to a plausible output point (i.e.,
another BFT protocol) in the design space.

BEDROCK can be used to analyze and navigate the evergrowing BFT landscape to principally compare and diï¬€erentiate
among BFT protocols. On one hand, BEDROCK design space and its design choices give fundamentally new insights
into the strengths and weaknesses of existing BFT protocols. On the other hand, BEDROCK enables new ways to experi-
mentally evaluate BFT protocols by providing a uniï¬ed deployment and experimentation environment, resulting in the
ability to compare diï¬€erent protocols proposed in diverse settings and contexts under one uniï¬ed framework.

The BEDROCK tool has several practical uses:

â€¢ Analyze and experiment with existing BFT protocols. First, BEDROCK supports within one uniï¬ed platform
a wide range of existing BFT protocols, e.g., PBFT [71], SBFT [120], HotStuï¬€ [230], Kauri [186], Themis
[135], Tendermint [63], Prime [22], PoE [123], CheapBFT [133], Q/U [4], FaB [174], and Zyzzyva [143].
â€¢ Evolve an existing protocol to derive new variants. Second, a key beneï¬t of BEDROCK is in evolving existing
protocols to propose new variants. This incremental design paradigm allows one to adapt a BFT implement-
ation over time to suit the deployment environment. For example, we can derive new protocol variants from
the well-known PBFT [71] protocol using a subset of design choices.

â€¢ Uncover new protocols. Finally, BEDROCK can also enable us to discover new protocols in the design space
simply by combining diï¬€erent design choices not previously explored. As a proof of concept, we present two
new protocol instances: a Fast Linear BFT protocol (FLB), that establishes consensus in two linear commu-
nication phases, and a Fast Tree-based balanced BFT protocol (FTB) that supports load balancing.

The paper makes the following contributions.

â€¢ Uniï¬ed design space. A design space for BFT protocols and a set of design choices is proposed. The pro-
posed design space captures fundamentally new insights into the characteristics, strengths and weaknesses of
BFT protocols and their design trade-oï¬€s. Moreover, studying the design space of BFT protocols leads to
identifying several plausible points that have not yet been explored.

â€¢ Uniï¬ed platform. We present the design and implementation of BEDROCK, a tool that aims to unify all BFT
protocols within a single platform. BEDROCK derives valid BFT protocols by combining diï¬€erent design
choices.

â€¢ Implementation and evaluation. Within BEDROCK, a wide range of BFT protocols are implemented and
evaluated. This uniï¬ed deployment and experimentation environment provides new opportunities to evaluate
and compare diï¬€erent existing BFT protocols in a fair and more eï¬ƒcient manner (e.g., identical programming
language, used libraries, cryptographic tools, etc).

The rest of this paper is organized as follows. Section 2 introduces BEDROCK. The design space of BEDROCK and its
design choices are presented in Sections 3 and 4. Section 5 maps some of the known BFT protocols and two new BFT
protocols, FLB and FTB, to the design space. The implementation of BEDROCK is introduced in Section 6. Section 7
shows the experimental results, Section 8 discusses the related work, and Section 9 concludes the paper.

2 The BEDROCK Overview

System model. A BFT protocol runs on a network consisting of a set of nodes that may exhibit arbitrary, potentially
malicious, behavior. BFT protocols use the State Machine Replication (SMR) algorithm [149, 205] where the system
provides a replicated service whose state is mirrored across diï¬€erent deterministic replicas. At a high level, the goal of a
BFT SMR protocol is to assign each client request an order in the global service history and execute it in that order [210].
In a BFT SMR protocol, all non-faulty replicas execute the same requests in the same order (safety) and all correct client

2

) specify valid points (i.e., BFT protocols) while red dots (

Figure 1: A simpliï¬ed design space with two dimensions: number of replicas and number of commitment phases. Green dots
(
) show invalid points (i.e., impossible protocols). A design choice,
i.e., phase reduction (through redundancy), is a one-to-one transformation function that maps a protocol in its domain to another
protocol in its range.

requests are eventually executed (liveness). In an asynchronous system, where replicas can fail, there are no consensus
solutions that guarantees both safety and liveness (FLP result) [109]. As a result, asynchronous consensus protocols
rely on diï¬€erent techniques such as randomization [46, 67, 198], failure detectors [76, 169], hybridization/wormholes
[83, 188] and partial synchrony [100, 103] to circumvent the FLP impossibility result.

BEDROCK assumes the partially synchrony model as it is used in most practical BFT protocols. In the partially synchrony
model, there exists an unknown global stabilization time (GST), after which all messages between correct replicas are
received within some unknown bound Î”. BEDROCK further inherits the standard assumptions of existing BFT protocols.
First, while there is no upper bound on the number of faulty clients, the maximum number of concurrent malicious
replicas is assumed to be ğ‘“ . Second, replicas are connected via an unreliable network that might drop, corrupt, or
delay messages. Third, the network uses point-to-point bi-directional communication channels to connect replicas.
Fourth, the failure of replicas is independent of each other, where a single fault does not lead to the failure of multiple
replicas. This can be achieve by either diversifying replica implementation (e.g., n-version programming) [38, 110]
or placing replicas at diï¬€erent geographic locations (e.g., datacenters) [49, 104, 213, 223]. Finally, a strong adversary
can coordinate malicious replicas and delay communication. However, the adversary cannot subvert cryptographic
assumptions.

Usage model. BEDROCK aims to help application developers experimentally analyze BFT protocols within one uniï¬ed
platform and ï¬nd the BFT protocol that ï¬ts the characteristics of their applications. To achieve this goal, the BEDROCK
tool makes available the design dimensions of BFT protocols and diï¬€erent design choices, i.e., trade-oï¬€s between di-
mensions, to application developers to tune. Figure 1 illustrates an example highlighting the relation between design
space, dimensions, design choices, and protocols in BEDROCK. For the sake of simplicity, we present only two dimen-
sions of the design space, i.e., number of replicas and number of commitment phases (the design space of BEDROCK
consists of more than 10 dimensions as described in Section 3). Each dimension, e.g., number of replicas, can take
diï¬€erent values, e.g., 3ğ‘“ + 1, 5ğ‘“ + 1, 7ğ‘“ + 1, etc. A BFT protocol is then a point in this design space, e.g., (3, 3ğ‘“ + 1).
Note that each dimension not presented in this ï¬gure also takes a value, e.g., communication strategy is assumed to be
pessimistic.
Moreover, a subset of points is valid and represents BFT protocols. In Figure 1, green dots ( ) specify valid points (i.e.,
BFT protocols) while red dots ( ) show invalid points (i.e., impossible protocols). For example, there is no (pessimistic)
BFT protocol with 3ğ‘“ + 1 nodes that commits requests in a single commitment phase. A design choice (Section 4)
is then a one-to-one function that maps a BFT protocol in its domain to another protocol in its range. For example,
phase reduction (through redundancy) maps a BFT protocol with 3ğ‘“ + 1 nodes and 3 phases of communication, e.g.,
PBFT [71], to a BFT protocol with 5ğ‘“ + 1 nodes and 2 phases of communication, e.g., FaB [174] (assuming both
protocols are pessimistic and follow clique topology). The domain and range of each design choice is a subset of BFT
protocols in the design space.

In a BFT protocol, as presented in Figure 2, clients communicate with a set of replicas that maintain a copy of the
application state. A replicaâ€™s lifecycle consists of ordering, execution, view-change, checkpointing, and recovery stages.
The goal of the ordering stage is to establish agreement on a unique order, among requests executing on the application
state. In leader-based consensus protocols, which are the focus of this paper, a designated leader node proposes the
order, and to ensure fault tolerance, needs to get agreement from a subset of the nodes, referred to as a quorum. In
the execution stage, requests are applied to the replicated state machine. The view-change stage replaces the current
leader. Checkpointing is used to garbage-collect data and enable trailing replicas to catch up, and ï¬nally, the recovery
stage recovers replicas from faults by applying software rejuvenation techniques.

3

Figure 2: Diï¬€erent stages of replicas in a BFT protocol

3 Design Space

In BEDROCK, each BFT protocol can be analyzed along several dimensions. These dimensions (and values associated
with each dimension) collectively help to deï¬ne the overall design space of BFT protocols supported by BEDROCK.
The dimensions are categorized into four families: environmental settings and protocol structure that present the core
dimensions of BFT protocols and are shared among all BFT protocols, a set of optional QoS features including order-
fairness and load balancing that a BFT protocol might support, and a set of performance optimizations for tuning BFT
protocols. In the rest of this section, we describe these families of dimensions in greater detail. As we describe each
dimension, we preï¬x label them with "E" for environmental settings, "P" for protocol structure, etc. Hence, "E 1" refers
to the ï¬rst dimension in the environmental settings dimensions family.

This section is not meant to provide a fully exhaustive set of dimensions, but rather to demonstrate the overall meth-
odology used to deï¬ne dimensions usable in BEDROCK.

3.1 Environmental Settings

Environmental settings broadly speaking encompass the deployment environment for a BFT protocol, e.g., network
size. These input parameters help scope the class of BFT protocols that can be supported to best ï¬t each deployment
environment.

E 1: Number of replicas. Our ï¬rst dimension concerns selecting BFT protocols based on the number of replicas (i.e.,
network and quorum size) used in a deployment. In the presence of ğ‘“ malicious failures, BFT protocols require at
least 3ğ‘“ +1 replicas to guarantee safety [56, 57, 86, 103, 156]. Using trusted hardware, however, the malicious behavior
of replicas can be restricted. Hence, 2ğ‘“ + 1 replicas are suï¬ƒcient to guarantee safety [79, 85, 87, 200, 223, 223, 224].
Prior proposals on reducing the required number of replicas to 2ğ‘“ + 1 [14â€“16] involve either leveraging new hardware
capabilities or using message-and-memory models. Increasing the number of replicas to 5ğ‘“ + 1 [174] (its proven lower
bound, 5ğ‘“ âˆ’ 1 [11, 147]) or 7ğ‘“ + 1 [212], on the other hand, reduces the number of communication phases. A BFT
protocol might also optimistically assume the existence of a quorum of 2ğ‘“ + 1 active non-faulty replicas to establish
consensus [96, 133]. Using both trusted hardware and active/passive replication, the quorum size is further reduced to
ğ‘“ + 1 during failure-free situations [96, 97, 133].
E 2: Communication topology. BEDROCK allows users to analyze BFT protocols based on communication topologies
including: (1) the star topology where communication is strictly from a designated replica, e.g., the leader, to all other
replicas and vice-versa, resulting in linear message complexity [143,230], (2) the clique topology where all (or a subset
of) replicas communicate directly with each other resulting in quadratic message complexity [71], (3) the tree topology
where the replicas are organized in a tree with the leader placed at the root, and at each phase, a replica communicates
with either its child replicas or its parent replica causing logarithmic message complexity [140, 141, 186], or (4) the
chain topology where replicas construct a pipeline and each replica communicates with its successor replica (constant
message complexity) [35].

E 3: Authentication. Participants authenticate their messages to enable other replicas to verify a messageâ€™s origin.
BEDROCK support both signatures, e.g., RSA [202] and authenticators [71], i.e., vectors of message authentication
codes (MACs) [219]. Constant-sized threshold signatures [67, 208] have also been used to reduce the size of a set
(quorum) of signatures. Signatures are typically more costly than MACs. However, in contrast to MACs, signatures
provide non-repudiation and are not vulnerable to MAC-based attacks from malicious clients. A BFT protocol might
even use diï¬€erent techniques (i.e., signatures and MACs) in diï¬€erent stages to authenticate messages sent by clients,
by replicas in the ordering stage, and by replicas during view-change.

E 4: Responsiveness, Synchronization and Timers. A BFT protocol is responsive if its normal case commit latency
depends only on the actual network delay needed for replicas to process and exchange messages rather than any (usually
much larger) predeï¬ned upper bound on message transmission delay [34, 192, 193, 209]. Responsiveness might be

4

sacriï¬ced in diï¬€erent ways. First, when the rotating leader mechanism is used, the new leader might need to wait for
a predeï¬ned time before initiating the next request to ensure that it receives the decided value from all non-faulty but
slow replicas, e.g., Tendermint [148] and Casper [65]. Second, optimistically assuming all replicas are non-faulty,
replicas (or clients) need to wait for a predeï¬ned upper bound to receive messages from all replicas, e.g., SBFT [120]
and Zyzzyva [143].

BFT protocols need to guarantee that all non-faulty replicas will eventually be synchronized to the same view with a
non-faulty leader enabling the leader to collect the decided values in previous views and making progress in the new
view [59, 182, 183]. This is needed because a quorum of 2ğ‘“ + 1 replicas might include ğ‘“ Byzantine replicas and
the remaining ğ‘“ "slow" non-faulty replicas might stay behind (i.e., in-dark) and may not even advance views at all.
View synchronization can be achieved in diï¬€erent ways such as integrating the functionality with the core consensus
protocol, e.g., PBFT [71], or assigning a distinct synchronizer component, e.g., Pacemaker in HotStuï¬€ [230], and
hardware clocks [5].

Depending on the environment, network characteristics, and processing strategy, BFT protocols use diï¬€erent timers
to ensure responsiveness and synchronization. Protocols can be conï¬gured with the following timers by BEDROCK to
achieve these goals.

. Waiting for reply messages, e.g., Zyzzyva [143],
. Triggering (consecutive) view-change, e.g., PBFT [71],
. Detecting backup failures, e.g., SBFT [120],
. Quorum construction in an ordering phase, e.g., prevote and precommit timeouts in Tendermint [62],
. Synchronization for view change, e.g., Tendermint [62],
. Finishing a (preordering) round, e.g., Themis [135],
. Performance check (heartbeat timer), e.g., Aardvark [81],
. Atomic recovery (watchdog timer) to periodically hand control to a recovery monitor [69], e.g., PBFT [70].

ğœ1
ğœ2
ğœ3
ğœ4
ğœ5
ğœ6
ğœ7
ğœ8

3.2 Protocol Structure

Our next family of dimensions concerns customization of the protocol structure by BEDROCK, which will further deï¬ne
the class of protocols permitted.

P 1: Commitment strategy. BEDROCK supports BFT protocols that process transactions in either an optimistic, pess-
imistic, or robust manner. Optimistic BFT protocols make optimistic assumptions on failures, synchrony, or data
contention and might execute requests without necessarily establishing consensus. An optimistic BFT protocol might
make a subset of the following assumptions:

ğ‘1
ğ‘2
ğ‘3
ğ‘4
ğ‘5
ğ‘6

. The leader is non-faulty, assigns a correct order to requests and sends it to all backups, e.g., Zyzzyva [143],

. The backups are non-faulty and actively and honestly participate in the protocol, e.g., CheapBFT [133],

. All non-leaf replicas in a tree topology are non-faulty, e.g., Kauri [186],

. The workload is conï¬‚ict-free and concurrent requests update disjoint sets of data objects, e.g., Q/U [4],

. The clients are honest, e.g., Quorum [35], and

. The network is synchronous (in a time window), and messages are not lost or highly delayed, e.g., Tendermint

[62].

Optimistic protocols are classiï¬ed into speculative and non-speculative protocols. In non-speculative protocols, e,g.,
SBFT [120] and CheapBFT [133], replicas execute a transaction only if the optimistic assumption holds. Speculative
protocols, e.g., Zyzzyva [143] and PoE [123], on the other hand, optimistically execute transactions. If the assumption is
not fulï¬lled, replicas might have to rollback the executed transactions. Optimistic BFT protocols improve performance
in fault-free situations. If the assumption does not hold, the replicas, e.g., SBFT [120], or clients, e.g., Zyzzyva [143],
detect the failure and use the fallback protocol.

Pessimistic BFT protocols, on the other hand, tolerate the maximum number of possible concurrent failures ğ‘“ without
making any assumptions on failures, synchrony, or data contention. In pessimistic BFT protocols, replicas communicate
to agree on the order of requests. Finally, robust protocols, e.g., Prime [22], Aardvark [81], R-Aliph [35], Spinning [222]
and RBFT [36], go one step further and consider scenarios where the system is under attack.

5

In summary, BFT protocols demonstrate diï¬€erent performance in failure-free, low failure, and under attack situations.
Optimistic protocols deliver superior performance in failure-free situations. However, in the presence of failure, their
performance is signiï¬cantly reduced especially when the system is under attack. On the other hand, pessimistic proto-
cols provide high performance in failure-free situations and are able to handle low failures with acceptable overhead.
However, they show poor performance when the system is under attack. Finally, robust protocols are designed for
under-attack situations and demonstrate moderate performance in all three situations.

P 2: Number of commitment phases. The number of commitment (ordering) phases or good-case latency [11] of a
BFT SMR protocol is the number of phases needed for all non-faulty replicas to commit when the leader is non-faulty,
and the network is synchronous. We consider the number of commitment phases from the ï¬rst time a replica (typically
the leader) receives a request to the ï¬rst time any participant (i.e., leader, backups, client) learns the commitment of
the request, e.g., PBFT executes in 3 phases.

P 3: View-change. BFT protocols follow either the stable leader or the rotating leader mechanism to replace the
current leader. The stable leader mechanism [71, 120, 143, 174] replaces the leader when the leader is suspected to
be faulty by other replicas. In the rotating leader mechanism [19, 64, 72â€“74, 81, 114, 127, 141, 148, 222, 223, 230], the
leader is replaced periodically, e.g., after a single attempt, insuï¬ƒcient performance, or an epoch (multiple requests).

Using the stable leader mechanism, the view-change stage becomes more complex. However, the routine is only ex-
ecuted when the leader is suspected to be faulty. On the other hand, the rotating leader mechanism requires ensuring
view synchronization frequently (whenever the leader is rotated). Rotating the leader has several beneï¬ts such as balan-
cing load across replicas [44,45,222], improving resilience against slow replicas [81], and minimizing communication
delays between clients and the leader [104, 173, 223].

P 4: Checkpointing. The checkpointing mechanism is used to ï¬rst, garbage-collect data of completed consensus
instances to save space and second, restore in-dark replicas (due to network unreliability or leader maliciousness) to
ensure all non-faulty replicas are up-to-date [71, 95, 123]. The checkpointing stage typically is initiated after a ï¬xed
checkpoint window in a decentralized manner without relying on a leader [71].

P 5: Recovery. When there are more than ğ‘“ failures, BFT protocols, apart from some exceptions [79,164], completely
fail and do not give any guarantees on their behavior [95]. BFT protocols perform recovery using reactive or proactive
mechanisms (or a combination [214]). Reactive recovery mechanisms detect faulty replica behavior [126] and recover
the replica by applying software rejuvenation techniques [90,130] where the replica reboots, reestablishes its connection
with other replicas and clients, and updates its state. On the other hand, proactive recovery mechanisms recover replicas
in periodic time intervals. Proactive mechanisms do not require any fault detection techniques, however, they might
unnecessarily recover non-faulty replicas [95]. During recovery, a replica is unavailable. A BFT protocol can rely on
3ğ‘“ + 2ğ‘˜ + 1 replicas to improve resilience and availability during recovery where ğ‘˜ is the maximum number of servers
that rejuvenate concurrently [214]. To prevent attackers from disrupting the recovery process, each replica requires a
trusted component, e.g., secure coprocessor [70], a synchronous wormhole [221] or a virtualization layer [97,200], that
remains operational even if the attacker controls the replica and a read-only memory that an attacker cannot manipulate.
The memory content remains persistent (e.g., on disk) across machine reboots and includes all information needed for
bootstrapping a correct replica after restart [95].

P 6: Types of Clients. BEDROCK supports three types of clients: requester, proposer, and repairer. Requester clients
perform a basic functionality and communicate with replicas by sending requests and receiving replies. A requester
client might need to verify the results by waiting for a number of matching replies, e.g., ğ‘“ + 1 in PBFT [71], 2ğ‘“ + 1
in PoE [123] and PBFT [71] (for read-only requests) , or 3ğ‘“ + 1 is Zyzzyva [143]. Using trusted components, e.g.,
Troxy [161], or threshold signatures, e.g., SBFT [120], the client does not even need to wait for and verify multiple
results from replicas. Clients might also play the proposer role by proposing a sequence number (acting as the leader)
for its request [4, 119, 170, 172]. Repairer clients, on the other hand, detect the failure of replicas, e.g., Zyzzyva [143],
or even change the protocol conï¬guration, e.g., Scrooge [206], Abstract [35], and Q/U [4].

3.3 Quality of Service

There are some optional QoS features that BEDROCK can analyze. We list two example dimensions.

, then ğ‘¡1

Q 1: Order fairness. Order-fairness deals with preventing adversarial manipulation of request ordering [40, 135, 136,
145, 146, 233]. Order-fairness is deï¬ned as: "if a large number of replicas receives a request ğ‘¡1
before another request
" [136]. Order fairness has been partially addressed using diï¬€erent techniques:
ğ‘¡2
(1) monitoring the leader to ensure it does not initiate two new requests from the same client before initiating an old
request of another client, e.g., Aardvark [81], (2) adding a preordering phase, e.g., Prime [22], where replicas order
the received requests locally and share their own ordering with each other, (3) encrypting requests and revealing the

should be ordered before ğ‘¡2

6

contents only once their ordering is ï¬xed [33, 66, 177, 216], (4) reputation-based systems [33, 93, 142, 159] to detect
unfair censorship of speciï¬c client requests, and (5) providing opportunities for every replica to propose and commit
its requests using fair election [8, 33, 114, 137, 159, 192, 228].

Q 2: Load balancing. The performance of fault-tolerant protocols is usually limited by the computing and bandwidth
capacity of the leader [17, 53, 77, 179, 180, 186, 226]. The leader coordinates the consensus protocol and multic-
asts/collects messages to all other replicas in diï¬€erent protocol phases. Load balancing is deï¬ned as distributing the
load among the replicas of the system to balance the number of messages any single replica has to process.

Load balancing can be partially achieved using the rotating leader mechanism, multi-layer, or multi-leader BFT proto-
cols. When the rotating leader mechanism is used, one (leader) replica still is highly loaded in each consensus instance.
In multi-layer BFT protocols [23,125,165,185,187] the load of the leader is distributed between the leaders of diï¬€erent
clusters. However, the system still suï¬€ers from load imbalance between the leader and backup replicas in each cluster.
In multi-leader protocols [20, 31, 37, 124, 215, 225], all replicas can initiate consensus to partially order requests in
parallel. However, slow replicas still aï¬€ect the global ordering of requests.

3.4 Performance Optimization Dimensions

Finally, we present a set of optimization dimensions that target the performance of a BFT protocol.

O 1: Out-of-order processing. The out-of-order processing mechanism enables the leader to continuously propose
new requests even when previous requests are still being processed by the backups [123]. Out-of-order processing of
requests is possible if the leader does not need to include any certiï¬cate or hash of the previous request (block) in its
next request.

O 2: Request pipelining. Using request pipelining, the messages of a new consensus instance are piggybacked on
the second round messages of the previous instance [186, 230]. This technique is especially eï¬ƒcient when a protocol
rotates the leader after every consensus instance.

O 3: Parallel ordering. Client requests can be ordered in parallel by relying on a set of independent ordering groups
[44, 45, 162] where each group orders a subset of client requests and then all results are deterministically merged into
the ï¬nal order. Similarly, in multi-leader protocols [20, 31, 32, 37, 105, 124, 162, 178, 215, 225, 225], diï¬€erent replicas
are designated as the leader for diï¬€erent consensus instances in parallel and then a global order is determined.

O 4: Parallel execution. Transactions can be executed in parallel to improve the systemâ€™s overall performance. One
approach is to detect non-conï¬‚icting transactions and execute them in parallel [25, 108, 144]. This approach requires a
priori knowledge of a transactionâ€™s read-set and write-set. Switching the order of agreement and execution stages and
optimistically executing transactions in parallel is another approach [30, 134]. If the execution results are inconsistent
(due to faulty replicas, conï¬‚icting transactions, or nondeterministic execution), replicas need to rollback their states
and sequentially and deterministically re-execute the requests. switching the order of agreement and execution stages
also enables replicas to detect any nondeterministic execution [30, 134].

O 5: Read-only requests processing. In pessimistic protocols, replicas can directly execute read-only requests without
establishing consensus. However, since replicas may execute the read requests on diï¬€erent states, even non-faulty
replicas might not return identical results. To resolve this, the number of required matching replies for both normal and
read-only requests needs to be increased from ğ‘“ + 1 to 2ğ‘“ + 1 in order to ensure consistency (i.e., quorum intersection
requirement) [70]. This, however, results in a liveness challenge because ğ‘“ non-faulty replicas might be slow (or in-
dark) and not receive the request. As a result, the client might not be able to collect 2ğ‘“ + 1 matching responses (since
Byzantine replicas may not send a correct reply to the client).

O 6: Separating ordering and execution. The ordering and execution stages can be separated and implemented in
diï¬€erent processes. This separation leads to several advantages [95] such as preventing malicious execution replicas
from leaking conï¬dential application state to clients [102,229], enabling large requests to bypass the ordering stage [80],
moving application logic to execution virtual machine [97, 200, 227] or simplifying the parallel ordering of requests
[44, 47]. Moreover, while 3ğ‘“ +1 replicas are needed for ordering, 2ğ‘“ + 1 replicas are suï¬ƒcient to execute transactions
[229].

O 7: Trusted hardware. Using Trusted execution environments (TEEs) such as Intelâ€™s SGX [175], Sanctum [89], and
Keystone [158], the number of required replicas can be lowered to 2ğ‘“ + 1 because the trusted component prevents
a faulty replica from sending conï¬‚icting messages to diï¬€erent replicas without being detected. A trusted component
may include an entire virtualization layer [97, 200, 223], a multicast ordering service executed on a hardened Linux

7

Figure 3: Diï¬€erent stages of PBFT protocol

kernel [84, 85], a centralized conï¬guration service [201], a trusted log [79], a trusted platform module, e.g., counter
[223, 224], a smart card TrInc [160], or an FPGA [96, 133].

O 8: Request/reply dissemination. A client can either multicast its request to all replicas [51, 91, 222] where each
replica relays the request to the leader or optimistically send its request to a contact replica, typically the leader. The
contact replica is known to the client through a reply to an earlier request [71, 143]. If the client timer for the request
(ğœ1
) expires, the client multicasts its request to all replicas. This optimistic mechanism requires fewer messages to be
sent from clients to the replicas. However, this comes at the cost of increased network traï¬ƒc between replicas, because
the leader needs to disseminate the full request to other replicas to enable them to eventually execute it.

On the other hand, all replicas can send the results to clients in their reply messages. This, however, leads to signiï¬cant
network overhead for large results. A protocol can optimistically rely on a designated responder replica (chosen by the
client or servers) to send the full results. Other replicas then either send the hash of the results to the client or send a
signed message to the responder enabling the responder to generate a proof for the results, e.g., SBFT [120]. While
this technique reduces network overhead, the client might not receive the results if the responder replica is faulty, the
network is unreliable, or the responder replica was in-dark and skipped the execution and applied a checkpoint to catch
up [95].

4 Design Choices Landscape

Given a set of speciï¬ed dimension values in Section 3, BEDROCK generates a set of valid protocols that meet a user
query. Each protocol represents a point in the BEDROCK design space. In this section, using the classical PBFT [70, 71]
as a driving example, we demonstrate how diï¬€erent points in the design space lead to diï¬€erent trade-oï¬€s. Each design
choice is a one-to-one function that maps a valid input point (i.e., a BFT protocol) to another valid output point in the
design space. The domain of each function (design choice) is a subset of the valid points in the design space. These
design choices enable BEDROCK to generate valid BFT protocols.

4.1 Background on PBFT

PBFT, as shown in Figure 3, is a leader-based protocol that operates in a succession of conï¬gurations called views
[106, 107]. Each view is coordinated by a stable leader (primary) and the protocol pessimistically processes requests.
In PBFT, the number of replicas, ğ‘›, is assumed to be 3ğ‘“ + 1 and the ordering stage consists of pre-prepare, prepare,
and commit phases. The pre-prepare phase assigns an order to the request, the prepare phase guarantees the uniqueness
of the assigned order and the commit phase guarantees that the next leader can assign order in a safe manner.
During a normal case execution of PBFT, clients send their signed request messages to the leader. In the pre-prepare
phase, the leader assigns a sequence number to the request to determine the execution order of the request and multicasts
a pre-prepare message including the full request to all backups. Upon receiving a valid pre-prepare message from
the leader, each backup replica multicasts a prepare message to all replicas and waits for prepare messages from 2ğ‘“
diï¬€erent replicas (including the replica itself) that match the pre-prepare message. The goal of the pre-prepare phase
is to guarantee safety within the view, i.e., a majority of non-faulty replicas received matching pre-prepare messages
from the leader replica and agree with the order of the request. Each replica then multicasts a commit message to all
replicas. Once a replica receives 2ğ‘“ + 1 valid commit messages from diï¬€erent replicas including itself) that match the
pre-prepare message, it commits the transaction. The goal of the commit phase is to ensure safety across views, i.e., the
request has been replicated on a majority of non-faulty replicas. The second and third phases of PBFT follow the clique
topology, i.e., have îˆ»(ğ‘›2) message complexity. If the replica has executed all requests with lower sequence numbers,
it executes the transaction and sends a reply to the client. The client waits (timer ğœ1
) for ğ‘“ + 1 matching results from
diï¬€erent replicas.

8

), backups exchange
In the view change stage, upon detecting the failure of the leader of view ğ‘£ using timeouts (timer ğœ2
view-change messages including transactions that have been received by the replicas. After receiving 2ğ‘“ + 1 view-
change messages, the designated stable leader of view ğ‘£ + 1 (the replica with ID = ğ‘£ + 1 mod ğ‘›) proposes a new view
message including a list of transactions that should be processed in the new view.

In PBFT replicas periodically generate and send checkpoint messages to all other replicas. If a replica receives 2ğ‘“ + 1
matching checkpoint messages, the checkpoint is stable. PBFT also includes a proactive recovery mechanism that
periodically (timer ğœ8
PBFT uses either signatures [71] or MACs [70] for authentication. Using MACs, replicas need to send view-change-
ack messages to the leader after receiving view-change messages. Since new view messages are not signed, these
view-change-ack enable replicas to verify the authenticity of new view messages.

) rejuvenates replicas one by one.

4.2 Expanding the Design Choices of PBFT

Using the PBFT protocol and our design dimensions as a baseline, we illustrate a series of design choices that expose
diï¬€erent trade-oï¬€s BFT protocols need to make. Each design choice acts as a one-to-one function that changes the
value of one or multiple dimensions to map a BFT protocol, e.g., PBFT, to another BFT protocol.

Design Choice 1: Linearization. This function explores a trade-oï¬€ between communication topology and communic-
ation phases. The function takes a quadratic communication phase, e.g., prepare or commit in PBFT, and split it into
two linear phases: one phase from all replicas to a collector (typically the leader) and one phase from the collector to
all replicas, e.g., SBFT [120], HotStuï¬€ [230]. The output protocol requires (threshold) signatures for authentication.
The collector collects a quorum of (typically ğ‘› âˆ’ ğ‘“ ) signatures from other replicas and broadcasts its message including
the signatures as the certiï¬cate of receiving messages to every replica. Using threshold signatures [66, 67, 199, 208]
the collector message size can be further reduced from linear to constant.

Design Choice 2: Phase reduction through redundancy. This function explores a trade-oï¬€ between the number of
ordering phases and the number of replicas. The function transforms a protocol with 3ğ‘“ + 1 replicas and 3 ordering
phases (i.e., one linear, two quadratic), e.g., PBFT, to a fast protocol with 5ğ‘“ + 1 replicas and 2 ordering phases
(one linear, one quadratic), e.g., FaB [174]. In the second phase of the protocol, matching messages from a quorum
of 4ğ‘“ + 1 replicas are required. Recently, 5ğ‘“ âˆ’ 1 has been proven as the lower bound for two-step (fast) Byzantine
consensus [11,147]. The intuition behind the 5ğ‘“ âˆ’1 lower bound is that in an authenticated model, when replicas detect
leader equivocation and initiate view-change, they do not include view-change messages coming from the malicious
leader reducing the maximum number of faulty messages to ğ‘“ âˆ’ 1 [11, 147].
Design Choice 3: Leader rotation. This function replaces the stable leader mechanism with the rotating leader mech-
anism, e.g., HotStuï¬€ [230] where the rotation happens after each request or epoch or due to low performance. The
function eliminates the view-change stage and adds a new quadratic phase or 2 linear phases (using the linearization
function) to the ordering stage to ensure that the new leader is aware of the correct state of the system.

Design Choice 4: Non-responsive leader rotation. This function replaces the stable leader mechanism with the rotat-
ing leader mechanism without adding a new ordering phase (in contrast to design choice 3) while sacriï¬cing respons-
iveness. The new leader optimistically assumes that the network is synchronous and waits for a predeï¬ned known
upper bound Î” (Timer ğœ4
) before initiating the next request. This is needed to ensure that the new leader is aware of
the highest assigned order to the requests, e.g., Tendermint [63, 148] and Casper [65].

Design Choice 5: Optimistic replica reduction. This function reduces the number of involved replicas in consensus
from 3ğ‘“ + 1 to 2ğ‘“ + 1 while optimistically assuming all 2ğ‘“ + 1 replicas are non-faulty (assumption ğ‘2
). In each phase
of a BFT protocol, matching messages from a quorum of 2ğ‘“ + 1 replicas is needed. If a quorum of 2ğ‘“ + 1 non-faulty
replicas is identiï¬ed, they can order (and execute) requests without the participation of the remaining ğ‘“ replicas. Those
ğ‘“ replicas remain passive and are needed if any of the 2ğ‘“ + 1 active replicas become faulty [96, 133]. Note that ğ‘› is
still 3ğ‘“ + 1.
Design Choice 6: Optimistic phase reduction. Given a linear BFT protocol, this function optimistically eliminates
two linear phases (i.e., equal to the quadratic phase prepare) assuming all replicas are non-faulty, e.g., SBFT [120].
The leader (collector) waits for signed messages from all 3ğ‘“ + 1 replicas in the second phase of ordering, combines
signatures and sends a signed message to all replicas. Upon receiving the signed message from the leader, each replica
ensures that all non-faulty replicas has received the request and agreed with the order. As a result, the third phase of
communication can be omitted and replicas can directly commit the request. If the leader has not received 3ğ‘“ + 1
), the protocol fall backs to its slow path and runs the third phase of ordering.
messages after a predeï¬ned time (timer ğœ3

9

Table 1: Comparing selected BFT protocols based on diï¬€erent dimensions of BEDROCK design space

Protocol

PBFT [71]

Zyzzyva [143]

Zyzzyva5 [143]

PoE [123]

SBFT [120]

HotStuï¬€ [230]

Tendermint [63]

Themis [135]

Kauri [186]

CheapBFT [133]

FaB [174]

Prime [22]

Q/U [4]

FLB

FTB

E1.
Replicas

E2.
Topo.

E3.
Auth.

E4.
Timers

3ğ‘“ + 1

3ğ‘“ + 1

5ğ‘“ + 1

3ğ‘“ + 1

3ğ‘“ + 1

3ğ‘“ + 1

3ğ‘“ + 1

4ğ‘“ + 1

3ğ‘“ + 1

2ğ‘“ + 1

5ğ‘“ + 1

3ğ‘“ + 1

5ğ‘“ + 1

5ğ‘“ âˆ’ 1

clique

MAC || Sign

star

star

star

star

star

clique

star

tree

clique

clique

clique

star

clique

MAC || Sign

MAC || Sign

MAC || T-Sign

T-Sign

T-Sign

Sign

T-Sign

T-Sign

MAC

(Sign)

Sign

MAC

Sign

ğœ1

ğœ1

ğœ1

, ğœ2
, ğœ8
, ğœ2
ğœ1
, ğœ2
, ğœ2
ğœ1
, ğœ2
, ğœ3
, ğœ2
ğœ1
, ğœ5
, ğœ2
, ğœ6
, ğœ2
, ğœ6
ğœ1
, ğœ2
ğœ1
, ğœ2
, ğœ2
, ğœ6
, ğœ2
, ğœ2
, ğœ2

ğœ1
, ğœ2
ğœ1

, ğœ7

ğœ1

ğœ1

ğœ1

ğœ1

P1.
Strategy

pessimistic

optimis: ğ‘1

, ğ‘2

|Spec

optimis: ğ‘1
optimis: ğ‘2

|Spec

|Spec

optimis: ğ‘2
pessimistic

optimis: ğ‘6
pessimistic

optimis: ğ‘3
optimis: ğ‘2
pessimistic

robust

optimis: ğ‘4

, ğ‘5
pessimistic

P2.
Phases

P3.
V-change

3

1 (3)

1 (3)

3

3 (5)

7

3

1 + 7

7â„

3

2

6

1 (3)

2

stable

stable

stable

stable

stable

rotating

rotating

rotating

stable*

stable

stable

stable

stable

stable

P5.
Recov.

pro.

-

-

-

-

-

-

-

-

-

-

-

-

-

P6.
Client

Q1.
Fair.

Q2.
Load.

Design
Choices

Req.

Rep.

Rep.

Req.

Req.

Req.

Req.

Req.

Req.

Req.

Req.

Req.

Rep.

Req.

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â– 

â–¡

â–¡

â–¡

â—ª

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â–¡

â– 

â–¡

â–¡

â–¡

â–¡

â–¡

(11)

8, (11)

8, 10, (11)

1, 7, 11

1, 6, 11

1, 3, 11

4, 11

1, 3, 13, 11

(3), 14, 11

5

2

11, 12

9, 10

1, 2, 11

5ğ‘“ âˆ’ 1

optimis: ğ‘3
Hint: T-Sign is used for threshold signatures. Speculative optimistic protocols are speciï¬ed by Spec. The number of phases in slow-path of optimistic protocols is
shown within parenthesis. While Kauri is implemented on top of HotStuï¬€, it does not use the rotating leader mechanism. Prime provides partial fairness.

T-Sign

stable

Req.

tree

3â„

ğœ1

â–¡

â– 

-

1, 2, 14, 11

Design Choice 7: Speculative phase reduction. This function, similar to the previous one, optimistically eliminates
two linear phases of the ordering stage assuming that non-faulty replicas construct the quorum of responses, e.g.,
PoE [123]. Th main diï¬€erence is that the leader waits for signed messages from only 2ğ‘“ + 1 replicas in the second
phase of ordering and sends a signed message to all replicas. Upon receiving a message signed by 2ğ‘“ + 1 replicas
from the leader, each replica speculatively executes the transaction, optimistically assuming that either (1) all 2ğ‘“ + 1
signatures are from non-faulty replicas or (2) at least ğ‘“ + 1 non-faulty replicas received the signed message from the
leader. If (1) does not hold, other replicas receive and execute transaction during the view-change. However, if (2)
does not hold, the replica might have to rollback the executed transaction.

Design Choice 8: Speculative execution. This function eliminates the prepare and commit phases while optimistically
assuming that all replicas are non-faulty (optimistic assumptions ğ‘1
), e.g., Zyzzyva [143]. Replicas speculatively
execute transactions upon receiving them from the leader. If the client does not receive 3ğ‘“ + 1 matching replies after
a predeï¬ned time (timer ğœ1
) or it receives conï¬‚icting messages, the client detects failures (repairer) and communicates
with replicas to receive 2ğ‘“ + 1 commit messages (two linear phases).

and ğ‘2

Design Choice 9: Optimistic conï¬‚ict-free. Assuming that requests of diï¬€erent clients are conï¬‚ict-free (assumption
), there is no need for a total order among all transactions. The function eliminates all three ordering phases while
ğ‘4
optimistically assuming that requests are conï¬‚ict-free and all replicas are non-faulty. The client becomes the proposer
and sends its request to all (or a quorum of) replicas where replicas execute the client requests without any further
communication [4, 91].

Design Choice 10: Resilience. This function increases the number of replicas by 2ğ‘“ to enable the protocol to tolerate ğ‘“
more failure with the same safety guarantees. In particular, optimistic BFT protocols that assume all 3ğ‘“ + 1 replicas are
non-faulty (quorum size is also 3ğ‘“ + 1) tolerate zero failures. By increasing the number of replicas to 5ğ‘“ + 1 replicas,
such BFT protocols can provide the same safety guarantees with quorums of size 4ğ‘“ + 1 while tolerating ğ‘“ failures,
e.g., Zyzzyva5 [143], Q/U [4]. Similarly, a protocol with 5ğ‘“ + 1 can tolerates ğ‘“ more faulty replicas by increasing the
network size to 7ğ‘“ + 1 [212].
The function can also be used to provide high availability during the (proactive) recovery stage by increasing the number
of replicas by 2ğ‘˜ (the quorum size by ğ‘˜) where ğ‘˜ is the maximum number of servers that recover in parallel [214].
Design Choice 11: Authentication. This function replaces MACs with signatures for a given stage of a protocol. If a
protocol follows the star communication topology where a replica needs to include a quorum of signatures as a proof
of its messages, e.g., HotStuï¬€ [230], ğ‘˜ signatures can be replaced with a single threshold signature. Note that in such
a situation MACs cannot be used (MACs do not provide non-repudiation).

10

Figure 4: Overview of BFT protocols

Design Choice 12: Robust. This function makes a pessimistic protocol robust by adding a preordering stage to the
protocol, e.g., Prime [22]. In the preordering stage and, upon receiving a request, each replica locally orders and
broadcasts the request to all other replica. All replicas then acknowledge the reception of the request in an all-to-all
communication phase and add the request to their local request vector. Replicas periodically share their vectors with
each other. The robust function provides (partial) fairness as well. Note that robustness has also been addressed in
other ways, e.g., using the leader rotation and a blacklisting mechanism in Spinning [222] or isolating the incoming
traï¬ƒc of diï¬€erent replicas, and check the performance of the leader in Aardvark [81].

Design Choice 13: Fair. This function transforms an unfair protocol, e.g., PBFT, to a fair protocol by adding a pre-
ordering phase to the protocol. In the preordering phase, clients send transactions to all replicas and once a round ends
), all replicas send a batch of requests in the order received1 to the leader. The leader then initiates consensus
(timer ğœ5
on the requests following the order of transactions in received blocks. Depending on the order-fairness parameter ğ›¾
(0.5 < ğ›¾ â‰¤ 1) that deï¬nes the fraction of replicas receiving the transactions in that speciï¬c order, at least 4ğ‘“ +1 replicas
(ğ‘› > 4ğ‘“
2ğ›¾âˆ’1

) replicas are needed to provide order fairness [135, 136] 2.

Design Choice 14: LoadBalancer. This function explores a trade-oï¬€ between communication topology and load
balancing where load balancing is supported by organizing replicas in a tree topology, with the leader placed at the
root, e.g., Kauri [186]. The function takes a linear communication phase, and splits it into â„ communication phases
where â„ is the height of the tree. Each replica then uniformly communicates with its child/parent replicas in the
).
tree. Using the tree topology, the protocol optimistically assumes all non-leaf replicas are non faulty (assumption ğ‘3
Otherwise the tree needs to be reconï¬gured (i.e., view change).

5 Deriving, Evolving and Inventing Protocols

This section demonstrate how BEDROCK is used to derive a wide range of BFT protocols using design choices. Figure 5
demonstrates the derivation of a wide spectrum of classical and recent BFT protocols from PBFT using design choices.
Table 1 provides insights into how each BFT protocol maps into the BEDROCK design space. The table also presents
the design choices used by each BFT protocol.

5.1 Case Studies on Protocol Evolution

In the following case studies, we provide insights into how each BFT protocol maps into the BEDROCK design space,
and relate to one another through using design choices. For illustrative purposes, we describe each protocol relative
to PBFT along one or more design choices. Figure 4 focuses on diï¬€erent stages of replicas and demonstrates the
communication complexity of each stage. The ï¬gure presents: (1) the preordering phases used in Themis and Prime,
), (3) the execution
(2) the three ordering phases, e.g., pre-prepare, prepare or commit in PBFT (labeled by ğ‘œ1

, and ğ‘œ3

, ğ‘œ2

1The request propagation time can be estimated by measuring network latency or relying on client timestamps for non-Byzantine

clients

2Order fairness can be provided using 3ğ‘“ + 1 replicas, however, as shown in [135], it either requires a synchronized clock [233]

or does not provide censorship resistance [145].

11

Zyzzyva5 [143]

10. Resilience

Zyzzyva [143]

Tendermint [63]

PoE [123]

SBFT [120]

Kauri [186]

Q/U [4]

10. Resilience

8. Speculative
execution

4. Non-responsive
leader rotation

7. Speculative phase
reduction

6. Optimistic
phase reduction

14. Load
Balancer

Quorum [35]

5. Robust

9. Optimistic
conï¬‚ict-free

PBFT [70]

1. Linearization

Linear PBFT

3. Leader
Rotation

HotStuï¬€ [230]

Prime [22]

5. Optimistic
replica reduction

2. Phase
reduction

2. Phase
reduction

13. Fair

CheapBFT [133]

Bosco [212]

10. Resilience

FaB [174]

1. Linearization

FLB

14. Load
Balancer

FTB

Themis [135]

Figure 5: Derivation of protocols from PBFT using design choices

Figure 6: Zyzzyva

Figure 7: Zyzzyva (slow)

Figure 8: Zyzzyva5

Figure 9: Zyzzyva5 (slow)

stage, (4) the view-change stages consisting of view-change and new-view phases (labeled by ğ‘£1
), and (5) the
checkpointing stage. As can be seen, some protocols do not have all three ordering phases, i.e., using diï¬€erent design
choices, the number of ordering phases is reduced. The dashed boxes present the slow-path of protocols, e.g., the third
ordering phase of SBFT is used only in its slow-path. Finally, the order of stages might be changed. For example,
HotStuï¬€ runs view-change (leader rotation) for every single message and this leader rotation phase takes place at the
beginning of a consensus instance to synchronize nodes within a view.

and ğ‘£2

Next, we describe each BFT protocol.
Zyzzyva [143]. Zyzzyva3 (Figure 6) can be derived from PBFT using the speculative execution function (design choice
8) of BEDROCK where assuming the primary and all backups are non-faulty, replicas speculatively execute requests
without running any agreement and send reply messages to the client. The client waits for 3ğ‘“ + 1 matching replies
to accept the results. If the timer ğœ1
is expired and the client received matching replies from between 2ğ‘“ + 1 and 3ğ‘“
replicas, as presented in Figure 7, two more linear rounds of communication is needed to ensure that at least 2ğ‘“ + 1
replicas have committed the request. Finally, Zyzzyva5 is derived from Zyzzyva by using the resilience function (design
choice 10) where the number of replicas is increased to 5ğ‘“ + 1 and the protocol is able to tolerate ğ‘“ and 2ğ‘“ failures
during its fast and slow path respectively (presented in (Figures 8 and 9) AZyzzyva [35, 119] also uses the fast path of
Zyzzyva (called ZLight) in its fault-free situations.

PoE [123]. PoE Figure 12 uses the linearization and speculative phase reduction functions (design choices 1 and 7).
PoE does not assumes that all replicas are non-faulty and constructs quorum of 2ğ‘“ + 1 replicas possibly including
Byzantine replicas. However, since a client waits for 2ğ‘“ + 1 matching reply messages, all 2ğ‘“ + 1 replicas constructing
the quorum need to be well-behaving to guarantee client liveness in the fast path.
SBFT [120]. BEDROCK derives SBFT4 from PBFT using the linearization and optimistic phase reduction functions
(design choices 1 and 6). SBFT presents an optimistic fast path (Figure 13) assuming all replicas are non-faulty. If
the primary does not receive messages from all backups (in the prepare phase) and its timer is expired (i.e., non-
responsiveness timer ğœ3
), SBFT switches to its slow path (Figure 14) and requires two more linear rounds of commu-
nication (commit phase). The Twin-path nature of SBFT requires replicas to sign each message with two schemes (i.e.,

3the view-change stage of the Zyzzyva protocol has a safety violation as described in [6]
4SBFT tolerates both crash and Byzantine failure (ğ‘› = 3ğ‘“ + 2ğ‘ + 1 where ğ‘ is the number of crashed replicas).Since the focus

of this paper is on Byzantine failures, we consider a variation of SBFT where ğ‘ = 0.

12

Figure 10: HotStuï¬€

Figure 11: Kauri

Figure 12: PoE

Figure 13: SBFT

Figure 14: SBFT (Linear PBFT)

2ğ‘“ + 1 and 3ğ‘“ + 1). To send replies to the client, a single (collector) replica receives replies from all replicas and sends
a single (threshold) signed reply message.

HotStuï¬€ [230]. HotStuï¬€ (Figure 10) can be derived from PBFT using the linearization and leader rotation functions
(design choices 1 and 3) of BEDROCK. The Chained HotStuï¬€ (performance optimization 2) beneï¬ts from pipelining to
reduce the latency of request processing.
Tendermint [62, 63, 148]. Tendermint5 leverages the non-responsive leader rotation function (design choice 4) to
rotate leader without adding any new phase. The new leader, however, needs to wait for a predeï¬ned time (timer ğœ4
),
i.e., the worst-case time it takes to propagate messages over a wide-area peer-to-peer gossip network, before proposing
a new block. Tendermint also uses timers in all phases where a replica discard the request if it does not receive 2ğ‘“ + 1
messages before the timeout (timer ğœ6
). Note that the original Tendermint uses a gossip all-to-all mechanism and has
îˆ»(ğ‘› log ğ‘›) message complexity.
Themis [135]. Themis is derived from HotStuï¬€ using the fair function (design choice 13). Themis add a new all-to-all
preordering phase where replicas send a batch of requests in the order they received to the leader replica and the leader
propose requests in the order received (depending on the order-fairness parameter ğ›¾) [136]. Themis requires at least
4ğ‘“ + 1 replicas (if ğ›¾ = 1) to provide order fairness.
Kauri [186]. Kauri (Figure 11) can be derived from HotStuï¬€ using the loadbalancer function (design choice 14) that
maps the star topology to the tree topology. The height of the tree is â„ = logğ‘‘ ğ‘› where ğ‘‘ is the fanout of each replica.
CheapBFT [133]. CheapBFT (Figure 15) and its revised version, REBFT [96] is derived from PBFT using the optim-
istic replica reduction function (design choice 5). Using trusted hardware (performance optimization O7), a variation of
REBFT, called RWMINBFT, processes requests with ğ‘“ + 1 active and ğ‘“ passive replicas in its normal case (optimistic)
execution.
FaB [174]. FaB6 (Figure 16) uses the phase reduction function (design choice 2) to reduce one phase of communication
while requiring 5ğ‘“ + 1 replicas. Fab does not use authentication in its ordering stage, however, requires signatures for
the view-change stage (design choice 11). Note that using authentication, 5ğ‘“ âˆ’ 1 replicas is suï¬ƒcient to reduce one
phase of communication [11, 147].

Prime [22]. Prime is derived from PBFT using the robust functions (design choice 12). In prime a preordering stage
is added where replicas exchange the requests they receive from clients and periodically share a vector of all received
requests, which they expect the leader to order request following those vectors. In this way, replicas can also monitor
the leader to order requests in a fair manner.

5Tendermint uses a Proof-of-Stake variation of PBFT where each replica has a voting power equal to its stake (i.e., locked coins).
6FaB similar to a family of Paxos-like protocol separates proposers from acceptors. In our implementation of FaB, however,

replicas act as both proposers and acceptors.

13

Figure 15: CheapBFT

Figure 16: FaB

Figure 17: Q/U

Figure 18: Q/U (Normal)

Figure 19: FLB

Figure 20: FTB

Q/U [4]. Q/U (Figure 17) utilizes optimistic conï¬‚ict-free and resilience functions (design choices 9 and 10). Clients
play the proposer role and replicas immediately execute an update request if the object has not been modiï¬ed since the
clientâ€™s last query. Since Q/U is able to tolerate ğ‘“ faulty replicas, a client can optionally communicate with a subset
(4ğ‘“ + 1) of replicas (preferred quorum). The client communicate with additional replicas only if it does not received
reply from all replicas of the preferred quorum (Figure 18). Both signatures (for large ğ‘›) and MACs (for small ğ‘›) can
be used for authentication in Q/U. Quorum [35] uses a similar technique with 3ğ‘“ + 1 replicas, i.e., only the conï¬‚ict-free
function (design choices 9) has been used.

5.2 Deriving Novel BFT Protocols

The previous case studies demonstrate the value of BEDROCK in providing a uniï¬ed platform for analyzing the strengths
and weaknesses of a range of existing BFT protocols. BEDROCKâ€™s utility goes beyond an experimental platform towards
a discovery tool as well. The system provides a systematic way to explore new valid points in the design space and help
BFT researchers uncover novel BFT protocols. We uncover several such new protocols, although not all are necessarily
practical or interesting. For example, simply making a protocol fair by adding the preordering phase of fairness results
in a new protocol. While this is an interesting insight, the resulting protocol may have limited practical impact. We
select as highlights two new BFT protocols (FLB and FTB) that are new and have practical value that we have uncovered
using BEDROCK.

Fast Linear BFT (FLB). FLB is a fast linear BFT protocol that commits transactions in two phases of communication
with linear message complexity. To achieve this, FLB uses the linearization and phase reduction through redundancy
functions (design choices 1 and 2). FLB requires 5ğ‘“ âˆ’ 1 replicas (following the lower bound results on fast Byzantine
agreement [11, 147]). The ordering stage of FLB is similar to the fast path of SBFT in terms of the linearity of
communication and the number of phases. However, FLB expands the network size to tolerate ğ‘“ failures (in contrast
to SBFT, which optimistically assumes all replicas are non-faulty).

Fast Tree-based balanced BFT (FTB). A performance bottleneck of consensus protocols is the computing and band-
width capacity of the leader. While Kauri [186] leverages a tree communication topology (design choice 14) to dis-
tribute the load among all replicas, Kauri requires 7â„ phases of communication to commit each request, where â„ is the
height of the communication tree. FTB reduces the latency of Kauri based on two observations. First, we noticed that
while Kauri is implemented on top of HotStuï¬€, it does not use the leader rotation mechanism. As a result, it does not
need the two linear phases of HotStuï¬€ (2â„ phases in Kauri) that are added for the purpose of leader rotation (design
choice 3). Second, similar to FLB, we can use the phase reduction through redundancy function (design choice 2) to
further reduce 2â„ more phases of communication. FTB establishes agreement with 5ğ‘“ âˆ’ 1 replicas in 3â„ phases. FTB
also uses the pipelining stretch mechanism of Kauri, where the leader continuously initiates consensus instances before
receiving a response from its child nodes for the ï¬rst instance. This is similar to the out-of-order processing mechanism
(performance optimization O 1) used by many BFT protocols.

14

Figure 21: Part of a decision tree constructed by the constraint checker

6

Implementation

BEDROCK enables users, e.g., application developers, to analyze and navigate the BFT landscape by querying their
required BFT protocol characteristics. Using a constraint checker, BEDROCK ï¬nds all plausible points (i.e., BFT pro-
tocols) in the design space that satisfy the input query. Moreover, BEDROCK provides an execution engine that enables
users to implement diï¬€erent BFT protocols.

Constraint checker. The constraint checker uses a decision tree-like algorithm to ï¬nd all plausible points in the design
space where each node of the tree is labeled with a dimension, and outgoing edges represent possible values for that
particular dimension. A user issues a query that indicates, for each dimension, an assigned value (chosen from a preset
of values) that characterizes the requirements of their application. The user can customize some dimensions while
leaving the rest unspeciï¬ed. For example the user might search for a pessimistic BFT protocol with 3f+1 nodes and
linear message complexity. Based on the initial query, the constraint checker begins with the dimensions that the
end-user has speciï¬ed, e.g., processing strategy, the number of replicas, and topology in Figure 21, and then checks
plausible values for unspeciï¬ed dimensions, e.g., since the end-user has chosen the star topology, the constraint checker
chooses signatures for authentication to be able to provide non-repudiation. The leaf nodes of the decision tree specify
the candidate BFT protocols. If some queries do not match any points in the design space (e.g. an impossible protocol
such as a pessimistic linear BFT protocol with 2 communication phases and 3f+1 replicas), the empty set is returned
to the user.

Execution engine. The user receives a list of BFT protocols, e.g., HotStuï¬€ [230] is returned as a pessimistic linear
protocol with 3f+1 nodes. BEDROCK maintains a library including the implementation of common BFT protocols.
When a protocol is chosen by the user, BEDROCK simply returns it if the protocol implementation already exists. Oth-
erwise, the system generates a protocol config ï¬le for the protocol, including all dimensions and the chosen value
for each dimension. The config ï¬le is then used by BEDROCK to implement the stateMachine of the protocol. The
stateMachine includes diï¬€erent states and their transitions for each role (leader, backups, and clients), exchange mes-
sages, quorum conditions, etc. The execution engine takes the stateMachine and a set of related classes that are deï¬ned
outside the stateMachine and handles environmental conditions such as message exchanges, timers, clocks, message
validation, etc. to implement the protocol. Once the protocol implementation is completed, the execution engine initial-
izes replicas and clients (based on the system conï¬g), deploys the stateMachine on nodes, coordinates key exchange,
etc. At this point, the system is ready to run experiments.

BEDROCK is implemented in Java. The modular design of BEDROCK enables a fair and eï¬ƒcient evaluation of BFT pro-
tocols using identical libraries, cryptographic functions, etc. In particular, we use java.security and javax.crypto
libraries for cryptographic operations, the eo-YAML library to facilitate the storage and retrieval of BFT protocol con-
ï¬guration ï¬les, protocol buï¬€ers (com.google.protobuf) to serialize data, and Java streams for parallel processing.

7 Experimental Evaluation

Our evaluation studies the practical impact of the design dimensions and the exposed trade-oï¬€s presented as design
choices on the performance of BFT protocols to reveal the strengths and weaknesses of existing BFT protocols. The
goal is to test the capability of the BEDROCK design space to analyze the performance of diï¬€erent protocols that were
proposed in diverse settings and diï¬€erent contexts under one uniï¬ed framework. We evaluate the performance of BFT
protocols in typical experimental scenarios used for existing BFT protocols and permissioned blockchains, including
(1) varying the number of replicas, (2) under a backup failure, (3) multiple request batch sizes, and (4) a geo-distributed
setup.

All protocols listed in Table 1 are implemented in BEDROCK. Using the platform, we also experimented with many
new protocols resulting from the combination of design choices. Due to space limitations, we present the performance
evaluation of a subset of protocols. In particular, we evaluate PBFT, Zyzzyva, SBFT, FaB, PoE, (Chained) HotStuï¬€,
Kauri, Themis, and two of the more interesting new variants (FLB and FTB). This set of protocols enables us to see
the impact of design choices 1, 2, 3, 6, 7, 8, 10, 11, 13, and 14 (discussed in Section 4). We also use the out-of-
order processing technique (optimization O1) for protocols with a stable leader and the request pipelining technique

15

250

200

150

100

50

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

4

16

PBFT

64

32
Number of replicas
Zyzzyva

SBFT

]
s

m

[

y
c
n
e
t
a
L

120

90

60

30

0

100

4

16

32
Number of replicas

64

100

PoE

FaB

HotStuï¬€

Kuari

Themis

FLB

FTB

Figure 22: Performance with diï¬€erent number of replicas

(optimization O2) for protocols with a rotating leader. In our experiments, Kauri and FTB are deployed on trees of
height 2 and the order-fairness parameter ğ›¾ of Themis is considered to be 1 (i.e., ğ‘› = 4ğ‘“ + 1). Kauri does not use the
rotating leader mechanism (although it was developed as an extension of HotStuï¬€). We use 4 as the base pipelining
stretch for both Kauri and FTB and change it depending on the batch size and deployment setting (local vs. geo-
distributed).

The experiments were conducted on the Amazon EC2 platform. Each VM is a c4.2xlarge instance with 8 vCPUs
and 15GB RAM, Intel Xeon E5-2666 v3 processor clocked at 3.50 GHz. When reporting throughput, we use an
increasing number of client requests until the end-to-end throughput is saturated and state the throughput and latency
just below saturation. The results reï¬‚ect end-to-end measurements from the clients. Clients execute in a closed loop.
We use micro-benchmarks commonly used to evaluate BFT systems, e.g., BFT-SMART. Each experiment is run for
120 seconds (including 30 s warm-up and cool-down). The reported results are the average of ï¬ve runs.

7.1 Fault Tolerance and Scalability

In the ï¬rst set of experiments, we evaluate the throughput and latency of the protocols by increasing the number of
replicas ğ‘› in a failure-free situation. We vary the number of replicas in an experiment from 4 to 100. For some protocols,
the smallest network size might diï¬€er, e.g., FaB requires 5ğ‘“ + 1 = 6 replicas. We use batching with the batch size of
400 (we discuss this choice in later experiments) and a workload with client request/reply payload sizes of 128âˆ•128
byte. Figure 22 reports the results.

Zyzzyva shows the highest throughput among all protocols in small networks due to its optimistic ordering stage that
does not require any communication among replicas (design choice 8). However, as ğ‘› increases, its throughput signi-
ï¬cantly reduces as clients need to wait for reply from all replicas. Increasing the number of replicas also has a large
impact on PBFT and FaB (65% and 63% reduction, respectively) due to their quadratic message complexity.
On the other hand, the throughput of Kauri and FTB is less aï¬€ected (31% and 32% reduction, respectively) by increasing
ğ‘› because of their tree topology (design choice 14) that reduced the bandwidth utilization of each replica. Similarly,
PoE, SBFT and HotStuï¬€ incur less throughput reduction (39%, 55% and 45% respectively) compared to PBFT and
FaB due to their linear message complexity (design choice 1). It should be noted that in BEDROCK, chained HotStuï¬€
has been implemented using the pipelining technique (optimization O 2). Hence, the average latency of requests has
been reduced. In comparison to HotStuï¬€, SBFT has slightly lower throughput in large networks (e.g., 8% lower when
ğ‘› = 100) because the leader waits for messages from all replicas. SBFT, on the other hand, shows higher throughput
compared to HotStuï¬€ in smaller networks (e.g., 12% higher when ğ‘› = 4) due to its fast ordering stage (design choice
6). PoE demonstrates higher throughput compared to both SBFT and HotStuï¬€, especially in larger networks (e.g., 39%
higher than SBFT and 26% higher than HotStuï¬€ when ğ‘› = 100). This is expected because, in PoE, the leader does
not need to wait for messages from all replicas and optimistically combines signatures from 2ğ‘“ + 1 replicas (design
choice 7). Compared to PBFT, while HotStuï¬€ demonstrates better throughput (e.g., 48% higher when ğ‘› = 64), the
latency of PBFT is lower (e.g., 32% lower when ğ‘› = 64). One reason behind the high latency of HotStuï¬€ is its extra
communication round (design choice 3).
Supporting order-fairness (design choice 13) leads to deï¬cient performance of Themis compared to HotStuï¬€ (83%
lower throughput when ğ‘› = 5). In Themis, replicas need to order transactions and send batches of transactions to the
leader, and the leader needs to generate a fair order. As the number of replicas increases, Themis incurs higher latency

16

250

200

150

100

50

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

160

120

80

40

0

]
s

m

[

y
c
n
e
t
a
L

80

60

40

20

1

5

PBFT

10
ğ‘“ value
Zyzzyva

20

0

1

5

SBFT

PoE

FaB

HotStuï¬€

Kuari

10
ğ‘“ value
Themis

20

FLB

FTB

Figure 23: Performance with diï¬€erent ğ‘“ value

]
s
m

[
y
c
n
e
t
a
L

120

90

60

30

0

4

16

32
Number of replicas

64

100

4

16

32
Number of replicas

64

100

PBFT

Zyzzyva

SBFT

PoE

FaB

HotStuï¬€

Kuari

Themis

FLB

FTB

Figure 24: Performance with faulty backups

(the latency increases from 9 ms to 137 as the ğ‘› increases from 5 to 101). One main source of latency is the time
the leader takes to generate the dependency graph and reach a fair order. It should also be noted that in the BEDROCK
implementation of Themis, ZKP has not been used and the leader sends all transaction orderings received from replicas
to all of them in the prepare phase. This might slightly increase the latency. Using design choice 2 and reducing the
number of communication phases results in 41% higher throughput and 46% lower latency of FTB compared to Kauri
in a setting with 99 replicas (100 for Kauri).
Finally, in FLB, by combining design choices 1 and 2 demonstrates high throughput and low latency for large value of
ğ‘› (2.25x throughput and 0.55x latency compared to PBFT). This is expected because ï¬rst, FLB reduces both message
complexity and communication phases, and second, in contrast to SBFT and Zyzzyva, replicas in FLB do not need to
wait for responses from all other replicas.

Figure 22 demonstrates the performance of protocols with diï¬€erent numbers of replicas. However, with the same
number of replicas, diï¬€erent protocols tolerate diï¬€erent numbers of failures. For instance, PBFT requires 3ğ‘“ + 1 and
when ğ‘› = 100 tolerates 33 failures while FaB requires 5ğ‘“ + 1 and tolerates 19 failures with ğ‘› = 100. To compare
protocols based on the maximum number of tolerated failures, we represent the results of the ï¬rst experiments in
Figure 23. When protocols tolerate 20 failures, Themis incurs the highest latency. This is because Themis requires 81
(4ğ‘“ + 1) replicas and deals with the high cost of achieving order-fairness.

7.2 Performance with Faulty Backups

In this set of experiments, we force a backup replica to fail and repeat the ï¬rst set of experiments. Figure 24 reports
the results. Zyzzyva is mostly aï¬€ected by failures (82% lower throughput) as clients need to collect responses from
all replicas. A client waits for Î” = 5ğ‘šğ‘  to receive reply from all replicas and then the protocol switches to its normal
path).

17

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

200

150

100

50

0

]
s

m

[

y
c
n
e
t
a
L

48

36

24

12

200

400
batch size

800

200

400
batch size

800

PBFT

Zyzzyva

SBFT

PoE

FaB HotStuï¬€ Kuari

Themis

FLB

FTB

Figure 25: Impact of request batching

We also run this experiment with and without faulty backups on Zyzzyva5 to validate design choice 10, i.e., tolerating
ğ‘“ faulty replicas by increasing the number of replicas. With a single faulty backup, Zyzzyva5 incurs only 8% lower
throughput when ğ‘› = 6.
Backup failure reduces the throughput of SBFT by 42%. In the fast path of SBFT, all replicas need to participate, and
even when a single replica is faulty, the protocol falls back to its slow path that requires two more phases. Interestingly,
while the throughput of PoE is reduced by 26% in a small network (4 replicas), its throughput is not signiï¬cantly aï¬€ected
in large networks. This is because the chance of the faulty replica (which participates in the quorum construction but
does not send reply messages to the clients) to be a member of the quorum is much higher in small networks.
Faulty backups also aï¬€ect the performance of HotStuï¬€, especially in small networks. This is expected because HotStuï¬€
uses the rotating leader mechanism. When ğ‘› is small, the faulty replica is the leader of more views during the exper-
iments, resulting in reduced performance. HotStuï¬€ demonstrates its best performance when ğ‘› = 31 (still, 36% lower
throughput and 2.7x latency compared to the failure-free scenario). While Themis uses HotStuï¬€ as its ordering stage,
a single faulty backup has less impact on its performance compared to HotStuï¬€ (25% reduction vs. 66% reduction in
throughput). This is because Themis has a larger network size (4ğ‘“ + 1 vs. 3ğ‘“ + 1) that reduces the impact of the faulty
replica. In Kauri and FTB, we force a leaf replica to fail in order to avoid triggering a reconï¬guration. As a result,
the failure of a backup does not signiï¬cantly aï¬€ect their performance (e.g., 3% lower throughput with 31 replicas in
Kauri). Finally, in small networks, FLB demonstrates the best performance as it incurs only 8% throughput reduction.

7.3

Impact of Request Batching

In the next set of experiments, we measure the impact of request batching on the performance of diï¬€erent protocols
implemented in BEDROCK. We consider three scenarios with batch size of 200, 400 and 800. The network includes 16
replicas (17 replicas for Themis, 14 replicas for FLB and FTB) and all replicas are non-faulty. Figure 25 depicts the
results for three batch sizes of 200, 400 and 800.
Increasing the batch size from 200 to 400 requests improves the performance of all protocols. This is expected because,
with larger batch sizes, more transactions can be committed while the number of communication phases and exchanged
messages is the same and the bandwidth and computing resources are not fully utilized yet. Diï¬€erent protocols behave
diï¬€erently when the batch size increases from 400 to 800. First, Kauri and FTB still process a higher number of trans-
actions (42% and 34% higher throughput). This is because Kauri and FTB balance the load and utilize the bandwidth
of all replicas. Second, SBFT and FaB demonstrate similar performance as before; a trade-oï¬€ between smaller con-
sensus quorums and higher cost of signature veriï¬cation and bandwidth utilization. Third, the performance of Themis
decreases (24% lower throughput and 3.16x latency) compared to a batch size of 400 due to two main reasons. First,
the higher cost of signature veriï¬cation and bandwidth utilization, and second, the higher complexity of generating fair
order for a block of 800 transactions (CPU utilization).

7.4

Impact of a Geo-distributed Setup

We measure the performance of diï¬€erent protocols in a wide-area network. Replicas are deployed in 4 diï¬€erent AWS
regions, i.e., Tokyo (TY), Seoul (SU), Virginia (VA), and California (CA) with an average Round-Trip Time (RTT) of

18

120

90

60

30

]
c
e
s
/
s
n
a
r
t
k
[

t
u
p
h
g
u
o
r
h
T

4

16

32
Number of replicas

64

100

]
s
[

y
c
n
e
t
a
L

3

2

1

0

4

16

32
Number of replicas

64

100

PBFT

Zyzzyva

SBFT

PoE

FaB

HotStuï¬€

Kuari

Themis

FLB

FTB

Figure 26: Performance with a geo-distributed setup

TY â‡Œ SU: 33 ms, TY â‡Œ VA: 148 ms, TY â‡Œ CA: 107 ms, SU â‡Œ VA: 175 ms, SU â‡Œ CA: 135 ms, and VA â‡Œ CA: 62 ms.
The clients are also placed in Oregon (OR) with an average RTT of 97, 126, 68 and 22 ms from TY, SU, VA and CA
respectively. We use a batch size of 400 and perform experiments in a failure-free situation. In this experiment, the
pipelining stretch of Kauri and FTB is increased to 6. Figure 26 depicts the results.
Zyzzyva demonstrates the best performance when ğ‘› is small. However, when ğ‘› increases, its performance is signi-
ï¬cantly reduced (87% throughput reduction and 115x latency when ğ‘› increases from 4 to 100). This is because, in
Zyzzyva, clients need to receive reply messages from all replicas. Similarly, SBFT incurs a signiï¬cant reduction in its
performance due to its optimistic assumption that all replicas participate in a timely manner. In both protocols, replicas
(client or leader) wait for Î” = 500 ms to receive responses from all replicas before switching to the normal path.
This reduction can be seen in PBFT as well (84% throughput reduction when ğ‘› increases to 100) due to its quadratic
communication complexity. PoE incurs a smaller throughput reduction (51%) in comparison to Zyzzyva, SBFT, and
PBFT because it does not need to wait for all replicas and it has a linear communication complexity. Increasing the
number of replicas does not signiï¬cantly aï¬€ect the throughput of FTB compared to other protocols (36% throughput
reduction when ğ‘› increases to 99) due to its logarithmic message complexity and pipelining technique.
In HotStuï¬€, the leader of the following view must wait for the previous viewâ€™s decision before initiating its value.
Even though Chained HotStuï¬€ is implemented in BEDROCK, the leader still needs to wait for one communication round
(an RTT). As a result, in contrast to the single datacenter setting where each round takes âˆ¼1 ms, request batches are
proposed on average every âˆ¼190 ms. In this setting, a larger batch size possibly improves the performance of HotStuï¬€.
Similarly, in Themis and FLB, the leader must wait for certiï¬cates from ğ‘› âˆ’ ğ‘“ replicas before initiating consensus
on the next request batch. In Themis, network latency also aï¬€ects achieving order-fairness as replicas might propose
diï¬€erent orders for client requests. This result demonstrates the signiï¬cant impact of the out-of-order processing of
requests (optimization O1) on the performance of the protocol, especially in a wide area network.

7.5 Evaluation Summary

We summarize some of the evaluation results as follows. First, optimistic protocols that require all nodes to participate,
e.g., Zyzzyva and SBFT, do not perform well in large networks, especially when nodes are far apart. In small networks
also, a single faulty node signiï¬cantly reduces the performance of optimistic protocols. Second, the performance
of pessimistic protocols highly depends on the communication topology. While the performance of protocols with
quadratic communication complexity, e.g., PBFT and FaB, is signiï¬cantly reduced by increasing the network size,
the performance of protocols with linear complexity, e.g., HotStuï¬€, and especially logarithmic complexity, e.g., Kauri
and FTB, is less aï¬€ected. Interestingly in small networks, protocols that use the leader rotation mechanism show poor
performance. This is because the chance of the faulty node becoming the leader is relatively high. Third, increasing the
request batch size enhances the performance of all protocols to the point where the bandwidth and computing resources
are fully utilized. However, the load-balancing techniques, e.g., tree topology, enable a protocol to process larger
batches. Finally, in a wide area network, out-of-order processing of transactions signiï¬cantly improves performance.
In such a setting, protocols that require a certiï¬cate of the previous round to start a new round, e.g., HotStuï¬€, show
poor performance even with pipelining techniques.

19

8 Related Work

SMR regulates the deterministic execution of client requests on multiple replicas, such that every non-faulty replica
executes every request in the same order [149, 205]. Several approaches [150, 191, 205] generalize SMR to support
crash failures. CFT protocols [18, 58, 75, 77, 99, 129, 129, 131, 151, 152, 154, 155, 157, 163, 166, 184, 190, 191, 194, 220]
utilize the design trade-oï¬€s between diï¬€erent design dimensions. For instance, Fast Paxos [152] adds ğ‘“ more replicas
to reduce one phase of communication.

Byzantine fault tolerance refers to nodes that behave arbitrarily after the seminal work by Lamport, et al. [156]. BFT
protocols have been analyzed in several surveys and empirical studies [6, 7, 21, 26, 42, 48, 51, 68, 88, 95, 111, 122, 195,
210, 232]. We discuss some of the more relevant studies.

Berger and Reiser [48] present a survey on BFT protocols used in blockchains where the focus is on the scalability tech-
niques. Similarly, a survey on BFT protocols consisting of classical protocols, e.g., PBFT, blockchain protocols, e.g.,
PoW, and hybrid protocols, e.g., OmniLedger [142], and their applications in permissionless blockchains, is conducted
by Bano et al. [42]. Platania et al. [195] classify BFT protocols into client-side and server-side protocols depending on
the clientâ€™s role. The paper compares these two classes of protocols and analyzes their performance and correctness
attacks. Three families of leader-based, leaderless, and robust BFT protocols with a focus on message and time com-
plexities have been analyzed by Zhang et al. [232]. Finally, Distler [95] analyzes BFT protocols along several main
dimensions: architecture, clients, agreement, execution, checkpoint, and recovery. The paper shares several dimensions
with BEDROCK.

A recent line of work [9â€“12] also study good-case latency of BFT protocols. BEDROCK, in contrast to all these survey and
analysis papers, provides a design space, systematically discusses design choices (trade-oï¬€s), and, more importantly,
provides a tool to experimentally analyze BFT protocols.

BFTSim [210] is a simulation environment for BFT protocols that leverages declarative networking system. The paper
also compares a set of representative protocols using the simulator. Abstract [35] presents a framework to design and re-
conï¬gure BFT protocols where each protocol is developed as a sequence of BFT instances. Abstract presents AZyzzyva,
Aliph, and R-Aliph as three BFT protocols. Each protocol itself is a composition of Abstract instances presented to
handle diï¬€erent situations (e.g., fault-free, under attack). For instance, R-Aliph is a composition of Quorum, Chain,
and Aardvark. In contrast to such studies, BEDROCK attempts to develop a uniï¬ed design space for BFT protocols,
enabling end-users to choose a protocol that best ï¬ts their application requirements.

In addition to CFT and BFT protocols, consensus with multiple failure modes has also been studied for both syn-
chronous [138, 176, 211, 218], and partial synchronous [28, 80, 120, 167, 196, 206] models. Finally, leaderless proto-
cols [55, 92, 101, 121, 153, 177, 217] have been proposed to avoid the implications of relying on a leader.

9 Conclusion

We present BEDROCK, a toolkit that uniï¬es all BFT protocols within a platform for analysis, design, implementation,
and experimentation. In using BEDROCK, we demonstrate how diï¬€erent BFT protocols relate to one another within a
design space and evaluate BFT protocols under a uniï¬ed deployment and experimentation environment in a fair and
eï¬ƒcient manner. By providing a uniï¬ed platform for all the diï¬€erent BFT protocols, BEDROCK is able to highlight
the strengths and weaknesses of diverse properties, e.g., optimistic vs. pessimistic. The tool also provides a basis for
discovering protocols not previously proposed.

While this paper focuses on the platform, the ultimate decision process lies with the end-user for selecting and gener-
ating the BFT protocol. As future work, we plan to explore incorporating automatic selection strategies in BEDROCK
based on deployment environment and application requirements. Machine learning techniques may be useful here in
aiding the user in selecting the appropriate BFT protocol, or evolving one protocol to another at runtime as system para-
meters are updated. Furthermore, we will extend BEDROCK by enabling protocol designers to deï¬ne new dimensions
and values systematically.

References

[1] Chain. http://chain.com.

[2] Corda. https://github.com/corda/corda.

[3] Hyperledger iroha. https://github.com/hyperledger/iroha.

20

[4] M. Abd-El-Malek, G. R. Ganger, G. R. Goodson, M. K. Reiter, and J. J. Wylie. Fault-scalable byzantine fault-

tolerant services. Operating Systems Review (OSR), 39(5):59â€“74, 2005.

[5] I. Abraham, S. Devadas, D. Dolev, K. Nayak, and L. Ren. Synchronous byzantine agreement with expected o(1)
rounds, expected ğ‘œ(ğ‘›2) communication, and optimal resilience. In Int. Conf. on Financial Cryptography and
Data Security, pages 320â€“334. Springer, 2019.

[6] I. Abraham, G. Gueta, D. Malkhi, L. Alvisi, R. Kotla, and J.-P. Martin. Revisiting fast practical byzantine fault

tolerance. arXiv preprint arXiv:1712.01367, 2017.

[7] I. Abraham, D. Malkhi, et al. The blockchain consensus layer and bft. Bulletin of EATCS, 3(123), 2017.
[8] I. Abraham, D. Malkhi, K. Nayak, L. Ren, and A. Spiegelman. Solida: A blockchain protocol based on re-
conï¬gurable byzantine consensus. In 21st Int. Conf. on Principles of Distributed Systems (OPODIS). Schloss
Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.

[9] I. Abraham, D. Malkhi, K. Nayak, L. Ren, and M. Yin. Sync hotstuï¬€: Simple and practical synchronous state
machine replication. In 2020 IEEE Symposium on Security and Privacy (SP), pages 106â€“118. IEEE, 2020.
[10] I. Abraham, K. Nayak, L. Ren, and Z. Xiang. Brief announcement: Byzantine agreement, broadcast and state
In Int. Symposium on Distributed Computing (DISC).

machine replication with optimal good-case latency.
Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik, 2020.

[11] I. Abraham, K. Nayak, L. Ren, and Z. Xiang. Good-case latency of byzantine broadcast: a complete categoriz-

ation. In Symposium on Principles of distributed computing (PODC), pages 331â€“341. ACM, 2021.

[12] I. Abraham, L. Ren, and Z. Xiang. Good-case and bad-case latency of unauthenticated byzantine broadcast: A

complete categorization. arXiv preprint arXiv:2109.12454, 2021.

[13] A. Adya, W. J. Bolosky, M. Castro, G. Cermak, R. Chaiken, J. R. Douceur, J. Howell, J. R. Lorch, M. Theimer,
and R. P. Wattenhofer. {FARSITE}: Federated, available, and reliable storage for an incompletely trusted
environment. In 5th Symposium on Operating Systems Design and Implementation (OSDI 02), 2002.

[14] M. K. Aguilera, N. Ben-David, I. Calciu, R. Guerraoui, E. Petrank, and S. Toueg. Passing messages while
sharing memory. In ACM Symposium on Principles of Distributed Computing (PODC), pages 51â€“60, 2018.
[15] M. K. Aguilera, N. Ben-David, R. Guerraoui, V. Marathe, and I. Zablotchi. The impact of rdma on agreement.

In ACM Symposium on Principles of Distributed Computing (PODC), pages 409â€“418, 2019.

[16] M. K. Aguilera, N. Ben-David, R. Guerraoui, D. Papuc, A. Xygkis, and I. Zablotchi. Frugal byzantine computing.

In Int. Symposium on Distributed Computing, 2021.

[17] A. Ailijiang, A. Charapko, and M. Demirbas. Dissecting the performance of strongly-consistent replication

protocols. In SIGMOD Int Conf on Management of Data, pages 1696â€“1710, 2019.

[18] A. Ailijiang, A. Charapko, M. Demirbas, and T. Kosar. Wpaxos: Wide area network ï¬‚exible consensus. IEEE

Transactions on Parallel and Distributed Systems, 31(1):211â€“223, 2019.

[19] A. S. Aiyer, L. Alvisi, A. Clement, M. Dahlin, J.-P. Martin, and C. Porth. Bar fault tolerance for cooperative

services. In ACM symposium on Operating systems principles (SOSP), pages 45â€“58, 2005.

[20] S. Alqahtani and M. Demirbas. Bigbft: A multileader byzantine fault tolerance protocol for high throughput. In

Int Performance Computing and Communications Conf (IPCCC), pages 1â€“10. IEEE, 2021.

[21] S. Alqahtani and M. Demirbas. Bottlenecks in blockchain consensus protocols. In Int Conf. on Omni-Layer

Intelligent Systems (COINS), pages 1â€“8. IEEE, 2021.

[22] Y. Amir, B. Coan, J. Kirsch, and J. Lane. Prime: Byzantine replication under attack. Transactions on Dependable

and Secure Computing, 8(4):564â€“577, 2011.

[23] Y. Amir, C. Danilov, D. Dolev, J. Kirsch, J. Lane, C. Nita-Rotaru, J. Olsen, and D. Zage. Steward: Scaling
byzantine fault-tolerant replication to wide area networks. IEEE Transactions on Dependable and Secure Com-
puting, 7(1):80â€“93, 2008.

[24] M. J. Amiri, D. Agrawal, and A. El Abbadi. Caper: a cross-application permissioned blockchain. Proc. of the

VLDB Endowment, 12(11):1385â€“1398, 2019.

[25] M. J. Amiri, D. Agrawal, and A. El Abbadi. Parblockchain: Leveraging transaction parallelism in permissioned
blockchain systems. In Int. Conf. on Distributed Computing Systems (ICDCS), pages 1337â€“1347. IEEE, 2019.
[26] M. J. Amiri, D. Agrawal, and A. El Abbadi. Modern large-scale data management systems after 40 years of

consensus. In Int. Conf. on Data Engineering (ICDE), page 1. IEEE, 2020.

21

[27] M. J. Amiri, D. Agrawal, and A. El Abbadi. Sharper: Sharding permissioned blockchains over network clusters.

In SIGMOD Int. Conf. on Management of Data, pages 76â€“88. ACM, 2021.

[28] M. J. Amiri, S. Maiyya, D. Agrawal, and A. El Abbadi. Seemore: A fault-tolerant protocol for hybrid cloud

environments. In 36th Int. Conf. on Data Engineering (ICDE), pages 1345â€“1356. IEEE, 2020.

[29] M. J. Amiri, B. Thau Loo, D. Agrawal, and A. El Abbadi. Qanaat: A scalable multi-enterprise permissioned

blockchain system with conï¬dentiality guarantees. Proc. of the VLDB Endowment, 15(11):1, 2022.

[30] E. Androulaki, A. Barger, V. Bortnikov, C. Cachin, et al. Hyperledger fabric: a distributed operating system for

permissioned blockchains. In European Conf. on Computer Systems (EuroSys), page 30. ACM, 2018.

[31] B. Arun, S. Peluso, and B. Ravindran. ezbft: Decentralizing byzantine fault-tolerant state machine replication.

In Int Conf on Distributed Computing Systems (ICDCS), pages 565â€“577. IEEE, 2019.

[32] B. Arun and B. Ravindran. Scalable byzantine fault tolerance via partial decentralization. Proc. of the VLDB

Endowment, 15(9):1739â€“1752, 2022.

[33] A. Asayag, G. Cohen, I. Grayevsky, M. Leshkowitz, O. Rottenstreich, R. Tamari, and D. Yakira. A fair consensus
protocol for transaction ordering. In Int. Conf. on Network Protocols (ICNP), pages 55â€“65. IEEE, 2018.
[34] H. Attiya, C. Dwork, N. Lynch, and L. Stockmeyer. Bounds on the time to reach agreement in the presence of

timing uncertainty. Journal of the ACM (JACM), 41(1):122â€“152, 1994.

[35] P.-L. Aublin, R. Guerraoui, N. KneÅ¾eviÄ‡, V. QuÃ©ma, and M. VukoliÄ‡. The next 700 bft protocols. Transactions

on Computer Systems (TOCS), 32(4):12, 2015.

[36] P.-L. Aublin, S. B. Mokhtar, and V. QuÃ©ma. Rbft: Redundant byzantine fault tolerance. In Int. Conf. on Dis-

tributed Computing Systems (ICDCS), pages 297â€“306. IEEE, 2013.

[37] Z. Avarikioti, L. Heimbach, R. Schmid, L. Vanbever, R. Wattenhofer, and P. Wintermeyer. Fnf-bft: Exploring

performance limits of bft protocols. arXiv preprint arXiv:2009.02235, 2020.

[38] A. Avizienis. The n-version approach to fault-tolerant software. IEEE Transactions on software engineering,

(12):1491â€“1501, 1985.

[39] A. Babay, J. Schultz, T. Tantillo, S. Beckley, E. Jordan, K. Ruddell, K. Jordan, and Y. Amir. Deploying intrusion-
tolerant scada for the power grid. In Int. Conf. on Dependable Systems and Networks (DSN), pages 328â€“335.
IEEE, 2019.

[40] L. Baird. The swirlds hashgraph consensus algorithm: Fair, fast, byzantine fault tolerance. Swirlds Tech Reports

SWIRLDS-TR-2016-01, Tech. Rep, 2016.

[41] J. Baker, C. Bond, J. C. Corbett, J. Furman, A. Khorlin, J. Larson, J.-M. Leon, Y. Li, A. Lloyd, and V. Yushprakh.
Megastore: Providing scalable, highly available storage for interactive services. In Conf. on Innovative Data
Systems Research (CIDR), 2011.

[42] S. Bano, A. Sonnino, M. Al-Bassam, S. Azouvi, P. McCorry, S. Meiklejohn, and G. Danezis. Sok: Consensus
in the age of blockchains. In ACM Conf. on Advances in Financial Technologies, pages 183â€“198, 2019.
[43] M. Baudet, A. Ching, A. Chursin, G. Danezis, F. Garillot, Z. Li, D. Malkhi, O. Naor, D. Perelman, and A. Son-

nino. State machine replication in the libra blockchain. The Libra Assn., Tech. Rep, 2019.

[44] J. Behl, T. Distler, and R. Kapitza. Consensus-oriented parallelization: How to earn your ï¬rst million.

In

Proceedings of the 16th Annual Middleware Conference, pages 173â€“184, 2015.

[45] J. Behl, T. Distler, and R. Kapitza. Hybrids on steroids: Sgx-based high performance bft. In Proceedings of the

Twelfth European Conference on Computer Systems, pages 222â€“237, 2017.

[46] M. Ben-Or. Another advantage of free choice: Completely asynchronous agreement protocols (extended ab-
stract). In Proceedings of the 2nd ACM Annual Symposium on Principles of Distributed Computing, Montreal,
Quebec, pages 27â€“30, 1983.

[47] E. Ben-Sasson, I. Bentov, Y. Horesh, and M. Riabzev. Scalable zero knowledge with no trusted setup. In Annual

international cryptology conference, pages 701â€“732. Springer, 2019.

[48] C. Berger and H. P. Reiser. Scaling byzantine consensus: A broad analysis. In Proceedings of the 2nd Workshop

on Scalable and Resilient Infrastructures for Distributed Ledgers, pages 13â€“18, 2018.

[49] C. Berger, H. P. Reiser, J. Sousa, and A. Bessani. Resilient wide-area byzantine consensus using adaptive

weighted replication. In Symposium on Reliable Distributed Systems (SRDS), pages 183â€“18309. IEEE, 2019.

[50] A. Bessani, M. Correia, B. Quaresma, F. AndrÃ©, and P. Sousa. Depsky: dependable and secure storage in a

cloud-of-clouds. Transactions on Storage (TOS), 9(4):12, 2013.

22

[51] A. Bessani, J. Sousa, and E. E. Alchieri. State machine replication for the masses with bft-smart.

In 2014
44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, pages 355â€“362. IEEE,
2014.

[52] A. N. Bessani, P. Sousa, M. Correia, N. F. Neves, and P. Verissimo. The crutial way of critical infrastructure

protection. IEEE Security & Privacy, 6(6):44â€“51, 2008.

[53] M. Biely, Z. Milosevic, N. Santos, and A. Schiper. S-paxos: Oï¬„oading the leader for high throughput state
machine replication. In Symposium on Reliable Distributed Systems (SRDS), pages 111â€“120. IEEE, 2012.
[54] K. P. Birman, T. A. Joseph, T. Raeuchle, and A. El Abbadi. Implementing fault-tolerant distributed objects.

Trans. on Software Engineering, (6):502â€“508, 1985.

[55] F. Borran and A. Schiper. A leader-free byzantine consensus algorithm. In International Conference on Dis-

tributed Computing and Networking, pages 67â€“78. Springer, 2010.

[56] G. Bracha. An asynchronous [(n-1)/3]-resilient consensus protocol. In Symposium on Principles of distributed

computing, pages 154â€“162, 1984.

[57] G. Bracha and S. Toueg. Asynchronous consensus and broadcast protocols. Journal of the ACM (JACM),

32(4):824â€“840, 1985.

[58] F. Brasileiro, F. Greve, A. MostÃ©faoui, and M. Raynal. Consensus in one communication step. In Int. Conf. on

Parallel Computing Technologies (PaCT), pages 42â€“50. Springer, 2001.

[59] M. Bravo, G. Chockler, and A. Gotsman. Making byzantine consensus live. In 34th International Symposium

on Distributed Computing (DISC 2020). Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik, 2020.

[60] N. Bronson, Z. Amsden, G. Cabrera, P. Chakka, P. Dimov, H. Ding, J. Ferris, A. Giardullo, S. Kulkarni, H. Li,
et al. Tao: Facebookâ€™s distributed data store for the social graph. In Annual Technical Conf. (ATC), pages 49â€“60.
USENIX Association, 2013.

[61] R. G. Brown, J. Carlyle, I. Grigg, and M. Hearn. Corda: an introduction. R3 CEV, August, 1(15):14, 2016.
[62] E. Buchman. Tendermint: Byzantine fault tolerance in the age of blockchains. PhD thesis, 2016.
[63] E. Buchman, J. Kwon, and Z. Milosevic. The latest gossip on bft consensus. arXiv preprint arXiv:1807.04938,

2018.

[64] Y. Buchnik and R. Friedman. Fireledger: A high throughput blockchain consensus protocol. Proceedings of the

VLDB Endowment, 13(9).

[65] V. Buterin and V. Griï¬ƒth. Casper the friendly ï¬nality gadget. arXiv preprint arXiv:1710.09437, 2017.
[66] C. Cachin, K. Kursawe, F. Petzold, and V. Shoup. Secure and eï¬ƒcient asynchronous broadcast protocols. In

Annual Int. Cryptology Conf., pages 524â€“541. Springer, 2001.

[67] C. Cachin, K. Kursawe, and V. Shoup. Random oracles in constantinople: Practical asynchronous byzantine

agreement using cryptography. Journal of Cryptology, 18(3):219â€“246, 2005.

[68] C. Cachin and M. VukoliÄ‡. Blockchain consensus protocols in the wild.

Computing (DISC), pages 1â€“16, 2017.

In Int. Symposium on Distributed

[69] M. Castro and B. Liskov. Proactive recovery in a byzantine-fault-tolerant system. In Symposium on Operating

Systems Design and Implementation (OSDI), 2000.

[70] M. Castro and B. Liskov. Practical byzantine fault tolerance and proactive recovery. Transactions on Computer

Systems (TOCS), 20(4):398â€“461, 2002.

[71] M. Castro, B. Liskov, et al. Practical byzantine fault tolerance. In Symposium on Operating systems design and

implementation (OSDI), volume 99, pages 173â€“186. USENIX Association, 1999.

[72] B. Y. Chan and E. Shi. Streamlet: Textbook streamlined blockchains. In Proceedings of the 2nd ACM Conf. on

Advances in Financial Technologies, pages 1â€“11, 2020.

[73] T. H. Chan, R. Pass, and E. Shi. Pala: A simple partially synchronous blockchain. Cryptology ePrint Archive,

2018.

[74] T. H. Chan, R. Pass, and E. Shi. Pili: An extremely simple synchronous blockchain. Cryptology ePrint Archive,

2018.

[75] T. D. Chandra, R. Griesemer, and J. Redstone. Paxos made live: an engineering perspective. In Proceedings of
the twenty-sixth annual ACM symposium on Principles of distributed computing, pages 398â€“407, 2007.
[76] T. D. Chandra and S. Toueg. Unreliable failure detectors for reliable distributed systems. Journal of the ACM

(JACM), 43(2):225â€“267, 1996.

23

[77] A. Charapko, A. Ailijiang, and M. Demirbas. Pigpaxos: Devouring the communication bottlenecks in distributed

consensus. In SIGMOD Int Conf on Management of Data, pages 235â€“247, 2021.

[78] J. M. Chase. Quorum white paper, 2016.

[79] B.-G. Chun, P. Maniatis, S. Shenker, and J. Kubiatowicz. Attested append-only memory: Making adversaries

stick to their word. In Operating Systems Review (OSR), volume 41-6, pages 189â€“204. ACM SIGOPS, 2007.

[80] A. Clement, M. Kapritsos, S. Lee, Y. Wang, L. Alvisi, M. Dahlin, and T. Riche. Upright cluster services. In

Symposium on Operating systems principles (SOSP), pages 277â€“290. ACM, 2009.

[81] A. Clement, E. L. Wong, L. Alvisi, M. Dahlin, and M. Marchetti. Making byzantine fault tolerant systems
tolerate byzantine faults. In Symposium on Networked Systems Design and Implementation (NSDI), volume 9,
pages 153â€“168. USENIX Association, 2009.

[82] J. C. Corbett, J. Dean, M. Epstein, A. Fikes, et al. Spanner: Googleâ€™s globally distributed database. Transactions

on Computer Systems (TOCS), 31(3):8, 2013.

[83] M. Correia, N. F. Neves, L. C. Lung, and P. VerÃ­ssimo. Low complexity byzantine-resilient consensus. Distrib-

uted Computing, 17(3):237â€“249, 2005.

[84] M. Correia, N. F. Neves, L. C. Lung, and P. VerÃ­ssimo. Worm-itâ€“a wormhole-based intrusion-tolerant group

communication system. Journal of Systems and Software, 80(2):178â€“197, 2007.

[85] M. Correia, N. F. Neves, and P. Verissimo. How to tolerate half less one byzantine nodes in practical distributed

systems. In Int. Symposium on Reliable Distributed Systems (SRDS), pages 174â€“183. IEEE, 2004.

[86] M. Correia, N. F. Neves, and P. VerÃ­ssimo. From consensus to atomic broadcast: Time-free byzantine-resistant

protocols without signatures. The Computer Journal, 49(1):82â€“96, 2006.

[87] M. Correia, N. F. Neves, and P. Verissimo. Bft-to: Intrusion tolerance with less replicas. The Computer Journal,

56(6):693â€“715, 2013.

[88] M. Correia, G. S. Veronese, N. F. Neves, and P. Verissimo. Byzantine consensus in asynchronous message-
passing systems: a survey. International Journal of Critical Computer-Based Systems, 2(2):141â€“161, 2011.
[89] V. Costan, I. Lebedev, and S. Devadas. Sanctum: Minimal hardware extensions for strong software isolation. In

USENIX Security Symposium, pages 857â€“874, 2016.

[90] D. Cotroneo, R. Natella, R. Pietrantuono, and S. Russo. A survey of software aging and rejuvenation studies.

ACM Journal on Emerging Technologies in Computing Systems (JETC), 10(1):1â€“34, 2014.

[91] J. Cowling, D. Myers, B. Liskov, R. Rodrigues, and L. Shrira. Hq replication: A hybrid quorum protocol for
byzantine fault tolerance. In Symposium on Operating systems design and implementation (OSDI), pages 177â€“
190. USENIX Association, 2006.

[92] T. Crain, V. Gramoli, M. Larrea, and M. Raynal. Dbft: Eï¬ƒcient leaderless byzantine consensus and its ap-
plication to blockchains. In Int. Symposium on Network Computing and Applications (NCA), pages 1â€“8. IEEE,
2018.

[93] T. Crain, C. Natoli, and V. Gramoli. Red belly: a secure, fair and scalable open blockchain. In Symposium on

Security and Privacy (S&Pâ€™21). IEEE, 2021.

[94] G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian,
P. Vosshall, and W. Vogels. Dynamo: amazonâ€™s highly available key-value store. In Operating Systems Re-
view (OSR), volume 41, pages 205â€“220. ACM SIGOPS, 2007.

[95] T. Distler. Byzantine fault-tolerant state-machine replication from a systems perspective. ACM Computing

Surveys (CSUR), 54(1):1â€“38, 2021.

[96] T. Distler, C. Cachin, and R. Kapitza. Resource-eï¬ƒcient byzantine fault tolerance. Transactions on Computers,

65(9):2807â€“2819, 2016.

[97] T. Distler, I. Popov, W. SchrÃ¶der-Preikschat, H. P. Reiser, and R. Kapitza. Spare: Replicas on hold. In Network

and Distributed System Security Symposium (NDSS, 2011.

[98] D. Dobre, G. Karame, W. Li, M. Majuntke, N. Suri, and M. VukoliÄ‡. Powerstore: Proofs of writing for eï¬ƒcient
In Proceedings of the 2013 ACM SIGSAC conference on Computer & communications

and robust storage.
security, pages 285â€“298, 2013.

[99] D. Dobre, M. Majuntke, M. Seraï¬ni, and N. Suri. Hp: Hybrid paxos for wans.

Computing Conf. (EDCC), pages 117â€“126. IEEE, 2010.

In European Dependable

24

[100] D. Dolev, C. Dwork, and L. Stockmeyer. On the minimal synchronism needed for distributed consensus. Journal

of the ACM (JACM), 34(1):77â€“97, 1987.

[101] S. Duan, M. K. Reiter, and H. Zhang. Beat: Asynchronous bft made practical. In ACM SIGSAC Conference on

Computer and Communications Security (CCS), pages 2028â€“2041, 2018.
[102] S. Duan and H. Zhang. Practical state machine replication with conï¬dentiality.

Distributed Systems (SRDS), pages 187â€“196. IEEE, 2016.

In Symposium on Reliable

[103] C. Dwork, N. Lynch, and L. Stockmeyer. Consensus in the presence of partial synchrony. Journal of the ACM

(JACM), 35(2):288â€“323, 1988.

[104] M. Eischer and T. Distler. Latency-aware leader selection for geo-replicated byzantine fault-tolerant systems. In

Int. Conf. on Dependable Systems and Networks Workshops (DSN-W), pages 140â€“145. IEEE, 2018.

[105] M. Eischer and T. Distler. Scalable byzantine fault-tolerant state-machine replication on heterogeneous servers.

Computing, 101(2):97â€“118, 2019.

[106] A. El Abbadi, D. Skeen, and F. Cristian. An eï¬ƒcient, fault-tolerant protocol for replicated data management. In

SIGACT-SIGMOD symposium on Principles of database systems, pages 215â€“229. ACM, 1985.

[107] A. El Abbadi and S. Toueg. Availability in partitioned replicated databases. In SIGACT-SIGMOD symposium

on Principles of database systems, pages 240â€“251. ACM, 1985.

[108] I. A. Escobar, E. Alchieri, F. L. Dotti, and F. Pedone. Boosting concurrency in parallel state machine replication.

In Int. Middleware Conf., pages 228â€“240, 2019.

[109] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distributed consensus with one faulty process.

Journal of the ACM (JACM), 32(2):374â€“382, 1985.

[110] S. Forrest, A. Somayaji, and D. H. Ackley. Building diverse computer systems. In Workshop on Hot Topics in

Operating Systems, pages 67â€“72. IEEE, 1997.

[111] F. Gai, A. Farahbakhsh, J. Niu, C. Feng, I. Beschastnikh, and H. Duan. Dissecting the performance of chained-

bft. In Int Conf on Distributed Computing Systems (ICDCS), pages 595â€“606. IEEE, 2021.

[112] M. Garcia, N. Neves, and A. Bessani. An intrusion-tolerant ï¬rewall design for protecting siem systems.

In
2013 43rd Annual IEEE/IFIP Conference on Dependable Systems and Networks Workshop (DSN-W), pages
1â€“7. IEEE, 2013.

[113] M. Garcia, N. Neves, and A. Bessani. Sieveq: A layered bft protection system for critical services.

Transactions on Dependable and Secure Computing, 15(3):511â€“525, 2016.

IEEE

[114] Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich. Algorand: Scaling byzantine agreements for
cryptocurrencies. In Proceedings of the 26th symposium on operating systems principles, pages 51â€“68, 2017.
[115] G. R. Goodson, J. J. Wylie, G. R. Ganger, and M. K. Reiter. Eï¬ƒcient byzantine-tolerant erasure-coded storage.
In International Conference on Dependable Systems and Networks, 2004, pages 135â€“144. IEEE, 2004.
[116] C. Gorenï¬‚o, L. Golab, and S. Keshav. Xox fabric: A hybrid approach to transaction execution. In Int. Conf. on

Blockchain and Cryptocurrency (ICBC), pages 1â€“9. IEEE, 2020.

[117] C. Gorenï¬‚o, S. Lee, L. Golab, and S. Keshav. Fastfabric: Scaling hyperledger fabric to 20,000 transactions per

second. In Int. Conf. on Blockchain and Cryptocurrency (ICBC), pages 455â€“463. IEEE, 2019.
Multichain private blockchain-white paper.

[118] G. Greenspan.

URl:

http://www. multichain.

com/download/MultiChain-White-Paper. pdf, 2015.

[119] R. Guerraoui, N. KneÅ¾eviÄ‡, V. QuÃ©ma, and M. VukoliÄ‡. The next 700 bft protocols. In Proceedings of the 5th

European conference on Computer systems, pages 363â€“376, 2010.

[120] G. G. Gueta, I. Abraham, S. Grossman, D. Malkhi, B. Pinkas, M. K. Reiter, D.-A. Seredinschi, O. Tamir, and
A. Tomescu. Sbft: a scalable decentralized trust infrastructure for blockchains. In Int. Conf. on Dependable
Systems and Networks (DSN), pages 568â€“580. IEEE/IFIP, 2019.

[121] B. Guo, Z. Lu, Q. Tang, J. Xu, and Z. Zhang. Dumbo: Faster asynchronous bft protocols. In Proceedings of the

2020 ACM SIGSAC Conference on Computer and Communications Security, pages 803â€“818, 2020.

[122] D. Gupta, L. Perronne, and S. Bouchenak. Bft-bench: Towards a practical evaluation of robustness and ef-
fectiveness of bft protocols. In IFIP International Conference on Distributed Applications and Interoperable
Systems, pages 115â€“128. Springer, 2016.

[123] S. Gupta, J. Hellings, S. Rahnama, and M. Sadoghi. Proof-of-execution: Reaching consensus through fault-

tolerant speculation. In Int Conf. on Extending Database Technology (EDBT), pages 301â€“312, 2021.

25

[124] S. Gupta, J. Hellings, and M. Sadoghi. Rcc: Resilient concurrent consensus for high-throughput secure trans-

action processing. In Int Conf on Data Engineering (ICDE), pages 1392â€“1403. IEEE, 2021.

[125] S. Gupta, S. Rahnama, J. Hellings, and M. Sadoghi. Resilientdb: Global scale resilient blockchain fabric.

Proceedings of the VLDB Endowment, 13(6):868â€“883, 2020.

[126] A. Haeberlen, P. Kouznetsov, and P. Druschel. The case for byzantine fault detection. In HotDep, 2006.
[127] T. Hanke, M. Movahedi, and D. Williams. Dï¬nity technology overview series, consensus system. arXiv preprint

arXiv:1805.04548, 2018.

[128] J. Hendricks, G. R. Ganger, and M. K. Reiter. Low-overhead byzantine fault-tolerant storage. ACM SIGOPS

Operating Systems Review, 41(6):73â€“86, 2007.

[129] H. Howard, D. Malkhi, and A. Spiegelman. Flexible paxos: Quorum intersection revisited. In 20th International

Conference on Principles of Distributed Systems, 2017.

[130] Y. Huang, C. Kintala, N. Kolettis, and N. D. Fulton. Software rejuvenation: Analysis, module and applications.

In Int. symposium on fault-tolerant computing. Digest of papers, pages 381â€“390. IEEE, 1995.

[131] F. P. Junqueira, B. C. Reed, and M. Seraï¬ni. Zab: High-performance broadcast for primary-backup systems.
In 2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN), pages 245â€“256.
IEEE, 2011.

[132] R. Kallman, H. Kimura, J. Natkins, A. Pavlo, A. Rasin, S. Zdonik, E. P. Jones, S. Madden, M. Stonebraker,
Y. Zhang, et al. H-store: a high-performance, distributed main memory transaction processing system. Proc. of
the VLDB Endowment, 1(2):1496â€“1499, 2008.

[133] R. Kapitza, J. Behl, C. Cachin, T. Distler, S. Kuhnle, S. V. Mohammadi, W. SchrÃ¶der-Preikschat, and K. Stengel.
Cheapbft: resource-eï¬ƒcient byzantine fault tolerance. In European Conf. on Computer Systems (EuroSys), pages
295â€“308. ACM, 2012.

[134] M. Kapritsos, Y. Wang, V. Quema, A. Clement, L. Alvisi, M. Dahlin, et al. All about eve: Execute-verify
In Symposium on Operating systems design and implementation (OSDI),

replication for multi-core servers.
volume 12, pages 237â€“250. USENIX Association, 2012.

[135] M. Kelkar, S. Deb, S. Long, A. Juels, and S. Kannan. Themis: Fast, strong order-fairness in byzantine consensus.

Cryptology ePrint Archive, 2021.

[136] M. Kelkar, F. Zhang, S. Goldfeder, and A. Juels. Order-fairness for byzantine consensus. In Annual Int. Crypto-

logy Conf., pages 451â€“480. Springer, 2020.

[137] A. Kiayias, A. Russell, B. David, and R. Oliynykov. Ouroboros: A provably secure proof-of-stake blockchain

protocol. In Annual Int. Cryptology Conf., pages 357â€“388. Springer, 2017.

[138] R. M. Kieckhafer and M. H. Azadmanesh. Reaching approximate agreement with mixed-mode faults. Transac-

tions on Parallel and Distributed Systems, 5(1):53â€“63, 1994.

[139] J. Kirsch, S. Goose, Y. Amir, D. Wei, and P. Skare. Survivable scada via intrusion-tolerant replication. IEEE

Transactions on Smart Grid, 5(1):60â€“70, 2013.

[140] E. K. Kogias, P. Jovanovic, N. Gailly, I. Khoï¬ƒ, L. Gasser, and B. Ford. Enhancing bitcoin security and per-
In Security Symposium, pages 279â€“296. USENIX

formance with strong consistency via collective signing.
Association, 2016.

[141] E. Kokoris-Kogias. Robust and scalable consensus for sharded distributed ledgers.
[142] E. Kokoris-Kogias, P. Jovanovic, L. Gasser, N. Gailly, E. Syta, and B. Ford. Omniledger: A secure, scale-out,

decentralized ledger via sharding. In Symposium on Security and Privacy (SP), pages 583â€“598. IEEE, 2018.

[143] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong. Zyzzyva: speculative byzantine fault tolerance.

Operating Systems Review (OSR), 41(6):45â€“58, 2007.

[144] R. Kotla and M. Dahlin. High throughput byzantine fault tolerance. In International Conference on Dependable

Systems and Networks, pages 575â€“584. IEEE, 2004.

[145] K. Kursawe. Wendy, the good little fairness widget: Achieving order fairness for blockchains.

Advances in Financial Technologies, pages 25â€“36. ACM, 2020.

In Conf. on

[146] K. Kursawe. Wendy grows up: More order fairness. In Int. Conf. on Financial Cryptography and Data Security

(FC), pages 191â€“196. Springer, 2021.

[147] P. Kuznetsov, A. Tonkikh, and Y. X. Zhang. Revisiting optimal resilience of fast byzantine consensus.

In

Symposium on Principles of distributed computing (PODC), pages 343â€“353. ACM, 2021.

26

[148] J. Kwon. Tendermint: Consensus without mining. 2014.
[149] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM,

21(7):558â€“565, 1978.

[150] L. Lamport. Paxos made simple. ACM Sigact News, 32(4):18â€“25, 2001.
[151] L. Lamport. Generalized consensus and paxos. 2005.
[152] L. Lamport. Fast paxos. Distributed Computing, 19(2):79â€“103, 2006.
[153] L. Lamport. Brief announcement: Leaderless byzantine paxos. In Int. Symposium on Distributed Computing

(DISC), pages 141â€“142, 2011.

[154] L. Lamport. The part-time parliament. In Concurrency: the Works of Leslie Lamport, pages 277â€“317. 2019.
[155] L. Lamport and M. Massa. Cheap paxos. In Int. Conf. on Dependable Systems and Networks (DSN), pages

307â€“314. IEEE, 2004.

[156] L. Lamport, R. Shostak, and M. Pease. The byzantine generals problem. Transactions on Programming Lan-

guages and Systems (TOPLAS), 4(3):382â€“401, 1982.

[157] B. Lampson. The abcdâ€™s of paxos. In PODC, volume 1, page 13. Citeseer, 2001.
[158] D. Lee, D. Kohlbrenner, S. Shinde, K. AsanoviÄ‡, and D. Song. Keystone: An open framework for architecting

trusted execution environments. In European Conference on Computer Systems (EuroSys, pages 1â€“16, 2020.

[159] K. Lev-Ari, A. Spiegelman, I. Keidar, and D. Malkhi. Fairledger: A fair blockchain protocol for ï¬nancial
In 23rd Int. Conf. on Principles of Distributed Systems (OPODIS). Schloss Dagstuhl-Leibniz-

institutions.
Zentrum fÃ¼r Informatik, 2019.

[160] D. Levin, J. R. Douceur, J. R. Lorch, and T. Moscibroda. Trinc: Small trusted hardware for large distributed

systems. In NSDI, volume 9, pages 1â€“14, 2009.

[161] B. Li, N. Weichbrodt, J. Behl, P.-L. Aublin, T. Distler, and R. Kapitza. Troxy: Transparent access to byzantine

fault-tolerant systems. In Int. Conf. on Dependable Systems and Networks (DSN), pages 59â€“70. IEEE, 2018.

[162] B. Li, W. Xu, M. Z. Abid, T. Distler, and R. Kapitza. Sarek: Optimistic parallel ordering in byzantine fault

tolerance. In European Dependable Computing Conf. (EDCC), pages 77â€“88. IEEE, 2016.

[163] H. C. Li, A. Clement, A. S. Aiyer, and L. Alvisi. The paxos register. In 2007 26th IEEE International Symposium

on Reliable Distributed Systems (SRDS 2007), pages 114â€“126. IEEE, 2007.

[164] J. Li and D. MaziÃ©res. Beyond one-third faulty replicas in byzantine fault tolerant systems. In Symposium on

Networked Systems Design and Implementation (NSDI). USENIX Association, 2007.

[165] W. Li, C. Feng, L. Zhang, H. Xu, B. Cao, and M. A. Imran. A scalable multi-layer pbft consensus for blockchain.

Transactions on Parallel and Distributed Systems, 32(5):1146â€“1160, 2020.

[166] B. Liskov and J. Cowling. Viewstamped replication revisited. 2012.
[167] S. Liu, P. Viotti, C. Cachin, V. QuÃ©ma, and M. Vukolic. Xft: Practical fault tolerance beyond crashes.

In
Symposium on Operating systems design and implementation (OSDI), pages 485â€“500. USENIX Association,
2016.

[168] L. Luu, V. Narayanan, C. Zheng, K. Baweja, S. Gilbert, and P. Saxena. A secure sharding protocol for open
blockchains. In SIGSAC Conf. on Computer and Communications Security (CCS), pages 17â€“30. ACM, 2016.
[169] D. Malkhi and M. Reiter. Unreliable intrusion detection in distributed computations. In Computer Security

Foundations Workshop, pages 116â€“124. IEEE, 1997.

[170] D. Malkhi and M. Reiter. Byzantine quorum systems. Distributed computing, 11(4):203â€“213, 1998.
[171] D. Malkhi and M. K. Reiter. Secure and scalable replication in phalanx.

In Proceedings Seventeenth IEEE

Symposium on Reliable Distributed Systems (Cat. No. 98CB36281), pages 51â€“58. IEEE, 1998.

[172] D. Malkhi and M. K. Reiter. Survivable consensus objects. In IEEE Symposium on Reliable Distributed Systems

(SRDS), pages 271â€“279. IEEE, 1998.

[173] Y. Mao, F. P. Junqueira, and K. Marzullo. Towards low latency state machine replication for uncivil wide-area

networks. In Workshop on Hot Topics in System Dependability. Citeseer, 2009.

[174] J.-P. Martin and L. Alvisi. Fast byzantine consensus. Transactions on Dependable and Secure Computing,

3(3):202â€“215, 2006.

[175] F. McKeen, I. Alexandrovich, A. Berenzon, C. V. Rozas, H. Shaï¬, V. Shanbhogue, and U. R. Savagaonkar.

Innovative instructions and software model for isolated execution. Hasp@ isca, 10(1), 2013.

27

[176] F. J. Meyer and D. K. Pradhan. Consensus with dual failure modes. Transactions on Parallel and Distributed

Systems, (2):214â€“222, 1991.

[177] A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song. The honey badger of bft protocols. In ACM SIGSAC Conf.

on Computer and Communications Security (CCS), pages 31â€“42, 2016.

[178] Z. Milosevic, M. Biely, and A. Schiper. Bounded delay in byzantine-tolerant state machine replication. In Int.

Symposium on Reliable Distributed Systems (SRDS), pages 61â€“70. IEEE, 2013.

[179] I. Moraru, D. G. Andersen, and M. Kaminsky. Egalitarian paxos. In ACM Symposium on Operating Systems

Principles, 2012.

[180] I. Moraru, D. G. Andersen, and M. Kaminsky. There is more consensus in egalitarian parliaments. In ACM

Symposium on Operating Systems Principles (SOSP), pages 358â€“372, 2013.

[181] L. E. Moser, P. M. Melliar-Smith, P. Narasimhan, L. A. Tewksbury, and V. Kalogeraki. The eternal system: An
architecture for enterprise applications. In Int. Enterprise Distributed Object Computing Conf. (EDOC), pages
214â€“222. IEEE, 1999.

[182] O. Naor, M. Baudet, D. Malkhi, and A. Spiegelman. Cogsworth: Byzantine view synchronization. arXiv preprint

arXiv:1909.05204, 2019.

[183] O. Naor and I. Keidar. Expected linear round synchronization: The missing link for linear byzantine smr. In

Int.l Symposium on Distributed Computing (DISC). Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik, 2020.

[184] F. Nawab, D. Agrawal, and A. El Abbadi. Dpaxos: Managing data closer to users for low-latency and mobile
applications. In Proceedings of the 2018 International Conference on Management of Data, pages 1221â€“1236.
ACM, 2018.

[185] F. Nawab and M. Sadoghi. Blockplane: A global-scale byzantizing middleware. In 2019 IEEE 35th Int. Conf.

on Data Engineering (ICDE), pages 124â€“135. IEEE, 2019.

[186] R. Neiheiser, M. Matos, and L. Rodrigues. Kauri: Scalable bft consensus with pipelined tree-based dissem-
ination and aggregation. In ACM SIGOPS Symposium on Operating Systems Principles (SOSP), pages 35â€“48,
2021.

[187] R. Neiheiser, D. Presser, L. Rech, M. Bravo, L. Rodrigues, and M. Correia. Fireplug: Flexible and robust n-
version geo-replication of graph databases. In Int Conf on Information Networking (ICOIN), pages 110â€“115.
IEEE, 2018.

[188] N. F. Neves, M. Correia, and P. Verissimo. Solving vector consensus with a wormhole. IEEE Transactions on

Parallel and Distributed Systems, 16(12):1120â€“1131, 2005.

[189] A. Nogueira, M. Garcia, A. Bessani, and N. Neves. On the challenges of building a bft scada. In Int. Conf. on

Dependable Systems and Networks (DSN), pages 163â€“170. IEEE, 2018.

[190] B. M. Oki and B. H. Liskov. Viewstamped replication: A new primary copy method to support highly-available
distributed systems. In Symposium on Principles of distributed computing (PODC), pages 8â€“17. ACM, 1988.
[191] D. Ongaro and J. K. Ousterhout. In search of an understandable consensus algorithm. In Annual Technical Conf.

(ATC), pages 305â€“319. USENIX Association, 2014.

[192] R. Pass and E. Shi. Hybrid consensus: Eï¬ƒcient consensus in the permissionless model. In 31 Int. Symposium

on Distributed Computing, page 6, 2017.

[193] R. Pass and E. Shi. Thunderella: Blockchains with optimistic instant conï¬rmation. In Annual Int. Conf. on the

Theory and Applications of Cryptographic Techniques, pages 3â€“33. Springer, 2018.

[194] F. Pedone. Boosting system performance with optimistic distributed protocols. Computer, 34(12):80â€“86, 2001.
[195] M. Platania, D. Obenshain, T. Tantillo, Y. Amir, and N. Suri. On choosing server-or client-side solutions for bft.

ACM Computing Surveys (CSUR), 48(4):1â€“30, 2016.

[196] D. Porto, J. LeitÃ£o, C. Li, A. Clement, A. Kate, F. Junqueira, and R. Rodrigues. Visigoth fault tolerance. In

European Conf. on Computer Systems (EuroSys), page 8. ACM, 2015.

[197] J. Qi, X. Chen, Y. Jiang, J. Jiang, T. Shen, S. Zhao, S. Wang, G. Zhang, L. Chen, M. H. Au, et al. Bidl: A
high-throughput, low-latency permissioned blockchain framework for datacenter networks. In Symposium on
Operating Systems Principles (SOSP), pages 18â€“34. ACM SIGOPS, 2021.

[198] M. O. Rabin. Randomized byzantine generals. In Symposium on Foundations of Computer Science (SFCS),

pages 403â€“409. IEEE, 1983.

28

[199] H. V. Ramasamy and C. Cachin. Parsimonious asynchronous byzantine-fault-tolerant atomic broadcast.

In

International Conference On Principles Of Distributed Systems, pages 88â€“102. Springer, 2005.

[200] H. P. Reiser and R. Kapitza. Hypervisor-based eï¬ƒcient proactive recovery.

Distributed Systems (SRDS), pages 83â€“92. IEEE, 2007.

In Int. Symposium on Reliable

[201] R. v. Renesse, C. Ho, and N. Schiper. Byzantine chain replication. In International Conference On Principles

Of Distributed Systems, pages 345â€“359. Springer, 2012.

[202] R. L. Rivest, A. Shamir, and L. M. Adleman. A method for obtaining digital signatures and public key cryptosys-

tems. Routledge, 2019.

[203] T. Roeder and F. B. Schneider. Proactive obfuscation. ACM Transactions on Computer Systems (TOCS), 28(2):1â€“

54, 2010.

[204] P. Ruan, D. Loghin, Q.-T. Ta, M. Zhang, G. Chen, and B. C. Ooi. A transactional perspective on execute-order-

validate blockchains. In SIGMOD Int. Conf. on Management of Data, pages 543â€“557. ACM, 2020.

[205] F. B. Schneider. Implementing fault-tolerant services using the state machine approach: A tutorial. Computing

Surveys (CSUR), 22(4):299â€“319, 1990.

[206] M. Seraï¬ni, P. Bokor, D. Dobre, M. Majuntke, and N. Suri. Scrooge: Reducing the costs of fast byzantine
replication in presence of unresponsive replicas. In Int. Conf. on Dependable Systems and Networks (DSN),
pages 353â€“362. IEEE, 2010.

[207] A. Sharma, F. M. Schuhknecht, D. Agrawal, and J. Dittrich. Blurring the lines between blockchains and database
systems: the case of hyperledger fabric. In SIGMOD Int. Conf. on Management of Data, pages 105â€“122. ACM,
2019.

[208] V. Shoup. Practical threshold signatures. In International Conference on the Theory and Applications of Cryp-

tographic Techniques, pages 207â€“220. Springer, 2000.

[209] N. Shrestha, I. Abraham, L. Ren, and K. Nayak. On the optimality of optimistic responsiveness. In ACM SIGSAC

Conf. on Computer and Communications Security (CCS), pages 839â€“857, 2020.

[210] A. Singh, T. Das, P. Maniatis, P. Druschel, and T. Roscoe. Bft protocols under ï¬re. In NSDI, volume 8, pages

189â€“204, 2008.

[211] H.-S. Siu, Y.-H. Chin, and W.-P. Yang. A note on consensus on dual failure modes. Transactions on Parallel

and Distributed Systems, 7(3):225â€“230, 1996.

[212] Y. J. Song and R. van Renesse. Bosco: One-step byzantine asynchronous consensus. In Int. Symposium on

Distributed Computing (DISC), pages 438â€“450. Springer, 2008.

[213] J. Sousa and A. Bessani. Separating the wheat from the chaï¬€: An empirical design for geo-replicated state

machines. In Symposium on Reliable Distributed Systems (SRDS), pages 146â€“155. IEEE, 2015.

[214] P. Sousa, A. N. Bessani, M. Correia, N. F. Neves, and P. Verissimo. Highly available intrusion-tolerant services
with proactive-reactive recovery. IEEE Transactions on Parallel and Distributed Systems, 21(4):452â€“465, 2009.
[215] C. Stathakopoulou, T. David, and M. Vukolic. Mir-bft: High-throughput bft for blockchains. arXiv preprint

arXiv:1906.05552, 2019.

[216] C. Stathakopoulou, S. RÃ¼sch, M. Brandenburger, and M. VukoliÄ‡. Adding fairness to order: Preventing front-
running attacks in bft protocols using tees. In Int Symp on Reliable Distributed Systems (SRDS), pages 34â€“45.
IEEE, 2021.

[217] F. Suri-Payer, M. Burke, Z. Wang, Y. Zhang, L. Alvisi, and N. Crooks. Basil: Breaking up bft with acid
(transactions). In ACM SIGOPS Symposium on Operating Systems Principles (SOSP), pages 1â€“17, 2021.
[218] P. Thambidurai, Y.-K. Park, et al. Interactive consistency with multiple failure modes. In Symposium on Reliable

Distributed Systems (SRDS), pages 93â€“100. IEEE, 1988.

[219] G. Tsudik. Message authentication with one-way hash functions. ACM SIGCOMM Computer Communication

Review, 22(5):29â€“38, 1992.

[220] R. Van Renesse, N. Schiper, and F. B. Schneider. Vive la diï¬€Ã©rence: Paxos vs. viewstamped replication vs. zab.

IEEE Transactions on Dependable and Secure Computing, 12(4):472â€“484, 2014.

[221] P. E. VerÃ­ssimo. Travelling through wormholes: a new look at distributed systems models. ACM SIGACT News,

37(1):66â€“81, 2006.

[222] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung. Spin oneâ€™s wheels? byzantine fault tolerance with

a spinning primary. In Int. Symposium on Reliable Distributed Systems (SRDS), pages 135â€“144. IEEE, 2009.

29

[223] G. S. Veronese, M. Correia, A. N. Bessani, and L. C. Lung. Ebawa: Eï¬ƒcient byzantine agreement for wide-area
networks. In Int. Symposium on High Assurance Systems Engineering (HASE), pages 10â€“19. IEEE, 2010.
[224] G. S. Veronese, M. Correia, A. N. Bessani, L. C. Lung, and P. Verissimo. Eï¬ƒcient byzantine fault-tolerance.

Transactions on Computers, 62(1):16â€“30, 2013.

[225] G. Voron and V. Gramoli. Dispel: Byzantine smr with distributed pipelining. arXiv preprint arXiv:1912.10367,

2019.

[226] M. Whittaker, A. Ailijiang, A. Charapko, M. Demirbas, N. Giridharan, J. M. Hellerstein, H. Howard, I. Stoica,
and A. Szekeres. Scaling replicated state machines with compartmentalization. Proceedings of the VLDB
Endowment, 14(11):2203â€“2215, 2021.

[227] T. Wood, R. Singh, A. Venkataramani, P. Shenoy, and E. Cecchet. Zz and the art of practical bft execution. In

Conf. on Computer systems, pages 123â€“138. ACM, 2011.

[228] D. Yakira, A. Asayag, G. Cohen, I. Grayevsky, M. Leshkowitz, O. Rottenstreich, and R. Tamari. Helix: A fair
blockchain consensus protocol resistant to ordering manipulation. IEEE Transactions on Network and Service
Management, 18(2):1584â€“1597, 2021.

[229] J. Yin, J.-P. Martin, A. Venkataramani, L. Alvisi, and M. Dahlin. Separating agreement from execution for

byzantine fault tolerant services. Operating Systems Review (OSR), 37(5):253â€“267, 2003.

[230] M. Yin, D. Malkhi, M. K. Reiter, G. G. Gueta, and I. Abraham. Hotstuï¬€: Bft consensus with linearity and
responsiveness. In Symposium on Principles of Distributed Computing (PODC), pages 347â€“356. ACM, 2019.
[231] M. Zamani, M. Movahedi, and M. Raykova. Rapidchain: Scaling blockchain via full sharding. In SIGSAC Conf.

on Computer and Communications Security, pages 931â€“948. ACM, 2018.

[232] G. Zhang, F. Pan, M. Dangâ€™ana, Y. Mao, S. Motepalli, S. Zhang, and H.-A. Jacobsen. Reaching consensus in
the byzantine empire: A comprehensive review of bft consensus algorithms. arXiv preprint arXiv:2204.03181,
2022.

[233] Y. Zhang, S. Setty, Q. Chen, L. Zhou, and L. Alvisi. Byzantine ordered consensus without byzantine oligarchy.
In USENIX Symposium on Operating Systems Design and Implementation (OSDI), pages 633â€“649, 2020.
[234] L. Zhou, F. Schneider, R. VanRenesse, and Z. Haas. Secure distributed on-line certiï¬cation authority, Aug. 22

2002. US Patent App. 10/001,588.

[235] L. Zhou, F. B. Schneider, and R. Van Renesse. Coca: A secure distributed online certiï¬cation authority. ACM

Transactions on Computer Systems (TOCS), 20(4):329â€“368, 2002.

30

