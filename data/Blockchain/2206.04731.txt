Leveraging Centric Data Federated Learning Using Blockchain For Integrity
Assurance

Riadh Ben Chaabene,1 Darine Amayed, 1 Mohamed Cheriet 1
1 ´Ecole de technologie sup´erieure ´ETS
1Systems Engineering Department (Synchromedia lab)
21100 Notre-Dame St W
Montreal, Quebec H3C 1K3
riadh.ben-chaabene.1@ens.etsmtl.ca, darine.ameyed.1@ens.etsmtl.ca, mohamed.cheriet@etsmtl.ca

2
2
0
2

n
u
J

9

]

G
L
.
s
c
[

1
v
1
3
7
4
0
.
6
0
2
2
:
v
i
X
r
a

Abstract

Machine learning abilities have become a vital component
for various solutions across industries, applications, and sec-
tors. Many organizations seek to leverage AI-based solutions
across their business services to unlock better efﬁciency and
increase productivity. Problems, however, can arise if there
is a lack of quality data for AI-model training, scalability,
and maintenance. We propose a data-centric federated learn-
ing architecture leveraged by a public blockchain and smart
contracts to overcome this signiﬁcant issue. Our proposed so-
lution provides a virtual public marketplace where develop-
ers, data scientists, and AI-engineer can publish their models
and collaboratively create and access quality data for train-
ing. We enhance data quality and integrity through an in-
centive mechanism that rewards contributors for data contri-
bution and veriﬁcation. Those combined with the proposed
framework helped increase with only one user simulation the
training dataset with an average of 100 input daily and the
model accuracy by approximately 4%.

Introduction

The machine learning market
is witnessing tremendous
growth. In 2020, the market saw an increase of 38.01%,
leading to a compound annual growth rate growth of 39%
and an investment intercepted at $11.16 Billion between
the year 2020 and 2024 (Technavio 2020). This is due to
the fragmentation of the market with large companies in-
vesting and contributing to the artiﬁcial intelligence ﬁeld,
such as Alphabet Inc, Amazon Inc, and many others. More-
over, in the last few years, machine learning research has
augmented tremendously due to its contribution to numer-
ous areas, making them more robust and systematic. Society
and companies have beneﬁted from this progress, leading to
multiple investments to encourage this growth. Researchers
and developers have been dedicated to providing more stud-
ies and contributions to this technology. Despite all of this,
machine learning is still facing signiﬁcant challenges; two of
them consist of data accessibility and trustability. Data (Bi-
derman and Scheirer 2021) is the key that is driving the ma-
jority of machine learning models. Acutely, the data is fac-
ing the following issues: high centralization, complex acces-
sibility, insufﬁcient security, and insufﬁcient quality. Com-

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

panies and research centers are acting with the problem of
acquiring the correct data for their research(Biderman and
Scheirer 2021), since most of the data tends to be central-
ized and inaccessible, leading to either unavailability or high
cost of acquirement. Also, data can easily be tampered with,
which causes a lack of data quality. Even if the data is avail-
able, noisy information could affect the whole model. Re-
sults of a model could variety greatly from one dataset to
another, and since the data commonly being used is vari-
able, there is no guarantee that the data is equitable.
Additionally, creating the model relies on data collection,
which consists of 60% of the overall machine learning work
and that 25% of the time is allocated for data cleaning and
data labeling(Roh, Heo, and Whang 2019). Despite the im-
portance of data, 92% of it is owned and stored in the west-
ern world (Amiot et al. 2020), where most of the big com-
panies lie. On the other hand, machine learning model train-
ing is dependent on human interaction, causing the learning
process to start or end in an insigniﬁcant time leading to the
problem of automation.
It is therefore mandatory to provide alternative solutions to
these problems. They will enhance the progress of machine
learning, leading to the creation of a more efﬁcient and so-
phisticated model to improve the development of this tech-
nology and our lives. All of this leads to the categorization
the problem into three areas:

• Model training issues: that consist of the insufﬁcient data
and resources for model training, leading to less accu-
racy.

• Data availability issues: that corresponds to the lack of
data available for people to train machine learning mod-
els.

• Data Integrity issues: that refer to the insurance of the

available data quality.

In this paper, we propose a framework for publishing and
contributing to the collaborative machine learning process.
The idea is to present a public marketplace with free access
to share a model on the one end and contribute to the contin-
uous training of this model on the other. We encourage par-
ticipants to share and verify the data to provide the needed
data quality for the training to avoid manipulation. Also, we
are looking for model diversity where this framework will
function with any machine learning model type. To reach our

 
 
 
 
 
 
objective, we will be using a collaborative learning approach
such as federated learning. Furthermore, we will be using
blockchain technology (Nakamoto 2008) as an infrastruc-
ture for our framework and the InterPlanetary File System
as a storage mechanism. Also, a monetary incentive mecha-
nism will be developed to reward contributors and withhold
malicious users.

Background
Federated Learning (Yang et al. 2019) conducts machine
learning in a decentralized approach. It aims to provide the
model to the data rather than the traditional way, where we
create a model then provide the data. The data is distributed
along with different users in their edge mobiles. Each pro-
vides his data to train the model, leading to multiple unique
training datasets rather than one. Only the metadata is
returned to the centralized model or the global model using
encrypted communication. At the same time, this provides
knowledge performance for the model and enhances data
privacy since all the calculations happen inside the owner’s
device without any exchange of their data to a centralized
server. There exists two types of federated learning:

• Model-Centric Federated Learning This is the most
common of the two types. In Model-Centric, the model
is hosted in a cloud service, and its API is pre-conﬁgured
(Layers, Weight, Etc ..). Each user downloads the model,
enhances it and uploads a newer version. But this could
happen over a long period of time, mostly weeks and
months (Yang et al. 2019). Figure 1 describes its pro-
cess. First, the model is sent to the edge device. Then,
the metadata is sent to the global model, which resends
an updated model.
A great example is Google’s GBoard mobile app. It
learns users typing preferences over time without send-
ing any private data.

Figure 1: Model-Centric Federated Learning

• Data-Centric Federated Learning Less common than
model-centric, but more suitable for experimentation sci-
ence(Majcherczyk, Srishankar, and Pinciroli 2020). It is
hosted on a cloud server, not as a model but as a dataset
whose API is also pre-conﬁgured (Schema, Attributes,

etc ...). Users can use those datasets to train their model
locally in a specially appointed practical way (Xie et al.
2020). As described in ﬁgure 2, the model owner sends
the model to a cloud server asking for a speciﬁc type of
data to train it. As soon the data is available and accepted
by its owner, the model starts to train and eventually is
sent back to its owner.
For example, suppose an individual wants to train a
model for a medical diagnosis. In that case, he needs to
be part of a major hospital or constantly interact with a
doctor. Otherwise, it may be highly troublesome or difﬁ-
cult to get a dataset that would be adequate for the model
training. With data-centric, one could possess that data
and add them to the cloud server for others to beneﬁt
from and train their model by submitting requests.

Figure 2: Data-Centric Federated Learning

Related Work
This section discusses previous research papers related to
the merging of blockchain technology, the machine learning
process, and the integration of collaborative training. This
idea is considered a new but promising one, considering the
advantages it offers. Few works have been done in this area.
Generally, traditional Federated learning relies on a single
central server to train a global model. If that server fails due
to malicious behavior, all could be lost. Decentralization of
the data and the training can provide a solution.

The research presented in the paper (Harris and Wag-
goner 2019) focuses on improving machine learning model
integrity and availability through decentralization. Their
idea was to study the impact of blockchain technology over
machine learning models, focusing on decentralization to
mainly to provide sharing access to machine learning mod-
els. They used blockchain to establish a distributed build
of large datasets accessible to all the network participants.
Collaboration is a signiﬁcant feature of their framework
where they encourage contributors to improve and train a
model constantly. Providing a continuously updated model,
yet free, open and accessible is what this paper aims for.
The framework managed to create a distributed, collabora-
tive environment where people could contribute effectively
to a machine learning model using quality data. This helped
improve the model’s accuracy, as it dealt with data ma-

nipulation thanks to a well designed incentive mechanism.
The blockchain helped to create an open, secure, and with
ownership datasets market where a participant could interact
with to train models either on-chain or off-chain.
Unfortunately, it only manages to work with small models
that can be efﬁciently updated with one sample. This
approach of storing the entire dataset inside the blockchain
is not very beneﬁcial due the amount of data you can store.
This is either because the amount is limited by the protocol,
or because of the huge transaction fees you would have to
pay. Therefore, the amount of data you store has to be held
by every full node on the network. Everyone that downloads
the blockchain is downloading your piece of data as well.
Even keeping kilobytes can cost a fortune. When storing
data on the blockchain, we often pay a base price for
the transaction itself plus an amount per byte we want to
keep. If smart contracts are involved, we also pay for the
execution time of the smart contract (Hu 2021). With that
being said, this option is not used to provide a collaborative
environment to train complex models that need a large
amount of data.

The work in the paper (Ma et al. 2021) presents a secure
and reliable federated learning framework. The idea is to en-
sure the model update process in a decentralized environ-
ment. Each client using the framework could train his ma-
chine learning model and mine blocks to publish aggregat-
ing results. The blockchain will track tasks related to collab-
orative learning, such as broadcasting learning models, pub-
lishing a task, or reviewing the aggregating learning results.
Using decentralization accountability, they enable all miners
to validate the uploaded model quality. This ensures mis-
behaving detection for low-quality contribution to the fed-
erated model. They start by integrating a local model into
each training node, training and updating the model using
a global model and its data. That leads to creating a train-
ing pool, conducting a decentralized aggregation process for
the federated learning. Once the models are updated and col-
lected, the clients calculate the global model update. Eventu-
ally, each block will record the local model update, data size,
computation time, and aggregated parameters. Once regis-
tered, the model is published to the whole network. Once
published and the hash value is found, miners need to verify
the block contained aggregated results, either by comparing
with the result found in the publishing block or by using
a public dataset to justify the performance of the uploaded
model. Reward allocation is available for the client to en-
courage them to mine and verify each block. Their study
showed signiﬁcant results for dealing with federated learn-
ing single server failure and avoiding unwanted distributed
training behavior. However, the study shows that the data is
available for the clients to train their model remotely and test
its efﬁciency. This is not always the case.

Tao Wang (Wang 2018) explored the establishment of a
link between machine learning and blockchain technology
to propose a uniﬁed framework aiming to deal with machine
learning traceability and automation problems. It discussed
the usage of multiple threads to translate core machine
learning implementation instead of a single thread. They

used a three-layer architecture composed of a Server Layer,
Streaming Layer, and a Smart Contract Layer. The server
and streaming layers aim to achieve the seven machine
learning steps (Initialization, Training, Validation, Testing,
Evaluation, Serialization and Clean-up). Still, the ﬁrst is us-
ing a static handle of data (data at rest) and the second a
dynamic one (events on the ﬂy). The Smart Contract layer is
essential to deal with trustability and the automation of the
process of training the model.
This work managed to achieve a stable framework to col-
lect data using cloud technology and train machine learning
models.
Nevertheless, the usage of smart contracts can cause run-
time problems while running thousands of contracts on the
blockchain, the response time can decrease tremendously.
Each study has contributed to understanding the applica-
tion of blockchain machine learning training, either in data
availability or model training. As we mentioned previously,
they provided the ability to conduct that contribution, such
as automated model training, data availability, and verify-
ing the quality and integrity of the data. But with each work
comes limitations. We are still facing some related to the
usage of blockchain technology. One of the main problems
is data storage. The biggest problem of storing data on a
blockchain is the amount of data you can store, either be-
cause the amount is restricted by the convention or given the
immense exchange expenses you would need to pay.
One other major limitation is the need for high computa-
tion power to train a machine learning model in an efﬁcient
time matter. When we are using onchain training with smart
contracts, this usage can cause runtime problems while run-
ning thousands of contracts; the response time can decrease
tremendously and even causes network failure.
In our study, we consider not only supervised models but
also unsupervised ones with a similar incentive mechanism
(Harris and Waggoner 2019). In this matter, we address the
Etherieum (Vujicic, Jagodic, and Randic 2018) memory al-
location problem. Our alternative solution is to use the In-
terPlanetary File System (IPFS) (Benet 2019) where we will
only store on the blockchain the links of the ﬁle; this will
help reduce the cost of storing inside Etherieum and allow
people to upload as much data as they want. This will pro-
vide the ability to use diverse models since it will provide the
necessary data regarding its training. And we offer the abil-
ity to train a model in an off-chain manner. This will allow
the possibility of training models faster by avoiding network
latency and then re-uploading them into the blockchain.

Proposed approach

Distributed data is a critical element of our study since it
provides a solution to data centralization. Data-centric Fed-
erated learning architecture was a reference approach in cre-
ating our architecture. While in federated learning, we dis-
tribute the model to the data in our study, we reverse it
by spreading the data to the model. We explain in the ﬁg-
ure below 3, the difference between two common types of
learning, centralized learning and federated learning, and
our framework contribution to model learning.

Figure 3: Our contribution overview : Blockchain integration for data-centric federated learning

System Overview
In classic centralized learning, we have data centralized in a
server sent to a model for training. We spent decades using
that technique until we were presented with federated learn-
ing. To be more speciﬁc, data-centric federated learning is
the idea that a super or global oracle that is centralized in a
server and contains multiple datasets added by contributors.
Any model owner wanting to use this data could send their
model and a request to use a speciﬁc dataset for its training.
If accepted, the oracle searches for the requested dataset and
starts the training of the model. As soon as the training is
ﬁnished, the model is provided back to the model owner.
In the case of our framework, we have a model which
is stored in a decentralized environment, being the public
blockchain (Kim et al. 2020). It is therefore accessible by
all the network participants. Those same participants pro-
vide the data to the model in a collaborative approach. With
every input of data or IPFS hash, we have data aggregation
(Ramanan, Nakayama, and Sharma 2019). But before any
update to the model, the data needs to be veriﬁed to ensure
its integrity. That function is provided using our incentive
mechanism. The approved data is then sent to the model for
training and the model is updated. For each beneﬁcial de-
posit, a refund is made, and for each beneﬁcial veriﬁcation
of the inputs, a reward is offered. We are aiming more at data
distribution rather than model distribution. Because that will
help us create datasets for future usage. All of this is de-
scribed in ﬁgure 3. Our system aims to apply blockchain
advantages in machine learning; by merging The Ethereum
Blockchain and the InterPlanetary File System, we will be
able to create a safe and public market of collaborative train-
ing and sharing of machine learning models. Smart contracts
are the core of our system, where users will be able to up-
load machine learning models and IPFS hashes that relate to
datasets stored in a decentralized manner. Other users could
reach those attributes for improving them. One other feature
is the ability to make IPFS hashes available to create a data-
sharing system. Our research concerns mainly two areas of
work:

• Collaborative Federated Data centric

Where we present our strategy for reducing Blockchain stor-
age costs so we could transform it into a data sharing and
collection environment where people could have access to
various data sets.

• Collaborative Training
Aiming to encourage users to add their machine learning
models inside Ethereum blockchain via a smart contract,
where contributors could interact with it to train, either by
adding data or retrieving it and train the model in an off-
chain manner before re-uploading.

Data Collection
Composed of the Data Handler and our Incentive Mecha-
nism, those components aim to add and verify the integrity
of the data added by the user to train the model.
The Data Handler collects and adds the data to the smart
contract, which will be available to all participants of the
network.
As for the incentive mechanism, it helps us ensure the
is being distributed throw the
quality of the data that
blockchain by imposing a deposit when adding data and
rewards for contributors when verifying it.
A smart contract is created and initialized to values of the
incentive mechanism parameters. It then accepts actions
from participants and triggers payments. Adding data
requires validation from the incentive mechanism. The data
will be stored in the smart contract either in the data handler
or as an IPFS hash. Figure 4 below describes the steps of
the process of adding and collecting data to and from the
smart contract in different scenarios. It consists of four
users, a virtual wallet, the IPFS, the smart contract, and The
Ethereum Blockchain.

• Step 1: User A which is the initial user add his data ﬁle

to IPFS

• Step 2: User A retrieves the hash of the ﬁle
• Step 3: Start by providing the model, the data corre-
sponding to the model, and the test data to the smart con-
tract

• Step 4: User A deﬁnes the reward fees for contributors
• Step 5: The smart contract retrieves the amount and

stores it

• Step 6: The smart contract is stored in the blockchain
Those six steps represent ”The model initialization steps”

• Step 7: User B retrieves the IPFS hash
• Step 8: User B accesses IPFS platform and retrieves the

ﬁle corresponding to the hash

• Step 9: User C makes a deposit
• Step 10: User C adds data to the smart contract
• Step 11: User D makes a deposit, veriﬁes and ﬁxes ma-

licious data

• Step 12: User D retrieves a refund plus a reward fee

Figure 4: Data Collection Process

Figure 5: Model training process

Model Training
In order to automate the training process, users will generate
a smart contract containing the machine learning model
with all the speciﬁc details and its method of training.
When data is received, it automatically starts the required
action. When user A adds the model to the blockchain,
it becomes public to other network users. When user B
wants to contribute, there are two options; if the model
isn’t complex and could be updated with one sample, the
user could manually add the input after acquiring a deposit.
Alternatively, they needs to add the data set to IPFS, then
add that hash to the contract. For the training to occur,
attributes of the dataset are required to be similar to the
initial ones used to train the model.
Figure 5 above describes the steps of the process of training
a model in different scenarios. It consists of three users,
the smart contract, and the
a virtual wallet,
Ethereum Blockchain.

the IPFS,

The model initialization steps

• Step 7: User B downloads the model and the IPFS hash
• Step 8: User B accesses IPFS platform and retrieves the

ﬁle corresponding to the hash

• Step 9: User B trains the model on his device
• Step 10: User B uploads the model in a new smart con-

tract with references to the previous one

• Step 11: User C makes a deposit and adds data to the

model that triggers and update function

• Step 12: The smart contract starts the training of the
model using the new input and tests it using the test data

Implementation
The whole system consists of three main components: back-
end blockchain, IPFS storage, and front-end web UI. The
client front-end will send a transaction to the blockchain to
deploy a User contract for the current account. The user will
be provided with an interface to upload the required ﬁles,
machine learning model, and IPFS ﬁle. Previous research
proved that blockchain could be suitable. We used Ethereum
as our blockchain since it is public and supports smart con-
tracts. On the other hand, IPFS is a distributed storage for
our databases. The framework will store the hash of the IPFS
ﬁle.

Data Hashing The model owner breaks down the whole
dataset into several data groups by using IPFS. It will create
an IPFS object and generate a hash leading to the ﬁle ad-
dress. This will allow us to beneﬁt from large datasets with-
out the need to store them inside the blockchain.
Moreover, the need to have a different hash to the test dataset
is required in order to prevent it from being visible to the
contributors. This help avoids overﬁtting problems. We used
Sha-256 (Appel 2015) as the hashing function for creating
different hashed data groups for testing datasets. It is the
same hashing algorithm used by IPFS.

Figure 6: Hashing Process

As mentioned in ﬁgure 6, the user needs to split the the
data into two datasets, training and testing. Moreover, he
needs to run the training dataset throw the IPFS to gener-
ate the hash and eventually store it into the smart contract.
As for the testing dataset, when added to the framework, a
hash function will be generated using the SHA-256 algo-
rithm to obtain a hash value. This value will also be stored
in the smart contract.

Adding Data We call an add function that will allow the
user to add data to the machine learning model either with
simple input or as a hash of an IPFS ﬁle. This data will aim
to train the model inside the blockchain. Those inputs will be
later reviewed by contributors thanks to the incentive mech-
anism that will allow them to beneﬁt from correcting and
ensuring the data integrity. Other people could also down-
load the data and use it to train their own machine models as
described in ﬁgure 7.

Figure 8: Incentive Mechanism: Data Veriﬁcation

contract to download the model and the IPFS hash to en-
hance the collaborative training. Using the hash, they obtain
the dataset and start training the model off-chain. Finally,
they add the updated model to a smart contract and store it
in the Blockchain.

Results
Each user will have one separate individual smart contract.
They will be able to upload a machine learning model and
IPFS ﬁle containing the data used to train the model. Mean-
while, contributors will be able to access the smart contract
to retrieve the model and the IPFS ﬁle. After training and im-
proving the model, they could re-upload it inside the smart
contract and the data they used for its training as an IPFS
ﬁle. The usage of a public blockchain will provide more fre-
quency of contributors willing to validate the data integrity.
The model will remain available inside the blockchain for
future work with a view of its accuracy. This ﬂow of interac-
tion created an efﬁcient environment for collaborative learn-
ing since it managed to make datasets available for all net-
work users either with direct input inside the smart contract
when we are dealing with simple inputs or with large ones
when adding the ﬁle inside IPFS and sharing its hash in the
blockchain. Their access will be at no cost. With the pres-
ence of the incentive mechanism, we managed to reduce the
amount of ambiguous data that could be added to the model.
The deposit function makes them costly and unworthy for
malicious users. On the other hand, it encourages contribu-
tors since discovering those insufﬁcient data beneﬁts them
with ﬁnancial rewards. All of this improves the deployed
model’s accuracy and provides usable datasets to test and
train models.

Results’ Validation Analyzing ﬁgure 9, we notice that
we managed to obtain multiple inputs to our initial dataset.
At the beginning of the experiment, the dataset contained
25,000 inputs of data. After using the framework, we ob-
served that the number kept increasing until reaching 25,500
data inputs. An additional 500 data inputs were obtained in
just six days with an average of 83 new inputs every day
with only two contributors. That proves that our framework

Figure 7: Incentive Mechanism: Data Adding

Data Integrity To ensure the integrity of the data, we
implement an incentive mechanism to encourage con-
tributors and oblige each user to insert a deposit to add
data to the model to make it costly for malicious users
to disturb the model efﬁciency. It starts with the model
owner providing a deposit, reward, and time-out function. It
creates the fundamental aspect of our incentive mechanism.
The deposit corresponds to the monetary amount that a user
needs to make to add data to the smart contract to train the
model on-chain. The reward represents the amount that a
user will receive when checking the integrity of the data and
restoring it. The time-out function deﬁnes the time between
a return of the deposit and the validation of the owner.
Figure 8 describes the process of veriﬁcation. Following
a data input, a user has the opportunity to check it due to
blockchain transparency. An Interface shows all the contrib-
utors’ transactions, meaning that all added data is visible
for the owner and other contributors. If a data is wrong
or does not ﬁt the process of learning of the concerned
model, he will be allowed to change it and update the model.

Training Process The user will be provided with an in-
terface where he creates his customized smart contract. The
model owner adds his dataset inside IPFS using a frame-
work interface to store it and then retrieves its correspond-
ing hash. Next, they provide the machine learning model
with some description of the model, the data used, and the
hash of the IPFS inside the smart contract. Then, they de-
ﬁne an amount related to the data veriﬁcation reward. The
smart contract will retrieve that sum from the user’s virtual
wallet and store it inside the Ethereum Blockchain. Network
participants connect to the framework and access the smart

could add the data inputted from the contributors into the
initial training dataset. Same as the previous dataset, after

courage other contributors to verify them. Our simulation
has yield the following graph 11. Figure 11 represents each

Figure 9: IMDB dataset growth

analyzing ﬁgure 10, we notice that the amount of inputs in
the training dataset had increased from 1000 in the initial
state to 1100 input. That provides us with an additional 100
pictures in the dataset with their corresponding label.
In our framework, any user can participate and train a public

Figure 11: Agents Balance

contributor’s ether balance, a good agent, and a malicious
agent in their virtual wallet. We consider a malicious agent
as a network participant who contributes to the model train-
ing with inadequate data. We can assume that there is a neg-
ative correlation between the two ratios. It is for the fact that
after each deposit, the two agents add data to the contract,
but while verifying the data, the good agent adjusts the in-
correct data. Hence, he obtains a full refund plus a reward
amount, while the other agent doesn’t get his deposit. Each
time the malicious agent kept adding data, his balance re-
duced until almost reaching zero.
As for the accuracy, starting with the IMDB sentiment
model, it was initially trained on 20,000 of the 25,000 train-
ing data samples, with a model accuracy of 78%.
We notice that despite the presence of the malicious agent,
the presence of the IM, and a good agent, the model was
able to train inside the smart contract improving from 78%
to 82.9% level of accuracy. This proves that with this model,
our framework was efﬁcient in terms of improving the train-
ing of the model thanks to the constant contribution of data
by the contributors.
As for the second model, The Hot Dog – Not Hot Dog
Model, it was initially trained on the full dataset using the
1000 inputs. By constantly adding data and verifying its in-
tegrity, we managed to obtain the following result illustrated
in ﬁgure 12.

Figure 10: Hotdog-not hotdog dataset growth

model after providing a deposit in cases that deposit contains
false data and leads to a lack of quality.
Our Incentive Mechanism (IM) was created to deal with ma-
licious users that aim to disturb the model accuracy by pro-
viding incorrect inputs into to training dataset. By requiring
a deposit with each input and a reward mechanism with each
veriﬁcation, we aim to stop those disturbing inputs and en-

Time of Execution in seconds
2 - 3
15 - 20
2 - 3

Operation

IPFS Node Creation
Smart Contract Creation
IPFS Hash Upload
Consensus &
Ledger Update
Data Adding
Deposit Payment &
Update
Model Training
With Single Input
Model Training
With Multiple Input
Smart Contract Update
Reward Payment
Model Download
Dataset Download
* Can vary tremendously depending the size

2 - 3

1 - 2

2 - 3

1 - 2

60 - 80*

2 - 3
1 - 2
1 - 2
10 - 15*

Figure 12: Accuracy evolution for the two models

Table 1: Framework process time consumption.

We observed that those added data helped to improve the
model accuracy from an initial 59% to 63%. Proving that
our smart contract was capable of training the model and
that the provided data were beneﬁcial to the accuracy.

Performance Evaluation
Our framework is expected to deal with many users, so per-
formance and scalability are major factors. However, at this
moment, public blockchain throughput is very limited. We
use the Ethereum blockchain, allowing around 15 transac-
tions per second with 15-second blocktime. Also, (Daniel
and Tschorsch 2022) states that IPFS instances possess lim-
ited bandwidth leading to low critical scalability of IPFS.
Plus, a quantitative analysis (Shen et al. 2019) toward IPFS
I/O performance from a client perspective showed that op-
erations are performing resolving and downloading result a
bottleneck while reading a remote node by an IPFS client.
So, scalability still necessitates future improvement for a
public blockchain and IPFS to ensure trouble-free for ex-
tensive usage.

Nonetheless, when we tested our framework locally run-
ning in a single wired LAN, the execution was smooth and
rapid, as described in table 1. However, with a more signiﬁ-
cant number of nodes and a distributed wan approach, those
results could tremendously alter time execution. In the cur-
rent version, we use Ethereum as the basic framework; as we
run the experimentation locally, we didn’t consider the in-
ﬂuence of malicious node behaviors on data quality. As for
deploying the framework on the public Ethereum network,
we recommend including machine learning-based detection
of malicious Ethereum entities as described by Poursafaei et
al (Poursafaei, Hamad, and Zilic 2020).

Discussion
The study discussed here relates to creating a decentral-
ized environment for collaborative learning of machine

learning models. This research seeks to provide public ac-
cess to datasets, share them, and automate the process of
training models. The results acquired prove that leveraging
Blockchain and IPFS offer a solid case to achieving our ob-
jectives.
The public blockchain makes such intuitively possible since
it offers transparency access to stored data sets by storing
their IPFS hashes and smart contracts. We were able to de-
ploy our machine model, thus automating the training pro-
cess. In addition, IPFS allows the shared ﬁles to be decen-
tralized, and it provides a hash speciﬁc to that ﬁle to ac-
cess it. Moreover, the results showed that it is non-beneﬁcial
and costly to add ambiguous data to the smart contract with
the incentive mechanism since it keeps asking for a de-
posit with each iteration. Previous research demonstrated
that blockchain and smart contracts are favorable for train-
ing non-complex machine learning models without a higher
complex model since storing data in a public blockchain is
very costly. However, with the presence of IPFS, we could
store those data without actually storing them and that by
only storing the hash of the ﬁle, hence the training of more
complex models.
Our framework manages to create a dissent-sharing envi-
ronment of machine learning resources and an automated
training ground for all models. This is considered one of
the most considerable system contributions since we man-
aged to train highly complex machine learning models while
avoiding Ethereum storage’s high-cost.

Limitations: Some limitations constrained the method-
ological choices; with the usage of IPFS, despite the mul-
tiple datasets, the incentive mechanism recognizes the ﬁle
as only one data input, which will cost one date deposit and
one data reward for the contributors. So despite the work put
in the reward will not be signiﬁcant.
Additionally, we didn’t manage to implement the DanKu
protocol (Kurtulmus and Daniel 2018) due to dependency

issues, which cost our system not to evaluate all the updated
models inside the same smart contract, leading to creating a
new smart contract with each update of the model. Further-
more, it is beyond the scope of this study to acknowledge
the required computation power for on-chain training of the
model, which could lead to low training efﬁciency.

Conclusion

Our research contributed to creating a decentralized public
framework to enhance collaborative learning of the machine
learning model on the Ethereum blockchain. The purpose is
to provide specialists seeking contribution a network where
they could ﬁnd the right resources to accomplish their work.
Data availability is crucial to the improvement of machine
learning; hence, they aren’t accessible; thus, this framework
could be beneﬁcial for such situations. Training also is a cru-
cial part of our objective. Having the opportunity to train a
model publicly is helpful to obtain a higher efﬁciency and
accuracy rate. With their anonymous and distributed nature,
smart contracts create a marketplace of model and data shar-
ing without the need for intermediaries. With the association
of an Incentive mechanism, quality insurance could be met.
Future studies should consider improving audit mechanism
to optimize the data veriﬁcation process and eradicate any
potential corruption in the network. Also, we will investi-
gate sharing computation power inside the blockchain to en-
hance the training of highly complex models such as deep
learning since GPU needs are becoming essential in training.
Blockchain could provide a sharing computation power net-
work where users could share their computation resources
inside the blockchain via a smart contract to train on-chain
models.

References
Amiot, E.; Palencia, I.; Baena, A.; and de Pommerol, C.
2020. European Digital Sovereignty .

Appel, A. W. 2015. Veriﬁcation of a Cryptographic Prim-
itive: SHA-256. ACM Transactions on Programming Lan-
guages and Systems, 31.

Benet, J. 2019. IPFS - Content Addressed, Versioned, P2P
File System. 11.

Biderman, S.; and Scheirer, W. J. 2021. Pitfalls in Machine
Learning Research: Reexamining the Development Cycle.
arXiv:2011.02832 [cs, stat]. ArXiv: 2011.02832.

IPFS and Friends:
Daniel, E.; and Tschorsch, F. 2022.
A Qualitative Comparison of Next Generation Peer-to-
arXiv:2102.12737 [cs]. ArXiv:
Peer Data Networks.
2102.12737.

Harris, J. D.; and Waggoner, B. 2019. Decentralized and
Collaborative AI on Blockchain. In 2019 IEEE International
Conference on Blockchain (Blockchain), 368–375. Atlanta,
GA, USA: IEEE. ISBN 978-1-72814-693-5.

Hu, B. 2021. A comprehensive survey on smart contract
construction and execution: paradigms, tools, and systems.
OPEN ACCESS, 51.

and Pinciroli, C.
Flow-FL: Data-Driven Federated Learning for
in Multi-Robot Systems.

Kim, H.; Park, J.; Bennis, M.; and Kim, S.-L. 2020.
Blockchained On-Device Federated Learning. IEEE Com-
munications Letters, 24(6): 1279–1283.
Kurtulmus, A. B.; and Daniel, K. 2018. Trustless Ma-
chine Learning Contracts; Evaluating and Exchanging
Machine Learning Models on the Ethereum Blockchain.
arXiv:1802.10185 [cs]. ArXiv: 1802.10185.
Ma, C.; Li, J.; Ding, M.; Shi, L.; Wang, T.; Han, Z.; and Poor,
H. V. 2021. When Federated Learning Meets Blockchain:
A New Distributed Learning Paradigm. arXiv:2009.09338
[cs]. ArXiv: 2009.09338.
Majcherczyk, N.; Srishankar, N.;
2020.
Spatio-Temporal Predictions
arXiv:2010.08595 [cs]. ArXiv: 2010.08595.
Nakamoto, S. 2008. Bitcoin: A Peer-to-Peer Electronic Cash
System. 9.
Poursafaei, F.; Hamad, G. B.; and Zilic, Z. 2020. Detect-
ing Malicious Ethereum Entities via Application of Ma-
chine Learning Classiﬁcation. In 2020 2nd Conference on
Blockchain Research & Applications for Innovative Net-
works and Services (BRAINS), 120–127. Paris, France:
IEEE. ISBN 978-1-72817-091-6.
Ramanan, P.; Nakayama, K.; and Sharma, R. 2019. BAFFLE
: Blockchain based Aggregator Free Federated Learning.
Roh, Y.; Heo, G.; and Whang, S. E. 2019. A Survey on
Data Collection for Machine Learning: a Big Data – AI In-
tegration Perspective. arXiv:1811.03402 [cs, stat]. ArXiv:
1811.03402.
Shen, J.; Li, Y.; Zhou, Y.; and Wang, X. 2019. Understand-
ing I/O performance of IPFS storage: a client’s perspective.
In Proceedings of the International Symposium on Quality of
Service, 1–10. Phoenix Arizona: ACM. ISBN 978-1-4503-
6778-3.
Technavio. 2020. Machine to Machine Services Market by
Technology, End-user, and Geography - Forecast and Anal-
ysis 2020-2024 .
Vujicic, D.; Jagodic, D.; and Randic, S. 2018. Blockchain
technology, bitcoin, and Ethereum: A brief overview.
In
2018 17th International Symposium INFOTEH-JAHORINA
(INFOTEH), 1–6. East Sarajevo: IEEE. ISBN 978-1-5386-
4907-7.
Wang, T. 2018. A Uniﬁed Analytical Framework for
Trustable Machine Learning and Automation Running with
In 2018 IEEE International Conference on
Blockchain.
Big Data (Big Data), 4974–4983. Seattle, WA, USA: IEEE.
ISBN 978-1-5386-5035-6.
Xie, M.; Long, G.; Shen, T.; Zhou, T.; Wang, X.;
and Jiang, J. 2020. Multi-Center Federated Learning.
arXiv:2005.01026 [cs, stat]. ArXiv: 2005.01026.
Yang, Q.; Liu, Y.; Chen, T.; and Tong, Y. 2019.
Fed-
erated Machine Learning: Concept and Applications.
arXiv:1902.04885 [cs]. ArXiv: 1902.04885.

