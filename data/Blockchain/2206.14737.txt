2
2
0
2

n
u
J

9
2

]

C
D
.
s
c
[

1
v
7
3
7
4
1
.
6
0
2
2
:
v
i
X
r
a

A Consensus-Based Load-Balancing Algorithm
for Sharded Blockchains

M. Toulouse1(cid:0), H. K. Dai2, and Q. L. Nguyen1

1 School of Information and Communication Technology, Hanoi University of Science
and Technology, Hanoi, Vietnam
michel.toulouse@soict.hust.edu.vn,lam.ntq166334@sis.hust.edu.vn
2 Computer Science Department, Oklahoma State University,
Stillwater, Oklahoma 74078, U. S. A.
dai@cs.okstate.edu

Abstract. Public blockchains are decentralized networks where each
participating node executes the same decision-making process. This form
of decentralization does not scale well because the same data are stored
on each network node, and because all nodes must validate each transac-
tion prior to their conﬁrmation. One solution approach decomposes the
nodes of a blockchain network into subsets called “shards”, each shard
processing and storing disjoint sets of transactions in parallel. To fully
beneﬁt from the parallelism of sharded blockchains, the processing load
of shards must be evenly distributed. However, the problem of comput-
ing balanced workloads is theoretically hard and further complicated in
practice as transaction processing times are unknown prior to be assigned
to shards. In this paper we introduce a dynamic workload-balancing al-
gorithm where the allocation strategy of transactions to shards is peri-
odically adapted based on the recent workload history of shards. Our
algorithm is an adaptation to sharded blockchains of a consensus-based
load-balancing algorithm. It is a fully distributed algorithm inline with
network based applications such as blockchains. Some preliminary re-
sults are reported based on simulations that shard transactions of three
well-known blockchain platforms.

Keywords: Sharded blockchains · Dynamic load-balancing · Distributed
average consensus · Distributed algorithms

1

Introduction

Replication in computer systems improves fault tolerance to network failures,
computer hardware components failures, software bugs, malicious activities or
to reduce systems access latencies. Databases have developed a variety of repli-
cation techniques as these systems face most of the above issues. Blockchain
technologies extend the reach of replication to implement transparent decentral-
ized control.

The extensive use of replication in blockchains raises a whole set of familiar
problematic issues which are often cited as a barrier to the adoption of this

 
 
 
 
 
 
2

M. Toulouse, H.K. Dai and Q. L. Nguyen

new technology. Cryptocurrency platforms such as Bitcoin and Ethereum can
only process a limited number of transactions per second given that transaction
validation is replicated across all the nodes of the blockchain network. Similarly,
storage requirement grows proportionally to the # of transactions × the number
of nodes as the blockhain is identically copied on each node. Several solutions
are proposed to address these two particular issues. One of them borrows from
database systems, it is called sharding.

Sharding is a parallelization strategy for databases. Sharding partitions a
large data set into multiple databases, where each database runs on a diﬀerent
server. Access requests to the data set are directed to the database where the
requested data are stored. As the databases run on diﬀerent servers, requests can
be served in parallel. Designers of sharded blockhains aim to exploit sharding
parallelism to reduce the amount of memory needed to store blockchains and to
speedup transactions processing.

Although sharding of blockchains has only been proposed recently, there
are already several sharded blockchain protocols or fully implemented sharded
blockchain platforms. Elastico [15], 2016, has been historically the ﬁrst proposed
sharding protocol. It has been followed by several others, among them Zillica [23],
Omniledger [12], Chainspace [1], RapidChain [32], Ethereum 2.0 [3], Monoxide
[27], Ostraka [16], Harmony [22], Logos [33], SSChain [4], Stegos [20], OptChain
[17], Sharper [2]. The most widely known in the blockchain community is prob-
ably Ethereum 2.0, a sharded version of Ethereum, to be released in 2021. The
following three recent surveys [14,26,31] among others are summarizing design
strategies related to sharded blockchains.

The full sharding of a blockhain partitions the blockchain network into a
set of sub-networks, i.e. shards, in which the nodes of each shard evaluates
and stores disjoint subsets of the transactions generated by users. Known issues
about this approach are cross-shard transactions and improper workload-balance
among shards. Cross-shard transactions connect accounts which are stored in
diﬀerent shards, the evaluation of these transactions requires some form of syn-
chronization, such as atomic commit protocols, among the shards involved in
the cross-shard transactions. Load-balancing for sharded blockhains consists to
assign transactions to shards such that the sum of the transaction processing
times is distributed evenly across shards. Mathematically, load-balancing is an
NP-Hard optimization problem, it is unlikely polynomial time algorithms can
be found to solve this problem. For sharded blockchains, load-balancing has an
extra layer of complication as the processing time of transactions is unknown
prior to assigning transactions to shards.

We are only aware of three recent publications addressing the issue of load-
balancing in sharded blockchains [13,18,29]. All these papers propose centralized
algorithms to periodically balance the workload of each shard. In the present
paper, given that control decisions for blockchains are bottom-up, and achieved
through consensus, we propose a fully distributed algorithm for sharded blockchain
load-balancing problem, inline with the decentralization of blockchain designs.
Our solution is based on distributed average consensus, a class of distributed

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

3

algorithms to compute a global average of initial parameters using only local
interactions among processing nodes. Distributed average consensus has been
used for problems in diverse ﬁelds, control theory [30], multi-agent systems [10],
physics [25], distributed optimization [24] including load-balancing, [6], 1989,
where it is called diﬀusion algorithm. The diﬀusion algorithm solves the dy-
namic load-balancing problem, which is how we formulate the load-balancing
problem for sharded blockhains. The work of [6] has been extended to several
dynamic load-balancing problems, parallel processing [28], cloud computing [8],
many others. Our algorithm is based on one of these extensions [28], adapted to
the load-balancing problem for sharded blockchains from dynamic load-balancing
techniques in distributed processing.

Lastly, we have adapted two well-known scheduling algorithms for solving
the shard load-balancing problem. The ﬁrst one is the “longest processing time”
(LPT) algorithm from [9] which assigns tasks to multiprocessors in decreasing
order of their processing time. LPT minimizes the ﬁnishing time of the last exe-
cuted task. Mathematically it is an approximation algorithm, which means it is
proved the solutions return by LPT are no worst than some constant factor of the
optimal solution. Our implementation of LPT is centralized. The second algo-
rithm is the MULTIFIT algorithm [11], also a scheduling algorithm minimizing
the makespan of the schedule. MULTIFIT is also an approximation algorithm,
compared to LPT it has a better approximation factor but its asymptotic run-
ning time is slightly worst than LPT. Our implementation of MULTIFIT is also
centralized. We use the approximation factor of these two algorithms to validate
the performance of the consensus-based algorithm.

The paper is organized as follow. Next section brieﬂy provides relevant back-
ground about blockchains, introduces a classiﬁcation of sharding techniques for
blockchains and ﬁnally describes the workload problem speciﬁcally addressed in
this paper. Section 3 describes three algorithms for our workload problem. Sec-
tion 4 reports preliminary results of sharding simulations for three well known
blockchain platforms. Finally, last section summarizes the paper and provides
avenues for future investigations.

2 Problem Deﬁnition

A blockchain can be described as a network of computing nodes running the
client part of a blockhain application. The main purpose of a blockchain is to
record the transfer of assets among nodes (clients) of the network. A transfer of
assets is manifested by the creation of a ﬁle called “transaction” which has an id
(the digest of a cryptographic hash function), and which holds data describing
the transfer itself and potentially some metadata for managing the transaction.
The main processing activity of blockchain network nodes is to validate trans-
actions, as each transaction, prior to its conﬁrmation, must be independently ver-
iﬁed by all the nodes in the blockchain. Details of the veriﬁcation process vary
across blockchain platforms. Minimally, each node veriﬁes whether the client
node initiating the transaction holds the assets it transfers. The independent

4

M. Toulouse, H.K. Dai and Q. L. Nguyen

transaction veriﬁcation is based on nodes having access on their local disk to the
ﬁle of each previously posted transaction across the whole blockchain network.
Once a transaction is conﬁrmed, the transaction ﬁle is indexed in a data struc-
ture called “block”. A block can hold a ﬁnite number of transactions, once this
limit is reached, the block is closed and broadcast to the blockchain network to
be stored on local hard disk of each node. Each block holds (in its header) the
cryptographic hash of the previously stored block. Because of this cryptographic
hash, the blocks form a chain, the “blockchain”. The blockchain must have the
same sequence of blocks at every node, otherwise nodes will not be able to ver-
ify consistently ownership of the assets written for transfer in newly initiated
transactions. All the transactions stored in the blockchain are visible to any user
having an account on the blockchain.

2.1 Types of Blockchains

There are several criteria along which one can classify blockchain platforms. Two
signiﬁcant ones in regard of sharding protocols is the diﬀerence between public
versus private blockchains and how the ownership of assets is recorded in the
blockchain.

Public vs Private Blockchains Blockchain platforms can be divided into
two categories: public, permissionless versus private, permissioned. In permis-
sionless blockchains, an user only need to install the client side of the blockchain
platform to attach itself to the blockchain network, without any form of au-
thentication. Like credit cards, permissionless blockchains support applications
targeting the consumer layer of the economy, they usually have a large net-
work of nodes, Bitcoin and Ethereum, the two largest blockchain networks, are
permissionless blockchains. Private blockchains support applications at the cor-
porate level, which may demand a great level of trust among users, a high level
of access securities, and which may place restrictions on which user may access
which data in the blockchain. Private, permissioned, blockchain networks re-
strict entries, they have far fewer nodes compared to permissionless blockchains.
In general, permissioned blockchains do not face the same pressing scalability
issues as for public blockchains, they have much less transactions to process and
to store. As a consequence, there are few private sharded blockchain proposals.
In the list of sharded blockchains in Section 1, only Chainspace [1] and Sharper
[2] are permissioned platforms.

Transaction Models A second distinction among blockchains that impacts
sharding designs is how assets are recorded. Most blockchain adopts one of two
approaches: the Unspent Transaction Output (UTXO) model or the account-
based model, the blockchains adopting one of these two models are said to be
UTXO-based or account-based blockchains. In UTXO blockchains, assets are
recorded only in the transactions. In this model, transactions have an input and
an output section. Once a new transaction tx is created to transfer assets, the

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

5

input section of tx refers to the output section of existing transactions as the
source of assets to transfer. The output section of tx list the public keys part
of private keys that can unlock the assets transferred by tx. It is like someone
write you a check, you give this check to pay for a given service, if your check
is larger then the value of the service, you receive a new check for the diﬀerence
in values. Your assets sums up to the checks you hold in your hand. In terms of
UTXO blockhains, your assets are openly display on the blockchain, except it is
not possible (in principle) to link the displayed assets to any speciﬁc user.

The account-based transaction model is more intuitive. Users have anony-
mous accounts which are made up of a cryptographic pair of keys: public and
private. Like bank accounts, each account has a balance. The account balance
represents the value of the assets held by the owner of an account. Each transac-
tion transfers assets from a source account to a destination account, the assets
transfer is reﬂected in the balance of the accounts listed in the transaction.

Account-based blockchains often have two types of accounts: user accounts
and smart contract accounts. Smart contract accounts are computer programs
stored on the blockchain. User accounts and smart contract accounts are stored
on every node of the blockchain network. When a transaction involves as source
or destination a smart contract, the veriﬁcation process of such transaction re-
quires the smart contract be executed by each validating node. Similarly, the
balance of user accounts listed in a transaction is veriﬁed by each node. The
veriﬁcation of transactions could not be executed independently if the user ac-
counts or the smart contract accounts were not stored on each of the blockchain
network node.

In this paper, we address the load-balancing problem for public, permission-

less account-based sharded blockchains.

2.2 Sharding Strategies

Sharding strategies can be classiﬁed into 3 groups [19]: state sharding, transac-
tion sharding and full sharding. State sharding means that the storage capac-
ity collectively available from all the client nodes in the blockchain network is
partitioned into shards, where each shard stores a disjoint subset of conﬁrmed
transactions. In state sharding, each transaction is veriﬁed by each node of the
blockchain network. We are not aware of any state sharding protocol alone, but
state sharding is often combined with transaction sharding in full sharding pro-
tocols.

In transaction sharding, computing nodes of the blockchain network are par-
titioned into subsets, called committees, where each committee veriﬁes a disjoint
subset of transactions. Transactions that are assigned to diﬀerent committees are
evaluated in parallel. Unlike state sharding, conﬁrmed transactions are stored
on all the nodes of the blockchain network, the storage requirement in transition
sharding is the same as for un-sharded blockchains. Zilliqa [23] and Elastico [15]
implement this form of sharding. Zilliqa is an account-based blockchain. Comput-
ing nodes are partitioned into a pre-deﬁned number of committees. Transactions
are assigned to committees for processing based on the address of the account

6

M. Toulouse, H.K. Dai and Q. L. Nguyen

that initiates the transaction. For example, if there are l committees from 0 to
l −1, the last (cid:98)log2 l(cid:99)+1 bits of the initiating account address, a number between
0 and l − 1, identiﬁes the committee where the transaction is sent for process-
ing. Elastico, an UTXO transaction model, assigns transactions to committees
according to the transaction hash id. For example, if there are 8 committees, the
decimal conversion of the ﬁrst 3 bits of a transaction id deﬁnes the committee
assignment of the transaction.

In full sharding, storage and transaction veriﬁcations are sharded. Several
recent blockchain platforms/protocols apply this sharding design: RapidChain
(UTXO) [32], SSChain [4] (UTXO), Omniledger [12] (UTXO), Harmony [22]
(account-based), Monoxide [27] (account-based) and Ethereum 2.0 (account-
based). Computing nodes are assigned to committees according to rules that vary
across these sharded platforms. In full sharding for account-based blockchains,
the storage of account balances as well as smart contract codes is also sharded,
that is user accounts and smart contracts are stored only on the computing
nodes of the committee where they are assigned. Consequently, the assignment
of transactions to shards is implicitly done through accounts, i.e. transactions are
veriﬁed in the committee where the account initiating the transaction is stored.
In full sharding, each shard is responsible for generating blocks from transac-
tions conﬁrmed in the shard, for maintaining its own blockchain and for storing
a subset of the account balances. Full sharding divides the storage requirement
of a blockchain platform by the number of shards. Note that in order to defend
against security vulnerabilities, some or all the nodes forming a committee are
re-assigned periodically to other shards.

2.3 The Load-Balancing Problem for Sharded Account-Based

Blockchains

k∈Ti

The general form of the load-balancing problem is formulated as follow: assign m
tasks t1, t2, . . . , tm with respectively computing times c1, c2, . . . , cm, to n process-
ing units p1, p2, . . . , pn such to minimize the maximum load on any processing
unit. Assume that Ti is the set of tasks assigned to processing unit pi, the load on
machine pi is Ci = (cid:80)

ck, the objective function is min max{Ci | i = 1..n}.

The load-balancing problem for account-based sharded blockchains can be
modeled as follow: given a set of m accounts a1, a2, am, assign the m accounts
to n shards s1, s2, . . . , sn such to minimize the maximum load on any shard.
Let Rj be the set of transactions initiated by account aj and pj
k the processing
time of transaction k initiated by aj. The computing time cj of account aj is
the sum of the processing times pj
k of the transactions initiated by account aj,
ci = (cid:80)
pj
k. Let Ti be the set of accounts assigned to shard si, the workload
k∈Rj
of shard si is Ci = (cid:80)

ck, the objective function is min max{Ci | i = 1..n}.

The sharded load-balancing problem is not static, new accounts are con-
stantly created from existing users or new users joining the blockchain network,
and transactions are constantly initiated from the account holders. Thus the
load of shards is changing, it has to be continuously re-evaluated, which is the

k∈Ti

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

7

deﬁnition of a dynamic load-balancing problem. The computing time of shards
is divided in periods between which load-balancing is performed. The duration
of the periods could be variable based on measurements of the shards load im-
balances. In this paper, periods have ﬁxed elapse times, we name these periods
“epochs”.

The processing time of an account is not known prior to assign an account to
a shard, thus load prediction must be applied. Consider two consecutive epochs
ek and ek+1. During epoch ek, the real processing time of each account ai is
recorded in ci. The processing time ci from epoch ek is used as a prediction of
the processing time of account ai in epoch ek+1. Load-balancing is performed
before epoch ek+1 starts, the balanced workload for ek+1 is computed based on
predictions from epoch ek.

3 Algorithms

This section describes three algorithms for solving the dynamic load-balancing
problem formulated in the previous section: the diﬀusion algorithm, LPT and
MULTIFIT.

3.1 Diﬀusion Algorithm

The diﬀusion algorithm belong to a class of algorithms called distributed av-
erage consensus algorithms which compute global averages in parallel in de-
centralized networks such as ad-hoc networks, peer-to-peer networks or sensor
networks. Mathematically the network is represented as an undirected graph
where adjacency among the graph vertices stands for direct communication
links among the network computing nodes. Let G = (V, E) be such a graph
where V = {v1, v2, . . . , vn} is a set of vertices (modeling the network comput-
ing nodes) and E denotes a set of edges pairing vertices (direct communication
links). Graphs like G have an adjacency structure represented by some n × n
adjacency matrix (denoted by A here) where aij = 1 if and only if (vi, vj) ∈ E,
aij = 0 otherwise. The adjacency structure of G deﬁnes for each node vi ∈ G
a neighborhood Ni where Ni = {vj ∈ V |(vi, vj) ∈ E}. The global average com-
puted by distributed consensus is 1
i=1 xi(0)) where xi(0) is some initial value
of vertex i. The global average is obtained from the execution by each node of
the network of the following iterative algorithm:

n ((cid:80)n

xi(t + 1) = wiixi(t) +

(cid:88)

j∈Ni

wijxj(t)

(1)

where wij is a weight associated to edge (i, j). The weight values wij belong to a
n × n weight matrix W satisfying some properties such as W = W T (transpose
of W ) and others algebraic properties that are used to prove the local updates
converge asymptotically to the global average 1
i=1 xi(0)). The Metropolis-
Hasting weight matrix below satisﬁes those conditions [30]:

n ((cid:80)n

8

M. Toulouse, H.K. Dai and Q. L. Nguyen

Wij =






1
1+max(di,dj )
1 − (cid:80)
0

k∈Ni

Wik

if i (cid:54)= j and j ∈ Ni
if i = j
if i (cid:54)= j and j (cid:54)∈ Ni

(2)

where di = |Ni|.

In order to solve the shard load-balancing problem using a consensus algo-
rithm, shards must be embedded in some network topology deﬁning the neigh-
borhood of each shard. For simplicity, we assume shards are connected through
a ring network, thus a regular graph, where each shard has two neighbors. The
weight matrix is Metropolis-Hasting matrix, wherein a ring network, wij = 0.33
if aij = 1, wij = 0 otherwise. Shard processing is synchronous, divided into ﬁx
length epochs. The initial value xi(0) in the consensus model is the sum of the
processing times of shard i, its workload. Each iteration of Equation (1) com-
putes the local workload average of shard i and its neighbors. This procedure
converges locally to the global average of the shards workload.

The diﬀusion algorithm computes local load averages and the amount of
load to transfer between neighbor shards once the distributed average consensus
algorithm has converged to a workload balance among all shards. We use the
same distributed average consensus as in [6], it diﬀers slightly from Equation (1):

Loadi(t + 1) = Loadi(t) −

(cid:88)

j∈Ni

wij(Loadi(t) − Loadj(t))

(3)

where Loadi(t) is the load of shard i at iteration t. The full description of the
diﬀusion algorithm appears in the pseudo-code of Algorithm 1. This algorithm
is executed by each shard.

Algorithm 1 Diﬀusion algorithm for sharded blockchain load-balancing

Diﬀusion algorithm(i, Ni, n, W , workload of shard i)

int Loadi(0) = workload of shard i
int t = 0
ﬂoat ∆i[n] = 0
while no convergence

for (j = 0; j < n, j + +)

if j ∈ Ni

∆i[j](t + 1) = ∆i[j](t) + wij(Loadi(t) − Loadj(t))

Loadi(t + 1) = Loadi(t) − (cid:80)
t = t + 1

wij(Loadi(t) − Loadj(t))

j∈Ni

Loadi(0) is the load of shard i at the end of an epoch (equivalent of xi(0)
in the consensus model). ∆i is a vector of n entries, called transfer vector in
[28]. At each iteration t of the “while” loop, ∆i[j](t + 1) stores the load that

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

9

j∈Ni
|Ni|+1

shard i must transfer to its neighbor j such that the load of shard i at iteration
t + 1 is the local average of its load and its neighbor loads at iteration t, i.e.
Loadi(t + 1) =

, note ∆i[j] = 0 if j (cid:54)∈ Ni.

Loadi(t)+(cid:80)

Loadj (t)

Computationally, instruction Loadi(t+1) = Loadi(t)−(cid:80)

wij(Loadi(t)−
Loadj(t)) is not necessary as the load of a shard at iteration t + 1 can be com-
puted using the transfer vectors and the loads of the previous iteration. However
this instruction allows us to claim without proof that Algorithm 1 converges
based on numerous convergence proofs in the
asymptotically to
literature for iterative procedures such as Equations (1) and (3).

i=1 Loadi(0)
n

j∈Ni

(cid:80)n

Algorithm 1 converges to a state where the local averages are all approxi-
mately the same, the same as the global average, and where the loads are bal-
anced among all shards. This is however not yet the case after the ﬁrst iteration
of the algorithm. At iteration 1 of the “while” loop, the load of shard i, Loadi(1),
is the average load of shard i and its neighbors at iteration 0, i.e. Loadi(1) =
Loadi(0)+(cid:80)
. However the load of shard i is not the same as the aver-
1:
neighbors

age
load
, the loads are not balanced at iteration 1.
Loadi(1) (cid:54)=
Average loads is not equivalent to balanced loads initially. It is only once the
average loads are all the same across all shards, where they are the same as the
global average, that we have the workloads to be balanced.

shard
loadj (1))+loadi(1)

of
((cid:80)
j∈Ni

iteration

j∈Ni
|Ni|+1

Loadj (0)

|Ni|+1

and

its

at

i

For all the iterations of Algorithm 1, the transfer vector ∆i[j] always contents
the load to transfer from shard i to shard j to get local average loads among
neighbor shards. However, once the local averages are the same as the global
average, where the global average is the balanced load, the values in the transfer
vectors take another meaning, they represent the amount of load to transfer
among neighbor shards to get a balanced workload, they are the solution to the
load balancing problem.

At the conclusion of Algorithm 1, if ∆i[j] is positive, shard i must send to
shard j some load. If negative, shard i must received some load from shard j.
The behavior of Algorithm 1 is illustrated in Table 1 using an extreme case of
initial load imbalance. It consists of an instance of 5 shards 0, 1, 2, 3, 4, with
initial loads 0, 26, 0, 0, 0.

In Table 1, given an “Iters” value i, the “Shards” columns list the workload
of each shard while the “Transfer vectors” columns provide the values of the two
relevant entries of each transfer vector, for example ∆0[4] ∆0[1] is the transfer
vector of shard 0. At iteration 0 the algorithm computes the transfer vectors for
the loads obtained from the previous epoch. For example, shard 0 receives no
load from shard 4 but receives 8.58 load from shard 1 as shown in ∆0[4] ∆0[1].
We can see also that at iteration 0 shard 1 send 8.58 load to shards 0 and 3.
As a consequence of the transfers computed at iteration 0, the loads of shards 0
and 2 is 8.58 at iteration 1. We can also see that each workload at iteration 1 is
the average of the workloads at iteration 0 (modulo some rounding errors) and

10

M. Toulouse, H.K. Dai and Q. L. Nguyen

Shards
2
0

0
0

1
26

3
0
8.58 8.84 8.58 0
5.83 8.66 5.83 2.83 2.83
5.77 6.79 5.77 3.82 3.82
5.46 6.12 5.46 4.46 4.46
5.35 5.69 5.35 4.69 5.35

Iters
0
1
2
3
4
5
.
.
.
8
9
.
.
.
14
15 5.20 5.20 5.20 5.19 5.19

5.21 5.24 5.21 5.16 5.16

Transfer vectors

4 ∆0[4] ∆0[1] ∆1[0] ∆1[2] ∆2[1] ∆2[3] ∆3[2] ∆3[4] ∆4[3] ∆4[0]
0
0

0.00 -8.58 8.58 8.58 -8.58
2.83 -8.66 8.66 8.66 -8.66 2.83 -2.83
3.82 -9.60 9.60 9.60 -9.60 3.82 -3.82
4.46 -9.93 9.93 9.93 -9.93 4.46 -4.46
4.79 -10.26 10.26 10.26 -10.26 4.98 -4.98

0
-2.83
-3.82
-4.46
-4.98

0
0
0
0
0

0
0
0
0
0

0

0

5.16 -10.37 10.37 10.37 -10.37 5.16 -5.16

0

0

-5.16

5.19 -10.39 10.39 10.39 -10.39 5.19 -5.19

0

0

-5.19

Table 1. Illustration of Algorithm 1

that the average loads at iteration 1 diﬀer among shards, thus the load among
the shards is not balanced.

The diﬀusion process starts with the transfers computed at iteration 1 where
shard 0 and 2 sends respectively 2.83 load to shard 4 and shard 3. This is diﬀusion
of the load originally in shard 1 to shards which are not neighbors of shard 1.
A similar diﬀusion process takes place with the transfer vectors. Eventually the
average consensus algorithm converges to balanced loads, such as in iteration
15. At this point, as in iteration 14, transfer vectors store the value of the load
that must be transferred from each shard to its neighbors such that each shard
starts the next epoch with a balanced workload.

Algorithm 1 computes the load that must be transferred, but it does not
actually migrate the accounts that actualize these workload transfers. The next
step consists to migrate accounts for which the sum of the processing times is
equivalent to the transfer values computed by Algorithm 1. During this step, each
shard i must ﬁnd |Ni| subsets of accounts such that the sum of the processing
times of the accounts in a subset is equal to a positive ∆i[j], j ∈ Ni. Computing
these subsets is equivalent to solve the subset sum problem, a problem that is
known to be NP-Hard. We use a heuristic to compute these subsets. The accounts
in a shard i are sorted in decreasing order of their processing time. Then the
list is traversed from the largest processing time to the smallest one, when the
processing time of an account is smaller than a current positive ∆i[j] value, the
corresponding account is selected to be migrated to shard j.

It may not be possible for all shards to complete the accounts migration in
one round. The load of some shards could be smaller than the positive values
it needs to transfer to its neighbors. In an extreme case, the workload might be
concentrated in one shard i (such as shard 1 in Table 1), thus the migration of
accounts in the ﬁrst round is only possible from shard i to its neighbors. In such
extreme load imbalance, the migration of accounts may only be completed after
several rounds, where in each round accounts migration only partially fulﬁll the
vector transfers. However, according to [28], it can be shown that the theoretical

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

11

number of rounds is bounded by the diameter of the network, which is consistent
with a lower-bound result in [7], in practice we have observed that the number
of rounds is much smaller.

3.2 Centralized Algorithms

In this section we introduce two algorithms for the independent task schedul-
ing problem on multiprocessors. The canonical formulations of this problem and
load-balancing are very similar. Algorithms for load-balancing are used for solv-
ing the task scheduling and vice-versa. Assuming accounts are independent, the
load-balancing problem for sharded blockchain easily conforms to the indepen-
dent task scheduling problem.

The algorithms described in this section are well-known approximation al-
gorithms for the static independent task scheduling problem: the Longest Pro-
cessing Time (LPT) and the MULTIFIT algorithm. The two algorithms are
proposed as centralized solutions. They have been adapted to solve the dynamic
load-balancing problem for sharded blockchains.

Longest Processing Time Longest Processing Time solves load-balancing
for shards by ﬁrst sorting the accounts in decreasing order of their respective
processing times. In that order, the algorithm sequentially assigns accounts to
the shard with the smallest load so far.

The easiest way to implement the sorting phase of this algorithm in a dis-
tributed computing environment is to send the processing time of each account
to a single network node. This node sorts the processing times and migrates ac-
counts to shard if their assignment diﬀers from the one they had in the previous
epoch. According to [9], LPT has an approximation factor of ( 4
3m ) (about
1.58) of the optimal solution. In [5], the authors report a new approximation fac-
tor of ( 4
3(m−1) ). The asymptotic running time of LPT is O(n log n + n log m)
(n = number of shards, m = number of accounts).

3 − 1

3 − 1

(cid:80)m

(cid:80)m

i=1 ci
n

2 (A =

+ B = 2

MULTIFIT MULTIFIT, ﬁrst introduced in [11], is a second approximation al-
gorithm for solving the independent task scheduling problem. MULTIFIT repet-
itively solves a bin packing problem. A set of n bins (shards) are assigned with
a same capacity C = 1
) (m = number of tasks,
ci = processing time of task i). The tasks are sorted in decreasing order of their
respective processing times. In that order, the tasks are assigned into a ﬁrst bin
until a ci overload the capacity C of the current bin. Then task ti is assigned to
the next bin, this continue until all the tasks are assigned in the n bins, or until
all the n bins are full. If all the tasks have been assigned then a new bin packing
round starts with a lower capacity C (cid:48) = 1
2 (A + (B = C)) for each bin. If not all
the tasks have been assigned in the previous round, then the new round starts
with an increased capacity C (cid:48) = 1
2 ((A = C) + B). This bin packing problem is
solved for k rounds. The value of B in the last round is the length of the longest

i=1 ci
n

12

M. Toulouse, H.K. Dai and Q. L. Nguyen

schedule. The asymptotic running time of MULTIFIT is O(n log n + kn log m),
and the approximation factor is 1.22 + ( 1
2 )k. Compared to LPT, MULTIFIT
has a slightly worst asymptotic running time but it has a tighter approximation
factor.

The repeat rounds of solving the bin packing problem can only be executed
by a single node. Thus the implementation of MULTIFIT for the shard load-
balancing problem follows a similar pattern as for the LPT algorithm. After the
last round, bins are mapped to shards, the content (accounts) of each bin is
migrated to the respective mapped shard, if needed.

4 Experimentation

websites:

This section provides preliminary results from tests which simulate the sharding
of three account-based blockchains: Ethereum, Zilliqa and Binance Smart Chain.
Data for the simulations are obtained from crawling the following transactions
tracker
Zilliqa
(https://viewblock.io/zilliqa) and Binance Smart Chain (https://bscscan.
com). Ethereum 2.0, a sharded version of Ethereum, is only known at the mo-
ment of writing this paper as protocol, thus here we use Ethereum transac-
tions. The transactions tracker website for Zilliqa does not list with which shard
transactions and accounts are associated. Binance Smart chain is a non-sharded
account-based blockchain.

(https://etherscan.io/),

Ethereum

Number of transactions
Number of accounts
Number of shards
Accounts per shard (approximately) 2155
Table 2. Number of accounts and transactions per blockchains

Ethereum Binance Zilliqa
49980
49989
44511
12487
5507
21548
10
10
10
1249
551

The data collected per transaction are the following ones: transaction hash id,
block hash id where the transaction is indexed, the account source, the account
destination, the time the transaction processing started, the amount of asset
transferred in the transaction and the transaction fees. We have tracked 44511
Ethereum transactions, 49989 Binance transactions and 49980 Zilliqa transac-
tions. Table 2 lists the number of source accounts for the transactions that have
been crawled.

4.1 Sharding Simulations

The simulations only implement the most basic components of account-based
shard designs such as the assignment of accounts and transactions to shards.
The processing time of Ethereum transactions is reﬂected into the transaction

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

13

Fig. 1. Transaction fees for the 44522 Ethereum transactions

fees expressed in “gwei”, one gwei = 0.000000001 ETH [21], the native Ethereum
currency. The fees that users must pay for processing a transaction are largely
based on the number of CPU cycles spent to verify the transaction and some
other marginal considerations such as the size of the transaction. The transaction
fees expressed in ETH, as reported on the tracking website, is used here as a
measure of transaction processing times. Processing times for Zilliqa and Binance
transactions are also based on transaction fees in a similar way.

Prior to run our load-balancing algorithms, we have excluded some outlier
transactions from the crawled data. An outlier transaction is one for which the
transaction fees are disproportionally high compared to the vast majority of the
transactions. Figure 1 illustrates this ﬁltering process for Ethereum transactions
where those transactions that cost more than 0.2 ETH are excluded, 11 trans-
actions have been ﬁltered out. Outlier transactions obscure at the present stage
the comparative analysis of the performance trends for the three load-balancing
algorithms. The number of transactions listed in Table 2 is after removing out-
liers.

We simulate 10 shards for each of the three blockchain platforms. In account-
based blockchains, shards are constituted by assigning each account to a shard.
The assignment of accounts to shards can be done by users. Alternatively, ac-
counts can be assigned by the sharded platform which statically assigns accounts
to shards. Our simulation follows the last approach. The accounts from the
crawled transactions are assigned randomly and as evenly as possible to shards.
This initial distribution of accounts per shards is reported in the ﬁrst column of
charts in Figure 2, in which the ﬁrst, second and third rows stand respectively for
results from Ethereum, Binance and Zilliqa. This ﬁgure shows that the number
of accounts per shard is distributed uniformly across all shards and blockchain
platforms. The second column of charts in Figure 2 reports the number of trans-
actions per shard given the initial random assignment of accounts to shards. The
number of transactions per shard varies substantially for Binance and Zilliqa.

The simulation runs for 2 epochs. For the ﬁrst epoch, the transactions fees
of transactions originating from a same account are summed up together, this
sum represents the processing time of the corresponding account. The sum of

14

M. Toulouse, H.K. Dai and Q. L. Nguyen

Fig. 2. Shards workload and transaction distributions

the processing time of the accounts assigned to a same shard is computed, which
is the workload of the shard. The third column in Figure 2 reports the initial
workload of each shard. For Binance, the imbalance in the number of transactions
for the last shard translates in an imbalance in the workload for this same shard.

4.2 Numerical Results

The three algorithms described in Section 3 are executed at the end of epoch
1 using as input the processing time of each account as calculated for epoch
1. According to the outputs of these algorithms, accounts are re-assigned to
shards. Then the workload of each shard is computed in the same way as for
epoch 1 using the same set of transactions (after re-assigning accounts). Figures
3, 4 and 5 report the computed workloads of epoch 2 respectively for the LPT,
MULTIFIT and the diﬀusion algorithm. The ﬁrst column of charts in each of
these three ﬁgures report the number of accounts per shard while the second
column of charts report the workload per shard.

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

15

Fig. 3. Shards workload-balance using LPT algorithm

The LPT algorithm in Figure 3 shows an excellent workload-balance per
shard as well as quite even distributions of the accounts per shard for the three
sharded blockchain simulations. For the MULTIFIT algorithm, Figure 4, we can
see that the workload of shard 10 for Ethereum and Zilliqa is oﬀ a little bit.
However, the striking aspect of the MULTIFIT solutions is the irregular distri-
bution of accounts per shard. MULTIFIT sorts accounts in decreasing order of
their transaction fees. Thus the ﬁrst accounts ﬁll the bins (shards) with rela-
tively few accounts while at the low end of the sorted accounts, many accounts
are needed to ﬁll a single bin. As a result, the number of accounts in the last
shards is substantially larger than those in beginning shards.

Last, Figure 5, reports the performance of the diﬀusion algorithm. The load-
balance for the diﬀusion algorithm appears as good as LPT. This indicates that
workloads are as good as the approximation factor of the LPT algorithm. Al-
though not shown in the charts, the second phase of the diﬀusion algorithm was
completed in one round for all the tests.

The number of tests reported in this section is too small to draw general
conclusions about these algorithms. Further tests are required over larger sets
of crawled transactions, over several epochs, each epoch using a diﬀerent set
of crawled transactions. Computing times of each load-balancing algorithm will
have to be recorded and compared. Intuitively, we expect the diﬀusion algorithm

16

M. Toulouse, H.K. Dai and Q. L. Nguyen

Fig. 4. Shards workload-balance using MULTIFIT algorithm

to have better computing times as it is a parallel algorithm. Outlier transactions
would have to be included in the test sets to evaluate the robustness of the
algorithms.

However, based on Ethereum online

statistical data available at
https://bitinfocharts.com/ethereum, it is not expected that signiﬁcant di-
vergences will exist between our preliminary results and results of more extensive
simulations. Statistics about Ethereum data extracted from Table 2 and Figure
2 conform with the daily reported statics on the above website. The number
of initiated transactions per account is about 2 in Table 2, similar to the daily
average number of initiated transactions per account. Transaction fees in Figure
2, which are on average about 0.003 ETH per transaction, are quite similar to
the daily average transaction fees reported for Ethereum. Simulations conducted
with a large sample of Ethereum transactions are likely to have similar work-
load distributions as the distributions in our current tests. Unfortunately, similar
online statistics are not available for Binance and Zilliqa.

4.3 Discussion

Load-balancing adds vulnerabilities and computing/communication overheads
to the operation of sharded blockchains. The diﬀusion algorithm mitigates some
of these issues, it is fast as it runs in parallel, it is not susceptible to single point

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

17

Fig. 5. Shards workload-balance using the diﬀusion algorithm

of failure issues such as DoS attacks or other attacks aiming at controlling the
load balancing protocol executed by a single node and migration of accounts is
also performed in parallel and is local which likely cost less in terms of com-
munication. Centralized algorithms such LPT and MULTIFIT rank low under
these criteria, account processing times must be forwarded to a single server,
account scheduling to shards is computed sequentially, and the migration of ac-
counts is again under the control of a single blockchain node. However literature
reports possible slow convergence speed for consensus algorithms and possible
poor load-balancing solutions for the diﬀusion algorithm.

The convergence speed of diﬀusion and consensus algorithms has been thor-
oughly analyzed in several publications. Convergence depends on several param-
eters, two of them, the diameter of the network and the initial load imbalance
can be brieﬂy discussed here. The diameter of a network is the maximum dis-
tance between a pair of nodes, for example the diameter of a ring network is
n
2 , the diameter of a line network is n − 1. In Table 1 for a ring network of 5
nodes, the system converges to a state where the diﬀerence between the largest
and smallest load is smaller/equal 1 after only 5 iterations. The same network
topology with 10 nodes requires 17 iterations to achieve the same degree of load
balance, 51 iterations with 20 nodes. Among regular graphs, ring networks have
a relatively large diameter, it is well documented that consensus algorithms em-
bedded in such network have slow convergence. In regular graphs, the diameter

18

M. Toulouse, H.K. Dai and Q. L. Nguyen

depends on the degree of the nodes, thus sharded blockchains with large number
of shards will have to be embedded in networks where shards are more highly
interconnected.

The distribution of the initial loads has a lesser impact on the convergence
behavior of the diﬀusion algorithm. Among the three shard blockchains that
have been tested, Zilliqa has the largest initial load imbalance in absolute val-
ues (because of the diﬀerences in scale, 1 ZIL ≈ ETH0.00003182 according to
https://www.coingecko.com/en/coins/zilliqa/eth). Our tests in Figures 3
to 5 have been run using the absolute values in Figure 2 where the diﬀerence
between the largest and the smallest load in Zilliqa is closed to 300 units, com-
pared to 9 for Binance and 3 for Ethereum. Zelliqa required 36 iterations to
reach a load balancing solution with a diﬀerence smaller than 1 among all the
shards, while Binance required 12 iterations and Ethereum only 4 iterations. If
we force the load imbalance to be smaller/equal to 0.03, which is the average
cost of an Ethereum transaction, Ethereum needs 22 iterations and Binance 36.
Considering much shorter epochs and 20 shards, literature [29] reports initial
load imbalances up to 65% for sharded simulations of Ethereum. Using a similar
initial Ethereum load imbalance for 10 shards, an unbalance < 1 is achieved
after 10 iterations and 52 iterations for an unbalance smaller/equal to 0.03.

There are two main reasons that may cause our implementation of the diﬀu-
sion algorithm to return poor load-balancing solutions for sharded blockchains.
One issue is algorithm 1 can divide the load into inﬁnitesimal values while ac-
count processing times are coarser, indivisible values. In some cases there may
not exist a combination of the account processing times corresponding to the
transfer vector values computed by algorithm 1. Thus the heuristic that com-
putes subsets of accounts to migrate across shards may produce eﬀective loads
that diﬀer with the values computed by the diﬀusion load-balancing algorithm.
The second issue is related with the locality of the migration process in the
diﬀusion algorithm. Each shard can only migrate its accounts to its neighbors.
Centralized algorithms such as LPT and MULTIFIT can schedule any account in
the system to migrate to any shard. Thus centralized algorithms have a theoreti-
cal advantage. This is why in the paper we compare the load-balancing solutions
based on the diﬀusion algorithm with solutions obtained using approximation
algorithms such LPT and MULTIFIT. These comparisons ensure that the eﬀec-
tive loads resulting from the diﬀusion algorithm are within the approximation
factors of the best centralized algorithms.

5 Conclusion

This paper proposes a consensus-based dynamic load-balancing algorithm for
account-based sharded blockchains. The proposed algorithm is fully distributed,
load-balance and accounts migration is computed and executed locally and pe-
riodically by each shard. Simulation tests of sharded blockchains have been run
for three well-known blockchain platforms. Load-balancing solutions of the dis-
tributed algorithm were as good or better than two other centralized approxima-

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

19

tion algorithms that have approximation factors for solutions no further apart
than 22% above the optimal solution.

Load-balancing in isolation may not by itself translate into overall improve-
ment sharded blockchain performance. User account migration in some account-
based permissionless sharded blockchain platforms incur fees which users may
refuse to pay. Accounts migration may also increase the number of cross-shard
transactions, increasing latency from transactions initiation to conﬁrmation. In
fact, load-balancing interacts or is bound by several design parameters of sharded
blockchain. One possible future work direction will be to include new constraints
in the current optimization model to represent some of these design parameters
interacting with load-balancing in sharded blockchains. Such constraints will
model for example increase latencies for cross-shard transactions or network
communication costs for migrating accounts across shards.

Acknowledgement

This work was funded by Gia Lam Urban Development and Investment Company
Limited, Vingroup and supported by Vingroup Innovation Foundation (VINIF)
under project code VINIF.2019.DA07

References

1. Al-Bassam, M., Sonnino, A., Bano, S., Hrycyszyn, D., Danezis, G.: Chainspace: A
sharded smart contracts platform. CoRR abs/1708.03778 (2017), http://arxiv.
org/abs/1708.03778

2. Amiri, M.J., Agrawal, D., Abbadi, A.E.: Sharper: Sharding permissioned
blockchains over network clusters. CoRR abs/1910.00765 (2019), http://arxiv.
org/abs/1910.00765

3. Buterin, V.: On sharding blockchains faqs (2020), https://eth.wiki/sharding/

Sharding-FAQs

4. Chen, H., Wang, Y.: SSChain: A full sharding protocol for public blockchain with-

out data migration overhead. Pervasive Mob. Comput. 59 (2019)

5. Croce, D.F., Scatamacchia, R.: The longest processing time rule for iden-
tical parallel machines revisited. Journal of Scheduling 23(2), 163–176 (Apr
2020). https://doi.org/10.1007/s10951-018-0597-6, https://doi.org/10.1007/
s10951-018-0597-6

6. Cybenko, G.: Dynamic load balancing for distributed memory multipro-
cessors. Journal of Parallel and Distributed Computing 7(2), 279–301
(1989). https://doi.org/https://doi.org/10.1016/0743-7315(89)90021-X, https://
www.sciencedirect.com/science/article/pii/074373158990021X

7. Dai, H.K., Toulouse, M.: Lower-bound study for function computation in dis-
tributed networks via vertex-eccentricity. SN Comput. Sci. 1(1), 10:1–10:14
(2020). https://doi.org/10.1007/s42979-019-0002-3, https://doi.org/10.1007/
s42979-019-0002-3

8. Doyle, J.: Load Balancing and Rate Limiting Based Algorithms for Improving
Cloud Computing Performance. Trinity College Dublin (2012), https://books.
google.com.vn/books?id=_ifroAEACAAJ

20

M. Toulouse, H.K. Dai and Q. L. Nguyen

9. Graham, R.L.: Bounds for certain multiprocessing anomalies. The Bell Sys-
tem Technical Journal 45(9), 1563–1581 (1966). https://doi.org/10.1002/j.1538-
7305.1966.tb01709.x

10. Jadbabaie, A., Lin, J., Morse, A.: Coordination of groups of mobile autonomous
agents using nearest neighbor rules. IEEE Transactions on Automatic Control
48(6), 988–1001 (2003). https://doi.org/10.1109/TAC.2003.812781

11. Jr., G.C.E., Garey, M.R., Johnson, D.S.: An application of bin-packing to multi-

processor scheduling. SIAM J. Comput. 7(1), 1–17 (1978)

12. Kokoris-Kogias, E., Jovanovic, P., Gasser, L., Gailly, N., Syta, E., Ford,
B.: Omniledger: A secure, scale-out, decentralized ledger via sharding. In:
2018 IEEE Symposium on Security and Privacy (SP). pp. 583–598 (2018).
https://doi.org/10.1109/SP.2018.000-5

13. Kr´ol, M., Ascigil, O., Rene, S., Sonnino, A., Al-Bassam, M., Rivi`ere, E.: Shard
scheduler: object placement and migration in sharded account-based blockchains.
CoRR abs/2107.07297 (2021), https://arxiv.org/abs/2107.07297

14. Liu, Y., Liu, J., Salles, M.A.V., Zhang, Z., Li, T., Hu, B., Henglein, F., Lu, R.:
Building blocks of sharding blockchain systems: Concepts, approaches, and open
problems. CoRR abs/2102.13364 (2021), https://arxiv.org/abs/2102.13364
15. Luu, L., Narayanan, V., Zheng, C., Baweja, K., Gilbert, S., Saxena, P.:
the
A secure sharding protocol
2016 ACM SIGSAC Conference on Computer and Communications Secu-
rity. p. 17–30. CCS ’16, Association for Computing Machinery, New York,
NY, USA (2016). https://doi.org/10.1145/2976749.2978389, https://doi.org/
10.1145/2976749.2978389

for open blockchains.

In: Proceedings of

16. Manuskin, A., Mirkin, M., Eyal, I.: Ostraka: Secure blockchain scaling by node
sharding. CoRR abs/1907.03331 (2019), http://arxiv.org/abs/1907.03331
17. Nguyen, L.N., Nguyen, T.D.T., Dinh, T.N., Thai, M.T.: Optchain: Opti-
mal transactions placement for scalable blockchain sharding. 2019 IEEE 39th
International Conference on Distributed Computing Systems (ICDCS) (Jul
2019). https://doi.org/10.1109/icdcs.2019.00059, http://dx.doi.org/10.1109/
ICDCS.2019.00059

18. Okanami, N., Nakamura, R., Nishide, T.: Load balancing for sharded blockchains.
In: Bernhard, M., Bracciali, A., Camp, L.J., Matsuo, S., Maurushat, A., Rønne,
P.B., Sala, M. (eds.) Financial Cryptography and Data Security - FC 2020 Inter-
national Workshops, AsiaUSEC, CoDeFi, VOTING, and WTSC, Kota Kinabalu,
Malaysia, February 14, 2020, Revised Selected Papers. Lecture Notes in Computer
Science, vol. 12063, pp. 512–524. Springer (2020). https://doi.org/10.1007/978-3-
030-54455-3 36, https://doi.org/10.1007/978-3-030-54455-3_36

19. Qing, C., Guo, B., Shen, Y., Li, T., Zhang, Y., Zhang, Z.: A secure and
eﬀective construction scheme for blockchain networks. Secur. Commun. Net-
works 2020, 8881881:1–8881881:20 (2020). https://doi.org/10.1155/2020/8881881,
https://doi.org/10.1155/2020/8881881

20. Stegos, A.G.: Stegos: A platform for privacy applications (2019), https://stegos.

com/docs/stegos-whitepaper.pdf

21. Team, E.: Gas and fees (2021), https://ethereum.org/en/developers/docs/gas/
22. Team, H.: Harmony: Technical whitepaper, version 2.0 (2020), https://harmony.

one/whitepaper.pdf

23. Team, T.Z.: The zilliqa technical whitepaper (2017), https://docs.zilliqa.com/

whitepaper.pdf

Consensus-Based Load-Balancing Algorithm for Sharded Blockchains

21

24. Tsitsiklis, J., Bertsekas, D., Athans, M.: Distributed asynchronous deterministic
and stochastic gradient optimization algorithms. IEEE Transactions on Automatic
Control 31(9), 803–812 (1986). https://doi.org/10.1109/TAC.1986.1104412

25. Vicsek, T., Czir´ok, A., Ben-Jacob, E., Cohen, I., Shochet, O.: Novel type of
phase transition in a system of self-driven particles. Phys. Rev. Lett. 75, 1226–
1229 (Aug 1995). https://doi.org/10.1103/PhysRevLett.75.1226, https://link.
aps.org/doi/10.1103/PhysRevLett.75.1226

26. Wang, G., Shi, Z.J., Nixon, M., Han, S.: Sok: Sharding on blockchain. In:
Proceedings of the 1st ACM Conference on Advances in Financial Technolo-
gies. p. 41–61. AFT ’19, Association for Computing Machinery, New York,
NY, USA (2019). https://doi.org/10.1145/3318041.3355457, https://doi.org/
10.1145/3318041.3355457

27. Wang, J., Wang, H.: Monoxide: Scale out blockchains with asynchronous consen-
sus zones. In: 16th USENIX Symposium on Networked Systems Design and Imple-
mentation (NSDI 19). pp. 95–112. USENIX Association, Boston, MA (Feb 2019),
https://www.usenix.org/conference/nsdi19/presentation/wang-jiaping
28. Watts, J., Taylor, S.: A practical approach to dynamic load balancing.
IEEE Transactions on Parallel and Distributed Systems 9(3), 235–248 (1998).
https://doi.org/10.1109/71.674316

29. Woo, S., Song, J., Kim, S., Kim, Y., Park, S.: GARET: improving throughput us-
ing gas consumption-aware relocation in ethereum sharding environments. Clust.
Comput. 23(3), 2235–2247 (2020). https://doi.org/10.1007/s10586-020-03087-1,
https://doi.org/10.1007/s10586-020-03087-1

30. Xiao, L., Boyd, S.P., Kim, S.J.: Distributed average consensus with least-
mean-square deviation. J. Parallel Distributed Comput. 67(1), 33–46 (2007).
https://doi.org/10.1016/j.jpdc.2006.08.010, https://doi.org/10.1016/j.jpdc.
2006.08.010

31. Yu, G., Wang, X., Yu, K., Ni, W., Zhang, J.A., Liu, R.P.: Sur-
(2020).

IEEE Access

14155–14181

blockchains.

Sharding

8,

in

vey:
https://doi.org/10.1109/ACCESS.2020.2965147

32. Zamani, M., Movahedi, M., Raykova, M.: Rapidchain: Scaling blockchain via full
sharding. In: Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security. p. 931–948. CCS ’18, Association for Computing
Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3243734.3243853,
https://doi.org/10.1145/3243734.3243853

33. Zochowski, M.: The logos network (2018), https://medium.com/logos-network

