VRChain: A Blockchain-Enabled Framework for
Visual Homing and Navigation Robots

Mohamed Rahouti
CIS Dept.
Fordham Univ.
NY, USA
mrahouti@fordham.edu

Damian Lyons
CIS Dept.
Fordham Univ.
NY, USA
dlyons@fordham.edu

Lesther Santana
CIS Dept.
Fordham Univ.
NY, USA
lsantanacarmona@fordham.edu

2
2
0
2

n
u
J

8

]

O
R
.
s
c
[

1
v
3
2
2
1
1
.
6
0
2
2
:
v
i
X
r
a

Abstract—Visual homing is a lightweight approach to robot
information of a
visual navigation. Based upon stored visual
home location, the navigation back to this location can be
accomplished from any other location in which this location is
visible by comparing home to the current image. However, a
key challenge of visual homing is that the target home location
must be within the robot’s ﬁeld of view (FOV) to start homing.
Therefore, this work addresses such a challenge by integrating
blockchain technology into the visual homing navigation system.
Based on the decentralized feature of blockchain, the proposed
solution enables visual homing robots to share their visual homing
information and synchronously access the stored data (visual
homing information) in the decentralized ledger to establish the
navigation path. The navigation path represents a per-robot
sequence of views stored in the ledger. If the home location
is not in the FOV, the proposed solution permits a robot to
ﬁnd another robot that can see the home location and travel
towards that desired location. The evaluation results demonstrate
the efﬁciency of the proposed framework in terms of end-to-end
latency, throughput, and scalability.

Index Terms—Blockchain, ﬁeld of view, navigation, robot,

visual homing

I. INTRODUCTION

Visual homing is a lightweight approach to visual navigation
for a mobile robot. Using stored visual information of a target
’home’ location, visual homing can be used to navigate back to
this home location from any other location in which the home
location is visible by comparing the home image to the current
image [1]. It does not require a stored map of the environment
and can be combined with an obstacle avoidance functionality
for generality. This makes visual homing very attractive for
robot platforms with a low computational capacity such as
small UAV drones and ground robots [2]. It is also attractive
for applications in which global map information may not be
available, e.g., GPS-denied environments or rapidly changing
environments. A robot might store multiple different home
location images related to tasks or activities it needs to
perform, or the images may be transmitted to it from another
robot or camera system.

A key limitation of visual homing is that the home location
must be within the ﬁeld of view (FOV) of the robot to start
homing, and this restricts the use of the method to just the
locality of the home location. One solution to address such a
limitation is by breaking a path into “line of sight” segments,

or using a topological stored map with the edges labeled
with intermediate home images. However, this additional map
requirement may be problematic for lightweight implementa-
tions or GPS-denied or rapidly changing environments. In this
paper, we propose to address the issue of the home location not
being in the FOV by considering the case where the robot is
part of a team of robots and by leveraging camera information
from other robots or camera resources in the team.

Therefore, the research problem of this study is formulated
as follows: If a robot is tasked with travelling to a home
location, and that location is not in its FOV, then the robot
attempts to ﬁnd another robot in the team that can see the home
location and travel towards that robot’s location. To stay within
the visual homing paradigm, we do not assume that the ﬁrst
robot has access to or can navigate to the spatial coordinates
of the second robot. Instead, the two must identify a common
visual landmark that can be used as an ‘intermediate’ home
location. While this example considers only two robots and
one shared visual landmark, the approach generalizes to n
robots and n−1 landmarks, as shown in the graph in Figure 1,
which depicts a team of individual visual homing robots with
visible landmarks of which there are several shared landmarks.

Fig. 1. Graph of team of robots (triangles) with visible landmarks (ellipses)
for each shown as an edge from robot to landmark. Chain of robots (triangles)
with common visual landmarks (ellipses) shown as heavier line.

In this work, blockchain technology is adopted to provide
decentralized communication among visual homing robots.
Such a decentralized communication is trustworthy in terms of
data integrity, data conﬁdentiality, and also authentication (i.e.,

 
 
 
 
 
 
Fig. 2.

Illustration of the proposed blockchain-enabled visual homing framework.

tamper-proof data sharing among robots as they complete their
moving task) [3], [4]. The deployment of blockchain structure
in visual homing will guarantee a tamper-proof record of
robots’ transactions (i.e., individual robot’s location record)
and rapid retrieval of ﬁeld views to facilitate the per-robot
navigation task [5]. One of the most challenging parts of
the problem is the analysis of video and identiﬁcation of
common landmarks, and in this paper we focus on building
and evaluating that part of the problem.

The remaining of this paper is organized as follows. Section
I provides a background of the research problem and chal-
lenges. Section II discusses the state-of-art literature related
to our work. Section III presents the methodology and archi-
tectural design of the proposed framework, in addition to the
research and practical implications related to the framework
implementation. Next, Section IV provides the experimental
setups and key evaluation ﬁndings demonstrating the efﬁciency
of the proposed solution. Last, Section V gives the concluding
remarks of this study along with potential future plans.

II. STATE OF THE ART

Visual homing, originally developed as a model of animal
behavior [6], has been extensively studied in robotics, see [7]–
[9] for a review. Its advantages are that it is lightweight and
does not require a metric map data structure or GPS synchro-
nization. The FOV restriction has been addressed by several
authors. Steltzer’s [10] Trailmap approach uses a map structure
to link several line of sight landmarks together, allowing wide-
area navigation. However, in a changing environment, e.g.,
operating outdoors in all seasons and weathers, such a map
would need to be constantly updated and shared. Furthermore,

a global map data structure might exceed the memory capacity
for a small robot.

Our approach proposes that robots in a team communicate
and identify shared common landmarks, and that these land-
marks, which are up to date and path speciﬁc, take the place
of the intermediate line of sight landmarks in other work.
Identifying common visual imagery is as a problem in which
deep-learning methods have shown great success. For example,
YOLO [11] employs a CNN architecture with 24 convolutional
layers followed by two fully connected layers. The image is
divided into a ﬁxed-size grid, and when an object is recognized
by a grid cell, that grid is responsible for predicting the object
class probability and bounding box. This allows YOLO to
propose multiple object class matches in a single image pass.
Lyons and Petzinger [12] evaluate several combinations of
CNN-based YOLO with SIFT-Based feature recognition to
identify common landmarks for two robots in a simulated
urban landscape. They that report using Yolo to identify
candidate objects and then SIFT to compare candidates yields
improved performance over either alone. However, they pro-
pose that robots share visual information by point-to-point
transmission of imagery. That approach does not scale well
to a team and to real deployment. We propose instead using
a blockchain approach to decentralize communication among
the team.

Blockchain technology has been integrated into a broad
range of modern applications,
limited
to, connected and autonomous vehicles (CAVs), Internet of
Things (IoT), and robotics [4]. The deployment of blockchain
technology in robotic systems, such as visual homing and
navigation, can be highly useful in tackling limitations be-

including, but not

-> Transaction ID -> Hash String -> Timestamp -> Data: Field View 1 Field View 2 Field View 6Block NTransactionTransaction 1Transaction 2Transaction 2Robot R1Robot R2Work  Area 1Work  Area 2Common LandmarksDecentralized LedgerUp-to-date Visual Homing State Transactionyond decentralized/distributed decision-making and security
challenges [13].

Several notable studies in the literature integrate blockchain
technology in robotic-driven applications. For
instance,
Castello et al. [2] proposed a blockchain-based framework to
improve the security and decision-making in robotic swarm
systems, while Fernandes and Alexandre [14] developed
blockchain-enabled event management for robotic platforms
using Tezos technology. Collective decision-making in robotic
systems has further been enhanced through the deployment
of blockchain. Namely, in the work by Strobel et al. [15],
the collaborative decision-making problem is addressed in
byzantine robots through smart contract-based coordination
mechanisms.

Moreover, several research works addressed the data sharing
and data monitoring problems in robotic systems. Among
these studies, Castello et al. [16] developed RoboChain, a
blockchain framework to enhance and secure data-sharing
for human-robot interaction. In contrast, Lopes et al. [17]
proposed a systematic approach for monitoring the robot
workspace using a blockchain-enabled 3D vision mechanism.
According to recent studies [18], even though state-of-
the-art algorithms have enabled specialized teams of robots
to handle individual-speciﬁc behaviors, such as aggregation,
ﬂocking, foraging, etc., there is little to no work that integrates
blockchain technology into visual homing robotic systems.
With its low operational cost, trustworthy functions, prove-
nance, and rigorous access control, blockchain in this work
will not only provide enhancements in visual homing systems,
but also in new robotic-driven use cases and applications [19].
To the best of our knowledge, no study has considered
the deployment of blockchain technology in visual homing
and navigation systems. The proposed framework enables
individual robots in a team to efﬁciently share and identify up-
to-date common landmarks in a timely, secure, and trustworthy
manner, with low operational cost/overhead.

III. METHODOLOGY AND FRAMEWORK DESIGN

This section describes the architectural design and operabil-
ity of our proposed blockchain-enabled visual homing robotic
system.

The proposed solution allows individual robots in a visual
homing environment
to efﬁciently share and identify up-
to-date common landmarks at a low operational cost and
timely manner. Robots are required to create and add a new
transaction to the ledger as soon as they complete the move
to a new position. A transaction includes the up-to-date visual
panorama (i.e., panoramic view) for an individual team robot.
The new location (position) is deﬁned as one or more of the
following:

1) If the robot has traveled over a threshold distance from

a prior location.

2) If the current visual panorama differs by more than
a threshold amount from the panorama of the prior
location.

3) If more than a threshold amount of time has elapsed

since the prior transaction.

A robot’s transaction includes the visual panorama seen
from the new robots’ location, in addition to a unique hash
value. The hash will serve as a unique ID to characterize
each transaction in the ledger). The core of the transaction is
depicted in Figure 2 which consists of the transaction ID, hash
string, transaction timestamp, and the core of the transaction.
The core of the transaction represents the panoramic view data.
Namely, the panoramic view is a sequence (list) of joining
images with slightly overlapping ﬁelds of view to create the
panoramic view for an individual robot. Furthermore, since
the ledger consists of a series of blocks that are connected
in a chain, each block comprises the core information to be
stored, the “hash” of the information in the block, and the
“hash” of the previous block in the chain. Hashing is done
here by converting the string of the transaction’s core content
into a series of unique numbers and letters.

A. Motivating Example

A simple motivating example is shown in Figure 3. In this
example, R1 needs to deliver its load of grain to a distant barn
storage area. R1 knows what the barn looks like, and searches
in its visual panorama for a match, but cannot ﬁnd one. It
concludes that the goal location is not in view, and it must
utilize network information to travel there.

Fig. 3. Motivating example.

As discussed, all robots enter their visual panorama informa-
tion into the blockchain data structure. R1 queries panorama
information from the blockchain and identiﬁes that Robot R3
has a visual panorama that includes the goal location for R1
to transport its grain. It then checks if R1 and R3 have any
common landmarks. Unfortunately, the geographic landscape
(the ‘walls’ in Figure 3) prevent R1 from seeing anything in
common with R3. At this point, R1 knows the goal location
is in view of R3, but there is still no way for R1 to make its
way to the goal.

R1 checks if there is any panorama in the blockchain that
has a common landmark with R3. Maybe this information
should be precomputed whenever a transaction is added? In
this case R2 has a single common landmark with R3. The

problem of R1 ﬁnding its way to its goal can now be reduced
to: can R1 ﬁnd its way to the common landmark of R2 and
R3? If it can, then it could look around, and the goal location
should be in view.

R1 next checks if it has a common landmark with R2, and
in this case there is one common landmark. R1 now has a
potential path to its goal, shown in orange in Figure 3.

1) Use visual homing to the common landmark of R1
and R2; this is possible since the common landmark
is already in the view of R1 by deﬁnition.

2) When R1 reaches the common landmark, it must look
around again and identify the common landmark of R2
and R3.

3) Use visual homing to common landmark of R2 and R3.
4) When R1 reaches the common landmark of R2 and R3,

look around and identify the goal.
5) Use visual homing to the goal location.

that

Informally,

this assumes

landmarks are spatially
grouped so that if a robot approaches within some distance of
one of the landmarks, it will see the other landmarks in that
area. The approach distance needs to be chosen so that the
landmark does not occlude some or all the other landmarks.

B. Navigation Use Case

Here is a use case scenario of how a robot can get to its
goal based on the provided use case with three visual homing
robots, R1, R2, and R3 in Figure 3:

1) R1 will ﬁrst check which robot (R2 or R3 or both) can
see the goal location, and in this case, it is R3 (this
task can be easily achieved by looking in the transaction
record of the last blockchain block).

2) R1 checks if it has a common landmark with R2 by
looking in the transaction record (in the last block), and
in this case there is one common landmark.

3) R1 next checks if R2 and R3 have a common landmark
by looking in the transaction record (in the last block),
and in this case there is one common landmark.

4) Last, R1 should now know the landmark path to get to

the goal destination.

C. Panoramic State Update

As discussed earlier, the blocks are linked through hashing
using SHA256. Each block will include the previous block’s
hash, its own hash, timestamp, and a list of all transactions
broadcast by individual robots in the visual homing team. Each
time an individual robot in the team changes its location, a new
block must be created and appended to the blockchain. The
newly created block will include all unchanged transactions
(robots remained in the same location) from the previous
block, in addition to the new transactions for individual robots
that have moved to a new location. Therefore, the blockchain is
ensured to maintain the up-to-date panoramic views/states for
all individual robots in the last block appended to the ledger.

IV. EVALUATION

A similar framework to that of [12] will be used to evaluate
our contribution. Pairs of robots will be placed at random
locations across 3D simulated urban landscape with spacing 1
to 20m. The quality and amount of common landmarks will be
collected as well as the latency and throughput information for
the blockchain calculations. The next subsection will describe
this testbed in more detail, and the subsequent subsection will
present the results.

A. Testbed

The software for the common landmark recognition testbed
is written in the widely-used open-source middleware Robot
Operating System (ROS)1. The 3D simulation engine, Gazebo
2, has been integrated with ROS to allow simulation testing of
robot software. Two Pioneer P3AT robot equipped with cam-
eras are used in conjunction with the modiﬁed UCIC python
software from [12] for these experiments. The modiﬁcations
to include the blockchain usage are described below.

Figure 4 shows an example scene from our ROS/Gazebo
suburban simulation. The simulation models a 130 × 180m2
ﬂat, suburban area ﬁlled with grass, trees, buildings, vehicles
and other objects. The simulation runs on a Digital Storm
Aventum with Intel Core-i9 processor and GeForce RTX 3080
GPU.

Fig. 4. Top: Example robot pair positions showing ROS/Gazebo 3D Urban
landscape. Bottom: Panoramic views from robot cameras.

B. Performance Evaluation

In order for us to evaluate the efﬁciency of the proposed

solution, we examine the following key metrics:

• Blockchain update time: This represents the time the
system takes to update the ledger in accordance with the
FoV state changes– message transmission/broadcast time
and transaction validation.

1http://www.ros.org
2http://gazebosim.org

(a)(b)Fig. 5. Update time,
ation/validation time with 40 positions for robots R1 and R2.

including transmission time and transaction cre-

Fig. 7. Update time,
ation/validation time with 200 positions for robots R1 and R2.

including transmission time and transaction cre-

panoramic views. The shown time is mostly smaller than 0.01
ms, except in some rare cases where the latency nearly reaches
0.04 ms. These experiments demonstrate the considerably low
delay/latency in our solution associated with the blockchain
update operation, meaning that when an individual robot
moves to a new position, a new transaction containing up-
to-date panoramic views can be created and validated in an
extremely low time.

Furthermore, Figure 6 shows the retrieval time in our sim-
ulated blockchain-enabled visual homing environment using
team robots with 40, 200, and 500 different positions and
panoramic views. The plotted time is bounded by 5.5x10−5
and mostly smaller than 3x10−5 ms, except in some rare cases
where it slightly increases for a couple of the 200 and 500
positions trails. Obviously, the latency/communication delay
associated with retrieving common landmarks by individual
team robots in our blockchain-enabled visual homing system
is incomparably small in contrast to state-of-the-art blockchain
solutions.

Figure 6 also shows that the overall number of positions
(for the individual team robots) does not remarkably impact
the time needed by individual robots to retrieve panoramic
views from the blockchain. Last, Table I presents the average
delay/communication overhead associated with our blockchain
solution over simulated FoVs with varying visual homing
robot positions. The table demonstrates the signiﬁcantly low
delay (overall latency) associated with the substantial oper-
ations in the visual homing and navigation system, namely,
panoramic state update and panoramic state retrieval. It also
shows that
there has been no loss of average landmark
quality as compared to the less sophisticated communication
mechanism of [12].

C. Performance Discussion

1) Latency and Transmission Time Overhead: It is impor-
tant to note that distributed solutions for the visual homing and

Fig. 6. Retrieval time of panoramic views with 40, 200, and 500 positions.

• Panoramic view retrieval time: This denotes the time in-
dividual team robots take to retrieve a common landmark
from the ledger to proceed with the navigation step.
• Landmark quality metric: The simulation model database
is used to measure how good the common landmark is,
as deﬁned in [12].

In this evaluation setup, we examined the proposed framework
using varying simulated positions with panoramic views to
better demonstrate the system’s behavior under a small and
large number of transactions (i.e., latency/scalability trade-off).
Figure 5 depicts the blockchain update time in a simulated
visual homing environment comprising two team robots, R1
and R2, with 40 different panoramic views. As shown in this
ﬁgure, the time associated with this operation is bounded
by 1.6x10−3 ms for both visual homing robots. Similarly,
Figure 7 shows the blockchain state update time with 200 total

navigation-based robotic systems use classical approaches to
broadcast and transmit all panoramic images (of each robot’s
ﬁeld of view) to all individual team robots. Therefore, the
transmission time complexity associated with broadcasting
all panoramas is a n2 graph as the visual homing team
increases. In contrast, with a team of n individual robots, our
blockchain-enabled solution, the transmission time complexity
is incomparably smaller than n2.

The latency in blockchain frameworks may become signif-
icant if team robots in a visual homing environment are used
in cooperative navigation tasks. Hence, rapid information will
be required to orchestrate the movements of the individual
robots. Further, collisions may arise when there is a mismatch
between the current FoV state and the ledger transaction syn-
chronization. A future extension will be required to improve
the latency overhead through afﬁliation or reputation-based
approaches. Team robots belonging to the same FoV region
will not need to wait long time frames to accept/process new
transactions among themselves.

2) Scalability and Throughput: In our solution, if a large
number of robots are deployed within the visual homing and
navigation environment, the size of the ledger will be signiﬁ-
cantly extended (i.e., ledger bloat). Hence, critical parameters
such as the block and transaction size and the number of
transactions per block, must be optimized in future works [20].
The blockchain throughput limitation may remarkably impact
the FoV state update and retrieval in the case of busy networks
with a large number of team robots. A future solution can be
the deployment of parallel ledgers with optimized frequency
and block size for different FoV information.

3) Denial of Service (DoS): The DoS issue may arise upon
the occurrence of an overwhelming number of team robots
interacting with the blockchain, leading to visual homing net-
work disruption. However, the visual homing and navigation
environment is generally restricted to small/medium scale,
keeping the blockchain infrastructure and network secure
against ﬂooding vulnerabilities.

TABLE I
AVERAGE LATENCY OVERHEAD EXAMINATION OVER SIMULATED FOVS
WITH VARYING VISUAL HOMING ROBOT POSITIONS.

# Pos

AVG Ledger Up-
date Time (ms)

40
200
500

0.82998 x10−3
0.100515 x10−2
1.09 x10−3

AVG Panoramic
View
Retrieval
Time (ms)
1.79 x10−5
1.99 x10−5
1.98 x10−5

AVG Landmark
Quality

3.41
3.35
3.40

V. CONCLUSIONS

Given stored visual information of a target home location,
robot navigation back to this location can be accomplished in
a light-weight manner using visual homing – as long that is as
the home location is within the FOV, a restriction that limits
the generality of this approach. To address this limitation,
we consider the robot to be part of a team and we integrate
blockchain technology into the visual homing and navigation

system for the team. Based on the decentralized nature of
blockchain, the proposed solution enabled team robots to share
their visual homing information and synchronously access the
stored data of panoramic views to identify common landmarks
and establish a navigation path. The evaluation results demon-
strated the efﬁciency of our solution in terms of latency and
delay overhead, throughput, and scalability.

REFERENCES

[1] P. Gaussier, C. Joulain, J.-P. Banquet, S. Leprˆetre, and A. Revel,
“The visual homing problem: an example of robotics/biology cross
fertilization,” Robotics and autonomous systems, vol. 30, no. 1-2, pp.
155–180, 2000.

[2] E. Castell´o Ferrer, “The blockchain: a new framework for robotic
swarm systems,” in Proceedings of the future technologies conference.
Springer, 2018, pp. 1037–1058.

[3] M. Rahouti, K. Xiong, and N. Ghani, “Bitcoin concepts, threats, and
machine-learning security solutions,” IEEE Access, vol. 6, pp. 67 189–
67 205, 2018.

[4] A. Ali, M. Rahouti, S. Latif, S. Kanhere, J. Singh, U. Janjua, A. N. Mian,
J. Qadir, J. Crowcroft et al., “Blockchain and the future of the internet:
A comprehensive review,” arXiv preprint arXiv:1904.00733, 2019.
[5] V. Vasylkovskyi, S. Guerreiro, and J. S. Sequeira, “Blockrobot: Increas-
ing privacy in human robot interaction by using blockchain,” in 2020
IEEE International Conference on Blockchain (Blockchain).
IEEE,
2020, pp. 106–115.

[6] B. A. Carwright and T. S. Collet, “Landmark learning in bees: Exper-
iments and models,” Journal of Comparative Physiology, vol. 151, pp.
521–543, 1983.

[7] P. Nirmal and D. M. Lyons, “Homing with stereovision,” Robotica,

vol. 34, no. 12, pp. 2741–2758, 2016.

[8] F. Fu and D. Lyons, “An approach to robust homing with stereovision,”
SPIE Defense & Security 2017 Conference on Unmanned Systems
Technology XX, April 2018.

[9] D. M. Lyons, B. Barriage, and L. Del Signore, “Evaluation of ﬁeld of
view width in stereo-vision-based visual homing,” Robotica, vol. 38,
no. 5, pp. 787–803, 2020.

[10] A. Stelzer, M. Vayugundla, E. Mair, M. Suppa, and W. Burgard, “To-
wards efﬁcient and scalable visual homing,” Int. J. Robotics Research,
vol. 37, no. 2-3, 2018.

[11] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,”

arXiv, 2018.

[12] D. Lyons and N. Petzinger, “Visual homing for robot teams: Do you see
what i see?” SPIE 2020 Conference on Unmanned Systems Technology
XXIV, April 2020.

[13] I. Afanasyev, A. Kolotov, R. Rezin, K. Danilov, A. Kashevnik, and
V. Jotsov, “Blockchain solutions for multi-agent robotic systems: Related
work and open questions,” arXiv preprint arXiv:1903.11041, 2019.
[14] M. Fernandes and L. A. Alexandre, “Robotchain: Using tezos technol-

ogy for robot event management,” Ledger, 2019.

[15] V. Strobel, E. Castell´o Ferrer, and M. Dorigo, “Managing byzantine
robots via blockchain technology in a swarm robotics collective decision
making scenario,” 2018.

[16] E. Castell´o Ferrer, O. O. Rudovic, T. Hardjono, and A. S. Pentland,
“Robochain: a secure data-sharing framework for human-robot interac-
tion,” 2018.

[17] V. Lopes, N. Pereira, and L. A. Alexandre, “Robot workspace monitoring
using a blockchain-based 3d vision approach,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops, 2019, pp. 0–0.

[18] I. Afanasyev, A. Kolotov, R. Rezin, K. Danilov, M. Mazzara,
S. Chakraborty, A. Kashevnik, A. Chechulin, A. Kapitonov, V. Jotsov
et al., “Towards blockchain-based multi-agent robotic systems: Analysis,
classiﬁcation and applications,” arXiv preprint arXiv:1907.07433, 2019.
[19] U. S. Aditya, R. Singh, P. K. Singh, and A. Kalla, “A survey on
blockchain in robotics: Issues, opportunities, challenges and future
directions,” Journal of Network and Computer Applications, vol. 196,
p. 103245, 2021.

[20] Q. Zhou, H. Huang, Z. Zheng, and J. Bian, “Solutions to scalability of

blockchain: A survey,” Ieee Access, vol. 8, pp. 16 440–16 455, 2020.

