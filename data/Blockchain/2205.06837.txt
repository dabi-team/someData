Strategic Latency Reduction in Blockchain Peer-to-Peer
Networks

Weizhao Tang
Carnegie Mellon Univerisity and IC3

Giulia Fanti
Carnegie Mellon Univerisity and IC3

Lucianna Kiffer
Northeastern Univerisity

Ari Juels∗
Cornell Tech, IC3, and Chainlink Labs

2
2
0
2

y
a
M
3
2

]

R
C
.
s
c
[

2
v
7
3
8
6
0
.
5
0
2
2
:
v
i
X
r
a

ABSTRACT
Most permissionless blockchain networks run on peer-to-peer (P2P)
networks, which offer flexibility and decentralization at the expense
of performance (e.g., network latency). Historically, this tradeoff has
not been a bottleneck for most blockchains. However, an emerging
host of blockchain-based applications (e.g., decentralized finance)
are increasingly sensitive to latency; users who can reduce their
network latency relative to other users can accrue (sometimes sig-
nificant) financial gains.

In this work, we initiate the study of strategic latency reduction
in blockchain P2P networks. We first define two classes of latency
that are of interest in blockchain applications. We then show em-
pirically that a strategic agent who controls only their local peering
decisions can manipulate both types of latency, achieving 60% of
the global latency gains provided by the centralized, paid service
bloXroute, or, in targeted scenarios, comparable gains. Finally, we
show that our results are not due to the poor design of existing P2P
networks. Under a simple network model, we theoretically prove
that an adversary can always manipulate the P2P network’s latency
to their advantage, provided the network experiences sufficient
peer churn and transaction activity.

CCS CONCEPTS
• Security and privacy → Network security; • Networks → Net-
work measurement; Network algorithms;

KEYWORDS
blockchains, P2P networks, strategic manipulation

1 INTRODUCTION
Most permissionless blockchains today run on peer-to-peer (P2P)
communication networks, which offer the advantages of flexibility
and an inherently distributed nature [17, 27]. These benefits of P2P
networks typically come at the expense of network performance,
particularly the latency of message delivery [16, 17, 39]. Histori-
cally, this has not been a bottleneck because most permissionless
blockchains are currently performance-limited not by network la-
tency, but by the rate of block production and transaction confir-
mation, both of which are dictated by the underlying consensus
mechanism [12, 16, 20, 26, 44]. For this reason, network latency
has not traditionally been a first-order concern in blockchain P2P
networks, except where substantial latency on the order of many
seconds or even minutes raises the risk of forking [14].

∗This work was performed in the author’s Chainlink Labs role.

1

Emerging concerns in blockchain systems, however, are begin-
ning to highlight the importance of variations in fine-grained net-
work latency—e.g., on the order of milliseconds—among nodes in
blockchain P2P networks. These concerns are largely independent
of the underlying consensus mechanism. They revolve instead
around strategic behaviors that arise particularly in smart-contract-
enabled blockchains, such as Ethereum, where decentralized finance
(DeFi) applications create timing-based opportunities for financial
gain. Key examples of such opportunities include:
• Arbitrage: Many blockchain systems offer highly profitable op-
portunities for arbitrage, in which assets are strategically sold
and bought at different prices on different markets (or at different
times) to take advantage of price differences [28]. Such arbitrage
can involve trading one cryptocurrency for another, buying / sell-
ing mispriced non-fungible tokens (NFTs), etc., on blockchains
and/or in centralized cryptocurrency exchanges. Strategic agents
performing arbitrage can obtain an important advantage through
low latency access to blockchain transactions.

• Strategic transaction ordering: Blockchains differ from tra-
ditional financial systems in their unprecedented transparency
and their introduction of new mechanisms for influencing the
ordering of transactions. As a result, strategic agents can and do
profit from placement of their transactions in ways that exploit /
victimize other users—a phenomenon often referred to as Miner-
Extractable Value (MEV) [10, 15]. For example, strategic agents
may front-run other users’ transactions, i.e., execute strategic
transactions immediately before those of victims, or back-run
key transactions—e.g., oracle report delivery or token sales—to
gain priority when important trading opportunities arise.

Small reductions in network latency can advantage a strategic
agent performing such exploits by enabling it to observe victims’
transactions before competing strategic agents do.

• Improved block composition: Miners (or validators) rely prin-
cipally on P2P networks to observe user transactions, which they
then include in the blocks they produce. Which transactions a
miner includes in a block determines the fees it receives and thus
its profits. Therefore the lower the latency a miner experiences
in obtaining new transactions, the higher its potential profit.1
• Targeted attacks: The security of nodes in blockchain P2P net-
works may be weakened or compromised by adversaries’ discov-
ery of their IP addresses. For example, a node may issue critical
transactions that update asset prices in a DeFi smart contract.
Discovery of its IP address could result in denial-of-service (DoS)

1Receiving blocks from other miners quickly is also beneficial. To reduce forking risks,
however, miners or validators often bypass P2P networks and instead use cut-through
networks to communicate blocks to one another [8].

 
 
 
 
 
 
attacks against it. Similarly, a user who uses her own node to
issue transactions that suggest possession of a large amount of
cryptocurrency could be victimized by targeted cyberattacks
should her IP address be revealed.

As we show in this work, small differences in network laten-
cies in a victim’s transactions can help an adversary distinguish
shorter from longer paths to that victim—and with some proba-
bility ultimately discover the victim’s IP address.

There is a strong incentive, therefore, for agents to employ ap-
proaches to minimize the latency they experience in P2P networks.
In this work, we initiate the study of strategic latency re-
duction in blockchain P2P networks. We focus on techniques
by which agents in such networks can lower the latencies they
experience—particularly relative to other agents.

1.1 Types of network latency
In our investigations, we explore two types of latency that play a
key role in blockchain P2P networks: direct latency and triangular
latency (formal definitions in §3).

Direct latency refers to the latency with which messages reach
a listener node from one or more vantage points—in other words,
source latency. We consider two variants:

• Direct targeted latency is the delay between a transaction being
pushed onto the network by a specific target node (the victim)
and a node belonging to a given agent receiving the transaction,
averaged across all transactions produced by the victim. We can
view targeted latency either in terms of an absolute latency, or
a relative latency compared to other nodes receiving the same
transaction; we focus in this paper on relative latency.

• Direct global latency for a given agent’s node refers to targeted la-
tency for that node averaged over all source nodes in the network.
In other words, there is no single target victim.

Triangular latency is a second type of latency that corresponds
to a node’s ability to inject itself between a pair of communicating
nodes. Low triangular latency is motivated by (for example) a node’s
desire to front-run another node’s transactions.
We consider two forms of triangular latency:

• Triangular targeted latency refers to the ability of an agent’s
node 𝑣 to “shortcut" paths between a sender 𝑠 (e.g., the creator
of a transaction) and a receiver 𝑟 (e.g., a miner or miners). That
is, suppose 𝑠 creates a transaction 𝑚 that is meant to reach a
(set of) target nodes 𝑟 . Triangular targeted latency measures
the difference between the total delay on path 𝑠 → 𝑣 → 𝑟 and
the smallest delay over all paths from 𝑠 → 𝑟 not involving 𝑣.
Given negative triangular targeted latency, an agent can front-
run successfully by injecting a competing transaction 𝑚′ into 𝑣
upon seeing a victim’s transaction 𝑚.

• Triangular global latency refers to the triangular relative targeted
latency averaged over all source-destination pairs in the network.

1.2 Latency-reduction strategies
High-frequency trading (HFT) firms in the traditional finance in-
dustry compete aggressively to reduce the latency of their trading
platforms and connections to markets—in some cases, by mere
nanoseconds. They invest heavily in technologies that can reduce
their communication latency, such as hollow-core fiber optics [36]
and satellite links [35].

Such approaches are not a viable means of improving latency
over blockchain P2P networks, in which network topology matters
more than individual nodes’ hardware configurations. These net-
works are also permissionless, meaning that they experience high
rates of churn. Their topology changes frequently and isn’t under
the control of any one entity or even any small subset of entities.
Nodes may rely on proprietary, cut-through networks to learn
transactions and/or blocks quickly. Miners maintain private net-
works [8]. There are also public, paid cut-through networks. The
best known—and a focus of our work—is bloXroute [5].

An agent in a blockchain P2P network, however, can also act
strategically by means of local actions, namely choosing which peers
a node under its control connects to.
Peri: A recently-proposed protocol called Perigee [29] leverages
agents’ ability in blockchain P2P networks to control their peering
in order to achieve systemic, i.e., network-wide, latency improve-
ments. In the Perigee protocol, every node favors peers that relay
blocks quickly and rejects peers that fail to do so.

A key observation in our work here, however, is that Perigee-like
strategies can instead be adopted by individual strategic agents. We
2 that modify
adopt a set of latency-reduction strategies called Peri
Perigee based on this observation and use optimizations that we
introduce for the individual-agent setting.

A major question we explore in this paper is whether agents
using Peri can gain a latency advantage over other agents, e.g., in
the service of strategic goals like the four enumerated above. We
show empirically that an agent can use Peri to reduce direct and
triangular latency, both targeted and global.

1.3 Our contributions
In general, in this work, we explore techniques for an agent to
reduce direct measurement latency and triangular latency in a
blockchain P2P network using only local peering choices, i.e., those
of node(s) controlled by the agent. Overall, we find that strategic
latency reduction is possible, both in theory and in practice.
Our contributions are as follows.
• Practical strategic peering: We demonstrate empirically and
in simulation that the strategic peering scheme Peri can achieve
direct latency improvement compared to the current status quo in
the Ethereum P2P network.3 We instrument the geth Ethereum
client to measure direct measurement latency and implement
Peri. We observe direct global latency and direct targeted la-
tency reductions each of about 11 ms—over half the reduction
observed for bloXroute for direct latency and comparable to that
of bloXroute for targeted latency. Our client modifications incur

It is possible to minimize direct measurement latency without
minimizing triangular latency and vice versa. The two forms of
latency are closely related, however, as we explain in §3 and §6.

2A mischievous winged spirit in Persian lore.
3We explicitly do not consider the design of Peri to be our main contribution, as the
design is very similar to Perigee. We make a distinction between the two purely to
highlight their differing goals and implementation details.

2

no extra cost, while bloXroute is a paid latency-reduction ser-
vice. We additionally show in testnet experiments that Peri can
discover the IP address of a victim node 7× more frequently than
baseline approaches.

• Hardness of triangular-latency minimization: We explore
the question of strategic triangular-latency reduction via a graph-
theoretic model. We show that solving this problem optimally
is NP-hard, and the greedy algorithm cannot approximate the
global solution. These graph-theoretic results may be of indepen-
dent interest. We show experimentally in simulations, however,
that Peri is effective in reducing such latency.

• Impossiblity of strategy-proof peering protocols: Within
a theoretical model, we prove that strategy-proof P2P network
design is fundamentally unachievable: For any default peering
algorithm used by nodes in a P2P network, as long as the net-
work experiences natural churn and a target node is active, a
strategic agent can always reduce direct targeted latency relative
to agents following the default peering algorithm. Specifically,
with probability at least 1 − 𝜀 for any 𝜀 ∈ (0, 1), an agent can
connect directly to a victim in time 𝑂 (𝜀−1 log2 (𝜀−1)).

Overall, our results highlight that it is both feasible and inevitable
that agents will reduce their network latency in P2P blockchain
networks. Our results have implications for fairness in blockchain
applications such as decentralized finance, as they suggest ways
that traders may advantage themselves through strategic network
positioning. Our results also bear on the security of P2P nodes, as
we show how adversarial agents can discover nodes’ IP addresses
for purposes such as targeted denial-of-service attacks.

2 BACKGROUND AND RELATED WORK
For the sake of concreteness, we focus on the Ethereum network
in this work. Many of the latency-sensitive applications discussed
in §1 arise in the context of decentralized finance (DeFi) and re-
quire a blockchain platform that supports general smart contracts.
The general concepts discussed in this work (and the theoretical
analysis) apply more broadly to other blockchain P2P networks.

Network Formation. Ethereum, like most permissionless blockchains,

maintains a P2P network among its nodes. Each node is represented
by its enode ID, which encodes the node’s IP address and TCP and
UDP ports [42]. Nodes in the network learn about each other via a
node discovery protocol based on the Kademlia distributed hash
table (DHT) protocol [30]. To bootstrap after a quiescent period
or upon first joining the network, a node either queries its previ-
ous peers or hard-coded bootstrap peers about other nodes in the
network. Specifically, it sends a FINDNODE request using its own
enode ID as the DHT query seed. The node’s peers respond with
the enode IDs and IP addresses of those nodes in their own peer
tables that have IDs closest in distance to the query ID. Nodes use
these responses to populate their local peer tables and identify new
potential peers.

Transaction Propagation. When a user wants to make a trans-
action, they first cryptographically sign the transaction using a
fixed public key, then broadcast the signed transaction over the P2P
network using a simple flooding protocol [7]. Each node 𝑣, upon

3

seeing a new transaction 𝑚, first checks the validity of the transac-
tion; if the transaction passes basic validity checks, it is added to 𝑣’s
local TxPool, which consists of unconfirmed transactions. Then, 𝑣
executes a simple three-step process: (1) It chooses a small random
subset {𝑢𝑖 } of its connected peer nodes; (2) 𝑣 sends a transaction
hash 𝐻 (𝑚) to each peer 𝑢𝑖 ; (3) If 𝑢𝑖 has already seen transaction 𝑚,
it communicates this to the sender 𝑣; otherwise it responds with
a GetTx request, and 𝑣 in turn sends 𝑚 in full [42]. There are two
main sources of latency in the P2P network: (1) network latency,
which stems from sending messages over a P2P overlay of the pub-
lic Internet, (2) node latency, which stems from local computation
at each node before relaying a transaction. In this work, we treat
node latency as fixed and try to manipulate network latency.

Transaction Confirmation. After transactions are disseminated
over the P2P network, special nodes known as miners confirm
transactions from the TxPool by compiling them into blocks [33, 42].
The exact confirmation process is not important to our work; the
key observation is that when forming blocks, miners choose the
order in which transactions will be executed in the final ledger.
As discussed in §2.0.1, this ordering can have significant financial
repercussions. Miners typically choose the ordering of transactions
in a block based on a combination of (a) transactions’ time of arrival,
and (b) incentives and fees associated with mining a transaction in
a particular order. In Ethereum, each block has a base fee, which
is the minimum cost for being included in the block. In addition,
transaction creators add a tip (priority fee), which is paid to the
miner to incentivize inclusion of the transaction in a block [1, 38].
These fees are commonly called gas fees in the Ethereum ecosystem.

2.0.1 Example: The role of latency in arbitrage. Among the appli-
cations in §1, the interplay between network latency and arbitrage
is particularly delicate. To perform successful arbitrage, the arbi-
trageur must often make a front-running transaction 𝑓 immediately
before the target transaction 𝑚, i.e., the transactions 𝑓 and 𝑚 are
mined within the same block, but 𝑓 is executed before 𝑚. Tech-
niques for achieving this goal have shifted over time.

Before 2020, arbitrage on the Ethereum blockchain happened
mostly via priority gas auctions (PGAs), in which arbitrageurs
would observe a victim transaction, then publicly broadcast front-
running transactions with progressively higher gas prices [15].
The arbitrageur with the highest gas price would win. In this set-
ting, it benefits an arbitrageur to have a low triangular targeted
latency (Definition 3.3) between target source nodes (e.g., the vic-
tim, other arbitrageurs) and a destination miner, or a low direct
targeted latency (Definition 3.1) to a miner. Low latency implies
swifter reactions to competing bids on the gas price, and hence
more rebidding opportunities before the block is finalized.

In 2020, the mechanisms for arbitrage began to shift to private
auction channels like Flashbots [10], in which an arbitrageur sub-
mits a miner-extractable-value (MEV) bundle with a tip for miners
to a public relay that privately forwards the bundle to miners. A
typical bundle consists of a front-running transaction and the tar-
get transaction, where the order of transactions is decided by the
creator and cannot be changed by the miner of the bundle. Among
competing bundles (which conflict with each other by including
the same victim transaction), the one with the highest tip is mined.

The key difference from PGA is that an arbitrageur is no longer
aware of the tips of its competitors; tips are chosen blindly to
balance cost and chance of success. However, the tip is upper-
bounded for a rational arbitrageur, because the arbitraging gain is
determined by the victim transaction itself. Competing arbitrageurs
may set up a grim trigger on the percentage of the tip compared to
the arbitrage profit [15]. For instance, they may agree that the tip
must not exceed 80% of the profit, so that 20% of profit is guaranteed
for the winner. Assuming that every arbitrageur is paying the miner
with the same maximum tip, the competition collapses to one of
speed. Therefore, even with MEV bundles, network latency remains
a critical component of arbitrage success.

2.1 Related Work
Reducing P2P network latency has been a topic of significant in-
terest in the blockchain community. There have been two relevant
classes of work for reducing latency: (1) decentralized protocol
changes, and (2) centralized relay networks.

Decentralized Protocol Changes. A number of decentralized pro-
tocols have been proposed to reduce the latency of propagating
blocks and/or transactions. Several of these focus on reducing band-
width usage, and reap latency benefits as a secondary effect. For
example, Erlay [34] uses set reconciliation to reduce bandwidth
costs of transaction propagation, and Shrec [21] further designs a
new encoding of transactions and a new relay protocol. These ap-
proaches are complementary to our results, which are not unique to
the broadcast protocol or to the encoding scheme for transactions.
Another body of work has attempted to bypass the effects of
latency and peer churn on the topology of the P2P network by cre-
ating highly-structured networks and/or propagation paths while
maintaining an open network. The Overchain protocol [11] pro-
poses a hypercubic overlay P2P network able to bootstrap new
nodes in a decentralized manner while handling high network
churn. In [40], the authors compare information dissemination net-
work performance of various network topologies and find that a
hypercube performs best in propagation delay and resilience to
adversaries. Another protocol, KadCast [37], uses a Kademlia struc-
tured overlay network to create routes for block and transaction
propagation to reduce complexity and overhead of flooding mes-
sages on the network. Such highly-structured networks would be
resilient to simple peering-choice strategies like Peri.

Centralized Relay Networks. There have been multiple efforts to
build centrally-operated, geographically-distributed relay networks
for blockchain transactions and blocks. These include bloXroute
[5], Fibre [8], Bitcoin Relay Network [41], and the Falcon network
[41]. In this work, we evaluate the latency reduction of centralized
services, using bloXroute as a representative example that operates
on the Ethereum blockchain. Nodes in the Ethereum P2P network
connect to the bloXroute relay network via a gateway, which can
either be local or hosted in the cloud [2]. BloXroute offers different
tiers of service, which affect the resources available to a subscriber.

3 MODEL
We begin by precisely defining direct and triangular latency, both
global and targeted variants. In Table 1, we show the relationship

Target node(s)

Agent node(s)

Agent-initiated connections

Targeted

Global

Direct
Latency

Optimized
by

Directly connecting to target
(§3.1)

SS-ASPDM approximation
algorithm [31]

Triangular
Latency

Optimized
by

Directly connecting to
targets (§3.2)

Unknown (§6.1)

Main
Challenge

Don’t know the IP addresses
of targets (§7.2.1)

Don’t know the graph
topology, computational
hardness of algorithms (§3.1,
§6.1)

Table 1: Summary of challenges and algorithms for optimiz-
ing direct and triangular latency, both targeted and global.

between these metrics, and summarize what is currently known
(and unknown) about how to optimize them. In this figure, solid
red nodes represent a strategic agent. Blue striped nodes represent
target nodes in targeted latency metrics. Red thick lines represent
the edges (peer connections) that can be formed by the agent to
attempt to optimize its network latency.

We model the P2P network N as a (possibly weighted) graph
(V, E). Let 𝑑 N (𝑠, 𝑡) denote the shortest distance between 𝑠 and 𝑡
over a graph N . Each individual node 𝑣 ∈ V can create a message
𝑚 and broadcast it to the network. The message 𝑚 traverses all
the nodes in V following the network protocol, which allows an
arbitrary node to forward the message 𝑚 upon receipt to a subset
of its neighbors.

A participant of the P2P network has two classes of identifiers:
Class 1. (Network ID) An ID assigned uniquely to each of its nodes.
For example, each node in the Ethereum P2P network is
assigned a unique enode ID including the IP address. For
simplicity, we let V denote the ID space, and a node 𝑣 ∈ V
represents an ID instance.

Class 2. (Logical ID) An ID that is used to identify the creator or
owner of a message (e.g., the public key of a wallet). We
let W denote the ID space.

In typical blockchain networks, a participant may own multiple
instances of both classes. In our analysis, we assume there exists a
mapping 𝑓 : W → V from a logical ID to a network ID for simplic-
ity. Our strategic agent can identify the signer 𝑤 of a transaction,
but is unaware of the mapping 𝑓 , and hence cannot peer to the
transaction sender 𝑣 = 𝑓 (𝑤) until it uncovers 𝑣.

We assume the source nodes of network traffic 𝑆 follow a distri-
bution S with support V, i.e., S specifies a probability distribution
over transaction emissions by nodes in V. We let a random variable
Λ(𝑎) denote the end-to-end traveling time of a message to 𝑎 from

4

asastastasta random source 𝑆 ∼ S. Further, we let Λ𝑣 (𝑎) denote the random
variable Λ(𝑎)|𝑆 = 𝑣, representing end-to-end traveling time of a
message from a single source 𝑣 to 𝑎.

This problem is called single-source average shortest path dis-
tance minimization (SS-ASPDM) [31]. It is NP-hard, but has an
𝛼-approximate algorithm [31].

3.1 Direct Latency Metrics
We define direct targeted latency in Def. 3.1 as the expectation of
the single-source message travelling time Λ𝑣.

Definition 3.1. In a P2P network N , the direct targeted latency

of node 𝑎 with respect to a target node 𝑣 is defined as:

𝐿𝑣 (𝑎) ≜ E [Λ𝑣 (𝑎)] .

Reducing direct targeted latency can help with applications such
as targeted attacks or front-running a specific set of victim nodes.

We define direct global latency as follows.

Definition 3.2. In a P2P network N , the direct global latency

of node 𝑎 is defined as:

𝐿(𝑎) ≜ E [Λ(𝑎)] .

Note that 𝐿(𝑎) = E𝑆∼S [𝐿𝑆 (𝑎)], which means the direct global
latency equals the average direct targeted latency weighted by
traffic source distribution S. In the absence of a meaningful source
node distribution S, we let S be the uniform distribution Unif (V)
over the nodes in V. Reducing direct global latency can help with
applications such as improving block composition for miners, or
observing arbitrage opportunities across the network as a whole.

3.1.1 Optimizing Direct Latency. We can consider the problem
of latency reduction by an agent as an optimization problem. As
a simplification to provide basic insights, we assume the network
topology is fixed, and the end-to-end delay of an arbitrary message
from source 𝑠 to destination 𝑡 is proportional to 𝑑 (𝑠, 𝑡), which de-
notes the distance between 𝑠 and 𝑡 on the network. We also assume
the agent has a budget of peer links 𝑘 ≥ 1. Then, we aim to solve
the following problem to optimize direct targeted latency:

minimize 𝐿𝑣 (𝑎)
subject to

|{𝑦 | (𝑎, 𝑦) ∈ E}| ≤ 𝑘.

(1)

The solution to optimization (1) is trivial under our assumption
that 𝑑 (𝑠, 𝑡) ≤ 𝑑 (𝑠, 𝑟 ) + 𝑑 (𝑟, 𝑡) for all 𝑟, 𝑠, 𝑡 ∈ V. The agent only
needs to add the target 𝑣 as a peer in order to achieve the minimum
targeted latency, as stated in Table 1. However, achieving this is
not necessarily trivial: an agent may not know 𝑣’s network address
(e.g., IP address), even if 𝑣’s logical address is known. We discuss
this further in §7.2.

The situation is very different with the optimization problem for
global latency reduction, which we formulate as follows. Let 𝑤𝑣
denote the probability of 𝑣 being a target source, i.e., a source of
messages of interest to the agent. The optimization problem is:

minimize 𝐿(𝑎) =

∑︁

𝑤𝑣𝑑 (𝑣, 𝑎)

subject to

𝑣 ∈V−{𝑎 }
|{𝑦 | (𝑎, 𝑦) ∈ E}| ≤ 𝑘.

3.2 Triangular Latency Metrics
Triangular latency is motivated by applications related to front-
running in P2P networks. It is defined with respect to one or more
pairs of target nodes, where a pair of target nodes includes a source
node 𝑠 and destination node 𝑡. For example, consider Table 1 (Tri-
angular Targeted Latency), and let 𝑠 represent the creator of a
transaction and 𝑡 a miner. The goal of agent node 𝑎 is to establish a
path 𝑠 → 𝑎 → 𝑡 with lower travel time than any path on N from 𝑠
to 𝑡 excluding 𝑎. It can do so by adding edges to the P2P network,
which are shown in red in Table 1; note that the (shortest) path
from 𝑠 to 𝑎 to 𝑡 is four hops, whereas the shortest path from 𝑠 to 𝑡
excluding 𝑎 is five hops. The existence of such a path enables 𝑎 to
front-run 𝑠’s transactions that are mined by 𝑡.

We let 𝐿′

𝑢 denote the direct targeted latency with respect to 𝑢
on the network excluding the agent 𝑎 and its incident edges. As a
front-runner, 𝑎 aims to satisfy the following condition:

𝐿′
𝑠 (𝑡) > 𝐿𝑠 (𝑎) + 𝐿𝑡 (𝑎).
Here we assume the symmetry of targeted latency, i.e., 𝐿𝑡 (𝑎) =
𝐿𝑎 (𝑡). The left-hand side 𝐿′
𝑠 (𝑡) is a constant, while the right-hand
side depends on the neighbors of 𝑎 over the P2P network N =
(V, E); we define this quantity as triangular targeted latency.

(2)

Definition 3.3. In a P2P network N , the triangular targeted
latency of node 𝑎 with respect to a target pair (𝑠, 𝑡) is defined as
𝐿𝑠,𝑡 (𝑎) ≜ 𝐿𝑠 (𝑎) + 𝐿𝑡 (𝑎).

As before, we can optimize this metric subject to a budget con-
straint on the number of peer connections maintained by the agent:

minimize 𝐿𝑠,𝑡 (𝑎)
subject to

|{𝑦 | (𝑎, 𝑦) ∈ E}| ≤ 𝑘.

(3)

The optimal strategy for solving (3) is again trivial: the agent should
connect to both 𝑠 and 𝑡 (Table 1). Hence, to optimize for a single
source and destination, it is key to find 𝑠 and 𝑡 on the network and
to ensure (2) holds. We discuss this further in §7.

For the rest of the section, we instead consider an agent who
attempts to manipulate the global triangular latency and capture
front-running opportunities over the entire network.

We start with a logical, but ultimately unsatisfactory, definition of
triangular global latency. Let 𝑄 denote the set of source-destination
pairs of network traffic. A front-running agent might naïvely try
to optimize the following quantity:
𝐿𝑄 (𝑎) ≜ ∑︁
(𝑠,𝑡 ) ∈𝑄

𝐿𝑠 (𝑎) + 𝐿𝑡 (𝑎).

𝐿𝑠,𝑡 (𝑎) =

(𝑠,𝑡 ) ∈𝑄

∑︁

(4)

This is a variation of the SS-ASPDM problem discussed previ-
ously in §3.1. Note, however, that an agent with minimal aggregated
pairwise triangular latency 𝐿𝑄 may not gain maximum profit from
front-running. To gain profit, the agent needs to ensure the inequal-
ity (2) holds for pairs (𝑠, 𝑡) not necessarily that right-hand side in (4)
is minimized.

We thus define the following proxy metric for triangular global
latency, which instead measures the quality of peering choices

5

made by front-runners. Intuitively, the metric counts the number
of node pairs (𝑠, 𝑡) that an adversarial agent can shortcut, and it is
desired to be as high as possible. For example, in Table 1 (Triangular
Global Latency), the agent 𝑎 is allowed to add three edges to the
network, and its goal is to select those edges so as to shortcut the
maximum number of node pairs (𝑠, 𝑡) from network N .

Definition 3.4. Let 𝑎 ∉ V denote an adversarial agent node, and
𝑈 ⊆ V the set of 𝑎’s peers. N ′ = (V ′, E ′) represents the network
modified by the agent 𝑎, where V ′ = V ∪ {𝑎} and E ′ = E ∪
(cid:208)𝑢 ∈𝑈 (𝑢, 𝑎). Let S, T ⊆ V be the sets of sources and destinations,
respectively. The agent node 𝑎 has a static distance penalty 𝜏, which
means that it can only successfully front-run a source-destination
pair (𝑠, 𝑡) if the path 𝑠 → 𝑎 → 𝑡 is at least 𝜏 units shorter than
the shortest path 𝑠 → 𝑡 on N that does not pass through 𝑎. The
adversarial advantage is defined as

𝐴N (𝑈 ) =

∑︁

(cid:18)

𝑠 ∈S,𝑡 ∈T

I(cid:2)𝑑 N′ (𝑠, 𝑡) + 𝜏 < 𝑑 N (𝑠, 𝑡)(cid:3)+

· I(cid:2)𝑑 N′ (𝑠, 𝑡) + 𝜏 = 𝑑 N (𝑠, 𝑡)(cid:3)

(cid:19)

.

(5)

1
2

In practice, T is the set of miners, and 𝜏 ∈ [0, ∞) is a parameter
that depends on the computational capabilities of the agent, as
well as randomness in the network. Ideally, we assume N is uni-
formly weighted. We also assume the weights of adversarial links
𝑑 N′ (𝑢, 𝑎) = 0 for all 𝑢 ∈ 𝑈 , so that we have maximum flexibility in
controlling the time costs over these links with parameter 𝜏.

3.3 Summary
In summary, existing latency metrics are informative as they ex-
press natural strategic-agent goals. They are difficult to optimize
directly, however, due to a combination of strong knowledge re-
quirements about the P2P network, high computational complexity,
and/or assumptions about the stationarity of the underlying net-
work. For example, to optimize global latency, we require two key
assumptions that are unrealistic for P2P networks: the network
topology should be static and the agent should know both the net-
work topology and traffic patterns. To optimize targeted latency,
we require knowledge of the target node’s network ID, i.e., its IP ad-
dress; this is often unknown in practice. These challenges motivate
the Peri algorithm in §4, which avoids all the above assumptions.

4 DESIGN
As we saw in §3, optimizing network latency requires knowledge
of the network topology and/or node network addresses, which are
unknown for typical P2P networks. Instead, an agent is typically
only aware of its own peers, i.e., its neighbors in the network. In this
section, we present the Peri peering algorithm to account for this
observation, and achieve reduced (if not necessarily optimal) latency.
Peri is a variant of the Perigee [29] algorithm, which was initially
proposed as a network-wide technique for reducing transaction
broadcast latency.

transactions and blocks. The Perigee algorithm is presented in Alg.
1. Lines that are highlighted in red are specific to Peri (§4.2).

Input: Number of peers 𝐾 kept after each iteration, maximum peer

count 𝑁 , length of period 𝑇 , score function 𝜙

Result: Peer set 𝑃 at each period ℎ = 1, 2, · · ·

1 𝑃 ← ∅, 𝐵 ← ∅ ;
2 Run Thread peer_manager():
3

while true do

// 𝐵 is a blocklist of evicted peers

4

5

6

Hang and wait for next peer;
𝑣 ← Random sampled node from V − 𝐵;
if |𝑃 | < 𝑁 then

𝑃 ← 𝑃 ∪ {𝑣 } ; // add peer when a slot is available

// peer_manager adds peers to 𝑃 when sleeping
// 𝑒 is the number of peers to evict

7
8 for ℎ ← 1, 2, · · · do
Sleep 𝑇 ;
9
𝑒 ← 𝑁 − 𝐾 ;
Init score map Φ;
for 𝑝 ∈ 𝑃 do

10

11

12

13

14

15

16

17

18

19

20

if not is_excused(𝑝) then

Φ(𝑝) ← 𝜙 (𝑝) ;

else

𝑒 ← 𝑒 − 1;

if 𝑒 > 0 then

𝑃 ← 𝑃 − largest_n_elements(Φ, 𝑒);
𝐵 ← 𝐵 ∪ largest_n_elements(Φ, 𝑒) ;

Output (ℎ, 𝑃 );

Algorithm 1: Perigee [29]/ Peri. Red text denotes parts that
are specific to Peri. Note: is_excused is a predicate that is true
when peer-delay information is insufficient to make a peering
decision. 𝜙 (𝑣) equals the average transaction-delivery delay
of 𝑣 with respect to the best peer, as defined in Eqn. (6). It
incorporates design choices highlighted in §4.2.

At a high level, Perigee requires every node in the network to
assign each of its peers a latency score and periodically tears down
connections to peers with high scores. In our context, the latency
score represents the latency with which transactions are delivered;
we want this score to be as low as possible. Over time, Perigee causes
nodes to remain connected to low-latency peers, while replacing
other peers with random new ones. Roughly speaking, [29] shows
that Perigee converges to a topology that is close to the optimal
one, in the sense of minimizing global broadcast latency.

More precisely, Perigee divides time into periods. Let 𝑢 denote a
given node in the network and 𝑀𝑣 denote the set of all transactions
received by 𝑢 in the current period from a current peer 𝑣. For a given
transaction 𝑚, let 𝑇 (𝑚) denote the time when 𝑚 is first received
by 𝑢 from any of its peers and 𝑇𝑣 (𝑚) denote the time when 𝑚 is
received by 𝑢 specifically from 𝑣. We define 𝑇𝑣 (𝑚) = ∞ if 𝑣 did not
deliver 𝑚 during the current period.

In every period, each node 𝑢 evaluates a score function 𝜙 over

each of its peers 𝑣, defined as

𝜙 (𝑣) ≜ ∑︁
𝑚 ∈𝑀𝑣

1
|𝑀𝑣 |

min (cid:110)

𝑇𝑣 (𝑚) − 𝑇 (𝑚), Δ(cid:111)

.

(6)

4.1 Perigee
Perigee was introduced in [29] as a decentralized algorithm for
generating network topologies with reduced broadcast latency for

Here, Δ is a parameter used by Perigee as an upper bound on
measured latency differences. Its effect is to bound the influence of
outliers on score-function computation.

6

For a node 𝑢, the score function 𝜙 (𝑣) may be viewed as capturing
the average over all transactions 𝑚 of the difference in latency
between delivery of 𝑚 by 𝑣 and that by the peer from which 𝑢 first
received 𝑚. In other words, 𝜙 (𝑣) may be viewed as the average
slowdown imposed by 𝑣 with respect to the fastest delivery of
transactions to 𝑢.

We briefly explain one element of the algorithm: the procedure
peer_manager is an asynchronous thread (or set of threads) that
handles peer connections on the P2P network, including accept-
ing incoming peer requests, requesting nodes for connection and
dropping peers. Ideally, it can randomly sample nodes from the
entire network, gradually add peers when the peer count is under
the maximum, and keep connections with specific nodes (targets).
Hence, while the main thread sleeps, peer_manager expands the
peer set 𝑃 until it reaches its maximum size 𝑁 .

4.2 Peri
Although Perigee was designed to be deployed by all network nodes
to improve broadcast latency, we observe that the same ideas can
be applied by a single agent to improve their observed direct and
triangular latency. In this section, we describe Peri, which is a slight
modification of Perigee enabling an agent to control their direct
and triangular latency. Although Peri does not differ from Perigee
in its basic approach, we give them different names to differentiate
their usage and certain key implementation choices. Again, the
steps unique to Peri are highlighted in Red in Alg. 1. Peri will be a
key baseline for all of our experiments in §5 and §6.

The main differences between Perigee and Peri are:

(a) Goal: Peri is meant to be applied by a single node to advan-
tage it over other nodes, whereas Perigee was proposed as a
protocol to optimize systemic network performance.

(b) Relevant transactions: In Perigee, nodes measure the la-
tency of all received transactions. In Peri, the score function
𝜙 (𝑣) enforces that only relevant transactions participate in
scoring peers. In Peri, for direct global latency reduction, all
transactions are considered relevant, while for direct targeted
latency reduction, only transactions made by the target are
relevant.

(c) Handling silent peers: Particularly when optimizing tar-
geted latency, the function 𝜙 may be undefined for some peers
in some periods. For example, if a peer 𝑝 is connected near
the end of a given period, there will not be sufficient data
to compare 𝑝 with other peers, which means 𝜙 (𝑝) may be
undefined. Instead of evicting 𝑝 in such cases and possibly
losing a good peer, we forego eviction of 𝑝. In Alg. 1, the pred-
icate is_excused(𝑣) is true if node 𝑣 should be excluded from
eviction for the current period.

(d) Blocklists. The Perigee [29] algorithm advocates for select-
ing a new set of peers at random. However, this increases the
likelihood of a peer tearing down connections, then connect-
ing to the same node(s) shortly thereafter, particularly since
some cryptocurrency clients (including geth) favor previously-
visited nodes during peer selection [22, 23]. To handle this, in
Peri we use blocklists: if a node tears down a connection to
peer 𝑣 in a Peri period, it refuses to re-connect to 𝑣 in future
periods. In Alg. 1, we maintain blocklist 𝐵 for this purpose.

7

(e) Sampling relevant transactions: Note that we cannot relay
relevant transactions; otherwise, the “late” peers who do not
deliver a relevant transaction first to our node will never deliver
it to our node (§2), thus removing them from Peri’s latency
comparison. If all transactions are treated as relevant, our node
will act as a black hole for transactions, and may impact the P2P
network. We avoid this issue by sampling 1/4 of all (relevant)
transactions for global latency measurement. This is realized by
redefining relevant transactions as those with hashes divisible
by 4 when computing 𝜙 (·) in Line 14 of Algorithm 1.

5 DIRECT LATENCY EVALUATION
In §5.1 and §5.2, we show the practical latency reduction effects of
Peri with experiments on the Ethereum P2P main network (mainnet)
and Rinkeby test network (testnet).

5.1 Evaluation: Direct Global Latency
We start by evaluating the feasibility of reducing global latency.

5.1.1 Methods. We evaluate four approaches.

(a) Baseline. Our experimental control node uses the default set-
tings of the Go-Ethereum client, version 1.10.16-unstable [9]4.
This was the latest version when we started the experiments.
(b) BloXroute. We compare against a centralized, private relay
network, using bloXroute as a representative example. We
use the bloXroute Professional Plan [5]; at the time our ex-
periments were run, this plan cost $300 per month. We ran
the bloXroute gateway locally to avoid incurring additional
latency (§2.1).

(c) Peri. We modify the Go-Ethereum client [9] to implement
the Peri algorithm for peer selection. We set the period to 20
minutes, and replace at most 25 peers every period.

(d) Hybrid. We implement a hybrid method that combines bloXroute
and Peri by applying Peri to a node with access to a bloXroute
relay. To ensure correctness, we ensure that the gateway con-
necting to the relay, which acts as a peer of the node, cannot
be removed by the Peri algorithm.

5.1.2 Experimental Setup. We establish 4 EC2 instances in the
us-east-1 AWS data center, where a public bloXroute relay is located.
On each instance, we deploy a full node on the Ethereum P2P main
network (the mainnet), which is implemented by a customized Go-
Ethereum (geth) client. Each node has at most 50 peers, which is
the default setting of Go-Ethereum. For a node running Peri or
Hybrid, we set the proportion of outbound peers (peers dialed by
the node itself) to 80% so that the node actively searches for new
peers. For a node running other baselines, we keep the proportion
at 33%, which is also default for Go-Ethereum.

We ran 63 experiments from Feb 18, 2022 to March 16, 2022, each
following the procedure below. First, we assign each latency reduc-
tion method (bloXroute, Peri, Hybrid and Baseline) exclusively to a
single node, so that all four nodes use different comparison meth-
ods. Then, we launch the nodes simultaneously. When a packet
arrives, the node checks if the packet contains a full relevant trans-
action or its hash; if so, it records the timestamp. At the end of the

4The
https://github.com/WeizhaoT/geth_peri.

customized

client

source

code

can

be

found

at

experiment, we stop the nodes and collect the arrival timestamps
of transactions from their host instances.

Bias reduction. We take the following steps to control for system-
atic bias in our experiments. We prohibit the 4 measurement nodes
from adding each other as peers. We reset all the enode IDs (unique
identifiers of the Ethereum nodes including IP address) before each
experiment. This prevents the other nodes from remembering our
nodes’ IP addresses and making peering decisions based on activity
from previous experiments. We record the clock differences with
periodic NTP queries between each pair of hosts to fix systematic
errors in the timestamps recorded locally by the hosts. Despite
being the same type of AWS instance, the host machines may also
introduce biases because they may provide different runtime en-
vironments for the Ethereum node program. To eliminate these
biases, we rotate the assignment of latency reduction methods after
every experiment, so that for every successive 4 experiments, each
node is assigned each method exactly once. We attempt to control
for temporal biases due to diurnal transaction traffic patterns by
running each experiment every 8 hours, so that the assignment of
methods to nodes rotates 3 times a day. Because the number of possi-
ble method assignments is 4, a co-prime of 3, each node experiences
every combination of latency reduction methods and time-of-day.
We did not control for contention (e.g., of network bandwidth, com-
putational resources) among EC2 instances by running experiments
on dedicated hardware due to financial constraints.

5.1.3 Results. Each node is allowed to warm up for 2.5 hours;
after this, we collect all transactions that are received by each of
the nodes for 3.5 hours. Then, for each transaction 𝑚 and each
node 𝑦 with a latency reduction method other than Baseline, we
compute the difference between the timestamp of 𝑚 at 𝑦 and the
timestamp at the baseline node 𝑏, which is effectively a sample of
random variable Λ(𝑦) − Λ(𝑏). The smaller the time difference (i.e.,
the more negative), the earlier 𝑦 delivers 𝑚, and the more effective
the latency reduction method is. We gather the latency differences
over 63 experiments, and plot their distributions for each node in
Fig. 1. In total, we analyzed the latencies of 6,228,520 transactions.

Finding 1: Peri alone is at least half as effective as bloXroute
private networks in reducing average direct global latency. The
Hybrid node (combining Peri with bloXroute) is superior to
bloXroute, achieving an additional 15% improvement in la-
tency reduction over bloXroute alone.

In the figure, all the latency differences are distributed with nega-
tive means and medians, showing an effective latency reduction. To
establish the statistical significance of the mean difference among
these distributions, we first perform the Kruskal-Wallis H test [25].
The resulting p-value equals 0, so that a following Dunn’s test is
recommended. We perform Dunn’s test [18] with the Bonferroni
correction [13], and obtained a 0 p-value between each pair of dis-
tributions. This test supports statistically significant differences in
the reduction of direct global latency effected by the three meth-
ods, ordered by the means of their corresponding distributions.
BloXroute reduced this latency by 18.45 milliseconds on average.
In comparison, Peri reduced it by 11.09 milliseconds, which is more
than half as effective as bloXroute, and at the same time cost-free,

8

Figure 1: PDFs of distributions of global latency.

in the sense that it does not require payment of the $300 / month of
the bloXroute plan used in our experiments. On the other hand, by
exploiting access to bloXroute services, Hybrid nodes can achieve
an additional 15% reduction in latency. This suggests the poten-
tial of peer-selection algorithms to further boost latency reduction
techniques over purely private relay networks.

5.2 Evaluation: Direct Targeted Latency
Reducing direct targeted latency is useful for different applications
compared to reducing direct global latency. An agent with a low
direct targeted latency can perform better targeted arbitraging
against a specific victim, or discover the IP address of a target
cryptocurrency node. In this section, we evaluate the feasibility of
reducing direct targeted latency. We apply the same methods we
used to reduce direct global latency.

Experimentally evaluating targeted latency reduction techniques
on the Ethereum mainnet can be costly and time-consuming. This
is because to evaluate latency distributions, we need to observe
many transactions with a known ground truth IP address. A natural
approach is to generate our own transactions from a single node and
measure their latency; unfortunately, nodes in the P2P network do
not forward transactions that are invalid or unlikely to be mined due
to low gas fees. Hence, we would need to create valid transactions.
For example, at the time of writing this paper, the recommended
base fee of a single transaction was about $2.36 per transaction [6].
Collecting even a fraction of the 6+ million transactions analyzed
in §5.1 would quickly become prohibitively costly.

We ran limited experiments on direct targeted latency, where
the setups and results are shown in details in Appendix A.2. The
findings of these experiments are summarized below.

Finding 2: Peri reduces direct targeted latency by 14% of the
end-to-end delay, which is as effective as bloXroute and Hy-
brid.

Unfortunately, we did not collect a sufficient amount of data
for Peri and Hybrid to show a statistically significant ability to
connect directly to a victim and learn its IP address. We attribute
this to the low transaction frequency and large size of the mainnet.
The testnet experiments we now describe, however, did result in
frequent victim discovery of this kind—provided a sufficiently large
number of Peri periods or a high frequency of victim transactions.

5.2.1 Targeted Latency on Ethereum testnets. Due to the finan-
cial cost of latency reduction experiments at scale on the Ethereum
mainnet, we also performed experiments on the Rinkeby testnet. 5
Our goal in these experiments is to simulate a highly active victim
in the network and compare the Peri algorithm’s ability to find and
connect to the victim against a baseline default client.

In these experiments we run a victim client on an EC2 instance
in the ap-northeast-1 location and the Peri and Baseline clients in
the us-east-2 location. Note that we cannot evaluate bloXroute or
Hybrid on the test network, because bloXroute does not support test
network traffic. Due to our observations of the Rinkeby network’s
smaller size, and to emulate a long-running victim whose peer slots
are frequently full, we set the peer cap of the victim to 25 peers. As
in previous experiments, the Baseline node is running the default
geth client with peer cap of 50. The Peri node is also running with
a peer cap of 50 but with the Peri period shortened to 2 minutes.

In each experiment, we start the victim client first and give
it a 30 minute warm-up period to ensure its peer slots are filled.
We then start the Peri and Baseline nodes and begin transmitting
transactions from multiple accounts on the victim node. We set
the frequency to 3 transactions per minute to control the variance
of timestamps and Peri scores. We then run the experiment for
just under 3 hours, enabling us to run 7 experiments per day and
alleviate time-of-day biases. As with the global latency experiments
in §5.1, we alternate which us-east-2 machine is running Peri v.s.
Baseline across runs in order to mitigate potential machine-specific
biases. We ran these experiments from April 16th - 26th, 2022, for
a total of 70 runs.

Finding 3: In testnet experiments, Peri is able to identify the
IP address and connect to a target (victim) node with a fre-
quency more than 7× that of a Baseline node.

The number of connections established by the Peri and Baseline
clients to our victim are listed in Table 2. We find that Peri gives a
notable advantage in terms of connecting to the victim node. Fig. 2
shows a CDF of the number of Peri periods until Peri connected to
the victim, with an mean/median of 39/45 periods (∼ 1.3/1.5 hrs).
Fig. 3 shows the delay PDF for the runs when Peri finds the victim
for all transactions before and after Peri connects to the victim. We
observe a mean latency advantage of 22ms over the Baseline client
once Peri connects to the victim. This is 30% of the end-to-end

5Though the Ropsten testnet is generally thought to be the test network that most
faithfully emulates the Ethereum mainnet [3], its behavior was too erratic for our
experiments during our period of study. This included high variance in our ability to
connect to peers from any machine and constant deep chain reorganizations, causing
lags in client synchronization.

Method
Successful Victim Connections

Peri

Baseline

47/70 = 67.1% 6/70 = 8.6%

Table 2: Rate of connection to victim node in Rinkeby test-
net experiments.

Figure 2: CDF of number of Peri periods before finding vic-
tim. Each Peri period lasts 2 minutes.

Figure 3: PDFs of distributions of targeted latency for runs
when Peri found the victim split between before either
found the victim, and after Peri finds the victim. The 𝑥 axis
represents the difference of timestamp at the Peri node and
at the baseline node for each single transaction.

delay between the victim and both the Peri and Baseline clients.
Unlike the case with our mainnet experiments, though, Peri has no
significant advantage before it connects to the victim. We attribute
the difficulty of establishing a network advantage prior to direct
connection to the victim to the Rinkeby network’s smaller size (1.8K
unique IPs we came across over the period of our study compared
to 13.6K in the mainnet). A consequence is that there are relatively
few peers in geographical proximity to the the Tokyo data center by
comparison with our mainnet victim node in Germany. There are
significant differences in size and topology between the Rinkeby
network and the Ethereum mainnet, and it is unclear how our
experiments would extrapolate to the mainnet. Our experiments
do, however, provide evidence that Peri can boost the likelihood of
a strategic agent connecting directly to a victim in P2P networks;
how this capability generalizes warrants further investigation.

9

-100-75-50-250255075100Time (ms)0255075100Distribution (PDF) of Latency of Peri w.r.t. Baseline(Before connecting to victim)Mean  = 2.49Median=  1.27PDF-100-75-50-250255075100Time (ms)050100150Distribution (PDF) of Latency of Peri w.r.t. Baseline(After connecting to victim)Mean  =-22.58Median=-19.82PDF6 TRIANGULAR LATENCY EVALUATION
As explained in §3, a front-runner may be interested in two types
of triangular latency: targeted and global. As with direct targeted
latency, we established in §3.2 that the optimal strategy for an agent
to reduce triangular targeted latency is to connect directly to the
target nodes. Since this procedure is identical to the experiments
in §5.2, we do not run additional experiments to demonstrate its
feasibility in this section.

The more complicated question is how to optimize triangular
global latency. Recall from §3.2 that we study triangular global
latency via a proxy metric, which we call adversarial advantage
𝐴N (𝑈 ), where N denotes the network and 𝑈 denotes the set of
strategic or adversarial agents. In the remainder of the section, we
first show that directly optimizing adversarial advantage is com-
putationally infeasible even if the agent knows the entire network
topology (§6.1). However, we also show through simulation that
efficient local strategies (namely, a variant of Peri) can be used to
outperform baselines (§6.2).

Note on evaluation: Adversarial advantage is even more difficult
to evaluate empirically than direct latency. To measure adversarial
advantage for a single source-destination pair, we would need to
observe transactions from a known source (e.g., a front-running
victim), which end up at a known target destination (e.g., a miner
or an auction platform [10]); we would additionally need to verify
that our agent node is able to reach the target node before the vic-
tim’s transaction reaches the target destination. Setting up our own
nodes on the mainnet to measure this would be twice as costly as
evaluating direct targeted latency (since we would need to gener-
ate both a valid victim and front-running transaction), which we
deemed infeasible in §5.2. Measuring adversarial advantage globally
would require visibility into every pair of nodes in the network,
which is clearly infeasible. For these reasons, we chose to evaluate
triangular latency reduction theoretically and in simulation.

6.1 Hardness of Optimizing Adversarial

Advantage

Generally, we are interested in the following problem.

Problem 1. Given a network N , sets of sources and destinations
S, T , and budget 𝑘 (number of edges the agent can add), we want to
maximize 𝐴N (𝑈 ) (Definition 3.4) subject to |𝑈 | ≤ 𝑘, where 𝑈 is the
set of peers to which the agent node connects.

Theorem 6.1. Problem 1 is NP-hard.

(Proof in Appendix A.1.1) The proof follows from a reduction
from the set cover problem. Not only is solving this problem opti-
mally NP-hard, we next show that it is not possible to approximate
the optimal solution with a greedy algorithm. A natural greedy
algorithm that solves the advantage maximization problem is pre-
sented in Algorithm 2. (Note that it includes a “bootstrapping” step
in which two nodes are initially added to the set 𝑈 of agent peers,
as no advantage is obtainable without at least two such nodes.)
The greedy algorithm is unable to approximately solve the ad-

vantage maximization problem.

Input: Number of peers 𝑘 ≥ 2, graph N
Output: Set of peers 𝑈

{𝑥,𝑦}:(𝑥,𝑦) ∈V2 𝐴N ( {𝑥, 𝑦 });

1 𝑈 ← argmax
2 for 𝑗 ∈ {3, 4, · · · , 𝑘 } do
3

¯𝑧 ← argmax𝑧:𝑧∈V−𝑈 𝐴N (𝑈 ∪ {𝑧 });
𝑈 ← 𝑈 ∪ { ¯𝑧 };

4
5 return 𝑈 ;

Algorithm 2: Greedy Algorithm for Maximizing 𝐴N.

(Proof in Appendix A.1.2) We show this by constructing a coun-
terexample for which the greedy algorithm achieves an adversarial
advantage whose suboptimality gap grows arbitrarily close to 1 as
the problem scales up. Whether a polynomial-time approximation
algorithm exists for maximization of 𝐴N is an open problem.

6.2 Approximations of Advantage

Maximization

The greedy algorithm in Algorithm 2 is provably suboptimal for
maximizing adversarial advantage in general, but we observed
in simulation that for small, random network topologies (up to 20
nodes), it attains a near-optimal adversarial advantage (results omit-
ted for space). We also show in this section that the greedy strategy
achieves a much higher advantage (2-4×) than a natural baseline
approach of randomly adding edges. Hence, the greedy strategy
may perform well in practice. However, the greedy strategy has a
critical flaw: it requires knowledge of the entire network topology.

In this section, we evaluate the feasibility of using local methods
(i.e., Peri) to approach the performance of the greedy strategy. Be-
cause the greedy strategy is centralized, we expect it to outperform
Peri. However, we show in simulation6 that for several natural
graph topologies, this margin is small.

Graph topology. We consider four models of random graph topolo-
gies: Erdös-Rényi, random regular graphs, Barabási-Albert (scale
free) graphs, and Watts–Strogatz (small world) graphs. The Ethereum
P2P network has been shown to exhibit properties of both small
world and scale-free networks [42]. The number of nodes is set
to 300 for all the models. The average degree is set to 9 for the
Erdös-Rényi model to ensure connectivity. The average degree,
however, is set to 4 for the other 3 models to make sure the average
distance between nodes is high enough for shortcuts to exist. For
each model, we sample 25 different graph instances. Further, we
centralize each instance by introducing 20 new hub nodes, each
connecting to 30 nodes randomly sampled from the original graph.
On each instance, the set of sources S is equal to the set of all
the nodes V, and the set of destinations T is a random subset of V
with |T | = 0.1 · |V |. We let the static front-running penalty 𝜏 = 0, a
minimum path advantage, and assign a unit weight to all the edges.

Peri Modification. To make Peri optimize adversarial advantage,
we alter the scoring function and the peer-selection criterion. This
alteration in turn reveals a clean way of simulating the (modified)
Peri algorithm without simulating individual message transmis-
sions. First, we make the following simplifying assumptions: (a)

Proposition 6.2. The output of Alg. 2 is not an 𝛼-approximate

solution to Problem 1 for any 𝛼 > 0.

6The simulator code can be found at https://github.com/WeizhaoT/Triangular-Latency-
Simulator.

10

Peer replacement is completed immediately at the beginning of a
new Peri period, and no peer is added or dropped during the period;
(b) The full set 𝑀 of messages sent during the current Peri period is
delivered to the adversarial agent during the Peri period by every
peer; (c) The end-to-end delay of each message is proportional to
the distance between its source and destination; and (d) Message
origination is uniformly distributed over the network.

Let 𝑑𝑣 (𝑚) denote the distance from the source of 𝑚 to peer 𝑣 of
the agent node 𝑎 and 𝑆 (𝑚) the time when message 𝑚 departs from
the source. Recall that we assume 𝑑 N′ (𝑣, 𝑎) = 0 for any peer 𝑣 of 𝑎
in §3.2, so for such a 𝑣, 𝑑𝑣 (𝑚) equals the distance from the source
of 𝑚 to 𝑎. The score function 𝜙 (𝑣) for peer 𝑣 can be transformed as
follows with constants 𝐶1, 𝐶2, 𝐶3 with respect to 𝑣 for a set topology.

𝜙 (𝑣) =

𝑇𝑣 (𝑚) − 𝑇 (𝑚)
|𝑀 |

=

∑︁

𝑚 ∈𝑀

𝑇𝑣 (𝑚) − 𝑆 (𝑚)
|𝑀 |

+

𝑆 (𝑚) − 𝑇 (𝑚)
|𝑀 |

= 𝐶1

∑︁

𝑚 ∈𝑀

𝑑𝑣 (𝑚) + 𝐶2 = 𝐶3

𝑑 N (𝑠, 𝑣) + 𝐶2.

∑︁

𝑚 ∈𝑀
∑︁

𝑠 ∈S

Since Peri’s choice of which peers to drop is invariant to 𝐶2 or

𝐶3, we can further simplify the score in (7):

𝜙 (𝑣) =

∑︁

𝑠 ∈S

𝑑 N (𝑠, 𝑣).

(7)

For a given peer budget 𝑘, we replace 𝑟 =

(cid:109) peers and keep
𝑘 peers. We only consider the kept peers for evaluation of the
advantage metric. In total we execute 800 Peri periods.

(cid:108) 𝑘
3

Results. We sweep the peer count budget |𝑈 | = 𝑘 of the agent
over 7 values ranging from 2 to 20. For each maximum peer count,
we obtain a resulting peer set 𝑈 ∗ with advantage 𝐴N (𝑈 ∗) using
the greedy algorithm, the Peri algorithm, and random sampling, in
which each agent chooses to peer with nodes selected uniformly
at random. The performance of each method is represented by its
advantage-peer-count (𝐴N-|𝑈 |) curve. For each graph model, we
plot the mean and standard deviation of the curves over 25 different
hub-enriched graph instances in Fig. 4.

Finding 4: Peri is competitive with the greedy algorithm for
maximizing adversarial advantage when the underlying net-
work has many hubs, or nodes of high degree.

The introduction of hubs is motivated by the simulations of the
original topologies without hub nodes, which we show in Appendix
A.3. Regardless of the original topology, the existence of hub nodes
enables the Peri algorithm to place shortcuts almost as effectively
as the greedy algorithm. We notice that Peri worked significantly
better on models with more hubs such as scale-free than the other
topologies. It is also notable that over 70% of the resulting peers
of the Peri algorithm are hubs, which signals the ability of the
Peri algorithm to locate hubs and perform front-running attacks
through them. Hubs have been shown to be common in real-world
cryptocurrency P2P networks [32, 42], motivating Peri to be a
potentially powerful tool in practice.

Figure 4: Advantage-Peer-count curves on hub-enriched
graph models. The mean of the curves are shown by solid
lines, and the standard deviations are shown by the trans-
parent color zones centered at the mean.

7 IMPOSSIBILITY OF STRATEGY-PROOF

PEERING PROTOCOLS

We have so far seen that Peri (and variants including the Hybrid
approach) can significantly improve direct and triangular latency. A
natural question is whether this is because most nodes are currently
running a poor peer selection strategy (Baseline). For example, if
every node were running Peri or Hybrid, would it still be possible
for an agent to strategically improve their targeted latency? we
show that no matter what peering strategy nodes are using, as long
as the P2P network has some natural churn and as long as the target
node(s) are active, a strategic agent can always manipulate their
targeted latency.

7.1 Model
We start with some additional modeling assumptions regarding
the temporal dynamics of our network. Consider a time-varying
network N (𝑡) = (V (𝑡), E (𝑡)) where V (𝑡) denotes the set of nodes
and E (𝑡) denotes the set of undirected edges of 𝐺 at time 𝑡. In the
network, any party can spawn a set 𝑉 ′ of nodes and add them to
V (𝑡). (V (𝑡 +) = V (𝑡) ∪ 𝑉 ′, where 𝑡 + denotes the time infinitesi-
mally after 𝑡). We assume there is an upper bound ¯𝑉 on the total
number of nodes in the network; that is, V (𝑡) ≤ ¯𝑉 for all 𝑡 ≥ 0,
due to the limited address space for nodes.

If a node 𝑢 knows the network (Class 1) ID of another node 𝑣,
𝑢 can attempt to add an edge, or a peer connection, (𝑢, 𝑣) at time
𝑡 (that is, E (𝑡 +) = E (𝑡) ∪ {(𝑢, 𝑣)}) to the network. This peering
attempt will succeed unless all of 𝑣’s peer connections are full.
Nodes have an upper bound of ℎ on their total number of peer
connections:

11

deg(𝑢) ≤ ℎ ∀𝑢 ∈ V (𝑡), ∀𝑡 ≥ 0.

Among these, each node has at least 𝑓 ∈ (0, ℎ] dynamic peer
slots. A connection from 𝑢 to 𝑣 that occupies one of either 𝑢’s or
𝑣’s dynamic peer slots is called dynamic and will be periodically
torn down (more on this in Definition 7.1). Additionally, a dynamic
peer slot is permissionless and is filled first-come-first-serve (FCFS).
We assume there exists an oracle in the network serving as an
abstraction of a distributed database of network IDs. Any node
may query the oracle, which responds by independently drawing
the network ID of a node in the network from some (unknown)
probability distribution, where each node may be drawn with a
non-zero probability. Specifically, there exists a universal constant
𝑞 > 0 where the probability of drawing an arbitrary node is lower-
bounded by 𝑞. This is based on the assumption that the number of
nodes is upper bounded. We assume the oracle can process a fixed
number of queries per unit time, so each query (across all nodes)
takes constant time. Upon processing a query, the oracle responds
with the network ID of an existing node. This is an abstraction of
peer discovery in many blockchains [43].

Each edge (𝑢, 𝑣) ∈ E (𝑡) has an associated link distance 𝑤 (𝑢, 𝑣),
which operationally represents the Internet latency from 𝑢 to 𝑣. We
assume latency 𝑤 (𝑢, 𝑣) is dominated by the physical length of the
path 𝑢 → 𝑣 on the Internet. Hence, we assume 𝑤 (𝑢, 𝑣) is a constant
over time and satisfies the triangle inequality:

𝑤 (𝑢, 𝑣) ≤ 𝑤 (𝑢, 𝑧) + 𝑤 (𝑣, 𝑧), ∀𝑢, 𝑣, 𝑧 ∈ V (𝑡), 𝑡 ≥ 0.

(8)

This is distinct from graph distance 𝑑 N (𝑢, 𝑣) introduced in §3.

Transaction dissemination. A node can broadcast an arbitrary
message (transaction) at an arbitrary time through the entire net-
work. When a message is sent from 𝑢 to its neighbor 𝑣 at time 𝑡, 𝑣
will receive the message at time 𝑡 +𝑤 (𝑢, 𝑣), where 𝑤 (𝑢, 𝑣) is the link
distance between 𝑢 and 𝑣. If 𝑣 is not adversarial, it will immediately
forward the message to each of its neighbors.7 We assume the P2P
network is connected at all times 𝑡 ≥ 0. We additionally assume
that the latency between any pair of agent nodes is negligible, such
that an agent connecting to multiple targets from multiple agent
nodes, is equivalent to these targets peering with one node.

Liveness. In our analysis, we will assume that nodes are suf-
ficiently active in terms of producing transactions and that the
network experiences some amount of peer churn. We make both
of these assumptions precise with the following definition.

Definition 7.1. A (𝜆, 𝜈)-live network has the following properties:
(I) All dynamic connections have a random duration Exp(𝜆).
(II) For each node 𝑢 ∈ S, where S denotes a set of target nodes,
we let {𝑀𝑢 (𝑡), 𝑡 ∈ [0, ∞)} denote the counting process of
messages that are generated and broadcast by 𝑢. 𝑀𝑢 (𝑡) is a
Poisson process with rate 𝜈.

7.2 Optimization of Targeted Latencies

7.2.1

Inferring Network ID. We argue in §3 that we can optimize
targeted latency (both direct and triangular) by connecting directly
to the target node(s). However, in practice, an agent typically only
knows the targets’ logical (Class 2) IDs (e.g., public key of wallet),
whereas it needs their network (Class 1) IDs (e.g., IP address) to
connect. In the following, we show when an adversarial agent can

7This is a special case of randomized flooding protocols like diffusion [19].

12

learn the network ID(s) of one or more targets. Our first result
states that in a live network (Definition 7.1), it is possible for an
agent to uncover the network ID(s) of a set of target nodes with
probability 1 − 𝜀 given only their logical IDs in time quasilinear in
𝜀−1 and quadratic in the number of target nodes.

Theorem 7.2. Consider a live network N (𝑡) = (V (𝑡), E (𝑡)). Let
A denote an adversarial agent capable of identifying the logical ID
of the sender of any message it received from the network. Given a set
of target nodes 𝑈 , with probability at least 1 − 𝜀 for any 𝜀 ∈ (0, 1),
it takes the agent 𝑂 (|𝑈 |2𝜀−1 log2 𝜀−1) time to find the network ID of
any message sender in N (𝑡).

(Proof in Appendix A.1.3) The proof shows that a variant of
Peri achieves the upper bound, showing Peri’s effectiveness. In
particular, note that to optimize triangular targeted latency, for a
target pair of nodes (𝑠, 𝑡), the agent must connect to both 𝑠 and 𝑡,
so |𝑈 | = 2 in Theorem 7.2.

Additionally, the following proposition shows that regardless of
the agent’s algorithm, we require time at least logarithmic in 1/𝜀.

Proposition 7.3 (Lower Bound). Consider a live network N (𝑡) =
(V (𝑡), E (𝑡)). Let A denote an adversary defined in Theorem 7.2. For
any 𝜀 ∈ (0, 1), to achieve a probability at least 1 − 𝜀 that the agent
is connected to its target, A must spend at least Ω(log 𝜀−1) time,
regardless of the algorithm it uses.

(Proof in Appendix A.1.4) Proposition 7.3 suggests that a node
aiming to prevent agents from learning its network ID with prob-
ability at least 1 − 𝜀, should cycle its logical ID on a timescale of
order log 𝜀−1.

Together, Theorem 7.2 and Proposition 7.3 show how and when
an agent can find a message source in a P2P network. They are an
important missing piece in the solution to optimization problems
(1) and (3) for targeted latency. Next, we discuss how the other
missing piece is filled in, i.e., how the agent ensures successful and
eternal peer connections with targets after finding them.

7.2.2 Connecting to Targets. By assumption, the target must
have at least 𝑓 > 0 dynamic peer slots, which are permissionless
and thus can be accessed by the agent. Since these slots are on
a FCFS basis, the agent may request an occupied peer slot at an
arbitrarily high frequency, such that it immediately gets it when
the slot becomes available after the old connection is torn down. 8
In this way, an agent can effectively make a dynamic peer slot of
any target no longer dynamic, and constantly dedicated to itself.

7.2.3 Latency Manipulation. Even if the agent is connected to
the target, the latency between them is still lower-bounded due
to the physical distance between the agent and the target. Agents
may be able to relocate their nodes geographically to manipulate
triangular latency. Currently, over 25% of Ethereum nodes are run-
ning on AWS [4]. An agent can map their IP addresses (§7.2) to the
locations of their cloud data centers, and deploy nodes on cloud
servers at the same locations. This can reduce the direct targeted
latency to the level of microseconds. For triangular targeted latency
where the source and the destination are at different geolocations,
the agent can deploy node 𝑎1 near the source 𝑠 and node 𝑎2 near

8To evade detection by the frequency of requests from single nodes, an agent can
spawn many nodes and split the periodic requests among its nodes.

the destination 𝑡, where 𝑎1 connects to 𝑠 and 𝑎2 connects to 𝑡, .
with 𝑎1 and 𝑎2 connected via a low-latency link. This ensures the
path 𝑠 → 𝑎1 → 𝑎2 → 𝑡 will be shortest, and guarantees success in
front-running messages between 𝑠 and 𝑡.

8 CONCLUSION
Motivated by the increasing importance of latency in blockchain
systems, in this work we have studied the empirical performance
of strategic latency reduction methods as well as their theoretical
limits. We formally defined the key notions of direct and triangular
latencies. We have proposed a strategic scheme for reducing both
types in a scheme called Peri and have shown its effectiveness
experimentally on the Ethereum mainnet and an Ethereum testnet.
In this work, we have also explored the theory of strategic latency
reduction in blockchain P2P networks. In a graph-based network
model, we have shown that maximizing a notion of triangular-
latency advantage is NP-hard. Even in this setting, however, we
have shown via simulations that Peri achieves effective latency
reduction. Finally, we have addressed the key question of whether
it is possible to ensure strategy-proof peering protocols in unstruc-
tured blockchain P2P networks, and shown that it is in fact im-
possible: Agents can achieve direct connection to victims in low
asymptotic time. Consequently, for any latency-sensitive applica-
tion, blockchain P2P networks will always raise concerns.

ACKNOWLEDGEMENTS
We wish to thank Lorenz Breidenbach from Chainlink Labs and
Peter van Mourik from Chainlayer for their gracious help in the
setup for our direct-targeted-latency experiments.

G. Fanti and W. Tang acknowledge the Air Force Office of Scien-
tific Research under award number FA9550-21-1-0090, as well as the
generous support of Chainlink Labs and IC3. L. Kiffer contributed
to this project while under a Facebook Fellowship.

REFERENCES
[1] Gas and Fees. https://ethereum.org/en/developers/docs/gas/. (????). Accessed

on April 29, 2022.

[2] 2022. bloXroute Documentation. (2022). https://docs.bloxroute.com/.
[3] 2022. Ethereum Networks Documentation. (2022). https://ethereum.org/en/

[4]

developers/docs/networks/.
[Accessed Apr. 2022]. Blockchain on AWS. https://aws.amazon.com/blockchain/.
([Accessed Apr. 2022]).

[5] [Accessed Apr. 2022]. BloXroute website. https://bloxroute.com/. ([Accessed

Apr. 2022]).

[6] [Accessed Apr. 2022]. Eth Gas Station. https://ethgasstation.info/. ([Accessed

Apr. 2022]).
[7] [Accessed Apr.

2022].

Ethereum Wire

https://github.com/ethereum/devp2p/blob/master/caps/eth.md.
Apr. 2022]).

Protocol

(ETH).
([Accessed

[8] [Accessed Apr. 2022]. Fast Internet Bitcoin Relay Engine (FIBRE) . https://

bitcoinfibre.org/. ([Accessed Apr. 2022]).

[9] [Accessed Apr. 2022]. Go Ethereum. https://geth.ethereum.org/. ([Accessed

[10]

Apr. 2022]).
[Accessed Apr. 2022]. MEV-Explore v1. https://explore.flashbots.net/. ([Accessed
Apr. 2022]).

[11] Vijeth Aradhya, Seth Gilbert, and Aquinas Hobor. 2022. OverChain: Building a
robust overlay with a blockchain. arXiv preprint arXiv:2201.12809 (2022).
[12] Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, and Pramod Viswanath.
2019. Prism: Deconstructing the blockchain to approach physical limits. In
Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications
Security. 585–602.

[13] C.E. Bonferroni. 1936. Teoria statistica delle classi e calcolo delle probabilità. Seeber.

https://books.google.com/books?id=3CY-HQAACAAJ

[14] Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe Gencer, Ari Juels, Ahmed
Kosba, Andrew Miller, Prateek Saxena, Elaine Shi, Emin Gün Sirer, et al. 2016.
On scaling decentralized blockchains. In International conference on financial
cryptography and data security. Springer, 106–125.

[15] Philip Daian, Steven Goldfeder, Tyler Kell, Yunqi Li, Xueyuan Zhao, Iddo Bentov,
Lorenz Breidenbach, and Ari Juels. 2020. Flash boys 2.0: Frontrunning in decen-
tralized exchanges, miner extractable value, and consensus instability. In 2020
IEEE Symposium on Security and Privacy (SP). IEEE, 910–927.

[16] Christian Decker and Roger Wattenhofer. 2013. Information propagation in the

bitcoin network. In IEEE P2P 2013 Proceedings. IEEE, 1–10.

[17] Sergi Delgado-Segura, Cristina Pérez-Solà, Jordi Herrera-Joancomartí, Guillermo
Navarro-Arribas, and Joan Borrell. 2018. Cryptocurrency networks: A new P2P
paradigm. Mobile Information Systems 2018 (2018).

[18] Olive Jean Dunn. 1961. Multiple Comparisons among Means. J. Amer. Statist.
Assoc. 56, 293 (1961), 52–64. https://doi.org/10.1080/01621459.1961.10482090
arXiv:https://www.tandfonline.com/doi/pdf/10.1080/01621459.1961.10482090

[20]

[19] Giulia Fanti and Pramod Viswanath. 2017. Deanonymization in the bitcoin P2P
network. Advances in Neural Information Processing Systems 30 (2017).
Juan Garay, Aggelos Kiayias, and Nikos Leonardos. 2015. The bitcoin backbone
protocol: Analysis and applications. In Annual international conference on the
theory and applications of cryptographic techniques. Springer, 281–310.

[21] Yilin Han, Chenxing Li, Peilun Li, Ming Wu, Dong Zhou, and Fan Long. 2020.
Shrec: Bandwidth-efficient transaction relay in high-throughput blockchain
systems. In Proceedings of the 11th ACM Symposium on Cloud Computing. 238–
252.

[22] Ethan Heilman, Alison Kendler, Aviv Zohar, and Sharon Goldberg. 2015. Eclipse
attacks on bitcoin’s peer-to-peer network. In 24th {USENIX} Security Symposium
({USENIX} Security 15). 129–144.

[23] Sebastian Henningsen, Daniel Teunis, Martin Florian, and Björn Scheuermann.
2019. Eclipsing ethereum peers with false friends. In 2019 IEEE European Sympo-
sium on Security and Privacy Workshops (EuroS&PW). IEEE, 300–309.

[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity

of computer computations. Springer, 85–103.

[25] William H. Kruskal and W. Allen Wallis. 1952. Use of Ranks in One-Criterion
Variance Analysis. J. Amer. Statist. Assoc. 47, 260 (1952), 583–621. http://www.
jstor.org/stable/2280779

[27]

[28]

[26] Chenxin Li, Peilun Li, Dong Zhou, Zhe Yang, Ming Wu, Guang Yang, Wei Xu,
Fan Long, and Andrew Chi-Chih Yao. 2020. A decentralized blockchain with
high throughput and fast confirmation. In 2020 {USENIX} Annual Technical
Conference ({USENIX} {ATC} 20). 515–528.
Jinyang Li. 2005. Routing tradeoffs in dynamic peer-to-peer networks. Ph.D.
Dissertation. Massachusetts Institute of Technology.
Igor Makarov and Antoinette Schoar. 2020. Trading and arbitrage in cryptocur-
rency markets. Journal of Financial Economics 135, 2 (2020), 293–319.

[29] Yifan Mao, Soubhik Deb, Shaileshh Bojja Venkatakrishnan, Sreeram Kannan,
and Kannan Srinivasan. 2020. Perigee: Efficient peer-to-peer network design for
blockchains. In Proceedings of the 39th Symposium on Principles of Distributed
Computing. 428–437.

[30] Petar Maymounkov and David Mazieres. 2002. Kademlia: A peer-to-peer infor-
mation system based on the xor metric. In International Workshop on Peer-to-Peer
Systems. Springer, 53–65.

[31] Adam Meyerson and Brian Tagiku. 2009. Minimizing average shortest path
In Approximation, Randomization, and

distances via shortcut edge addition.
Combinatorial Optimization. Algorithms and Techniques. Springer, 272–285.
[32] Andrew Miller, James Litton, Andrew Pachulski, Neal Gupta, Dave Levin, Neil
Spring, and Bobby Bhattacharjee. 2015. Discovering bitcoin’s public topology
and influential nodes. et al (2015).

[33] Ahmed Afif Monrat, Olov Schelén, and Karl Andersson. 2019. A survey of
blockchain from the perspectives of applications, challenges, and opportunities.
IEEE Access 7 (2019), 117134–117151.

[34] Gleb Naumenko, Gregory Maxwell, Pieter Wuille, Alexandra Fedorova, and Ivan
Beschastnikh. 2019. Erlay: Efficient transaction relay for bitcoin. In Proceedings
of the 2019 ACM SIGSAC Conference on Computer and Communications Security.
817–831.

[35] A. Osipovich. 1 Apr. 2021. High-Frequency Traders Eye Satellites for Ultimate

Speed Boost. Wall Street Journal (1 Apr. 2021).

[36] A. Osipovich. 15 Dec. 2020. High-Frequency Traders Push Closer to Light Speed

With Cutting-Edge Cables. Wall Street Journal (15 Dec. 2020).

[37] Elias Rohrer and Florian Tschorsch. 2019. Kadcast: A structured approach to
broadcast in blockchain networks. In Proceedings of the 1st ACM Conference on
Advances in Financial Technologies. 199–213.

[38] Tim Roughgarden. 2020. Transaction fee mechanism design for the Ethereum
blockchain: An economic analysis of EIP-1559. arXiv preprint arXiv:2012.00854
(2020).

[39] Avron Shane. 2021. Advantages and Disadvantages of a Peer-to-Peer Network.

Flevy Blog (2021).

13

[40] Bhavesh Toshniwal and Kotaro Kataoka. 2021. Comparative Performance Anal-
ysis of Underlying Network Topologies for Blockchain. In 2021 International
Conference on Information Networking (ICOIN). IEEE, 367–372.

[41] Aaron Van Wirdum. 2016. HOW FALCON, FIBRE AND THE FAST RELAY
NETWORK SPEED UP BITCOIN BLOCK PROPAGATION (PART 2). Bitcoin
Magazine (2016).

[42] Taotao Wang, Chonghe Zhao, Qing Yang, Shengli Zhang, and Soung Chang
Liew. 2021. Ethna: Analyzing the Underlying Peer-to-Peer Network of Ethereum
Blockchain. IEEE Transactions on Network Science and Engineering 8, 3 (2021),
2131–2146.

[43] Wenbo Wang, Dinh Thai Hoang, Peizhao Hu, Zehui Xiong, Dusit Niyato, Ping
Wang, Yonggang Wen, and Dong In Kim. 2019. A survey on consensus mecha-
nisms and mining strategy management in blockchain networks. Ieee Access 7
(2019), 22328–22370.

[44] Haifeng Yu, Ivica Nikolić, Ruomu Hou, and Prateek Saxena. 2020. Ohie:
Blockchain scaling made simple. In 2020 IEEE Symposium on Security and Privacy
(SP). IEEE, 90–105.

A APPENDIX
A.1 Proofs

A.1.1 Proof of Theorem 6.1. It is known that the set cover prob-
lem is NP-complete [24]. We take an arbitrary instance of the set
cover problem:

Problem 2. Given a finite set of elements Σ = {𝜎1, · · · , 𝜎𝑝 } and
its subsets Γ1, · · · , Γ𝑞. We aim to find the fewest collection of subsets
from Γ1:𝑞, whose union equals Σ.

We construct a graph 𝐺 = (V, E) as below. Each element 𝜎𝑖 in
Σ corresponds to a unique node of the same name. Each subset Γ𝑗
corresponds to 2 nodes 𝛾 +
𝑗 . Finally, a fresh node 𝑐 is added. In
other words, V = {𝑐} ∪ (cid:208)𝑝
𝑗 , 𝛾 −
𝑗 }.
𝑗 ) ∈ E if and only if
𝑗 , 𝑐) for each 𝑗.

𝑗=1{𝛾 +
For each pair (𝑖, 𝑗) ∈ [𝑝] × [𝑞], edge (𝜎𝑖, 𝛾 −

𝑗 , 𝛾 −
𝑖=1{𝜎𝑖 } ∪ (cid:208)𝑞

𝜎𝑖 ∈ Γ𝑗 . Besides these, E contains (𝛾 −
Fig. 5 illustrates an example of topology of 𝐺.

𝑗 ) and (𝛾 +

𝑗 , 𝛾 +

Figure 5: Topology of 𝐺 given a set cover instance where Γ1 =
{𝜎1, 𝜎2}, Γ2 = {𝜎2, 𝜎3} and Γ3 = {𝜎2, 𝜎3, 𝜎4}.

We take set of sources S ≜ {𝜎1, 𝜎2, · · · , 𝜎𝑝 } and set of desti-
nations T ≜ {𝑐}. As expected, the original distance 𝑑𝐺 (𝑠, 𝑐) = 3
for each 𝑠 ∈ S. Now we claim that if we can solve Problem 1 in
polynomial time for 𝐺, S, T where ∀𝑒 ∈ E (cid:0)𝑤 (𝑒) = 1(cid:1), 𝜏 = 1.99
(breaking ties) and 𝑘 ∈ [2, 1 +𝑞], then we can also solve the original
set cover problem in polynomial time.

14

When 𝑘 ∈ [2, 1 + 𝑞], we are able to select 𝑘 spy nodes amongst

nodes in V. The optimal choice must
• contain 𝑐: Otherwise, there must exist a spy node in Γ+, or the
placement will not be effective at all. Replacing this spy node
with 𝑐 will not decrease the advantage.

• contain only nodes in Γ− other than 𝑐: If 𝜎𝑖 is chosen, then this spy
node serves no other pair than (𝜎𝑖, 𝑐). We pick 𝑗 where 𝜎𝑖 ∈ Γ𝑗
and choose 𝛾 −
instead. This will not decrease the advantage. If
𝑗
𝑗 is chosen, then it benefits the advantage by moving it to 𝛾 −
𝛾 +
𝑗 ,
even when this creates duplicates.

Therefore, to solve the advantage maximization, we are essen-
tially picking nodes in Γ−, which corresponds to picking subsets
among Γ1, · · · , Γ𝑞. If we can solve the advantage maximization in
polynomial time for all 𝑘 ∈ [2, 𝑞 +1], we can enumerate all solutions
and pick the smallest 𝑘 where the advantage reaches 𝑝, the total
number of elements in S, and also the best advantage we have
between S and T = {𝑐}. This also solves the minimum set cover
problem known to be NP-hard, which is a contradiction. Therefore,
Problem 1 is NP-hard, and such polynomial-time algorithm exists
■
only if P = NP.

A.1.2 Proof of Proposition 6.2. We present the counter-example
in Fig. 6, which is a tree with 2𝑘 + 1 branches. In this tree, the initial
pair of nodes chosen by the greedy algorithm must be 𝑔 and ℎ𝑖
for some 𝑖 ∈ [𝑘], because a shortcut between them puts 3 pairs
at maximum of sources and destinations {(𝑔, 𝑡3𝑖−𝑗 )| 𝑗 ∈ {0, 1, 2}}
under the risk of being front-run. At each of the following steps,
the greedy algorithm will have to continue choosing an additional
peer from {ℎ𝑖 |𝑖 ∈ [𝑘]}, which further increases the advantage
by 3. Hence, with 2ℓ peers where ℓ < 𝑘/2 is an integer, the total
advantage equals 6ℓ − 3.

However, these 2ℓ peer connections can be put to better uses. If
we choose {𝑠1, · · · , 𝑠ℓ }∪{𝑟1, · · · , 𝑟ℓ } instead, then the shortcut pairs
of sources and destinations can be described by 𝑃 = {(𝑠𝑖, 𝑟 𝑗 )|𝑖 ≠
𝑗, 𝑖 ∈ [ℓ], 𝑗 ∈ [ℓ]}, where |𝑃 | = ℓ (ℓ − 1). Therefore, the ratio
of the greedy algorithm solution to the maximum advantage is
at most
ℓ , which can be arbitrarily close to 0 as ℓ, 𝑘
become arbitrarily large. Equivalently, the greedy algorithm cannot
guarantee an 𝛼-approximately optimal solution for any 𝛼 > 0. ■

ℓ (ℓ−1) ∼ 6
6ℓ−3

A.1.3 Proof of Theorem 7.2. We first show the result for a single
target, i.e., |𝑈 | = 1. Let 𝑎 denote an adversarial node and 𝑥 denote
the message sender (that is, 𝑈 = {𝑥 }). We let the adversary perform
the following 3-step strategy iteratively.

Step 1. Wait Δ1 for the first arrival of a new message 𝑀 sent by 𝑥.
Step 2. Keep only the peer that contributed to the first arrival,
and get 𝑑 − 1 random new addresses from the oracle to
replace the other peers. The probability of 𝑥 belonging in
these addresses is lower-bounded by 𝑞 > 0. Send peering
requests repeatedly until a peer slot becomes available.

If 𝑥 is already one of the neighbors at Step 1, it will surely be kept
in Step 2 because 𝑤 (𝑥, 𝑎) is already the shortest path length between
𝑥 and 𝑎 on the network, implied by the triangle inequality (8). In the
end, it will remain a peer of the adversary. Because the samplings
for replacement nodes are independent across 𝐾 iterations, we can
geometrically decrease the probability that 𝑥 is missed in the entire

cσ1σ2σ3σ4γ−1γ+1γ−2γ+2γ−3γ+3SΓ−Γ+T𝑑 − 1 peers are available.

𝑃3 ≥ 1 −

(𝑑 − 1)𝜁
Δ2

.

.

Δ2 =

We want 𝑃3 to be at least 1 − 𝜀2. This can be achieved by letting
(𝑑 − 1)𝜁
𝜀2
After taking 𝐾 iterations of these steps, the probability of finding
𝑥 equals 1 − (1 − 𝑞)𝐾 , while the probability that all executions of
Steps 1 & 3 are successful is at least 1 − 𝐾𝜀1 − 𝐾𝜀2. The overall
probability equals
(cid:104)1 − (1 − 𝑞)𝐾 (cid:105)

(1 − 𝐾𝜀1 − 𝐾𝜀2) ≥ 1 − (1 − 𝑞)𝐾 − 𝐾 (𝜀1 + 𝜀2)
𝜀
2
= 1 − 𝜀.

= 1 −

𝜀
2

−

On the other hand, the total time consumption equals
𝜀−1(cid:17)(cid:105)
(cid:16)

𝐾Δ1 + 𝐾Δ2 = 𝑂 (log 𝜀−1)

+ 𝑂

(cid:104)
𝑂

𝜀−1 log 𝜀−1(cid:17)
(cid:16)
(cid:17)

= 𝑂

(cid:16)
𝜀−1 log2 (𝜀−1)

.

(9)

Next, we show how to extend this result to an arbitrary 𝑈 via a

union bound.

From (9), we know that with probability 1 − 𝜀/|𝑈 |, the adversary
is able to find the network ID of an arbitrary node 𝑢 ∈ 𝑈 within
time

(cid:16)

𝑂

(𝜀/|𝑈 |)−1 log2 (𝜀/|𝑈 |)

(cid:17)

= 𝑂

(cid:16)

|𝑈 |𝜀−1 log2 (cid:16)

𝜀−1(cid:17)(cid:17)

.

Regardless of how the adversary allocates time to the tasks of
finding each 𝑢 ∈ 𝑈 , the probability of finding all of them is at
least 1 − |𝑈 | × 𝜀/|𝑈 | = 1 − 𝜀 by the union bound. As for the time
consumption, we consider the worst case where the agent has to
run the algorithms for finding each 𝑢 ∈ 𝑈 sequentially. In this case,
the total time consumption is the time consumption above of each
single task multiplied by number of tasks |𝑈 |, which equals
|𝑈 |2𝜀−1 log2 (cid:16)

𝜀−1(cid:17)(cid:17)

𝑂

■

(cid:16)

.

A.1.4 Proof of Proposition 7.3. First of all, we assume 𝑁 , the
number of nodes, to be upper-bounded so that the probability 𝑞
that the oracle returns a target after a single draw satisfies 0 <
𝑞1 ≤ 𝑞 ≤ 𝑞2 < 1 for some constants 𝑞1, 𝑞2. In order to connect to
the target, it is necessary for the adversary to draw it using the
oracle. Temporarily, we ignore the other necessary steps (such as
judging if an existing peer is the target) and consider only drawing
nodes. Then, the probability 𝑃 that the target is drawn within 𝐾
steps satisfies

1 − (1 − 𝑞1)𝐾 ≤ 𝑃 ≤ 1 − (1 − 𝑞2)𝐾 .
To let 𝑃 = 1 − 𝜀, we should equivalently have 𝐾 = Θ(log 𝜀−1)

because

log 𝜀
log(1 − 𝑞2)

≤ 𝐾 ≤

log 𝜀
log(1 − 𝑞1)

.

Considering that each drawing takes Θ(1) time, it takes at least
Θ(𝐾) = Θ(log 𝜀−1) time to find the target with the oracle w.p. at
least 1 − 𝜀. As argued above, drawing the target is a necessary
condition for the final peer connection to it. Hence, Ω(log 𝜀−1) is a
■
lower bound of time consumption.

15

Figure 6: Greedy algorithm cannot maximize advantage
with proximity greater than 0. 𝜏 = 3.99. Red nodes are
sources and blue nodes are destinations.

procedure by increasing 𝐾. This intuitively explains why 𝐾 is of
order 𝑂 (log 𝜀−1). Next, we make this intuition precise.

Let

𝐾 ≜

log(𝜀/2)
log(1 − 𝑞)

,

𝜀1 = 𝜀2 ≜

𝜀 log(1 − 𝑞)
4 log(𝜀/2)

.

In Step 1, it is desired that the sender sends at least 1 message
during the time window of length Δ1 with probability at least 1 −𝜀1.
We will use the following lemma, which states that it is highly
probable that during a sufficiently long window of time, a node
broadcasts at least 𝛾 messages every unit of time on average.

Lemma A.1. A Poisson process {𝑀𝑢 (𝑡), 𝑡 ≥ 0} with rate 𝜈 satisfies

that there exist constants 𝛾, 𝜇 > 0, such that
𝜇
Δ

P [𝑀𝑢 (𝑡 + Δ) − 𝑀𝑢 (𝑡) < 𝛾 Δ] <

, ∀𝑡, Δ > 0.

By Assumption (II) from Definition 7.1 and Lemma A.1, there

exist constants 𝜇, 𝛾 such that

P [𝑀𝑥 (𝑡 + Δ1) − 𝑀𝑥 (𝑡) ≥ 𝛾 Δ1] ≥ 1 −

𝜇
Δ1
1 }, we obtain the lower-bound
probability that 𝑥 broadcasts at least 1 message during any time
interval of length at least Δ1.

Here by taking Δ1 = max{𝛾 −1, 𝜇𝜀−1

.

In Step 2, we would like to wait Δ2 for all the 𝑑 − 1 new nodes
being connected to the adversary. This requires each new node to
have at least 1 free slot. We consider the worst case where none
of the slots are free in the beginning. For each target peer 𝑝, each
one of the 𝑓 slots will become free after a period of time 𝑇0, where
𝑇0 ∼ Exp(𝜆). Since one slot is already sufficient, the probability the
peer 𝑝 becomes available after Δ2 is lower-bounded by 1 − 𝑒−𝜆Δ2 .
This expression is further lower-bounded by 1 −

for 𝜁 =

.

𝜁
Δ2

1
𝑒𝜆

Then, considering all the (𝑑 − 1) peers, we may use the union
bound to derive a lower bound of the probability 𝑃3 that all the

···············h1t1t2hk−1hkt3kgs1r1sn−1rn−1snrnkbrancheskbranchesA.1.5 Proof of Lemma A.1. Let 𝜈 denote the arrival rate. For any

Δ, we assign 𝑘 = 𝜈Δ/2, and obtain

(cid:20)

P

𝑀𝑢 (𝑡 + Δ) − 𝑀𝑢 (𝑡) <

(cid:21)

𝜈Δ
2

= 𝑒−𝜈Δ

∼ 𝑒−2𝑘

⌊𝜈Δ/2⌋
∑︁

(𝜈Δ)𝑖
𝑖!

𝑖=0
𝑘
∑︁

𝑖=0

(2𝑘)𝑖
𝑖!

≲ 𝑘𝑒−2𝑘

(2𝑘)𝑡
𝑡!

sup
𝑡 ∈ [0,𝑘 ]

≲ 𝑘𝑒−𝑘
1
𝑘

∼

≲

(*)

1
𝜈Δ

.

This already justifies the claim. It remains to prove (*). By Stir-

ling’s approximation, let
(2𝑘)𝑡
𝑡!

𝑓 (𝑡) = log

As a result,

≈ 𝑡 log(2𝑘) −

log 𝑡
2

− 𝑡 log 𝑡 + 𝑡 .

𝑓 ′(𝑡) ≈ log(2𝑘) −

1
2𝑡

− log 𝑡 .

As log 𝑡 + 1/(2𝑡) monotonically increases in (1/2, ∞), we may

assert that the maximizer 𝜏 satisfies

𝑓 ′(𝜏) = 0 ⇐⇒ 2𝑘 = 𝜏𝑒

1
2𝜏 .

It can be confirmed that 𝜏 ∈ [0, 𝑘]. Hence,
(2𝑘)𝑡
𝑡!

2𝜏 )𝜏
(𝜏𝑒
√
𝜏𝜏𝜏𝑒−𝜏

∼

1

sup
𝑡 ∈ [0,𝑘 ]

≲ 𝑒𝜏 ≲ 𝑒𝑘 .

■

A.2 Evaluation of direct targeted latency over

Ethereum Mainnet

We ran limited experiments on targeted latency by measuring trans-
actions from a target / victim node in Germany operated by Chain-
link ; the node sends 2 transactions per hour on average. These
transactions originate from an account (Ethereum address) exclu-
sive to the victim node, i.e., all transactions are sent only by the
victim. The node kept running on the network during our experi-
ments.

We evaluate each of the four methods from §5.1.1, measuring
both their ability to reduce targeted latency and their ability to con-
nect to (i.e., infer) the IP address of our target node on the Ethereum
main P2P network. To this end, we establish 4 EC2 instances in the
ap-southeast-1 AWS data center in Singapore, which have a 160
ms round-trip time to Germany. They are located within the same
subnet as a public bloXroute relay. As in §5.1, we deploy a full Go-
Ethereum node on each host with at most 50 peers. The proportion
of outbound peers remain the same. We conduct the experiments
under the same procedure and take similar anti-biasing measures,
except for three key differences. First, we define the relevant trans-
actions as only those sent by the target’s account. Second, we set
the Peri period to 30 minutes to match the frequency of source
transactions. Third, the duration of each experiment is extended
from 8 hours to 16 hours to permit a larger number of Peri periods.
We ran 23 experiments from March 19, 2022 to April 19, 2022.
The Hybrid and Baseline clients were able to establish connections

16

Figure 7: PDFs of distributions of targeted latency.

to the victim node once each. The distributions of targeted latency
of each method are displayed in Fig. 7. In total, we analyzed the
latencies of 654 transactions.

Over distributions in Fig. 7, we performed Kruskal-Wallis test
and obtained p-value = 0.0235, which allows us to reject the null
hypothesis at significance level 5% and continue with a Dunn’s test.
Then, we performed Dunn’s test with Bonferroni correction over
these distributions. The p-values are listed in Table 3. At significance
level 5%, we cannot assert a difference between the means of any
pair of distributions. This indicates that all the methods share a
similar ability to reduce direct targeted latency. We can further
observe this phenomenon from Fig. 7, where they share a similar
ability to reduce direct targeted latency by 14% to 18% of the end-
to-end delay, and the hybrid method outperforms the other two
methods with a slight advantage. Unlike reduction of direct global
latency, the Peri algorithm is no longer significantly worse than the
bloXroute relay network at reducing direct targeted latency, which
makes it a free replacement of bloXroute services for agents with
specific targets. An agent with bloXroute services can also further
boost the latency reduction by an additional 13% by stacking the Peri
algorithm and turning hybrid. In addition, Peri can potentially help
an agent identify the IP address of a victim, while bloXroute, which
intermediates connections, cannot support such functionality.

Pair
p-value

(B, P)
1.0

(B, H)
0.0532

(P, H)
0.0529

Table 3: Pairwise p-values of Dunn’s test over distributions
of targeted latency in Fig. 7, comparing bloXroute (B), Peri
(P), and Hybrid (H).

Figure 8: Advantage-Peer-count curves on original random
graph models. The mean of the curves are shown by solid
lines, and the standard deviations are shown by the trans-
parent color zones centered at the mean.

A.3 Simulations of Advantage Maximization
Algorithms on Original Topologies

We synthesize the network models as in §6.2 without centralizing
them by introducing hub nodes. We reuse other setups in the orig-
inal simulation, and plot the advantage-peer-count curves in Fig.
8.

For all the graph models, both Peri and Greedy achieve a higher
advantage 𝐴N than the random baseline, with Greedy outperform-
ing Peri by varying amounts. For the most decentralized models,
such as the random regular and small world, the advantage of Peri
is much closer to that of random baseline than the greedy algo-
rithm. On the other hand, for models with a few high degree nodes
(i.e., hubs), Peri inserts shortcut peering connections almost as well
as the greedy algorithm, in spite of its limited knowledge of the
graph. Therefore, the performance of Peri is likely to be stronger on
networks with many hubs. This is consistent with our conclusion
in §6.2.

17

