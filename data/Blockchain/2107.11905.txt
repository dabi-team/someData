Revealing the Landscape of Privacy-Enhancing Technologies in the Context of Data
Markets for the IoT: A Systematic Literature Review

Gonzalo Munilla Garridoa,˚, Johannes Sedlmeirb, ¨Omer Uluda˘ga, Ilias Soto Alaouia, Andre Luckowc, Florian Matthesa

aTechnical University of Munich, Department of Informatics, Munich, Germany
bProject Group Business & Information Systems Engineering of the Fraunhofer FIT, Bayreuth, Germany
cLudwig Maximilian University of Munich, Department of Computer Science, Munich, Germany

2
2
0
2

l
u
J

2
1

]

R
C
.
s
c
[

2
v
5
0
9
1
1
.
7
0
1
2
:
v
i
X
r
a

Abstract

IoT data markets in public and private institutions have become increasingly relevant in recent years because of their potential to
improve data availability and unlock new business models. However, exchanging data in markets bears considerable challenges
related to disclosing sensitive information. Despite considerable research focused on diﬀerent aspects of privacy-enhancing data
markets for the IoT, none of the solutions proposed so far seems to ﬁnd a practical adoption. Thus, this study aims to organize the
state-of-the-art solutions, analyze and scope the technologies that have been suggested in this context, and structure the remaining
challenges to determine areas where future research is required. To accomplish this goal, we conducted a systematic literature
review on privacy enhancement in data markets for the IoT, covering 50 publications dated up to July 2020, and provided updates
with 24 publications dated up to May 2022. Our results indicate that most research in this area has emerged only recently, and
no IoT data market architecture has established itself as canonical. Existing solutions frequently lack the required combination of
anonymization and secure computation technologies. Furthermore, there is no consensus on the appropriate use of blockchain tech-
nology for IoT data markets and a low degree of leveraging existing libraries or reusing generic data market architectures. We also
identiﬁed signiﬁcant challenges remaining, such as the copy problem and the recursive enforcement problem that – while solutions
have been suggested to some extent – are often not suﬃciently addressed in proposed designs. We conclude that privacy-enhancing
technologies need further improvements to positively impact data markets so that, ultimately, the value of data is preserved through
data scarcity and users’ privacy and businesses-critical information are protected.

Keywords: Anonymization, Big Data, Copy Problem, Data Exchange, Marketplace, Platform, Secure Computation

1. Introduction

IoT devices have been improved, mass-produced, and deployed
in the past few decades through steady progress in informa-
tion and communication technologies (ICTs) and motivated by
a trend of data-driven decision-making, automation, and the
opportunity for new business models.
IoT devices’ primary
collective purpose is to interact with the physical world and
enable the measurement and collection of events and interac-
tions [S23]. These characteristics apply to IoT devices de-
ployed in, for example, a factory or a powerline network and
many devices employed by people, such as cell phones, laptops,
or wearables. The volume, velocity, and variety of the informa-
tion generated by the IoT is immense, which drove practition-
ers to coin the term big data and develop tools for their analy-
sis [S4]. Public and private institutions use big data to promote
the public good, innovations, and improve products and ser-
vices. Big data has become the foundation of the emerging data
economy, which in Europe was worth nearly 2 % of its GDP
in 2016, close to 300 billion Euros [1]. However, the genera-
tion, collection, storage, processing, distribution, and analysis

˚Corresponding author: Gonzalo Monilla Garrido

Email address: gonzalo.munilla-garrido@tum.de
Address: Boltzmannstrasse 3, 85748, Garching

of big data to realize such economic potential also come with
challenges for enterprises and responsibilities towards society.
Big data needs to be accessible to institutions that can har-
ness their potential and develop innovations, lest society fails to
materialize their advantages. Unfortunately, a signiﬁcant share
of the world’s data is siloed and exploited solely by the institu-
tions that host them [2], consequently locking the untapped po-
tential of the data economy and hindering progress in science,
business, and society. To surmount this obstacle, a paradigm
shift towards openness emerged in the form of electronic data
markets for the IoT, i.e., mediums for the trade of information
across the Internet based on electronic infrastructure [3]. This
paradigm brings potential beneﬁts, such as increasing the eﬃ-
ciency of business processes, facilitating growth by unlocking
new business models [4], and proﬁting from trading. Decision-
makers in governments and businesses have recognized the
economic potential of data markets and hence recently sup-
ported signiﬁcant projects that provide a shared digital infras-
tructure for data-sharing initiatives such as GAIA-X [5] or the
automotive-related Catena-X, which promote the collaboration
of large enterprises in data markets.

Despite data markets’ promise to beneﬁt society by fostering
innovation and collaboration across enterprises, these markets
hold the risk of exposing individuals’ and businesses’ sensitive

Preprint submitted to Journal of Network and Computer Applications

July 14, 2022

 
 
 
 
 
 
information [6, 7]. Moreover, conﬁdence in privacy protec-
tion is an essential driver of users’ willingness to share their
data [S8]. Similarly, businesses are unwilling to bear the risk
of unintentionally leaking their customers’ private or business-
critical information. Consequently, the adoption of data mar-
kets is generally hampered. Additionally, while a corporation
may have taken security measures to protect collected data from
unauthorized access or unintended use, data buyers might not
have the same standards. Hence, in this case, exchanging data
entails an additional risk that the seller needs to mitigate be-
fore the data are shared. Furthermore, blockchains are expected
to play an essential role in the ability of institutions to trade
data in tokenized form [8], but their inherent transparency fur-
ther increases the need to make data exchanged in markets less
sensitive [S25]. Additional trends that aggravate the negative
consequences of lacking data protection for institutions are re-
cent privacy laws such as the GDPR in Europe or the CCPA
in California with their increasingly expensive ﬁnes for data
breaches [9].

As a ﬁrst reaction to these risks, practitioners and corpora-
tions have increased their systems’ security. However, if data
are sold and, thus, replicated, conﬁdentiality is not suﬃcient
to protect privacy: Only managing or modifying the data in
a way that enhances privacy while preserving as much utility
as possible is eﬀective [10]. Thus, institutions have started
to allocate more resources to balance data utility and privacy,
employing privacy-enhancing technologies (PETs) [S27]. The
term PET was coined in 1995 in a report by the Dutch Data
Protection Authority and the Ontario Information Commis-
sioner [11] that explored a novel approach to privacy protec-
tion [12]. These technologies take the form of architectures
built with privacy-by-design principles and policies [S39][S6],
or data modiﬁcations based on heuristics or mathematical pri-
vacy guarantees. Prominent examples of PETs are diﬀeren-
tial privacy [13, 14], syntactic anonymization deﬁnitions like k-
anonymity [15], homomorphic encryption [16–18], trusted ex-
ecution environments [19], secure multiparty computation [20],
zero-knowledge proofs [21][22], and a set of conventional de-
identiﬁcation approaches such as masking, rounding, or hash-
ing [23].

The relevance of PETs in data markets also increases with
the growing adoption of IoT devices, such as in vehicles,
wearables, smartphones, and the applications that stream data
daily from millions of individuals’ private lives to data mar-
ketplaces [24]. Despite their current relevance and growing at-
tention [S27], researchers and institutions still ﬁnd PETs chal-
lenging to understand, integrate, and deploy in IoT data mar-
kets because most PETs are technically complex and have a
wide range of variations and combinations with diﬀerent trade-
oﬀs [25]. Regarding research addressing these challenges, pri-
mary studies are predominant, i.e., studies based on original
designs developed or data collected by their authors, while sec-
ondary studies collecting and systematizing existing knowledge
are less frequent. The applications proposed by primary studies
range from funneling data from markets into machine learn-
ing (ML) algorithms [S2][S25][S43], crowdsourcing data into
markets [S16][S22][S24][S47], adopting data markets for smart

mobility ecosystems [S3], smart manufacturing [S19], smart
homes [S11], and smart wearables in the health industry [S48].

On the other hand, 9 out of the 50 studies that we identi-
ﬁed in our systematic literature review (SLR) are secondary,
and out of these, four studies [S19][S23][S35][S48] cover
some of the PETs available for data markets for the IoT,
yet without giving a detailed comparison of their functionali-
ties, beneﬁts, and limitations. The other ﬁve secondary stud-
ies [S4][S8][S14][S27][S38] perform high-level surveys re-
volving around challenges, non-technical privacy strategies,
and user-centric perspectives on data markets for the IoT. How-
ever, none of these secondary studies provided a rigorous, sys-
tematic review that collected and mapped PETs and challenges
comprehensively. Moreover, as we discuss in Section 9, we
noted a low level of re-using existing components to build a
more holistic architecture for data markets in related work,
which may indicate the need for systematically analyzing the
current seminal components, strengths, and weaknesses of so-
lutions proposed for privacy-enhancing IoT data markets.

Consequently, we tackle the research gaps mentioned
above with a comprehensive and detailed SLR that aims to
guide decision-makers, privacy oﬃcers, policymakers, and re-
searchers in the challenge of employing PETs to build or par-
ticipate in privacy-enhancing IoT data markets. We guide these
stakeholders by identifying, classifying, and describing how
PETs are leveraged in the current body of scientiﬁc knowledge
(see Sections 6 and 7) and presenting key ﬁndings from our
SLR (see Section 9). Moreover, for the beneﬁt of the reader, we
distill terminology from the extant literature to diﬀerentiate and
navigate the concepts of PETs in the scope of this SLR (see Sec-
tion 5). We also organize related work into a reference model
for the use of PETs in IoT data markets in distinct categories
(see Fig. 10 and Fig. C.11) and identify narrow and broad chal-
lenges that PETs can tackle or circumvent (see Fig. 8). Through
mapping PETs to the distilled terminology and the identiﬁed
narrow challenges, we want to support practitioners in making
informed decisions about the appropriate PETs to employ in the
context of IoT data markets (see Table 3).

The remainder of the paper is structured as follows. Sec-
tion 2 introduces the main concepts of privacy, data markets,
and the IoT. Section 3 portrays how we conducted our SLR on
publications dated before July 2020, followed by a discussion
of related work in Section 4 and a distillation of terminology
in Section 5. Sections 6 and 7 present the main results from
analyzing the content of the studies in our SLR, followed by a
structured review of challenges in Section 8. Based on these re-
sults and the studies’ metadata, we extract a set of key ﬁndings
and artifacts in Section 9, where we also provide future work
and discuss the limitations of our research. Finally, Section 10
updates our study by including new selected publications from
May 2022 to July 2020, and Section 11 concludes with a sum-
mary of the results.

2

2. Background

2.1. Privacy
Given the increased attention and relevance of privacy during
the past decades, practitioners have provided many acknowl-
edged deﬁnitions. For example, A. F. Westin [26] stated that
“[Privacy is] the claim of individuals [...] to determine for them-
selves when, how and to what extent information about them
is communicated [...]”. Similar deﬁnitions have been given
by other authors like G. A. Fink et al. [27] or K. Renaud and
D. Galvez-Cruz [28]. Despite these eﬀorts, D. J. Solove argued
that any attempt to distill a unique, timeless deﬁnition is infeasi-
ble due to privacy’s multifaceted concept [29]. However, in the
ﬁeld of computer science, a narrower deﬁnition may be possible
by adopting an attack model perspective, as the concept of pri-
vacy would likely not have emerged if transgressors would not
exist: attackers of one’s secrets tacitly give meaning to privacy.
Therefore, a helpful deﬁnition in the context of computer sci-
ence may be F. T. Wu’s [30]: “[Privacy] is deﬁned not by what
it is, but by what it is not – it is the absence of a privacy breach
that deﬁnes a state of privacy.” F. T. Wu hence deﬁned privacy
as a product of a threat model, the one from M. Deng et al. [31],
in which a practitioner needs to determine what information to
hide, from whom, and what harms should be prevented before
deﬁning legal and technical tools.

Once IT architectures and tools enhance privacy appropri-
ately, other advantages emerge. For example, from an eco-
nomic perspective, privacy enables data utilization across or-
ganizations and applications to create new fair products and
services and prevent price discrimination [32]. Furthermore,
employing PETs may increase the number of sources and data
harvested by institutions because PETs help to overcome reg-
ulatory barriers [33], in addition to mitigating the risk of ﬁnes
and diﬀerentiating and appreciating a brand [4]. Moreover, po-
litical freedom and stability may only be achieved by unobtru-
sive forms of governments [34], privacy-enhancing journalism,
and less pervasive forms of digital products such as social me-
dia that can enable malicious social engineering [S38]. More-
over, research indicates that compromising privacy can result in
negative long-term economic eﬀects [35].

Despite these beneﬁts, and while consumers emphasize that
privacy is important to them, they are typically not willing to
make small additional eﬀorts or pay for privacy [36], the so-
called privacy paradox. Thus, in the past decades, governments
have enacted rules and laws to protect consumers against viola-
tions of their privacy, speciﬁcally for data captured through ad-
vanced ICTs such as personal computers, the world wide web,
or smartphones. Examples include the European Data Protec-
tion Directive in 1995, the HIPAA Privacy and Security Rule of
1996, the APEC cross-border privacy rules of 2011, the GDPR
of 2016, and the Consumer Privacy Act of 2020 in the USA,
which comprises Acts such as the CCPA of 2018 in California.

2.2. Data markets
According to F. Stahl et al. [3], there is a misconception in
everyday language between the terms “market” and “market-
place”: A marketplace is the implementation of a market in

terms of infrastructure, time, and location (virtual or physical)
where the participants transact. Markets are the environments
where buyers and sellers set the price and quantity of a particu-
lar good. Marketplaces have evolved over millennia; however,
the most drastic changes arguably have happened in the past
few decades. ICT has driven the costs of instant and ubiquitous
communication to an often negligible amount, which has led to
the digitization of many existing transaction-based ecosystems,
including marketplaces [3]. Moreover, ICT has enabled the cre-
ation of virtual marketplaces that did not exist before [37] – the
most prominent example being e-commerce.
In this context,
data have become goods themselves [3]. Data markets incen-
tivize institutions to collect more data and to proﬁt from trading,
and, in turn, the resulting improvements and innovations beneﬁt
the public good [38].

Beyond the above formal deﬁnition, from the selected stud-
ies, we can carve out several characteristics of data market-
places: Y. Li et al. [S1] indicated that most of the data market-
places in operation are centralized, where the platform is run
by either a trusted third party (a broker) that coordinates buyers
and sellers or by the data owner (e.g., a large institution) who is
also selling the data. Another 15 selected studies also proposed
decentralized architectures employing distributed ledger tech-
nology to counter the drawbacks of centralized systems (e.g.,
single point of failure or trust on a potentially malicious en-
tity). On the other hand, Z. Guan et al. [S46] took another
perspective, characterizing data trading platforms depending on
the number and type of data domains: general platforms include
data from any source type, while specialized platforms focus on
one domain, e.g., ﬁnancial, healthcare, or social media. C. Per-
era et al. [S27] identiﬁed two categories for data markets based
on the type of participant: companies or private individual cus-
tomers, e.g., owners of a smart home. Altogether, we distilled
three dimensions for characterizing data markets: (i) the degree
of centralization, (ii) the types and number of data domains,
and (iii) the types of sellers and consumers. These dimensions
permeate most of the identiﬁed solutions in this study, and all
exhibit individual privacy trade-oﬀs of which practitioners need
to be aware (see Section 9).

2.3. The Internet of Things

The IoT is considered a network of physical devices that lever-
ages sensors to measure and collect information from the real
world and support the access and exchange of data via the Inter-
net instantly and ubiquitously [39]. IoT devices are considered
essential for gathering big data [40], which in turn brings new
opportunities such as targeted advertisement, predictive main-
tenance, and quality improvements. Consequently, many com-
panies have introduced the IoT in their strategy for participat-
ing in the data economy [39] and make substantial investments
in the technologies that make them possible: sensors, wire-
less networks, and cloud computing infrastructure [40]. In this
SLR, the deﬁnition of the IoT includes any device with a CPU
connected to the Internet, including sensors in factories, sup-
ply chains, or vehicles, and devices such as smartphones, wear-
ables, and computers that people use daily. These devices act

3

as data collectors and as the gateway to a plethora of applica-
tions that collect users’ actions and behavior, such as browsers,
social media, e-commerce, or media entertainment, as well as
sensor data generated in business processes like manufacturing
and predictive maintenance, and use them for analyses and pre-
dictions.

The design and implementation of data markets are depen-
dent on the IoT. The ubiquity of IoT devices generates many
constellations for diﬀerent degrees of decentralization, with a
myriad of possible sources and prosumer types. Furthermore,
while such ubiquity will likely boost the data economy and its
products and services, IoT devices also permeate many aspects
of an individual’s life, e.g., dealing with highly sensitive health-
care data or capturing sensitive information from a business per-
spective. Hence, the sensitivity of the data gathered from IoT
devices calls for the implementation of PETs.

3. Research process

3.1. Goal and research questions

We employed the Goal-Question-Metric paradigm [41] to for-
mulate the focus of this study as follows: we systematically
analyze peer-reviewed literature to provide an overview of the
state-of-the-art concerning available research and trade-oﬀs on
privacy-enhancing data markets for the IoT as well as potential
research gaps from the point of view of both scholars and prac-
titioners. Based on this paradigm, the research questions (RQs)
we pursued were:

RQ1. What relevant PETs enable IoT data markets?
By answering this RQ, we aim to reveal, describe, and classify
PETs in the context of data markets for the IoT based on their
fundamentals and applications to give researchers and practi-
tioners an overview of the PETs researched and employed so
far.

challenges and trade-oﬀs hinder privacy-

RQ2. What
enhancing IoT data markets?
Through answering this RQ, we account for explicit and
implicit challenges depicted and tackled in existing work so
that researchers may quickly identify pain points in the ﬁeld
and focus their research.

3.2. SLR execution

We conducted a SLR based on the guidelines of B. A. Kitchen-
ham and D. Budgen [42]. SLRs aim to collect, structure, and
summarize the existing evidence and gaps in a particular re-
search ﬁeld to pave the way for future research. Furthermore,
SLRs need to provide a rigorous and auditable methodology
that can be reviewed and replicated [43]. SLRs deﬁne research
questions, and a set of predeﬁned inclusion and exclusion cri-
teria that assess potentially relevant primary studies to answer
them [44][45]. Table F.10 of the Appendix contains the criteria
for this SLR related to focus, quality, and accessibility.

To conduct the study search, we identiﬁed the most relevant
publications in the ﬁeld of privacy-enhancing data markets for

the IoT to answer our research questions [46][47]. To obtain a
corpus of high-quality publications, we deﬁned a search strat-
egy based on the work of H. Zhang et al. [47]. Accordingly, our
strategy consisted of three phases:

(i) A preliminary search of the base literature. The base liter-
ature includes representative papers (8) in the ﬁeld of privacy-
enhancing data markets for the IoT known to the researchers
before the SLR, and some other publications found manually
in the digital library of the researchers’ university, which also
complied with the criteria described in Table F.10. We cre-
ated preliminary search strings based on identiﬁed keywords
and synonyms that we found in the base literature and research
questions. Afterward, we parsed our base literature with a tool
to analyze frequent phrases and keywords. Using the results of
this analysis, we reﬁned our search terms [48]. Finally, we clus-
tered the search terms into three strings based on the ﬁeld of this
SLR: Privacy, data markets, and IoT. Altogether, we composed
the following search strings:

C1: privacy OR private OR encryption OR encrypted OR en-

crypt OR data protection

C2: data market OR data marketplace OR data trading OR

data broker OR data trader OR data auction

C3: Internet of things OR Internet of everything OR IoT OR
sensor OR connected devices OR networked devices OR smart
devices OR controller OR edge computing OR cloud infrastruc-
ture OR machine to machine OR M2M OR web-of-things OR
WoT OR mobility OR automotive OR vehicle OR car OR auto-
mobile OR industry 4.0 OR smart grids OR V2V OR IIoT OR
machine learning OR mobile OR cyber-physical OR microser-
vice OR microcontroller OR micro-service OR micro-controller
OR blockchain OR neural network OR smart learning OR auto-
mated driving OR autonomous driving OR smart city OR smart
factory.

Consequently, we deﬁned our ﬁnal search string as C1 AND

C2 AND C3.

(ii) The main search. Since no single source may contain
all the high-quality, relevant publications [49][50], we selected
seven electronic databases (see Table F.9 that focus on computer
science or software engineering and, according to L. Chen et al.,
cover the most relevant databases in these ﬁelds [51]. The time
frame that we speciﬁed covers any publication included in the
selected digital libraries before the 13th of July 2020. With the
deﬁned search string, time interval, databases, and following
the process Fig. 1 depicts, we collected 1291 studies (1136 af-
ter duplicates removal), which two researchers ﬁltered indepen-
dently and redundantly by title (119 selected out of 1136), ab-
stract (79 selected out of 119), and body (37 selected out of 79)
following the predeﬁned inclusion and exclusion criteria of Ta-
ble F.10 to reduce bias. After each of the three ﬁltering phases,
both researchers resolved conﬂicts in an informed discussion
and attended to the criteria.

(iii) A backward search of the references of the 37 studies
resulting from the main search. After ﬁltering by title, abstract,
and body, considering our inclusion and exclusion criteria, we
included another 11 studies in our corpus. The process resulted
in a total of 50 studies from which we subsequently extracted
and synthesized data. Hence, the SLR yielded a considerable

4

Figure 1: Study selection process.

but not excessive number of results. Furthermore, thanks to
the multiple synonyms in the search string,
the 37 studies
only missed two studies from the base literature. Moreover,
the backward search only added a modest number of new
works (11). Thus, the process suggests that the choice of
search terms was suitable.

To answer the research questions, we performed a data ex-
traction of key information from the 50 publications in a struc-
tured manner [52]. To reduce the degree of bias, two scien-
tists deﬁned and independently followed an extraction card,
which contained the following twenty ﬁelds: Authors, cite
count, year, country, publication channel, publication type, pub-
lication source, research type, research approach, contribution
type, tags, topic, subtopic, sub-subtopic, research goal, re-
search questions, study ﬁndings, privacy-enhancing architec-
ture or technologies, challenges, and future work. After the
two scientists completed the data extraction, they held an in-
formed discussion to resolve any possible conﬂicts on the ex-
tracted information. For the data synthesis necessary to answer
RQ1 and RQ2, we adapted the “narrative synthesis” method
described by B. A. Kitchenham and D. Budgen [42] and per-
formed the following synthesis procedure: (i) we developed a
preliminary synthesis of the ﬁndings, followed by (ii) explor-
ing relationships in the data and (iii) reﬁning the preliminary
synthesis with the newly acquired knowledge. After the reﬁne-
ment, we returned to the second step until we deemed the RQs
answered.

Finally, with the goal of including signiﬁcant updates and
reaﬃrm the ﬁndings extracted from our initial research process,
we conducted the same systematic search process for publica-

tions dated between July 2020 and May 2022 (24 new publica-
tions) and included the ﬁndings in Section 10.

4. Related work

In our SLR, we found nine secondary studies,
i.e., stud-
ies that systematize and organize existing knowledge, con-
ducted in the context of privacy enhancements for
IoT
data markets [S4][S8][S14][S19][S23][S27][S35][S38][S48],
which Table B.4 summarizes.
Some of these studies fo-
cused on privacy-related challenges in data markets for the
IoT [S14][S19][S27][S38], while others delved into PETs from
a technical perspective and discussed their challenges and op-
portunities [S23][S48][S35]. Lastly, [S8] analyzed users’ pref-
erences in privacy-enhancing data markets, and [S4] listed tech-
nical design choices for data markets for the IoT.

These secondary studies provided valuable contributions and
built the foundation of our work; however, they focused on
diﬀerent aspects of IoT data markets and, therefore, lacked
depth in the concepts we present in this study. For exam-
ple, [S23][S35][S48] brieﬂy discussed, altogether, blockchain
technology, secure and outsourced computation, k-anonymity,
and diﬀerential privacy and sketched their implications with-
out considering other PETs that we identiﬁed in our SLR. Fur-
thermore, although [S19] provided an overview of the available
technologies and their challenges, the authors did not discuss
PETs in detail, e.g., the study mentioned anonymization but
did not delve into k-anonymity or diﬀerential privacy. Addi-
tionally, the authors only discussed three out of the six chal-
lenges we found: the recursive enforcement problem, the util-
ity and privacy trade-oﬀ, and attacks on privacy. Moreover,
J. Pennekamp et al. [S19] based their results on observations

5

Filter based on (F3, F4, F6 & F7)StartRemove Duplicates (F5)37 (main search)Add base literature and backwards search results50(final selection)EndSelection criteriaF1: Coarse focus F2: Narrow focusF3: Publication channel typeF4: LanguageProcess flowNumber of studies after a process stepPerformed by researcher 1Performed by researcher 2F5: DuplicatesF6: Peer-reviewF7: Full-text accessFilter based on title (F1, F2)Filter based on title (F1, F2)156CR12711361291Filter based on abstract (F1, F2)Filter based on abstract (F1, F2)78CR70119Filter based on content (F1, F2)Filter based on content (F1, F2)35CR3879CRConflict resolutionfrom exemplary use cases and, therefore, cannot provide the
scientiﬁc rigor and comprehensiveness of a SLR. The remain-
ing secondary studies focused on privacy strategies instead of
technology, discussed digital rights, described challenges at a
high level, or provided a user-centric view of data markets.

Regarding PETs classiﬁcation, some selected papers in-
cluded in our SLR provided a framework.
Notably,
S. Sharma et al. [S48] considered two categories for clas-
siﬁcation (outsourced computations and information shar-
ing), whereas [S19] provided a more involved classiﬁcation
than [S48] with ﬁve layers: data security, data processing,
proving support, platform capabilities, and external measures.
Furthermore, outside of our SLR, a notable framework devel-
oped by A. Trask et al. [10], which was heavily inspired by
H. Nissenbaum’s work on contextual integrity [53], dissected
an information ﬂow into input, computation, and output, and as-
sessed privacy and veriﬁability in each step. They also wrapped
their framework with ﬂow governance, i.e., the information
ﬂow rules upon which participants agree. To provide an im-
proved classiﬁcation of PETs, we inspired some components
of our classiﬁcation from [10][S19] and distilled from the 50
selected papers the set of layers necessary to build a privacy-
enhancing IoT data market: veriﬁcation, storage, communica-
tion, processing, and sovereignty, which Table 1 describes suc-
cinctly. Moreover, we also considered important layers neces-
sary for a functional data market that do not require PETs (see
Fig. 10).

Furthermore, not all of the technologies included in [S19] en-
hance privacy, e.g., version control and most distributed ledger
technologies. Therefore, unlike in [S19], we have introduced
another branch for technologies focused on authenticity, which
we call authenticity-enhancing technologies (AET). Note that
some PETs accomplish data authenticity or integrity while en-
hancing privacy or conﬁdentiality, e.g., zero-knowledge proofs,
homomorphic encryption, or some digital signatures (see ter-
minology in Section 5). Speciﬁcally, some PETs can also
be AETs, but AETs are not always PETs. Lastly, we clas-
siﬁed the identiﬁed AETs into the veriﬁcation and consensus
layers, which are strongly associated with distributed ledger
technology, as they coordinate entities and provide veriﬁca-
tion guarantees. We display our classiﬁcation framework in
Fig. 2. Accordingly, we structure our key results into privacy-

Table 1: Short description of the involved layers.

Layer

Storage
Processing

Description

Persists data for future use.
Uses data from storage and runs algorithms on it, typi-
cally to extract information.

Communication Exchange of data with other devices.
Veriﬁcation

Sovereignty

Consensus

Checking via processing whether the data received and
the identities involved are authentic.
Ability to govern which (sensitive) information the
communication with others exposes.
A special case of communication and veriﬁcation in
which data is compared and synchronized with other de-
vices’ data.

Figure 2: Overview of the categories of our classiﬁcation of the identiﬁed tech-
nologies among the selected studies. Note that some PETs also enhance au-
thenticity.

enhancing technologies (PETs, Section 6), and authenticity-
enhancing technologies (AETs, Section 7). The authors of the
selected 41 primary studies jointly employed the technologies
included in our classiﬁcation to create holistic or parts of data
market architectures for the IoT; Tables D.5, D.6, and D.7 de-
scribe the most salient architectures.

Overall, none of the related work conducted a SLR to create
a holistic view of the body of scientiﬁc knowledge in privacy-
enhancing IoT data markets, which, therefore, indicates the lack
of an academically rigorous secondary study in this ﬁeld [42].
Furthermore, despite the eﬀorts in [S48][S19][10], there is not
yet a comprehensive classiﬁcation and ﬁne-grained analysis of
technologies and challenges that researchers have studied in
the context of privacy-enhancing IoT data markets (see Sec-
tions 6, 7, and 8). Lastly, unlike other secondary studies, we
also provide a detailed mapping of technologies, IoT data mar-
ket layers, and challenges in Table 3.

5. Terminology

To help the reader follow our SLR, we ﬁrst provide some ter-
minology. These deﬁnitions are the distillation of the concepts
found in the 50 selected studies and other seminal studies re-
garding utility and integrity [54], and conﬁdentiality and pri-
vacy [55]. When we use the word assure, a technology fully
guarantees the quality of the data or computation. In contrast,
when we use the term enhance, a technology improves the qual-
ity of the data or computation to some extent. These qualities
concern with authenticity, integrity, conﬁdentiality, privacy,
and utility. In line with the deﬁnition of privacy of Section 2.1
in the context of computer science, we deﬁne these qualities by
the absence of an attack against them, if applicable.

Data authenticity is preserved when a malicious entity has
not tampered with the truthfulness of the original data; truth-
fulness covers both provenance and integrity.
In the context
of PETs, the degree of authenticity of data can be reduced to
enhance privacy. Correspondingly, identity authenticity is pre-
served when a malicious entity has not impersonated another
entity. If identity authenticity is assured, then the provenance
of the data is also assured. In the context of PETs, the identity of
an entity can be concealed to enhance privacy. Data integrity is
preserved if data that have been copied and stored or are in mo-
tion are equal to the original [54]. In practical scenarios where
data are exchanged, if integrity is not preserved, then data au-
thenticity is also inherently violated. Computational integrity is

6

 Privacy- and  authenticity- enhancing  technologies for IoT  data markets Authenticity-enhancing   technologies Consensus layer Verification layer Privacy-enhancing  technologies Verification layer Communication layer Processing layer Sovereignty layer Storage layerFigure 3: Classiﬁcation of PETs employed for data processing. Any other privacy approach encountered in the SLR without explicit inclusion of the underlying
technology was either not included in a leaf node but in a parent node or completely dismissed if too vague. *The publication reviews or brieﬂy comments on the
technology without delving in-depth or using it as a building block of the architecture concept, e.g., included for future work.

7

 Privacy-enhancing  technologies Verification layer Storage layer Communication layer Processing layer Secure and  outsourced  computation Zero-knowledge proofs Non-interactive [S43] Interactive [S43*] Secure multiparty  computation  [S23*][S48*] Secret sharing  [S19*] Additive  [S5][S34] Shamir ECIES  [S10] Garbled circuits  [S34]  Homomorphic encryption  [S4*][S19*][S48*] Partial  [S46] Paillier  [S26][S41] Boneh-Goh-Nissim  [S7] Hash-ElGamal  [S34] Fully  [S10] Functional encryption Multi-client [S43] Single Trusted execution environments [S19*] SGX  [S2][S25][S45] Keystone  [S25*] Sanctum [S25*] Privacy-preserving data mining [S23*][S2][S25][S45] Anonymization Semantic Differential privacy  [S4*][S8*][S19*][S27*] [S48*][S35*][S49*] Local [S3][S9][S15] [S16][S37][S42] Global  [S2][S18][S22][S24] [S25][S28][S50] Syntactic K-Anonymity  [S3][S8*][S27*][S28][S33] [S49*] [S35*][S36][S47] Non-sensitive  attribute re- identification  prevention L-Diversity  [S47] T-Closeness  [S47] Generalization  ß-likeness  [S44] Suppression  [S36] Generalization  [S21][S36] Perturbative Categorical Perturbation  ß-likeness  [S44] Numerical Additive noise  [S19*][S36] [S47][S48*] Random space  perturbation [S48*] Geometric  perturbation [S48*] Pseudonym  creation  [S29*] Asymmetric encryption ElGamal  [S7] Deterministic encryption  Order-preserving  encryption  [S48*] Sovereignty layerpreserved when, even in the presence of malicious entities, the
output of an algorithm that runs on data is computed correctly.
In the context of PETs, the computation can be concealed to en-
hance privacy. Furthermore, some technologies enhance conﬁ-
dentiality, i.e., ensure that data or speciﬁc properties thereof
are only shared only with the intended parties. Furthermore,
we refer to utility as a measure of the usefulness of data for the
successful completion of a task; it is high when the data is au-
thentic. While PETs reduce authenticity, they are helpful in the
balancing act between utility and privacy, as PETs may help to
facilitate the sharing of data, which otherwise would not have
been revealed (zero utility).

We brieﬂy give some illustrative examples of the interplay of
concepts: A digital signature can assure data integrity provided
the corresponding private key is not accessible to an adversary,
and identity authenticity if the signature includes a digital cer-
tiﬁcate that a trusted third party issued; otherwise, digital sig-
natures cannot assure identity authenticity. Distributed ledgers
can assure data and computational integrity by replicated stor-
age and computation [56], but these ledgers cannot enhance
data authenticity; additionally, replication is often problem-
atic regarding conﬁdentiality and, hence, privacy [57]. Fur-
thermore, zero-knowledge proofs can provide evidence for data
and identity authenticity and computational integrity without
violating privacy, and truth discovery can enhance these quali-
ties independent of privacy considerations. Moreover, privacy-
preserving data mining can enhance privacy; however, if the
right PETs are not employed, qualities such as computational
integrity may not be enhanced or assured. As last examples,
technologies such as diﬀerential privacy or k-anonymization en-
hance privacy by reducing data authenticity, and onion rout-
ing or ring signatures enhance privacy by forgoing or reduc-
ing identity authenticity, respectively. These latter technologies
consequently reduce data utility in exchange for privacy.

Lastly, we are mindful of the term tackling, which refers to
a technology that directly and fully or partially solves a cur-
rent challenge in the context of privacy enhancement, e.g., the
copy problem or the recursive enforcement problem (REP) (see
challenges in Section 8). We use the term circumvent when a
technology bypasses a problem, i.e., the technology does not di-
rectly address the issue. However, the entities that leverage the
circumventing technology are still not aﬀected by the problem.
For example, obscuring the data and computation in a third-
party server with homomorphic encryption (HE) does not tackle
the REP; instead, HE circumvents such problem because the
third-party server cannot see the contents. On the other hand,
distributed ledger technology tackles the REP with a redundant
and hence tamper-evident storage and execution.

The following sections 6 and 7 describe the technologies that
we identiﬁed in our SLR. We provide a new categorization of
these technologies based on the characteristics emphasized in
the corresponding selected publications and the technical prop-
erties described in this Section.

6. Privacy-enhancing technologies

6.1. Processing layer

The PETs we included in data processing aim to enhance the
privacy of either data inputs, outputs, the intermediate steps of a
computation, or a combination thereof while maintaining a high
degree of utility. This Section follows the structure of Fig. 3.

6.1.1. Secure and outsourced computation

Secure and outsourced computation comprises PETs that en-
hance privacy through conﬁdentiality. Furthermore, if the PET
also employs digital signatures and their cryptography primi-
tives, then the PET can also assure the integrity of the data and
computation and identity authenticity in the presence of a digi-
tal certiﬁcate.

Zero-knowledge proofs (ZKPs). With ZKPs, a technology
ﬁrstly conceived in the 1980s by S. Goldwasser et al. [21], a
veriﬁer can verify the authenticity of the data and the integrity
of a computation conducted by a prover without the need to ac-
If the
cess the data or replicate the computation itself [22].
statement that is proven is about claims attested in a digital
certiﬁcate signed by a trusted entity (e.g., age over 18), ZKPs
can verify identity authenticity while keeping the information
leaked about the identity minimal.

Speciﬁcally, ZKPs exhibit (i) zero-knowledgeness, i.e., the
veriﬁer learns nothing new from the prover beyond the correct-
ness of their statement, (ii) completeness, i.e., the prover can
convince the veriﬁer of a correct statement with high proba-
bility, and (iii) soundness, i.e., the prover cannot convince the
veriﬁer of a wrong statement with high probability [58][S43].
Furthermore, there are interactive and non-interactive ZKP pro-
tocols. With the latter, there is no need to engage in sequential
message exchange, and the prover can convince multiple par-
ties of a claim with a single, potentially short, message [58].
These characteristics make non-interactive ZKPs highly attrac-
tive for use in blockchains [57]. ZKPs are also the building
block of many anonymous credentials, which are also known
as privacy-preserving attributed-based credentials [59]. They
allow the veriﬁcation of information in a digital certiﬁcate with-
out disclosing any unnecessary data, including the highly cor-
relating value of the signature. Anonymous credentials were
initially proposed in 1985 by D. Chaum [60], and developed
further with ZKPs and blind signatures [61] chieﬂy by J. Ca-
menisch and A. Lysyanskaya [62] [63] and by S. A. Brands
[64]. Lately, anonymous credentials have seen renewed inter-
est also in the context of digital wallets for end users’ identity
management [65, 66].

Within our SLR in IoT data markets, V. Koutsos et al. [S43]
employ non-interactive ZKPs to verify the correct computa-
tion of outputs, which, in turn, unlocks the payment from a
smart contract in the Agora blockchain, eliminating a third-
party veriﬁer. While ZKPs have their limitations due to com-
putational complexity, typically for the prover, and there is still
a considerable gap between cryptographers and software engi-
neers [67], we expect to see more publications such as V. Kout-
sos et al.’s [S43]. This projection is justiﬁed by the signiﬁcant

8

improvements in ZKPs’ performance, and ease of use in re-
cent years [68][69] and the availability of an increasing vari-
ety of domain-speciﬁc programming languages to implement
ZKPs, such as bellman or circom in combination with snarkjs.
Recently, ﬁrst research has emerged that uses ZKPs to prove
that a machine learning model was trained correctly on speciﬁc
data [70], and there are many opportunities to leverage them
in data markets, such as demonstrating that the input data of a
computation was signed by a sensor that received a certiﬁcate
from a trusted third party without revealing the sensor’s iden-
tity or the data. In this case, the digital signature and certiﬁcate
can be regarded as AETs, while their veriﬁcation inside a ZKP
enhances privacy and, hence, qualiﬁes ZKPs as a PET.

Secure multiparty computation (MPC). In broad terms, MPC
enables multiple parties to exchange information obliviously
and jointly compute a function without revealing individ-
ual inputs to each other [20][71]. The MPC implementa-
tions that we observed in our SLR employ either secret shar-
ing [S5][S10][S34] or garbled circuits [S34]. In secret-sharing-
based MPC, each party ﬁrst obfuscates the input by splitting it
into shares. Secondly, this party distributes the shares among
the other computing parties. Afterward, each party executes
arithmetic operations independently on these shares, and ﬁ-
nally, all parties share the outputs to reconstruct the result [20].
In Shamir’s scheme [72], one can specify a minimum of
shares that the recipient needs to reconstruct the output, and
any combination of fewer shares does not reveal anything about
the secret to the receiving entity [S19] [73][74][S48]. On the
other hand, in additive secret sharing, all the shares are needed.
Outside MPC, Shamir’s scheme has been commonly used for
key management schemata for cryptographic systems so that if
some shares that represent a private key are lost, one can still
reconstruct the key with the remaining shares [72]. On the other
hand, MPC can also be implemented by garbled circuits [75],
for only two [76] or multiple [77] parties. Garbled circuits are
protocols that enable secure computation by using functions
translated into Boolean circuits, i.e., a sequence of basic logic
gates such as AND, XOR, and OR that may be combined to
construct any function [75][78][S48]. Garbled circuits make
use of oblivious transfer [79], which in turn utilizes asymmet-
ric encryption, and symmetric encryption for encrypting and
decrypting each gate’s truth table. Lastly, there are MPC hy-
brids that combine these approaches [80].

MPC allows computing functions without revealing the in-
puts to other participating parties. MPC protects inputs against
brute force attacks and it is to date considered less compu-
tationally expensive than alternatives such as fully homomor-
phic encryption [81]. Drawbacks of MPC include its high
processing and communication costs [59] and sensitivity to
network latency, which can considerably decrease the perfor-
mance [S5][S9][S48]. Additionally, MPC protocols often need
to be supplemented by mechanisms that prevent collusion [S5].
Moreover, since the individually provided inputs are only lo-
cally available, one cannot stop malicious entities from jeop-
ardizing the authenticity of the input with false inputs. MPC
can only prevent curious entities from learning information.

A countermeasure for this reduction in accountability is zero-
knowledge proofs to enforce the authenticity of participants’
local computations while maintaining them conﬁdential [82].

Three of the papers in our SLR implement MPC in their
architecture: [S5] uses additive secret sharing, [S10] employs
Shamir’s secret sharing, and [S34] leverages a combination of
garbled circuits an additive secret sharing. Additionally, other
publications acknowledge the importance of MPC schemata by
including them in their review [S19][S23][S48]. We provide
additional details in Table D.5. While several frameworks for
MPC are available, the MPC solutions employed by these three
publications were handcrafted. This may indicate that the in-
tegration of MPC into existing systems requires features that
are not yet available with generic tools, such as performance
aspects.

Homomorphic encryption (HE). HE allows performing oper-
ations on encrypted data (ciphertext) as if they were not en-
crypted. After the computation, the entities with the corre-
sponding secret key can decrypt the output [83]. There are
variations of HE depending on the diversity of operations it
can perform [16][17]: Fully homomorphic encryption (FHE)
schemata support addition and multiplication, while partially
homomorphic encryption (PHE) schemata allow for only one
of these alternatives – typically in exchange for drastically im-
proved performance. Any other schema in-between is called
somewhat homomorphic encryption [S48].

Five out of the six studies that use HE in our SLR use a PHE
variation [S41][S26][S7][S34][S46]. Each of the former four
speciﬁes the name of the employed schema, namely the Pail-
lier cryptosystem in the ﬁrst two [18], Boneh-Goh-Nissim [84],
and Hash-ElGamal [85]. The latter study only brieﬂy mentions
the additive homomorphic property of their handled data. On
the other hand, [S10] uses FHE with a schema called fully ho-
momorphic non-interactive veriﬁable secret sharing [86]. Sev-
eral other articles in our SLR underline the importance of
HE [S4][S19][S48]. On the other hand, V. Koutsos et al. [S43]
suggested the use of multi-client functional encryption [87][88]
instead of HE so that the scheme combines data from some in-
dividuals with others, and, in turn, malicious entities cannot
trace back the output of the computation to a single user, as
it may happen in HE. Furthermore, there is a related scheme
called functional encryption [S43] that allows to retrieve a pre-
speciﬁed function executed on a set of cyphertexts [87], e.g.,
decrypt only the mean of a set of encrypted numbers by deriv-
ing a function-speciﬁc decryption key from the secret key that
was used for encrypting the data. A summary of papers from
our SLR that mention or use HE is given in Table D.5.

The major limitation of FHE is its high computational
complexity and the comparatively large storage needs of its
cyphertext, which poses a signiﬁcant challenge for its use
and is aggravated in the context of IoT devices’ limita-
tions [S41][S7][S48][S24]. Therefore, the approach adopted
by most authors is the use of PHE instead of FHE [89], which,
while still not as eﬃcient as other PETs, consumes signiﬁcantly
more computing resources than PHE [90].

9

As observed for the case of MPC, while there exist generic
frameworks such as SEAL, HElib, or TFHE, the authors of the
publications in our SLR utilized handcrafted solutions, which
may indicate the lack of framework versatility or performance.
Overall, practitioners and companies may use HE to perform
lightweight functions on data privately on non-local resources,
e.g., computing in the cloud, which otherwise would be too ex-
pensive to maintain in-house. MPC would usually be preferred
over HE when the inputs to the function belong to multiple par-
ties. Nonetheless, some selected publications also employ HE
in these cases, e.g., when data brokers determine the winner of
an auction [S26][S34][S41].

Trusted execution environments (TEE). TEEs were ﬁrst de-
ﬁned in 2009 by the Open Mobile Terminal Platform as “hard-
ware and software components providing facilities necessary to
support applications” that are secure against attacks that aim
to retrieve cryptographic key material or other sensitive infor-
mation. These features include defense against more sophisti-
cated hardware attacks such as probing external memory [19]
or measuring execution times and energy consumption. More-
over, TEEs defend against adversaries who are legitimate own-
ers of the hardware or remote access to the operating system
that can run the code themselves. TEEs allow a user to deﬁne
secure areas of memory (“enclaves”) that enhance conﬁdential-
ity and assure data and computation integrity of the code and
data loaded in the TEE [S2], i.e., any other program outside
the enclave cannot act on the data. Speciﬁcally, TEEs associate
unique encryption keys to computer hardware, making software
tampering at least as hard as hardware tampering and certifying
the computation results within the TEE. The reason is that the
only way to hacking a TEE is physical access to the hardware
and, consequently, performing manipulations so that the hard-
ware provides false certiﬁcations to bypass remote attestation
and sealed storage [S45]. Seal-stored data may not be accessed
unless the user employs the correct hardware and software, and
remote attestation is a process whereby a trusted third-party as-
sures that the execution of a program in a speciﬁc piece of hard-
ware is correct [S45].

the

Four of

selected papers

in our SLR leverage
TEEs [S2][S12][S25][S45], and a review mentions their im-
portance [S19]. [S2], [S25] and [S45] proposed TEEs to con-
ﬁdentially train and evaluate machine learning models on data
available through a data market. While the role of TEEs in data
markets overlaps with the use of HE and MPC, authors have
preferred the latter technologies to enhance conﬁdentiality in
auctions and data processing, which may be due to the limited
memory TEEs oﬀered at the time. The reviewed four studies
used Intel’s Software Guard Extension (SGX) [91], where In-
tel is the trusted third party, and, therefore, the single point of
failure. However, practitioners should be mindful of the numer-
ous vulnerabilities present in TEEs [92–94], and Intel’s SGX
deprecation in 2022 [95], which aﬀects many of the designs
found in this SLR dated before July 2020. Therefore, we sug-
gest practitioners to explore Sanctum [96], Keystone [97] and
AWS Nitro [98]. Speciﬁcally, [S2], [S25], and [S45] used SGX
for conﬁdential computing, and L. Ruinian et al. [S12], em-

ployed SGX for their blockchain architecture to perform “Proof
of Useful Work”. In this type of consensus mechanism, nodes
perform useful computations instead of computing hashes like
in Bitcoin or Ethereum mining. Moreover, N. Hynes et al. [S2]
decided to use TEEs to enhance data and computation conﬁ-
dentiality for machine learning algorithms because of the low
performance of MPC and HE on machine learning [99].

On the other hand, we noted that TEEs designed for resource-
constrained devices – potentially at the cost of oﬀering less
functionality – were not prominently discussed in the selected
papers. This includes, for instance, ARM TrustZone [100],
which is relevant as many IoT devices run on ARM processors,
and trusted execution modules [101].

A popular

Privacy-preserving data mining (PPDM). J. Du et al. [S23]
describe PPDM as a means to enhance privacy while extract-
ing useful information from data mining. Data mining includes
ML and conventional statistical analyses such as aggregations
(e.g., mean or quantiles). PPDM is achieved by performing the
computation where the data reside, protecting the computation
with cryptographic or data perturbation means, or a combina-
tion thereof. As a comprehensive example, suppose the clients’
local data and computation are cryptographically protected and
the clients have the capability to perturb data. In that case, the
computation can run anywhere, which is accomplished by de-
ploying a ML model and input data in a trusted execution en-
vironment (TEE) or implementing a ML model using MPC or
HE. With input or computation perturbation, the clients also en-
hance the privacy of the outputs.
tool

for PPDM is federated learning (FL)
[102][103][104], as it avoids collecting users’ data. Speciﬁ-
cally, FL collaboratively trains a seed ML model across mul-
tiple clients’ local data, after which a server aggregates the
resulting weights to form a unique model (process repeated
across rounds). Researchers have increased the privacy of FL
by sharing weights with secure aggregation protocols [105]
(MPC, Shamir’s secret sharing), and protected the privacy in
client selection [106] and update parameter sharing [107] with
additive HE (i.e., partial HE). Alternatively, split learning ap-
proaches [108][109] decompose neural networks’ layers into
elements and, thus, the input data and labels do not need to be
within the same machine. Split learning presents advantages
over FL when the local hardware for computations belongs
to diﬀerent network speeds or hardware conﬁgurations [110].
Additionally, gossip learning [111][112] proposes a framework
whereby multiple models perform a random walk over clients,
where they are trained and merged with other models they en-
counter.

Another way to achieve PPDM is by perturbing input data or
weights of the ML model with anonymization techniques such
as diﬀerential privacy (DP), resulting in privacy-enhancing op-
timization schemata like DP-stochastic-gradient-descent (DP-
SGD) [113]. DP-SGD perturbs the weights’ updates with noise
and, therefore, one may not reconstruct the inputs based on the
outputs, which may happen in ML or stand-alone FL [99]. Prac-
titioners can plug in weight DP perturbation in central ML, FL,
gossip learning, or split learning, in combination with MPC as

10

well. We depict such leverage of anonymization technologies
for PPDM with the dashed line connecting both elements in
Fig. 3.

With PPDM, individuals may enjoy a higher degree of pri-
vacy than outsourcing the computation transparently to a trusted
third party. Data markets can oﬀer an infrastructure leveraged
by PPDM, where data prosumers and consumers only need to
provide the input data and ML models, much like the studies
in our SLR propose [S2][S25][S45] using TEEs to train models
with DP. Like data, trained ML models could also be exchanged
in markets.

6.1.2. Anonymization

While the previously presented PETs hide sensitive data from
unsolicited parties and, thus, provide conﬁdentiality while en-
hancing or assuring data and computation integrity, the autho-
rized receiver of the plaintext may reverse engineer the output
and correlate data records with individuals (re-identiﬁcation at-
tack). Consequently, employing only secure and outsourced
computation PETs is insuﬃcient to provide the required de-
gree of privacy in cases where the recipient may not be fully
trusted. Anonymization technologies can help in these situ-
ations by protecting non-explicit identiﬁers and sensitive at-
tributes [S47][S44]. The cost of this protection is forgoing data
authenticity and thus decreasing utility. Given the frequency
of re-identiﬁcation attacks, anonymization should be a criti-
cal element of any survey or modern online application, and,
in particular, IoT data markets [S8]. One may observe that
anonymization technologies rely on statistics, probability the-
ory, and heuristics, while secure and outsourced computation
usually employs cryptography and trusted hardware.

This sub-section describes our ﬁndings for the most em-
ployed anonymization technologies identiﬁed in our SLR. We
categorize most of them into two groups [114]. Syntactic tech-
nologies provide a numerical value to the degree of individuals’
protection in a dataset, resulting in a perceptible perturbation of
data, e.g., generalizing the values 42, 44, and 45 to the inter-
val r40, 45s such that it is harder for an attacker to distinguish
between the three individuals. Semantic technologies enforce
a privacy deﬁnition to a learning mechanism executed over a
dataset, namely diﬀerential privacy, whereby the output distri-
bution of the mechanism should be insensitive to the removal or
addition of an individual in the dataset. Typically, the property
is fulﬁlled by adding calibrated noise to the output of a mecha-
nism, yielding a result that is not syntactically diﬀerent from the
original value, e.g., 42 could become 45 after noise addition.

Semantic technologies have an advantage over syntactic
technologies, as they provide a mathematical guarantee of pri-
vacy agnostic to background information, i.e., an attacker can-
not use related information to re-identify an individual in the
dataset. Additionally, we discuss other anonymization tech-
nologies not covered in these two groups, namely noise pertur-
bation and pseudonym creation. Perturbation, in this context, is
not classiﬁed as semantic because its process does not necessar-
ily provide a formal semantic privacy guarantee (such as in dif-
ferential privacy) and, simultaneously, the outputs are not syn-

tactically modiﬁed. In essence, anonymization techniques ob-
fuscate information by perturbing the data during measurement
or processing [59]. From this perspective, anonymization can
be understood as a kind of statistical disclosure control [115],
and, thus, also encompasses semantic techniques such as diﬀer-
ential privacy.

Syntactic technologies. We identify the implementation of the
privacy deﬁnitions of k-anonymity and its variations l-diversity
and t-closeness, a newly proposed model called β-likeness,
and also their building-blocks: generalization, and suppression.
The most frequently utilized model for syntactic anonymization
in our SLR is k-anonymity [S3][S28][S33][S36][S47], which
was also reviewed or highlighted by [S8][S27][S49][S35]. K-
anonymity is a privacy model that guarantees any individ-
ual in a dataset to be indistinguishable from at least k ´ 1
others. K-anonymization, i.e., altering a dataset to fulﬁll k-
anonymity, clusters a set of sensitive attribute values into equiv-
alence classes of size k. However, ﬁnding an optimal value
of k for minimum information loss is NP-hard. Thus, re-
searchers have proposed alternative heuristics [116]. Nonethe-
less, some of the selected studies used the building blocks of
k-anonymization (transformations): generalization [S21][S36]
and suppression [S36].
Suppression deletes selected data
points, while generalization substitutes data points for others
that belong to a higher level in a manually pre-deﬁned hierar-
chy, e.g., substituting a city by a country to make the location
less detailed.

The selected studies [S28][S33] applied k-anonymization to
aggregate data from a set of entities. M. A. Alsheikh et al. [S47]
innovated upon [S28][S33] by also employing the l-diversity
model to ensure at least l diﬀerent values in sensitive attributes,
and t-closeness so that the distribution of the sensitive attributes
within each equivalence class was at most at a distance t from
the overall dataset distribution of that attribute. These two mod-
els have their own limitations, they prevent homogeneity and
external knowledge attacks (l-diversity) and skewness and sim-
ilarity attacks (t-closeness) [114], to which k-anonymity is vul-
nerable. Furthermore, D. S´anchez et al. [S36] tailored the use
of k-anonymity based on record history, privacy policies, and
disclosure context. Their new approach prevented a signiﬁcant
decrease in the data utility compared to homogeneously apply-
ing k-anonymity to all individuals’ records equally.

Nonetheless, there are detractors of syntactic technologies in
data markets because of the need for a centralized intermediary
that sees and aggregates the data in a, e.g., k-anonymous fash-
ion [S18]. Moreover, J. Cao et al. [S44] stated that these con-
ventional syntactic approaches are not suﬃcient because they
lack an attacker perspective in the model. For this reason, they
designed a novel model called β-likeness that explicitly bounds
the additional knowledge that an adversary gains from seeing
the released data.

In the context of this SLR, k-anonymization is mainly em-
ployed before sharing data in an IoT data market. However,
researchers also employ k-anonymity for privacy-enhancing
location-based services that exchange location data in IoT data
markets, whereby similar fake locations hide the real ones.

11

This type of approach ﬁts well with IoT devices embedded
in phones, vehicles, and laptops, among other mobile things.
Some of the approaches named by [S3] were cloaking, which
consists of sending a more extensive region that encompasses
the real one, and geomasking, whereby the real location is ran-
domly displaced outside of an inner circle but within an outer
one. D. Lopez et al. [S3] adopted geomasking for situations
where low accuracy is suﬃcient, and a high degree of pri-
vacy is required. A modern alternative to releasing anonymized
data is synthetic data generation, which creates data by ran-
domly sampling from a distribution representative of the real
data. Practitioners can employ generative adversarial networks
(GANs) [117], or GANs with diﬀerential privacy for a higher
protection [118] to synthesize data. Synthetic data could be
helpful in some contexts as they “look” similar to the real data
(unlike fulﬁlling k-anonymity), e.g., developing applications
before testing them with the real data.

Altogether, anonymization technologies and PPDM com-
pose the building blocks of statistical disclosure control [119],
which organizations may leverage internally in their privacy-
preserving data management and analysis solutions, or exter-
nally, by using privacy-enhancing publishing solutions [120]
in, e.g., data markets. Among past surveys focused on the lat-
ter solutions (namely k-anonymity and related models, in addi-
tion to a few cryptographic primitives), the reader may refer
to [120][121][122][123][124] for further specialized reading.
Most notably, C. C. Aggarwal and P. S. Yu [122] provide a com-
prehensive survey of syntactic models and diﬀerential privacy
and their attacks, and M. Cunha et al. [124] compile a helpful
mapping of data types to the appropriate syntactic and semantic
techniques for anonymization.

Semantic technologies. Introduced by C. Dwork et al. [13]
in 2006, diﬀerential privacy (DP) proposes a formal guaran-
tee of privacy that has become the golden standard for re-
searchers [S37]. DP appears in its pure form or one of its ﬂavors
in 13 of the 35 studies that propose a solution in our SLR. Fur-
thermore, another seven studies refer to DP to underline its im-
portance or drawbacks. The potential reasons behind the high
number of references and use of DP are multifaceted. While
HE or MPC may protect the inputs’ and computations’ conﬁ-
dentiality, they do not prevent reverse-engineering the outputs
(re-identiﬁcation attacks). Moreover, syntactic technologies or
other conventional anonymization technologies, e.g., additive
noise, lack a mathematical guarantee of privacy and are subject
to background knowledge attacks. DP, however, tackles these
issues.

In broad terms, DP guarantees that the output distribution of
an analysis (a statistical query or a ML model) over a dataset is
“essentially” identical, irrespective of the presence or absence
of an individual in the dataset. Additionally, DP is agnostic to
auxiliary information available in the present or the future. DP
is typically achieved by adding random noise sampled from a
probability distribution such as the Laplacian or the Gaussian.
Speciﬁcally, the noise limits the output distribution diﬀerence of
an analysis executed over two datasets (one with and one with-
out an individual) to be no greater than an upper bound, mak-

ing the outputs “diﬀerentially” indistinguishable. Overall, DP
bounds the amount of new information gained by an attacker
after observing the output of an analysis.

The set of

selected studies of our SLR that used
[S2][S3][S9][S15]
DP in their proposed solutions
are
[S16][S18][S22][S24][S25][S28] [S37][S42][S50]. Tables D.6
and D.7 summarize their proposed architectures. Six of these
studies employ DP locally [S3][S9][S15][S16][S37][S42], i.e.,
the noise is added to the data of an individual on the client-side.
In contrast, the rest of the studies apply DP centrally, i.e.,
on aggregated data on the server-side. Furthermore, we can
cluster the studies into those that focus on a data trading
design for data markets [S22][S37][S15][S50], crowdsens-
ing data markets [S42][S9][S16][S24], and architectures
that host a data market in an attempt to achieve end-to-end
privacy [S2][S3][S18][S25][S28].

While DP oﬀers a mathematical privacy guarantee, DP is
not a panacea. DP still holds ﬂaws in its real-world imple-
mentations [125] that the research community and practitioners
should address. Moreover, DP’s combination with ML needs
further improvements regarding balancing privacy and accu-
racy [115]. In our SLR, S. Sharma et al. [S48] and D. S´anchez
and A. Viejo [S36] identify two speciﬁc problems with DP:
ﬁrstly, DP cannot be used when a high level of accuracy is re-
quired [S48], e.g., analyzing data from the brakes of vehicles to
improve safety. Secondly, releasing an entire dataset with cur-
rent DP approaches is troublesome. Despite these challenges,
the authors of [S2][S50] argue that the beneﬁts of DP predom-
inate, as DP can adapt to many use-cases and allows a practi-
tioner to ﬁne-tune the added noise to enhance privacy.

As we already noted with ZKP, MPC and HE, the authors of
the selected publications that used DP did not employ open-
source DP libraries such as OpenDP, Google-DP, diﬀprivlib,
TensorFlow privacy, or Chorus. Instead, they used handcrafted
implementations of DP. Aside from syntactic and semantic
technologies, other anonymization technologies are simpler
to implement, e.g., sampled data release, character masking,
truncation, rounding, top and bottom coding, data swapping,
randomization, creating pseudonyms, character scrambling,
microaggregation, or noise perturbation [23][126]. More-
over, N. Li et al. [127] designed an algorithm that combines
the syntactic deﬁnition of k-anonymity with DP. The two
other anonymization technologies employed by another three
studies were noise perturbation [S36][S47] and pseudonym
creation [S7].

Perturbative anonymization. Perturbation relies on the use
of noise to obfuscate sensitive information. One of the
simplest forms of perturbation is additive noise, employed
in [S36] and [S47]. Additive noise consists of adding to a de-
terministic value a random value sampled from a uniform dis-
tribution whose bounds are set by a speciﬁc percentage of the
deterministic value. Furthermore, [S48] reviews two novel per-
turbative technologies: First, random space perturbation [128],
which strives to protect the privacy of cloud-stored data by uti-
lizing a conﬂuence of order-preserving encryption, dimension-

12

data [S1][S2][S3][S10][S17][S25][S33][S41][S42][S46] (most
of these publications also employed asymmetric encryption
for digital signatures, hence the high frequency of dig-
ital signatures in Fig. G.14), other publications such as
[S1][S25][S32][S33][S41][S41][S45] employed symmetric en-
cryption to also conﬁdentially store data. Naturally, encryption
is also a building block for digital signatures (veriﬁcation layer)
and for the storage layer to maintain data at rest conﬁdential.
We depict this relationship with the dashed lines connecting
these elements in Fig. 5.

Onion routing. Onion routing, the backbone of the P2P net-
work resulting from the Tor project [130], consists of a series of
re-transmission steps through the network’s nodes. A sender’s
message is encrypted once for each step. The intermediaries
decrypt only their appointed encryption layer. Thus, the node
only knows the immediate sender and receiver but not the ori-
gin of the chain of messages. As the messages are encrypted,
the nodes cannot see the contents either. Overall, onion rout-
ing renders one’s messages unreadable and untraceable. The
paper that suggests employing onion routing in a data market
context is [S26], which some of the identiﬁed reviews equally
appreciate [S4][S27][S35].

However, some drawbacks exist.

Implementations backed
by Tor have high-latency and redundant communication that
challenges bandwidth, which can be hard to align with high
transactional environments, such as IoT data markets. More-
over, if an architecture decides to use Tor, the network is often
blocked by IT departments within organizations or even subject
to state-level censorship by some governments [130]. There-
fore, practitioners can use alternative technologies such as a
VPN to enhance entities’ privacy in a network in these contexts.
However, these typically centralized alternatives generally oﬀer
lower anonymity guarantees, e.g., a VPN provider can identify
a user [59].

Because there is no central authority to set privacy policies
unilaterally, one must remember that onion routing enhances
privacy by preventing malicious entities from collecting IP ad-
dresses to identify users. Onion routing will not help if the
data that users submit to the network is intrinsically sensitive or
correlating. To tackle these limitations, practitioners may use
onion routing as a building block of a more extensive privacy-
enhancing system that leverages other PETs [S26].

Notably, the publications surveyed in our SLR did not in-
clude many other untraceability protocols, which would include
mixnet-based alternatives to onion routing (e.g., anonymous re-
mailers, Chaum’s mixes [131]), DC-nets [132], or peer-to-peer
anonymous communication systems. An extensive overview of
these systems is given by Ren and Wu [133].

6.3. Storage layer

The authors of the selected papers that propose conﬁdential
storage functionality in their architecture leverage symmetric
encryption, mostly AES [S1][S25][S32][S33][S41][S41][S45]
(encryption is described in the communication layer). Further-
more, researchers could leverage InterPlanetary File Systems

Figure 4: Classiﬁcation of PETs employed for communication.

ality expansion, random noise injection, and projection. Sec-
ond, geometric perturbation [129], which is motivated by the
idea of protecting the geometric transformations that a machine
learning model may perform on a dataset rather than the data
itself. While perturbative technologies aim to tackle the same
problems, unlike DP, they do not provide mathematical guar-
antees of privacy, even though some are also based on noise
addition.

Pseudonym creation. Pseudonym creation is applied to di-
rect identiﬁers, e.g., names or social security numbers, to en-
hance privacy while uniquely identifying each record. Prac-
titioners create pseudonyms by hashing or deterministically
encrypting an identiﬁer, e.g., using order-preserving encryp-
tion [S48], or by applying asymmetric key encryption like
ElGamal [S7]. However, researchers have demonstrated that
pseudo-anonymization falters against some attacks like proﬁl-
ing, task tracing, or re-identiﬁcation [S35].

6.2. Communication layer

The PETs included in this Section enhance the conﬁdentiality
of data in transit or of the sender’s identity (see Fig. 4). These
PETs rely on cryptography.

Encryption. Encryption is one of the most fundamental tech-
nologies to enhance conﬁdentiality [S19] because after encrypt-
ing a piece of data (cipher), only the anointed holders of a
decryption key can decipher such data. We underline that en-
cryption cannot guarantee privacy because nothing stops an in-
tended receiver from publicly sharing the decrypted message;
this also emphasizes employing anonymization PETs. Encryp-
tion may be symmetric (one key to both encrypt and decrypt
data) or asymmetric, known as public-key cryptography (two
keys, a public key to encrypt and a private key to decrypt, or
vice versa). Encryption is the building block of virtually every
secure communication established through a network and takes
a key role in digital signatures.

While some publications from our SLR employed asym-
the conﬁdential communication of

metric encryption for

13

 Privacy-enhancing  technologies Verification layer Communication layer Encryption Symmetric  encryption  [S1][S25][S32] [S33][S41][S42] [S45] Asymmetric encryption  [S1][S2][S3][S10] [S17][S25][S33] [S41][S42][S46] Onion Routing  [S4*][S26*][S27*][S35*] Processing layer Sovereignty layer Storage layerMoreover, although not all of the selected studies explicitly
mention DSs, we can safely assume that since DSs are already
a living part of virtually any enterprise IT system, most selected
studies employ them in their architectures (hence the high fre-
quency of DS utilization in Fig. G.14). Nonetheless, while DSs
allow verifying the integrity of data or the authentic identity of
the sender, users still need to trust the sender with the authen-
ticity of the data.

So far, we have only described DS as an authenticity-
enhancing technology. However, some of the studies selected
in this SLR employed two DS schemata based on asymmetric
encryption primitives that make DSs privacy-enhancing:

• Ring signatures [S32], whereby any party within a pre-
deﬁned set of parties could have been the signer of a mes-
sage. Thus, the identity of the authentic signer is kept hid-
den [136][137].

• Blind signatures [S20][S26], whereby the signer does not
have access to the content being signed [61].
It is pos-
sible to use blind signatures in combination with zero-
knowledge proofs to convince the signer that the content
to be signed has the expected properties. Also, one can
make an entity sign multiple contents and allow for spot
checks to detect fraud. The latter procedure has been em-
ployed in the ﬁrst approaches toward privacy-enhancing
payments [60].

Hashing. Hashing is a tool to deterministically map data of an
arbitrary length to a ﬁxed output length. In the context of pri-
vacy and veriﬁcation, and aligned explicitly with some of the
selected studies [S1][S10][S33][S46], hashing is used to verify
the integrity of transferred data by hashing the data and mak-
ing the hash public before transferring the data. In this manner,
the recipient can verify the integrity of the conﬁdentially trans-
ferred data by comparing the hash of the received data with the
previously published hash. Provided the entropy of the data is
suﬃciently high, nobody except the intended recipient can de-
termine the data from the published hashed value. Hence, hash-
ing can be considered a form of version control with a privacy
component.

The hash function employed by the publications mentioned
above was SHA-256. Their authors commonly use the pub-
lished hashed data on distributed ledger technologies to ensure
immutability and availability. In this setting, hashing enhances
the conﬁdentiality of the sender’s data while the parties (net-
work nodes) ensuring the ledger’s integrity (and inherently the
persisted hash) cannot unveil the original data. The original
data is only viewed by the intended receiver, which validates
the integrity of the data received through another channel with
the hash persisted in the ledger.

6.5. Sovereignty layer

The sovereignty layer deals with the concept of information
control, the perceived ability to govern what is exposed from
one’s data [55]. Speciﬁcally, based on an entity’s requirements,
this layer deﬁnes the entity’s rules and guidelines regarding

Figure 5: Classiﬁcation of PETs employed for veriﬁcation.

(IPFS) [134] to compensate for the lack of storage capacity in
blockchains to some extent. Speciﬁcally, IPFS is a peer-to-peer
protocol for data storage and access in a distributed ﬁle sys-
tem. Among the PETs in the processing layer, practitioners
could employ homomorphic encryption [135] to encrypt and
store certain types of data, so that data are readily available to
compute conﬁdential operations. Furthermore, unless strictly
necessary, practitioners should store encrypted data that are, in
turn, anonymized with syntactic or semantic technologies. In
case of a breach that leaks the decryption key, anonymized data
would reduce the likelihood of attackers re-identifying individ-
uals.

6.4. Veriﬁcation layer

Some of the PETs that support data processing cannot verify
the authenticity of data, identities, or the integrity of data [S19]
by themselves. The PETs we include in this Section accomplish
these veriﬁcations with diﬀerent levels of privacy enhancement.
The data processing PETs that can assure identity authenticity
and data integrity use the digital signatures of the veriﬁcation
layer and the encryption technologies of the communication
layer as building blocks. Furthermore, the credibility associ-
ated with verifying the information exchanged, analysis out-
puts, and identities can increase the willingness of users to share
data [S8]. To navigate this Section, we refer to Fig. 5.

Privacy-enhancing digital signatures (DSs). DS schemata as-
sure data integrity and identity authenticity if accompanied by
a digital certiﬁcate. As a consequence, DSs also provide non-
repudiation [S1], i.e., actions that an entity cannot deny later.
The steps that usually constitute a DS scheme are private and
public key generation, encrypting a digest of data with a pri-
vate key, and a signature veriﬁer that employs the public key to
check whether the sender signed the data with the private key.
DSs and the encryption primitives of the communication
layer are so fundamental that one of the selected studies solely
relies on HTTPS for their data market architecture [S17]. How-
ever, this architecture does not consider privacy beyond data in
transit. Hence, most selected studies rely on multiple PETs.

14

 Privacy-enhancing  technologies Verification layer Hashing SHA-256  [S1][S10][S33][S46] Digital signatures [S19*] Ring signatures  [S32] Blind signatures  [S20][S26] Storage layer Communication layer Encryption Processing layer Sovereignty layerother hand, S. Duri et al. [S29] presented an approach where
the data owner could choose among a set of four privacy poli-
cies, which included how data was aggregated, and S. Kiy-
omoto et al. [S33] relied on a privacy policy manager that acted
as a gatekeeper and managed the privacy settings from a set of
users. Furthermore, another set employs a pre-deﬁned logic to
execute PETs based on the desires and track record of the data
shared by the individual [S21][S36], while the last set relied
on smart contracts for decentralized pre-deﬁned [S2][S25] or
negotiated [S28] policies.

Firstly,

Secondly,

Nonetheless,

the implementation of policies faces chal-
lenges.
there may be multiple colliding policies,
i.e., applications must prioritize policies depending on the
context [S21].
there is no uniformly accepted
global standard for electronic privacy policies [S6]. C. Per-
era et al. [S40] investigate how practitioners model privacy poli-
cies in diﬀerent domains, focusing on IoT applications. They
point to the lack of a uniform standard and propose to utilize
ontology-based privacy-knowledge modeling. Thirdly, policy
enforcement also causes overhead and an increase in latency
due to the need for compliance checks and a lack of automa-
tion [S21]. Furthermore, conventional users should include
their privacy preferences with minimal manual eﬀort, as they
could be overwhelmed otherwise. C. Perera et al. [S40] sug-
gested using recommender systems based on similar users’ data
to address this issue. However, this solution may incur a bi-
ased recommendation. Moreover, data acquisition expenditure
for privacy policies should not incur costly computational re-
sources as they scale to a growing number of transactions [S40].

Privacy policies are crucial to protect users’ privacy; how-
ever, they are not enough. Organizations must consider privacy
issues at each stage of the data pipeline (i.e., processing data
end-to-end with the extract-transform-load framework [139]),
contemplating aspects that escape user-deﬁned or mutually-
agreed policies, and taking into account that typically, neither
users nor data brokers will be privacy experts. If a user does not
know the potential harms of sharing sensitive information such
as DNA data, a data consumer may take advantage of the user.
Therefore, while privacy policies are a stepping stone toward
end-to-end privacy, practitioners must develop systems with a
privacy-by-design philosophy [S36][S4].

Privacy by design is a term coined in the ’90s by the for-
mer information and privacy commissioner for the Canadian
province of Ontario, A. Cavoukian, who created seven prin-
ciples [140]. Privacy by design claims that privacy goes be-
yond current regulations and must be an ever-present concern in
the minds of organizations [140]. Following privacy-by-design
principles entails, for example, preventing sensitive information
extraction by default [S36], minimizing the amount of shared
data at each exchange (proportionality) [141], and increasing
the price of large data packages [141], among others. However,
adopting these design principles comes with eﬀort, forcing de-
velopers to adapt their design patterns. For example, current
homomorphic encryption techniques force data scientists to ex-
press their analysis in terms of additions and multiplications,
and diﬀerential privacy requires new software engineering de-

15

Figure 6: Classiﬁcation of PETs used for sovereignty purposes.

ownership and management that can indirectly govern data pro-
cessing, veriﬁcation, and other IoT data market layers. Fur-
thermore, to prevent entities’ violation of privacy, practitioners
should map these rules to the PETs capable of fulﬁlling them.
For example, GAIA-X’s high-level architecture contemplates
privacy policies in their data sovereignty layer [5]. Privacy poli-
cies (closely related to access controls), have been predominant
across publications, and, thus, dominate the sovereignty layer
depicted in Fig. 6, which illustrates the three identiﬁed types.

Privacy policies and privacy by design. Privacy policies em-
body the requirements and guidelines of a data governance
model and are meant to be part of any privacy-enhancing ap-
plication. To deﬁne them, given the regulatory and human as-
pects of privacy policies, it is also helpful to adopt perspectives
from deﬁnitions beyond computer science, such as the one un-
derlined in Section 2. A. F. Westin [138] indicates that the
privacy requirements depend on the recipient of the informa-
tion, e.g., an individual can have diﬀerent reservations when
disclosing information to a family member than to the gov-
ernment. Privacy policies should reﬂect this deﬁnition, which
means that individuals should express the privacy policies they
expect. Several studies in our SLR explicitly proposed policies
as part of their solution [S6][S21][S25][S28][S29][S33][S36],
while many others reviewed privacy policies [S19][S35][S40]
or mentioned similar ideas. For example, E. M. Schomakers et
al. [S8] did not provide a concrete implementation or explic-
itly named privacy policies. However, they mentioned that, in
data sharing scenarios, the data owners should be able to control
some fundamental aspects: data types to share, with whom to
share, the required degree of trust in another party, the purpose
of sharing, and for which beneﬁt.

Four of

Among the publications discussing privacy policies, there
these publica-
is a discernible classiﬁcation.
tions [S6][S28][S29][S33] considered privacy policies as a ne-
gotiation between the user and a third party, such as the data
consumer or data broker. S. Spiekermann et al. [S6] provided
a set of legal requirements and high-level technical solutions
that facilitated the introduction of policies in international data
markets, e.g., writing policies in a standard language. On the

 Privacy-enhancing  technologies Sovereignty layer Privacy policies (Access control)  [S19*][S35*][S40*] Decentralized Smart contracts (Distributed ledger technology) Ekiden  [S2][S25] Ethereum [S28] Negotiated  [S6*][S28][S29][S33] Pre-defined logic  [S21][S36] ...sign patterns that track the privacy budget of individuals or data
scientists.

Smart contracts (SCs). A SC alone is mainly equivalent to
conventional scripts. Nevertheless, because SCs are executed in
distributed-ledger-technology-based architectures (DLT) (see
Section 7.1, SCs inherit from DLT their enhanced availability
and integrity guarantees [56]. DLTs execute SCs synchronously
on every node of a P2P network if an arbitrary transaction
demands a function’s execution. Once deployed, no one can
change the script, not even the creators (unless there is an in-
tended call of the script that enables modiﬁcation), and the
script will remain in the network as long as the network exists
unless speciﬁed diﬀerently (e.g., through a self-destruct call).
This inherited integrity property of SCs makes them a unique
tool to specify and enforce policies between parties or any other
process where no trusted third party is available.

Within this review, all the studies that used the Ethereum,
Quorum, Hyperledger Iroha, or Ekiden blockchains relied
on SCs to declare privacy policies [S2][S25] (Ekiden) [S28]
(Ethereum), fair auctions [S3] (Hyperledger Iroha), or pay-
ments or incentives [S32] (Ethereum) [S43] (Agora) [S3] (Hy-
perledger Iroha). However, while SCs ease veriﬁcation and
enable democratic proposals of privacy policies, SCs also
inherit the privacy ﬂaws of DLT, i.e., SCs by default im-
ply the disclosure of data and computations to all DLT net-
work nodes [57][142]. For example, the architecture from
R. Cheng et al. [S25] employed SCs to set user-deﬁned poli-
cies, yet it relied on trusted execution environments to enforce
them. SCs alone cannot enforce privacy policies without rely-
ing on other PETs. The only privacy-related feature that a SC
can oﬀer to an IoT data market is declaring privacy policies.

Data access control. Data access control refers to allowing an
organization or an individual to choose who has access to which
data. Access control represents a subset of privacy policies in
data markets and may utilize diﬀerent PETs to enforce access
rights. While access control is a long-established approach, R.
Cheng et al. [S25] propose a novel method, using a key-rotation
system [143] in combination with a key manager. Thus, the po-
tential impact of a leaked key is only temporal, with the down-
side of shorter access permissions.

7. Authenticity-enhancing technologies

The included authenticity-enhancing technologies (AET) focus
on enhancing the authenticity of data and identities and also
cover data integrity as described in Section 5. Some of the
AETs that we describe incorporate privacy-enhancing features,
while others do not address or even aggravate privacy protec-
tion issues and, thus, need to be combined with PETs.

7.1. Consensus layer

Distributed ledger technology (DLT). While DLT may take
diﬀerent forms, most architectures follow the blockchain design
pattern, except for IOTA, which uses the so-called Tangle [144].

A blockchain is a tamper-proof distributed database whose state
is stored, synchronized, and replicated by nodes in a P2P net-
work following a consensus algorithm [56]. By its distributed
nature, the shared ledger becomes a medium to verify claims,
data, payments, or contracts, as once an entity writes something
on the ledger, it is practically impossible to modify or erase this
record in the future. This property makes blockchain a decen-
tralized and highly reliable alternative to conventional auditing
methods like version control [S19].

Beneﬁts of DLT in IoT data markets are the ability to rep-
resent the governance, distribution, and roles of authorities on
a technical basis [S3], and the enforcement or transparent stor-
age of pre-deﬁned rules by the architects of the respective plat-
form [S25]. Other beneﬁts include eliminating the need for a
trusted third party, which removes a single point of failure, im-
proves censorship resistance, and provides more robust data and
computational integrity guarantees. DLTs also enable payments
through their often built-in cryptocurrencies or other payment
systems implemented via smart contracts [S32][S46].

However, some of the studies in our SLR also point at the
challenges of current DLT designs: IOTA fails to deliver re-
garding throughput [S28], is still centralized [S20], and prov-
ably has security ﬂaws [145]. Furthermore, blockchains exhibit
low transaction throughput [S25], high latency [S11], limited
storage [S1] and scalability [S11][S25], computational over-
head [S25], high energy consumption [S12], and, most impor-
tantly, excessive information exposure that can entail a privacy
violation [146]. However, some of these aspects can be miti-
gated. For example, the energy consumption issue only con-
cerns proof-of-work blockchains [147], and performance can be
improved to some extent by private permissioned blockchains
that restrict participation in consensus and read access to a small
number of nodes in a consortium [148].

Despite the possible operational improvements, employing a
DLT for a privacy-enhancing IoT data market needs in-depth
consideration. Firstly, through highly replicated storage, a DLT
is not suitable for storing large amounts of data produced by
IoT devices, not even in a privacy-compliant manner. Con-
sequently, most architectures of the selected studies transfer
data through interplanetary ﬁle systems [S28], employ a hash-
ing veriﬁcation approach as described in the communication
layer [S1][S10][S33] or use Merkle trees [S46]. Secondly,
while DLT allows for disintermediation and veriﬁcation in a
trust-less manner, it exposes to the network whatever infor-
mation someone writes on the ledger for as long as the net-
work exists, which may, among others, violate GDPR’s Arti-
cle 17 ”Right to be forgotten” for personally identiﬁable in-
formation [149]. Lastly, even if an organization uses a DLT
only for the matching and clearing steps of an auction, poten-
tially sensitive business information such as turnover can be-
come available to other network participants, which can conﬂict
with antitrust regulation.

Despite the privacy and performance issues of DLT, 31% of
the included papers implemented a DLT as the backbone of IoT
data market architectures, employing the Ethereum blockchain
[S1][S13][S20][S28][S32][S45][S46], Quorum [S18],
the
Agora blockchain [S43], Hyperledger Iroha [S3], Hyperledger

16

Fabric [S10][S33], IOTA [S20][S28][S30], Intel’s TEE-based
consensus Rem [S12], and Ekiden [S2][S25]. Other publica-
tions only considered them agnostically [S11][S49] or in a re-
view [S19][S23]. The most salient architectures are described
in Table D.7.

Some of the selected studies included privacy-enhancing fea-
tures in their stack. For example, Quorum supports private
transactions and private contracts through a public-private state
separation and P2P encrypted message exchange for the direct
transfer of private data [S18]. However, the interaction between
the private and public ledgers is thus naturally limited and can-
not be directly applied, for example, to an on-chain payment
system. Another example is Ekiden, which oﬀers a horizon-
tally scalable blockchain potentially capable of hosting end-to-
end privacy-enhancing applications through key management
protocols and Intel’s TEEs [S25] (Note that [S12] uses these
TEEs only for consensus, not privacy). Like the solution that
W. Dai et al. [S45] presented, Ekiden allows for smart contracts
to execute data analysis in TEEs. However, it is essential to
note that these DLTs accomplish the described privacy and in-
tegrity functionalities not because of the DLT characteristics but
by leveraging the PETs described throughout Section 6.

7.2. Veriﬁcation layer

This veriﬁcation layer corresponds to AETs that can be em-
ployed for the veriﬁcation of data and identities. We structure
this Section according to Fig. 7.

Truth discovery (TD). TD encompasses algorithms aiming to
ﬁnd the authentic value when diﬀerent data sources provide
conﬂicting information. As a consequence, TD enhances data
and computation integrity and can also enhance identity au-
thenticity, e.g., through reputation systems [S9]. In our SLR,
we found that TD takes diﬀerent forms. For example, the sur-
vey by J. Du et al. [S23] mentioned a mechanism called peer-
prediction-based trustable data aggregation [150], in which the
system administrator rewards participants for predicting out-
comes of arbitrary events based on other participants’ data.
This design created incentives for honest reports and there-
fore enhanced data correctness, resulting in almost all partic-
ipants choosing to report their bids truthfully [S23]. Moreover,
J. Du et al. [S23] also proposed mutual validation in which an
IoT device compares its data with that of other nearby IoT de-
vices. However, this only applies to speciﬁc measurements that
are positively correlated for neighboring devices, e.g., temper-
ature, speed of a vehicle, or location in particular settings. It
also seems challenging to establish generic handling of diﬀer-
ences. Other TD approaches are majority voting, implemented
by Y. Li et al. [S9] in their crowdsourcing architecture and by
W. Dai et al. [S45] in their data processing-as-a-service model.
Speciﬁcally, given the use of diﬀerential privacy in the former
approach, they systematically discovered high-quality data with
an estimated measure of utility that compares individual data
points with an aggregate (the “majority”). The latter publication
created a reputation system based on the quality of previously
sold data.

While most TD approaches leverage transparency to enhance
data and identity authenticity and data and computation in-
tegrity, TDs are ﬂexible to include PETs such as ZKPs, MPC,
HE, TEEs, and DP such as in [S9]. Furthermore, TD can also
tackle the oracle problem of DLT, i.e., nodes within the network
cannot assure the authenticity of data from outside the network,
e.g., the price of a physical asset or the result of an election. For
example, ChainLink [151] is an initiative that utilizes incentives
to create a trusted oracle network and incorporates many of the
principles of TD.

Digital signatures (DS). While DS1 schemata are common-
place for authentication purposes in today’s IT architectures,
we have found in selected studies the use of two notable public
key cryptography (PKC) schemata that oﬀer some convenience-
related advantages over conventional PKC systems:

• Identity-based [S7], where a key generation center
(KGC) creates a secret key in a way that the entity’s pub-
lic key can be a publicly available unique string, e.g., the
entity’s email address. The KGC must be trusted because
it holds the master secret key from which all parties’ se-
cret keys can be derived. This digital signature assures
identity authenticity as the signature is digitally certiﬁed
by the KGC.

• Certiﬁcateless [S12] DS schemata are a special form of
identity-based PKC whereby an entity’s private key is gen-
erated by both the entity and a KGC so that the KGC is not
aware of the private key of the entity. However, the entity
can prove that the KGC was involved in the key gener-
ation [152]. This approach assures the authenticity of an
entity while tackling the single point of failure of the KGC.

Decentralized identiﬁers (DIDs). Identiﬁers can link an en-
tity electronically across multiple IT systems, such as mobile
phone numbers, ID cards, user names, or emails. These links
are sometimes but not always unique and are facilitated by
identity providers that centrally host registries of these iden-
tiﬁers’ [153].
In contrast, DIDs are globally unique (with
certainty through publishing them on a DLT or probabilis-
tically through randomized generation) identiﬁers decoupled
from centralized registries. DIDs essentially correspond to
URLs linked to a ﬁle containing one or several public keys and
associated metadata that speciﬁes the policies of controlling or
interacting with the associated identity. There are two studies in
our SLR that employed DIDs in combination with DLT in their
conceptual frameworks [S3][S20], described in Table D.7.

Digital ﬁngerprints (DF). DFs are unique physical identiﬁers
that can be attached to or are inherent of items, and thus, one
can be sure to interact with, e.g., the right IoT device [S19].
DFs can be seen as a form of version control at a high level.
However, attaching an identiﬁer securely to a physical object is

1We introduced the fundamentals of DSs in the veriﬁcation layer within

the PETs branch.

17

Figure 7: Classiﬁcation of the identiﬁed authenticity-enhancing technologies in the selected studies of this SLR.

diﬃcult unless it has a unique property, e.g., unique metal pat-
terns in the soldering of a chip. However, even if the attachment
is relatively tamper-proof, e.g., with a crypto-chip, the same
problem also pervades the items that interact with the digital
ﬁngerprinted item, e.g., tracking scanners. Therefore, despite
the authenticity assurance of DFs, their authentication can only
be as truthful as the honesty of the devices that scan the DF.

8. Privacy challenges in IoT data markets

This Section aims to answer RQ2 by distilling the implicit and
explicit challenges unveiled in our SLR and other seminal stud-
ies [10][25] concerning privacy in the context of IoT data mar-
kets. We further classify them into narrow and broad challenges
depending on the scope of their deﬁnition. Fig. 8 summarizes
and outlines the structure of this Section.

8.1. Narrow challenges

Aside from the inherent complexity and low maturity of some
PETs and the compatibility issues with legacy systems [25], we
identiﬁed another speciﬁc set of challenges tackled or circum-
vented by the selected studies.

8.1.1. The trade-oﬀ between utility and privacy

Practitioners working with personal data face the challenge
of balancing the enhancement of individuals’ privacy with the

preservation of data’s utility [S14]. This challenge is explicitly
mentioned by some of the selected studies [S6][S9][S13] and
implicitly tackled by others [S2][S24][S25][S35][S47]. This
dichotomy is the underlying reason behind the tension between
data owners and consumers: the former aim to maximize pri-
vacy while the latter intends to maximize utility, which, in
turn, is frequently determined by data authenticity (see termi-
nology in Section 5). Furthermore, privacy oﬃcers should con-
sider balancing this trade-oﬀ at each stage of an information
ﬂow [10]:
input, computation, output, in transit, and at rest,
which increases the complexity of the task. On the other hand,
decision makers’ or data scientists’ quality of judgment de-
pends on computational integrity and the authenticity of data
and identities, which is aﬀected by the privacy-utility trade-oﬀ.
While PETs from the secure and outsourced computation cat-
egory seem to circumvent the utility-privacy trade-oﬀ by con-
cealing inputs, computation, and outputs, the anointed recipi-
ents of these outputs can still perform a re-identiﬁcation attack.
Thus, anonymization PETs, such as diﬀerential privacy, should
also be included in the stack as they lower the probability of
successful re-identiﬁcation attacks [10].

Data and identity authenticity and accountability bring an-
other problem in the utility-privacy trade-oﬀ. Some PETs,
namely anonymization technologies, increase plausible deni-
ability at the expense of reducing authenticity and, therefore,
accountability [S14][S43]. If the data is fuzzy, the data owner
may claim that such a result is not resembling the truth, which

18

 Privacy- and  authenticity-enhancing  technologies for IoT  data markets Authenticity- enhancing    technologies   Consensus layer Distributed ledger  technology [S11][S19*][23*][S49] Permissioned Hyperledger Iroha  [S3] Hyperledger Fabric  [S10][S33] Rem (SGX) [S12] Quorum  [S18] Permissionless Ethereum  [S1][S13][S20][S28] [S32][S45][S46] Agora [S43] IOTA  [S20][S28][S30] Ekiden  [S2][S25] Verification layer Truth discovery  Majority voting  [S9][S45] Reputation system  [S9] Peer-prediction-based  trustable data aggregation [S23*] Mutual validation  [S23*] Digital signatures [S19*] Identity-based [S7] Certificateless [S12] Decentralized identifiers  [S3][S20] Version control  [S19*] Digital fingerprint  [S19*] Privacy-enhancing  technologies Verification layer Hashing ... ...Figure 8: Overview of the narrow and broad challenges facing privacy-enhancing IoT data markets.

is favorable for individual users. However, such protection is
not beneﬁcial for society in some contexts, e.g., in criminal
contexts. Regarding authentic data from fuzzy identities, if
an authority cannot trace data back to the origin, an individ-
ual could try to claim plausible deniability, which would hinder
processes such as tracking COVID-19 patients to improve pan-
demic countermeasures. For practitioners to ﬁnd a balance in
these contexts, the requirements of any application or platform
should ﬁrst deﬁne the accountability of the involved entities to
strike an optimal balance between utility and privacy.

Regarding accountability in data markets speciﬁcally, mech-
anisms to punish misbehavior, such as banning an entity for
re-selling or not selling authentic data [S7], can be beneﬁcial
to enhance the utility of the market. While AETs such as truth
discovery, e.g., majority voting [S9] [S45] or reputation sys-
tems [S9], incentivize market participants to report honestly
about the exchanged data, their identity, and computation in-
tegrity, among others, the privacy of the entities is not necessar-
ily enhanced. Additional related issues that may arise are veri-
fying the purchased data’s authenticity without violating the in-
dividuals’ privacy [S7]. For example, if a data broker sells anal-
ysis outputs (insights) and not the (privacy-enhanced) original
data, the original data owner’s digital signature is not valid to
authenticate the insights [S7]. Nevertheless, systems could use
zero-knowledge-proof-based authentication of data and compu-
tation, coupled with value deposits locked in smart contracts
to hold participants accountable through enforcing reimburse-
ment across intermediaries. Moreover, other nascent solutions
exploit the ubiquity and proximity of IoT devices in speciﬁc
contexts because the data gathered is likely to be correlated,
which allows for mutual data authenticity veriﬁcation among
IoT devices [S23]. However, exploiting correlation can only be
used for speciﬁc measurements, e.g., weather conditions, vehi-
cle speed, and location, among others.

8.1.2. The recursive enforcement problem

Trust is an essential component in distributed systems that in-
volve diﬀerent stakeholders and, thus, trust is speciﬁcally rele-

vant in the context of IoT data markets. Deﬁnitions of trust gen-
erally refer to a “[...] directional relationship between two enti-
ties” [S49] where one entity (the trustor) has subjective expec-
tations on the behaviour of the other entity (the trustee) based
on previously observed behaviour (reputation-based trust) [154]
or the belief in competencies and corresponding actions – often
within a speciﬁc context [155, 156] and incentivized by joint
interests [157]. Following this deﬁnition, we can consider users
as trustors of application owners protecting their data when
they engage in digital activity. However, the number of data
breaches [9] and privacy scandals such as Cambridge Analyt-
ica indicate that this trust is not always deserved. The recursive
enforcement problem (REP) encompasses the underlying prob-
lem of third-party trust with more nuance: Given a third-party
authority (A), there ought to be another authority (B) to super-
vise A, so that A can be trusted. In turn, there should be yet
another authority C to supervise B [10], and so forth.

The REP is a signiﬁcant challenge that has been cov-
ered and tackled implicitly by some of the studies in our
review [S1][S3][S4][S7][S10][S16][S25][S41]. Additionally,
others tackle a sub-set of the REP, which is the single point
of failure of trusting a unique third party [S6][S12]. Accord-
ing to E. Schomakers et al. [S8], the hesitation in trusting third
parties is one of the main reasons for the slow adoption of IoT
data markets. Indeed, it is hard to technically ensure and prove
that the third party will not use one’s data for purposes other
than those agreed [S8]. Additionally, adoption is further slowed
down because the use of third parties to supervise other parties
incurs costs [S46]. Furthermore, users’ daily interactions with
“trusted” third parties can be regarded as a product of contrived
trust, another form of the REP. For instance, applications from
large service providers with negligible competitors push users
to accept the sometimes poor privacy conditions, e.g., GPS
apps. Note that contrived trust is diﬀerent from the trust users
have on cryptography, open-source code, or consensus mecha-
nisms that the broad scientiﬁc community has audited over the
years.

19

(cid:1)(cid:23)(cid:231)(cid:176)(cid:256)(cid:256)(cid:209)(cid:266)(cid:224)(cid:209)(cid:303)(cid:1)(cid:223)(cid:176)(cid:196)(cid:236)(cid:266)(cid:224)(cid:1)(cid:1)(cid:294)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:388)(cid:209)(cid:266)(cid:231)(cid:176)(cid:266)(cid:196)(cid:236)(cid:266)(cid:224)(cid:1)(cid:1)(cid:312)(cid:209)(cid:196)(cid:231)(cid:266)(cid:276)(cid:256)(cid:276)(cid:224)(cid:236)(cid:209)(cid:303)(cid:1)(cid:236)(cid:266)(cid:1)(cid:312)(cid:231)(cid:209)(cid:1)(cid:1)(cid:61)(cid:276)(cid:129)(cid:1)(cid:129)(cid:231)(cid:209)(cid:1)(cid:61)(cid:276)(cid:129)(cid:1)(cid:236)(cid:263)(cid:294)(cid:176)(cid:196)(cid:312)(cid:1)(cid:276)(cid:266)(cid:1)(cid:294)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:1)(cid:56)(cid:209)(cid:312)(cid:209)(cid:297)(cid:276)(cid:224)(cid:209)(cid:266)(cid:209)(cid:236)(cid:312)(cid:344)(cid:1)(cid:176)(cid:266)(cid:202)(cid:1)(cid:1)(cid:236)(cid:266)(cid:312)(cid:209)(cid:297)(cid:276)(cid:294)(cid:209)(cid:297)(cid:176)(cid:194)(cid:236)(cid:256)(cid:236)(cid:312)(cid:344)(cid:1)(cid:23)(cid:276)(cid:263)(cid:294)(cid:321)(cid:312)(cid:176)(cid:312)(cid:236)(cid:276)(cid:266)(cid:1)(cid:294)(cid:276)(cid:338)(cid:209)(cid:297)(cid:1)(cid:120)(cid:312)(cid:276)(cid:297)(cid:176)(cid:224)(cid:209)(cid:1)(cid:196)(cid:176)(cid:294)(cid:176)(cid:196)(cid:236)(cid:312)(cid:344)(cid:1)(cid:116)(cid:209)(cid:176)(cid:256)(cid:388)(cid:312)(cid:236)(cid:263)(cid:209)(cid:1)(cid:196)(cid:276)(cid:263)(cid:263)(cid:321)(cid:266)(cid:236)(cid:196)(cid:176)(cid:312)(cid:236)(cid:276)(cid:266)(cid:1)(cid:29)(cid:176)(cid:312)(cid:176)(cid:1)(cid:296)(cid:321)(cid:176)(cid:256)(cid:236)(cid:312)(cid:344)(cid:1)(cid:2)(cid:263)(cid:194)(cid:236)(cid:224)(cid:321)(cid:276)(cid:321)(cid:303)(cid:1)(cid:202)(cid:176)(cid:312)(cid:176)(cid:1)(cid:276)(cid:338)(cid:266)(cid:209)(cid:297)(cid:303)(cid:231)(cid:236)(cid:294)(cid:1)(cid:113)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:1)(cid:202)(cid:236)(cid:303)(cid:294)(cid:176)(cid:297)(cid:236)(cid:312)(cid:344)(cid:1)(cid:113)(cid:297)(cid:236)(cid:196)(cid:236)(cid:266)(cid:224)(cid:1)(cid:120)(cid:196)(cid:176)(cid:256)(cid:176)(cid:194)(cid:236)(cid:256)(cid:236)(cid:312)(cid:344)(cid:1)(cid:2)(cid:312)(cid:312)(cid:176)(cid:196)(cid:253)(cid:303)(cid:1)(cid:276)(cid:266)(cid:1)(cid:294)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:1)(cid:77)(cid:209)(cid:224)(cid:176)(cid:256)(cid:1)(cid:129)(cid:231)(cid:209)(cid:1)(cid:196)(cid:276)(cid:294)(cid:344)(cid:1)(cid:294)(cid:297)(cid:276)(cid:194)(cid:256)(cid:209)(cid:263)(cid:1)(cid:129)(cid:231)(cid:209)(cid:1)(cid:194)(cid:321)(cid:266)(cid:202)(cid:256)(cid:236)(cid:266)(cid:224)(cid:1)(cid:294)(cid:297)(cid:276)(cid:194)(cid:256)(cid:209)(cid:263)(cid:1)(cid:116)(cid:209)(cid:196)(cid:321)(cid:297)(cid:303)(cid:236)(cid:337)(cid:209)(cid:1)(cid:209)(cid:266)(cid:223)(cid:276)(cid:297)(cid:196)(cid:209)(cid:263)(cid:209)(cid:266)(cid:312)(cid:1)(cid:1)(cid:294)(cid:297)(cid:276)(cid:194)(cid:256)(cid:209)(cid:263)(cid:1)(cid:129)(cid:231)(cid:236)(cid:297)(cid:202)(cid:388)(cid:294)(cid:176)(cid:297)(cid:312)(cid:344)(cid:1)(cid:312)(cid:297)(cid:321)(cid:303)(cid:312)(cid:1)(cid:23)(cid:276)(cid:266)(cid:312)(cid:297)(cid:236)(cid:337)(cid:209)(cid:202)(cid:1)(cid:312)(cid:297)(cid:321)(cid:303)(cid:312)(cid:1)(cid:120)(cid:236)(cid:266)(cid:224)(cid:256)(cid:209)(cid:1)(cid:294)(cid:276)(cid:236)(cid:266)(cid:312)(cid:1)(cid:276)(cid:223)(cid:1)(cid:223)(cid:176)(cid:236)(cid:256)(cid:321)(cid:297)(cid:209)(cid:1)(cid:129)(cid:231)(cid:209)(cid:1)(cid:312)(cid:297)(cid:176)(cid:202)(cid:209)(cid:276)(cid:223)(cid:223)(cid:1)(cid:194)(cid:209)(cid:312)(cid:338)(cid:209)(cid:209)(cid:266)(cid:1)(cid:321)(cid:312)(cid:236)(cid:256)(cid:236)(cid:312)(cid:344)(cid:1)(cid:1)(cid:176)(cid:266)(cid:202)(cid:1)(cid:294)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:1)(cid:137)(cid:312)(cid:236)(cid:256)(cid:236)(cid:312)(cid:344)(cid:1)(cid:61)(cid:202)(cid:209)(cid:266)(cid:312)(cid:236)(cid:312)(cid:344)(cid:1)(cid:176)(cid:321)(cid:312)(cid:231)(cid:209)(cid:266)(cid:312)(cid:236)(cid:196)(cid:236)(cid:312)(cid:344)(cid:1)(cid:29)(cid:176)(cid:312)(cid:176)(cid:1)(cid:176)(cid:321)(cid:312)(cid:231)(cid:209)(cid:266)(cid:312)(cid:236)(cid:196)(cid:236)(cid:312)(cid:344)(cid:1)(cid:29)(cid:176)(cid:312)(cid:176)(cid:1)(cid:236)(cid:266)(cid:312)(cid:209)(cid:224)(cid:297)(cid:236)(cid:312)(cid:344)(cid:1)(cid:23)(cid:276)(cid:263)(cid:294)(cid:321)(cid:312)(cid:176)(cid:312)(cid:236)(cid:276)(cid:266)(cid:176)(cid:256)(cid:1)(cid:236)(cid:266)(cid:312)(cid:209)(cid:224)(cid:297)(cid:236)(cid:312)(cid:344)(cid:1)(cid:113)(cid:297)(cid:236)(cid:337)(cid:176)(cid:196)(cid:344)(cid:1)(cid:61)(cid:266)(cid:294)(cid:321)(cid:312)(cid:1)(cid:23)(cid:276)(cid:263)(cid:294)(cid:321)(cid:312)(cid:176)(cid:312)(cid:236)(cid:276)(cid:266)(cid:1)(cid:95)(cid:321)(cid:312)(cid:294)(cid:321)(cid:312)Narrow ChallengesBroad ChallengesTackling the REP requires reducing the power and the re-
sponsibility of the third party in a particular aspect of a speciﬁc
service by, e.g., distributing such responsibility among other
parties or distributing the power among multiple parties that
enforce rules on each other. These measures can ease the hes-
itation to trust a single third party, tackle contrived trust, and
reduce the single point of failure because the third party would
be supervised and held accountable by other third parties in
a ﬂat hierarchy. Fortunately, the PETs included in this study
can also circumvent—only onion routing can tackle—the REP,
which, in turn, reduces the need for third-party trust and, there-
fore, reduce contrived trust and a single point of failure. Ad-
ditionally, 5 of the 7 AETs included in this study can tackle
the REP, primarily distributed ledger technology, whose archi-
tecture was purposefully built to tackle the byzantine generals’
problem [158][159], a manifestation of the REP.

8.1.3. The copy problem

Once an entity releases data freely or for proﬁt-seeking, the
data is no longer under the original owner’s control. Conse-
quently, the recipients of such data can copy and, e.g., re-sell
or use the entity’s data for a non-agreed purpose without in-
forming or acknowledging the original owner [10]. Beyond the
privacy threats the copy problem (CP) entails for users of ser-
vice providers under poor privacy conditions, the CP is a ma-
jor obstacle for organizations to engage in data markets, which
some of the selected studies implicitly tackle [S2][S25][S45].
The CP leads companies to either hoard or sell data as fast as
organizations obtain the data, lest its value drops [10]. Nonethe-
less, secure and outsourced computation PETs such as trusted
execution environments or homomorphic encryption can tackle
the CP by allowing other entities to extract value without los-
ing control over the input data beyond the speciﬁc informa-
tion sold, such as an algorithm’s evaluated output on this data.
This paradigm is profound because tackling the CP makes data
scarce (to some extent), as the original data is not shared, and
the data owner would not allow a non-agreed computation.
Thus, selling the access to data can be more attractive to com-
panies, as data would preserve their value longer than releasing
the data.

A subset of the CP is the bundling problem (BP) [10], which
is an attack vector diﬀerent from re-identiﬁcation that occurs
when an entity requests actively or passively more data than
strictly needed to (i) prove a claim or (ii) perform an analy-
sis. Harvesting more information than needed worsens data
breaches’ consequences for individuals and companies and in-
dicates questionable business ethics. For instance, (i) to prove
one’s age with an ID card, the prover usually shares all the in-
formation in the ID instead of only the age and proof of the
card’s authenticity. The BP is a subset of the CP because if one
tackles the CP, neither necessary nor additional information is
released beyond the required computation. For example, tack-
ling the CP by restricting veriﬁcation and processing to a trusted
execution environment also tackles the BP. In this setting, the
data consumers cannot copy the necessary data or metadata for
other unsolicited analyses, despite being able to process meta-
data to verify the authenticity of the data and the integrity of the

computation and obtaining the desired outputs of the analysis.
Additionally, (ii) anonymization-based PETs such as diﬀeren-
tial privacy or k-anonymity reduce data authenticity to tackle
the BP. For instance, in a demographic analysis that only re-
quires the ﬁrst digits of the ZIP code to perform clustering,
data curators can generalize the ZIP codes with k-anonymity, so
only the strictly necessary information is revealed to data scien-
tists. Nonetheless, anonymization can suﬀer from background-
knowledge-based attacks [S48][160] and does not solve the CP
because the data consumers can replicate the privacy-enhanced
data.

8.2. Broad challenges

8.2.1. The IoT impact on privacy

The paradigm brought by the IoT brings signiﬁcant amounts
of data to markets. However, this paradigm also bears some
of the shortcomings of IoT devices [161]. Table 2 contains an
overview of these challenges and brieﬂy discusses their impact
on privacy. In summary, privacy is always aﬀected by the con-
text and employed technologies, which underlines the impor-
tance of adhering to privacy-by-design principles [140] and the
need for practitioners in other ﬁelds such as software engineer-
ing, economics, law, and politics to tackle together the diverse
issues that IoT entails for privacy.

8.2.2. Attacks on privacy

Adversaries can be malicious, actively trying to breach users’
privacy through hacking, or honest but curious, passively gath-
ering data from users to reveal hidden insights [S35]. Both of
these entities can carry re-identiﬁcation attacks with the col-
lected information. Within the context of the IoT, the list of se-
curity and privacy attacks is extensive (sniﬃng, cache poison-
ing, DoS/DDoS, sinkhole attacks, replay attacks, among oth-
ers) [168]. Furthermore, within our SLR, W. Dai et al. [S45]
discuss some of the additional attack vectors these malicious or
curious entities may execute in the context of IoT data markets
to learn sensitive information from users. Notable ones include:
Data forwarding, which is one way the copy problem material-
izes; roles collision, where data brokers and buyers may be the
same or collaborating entities, and, therefore, the broker could
rig the auction for its beneﬁt and access the sold data; and side
channel attacks, where attackers exploit the physical properties
of the hardware or its power consumption to extract knowledge
from the hidden computations (trusted execution environments
suﬀer mainly from this attack).

Such attacks make the possession of data intrinsically risky
because if attackers are successful, data re-identiﬁcation is pos-
sible [S38], even if data have undergone some form of pri-
vacy enhancement [169]. There are common attacks used to
re-identify data, e.g., reconstruction, tracing, or linkage at-
tacks [125][170]. Some of the most famous re-identiﬁcation
white-hat attacks involve A. Narayanan and V. Shmatikov [160]
who deanonymized the Netﬂix Prize dataset with IMDB’s pub-
lic dataset in 2008, M. Archie et al. [171] who performed the
same feat with Amazon’s public review data, and L. Sweeney

20

Table 2: Overview of challenges brought by the IoT paradigm into data markets explicitly covered by some of the studies included in this SLR.

Challenge

Studies

Description

Impact on privacy

An IoT data market should be agnostic to these diﬀerences
and minimize any additional requirements; however, it is un-
clear how global data markets should harmonize data coming
from diﬀerent jurisdictions with diﬀerent privacy regulations
and how an IoT device can interact with another whose, e.g.,
veriﬁcation schemata are considered inadequate. In addition
to these obstacles, a lack of interoperability may restrain PETs
that involve the communication between many devices, e.g.,
MPC.

Any additional computation requires a higher investment in re-
sources and manufacturing, and running some PETs becomes
infeasible without this extra investment. Consequently, a set
of PETs is excluded without more computation power, e.g.,
cryptography-based PETs such as HE, MPC, ZKP, some digi-
tal signatures, or consensus algorithms. This limitation, how-
ever, may only apply to contexts where it might not be possi-
ble to connect IoT devices acting as clients with proprietary or
trusted third-party nodes where these PETs are executed.

Processing time constrains the number of usable PETs, exclud-
ing those that require long execution times, such as fully HE
or creating a ZKP.

The impact may seem beneﬁcial in terms of privacy; how-
ever, unreliable data leads to veriﬁcation and secure computa-
tion schemata to fail and anonymization technologies to over-
perturb the data as the underlying data is not entirely truthful.

Having unclear data ownership leads to a misguided deploy-
ment of PETs, which may cause detrimental consequences if
the privacy measures fall short. On the other hand, if the prac-
titioner knows who has the right to the data and what the owner
is reticent to share with a third party, then selected PETs and
their privacy tuning can be optimized accordingly.

Some PETs, such as semantic and syntactic technologies, al-
low adjusting the degree of privacy; however, others are more
rigid. Selecting and adapting a PET to the IoT devices’ de-
ployment context requires expertise.

Heterogeneity
and
Interoperability

[S36]
[S39]

The IoT consists of billions of IoT devices from diﬀer-
ent manufacturers, running diﬀerent software on diﬀer-
ent local networks and geographic regions, with diﬀer-
ent computation power and storage capacity [162]. Fur-
thermore, diﬀerent communications standards, connectiv-
ity and availability aggravate the interplay of IoT devices.

Computation
power

[S5]
[S11]
[S48]

Manufacturers produce many IoT devices designed to con-
sume low energy and require minimal volume, limiting
these IoT devices to the core functionalities of monitoring
and communication [S11].

Minimizing the physical volume of an IoT device reduces
their price but limits their storage capacity, forcing IoT
devices to transmit the data to a data warehouse or a data
market as quickly as possible. This tendency intensiﬁes in
some IoT applications where the time delay tolerance is
low to enhance the utility of real-time information [S17].

An unreliable IoT design may aﬄict thousands of IoT de-
vices mass-produced by a manufacturer, which at deploy-
ment may lead to millions of unreliable data points. Fur-
thermore, networks may also be unreliable, further wors-
ening the quality [S14].

When purchasing a device or a cluster of IoT devices, e.g.,
a phone, consumers also expect to own the data they are
generating. However, the phone manufacturer and service
providers expect to receive parts of this data nowadays
In addition to this clash of inter-
with meager consent.
ests, there are scholars that ponder whether data belongs
to anyone in the ﬁrst place, like L. Determann [163].

Depending on the IoT devices’ deployment location, the
degree of privacy measures should be higher or lower, e.g.,
sensors in vehicles, smart homes, phones, and wearables.
Furthermore, IoT deployments should adapt the monitor-
ing time to an adequate amount depending on the con-
text [S40].

Storage capacity
and real-time
communication

Data quality

Ambiguous data
ownership

[S1]
[S5]
[S11]
[S17]
[S29]
[S48]

[S14]

[S4]
[S10]
[S14]

Privacy disparity

[S4]
[S40]

Pricing

Scalability

[S4]
[S14]
[S17]
[S50]

[S25]
[S36]

There are multiple variables imposing the price of data
aside from supply and demand:
the
source, either purchasing the data or the access [S17], and
the privacy level. These factors add additional complexity
to pricing, e.g., the sources have become disparate with
the IoT, which drives pricing to a more granular task than
before, when aggregated data could be sold as a unit [S17].

the truthfulness,

Aside from payment enforcement mechanisms, pricing in-
volves negotiations, which frequently must ensure privacy.
This adds an extra layer of complexity to the deployment of
PETs. Furthermore, as data markets trade with more gran-
ular data points, PETs that need aggregation might be ex-
cluded in some contexts, e.g., syntactic technologies such as
k-anonymity.

The number of IoT devices and streamed data grow ex-
ponentially across industries [164][162], which extends
data collection and improves analytics across diﬀerent do-
mains, e.g., health, insurance, or ﬁnance. To gain these
beneﬁts, there is a need to increase networks’ communica-
tion and overall storage capacity as well as interoperability
and security eﬀorts.

As the IoT scales, analysts will access more datasets from
diﬀerent domains to create new products and services, e.g.,
linking driving behavior with insurance in pay-how-you-drive
schemata [165]. Such innovations stem from the “mosaic ef-
fect” [166], where disparate datasets with limited informa-
tion value can obtain signiﬁcance when combined with other
datasets. However, malicious entities can leverage such an ef-
fect to extract sensitive personal information not explicitly con-
tained in a dataset [167].

21

et al. [6] (the inventor of k-anonymity) who re-identiﬁed partic-
ipants within a genome sequence dataset in 2013. Furthermore,
in 2014, X. Gao et al. [7] tracked drivers with home address
and vehicle speed as inputs, and in 2020, D. Kondor et al. [169]
matched users with large-scale mobility datasets from a mobile
network operator and transportation smart card usage.

Overall, IoT data markets will facilitate access to large quan-
tities of data from diﬀerent domains,
including biometrics,
which will increase the impact of these attacks and the potential
harms to individuals, e.g., insurance, employment, or price dis-
crimination. Therefore, IoT data markets require a more robust
adoption of PETs and security standards.

8.2.3. Legal challenges

Progressively along the past decades, governmental institu-
tions have released laws to protect the privacy of their citizens
(see Section 2.1). These laws also refer to an individual’s and
businesses’ right to exploit their data commercially, which pro-
vides leeway for data markets [S28] and aims to uncover the
untapped potential of data for innovations.

Nonetheless, research points out the sometimes unrealistic
expectation to monitor the entirety of the Internet for privacy
violations [S45], and the dexterity of hackers to ﬁnd novel de-
ception methods [S3], and that laws are more reactive than pre-
ventative. Well-known networks of illegal proprietary digital
asset exchanges, e.g., scientiﬁc works and how users of digital
services give away data, tacitly provide testimony of the fail-
ure of data-related legal measures today, and the problems will
likely increase with the accruing number of IoT devices [S38].
Moreover, privacy regulations can strangle free markets and in-
novations if they are too stringent [S45].

Aligned with these deﬁciencies, J. Henrik et al. [S38] intro-
duced privacy regulation pitfalls that the IoT unfolds in data
markets in 2013. They note that (i) deﬁnitions of personally
identiﬁable information will be deprecated as unprecedented
amounts of data can be aggregated, easing re-identiﬁcation, (ii)
the development and audit of PETs is costly, which may limit
business models and potentially make disregarding privacy reg-
ulation proﬁtable [172], (iii) privacy violations result on small
ﬁnes or remain unpunished, (iv) technology tends to outpace
regulation, and (v) the ubiquity of IoT devices will yield more
illegal secondary personal data markets. After almost a decade
of further research, (i) seems valid, at least in some scenarios.
The ambiguity of privacy regulation is a barrier in some cases,
as practitioners may default to weaker forms of privacy if their
architecture appears to comply. This leads to re-identiﬁcation –
an attack that is also more practical with the increasing number
of IoT devices [160] – being more likely to succeed. However,
in defense of these practitioners, while PETs have improved
since 2013, some PETs that oﬀer better privacy enhancements
are still complex and not yet performant in 2021.

Based on the prior arguments, pitfall (ii) seems to hold; how-
ever, (iii) is no longer a strong pitfall. Since the enforcement
of GDPR [149] in 2018, GDPR has punished multiple corpora-
tions with considerable ﬁnes ranging between e20 million and
up to 4 % of a corporation’s annual worldwide turnover of the
preceding ﬁnancial year. As of the writing of this publication,

GDPR has harvested considerable ﬁnes assigned to Google in
France on two occasions [173], Amazon [174], H&M [175]
or the telecommunications operator TIM [176]. These ﬁnes
alone accumulate to e282 million. These statistics are a sign
that PETs are not appropriately introduced in production appli-
cations even by big technology companies and that not com-
plying with privacy regulations in an IoT data market has dire
economic consequences. While these ﬁnes could indicate how
proﬁtable it still is to violate privacy regulation (iii), one can
no longer vigorously defend (iii). Pitfall (iv) seems to mate-
rialize as long as the nature of law-making does not change.
Lastly, pitfall (v) is concerning, given the existence of legal per-
sonal data markets that store up to 750 million user proﬁles and
trade 75 million online auctions daily like BlueKai [24], whose
data could leak to the increasing number of illegal shadow mar-
kets [S6].

9. Discussion

This Section presents a set of key ﬁndings (KF) distilled from
the two research questions answered in Sections 6 and 7 as
well as 8, the content and metadata of the 50 publications in-
cluded in our SLR, and other seminal studies that we encoun-
tered throughout our SLR but which do not necessarily address
IoT data markets directly. Lastly, we cover the limitations of
this study and future work.

9.1. Key ﬁndings
pKF1q The attention of scientists towards privacy-enhancing
technologies in the ﬁeld of data markets for IoT devices has in-
creased notably in recent years. The selected publications are
modern, as 49 of the 50 studies were published between 2012
and 2020, and 34 of them (68 %) were published either in 2018,
2019, or during the ﬁrst half of 2020. While the absolute num-
ber of publications in 2020 is lower than in 2019 because we
captured only the ﬁrst seven months of 2020, Fig. 9 illustrates
the arguably accelerating trend of the cumulative curve of pub-
lications in the ﬁeld of privacy-enhancing IoT data markets.

pKF2q The most frequent research type (design and creation)
and least common research contribution (lessons learned) sug-
gest that privacy-oriented IoT data markets are still maturing
and have not faced many production-grade implementations
yet. According to Fig. G.13, around 76 % of the publications
use a design and creation research approach, while only 4 %
perform a case study. A further indication of ﬁeld novelty is that
only one out of the 50 publications had the contribution type
lessons learned [S40]. Furthermore, while 35 studies (70 %)
were of research type solution proposal, to the best of the au-
thor’s knowledge, only one solution appears to have an imple-
mented system that is applied in production [S25].

pKF3q The selected studies rarely leverage existing libraries
that provide PETs and often only build upon architectures de-
veloped in previous work to a small degree. Therefore, to gain
more practical relevance, it may be beneﬁcial for researchers
to improve and extend existing work instead of reinventing the

22

Table 3: A mapping of pivacy- and authenticity-enhancing technologies (PET and AET) to the narrow challenges using the terminology deﬁned for this review. The
extent of enhancement of privacy, utility, and characteristics of the diﬀerent PETs and AETs varies from signiﬁcantly increasing ++, over +, +-, - to signiﬁcantly
decreasing - -. na denotes not applicable. w/ denotes with. *Considering a digital certiﬁcate when using digital signatures, if applicable. The privacy column
assumes data and identity are authentic.

wheel. The research community and industry have developed
many open-source libraries to employ zero-knowledge proofs,
homomorphic encryption, secure multiparty computation, or
diﬀerential privacy (see Section 6). However, none of the stud-
ies have indicated their use. Furthermore, studies often do not
build upon each other, leading to overlapping further. For exam-
ple, W. Gao et al. [S41] and Z. Chen et al. [S34] both showcase

an auction that obscures the bids by employing partially homo-
morphic encryption. However, W. Gao et al. only refers to the
work from Z. Chen et al. in one line, noting that “[...] there
is only few literatures on designing privacy-preserving schemes
in data market auctions.” Moreover, [S42] builds upon [S27],
and [S2] upon [S25], but each of these two sets belongs to the
same group of researchers. In conclusion, it may be beneﬁcial

23

ProcessingLayerPrivacy  [Conﬁdentiality (Conf)]PETNarrow challengesTechnologyInput
[Data (D)]
[Identity (I)]ComputationOutputUtilityAuthenticity
[Data (D)] [Identity (I)]Integrity
[Data (D)]
[Computation (C)]Recursive enforcement problemCopy problem[Bundling problem (BP)]Trusted execution environmentsPartially homomorphic encryptionFully homomorphic encryptionSecure multiparty computationPrivacy-preserving
data miningDiﬀerential 
privacy (DP)K-anonymityPerturbativePseudonym 
creationZero knowledge proofs 
for computational integrityZKPs of 
anonymous credentialsSecure and outsourced computation (SOC)Anonymization (AN)CommunicationEncryptionOnion routingStorageTechnology categoryVeriﬁcationHashingIdentity-based digital signaturesCertiﬁcateless 
digital signaturesSovereigntySmart contracts
(for privacy policies)Privacy policies
(Access control)AETCharacteristics and challenges are pegged to distributed ledger technology.Characteristics and challenges are pegged to the selected PETs and AETs employed to fulﬁl the privacy requirements.VeriﬁcationConsensusDistributed ledger technology (permissioned)Distributed ledger technology (permissionless)Truth discoveryDecentralized identiﬁersDigital ﬁngerprint
(Version control)++
D+++-
Conf++
D++
D++ D 
(+ D)+
D+-
D+-
D+-
I++++++ 
(na)++
D++++
I++
D++
D, C++++++Storage layer: ++ D, Conf 
Communication layer: ++ D, Conf++
I+-
D, Conf+-
D, Conf+-
Conf+-
Conf+-
Conf-
I+-
D, Conf+-
I-
D--+-
Conf+-
Conf-- 
I--
I-
I-
D--++
D++
I++
D, C++
D, C++
D, C++
D, C++
D, C++ D, C (na)na (+- D)++
*D++++
Dnananana++-+-+-nanananaCircumventsCircumventsCircumventsCircumventsCircumventsCircumventsCircumventsnanananananananananananaTackles
(BP)Tackles
(BP)TacklesTacklesTacklesTacklesTacklesTackles
(BP)Tackles
(BP)Tackles
(BP)Tackles
(BP)CircumventsnaTacklesna+- , Conf 
(+)W/ SOC (w/ AN)Federated (w/ AN)na (na)na (+- D)++
DCircumventsna++
*I++
*I++
D++
DnananaTackles++
Inananana+
I+
D++
D, CTacklesnaTacklesnana++
I++
D--
Dnana+
I+-
I+
D, C++
D, CTacklesTacklesnana+-
D-
D-
D-
InananananananaPrivacy-utility tradeoﬀRing 
digital signaturesBlind
digital signatures+
I--
*I+
Dnananana-
I++
D++
D++
*InaTackles
(BP)Tackles
(BP)nana
D--
*I--
*I++
Dna
Dna
DFigure 9: Publications in the ﬁeld of privacy-enhancing data markets for the IoT from January 2002 to July 2020.

for researchers to incorporate building blocks from previous
data market architectures to advance privacy-oriented research.
Moreover, many studies included in Table D.7 aim to create
an IoT data marketplace employing distributed ledger technol-
ogy (DLT). However, there seems not to be a consensus about
which DLT to use for IoT data markets, as the authors build
upon Ethereum, IOTA, Hyperledger Iroha, Fabric, Agora, or
Quorum, among others. Speciﬁcally, as an example, [S32] uses
Ethereum smart contracts for payments while [S28] only uses
these contracts for safelisting and employs IOTA for payments
instead.

pKF4q The content of the selected studies can be categorized
into two main orthogonal research streams within the context
of privacy-enhancing IoT data markets: architectures and data
trading schemata. The ﬁrst research stream is dedicated to the
design of privacy-enhancing architectures for the exchange of
data in IoT data markets (25 studies, 50 %), and the second one
focuses on the design of privacy-enhancing data trading such as
auctions (12 studies, 24 %). The remaining studies can be as-
sociated with domains like legal [S6], user preferences [S8], or
IoT data market challenges [S14][S19]. The selected studies,
and also international initiatives such as the European GAIA-
X [5], hence envision data markets beyond matchmaking and
auction capabilities. Speciﬁcally, the studies that we analyzed
structure the software, hardware, abstract entities, and their co-
ordination, data processing, storage, communication, and the
oﬀered services to build a holistic or part of a privacy-enhancing
IoT data market that includes PETs to tackle some of the chal-
lenges described in Section 8.

the acknowledged need for

pKF5q Despite
combining
anonymization and secure and outsourced computation tech-
niques, none of the researchers behind the 12 studies propos-
ing data trading schemata, and only two publications out of
the 25 designing data market architectures employ both PET
categories in combination. Although PETs such as homomor-
phic encryption (HE) or secure multiparty computation con-
ceal inputs and computation, the outputs can leak information
about the underlying data and hence may be exposed to re-
identiﬁcation attacks [10]. Combining secure and outsourced
computation techniques with anonymization-based PETs like

diﬀerential privacy (DP) can help to make the outputs less sen-
sitive. Moreover, leveraging only anonymization PETs does not
suﬃciently address the copy problem.

Within the 12 selected studies focused on data trading, DP
is the most frequently used PET in auctions to enhance the pri-
vacy of the exchanged data. [S16][S22][S24][S37] employ DP
in various forms to set the privacy levels and, subsequently,
the price of the traded IoT data. Researchers might choose
DP over other anonymization technologies because DP is the
only PET with a mathematical guarantee of privacy [14]. At
the same time, partially HE (PHE) is the PET of choice to
enhance the privacy of the bidding process. A group of au-
thors [S26][S34][S41] chose PHE primarily for hiding the bids,
conﬁdentially computing the winner, and only revealing the
output to the auction’s winner. Researchers might decide to use
PHE over other forms of HE, secure multiparty computation,
or trusted execution environments (TEEs) despite PHE’s sig-
niﬁcantly less general scope because PHE has relatively high
performance and is conceptually simple.

Together, DP and PHE can holistically enhance the privacy
of auctions, which is a contribution we have not found in this re-
view. S. Sharma et al. [S48] emphasize that some HE schemata,
such as Paillier’s, must complement other methods to guarantee
more protection. Moreover, while HE protects the input and the
computation itself, if the intended recipients of the decrypted
output are malicious, they may reverse engineer the output to
learn properties about the input. An additional modiﬁcation
employing, e.g., diﬀerential privacy, of inputs or decrypted out-
puts before sharing may help prevent this attack in exchange
for accuracy and thus utility. The same argument applies to
other secure and outsourced computation methods when used
in isolation. We consequently point to a lack of combination
in the research stream of data market architectures, except for
two publications from the same group of researchers [S2][S25],
which use TEEs to train machine learning models with DP.

pKF6q The selected studies employ three dimensions to char-
acterize data markets that entail privacy concerns:
the de-
gree of decentralization, the types and number of data do-
mains, and the types of sellers and consumers. Each of these
dimensions, for example, characterized by [S1][S27][S46] re-
spectively, brings privacy concerns. Data may be stored by the
seller, the platform provider, or a decentralized platform using,

24

1214531016811111111113481316264250010203040506020022003200420052006200720082009201020112012201420152016201720182019July 2020Number of studiesNumber of publications per yearCumulative number of publications over timee.g., a combination of commercial cloud storage, interplane-
tary ﬁle systems, or blockchains. Depending on the degree of
decentralization and replication, practitioners need to consider
diﬀerent leakage risks. In particular, if the architecture relies on
a blockchain, PETs are particularly important [146].

An increase in the number and types of data domains opens
additional attack vectors and more possibilities for malicious
entities to link an individual’s data across databases. This
hyper-connectivity between datasets can render the deﬁnitions
of de-identiﬁed data, such as HIPAA’s, obsolete and suggests
that privacy enhancements in the data economy should be de-
ﬁned globally and not locally.

The degree of privacy enhancement should depend on the
type of seller and consumer, e.g., consumers may expect higher
privacy guarantees when a health insurance company gathers
their data than when the collector is a renowned health research
institution.

pKF7q Based on our classiﬁcations in Sections 6 and 7 and
inspired by a set of seminal selected studies, we have cre-
ated a reference model for the design of IoT data markets in
Fig. 10, and detailed in Table 3. Most of the studies included
in this SLR proposed solutions without following a reference
model, except for D. L´opez and B. Farooq [S3] and C. Niu
et al. [S7], who developed their own without a systematic re-
search. C. Niu et al. [S7] condense their architecture into
two layers: data acquisition and trading. On the other hand,
D. L´opez and B. Farooq [S3] present a more holistic view of
privacy-enhancing IoT data markets with six layers (identiﬁca-
tion, privacy, contractual, communication, consensus, and in-
centive) inspired by the Open System Interconnection model
and heavily conditioned by the use of blockchain technology.
This model, however, lacks essential steps of an IoT data market
that several publications in our SLR focused on, namely stor-
age [S1][S12][S25][S28] and processing [S7][S19][S20][S45].
Furthermore, the identiﬁcation layer [S3] can be regarded as
a subset of veriﬁcation, which also includes data veriﬁcation.
Other studies base their market design on the type of partic-
ipants [S15][S24][S27][S33][S43], e.g., sellers, aggregators,
brokers, among others, and the type of data domain [S46], e.g.,
health, ﬁnancial, or a combination. However, these categories
cannot be transferred to other contexts as easily as a reference
model agnostic to entity and data domain types.

Our reference model hence combines and generalizes some
of the layers from [S3] and [S7] and complements them with
additional layers such as the data auction, storage, veriﬁca-
tion, processing, and sovereignty layer (see Fig. 10). Most of
these layers need multiple PETs, as there is no “one-size-ﬁts-
all” technology to enhance privacy. To navigate these layers in
detail, refer to Table 3 and Fig. C.11. Furthermore, we distin-
guish between a contractual and sovereignty-related design to
separate formal agreements from privacy and ownership poli-
cies. Furthermore, given the distinct purpose and implementa-
tion that auction schemata play in a data market, they should
be respected by a unique IoT data market layer (auction ded-
icated studies:
[S16][S22][S24][S37], among others). Lastly,
incentives are necessary to encourage behavior that preserves

the pre-deﬁned qualities of the IoT data market, e.g., optimized
prices [S3][S16], data authenticity [S16][S23], or maintaining
the infrastructure like a permissionless DLT.

Figure 10: Reference model for the layers of a privacy-enhancing IoT data
market.

pKF8q Aside from the ubiquity of digital signatures in IT sys-
tems, in this SLR, distributed ledger technology (DLT) is most
frequently employed as the backbone of IoT data market de-
sign (see Fig. G.14), despite the lack of consensus on its use
and DLT-based applications in production. Although central-
ized systems seem more eﬃcient and easier to deploy, and
despite the seemingly few industrial applications running on
blockchain today, many researchers in this SLR still advocate
for distributed systems using DLT. Within the 35 solution pro-
posals, around 31 % chose permissionless DLT, 14 % consor-
tium DLTs, and the authors of the remaining 55 % either re-
viewed DLT, implemented a centralized solution, or focused on
designing narrow features. However, we noted that within the
45 % of DLT-based designs, many authors still relied on single
entities for data processing or storage. Speciﬁcally, only one of
the 50 studies [S25] has a public blockchain-based ecosystem
in production, yet without a real-world use case running. These
statistics indicate a lack of adoption despite substantial research
eﬀorts.

Furthermore, while blockchains enhance authenticity, assure
integrity, and enable payments without the need for a trusted
third party, blockchains are limited in storage capacity [S1],
computation power [S25], and can exacerbate privacy issues be-
cause of their tamper proof-quality and inherent data and com-
putation replication [57][142][148][177]. Consequently, almost
all studies that include blockchain technology to support an IoT
data market require PETs to protect users’ data and identities.
These studies go as far as creating innovative privacy-enhancing
blockchain architectures with other PETs as building blocks,
e.g., trusted execution environments [S25][S45], or adding a
privacy layer to their market design based on diﬀerential pri-
vacy [S3]. However, within the literature, there are also ques-

25

PETsStorageConsensusData AuctionProcessingVeriﬁcationCommunicationSovereigntyIncentivesContractualAETsAETsPETsHardware Infrastructuretionable statements such as “[...] researchers and technologists
have found that blockchain can be a potential solution to the
privacy problem by decentralizing information [...] Blockchain
can be used to securely share private information [...]” [S3],
“Blockchain-based approaches provide decentralized security
and privacy [...]” [S11], or “Blockchain has been proven to pos-
sess security, immutability, and privacy properties, which has
caused a lot of researchers to introduce it into the privacy and
security concerned IoT” [S23]. These statements, coupled with
the current excitement around blockchain, can lead practition-
ers in the industry to wrongfully push blockchain for “privacy”.
Therefore, the community would beneﬁt from clear explana-
tions of why authors employ blockchain and clearly state the
need for other technologies to enhance privacy.

9.2. Limitations

Even though we have adopted a rigorous research design and
paid particular attention to the selection and analysis of pub-
lished studies, SLRs have limitations that may have undermined
our eﬀectiveness. These threats include (i) incompleteness of
study search, (ii) bias in study selection and (iii) inaccuracy of
data extraction.

(i) Some relevant publications might be absent. To mitigate
this limitation, we searched in several highly reputed digital li-
braries, performed a preliminary search to determine suitable
search strings, conducted a backward search to identify addi-
tional related work, and included studies in advance that met
the standards and ﬁlters of this SLR. These measures reduce the
probability of missing relevant publications. (ii) The experience
and knowledge of the researchers may drive the study selection
with an inherent bias. Nonetheless, following Kitchenham [44],
we aimed to create a set of explicit inclusion and exclusion cri-
teria to maximize the degree of objectivity. To mitigate diﬀer-
ent appreciations of these criteria, we conducted a preliminary
search to ensure researchers have a consistent understanding of
the requirements. Furthermore, two researchers conducted the
selection process independently and resolved the conﬂicts be-
tween their decisions interactively. (iii) There might be a bias in
selecting the extracted data, which may aﬀect the classiﬁcation
results of the selected studies. To mitigate this potential limita-
tion, the two researchers speciﬁed a set of data extraction cards
(see Section 3.2) to eliminate any misalignment in the data ex-
traction process results.

9.3. Future work

The opportunities and need for future work in the context
of privacy and data markets for the IoT highlighted by the
selected studies resonate with the challenges covered in Sec-
tion 8. Most notably, there is a need to solve the copy prob-
lem [10][S4][S17] and to lessen IoT devices’ limitations regard-
ing computation [S5][S11], storage and capacity [S29][S48] to
tackle or circumvent the constraints PETs may induce. More-
over, to decrease the probability of re-identiﬁcation attacks, fur-
ther work is needed to advance the maturity of PETs and com-
bine them, e.g., bringing together diﬀerential privacy and secure
and outsourced computation eﬃciently. Additional research is

also necessary to create standards for data markets, such as a
language to describe privacy requirements, universal APIs to
interact between diﬀerent IoT devices with various degrees and
techniques for privacy protection, and machine-readable deﬁni-
tions of privacy, e.g., using ontologies [S40]. In this context,
a more detailed description and classiﬁcation of the layers that
we found relevant for classifying privacy and authenticity en-
hancing technologies (see Figure 10) constitutes a promising
and relevant avenue for future research. Nonetheless, we want
to emphasize that privacy is not the only challenge that needs
to be addressed, as future research must also consider, for in-
stance, scalability.

If society considers privacy a necessity, it should be enhanced
by default and optimally in any system without attaching price
tags to one’s privacy, as some of the selected studies pursued
suggest [S16][S15][S22]. We ﬁnd this posture a worthy re-
search endeavor and encourage researchers to ponder whether
monetizing privacy in a competitive market ultimately beneﬁts
society. Furthermore, legal practitioners have ample ground to
develop legislation speciﬁcally around privacy in IoT data mar-
kets and for economists to delve into data pricing and decen-
tralized market interactions. Legal researchers could investi-
gate how stringent privacy regulations should be, as heavy reg-
ulation may strangle free markets and innovations [S45]. Addi-
tionally, the legal, pricing and privacy aspects hinge around data
sovereignty. As long as ownership is ambiguous, researchers’
eﬀorts will struggle to maximize impact. Furthermore, the rele-
vance of our results may reach beyond IoT data markets, as the
analysis of PETs and derived insights, e.g., how IoT impacts
privacy, can permeate other research areas such as privacy-by-
design software engineering, policy-making, and data gover-
nance, politics, and economics. Moreover, most PETs have spe-
ciﬁc performance-, complexity- or utility-related shortcomings
(which we describe in Section 6) that researchers can address.
Lastly, we recommend that researchers derive decision trees
based on Table 3 to enhance the decision-making of privacy
oﬃcers beyond our work. Moreover, we could not ﬁnd any for-
mulation of an information-theoretic quantiﬁcation of the data
leaked from a data market. We also encourage social scientists
to focus on questions related to data sovereignty. To realize a vi-
sion of data markets that beneﬁt society, we suggest researchers
concentrate on roadblocks such as the copy problem. Finally,
institutions should consider updating their privacy-enhancing
processes to eﬀectively participate in IoT data markets.

10. Reassessment of the results

This section provides and discusses new key publications
since the research process ended. Accordingly, we conducted
a research process as per Section 3 for studies dated between
July 2020 and May 2022 and, among them, picked for discus-
sion the ones providing the most signiﬁcant updates to our sys-
tematic literature review or, on the contrary, underlining our
previous ﬁndings. Note that the references included in this sec-
tion correspond only to the newly found publications.

In our new search, we again selected primary and secondary
studies. Overall, our new search resulted in 24 publications:

26

3 more from 2020, 14 from 2021, and—as of May—7 from
2022. These statistics indicate that the trend depicted in Fig. 9
(consolidating KF1) has not reversed. Notably, we could still
not ﬁnd publications discussing production-ready deployments
of privacy-enhancing architectures or auction schemata and no
reference to open-source tooling despite PETs being more ma-
ture since July 2020 (underlining KF2 and KF3).

Among the secondary studies, Wang et al. [178] explored
the concept of privacy in the digital economy more broadly and
pointed out the need for interdisciplinary research to supple-
ment the purely technical PET constructions with the economic
(tradeoﬀ between accuracy and privacy) and governance per-
spectives (privacy policies) that we elaborate on in our paper.
M. Akil et al. [179] systematically investigated privacy-oriented
identity management in the context of the IoT, such as anony-
mous credentials and other techniques that our review covers.
Moreover, T. Gebremichael et al. [180] conducted a less sys-
tematic survey of standards and future challenges, including
discussions regarding authentication and access control, and
highlighted a subset of the privacy challenges of IoT that we
present in Table 2. Additionally, S. W. Driessen et al. [181] pre-
sented a recent systematic literature review on designing data
markets. However, their work did not focus on privacy.

More secondary studies, such as from Deepa et al. [182]
considered blockchain and smart contracts beneﬁcial for pri-
vacy. Furthermore, Perez et al. [183] reviewed PETs in the
context of crowdsensing and emphasized the privacy issues
with smart contracts, along with practical challenges in se-
curity and feed-in of reliable data. They suggested a sub-
set of the anonymization techniques that we present in Sec-
tion 6, such as privacy-oriented digital signatures, anonymous
networking, k-anonymity, l-diversity, t-closeness, and diﬀeren-
tial privacy (DP), and some more speciﬁc ones in the context
of location. While they also mentioned ZKPs, there is no de-
tailed discussion of the secure computing techniques we survey.
Gonc¸alves et al. [184] considered privacy mechanisms in data
sharing for collaborative forecasting and discussed the tradeoﬀ
between privacy and accuracy. They distinguished between per-
turbative techniques (“data transformation”), MPC-based pro-
tocols, and distributed or federated approaches (“decomposi-
tion”) combined with DP. Lastly, Y. Wu et al. [185] provided
a survey that examined the privacy risks that machine learning
poses on IoT data markets supported by blockchains. Hence,
our SLR, with its comprehensive focus on privacy, still ﬁlls the
gaps that we discussed in Section 4.

The primary studies followed a similar pattern to the pre-
viously collected studies. Above all, many still employed
blockchains and often did not provide clear explanations the
corresponding beneﬁts and acknowledgements of the corre-
sponding challenges, speciﬁcally regarding privacy (reaﬃrming
KF8). We again encountered questionable claims such as “[...]
a decentralized approach based on distributed ledger technolo-
gies (DLT) enables data trading while ensuring trust, security,
and privacy” [186], without discussing why DLT enhances pri-
vacy in the rest of the publication about benchmarking IoT data
trading protocols in blockchains. Others followed suit on the
use of blockchain to support electric vehicle trading market-

places with IPFS and a scheme to hide payment sources [187]
and cloaking location with k-anonymity [188] or proposing
a new blockchain architecture with permissioned domains to
enhance privacy for data market places [189]. Another pre-
sented several building blocks (blockchain, trusted execution
environments, gossip learning) without an evaluation of the pro-
posal [190].

A notable exception is the comprehensive details provided
by Manzoor et al. [191] in their blockchain architecture. Their
architecture stores encrypted sensor data in cloud storage, and
smart contracts support sensor registration, data auctioning, and
payments. While the smart contract emits notiﬁcations and
displays the endpoint for retrieving proxy re-encrypted data,
the data are exchanged oﬀ-chain conﬁdentially via proxy re-
encryption. This construction addresses transparency and scal-
ability issues regarding sensor data. Nevertheless, bidding and
payment processes may still reveal sensitive information and re-
quire future research by combining this approach with some of
the PETs we surveyed. R¨uckel et al. [192] also acknowledged
the aggravation of privacy issues on blockchains and combined
federated learning with DP to obfuscate clients’ weights and
use ZKPs to prove the integrity of the training and evalua-
tion process, which they required for providing fair incentives
managed by a smart contract. Another related publication by
Gupta et al. [193] presented a blockchain-based solution for
tracking IoT sensor data across marketplaces and, thus, only
detecting but not preventing illegitimate replication and resale.
These publications fall into the category of architectures
identiﬁed in KF4. We also found papers in the data trading
schemata category (or related): two new data auction schemata
enhanced with DP [194][195], a task assignment scheme in
crowdsensing that hides the tasks’ content with homomorphic
encryption (HE) for crowdsensing [196], and another where
they employ DP on billing data [197]. The latter publica-
tion, however, does not discuss fairness, which is critical in
monetary use cases as a noisy bill can make data prosumers
proﬁt less from their data on some occasions. Furthermore,
Shen et al. [198] and Hu et al. [199] focused on determining
fair prices for end users’ data sets that are anonymized with
DP according to their accuracy and, correspondingly, risks of
revealing sensitive information.

One interesting development that explored the paradigm in
ML markets comes from Q. Song et al. [200]. They developed
a privacy-enhanced framework to evaluate the quality of ML
models and data for sale with functional encryption, achiev-
ing improvements over similar schemata implemented with HE.
Another novel concept by M. N. Alraja et al. [201] strives to
empower users with tools to help them determine the risk of
sharing their information and, accordingly, make an informed
decision about their data framework. It is thus closely related to
enforcing privacy policies in the sovereignty layer. Except for
R¨uckel et al. [192], who focused narrowly on federated learn-
ing, the rest of the new (notable) primary studies did not lever-
age the combination of anonymization and outsourced compu-
tation technologies, underlining KF5. Notably, the new publi-
cations have not altered the data market characterization of KF6
or the reference model of KF7.

27

11. Conclusion

Selected Studies

With this review, we reveal the landscape of PETs in data
markets for the IoT. We have conducted a systematic literature
review (SLR) to identify and ﬁlter the studies aiming to solve
this landscape’s challenges. Consecutively, we formulated ter-
minology to dissect the selected studies’ architectures and ﬁnd-
ings and identiﬁed the PETs that related work employed and
which speciﬁc challenges they addressed.

The authors of the selected studies in this SLR have de-
vised proposals for privacy-enhancing IoT data marketplaces
to comply with privacy requirements while maintaining util-
ity, proﬁtability, and fair and seamless data exchange. Since
this is a relatively new, multidisciplinary research ﬁeld, the op-
timal combination of technologies and theoretical foundations
employed in these proposals is still in the development phase.
Therefore, no proposal has established itself as canonical yet.
Moreover, we observed that the research community needs to
further explore the balancing act of utility and privacy before
data markets ﬂourish. We conclude that the practicality of PETs
needs to advance further to positively impact data markets for
the IoT. Additionally, we suggest researchers solve the copy
problem and improve privacy-enhancing veriﬁcation as their
absence discourages data markets from forming. We also dis-
covered that research on privacy-oriented data markets could
beneﬁt from increased reuse of components from previous arti-
cles and existing open-source libraries and a more explicit de-
scription of critical objectives. For example, the beneﬁts of uti-
lizing distributed ledger technology (DLT) in data markets for
IoT architectures often remain unclear, and authors do not suf-
ﬁciently consider DLT’s lack of maturity and inherent privacy
challenges.

The IoT’s particular characteristics bring new challenges for
privacy enhancement, most notably, the consequences of a lack
of interoperability, computation and storage constraints, and the
privacy disparity across jurisdictions. We have also observed
the importance of ﬁrst determining the sovereignty layer in data
market design, as the participants’ ownership and management
rules impact the PETs in the rest of the layers. We also must
underline that there is no “one-size-ﬁts-all” PET. Only a com-
bination may tackle the various privacy challenges facing data
markets for the IoT. Lastly, we recommend that institutions in-
vest resources in the research and adoption of PETs to remain
competitive in the advent of a more privacy-enhancing IoT.

Acknowledgements

We would like to thank the Bayerisches Forschungsinstitut f¨ur
Digitale Transformation for supporting our research on dif-
ferential privacy, and the Bavarian Ministry of Economic Af-
fairs, Regional Development and Energy for their funding of
the project “Fraunhofer Blockchain Center (20-3066-2-6-14)”
that made this paper possible.

28

[S1] Y. N. Li, X. Feng, J. Xie, H. Feng, Z. Guan, Q. Wu, A decentralized
and secure blockchain platform for open fair data trading, Concurrency
Computation 32 (7) (2019) 1–11. doi:10.1002/cpe.5578.

[S2] N. Hynes, D. Dao, D. Yan, R. Cheng, D. Song, A demonstration of ster-
ling: A privacy-preserving data marketplace, Proceedings of the VLDB
Endowment 11 (12) (2018) 2086–2089. doi:10.14778/3229863.323
6266.

[S3] D. L´opez, B. Farooq, A multi-layered blockchain framework for smart
mobility data-markets, Transportation Research Part C: Emerging Tech-
nologies 111 (June 2019) (2020) 588–615. doi:10.1016/j.trc.2020
.01.002.

[S4] F. Liang, W. Yu, D. An, Q. Yang, X. Fu, W. Zhao, A Survey on Big Data
Market: Pricing, Trading and Protection, IEEE Access 6 (May) (2018)
15132–15154. doi:10.1109/ACCESS.2018.2806881.

[S5] D. Bogdanov, R. Jagom¨agis, S. Laur, A Universal Toolkit for Crypto-
graphically Secure Privacy-Preserving Data Mining, LNCS 7299 - Intel-
ligence and Security Informatics 7299 (2012).
URL https://link.springer.com/content/pdf/10.1007{%}2
F978-3-642-30428-6.pdf

[S6] S. Spiekermann, A. Novotny, A vision for global privacy bridges: Tech-
nical and legal measures for international data markets, Computer Law
and Security Review 31 (2) (2015) 181–200. doi:10.1016/j.clsr.2
015.01.009.
URL http://dx.doi.org/10.1016/j.clsr.2015.01.009

[S7] C. Niu, Z. Zheng, F. Wu, X. Gao, G. Chen, Achieving Data Truthfulness
and Privacy Preservation in Data Markets, IEEE Transactions on Knowl-
edge and Data Engineering 31 (1) (2019) 105–119. arXiv:1812.03280,
doi:10.1109/TKDE.2018.2822727.

[S8] E. M. Schomakers, C. Lidynia, M. Zieﬂe, All of me? Users’ preferences
for privacy-preserving data markets and the importance of anonymity,
Electronic Markets (2020). doi:10.1007/s12525-020-00404-9.
[S9] Y. Li, C. Miao, L. Su, J. Gao, Q. Li, B. Ding, Z. Qin, K. Ren, An eﬃcient
two-layer mechanism for privacy-preserving truth discovery, Proceedings
of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (2018) 1705–1714doi:10.1145/3219819.3219998.
[S10] L. Zhou, L. Wang, T. Ai, Y. Sun, BeeKeeper 2.0: Conﬁdential
blockchain-enabled IoT system with fully homomorphic computation,
Sensors (Switzerland) 18 (11) (2018). doi:10.3390/s18113785.
[S11] A. Dorri, S. S. Kanhere, R. Jurdak, P. Gauravaram, Blockchain for IoT
security and privacy: The case study of a smart home, 2017 IEEE Interna-
tional Conference on Pervasive Computing and Communications Work-
shops, PerCom Workshops 2017 (2017) 618–623doi:10.1109/PERCOM
W.2017.7917634.

[S12] R. Li, T. Song, B. Mei, H. Li, X. Cheng, L. Sun, Blockchain for Large-
Scale Internet of Things Data Storage and Protection, IEEE Transactions
on Services Computing 12 (5) (2019) 762–771. doi:10.1109/TSC.20
18.2853167.

[S13] J. Wei, M. Sabonuchi, R. Roche, Blockchain-enabled peer-to-peer data
trading mechanism, 2018 IEEE International Conference on Internet
of Things (iThings) and IEEE Green Computing and Communications
(GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom)
and IEEE Smart Data (SmartData) 1349–1354doi:10.1109/Cybermat
ics.

[S14] Z. Zheng, W. Mao, F. Wu, G. Chen, Challenges and opportunities in IoT
data markets, SocialSense 2019 - Proceedings of the 2019 4th Interna-
tional Workshop on Social Sensing (2019) 1–2doi:10.1145/3313294.
3313378.

[S15] M. M. Khalili, X. Zhang, M. Liu, Contract design for purchasing private
data using a biased diﬀerentially private algorithm, Proceedings of NetE-
con 2019: 14th Workshop on the Economics of Networks, Systems and
Computation - In conjunction with ACM EC 2019 and ACM SIGMET-
RICS 2019 (2019). doi:10.1145/3338506.3340273.

[S16] L. Yang, M. Zhang, S. He, M. Li, J. Zhang, Crowd-empowered privacy-
preserving data aggregation for mobile crowdsensing, Proceedings of the
International Symposium on Mobile Ad Hoc Networking and Computing
(MobiHoc) (2018) 151–160doi:10.1145/3209582.3209598.
[S17] K. Miˇsura, M. ˇZagar, Data marketplace for Internet of Things, Proceed-
ings of 2016 International Conference on Smart Systems and Technolo-
gies, SST 2016 (2016) 255–260doi:10.1109/SST.2016.7765669.

[S18] X. Zheng, Data trading with diﬀerential privacy in data market, ACM
International Conference Proceeding Series (8) (2020) 112–115. doi:
10.1145/3379247.3379271.

[S19] J. Pennekamp, M. Henze, S. Schmidt, P. Niemietz, M. Fey, D. Trauth,
T. Bergs, C. Brecher, K. Wehrle, Dataﬂow Challenges in an Internet of
Production, in: ACMWorkshop on Cyber-Physical Systems Security &
Privacy (CPS-SPC’19), November 11, 2019, London, United Kingdom.
ACM, 2019, pp. 27–38. doi:10.1145/3338499.3357357.

[S20] Z. J. Wang, C. H. V. Lin, Y. H. Yuan, C. C. J. Huang, Decentralized
Data Marketplace to Enable Trusted Machine Economy, 2019 IEEE Eura-
sia Conference on IOT, Communication and Engineering, ECICE 2019
(2019) 246–250doi:10.1109/ECICE47484.2019.8942729.

[S21] M. Guerriero, D. A. Tamburri, E. Di Nitto, Deﬁning, enforcing and
checking privacy policies in data-intensive applications, Proceedings -
International Conference on Software Engineering (2018) 172–182doi:
10.1145/3194133.3194140.

[S22] M. Shi, Y. Qiao, X. Wang, Diﬀerentially private auctions for private
data crowdsourcing, Proceedings - 2019 IEEE Intl Conf on Parallel and
Distributed Processing with Applications, Big Data and Cloud Com-
puting, Sustainable Computing and Communications, Social Computing
and Networking, ISPA/BDCloud/SustainCom/SocialCom 2019 (2019) 1–
8doi:10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2
019.00013.

[S23] J. Du, C. Jiang, E. Gelenbe, L. Xu, J. Li, Y. Ren, Distributed Data Pri-
vacy Preservation in IoT Applications, IEEE Wireless Communications
25 (December) (2018) 68–76. doi:10.1109/MWC.2017.1800094.
[S24] G. Gao, M. Xiao, J. Wu, S. Zhang, L. Huang, G. Xiao, DPDT: A Diﬀer-
entially Private Crowd-Sensed Data Trading Mechanism, IEEE Internet
of Things Journal 7 (1) (2020) 751–762. doi:10.1109/JIOT.2019.29
44107.

[S25] R. Cheng, F. Zhang, J. Kos, W. He, N. Hynes, N. Johnson, A. Juels,
A. Miller, D. Song, Ekiden: A platform for conﬁdentiality-preserving,
trustworthy, and performant smart contracts, Proceedings - 4th IEEE Eu-
ropean Symposium on Security and Privacy, EURO S and P 2019 (2019)
185–200doi:10.1109/EuroSP.2019.00023.

[S26] T. Jung, X. Y. Li, Enabling privacy-preserving auctions in big data, Pro-
ceedings - IEEE INFOCOM 2015-Augus (BigSecurity) (2015) 173–178.
arXiv:1308.6202, doi:10.1109/INFCOMW.2015.7179380.

[S27] C. Perera, R. Ranjan, L. Wang, End-to-end privacy for open big data
markets, IEEE Cloud Computing 2 (4) (2015) 44–53. doi:10.1109/MC
C.2015.78.

[S28] M. Zichichi, M. Contu, S. Ferretti, V. Rodr´ıguez-Doncel, Ensuring per-
sonal data anonymity in data marketplaces through sensing-as-a-service
and distributed ledger technologies, CEUR Workshop Proceedings 2580
(2020).
URL https://www.researchgate.net/publication/340183476
Ensuring Personal Data Anonymity in Data Marketplaces t
hrough Sensing-as-a-Service and Distributed Ledger
[S29] S. Duri, M. Gruteser, X. Liu, P. Moskowitz, R. Perez, M. Singh, J. M.
Tang, Framework for security and privacy in automotive telematics, Pro-
ceedings of the ACM International Workshop on Mobile Commerce
(2002) 25–32doi:10.1145/570709.570711.

[S30] P. Tzianos, G. Pipelidis, N. Tsiamitros, Hermes: An open and transpar-
ent marketplace for iot sensor data over distributed ledgers, ICBC 2019 -
IEEE International Conference on Blockchain and Cryptocurrency (2019)
167–170doi:10.1109/BLOC.2019.8751331.

[S31] K. Li, L. Tian, W. Li, G. Luo, Z. Cai, Incorporating social interaction
into three-party game towards privacy protection in IoT, Computer Net-
works 150 (2019) 90–101. doi:10.1016/j.comnet.2018.11.036.

[S32] Y. Zhao, Y. Yu, Y. Li, G. Han, X. Du, Machine learning based privacy-
preserving fair data trading in big data market, Information Sciences 478
(2019) 449–460. doi:10.1016/j.ins.2018.11.028.

[S33] S. Kiyomoto, M. S. Rahman, A. Basu, On Blockchain-Based
Anonymized Dataset Distribution Platform, 2017 IEEE 15th International
Conference on Software Engineering Research, Management and Appli-
cations (SERA) (2017) 85–92.
URL https://ieeexplore.ieee.org/document/7965711

[S34] Z. Chen, L. Chen, L. Huang, H. Zhong, On Privacy-Preserving Cloud
Auction, Proceedings of the IEEE Symposium on Reliable Distributed
Systems (2016) 279–288doi:10.1109/SRDS.2016.045.

[S35] L. Pournajaf, D. A. Garcia-Ulloa, L. Xiong, V. Sunderam, Participant
Privacy in Mobile Crowd Sensing Task Management, ACM SIGMOD
Record 44 (4) (2016) 23–34. doi:10.1145/2935694.2935700.
[S36] D. S´anchez, A. Viejo, Personalized privacy in open data sharing scenar-
ios, Online Information Review 41 (3) (2017) 298–310. doi:10.1108/
OIR-01-2016-0011.

[S37] K. Jung, S. Park, Privacy Bargaining with Fairness: Privacy-Price Nego-
tiation System for Applying Diﬀerential Privacy in Data Market Environ-
ments, 2019 IEEE International Conference on Big Data (2019) 1389–
1394doi:10.1109/BigData47090.2019.9006101.

[S38] J. H. Ziegeldorf, O. G. Morchon, K. Wehrle, Privacy in the internet of
things: Threats and challenges, Security and Communication Networks
7 (12) (2014) 2728–2742. doi:10.1002/sec.795.

[S39] C. Perera, C. McCormick, A. K. Bandara, B. A. Price, B. Nuseibeh,
Privacy-by-design framework for assessing internet of things applications
and platforms, ACM International Conference Proceeding Series 07-09-
Nove (2016) 83–92. doi:10.1145/2991561.2991566.

[S40] C. Perera, C. Liu, R. Ranjan, L. Wang, A. Zomaya, Privacy-knowledge
Modeling for the Internet of things: A look back, Computer 49 (12)
(2016) 60–68. doi:10.1109/MC.2016.366.

[S41] W. Gao, W. Yu, F. Liang, W. G. Hatcher, C. Lu, Privacy-Preserving
Auction for Big Data Trading Using Homomorphic Encryption, IEEE
Transactions on Network Science and Engineering 7 (2) (2020) 776–791.
doi:10.1109/TNSE.2018.2846736.

[S42] S. Park, K. Park, J. Lee, K. Jung, PRIVATA: Diﬀerentially private Data
market framework using Negotiation-based Pricing mechanism, Proceed-
ings of ACM CIKM conference (CIKM’19), November 3–7, 2019, Bei-
jing, China. (2019) 156–157doi:10.1007/978-3-663-10915-0 47.
[S43] V. Koutsos, D. Papadopoulos, D. Chatzopoulos, S. Tarkoma, P. Hui,

Agora: A Privacy-Aware Data Marketplace (2020) 13.
URL https://eprint.iacr.org/2020/865.pdf

[S44] J. Cao, P. Karras, Publishing microdata with a robust privacy guarantee,
Proceedings of the VLDB Endowment 5 (11) (2012) 1388–1399. arXiv:
1208.0220, doi:10.14778/2350229.2350255.

[S45] W. Dai, C. Dai, K. K. R. Choo, C. Cui, D. Zou, H. Jin, SDTE: A Secure
Blockchain-Based Data Trading Ecosystem, IEEE Transactions on Infor-
mation Forensics and Security 15 (2020) 725–737. doi:10.1109/TIFS
.2019.2928256.

[S46] Z. Guan, X. Shao, Z. Wan, Secure, Fair and Eﬃcient Data Trading
without Third Party Using Blockchain, 2018 IEEE International Con-
ference on Internet of Things (iThings) and IEEE Green Computing
and Communications (GreenCom) and IEEE Cyber, Physical and Social
Computing (CPSCom) and IEEE Smart Data (SmartData) (2018) 1349–
1354doi:10.1109/Cybermatics.

[S47] M. A. Alsheikh, Y. Jiao, D. Niyato, P. Wang, D. Leong, Z. Han,
The Accuracy-Privacy Trade-oﬀ of Mobile Crowdsensing, IEEE Com-
munications Magazine 55 (6) (2017) 132–139. arXiv:1702.04565,
doi:10.1109/MCOM.2017.1600737.

[S48] S. Sharma, K. Chen, A. Sheth, Toward practical privacy-preserving an-
alytics for IoT and cloud-based healthcare systems, IEEE Internet Com-
puting 22 (2) (2018) 42–51. doi:10.1109/MIC.2018.112102519.
[S49] A. Colman, M. J. M. Chowdhury, M. Baruwal Chhetri, Towards a trusted
marketplace for wearable data, Proceedings - 2019 IEEE 5th International
Conference on Collaboration and Internet Computing, CIC 2019 (Cic)
(2019) 314–321. doi:10.1109/CIC48465.2019.00044.

[S50] Z. Cai, Z. He, Trading private range counting over big IoT data, Pro-
ceedings - International Conference on Distributed Computing Systems
2019-July (2019) 144–153. doi:10.1109/ICDCS.2019.00023.

References

[1] IDC, Open Evidence, European data market smart (feb 2017).

URL https://ec.europa.eu/newsroom/dae/document.cfm?d
oc id=44400

[2] A. R. Miller, C. Tucker, Health Information Exchange, System Size and

Information Silos (2013) 29.
URL https://papers.ssrn.com/sol3/papers.cfm?abstract i
d=1457719

29

[3] F. Stahl, F. Schomm, G. Vossen, L. Vomfell, A classiﬁcation frame-
work for data marketplaces, Vietnam Journal of Computer Science 3 (3)
(2016) 137–143. doi:10.1007/s40595-016-0064-2.

[4] McKinsey & Company, Four ways to accelerate the creation of data
ecosystems, https://www.mckinsey.com/business-functions/
mckinsey-analytics/our-insights/four-ways-to-acceler
ate-the-creation-of-data-ecosystems (November 2020).
[5] G. Eggers, B. Fondermann, B. Maier, K. Ottradovetz, J. Pformmer,
R. Reinhardt, H. Rollin, A. Schmieg, S. Steinbuß, P. Trinius, A. Weis,
C. Weiss, S. Wilﬂing, GAIA-X: Technical Architecture (2020).
URL https://www.data-infrastructure.eu/GAIAX/Redakti
on/EN/Publications/gaia- x- technical- architecture.p
df? blob=publicationFile&v=5

[6] L. Sweeney, A. Abu, J. Winn, Identifying Participants in the Personal
Genome Project by Name, SSRN Electronic Journal (2013). doi:10.2
139/ssrn.2257732.

[7] X. Gao, B. Firner, S. Sugrim, V. Kaiser-Pendergrast, Y. Yang,
J. Lindqvist, Elastic pathing: your speed is enough to track you, in: Pro-
ceedings of the 2014 ACM International Joint Conference on Pervasive
and Ubiquitous Computing - UbiComp ’14 Adjunct, ACM Press, Seat-
tle, Washington, 2014, pp. 975–986. doi:10.1145/2632048.263207
7.

[8] A. Sunyaev, N. Kannengießer, R. Beck, H. Treiblmaier, M. Lacity,
J. Kranz, G. Fridgen, U. Spankowski, A. Luckow, Token economy, Busi-
ness & Information Systems Engineering (2021).
URL https://link.springer.com/article/10.1007/s12599
-021-00684-1

[9] IBM Security and Ponemon Institute LLC, 2018 cost of a data breach

study: Global overview (Jul 2018).
URL https://www.intlxsolutions.com/hubfs/2018 Global C
ost of a Data Breach Report.pdf

[10] A. Trask, E. Bluemke, B. Garﬁnkel, C. G. Cuervas-Mons, A. Dafoe,
Beyond privacy trade-oﬀs with structured transparency (2020). arXiv:
2012.08347.
URL https://www.researchgate.net/publication/3473008
76 Beyond Privacy Trade-offs with Structured Transpare
ncy

[11] R. Hes, J. J. Borking, Netherlands, I. a. P. Commissioner/Ontario (Eds.),
Privacy-enhancing technologies: the path to anonymity, rev. ed Edition,
no. 11 in Achtergrondstudies en verkenningen, Registratiekamer, The
Hague, 1998.
URL https://www.researchgate.net/publication/2437776
45 Privacy-Enhancing Technologies The Path to Anonymity
[12] R. Oppliger, Privacy-enhancing technologies for the world wide web,
Computer Communications 28 (16) (2005) 1791–1797. doi:10.1016/
j.comcom.2005.02.003.

[13] C. Dwork, F. McSherry, K. Nissim, A. Smith, Calibrating noise to sen-
sitivity in private data analysis, in: S. Halevi, T. Rabin (Eds.), Theory
of Cryptography, Springer Berlin Heidelberg, Berlin, Heidelberg, 2006,
pp. 265–284, online; accessed 30 December 2021.
URL https://link.springer.com/chapter/10.1007/116818
78 14

[14] C. Dwork, A. Roth, The Algorithmic Foundations of Diﬀerential Pri-
vacy, Foundations and Trends® in Theoretical Computer Science 9 (3-
4) (2013) 211–407. doi:10.1561/0400000042.

[15] P. Samarati, L. Sweeney, Protecting Privacy when Disclosing Infor-
mation: k-Anonymity and Its Enforcement through Generalization and
Suppression 19.
URL https://epic.org/privacy/reidentification/Samara
ti Sweeney paper.pdf

[16] M. A. Will, R. K. Ko, A guide to homomorphic encryption, Elsevier

Inc., 2015. doi:10.1016/B978-0-12-801595-7.00005-7.

[17] P. Chaudhary, R. Gupta, A. Singh, P. Majumder, Analysis and Compari-
son of Various Fully Homomorphic Encryption Techniques, 2019 Inter-
national Conference on Computing, Power and Communication Tech-
nologies, GUCON 2019 (2019) 58–62.
URL https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&a
rnumber=8940577

[18] P. Paillier, Public-key cryptosystems based on composite degree residu-

osity classes, Eurocrypt (1999). doi:10.1007/3-540-48910-X 9.

[19] OMTP, Advanced trusted environment: Omtp tr1 (May 2009).

URL http://www.gsma.com/newsroom/wp-content/uploads/2
012/03/omtpadvancedtrustedenvironmentomtptr1v11.pdf
[20] A. C. Yao, Protocols for secure computations, in: 23rd Annual Sympo-
sium on Foundations of Computer Science (sfcs 1982), 1982, pp. 160–
164. doi:10.1109/SFCS.1982.38.

[21] S. Goldwasser, S. Micali, C. Rackoﬀ, The knowledge complexity of
interactive proof systems, SIAM J. Comput. 18 (1) (1989) 186–208.
doi:10.1137/0218012.

[22] O. Goldreich, Y. Oren, Deﬁnitions and properties of zero-knowledge
proof systems, Journal of Cryptology 7 (1) (1994) 1–32. doi:10.100
7/BF00195207.

[23] G. Bondel, G. M. Garrido, K. Baumer, F. Matthes, Towards a Privacy-

Enhancing Tool Based on De- Identiﬁcation Methods 8.
URL https://aisel.aisnet.org/pacis2020/157/

[24] S. Spiekermann, R. B¨ohme, A. Acquisti, K.-L. Hui, Personal data mar-
kets, Electronic Markets 25 (2) (2015) 91–93. doi:10.1007/s12525
-015-0190-1.

[25] P. B. Anne Z¨oll, Christian M. Olt, Privacy-sensitive Business Models:
Barriers of Organizational Adoption of Privacy-Enhancing Technologies
(2021) 22.
URL https://aisel.aisnet.org/ecis2021 rp/34/

[26] A. F. Westin, Privacy and Freedom, IG Publishing, New York, 1967.

URL https://scholarlycommons.law.wlu.edu/wlulr/vol25
/iss1/20/

[27] G. A. Fink, H. Song, S. Jeschke (Eds.), Security and privacy in cyber-
physical systems: Foundations, principles, and applications, ﬁrst edition
Edition, Wiley IEEE Press, Hoboken, NJ, 2018.
URL https://ieeexplore.ieee.org/servlet/opac?bknumbe
r=8068866

[28] K. Renaud, D. Galvez-Cruz, Privacy: Aspects, deﬁnitions and a multi-
faceted privacy preservation approach, 2010, pp. 1–8. doi:10.1109/
ISSA.2010.5588297.

[29] D. J. Solove, The meaning and value of privacy,

in: B. Roessler,
D. Mokrosinska (Eds.), Social Dimensions of Privacy, Cambridge Uni-
versity Press, Cambridge, 2015, pp. 71–82. doi:10.1017/CBO97811
07280557.005.

[30] F. T. Wu, Deﬁning privacy and utility in data sets, 84 University of Col-
orado Law Review 1117 (2013); 2012 TRPC (2012) 1117–1177doi:
10.2139/ssrn.2031808.

[31] M. Deng, K. Wuyts, R. Scandariato, B. Preneel, W. Joosen, A privacy
threat analysis framework: supporting the elicitation and fulﬁllment of
privacy requirements, Requirements Engineering 16 (1) (2011) 3–32.
doi:10.1007/s00766-010-0115-7.

[32] R. Garratt, M. R. v. Oordt, Privacy as a public good: A case for electronic
cash, Journal of Political Economy (2018). doi:10.1086/714133.
[33] N. Kaaniche, M. Laurent, Attribute-based signatures for supporting
anonymous certiﬁcation, in: I. Askoxylakis, S. Ioannidis, S. Katsikas,
C. Meadows (Eds.), Computer Security – ESORICS 2016, Springer In-
ternational Publishing, Cham, 2016, pp. 279–300.
URL https://www.semanticscholar.org/paper/Attribute-
Based-Signatures-for-Supporting-Anonymous-Kaaniche-L
aurent-Maknavicius/3b0624ff32b9258ca2351c894d320d83a
546fcd6

[34] J. E. Campbell, M. Carlson, Panopticon.com: Online surveillance and
the commodiﬁcation of privacy, Journal of Broadcasting & Electronic
Media 46 (4) (2002) 586–606. doi:10.1207/s15506878jobem4604
\ 6.

[35] A. Lichter, M. L¨oﬄer, S. Siegloch, The Long-Term Costs of Govern-
ment Surveillance: Insights from Stasi Spying in East Germany, Jour-
nal of the European Economic Association 19 (2) (2020) 741–789.
arXiv:https://academic.oup.com/jeea/article- pdf/19
/2/741/37108669/jvaa009.pdf, doi:10.1093/jeea/jvaa009.
URL https://doi.org/10.1093/jeea/jvaa009

[36] S. Kokolakis, Privacy attitudes and privacy behaviour: A review of cur-
rent research on the privacy paradox phenomenon, Computers & Secu-
rity 64 (2017) 122–134. doi:https://doi.org/10.1016/j.cose
.2015.07.002.

[37] J. Coppel, E-Commerce: Impacts and Policy Challenges, OECD Eco-
nomics Department Working Papers 252, series: OECD Economics De-
partment Working Papers Volume: 252 (Jun. 2000). doi:10.1787/80

30

1315684632.

[38] J. Kennedy, Big data’s economic impact,

https://www.ced.org/blog/entry/big-datas-economic-impact,
on 04 Jul. 2021].

[Online]. Available:
[Accessed

[39] A. M. Oberl¨ander, M. R¨oglinger, M. Rosemann, A. Kees, Conceptu-
alizing business-to-thing interactions – a sociomaterial perspective on
the internet of things, European Journal of Information Systems 27 (4)
(2018) 486–502. doi:10.1080/0960085X.2017.1387714.

[40] I. Lee, K. Lee, The Internet of Things (IoT): Applications, investments,
and challenges for enterprises, Business Horizons 58 (4) (2015) 431–
440. doi:10.1016/j.bushor.2015.03.008.

[41] V. Basili, G. Caldiera, D. Rombach, The goal question metric approach,

Encyclopedia of Software Engineering (1994) 528–532.
URL http://www.cs.toronto.edu/~sme/CSC444F/handouts/
GQM-paper.pdf

[42] B. A. Kitchenham, D. Budgen, Evidence-based software engineering

and systematic reviews, Chapman and Hall/CRC, 2015.
URL https://dl.acm.org/doi/book/10.5555/2994449

[43] B. Kitchenham, Procedures for Performing Systematic Reviews, Joint
Technical Report (2004). doi:10.5144/0256-4947.2017.79.
[44] D. C. B. Mariano, C. Leite, L. H. S. Santos, R. E. O. Rocha, R. C.
de Melo-Minardi, A guide to performing systematic literature reviews
in bioinformatics (2017). arXiv:1707.05813.

[45] T. Dybå, T. Dingsøyr, G. Hanssen, Applying Systematic Reviews to
Diverse Study Types: An Experience Report, Proceedings - 1st Inter-
national Symposium on Empirical Software Engineering and Measure-
ment, ESEM 2007 (7465) (2007) 126–135. doi:10.1109/ESEM.200
7.59.

[46] O. Dieste, A. Grim´an, N. Juristo, Developing search strategies for detect-
ing relevant experiments, Empirical Software Engineering 14 (5) (2009)
513–539. doi:10.1007/s10664-008-9091-7.

[47] H. Zhang, M. A. Babar, P. Tell, Identifying relevant studies in software
engineering, Information and Software Technology 53 (6) (2011) 625–
637. doi:10.1016/j.infsof.2010.12.010.

[48] A. Kilgarriﬀ, V. Baisa, J. Buˇsta, M. Jakub´ıˇcek, V. Kov´aˇr, J. Michelfeit,
P. Rychl´y, V. Suchomel, The Sketch Engine: ten years on, Lexicography
(2014).
URL https://www.researchgate.net/publication/2718480
17 The Sketch Engine Ten Years On

[49] O. P. Brereton, B. A. Kitchenham, D. Budgen, M. Turner, M. Khalil,
Lessons from applying the systematic literature review process within
the software engineering domain, Journal of Systems and Software
80 (4) (2007) 571–583. doi:10.1016/j.jss.2006.07.009.

[50] B. A. Kitchenham, O. P. Brereton, A systematic review of systematic
review process research in software engineering, Information and Soft-
ware Technology 55 (12) (2013) 2049–2075. doi:10.1016/j.infs
of.2013.07.010.

[51] L. Chen, M. A. Babar, H. Zhang, Towards an Evidence-Based Un-
derstanding of Electronic Data Sources (January 2015) (2010). doi:
10.14236/ewic/ease2010.17.

[52] C. Wohlin, P. Runeson, M. H¨ost, M. C. Ohlsson, B. Regnell, A. Wessl´en,
Experimentation in software engineering, Springer Science & Business
Media, 2012. doi:10.1007/978-3-642-29044-2.

[53] H. Nissenbaum, Privacy in Context: Technology, Policy, and the In-

tegrity of Social Life, Stanford University Press, 2009.
URL https://www.sup.org/books/title/?id=8862

[54] R. Y. Wang, D. M. Strong, Beyond Accuracy: What Data Quality Means
to Data Consumers, Journal of Management Information Systems 12 (4)
(1996) 5–33. doi:10.1080/07421222.1996.11518099.

[55] T. Dinev, H. Xu, J. H. Smith, P. Hart, Information privacy and correlates:
an empirical attempt to bridge and distinguish privacy-related concepts,
European Journal of Information Systems 22 (3) (2013) 295–316. doi:
10.1057/ejis.2012.23.

[56] B.-J. Butijn, D. A. Tamburri, W.-J. v. d. Heuvel, Blockchains: a sys-
tematic multivocal literature review, ACM Computing Surveys (CSUR)
53 (3) (2020) 1–37.
URL https://dl.acm.org/doi/abs/10.1145/3369052

[57] R. Zhang, R. Xue, L. Liu, Security and privacy on blockchain, ACM

Computing Surveys (CSUR) 52 (3) (2019) 1–34.
URL https://dl.acm.org/doi/10.1145/3316481

[58] G. I. Simari, A Primer on Zero Knowledge Protocols (2002) 12.

URL http://cs.uns.edu.ar/~gis/publications/zkp-simari
2002.pdf

[59] N. Kaaniche, M. Laurent, S. Belguith, Privacy enhancing technologies
for solving the privacy-personalization paradox: Taxonomy and survey,
Journal of Network and Computer Applications (2020).

[60] D. Chaum, Security without identiﬁcation: transaction systems to make
big brother obsolete, Communications of the ACM 28 (10) (1985) 1030–
1044. doi:10.1145/4372.4373.

[61] J. L. Camenisch, J.-M. Piveteau, M. A. Stadler, Blind signatures based
on the discrete logarithm problem, in: A. De Santis (Ed.), Advances
in Cryptology — EUROCRYPT’94, Springer Berlin Heidelberg, Berlin,
Heidelberg, 1995, pp. 428–432.
URL https://link.springer.com/chapter/10.1007/BFb005
3458

[62] J. Camenisch, A. Lysyanskaya, Dynamic Accumulators and Application
to Eﬃcient Revocation of Anonymous Credentials, in: G. Goos, J. Hart-
manis, J. van Leeuwen, M. Yung (Eds.), Advances in Cryptology —
CRYPTO 2002, Vol. 2442, Springer Berlin Heidelberg, Berlin, Heidel-
berg, 2002, pp. 61–76, series Title: Lecture Notes in Computer Science.
doi:10.1007/3-540-45708-9 5.

[63] J. Camenisch, T. Groß, Eﬃcient Attributes for Anonymous Credentials

(2010) 29.
URL https://eprint.iacr.org/2010/496.pdf

[64] S. A. Brands, Rethinking Public Key Infrastructures and Digital Certiﬁ-
cates: Building in Privacy, MIT Press, Cambridge, MA, USA, 2000.
URL https://direct.mit.edu/books/book/1912/Rethinkin
g-Public-Key-Infrastructures-and-Digital

[65] J. Sedlmeir, R. Smethurst, A. Rieger, G. Fridgen, Digital identities
and veriﬁable credentials, Business & Information Systems Engineer-
ing 63 (5) (2021) 603–613.

[66] V. Schlatt, J. Sedlmeir, S. Feulner, N. Urbach, Designing a framework
for digital KYC processes built on blockchain-based self-sovereign iden-
tity, Information & Management (2021) 103553.

[67] E. Bangerter, S. Barzan, S. Krenn, A.-R. Sadeghi, T. Schneider, J.-
K. Tsay, Bringing Zero-Knowledge Proofs of Knowledge to Practice
(2009) 12.
URL https://eprint.iacr.org/2009/211.pdf

[68] M. Hoﬀmann, M. Klooß, A. Rupp, Eﬃcient Zero-Knowledge Argu-
ments in the Discrete Log Setting, Revisited, in: Proceedings of the
2019 ACM SIGSAC Conference on Computer and Communications Se-
curity, ACM, London United Kingdom, 2019, pp. 2093–2110. doi:
10.1145/3319535.3354251.

[69] T. Nakanishi, H. Yoshino, T. Murakami, G.-V. Policharla, Eﬃcient Zero-
Knowledge Proofs of Graph Signature for Connectivity and Isolation
Using Bilinear-Map Accumulator, in: Proceedings of the 7th ACM
Workshop on ASIA Public-Key Cryptography, ACM, Taipei Taiwan,
2020, pp. 9–18. doi:10.1145/3384940.3388959.

[70] Y. Zhang, Zero-knowledge proofs for machine learning, in: Proceedings
of the 2020 Workshop on Privacy-Preserving Machine Learning in Prac-
tice, Association for Computing Machinery, 2020, p. 7.
URL https://doi.org/10.1145/3411501.3418608

[71] M. Stadler, Publicly veriﬁable secret sharing, Lecture Notes in Computer
Science (including subseries Lecture Notes in Artiﬁcial Intelligence and
Lecture Notes in Bioinformatics) 1070 (1996) 190–199. doi:10.100
7/3-540-68339-9 17.

[72] A. Shamir, How to share a secret, Commun. ACM 22 (11) (1979)

612–613. doi:10.1145/359168.359176.

[73] T. P. Pedersen, Non-interactive and information-theoretic secure ver-
iﬁable secret sharing, Lecture Notes in Computer Science (including
subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in
Bioinformatics) 576 LNCS (1992) 129–140. doi:10.1007/3-540-
46766-1 9.

[74] A. Shamir, How to share a secret, Publications of the ACM (1979). doi:

10.1007/978-3-642-15328-0 17.

[75] A. C. Yao, Protocols for secure computations, in: 23rd Annual Sympo-
sium on Foundations of Computer Science (sfcs 1982), IEEE, Chicago,
IL, USA, 1982, pp. 160–164. doi:10.1109/SFCS.1982.38.

[76] Y. Lindell, B. Pinkas, A Proof of Security of Yao’s Protocol for Two-
Party Computation, Journal of Cryptology 22 (2) (2009) 161–188. doi:
10.1007/s00145-008-9036-8.

31

[77] A. Ben-David, N. Nisan, B. Pinkas, FairplayMP: a system for secure
multi-party computation, in: Proceedings of the 15th ACM conference
on Computer and communications security - CCS ’08, ACM Press,
Alexandria, Virginia, USA, 2008, p. 257. doi:10.1145/1455770.
1455804.

[78] S. Yakoubov, A Gentle Introduction to Yao’ s Garbled Circuits (2017).
URL https://web.mit.edu/sonka89/www/papers/2017ygc.p
df

[79] Z. A. Genc¸, V. Iovino, A. Rial, The simplest protocol for oblivious trans-
fer, Information Processing Letters 161 (2020) 1–12. doi:10.1016/
j.ipl.2020.105975.

[80] P. Pullonen, S. Siim, Combining Secret Sharing and Garbled Circuits for
Eﬃcient Private IEEE 754 Floating-Point Computations, in: M. Bren-
ner, N. Christin, B. Johnson, K. Rohloﬀ (Eds.), Financial Cryptography
and Data Security, Vol. 8976, Springer Berlin Heidelberg, Berlin, Hei-
delberg, 2015, pp. 172–183, series Title: Lecture Notes in Computer
Science. doi:10.1007/978-3-662-48051-9 13.

[81] Y. Yang, X. Huang, X. Liu, H. Cheng, J. Weng, X. Luo, V. Chang, A
Comprehensive Survey on Secure Outsourced Computation and Its Ap-
plications, IEEE Access 7 (2019) 159426–159465.
URL https://ieeexplore.ieee.org/document/8884162/
[82] E. Boyle, N. Gilboa, Y. Ishai, A. Nof, Practical Fully Secure Three-
Party Computation via Sublinear Distributed Zero-Knowledge Proofs,
in: Proceedings of the 2019 ACM SIGSAC Conference on Computer
and Communications Security, ACM, London United Kingdom, 2019,
pp. 869–886.
URL https://dl.acm.org/doi/10.1145/3319535.3363227
[83] X. Chen, Introduction to Secure Outsourcing Computation, Morgan &

Claypool publishers, 2016.
URL https://www.researchgate.net/publication/2956814
72 Introduction to Secure Outsourcing Computation

[84] D. Boneh, E.-J. Goh, K. Nissim, Evaluating 2-dnf formulas on cipher-

texts, Vol. 3378, 2005, pp. 325–341.
URL https://www.researchgate.net/publication/2213541
38 Evaluating 2-DNF Formulas on Ciphertexts

[85] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Taft, D. Boneh,
Privacy-preserving matrix factorization, in: Proceedings of the 2013
ACM SIGSAC Conference on Computer & Communications Security,
Association for Computing Machinery, 2013, p. 801–812.
URL https://doi.org/10.1145/2508859.2516751

[86] L. Zhou, L. Wang, Y. Sun, T. Ai, AntNest: Fully Non-Interactive Secure
Multi-Party Computation, IEEE Access 6 (2018) 75639–75649.
URL https://ieeexplore.ieee.org/document/8550709/
[87] D. Boneh, A. Sahai, B. Waters, Functional Encryption: Deﬁnitions and
Challenges, in: Y. Ishai (Ed.), Theory of Cryptography, Springer Berlin
Heidelberg, Berlin, Heidelberg, 2011, pp. 253–273.
URL https://eprint.iacr.org/2010/543.pdf

[88] J. Chotard, E. Dufour Sans, R. Gay, D. H. Phan, D. Pointcheval, De-
centralized Multi-Client Functional Encryption for Inner Product, in:
T. Peyrin, S. Galbraith (Eds.), Advances in Cryptology – ASIACRYPT
2018, Springer International Publishing, Cham, 2018, pp. 703–732.
URL https://eprint.iacr.org/2017/989.pdf

[89] Z. Brakerski, C. Gentry, V. Vaikuntanathan, (leveled) fully homomor-
phic encryption without bootstrapping, ACM Trans. Comput. Theory
6 (3) (Jul. 2014). doi:10.1145/2633600.

[90] W. Wang, Y. Hu, L. Chen, X. Huang, B. Sunar, Exploring the Feasibility
of Fully Homomorphic Encryption, IEEE Transactions on Computers
64 (3) (2015) 698–706. doi:10.1109/TC.2013.154.

[91] I. Anati, S. Gueron, S. P. Johnson, V. R. Scarlata, Innovative Technology

for CPU Based Attestation and Sealing (2013) 7.
URL https://software.intel.com/content/dam/develop/e
xternal/us/en/documents/hasp-2013-innovative-technol
ogy-for-attestation-and-sealing-413939.pdf

[92] F. Khalid, A. Masood, Vulnerability analysis of qualcomm secure exe-
cution environment, Computers & Security 116 (2022) 102628. doi:
https://doi.org/10.1016/j.cose.2022.102628.
URL https://www.sciencedirect.com/science/article/pi
i/S016740482200027X

[93] F. Alder, J. Van Bulck, J. Spielman, D. Oswald, F. Piessens, Faulty point
unit: Abi poisoning attacks on trusted execution environments, Digital
Threats: Research and Practice 3 (2) (2022). doi:10.1145/3491264.

URL https://doi-org.eaccess.ub.tum.de/10.1145/3491264
[94] D. Skarlatos, M. Yan, B. Gopireddy, R. Sprabery, J. Torrellas, C. W.
Fletcher, Microscope: Enabling microarchitectural replay attacks, in:
Proceedings of the 46th International Symposium on Computer Archi-
tecture, ACM, 2019, p. 318–331. doi:10.1145/3307650.3322228.
URL https://doi.org/10.1145/3307650.3322228

[95] Intel, 12th generation intel core processors (2022).

URL https://cdrdv2.intel.com/v1/dl/getContent/655258

[96] V. Costan, I. Lebedev, S. Devadas, Sanctum: Minimal Hardware Exten-

sions for Strong Software Isolation (2016) 19.
URL https://www.usenix.org/conference/usenixsecurity
16/technical-sessions/presentation/costan

[97] D. Lee, D. Kohlbrenner, S. Shinde, K. Asanovi´c, D. Song, Keystone:
an open framework for architecting trusted execution environments, in:
Proceedings of the Fifteenth European Conference on Computer Sys-
tems, ACM, Heraklion Greece, 2020, pp. 1–16. doi:10.1145/334219
5.3387532.

[98] AWS, Nitro System, 2022, online; accessed 4 May 2022.
URL https://aws.amazon.com/ec2/nitro/

[99] W. Wei, L. Liu, M. Loper, K.-H. Chow, M. E. Gursoy, S. Truex, Y. Wu, A
framework for evaluating gradient leakage attacks in federated learning
(2020). arXiv:2004.10397.
URL https://www.semanticscholar.org/paper/A-Framewor
k-for-Evaluating-Gradient-Leakage-Attacks-Wei-Liu/98
53a348f61aec83b410f307ab905a4ae001fcd4

[100] S. Pinto, N. Santos, Demystifying ARM TrustZone: A comprehensive

survey, ACM Computing Surveys (CSUR) 51 (6) (2019).

[101] V. Costan, L. F. Sarmenta, M. v. Dijk, S. Devadas, The trusted execution
module: Commodity general-purpose trusted computing, in: Interna-
tional Conference on Smart Card Research and Advanced Applications,
Springer, 2008, pp. 133–148.

[102] J. Koneˇcn´y, B. McMahan, D. Ramage, Federated optimiza-
tion:distributed optimization beyond the datacenter (2015). arXiv:
1511.03575.
URL https://docplayer.net/15450695-Federated-optimiz
ation-distributed-optimization-beyond-the-datacenter
.html

[103] T. Li, A. K. Sahu, A. Talwalkar, V. Smith, Federated learning: Chal-
lenges, methods, and future directions, IEEE Signal Processing Maga-
zine 37 (3) (2020) 50–60. doi:10.1109/MSP.2020.2975749.
[104] Q. Yang, Y. Liu, Y. Cheng, Y. Kang, T. Chen, H. Yu, Federated Learn-
ing, Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning
13 (3) (2019) 1–207. doi:10.2200/S00960ED2V01Y201910AIM043.
[105] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan,
S. Patel, D. Ramage, A. Segal, K. Seth, Practical secure aggregation for
privacy-preserving machine learning, in: Proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security, CCS
’17, Association for Computing Machinery, New York, NY, USA, 2017,
p. 1175–1191. doi:10.1145/3133956.3133982.
URL https://doi.org/10.1145/3133956.3133982

[106] S. Zhang, Z. Li, Q. Chen, W. Zheng, J. Leng, M. Guo, Dubhe: Towards
data unbiasedness with homomorphic encryption in federated learning
client selection, in: 50th International Conference on Parallel Process-
ing, ICPP 2021, Association for Computing Machinery, New York, NY,
USA, 2021. doi:10.1145/3472456.3473513.
URL https://doi-org.eaccess.ub.tum.de/10.1145/347245
6.3473513

[107] W. Yang, B. Liu, C. Lu, N. Yu, Privacy preserving on updated parame-
ters in federated learning, ACM TURC’20, Association for Computing
Machinery, New York, NY, USA, 2020, p. 27–31. doi:10.1145/3393
527.3393533.
URL https://doi-org.eaccess.ub.tum.de/10.1145/339352
7.3393533

[108] P. Vepakomma, O. Gupta, T. Swedish, R. Raskar, Split learning for
health: Distributed deep learning without sharing raw patient data
(2018).
URL https://aiforsocialgood.github.io/iclr2019/accep
ted/track1/pdfs/31 aisg iclr2019.pdf

[109] O. Gupta, R. Raskar, Distributed learning of deep neural network over
multiple agents, Journal of Network and Computer Applications 116
(2018) 1–8. doi:https://doi.org/10.1016/j.jnca.2018.

32

05.003.

[110] M. G. Poirot, P. Vepakomma, K. Chang, J. Kalpathy-Cramer, R. Gupta,
R. Raskar, Split Learning for collaborative deep learning in healthcare
9.
URL https://www.researchgate.net/publication/3382283
19 Split Learning for collaborative deep learning in hea
lthcare

[111] L. Giaretta, S. Girdzijauskas, Gossip learning: Oﬀ the beaten path, in:
2019 IEEE International Conference on Big Data (Big Data), 2019, pp.
1117–1124. doi:10.1109/BigData47090.2019.9006216.
[112] R. Orm´andi, I. Heged˝us, M. Jelasity, Gossip learning with linear models
on fully distributed data: EFFICIENT p2p ENSEMBLE LEARNING
WITH LINEAR MODELS ON FULLY DISTRIBUTED DATA 25 (4)
556–571. doi:10.1002/cpe.2858.
URL https://onlinelibrary.wiley.com/doi/10.1002/cpe.
2858

[113] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal-
war, L. Zhang, Deep learning with diﬀerential privacy, Proceedings of
the 2016 ACM SIGSAC Conference on Computer and Communications
Security (Oct 2016). doi:10.1145/2976749.2978318.

[114] S. De Capitani Di Vimercati, S. Foresti, G. Livraga, P. Samarati, Data
privacy: Deﬁnitions and techniques, International Journal of Uncer-
tainty, Fuzziness and Knowlege-Based Systems 20 (6) (2012) 793–817.
doi:10.1142/S0218488512400247.
URL https://www.worldscientific.com/doi/abs/10.1142/
S0218488512400247

[115] J. Domingo-Ferrer, D. S´anchez, A. Blanco-Justicia, The limits of dif-
ferential privacy (and its misuse in data release and machine learning),
Communications of the ACM 64 (7) (2021) 33–35. doi:10.1145/34
33638.
URL https://doi-org.eaccess.ub.tum.de/10.1145/3433638
[116] A. Meyerson, R. Williams, On the complexity of optimal k-anonymity,
Proceedings of the ACM SIGACT-SIGMOD-SIGART Symposium on
Principles of Database Systems 23 (2004) 223–228. doi:10.1145/10
55558.1055591.
URL https://dl.acm.org/doi/10.1145/1055558.1055591
[117] E. Dikici, L. M. Prevedello, M. Bigelow, R. D. White, B. S. Erdal, Con-
strained generative adversarial network ensembles for sharable synthetic
data generation (2020). arXiv:2003.00086.
URL https://www.researchgate.net/publication/3396423
58 Constrained Generative Adversarial Network Ensembles
for Sharable Synthetic Data Generation

[118] A. Torﬁ, E. A. Fox, C. K. Reddy, Diﬀerentially private synthetic medical

data generation using convolutional gans (2020).
URL https://www.researchgate.net/publication/3476246
71 Differentially Private Synthetic Medical Data Gener
ation using Convolutional GANs

[119] Privacy and anonymity in information management systems. doi:10.1

007/978-1-84996-238-4.
URL http://link.springer.com/10.1007/978-1-84996-238-
4

[120] V. Puri, S. Sachdeva, P. Kaur, Privacy preserving publication of rela-
tional and transaction data: Survey on the anonymization of patient data,
Comput. Sci. Rev. 32 (C) (2019) 45–61. doi:10.1016/j.cosrev.2
019.02.001.
URL https://doi.org/10.1016/j.cosrev.2019.02.001
[121] B. C. M. Fung, K. Wang, R. Chen, P. S. Yu, Privacy-preserving data pub-
lishing: A survey of recent developments, ACM Comput. Surv. 42 (4)
(jun 2010). doi:10.1145/1749603.1749605.
URL https://doi.org/10.1145/1749603.1749605

[122] C. C. Aggarwal, P. S. Yu (Eds.), Privacy-Preserving Data Mining - Mod-
els and Algorithms, Vol. 34 of Advances in Database Systems, Springer,
2008. doi:10.1007/978-0-387-70992-5.
URL https://doi.org/10.1007/978-0-387-70992-5

[123] P. Ram Mohan Rao, S. Murali Krishna, A. P. Siva Kumar, Privacy
preservation techniques in big data analytics: a survey 5 (1) 33. doi:
10.1186/s40537-018-0141-8.
URL https://journalofbigdata.springeropen.com/articl
es/10.1186/s40537-018-0141-8

[124] M. Cunha, R. Mendes, J. P. Vilela, A survey of privacy-preserving
mechanisms for heterogeneous data types, Computer Science Review

41 (2021) 100403. doi:https://doi.org/10.1016/j.cosrev.2
021.100403.
URL https://www.sciencedirect.com/science/article/pi
i/S1574013721000435

[125] C. Dwork, A. Smith, T. Steinke, J. Ullman, Exposed! A Survey of At-
tacks on Private Data, Annual Review of Statistics and Its Application
4 (1) (2017) 61–84. doi:10.1146/annurev-statistics-060116-
054123.

[126] ISO, Privacy enhancing data de-identiﬁcation terminology and classiﬁ-

cation of techniques (2018).
URL https://www.iso.org/standard/69373.html

[127] N. Li, W. Qardaji, D. Su, On sampling, anonymization, and diﬀerential
privacy or, k-anonymization meets diﬀerential privacy, in: Proceedings
of the 7th ACM Symposium on Information, Computer and Communi-
cations Security, ASIACCS ’12, Association for Computing Machinery,
New York, NY, USA, 2012, p. 32–33. doi:10.1145/2414456.2414
474.
URL https://doi.org/10.1145/2414456.2414474

[128] H. Xu, S. Guo, K. Chen, Building conﬁdential and eﬃcient query ser-
vices in the cloud with rasp data perturbation (2013). doi:10.1109/
TKDE.2012.251.

[129] K. Chen, L. Liu, Geometric data perturbation for privacy preserving out-
sourced data mining, Knowledge and Information Systems 29 (3) (2011)
657–695.
URL http://link.springer.com/10.1007/s10115-010-0362-
4

[130] R. Henry, A. Herzberg, A. Kate, Blockchain access privacy: Challenges
and directions, IEEE Security and Privacy 16 (4) (2018) 38–45. doi:
10.1109/MSP.2018.3111245.

[131] D. Chaum, Untraceable electronic mail, return addresses, and digital
pseudonyms, Communications of the ACM 24 (2) (1981) 84–90.
[132] D. Chaum, The dining cryptographers problem: Unconditional sender

and recipient untraceability, Journal of cryptology 1 (1) (1988) 65–75.

[133] J. Ren, J. Wu, Survey on anonymous communications in computer net-

works, Computer Communications 33 (4) (2010) 420–431.

[134] M. S. Ali, K. Dolui, F. Antonelli, Iot data privacy via blockchains and
ipfs, in: Proceedings of the Seventh International Conference on the
Internet of Things, IoT ’17, Association for Computing Machinery, New
York, NY, USA, 2017. doi:10.1145/3131542.3131563.
URL https://doi.org/10.1145/3131542.3131563

[135] M. Kesarwani, A. Kaul, S. Braghin, N. Holohan, S. Antonatos, Secure
k-anonymization over encrypted databases, in: 2021 IEEE 14th Inter-
national Conference on Cloud Computing (CLOUD), 2021, pp. 20–30.
doi:10.1109/CLOUD53861.2021.00015.

[136] R. Rivest, A. Shamir, Y. Tauman, How to Leak a secret, Lecture Notes

in Computer Science, vol 2248. Springer, Berlin, Heidelberg. (2001).
URL https://ieeexplore.ieee.org/document/6032224{%}0
Ahttps://cryptoslate.com/ethereum-network-congestion-
doubles-gas-fees-as-game

[137] M. Bellare, D. Micciancio, B. Warinschi, Foundations of Group Signa-
tures: Formal Deﬁnitions, Simpliﬁed Requirements, and a Construction
Based on General Assumptions, Eurocrypt 2656 (2003) 1–27.
URL https://cseweb.ucsd.edu/~mihir/papers/gs.pdf

[138] A. F. Westin, Privacy And Freedom (1970).

URL https://www.worldcat.org/title/privacy-and-freed
om/oclc/792862

[139] A. Raj, J. Bosch, H. H. Olsson, T. J. Wang, Modelling data pipelines,
in: 2020 46th Euromicro Conference on Software Engineering and Ad-
vanced Applications (SEAA), 2020, pp. 13–20. doi:10.1109/SEAA51
224.2020.00014.

[140] A. Cavoukian, The 7 Foundational Principles (2011) 2.

URL https://sites.psu.edu/digitalshred/2020/11/13/pr
ivacy-by-design-pbd-the-7-foundational-principles-ca
voukian/

[141] S. G¨urses, C. Troncoso, C. Diaz, Engineering: Privacy by design, Sci-

ence 317 (5842) (2011) 1178–1179.
URL https://www.esat.kuleuven.be/cosic/publications/
article-1542.pdf

[142] J. Sedlmeir, J. Lautenschlager, G. Fridgen, N. Urbach, The transparency
challenge of blockchain in organizations, Electronic Markets (2022). do
i:https://doi.org/10.1007/s12525-022-00536-0.

33

[143] M. Yung, S. Jarecki, H. Krawczyk, A. Herzberg, Proactive secret sharing
or : How to cope with perpetual leakage, Communication (1995).
URL https://www.researchgate.net/publication/2213553
99 Proactive Secret Sharing Or How to Cope With Perpetu
al Leakage

[162] C. Perera, C. H. Liu, S. Jayawardena, The Emerging Internet of Things
Marketplace from an Industrial Perspective: A Survey, IEEE Transac-
tions on Emerging Topics in Computing 3 (4) (2015) 585–598.
URL https://ieeexplore.ieee.org/document/7004800
[163] L. Determann, No one owns data, UC Hastings Law 70 (2018) 44. doi:

[144] IOTA-Foundation, About the Tangle (2020).

10.2139/ssrn.3123957.

URL https://legacy.docs.iota.org/docs/getting-starte
d/1.1/the-tangle/overview

[145] E. Heilman, N. Narula, G. Tanzer, J. Lovejoy, M. Colavita, M. Virza,
T. Dryja, Cryptanalysis of Curl-P and Other Attacks on the IOTA Cryp-
tocurrency, IACR Transactions on Symmetric Cryptology (2020) 367–
391doi:10.46586/tosc.v2020.i3.367-391.

[146] D. Wang, J. Zhao, Y. Wang, A Survey on Privacy Protection of
Blockchain: The Technology and Application, IEEE Access 8 (2020)
108766–108781. doi:10.1109/ACCESS.2020.2994294.
URL https://ieeexplore.ieee.org/document/9093015/
[147] J. Sedlmeir, H. U. Buhl, G. Fridgen, R. Keller, The energy consumption
of blockchain technology: beyond myth, Business & Information Sys-
tems Engineering 62 (6) (2020) 599–608.
URL https://www.researchgate.net/publication/3423132
38 The Energy Consumption of Blockchain Technology Bey
ond Myth

[148] N. Kannengießer, S. Lins, T. Dehling, A. Sunyaev, Trade-oﬀs between
distributed ledger technology characteristics, ACM Computing Surveys
(CSUR) 53 (2) (2020) 1–37.
URL https://dl.acm.org/doi/10.1145/3379463

[149] European Parliament and Council of the European Union, Regulation
(eu) 2016/679 directive 95/46/ec (general data protection regulation):
General data protection regulation (4 May 2016).
URL https://eur-lex.europa.eu/legal-content/EN/TXT/?u
ri=CELEX%3A32016R0679

[150] A. Ghosh, K. Ligett, A. Roth, G. Schoenebeck, Buying private data with-
out veriﬁcation, EC 2014 - Proceedings of the 15th ACM Conference
on Economics and Computation (2014) 931–948arXiv:1404.6003,
doi:10.1145/2600057.2602902.

[151] S. Ellis, A. Juels, S. Nazarov, Chainlink a decentralized oracle network

(2021).
URL https://link.smartcontract.com/whitepaper

[152] S. S. Al-Riyami, K. G. Paterson, Certiﬁcateless public key cryptogra-
phy, Lecture Notes in Computer Science (including subseries Lecture
Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics)
2894 (2003) 452–473.
URL https://eprint.iacr.org/2003/126.pdf

[153] D. Reed, M. Sporny, D. Longley, C. Allen, A. Grant, M. Sabadello,

Decentralized identiﬁers (dids) v1.0 (2021).
URL https://w3c.github.io/did-core/

[154] L. Mui, M. Mohtashemi, A. Halberstadt, A computational model of trust
and reputation, in: Proceedings of the 35th Hawaii International Confer-
ence on System Sciences, IEEE, 2002, pp. 2431–2439.

[155] T. Grandison, M. Sloman, A survey of trust in internet applications,
IEEE Communications Surveys & Tutorials 3 (4) (2000) 2–16.
[156] D. Artz, Y. Gil, A survey of trust in computer science and the semantic

[164] J. H. Nord, A. Koohang, J. Paliszkiewicz, The internet of things: Re-
view and theoretical framework, Expert Systems with Applications 133
(2019) 97–108. doi:https://doi.org/10.1016/j.eswa.2019.
05.014.
URL https://www.sciencedirect.com/science/article/pi
i/S0957417419303331

[165] S. Arumugam, R. Bhargavi, A survey on driving behavior analysis in
usage based insurance using big data 6 (1) 86. doi:10.1186/s40537
-019-0249-5.
URL https://journalofbigdata.springeropen.com/articl
es/10.1186/s40537-019-0249-5

[166] D. E. Pozen, The mosaic theory, national security, and the freedom of

information act 52.

[167] M. Archie, S. Gershon, A. Katcoﬀ, A. Zeng, Who1s Watching? De-
anonymization of Netﬂix Reviews using Amazon Reviews, 2018, online;
accessed 30 December 2021.
URL https://www.oasislabs.com/how-it-works

[168] L. M. Zagi, B. Aziz, Privacy Attack on IoT: A Systematic Literature Re-
view, in: 7th International Conference on ICT for Smart Society: AIoT
for Smart Society, ICISS 2020 - Proceeding, Institute of Electrical and
Electronics Engineers Inc., 2020.
URL https://ieeexplore.ieee.org/document/9307568
[169] D. Kondor, B. Hashemian, Y.-A. de Montjoye, C. Ratti, Towards Match-
ing User Mobility Traces in Large-Scale Datasets, IEEE Transactions on
Big Data 6 (4) (2020) 714–726. doi:10.1109/TBDATA.2018.2871
693.

[170] A. Wood, M. Altman, A. Bembenek, M. Bun, M. Gaboardi, J. Honaker,
K. Nissim, D. O’Brien, T. Steinke, S. Vadhan, Diﬀerential Privacy: A
Primer for a Non-Technical Audience, SSRN Electronic Journal (2018).
doi:10.2139/ssrn.3338027.

[171] M. Archie, S. Gershon, A. Katcoﬀ, A. Zeng, De-anonymization of Net-

ﬂix Reviews using Amazon Reviews (2018) 5.
URL https://www.readkong.com/page/de-anonymization-o
f-netflix-reviews-using-amazon-reviews-1439089

[172] T. W. S. Journal, Google to pay $22.5 Million in FTC settlement (2012).
URL https://www.wsj.com/articles/SB10000872396390443
404004577579232818727246

[173] J. Porter, Google ﬁned €50 million for GDPR violation in France

(2019).
URL https://www.theverge.com/2019/1/21/18191591/goog
le-gdpr-fine-50-million-euros-data-consent-cnil
[174] N. Lomas, France ﬁnes Google $120M and Amazon $42M for dropping

tracking cookies without consent (2020) 1–12.
URL https://dataprotection.news/france-fines-google-
120m-and-amazon-42m-for-dropping-tracking-cookies-wi
thout-consent/

web, Journal of Web Semantics 5 (2) (2007) 58–71.

[175] BBC, H&M ﬁned for breaking GDPR over employee surveillance - BBC

[157] K. S. Cook, R. Hardin, M. Levi, Cooperation without Trust?, Russell

Sage Foundation, 2005.

[158] L. Ismail, H. Hameed, M. AlShamsi, M. AlHammadi, N. AlDhanhani,
Towards a Blockchain Deployment at UAE University: Performance
Evaluation and Blockchain Taxonomy, in: Proceedings of the 2019 In-
ternational Conference on Blockchain Technology, ACM, Honolulu HI
USA, 2019, pp. 30–38. doi:10.1145/3320154.3320156.

[159] L. Lamport, R. Shostak, M. Pease, The Byzantine Generals Problem,
Association for Computing Machinery, New York, NY, USA, 2019, p.
203–226. doi:10.1145/3335772.3335936.

[160] A. Narayanan, V. Shmatikov, Robust De-anonymization of Large Sparse
Datasets, in: 2008 IEEE Symposium on Security and Privacy (sp 2008),
IEEE, Oakland, CA, USA, 2008, pp. 111–125, iSSN: 1081-6011. doi:
10.1109/SP.2008.33.

[161] C. Perera, R. Ranjan, L. Wang, S. U. Khan, A. Y. Zomaya, Big Data
Privacy in the Internet of Things Era, IT Professional 17 (3) (2015) 32–
39. doi:10.1109/MITP.2015.34.

News (2020).
URL https://www.bbc.com/news/technology-54418936

[176] Marketing : the Italian SA ﬁnes TIM EUR27.8 million (2020).

URL https://edpb.europa.eu/news/national-news/2020/m
arketing-italian-sa-fines-tim-eur-278-million en
[177] Q. Feng, D. He, S. Zeadally, M. K. Khan, N. Kumar, A survey on privacy
protection in blockchain system, Journal of Network and Computer Ap-
plications 126 (2019) 45–58. doi:10.1016/j.jnca.2018.10.020.

[178] C. Wang, N. Zhang, C. Wang, Managing privacy in the digital economy,
Fundamental Research 1 (5) (2021) 543–551. doi:10.1016/j.fmre
.2021.08.009.

[179] M. Akil, L. Islami, S. Fischer-H¨ubner, L. A. Martucci, A. Zuccato,
Privacy-preserving identiﬁers for IoT: A systematic literature review,
IEEE Access 8 (2020) 168470–168485. doi:10.1109/ACCESS.2
020.3023659.

[180] T. Gebremichael, L. P. I. Ledwaba, M. H. Eldefrawy, G. P. Hancke,
N. Pereira, M. Gidlund, J. Akerberg, Security and privacy in the Indus-
trial Internet of Things: Current standards and future challenges, IEEE

34

021.102529.

[199] Y. Hu, C. Li, A. Hu, A. Hu, J. Zhao, Trading oﬀ data resource availabil-
ity and privacy preservation in multi-layer network transaction, Physical
Communication 46 (2021) 101317. doi:10.1016/j.phycom.2021.
101317.

[200] Q. Song, J. Cao, K. Sun, Q. Li, K. Xu, Try before you buy: Privacy-
preserving data evaluation on cloud-based machine learning data mar-
ketplace,
in: Annual Computer Security Applications Conference,
ACM, 2021, p. 260–272. doi:10.1145/3485832.3485921.
[201] M. N. Alraja, H. Barhamgi, A. Rattrout, M. Barhamgi, An integrated
framework for privacy protection in IoT — applied to smart healthcare
91 (2021) 107060. doi:10.1016/j.compeleceng.2021.107060.
URL https://www.sciencedirect.com/science/article/pi
i/S0045790621000744

[202] G. Goos, J. Hartmanis, J. van Leeuwen, D. Hutchison, T. Kanade, J. Kit-
tler, J. M. Kleinberg, F. Mattern, J. C. Mitchell, M. Naor, O. Nierstrasz,
C. P. Rangan, B. Steﬀen, Lecture Notes in Computer Science 556.
URL https://doi.org/10.1007/978-3-319-70139-4 56
[203] P. Cramton, Y. Shoham, R. Steinberg, An overview of combinatorial

auctions, ACM SIGecom Exchanges 7 (1) (2007) 3–14.
URL https://dl.acm.org/doi/10.1145/1345037.1345039
[204] R. Dingledine, N. Mathewson, P. Syverson, Tor: The Second-Generation
Onion Router:, Tech. rep., Defense Technical Information Center, Fort
Belvoir, VA (Jan. 2004).
URL http://www.dtic.mil/docs/citations/ADA465464
[205] D. Bogdanov, S. Laur, J. Willemson, Sharemind: a framework for fast

privacy-preserving computations 15.
URL https://link.springer.com/chapter/10.1007/978-3-
540-88313-5 13

[206] N. Wang, X. Xiao, Y. Yang, J. Zhao, S. C. Hui, H. Shin, J. Shin, G. Yu,
Collecting and Analyzing Multidimensional Data with Local Diﬀeren-
tial Privacy (2019) 13.
URL https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&a
rnumber=8731512

[207] R. Bassily, A. Smith, Local, private, eﬃcient protocols for succinct his-
tograms, Proceedings of the forty-seventh annual ACM symposium on
Theory of Computing (Jun 2015). doi:10.1145/2746539.2746632.
[208] A. Herzberg, S. Jarecki, H. Krawczyk, M. Yung, Proactive Secret Shar-
ing Or: How to Cope With Perpetual Leakage, in: D. Coppersmith (Ed.),
Advances in Cryptology — CRYPT0’ 95, Springer Berlin Heidelberg,
Berlin, Heidelberg, 1995, pp. 339–352.
URL https://link.springer.com/chapter/10.1007/3-540-
44750-4 27

[209] A. Bellet, A. Habrard, M. Sebban, A survey on metric learning for fea-

ture vectors and structured data (2014).
URL https://arxiv.org/abs/1306.6709

[210] B. Poettering, D. Stebila, Double-authentication-preventing signatures,

International Journal of Information Security 16 (1) (2017).
URL http://link.springer.com/10.1007/s10207-015-0307-
8

[211] Achieving secure, scalable, and ﬁne-grained data access control in cloud

computing.

[212] R. Wieringa, N. Maiden, N. Mead, C. Rolland, Requirements engi-
neering paper classiﬁcation and evaluation criteria: A proposal and a
discussion, Requirements Engineering 11 (1) (2006) 102–107. doi:
10.1007/s00766-005-0021-6.

Access 8 (2020) 152351–152366. doi:10.1109/ACCESS.2020.3016
937.

[181] S. W. Driessen, G. Monsieur, W.-J. Van Den Heuvel, Data market de-
sign: A systematic literature review, IEEE Access 10 (2022) 33123–
33153. doi:10.1109/ACCESS.2022.3161478.

[182] N. Deepa, Q.-V. Pham, D. C. Nguyen, S. Bhattacharya, B. Prabadevi,
T. R. Gadekallu, P. K. R. Maddikunta, F. Fang, P. N. Pathirana, A sur-
vey on blockchain for big data: Approaches, opportunities, and future
directions, Future Generation Computer Systems 131 (2022) 209–226.
[183] A. J. Perez, S. Zeadally, Secure and privacy-preserving crowdsensing
using smart contracts: Issues and solutions, Computer Science Review
43 (2022) 100450. doi:10.1016/j.cosrev.2021.100450.
[184] C. Gonc¸alves, R. J. Bessa, P. Pinson, A critical overview of privacy-
preserving approaches for collaborative forecasting, International Jour-
nal of Forecasting 37 (1) (2021) 322–342. doi:10.1016/j.ijforeca
st.2020.06.003.

[185] Y. Wu, Z. Wang, Y. Ma, V. C. M. Leung, Deep reinforcement learn-
ing for blockchain in industrial IoT: A survey, Computer Networks 191
(2021) 108004. doi:https://doi.org/10.1016/j.comnet.202
1.108004.
URL https://www.sciencedirect.com/science/article/pi
i/S1389128621001213

[186] L. D. Nguyen, I. Leyva-Mayorga, A. N. Lewis, P. Popovski, Model-
ing and analysis of data trading on blockchain-based market in IoT
networks, IEEE Internet of Things Journal 8 (8) (2021) 6487–6497.
doi:10.1109/JIOT.2021.3051923.

[187] A. Sadiq, M. U. Javed, R. Khalid, A. Almogren, M. Shaﬁq, N. Javaid,
Blockchain based data and energy trading in internet of electric vehicles,
IEEE Access 9 (2021) 7000–7020. doi:10.1109/ACCESS.2020.30
48169.

[188] Y. Long, Y. Chen, W. Ren, H. Dou, N. N. Xiong, Depet: A decentral-
ized privacy-preserving energy trading scheme for vehicular energy net-
work via blockchain and k-anonymity, IEEE Access 8 (2020) 192587–
192596. doi:10.1109/ACCESS.2020.3030241.

[189] R. Xu, Y. Chen, Fed-DDM: A federated ledgers based framework for
hierarchical decentralized data marketplaces, 2021 International Con-
ference on Computer Communications and Networks (2021).

[190] L. Giaretta, I. Savvidis, T. Marchioro, S. Girdzijauskas, G. Pallis, M. D.
Dikaiakos, E. Markatos, Pds2: A user-centered decentralized market-
place for privacy preserving data processing, in: 2021 IEEE 37th Inter-
national Conference on Data Engineering Workshops (ICDEW), 2021,
pp. 92–99. doi:10.1109/ICDEW53142.2021.00024.

[191] A. Manzoor, A. Braeken, S. S. Kanhere, M. Ylianttila, M. Liyanage,
Proxy re-encryption enabled secure and anonymous IoT data sharing
platform based on blockchain, Journal of Network and Computer Appli-
cations 176 (2021) 102917. doi:10.1016/j.jnca.2020.102917.

[192] T. R¨uckel, J. Sedlmeir, P. Hofmann, Fairness, integrity, and privacy in
a scalable blockchain-based federated learning system, Computer Net-
works 202 (2022) 108621. doi:10.1016/j.comnet.2021.108621.
[193] P. Gupta, V. Dedeoglu, S. S. Kanhere, R. Jurdak, TrailChain: Traceabil-
ity of data ownership across blockchain-enabled multiple marketplaces,
Journal of Network and Computer Applications (2022) 103389doi:
10.1016/j.jnca.2022.103389.

[194] Y. Tian, B. Song, T. Ma, A. Al-Dhelaan, M. Al-Dhelaan, Bi-tier diﬀeren-
tial privacy for precise auction-based people-centric IoT service, IEEE
Access 9 (2021) 55036–55044. doi:10.1109/ACCESS.2021.306713
8.

[195] M. Zhang, L. Yang, S. He, M. Li, J. Zhang, Privacy-preserving data
aggregation for mobile crowdsensing with externality: An auction ap-
proach, IEEE/ACM Trans. Netw. 29 (3) (2021) 1046–1059. doi:
10.1109/TNET.2021.3056490.

[196] X. Xu, Z. Yang, Y. Xian, Atm: Attribute-based privacy-preserving task
assignment and incentive mechanism for crowdsensing, IEEE Access 9
(2021) 60923–60933. doi:10.1109/ACCESS.2021.3074142.
[197] F. Kserawi, S. Al-Marri, Q. Malluhi, Privacy-preserving fog aggregation
of smart grid data using dynamic diﬀerentially-private data perturbation,
IEEE Access 10 (2022) 43159–43174. doi:10.1109/ACCESS.2022.
3167015.

[198] Y. Shen, B. Guo, Y. Shen, X. Duan, X. Dong, H. Zhang, C. Zhang,
Y. Jiang, Personal big data pricing method based on diﬀerential privacy,
Computers & Security 113 (2022) 102529. doi:10.1016/j.cose.2

35

Appendix A. Acronyms

Authenticity-enhancing technologies
Bundling problem
Copy problem
Digital ﬁngerprint
Decentralized identiﬁer
Distributed ledger technology
Diﬀerential privacy
Digital signature
Fully homomorphic encryption
Federated learning
Generative adversarial networks
General Data Protection Regulation
Homomorphic encryption
Information and communication technology
Internet of things
Key generation center
Machine learning
Privacy-enhancing technology
Partially homomorphic encryption
Public key infrastructure

AET
BP
CP
DF
DID
DLT
DP
DS
FHE
FL
GAN
GDPR
HE
ICT
IoT
KGC
ML
PET
PHE
PKI
PPDM Privacy-preserving data mining
Recursive enforcement problem
REP
Research question
RQ
Smart contract
SC
Systematic literature review
SLR
Secure multiparty computation
MPC
Truth discovery
TD
Trusted execution environment
TEE
Zero-knowledge proof
ZKP

36

Appendix B. Summaries of the selected secondary studies

Table B.4: Secondary studies on privacy in data markets.

Year

Study Topic

Description

2020

[S8]

Privacy-enhancing design of
data markets

2019

[S19] Privacy and security data ﬂow

challenges in an internet of
production

Analyzes internet users’ preferences for privacy in data sharing to uncover mental models of these
preferences and their motives, barriers, and conditions for a privacy-enhancing data market. It
provides a set of key ﬁndings, the two most notable ones being that the primary barrier to creating
data markets is privacy and moral concerns and that the level of anonymization has the largest
eﬀect on the willingness to share.

Introduces the internet of production and illustrates its inter-organizational data ﬂows.
It also
identiﬁes security and privacy demands and challenges within these data ﬂows: authenticity, data
access scope, and anonymity. Furthermore, it provides a survey of PETs to tackle these chal-
lenges: provide conﬁdentiality, hide information during computation (data processing), verify the
authenticity of information (providing support), deploy mechanisms that enforce rules (platform
capabilities), and support approaches that focus on the security of data ﬂows (external measures).

2019

[S14] Challenges and research oppor-
tunities in data markets for the
IoT

A short study that identiﬁes three research opportunities in IoT data markets: Procurement, pric-
ing, and privacy. Signiﬁcant identiﬁed challenges are: ambiguity in data ownership that hinders
trading, the diﬃculty to detect data piracy, and that privacy must be considered before trading.

2018

[S23] Privacy enhancing in IoT ap-

plications

Introduces and surveys privacy-enhancing technologies in the processes of data aggregation,
trading, and analysis; in particular, it discusses outsourced computation, data validation, and
blockchain technology. Additionally, it describes types of privacy breaches and their countermea-
sures. Furthermore, it reviews relevant aspects of pricing procedures as well as game-theoretical
approaches and auction schemes.

2018

[S4]

Pricing, trading, and protection
in data markets for the IoT

Surveys the three ﬁelds of pricing models and strategies, design of platforms and data trading, and
digital copyright mechanisms with a focus on privacy enhancement.

2018

[S48] Privacy-enhancing analytics
for IoT and cloud-based sys-
tems

Summarizes privacy-enhancing technologies in the speciﬁc use case of a health data collecting
app in the health industry. More speciﬁcally, it separates privacy-enhancing technologies into two
scenarios: Outsourced computation and information sharing.

2016

[S35] Privacy enhancing in crowd-

sourcing task management

2015

[S27] Privacy enhancing and chal-
lenges in data markets for the
IoT

2014

[S38] Privacy threats and challenges

in the IoT

Surveys privacy-enhancing technologies and the challenges of crowdsourcing task management.
The proposed technologies are anonymization, such as k-anonymity, spatio-temporal privacy ap-
proaches, such as spatial cloaking or aggregated location via diﬀerential privacy, and policy-
based privacy preferences. The challenges that they present revolve around trust and credibil-
ity, reward-based tasking, utility, eﬃciency, enforcing privacy-enhancing technologies, and raise
privacy awareness.

The study introduces privacy enhancing for data markets for IoT devices, focusing on sensing-as-
a-service (data analysis of user-aggregated data). It identiﬁes three challenges: Developing IoT
middleware for data analysis and autonomous privacy enhancing, autonomous end-user consent
acquisition and negotiation, and the autonomous modeling and negotiation of privacy risk and
economic reward. The most prevalent privacy-enhancing technologies and strategies they intro-
duce are personal information hubs, onion routing, and data aggregation via diﬀerential privacy or
k-anonymity.

Classiﬁes the threats and challenges that come along with privacy in IoT applications for individ-
uals into seven categories: Re-identiﬁcation of individuals through persistent pseudo-identiﬁers,
localization and tracking, proﬁling for social engineering and price discrimination, information
disclosure in life cycle transitions, information linkage of previously separated systems, inventory
attacks, and the disclosure of private information to an uninvited audience.

37

Appendix C. Mappings of privacy- and authenticity-enhancing technologies

Figure C.11: Classiﬁcation of the identiﬁed privacy- and authenticity-enhancing technologies in this SLR, together with the challenges they tackle. Any other
privacy approach encountered in the SLR without a succinct inclusion of the underlying technology was either not included in a leaf node but in a parent node
or completely dismissed if too vague. *The publication reviews the technology without delving into it in-depth or using it as a building block of the architecture
concept, e.g., the technology is only mentioned in the opportunities for future work.

38

 Privacy- and authenticity- enhancing technologies for  IoT data markets Authenticity-enhancing   technologies Consensus layer Distributed ledger  technology [S11][S19*][23*][S49] Permissioned Hyperledger Iroha  [S3] Hyperledger Fabric*  [S10][S33] Rem (SGX) [S12] Quorum*  (Private Ethereum)  [S18] Permissionless Ethereum  [S1][S13][S20][S28] [S32][S45][S46] Agora* [S43] IOTA  [S20][S28][S30] Ekiden*  [S2][S25] Verification layer Truth discovery  Majority voting  [S9][S45] Reputation system  [S9] Peer-prediction-based  trustable data aggregation [S23*] Mutual validation  [S23*] Digital signatures [S19*] Identity-based [S7] Certificateless [S12] Assures identity authenticity. Assures identity authenticity.  Tackles the recursive enforcement problem.  Decentralized identifiers  [S3][S20] Version control [S19*] Digital fingerprint  [S19*] Enhance data and identity authenticity, and  integrity of computation and data.  Tackle the recursive enforcement problem  by removing the dependence on a single  trust anchor. (Forgo privacy) Assure data and identity authenticity, and  integrity of data. (Forgo privacy) Assure identity authenticity. Tackle the recursive enforcement  problem by removing the dependence on a single trust anchor. Enhance identity and data authenticity, and the integrity of computation  and data.  Tackle the recursive enforcement problem by removing the dependence on  a single trust anchor.  Distributed ledger technology forgoes privacy due to its inherent  transparency and replication; however, undesired information exposure can  be mitigated through the inclusion of PETs *(Ekiden employs TEEs, Agora  employs functional encryption, Quorum and Fabric restrict the visibility of  transactions). Privacy-enhancing  technologies Verification layer Hashing SHA-256  [S1][S10][S33][S46] Assures data integrity while enhancing the confidentiality of the input data.  Circumvents the recursive enforcement problem. Digital signatures [S19*] Ring signatures  [S32] Blind signatures  [S20][S26] Enhances privacy further by concealing the identity's  authenticity. Tackles the bundling problem Can assure identity authenticity. Enhances privacy further by concealing  the sent data. Tackles the bundling problem Storage layer Communication layer Encryption Symmetric encryption  [S25][S42] AES  [S1][S32][S33][S41][S45] Asymmetric encryption  [S1][S2][S3][S10] [S17][S25][S33] [S41][S42][S46] Onion Routing  [S4*][S26*][S27*][S35*] Enhance confidentiality. Enhances the privacy of the data sender's identity. Tackle the recursive  enforcement problem by removing the dependence on a single trust anchor. Processing layer Secure and  outsourced  computation Zero-knowledge proofs Non-interactive [S43] Interactive [S43*] Secure multiparty  computation  [S23*][S48*] Secret sharing  [S19*] Additive  [S5][S34] Shamir ECIES  [S10] Garbled circuits  [S34]  Homomorphic  encryption  [S4*][S19*][S48*] Partial  [S46] Paillier  [S26][S41] Boneh-Goh-Nissim  [S7] Hash-ElGamal  [S34] Fully  [S10] Functional encryption Multi-client [S43] Single Trusted execution environments [S19*] SGX  [S2][S25][S45] Keystone  [S25*] Sanctum [S25*] Privacy-preserving data mining [S23*][S2][S25][S45] Tackle the copy problem by computing without disclosing the inputs. Circumvent the recursive enforcement problem. Can provide proof of data and identity authenticity leveraging zero- knowledge proofs or digital signatures with digital certificates. Can provide proof of data and computation integrity. Enhance confidentiality. Anonymization Semantic Differential privacy  [S4*][S8*][S19*][S27*] [S48*][S35*][S49*] Local [S3][S9][S15] [S16][S37][S42] Global [S2][S18][S22][S24] [S25][S28][S50] Syntactic K-Anonymity  [S3][S8*][S27*][S28][S33] [S49*] [S35*][S36][S47] Non-sensitive  attribute re- identification  prevention L-Diversity  [S47] T-Closeness  [S47] Generalization  ß-likeness  [S44] Suppression  [S36] Generalization  [S21][S36] Perturbative Categorical Perturbation  ß-likeness  [S44] Numerical Additive noise  [S19*][S36] [S47][S48*] Random space  perturbation [S48*] Geometric  perturbation [S48*] Pseudonym  creation  [S29*] Asymmetric encryption ElGamal  [S7] Deterministic encryption  Order-preserving  encryption  [S48*] Tackle the bundling problem by  reducing data authenticity. Sovereignty layer Privacy policies (Access control) [S19*][S35*][S40*] Decentralized Smart contracts (Distributed ledger  technology) Ekiden  [S2][S25] Ethereum [S28] Negotiated  [S6*][S28][S29][S33] Pre-defined logic  [S21][S36] Determine the privacy requirements that should be enforced by PETs. If policies are decentralized, then they tackle the recursive enforcement  problem by removing the dependence on a single trust anchor. *Some PETs  also contain  authenticity- enhancing  properties.Appendix D. Summaries of the selected primary studies

Table D.5: Set of examples from the selected studies describing diﬀerent data marketplace
architectures and auctions based on PETs and AETs with a focus on secure computation
technologies.

Year

Study Privacy-enhancing ap-

Description

proaches

2020

[S41] Partially homomorphic en-

cryption, symmetric encryp-
tion, and digital signatures

2018

[S7]

Partially homomorphic en-
cryption, digital signatures,
and pseudonym creation

2018

[S10] Fully homomorphic encryp-
tion, secure multiparty com-
putation, distributed ledger
technology, and hashing

Develops a privacy-enhancing auction for big data trading using Paillier’s cryptosystem [18] and
a one-time pad. They consider four entities: sellers, buyers, an auctioneer, and an intermediary
platform. A data auction is carried out without any entity seeing the data (except the auction win-
ner) or the bid values, which are ordered obliviously by the auctioneer thanks to the homomorphic
properties of the ciphertext. Furthermore, to eﬃciently encrypt the data, the authors use symmetric
encryption (AES). Lastly, digital signatures are created with the same homomorphic cryptosystem,
which the authors use to encrypt the symmetric keys.

Implements a platform for data markets that facilitates data processing and outcome veriﬁcation
while enhancing the privacy of identities and their data. The authors consider four entities: data
contributor, service provider, data consumer, and a registration center; in a two-layer system model:
data acquisition and trading. Furthermore, the platform synchronizes data processing and signa-
ture veriﬁcation into the same homomorphic ciphertext space (encrypt and then sign). Addition-
ally, they tightly integrate data processing with outcome veriﬁcation via a set of homomorphic
properties. To achieve a trade-oﬀ between functionality and performance, they selected a partially
homomorphic scheme called Boneh-Goh-Nissim cryptosystem [84].

Provides a distributed outsourcing computation architecture, whereby data owners may request
fully homomorphic computations with a schema called fully homomorphic non-interactive veri-
ﬁable secret sharing [86]. Moreover, the proposed architecture allows transactions to be veriﬁed
by the participants of the permissioned blockchain thanks to the immutability properties of the
blockchain; Hyperledger Fabric was the selected blockchain architecture. Moreover, the hash
value of the shared data is stored in the blockchain, for data recipients to verify the truthfulness of
the received data. Furthermore, for secure multiparty computation, the authors implement Shamir’s
secret sharing [72] with ECIES by leveraging the distributed nature of the blockchain. In this man-
ner, the data owner may share veriﬁable pieces of information with a set of servers. Then, the
servers execute the necessary computations, and when several veriﬁed responses are received by
the agreed data consumer, the true result is recovered.

2016

[S34] Partially homomorphic en-
cryption, and secure multi-
party computation

Develops an auction cloud-based framework that cryptographically hides the bids from all auction
participants until a winner is determined. It achieves this by combining PHE based on the hashed
scheme [85] of ElGamal [202], and secure two-party computation through garble circuits and
additive secret sharing.

2015

[S26] Partially homomorphic en-
cryption, digital signatures,
and onion routing

2012

[S5]

Secure multiparty computation

Proposes a combinatorial auction [203] mechanism that ensures the privacy of the bidders. The
bidders bids are blindly signed through the third party [61] so that the third party does not learn
the contents. Later, these signatures are used by the bidder to prove the authenticity of the bids.
The winner is determined by the third party through a partially homomorphically encrypted com-
putation using the Paillier cryptosystem [18]. Lastly, the identities of the bidders are enhanced by
using onion routing [204].

Implements a privacy-enhancing data mining service market, whereby data donors distribute conﬁ-
dential data among a set of participants employing additive secret sharing. The miners collectively
perform secure multiparty computation based on the author’s algorithm [205]. Finally, the results
are in turn sent to the previously agreed analyst, who combines them to obtain the intelligible
output.

39

Table D.6: Set of examples from the selected studies describing diﬀerent data marketplace
architectures and trading mechanisms based on PETs and AETs with a focus on anonymiza-
tion technologies.

Year

Study Privacy-enhancing
approaches

2020

[S24] Diﬀerential privacy

2019

[S37] Diﬀerential privacy

2019

[S42] Diﬀerential privacy and digital

signatures

2019

[S15] Diﬀerential privacy

2019

[S50] Diﬀerential privacy

2019

[S22] Diﬀerential privacy

2018

[S2]

Diﬀerential privacy, distributed
ledger technology, smart con-
tracts, privacy policies, asym-
metric encryption, and trusted
execution environments

2018

[S16] Diﬀerential privacy

2018

[S9]

Diﬀerential privacy and truth
discovery

Description

Designs a privacy-enhancing crowd-sensed data trading mechanism. First, the data broker orches-
trates an auction whereby data consumers bid in a diﬀerentially private manner for a data asset.
Secondly, to form a data asset, the data broker creates a set of data generation tasks, some of which
are fake to protect the privacy of the auction winner. Lastly, the data broker selects data owner
outputs in a diﬀerentially private manner. More speciﬁcally, both the auction-based data pricing
and the data collection are based on the diﬀerentially private exponential mechanism.

Proposes a diﬀerentially private data market auction framework with a fair negotiation method to
set the price and noise; this study is extended in [S42]. The entities involved are a data provider,
a consumer, and a trusted market manager that matches providers with consumers and enforces
Rubinstein bargaining. Firstly, the data provider and consumer enter a negotiation phase that
involves the data query, the ε values, and the unit price for ε. Once the negotiation is over, the data
provider answers the query with the agreed ε with local diﬀerential privacy.

Proposes a diﬀerentially private data market framework. This study extends [S37] by specifying
the type of diﬀerential privacy algorithm, and the digital signature schemata followed to deploy
the framework in practice. The authors use local diﬀerential privacy for numeric [206] and for
categorical [207] data types.

Designs contracts for a data marketplace whereby a data broker matches the required accuracy from
a data consumer with the degree of privacy that data owners desire. Furthermore, by handpicking
the data sources, the diﬀerentially private algorithm incurs a bias that makes the output more
accurate while maintaining the desired privacy from the data owners. Lastly, the authors derive an
optimal data contract to minimize payment while satisfying accuracy and privacy.

Proposes a framework for counting trading range query results, and designs a pricing approach for
the traded results. Firstly, the framework calculates the range counts approximately, and secondly,
it protects the results further by using diﬀerential privacy, while satisfying the accuracy demands
of data consumers. The authors also design the pricing scheme in a way that prevents arbitrage.

Designs an online auction with two stages, whereby a trusted auctioneer aggregates data from data
owners and applies diﬀerential privacy before selling the data to consumers. In the ﬁrst stage,
the auctioneer selects data owners based on their privacy requirements to maximize proﬁt. In the
second stage, the auctioneer applies diﬀerential privacy to the aggregated data and subsequently
sells the data to a consumer in an auction.

Implements an end-to-end privacy-enhancing decentralized data marketplace for data consumers
to train machine learning algorithms. The authors achieve end-to-end privacy by protecting in-
puts with asymmetric encryption and diﬀerential privacy, and the execution with trusted execution
environments. More speciﬁcally, diﬀerential privacy prevents the weights of machine learning al-
gorithms from overﬁtting to the inputs. Because of the privacy limitations of current distributed
ledger technology applications, the authors of this study and of the subsequent publication [S25]
created a novel concept unlike any other blockchain-based system. For example, in principle, the
smart contracts of their architecture may contain machine learning algorithms which may be ex-
ecuted in trusted execution environments, hold privacy policies and payment logic, and point to
where encrypted data and decryption keys are stored privately. On the other hand, data consumers
also deploy smart contracts that may interact with the data owners.

Develops an auction framework for privacy-enhancing data aggregation for mobile crowdsensing.
The auctioneer chooses data owners based on their sensing capabilities, and the data owners apply
diﬀerential privacy to their inputs sampling from a noise distribution tailored by the auctioneer for
each data owner based on its qualities. The goal of the platform is to optimize task allocation to
a set of data owners while minimizing their payment, taking into account accuracy and privacy
constraints.

Designs two locally diﬀerentially private mechanisms for truth discovery in crowd sensing, so
that the answers from edge devices are protected while being useful in aggregate. The second
mechanism provides more utility for an equal degree of privacy, and consists on the users randomly
selecting a probability distribution, and in turn, adding noise sampled thereof to their truthful
answer.

40

Table D.6: PETs and AETs in the context of anonymization (continued).

Year

Study Privacy-enhancing
approaches

Description

2018

[S21] Privacy policies and general-

ization

2017

[S47] K-anonymity
noise

and

additive

2016

[S36] K-anonymity, additive noise,

and privacy policies

Proposes a data market framework that models and enforces privacy policies dynamically for data-
intensive applications. More speciﬁcally, the authors implement a data-ﬂow-focused system with
a policy enforcement algorithm deﬁned by users and a context. In data-ﬂow computing, directed
graphs embody the application, where edges represent data streams and nodes represent functional
operators and data sources or sinks. The data is anonymized based on policies and enforced by
generalization, e.g., substituting Munich with Germany. To formalize a language to model the
privacy policies, the authors use metric ﬁrst-order temporal logic.

Models a data marketplace in which groups of users may actively monetize their data through a
mediator and a set of mobile crowd sensing service providers. The authors use a reverse auc-
tion, where users bid for performing sensing tasks. Individual users may set their own privacy
preferences, and if they are a coalition of users, they are protected by k-anonymity, t-closeness,
l-diversity and local noise addition approaches. The total coalition payoﬀ is divided among the
cooperative users based on their marginal contributions to the total data quality at the end of the
sensing service.

Designs a one-to-one privacy enhancing paradigm for a data market place in which privacy policies
and data requirements are deﬁned based on the publication record of the data owner. Because
published records of a user aggregate over time and thus accrue privacy risk, the paradigm relies
on privacy risk management, which is enforced by evaluating the risk associated with revealing yet
another piece of information with regard to the privacy requirements. This evaluation is based on
the preferences of the user, or if unfeasible, based on current regulation; furthermore, it is based on
an assessment of the background information, achieved by semantically analyzing attributes that
if released could be linked to externally available information. Ultimately, to privatise the data, the
authors propose syntactic technologies such as k-anonymity, suppression, and generalization, and
semantic ones like additive noise.

41

Table D.7: Set of examples from the selected studies describing diﬀerent data marketplace
architectures based on PETs and AETs with an underlying distributed ledger technology.

Year

Study Privacy-enhancing
approaches

Description

2020

[S3]

Distributed ledger technology,
smart contracts, decentralized
identiﬁers, digital signatures,
k-anonymity, and diﬀerential
privacy

2020

[S43] Distributed ledger technology,
smart contracts, functional en-
cryption, and zero-knowledge
proofs

2020

[S45] Distributed ledger technology,
smart contracts, trusted execu-
tion environments,
truth dis-
covery, digital signatures

2020

[S28] Distributed ledger technology,
privacy policies, diﬀerential
privacy, k-anonymity, and dig-
ital signatures

2020

[S18] Distributed ledger technology,
smart contracts, and diﬀeren-
tial privacy

Implements a framework for mobility data markets with six layers, each with a purpose and a
technology to execute. Furthermore, the framework focuses on location-based services. The iden-
tity layer uses asymmetric identity keys, i.e. a key issued only to a real person, to verify that an
entity is a real individual. The privacy layer leverages k-anonymity for Geomasking (low utility
and high privacy), and when the service needs an exact location, diﬀerential privacy for Geo-
Indistinguishability (high utility and low privacy). Moreover, the contract layer is based on smart
contracts that enforce fair trade and the resolve disputes automatically. For the private communi-
cation layer, the authors use decentralized identiﬁers (DID) [153] issued by the device of a person
itself. When devices communicate, the communication has a unique ID based on both of the DIDs,
thus, even though the communication data is persisted in a blockchain, it is nontrivial to track the
locations of a user. Consecutively, the incentive layer uses smart contracts and data brokers to
promote data exchange for a proﬁt; however, this architecture does not tackle the copy problem.
The consensus layer is based on a consortium blockchain for distributed governance among non-
anonymous honest entities. The blockchain selected was Hyperledger Iroha, chieﬂy because of its
lightweight quality that couples with deployments in IoT devices.

Proposes a privacy-enhancing decentralized data marketplace employing the Agora blockchain
with veriﬁcation technology that enable data prosumers to monetize their data. The privacy-
enhancing aspect is achieved by sending encrypted data to brokers employing a primitive called
multi-client functional encryption [87][88], which ensures that the receiver may only decrypt the
output of a formerly agreed-on function. Moreover, consumers may purchase these outputs, to-
gether with a proof of correctness from the broker by using non-interactive zero-knowledge proofs.
For the decentralized architecture, the authors employ the Agora blockchain, and atomic payments
are performed via smart contracts.

Proposes a data processing-as-a-service model based on a blockchain-based data trading ecosys-
tem, whereby neither data brokers nor consumers have access to the raw data, only to the analysis.
The use of a blockchain (Ethereum) prevents a single point of failure and allows for immutability
and transparency in transactions. Furthermore, to protect the data, the analysis results, and the pro-
cessing itself, the authors use Intel’s SGX trusted execution environment [95], in addition to the
symmetric encryption algorithm AES-256 to provide encryption and decryption within and outside
the secure environment. The architecture uses the conventional Ethereum Virtual Machine (EVM)
for traditional smart contracts, while the data analysis contracts are executed in a SGX-protected
EVM where an initial key exchange is needed. Lastly, the nodes in the network form a compute
market, i.e. multiple nodes execute the analysis and only the most frequent result is delivered to
the data consumer, and the corresponding nodes are rewarded.

Proposes an architecture for a personal data marketplace in which personal data is stored decen-
trally in a allegedly GDPR compliant manner. To accomplish this, transactions and pointers to
the data are encrypted and stored using a distributed ledger technology, namely IOTA. The data
is stored either in an interplanetary ﬁle system, or in an IOTA-based storage format. In order to
access such data, a data aggregator must request permissions through Ethereum-based smart con-
tracts (whitelists) owned by data consumers. Once the permission has been granted, the trusted
data aggregator, whose mutually agreed privacy policies are persisted in another Ethereum-based
smart contract, waits until enough data owners exist to fulﬁll a particular analysis, so that the ag-
gregator may perform k-anonymity. The data aggregator sells the anonymized data to consumers
and remunerates data owners accordingly. However, the presence of a trusted aggregator defeats to
some extent the purpose of a decentralized platform. Furthermore, the link between Ethereum and
IOTA is carried out by trusted authentication services, which allow data aggregators to decrypt the
data. Lastly, in order for data owners to grant access to their data, the authors recommend dynamic
threshold encryption [208] over centralized forms of authentication services.

Proposes a data trading approach in which privacy loss is publicly auditable and data owners set
their privacy requirements on publicly available contracts. To accomplish this, the author uses a
private Ethereum blockchain called Quorum that supports a set of built-in privacy measures, such
as private transactions, messaging, and contracts; however, this design also restricts the interactions
that are possible with smart contracts outside the private subset. Furthermore, the data owner
applies diﬀerential privacy locally before sharing the data with the consumer.

42

Table D.7: PETs and AETs in the context of DLT (continued).

Year

Study Privacy-enhancing
approaches

Description

2019

[S25] Distributed ledger technology,
trusted
environ-
execution
ments, smart contracts, privacy
policies, digital signatures, and
diﬀerential privacy

2019

[S32] Distributed ledger technology,
smart contracts, and digital sig-
natures

2019

[S20] Distributed ledger technology,
smart contracts, digital signa-
tures, and decentralized identi-
ﬁers

2019

[S12] Distributed ledger technology,
environ-

execution
trusted
ments, and digital signatures

2019

[S1]

Distributed ledger technology,
digital signatures, and hashing

Implements an end-to-end privacy-enhancing decentralized data marketplace for data consumers
to train machine learning algorithms, among other Turing-complete tasks. The architecture pro-
posed is a mature version of [S2]. Containing all the features of [S2], R. Cheng, et al. [S25]
improve the performance of a newly designed distributed ledger technology to allow for horizontal
the more nodes are added to the network, the more performant the network is, un-
scaling, i.e.
like e.g. Bitcoin or Ethereum; furthermore, the authors tackle the problem of conﬁdentiality by
separating consensus from execution, whose computations are performed in a trusted execution
environment. Horizontal scaling is achieved by allowing for parallel transaction execution, which
is, in turn, accomplished by a set of transaction schedulers, and by creating dedicated committees
for computation, storage, merging outputs, key management, and consensus. However, scalability
through restricting the degree of redundancy entails a security/integrity tradeoﬀ. Key management
committees are necessary for the use of trusted execution environments to enable conﬁdential com-
putations. The architecture uses symmetric keys for state encryption and asymmetric encryption
for concealing user inputs. The authors achieve end-to-end privacy by protecting inputs with asym-
metric encryption and diﬀerential privacy, and the execution with trusted execution environments.
More speciﬁcally, diﬀerential privacy prevents the weights of machine learning algorithms from
overﬁtting to the inputs. Because of the privacy limitations of current distributed ledger technol-
ogy applications, they created a new concept so that smart contracts allow for privacy-enhancing
features; this concept was introduced by [S2].

Proposes a privacy-enhancing fair data trading protocol. The protocol relies on the Ethereum
blockchain to achieve a decentralized nature, however, the authors claim the protocol is blockchain
agnostic. Nonetheless, despite using a decentralized network, the market manager holds non-
negligible authority, as it may trace the identity of sellers so that they can be punished monetarily
in case they misbehave. Furthermore, once the buyers have decided which data asset to purchase,
the sellers use symmetric keys to encrypt data in chunks before sending it to the buyers. Upon
receiving the data chunks, the buyer (i) challenges a set of data chunks, and upon veriﬁcation
of truthful data, (ii) employs similarity learning, a machine learning technology [209], to decide
whether to ﬁnally purchase the data. Consequently, once the buyer decides to purchase the data,
the seller and buyer interact via a payment smart contract and double-authentication-preventing
signatures [210] to ensure payment and data decryption. Lastly, in order to enhance the anonymity
of the actors, the protocol uses ring signatures [136][137].

Implements a decentralized data market architecture with secure data processing for the IoT.
To achieve decentralization, the authors rely on the distributed ledger technology IOTA, and
Ethereum-based smart contracts for subscribing to data streams. The constellation of actors con-
sists of three entities: a data provider, a consumer, and a broker; the former two entities are included
in a registry via decentralized identiﬁers [153]. The product that the consumers purchase is a key
to a data stream for a predetermined period of time, created but not accessible by the data broker.
For the consumer to attain data access in a private manner, blind signatures [61] are employed,
which enable a data broker to verify stream access keys from the data provider without ever ac-
cessing these keys. More speciﬁcally, the data provider ”blinds” the session key with the broker’s
public key and sends the blinded key to the broker, consequently, the broker certiﬁes the key with
its signature and returns the signature to the provider who removes the blinding factor to access
the stream. Lastly, to exchange stream data, an inter-planetary ﬁle system is employed.

Proposes a distributed IoT data storage system and a data trading scheme. The authors use the
blockchain for its distributed nature, immutability, and requester authentication; moreover, their
solution is blockchain agnostic. However, for the consensus algorithm, the authors rely on In-
tel’s Software Guard Extension (SGX) [95] to deploy a trusted execution environment, to per-
form ”Proof of Useful Work”. The blockchain only contains pointers (addresses) to a distributed
hash table, where the data is stored oﬀ-chain by peers of the network. Only certiﬁed data con-
sumers, e.g. other IoT devices, would be able to query addresses in the blockchain. Furthermore,
the authors employ certiﬁcateless cryptography so that the key generation center of conventional
identity-based encryption does not need to be trusted [152]. To perform the cryptographic opera-
tions, edge devices are deployed. Lastly, to share data with purchasers, the authors propose to use
either asymmetric encryption or re-encryption [211].

Prototypes a decentralized fair data trading platform. The authors rely on the Ethereum blockchain
to avoid third-party data brokers and to leverage the ledger’s immutability properties. Moreover,
data sellers utilize smart contracts to propose their data oﬀers and to interact with sellers. Sellers
include the hash of the data in the ledger so that the buyer may initiate a rebuttal if there is an
expectation mismatch. Additionally, to ensure accountability the authors rely on digital signatures
to verify that the data was encrypted using a speciﬁc key that belongs to the seller, and encrypt the
data eﬃciently using symmetric encryption. The asset traded are decryption keys, and buyers may
retrieve data as ciphertexts from untrusted storage.

43

Table D.7: PETs and AETs in the context of DLT (continued).

Year

Study Privacy-enhancing
approaches

Description

2018

[S46] Distributed ledger technology,
smart contracts, partially ho-
momorphic encryption, hash-
ing, and digital signatures

2017

[S33] Distributed

ledger

technol-
ogy, privacy policies, digital
signatures,
and
k-anonymity

hashing,

Proposes two secure, and fair data trading decentralized schemata built on the Ethereum
blockchain. One scheme enables entities to trade raw data, while the other scheme enables them
to exchange statistics. The authors chose blockchain in both schemata for its immutability, smart
contracts, P2P payment, and disintermediation. Furthermore, for the second scheme, the authors
use partially homomorphic encryption to perform conﬁdential statistics. For the data structure
to compute the statistics, the authors chose a Merkle Accumulative Tree, where the leaf nodes
hold the encrypted data and the non-leaf nodes contain the hash values and a cumulative sum of
homomorphic ciphertexts. The data exchanged is veriﬁable through digital signatures based on
asymmetric encryption.

Prototypes a decentralized data market platform for anonymized data. The underlying distributed
ledger technology is Hyperledger Fabric, whose peers act as data brokers. The data brokers may
only handle datasets based on a set of privacy policies in the interest of the data owner and dic-
tated by a data domain-speciﬁc privacy policy manager. The blockchain acts as an auditable
ledger for transactions between data brokers and consumers, while the exchange of data is han-
dled oﬀ-chain. Furthermore, the anonymization of data is suggested to be performed employing
k-anonymity by the broker upon dataset reception from a secure channel, however, the solution
remains anonymization-agnostic. For every actor to verify that the correct anonymized dataset
has been shared, the broker sends its hash value using SHA-256 to the blockchain before send-
ing it to the consumer. Upon reception, both the policy manager and data receiver may verify
the dataset. Lastly, cryptography technologies are employed to encrypt the dataset symmetrically
(128-bit AES) before sharing the dataset to the consumer, and the actors use ECDSA to sign con-
ﬁrmations and transactions.

44

Appendix E. Distribution of selected studies by publication channels

Table E.8: Publication channels for the studies from our SLR.

Publication source

Type

No. %

#

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

ACM International Conference Proceeding Series
IEEE International Conference on Internet of Things
VLDB Endowment
ACM Conference on Computer and Communications Security
ACM International Workshop on Mobile Commerce
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
ACM SIGMOD Record
CEUR Workshop
Computer Law and Security Review
Computer Networks
Concurrency Computation: Practice and Experience
Conference on Information and Knowledge Management
Electronic Markets
IACR Cryptology ePrint Archive
IEEE Access
IEEE Cloud Computing
IEEE Communications Magazine
IEEE Computer
IEEE Eurasia Conference on IOT, Communication and Engineering
IEEE European Symposium on Security and Privacy
IEEE International Conference on Big Data
IEEE International Conference on Blockchain and Cryptocurrency
IEEE International Conference on Collaboration and Internet Computing
IEEE International Conference on Pervasive Computing and Communications Workshops
IEEE International Conference on Software Engineering Research, Management and Applications
IEEE Internet Computing
IEEE Internet of Things Journal
IEEE International Conference on Parallel and Distributed Processing with Applications, Big Data and
Cloud Computing, Sustainable Computing and Communications, Social Computing and Networking
IEEE Symposium on Reliable Distributed Systems
IEEE Transactions on Information Forensics and Security
IEEE Transactions on Knowledge and Data Engineering
IEEE Transactions on Network Science and Engineering
IEEE Transactions on Services Computing
IEEE Wireless Communications
Information Sciences
International Conference on Distributed Computing Systems
International Conference on Smart Systems and Technologies
International Conference on Software Engineering
International Symposium on Mobile Ad Hoc Networking and Computing
International Workshop on Security and Privacy in Big Data
International Workshop on Social Sensing
LNCS 7299 – Intelligence and Security Informatics
Online Information Review
Security and Communication Networks
Sensors
Transportation Research Part C: Emerging Technologies

29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47 Workshop on the Economics of Networks, Systems and Computation

Conference
Conference
Journal
Conference
Workshop
Conference
Journal
Workshop
Journal
Journal
Journal
Conference
Journal
Journal
Journal
Journal
Journal
Journal
Conference
Conference
Conference
Conference
Conference
Conference
Conference
Journal
Journal
Conference

Conference
Journal
Journal
Journal
Journal
Journal
Journal
Conference
Conference
Conference
Conference
Workshop
Workshop
Journal
Journal
Journal
Journal
Journal
Workshop

2
2
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

4
4
4
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

Total

50

100

45

Appendix F. Electronic data sources and inclusion and exclusion criteria

Table F.9: Electronic data sources (SDS) used in automated search.

ID

Name (Acronym)

Website

EDS1

IEEE Xplore (IEEE)

https://ieeexplore.ieee.org/

EDS2

ACM Digital Library (ACM)

https://dl.acm.org/

EDS3

ISI Web of Science (WoS)

https://www.webofknowledge.com

EDS4

ScienceDirect (SD)

https://www.sciencedirect.com/

EDS5

SpringerLink (SL)

https://link.springer.com/

EDS6 Wiley InterScience (WIS)

https://onlinelibrary.wiley.com/

EDS7

SCOPUS (SCOPUS)

https://www.scopus.com/

Table F.10: Selection criteria used to identify relevant papers. Fulﬁlling only one exclusion criterion discards the publication from being included.

ID

Facet

Inclusion criterion

Exclusion criterion

F1

Coarse focus

F2

Narrow focus

The privacy and data market topic must be within the
ﬁeld of computer science and technology

Any other privacy and data market sub-
ﬁeld

The paper must explicitly focus on privacy within data
marketplaces within the deﬁned applications

The paper does not explicitly address
this research direction

F3

Publication channel type

Conference publication OR journal publication (full
text) OR workshop publication

The paper is any other type of publica-
tion

F4

F5

F6

F7

Language

Duplicates

Peer-review

English

Non-English

Publications are new to the ﬁltering process

Publication has already been processed

The publication has been peer-reviewed

The publication is a grey publication

Full-text access

TUM-Access granted

TUM-Access not granted

46

Appendix G. Figures of the metadata analysis

Figure G.12: Map of most active countries in the ﬁeld of privacy-enhancing data markets for the IoT research.

Table G.11: Classiﬁcation scheme of research types as described by [212].

Research type

Description

Evaluation research

The authors implement existing techniques, and the solutions are evaluated in practice.

Philosophical papers

Solution proposal

These studies present a new perspective on existing research by organizing the domain into a
taxonomy or a conceptual framework.

The authors propose a solution to a problem. The solution can be either novel or a signiﬁcantly
enhanced version of an existent technique. A small example or argumentation demonstrates the
beneﬁt and applicability of the solution.

47

United States13 Canada1 Germany4China14United Kingdom3 Australia2Finland1 Spain1Singapore2Japan1Taiwan1Estonia1Italy2Croatia1 Austria1Korea2© 2020 Mapbox © OpenStreetMapFigure G.13: Mapping of contribution types against research types and approaches.

Figure G.14: Distribution of PETs and AETs explicitly employed in the corpus of selected studies.

48

2963114321113171111111111ResearchtypeResearchapproachContribution typeLessons learned1 (2%)Guideline1 (2%)Framework/Method34 (68%)Model11 (22%)Theory3 (6%)Solution proposal35 (70%)Philosophicalpaper12 (24%)Evaluation research 3 (6%)Casestudy2 (4%)Design and creation38 (76%)Not stated2 (4%)Survey4 (8%)Theoretical4 (8%)112233344577131628051015202530Pseudonym creationZero-knowledge proofsTruth discoveryDecentralized identifiersPrivacy-preserving data miningTrusted execution environmentsPerturbativeSecure multiparty computationHashingK-anonymityPrivacy policiesHomomorphic encryptionDifferential privacyDistributed ledger technologyDigital signatures and encryptionNumber of studiesFigure G.15: Distribution of studies over publication domains.

Figure G.16: Number of studies per research approach over time.

Figure G.17: Distribution of research outcomes over time.

49

213215563259111210246810121416182002200320042005200620072008200920102011201220132014201520162017201820192020Number of studiesYearJournalConferenceWorkshop000000000111000000000223271471211111110246810121416182002200320042005200620072008200920102011201220132014201520162017201820192020Number of studiesYearCase StudyDesign and creationSurveyTheoreticalNot stated11123411111213371160246810121416182002200320042005200620072008200920102011201220132014201520162017201820192020Number of studiesYearFramework/MethodGuidelineLessons learnedModelTheory