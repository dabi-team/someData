2
2
0
2

n
a
J

6

]

R
C
.
s
c
[

1
v
9
8
9
1
0
.
1
0
2
2
:
v
i
X
r
a

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

SPDL: Blockchain-secured and
Privacy-preserving Decentralized Learning

Minghui Xu, Member, IEEE, Zongrui Zou,Ye Cheng, Qin Hu, Member, IEEE,
Dongxiao Yu, Senior Member, IEEE, Xiuzhen Cheng, Fellow, IEEE

Abstract—Decentralized learning involves training machine learning models over remote mobile devices, edge servers, or cloud
servers while keeping data localized. Even though many studies have shown the feasibility of preserving privacy, enhancing training
performance or introducing Byzantine resilience, but none of them simultaneously considers all of them. Therefore we face the
following problem: how can we efﬁciently coordinate the decentralized learning process while simultaneously maintaining learning
security and data privacy? To address this issue, in this paper we propose SPDL, a blockchain-secured and privacy-preserving
decentralized learning scheme. SPDL integrates blockchain, Byzantine Fault-Tolerant (BFT) consensus, BFT Gradients Aggregation
Rule (GAR), and differential privacy seamlessly into one system, ensuring efﬁcient machine learning while maintaining data privacy,
Byzantine fault tolerance, transparency, and traceability. To validate our scheme, we provide rigorous analysis on convergence and
regret in the presence of Byzantine nodes. We also build a SPDL prototype and conduct extensive experiments to demonstrate that
SPDL is effective and efﬁcient with strong security and privacy guarantees.

Index Terms—Decentralized Learning; Byzantine resilience; Blockchain; Privacy preservation

(cid:70)

1 INTRODUCTION

With the increasing amount of data and growing complexity
of machine learning models, there is a rigid demand for
utilizing computational hardware and storage owned by
various entities in a distributed network. State-of-the-art
distributed machine learning schemes adopt three major
network topologies shown in Fig. 1. Federated learning
[1] utilizes an efﬁcient centralized network illustrated in
Fig. 1(a), where a parameter server aggregates gradients
computed by distributed devices and updates the global
model for them while preserving privacy since devices
compute locally without communicating with each other.
The fragility of the centralized network topology lies in that
a centralized parameter server suffers from the single point
of failure problem (a server might crash or be Byzantine). To
solve this issue, El-Mhamdi et al. [2] proposed a Byzantine-
resilient learning network shown in Fig. 1(b), which substi-
tutes the centralized server with a server group in which no
more than 1/3 servers can be Byzantine.

In this paper, we make a step further to break the barriers
among the parameter servers and the computational de-
vices, and allow all nodes to train models in a decentralized
network, as shown in Fig. 1(3). Such a decentralized net-
work is frequently adopted in ad hoc networks, edge com-
puting, Internet-of-Things (IoT), decentralized applications
(Dapp), etc. It can greatly unleash the potential for build-
ing large-scale (even worldwide) machine learning models
that can reasonably and fully maximize the utilization of
computational resources [3]. Besides, devices such as mobile

M. Xu, Z. Zou, Y. Cheng, D. Yu, and X. Cheng are with the School of
Computer Science and Technology, Shandong University, Qingdao, 266510,
P. R. China. E-mail: mhxu@sdu.edu.cn; zou.zongrui@mail.sdu.edu.cn;
chengye0311@163.com; {xzcheng, dxyu}@sdu.edu.cn
Q. Hu is with the Department of Computer and Information Science, Indiana
University-Purdue University Indianapolis, USA. E-mail: qinhu@iu.edu

Fig. 1. Three major network topologies adopted in distributed learning

phones, IoT sensors and vehicles are generating a large
amount of data nowadays. A decentralized network ﬁlled
with real-time big data can lead to tremendous improve-
ments in large-scale applications, e.g.,
illness detection,
outbreak discovery, and disaster warning, which involve a
large number of decentralized edge and cloud servers from
different regions and/or countries. However, this poses a
new critical challenge: How can we efﬁciently coordinate the
decentralized learning process while simultaneously maintaining
learning security and data privacy?

Speciﬁcally, decentralized learning confronts with the
following challenges. 1) Without a fully trusted centralized
custodian, users have no incentive but is reluctant to par-
ticipate in the learning process due to the lack of trust in a
decentralized network. Therefore, it is highly possible that
the volume of data might be insufﬁcient to train a reli-
able model. 2) In decentralized learning, a Byzantine node
who can behave arbitrarily (e.g., crash or launch attacks)
might prevent the model from convergence or interrupt
the training process. 3) It is challenging to make the trade-
off between privacy & security as well as efﬁciency, and
to make data sharing frictionless with privacy and trans-
parency guarantees.

This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version
may no longer be accessible.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

To overcome the above challenges, we propose SPDL,
a decentralized learning framework that simultaneously
ensures strong security using blockchain (as an immutable
distributed ledger), BFT consensus, and BFT GAR, and
preserves privacy utilizing local gradient computation and
differential privacy (DP). Blockchain, as a key component,
maintains a complete, immutable, traceable record of the
machine learning process, covering user registration, gra-
dients by rounds, and model parameters. With blockchain,
a user can identify illegal and Byzantine peers, update
its local model without concerns, and ultimately trust the
SPDL scheme. The BFT consensus algorithm and the BFT
GAR are embedded into the blockchain. Concretely, the
BFT consensus algorithm ensures consistency of model tran-
sition during multiple rounds while the BFT GAR offers
an effective method of detecting and ﬁltering Byzantine
gradients at each round. Concerning privacy, we let nodes
compute gradients with their local training data, and only
share perturbed gradients with peers, which provides a
strong privacy protection.

Our contributions are summarized as follows

2)

1) To our best knowledge, this is the ﬁrst secure and
privacy-preserving machine learning scheme for de-
centralized networks in which learning processes
are free from trusted parameter servers.
SPDL makes use of DP for data privacy protec-
tion and seamlessly embeds BFT consensus and
BFT GAR into a blockchain system to beneﬁt
model training with Byzantine fault tolerance, trans-
parency, and traceability while retaining high efﬁ-
ciency.

3) We conduct rigorous convergence and regret anal-
yses on SPDL in the presence of Byzantine nodes,
build a prototype, and carry out extensive experi-
ments to demonstrate the feasibility and effective-
ness of SPDL.

This paper is organized as follows. Section 3 outlines
the necessary preliminary knowledge needed by the de-
velopment of SPDL. Section 4 details the SPDL protocol.
The analysis on convergence and regret are presented in
Section 5. Evaluation results are reported in Section 6. We
summarize the most related work in Section 2 and conclude
this paper in Section 7.

2 RELATED WORK
2.1 Privacy and Byzantine Resilience in Distributed
Learning

Private learning schemes include secure multiparty com-
putation, encryption, homomorphic encryption, differential
privacy, and aggregation models. For details, we recom-
mend two comprehensive surveys [4], [5] to the interested
readers. Su and Vaidya [6] introduced the distributed opti-
mization problem in the presence of Byzantine failures. The
problem was formulated as one in which each node has
a local cost function, and aims to optimize the global cost
function. The proposed method, namely the synchronous
Byzantine gradient method (SBG), ﬁrst trims the largest f
gradients and the smallest f gradients, then computes the
average of the minimum and the maximum of the remaining

2

N − 2f values. This approach sheds light on providing
byzantine resilience for distributed learning. Following this
idea, many byzantine-resilient aggregation rules were pro-
posed. They all work towards a common goal – more pre-
cisely and efﬁciently trim the byzantine values. Blanchard et
al. were the earliest to tackle the Byzantine resilience prob-
lem in distributed learning by the Krum algorithm which
can guarantee convergence despite f Byzantine workers
(in a server-worker architecture). Krum stimulates the ar-
rivals of many BFT aggregation rules including Median [7],
Bulyan [8], and MDA [2]. A recent work [9] demonstrates
that these aggregation rules can function together with the
DP technique under proper assumptions.

2.2 Blockchain-Enhanced Distributed Learning

The BinDaaS [10] framework provides a blockchain-based
deep learning service, ensuring data privacy and conﬁden-
tiality in sharing Electronic Health Records (EHRs). With
BinDaaS, a patient can mine a block ﬁlled with a private
health record, which can be accessed by legitimate doctors.
BinDaaS trains each model based on a patient’s private
EHRs independent of others’ data. In contrast, decentral-
ized learning intends to train a global model using the
data owned by individual nodes, arising more security and
privacy concerns. Hu et al. [11] utilized a blockchain and
a game-theoretic approach to protect the user privacy for
federated learning in mobile crowdsensing. The collective
extortion (CE) strategy was proposed in [?] as an incentive
mechanism that can regulate workers’s behavior. These two
game-based approaches cannot strictly guarantee the safety
of model training against byzantine nodes. Lu et al. [12]
developed a learning scheme that protects privacy by differ-
ential privacy, and proposed the Proof of Training Quality
(PoQ) consensus algorithm for model convergence. This
scheme does not consider byzantine users who might dis-
turb the training processes by proposing erroneous model
parameters.

FL-Block [13] allows end devices to train a global model
secured by a PoW-based blockchain. LearningChain [14] is
a differential privacy based scheme to protect each party’s
data privacy, also resting on a PoW-based blockchain.
Warnat-Herresthal et al. [15] proposed the concept of swarm
learning which is analogous to decentralized learning, in
which each node can join the learning process managed
by Ethereum (again, PoW-based) and smart contracts. Com-
pared to a pure adoption of centralized federated learning
schemes, PoW-based blockchains can help avoid the single
point of failures and mitigate poisoning attacks caused by a
central parameter server. With PoW, a miner can propose a
block containing model parameters used for a global model
update. However, this method cannot rigorously prevent
malicious miners from harming the training processes by
proposing wrong updates. The PoW consensus itself is not
sufﬁcient to judge whether given parameters are byzantine
or not. Therefore, PoW-based blockchains are vulnerable to
poisoning attacks launched by byzantine nodes. Besides,
PoW-based blockchains are hard to scale since PoW can
incur much overhead and result in heavy consumption
of computational resources. Another noteworthy system is
Biscotti [16], which utilizes a commitment scheme to protect

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

data privacy and adopts a novel Proof-of-Federation based
blockchain for security guarantee in decentralized learning.
Using a commitment scheme, the aggregation of parameters
can be veriﬁable and tamper-proof.

In this paper, we leverage the DP technique for privacy
protection since adding noises (only needs an addition
operation using pre-calculated noise) is more efﬁcient than
employing complicated cryptographic tools. In addition,
using DP and a BFT consensus, we can ensure the byzantine
fault tolerance for the whole training process, which can not
be realized by existing works. Our unique contributions lie
in two aspects: 1) providing security and privacy guarantees
for the complete training process, by seamlessly integrating
DP, BFT aggregation, and blockchain in one system, con-
sidering byzantine nodes; and 2) offering rigorous analysis
on the Byzantine fault-tolerance, convergence and regret for
decentralized learning.

3 PRELIMINARIES
3.1 Decentralized Learning

With the prosperity of machine learning tasks that need to
process massive data, distributed learning was proposed to
coordinate a large number of devices to complete a training
process so as to achieve a rapid convergence. In recent years,
the research on distributed learning was mainly carried out
along two directions: centralized learning and decentralized
learning, whose schematic diagrams are shown in Fig. 1.
A centralized topology uses a parameter server (PS) to
coordinate all workers by gathering gradients, performing
local updates, and broadcasting new parameters; while in
a decentralized topology, all nodes are considered as equal
and exchanges information without the intervention of a PS.
Decentralized learning has many advantages over cen-
tralized learning. In [3], Lian et al. rigorously proved that
a decentralized algorithm has lower communication com-
plexity and possesses the same convergence rate as those
under a centralized parameter-server model. Furthermore,
a centralized topology might not hold in a decentralized
network where no one can be trusted enough to act as
a parameter server. Therefore, We consider the following
decentralized optimization during a learning process:

min
x∈RN

f (x) =

1
N

N
(cid:88)

Eξ∼DiF (x; ξ),

i=1
where N is the network size, Di is the local data distribution
for node i, F (x; ξ) denotes the loss function given model
parameter x and data sample ξ. Let fi(x) = Eξ∼DiF (x; ξ).
In a fully-connected graph, during any synchronous round,
each node performs a deterministic aggregation function
(e.g. average function) K on perturbed gradients received
from all other peers to update its local parameter, i.e.,
i − γ · K(g(t)

2 , · · · , g(t)

1 , g(t)

x(t+1)
i

= x(t)

n ),

round t. In the rest of this paper we omit the subscript if all
peers have the same local parameter.

3

3.2 Blockchain Basics

Blockchain, as a distributed ledger, refers to a chain of blocks
linked by hashes and spread over all the nodes in a peer-
to-peer network, namely blockchain network. A full node
stores a full blockchain in its local database. A blockchain
starts from a genesis block, and each block except for the
genesis block is chained to a previous block by referencing
its hash. Typically, there are two categories of blockchain
systems based on scale and openness: permissioned and
permissionless. In this paper, we adopt a permissioned
blockchain since it can provide faster speed and more re-
stricted registration control than permissionless ones.

three major

Blockchain consists of

components:
blockchain network, distributed ledger, and consensus algo-
rithm. A blockchain system organizes registered nodes into
a P2P network, formulating a complete graph. A distributed
ledger is immutable and can be organized as a chain, a
Direct Acyclic Graph (DAG), or a mesh. In this paper, we
use a chain as the data structure of our ledger. As the core of
a blockchain system, the consensus process determines how
to append a new block to the chain. Two types of consen-
sus algorithms are commonly adopted: proof-of-resources
and message passing. Proof-of-resources means that nodes
compete for proposing blocks by demonstrating their uti-
lization of resources, e.g., computational resources, stake,
storage, memory and speciﬁc trust hardware. On the other
hand, message passing based consensus has been widely
researched in the area of distributed computing. Such algo-
rithms always provide clear assumptions on nodes’ faulty
behaviors such as fail-stop and Byzantine attacks. In this
paper, we leverage Byzantine fault tolerance (BFT) consen-
sus algorithm, which can address Byzantine nodes who can
launch arbitrary attacks.

3.3 Gradient Aggregation Rule (GAR)

A Gradient Aggregation Rule (GAR) is used to aggregate
gradients received from peers during each round. A tradi-
tional GAR averages gradients to eliminate errors. Concern-
ing gradients generated by Byzantine nodes, a GAR can be
more elaborately designed to inject robustness. For example,
Krum and Multi-Krum are the pioneering GARs that satisfy
Byzantine resilience [17]. The essence behind these two
GARs are to choose the gradient with the closest (N − f )
(N denotes the network size and f denotes the number
of Byzantine nodes) neighbors based on the assumption
that the honest majority should have similar gradients.
Median [7] and MDA [2] are another two GARs that adopt
analogous ideas to ensure the BFT gradient aggregation. In
this paper, SPDL leverages Krum as the BFT GAR but is not
limited to it.

where γ is the learning rate. Notice that since K is deter-
ministic, all nodes should have exactly the same parameter
and should initialize it with the same value. In this case, we
denote by A an arbitrary synchronous distributed learning
algorithm that updates the mutual parameter x, and use
A(D1, D2, · · · , Dn; t) to denote the output parameter x at

3.4 Differential Privacy

The privacy guarantee is of vital importance if the nodes
carrying out decentralized learning do not admit their local
training data to be shared. Although each node communi-
cates with its neighbors by transmitting parameters instead

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

of sending raw data, the risk of leaking information still
exists [18]. Differential privacy is an effective method to
avoid leaking any information of a single individual by
adjusting the feedback of the query operations, no matter
what auxiliary information the adversary node might have.
In decentralized learning, the process of exchanging param-
eters involves a sequence of queries. The differential privacy
in our setup can be formally deﬁned as follows:

Deﬁnition 1. (((cid:15), δ)-differential-privacy) Denote by A an ar-
bitrary synchronous distributed learning algorithm that updates
model parameter x. For any node i in a decentralized system and
any two possible local data-sets Di and D(cid:48)
i with Di differing
from D(cid:48)
i by at most one record, if for any round t and any
S ⊆ Range(A), it holds that

P r (A(D, Di; t) ∈ S) ≤ e(cid:15)P r(A(D, D(cid:48)

i; t) ∈ S) + δ,

we then claim that A preserves ((cid:15), δ)-differential-privacy, and
pack all Dj(j (cid:54)= i) into D.
Deﬁnition 2. For any function f : D → RN , the L2-sensitivity
of f is deﬁned as

TABLE 1
Summary of Notations

4

Symbol
˜g(t)
i
G(t)
i
g(t)
i
η
x(t)
ξ(t)
i
n, f
σ2
σ2
f
∆(t)
k , BC(t)
B(t)
(cid:15), δ
d
N (i)

k

Description
the gradient computed by i in round t
the random noise added to i’s gradient in round t
the perturbed gradient to be transmitted in round t
the learning rate
model parameters at the end of round t
datasets randomly sampled from local datasets Di
the number of nodes and Byzantine nodes
the variance of Gaussian noise

the upper bound of E(cid:107)E(g(t)

i

) − g(t)

i (cid:107)2

the output of BFT GARs
block and blockchain in t-th round of k-th epoch
budgets of differential privacy
the dimension of gradients
the neighbors of node i

to mislead correct nodes, and make incorrect votes during
the consensus process. When i chooses not to send any data
in a synchronous round t, it’s neighbor acts like receiving
g(t)
i = 0.

∆2f = max
d1,d2

||f (d1) − f (d2)||,

4.2 Design Objectives

for all d1, d2 differing in at most one element.

In this subsection, we brieﬂy summarize our design goals.

Differential privacy can be realized by adding Gaussian
noises to the query results [19]. The following lemma,
proved in [20], demonstrates how to properly choose a
Gaussian noise.

Lemma 1. For each node transmitting gradients perturbed by
Gaussian noise with distribution N (0, σ2), the gradient ex-
changes within T successive rounds preserve ((cid:15), δ)-differential-
privacy as long as σ ≥ CT γ(cid:112)2 ln(1.25/δ)/(cid:15), where C =
∆2g(t) and γ is the learning rate.

In brief, the differential private scheme we employed
in this paper is sketched as follows. For any node i, we
use random Gaussian noise to perturb its gradients before
transmitting to other nodes. When nodes obtain the result of
the aggregation function K whose inputs are their perturbed
gradients, the differential privacy for node i is guaranteed.

4 THE PROTOCOL

4.1 Model and Assumptions

We focus on scenarios where nodes are able to communicate
with each other in a decentralized network. Speciﬁcally, we
formalize the decentralized communication topology as a
directed and fully connected graph G = (V, E), where
V (|V | = N ) denotes the set of all peers and for any
i,j ∈ V ,we have (i, j) ∈ E. Time is divided into epochs
(denoted by k), with each consisting of synchronous rounds
(denoted by t), and a model can be trained within each
epoch. We denote frequently-used notations of transaction,
k , BC (t)
block, blockchain, chain of block headers, by tx, B(t)
k ,
and BH (t)

k , respectively.

We assume the network is unreliable with at most f
possible Byzantine nodes, and N = 3f + 1 . A Byzantine
node i can behave arbitrarily. For example, it may refuse to
compute gradient, transmit arbitrary but unlawful gradients

1) Decentralization: SPDL should work in a decentral-
ized network setting without the intervention of any
centralized party such as a parameter server.
2) Differential Privacy: SPDL should guarantee ((cid:15), δ)-
DP by adding random Gaussian noises to perturb
gradients. Meanwhile we aim to reach a balance
between privacy leakage and convergence rate.
3) Byzantine Fault-Tolerance: SPDL can ensure conver-
gence against at most f (N = 3f + 1) Byzantine
nodes who can behave arbitrarily.
Immutability, Transparency, and Traceability: the
full record of the machine learning process should
be immutable and transparent, and provide trace-
ability enabling Byzantine node detection.

4)

4.3 Protocol Details

4.3.1 Initialization
Initially, each node creates a pair of private key sk and public
key pk, and generates its unique 256-bit identity id based
on pk. Public keys and identities are broadcast through the
network to be publicly known by all nodes. Then a genesis
block B0 is created, which records the information of the
nodes who initially participate in the blockchain network.
A new coming node should have a related tx being added
to the blockchain before joining the permissioned network,
where tx contains necessary information including its pk,
id, IP address, etc. Initially, all nodes have an identical
reputation value, that is wi = ˆw. Each node initializes itself
with a learning rate γ, the number of total rounds T , and
the variance of Gaussian noise for perturbing the gradients.
For simplicity and consistency of the iterating process, we
assume all nodes start the learning procedure with the
same initial parameter value x(0)
. After initialization, nodes
i
undergo a leader election process to determine who is in
charge of the training process.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Fig. 2. SPDL workﬂow (t-th round)

4.3.2 Leader Election

Algorithm 1: Leader Election
1 (hi, πi) = VRF(ski, seed)
2 Li = (hi, πi, idi)
3 Broadcast (hi, πi) to the network and receive (h, π)

form peers
4 while TRUE do
5

if receive (hj, πj) from idj &&
VerifyVRF(pk, hj, πj, seed)=1 then

6

7

8

add (hj, πj, idj) to Li
if Time() > start + δ1 then

select the largest hmax from Li and obtain
idmax
Output: idmax

Each node executes the leader election algorithm shown
in Algorithm 1. The algorithm is based on the veriﬁable
random function (VRF), which takes as inputs a private
key sk and a random seed, and outputs a hash string
h as well as the corresponding proof π. Each contender
broadcasts (hi, πi) to the network and receives (h, π) from
its peers. Note that each node is assigned with a reputation
variable r ∈ [0, 1]. A node i with ri = 0 is prohibited
from being a leader. Then the node with the largest h and
r > 0 is recognized as the leader who is responsible for the
blockchain consensus. The case when more than one leaders
are selected is extremely small since h has a large space of
2256 if we adopt the commonly used SHA-256; but if this
extreme case happens, all nodes relaunch the leader election
process to ensure that only one leader is ﬁnally selected.
We also set a timeout for the leader election process as
Time() > start+δ1, where Time() extracts the current UNIX

time. To summarize, our leader election algorithm achieves
the following three basic functionalities:

• A node with a zero reputation value has no right of

being a leader.

• The leader election process possesses full random-

ness and unpredictability properties.

• A Byzantine node cannot disguise itself as a leader
since VRF ensures that the proof h is unforgeable.

After leader election, nodes start the round-based train-
ing process, with each round consisting of the gradient
computation and blockchain consensus processes.

4.3.3 Gradient Computation
At each round, node i exchanges its perturbed gradients
with all other nodes in the blockchain network. Speciﬁcally,
each node preserves a true stochastic gradient ˜g(t)
and a
perturbed one g(t)
to be shared. The whole exchange process
can be summarized into the following steps:

i

i

• Local Gradient computation: compute local stochas-
i = ∇Fi(x(t), ξ(t)
is ran-

tic gradient ˜g(t)
domly sampled from local dataset Di.

i ), where ξ(t)

i

• Adding noise: add random Gaussian noise to the
local gradient to be shared. The variance of the noise
is denoted by input variable σ.

• Broadcast gradients: send the perturbed local gradi-
ents to all other nodes, and receive gradients from
others at the same time.

4.3.4 Blockchain Consensus

The blockchain consensus process deeply integrates a
blockchain, a BFT consensus protocol (e.g., PBFT, Tender-
mint), and a BFT aggregation function (e.g., Krum, Me-
dian). In this paper, we adopt the Practical Byzantine Fault

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

Algorithm 2: Gradient Computation
1 Initialize: x(0)

, learning rate γ, number of total

i

rounds T , and variance of noise σ

2 for t = 0 to T − 1 do
3

i

and compute local
i = ∇Fi(x(t), ξ(t)
i )

(cid:46) Local Computation
Randomly sample ξ(t)
stochastic gradient ˜g(t)
(cid:46) Adding Noise
Randomly generate Gaussian noise
G(t)
i ∼ N (0, σ2) and add noise to the variable
g(t)
i = ˜g(t)
(cid:46) Broadcast Gradients
Broadcast g(t)
from each peer j

to the network and receive g(t)
j

i + G(t)

i

i

Algorithm 3: Blockchain Consensus
1 (cid:46) To prevent deadlock, each node starts a view

change if Time() > start + δ2

2 (cid:46) PRE-PREPARE
3 if role is leader then

4

5

6

2 , · · · , g(t)
n )

1 , g(t)
k ← MSGB(∆(t))

∆(t) = K(g(t)
B(t)
broadcast (cid:104)PRE-PREPARE, id, B(t)

k , h(cid:105)ˆσ

7 (cid:46) PREPARE
8 if role is follower then

2 , · · · , g(t)
compute ˜∆(t) = K(g(t)
n )
while receive (cid:104)PRE-PREPARE, id, B(t)
k , h(cid:105)ˆσ do

1 , g(t)

if σ and B(t)
k
then

are valid and ˜∆(t) ≈ B(t)

k .∆(t)

broadcast (cid:104)PREPARE, id, h, vote(cid:105)ˆσ

4

5

6

7

8

9

10

11

12

13 (cid:46) COMMIT
14 while receive 2f + 1 (cid:104)PREPARE, id, h, vote(cid:105)ˆσ do
15

broadcast (cid:104)COMMIT, id, vote(cid:105)ˆσ

16 (cid:46) DECIDE
17 while receive 2f + 1 (cid:104)COMMIT, id, h, vote(cid:105)ˆσ do
k , B(t)
k )
i − γ∆(t)

Append(BC (t)
x(t+1)
= x(t)
i
Update reputation

19

20

18

Tolerant (PBFT) protocol as our consensus backbone due
to its effectiveness validated by the Hyperledger Saw-
tooth. The aggregation rule used in blockchain consensus is
Krum. Concretely, the blockchain consensus consists of four
phases: PRE-PREPARE, PREPARE, COMMIT, and DECIDE.

1 , g(t)

2 , · · · , g(t)

In the PRE-PREPARE phase, the leader computes an
aggregated gradient using an aggregation function ∆(t) =
K(g(t)
n ), where K is a (b, α)-Byzantine resilient
Krum function. The core idea of Krum is to eliminate the
gradients that are too far away from others. We use Euclid
distance (cid:107)g(t)
j (cid:107)2 to measure how far two gradients
are separated. Then we deﬁne near(i) to be the set of
n − f − 2 closest gradients to g(t)
. We expect that the Krum

i − g(t)

i

function chooses one gradient g(t)
that is the “closest” to its
surrounding gradients. More precisely, the output of Krum
is one of its input gradients, and the index of this gradient
is:

i

arg min

i

(cid:88)

(cid:107)g(t)

i − g(t)

j (cid:107).

j∈near(i)

k , h(cid:105)ˆσ.

MSGB takes as input ∆(t) and forms a new block B(t)
k
which records ∆(t). Then the leader broadcasts a signed pre-
prepare message as (cid:104)PRE-PREPARE, id, B(t)

2 , · · · , g(t)

Inthe PREPARE phase, each follower computes ˜∆(t) =
K(g(t)
1 , g(t)
n ) based on its local perturbed gradi-
ents, then waits for pre-prepare messages. If a pre-prepare
message is received, the follower ﬁrst veriﬁes the digi-
tal signature σ and the block (height, block hash, etc.).
Then it compares ˜∆(t) with B(t)
k .∆(t). The requirement of
˜∆(t) ≈ B(t)
k .∆(t) +δ),
where δ is a small variation. This condition indicates that
each follower should have a similar view on non-Byzantine
gradients as the leader. If veriﬁcation is passed, the follower
broadcasts a prepare message (cid:104)PREPARE, id, h, vote(cid:105)ˆσ.

k .∆(t) means ˜∆(t) ∈ (B(t)

k .∆(t) −δ, B(t)

In the COMMIT phase, if a node receives 2f +1 valid com-
mit messages, it can broadcast a decision (cid:104)COMMIT, id, vote(cid:105)ˆσ
and enter into the following DECIDE phase.

= x(t)

In the DECIDE phase, upon receiving 2f + 1 valid
commit messages, a node can append a new block B(t)
k
to its local blockchain BCk, update its local gradient as
x(t+1)
i − γ∆(t), and ﬁnally update its reputation.
i
The reputation of a certain node i can be reduced if its
gradient deviates from the aggregated gradient by more
than π/2. Even though we do not explicitly introduce the
view change, we do have such process to address the case
when a leader is a Byzantine node. To avoid the occurrence
of a deadlock, we set a timeout in the blockchain consensus
process. If Time() > start + δ2, each node broadcasts a
view change message (cid:104)VIEW-CHANGE, id, h(cid:105)σ and waits for
other peers’ responses. Upon receiving 2f + 1 view change
messages, a node can abandon the current round.

5 THEORETICAL ANALYSIS

1 , g(t)

n−f +1, g(t)

2 , · · · , g(t)

n−f +2, · · · , g(t)

Without loss of generality, let g(t)

In this section, we provide both convergence analysis and
regret analysis on SPDL in the presence of Byzantine nodes.
n−f be the
perturbed gradients sent out by honest nodes in round
t, and g(t)
n be the gradients sent out
by possible Byzantine nodes in round t. We assume that
the gradients derived by correct nodes are independently
sampled from the random viable ˜G and that E( ˜G) = g. By
adding Gaussian noise, a perturbed gradient then can be
considered as an instance sampled from random variable
G = ˜G + G, where G follows the Gaussian distribution of
mean 0 and variance σ2. Therefore we also have E(G) = g.
In this section, if we only concentrate on a certain round t,
we omit the superscript on variables when ambiguity can
be avoided from context.

Deﬁnition 3. ((k, f )-Byzantine Resilience) Let 0 < k ≤ 1 and
f be the number of Byzantine nodes in a distributed system, then

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

our anti-Byzantine mechanism K is said to be (k, f )-Byzantine
Resilient if

h(t) = K(g(t)

1 , g(t)

2 , · · · , g(t)
n )

satisﬁes that (cid:104)Eh(t), g(t)(cid:105) ≥ k(cid:107)g(t)(cid:107)2.
Theorem 1. In round t, If (cid:107)g(cid:107)2f − 3

4 > 18dσ2

f and

(cid:15) >

(cid:112)2C ln(1.25/δ)
C2

,

where

C2 = (

(cid:107)g(cid:107)2
18d

f − 3

4 − σ2
f )

1
2 ,

(1)

our anti-Byzantine mechanism K achieves (k, f )-Byzantine Re-
silience with

k = 1 −

√

(cid:18)

d

√
3

2

2f 3
(cid:107)g(cid:107)

σ2
f +

2C 2 ln(1.25/δ)
(cid:15)2

(cid:19)

.

Proof. Denote by N (i) the set of n−f −2 closest gradients of
the i-th node, Nc(i) the collection of the perturbed gradients
in N (i) sent by correct nodes, while Nf (i) the gradients in
N (i) sent by the Byzantine nodes. Let i∗ be the index of the
gradient chosen by K, we then have

(cid:107)Eh(t) − g(cid:107)2 =


h(t) −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E

≤ E

(cid:13)

(cid:13)
(cid:13)
h(t) −
(cid:13)
(cid:13)
(cid:13)

1
|Nc(i∗)|

1
|Nc(i∗)|

(cid:88)

(gi + ri)

j∈Nc(i∗)

(cid:88)

(gi + ri)

j∈Nc(i∗)

(the above inequality holds since (cid:107) · (cid:107) is convex.)









(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

E

≤

i /∈B

(cid:88)

E

+

i∈B

(cid:13)

(cid:13)
(cid:13)
gi + ri −
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
Bi −
(cid:13)
(cid:13)
(cid:13)

1
|Nc(i)|

1
|Nc(i)|

(cid:88)

(gj + rj)

j∈Nc(i)





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:88)

j∈Nc(i)



(gj + rj)



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤

+

≤

The above inequality holds since E(ri−rj) and E(gi−gj)
are both 0. Because there are exactly n − f correct nodes, we
have

7

E

(cid:13)

(cid:13)
(cid:13)
gi + ri −
(cid:13)
(cid:13)
(cid:13)
≤ 2d(n − f )(σ2 + σ2

1
|Nc(i)|

(cid:88)

j∈Nc(i)

(gj + rj)





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

f ).
If i∗ = k is one of the Byzantine nodes, i.e., i∗ ∈ B,

E

(cid:13)

(cid:13)
(cid:13)
Bk −
(cid:13)
(cid:13)
(cid:13)

≤

≤

1
|Nc(k)|

1
|Nc(k)|

+

1
|Nc(k)|

1
|Nc(k)|

(cid:88)

j∈Nc(k)
(cid:88)

(cid:88)

j∈Nc(k)



(gj + rj)



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E(cid:107)Bk − gj − rj(cid:107)2

E(cid:107)gi + ri − gj − rj(cid:107)2

j∈Nc(i)
(cid:88)

j∈Nf (i)

E(cid:107)gi + ri − gj − rj(cid:107)2,

where i /∈ B is any correct node. By the deﬁnition of N (i),
a node labeled ζ(i) is correct, but is farther away from any
node in N (i). Therefore we have:

E

(cid:13)

(cid:13)
(cid:13)
Bk −
(cid:13)
(cid:13)
(cid:13)

1
|Nc(k)|

(cid:88)

j∈Nc(k)



(gj + rj)



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E(cid:107)gi + ri − gj − rj(cid:107)2

1
|Nc(k)|

(cid:88)

j∈Nc(i)

Nf (i)
Nc(k)
Nc(i)
Nc(k)

E(cid:107)gi + ri − gζ(i) − rζ(i)(cid:107)2

2d(σ2 + σ2

f ) +

Nf (i)
Nc(k)

(cid:88)

j /∈B∧i(cid:54)=i

E(cid:107)gi − gj + ri − rj(cid:107)2

≤ 2d(σ2 + σ2
f )

≤ 2d(σ2 + σ2
f )

Nf (i)
Nc(k)

+

(cid:18) Nc(i)
Nc(k)
(cid:18) n − f − 2
n − 2f − 2

(cid:19)

(n − f − 1)

+

b
n − 2f − 2

(cid:19)

(n − f − 1)

.

Combining the two results where i∗ is a correct node or a
Byzantine node, we have:

(cid:107)Eh(t) − g(cid:107)2 ≤ 2d(σ2 + σ2
f )

+ 2d(σ2 + σ2
f )

(cid:18)

(cid:18)

n − f +

(cid:19)

n − f − 2
n − 2f − 2

b
n − 2f − 2

(cid:19)

(n − f − 1)

If i∗ = i is one of the correct nodes, i.e., i∗ /∈ B,

E

(cid:13)

(cid:13)
(cid:13)
gi + ri −
(cid:13)
(cid:13)
(cid:13)

1
|Nc(i)|

(cid:88)

j∈Nc(i)

(gj + rj)





(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= E





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
|Nc(i)|

(cid:88)

j∈Nc(i)



(gi + ri − gj − rj)



(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:88)





=

1
|Nc(i)|2

E

=

≤

1
|Nc(i)|2

E

1
|Nc(i)|

≤ 2d(σ2 + σ2

j∈Nc(i)
f ).

(cid:88)

j∈Nc(i)

(cid:88)

(gi − gj) + (ri − rj)





(gi − gj) + (ri − rj)



j∈Nc(i)
E (cid:107)gi − gj(cid:107)2 + E (cid:107)ri − rj(cid:107)2

≤ 2d(σ2 + σ2
≤ 36df 3(σ2 + σ2

f ).

f )(f + 3 + f (f − 1) + f 2(f + 2))

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Since

(cid:15) >

(cid:112)2C ln(1.25/δ)
C2

,

then

(cid:15)2

(cid:18) (cid:107)g(cid:107)2
18d

(cid:19)

b− 4

3 − σ2
f

> 2c ln(1.25/δ).

Therefore

(cid:107)g(cid:107) > 3

√

√

3
2

2b

d(σ2

f + σ2)

1
2 .

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

Finally, we have
(cid:16)

(cid:104)Eh(t), g(cid:105) ≥

(cid:107)g(cid:107) − 3

√

√

3
2

2b

d(σ2

f + σ2)

(cid:17)

1
2

(cid:107)g(cid:107) = k(cid:107)g(cid:107)2.

Regret analysis is commonly used in online learning to
investigate the loss difference caused by two learning meth-
ods. Therefore we present a regret analysis on SPDL to show
the incurred loss difference with and without Byzantine
nodes.

In a decentralized system with Byzantine nodes, let At ∈
Rd×1 be the parameter model of a node in round t, and
Xt ∈ Rd×n be the raw data randomly sampled by nodes
in round t. The loss function in round t can be written as
F (At, Xt). Consider the destructive effect on the training
process caused by Byzantine nodes, we denote by ˜At the
parameter learned by the system per round in the absence
of Byzantine nodes. It’s meaningful to compare the gap of
loss function computed by two different parameters At and
˜At. If there is no Byzantine node, we assume ˜At is updated
by any correct gradient g(t)
ξ(t) given by a random node ξ(t) ∈
{1, 2, · · · , n}.

Deﬁnition 4. (Regret)

R(A, ˜A) =

T −1
(cid:88)

i=0

F (EAt, Xt) − F (E ˜At, Xt).

Theorem 2. If the loss function satisﬁes L1-Lipschitz continuity,
√
and L1 < 1, then R(A, ˜A) ≤ ρ
(cid:113)
6L1f 3

T ), where

T + o(

d(σ2 + σ2
f )

√

2

ρ =

1 − L1

.

Proof. By the property of L1-Lipschitz continuity, we have

T −1
(cid:88)

i=0

F (EAt, Xt) − F (E ˜At, Xt)

≤

≤

≤

T −1
(cid:88)

i=0
T −1
(cid:88)

i=1
T −1
(cid:88)

i=1

L1(cid:107)EAt − E ˜At(cid:107)

L1(cid:107)EAt−1 − E ˜At−1 + ηE

(cid:16)
h(t−1) − g(t−1)

ξ(t)

(cid:17)

(cid:107)

L1E(cid:107)At−1 − ˜At−1(cid:107) + ηL1(cid:107)Eh(t−1) − Eg(t−1)

ξ(t−1)(cid:107).

Since

E(cid:107)Eh(t) − g(t−1)(cid:107) ≤ 6f

(cid:113)

3
2

d(σ2 + σ2

f ),

we have

η(cid:107)Eh(t−1) − Eg(t−1)

ξ(t−1)(cid:107) ≤ 6ηf

3
2

Then we obtain
T −1
(cid:88)

F (EAt, Xt) − F (E ˜At, Xt)

(cid:113)

d(σ2 + σ2

f ).

i=0

≤

T −1
(cid:88)

i=1

L1E(cid:107)At−1 − ˜At−1(cid:107) + 6L1ηf

3
2

(cid:113)

d(σ2 + σ2
f )

≤ 6L1ηf

(cid:113)

3
2

d(σ2 + σ2

f ) +

6T L1ηf 3

2

(cid:113)

d(σ2 + σ2
f )

1 − L1

Thus the theorem can be immediately proved by setting

η =

1
√
T

.

6 EVALUATION
6.1 Conﬁguration

We implement SPDL with 3500 lines of Python code and
conduct
the experiments on a DELL PowerEdge R740
server which has 2 CPUs (Intel Xeon 4214R) with 24 cores
(2.40 GHz) and 128 GB of RAM. SPDL adopts the gRPC
framework, a P2P network for underlying communications,
Pytorch for machine learning libraries, and a blockchain
system with the PBFT consensus algorithm. We make SPDL
open-sourced at Github1. Nodes bootstrap by generating
key pairs using ECDSA, initializing the genesis block, es-
tablishing the gRPC connection, exchanging node list, and
joining the P2P network. We evaluate SPDL over the image
classiﬁcation of MNIST dataset, which consists of hand-
written digits of 70,000 28 × 28 images in 10 classes. The
dataset is equally divided into N groups, with each assigned
to one node. Each node can add Gaussian noise to its
local gradients with the setting of (cid:15) = 0.02 (if not stated
otherwise) and δ = 10−6. We evaluate the performance of
SPDL using the following standard metrics. 1) Test error: the
fraction of wrong predictions among all predictions, using
the test dataset. We measure the test error with respect
to rounds, network size, batch size, privacy budget, and
Byzantine ratio. 2) Latency: the latency of each round.

6.2 Evaluation Results

Convergence with Network Size: For simplicity in our
context, we denote “PURE” as the decentralized learning
scheme without leveraging any DP technique, BFT GARs,
and blockchain system, and use “DP” to represent a de-
centralized learning scheme using the DP technique only
based on “PURE”. We ﬁrst compare our SPDL with PURE
and DP schemes in a non-Byzantine environment. As shown
in Fig. 3, the test error nearly converges after 20 rounds,
but ﬂuctuates a lot when N is as small as four. When
N = 30, all schemes almost achieve the same convergence.
The SPDL and DP schemes sometimes (e.g., N = 20 or
N = 30 in our experiments) have lower test error than
PURE because adding noises could prevent the training
process from over-ﬁtting. Besides, the network size does
not impact the convergence rate and a large network size
contributes to stable convergence.

Latency: To better illustrate the latency of each round,
we divide a round into three stages: local gradient com-
putation plus adding noise whose overall time overhead is
denoted by TLGC , gradient exchange (TGE), and blockchain
consensus (TBC ). As Fig. 4 shows, TLGC , TGE and TBC are
in the same order of magnitude. TGE grows with N simply
because more nodes contend for computational resources.
The MNIST classiﬁcation task can be ﬁnished quickly (<0.1
s/round), so TLGC is lower than TBC in our experiments.
When the machine learning task becomes more difﬁcult

.

1. https://github.com/isSPDL/SPDL

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

(a) N = 4

(b) N = 10

(c) N = 20

(d) N = 30

Fig. 3. Test error evolution with various network size N = 4, 10, 20, 30.

(a) N = 4

(b) N = 10

(c) N = 20

(d) N = 30

Fig. 4. Latency of different stages (TLGC , TGE and TBC ) concerning N = 4, 10, 20, 30.

(a) BR = 0%

(b) BR = 10%

(c) BR = 20%

(d) BR = 30%

Fig. 5. Test error evolution with various Byzantine ratio BR ∈ 0%, 10%, 20%, 30%.

(e.g., 10 min/round), TBC ≈ 1s is an acceptable overhead
and could be even ignored.

Convergence in the Presence of Byzantine Nodes: We
then make a comparison of three different schemes PURE,
DP, SPDL with respect to Byzantine Ratio (BR), which is the
number of existing Byzantine nodes over N . We set N = 20
and BR ∈ 0%, 10%, 20%, 30% considering f = 33% × N ,
where BR = 0 represents the non-Byzantine case. As shown
in Fig. 5, the results of the non–Byzantine experiments indi-
cate that the three schemes can achieve similar convergence.
However, DP and PURE schemes have high test error when
BR > 0%, and fail to ensure model convergence even in the
presence of 10%N Byzantine nodes. It is clearly shown that
SPDL can still grantee the same convergence with respect to
different levels of Byzantine attacks.

Batch Size: We then present the performance of the three
schemes PURE, DP, SPDL with two different batch sizes
(abbreviated as “BS”) in Fig. 6. When BS = 10, the test error
in all deployments ﬂuctuate a lot, with the PURE scheme
outperforming others because adding noises can perturb the
model convergence. However, Fig. 6(b) indicates that we can
increase the batch size to ensure a stable convergence and
make our SPDL perform well as a PURE scheme.

(a) BS = 10

(b) BS = 100

Fig. 6. Test error evolution with batch sizes BS = 10, 100.

(a) N = 10

(b) N = 20

Fig. 7. Test error evolution with (cid:15) = 0.4, 0.04 and N = 10, 20.

020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDLTLGCTGETBC0.00.20.40.60.81.0Latency(s)TLGCTGETBC0.00.20.40.60.81.0Latency(s)TLGCTGETBC0.00.20.40.60.81.0Latency(s)TLGCTGETBC0.00.20.40.60.81.0Latency(s)020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Test errorPUREDPSPDL020406080100Round0.00.20.40.60.81.0Testerror(cid:15)=0.04(cid:15)=0.4020406080100Round0.00.20.40.60.81.0Testerror(cid:15)=0.04(cid:15)=0.4JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Privacy Budget: We ﬁnally test our SPDL scheme by
setting δ = 10−6 with two varying (cid:15) ∈ 0.4, 0.04. A smaller (cid:15)
represents stronger privacy guarantee. The results presented
in Fig. 7 demonstrate that when N = 10, adding noise with
(cid:15) = 0.4 or (cid:15) = 0.04 have similar convergence. However,
when N = 20, smaller (cid:15) can cause larger test error. This
implies that the tradeoff between accuracy and privacy
preservation should be carefully adjusted according to spe-
ciﬁc demands on privacy protection and model accuracy.

7 CONCLUSION

SPDL is a new decentralized machine learning scheme
which ensures efﬁciency while achieving strong security
and privacy gurantee. In particular, SPDL utilizes BFT
consensus and BFT GAR to protect model updates from
harsh Byzantine behaviors, leverages blockchain to enjoy
the beneﬁts of transparency and traceability, and adopts the
DP technique for privacy protection. We provide rigorous
theoretical analysis on the effectiveness of our scheme and
conduct extensive studies on the performance of SPDL with
variations of network size, batch size, privacy budget, and
Byzantine ratio.

REFERENCES

[1] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan,
S. Patel, D. Ramage, A. Segal, and K. Seth, “Practical secure aggre-
gation for privacy-preserving machine learning,” in proceedings of
the 2017 ACM SIGSAC Conference on Computer and Communications
Security, 2017, pp. 1175–1191.

[2] E.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, L. N. Hoang, and
S. Rouault, “Genuinely distributed byzantine machine learning,”
in Proceedings of the 39th Symposium on Principles of Distributed
Computing, 2020, pp. 355–364.

[3] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and
J. Liu, “Can decentralized algorithms outperform centralized algo-
rithms? a case study for decentralized parallel stochastic gradient
descent,” arXiv preprint arXiv:1705.09056, 2017.

[4] B. Liu, M. Ding, S. Shaham, W. Rahayu, F. Farokhi, and Z. Lin,
“When machine learning meets privacy: A survey and outlook,”
ACM Computing Surveys (CSUR), vol. 54, no. 2, pp. 1–36, 2021.
[5] Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learn-
ing: Concept and applications,” ACM Transactions on Intelligent
Systems and Technology (TIST), vol. 10, no. 2, pp. 1–19, 2019.
[6] L. Su and N. H. Vaidya, “Fault-tolerant multi-agent optimization:
optimal iterative distributed algorithms,” in Proceedings of the 2016
ACM symposium on principles of distributed computing, 2016, pp. 425–
434.

[7] D. Yin, Y. Chen, R. Kannan, and P. Bartlett, “Byzantine-robust
distributed learning: Towards optimal statistical rates,” in Interna-
tional Conference on Machine Learning. PMLR, 2018, pp. 5650–5659.
[8] R. Guerraoui, S. Rouault et al., “The hidden vulnerability of
distributed learning in byzantium,” in International Conference on
Machine Learning. PMLR, 2018, pp. 3521–3530.

[9] R. Guerraoui, N. Gupta, R. Pinot, S. Rouault, and J. Stephan,
“Differential privacy and byzantine resilience in sgd: Do they add
up?” arXiv preprint arXiv:2102.08166, 2021.

[10] P. Bhattacharya, S. Tanwar, U. Bodke, S. Tyagi, and N. Kumar,
“Bindaas: Blockchain-based deep-learning as-a-service in health-
care 4.0 applications,” IEEE Transactions on Network Science and
Engineering, 2019.

[11] Q. Hu, Z. Wang, M. Xu, and X. Cheng, “Blockchain and federated
edge learning for privacy-preserving mobile crowdsensing,” IEEE
Internet of Things Journal, pp. 1–1, 2021.

[12] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and federated learning for privacy-preserved data sharing in
industrial iot,” IEEE Transactions on Industrial Informatics, vol. 16,
no. 6, pp. 4177–4186, 2019.

10

[13] Y. Qu, L. Gao, T. H. Luan, Y. Xiang, S. Yu, B. Li, and G. Zheng,
“Decentralized privacy using blockchain-enabled federated learn-
ing in fog computing,” IEEE Internet of Things Journal, vol. 7, no. 6,
pp. 5171–5183, 2020.

[14] X. Chen, J. Ji, C. Luo, W. Liao, and P. Li, “When machine learning
meets blockchain: A decentralized, privacy-preserving and secure
design,” in 2018 IEEE International Conference on Big Data (Big
Data).

IEEE, 2018, pp. 1178–1187.

[15] S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan,
S. Mukherjee, V. Garg, R. Sarveswara, K. H¨andler, P. Pickkers,
N. A. Aziz et al., “Swarm learning for decentralized and conﬁ-
dential clinical machine learning,” Nature, vol. 594, no. 7862, pp.
265–270, 2021.

[16] M. Shayan, C. Fung, C. J. Yoon, and I. Beschastnikh, “Biscotti:
A blockchain system for private and secure federated learning,”
IEEE Transactions on Parallel and Distributed Systems, vol. 32, no. 7,
pp. 1513–1525, 2020.

[17] P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer,
“Machine learning with adversaries: Byzantine tolerant gradient
descent,” in Proceedings of the 31st International Conference on Neural
Information Processing Systems, 2017, pp. 118–128.

[18] Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi, “Beyond
inferring class representatives: User-level privacy leakage from
federated learning,” in IEEE INFOCOM 2019-IEEE Conference on
Computer Communications.

IEEE, 2019, pp. 2512–2520.

[19] H. Jiang, J. Pei, D. Yu, J. Yu, B. Gong, and X. Cheng, “Applications
of differential privacy in social network analysis: A survey,” IEEE
Transactions on Knowledge & Data Engineering, no. 01, pp. 1–1, apr
5555.

[20] D. Yu, Z. Zou, S. Chen, Y. Tao, B. Tian, W. Lv, and X. Cheng,
“Decentralized parallel sgd with privacy preservation in vehicular
networks,” IEEE Transactions on Vehicular Technology, 2021.

Minghui Xu received his PhD degree in Com-
puter Science from The George Washington Uni-
versity in 2021, and received the BS degree in
Physics from the Beijing Normal University in
2018. He is currently an Assistant Professor in
the School of Computer Science and Technol-
ogy, Shandong University, China. His current re-
search focuses on blockchain, distributed com-
puting, and applied cryptography.

Zongrui Zou is currently working toward the
under-graduate degree with the School of Com-
puter Science and Technology, Shandong Uni-
versity, Qingdao, China. His research interests
mainly include theoretical aspects of private data
analysis and machine learning.

Ye Cheng received his bachelor’s degree in
mechanical engineering from Wuhan University
of Technology in 2018. He is working toward a
master’s degree in Computer Science and Tech-
nology at Shandong University in China. His cur-
rent research direction is blockchain and privacy
protection.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

Qin Hu received her Ph.D. degree in Computer
Science from the George Washington Univer-
sity in 2019. She is currently an Assistant Pro-
fessor with the Department of Computer and
Information Science, Indiana University-Purdue
University Indianapolis (IUPUI). Her research
interests include wireless and mobile security,
edge computing, blockchain, and crowdsourc-
ing/crowdsensing.

Dongxiao Yu received his BS degree in Math-
ematics in 2006 from Shandong University, and
PhD degree in Computer Science in 2014 from
The University of Hong Kong. He became an
associate professor in the School of Computer
Science and Technology, Huazhong University
of Science and Technology, in 2016. Currently
he is a professor at the School of Computer Sci-
ence and Technology, Shandong University. His
research interests include wireless networking,
distributed computing, and graph algorithms.

Xiuzhen Cheng received her MS and PhD de-
grees in computer science from University of
Minnesota, Twin Cities, in 2000 and 2002, re-
spectively. She was a faculty member at the
Department of Computer Science, The George
from 2002-2020. Cur-
Washington University,
rently she is a professor of computer science
at Shandong University, Qingdao, China. Her
research focuses on blockchain computing, se-
curity and privacy, and Internet of Things. She is
a Fellow of IEEE.

