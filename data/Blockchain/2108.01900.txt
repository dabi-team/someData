LACHESIS: SCALABLE ASYNCHRONOUS BFT ON DAG
STREAMS

A PREPRINT

Quan Nguyen, Andre Cronje, Michael Kong, Egor Lysenko, Alex Guzev

FANTOM

August 5, 2021

ABSTRACT

This paper consolidates the core technologies and key concepts of our novel Lachesis consensus
protocol and Fantom Opera platform, which is permissionless, leaderless and EVM compatible.
We introduce our new protocol, so-called Lachesis, for distributed networks achieving Byzantine
fault tolerance (BFT) [10]. Each node in Lachesis protocol operates on a local block DAG, namely
OPERA DAG. Aiming for a low time to ﬁnality (TTF) for transactions, our general model considers
DAG streams of high speed but asynchronous events. We integrate Proof-of-Stake (PoS) into a DAG
model in Lachesis protocol to improve performance and security. Our general model of trustless sys-
tem leverages participants’ stake as their validating power [33]. Lachesis’s consensus algorithm uses
Lamport timestamps, graph layering and concurrent common knowledge to guarantee a consistent
total ordering of event blocks and transactions. In addition, Lachesis protocol allows dynamic par-
ticipation of new nodes into Opera network. Lachesis optimizes DAG storage and processing time
by splitting local history into checkpoints (so-called epochs). We also propose a model to improve
stake decentralization, and network safety and liveness [32].
Built on our novel Lachesis protocol, Fantom’s Opera platform is a public, leaderless, asynchronous
BFT, layer-1 blockchain, with guaranteed deterministic ﬁnality. Hence, Lachesis protocol is suitable
for distributed ledgers by leveraging asynchronous partially ordered sets with logical time ordering
instead of blockchains. We also present our proofs into a model that can be applied to abstract
asynchronous distributed system.

Keywords Lachesis protocol · Consensus algorithm · DAG · Proof of Stake · Permissionless · Leaderless · aBFT ·
Lamport timestamp · Root · Clotho · Atropos · Main chain · Happened-before · Layering · CCK · Trustless System ·
Validating power · Staking model

1
2
0
2

g
u
A
4

]

C
D
.
s
c
[

1
v
0
0
9
1
0
.
8
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
A PREPRINT - AUGUST 5, 2021

Contents

1 Introduction

1.1 Motivation .

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1.2 Lachesis protocol: Proof-Of-Stake DAG aBFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

1.3 Contributions .

1.4 Paper structure

.

.

.

.

.

.

.

.

.

.

.

.

.

.

2 Related work

2.1 An overview of Blockchains

2.1.1 Consensus algorithms

2.1.2

Proof of Work .

2.1.3

Proof of Stake

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

2.1.4 DAG-based approaches .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Lamport timestamps

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

4

5

6

6

6

6

7

7

7

8

9

2.3 Concurrent common knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

3 Lachesis: Proof-Of-Stake DAG-based Protocol

3.1 OPERA DAG .

.

3.2 PoS Deﬁnitions .

3.2.1

Stake .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.2 Validating Power and Block Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2.3

Stake-based Validation .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

3.2.4 Validation Score .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 S-OPERA DAG: A weighted DAG model

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Overall Framework .

3.5 Quorum .

.

.

.

.

.

.

.

.

3.6 Event Block Creation .

3.7 Event Block Structure .

3.8 Root Selection .

.

3.9 Clotho Selection .

.

.

3.10 Atropos Selection .

.

.

.

3.11 Atropos timestamp .

3.12 Main chain .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

3.13 Topological ordering of events .

3.14 Blocks .

.

.

.

.

.

.

3.15 Block timestamp .

3.16 Peer selection .

.

.

.

.

.

.

.

.

3.17 Dynamic participants .

3.18 Peer synchronization .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

11

11

12

12

12

12

12

13

13

14

14

14

15

16

16

16

17

17

18

18

18

19

1

A PREPRINT - AUGUST 5, 2021

3.19 Detecting Forks .

.

.

.

.

.

3.20 Transaction conﬁrmations .

3.21 Checkpoint

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Staking model

4.0.1 Deﬁnitions and Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

4.0.2 Validating power .

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.0.3 Block consensus and Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.0.4 Token Staking and Delegation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

5 Opera Network

5.1 Network Architecture .

5.2 Nodes and Roles

.

.

5.3 Boot Node Service .

5.4 API Service .

5.5 Mainnet

5.6 Testnet .

.

.

.

.

.

.

.

.

.

.

.

.

5.7

Implementation .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

5.8 Performance and Statistics

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.8.1 Normal emission rate and gas allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

5.8.2 High latency nodes .

5.8.3 Emission rates

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Discussions

6.1 Comparison with Existing Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2 Protocol Fairness and Security .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.2.1

Protocol Fairness .

6.2.2

Security .

.

.

6.3 Response to Attacks .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4 Choices of Stake Decentralization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4.1 Low staked validators

6.4.2 Choices of L and U .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . .

. . . . .

. . .

. . . . . . . . . . . . . . . . . . . . . . . .

6.4.3 Alternative models for gossip and staking . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6.4.4 Network load .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7 Conclusion

7.1 Future work .

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Appendix

8.1 Basic Deﬁnitions

8.1.1 Events .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

19

20

20

20

21

21

21

22

22

22

23

23

23

23

23

24

25

25

25

26

26

26

26

27

28

29

29

29

30

30

30

30

31

31

31

2

A PREPRINT - AUGUST 5, 2021

31

32

32

32

33

33

33

34

35

35

35

36

36

37

37

37

38

39

39

40

40

40

41

42

42

42

44

8.1.2 OPERA DAG .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.1.3 Happened-Before relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

8.1.4 Lamport timestamp .

.

8.1.5 Domination relation .

8.2 Proof of aBFT .

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.1

S-OPERA DAG - A Weighted DAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.2.2 Consistency of DAGs .

8.2.3 Root .

.

8.2.4 Clotho .

.

.

8.2.5 Atropos .

8.2.6 Block .

.

.

.

.

.

.

.

.

.

8.2.7 Main chain .

8.2.8

Fork free .

.

.

.

.

.

.

.

8.3 Semantics of Lachesis .

8.3.1 Node State .

.

8.3.2 Consistent Cut

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8.3.3

State in aBFT Lachesis .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

9 Alternatives for scalability

9.1 Layering-based Model

.

.

.

.

9.1.1 Layering Deﬁnitions .

9.1.2 H-OPERA DAG .

9.1.3 Root Graph .

.

.

.

9.1.4

Frame Assignment

9.1.5 Consensus

.

.

.

9.1.6 Main procedure .

9.2 STAIR Model .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

10 Reference

.

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

A PREPRINT - AUGUST 5, 2021

1

Introduction

Interests in blockchains and distributed ledger technologies have surged signiﬁcantly since Bitcoin [30] was intro-
duced in late 2008. Bitcoin and blockchain technologies have enabled numerous opportunities for business and inno-
vation. Beyond the success over cryptocurrency, the decentralization principle of blockchains has attracted numerous
applications across different domains from ﬁnancial, healthcare to logistics. Blockchains provide immutability and
transparency of blocks that facilitate highly trustworthy, append-only, transparent public distributed ledgers and area
promising solution for building trustless systems. Public blockchain systems support third-party auditing and some of
them offer a high level of anonymity.

Blockchain is generally a distributed database system, which stores a database of all transactions within the network
and replicate it to all participating nodes. A distributed consensus protocol running on each node to guarantee con-
sistency of these replicas so as to maintain a common transaction ledger. In a distributed database system, Byzantine
fault tolerance (BFT) [23] is paramount to reliability of the system to make sure it is tolerant up to one-third of the
participants in failure.

Consensus algorithms ensure the integrity of transactions over the distributed network, and they are equivalent to the
proof of BFT [1, 22]. In practical BFT (pBFT) can reach a consensus for a block once the block is shared with other
participants and the share information is further shared with others [19, 29].

Despite of the great success, blockchain systems are still facing some limitations. Recent advances in consensus
algorithms [44, 24, 26, 17, 41, 42] have improved the consensus conﬁrmation time and power consumption over
blockchain-powered distributed ledgers.

Proof of Work (PoW): is introduced in the Bitcoin’s Nakamoto consensus protocol [30]. Under PoW, validators are
randomly selected based on their computation power. PoW protocol requires exhaustive computational work and high
demand of electricity from participants for block generation. It also requires longer time for transaction conﬁrmation.

Proof Of Stake (PoS): leverages participants’ stakes for selecting the creator of the next block [44, 24]. Validators have
voting power proportional to their stake. PoS requires less power consumption and is more secure than PoW, because
the stake of a participant is voided and burnt if found dishonest.

DAG (Directed Acyclic Graph): DAG based currency was ﬁrst introduced in DagCoin paper [26]. DAG technology
allows cryptocurrencies to function similarly to those that utilize blockchain technology without the need for blocks
and miners. DAG-based approaches utilize directed acyclic graphs (DAG) [26, 41, 42, 37, 27] to facilitate consensus.
Examples of DAG-based consensus algorithms include Tangle [38], Byteball [11], and Hashgraph [2]. Tangle selects
the blocks to connect in the network utilizing accumulated weight of nonce and Monte Carlo Markov Chain (MCMC).
Byteball generates a main chain from the DAG and reaches consensus through index information of the chain. Hash-
graph connects each block from a node to another random node. Hashgraph searches whether 2/3 members can reach
each block and provides a proof of Byzantine fault tolerance via graph search.

1.1 Motivation

Practical Byzantine Fault Tolerance (pBFT) allows all nodes to successfully reach an agreement for a block (infor-
mation) when a Byzantine node exists [7]. In pBFT, consensus is reached once a created block is shared with other
participants and the share information is shared with others again [19, 29]. After consensus is achieved, the block is
added to the participants’ chains [7, 14]. It takes O(n4) for pBFT, where n is the number of participants.

DAG-based approaches [26, 41, 42, 37, 27] and Hashgraph [2] leverage block DAG of event blocks to reach con-
sensus. The propose to use virtual voting on the local block DAG to determine consensus, yet their approach has
some limitations. First, the algorithm operates on a known network comprised of known authoritative participants aka
permissioned network. Second, gossip propagation is slow with a latency of O(n) for n participants. Third, it remains
unclear whether their consensus and ﬁnal event ordering algorithms are truly asynchronous BFT.

In addition, there is only a few research work studying Proof-of-Stake in DAG-based consensus protocols. One
example is [2], but it is brieﬂy mentioned in that paper.

Hence, in this paper, we are interested in a new approach to address the aforementioned issues in existing pBFT
and DAG-based approaches, and the new approach leverages participant’s stake to improve DAG-based consensus
decisions. Speciﬁcally, we propose a new consensus algorithm that addresses the following questions:

• Can we achieve a public Proof-Of-Stake + DAG-based consensus protocol?
• Can pBFT be achieved when time-to-ﬁnality (TTF) for transactions is kept closed to 1 second?
• Can we reach local consensus in a k-cluster faster for some k?

4

A PREPRINT - AUGUST 5, 2021

• Can we achieve faster event propagation such as using a subset broadcast?
• Can continuous common knowledge be used for consensus decisions with high probability?

1.2 Lachesis protocol: Proof-Of-Stake DAG aBFT

In this paper, we introduce our Lachesis protocol, denoted by L to reach faster consensus using topological ordering
of events for an asynchronous non-deterministic distributed system that guarantees pBFT with deterministic ﬁnality.

The core idea of Lachesis is the OPERA DAG, which is a block DAG. Nodes generate and propagate event blocks
asynchronously and the Lachesis algorithm achieves consensus by conﬁrming how many nodes know the event blocks
using the OPERA DAG. In Lachesis protocol, a node can create a new event block, which has a set of 2 to k parents.
OPERA DAG is used to compute special event blocks, such as Root, Clotho, and Atropos. The Main chain consists of
ordered Atropos event blocks. It can maintain reliable information between event blocks. The OPERA DAG and Main
chain are updated frequently with newly generated event blocks and can respond strongly to attack situations such as
forking and parasite attack.

PoS + DAG: We introduce StakeDag protocol [33] and StairDag protocol [32] presents a general model that integrates
Proof of Stake model into DAG-based consensus protocol Lachesis. Both generate blocks asynchronously to build
a weighted DAG from Validator blocks. Consensus on a block is computed from the gained validating power of
validators on the block. We use Lamport timestamp, Happened-before relation between event blocks, graph layering
and hierarchical graphs on the weighted DAG, to achieve deterministic topological ordering of ﬁnalized event blocks
in an asynchronous leaderless DAG-based system.

Figure 1: Consensus Method through Path Search in a DAG (combines chain with consensus process of pBFT)

Figure 1 illustrates how consensus is reached through the path search in the OPERA DAG. Leaf set, denoted by Rs0,
consists of the ﬁrst event blocks created by individual participant nodes. Let Rsi be the i-th root set. A root r1 in Rs1
can reach more than a quorum in the leaf set. A quorum is 2/3W, where W is the total validating power in a weighted
DAG, or 2/3n where n is the number nodes in an unweighted DAG. A root r3 in a later root set may conﬁrm some
root r(cid:48) in Rs1 and the subgraph of r(cid:48). Lachesis protocol reaches consensus in this manner and is similar to the proof
of pBFT approach.

Dynamic participation: Lachesis protocol supports dynamic participation so that all participants can join the net-
work [9].

Layering: Lachesis protocol leverages the concepts of graph layering and hierarchical graphs on the DAG, as in-
troduced in our ONLAY framework [31]. Assigned layers are used to achieve deterministic topological ordering of
ﬁnalized event blocks in an asynchronous leaderless DAG-based system.

The main concepts of Lachesis protocol are given as follows:

• Event block: Nodes can create event blocks. Event block includes the signature, generation time, transaction

history, and reference to parent event blocks.

• Happened-before: is the relationship between nodes which have event blocks. If there is a path from an event
block x to y, then x Happened-before y. “x Happened-before y” means that the node creating y knows event
block x.

• Lamport timestamp: For topological ordering, Lamport timestamp algorithm uses the happened-before relation

to determine a partial order of the whole event block based on logical clocks.

• Stake: This corresponds to the amount of tokens each node possesses in their deposit. This value decides the

validating power a node can have.

• User node: A user node has a small amount stake (e.g., containing 1 token).
• Validator node: A validator node has large amount of stake (≥ 2 tokens).

5

A PREPRINT - AUGUST 5, 2021

• Validation score: Each event block has a validation score, which is the sum of the weights of the roots that are

reachable from the block.

• OPERA DAG: is the local view of the DAG held by each node, this local view is used to identify topological

ordering, select Clotho, and create time consensus through Atropos selection.

• S-OPERA DAG: is the local view of the weighted Directed Acyclic Graph (DAG) held by each node. This local

view is used to determine consensus.

• Root: An event block is called a root if either (1) it is the ﬁrst event block of a node, or (2) it can reach more than
2/3 of the network’s validating power from other roots. A root set Rs contains all the roots of a frame. A frame
f is a natural number assigned to Root sets and its dependent event blocks.

• Clotho: A Clotho is a root at layer i that is known by a root of a higher frame (i + 1), and which in turns is known

by another root in a higher frame (i +2).

• Atropos: is a Clotho assigned with a consensus time.
• Main chain: StakeDag’s Main chain is a list of Atropos blocks and the subgraphs reachable from those Atropos

blocks.

1.3 Contributions

In summary, this paper makes the following contributions:

• We propose a novel consensus protocol, namely Lachesis, which is Proof-Of-Stake DAG-based, aiming for

practical aBFT distributed ledger.

• Our Lachesis protocol uses Lamport timestamp and happened-before relation on the DAG, for faster consensus.

Our consensus algorithms for selection of roots, Clothos and Atropos are deterministic and scalable.

• Lachesis protocol allows dynamic participation for all participants to join the network.
• Our protocol leverages epochs or checkpoints to signiﬁcantly improve storage usage and achieve faster consensus

computation.

• We deﬁne a formal model using continuous consistent cuts of a local view to achieve consensus via layer assign-
ment. Our work is the ﬁrst that give a detailed model, formal semantics and proofs for an aBFT PoS DAG-based
consensus protocol.

1.4 Paper structure

The rest of this paper is organized as follows. Section 2 gives the related work. Section 3 presents our model of
Lachesis protocol that is Proof of Stake DAG-based. Section 4 introduces a staking model that is used for our STAIR
consensus protocol. Section 5 describes the Opera network’s overall architecture, node roles and running services to
support Fantom’s ecosystem. We also give a brief performance analysis and some statistics of the Opera network.
Section 6 gives some discussions about our Proof of Stake STAIR protocol, such as fairness and security. Section 7
concludes.

2 Related work

2.1 An overview of Blockchains

A blockchain is a type of distributed ledger technology (DLT) to build record-keeping system in which its users possess
a copy of the ledger. The system stores transactions into blocks that are linked together. The blocks and the resulting
chain are immutable and therefore serving as a proof of existence of a transaction. In recent years, the blockchain
technologies have seen a widespread interest, with applications across many sectors such as ﬁnance, energy, public
services and sharing platforms.

For a public or permissionless blockchain, it has no central authority. Instead, consensus among users is paramount
to guarantee the security and the sustainability of the system. For private, permissioned or consortium blockchains,
one entity or a group of entities can control who sees, writes and modiﬁes the data on it. We are interested in the
decentralization of BCT and hence only public blockchains are considered in this section.

In order to reach a global consensus on the blockchain, users must follow the rules set by the consensus protocol of
the system.

6

A PREPRINT - AUGUST 5, 2021

2.1.1 Consensus algorithms

In a consensus algorithm, all participant nodes of a distributed network share transactions and agree integrity of the
shared transactions [23]. It is equivalent to the proof of Byzantine fault tolerance in distributed database systems [1,
22]. The Practical Byzantine Fault Tolerance (pBFT) allows all nodes to successfully reach an agreement for a block
when a Byzantine node exists [7].

There have been extensive research in consensus algorithms. Proof of Work (PoW) [30], used in the original Nakamoto
consensus protocol in Bitcoin, requires exhaustive computational work from participants for block generation. Proof
Of Stake (PoS) [44, 24] uses participants’ stakes for generating blocks. Some consensus algorithms [8, 17, 41, 42] have
addressed to improve the consensus conﬁrmation time and power consumption over blockchain-powered distributed
ledges. These approaches utilize directed acyclic graphs (DAG) [26, 41, 42, 37, 27] to facilitate consensus. Examples
of DAG-based consensus algorithms include Tangle [38], Byteball [11], and Hashgraph [2]. Lachesis protocol [10]
presents a general model of DAG-based consensus protocols.

2.1.2 Proof of Work

Bitcoin was the ﬁrst and most used BCT application. Proof of Work (PoW) is the consensus protocol introduced by
the Bitcoin in 2008 [30]. In PoW protocol, it relies on user’s computational power to solve a cryptographic puzzle
that creates consensus and ensures the integrity of data stored in the chain. Nodes validate transactions in blocks (i.e.
verify if sender has sufﬁcient funds and is not double-spending) and competes with each other to solve the puzzle set
by the protocol. The incentive for miners to join this mining process is two-fold: the ﬁrst miner, who ﬁnds a solution,
is rewarded (block reward) and gains all the transaction fees associated to the transactions. Nodes validate transactions
in blocks with an incentive to gain block reward and transaction fees associated to the transactions.

The key component in Bitcoin protocol and its successors is the PoW puzzle solving. The miner that ﬁnds it ﬁrst
can issue the next block and her work is rewarded in cryptocurrency. PoW comes together with an enormous energy
demand.

From an abstract view, there are two properties of PoW blockchain:

• Randomized leader election: The puzzle contest winner is elected to be the leader and hence has the right to

issue the next block. The more computational power a miner has, the more likely it can be elected.

• Incentive structure: that keeps miners behaving honestly and extending the blockchain. In Bitcoin this is achieved
by miners getting a block discovery reward and transaction fees from users. In contrast, subverting the protocol
would reduce the trust of the currency and would lead to price loss. Hence, a miner would end up undermining
the currency that she itself collects.

Based on those two characteristics, several alternatives to Proof-of-Work have been proposed.

2.1.3 Proof of Stake

Proof of Stake(PoS) is an alternative to PoW for blockchains. PoS relies on a lottery like system to select the next
leader or block submitter. Instead of using puzzle solving, PoS is more of a lottery system. Each node has a certain
amount of stake in a blockchain. Stake can be the amount of currency, or the age of the coin that a miner holds. In PoS,
the leader or block submitter is randomly elected with a probability proportional to the amount of stake it owns in the
system. The elected participant can issue the next block and then is rewarded with transaction fees from participants
whose data are included. The more stake a party has, the more likely it can be elected as a leader. Similarly to PoW,
block issuing is rewarded with transaction fees from participants whose data are included. The underlying assumption
is that: stakeholders are incentivized to act in its interests, so as to preserve the system.

There are two major types of PoS. The ﬁrst type is chain-based PoS [36], which uses chain of blocks like in PoW,
but stakeholders are randomly selected based on their stake to create new blocks. This includes Peercoin [18], Black-
coin [45], and Iddo Bentov’s work [4], just to name a few. The second type is BFT-based PoS that is based on BFT
consensus algorithms such as pBFT [7]. Proof of stake using BFT was ﬁrst introduced by Tendermint [20], and has
attracted more research [8]. Ethereum’s Parity project have investigated to migrate into a PoS blockchain [46].

PoS is more advantageous than PoW because it only requires cheap hardware and every validator can submit blocks.
PoS approach reduces the energy demand and is more secure than PoW.

Every validator can submit blocks and the likelihood of acceptance is proportional to the % of network weight (i.e.,
total amount of tokens being staked) they possess. Thus, to secure the blockchain, nodes need the actual native token
of that blockchain. To acquire the native tokens, one has to purchase or earn them with staking rewards. Generally,
gaining 51% of a network’s stake is much harder than renting computation.

7

A PREPRINT - AUGUST 5, 2021

Security Although PoS approach reduces the energy demand, new issues arise that were not present in PoW-based
blockchains. These issues are shown as follows:

• Grinding attack: Malicious nodes can play their bias in the election process to gain more rewards or to double

spend their money.

• Nothing at stake attack: In PoS, constructing alternative chains becomes easier. A node in PoS seemingly does
not lose anything by also mining on an alternative chain, whereas it would lose CPU time if working on an
alternative chain in PoW.

Delegated Proof of Stake To tackle the above issues in PoS, Delegated Proof of Stake (DPoS) consensus protocols
are introduced, such as Lisk, EOS [15], Steem [43], BitShares [5] and Ark [13]. DPoS uses voting to reach consensus
among nodes more efﬁciently by speeding up transactions and block creation. Users have different roles and have a
incentive to behave honestly in their role.

In DPoS systems, users can vote to select witnesses, to whom they trust, to create blocks and validate transactions.
For top tier witnesses that have earned most of the votes, they earn the right to validate transactions. Further, users
can also delegate their voting power to other users, whom they trust, to vote for witnesses on their behalf. In DPoS,
votes are weighted based on the stake of each voter. A user with small stake amount can become a top tier witness, if
it receives votes from users with large stakes.

Top witnesses are responsible for validating transactions and creating blocks, and then get fee rewards. Witnesses in
the top tier can exclude certain transactions into the next block. But they cannot change the details of any transaction.
There are a limited number of witnesses in the system. A user can replace a top tier witness if s/he gets more votes or
is more trusted. Users can also vote to remove a top tier witness who has lost their trust. Thus, the potential loss of
income and reputation is the main incentive against malicious behavior in DPoS.

Users in DPoS systems also vote for a group of delegates, who are trusted parties responsible for maintaining the
network. Delegates are in charge of the governance and performance of the entire blockchain protocol. But the
delegates cannot do transaction validation and block generation. For example, they can propose to change block size,
or the reward a witness can earn from validating a block. The proposed changes will be voted by the system’s users.

DPoS brings various beneﬁts: (1) faster than traditional PoW and PoS Stake systems; (2) enhance security and integrity
of the blockchains as each user has an incentive to perform their role honestly; (3) normal hardware is sufﬁcient to
join the network and become a user, witness, or delegate; (4) more energy efﬁcient than PoW.

Leasing Proof Of Stake Another type of widely known PoS is Leasing Proof Of Stake (LPoS). Like DPoS, LPoS
allows users to vote for a delegate that will maintain the integrity of the system. Further, users in a LPoS system can
lease out their coins and share the rewards gained by validating a block.

Proof of Authority Another successor of PoS is Proof of Authority (PoA). In PoA, the reputation of a validator acts
as the stake [16]. PoA runs by a set of validators in a permissioned system. It gains higher throughput by reducing the
number of messages sent between the validators. Reputation is difﬁcult to regain once lost and thus is a better choice
for “stake”.

There are a number of surveys that give comprehensive details of PoW and PoS, such as [40, 35].

Based on STAIR [32] and StakeDag [33], our Lachesis consensus protocos can be used for public permissionless
asynchronous Proof of Stake systems. Participants are allowed to delegate their stake to a node to increase validating
power of a node and share the validation rewards. Unlike existing DAG-based previous work, our Lachesis protocols
are PoS+DAG approach.

2.1.4 DAG-based approaches

DAG-based approaches have currently emerged as a promising alternative to the PoW and PoS blockchains. The
notion of a DAG (directed acyclic graph) was ﬁrst coined in 2015 by DagCoin [26]. Since then, DAG technology
has been adopted in numerous systems, for example, [26, 41, 42, 37, 27]. Unlike a blockchain, DAG-based system
facilitate consensus while achieving horizontal scalability.

DAG technology has been adopted in numerous systems. This section will present the popular DAG-based approaches.
Examples of DAG-based approaches include Tangle [38], Byteball [11], Hashgraph [2], RaiBlocks [25], Phantom [42],
Spectre [41], Conﬂux [27], Parsec [37] and Blockmania [14] .

Tangle is a DAG-based approach proposed by IOTA [38]. Tangle uses PoW to defend against sybil and spam attacks.
Good actors need to spend a considerable amount of computational power, but a bad actor has to spend increasing

8

A PREPRINT - AUGUST 5, 2021

amounts of power for diminishing returns. Tips based on transaction weights are used to address the double spending
and parasite attack.

Byteball [11] introduces an internal pay system called Bytes used in distributed database. Each storage unit is linked
to previous earlier storage units. The consensus ordering is computed from a single main chain consisting of roots.
Double spends are detected by a majority of roots in the chain.

Hashgraph [2] introduces an asynchronous DAG-based approach in which each block is connected with its own ances-
tor. Nodes randomly communicate with each other about their known events. Famous blocks are computed by using
see and strong see relationship at each round. Block consensus is achieved with the quorum of more than 2/3 of the
nodes.

RaiBlocks [25] was proposed to improve high fees and slow transaction processing. Consensus is obtained through
the balance weighted vote on conﬂicting transactions. Each participating node manages its local data history. Block
generation is carried similarly as the anti-spam tool of PoW. The protocol requires veriﬁcation of the entire history of
transactions when a new block is added.

Phantom [42] is a PoW based permissionless protocol that generalizes Nakamoto’s blockchain to a DAG. A parameter
k is used to adjust the tolerance level of the protocol to blocks that were created concurrently. The adjustment can
accommodate higher throughput; thus avoids the security-scalability trade-off as in Satoshi’s protocol. A greedy
algorithm is used on the DAG to distinguish between blocks by honest nodes and the others. It allows a robust total
order of the blocks that is eventually agreed upon by all honest nodes.

Like PHANTOM, the GHOSTDAG protocol selects a k-cluster, which induces a colouring of the blocks as Blues
(blocks in the selected cluster) and Reds (blocks outside the cluster). GHOSTDAG ﬁnds a cluster using a greedy
algorithm, rather than looking for the largest k-cluster.

Spectre [41] uses DAG in a PoW-based protocol to tolerate from attacks with up to 50% of the computational power.
The protocol gives a high throughput and fast conﬁrmation time. Sprectre protocol satisﬁes weaker properties in which
the order between any two transactions can be decided from the transactions by honest users; whilst conventionally
the order must be decided by all non-corrupt nodes.

Conﬂux [27] is a DAG-based Nakamoto consensus protocol. It optimistically processes concurrent blocks without
discarding any forks. The protocol achieves consensus on a total order of the blocks, which is decided by all partici-
pants. Conﬂux can tolerate up to half of the network as malicious while the BFT-based approaches can only tolerate
up to one third of malicious nodes.

Parsec [37] proposes a consensus algorithm in a randomly synchronous BFT network. It has no leaders, no round robin,
no PoW and reaches eventual consensus with probability one. Parsec can reach consensus quicker than Hashgraph [2].
The algorithm reaches 1/3-BFT consensus with very weak synchrony assumptions. Messages are delivered with
random delays, with a ﬁnite delay in average.

Blockmania [14] achieves consensus with several advantages over the traditional pBFT protocol.
In Blockmania,
nodes in a quorum only emit blocks linking to other blocks, irrespective of the consensus state machine. The resulting
DAG of blocks is used to ensure consensus safety, ﬁnality and liveliness. It reduces the communication complexity to
O(N 2) even in the worse case, as compared to pBFT’s complexity of O(N 4).

In this paper, our StakeDag protocol is different from the previous work. We propose a general model of DAG-based
consensus protocols, which uses Proof of Stake for asynchronous permissionless BFT systems. StakeDag protocol
is based on our previous DAG-based protocols[10, 9] to achieve asynchronous non-deterministic pBFT. The new Sφ
protocol, which is based on our ONLAY framework [31], uses graph layering to achieve scalable, reliable consensus
in a leaderless aBFT DAG.

2.2 Lamport timestamps

Lamport [21] deﬁnes the ”happened before” relation between any pair of events in a distributed system of machines.
The happened before relation, denoted by →, is deﬁned without using physical clocks to give a partial ordering of
events in the system. The relation ”→” satisﬁes the following three conditions: (1) If b and b(cid:48) are events in the same
process, and b comes before b(cid:48), then b → b(cid:48). (2) If b is the sending of a message by one process and b(cid:48) is the receipt of
the same message by another process, then b → b(cid:48). (3) If b → b(cid:48) and b(cid:48) → b(cid:48)(cid:48) then b → b(cid:48)(cid:48). Two distinct events b and
b(cid:48) are said to be concurrent if b (cid:57) b(cid:48) and b(cid:48) (cid:57) b.
The happened-before relation can be viewed as a causality effect: that b → b(cid:48) implies event b may causally affect event
b(cid:48). Two events are concurrent if neither can causally affect the other.

9

A PREPRINT - AUGUST 5, 2021

Lamport introduces logical clocks which is a way of assigning a number to an event. A clock Ci for each process Pi
is a function which assigns a number Ci(b) to any event b ∈ Pi. The entire system of blocks is represented by the
function C which assigns to any event b the number C(b), where C(b) = Cj(b) if b is an event in process Pj. The
Clock Condition states that for any events b, b(cid:48): if b → b(cid:48) then C(b) < C(b(cid:48)).

The clocks must satisfy two conditions. First, each process Pi increments Ci between any two successive events.
Second, we require that each message m contains a timestamp Tm, which equals the time at which the message was
sent. Upon receiving a message timestamped Tm, a process must advance its clock to be later than Tm.

Given any arbitrary total ordering ≺ of the processes, the total ordering ⇒ is deﬁned as follows: if a is an event in
process Pi and b is an event in process Pj, then b ⇒ b(cid:48) if and only if either (i) Ci(b) < Cj(b(cid:48)) or (ii) C(b) = Cj(b(cid:48))
and Pi ≺ Pj. The Clock Condition implies that if b → b(cid:48) then b ⇒ b(cid:48).

2.3 Concurrent common knowledge

In the Concurrent common knowledge (CCK) paper [34], they deﬁne a model to reason about the concurrent common
knowledge in asynchronous, distributed systems. A system is composed of a set of processes that can communicate
only by sending messages along a ﬁxed set of channels. The network is not necessarily completely connected. The
system is asynchronous in the sense that there is no global clock in the system, the relative speeds of processes are
independent, and the delivery time of messages is ﬁnite but unbounded.
A local state of a process is denoted by sj
i . Actions are state transformers; an action is a function from local states
to local states. An action can be either: a send(m) action where m is a message, a receive(m) action, and an internal
action. A local history, hi, of process i, is a (possibly inﬁnite) sequence of alternating local states—beginning with
a distinguished initial state—and actions. We write such a sequence as follows: hi = s0
i
notation of sj
of a state, an action, and a state. The jth event in process i’s history is ej

α3
i−→ ... The
i ) refers to the j-th state (action) in process i’s local history An event is a tuple (cid:104)s, α, s(cid:48)(cid:105) consisting

α2
i−→ s2
i

α1
i−→ s1
i

i (αj

, αj

i denoting (cid:104)sj−1

i , sj
i (cid:105).

i

An asynchronous system consists of the following sets.

1. A set P = {1,...,N } of process identiﬁers, where N is the total number of processes in the system.
2. A set C ⊆ {(i,j) s.t. i, j ∈ P } of channels. The occurrence of (i, j) in C indicates that process i can send

messages to process j.

3. A set Hi of possible local histories for each process i in P .
4. A set A of asynchronous runs. Each asynchronous run is a vector of local histories, one per process, indexed
by process identiﬁers. Thus, we use the notation a = (cid:104)h1, h2, h3, ...hN (cid:105). Constraints on the set A are
described throughout this section.

5. A set M of messages. A message is a triple (cid:104)i, j, B(cid:105) where i ∈ P is the sender of the message, j ∈ P is the
message recipient, and B is the body of the message. B can be either a special value (e.g. a tag to denote a
special-purpose message), or some proposition about the run (e.g. “i has reset variable X to zero”), or both.
We assume, for ease of exposition only, that messages are unique.

The set of channels C and our assumptions about their behavior induce two constraints on the runs in A. First, i cannot
send a message to j unless (i, j) is a channel. Second, if the reception of a message m is in the run, then the sending
of m must also be in that run; this implies that the network cannot introduce spurious messages or alter messages.

The CCK model of an asynchronous system does not mention time. Events are ordered based on Lamport’s happened-
before relation. They use Lamport’s theory to describe global states of an asynchronous system. A global state of run
a is an n-vector of preﬁxes of local histories of a, one preﬁx per process. Happened-before relation can be used to
deﬁne a consistent global state, often termed a consistent cut, as follows.
Deﬁnition 2.1 (Consistent cut). A consistent cut of a run is any global state such that if ex
global state, then ex

j and ey

j is in the

i → ey

i is also in the global state.

A message chain of an asynchronous run is a sequence of messages m1, m2, m3, . . . , such that, for all i, receive(mi)
→ send(mi+1). Consequently, send(m1) → receive(m1) → send(m2) → receive(m2) → send(m3) . . . .

3 Lachesis: Proof-Of-Stake DAG-based Protocol

This section presents the key concepts of our new PoS+DAG consensus protocol, namely Lachesis.

10

A PREPRINT - AUGUST 5, 2021

In BFT systems, a synchronous approach utilizes a broadcast voting and asks each node to vote on the validity of each
block. Instead, we aim for an asynchronous system where we leverage the concepts of distributed common knowledge
and network broadcast to achieve a local view with high probability of being a consistent global view.

Each node in Lachesis protocol receives transactions from clients and then batches them into an event block. The new
event block is then communicated with other nodes through asynchronous event transmission. During communication,
nodes share their own blocks as well as the ones they received from other nodes. Consequently, this spreads all
information through the network. The process is asynchronous and thus it can increase throughput near linearly as
nodes enter the network.

3.1 OPERA DAG

In Lachesis protocol, each (participant) node, which is a server (machine) of the distributed system, can create mes-
sages, send messages to and receive messages from other nodes. The communication between nodes is asynchronous.
Each node stores a local block DAG, so-called OPERA DAG, which is Directed Acyclic Graph (DAG) of event blocks.
A block has some edges to its parent event blocks. A node can create an event block after the node communicates the
current status of its OPERA DAG with its peers.

[OPERA DAG] The OPERA DAG is a graph structure stored on each node. The OPERA DAG consists of event
blocks and references between them as edges.

OPERA DAG is a DAG graph G=(V ,E) consisting of V vertices and E edges. Each vertex vi ∈ V is an event block.
An edge (vi,vj) ∈ E refers to a hashing reference from vi to vj; that is, vi (cid:44)→ vj. We extend with G=(V ,E,(cid:62),⊥),
where (cid:62) is a pseudo vertex, called top, which is the parent of all top event blocks, and ⊥ is a pseudo vertex, called
bottom, which is the child of all leaf event blocks. With the pseudo vertices, we have ⊥ happened-before all event
blocks. Also all event blocks happened-before (cid:62). That is, for all event vi, ⊥ → vi and vi → (cid:62).

Figure 2 shows an example of an OPERA DAG (DAG) constructed through the Lachesis protocol. Event blocks are
represented by circles.

Figure 2: An Example of OPERA DAG

The local DAG is updated quickly as each node creates and synchronizes events with each other. For high speed
transaction processing, we consider a model of DAG as DAG streams. Event blocks are assumed to arrive at very high
speed, and they are asynchronous. Let G=(V ,E) be the current OPERA DAG and G(cid:48)=(V (cid:48),E(cid:48)) denote the diff graph,
which consists of the changes to G at a time, either at event block creation or arrival. The vertex sets V and V (cid:48) are
disjoint; similar to the edge sets E and E(cid:48). At each graph update, the updated OPERA DAG becomes Gnew=(V ∪ V (cid:48),
E ∪ E(cid:48)).

Each node uses a local view of OPERA DAG to identify Root, Clotho and Atropos vertices, and to determine topo-
logical ordering of the event blocks.

3.2 PoS Deﬁnitions

Inspired from the success of PoS [36, 18, 45, 4, 20, 8], we introduce a general model of StakeDag protocols that
are Proof of Stake DAG-based for trustless systems [33]. In this general model, the stakes of the participants are
paramount to a trust indicator.

A stake-based consensus protocol consists of nodes that vary in their amount of stake. Based on our generic Lachesis
protocol, any participant can join the network. Participants can increase their impact as well as their contributions
over the Fantom network by increasing the stake amount (FTM tokens) they possess. Validating power is based on the
amount of FTM tokens a node possesses. The validating power is used for online validation of new event blocks to
maintain the validity of blocks and the underlying DAG.

11

A PREPRINT - AUGUST 5, 2021

3.2.1 Stake

Each participant node of the system has an account. The stake of a participant is deﬁned based on their account balance.
Account balance is the number of tokens that was purchased, accumulated and/or delegated from other account. Each
participant has an amount of stake wi, which is a non-negative integer.

Each participant with a positive balance can join as part of the system. A user has a stake of 1, whereas a validator has
a stake of wi > 1. The number of stakes is the number of tokens that they can prove that they possess. In our model, a
participant, whose has a zero balance, cannot participate in the network, either for block creation nor block validation.

3.2.2 Validating Power and Block Validation

In the base Lachesis protocol, every node can submit new transactions, which are batched in a new event block, and it
can communicate its new (own) event blocks with peers. Blocks are then validated by all nodes, which have the same
validating power.

Here, we present a general model that distinguishes Lachesis participants by their validating power. In this model,
every node ni in Lachesis has a stake value of wi. This value is then used to determine their validating power to
validate event blocks.

The base Lachesis protocol contains only nodes of the same weights; hence, it is equivalent to say that all nodes have a
unit weight (wi=1). In contrast, the full version of Lachesis protocol is more general in which nodes can have different
validating power wi.

In order to reach asynchronous DAG-based consensus, validators in Lachesis protocol are required to create event
block (with or without transactions) to indicate that which block (and all of its ancestors) they have validated.

3.2.3 Stake-based Validation

In Lachesis, a node must validate its current block and the received ones before it attempts to create (or add) a new
event block into its local DAG. A node must validate its (own) new event block before it communicates the new block
to other nodes.

A root is an important block that is used to compute the ﬁnal consensus of the event blocks of the DAG. Lachesis
protocol take the stakes of participants into account to compute the consensus of blocks. The weight of a block is the
sum of the validating power of the nodes whose roots can be reached from the block.

3.2.4 Validation Score

The validation score of a block is the total of all validating powers that the block has gained. The validation score of
a block vi ∈ G is denoted by s(vi). If the validation score of a block is greater than 2/3 of the total validating power,
the block becomes a root. The weight of a root ri is denoted by w(ri), which is the weight wj of the creator node j of
the ri. When an event block becomes a root, its score is the validating power of the root’s creator.

3.3 S-OPERA DAG: A weighted DAG model

Like StakeDag protocol, Lachesis protocol uses a DAG-based structure, namely S-OPERA DAG, which is a weighted
DAG G=(V ,E, w), where V is the set of event blocks, E is the set of edges between the event blocks, and w is a
weight mapping to associates a vertex vi with its weight w(vi). Each vertex (or event block) is associated with a
weight (validation score). Event block has a validating score, which is the total weights of the roots reachable from it.
A root vertex has a weight equal to the validating power of the root’s creator node.

Figure 3 depicts an S-OPERA DAG obtained from the DAG. In this example, validators have validating power of 2,
while users have validating power of 1. Blocks created by validators are highlighted in red color. First few event
blocks are marked with their validating power. Leaf event blocks are special as each of them has a validation score
equal to their creator’s validating power.

[S-OPERA DAG] In Lachesis, a S-OPERA DAG is weighted DAG stored on each node.

Let W be the total validating power of all nodes. For consensus, the algorithm examines whether an event block
has a validation score of at least a quorum of 2W/3, which means the event block has been validated by more than
two-thirds of total validating power in the S-OPERA DAG.

12

A PREPRINT - AUGUST 5, 2021

Figure 3: An example of an S-OPERA DAG in Lachesis. The validation scores of some selected blocks are shown.

3.4 Overall Framework

Figure 4: A General Framework of StakeDag Protocol

Figure 4 shows a general framework of Lachesis protocol. Each node contains a DAG consisting of event blocks. In
each node, the information of accounts and their stakes are stored. The main steps in a PoS DAG-based consensus
protocol include (a) block creation, (b) computing validation score, (c) selecting roots and updating the root sets, (d)
assigning weights to new roots, (e) decide frames, (f) decide Clothos/Atropos, and (f) order the ﬁnal blocks.

3.5 Quorum

To become a root, an event block requires more than 2/3 of validating power (i.e., 2W/3) rather than 2n/3 in unweighted
version of Lachesis. For simplicity, we denote quorum Q to be 2W/3+1 for weighted DAG. In the original unweighted
version, quorum Q is 2n/3 + 1 for unweighted DAG.

13

A PREPRINT - AUGUST 5, 2021

3.6 Event Block Creation

Each event block has at most k references to other event blocks using their hash values. One of the references must
be a self-ref (or self-parent) that references to an event block of the same node. The other references, which are also
known as other-parent or other-ref references, are the top event blocks of other nodes.

Prior to creating a new event block, the node will ﬁrst validate its current block and the selected top event block(s) from
other node(s). With cryptographic hashing, an event block can only be created or added if the self-ref and other-ref
event blocks exist.

Nodes with a higher stake value wi have a higher chance to create a new event block than other nodes with a lower
stake.

3.7 Event Block Structure

An event (or an event block) is a vertex in the DAG, which stores the structure of consensus messages. Each event has
a number of links to past events (parents). The links are the graph edges. Sometimes, ”event” and ”event block” can
be used interchangeably, but they are different from ”block”, which means a ﬁnalized block in the blockchain.

Each event is a signed consensus message by a validator sent to the network, which guarantees the following that:
”The creator of this event has observed and validated the past events (and their parents), including the parents of this
event. The event’s creator has already validated the transactions list contained in this event.”

The structure of an event block in Lachesis is as follows:

• event.Epoch: epoch number (see epoch). Not less than 1.
• event.Seq: sequence number. Equal to self-parent’s seq + 1, if no self-parent, then 1.
• event.Frame: frame number (see consensus). Not less than 1.
• event.Creator: ID of validator which created event.
• event.PrevEpochHash: hash of ﬁnalized state of previous epoch.
• event.Parents: list of parents (graph edges). May be empty. If Seq ¿ 1, then ﬁrst element is self-parent.
• event.GasPowerLeft: amount of not spent validator’s gas power for each gas power window, after connection of

this event.

• event.GasPowerUsed: amount of spent validator’s gas power in this event.
• event.Lamport: Lamport timestamp. If parents list is not empty, then Lamport timestamp of an event v, denoted

by LS(v) is maxLS(u) — u is a parent of v + 1. Otherwise, LS(v) is 1

• event.CreationTime: a UnixNano timestamp that is speciﬁed by the creator of event. Cannot be lower than

creation time of self-parent (if self-parent exists). Can be too-far-in-future, or too-far-in-past.

• event.MedianTime: a UnixNano timestamp, which is a weighted median of highest observed events (their Cre-
ationTime) from each validator. Events from the cheater validators are not counted. This timestamp is used to
protect against ”too-far-in-future” and ”too-far-in-past”.

• event.TxHash: Merkle tree root of event transactions.
• event.Transactions: list of originated transactions.
• event.Sig: ECDSA256 secp256k1 validator’s signature in the R/S format (64 bytes).

Each event has two timestamps. Event’s creationTime is the created timestamp an assigned by the creator of event
block. Event’s MedianTime is a weighted median of highest observed events (their CreationTime) from each validator.
Events from the cheater validators aren’t counted.

3.8 Root Selection

Root events get consensus when 2/3 validating power is reached. When a new root is found, ’Assign weight’ step will
assign a weight to the root. The root’s weight is set to be equal to the validating power wi of its creator. The validation
score of a root is unchanged. Figure 4 depicts the process of selecting a new root in Lachesis. A node can create a new
event block or receive a new block from other nodes.

To guarantee that root can be selected correctly even in case a fork exists in the DAG, we introduce f orklessCause
concept. The relation f orklessCause(A, B) denote that event x is forkless caused by event y. It means, in the
subgraph of event x, x does not observe any forks from y’s creator and at least QUORUM non-cheater validators have
observed event y.

An event is called a root, if it is forkless causes by QUORUM roots of previous frame. The root starts a new frame.
For every event, its frame number is no smaller than self-parent’s frame number. The lowest possible frame number

14

A PREPRINT - AUGUST 5, 2021

is 1. Relation f orklessCause is a stricter version of happened-before relation, i.e. if y forkless causes x, then y is
happened before A. Yet, unlike happened-before relation, forklessCause is not transitive because it returns false
if fork is observed. So if x forkless causes y, and y forkless causes z, it does not mean that x forkless causes z. If
there’s no forks in the graph, then the forklessCause is always transitive.

3.9 Clotho Selection

Next, we present our algorithm to select Clotho from roots. A Clotho is a root that is known by more than a quorum Q
of nodes, and there is another root that knows this information. In order for a root ri in frame fi to become a Clotho,
ri must be reach by some root ri+3 in frame fi+3. This condition ensures an equivalence to a proof of practical BFT.

for x range roots: lastDecidedFrame + 1 to up do
for y range roots: x.f rame + 1 to up do

// y is a root which votes, x is a voting subject root
round ← y.frame - x.frame
if round == 1 then

// y votes positively if x forkless causes y
y.vote[x] ← forklessCause(x, y)

Algorithm 1 Clotho Selection
1: procedure DECIDECLOTHO
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

else

else

prevVoters ← getRoots(y.frame-1) // contains at least QUORUM events, from deﬁnition of root
yesVotes ← 0
noVotes ← 0
for prevRoot range prevRoots do

if forklessCause(prevRoot, y) == TRUE then

// count number of positive and negative votes for x from roots which forkless cause y
if prevRoot.vote[x] == TRUE then

yesVotes ← yesVotes + prevVoter’s STAKE

20:
21:
22:
23:
24:
25:
26:
27:
28:

noVotes ← noVotes + prevVoter’s STAKE

// y votes likes a majority of roots from previous frame which forkless cause y
y.vote[x] ← (yesVotes - noVotes) >= 0
if yesVotes >= QUORUM then

x.candidate ← TRUE // decided as a candidate for Atropos
decidedRoots[x.validator] ← x
break

if noVotes >= QUORUM then

x.candidate ← FALSE // decided as a non-candidate for Atropos
decidedRoots[x.validator] ← x
break

Algorithm 1 shows a pseudo code to select Clothos from the current set of roots. For every root x from last decided
frame + 1, it then loops for every root y from x.frame + 1, and executes the inner loop body. Coloring of the graph
is performed such that each root is assigned with one of the colors: IS-CLOTHO (decided as candidate for Clotho),
IS-NOT-CLOTHO (decided as not a candidate for Clotho), or a temporary state UNDECIDED (not decided). Once a root
is assigned a color by a node, the same color is assigned to the root by all other honest nodes, unless more than 1/3W
are Byzantine. This is the core property of Lachesis consensus algorithm.

• If a root has received w ≥ QUORUM votes for a color, then for any other root on next frame, its prevRoots
(contains at least QUORUM events, from deﬁnition of root) will overlap with V, and resulting overlapping will
contain more than 1/2 of the prevVoters. It iscrucial that QUORUM is 2/3W + 1, not just 2/3W. The condition
implies that ALL the roots on next frame will vote for the same color unanimously.

• A malicious validator may have multiple roots on the same frame, as they may make some forks. The
forklessCause relation and the condition that no more than 1/3W are Byzantine will guarantee that no more
than one of these roots may be voted YES. The algorithm assigns a color only in round >= 2, so prevRoots will
be ”ﬁltered” by forklessCause relation, to prevent deciding differently due to forks.

• If a root has received at least one YES vote, then the root is guaranteed to be included in the graph.

15

A PREPRINT - AUGUST 5, 2021

• A root event x from an ofﬂine validator will be decided NO by all. Thus, x is considered as non-existent event

and cannot be assigned IS-CLOTHO color, and hence it cannot become Atropos.

• Technically, the election may take many rounds. Practically, however, the election ends mostly in 2nd or 3rd

round.

Unlike HashGraph’s decideFame function in page 13 [2], our Lachesis algorithm has no coin round and is completely
deterministic.

A Clotho is an Atropos candidate that has not been assigned a timestamp.

3.10 Atropos Selection

From the array of roots that are decided as candidate, there are several ways to sort these Atropos candidates. Atropos
candidates can be sorted based on the Lamport timestamp, layering information, validator’s stake, validator’s id, and
the Atropos’s id (see our ONLAY paper [31]).

Here, we present an algorithm that sorts these Atropos candidates using validator’s stake and validator’s id. Algo-
rithm 2 shows the pseudo code for Atropos selection. The algorithm sorts the decided roots based on their validator’s
information. Validators are sorted based on their stake amount ﬁrst, and then by their Id. The algorithms requires a
sorted list of validators. The sorting is O(n.log(n)), where n is the number of roots to process.

for validator: range sorted validators do
root = decidedRoots[validator]
if root == NULL then // not decided

return nil // frame isn’t decided yet

Algorithm 2 Atropos Selection
1: function DECIDEATROPOS
2:
3:
4:
5:
6:
7:
8:
9:

continue

if root.candidate == TRUE then
return root // found Atropos
if root.candidate == FALSE then

Nodes in Lachesis reach consensus about Atropos selection and Atropos timestamp without additional communication
(i.e., exchanging candidate time) with each other. Through the Lachesis protocol, the OPERA DAGs of all nodes are
“consistent“. This allows each node to know the candidate time of other nodes based on its OPERA DAG and reach a
consensus agreement. Our proof is shown in Section 8.2.

Atropos event block, once decided, is ﬁnal and non-modiﬁable. All event blocks can be in the subgraph of an Atropos
guarantee ﬁnality.

3.11 Atropos timestamp

Validators are sorted by their stake amount and then by their ID. For roots from different validators, they are sorted
based on the ordering of their validators. Atropos is chosen as a ﬁrst candidate root in the sorted order. Notice that,
an Atropos may be decided even though not all the roots on a frame are decided. Our Lachesis algorithm allows to
decide frames much earlier when there’s a lot of validators, and hence reduce TTF (time to ﬁnality).

3.12 Main chain

The Main-chain is an append-only list of blocks that caches the ﬁnal consensus ordering of the ﬁnalized Atropos
blocks. The local hashing chain is useful to improve path search to quickly determine the closest root to an event
block. Each participant has an own copy of the Main chain and can search consensus position of its own event blocks
from the nearest Atropos. The chain provides quick access to the previous transaction history to efﬁciently process
new coming event blocks. After a new Atropos is determined, a topological ordering is computed over all event blocks
which are yet ordered. After the topological ordering is computed over all event block, Atropos blocks are determined
and form the Main chain.

16

A PREPRINT - AUGUST 5, 2021

3.13 Topological ordering of events

Every node has a physical clock, and each event block is assigned with a physical time (creation time) from the event’s
creator. Each event block is also assigned a Lamport timestamp. For consensus, Lachesis protocol relies on a logical
clock (Lamport timestamp) for each node.

We use Lamport timestamp [21] to determine the time ordering between event blocks in a asynchronous distributed
system. The Lamport timestamps algorithm is as follows: (1) Each node increments its count value before creating an
event block; (2) When sending a message, sender includes its count value. Receiver should consider the count sent in
sender’s message is received and increments its count value, (3) If current counter is less than or equal to the received
count value from another node, then the count value of the recipient is updated; (4) If current counter is greater than
the received count value from another node, then the current count value is updated.

3.14 Blocks

Next, we present our approach to ordering event blocks under Atropos event block that reaches ﬁnality.

Once a new Atropos is decided, a new block will be created. The new block contains all the events in the Atropos’s
subgraph; it only includes events, which were yet included in the subgraphs of any previous Atroposes. The events in
the new block are ordered by Lamport time, then by event’s hash. Ordering by Lamport time ensures that parents are
ordered before their children. Our approach gives a deterministic ﬁnal ordering of the events in the block.

Figure 5 illustrates the steps to make blocks from decided Atroposes. In the example, Clotho/Atropos vertices are
colored red. For each Atropos vertex, we compute the subgraph under it. Atropos vertices can be selected by using
of several criteria (see ONLAY paper [31]). Subgraphs under Atropos vertices are shown in different colors. Atropos
event blocks are processed in the order from lower layer to high layer.

Figure 5: Peeling the subgraphs in topological sorting order

We present an algorithm for topological ordering of Atropos event blocks (introduced in our ONLAY paper [31]).
Algorithm 3 ﬁrst orders the Atropos candidates using SortByLayer function, which sorts vertices based on their
layer, Lamport timestamp and then hash information of the event blocks. Second, we process every Atropos in the
sorted order, and then compute the subgraph G[a] = (Va, Ea) under each Atropos. The set of vertices Vu contains
vertices from Va that are not yet processed in U . We then apply SorByLayer to order vertices in Vu and then append
the ordered result into the ﬁnal ordered list S.

17

A PREPRINT - AUGUST 5, 2021

Algorithm 3 TopoSort
1: Require: OPERA DAG H, φ, φF .
2: function TOPOSORT(A)
S ← empty list
3:
U ← ∅
4:
Q ← SortByLayer(A)
5:
for Atropos a ∈ Q do
6:
7:
8:
9:
10:
11: function SORTBYLAYER(V)
12:

Compute subgraph G[a] = (Va, Ea)
Vu ← Va \ U
T ← SortByLayer(Vu)
Append T into the end of ordered list S.

Sort the vertices in Vu by layer, Lamport timestamp and hash in that order.

With the ﬁnal ordering computed by using above algorithms, we can assign the consensus time to the ﬁnally ordered
event blocks.

Then the Atropos is assigned a timestamp from the event.MedianTimestamp, which is computed as a median of
the event’s timestamps sent by all the nodes. The median time is used to guarantee aBFT for the consensus time
for the Atropos to avoid incorrect time from malicious nodes. Once Atropos consensus time is assigned, the Clotho
becomes an Atropos and each node stores the hash value of Atropos and Atropos consensus time in Main-Chain. The
Main-chain is used for time order between event blocks.

3.15 Block timestamp

In Lachesis, block timestamp is assigned using Atropos’s median time.

3.16 Peer selection

In Lachesis protocol, we use a model in which each node only stores a subset of n peers. To create a new event block, a
node ﬁrst synchronizes the latest blocks from other peers and then it selects the top blocks of at most k peers. Lachesis
protocol does not depend on how peer nodes are selected.

There are multiple ways to select k peers from the set of n nodes. An simple approach can use random selection from
the pool of n nodes. In ONLAY [31], we have tried a few peer selection algorithms as follows:

• Random: randomly select a peer from n peers;
• Least Used: select the least use peer(s).
• Most Used (MFU): select the most use peer(s).
• Fair: select a peer that aims for a balanced distribution.
• Smart: select a peer based on some other criteria, such as successful throughput rates, number of own events.

Stake-based Peer Selection: We also propose new peer selection algorithms, which utilize stakes to select the next
peer. Each node has a mapping of the peer i and the frequency fi showing how many times that peer was selected.
New selection algorithms deﬁne some function αi that take user stake wi and frequency fi into account. We give a
few examples of the new algorithms, described as follows:

• Stake-based: select a random peer from n peers with a probability proportional to their stakes wi.
• Lowest: select the peer with the lowest value of αi.
• Highest: select the peer with the highest value of αi.
• Balanced: aim for a balanced distribution of selected peers of a node, based on the values αi.

There are other possible criteria can be used a peer selection algorithm, such as successful validation rates, total
rewards, etc. A more complex approach is to deﬁne a cost model for the node selection, such as low communication
cost, low network latency, high bandwidth and high successful transaction throughput. In Lachesis, available gas and
origination power are used.

3.17 Dynamic participants

Unlike some existing DAG-based approaches [2], our Lachesis protocol allows an arbitrary number of participants to
dynamically join the system.

18

OPERA DAG can still operate with new participants. Algorithms for selection of Roots, Clothos and Atroposes are
ﬂexible enough and not dependent on a ﬁxed number of participants.

A PREPRINT - AUGUST 5, 2021

3.18 Peer synchronization

Algorithm 4 shows a pseudo code to synchronize events between the nodes.

Algorithm 4 EventSync
1: procedure SYNC-EVENTS()
2:
3:
4:
5:
6:
7:

Node n1 selects random peer to synchronize with
n1 gets local known events
n1 sends RPC sync request to peer
n2 receives RPC sync request
n2 does an graph-diff check on the known map
n2 returns unknown events, and mapping of known events to n1

The algorithm assumes that a node always needs the events in topological ordering (speciﬁcally in reference to the
Lamport timestamps), an alternative would be to use an inverse bloom lookup table (IBLT) for completely potential
randomized events. Alternatively, one can simply use a ﬁxed incrementing index to keep track of the top event for
each node.

In Lachesis, when a node receives a coming (new) event block from a peer, it will perform several checks to make sure
the event block has been signed by an authorized peer, conforms to data integrity, and has all parent blocks already
added in the node’s local DAG.

3.19 Detecting Forks

[Fork] A pair of events (vx, vy) is a fork if vx and vy have the same creator, but neither is a self-ancestor of the other.
Denoted by vx (cid:116) vy.
For example, let vz be an event in node n1 and two child events vx and vy of vz. if vx (cid:44)→s vz, vy (cid:44)→s vz, vx (cid:54)(cid:44)→s vy,
vy (cid:54)(cid:44)→s vz, then (vx, vy) is a fork. The fork relation is symmetric; that is vx (cid:116) vy iff vy (cid:116) vx.
By deﬁnition, (vx, vy) is a fork if cr(vx) = cr(vy), vx (cid:54)(cid:44)→a vy and vy (cid:54)(cid:44)→a vx. Using Happened-Before, the second
part means vx (cid:54)→ vy and vy (cid:54)→ vx. By deﬁnition of concurrent, we get vx (cid:107) vy.
Lemma 3.1. If there is a fork vx (cid:116) vy, then vx and vy cannot both be roots on honest nodes.

Here, we show a proof by contradiction. Any honest node cannot accept a fork so vx and vy cannot be roots on the
same honest node. Now we prove a more general case. Suppose that both vx is a root of nx and vy is root of ny, where
nx and ny are honest nodes. Since vx is a root, it reached events created by nodes having more than 2W/3. Similarly,
vy is a root, it reached events created by nodes of more than 2W/3. Thus, there must be an overlap of more than W /3.
Since we assume less than W /3 are from malicious nodes, so there must be at least one honest member in the overlap
set. Let nm be such an honest member. Because nm is honest, nm does not allow the fork.

3.20 Transaction conﬁrmations

Here are some steps for a transaction to reach ﬁnality in our system.

• First, when user submits a transaction into a node, a successful submission receipt will be issued to the client as

a conﬁrmation of the submitted transaction.

• Second, the node will batch the submitted transaction(s) into a new event block, and add it into its OPERA DAG.
Then will broadcast the event block to all other nodes of the system. Peer nodes will update its own record
conﬁrming that the containing event block identiﬁer is being processed.

• Third, when the event block is known by majority of the nodes (e.g., it becomes a Root event block), or being

known by such a Root block, new status of the event block is updated.

• Fourth, our system will determine the condition at which a Root event block becomes a Clotho for being further
acknowledged by a majority of the nodes. A conﬁrmation is then sent to the client to indicate that the event
block has come to the semi-ﬁnal stage as a Clotho or being conﬁrmed by a Clotho. After the Clotho stage, we
will determine the consensus timestamp for the Clotho and its dependent event blocks.

• Once an event block gets the ﬁnal consensus timestamp, it is ﬁnalized and a ﬁnal conﬁrmation will be issued to

the client that the transaction has been successfully ﬁnalized.

19

A PREPRINT - AUGUST 5, 2021

The above ﬁve steps are done automatically by ONLAY. Steps (2), (3) and (4) are internal steps. Steps (1) and (5) are
visible to the end users. Currently, the whole process will take 1-2 seconds from transaction submission to transaction
conﬁrmation (through the ﬁve steps). Once a block is conﬁrmed, it will be assigned with a block id in the blockchain
and that conﬁrmation is ﬁnal.

There are some cases that a submitted transaction can fail to reach ﬁnality. Examples include a transaction does
not pass the validation, e.g., insufﬁcient account balance, or violation of account rules. The other kind of failure is
when the integrity of DAG structure and event blocks is not complied due to the existence of compromised or faulty
nodes. In such unsuccessful cases, the event block’s status is updated accordingly. Clients can always query the latest
transaction status regardless of its success or failure.

3.21 Checkpoint

STAIR uses the following procedure, which is similar to the Casper model [46]. Our approach can be summarized as
follows:

1. We can divide into checkpoints, each will take every 100th (some conﬁgurable number) frame.
2. Stakeholders can choose to make more deposits at each check point, if they want to become validators and
earn more rewards. Validators can choose to exit, but cannot withdraw their deposits until three months later.
3. A checkpoint is selected based on a consistent global history that is ﬁnalized with more than 2/3 of the
validating power for the checkpoint. When a checkpoint is ﬁnalized, the transactions will not be reverted.
4. With asynchronous system model, validators are incentivized to coordinate with each other on which check-
points the history should be updated. Nodes gossip their latest local views to synchronize frames and ac-
counts. Attackers may attempt double voting to target double spending attacks. Honest validators are incen-
tivized to report such behaviors and burn the deposits of the attackers.

Each frame in Lachesis is about 0.3-0.6s, and blocks are conﬁrmed in 0.7-1.5s depending on the number of transactions
put into the system. To have more deterministic checkpoints, we divide into epochs, each of which lasts around 4 hours.

4 Staking model

This section presents our staking model for Opera network. We ﬁrst give general deﬁnitions and variables of our
staking model, and then present the model in details.

4.0.1 Deﬁnitions and Variables

Below are the general deﬁnitions and variables that are used in Opera network.

General deﬁnitions

F
SF C

denotes the network itself
”Special Fee Contract” – managing the collection of transaction fees and the payment of all
rewards
F T M main network token

Accounts

U
A
S
V

⊂ U
⊆ A
⊆ S

set of all participant accounts in the network
accounts with a positive FTM token balance
accounts that have staked for validation (some of which may not actually be validating nodes)
validating accounts, corresponding to the set of the network’s validating nodes

A participant with an account having a positive FTM token balance, say i ∈ A, can join the network (as a delegator or
a validator). But an account i in S may not participate in the protocol yet. Those who join the network belong to the
set V.

[Validating account] A validating account has more than U tokens.

20

A PREPRINT - AUGUST 5, 2021

Network parameters subject to on-chain governance decisions

F
λ
ε
φ
µ

3.175e9
7
1
30%
15%

total supply of FTM tokens
period in days after which validator staking must be renewed, to ensure activity
minimum number of tokens that can be staked by an account for any purpose
SPV commission on transaction fees
validator commission on delegated tokens

Tokens held and staked

Unless otherwise speciﬁed, any mention of tokens refers to FTM tokens.
[Token helding] The token helding ti of an account is number of FTM tokens held by account i ∈ A.

ti
t[x]
> ε
i
t[d]
i (s) > ε
t[d](s)
t[d]
i
t[s]
i

number of FTM tokens held by account i ∈ A
transaction-staked tokens by account i
tokens delegated by account i to account s ∈ S
total of tokens delegated to account s ∈ S
total of tokens delegated by account i to accounts in S
validation-staked tokens by account i

The sum of tokens staked or delegated by an account i ∈ A cannot exceed the amount of tokens held: t[x]
(cid:80)
amount of tokens delegated by an account i ∈ A is: t[d]

i (s) ≤ ti. The total amount of tokens delegated to an account s ∈ S is: t[d](s) = (cid:80)

i∈A t[d]

s∈S t[d]

i + t[s]
i +
i (s). The total

i = (cid:80)

s∈S t[d]

i (s).

4.0.2 Validating power

We use a simple model of validating power, which is deﬁned as the number of tokens held by an account. The weight
of an account i ∈ A is equal to its token holding ti.

4.0.3 Block consensus and Rewards

[Validation score] Validation score of a block is the total validating power that a given block can achieve from the
validators v ∈ V.

[Quorum] Validating threshold is deﬁned by 2/3 of the validating power that is needed to conﬁrm an event block to
reach consensus.

Below are the variables that deﬁne the block rewards and their contributions for participants in Fantom network.

Z

Fs
Fc

996,341,176

F − Fs

total available block rewards of F T M , for distribution by the SP V during the
ﬁrst 1460 days after mainnet launch
F T M tokens held by the SP V
total circulating supply

Block rewards will be distributed over each epoch.

4.0.4 Token Staking and Delegation

Participants can chose to stake or delegate their FTM tokens. When staking or delegating, the validating power of a
node is based on the number of FTM tokens held. Three possible ways to stake are given as follows.

[Transaction staking] Participants can gain more stakes or tokens via the transaction staking. Transaction submitter
can gain reward from the transaction fees of submitted transactions. This style of staking helps increases transaction
volume on the network. The more liquidity, the more transaction staking they can gain.

21

A PREPRINT - AUGUST 5, 2021

[Validation staking] By validating blocks, a participant can gain validation rewards. Honest participants can gain
block rewards for successfully validated blocks.

Participants can achieve block rewards for blocks that they co-validated and gain transaction fees from transaction
submitter for the successfully ﬁnalized transactions. The more stake they gain, the more validating power they will
have and thus the more rewards they can receive as it is proportional to their validating power.

[Validation delegation] Validation delegation allows a participant to delegate all or part of their tokens to another
participant(s). Delegating participants can gain a share of block rewards and transaction fees, based on the amount of
delegated stake.

Delegators can delegate their stake into a validator or into multiple validators. Participants with large amount of stake
can delegate their stakes into multiple validators. Delegators earn rewards from their delegations. Validators will not
be able to spend delegated tokens, which will remain secured in the stakeholder’s own address. Validators will receive
a ﬁxed proportion of the validator fees attributable to delegators.

[Validation performance] Participants are rewarded for their staked or delegated amount. Delegators will be incen-
tivized to choose validator nodes that have a high self-stake, i.e. are honest and high performing. Delegators can
delegate their tokens for a maximum period of days, after which they need to re-delegate. The requirements to dele-
gate are minimal:

• Minimum number of tokens per delegation: 1
• Minimum lock period: 1 day
• Maximum lock period: 1 year
• Maximum number of delegations made by a user: None
• Maximum number of delegated tokens per validator: 15 times of the validator’s tokens.

5 Opera Network

5.1 Network Architecture

Figure 6 depicts an overview of Opera network, as introduced in our STAIR framework [32].

A Validator node consists of three components: state machine, consensus and networking. A application can commu-
nicate to a node via CLI. Opera network supports auditing by permitting participants to join in post-validation mode.
An observer (or Monitor) node consists of state machine, post validation component and networking component.

Figure 6: An overview of Opera network

5.2 Nodes and Roles

For an asynchronous DAG-based system, Lachesis supports three types of participants: users, validators and monitors.

22

A PREPRINT - AUGUST 5, 2021

Each validating node can create new event blocks. Generation of a new even block indicates that the new block and
all of its ancestors have been validated by the creator node of that new event block.

Users have less than U FTM tokens, where U is a predeﬁned threshold value. The current threshold value, which is
voted by the community is U = 1,000,0000 FTMs. Each validator must have more than U tokens. In Lachesis, users
can only delegate their stake to validator nodes. User nodes cannot run as a validating node, which can create and
validate event blocks.

Besides validating nodes, Opera network allows another type of participants — observers, who can join the network,
but do not act as validating nodes. Observers are not required to have any stake to join the network and thus they cannot
generate nor perform online voting (online validation). But observers can do post-validation. This is to encourage the
community to join the checks-and-balances of our public network.

5.3 Boot Node Service

A new node that joins Opera network for the ﬁrst time will connect to one of our boot nodes, which provides a
discovery service to connect with other peers in the network. Currently, there are ﬁve boot nodes in Opera network
and the boot nodes are distributed in different zones. Boot nodes serve RPC requests from peers and help new nodes
to discover peers in the network.

5.4 API Service

Since the ﬁrst launch of Opera mainnet, the network has served lots of dApp projects and users. We have scaled our
infrastructure on EC2 AWS accordingly to cope with a huge amount of requests.

There are currently about 30 servers used for RPC API. The RPC API load is about 250,000 requests per second at
peaks. Each server can serve about 9000 requests per second. They are run in 3 hives that are independently serviced
by their own balancing gateways and the gateways are switched by round robin DNS setup in 60s roundtrip time. Also,
we have additional 3 public RPC endpoints for API calls that are use for backup service.

Our infrastructure include other servers to serve explorer and wallet service. We also maintain a different set of servers
for cross-chain integration.

Apart from the servers run by the Foundation, there are many more API servers operated by our community members,
who are active contributors to the Opera network.

5.5 Mainnet

There are currently 46 validator nodes, 5 boot nodes and more than 50 servers running API service. Our Mainnet has
served more than 35 million transactions, and have produced more than 12.8 million blocks. There are more than 300k
accounts.

5.6 Testnet

There are currently 7 validator nodes and 2 API nodes. The testnet has been used by developers to test and deploy new
smart contracts and to prepare for new integration prior into launch it on our mainnet.

5.7

Implementation

We have implemented our Lachesis protocol in GoLang 1.15+. Our implementation is available at https://
github.com/Fantom-foundation/go-opera. Our previous repository is available at https://github.com/
Fantom-foundation/go-lachesis.

Opera network is fully EVM compatible and smart contracts can be deployed and run on our Opera network. We
implemented Lachesis on top of Geth 1. We provide Web3 API calls support through our API Service layer. De-
centralized applications (dApps) can query and send transactions into the network for execution. Documentation and
guides are available at https://docs.fantom.foundation/.

Figure 7 shows the overall architecture of a Lachesis node.

1https://github.com/ethereum/go-ethereum

23

A PREPRINT - AUGUST 5, 2021

Figure 7: Architecture of a Lachesis node

Transaction Fee: Like many existing platforms, Opera network leverages transaction fee, which is paramount to
prevent transaction ﬂooding. It will cost attackers when they attempt to send a lot of spamming transactions to the
network.

Clients can send transactions to Opera network. Each transaction requires a small amount of fee to compensate for
the cost to execute the transaction on Validator nodes. Transaction can range from simple account transfer, new
smart contract deployment or a complex smart contract call. Transaction fee is proportional to the mount of op codes
included in the transaction.

In Ethereum and few others, transaction fees are calculated in a separate gas token rather than the native token of the
network. Instead, Fantom’s Opera network calculates transaction fees in native FTM. This design removes the burden
of acquiring two types of token for users and developers to be able to use and run dApps on our Opera mainnet.

Transaction Pool: Transactions once submitted will be appended into a transaction pool of a node. The transaction
pool is a buffer to receive incoming transactions sent from peers. Similar to Ethereum’s txpool implementation,
transactions are placed in a priority queue in which transactions with higher gas will have more priority.

Each node then pulls transactions from the pool and batches them into new event block. The number of new event
blocks a node can create is proportional to the validating power it has.

5.8 Performance and Statistics

For experiments and benchmarking, we set up a private testnet with 7 nodes on EC2 AWS. Each node is running
m2.medium instance, which is 2vCPU with 3.3 GHz each, 4GB RAM and 200GB SSD storage. We also experimented
using a local machine with the following specs (CPU: AMD Ryzen 7 2700, Memory: 2x DDR4/16G 2.6 Ghz, SSD
storage).

24

A PREPRINT - AUGUST 5, 2021

5.8.1 Normal emission rate and gas allocation

We ran experiments with the private testnet with 7 nodes. The experiment was run using normal emission rate (200ms)
and normal gas allocation rules. We ran the local machine to sync with the testnet.

On the local machine, the maximum raw TPS is 11000, and maximum syncing TPS is 4000-10000 TPS. The total
number of transactions executed is 14230, the total of events emitted is 109, and there are 108 ﬁnalized blocks. The
peak TPS for P2P syncing on a local non-validator node (16 CPUs, SSD) is 3705.

Our results on a validator node show that the peak TPS for P2P syncing is 2760. Maximum syncing TPS is 3000-7000
on testnet server, whereas maximum network TPS is 500 due to txpool bottleneck on testnet servers.

5.8.2 High latency nodes

We also ran an experiment where 1/3 of the nodes in the network were lagged. Additional latency of 300-700ms was
added into 3 validators, whereas normal latency was used for others.

Test duration (seconds)
Blocks ﬁnalized
Transactions executed
Events emitted
Average TTF (seconds)
Average TPS

485
1239
24283
10930
0.92
50.06

793
605
39606
5959
4.64
49.94

Table 6: Statistics of with 3 nodes of high latency

Table 6 shows that the added latency into 3 nodes can increase TTF by 4-fold, but TPS is slightly reduced.

5.8.3 Emission rates

We ran experiments in that each node will generate 1428 transactions per second and thus the whole network of 7
test servers generated 10000 transactions per second. Each generated transaction has around 100 bytes, like a simple
FTM transfer. To benchmark the consensus, this experiment disabled transaction execution, so it only counted the
transaction creation, consensus messages, and ﬁnalized transactions and blocks.

(a) tps

(b) ttf

Figure 8: TPS and TTF measures on a testnet with 7 nodes

Figure 8 shows the statistics of block TPS and block TTF when 10000 transactions are submitted into the network
per second. We experimented with different emission rates (100ms, 150ms and 200ms), which is the amount of time
each new event created per node. As depicted in the ﬁgure, when increase emission rate from 100ms to 200ms, the tps
increased and the average TTF also increased.

25

A PREPRINT - AUGUST 5, 2021

6 Discussions

6.1 Comparison with Existing Approaches

Compared with existing PoS protocols [36, 18, 45, 4, 20, 8], our Lachesis protocol is built on top of DAG and it is
more deterministic and more secure.

Compared with existing DAG-based approaches [26, 41, 11, 2, 38, 42, 37, 27], Lachesis algorithms are quite dif-
ferent. Among them, Hashgraph [2] is the closest to ours. However, our Lachesis algorithm is quite different from
Hashgraph’s algorithm in many ways:

• Lachesis algorithm for ﬁnding roots and Atropos is deterministic, whereas Hashgraph uses coin round to decide

’famous’ event blocks.

• Our Lachesis leverages the notion of Happened-before relation, Lamport timestamp, and graph layering to reach

consensus on the ﬁnal ordering for Atropos blocks.

• Our algorithm to create blocks (i.e., ﬁnalized blocks of the blockchains) is unique. New block is created for
every newly decided Atropos, whereas Hashgraph tends to process with multiple Famous event blocks at once.
• The event blocks under Atropos are sorted based on Lamport timestamp and graph layering information. On
the contrary, Hashgraph uses a heuristics, so-called ﬁndOrder function described in their paper, to compute the
ordering of events as well as their timestamps.

• Lachesis protocol is permissionless aBFT, whereas it is unclear if their algorithm is truly BFT.
• Fantom’s Opera network is a permissionless (public) network and our Lachesis protocol has run successfully. In

contrast, Hashgraph aims for a permissioned network.

• We give a detailed model of our Proof-of-Stake model in StakeDag and StairDag framework [33, 32], whereas

PoS is brieﬂy mentioned in their Hashgraph paper [2].

• We also provide a formal semantics and proof for our Lachesis protocol using common concurrent knowl-

edge [34].

6.2 Protocol Fairness and Security

There has been extensive research in the protocol fairness and security aspects of existing PoW and PoS blockchains
(see surveys [40, 35]). Here, we highlight the beneﬁts of our proposed PoS+DAG approach.

6.2.1 Protocol Fairness

Regarding the fairness, PoW protocol is fair because a miner with pi fraction of the total computational power can win
the reward and create a block with the probability pi. PoS protocol is fair given that an individual node, who has wi
fraction of the total stake or coins, can a new block with wi probability. However, in PoS systems, initial holders of
coins tend to keep their coins in their balance in order to gain more rewards.

Our Lachesis protocol is fair since every node has an equal chance to create an event block. Nodes in Lachesis
protocol can enter the network without an expensive hardware like what is required in PoW. Further, any node in
Lachesis protocol can create a new event block with a stake-based probability, like the block creation in other PoS
blockchains.

Like a PoS blockchain system, it is a possible concern that the initial holders of coins will not have an incentive
to release their coins to third parties, as the coin balance directly contributes to their wealth. Unlike existing PoS
protocols, each node in Lachesis protocol are required to validate parent event blocks before ot can create a new block.
Thus, the economic rewards a node earns through event creation is, in fact, to compensate for their contribution to the
onchain validation of past event blocks and it’s new event block.

Criterion
Block creation probability
Validation probability
Validation reward
Txn Reward
Performance reward

PoW PoS
pi/P wi/W
1/n
wi/W
pi/P wi/W
pi/P wi/W

-

-

Lachesis
wi/W
1/n
wi/W
wi/W
(+1 Saga point)

Table 7: Comparison of PoW, PoS and Lachesis protocol

26

A PREPRINT - AUGUST 5, 2021

Table 7 gives a comparison of PoW, PoS and Lachesis protocols. Let pi denote the computation power of a node and
P denote the total computation power of the network. Let wi be the stake of a node and W denote the total stake of
the whole network. Let αi denote the number of Saga points a node has been rewarded from successful validations of
ﬁnalized blocks.

Remarkably, our Lachesis protocol is more intuitive because our reward model used in stake-based validation can lead
to a more reliable and sustainable network.

6.2.2 Security

Regarding security, Lachesis has less vulnerabilities than PoW, PoS and DPoS. As for a guideline, Table 8 shows a
comparison between existing PoW, PoS, DPoS and our Lachesis protocol, with respect to the effects of the common
types of attack on previous protocols. Note that, existing PoS and DPos approaches have addressed one or some of the
known vulnerabilities in one way or the other. It is beyond the scope of this paper to give details of such approaches.

Attack type
Selﬁsh mining
Denial of service
Sybil attack
Precomputing attack
Short range attack (e.g., bribe)
Long range attack
Coin age accumulation attack
Bribe attack
Grinding attack
Nothing at stake attack
Sabotage
Double-Spending

PoW PoS
++
++
++
-
-
-
-
+
-
-
-
-

-
+
+
+
+
+
maybe
++
+
+
+
+

DPoS Lachesis (PoS+DAG)

-
+
+
-
-
+
-
+
+
+
+
+

-
+
-
-
-
-
-
-
-
-
+
-

Table 8: Comparison of Vulnerability Between PoW, PoS, DPoS and Lachesis Protols

PoW-based systems are facing selﬁsh mining attack, in which an attacker selectively reveals mined blocks in an attempt
to waste computational resources of honest miners.

Both PoW and PoS share some common vulnerabilities. DoS attack and Sybil attack are shared vulnerabilities, but
PoW are found more vulnerable. A DoS attack disrupts the network by ﬂooding the nodes. In a Sybil attack, the
attacker creates multiple fake nodes to disrupt the network. Another shared vulnerable is Bribe attack. In bribing, the
attacker performs a spending transaction, and at the same time builds an alternative chain secretly, based on the block
prior to the one containing the transaction. After the transaction gains the necessary number of conﬁrmations, the
attacker publishes his/her chain as the new valid blockchain, and the transaction is reversed. PoS is more vulnerable
because a PoS Bribe attack costs 50x lower than PoW Bribe attack.

PoS has encountered issues that were not present in PoW-based blockchains. These issues are: (1) Grinding attack:
malicious nodes can play their bias in the election process to gain more rewards or to double spend their money: (2)
Nothing at stake attack: A malicious node can mine on an alternative chain in PoS at no cost, whereas it would lose
CPU time if working on an alternative chain in PoW.

There are possible attacks that are only encountered in PoS. Both types of attack can induce conﬂicting ﬁnalized
checkpoints that will require ofﬂine coordination by honest users.

Double-Spending An attacker (a) acquires 2W/3 of stakes; (b) submits a transaction to spend some amount and then
votes to ﬁnalize that includes the transactions; (c) sends another transaction to double-spends; (d) gets caught and his
stakes are burned as honest validators have incentives to report such a misbehavior. In another scenario, an attacker
acquires (a) W/3 + (cid:15) to attempt for an attack and suppose Blue validators own W/3 − (cid:15)/2, and Red validators own the
rest W/3 − (cid:15)/2. The attacker can (b) vote for the transaction with Blue validators, and then(c) vote for the conﬂicting
transaction with the Red validations. Then both transactions will be ﬁnalized because they have 2W/3 + (cid:15)/2 votes.
Blue and Red validators may later see the ﬁnalized checkpoint, approve the transaction, but only one of them will get
paid eventually.

Sabotage (going ofﬂine) An attacker owning W/3 + (cid:15) of the stakes can appear ofﬂine by not voting and hence check-
points and transactions cannot be ﬁnalized. Users are expected to coordinate outside of the network to censor the
malicious validators.

27

A PREPRINT - AUGUST 5, 2021

Attack cost in PoS versus PoW It will cost more for an attack in PoS blockchain due to the scarcity of the native token
than in a PoW blockchain. In order to gain more stake for an attack in PoS, it will cost a lot for an outside attacker.
S/he will need 2W/3 (or W/3 for certain attacks) tokens, where W is the total number of tokens, regardless of the
token price. Acquiring more tokens will deﬁnitely increase its price, leading to a massive cost. Another challenge
is that all the tokens of a detected attempt will be burned. In contrast, PoW has no mechanism nor enforcement to
prevent an attacker from reattempting another attack. An attacker may purchase or rent the hash power again for his
next attempts.

Like PoS, our Lachesis protocol can effectively prevent potential attacks as attackers will need to acquire 2W/3 tokens
(or at least W/3 for certain attacks) to inﬂuence the validation process. Any attempt that is detected by peers will void
the attacker’s deposit.

6.3 Response to Attacks

Like other decentralized blockchain technologies, Fantom platform may also face potential attacks by attackers. Here,
we present several possible attack scenarios that are commonly studied in previous work, and we show how Lachesis
protocol can be used to prevent such attacks.

Transaction ﬂooding: A malicious participant may run a large number of valid transactions from their accounts with
the purpose of overloading the network. In order to prevent such attempts, our Fantom platform has applied a minimal
transaction fee, which is still reasonable for normal users, but will cost the attackers a signiﬁcant amount if they send
a large number of transactions.

Parasite chain attack: In some blockchains, malicious node can make a parasite chain with an attempt to make a
malicious event block. In Lachesis protocol, when a consensus is reached, ﬁnalized event block is veriﬁed before it is
added into the Main chain.

Our Lachesis protocol is 1/3-BFT, which requires less than one-third of nodes are malicious. The malicious nodes may
create a parasite chain. As root event blocks are known by nodes with more than 2W/3 validating power, a parasite
chain can only be shared between malicious nodes, which are accounted for less than one-third of participating nodes.
Nodes with a parasite chain are unable to generate roots and they will be detected by the Atropos blocks.

Double spending: A double spending attack is when a malicious actor attempts to spend their funds twice.

Let us consider an example of double spending. Entity A has 10 tokens, but it sends 10 tokens to B via node nA and
at the same time it also sends 10 tokens to C via node nC. Both node nA and node nC agree that the transaction is
valid, since A has the funds to send to B (according to nA) and C (according to nC). In case one or both of nA and
nC are malicious, they can create a fork x and y.

Lachesis protocol can structurally detect the fork by all honest nodes at some Atropos block. Specially, let us consider
the case an attacker in Lachesis protocol with W/3 + (cid:15). Assume the attacker can manage to achieve a root block rb
with Blue validators and a conﬂict root block rr with Red validators. However, in order for either of the two event
blocks to be ﬁnalized (becoming Clotho and then Atropos), each of root blocks need to be conﬁrmed by roots of next
levels. Since peers always propagate new event blocks, an attacker cannot stop the Blue and Red validators (honest)
to sent and receive event blocks to each other. Therefore, there will exist an honest validator from Blue or Red groups,
who detects conﬂicting event blocks in the next level roots aka Atroposes. Because honest validators are incentivized
to detect and prevent these misbehavior, the attacker will get caught and his stakes are burned.

Sabotage attack: Similarly, for Sabotage attack, the attacker may refuse to vote for a while. In Lachesis, honest
validators will ﬁnd out the absence of those high stake validators after some time period. Validators what are ofﬂine
for a while will be pruned from the network. After that, those pruned validators will not be counted and the network
will function like normal with the rest of nodes.

Long-range attack: In some blockchains such as Bitcoin [30], an adversary can create another chain. If this chain
is longer than the original, the network will accept the longer chain because the longer chain has had more work (or
stake) involved in its creation.

In Lachesis, this long-range attach is not possible. Adding a new block into the block chain requires an agreement of
2n/3 of nodes (or 2W/3 in our PoS model). To accomplish a long-range attack, attackers would ﬁrst need to create
more than 2n/3 malicious nodes (or gain validating power of 2W/3 in our PoS model) to create the new chain.

Bribery attack: An adversary could bribe nodes to validate conﬂicting transactions. Since 2n/3 participating nodes
(or nodes with a total more than 2W/3) are required, this would require the adversary to bribe more than n/3 of all
nodes (or W/3) to begin a bribery attack.

28

A PREPRINT - AUGUST 5, 2021

Denial of Service: Lachesis protocol is leaderless requiring 2n/3 participation. An adversary would have to attack
more than n/3 (or W/3) validators to be able to successfully mount a DDoS attack.

Sybil: Each participating node must stake a minimum amount of FTM tokens to participate in the network. But staking
2/3 of the total stake would be prohibitively expensive.

6.4 Choices of Stake Decentralization

This section presents discussions and beneﬁts of our proposed DAG-based PoS approach.

6.4.1 Low staked validators

To have more validating nodes, STAIR framework introduces a model which allows Users with small of tokens can
still run a validating node. Details are given in the paper [32]. Here, we give a summary.

There are three general types of nodes that we are considered in our new protocol. They are described as follows.

• Validators Validator node can create and validate event blocks. They can submit new transactions via their new
event blocks. To be a validator, a node needs to have more than U = 1,000,000 FTM tokens. The validating
power of a Validator node is computed by wi = U × (cid:98) ti
U (cid:99).

• Users User node can create and validate event blocks. They can submit new transactions via their new event
blocks. To be a user, a node needs to have more than 1 FTM token. The number of tokens of a User is from 1 to
U -1 = 999,999 FTM tokens. All users have the same validating power of wi = 1.

• Observers: Observer node can retrieve even blocks and do post-validation. Observers cannot perform onchain

voting (validation). Observers have zero FTM tokens and thus zero validating power (e.g., wi = 0).

The value of U is system conﬁgurable, which is determined and updated by the community. Initially, U can be set to
a high value, but can be reduced later on as the token value will increase. Note that, there is a simple model in which
validators have the validating power wi = U , instead of U × (cid:98) ti

U (cid:99).

6.4.2 Choices of L and U

The value of lower bound L=1 is used to separate between Observers and Users. The upper bound U =1, 000, 000
separates between Users and Validators. Both L and U are conﬁgurable and will vary based on the community’s
decision.

Figure 9 shows the differences between three different account types. The level of trust is proportional to the number
of tokens. Observers have a trust level of zero and they have incentives to do post-validation for Saga points. Users
have low trust as their token holding varies from 1 to U , whereas Validators have high trust level with more than U
tokens in their account. Both Users and Validators have incentives to join the network to earn transaction fees and
validation rewards.

Figure 9: Token holding, trust levels and incentives of the three account types

There are a limited number of Observers and a small pool of Validators in the entire network. The number of Users
may take a great part of the network.

29

A PREPRINT - AUGUST 5, 2021

6.4.3 Alternative models for gossip and staking

We also propose an alternative model to ensure a more balanced chance for both Users and Validators. This is to
improve liveness and safety of the network. Speciﬁcally, each User node randomly selects a Validator node in V and
each Validator node randomly chooses a User node in U. The probability of being selected is proportional to the value
wi in the set, in both cases. Alternatively, each Validator node can randomly select a node from all nodes with a
probability reversely proportional to the stake wi.

6.4.4 Network load

More observers may consume more network capacity from the validating nodes. To support more nodes, observers
may not be allowed to synchronize and retrieve events directly from Validators. Instead, we suggest to use a small
pool of Moderator nodes that will retrieve updated events from Validators. Observers will then synchronize with
the Moderator nodes to download new updates and to perform post-validation. Similarly, low staked validators can
synchronize with such Moderator nodes as well.

7 Conclusion

In this paper, we consolidate the key technologies and algorithms of our Lachesis consensus protocol used in Fantom’s
Opera network. Lachesis protocol integrates Proof of Stake into a DAG-based model to achieve more scalable and fast
consensus. It guarantees asynchronous practical BFT. Further, we present a staking model.

By using Lamport timestamps, Happened-before relation, and layer information, our algorithm for topological order-
ing of event blocks is deterministic and guarantees a consistency and ﬁnality of events in all honest nodes.

We present a formal proofs and semantics of our PoS+DAG Lachesis protocol in the Appendix. Our work extends
the formal foundation established in our previous papers [9, 31, 33], which is the ﬁrst that studies concurrent common
knowledge sematics [34] in DAG-based protocols. Formal proofs for our layering-based protocol is also presented.

7.1 Future work

We are considering several models to improve the scalability and sustainability of Opera network.
In particular,
we investigate several approaches to allow more validating nodes to join the network, whilst we aim to improve
decentralization, increase TPS and keep TTF low. A brief introduction of these approaches is given in the Appendix.

30

A PREPRINT - AUGUST 5, 2021

8 Appendix

8.1 Basic Deﬁnitions

Lachesis protocol is run via nodes representing users’ machines which together create the Opera network. The basic
units of the protocol are called event blocks - a data structure created by a single node, which contains transactions.
These event blocks reference previous event blocks that are known to the node. This ﬂow or stream of information
creates a sequence of history.

[Lachesis] Lachesis protocol is a consensus protocol

[Node] Each machine that participates in the Lachesis protocol is called a node. Let n denote the total number of
nodes.

[k] A constant deﬁned in the system.

[Peer node] A node ni has k peer nodes.

[Process] A process pi represents a machine or a node. The process identiﬁer of pi is i. A set P = {1,...,n} denotes
the set of process identiﬁers.

[Channel] A process i can send messages to process j if there is a channel (i,j). Let C ⊆ {(i,j) s.t. i, j ∈ P } denote
the set of channels.

8.1.1 Events

[Event block] Each node can create event blocks, send (receive) messages to (from) other nodes.

The structure of an event block includes the signature, generation time, transaction history, and hash information to
references. All nodes can create event blocks. The ﬁrst event block of each node is called a leaf event.

Suppose a node ni creates an event vc after an event vs in ni. Each event block has exactly k references. One of the
references is self-reference, and the other k-1 references point to the top events of ni’s k-1 peer nodes.

[Top Event] An event v is a top event of a node ni if there is no other event in ni referencing v.

[Height Vector] The height vector is the number of event blocks created by the i-th node.
[Ref] An event vr is called “ref” of event vc if the reference hash of vc points to the event vr. Denoted by vc (cid:44)→r vr.
For simplicity, we can use (cid:44)→ to denote a reference relationship (either (cid:44)→r or (cid:44)→s).

[Self-ref] An event vs is called “self-ref” of event vc, if the self-ref hash of vc points to the event vs. Denoted by
vc (cid:44)→s vs.
[Event references] Each event block has at least k references. One of the references is self-reference, and the others
point to the top events of other peer nodes.

[Self-ancestor] An event block va is self-ancestor of an event block vc if there is a sequence of events such that
vc (cid:44)→s v1 (cid:44)→s . . . (cid:44)→s vm (cid:44)→s va. Denoted by vc (cid:44)→sa va.
[Ancestor] An event block va is an ancestor of an event block vc if there is a sequence of events such that vc (cid:44)→ v1 (cid:44)→
. . . (cid:44)→ vm (cid:44)→ va. Denoted by vc (cid:44)→a va.
For simplicity, we simply use vc (cid:44)→a vs to refer both ancestor and self-ancestor relationship, unless we need to
distinguish the two cases.

8.1.2 OPERA DAG

In Lachesis protocol, the history of event blocks form a directed acyclic graph, called OPERA DAG. OPERA DAG
G = (V, E) consists of a set of vertices V and a set of edges E . A path in G is a sequence of vertices (v1, v2, . . . ,
vk) by following the edges in E such that it uses no edge more than once. Let vc be a vertex in G. A vertex vp is the
parent of vc if there is an edge from vp to vc. A vertex va is an ancestor of vc if there is a path from va to vc.

Deﬁnition 8.1 (OPERA DAG). OPERA DAG is a DAG graph G = (V, E) consisting of V vertices and E edges. Each
vertex vi ∈ V is an event block. An edge (vi, vj) ∈ E refers to a hashing reference from vi to vj; that is, vi (cid:44)→ vj.

Deﬁnition 8.2 (vertex). An event block is a vertex of the OPERA DAG.

31

A PREPRINT - AUGUST 5, 2021

Suppose a node ni creates an event vc after an event vs in ni. Each event block has at most k references. Thus, each
vertex has at most k out-edges. One of the references is self-reference, and the other k-1 references point to the top
events of ni’s k-1 peer nodes.

8.1.3 Happened-Before relation

The “happened before” relation, denoted by →, gives a partial ordering of events from a distributed system of nodes.
Each node ni (also called a process) is identiﬁed by its process identiﬁer i. For a pair of event blocks v and v(cid:48), the
relation ”→” satisﬁes: (1) If v and v(cid:48) are events of process Pi, and v comes before v(cid:48), then b → v(cid:48). (2) If v is the
send(m) by one process and v(cid:48) is the receive(m) by another process, then v → v(cid:48). (3) If v → v(cid:48) and v(cid:48) → v(cid:48)(cid:48) then
v → v(cid:48)(cid:48). Two distinct events v and v(cid:48) are said to be concurrent if v (cid:57) v(cid:48) and v(cid:48) (cid:57) v.

[Happened-Immediate-Before] An event block vx is said Happened-Immediate-Before an event block vy if vx is a
(self-) ref of vy. Denoted by vx (cid:55)→ vy.

[Happened-Before] An event block vx is said Happened-Before an event block vy if vx is a (self-) ancestor of vy.
Denoted by vx → vy.

The happened-before relation is the transitive closure of happens-immediately-before. An event vx happened before
an event vy if one of the followings happens: (a) vy (cid:44)→s vx, (b) vy (cid:44)→r vx, or (c) vy (cid:44)→a vx.
In Lachesis, we come up with the following proposition:

Proposition 8.1 (Happened-Immediate-Before OPERA). vx (cid:55)→ vy iff vy (cid:44)→ vx iff edge (vy, vx) ∈ E in OPERA DAG.
Lemma 8.2 (Happened-Before Lemma). vx → vy iff vy (cid:44)→a vx.

[Concurrent] Two event blocks vx and vy are said concurrent if neither of them happened before the other. Denoted
by vx (cid:107) vy.

Let G1 and G2 be the two OPERA DAGS of any two honest nodes. For any two vertices vx and vy, if both of them
are contained in two OPERA DAGs G1 and G2, then they are satisﬁed the following:

• vx → vy in G1 if vx → vy in G2.
• vx (cid:107) vy in G1 if vx (cid:107) vy in G2.

Happened-before deﬁnes the relationship between event blocks created and shared between nodes. If there is a path
from an event block vx to vy, then vx Happened-before vy. “vx Happened-before vy” means that the node creating
vy knows event block vx. This relation is the transitive closure of happens-immediately-before. Thus, an event vx
happened before an event vy if one of the followings happens: (a) vy (cid:44)→s vx, (b) vy (cid:44)→r vx, or (c) vy (cid:44)→a vx. The
happened-before relation of events form an acyclic directed graph G(cid:48) = (V, E(cid:48)) such that an edge (vi, vj) ∈ E(cid:48) has a
reverse direction of the same edge in E.

8.1.4 Lamport timestamp

Lachesis protocol relies on Lamport timestamps to deﬁne a topological ordering of event blocks in OPERA DAG. By
using Lamport timestamps, we do not rely on physical clocks to determine a partial ordering of events.

We use this total ordering in our Lachesis protocol to determine consensus time.

For an arbitrary total ordering ‘≺‘ of the processes, a relation ‘⇒‘ is deﬁned as follows: if v is an event in process
Pi and v(cid:48) is an event in process Pj, then v ⇒ v(cid:48) if and only if either (i) Ci(v) < Cj(v(cid:48)) or (ii) C(v) = Cj(v(cid:48)) and
Pi ≺ Pj. This deﬁnes a total ordering, and that the Clock Condition implies that if v → v(cid:48) then v ⇒ v(cid:48).

8.1.5 Domination relation

In a graph G = (V, E, r), a dominator is the relation between two vertices. A vertex v is dominated by another vertex
w, if every path in the graph from the root r to v have to go through w. The immediate dominator for a vertex v is the
last of v’s dominators, which every path in the graph have to go through to reach v.

[Pseudo top] A pseudo vertex, called top, is the parent of all top event blocks. Denoted by (cid:62).

[Pseudo bottom] A pseudo vertex, called bottom, is the child of all leaf event blocks. Denoted by ⊥.

With the pseudo vertices, we have ⊥ happened-before all event blocks. Also all event blocks happened-before (cid:62). That
is, for all event vi, ⊥ → vi and vi → (cid:62).

32

Then we deﬁne the domination relation for event blocks. To begin with, we ﬁrst introduce pseudo vertices, top and
bot, of the DAG OPERA DAG G.
[pseudo top] A pseudo vertex, called top, is the parent of all top event blocks.
Denoted by (cid:62).

A PREPRINT - AUGUST 5, 2021

[dom] An event vd dominates an event vx if every path from (cid:62) to vx must go through vd. Denoted by vd (cid:29) vx.
[strict dom] An event vd strictly dominates an event vx if vd (cid:29) vx and vd does not equal vx. Denoted by vd (cid:29)s vx.
[domfront] A vertex vd is said “domfront” a vertex vx if vd dominates an immediate predecessor of vx, but vd does
not strictly dominate vx. Denoted by vd (cid:29)f vx.
[dominance frontier] The dominance frontier of a vertex vd is the set of all nodes vx such that vd (cid:29)f vx. Denoted by
DF (vd).
From the above deﬁnitions of domfront and dominance frontier, the following holds. If vd (cid:29)f vx, then vx ∈ DF (vd).
2
For a set S of vertices, an event vd
3 -dominates S if there are more than 2/3 of vertices vx in S such that vd dominates
vx. Recall that R1 is the set of all leaf vertices in G. The 2
3 -dom set Di is
deﬁned as follows:
[ 2
3 -dom set]] A vertex vd belongs to a 2
consists of all roots di such that di (cid:54)∈ Di, ∀ i = 1..(k-1), and di
Lemma 8.3. The 2

3 -dom set D0 is the same as the set R1.The 2

3 -dom set within the graph G[vd], if vd

3 -dom set Di is the same with the root set Ri, for all nodes.

3 -dominates R1. The 2

2
3 -dominates Di−1.

3 -dom set Dk

2

8.2 Proof of aBFT

This section presents a proof to show that our Lachesis protocol is Byzantine fault tolerant when at most one-third of
participants are compromised. We ﬁrst provide some deﬁnitions, lemmas and theorems. We give formal semantics of
PoS+DAG Lachesis protocol using the semantics in concurrent common knowledge.

8.2.1 S-OPERA DAG - A Weighted DAG

The OPERA DAG (DAG) is the local view of the DAG held by each node, this local view is used to identify topological
ordering between events, to decide Clotho candidates and to compute consensus time of Atropos and events under it’s
subgraph.
Deﬁnition 8.3 (S-OPERA DAG). An S-OPERA DAG is the local view of a weighted DAG G=(V ,E, w). Each vertex
vi ∈ V is an event block. Each block has a weight, which is the validation score. An edge (vi,vj) ∈ E refers to a
hashing reference from vi to vj; that is, vi (cid:44)→ vj.

8.2.2 Consistency of DAGs

[Leaf] The ﬁrst created event block of a node is called a leaf event block.

[Root] An event block v is a root if either (1) it is the leaf event block of a node, or (2) v can reach more than 2W/3
validating power from previous roots.

[Root set] The set of all ﬁrst event blocks (leaf events) of all nodes form the ﬁrst root set R1 (|R1| = n). The root set
Rk consists of all roots ri such that ri (cid:54)∈ Ri, ∀ i = 1..(k-1) and ri can reach more than 2W/3 validating power from
other roots in the current frame, i = 1..(k-1).

[Frame] The history of events are divided into frames. Each frame contains a disjoint set of roots and event blocks.

[Subgraph] For a vertex v in a DAG G, let G[v] = (Vv, Ev) denote an induced-subgraph of G such that Vv consists
of all ancestors of v including v, and Ev is the induced edges of Vv in G.

We deﬁne a deﬁnition of consistent DAGs, which is important to deﬁne the semantics of Lachesis protocol.
Deﬁnition 8.4 (Consistent DAGs). Two OPERA DAGs G1 and G2 are consistent if for any event v contained in both
chains, G1[v] = G2[v]. Denoted by G1 ∼ G2.
Theorem 8.4 (Honest nodes have consistent DAGs). All honest nodes have consistent OPERA DAGs.

If two nodes have OPERA DAGs containing event v, v is valid and both contain all the parents referenced by v. In
Lachesis, a node will not accept an event during a sync unless that node already has all references for that event,
and thus both OPERA DAGs must contain k references for v. The cryptographic hashes are assumed to be secure,
therefore the references must be the same. By induction, all ancestors of v must be the same. Therefore, for any two

33

A PREPRINT - AUGUST 5, 2021

honest nodes, their OPERA DAGs are consistent. Thus, all honest nodes have consistent OPERA DAGs.

[Creator] If a node nx creates an event block v, then the creator of v, denoted by cr(v), is nx.
Deﬁnition 8.5 (Global DAG). A DAG GC is a global DAG of all Gi, if GC ∼ Gi for all Gi.

Let denote G (cid:118) G(cid:48) to stand for G is a subgraph of G(cid:48). Some properties of the global DAG GC are given as follows:

1. ∀Gi (GC (cid:118) Gi).
2. ∀v ∈ GC ∀Gi (GC[v] (cid:118) Gi[v]).
3. (∀vc ∈ GC) (∀vp ∈ Gi) ((vp → vc) ⇒ vp ∈ GC).

8.2.3 Root

We deﬁne the ’forkless cause’ relation, as follows:
Deﬁnition 8.6 (forkless cause). The relation f orklessCause(A, B) denote that event x is forkless caused by event y.

It means, in the subgraph of event x, x did not observe any forks from y’s creator and at least QUORUM non-cheater
validators have observed event y.
Proposition 8.5. If G1 ∼ G2, and x and y exist in both, then forklessCause(x, y) is true in G1 iff forklessCause(x, y)
is true in G2.

For any two consistent graphs G1 and G2, the subgraphs G1[x] = G2[x] and G1[y] = G2[y]. Also y’s creator and
QUORUM is the same on both DAGs.

[Validation Score] For event block v in both G1 and G2, and G1 ∼ G2, the validation score of v in G1 is identical
with that of v in G2.
Deﬁnition 8.7 (root). An event is called a root, if it is forkless causes by QUORUM roots of previous frame.

For every event, its frame number is no smaller than self-parent’s frame number.

The lowest possible frame number is 1. Relation f orklessCause is a stricter version of happened-before relation,
i.e. if y forkless causes x, then y is happened before A. Yet, unlike happened-before relation, forklessCause is
not transitive because it returns false if fork is observed. So if x forkless causes y, and y forkless causes z, it does not
mean that x forkless causes z. If there’s no forks in the graph, then the forklessCause is always transitive.
Proposition 8.6. If G1 ∼ G2, and then G1 and G2 are root consistent.

Now we state the following important propositions.
Deﬁnition 8.8 (Root consistency). Two DAGs G1 and G2 are root consistent: if for every v contained in both DAGs,
and v is a root of j-th frame in G1, then v is a root of j-th frame in G2.
Proposition 8.7. If G1 ∼ G2, then G1 and G2 are root consistent.

Proof. By consistent chains, if G1 ∼ G2 and v belongs to both chains, then G1[v] = G2[v]. We can prove the
proposition by induction. For j = 0, the ﬁrst root set is the same in both G1 and G2. Hence, it holds for j = 0. Suppose
that the proposition holds for every j from 0 to k. We prove that it also holds for j= k + 1. Suppose that v is a root of
frame fk+1 in G1. Then there exists a set S reaching 2/3 of members in G1 of frame fk such that ∀u ∈ S (u → v).
As G1 ∼ G2, and v in G2, then ∀u ∈ S (u ∈ G2). Since the proposition holds for j=k, As u is a root of frame fk in
G1, u is a root of frame fk in G2. Hence, the set S of 2/3 members u happens before v in G2. So v belongs to fk+1
in G2. The proposition is proved.

From the above proposition, one can deduce the following:
Lemma 8.8. GC is root consistent with Gi for all nodes.

By consistent DAGs, if G1 ∼ G2 and v belongs to both chains, then G1[v] = G2[v]. We can prove the proposition
by induction. For j = 0, the ﬁrst root set is the same in both G1 and G2. Hence, it holds for j = 0. Suppose that the
proposition holds for every j from 0 to k. We prove that it also holds for j= k + 1. Suppose that v is a root of frame
fk+1 in G1. Then there exists a set S reaching 2/3 of members in G1 of frame fk such that ∀u ∈ S (u → v). As
G1 ∼ G2, and v in G2, then ∀u ∈ S (u ∈ G2). Since the proposition holds for j=k, As u is a root of frame fk in G1,
u is a root of frame fk in G2. Hence, the set S of 2/3 members u happens before v in G2. So v belongs to fk+1 in G2.

34

A PREPRINT - AUGUST 5, 2021

Thus, all honest nodes have the same consistent root sets, which are the root sets in GC. Frame numbers are consistent
for all nodes.

8.2.4 Clotho

Algorithm 1 shows our Lachesis’s algorithm to decide a Clotho candidate, as given in Section 3.9.

Deﬁnition 8.9 (Clotho consistency). Two DAGs G1 and G2 are Clotho consistent: if every v contained in both DAGs,
and v is a Clotho candidate in j-th frame in G1, then v is a Clotho candidate in j-th frame in G2.

Thus, we have the following proposition.

Proposition 8.9. If G1 ∼ G2, and then G1 and G2 are Clotho consistent.

The Algorithm 1 uses computed roots, and f orklessCause relation to determine which roots are candidates for
Clotho. As proved in previous proposition and lemma, honest nodes have consistent DAGs, consistent roots, and so is
the f orklessCause relation. Therefore, the Clotho candidate selection is consistent across honest nodes. All honest
nodes will decide the same set of Clotho events from the global DAG GC.

8.2.5 Atropos

Algorithm for deciding Atropos is shown in Algorithm 2 in Section 3.10.

The set of roots and Clothos are consistent in all honest nodes, as proved in previous sections. Validators and their
stakes are unchanged within an epoch. The selection of ﬁrst Atropos from the sorted list of Clotho is deterministic. So
Atropos candidate is consistent.

Deﬁnition 8.10 (Atropos consistency). Two DAGs G1 and G2 are Atropos consistent: if every v contained in both
DAGs, and v is an Atropos in j-th frame in G1, then v is an Atropos in j-th frame in G2.

Thus, we have the following proposition.

Proposition 8.10. If G1 ∼ G2, then G1 and G2 are Atropos consistent.

All honest nodes will decide the same set of Atropos events from the global DAG GC.

8.2.6 Block

After a new Atropos is decided, the Atropos is assigned a consensus time using the Atropos’s median timestamp. The
median timestamp is the median of the times sent by all nodes. The median time makes sure that the consensus time
is BFT, even in case there may exist Byzantine nodes accounting for up to 1/3 of the validation power.

When an Atropos is decided and assigned a consensus time, a new block is created. The events under the Atropos ai
but not included yet on previous blocks will be sorted using a topological sorting algorithms outlined in Section 3.14
and 3.13. The subgraph G[ai] under the Atropos ai is consistent (as proved previous lemma). The sorting algorithms
are deterministic, and thus the sorted list of events will be the same on all honest nodes.

After sorting, all these events under the new Atropos will be added into the new block, and they are all assigned the
same consensus time, which is the Atropos’s consensus time. Transactions of these events are added into the new
block in the sorted order.

Deﬁnition 8.11 (Block Consistency). Two DAGs G1 and G2 are Block consistent: if a block bi at position i will
consist of the same sequence of event blocks and transactions, for every i.

From the above, event blocks in the subgraph rooted at the Atropos are also ﬁnal events. It leads to the following
proposition:

Proposition 8.11. If G1 ∼ G2, then G1 and G2 are Block consistent.

Let ≺ denote an arbitrary total ordering of the nodes (processes) pi and pj.

[Total ordering] Total ordering is a relation ⇒ satisfying the following: for any event vi in pi and any event vj in pj,
vi ⇒ vj if and only if either (i) Ci(vi) < Cj(vj) or (ii) Ci(vi)=Cj(vj) and pi ≺ pj.

This deﬁnes a total ordering relation. The Clock Condition implies that if vi → vj then vi ⇒ vj.

35

A PREPRINT - AUGUST 5, 2021

8.2.7 Main chain

The Main chain consists of new blocks, each of which is created based on a new Atropos.

Deﬁnition 8.12 (Consistent Chain). Two DAGs G1 and G2 are Chain consistent: if G1 and G2 are Block consistent
and they have the same number of blocks.

Proposition 8.12. All honest nodes are Chain consistent.

Theorem 8.13 (Finality). All honest nodes produce the same ordering and consensus time for ﬁnalized event blocks.

Proof. As proved in the above Section 8.2.3, the set of roots is consistent across the nodes. From section 8.2.4, the
set of Clotho events is consistent across the nodes. In Section 8.2.5, our topological sorting of Clotho, which uses
validator’stake, Lamport’s timestamp is deterministic, and so it will give the same ordering of Atropos events. The
Atropos’s timestamp is consistent between the nodes, as proved in Section 8.2.5. Section 8.2.6 outlines a proof that
the topology ordering of vertices in each subgraph under an Atropos is the same across the nodes. Also ﬁnalized event
blocks in each block is assigned the same consensus time, which is the Atropos’s consensus timestamp.

8.2.8 Fork free

Deﬁnition 8.13 (Fork). The pair of events (vx, vy) is a fork if vx and vy have the same creator, but neither is a
self-ancestor of the other. Denoted by vx (cid:116) vy.

For example, let vz be an event in node n1 and two child events vx and vy of vz. if vx (cid:44)→s vz, vy (cid:44)→s vz, vx (cid:54)(cid:44)→s vy,
vy (cid:54)(cid:44)→s vz, then (vx, vy) is a fork. The fork relation is symmetric; that is vx (cid:116) vy iff vy (cid:116) vx.
Lemma 8.14. vx (cid:116) vy iff cr(vx) = cr(vy) and vx (cid:107) vy.

Proof. By deﬁnition, (vx, vy) is a fork if cr(vx) = cr(vy), vx (cid:54)(cid:44)→a vy and vy (cid:54)(cid:44)→a vx. Using Happened-Before, the
second part means vx (cid:54)→ vy and vy (cid:54)→ vx. By deﬁnition of concurrent, we get vx (cid:107) vy.

Lemma 8.15. (Fork detection). If there is a fork vx (cid:116) vy, then vx and vy cannot both be roots on honest nodes.

Proof. Here, we show a proof by contradiction. Any honest node cannot accept a fork so vx and vy cannot be roots
on the same honest node. Now we prove a more general case. Suppose that both vx is a root on node nx and vy is root
on node ny, where nx and ny are honest nodes. Since vx is a root, it reached events created by more than 2/3 of total
validating power. Similarly, vy is a root, it reached events created by more than 2W/3. Thus, there must be an overlap
of more than W /3 members of those events in both sets. Since Lachesis protocol is 1/3-aBFT, we assume less than 1/3
of total validating power are from malicious nodes, so there must be at least one honest member in the overlap set. Let
nm be such an honest member. Because nm is honest, nm can detect and it does not allow the fork. This contradicts
the assumption. Thus, the lemma is proved.

Proposition 8.16 (Fork-free). For any Clotho c, the subgraph G[c] is fork-free.
Lemma 8.17 (Fork free lemma). If there is a fork vx (cid:116) vy, this fork will be detected by a Clotho.

Proof. Suppose that a node creates two event blocks (vx, vy), which forms a fork. Let’s assume there exist two Clotho
cx and cy that can reach both events. From Algorithm 1, each Clotho must reach more than 2W/3. Thus, there must
more than W/3 of honest nodes would know both cx and cy. Lachesis protocol assumes that less than W/3 are from
malicious nodes. Therefore, there must exist at least a node that knows both Clothos cx and cy. Then that node must
know both vx and vy. So the node will detect the fork. This is a contradiction. The lemma is proved.

Theorem 8.18 (Fork Absence). All honest nodes build the same global DAG GC, which contains no fork.

Proof. Suppose that there are two event blocks vx and vy contained in both G1 and G2, and their path between vx and
vy in G1 is not equal to that in G2. We can consider that the path difference between the nodes is a kind of fork attack.
Based on Lemma 8.17, if an attacker forks an event block, each chain of Gi and G2 can detect and remove the fork
before the Clotho is generated. Thus, any two nodes have consistent OPERA DAG.

36

A PREPRINT - AUGUST 5, 2021

8.3 Semantics of Lachesis

This section gives the formal semantics of Lachesis consensus protocol. We use CCK model [34] of an asynchronous
system as the base of the semantics of our Lachesis protocol. We present notations and concepts, which are important
for Lachesis protocol. We adapt the notations and concepts of CCK paper to suit our Lachesis protocol. Events are
ordered based on Lamport’s happened-before relation. In particular, we use Lamport’s theory to describe global states
of an asynchronous system.

8.3.1 Node State

A node is a machine participating in the Lachesis protocol. Each node has a local state consisting of local histories,
messages, event blocks, and peer information.
[State] A (local) state of node i is denoted by si

In a DAG-based protocol, each event block vi
corresponding to a unique DAG. In StakeDag, we simply denote the j-th local state of a node i by the DAG gi
Gi denote the current local DAG of a process i.

j consisting of a sequence of event blocks si
j is valid only if the reference blocks exist before it. A local state si

1, . . . , vi
j.

j=vi

0, vi

j is
j. Let

[Action] An action is a function from one local state to another local state.

An action can be either: a send(m) action of a message m, a receive(m) action, and an internal action. A message
m is a triple (cid:104)i, j, B(cid:105) where the sender i, the message recipient j, and the message body B. In StakeDag, B consists
of the content of an event block v. Let M denote the set of messages. Semantics-wise, there are two actions that can
change a process’s local state: creating a new event and receiving an event from another process.
[Event] An event is a tuple (cid:104)s, α, s(cid:48)(cid:105) consisting of a state, an action, and a state.
Sometimes, the event can be represented by the end state s(cid:48). The j-th event in history hi of process i is (cid:104)si
denoted by vi
j.

j−1, α, si

j(cid:105),

[Local history] A local history hi of i is a sequence of local states starting with an initial state. A set Hi of possible
local histories for each process i.

A process’s state can be obtained from its initial state and the sequence of actions or events that have occurred up to
the current state. Lachesis protocol uses append-only semantics. The local history may be equivalently described as
either of the followings:

hi = si
hi = si
hi = si

0, αi
0, vi
0, si

1, αi
1, vi
1, si

2, αi
2, vi
2, si

3 . . .
3 . . .
3, . . .

In Lachesis,, a local history is equivalently expressed as:
hi = gi

0, gi
j is the j-th local DAG (local state) of the process i.

where gi

1, gi

2, gi

3, . . .

[Run] Each asynchronous run is a vector of local histories. Denoted by σ = (cid:104)h1, h2, h3, ...hN (cid:105).

Let Σ denote the set of asynchronous runs. A global state of run σ is an n-vector of preﬁxes of local histories of σ,
one preﬁx per process. The happened-before relation can be used to deﬁne a consistent global state, often termed a
consistent cut, as follows.

8.3.2 Consistent Cut

An asynchronous system consists of the following sets: a set P of process identiﬁers; a set C of channels; a set Hi is
the set of possible local histories for each process i; a set A of asynchronous runs; a set M of all messages. If node i
selects node j as a peer, then (i, j) ∈ C. In general, one can express the history of each node in DAG-based protocol
in general or in Lachesis protocol in particular, in the same manner as in the CCK paper [34].

We can now use Lamport’s theory to talk about global states of an asynchronous system. A global state of run σ is an
n-vector of preﬁxes of local histories of σ, one preﬁx per process. The happened-before relation can be used to deﬁne
a consistent global state, often termed a consistent cut, as follows.
[Consistent cut] A consistent cut of a run σ is any global state such that if vi
vi
x is also in the global state. Denoted by c(σ).

y is in the global state, then

y and vj

x → vj

37

A PREPRINT - AUGUST 5, 2021

By Theorem 8.4, all nodes have consistent local OPERA DAGs. The concept of consistent cut formalizes such a
global state of a run. Consistent cuts represent the concept of scalar time in distributed computation, it is possible to
distinguish between a “before” and an “after”. In Lachesis, a consistent cut consists of all consistent OPERA DAGs.
A received event block exists in the global state implies the existence of the original event block. Note that a consistent
cut is simply a vector of local states; we will use the notation c(σ)[i] to indicate the local state of i in cut c of run σ.

The formal semantics of an asynchronous system is given via the satisfaction relation (cid:96). Intuitively c(σ) (cid:96) φ, “c(σ)
satisﬁes φ,” if fact φ is true in cut c of run σ. We assume that we are given a function π that assigns a truth value to
each primitive proposition p. The truth of a primitive proposition p in c(σ) is determined by π and c. This deﬁnes
c(σ) (cid:96) p.
Deﬁnition 8.14 (Equivalent cuts). Two cuts c(σ) and c(cid:48)(σ(cid:48)) are equivalent with respect to i if:

c(σ) ∼i c(cid:48)(σ(cid:48)) ⇔ c(σ)[i] = c(cid:48)(σ(cid:48))[i]

A message chain of an asynchronous run is a sequence of messages m1, m2, m3, . . . , such that, for all i, receive(mi)
→ send(mi+1). Consequently, send(m1) → receive(m1) → send(m2) → receive(m2) → send(m3) . . . .

We introduce two families of modal operators, denoted by Ki and Pi, respectively. Each family indexed by process
identiﬁers. Given a fact φ, the modal operators are deﬁned as follows:

[i knows φ] Ki(φ) represents the statement “φ is true in all possible consistent global states that include i’s local
state”.

c(σ) (cid:96) Ki(φ) ⇔ ∀c(cid:48)(σ(cid:48))(c(cid:48)(σ(cid:48)) ∼i c(σ) ⇒ c(cid:48)(σ(cid:48)) (cid:96) φ)

[i partially knows φ] Pi(φ) represents the statement “there is some consistent global state in this run that includes i’s
local state, in which φ is true.”

c(σ) (cid:96) Pi(φ) ⇔ ∃c(cid:48)(σ)(c(cid:48)(σ) ∼i c(σ) ∧ c(cid:48)(σ) (cid:96) φ)

The last modal operator is concurrent common knowledge (CCK), denoted by C C.
Deﬁnition 8.15 (Concurrent common knowledge). C C(φ) is deﬁned as a ﬁxed point of M C(φ ∧ X)

CCK deﬁnes a state of process knowledge that implies that all processes are in that same state of knowledge, with
respect to φ, along some cut of the run. In other words, we want a state of knowledge X satisfying: X = M C(φ ∧ X).
C C will be deﬁned semantically as the weakest such ﬁxed point, namely as the greatest ﬁxed-point of M C(φ ∧ X). It
therefore satisﬁes:

C C(φ) ⇔ M C(φ ∧ C C(φ))

Thus, Pi(φ) states that there is some cut in the same asynchronous run σ including i’s local state, such that φ is true
in that cut.

Note that φ implies Pi(φ). But it is not the case, in general, that Pi(φ) implies φ or even that M C(φ) implies φ. The
truth of M C(φ) is determined with respect to some cut c(σ). A process cannot distinguish which cut, of the perhaps
many cuts that are in the run and consistent with its local state, satisﬁes φ; it can only know the existence of such a
cut.

[Global fact] Fact φ is valid in system Σ, denoted by Σ (cid:96) φ, if φ is true in all cuts of all runs of Σ.

Σ (cid:96) φ ⇔ (∀σ ∈ Σ)(∀c)(c(a) (cid:96) φ)

[Valid Fact] Fact φ is valid, denoted (cid:96) φ, if φ is valid in all systems, i.e. (∀Σ)(Σ (cid:96) φ).

[Local fact] A fact φ is local to process i in system Σ if Σ (cid:96) (φ ⇒ Kiφ).

8.3.3 State in aBFT Lachesis

We introduce a new modal operator, written as M C, which stands for “majority concurrently knows”. The deﬁnition
of M C(φ) is as follows.

38

A PREPRINT - AUGUST 5, 2021

Deﬁnition 8.16 (Majority concurrently knows).

M C(φ) =def

(cid:94)

i∈S

KiPi(φ),

where S ⊆ P and |S| > 2n/3.

This is adapted from the “everyone concurrently knows” in CCK paper [34]. In the presence of one-third of faulty
nodes, the original operator “everyone concurrently knows” is sometimes not feasible. Our modal operator M C(φ)
ﬁts precisely the semantics for BFT systems, in which unreliable processes may exist.

A weighted version of “majority concurrently knows” is “Quorum concurrently knows”, which is deﬁned as follows:
Deﬁnition 8.17 (Quorum concurrently knows).

QC(φ) =def

(cid:94)

i∈S

KiPi(φ),

where S ⊆ P and w(S) > 2W/3.

The total weight of a set is equal to the sum of the weights of each node in the set; that is, w(S) = (cid:80)|S|
Theorem 8.19. If φ is local to process i in system Σ, then Σ (cid:96) (Pi(φ) ⇒ φ).
Lemma 8.20. If fact φ is local to 2/3 of the processes in a system Σ, then Σ (cid:96) (M C(φ) ⇒ φ) and furthermore
Σ (cid:96) (C C(φ) ⇒ φ).
Deﬁnition 8.18. A fact φ is attained in run σ if ∃c(σ)(c(σ) (cid:96) φ).

1 w(ni).

Often, we refer to “knowing” a fact φ in a state rather than in a consistent cut, since knowledge is dependent only on
the local state of a process. Formally, i knows φ in state s is shorthand for

∀c(σ)(c(σ)[i] = s ⇒ c(σ) (cid:96) φ)

For example, if a process in Lachesis protocol knows a fork exists (i.e., φ is the exsistenc of fork) in its local state s
(i.e., gi

j), then a consistent cut contains the state s will know the existence of that fork.

Deﬁnition 8.19 (i learns φ). Process i learns φ in state si
run σ, k < j, i does not know φ.

j of run σ if i knows φ in si

j and, for all previous states si

k in

The following theorem says that if CC(φ is attained in a run then all processes i learn PiC C(φ) along a single
consistent cut.
Theorem 8.21 (attainment). If C C(φ) is attained in a run σ, then the set of states in which all processes learn
PiC C(φ) forms a consistent cut in σ.

We have presented a formal semantics of Lachesis protocol based on the concepts and notations of concurrent common
knowledge [34]. For a proof of the above theorems and lemmas in this Section, we can use similar proofs as described
in the original CCK paper.

With the formal semantics of Lachesis, the theorems and lemmas described in Section 8.2 can be expressed in term
of CCK. For example, one can study a fact φ (or some primitive proposition p) in the following forms: ‘is there any
existence of fork?’. One can make Lachesis-speciﬁc questions like ’is event block v a root?’, ’is v a clotho?’, or ’is
v a atropos?’. This is a remarkable result, since we are the ﬁrst that deﬁne such a formal semantics for DAG-based
protocol.

9 Alternatives for scalability

9.1 Layering-based Model

In this section, we present an approach that uses graph layering on top of our PoS DAG-based Lachesis protocol.
Our general model is given in StakeDag protocol [33], which is based on layering-based approach of ONLAY frame-
work [31]. After using layering algorithms on the OPERA DAG, the assigned layers are then used to determine
consensus of the event blocks.

39

A PREPRINT - AUGUST 5, 2021

9.1.1 Layering Deﬁnitions

For a directed acyclic graph G=(V ,E), a layering is to assign a layer number to each vertex in G.

[Layering] A layering (or levelling) of G is a topological numbering φ of G, φ : V → Z, mapping the set of vertices
V of G to integers such that φ(v) ≥ φ(u) + 1 for every directed edge (u, v) ∈ E. If φ(v)=j, then v is a layer-j vertex
and Vj = φ−1(j) is the jth layer of G.
From happened before relation, vi → vj iff φ(vi) < φ(vj). Layering φ partitions the set of vertices V into a ﬁnite
number l of non-empty disjoint subsets (called layers) V1,V2,. . . , Vl, such that V = ∪l
i=1Vi. Each vertex is assigned to
a layer Vj, where 1 ≤ j ≤ l, such that every edge (u,v) ∈ E, u ∈ Vi, v ∈ Vj, 1 ≤ i < j ≤ l.

[Hierarchical graph] For a layering φ, the produced graph H=(V ,E,φ) is a hierarchical graph, which is also called
an l-layered directed graph and could be represented as H=(V1,V2,. . . ,Vl;E).

Approaches to DAG layering include minimizing the height [3, 39], ﬁxed width layering [12] and minimizing the
total edge span. A simple layering algorithm to achieve minimum height is as follows: First, all source vertices
are placed in the ﬁrst layer V1. Then, the layer φ(v) for every remaining vertex v is recursively deﬁned by φ(v) =
max{φ(u)|(u, v) ∈ E} + 1. This algorithm produces a layering where many vertices will stay close to the bottom,
and hence the number of layers k is kept minimized. By using a topological ordering of the vertices [28], the algorithm
can be implemented in linear time O(|V |+|E|).

9.1.2 H-OPERA DAG

We introduce a new notion of H-OPERA DAG, which is built on top of the OPERA DAG by using graph layering.

Deﬁnition 9.1 (H-OPERA DAG). An H-OPERA DAG is the result hierarchical graph H = (V, E, φ).

H-OPERA DAG can also be represented by H=(V1,V2,. . . ,Vl;E). Vertices are assigned with layers such that each
edge (u,v) ∈ E ﬂows from a higher layer to a lower layer i.e., φ(u) > φ(v).

Generally speaking, a layering can produce a proper hierarchical graph by introducing dummy vertices along every
long edge. However, for consensus protocol, such dummy vertices can increase the computational cost of the H-
OPERA itself and of the following steps to determine consensus. Thus, in ONLAY we consider layering algorithms
that do not introduce dummy vertices.

A simple approach to compute H-OPERA DAG is to consider OPERA DAG as a static graph G and then apply
a layering algorithm such as either LongestPathLayer (LPL) algorithm or Coffman-Graham (CG) algorithm on the
graph G. LPL algorithm can achieve HG in linear time O(|V |+|E|) with the minimum height. CG algorithm has a
time complexity of O(|V |2), for a given W .

To handle large G more efﬁciently, we introduce online layering algorithms. We adopt the online layering algorithms
introduced in [31]. Speciﬁcally, we use Online Longest Path Layering (O-LPL) and Online Coffman-Graham (O-CG)
algorithm, which assign layers to the vertices in diff graph G(cid:48)=(V (cid:48),E(cid:48)) consisting of new self events and received
unknown events. The algorithms are efﬁcient and scalable to compute the layering of large dynamic S-OPERA DAG.
Event blocks from a new graph update are sorted in topological order before being assigned layering information.

9.1.3 Root Graph

There is a Root selection algorithm given in Section 3.8. Here, we present a new approach that uses root graph and
frame assignment.
Deﬁnition 9.2 (Root graph). A root graph GR=(VR, ER) is a DAG consisting of vertices as roots and edges represent
their reachability.

In root graph GR, the set of roots VR is a subset of V . Edge (u,v) ∈ ER only if u and v are roots and u can reach v
following edges in E i.e., v → u. Root graph contains the n genesis vertices i.e., the n leaf event blocks. A vertex
v that can reach at least 2/3W + 1 of the current set of all roots VR is itself a root. For each root ri that new root r
reaches, we include a new edge (r, ri) into the set of root edges ER. Note that, if a root r reaches two other roots r1
and r2 of the same node, and φ(r1) > φ(r2) then we include only one edge (r, r1) in ER. This requirement ensures
that each root of a node can have at most one edge to any other node.

Figure 10a shows an example of the H-OPERA DAG, which is the resulting hierarchical graph from a laying algorithm.
Vertices of the same layer are placed on a horizontal line. There are 14 layers which are labelled as L0, L1, ..., to L14.
Roots are shown in red colors. Figure 10b gives an example of the root graph constructed from the H-OPERA DAG.

40

A PREPRINT - AUGUST 5, 2021

Algorithm 5 Root graph algorithm
1: Require: H-OPERA DAG
2: Output: root graph GR = (VR, ER)
3: R ← set of leaf events
4: VR ← R
5: ER ← ∅
6: function BUILDROOTGRAPH(φ, l)
for each layer i=1..l do
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

VR ← VR ∪ {v}
Z ← Z ∪ {v}

for each vertex vj in Z do

Z ← ∅
for each vertex v in layer φi do

for each vertex vi in S do
ER ← ER ∪ {(v, vi)}

S ← the set of vertices in R that v reaches
if wS > 2/3W then

Let vold be a root in R such that cr(vj) = cr(vold)
R ← R \ {vold} ∪ {vj}

(a) Hierarchical graph with frame
assignment

(b) Root graph

(c) Root-layering of a root graph

Figure 10: An example of hierachical graph and root graph

9.1.4 Frame Assignment

We then present a determinstic approach to frame assignment, which assigns each event block a unique frame number.
First, we show how to assign frames to root vertices via the so-called root-layering. Second, we then assign non-root
vertices to the determined frame numbers with respect to the topological ordering of the vertices.

Deﬁnition 9.3 (Root-layering). A root layering assigns each root with a unique layer (frame) number.

Here, we apply root layering on the root graph GR = (VR, ER). Then, we use root layering information to assign
frames to non-root vertices. A frame assignment assigns every vertex v in H-OPERA DAG H a frame number φF (v)
such that

• φF (u) ≥ φF (v) for every directed edge (u, v) in H;
• for each root r, φF (r) = φR(r);
• for every pair of u and v: if φ(u) ≥ φ(v) then φF (u) ≥ φF (v); if φ(u) = φ(v) then φF (u) = φF (v).

41

A PREPRINT - AUGUST 5, 2021

Figure 10c depicts an example of root layering for the root graph in Figure 10b.

9.1.5 Consensus

For an OPERA DAG G, let G[v] denote the subgraph of G that contains nodes and edges reachable from v.

[Consistent chains] For two chains G1 and G2, they are consistent if for any event v contained in both chains,
G1[v] = G2[v]. Denoted by G1 ∼ G2.
[Global OPERA DAG] A global consistent chain GC is a chain such that GC ∼ Gi for all Gi.
The layering of consistent OPERA DAGs is consistent itself.
[Consistent layering] For any two consistent OPERA DAGs G1 and G2, layering results φG1 and φG2 are consistent
if φG1(v) = φG2 (v), for any vertex v common to both chains. Denoted by φG1 ∼ φG2 .
Theorem 9.1. For two consistent OPERA DAGs G1 and G2, the resulting H-OPERA DAGs using layering φLP L are
consistent.

The theorem states that for any event v contained in both OPERA DAGs, φG1
LP L(v). Since G1 ∼ G2, we
have G1[v] = G2[v]. Thus, the height of v is the same in both G1 and G2. Thus, the assigned layer using φLP L is the
same for v in both chains.
Proposition 9.2 (Consistent root graphs). Two root graphs GR and G(cid:48)
consistent.

R from two consistent H-OPERA DAGs are

LP L(v) = φG2

We can have similar proofs for the consistency of roots, clothos, and atropos as in Section 8.2.3 8.2.4, 8.2.5.

9.1.6 Main procedure

Algorithm 7 shows the main function serving as the entry point to launch StakeDag protocol. The main function
consists of two main loops, which are they run asynchronously in parallel. The ﬁrst loop attempts to request for new
nodes from other nodes, to create new event blocks and then communicate about them with all other nodes. The
second loop will accept any incoming sync request from other nodes. The node will retrieve updates from other nodes
and will then send responses that consist of its known events.

Algorithm 6 StakeDag Main Function
1: function MAIN FUNCTION
2: loop:
3:
4:
5:
6:
7:
8:
9:
10: loop:
11:
12:

Let {ni} ← k-PeerSelectionAlgo()
Sync request to each node in {ni}
(SyncPeer) all known events to each node in {ni}
Create new event block: newBlock(n, {ni})
(SyncOther) Broadcast out the message
Update DAG G
Call ComputeConsensus(G)

Accepts sync request from a node
Sync all known events by StakeDag protocol

1: function COMPUTECONSENSUS(DAG)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

Apply layering to obtain H-OPERA DAG*
Update ﬂagtable
Compute validation score*
Root selection
Compute root graph*
Compute global state
Clotho selection
Atropos selection
Order vertices and assign consensus time
Compute Main chain

9.2 STAIR Model

We present a model to allow low-staked accounts to join as a validating node, as described in STAIR [32]. In this
model, we distinguish nodes into users (low-staked), validators (high-staked) and monitors (zero-staked). Users have
less than U tokens, but can join the network as a validator node. We give a summary to highlights the idea of STAIR’s
model.

Event Block Creation: Unlike StakeDag, STAIR requires one of the other-ref blocks is from a creator of an opposite
type. That is, if ni is a User, then one of the other-ref parents must be from a Validator block. If ni is a Validator, then
one of the other-ref parents must be a User block.

x-DAG: Cross-type DAG: STAIR protocol maintains the DAG structure x-DAG, which is based on the concept of
S-OPERA DAG in our StakeDag protocol [33]. x-Dag chain is a weighted directed acyclic graph G=(V ,E), where

42

V is the set of event blocks, E is the set of edges. Each vertex (or event block) is associated with a validation score,
which is the total weights of the roots reachable from it. When a block becomes a root, it is assigned a weight, which
is the same with the validating power pi of the creator node.

A PREPRINT - AUGUST 5, 2021

Figure 11: Examples of x-DAG with users and validators

Figure 11 gives an example of x-DAG, which is a weighted acyclic directed graph stored as the local view of each
node. There are ﬁve nodes in the example, three of which are normal users with validating power of 1, and the rest
are validators. Each event block has k=2 references. User blocks are colored Black and Validator blocks are in Red.
Each Red block has one Red parent (same creator) and one Black parent (created by a User). Each Black block has
one Black parent (same creator) and one Red parent (created by a Validator). As depicted, the example x-DAG is a
RedBlack k-ary (directed acyclic) graph.

Stake-aware Peer Synchronization: In STAIR protocol, we present a approach for stake-aware peer synchronization
to improve the fairness, safety and liveness of the protocol. STAIR protocol enforces User nodes (low stake) to connect
with Validators (high stake) in order to gain more validating power for User’s new own blocks. Validators are also
required to connect User nodes so as to validate the User’s new blocks. Algorithm 7 shows the main function serving
as the entry point to launch STAIR protocol.

Algorithm 7 STAIR Main Function
1: U ← set of users (ti < U ).
2: V ← set of validators (ti ≥ U ).
3: function MAIN FUNCTION(nm)
4: loop:
5:
6:
7:
8:
9:
10:
11:
12: loop:
13:
14:

Let {ni} ← k-PeerSelectionAlgo(nm, U, V)
Sync request to each node in {ni}
(SyncPeer) all known events to each node in {ni}
Create new event block: newBlock(nm, {ni})
(SyncAll) Broadcast out the message
Update x-DAG G
Call ComputeConsensus(G)

Accepts sync request from a node
Sync all known events by STAIR protocol

Saga points: To measure the performance of participants, we propose a new notion, called Saga points (Sp). The Saga
points or simply ’points’ of a node ni, is denoted by αi. As an example, Sp point is deﬁned as the number of own
events created by ni that has a parent (other-ref) as a ﬁnalized event (Atropos).
In STAIR, we propose to use Saga points to scale the ﬁnal value of validating power, say w(cid:48)
i = αi.wi. Saga point is
perfectly a good evidence for the validation achievement made by a node. Saga point is hard to get and nodes have to
make a long time commitment to accumulate the points.

43

A PREPRINT - AUGUST 5, 2021

10 Reference

[1] J. Aspnes. Randomized protocols for asynchronous consensus. Distributed Computing, 16(2-3):165–175, 2003.
[2] L. Baird. Hashgraph consensus: fair, fast, byzantine fault tolerance. Technical report, 2016.
[3] J. Bang-Jensen and G. Z. Gutin. Digraphs: theory, algorithms and applications. Springer Science & Business

Media, 2008.

[4] I. Bentov, A. Gabizon, and A. Mizrahi. Cryptocurrencies without proof of work. In International Conference on

Financial Cryptography and Data Security, pages 142–157. Springer, 2016.

[5] BitShares. Delegated proof-of-stake consensus.
[6] V. Buterin et al. Ethereum white paper. GitHub repository, 1:22–23, 2013.
[7] M. Castro and B. Liskov. Practical byzantine fault tolerance. In Proceedings of the Third Symposium on Op-
erating Systems Design and Implementation, OSDI ’99, pages 173–186, Berkeley, CA, USA, 1999. USENIX
Association.

[8] J. Chen and S. Micali. ALGORAND: the efﬁcient and democratic ledger. CoRR, abs/1607.01341, 2016.
[9] S.-M. Choi, J. Park, Q. Nguyen, and A. Cronje. Fantom: A scalable framework for asynchronous distributed

systems. arXiv preprint arXiv:1810.10360, 2018.

[10] S.-M. Choi, J. Park, Q. Nguyen, K. Jang, H. Cheob, Y.-S. Han, and B.-I. Ahn. Opera: Reasoning about continuous

common knowledge in asynchronous distributed systems, 2018.

[11] A. Churyumov. Byteball: A decentralized system for storage and transfer of value, 2016.
[12] E. G. Coffman, Jr. and R. L. Graham. Optimal scheduling for two-processor systems. Acta Inf., 1(3):200–213,

Sept. 1972.

[13] T. A. Crew. Ark whitepaper.
[14] G. Danezis and D. Hrycyszyn. Blockmania: from block dags to consensus, 2018.
[15] EOS. EOS.IO technical white paper.
[16] Ethcore. Parity: Next generation ethereum browser.
[17] Y. Gilad, R. Hemo, S. Micali, G. Vlachos, and N. Zeldovich. Algorand: Scaling byzantine agreements for
cryptocurrencies. In Proceedings of the 26th Symposium on Operating Systems Principles, pages 51–68. ACM,
2017.

[18] S. King and S. Nadal. Ppcoin: Peer-to-peer crypto-currency with proof-of-stake. 19, 2012.
[19] R. Kotla, L. Alvisi, M. Dahlin, A. Clement, and E. Wong. Zyzzyva: speculative byzantine fault tolerance. ACM

SIGOPS Operating Systems Review, 41(6):45–58, 2007.

[20] J. Kwon. Tendermint: Consensus without mining. https://tendermint.com/static/docs/tendermint.pdf, 2014.
[21] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of the ACM,

21(7):558–565, 1978.

[22] L. Lamport et al. Paxos made simple. ACM Sigact News, 32(4):18–25, 2001.
[23] L. Lamport, R. Shostak, and M. Pease. The byzantine generals problem. ACM Trans. Program. Lang. Syst.,

4(3):382–401, July 1982.

[24] D. Larimer. Delegated proof-of-stake (dpos), 2014.
[25] C. LeMahieu. Raiblocks: A feeless distributed cryptocurrency network, 2017.
[26] S. D. Lerner. Dagcoin, 2015.
[27] C. Li, P. Li, W. Xu, F. Long, and A. C.-c. Yao. Scaling nakamoto consensus to thousands of transactions per

second. arXiv preprint arXiv:1805.03870, 2018.

[28] K. Mehlhorn. Data Structures and Algorithms 2: Graph Algorithms and NP-Completeness, volume 2 of EATCS

Monographs on Theoretical Computer Science. Springer, 1984.

[29] A. Miller, Y. Xia, K. Croman, E. Shi, and D. Song. The honey badger of bft protocols. In Proceedings of the

2016 ACM SIGSAC Conference on Computer and Communications Security, pages 31–42. ACM, 2016.

[30] S. Nakamoto. Bitcoin: A peer-to-peer electronic cash system, 2008.
[31] Q. Nguyen and A. Cronje. ONLAY: Online Layering for scalable asynchronous BFT system. arXiv preprint

arXiv:1905.04867, 2019.

44

A PREPRINT - AUGUST 5, 2021

[32] Q. Nguyen, A. Cronje, M. Kong, A. Kampa, and G. Samman. Stairdag: Cross-dag validation for scalable BFT

consensus. CoRR, abs/1908.11810, 2019.

[33] Q. Nguyen, A. Cronje, M. Kong, A. Kampa, and G. Samman. StakeDag: Stake-based Consensus For Scalable

Trustless Systems. CoRR, abs/1907.03655, 2019.

[34] P. Panangaden and K. Taylor. Concurrent common knowledge: deﬁning agreement for asynchronous systems.

Distributed Computing, 6(2):73–93, 1992.

[35] A. Panarello, N. Tapas, G. Merlino, F. Longo, and A. Puliaﬁto. Blockchain and iot integration: A systematic

survey. Sensors, 18(8):2575, 2018.

[36] R. Pass and E. Shi. Fruitchains: A fair blockchain. In Proceedings of the ACM Symposium on Principles of

Distributed Computing, pages 315–324. ACM, 2017.

[37] F. H. Q. M. S. S. Pierre Chevalier, Bartomiej Kamin ski. Protocol for asynchronous, reliable, secure and efﬁcient

consensus (parsec), 2018.

[38] S. Popov. The tangle, 2017.
[39] R. Sedgewick and K. Wayne. Algorithms. Addison-Wesley Professional, 4th edition, 2011.
[40] H. H. S. Sheikh, R. M. R. Azmathullah, and F. R. R. HAQUE. Proof-of-work vs proof-of-stake: A comparative

analysis and an approach to blockchain consensus mechanism. 2018.

[41] Y. Sompolinsky, Y. Lewenberg, and A. Zohar. Spectre: A fast and scalable cryptocurrency protocol.

IACR

Cryptology ePrint Archive, 2016:1159, 2016.

[42] Y. Sompolinsky and A. Zohar. Phantom, ghostdag: Two scalable blockdag protocols, 2008.
[43] Steem. Steem white paper.
[44] S. N. Sunny King. Ppcoin: Peer-to-peer crypto-currency with proof-of-stake, 2012.
[45] P. Vasin. Blackcoin’s proof-of-stake protocol v2. URL: https://blackcoin.co/blackcoin-pos-protocol-v2-

whitepaper.pdf, 71, 2014.

[46] B. Vitalik and G. Virgil. Casper the friendly ﬁnality gadget. CoRR, abs/1710.09437, 2017.

45

