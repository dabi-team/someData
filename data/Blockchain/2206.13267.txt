2
2
0
2

n
u
J

7
2

]
P
A
.
h
t
a
m

[

1
v
7
6
2
3
1
.
6
0
2
2
:
v
i
X
r
a

A Stochastic Target Problem for Branching Diﬀusions

Idris Kharroubi∗
LPSM, UMR CNRS 8001,
Sorbonne Universit´e and Universit´e Paris Cit´e,
idris.kharroubi @ sorbonne-universite.fr

Antonio Ocello
LPSM, UMR CNRS 8001,
Sorbonne Universit´e and Universit´e Paris Cit´e,
antonio.ocello @ sorbonne-universite.fr

June 28, 2022

Abstract

We consider an optimal stochastic target problem for branching diﬀusion processes.
This problem consists in ﬁnding the minimal condition for which a control allows the
underlying branching process to reach a target set at a ﬁnite terminal time for each of
its branches. This problem is motivated by an example from ﬁntech where we look for
the super-replication price of options on blockchain based cryptocurrencies. We ﬁrst
state a dynamic programming principle for the value function of the stochastic target
problem. We then show that the value function can be reduced to a new function
with a ﬁnite dimensional argument by a so called branching property. Under wide
conditions, this last function is shown to be the unique viscosity solution to an HJB
variational inequality.

MSC Classiﬁcation- 35K10, 49L20, 49L25, 60J80, 91G20
Keywords— Stochastic target control, ﬁntech, cryptocurrencies options, branching diﬀusion
process, dynamic programming principle, Hamilton-Jacobi-Bellman equation, viscosity solution.

1

Introduction

The theory of optimal stochastic control has been extensively developed since the pioneering works
in the 1950 decade. One reason for the growing attraction of this theory is the variety of its
applications, such as physics, biology, economics or ﬁnance.

In the last ﬁeld, stochastic control theory appears to be a very natural tool as it provides
solutions to the optimal portfolio choice issue. The need to control risks related to ﬁnancial

∗Research of the author partially supported by ANR grant RELISCOP.

1

 
 
 
 
 
 
investments leads to new stochastic optimization problems. Here, one looks for the minimal initial
endowment needed to ﬁnd a ﬁnancial strategy whose ﬁnal position satisﬁes some given constraints.
Such optimization problems are called optimal stochastic target problem and have been widely
studied (see e.g. [24, 25, 2, 4, 3]).

The classical stochastic control theory has also been developed for other kind of stochastic
processes such as branching diﬀusions. Those processes describe the evolution of a population
of individuals with similar features concerning their dynamics and their reproduction. Branching
[13, 14, 15], who provided
processes have been ﬁrst studied by Skorohod [23] and Ikeda et al.
Feynmann-Kac presentation of solution to parabolic semi-linear partial diﬀerential equations (PDE
for short). Since those pioneering works, branching processes have been extensively studied in
particular their scaling limits and the link with superprocesses (see [8]). Recently, they were also
used by Henry-Labord`ere et al. [12] for Monte Carlo based numerical approximation of solutions
to semilinear parabolic PDEs.

In the case where the branching processes are controlled, ¨Ust¨unel [27] considers a ﬁnite horizon
optimization problem. He restricted to Markov controls acting only on the drift coeﬃcient. Follow-
ing a martingale problem approach, he proved existence of optimal controls under wide conditions.
Nisio [21] considers the case where both the drift and diﬀusion coeﬃcients are controlled. She char-
acterizes the related value function as a viscosity solution to a nonlinear parabolic PDE of HJB
type. Then, Claisse [5] extends the previous results by allowing controls that may not preserve
independance of the particles and considering the lifespan and the progeny coeﬃcients to depend
on the position and the control. Following the approach of Fleming and Soner [11] which relies
on a result due to Krylov [19], the value function is approximated by a sequence of smooth value
functions corresponding to small perturbations of the initial problem. This is what allows to prove
a dynamic programming principle (DPP for short) and to derive a related dynamic programming
equation.

In this paper, we investigate a stochastic target problem where the underlying controlled pro-
cess is a branching diﬀusion. The problem consists in ﬁnding a minimal initial condition for a
given target branching diﬀusion such that it dominates a function of another controlled branching
diﬀusion for each particle alive at a given terminal time.

We then give an extended equivalent formulation of the problem.

Indeed, as the starting
condition of the target branching process may contain several points, the previous problem is not
well posed. We therefore look for the minimal value dominating all starting points such that the
related branching process satisﬁes the terminal constraint.

Such a problem ﬁnds an application in mathematical ﬁnance, when dealing with the optimal
investment on crypto-currencies. For these assets, branching may appear due to their structure,
leading to new assets (see e.g. [10]). In this framework, the super-replication issue remains to the
best of our knowledge unsolved. Our setting provides a possible solution and we give a detailed
example as an illustration.

We adopt a DPP approach to characterize the value function of our branching stochastic
target problem. Contrary to [5], our argument do not rely on the existence of regular solution to
approximated PDEs but on probabilistic results. We use a measurable selection theorem similar
to that of [24]. Combining it with a conditioning property for the law of the controlled process, we
get the DPP.

We use it to identify the value function as a solution to a dynamic programming PDE. We ﬁrst
show, as in [5], a branching property. It relates the value function at a given starting condition to
the optimal values at its points. This allows to see the value function as a sequence of functions
Rd to R indexed by the (countable) set of particle labels. Contrary to the classical
from [0, T ]
branching property, ours writes the value function as a maximum instead of a product. Hence, it

×

2

entails irregularity bringing us out of the range of regular solutions.

We therefore adopt the framework of viscosity solutions. The dependence in the label variable
leads to adapt the deﬁnition of viscosity solutions and to impose a continuous bound in the label.
Using the DPP, the value function is shown to be a viscosity solution to a partial diﬀerential
inequality of two terms. The ﬁrst one is the classical nonlinear second order operator for classical
diﬀusion processes, written as a supremum of a linear operator over controls that kill the diﬀusive
part. The restriction to these controls is due to the terminal constraint, imposed with probability
one (see [4]). The second term expresses a monotonicity with respect to the label. More precisely,
the value function taken at some label must be greater than its value on any other oﬀspring label.
Surprisingly, our PDE do not contain any polynomial of the value function function as we classically
have in PDEs related to branching processes. This is due to the speciﬁc structure of the considered
control problem. We complete this parabolic PDE property by a terminal condition.

To get a full characterization of our value function, we ﬁnally consider the uniqueness to the
PDE. Under additional assumptions, we prove a comparison theorem using the classical approach
of doubling variable combined with Ishii’s lemma. This shows that the value function is the unique
viscosity solution to the PDE. As a byproduct we get the continuity of the value function on the
parabolic interior of the domain.

The remainder of the paper is organized as follows. In Section 2, we present the branching
stochastic target problem and provide an example of application inspired from ﬁntech. In Section
3, we set the dynamic programming principle. We ﬁnally show in Section 4 viscosity properties of
the value function and provide a uniqueness result to the related PDE. Finally we relegate some
technical results needed in the proof of the conditionning property to the appendix.

2 The problem

2.1 Branching diﬀusions

We start by a description of the underling controlled processes. As those processes are of branching
type, we ﬁrst introduce the label set.

Label set For n
For n, m
concatenation ij

≥

≥
Nn+m by

∈

1, a multi-integer i = (i1, . . . , in)

1 and two multi-integers i = i1 . . . in

Nn is simply denoted by i = i1 . . . in.
Nm, we deﬁne their

Nn and j = j1 . . . jm

∈

∈

∈

To describe the evolution of the particle population, we introduce the set of labels

ij = i1 . . . inj1 . . . jm .

(2.1)

deﬁned by

I

=

∅
{

} ∪

I

+∞

Nn .

n=1
[

The label ∅ corresponds to the mother particle. We extend the concatenation (2.1) to the whole
set

by

I

for all i
are labelled i0, . . . , i(k

∈ I

−

. When the particle labelled i = i1 . . . in

Nn gives birth to k particles, the oﬀ-springs

1). We also deﬁne the partial ordering relation

∈

(resp.

) by

≺

(cid:22)

∅i = i∅ = i

j

(resp. j

i

i

(cid:22)

≺

ℓ

ℓ

⇔ ∃

⇔ ∃

∈ I

∈ I \ {

∅

: i = jℓ)

}

: i = jℓ

3

for all i, j

∈ I

. We introduce the distance dI on

n

deﬁned by

I

m

dI (i, j) =

(iℓ + 1) +

(jℓ′ + 1) ,

for i = i1 · · ·

in

∈

Nn, j = j1 · · ·

jm

Xℓ=p+1
Nm, with

∈
p = max

Xℓ′=p+1

ℓ
{

≥

1 : iℓ = jℓ

.

}

We next write

:= dI (i,

i
|
|

) for i
∅

.

∈ I

Set of ﬁnite measures
that, we endow the set

I ×

In the sequel we shall consider ﬁnite measure on
Rℓ with the metric d deﬁned by

I ×

Rℓ for ℓ

1. For

≥

d ((i, x), (j, y)) = dI (i, j) +

x
|

y

,

|

i, j

, x, y

Rℓ .

∈

∈ I

−
Rℓ is then separable and complete. We denote by

I ×

Rℓ. From Lemma 4.5 [17],

I ×
on
is Polish. We recall that we say that a sequence (νn)n≥0 weakly converges to ν in
for any continuous and bounded function f from
if
→
A possible metric associated to the weak topology on
F (
Lemma 4.3 in [17]). We next deﬁne the subset Eℓ of

Rℓ) set of the set of ﬁnite measures
Rℓ) endowed with the topology of the weak convergence
Rℓ)
I ×
Rℓ to R.
Rℓ) is the Prokhorov metric (see

f dν as n

M
I ×

Rℓ) by

f dνn

M
F (

I ×

I ×

I ×

M

M

F (

F (

F (

→

∞

+

R

R

M

I ×

Eℓ =

(

Xi∈V

δ(i,x) ; V

⊆ I

, V ﬁnite , xi

∈

Rℓ and i ⊀ j for i, j

V

∈

)

.

(2.2)

By Proposition A.6, Ed is Polish as well.

Probabilistic setting We ﬁx a deterministic terminal time T > 0 and a ﬁltered probability
space (Ω,
t)t∈[0,T ], P) satisfying the usual conditions. Suppose that this probability space
is endowed with a family of processes (Bi, Qi)i∈I such that

, ¯F = (

F

F

– (Bi

t)t∈[0,T ] is an F-standard Brownian motion in Rm for all i

;

∈ I

– Qi(dt, dk) is an F-Poisson random measure on [0, T ]

N with intensity measure dt γpkδk for

, with γ > 0, pk

0 for k

0 and

≥

≥

×

k≥0 pk = 1, δk being the Dirac measure at k;

P
forms a family of mutually independent processes.

∈ I}

all i

∈ I
Bi, Qj , i, j
{

–

Having in mind these processes, we precise a better probability space.

– Let Ω0 be the space of continuous functions from [0, T ] that are Rm-valued starting at 0. Let
Ω0.

t )t∈[0,T ] the ﬁltration generated by the canonical process B(ω0) := ω0, ω0
0

∈

T ) with the Wiener measure P0.
0

F0 := (
F
We endow (Ω0,

F

– Let Ω1 be the set of measures ω1 on R+ ×

t )t∈[0,T ] be the ﬁltration generated by the canonical process Q(ω1) = ω1:
1

N of the form ω1 =

k≥0 δ(tk ,nk). Let F1 :=

(
F

1
t := σ (Q([0, s]

k

) : s
}

F
We endow (Ω1,
probability measure such that Q is a Poisson point process with intensity dt γ

∈
1
T ) with the Poisson measure P1 of intensity dt γ

× {

F

∈

∈

[0, t], k

N) ,

t

[0, T ] .

P

P

k≥0 pkδk, that is the

k≥0 pkδk.

P

4

Ω1)I , P = (P0

Bi, Qj , i, j
Following the structure we expect for
{
∈ I}
where Ω = (Ω0
P1)⊗I,
is the P-augmentation of (
F
0
is the P-augmentation of the ﬁltration ((
t ⊗ F
F
of the processes Bi and Qi for i
the projections on each component, i.e.

, F, P),
F
0
t)t∈[0,T ]
T ⊗F
F
t )⊗I)t∈[0,T ]. On this space we extend the deﬁnition
1
as the previously described processes B and Q composed with

, we deﬁne the ﬁltered space (Ω,
1
T )⊗I and F = (

∈ I

×

⊗

F

Bi(ω) := ω0,i, Qj(ω) := ω1,j, ω = (ω0,i, ω1,i)i∈I ∈

Ω .

We also deﬁne the process ξ valued in

F (

M

Rm+1) by

N

×

I ×
1

22(|i|+n) δ(i,n,Bi

t,Qi([0,t]×{n}))

ξt =

Xi∈I,n∈N

(2.3)

[0, T ]. We then notice that the ﬁltration F is the completed ﬁltration generated by the

for t
process ξ.

∈

To stress the dependence in time, we will use the following notations. For t
.∧t) where

Ω, we deﬁne the stopped path at time t by ω.∧t = (ω0

.∧t, ω1

ω = (ω0, ω1)

∈

[0, T ] and

∈

.∧t = (ω0
ω0

s∧t)s≥0

and

.∧t = ω1(
ω1

· ∩

[0, t]

N) .

×

For a process (Xt)t∈[0,T ] and a random time τ : Ω
deﬁned by

→

[0, T ], we denote by (Xt∧τ )t∈[0,T ] the process

Xt∧τ (ω) = Xt(ω.∧τ (ω)) ,

[0, T ], ω

t

∈

Ω .

∈

Ω and a random time τ : Ω

[0, T ], we deﬁne the concatenation path ω

τ ˜ω =

⊕

→

1s<τ (ω) + (˜ω0,i

s −

τ (ω) + ω0,i
˜ω0,i

τ (ω))1s≥τ (ω) ,

[0, T ] ,

s

∈

For ω, ˜ω
(ω0,i

∈
τ ˜ω0,i, ω1,i

⊕

(ω0,i

and

⊕

τ ˜ω1,i)i∈I by

⊕
τ ˜ω0,i)s = ω0,i
s

ω1,i

⊕τ (ω) ˜ω1,i = ω1,i(

· ∩

[0, τ (ω)]

×

N) + ˜ω1,i(

· ∩

(τ (ω), T ]

N) .

×

. For a random variable S valued in some Polish space, we also deﬁne the shifted random

for i
variable Sτ,ω by

∈ I

Sτ,ω(˜ω) = S(ω

τ ˜ω) ,

˜ω

Ω .

∈

⊕

(2.4)

Alive particles We deﬁne the set

t of alive particles at time t as follows.

V

– At time t = 0, the set is reduced to the mother particle :

∅
{
t dies at the ﬁrst time τi > t the related Poisson measure

V0 =

.
}

– For a time t

0, a particle i

Qi jumps after t, i.e.

≥

∈ V

τi = inf

s > t : Qi((t, s]
{

N) = 1
}

.

×

– At time τi, this particle gives birth to k particles i0, . . . , i(k

k
{

) = 1:
}

1), with k such that Qi(
τi
{

−

} ×

τi = (

)
i
τi− \ {
}

V

V

i0, . . . , i(k

∪ {

.

1)
}

−

5

Controlled population Take A a Polish space with metric dA. We assume dA to be bounded
1 and still have a Polish space). We deﬁne a control α as a
(if not so, we replace dA by dA
family (αi)i∈I of F-progressively measurable processes valued in A. We denote by
the set of such
controls.

A

∧

Let λ : Rd

A

×

→
, each particle i

∈ A

α
oﬀ-springs according to the set
particle i
to k oﬀ-springs. The position at a time s

s. For i

∈ V

∈ I

∈ I

×

Rd and σ : Rd

A

Rd×m be measurable functions. For a given control
of the controlled population is born, evolves and dies to give birth to
s the position at time s of a
t be the random time of its death, giving birth

deﬁned above. We denote by X i

alive at time t, let τi

→

V

τi of the oﬀ-springs i0, . . . , i(k

≥

≥

X iℓ
dX iℓ

τi = X i
τi
s = λ(X iℓ

s , αiℓ

s )ds + σ(X iℓ

s , αiℓ

s )dBiℓ
s ,

1) are given by

−

(2.5)

(2.6)

for ℓ = 0, . . . , k
by the following measure valued process

−

1, such that iℓ is alive at time s. We represent the population of alive particles

Zs =

Xi∈Vs

δ(i,X i

s) ,

0 .

s

≥

The process Z takes values in the Polish space Ed deﬁned by (2.2).
i∈V δ(i,xi) ∈

R, and a measure µ =

For a function f :

I ×

Rd

→

Ed, we set

f (µ) =

ZI×Rd

P

f dµ =

fi(xi) .

Xi∈V

We introduce the second order local operators La, a

Laϕ(x) = λ(x, a)⊤Dϕ(x) +

1
2

Tr

A deﬁned by

∈
σσ⊤(x, a)D2ϕ(x)
(cid:17)
(cid:16)

,

x

Rd,

∈

for ϕ

C 2(Rd), where Dϕ and D2ϕ denote respectively, the gradient and the Hessian matrix of ϕ.
Rd)

and a function f :

C 1,2([0, T ]

For a control α

[0, T ]

Rd

∈

× I ×

)
R such that fi(
·

∈

→

∈ A

×

, the following SDE characterises the behaviour of Z:

for all i

∈ I

f (t, Zt) = f (s, Zs) +

t

Dfi(u, X i

u)⊤σ(X i

u, αi

u)dBi
u

s
Z
Xi∈Vu
(∂t + Lαi

u)fi(u, X i

u)du

t

s
Z

Xi∈Vu

+

+

k−1

Z(s,t]×N

Xi∈Vu− Xk≥0  

Xℓ=0

fiℓ

fi

−

!

(u, X i

u)Qi(dudk)

(2.7)

for all s, t

∈

[0, T ] such that s

t.

≤

Target branching diﬀusion To each alive particle i
time s denoted by Y i
R and σY : Rd
Let τi

s . Let λY : Rd
t be the random time of death of i

s, we associate a target position at
∈ V
R1×m be measurable functions.
A
×
, the target position at time s

τi is given by

→

→

×

×

A

R

∈ I

≥

≥

Y iℓ
τi = Y i
τi
s = λY (X iℓ,α
dY iℓ

s

, Y iℓ,α
s

, αiℓ

s ) ds + σY (X iℓ,α

s

, αiℓ

s ) dBiℓ
s ,

(2.8)

(2.9)

6

for ℓ = 0, . . . , k

1, such that particle iℓ is alive at time s.

−

to deﬁne the quantities associated to the pair

We use the notation ˆ
·
previous problem but on Rd+1. Therefore, we have ˆX i
ˆσ( ˆX i
s, αi
s) :=
according to

X i
s
Y i
s
. Under those hypotheses, assuming i is alive, its position ˆX i evolves
(cid:17)

, ˆλ( ˆX i
(cid:17)

, considering the

λ(X i
(cid:17)
λY (X i

s,αi
s)
s,Y i
s ,αs)

σ(X i
σY (X i

s,αi
s)
s,αs)

s, αi

s :=

and

(cid:16)

(cid:17)

(cid:16)

(cid:16)

X i
s
Y i
s
(cid:16)
s) :=

The resulting population process valued in Ed+1 is

d ˆX i

s = ˆλ( ˆX i

s, αi

s)ds + ˆσ( ˆX i

s, αi

s) dBi
s .

(2.10)

ˆZt =

Xi∈Vs

δ(i,X i

s,Y i

s ) ,

0 .

s

≥

As before, we deﬁne the related second order local operators ˆLa, a

ˆLa ˆϕ(ˆx) = ˆλ(ˆx, a)⊤D ˆϕ(ˆx) +

Tr(ˆσˆσ⊤(ˆx, a)D2 ˆϕ(ˆx)) ,

1
2

A by

∈

Rd+1,

ˆx

∈

C 2(Rd+1), where D ˆϕ and D2 ˆϕ denote respectively, the gradient and the Hessian matrix of

For a control α

and a function ˆf : [0, T ]
→
, the SDE related to ˆZ takes the following form:

Rd+1

×I ×

∈ A

R such that ˆfi(
)
·

∈

C 1,2([0, T ]

Rd+1)

×

for ˆϕ
ˆϕ.

∈

for all i

∈ I

ˆf (t, ˆZt) = ˆf (s, ˆZs) +

t

D ˆfi(u, ˆX i

u)⊤ ˆσ( ˆX i

u, αi

u)dBi
u

s
Z
Xi∈Vu
(∂t + ˆLαi

u) ˆfi(u, ˆX i

u)du

t

s
Z

Xi∈Vu

+

+

k−1

ˆfiℓ

ˆfi

−

!

(u, ˆX i

u)Qi(dudk)

(2.11)

for all s, t

∈

[0, T ] such that s

t.

≤

Z(s,t]×N

Xi∈Vu− Xk≥0  

Xℓ=0

Well posedness To ensure the well deﬁnition of the presented controlled processes, we make
the following assumption.

Assumption A1.

(i) The coeﬃcients pk, k

0, satisfy

≥

kpk = M < +

.

∞

Xk≥0

(ii) The functions λ, σ, λY and σY satisfy

sup
a∈A |

λ(0, a)
|

+

σ(0, a)
|
|

+

λY (0, 0, a)
|
|

+

σY (0, a)
|
|

< +

.

∞

(iii) There exists a constant L > 0 such that

+

λY (x, y, a)
|

−
Rd, y, y′

for all x, x′

∈

λ(x, a)
|

λ(x′, a)
|
+

−
λY (x′, y′, a′)
|

R and a

∈

∈

σ(x, a)
|

+
σY (x, a)
|
A.

−

σ(x′, a)
−
|
σY (x′, a)

| ≤

L

x
|

−

x′

|

+

y
|

−

y′

(cid:0)

|
(cid:1)

7

(iv) There exists a nondecreasing function w : R+ →
σ(x, a)
|

λ(x, a)
|

R+ such that w(r)

0 and

−−−→r→0

λ(x, a′)
|
+

−
λY (x, a′)
|

+
σY (x, a)
|

−

σ(x, a′)
−
|
σY (x, a′)

| ≤

w(dA(a, a′))

+

λY (x, y, a)
|
Rd, y

R and a, a′

−

∈

A.

∈

for all x

∈

For any initial condition t

[0, T ], µ =

∈

controlled branching processes (X, Y ). For that the set of alive particles

i∈V δ(i,xi) ∈

Ed and yi

∈

R for i

V , we extend the
t,µ is deﬁned as follows.

∈

P

V

[0, t],

t,µ
s = V .

V
t, a particle i

– For s

∈

– For s

≥
after s:

s dies at the ﬁrst time τi > s the related Poisson measure Qi jumps

∈ V

τi = inf

r > s : Qi(]s, r]
{

N) = 1
}

.

×

– At time τi, the particle i gives birth to k particles i0, . . . , i(k

1), with k such that Qi(
τi
{

−

} ×

k
{

) = 1:
}

t,µ
τi =

V

t,µ
i
τi− \ {
}
(cid:17)

V

(cid:16)

i0, . . . , i(k

∪ {

.

1)
}

−

Then, the controlled branching population process X t,µ,α = (X t,µ,α,i
the initial condition

s

, i

∈ V

t,µ
s )s∈[0,T ] is deﬁned by

X t,µ,α
s

= (xi, i

V ) ,

s

∈

together with dynamics (2.5)-(2.6). We also denote by ˆµ

∈
δ(i,xi,yi) ,

ˆµ =

Xi∈V

[0, t] ,

∈
Ed+1 the extended measure as

and Y t,ˆµ,α = (Y t,ˆµ,α,i

s

, i

∈ V

t,µ
s )s∈[0,T ] the controlled branching target process with initial condition

V , together with dynamics (2.8)-(2.9). Let Z t,µ,α and ˆZ t,ˆµ,α be

Y t,ˆµ,α,i
s

= yi ,

s

[0, t] ,

∈

for all i

∈

Z t,µ,α
s

=

δ(i,X t,µ,α,i

s

)

and

ˆZ t,ˆµ,α
s

=

δ(i,X t,µ,α,i

s

,Y t, ˆµ,α,i

s

)

Xi∈V t,µ

s

Xi∈V t,µ

s

[0, T ].

In this setting, we have the following non-explosion result.

for s

∈

Proposition 2.1. Suppose that Assumptions A1 (i)-(ii)-(iii) hold. Fix t
Ed, ˆµ =

.

i∈V δ(i,xi,yi) ∈

Ed+1 and α
t,µ
s

∈ A

(i) The set of alive particles
we have

P

V

is uniquely deﬁned and is ﬁnite for all s

[0, T ], µ =

∈

i∈V δ(i,xi) ∈

P
[0, T ]. More precisely,

∈

E

sup
s∈[0,T ] |V

"

t,µ
s

V
|# ≤ |

eγM (T −t)
|

|V|

stands for the cardinal of a subset

where
of
(ii) There exists a unique F-adapted process (Z t,µ,α) (resp. ( ˆZ t,ˆµ,α)) valued in Ed (resp. Ed+1).
Moreover, the process Z t,µ,α (resp. ( ˆZ t,ˆµ,α)) satisﬁes (2.7) (resp. (2.11)).

V

I

.

8

We refer to [5, Proposition 2.1] for the proof of this proposition.

[t, T ] such
. However, we can extend their deﬁnition to the whole interval [t, T ]. Suppose ﬁrst

the processes X t,µ,α,i and Y t,ˆµ,α,i are deﬁned on times s

∈

Remark 2.1. For any i
t,µ
that i
s
that i has no ancestor in

∈ V

∈ I
t,µ
t

V

:

j (cid:14) i

for all

j

t,µ
t

.

∈ V

Then we deﬁne processes X t,µ,α,i and Y t,ˆµ,α,i as the unique solutions to

dX t,µ,α,i
s
dY t,µ,α,i
s

= λ(X t,µ,α,i
s
= λY (X t,µ,α,i

, αi

s)ds + σ(X t,µ,α,i
, αi
, Y t,ˆµ,α,i
s

, αi
s
s) ds + σY (X t,µ,α,i

s)dBi
s

s

s

, αi

s) dBi
s

for s
∈
exists j

t,µ
t

∈ V

[t, T ], with initial condition X t,µ,α,i

= 0 and Y t,µ,α,i

t

such that j

(cid:22)

i. Then there exists k

≥

= 0. On the complementary case, it

t
1 and ℓ1, . . . , ℓk such that

i = jℓ1 . . . ℓk .

We denote the associated branching times by (S0, . . . , Sk):

Sm = inf

s > Sm−1 : Qjℓ1...ℓm ((Sm−1, s]

ℓm+1 + 1 for m = 0, . . . , k with S−1 = t. Then we deﬁne the extended processes X t,µ,α,i

n

nm

× {

) = 1
}

o

where nm
and Y t,µ,α,i by

≥

X t,µ,α,i
s

= 1

[t,S0)(s)X t,µ,α,j

s

+

k−1

m=1
X
k−1

1

[Sm−1,Sm)(s)X t,µ,α,jℓ1...ℓm

s

+ 1

[Sk−1,+∞)(s)X t,µ,α,i

s

Y t,ˆµ,α,i
s

= 1

[t,S0)(s)Y t,ˆµ,α,j

s

+

1

[Sm−1,Sm)(s)Y t,ˆµ,α,jℓ1...ℓm
s

+ 1

[Sk−1,+∞)(s)Y t,ˆµ,α,i

s

[t, T ].

m=1
X

for s

∈

These extended processes can be seen as solution to a Brownian stochastic diﬀerential equation
with Lipschitz coeﬃcients. Obvious in the ﬁrst case, to show it in the second one, we consider the
ancestor Brownian motion ¯Bi deﬁned by

¯Bi

s = Bj
s

1

[t,S0) +

1

[Sm−1,Sm)(s)

Bjℓ1...ℓm

s

k−1

m=1
X
+1[Sk−1,+∞)(s)

(cid:16)

Bi

Sk−1 + B

Bi

s −

(cid:16)

jℓ1...ℓk−1
Sk−1

,

(cid:17)

Bjℓ1...ℓm
Sm−1

+ Bjℓ1...ℓm−1
Sm−1

−

(cid:17)

∈

[t, T ]. This process is continuous, centered, with independent increments and variance equal
for s
to t, therefore a Brownian motion by L´evy’s characterisation. Then the extended processes X t,µ,α,i
and Y t,µ,α,i are the unique solutions to the SDE

dX t,µ,α,i
s
dY t,ˆµ,α,i
s

= ¯λ(s, X t,µ,α,i
= ¯λY (s, X t,µ,α,i

s

s

)ds + ¯σ(s, X t,µ,α,i

s

)d ¯Bi
s
) ds + ¯σY (s, X t,µ,α,i

s

, Y t,ˆµ,α,i
s

9

) d ¯Bi
s

(2.12)

(2.13)

for s

∈

[t, T ], with initial condition X t,µ,α,i

t

= xi and Y t,µ,α,i

t

= yi. The coeﬃcients being given by

¯λ(s, x) = 1

[t,S0)λ(x, αj

s) +

k−1

m=1
X
k−1

1

[Sm−1,Sm)(s)λ(x, αjℓ1...ℓm

s

) + 1

[Sk−1,+∞)(s)λ(x, αi
s)

¯σ(s, x) = 1

[t,S0)σ(x, αj

s) +

1

[Sm−1,Sm)(s)σ(x, αjℓ1...ℓm

s

) + 1

[Sk−1,+∞)(s)σ(x, αi
s)

m=1
X

k−1

¯λY (s, x, y) = 1

[t,S0)λY (x, y, αj

s) +

1

[Sm−1,Sm)(s)λY (x, y, αjℓ1...ℓm

s

) + 1

[Sk−1,+∞)(s)λY (x, y, αi
s)

m=1
X

k−1

¯σY (s, x) = 1

[t,S0)σY (x, αj

s) +

1

[Sm−1,Sm)(s)σY (x, αjℓ1...ℓm

s

) + 1

[Sk−1,+∞)(s)σY (x, αi
s)

m=1
X

Rd

∈

[0, T ]

for (s, x, y)
R. Under Assumption A1, those coeﬃcients satisfy classical Lipschitz
and boundedness assumption to have uniqueness and stability of solutions. In the sequel, we shall
refer by X t,µ,α,i and Y t,ˆµ,α,i either to the processes themselves or to their extended deﬁnitions if
the processes are considered outside their living interval.

×

×

Under the additional regularity assumption on the coeﬃcients with respect to the control, we

have a stability result for the branching system.

Proposition 2.2. Suppose that Assumptions A1 holds and ﬁx t

[0, T ], µ =

∈
i∈Vn δ(i,xn

i ,yn
i )

n≥1

(cid:17)

i∈V δ(i,xi) ∈
and (αn)n≥1 be
P

i∈V δ(i,xi,yi) ∈
Ed, ˆµ =
sequences of R+, Ed+1 and

P

∈ A
such that

A

Ed+1, α

. Let (tn)n≥1,

and

(tn, ˆµn)

−−−−−→n→+∞

ˆµn =

(cid:16)

P

(t, ˆµ) ,

T

E

0

Z

dA

s, αn,i
αi
s

ds

0

−−−−−→n→+∞

(cid:0)

(cid:1)

1

i∈V tn,µn
s

−

X t,µ,α,i
s

1

i∈V t,µ
s

2 +
|

Y tn,ˆµn,αn,i
s
|

1

i∈V tn,µn
s

−

Y t,ˆµ,α,i
s

1

i∈V t,µ
s

2
|

0

−−−−−→n→+∞

(cid:17)i

. Then,

for all i

E

∈ I
X tn,µn,αn,i
|
h(cid:16)
for all s

s

∈

[t, T ], where µn =

i∈Vn δ(i,xn

i ) ∈

Ed for any n

1.

≥

Proof. We proceed in three steps.

P

Step 1. We ﬁrst prove that

1

V tn,µn

s

(i)

P−a.s.
−−−−−→n→+∞

1

(i)

V t,µ
s

. For that, we distinguish two cases.

for all i
Case 1. Suppose that 1
and

∈ I

(i) = 1. Then, there exist j

V t,µ
s

t,µ
t

∈ V

and ℓ1, . . . , ℓk such that i = jℓ1 . . . ℓk

t < S1 <

< Sk−1 ≤

· · ·

s < Sk

10

where S1, . . . , Sk are the successive branching times:

Sm = inf

r > Sm−1 : Qjℓ1...ℓm ((Sm−1, r]

with nm

≥

ℓm+1 + 1 for m = 1, . . . , k. Since ˆµn

n

ˆµ and j

→

t,µ
t

∈ V

We then get from (2.14) and (2.15) that

j

∈ V

tn,µn
tn

for all n

N.

≥

nm

× {

) = 1
}

o

(2.14)

, there exists N

1 such that

≥

(2.15)

for n large enough.
Case 2. Suppose that 1
V t,µ
s
Subcase 2.1. There exist j

i

∈ V

tn,µn
s

.

(i) = 0. We then have two subcases.

∈ V

t,µ
t

and ℓ1, . . . , ℓk such that i = jℓ1 . . . ℓk. We then have

s > Sk

or

s < Sk−1

(2.16)

where S1, . . . , Sk are deﬁned by (2.14). Since ˆµn
we get from (2.16) that 1
Subcase 2.1. j /
tn,µn
j /
tn

V tn,µn
s
for any j
i for n large enough. Therefore, we have 1

→
(i) = 0 large enough.

t,µ
t
∈ V
for any j

ˆµ, we have i

(cid:22)

i. Since the set of ancestor of i is ﬁnite and ˆµn

ˆµ, we get
(i) = 0 for n large enough.

→

V tn,µn

s

tn,µn
tn

∈ V

for n large enough and

∈ V

(cid:22)
Step 2. We prove that

E

s

X tn,µn,αn,i
|
h(cid:16)

X t,µ,α,i
s

2 +
|

Y tn,ˆµn,αn,i
s
|

−

Y t,ˆµ,α,i
s

2
|

−

0

−−−−−→n→+∞

(cid:17)i
, we have X tn,µn,αn,i
∈ I
→
. Using Assumption A1 (iii), we can apply Theorem 8.1 in [18] and we get the

and Y tn,ˆµn,αn,i
tn

. Since ˆµn

X t,µ,α,i
t

ˆµ as n

→

→

∞

→

+

tn

for s
∈
Y t,ˆµ,α,i
t
result.

[0, T ] and i
+
as n

→

∞

Step 3. We then write

E

s

X tn,µn,αn,i
|
h(cid:16)

1

i∈V tn,µn
s

−

X t,µ,α,i
s

1

1

2E

i∈V t,µ
s

Y tn,ˆµn,αn,i
2 +
s
|
|
X tn,µn,αn,i
|
h(cid:16)
+2E

X t,µ,α,i
s

2 +
|
Y t,ˆµ,α,i
s
|

−
2 +
|

2
|

X t,µ,α,i
s
|
(cid:20)(cid:16)

s

(cid:17) (cid:16)

i∈V tn,µn
s

Y t,ˆµ,α,i
s

1

i∈V t,µ
s

−
Y tn,ˆµn,αn,i
s
|

Y t,ˆµ,α,i
s

−

1

i∈V t,µ

s −

1

i∈V tn,µn
s

≤

2
|
2
|
2

(cid:17)i

(cid:17)i
.

(cid:21)

(cid:17)

Using the dominated convergence theorem we get from Step 1

X t,µ,α,i
s
|

2 +
|

Y t,ˆµ,α,i
s
|

2
|

E

(cid:20)(cid:16)

i∈V t,µ

s −

1

i∈V tn,µn
s

1

(cid:17) (cid:16)

2

(cid:21)

(cid:17)

0 .

−−−−−→n→+∞

This last convergence and Step 2 give the result.

F (I

Rm+1)) as the set of c`adl`ag functions from [0, T ] to

Focusing on conditional laws of the controlled processes, we have a representation result. Deﬁne
Rm+1). We endow
D([0, T ],
this set with the Skrorkhod metric related to the Prokhorov distance and the related Borel σ-
algebra. From Doob’s functional representation Theorem (see e.g. Lemma 1.13 in [16]) for any
control α, there exists a

Rm+1))-measurable function ˜α :

(D([0, T ],

([0, T ])

[0, T ]

F (I

M

M

F (I

×

×

B

⊗ B

×

×

M

11

Rm+1))
[0, T ],
. In the sequel, we identify the control α with its related function ˜α and we still

AI such that αi(ω) = ˜αi(s, ξ(ω.∧s)) = ˜αi(s, ξ(ω)) for any s

∈

F (I

M
Ω and i

D([0, T ],
ω
denote by
For α

∈

A
∈ A

→

×
∈ I
the set of those controls.
, an F-stopping time τ and ω

Ω, we deﬁne the control ατ (ω),ω by

∈

i

(s, ξ(˜ω)) = αi

ατ (ω),ω
(cid:16)

(cid:17)

s, ξτ (ω),ω(˜ω)
(cid:17)

(cid:16)

for i

, s

≥

∈ I

0 and ˜ω

∈

Ω, where ξω,τ (ω) is given by (2.4).

Theorem 2.1 (Conditioning property). Suppose that Assumption A1 holds and ﬁx t

[0, T ], ˆµ =

Ed+1 and α

. Then, for any bounded measurable function f : D([0, T ], Ed+1)

∈

→

i∈V δ(i,xi,yi) ∈

R and any F-stopping time τ , we have
P

∈ A

E

f

h

ˆX t,ˆµ,α
(cid:16)

τ
F
(cid:17) (cid:12)
(cid:12)
(cid:12)

i

where

(ω) = F

.∧τ (ω), ατ (ω),ω

τ (ω), ˆX t,ˆµ,α
(cid:16)

,

(cid:17)

P(dω)

a.s.

−

for all s

[0, T ], ˆx

∈

∈

F (s, ˆx, β) = E

f

h
D([0, T ], Ed+1) and β

(ˆxt1t<s + ˆX s,ˆxs,β
(cid:16)

t

1t≥s)t∈[0,T ]

(cid:17)i

.

∈ A

The proof of this result is postponed to Appendix A.3. It follows the same lines as the proof
of Theorem 2 in [6], and relies on a uniqueness property for the related branching martingale
controlled problem which is studied in Appendix A.2.

2.2 The stochastic target problem

To deﬁne the stochastic target problem, let g :
assumption.

Rd

→

I ×

R be a function satisfying the following

Assumption A2. The function gi is continuous on Rd for all i

Fix an initial time t

∈
position y for the target process and a control α

[0, T ] and an initial population µ =
such that

∈ A

P

.

∈ I
i∈V δ(i,xi). We look for an initial

and Y t,ˆµ,α and X t,µ,α satisﬁes the terminal constraints

Y t,ˆµ,α,i
t

= y ,

i

V,

∈

More precisely, we look for the reachability set

Y t,ˆµ,α,i
T

≥

gi(X t,µ,α,i
T

) ,

t,µ
T

.

i

∈ V

(t, µ) =

R

y

n

∈

R,

:

α

∃

∈ A

: Y t,ˆµ,α,i
T

≥

gi(X t,µ,α,i
T

) ,

with ˆµ =

i

t,µ
T

∈ V
δ(i,xi,y)

Xi∈V

.

o

∈

[0, T ] and µ =

Ed. Since the target processes Y i has an explicit impact only
for t
on its drift λY and not on its diﬀusion coeﬃcient σY , the reachability set satisﬁes the following
monotonicity property.

i∈V δ(i,xi) ∈

P

12

Proposition 2.3. Suppose that Assumptions A1 holds. For µ =
we have [y,

(t, µ).

[
⊆ R

∞

i∈V δ(i,xi) ∈

Ed and y

(t, µ)

∈ R

P

Proof. Fix a control α = (αi)i∈I and a starting point (t, µ). We take y
ˆµ (resp. ˆµ′) for
δY i for Y ′i

y and write
i∈V δ(i,xi,y′)), Y i (resp. Y ′i) for Y t,ˆµ,α,i (resp. Y t,ˆµ′,α,i) and

Y i. We then have

i∈V δ(i,xi,y) (resp.

(t, µ), y′

∈ R

≥

−

P

P
s = (y′

δY i

y) +

−

s

t

Z

χuδY i

udu

for s

≥

t, where χ is given by

¯λY

u, X i

u, Y i
u

χu :=

¯λY

u, X i

u, Y ′i
u

,

u

0 ,

≥

−
δYu
(cid:1)

(cid:0)
with ¯λY deﬁned in Remark 2.1. From the Lipschitz property of λY in Assumption A1, χ is bounded
and

(cid:0)

(cid:1)

δ ¯Y i

T = (y′

y) exp

−

T

χudu

t
(cid:18)Z

(cid:19)

0 , P

a.s.

−

≥

Since y

∈ Y

(t, µ), we get

Y t,µ,α,y′,i
T

Y t,µ,α,y,i
T

≥

≥

This is true for all i

t,µ
T , therefore y′

∈ V

(t, µ).

gi

X t,µ,α,i
T
(cid:16)

(cid:17)

, P

a.s.

−

From Proposition 2.3, the closure

R

terized by its lower bound. We then deﬁne the value function v as the inﬁmum of

∈ R
(t, µ) of the reachability set is a half line interval charac-

:
R

v(t, µ)

:= inf

= inf

(t, µ)

R
y

n

R :

∈

α

∃

∈ A

, Y t,ˆµ,α,i
t

= y

i

V,

∀

∈
X t,µ,α,i
T
(cid:16)

and Y t,ˆµ,α,i
T

gi

≥

i

∀

∈ V

t,µ
T a.s.

(2.17)

for all t
i∈V δ(i,xi) ∈
is to provide an analytical characterisation of the value function v.

[0, T ] and µ =

o
) = +
Ed, with the usual convention that inf(
∞
∅

∈

(cid:17)

. Our aim

P

Remark 2.2. The value function v or the reachability set
might not be well deﬁned in the case
where an extinction of the alive population of particle happens before T . In this case we take the
convention that the terminal condition is always satisﬁed if
. In the sequel, we keep this
∅
convention for other constraints on (X t,µ,α,i
and θ a stopping time.

t,µ
T =

) with

R

, Y t,ˆµ,α,i
θ

θ

V
t,µ
θ =

V

∅

We next provide a new formulation of the function v.

Proposition 2.4. Under Assumptions A1, the value function function v satisﬁes the following
identity

v(t, µ) = inf

y

R :

∈

α

∃

∈ A

ˆµ =

,

∃

δ(i,xi,yi) ∈

Ed+1 such that

for all t

∈

[0, T ] and µ =

n

P

y

i

∈

yi

≤
∀
i∈V δ(i,xi) ∈

V, and Y t,ˆµ,α,i

Xi∈V
T

Ed.

13

gi(X t,µ,α,i
T

)

i

∀

∈ V

t,µ
T a.s.

≥

(2.18)

o

Proof. Denote by ˜v(t, µ) the right hand side of (2.18). Since the set whose inﬁmum is v(t, µ) is
included in the one whose inﬁmum is ˜v(t, µ), we obviously have

Fix now y

∈

R for which there exist α

∈ A
yi

and

˜v(t, µ)

v(t, µ) .

≤
and ˆµ =

y ,

≤

Ed+1 such that

i∈V δ(i,xi,yi) ∈
V ,

P
i
∈

Y t,ˆµ,α,i
T

gi(X t,µ,α,i
T

) ,

i

t,µ
T

∈ V
Ed+1. By the comparison argument used in the proof of Proposition 2.3,

≥

Set ¯µ =
we have

P

i∈V δ(i,xi,yi) ∈

Y t,¯µ,α,i
T

Y t,ˆµ,α,i
T

≥

≥

gi(X t,µ,α,i
T

) ,

t,µ
T

i

∈ V

Therefore y

≥

v(t, µ) and ˜v(t, µ)

v(t, µ).

≥

2.3 An example of application from ﬁntech

Fintech is the contraction of the words ﬁnance and technology. It refers to recent technologies that
allows for the improvement and the automation of the delivery and use of ﬁnancial services. The
ﬁeld has emerged at the beginning of the 21-st century and covered technologies used by established
ﬁnancial institutions. Since that time, the ﬁeld has evolved to also include crypto-currencies which
are decentralised ﬁnancial assets. Those assets are based on the block-chain technology. The main
idea of that structure is to keep any new transaction registered in a chain by adding new blocks
and sharing the extension of the original chain over the network, so that every user keeps in mind
the transaction and can certify it. We refer to [20] for a description of how a block-chain base
crypto-currency works in the case of the Bitcoin.

Due to the structure of this kind of assets, a fork can appear in the chain (see [10]). In this
case, the original asset is transformed into several assets. A natural question that arises is how to
evaluate an option on crypto-currencies in this case. We present here the example of the super-
replication of options on asset that may fork and show that it is a particular case of the branching
stochastic target presented above.

We consider a ﬁnancial market on which is deﬁned a crypto-currency with price process
(St)t∈[0,T ]. We suppose that the process S is a branching diﬀusion and describe its dynamics.
[0, T ] as previously done in Section 2.1.
We ﬁrst deﬁne the set
of the
The initial condition for the process S is a constant (S0 > 0). Assume the version i
crypto-currency is alive at time t
t and gives birth to k
new versions i0, . . . , i(k
τi of the new the crypto-currencies are
given by

[0, T ], dies at some random time τi

t of alive particles at time t

1). The position at a time s

∈ I

≥

≥

−

∈

∈

V

τi = Si
Siℓ
τi
s = Siℓ
s

dSiℓ

bds + cdBiℓ
s
(cid:16)

(cid:17)

,

(2.19)

(2.20)

τi such that version iℓ is alive at time s. Here b and c are two positive

for ℓ = 0, . . . , k
constants.

−

1 and s

≥

In addition to that asset, we assume that there exists on the market a non-risky asset S0 with

deterministic interest rate r > 0 and with initial condition S0

0 = 1, that is St = ert for t

[0, T ].

∈

14

An investment strategy consists in a process π = (πi

t)t∈[0,T ],i∈I of F-progressive processes valued
t represents the proportion of the wealth invested in the version Si of the crypto-
in [0, 1], where πi
, we also denote by V V0,π the self
currency. We denote by
ﬁnancing wealth process related to the initial capital V0 and strategy π. According to (2.19)-(2.20)
it is given by

the set of such strategies. For π

∈ A

A

V V0,π,iℓ
τi
dV V0,π,iℓ
s

= V V0,π,i
τi
= V V0,π,iℓ
s

((b

r)πiℓ

s + r)ds + cπiℓ

s dBiℓ
s

−

(cid:16)

(2.21)

(2.22)

,

(cid:17)

for ℓ = 0, . . . , k

1 and s

τi such that version iℓ is alive at time s.

−

≥

We then consider a ﬁnancial derivative on the asset S that consists in a Put Option but with a
strike Ki depending on the version of the crypto-currency S. Such a product can express the need
to hedge againts a decrease of the value of the asset S that depends on the branch.

The computation of the super-replication problem leads to solve the following stochastic target

problem

w0 = inf

ν

n

R+ :

∈

π

∃

∈ A

, V ν,π,i
T

(Ki

−

≥

Si
T )+ + κ

i

∀

∈ VT a.s.

,

o

where κ is a positive constant representing some friction. We next modify this problem to satisfy
our assumptions. For that, we ﬁrst deﬁne the processes

Y y,π,i
t

= log

X i

t = log

V ey,π,i
t
(cid:16)
Si
t

(cid:17)

for t

∈

[0, T ] and i

∈ V

t. From (2.19)-(2.20) and (2.21)-(2.21), we get

(cid:0)

(cid:1)

X iℓ

τi = X i

τi ,

dX iℓ

s = (b

Y y,π,iℓ
τi

= Y y,π,i
τi

,

dY y,π,iℓ
s

=

c2
2

(b

−

(cid:18)

)ds + cdBiℓ
s ,

(2.23)

r)πiℓ

s −

−

1
2

c2(πiℓ

s )2 + r

ds + cπiℓ

s dBiℓ
s ,

(2.24)

(cid:19)

for ℓ = 0, . . . , k
of the processes Y and X satisfy Assumption A1. We also deﬁne the functions g as

τi such that version iℓ is alive at time s. We observe that the dynamics

1 and s

≥

−

which satisﬁes Assumption A2. Finally, we deﬁne the optimal value

gi (x) = log

(Ki

−

,

(x, i)

R

∈

× I

,

ex)+ + κ
(cid:1)

(cid:0)

v0 = inf

∈ VT a.s.
o
a special case of (2.17). We notice that the optimal value w0 is related to v0 by

∈ A

R :

T )

≥

∈

n

π

∃

∀

y

i

,

, Y y,π,i
T

gi(X i

w0 = exp(v0) .

We suppose that ¯K := supi∈I Ki < +
Indeed, by taking the initial condition t
πi
and t
t = 0 for i

∞
∈

[0, T ], we get from (2.24)

∈ I

∈

. The value function v related to v0 is then bounded.
t) + log( ¯K + κ) and the control
[0, T ] and y =

r(T

−

−

Y t,ˆµ,π,i
T

≥

gi(X t,µ,i
T

) ,

t,µ
T

i

∈ V

15

for µ =

i∈V δ(i,xi) ∈

Ed and µ =

i∈V δ(i,xi,y) ∈

Ed+1. Therefore

P

v(t, µ)

P
r(T

−

≤ −

t) + log( ¯K + κ) ,

(t, µ)

[0, T ]

Ed .

×

∈

Moreover, for any y

(t, µ) and π the related admissible control, we have

∈ R

(b

r)πiℓ

s −

−

1
2

(cid:18)

c2(πiℓ

s )2 + r

b

−
c

2

r

(cid:19)

+ r

≤

(cid:19)

(cid:18)

Therefore we get

y +

b

−
c

2

r

(cid:19)

 (cid:18)

+ r

(T

!

t)

−

≥

E

Y t,ˆµ,π,i
T

h

≥

i

E

gi(X t,µ,π,i
T
h

)

i

log(κ) .

≥

Therefore,

v(t, µ)

b

−
c

2

r

(cid:19)

+ r

(T

!

−

≥ −  (cid:18)

t) + log( ¯K + κ) ,

(t, µ)

[0, T ]

Ed .

×

∈

In particular, v satisﬁes the growth condition (4.66) of the comparison Theorem 4.5. If we suppose
also that r = 0 and gi = 0 for i
I for some ℓ where I is a
of the form i = i1 · · ·
given bound, then v also satisﬁes condition (4.65) of Theorem 4.5.

in with iℓ

∈ I

≥

3 Dynamic programming

3.1 Measurable selection

In establishing a dynamic programming principle, we need an admissible control as concatenation
of admissible controls depending on the position of the branching processes at an intermediary
time. For this end, we use a measurable selection approach.

Let

U

be the target set deﬁned by

for (t, ˆµ)
and

∈

[0, T ]

U

×

(t, ˆµ) =

α

: Y t,ˆµ,α,i
T

≥

∈ A

gi(X t,µ,α,i
T

)

i

t,µ,α
T

a.s.

,

n

Ed+1 with ˆµ =

i∈V δ(i,xi,yi) and µ =

∀
∈ V
i∈V δ(i,xi) ∈

o

Ed. Let S := [0, T ]

Ed+1

×

P

D :=

P

(t, ˆµ)

{

∈

S :

U

(t, ˆµ)

=

.

∅}

Our aim is to exhibit a function that associates to each (t, ˆµ)
measurable way.

∈

D a control α

(t, ˆµ) in a

∈ U

We denote by
with the Borel σ-algebra

P

A

) related to the distance

(
A

B

(S) the set of probability measures on (S,

([0, T ])

B

⊗ B

(Ed+1)) and we endow

(α, α′)

1
2|i| ∧

E

T

0
Z

αi
s −
|

α′i
s |

ds

7→

Xi∈I

where
= i1 +
i
|
|
selection result.

· · ·

+ in for i = (i1, . . . , in)

Nn and n

∈

≥

1. We then have the following measurable

16

6
Lemma 3.1. Suppose that Assumptions A1 and A2 hold. For each ν
measurable function φν : (D,

)) such that

(D))

,

B

(
A

(
A

B

→

(S), there exists a

∈ P

φν(t, ˆµ)

∈ U

(t, ˆµ) for ν-a.e. (t, ˆµ)

D .

∈

Proof. S being endowed with the product σ-algebra
of Borel spaces. Also

endowed with

B

⊗ B

) is a Borel space. Let C be the following set

([0, T ])

(Ed+1) is a Borel space as product

A

(
A

B

C :=

(t, ˆµ)

{

S

∈

× A

: α

(t, ˆµ)
}

.

∈ U

From Proposition 2.2 and Assumption A2, C is closed and a fortiori a Borel subset of S

.

× A

Step 1: Measurable selector.

•

•

Since C is a Borel set, it is analytic by [1, Proposition 7.36]. From the Jankov-von Neumann
measurable selection theorem (see e.g. [1, Proposition 7.49]), there exists an analytically measur-
able function φ : D

such that

→ A

Step 2: Construction of a Borel measurable φν such that φν = φ ν-almost everywhere.

(t, ˆµ, φ(t, ˆµ))
{

: (t, ˆµ)

S

∈

} ⊂

C .

∈ P

Fix ν

(S) and denote by

ν(S) the completion of the Borel σ-algebra

(S) under ν. From [1,
Corollary 7.42.1] any analytic set is universally measurable. Therefore φ is universally measurable,
and, from the deﬁnition of the universal σ-algebra, φ is
ν(S) is the
completion of
(S) under ν, there exists a Borel measurable map φν such that φν(t, ˆµ) = φ(t, ˆµ)
for ν-almost every (t, ˆµ)

ν(S)-measurable. Since

S.

B

B

B

B

B

∈

3.2 Dynamic programming principle

For t
programming principle may be stated as follows.

[0, T ], we denote by

T[t,T ] the set of F stopping times valued in [t, T ]. The dynamic

∈

Theorem 3.2. Under Assumptions A1 and A2, the value function satisﬁes

v(t, µ) = inf

y

(cid:26)

∈

yi

R :

α

∃

∈ A

ˆµ =

,

∃

Xi∈V

δ(i,xi,yi) ∈

Ed+1 such that

y

i

∀

∈

≤

V, and Y t,ˆµ,α,i

θ

≥

v

θ, δ(i,X t,µ,α,i

θ

)

(cid:16)

(cid:17)

i

∀

∈ V

t,µ
θ

a.s.

(3.25)

(cid:27)

for any (t, µ)

[0, T ]

Ed and θ

×
Proof. We ﬁrst deﬁne the reachability sets by

∈

∈ T[t,T ].

Y

(t, µ)

:=

(yi)i∈V

(cid:26)

RV

:

∈

U

(t, ˆµ)

=

∅

with ˆµ =

δ(i,xi,yi)

.

Xi∈V

(cid:27)

and

Y

θ(t, µ) =

(yi)i∈V

(cid:26)

RV :

∈

α

∃

∈ A

such that

Y t,ˆµ,y,α,i
θ

≥

v

θ, δ(i,X t,µ,α,i
(cid:16)

θ

)

(cid:17)

i

∀

∈ V

t,µ
θ

a.s. with ˆµ =

δ(i,xi,yi) ∈

Ed+1

.

(cid:27)

i∈V
X

17

6
∈

[0, T ] and µ =

for t
Denote by vθ(t, µ) the right hand side of (3.25).

i∈V δ(i,xi) ∈
P
vθ(t, µ), we show that

To prove v(t, µ)

∈ T[t,T ]. Fix now t
(t, µ)

∈
θ(t, µ). Let (yi)i∈V

Ed and θ

[0, T ] and µ =

Ed.

i∈V δ(i,xi) ∈
(t, µ). By deﬁnition

P

Y

⊂ Y

∈ Y

there exists α

∈ A

≥
such that

Y t,ˆµ,α,j
T

gj

≥

X t,µ,α,j
T

j

∀

∈ V

t,µ
T

.

(cid:16)

(cid:17)

From the uniqueness of solutions to (2.5)-(2.6) and (2.8)-(2.9) (or equivalently (2.12)-(2.12)) we
get the following ﬂow property

X t,µ,α,j
T

Y t,ˆµ,α,j
T

θ,δ(i,X
= X
T
θ,δ(i,X
T

= Y

t,µ,α,i
θ

),α,j

,

t,µ,α,i
θ

,Y

t, ˆµ,y,α,i
θ

),α,j

,

for all i

t,µ
and j
θ
We therefore get

∈ V

t,µ
T

∈ V

such that i

j.

(cid:22)

θ,δ(i,X
T

Y

t,µ,α,i
θ

,Y

t, ˆµ,α,i
θ

),α,j

gj

≥

θ,δ(i,X
X
T

t,µ,α,i
θ

),α,j

,

!

θ,δ(i,X
T

t,µ,α,i
θ

)

,

j

∈ V

. Given the deﬁnition of the value function v, we get Y t,ˆµ,α,i

θ

for all i

all i

t,µ
θ
a.s. and (yi)i∈V

∈ V
t,µ
θ

θ(t, µ).

∈ V
We now turn to the reverse inequality vθ(t, µ)

∈ Y

v(t, µ). To this end, we prove that

≥

Y

v

≥

θ, δ(i,X t,µ,α,i
(cid:16)

θ

for

)

(cid:17)
θ
ε (t, µ)

⊂

(t, µ) for any ε > 0, where

Y

θ
ε (t, µ) =

Y

(yi + ε)i∈V : (yi)i∈V

n

θ(t, µ)

.

∈ Y

o

∈ Y

θ(t, µ) and α

such that Y t,ˆµ,α,i

θ
i∈V δ(i,xi,yi). Fix now ε > 0 and set ˆµ =

t,µ
a.s. where
Let (yi)i∈V
θ
i∈V δ(i,xi,yi+ε). From the
ˆµ =
deﬁnition of the value function and the strict monotonicity of the ﬂow w.r.t. the initial value, we
P
P
get Y t,ˆµ,α,i
(ω) < Y t,ˆµε,α,i
Ω. Consider the
(ω) for all i
θ
θ
probability measure ν induced on S by

)
i∈V δ(i,xi,yi) and ˆµε =
(cid:17)

θ, δ(i,X t,µ,α,i
(cid:16)

P
for P-a.e. ω

θ, δ(i,X t,µ,α,i
(cid:16)

for all i

∈ A

∈ Y

∈ V

t,µ
θ

∈ V

(ω)

≥

∈

(cid:17)

v

)

θ

θ

and φν the measurable map deﬁned in Lemma 3.1. We have

ω

7→

(cid:16)

θ, ˆZ t,ˆµε,α
θ

(ω) ,

(cid:17)

˜t,˜µ,˜y,φν (˜t,˜µ,˜y),i
T

Y

≥

gi

˜t,˜µ,φν (˜t,˜µ,˜y),i
X
T

We deﬁne ˆΞ :=

θ, ˆZ t,ˆµε,α
θ
the initial conditions at time θ for Y iT
(cid:16)

(cid:17)

and Ξ :=

(cid:16)

(cid:17)
θ, Z t,µ,α
θ
T and X iT
(cid:16)

i

∀

∈ V

T P-a.s. for ν-a.e. (˜t, ˜µ, ˜y)

D .

(3.26)

∈

. For iT

t,µ
T , if iθ

t,µ
θ

such that iθ

iT ,

(cid:22)

∈ V

∈ V

T are respectively Ξ and ˆΞ.

(cid:17)

that there exists negligible set N2,ω1 of

Binding ﬂow properties with the measurable selector, we can ﬁnd a negligible set N1 of
for each ω1 ∈
X Ξ(ω1),φν (ˆΞ(ω1)),i
(cid:16)

ˆΞ(ω1),φν (ˆΞ(ω1)),i
T

1 such that

Ξ(ω1)
T

(ω2)

(ω2)

(ω2)

∈ V

N c

giT

≥

F

(cid:17)

Y

∀

i

T

for all ω1 ∈

N c
1 and ω2 ∈

N c

2,ω1.

such

F

18

 
We now deﬁne the set ¯N :=
N c

¯N2 where

ﬁrst have ¯N

⊂

1 ∩

ω : ω
{

∈

N c

1 , ω

N2,ω

}

∈

and we prove that ¯N is negligible. We

¯N2 =

ω

Ω :

i

∈ V
n
The set ¯N2 can be rewritten as

∈

∃

Ξ(ω)
T

(ω) ,

ˆΞ(ω),φν (ˆΞ(ω)),i
T

Y

(ω) < gi

X Ξ(ω),φν (ˆΞ(i)(ω)),i
(cid:16)

T

(ω)

.

(cid:17)o

¯N2 =

Ω :

∈

ω




1

Yi∈V Ξ

T

Y

ˆΞ,φν ( ˆΞ),i
T

≥gi(cid:16)X Ξ,φν ( ˆΞ),i

T

(cid:17)

.

(ω) = 0




Taking the conditional expectation w.r.t.




θ, we have up to a negligible set

F

¯N2 =

Ω : E

∈

ω




Using Theorem 2.1 wet get



ˆΞ,φν ( ˆΞ),i
T

Y

≥gi(cid:16)X Ξ,φν ( ˆΞ),i

T

1





Yi∈V Ξ

T

θ



F
(cid:17)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(ω) = 0




.



∈

ZΩ

Ω :

1

ω

¯N2 = 

ω

{

=

Yi∈V Ξ(ω)
Ω : P(N2,ω) = 0
}
Therefore we get, up to a negligible set, ¯N2 ⊂

(ω⊕θω′)
.

∈

T

ˆΞ(ω),φν ( ˆΞ(ω)),i
T

Y

We now ﬁx α

A and deﬁne the control ¯α = (¯αi)i∈I by

N1 and P( ¯N2) = 0

(ω⊕θω′)≥gi(cid:16)X Ξ(ω),φν ( ˆΞ(ω)),i

T

(ω⊕θω′)(cid:17)

dP(ω′) = 0




∈

¯αi(ω) :=

αi(ω)1[0,θ(ω)) + φi
a

(

ν (ˆΞ(ω))(ω)1[θ(ω),T ]

¯N

Ω
¯N

\

if ω
if ω

∈
∈

for all i
and ¯N2 is negligible, we get (yi + ε)i∈V

A. Since

wirth a

∈ I

∈

(cid:16)

Y

ˆΞ,φν (ˆΞ)
T

, X Ξ,φν (ˆΞ),i
T
(t, µ).

∈ Y

=

Y t,ˆµε, ¯α,i
T

, X t,µ, ¯α,i
T

for each i

(cid:17)

(cid:16)

(cid:17)

t,µ
T

a.s.

∈ V

4 PDE characterisation

4.1 Branching property

Conditionally to their birth, the alive particles , and consequently their branches, are independent
in the uncontrolled case. In out case, this branching property is passed down to the value function
in the following way.

Proposition 4.5 (Branching property). Let Assumption A1 holds. The value function v satisﬁes

v(t, µ) = max
i∈V

v(t, δ(i,xi))

(4.27)

for any (t, µ =

i∈V δ(i,xi))

[0, T ]

Ed.

×

∈

P

19

Proof. For µ =

i∈V δ(i,xi) ∈

Ed, we deﬁne

K µ :=

y
(

∈

P
R :

α

∃

∈ A

, Y t,ˆµ,α,i
T

≥

gi(X t,µ,α,i
T

)

i

∀

∈ V

t,µ,α
T

a.s. with , ˆµ =

δ(i,xi,y)

.

)

Xi∈V
j∈V K δ(j,xj ), i.e. K µ

Proving v(t, µ)

for each j
∈
zooming in on the sub-population generated by each j
K δ(j,xj ).
K δ(j,xj ). Therefore, y

∈

maxi∈V v(t, δ(i,xi)) comes to verify that K µ

K δ(j,xj )
K µ, there exists α satisfying the constraints in T a.s. With this same α,
V , we must satisfy the condition of

T

⊆

⊆

≥
V . If y

∈

Let j be the index that realises the maximum in the righthand side of (4.27). The monotonicity
K δ(j,xj ), let
property given by Proposition 2.3 implies K δ(j,xj )
V that meets the demand of K δ(i,xi). To prove y
K µ we must exhibit
αi be a control for i
a control that satisﬁes the requirements of such a set. Having a control α taken as αi on the
V , we meet the conditions of K µ. Therefore, maxi∈V v(t, δ(i,xi)) =
branches generated by each i
v(t, δ(j,xj ))

K δ(i,xi) for all i

V . Then, if y

v(t, µ)

⊆

∈

∈

∈

∈

∈

∈

≤

From this result, we can focus on the function ¯v deﬁned on

[0, T ]

×

I ×

Rd by

¯vi(t, x) = v(t, δ(i,x))

for (i, t, x)
¯v.

∈ I ×

[0, T ]

×

Rd. We provide in the next sections a PDE characterisation of the function

4.2 Dynamic programming equation

4.2.1 The equation on the parabolic interior

In a stochastic target problem, wishing to hit a given target with probability one, we must de-
generate along certain directions. Moreover, we also need to control the uncertainty related the
possible branching. This property enables the characterisation of the value function ¯v as a solution
the following PDE

∂t¯vi(t, x) + F

x, ¯vi(t, x), D¯vi(t, x), D2

x ¯vi(t, x)

; ¯vi(t, x)

sup
0≤k< ¯K

−

¯vik(t, x)

)

= 0 (4.28)

min

(−

(cid:0)
Rd, where

for (t, x)

∈

[0, T )

×
¯K = sup

F(Θ) = sup

for Θ = (x, y, p, M )

Rd

∈

(cid:26)

(x, p) =

N
Rd.

a
{

∈

for x, p

∈

(cid:1)

,

N : pk > 0
}

k + 1

{

∈
λY (x, y, a)

λ(x, a)⊤p

−

R

Rd

Sd, and

×

×

×
A : N a(x, p) = 0
}

1
2

Tr

−

σσ⊤(x, a)M
(cid:16)

(cid:17)

: a

∈ N

(x, p)

(cid:27)

and N a(x, p) = σY (x, a)

σ(x, a)⊤p

−

Since the control set A is not necessarily compact, the operator associated to this PDE may not
be continuous. We therefore need to deﬁne a weak formulation of (4.28). For that, we introduce
the relaxed semilimits of F given by

F ∗(Θ) = lim sup
ε→0,Θ′→Θ

Fε(Θ′) and F∗(Θ) = lim inf

ε→0,Θ′→Θ

Fε(Θ′)

20

where

Fε(Θ) = sup

λY (x, y, a)

λ(x, a)⊤p

−

1
2

Tr

−

σσ⊤(x, a)M
(cid:16)

(cid:17)

: a

∈ N

ε(x, p)

(cid:27)

(cid:26)
Rd

∈
a
{

R

×
A :

×

×
N a(x, p)

for Θ = (x, y, p, M )

Rd

Sd and ε

0, and

ε(x, p) =

N
Rd. Observe that (

∈

ε
|
}
ε)ε≥0 is non-decreasing so that

| ≤

N

for x, p

∈

≥
and N a(x, p) = σY (x, a)

σ(x, a)⊤p

−

F∗(Θ) = lim inf
Θ′→Θ

F0(Θ′)

(4.29)

Since some

ε(x, p) may be empty, we shall use the standard convention sup

−∞
this paper. For ease of notations, we also write F ϕ(t, x) in place of F (x, ϕ(t, x), Dϕ(t, x), D2
for a regular function ϕ. We similarly use the notations F ∗ϕ and F∗ϕ.

N

=

∅

all over
x ϕ(t, x))

As the value function may not be regular, we use the framework of discontinuous viscosity
solutions. To this end, we deﬁne the lower- and upper-semicontinuous envelopes f∗ and f ∗ of a
R by
locally bounded function f : [0, T ]

Rd

f ∗
i (t, x) = lim sup

′

(t

′
) → (t, x)
′

< T

, x
t

and

fi,∗(t, x) =

′

(t

lim inf
, x
t

′
) → (t, x)
′

< T

fi(t′, x′)

(4.30)

× I →

×
fi(t′, x′)

for (t, x, i)
(4.28).

∈

[0, T ]

Rd

×

× I

. We are now able to provide the deﬁnition of a viscosity solution to

Deﬁnition 4.1. Let u :
×
(i) u is a viscosity supersolution to (4.28) if for any (t0, x0, i0)
C 1,2([0, T ]

Rd) such that

Rd) for i

C 0([0, T ]

× I →

and ¯ϕ

[0, T ]

Rd

R be a locally bounded function.

[0, T )

Rd

×

× I

∈

and any ϕi

∈

×

∈ I

×

ϕi(t, x)

∈
sup
i∈I |
| ≤
ϕi0) (t0, x0) =

0 = (ui0,∗ −

¯ϕ(t, x) ,

min
I×[0,T ]×Rd

(t, x)

∀
(u·,∗ −

[0, T ]

∈
ϕ·) .

Rd ,

×

we have

min

(−

∂tϕi0 (t0, x0) + F ∗ϕi0(t0, x0) ;

ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(t0, x0)

) ≥

0 .

(ii) u is a viscosity subsolution to (4.28) if for any (t0, x0, i0)
C 1,2([0, T ]

Rd) such that

Rd) for i

C 0([0, T ]

and ¯ϕ

×

∈ I

[0, T )

Rd

×

× I

∈

and any ϕi

∈

0 =

(cid:0)

we have

u∗
i0 −

∈
ϕi(t, x)

sup
i∈I |
ϕi0)(t0, x0

×

| ≤

¯ϕ(t, x) ,

(t, x)

[0, T ]

Rd ,

×

∀
(u∗

· −

∈
ϕ·) .

=

max
I×[0,T ]×Rd

(cid:1)

min

(−

∂tϕi0(t0, x0) + F∗ϕi0(t0, x0) ;

ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(t0, x0)

) ≤

0 .

(iii) u is a viscosity solution to (4.28) if it is both a viscosity sub and supersolution to (4.28).

21

 
 
We notice that the deﬁnition of viscosity solution is slightly diﬀerent from the classical one as

we impose a bound in the label particle i for test functions.

Following [4], we introduce the a continuity assumption on the kernel that is used to prove the

subsolution property.

∈

Assumption A3. Let B be a subset of Rd
(x0, p0)
Lipschitz map ˆa deﬁned on B′ such that

Rd such that
on B. Then, for every ε > 0,
int(B), and a0 ∈ N0(x0, p0), there exists an open neighborhood B′ of (x0, p0) and a locally
a0| ≤
∈ N0(x, p) for all (x, p)

ˆa(x0, p0)
|

ˆa(x, p)

ε and

N0 6

B′ .

−

×

=

∈

∅

We are now able to state our result.

Theorem 4.3. Suppose that ¯v is locally bounded on [0, T ]

Rd

.

× I

×

(i) Under Assumptions A1, the value function ¯v is a viscosity supersolution to (4.28)

(ii) If in addition Assumption A3 holds, ¯v is a viscosity subsolution to (4.28)

4.2.2 Terminal condition

To get a complete characterisation of the function ¯v, we need to add a terminal equation to (4.28).
By the deﬁnition of the stochastic target problem, we have

¯vi(T, x) = gi(x)

(4.31)

. The possible discontinuities of ¯v might imply that ¯v∗ and ¯v∗ do not
for every (x, i)
agree with the boundary condition (4.31). To get the proper terminal condition, we introduce the
set-valued map

× I

∈

Rd

N(x, p) =

r
{

∈

Rm : r = N a(x, p) for some a

A
}

∈

together with the signed distance function from its complement set Nc to the origin

where dist stands for the Euclidean distance. Then,

δ = dist(0, Nc)

dist(0, N) ,

−

intN(x, p)

0

∈

⇔

δ(x, p) > 0 .

(4.32)

For simplicity of notations, we will write δϕ(x) for δ(x, Dϕ(x)) for a regular function ϕ. Then, the
terminal condition takes the following form

min

¯vi(T, x)

−

gi(x) ; δ¯vi(T, x) ;

¯vi

sup
0≤k< ¯K

−

¯vik

!

(T, x)

= 0

(4.33)

)

(

.

We give the deﬁnition of a viscosity solution to (4.33). We recall that the deﬁnitions of the

for (x, i)

Rd

∈

× I

envelopes u∗ and u∗ of a locally bounded function u are given by (4.30).

22

 
Deﬁnition 4.2. Let u : [0, T ]
×
(i) u is a viscosity supersolution to (4.33) if for any (x0, i0)
C 0(Rd) such that
i

× I →

and ¯ϕ

Rd

R be a locally bounded function.

Rd

∈

and any ϕi

∈

× I

C 2(Rd) for

∈ I

∈

0 = u∗

i0(T, x0)

−

we have

ϕi(x)

sup
i∈I |

¯ϕ(x) ,

x

| ≤
ϕi0(x0) = min
I×Rd

∀
· (T,

(u∗

∈
)
·

−

Rd ,

ϕ·)

min

ϕi0 (x)

−

(

gi0(x0) ; δ∗ϕi0(x0) ; ϕi0(T, x0)

sup
0≤k< ¯K

−

ϕi0k(T, x0)

) ≥

0 .

(ii) u is a viscosity subrsolution solution to (4.33) if for any (x0, i0)
for i

C 0(Rd) such that

and ¯ϕ

∈ I

∈

Rd

∈

× I

and any ϕi

C 2(Rd)

∈

0 = ui0,∗(T, x0)

−

we have

¯ϕ(x) ,

ϕi(x)

sup
i∈I |

| ≤
ϕi0(x0) = max
I×Rd

x

∀
(u·,∗(T,

Rd ,

ϕ·)

−

∈
)
·

min

(ϕi0(x)

(

−

gi(x)) 1F ∗ϕi0 (x)<∞; δ∗ϕi0(x) ; ϕi0(T, x)

sup
0≤k< ¯K

−

ϕi0k(T, x)

) ≤

0 .

(iii) u is a viscosity solution to (4.33) if it is both a viscosity sub and supersolution to (4.33).

The terminal viscosity property is stated as follows.

Theorem 4.4. Suppose that ¯v is locally bounded on [0, T ]

Rd

.

× I

×

(i) Under Assumptions A1 and A2, ¯v is a viscosity supersolution to (4.33).

(ii) If in addition Assumption A3 holds, ¯v is a viscosity subsolution to (4.33).

4.3 Viscosity properties on [0, T )

4.3.1 Viscosity supersolution property

Rd

×

× I

Fix (i0, t0, x0)
such that

∈ I ×

[0, T )

×

Rd and let ϕ

C 0([0, T ]

Rd) and ϕi

C 1,2([0, T ]

Rd) for i

×

be

∈ I

∈

×

∈

and

sup
i

ϕi
|

| ≤

ϕ

0 = (¯vi0,∗ −

ϕi0) (t0, x0) =

min
(i,t,x)∈I×[0,T ]×Rd

(¯vi,∗ −

ϕi) (t, x) .

Without loss of generality we can assume this minimum to be strict in (t, x) once ﬁxed i0.

(4.34)

(4.35)

23

Step 1. We ﬁrst prove that ϕi0(t0, x0)
Let (tn, xn) be a sequence in [0, T ]

−

Rd such that

sup0≤ℓ≤k−1 ϕi0k(t0, x0)

×

0 for any k such that pk > 0.

≥

(tn, xn)

→

(t0, x0) and ¯vi0(tn, xn)

¯vi0,∗(t0, x0) as n

.

→ ∞

→

Set y0 := ϕi0(t0, x0), ˆx0 := (x0, y0), yn := ¯vi0(tn, xn) + 1/n and ˆxn := (xn, yn). Deﬁne the
stopping time θn = inf
and the random variable kn such that
1
s
}
{
Qi0((tn, θn]
) = 1. From Theorem 3.2, the continuity of the trajectories and since yn >
}
× {
¯vin(tn, xn) there exists αn

tn : Qi0((tn, s]

such that

N)

kn

×

≥

≥

tn,δ(i0,ˆxn),αn,i0
θn−

Y

θn, X

tn,δ(i0,xn),αn,i0
θn−

∈ A

max
0≤ℓ≤kn−1

¯vi0ℓ

≥

(cid:16)

on

θn
{

≤
tn,δ(i0,xn),yn,αn,i
t

Y

T

. To alleviate the notation, we shall denote X n,i
}

t

for n

1 and t

[tn, T ]. Therefore, we get

≥

∈

max
0≤ℓ≤kn−1

ϕi0ℓ

≥

(cid:17)

θn, X

tn,δ(i0,xn),αn,i0
θn

.

(cid:16)

tn,δ(i0,xn),αn,i
:= X
t

(cid:17)
and Y n,i

t

:=

+∞

T

γ

pk

Xk=0

0

Z

E

1s≤θn≤T 1

(cid:20)

Y

n,i0
s <max0≤ℓ≤k−1 ϕi0ℓ(cid:16)s,X

n,i0
s

(cid:17)(cid:21)

ds = 0 ,

which means

T

0
Z

E

1s≤θn≤T 1

(cid:20)

Y

n,i0
s <max0≤ℓ≤k−1 ϕi0ℓ(cid:16)s,X

n,i0
s

ds = 0

(cid:17)(cid:21)

for all k

≥

1 such that pk > 0. We therefore get

E

1s≤θn≤T 1
(cid:20)
[tn, T ]. Since the process Y n,i0

n,i0
s <max0≤ℓ≤k−1 ϕi0ℓ(cid:16)s,X

Y

= 0

n,i0
s

(cid:17)(cid:21)

(4.36)

for Lebesgue almost all s
and P (θn
∈
(4.36) gives

is continuous
[tn, T ]) > 0, Fatou’s Lemma applied to a sequence (sk)k converging to tn and satisfying

max0≤ℓ≤k−1 ϕi0ℓ

−

∈

, X n,i0
·
(cid:0)

(cid:1)

yn

max
0≤ℓ≤k−1

≥

ϕi0ℓ(tn, xn)

for all k

≥

1 such that pk > 0. Sending n to inﬁnity gives the result.

Step 2. We now prove that

−

(t0, x0) + F ∗ϕi0 (t0, x0)

∂ϕi0
∂t
∂tϕi0 + F ∗ϕi0)(t0, x0) =

0

≥

2η for some η > 0, and let us work

Assume to the contrary that (
−
towards a contradiction. By deﬁnition of F ∗ , we may ﬁnd ε

−

∂tϕi0(t, x) + λY (x, y, a)

−

and (t, x, y)

[0, T ]

Rd

Laϕi0(t, x)
R such that (t, x)

−

η

∈
for all a
Bε(t0, x0) and

≤ −

(0, T

t0), such that

−

ε(x, Dϕi0(t, x))
ε ,
ϕi0(t, x)

∈ N
y
|

(4.37)

∈

| ≤
where Bε(t0, x0) denotes the ball of radius ε around (t0, x0). Let ∂pBε(t0, x0) =
cl(Bε(t0, x0))
that

} ×
∂Bε(x0) denote the parabolic boundary of Bε(t0, x0) and observe

t0 + ε
{

[t0, t0 + ε)

−

×

×

×

∪

∈

ζ = min

∂pBε(t0,x0)

(¯vi0,∗ −

ϕi0) > 0

(4.38)

since (t0, x0) is a strict minimizer of ¯vi0,∗ −

ϕi0 on [0, T )

Rd.

×

24

Step 3. We now show that (4.37) and (4.38) lead to a contradiction to (3.25). Let (tn, xn) in
[0, T ]

Rd such that

×

(tn, xn)

→

(t0, x0) and ¯vi0(tn, xn)

¯vi0,∗(t0, x0) as n

.

→ ∞

→

We then set y0 := ϕi0(t0, x0), ˆx0 := (x0, y0), yn := ¯vi0(tn, xn) + 1/n, ˆxn := (xn, yn), βn :=
yn

ϕi0(tn, xn) and notice that

−

βn

→

0 as n

.

→ ∞

(4.39)

From the deﬁnition of the value function and the fact that yn > ¯vi0(tn, xn) for each n
exists some αn in
A
alleviate the notation, we shall denote

tn,δ(i0,xn),yn,αn,i
T

tn,δ(i0,xn),αn,i
X
T

such that Y

for all i

∈ V

gi

≥

≥
tn,δ(i0,xn),αn
T

1, there

. To

(cid:16)

(cid:17)

X n,i
t

tn,δ(i0,xn),αn,i
:= X
t

, Y n,i
t

:= Y

tn,δ(i0,xn),yn,αn,i
t

and

n
t

:=

V

V

tn,δ(i0,xn),αn
t

for n

≥

1 and t

∈

[tn, T ]. Deﬁne the following stopping times

τn
τ ε
n
τ r
n
θn

:= inf

:= inf

:= inf
:= τn

We also set

≥

≥

s
{
s
{
s
≥
{
τ ε
n ∧

∧

i

∃

∈ V

tn :

n
s ,
n
s ,
i
tn :
tn : Qi0((tn, s]
τ r
n .

∈ V

∃

s, X n,i
s
Y n,i
ϕi
(cid:0)
(cid:1)
s −
|
N) = 1

×

/
∈

(cid:0)
}

Bε(t0, x0)
}
s, X n,i
s
,

| ≥

(cid:1)

,

,

ε
}

,(4.40)

) >

η

−

o

(4.41)

(4.42)

[tn, θn) :

An =
s
∈
s = N αn
i0 (X n,i0
ψn
s

n

, Dϕi0 (s, X n,i0

s

)) .

∂tϕi0(s, X n,i0

s

) + λY (X n,i0

s

, Y n,i0
s

, αn
i0)

−

−

Lαn

i0 ϕi0(s, X n,i0

s

We notice that (4.37) implies

It follows from Theorem 3.2 that

Y n,i
t∧θn ≥

and since ¯vi

¯vi,∗ ≥

≥

ϕi

ψn
s |
|

> ε

for s

An .

∈

¯vi

t

(cid:16)

∧

θn, X n,i
t∧θn

(cid:17)

i

∀

∈ V

n
t∧θn, t

∈

[tn, T ] .

Y n,i
θn∧t ≥

ϕi

θn

t, X n,i

θn ∧

t

∧

i

∀

∈ V

n
θn .

(cid:16)

(cid:17)

Using the deﬁnition of ζ in (4.38) and θn, and the continuity of the trajectories, we get

Y n,i0
t∧θn ≥

≥

θn, X n,i0
t∧θn
θn, X n,i0
t∧θn

∧

∧

ϕi0

ϕi0

t

t

(cid:16)

(cid:16)

(cid:17)

+ ζ

(cid:17)

ε1

{θn≤t}∩{θn<τ r

n} .

∧

Therefore, from (4.42) and the previous inequality, we have

+ (ζ1

{θn=τn} + ε1

n=θn}∩{θn<τn})1
{τ ε

{θn≤t}∩{θn<τ r

n}

ε1

ζ

−

∧

{θn>t}∪{θn=τ r

n} ≤ −

ζ

∧

ε + Y n,i

t∧θn −

ϕi

t

(cid:16)

∧

θn, X n,i
t∧θn

(cid:17)

25

tn,δ(i0,ˆxn),αn
Applying the dynamics (2.11) of ˆZ
·
from the deﬁnition of ψn and θn, and (4.40) that

to the function (t, x, y, i)

y

−

7→

ϕi0(t, x), it follows

t∧θn

ε +

∧

tn

Z

ψn
s

⊤dBi0
u

βn

ζ

−
t∧θn

tn

Z

−

h

∂tϕi0

u, X n,i0
u

+ λY

X n,i0
u

, Y n,i0
u

, αn,i0
u

−

Lα

n,i0
u ϕi0

u, X n,i0
u

du

(cid:0)
(k

−

t∧θn

(cid:0)
(cid:1)
k−1
1)Y n,i0
u −  

Xℓ=0

ψn
s

⊤dBi0
u

ϕi0ℓ

ϕi0

−

!

(cid:1)
u, X n,i0
u

(cid:0)

!

(cid:1)

(cid:0)

(cid:1)i

Qi0(dudk)

∂tϕi0

u, X n,i0
u

+ λY

X n,i0
u

, Y n,i0
u

, αn,i0
u

Lα

n,i0
u ϕi0

u, X n,i0
u

1An(u)du

(cid:0)
(k

−

(cid:0)
(cid:1)
k−1
1)Y n,i0
u −  

Xℓ=0

−

(cid:1)

(cid:0)

(cid:1)i

Qi0(dudk) .

ϕi0ℓ

ϕi0

−

!

(u, X n,i0

u

)
!

Z(tn,θn∧t]

βn

ζ

−
t∧θn

∧

Xk≥0  
ε +

tn

Z

tn

Z

−

h

Z(tn,θn∧t]

Xk≥0  

ζ

−

∧

ε1{θn>t}∪{θn=τ r

n} ≤

M B,n
t∧θn

+ M Q,n
t∧θn

,

(4.43)

ζ

−

∧

ε1{θn>t}∪{θn=τ r

n} ≤

+

+

≤

+

+

We then get

where

M B,n
s

= βn

bn
s =

−

h
+

s

ζ

−

∂tϕ

ε +

∧
tn
Z
s, X n,i0
s

+ λY

s

tn
Z
X n,i0
s

bn
udu +

ψn
s

⊤dBi0

u ,

(cid:0)
(k

(cid:0)
(cid:1)
1)Y n,i0
s −  

−

k−1

Xℓ=0

Xk≥0  

ϕi0ℓ

ϕi0

−

!

(cid:0)

, Y n,i0
s

, αn,i0
s

Lα

n,i0
s ϕ

−

s, X n,i0
s

1An(s) +

(cid:1)
s, X n,i0
s

(cid:0)
γpk ,

(cid:1)i

!
(cid:1)
(u, X n,i0

u

M Q,n
s

=

(k

−

1)Y n,i0

u −  

Z(tn,s]

Xk≥0  

ϕi0ℓ

ϕi0

−

!

k−1

Xℓ=0

)
!

(cid:0)

Qi0(dudk)

γpkdu

.

−

(cid:1)

for s
∈
A1, M Q,n
·∧θn
and

[tn, T ]. From to Step 1, the deﬁnition of θn, the domination condition (4.34) and Assumption
is a pure jump martingale. Let Ln be the exponential local martingale deﬁned by Ln
tn = 1

dLn

s =

−

s bn
Ln
s |

ψn
s |

−2ψn
s

1An(s)⊤dBi0
s

∈

[tn, T ]. Ln is well deﬁned by (4.41), Assumption A1 and the deﬁnition of the set of
for s
·∧θn is a martingale. From Girsanov
admissible controls
Theorem for jump diﬀusion processes (see e.g. Theorem 1.35 in [22]) and the deﬁnition of θn, we
get that Ln

. Moreover, From the deﬁnition of θn, Ln

is a martingale. It follows from (4.43) that

A
+ Ln

·∧θnM B,n

·∧θn

·∧θnM Q,n

·∧θn

ζ

−

∧

εE[1

{θn=τ r

n}Ln
θn]

≤

≤

E

θnM B,n
Ln
θn
h
tnM B,n
Ln
tn + Ln

+ Ln
tnM Q,n

θnM Q,n
θn

i
tn = βn

ζ

−

∧

ε .

26

Since Ln
Therefore, the previous inequality becomes

·∧θn is a martingale and θn is a stopping time bounded by ε, we have E[Ln

θn] = Ln

tn = 1.

εE

ζ

∧

1

{θn<τ r

n}Ln
θn

βn .

≤

(4.44)

We next deﬁne the probability measure on

(cid:2)

T by the Radon-Nikodym derivative

(cid:3)

F
dPn
dP

= Ln
θn

(cid:12)
(cid:12)
and denote by En the expectation under Pn. Using Girsanov Theorem, we notice that τ r
(cid:12)
(cid:12)
same law under P and Pn. In particular, we have

FT

n has the

E

1{θn<τ r

n}Ln
θn
Comparing with (4.44), we have

1{τ r

≥

E

(cid:2)

(cid:3)

(cid:2)

n>ε}Ln
θn

= En

1{τ r

n>ε}

= E

1{τ r

n>ε}

= exp(

−

εγ) .

(cid:3)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

0

βn

ζ

−

∧

≤

ε exp(

−

εγ) ,

which contradicts (4.39) for n large enough.

4.3.2 Viscosity subsolution property

Step 1. Let ϕ
such that

∈

C 0([0, T ]

Rd), ϕi

×

∈

C 1,2([0, T ]

Rd) for i

×

∈ I

and (t0, x0, i0)

[0, T )

Rd

×

× I

∈

and

sup
i

ϕi
|

| ≤

ϕ

0 =

¯v∗
i0 −

ϕi0

(t0, x0) =

max
(t,x,i)∈[0,T ]×Rd×I

(¯v∗

i −

ϕi) (t, x) .

(4.45)

Without loss of generality we can assume that the maximum is strict in (t, x) once ﬁxed i0. We
then argue by contradiction and assume that

(cid:0)

(cid:1)

4η = min

(
−
(

∂tϕi0 + F∗ϕi0) (t0, x0) ;

ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(t0, x0)

)

> 0 .

(4.46)

By (4.29), Assumption A3 and (4.46) we may ﬁnd ε > 0 such that

ρ(t, x, y) =

−

∂tϕi0(t, x) + λY (x, y, ˆa(x, Dϕi0 (t, x)))

Lˆa(x,Dϕi0 (t,x))ϕi0 (t, x)

−
ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(t, x)

η , (4.47)

η

(4.48)

≥

≥

for all (t, x, y)
∈
locally Lipschitz map satisfying

[0, T )

×

×

Rd

R such that (t, x)

Bε(t0, x0) and

y
|

−

ϕi0(t, x)

| ≤

∈

ε, where ˆa is a

ˆa(x, Dϕi0 (t, x))

∈ N0(x, Dϕi0 (t, x)) on Bε(t0, x0) .

Observe that, since (t0, x0) is a strict maximizer, we have

ζ =

−

max
∂pBε(t0,x0)

(¯v∗

i0 −

ϕi0)(t, x) < 0 ,

(4.49)

(4.50)

where ∂pBε(t0, x0) =
t0 + ε
{
boundary of Bε(t0, x0).

} ×

cl(Bε(t0, x0))

∪

27

[t0, t0 + ε)

×

∂Bε(t0, x0) denotes the parabolic

 
 
Step 2. We now show that (4.47), (4.48), (4.49) and (4.50) lead to a contradiction to (3.25).
Let (tn, xn)n≥1 be a sequence such that

(tn, xn)

→

(t0, x0) and ¯vi0(tn, xn)

¯v∗
i0(t0, x0)

→

as

n

.

+

∞

→

Set y0 := ϕi0(t0, x0), ˆx0 := (x0, y0) and yn := ¯vi0(tn, xn)
that

−

n−1, ˆxn := (xn, yn) for n

1 and notice

≥

βn := yn

−

ϕi0(tn, xn)

−−−−−→n→+∞

0 .

(4.51)

Deﬁne the following stopping times

τn
τ ε
n
τ r
n
θn

:= inf

:= inf

:= inf
:= τn

i

∃

∈ V

tn :

n
s ,
n
tn :
s ,
i
tn : Qi0((tn, s]
τ r
n .

∈ V

∃

≥

≥

s
{
s
{
s
≥
{
τ ε
n ∧

∧

To alleviate the notations, we shall write

s, X n,i
s
Y n,i
(cid:0)
s −
|
N)
≥

×

ϕi
(cid:1)
1

/
∈

Bε(t0, x0)
}
s, X n,i
s

| ≥

,

,

ε
}

,

(cid:0)
}

(cid:1)

.

X n,i
.

, Y n,i
.

:= X tn,δ(i0,xn),αn,i

, Y n,i
X n,i
·
·
tn,δ(i0,xn), ˆαn
(cid:0)
(cid:1)
·
V
V
, X n,i
· = ˆa(X n,i
, Dϕi0 (
·
·
1. Since ˆa is locally Lipschitz, this solution is well-deﬁned. Since ¯vi

:= Y tn,δ(i0,xn),yn,αn,i
.
tn,δ(i0,ˆxn), ˆαn
· = ˆZ
ˆZ n
·

where ˆαn is the feedback control process given by ˆαn,i
for n
deduce from (4.50) and the deﬁnition of θn that on

· =
n
· =

, ˆX n,i

and

≤

≥

·

)) deﬁned on [tn, θn)
ϕi, we then

¯v∗
i ≤

Y n,i0
θn −

¯vi0

θn, X n,i0
θn
(cid:16)

(cid:17)

1

≥

{θn=τ ε

n}

Y n,i0
θn −
(cid:16)
{θn=τn}

ϕi0

we have

θn < τ r
n}
{
θn, X n,i0
θn
(cid:17)(cid:17)
(cid:16)
θn, X n,i0
Y n,i0
θn
θn −
(cid:16)

+1

{θn=τ ε

{θn=τn<τ ε

= ε1

¯v∗
i0
(cid:16)
Y n,i0
θn −
(cid:16)
Y n,i0
n}
θn
(cid:16)
Y n,i0
θn −
(cid:16)
, using the continuity of the trajectories of the particles Y i0ℓ
θn

n} + 1
n} + 1
ζ + 1{θn=τn<τ ε
n}

(cid:16)
ϕi0
−
(cid:16)
θn, X n,i0
θn
(cid:16)

(cid:17)(cid:17)
θn, X n,i0
θn
(cid:17)(cid:17)
θn, X n,i0
θn

{θn=τn<τ ε

{θn=τ ε

¯v∗
i0

+ ζ

ϕi0

ε1

(cid:17)(cid:17)

≥

≥

n}

∧

ε

.

(cid:17)(cid:17)

= Y i0
θn

and

Secondly, on
X i0ℓ
θn

= X i0
θn

ϕi0ℓ

Y n,i0ℓ
θn −
and from (4.48),

θn = τ r
n}
{
for all i0ℓ
∈ V
θn, X n,i0ℓ
θn

(cid:16)

(cid:17)

n
θn, we have

= Y n,i0

τ r
n −

ϕi0

n, X n,i0
τ r
τ r
n
(cid:16)

(cid:17)

+ ϕi0

n, X n,i0
τ r
τ r
n
(cid:16)

(cid:17)

ϕi0ℓ

−

n, X n,i0
τ r
τ r
n
(cid:16)

(cid:17)

,

Y n,i0ℓ
θn −

ϕi0ℓ

θn, X n,i0ℓ
θn
(cid:16)

(cid:17)

Y n,i0
θn −

ϕi0

≥

θn, X n,i0
θn

+ η ,

(cid:16)

(cid:17)

(4.52)

for all i0ℓ

n
θn.

∈ V

From (4.47) and (4.49), we get by Itˆo’s formula

¯vi

Y n,i
θn −
n−1 < ¯vi0(tn, xn), this is in contradiction with the dynamic programming

θn, X n,i
θn
(cid:16)

η + βn

n
θn .

∈ V

≥

∧

∧

(cid:17)

∀

ε

ζ

i

Since yn = ¯vi0(tn, xn)
principle (3.25) for suﬃciently large n by (4.51).

−

28

4.4 Viscosity properties on

4.4.1 Viscosity supersolution

T

{

} ×

Rd

× I

Let (x0, i0)

Rd

∈

× I

and ϕi

C 2(Rd) for i

∈
0 = ¯vi0,∗(T, x0)

satisfying

∈ I

ϕi0(x0) = min
I×Rd

−

(¯v·,∗(T,

)
·

−

ϕ·) .

Without loss of generalities we can take this minimum to be strict in x once ﬁxed i0.

Step 1. From the convention sup
[0, T )

, we have

Rd

×

× I

:=

∅

−∞

and since ¯v is a viscosity supersolution for (4.28) on

δ∗¯v·,∗ ≥

0 on [0, T )

Rd

×

× I

in the viscosity sense. From the upper-semicontinuity of δ∗, we can then deduce by a standard
argument (see e.g. proof of Lemma 5.2 in [25]) that δ∗ϕ(x0)

0.

≥

Step 2. We now prove

ϕi0(x0)

sup
0≤k< ¯K

−

ϕi0k(x0)

0 .

≥

From the deﬁnition of ¯v∗, there exists a sequence (sn, ξn)n≥1 converging to (T, x0) such that sn < T
for n

1 and

≥

Rd

.

× I

[sn, T ]

×

×

∈

¯B1(x0), which

For n

≥

lim
n→∞

¯vi0,∗(sn, ξn) = ¯vi0,∗(T, x0) .

1, consider the auxiliary test function

ϕn,i(t, x) := ϕi(x)

1
2 |

x

−

2 +

x0|

−

T

t
−
sn)2

(T

−

(t, x, i)

[0, T ]

∈

Let B1(x0) be the unit open ball in Rd centered at x0. Choose (tn, xn)
minimizes the diﬀerence ¯vi0,∗ −
n we have

ϕn,i0 on [sn, T ]

¯B1(x0).

×

We claim that, for n large enough tn < T , and xn converges to x0. Indeed, for suﬃciently large

(¯vi0,∗ −

ϕn,i0)(sn, x0)

≤ −

(T

1

−

sn)

< 0 .

On the other hand, for any x

¯B1(x0)

∈

(¯vi0,∗ −

ϕn,i0)(T, x) = ¯vi0,∗(T, x)

ϕi0(x) +

−

1
2 |

x

2

x0|

−

≥

¯vi0,∗(T, x)

ϕi0(x)

−

0 .

≥

Comparing the two inequalities leads us to conclude that tn < T for large n. Let x∗ be an adherence
value of the sequence (xn)n≥1. Since tn
ϕn,i0),
we have

sn and (tn, xn) minimizes the diﬀerence (¯vi0,∗ −

≥

lim inf
n→∞

lim sup
n→∞

(¯vi0,∗ −
(¯vi0,∗ −

ϕn,i0)(tn, xn)

ϕn,i0)(tn, xn)

−

−

(¯vi0,∗(T, .)

ϕi0)(x∗)

−

−
(¯vi0,∗ −
(¯vi0,∗ −

ϕn,i0)(sn, ξn)

ϕn,i0)(sn, ξn)

2

−

−

(¯vi0,∗(T, .)
−
1
xn
2 |
1
2 |
−
1
2 |

ϕi0)(x0)
x0|
x0|
2 .

x0|

x∗

xn

−

−

−

2

≤

≤

≤

29

Since x0 minimizes the diﬀerence ¯vi0,∗(T,

)
·

−

ϕi0 we have

0

(¯vi0,∗(T,

)
·

−
Hence x∗ = x0 and (xn)n≥1 converges to x0.

−

≤

ϕi0)(x∗)

(¯vi0,∗(T,

)
·

−

ϕi0 )(x0)

≤ −

1
2 |

x∗

2 .

x0|

−

We now use the viscosity supersolution property of ¯v on [0, T )

˜ϕn = ϕn + ¯v∗,i0(tn, xn)

−

ϕn,i0(tn, xn) and we have

Rd

×

× I

with the test function

˜ϕn,i0(tn, xn)

sup
0≤k< ¯K

−

˜ϕn,i0k(tn, xn)

0

≥

(4.53)

for all n

≥

1. We clearly have

ϕi0 (xn)

sup
0≤k< ¯K

−

ϕi0k(xn) = ˜ϕn,i0(tn, xn)

sup
0≤k< ¯K

−

˜ϕn,i0k(tn, xn) .

Since xn converges to x0, we get by sending n to inﬁnity that ϕi0 (x0)

sup0≤k< ¯K ϕi0k(x0)

0.

≥

−

Step 3. We now prove the last assertion. Assume that

F ∗ϕi0(x0) <

∞

and ϕi0(x0) = ¯vi0,∗(T, x0) < gi0

and let us work towards a contradiction. Since ¯v·(T,
is a constant η > 0 such that

) = g· by the deﬁnition of the problem, there
·

ϕi0 −
for some ε > 0. Since x0 is a strict minimizer, let ζ be

) = ϕi0 −
·

¯vi0(T,

gi0 ≤ −

η on Bε(x0)

2ζ = min

x∈∂Bε(x0)

¯vi0,∗(T, x)

−

ϕi0 (x) > 0 .

It follows that there exists r > 0 such that ¯vi0(t, x)
∂Bε(x0). This holds, otherwise, for each r > 0, we could ﬁnd (tr, xr)
that ¯vi0(tr, xr)
would have ¯vi0,∗(T, x∗)
of ζ.

∈
×
∂Bε(x0) such
ζ. Sending r to 0, since ∂Bε(x0) is compact, up to a subsequence we
∂Bε(x0), in contradiction with the deﬁnition

ζ > 0 for all (t, x)
[T

ζ for some x∗

≤
ϕi0(x∗)

ϕi0(xr)

ϕi0(x)

r, T ]

r, T ]

[T

≥

−

×

−

−

−

≤

−

∈

∈

Therefore, we have

¯vi0(t, x)
Since F ∗ϕi0(x0) <

−

ϕi0 (x)

ζ

η > 0 for (t, x)

([T

∧

≥

−
, up to smaller ε > 0, we have

∈

∞

r, T ]

×

∂Bε(x0))

T
(
{

∪

} ×

Bε(x0)) .

λY (x, y, a)
Rd

Laϕi0(x)
−
≤
R such that x

C for all a
Bε(x0) and

and (x, y)

ε(x, Dϕi0(x))
ε .
ϕi0(x)

| ≤

∈ N
y
|

−

∈
for some constant C > 0. Let ˜ϕi(t, x) := ϕi(x) + 2C(t

×

∈

T ). Then, for suﬃciently small r > 0,

−

¯vi0(t, x)

−

˜ϕi0(t, x)

(ζ

η) > 0

1
2

≥
Bε(x0)), and

∧

for (t, x)

([T

∈

−

r, T ]

×

∂Bε(x0))

T
(
{
∂t ˜ϕi0(t, x) + λY (x, y, a)

} ×

∪

−
ε(x, D ˜ϕi0 (t, x)) and (x, y)

Rd

La ˜ϕi0(t, x)

−

C

≤ −
Bε(x0) and

for all a
ε.
By following the same arguments as in Step 3 of Section 4.3.1, the latter inequalities lead to a
contradiction of (3.25).

R such that x

˜ϕi0(t, x)

∈ N

| ≤

y
|

×

−

∈

∈

30

4.4.2 Viscosity subsolution

Let (x0, i0)

Rd

∈

× I

and ϕi

C 2(Rd) for i

satisfying

∈ I

∈
0 = ¯v∗

i0(T, x0)

ϕi0 (x0) = max
I×Rd

−

(¯v∗

· (T,

)
·

−

ϕ·) .

Without loss of generalities we can take this maximum to be strict in x once have ﬁxed i0. We
argue by contradiction and assume δ∗ϕi0(x0) > 0 and

4η = min

ϕi0(x0)

−

(

gi0(x0) ;

ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(x0)

)

> 0 .

(4.54)

Step 1. By (4.32) and Assumption A3, we can ﬁnd r > 0 and a locally Lipschitz map ˆa satisfying

ˆa(x, Dϕi0 (x))

∈ N0(x, Dϕi0 (x))

(4.55)

for all x
that, for r, ε > 0 small enough,

∈

Br(x0). Set ˜ϕi(t, x) := ϕi(x) + √T

t. Since ∂ ˜ϕi(t, x)

→ −∞

as t

→

T , we deduce

−

ρ(t, x, y) =

for all (t, x, y)
notice that

∈

−
[T

∂ ˜ϕi0(t, x) + λY (x, y, ˆa (x, D ˜ϕi0 (t, x)))

Lˆa(x,D ˜ϕi0 (t,x)) ˜ϕi0(t, x)

r, T )

Rd

×

×

−

R such that x

∈

y
|

−

˜ϕi0 (t, x)

| ≤

−
Br(x0) and

η ,

(4.56)

≥
ε. We can also

˜ϕi0 −

sup
0≤k< ¯K

˜ϕi0k

!

(t, x0) =

ϕi0 −

sup
0≤k< ¯K

ϕi0k

!

(x0) .

Therefore, we get from (4.54)

˜ϕi0 −

sup
0≤k< ¯K

˜ϕi0k

!

(t, x)

≥

η ,

for all (t, x)

[T

∈

−

r, T ]

×

Br(x0) .

(4.57)

for r > 0 small enough.

Also observe that, since ¯v∗

i0 −

˜ϕi0 is upper-semicontinuous and

¯v∗
i0(t, x)

≤

˜ϕi0(t, x) + ε/2 for all (t, x)

[T

∈

−

r, T ]

˜ϕi0

¯v∗
i0 −
(cid:1)
Br(x0) .

(cid:0)
×

(T, x0) = 0, we have

(4.58)

for r > 0 small enough. Since ¯v·(T,

) = g·, we have for r small enough
·
¯vi0(T,

η on Br(x0) .

˜ϕi0 −

Since x0 is a strict maximizer for vi0,∗(T,

ϕi0, we can deﬁne ζ > 0 such that

gi0 ≥

) = ˜ϕi0 −
·
)
·

−
¯v∗
i0(T, x)

2ζ = max

x∈∂Br(x0)

−

ϕi0(x) < 0 .

−

for r > 0 small enough. It follows that, for r > 0 small enough, ¯vi0(t, x)
(t, x)

∂Br(x0). This means

r, T ]

[T

˜ϕi0(x)

−

≤ −

ζ < 0 for all

−
∈
¯vi0(t, x)

×
˜ϕi0(x)

−

ζ

∧

≤ −

η for all (t, x)

[T

−

∈

r′, T ]

×

∂Br(x0)

T
(
{

∪

} ×

Br(x0)) .

(4.59)

By following the arguments in Step 2 of Section 4.3.2, we see that (4.55), (4.56), (4.57), (4.58),
(4.59), lead to a contradiction of (3.25).

(cid:1)

(cid:0)

31

 
 
 
 
4.5 Uniqueness

We turn to the uniqueness of solution to the dynamic programming equation (4.28)-(4.33). To this
end, we need to introduce additional assumptions. We ﬁrst recall that the Hausdorﬀ distance dH
on closed subsets of A is deﬁned by

dH(B, C) = min

r

{

≥

0 : B

⊂

Cr and C

Br

}

⊂

for B, C

⊂

A closed and nonempty, with

Dr =

A :

a

∈

a′

∃

∈

D , dA(a, a′)

for any D

⊂

A and any r

≥

0. We use the convention

(cid:8)

r

≤

(cid:9)

dH(B, C) = +

∞

∅

if B =

or C =

.
∅
Assumption A4.
and σ : Rd

→

(i) The the functions λ and σ do not depend on the control, i.e. λ : Rd

Rd×m.

(4.60)

Rd

→

(ii) There exist two constants C > 0 and η

(0, 1] such that the function w appearing in As-

sumption A1(iii) satisﬁes w(x)

≤

∈
Cxη for x

R+.

∈

(iii) There exists a constant C > 0 such that

dH

for all ε, ε′

(iv)

N0(0, 0)

=

(cid:0)
≥
.
∅

ε(x, p) ,

N
N
0, x, x′, p, p′

ε′(x′, p′)

Rd.

(cid:1)

∈

p′

|

−

C

≤

p
|
(cid:0)

+ ε + ε′

(1 +

x
|

) + C
|

x
|

−

x′

|

(cid:1)

Remark 4.3. Since we use the convention (4.60), the combination of (iii) and (iv) implies that

=

ε(x, p)

Rd
for any (ε, x, p)
In particular we always have that δϕ

R+ ×

∈

∅

N

Rd.
0 for any ϕ

∈
supersolution solution property takes the following form

×
≥

C 2(Rd). Therefore, the terminal viscosity

min

(

ϕi(x)

−

gi(x) ;

ϕi

sup
0≤k< ¯K

−

ϕik

!

(x)

) ≥

0

(4.61)

for (x, i)

Rd

× I

∈
Lemma 4.2. let u :
Deﬁne the function Λ :

[0, T ]

×
[0, T ]

Rd
Rd

× I
→

×
Λ(t, x) = θe−κt(1 +

and (ϕj )j∈I a test function according to Deﬁnition 4.2 (i).

be a lower semi-continuous supersolution of (4.28)-(4.61).
R by

x
|
×
R+. Then, under Assumptions A1 and A4, for any γ

[0, T ]

(t, x)

∈

2γ+2) ,
|

Rd .

≥

κ0 and θ > 0, the function u + Λ is a supersolution to (4.28)-(4.33).

0 there exists κ0 > 0 such

with θ, κ, γ
∈
that for any κ

≥

32

6
6
 
C 1,2([0, T ]
Proof. Let ϕj
maximum in (t, x, i) which is equal to 0 and ¯ϕ
u is a supersolution for (4.28), we have

Rd) for j

∈ I

×

∈

be such that the function ϕ· −
∈

Rd) such that supj∈I |

C 0([0, T ]

×

(u + Λ) has a local
¯ϕ. Since

ϕj

| ≤

min

∂t(ϕi

−

(−

Λ)(t, x) + F ∗(ϕi

Λ)(t, x) ;

−

(ϕi

Λ)

sup
0≤k< ¯K

−

−

(ϕik

−

Λ)

!

(t, x)

) ≥

0 .

We then have

ϕi

sup
0≤k< ¯K

−

ϕik

!

(t, x) =

(ϕi

Λ)

sup
0≤k< ¯K

−

−

(ϕik

−

Λ)

!

(t, x)

≥

0 .

(4.62)

We now prove that

∂tϕi(t, x) + F ∗ϕi(t, x)

−

0 .

≥

If F ∗ϕi(t, x) = +
. From
Assumption A4, we get that F ∗ is locally bounded. Since u is a viscosity supersolution to (4.28),
we have

, then the inequality is obvious. Suppose that F ∗ϕi(t, x) < +

∞

∞

∂t(ϕi

−

−

Λ)(t, x) + F ∗(ϕi

Λ)(t, x)

−

0 .

≥

Using the deﬁnition of Λ and F , Assumption A4 and the continuity of the functions considered,
we get

+ lim
ε→0

∂tϕi(t, x)

−
sup

′

θκe−κt(1 +

2γ+2)
x
|
|
λY (x′, y′, a)

−
sup
a∈Nε(x′,p)

(cid:8)

(cid:9)

|x − x
| ≤ ε
|(ϕi − Λ)(t, x) − y
| ≤ ε
|D(ϕi − Λ)(t, x) − p| ≤ ε

′

Tr

1
2
We next deﬁne the function Γε : Rd
(x′, y′, p)
R

σσ⊤(x)D2ϕi(t, x)
(cid:17)

Rd

−

R

(cid:16)

×
Rd. Then, we get from (4.63)

→

×

λ(x)⊤Dϕi(t, x) + θe−κtλ(x)⊤D

x
|
+ θe−κt 1
x
2
|
R by Γε(′x, y′, p) = supa∈Nε(x′,p) {

σσ⊤(x)D2
(cid:16)

2γ+2
|
2γ+2
|

Rd

Tr

≥

(cid:17)

0 .

−

(4.63)

λY (x′, y′, a)
}

for

∈

×

×

∂tϕi(t, x) + lim
ε→0

−

Γε(x′, y′, p)

sup

′

| ≤ ε
′

|x − x
|ϕi(t, x) − y
| ≤ ε
|Dϕi(t, x) − p| ≤ ε
1
2

Tr

′

|x − x
|ϕi(t, x) − y
| ≤ ε
|Dϕi(t, x) − p| ≤ ε

| ≤ ε
′

−
θκe−κt(1 +

λ(x)⊤Dϕi(t, x)
2γ+2) + lim
|
ε→0

x
|

−

σσ⊤(x)D2ϕi(t, x)
(cid:16)
(cid:17)
Γε(x′, y′, p)
sup

lim
ε→0

−

sup

′

| ≤ ε
|x − x
|(ϕi − Λ)(t, x) − y
| ≤ ε
|D(ϕi − Λ)(t, x) − p| ≤ ε

′

Γε(x′, y′, p)

θe−κtλ(x)⊤D

−

x
|

2γ+2
|
−
θκe−κt(1 +

θe−κt 1
2

−

Tr

σσ⊤(x)D2
(cid:16)

x
|

θe−κt 1
2
2γ+2)
x
|
|
2γ+2
|

(cid:17)
33

Tr

−

σσ⊤(x)D2
x
|
(cid:16)
θe−κtλ(x)⊤D

2γ+2
|
x
|

(cid:17)
2γ+2
|

+ ∆Γ1(t, x) + ∆Γ2(t, x) ,

≥

=

(4.64)

 
 
 
where

∆Γ1(t, x) = lim
ε→0

∆Γ2(t, x) = lim
ε→0

sup

′

|x − x
|ϕi(t, x) − y
| ≤ ε
|Dϕi(t, x) − p| ≤ ε

| ≤ ε
′

Γε(x′, y′, p)

lim
ε→0

−

sup

′

|x − x
|ϕi(t, x) − y

| ≤ ε
′

| ≤ ε

|D(ϕi − Λ)(t, x) − p| ≤ ε

sup

′

|x − x
|ϕi(t, x) − y

| ≤ ε
′

| ≤ ε

|D(ϕi − Λ)(t, x) − p| ≤ ε

Γε(x′, y′, p)

lim
ε→0

−

sup

′

|x − x
| ≤ ε
|(ϕi − Λ)(t, x) − y
| ≤ ε
|D(ϕi − Λ)(t, x) − p| ≤ ε

′

Γε(x′, y′, p) ,

Γε(x′, y′, p) .

From Assumptions A1 and A4, we get a constant C1 that does not depend on (t, x, i) such that

∆Γ1(t, x)

≥ −

lim
ε→0

′

|x − x
|ϕi(t, x) − y
| ≤ ε
|Dϕi(t, x) − p| ≤ ε

| ≤ ε
′

sup

′

| ≤ ε
|x − ˜x
′
|ϕi(t, x) − ˜y

| ≤ ε

|D(ϕi − Λ)(t, x) − ˜p| ≤ ε

Γε(x′, y′, p)

Γε(˜x′, ˜y′, ˜p)

−

C1|

η(1 +
DΛ(t, x)
|

x
|

η) ,
|

≥ −

Analogously, for the second term we get a constant C2 > 0 that does not depend on (t, x, i) such
that

∆Γ2(t, x)

C2Λ(t, x) .

≥ −

Considering the right-hand side of (4.64) and taking into account the growth condition of the
diﬀerent terms, there exists a constant κ0, which does not depend on θ, such that if κ
κ0 this
expression is non-negative. Henceforth, with (4.62), we obtain that u+Λ is a viscosity supersolution
to (4.28).

≥

We ﬁnally take (i, x)

supi∈I |

ϕi

| ≤

¯ϕ and

∈ I ×

Rd and functions ϕj

∈

C 2(Rd) and ¯φ

C 0(Rd) such that

∈

0 = ui0,∗(T, x) + Λ(T, x)

ϕi(x) = max
I×Rd

−

(u·,∗(T, .) + Λ(T, .)

ϕ·) .

−

Since u is a supersolution to (4.33), we have

ϕi(x)

−

Λ(T, x)

≥

gi(x) ,

since Λ
u + Λ is a viscosity supersolution to (4.33).

0, we get ϕi(T, x)

≥

≥

gi(x). Combining it with (4.62), we obtain from Remark 4.3 that

We turn to the main result of this section which is a comparison theorem. We recall that the

deﬁnition of

on

.
|
|

I

is given in Section 2.1.

Theorem 4.5. Let ¯w· (resp. ¯u·) be a lsc (resp. usc) viscosity supersolution (resp. subsolution) to
(4.28)-(4.61). Suppose that there exists γ > 0 such that

and

sup
(t,x,i)∈[0,T ]×Rd×I

¯wi(t, x)
|
|
1 +

¯ui(t, x)
+
|
|
γ
x
|
|

< +

,

∞

sup
(t,x)∈[0,T ]×Rd |

¯wi(t, x)
|

+

¯ui(t, x)
|

| −−−−→|i|→∞

0 .

Then, under Assumption A1-A2-A4, we have ¯u· ≤
Proof. We proceed in six steps.

¯w· on [0, T ]

Rd

.

× I

×

34

(4.65)

(4.66)

Step 1. We deﬁne Λθ,κ(t, x) = θe−κt(1 +
R+. From
Lemma 4.2, there exist κ large enough such that for any θ > 0, ¯w· + Λθ,κ is also a supersolution
for (4.28)-(4.33). Set ¯wi,θ,κ(t, x) = ¯wi(t, x) + Λθ,κ(t, x), (i, t, x)

2γ+2) for (t, x)
|

Rd with θ, κ

[0, T ]

[0, T ]

Rd.

x
|

×

∈

∈

∈ I ×
For some η, η′ > 0 to be chosen below, let βt = e(η+η′ )t for t

×

[0, T ]. A straightforward

derivation shows that βt ¯wi,θ,κ (resp. βt ¯ui) is a viscosity supersolution (resp. subsolution) to

∈

min

ηwi

(

−

∂twi + ˜F (t, x, wi, Dwi)

λ⊤Dwi

−

1
2

Tr

−

σσ⊤D2wi
(cid:16)

(cid:17)

;

wi

sup
0≤k< ¯K

−

wik

)

= 0 on [0, T ]

Rd,(4.67)

×

min

wi

(

−

˜g ; δwi ; wi

sup
0≤k< ¯K

−

wik

)

= 0 on

T
{

} ×

Rd . (4.68)

where

˜F (t, x, y, p) =

sup
a∈ ˜N0(t,x,p)
˜λY (t, x, y, a) = βtλY (x, β−1

˜λY (x, y, a)

,

˜
N0(t, x, p) =

N0(x, β−1

t p) ,

t y, a) + η′y

, ˜gi(x) = βT gi(x) ,

Rd
for all (t, x, i, y, p, a)
enough so that ˜λY and, consequently, ˜F are nondecreasing in y.

× I ×

[0, T ]

Rd

×

×

×

R

∈

A. Since λY is Lipschitz, we can choose η′ large

Let ε > 0. From an analogous computation, using the monotonicity of ˜F , we see that βt ¯wi +

ε/2|i| is a viscosity supersolution to

ηwi

−

∂twi + ˜F (t, x, wi, Dwi)

−

λ(x)⊤Dwi

−
min

Tr

1
2
(cid:16)
wi(T,

{

σσ(x)⊤D2wi

0 ,

≥

˜g ; δwi

(cid:17)
} ≥
=: ∆i > 0 .

0 ,

)
·

−
ε
2|i|+1

wi

sup
0≤k< ¯K

−

wik

≥

(4.69)

(4.70)

(4.71)

Step 2. Set ˜ui = βt ¯ui and ˜wi,θ,κ,ε = βt¯vi + βtΛθ,κ + ε/2|i| = βt ¯wi,θ,κ + ε/2|i| . To prove our result,
it is enough to show that

˜ui(t, x)

≤

˜wi,θ,κ,ε(t, x)

Rd and θ, ε > 0. Then taking the limit as θ
for each (i, t, x)
0, we obtain
the desired result. For simplicity, we write ˜wi,θ,κ,ε for ˜wi in the sequel. We argue by contradiction
and suppose that

0 and ε

∈ I ×

[0, T ]

→

→

×

Due to the growth condition on ˜u· and ˜w·, there exist R > 0 such that

sup
I×[0,T ]×Rd

˜u· −

˜w· > 0 .

for all (i, t, x)
∈ I ×
semicontinuous, there exist (i0, t0, x0)

[0, T ]

×

Rd such that

sup
(i,t,x)∈I×[0,T ]×Rd

(˜ui

˜ui(t, x)

˜wi(t, x) < 0

−
x
R. Then from (4.66) and since u· −
| ≥
|
[0, T ]
∈ I ×
×
˜wi)(t, x) = (˜ui0 −

˜wi0)(t0, x0) > 0 .

Rd such that

−

(4.72)

(4.73)

˜w· is upper

(4.74)

35

Step 3. For n

≥

1, we deﬁne the function

Θn(t, x, y, i) = ˜ui(t, x)

˜wi(t, y)

−

−

ϕn(t, x, y, i)

with

ϕn(t, x, y, i) = n

for all (t, x, y, i)
×
there exists (tn, xn, yn, in)
We have

[0, T ]

∈

Rd

Rd
×
[0, T ]

∈

× I
Rd
×

×

×I

x
|

y

2 +
|

x
|

4 +

x0|

t
|

2 + 1i6=i0.

t0|

−

−
. By the growth assumption on ˜u and ˜v and (4.66), for all n,
.

attaining the maximum of Θn on [0, T ]

Rd

Rd

Rd

−

×

×

×I

Θn(tn, xn, yn, in)

Θn(t0, x0, x0, i0) = (˜ui0 −
By (4.73) and (4.66), up to a subsequence, (tn, xn, yn, in) converge to (ˆt, ˆx, ˆy,ˆi). Sending n to
inﬁnity, this provides

˜wi0)(t0, x0) .

≥

¯ℓ := lim sup
n→∞

ϕn(tn, xn, yn, in)

≤

[˜uin(tn, xn)

˜win(tn, yn)

˜wi0)(t0, x0)]

lim sup
n→∞
˜uˆi(ˆt, ˆx)

˜wˆi(ˆt, ˆy)

(˜ui0 −
−
˜wi0)(t0, x0) .

−
(˜ui0 −

≤

−
and ˆx = ˆy. Using the deﬁnition of (t0, x0, i0) as a maximizer of ˜u· −

−

In particular, ¯ℓ < +
see that:

∞

0

≤

¯ℓ

(˜uˆi −

≤

˜wˆi)(ˆt, ˆx)

(˜ui0 −

−

˜wi0)(t0, x0)

0 ,

≤

which implies

(tn, xn, yn, in)
2
xn
n
|
−
|
˜win(tn, yn)

yn

−

˜uin(tn, xn)

(t0, x0, x0, i0) ,
0 ,
(˜ui0 −

˜wi0)(t0, y0) .

→

→

→

Being

I

endowed with the discrete topology, we can assume in = i0 for all n

1.

≥

Step 4. We now show that for n large enough

˜ui0(tn, xn)

sup
0≤k< ¯K

−

˜ui0k(tn, xn) > 0 .

On the contrary, up to a subsequence, we would have for all n,

˜ui0(tn, xn)

sup
0≤k< ¯K

−

˜ui0k(tn, xn)

0 .

≤

Moreover, by the viscosity supersolution property of ˜w to (4.71), we have

˜wi0(tn, yn)

sup
0≤k< ¯K

−

˜wi0k(tn, yn)

≥

∆i0 > 0 .

We deduce from the two previous inequalities

˜w· , we

(4.75)

(4.76)

(4.77)

(4.78)

(4.79)

˜ui0(tn, xn)

−

˜ui0(tn, xn)

˜ui0k(tn, xn)

sup
≤
0≤k< ¯K
˜wi0(tn, yn) + ∆i0 ≤

−

≤

˜wi0(tn, yn)

sup
0≤k< ¯K

−

˜wi0k(tn, yn)

∆i0

−

sup
0≤k< ¯K
sup
0≤k< ¯K

˜ui0k(tn, xn)

[˜ui0k(tn, xn)

˜wi0k(tn, yn)

sup
0≤k< ¯K
˜wi0k(tn, yn)] .

−

−

(4.80)

36

Since ∆i0 > 0, for all n there exists kn such that

sup
0≤k< ¯K

[˜ui0k(tn, xn)

−

˜wi0k(tn, yn)]

∆i0
2 ≤

−

˜ui0kn(tn, xn)

−

˜wi0kn(tn, yn) .

From (4.66), up to a subsequence, we may assume that (kn) converges to k0 in N. Hence, by
sending n to inﬁnity into (4.80), it follows with (4.77) and the upper (resp. lower)-semicontinuity
of ˜u (resp. ˜w) that :

(˜ui0 −

˜wi0)(t0, x0) +

∆i0
2 ≤

(˜ui0k0 −

˜wi0k0)(t0, x0) ,

which is a contradiction to (4.74).

Step 5. Let us check that, up to a subsequence, tn < T for all n. On the contrary, tn = t0 = T
for n large enough, and from (4.78), and the viscosity subsolution property of ˜u to (4.68), we would
get

On the other hand, by the viscosity supersolution property of ˜w to (4.68), we have ˜w(T, yn)
˜gi0(yn), and so

≥

˜ui0(T, xn)

˜gi0 (xn) .

≤

˜ui0(T, xn)

˜w(T, yn)

˜gi0(xn)

˜gi0(yn) .

≤
By sending n to inﬁnity, and from Assumption (A2) and (4.77), this would imply ˜ui0(t0, x0)
˜v(t0, x0)

0, a contradiction to (4.72).

−

−

−

≤

Step 6. We may then apply Ishii’s lemma (see Theorem 8.3 in [7]) to (tn, xn, yn)
that attains the maximum of Θn(., i0) and we get (pn
¯J 2,−˜vi0(tn, yn) such that

¯J 2,+ ˜ui0(tn, xn) and (pn
˜U

˜u , Mn)

˜u, qn

∈

∈

[0, T )

Rd

Rd
×
×
, qn
˜v , Nn)

∈

pn
˜u −

and

where

pn
˜v = ∂tϕn(tn, xn, yn, i0) = 2(tn
−
qn
˜u = Dxϕn(tn, xn, yn, i0) = n(xn
qn
˜w =

−
Dyϕn(tn, xn, yn, i0) = n(xn

t0) ,

−

yn) + 4(xn
yn) ,

−

xn
x0)
|

−

−

2 ,

x0|

Mn
0

(cid:18)

0
Nn(cid:19)

−

An +

≤

1
2n

A2

n ,

(4.81)

An = D2

(x,y)ϕn(tn, xn, yn, i0) = n

(cid:18)
with Id and Od respectively the identity and the zero matrix of Rd×d. We can therefore bound the
right-hand side of (4.81) by

−

(cid:18)

Id
Id

Id
−
Id (cid:19)

−

xn
4
|

−

x0|

2Id + 8(xn
Od

x0)(xn

−

−

x0)⊤ Od
Od(cid:19)

,

An +

1
2n

A2

n ≤

3n

Id
Id

−

(cid:18)

Id
−
Id (cid:19)

+

Od
A′
n
Od Od(cid:19)

(cid:18)

,

(4.82)

37

n such that lim supn→∞

with A′
˜wi0 to (4.67), we have

1
|xn−x0|2 |

A′
n|

< +

∞

. From the viscosity supersolution property of

η ˜wi0(tn, yn)

−

˜w + ˜F ∗(tn, yn, ˜wi0(tn, yn), qn
pn
˜w)

λ(yn)⊤qn

˜w −

−

1
2

Tr(σσ⊤(yn)Nn)

0 .

≥

On the other hand, from (4.78) and the viscosity subsolution property of ˜u to (4.67), we have

η˜ui0(tn, xn)

−

˜u + ˜F∗(tn, xn, ˜ui0(tn, xn), qn
pn
˜u )

λ(xn)⊤qn

˜u −

−

1
2

Tr(σσ⊤(xn)Mn)

0 .

≤

By subtracting the two previous inequalities, we obtain

η(˜ui0 (tn, xn)

˜wi0(tn, yn))

−

≤

pn
˜u −
+λ(xn)⊤qn
˜u −
pn
˜u + ∆C 1

˜u −

= pn

˜w + ˜F ∗(tn, yn, ˜wi0(tn, yn), qn
pn
˜w)

˜F∗(tn, xn, ˜ui0(tn, xn), qn

−
Tr(σσ⊤(xn)Mn)

1
2

−

˜u ) +
Tr(σσ⊤(yn)Nn)

λ(yn)⊤qn
n + ∆C 2

1
˜w +
2
n + ∆C 3
n

(4.83)

where

∆C 1
∆C 2

∆C 3

n = ˜F ∗(tn, yn, ˜wi0(tn, yn), qn
˜w)
λ(yn)⊤qn
n = λ(xn)⊤qn
˜w ,
1
1
2
2

˜u −
Tr(σσ⊤(xn)Mn)

n =

−

Tr(σσ⊤(yn)Nn) .

˜F∗(tn, xn, ˜ui0(tn, xn), qn

˜u ) ,

−

From (4.75)), we have pn
have ∆C 2
have ∆C 3
→
Assumptions A1 and A4 and (4.75) ∆C 1

0 as n
0 as n

pn
˜u →

0 as n

→
→

→

0. From the Lipschitz continuity of λ and (4.76), we
˜u −
0. From (4.82), (4.75), (4.76), and the Lipschitz property of σ, we also
0. Following the same argument as in the proof of lemma 4.2, we get from

→

Therefore, by sending n
0, a contradiction with (4.74).

→ ∞

into (4.83), we conclude with (4.77) that η(˜ui0 (t0, x0)

˜wi0(t0, y0))

≤

−

n →

0 as n

.

→ ∞

From Theorems 4.3, 4.4 and 4.5, we get the following characterisation of the function ¯v.

Corollary 4.1. Suppose that ¯v satisﬁes

sup
(t,x,i)∈[0,T ]×Rd×I

for some γ > 0 and

+

¯vi(t, x)
|
|
1 +

¯ui(t, x)
|
|
γ
x
|
|

< +

,

∞

sup
(t,x)∈[0,T ]×Rd |

¯vi(t, x)
|

+

¯ui(t, x)
|

| −−−−→|i|→∞

0 .

(4.84)

(4.85)

Under Assumptions A1-A2-A3-A4, ¯v is the unique viscosity solution to (4.28)-(4.61) satisfying
(4.84)-(4.85). Moreover, ¯v is continuous on [0, T )

×
We recall that Section 2.3 provides an example of a value function satisfying conditions (4.28)-

× I

Rd

.

(4.61).

38

A Appendix

A.1 Set of atomic ﬁnite measures

Proposition A.6. For ℓ
convergence of measures.

≥

1, Eℓ is a closed subset of

F (

M

I ×

Rℓ) for the topology of the weak

Proof. Let (µn)n∈N be a sequence of Eℓ such that µn =
prove that µ is an element of Eℓ, i.e.
V
|

<
and some points (xi)i∈V .
Consider the continuous functions 1

∞

|

{i}×Rℓ for i

it can be written as µ =

P

. We then have

P

i∈Vn δ(i,xi
n)

∈ I

w
→

µ

∈ M
i∈V δ(i,xi) for some set V

I ×

F (

Rℓ). We
,

⊆ I

µn , 1
h

{i}×Rℓ

=

i

1
0
(

if i
∈
if i /
∈

Vn
Vn

µn , 1
For each i
, we have that the sequence (
)n is a convergent sequence in
h
i
which is in particular stationary. Let V be deﬁned as follow:

{i}×Rℓ

∈ I

,
0, 1
}
{

V :=

i

:

µn , 1
h

∈ I

{i}×Rℓ

i −→n→∞

1

.

n

o

Let i

∈

V . Since the functions previously described converge, they are constant from a certain
C(Rℓ) and consider the

ni we have i

Vn. For f

∈

∈

rank and there exists ni
function 1{i} ⊗

I ×

f :

∈
Rℓ

N such that for n
R. We have:

≥

→
f (xi

∈

n) =

µn, 1{i} ⊗
h

f

µ, 1{i} ⊗

f

i −→ h

i ∈
C(Rℓ) the sequence (f (xi

R.

This means that for each i
converges to a point xi

∈
Rℓ.

V and f

∈

We then notice that any continuous and bounded function f on

with fi is continuous and bounded on Rℓ. In particular, we get

for n large enough, and

ZI×Rℓ

f dµn =

fi(xi
n)

Xi∈V

n))n converges, therefore (xi

n)n

Rℓ is of the form f =

i∈I fi

I ×

P

f dµn

ZI×Rℓ

−−−−−→n→+∞

ZI×Rℓ

f d

δ(i,xi)

!

Xi∈V

so we have µ =

i∈V δ(i,xi).

To ﬁnally prove that µ

Eℓ we need to show that there do not exist i, j

V such that i

P

∈

Fix i, j
get i ⊀ j and j ⊀ i. Therefore, we have µ

∈

V . From the previous steps, there exists some n such that i, j

∈

∈

Vn. Since µn

j.
≺
Eℓ, we

∈

Eℓ.

∈

A.2 Branching martingale controlled problem
We ﬁrst set our controlled martingale problem. We deﬁne the set ˜Em+1 as the set of ﬁnite measure
µ on

Rm+1 of the form

N

I ×

×

µ =

Xi∈I,n∈N

1
22(|i|+n)

δ(i,n,bi,qi,n)

39

 
with bi
∈
˜Em+1 is Polish.

Rm and qi,n

∈

R for i

and n

∈

∈ I

N. From the same argument as for Eℓ, we have that

We then set X = D([0, T ], Ed+1)

×
to Ed+1 and ˜Em+1. We denote by x the canonical process and by G = (
G
ﬁltration on X.
For ¯x = (

X, we write

1

i∈Vs δ(i,ˆxi

s),

i∈I,n∈N

22(|i|+n) δ(i,n,bi

s,qi,n

s ))s∈[0,T ] ∈

D([0, T ], ˜Em+1) the space pairs of c`adl`ag functions from [0, T ]
t)t∈[0,T ] the canonical

P
1 ¯x =

P

δ(i,ˆxi
s)

!

s∈[0,T ]

and

2 ¯x =

i∈Vs
X

1
22(|i|+n)

δ(i,n,bi

s,qi,n

s )

(A.86)

s∈[0,T ]



Xi∈I,n∈N


We also deﬁne 1x and 2x the ﬁrst and second component of the canonical process.



∈

×I ×
R (resp. g :

→
C 1,2([0, T ]
N

Rd+1) (resp. C 1,2([0, T ]
Rm+1
(resp. (i, n)
C 1,2([0, T ]

Let C 1,2([0, T ]
Rd+1
I ×
)
gi,n(
·
C 1,2
c ([0, T ]
∈
Rm+1)) such that there exists a compact K
gi,n(t, x) = 0) for (t, i, x)
such that x /
∈

[0, T ]
×
× I ×
Rm+1)) for all i
∈ I
Rm+1)) the set of f

Rd+1 such that x /
∈

×I ×
→

× I ×

×I ×

[0, T ]

×
×

K).

∈ I ×
× I ×
Rd+1 (resp. K

⊂

N

∈

We deﬁne the operator ∆ by

×

N
)
R) such that fi(
·

Rm+1)) be the set of functions f : [0, T ]

C 1,2([0, T ]

∈
N) and C 1,2
c ([0, T ]
Rd+1) (resp. g

×
× I ×
C 1,2([0, T ]

×
Rd+1) (resp.
Rd+1) (resp.

N

∈

×
Rm+1) satisfying fi(t, x) = 0 (resp.
Rm+1
[0, T ]

× I ×

N

K (resp. (t, i, n, x)

⊂

∈

×I ×

×

∆g(s, µ) =

Xi∈I,n∈N

1

22(|i|+n) ∆bgi,n(bi, qi,n)

C 1,2

c ([0, T ]

˜Em+1, where ∆b stands
for g
for the Laplacian operator with respect to the third variable of the function g. For a given control
α

∈
, let ¯Lα be the following second order local operator

22(|i|+n) δ(i,n,bi,qi,n) ∈

Rm+1) and µ

× I ×

i∈I,n∈N

P

×

N

∈

1

∈ A

¯LαFf,g(s, ¯x) = ∂1F (f (s, 1 ¯xs), g(s, 2 ¯xs))

∂t + ˆLαi(s,2 ¯xs)

fi(s, ˆxi
s)

+∂1F (f (s, 1 ¯xs), g(s, 2 ¯xs))
(cid:16)
∂11F (f (s, 1 ¯xs), g(s, 2 ¯xs))

+

1
2

+∂12F (f (s, 1 ¯xs), g(s, 2 ¯xs))
1
∂bgi,n(s, bi
22(|i|+n)

Xi∈Vs

(cid:0)
∂t +

1
2

∆

(cid:1)
g(s, 2 ¯xs)
(cid:17)
ˆσ(ˆxi

s, α(s, 2 ¯x)

⊤Dfi(s, ˆxi
s)

2

Xi∈Vs (cid:12)
(cid:12)

(cid:1)

(cid:12)
(cid:12)

s, qi,n

s )⊤ ˆσ(ˆxi

s, α(s, 2 ¯x)

⊤Dfi(s, ˆxi
s)

Xi∈Vs,n∈N
1
2

+

∂22F (f (s, 1 ¯xs), g(s, 2 ¯xs))

+γ

pn

F

Xi∈I,n∈N (cid:12)
(cid:12)
(cid:12)
n−1
(cid:12)
f (s, 1¯xs) + 1Vs(i)

1

(cid:1)
22(|i|+n) ∂bgi,n(s, bi

(fiℓ

−

fi)(s, ˆxi

s),

2

s, qi,n
s )
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi∈I,n∈N

(cid:16)

n
g(s, 2 ¯xs) +

1
22(|i|+n)

Xℓ=0
g(i, n, s, bi

s, qi,n

s + 1)

−

g(i, n, s, bi

s, qi,n
s )

(cid:17)

−

F (f (s, 1 ¯xs), g(s, 2 ¯xs))

(cid:0)

o

40

 
for s

[0, T ], ¯x =
C 1,2

∈
Rd+1), g
∈
process ¯M t,α,Ff by

c ([0, T ]

(cid:16)P

i∈Vs δ(i,ˆxi

N

× I ×

×

i∈I,n∈N

u),
Rm+1) and F

P

22(|i|+n) δ(i,n,bi
C 2

u,qi,n
u )
c (R2) with Ff,g = F

u∈[0,T ] ∈

(cid:17)

X, f

C 1,2

c ([0, T ]

∈

× I ×
(f, g). We then deﬁne the

1

∈

◦

t,α,Ff,g
¯M
s

= Ff (s, xs)

s

−

t
Z

¯LαFf,g(u, x)du ,

[t, T ] .

s

∈

∈ A

X, and a
. A probability measure ¯Pt,1 ¯x,α is a solution to the controlled martingale problem

Deﬁnition A.3 (Martingale problem). Consider the initial condition (t, ¯x)
control α
if the process ¯M t,α,Ff,g is a G-martingale under ¯Pt,1 ¯x,α for any f
C 1,2
Rm+1), and any F
c ([0, T ]
¯Pt,1 ¯x,α(2x
2G = (2

× I ×
∈
t)t∈[0,T ] stands for the canonical ﬁltration on D([0, T ], ˜Em+1).

×
Rd+1), g
c ([0, T ]
∈
c (R2), ¯Pt,1 ¯x,α(1xs = 1 ¯xs for s
C 2
[0, t]) = 1 and
t where W stands for the law of the process ξ and

2G) = W(2G) for any 2G

× I ×
∈

C 1,2

∈
2
G

[0, T ]

×

N

∈

∈

∈

G

Deﬁnition A.4 (Shifted martingale problem). Consider the initial condition (t, ¯x)
and a control α
problem if the process ¯M t,α,Ff,g is a G-martingale under ¯Pt,¯x,α for any f
g

X,
×
. A probability measure ¯Pt,¯x,α is a solution to the shifted controlled martingale
Rd+1),

∈
c (R), and ¯Pt,¯x,α(xs = ¯xs for s

c ([0, T ]
[0, t]) = 1.

Rm+1), and any F

c ([0, T ]

× I ×

C 1,2

C 1,2

[0, T ]

∈ A

C 2

∈

∈

× I ×

∈

∈

We are now able to state the main result of this section. For that we need the following
on D([0, T ], ˜Em+1) as

notations. We ﬁrst extend the deﬁnition of the concatenation operator
follows:

⊕

t ˜y)s =

(y

⊕

1
22(|i|+n)

Xi∈I,n∈N

δ(i,n,(bi⊕t˜bi)s,(qi,n⊕t ˜qi,n)s)

with

(bi

[0, T ], y = (

for s
In particular, we have
P

∈

i∈I,n∈N

(qi,n

t ˜bi)s = bi
s∧t
⊕
t ˜qi,n)s = qi,n
s∧t

1s<t + (˜bi
s −
1s<t + (˜qi,n
s −

˜bi
t + bi
t)1s≥t
t + qi,n
˜qi,n

t )1s≥t

⊕
22(|i|+n) δ(i,n,bi

1

u ))u∈[0,T ] and ˜y = (

u,qi,n

i∈I,n∈N

1

22(|i|+n) δ(i,n,˜bi

u ))u∈[0,T ].

u,˜qi,n

ξ(ω

⊕

t ˜ω) = ξ(ω)

t ξ(˜ω)

⊕

P

for ω, ˜ω
by

∈

Ω and t

∈

[0, T ]. For η : [0, T ]

2X

×

→

R and ¯x

∈

X we ﬁnally deﬁne the function ηt,2 ¯x

for ¯x′

∈

X and s

[0, T ].

∈

ηt,2 ¯x(s, 2 ¯x′) = η(s, 2 ¯x

2 ¯x′)

t

⊕

Theorem A.6. Suppose that Assumption A1 holds and that there exists a unique solution to the
martingale problem and the shifted martingale problem for each initial condition and control. Let
(t, ¯x, α)

and τ a G-stopping time valued in [t, T ]. Then, we have

[0, T ]

X

∈

×

× A

¯Pt,1 ¯x,α
¯x′

= Pτ (¯x′),¯x′,ατ (¯x′),2 ¯x′

,

¯Pt,¯x,α(d¯x′)

a.s.

−

where (¯Pt,¯x,α

¯x′

, ¯x′

∈

X) is a regular conditional probability distribution of ¯Pt,¯x,α given

τ .

G

41

Proof. We ﬁrst deﬁne the function ˆλα and ˆσα by

ˆλα(s, ¯x) = ˆλ(1 ¯xs, α(s, 2 ¯x))

and

ˆσα(s, ¯x) = ˆσ(1 ¯xs, α(s, 2 ¯x)) ,

(s, ¯x)

[0, T ]

X ,

∈ A

c (R2)

. Since C 2

×
Rm+1) admits a dense
Rd+1)
for α
countable subset, we can apply Theorem 6.1.3 of [26] to our framework and we get a negligible set
C 1,2
Rm+1),
N
N
×
t,α,Ff,g
the process ( ¯M
N . We notice that
s

× I ×
c (R2)
× I ×
)s∈[τ (¯x′),T ] is a G-martingale under ¯Pt,1 ¯x,α
¯x′

τ such that for any (F, f, g)

Rd+1)
×
for any ¯x′

c ([0, T ]
X

×
C 1,2

c ([0, T ]

c ([0, T ]

c ([0, T ]

× I ×

× I ×

C 1,2

C 1,2

∈ G

C 2

×

×

×

N

∈

∈

∈

\

and

¯Pt,1 ¯x,α
¯x′

¯x′′

∈

(cid:18)(cid:26)

¯Pt,1 ¯x,α
¯x′

¯x′′

(cid:18)(cid:26)

∈

X : ˆλατ (¯x′),2 ¯x′

(s, ¯x′′) = ˆλα(s, ¯x′′) for all s

X : ˆσατ (¯x′),2 ¯x′

(s, ¯x′′) = ˆσα(s, ¯x′′) for all s

∈

∈

[τ (¯x′), T ]

= 1

(cid:27)(cid:19)

[τ (¯x′), T ]

= 1

for any ¯x′

\

X

C 1,2
C 2
N . Therefore, for any (F, f, g)
∈
)s∈[τ (¯x′),T ] is a G-martingale under ¯Pt,1 ¯x,α
,Ff,g

∈
×
τ (¯x′),ατ (¯x′),2 ¯x′
Rm+1), the process ( ¯M
for any
s
N . By uniqueness to the shifted controlled martingale problem with initial condition

c ([0, T ]

c ([0, T ]

c (R2)

× I ×

C 1,2

×

×

¯x′

N
×
X
\

I ×
¯x′
∈
(τ (¯x′), ¯x′) and control ατ (¯x′),2 ¯x′

(cid:27)(cid:19)
Rd+1)

, we get

¯Pt,1 ¯x,α
¯x′

= Pτ (¯x′),¯x′,ατ (¯x′),2 ¯x′

for any ¯x′

X

\

∈

N .

Theorem A.7. Under Assumption A1, the martingale problem and the shifted martingale problem
admit unique solutions for any initial condition (t, ¯x)

X and any control α

[0, T ]

.

∈

×

∈ A

To prove Theorem A.7, we need to consider an extended process x deﬁned by

xs = (s, (xu∧s)) ,

[t, T ]

s

∈

The process x is valued in X = R
of function h : X
R of the form

×

→

X which is separable. We introduce the domain

as the set

D

h(s, ¯x) = H

f 1,g1(s, ¯xs∧t1), . . . , F p
F 1

(cid:16)
t1 <

for some p
≥
C 1,2
c ([0, T ]
× I ×
operator Lt,α by

1, 0

< tp
Rd+1) and g1, . . . , gp

· · ·

≤

T , H
C 1,2

c ([0, T ]

∈

≤
∈

,

(s, ¯x)

X ,

f p,gp(s, ¯xs∧tp)
(cid:17)

C 2(Rp), F 1, . . . , F p
N

∈
C 1,2
Rm+1). We then deﬁne on

c (R2), f 1, . . . , f p

∈

× I ×

×

∈
the

D

Lt,αh(s, ¯x) = Lt,α(s, ¯x)

·
Tr

DH(h1(s, ¯xs∧t1), . . . , hp(s, ¯xs∧tp))
Sα(Sα)⊤(s, ¯x)D2H(F 1
(cid:16)
1tj−1<s≤tj

γpk

Xi∈I
p

+

1
2

+

f 1,g1(s, ¯xs∧t1), . . . , F p

f p,gp(s, ¯xs∧tp))
(cid:17)

i∈I Xk≥0
X

j=1
X

H

(cid:18)

(cid:16)

f 1,g1(s, ¯xt1 ), . . . , F j−1
F 1

f j−1,gj−1(s, ¯xtj−1 ), Gi,kF j

f j ,gj (s, ¯xs), . . . , Gi,kF p

f p,gp(s, ¯xs)
(cid:17)

f 1,g1(s, ¯xs∧t1 ), . . . , F p
F 1
(cid:16)

f p,gp(s, ¯xs∧tp)

(cid:17) (cid:19)

H

−

42

with t0 = 0, where

Lt,α(s, ¯x) = 



1s≤t1Lt,α,1(s, ¯x)
...
1s≤t1Lt,α,p(s, ¯x)






with

and

with

Lt,α,q(s, ¯x) = ¯LαF q

f q,gq (s, ¯x)

n−1

γ

−

pn

F q

f q(s, 1 ¯xs) + 1Vs(i)
(cid:16)

Xi∈I,n∈N

n
gq(s, 2 ¯xs) +

1
22(|i|+n)

Xℓ=0
gq(i, n, s, bi

s, qi,n

s + 1)

(f q

iℓ −

f q
i )(s, ˆxi

s),

−

gq(i, n, s, bi

s, qi,n
s )

(cid:17)

−

F q(f q(s, 1 ¯xs), gq(s, 2 ¯xs))

(cid:0)

o
St,α,1(s, ¯x)
...
St,α,p(s, ¯x)






St,α(s, ¯x) = 



St,α,q(s, ¯x) =

1s≤tq

∂2F q(f q(s, 1 ¯xs), gq(s, 2 ¯xs))
(cid:16)

i∈I
X
+1i∈Vs∂1F q(f q(s, 1 ¯xs), gq(s, 2 ¯xs))Df q

Xn∈N
i (s, ˆxi

1
22(|i|+n)

∂bgq

i,n(s, bi

s, qi,n
s )

s)⊤ ˆσ(ˆxi

s, α(s, 2 ¯x)

for q = 1, . . . , p, (s, ¯x)

[0, T ]

×

∈

X and

Gi,nF j

f j ,gj (s, ¯x) = F

f (s, 1 ¯xs) + 1Vs(i)
(cid:16)

g(s, 2 ¯xs) +

n−1

(fiℓ

Xℓ=0
1
22(|i|+n)

fi)(s, ˆxi

s),

−

g(i, n, s, bi

s, qi,n

s + 1)

−

(cid:1)(cid:17)

g(i, n, s, bi

s, qi,n
s )

(cid:17)

[0, T ]

0. We then notice that for ¯Pt,1 ¯x,α (resp. ¯Pt,¯x,α) solution to the
for (s, ¯x)
martingale problem (resp. shifted martingale problem) with initial condition (t, 1 ¯x) (resp. (t, ¯x))
and control α the process

and n

X, i

∈ I

×

≥

∈

(cid:0)

h(xs)

s

−

t

Z

Lt,αh(xu)du ,

u

t

≤

≤

T ,

is a G-martingale under ¯Pt,1 ¯x,α (resp. ¯Pt,¯x,α)

Lemma A.3. Let (t, ¯x, α)
) two
solutions to the martingale problem (resp. shifted martingale problem) with initial condition (t, 1 ¯x)
(resp. (t, ¯x)) and control α. Then, ¯Pt,¯x,α

have the same one dimensional marginals:

× A
and ¯Pt,¯x,α
2

[0, T ]

X

×

∈

1

2

1

2

1

(resp. ¯Pt,¯x,α

and ¯Pt,¯x,α

and ¯Pt,1 ¯x,α

and ¯Pt,1 ¯x,α

¯Pt,1 ¯x,α
1
(resp. ¯Pt,¯x,α

1

(xs
(xs

∈

∈

B) = ¯Pt,1 ¯x,α
B) = ¯Pt,¯x,α

(xs
(xs

2

2

∈

B)

B))

∈

(A.87)

(A.88)

for s

∈

[t, T ] and B

(X).

∈ B

43

Proof. We ﬁrst endow the measurable space (X
¯Pt,1 ¯x,α
(resp. ¯P = ¯Pt,¯x,α
). For h
1

¯Pt,1 ¯x,α
2

¯Pt,¯x,α
2

⊗

T ) with the probability measure ¯P =

X,

×
∈ D

T
G
, we have

⊗ G

1 ⊗
¯P [h

E

h(xs, xt)] = E

⊗

¯P [h

⊗

h(xt, xs)]

Indeed, the processes

and

s

s

h(xs, xt)

h

⊗

−

t
Z

h(xt, xs)

h

⊗

−

t
Z

Lt,αh(xu)h(xt)du ,

s

t

≤

≤

T

h(xt)Lt,αh(xu)du ,

s

t

≤

≤

T

are both martingales under ¯P. Since all the considered functions are bounded, we can take the
expectation and we get

and

EP [h

⊗

h(xt, xs)] = EP [h

h(xs, xt)]

⊗

¯P

t,1 ¯x,α
1

E

[h(xs)] = E

¯P

t,1 ¯x,α
2

[h(xs)]

(resp. E

¯Pt,¯x,α
1

[h(xs)] = E

¯Pt,¯x,α
2

[h(xs)] ) .

Since any bounded
and ¯Pt,1 ¯x,α

(resp. ¯Pt,¯x,α

B
1

2

(X)-measurable function can be approximated almost everywhere for ¯Pt,1 ¯x,α

1

and ¯Pt,¯x,α
2

) by a sequence of

D

we get (A.87) (resp. (A.88)).

Proof of Theorem A.7. The proof is a direct consequence of Theorem 4.2 in [9] and Lemma A.3.

A.3 Proof of Theorem 2.1

. From Proposition 2.1, the
∈
P( ˆZ t,ˆµ,α, ξ) under P of ( ˆZ t,ˆµ,α, ξ) provides a solution to the controlled martingale problem
X such that 1 ¯xs = ˆµ for s
[0, T ], and control α given by

We keep the notations of Section A.2. Fix (t, ˆµ, α)
law
with initial condition (t, ¯x), where ¯x
Deﬁnition A.3. Therefore, we get from Theorem A.7

Ed+1 × A

[0, T ]

×

L

∈

∈

P( ˆZ t,ˆµ,α, ξ) = ¯Pt,1 ¯x,α

L

(A.89)

)) under P of
In the same way, for β
·
( ˆZ t,ˆµ,β, ξ(¯ω
)) is the unique solution to the shifted controlled martingale problem with initial
·
condition (t, ¯x) and control β given by Deﬁnition A.4. Therefore, we also get from Theorem A.7
that

Ω such that ξ(¯ω) = 2 ¯x, the law

P( ˆZ t,ˆµ,β, ξ(¯ω

∈ A

, ¯ω

⊕

⊕

L

∈

t

t

P( ˆZ t,ˆµ,β, ξ(¯ω

)) = ¯Pt,¯x,β .
·

t

⊕

L

(A.90)

F

τ -measurable random variable Y . From Doob’s functional representation Theorem
Fix now an
(see Lemma 1.13 in [16]) there exists a random time ˜τ : D([0, T ], ˜Em+1)
R+ that is a stopping
time with respect to the ﬁltration generated by the canonical process on D([0, T ], ˜Em+1), and a
measurable function gY : D([0, T ], ˜Em+1)

R such that

→

τ (ω) = ˜τ (ξ(ω))

and

ξ(ωτ (ω)∧.)

= gY (ξ(ω)) .

→
Y (ω) = gY

(cid:0)

(cid:1)

44

We then deﬁne ¯τ : X
observe that ¯τ is a G-stopping time and gY

R+ by ¯τ = ˜τ

→

◦

2x is

2x where we recall that 1x and 2x are given by (A.86). We
G¯τ -measurable. We therefore have from (A.89)
1x

◦
¯Pt,1 ¯x,α

gY (2x)

f

¯P

t,1 ¯x,α
(cid:2)
(cid:0)
¯x′

E

(cid:1)
1x

f

(cid:3)

gY (2x(¯x′))d¯Pt,1 ¯x,α(¯x′) .

X

E

f

h

ˆZ t,ˆµ,α
(cid:16)

Y

(cid:17)

i

= E

=

Z

where (¯Pt,¯x,α
Using Theorem A.6 we ﬁnally get

, ¯x′

∈

¯x′

(cid:1)(cid:3)
X) stands for a regular conditional probability distribution of ¯Pt,¯x,α given

(cid:0)

(cid:2)

G¯τ .

E

f

h

ˆZ t,ˆµ,α
(cid:16)

Y

(cid:17)

i

=

=

X

Z

X

Z

EP¯τ (¯x′),¯x′,α¯τ (¯x′),2 ¯x′

f

1x

gY (2x(¯x′))d¯Pt,1 ¯x,α(¯x′)

F

(cid:2)

(cid:0)
¯τ (¯x′), ¯x′, α¯τ (¯x′),2 ¯x′
(cid:16)

(cid:17)

(cid:1)(cid:3)
gY (2x(¯x′))d¯Pt,1 ¯x,α(¯x′) .

Then, using (A.90), we get

E

f

h

ˆZ t,ˆµ,α
(cid:16)

Y

=

F

(cid:17)

i

ZΩ

.∧τ (ω), ατ (ω),ω

τ (ω), ˆZ t,ˆµ,α
(cid:16)

(cid:17)

Y (ω)dP(ω) .

References

[1] D. P. Bertsekas and S. E. Shreve. Stochastic optimal control: the discrete time case. Opti-

mization and Neural Computation Series. Athena Scientiﬁc, 2007.

[2] B. Bouchard. Stochastic targets with mixed diﬀusion processes and viscosity solutions.

Stochastic Processes and their Applications, 101(2):273–302, 2002.

[3] B. Bouchard, B. Djehiche, and I. Kharroubi. Quenched mass transport of particles towards a

target. Journal of Optimization Theory and Applications, 186 (2):365–374, 2020.

[4] B. Bouchard, R. Elie, and C. Imbert. Optimal Control under Stochastic Target Constraints.

SIAM Journal on Control and Optimization, 48(5):3501–3531, 2010.

[5] J. Claisse. Optimal control of branching diﬀusion processes: A ﬁnite horizon problem. The

Annals of Applied Probability, 28 (1):1–34, 2018.

[6] J. Claisse, D. Talay, and X. Tan. A pseudo-markov property for controlled diﬀusion processes.

SIAM Journal on Control and Optimization, 54 (2):1017–1029, 2016.

[7] M. G. Crandall, H. Ishii, and P.-L. Lions. User’s guide to viscosity solutions of second order

partial diﬀerential equations. Bulletin of the American Mathematical Society, 1992.

[8] D. A. Dawson. Measure-valued markov processes. In Ecole d’´et´e de probabilit´es de Saint-Flour

XXI, number 1541 in Lecture Notes in Math., pages 1–260. Springer Berlin, 1993.

[9] S. N. Ethier and T. G. Kurtz. Markov Processes, Characterization and Convergence. John

Willey & Sons, 1986.

[10] S. Fahad. Blockchain without Waste: Proof-of-Stake. The Review of Financial Studies,

34(3):1156–1190, 2021.

45

[11] W. H. Fleming and H. M. Soner. Controlled Markov Processes and Viscosity Solutions, vol-
ume 25 of Stochastic Modelling and Applied Probability. Springer Berlin, second edition edition,
2006.

[12] P. Henry-Labord`ere, X. Tan, and N. Touzi. A numerical algorithm for a class of bsdes via the

branching process. Stochastic Process. Appl., 124:1112–1140, 2014.

[13] N. Ikeda, M. Nagasawa, and S. Watanabe. Branching Markov processes I. I. J. Math. Kyoto

Univ., 8:233–278, 1968.

[14] N. Ikeda, M. Nagasawa, and S. Watanabe. Branching Markov processes II. I. J. Math. Kyoto

Univ., pages 365–410, 1968.

[15] N. Ikeda, M. Nagasawa, and S. Watanabe. Branching Markov processes III. I. J. Math. Kyoto

Univ., 9:95–160, 1969.

[16] O. Kallenberg. Foundation of Modern Probability. Probability and its Applications. Springer-

Verlag New York, second edition edition, 2002.

[17] O. Kallenberg. Random Measures, Theory and Applications, volume 77 of Probability Theory

and Stochastic Modelling. Springer International Publishing Switzerland, 2017.

[18] N. V. Krylov. Controlled Diﬀusion Processes. Number 14 in Application of Mathematics.

Springer-Verlag, 1980.

[19] N. V. Krylov. Nonlinear Elliptic and Parabolic Equations of the Second Order, volume 7 of

Mathematics and Its Applications (Soviet Series). Reidel Dordrecht, 1987.

[20] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash system.

, 2008.

[21] M. Nisio. Stochastic control related to branching diﬀusion processes. J. Math. Kyoto Univ.,

25:549–575, 1985.

[22] B. Oksendal and A. Sulem. Applied Stochastic Control for Jump Diﬀusions, Second Edition.

Universitext. Springer-Verlag Berlin Heidelberg, 2007.

[23] A. V. Skorohod. Branching diﬀusion processes. Teor. Veroyatnost. i Primenen., 9(3):492–497,

1964.

[24] H. Mete Soner and Nizar Touzi. Dynamic programming for stochastic target problems and

geometric ﬂows. Journal of the European Mathematical Society, 4:201–236, 09 2002.

[25] H. Mete Soner and Nizar Touzi. Stochastic Target Problems, Dynamic Programming, and
Viscosity Solutions . SIAM Journal on Control and Optimization, 41(2):404–424, 2002.

[26] D. Stroock and S.R.S. Varadhan. Multidimensional Diﬀusion Processes. Reprint of the 1997

Edition, Classics in Mathematics. Springer, 1997.

[27] S. Ustunel. Construction of branching diﬀusion processes and their optimal stochastic control.

Appl. Math. Optim., 7:11–33, 1981.

46

