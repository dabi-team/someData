Artiﬁcial Intelligence for the Metaverse: A Survey

Thien Huynh-The, Member, IEEE, Quoc-Viet Pham, Member, IEEE, Xuan-Qui Pham,
Thanh Thi Nguyen, Zhu Han, Fellow, IEEE, and Dong-Seong Kim, Senior, IEEE

1

2
2
0
2

b
e
F
5
1

]

Y
C
.
s
c
[

1
v
6
3
3
0
1
.
2
0
2
2
:
v
i
X
r
a

Abstract—Along with the massive growth of

the Internet
from the 1990s until now, various innovative technologies have
been created to bring users breathtaking experiences with more
virtual interactions in cyberspace. Many virtual environments
with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive
experience and digital transformation, but most are incoherent
instead of being integrated into a platform. In this context,
metaverse, a term formed by combining meta and universe, has
been introduced as a shared virtual world that is fueled by many
emerging technologies, such as ﬁfth-generation networks and be-
yond, virtual reality, and artiﬁcial intelligence (AI). Among such
technologies, AI has shown the great importance of processing
big data to enhance immersive experience and enable human-
like intelligence of virtual agents. In this survey, we make a
beneﬁcial effort to explore the role of AI in the foundation and
development of the metaverse. We ﬁrst deliver a preliminary
of AI, including machine learning algorithms and deep learning
architectures, and its role in the metaverse. We then convey a
comprehensive investigation of AI-based methods concerning six
technical aspects that have potentials for the metaverse: natural
language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the
metaverse. Subsequently, several AI-aided applications, such as
healthcare, manufacturing, smart cities, and gaming, are studied
to be deployed in the virtual worlds. Finally, we conclude the
key contribution of this survey and open some future research
directions in AI for the metaverse.

Index Terms—3D virtual world, artiﬁcial

intelligence, deep

learning, machine learning, metaverse.

I. INTRODUCTION

S INCE Facebook rebranded itself as Meta, announced by

Mark Zuckerberg in October 2021, the marvelous concept
regarding the new name has become a hot trend on social
media and received huge attention and much more discussions
by various communities,
including academia and industry.
Besides Meta, some big tech companies have some metaverse
investment and development activities, such as Microsoft
bought Activision Blizzard, a video game holding company,
for $68.7 billion as the deal of gaming expansion into the
metaverse. Recently, Metaverse Group, a metaverse real estate
investment company bought a parcel of land on a decentralized
virtual reality platform known as Decentraland for a shocking

Thien Huynh-Them, Xuan-Qui Pham, and Dong-Seong Kim are with
the Department of IT Convergence, Kumoh National Institute of Tech-
nology, Gumi, Gyeongsangbuk-do 39177, Republic of Korea (email:
thienht@kumoh.ac.kr, pxuanqui@kumoh.ac.kr, dskim@kumoh.ac.kr).

Quoc-Viet Pham is with the Korean Southeast Center for the 4th Indus-
trial Revolution Leader Education, Pusan National University, Busan 46241,
Republic of Korea (email: vietpq@pusan.ac.kr).

Thanh Thi Nguyen is with the School of

Information Technol-
ogy, Deakin University, Waurn Ponds, VIC 3216, Australia (email:
thanh.nguyen@deakin.edu.au).

Zhu Han is with the Department of Electrical and Computer Engineering,

University of Houston, Houston, TX 77004 USA (e-mail: zhan2@uh.edu).

Fig. 1. Gaming revenue growth aided by 3D virtual worlds.

price $2.43 million, and recorded as the highest ever amount
for a virtual real estate. A famous rapper who bought a plot
of land in the Sandbox metaverse for $450, 000 is Snoop
Dogg, in which this rapper can hold virtual events like music
festivals and concerts to bring an immersive experience to
the audience participating in the virtual world via the virtual
reality technology. In the near future, the metaverse is realized
as the next big technology and currently attracting online
game makers, internet ﬁnance businesses, social networks,
and other technology leaders. The Seoul metropolitan govern-
ment just very recently announced a plan called Metaverse
Seoul
that creates a virtual communication ecosystem for
all municipal administrative areas, such as culture, tourism,
economic, educational, and civic service. Besides providing
different business support services and facilities, the Metaverse
Seoul will offer some specialized services for people with
disabilities to take pleasure in safety and convenience contents
using extended reality (XR) technology. Based on the analysis
of Bloomberg Intelligence [1], the global metaverse revenue
opportunity will increase from USD 500 billion in 2020 to
USD 800 billion in 2024, in which the online game industry
will take half of the global revenue. Remarkably, the video
game companies and studios have some plans to upgrade
existing traditional games to three-dimensional (3D) virtual
world convolving social networks, in which some attractive
activities, such as live entertainment and media advertising
events, can be held besides gaming. In Fig. 1, the revenue
of virtual reality (VR) hardware and in-game advertisement
signiﬁcantly increases through the advancement of virtual
activities in the metaverse.

The metaverse is not a new idea because it has circulated
along with the development of the Internet and other tech-
nologies for decades. Fig. 2 describes the timeline of the
metaverse development that involves many primary events,
from the birth of the Internet and the ﬁrst mention in liter-
ature to the ﬁrst virtual world project with Second Life and

Software &service$183.3BAR/VR device$4.7BIn-game ads$31.8BGamehardware$63.5B20202024Software &service$250.5BIn-game ads$53.7BGamehardware$77.8BAR/VR device$40.6B 
 
 
 
 
 
2

Fig. 2. A timeline of the metaverse development involving primary events from 1991 to 2021.

recent metaverse projects of big tech companies like Microsoft
and Facebook. Metaverse is the term formed by combining
Meta and Universe [2], which may be ﬁrst mentioned in
the dystopian cyberpunk novel Snow Crash in 1992 to de-
scribe a virtual reality world called the matrix. At present,
the metaverse is deﬁned as a shared virtual 3D world or
even multiple cross-platform worlds that can provide users
a comprehensively immersive experience with interactive and
collaborative activities. Besides virtual places and construc-
tions ﬁxed in the virtual world, many other entities, such as
objects, user identities, and digital goods, can be exchanged
between different virtual worlds and even reﬂected into the
reality world [3]. Few recent years have witnessed an un-
precedented explosion of the metaverse, mostly derived from
3D gaming, which is fueled by the improvement of hardware
(e.g., big data storage infrastructure, wireless communication
networks, built-in sensors, and graphic processing unit - GPU)
and the optimization of software (e.g., resource allocation in
communications, language processing, and computer vision) to
build the virtual world more solidly and creatively. Different
from the traditional metaverse modality that limits immersive
experience poorly by insufﬁcient data, the new one not only
generates a huge new source of user and behavioral data
for enterprises (where users freely make creative content) but
also presents a plentiful foundation to deploy artiﬁcial intel-
ligence (AI) into various domains, such as natural language
processing, computer vision, and neural interface. Besides,
a standard platform built for a modern metaverse should
satisfy the following characteristics: virtual world, persistency,
scalability, always-on with synchronicity, ﬁnancial allowance,
decentralization, security, and interoperability. In [4], a meta-
verse platform can include several layers (see Fig. 3) which
are expressed as follows:

• Infrastructure: 5G, 6G, WiFi, cloud, data center, central

processing units, and GPUs.
• Human interface: mobile,

smartglasses,
wearable devices, head-mounted display, gestures, voice,

smartwatch,

Fig. 3. Seven layers of a metaverse platform.

and electrode bundle.

• Decentralization: edge computing, AI agents, blockchain,

and microservices.

• Spatial computing: 3D engines, VR, augmented reality

(AR), XR, geospatial mapping, and multitasking.

• Creator economy: design tools, asset markets, E-

commerce, and workﬂow.

• Discovery: advertising networks, virtual stores, social

curation, ratings, avatar, and chatbot.

• Experience: games, social, E-sports, shopping, festivals,

events, learning, and working.
is not hard to ﬁnd out

It

the presence of AI inside
layers, with machine learning (ML) algorithms and deep
learning (DL) architectures, along with their importance in
many diversiﬁed aspects. For instance, many ML algorithms
with supervised and unsupervised learning were applied in
classiﬁcation and regression models for voice recognition and
other language processing tasks that enable system agents to
understand user commands. With the input data of sensor-
based signals collected by multiple devices, such as mobile,

1991Birth of theInternet1992Term "metaverse"in Snow CrashProof of workB-Money19931998201720162002201520062011201520212016DigitalTwinsOnline virtualworld20092003BitcoinReadyplayer oneCreativegrame2009BlockchaintechnologyVR in novel20122014NFT - non-fungible tokenOculus - VRhardwareEthereumnetworkDecentralandPokemon GODAODecentralizedAutonomousOrganizationMultiplayer gameand social hub2018Virtual game withtraining andtrading creatures2021MicrosoftMeshMeta Platform(formerly knownas Facebook)HumanInterfaceDecentralizationSpatialComputingCreatorEconomyDiscoveryExperienceInfrastructuresmartwatch, and other wearable devices, the complex patterns
of human actions can be analyzed and learned for some
applications like physical activity recognition that allows a
system to perceive user activities and interactions in the virtual
world. Recently, DL has emerged as a powerful AI tool to
deal with the practical issue of understanding complicated
patterns from large-messy-confusing data. With considerable
success in the computer vision domain, DL is now being
leveraged in different domains, such as wireless communi-
cations, human-computer interaction, gaming, and ﬁnance.
A few years ago, NVIDIA introduced DL super sampling
(DLSS), a groundbreaking technology that exploits the power
of DL and other AI algorithms to boost the frame rate while
maintaining beautiful and sharp in-game images, thus being
potential to improve the visual experience in the metaverse. AI
was also leveraged to improve game balance in several online
multiplayer games by training supervised learning models
iteratively until satisfying designers and play-testers. To dive
into a new era of 3D design simulation and collaboration for
creating an impressive virtual-reality world, in the metaverse,
as rich as the real world, NVIDIA introduced Omniverse1, an
open-and-extensible platform, owning many valuable features,
including physically-accurate simulation, multi-user design
collaboration, photorealistic and real-time rendering, and AI-
accelerated workﬂows.

A. Our Contributions

The metaverse platform is built by merging many advanced
technologies to bring a completely 3D immersive experience
to users, where they can truly interact and collaborate with
others in the virtual worlds. Among such technologies like
blockchain, XR/VR, and 5G, AI has a silent but important
role in the foundation and development of the metaverse.
However, understanding how AI can affect and contribute
to the metaverse in the technical and application aspects
is dubious, especially in the context of which it is neither
mentioned in a fancy way like XR/VR nor discussed gloriously
on social media like blockchain. No existing work is done to
provide a comprehensive review of the role and use of AI in
the metaverse.

In the paper, we convey a comprehensive survey of the
existing AI-based works in the technical and application
perspectives and further discuss their potentials for the meta-
verse. In a nutshell, the main contributions of this paper are
summarized as follows.

• We brieﬂy review AI techniques, including conventional
ML algorithms and innovative DL architectures, with
various learning strategies like supervised learning, unsu-
pervised learning, and reinforcement learning. Based on
that, the role of AI in the metaverse is initially revealed.
• We survey the state-of-the-art AI-powered approaches in
six technical aspects, including natural language process-
ing, machine vision, blockchain, networking, digital twin
(DT), and neural interface, which show great potential
for the metaverse platform.

1https://www.nvidia.com/en-us/omniverse/

3

• We investigate the existing AI-aided methods relying on
several application aspects, such as healthcare, gaming,
manufacturing, smart cities, E-commerce, real estate, and
decentralized ﬁnance, which receive more interest to be
deployed in the virtual world.

• We introduce several interesting metaverse projects which
have applied AI to enhance the immersive experience and
develop user-oriented services. Moreover, some future
research directions in AI for the metaverse are discussed.

B. Paper Organization

this paper

The remaining of

is organized as follows.
The fundamentals of blockchain, metaverse, and the role of
blockchain in the metaverse are presented in Section II. The
adoptions of AI for the metaverse in technical aspects, such
as natural language processing, machine vision, blockchain,
networking, DT, and neural interface, are investigated in Sec-
tion III. In Section IV, we discuss several AI-powered applica-
tions to be promisingly developed in the metaverse, including
healthcare, manufacturing, smart cities, and gaming, besides
other minor areas as E-commerce, human resources, real
estate, and decentralized ﬁnance. Some remarkable metaverse
projects are presented in Section V. Finally, we conclude the
paper with some future research directions for the development
of the metaverse in Section VI.

II. AI FOR THE METAVERSE: PRELIMINARIES

This section brieﬂy conveys a wide spectrum of AI, from
traditional ML algorithms to advanced DL networks that
embrace different learning mechanisms, and then articulates
the role of AI in the metaverse.

A. Categorization of AI

This part reviews some common AI/ML algorithms that
are potential for the metaverse. Fundamentally, most of the
existing AI/ML algorithms can be categorized into two sectors:
conventional techniques and advanced techniques, which are
studied for three principal problems: clustering, classiﬁcation,
and regression.

1) Conventional Techniques: Conventional AI/ML algo-
rithms can be grouped based on the kinds of data available for
the learning model: supervised learning, unsupervised learn-
ing, semi-supervised learning, and reinforcement learning.

Supervised Learning: The ML algorithms of this learning
approach learn the relation between input and output via a
mapping function using labeled data. Each input sample in
a training dataset is tagged with the answer (a.k.a., label),
which allows the trained model
to classify or predict the
outcome for an unforeseen input sample [5]. Supervised
learning algorithms are usually used to handle classiﬁcation
problems (assign a sample in the test set into a discrete class)
and regression problems (express the relationship between
dependent and independent variables in continuous data).
Some regular supervised learning algorithms are decision tree,
random forest, Naive Bayes, k-nearest neighbor, and support
vector machine (SVM).

Unsupervised Learning: Unsupervised learning involves the
utilization of AI/ML algorithms for unlabeled data analysis
and clustering. These algorithms cannot be applied to classi-
ﬁcation and regression problems directly, but they are capable
of modeling hidden patterns and ﬁnding out data groups
without the need for human intervention. Unsupervised learn-
ing algorithms (for example, hierarchical clustering, k-means
clustering, principal component analysis, and association rule)
can be used for some common tasks of data mining, such as
clustering, association, and dimensionality reduction [6].

Semi-supervised Learning: Semi-supervised learning is in-
troduced to partly counter the disadvantages of supervised
learning (e.g., expensive cost for labeling data by scientists
and ML engineers) and unsupervised learning (e.g., the lim-
itation of application spectrum). In semi-supervised learning,
an AI model is trained upon the combination of labeled and
unlabeled data. A basic procedure of this type of learning
involves two steps: clustering similar data using an unsuper-
vised learning algorithm and then using the existing labeled
data to label the remaining unlabeled data [7]. Some well-
known algorithms for semi-supervised learning are graph-
based model, generative model, boosting, and self-training [8].
Reinforcement Learning Reinforcement learning (RL) is a
group of ML algorithms for making a sequence of decisions,
in which an agent learns to attain a goal in an uncertain
and complex environment [9]. An AI machine should come
through trial and error to reach a nearly optimal solution for
a game-like scenario. The object of an RL model is how to
perform the task to maximize the reward and minimize the
penalty, beginning with totally random trials and ending with
sophisticated tactics and superhuman skills [10]. By exploiting
the power of the searching scheme with many trials, RL is one
of the most effective ways to imply machine creativity.

2) Advanced Techniques: DL, a subset of AI and ML
that develops multi-layered artiﬁcial neural networks to attain
state-of-the-art accuracy in many classiﬁcation and regression
tasks, has been exploited for various applications in multiple
domains [11]–[13]. Unlike traditional ML techniques, DL can
automatically learn underlying features of unstructured data
without human intervention or human domain knowledge. The
highly ﬂexible architectures of DL allow learning systems to
process raw data directly and improve learning performance
when the data is provided enough. Here we discover some
including recurrent neural
well-known deep architectures,
network (RNN), convolutional neural network (CNN), self-
organizing map (SOM), and autoencoders.

Recurrent neural network: RNN is one of the founda-
tional neural network architectures from which various deep
architectures, such as long short-term memory (LSTM) and
gated recurrent unit (GRU) networks, are developed with some
structural
improvements. Besides feed-forward connections
in regular multilayer networks, an RNN has some feedback
connections associated with the preceding layers. The com-
puting ﬂow derived by the feedback connections allows RNNs
to maintain memory of past inputs and process models in
time [14]. RNNs can be unfolded in time and trained with
back-propagation mechanisms.

Convolutional neural network As one of the most suc-

4

cessful deep network architectures, CNN leverages princi-
ples from linear algebra (especially matrix multiplication) to
identify complex patterns from high-dimensional unstructured
data [15]. Early layers compute features from coarse to ﬁne
in a regular CNN, and later layers recombine these features
into higher-level representations. CNNs are distinguished from
other deep network architectures by their superior performance
with different data types, including images, videos, audio sig-
nals, and communication signals. There are three main layers
in a CNN: convolutional layers for feature extraction, pool-
ing layers for dimensionality reduction, and fully-connected
layers for classiﬁcation. Several standard architectures were
introduced to solve various challenging tasks in the computer
vision domain: AlexNet, VGG, GoogleNet, ResNet, DenseNet,
Inception, and EfﬁcientNet.

Self-organizing map: SOM is an unsupervised neural net-
work to ﬁnd clusters of the input data points by reducing its
dimensionality [16]. In common SOM architectures, weights
serve as a characteristic of the node. At the beginning, the
inputs are normalized and then randomly chosen to feed the
network. Random weights close to zero are associated with
each feature of the input record, which represents the input
node. The node with the least Euclidean distance (between
each of the output nodes and the input node) is recognized
as the most accurate input representation and denoted as the
best matching unit (BMU). By establishing these BMUs as
centroid, other units are calculated similarly and assigned to
the shortest distance cluster.

Autoencoder: An autoencoder is a special type of neural
networks, which is trained with the compression and decom-
pression functions to map its input to output [17]. In an au-
toencoder network, the input layer is encoded into the hidden
layer using an encoding function for compression, where the
number of hidden nodes is much less than the number of
input nodes. Accordingly, this hidden layer contains the com-
pressed representation of the original input. The output layer
aims to reconstruct a decoding function for decompression
input information. The difference between the input and the
reconstructed output in the training phase is calculated using
an error function. Since autoencoders can learn continuously
with backward propagation, they are usually applied for self-
supervised learning tasks [18].

B. Role of AI in the Metaverse

By merging AI with other technologies, such as AR/VR,
blockchain, and networking, the metaverse can create secure,
scalable, and realistic virtual worlds on a reliable and always-
on platform. According to the seven-layer metaverse platform,
it is undoubted to realize the important role of AI to guarantee
the reliability of infrastructure and improve its performance
so far. In the 5G and future 6G systems, many advanced ML
algorithms with supervised learning and reinforcement learn-
ing have been adopted for different challenging tasks, such as
efﬁcient spectrum monitoring, automatic resource allocation,
channel estimation, trafﬁc off-loading, attack prevention, and
network fault detection. With sensor-based wearable devices
and other human-machine interaction gadgets, simple human

5

Fig. 4. Primary technical aspects in the metaverse, in which AI with ML algorithms and DL architectures is advancing the user experience in the virtual
world.

movements and complex actions can be analyzed and rec-
ognized based on learning ML and DL models. Therefore,
users’ movements in the real world are projected into the
virtual worlds, allowing users to fully control their avatars
to interact with other objects in the metaverse comfortably.
Moreover, these avatars can engage with many modalities
adopted in the real world, such as facial expressions, emotions,
interactions, besides speech
body movement, and physical
recognition and sentiment analysis, which are powered by AI
in terms of accuracy and processing speed.

Although XR/VR someway represents the facade of a
metaverse with immersive devices like head-mounted displays,
AI is a pivotal
technology working behind the scenes to
build a creative and beautiful world, thus bringing a seam-
less virtual-reality experience to users. AI can facilitate the
content creation process, for example, some AI modules like
GANverse3D introduced by NVIDIA enable developers and
creators to take photos of objects and then make virtual
replicas. Several DL-based methods have been proposed for
rendering 3D objects (including human body parts), which
can achieve very impressive accuracy while presenting real-
time processing accelerated by both software (e.g., PyTorch3D
library from Facebook AI and TensorRT from NVIDIA) and
hardware (e.g., GPUs) Meta just very recently introduced the
AI research supercluster (RSC) [19], believed as among the
world-class fastest AI supercomputer that will speed up AI
research and be served for building the metaverse. Further-
more, RSC can help AI researchers and scientists develop
better DL models from massive data, including text, speech,
image, video, for various services/applications. Accordingly,
any achievements and outcomes derived from RSC will be
used as fabrics to build the metaverse platform, in which AI-
driven products will be of considerable importance.

III. AI FOR THE METAVERSE: TECHNICAL ASPECT

This section investigates the state-of-the-art AI-based meth-
language processing,
ods in six technical aspects: natural
machine vision, blockchain, networking, DTs, and neural
interface; which present the potential for the metaverse as
shown in Fig. 4. Accordingly, the experience of users in the
metaverse is enhanced signiﬁcantly with nearly no boundary
between the virtual world and the real world.

A. Natural Language Processing

Natural language processing (NPL), also known as com-
putational linguistics, encompasses a variety of computational
models and learning processes to solve practical problems of
automatically analyzing and understanding human languages,
including speech and text. Besides, the ﬁeld of NLS considers
many topics, such as speech-to-text, text-to-speech, conver-
sation design, voice branding, and multi-language and multi-
cultural in voice. Furthermore, NLP plays a vital role in the
metaverse regarding intelligent virtual assistants (a.k.a., chat-
bot). Particularly, NLP is principally responsible for enabling
chatbots to understand complicated human conversation in
the context of varying dialects and undertones. Empowered
by AI, chatbots can answer nuanced questions and learn
from interaction to improve the quality of responses. The
AI chatbots are developed to assist users in some virtual
environments like the metaverse.

tasks in NLP,

As one of the most

language
important
modeling predicts words or simple linguistic units by cap-
turing syntactic and semantic relations of preceding words
and units, which is useful for machine translation and text
recommendation. In [20], many neural networks with key-
value attention mechanisms were built and evaluated on the
Wikipedia corpus dataset to conclude that RNNs and LSTM
networks with the attention mechanisms can outperforms
large-scale networks while reducing memory in use. In [21],
a memory network with residual connection was designed to
improve the performance of language modeling in terms of
test perplexity if compared with regular LSTM [22] having
an equivalent size. Some recent CNNs have been leveraged
to address the long-term dependencies in long sentences and
short paragraphs, especially being efﬁcient to speciﬁc and
complicated word patterns [23]. Some deep networks were de-
signed with advanced modules and connection structures to en-
hance language modeling efﬁciency, such as gated connection
and bi-directional structure [24]. Besides word-aware language
models, many character-aware models have been introduced
with AI algorithms to deal with various diversiﬁed languages
in the world. Both the CNN and LSTM architectures [25]
were applied to analyze the representation of words from
characters as the input. Some models were evaluated on many
datasets of English, German, Spanish, French, and Arabic, in

Metaverse'sTechniquesLanguage modelingWord predictionText-to-speech processingSemantic labelingObject detection and segmentationImage restoration and enhancementPose estimation and action recognitionVirtual realityAugmented realityMixed realityNLPMachineVisionBlockchainData collection and sharingData storage and managementData security and privacyUltra-reliable and low-latency communicationsMulti-access edge computingIntelligent spectrum utilizationData-driven modelingPhysical-digital view integrationAnalysis-monitoring-prediction-simulationBrain-computer interfaceInvasive and non-invasive signalsMental state analysisNetworkingDigitalTwinNeuralInterface6

Fig. 5. The difference between AR, MR, and VR under the umbrella of XR.

which they showed the effectiveness in identifying preﬁxes
and sufﬁxes, recognizing hyphenated words, and detecting
misspelled words [26]–[28]. Generally speaking, character-
aware and word-aware modeling techniques allow natural
language understanding systems to extract syntactic and se-
mantic information for some common tasks in the metaverse,
such as part-of-speech tagging, named-entity recognition, and
semantic role labeling.

DL has been further exploited to overcome the learning
limitation of conventional ML algorithms and effectively deal
with many challenging tasks in NLP. Some CNNs with sample
and advanced architecture in [29] were leveraged to cope
with multiple sentence-based tasks, such as sentiment pre-
diction and question type classiﬁcation. Moreover, sentiment
analysis and recognition tasks may require feature extraction
of aspects and sentiment polarities [30], which are potential
to improve the reliability and ﬂexibility of virtual assistant
units in the metaverse. Natural
language generation is an
advanced functionality of chatbot to generate reasonable task-
speciﬁc conversation-oriented text. Some single RNN/LSTM
and mixture LSTM-CNN models were proposed to generate
short text in image captioning and long text in virtual question
answer [31]. Besides supervised learning, unsupervised and
reinforcement learning with deep models have been adopted
for some speciﬁc NLP tasks, such as text parsing, seman-
tic labeling, context retrieval,
language interpretation, and
dialogue generation [32]. In the metaverse, NLP techniques
should be combined to fully provide text-based and speech-
based interactive experiences between human users and virtual
assistant.

B. Machine Vision

Machine vision,

including computer vision and XR in
cooperation, is one of the central technologies to obtain the
foundation of the metaverse. The raw data perceived from
visual environments (via optical display and video player)
information,
is captured and processed to infer high-level
which is then shown to users over head-mounted devices
and others, such as smart glasses and smartphones. Indeed,
computer vision allows XR devices to analyze and understand
user activities based on visual-based meaningful information.
Represented as avatars in the virtual worlds, the users can

freely move in 3D maps and interact with virtual objects in
the metaverse.

1) Extended Reality: XR is deﬁned as an umbrella term that
encapsulates VR, AR, mixed reality (MR), and everything in
their gaps as shown in Fig. 5. Although some revolutionary
experiences are offered for VR and AR, the same original
technologies are fueling the innovation and development of
MR. While AR provides the experiences of graphics, video
streams, and holograms in the physical world and VR offers
viewing experiences in a fully immersive digital world, MR
can deliver a transition experience between AR and VR. Along
with these reality technologies, human users can experience
the metaverse and enjoy diversiﬁed services in both the
physical and digital worlds. While XR and AI are distinct
sectors, they can be combined to reach a fully immersive in
the metaverse.

While conventional two-dimensional (2D) videos are lim-
ited by the small ﬁeld of view (FoV), the 360-degree videos
providing unlimited view-point with all directions are suitable
for VR performance. Many commercial VR headsets are
designed to satisfy the high-class using requirements, such as
performance and comfortableness, which encompass multiple
tasks driven by AI automatically. With the VR headset, a
human user can experience various services and applications in
the metaverse, and further create hyperreal media contents in
the virtual world. Some AI algorithms have been applied in VR
devices to improve the human-machine interaction experience
based on visual-based information. For the prediction of
user’s eye ﬁxations in some gaze-based applications, such as
content design and rendering, a DL framework with multiple
CNNs [33] was built to deal with various kinds of input
data, e.g., VR image, gaze data, and head data. This model
effectively exploited the correlations between eye ﬁxations and
other factors likes VR content and headset motion. In [34],
neural networks were adopted for human identiﬁcation and
authentication by analyzing periodic behaviors between users
and VR gears (e.g., controllers and head-mounted display).
information
This work’s effectiveness in delivering useful
and treatment recommendation was veriﬁed in collaboration
and gaming scenarios and has shown some applicable po-
tentials in other scenarios, such as working and shopping.
To enhance users’ quality of experience (QoE) in the virtual
world, an innovative human-machine interface approach [35]

Augmented RealityNon-environment aware2D/3D content is overlaidonto the physical spaceMixed RealityEnvironment aware2D/3D content is overlaidonto the physical spaceUser is completelyimmersed into a virtualworldVirtual RealityExtended RealityEntire experiencespectrum from fullyvirtual to fully realwas proposed by incorporating triboelectric sensory gloves
and display components in VR devices to recognize multi-
dimensional motion of gestures. As a result, virtual objects
which are recognized by leveraging ML/DL algorithms can
be manipulated in the real-time VR/AR space. To access the
contents in the metaverse and interact with virtual objects in
the digital world, not only AR headsets but also other devices
(e.g., triboelectric gloves, hand-held touchscreen devices, and
tabletops) [36] are taken into consideration regarding speciﬁc
applications, services, and infrastructures.

To satisfy the service’s demands about high-resolution
video viewing experiences with VR devices, it is necessary
to develop an effective video quality assessment method,
in which DL has represented as a powerful tool to obtain
quantitative and qualitative benchmark objectives. In [37],
a high-performance method was developed for VR quality
assessment by building a 3D CNN architecture,
in which
the video prediction results are validated via some common
image quality assessment metrics without video reference.
Compared with the baseline, which performed handcrafted
feature extraction and ML algorithms, the 3D CNN-based
approach showed the superiority in term of VR video qual-
ity assessment and benchmark [38]. Quality assessment was
extended for 2D and 3D foveated-compressed videos in [39],
which allows VR systems to effectively handle the limited
data transmission bandwidth. The advantages and limitations
of current video quality assessment methods were analyzed
and exposed in [40], which can be useful to design an effective
video transmission mechanism for various AR systems and
diversiﬁed video contents. On the way to become the next
mainstream for consumers and business, MR is deﬁned as a
blend of physical and digital worlds, which establishes the
natural and intuitive interactions between 3D human, com-
puter, and surrounding environment [41]. This new reality is
activated by the recent revolutions in computer vision, graphic
processing, display, remote sensing, and AI technologies [42].
Compared with VR and AR, MR has more potentials for
the metaverse thanks to its hybrid physical-virtual experiences
via two main types of devices2: holographic device with see-
through display allows users to manipulate physical objects
while wearing it and immersive device allows users to interact
with virtual objects in the virtual world. In the future, new
devices to enhance the visual-interactive experiences of users
in the metaverse should minimize the different gaps in terms
of speciﬁcation and utility between holographic devices and
immersive devices.

2) Computer vision: In the last decades, computer vision
has been empowered by AI, especially DL with a variety
of network architectures to improve the overall accuracy of
visual systems with efﬁcient cost thanks to high-performance
graphic processing units. Some fundamental computer vision
technologies are potential to enhance the experience of human
users in the metaverse, thus enabling users in the physical
world to interact with the virtual environment in the digital
world smoothly.

2https://docs.microsoft.com/en-us/windows/mixed-reality/discover/mixed-

reality

7

Semantic segmentation and object detection are two funda-
mental tasks in the computer vision domain, where semantic
segmentation categorizes each pixel in an image to be one of
pre-deﬁned semantic classes [43] and object detection aims to
localize all possible objects in an input image by drawing
corresponding bounding boxes with object
information in
tag [44]. Early segmentation works mostly adopted local fea-
ture extraction and tracking in cooperation with classiﬁcation
algorithms, which were limited by unacceptable segmenta-
tion performance when dealing with large-messy datasets.
Recently, numerous DL-based approaches have shown con-
siderable improvement in terms of performance by exploiting
different deep architectures if compared with traditional meth-
ods [45]. The powerful capability of CNNs in extracting deep
visual features at multi-scale image resolutions has been ex-
ploited to design advanced segmentation models in [46]–[50].
For example, DeepLab [48] leveraged atrous convolution to
improve feature learning efﬁciency by enlarging the receptive
ﬁeld of ﬁlters while keeping a small number of parameters or
a low computational cost. Because of learning classiﬁcation
models at pixel level, image segmentation usually consumes
considerable computation and large memory [51], [52]. To
overcome this challenge, some efforts have been presented
with some skills of network designs and learning techniques,
such as transfer learning [53]. The virtual environment in the
metaverse is usually built with diversiﬁed visual units (e.g.,
single objects and multi-object modules); therefore, AI-based
object detection must deal with a huge number of complicated
classes, including real and virtual objects. Numerous recent
object detection works have exploited CNN architectures to
achieve impressive performance in terms of accuracy and
processing speed [54]–[56]. DL-based semi-supervised and
unsupervised learning models have been recommended to
tackle unseen classes in the training dataset [57]. Some natural
problems of object detection in the 3D environment, such as
occlusion, illumination variation, and view-point change, have
been taken into consideration by incorporating advanced image
processing and depth sensing algorithms [58], [59]. In this
context, depth estimation can improve the accuracy of object
positioning [60] in the 3D virtual world, but more geometric
sensors are required to estimate depth information.

In the virtual world, some image quality reduction prob-
lems, such as noise, blurring, and low resolution, should be
addressed to enrich the visual perception of users. In the
perspective of image processing and computer vision, these
problems are studied over two tasks: image restoration and
image enhancement. In [61], a decomposition-guided multi-
scale CNN-based method was proposed to remove single
image haze, in which deep residual structure and U-Net [62]
learning frame are combined to improve decomposed image
components (so-called as feature maps) while avoiding color
distortion. Some other advanced image restoration works
exploited CNN architectures to reduce image compression
artifacts, restore clean images from downscaled and blurred
images, and reconstruct missing details [63]–[65]. It is noted
that the differences in terms of image quality and video spec-
iﬁcation between the clean virtual contents and real displayed
images/videos can appear in VR devices [66], [67]. These gaps

8

single actions and complex interactive activities (e.g., human-
machine interaction and human-human interaction) [80]. To
deal with the problem of estimating human pose in cluttered
environments, two discriminative models based on standard
structural support vector regression (SVR) and latent struc-
tural SVR were studied in [81], which are capable of ex-
tracting structural dependencies as the correlations between
local features regarding pose representation. To improve the
accuracy of body part localization and deal with varying views,
the depth information acquired by depth cameras has been
learned along with color information by advanced ML and DL
models [82]–[84]. In these works, some CNN architectures
with high-class structural connections, such as dense layer
connection, skip connection, and channel-attention connection,
were designed to estimate skeletal joints precisely besides
addressing some challenging problems in computer vision like
object occlusion. In the line, human pose estimation has a
close relation to action recognition, where the captured body
information is useful to identify actions via pattern recognition
models. Instead of detecting instant posture that can expose
high confusion, many current works have tracked body motion
in the temporal domain for a long-term observation to improve
the accuracy of action recognition. For example, some gener-
ative statistic models have been developed in [85]–[87] to an-
alyze human pose transition by capturing the spatio-temporal
geometric features between different body parts. Notably, the
last decade has witnessed the revolution of visual-based ac-
tion recognition with DL to signiﬁcantly improve recognition
accuracy and effectively deal with numerous realistic single
actions and grouped activities [88]. Some methods proposed
innovative networks with advanced CNN architectures [89]
and hybrid CNN-RNN architectures [88] to improve learning
efﬁciency of action discrimination models. Additionally, hand
gesture recognition, gait identiﬁcation, and eye tracking [90],
[91] have been considered to improve interactive experiences
in XR environments.

C. Blockchain

In general, blockchain is deﬁned as a digital ledger that
contains a list of recorded transactions and tracked assets
interconnected in a business network by using cryptography
techniques. Blockchain can provide immediate, shared, and
transparent information stored in an immutable and impen-
etrable ledger which can be accessed by only the network
members with permission [92]. A typical blockchain network
can track orders, payments, accounts, and other transactions.
In the metaverse, a large amount of data (e.g., videos and
other digital contents) is acquired by VR devices, transmitted
over networks, and stored in data center without any security
and privacy protection mechanisms, which can become the
sensitive target of cyberattacks. In this context, blockchain
with several unique features reveals a promising solution for
security and privacy issues in the metaverse [93], especially
when it is empowered by AI technologies. Besides, many
creative activities and events offered by service providers to
users will yield numerous in-metaverse objects/items (a.k.a.,
digital assets) which should be recorded and tracked via
transparent transactions with smart contracts in blockchain.

Fig. 6. Computer vision in the metaverse with scene understanding, object
detection, and human action/activity recognition.

can be ﬁlled effectively with AI-empowered image restora-
tion methods, such as blur estimation, hazy removal, color
correction, and texture reconstruction, but the computational
complexity should satisfy the real-time video processing speed
(usually measured by frames per second – FPS metric) [68]
to guarantee high-class user experience in the metaverse.
Image enhancement has been widely considered for XR with
some common tasks, such as contrast increment and super-
resolution construction. In the past, many traditional image
enhancement methods have been studied by applying image
processing techniques, for example, histogram analysis and
image decomposition [69], [70]. Recently, numerous impres-
sive works of image enhancement have achieved considerable
performance improvement by exploiting ML algorithms [71],
especially DL with CNN architectures [72]–[75]. For example,
a convolutional down-sampling and up-sampling network was
introduced in [72] to improve the overall contrast of images,
in which the deep features of RGB (red, green, and blue)
channels are combined via a feature-based fusion scheme to
obtain cross-channel contrast balance. In [76], a fully CNN
for image super-resolution was proposed with a lightweight
structure, which can learn an end-to-end relation between
input low-resolution images and output high-resolution im-
ages. Compared with some traditional sparse-coding-based
methods [77], this approach has shown the superiority in terms
of image quality and processing speed. Super-resolution can
become a cost-efﬁcient solution that allows service providers
to build high-resolution virtual worlds from low-resolution
image/video sources.

In the metaverse, play users can control their avatars (or
virtual characters) and interact with other users or non-
player characters (NPCs), in which the posture and action
of avatars should be estimated and recognized automatically
with the support of motion sensing interactive devices, such
as controllers, gloves, and cameras [78]. While human pose
estimation aims to identify the body parts (or key body joints
of a skeleton) and then track them in the real-time environ-
ment [79], action recognition allows systems to understand

human, standingflowers, pink, yellow, orangeflowers, pinkScene: gardenflowers,pink, violet human, watering sprinkler, greenfence lamp, white9

local data and collaboratively learn a global model at the
server by a parameter aggregation mechanism. In [101],
FL was applied to address privacy issues in data sharing
among multiple untrusted parties in a blockchain network.
This work integrates FL into a proof of training quality, a
novel consensus mechanism, of permissioned blockchain to
reduce computing and communication costs. To guarantee
high privacy of massive data generated by heterogeneous IoT
devices, FL was deployed in a blockchain-based resource
trading system [102]. A smart contract-based incentive algo-
rithm was proposed to encourage edge nodes to contribute and
evaluate FL tasks. For vehicular edge computing in intelligent
transportation systems, FL was combined with blockchain to
collaboratively detect malicious attacks [103]. While FL can
ofﬂoad the trained intrusion detection model to distributed
edge devices to reduce computing resources of the central
server, blockchain can ensure the security of the aggregation
model
in both the model storage and sharing processes.
Besides data security and privacy, interoperability is another
important concern in blockchain to collaborate with different
parties using different data infrastructures. For example, a
learning analytics framework [104] was studied to obtain
solid interoperability between multiple blockchain participants
which have to share a single ledger. Integrating blockchain
into FL were recently found in computing resource allocation
and management applications [105], [106] to address various
problematic issues in centralized systems, such as external
cyberattacks, server malfunctions, and untrustworthy server.
In the metaverse, wherein multiple parties join and contribute
digital content having different formats and structures, data
security, privacy, and interoperability can be fully handled by
collaboratively developing blockchain and AI.

D. Networking

The metaverse serves a massive number of users regarding
pervasive network access over wireless networks. In the last
decade, several innovative technologies have been introduced
to improve the overall performance of wireless communication
and networking systems, in which AI has been intensively
used at multiple layers of a network architecture [107]. Real-
time multimedia services and applications in the metaverse
usually demand a reliable connection with high throughput
and low latency to guarantee a basic-level user experience
at least. As the requirements of ﬁfth-generation (5G) net-
works, the peak data rate should be around 10 Gbps (gi-
gabits per second) and the end-to-end delay cannot exceed
10 ms (millisecond). In this context, ultra-reliable and low-
latency communications (uRLLC) represent the foundation
to enable the development of emerging mission-critical ap-
plications [108]. Several optimization algorithms have been
introduced to achieve uRLLC in 5G networks and beyond
(e.g., sixth-generation 6G), but most of them require high
computing resources. ML and DL have shown great poten-
tial to effectively handle existing challenging tasks, such as
intelligent radio resource allocation [109], in 5G/6G networks
while meeting a very low latency. RL was leveraged to address
the resource slicing problem for enhanced mobile broadband

Fig. 7. A blockchain-based IoT framework with ML to enhance security and
privacy.

In the last decade, numerous advanced methods for data
acquisition, storage, and sharing have been proposed by com-
bining blockchain and AI technologies in various application
domains to obtain high data security and privacy [94], which
have shown great potential to be deployed in the metaverse.
In [95], various conventional ML algorithms (e.g., clustering,
SVM, and bagging) and innovative DL architectures (e.g.,
CNN and LSTM) were investigated for data analytics to
detect and classify cyberattacks in blockchain-based networks.
Some other concerns were also considered in this work, such
as incentive mechanisms to encourage users to contribute
authenticated data, AI-based smart contract evaluation, and
cost-efﬁcient model learning in on-chain environment. For
the Internet of Things (IoT)-aided smart cities, an effective
privacy-preserving and secure framework [96] was introduced
by integrating blockchain with enhanced proof of work and
ML with data transformation, which in turn robustly deals
with various cyberattacks in smart city networks. In [97],
deep extreme learning machine was exploited in a resource-
efﬁcient blockchain-based IoT framework, which improved the
system security and privacy based on data interpretation and
abnormality prediction. This framework (see Fig. 7) has shown
high performance of fraud detection and threat prediction, and
can be extended for dealing with security and privacy problems
in data storage and sharing instead of data collection. Recently,
DL has replaced traditional ML in terms of cooperating with
blockchain to solve some challenging security and privacy
issues of big data, where ﬁve essential characteristics of
big data (i.e., velocity, volume, value, variety, and veracity)
are presented. For example, DeepChain [98], a CNN-based
blockchain framework, was developed to ensure the privacy
and integrity of data contributed by participants in a network.
Deep RL was exploited to achieve a secure mobile ofﬂoading
in multi-access edge computing (MEC) based blockchain
networks [99] and to obtain a secure vehicular crowdsensing in
the blockchain-based Internet of vehicles (IoV) systems [100].
Federated learning (FL) has recently emerged as an effective
solution to address the privacy problems of data sharing,
in which multiple users train AI models with their own

SmartWatchSmartTVSmartCameraSmartPhoneSmartSensorServerSmart ContractChain NetworkSmartCitySmartHomeUtility ControlMarketPlaceUser 1User 2User kUser NAccessLayerBlockchainLayerAppsLayerIoT SystemLayerReal TimeAnalyticsDataAnalyticsData Acquisition LayerPre-processingEvaluation PhaseMachineLearning ModelCloudPlatform- Trained Model for Access Layer- Import Trained Model to Cloud1212(eMBB) and uRLLC [110], in which the complicated patterns
of resource allocation and scheduling are formulated to col-
laboratively learn network states and channel conditions. In
another work [111], RL showed the effectiveness in terms
of joint subcarrier-power management and allocation, thus
signiﬁcantly reducing latency and improving reliability on the
Internet of controllable things. Particularly, this work proposed
a double Q-learning network to optimize the total spectrum
efﬁciency via subcarrier assignment and power control policy,
and accelerate learning convergence. As a vital role to enable
uRLLC, efﬁcient radio resource management was investigated
in [112] with a distributed risk-aware ML approach to monitor
and manage the transmission of non-scheduled and scheduled
uRLLC trafﬁcs.

Lately, DL has been exploited for many tasks in uRLLC,
including spectrum management, channel prediction, trafﬁc
estimation, and mobility prediction. Two advanced CNN ar-
chitectures, namely MCNet [113] and SCGNet [114], were
designed in physical layer to automatically identify the mod-
ulation types of incoming signals, which in turn allows the
receiver to demodulate accurately and enhance the spectrum
utilization efﬁciency accordingly. To overcome the high com-
putational cost of conventional channel state information (CSI)
estimation approach, an online CSI prediction method [115]
was proposed a supervised learning framework by combining
CNN and LSTM, in which two-stage training mechanism was
deployed to improve the robustness and stableness of CSI es-
timation in practical 5G wireless systems. In [116], an end-to-
end CNN architecture was designed with 3D convolution for
intelligent cellular trafﬁc forecasting, in which the deep model
can learn the underlying correlations of trafﬁc data in both the
short-term and long-term spatial patterns. Besides achieving
high accuracy of trafﬁc prediction, the deep network showed
the effectiveness with different real-world scenarios, such as
trafﬁc congestion data and crowd ﬂow data. In conclusion,
with ML algorithms and DL architectures, AI is a powerful
tool to address many challenging problems of uRLLC in future
wireless networks, which allows users to experience high-class
integrated services in the metaverse with the guarantee of high
throughput low latency.

E. Digital Twins

As a digital representation of real-world entities, a DT can
synchronize operational assets, processes, and systems with
the real world along with some other regular actions, such
as monitoring, visualizing, analyzing, and predicting [117].
DTs are at the central of where the physical world and the
virtual world interact via IoT connections [118]; and therefore,
any change in the real world will be rejected in the digital
representation. With these distinctive properties, DT is found
as one the fundamental building sectors of the metaverse and
plays as the gateway for users to enter and enjoy services
in the virtual world by creating exact replications of reality,
including structure and functionality. For example, technicians
can maneuver 3D representations of complex systems at multi-
level sophistications (i.e., descriptive, informative, predictive,
comprehensive, and autonomous) for a wide spectrum of

10

Fig. 8. A general architecture in 5G and beyond for metaverse services and
applications, in which AI with ML algorithms and DL models contribute in
multi-level tasks.

purposes, such as technical
training and commercial cus-
tomization. Accordingly, DTs allow application developers
and service providers to reconstruct virtual replications of
machines and processes, in which any kind of physical analysis
can be done remotely with AI [119].

For industry 4.0, a reliable DT framework [120] was pro-
posed for sensor-fault detection, isolation, and accommoda-
tion, in which a multi-purpose ML method with multi-layer
perception neural network was deployed to validate sensory
data, estimate fault condition, and identify fault sensor. As
a digital replication to operate human-robot welding actions,
a DT system in [121] was developed along with VR and
AI technologies to monitor and analyze welder behaviors. A
generic ML framework with domain transformation, feature
engineering, and classiﬁcation was applied to recognize proper
welding behaviors based on the data acquired from a bi-
directional ﬂow between robot and VR. In [122], a data-driven-
based DT framework (see Fig. 9) was studied to improve
health diagnosis performance and promote better health op-
eration in intelligent healthcare systems. DTs contributed in
different phases to create virtual replications of patent health
proﬁles, carry out collaborative activities of health profes-
sionals, and formulate a universal treatment plan for patients
in same cases. ML models were built to learn meaningful
information from raw data collected by the Internet of Medical
Things (IoMT) devices to early detect health abnormalities
and precisely recognize health problems. In another work
that proposed a cyber-physical framework for smart urban
agriculture services and applications [123], DTs were designed
to replicate a virtual representation of farming production,
in which the sensory data acquired by practical sensors was
processed by ML algorithms in a decision support system. To
adapt to different types of product magniﬁcation, the DTs were
built from small functioning modules to a whole process twin.
With a great capability of automatically learning features
from high-dimensional unstructured data and effectively deal-

Devices with processing capabilityfor users to experience in themetaverseBase station (BS) with cachesand edge servers  Centralized control planeCentral serverUser-level tasks  Predict traffic  Estimate mobility  Detect anomalyBS-level tasks  Control user access  Design scheduler  Optimize resourceNetwork-level tasks  Routing  Handover  Network slicingML algorithms /DL architecturesLearning models fordetection, classification,and regression11

Fig. 9. A data-driven DT framework for intelligent healthcare systems using
ML to process raw data of IoMT devices.

Fig. 10. A common BMI cycle with primary components for processing
neural signals and responding neural stimulations.

ing with spatiotemporal learning models, DL has been recently
applied in DT architectures for different services and appli-
cations. In [124], a DT architecture was developed for the
edge computing-aided Internet of vehicles (IoV) to improve
the utilization efﬁciency of vehicle’s computational resources.
To overcome the overload problem of edge devices, deep Q-
network optimized the function approximation of DL and RL.
For the performance investigation of uRLLC services and
delay tolerant services in mobile edge computing systems,
a DT framework was built in [125] by replicating a virtual
pattern of the real network environment. Remarkably, DL
with feedforward neural network architecture was carried
out to deal with varying network parameters of real-world
phenomena. For industrial IoT, the work in [126] built DTs to
simulate and capture the operation state and real-time behavior
of industrial devices, which were map into a digital world. To
address the bias between real entity and its digital replication,
a trusted-based aggregation with FL was carried out with a
deep RL model to general improve performance while meeting
resource constraints. With AI as a powerful analytics tool,
DT can improve system performance, reduce process-related
incidents, minimize maintenance costs, and optimize business
and production. In addition, DT allows users to view the
metaverse as an advancing replication of reality with full real-
time synchronization from the physical world.

F. Neural Interface

Technology is deﬁnitely enriching the world around us by
enhancing the human experience and fully ﬁlling gap between
reality and virtual world in the metaverse. In this context, the
most immersive popular interface to interactive with the virtual
work is a VR headset with a controller. Many technology com-
panies currently pay attention to neural interfaces, so-called
brain-machine interfaces (BMIs) or brain-computer interfaces
(BCI), that go beyond VR devices. The BMIs help to nearly
clean the borderline between human and wearable devices.
Many BMIs detect neural signals using external electrodes or
optical sensors that adhere to the skull and other parts of the

human body. According to these noninvasive devices, which
only read and control mind at a rudimentary level, BMIs
can manipulate thoughts with transcranial electromagnetic
pulses. Fig. 10 describes a common BMI cycle with primary
components for processing neural signals and responding
neural stimulations [127]. Besides data engineering techniques
in the preprocessing stage, AI/ML algorithms in the pattern
recognition stage enable analysis of complicated and sensitive
neural signals accurately.

With electroencephalogram (EEG) signal as one of the most
popular inputs of BCI systems, the work in [128] studied a
brain signal classiﬁcation by two learning approaches: one
is ofﬂine unsupervised classiﬁcation and another is simulated
online supervised classiﬁcation. Besides that, two approaches
achieved lower computational cost and better performance
in common tasks, e.g., motor imagery, mental analysis, and
event-related potential, the ofﬂine unsupervised mechanism
did not require signal labeling for new subjects in the learning
phase. As building an accurate predictive model in BCIs to
decipher brain activities into communication and control com-
mands, the work in [129] learned discriminative spatiotempo-
ral features to seize the most relevant correlations between
different neural activities from EEG signals. Based on the re-
constructed signal waveforms containing dominant frequency
characteristics as feature vectors with lower dimensionality,
several ML algorithms, e.g., logistic regression (LR), Naive
Bayes, and SVM, were applied to investigate the performance
of ERP. In [130], the feasibility of using visual hemisphere
to extract relevant information about the spatial location of
targets in aerial images was investigated with feature selection
and SVM classiﬁcation, which were deployed in a rapid
serial visual presentation (RSVP) procedure. Concretely, by
learning ERP patterns from extracted discriminative features
of EEG signals, the concerning target and its location in aerial
images can be identiﬁed. As the effort to increase the correct
classiﬁcation rate of EEG signals in BCIs, an advanced ML
framework was introduced in [131] by combining an improved
common spatial pattern algorithm and a transfer learning

DataAnalyticsCleaningPreprocessingMachinelearningRepresentationResultRepositoryRaw DataRepositoryPhysical TwinHealthcareProfessionalsResultFeedbackResult & FeedbackSimilar-case PatientsDigital twinsResult & FeedbackRawDataDigital Twin ArchitectureStorageSmart SystemFeedbackMonitoringIoMTDevicesNeural activitygenerationDataacquisition &stimulationDatapreprocessingPatternrecognitionApplications& servicesBrain signalsRaw dataFine dataAcquisitionactionUserfeedbackStimulationactionFiringpatternStimulationStimulationmodelNeural dataacquisiton flowNeuralstimulation flowmechanism. Besides achieving high accuracy of classifying
left-hand and right-hand imaginary movements, the trained AI
model can be used for other classiﬁcation and recognition tasks
in the same domain via knowledge transferring technique.

learning models (e.g.,

Based on the superiority of capsule network (CapsNet)
compared with traditional neural networks in terms of feature
extraction and feature explanation, the work in [132] applied
CapsNet to improve the accuracy of ERP detection in BCI
systems. With highly discriminative spatial features and key
temporal correlations extracted from EEG signals by capsule
layers, CapsNet not only outperformed some state-of-the-
art
linear discriminant analysis and
CNN [133]) but also obtained the practicality with differ-
ent common spellers in the cognitive neuronscience domain.
In [134], a hybrid DL framework was proposed with multi-
directional CNN and bidirectional LSTM in an accurate brain-
controlled robotic arm system. This proposed learning ap-
proach effectively calculated underlying spatial signal corre-
lations in time, boosting the decoding performance for 3D
multi-directional arm-based object grasping tasks. Inspired by
GoogleNet [135], the work in [136] proposed EEG-Inception,
a novel CNN for EEG-based classiﬁcation tasks in BCI sys-
tems, which involved multiple inception modules in improving
feature learning efﬁciency. Furthermore, an effective training
strategy that incorporated cross-subject transfer learning and
ﬁne-tuning to reduce calibration time of ERPs and demonstrate
the feasibility in real-world assistive applications. In the future,
brain-computer interfaces will truly promote the ultimate im-
mersive interaction between the reality and the virtual world in
the metaverse via consumer-ready mind-control systems. The
existing AI-enabled works on the subject of six concerning
technical aspects, which are promisingly for the metaverse,
are summarized in Table I.

IV. AI FOR THE METAVERSE: APPLICATION ASPECT

This section conveys the existing AI-aided works in the
perspectives of four key applications: healthcare, manufactur-
ing, smart cities, and gaming (see Fig. 11); which are proba-
bly considered to delivery specialized services in the meta-
verse Besides, some other potential applications, including
E-commerce, human resources, real estate, and decentralized
ﬁnance, are discussed in brief.

A. Healthcare

The health industry has recently started exploiting some
revolutionary techniques like VR and big data incorporated
with AI in software and hardware to increase the proﬁciency
of medical devices, reduce the cost of health services, improve
healthcare operations, and expand the reach of medical care.
From a 2D environment to a 3D virtual world, the metaverse
allows users to learn, understand, and share patients’ health
conditions and medical reports in an immersive manner. By
means of VR/XR systems, AI plays a vital role in many
healthcare and medical sectors, for example, achieving better
efﬁciency in providing diagnosis, delivering accurate and faster
medical decisions, providing better real-time medical imaging

12

and radiology, and supporting more convenient simulated
environments to educate interns and medical students.

In many wearable devices for healthcare and wellness
applications and services [137], AI has been applied to au-
tomatically recognize complex patterns of sensory data. For
supporting physicians and health-wellness experts to make
decisions in daily living assistance and early healthy risk
awareness, a physical activity recognition method was intro-
duced in [138] by using the sensory data of multiple wear-
able devices. The method combined the globally handcrafted
features and locally deep features (i.e., extracted by a deep
CNN) over an intermediate fusion mechanism to improve the
activity recognition rate. In [139], a novel encoding algorithm,
namely Iss2Image, was introduced to convert inertial sensory
signal (e.g., accelerometer, gyroscope, and magnetometer) into
a color image for CNN-based human activity classiﬁcation.
Furthermore, a lightweight CNN with few layers in a cascade
connection was designed to learn the physical activity patterns
from encoded activity images. In [140], a system of fall
detection using wearable devices was proposed for IoT-based
healthcare services, in which a hierarchical DL framework
with CNN architectures was developed for collaboratively
processing sensory data at local devices and a cloud server. As
capably working with multiple wearable devices (e.g., smart-
phone, smartwatch, and smart insoles), the system yielded high
correct detection rate with high data privacy. Besides CNN,
RNN and LSTM networks were exploited to process wearable
sensory data in some early healthy risk attentions, such as fall
detection and heart failure [141].

With the great success of DL, especially CNN architectures,
in the image processing and computer vision domains, few
recent years have witnessed a vast emergence of DL to
address various challenging tasks of medical image analysis
because of the requirement of much more specialized knowl-
edge from technicians and medical experts if compared with
natural image analysis [142], [143]. For lesion segmentation
in breast ultrasound (BUS) images, the work [144] studied an
advanced network, namely saliency-guided morphology-aware
U-Net (SMU-Net), by involving an additional middle feature
learning stream and an auxiliary network. The coarse-to-ﬁne
representative features from the auxiliary network were fused
with other features (e.g., background-assisted, shape-aware,
edge-aware, and position-aware) to effectively discriminate
morphological texture in BUS images. In [145], a cost-efﬁcient
unsupervised DL approach was introduced to accelerate the
processing speed of non-rigid motion estimation of heart in
free-breathing 3D coronary magnetic resonance angiography
images. Replying on a deep encoder-decoder architecture, the
network can learn image similarity and motion smoothness
without ground truth information in a patch-wise manner to
save computing resources signiﬁcantly instead of a regular
volume-wise manner. To overcome the obstacle of increasing
network size and computation of 3D CNNs in mining com-
plicated patterns in 3D images [144], 2D neuroevolutionary
networks were investigated for 3D medical image segmenta-
tion [146], in which an optimal evolutionary 3D CNN was
renovated to reduce computational cost without sacriﬁcing
accuracy. With AI in use as the core technology for data ana-

TABLE I
SUMMARY OF AI FOR THE METAVERSE IN THE TECHNICAL ASPECT.

13

Task

Word and linguistic prediction for language
modeling.

Analyzing and understand the representation of
words from characters
Identifying preﬁxes and sufﬁxes and detecting mis-
spelled words
Sentiment prediction and question type classiﬁca-
tion.
Generate short text in image captioning and long
text in virtual question answer.
Semantic labeling, context retrieval, and language
interpretation.
Forecasting eye ﬁxations in VR task-oriented vir-
tual environment.
VR quality assessment for 2D and 3D
foveated-compressed videos
Forecasting eye ﬁxations in VR task-oriented vir-
tual environment.

Semantic segmentation and object detection.

Image/video quality enhancement (e.g., hazy
removal, color correction, texture reconstruction,
and super-resolution)

Human pose estiation and action/activity
recognition.

Detection and classiﬁcation of cyberattacks in
blockchain-based networks.
Resource management
framework.
Preservation of data privacy of heterogeneous IoT
devices.

in blockchain-based IoT

AI Technique
RNNs and LSTM networks with the attention mechanisms.
Advanced memory network with residual connection.
Deep networks with gated connection and bi-directional structure.
General deep networks with CNN and LSTM architectures.

DL framework with CNN, Bi-LSTM, and conditional random ﬁeld.

Various CNNs and LSTM networks with simple structures and
advanced-designed architectures.
DL framework with single RNN/LSTM and mixture LSTM-CNN
models.
Unsupervised and reinforcement learning with common RNN/LSTM
and CNN models.
A DL framework with multiple CNNs for feature extraction and
prediction.
DL framework with CNNs architecture, in which 3D convolutional
layers are built in feature extraction blocks.
DL framework with multiple CNNs for feature extraction and pre-
diction.
CNNs with atrous convolution.
CNNs with channel-wisely and spatially attentional schemes.
CNNs with pixel-wise local and global attention pooling-convolution.
CNNs with knowledge transfer via visual similarity and semantic
relatedness.
CNNs with 3D object-object relation graphs.
Decomposition-guided multi-scale CNN architectures with deep
residual structure and U-Net learning frame.
Combination of deep features extracted by CNNs via a feature-based
fusion scheme.
Full 3D CNN architectures with simultaneous and separated spatial-
spectral joint feature learning mechanisms.
Discriminative model with latent structural SVRs.
CNNs with dense layer connection and channel-attention connection.
Generative models with latent Dirichlet and Pachinko allocations.
Advanced CNNs with geometric feature transformation.
Conventional ML algorithms (e.g., clustering, SVM, and bagging)
and DL architectures (e.g., CNN and LSTM).
Deep extreme learning machine.

FL framework with deep model using average aggregation mecha-
nism.
FL framework with CNN model averaging and training.

[27]

[29]

[31]

[32]

[33]

[37]
[39]
[33]

[48]
[51]
[56]
[57]

[59]
[61]

[72]

[73]

[81]
[82]
[87]
[89]
[95]

[97]

[102]

[103] Detection of malicious attacks in intelligent trans-

portation systems.

[109] Resource slicing problem for eMBB and uRLLC. RL with a policy gradient based actor-critic learning mechanism.
[111]
[112] Management of the transmission of non-scheduled

RL with double Q-learning network.
Supervised learning framework with conventional ML algorithms.

Subcarrier-power management and allocation.

Technical Aspect Ref
NLP

[20]
[21]
[24]
[25]

Vision Machine

Blockchain

Networking

Digital Twin

and schedule uRLLC trafﬁcs.

[113] Automatic modulation classiﬁcation in wireless

[115]

[116]
[120]

systems.
Prediction of CSI in 5G wireless systems.

Forecasting intelligent cellular trafﬁc.
Sensor-fault detection, isolation, and accommoda-
tion

[122] Diagnosis of heart disease and detection of heart

[123]

[124]

[126]

problems.
Prediction of complete future system states relying
DT of urban farming.
Improvement of
computing-aided IoVs network.
Simulation of the operation state and analysis real-
time behavior via DT.

resource utilization in edge

Neural Interface

[129] ERP classiﬁcation in BCI systems.

Spatial object localization in aerial images.

[130]
[132] ERP detection in BCI systems.
[134] Brain-controlled robotic arm system.
[136] EEG-based classiﬁcation tasks in BCI systems.

Advanced CNN architecture with sophisticated-designed modules of
convolutional layers.
A supervised learning framework by combining CNN and LSTM
with two-stage training mechanism
An end-to-end CNN architecture was designed with 3D convolution.
A multi-purpose ML method with multi-layer perception neural
network.
A data-driven-based analysis framework with traditional classiﬁcation
algorithms.
Several common ML algorithms for regression (LR) and classiﬁca-
tion (SVM).
Deep Q-network optimized the function approximation of DL and
RL.
FL framework with deep RL model.

Spatiotemporal feature extraction and ML algorithms (e.g., LR, Naive
Bayes, and SVM).
Feature selection with SVM classiﬁcation.
Capsule network with primary capsule components.
DL framework with multi-directional CNN and bidirectional LSTM.
EEG-Inception network with cross-subject transfer learning and ﬁne-
tuning.

14

Fig. 11. AI for the metaverse in the application aspects with healthcare, manufacturing, smart cities, and gaming besides other promising domains, such as
E-commerce, human resources, real estate, and DeFi.

lytics, several healthcare and medical diagnosis applications
(e.g., motor rehabilitation and magnetic resonance imaging
neurofeedback) can be developed in the VR environment
for multipurpose, such as collaborative treatment planning
and educative training [147]. Indeed, several healthcare and
medical services can be provided in the metaverse. For ex-
ample, medical students can improve surgical skills by doing
interactive practice lessons built for medical education in the
virtual world or patients can ﬁnd some healthcare services via
virtual assistants at virtual health centers and hospitals.

B. Manufacturing

As the current wave of industrial revolution, digital trans-
formation in manufacturing has been happening with digital
connection between machines and systems to better analyze
and understand the physical entities. Different from digital
transformation to enhance the physical world via digital oper-
ations, the metaverse creates a virtual world that is translated
onto the physical world based on the foundation of reality in-
teraction and persistence. By collaboratively adopting cutting-
edge technologies, such as AI and DT, the metaverse for man-
ufacturing can signiﬁcantly modernize digital operations in the
current digital revolution. Currently, AI with ML algorithms
and DL architectures have considerably contributed to the
manufacturing domain via numerous industrial applications.

In manufacturing, shortening product lifecycles and increas-
ing the number of product variants are the main reason of high

expense for frequent production system reconﬁgurations and
upgrades, especially with ML-based systems which have spent
more time and computing resources for new data collection,
preprocessing, and model learning. To overcome the above
challenges, a symbiotic human-ML framework [148] was
leveraged with a reinforcement learning strategy by combining
the learning capacity of Q-learning models and the domain
knowledge of experts. This framework also considered human
exploration to reduce noise in data and improve the quality
of automatic decision-making systems. As a great importance
in the modern manufacturing systems, quality inspection has
been recently attracting more attention with intelligent data-
driven condition supervision approaches; however, they had
to face some difﬁculties from different operating conditions
with diversiﬁed tasks and applications [149]. For reliable fault
detection and diagnosis in manufacturing, numerous methods
have exploited DL with RNN and CNN architectures to
achieve high accuracy while keeping a real-time monitoring.
For instance, a RNN [150] was developed with an encoder-
decoder structure coupled with attention mechanism to pre-
dict and diagnose interturn short-circuit faults in permanent
magnet synchronous systems. In [151], a data-driven LSTM-
based fault diagnosis approach was introduced to early detect
multiple open-circuit faults in wind turbine systems. In [152],
a DL-based intelligent fault diagnosis method was introduced
to address two challenging problems, i.e., the lack of labeled
data for learning model and the data distribution discrepancy

HealthcareManufacturingSmartCitiesGamingPhysical activity recognitionSensor-based fall detectionLesion segmentation in breast imageNon-rigid heart motion estimationLiving assistance and risk awarenessVirtual health centers and hospitalTreatment planning and educative trainingShortening product lifecycleMachine condition supervisionFault detection and diagnosis Production line optimizationManufacturing scalability and compatibilityMake-to-order manufacturing enterpriseVirtual entities for operating transparencyIntelligent transport systemSmart community portalVideo-based surveillance systemCollaborative home appliances controlSmart environmental trackind and awarenessSustainable green agricultureVirtual replication in metaverse ecosystemConsole, mobile, and PC gaming platformsAI-assisted game store tellingProcedural content creationTactical planning for AI agentImmersive gamming experience evaluationAI-aided gaming developing optimizationRealistic player-NPC interactionDevelopment of virtual stores Shopping experience improvementPersonalizing customer experienceShopping behavior analysis and understandingE-commerceVirtual job fairsImmersive recruitment experienceRevolution of working style and workplaceVirtual meeting platform supporting metaverseHuman ResourceVirtual land investment in the metaverseLand and house for tradingNFT-associated real estate in virtual worldsCost-efficient marketing channel for real estate companiesReal EstateCryptocurrency-based financial platformLeaning, borrowing, farming, and stakingDecentralized exchange and applicationTrading products and NFT using cryptoDeFiMetaverse'sApplicationsbetween training and testing sets, by incorporating CNN
architecture and transfer learning mechanism.

Design and implementation of an optimal serial production
line are crucial
to increase the productivity of the whole
manufacturing process. Many recent works have applied AI
to optimize some speciﬁc sectors in a production system
and improve the performance of production line accordingly
while meeting scalability and compatibility. For example, a
prediction model [153] was developed to estimate the optimal
buffer size in production lines by combining a regular artiﬁcial
neural network (ANN) and a generic algorithm. The prediction
model was further integrated with an optimization mechanism
to evaluate and forecast the optimal buffer size in need to max-
imize productivity. In [154], an efﬁcient production progress
prediction method was formulated with the combination of
DL and IoT to optimize the dynamic production and on-
time order delivery activities in make-to-order manufacturing
enterprises. In the proposed method, a two-stage transfer
learning mechanism was executed with historical data and
real-time state data using a deep belief network to solve the
nonlinearity of production progress. Nowadays many manufac-
turing plants have developed industrial collaborative robots to
undertake different advanced tasks which require much more
cognitive skills, intelligence, and domain knowledge of human
to immediately respond unexpected actions or events with
high precision and conﬁdence [155]. Therefore, it demands
a cooperative AI model to learn complicated patterns from
multimodal data for different correlated tasks of manufacturing
process and production line, in which the AI model should
be equipped with the capability of explanation and reason-
ing. Through virtual entities in the metaverse, the industrial
manufacturing efﬁciency is generally improved with AI to
speed up production process design, motivate collaborative
product development, reduce operation risk to quality control,
and obtain high transparency for producer and customers.

C. Smart Cities

Smart cities acquire the meaningful information about the
needs of citizens through the IoT, video cameras, social media,
and other sources. Based on the feedbacks automatically
collected from users, city governments need to make decisions
about which services to remove, offer, and improve. By using
more digital tools and pioneering technologies, smart cities
will provide smarter interactive services to users over the
metaverse platform [156]. The environmental data (e.g., air
quality, weather, energy consumption, trafﬁc status, and avail-
able parking space) are fully replicated in the virtual world
for user-friendly interface. Several smart services, such as
utility payment and smart home control, can be now executed
in the virtual world via the platforms and systems deployed
in the metaverse:
transportation systems (ITS),
light management systems, automatic parking
smart street
systems, smart community portals, and indoor/outdoor video
surveillance systems. Currently, the actual impact and beneﬁt
of these technologies to the smart cities is limited; however,
the metaverse can be an accelerant to spread the presentation
of smart services in the daily life of citizens [157].

intelligent

15

Among different

technologies to enable smart cities in
the physical world and in the metaverse, AI has shown a
great signiﬁcance to achieve automation and intelligence in
smart services. By integrating EEG-based BCI, VR, and IoT
technologies fueled by AI, the work in [158] introduced a
steady-state visual evoked potential-based BCI architecture
to collaboratively control home appliances. With the visual
information captured through head-mounted display, the brain
signals were recorded to analyze with ML algorithms and
respond control commands with stimulation, which allow users
to control home appliances over the IoT network. In the
effort to develop a hybrid ITS from the physical world to
its virtual replication in the digital world, the comprehensive
work in [159] introduced an intelligent and ubiquitous IoT-
enabled architecture to control and manage urban trafﬁc. Many
scenarios with practical data processing and decision making
of different transportation services were investigated simulta-
neously in both of physical and virtual world, thus conducting
high-quality real-time services to users and reducing operation
and maintenance costs. Because of the hastiness of industrial-
ization and the explosion of urbanization, air pollution has
become a life-warning problem, which has affected living
environment and physical health defectively. For early air
pollution warning and management, an efﬁcient forecasting
approach was studied with a hybrid DL architecture [160],
which combined 1-D CNNs and bi-directional LSTM net-
works to fully extract intrinsic correlation features and in-
terdependence of multivariate time series data acquired from
multiple sensors. Besides environmental pollution, sustainable
agriculture has been attracting much more concerns in smart
green cities [161]. In this context, AI is one of the vital
information and communication (ICT) technologies, which has
been widely used in precision agriculture systems with yield
prediction, quality evaluation, and pest and disease detection.
Designing and implementing the metaverse ecosystem for
smart cities with all administrative services, such as envi-
ronment, education,
transportation, culture, and other civil
services, is really a challenging mission of metropolitan gov-
ernment. By gathering big data from multiple authenticated
sources, many administrative services can be provided and
improved in the metaverse thanks to AI technology for data
analytics, in which usage rules, ethics, and security will be
released to guarantee a safe experience environment.

D. Gaming

Gaming has always been a prime application in the meta-
verse, in which ML and DL are redeﬁning and revolutionizing
the gaming industry across multiple platforms, from console
to mobile and PC platforms. This part will explore how ML
and DL can revolutionize game development and how building
a next gaming generation in the metaverse.

In the last decade, ML has had a huge impact on the way
video games are developed. To build more realistic worlds
with attractive challenges and unique stories, video game
developers and studios have been increasingly turning to ML
as a powerful tool set that help systems and NPCs to respond
to player’s action dynamically and reasonably. In [166], the

TABLE II
SUMMARY OF AI FOR THE METAVERSE IN THE APPLICATION ASPECT.

16

Application Aspect Ref
Healthcare

[139]

[138]

Description
Inertial sensory -based physical activity recogni-
tion for healthcare and wellness.
Physical activity recognition to support physicians
and health-wellness experts in making decisions in
living assistance and healthy risk awareness.

Manufacturing

Smart Cities

[140] Wearable devices based fall detection for IoT-

based healthcare and medical services.

[144] Lesion segmentation in breast ultrasound images

for detecting abnormalities.

[145] Estimation of non-rigid motion of heart in free-
breathing 3D coronary magnetic resonance angiog-
raphy images.

[148] Automatic production system reconﬁgurations and
upgrade to shorten product lifecycles and increase
the number of product variants.

[150] Reliable fault detection and diagnosis for quality

inspection of production line in manufacturing.

[151]

[152]

[153]

[154]

[158]

Forecasting the optimal buffer size required in
production systems to maximize productivity.
Production progress prediction to optimize on-
time order delivery activities in in make-to-order
manufacturing enterprises.
Smart home appliances control and management
over IoT networks.
Intelligent and ubiquitous IoT-enabled urban trafﬁc
control and management in ITS systems.
[160] Early air pollution warning and management in
smart environment surveillance systems.
Sustainable agriculture from smart farm to intelli-
gent food production line.

[159]

[161]

Gaming

[162] Modeling multi-scale uncertainty and multi-level

abstraction levels in RTS games.

[163] Design of a metamorphic testing mechanism for

game ﬂow evaluation in artiﬁcial chess games.

[164] Development of AI agents in player-vs-NPC and

player-vs-player ﬁghting games.

[165] Development of AI agents and improvement of

tactical intelligence in RTS games.

AI Technique
CNNs with an encoding algorithm to convert
signals to color images.
Residual CNNs to extract deep features in combination with
handcrafted features based on an intermediate fusion mechanism.

inertial sensory

Hierarchical DL framework with CNNs for learning models at local
devices and global cloud center.
Saliency-guided morphology-aware U-Net (SMU-Net) involving an
additional middle feature learning stream and an auxiliary network.
Deep encoder-decoder learning framework with CNN architectures.

Symbiotic with reinforcement learning strategy by combining the
learning capacity of Q-learning models and the domain knowledge
of experts.
Deep encoder-decoder framework using RNN architectures with
attention mechanism.
Data-driven learning model using deep networks with LSTM ar-
chitecture.
Data-driven learning model using transfer learning mechanism on
CNN architecture.
A combination of a regular artiﬁcial neural network and a generic
algorithm.
Data-driven learning model using two-stage transfer learning mech-
anism on deep belief network architecture.

A set of AI algorithms for brain signal processing and decision
making to respond stimulation commands.
Hierarchical AI framework by deploying ML algorithms in the
digital world to automatically make decisions in the physical world.
DL framework with 1-D CNNs coupled with bi-directional LSTM
networks.
AI framework for supervised learning with conventional ML algo-
rithms applied in detection, classiﬁcation, and recognition models.
Multi-scale Bayesian models including Bayesian networks and
maps with probabilistic learning for multi-level units control,
tactics, and strategy.
ML framework with decision tree algorithm to determine the
optimal move among all possible ones for AI agents.
A combination of RL and deep networks with different architec-
tures, such as RNN and CNN.
Supervised learning framework with CNN architectures and rein-
forcemen learning with deep Q-learning networks.

role of artiﬁcial and computational intelligence in games has
been discussed regarding many research topics: NPC behav-
ior strategy and learning, tactical planning, player response
modeling, procedural content creation, player-NPC interaction
design, general game AI, AI-assisted game story telling, and
AI in commercial games. This work further investigated these
topics according to three viewpoints: AI algorithms used in
each topic, effectiveness of AI to human user in every topic,
and human-computer interaction. For in-game decision making
and learning, a comprehensive survey in [167] investigated
the use of AI algorithms in intelligent video and computer
games. Regarding decision making, some primary AI algo-
rithms (such as decision tree, fuzzy logic, Markov model, rule-
based system, and ﬁnite-state machine) were deployed for dif-
ferent game developing tasks: modeling game ﬂow, assessing
playing motivation, evaluating immersive experience, adapting
gameplay, adapting gaming strategy, customizing gameplay,
and modeling and controlling NPC behavior. Besides, many
learning-based tasks were accomplished with Na¨ıve Bayes,

ANN, SVM, and case-based reasoning system to classify user
gameplay, classify NPC behavior, recognize user behavior, and
adapt game ﬂow based on personal experience.

In real-time strategy (RTS) games, such as StarCraft,
Bayesian models have been used for modeling multi-scale
uncertainty and multi-level abstraction levels [162]: micro-
management, tactics, and strategy. These probabilistic learning
models were able to cope reactive units control, recognize
objectives from tactical data, and predict opponent’s gameplay
based on strategic information. To reach an intelligent human-
like response mechanism, several game software companies
has applied AI in various testing tasks during design and de-
velopment stages. In [163], a metamorphic testing mechanism
was proposed to overcome the impracticability of controlling
a large amount of possible moving strategies in artiﬁcial chess
games. This testing mechanism deployed a decision tree model
to reveal metamorphic relations, which in turn effectively
determine the optimal move among all possible ones. With
a combination of RL and deep networks, AI agents in [164]

were developed to address some inherent difﬁculties in real-
time ﬁghting games and to defeat pro players in one-vs-
one battle. In addition to creating different ﬁghting styles
through self-play curriculum, this deep RL framework was
capable of all two-player competitive games which have level
upgrade and balance policies. RL and supervised learning were
also exploited to improve AI agents in RTS games [165].
Being more superior than the Puppet search algorithm, CNNs
and deep Q-learning networks inferred the outcome of costly
high-level search and optimized the available time to execute
tactical searches. In a nutshell, AI with traditional ML and
innovative DL algorithms have been making an unprecedented
revolution of gaming experience in many aspects: improving
the intelligence of NPCs, modeling complex systems, making
games more beautiful and rational, conducting more realistic
human-NPC interactions, reducing cost of in-game world
creation, and opening more opportunities of developing mobile
games. In Table II, we summarized the existing application-
oriented works utilizing AI technology which have shown the
potential to be integrated and deployed in the metaverse.

E. Other Potential Applications

Besides healthcare, manufacturing, smart cities, and gaming,
we have found some auxiliary business applications ahead for
the metaverse.

E-commerce: For the purpose of which E-commerce has
been integrated into the metaverse, numerous consumer brands
have been diving into the digital world to create more de-
lightful and seamless shopping experiences regardless of the
unpopularity of VR devices for mainstream consumers. Many
brands have moved forward step-by-step to build something
entirely new by integrating digital stores which are able
to bring the best ofﬂine and online shopping without any
difference in user experience [168]. Indeed, virtual shopping
can convey remotely real-time experience of static products,
in which consumers, represented by an avatar, can walk
around stores in a 3D rendered space and talk with virtual
cashier/seller powered by VR and AI technologies. Person-
alizing the customer experience is currently attracting more
attention from retailers, not only for business survival but also
for revenue growth, which can be performed effortlessly in the
meta with AI-based shopping behavior understanding.

Human resources: Nowadays, many big tech companies
are being creative to seek and communicate with young
talents who are looking for jobs. The recruitment manners
range from dispatching younger employees/leaders to online
interview applicants with video calls to holding job fairs in the
metaverse. Potential applicants can login into the metaverse
with blockchain-aided authenticated account and then control
their avatars to freely discuss with other avatars representing
the company’s human resource managers and project lead-
ers [169]. For recruitment guidance, the applicants can ask
or receive help from the virtual assistant with AI-based NLP.
In these kinds of recruitment events, the goal is to generate
a friendly environment for both the recruiters and applicants
in which the applicants can
for free-style communication,
actively discover more information about job positions instead

17

of passively being asked questions by recruiters. In the last
decade, emerging technologies (such as 5G, IoT, and DL)
have brought workers/employees many convenient alterna-
tives (fully remote and hybrid ofﬂine-online) to traditional
work; however, the metaverse will revolutionize the future
of work and the workplace. Recently, Facebook introduced
Horizon Workrooms [170], a well-designed meeting platform
that allows users, represented as avatars, to work, collaborate,
and communicate with others, besides training and coaching
activities, in the virtual space by VR devices.

Real estate: We have seen a huge investment from individual
investors and institutions to virtual land in the metaverse.
Some metaverses have already been released, including vir-
tual gaming platforms like the Sandbox and Axie Inﬁnity
and virtual worlds like Decentraland and Upland [171], in
which users can buy, sell, and trade things, including real
estate (plots of land and virtual houses). These digital real
estates, usually associated by non-fungible tokens (NFTs), are
limited by supply to guarantee their values over time based
on scarcity. The estate in the metaverse can be used as a
virtual place for building constructions (houses and ofﬁces) or
holding digital events (e.g., art exhibition and fashion show).
Moreover, the metaverse is another cost-efﬁcient channel for
real estate companies to ultimately show the property to clients
before making decision. With VR-aided immersive experience,
the clients can discover the property, including interior and
exterior from detailed furniture and overall structure, via VR
tours and interactive walkthroughs.

Decentralized ﬁnance: Based on an open system of ﬁnance,
decentralized ﬁnance (DeFi) is a cryptocurrency-based ﬁnan-
cial service which is regularly programmed through smart
contracts to build exchanges besides providing many major
services, such as lending, yield farming, and insurance with-
out centralized authorities. Different from centralized ﬁnance
which is controlled or managed by a centralized entity or a
person, DeFi, with blockchain technology, facilitates ﬁnancial
services from peer-to-peer and allows users to fully control
their assets while ensuring security and privacy. DeFi services
are usually delivered via decentralized applications (Dapps)
which are entirely built on open-sourced distributed platforms.
By integrating DeFi (including basic and professional services)
into the metaverse, users can make purchases virtual products
identiﬁed by NFTs in the digital world, but will receive the
real products in the real life. Furthermore, users can make
proﬁts in the metaverse based on the DeFi ecosystem with the
lending, borrowing, mining, and staking cryptocurrencies or
other tokens. Users can provide liquidity to the liquidity pool
with an underlying AI-based mechanism of a decentralized
exchange to earn incentives. Swapping tokens (can belong to
the same chain or different chains) is the basic service that is
prioritized to develop ﬁrst on any Dapps.

V. METAVERSE PROJECTS

This section brieﬂy introduces some attractive metaverse
projects, including Decentraland, Sandbox, Realy, Star Atlas,
Bit.Country, and DeHealth, which have applied AI to deliver
multifarious services and applications in the virtual world,

18

Fig. 12.

Inside the virtual worlds of different metaverse projects (left to right): Decentraland, Sandbox, Realy, Star Atlas, Bit.Country, and DeHealth.

from real estate to E-commerce and real estate. The landscapes
inside the virtual worlds of the projects are shown in Fig. 12,
for DeHealth, virtual doctors as avatars in the metaverse.

Decentraland3: This is a decentralized virtual reality plat-
form built on the Ethereum blockchain, in which users can
experience, create, and monetize assets, contents, and appli-
cations. In Decentraland, a virtual land is determined as a
non-fungible, transferrable, and scare digital asset recorded by
the Ethereum smart contract. Different from traditional virtual
worlds and social networks, Decentraland is not controlled
by any centralized organization; that is, no single agent has
a permission to modify the rules of software, content, eco-
nomic of cryptocurrency, or prevent others from accessing
the world, trading digital products, and experienced services.
The traversable 3D world in Decentraland allows embedding
immersive component and adjacency to creative content that
makes this project attractive and unique. A scripting pro-
gramming language of Decentraland enables developers to
easily code AI-based service-oriented applications for users
and accordingly encourage users to create new contents. Some
principal use cases are content curation, advertising, digital
collectibles, and social besides other minor ones, such as
education, virtual tourism, healthcare, and virtual shopping.
Regarding architecture, the Decentraland protocol has three
layers: a consensus layer to track land ownership and its
content, a land content layer to distribute the materials for
rendering via a decentralized storage system, and a real-time
layer to establish peer-to-peer connections for world viewing.
The native token, for in-world purchase goods and services, of
Decentraland is MANA, a fungible token built on the ERC-20
(Ethereum Request for Comments 20) protocol.

Sandbox4: The Sandbox metaverse is a user-generated de-
centralized Ethereum blockchain-based virtual world, which
allows users and gamers to build, own, and monetize gaming
experiences. Inspired by Minecraft5, the Sandbox metaverse
is ﬁrst built as a 2D mobile pixel game and then extended
to a fully-ﬂedged 3D world with a voxel gaming platform,
wherein users are capable of playing, sharing, collecting, and
trading virtual goods and services without central control.
Remarkably, creators can earn SAND, the native token of
Sandbox, by selling their creations on a marketplace with
secure copyright ownership which is associated and guaranteed
via NFT, i.e., every item in the metaverse will be authenticated
by a unique and immutable blockchain mechanism. As the
primary use cases of SAND in the Sandbox metaverse, the
token holders can access and experience the virtual world,
vote governance decisions via DAO mechanism, stake tokens

to earn revenues, and donate tokens as incentive to developers
for the metaverse growth. Besides the blockchain technology
with ERC-20 to generate SAND tokens and ERC-1155 for
digital assets trading, AI has been utilized in the Sandbox
metaverse. For example, as a powerful toolkit, gaming coders
can deploy ML models to improve the intelligence of virtual
agents/assistants and DL models to enhance the render quality,
and developers can leverage different AI frameworks to mini-
mize gaming crashes and errors. Building and training ML/DL
models are trouble-free with intuitive high-level APIs.

Realy6: From the real world to the fully virtual world by
creating a unique metaverse ecological world, the Realy meta-
verse is deﬁned as a super-realistic, futuristic, technologically
conscious world, in which E-commerce, social, gaming, and
trading are truly integrated to bring a seamless virtual-reality
experience to users. In the virtual world of Realy metaverse,
users can enjoy colorful journeys through their personalized
avatars with 3D virtual clothes which are available in the mar-
ketplace and linked with unique NFTs. There is an interesting
feature of Realy compared with other metaverse projects, that
is about the avatar control and management procedures. When
users are online, their avatars bring immersive experience.
When users are ofﬂine, the avatars are driven by a set AI-
based self-discipline systems. The whole virtual world is
automatically operated, controlled, and managed by a decen-
tralized DAO organization. Regarding technology in the Realy
metaverse, besides VR and blockchain, AI is adopted in many
aspects to generally improve the user immersive experience,
such as enhancing 3D visual rendered effects, boosting the
intelligence of avatars for realistic behaviors, and integrating
VR and holographic projection.

Star Atlas7: As one of the most recent innovative metaverse
projects, Star Atlas introduces a virtual gaming world built
on the integration of multiplayer video game platforms, real-
time immersive experiences with 3D rendered visualization,
blockchain-based decentralized ﬁnancial services, and AI-
powered game engines. Star Atlas helps to complete an
ecosystem on the Solana blockchain by ﬁlling the gap be-
tween metaverse and blockchain technology. In the gaming
metaverse of Star Atlas, users can trade digital assets like land,
equipment, crew, ship, and components by using the in-game
cryptocurrency token known as POLIS which can be used for
multiple cross-metaverse games. To tune the gameplay more
logical and realistic, ML algorithms are applied to improve
the intelligence of NPCs and AI agents in tactical planning of
actions and ﬁghting strategy in player-NPC combats.

Bit.Country8: As

a user-oriented metaverse project,

3https://decentraland.org/
4https://www.sandbox.game/en/
5https://www.minecraft.net/en-us

6https://realy.pro/
7https://staratlas.com/
8https://bit.country/

Bit.Country builds a 3D virtual world for everyone who can
set up its own community in the metaverse with rules and
operations to attract followers and contributors. By introducing
a new level of virtual social interaction, Bit.Country has two
platforms: one is traditional web view for content creation
and service provision, and another is addition 3D gaming
view for VR-aided immersive experience. In each individual
community conﬁgured by users holding platform tokens, all
the rules are managed and linked together by AI models to
ensure logical operations without any conﬂicts. Being more
than a virtual world, Bit.Country is capable of connecting
some virtual aspects to the real world to obtain a sustainable
future instead of joyful immersive experience.

in a full 3D virtual world.

DeHealth9: Introduced by a British non-proﬁt organiza-
tion, DeHealth is the world’s ﬁrst decentralized healthcare
metaverse, which allows doctors and patients to work and
interact with each other
In
the DeHealth metaverse, several high-quality healthcare and
medical services are delivered, such as health analytics on
the go, recommendations from advanced AI-bot, and real-
time dialogues with doctors and health experts worldwide.
To encourage informative data sharing activities, the meta-
verse has some trading cryptocurrency pools for users and
patients to earn assets by selling anonymized medical data.
The data collected from a decentralized network will be used
to build AI-based diagnostic models for diversiﬁed tasks in
the healthcare and medical domains. In the metaverse, doctors
and patients are able to communicate via a virtual space
replicating the real-world environment. Some virtual hospitals
and healthcare centers can be built to provide virtual services
with real data and diagnosis results. The DeHealth metaverse
is in consideration to be extended with additional education-
oriented services based on VR technology.

VI. CONCLUSION AND RESEARCH DIRECTIONS

In this survey, we have comprehensively investigated the
role of AI in the foundation of the metaverse and its potential
to enhance user immersive experience in the virtual world.
At the beginning of this work, the fundamental concepts of
the metaverse and AI techniques have been provided, along
with the role of AI in the metaverse. Subsequently, several
principal
technical aspects, such as NLP, machine vision,
blockchain, networking, DT, and neural interface, and many
application aspects, such as healthcare, manufacturing, smart
cities, gaming, E-commerce, and DeFi, have been analyzed.
The reviewed AI-based solutions have shown that AI has
great potential to toughen the systems infrastructure, uplift
the 3D immersive experience, and ﬂourish the built-in services
in the virtual worlds signiﬁcantly. Finally, we have examined
prominent metaverse projects, in which AI techniques were
used to sharpen the quality of services and encompass the
ecosystem of the metaverse.

We now delineate some AI research directions in the
metaverse. Being more advanced than regular virtual per-
sonal assistants which are developed for a general purpose
with simple dialog management, virtual customers/employee

9https://www.dehealth.world/

19

Fig. 13. General processing ﬂow of conversational AI to deliver contextual
and personal experience to users.

assistants powered by conversational AI can serve many
speciﬁc purposes of multi-level philosophical conversations to
enhance user interactive experience. Conversational AI with
a processing ﬂow in Fig. 13 is a set of technologies (e.g.,
automatic speech recognition, language processing, advanced
dialog management, and ML) that can offer human-like in-
teractions in the metaverse based on recognizing speech and
text, understanding intention, deciphering various languages,
and responding human-mimicking conversations over voice
modality.

Most of the current metaverse projects limit users to ex-
plore, own, and customize things in the virtual world. In
the future, users will be allowed to create hyperreal objects
and content easily and quickly with the help of AI. Various
kinds of hyperreal objects (e.g., faces, bodies, plants, ani-
mals, vehicles, buildings, and other inanimate objects) can be
remixed endlessly by users to make unique experiences and
excite creation. Accordingly, the combination of VR and AI-
based content generation can bring a complete immersion in
alternative realities. In this context, AI tools should be cheap
to everyone and have user-friendly interfaces. Further, ethical
issues relating to user-generated metaverse need to be seriously
examined with constraints and policies between users and
third-party organizations to mitigate risks and harmful threats
to individuals and societies when users synthesize hyperreal
media contents.

In many AI-aided services and applications in the meta-
verse, the decisions are made by AI agents, which are driven
by ML models as black boxes without the capability of in-
terpretability and explainability. Metaverse developers, virtual
world designers, and users cannot completely understand AI
decision-making processes (e.g., how and why an AI model
delivers a prediction), and probably trust them blindly. To
overcome these problems, explainable AI (XAI) is a set of
tools and methods to describe AI models, analyze their ex-
pected impacts, characterize model transparency, and examine
outcomes, allowing human users to entirely comprehend and
trust the AI models with end-to-end process monitoring and
accountability. With XAI, system engineers and data scientists
who apply AI in the metaverse (from system infrastructure to
services and applications in the virtual worlds) can understand
and explain what exactly is happening inside an AI model, how
is a speciﬁc result generated by an AI algorithm, and when is
a prediction model likely to crash. Besides increasing end user

Automated SpeechRecognitionListeningNatual LanguageUnderstandingComprehendingDialogManagementForming ResponseNatual LanguageGenerationOffering responseHi, I need toaccess a virtualmeeting.Sure, I need yourID to check theaccesspermission? Machine Learning and Deep Neural Networksconﬁdence, model auditability, and operative efﬁciency, XAI
mitigates legal risks and security threats of production AI in
the metaverse while guaranteeing users’ reliable experience.

REFERENCES

[1] M.

Kanterman

and
billion market,

$800
[Online]. Available:
metaverse-may-be-800-billion-market-next-tech-platform/

be
2021.
https://www.bloomberg.com/professional/blog/

“Metaverse may

platform,” Dec.

Naidu,
tech

N.
next

[2] Wikipedia, “Metaverse.” [Online]. Available: https://en.wikipedia.org/

wiki/Metaverse

[3] S.-M. Park and Y.-G. Kim, “A metaverse: Taxonomy, components,
applications, and open challenges,” IEEE Access, pp. 4209–4251, 2022.
metaverse
2021.
https://medium.com/building-the-metaverse/

“The
Available:

value-chain,”

Radoff,

[4] J.

Apr.

[Online].
the-metaverse-value-chain-afcf9e09e3a7

[5] S. B. Kotsiantis, I. Zaharakis, P. Pintelas et al., “Supervised machine
learning: A review of classiﬁcation techniques,” Emerging Artiﬁcial
Intelligence Applications in Computer Engineering, vol. 160, no. 1,
pp. 3–24, Jun. 2007.

[6] F. Hu, G.-S. Xia, Z. Wang, X. Huang, L. Zhang, and H. Sun, “Unsu-
pervised feature learning via spectral clustering of multidimensional
patches for remotely sensed scene classiﬁcation,” IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing,
vol. 8, no. 5, May 2015.

[7] R. Sheikhpour, M. A. Sarram, S. Gharaghani, and M. A. Z. Chahooki,
“A survey on semi-supervised feature selection methods,” Pattern
Recognition, vol. 64, pp. 141–158, Apr. 2017.

[8] J. E. Van Engelen and H. H. Hoos, “A survey on semi-supervised
learning,” Machine Learning, vol. 109, no. 2, pp. 373–440, 2020.
[9] B. Kiumarsi, K. G. Vamvoudakis, H. Modares, and F. L. Lewis,
“Optimal and autonomous control using reinforcement learning: A
survey,” IEEE Transactions on Neural Networks and Learning Systems,
vol. 29, no. 6, pp. 2042–2062, Jun. 2018.

[10] W. Chen, X. Qiu, T. Cai, H.-N. Dai, Z. Zheng, and Y. Zhang, “Deep
reinforcement learning for internet of things: A comprehensive survey,”
IEEE Communications Surveys & Tutorials, vol. 23, no. 3, pp. 1659–
1692, Thirdquarter 2021.

[11] C. D. Ho, T.-V. Nguyen, T. Huynh-The, T.-T. Nguyen, D. B. da Costa,
and B. An, “Short-packet communications in wireless-powered cogni-
tive IoT networks: Performance analysis and deep learning evaluation,”
IEEE Transactions on Vehicular Technology, vol. 70, no. 3, pp. 2894–
2899, Mar. 2021.

[12] C.-H. Hua, T. Huynh-The, and S. Lee, “Dran: Densely reversed atten-
tion based convolutional network for diabetic retinopathy detection,” in
Proc. 42nd Annual International Conference of the IEEE Engineering
in Medicine & Biology Society (EMBC), Montreal, QC, Canada, Jul.
2020, pp. 1992–1995.

[13] Q.-V. Pham, N. T. Nguyen, T. Huynh-The, L. B. Le, K. Lee, and W.-J.
Hwang, “Intelligent radio signal processing: A survey,” IEEE Access,
vol. 9, pp. 83 818–83 850, 2021.

[14] V. S. Lalapura, J. Amudha, and H. S. Satheesh, “Recurrent neural
networks for edge intelligence: A survey,” ACM Computing Surveys
(CSUR), vol. 54, no. 4, pp. 1–38, May 2022.

[15] R. C. Gonzalez, “Deep convolutional neural networks [lecture notes],”
IEEE Signal Processing Magazine, vol. 35, no. 6, pp. 79–87, Nov.
2018.

[16] A. D. Ramos, E. L´opez-Rubio, and E. J. Palomo, “The forbidden region
self-organizing map neural network,” IEEE Transactions on Neural
Networks and Learning Systems, vol. 31, no. 1, pp. 201–211, Jan. 2020.
[17] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
neural networks: A survey,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 43, no. 11, pp. 4037–4058, Nov. 2021.
[18] J. Song, H. Zhang, X. Li, L. Gao, M. Wang, and R. Hong, “Self-
supervised video hashing with hierarchical binary auto-encoder,” IEEE
Transactions on Image Processing, vol. 27, no. 7, pp. 3210–3221, Jul.
2018.

[19] S. K. Moore,

ai
supercomputer,” Jan. 2022. [Online]. Available: https://spectrum.ieee.
org/meta-ai-supercomputer

to build the world’s

fastest

“Meta

aims

[20] M. Daniluk, T. Rockt¨aschel, J. Welbl, and S. Riedel, “Frustratingly
short attention spans in neural language modeling,” arXiv preprint
arXiv:1702.04521, 2017.

20

[21] K. Benes, M. K. Baskar, and L. Burget, “Residual memory networks
in language modeling: Improving the reputation of feed-forward net-
works,” in INTERSPEECH, Stockholm, Sweden, Aug. 2017, pp. 284–
288.

[22] R. Jozefowicz, W. Zaremba, and I. Sutskever, “An empirical exploration
of recurrent network architectures,” in Proc. International Conference
on Machine Learning, Lille, France, Jul. 2015, pp. 2342–2350.
[23] N.-Q. Pham, G. Kruszewski, and G. Boleda, “Convolutional neural
network language models,” in Proc. Conference on Empirical Methods
in Natural Language Processing, Austin, Texas, Nov. 2016, pp. 1153–
1162.

[24] B. Liu and G. Yin, “Chinese document classiﬁcation with bi-directional
convolutional language model,” in Proc. 43rd International ACM SIGIR
Conference on Research and Development in Information Retrieval,
New York, NY, United States, Jul. 2020, pp. 1785–1788.

[25] B. Athiwaratkun and J. W. Stokes, “Malware classiﬁcation with LSTM
and GRU language models and a character-level CNN,” in Proc. IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), New Orleans, LA, USA, Jun. 2017, pp. 2482–2486.
[26] W. Ma, Y. Cui, C. Si, T. Liu, S. Wang, and G. Hu, “Char-
BERT: Character-aware pre-trained language model,” arXiv preprint
arXiv:2011.01513, 2020.

[27] R. Sharma, S. Morwal, B. Agarwal, R. Chandra, and M. S. Khan,
“A deep neural network-based model for named entity recognition for
hindi language,” Neural Computing and Applications, vol. 32, no. 20,
pp. 16 191–16 203, Apr. 2020.

[28] K. Khysru, D. Jin, Y. Huang, H. Feng, and J. Dang, “A tibetan language
model that considers the relationship between sufﬁxes and functional
words,” IEEE Signal Processing Letters, vol. 28, pp. 459–463, Feb.
2021.

[29] N. Jin, J. Wu, X. Ma, K. Yan, and Y. Mo, “Multi-task learning model
based on multi-scale CNN and LSTM for sentiment classiﬁcation,”
IEEE Access, vol. 8, pp. 77 060–77 072, 2020.

[30] J. Wang, L.-C. Yu, K. R. Lai, and X. Zhang, “Tree-structured regional
CNN-LSTM model for dimensional sentiment analysis,” IEEE/ACM
Transactions on Audio, Speech, and Language Processing, vol. 28, pp.
581–591, Dec. 2019.

[31] D. Liu, J. Fu, Q. Qu, and J. Lv, “BFGAN: backward and forward
generative adversarial networks for lexically constrained sentence gen-
eration,” IEEE/ACM Transactions on Audio, Speech, and Language
Processing, vol. 27, no. 12, pp. 2350–2361, Dec. 2019.

[32] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in
deep learning based natural language processing [review article],” IEEE
Computational Intelligence Magazine, vol. 13, no. 3, pp. 55–75, Aug.
2018.

[33] Z. Hu, A. Bulling, S. Li, and G. Wang, “Fixationnet: Forecasting eye
ﬁxations in task-oriented virtual environments,” IEEE Transactions on
Visualization and Computer Graphics, vol. 27, no. 5, pp. 2681–2690,
May 2021.

[34] D.-M. Pham, “Human identiﬁcation using neural network-based clas-
siﬁcation of periodic behaviors in virtual reality,” in Proc. IEEE
Conference on Virtual Reality and 3D User Interfaces (VR), Tuebin-
gen/Reutlingen, Germany, Mar. 2018, pp. 657–658.

[35] M. Zhu, Z. Sun, Z. Zhang, Q. Shi, T. Chen, H. Liu, and C. Lee,
“Sensory-glove-based human machine interface for augmented reality
(AR) applications,” in Proc. IEEE 33rd International Conference on
Micro Electro Mechanical Systems (MEMS), Vancouver, BC, Canada,
Apr. 2020, pp. 16–19.

[36] K. Kataoka, T. Yamamoto, M. Otsuki, F. Shibata, and A. Kimura, “A
new interactive haptic device for getting physical contact feeling of
virtual objects,” in Proc. IEEE Conference on Virtual Reality and 3D
User Interfaces (VR), Osaka, Japan, Aug. 2019, pp. 1323–1324.
[37] P. Wu, W. Ding, Z. You, and P. An, “Virtual reality video quality
assessment based on 3D convolutional neural networks,” in Proc. IEEE
International Conference on Image Processing (ICIP), Taipei, Taiwan,
Aug. 2019, pp. 3187–3191.

[38] J. Yang, T. Liu, B. Jiang, H. Song, and W. Lu, “3D panoramic virtual
reality video quality assessment based on 3D convolutional neural
networks,” IEEE Access, vol. 6, pp. 38 669–38 682, 2018.

[39] Y. Jin, M. Chen, T. Goodall, A. Patney, and A. C. Bovik, “Subjective
and objective quality assessment of 2D and 3D foveated video com-
pression in virtual reality,” IEEE Transactions on Image Processing,
pp. 5905–5919, Jun. 2021.

[40] Y. Han, C. Yu, D. Li, J. Zhang, and Y. Lai, “Accuracy analysis on 360°
virtual reality video quality assessment methods,” in Proc. IEEE/ACM
13th International Conference on Utility and Cloud Computing (UCC),
Leicester, UK, Dec. 2020, pp. 414–419.

[41] E. Barba, B. MacIntyre, and E. D. Mynatt, “Here we are! where are
we? locating mixed reality in the age of the smartphone,” Proceedings
of the IEEE, vol. 100, no. 4, pp. 929–936, Apr. 2012.

[42] M. Kersten-Oertel, P. Jannin, and D. L. Collins, “DVV: a taxonomy
for mixed reality visualization in image guided surgery,” IEEE Trans-
actions on Visualization and Computer Graphics, vol. 18, no. 2, pp.
332–352, Feb. 2012.

[43] D. Liu, M. Bober, and J. Kittler, “Visual semantic information pur-
suit: A survey,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 43, no. 4, pp. 1404–1422, Apr. 2021.

[44] T. Huynh-The, O. Banos, S. Lee, B. H. Kang, E.-S. Kim, and T. Le-
Tien, “NIC: A robust background extraction algorithm for foreground
detection in dynamic scenes,” IEEE Transactions on Circuits and
Systems for Video Technology, vol. 27, no. 7, pp. 1478–1490, Jul. 2017.
[45] S. Minaee, Y. Y. Boykov, F. Porikli, A. J. Plaza, N. Kehtarnavaz, and
D. Terzopoulos, “Image segmentation using deep learning: A survey,”
IEEE Transactions on Pattern Analysis and Machine Intelligence, Feb.
2021, (Early Access).

[46] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proc. IEEE Conference on Computer
Vision and Pattern Recognition, Boston, MA, USA, Oct. 2015, pp.
3431–3440.

[47] E. Shelhamer, J. Long, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 39, no. 4, pp. 640–651, Apr. 2017.

[48] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,
“Deeplab: Semantic image segmentation with deep convolutional nets,
atrous convolution, and fully connected CRFs,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 40, no. 4, pp. 834–848,
Apr. 2018.

[49] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, J. Feng, Y. Zhao,
and S. Yan, “STC: A simple to complex framework for weakly-
supervised semantic segmentation,” IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 39, no. 11, pp. 2314–2320,
Nov. 2017.

[50] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++:
Redesigning skip connections to exploit multiscale features in image
segmentation,” IEEE Transactions on Medical Imaging, vol. 39, no. 6,
pp. 1856–1867, Jun. 2020.

[51] C.-H. Hua, T. Huynh-The, S.-H. Bae, and S. Lee, “Cross-attentional
bracket-shaped convolutional network for semantic image segmenta-
tion,” Information Sciences, vol. 539, pp. 277–294, Oct. 2020.
[52] C.-H. Hua, T. Huynh-The, and S. Lee, “Convolutional networks with
bracket-style decoder for semantic scene segmentation,” in Proc. IEEE
International Conference on Systems, Man, and Cybernetics (SMC),
Miyazaki, Japan, Jan. 2018, pp. 2980–2985.

[53] Y. Liu, K. Chen, C. Liu, Z. Qin, Z. Luo, and J. Wang, “Structured
knowledge distillation for semantic segmentation,” in Proc. IEEE/CVF
Conference on Computer Vision and Pattern Recognition, Long Beach,
CA, USA, Jan. 2019, pp. 2604–2613.

[54] K.-W. Cheng, Y.-T. Chen, and W.-H. Fang, “Improved object detection
with iterative localization reﬁnement in convolutional neural networks,”
IEEE Transactions on Circuits and Systems for Video Technology,
vol. 28, no. 9, pp. 2261–2275, Sep. 2018.

[55] Y. Zhu, C. Zhao, H. Guo, J. Wang, X. Zhao, and H. Lu, “Attention
couplenet: Fully convolutional attention coupling network for object
detection,” IEEE Transactions on Image Processing, vol. 28, no. 1, pp.
113–126, Jan. 2019.

[56] N. Liu, J. Han, and M.-H. Yang, “Picanet: Pixel-wise contextual
attention learning for accurate saliency detection,” IEEE Transactions
on Image Processing, vol. 29, pp. 6438–6451, Apr. 2020.

[57] Y. Tang, J. Wang, X. Wang, B. Gao, E. Dellandr´ea, R. Gaizauskas, and
L. Chen, “Visual and semantic knowledge transfer for large scale semi-
supervised object detection,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 40, no. 12, pp. 3045–3058, Dec. 2018.
[58] X. Chen, K. Kundu, Y. Zhu, H. Ma, S. Fidler, and R. Urtasun, “3D
object proposals using stereo imagery for accurate object class detec-
tion,” IEEE Transactions on Pattern Analysis and Machine Intelligence,
vol. 40, no. 5, pp. 1259–1272, May 2018.

[59] M. Feng, S. Z. Gilani, Y. Wang, L. Zhang, and A. Mian, “Relation
graph network for 3D object detection in point clouds,” IEEE Trans-
actions on Image Processing, vol. 30, pp. 92–107, Oct. 2021.

[60] P. K. Lai, S. Xie, J. Lang, and R. Lagani`ere, “Real-time panoramic
depth maps from omni-directional stereo images for 6 DoF videos in
virtual reality,” in Proc. IEEE Conference on Virtual Reality and 3D
User Interfaces (VR), Osaka, Japan, Aug. 2019, pp. 405–412.

[61] C.-H. Yeh, C.-H. Huang, and L.-W. Kang, “Multi-scale deep residual
learning-based single image haze removal via image decomposition,”

21

IEEE Transactions on Image Processing, vol. 29, pp. 3153–3167, Dec.
2019.

[62] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional net-
image segmentation,” in Proc. International
works for biomedical
Conference on Medical Image Computing and Computer-Assisted
Intervention. Munich, Germany: Springer, Oct. 2015, pp. 234–241.

[63] A. Wang, W. Wang, J. Liu, and N. Gu, “AIPNet: Image-to-image
single image dehazing with atmospheric illumination prior,” IEEE
Transactions on Image Processing, vol. 28, no. 1, pp. 381–393, Jan.
2019.

[64] Z. Jin, M. Z. Iqbal, D. Bobkov, W. Zou, X. Li, and E. Steinbach, “A
ﬂexible deep CNN framework for image restoration,” IEEE Transac-
tions on Multimedia, vol. 22, no. 4, pp. 1055–1068, Apr. 2020.
[65] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense
network for image restoration,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 43, no. 7, pp. 2480–2495, Jul. 2021.

[66] S.-A. Wang, “Research on fuzzy image reconstruction method based
on real-time fusion technology of VR and AR,” in Proc. International
Conference on Virtual Reality and Intelligent Systems (ICVRIS), Jishou,
China, Sep. 2019, pp. 47–50.

[67] P. Saalfeld, C. B¨ottcher, F. Klink, and B. Preim, “VR system for the
restoration of broken cultural artifacts on the example of a funerary
monument,” in Proc. IEEE Virtual Reality and 3D User Interfaces
(VR), Lisboa, Portugal, Apr. 2021, pp. 739–748.

[68] A. Lahiri, S. Bairagya, S. Bera, S. Haldar, and P. K. Biswas,
“Lightweight modules for efﬁcient deep learning based image restora-
tion,” IEEE Transactions on Circuits and Systems for Video Technology,
vol. 31, no. 4, pp. 1395–1410, Apr. 2020.

[69] T. Huynh-The, B.-V. Le, S. Lee, T. Le-Tien, Y. Yoon et al., “Using
weighted dynamic range for histogram equalization to improve the
image contrast,” EURASIP Journal on Image and Video Processing,
vol. 2014, no. 1, pp. 1–17, Sep. 2014.

[70] W. Zhang, L. Dong, X. Pan, P. Zou, L. Qin, and W. Xu, “A survey
of restoration and enhancement for underwater images,” IEEE Access,
vol. 7, pp. 182 259–182 279, 2019.

[71] V. Syrris, S. Ferri, D. Ehrlich, and M. Pesaresi, “Image enhancement
and feature extraction based on low-resolution satellite data,” IEEE
Journal Of Selected Topics In Applied Earth Observations And Remote
Sensing, vol. 8, no. 5, pp. 1986–1995, May 2015.

[72] J. Wang and Y. Hu, “An improved enhancement algorithm based on
CNN applicable for weak contrast images,” IEEE Access, vol. 8, pp.
8459–8476, 2020.

[73] S. Mei, R. Jiang, X. Li, and Q. Du, “Spatial and spectral joint super-
resolution using convolutional neural network,” IEEE Transactions on
Geoscience and Remote Sensing, vol. 58, no. 7, pp. 4590–4603, 2020.
[74] J. Lee, J. Lee, and H.-J. Yoo, “SRNPU: An energy-efﬁcient CNN-based
super-resolution processor with tile-based selective super-resolution in
mobile devices,” IEEE Journal on Emerging and Selected Topics in
Circuits and Systems, vol. 10, no. 3, pp. 320–334, 2020.

[75] J. Yang, L. Xiao, Y.-Q. Zhao, and J. C.-W. Chan, “Hybrid local and
nonlocal 3-D attentive CNN for hyperspectral image super-resolution,”
IEEE Geoscience and Remote Sensing Letters, 2020.

[76] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
deep convolutional networks,” IEEE Transactions on Pattern Analysis
and Machine Intelligence, vol. 38, no. 2, pp. 295–307, Feb. 2016.
[77] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution
via sparse representation,” IEEE Transactions on Image Processing,
vol. 19, no. 11, pp. 2861–2873, Nov. 2010.

[78] Q. Dang, J. Yin, B. Wang, and W. Zheng, “Deep learning based 2D
human pose estimation: A survey,” Tsinghua Science and Technology,
vol. 24, no. 6, pp. 663–676, Dec. 2019.

[79] C. Zheng, W. Wu, T. Yang, S. Zhu, C. Chen, R. Liu, J. Shen,
N. Kehtarnavaz, and M. Shah, “Deep learning-based human pose
estimation: A survey,” arXiv preprint arXiv:2012.13392, 2020.
[80] M. Ramanathan, W.-Y. Yau, and E. K. Teoh, “Human action recognition
with video data: research and evaluation challenges,” IEEE Transac-
tions on Human-Machine Systems, vol. 44, no. 5, pp. 650–663, Oct.
2014.

[81] K. Chen, S. Gong, and T. Xiang, “Human pose estimation using
structural support vector machines,” in Proc. 2011 IEEE Interna-
tional Conference on Computer Vision Workshops (ICCV Workshops),
Barcelona, Spain, Nov. 2011, pp. 846–851.

[82] G. Rogez, P. Weinzaepfel, and C. Schmid, “LCR-net++: Multi-person
2D and 3D pose detection in natural images,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 42, no. 5, pp. 1146–
1161, May 2020.

[83] H. He, G. Liu, X. Zhu, L. He, and G. Tian, “Interacting multiple model-
based human pose estimation using a distributed 3D camera network,”
IEEE Sensors Journal, vol. 19, no. 22, pp. 10 584–10 590, Nov. 2019.
[84] L. Zhao, J. Xu, C. Gong, J. Yang, W. Zuo, and X. Gao, “Learning to
acquire the quality of human pose estimation,” IEEE Transactions on
Circuits and Systems for Video Technology, vol. 31, no. 4, pp. 1555–
1568, Apr. 2021.

[85] T. Huynh-The, B.-V. Le, S. Lee, and Y. Yoon, “Interactive activity
recognition using pose-based spatio–temporal relation features and
four-level pachinko allocation model,” Information Sciences, vol. 369,
pp. 317–333, Nov. 2016.

[86] N. A. Tu, T. Huynh-The, K. U. Khan, and Y.-K. Lee, “ML-HDP:
A hierarchical bayesian nonparametric model for recognizing human
actions in video,” IEEE Transactions on Circuits and Systems for Video
Technology, vol. 29, no. 3, pp. 800–814, Mar. 2019.

[87] T. Huynh-The, C.-H. Hua, N. A. Tu, T. Hur, J. Bang, D. Kim, M. B.
Amin, B. H. Kang, H. Seung, S.-Y. Shin et al., “Hierarchical topic
modeling with pose-transition feature for action recognition using 3D
skeleton data,” Information Sciences, vol. 444, pp. 20–35, May 2018.
[88] X. Wang, L. Gao, J. Song, and H. Shen, “Beyond frame-level CNN:
saliency-aware 3-D CNN with LSTM for video action recognition,”
IEEE Signal Processing Letters, vol. 24, no. 4, pp. 510–514, Apr. 2017.
[89] T. Huynh-The, C.-H. Hua, and D.-S. Kim, “Encoding pose features
to images with data augmentation for 3-D action recognition,” IEEE
Transactions on Industrial Informatics, vol. 16, no. 5, pp. 3100–3111,
May 2020.

[90] T. Huynh-The, C.-H. Hua, N. A. Tu, and D.-S. Kim, “Learning
3D spatiotemporal gait feature by convolutional network for person
identiﬁcation,” Neurocomputing, vol. 397, pp. 192–202, Jul. 2020.
[91] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, and D. Manocha, “DGaze:
CNN-based gaze prediction in dynamic scenes,” IEEE Transactions on
Visualization and Computer Graphics, vol. 26, no. 5, pp. 1902–1911,
May 2020.

[92] T. R. Gadekallu, Q.-V. Pham, D. C. Nguyen, P. K. R. Maddikunta,
N. Deepa, B. Prabadevi, P. N. Pathirana, J. Zhao, and W.-J. Hwang,
“Blockchain for edge of things: applications, opportunities, and chal-
lenges,” IEEE Internet of Things Journal, vol. 9, no. 2, pp. 964–988,
Jan. 2022.

[93] A. Cannav`o and F. Lamberti, “How blockchain, virtual reality, and aug-
mented reality are converging, and why,” IEEE Consumer Electronics
Magazine, vol. 10, no. 5, pp. 6–13, Sep. 2021.

[94] Y. Liu, F. R. Yu, X. Li, H. Ji, and V. C. M. Leung, “Blockchain and
machine learning for communications and networking systems,” IEEE
Communications Surveys & Tutorials, vol. 22, no. 2, pp. 1392–1431,
Secondquarter 2020.

[95] S. Tanwar, Q. Bhatia, P. Patel, A. Kumari, P. K. Singh, and W.-C. Hong,
“Machine learning adoption in blockchain-based smart applications:
The challenges, and a way forward,” IEEE Access, vol. 8, pp. 474–
488, 2019.

[96] P. Kumar, R. Kumar, G. Srivastava, G. P. Gupta, R. Tripathi, T. R.
Gadekallu, and N. Xiong, “PPSF: A privacy-preserving and secure
framework using blockchain-based machine-learning for IoT-driven
smart cities,” IEEE Transactions on Network Science and Engineering,
vol. 8, no. 3, pp. 2326–2341, Jul.-Sep. 2021.

[97] M. A. Khan, S. Abbas, A. Rehman, Y. Saeed, A. Zeb, M. I. Uddin,
N. Nasser, and A. Ali, “A machine learning approach for blockchain-
based smart home networks security,” IEEE Network, vol. 35, no. 3,
pp. 223–229, May/Jun. 2021.

[98] J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo, “DeepChain:
Auditable and privacy-preserving deep learning with blockchain-based
incentive,” IEEE Transactions on Dependable and Secure Computing,
vol. 18, no. 5, pp. 2438–2455, Sep.-Oct. 2021.

[99] D. C. Nguyen, P. N. Pathirana, M. Ding, and A. Seneviratne, “Privacy-
preserved task ofﬂoading in mobile blockchain with deep reinforcement
learning,” IEEE Transactions on Network and Service Management,
vol. 17, no. 4, pp. 2536–2549, Dec. 2020.

[100] S. Wang, S. Sun, X. Wang, Z. Ning, and J. J. P. C. Rodrigues, “Secure
crowdsensing in 5G internet of vehicles: When deep reinforcement
learning meets blockchain,” IEEE Consumer Electronics Magazine,
vol. 10, no. 5, pp. 72–81, Sep. 2021.

[101] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain
and federated learning for privacy-preserved data sharing in industrial
IoT,” IEEE Transactions on Industrial Informatics, vol. 16, no. 6, pp.
4177–4186, Jun. 2020.

[102] S. Fan, H. Zhang, Y. Zeng, and W. Cai, “Hybrid blockchain-based
resource trading system for federated learning in edge computing,”
IEEE Internet of Things Journal, vol. 8, no. 4, pp. 2252–2264, Feb.
2021.

22

[103] H. Liu, S. Zhang, P. Zhang, X. Zhou, X. Shao, G. Pu, and Y. Zhang,
“Blockchain and federated learning for collaborative intrusion detec-
tion in vehicular edge computing,” IEEE Transactions on Vehicular
Technology, vol. 70, no. 6, pp. 6073–6084, Jun. 2021.

[104] P. Ocheja, B. Flanagan, and H. Ogata, “Connecting decentralized learn-
ing records: a blockchain based learning analytics platform,” in Proc.
8th International Conference on Learning Analytics and Knowledge,
Sydney, New South Wales, Australia, Mar. 2018, pp. 265–269.
[105] U. Majeed, L. U. Khan, A. Yousafzai, Z. Han, B. J. Park, and
C. S. Hong, “ST-BFL: A structured transparency empowered cross-
silo federated learning on the blockchain framework,” IEEE Access,
vol. 9, pp. 155 634–155 650, 2021.

[106] J. Li, Y. Shao, K. Wei, M. Ding, C. Ma, L. Shi, Z. Han, and H. V.
Poor, “Blockchain assisted decentralized federated learning (BLADE-
FL): Performance analysis and resource allocation,” arXiv preprint
arXiv:2101.06905, 2021.

[107] M. Chen, U. Challita, W. Saad, C. Yin, and M. Debbah, “Artiﬁcial
neural networks-based machine learning for wireless networks: A
tutorial,” IEEE Communications Surveys & Tutorials, vol. 21, no. 4,
pp. 3039–3071, Fourthquarter 2019.

[108] C. She, C. Sun, Z. Gu, Y. Li, C. Yang, H. V. Poor, and B. Vucetic,
“A tutorial on ultrareliable and low-latency communications in 6G:
Integrating domain knowledge into deep learning,” Proceedings of the
IEEE, vol. 109, no. 3, pp. 204–246, Mar. 2021.

[109] C. She, R. Dong, Z. Gu, Z. Hou, Y. Li, W. Hardjawana, C. Yang,
L. Song, and B. Vucetic, “Deep learning for ultra-reliable and low-
latency communications in 6G networks,” IEEE Network, vol. 34, no. 5,
pp. 219–225, Sep./Oct. 2020.

[110] M. Alsenwi, N. H. Tran, M. Bennis, S. R. Pandey, A. K. Bairagi,
and C. S. Hong, “Intelligent resource slicing for eMBB and URLLC
coexistence in 5G and beyond: A deep reinforcement learning based
approach,” IEEE Transactions on Wireless Communications, vol. 20,
no. 7, pp. 4585–4600, Jul. 2021.

[111] B. Gu, X. Zhang, Z. Lin, and M. Alazab, “Deep multiagent
reinforcement-learning-based resource allocation for internet of con-
trollable things,” IEEE Internet of Things Journal, vol. 8, no. 5, pp.
3066–3074, Mar. 2021.

[112] A. Azari, M. Ozger, and C. Cavdar, “Risk-aware resource allocation
for URLLC: Challenges and strategies with machine learning,” IEEE
Communications Magazine, vol. 57, no. 3, pp. 42–48, Mar. 2019.

[113] T. Huynh-The, C.-H. Hua, Q.-V. Pham, and D.-S. Kim, “MCNet: An
efﬁcient CNN architecture for robust automatic modulation classiﬁca-
tion,” IEEE Communications Letters, vol. 24, no. 4, pp. 811–815, Apr.
2020.

[114] G. B. Tunze, T. Huynh-The, J.-M. Lee, and D.-S. Kim, “Sparsely
connected CNN for efﬁcient automatic modulation recognition,” IEEE
Transactions on Vehicular Technology, vol. 69, no. 12, pp. 15 557–
15 568, Dec. 2020.

[115] C. Luo, J. Ji, Q. Wang, X. Chen, and P. Li, “Channel state information
prediction for 5G wireless communications: A deep learning approach,”
IEEE Transactions on Network Science and Engineering, vol. 7, no. 1,
pp. 227–236, Jan.-Mar. 2020.

[116] S. Guo, Y. Lin, S. Li, Z. Chen, and H. Wan, “Deep spatial–temporal
3D convolutional neural networks for trafﬁc data forecasting,” IEEE
Transactions on Intelligent Transportation Systems, vol. 20, no. 10,
pp. 3913–3926, Oct. 2019.

[117] F. Tao, H. Zhang, A. Liu, and A. Y. C. Nee, “Digital twin in industry:
State-of-the-art,” IEEE Transactions on Industrial Informatics, vol. 15,
no. 4, pp. 2405–2415, Apr. 2019.

[118] D. Chen, D. Wang, Y. Zhu, and Z. Han, “Digital twin for federated
analytics using a bayesian approach,” IEEE Internet of Things Journal,
vol. 8, no. 22, pp. 16 301–16 312, Nov. 2021.

[119] M. M. Rathore, S. A. Shah, D. Shukla, E. Bentafat, and S. Bakiras,
“The role of AI, machine learning, and big data in digital twinning:
A systematic literature review, challenges, and opportunities,” IEEE
Access, vol. 9, pp. 32 030–32 052, 2021.

[120] H. Darvishi, D. Ciuonzo, E. R. Eide, and P. S. Rossi, “Sensor-fault
detection, isolation and accommodation for digital twins via modular
data-driven architecture,” IEEE Sensors Journal, vol. 21, no. 4, pp.
4827–4838, Feb. 2021.

[121] Q. Wang, W. Jiao, P. Wang, and Y. Zhang, “Digital twin for human-
robot interactive welding and welder behavior analysis,” IEEE/CAA
Journal of Automatica Sinica, vol. 8, no. 2, pp. 334–343, Feb. 2021.
[122] H. Elayan, M. Aloqaily, and M. Guizani, “Digital twin for intelligent
context-aware IoT healthcare systems,” IEEE Internet of Things Jour-
nal, vol. 8, no. 23, pp. 16 749–16 757, Dec. 2021.

[123] A. Ghandar, A. Ahmed, S. Zulﬁqar, Z. Hua, M. Hanai, and G. Theodor-
opoulos, “A decision support system for urban agriculture using digital
twin: A case study with aquaponics,” IEEE Access, vol. 9, pp. 35 691–
35 708, 2021.

[124] X. Xu, B. Shen, S. Ding, G. Srivastava, M. Bilal, M. R. Khosravi,
V. G. Menon, M. A. Jan, and M. Wang, “Service ofﬂoading with
deep Q-network for digital twinning-empowered internet of vehicles in
edge computing,” IEEE Transactions on Industrial Informatics, vol. 18,
no. 2, pp. 1414–1423, Feb. 2022.

[125] R. Dong, C. She, W. Hardjawana, Y. Li, and B. Vucetic, “Deep learning
for hybrid 5G services in mobile edge computing systems: Learn from a
digital twin,” IEEE Transactions on Wireless Communications, vol. 18,
no. 10, pp. 4692–4707, Oct. 2019.

[126] W. Sun, S. Lei, L. Wang, Z. Liu, and Y. Zhang, “Adaptive federated
internet of things,” IEEE
learning and digital
twin for industrial
Transactions on Industrial Informatics, vol. 17, no. 8, pp. 5605–5614,
Aug. 2021.

[127] S. L. Bernal, A. H. Celdr´an, G. M. P´erez, M. T. Barros, and S. Bala-
subramaniam, “Security in brain-computer interfaces: State-of-the-art,
opportunities, and future challenges,” ACM Computing Surveys, vol. 54,
no. 1, pp. 1–35, Jan. 2022.

[128] H. He and D. Wu, “Transfer learning for brain–computer interfaces:
A euclidean space data alignment approach,” IEEE Transactions on
Biomedical Engineering, vol. 67, no. 2, pp. 399–410, Feb. 2020.
[129] B. Abibullaev and A. Zollanvari, “Learning discriminative spatiospec-
tral features of ERPs for accurate brain–computer interfaces,” IEEE
Journal of Biomedical and Health Informatics, vol. 23, no. 5, pp. 2009–
2020, Sep. 2019.

[130] A. Matran-Fernandez and R. Poli, “Brain–computer interfaces for de-
tection and localization of targets in aerial images,” IEEE Transactions
on Biomedical Engineering, vol. 64, no. 4, pp. 959–969, Apr. 2017.

[131] Z. Lv, L. Qiao, Q. Wang, and F. Piccialli, “Advanced machine-learning
methods for brain-computer interfacing,” IEEE/ACM Transactions on
Computational Biology and Bioinformatics, vol. 18, no. 5, pp. 1688–
1698, 2021.

[132] R. Ma, T. Yu, X. Zhong, Z. L. Yu, Y. Li, and Z. Gu, “Capsule network
for ERP detection in brain-computer interface,” IEEE Transactions on
Neural Systems and Rehabilitation Engineering, vol. 29, pp. 718–730,
Apr. 2021.

[133] S. Sakhavi, C. Guan, and S. Yan, “Learning temporal information for
brain-computer interface using convolutional neural networks,” IEEE
Transactions on Neural Networks and Learning Systems, vol. 29,
no. 11, pp. 5619–5629, Nov. 2018.

[134] J.-H. Jeong, K.-H. Shim, D.-J. Kim, and S.-W. Lee, “Brain-controlled
robotic arm system based on multi-directional CNN-BiLSTM network
using EEG signals,” IEEE Transactions on Neural Systems and Reha-
bilitation Engineering, vol. 28, no. 5, pp. 1226–1238, May 2020.
[135] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with
convolutions,” in Proc. IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), Boston, MA, Jun. 2015, pp. 1–9.
[136] E. Santamar´ıa-V´azquez, V. Mart´ınez-Cagigal, F. Vaquerizo-Villar, and
R. Hornero, “EEG-Inception: A novel deep convolutional neural net-
work for assistive ERP-based brain-computer interfaces,” IEEE Trans-
actions on Neural Systems and Rehabilitation Engineering, vol. 28,
no. 12, pp. 2773–2782, Dec. 2020.

[137] O. Banos, J. Bang, T. Hur, M. H. Siddiqi, H.-T. Thien, L.-B. Vui,
W. Ali Khan, T. Ali, C. Villalonga, and S. Lee, “Mining human
behavior for health promotion,” in Proc. 37th Annual International
Conference of the IEEE Engineering in Medicine and Biology Society
(EMBC), Milan, Italy, Aug. 2015, pp. 5062–5065.

[138] T. Huynh-The, C.-H. Hua, N. A. Tu, and D.-S. Kim, “Physical activity
recognition with statistical-deep fusion model using multiple sensory
data for smart health,” IEEE Internet of Things Journal, vol. 8, no. 3,
pp. 1533–1543, Feb. 2021.

[139] T. Hur, J. Bang, J. Lee, J.-I. Kim, S. Lee et al., “Iss2Image: A novel
signal-encoding technique for CNN-based human activity recognition,”
Sensors, vol. 18, no. 11, p. 3910, Nov. 2018.

[140] X. Qian, H. Chen, H. Jiang, J. Green, H. Cheng, and M.-C. Huang,
“Wearable computing with distributed deep learning hierarchy: A study
of fall detection,” IEEE Sensors Journal, vol. 20, no. 16, pp. 9408–
9416, Aug. 2020.

[141] H. Li, A. Shrestha, H. Heidari, J. Le Kernec, and F. Fioranelli, “Bi-
LSTM network for multimodal continuous human activity recognition
and fall detection,” IEEE Sensors Journal, vol. 20, no. 3, pp. 1191–
1201, Feb. 2020.

23

[142] C.-H. Hua, T. Huynh-The, K. Kim, S.-Y. Yu, T. Le-Tien, G. H. Park,
J. Bang, W. A. Khan, S.-H. Bae, and S. Lee, “Bimodal learning via
trilogy of skip-connection deep networks for diabetic retinopathy risk
progression identiﬁcation,” International Journal of Medical Informat-
ics, vol. 132, p. 103926, Dec. 2019.

[143] C.-H. Hua, K. Kim, T. Huynh-The, J. I. You, S.-Y. Yu, T. Le-Tien,
S.-H. Bae, and S. Lee, “Convolutional network with twofold feature
augmentation for diabetic retinopathy recognition from multi-modal
images,” IEEE Journal of Biomedical and Health Informatics, vol. 25,
no. 7, pp. 2686–2697, Jul. 2021.

[144] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y. Zhang, “SMU-Net:
Saliency-guided morphology-aware U-Net for breast lesion segmen-
tation in ultrasound image,” IEEE Transactions on Medical Imaging,
vol. 41, no. 2, pp. 476–490, Feb. 2022.

[145] H. Qi, N. Fuin, G. Cruz, J. Pan, T. Kuestner, A. Bustin, R. M. Botnar,
and C. Prieto, “Non-rigid respiratory motion estimation of whole-
heart coronary MR images using unsupervised deep learning,” IEEE
Transactions on Medical Imaging, vol. 40, no. 1, pp. 444–454, Jan.
2021.

[146] T. Hassanzadeh, D. Essam, and R. Sarker, “2D to 3D evolutionary deep
convolutional neural networks for medical image segmentation,” IEEE
Transactions on Medical Imaging, vol. 40, no. 2, pp. 712–721, Feb.
2021.

[147] J. Torner, S. Skouras, J. L. Molinuevo, J. D. Gispert, and F. Alpiste,
“Multipurpose virtual reality environment for biomedical and health
applications,” IEEE Transactions on Neural Systems and Rehabilitation
Engineering, vol. 27, no. 8, pp. 1511–1520, Aug. 2019.

[148] S. Doltsinis, P. Ferreira, and N. Lohse, “A symbiotic human–machine
learning approach for production ramp-up,” IEEE Transactions on
Human-Machine Systems, vol. 48, no. 3, pp. 229–240, Jun. 2018.

[149] M. Azamfar, X. Li, and J. Lee, “Deep learning-based domain adaptation
method for fault diagnosis in semiconductor manufacturing,” IEEE
Transactions on Semiconductor Manufacturing, vol. 33, no. 3, pp. 445–
453, Aug. 2020.

[150] H. Lee, H. Jeong, G. Koo, J. Ban, and S. W. Kim, “Attention
recurrent neural network-based severity estimation method for interturn
short-circuit fault in permanent magnet synchronous machines,” IEEE
Transactions on Industrial Electronics, vol. 68, no. 4, pp. 3445–3453,
Apr. 2021.

[151] Z. Y. Xue, K. S. Xiahou, M. S. Li, T. Y. Ji, and Q. H. Wu, “Diagnosis
of multiple open-circuit switch faults based on long short-term memory
network for DFIG-based wind turbine systems,” IEEE Journal of
Emerging and Selected Topics in Power Electronics, vol. 8, no. 3, pp.
2600–2610, Sep. 2020.

[152] L. Guo, Y. Lei, S. Xing, T. Yan, and N. Li, “Deep convolutional
transfer learning network: A new method for intelligent fault diagnosis
of machines with unlabeled data,” IEEE Transactions on Industrial
Electronics, vol. 66, no. 9, pp. 7316–7325, Sep. 2019.

[153] H. Alkhalefah, J. E. A. Qudeiri, U. Umer, M. H. Abidi, and A. Elkaseer,
“Development of an efﬁcient prediction model for optimal design of
serial production lines,” IEEE Access, vol. 9, pp. 61 807–61 818, 2021.
[154] S. Huang, Y. Guo, D. Liu, S. Zha, and W. Fang, “A two-stage
transfer learning-based deep learning approach for production progress
prediction in IoT-enabled manufacturing,” IEEE Internet of Things
Journal, vol. 6, no. 6, pp. 10 627–10 638, Dec. 2019.

[155] R. G. Lins and S. N. Givigi, “Cooperative robotics and machine
learning for smart manufacturing: Platform design and trends within
the context of industrial internet of things,” IEEE Access, vol. 9, pp.
95 444–95 455, 2021.

[156] V. Kohli, U. Tripathi, V. Chamola, B. K. Rout, and S. S. Kanhere,
“A review on virtual reality and augmented reality use-cases of brain
computer interface based applications for smart cities,” Microproces-
sors and Microsystems, vol. 88, p. 104392, Feb. 2022.

[157] M. Mohammadi and A. Al-Fuqaha, “Enabling cognitive smart cities
using big data and machine learning: Approaches and challenges,”
IEEE Communications Magazine, vol. 56, no. 2, pp. 94–101, Feb. 2018.
[158] S. Park, H.-S. Cha, and C.-H. Im, “Development of an online home
appliance control system using augmented reality and an SSVEP-based
brain–computer interface,” IEEE Access, vol. 7, pp. 163 604–163 614,
2019.

[159] F. Zhu, Y. Lv, Y. Chen, X. Wang, G. Xiong, and F.-Y. Wang, “Parallel
transportation systems: Toward IoT-enabled smart urban trafﬁc control
and management,” IEEE Transactions on Intelligent Transportation
Systems, vol. 21, no. 10, pp. 4063–4071, Oct. 2020.

[160] S. Du, T. Li, Y. Yang, and S.-J. Horng, “Deep air quality forecasting
using hybrid deep learning framework,” IEEE Transactions on Knowl-
edge and Data Engineering, vol. 33, no. 6, pp. 2412–2424, Jun. 2021.

[161] S. A. Bhat and N.-F. Huang, “Big data and AI revolution in precision
agriculture: Survey and challenges,” IEEE Access, vol. 9, pp. 110 209–
110 222, 2021.

[162] G. Synnaeve and P. Bessi`ere, “Multiscale bayesian modeling for
RTS games: An application to StarCraft AI,” IEEE Transactions on
Computational Intelligence and AI in Games, vol. 8, no. 4, pp. 338–
350, Dec. 2016.

[163] A. Liaqat, M. A. Sindhu, and G. F. Siddiqui, “Metamorphic testing of
an artiﬁcially intelligent chess game,” IEEE Access, vol. 8, pp. 174 179–
174 190, 2020.

[164] I. Oh, S. Rho, S. Moon, S. Son, H. Lee, and J. Chung, “Creating
pro-level AI for a real-time ﬁghting game using deep reinforcement
learning,” IEEE Transactions on Games, Jan. 2021, (Early Access).

[165] N. A. Barriga, M. Stanescu, F. Besoain, and M. Buro, “Improving
RTS game AI by supervised policy learning, tactical search, and deep
reinforcement learning,” IEEE Computational Intelligence Magazine,
vol. 14, no. 3, pp. 8–18, Aug. 2019.

[166] G. N. Yannakakis and J. Togelius, “A panorama of artiﬁcial and com-
putational intelligence in games,” IEEE Transactions on Computational

24

Intelligence and AI in Games, vol. 7, no. 4, pp. 317–335, Dec. 2015.
[167] M. Frutos-Pascual and B. G. Zapirain, “Review of the use of ai
techniques in serious games: Decision making and machine learning,”
IEEE Transactions on Computational Intelligence and AI in Games,
vol. 9, no. 2, pp. 133–152, Jun. 2017.

[169] L.

[168] E. Riedl, “A look into e-commerce and more in the metaverse,”
[Online]. Available: https://www.mastercard.com/news/

Sep. 2021.
perspectives/2021/ecommerce-metaverse-augmented-mixed-reality/
for
metaverse,

get
a
experts
2021.
hired
[Online].
https://www.cnbc.com/2021/11/30/
looking-for-a-job-you-might-get-hired-via-the-metaverse-experts-say.
html

Handley,
via

you might

Available:

“Looking

Nov.

say,”

job?

the

[170] Meta, “Introducing horizon workrooms: Remote collaboration reimag-
ined,” Aug. 2021. [Online]. Available: https://about.fb.com/news/2021/
08/introducing-horizon-workrooms-remote-collaboration-reimagined/
[171] D. Kamin, “Investors snap up metaverse real estate in a virtual
land boom,” Nov. 2021. [Online]. Available: https://www.nytimes.
com/2021/11/30/business/metaverse-real-estate.html

