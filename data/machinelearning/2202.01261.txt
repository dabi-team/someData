Efficient Memory Partitioning in Software Defined
Hardware

Matthew Feldman
Electrical Engineering
Stanford University
United States
mattfel@stanford.edu

Tian Zhao
Electrical Engineering
Stanford University
United States
tianzhao@stanford.edu

Kunle Olukotun
Electrical Engineering and Computer
Science
Stanford University
United States
kunle@stanford.edu

2
2
0
2

r
a

M
9
2

]

R
A
.
s
c
[

3
v
1
6
2
1
0
.
2
0
2
2
:
v
i
X
r
a

Abstract
As programmers turn to software-defined hardware (SDH)
to maintain a high level of productivity while programming
hardware to run complex algorithms, heavy-lifting must
be done by the compiler to automatically partition on-chip
arrays. In this paper, we introduce an automatic memory
partitioning system that can quickly compute more efficient
partitioning schemes than prior systems. Our system em-
ploys a variety of resource-saving optimizations and an ML
cost model to select the best partitioning scheme from an
array of candidates. We compared our system against vari-
ous state-of-the-art SDH compilers and FPGAs on a variety
of benchmarks and found that our system generates solu-
tions that, on average, consume 40.3% fewer logic resources,
78.3% fewer FFs, 54.9% fewer Block RAMs (BRAMs), and
100% fewer DSPs.

ACM Reference Format:
Matthew Feldman, Tian Zhao, and Kunle Olukotun. 2022. Efficient
Memory Partitioning in Software Defined Hardware. In Proceedings
of Work in Progress (arXiv Preprint). ACM, New York, NY, USA,
12 pages.

1 Introduction
In recent years, there has been a growing demand for com-
pute devices that can efficiently run increasingly complex
algorithms. Experts from a wide variety of domains, such
as Machine Learning [4], Computational Physics [8], and
Genomics [39], are facing unprecedented challenges in pro-
cessing massive data sets quickly and efficiently. Spatial ar-
chitectures, such as field-programmable gate arrays (FPGAs)
and reconfigurable dataflow architectures (RDAs), provide
performance-per-watt advantages over CPUs and GPUs [20]
and are becoming popular for these kinds of domain experts.
They allow the programmer to create a digital circuit that can
act as an accelerator for a particular algorithm. Hardware
accelerators perform computation by flowing data through a
pipelined circuit, rather than by serially executing a sequence
of instructions. Unfortunately, they traditionally require com-
plicated register-transfer level (RTL) languages, like VHDL

arXiv Preprint, Work in Progress, Palo Alto, California
2022.

and Verilog, that domain experts often find prohibitively
difficult to use.

For this reason, software-defined hardware (SDH) has
emerged as an active research field to solve this problem of
programmability without sacrificing performance. For cer-
tain domains, there are domain specific languages (DSLs),
such as Halide [28], DNNWeaver [36], Aetherling [15], RIPL
[38], and SODA [41], which limit what the programmer can
express in order to generate high performance designs for
a particular domain. There are also numerous general pur-
pose SDH languages available today, including Vivado High
Level Synthesis (HLS) [1], SDAccel [3], OpenCL [2], and
Spatial [23]. These languages are embedded in high level
software languages, such as C or Scala, and allow the pro-
grammer to use high level abstractions to create arbitrary
hardware accelerators.

SDH languages allow the programmer to exploit both
pipeline and spatial parallelism by nesting loops and an-
notating iterators. Pipeline parallelism refers to concurrent
execution of consecutive stages in a computation graph, and
spatial parallelism refers to the concurrent execution of the
same stage in a computation graph. These frameworks also
allow the programmer to declare multidimensional arrays,
or memories, that reside on-chip. The interplay between
parallelism and how these memories are accessed in general-
purpose SDH languages is the focus of this paper. "Memory
banking" is the process in which the compiler decides how
to manage concurrent accesses to on-chip memories by par-
titioning them across multiple physical resources.

In the worst case, a banking system that is not well opti-
mized could increase the compile time of an SDH program by
minutes to hours, as we show in this paper. This increase in
the programmer’s development cycle could significantly hin-
der the productivity of the SDH tool. Furthermore, a poorly
optimized banking system could also result in wildly ineffi-
cient banking schemes. A naive banking scheme could result
in more resources being dedicated to the implementation
of the banking scheme than those dedicated to the datap-
ath of the algorithm that does actual computational work.
These two issues could render an SDH framework entirely
unusable for a performance-oriented programmer.

 
 
 
 
 
 
arXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

Figure 1. Top level process of banking system.

State-of-the-art implementations of banking analysis were
generally designed for certain kinds of compute patterns in
mind, such as linear algebra [23] and stencil operations [42].
However, these approaches do not scale when presented
with more challenging banking problems that are found
in applications with more dynamic access patterns, larger
parallelization factors, and more complex control structures.
In this paper, we introduce a novel banking system capable
of automatically solving challenging banking problems with
solutions that are more efficient than existing state-of-the-art
tools. Our system makes the following contributions:

• Use heuristics to quickly identify a collection of valid

banking schemes (Section 3.3).

• Apply targeted transformations to the datapath asso-

ciated with each solution. (Section 3.4).

• Rapidly estimate resource utilization for each solution
using a machine learning (ML) pipeline (Section 3.5).

Figure 1 shows the conceptual view of our system. The in-
put is logical accesses to an array, coupled with information
about the concurrency of these accesses. The output is an
elaborated, retimed circuit that implements memory virtual-
ization to satisfy the constraints of the original program.

We compared our system against various state-of-the-art
SDH compilers on a variety of benchmarks and found that
our system generates solutions that, on average, consume
40.3% fewer logic resources, 78.3% fewer FFs, 54.9% fewer
BRAMs, and 100% fewer DSPs without losing performance.

2 Background
Memory banking is the process of partitioning a multidimen-
sional array so that it can be implemented in hardware as
a collection of physical BRAM resources and surrounding
logic to index into them. In this paper, we refer to the access
pattern to an array in the program as the logical access (i.e.
arr[2*i+3]), and the circuits which connect to BRAMs as
physical accesses.

(a)

(b)

Figure 2. Example 4x4 array partitioned into 4 banks using
(a) hyperplane and (b) lattice. Color represents bank

2.1 Representing The Problem Using the

Polyhedral Model

The polyhedral model is a formalism for representing a pro-
gram’s iteration space, memory space, and access patterns
algebraically for a compiler to statically analyze [21]. It has
been used by others to solve various problems such as synthe-
sis, verification, and optimization of systolic arrays, detecting
parallelism and efficiently scheduling programs in parallel
compute environments, and on-chip memory partitioning
in dataflow architectures [5, 16–18, 29–33, 40, 42].

In this work, we use it as a tool to solve the banking
problem, and briefly summarize the important concepts used
in prior work as it relates to memory partitioning on FPGAs.

Definition 2.1. A Polyhedron is a set of points in n-dimensional
space that satisfy a set of linear inequalities. Namely, the
set of points 𝑃 = { (cid:174)𝑥 ∈ Q𝑛 |𝐴 · (cid:174)𝑥 ≤ (cid:174)𝑏} where Q𝑛 is an n-
dimensional vector of rational numbers, A is a matrix where
𝐴𝑖,𝑗 ∈ Q, and (cid:174)𝑏 ∈ Q𝑛.

Definition 2.2. A Polytope is a bounded polyhedron, e.g.
one that contains a finite amount of integer points.

Definition 2.3. A Parallelotope is a polytope where every
pair of opposite facets are congruent and parallel, i.e. the
n-dimensional generalization of a parallelogram (2D) or par-
allelepiped (3D).

Definition 2.4. The iterator space is a polytope defined by
the iterators, (cid:174)𝑖, of a loop nest.

Definition 2.5. An array is an n-dimensional on-chip mem-
ory whose addresses represent a polytope

Definition 2.6. An access pattern for access 𝑎 is an expres-
sion representing the mapping from the iterator space to an
n-dimensional reference to the array polytope.

Definition 2.7. An access group is a collection of access pat-
terns which can be active during the same cycle at runtime.

2.2 Partitioning Methods

There are two common methods for partitioning the mem-
ory: hyperplane partitioning [42] and lattice partitioning [10],

Efficient Memory Partitioning in Software Defined Hardware

arXiv Preprint, Work in Progress, Palo Alto, California

Type Name Description

Input

Solve

Metrics

(cid:174)𝐷
(cid:174)𝑎

(cid:174)𝑖
(cid:174)𝑥

k
(cid:174)𝑁
(cid:174)𝐵
(cid:174)𝛼
(cid:174)𝑃

𝐹𝑂𝑎

𝐹 𝐼𝑏

(cid:174)𝛿

Dimensions of memory, | (cid:174)𝐷 | = 𝑛
Logical accesses, grouped by concur-
rency
iterators in the scope of an access
Memory address reference for an ac-
cess, | (cid:174)𝑥 | = 𝑛
Number of ports on underlying BRAM
Number of banks, | (cid:174)𝑁 | ∈ {1, 𝑛}
Blocking factor, | (cid:174)𝐵| = | (cid:174)𝑁 |
Partition vector, | (cid:174)𝛼 | = 𝑛
Partition parallelotope, | (cid:174)𝑃 | = 𝑛

# of banks an access touches (Fan-Out),
𝐹𝑂𝑎 ≤ (cid:206) (cid:174)𝑁
# of accesses a bank feeds (Fan-In),
(cid:205) 𝐹 𝐼𝑏 = (cid:205) 𝐹𝑂𝑎
Per-dimension padding

Table 1. Partitioning parameters and definitions.

sketched in Figure 2. Under hyperplane partitioning, the ar-
ray polytope is divided into parallel “hyperplanes," where
each hyperplane represents a bank. Under lattice partition-
ing, the array is divided into a tessellation of congruent par-
allelotopes, where each position within the parallelotope is
assigned to a unique bank. While most common partitioning
problems have both hyperplane and lattice solutions, there
are certain patterns where only one can provide a solution.
For example, hyperplane partitioning can solve problems
that require a block-cyclic pattern of banks, while lattice par-
titioning can solve problems with concurrent accesses that
do not lie within a rectangular parallelotope

In this paper, we introduce a system based on hyperplane
partitioning with an extension that captures a subset of lat-
tice partitioning solutions, namely those composed of or-
thogonal parallelotopes. These are referred to as “multidi-
mensional" hyperplane geometries and are discussed in 3.3.
Table 1 concisely summarizes all of the quantities involved
in our banking system. Input parameters are provided by
the SDH framework and represent the access pattern the
programmer specified in the code. Solve parameters are
computed by our banking system. Our system internally
computes a collection of these but only returns the optimal
set. Metrics which are helpful quantities for understanding
how the partitioning scheme will map to hardware.

Equations 1 and 2 show how the solve parameters are used
to compute a bank address (𝐵𝐴) and offset (𝐵𝑂). We refer
to these collectively as the “bank resolution" equations.

𝐵𝐴 = ⌊

(cid:174)𝑥 · (cid:174)𝛼
𝐵

⌋ mod 𝑁

(1)

(cid:34)

𝐵𝑂 =

𝐵 ·

(cid:32)

𝑛
∑︁

𝑖=0

⌊

(cid:174)𝑥𝑖
(cid:174)𝑃𝑖

⌋ ·

𝑛
(cid:214)

𝑗=𝑖+1

⌈

(cid:174)𝐷 𝑗
(cid:174)𝑃 𝑗

⌉

(cid:33)(cid:35)

+ ( (cid:174)𝑥 · (cid:174)𝛼 mod 𝐵)

(2)

The equation for 𝐵𝐴 divides the array polytope into par-
allel hyperplanes, and we therefore refer to (cid:174)𝛼, 𝑁 , and 𝐵 as
representing a “hyperplane geometry." The parameter (cid:174)𝑃 is
only used to compute a physical offset, and a hyperplane
geometry may have many valid choices for (cid:174)𝑃. It is related to
the periodicity of Equation 1 and represents a region in the
array such that each 𝐵𝐴 appears at least once and no more
than 𝐵 times.

Definition 2.8. A conflict polytope is the polytope generated
by applying Equation 1 to the delta between the address
patterns of two different accesses (𝐵𝐴( (cid:174)𝑥𝑎1 − (cid:174)𝑥𝑎2))

Definition 2.9. A hyperplane geometry is deemed valid for
a 𝑘-ported memory if no set of 𝑘 accesses contain more than
𝑘 − 1 pairs of non-empty conflict polytopes.

These definitions allow our system to handle addresses
with non-affine components using quantifier-free Presburger
arithmetic [37]. Namely, any function call without side-effects
(including indirection arrays) that is used in an address ex-
pression (i.e. f(𝑖0) in arr[f(𝑖0) + 𝑖1]) can be represented
an uninterpreted function symbol. This means that even
though the compiler cannot analyze 𝐵𝐴(f(𝑖0) + 𝑖1), it can
analyze any conflict polytope it generates against another
access containing the same function symbol.

Finally, 𝐹𝑂𝑎 and 𝐹 𝐼𝑏 describe the fan-out of accesses and
fan-in of banks, respectively. They describe the size of the
crossbars required to arbitrate between accesses and banks.
If (cid:174)𝑃𝑖 does not evenly divide (cid:174)𝐷𝑖 , then the banking equations
result in mathematically “inaccessible" elements that require
the compiler to pad the array with (cid:174)𝛿. When 𝐵 > 1, there may
be more inaccessible elements due to unequal representation
of each bank within a (cid:174)𝑃 region.

2.3 Consequences of Parameters
The values of (cid:174)𝑁 , (cid:174)𝐵, (cid:174)𝛼, and (cid:174)𝑃 have implications on the overall
FPGA resource utilization and latency of the circuit, since
they impact the bank resolution logic, crossbar sizes, and
bank volumes which must all map to quantized resources on
the FPGA.

Consider the snippet in Figure 3a with access patterns
6 · 𝑖 + 1, 6 · 𝑖 + 2, 6 · 𝑖 + 4, and 6 · 𝑖 + 5 (we substitute 𝑘 for
unit-step iterator 𝑖). Figure 3b shows three potential ways
this problem can be solved.

Option 1 uses 5 banks and full crossbars for each access
(𝐹𝑂𝑎 = 5). Option 2 appears better since it uses only 4 banks
and 𝐹𝑂𝑎 = 1, but requires a costly divide-by-3 and a multiply-
by-2 operation. Option 3 uses 6 banks but achieves 𝐹𝑂𝑎 =
1 while also eliminating B and 𝛼 arithmetic. Depending on
the size of the array and the target FPGA, the volume of one

arXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

,

1

2

for (k = 0->3->M par 2)
.. = m[k+1] + m[k+2];

(a)

(b)

Figure 3. (a) Sample access pattern with iterator 𝑘 start=0,
step=3, stop=M, and parallelization=2. (b) Three valid bank-
ing schemes for arr with 𝐵𝐴 values shown.

“bank" may spill into 2 BRAMs, so solutions with smaller 𝑁
may consume more BRAMs.

The key point is that there are always many solutions
to a banking problem and it is difficult to predict which
one is the best. Additionally, the definition of “best" may
change depending on which resource is scarcest for a given
application.

2.4 Concepts in Hierarchically-Nested State

Machine Programming

A programming model based on parallel patterns [7] is a
good starting point for exposing a high level of abstraction
without losing handles on performance [23]. For this reason,
we chose to use Spatial as the host compiler for our bank-
ing system since it is an open-source, extensible language
that can express massive design spaces, an explicit memory
hierarchy, and multi-level parallelism.

Our system targets programs composed as a set of hierar-
chically nested state machines, or controllers. A controller is
expressed as a traditional software loop and is represented in
the IR as a multi-level counter chain that feeds iterator values
to a basic block. A multi-level counter chain is a counter that
spans a multidimensional iteration space whose bounds do
not need to be compile-time static.

2.4.1 Controller Level

The contents of a controller’s basic block define it’s level:

• Inner controllers only contain dataflow graphs made

up of primitives.

• Outer controllers only contain other controllers

An outer controller and the controllers in its basic block
define a parent-child relationship in the controller hierarchy.
An outer controller’s width is the number of children it con-
tains. A controller’s sub-tree is the set of controllers found
by following its children recursively. A node’s ancestors is
a list of every controller in the hierarchy that encloses it.
The least common ancestor (LCA) of two nodes is the most
deeply-nested controller which they both share.

2.4.2 Controller Schedule

"Scheduling" refers to how nodes in a controller’s basic block
execute relative to each other but has different meanings for
inner and outer controllers.

For outer controllers, there are five schedules describing

how the children execute in hardware:

• Sequential - Child controllers execute one at a time

with no overlap.

• Pipelined - Child controllers execute in pipelined (i.e.

overlapping) fashion.

• Fork-Join - Child controllers execute simultaneously

and independently of each other.

• Fork - One child controller executes per iteration,

based on a set of if/then/else conditions.

• Streaming - Child controllers execute as long as their

input data is available.

For inner controllers, the schedule refers to the mapping
from each node in the pipelined dataflow graph to the cycle
it will execute. Scheduling nodes to execute during different
cycles allows the design to safely run at a higher clock rate.
Their runtime is entirely defined by their latency, which is the
cycle at which its latest node is scheduled, and its initiation
interval, which is the number of cycles the controller must
wait before it can increment the multi-level counter and issue
another iteration. The hardware-complexity of the datapath
determines the latency, and the loop-carry dependencies
determine the initiation interval.

2.4.3 Controller Parallelization

Parallelization is one of the ways that programs mapped to
FPGA can improve performance. Specifically, parallelizing a
loop by 𝑃 means that 𝑃 consecutive iterations of the loop will
be executed simultaneously. To a first-order approximation,
this means that parallelization results in a factor of 𝑃 per-
formance improvement by using 𝑃-times as many resources
compared to the un-parallelized loop.

In reality, this typically improves performance by less than
𝑃 and increases resource utilization by more than 𝑃. This
is because initiation interval and latency may change with
parallelization. We must distinguish between parallelization
of inner and outer controllers separately to understand why.
We walk through the example snippet in Figure 4 to demon-

strate this.

Parallelization applied to an inner controller results in
vectorization of the datapath. Figure 5 shows how this is
done for loop k with IP = 2.

Parallelization applied to an outer controller results in un-
rolling. Each child controller is cloned in whole and added to
the hierarchy. This is shown in Figure 6 as the "Pre-Unrolled"
tree structure is transformed into the "Intermediate" struc-
ture for OP = 2. The compiler must then inject Fork-Join
controllers to this intermediate structure to achieve the paral-
lelism specified in the program. There are two strategies that

Efficient Memory Partitioning in Software Defined Hardware

arXiv Preprint, Work in Progress, Palo Alto, California

// Parent (Outer Ctrl)
Foreach(N by 1 par OP){i =>

// Child 0
Foreach(M by 1){j => /*...*/}
// Child 1 (Inner Ctrl)
Foreach(K by 1 par IP){k =>

val t = i*3
mem(k) = mux(k == 0, t + mem(k), t)

1
2

3
4
5

6
7
8

9
10

}

}

Figure 4. Example construction of an outer controller (Loop
i) with parallelization (OP) and two child controllers (Loops
j and k). Loop k has parallelization factor IP.

Figure 5. Dataflow path for the inner controller of loop k in
Figure 4 before and after applying vectorization (IP=2).

the compiler may employ to capture this level of parallelism,
shown in the "Post-Unrolled" trees in Figure 6:

• ForkJoin-of-Pipelines unrolling is when the Fork-Join
controllers are injected between the outer controller
and child stages such that all lanes of each child are
synchronized (i.e. always begin their execution simul-
taneously). This guarantees that child c will not begin
executing iteration i+P until iterations i/P*P to (i/P*P)
+ P - 1 are completed, for any child and iteration of the
parent controller.

• Pipeline-of-ForkJoins unrolling is when each lane of
the original outer is separated into its own controller
that is structurally identical to the pre-unrolled loop. A
single Fork-Join controller is injected above the outer
controllers, such that all lanes begin executing simul-
taneously. This guarantees that child c will not begin
executing iteration i+P until iteration i is completed,
for any child and iteration of the parent controller.
However, there is no guarantee about the relative or-
dering between iteration i/P*P and i/P*P+K, where
K % P != 0.

For any node in the IR, its "unroll ID" (UID) is a list of
integers describing which lane of each ancestor controller it
belongs to. The UID of a node is described as the "base UID"
if all integers in the UID are 0.

Figure 6. Controller trees before and after unrolling loop
i in Figure 4 (OP=2, IP=2) for stage-based and lane-based
synchronization.

polytope emptiness problem. Then, we describe the three
steps in our system for solving this problem:

• Compute a list of solutions (constants for (cid:174)𝑁 , (cid:174)𝐵, (cid:174)𝛼, and

(cid:174)𝑃) that satisfy the constraints

• Apply resource-saving optimizations to the bank reso-

lution datapaths

• Estimate the cost of each solution and return the best

3.1 A Running Example

In order to drive the components of our banking system, we
introduce a motivating example that presents an interesting
banking problem. Figure 7 shows part of the grid-variant of
the Molecular Dynamics (MD) benchmark from Machsuite,
as it relates to the accesses to one memory.

At a high level, this algorithm models a 3-dimensional
field of molecules. The molecules are grouped into "cells,"
which means the field is represented by the four-dimensional
structure dvec_sram. The first three dimensions describe
the location of a cell in space, and the fourth dimension
enumerates each molecule within the cell. The algorithm
computes how each molecule interacts with each molecule
in its 26 adjacent cells.

In this snippet, we are initializing the memory in Line 1
by fetching data from DRAM to it and writing PL elements
per cycle in its leading dimension. Later, in Line 7, we read
this data from the memory to drive the algorithm. Note the
innermost controller’s counter spans Q_RNG(x,y,z), which
refers to the amount of molecules in the cell at x, y, and z.
Each cell contains a different number of molecules, so this is
a data-dependent value. There are a total of PL writers and
PX*PY*PZ*PP*PQ readers.

3.2 Distilling the Program to a Polytope Emptiness

Problem

3 Banking System Design
In this section, we begin by discussing the analysis that
analyzes accesses in a program and converts them into a

There are three steps involved in constructing the polytope
emptiness problem: group placement, address pattern extrac-
tion, and synchronization analysis.

arXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

1
2

3
4
5

6
7
8

9
10

dvec_sram loadTile dvec_dram par PL
Foreach(X_RNG, Y_RNG, Z_RNG par PX,PY,PZ)

{ (x, y, z) =>

Foreach(P_RNG par PP) { p =>

Foreach(Q_RNG(x,y,z) par PQ) { q =>

... = dvec_sram(x,y,z,q)

}
Foreach(P_RNG_dyn to density) { ... }

}

}

1
2
3
4
5
6
7
8
9
10

for a in (cid:174)𝑎:

clash = false
gId = -1
while !clash && gId < (cid:174)𝐺 .len:

gId++
G = (cid:174)𝐺𝑔𝐼𝑑
for b in G:

clash = clash || lca(a,b).isConcurrent

(cid:174)𝐺𝑔𝐼𝑑 .append(a)

Figure 7. Access pattern on 4D dvec_sram from the
MD_Grid benchmark.

Figure 8. Algorithm for computing iterator synchronization.

is the first step of this process. A group
Group placement
is a collection of accesses which may be active simultane-
ously on the same buffer of a memory. Each access, 𝑎, is
assigned to a group, 𝐺, during this step. If accesses occur
in different stages of an outer pipeline controller, they are
not grouped together because they would access different
buffers. The banking system only needs to compute a bank-
ing scheme that satisfies each group in isolation since only
one group will be active at a time.

For the running example, we start by collecting all of our
accesses and initializing our groups list, (cid:174)𝐺 with the first
access:

(cid:174)𝑎 = {𝑤1, ..., 𝑤𝑃𝐿−1, 𝑟000, 𝑟001, ..., 𝑟𝑃𝑋 ∗𝑃𝑌 ∗𝑃𝑍 −1,𝑃𝑃 −1,𝑃𝑄−1}
(cid:174)𝐺 = {{𝑤0}}
The subscript integers refer to the UID of each access. 𝑤𝑛
refers to vectorization from PL and 𝑟𝑖 𝑗𝑘 refers to the lanes of
loops x/y/z (flattened), p, and q.

(3)

The compiler must inspect the ancestors of each access and
place it into one group. The pseudo-code in Figure 8 shows
how to perform the grouping. isConcurrent is defined for
each controller based on the following:

• If lca is an inner controller, isConcurrent returns
true if the distance (in cycles) between the scheduling
of a and b is less than the initiation interval of the lca.
• If lca is an outer controller, isConcurrent returns
true if the schedule is Fork-Join or Stream and returns
false if the schedule is Sequential, Fork, or Pipelined
scheduling. Note in the case of a Pipelined controller,
the accesses are concurrent but routed to different
buffers.

The lca of two accesses is often a Fork-Join controller
due to the unrolling strategies described in Section 2.4.3.
Applying the algorithm to our running example yields a
banking problem with two groups:

(cid:174)𝐺 = {{𝑤0, 𝑤1, ..., 𝑤𝑃𝐿−1},

{𝑟000, 𝑟001, ..., 𝑟𝑃𝑋 ∗𝑃𝑌 ∗𝑃𝑍 −1,𝑃𝑃 −1,𝑃𝑄−1}}

(4)

Address Pattern Extraction is the second step of the pro-
cess. In this step, we convert each access into a polytope so
that we can apply polytope-emptiness testing to each access
conflict within a group [42]. An affine address equation is
one in the form of:

(cid:174)𝑥 = 𝐴𝑛×𝑚 · (cid:174)𝑖 + 𝐶𝑛×1
(5)
The accesses in our example can conveniently be written

with two parameterized patterns:












𝑑3
𝑐
0
0
0
0
0
0
𝑃𝐿 𝑛

𝑤𝑛
𝑑0 𝑑1 𝑑2
0
0
1
0
1
0
1
0
0
0
0
0
𝑑0 = 0 to W by 1


𝑑1 = 0 to W by 1


𝑑2 = 0 to W by 1


𝑑3 = 0 to N by PL


































𝑦
0
𝑃𝑌
0
0

𝑥
𝑃𝑋
0
0
0

𝑞
𝑐
𝑖𝑋
0
𝑖𝑌
0
𝑖𝑍
0
𝑃𝑄 𝑘

𝑟𝑖 𝑗𝑘
𝑧
0
0
𝑃𝑍
0
𝑥 = X_RNG by PX


𝑦 = Y_RNG by PY


𝑧 = Z_RNG by PZ


𝑞 = Q_RNG(x,y,z) by PQ























Figure 9. Affine address patterns and iterator constraints
for 𝑤𝑛 and 𝑟𝑖 𝑗𝑘 .

The iterators d0-3 are implicit in the expansion of the
loadTile syntax. Address pattern expansion can also be
applied to accesses that are only partially affine. For example,
... = mem(i + 𝛿0*j + 𝛿1) would treat the 𝛿0*j as an
unbounded iterator and 𝛿1 as a static iterator with unity
range if it were run-time static.

Synchronization is the final step of this process. While it
may appear that we are ready to apply polytope-emptiness
testing to our equations, the solutions we find would almost
certainly result in run-time bugs due to bank collisions. This
is because we did not account for iterator synchronization
in the SDH programming model.

An iterator from two different UIDs is synchronized if it
is guaranteed to always increment during the same cycle
for both UIDs, start from the same value, and step by the
same amount. The iterator is partially synchronized if it is
guaranteed to increment during the same cycle for both

Efficient Memory Partitioning in Software Defined Hardware

arXiv Preprint, Work in Progress, Palo Alto, California

UIDs, but the starts or steps for the two UIDs always varies
by a fixed amount. The iterator is unsynchronized if none
of these guarantees can be proven by analyzing the control
structures.

The compiler must determine global substitution rules for
each iterator based on its UID and apply them uniformly
to all accesses so that we achieve a globally-synchronized
banking problem.

In our example, Q_RNG(x,y,z) varies with the first integer
in an access’ UID. For example, if PX = 2, then readers 𝑟0∗∗
and 𝑟1∗∗ exist in separate subtrees. Each subtree’s loop q
experiences a different value for Q_RNG(x,y,z), which in
turn means that these subtrees will have different execution
times.

Under PoF unrolling, all UIDs of loop q will start simul-
taneously but may start from a different value depending
on Q_RNG(x,y,z). This means q is unsynchronized between
UIDs. Under FoP unrolling, different UIDs of loop q may ini-
tiate at different points in time since different lanes of loop
x/y/z run independently of one another without synchro-
nization. q, in addition to p (not used in the address pattern),
x, y, and z, would all be unsynchronized between UIDs.

Note that parallelization of iterators that are part of an
access’ ancestors but not directly used in the address pattern
can still impact synchronization. In this example, we would
have to carry out synchronization analysis if PP > 1.

3.3 Building a Solution Set
Our solution set is a collection of (cid:174)𝑁 , (cid:174)𝐵, (cid:174)𝛼, and (cid:174)𝑃 tuples which
satisfy the banking constraints. Finding this set requires
us to build candidate sets for (cid:174)𝑁 , (cid:174)𝐵, and (cid:174)𝛼, and check for
combinations that can be proven to be valid geometries for
the given access pattern. Then various possible values for (cid:174)𝑃
are calculated for each geometry.

In order to increase the prob-
Prioritizing Candidate Sets
ability of finding a “good" solution quickly, we prioritize cer-
tain parameters in these candidate sets. Specifically, we find
the LCM of the access groups’ sizes and prioritize the first
few multiples of this. This is more likely to find schemes that
do not require full cross-bars between the accesses and the
banks (e.g. small 𝐹𝑂𝑎). We also remove candidates for (cid:174)𝛼 if
the elements are not mutually co-prime with the element(s)
in (cid:174)𝐵, since the same geometry can be expressed by dividing
all parameters by the GCD. Finally, we prioritize integers for
all parameters that can be broken down by rules introduced
in 3.4. All other values are de-prioritized.

Multidimensional Banking In addition to flat schemes
[42], our system also searches for multidimensional schemes.
In these schemes, we bank each dimension of the memory
separately with a 1-dimensional hyperplane geometry based
on the projections of the original accesses. This means that
we produce a 𝐵𝐴 per-dimension (𝐵𝑂 is still a scalar, namely

the intra-bank offset of a multidimensionally-indexed bank)
Multidimensional schemes is the subset of lattice partitioning
schemes based on orthogonal parallelotopes.

Multidimensional banking schemes can also be verified
more quickly. This is because ℓ𝐶𝑘 polytope-emptiness checks
must occur to prove ℓ concurrent accesses are properly
banked for a 𝑘-ported memory. The complexity of verifi-
cation is therefore O (ℓ𝑘 ), since 𝑘 is typically much smaller
than ℓ. Projecting accesses results in smaller groups per di-
mension, either due to redundancy (two projections are iden-
tical) or regrouping (two accesses are guaranteed to never
conflict because 𝐵𝐴 on at least one other dimension always
differs). This reduction is most significant when the accesses
are heavily-parallelized on multiple dimensions, allowing
the system to verify banking solutions more rapidly.

If the underlying BRAM supports
Fewer-Ported Solutions
𝑘 ports, there may be some area overhead associated with
the memory template when there are more than 𝑘 accesses
that may connect to one bank. For this reason, our analyzer
adds sub-𝑘-ported solutions to the solution set.

Bank-by-duplication Our system iteratively partitions
readers into separate groups and routes each group to a differ-
ent duplicate of the array in certain cases. It then re-runs the
banking analysis on each duplicate separately. Banking-by-
duplication is occasionally the best strategy in cases where
LUTs are scarce but BRAMs are abundant.

3.4 Resource-Saving Datapath Transforms

The banking resolution logic (Equations 1 and 2) include
multiplication, division, and modulo operations that may
be costly on an FPGA. Because our banking analyzer has
the freedom to choose the actual constants used in these
equations, we can aim for those constants that allow for
resource-saving transformations and avoid calling vendor-
specific arithmetic IPs.

Crandall’s Algorithm We apply Crandall’s algorithm to
perform division and modulo by Mersenne numbers (i.e. those
in the form 𝑀 = 2𝑛 − 1). This allows us to perform a cascade
of bit-wise operations and simple additions rather than call-
ing division or modulo IPs in hardware. We further rewrite
modulo operations when the operand can evenly divide a
Mersenne number. Specifically, if 𝑀2 · 𝑘 = 2𝑛 − 1 for some
1 < 𝑘 < 𝑅, then we can apply Crandall’s algorithm on 𝑀
followed by a 𝑘-wide one-hot mux to compute mod 𝑀2.

𝑥 mod 𝑀2 ≡ (𝑥 mod 𝑀) mod 𝑀2

(6)

For reference, there are 5 Mersenne integers, 5 integers
that evenly divide a Mersenne integers with R=16, and 6
power-of-2 integers between 1 and 65. This provides a suffi-
ciently large pool of desirable constants towards which we
can steer our system’s search.

arXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

• Subgraph features that include neighbors and acces-

sors of a memory node in the dataflow.

The first stage in the pipeline generates second-degree poly-
nomial combinations of raw features. This approach helps
create stronger features, e.g., the product of the number of
banks over all the dimensions in a high-dimensional memory
node, at the expense of generating an ample feature space
that can hurt the training speed. The second stage consists
of a regressor based on the gradient-boosting tree [9]. The
last stage re-selects the generated features based on their
importance. We define importance as the frequency each
generated feature appears in the trained model. In our ex-
periment, we found that 36 generated features provided the
searching process with enough accuracy.

3.5.2 Training and Fine-Tuning the Estimator

We created the dataset by using Spatial’s regression bench-
mark suite 1. However, our approach can be applied to any
SDH framework with a reasonably explicit representation
of the memory template and bank resolution logic exposed
in its IR. We ran PnR on all the RTL files generated by this
benchmark suite to collect the resources used for every mem-
ory and arithmetic node in each application. A sample in
the dataset contains a memory node’s raw features and its
resources in terms of look-up tables (LUT), flip-flops (FF),
and RAMs. The created dataset contains 831 samples.

Due to the small size of this dataset, we carefully fine-
tuned our proposed pipeline to control overfitting. We trained
two models: a baseline MLP model similar to the one pro-
posed in [24], and the model pipeline proposed in this work.
We were not able to achieve high performance by using the
original baseline model; hence, we augmented its architec-
ture and fine-tuned it to get the best possible performance
on the dataset. Specifically, we performed an exhaustive grid
search on the training and regularization parameters for the
baseline model and chose the one with the best performance.
Due to the complexity of the proposed model, we were not
able to search for the best parameters exhaustively. Hence,
we focused on tuning the regularization parameters to avoid
overfitting. Please refer to Table 3 in the appendix for the
final parameters of each model.

For cross-validation, we randomly permuted the raw dataset

ten times. Every time, we split the dataset into a training
set and a test set randomly with a 7-to-3 ratio. We collected
the training scores and test scores for both models. Figure
11 shows learning curves for both models when predicting
the LUT resource. For learning curves showing both models
predicting other resources, please refer to Appendix A.

1Spatial regression suite: https://github.com/stanford-ppl/spatial/
tree/master/test/spatial/tests

Figure 10. A machine learning architecture to predict a
banking scheme’s resources after PnR.

Binary decomposition of multiplication We also opti-
mize multiplication in the form of 𝑎 ∗ 𝑐, when 𝑐 is a constant
in the form 𝑐 = (cid:205)
0≤𝑘<𝑅 𝑆 (𝑘)2𝑛𝑘 , where 𝑆 (𝑘) ∈ {±1} and 𝑅
is the radius of the optimization. This allows the compiler to
apply the rewrite rule 𝑎 ∗𝑐 = (cid:205)𝑘 𝑎 ∗ 𝑆 (𝑘)2𝑛𝑘 , which is simply
the sum of bit-shifts of 𝑎.

For reference, with 𝑅=2, half of the integers between 1
and 65 can be rewritten using only bit-shifts and addition.

3.5 Machine Learning Model for Resources

Estimation

Finally, the compiler chooses a valid banking scheme requir-
ing minimal resources, i.e., the RTL code generated from
the scheme leads to the least hardware resource after the
downstream FPGA toolchain completes PnR. Previous work
[42] used an analytical model to estimate the hardware re-
source. However, we found that building an accurate analyt-
ical model is very challenging due to the complexity of our
RTL templates. Instead, we built a machine learning pipeline
to provide the searching process with reasonable estimates
of the hardware resources required for a banking scheme.

3.5.1 Architecture of Resource Estimator

Previous work [24] used a Multi-layer Perceptron (MLP)
model for hardware resource estimation. From our experi-
ments, we found that MLP performed worse on small datasets
due to overfitting. Hence, we built a pipeline based on de-
cision trees and fine-tuned it to reduce overfitting. Figure
10 shows the pipeline’s architecture. It takes two classes of
features from the parameters of a banking scheme:

• Template features that include primitives and de-

rived parameters.

FeatureScoreBanking resolution pathBanked memoryWriterReader!"owSubgraph featuresTemplate featuresNumber of Banks (N)Blocking Factor (B)Dimensions (dim)Bit Precisions (prec)Partition Factor (alpha)…Polynomial feature generatorGradient-boosting decision treeFeature re-selectionEfficient Memory Partitioning in Software Defined Hardware

arXiv Preprint, Work in Progress, Palo Alto, California

(a)

(b)

(c)

(a) The baseline model.

(b) The proposed pipeline.

Figure 12. Access patterns for sw (a), spmv (b), and sgd (c).
Strides are marked in (b) and (c)

Figure 11. Learning curves of the baseline model (left) and
our proposed model pipeline when fine-tuned on the dataset
from Spatial’s regression suite.

In our experiment, we scored both curves using 𝑅2. We
averaged the score curves over all the ten splits. The col-
ored fields show the standard deviation of all the scores col-
lected during cross-validation. Our proposed model pipeline
achieves an average 𝑅2 score of 0.86, which is higher than the
baseline model’s average 𝑅2 score of 0.60. Besides, the model
shows a smaller standard deviation of its cross-validation test
scores, which indicates that it suffers less from overfitting.

4 Evaluation
We implemented our banking system as a compiler pass in
Spatial [23], a fully open-source SDH framework that can be
easily modified and target a variety of FPGAs. The system
uses the Integer Set Library [40] to perform the polytope
emptiness checks. We evaluated our system on both a Xilinx
Virtex 7 and Amazon EC2 F1 instance (Xilinx VU9P) for
eight stencil patterns and an additional three patterns from
real-world applications described here.

Smith-Waterman SW is a sequence alignment algorithm
used in genomics. It contains a dynamic programming com-
ponent known as Genome Alignment using Constant-memory
Traceback (GACT) [39]. This essentially creates a sliding-
window access pattern where a cell is updated based on
the values of its north, west, and north-west neighbor. We
parallelized this access pattern by 4 to expose wavefront
parallelism in the algorithm.

Sparse Matrix-Vector Multiplication SPMV is a common
linear algebra kernel [35]. Our version uses an edge-list rep-
resentation to identify dense regions in the matrix. We paral-
lelized this algorithm over four rows and three columns, so
that each row’s strided access pattern has a "random" relative
offset. This type of pattern is a good candidate for multidi-
mensional banking because this random offset effectively
disappears from projection regrouping (see 3.3).

Stochastic gradient descent SGD is a training algorithm
ubiquitous in machine learning [14]. Our version uses mini-
batch, which stores a matrix of input data on-chip and has

two modes of access: first in column-major to compute pre-
dictions with the model, then in row-major to compute the
gradients. These two modes of access can each be parallelized
across rows and columns, and will never be concurrent (i.e.
two access groups). Our version parallelized both access
patterns so that there are 12 accesses in each group.

4.1 Comparisons on Virtex 7

We first compared our system on eight stencil patterns against
[42], called “baseline", as well as unmodified Spatial. Unmod-
ified Spatial uses the first valid scheme it finds. These re-
sults show that solving for numerous solutions and applying
resource-saving transformations to each reveals more effi-
cient partitioning schemes in all cases. Our system always
finds parameters that result in DSP-free circuits.;5

4.2 Comparisons on AWS F1 (VU9P)

We also tested our system on a larger FPGA against a state-
of-the-art commercial SDH framework called Merlin [45] on
Amazon’s EC2 F1 instances. To the best of our knowledge,
Merlin does not target the Virtex 7 FPGA so we chose to use
the popular F1 backend. The Merlin and Spatial compiler
often land on a partitioning solution that over-utilizes re-
sources. The key is that our system can view a more diverse
set of solutions, take advantage of the constants in the bank
resolution arithmetic. The ML model is what allows our sys-
tem to choose which one of the many valid solutions will be
the best.

For example, the Merlin compiler appears to bank the de-
noise and bicubic kernels as sobel-like patterns rather than 4-
point accesses, hence producing a scheme requiring 9 banks
and resource-intensive arithmetic. The Spatial compiler de-
tects the “leaner" solution for these 4-point access patterns
by having (cid:174)𝐵 ≠ 1. Our system recognizes both of these kinds
of solutions, and improves particularly on Spatial’s base solu-
tion by applying resource-saving transformations on a true
dual-ported scheme.

5 Related Work
The theory of the partitioning problem was pioneered by
research in the memory allocation problem in the polyhe-
dral model [29] [44] [25] [26] [13] [12] [6]. There is a long
history of researchers using this model to build systems that

100200300400500Training examples0.40.60.81.0ScoreLearning CurveTraining scoreCross-validation score100200300400500Training examples0.00.20.40.60.81.0ScoreLearning CurveTraining scoreCross-validation scorearXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

App

Pattern System Slice

BRAM DSP

App

Pattern System LUT FF

BRAM DSP

denoise

deconv

denoise-
ur

bicubic

sobel

motion-
lv

motion-
lh

motion-c

Avg.
Change

Baseline 303
330
Spatial
213
Ours

Baseline 597
743
Spatial
532
Ours

Baseline 795
1116
Spatial
659
Ours

Baseline 238
309
Spatial
209
Ours

Baseline 1523
1801
Spatial
1214
Ours

Baseline 425
1737
Spatial
187
Ours

Baseline 334
1333
Spatial
210
Ours

Baseline 155
93
Spatial
69
Ours

4
4
2

5
6
3

8
8
6

4
4
2

9
10
5

4
6
3

4
6
3

2
4
2

0
0
0

5
4
0

0
5
0

0
0
0

9
4
0

0
0
0

0
2
0

0
0
0

denoise

deconv

denoise-
ur

bicubic

sobel

motion-
lv

motion-
lh

motion-c

Baseline
Spatial

-29.8% -32.4%
-46.1% -46.8%

-100%
-100%

sw

Table 2. Comparisons on Virtex 7

spmv

sgd

Avg.
Change

efficiently generate a memory allocation scheme that mini-
mizes the overall memory footprint of an application. The
process is focused on determining the live-ness of data in the
program and determining a pseudoprojection mapping for
memory references such that the program can be realized
with a smaller memory footprint.

A related line of research involves computing memory par-
titioning schemes for distributed memory machines (DMMs)
[27] [19] [34]. In these systems, the compiler builds a parti-
tioning scheme of the program’s arrays such that the data is
distributed across different parallel processors as efficiently
as possible. The systems use a model that estimates the cost
of communicating data between processors to determine the
best way to generate the partitioning scheme.

Many SDH frameworks require the programmer to manu-
ally partition their arrays, but there has been recent work on
developing tools that solve the problem automatically [42]
[43] [10] [11] [22] using either hyperplane- or lattice-based
techniques. Either technique can automatically solve the
most common partitioning problems, but one may be more

Merlin 4630
1416
Spatial
184
Ours

Merlin 4795
2939
Spatial
2435
Ours

Merlin 3433
2068
Spatial
638
Ours

Merlin 4432
467
Spatial
179
Ours

11523
1241
76

12757
2537
1119

13342
1240
185

10788
106
65

Merlin 5417
Spatial
Ours

17747
23157 30738
1004
3482

Merlin 2894
Spatial
Ours

11492
19763 26706
1351

728

289

Merlin 235
Spatial
Ours

19444 26432
285

97

Merlin 251
241
Spatial
91
Ours

93
102
54

9
4
2

9
5
6

48
8
4

9
4
2

9
9
9

6
6
3

24
6
3

16
4
2

Fig
12a

Fig
12b

Fig
12c

Merlin 6983
Spatial
Ours

7625
40286 48887
3916
9485

560
396
270

Merlin 31224 11018
Spatial
Ours

256
93972 128015 20
6
12269 7359

Merlin 9466
Spatial
Ours

9255
21022 27725
4711

458

504
12
12

26
0
0

26
12
0

12
15
0

0
0
0

24
12
0

24
6
0

0
0
0

0
0
0

7
21
0

0
30
0

38
21
0

Merlin -48.1% -78.3% -71.4% -100%
-74.0% -81.7% -37.7% -100%
Spatial

Table 3. Comparisons on AWS F1 (VU9P).

efficient for a certain problem than the other. Furthermore,
one technique may have many solutions to the problem and
estimating which one is the most cost-effective when synthe-
sized and mapped to hardware can be tricky. These prior sys-
tems generally have first-order rules for determining which
solution it finds is the most cost-effective one, but this does
not always provide the best answer. Our system incorpo-
rates the advantages of both techniques by looking for both
hyperplane and orthogonal lattice solutions. It also finds
alternative parameters for calculating physical addresses,

Efficient Memory Partitioning in Software Defined Hardware

arXiv Preprint, Work in Progress, Palo Alto, California

specifically for hyperplane partitioning. It then uses built-
in ML models for estimating the cost of each partitioning
scheme when mapped to the underlying FPGA resources. It
also applies mathematical transformations to the bank reso-
lution arithmetic to generate circuits that are more efficient
in hardware. This almost always removes the need for allo-
cating DSPs to the bank resolution logic, which is generally
the scarcest resource for algorithms that are accelerated by
FPGAs.

6 Conclusion
As the compiler community continues to develop SDH tools
that can express increasingly complicated loop scheduling
and parallelization patterns, the problem of automatically
partitioning memories quickly and efficiently is becoming
increasingly important. We have introduced a banking an-
alyzer that is scalable with programming models that are
equipped to express complicated applications. Our system
provides an appropriate solution to three challenges regard-
ing the problem of memory partitioning: formulating con-
straints in a flexible SDH framework, quickly finding cheap
solutions, and choosing the most cost-efficient one.

First, our system is capable of producing a set of bank-
ing constraints regardless of the programmer’s choices in
loop scheduling, access patterns, and parallelization such
that it can guarantee the correctness of its partitioning so-
lution. Next, we take advantage of the fact that the bank
resolution logic is fully exposed in the IR to implement IR
transformations aimed at nodes prevalent in banking res-
olution arithmetic. We use heuristics to steer the analyzer
towards solutions that can be optimized in hardware. Finally,
we have trained a simple ML model based on decision trees
to quickly estimate the resource utilization of laying out var-
ious banking schemes in hardware. The model is accurate
enough to select the best scheme.

Overall, all three of these features are necessary to have a
practical solution for automatically partitioning memories
in the compiler. Our system is capable of reducing LUT uti-
lization by 86% and BRAM utilization by 38% over a naive
system for a variety of complicated, real-world benchmarks.
It almost always eliminates all DSP usage from the banking
arithmetic for stencil and complex access patterns. For prob-
lems with massive solution spaces, it can cut the time spent
searching for solutions in half.

References
[1] 2016. Vivado High-Level Synthesis. http://www.xilinx.com/products/

design-tools/vivado/integration/esl-design.html.

[2] 2017. Intel FPGA SDK for OpenCL. https://www.altera.com/products/
design-software/embedded-software-developers/opencl/overview.
html.
[3] 2019.

SDAccel Documentation.

https://www.xilinx.com/

support/documentation-navigation/development-tools/
software-development/sdaccel.html.

[4] Kamel Abdelouahab, Maxime Pelcat, Jocelyn Sérot, and François Berry.
2018. Accelerating CNN inference on FPGAs: A Survey. ArXiv
abs/1806.01683 (2018).

[5] Mohamed-Walid Benabderrahmane, Louis-Noël Pouchet, Albert Co-
hen, and Cédric Bastoul. 2010. The polyhedral model is more widely
applicable than you think. In International Conference on Compiler
Construction. Springer, 283–303.

[6] Somashekaracharya G. Bhaskaracharya, Uday Bondhugula, and Albert
Cohen. 2016. Automatic Storage Optimization for Arrays. ACM Trans.
Program. Lang. Syst. 38, 3, Article 11 (April 2016), 23 pages. https:
//doi.org/10.1145/2845078

[7] K. J. Brown, H. Lee, T. Romp, A. K. Sujeeth, C. De Sa, C. Aberger, and K.
Olukotun. 2016. Have abstraction and eat performance, too: Optimized
heterogeneous computing with parallel patterns. In 2016 IEEE/ACM
International Symposium on Code Generation and Optimization (CGO).
194–205.

[8] Salvatore Cardamone, Jonathan Kimmitt, Hugh Burton, Timothy Tod-
man, Shurui Li, Wayne Luk, and Alex Thom. 2019. Field-programmable
gate arrays and quantum Monte Carlo: Power efficient coprocessing for
scalable high-performance computing. International Journal of Quan-
tum Chemistry 119 (02 2019), e25853. https://doi.org/10.1002/qua.25853
[9] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree
boosting system. In Proceedings of the 22nd acm sigkdd international
conference on knowledge discovery and data mining. ACM, 785–794.

[10] Alessandro Cilardo and Luca Gallo. 2015.

Improving Multibank
Memory Access Parallelism with Lattice-Based Partitioning. ACM
Trans. Archit. Code Optim. 11, 4, Article 45 (Jan. 2015), 25 pages.
https://doi.org/10.1145/2675359

[11] Jason Cong, Wei Jiang, Bin Liu, and Yi Zou. 2011. Automatic Memory
Partitioning and Scheduling for Throughput and Power Optimization.
ACM Trans. Des. Autom. Electron. Syst. 16, 2, Article 15 (April 2011),
25 pages. https://doi.org/10.1145/1929943.1929947

[12] Alain Darte, Alexandre Isoard, and Tomofumi Yuki. 2016. Extended
Lattice-based Memory Allocation. In Proceedings of the 25th Interna-
tional Conference on Compiler Construction (CC 2016). ACM, New York,
NY, USA, 218–228. https://doi.org/10.1145/2892208.2892213

[13] A. Darte, R. Schreiber, and G. Villard. 2005. Lattice-based memory
allocation. IEEE Trans. Comput. 54, 10 (Oct 2005), 1242–1257. https:
//doi.org/10.1109/TC.2005.167

[14] C. De Sa, M. Feldman, C. Ré, and K. Olukotun. 2017. Understanding and
optimizing asynchronous low-precision stochastic gradient descent.
In 2017 ACM/IEEE 44th Annual International Symposium on Computer
Architecture (ISCA). 561–574. https://doi.org/10.1145/3079856.3080248
[15] David Durst, Matthew Feldman, Dillon Huff, David Akeley, Ross Daly,
Gilbert Louis Bernstein, Marco Patrignani, Kayvon Fatahalian, and
Pat Hanrahan. 2020. Type-Directed Scheduling of Streaming Ac-
celerators. In Proceedings of the 41st ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI 2020). As-
sociation for Computing Machinery, New York, NY, USA, 408–422.
https://doi.org/10.1145/3385412.3385983

[16] Paul Feautrier. 1991. Dataflow analysis of array and scalar references.
International Journal of Parallel Programming 20, 1 (1991), 23–53.
[17] Paul Feautrier. 1992. Some efficient solutions to the affine scheduling
International journal of parallel

problem. I. One-dimensional time.
programming 21, 5 (1992), 313–347.

[18] Paul Feautrier. 1992. Some efficient solutions to the affine scheduling
problem. Part II. Multidimensional time. International journal of parallel
programming 21, 6 (1992), 389–420.

[19] C. H. Huang and P. Sadayappan. 1992. Communication-free hyper-
plane partitioning of nested loops. In Languages and Compilers for
Parallel Computing, Utpal Banerjee, David Gelernter, Alex Nicolau,
and David Padua (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg,
186–200.

arXiv Preprint, Work in Progress, Palo Alto, California

Matthew Feldman, Tian Zhao, and Kunle Olukotun

[36] Hardik Sharma, Jongse Park, Emmanuel Amaro, Bradley Thwaites,
Praneetha Kotha, Anmol Gupta, Joon Kyung Kim, Asit Mishra, and
Hadi Esmaeilzadeh. 2016. Dnnweaver: From high-level deep network
models to fpga acceleration. In the Workshop on Cognitive Architectures.
[37] Robert E. Shostak. 1979. A Practical Decision Procedure for Arithmetic
with Function Symbols. J. ACM 26, 2 (April 1979), 351–360. https:
//doi.org/10.1145/322123.322137

[38] Robert Stewart, Kirsty Duncan, Greg Michaelson, Paulo Garcia, Deep-
ayan Bhowmik, and Andrew Wallace. 2018. RIPL: A Parallel Image Pro-
cessing Language for FPGAs. ACM Trans. Reconfigurable Technol. Syst.
11, 1, Article 7 (March 2018), 24 pages. https://doi.org/10.1145/3180481
[39] Yatish Turakhia, Kevin Jie Zheng, Gill Bejerano, and William J. Dally.
2017. Darwin: A Hardware-acceleration Framework for Genomic
Sequence Alignment. bioRxiv (2017). https://doi.org/10.1101/092171
arXiv:https://www.biorxiv.org/content/early/2017/01/24/092171.full.pdf

[40] Sven Verdoolaege. 2010. isl: An integer set library for the polyhedral
model. In International Congress on Mathematical Software. Springer,
299–302.

[41] Chao Wang, Xi Li, and Xuehai Zhou. 2015. SODA: Software Defined
FPGA Based Accelerators for Big Data. In Proceedings of the 2015
Design, Automation &#38; Test in Europe Conference &#38; Exhibition
(DATE ’15). EDA Consortium, San Jose, CA, USA, 884–887. http:
//dl.acm.org/citation.cfm?id=2755753.2757017

[42] Yuxin Wang, Peng Li, and Jason Cong. 2014. Theory and Algorithm
for Generalized Memory Partitioning in High-level Synthesis. In Pro-
ceedings of the 2014 ACM/SIGDA International Symposium on Field-
programmable Gate Arrays (FPGA ’14). ACM, New York, NY, USA,
199–208. https://doi.org/10.1145/2554688.2554780

[43] Yuxin Wang, Peng Li, Peng Zhang, Chen Zhang, and Jason Cong.
2013. Memory Partitioning for Multidimensional Arrays in High-
level Synthesis. In Proceedings of the 50th Annual Design Automation
Conference (DAC ’13). ACM, New York, NY, USA, Article 12, 8 pages.
https://doi.org/10.1145/2463209.2488748

[44] Doran Wilde and Sanjay Rajopadhye. 1996. Memory reuse analysis in
the polyhedral model. In Euro-Par’96 Parallel Processing, Luc Bougé,
Pierre Fraigniaud, Anne Mignotte, and Yves Robert (Eds.). Springer
Berlin Heidelberg, Berlin, Heidelberg, 389–397.

[45] Xilinx. 2020.

merlin-compiler.

https://github.com/Xilinx/

merlin-compiler.

[20] L. Kalms and D. Göhringer. 2017. Exploration of OpenCL for FPGAs us-
ing SDAccel and comparison to GPUs and multicore CPUs. In 2017 27th
International Conference on Field Programmable Logic and Applications
(FPL). 1–4. https://doi.org/10.23919/FPL.2017.8056847

[21] Richard M Karp, Raymond E Miller, and Shmuel Winograd. 1967. The
organization of computations for uniform recurrence equations. Jour-
nal of the ACM (JACM) 14, 3 (1967), 563–590.

[22] Ze ke Wang, Bingsheng He, and Wei Zhang. 2015. A study of data par-
titioning on OpenCL-based FPGAs. 2015 25th International Conference
on Field Programmable Logic and Applications (FPL) (2015), 1–8.
[23] David Koeplinger, Matthew Feldman, Raghu Prabhakar, Yaqi Zhang,
Stefan Hadjis, Ruben Fiszel, Tian Zhao, Luigi Nardi, Ardavan Pe-
dram, Christos Kozyrakis, and Kunle Olukotun. 2018. Spatial: A Lan-
guage and Compiler for Application Accelerators. In Proceedings of
the 39th ACM SIGPLAN Conference on Programming Language Design
and Implementation (PLDI 2018). ACM, New York, NY, USA, 296–311.
https://doi.org/10.1145/3192366.3192379

[24] D. Koeplinger, R. Prabhakar, Y. Zhang, C. Delimitrou, C. Kozyrakis, and
K. Olukotun. 2016. Automatic Generation of Efficient Accelerators for
Reconfigurable Hardware. In 2016 ACM/IEEE 43rd Annual International
Symposium on Computer Architecture (ISCA). 115–127. https://doi.
org/10.1109/ISCA.2016.20

[25] Vincent Lefebvre and Paul Feautrier. 1997. Optimizing storage size for
static control programs in automatic parallelizers. In Euro-Par’97 Paral-
lel Processing, Christian Lengauer, Martin Griebl, and Sergei Gorlatch
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 356–363.
[26] Vincent Lefebvre and Paul Feautrier. 1998. Automatic storage manage-
ment for parallel programs. Parallel Comput. 24, 3 (1998), 649 – 671.
https://doi.org/10.1016/S0167-8191(98)00029-5

[27] S. R. Prakash and Y. N. Srikant. 1999. Hyperplane partitioning: an
approach to global data partitioning for distributed memory machines.
In Proceedings 13th International Parallel Processing Symposium and
10th Symposium on Parallel and Distributed Processing. IPPS/SPDP 1999.
744–748. https://doi.org/10.1109/IPPS.1999.760559

[28] Jing Pu, Steven Bell, Xuan Yang, Jeff Setter, Stephen Richardson,
Jonathan Ragan-Kelley, and Mark Horowitz. 2017. Programming Het-
erogeneous Systems from an Image Processing DSL. ACM Trans.
Archit. Code Optim. 14, 3, Article 26 (Aug. 2017), 25 pages. https:
//doi.org/10.1145/3107953

[29] Fabien Quilleré and Sanjay Rajopadhye. 2000. Optimizing Memory
Usage in the Polyhedral Model. ACM Trans. Program. Lang. Syst. 22, 5
(Sept. 2000), 773–815. https://doi.org/10.1145/365151.365152

[30] Patrice Quinton and Vincent Van Dongen. 1989. The mapping of
linear recurrence equations on regular arrays. Journal of VLSI signal
processing systems for signal, image and video technology 1, 2 (1989),
95–113.

[31] Sanjay V Rajopadhye. 1986. Synthesis, optimization and verification
of systolic architectures. Th ese de doctorat, University of Utah, Salt
Lake City, Utah 84112 (1986).

[32] Sanjay V Rajopadhye. 1989. Synthesizing systolic arrays with control
signals from recurrence equations. Distributed Computing 3, 2 (1989),
88–105.

[33] Sanjay V Rajopadhye, S Purushothaman, and Richard M Fujimoto. 1986.
On synthesizing systolic arrays from recurrence equations with linear
dependencies. In International Conference on Foundations of Software
Technology and Theoretical Computer Science. Springer, 488–503.
[34] J Ramanujam and P Sadayappan. 1991. Compile-time techniques for
data distribution in distributed memory machines. IEEE Transactions
on Parallel & Distributed Systems 4 (1991), 472–482.

[35] Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and
David Brooks. 2014. MachSuite: Benchmarks for accelerator design
and customized architectures. 110–119. https://doi.org/10.1109/IISWC.
2014.6983050

