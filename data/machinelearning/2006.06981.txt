1
2
0
2

b
e
F
7
2

]

C
O
.
h
t
a
m

[

3
v
1
8
9
6
0
.
6
0
0
2
:
v
i
X
r
a

Kernel Distributionally Robust Optimization

Jia-Jie Zhu
Empirical Inference Department
Max Planck Institute for Intelligent Systems
Tübingen, Germany
jia-jie.zhu@tuebingen.mpg.de

Wittawat Jitkrittum
Empirical Inference Department
Max Planck Institute for Intelligent Systems
Tübingen, Germany
Currently at Google Research, NYC, USA
wittawatj@gmail.com

Moritz Diehl
Department of Microsystems Engineering
& Department of Mathematics
University of Freiburg
Freiburg, Germany
moritz.diehl@imtek.uni-freiburg.de

Bernhard Schölkopf
Empirical Inference Department
Max Planck Institute for Intelligent Systems
Tübingen, Germany
bernhard.schoelkopf@tuebingen.mpg.de

Abstract

1

INTRODUCTION

We propose kernel distributionally robust op-
timization (Kernel DRO) using insights from
the robust optimization theory and functional
analysis. Our method uses reproducing kernel
Hilbert spaces (RKHS) to construct a wide
range of convex ambiguity sets, which can be
generalized to sets based on integral probabil-
ity metrics and ﬁnite-order moment bounds.
This perspective uniﬁes multiple existing ro-
bust and stochastic optimization methods.
We prove a theorem that generalizes the clas-
sical duality in the mathematical problem of
moments. Enabled by this theorem, we re-
formulate the maximization with respect to
measures in DRO into the dual program that
searches for RKHS functions. Using universal
RKHSs, the theorem applies to a broad class
of loss functions, lifting common limitations
such as polynomial losses and knowledge of
the Lipschitz constant. We then establish a
connection between DRO and stochastic op-
timization with expectation constraints. Fi-
nally, we propose practical algorithms based
on both batch convex solvers and stochastic
functional gradient, which apply to general
optimization and machine learning tasks.

appeared in the proceedings

This work
the
24th International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS) 2021, San Diego, California, USA.
PMLR: Volume 130.

of

Imagine a hypothetical scenario in the illustrative ﬁgure
where we want to arrive at a destination while avoiding
unknown obstacles. A worst-case robust optimization
(RO) (Ben-Tal et al., 2009) approach is then to avoid
the entire unsafe area (left, blue). Suppose we have
historical locations of the obstacles (right, dots). We
may choose to avoid only the convex polytope that
contains all the samples (pink). This data-driven robust
decision-making idea improves eﬃciency while retaining
robustness.

The concept of distributional ambiguity concerns the
uncertainty of uncertainty — the underlying probability
measure is only partially known or subject to change.
This idea is by no means a new one. The classical
moment problem concerns itself with estimating the
(cid:82) l dP where l is
worst-case risk expressed by maxP ∈K
some loss function. The constraint P ∈ K describes
the distribution ambiguity, i.e., P is only known to live
within a subset K of probability measures. The solution
to the moment problem gives the risk under some
worst-case distribution within K. To make decisions
that will minimize this worst-case risk is the idea of
distributionally robust optimization (DRO) (Delage and
Ye, 2010; Scarf, 1958).

Many of today’s learning tasks suﬀer from various man-
ifestations of distributional ambiguity — e.g., covariate

 
 
 
 
 
 
Kernel Distributionally Robust Optimization

shift, adversarial attacks, simulation to reality trans-
fer — phenomena that are caused by the discrepancy
between training and test distributions. Kernel meth-
ods are known to possess robustness properties, e.g.,
(Christmann and Steinwart, 2007; Xu et al., 2009).
However, this robustness only applies to kernelized
models. This paper extends the robustness of kernel
methods using the robust counterpart formulation tech-
niques (Ben-Tal et al., 2009) as well as the principled
conic duality theory (Shapiro, 2001). We term our
approach kernel distributionally robust optimization
(Kernel DRO), which can robustify general optimiza-
tion solutions not limited to kernelized models.

The main contributions of this paper are:

1. We rigorously prove the generalized duality theo-
rem (Theorem 3.1) that reformulates general DRO
into a convex dual problem searching for RKHS
functions, lifting common limitations of DRO on
the loss functions, such as the knowledge of Lip-
schitz constant. The theorem also constitutes a
generalization of the duality results from the liter-
ature of mathematical problem of moments.
2. We use RKHSs to construct a wide range of convex
ambiguity sets (in Table 1, 3), including sets based
on integral probability metrics (IPM) and ﬁnite-
order moment bounds. This perspective uniﬁes
existing RO and DRO methods.

3. We propose computational algorithms based on
both convex solvers and stochastic approximation,
which can be applied to robustify general optimiza-
tion and machine learning models not limited to
kernelized or known-Lipschitz-constant ones.
4. Finally, we establish an explicit connection be-
tween DRO and stochastic optimization with
expectation constraints. This leads to a novel
stochastic functional gradient DRO (SFG-DRO)
algorithm which can scale up to modern machine
learning tasks.

In addition, we give complete self-contained proofs in
the appendix that shed light on the connection be-
tween RKHSs, conic duality, and DRO. We also show
that universal RKHSs are large enough for DRO from
the perspective of functional analysis through concrete
examples.

2 BACKGROUND

Notation. X ⊂ Rd denotes the input domain, which
is assumed to be compact unless otherwise speciﬁed.
P := P(X ) denotes the set of all Borel probability
measures on X . We use ˆP to denote the empirical dis-
tribution ˆP = (cid:80)N
, where δ is a Dirac measure
i=1
and {ξi}N
are data samples. We refer to the function
δC(x) := 0 if x ∈ C, ∞ if x /∈ C, as the indicator func-

1
N δξi

i=1

C(f ) := supµ∈C(cid:104)f, µ(cid:105)H is the support function of
tion. δ∗
C. SN denotes the N -dimensional simplex. ri(·) denotes
the relative interior of a set. A function f is upper semi-
continuous on X if lim supx→x0 f (x) ≤ f (x0), ∀x0 ∈ X ;
it is proper if it is not identically −∞. When there is no
ambiguity, we simplify the loss function notation l(θ, ·)
by using l to indicate that results hold for θ point-wise.

2.1 Robust and distributionally robust

optimization

Robust optimization (RO) (Ben-Tal et al., 2009) stud-
ies mathematical decision-making under uncertainty.
It solves the min-max problem (omitting constraints)
minθ supξ∈X l(θ, ξ), where l(θ, ξ) denotes a general loss
function, θ is the decision variable, and ξ is a variable
representing the uncertainty. Intuitively, RO makes
the decision assuming an adversarial scenario, as re-
ﬂected in taking supremum w.r.t. ξ. For this reason,
it is often referred to as the worst-case RO. Recently,
RO has been applied to the setting of adversarially
robust learning, e.g., in (Madry et al., 2019; Wong and
Kolter, 2018), which we will visit in this paper. In the
optimization literature, a typical approach to solving
RO is via reformulating the min-max program using
duality to obtain a single minimization problem. In
contrast, distributionally robust optimization (DRO)
minimizes the expected loss assuming the worst-case
distribution:

(cid:26) (cid:90)

(cid:27)

l(θ, ξ) dP (ξ)

,

(1)

min
θ

sup
P ∈K

where K ⊆ P, called the ambiguity set, is a subset
of distributions, e.g., all distributions with the given
mean and variance. Compared with RO, DRO only
robustiﬁes the solution against a subset K of distribu-
tions on X and is, therefore, less conservative (since
supP ∈K{(cid:82) l(θ, ξ) dP (ξ)} ≤ supξ∈X l(θ, ξ)).
The inner problem of (1), historically known as the
problem of moments traced back at least to Thomas
Joannes Stieltjes, estimates the worst-case risk under
uncertainty in distributions. The modern approaches,
pioneered by the work of (Isii, 1962) (see also (Lasserre,
2002; Shapiro, 2001; Bertsimas and Popescu, 2005;
Popescu, 2005; Vandenberghe et al., 2007; Van Parys
et al., 2016; Zhu et al., 2020)), typically seek a sharp
upper bound via duality. This duality, rigorously jus-
tiﬁed in (Shapiro, 2001), is diﬀerent from that in the
Euclidean space because inﬁnite-dimensional convex
sets can become pathological. Using that methodol-
ogy, we can reformulate DRO (1) into a single solvable
minimization problem.

Existing DRO approaches can be grouped into three
main categories by the type of ambiguity sets used.
DRO with (ﬁnite-order) moment constraints has been

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

studied in (Delage and Ye, 2010; Scarf, 1958; Zymler
et al., 2013). The authors of (Ben-Tal et al., 2013;
Iyengar, 2005; Nilim and El Ghaoui, 2005; Wang et al.,
2016; Duchi et al., 2016) studied DRO using likelihood
bounds as well as φ−divergence. Wasserstein-distance-
based DRO has been studied by the authors of (Mo-
hajerin Esfahani and Kuhn, 2018; Zhao and Guan,
2018; Gao and Kleywegt, 2016; Blanchet et al., 2019),
and applied in a large body of literature. Many exist-
ing approaches require either the assumptions such as
quadratic loss functions or the knowledge of Lipschitz
constant or RKHS norm of the loss l, which are often
hard to obtain in practice; see (Virmaux and Scaman,
2018; Bietti et al., 2019).

2012). With a universal H, given two distributions P, Q,
(cid:107)µP − µQ(cid:107)H deﬁnes a metric. This quantity is known
as the maximum mean discrepancy (MMD) (Gretton
et al., 2012). With (cid:107)f (cid:107)H := (cid:112)(cid:104)f, f (cid:105)H and the repro-
ducing property, it can be shown that (cid:107)µP − µQ(cid:107)2
H =
Ex,x(cid:48)∼P k(x, x(cid:48)) + Ey,y(cid:48)∼Qk(y, y(cid:48)) − 2Ex∼P,y∼Qk(x, y),
allowing the plug-in estimator to be used for esti-
mating the MMD from empirical data. The MMD
is an instance of the class of integral probability
metrics (IPMs), and can equivalently be written as
(cid:82) f d(P − Q), where the op-
(cid:107)µP − µQ(cid:107)H = sup(cid:107)f (cid:107)H≤1
timum f ∗ is a witness function (Gretton et al., 2012;
Sriperumbudur et al., 2012).

2.2 Reproducing kernel Hilbert spaces

3 THEORY

i=1

(cid:80)n
i=1 ⊂ X , and {ai}n

A symmetric function k : X ×X → R is called a positive
deﬁnite kernel if (cid:80)n
i=1 aiajk(xi, xj) ≥ 0 for any
n ∈ N, {xi}n
i=1 ⊂ R. Given a
positive deﬁnite kernel k, there exists a Hilbert space
H and a feature map φ : X → H, for which k(x, y) =
(cid:104)φ(x), φ(y)(cid:105)H deﬁnes an inner product on H, where H
is a space of real-valued functions on X . The space H
is called a reproducing kernel Hilbert space (RKHS).
It is equipped with the reproducing property: f (x) =
(cid:104)f, φ(x)(cid:105)H for any f ∈ H, x ∈ X . By convention, we
will denote the canonical feature map as φ(x) := k(x, ·).
Properties of the functions in H are inherited from the
properties of k. For instance, if k is continuous, then
any f ∈ H is continuous. A continuous kernel k on
a compact metric space X is said to be universal if
H is dense in C(X ) (Steinwart and Christmann, 2008,
Section 4.5). A universal H can thus be considered
a large RKHS since any continuous function can be
approximated arbitrarily well by a function in H. An
example of a universal kernel is the Gaussian kernel
deﬁned on X where σ > 0 is

k(x, y) = exp
the bandwidth parameter.

− (cid:107)x−y(cid:107)2
2σ2

(cid:16)

(cid:17)

2

RKHSs ﬁrst gained widespread attention following the
advent of the kernelized support vector machine (SVM)
for classiﬁcation problems (Cortes and Vapnik, 1995;
Boser et al., 1992; Schölkopf et al., 2000). More recently,
the use of RKHSs has been extended to manipulat-
ing and comparing probability distributions via kernel
mean embedding (Smola et al., 2007). Given a distri-
bution P , and a (positive deﬁnite) kernel k, the kernel
mean embedding of P is deﬁned as µP := (cid:82) k(x, ·) dP .
If Ex∼P [k(x, x)] < ∞, then µP ∈ H (Smola et al.,
2007, Section 1.2). The reproducing property allows
one to easily compute the expectation of any function
f ∈ H since Ex∼P [f (x)] = (cid:104)f, µP (cid:105)H. Embedding distri-
butions into H also allows one to measure the distance
between distributions in H. If k is universal, then the
mean map P (cid:55)→ µP is injective on P (Gretton et al.,

We make the following assumption for the proof.

Assumption 3.1. l(θ, ·) is proper, upper semicontin-
uous. C is closed convex. ri(KC) (cid:54)= ∅.
This assumption is general in that it does not require
the knowledge of the Lipschitz constant or the RKHS
l(θ, ·) lives in. Generally speaking, the DRO prob-
lem (1) requires two essential elements: an appropriate
ambiguity set that contains meaningful distributions
and a sharp reformulation of the min-max problem.
We ﬁrst present the former in Section 3.1, and then the
latter in Section 3.2. Complete proofs of our theory
are deferred to the appendix.

3.1 Generalized primal formulation

We now present the primal formulation of kernel dis-
tributionally robust optimization (Kernel DRO) as a
generalization of existing DRO frameworks.

(P ) := min

θ

(cid:26)(cid:90)

sup
P,µ

(cid:90)

l(θ, ξ) dP (ξ) :

φ dP = µ, P ∈ P, µ ∈ C

,

(cid:27)

(2)

where H is an RKHS whose feature map is φ. Both
sides of the constraint (cid:82) φ dP = µ are functions in
H. Note µ can be viewed as a generalized moment
vector, which is constrained to lie within the set C ⊆
H, referred to as an (RKHS) ambiguity set. Let us
denote the set of all feasible distributions in (2) as
KC = {P : (cid:82) φ dP = µ, µ ∈ C, P ∈ P}, i.e., KC is the
usual ambiguity set. Intuitively, the set C restricts the
RKHS embeddings of distributions in the ambiguity
set KC. In this paper, we take a geometric perspective
to construct C using convex sets in H. Given data
samples {ξi}N
, we outline various choices for C in the
left column of Table 1 (and 3 in the appendix), and
illustrate our intuition in Figure 1.

i=1

Kernel Distributionally Robust Optimization

Table 1: Examples of support functions for Kernel DRO. See Table 3 for more details.

RKHS ambiguity set C
RKHS norm-ball C = {µ : (cid:107)µ − µ ˆP (cid:107)H ≤ (cid:15)}
Polytope C = conv{φ(ξ1), . . . , φ(ξN )}
Minkowski sum C = (cid:80)N
Whole space C = H

i=1 Ci

Support function δ∗

C(f )

(cid:80)N

i=1 f (ξi) + (cid:15)(cid:107)f (cid:107)H

1
N
maxi f (ξi) (scenario optimization; SVMs with no slack)
(cid:80)N
0 if f = 0, ∞ otherwise (worst-case RO (Ben-Tal et al., 2009))

i=1 δ∗
Ci

(f )

(a) RKHS ambiguity sets C

(b) Interpretation of Kernel DRO

Figure 1: (a): Geometric intuition for choosing am-
biguity set C in H such as norm-ball, polytope, and
Minkowski sum of sets. The scattered points are the
embeddings of empirical samples. See Table 3 for more
examples. (b): Geometric interpretation of Kernel
DRO (4). The (red) curve depicts f0 + f , which ma-
jorizes l(θ, ·) (black). The horizontal axis is ξ. The
dashed lines denote the boundary of the domain X .

To better understand our unifying formulation, let us
examine the celebrated SVM through the lens of our
generalized formulation.

Example 3.1 (SVM as generalized DRO). Let us con-
sider SVM for regression, without using slack variables
or regularization for simplicity. This can be formulated
as optimizing the loss minf ∈H maxi[|yi − f (xi)| − η]+
where η > 0 is the parameter for the hinge loss. This
can be seen as the generalized DRO

(cid:90)

min
f ∈H

sup
P ∈K

[|y − f (x)| − η]+dP (x, y),

where the ambiguity set is given by the polytope K =
clconv{δξ1 , . . . , δξN }, ξi = (xi, yi).

Let us now consider a small RKHS to understand the
eﬀect of diﬀerent RKHSs on Kernel DRO.

Example 3.2 (DRO with non-universal kernels). Con-
sider distributions ˆP = N (0, 1), Qv = N (0, v2) and
H1 induced by the linear kernel k1(x, y)
:= xy.
H1 is small since it only contains linear functions.
MMDk1( ˆP , Qv) = 0, ∀v (cid:54)= 0 since they share the ﬁrst

moment. Therefore, any C that contains µ ˆP
tains all the distributions in {µQv , v (cid:54)= 0}.

also con-

This example shows that small RKHSs force Kernel
DRO to robustify against a large set of distributions,
resulting in conservativeness. In the extreme, if we
choose the smallest possible RKHS H = {0}, then
the space does not contain functions to separate any
distinct distributions. This renders Kernel DRO (4)
overly conservative since we can only choose f = 0 in
(4) — it is precisely reduced to worst-case RO. On the
other extreme, the next example shows the downside
of function spaces that are too large.
Example 3.3 (DRO with large function space). Sup-
pose H is the space of all bounded measurable functions,
the metric induced by H becomes the total variation
distance (Sriperumbudur et al., 2011). While the in-
duced topology is strong, (4) has a trivial solution
f = l, f0 = 0. By plugging this solution into (4), we
recover (2). Hence the reformulation becomes mean-
ingless.

We distinguish between DRO without metrics, e.g., mo-
ment constraints and SVMs, and DRO with probability
metric or divergence, e.g., Wasserstein metric. Let us
ﬁrst examine an instance of the former using Kernel
DRO. We return to the latter at the end of this section.
Example 3.4 (Reduction to DRO with moment con-
straints). Kernel DRO with the second-order polyno-
mial kernel k2(x, y) := (1 + x(cid:62)y)2 and a singleton
ambiguity set C = {µ ˆP } robustiﬁes against all distri-
butions sharing the ﬁrst two moments with ˆP . This
is equivalent to DRO with known ﬁrst two moments,
such as in (Delage and Ye, 2010; Scarf, 1958). More
generally, the choice of the pth-order polynomial kernel
kp(x, y) := (1+x(cid:62)y)p corresponds to DRO with known
ﬁrst p moments.

If H is associated with a universal kernel (e.g., Gaus-
sian), it is large since H is dense in the space of con-
tinuous functions (cf. (Sriperumbudur et al., 2011)).
Then the induced topology (MMD) is strong enough
to separate distinct probability measures. Meanwhile,
RKHS allows for eﬃcient computation using tools from
kernel methods, as shown in Section 4. Therefore, our
insight is that universal RKHSs are large enough for

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

DRO applications.

Remark. For the RKHS associated with the Gaus-
sian kernel, the diameter of the space can be com-
puted:
∀p, q, (cid:107)µp − µq(cid:107)H ≤ (cid:107)µp(cid:107)H + (cid:107)µq(cid:107)H ≤
(cid:112)k(x, y) = 2. Hence, if (cid:15) ≥ 2, C contains
2 supx,y
all probability distributions. Then, Kernel DRO is
again reduced to worst-case RO on domain X .

We now turn to DRO with a generalized class of integral
probability metrics (IPM).

Example 3.5 (Generalization to IPM-DRO). Suppose
(cid:82) f d(P − ˆP ) is the IPM deﬁned
dF (P, ˆP ) := supf ∈F
by some function class F. The IPM-DRO primal for-
mulation is given by

(cid:90)

min
θ

sup
dF (P, ˆP )≤(cid:15)

l(θ, ξ) dP (ξ).

(3)

If we choose the class F = {f : (cid:107)f (cid:107)H ≤ 1}, we recover
Kernel DRO with the RKHS norm-ball set in Table 1.
Similarly, F = {f : lip(f ) ≤ 1} recovers the (type-
1) Wasserstein-DRO. This puts Wasserstein-DRO and
Kernel DRO into a uniﬁed perspective.

3.2 Generalized duality theorem

We now present the main theorem of this paper, the
generalized duality theorem of Kernel DRO (2).

Theorem 3.1 (Generalized Duality). Under As-
sumption 3.1, (2) is equivalent to

(D) := min

θ,f0∈R,f ∈H
subject to

f0 + δ∗

C(f )

(4)

l(θ, ξ) ≤ f0 + f (ξ), ∀ξ ∈ X

where δ∗
C(f ) := supµ∈C(cid:104)f, µ(cid:105)H is the support function
of C, i.e., (P ) = (D), strong duality holds for the inner
moment problem for any θ point-wise.

The theorem holds regardless of the dependency of
l on θ, e.g., non-convexity. If l is convex in θ, then
(4) is a convex program. Formulation (4) has a clear
geometric interpretation: we ﬁnd a function f0 + f that
majorizes l(θ, ·) and subsequently minimize a surrogate
loss involving f0 and f . This is illustrated in Figure 1b.
Note the term duality here refers to the inner moment
problem. The statement can be further simpliﬁed by
replacing f0 +f with f . However, we choose the current
notation for the sake of its explicit connection to RO.

Using the reproducing property of RKHSs and conic
duality, we arrive at (4) with weak duality. Our strong
duality proof is an extension of the conic strong du-
ality in Eulidean spaces. We rely on the existance of
separating hyperplnes between convex sets in locally
convex function spaces, e.g., H. See the illustration
in Figure 2. In our generalized duality theorem, this
separating hyperplane is determined by the witness
function f ∗, which is the optimal dual variable in (4).
See the appendix for the full proof.

Figure 2: Illustration of a separating hyperplane in H
Theorem 3.1 generalizes the classical bounds in gen-
eralized moment problems (Isii, 1962; Lasserre, 2002;
Shapiro, 2001; Bertsimas and Popescu, 2005; Popescu,
2005; Vandenberghe et al., 2007; Van Parys et al., 2016)
to inﬁnitely many moments using RKHSs. A distinc-
tion between Theorem 3.1 and other DRO approaches
is that it uses the density of universal RKHSs to ﬁnd
a surrogate which can sharply bound the worst-case
risk. This means that we do not require the loss l(θ, ·)
to be aﬃne, quadratic, or living in a known RKHS,
nor do we require the knowledge of Lipschitz constant
or RKHS norm of l(θ, ·). To our knowledge, existing
works typically require one of such assumptions.

Moreover, Theorem 3.1 generalizes existing RO and
DRO in the sense that it gives us a unifying tool to
work with various ambiguity and ambiguity sets, which
may be customized for speciﬁc applications. We outline
a few closed-form expressions of the support function
C(f ) in Table 1, and more in Table 3. We now return
δ∗
IPM-DRO with a duality result.

Corollary 3.1.1 (IPM-DRO duality). Given the inte-
(cid:82) f d(P −
gral probability metric dF (P, ˆP ) := supf ∈F
ˆP ), a dual program to (3) is given by

min
θ,λ≥0,f0∈R,f ∈F

f0 +

1
N

N
(cid:88)

i=1

λf (ξi) + λ(cid:15)

(5)

subject to

l(θ, ξ) ≤ f0 + λf (ξ), ∀ξ ∈ X .

The reduction to (4) as a special case can be seen by
replacing λf with f and choosing F = H.

Proof sketch. Our weak duality proof follows stan-
dard paradigms of Lagrangian relaxation by introduing
dual variables. Notably, we associate the functional
constraint (cid:82) φ dP = µ with a dual function f ∈ H,
which is the decision variable in the dual problem (4).

We now establish an explicit connection between DRO
and stochastic optimization with expectation con-
straint, whose solution methods using stochastic ap-
proximation are an topic of active research (Lan and
Zhou, 2020; Xu, 2020).

Kernel Distributionally Robust Optimization

Corollary 3.1.2. (Stochastic optimization with ex-
pectation constraint) Under the Assumption 3.1, the
optimal value of (2) coincides with that of

min
θ,f0∈R,f ∈H
subject to

f0 + δ∗

C(f )

Eh (l (θ, ζ) − f0 − λf (ζ)) ≤ 0

(6)

for some function h that satisﬁes h(t) = 0 if t ≤
0, h(t) > 0 if t > 0, and random variable ζ ∼ µ whose
probability measure places positive mass on any non-
empty open subset of X , i.e., µ(B) > 0, ∀B ⊆ X , B (cid:54)=
∅, B is open.

A choice for h is h(·) = [·]+, which is used in the con-
ditional value-at-risk (Rockafellar and Uryasev, 2000).
We will see the computational implication of Corol-
lary 3.1.2 in Section 4.

N

(cid:80)N

We now establish further theoretical results as a con-
sequence of the generalized duality theorem to help
us understand the geometric intuition of how Kernel
DRO works. By the weak duality (P ) ≤ (D) of (11)
and (12), we have (cid:82) l dP ≤ f0 + δ∗
C(f ). Speciﬁcally, if
C is the RKHS norm-ball in Table 1 , this inequal-
ity becomes (cid:82) l dP ≤ f0 + 1
i=1 f (ξi) + (cid:15)(cid:107)f (cid:107)H. Its
right-hand-side can be seen as a computable bound for
the worst-case risk when generalizing to P . This may
be useful when the Lipschitz constant of l is not known
or hard to obtain, as is often the case in practice. The
following insight is a consequence of a generalization
of the classical complementarity condition of convex
optimization; see the appendix.
Corollary 3.1.3 (Interpolation property). Given θ,
be a set of optimal primal-dual solutions
let P ∗, f ∗, f ∗
0
associated with (P) and (D), then l(θ, ξ) = f ∗
0 + f ∗(ξ)
holds P ∗-almost everywhere.

Intuitively, this result states that f ∗
0 + f ∗ interpolates
the loss l(θ, ·) at the support points of P ∗. This is illus-
trated in Figure 1 (b) and later empirically validated
in Figure 3. We can also see that the size of RKHS H
matters since, if H is small (e.g., H = {0}), f ∗
0 + f ∗
cannot interpolate the loss l well. On the other hand,
the density of universal RKHS allows the interpolation
of general loss functions.

It is tempting to approximately solve (4) by relaxing
the constraint to hold for only the empirical samples,
i.e., l(θ, ξi) ≤ f0 + f (ξi), i = 1 . . . N. The following ob-
servation cautions us against this.
Example 3.6 (Counterexample:
relaxation of the
semi-inﬁnite constraint). Let H be a Gaussian
RKHS with the bandwidth σ =
Suppose
our data set is {0} and the ambiguity set is
C := {µ : (cid:107)µ − φ(0)(cid:107)H ≤ (cid:15)}. Let (cid:15) = (cid:112)2 − 2/e. We
consider the loss function l(ξ) = [|θ + ξ| − 1]+ and re-
laxing the constraint of (4) to only hold at the empirical

2.

√

sample, i.e.,

(d) :=

min
θ,f ∈H,f0∈R
subject to

f0 + f (0) + (cid:15)(cid:107)f (cid:107)H

subject to [|θ| − 1]+ ≤ f0 + f (0)

which admits an optimal solution θ∗ = 0, f ∗ = 0, f ∗
0 = 0
and the worst-case risk (d) = 0. However, let µP (cid:48) =
It is straightforward to verify P (cid:48) ∈
2 φ(2).
1
2 φ(0) + 1
C, (cid:82) l(θ∗, ξ) dP (cid:48)(ξ) = 1
2 > (d), i.e., the solution θ∗ is
not robust against P (cid:48).

4 COMPUTATION

Given a certain parametrization of the RKHS function
f , (4) is a semi-inﬁnite program (SIP) (Guerra Vázquez
et al., 2008). In the following, we propose two computa-
tional methods that do not require a polynomial loss l or
the knowledge of its Lipschitz constant. For simplicity,
we only derive the formulations for the RKHS-norm-
ball ambiguity set, while other formulations are given
in Table 1, 3.

A batch approach by discretization of SIP. We
ﬁrst consider an approach based on the discretization
method of SIP (Guerra Vázquez et al., 2008). Let
us consider an ambiguity set smaller than the RKHS-
norm ball of distributions supported on some {ζj}M
j=1 ⊆
X . Then it suﬃces to consider the following program,
which relaxes the constraint of (4) to ﬁnite support.

min
θ,f ∈H,f0∈R

f0 +

1
N

N
(cid:88)

i=1

f (ξi) + (cid:15)(cid:107)f (cid:107)H

(7)

subject to

l(θ, ζi) ≤ f (ζj) + f0, j = 1 . . . M.

Note (7) is a convex program if l(θ, ξ) in convex in θ.

We can parametrize the RKHS function f by a wealth of
tools from kernel methods, such as the random features
ˆf (ξ) = w(cid:62) ˆφ(ξ) for large scale problems (Rahimi and
Recht, 2008). Alternatively, for small problems, we can
parametrize f by a kernel expansion on the the points
ζi. We provide concrete plug-in forms in the appendix.
As an interesting by-product of (7), let us derive an
unconstrained version of (7), which gives rise to a gen-
eralized risk measure that we term kernel conditional
value-at-risk (Kernel CVaR).
Example 4.1 (Kernel CVaR).

K-CVaRα(X) := inf

f ∈H,f0∈R

1
αM

M
(cid:88)

[X −f (ζj)−f0]+

j=1

+ f0 +

1
N

N
(cid:88)

i=1

f (ξi) + (cid:15)(cid:107)f (cid:107)H.

(8)

If f = 0 and {ζj}M
the classical CVaR (Rockafellar and Uryasev, 2000).

, then (8) is reduced to

j=1 = {ξi}N

i=1

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

Program (7) can be readily solved using oﬀ-the-shelf
convex solvers. However, to scale up to large data
sets, we next develop a stochastic approximation (SA)
method for Kernel DRO.

Stochastic functional gradient DRO. We now
present our SA approach enabled by Theorem 3.1 by em-
ploying two key tools: 1) scalable approximate RKHS
features, such as random Fourier features (Rahimi and
Recht, 2008; Dai et al., 2014; Carratino et al., 2018),
and 2) stochastic approximation with semi-inﬁnite and
expectation constraints (Tadić et al., 2006; Lan and
Zhou, 2020; Baes et al., 2011; Xu, 2020).

Let us summon Corollary 3.1.2 to formulate a stochastic
program with expectation constraint.

min
θ,f0∈R,f ∈H

f0 +

1
N

N
(cid:88)

i=1

f (ξi) + (cid:15)(cid:107)f (cid:107)H

(9)

subject to

E[l (θ, ζ) − f0 − λf (ζ)]+ ≤ 0

where ζ follows a certain proposal distribution on X ,
e.g., uniform or by adaptive sampling. An alternative
is to directly solve (4) using SA techniques with SI
constraints, such as (Tadić et al., 2006; Wei et al.,
2020). (4), (6), and (9) are all convex in function f .
We can compute the functional gradient by

∇f f = φ, ∇f (cid:107)f (cid:107)H =

f
(cid:107)f (cid:107)H

.

(10)

When used with approximate features of the form
ˆf (ξ) = w(cid:62) ˆφ(ξ), we further have ∇w
ˆf (ξ) =
ˆφ(ξ), ∇w(cid:107) ˆf (cid:107)H = w/(cid:107)w(cid:107)2. We outline our stochastic
functional gradient DRO (SFG-DRO) in Algorithm 1.

Algorithm 1 Stochastic Functional Gradient DRO
(SFG-DRO)

1: for k = 1, 2, . . . do
2:

3:

4:

5:

i ).

i ) = w(cid:62) ˆφ(ξk

i } := {xi, yi}. Sam-

Sample mini-batch data {ξk
ple {ζi} from some proposing distribution.
Approximate f , e.g., by random Fourier feature
ˆf (ξk
Estimate the stochastic functional gradient of
the objective and constraint in (9) using (10).
Update θ, f0, f using the functional gradient with
any SA routine with expectation or semi-inﬁnite
constraints, e.g., (Lan and Zhou, 2020; Xu, 2020;
Tadić et al., 2006; Wei et al., 2020) .

Compared with many batch-setting DRO approaches,
SFG-DRO can be used with general model classes, such
as neural networks, and is applicable to a broad class of
optimization and modern learning tasks. The conver-
gence guarantee follows that of the speciﬁc SA routine

used in Step 5 of the algorithm. It is worth noting that,
when used with a primal SA approach such as (Lan
and Zhou, 2020), SFG-DRO completely operates in the
dual space (an RKHS) since Kernel DRO (4) is based
on the generalized duality Theorem 3.1. This interplay
between the primal (measures) and dual (functions) is
the essence of our theory.

5 NUMERICAL STUDIES

This section showcases the applicability of Kernel DRO
(and hence SFG-DRO) and discusses the robustness-
optimality trade-oﬀ. Our purpose is not to bench-
mark state-of-art performances or to demonstrate
the superiority of a speciﬁc algorithm.
Indeed, we
believe both RO and DRO are elegant theoretical
frameworks that have their speciﬁc use cases. We
note that our theory can be applied to a broader
scope of applications than the examples here, such
as stochastic optimal control. See the appendix for
more experimental results. The code is available at
https://github.com/jj-zhu/kdro.

(cid:80)N

Distributionally robust solution to uncertain
least squares. We ﬁrst consider a robust least
squares problem adapted from (El Ghaoui and Le-
bret, 1997), which demonstrated an important appli-
cation of RO to statistical learning historically. (See
also (Boyd et al., 2004, Ch. 6.4).) The task is to
w.r.t. θ. A is mod-
minimize the objective (cid:107)Aθ − b(cid:107)2
2
eled by A(ξ) = A0 + ξA1, where ξ ∈ X is uncertain,
X = [−1, 1], and A0, A1 ∈ R10×10, b ∈ R10 are given.
We compare Kernel DRO against using (a) empirical
risk minimization (ERM; also known as sample average
approximation) that minimizes 1
,
i=1 (cid:107)A(ξi) θ − b(cid:107)2
2
N
(b) worst-case RO via SDP from (El Ghaoui and
Lebret, 1997). We consider a data-driven set-
with the Kernel
ting with given samples {ξi}N
DRO formulation minθ maxP ∈P,µ∈C Eξ∼P (cid:107)A(ξ) θ −
subject to (cid:82) φdP = µ, where we choose the ambi-
b(cid:107)2
2
guity set to be the (cid:15)-norm-ball in the RKHS (Table 1).
i=1(N = 10) are generated
Empirical samples {ξi}N
uniformly from [−0.5, 0.5]. We then apply Kernel
DRO formulation (7). To test the solution, we cre-
ate a distribution shift by generating test samples from
[−0.5 · (1 + ∆), 0.5 · (1 + ∆)], where ∆ is a perturbation
varying within [0, 4]. Figure 3a shows this comparison.
As the perturbation increases, ERM quickly lost ro-
bustness. On the other hand, RO is the most robust
with the trade-oﬀ of being conservative. As expected,
Kernel DRO achieves some level of optimality while
retaining robustness.

i=1

We then ran Kernel DRO with fewer empirical samples
(N = 5) to show the geometric interpretations. We plot

Kernel Distributionally Robust Optimization

(a) Uncertain least squares loss

(b) Geometric interpretation

(c) MNIST classiﬁcation error

(d) (Left) unperturbed data (Center) ERM classiﬁcation result (red indicates errors) (Right) SFG-DRO (Kernel DRO)

Figure 3: Uncertain least squares. (a) This plot depicts the test loss of algorithms. All error bars are in
standard error. We ran 10 independent trials. In each trial, we solved Kernel DRO to obtain θ ∗ and tested it
on a test dataset of 500 samples. We then vary the perturbation ∆ from 0 to 4. (b) (red) is the dual optimal
solution f ∗
0 + f ∗. (black) is the function l(θ∗, ·). The pink bars depict a worst-case distribution while the blue
0 + f ∗ touches loss l(θ∗, ·) at the support of the worst-case
bars the empirical distribution. We can observe that f ∗
distribution P ∗ (pink dots). Note f ∗ (normalized) can be viewed as a witness function of the two distributions.
Classiﬁcation under perturbation (c) We plot the classiﬁcation error rate during test time. The x−axis is
the perturbation magnitude allowed on the test data. For ERM, PGD, and SFG-DRO (Kernel DRO), we train 5
independent models. Each model is tested on 500 randomly sampled images. (d) We visualize the predictions
of ERM and SFG-DRO on the perturbed images with perturbation magnitude ∆ = 0.2. Blue frames indicate
correct predictions while the red ones indicate errors.

0 +f ∗ in Figure 3b. Recall it
the optimal dual solution f ∗
is an over-estimator of the loss l(θ, ·). We solve the inner
moment problem (see appendix) to obtain a worst-case
distribution P ∗. Comparing P ∗ with ˆP , we can observe
the adversarial behavior of the worst-case distribution.
See the caption for more description. From Figure 3b,
we can see that the intuition of Kernel DRO is to
ﬂatten the loss curve using a smooth function.

Distributionally robust learning under adver-
sarial perturbation. We now demonstrate the
framework of SFG-DRO in Algorithm 1 in a non-convex
setting. For simplicity, we consider a MNIST binary
classiﬁcation task with a two-layer neural network. We
emphasize that the deliberate choice of this simple ar-
chitecture ablates factors known to implicitly inﬂuence
robustness, such as regularization and dropout. The
data set contains images of zero and one (i.e., two
classes). Each image x is represented by x ∈ [0, 1]28×28.
The test data is perturbed by an unknown disturbance,
i.e., ˜xtest := x + δ where x ∼ Ptest is the unperturbed
test data and δ is the perturbation. In the plots, δ is
generated by the PGD algorithm (Madry et al., 2019)
using projected gradient descent to ﬁnd the worst-case
perturbation within a box {δ : (cid:107)δ(cid:107)∞ ≤ ∆}}. We com-
pared SFG-DRO (Kernel DRO) with ERM and PGD

(cf. (Madry et al., 2019; Madry)). Note the overall loss
of PGD is an average loss instead of a worst-case one.
Hence it is already less conservative than RO. We train
a classiﬁcation model gθ : x (cid:55)→ y using SFG-DRO in
Algorithm 1, with the SA subroutine of (Lan and Zhou,
2020). During training, we set the ambiguity size of
SFG-DRO as (cid:15) = 0.5 and domain X to be norm-balls
around the training data X = {ζ = X+δ : (cid:107)δ(cid:107)∞ ≤ 0.5}
where X is the training data.

Figure 3d (left) plots unperturbed test samples. Fig-
ure 3c shows the classiﬁcation error rate as we increase
the magnitude of the perturbation ∆. We observe that
ERM attains good performance when there is no test-
time perturbation but quickly underperforms as the
noise level increases. PGD is the most robust under
large perturbation, but has the worst nominal perfor-
mance. SFG-DRO possesses improved robustness while
its performance under no perturbation does not become
much worse. This is consistent with our theoretical
insights into RO and DRO.

01234perturbation 0123test loss0.00.20.40.60.81.00.00.20.40.60.81.0K-DRO =0.5K-DRO =0.1ROERM1010.00.51.01.52.0loss0.00.20.40.60.81.00.00.20.40.60.81.0lossf0+fP*P0.00.20.4test perturbation 0.000.050.100.150.20test error0.000.250.500.751.000.00.20.40.60.81.0ERMPGDSFG-DRO100010100100000011000000001011101000Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

6 OTHER RELATED WORK AND

DISCUSSION

This paper uses similar techniques of reformulating
min-max programs as in (Ben-Tal et al., 2015; Bertsi-
mas et al., 2017), but our ambiguity set is constructed
in an RKHS. Duchi et al. (2020) proposed variational
approximations to marginal DRO to treat covariate
shift in supervised learning. The authors of (Zhu et al.,
2020) used kernel mean embedding for the inner mo-
ment problem (but not DRO) and proved the statis-
tical consistency of the solution. The work of Staib
and Jegelka (2019) used insights from DRO to moti-
vate a regularizer for kernel ridge regression. DRO has
been also applied to Bayesian optimization in (Rontsis
et al., 2020; Kirschner et al., 2020), where the latter
work used MMD ambiguity sets of distributions over
discrete spaces. In terms of scalability, recent works
such as (Sinha et al., 2020; Li et al., 2019; Namkoong
and Duchi, 2016) also explored DRO for modern ma-
chine learning tasks. To the best of our knowledge, no
existing work contains the results such as generalized
ambiguity set constructions in Table 1, 3, generalized
duality theory underpinned by Theorem 3.1, or the
stochastic functional gradient algorithm SFG-DRO.

In summary, this paper proves Theorem 3.1 that gen-
eralizes the classical duality theory in the literature of
mathematical problem of moments and DRO. Using
the density of universal RKHSs, the dual bound in The-
orem 3.1 is sharp while lifting restrictions on the loss
function class. The generalized primal formulations
shed light on the connection between Kernel DRO and
existing robust and stochastic optimization approaches.
Finally, the proposed stochastic approximation algo-
rithm SFG-DRO enables the applications of Kernel
DRO to modern learning tasks.

The compactness assumption on X can be further ex-
tended, as universality can be extended to non-compact
domains (Sriperumbudur et al., 2011). In the special
case of RKHS-norm-ball ambiguity sets, choosing the
size (cid:15) can be motivated using kernel statistical test-
ing (Gretton et al., 2012). However, when DRO is used
in the setting where test distributions are perturbed as
in our examples, existing statistical guarantees in the
literature for unperturbed settings cannot be directly
applied. This is a topic of future work.

Acknowledgements

We thank Daniel Kuhn for the helpful discussion during
a workshop at IPAM, UCLA. We also thank Yassine
Nemmour and Simon Buchholz for sending us their feed-
back on the paper draft. During part of this project,
Jia-Jie Zhu was supported by the European Union’s
Horizon 2020 research and innovation programme un-

der the Marie Skłodowska-Curie grant agreement No
798321. Moritz Diehl would like to acknowledge the
funding support from the DFG via Project DI 905/3-1
(on robust model predictive control)

References
Michel Baes, Michael Bürgisser, and Arkadi Ne-
mirovski. A randomized Mirror-Prox method for
solving structured large-scale matrix saddle-point
problems. arXiv:1112.1274 [math], December 2011.
Alexander Barvinok. A Course in Convexity, volume 54.

American Mathematical Soc., 2002.

Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Ne-
mirovski. Robust Optimization, volume 28. Princeton
University Press, 2009.

Aharon Ben-Tal, Dick den Hertog, Anja De Waege-
naere, Bertrand Melenberg, and Gijs Rennen. Ro-
bust Solutions of Optimization Problems Aﬀected by
Uncertain Probabilities. Management Science, 59(2):
341–357, February 2013. ISSN 0025-1909, 1526-5501.
doi: 10.1287/mnsc.1120.1641.

Aharon Ben-Tal, Dick den Hertog, and Jean-Philippe
Vial. Deriving robust counterparts of nonlinear un-
certain inequalities. Mathematical Programming, 149
(1):265–299, February 2015. ISSN 1436-4646. doi:
10.1007/s10107-014-0750-8.

Dimitris Bertsimas and Ioana Popescu. Optimal In-
equalities in Probability Theory: A Convex Opti-
mization Approach. SIAM Journal on Optimization,
15(3):780–804, January 2005. ISSN 1052-6234, 1095-
7189. doi: 10.1137/S1052623401399903.

Dimitris Bertsimas, Nathan Kallus, and Vishal Gupta.
Data-Driven Robust Optimization. Springer Berlin
Heidelberg, 2017.
ISBN 1010701711258. doi: 10.
1007/s10107-017-1125-8.

Alberto Bietti, Grégoire Mialon, Dexiong Chen, and
Julien Mairal. A Kernel Perspective for Regularizing
Deep Neural Networks. arXiv:1810.00363 [cs, stat],
May 2019.

Jose Blanchet, Yang Kang, and Karthyek Murthy. Ro-
bust Wasserstein Proﬁle Inference and Applications
to Machine Learning. Journal of Applied Probability,
56(03):830–857, September 2019. ISSN 0021-9002,
1475-6072. doi: 10.1017/jpr.2019.49.

Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N.
Vapnik. A training algorithm for optimal margin clas-
siﬁers. In Proceedings of the Fifth Annual Workshop
on Computational Learning Theory, pages 144–152,
1992.

Stephen Boyd, Stephen P. Boyd, and Lieven Vanden-
berghe. Convex Optimization. Cambridge University
Press, March 2004. ISBN 978-0-521-83378-3.

Kernel Distributionally Robust Optimization

G.C. Calaﬁore and M.C. Campi. The scenario approach
to robust control design.
IEEE Transactions on
Automatic Control, 51(5):742–753, May 2006. ISSN
2334-3303. doi: 10.1109/TAC.2006.875041.

Luigi Carratino, Alessandro Rudi, and Lorenzo Rosasco.
Learning with SGD and Random Features. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett, editors, Advances in Neu-
ral Information Processing Systems 31, pages 10192–
10203. Curran Associates, Inc., 2018.

Andreas Christmann and Ingo Steinwart. Consistency
and robustness of kernel-based regression in convex
risk minimization. Bernoulli, 13(3):799–819, August
2007. ISSN 1350-7265. doi: 10.3150/07-BEJ5102.
John B Conway. A course in functional analysis, vol-

ume 96. Springer, 2019.

Corinna Cortes and Vladimir Vapnik. Support-vector
networks. Machine learning, 20(3):273–297, 1995.
Bo Dai, Bo Xie, Niao He, Yingyu Liang, Anant Raj,
Maria-Florina F Balcan, and Le Song. Scalable
Kernel Methods via Doubly Stochastic Gradients.
In Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
3041–3049. Curran Associates, Inc., 2014.

Erick Delage and Yinyu Ye. Distributionally Robust
Optimization Under Moment Uncertainty with Ap-
plication to Data-Driven Problems. Operations Re-
search, 58(3):595–612, June 2010. ISSN 0030-364X,
1526-5463. doi: 10.1287/opre.1090.0741.

John Duchi, Peter Glynn, and Hongseok Namkoong.
Statistics of robust optimization: A generalized
empirical
arXiv preprint
arXiv:1610.03425, 2016.

likelihood approach.

John Duchi, Tatsunori Hashimoto, and Hongseok
Namkoong. Distributionally Robust Losses for La-
tent Covariate Mixtures. arXiv:2007.13982 [cs, stat],
July 2020.

Laurent El Ghaoui and Hervé Lebret. Robust So-
lutions to Least-Squares Problems with Uncertain
Data. SIAM Journal on Matrix Analysis and Ap-
plications, 18(4):1035–1064, October 1997.
ISSN
0895-4798. doi: 10.1137/S0895479896298130.

Rui Gao and Anton J. Kleywegt. Distributionally
Robust Stochastic Optimization with Wasserstein
Distance. arXiv:1604.02199 [math], July 2016.

Arthur Gretton, Karsten M. Borgwardt, Malte J.
Rasch, Bernhard Schölkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning
Research, 13(Mar):723–773, 2012.

F. Guerra Vázquez, J. J. Rückmann, O. Stein, and
G. Still. Generalized semi-inﬁnite programming:

A tutorial. Journal of Computational and Applied
Mathematics, 217(2):394–419, August 2008. ISSN
0377-0427. doi: 10.1016/j.cam.2007.02.012.

Keiiti Isii. On sharpness of tchebycheﬀ-type inequalities.
Annals of the Institute of Statistical Mathematics, 14
(1):185–197, December 1962. ISSN 1572-9052. doi:
10.1007/BF02868641.

Garud N. Iyengar. Robust Dynamic Programming.
Mathematics of Operations Research, 30(2):257–280,
2005. ISSN 0364-765X.

Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka,
and Andreas Krause. Distributionally Robust
Bayesian Optimization. arXiv:2002.09038 [cs, stat],
March 2020.

Guanghui Lan and Zhiqiang Zhou. Algorithms for
stochastic optimization with function or expectation
constraints. Computational Optimization and Appli-
cations, February 2020. ISSN 0926-6003, 1573-2894.
doi: 10.1007/s10589-020-00179-x.

Jean B. Lasserre. Bounds on measures satisfying mo-
ment conditions. The Annals of Applied Probability,
12(3):1114–1137, 2002.

Jiajin Li, Sen Huang, and Anthony Man-Cho So.
A First-Order Algorithmic Framework for Wasser-
stein Distributionally Robust Logistic Regression.
arXiv:1910.12778 [cs, math, stat], October 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig
Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards Deep Learning Models Resistant to Adversar-
ial Attacks. arXiv:1706.06083 [cs, stat], September
2019.

Zico Kolter and Aleksander Madry. Adversarial Ro-
bustness - Theory and Practice. http://adversarial-
ml-tutorial.org/.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-
driven distributionally robust optimization using the
Wasserstein metric: Performance guarantees and
tractable reformulations. Mathematical Program-
ming, 171(1):115–166, September 2018. ISSN 1436-
4646. doi: 10.1007/s10107-017-1172-1.

Hongseok Namkoong and John C Duchi. Stochas-
tic Gradient Methods for Distributionally Robust
Optimization with f-divergences.
In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Gar-
nett, editors, Advances in Neural Information Pro-
cessing Systems 29, pages 2208–2216. Curran Asso-
ciates, Inc., 2016.

Arnab Nilim and Laurent El Ghaoui. Robust Con-
trol of Markov Decision Processes with Uncertain
Transition Matrices. Operations Research, 53(5):780–
798, October 2005. ISSN 0030-364X, 1526-5463. doi:
10.1287/opre.1050.0216.

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

Imre Pólik and Tamás Terlaky. A Survey of the S-
Lemma. SIAM Review, 49(3):371–418, January
2007.
ISSN 0036-1445, 1095-7200. doi: 10.1137/
S003614450444614X.

Ioana Popescu. A Semideﬁnite Programming Approach
to Optimal-Moment Bounds for Convex Classes of
Distributions. Mathematics of Operations Research,
30(3):632–657, August 2005. ISSN 0364-765X, 1526-
5471. doi: 10.1287/moor.1040.0137.

Ali Rahimi and Benjamin Recht. Random Features
for Large-Scale Kernel Machines.
In J. C. Platt,
D. Koller, Y. Singer, and S. T. Roweis, editors, Ad-
vances in Neural Information Processing Systems 20,
pages 1177–1184. Curran Associates, Inc., 2008.
R. Tyrrell Rockafellar. Convex Analysis. Number 28.

Princeton university press, 1970.

R. Tyrrell Rockafellar and Stanislav Uryasev. Opti-
mization of conditional value-at-risk. Journal of risk,
2:21–42, 2000.

W W Rogosinski. Moments of Non-Negative Mass.

page 28.

Nikitas Rontsis, Michael A. Osborne, and Paul J.
Goulart. Distributionally Ambiguous Optimization
for Batch Bayesian Optimization. Journal of Ma-
chine Learning Research, 21(149):1–26, 2020. ISSN
1533-7928.

Herbert Scarf. A min-max solution of an inventory
problem. Studies in the mathematical theory of in-
ventory and production, 1958.

B. Schölkopf, R. Herbrich, and A. J. Smola. A gen-
eralized representer theorem. In D. Helmbold and
R. Williamson, editors, Annual Conference on Com-
putational Learning Theory, number 2111 in Lecture
Notes in Computer Science, pages 416–426, Berlin,
2001. Springer.

Bernhard Schölkopf, Alex J. Smola, Robert C.
Williamson, and Peter L. Bartlett. New support
vector algorithms. Neural computation, 12(5):1207–
1245, 2000.

Alexander Shapiro. On Duality Theory of Conic Lin-
ear Problems. In Panos Pardalos, Miguel Á. Gob-
erna, and Marco A. López, editors, Semi-Inﬁnite
Programming, volume 57, pages 135–165. Springer
US, Boston, MA, 2001. ISBN 978-1-4419-5204-2 978-
1-4757-3403-4. doi: 10.1007/978-1-4757-3403-4_7.

Alexander Shapiro, Darinka Dentcheva, and Andrzej
Ruszczyński. Lectures on Stochastic Programming:
Modeling and Theory. SIAM, 2014.

Aman Sinha, Hongseok Namkoong, Riccardo Volpi,
and John Duchi. Certifying Some Distributional
Robustness with Principled Adversarial Training.
arXiv:1710.10571 [cs, stat], May 2020.

Alex Smola, Arthur Gretton, Le Song, and Bernhard
Schölkopf. A hilbert space embedding for distribu-
tions. In International Conference on Algorithmic
Learning Theory, pages 13–31. Springer, 2007.

Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert
R. G. Lanckriet. Universality, Characteristic Kernels
and RKHS Embedding of Measures. Journal of
Machine Learning Research, 12(Jul):2389–2410, 2011.
ISSN ISSN 1533-7928.

Bharath K. Sriperumbudur, Kenji Fukumizu, Arthur
Gretton, Bernhard Schölkopf, and Gert R. G. Lanck-
riet. On the empirical estimation of integral prob-
ability metrics. Electronic Journal of Statistics, 6:
1550–1599, 2012.
ISSN 1935-7524. doi: 10.1214/
12-EJS722.

Matthew Staib and Stefanie Jegelka. Distribution-
ally robust optimization and generalization in kernel
methods. In Advances in Neural Information Pro-
cessing Systems, pages 9131–9141, 2019.

Ingo Steinwart and Andreas Christmann. Support Vec-
tor Machines. Springer Science & Business Media,
2008.

Vladislav B. Tadić, Sean P. Meyn, and Roberto Tempo.
Randomized Algorithms for Semi-Inﬁnite Program-
In Giuseppe Calaﬁore and Fab-
ming Problems.
rizio Dabbene, editors, Probabilistic and Randomized
Methods for Design under Uncertainty, pages 243–
261. Springer, London, 2006. ISBN 978-1-84628-095-
5. doi: 10.1007/1-84628-095-8_9.

Bart P. G. Van Parys, Paul J. Goulart, and Daniel
Kuhn. Generalized Gauss inequalities via semideﬁ-
nite programming. Mathematical Programming, 156
(1-2):271–302, March 2016. ISSN 0025-5610, 1436-
4646. doi: 10.1007/s10107-015-0878-1.

Lieven. Vandenberghe, Stephen. Boyd, and Kather-
ine. Comanor. Generalized Chebyshev Bounds
via Semideﬁnite Programming. SIAM Review, 49
(1):52–64, January 2007.
ISSN 0036-1445. doi:
10.1137/S0036144504440543.

Aladin Virmaux and Kevin Scaman. Lipschitz regular-
ity of deep neural networks: Analysis and eﬃcient
estimation. In S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-
itors, Advances in Neural Information Processing
Systems 31, pages 3835–3844. Curran Associates,
Inc., 2018.

Zizhuo Wang, Peter W. Glynn, and Yinyu Ye. Like-
lihood robust optimization for data-driven prob-
lems. Computational Management Science, 13(2):
241–261, April 2016. ISSN 1619-697X, 1619-6988.
doi: 10.1007/s10287-015-0240-3.

Bo Wei, William B. Haskell, and Sixiang Zhao. The
CoMirror algorithm with random constraint sam-

Kernel Distributionally Robust Optimization

pling for convex semi-inﬁnite programming. Annals
of Operations Research, September 2020. ISSN 1572-
9338. doi: 10.1007/s10479-020-03766-7.

Eric Wong and Zico Kolter. Provable defenses against
adversarial examples via the convex outer adversarial
polytope. In International Conference on Machine
Learning, pages 5286–5295. PMLR, 2018.

Huan Xu, Constantine Caramanis, and Shie Mannor.
Robustness and regularization of support vector ma-
chines. Journal of machine learning research, 10(7),
2009.

Yangyang Xu.

Primal-Dual Stochastic Gradient
Method for Convex Programs with Many Functional
Constraints. SIAM Journal on Optimization, 30(2):
1664–1692, January 2020.
ISSN 1052-6234, 1095-
7189. doi: 10.1137/18M1229869.

Chaoyue Zhao and Yongpei Guan. Data-driven risk-
averse stochastic optimization with Wasserstein met-
ric. Operations Research Letters, 46(2):262–267,
March 2018.
ISSN 01676377. doi: 10.1016/j.orl.
2018.01.011.

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, and
Bernhard Schölkopf. Worst-Case Risk Quantiﬁcation
under Distributional Ambiguity using Kernel Mean
Embedding in Moment Problem. arXiv:2004.00166
[cs, eess, math], March 2020.

Steve Zymler, Daniel Kuhn, and Berç Rustem. Distribu-
tionally robust joint chance constraints with second-
order moment information. Mathematical Program-
ming, 137(1-2):167–198, February 2013. ISSN 0025-
5610, 1436-4646. doi: 10.1007/s10107-011-0494-7.

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

Appendix: Kernel Distributionally Robust Optimization

A PROOFS OF THEORETICAL RESULTS

Table 2 provides an overview to help readers navigate the theoretical results in this paper.

Table 2: List of the theoretical results in this paper

Theorems

Generalized Duality Theorem 3.1
Strong duality of the inner moment problem Proposition A.1
Interpolation property Proposition 3.1.3
Complementarity condition Lemma A.2
Robust representer theorem Proposition B.1
IPM-DRO duality Corollary 3.1.1
Kernel DRO as stochastic optimization with expectation constraint Corollary 3.1.2

Formulations Kernel DRO primal (P) (2), dual (D) (4)

IPM-DRO primal (3), dual (5)
Formulations for various RKHS ambiguity sets Table 1, 3
Stochastic program with expectation constraint formulation of Kernel DRO (6),(9)
Program to compute worst-case distributions (23),(24),(25)
Kernel DRO convex program by the discretization of SIP (7)
Kernel conditional value-at-risk (8)

In general, we refer to standard texts in optimization (Boyd et al., 2004; Shapiro et al., 2014; Ben-Tal et al.,
2009), convex analysis (Rockafellar, 1970; Barvinok, 2002), and functional analysis (Conway, 2019) for more
mathematical background.

In the proofs, we use M to denote the space of signed measures on X . The dual cone of a set of
Notation.
signed measures K ⊆ M is deﬁned as K∗ := {h : (cid:82) h dm ≥ 0, ∀m ∈ K, h measurable}. Using the reproducing
property, we have the identity (cid:82) f dP = (cid:104)f, µP (cid:105)H for f ∈ H and P ∈ P, which we will frequently use in the
proofs.

A.1 Proof of the Generalized Duality Theorem 3.1

We now derive our key result for Kernel DRO — the Generalized Duality Theorem, in Theorem 3.1. Let us ﬁrst
consider the inner moment problem of (2)

(cid:90)

sup
P ∈P,µ∈C

l dP

subject to

(cid:90)

φ dP = µ,

(11)

where we suppress θ in l(θ, ·) as we ﬁx it for the moment. (11) generalizes the problem of moments in the sense
that the constraint can be viewed as inﬁnite-order moment constraints. Using conic duality, we obtain the strong
duality of the inner moment problem.

Proposition A.1 (Strong dual to (11)). Under Assumption 3.1, (11) is equivalent to solving

min
f0∈R,f ∈H
subject to

δ∗
C(f ) + f0

l(ξ) ≤ f0 + f (ξ), ∀ξ ∈ X

(12)

where δ∗

C is the support function of C, i.e., strong duality holds.

Using Proposition A.1, we can reformulate the inner moment problem in (2) to obtain Theorem 3.1. We now
prove this generalized duality result for the inner moment problem in Proposition A.1. We ﬁrst derive the weak
dual and then prove the strong duality.

Kernel Distributionally Robust Optimization

Proof. We ﬁrst relax the constraint P ∈ P to its conic hull P ∈ co(P). To constrain P to still be a probability
measure, we impose (cid:82) 1 dP (x) = 1, which results in the primal problem equivalent to (11)

(P ) :=

max
P ∈co(K),µ∈C

(cid:90)

l dP

subject to

(cid:90)

(cid:90)

φ dP = µ,

1 dP = 1.

We construct the Lagrangian relaxation by associating the constraints with the dual variables f ∈ H, f0 ∈ R, as
well as adding the indicator function of C. Note both sides of the constraint (cid:82) φ dP = µ are functions in H, hence
the multiplier f is an RKHS function.

L(P, µ; f, f0) =

(cid:90)

l dP − δC(µ) + (cid:104)µ −

(cid:90)

φ dP , f (cid:105)H + f0(1 −

(cid:90)

1 dP )

(cid:90)

=

l dP − δC(µ) + (cid:104)µ, f (cid:105)H −

(cid:90)

f dP + f0 −

(cid:90)

f0 dP

(cid:90)

=

l − f − f0 dP + ((cid:104)µ, f (cid:105)H − δC(µ)) + f0.

(13)

The second equality is due to the reproducing property of RKHS. The dual function is given by

(cid:90)

g(f, f0) = sup
P,µ

l − f − f0 dP + ((cid:104)µ, f (cid:105)H − δC(µ)) + f0.

The ﬁrst term is bounded above by 0 iﬀ l − f − f0 ∈ −K ∗. By Lemma D.1, this conic constraint is equivalent to
the constraint of (12), l(ξ) ≤ f0 + f (ξ), ∀ξ ∈ X .
Finally, expressing the second term using convex conjugate δ∗

C(f ) = supµ(cid:104)µ, f (cid:105)H − δC(µ) concludes the derivation.

Strong duality can potentially be adapted from the strong duality result of moment problem, e.g., (Shapiro, 2001).
However, we give a self-contained proof with only elementary mathematics that sheds light on the connection
between the RKHS theory and distributionally robust optimization. The proof is a generalization of the Euclidean
space conic duality theorem ((Ben-Tal et al., 2009) Theorem A.2.1) to inﬁnite dimensions. Figure 4 illustrates
the idea of the proof.

Figure 4: Illustration of the strong duality proof that uses a separating hyperplane. See the proof for detailed
descriptions.

Proof. We assume the dual optimal value of (12) is ﬁnite (D) < ∞. Since the converse means that the dual
problem is infeasible, which implies that the primal problem is unbounded. Due to the upper semicontinuity of l
in Assumption 3.1, this can not happen on a compact X .
Let us consider the Hilbert space R × H × R equipped with the inner product (cid:104), (cid:105)R + (cid:104),(cid:105)H + (cid:104), (cid:105)R. We construct a
cone in R × H × R

(cid:26) (cid:18)(cid:90)

(cid:90)

(cid:90)

(cid:19)

(cid:27)

1 dP ,

φdP,

l dP

: P ∈ co(P)

,

where co again denotes conic hull.

A =

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

Let t = (P ) denote the optimal primal value. ∀(cid:15) > 0, we construct the set

(cid:26)

(cid:27)

B(cid:15) =

(1, µ, t + (cid:15)) : µ ∈ C

,

which is a closed convex set with non-empty relative interior by Assumption 3.1 (i.e., Slater condition is satisﬁed).

It is straightforward to verify that those two sets do not intersect. Suppose x = (x1, x2, x3) ∈ A ∩ B(cid:15), this means
∃µ(cid:48), P (cid:48) such that x1 = 1 = (cid:82) 1 dP (cid:48), x2 = µ(cid:48) = (cid:82) φdP (cid:48), i.e., µ(cid:48), P (cid:48) is a primal feasible solution. Then the third
coordinate of x satisﬁes x3 = (cid:82) l dP (cid:48) ≤ (P ) < t + (cid:15) = x3, which is impossible. Hence, A ∩ B(cid:15) = ∅.
In the rest of the proof, we will show that, ∀(cid:15) > 0, the dual optimal value (D) satisﬁes

(D) ≤ (P ) + (cid:15).

Combining this with weak duality (D) ≥ (P ) will result in strong duality. We now justify this inequality.

By the separation theorem, (see, e.g., (Barvinok, 2002) Theorem III.3.2, 3.4), there exists a closed hyperplane
that strictly separates A and B(cid:15). The separation is strict because t + (cid:15) > t = (cid:82) l dP . By the Riesz representation
theorem, ∃(f0, f, τ ) ∈ R × H × R, s ∈ R, such that

f0 + (cid:104)f, µ(cid:105)H + τ (t + (cid:15)) < s,

(cid:90)

f0

(cid:90)

1 dP + (cid:104)f,

(cid:90)

φdP (cid:105)H + τ

l dP > s.

Plugging in P = 0, we obtain s < 0. Since P lives in a cone, the left-hand side of the second inequality must be
non-negative. Otherwise, we can scale P so that the separation will fail. In summary, we have

f0 + (cid:104)f, µ(cid:105)H + τ (t + (cid:15)) < 0, ∀µ ∈ C,

(cid:90)

f0

(cid:90)

1 dP + (cid:104)f,

(cid:90)

φdP (cid:105)H + τ

l dP ≥ 0, ∀P ∈ co(P).

(14)

By Assumption 3.1 (Slater condition), the primal problem has a non-empty solution set. Because l is proper and
upper semi-continuous and the feasible solution set for the optimization problem is compact (see Section D) , the
primal optimum is attained by the extreme value theorem. Suppose P ∗ is a primal optimal solution, from the
second inequality of (14),

(cid:90)

f0

(cid:90)

1 dP ∗ + (cid:104)f,

(cid:90)

φdP ∗(cid:105)H + τ

l dP ∗ = f0 + (cid:104)f, µP ∗ (cid:105)H + τ t ≥ 0.

Using this and the ﬁrst inequality of (14), we obtain τ < 0. Without loss of generality, we let τ = −1.

From the second inequality of (14), we have

(cid:90)

f0

1 dP + (cid:104)f,

(cid:90)

φdP (cid:105)H −

(cid:90)

(cid:90)

l dP =

f0 + f − l dP ≥ 0, ∀P ∈ co(P).

This tells us that f0, f is a feasible dual solution because it satisﬁes the semi-inﬁnite constraint in (12).
By the ﬁrst inequality of (14),

f0 + sup
µ∈C

(cid:104)f, µ(cid:105)H ≤ t + (cid:15), ∀(cid:15) > 0,

where the left-hand side is precisely the dual objective in (12). This implies (D) ≤ (P ) + (cid:15). By weak duality,
(D) ≥ (P ). Therefore, strong duality holds.

This proof gives us the third interpretation of the dual variables f0, f — they deﬁne a separating hyperplane of A
and B(cid:15).
Remark. From the proof, we see that the Slater condition in Assumption 3.1 is stronger than needed be. If C
is singleton, we can still ﬁnd a convex neighborhood W(cid:15) of the singleton B(cid:15) since R × H × R is locally convex.
Then W(cid:15) and A can still be strictly separated using the same technique in the proof. Hence strong duality still
holds when C is a singleton.

Kernel Distributionally Robust Optimization

Table 3: Robust counterpart formulations of Kernel DRO.

RKHS ambiguity set C
norm-ball C = {µ : (cid:107)µ − µ ˆP (cid:107)H ≤ (cid:15)}
convex hull C = conv{C1, . . . , CN }
(same under closure clconv{C} )
example: polytope
C = conv{φ(ξ1), . . . , φ(ξN )}
Minkowski sum (cid:80)N

i=1 Ci

example: C = C1 + C2
C1 = {µ : (cid:107)µ(cid:107)H ≤ (cid:15)}
C2 = conv{φ(ξ1), . . . , φ(ξN )}

i=1 αi = 1

aﬃne combination
C = (cid:80)N
i=1 αiCi, (cid:80)N
example: data contamination
C = {αµ ˆP + (1 − α)µQ : µQ ∈ CQ}
Intersection C = ∩N

i=1Ci
multiple kernels Ci ⊆ Hi
example: Ci = {µ : (cid:107)µ − µ ˆP (cid:107)H ≤ (cid:15)i}
(cid:110)(cid:80)N
singleton C =
i=1
entire RKHS C = H

1
N φ(ξi)

(cid:111)

Robust counterpart formulation

(cid:80)N

f0 + 1
N
f0 + maxi δ∗
Ci

(f )

i=1 f (ξi) + (cid:15)(cid:107)f (cid:107)H

f0 + maxi f (ξi)
(equivalent to SVMs/scenario opt. (Calaﬁore and Campi, 2006))
f0 + (cid:80)N
i=1 δ∗
Ci
f0 + maxi f (ξi) + (cid:15)(cid:107)f (cid:107)H

(f )

f0 + (cid:80)N

i=1 αiδ∗
Ci

(f )

f0 + α
N

(cid:80)N

i=1 f (ξi) + (1 − α)δ∗
CQ

(f )

f0 + (cid:80)N
f0 + (cid:80)N
f0 + 1
N

i=1 δ∗
Ci
i=1 δ∗
Ci
(cid:80)N

i=1

(fi), (cid:80)N
i=1 fi = f
(fi) where fi ∈ H
(cid:80)N
i=j fi(ξj) + (cid:15) (cid:80)N

i=1 (cid:107)fi(cid:107)Hi

f0 + 1
N

(cid:80)N

i=1 f (ξi) (equivalent to ERM/SAA)

f0+ δ0(f )
(equivalent to worst-case RO (Ben-Tal et al., 2009))

Finally, we summarize the results above to prove the Kernel DRO Generalized Duality Theorem 3.1

Proof. Theorem (3.1) is obtained by reformulating the inner moment problem in (2) using the strong duality
result in Proposition A.1, i.e.,

(cid:26) (cid:90)

sup
P,µ

min
θ

l(θ, ξ) dP (ξ) :

(cid:90)

φ dP = µ, P ∈ P, µ ∈ C

(cid:27)

(cid:26)

= min

θ

min
f0∈R,f ∈H

f0 + δ∗

C(f ) : l(θ, ξ) ≤ f0 + f (ξ), ∀ξ ∈ X

(cid:27)

,

(15)

which results in formulation (4).

A.2 Table 3 deriving formulations for various choices of RKHS ambiguity set C

We now derive the formulations of support functions for various RKHS ambiguity sets in Table 3.

(RKHS norm-ball)
The support function is given by

Let us consider the ambiguity set of C = {µ : (cid:107)µ − ˆµ(cid:107)H ≤ (cid:15)}, where ˆP = (cid:80)N

1
N δξi

.

i=1

δ∗
C(f ) = sup
µ∈C

(cid:104)f, µ(cid:105)H = (cid:104)f, ˆµ(cid:105)H + sup

(cid:104)f, µ − ˆµ(cid:105)H = (cid:104)f, ˆµ(cid:105)H + (cid:15)(cid:107)f (cid:107)H

(cid:107)µ−ˆµ(cid:107)H≤(cid:15)

where the last equality is by the Cauchy-Schwarz inequality, or alternatively by the self-duality of Hilbert norms.
(Note we assume there exists some µ ∈ H such that (cid:107)µ − ˆµ(cid:107)H = (cid:15).)

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

(Polytope, convex hull of ambiguity set) The result for convex hull follows from standard support function
calculus. If the ambiguity set C is described by the polytope conv{φ(ξ1), . . . , φ(ξN )}, then δ∗
C(f ) = max1≤i≤N f (ξi).
Furthermore, the support function value remains the same under closure operation 1 . The equivalence to
the scenario approach in (Calaﬁore and Campi, 2006) can be seen by noticing that max1≤i≤N l(ξi) ≤ f0 +
max1≤i≤N f (ξi). If H is universal, then there exists f0, f such that the equality is attained.

(Minkowski sum, aﬃne combination, intersection) Those cases follow directly from the support function
calculus; cf. (Ben-Tal et al., 2015).

(Kernel DRO with multiple kernels) Let us consider multiple ambiguity sets from diﬀerent RKHSs.
Suppose H1, . . . , HNh
be the ambiguity
sets in the respective RKHSs. Kernel DRO formulation with multiple kernels is given By

are RKHSs associated with feature maps φ1, . . . , φNh

. Let C1, . . . , CNh

(cid:26)(cid:90)

sup
P,µ

min
θ

l(θ, ξ) dP (ξ) :

(cid:90)

φi dP = µi, P ∈ P, µi ∈ Ci, i = 1 . . . Nh

,

(cid:27)

Using the same proof as Proposition A.1, we have the Kernel DRO reformulation

min
θ,f0∈R,fi∈Hi

f0 +

N
(cid:88)

i=1

δ∗
Ci

(fi)

subject to

l(θ, ξ) ≤ f0 +

N
(cid:88)

i=1

.

fi(ξ), ∀ξ ∈ X

Hence we obtain the formulation in Table 3.

(16)

(17)

(Singleton ambiguity set C =
singleton ambiguity set is given by δ∗

(cid:111)

1
N φ(ξi)

(cid:110)(cid:80)N
i=1
C(f ) = 1
N

(cid:80)N

) By the reproducing property, the support function of the
i=1 f (ξi).

(If C = H, reduction to classical RO)

H(f ) (cid:54)= ∞ iﬀ f = 0. Then (4) is reduced to
δ∗

min
θ,f0∈R
subject to

f0

l(θ, ξ) ≤ f0, ∀ξ ∈ X

(18)

which is the epigraphic form of the worst-case RO. 2

A.3 Complementarity condition and proof

Lemma A.2 (Complementarity condition). Let P ∗, f ∗, f ∗
(D), then

0 be a set of optimal primal-dual solutions of (P) and

(cid:90)

l − f ∗ − f ∗

0 dP ∗ = 0,

C(f ∗) =
δ∗

(cid:90)

f ∗ dP ∗

If C = {µ : (cid:107)µ − µ ˆP (cid:107)H ≤ (cid:15)}, the second equality implies

(cid:90)

f ∗
(cid:107)f ∗(cid:107)H

d(P ∗ − ˆP ) = MMD(P ∗, ˆP ),

which gives a second interpretation of the dual solution f ∗ as a witness function.

It is well known that complementarity condition holds iﬀ strong duality holds in the moment problem; cf. (Shapiro,
2001). The following is a straightforward proof.

1The convex hull can be replaced with its closure clconv(·). Note convex hulls in inﬁnite-dimensional spaces are not

automatically closed; cf. Krein-Milman theorem.

2Note that C = H is no longer closed. However, the resulting ambiguity set becomes P, which is still compact if X is

compact.

(19)

(20)

Kernel Distributionally Robust Optimization

Proof. Plug P ∗, f ∗, f ∗
0

into Lagrangian (13),

(cid:90)

(cid:90)

l dP ∗ ≤

l − f − f0 dP ∗ + δ∗

C(f ∗) + f ∗

0 ≤ δ∗

C(f ∗) + f ∗
0 .

By strong duality, all inequalities above are equalities. Therefore, the ﬁrst equality gives the condition δ∗
(cid:82) f ∗ dP ∗ while the second yields (cid:82) l − f ∗ − f ∗

0 dP ∗ = 0.

C(f ∗) =

A.4 Proof of Proposition 3.1.3 (Interpolation property)

Proof. Since f ∗
0 , f ∗ is a solution to the inner moment problem of Kernel DRO (12), we have l(θ, ξ) ≤
0 + f ∗(ξ), ∀ξ ∈ X for any given θ. By the ﬁrst equation in the complementarity condition (19), we have
f ∗
(cid:82) l − f ∗ − f ∗

0 dP ∗ = 0. Hence the integrand must be zero P ∗-a.e.

A.5 Corollary 3.1.1 IPM-DRO duality

We provide a derivation using a technique alternative to the proof of Proposition A.1.

Proof. We consider the Lagrangian

(cid:90)

L(P ; λ) =

l dP − λ(dF (P, ˆP ) − (cid:15))

(cid:90)

=

l dP − λ sup
f ∈F

(cid:90)

f d(P − ˆP ) + λ(cid:15)

(cid:90)

= inf
f ∈F

l − λf dP + λ

(cid:90)

f d ˆP + λ(cid:15)

The second equality above is due to the dual representation of IPM. The last inequality is due to that the
expectation is always dominated by the supremum. This results in the reformulation

≤ inf
f ∈F

sup
ξ∈X

[l(ξ) − λf (ξ)] +

λ
N

N
(cid:88)

i=1

f (ξi) + λ(cid:15).

(21)

min
θ,λ≥0,f ∈F

sup
ξ∈X

[l(ξ) − λf (ξ)] +

λ
N

N
(cid:88)

i=1

f (ξi) + λ(cid:15).

By introducing the epigraphic variable f0, we obtain the reformulation (5).

A.6 Corollary 3.1.2 Kernel DRO as stochastic optimization with expectation constraint

Using the known relationship between semi-inﬁnite constraint and expectation constraint (see, e.g., (Tadić et al.,
2006, Theorem 1)), the SI constraint in (4) is equivalent to the expectation constraint in (6).

B COMPUTATIONAL FORMULATIONS

We now provide practical plug-in formulations for computation. Speciﬁcally, we can parametrize the RKHS
function f by, e.g., the following methods. We note that the random feature method is well-suited for large scale
problems, such as in SFG-DRO applications.

B.1 Random features

Common ways to parametrize an RKHS function include the representer theorem as well as approximations such
as the random Fourier features (Rahimi and Recht, 2008). Recall that an RKHS function can be approximated
by the ﬁnite feature expansion

f (ξ) ≈ ˆf (ξ) = w(cid:62) ˆφ(ξ), k(x, x(cid:48)) ≈

N
(cid:88)

i=1

ˆφi(x) ˆφi(x(cid:48))

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

are the random features, e.g., random Fourier features ˆφi(x) = cos(wix + bi), wi ∼ N(0, σ2), bi ∼
where { ˆφi(x)}N
i=1
Uniform[0, 2π]. If x is a vector, then wi ∼ N (0, Iσ2), and wix is the dot product. See, e.g., (Rahimi and Recht,
2008), for more properties.

One strength of the Generalized Duality Theorem 3.1 is that it does not require the knowledge of the RKHS that
the loss l lives in, which is typically not available in non-kernelized models. This enables us to use approximate
features for commonly used RKHSs, e.g., random Fourier feature. This is a strength of our Kernel DRO theory.

Note program (7) is a convex optimization problem with the random feature parametrization.

B.2 Distributionally robust version of representer theorem

In program (7), we may parametrize the RKHS function by f (·) = (cid:80)N
√

j=1 γjk(ζj, ·), (cid:107)f (cid:107)H =
α(cid:62)Kα, where α = (β1, . . . , βN , γ1, . . . , γM )(cid:62), K = [k(ηi, ηj)], η = (ξ1, . . . , ξN , ζ1, . . . , ζM )(cid:62). We justify this

i=1 βik(ξi, ·) + (cid:80)M

parametrization by the following DRO version of the RKHS representer theorem (Schölkopf et al., 2001).

The intuition of the following result is to restrict Kernel DRO to a smaller ambiguity set of distributions supported
on {ζi}M
(i.e., replace P ∈ P by P ∈ PM , an inner approximation depending on M ). In this setting, the
i=1
ambiguity set only contains only distributions supported on (a subset of) ζi. Then it suﬃces to parametrize f in
(7) by f (·) = (cid:80)N
i=1 βik(ξi, ·) + (cid:80)M
Lemma B.1 (Robust representer). Given data {ξi}N
the form (cid:80)M
(cid:15)}. Then, it suﬃces to consider the RKHS function of the form f (·) = (cid:80)N
βi, γj ∈ R, i = 1...N, j = 1...M .

i=1 and the ambiguity set chosen to be a set of embeddings with
j=1 αj = 1, and within the RKHS norm-ball C = {µ : (cid:107)µ − µ ˆP (cid:107)H ≤
j=1 γjk(ζj, ·) for some

j=1 αjφ(ζj), for some 0 ≤ αj ≤ 1, (cid:80)M

i=1 βik(ξi, ·) + (cid:80)M

j=1 γjk(ζj, ·).

Lemma B.1 states that the expansion points of the RKHS representer in (7) are exactly the support of the
empirical and worst-case distributions. It extends the classical RKHS representer theorem (Schölkopf et al., 2001),
which uses only the empirical samples as expansion points. The implication is that, to be distributionally robust,
we should choose the representers as in Lemma B.1 instead of only using empirical samples. Below is a proof that
is similar to the original representer theorem.

Proof. In (7) we consider f = fs + f⊥, where fs = (cid:80)N
H and f⊥ its complement. Plug in f = fs + f⊥ to (7) and note the orthogonality, we obtained

i=1 βik(ξi, ·) + (cid:80)M

j=1 γjk(ζj, ·) belongs to a subspace of the

min
θ,fs,f⊥,f0

f0 +

1
N

N
(cid:88)

i=1

fs(ξi) + (cid:15)((cid:107)fs(cid:107)H + (cid:107)f⊥(cid:107)H)

subject to

l(θ, ζi) ≤ fs(ζj) + f0, j = 1 . . . M.

(22)

It suﬃces to choose f⊥ = 0 in this optimization problem. Hence the conclusion follows.

Remark. Note the existence of a worst case distribution in more general settings is not yet proven. The discussion
here is restricted to the setting of (7).

C FURTHER NUMERICAL EXPERIMENT RESULTS

We carry out additional numerical experiments to study Kernel DRO.

C.1 Testing other variants of Kernel DRO

We empirically test the following proposed variants of Kernel DRO.

• Relaxed Kernel DRO formulation (Kernel DRO-relaxed) with constraint hold for only the empirical samples,

i.e., l(θ, ξi) ≤ f0 + f (ξi), i = 1 . . . N.

• Unconstrained Kernel DRO using Kernel CVaR in Example 4.1

Kernel Distributionally Robust Optimization

Figure 5: Comparing Kernel DRO-relaxed, Kernel DRO-KCVaR, ERM, and regular Kernel DRO. y-axis limit is
adjusted to show the plot. All error bars are in standard error.

We compare Kernel DRO-relaxed with the ERM as well as the regular Kernel DRO. Compared with ERM, Kernel
DRO-relaxed still possesses moderate robustness. In this case, we eﬀectively proposed a way to apply RKHS
regularization to general optimization problems, not limited to kernelized models. Hence, it may be used in
practice as a ﬁnite-sample approximation to Kernel DRO.

We then test the Kernel DRO using the unconstrained objective given by Kernel CVaR. We observe no signiﬁcant
diﬀerence in performance between Kernel DRO-KCVaR (with small chance constraint level α.) and regular Kernel
DRO (7).

C.2 Analyzing the generalization behavior

An insight can be obtained by observing the plot of the MMD estimator between the training and test data in
Figure 6 (left). As Kernel DRO with (cid:15) = 0.5 robustiﬁed against perturbation less than the level MMD = 0.5, we see
this threshold was exceeded as we increase the perturbation in test data. Meanwhile, this is the same time (∆ ≈ 1.5)
where Kernel DRO solutions start to exceed the generalization bound (cid:82) l dP ≤ f0 + 1
i=1 f (ξi) + (cid:15)(cid:107)f (cid:107)H, see
Figure 6 (right). This empirically validates our theoretical results for robustiﬁcation.

(cid:80)N

N

Figure 6: (Left) MMD estimator between the empirical samples and test samples. The level MMD = 0.5 is marked
in red. (Right) Loss compared to the generalization bound. As the test data falls outside the robustiﬁcation level
(cid:15), the loss starts to exceed the generalization bound (red) f0 + 1
N

i=1 f (ξi) + (cid:15)(cid:107)f (cid:107)H.

(cid:80)N

C.3 Miscellaneous details for experimental set-up

Robust least squares example. Our experiments are implemented in Python. The convex optimization
problems are solved using ECOS or MOSEK interfaced with CVXPY. In the experiments, we chose the bandwidth
for the Gaussian kernel using the medium heuristic (Gretton et al., 2012). (cid:15) in this paper are ﬁxed to constants
below 2 for Gaussian kernels. Choosing (cid:15) can be further motivated by kernel statistical tests (Gretton et al., 2012)
and is left for future work.

In applying Kernel DRO using (7), we may obtain ζj by simply sampling in X . {ζj}i need not
Sampled ζj
be real data, e.g., in stochastic control, they can be a grid of system states; in learning, they can be synthetic

01234perturbation 012345test lossK-DRO =0.1ERMK-DRO-relaxed =0.1K-DRO-CVaR =0.1 =0.01024perturbation 0.20.30.40.50.6MMD test vs. train01234perturbation 0.00.51.01.52.02.53.0test lossK-DRO =0.5boundJia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

(a) Support sampling

(b) Perturbation

Figure 7: Computing the worst-case distribution P ∗ using (a): (23) samples possible support ζj then optimizes
w.r.t. weights α. (b): (24) moves the empirical samples directly.
samples such as convex combinations of data ζj = (cid:80)N
i=1 aijξi, a·j ∈ SN (simplex), or perturbations ζj = ˆξi + ∆i
where ∆i can be a small perturbation, or they can be obtained by domain knowledge of the speciﬁc application.
In the setting of supervised machine learning, there is a diﬀerence between this paper’s approach of sampling
ζj and commonly used data-augmentation techniques: ζj need not have the correct labels or targets. Directly
training on them may have unforeseen consequences. For example, in the robust least squares experiment, we
sampled the support ζi uniformly random from [−1, 1].

Robust learning under adversarial perturbation example. For the MNIST robust classiﬁcation example,
we used a neural network with two hidden layers with 64 units each. For the training of ERM and PGD, we
used the ADAM optimization routine implemented in the PyTorch library. In Step 3 of SFG-DRO, we used
random Fourier features (Rahimi and Recht, 2008) with 500 features. In Step 5 of SFG-DRO, we used the SA
routine from CSA algorithm (Lan and Zhou, 2020). While other SA routines can be used, we prefer the simplicity
of CSA in that it does not use a dual variable. We set the threshold and step-size of the CSA algorithm (Lan
as suggested in that paper. We did not attempt further adaptive
and Zhou, 2020) to decay at the rate of
tuning of the step-sizes or the proposing distribution for ζ (we generate 3000 samples uniformly in Step 2 of
SFG-DRO), which may further improve the performance. Parameter (weights of the neural nets) averaging is
used for training all models. In the visualization of the predictions in Figure 3d, we perturbed the images by the
PGD method (Madry et al., 2019; Madry) based on the ERM loss and linear model. SFG-DRO does not have
the knowledge of the perturbation method.

1√
k

C.4 Computing worst-case distributions

We have proposed Kernel DRO for making the decision θ via reformulation (4). In practice, it is often useful to
ﬁnd the worst-case distribution P ∗ (e.g., to study adversarial examples). We now propose two practical methods
to compute P ∗ for a given θ, based on support sampling and perturbation, respectively. We illustrate the ideas in
Figure 7.
Support sampling. We consider the moment problem (11) where the distribution is restricted to discrete
distributions supported on some sampled support {ζj}M

max
α∈SM

M
(cid:88)

j=1

αil(θ, ζj)

subject to

j=1 ⊆ X . 2 For any given θ,
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)H
(cid:13)

αiφ(ζj) −

φ(ξi)

N
(cid:88)

M
(cid:88)

1
N

j=1

i=1

≤ (cid:15).

(23)

(23) can be written as a quadratically constrained program with linear objective, which admits a (strong)
semideﬁnite program dual via what is historically known as the S-lemma (Pólik and Terlaky, 2007) (cf. appendix).
Alternatively, (23) can be directly handled by convex solvers for a given θ. Note this approach was previously
used in solving the problem of moments in (Zhu et al., 2020).
Perturbation. Alternatively, we search for worst-case distributions that are perturbations of the empirical
distribution. Let di ∈ X be some perturbation vector, given θ,
(cid:13)
N
(cid:88)
(cid:13)
(cid:13)
(cid:13)

(φ(ξi + di) − φ(ξi))

l(θ, ξi + di)

subject to

N
(cid:88)

1
N

1
N

(24)

≤ (cid:15).

(cid:13)
(cid:13)
(cid:13)
(cid:13)H

max
di,i=1...N,
ξi+di∈X

i=1

i=1

2 Note the sampled support {ζj}M

j=1 need not be real data; they are only the candidates for the worst-case support.

The purpose is to make the the semi-inﬁnite constraint approximately satisﬁed. See the appendix for more details.

1010.00.20.4prob. mass.0.00.20.40.60.81.00.00.51.0P*PKernel Distributionally Robust Optimization

Compared with (23), (24) directly searches for the support of the worst-case distribution. It can be interpreted
as transporting the probability mass from empirical samples ξi to form the worst-case distribution. Depending on
the kernel used, (24) may become a nonlinear program. However, its feasibility is guaranteed since it can always
be initialized with a feasible solution di = 0.
We now empirically examine the support sampling method (23) and perturbation method (24) to recover the
worst-case distribution. Since both programs (23) and (24) search for the worst-case distribution within a subset
of all distributions, their optimal values lower-bound the true worst-case risk (P) in (11), i.e., with ﬁnite samples,
they are optimistic bound.

Under the experimental setting as in Figure 3b, we ran Kernel DRO with fewer empirical samples (N = 5). After
we obtain the Kernel DRO solution θ∗, we plug it into (23) and (24), respectively, to compute the worst-case
distribution P ∗. Figure 7 plots the results. Note (23) is a convex optimization problem, while (24) results in a
nonlinear program (with Gaussian kernel). Nonetheless, we solve it with an always-feasible initialization di = 0.

C.5 SDP dual via S-lemma

We consider a discretized version of the primal moment problem in (23) where the distribution is constrained to
be a discrete distribution. We rewrite (23) as a quadratically constrained program using the plug-in estimator of
MMD,

max
α

M
(cid:88)

i=1

αil(ζi)

subject to α(cid:62)Kzα − 2

1
N

α(cid:62)Kzx1 +

1
N 2 1(cid:62)Kx1 ≤ (cid:15)2

M
(cid:88)

i=1

αi = 1, αi ≥ 0, i = 1 . . . M.

This is a quadratically constrained linear objective convex optimization problem, where the Gram matrix Kz
almost always has exponentially decaying eigenvalues. By applying S-lemma (Pólik and Terlaky, 2007), this
program can be reformulated as the following SDP,

min
λ≥0,x,y≥0,t

subject to

t

(cid:20)

λP

(−λq − 1

2 (l + x · 1 + y))(cid:62)

−λq − 1

(cid:21)
2 (l + x · 1 + y)

t − λ(cid:15)2 + x + λr

≥ 0,

(25)

where P := Kz, q := 1
[l(ζ1), . . . , l(ζM )](cid:62).

N 1(cid:62)Kzx, r := 1

N 2 1(cid:62)Kx1, and Kz = [k(ζi, ζj)]ij, Kzx = [k(ζi, ˆξj)]ij, Kx = [k( ˆξi, ˆξj)]ij, l =

D SUPPORTING LEMMAS

We establish a few technical results that are used in the proofs.

D.1 Reducing conic constraint to inﬁnite constraint

To derive the semi-inﬁnite constraint in (12), we need a standard result from the literature of the moment problem.
We give a self-contained proof below.

Lemma D.1. Let K ∗ be the dual cone to the probability simplex P. The conic constraint l − f − f0 ∈ −K ∗ is
equivalent to

l(θ, ξ) ≤ f0 + f (ξ), ∀ξ ∈ X .

(26)

Proof. “ =⇒ ”: Let us consider the set of all Dirac measures on X , D := {δξ : ξ ∈ X }. For any ξ ∈ X , we have

l(ξ) − f0 − f (ξ) =

(cid:90)

l − f0 − f dδξ ≤ 0.

Jia-Jie Zhu, Wittawat Jitkrittum, Moritz Diehl, Bernhard Schölkopf

Hence suﬃciency.
“ ⇐= ”: Suppose there exists P (cid:48) ∈ co(P) such that (cid:82) l − f0 − f dP (cid:48) > 0. Without loss of generality, we assume
P (cid:48) ∈ P, or we can normalize it to be a probability measure. Then,

(cid:90)

0 <

l − f0 − f dP (cid:48) ≤ sup
ξ∈X

l(ξ) − f0 − f (ξ) ≤ 0.

The second inequality is due to that expectation is always less than or equal to the supremum. The last inequality
holds because l is u.s.c. This double inequality is impossible, hence l − f0 − f ∈ −K ∗.

Note an extension of this result to generating classes other than all Dirac measures D can be proved using Choquet
theory, cf. (Shapiro et al., 2014, Proposition 6.66) (Popescu, 2005, Lemma 3.1), as well as in (Shapiro, 2001;
Rogosinski).

D.2 Compactness of the ambiguity set

We now prove the compactness of the ambiguity set. We use the mean map notation T : P (cid:55)→ µP to denote a
map between the space of P equipped with MMD, and H equipped with its norm. Let us denote the image of a
subset K of measures under T by T (K) := {µP | P ∈ K} ⊆ H. If H is universal, then MMD is a metric. By the
deﬁnition of MMD, T is an isometry (i.e., distance-preserving map) between P and H.

Lemma D.2. T (P) is compact if X is compact.

Proof. If X is compact, by Prokhorov’s theorem P is compact. Since T is an isometry, T (P) is compact.

It is straightforward to verify that T (P) is convex.

Lemma D.3. Let CP = C ∩ T (P). If X is compact, under Assumption 3.1, Cp is compact.

Proof. By the Krein-Milman theorem, the convexity and compactness of T (P) (proved in the previous lemma)
imply that T (P) is closed. By Assumption 3.1, C is closed, which results in the closedness of Cp. Since Cp is a
closed subset of a compact set T (P), it is compact.

Recall that we denote the feasible set of probability measures, i.e., ambiguity set, for primal Kernel DRO (2) by
KC = {P : (cid:82) φ dP = µ, µ ∈ C, P ∈ P}. It is convex by straightforward veriﬁcation. Let us derive the following
compactness property of the ambiguity set.

Lemma D.4. If X is compact, under Assumption 3.1, KC is compact.

Proof. We ﬁrst note KC = T −1(Cp) and T is an isometric isomorphism (i.e., bijective isometry) between KC and
Cp. Then KC is compact since Cp is compact.

