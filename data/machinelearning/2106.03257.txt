1
2
0
2

t
c
O
6
2

]
L
C
.
s
c
[

3
v
7
5
2
3
0
.
6
0
1
2
:
v
i
X
r
a

Structured Reordering for Modeling Latent
Alignments in Sequence Transduction

Bailin Wang1 Mirella Lapata1

Ivan Titov1,2

1University of Edinburgh 2University of Amsterdam
bailin.wang@ed.ac.uk, {mlap, ititov}@inf.ed.ac.uk

Abstract

Despite success in many domains, neural models struggle in settings where train
and test examples are drawn from different distributions. In particular, in contrast
to humans, conventional sequence-to-sequence (seq2seq) models fail to generalize
systematically, i.e., interpret sentences representing novel combinations of concepts
(e.g., text segments) seen in training. Traditional grammar formalisms excel in such
settings by implicitly encoding alignments between input and output segments,
but are hard to scale and maintain. Instead of engineering a grammar, we directly
model segment-to-segment alignments as discrete structured latent variables within
a neural seq2seq model. To efﬁciently explore the large space of alignments, we
introduce a reorder-ﬁrst align-later framework whose central component is a neural
reordering module producing separable permutations. We present an efﬁcient
dynamic programming algorithm performing exact marginal and MAP inference
of separable permutations, and, thus, enabling end-to-end differentiable training of
our model. The resulting seq2seq model exhibits better systematic generalization
than standard models on synthetic problems and NLP tasks (i.e., semantic parsing
and machine translation).

1

Introduction

Recent advances in deep learning have led to ma-
jor progress in many domains, with neural models
sometimes achieving or even surpassing human perfor-
mance [53]. However, these methods often struggle in
out-of-distribution (ood) settings where train and test ex-
amples are drawn from different distributions. In partic-
ular, unlike humans, conventional sequence-to-sequence
(seq2seq) models, widely used in natural language pro-
cessing (NLP), fail to generalize systematically [4, 30, 31],
i.e., correctly interpret sentences representing novel combi-
nations of concepts seen in training. Our goal is to provide
a mechanism for encouraging systematic generalization in
seq2seq models.

To get an intuition about our method, consider the semantic
parsing task shown in Figure 1. A learner needs to map
a natural language (NL) utterance to a program which
can then be executed on a knowledge base. To process
the test utterance, the learner needs to ﬁrst decompose it
into two segments previously observed in training (shown
in green and blue), and then combine their corresponding program fragments to create a new

Figure 1: A semantic parser needs to
generalize to test examples which con-
tain segments from multiple training ex-
amples (shown in green and blue).

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

Training Exampleswhat is the length of the coloradoriver ?len( river( riverid( ‘colorado‘)))what is the longest river ?longest(river(all ) ) )what is the length of the longest river ?len( longest(river(all)))Test Example 
 
 
 
 
 
program. Current seq2seq models fail in this systematic generalization setting [13, 27]. In contrast,
traditional grammar formalisms decompose correspondences between utterances and programs into
compositional mappings of substructures [50], enabling grammar-based parsers to recombine rules
acquired during training, as needed for systematic generalization. Grammars have proven essential in
statistical semantic parsing in the pre-neural era [56, 62], and have gained renewed interest now as a
means of achieving systematic generalization [20, 45]. However, grammars are hard to create and
maintain (e.g., requiring grammar engineering or grammar induction stages) and do not scale well
to NLP problems beyond semantic parsing (e.g., machine translation). In this work, we argue that
the key property of grammar-based models, giving rise to their improved ood performance, is that a
grammar implicitly encodes alignments between input and output segments. For example, in Figure 1,
the expected segment-level alignments are ‘the length → len’ and ‘the longest river →
longest(river(all))’. The encoded alignments allow for explicit decomposition of input and
output into segments, and consistent mapping between input and output segments. In contrast,
decision rules employed by conventional seq2seq models do not exhibit such properties. For example,
recent work [16] shows that primitive units such as words are usually inconsistently mapped across
different contexts, preventing these models from generalizing primitive units to new contexts. Instead
of developing a full-ﬂedged grammar-based method, we directly model segment-level alignments
as structured latent variables. The resulting alignment-driven seq2seq model remains end-to-end
differentiable, and, in principle, applicable to any sequence transduction problem.

Modeling segment-level alignments requires simultaneously inducing a segmentation of input and
output sequences and discovering correspondences between the input and output segments. While
segment-level alignments have been previously incorporated in neural models [54, 60], to maintain
tractability, these approaches support only monotonic alignments. The monotonicity assumption is
reasonable for certain tasks (e.g., summarization), but it is generally overly restrictive (e.g., consider
semantic parsing and machine translation). To relax this assumption, we complement monotonic
alignments with an extra reordering step. That is, we ﬁrst permute the source sequence so that
segments within the reordered sequence can be aligned monotonically to segments of the target
sequence. Coupling latent permutations with monotonic alignments dramatically increases the space
of admissible segment alignments.

The space of general permutations is exceedingly large, so, to allow for efﬁcient training, we restrict
ourselves to separable permutations [5]. We model separable permutations as hierarchical reordering
of segments using permutation trees. This hierarchical way of modeling permutations reﬂects the
hierarchical nature of language and hence is arguably more appropriate than ‘ﬂat’ alternatives [36].
Interestingly, recent studies [49, 51] demonstrated that separable permutations are sufﬁcient for
capturing the variability of permutations in linguistic constructions across natural languages, providing
further motivation for our modeling choice.

Simply marginalizing over all possible separable permutations remains intractable. Instead, inspired
by recent work on modeling latent discrete structures [10, 15], we introduce a continuous relaxation of
the reordering problem. The key ingredients of the relaxation are two inference strategies: marginal
inference, which yields the expected permutation under a distribution; MAP inference, which returns
the most probable permutation. In this work, we propose efﬁcient dynamic programming algorithms
to perform exact marginal and MAP inference with separable permutations, resulting in effective
differentiable neural modules producing relaxed separable permutations. By plugging these modules
into an existing module supporting monotonic segment alignments [60], we obtain end-to-end
differentiable seq2seq models, supporting non-monotonic segment-level alignments.

In summary, our contributions are:

• A general seq2seq model for NLP tasks that accounts for latent non-monotonic segment-level

alignments.

• Novel and efﬁcient algorithms for exact marginal and MAP inference with separable permu-

tations, allowing for end-to-end training using a continuous relaxation.1

• Experiments on synthetic problems and NLP tasks (semantic parsing and machine transla-
tion) showing that modeling segment alignments is beneﬁcial for systematic generalization.

1Our code and data are available at https://github.com/berlino/tensor2struct-public.

2

2 Background and Related Work

2.1 Systematic Generalization

Human learners exhibit systematic generalization, which refers to their ability to generalize from
training data to novel situations. This is possible due to the compositionality of natural languages -
to a large degree, sentences are built using an inventory of primitive concepts and ﬁnite structure-
building mechanisms [9]. For example, if one understands ‘John loves the girl’, they should also
understand ‘The girl loves John’ [14]. This is done by ‘knowing’ the meaning of individual words
and the grammatical principle of subject-verb-object composition. As pointed out by Goodwin et al.
[16], systematicity entails that primitive units have consistent meaning across different contexts. In
contrast, in seq2seq models, the representations of a word are highly inﬂuenced by context (see
experiments in Lake and Baroni [30]). This is also consistent with the observation that seq2seq
models tend to memorize large chunks rather than discover underlying compositional principles [22].
The memorization of large sequences lets the model ﬁt the training distribution but harms out-of-
distribution generalization.

2.2 Discrete Alignments as Conditional Computation Graphs

Latent discrete structures enable the incorporation of inductive biases into neural models and have
been beneﬁcial for a range of problems. For example, input-dependent module layouts [2] or
graphs [40] have been explored in visual question answering. There is also a large body of work on
inducing task-speciﬁc discrete representations (usually trees) for NL sentences [10, 18, 39, 59]. The
trees are induced simultaneously with learning a model performing a computation relying on the
tree (typically a recursive neural network [47]), while optimizing a task-speciﬁc loss. Given the role
the structures play in these approaches – i.e., deﬁning the computation ﬂow – we can think of the
structures as conditional computation graphs.

In this work, we induce discrete alignments as conditional computation graphs to guide seq2seq
models. Given a source sequence x with n tokens and a target sequence y with m tokens, we optimize
the following objective:

X = Encodeθ(x)

Lθ,φ(x, y) = − log Epφ(M |X)pθ(y|X, M )

(1)

where Encode is a function that embeds x into X ∈ Rn×h with h being the hidden size, M ∈
{0, 1}n×m is the alignment matrix between input and output tokens. In this framework, alignments
M are separately predicted by pφ(M |X) to guide the computation pθ(y|X, M ) that maps x to y.
The parameters of both model components (φ and θ) are disjoint.

Relation to Attention Standard encoder-decoder models [3] rely on continuous attention weights
i.e., M [:, i] ∈ (cid:52)n−1 for each target token 1 ≤ i ≤ m. Discrete versions of attention (aka hard
attention) have been studied in previous work [12, 58] and show superior performance in certain
tasks. In the discrete case M is a sequence of m categorical random variables. Though discrete, the
hard attention only considers word-level alignments, i.e., assumes that each target token is aligned
with a single source token. This is a limiting assumption; for example, in traditional statistical
machine translation, word-based models (e.g., [7]) are known to achieve dramatically weaker results
than phrase-based models (e.g., [29]). In this work, we aim to bring the power of phrase-level (aka
segment-level) alignments to neural seq2seq models. 2

3 Latent Segment Alignments via Separable Permutations

Our method integrates a layer of segment-level alignments with a seq2seq model. The architecture of
our model is shown in Figure 2. Central to this model is the alignment network, which decomposes
the alignment problem into two stages: (i) input reordering and (ii) monotonic alignment between the
reordered sequence and the output. Conceptually, we decompose the alignment matrix from Eq 1
into two parts:

M = MpeMmo

(2)

2One of our models (see Section 3.2) still has a ﬂavor of standard continuous attention in that it approximates

discrete alignments with continuous expectation.

3

Figure 2: The architecture of our seq2seq model for semantic parsing. After encoding the input
utterance, our model permutes the input representations using our reordering module. Then, the
reordered encodings will be used for decoding the output program in a monotonic manner.

where Mpe ∈ Rn×n is a permutation matrix, and Mmo ∈ Rn×m represents monotonic alignments.
With this conceptual decomposition, we can rewrite the objective in Eq 1 as follows:
Lθ,φ(x, y) = − log Epφ(Mpe|x)Epφ(cid:48) (Mmo|MpeX)pθ(y|MpeX, Mmo)
where MpeX denotes the reordered representation. With a slight abuse of notation, φ now de-
notes the parameters of the model generating permutations, and φ(cid:48) denotes the parameters used
to produce monotonic alignments. Given the permutation matrix Mpe, the second expectation
Epφ(Mmo|MpeX)pθ(y|MpeX, Mmo), which we denote as pθ,φ(cid:48)(y|MpeX), can be handled by existing
methods, such as SSNT [60] and SWAN [54]. In the rest of the paper, we choose SSNT as the module
for handling monotonic alignment.3 We can rewrite the objective we optimize in the following
compact form:

(3)

Lθ,φ,φ(cid:48)(x, y) = − log Epφ(Mpe|x)pθ,φ(cid:48)(y|MpeX)

(4)

3.1 Structured Latent Reordering by Binary Permutation Trees

Inspired by Steedman [51], we restrict word reorderings to separable permutations. Formally, separa-
ble permutations are deﬁned in terms of binary permutation trees (aka separating trees [5]), i.e., if a per-
mutation can be represented by a permutation tree, it is separable. A binary permutation tree over a per-
mutation of a sequence 1 . . . n is a binary tree in which each node represents the ordering of a segment
i . . . j; the children exhaustively split their parent into sub-segments i . . . k and k + 1 . . . j. Each node
has a binary label that decides whether the segment of the left child precedes that of the right child.
Bracketing transduction grammar [BTG, 57], which is
proposed in the context of machine translation, is the cor-
responding context-free grammar to represent binary per-
mutation trees. Speciﬁcally, BTG has one non-terminal
(X) and three anchored rules:

Si,j,k : X k
i
Ii,j,k : X k
i
i → xi
Ti

: X i+1

Straight
−−−−−→ X j
Inverted
−−−−−−→ X j

i X k
j
i X k
j

Figure 3: The tree represents the re-
the
ordered sentences ‘saw the girl
hedgehog’ where (cid:52), ∧ denotes Inverted
and Straight, respectively.

where X k
i is the anchored non-terminal covering the seg-
ment from i to k (excluding k). The ﬁrst two rules decide
whether to keep or invert two segments when construct-
ing a larger segment; the last rule states that every word xi in an utterance is associated with a
non-terminal X i+1
. An example is shown in Figure 3. Through this example, we note that the
ﬁrst two rules only signify which segments to inverse; an additional process of interpreting the tree
(i.e., performing actual actions of keeping or inverting segments) is needed to obtain the permutated
sequence. This hierarchical approach to generating separable permutations reﬂects the compositional
nature of language, and, thus, appears more appealing than using ‘ﬂat’ alternatives [11, 17, 36].
Moreover, with BTGs, we can incorporate segment-level features to model separable permutations,
and design tractable algorithms for learning and inference.

i

3In our initial experiments, we found that SWAN works as well as SSNT but is considerably slower.

4

countexcludestate_all loc1 river_all howmanystatesdonothaveriversStructuredReorderingMonotonicDecodingoriginalencodingreorderedencodingthegirlsawthehedgehogsawthegirlthehedgehogre-orderedsentenceX1,2X2,3X3,4X4,5X5,6X1,3X4,6X1,4X1,6By assigning a score to each anchored rule using segment-level features, we obtain a distribution over
all possible derivations, and use it to compute the objective in Eq 4.

pφ(D|x) =

(cid:81)

R∈D fφ(R)
Z(x, φ)

, Lθ,φ,φ(cid:48)(x, y) = − log Epφ(D|x)pθ,φ(cid:48)(y|M D

pe X)

(5)

(cid:81)

where fφ is a score function assigning a (non-negative) weight to an anchored rule R ∈ {S, I, T },
Z(x, φ) = (cid:80)
R∈D(cid:48) fφ(R) is the partition function, which can be computed using the inside
D(cid:48)
algorithm, M D
pe is the permutation matrix corresponding to the derivation D. BTG, along with the
weight assigned for each rule, is a weighted context-free grammar (WCFG). In this WCFG, the
weight is only normalized at the derivation level. As we will see in Algorithm 1, we are interested
in normalizing the weight of production rules and converting the WCFG to an equivalent PCFG
following Smith and Johnson [46], so that the probability of a derivation can be computed as follows:

pφ(D|x) =

(cid:89)

R∈D

Gφ(R)

(6)

where Gφ(R) is the weight of the production rule R under the transformed PCFG. The details of the
conversion are provided in the Appendix.

The challenge with optimizing the objective in Eq 5 is that the search space of possible derivations
is exponential, making the estimation of the gradients with respect to parameters of the reordering
component (φ) non-trivial. We now present two differentiable surrogates we use.

3.2 Soft Reordering: Computing Marginal Permutations

The ﬁrst strategy is to use the deterministic ex-
pectation of permutations to softly reorder a
sentence, analogous to the way standard atten-
tion approximates categorical random variables.
Speciﬁcally, we use the following approxima-
tion:

M (cid:48)

pe = Epφ(D|x)M D
pe

Lθ,φ,φ(cid:48)(x, y) ≈ − log pθ,φ(cid:48)(y|M (cid:48)

peX)

where M (cid:48)
pe is the marginal permutation matrix,
and it can be treated as structured attention [28].
Methods for performing marginal inference for
anchored rules, i.e., computing the marginal dis-
tribution of production rules are well-known in
NLP [35]. However, we are interested in the
marginal permutation matrix (or equivalently
the expectation of the matrix components) as
the matrix is the data structure that is ultimately
used in our model. As a key contribution of this
work, we propose an efﬁcient algorithm to ex-
actly compute the marginal permutation matrix
using dynamic programming.

Algorithm 1 Dynamic programming for comput-
ing marginals and differentiable sampling of per-
mutation matrix wrt. a parameterized grammar
Input: Gφ(R): probability of an anchored rule R
sampling: whether perform sampling

i = 1

1: for i := 1 to n do
Ei+1
2:
3: end for
4: for w := 2 to n do
5:
6:
7:
8:
9:
10:
11:
12:
13:

else

14:
15:
16:
17: end for
18: return En+1

end for

(cid:46) width of spans

for i := 1 to n − w + 1 do

k := i + w
if sampling then

ˆGφ(R) = s_arg max(Gφ(R))

(cid:46) computing marginals

ˆGφ(R) = Gφ(R)

end if
for j := i + 1 to k − 1 do
i += ˆGφ(Si,j,k)(Ej
i += ˆGφ(Ii,j,k)(Ej

Ek
Ek
end for

i ⊕ Ek
j )
i (cid:9) Ek
j )

In order to compute the marginal permutation
matrix we need to marginalize over the expo-
nentially many derivations of each permutation.
We propose to map a derivation of BTG into its corresponding permutation matrix in a recursive
manner. Speciﬁcally, we ﬁrst associate word i with an identity permutation matrix M i+1
i = 1; then
we associate Straight and Inverted rules with direct ⊕ and skew (cid:9) sums of permutation matrices,
respectively:

1

A ⊕ B =

A (cid:9) B =

(cid:21)

(cid:20)A 0
0 B

(cid:21)

(cid:20) 0 A
B 0

For example, the permutation matrix of the derivation tree shown in Figure 3 can be obtained by:

(cid:18)

M 6

1 =

(cid:0)(M 2

1 ⊕ M 3

2 ) (cid:9) M 4
3

5

(cid:1) ⊕ (M 5

4 ⊕ M 6
5 )

(cid:19)

(7)

Intuitively, the permutation matrix of long segments can be constructed by composing permutation
matrices of short segments. Motivated by this, we propose a dynamic programming algorithm, which
takes advantage of the observation that we can reuse the permutation matrices of short segments
when computing permutation matrices of long segments, as shown in Algorithm 1. While the above
equation is deﬁned over discrete permutation matrices encoding a single derivation, the algorithm
applies recursive rules to expected permutation matrices. Central to the algorithm is the following
recursion:

(cid:88)

Ek

i =

Gφ(Si,j,k)(Ej

i ⊕ Ek

j ) + Gφ(Ii,j,k)(Ej

i (cid:9) Ek
j )

(8)

i<j<k

where Ek
i is the expected permutation matrix for the segment from i to k, Gφ(R) is the probability
of employing the production rule R, deﬁned in Eq 6. Overall, Algorithm 1 is a bottom-up method
that constructs expected permutation matrices incrementally in Step 13 and 14, while relying on the
probability of the associated production rule. We prove the correctness of this algorithm by induction
in the Appendix.

3.3 Hard Reordering: Gumbel-Permutation by Differentiable Sampling

During inference, for efﬁciency, it is convenient to rely on the most probable derivation D(cid:48) and its
corresponding most probable y:

arg max
y

pθ,φ(cid:48)(y|M D(cid:48)

pe X)

(9)

where D(cid:48) = arg maxD pφ(D|x). The use of discrete permutations M D(cid:48)
pe during inference and soft
reorderings during training lead to a training-inference gap which may be problematic. Inspired by
recent Gumbel-Softmax operator [23, 34] that relaxes the sampling procedure of a categorical distri-
bution using the Gumbel-Max trick, we propose a differentiable procedure to obtain an approximate
sample M D
pe from p(D|x). Concretely, the Gumbel-Softmax operator relaxes the perturb-and-MAP
procedure [42], where we add noises to probability logits and then relax the MAP inference (i.e.,
arg max in the categorical case); we denote this operator as s_arg max. In our structured case, we
perturb the logits of the probabilities of production rules Gφ(R), and relax the structured MAP
inference for our problem. Recall that p(D|x) is converted to a PCFG, and MAP inference for
PCFG is algorithmically similar to marginal inference. Intuitively, for each segment, instead of
marginalizing over all possible production rules in marginal inference, we choose the one with the
highest probability (i.e., a local MAP inference with categorical random variables) during MAP
inference. By relaxing each local MAP inference with Gumbel-Softmax (Step 8 of Algorithm 1),
we obtain a differentiable sampling procedure. 4 We choose Straight-Through Gumbel-Softmax
so that the return of Algorithm 1 is a discrete permutation matrix, and in this way we close the
training-inference gap faced by soft reordering.

Summary We propose two efﬁcient algorithms for computing marginals and obtaining samples
of separable permutations with their distribution parameterized via BTG. In both algorithms, PCFG
plays an important role of decomposing a global problem into sub-problems, which explains why we
convert p(D|x) into a PCFG in Eq 6. Relying on the proposed algorithms, we present two relaxations
of the discrete permutations that let us induce latent reorderings with end-to-end training. We refer to
the resulting system as ReMoto, short for a seq2seq model with Reordered-then-Monotone alignments.
Soft-ReMoto and Hard-ReMoto denote the versions which use soft marginal permutations and hard
Gumbel permutations, respectively.

Segment-Level Alignments Segments are considered as the basic elements being manipulated in
our reordering module. Concretely, permutation matrices are constructed by hierarchically reordering
input segments. SSNT, which is the module on top of our reordering module for monotonically
generating output, conceptually also considers segments as basic elements.
Intuitively, SSNT
alternates between consuming an input segment and generating an output segment. Modeling
segments provides a strong inductive bias, reﬂecting the intuition that sequence transduction in NLP
can be largely accomplished by manipulations at the level of segments. In contrast, there is no explicit
notion of segments in conventional seq2seq methods.

4If we change s_arg max with arg max in Step 8 of Algorithm 1, we will obtain the algorithm for exact

MAP inference.

6

Dataset

Input

Output

Arithmetic
SCAN-SP
GeoQuery

((1 + 9) ∗ ((7 + 8)/4))
jump twice after walk around left thrice
how many states do not have rivers ?

((19+)((78+)4/)∗)
after (twice (jump), thrice(walk (around, left)))
count(exclude(state(all), loc_1(river(all))))

Table 1: Examples of input-output pairs for parsing tasks.

Model
Seq2Seq
LSTM-based Tagging
Sinkhorn-Attention Tagging
Soft-ReMoto

LEN
13.9
57.7
48.2
100.0
100.0
Hard-ReMoto
100.0
Table 2: Accuracy (%) on the arithmetic and SCAN-SP tasks.

Arithmetic
LEN
IID
0.0
100.0
20.6
100.0
8.8
99.5
86.9
100.0
40.9
100.0
83.3
100.0

SCAN-SP
IID
100.0
100.0
100.0
100.0
100.0
100.0

- shared parameters

However, different from our reordering module where segments are ﬁrst-class objects during modeling,
the alternating process of SSNT is realized by a series of token-level decisions (e.g., whether to
keep consuming the next input token). Thus, properties of segments (e.g., segment-level features)
are not fully exploited in SSNT. In this sense, one potential way to further improve ReMoto is to
explore better alternatives to SSNT that can treat segments as ﬁrst-class objects as well. We leave this
direction for future work.

Reordering in Previous Work In traditional statistical machine translation (SMT), reorderings are
typically handled by a distortion model [e.g., 1] in a pipeline manner. Neubig et al. [38], Nakagawa
[37] and Stanojevi´c and Sima’an [48] also use BTGs for modeling reorderings. Stanojevi´c and
Sima’an [48] go beyond binarized grammars, showing how to support 5-ary branching permutation
trees. Still, they assume the word alignments have been produced on a preprocessing step, using an
alignment tool [41]. Relying on these alignments, they induce reorderings. Inversely, we rely on
latent reordering to induce the underlying word and segment alignments.

Reordering modules have been previously used in neural models, and can be assigned to the following
two categories. First, reordering components [8, 21] were proposed for neural machine translation.
However, they are not structured or sufﬁciently constrained in the sense that they may produce invalid
reorderings (e.g., a word is likely to be moved to more than one new position). In contrast, our
module is a principled way of dealing with latent reorderings. Second, the generic permutations
(i.e., one-to-one matchings or sorting), though having differentiable counterparts [11, 17, 36], do not
suit our needs as they are deﬁned in terms of tokens, rather than segments. For comparison, in our
experiments, we design baselines that are based on Gumbel-Sinkhorn Network [36], which is used
previously in NLP (e.g., [33]).

4 Experiments

First, we consider two diagnostic tasks where we can test the neural reordering module on its own.
Then we further assess our general seq2seq model ReMoto on two real-world NLP tasks, namely
semantic parsing and machine translation.

4.1 Diagnostic Tasks

Arithmetic We design a task of converting an arithmetic expression in inﬁx format to the one in
postﬁx format. An example is shown in Table 1. We create a synthetic dataset by sampling data from
a PCFG. In order to generalize, a system needs to learn how to manipulate internal sub-structures (i.e.,
segments) while respecting well-formedness constraints. This task can be solved by the shunting-yard
algorithm but we are interested to see if neural networks can solve it and generalize ood by learning
from raw inﬁx-postﬁx pairs. For standard splits (IID), we randomly sample 20k inﬁx-postﬁx pairs
whose nesting depth is set to be between 1 and 6; 10k, 5k, 5k of these pairs are used as train, dev
and test sets, respectively. To test systematic generalization, we create a Length split (LEN) where

7

training and dev examples remain the same as IID splits, but test examples have a nesting depth of 7.
In this way, we test whether a system can generalize to unseen longer input.

SCAN-SP We use the SCAN dataset [30], which consists of simple English commands coupled
with sequences of discrete actions. Here we use the semantic parsing version, SCAN-SP [20], where
the goal is to predict programs corresponding to the action sequences. An example is shown in Table 1.
As in these experiments our goal is to test the reordering component alone, we remove parentheses
and commas in programs. For example, the program after (twice (jump), thrice(walk
(around, left))) is converted to a sequence: after twice jump thrice walk around
left. In this way, the resulting parentheses-free sequence can be viewed as a reordered sequence of
the NL utterance ‘jump twice after walk around left thrice’. The grammar of the programs is known
so we can reconstruct the original program from the intermediate parentheses-free sequences using
the grammar. Apart from the standard split (IID, aka simple split [30]), we create a Length split
(LEN) where the training set contains NL utterances with a maximum length 5, while utterances in
the dev and test sets have a minimum length of 6.5

In both diagnostic tasks, we use ReMoto with a trivial monotonic alignment
Baselines and Results
matrix Mmo (an identity matrix) in Eq 3. Essentially, ReMoto becomes a sequence tagging model. We
consider three baselines: (1) vanilla Seq2Seq models with Luong attention [32]; (2) an LSTM-based
tagging model which learns the reordering implicitly, and can be viewed as a version ReMoto with a
trivial Mpe and Mmo; (3) Sinkhorn Attention that replaces the permutation matrix of Soft-ReMoto in
Eq 4 by Gumbel-Sinkhorn networks [36].

We report results by averaging over three runs in Table 2. In both datasets, almost all methods achieve
perfect accuracy in IID splits. However, baseline systems cannot generalize well to the challenging
LEN splits. In contrast, our methods, both Soft-ReMoto and Hard-ReMoto, perform very well on
LEN splits, surpassing the best baseline system by large margins (> 40%). The results indicate that
ReMoto, particularly its neural reordering module, has the right inductive bias to learn reorderings.
We also test a variant Soft-ReMoto where parameters θ, φ with shared input embeddings. This variant
does not generalize well to the LEN split on the arithmetic task, showing that it is beneﬁcial to
split models of the ‘syntax’ (i.e., alignment) and ‘semantics’, conﬁrming what has been previously
observed [18, 44].

4.2 Semantic Parsing

Our second experiment is on semantic parsing where ReMoto models the latent alignment between
NL utterances and their corresponding programs. We use GeoQuery dataset [61] which contains 880
utterance-programs pairs. The programs are in variable-free form [25]; an example is shown in Table
1. 6 Similarly to SCAN-SP, we transform the programs into parentheses-free form which have better
structural correspondence with utterances.Again, we can reconstruct the original programs based
on the grammar. An example of such parentheses-free form is shown in Figure 2. Apart from the
standard version, we also experiment with the Chinese and German versions of GeoQuery [24, 52].
Since different languages exhibit divergent word orders [51], the results in the multilingual setting
will tell us if our model can deal with this variability.

In addition to standard IID splits, we create a LEN split where the training examples have parentheses-
free programs with a maximum length 4; the dev and test examples have programs with a minimum
length 5. We also experiment with the TEMP split [20] where training and test examples have
programs with disjoint templates.

Baselines and Results Apart from conventional seq2seq models, for comparison, we also imple-
mented the syntactic attention [44]. Our model ReMoto is similar in spirit to the syntactic attention,
‘syntax’ in their model (i.e., alignment) and ‘semantics’ (i.e., producing the representation relying

5Since we use the program form, the original length split [30], which is based on the length of action

sequence, is not very suitable in our experiments.

6We use the varaible-free form, as opposed to other alternatives such lambda calculus, for two reasons: 1)
variable-free programs have been commonly used in systematic generalization settings [20, 45], probably it is
easier to construct generalization splits using this form; 2) the variable-free form is more suitable for modeling
alignments since variables in programs usually make alignments hard to deﬁne.

8

Model

EN
IID TEMP

LEN

ZH
IID TEMP

LEN

DE
IID TEMP

LEN

Seq2Seq
Syntactic Attention [44]
SSNT [60]
Soft-ReMoto
Hard-ReMoto

15.2
14.2
14.1
13.4
16.6
Table 3: Exact-match accuracy (%) on three splits of the multilingual GeoQuery dataset. Numbers
underlined are signiﬁcantly better than others (p-value ≤ 0.05 using the paired permutation test).

19.8
18.7
17.8
17.3
22.3

25.4
27.9
23.8
30.3
45.7

75.7
74.3
75.3
74.5
75.2

72.5
70.2
71.6
73.4
74.3

21.8
18.3
19.1
19.8
23.2

38.8
39.1
38.7
39.3
43.2

56.1
54.3
55.2
55.8
55.6

18.8
19.3
19.8
19.5
22.3

on the alignment) are separately modeled. In contrast to our structured mechanism for modeling
alignments, their syntactic attention still relies on the conventional attention mechanism. We also com-
pare with SSNT, which can be viewed as an ablated version of ReMoto by removing our reordering
module.

Results are shown in Table 3. For the challenging TEMP and LEN splits, our best performing model
Hard-ReMoto achieves consistently stronger performance than seq2seq, syntactic attention and SSNT.
Thus, our model bridges the gap between conventional seq2seq models and specialized state-of-the-art
grammar-based models [20, 45].7

4.3 Machine Translation

Our ﬁnal experiment is on small-scale machine translation tasks, where ReMoto models the latent
alignments between parallel sentences from two different languages. To probe systematic generaliza-
tion, we also create a LEN split for each language pair in addition to the standard IID splits.

English-Japanese We use the small en-ja dataset extracted from TANKA Corpus. The original
split (IID) has 50k/500/500 examples for train/dev/test with lengths 4-16 words.8 We create a LEN
split where the English sentences of training examples have a maximum length 12 whereas the
English sentences in dev/test have a minimum length 13. The LEN split has 50k/538/538 examples
for train/dev/test, respectively.

Chinese-English We extract a subset from FBIS corpus (LDC2003E14) by ﬁltering English sen-
tences with length 4-30. We randomly shufﬂe the resulting data to obtain an IID split which has
141k/3k/3k examples for train/dev/test, respectively. In addition, we create a LEN split where English
sentences of training examples have a maximum length 29 whereas the English sentences of dev/test
examples have a length 30. The LEN split has 140k/4k/4k examples as train/dev/test sets respectively.

Baselines and Results
In addition
to the conventional seq2seq, we com-
pare with the original SSNT model
which only accounts for monotonic
alignments. We also implemented a
variant that combines SSNT with the
local reordering module [21] as our
baseline to show the advantage of our
structured ordering module.

EN-JA
IID LEN

ZH-EN
IID LEN

Seq2Seq
SSNT [60]
Local Reordering [21]
Soft-ReMoto
Hard-ReMoto

35.6
36.3
36.0
36.6
37.4

25.3
26.5
27.1
27.5
28.7

21.4
20.5
21.8
22.3
22.6

18.1
17.3
17.8
19.2
19.5

Table 4: BLEU scores on the EN-JA and ZH-EN translation.

Results are shown in Table 4. Our
model, especially Hard-ReMoto, con-
sistently outperforms other baselines on both splits. In EN-JA translation, the advantage of our
best-performance Hard-ReMoto is slightly more pronounced in the LEN split than in the IID split. In
ZH-EN translation, while SSNT and its variant do not outperform seq2seq in the LEN split, ReMoto
can still achieve better results than seq2seq. These results show that our model is better than its
alternatives at generalizing to longer sentences for machine translation.

7NQG [45] achieves 35.0% in the English LEN, and SBSP [20] (without lexicon) achieves 65.9% in the

English TEMP in execution accuracy. Both models are augmented with pre-trained representations (BERT).

8https://github.com/odashi/small_parallel_enja

9

original input: 在1
reordered input: 州4

in 美国2

usa
接壤9

state

哪些3

which
最长6

州4

state

与5 最长6

longest

的7 河流8

river

接壤9

的7 河流8

river

与5 哪些3

which 美国2

usa

border
在1
in

border

longest

prediction: state4 next_to_29 longest river6,7,8 loc_2 countryid_ENTITY5,3,2

ground truth: state next_to_2 longest river loc_2 countryid_ENTITY
the3 newspaper4
original input: according1
the3 newspaper4
reordered input: according1
prediction: 新によれば、1,2,3,4,5 昨夜12 大11,9 火事10 があ8,6 った7

there6 was7 a8 big9 ﬁre 10
last11 big9 ﬁre10 a8

,5
,5 night12

to2
to2

ground truth: 新によると昨夜大火事があった

last11 night12
there6 was7

Table 5: Output examples of Chinese semantic parsing and English-Japanese translation. For clarity,
the input words are labeled with position indices, and, for semantic parsing, with English translations.
A prediction consists of multiple segments, each annotated with a superscript referring to input
tokens.

Interpretability Latent alignments, apart from promoting systematic generalization, also lead to
better interpretability as discrete alignments reveal the internal process for generating output. For
example, in Table 5, we show a few examples from our model. Each output segment is associated
with an underlying rationale, i.e. a segment of the reordered input.

5 Conclusion and Future Work

In this work, we propose a new general seq2seq model that accounts for latent segment-level
alignments. Central to this model is a novel structured reordering module which is coupled with
existing modules to handle non-monotonic segment alignments. We model reorderings as separable
permutations and propose an efﬁcient dynamic programming algorithm to perform marginal inference
and sampling.
It allows latent reorderings to be induced with end-to-end training. Empirical
results on both synthetic and real-world datasets show that our model can achieve better systematic
generalization than conventional seq2seq models.

The strong inductive bias introduced by modeling alignments in this work could be potentially
beneﬁcial in weakly-supervised and low-resource settings, such as weakly-supervised semantic
parsing and low-resource machine translation where conventional seq2seq models usually do not
perform well.

Acknowledgements

We thank Miloš Stanojevi´c and Khalil Sima’an for their valuable comments; Lei Yu and Chris Dyer
for providing the preprocessed data for machine translation; the anonymous reviewers for their helpful
feedback. We gratefully acknowledge the support of the European Research Council (Titov: ERC
StG BroadSem 678254; Lapata: ERC CoG TransModal 681760) and the Dutch National Science
Foundation (NWO VIDI 639.022.518).

References

[1] Yaser Al-Onaizan and Kishore Papineni. Distortion models for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational Linguistics, pages 529–536, 2006.

[2] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39–48,
2016.

[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly

learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

[4] Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries,
and Aaron Courville. Systematic generalization: what is required and can it be learned? ICLR,
2019.

10

[5] Prosenjit Bose, Jonathan F Buss, and Anna Lubiw. Pattern matching for permutations. Informa-

tion Processing Letters, 65(5):277–283, 1998.

[6] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy
Bengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349,
2015.

[7] Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, and Robert L Mercer. The
mathematics of statistical machine translation: Parameter estimation. Computational linguistics,
19(2):263–311, 1993.

[8] Kehai Chen, Rui Wang, Masao Utiyama, and Eiichiro Sumita. Neural machine translation
with reordering embeddings. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 1787–1799, 2019.

[9] Noam Chomsky. Aspects of the theory of syntax. Number no. 11 in Massachusetts Institute
of Technology. Research Laboratory of Electronics. Special technical report. The MIT Press,
Cambridge, Massachusetts, 50th anniversary edition edition, 1965. ISBN 978-0-262-52740-8.

[10] Caio Corro and Ivan Titov. Learning latent trees with stochastic perturbations and differentiable
dynamic programming. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 5508–5521, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1551. URL https://www.aclweb.org/
anthology/P19-1551.

[11] Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranks and sorting using

optimal transport. arXiv preprint arXiv:1905.11885, 2019.

[12] Yuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, and Alexander M Rush. Latent alignment

and variational attention. arXiv preprint arXiv:1807.03756, 2018.

[13] Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh
Sadasivam, Rui Zhang, and Dragomir Radev. Improving text-to-SQL evaluation methodology.
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 351–360, Melbourne, Australia, July 2018. Association for
Computational Linguistics. doi: 10.18653/v1/P18-1033. URL https://www.aclweb.org/
anthology/P18-1033.

[14] Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical

analysis. Cognition, 28(1-2):3–71, 1988.

[15] Yao Fu, Chuanqi Tan, Bin Bi, Mosha Chen, Yansong Feng, and Alexander M Rush. Latent

template induction with Gumbel-CRFs. arXiv preprint arXiv:2011.14244, 2020.

[16] Emily Goodwin, Koustuv Sinha, and Timothy J O’Donnell. Probing linguistic systematicity.

arXiv preprint arXiv:2005.04315, 2020.

[17] Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting

networks via continuous relaxations. arXiv preprint arXiv:1903.08850, 2019.

[18] Serhii Havrylov, Germán Kruszewski, and Armand Joulin. Cooperative learning of disjoint
syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 1118–1128, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-1115. URL https://www.aclweb.
org/anthology/N19-1115.

[19] Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534,
2019.

[20] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional general-

ization. arXiv preprint arXiv:2009.06040, 2020.

11

[21] Po-Sen Huang, Chong Wang, Sitao Huang, Dengyong Zhou, and Li Deng. Towards neural

phrase-based machine translation. arXiv preprint arXiv:1706.05565, 2017.

[22] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. The compositionality of neural
networks: integrating symbolism and connectionism. arXiv:1908.08351 [cs, stat], August 2019.
URL http://arxiv.org/abs/1908.08351. arXiv: 1908.08351.

[23] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.

arXiv preprint arXiv:1611.01144, 2016.

[24] Bevan Jones, Mark Johnson, and Sharon Goldwater. Semantic parsing with Bayesian tree
transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 488–496, Jeju Island, Korea, July 2012. Association
for Computational Linguistics. URL https://www.aclweb.org/anthology/P12-1051.

[25] Rohit J Kate, Yuk Wah Wong, and Raymond J Mooney. Learning to transform natural to formal

languages. In AAAI, volume 5, pages 1062–1068, 2005.

[26] Jason Katz-Brown and Michael Collins. Syntactic reordering in preprocessing for japanese
english translation: Mit system description for ntcir-7 patent translation task. In NTCIR, 2008.

[27] Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-
bin, Nikola Momchev, Danila Sinopalnikov, Lukasz Staﬁniak, Tibor Tihon, et al. Measuring
compositional generalization: A comprehensive method on realistic data. arXiv preprint
arXiv:1912.09713, 2019.

[28] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. Structured attention networks.

arXiv preprint arXiv:1702.00887, 2017.

[29] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar,
Alexandra Constantin, and Evan Herbst. Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/P07-2045.

[30] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional
skills of sequence-to-sequence recurrent networks. In International Conference on Machine
Learning, pages 2873–2882. PMLR, 2018.

[31] Joao Loula, Marco Baroni, and Brenden M Lake. Rearranging the familiar: Testing composi-

tional generalization in recurrent networks. arXiv preprint arXiv:1807.07545, 2018.

[32] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-

based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.

[33] Chunchuan Lyu and Ivan Titov. AMR parsing as graph prediction with latent alignment. In
Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 397–407, Melbourne, Australia, July 2018. Association for
Computational Linguistics. doi: 10.18653/v1/P18-1037. URL https://www.aclweb.org/
anthology/P18-1037.

[34] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous

relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[35] Christopher Manning and Hinrich Schutze. Foundations of statistical natural language process-

ing. MIT press, 1999.

[36] Gonzalo Mena, David Belanger, Scott Linderman, and Jasper Snoek. Learning latent permuta-

tions with Gumbel-Sinkhorn networks. arXiv preprint arXiv:1802.08665, 2018.

12

[37] Tetsuji Nakagawa. Efﬁcient top-down BTG parsing for machine translation preordering. In
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long
Papers), pages 208–218, Beijing, China, July 2015. Association for Computational Linguistics.
doi: 10.3115/v1/P15-1021. URL https://www.aclweb.org/anthology/P15-1021.

[38] Graham Neubig, Taro Watanabe, and Shinsuke Mori. Inducing a discriminative parser to opti-
mize machine translation reordering. In Proceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Computational Natural Language Learning,
pages 843–853, Jeju Island, Korea, July 2012. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/D12-1077.

[39] Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie. Sparsemap: Differentiable
sparse structured inference. In International Conference on Machine Learning, pages 3799–
3808. PMLR, 2018.

[40] Will Norcliffe-Brown, Efstathios Vafeias, and Sarah Parisot. Learning conditioned graph
structures for interpretable visual question answering. arXiv preprint arXiv:1806.07243, 2018.

[41] Franz Josef Och and Hermann Ney. A systematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51, 2003. doi: 10.1162/089120103321337421.
URL https://www.aclweb.org/anthology/J03-1002.

[42] George Papandreou and Alan L Yuille. Perturb-and-map random ﬁelds: Using discrete optimiza-
tion to learn and sample from energy models. In 2011 International Conference on Computer
Vision, pages 193–200. IEEE, 2011.

[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative
style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.

[44] Jake Russin, Jason Jo, Randall C O’Reilly, and Yoshua Bengio. Compositional generalization
in a deep seq2seq model by separating syntax and semantics. arXiv preprint arXiv:1904.09708,
2019.

[45] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional
generalization and natural language variation: Can a semantic parsing approach handle both?
arXiv preprint arXiv:2010.12725, 2020.

[46] Noah A Smith and Mark Johnson. Weighted and probabilistic context-free grammars are equally

expressive. Computational Linguistics, 33(4):477–491, 2007.

[47] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.
Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings
of the 2011 conference on empirical methods in natural language processing, pages 151–161,
2011.

[48] Miloš Stanojevi´c and Khalil Sima’an. Reordering grammar induction. In Proceedings of the
2015 Conference on Empirical Methods in Natural Language Processing, pages 44–54, 2015.

[49] Miloš Stanojevi´c and Mark Steedman. Formal basis of a language universal. Computational

Linguistics, pages 1–34, 2018.

[50] Mark Steedman. The syntactic process, volume 24. MIT press Cambridge, MA, 2000.

[51] Mark Steedman. A formal universal of natural language grammar. Language, 96(3):618–660,

2020.

[52] Raymond Hendy Susanto and Wei Lu. Semantic parsing with neural hybrid trees. In Proceedings

of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017.

[53] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. arXiv preprint arXiv:1905.00537, 2019.

13

[54] Chong Wang, Yining Wang, Po-Sen Huang, Abdelrahman Mohamed, Dengyong Zhou, and
Li Deng. Sequence modeling via segmentations. In International Conference on Machine
Learning, pages 3674–3683. PMLR, 2017.

[55] Wenhui Wang and Baobao Chang. Graph-based dependency parsing with bidirectional lstm.
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2306–2315, 2016.

[56] Yuk Wah Wong and Raymond Mooney. Learning for semantic parsing with statistical machine
translation. In Proceedings of the Human Language Technology Conference of the NAACL, Main
Conference, pages 439–446, New York City, USA, June 2006. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/N06-1056.

[57] Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.

Computational linguistics, 23(3):377–403, 1997.

[58] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,
Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. In International conference on machine learning, pages 2048–2057. PMLR,
2015.

[59] Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. Learning to
compose words into sentences with reinforcement learning. arXiv preprint arXiv:1611.09100,
2016.

[60] Lei Yu, Jan Buys, and Phil Blunsom. Online segment to segment neural transduction. arXiv

preprint arXiv:1609.08194, 2016.

[61] John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive
logic programming. In Proceedings of the national conference on artiﬁcial intelligence, pages
1050–1055, 1996.

[62] Luke S Zettlemoyer and Michael Collins. Learning to map sentences to logical form: Structured

classiﬁcation with probabilistic categorial grammars. arXiv preprint arXiv:1207.1420, 2012.

14

A Appendix

A.1 WCFG to PCFG Conversion

The algorithm of converting a WCFG to its equivalent PCFG is shown in Algorithm 2. In a bottom-up
manner, the algorithm ﬁrst computes an inner weight β[X k
i ] for each segment, which is the total
weight of all derivations with root X k
i . Then the algorithm normalizes the weight of production rules
whose left-hand side is X k
i using the inner weight. The resulting normalized weight for a production
j ], is the conditional probability of applying the rule X k
i X k
rule, e.g., G[X k
j given
i
i
the presence of the segment X k
i . The PCFG is equivalent to the original WCFG in the sense that for
each derivation D, we have

S−→ X j

S−→ X j

i X k

pφ(D|x) =

(cid:81)

R∈D fφ(R)
Z(x, φ)

(cid:89)

=

R∈D

Gφ(R)

where Z(x, φ) = (cid:80)
R∈D(cid:48) fφ(R). Full proof of this equivalence can be found in Smith and
Johnson [46]. The factorization of the derivation-level probability to rule-level probability facilitates
our design of dynamic programming for marginal inference.

(cid:81)

D(cid:48)

Algorithm 2 Converting WCFG to PCFG

] = 1

β[X i+1
i

1: initialize β[. . . ] to 0
2: for i := 0 to n − 1 do
3:
4: end for
5: for w := 2 to n do
6:
7:
8:

for i := 0 to n − w do

k := i + w
for j := i + 1 to k − 1 do

9:

10:
11:
12:

13:

14:

β[X k
β[X k

i ]+ = fφ(X k
i
i ]+ = fφ(X k
i

S−→ X j
I−→ X j

i X k
i X k

j )β[X j
j )β[X j

i ]β[X k
j ]
i ]β[X k
j ]

end for
for j := i + 1 to k − 1 do
S−→ X j

G(X k
i

j ) =

i X k

fφ(X k
i

S−→X j

G(X k
i

I−→ X j

i X k

j ) =

fφ(X k
i

I−→X j

j )β[X j
i X k
β[X k
i ]
j )β[X j
i X k
β[X k
i ]

i ]β[X k
j ]

i ]β[X k
j ]

end for

end for

15:
16:
17: end for
18: return G[. . . ]

(cid:46) width-1 spans

(cid:46) width of spans
(cid:46) start point
(cid:46) end point
(cid:46) compute inner weight
(cid:46) S: Straight

(cid:46) I: Inverted

(cid:46) normalize weight

A.2 Proof of the Dynamic Programming for Marginal Inference

We prove the correctness of the dynamic programming algorithm for computing the marginal
permutation matrix of separable permutations by induction as follows.

Proof. As a base case, each word (i.e., segment with length 1) is associated with an identity per-
mutation matrix 1. Then we assume that the marginal permutation matrix for all segments with
length 1 < k − i < n is Ek
i is the derivation tree
of segment i to k, and M (Dk
i . It is obvious that

p(Dk
i ) is the permutation matrix corresponding to Dk

i , which is deﬁned as E

i )] where Dk

i )[M (Dk

15

Figure 4: The detailed architecture of our seq2seq model for semantic parsing (view in color). First,
the structured reordering module genearates a (relaxed) permutation matrix given the input utterrance.
Then, the encoding module generates the representations of the input utterance based on the reordered
embeddings, which are computed based on the original embedding and the permutation matrix
computed in the ﬁrst step. Finally, the decoding module, namely SSNT, generates the output program
monotonically based on the input encodings.

Name

Range

[128, 256, 512]
embedding size
[1,2]
number of encoder LSTM layer
[128, 256, 512]
encoder LSTM hidden size
[1,2]
decoder LSTM layer
[128, 256, 512]
decoder LSTM hidden size
[0.1, 0.3, 0.5, 0.7, 0.9]
decoder dropout
[0.1, 1, 2, 10]
temperature of Gumbel-softmax
label smoothing
[0.0, 0.1]
Table 6: Main hyperparameters of ReMoto.

Ei+1

i = 1. The marginal permutation matrix for all segments with length n can be obtained by

Ek

i = E

p(Dk
(cid:88)

i )[M (Dk
i )]
(cid:16)
Gφ(Si,j,k)(cid:0)E

i<j<k

p(Dj

i )[M (Dj

i )]) ⊕ E

p(Dk

+ Gφ(Ii,j,k)(cid:0)E
(cid:88)

(cid:16)

Gφ(Si,j,k)(Ej

p(Dj

i )[M (Dj
i ⊕ Ek

i )] (cid:9) E

p(Dk

j )[M (Dk

j ) + Gφ(Ii,j,k)(Ej

j )](cid:1)

j )[M (Dk
j )](cid:1)(cid:17)
(cid:17)
i (cid:9) Ek
j )

=

=

i<j<k

where in the second step we consider all the possible expansions of the derivation tree Dk
i ; in the
third step, we obtain the recursion that is used in Step 12-14 of Algorithm 1 by reusing the marginal
permutations matrices of shorter segments.

A.3 Architecture and Hyperparameters

The detailed architecture of ReMoto is shown in Figure 4. In the structured reordering module, we
compute the scores for BTG production rules using span embeddings [55] followed by a multi-layer
perceptron. Speciﬁcally, the score function for each rule has form G(Ri,j,k) = MLP(sij, sjk), where
sij and sjk are the span embeddings based on [55], MLP is a multi-layer perceptron that outputs
a 2-d vector, which corresponds to the score of R=Straight and R=Inverted, respectively. Similar
to a conventional LSTM-based encoder-decoder model, LSTMs used in structured reordering and

16

Input:howmanystatesdonothaveriversEmbeddingLSTMParserStructuredReorderingEmbeddingLSTMReorderedEmbeddingMonotonicDecodingSSNTcount exclude state_all loc1 river_all Output:permutationmatrixinputencodingsinput/outputlinkintra-modulelinkinter-modulelinkEncodingwithReorderingencoding module are bidirectional whereas the LSTM for decoding (within SSNT) is unidirectional.
We implemented all models using Pytorch [43]. We list the main hyperparameters we tuned are
shown in Table 6. The full hyperparameters for each experiment will be released along with the code.

A.4 Training Strategy

Empirically, we found that during training the structured reordering module tends to converge to a
sub-optimal point where it develops a simple reordering strategy and the subsequent modules (i.e.,
the encoding and decoding module in Figure 4) quickly adapt to naive reorderings. For example,
in the EN-JA translation task, the reordering module tends to completely invert the input English
translation after training. While this simple strategy proves to be a useful heuristic [26], we would
like more accurate reordering to emerge during training. This issue is similar to posterior collapse [6],
a common issue in training variational autoencoders.

Inspired by He et al. [19], we speculate that the issue occurred due to that optimization of the
structured reordering module usually lags far behind the optimization of subsequent modules during
the initial stages of training. We use a simple training strategy to alleviate the issue. Speciﬁcally,
during the initial M training steps, with a certain probability p, we only update the parameters of
the structured reordering module and ignore the gradients of the parameters from the subsequence
modules. M and p are treated as hyperparameters. With this strategy, the structured reordering
module is updated more often than the subsequent modules, and has a better chance to catch up with
the optimization of subsequent modules. We ﬁnd that this simple training strategy usually leads to
better segment alignments and better performance.

17

