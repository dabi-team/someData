1
2
0
2

c
e
D
6
2

]

G
L
.
s
c
[

1
v
8
1
4
3
1
.
2
1
1
2
:
v
i
X
r
a

Neuro-Symbolic Hierarchical Rule Induction

Claire Glanois † Xuening Feng † Zhaohui Jiang †
Paul Weng † Matthieu Zimmer † Dong Li ‡ Wulong Liu ‡

Abstract

We propose an efﬁcient
interpretable neuro-symbolic
model to solve Inductive Logic Programming (ILP) prob-
lems. In this model, which is built from a set of meta-rules
organised in a hierarchical structure, ﬁrst-order rules are
invented by learning embeddings to match facts and body
predicates of a meta-rule. To instantiate it, we speciﬁ-
cally design an expressive set of generic meta-rules, and
demonstrate they generate a consequent fragment of Horn
clauses. During training, we inject a controlled Gumbel
noise to avoid local optima and employ interpretability-
regularization term to further guide the convergence to in-
terpretable rules. We empirically validate our model on
various tasks (ILP, visual genome, reinforcement learn-
ing) against several state-of-the-art methods.

1 Introduction

Research on neuro-symbolic approaches [Santoro et al.,
2017, Donadello et al., 2017, Manhaeve et al., 2018, Dai
et al., 2019, d’Avila Garcez and Lamb, 2020] has become
very active recently and has been fueled by the successes
of deep learning [Krizhevsky et al., 2017] and the recog-
nition of its limitations [Marcus, 2018]. At a high level,
these approaches aim at combining the strengths of deep
learning (e.g., high expressivity and differentiable learn-
ing) and symbolic methods (e.g., interpretability and gen-
eralizability) while addressing their respective limitations
(e.g., brittleness for deep learning and scalability for sym-
bolic methods).

In this paper, we speciﬁcally focus on inductive logic
programming (ILP) tasks [Cropper and Dumanˇci´c, 2021].

†Shanghai Jiao Tong University
‡Huawei Noah’s Ark Lab

The goal in ILP is to ﬁnd a ﬁrst-order logic (FOL) pro-
gram that explains positive and negative examples given
some background knowledge also expressed in FOL. In
contrast to classic ILP methods, which are based on com-
binatorial search over the space of logic programs, neuro-
symbolic methods (see Section 2) usually work on a con-
tinuous relaxation of this process.

For tackling ILP problems, we propose a new neuro-
symbolic hierarchical model that is built upon a set of
meta-rules, denoted HRI . For a more compact yet ex-
pressive representation, we introduce the notion of proto-
rule, which encompasses multiple meta-rules. The valua-
tions of predicates are computed via a soft uniﬁcation be-
tween proto-rules and predicates using learnable embed-
dings. Like most ILP methods, our model can provide an
interpretable solution and, being independent of the num-
ber of objects, manifests some combinatorial generalisa-
tion skills1. In contrast to most other approaches, HRI is
also independent of the number of predicates —e.g. this
number may vary between training and testing. Our model
can also allow recursive deﬁnitions, if needed. Since it is
based on embeddings, it is able to generalize and is par-
ticularly suitable for multi-task ILP problems. Moreover,
any semantic or visual priors on concepts can be lever-
aged, to initialize the predicate embeddings.

The design of proto-rules, crucially restricting the hy-
pothesis space, embodies a well-known trade-off between
efﬁciency and expressivity. Relying on minimal sets of
proto-rules for rule induction models has been shown
to improve both learning time and predictive accuracies
[Cropper and Muggleton, 2014, Fonseca et al., 2004]. For
our model to be both adaptive and efﬁcient, we initially
designed an expressive and minimal set R0 of proto-rules.
While most neuro-symbolic approaches do not formally
discuss the expressivity of their models, we provide a the-

1Trained on smaller instances, it can generalize to larger ones.

1

 
 
 
 
 
 
oretical analysis to characterize the expressivity of R0.
Moreover, in contrast to ILP work based on combinato-
rial search [Cropper and Tourret, 2020], we found that a
certain redundancy in the proto-rules may experimentally
help learning in neuro-symbolic approaches. Therefore,
as a replacement of R0, we propose an extended set R∗
of proto-rules.

We validate our model using proto-rules in R∗ on
classic ILP tasks. Despite using the same set of proto-
rules, our model is competitive with other approaches
that may require specifying different meta-rules for dif-
ferent tasks to work, which, unsatisfactorily, requires an
a priori knowledge of the solution. In order to exploit the
embeddings of our model and demonstrate its scalability,
we distinctly evaluate our approach on a large domain,
GQA [Hudson and Manning, 2019b] extracted from Vi-
sual Genome [Krishna et al., 2017]). Our model outper-
forms other methods such as NLIL [Yang and Song, 2020]
on those tasks. We also empirically validate all our design
choices.

Contributions Our contributions can be summarized as
follows: (1) Hierarchical model for embedding-based rule
induction (see Section 4), (2) Expressive set of generic
proto-rules and theoretical analysis (see Section 4.2), (3)
interpretability-oriented training method (see Section 5),
(4) Empirical validation on various domains (see Sec-
tion 6).

2 Related Work

Classic ILP methods Classic symbolic ILP methods
[Quinlan, 1990, Muggleton, Cropper and Tourret, 2020]
aim to learn logic rules from examples by a direct search
in the space of logic programs. To scale to large knowl-
edge graphs, recent work [Gal´arraga et al., 2013, Omran
et al., 2018] learns predicate and entity embeddings to
guide and prune the search. These methods typically rely
on carefully hand-designed and task-speciﬁc templates in
order to narrow down the hypothesis space. Their draw-
back is their difﬁculties with noisy data.

Differentiable ILP methods By framing an ILP task
as a classiﬁcation problem, these approaches based on
a continuous relaxation of the logical reasoning process

can learn via gradient descent. The learnable weights may
be assigned to rules [Evans and Grefenstette, 2018]—
although combinatorially less attractive—or to the mem-
bership of predicates in a clause [Payani and Fekri, 2019,
Zimmer et al., 2021]. However, as ILP solvers, these mod-
els are hard to scale and have a constrained implemen-
tation, e.g., task-speciﬁc template-based variable assign-
ments or limited number of predicates and objects. In an-
other direction, NLM [Dong et al., 2019] learns rules as
shallow MLPs; by doing so, it crucially loses in inter-
pretability. In contrast to these methods, our model HRI
ascribes embeddings to predicates, and the membership
weights derive from a similarity score, which may be ben-
eﬁcial in rich and multi-task learning domains.

Multi-hop reasoning methods
In multi-hop reasoning
methods, developed around knowledge base (KB) com-
pletion tasks, predicate invention is understood as ﬁnding
a relational path [Yang et al., 2017] or a combination of
them [Yang and Song, 2020] on the underlying knowl-
edge graph; this path amounts to a chain-like ﬁrst-order
rule. However, although computationally efﬁcient, the re-
stricted expressiveness of these methods limit their per-
formances.

Embedding-based models
In the context of KB com-
pletion or rule mining notably, many previous studies
learn embeddings of both binary predicates and entities.
Entities are typically attached to low-dimensional vectors,
while relations are understood as bilinear or linear op-
erators applied on entities possibly involving some non-
linear operators

[Bordes et al., 2013, Lin et al., 2015, Socher et al.,
Nickel et al., 2011, Guu et al., 2015]. These methods are
either limited in their reasoning power or suffer from sim-
ilar problems as multi-hop reasoning.

Our work is closely related to that of Campero et al.
[2018] whose representation model is inspired by NTP
[roc, 2017], attaching vector embeddings to predicates
and rules. Their major drawback, like most classic or dif-
ferentiable ILP methods, is the need for a carefully hand-
designed template set for each ILP task. In contrast, we
extend their model to a hierarchical structure with an ex-
pressive set of meta-rules, which can be used for various
tasks. We further demonstrate our approach in the multi-

2

task setting.

3 Background

We ﬁrst deﬁne ﬁrst-order logic and inductive logic pro-
gramming (ILP). Then, we recall some approaches for
predicate invention, an essential aspect of ILP.

Notations Sets are denoted in calligraphic font (e.g., C).
Constants (resp. variables) are denoted in lowercase (resp.
uppercase). Predicates have the ﬁrst letter of their names
capitalized (e.g., P or Even), their corresponding atoms
are denoted sans serif (e.g., P), while predicate variables
are denoted in roman font (e.g., P). Integer hyperparame-
ters are denoted n with a subscript (e.g., nL).

3.1

Inductive Logic Programming

First-order logic (FOL) is a formal language that can
be deﬁned with a set of constants C, a set of predicates
P, a set of functions, and a set of variables. Constants
correspond to the objects of discourse, n-ary predicates
can be seen as mappings from Cn to the Boolean set
B = {True, False}, n-ary functions are mappings from
Cn to the set of constants C, and variables correspond to
unspeciﬁed constants. As customary in most ILP work,
we focus on a function-free fragment2 of FOL.

An atom is a predicate applied to a tuple of arguments,
either constants or variables; it is called a ground atom
if all its arguments are constants. Well-formed FOL for-
mulas are deﬁned recursively as combinations of atoms
with logic connectives (e.g., negation, conjunction, dis-
junction, implication) and existential or universal quanti-
ﬁers (e.g., ∃, ∀). Clauses are a special class of formulas,
which can be written as a disjunction of literals, which are
atoms or their negations. Horn clauses are clauses with at
most one positive literal, while deﬁnite Horn clauses con-
tain exactly one. In the context of ILP, deﬁnite clauses
play an important role since they can be re-written as
rules:

H ← B1 ∧ B2 ∧ · · · ∧ Bk

(1)

where H is called the head atom and Bi’s the body atoms.
The variables appearing in the head atom are instantiated
with a universal quantiﬁer, while the other variables in the
body atoms are existentially quantiﬁed3.

Inductive Logic Programming (ILP) aims at ﬁnding a
set of rules such that all the positive examples and none of
the negative ones of a target predicate are entailed by both
these rules and some background knowledge expressed
in FOL. The background knowledge may contain ground
atoms, called facts, but also some rules.

Forward chaining can be used to chain rules to-
gether to deduce a target predicate valuation from some
facts. Consider the Even-Succ task as an example. Given
background facts {Zero(0), Succ(0, 1), Succ(1, 2)} and
rules:

Even(X) ← Zero(X)

Even(X) ← Even(Y ) ∧ Aux(Y, X)

(2)

Aux(X, Y ) ← Succ(X, Z) ∧ Succ(Z, Y ),

we can easily deduce the facts {Aux(0, 2), Even(2)} by
unifying the body atoms of the rules with the facts; repeat-
ing this process, we can, iteratively, infer all even num-
bers.

The predicates appearing in the background facts are
called input predicates. Any other predicates, apart from
the target predicate, are called auxiliary predicates. A
predicate can be extensional, as deﬁned by a set of ground
atoms, or intensional, deﬁned by a set of clauses .

3.2 Predicate Invention and Meta-Rules

One important part of solving an ILP problem is predicate
invention, which consists in creating auxiliary predicates,
intensionally deﬁned, which would ultimately help deﬁne
the target predicate. Without language bias, the set of pos-
sible rules to consider grows exponentially in the number
of body atoms and in the maximum arity of the predicates
allowed. In previous work, this bias is often enforced us-
ing meta-rules, also called rule templates, which restrict
the hypothesis space by imposing syntactic constraints.
The hypothesis space is generated by the successive ap-
plications of the meta-rules on the predicate symbols from
the background knowledge.

2A fragment of a logical language is a subset of this language ob-

3This rule can be read as: if, for a certain grounding, all the body

tained by imposing syntactical restrictions on it.

atoms are true, then the head atom is also true.

3

A meta-rule corresponds to a second-order clause with

predicate variables. For instance, the chain meta-rule is:

leads to an underestimation issue. The valuation of atom
P with respect to meta-rule R is then computed as5:

H(X, Y ) ← B1(X, Z) ∧ B2(Z, Y ),

(3)

v(P, R) = max
P1,P2

v(P, R, P1, P2).

(5)

where H, B1, and B2 correspond to predicate variables.

LRI Model We recall the differentiable approach to
predicate invention proposed by Campero et al. [2018].
Their model, Logical Rule Induction (LRI), learns an
embedding for each predicate and each meta-rule atom;
then, predicates and atoms of meta-rules are matched
via a soft uniﬁcation technique. For any predicate P , let
θP ∈ Rd denotes its d-dimensional4 embedding. The
embeddings attached to a meta-rule like (3) with one
head (H) and two body (B1, B2) predicate variables are
(θH, θB1, θB2 ) ∈ Rd × Rd × Rd. The soft uniﬁcation is
based on cosine to measure the degree of similarity of em-
beddings of predicates and meta-rule atoms. For example,
αP H = cos(θP , θH) ∈ [0, 1] is the uniﬁcation score be-
tween predicate P and head predicate variable H: a higher
uniﬁcation score indicates a higher belief that P is the
correct predicate for H. Similarly, uniﬁcation scores are
computed for any pair of predicate and meta-rule atom.

In this model, a ground atom P = P (s, o) is repre-
sented as (θP , s, o, vP), where θP is the embedding of
predicate P , s and o are respectively the subject and ob-
ject constants, and vP ∈ [0, 1] is a valuation measuring
the belief that P is true. In ILP tasks, the valuations are
initialized to 1 for background facts and are otherwise set
to 0, reﬂecting a closed-world assumption.

For better clarity, let use meta-rule (3) as a running ex-
ample to present how one inference step is performed. For
an intensional predicate P , the valuation of a correspond-
ing ground atom P = P (x, y) with respect to a meta-rule
R of the form (3) and two ground atoms, P1 = P1(x, z)
and P2 = P2(z, y), is computed as follows:

v(P, R, P1, P2) = (αP H · αP1B1 · αP2B2) · (vP1 · vP2 ) .

(4)
The term in the ﬁrst brackets measures how well the pred-
icate tuple (P, P1, P2) matches the meta-rule, while the
term in the second brackets corresponds to a fuzzy AND.
Note that (4), deﬁned as a product of many terms in [0, 1],

Since P can be matched with the head of several meta-
rules, the new valuation of P is, after one inference step:

(cid:18)

v(P) = max

vold(P), max

R

v(P, R)

(cid:19)

,

(6)

where vold(P) is the previous valuation of P. The max
over meta-rules allows an intensional predicate to be de-
ﬁned as a disjunction. After nI steps of forward chaining,
the model uses binary cross-entropy as loss function to
measure the difference between inferred values and target
values.

4 Hierarchical Rule Induction

Let us introduce our model HRI and our generic meta-
rules set, alongside some theoretical results about its ex-
pressivity.

4.1 Proposed Model

Building on LRI [Campero et al., 2018], our model in-
cludes several innovations, backed both by theoretical and
experimental results: proto-rules, incremental prior, hier-
archical prior, and improved model inference.

The priors reﬂect the hierarchical and progressive struc-
ture arguably present in the formation of human con-
ceptual knowledge. HRI is illustrated in Figure 1. The
boxes, spread in different layers, denote auxiliary pred-
icates, which embody an asymmetrical disjunction of a
conjunction of two atoms with a third body predicate. The
arrows represent the soft uniﬁcation between one body
predicate and the heads of lower-level predicates, follow-
ing (12).

Proto-rules We ﬁrst deﬁne the notion of proto-rules,
which implicitly correspond to sets of meta-rules. A

5The max over ground atoms is taken both over possible existential

4In Campero et al. [2018], d equals the number of predicates.

variable (e.g., Z above), and over predicates P1, P2 ∈ P.

4

Incremental Prior To reinforce the incremental aspect
of predicate invention, in contrast to LRI, each auxiliary
predicate is directly associated with a unique proto-rule
deﬁning it. This has two advantages. First, it partially ad-
dresses the underestimation issue of (4) since it amounts
to setting αP H = 1. Second, it reduces computational
costs since the soft uniﬁcation for a rule can be com-
puted only by matching the body atoms7. In order to pre-
serve expressivity8, we can re-introduce disjunctions in
the model by considering proto-rules with disjunctions in
their bodies (see Section 4.2); e.g., (7) could be extended
to:

H(X, Y ) ← (cid:0)B1(X, Z) ∧ B2(Z, Y )(cid:1) ∨ B3(X, Y ). (9)

Hierarchical Prior A hierarchical architecture is en-
forced by organizing auxiliary predicates in successive
layers, from layer 1 to the max layer nL, as illustrated
Figure 1 . Layer 0 contains all input predicates. For each
layer l = 1, . . . , nL, each proto-rule9 in R generates one
auxiliary predicate.

Lastly, the target predicate is matched with the auxil-

iary predicates in layer nL only.

With this hierarchical architecture, an auxiliary predi-
cate P at layer (cid:96) can be only deﬁned from a set P ↓
(cid:96) of
predicates at a layer lower or equal to (cid:96). This set can be
deﬁned in several ways depending on whether recursiv-
ity is allowed. We consider three cases: no recursivity,
iso-recursivity, and full recursivity. For the no recursivity
(resp. full recursivity) case, P ↓
(cid:96) is deﬁned as the set P<(cid:96)
(resp. P≤(cid:96)) of predicates at layer strictly lower than (resp.
lower or equal to) (cid:96). For iso-recursivity, P ↓
(cid:96) is deﬁned as
P<(cid:96) ∪ {P }.

Enforcing this hierarchical prior has two beneﬁts. First,
it imposes a stronger language bias, which facilitates
learning. Second, it also reduces computational costs
since the soft uniﬁcation does not need to consider all
predicates.

7The max over meta-rules in (6) is not needed anymore.
8Since identifying intensional predicates to proto-rules prevent

them to have disjunctive deﬁnitions.

9Alternatively, several auxiliary predicates could be generated per
proto-rules. For simplicity, we only generate one and control the model
expressivity with only one hyperparameter nL.

Figure 1: Hierarchical Model

proto-rule is a meta-rule adaptive6 (see Appendix A for
a formal deﬁnition). For instance, (3) extended as a proto-
rule could be written as:

H(X, Y ) ← B1(X, Z) ∧ B2(Z, Y ),

(7)

where the Bi’s correspond to predicate variables, of arity
2 or 1, where the second argument is optional. For in-
stance, (7) can be instantiated as the following rule:

P (X, Y ) ← P1(X) ∧ P2(Z, Y )

(8)

where P , P2 are binary predicates, and P1 is a unary pred-
icate implicitly viewed as the binary predicate P1, where
∀z, P1(x, z) := P1(x).

We propose a minimal and expressive set of proto-rules
in Section 4.2. However, HRI can be instantiated with any
set R of proto-rules. The choice of R deﬁnes the language
bias used in the model.

6It amounts to embed some lower-arity predicates as higher-arity

predicates.

5

Improved Model Inference We improve LRI’s infer-
ence technique with a soft uniﬁcation computation that
reduces the underestimation issue in (4), which is due to
the product of many values in [0, 1]. For clarity’s sake, we
illustrate the inference with proto-rule (9); although the
equations can be straightforwardly adapted to any proto-
rule. One inference step in our model is formulated as fol-
lows10:

vand = POOLP1,P2 (αP1B1 · αP2B2 · AND[vP1, vP2 ])

vor = OR [vand, POOLP3 (αP3B3 · vP3)]

v

= MERGE (vold, vor) ,

(10)
where v (resp. vold) denotes the new (resp. old) valuation
of a grounded auxiliary predicate P. For an auxiliary pred-
icate at layer (cid:96), the POOL operation is performed over both
predicates P1, P2, P3 ∈ P ↓
(cid:96) and the groundings that are
compatible to P. However, before pooling, in presence of
an existential quantiﬁer as in proto-rule (9), a max is ap-
plied to the corresponding dimension of the tensor. The
valuation vt of the target predicate (with variable denoted
Pt) is computed as a MERGE of its previous valuation vt
and a POOL over atoms at layer nL:

old

vt = MERGE (cid:0)vt

old, POOLP1 (αP1Pt · vP1 )(cid:1) .

(11)

The underestimation issue is alleviated in two ways.
First, the POOL operation is implemented as a sum, AND
as min, OR as max, and MERGE as max. Second, the uni-
ﬁcation scores αPiBi’s based on cosine are renormalized
with a softmax transformation:

αPiBi =

(cid:80)

Pj

exp(cos(θPi, θBi)/τ )

exp(cos(θPj , θBi)/τ )

,

(12)

where Pj ∈ P ↓
(cid:96) and hyperparameter τ , called temper-
ature, controls the renormalization. Score αP1Pt can be
computed in a similar way with embedding θPt. These
choices have been further experimentally validated (see
Appendix B).

Equation (10) implies that the computational complex-
ity of one inference step at layer (cid:96) if there is no recursion
is O(nL ×|P(cid:96)|×|P ↓
(cid:96) |2 ×|C|2) where P(cid:96) is the set of pred-
icates at layer (cid:96) and P ↓
(cid:96) is the set of predicates available

10It generalizes (4)-(6), under the incremental prior.

6

(a) Meta-rule A (b) Meta-rule B (c) Meta-rule C

Figure 2: Three types of meta-rules: boxes represent vari-
ables, green arrows represent relations for head atoms and
blue arrows represent relations for body atoms

for deﬁning an auxiliary predicate at layer (cid:96). Its spatial
complexity at layer (cid:96) is O(nL × |P(cid:96)| × |C|2).

The inference step described above is iterated nI times,
updating the valuations of auxiliary and target predicates.
Note that when recursive deﬁnitions of predicates are al-
lowed, it could be conceivable to iterate this inference step
until convergence. A nice property of this inference proce-
dure is that it is parallelizable, which our implementation
on GPU takes advantage of.

4.2 Generic Set of Proto-Rules

Although HRI is generic, we design a small set of expres-
sive proto-rules to instantiate our model. We introduce
ﬁrst the following set of proto-rules R0 (see Figure 2):





A : H(X) ← B1(X, Y ) ∧ B2(Y, X)

B : H(X, Y ) ← B1(X, Z) ∧ B2(Z, Y )

C : H(X, Y ) ← B1(X, Y ) ∧ B2(Y, X)





where the Bi’s correspond to predicate variables where
the second argument is optional (see Section 4.1).

To support our design, we analyzed the expressivity of
R0, by investigating which fragment of ﬁrst-order logic
can be generated by R0 from a set of predicates P. Be-
cause of space constraints, we only present here our main
result, where we assume that P contains the zero-ary
predicate True and the equality symbol:

Theorem 4.1. The hypothesis space generated by R0
from P is exactly the set of function-free deﬁnite Horn
clause fragment F {1,2}
P,≤2 composed of clauses with at most
two body atoms involving unary and binary predicates in
P.

We refer the reader to Appendix A for all formal deﬁ-
nitions, proofs and further theoretical results. This results
implies that our model with R0 can potentially solve per-
fectly any ILP task whose target predicate is expressible
in F {1,2}

P,≤2 with a large enough max layer nL.

However, to potentially reduce the max layer nL and
facilitate the deﬁnition of recursive predicates, we extend
R0 to the set R∨
0 by including a disjunction with a third
atom in all the proto-rules:





A∗ : H(X) ← (cid:0)B1(X, Y ) ∧ B2(Y, X)(cid:1) ∨ B3(X, T )
B∗ : H(X, Y ) ← (cid:0)B1(X, Z) ∧ B2(Z, Y )(cid:1) ∨ B3(X, Y )
C∗ : H(X, Y ) ← (cid:0)B1(X, Y ) ∧ B2(Y, X)(cid:1) ∨ B3(X, Y )





Moreover, as we have observed a certain redundancy to be
beneﬁcial to the learning, we incorporate the permutation
rule I in our experiments:

R∗ = R∨

0 ∪ {I : H(X, Y ) ← F (Y, X)}

This small set of protorules R∗, which has the same ex-
pressivity as R0 characterized in Theorem 4.1, is used to
instantiate our model in all our experiments.

5 Proposed Training Method

Training We train our model with a ﬁxed number nT of
training iterations. For each iteration, the model is trained
using one training instance. During each iteration, we add
a geometrically-decaying Gaussian-distributed noise to
the embeddings for both predicates and rules to avoid lo-
cal optima, similarly to LRI [Campero et al., 2018]. Then
the improved inference procedure described in (10) is per-
formed with these noisy embeddings for a number nI of
steps to get the valuations for all predicates. After these
nI inference steps, the binary cross entropy (BCE) loss of
the ground-truth target predicate valuation and the learned
target predicate valuation is calculated:

−Gt

x,y log (cid:0)v(Pt

x,y)(cid:1) − (1−Gt

x,y) log (cid:0)1−v(Pt

x,y)(cid:1),

(cid:88)

x,y

x,y = P t(x, y). To encourage the uniﬁcation
of atom Pt
scores to be closer to 0 or 1, an extra regularisation term
is added to the BCE loss to update the embeddings:

+ λ

(cid:88)

P,Q

αP Q

(cid:0)1 − αP Q

(cid:1).

(14)

where hyperparameter λ controls the regularization
weight and the sum is over all predicates P that can be
matched with a predicate variable Q appearing in some
meta-rules.

In addition, to further help with convergence to an in-
terpretable solution and avoid local optima, we replace
during training the softmax transformation by a variant
of Gumbel-softmax [Jang et al., 2017, Maddison et al.,
2017]. Commonly, it amounts to injecting a Gumbel
noise Gj in the softmax, i.e., in (12), we replace each
cos(θPj , θBi) by cos(θPj , θBi)+Gj where Gumbel noise
Gj = −g log (− log (Uj)) is obtained by sampling Uj ac-
cording to a uniform distribution. However, we rely on a
variant of this noise: ˜Gj = −g log (− log (Uj)), which
is related as ˜Gj = log(− log g)
Gj. This choice has been
experimentally favored. We discuss it further in the Ap-
pendix. Gumbel scale g ∈ (0, +∞) is linearly decreased
during training. In contrast to the low-level parameter-
noise applied directly to the embeddings, the Gumbel
noise may be understood as a higher-level noise, enacted
on the similarity coefﬁcients themselves.

g

Convergence to an Interpretable Model The passage
from our soft model to a symbolic model may be real-
ized by taking the limit of the softmax temperature in the
uniﬁcation score to zero, or equivalently, switching to an
argmax in the uniﬁcation score; i.e., for each proto-rule,
the ﬁnal learned rules can be interpreted by assigning head
and body atoms to the predicates that obtain the highest
uniﬁcation score.

For instance, at a layer (cid:96), the extracted symbolic rule (8)
could be extracted from proto-rule (7) if P is the auxiliary
predicate associated to that proto-rule at layer (cid:96) and
(cid:40) P1 = arg maxP ∈P ↓
P2 = arg maxP ∈P ↓

αP B1
αP B2

(15)

(cid:96)

(cid:96)

(13)
where the sum is over all pairs of objects x, y in one train-
ing instance, Gt
x,y ∈ {0, 1} is the ground truth value

Then we can interpret the solution by successively un-
folding the logical formula extracted for each involved
predicate, starting from the target predicate.

7

Task

|I| Recursive

∂ILP LRI

Ours

train

soft evaluation

symbolic evaluation

Predecessor
Undirected Edge
Less than
Member
Connectedness
Son
Grandparent
Adjacent to Red
Two Children
Relatedness
Cyclic
Graph Coloring
Even-Succ2
Buzz
Fizz

1
1
1
1
1
2
2
2
2
2
2
2
2
2
3

No
No
Yes
Yes
Yes
No
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes

100
100
100
100
100
100
96.5
50.5
95
100
100
94.5
48.5
35
10

100
100
100
100
100
100
100
100
0
100
100
0
100
70
10

100
100
100
100
100
100
100
100
100
100
100
100
40
100
0

100
100
100
100
100
100
100
100
100
100
100
100
40
40
0

100
100
100
100
100
100
100
100
100
100
100
100
40
40
0

Table 1: Percentage of successful runs among 10 runs. |I| is the smallest number of intensional predicates needed.
Recursive means whether or not the solution needs to learn recursive rules.

6 Experimental Results

We evaluated our model on multiple domains and com-
pared with relevant baselines. In the main paper, for space
constraints, we only present two sets of experiments.
First, on standard ILP tasks, we show that our model
is competitive against other neuro-symbolic methods in
terms of performance, but also computational/space costs.
Second, on a large domain from Visual Genome [Krishna
et al., 2017], we show that our model can scale to a large
multi-task problem and is superior to NLIL [Yang and
Song, 2020].

Our results are averaged over several runs with dif-
ferent random seeds. In each run, the model is trained
over a ﬁxed number of iterations. Hyper-parameter de-
tails are given in Appendix D, and additional experiments
(e.g., choice of operators, limitations of LRI, reinforce-
ment learning) are discussed in Appendices B and C.

For simplicity and consistency, we instantiate our
model with R∗, with full recursivity in all the tasks ex-
cept Visual Genome (no recursivity).

Note the performances can be improved and/or learning
can be accelerated if we customize R∗ to a task or restrict

the recursivity. However, we intended to emphasize the
genericity of our approach.

6.1

ILP Tasks

We evaluated our model on the ILP tasks from [Evans and
Grefenstette, 2018], which are detailed in Appendix B,
where we also provide the interpretable solutions found
by our model.

We compare our model with ∂ILP [Evans and Grefen-
stette, 2018] and LRI [Campero et al., 2018] using their
reported results. In contrast to LRI or ∂ILP, which use a
speciﬁc set of meta-rules tailored for each task, we use
the same generic and more expressive template set R∗ to
tackle all tasks; naturally, this generic approach impedes
the learning for a few tasks.

A selection of the results on these ILP tasks are pre-
sented in Table 1 (see Appendix B). A run is counted as
successful if the mean square error between inferred and
given target valuations is less than 1e-4 for new evaluation
datasets. Column train corresponds to the performance
measured during training, while Column soft evaluation
resp. symbolic evaluation to evaluation performance (no

8

Task

Adjacent to Red

Grandparent

% successful runs

Training time (secs)

#Training
constants NLM DLM Ours NLM DLM Ours

Model

Visual Genome

R@1 R@5

7
10

9
20

100
90

90
100

90
90

100
100

100
100

100
100

163
334

402
1441

920
6629

2047
3331

62
71

79
89

MLP+RCNN 0.53
0.40
Freq
0.51
NLIL

Ours

0.53

0.81
0.44
0.52

0.60

Table 2: Comparisons with NLM/DLM in terms of percentage of suc-
cessful runs and average training times over 10 runs.

Table 3: R@1 and R@5 for 150 ob-
jects classiﬁcation on VG.

noise), using (11) resp. the interpretable solution (see end
of Section 5).

We also ran our model on two other ILP tasks used
by NLM [Dong et al., 2019] and DLM [Zimmer et al.,
2021] to compare their performances and training times
for a varying number of training constants. Those two ILP
tasks are similar to those used in [Evans and Grefenstette,
2018], but with different background predicates. The re-
sults reported in Table 2 show that our model is one to two
orders of magnitude faster and that it scales much better
in terms of training constants, while achieving at least as
good performances.

6.2 Visual Genome

Our model can be applied beyond classical ILP domains,
and even beneﬁt from richer environments and semantic
structure. Futhermore, initial predicates embeddings can
be boostrapped by visual or semantic priors. We illustrate
it by applying it to the larger dataset of Visual Genome
[Krishna et al., 2017], or more precisely a pre-processed
(less noisy) version known as the GQA [Hudson and Man-
ning, 2019a]. Similarly to Yang and Song [2020], we ﬁl-
tered out the predicates with less than 1500 occurrences,
which lead to a KB with 213 predicates; then, we solved
ILP-like task to learn the explanations for the top 150 ob-
jects in the dataset. More details on task, results, baseline
and metrics used are provided in Appendix C.

We ran a ﬁrst set of experiments to validate that em-
beddings priors may be leveraged and compared three ini-
tialization methods: random initialization, pretrained em-
beddings from NLIL [Yang and Song, 2020], and pre-
trained embeddings from NLP model GPT2 [Radford

et al., 2019]. We trained our model on a single ILP task,
which consists in predicting predicate Car. For this task,
we further ﬁltered the GQA dataset to keep 185 instances
containing cars. Table 4 presents the accuracy, precision,
and recall, obtained over 10 runs. Pretrained embed-
dings outperform the randomly initialized ones in all met-
rics. Embeddings from NLIL, trained speciﬁcally on this
dataset, expectedly yield the best results. Although, to be
fair, for the next experiments, we rely on the NLP embed-
dings.

In multitask setting, we trained 150 target models to-
gether with shared background predicate embeddings,
where each model corresponds to one object classiﬁca-
tion. The training consists of Nr rounds. In each round,
we trained each model sequentially, by sampling for each
target np instances containing positive examples, and nr
with or without positive examples.

We compared our model with NLIL [Yang and Song,
2020] and two other supervised baselines (classiﬁers)
mentioned in their paper: Freq, which predicts object class
by checking the related relation that contains the target
with the highest frequency; and MLP-RCNN, a MLP clas-
siﬁer trained with RCNN features extracted from object
images in Visual Genome dataset. The latter is a strong
baseline because it uses visual information for its predic-
tions while the other methods only use relational infor-
mation. Like NLIL, we use R@1 and R@5 to evaluate
the trained models. Table 3 summarizes the results. We
see that both our method and MLP+RCNN achieve best
performance for R@1. For R@5, we outperform Freq and
NLIL. Table 5 shows some extracted learned rules for the
multitask model.

9

Initialization

Accuracy

Precision

R@1

soft evaluation

symbolic evaluation

soft evaluation

symbolic evaluation

soft evaluation

symbolic evaluation

Random
NLIL
GPT2

0.63
0.75
0.65

0.49
0.6
0.45

0.57
0.87
0.72

0.5
0.75
0.66

0.23
0.46
0.27

0.38
0.58
0.5

Table 4: Performance of different embedding initializations for single Visual Genome task.

Target

wrist

person

vase

logo

Rules

wrist(X) ← (watch(Y ) ∧ on(Y, X)) ∨ F alse

person(X) ← (on(X, Y ) ∧ bench(Y )) ∨ wearing(X, T )

vase(X) ← (aux(X, Y ) ∧ f lowers(Y )) ∨ F alse

aux(X, Y ) ← (T rue ∧ on(Y, X)) ∨ with(X, Y )

logo(X) ← (aux0(X) ∧ T rue) ∨ aux1(X)
aux1(X) ← (on(X, Y ) ∧ laptop(Y )) ∨ F alse
aux0(X) ← (on(X, Y ) ∧ kite(Y )) ∨ F alse

Table 5: Examples of extracted rules from multi-task
model

7 Ethical Considerations

Regarding our last experiment choice, let us point out that
the use of data coming from annotated images, as well as
the use of pretrained embeddings produced by the NLP
model GPT2 should be regarded with circumspection. As
it as been well documented [??], such data holds strong
biases, which would raise ethical issues when deployed
in the real-world. Our motive was merely to illustrate the
performance of our model in more noisy domains typi-
cal of real-world scenarios (such as autonomous driving
scenarios). We strongly advise against the use of such un-
biased data for any real-world deployement.

8 Conclusion

We presented a new neuro-symbolic interpretable model
performing hierarchical rule induction through soft uni-
ﬁcation with learned embeddings;
is initialised by
a theoretically supported small-yet-expressive set of
proto-rules, which is sufﬁcient to tackle many classi-
cal ILP benchmark tasks, as it encompasses a conse-
quent function-free deﬁnite Horn clause fragment. Our

it

model has demonstrated its efﬁciency and performance in
both ILP, reinforcement learning (RL) and richer domains
against state-of-the-art baselines, where it is typically one
to two orders of magnitude faster to train.

As future work, we plan to apply it to broader RL do-
mains (e.g., inducing game rules from a game trace) but
also to continual learning scenarios where an interpretable
logic-oriented higher-level policy would be particularly
pertinent (e.g., autonomous driving). Indeed, we postu-
late that our model may be suited for continual learning
in semantically-richer domains, for multiple reasons de-
tailed in Appendix E, alongside current limitations of our
approach, which could be further investigated upon.

References

End-to-end differentiable proving.

In Advances in
Information Processing Systems 30: An-
Neural
nual Conference on Neural
Information Process-
ing Systems 2017, December 4-9, 2017, Long
Beach, CA, USA, pages 3788–3800, 2017. URL
https://proceedings.neurips.cc/paper/2017/hash/
b2ab001909a8a6f04b51920306046ce5-Abstract.html.

A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and
O. Yakhnenko. Translating embeddings for modeling
multi-relational data. In NeurIPS, 2013.

A. Campero, A. Pareja, T. Klinger, J. Tenenbaum, and
S. Riedel. Logical rule induction and theory learn-
arXiv preprint
ing using neural theorem proving.
arXiv:1809.02193, 2018.

A. Cropper and S. Dumanˇci´c. Inductive logic program-
ming at 30: a new introduction. Machine Learning,
43, 2021. ISSN 1573-0565. doi: 10.1007/s10994-021-

10

06089-1. URL https://doi.org/10.1007/s10994-021-
06089-1.

A. Cropper and S. Muggleton. Logical minimisation of
In In-
meta-rules within meta-interpretive learning.
ductive Logic Programming - 24th International Con-
ference, ILP 2014, volume 9046 of Lecture Notes in
Computer Science, pages 62–75. Springer, 2014. doi:
10.1007/978-3-319-23708-4 5.

A. Cropper and S. Tourret. Derivation reduction of
metarules in meta-interpretive learning. In ILP, pages
1–21, 01 2018.
ISBN 978-3-319-99959-3. doi: 10.
1007/978-3-319-99960-9 1.

A. Cropper and S. Tourret.

Logical reduction of
metarules. 109(7):1323–1369, 2020. doi: 10.1007/
s10994-019-05834-x.

W.-Z. Dai, Q. Xu, Y. Yu, and Z.-H. Zhou. Bridg-
reasoning by
ing machine learning and logical
In Advances in Neural Infor-
abductive learning.
mation Processing Systems 32, pages 2815–2826.
2019.
URL http://papers.nips.cc/paper/8548-
bridging-machine-learning-and-logical-reasoning-
by-abductive-learning.pdf.

A. d’Avila Garcez and L. C. Lamb. Neurosymbolic ai:

The 3rd wave, 2020.

for

I. Donadello, L. Seraﬁni, and A. D’Avila Garcez.
semantic image in-
In International Joint Conference
Intelligence,
1596–1602,
pages
doi: 10.24963/

Logic tensor networks
terpretation.
on
2017.
InternationalJointConferenceonArtiﬁcialIntelligence.
2017/221.

ISBN 978-0-9992411-0-3.

Artiﬁcial

H. Dong, J. Mao, T. Lin, C. Wang, L. Li, and D. Zhou.
Neural Logic Machines. arXiv:1904.11694 [cs, stat],
Apr. 2019. arXiv: 1904.11694.

R. Evans and E. Grefenstette. Learning explanatory rules

from noisy data. Journal of AI Research, 2018.

N. Fonseca, V. Costa, F. Silva, and R. Camacho. On avoid-
ing redundancy in inductive logic programming. vol-
ume 3194, pages 132–146, 09 2004. ISBN 978-3-540-
22941-4. doi: 10.1007/978-3-540-30109-7 13.

L. A. Gal´arraga, C. Teﬂioudi, K. Hose, and F. Suchanek.
AMIE: association rule mining under incomplete ev-
In Proceed-
idence in ontological knowledge bases.
ings of the 22nd international conference on World
Wide Web, pages 413–422, Rio de Janeiro, Brazil,
2013. ACM Press.
ISBN 978-1-4503-2035-1. doi:
10.1145/2488388.2488425.

K. Guu, J. Miller, and P. Liang. Traversing knowledge

graphs in vector space. In EMNLP, 2015.

D. A. Hudson and C. D. Manning. Gqa: A new dataset for
real-world visual reasoning and compositional question
answering. 2019 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 6693–
6702, 2019a.

D. A. Hudson and C. D. Manning. Gqa: A new dataset for
real-world visual reasoning and compositional question
answering. In CVPR, 2019b.

E. Jang, S. Gu, and B. Poole. Categorical reparameteriza-
tion with gumbel-softmax. In International Conference
on Learning Representations, 2017.

Z. Jiang and S. Luo. Neural Logic Reinforcement Learn-
ing. In International Conference on Machine Learning,
Apr. 2019.

R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata,
J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A.
Shamma, M. S. Bernstein, and L. Fei-Fei. Visual
genome: Connecting language and vision using crowd-
sourced dense image annotations. Int. J. Comput. Vi-
sion, 123(1):32–73, May 2017. ISSN 0920-5691. doi:
10.1007/s11263-016-0981-7.

A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
classiﬁcation with deep convolutional neural networks.
Commun. ACM, 60(6):84–90, May 2017. ISSN 0001-
0782. doi: 10.1145/3065386. URL https://doi.org/10.
1145/3065386.

Y. Lin, Z. Liu, M. Sun, Y. Liu, and X. Zhu. Learning entity
and relation embeddings for knowledge graph comple-
tion. In In Proceedings of AAAI’15, 2015.

C. J. Maddison, A. Mnih, and Y. W. Teh. The concrete dis-
tribution: A continuous relaxation of discrete random

11

F. Yang, Z. Yang, and W. W. Cohen. Differentiable learn-
ing of logical rules for knowledge base reasoning. In
Neural Information Processing Systems, 2017.

Y. Yang and L. Song. Learn to explain efﬁciently via neu-
In International Confer-

ral logic inductive learning.
ence on Learning Representations, 2020.

R. Zellers, M. Yatskar, S. Thomson, and Y. Choi. Neural
In
motifs: Scene graph parsing with global context.
CVPR, 2018. URL http://arxiv.org/abs/1711.06640.

M. Zimmer, X. Feng, C. Glanois, Z. Jiang, J. Zhang,
P. Weng, L. Dong, H. Jianye, and L. Wulong.
arXiv preprint
Differentiable logic machines.
arXiv:2102.11529, 2021.

variables. In ICLR, 2017. URL https://openreview.net/
forum?id=S1jE5L5gl.

R. Manhaeve, S. Dumanˇci´c, A. Kimmig, T. Demeester,
and L. De Raedt. Deepproblog: Neural probabilistic
logic programming. In Neural Information Processing
Systems, 2018.

G. Marcus. Deep learning: A critical appraisal. arXiv:

1801.00631, 2018.

S. Muggleton. Inverse entailment and progol. New Gen-

eration Computing, 13:245–286.

S. Muggleton. Inverse entailment and progol, 1995. URL

https://doi.org/10.1007/BF03037227.

M. Nickel, V. Tresp, and H.-P. Kriegel. A three-way
model for collective learning on multi-relational data.
In ICML, 2011.

P. Omran, K. Wang, and Z. Wang. Scalable rule learning
via learning representation. pages 2149–2155, 07 2018.
doi: 10.24963/ijcai.2018/297.

A. Payani and F. Fekri. Learning algorithms via neural

logic networks. arXiv:1904.01554, 2019.

J. R. Quinlan. Learning logical deﬁnitions from relations.

MACHINE LEARNING, 5:239–266, 1990.

A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and
I. Sutskever. Language models are unsupervised multi-
task learners. 2019.

J. A. Robinson. A machine-oriented logic based on the
resolution principle. J. ACM, 12(1):23–41, Jan. 1965.
ISSN 0004-5411. doi: 10.1145/321250.321253. URL
https://doi.org/10.1145/321250.321253.

A. Santoro, D. Raposo, D. G. T. Barrett, M. Malinowski,
R. Pascanu, P. Battaglia, and T. Lillicrap. A simple neu-
ral network module for relational reasoning. 2017.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov. Proximal Policy Optimization Algorithms.
arXiv preprint: 1707.06347, Aug. 2017.

R. Socher, D. Chen, C. D. Manning, and A. Y. Ng. Rea-
soning with neural tensor networks for knowledge base
completion.

12

Appendix

A Expressivity Analysis

Many methods within inductive logic programming (ILP) deﬁne meta-rules, i.e., second-order Horn clauses which
delineates the structure of learnable programs. The choice of these meta-rules, referred to as a language bias and
restricting the hypothesis space, results in a well-known trade-off between efﬁciency and expressivity. In this section,
we bring some theoretical justiﬁcation for our designed set of second-order Horn Clause, by characterizing its expres-
sivity. Relying on minimal sets of metarules for rule induction models has been shown to improve both learning time
and predictive accuracies [Cropper and Muggleton, 2014, Fonseca et al., 2004]. For a model to be both adaptive and
efﬁcient, it seems pertinent to aim for a minimal set of meta-rules generating a sufﬁciently expressive subset of Horn
clauses. The desired expressivity is ineluctably contingent to the tackled domain(s); a commonly-considered fragment
is Datalog D which has been proven expressive enough for many problems. Datalog is a syntactic subset of Prolog
which, by losing the Turing-completeness of Prolog/Horn, provides the undeniable advantage that its computations al-
ways terminate. Below, we focus our attention to similar fragments, in concordance with previous literature [Gal´arraga
et al., 2013, Cropper and Tourret, 2020].

To make the document self-contained, before stating our results, let us introduce both concepts and notations related

to First and Second Order Logic. Recall P denote the set of all considered predicates.

Horn Logic We focus on Horn clause logic, since it is a widely adopted11 Turing-complete12 subset of FOL.

A Horn clause is a clause with at most one positive literal. Horn clauses may be of the following types (where P,

Q, T, and U are atoms):

• goal clauses which have no positive literal; they are expressed as ¬P ∨ ¬Q ∨ ... ∨ ¬T or equivalently False ←

P ∧ Q ∧ ... ∧ T,

• deﬁnite clauses which have exactly one positive literal; they are expressed as ¬P∨¬Q∨...∨¬T∨U or equivalently

as a Horn rule U ← P ∧ Q ∧ ... ∧ T,

• facts which can be expressed as U ← True13 where U is grounded.

Meta-rules We follow the terminology from the Meta Interpretative Learning literature [Cropper and Tourret, 2020]:

Deﬁnition 8.1. A meta-rule is a second-order Horn clause of the form: A0 ← A1 ∧ ... ∧ Am where each Ai is a
literal of the form P (T1, ..., Tni) where P is a predicate symbol or a second-order variable that can be substituted
by a predicate symbol, and each Ti is either a constant symbol or a ﬁrst-order variable that can be substituted by a
constant symbol.

Here are some examples of intuitive meta-rules, which will be of later use:

P(A, B) ← Q(A)

(σ) P(A, B) ← Q(B, A)
(ι)
(∃) P(A)
(∀) P(A)
(∇) P(A)
(∆) P(A, A) ← Q(A)

← ∃B, Q(A, B)
← ∀B, Q(A, B) universal contraction
← Q(A, A)

diagonal extract
diagonal f ill

permute
expand
existential contraction

(16)

11Pure Prolog programs are composed by deﬁnite clauses and any query in Prolog is a goal clause.
12Horn clause logic and universal Turing machines are equivalent in terms of computational power.
13Facts can be seen as a sub-case of deﬁnite clause assuming True ∈ P.

13

A common meta-rule set, which has been used and tested in the literature [Cropper and Tourret, 2020], is the following:

RM IL =






(Indent1)
(DIndent1)
(Indent2)
(DIndent2)
(Precon)
(Postcon)
(Curry)
(Chain)

← Q(A)
← Q(A) ∧ R(A)

P(A)
P(A)
P(A, B) ← Q(A, B)
P(A, B) ← Q(A, B) ∧ R(A, B)
P(A, B) ← Q(A) ∧ R(A, B)
P(A, B) ← Q(A, B) ∧ R(B)
P(A, B) ← Q(A, B, R)
P(A, B) ← Q(A, C) ∧ R(C, B)






(17)

where the letters P, Q, R correspond to existentially-quantiﬁed second-order variables and the letters A, B, C to
universally-quantiﬁed ﬁrst-order variables.

Proto-rules Our model relies on an extended notion of meta-rules, which we refer to as proto-rules. These templates
have the speciﬁcity that they do not ﬁx the arity of their body predicates; only the arity of the head predicate is
determined. Formally, they can be deﬁned as follows. For any n ∈ N, let P n (resp. P ≤n) be the predicates in P that
have arity equal to (resp. lower or equal to) n.

Deﬁne the projection operator νn which canonically embeds predicates of arity i ≤ n in the space of predicates of

arity n:

νn : P ≤n −→ P n

(18)
Note that the restriction of νn on the space of predicates of arity n is identity: νn |P n = idP n . Moreover, this projection
naturally extends to second-order variables. To ease the notations, we denote below the projection νni with an overline
(e.g., for a predicate P , νni(P ) = P ), since there is no risk of confusion because ni is speciﬁed by its arguments
(T1, . . . , Tni ).

νn(P )(X1, · · · , Xn) = P (X1, · · · , Xi) for P ∈ P i.

s.t.

Deﬁnition 8.2. A proto-rule is an extension of a second-order Horn clause of the form: A0 ← A1 ∧ . . . ∧ Am, where
A0 (resp. each Ai, i > 0 is a literal of the form P (T1, ..., Tni) (resp. νni (cid:0)P (cid:1)(T1, ..., Tni) in which P , which is a
predicate symbol or a second-order variable that can be substituted by a predicate symbol, is of arity lower or equal
to ni, and each Ti is either a constant symbol or a ﬁrst-order variable that can be substituted by a constant symbol.

Seeing second-order rules as functions over predicate spaces to ﬁrst-order logic rules, we can provide another
characterization of proto-rules: a meta-rule can be understood as a mapping P n1 × . . . × P nm → H for some indices
ni, i.e., taking m predicates of speciﬁc arities and returning a ﬁrst-order Horn clause; in contrast, a proto-rule is a
mapping P ≤n1 × . . . × P ≤nm → H, for some indices ni.
The ﬁrst set of protorules we propose is the following:

R0 :=






A : P (A)
← Q(A, B) ∧ R(B, A)
B : P (A, B) ← Q(A, C) ∧ R(C, B)
C : P (A, B) ← Q(A, B) ∧ R(B, A)






(19)

Horn Fragments A Horn theory T is a set of Horn clauses.

Let us denote the set of second order Horn clause, the set of deﬁnite Horn clauses, and the set of function-free
deﬁnite Horn clauses respectively by H, S, and F. Within Horn clause logic, several restrictions have been proposed
in the literature to narrow the hypothesis space, under the name of fragment. A fragment is a syntactically restricted
subset of a theory. Following previous work (such as [Cropper and Tourret, 2020]), we introduce below a list of classic
fragments of F, in decreasing order:

14

• connected fragment, C: connected deﬁnite function-free clauses, i.e., clauses in F whose literals can not be
partitioned into two sets such that the variables attached to one set are disjoint from the variables attached to
literals in the other set; e.g., P (A, B) ← Q(A, C) ∧ R(A, B) is connected while the following clause is not
P (A, B) ← Q(A, C) ∧ R(B).

• Datalog14 fragment, D: sub-fragment of C, composed of clauses such that every variable present in the head of

the Horn rule also appears in its body. 15

• two-connected fragment, K: sub-fragment of D composed of clauses such that the variables appearing in the

body must appear at least twice.

• exactly-two-connected fragment, E: sub-fragment of K such that the variables appearing in the body must appear

exactly twice.

• duplicate-free fragment, U: sub-fragment of K excluding clauses in which a literal contains multiple occurrences

of the same variable, e.g., excluding P (A, A) ← Q(A).

These fragments are related as follows:

H ⊃ S ⊃ F ⊃ C ⊃ D ⊃ K ⊃ E
⊃ U

(20)

For M a set of meta-rules, we denote M≤a

≤m the fragment of M where each literal has arity at most a and each
meta-rule has at most m body literals. We extend the notation in order to allow both a and m to be a set of integers;
e.g., M{1,2}
is the fragment of M where each literal corresponds to an unary or binary predicate and the body of each
{3}
meta-rule contains exactly three literals.

Binary Resolution As it would be of future use, let us introduce some notions around resolution. To gain intuition
about the notion of resolvent16, let us look at an example before stating more formal deﬁnitions. Consider the following
Horn clauses:

C1 : P (X, Y ) ∨ ¬Q(X) ∨ ¬R(X, Y )
C2 : R(X, a) ∨ ¬T (X, a) ∨ ¬S(a)

or, equivalently, P (X, Y ) ← Q(X) ∧ R(X, Y )
or, equivalently, R(X, a) ← S(a) ∧ T (X, a)

The atoms R(X, Y ) and R(X, a) are uniﬁable, under the substitution σ : {a/Y }; under this substitution, we obtain
the following clauses:

{P (X, a) ∨ ¬Q(X) ∨ ¬R(X, a), R(X, a) ∨ ¬T (X, a) ∨ ¬S(a)}

Since either R(X, a) is True, or ¬R(X, a) is True, assuming C1 and C2 are True, we can deduce the following clause
is True:

(P (X, a) ∨ ¬Q(X)) ∨ (¬T (X, a) ∨ ¬S(a)) or, equivalently, P (X, a) ← Q(X) ∧ T (X, a) ∧ S(a)

14Some wider Datalog fragment have been proposed, notably allowing negation in the body, under certain stratiﬁcation constraints; here we

consider only its intersection with function-free deﬁnite Horn clauses.

15Since below each clause is assume to be a deﬁnite Horn clause, it can be equivalently represented as a Horn rule, with one head and several

non-negated literals in the body; we below alternately switch between these representations.

16Another way to gain intuition, is to think about propositional logic; there, a resolvent of two parent clauses containing complementary literals,
such as P and ¬P , is simply obtained by taking the disjunction of these clauses after removing these complementary literals. In the case of FOL or
HOL, such resolvent may involve substitutions.

15

This resulting clause, is a binary resolvent of C1 and C2.

Now, let us state more formal deﬁnitions. We denote σ = {t1/X1, · · · , tn/Xn}, a substitution and Eσ the ex-
pression obtained from an expression E by simultaneously replacing all occurrences of the variables Xi by the terms
ti.

Deﬁnition 8.3.

• A substitution σ is called a uniﬁer of a given set of expressions {E1, · · · , Em} , m ≥ 2 if

E1σ = · · · = Emσ.

• A uniﬁer θ of a uniﬁable set of expressions E = {E1, · · · , Em} , m ≥ 2, is said to be a most general uniﬁer if

for each uniﬁer σ of E there exists a substitution λ such that σ = θλ.

Deﬁnition 8.4. Consider two parent clauses C1 and C2 containing the literals l1 and l2 respectively. If l1 and ¬l2
have a most general uniﬁer σ, then the clause (C1σ \ l2σ) ∨ (C2σ \ l2σ) is called a binary resolvent of C1 and C2.

Derivation Reduction Diverse reductions methods have been proposed in the literature, such as subsumption
[Robinson, 1965], entailment [Muggleton], and derivation [Cropper and Tourret, 2018]. As previously pointed out
(e.g., [Cropper and Tourret, 2020]), common entailment methods may be too strong and remove useful clauses en-
abling to make predicates more speciﬁc. The relevant notion to ensure the reduction would not affect the span of our
hypothesis space is the one of derivation-reduction (D-reduction) [Cropper and Tourret, 2018], as deﬁned in Deﬁni-
tion 8.6.

Let us ﬁrst deﬁne for the following operator for a Horn theory T :

(cid:26) R0(T ) = T

Rn(T ) = {C | C is the binary resolvent of C1 and C2 with C1 ∈ Rn−1(T ), C2 ∈ T }

Deﬁnition 8.5. The Horn closure of a Horn theory T is: R∗(T ) = ∪nRn(T ).
Deﬁnition 8.6.

(i) A Horn clause C is derivationally redundant in the Horn theory T if T (cid:96) C, i.e., C ∈ R∗(T ); it

is said to be k-derivable from T if C ∈ Rk(T ).

(ii) A Horn theory T is derivationally reduced (D-reduced) if and only if it does not contain any derivationally

redundant clauses.

A clausal theory may have several D-reductions. For instance, we can easily see that






C1 : P (A, B) ← Q(B, A)
C2 : P (A, B) ← Q(A, C), R(C, B)
C3 : P (A, B) ← Q(A, C), R(B, C)






may be D-reduced to either {C1, C2} or {C1, C3}

Hypothesis Space Recall that P (resp. P0) denotes the set of predicates (resp. of initial predicates); and B the
background knowledge, may encompass both initial predicates and pre-given rules. As we are interested in character-
izing the expressivity of our model, let us deﬁne the relevant notion of hypothesis space17, which could apply to any
rule-induction model attached to at meta-rule or proto-rule set:

Deﬁnition 8.7. Given a predicate set P, and a meta-rule or proto-rule set M, let us deﬁne:

(i) the set MP as the set of all Horn clauses generated by all the possible substitutions of predicates variables in

M by predicates symbols from P.

(ii) the hypothesis space M[P] generated by M from P as the Horn closure of MP .

17In this paper, we adopted the denomination of hypothesis space, which is unconventional in logic, because it refers to the space accessible to
our learning algorithm. Similarly, by incorporating initial rules, we could also deﬁne the Horn theory generated by a set of meta-rules or proto-rules
given a certain background knowledge B and a set of predicates P.

16

Equality Assumption The Equality relation is typically assumed part of FOL, so it is reasonable to assume it
belongs to the background knowledge in our results below. In our approach, it amounts to integrating the predicate
Equal to the set of initial predicates P0; Equal, which corresponds to the identity, is intensionally deﬁned on any
domain D by:

Equal :






D × D → B = {True, False}
(cid:26) True
False

if X = Y
if X (cid:54)= Y

(X, Y )

(cid:55)→

(21)

Main Result Here is our main result, concerning the investigation of the expressivity of the minimal set R0:

Theorem 8.8.

(i) Assuming True ∈ P0, the hypothesis space generated by R0 from P encompasses the set of
duplicate-free and function-free deﬁnite Horn clause composed of clauses with at most two body atoms involving
unary and binary predicates in P:

True ∈ P0 =⇒ R0[P] ⊃ U {1,2}

≤2

[P]

(ii) Assuming True, Equal ∈ P0, the hypothesis space generated by R0 from P corresponds to the set of function-
free deﬁnite Horn clause composed of clauses with at most two body atoms involving unary and binary predicates
in P; i.e., in terms of the second order logic fragment:

True, Equal ∈ P0 =⇒ R0[P] = F {1,2}

≤2

[P]

We refer to the end of this section for the proof of this theorem.
Theorem 8.8 allows us to conclude that R0 is more expressive than the set commonly found in the literature RM IL,

assuming we are working with predicates of arity at most 2:

Corollary 8.9. Assuming the initial predicates P0 contains only predicates of arity at most 2, the hypothesis space
generated by R0 given P encompasses the one generated by RM IL as deﬁned in (17).

∀P ∈ P0,

arity(P ) ≤ 2 =⇒ R0[P] ⊃ RM IL[P]

Proof. Under our assumption, the meta-rule (Curry) in (17) may be disregarded. The remaining meta-rules present
in RM IL have already been examined in Theorem 8.8. R0 has therefore at least the same expressivity than RM IL.
In order to be able to conclude R0 is strictly more expressive, we can mention the rules P (A) ← P (A, B) or
P (A, B) ← P (B, A), which are reached by R0 not RM IL.

Disjunction In our model, to ensure an incremental learning of the target rule, we impose the restriction that each
auxiliary predicate corresponds to one rule. Since such constraint risk to remove the possibility of disjunction, we
redeﬁne new proto-rules versions integrating disjunctions:

R∨

0 =






← (cid:0)Q(A, B) ∧ R(B, A)(cid:1) ∨ S(A, B)
A : P (A)
B : P (A, B) ← (cid:0)Q(A, C) ∧ R(C, B)(cid:1) ∨ S(A, B)
C : P (A, B) ← (cid:0)Q(A, B) ∧ R(B, A)(cid:1) ∨ S(A, B)






(22)

Adopting the minimal set R∨

0 , in our incremental setting does not affect the expressivity of the hypothesis space
compared to when relying on the set R0 in a non incremental setting. Let us remind an incremental setting imposes a
predicate symbol to be the head of exactly one rule.

17

Lemma 8.10. The minimal proto-rule set R∨

0 in an incremental setting holds the same expressivity as R0.

Proof. It is straightforward to see that R∨
0 is a minimal set.
The hypothesis space generated by R0 obviously encompasses the hypothesis space generated by R∨
0 in an incremen-
tal setting. Reciprocally, let us assume n rules instantiated from R0 are attached to one predicate symbol P . They can
be expressed as:

P (X, Y ) ← f1(X, Y )
P (X, Y ) ← f2(X, Y )
· · ·
P (X, Y ) ← fn(X, Y )

· · ·

· · ·

Recursively, we can deﬁne Pi auxiliary predicates in the hypothesis space of R∨
0 .

P1(X, Y ) ←
P2(X, Y ) ← P1(X, Y ) ∨ f2(X, Y )

f1(X, Y )

· · ·

· · ·

· · ·

Pn(X, Y ) ← Pn−1(X, Y ) ∨ fn(X, Y )

Since Pn is then equivalent to P , we can conclude.

Redundancy Since we have observed a certain redundancy to be beneﬁcial to the learning, we incorporate the
permutation rule I in our experiments, and rely on the following set:

R∗ = R∨

0 ∪ {I : H(X, Y ) ← F (Y, X)}

This small—yet non minimal—set of protorules R∗, has the same expressivity as R0 (characterized in Theorem 8.8)

Lemma 8.11. The proto-rule set R∗ holds the same expressivity than R0.

Proof. It stems from the fact, observed in the proof of Theorem 8.8 that the rule (σ), equivalent to (I) can be D-
reduced from R0.

Recursivity The development of Recursive Theory arose ﬁrst around the Hilbert’s Program and G¨odel’s proof
of the Incompleteness Theorems (1931) and is tightly linked to questions of computability. While implementing
recursive-friendly model, we should keep in mind considerations of computability and decidability. Enabling full
recursivity within the templates is likely to substantially affect both the learning and the convergence, by letting room
to numerous inconsistencies.

With this in mind, let us consider different stratiﬁcations in the rule space, and in our model. First, we introduce
a hierarchical ﬁltration of the rule space: initial predicates are set of layer 0, and auxiliary predicates in layer (cid:96) are
deﬁned from predicates from layers at most (cid:96). Within such stratiﬁed space, different restrictions may be considered:

• recursive-free hierarchical hypothesis space:, where rules of layer (cid:96) are deﬁned from rules of strictly lower layer.

The proto-rules from R∨

0 are therefore restricted to the following form:

with P ∈ P(cid:96), Q, R, O ∈ P<(cid:96).

P (·) ← [Q(·) ∧ R(·)] ∨ O(·),

18

• iso-recursive hierarchical hypothesis space, where the only recursive rules allowed from R∨

0 have the form:

P (·) ← [Q(·) ∧ R(·)] ∨ O(·), with P ∈ P(cid:96), Q, R ∈ P<(cid:96) ∪ {P }, O ∈ P<(cid:96)

Such space ﬁts most recursive ILP tasks (e.g., Fizz, Buzz, Even, LessThan).

• recursive hierarchical hypothesis space, where the recursive rules allowed from R∨

0 have the form:

P (·) ← [Q(·) ∧ R(·)] ∨ O(·) with P ∈ P(cid:96), Q, R ∈ P≤(cid:96), O ∈ P≤(cid:96).

Such space is required for the recursivity present in EvenOdd or Length. Such hierarchical notion is also present
in the Logic Programming literature under the name of stratiﬁed programs.

Proof of Theorem 8.8 Before we present the proof, let us introduce two additional notations:

• R i(cid:12)R(cid:48) refers to the resolvent of R with R(cid:48), where the ith body member of R is resolved with the head of R(cid:48).

To illustrate it, consider the following meta-rules:

R :
P (A) ← Q(A) ∧ R(A, B)
R(cid:48) : P (A) ← Q(A) ∧ R(B, A)
R(cid:48)(cid:48) :

P (A, B) ← Q(B, A)

The resolvent R(cid:48)

2(cid:12)R(cid:48)(cid:48) corresponds to resolve the second body atom of R(cid:48) with R(cid:48)(cid:48), which therefore leads to R.

• We deﬁne the operation ρ as the composition of a projection ν, a permutation (σ), and an existential contraction

over the second variable (∃), which amounts to:

ρ(R)(X) := ∃ ◦ σ(R(X, Y )) = ∃Y R(Y )

(23)

Here, we identiﬁed the permutation clause (σ) (as deﬁned in (16)) with the operation it deﬁnes on predicates σ;
similarly for the existential clause (∃) with the contraction operation ∃.

Likewise, we can deﬁne the corresponding non-connected meta-rule attached to the operation ρ by:

(ρ) : P (X) ← ∃Y R(Y )

(24)

Proof. The inclusion R0[P] ⊂ F {1,2}
[P] is trivial since all the rules instantiated from R0 are deﬁnite Horn clause
with two body predicates, and duplicate-free; similarly, if adding the duplicate-free constraint to F (and denoting it
by F D), we can trivially state: R0[P] ⊂ F D,{1,2}

[P].

≤2

≤2

It remains to prove the other inclusion; e.g., for (ii), it amounts to R0[P] ⊃ F {1,2}

[P], which boils down to
R0[P] ⊃ F {1,2}
P,≤2 since R0[P] is closed under resolution. We will proceed by successively extending the sub-fragments
M we are considering while proving R0[P] ⊃ MP . For instance, under the assumption True, Equal ∈ P0, we
demonstrate that the hypothesis space generated by R0 encompasses the one generated by the increasingly larger
fragments: (a) U (b) K; (c) D; (d) C; (e) F. For (i), under the simpler assumption True ∈ P0, we can follow almost
identical steps, although the duplicate-free constraint can not be lifted, and therefore step (b) is skipped. Since the
proof of (i) is more or less included in the proof of (ii), we will below focus only on proving (ii).

≤2

For each fragment M, to demonstrate such inclusion, we can simply show that every rule within the fragment MP
belongs to the hypothesis space generated by R0; more speciﬁcally, we will mostly work at the level of second order
logic; there, we will show that any meta-rule in M can be reduced from meta-rules in R0, or generated by R0.

19

However, to avoid listing all the meta-rules generating these fragments, and restrict our enumeration, we leverage
several observations: ﬁrst, since we assume the predicate set includes True, we can restrict our attention to clauses
with exactly two body literals; secondly, the permutation meta-rule (σ) (introduced in 16) can be derived from the
proto-rule C upon matching the ﬁrst body in C with True; therefore, we can examine rules only upon permutation (σ)
applied to their body or head predicates.

1. First, let us demonstrate the following inclusion:

Claim1 :

True ∈ P0 =⇒ R0[P] ⊃ U {1,2}
P,≤2

(25)

By the previous observations, we can narrow down to examine a subset of the meta-rules generating U {1,2}
, as
listed and justiﬁed below. Our claim is that all these clauses are derivationally redundant from the Horn theory
generated by R0; for each of them, we therefore explicit the clauses used to reduce them18.

≤2

Meta-Rules
P (A) ← Q(A) ∧ R(A)
(i)
(ii)
P (A) ← Q(A) ∧ R(A, B)
(iii) P (A) ← Q(A, B) ∧ R(B)
P (A) ← Q(A, B) ∧ R(B, A)
(iv)
P (A) ← Q(A, B) ∧ R(B, C)
(v)
(vi)
P (A, B) ← Q(A) ∧ R(A, B)
(vii) P (A, B) ← Q(A, C) ∧ R(C, B) B
(viii) P (A, B) ← Q(A, B) ∧ R(B, A) C
(ix)

A(Q, σ(R))
A(Q, σ(R))

Reduction Comment
A 2(cid:12)σ
A 2(cid:12)σ
A
A
A
A 2(cid:12)σ

C(Q, σ(R))

P (A, B) ← Q(A, B) ∧ R(B, C) A 2(cid:12)∃

A(Q, ∃(R))

Beside the notation of the resolvent (cid:12)H explained previously, we provided a more intuitive form for the resolution
(under ’Comment’), in terms of composition between maps, where we identify a proto-rule (or meta-rule) with a
map of the following form:

P • × · · · × P • → P •

Let us justify why these clauses are the only clauses in U {1,2}

≤2 worth to examine in two steps:

• Let us consider clauses whose head predicate arity is 1. First, by the Datalog assumption, we can indeed
restrict our attention to body clauses where A is appearing at least once. By symmetry, we can assume A
appears in the ﬁrst body. By the connected assumption, either A appears in the second body too, and/or an
existential variable (say B) appears both in the ﬁrst or second body. Upon permutation, it leads us to clauses
(i) − (v).

• Let us consider clauses whose head predicate arity is 2. First, by the Datalog assumption, we can indeed
restrict our attention to body clauses where A and B have to appear at least once. The only body clause of
arity (1, 1) satisfying this condition is not connected (Q(A) ∧ R(B)); upon permutation of A and B, and of
the body predicates Q and R, and upon inversion (σ) of the body predicates, the only body clause of arity
(1, 2) is of the form Q(A) ∧ R(A, B) (denoted (vi) below, which reduces to C upon permutation); for arity
(2, 2), upon similar permutations, we can list three types of body clause, listed below as (vii), (viii), (ix).
While (vii), (viii) are directly pointing to B, C, (ix), can be obtained by reducing the second body of C
through (∃) (as deﬁned in 16). This justiﬁes the remaining clauses, (vi) − (ix).

18Note that we are allowed to use clauses ∃ and σ in the reduction as we already explained why we can derive them from R0 in the beginning

of the proof of Theorem 8.8.

20

We can therefore conclude by Claim1.

2. Let us extend (25) by enabling duplicates in the clause:

Claim2 : True, Equal ∈ P0 =⇒ R0[P] ⊃ K{1,2}
P,≤2

(26)

To extend the result from U {1,2}
, it is sufﬁcient to show the meta-rules (∇), (∆) from (16) can be
derived from R0, under the extra-assumption that Equal is included as background knowledge. This stems from
the fact that these meta-rules can be written as follows:

to K{1,2}
≤2

≤2

Meta-rule

Equivalent Form

(∇) P (A, A) ← Q(A) P (A, B) ← Q(A) ∧ Equal(A, B)
(∆) P (A) ← Q(A, A) P (A) ← Q(A, B) ∧ Equal(A, B)

Since the equivalent forms above are duplicate-free and two-connected, by (25), (∇), (∆) can be derived from
R0, which implies Claim2.

3. Removing the assumption of two-connectedness from (26), we claim the following inclusion holds:

Claim3 :

True, Equal ∈ P0 =⇒ R0[P] ⊃ D{1,2}
P,≤2

(27)

The additional meta-rules we have to reduce can be narrowed down to:

P (A) ← Q(A, A) ∧ R(A, C)

; P (A, B) ← Q(A, A) ∧ R(A, B)

; P (A, A) ← Q(A, A) ∧ R(A, B)

By using the reduction for duplicated forms P (A, A), Q(A, A) stated previously in (b), these meta-rules may be
derived from R0, and Claim3 follows.

4. In the next step, we extend (27) to the connected Horn fragment;

Claim4 :

True, Equal ∈ P0 =⇒ R0[P] ⊃ C{1,2}
P,≤2

(28)

To get rid of the Datalog constraint that each head variable appears in the body, we are brought to examine the
following meta-rules:

P (A, B) ← Q(A, C)
This meta-rule may be seen as a subcase of B once we have matched its second body with True. It ensues that
R0 is able to generate the fragment C{1,2}

, as stated in Claim4.

2

5. Lastly, we can get rid of the assumption of ”connected”.

Upon symmetries and permutations, we are brought to examine the following non-connected meta-rules:

Claim5 :

True, Equal ∈ P0 =⇒ R0[P] ⊃ F {1,2}
P,≤2

(29)

Comment

Meta-rule
P (A) ← Q(A) ∧ R(B)

(i)
(ii) P (A) ← Q(A, B) ∧ R(C)
(iii) P (A) ← Q(A, B) ∧ R(C, D)
(iv) P (A, B) ← Q(A, B) ∧ R(C)
(v)
P (A, B) ← Q(A) ∧ R(B, C)
(vi) P (A, B) ← Q(A, B) ∧ R(C, D) C 2(cid:12)(ρ ◦ ∃)
(vii) P (A, B) ← Q(A, C) ∧ R(B, D)

Reduction
A
A 1(cid:12)∃
(A 1(cid:12)∃) 2(cid:12)∃ A(∃(Q) ∧ ∃(R))
C 2(cid:12)ρ
B 2(cid:12)σ

C(Q, ρ(R))
B(Q, σ(R))
C(Q, ρ ◦ ∃(R)))
(C 1(cid:12)∃) 2(cid:12)σ C(∃(Q) ∧ σ(R)))

A(∃(Q), R)

21

Note that the clause/operation ρ, deﬁned in (23,24), enables to express the clause R(C) (resp. R(C, D)) appearing
in the body of (iv) (resp. (vi)) as (ρ)(R)(B). Since, by deﬁnition of (ρ), it can be derived from R0, we can
thereupon conclude that all the above meta-rules are reducible to R0, which concludes the proof.

We have therefore proven the following inclusions:

True, Equal ∈ P0 =⇒ R0[P] = F {1,2}

P,≤2 ⊃ C{1,2}

P,≤2 ⊃ D{1,2}

P,≤2 ⊃ KP{1,2}

2

⊃ U {1,2}
P,≤2

B ILP Experiment Results

Extracted Interpretable Solutions

We give a detailed description of the ILP tasks in our experiments, including the target, background knowledge,
positive/negative examples, and the learned solution by our method for each task. Note that the solution for Fizz is
missing since our current approach does not solve it with the generic proto-rules in R∗.
Predecessor
knowledge is the set of facts deﬁning predicate zero and the successor relation succ

The goal of this task is to learn the predecessor(X, Y ) relation from examples. The background

B = {True, False, zero(0), succ(0, 1), succ(1, 2), . . . }.

The set of positive examples is

P = {target(1, 0), target(2, 1), target(3, 2), . . . }

and the set of negative examples is

N = {target(X, Y )|(X, Y ) ∈ {0, 1, . . . }2} − P.

Among these examples, target is the name of the target predicate to be learned. For example, in this task, target =
predecessor. We use ﬁxed training data for this task given the range of integers. One solution found by our method is:

target(X, Y ) ← succ(X, Y ).

Undirected Edge A graph is represented by a set of edge(X,Y) atoms which deﬁne the existence of an edge from
node X to Y . The goal of this task is to learn the undirected-edge(X, Y ) relation from examples. This relation
deﬁnes the existence of an edge between nodes X and Y regardless of the direction of the edge. An example set of
background knowledge is

B = {T rue, F alse, edge(a, b), edge(b, c)}.

The corresponding set of positive examples is

P = {target(a, b), target(b, a), target(b, c), target(c, b)}

and the set of negative examples is

P = {target(a, c), target(c, a)}.

22

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X, Y ) ← (aux1(X, Y ) ∧ edge(Y, X)) ∨ edge(X, Y ),

aux1(X, Y ) ← edge(X, Y ),

where aux1 is an invented auxiliary predicate.
Less Than
background knowledge is the same as that in Predecessor task. The set of positive examples is

The goal of this task is to learn the less-than(X, Y ) relation which is true if X is less than Y . Here the

and the set of negative examples is

P = {target(X, Y )|X < Y }

N = {target(X, Y )|X ≥ Y }.

We use ﬁxed training data for this task given the range of integers. One solution found by our method is:

target(X, Y ) ← (target(X, Z) ∧ target(Z, Y )) ∨ succ(X, Y ).

The goal of this task is to learn the member(X, Y ) relation which is true if X is an element in list Y .
Member
Elements in a list are encoded with two relations cons and values, where cons(X, Y ) if Y is a node after X with null
node 0 being the termination of lists and value(X, Y ) if Y is the value of node X.

Take the list [3, 2, 1] as an example. The corresponding set of positive examples is

P = {target(3, [3, 2, 1]),

target(2, [3, 2, 1]),

target(1, [3, 2, 1]),

target(2, [2, 1]),

target(1, [2, 1],

target(3, [3, 2]),

target(2, [3, 2]),

target(3, [3, 1]),

target(1, [3, 1]),

T rue, F alse}

and the set of negative examples is

P = {target(3, [2, 1]), target(1, [3, 2]), target(2, [3, 1])}.

We use randomly generated training data for this task given the length of the list. One solution found by our method

is:

target(X, Y ) ← (target(X, Z) ∧ cons(Z, Y )) ∨ value(X, Y ).

Connectedness
of edges connecting nodes X and Y . An example set of background knowledge is

The goal of this task is to learn the connected(X, Y ) relation which is true if there is a sequence

B = {T rue, F alse, edge(a, b), edge(b, c), edge(c, d)}.

The corresponding set of positive examples is

P = {target(a, b), target(b, c), target(c, d), target(a, c), target(a, d), target(b, d)}

and the set of negative examples is

P = {target(b, a), target(c, b), target(d, c), target(d, a), target(d, b), target(c, a)}.

23

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X, Y ) ← (target(X, Z) ∧ target(Z, Y )) ∨ edge(X, Y ).

The goal of this task is to learn the son-of (X, Y ) relation which is true if X is the son of Y . The background
Son
knowledge consists of various facts about a family tree containing the relations f ather-of, bother-of and sister-of .
An example set of background knowledge is

B = {T rue, F alse, f ather(a, b), f ather(a, c), f ather(a, d), bother(b, c), bother(d, c), sister(c, b)}.

The corresponding set of positive examples is

P = {target(b, a), target(d, a)}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the family tree. One solution

found by our method is:

target(X, Y ) ← (aux1(X, Y ) ∧ f ather(Y, X)) ∨ F alse

aux1(X) ← (f ather(X, Z) ∧ T rue) ∨ brother(X, T )

where aux1 is an invented auxiliary predicate.
The goal of this task is to learn the grandparent(X, Y ) relation which is true if X is the grand-
Grandparent
parent of Y . The background knowledge consists of various facts about a family tree containing the relations
f ather-of and mother-of . An example set of background knowledge is

B = {f ather(c, b), f ather(b, a), mother(d, b), mother(e, a), T rue, F alse}.

The corresponding set of positive examples is

P = {target(c, a), target(d, a)}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the family tree. One solution

found by our method is:

target(X, Y ) ← (aux1(X, Z) ∧ aux1(Z, Y )) ∨ F alse,

aux1(X, Y ) ← (mother(X, Y ) ∧ T rue) ∨ f ather(X, Y ),

where aux1 is an invented auxiliary predicate.
Adjacent to Red
In this task, nodes of the graph are colored either green or red. The goal of this task is to learn
the is-adjacent-to-a-red-node(X) relation which is true if node X is adjacent to a red node. Other than the relation
edge, the background knowledge also consists of facts of relations colour and red, where colour(X, C) if node X
has colour C and red(C) if colour of C is red.
An example set of background knowledge is

B = {T rue, F alse, edge(a, b), edge(b, a), edge(d, e), edge(d, f ), colour(a, p), red(p), colour(d, q), red(q)}.

24

The corresponding set of positive examples is

P = {target(b), target(e), target(f )}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X) ← (edge(X, Z) ∧ aux1(Z, X)) ∨ F alse,

aux1(X) ← (colour(X, Z) ∧ red(Z, X)) ∨ F alse,

where aux1 is an invented auxiliary predicate.
The goal of this task is to learn the has-at-least-two-children(X) relation which is true if node
Two Children
X has at least two child nodes. Other than the relation edge, the background knowledge also consists of facts of
not-equals relation neq, where neq(X, Y ) if node X does not equal to node Y .

An example set of background knowledge is

B = {T rue, F alse, edge(a, b), edge(a, c), edge(c, d), neq(a, b), neq(a, c), neq(a, d), neq(b, c), neq(b, d), neq(c, d)}.

The corresponding set of positive example(s) is

P = {target(a)}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X) ← (aux1(X, Z) ∧ edge(Z, X)) ∨ F alse,

aux1(X, Y ) ← (edge(X, Z), ∧neq(Z, Y )) ∨ F alse,

where aux1 is an invented auxiliary predicate.
Relatedness
any family relations. The background knowledge is parent(X, Y ) if X is Y ’s parent.

The goal of this task is to learn the related(X, Y ) relation, which is true if two nodes X and Y have

An example set of background knowledge is

B = {T rue, F alse, parent(a, b), parent(a, c), parent(c, e), parent(c, f ), parent(d, c), parent(g, h)}.

The corresponding set of positive examples is

P = {target(a, b), target(a, c), target(a, d), target(a, e), target(a, f ),

target(b, a), target(b, c), target(b, d), target(b, e), target(b, f ),

target(c, a), target(c, b), target(c, d), target(c, e), target(c, f ),

target(d, a), target(d, b), target(d, c), target(d, e), target(d, f ),

target(e, a), target(e, b), target(e, c), target(e, d), target(e, f ),

target(f, a), target(f, b), target(f, c), target(f, d), target(f, e)

target(g, h), target(h, g)}

25

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the family tree. One solution

found by our method is:

target(X, Y ) ← (target(X, Z) ∧ aux1(Z, Y )) ∨ parent(X, Y ),

aux1(X, Y ) ← (target(X, Y ) ∧ target(Y, X)) ∨ parent(X, Y ),

where aux1 is an invented auxiliary predicate.
Cyclic
edge connections, from node X back to itself. An example set of background knowledge is

The goal of this task is to learn the is-cyclic(X) relation which is true if there is a path, i.e., a sequence of

B = {T rue, F alse, edge(a, b), edge(b, a), edge(d, c), edge(d, b)}.

The corresponding set of positive examples is

P = {target(a), target(b)}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X) ← (aux1(X, Z) ∧ aux1(Z, X)) ∨ F alse,

aux1(X, Y ) ← (aux1(X, Z) ∧ edge(Z, Y )) ∨ edge(X, Y ).

where aux1 is an invented auxiliary predicate.
The goal of this task is to learn the adj-to-same(X, Y ) relation which is true if nodes X and Y
Graph Coloring
are of the same colour and there is an edge connection between them. The background knowledge consists of facts
about a coloured graph containing the relations edge and colour, which are similar to those in the task Adjacent to
Red. An example set of background knowledge is

B = {T rue, F alse, edge(a, b), edge(b, a), edge(b, c), edge(a, d), colour(a, p), colour(b, p), colour(c, q), colour(d, q)}.

The corresponding set of positive examples is

P = {target(a, b), target(b, a)}

and the set of negative examples N is a subset of all ground atoms involving the target predicates that are not in P.

We use randomly generated training data for this task given the number of nodes in the graph. One solution found

by our method is:

target(X, Y ) ← (edge(X, Z) ∧ aux1(Z, X)) ∨ F alse,

aux1(X, Y ) ← (colour(X, Z) ∧ colour(Z, Y )) ∨ F alse,

where aux1 is an invented auxiliary predicate.
The goal of this task is to learn the length(X, Y ) relation which is true if the length of list X is Y . Similar
Length
to the task Member, elements in a list are encoded with two relations cons and succ, where cons(X, Y ) if Y is a node
after X with null node 0 being the termination of lists and succ(X, Y ) if Y is the next value of integer X. Moreover,
this task adds zero(X) as another background predicate, which is true if X is 0.

26

Take the list [3, 2, 1] as an example, suppose node 0 is the end of a list. The background knowledge is

B = {T rue, F alse, zero(0), succ(0, 1), succ(1, 2), succ(2, 3)}.

The corresponding set of positive examples is

P = {target([3, 2, 1], 3),

target([2, 1], 2),

target([1], 1)}

We use randomly generated training data for this task given the number of nodes in a list.
The goal of this task is to learn the even(X) relation which is true if value X is an even number. The
Even-Odd
background knowledge includes two predicates, one is zero(X), which is true if X is 0, another one is succ(X, Y ),
which is true if Y is the next value of X. An example set of background knowledge is

B = {T rue, F alse, zero(0), succ(0, 1), succ(1, 2), . . . }.

The corresponding set of positive examples is

and the set of negative examples is

P = {target(0), target(2), target(4), . . . }

N = {target(1), target(3), target(5), . . . }.

Once the number of constants is given, the dataset is deterministic. One solution found by our method is:

target(X) ← (zero(X) ∧ aux1(Z, X)) ∨ zero(X),

aux1(X, Y ) ← (aux1(X, Z) ∧ aux1(Z, Y )) ∨ aux2(X, Y ),

aux2(X, Y ) ← (succ(X, Z) ∧ succ(Z, Y )) ∨ F alse,

where aux1 and aux2 are invented auxiliary predicates.
Even-Succ2
Even-succ has the same backgrounds and target predicates as Even-Odd. In Campero et al. [2018],
they provide two different templates set tailored to Even-Succ respectively to Even-Odd. However, in our approach,
this difference is not relevant anymore: since we provide a uniform template set, these two tasks become identical.
Buzz The goal of this task is to learn the buzz(X) relation which is true if value X is divisible by 5. The background
knowledge consists of 4 predicates: zero(X) is true if X is 0, succ(X, Y ) is true if Y is the next value of X,
pred1(X, Y ) is true if Y = X + 3, pred2(X, Y ) is true if Y = X + 2. An example set of background knowledge is

B = {T rue, F alse, zero(0), succ(0, 1), succ(1, 2), . . . , pred1(0, 3), pred2(2, 4), . . . , pred2(0, 2), pred2(1, 3), . . . }.

The corresponding set of positive examples is

P = {target(0), target(5), . . . }

and the set of negative examples is

N = {target(1), target(2), target(3), target(4), target(6), target(7), . . . }.

27

Once the number of constants is given, the dataset is deterministic. One solution found by our method is:

target(X) ← (aux1(X, Z) ∧ pred2(Z, X)) ∨ zero(X),

aux1(X, Y ) ← (aux2(X, Z) ∧ pred1(Z, Y )) ∨ F alse,

aux2(X, Y ) ← (T rue ∧ aux3(Y )) ∨ zero(X),

aux3(X) ← (aux1(X, Z) ∧ pred2(Z, X)) ∨ zero(X),

where aux1, aux2 and aux3 are invented auxiliary predicates.
Fizz
knowledge is the same with Even − Odd task. The corresponding set of positive examples is

The goal of this task is to learn the f izz(X) relation which is true if value X is divisible by 3. The background

P = {target(0), target(5), . . . }

and the set of negative examples is

N = {target(1), target(2), target(3), target(4), target(6), target(7), . . . }.

Once the number of constants is given, the dataset is deterministic.

Further Experimental Results

In Table 6, we provide the results for all the ILP tasks.

Task

|I| Recursive

∂ILP LRI

Ours

train

soft evaluation

symbolic evaluation

Predecessor
Undirected Edge
Less than
Member
Connectedness
Son
Grandparent
Adjacent to Red
Two Children
Relatedness
Cyclic
Graph Coloring
Length
Even-Odd
Even-Succ2
Buzz
Fizz

1
1
1
1
1
2
2
2
2
2
2
2
2
2
2
2
3

No
No
Yes
Yes
Yes
No
No
No
No
Yes
Yes
Yes
Yes
Yes
Yes
Yes
Yes

100
100
100
100
100
100
96.5
50.5
95
100
100
94.5
92.5
100
48.5
35
10

100
100
100
100
100
100
100
100
0
100
100
0
100
100
100
70
10

100
100
100
100
100
100
100
100
100
100
100
100
20
40
40
100
0

100
100
100
100
100
100
100
100
100
100
100
100
0
40
40
40
0

100
100
100
100
100
100
100
100
100
100
100
100
0
40
40
40
0

Table 6: Percentage of successful runs among 10 runs. |I| is the smallest number of intensional predicates needed.
Recursive means whether or not the solution needs to learn recursive rules.

28

About Length Task: The deceiving performance in this table for task Length can be easily explained: in the-
ory our model corresponds to rule-induction with function-free deﬁnite Horn clause in theory; yet, de facto, in the
recursive-case, both the number of layers and the number of instantiated auxiliary predicate by proto-rule per layer
deﬁne the actual expressivity of the model. The number of of instantiated auxiliary predicate by proto-rule per layer is
by default set at 1 in all our experiments, as we intended to share the same hyperparameters set on all tasks; however,
to have a chance to solve the task Length, we should increase this number to 2, to widen the hypothesis space.

About Even-Odd Task: As mentionned above, task Even-Odd corresponds, for our method, to the same task as
the task Even-Succ2, as the target predicate is identical. This is not the case for other ILP approaches like ∂ILP or
LRI, as they hand-engineer different template sets for each of these two tasks, corresponding to the desired auxiliary
predicate.

Operation Choices

In our implementation, the default choice is sum for POOL, min for AND, max for OR. In 7, we experimentally
compare different choices for these operations. The ﬁrst column shows the results with our default choices, while other
columns show the results by using max for POOL, product for AND, and prodminus19 for OR.

Task

default

POOL-max AND-product OR-prodminus

Adjacent To Red
Member
Cyclic
Two Children

100
100
100
100

20
40
50
70

100
100
90
80

100
90
100
80

Table 7: Percentage of successful runs among 10 runs using soft evaluation, obtained by models trained with different
implementations for POOL, MERGE, AND, OR.

Limitations of LRI [Campero et al., 2018]

Using LRI, if gathering all the templates needed for all ILP tasks (in Table 6) in a template set RLRI , we obtain
18 templates. Table 8 demonstrates how the evaluation success rate decrease if trained LRI with RLRI . While LRI
can obtain good results as indicated in Table 6 if provided with meta-rules customized for each task, the performance
quickly degrades for hard tasks when provided a set of more generic meta-rules. This phenomenon is more accentuated
for more complex tasks. For easy tasks, like Predecessor, the performance didn’t change.

19prodminus(v1, v2) = v1 + v2 − v1 ∗ v2

29

Tasks

LRI with speciﬁc templates LRI with RLRI

Predecessor
Undirected Edge
Less than
Member
Connectedness
Son
Grandparent
Adjacent to Red
Two Children
Relatedness
Cyclic
Graph Coloring
Length
Even-Odd
Even-Succ2
Buzz
Fizz

100
100
100
100
100
100
100
100
0
100
100
0
100
100
100
70
10

100
80
100
30
100
80
90
0
0
100
0
0
50
20
10
0
0

Table 8: LRI’s performance with increasing number of rule templates. Mearsured by percentage of successful runs
among 10 runs using soft evaluation.

C Other Experimental Results

Visual Genome Experiments

For this experiment, we use a dataset called GQA [Hudson and Manning, 2019b] which is a preprocessed version of
the Visual Genome dataset [Krishna et al., 2017], since the original is commonly considered to be too noisy [Zellers
et al., 2018]. In GQA, the original scene graphs have been converted into a collection of KBs, leading to 1.9M facts,
1.4M constants, and 2100 predicates. Following [Yang and Song, 2020], we ﬁlter this dataset to remove the predicates
that appear less than 1500 times and to focus on the top 150 objects.

We train 150 models to provide a logic explanation to those 150 objects. For the evaluation metrics, we use recall
@1(R@1) and recall @5(R@5), which are computed on a held-out set. R@k measures the fraction of ground-truth
atoms that appear among the top k most conﬁdent predictions in an image. In our model, even though all the models
are instantiated with the same max layer nL, the trained model may have different layers. Indeed, since each auxiliary
predicate at layer (cid:96) may be formed with predicates from any layer 0 to (cid:96) in P ↓
(cid:96) ), the trained model may contain 1 to
nL layers (without counting the target predicate). Given the soft uniﬁcation computation in (10), a trained model with
a larger number of layers has a tendency to output smaller values. Therefore, to make the output of all the models
comparable, we use a simple L2 normalization before comparing the outputs of those trained models in order to
compute the evaluation metrics.

In the multi-task setting we initialized background embeddings from GPT2. We compare the semantic space un-
derlying these embeddings after ﬁne-tuning (i.e. training of our model). More precisely, we use cosine similarity to
measure distances between all pairs of background embeddings, sort and select top 10 closest pairs. As Table 9 sug-

30

gests, the ﬁne-tuned embeddings pairs have akin similarities; this initialization choice may therefore help with the
performance to some extent.

Top 10 pairs of similar embeddings

GPT2

(bag, backpack), (arrow, apple), (airplane, air), (backpack, airplane), (apple, airplane),
(animal, airplane), (arrow, animal), (at, above), (apple, animal), (bag, airplane)

Fine-tuned

(bag, backpack), (bag, airplane), (arrow, apple), (airplane, air), (arm, air),
(backpack, airplane), (air, above), (apple, air), (arrow, animal), (arm, airplane)

Table 9: Top 10 closest embeddings from GPT2 and our trained multi-task model.

RL Experiments

We also tested our model in a sequential decision-making framework: on the tasks Stack, Unstack and On from the
Blocksworld environment, as described in Jiang and Luo [2019]. In those tasks, an agent has 50 steps to build a stack
(Stack task), place all the blocks directly on the ﬂoor (Unstack task), or move a speciﬁc block into another (On task).
From all the valuations of the target predicate Move(X,Y), we compute a softmax policy used during the exploration.
During training, the supervised BCE loss (13) is replaced by a standard PPO loss [Schulman et al., 2017] on the
softmax policy. To estimate the advantage function, we relied on the same critic architecture deﬁned by Zimmer et al.
[2021].

At the end of the training, the symbolic policy is extracted and evaluated on 5 testing scenarios not seen during
training. We reported the performance in Table 10. Our model can be better than NLRL and as good as NLM and
DLM.

Task

Rewards

Training time

NLRL NLM DLM Ours NLRL

NLM

DLM

Ours

Unstack
Stack
On

0.914
0.877
0.885

0.920
0.920
0.896

0.920
0.920
0.896

0.920
0.920
0.896

hours minutes minutes minutes

Table 10: Comparisons with NLRL/NLM/DLM in terms of rewards on
the testing scenarios and the order of magnitude of training times.

Sensitivity to Hyperparameters

To evaluate the sensitivity of our model to hyperparameter, we tested other hyperparameter choices on some ILP
tasks. We refer to Table 13 for the results obtained with default hyperparameters, presented Table 12. As Table 11
suggests, both smaller inference steps, or smaller depth will affect performance; this performance decrease natural,
since it narrows down the hypothesis space, which may not contain anymore the solution needed for the task. However,

31

we can appreciate the fact that larger inference step or depth usually will not affect much the performance (until a
limit). We can also notice that cosine performs better than other similarity functions; naturally, restricting or excluding
the recursivity would perform well compared with full recursivity, unless the Task require it.

task

default

Adjacent to Red
Member
Cyclic
Two Children

task

Adjacent to Red
Member
Cyclic
Two Children

100
100
100
100

L1

10
60
60
0

inference-steps (train-steps, eval-steps)

(st − 2, se − 2)

(st − 1, se − 1)

(st + 1, se + 1)

(st + 2, se + 2)

90
100
90
80

100
100
90
100

100
100
90
100

100
100
100
100

similarity

recursivity

max-depth

L2

scalar-product

none

iso-recursive

80
100
80
60

40
80
50
10

100
0
90
90

100
100
100
100

1

0
50
10
0

2

90
100
60
70

3

100
100
80
70

5

100
100
90
100

Table 11: Percentage of successful runs among 10 runs using soft evaluation, obtained by models trained with different
hyper-parameter choices. Here, st and se are default inference steps used in training and evaluation, shown in Table
13.

D Hyperparameters

We list relevant generic and task-speciﬁc hyper-parameters used for our training method in Tables 12 and 13, re-
spectively. In Table 13, the hyper-parameters train-num-constants and eval-num-constants represent the number of
constants during training and evaluation, respectively. We keep the values of the two hyper-parameters for each task
the same as those used in Campero et al. [2018]. Note that our model do not require the actual knowledge of the depth
of the solution; the max-depth parameter simply could be an upper bound. Although, this parameter could be reduced
for simpler tasks, we set max-depth= 4 for all tasks to make our training method more generic.

Hyper-parameter

recursivity

fuzzy-and

Value

full

min

fuzzy-or

max

similarity

lr

lr-rules

cosine

0.01

0.03

Hyper-parameter

temperature Gumbel-noise Gumbel-noise-decay-mode

Value

0.1

0.3

linear

Table 12: Generic hyper-parameters for all tasks.

32

Task

max-depth

train-steps

eval-steps

train-num-constants

eval-num-constants

Predecessor
Undirected Edge
Less than
Member
Connectedness
Son
Grandparent
Adjacent to Red
Two Children
Relatedness
Cyclic
Graph Coloring
Even-Odd
Even-Succ2
Buzz

4
4
4
4
4
4
4
4
4
4
4
4
4
4
4

2
2
12
12
4
4
4
4
4
10
4
4
6
6
8

4
2
12
12
4
4
4
4
5
12
4
4
8
8
10

10
4
10
5
5
9
9
7
5
8
6
8
11
11
11

Table 13: Specialized hyper-parameters for each ILP task.

14
6
12
7
5
10
11
9
7
10
7
10
15
15
16

In multi-task GQA, we use used the same generic hyperparameters as given in Table 12 except that we disallow
recursivity and decreased the learning rate (lr) to 0.001 for background embeddings and rule learning rate (lr-rules) to
0.01 for intensional embeddings. Other speciﬁc hyperparametes are given in Table 14.

Hyperparameter

Value

Max depth
Train-steps
Embedding-dim
Train-iterations nT
Train-num-positive-instances np
Train-num-random-instances nr

3
4
30
3000
5
5

Table 14: Multi-task GQA hyperparameters.

In reinforcement learning, we used the same generic hyperparameters as given in Table 12 except that we don’t use

recursivity and decreased the learning rate to 0.005. Additionally, hyperparameters are given in Table 15.

33

Hyperparameter

Value

Max depth
Train-steps
Train-num-constants
Temperature of softmax policy
GAE λ
γ
Trajectory per update
PPO (cid:15)-clipping
GRU hidden neurons (critic)

6
6
5
0.01
0.9
0.99
5
0.2
64

Table 15: Reinforcement learning hyperparameters.

E Some promises and limitations of our Model

Expressivity Limitations As made explicit in Theorem8.8, our model corresponds to rule-induction with function-
free deﬁnite Horn clause. Of course, the number of layers, affects the actual expressivity of each model; in the non-
recursive case, this parameter is sufﬁcient to reach all the function-free recursive-free deﬁnite Horn clause. In the
recursive case, the number of instantiated rules from the proto-rule set (by default set at 1) also affects the expressivity;
this may be seen in the task Length in ILP, where two rules should be initiated per template set to widen sufﬁcient
hypothesis space.

The expressivity of our model could be extended by different ways, more or less computationally-expensive, and
therefore more or less judicious. The points below would be object of further investigations and experiments; for this
reason, we only share in this paper some succinct comments on different tracks to gain expressivity:

+ Enabling negations.

+ Enabling functions.

+ Enabling zero-ary predicates: We could easily extend our result to include 0 ary predicates.

Claim : By adding the zero-ary predicate Z to R0, the hypothesis space reaches the fragment F ≤2

P,≤2

(30)

where Z : P () ← Q(A, B).

+ Enabling more body atoms:20

Claim : Enabling 3 or 4 body atoms would result in the same hypothesis space.

(31)

This can be proven by reducing clauses with 3 resp. 4 body atoms to 2 resp. 3 body atoms. Instead of a formal
proof, let us gives a visual illustration of these reductions21, in Figure 3 resp. Figure 5. Red arrows denote head
predicates; full grey arrows indicates two arrows which can be reduced into the dotted grey arrow to lower the
number of body atoms by introducing an auxiliary predicate.

20For instance the language bias present in Gal´arraga et al. [2013] is narrowing the space to connected, two-connected and duplicate-free Horn
clauses U . On the one hand, U is a smaller fragment than F which our model is reaching. However, they consider clauses with N body atoms (N
hypothetically small in their experiments), which could enable greater expressivity.

21We considered clauses upon permutation and symmetries, as usual; we omitted clauses having two body atoms with the same variables, as

they can be trivially reduced.

34

In contrast, as illustrated Figure 5 some rules with 5 body clauses are not reducible to 2 body clauses, such as:

P (A, B) ← Q(A, C), R(A, D), S(B, C), T (B, D), U (C, D).

(32)

As mentioned in Cropper and Tourret [2020], F {1,2}
higher number of clauses (such as 5) may drastically increase the computational cost.

is therefore not D-reducible to F {1,2}

≤2

≤5

. However, allowing

+ Enabling higher-arity.

For instance, considering arity 3 predicates, while keeping two body clauses, could enable to reach C{1,2}

≤5

Claim : F {1,2,3}

≤2

[P] ⊃ F {1,2}

≤5

:

(33)

Figure 3: Reduction of Meta-rules with 3 body atoms

Figure 4: Reduction of Meta-rules with 4 body atoms

Figure 5: Irreducible Meta-Rule with 5 body atoms

Further comparative experiments could be led in the future, to test different minimal or small meta-rules or proto-
rules set, and investigate some judicious balance of minimalism/redundancy and expressivity/efﬁciency in diverse
tasks.

35

Promises
In contrast to most of previous ILP or differentiable ILP works, we hypothesize that our model could
be particularly suited for continual learning in semantically-richer domains; for instance, in RL scenarios where an
interpretable logic-oriented higher level policy seems pertinent (e.g., autonomous driving). Here some arguments to
support this belief:

+ In contrast to previous works, our model is independent of the number of predicates, which may vary between

training and testing.

+ As our experiments in Visual Genome suggest, our model can be boostrapped by semantic or visual priors, to

initialise the predicates.

+ In semantically-rich domains, the complexity of our model would be more advantageous than other ILP works.
Semantically rich domains are characterized by a high number of initial predicates, while enabling a much lower
embedding dimension d (as these predicates are highly interdependent), d <<| P0 |; such inequality implies a
lower complexity for our model than common methods which have to learn one coefﬁcient per predicate.

+ The embedding-based approach enables to reason by analogy, and possibly quickly generalise to new predicates.
For instance, if the model has learned the rule OverT ake() ← ¬(P (X) ∧ OnN extLane(X)), and the embed-
ding θP is learned close to θCar, the rule would generalise to include Truck, assuming θCar ∼ θT ruck, despite
having never seen the new predicate Truck during training.

36

