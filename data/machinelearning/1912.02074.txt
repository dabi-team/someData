AlgaeDICE: Policy Gradient
from Arbitrary Experience

Oﬁr Nachum∗

Bo Dai∗

Ilya Kostrikov†

{ofirnachum, bodai, kostrikov}@google.com

Yinlam Chow

Lihong Li

Dale Schuurmans‡

{yinlamchow, lihong, schuurmans}@google.com

Google Research

Abstract

In many real-world applications of reinforcement learning (RL), interactions with
the environment are limited due to cost or feasibility. This presents a challenge to
traditional RL algorithms since the max-return objective involves an expectation
over on-policy samples. We introduce a new formulation of max-return optimization
that allows the problem to be re-expressed by an expectation over an arbitrary
behavior-agnostic and oﬀ-policy data distribution. We ﬁrst derive this result by
considering a regularized version of the dual max-return objective before extending
our ﬁndings to unregularized objectives through the use of a Lagrangian formulation
of the linear programming characterization of Q-values. We show that, if auxiliary
dual variables of the objective are optimized, then the gradient of the oﬀ-policy
objective is exactly the on-policy policy gradient, without any use of importance
In addition to revealing the appealing theoretical properties of this
weighting.
approach, we also show that it delivers good practical performance.

9
1
0
2

c
e
D
4

]

G
L
.
s
c
[

1
v
4
7
0
2
0
.
2
1
9
1
:
v
i
X
r
a

1

Introduction

The use of model-free reinforcement learning (RL) in conjunction with function approx-
imation has proliferated in recent years, demonstrating successful applications in ﬁelds
such as robotics (Andrychowicz et al., 2018; Nachum et al., 2019a), game playing (Mnih
et al., 2013), and conversational systems (Gao et al., 2019). These successes often rely on

∗Equal contribution.
†Also at NYU.
‡Also at University of Alberta.

1

 
 
 
 
 
 
on-policy access to the environment; i.e., during the learning process agents may collect
new experience from the environment using policies they choose, and these interactions are
eﬀectively unlimited. By contrast, in many real-world applications of RL, interaction with
the environment is costly, if not impossible, hence experience collection during learning
is limited, necessitating the use of oﬀ-policy RL methods, i.e., algorithms which are able
to learn from logged experience collected by potentially multiple and possibly unknown
behavior policies.

The oﬀ-policy nature of many practical applications presents a signiﬁcant challenge for RL
algorithms. The traditional max-return objective is in the form of an on-policy expectation,
and thus, policy gradient methods (Sutton et al., 2000; Konda and Tsitsiklis, 2000) require
samples from the on-policy distribution to estimate the gradient of this objective. The most
straightforward way to reconcile policy gradient with oﬀ-policy settings is via importance
weighting (Precup et al., 2000). However, this approach is prone to high variance and
instability without appropriate damping (Munos et al., 2016; Wang et al., 2016; Gruslys
et al., 2017; Schulman et al., 2017). The more common approach to the oﬀ-policy problem
is to simply ignore it, which is exactly what has been proposed by many existing oﬀ-policy
policy gradient methods (Degris et al., 2012; Silver et al., 2014). These algorithms simply
compute the gradients of the max-return objective with respect to samples from the oﬀ-
policy data, ignoring distribution shift in the samples. The justiﬁcation for this approach
is that the maximum return policy will be optimal regardless of the sampling distribution
of states. However, such a justiﬁcation is unsound in function approximation settings,
where models have limited expressiveness, with potentially disastrous consequences on
optimization and convergence (e.g., Lu et al., 2018).

Value-based methods provide an alternative that may be more promising for the oﬀ-policy
setting. In these methods, a value function is learned either as a critic to a learned policy
(as in actor-critic) or as the maximum return value function itself (as in Q-learning).
This approach is based on dynamic programming in tabular settings, which is inherently
oﬀ-policy and independent of any underlying data distribution. Nevertheless, when using
function approximation, the objective is traditionally expressed as an expectation over
single-step Bellman errors, which re-raises the question, “What should the expectation
be?” Some theoretical work suggests that the ideal expectation is in fact the on-policy
expectation (Sutton et al., 2000; Silver et al., 2014; Nachum et al., 2018). In practice, this
problem is usually ignored, with the same justiﬁcation as that made for oﬀ-policy policy
gradient methods. It is telling that actor-critic or Q-learning algorithms advertised as
oﬀ-policy still require large amounts of online interaction with the environment (Haarnoja
et al., 2018; Hessel et al., 2018).

In this work, we present an ALgorithm for policy Gradient from Arbitrary Experience via
DICE (AlgaeDICE)1 as an alternative to policy gradient and value-based methods. We

1DICE is an abbreviation for distribution correction estimation and is taken from the DualDICE
work (Nachum et al., 2019b) on oﬀ-policy policy evaluation. Although our current work notably focuses
on policy optimization as opposed to evaluation and only implicitly estimates the distribution corrections,
our derivations are nevertheless partly inspired by this previous work.

2

start with the dual formulation of the max-return objective, which is expressed in terms of
normalized state-action occupancies rather than a policy or value function. Traditionally,
this objective is considered unattractive, since access to the occupancies either requires
an on-policy expectation (similar to policy gradient methods) or learning a function
approximator to satisfy single-step constraints (similar to value-based methods). We
demonstrate how these problems can be remedied by adding a controllable regularizer
and applying a carefully chosen change of variables, obtaining a joint objective over a
policy and an auxiliary dual function (that can be interpreted as a critic). Crucially, this
objective relies only on access to samples from an arbitrary oﬀ-policy data distribution,
collected by potentially multiple and possibly unknown behavior policies (under some
mild conditions). Unlike traditional actor-critic methods, which use a separate objective
for actor and critic, this formulation trains the policy (actor) and dual function (critic)
to optimize the same objective. Further illuminating the connection to policy gradient
methods, we show that if the dual function is optimized, the gradient of the proposed
objective with respect to the policy parameters is exactly the on-policy policy gradient.
This way, our approach naturally avoids issues of distribution mismatch without any
explicit use of importance weights. We continue to provide an alternative derivation of
the same results, based on a primal-dual form of the return-maximizing RL problem, and
notably this perspective extends the previous results to both undiscounted γ = 1 settings
and unregularized max-return objectives. Finally, we provide empirical evidence that
AlgaeDICE can perform well on benchmark RL tasks.

2 Background

We consider the RL problem presented as a Markov Decision Process (MDP) (Puterman,
1994), which is speciﬁed by a tuple M = hS, A, r, T, µ0i consisting of a state space, an
action space, a reward function, a transition probability function, and an initial state
distribution. A policy π interacts with the environment by starting at an initial state
s0 ∼ µ0, and iteratively producing a sequence of distributions π(·|st) over A, at steps
t = 0, 1, ..., from which actions at are sampled and successively applied to the environment.
At each step, the environment produces a scalar reward rt = r(st, at) and a next state
st+1 ∼ T (st, at). In RL, one wishes to learn a return-maximizing policy:

maxπ JP(π) := (1 − γ) Es0∼µ0,a0∼π(s0) [Qπ(s0, a0)] ,
(1)
where Qπ describes the future rewards accumulated by π from any state-action pair (s, a),
Qπ(s, a) = Eh ∞
and 0 ≤ γ < 1 is a discount factor. This objective may be equivalently written in its
dual form (Puterman, 1994; Wang et al., 2008) in terms of the policy’s normalized state
visitation distribution as

i
s0 = s, a0 = a, st ∼ T (st−1, at−1), at ∼ π(st) for t ≥ 1

γtr(st, at)

, (2)

X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

t=0

maxπ JD(π) := E(s,a)∼dπ [r(s, a)] ,

(3)

3

where
dπ(s, a)=(1−γ)

∞
X

t=0

γtPr [ st = s, at = s| s0 ∼ µ0, at ∼ π(st), st+1 ∼ T (st, at) for t ≥ 0] . (4)

As we will discuss in Section 4 and Appendix A, these objectives are the primal and dual
of the same linear programming (LP) problem.

In function approximation settings, optimizing π requires access to gradients. The policy
gradient theorem (Sutton et al., 2000) provides the gradient of JP(π) as

∂

∂π JP(π) = E(s,a)∼dπ [Qπ(s, a)∇ log π(a|s)] .
(5)
To properly estimate this gradient one requires access to on-policy samples from dπ and
access to estimates of the Q-value function Qπ(s, a). The ﬁrst requirement means that
every gradient estimate of JP necessitates interaction with the environment, which limits
applicability of this method in settings where interaction with the environment is expensive
or infeasible. The second requirement means that one must maintain estimates of the
Q-function to learn π. This leads to the family of actor-critic algorithms that alternate
between updates to π (the actor) and updates to a Q-approximator Qθ (the critic). The
critic is learned by encouraging it to satisfy single-step Bellman consistencies,
Qπ(s, a) = BπQπ(s, a) := r(s, a) + γ · Es0∼T (s,a),a0∼π(s0) [Qπ(s0, a0)] ,
(6)
where Bπ is the expected Bellman operator with respect to π. Thus, the critic is learned
according to some variation on

minQθ Jcritic(Qθ) := 1
2

E(s,a)∼β [(BπQθ − Qθ)(s, a)2] ,

(7)
for some distribution β. Although the use of an arbitrary β suggests the critic may be
learned oﬀ-policy, to achieve satisfactory performance, actor-critic algorithms generally
rely on augmenting a replay buﬀer with new on-policy experience. Theoretical work has
suggested that if one desires compatible function approximation, then an appropriate β
is, in fact, the on-policy distribution dπ (Sutton et al., 2000; Silver et al., 2014; Nachum
et al., 2018).

In this work, we focus on the oﬀ-policy setting directly. Speciﬁcally, we are given a dataset
D = {(sk, ak, rk, s0
k ∼ T (sk, ak); and ak has been sampled
according to an unknown process. We let dD denote the unknown state-action distribution,
and additionally assume access to a sample of initial states, U = {s0,k}N
k=1, such that
s0,k ∼ µ0.

k=1, where rk = r(sk, ak); s0

k)}N

3 AlgaeDICE via Density Regularization

We begin by presenting an informal derivation of our method, motivated as a regularization
of the dual max-return objective in (3). In Section 4 we will present our results more
formally as a consequence of the Lagrangian of a linear programming formulation of the
max-return objective.

4

3.1 A Regularized Oﬀ-Policy Max-Return Objective

The max-return objective (3) is written exclusively in terms of the on-policy distribution dπ.
To introduce an oﬀ-policy distribution dD into the objective, we incorporate a regularizer:
(8)

JD,f (π) := E(s,a)∼dπ [r(s, a)] − αDf (dπkdD),

max
π

with α > 0 and Df denoting the f -divergence induced by a convex function f :

(cid:16)

h
f

(cid:17)i

,

Df (dπkdD) = E(s,a)∼dD

wπ/D(s, a)
where we have used the shorthand wπ/D(s, a) := dπ(s,a)
dD(s,a) . This form of regularization
encourages conservative behavior, compelling the state-action occupancies of π to remain
close to the oﬀ-policy distribution, which can improve generalization. We emphasize that
the introduction of this regularizer is to enable the subsequent derivations and not to
impose a strong constraint on the optimal policy. Indeed, by appropriately choosing α and
f , the strength of the regularization can be controlled. Later, we will show that many of
our results also hold for exploratory regularization (α < 0) and even for no regularization
at all (α = 0).

(9)

At ﬁrst glance, the regularization in (8) seems to complicate things. Not only do we
still require on-policy samples from dπ, but we also have to compute Df (dπkdD), which
in general can be diﬃcult. To make this objective more approachable, we transform
the f -divergence to its variational form (Nguyen et al., 2010) by use of a dual function
x : S × A → R:
min
max
x:S×A→R
π

˜JD,f (π, x)
:= E(s,a)∼dπ [r(s, a)] + α · E(s,a)∼dD[f∗(x(s, a))] − α · E(s,a)∼dπ [x(s, a)]
= E(s,a)∼dπ [r(s, a) − α · x(s, a)] + α · E(s,a)∼dD[f∗(x(s, a))] ,

(10)
where f∗ is the convex (or Fenchel) conjugate of f . With the objective in (10), we are
ﬁnally ready to eliminate the expectation over on-policy samples from dπ. To do so,
we make a change of variables, inspired by DualDICE (Nachum et al., 2019b). Deﬁne
ν : S × A → R as the ﬁxed point of a variant of the Bellman equation,

(11)
Equivalently, x(s, a) = 1
α(Bπν − ν)(s, a). Note that ν always exists and is bounded when
x and r are bounded (Puterman, 1994). Applying this change of variables to (10) (after
some telescoping, see Nachum et al. (2019b)) yields

ν(s, a) := −α · x(s, a) + Bπν(s, a).

max
π

min
ν:S×A→R

JD,f (π, ν)
:= (1 − γ)E s0∼µ0
a0∼π(s0)

[ν(s0, a0)] + α · E(s,a)∼dD[f∗((Bπν − ν)(s, a)/α)] .

(12)

The resulting objective is now completely oﬀ-policy, relying only on access to samples
from the initial state distribution µ0 and the oﬀ-policy dataset dD. Thus, we have our ﬁrst
theorem, providing an oﬀ-policy formulation of the max-return objective:

Theorem 1 (Primal AlgaeDICE) Under mild conditions on dD, α, f , the regularized
max-return objective may be expressed as a max-min optimization:

max
π

E(s,a)∼dπ [r(s, a)] − αDf (dπkdD) ≡ max

π

min
ν:S×A→R

JD,f (π, ν) .

(13)

5

Remark (extension to α < 0):
It is clear that the same derivations above may apply
to an exploratory regularizer of the same form with α < 0, which leads to the following
optimization problem:
max
ν:S×A→R

[ν(s0, a0)] + α · E(s,a)∼dD[f∗((Bπν − ν)(s, a)/α)].

(1 − γ) · E s0∼µ0
a0∼π(s0)

max
π

(14)

Remark (Fenchel AlgaeDICE) The appearance of Bπ inside f∗ in the second term
of (12) presents a challenge in practice, since Bπ involves an expectation over the transition
function T , whereas one typically only has access to a single empirical sample from
T for a given state-action pair. This challenge, known as double sampling in the RL
literature (Baird, 1995), can prevent the algorithm from ﬁnding the desired value function,
even with inﬁnite data. There are several alternatives to handle this issue (e.g., Antos
et al., 2008; Farahmand et al., 2016; Feng et al., 2019). Here, we apply the dual embedding
technique (Dai et al., 2016, 2018b). Speciﬁcally, the dual representation of f∗,

f∗((Bπν − ν)(s, a)/α) = max
ζ∈R

1
α

(Bπν − ν)(s, a) · ζ − f (ζ),

can be substituted into (12), to result in a max-min-max problem:

max
π
= max
π

E(s,a)∼dπ [r(s, a)] − αDf (dπkdD)
min
ν:S×A→R

max
ζ:S×A→R

(1 − γ)Es0∼µ0,a0∼π(s0)[ν(s0, a0)]+

E(s,a)∼dD,s0∼T (s,a),a0∼π(s0)[(γν(s0, a0) + r(s, a) − ν(s, a)) · ζ(s, a) − α · f (ζ(s, a))] . (15)
As we will see in Section 4, under mild conditions, strong duality holds in the inner
min-max of (15), hence one can switch the minν and maxζ to reduce to a more convenient
max-max-min form.

3.2 Consistent Policy Gradient using Oﬀ-Policy Data

The equivalence between the objective in (12) and the on-policy max-return objective
can be highlighted by considering the gradient of this objective with respect to π. First,
consider the optimal x∗
π := argminx JD,f (π, x) for (10). By taking the gradient of JD,f with
respect to x and setting this to 0, one ﬁnds that x∗
π satisﬁes

∗(x∗
f 0
Accordingly, for any π, the optimal ν∗
π − ν∗

∗((Bπν∗
f 0

π(s, a)) = wπ/D(s, a).
π := argminν JD,f (π, ν) for (12) satisﬁes

π)(s, a)/α) = wπ/D(s, a).

(16)

(17)

Thus, we may express the gradient of JD,f (π, ν∗
∂
∂π

Ea0∼π(s0)
s0∼µ0

π) = (1 − γ)

JD,f (π, ν∗

[ν∗

π) with respect to π as
"
wπ/D(s, a)

π(s0, a0)] + E(s,a)∼dD

∂
∂π

∂
∂π
∂
∂π
= E(s,a)∼dπ [ν∗

= (1 − γ)

Ea0∼π(s0)
s0∼µ0

[ν∗

π(s0, a0)] + γ · E(s,a)∼dπ,
s0∼T (s,a)

" ∂
∂π

Ea0∼π(s0)[ν∗

π(s, a)∇ log π(a|s)] ,

(Bπν∗

π − ν∗

#
π)(s, a)
#
π(s0, a0)]

6

where we have used Danskin’s theorem (Bertsekas, 1999) to ignore gradients of π through
ν∗
π. Hence, if the dual function ν is optimized, the gradient of the oﬀ-policy objective
JD,f (ν, π) is exactly the on-policy policy gradient, with Q-value function given by ν∗
π.

To characterize this Q-value function, note that from (11), ν∗
respect to augmented reward ˜r(s, a) := r(s, a) − α · x∗
π in (16) and the fact that the derivatives f 0 and f 0
x∗
˜r(s, a) = r(s, a) − α · f 0(wπ/D(s, a)). This derivation leads to our second theorem:

π is a Q-value function with
π(s, a). Recalling the expression for
∗ are inverses of each other, we have,

Theorem 2 If the dual function ν is optimized, the gradient of the oﬀ-policy objective
JD,f (π, ν) with respect to π is the regularized on-policy policy gradient:
i
h ˜Qπ(s, a)∇ log π(a|s)
JD,f (π, ν) = E(s,a)∼dπ

(18)
where, ˜Qπ(s, a) is the Q-value function of π with respect to rewards ˜r(s, a) := r(s, a) − α ·
f 0(wπ/D(s, a)).

∂
∂π

min
ν

,

Remark We note that Theorem 2 also holds when using the more sophisticated objective
in (15), since the optimal ζ ∗
π is equal to wπ/D, regardless of π.

3.3 Connection to Actor-Critic

The relationship between the proposed oﬀ-policy objective and the classic policy gradient
becomes more profound when we consider the form of the objective under speciﬁc choices
of convex function f . If we take f (x) = 1
2x2 and the proposed objective
is reminiscent of actor-critic:

2x2, then f∗(x) = 1

max
π

min
ν:S×A→R

JD,f (π, ν) := (1−γ)Es0∼µ0,a0∼π(s0)[ν(s0, a0)]+

·E(s,a)∼dD[((Bπν −ν)(s, a))2].
The second term alone is an instantiation of the oﬀ-policy critic objective in actor-critic.
However, in actor-critic, the use of an oﬀ-policy objective for the critic is diﬃcult to
theoretically motivate. Moreover, in practice, critic and actor learning can both suﬀer from
the mismatch between the oﬀ-policy distribution dD and the on-policy dπ. By contrast,
our derivations show that the introduction of the ﬁrst term to the objective transforms
the oﬀ-policy actor-critic algorithm to an on-policy actor-critic, without any explicit use
of importance weights. Moreover, while standard actor-critic has two separate objectives
for value and policy, our proposed objective is a single, uniﬁed objective. Both the policy
and value functions are trained with respect to the same oﬀ-policy objective.

1
2α

4 A Lagrangian View of AlgaeDICE

We now show how AlgaeDICE can be alternatively derived from the Lagrangian of a linear
programming (LP) formulation of the Qπ-function. Please refer to Appendix A for details.

7

We begin by introducing some notations and assumptions, which have appeared in the
literature (e.g., Puterman, 1994; Nachum et al., 2019b; Zhang et al., 2020).

Assumption 1 (Bounded rewards) The rewards of the MDP are bounded by some
ﬁnite constant Rmax: krk∞ ≤ Rmax.
For the next assumption, we introduce the transpose Bellman operator:

π ρ (s0, a0) := γ X
B>

s,a

π (a0|s0) T (s0|s, a) ρ (s, a) + (1 − γ) µ0 (s0) π (a0|s0) .

(19)

Assumption 2 (MDP regularity) The transposed Bellman operator B>
ﬁxed point solution.2

π has a unique

Assumption 3 (Bounded ratio) The target density ratio is bounded by some ﬁnite
constant Wmax:

≤ Wmax.

(cid:13)
(cid:13)
(cid:13)wπ/D

(cid:13)
(cid:13)
(cid:13)∞

Assumption 4 (Characterization of f ) The function f is convex with domain R and
continuous derivative f 0. The convex (Fenchel) conjugate of f is f∗, and f∗ is closed and
strictly convex. The derivative f 0

∗ is continuous and its range is a superset of [0, Wmax].
1−γ ] where C = Rmax + |α| · f 0(Wmax). N will

1−γ , C

For convenience, we deﬁne N := [− C
serve as the range for ν.

Our derivation begins with a formalization of the LP characterization of the Qπ-function
and its dual form:

Theorem 3 Given a policy π, the average return of π may be expressed in the primal
and dual forms as

min
ν:S×A→N
s.t.

JP(π, ν) := (1 − γ) Eµ0π [ν (s0, a0)]

ν (s, a) ≥ Bπν(s, a),
∀ (s, a) ∈ S × A,

(20)

and,

max
ρ:S×A→R+
s.t.

JD(π, ρ) := Eρ [r (s, a)]

ρ (s, a) = B>
∀ (s, a) ∈ S × A,

π ρ(s, a),

(21)

respectively. Under Assumptions 1 and 2, strong duality holds, i.e., JP(π, ν∗
for optimal solutions ν∗
reachable by π and the optimal dual ρ∗

π. The optimal primal satisﬁes ν∗
π is dπ.

π) = JD(π, ρ∗
π)
π (s, a) = Qπ (s, a) for all (s, a)

π, ρ∗

Consider the Lagrangian of JP, which would typically be expressed with a sum (or integral)
of constraints weighted by ρ. We can reparametrize the dual variable as ζ (s, a) = ρ(s,a)
dD(s,a)
to express the Lagrangian as,
max
ζ:S×A→R+

(1 − γ) E(s0,a0)∼µ0π [ν (s0, a0)] + E(s,a)∼dD [ζ (s, a) (Bπν − ν) (s, a)] .(22)
min
ν:S×A→N
The optimal ζ ∗
π of this Lagrangian is wπ/D, and this optimal solution is not aﬀected by
expanding the allowable range of ζ to all of R. However, in practice, the linear structure
in (22) can induce numerical instability. Therefore, inspired by the augmented Lagrangian

2 When γ ∈ [0, 1), B>

discrete case, the assumption reduces to requiring that B>
more involved; see Meyn and Tweedie (2012); Levin and Peres (2017) for a detailed discussion.

π has a unique ﬁxed point regardless of the underlying MDP. For γ = 1, in the
π be ergodic. The continuous case for γ = 1 is

8

method, we introduce regularization. By adding a special regularizer α · EdD [f (ζ (s, a))]
using convex f , we obtain

min
ν:S×A→N

max
ζ:S×A→R

L (ν, ζ; π) := (1 − γ) E(s0,a0∼µ0π [ν (s0, a0)] +

E(s,a)∼dD [ζ (s, a) (Bπν − ν) (s, a)] − α · E(s,a)∼dD [f (ζ (s, a))] .

(23)
We characterize the optimizers ν∗
π; π) of this
objective in the following theorem. Interestingly, although the regularization can aﬀect
π, the optimal dual solution ζ ∗
the optimal primal solution ν∗

π and the optimum value L(ν∗

π is unchanged.

π and ζ ∗

π, ζ ∗

Theorem 4 Under Assumptions 1–4, the solution to (23) is given by,

π (s, a) = −αf 0 (cid:16)
ν∗
ζ ∗
π (s, a) = wπ/D (s, a) .

(cid:17)
wπ/D (s, a)

+ Bπν∗

π (s, a) ,

π, ζ ∗

The optimal value is L (ν∗

π; π) = Edπ [r(s, a)] − αDf (dπkdD).
Thus, we have recovered the Fenchel AlgaeDICE objective for π, given in Equa-
tion 15. Furthermore, one may reverse the Legendre transform, f∗((Bπν − ν)(s, a)/α) =
1
maxζ
α (Bπν − ν)(s, a) · ζ − f (ζ), to recover the Primal AlgaeDICE objective in Equa-
tion (13).

The derivation of this same result from the LP perspective allows us to exploit strong
duality. Speciﬁcally, under the assumption that wπ/D and r are bounded, (ν∗
π) does not
change if we optimize L (ν, ζ; π) over a bounded space H × F, as long as (ν∗
π) ∈ H × F.
In this case, strong duality holds (Ekeland and Temam, 1999, Proposition 2.1), and we
obtain

π, ζ ∗
π, ζ ∗

min
ν∈H

max
ζ∈F

L (ν, ζ; π) = max
ζ∈F

min
ν∈H

L (ν, ζ; π) .

This implies that, for computational eﬃciency, we can optimize the policy via
(1 − γ) E(s0,a0)∼µ0π [ν (s0, a0)] +

max
π

‘ (π) := max
ζ∈F

min
ν∈H

EdD [ζ (s, a) (Bπν − ν) (s, a)] − α · EdD [f (ζ (s, a))] .

(24)

Remark (extensions to γ = 1 or α = 0): Although AlgaeDICE is originally derived
for γ ∈ [0, 1) and α > 0 in Section 3, the Lagrangian view of the LP formulation of Qπ can
be used to generalize the algorithm to γ = 1 and α = 0. In particular, for α = 0, one can
directly use the original Lagrangian for the LP. For the case γ = 1, the problem reduces
to the Lagrangian of the LP for an undiscounted Qπ-function; details are delegated to
Appendix A.

Remark (oﬀ-policy evaluation): The LP form of the Q-values leading to the La-
grangian (22) can be directly used for behavior-agnostic oﬀ-policy evaluation (OPE). In
fact, existing estimators for OPE in the behavior-agnostic setting which typically reduce
the OPE problem to estimation of quantities wπ/D (e.g., DualDICE (Nachum et al., 2019b)
and GenDICE (Zhang et al., 2020)) can be recast as special cases by introducing diﬀerent
regularizations to the Lagrangian. As we have shown, the solution to the Lagrangian

9

provides both (regularized) Q-values and the desired state-action corrections wπ/D as
primal and dual variables simultaneously.

5 Related Work

Algorithmically, our proposed method follows a Lagrangian primal-dual view of the LP
characterization of the Q-function, which leads to a saddle-point problem. Several recent
works (e.g., Chen and Wang, 2016; Wang, 2017; Dai et al., 2018a,b; Chen et al., 2018; Lee
and He, 2018) also considered saddle-point formulations for policy improvement, derived
from fundamentally diﬀerent perspectives. In particular, Dai et al. (2018a) exploit a saddle-
point formulation for the multi-step (path) conditions on the consistency between optimal
value function and policy. Other works (Chen and Wang, 2016; Wang, 2017; Dai et al.,
2018b; Chen et al., 2018) consider the (augmented) Lagrangian of the LP characterization
of Bellman optimality for the optimal V -function, which is slightly diﬀerent from the LP
characterization with respect to the optimal Q-function we consider. Although slight,
the diﬀerence between the V - and Q-LPs is crucial to enable behavior-agnostic policy
optimization in AlgaeDICE. If one were to follow derivations similar to AlgaeDICE but
for the V -function LP, some form of explicit importance weighting (and thus knowledge of
the behavior policy) would be required, as in recent work on oﬀ-policy estimation (Tang
et al., 2019; Uehara and Jiang, 2019). We further note that the application of a regularizer
on the dual variable to yield Primal AlgaeDICE is key to transforming the Lagrangian
optimization over values and state-action occupancies — typical in these previous works —
to an optimization over values and policies, which is more common in practice and can
help generalization (e.g., Swaminathan and Joachims, 2015).

The regularization we employ is inspired by previous uses of regularization in RL. Adding
regularization to MDPs (Neu et al., 2017; Geist et al., 2019) has been investigated for
many diﬀerent purposes in the literature, including exploration (de Farias and Van Roy,
2000; Haarnoja et al., 2017, 2018), smoothing (Dai et al., 2018b), avoiding premature
convergence (Nachum et al., 2017a), ensuring tractability (Todorov, 2006), and mitigating
observation noise (Rubin et al., 2012; Fox et al., 2016). We note that the regularization
employed by AlgaeDICE as a divergence over state-action densities is markedly diﬀerent
from these previous works, which mostly regularize only the action distributions of a policy
conditioned on state. An approach more similar to ours is given by Belousov and Peters
(2017), which regularizes the max-return objective using an f -divergence over state-action
densities. Their derivations are similar in spirit to ours, using the method of Lagrange
multipliers, but their result is distinct in a number of key characteristics. First, their
objective (analogous to ours in (12)) includes not only policy and values but also a number
of additional functions, complicating any practical implementation. Second, their results
are restricted to conservative regularization (α > 0), whereas our ﬁndings extend to both
exploratory regularization and unregularized objectives (α ≤ 0). Third, the algorithm
proposed by Belousov and Peters (2017) follows a bi-level optimization, in which the policy

10

is learned using a separate and distinct objective. In contrast, our proposed AlgaeDICE
uses a single, uniﬁed objective for both policy and value learning.

Lastly, there are a number of works which (like ours) perform policy gradient on oﬀ-policy
data via distribution correction. The key diﬀerentiator is in how the distribution corrections
are computed. One common method is to re-weight oﬀ-policy samples by considering
eligibility traces (Precup et al., 2000; Geist and Scherrer, 2014), i.e., compute weights
by taking the product of per-action importance weights over a trajectory. Thus, these
methods can suﬀer from high variance as the length of trajectory increases, known as the
“curse of horizon” (Liu et al., 2018). A more recent work (Liu et al., 2019) attempts to
weight updates by estimated state-action distribution corrections. This is more in line with
our proposed AlgaeDICE, which implicitly estimates these quantities. One key diﬀerence
is that this previous work explicitly estimates these corrections, which results in a bi-level
optimization, as opposed to our more appealing uniﬁed objective. It is also important to
note that both eligibility trace methods and the technique outlined in Liu et al. (2019)
require knowledge of the behavior policy. In contrast, AlgaeDICE is a behavior-agnostic
oﬀ-policy policy gradient method, which may be more relevant in practice. Compared to
existing behavior-agnostic oﬀ-policy estimators (Nachum et al., 2019b; Zhang et al., 2020),
this work considers the substantially more challenging problem of policy optimization.

6 Experiments

We present empirical evaluations of AlgaeDICE, ﬁrst in a tabular setting using the Four
Rooms domain (Sutton et al., 1999) and then on a suite of continuous control benchmarks
using MuJoCo (Todorov et al., 2012) and OpenAI Gym (Brockman et al., 2016).

6.1 Four Rooms

We begin by considering the tabular setting given by the Four Rooms environment (Sutton
et al., 1999), in which an agent must navigate to a target location within a gridworld. In
this tabular setting, we evaluate Primal AlgaeDICE (Equations 12 and 13) with f (x) = 1
2x2.
This way, for any π, the dual value function ν may be solved exactly using standard matrix
operations. Thus, we train π by iteratively solving for ν via matrix operations and then
taking a gradient step for π. We collect an oﬀ-policy dataset by running a uniformly
random policy for 500 trajectories, where each trajectory is initalized at a random state
and is of length 10. This dataset is kept ﬁxed, placing us in the completely oﬄine regime.
We use α = 0.01 and γ = 0.97.

Graphical depictions of learned policies π and dual value functions ν are presented in
Figure 1, where each plot shows π and ν during the ﬁrst, fourth, seventh, and tenth
iterations of training. The opacity of each square is determined by the Bellman residuals
of ν at that state. Recall that the Bellman residuals of the optimal ν∗
π are the density

11

Figure 1: We provide a pictoral representation of learned policies π and dual variables ν
during training of AlgaeDICE on the Four Rooms domain (Sutton et al., 1999). The agent
is initialized at the state denoted by an orange square and receives zero reward everywhere
except at the target state, denoted by a green square. We use a ﬁxed oﬄine experience data
distribution that is near-uniform. The progression of learned π and ν during training is
shown from left to right. The policy π is presented via arrows for each action at each square,
with the opacity of the arrow determined by the probability π(a|s). The dual variables
ν are presented via their Bellman residuals: the opacity of each square is determined by
the sum of the Bellman residuals at that state P
a(Bπν − ν)(s, a). Recall that for any π,
the Bellman residuals of the optimal ν∗
π)(s, a) = wπ/D(s, a). As
expected, we see that in the beginning of training the residuals are high near the initial
state, while towards the end of training the residuals show the preferred trajectories of the
near-optimal policy.

π should satisfy (Bπν∗

π − ν∗

ratios wπ/D. We see that this is reﬂected in the learned ν. At the beginning of training,
the residuals are high around the initial state. As training progresses, there is a clear path
(or paths) of high-residual states going from initial to target state. Thus we see that ν
learns to properly correct for distribution shifts in the oﬀ-policy experience distributions.
The algorithm successfully learns to optimize a policy using these corrected gradients, as
shown by the arrows denoting preferred actions of the learned policy.

We further provide quantitative results in Figure 2. We plot the average per-step reward
of AlgaeDICE compared to actor-critic in both online and oﬄine settings. As a point
of comparison, the behavior policy used to collect data for the oﬄine setting achieves
average reward of 0.03. Although all the variants are able to signiﬁcantly improve upon
this baseline, we see that AlgaeDICE performance is only negligibly aﬀected by the type of
dataset, while performance of actor-critic degrades in the oﬄine regime. See Appendix B
for experimental details.

6.2 Continuous Control

We now present results of AlgaeDICE on a set of continuous control benchmarks using
MuJoCo (Todorov et al., 2012) and OpenAI Gym (Brockman et al., 2016). We evaluate
the performance of Primal AlgaeDICE with f (x) = 1
2x2. Our empirical objective is thus

12

given by

J(π, ν) := 2α(1 − γ) · Es0∼U ,a0∼π(s0)[ν(s0, a0)] + E(s,a,r,s0)∼D,a0∼π(s0)[δν,π(s, a, r, s0, a0)2],

where δν,π is a single-sample estimate of the Bellman residual:

δν,π(s, a, r, s0, a0) := r + γν(s0, a0) − ν(s, a).
(25)
We note that using a single-sample estimate for the Bellman residual in general leads
to biased gradients, although previous works have found this to not have a signiﬁcant
practical eﬀect in these domains (Kostrikov et al., 2019). We make the following additional
practical modiﬁcations:

• As entropy regularization has been shown to be important on these tasks (Nachum
et al., 2017b; Haarnoja et al., 2018), we augment the rewards with a causal entropy
term; i.e., replace r in (25) with r − τ log π(a0|s0), where τ is learned adaptively as
in Haarnoja et al. (2018).

• As residual learning is known to be hard in function approximation settings (Baird,
1995), we replace ν(s0, a0) in (25) with a mixture η · ν(s0, a0) + (1 − η) · ν(s0, a0) where
ν(s0, a0) is a target value calculated as in Haarnoja et al. (2018). We use η = 0.05.

• If ν is fully optimized, δ will be the density ratio wπ/D, and thus always non-negative.
However, during optimization, this may not always hold, which can aﬀect policy
learning. Thus, when calculating gradients of this objective with respect to π, we
clip the value of δ from below at 0.

For training we parameterize π and ν using neural networks and perform alternating
stochastic gradient descent on their parameters.

We present our results in Figure 3. We see that Al-
gaeDICE can perform well in these settings, achiev-
ing performance that is roughly competitive with
the state-of-the-art SAC and TD3 algorithms. There
are potentially more possible improvements to these
practical results by choosing f (or f∗) appropriately.
In Appendix C, we conduct a preliminary investiga-
tion into polynomial f , showing that certain polyno-
mials can at times provide better performance than
f (x) = 1
2x2. A more detailed and systematic study
of this and other design choices for implementing Al-
gaeDICE is an interesting avenue for future work.

7 Conclusion

Figure 2: Average per-step reward
of policies on Four Rooms learned
by AlgaeDICE compared to actor-
critic (AC) over training iterations.

We have introduced an ALgorithm for policy Gradient from Arbitrary Experience via
DICE, or AlgaeDICE, for behavior-agnostic, oﬀ-policy policy improvement in reinforcement

13

HalfCheetah

Hopper

Walker2d

Ant

Humanoid

Figure 3: We show the results of AlgaeDICE compared to SAC (Haarnoja et al., 2018),
TD3 (Fujimoto et al., 2018), and DDPG (Lillicrap et al., 2015). We follow the evaluation
protocol of Fujimoto et al. (2018), plotting the performance of 10 randomly seeded training
runs, with shaded region representing half a standard deviation and x-axis given by
environment steps. There are potentially better results achievable by using a choice of f
other than f (x) = 1
2x2 for AlgaeDICE; see Appendix C for a preliminary investigation.

learning. Based on a linear programming characterization of the Q-function, we derived
the new approach from a Lagrangian saddle-point formulation. The resulting algorithm,
AlgaeDICE, automatically compensates for the distribution shift in collected oﬀ-policy
data, and achieves an estimate of the on-policy policy gradient using this oﬀ-policy data.

Acknowledgments

We thank Marc Bellemare, Nicolas Le Roux, George Tucker, Rishabh Agarwal, Dibya
Ghosh, and the rest of the Google Brain team for insightful thoughts and discussions.

References

Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al.
Learning dexterous in-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.

14

András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with
Bellman-residual minimization based ﬁtted policy iteration and a single sample path.
Machine Learning, 71(1):89–129, 2008.

Leemon Baird. Residual algorithms: Reinforcement learning with function approximation.

In Machine Learning Proceedings 1995, pages 30–37. Elsevier, 1995.

Boris Belousov and Jan Peters.

f-divergence constrained policy improvement. arXiv

preprint arXiv:1801.00056, 2017.

D. P. Bertsekas. Nonlinear Programming. Athena Scientiﬁc, Belmont, MA, second edition,

1999.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie
Tang, and Wojciech Zaremba. OpenAI gym. arXiv preprint arXiv:1606.01540, 2016.

Yichen Chen and Mengdi Wang. Stochastic primal-dual methods and sample complexity

of reinforcement learning. arXiv preprint arXiv:1612.02516, 2016.

Yichen Chen, Lihong Li, and Mengdi Wang. Scalable bilinear π learning using state and

action features. arXiv preprint arXiv:1804.10328, 2018.

Bo Dai, Niao He, Yunpeng Pan, Byron Boots, and Le Song. Learning from conditional

distributions via dual embeddings. CoRR, abs/1607.04579, 2016.

Bo Dai, Albert Shaw, Niao He, Lihong Li, and Le Song. Boosting the actor with dual

critic. ICLR, 2018a. arXiv:1712.10282.

Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song.
SBEED: Convergent reinforcement learning with nonlinear function approximation. In
Proceedings of the Thirty-Fifth International Conference on Machine Learning (ICML),
pages 1133–1142, 2018b.

Daniela Pucci de Farias and Benjamin Van Roy. On the existence of ﬁxed points for
approximate value iteration and temporal-diﬀerence learning. Journal of Optimization
Theory and Applications, 105(3):589–608, 2000.

Thomas Degris, Martha White, and Richard S Sutton. Oﬀ-policy actor-critic. arXiv

preprint arXiv:1205.4839, 2012.

Ivar Ekeland and Roger Temam. Convex analysis and variational problems, volume 28.

Siam, 1999.

Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvári, and Shie Mannor.
Regularized policy iteration with nonparametric function spaces. Journal of Machine
Learning Research, 17(130):1–66, 2016.

Yihao Feng, Lihong Li, and Qiang Liu. A kernel loss for solving the Bellman equation. In

Advances in Neural Information Processing Systems 32 (NeurIPS), 2019.

15

Roy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via

soft updates. In UAI, 2016.

Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation

error in actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.

Jianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to Conversational AI.

Foundations and Trends in Information Retrieval, 13(2–3):127–298, 2019.

M. Geist and B. Scherrer. Oﬀ-policy learning with eligibility traces: A survey. The Journal

of Machine Learning Research, 15(1):289–333, 2014.

Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov

decision processes. arXiv preprint arXiv:1901.11275, 2019.

A. Gruslys, M. Azar, M. Bellemare, and R. Munos. The reactor: A sample-eﬃcient

actor-critic architecture. arXiv preprint arXiv:1704.04651, 2017.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning

with deep energy-based policies. arXiv preprint arXiv:1702.08165, 2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic:
Oﬀ-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv
preprint arXiv:1801.01290, 2018.

Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will
Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Com-
bining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference
on Artiﬁcial Intelligence, 2018.

Vijay R. Konda and John N. Tsitsiklis. Actor-critic algorithms. In Advances in Neural

Information Processing Systems 12 (NIPS), pages 1008–1014, 2000.

Ilya Kostrikov, Oﬁr Nachum, and Jonathan Tompson. Imitation learning via oﬀ-policy

distribution matching. 2019.

Donghwan Lee and Niao He.

Stochastic primal-dual q-learning.

arXiv preprint

arXiv:1810.08298, 2018.

David A Levin and Yuval Peres. Markov chains and mixing times, volume 107. American

Mathematical Soc., 2017.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval
Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement
learning. arXiv preprint arXiv:1509.02971, 2015.

Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon:
Inﬁnite-horizon oﬀ-policy estimation. In Advances in Neural Information Processing
Systems, pages 5356–5366, 2018.

16

Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Oﬀ-policy policy
gradient with state distribution correction. In Proceedings of the Thirty-Fifth Conference
on Uncertainty in Artiﬁcial Intelligence (UAI), 2019.

Tyler Lu, Dale Schuurmans, and Craig Boutilier. Non-delusional Q-learning and value-
iteration. In Advances in Neural Information Processing Systems, pages 9949–9959,
2018.

Sean P Meyn and Richard L Tweedie. Markov chains and stochastic stability. Springer

Science & Business Media, 2012.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou,
Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602, 2013.

R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare. Safe and eﬃcient oﬀ-policy
reinforcement learning. In Advances in Neural Information Processing Systems, pages
1054–1062, 2016.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap
between value and policy based reinforcement learning. arXiv preprint arXiv:1702.08892,
2017a.

Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-PCL: An
oﬀ-policy trust region method for continuous control. arXiv preprint arXiv:1707.01891,
2017b.

Oﬁr Nachum, Mohammad Norouzi, George Tucker, and Dale Schuurmans. Smoothed
action value functions for learning Gaussian policies. arXiv preprint arXiv:1803.02348,
2018.

Oﬁr Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent ma-
nipulation via locomotion using hierarchical sim2real. arXiv preprint arXiv:1908.05224,
2019a.

Oﬁr Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-agnostic
estimation of discounted stationary distribution corrections. In Advances in Neural
Information Processing Systems 32 (NeurIPS), 2019b.

Gergely Neu, Anders Jonsson, and Vicenç Gómez. A uniﬁed view of entropy-regularized

markov decision processes. arXiv preprint arXiv:1705.07798, 2017.

XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence
functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on
Information Theory, 56(11):5847–5861, 2010.

17

D. Precup, R. Sutton, and S. Singh. Eligibility traces for oﬀ-policy policy evaluation. In
Proceedings of the 17th International Conference on Machine Learning, pages 759–766,
2000.

Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Program-

ming. John Wiley & Sons, Inc., 1994.

Jonathan Rubin, Ohad Shamir, and Naftali Tishby. Trading value and information in

MDPs. Decision Making with Imperfect Decision Makers, pages 57–74, 2012.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal

policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Ried-
miller. Deterministic policy gradient algorithms. In Proceedings of the 31st International
Conference on Machine Learning (ICML), pages 387–395, 2014.

Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A
framework for temporal abstraction in reinforcement learning. Artiﬁcial intelligence,
112(1-2):181–211, 1999.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy
gradient methods for reinforcement learning with function approximation. In Advances
in neural information processing systems, pages 1057–1063, 2000.

Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. Journal of Machine Learning Research, 16(1):
1731–1755, 2015.

Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu. Doubly robust bias
reduction in inﬁnite horizon oﬀ-policy estimation. arXiv preprint arXiv:1910.07186,
2019.

Emanuel Todorov. Linearly-solvable Markov decision problems. In NIPS, pages 1369–1376,

2006.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International

control.
Conference on, pages 5026–5033. IEEE, 2012.

Masatoshi Uehara and Nan Jiang. Minimax weight and Q-function learning for oﬀ-policy

evaluation. arXiv preprint arXiv:1910.12809, 2019.

Mengdi Wang. Randomized Linear Programming Solves the Discounted Markov Decision

Problem In Nearly-Linear Running Time. ArXiv e-prints, 2017.

Tao Wang, Daniel Lizotte, Michael Bowling, and Dale Schuurmans. Dual representations

for dynamic programming. 2008.

18

Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray
Kavukcuoglu, and Nando de Freitas. Sample eﬃcient actor-critic with experience
replay. arXiv preprint arXiv:1611.01224, 2016.

Ruiyi Zhang, Bo Dai, Li Lihong, and Dale Schuurmans. GenDICE: Generalized oﬄine

estimation of stationary values, 2020. Preprint.

19

min
ν:S×A→N
s.t.

and,

max
ρ:S×A→R+
s.t.

Appendix

A Proof Details

We follow the notations in main text. Abusing notation slightly, we will use P and R
interchangeably.

Theorem 3 Given a policy π, the average return of π may be expressed in the primal
and dual forms as

JP(π, ν) := (1 − γ) Eµ0π [ν (s0, a0)]

JD(π, ρ) := Eρ [r (s, a)]

(20)

ν (s, a) ≥ Bπν(s, a),
∀ (s, a) ∈ S × A,

ρ (s, a) = B>
∀ (s, a) ∈ S × A,
respectively. Under Assumptions 1 and 2, strong duality holds, i.e., JP(π, ν∗
for optimal solutions ν∗
reachable by π and the optimal dual ρ∗

π. The optimal primal satisﬁes ν∗
π is dπ.

π, ρ∗

π) = JD(π, ρ∗
π)
π (s, a) = Qπ (s, a) for all (s, a)

π ρ(s, a),

(21)

Proof Recall that Bπ is monotonic; that is, given two bounded functions ν1 and ν2,
ν1 ≥ ν2 implies Bπν1 ≥ Bπν2. Therefore, for any feasbile ν, we have ν ≥ (Bπ) ν ≥
(Bπ)2 ν ≥ (Bπ)3 ν ≥ . . . ≥ (Bπ)∞ ν = Qπ, proving the ﬁrst claim.

The duality of the linear program (20) can be obtained as

max
ρ:S×A→R+
s.t.

Eρ [r (s, a)]
ρ (s0, a0) = γ X

π (a0|s0) T (s0|s, a) ρ (s, a) + (1 − γ) µ0 (s0) π (a0|s0)
,

s,a

|

∀ (s0, a0) ∈ S × A ,

{z
π ρ(s0,a0)

B>

}

which is exactly (21). Notice that the equality constraints correspond to a system of linear
equations of dimension |S| × |A|: (I − γ (Pπ)>)ρ = (1 − γ)(µ0π), where (µ0π)(s, a) =
µ0(s)π(a|s), Pπ (s0, a0|s, a) = π (a0|s0) T (s0|s, a), and I is the identity matrix. Since the
matrix I − γ (Pπ)> is nonsingular, the system has a unique solution given by
(cid:16)

ρ∗ = (1 − γ)

I − γ (Pπ)>(cid:17)−1

(µ0π) .
I − γ (Pπ)>(cid:17)−1

= P∞

t=0 γt (Pπ)t,

so

Finally, when γ ∈ [0, 1), we can rewrite
t=0 γt (Pπ)t (µ0π) = dπ, as desired.
ρ∗ = (1 − γ) P∞

(cid:16)

20

Theorem 4 Under Assumptions 1–4, the solution to (23) is given by,

π (s, a) = −αf 0 (cid:16)
ν∗
ζ ∗
π (s, a) = wπ/D (s, a) .

(cid:17)
wπ/D (s, a)

+ Bπν∗

π (s, a) ,

The optimal value is L (ν∗

π, ζ ∗

π; π) = Edπ [r(s, a)] − αDf (dπkdD).

Proof By Fenchel duality, we have

max
ζ:S×A→R

EdD [ζ (s, a) (Bπν − ν) (s, a)] − α EdD [f (ζ (s, a))]

= α EdD
Plugging this into (23), we have

(cid:20)
f∗

(cid:18) 1
α

(Bπν − ν) (s, a)

(cid:19)(cid:21)

.

L(ν, ζ ∗

π; π) = min

ν:S×A→N

(1 − γ) Eµ0π [ν (s0, a0)] + αEdD

(cid:20)
f∗

(cid:18) 1
α

(Bπν − ν) (s, a)

(cid:19)(cid:21)

.

(26)

To investigate the optimality, we apply the change-of-variable, x (s, a) := 1
Let βt (s) = P
(1 − γ)Eµ0π[ν(s0, a0)]

s = st|s0 ∼ µ0, {ai}t

i=0 ∼ π

(cid:16)

(cid:17)

, and consider the ﬁrst expectation in (26):

α (Bπν − ν) (s, a).

= (1 − γ)

∞
X

γtEs∼βt,a∼π(s)[ν(s, a)] − (1 − γ)

∞
X

γt+1Es0∼βt+1,a0∼π(s0)[ν(s0, a0)]

t=0

t=0
= E(s,a)∼dπ [ν(s, a) − γEs0∼T (s,a),a0∼π(s0)[ν(s0, a0)]]
= E(s,a)∼dπ [r(s, a)] + E(s,a)∼dπ [ν(s, a) − r(s, a) − γEs0∼T (s,a),a0∼π(s0)[ν(s0, a0)]]
= E(s,a)∼dπ [r(s, a)] − αE(s,a)∼dπ [x(s, a)].

Let C denote the set of functions x in the image of (Bπν − ν) for ν : S ×A → N . Therefore,
the change of variables yields the following re-formulation of L:

L(ν∗

π, ζ ∗

π; π) = min
x∈C

E(s,a)∼dπ [r (s, a)] − αE(s,a)∼dπ [x (s, a)] + αEdD [f∗ (x (s, a))]

= E(s,a)∼dπ [r(s, a)] − α

(cid:18)

max
x∈C

Edπ [x(s, a)] − EdD [f∗ (x (s, a))]
∗(x∗

(cid:19)

π satisﬁes f 0
π(s, a)) =
(f∗)0i−1
(·) exists, and equals f 0(·). Thus, we
π(s, a) = f 0(wπ/D(s, a)) for all s, a. Due to the Assumption 3 that wπ/D is bounded,
π ∈ C. Therefore, by deﬁnition of the

Note that, ignoring the restriction of x to C (for now), the optimal x∗
wπ/D(s, a). By Assumption 4, we have that
have x∗
we have that x∗
f -divergence, we have

π is bounded by f 0(Wmax) and thus x∗

h

L(ν∗

π, ζ ∗

π; π) = E(s,a)∼dπ [r(s, a)] − αDf (dπ||dD),

as desired.

To characterize ν∗

π, we note,

x∗ (s, a) = f 0(wπ/D(s, a)) ⇒ ν∗

π (s, a) = Bπν∗

π(s, a) − αf 0(wπ/D(s, a)).

(27)

(28)

To characterize the optimal dual ζ ∗
ζ · x∗

ζ ∗
π (s, a) = argmax

π (s, a), we have
π(s, a) − f (ζ) = f 0

∗(x∗

π(s, a)) = wπ/D(s, a)

ζ

where the second equality comes from the fact that f 0(ζ ∗
∗(x∗
f 0

π(s, a)).

π(s, a)) = x∗

π(s, a) ⇒ ζ ∗

π(s, a) =

21

A.1 Extension to γ = 1

We follow similar steps in the previous section, and extend the analysis to the undiscounted
case. Diﬀerent from the discounted case, the primal variable has an extra scalar component,
denoted λ. Under Assumption 2, the Markov chain induced by π is ergodic, with a unique
invariant distribution dπ and mixing time Tmix. The mixing time quantiﬁes the number of
steps for the state distribution in the induced Markov chain to be close to dπ, measured
by total variation (Levin and Peres, 2017). Precisely,

(

)

Tmix := min

t :

sup
(s,a)∈S×A

kδs,aT t

π − dπkTV ≤ 1/4

,

where δs,a is the delta measure concentrated on the state-action pair (s, a), and
Tπ (s0, a0|s, a) := T (s0|s, a) π (a0|s0). The range of the primal variables ν is changed to
N := [−CTmix, CTmix] where C = 2Rmax + |α| · (f 0(Wmax) − f 0(0)).

We begin with the Q-LP characterization of Qπ-values and visitations dπ.

Theorem 5 Under Assumption 2, the average return of π may be expressed in the primal
and dual forms as

min
λ∈R,ν:S×A→N
s.t.

JP(π, ν, λ) := λ

ν (s, a) ≥ −λ + Bπν(s, a),
(29)

and,

∀ (s, a) ∈ S × A,

max
ρ:S×A→R+
s.t.

JD(π, ρ) := Eρ [r (s, a)]

π ρ(s, a),

ρ (s, a) = B>
∀ (s, a) ∈ S × A,
X ρ (s, a) = 1 ,

(30)

respectively. Under further Assumption 1, strong duality holds, i.e., JP(π, λ∗
π, ρ∗
π, ν∗
JD(π, ρ∗
π). As in the discounted case, we have ρ∗
Unlike the discounted case, there are inﬁnitely many solutions for ν∗
solution for ν remains optimal with a constant oﬀset.

π, ν∗
π) =
π = dπ.
π, as any optimal

π) for optimal solutions (λ∗

Given these LP formulations, the Primal and Fenchel AlgaeDICE optimization problems
for γ = 1 are given by
min
λ∈R,ν:S×A→N

JD,f (π, λ, ν) := λ + α · E(s,a)∼dD[f∗((−λ + Bπν(s, a) − ν(s, a))/α)], (31)

max
π
and

max
π

min
λ∈R,ν:S×A→N

max
ζ:S×A→R

L (λ, ν, ζ; π) := λ+

E(s,a)∼dD [ζ(s, a)(−λ + Bπν(s, a) − ν(s, a))] − α · E(s,a)∼dD [f (ζ (s, a))] ,

(32)
respectively. We will show in Theorem 6 that the optimal ζ ∗ in Fenchel AlgaeDICE
automatically satisﬁes the constraint ζ ∈ Z := {ζ ≥ 0, EdD [ζ (s, a)] = 1}, avoiding the
nontrivial self-normalization step in Liu et al. (2018). In fact, by comparing Fenchel Al-
gaeDICE with GenDICE (Zhang et al., 2020), the GenDICE objective with unit penalty

22

weight could be understood as primal variables regularized Lagrangian of Q-LP, while the
Fenchel AlgaeDICE is derived by regularizing the dual variable in the Lagrangian of Q-LP.

We have the following analogue to Theorem 4.

Theorem 6 Under Assumptions 1–4, the solution to (32) is given by,
π(s, a),

(cid:17)
wπ/D (s, a)

+ Bπν∗

π − αf 0 (cid:16)
π (s, a) = −λ∗
ν∗
ζ ∗
π (s, a) = wπ/D (s, a) .

The optimal value is L (λ∗

π, ν∗

π, ζ ∗

π; π) = Edπ [r(s, a)] − αDf (dπkdD).

Proof By Fenchel duality, we have

max
ζ:S×A→R

EdD [ζ (s, a) (−λ + Bπν(s, a) − ν(s, a))] − αEdD [f (ζ (s, a))]

Plugging this into (32), we have

= αEdD

(cid:20)
f∗

(cid:18) 1
α

(−λ + Bπν(s, a) − ν(s, a))

(cid:19)(cid:21)

.

(33)

π; π) = λ + α EdD
To investigate the optimality, we apply the change-of-variable,

L(λ, ν, ζ ∗

(−λ + Bπν(s, a) − ν(s, a))

(cid:20)
f∗

(cid:18) 1
α

(cid:19)(cid:21)

.

(34)

We have,

x (s, a) :=

1
α

(−λ + Bπν(s, a) − ν(s, a)) .

Edπ [x(s, a)] = α−1Edπ [−λ + Bπν(s, a) − ν(s, a)]

= α−1Edπ [(Pπν − ν) (s, a) + r (s, a) − λ]
= α−1Edπ [r (s, a) − λ] ,

(35)

where the last equality holds because
Edπ [(Pπν − ν) (s, a)] = Edπ [Pπν (s, a)] − Edπ [ν (s, a)] = Edπ [ν (s, a)] − Edπ [ν (s, a)] = 0 .
Therefore,

L(λ, ν, ζ ∗

π; π) = λ + Edπ [r (s, a) − λ] − Edπ [r (s, a) − λ]
(cid:18) 1
α

(−λ + Bπν(s, a) − ν(s, a))

+ αEdD

(cid:20)
f∗

(cid:19)(cid:21)

= Edπ [r (s, a)] − αEdπ [x(s, a)] + αEdD [f∗ (x (s, a))] .

Let C denote the set of functions x in the image of −λ + (Bπν − ν) for ν : S × A → N .
Therefore, the change of variables yields the following re-formulation of L:

L(λ∗

π, ν∗

π, ζ ∗

π; π) = min
x∈C

E(s,a)∼dπ [r (s, a)] − αE(s,a)∼dπ [x (s, a)] + αEdD [f∗ (x (s, a))]

= E(s,a)∼dπ [r(s, a)] − α

(cid:18)

max
x∈C

Edπ [x(s, a)] − EdD [f∗ (x (s, a))]

(cid:19)

.

Note that, ignoring the restriction of x to C (for now), the optimal x∗
wπ/D(s, a). By Assumption 4, we have that
Thus, we have x∗
is bounded, we have that x∗
deﬁnition of the f -divergence, we have

π satisﬁes f 0
∗(x(s, a)) =
(·) exists, and it is given by f 0(·).
π(s, a) = f 0(wπ/D(s, a)) for all s, a. Due to the Assumption 3 that wπ/D
π ∈ C. Therefore, by

π is bounded by f 0(Wmax) and thus x∗

(f∗)0i−1

h

L(λ∗

π, ν∗

π, ζ ∗

π; π) = E(s,a)∼dπ [r(s, a)] − αDf (dπ||dD),

(36)

23

as desired.

To characterize ν∗

π, we note,

x∗ (s, a) = f 0(wπ/D(s, a)) ⇒ ν∗

π (s, a) = −λ∗

π + Bπν∗

π(s, a) − αf 0(wπ/D(s, a)).

(37)

To characterize the optimal dual ζ ∗
ζ · x∗

ζ ∗
π (s, a) = argmax

π (s, a), we have
π(s, a) − f (ζ) = f 0

∗(x∗

π(s, a)) = wπ/D(s, a)

ζ

π(s, a)).

where the second equality comes from the fact that f 0(ζ ∗
∗(x∗
f 0
The ﬁnal step is to show λ∗
π whose
image is in N . As discussed, the x∗ (s, a) = f 0(wπ/D(s, a)) is bounded by f 0 (Wmax), then,
we may use (35) to characterize λ∗

π is bounded, and there exists an optimal solution ν∗

π(s, a)) = x∗

π(s, a) ⇒ ζ ∗

π(s, a) =

π as

π = Edπ [r (s, a)] − αEdπ [x∗(s, a)] ⇒ |λ| ≤ C < ∞.
λ∗
π, and let (ˆs, ˆa) = argmax ν∗(s, a) and (¯s, ¯a) =
We now consider an optimal solution ν∗
argmin ν∗(s, a).3 As the set of ν∗
π is oﬀset-invariant, we assume without loss of gener-
ality that ν∗(¯s, ¯a) = 0. Consider a trajectory starting from (ˆs, ˆa) and controlled by π,
(ˆs0, ˆa0, ˆs1, ˆa1, . . .), on which repeated applications of Bπ yield

π(ˆs, ˆa) = Eˆs0=ˆs,ˆa0=ˆa,π
ν∗



X

(cid:16)

r(ˆst, ˆat) − αf 0(wπ/D(ˆst, ˆat)) − λ∗
π

(cid:17)

+ ν∗


π(ˆsTmix, ˆaTmix)
 .



t<Tmix

We may obtain a similar recurrence for a trajectory, (¯s0, ¯a0, ¯s1, ¯a1, . . .), starting from (¯s, ¯a).
Subtracting them on both sides, we have



π(ˆs, ˆa) − ν∗
ν∗

π(¯s, ¯a) = Eˆs0=ˆs,ˆa0=ˆa,π



X

(cid:16)

(cid:17)
r(ˆst, ˆat) − αf 0(wπ/D(ˆst, ˆat))

−E¯s0=¯s,¯a0=¯a,π





t<Tmix

X

(cid:16)

(cid:17)
r(¯st, ¯at) − αf 0(wπ/D(¯st, ¯at))





t<Tmix

+ (Eˆs0=ˆs,ˆa0=ˆa,π [ν∗

π(ˆsTmix, ˆaTmix)] − E¯s0=¯s,¯a0=¯a,π [ν∗

π(¯sTmix, ¯aTmix)]) .





(38)
π, we have

Consider the last term above. By the deﬁnition of Tmix and nonnegativity of ν∗

Eˆs0=ˆs,ˆa0=ˆa,π [ν∗
≤ kδˆs,ˆaT Tmix
≤ (kδˆs,ˆaT Tmix
Plugging the above in (38) and realizing ν∗

π − δ¯s,¯aT Tmix
π − dπkTV + kδ¯s,¯aT Tmix

π(ˆsTmix, ˆaTmix)] − E¯s0=¯s,¯a0=¯a,π [ν∗
kTV ν∗

max
π − dπkTV)ν∗
π(¯s, ¯a) = 0, we obtain

π

π(¯sTmix, ¯aTmix)]

max = ν∗

max/2 .

max/2 ≤ Tmix(2Rmax + |α|(f 0(Wmax) − f 0(0))) = CTmix .
ν∗
Hence, ν∗ is in the range [0, 2CTmix]. Then, by the oﬀset-invariance property, we can shift
ν∗ by the constant −CTmix, so that its range is now in N = [−CTmix, CTmix].

3The analysis here can be adapted to the case where max ν∗ and inf ν∗ are replaced by sup ν∗ and

inf ν∗, respectively.

24

B Experiment Details

For the Four Rooms quantitative results, we used 100 trajectories of length 100. For
online training, these trajectories were sampled anew during each iteration. For oﬄine
training, the trajectories were sampled once, using the behavior policy for the GridWalk
environment from Nachum et al. (2019b), and this dataset was ﬁxed throughout training.
We implemented actor-critic analogous to the tabular version of AlgaeDICE. At each
training iteration, we ﬁrst solve for Qπ in closed form using standard matrix operations,
and subsequently we take one gradient step for π to maximize Es∼D[P
a π(a|s)Qπ(s, a)].
The discount factor was 0.99.

The table below gives hyperparameters used in the continuous control experiments. Many
of our settings for AlgaeDICE were taken from Haarnoja et al. (2018) and Fujimoto et al.
(2018). We set the regularization coeﬃcient α in AlgaeDICE to 0.01.

Hyperparameter
Policy network size
Critic network size
Actor learning rate
Critic learning rate
Batch size
Critic updates per time step
Critic updates per actor update

DDPG
400-300
400-300
10−3
10−3
256
1
1

TD3
400-300
400-300
10−3
10−3
100
1
2

AlgaeDICE
256-256
256-256
10−3
10−3
256
1
2

SAC
256-256
256-256
10−3
10−3
256
1
2

C Additional Results

Figure 4: Results on the Four Rooms domain with other initial states.

25

HalfCheetah

Hopper

Walker2d

Ant

Humanoid

Figure 5: We show the results of AlgaeDICE over the choice of function f∗(x) = 1
p |x|p.
In AlgaeDICE runs, we use α = 1 and perform 4 training steps per environment step (as
opposed to α = 0.01 and 1 training step per environment step used in the main text). We
see that diﬀerent values of p lead to slightly diﬀerent results. Interestingly, we ﬁnd p = 1.5
to typically perform the best, similar to the ﬁndings in Nachum et al. (2019b).

26

