2
2
0
2

p
e
S
2
1

]

G
L
.
s
c
[

3
v
4
0
7
8
0
.
6
0
1
2
:
v
i
X
r
a

Memorization and Generalization in Neural Code Intelligence
Models

Md Rafiqul Islam Rabin
mrabin@uh.edu
University of Houston
Houston, TX, USA

Mohammad Amin Alipour
maalipou@central.uh.edu
University of Houston
Houston, TX, USA

Aftab Hussain
ahussain27@uh.edu
University of Houston
Houston, TX, USA

Vincent J. Hellendoorn
vhellendoorn@cmu.edu
Carnegie Mellon University
Pittsburgh, PA, USA

ABSTRACT
Context: Deep Neural Networks (DNNs) are increasingly being
used in software engineering and code intelligence tasks. These
are powerful tools that are capable of learning highly generalizable
patterns from large datasets through millions of parameters. At the
same time, their large capacity can render them prone to memoriz-
ing data points. Recent work suggests that the memorization risk
manifests especially strongly when the training dataset is noisy,
involving many ambiguous or questionable samples, and memo-
rization is the only recourse.
Objective: The goal of this paper is to evaluate and compare the
extent of memorization and generalization in neural code intelli-
gence models. It aims to provide insights on how memorization may
impact the learning behavior of neural models in code intelligence
systems.
Method: To observe the extent of memorization in models, we
add random noise to the original training dataset and use various
metrics to quantify the impact of noise on various aspects of train-
ing and testing. We evaluate several state-of-the-art neural code
intelligence models and benchmarks based on Java, Python, and
Ruby codebases.
Results: Our results highlight important risks: millions of trainable
parameters allow the neural networks to memorize anything, in-
cluding noisy data, and provide a false sense of generalization. We
observed all models manifest some forms of memorization. This can
be potentially troublesome in most code intelligence tasks where
they rely on rather noise-prone and repetitive data sources, such
as code from GitHub.
Conclusion: To the best of our knowledge, we provide the first
study to quantify memorization effects in the domain of software
engineering and code intelligence systems. This work raises aware-
ness and provides new insights into important issues of training
neural models in code intelligence systems that are usually over-
looked by software engineering researchers.

KEYWORDS
machine learning, software engineering, memorization and gener-
alization, empirical results, models of code

1 INTRODUCTION
Data-driven program analysis approaches have been increasingly
used in software engineering tasks such as bug detection [17, 66],

method name prediction [2, 5], code comment generation [19, 31]
and many more [4, 25, 72]. These approaches predominantly use
deep neural networks to extract useful patterns and insights about
programs from a large corpus of code. Neural networks thrive at this
because they are high-capacity universal approximators, capable
of expressing any hypothesis class through their layered archi-
tectures [38]. Yet, while they can match or even exceed humansâ€™
performance in tasks ranging from board games [21] to image recog-
nition [36], it is notoriously unclear what insights they extract from
training data. Their tremendous capacity â€“ now spanning many
billions of trainable parameters â€“ allows neural networks to both
learn many generalizable patterns and simply memorize myriad
training samples [8, 77].

Memorization is a significant, and non-obvious threat to training
deep learners, perhaps especially so for models trained on software
engineering data. Source code from the open-source ecosystem is
exceptionally repetitive [12, 20, 29], as well as particularly noisy
[35, 48, 60]. Both factors encourage memorization, which is directly
adverse to the ability of neural models to generalize [8]. Indeed,
deep learners are easily led astray by factors like code duplication
[1], and often biased towards superficial features such as common
variable names [15, 75]. As a consequence, they are vulnerable to
adversarial examples in which a small, often semantics-preserving,
transformation radically changes the modelsâ€™ predictions [54, 58,
75].

In this work, we perform a large-scale study of memorization and
generalization in training neural code intelligence models through
a large-scale study that follows the framework of pioneering work
in computer vision by Arpit et al. [8] and Zhang et al. [77]. To
do so, we introduce noise in programming datasets (i.e., Figure 2)
where we add various degrees of noise into popular existing
datasets by randomly altering target labels or input programs, and
then observe the impact on a range of characteristics of training
(e.g., the spread of loss values). By studying the resulting trends
broadly, across six types of models, three programming languages,
six datasets, and five noise levels each, we can gain insight into
the impact of memorization artifacts across datasets and models
uniquely proposed in software engineering and code intelligence
systems. We contribute the following observations:
1. Memorization is a significant concern in software en-
gineering: models with excessive parameter capacity easily
memorize large datasets. When trained with real versus nonsensical

 
 
 
 
 
 
Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

labels (the latter necessitating memorization), they yielded training
curves that were virtually indistinguishable.
+ A positive: the use of noise as a contrast to clean data helped
us distinguish memorization from generalization even when test
(or held-out) performance did not, providing a promising new
indicator for choosing the right parameter budget when modeling
source code.
2. Noise-driven memorization inhibits confident learning:
even small amounts of noise-induced memorization substantially
altered the distribution of scores assigned by models, towards
more uniform probabilities and conservative predictions, which
undermines their usefulness for ranking and high-precision
settings.
+ A positive: our metrics consistently distinguished between
generalization and memorization-prone training, supporting the
use of this analysis to detect memorization-related problems early.
3. Training curves do not betray the degree of noise: all
models traced similarly shaped loss distributions during training,
regardless of memorization. As such, we cannot tell how much
memorization is involved even in learning the original datasets.
Previous work suggests that this is a non-trivial amount [8, 77].
Quantifying this is a key challenge: our work reinforces that even
small amounts of forced memorization can have a major impact on
model accuracy.
+ A positive: across multiple models and datasets, a bigger increase
in the spread of loss over the course of training correlated well with
both reduced noise and better performing models. This may be
a fruitful metric for gauging model and dataset generalization
potential.

To the best of our knowledge, this work is the first to empiri-
cally study memorization and generalization phenomena of neural
code intelligence models. Our findings provide quantitative evi-
dence of memorization and a rich suite of new metrics to assess the
generalization potential of models and datasets used in software
engineering research.

Artifacts. The evaluation scripts and detailed results will be pub-
licly available at https://github.com/UH-SERG/CI-Memorization.

2 BACKGROUND
High capacity neural networks, with many millions or even billions
of trainable parameters, are capable of memorizing large volumes
of high-dimensional data. They are apparently more prone to doing
so when finding generalizable patterns in the data is challenging [8].
This problem commonly occurs when the training data contains
excessive repetition or noise, and causes a discrepancy between
performance while training and at inference time. Previous studies
have shown that datasets of code are extremely repetitive. Lopes
et al. [44] found that 70% of source code files on the GitHub had
clones of previously created files. Gabel and Su [20] found that code
fragments, especially large ones, are repetitive and likely to reoccur
in other programs. Allamanis [1] showed that almost all datasets
used to train neural code intelligence models contain levels of code
duplication of 20% or more, and this repetition spuriously inflated
their performance. Rabin et al. [55, 56] show that models often
take shortcuts and heavily rely on superficial features for making

predictions. Several studies have shown that neural code intelli-
gence models are vulnerable to small, often semantic-preserving,
transformation [52, 58, 70], suffer from generalizability and robust-
ness [32, 54, 75], and may rely on few tokens [55, 56, 64] or merely
structures [53, 57]. A few works also survey the taxonomy of ex-
isting models or methods for source code [3, 62] and provide a
comprehensive review to categorize, investigate, and recommend
on applications and challenges [37, 78].

A large body of work in ML literature has tried to evaluate the
tension between memorization and generalization. Zhang et al. [77]
was perhaps the first study that showed that random data can be fit
perfectly with deep neural networks. They introduced noise into
the training dataset of an image classification task and observed
that the neural model could easily learn the noisy data. The results
underscored that DNNs have the potential to employ a high degree
of memorization in learning datasets. For an Alexnet-style CNN on
CIFAR10 dataset, they found that the difference between the relative
convergence times to fit training data without label corruption and
to do the same with data with the maximum label corruption is quite
small. This observation was corroborated by similar experiments
by Arpit et al. [8] on the CIFAR10 and MNIST image datasets; who
found that models achieved optimal training accuracy within just
about 50 epochs of training, for any amount of noise. However,
each modelâ€™s initial training progress did correspond to how clean
the dataset is.

Zhang et al. [77]â€™s observations ignited intense empirical and
theoretical research in evaluation and characterization of memo-
rization in neural networks. Morcos et al. [47] showed that neural
networks that generalize from the same data tend to converge to
similar representations, whereas networks that memorize do not.
Recht et al. [61] evaluated the generalization of ImageNet networks
on new test data and found that the models do not generalize re-
liably. They found that, in addition to overfitting, preprocessing
mechanisms for data cleaning can pose challenges in downstream
training. Chen et al. [14] further quantified the generalization per-
formance of deep neural networks with noisy labels. They provided
an iterative noisy cross-validation approach for identifying cor-
rect labels from noisy datasets and adopted a co-teaching strategy
to train robust models against noisy labels. Hacohen et al. [26]
observed that different neural models memorize data in different
orders. In contrast, when training a DNN with real data, differ-
ent DNNs with similar architectures learn the data in the same
order. They observed this behavior on several image classification
benchmarks and one text classification benchmark. Zhang et al.
[76] studied the interplay between memorization and generaliza-
tion in deep neural networks by focusing on an identity-mapping
task. They demonstrated that over-parameterized neural networks
such as deep CNNs often memorize training labels by learning a
constant function, while shallow neural networks like single-layer
linear networks fail to either generalize the identity function or
memorize a constant function. Northcutt et al. [50] identified erro-
neously labeled data in the test sets of commonly-used benchmarks
such as MNIST, CIFAR-10, CIFAR-100, ImageNet, etc. They found
that lower-capacity models may be practically more useful than
higher-capacity models when the dataset contains enormous noisy
labels.

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

Much of the most closely related work comes from the domain of
language modeling, as modern language models are often trained
with very large datasets and parameters budgets [9, 74], and may
leak information in unexpected ways when released to the public
[11, 18]. Carlini et al. [11] investigated the unintended memoriza-
tion in neural networks via an exposure-based testing strategy,
where they insert secret word sequences in training data and mea-
sure the exposure of secrets at inference time. They showed that a
generative sequence model trained on sensitive data can actually
memorize secret sequences from its training data, e.g., credit card
numbers. Elangovan et al. [18] examined the impact of data leak-
age in publicly available datasets by assessing the overlap of similar
instances between train and test sets on the modelâ€™s performance
for shared tasks [69]. They found that models that memorize from
overlapping data may have higher performance than models that
are more robust in generalization, however, such models may not
provide an effective performance in real-world unseen scenarios.
Recently, Tirumala et al. [65] analyzed the effects of dataset size
and model size on the training dynamics of large language models.
They demonstrated that larger models memorize training data faster
even before overfitting and memorize unique identifiers quickly in
the training set. They concluded by suggesting memorization as a
critical metric when scaling the size of language models.

We argue that a similar comprehensive analysis of noise, memo-
rization, and generalization is needed for neural code intelligence
models. We base our methodology on the above observations, and
especially on the suite of metrics proposed in computer vision
[8, 77].

3 STUDY SUBJECTS
In this section, we provide a brief description of the design and
implementation aspects of different models, tasks, and datasets
used in our study. We study six neural code intelligence models
(Code2Vec, Code2Seq, Transformer, GGNN, Great,
and
CodeBERT) for four well-known tasks (method name prediction,
variable misuse, code document generation, and natural language
code search) across datasets of three programming languages (Java,
Python, and Ruby).

The method name prediction [2, 5] and variable misuse localization-
repair tasks [4, 66] have been heavily studies of generalizability,
adversarial examples, and transparency of neural code intelligence
models [15, 32, 52, 54â€“58, 64, 70, 71, 75]. For these two tasks, we
choose models and datasets, shown in Table 1, for which the train-
ing scripts and data are readily available [6, 7, 28], to which we
only make minor modifications, such as logging the number of
parameters and/or changing the batch size.

3.1 Method Name Prediction (MethodName)
3.1.1 Task. In the method name prediction task, the model at-
tempts to predict the name of a method from its body. This task has
several applications, such as code search [42], code summarization
[5], and reasoning about code analogies [7]. This task has been used
as the downstream task to evaluate several state-of-the-art neural
code intelligence models [6, 7].

3.1.2 Data. We use the following three datasets for the Method-
Name task:

â€¢ Java-Top10: This dataset [57] contains 1, 000 randomly
selected samples for each of the ten most frequent labels
from the Java-Large dataset in [6]. Those are: equals,
main, setUp, onCreate, toString, run, hashCode, init,
execute, and get.

â€¢ Java-Small: This dataset [6] contains nine Java projects for
training, one Java project for validation, and one Java project
for testing. In total, it contains about 700K methods.

â€¢ Java-Med: This dataset [6] contains 800 Java projects for
training, 100 Java projects for validation, and 100 Java
projects for testing. In total, it contains about 4M methods.

3.1.3 Models. We study two commonly used models for the
MethodName task: Code2Vec [7], and Code2Seq [6]. The models
are similar, in that they rely on extracting â€œpathsâ€ from the
methodâ€™s abstract syntax tree (AST) that connect one terminal
or token to another. These paths, mapped to vector embeddings,
are enumerated exhaustively and used by the models in different
ways. Since these paths consolidate both lexical and syntactic
information, the models tend to outperform strictly token-based
models.

In Code2Vec [7], each path, along with its source and desti-
nation terminals, is mapped into a vector embedding, which is
learned together with other network parameters. Then, the sepa-
rate vectors obtained from each path-context are concatenated into
a single context vector using a fully-connected layer. Additionally,
the model learns an attention vector that is used to aggregate the
path-context representations into a single code vector that repre-
sents a method body. Finally, given a method bodyâ€™s code vector,
the model computes the probability of each target method name
using a softmax-normalization between the code vector and each
of the embeddings of target method names.

In Code2Seq [6], a bi-directional LSTM encoder is used to rep-
resent paths instead, which encodes paths node-by-node while
splitting tokens into sub-tokens. The decoder similarly uses at-
tention to select relevant paths while decoding, but now predicts
sub-tokens of a target sequence one-by-one to generate a method
name.

3.2 Variable Misuse (VarMisuse)
3.2.1 Task. A variable misuse occurs when a different, but also
declared and correctly typed, variable is used than intended [4].
These bugs are common in software development and are usually
by-products of code copy-pasting [33]. In the variable misuse lo-
calization and repair task, the model should both locate the misuse
bug and then propose a repair in the form of the correct identifier
to use [28, 66].

3.2.2 Data. We study memorization on this task in the context
of Py150-Great [28], which is derived from the ETH Py150
dataset [59]. In this synthetic dataset, all top-level function
definitions were extracted from open-source projects. For each
sample, buggy samples were generated by randomly replacing one
variable with another based on declared variables, generating up
to three random samples per function. Any sample thus contains

Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

Model (Dataset)
Code2Vec (Java-Top10)
Code2Vec (Java-Small)
Code2Vec (Java-Med)
Code2Seq (Java-Top10)
Code2Seq (Java-Small)
Code2Seq (Java-Med)
GGNN (Py150-Great)
Transformer (Py150-Great)
Great (Py150-Great)
CodeBERT (CodeSearch - Ruby)
CodeBERT (Code-to-Text - Ruby)

#Trainable Parameters
62,528,256
256,082,176
367,681,920
3,826,368
16,128,448
37,411,328
41,190,914
26,215,938
26,225,538
124,647,170
172,503,552
Table 1: Number of trainable parameters for each model including
vocabulary.

a function definition as a token sequence and a â€œhas bugâ€ flag.
Buggy samples, in addition, have error-related data in the form of
pointers into the token sequence (e.g. error location, set of repair
targets). The training and validation sets span ca. 1.8 million and
185k samples, each set having a balanced number of buggy and
bug-free samples.

3.2.3 Models. We use three models for the VarMisuse task: Trans-
former [67], GGNN [4, 40] and GREAT [28]. The first is a widely
used, attention-based model in which the representation of tokens
are iteratively refined through all-to-all communication. The second
uses more targeted message passing, along expert-derived connec-
tions in the code such as data-flow and syntactic dependencies. The
final model combines both of these, using biased attention to make
the models both aware of important relations while still allowing
global communication. This model was shown to outperform both
others by a significant margin and represents the current state-of-
the-art on this task [28]. We include all models due to their unique
characteristics and generally strong performance.

For use in the VarMisuse task, all models follow Vasic et al.â€™s ap-
proach [66] that consists of: (1) stacking an initial token-embedding
layer, (2) computing a distributed representation of the code input
using a â€œcoreâ€ model (one of the above), and (3) generating two
pointers into the input code token sequence, one each for the lo-
calization and repair tasks. More specifically, given a buggy (or
correct) tokenized code sample the task is to predict two pointers
into the sequence of sample tokens: a pointer to the position of the
token that has the wrong variable (or a default token for correct
samples), and a pointer to the position of any token that contains
the correct variable (this pointer is ignored for correct samples).

In recent years, researchers have been increasingly using large,
pre-trained models that have been learned from datasets including
code and natural language [13, 49]. There are several benchmarks
[45, 69] that provide publicly available datasets in order to eval-
uate and compare the performance of different models on same
tasks. In this study, we experiment with BERT-style pre-trained
models [16], mainly CodeBERT [19], for natural language code
search and code document generation tasks. The CodeBERT is a
bimodal pre-trained model for programming languages (PL) and
natural languages (NL) such as Python, Java, Ruby, Document, etc.

It captures the semantic connection between NL and PL and pro-
duces general-purpose representations that can broadly support
various downstream NL-PL tasks [19]. It has been developed fol-
lowing the architecture of BERT [16] and RoBERTa [43], which
itself is based on the Transformer [67] that is used in most large
pre-trained models. The CodeXGLUE repository provides code and
data for fine-tuning CodeBERT on various datasets and tasks [45].

3.3 Code-to-Text Generation (Code-to-Text)
3.3.1 Task. The task is commonly known as code documentation
generation or code summarization where the objective is to gen-
erate natural language comments for a code snippet. It provides
a high-level summary of the functionality performed by the code
snippet. This task is also referred to as code-to-text, code-to-NL gen-
eration, and code-to-documentation [19, 45]. It can benefit software
maintenance, code understanding, and retrieval [31, 46, 68].

3.3.2 Data. We conduct experiments on the CodeSearchNet
dataset [30] for the Code-to-Text task, where the dataset has
already been cleaned1 by removing comments and removing
examples that cannot be parsed, contain limited/special tokens, or
are not English. We select the Ruby language data which contains
24, 927 samples for training, 1, 400 samples for development, and
1, 261 samples for testing. In each sample, the input is a code
snippet (i.e., function), and a natural language text (i.e., docstring)
that briefly describes the code is the output.

3.3.3 Model. The CodeXGLUE repository provides a pipeline1
for fine-tuning the CodeBERT model on the Code-to-Text task,
which is evaluated by the smoothed BLEU-4 score [41, 51]. The
architecture consists of the CodeBERT as the encoder and a 6-layer
Transformer as the decoder. It uses the Adam optimizer and cross-
entropy loss function to update the modelâ€™s parameters. We fine-
tune the CodeBERT model using the Ruby programming language
data of the cleaned CodeSearchNet dataset for the Code-to-Text
task.

3.4 Natural Language Code Search

(CodeSearch)

3.4.1 Task. In this task, given a natural language query, the target
is to find the most semantically relevant source code from a collec-
tion of candidates. The task is formulated as a binary classification
problem, where given a pair of query and code, a model aims to
classify whether the code is semantically related to the query or not
[19, 45]. It has been actively studied and applied in many software
development practices [23, 24, 30].

3.4.2 Data. We use the preprocessed dataset2 derived from the
original CodeSearchNet dataset [30] for the CodeSearch task,
where each sample includes a code snippet paired with a natu-
ral language query. The dataset consists of a balanced number of
positive and negative samples. Samples where the code is related
to the query are positive samples and are labeled as â€œ1â€. Contrary,
negative samples contains randomly replaced irrelevant code or
query and are labeled as â€œ0â€. We choose the Ruby language data

1https://github.com/microsoft/CodeXGLUE/tree/main/Code-Text/code-to-text
2https://github.com/microsoft/CodeBERT/tree/master/CodeBERT/codesearch

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

Figure 2 shows the examples of output noise and input noise for
different tasks. Concretely, for source code, we randomly introduce
noise into the training set, as follows:

â€¢ Figure 2ğ‘âˆ’ğ‘ shows an example of the MethodName task,
where noise is added to randomly selected samples over all
training samples. We add output noise by replacing the target
label (method name) with another randomly chosen different
label from the set of available method names. We add input
noise in two ways: (a) by deleting a random statement from
the method body, and (b) by replacing a variable name in
the method body with the method name.

â€¢ Figure 2ğ‘‘âˆ’ğ‘“ shows an example of the VarMisuse task, where
noise is added to the same percentage of randomly selected
buggy and correct training samples. We add output noise
to a buggy sample by simply labeling it as correct, where
we set the error location to 0 and clear the repair targets.
Conversely, a correct sample is changed to buggy by labeling
it as such and assigning both a random location as the error
location and a set of randomly chosen variables as repair
targets. =Towards adding input noise to a buggy sample,
we replace all occurrences of the repair target token with
â€œTARGETâ€ and replace the token at the error location with
â€œBUGGYâ€ in the input code. Conversely, we add input noise
to a correct sample by replacing all occurrences of the most
frequent token in the input code with â€œNONBUGGYâ€.

â€¢ Figure 2ğ‘”âˆ’ğ‘– shows an example of the Code-to-Text task,
where noise is added to the randomly selected samples over
all training samples. We add output noise by replacing the
target docstring (i.e., comments) with another randomly cho-
sen different docstring from the set of all available docstrings.
We add input noise by replacing the tokens of the input code
snippets that appear in the target docstring with the â€œMASKâ€
token.

â€¢ Figure 2ğ‘—âˆ’ğ‘™ shows an example of the CodeSearch task,
where noise is added to the same percentage of randomly
selected positive and negative training samples. We add
output noise to a positive (resp. negative) sample by simply
labeling it as negative (resp. positive). We add input noise by
replacing the most frequent tokens of input code snippets
and target docstring with the â€œPOSITIVEâ€ (resp. â€œNEGATIVEâ€)
token for positive (resp. negative) samples.

4.2 Training the Models
For training the MethodName models (Code2Vec and Code2Seq),
we follow the exact training configurations from prior work [6, 7],
only using a slightly smaller batch size due to memory limitations.
We use a batch size of 128 for Java-Top10 and 256 for Java-Small
and Java-Med. We train the models on a NVIDIA Tesla-P100 GPU
(with 12GB of memory) up to 50 epochs for Java-Top10 and Java-
Small but 20 epochs for Java-Med because of the time budget. To
train the VarMisuse models (Transformer, GGNN, and GREAT), we
use the default base configurations from Hellendoorn et al. [28]â€™s
public replication package 3. Training proceeds in â€œstepsâ€ of 250k
training samples, after each of which performance is assessed on
25k held-out samples. A full epoch is completed roughly every 7

3https://github.com/VHellendoorn/ICLR20-Great

Figure 1: Workflow of our approach for evaluating memorization
and generalization.

which contains 97, 580 samples for training, 4, 417 samples for
development, and 2, 279 samples for testing.

3.4.3 Model. We run the implementation of the CodeBERT model
provided by the authors in their GitHub repository2. It uses the
Adam optimizer and binary classification loss function to update
the modelâ€™s parameters. We fine-tune the CodeBERT model using
the Ruby programming language data of the preprocessed Code-
SearchNet dataset for the CodeSearch task.

4 METHODOLOGY
This section describes the methodology we use to study memoriza-
tion in neural code intelligence models.

4.1 Characterizing Memorization
We follow and adapt the methodology used in computer vision
literature [8, 77]. The ultimate objective of a trained model is to
find patterns consistent in the dataset that helps it to generalize on
unseen data. If the dataset contains random noise, it is quite difficult
for the model to find effective patterns. Therefore, for a model to
learn on a noisy dataset, i.e., randomized labels, it needs to memorize
data points. By tuning the degree of such noise and comparing
learning patterns between the original dataset and various noisy
datasets, we can characterize memorization artifacts of a model
on a specific dataset. And by identifying consistent trends across
models and/or datasets, we can derive more general conclusions
about memorization artifacts in neural models of source code.

Figure 1 depicts a high-level view of the workflow in the pro-
posed methodology. Given the original training dataset, the ap-
proach creates several noisy training datasets by noising a portion
of the data in two ways: 1) output noise where we add noise into
target labels, and 2) input noise where we add noise into input pro-
grams. We create multiple training datasets with {0%, 25%, 50%, 75%,
100%}-noise, where 0%-noise denotes the original training dataset,
and 100%-noise denotes a dataset where where all examples in
the training set are fully noisified with output noise or input noise.
We use each noisy training set to train models, and compare and
contrast the training characteristics of each noisy model on the
original test set for MethodName, held-out set for VarMisuse, and
development set for CodeBERT.

Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

Figure 2: Examples of original samples, output noise and input noise for source code in different tasks.

such steps and continued for 50 epochs on a NVIDIA Tesla-P100
GPU with 12GB of memory and a NVIDIA RTX-3090 GPU with
24GB of memory. Finally, we fine-tune the CodeBERT model up
to 50 epochs using the CodeXGLUE [45] pipeline on two NVIDIA
RTX-3090 GPUs (with 24GB of memory each) for Code-to-Text
and CodeSearch tasks. We mostly use the default configurations,
only adjusting the batch size to 64, and the maximum length of

input and output sequences to 256 and 64, respectively, due to
memory limitations.

4.3 Metrics
We collect a wide range of metrics [8, 77] to characterize training
in the datasets. We describe those metrics in the rest of this section.

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

4.3.1 Predicted Score. We use â€œPredicted scoreâ€ to refer to the
(probability) score assigned to a predicted output by the model.
Depending on the model, we compute it differently. In Code2Vec,
the model computes the probability of the target name via a softmax-
normalization operation between the code vector of a given method
body and the embeddings of all possible method names.

P (method_nameğ‘– ) =

exp (ğ‘ğ‘œğ‘‘ğ‘’_ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘‡ Â· ğ‘›ğ‘ğ‘šğ‘’_ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘– )
(cid:205)method_nameğ‘— âˆˆ all_name exp (ğ‘ğ‘œğ‘‘ğ‘’_ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿğ‘‡ Â· ğ‘›ğ‘ğ‘šğ‘’_ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘”ğ‘— )
In Code2Seq, when predicting the method name, the model makes
predictions for each sub-token of a target sequence at each step.
Hence, we compute an average score for a prediction as follows:

Pğ‘ğ‘£ğ‘” (method_nameğ‘– ) =

(cid:205)ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ method_nameğ‘– P (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— )
|ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ method_nameğ‘– |

For VarMisuse, the models emit two logits per token (logit0 for lo-
calization and logit1 for repair), which are converted to probabilities
via the softmax operation.

Pğ‘™ğ‘œğ‘ (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘– ) =

Pğ‘Ÿğ‘’ğ‘ (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘– ) =

exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡0,ğ‘– )
(cid:205)ğ‘— exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡0,ğ‘— )

âˆ‘ï¸

{ğ‘– |ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘– âˆˆ repair_candidates}

exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡1,ğ‘– )
(cid:205)ğ‘— exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡1,ğ‘— )

Here, the location probability is simply computed per token,
while the repair probability is aggregated across all occurrences of
each declared variable (the â€œrepair candidateâ€ set). For bug-free sam-
ples, the location component should point to token 0, and the repair
predictions are ignored. For buggy samples, the location probability
of the incorrect variable use (var-use) should be maximized by Pğ‘™ğ‘œğ‘ ,
as should the sum of the repair probabilities of all occurrences of
the correct variable.

In CodeSearch, the CodeBERT model emits two logits for two
target labels ({0, 1}), which are converted to probabilities via the
softmax operation.

P (ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘– ) =

exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘– )
exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡0) + exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡1)

In Code-to-Text, when generating docstring, the CodeBERT
model makes predictions for each token as a sequence. Therefore,
we compute an average score for a single sample as follows:

Pğ‘ğ‘£ğ‘” (docstringğ‘– ) =

(cid:205)ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ docstringğ‘– P (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— )
|ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ docstringğ‘– |

F1-Score and Accuracy. We use the evaluation metric, F1-
4.3.2
Score over sub-tokens, as commonly used in the literature [6, 7] for
MethodName task. The F1-Score is a harmonic mean of precision
(ğ‘ƒ) and recall (ğ‘…).

ğ¹1â€“ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’ =

2
ğ‘ƒ âˆ’1 + ğ‘…âˆ’1
In VarMisuse models, we use the two accuracy metrics: localization
accuracy and repair accuracy. Localization accuracy measures the

= 2 Â·

ğ‘ƒ Â· ğ‘…
ğ‘ƒ + ğ‘…

proportion of buggy samples for which the model correctly predicts
the location:

predicted_location = arg max

ğ‘–

Pğ‘™ğ‘œğ‘ (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›ğ‘– )

localization_accuracy =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(actual_locationğ‘– = predicted_locationğ‘– )

Repair accuracy captures the proportion of buggy samples for which
the model correctly assigns at least half the probability mass to
occurrences of the correct variable name:

repair_accuracy =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(

âˆ‘ï¸

Pğ‘Ÿğ‘’ğ‘ (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— ) â‰¥ 0.5)

{ ğ‘— |ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— =repair_targetğ‘– }

In CodeSearch, the accuracy is computed as the proportion of sam-
ples in the balanced set for which the CodeBERT model correctly
predicts the actual label.

predicted_label = arg max

ğ‘–

P (ğ‘™ğ‘ğ‘ğ‘’ğ‘™ğ‘– )

ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦ =

1
ğ‘›

ğ‘›
âˆ‘ï¸

ğ‘–=1

(actual_labelğ‘– = predicted_labelğ‘– )

In Code-to-Text, the CodeBERT model is evaluated with the
smoothed BLEU-4 score [41, 51] that adds one count to the ğ‘›-ğ‘”ğ‘Ÿğ‘ğ‘š
for ğ‘› > 1, which is suitable for short documents or where ğ‘›-ğ‘”ğ‘Ÿğ‘ğ‘š
may not overlap. It is computed as follows:

smoothed BLEU-4 = BP âˆ— exp(

1
4

4
âˆ‘ï¸

ğ‘›=1

log ğ‘ƒğ‘›)

where, ğµğ‘ƒ is the brevity penalty factor, and ğ‘ƒğ‘› is the modified
precision of ğ‘›-ğ‘”ğ‘Ÿğ‘ğ‘š after smoothing.

ğ‘ƒğ‘› =

(cid:205) ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ğ‘ğ‘™ğ‘–ğ‘ (ğ‘›-ğ‘”ğ‘Ÿğ‘ğ‘š) + 1 (ğ‘› > 1)
(cid:205) ğ¶ğ‘œğ‘¢ğ‘›ğ‘¡ (ğ‘›-ğ‘”ğ‘Ÿğ‘ğ‘š) + 1 (ğ‘› > 1)

BP = exp(min(0, 1 âˆ’

len (reference)
len (candidate)

))

Spread of Loss. The loss refers to the error in the modelâ€™s
4.3.3
prediction, typically tied to the probability it assigned to the ground-
truth label. In Code2Vec, the model computes cross-entropy loss
between the softmax of raw logits (predicted distribution ğ‘) and
the ground-truth targets (true distribution ğ‘). The true distribution
ğ‘ assigns a value of 1 to the actual name and 0 otherwise, so the
cross-entropy loss for a single input method is equivalent to the
negative log-likelihood of the actual name.

L (method_nameğ‘– ) = âˆ’

âˆ‘ï¸

ğ‘ (ğ‘›ğ‘ğ‘šğ‘’ ğ‘— ) ğ‘™ğ‘œğ‘” ğ‘(ğ‘›ğ‘ğ‘šğ‘’ ğ‘— )

ğ‘›ğ‘ğ‘šğ‘’ ğ‘— âˆˆ all_name
= âˆ’ ğ‘™ğ‘œğ‘” ğ‘(method_nameğ‘– )

In Code2Seq, the model makes predictions for each sub-token of a
target sequence at each step; hence, we compute an average loss
for a single input method as follows,

Lğ‘ğ‘£ğ‘” (method_nameğ‘– ) =

(cid:205)ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ method_nameğ‘– L (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— )
|ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ method_nameğ‘– |

In VarMisuse models, loss is based on the sum of the location loss
and repair loss, both calculated based on the negative log-likelihood

Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

of the target valueâ€™s probabilities (see above):

Lğ‘‰ ğ‘€ = Lğ‘™ğ‘œğ‘ + Lğ‘Ÿğ‘’ğ‘

= âˆ’ log Pğ‘™ğ‘œğ‘ (bug_location) âˆ’ log Pğ‘Ÿğ‘’ğ‘ (repair_target)

In CodeSearch, the CodeBERT model computes the binary cross-
entropy loss between the target value (ğ‘¦ âˆˆ {0, 1}) and predicted
probability (ğ‘) as follows,

L (ğ‘¦, ğ‘) = âˆ’(ğ‘¦ . ğ‘™ğ‘œğ‘”(ğ‘) + (1 âˆ’ ğ‘¦) . ğ‘™ğ‘œğ‘”(1 âˆ’ ğ‘))
In Code-to-Text, the CodeBERT model generates docstring as a
sequence of tokens; hence, we compute an average loss for a single
sample as follows,

Lğ‘ğ‘£ğ‘” (docstringğ‘– ) =

(cid:205)ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ docstringğ‘– L (ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— )
|ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› ğ‘— âˆˆ docstringğ‘– |

Gini coefficient. We measure the spread of loss by computing
the Gini coefficient [22] over training loss after each epoch as
training progress. A Gini coefficient of 0 means perfect equality
where all values are the same. On the other end of the spectrum, a
Gini coefficient of 1 means maximal inequality among values. The
Gini coefficient is computed as the relative mean absolute difference
of all pairs of items in the population. If Lğ‘– is the loss of an input
program ğ‘¡ğ‘– , and there are ğ‘› input programs, then the Gini coefficient
(ğº) is computed as follows:

(cid:205)ğ‘›

ğ‘–=1

ğº (ğ‘™ğ‘œğ‘ ğ‘ ) =

ğ‘—=1 |Lğ‘– âˆ’ L ğ‘— |

(cid:205)ğ‘›
2ğ‘› (cid:205)ğ‘›

ğ‘–=1 Lğ‘–

4.3.4 Critical Sample Ratio. We also estimate the complexity of
decision boundaries by computing the critical sample ratio (CSR)
[8] for the MethodName task. An input program is called a critical
sample if there exists at least one adversarial example4 in close
proximity (ğ›¿) of the input program. For a given test set ğ·ğ‘¡ , the
critical sample ratio (CSR) is measured as follows,

ğ¶ğ‘†ğ‘…ğ›¿ =

# ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘ğ‘ğ‘™_ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ ğ›¿
|ğ·ğ‘¡ |

A higher value of CSR (closer to 1) indicates a complex decision
boundary, where many samples are just a small transformation
away from being labeled differently, whereas a lower value of CSR
(closer to 0) indicates a simpler, more robust decision boundary.

We explore programs within single-transformation distance (ğ›¿ =
1) of a given input program for adversarial programs following
recent work [10, 54]. We check for an adversarial program within
the single transformation distance of a given input program ğ‘¡ğ‘– to
identify whether ğ‘¡ğ‘– is a critical sample. Specifically, we apply the
single-place variable renaming transformation [54] on the input
program ğ‘¡ğ‘– to generate candidate programs. The transformation
changes the name of a single variable in the input program to a
new name following the predefined format ğ‘£ğ‘ğ‘Ÿ [0 âˆ’ 9]+ (e.g. â€œğ‘£ğ‘ğ‘Ÿ 3â€).
The transformation is performed one-by-one on each variable in
the input program, creating a set of candidate programs within the
single transformation distance. Suppose, ğ‘‡ğ¶ğ‘–
is a set of candidate
programs generated within the single transformation distance of

4In machine learning, an adversarial example of an input in a model is a sample with
a slight, ideally imperceptible and/or irrelevant difference to the original input, that
misleads the model into providing a different prediction. In the method name prediction
task, this can be a semantically-equivalent and largely syntactically identical sample
program that leads to a different predicted method name.

ğ‘¡ğ‘– . Then, ğ‘¡ğ‘– is a critical sample if there exist at least one candidate
program ğ‘¡ ğ‘— âˆˆ ğ‘‡ğ¶ğ‘–
such that ğ‘€ (ğ‘¡ ğ‘— ) â‰  ğ‘€ (ğ‘¡ğ‘– ), where ğ‘€ (ğ‘¡) indicates
the predicted name of the program ğ‘¡ by model ğ‘€.

5 RESULTS
In this section, we present the results of our experiments, in which
we study the modelsâ€™ behavior under various rates of output noise
and input noise across four key metrics: modelsâ€™ performance (i.e.,
accuracy), distribution of prediction score, spread of training loss,
and critical sample ratio.

5.1 Analyses of Memorization with Output

Noise

Erroneously labeled data, typically known as mislabeled data, is
a common source of output noise in publicly available datasets
[14, 39, 50, 65]. In this section, we present the impact of adding
such output noise on the training of neural code intelligence models.

5.1.1 Memorization and Performance. We tracked modelsâ€™ predic-
tion in each epoch (or step) while training, after which we evaluate
the same on the test (or held-out) data. Figure 3 shows the result-
ing changes in F1-Score on the training set (solid line) and test set
(dashed line) at different noise levels for the MethodName task; Fig-
ure 4 shows the same for both the localization and repair accuracy
metrics on the VarMisuse task. Most models experience a signif-
icant impact on their training and test (or held-out) performance
with the introduction of noise. Some models show higher test (or
held-out) than training performance in the presence of noise; this
might be due to the absence of noise in the test (or held-out) data.
Remarkably, the training performance of the Code2Vec models
converges to nearly the same point in both the original and noisy
settings; most noisy curves follow a nearly identical trajectory. The
remaining Code2Seq models do not show this pattern on the same
dataset, struggles more with the memorization required by the noisy
samples and is unable to fit noisy training data perfectly. Similar to
the VarMisuse models, it instead experiences a consistent decline
in training and test performance with increased noise levels. The
VarMisuse models fail to learn bug localization almost completely
at noise levels of 75% and above â€“ instead, we observed that these
models converged to marking every sample as bug-free.

Reflecting back on Table 1, we find a likely explanation for the
discrepancy between Code2Vec and Code2Seq on the same dataset:
the former, used in its default configuration, has access to substan-
tially more parameters than the latter (10x or more). Evidently, this
more than suffices for memorizing previously seen training sam-
ples. This happens especially quickly on Java-Top10, the smallest
of these datasets. Typically, we like to detect such memorization
by contrasting training and test (or held-out) performance. How-
ever, considering the gap between training and test (or held-out)
performance across all sub-figures, it is not serving this purpose:
this gap does not follow any specific patterns in most cases. If one
had only access to a dataset with an unknown degree of noise, as
we normally do, knowing the test (or held-out) performance in
relation to the training results would thus be of little use. On the
contrary, explicitly contrasting the original data with a variant in
which some non-trivial degree of noise is introduced immediately
highlights the problem of a model with too many parameters.

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

(a) Code2Vec (Java-Top10)

(b) Code2Vec (Java-Small)

(c) Code2Vec (Java-Med)

(d) Code2Seq (Java-Top10)

(e) Code2Seq (Java-Small)

(f) Code2Seq (Java-Med)

Figure 3: F1-Score at different noise levels - solid is training, dashed is test (MethodName).

(a) Transformer (Localization)

(b) GGNN (Localization)

(c) Great (Localization)

(d) Transformer (Repair)

(e) GGNN (Repair)

(f) Great (Repair)

Figure 4: Localization and Repair accuracy - solid is training, dashed is held-out (VarMisuse).

11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Localization accuracyNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Localization accuracyNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Localization accuracyNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Repair accuracyNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Repair accuracyNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Repair accuracyNoise level:0%25%50%75%100%Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

(cid:11)

Observation 1: Increasing the degree of memorization gen-
erally causes a decrease in the training and generalization
performance, except in models with excessive capacity: their
training performance barely suffered, probably making this
a strong indicator of memorization over generalization.

(cid:10)

(cid:8)

(cid:9)

5.1.2 Memorization and Prediction Confidence. Some instances fit
highly predictive patterns better than others. As a result, easily
predicted samples will have (much) higher associated probability
scores than harder samples. This difference in scores should be
quite pronounced when comparing original data (some instances fit
patterns) with noisy data (fits independently) [8]. In order to observe
these behaviors in neural code intelligence models, we obtained
the probability assigned to the predicted label for all samples in
the test set (or held-out) after each epoch (or step). Figures 5 and 6
show the sorted average probability of the predicted label for all
samples considering all epochs (or steps) of training.

The results show that the distribution of prediction score changes
significantly in shape as the amount of noise increases. Whereas
models nearly follow a convex trend on their original dataset, in
which only a small number of samples are predicted with low con-
fidence, the introduction of noise changes this abruptly. Models
with even low rates of noise are far more likely to show a concave
distribution with mostly conservative, or even virtually uniform,
probabilities, such as at 100% noise in Figures 5e and 5f. Models
needing to memorize rarely expressed high confidence in any pre-
dictions. This is to be expected; highly confident mispredictions
would incur a very high loss penalty during training. This effect
is again particularly weak on the Code2Vec tasks, on which we
previously established ample capacity for memorizing.

In some cases, especially for localization probability in Fig-
ure 6ğ‘âˆ’ğ‘ , a phase transition occurs where models trained with very
high rates of noise regain some confidence on a subset of samples.
We hypothesize that this reflects cases where these models have
confidently memorized â€” which is perhaps easier to do in the
absence of the ambiguity that comes with having both noisy and
noise-free samples. Some other figures show a surprisingly small
gap between the 25% and 0% trends, among them, Figures 5c and 5e
from MethodName. This may signal some degree of noise already
present in the original dataset.

(cid:7)

(cid:4)

Observation 2: Even at low rates of memorization, the dis-
tribution of prediction scores assigned by the model changes
drastically, reflecting a decrease in confidence. This transi-
tion can be useful for detecting memorization characteristics.

(cid:6)

(cid:5)

5.1.3 Memorization and Loss Change. We now study the spread of
loss over the course of training and observe the difference between
noisy data and original data. To do so, we compute the Gini coeffi-
cient (see section 4.3.3) across training samples, which captures the
spread of the modelsâ€™ losses. We know that only a small number of
training samples have a high loss in the original data, while the loss
tends to be high for virtually all samples in noisy data. Therefore,
the spread of loss (computed with the Gini coefficient) ought to
be significantly higher (indicating less uniformity) in the original
data than in the noisy data [8]. Figures 7 and 8 show that this is
indeed the case across virtually all tasks and models, expect for the

Code2Vec models where basically all noise levels nearly yield the
same trends.

In most cases, Gini coefficient value decreases significantly and
rather consistently with an increasing noise level. The curves cor-
responding to various noise levels of the same model tend to follow
a similar shape across training epochs (or steps) that mainly differs
in the eventual spread of loss. This further reinforces that even
small degrees of memorization can significantly alter the modelâ€™s
training pattern, and shows that it already does so at the very start,
with no apparent change of trajectory over time. We also see a near-
universal trend in which the more a model increases its Gini coeffi-
cient, the better it performs. For instance, both the Transformer and
Great models of VarMisuse task eclipse the GGNN model in the
latter part of training; the latter shows a marked downward trend
in Gini coefficient (Figure 8) right as its accuracy decreases as well
(Figure 4). In the same way, the Code2Seq model of MethodName
task on the Java-Top10 dataset shows better performance than both
the Java-Small and Java-Med datasets (Figure 3); the former shows
an upward trend in the Gini coefficient (Figure 7). However, the
Code2Vec models defy this trend for previously identified reasons
of ample capacity for memorizing.

(cid:11)

Observation 3: A larger increase in the spread of loss dur-
ing training correlated well with both reduced memorization
and better performing models. This can be a fruitful metric
for gauging generalization potential, except for models that
prone to memorizing.

(cid:10)

(cid:8)

(cid:9)

5.1.4 Memorization and Decision Surface. In order to understand
how noise affects the complexity of the hypotheses learned by
neural code intelligence models on the MethodName task, we
examined the number of critical samples in the test set after each
epoch while training. A higher number of critical samples indicates
a more complex learned decision surface, likely making it more
susceptible to adversarial perturbations. In general, models with
more noisy datasets are expected to have higher critical sample
ratio (CSR) values [8]. Figure 9 shows the CSR obtained for all
samples (both correct and incorrect prediction) at different noise
levels. In Code2Vec, as expected, models trained with higher noise
manifested higher CSR. However, in Code2Seq, these echo this
expectation only on the smaller-balanced Java-Top10 dataset (Fig-
ure 9d), and we did not observe almost any such relation on the
Java-Small and Java-Med datasets (Figure 9e & Figure 9f); the
100%-noisy datasets is a clear outlier. Code2Vec mostly exhibits
significant CSR differences in varying noise levels, which consis-
tently suffers from a very high CSR for higher noise. Results on
Code2Seq only weakly support previous findings of Code2Vec,
thus deserving further investigation regarding the relation between
noise and CSR.

(cid:7)

Observation 4: The unusual and strongly divergent pat-
terns found across models and datasets suggest that further
analyses are necessary for using CSR metrics to quantify
training robustness in neural code intelligence models.

(cid:6)

(cid:4)

(cid:5)

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

(a) Code2Vec (Java-Top10)

(b) Code2Vec (Java-Small)

(c) Code2Vec (Java-Med)

(d) Code2Seq (Java-Top10)

(e) Code2Seq (Java-Small)

(f) Code2Seq (Java-Med)

Figure 5: Distribution of prediction score considering all epochs (MethodName).

(a) Transformer (Localization)

(b) GGNN (Localization)

(c) Great (Localization)

(d) Transformer (Repair)

(e) GGNN (Repair)

(f) Great (Repair)

Figure 6: Distribution of prediction score considering all steps (VarMisuse).

12000400060007100Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%110000200003000044426Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%1100000200000351627Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%12000400060007100Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%110000200003000044426Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%1100000200000351627Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%130006000900012619Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

(a) Code2Vec (Java-Top10)

(b) Code2Vec (Java-Small)

(c) Code2Vec (Java-Med)

(d) Code2Seq (Java-Top10)

(e) Code2Seq (Java-Small)

(f) Code2Seq (Java-Med)

Figure 7: Spread of training loss after each epoch (MethodName).

(a) Transformer (Localization)

(b) GGNN (Localization)

(c) Great (Localization)

(d) Transformer (Repair)

(e) GGNN (Repair)

(f) Great (Repair)

Figure 8: Spread of training loss after each step (VarMisuse).

11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

(a) Code2Vec (Java-Top10)

(b) Code2Vec (Java-Small)

(c) Code2Vec (Java-Med)

(d) Code2Seq (Java-Top10)

(e) Code2Seq (Java-Small)

(f) Code2Seq (Java-Med)

Figure 9: Critical Sample Ratio over all samples (MethodName).

5.2 Analyses of Memorization with Input Noise
Similar to the experiments with output noise in Section 5.1, we also
tracked the modelsâ€™ performance, distribution of prediction score,
and spread of training loss in each epoch (or step) while training
the models with input noise. Input noise is added in a variety of
ways that depend on the target task and dataset, as illustrated in
Figure 2.

Statement Deletion. Figures 10 and 11 show the impact of
5.2.1
deleting a random statement from the method body on Method-
Name task for Code2Vec and Code2Seq models with Java-Top10
dataset and Java-Small dataset, respectively. This impact is sig-
nificantly less pronounced than what was observed with output
noise in Section 5.1. In that analysis, models trained on the original
data demonstrated much higher performance than those trained on
noisy data: the latter generalized less well as noise somewhat forced
models to memorize data points. In contrast, in Figures 10 and 11,
models trained with this type of input noise converge to nearly
the same performance regardless of noise level. That these neural
models barely suffer from the omission of a random statement can
be explained by our previous findings [55, 56]: models typically
rely on very few tokens for making their predictions. Given that
methods often contain many statements, it is likely that the model
receives all the necessary information from the remaining code
snippets. In fact, this form of training is similar to input dropout
[63], a strategy occasionally employed in language modeling to re-
duce overfitting on salient input features. Figure 11ğ‘ indeed shows
that models trained with this form of noise may achieve better

training (although not held-out) performance. We observed similar
trends when experimenting on other models and datasets with this
form of input noise.

(cid:7)

(cid:4)

Observation 5: The neural code intelligence models barely
suffer from input noise based on random statement deletion,
which can be explained by our previous findings [55, 56] -
models usually rely on a few tokens for making predictions.

(cid:6)

(cid:5)

5.2.2 Method Name Leakage. We next evaluate a more salient
form of input noise for the method name prediction task, in which
we replace all occurrences of a variable name with the reference
method name, as shown in Figure 2ğ‘. This enables the model to
â€œcheatâ€ by simply copying the method name, if and when present. To
make this signal more obvious, we replace the variable that occurs
most frequently in the method body. As such, the MethodName
task in noisy training samples is reduced to an identity mapping
task [27, 76] in which the target is already present in the input.
In this case, especially at higher noise levels, the training target
is simply picking the maximum occurring variable name in the
method body. These identity cues are not inserted into the test data.
Figures 12 and 13 show the resulting changes of input noise by
replacing a variable name with the method name on MethodName
task for Code2Vec and Code2Seq models with Java-Top10 dataset
and Java-Small dataset, respectively. Similar to the output noise
characteristics of F1-Score (Figure 3), the training performance
of Code2Vec models converges to nearly the same point in both
the original and noisy training sets. On the other hand, Code2Seq

11020304050Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%15101520Epochs0.00.20.40.60.81.0Critical Sample RatioNoise level:0%25%50%75%100%Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

(a) F1-Score (Code2Vec)

(b) Distribution of Prediction Score
(Code2Vec)

(c) Spread of Training Loss (Code2Vec)

(d) F1-Score (Code2Seq)

(e) Distribution of Prediction Score
(Code2Seq)

(f) Spread of Training Loss (Code2Seq)

Figure 10: Input noise by statement deletion (MethodName, Java-Top10).

(a) F1-Score (Code2Vec)

(b) Distribution of Prediction Score
(Code2Vec)

(c) Spread of Training Loss (Code2Vec)

(d) F1-Score (Code2Seq)

(e) Distribution of Prediction Score
(Code2Seq)

(f) Spread of Training Loss (Code2Seq)

Figure 11: Input noise by statement deletion (MethodName, Java-Small).

11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%1200040007100Samples sorted by average score0.00.20.40.60.81.0Predicted average scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%1200040007100Samples sorted by average score0.00.20.40.60.81.0Predicted average scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%110000200003000044426Samples sorted by average score0.00.20.40.60.81.0Predicted average scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%110000200003000044426Samples sorted by average score0.00.20.40.60.81.0Predicted average scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

(a) F1-Score (Code2Vec)

(b) Distribution of Prediction Score
(Code2Vec)

(c) Spread of Training Loss (Code2Vec)

(d) F1-Score (Code2Seq)

(e) Distribution of Prediction Score
(Code2Seq)

(f) Spread of Training Loss (Code2Seq)

Figure 12: Input noise by method name leakage (MethodName, Java-Top10).

(a) F1-Score (Code2Vec)

(b) Distribution of Prediction Score
(Code2Vec)

(c) Spread of Training Loss (Code2Vec)

(d) F1-Score (Code2Seq)

(e) Distribution of Prediction Score
(Code2Seq)

(f) Spread of Training Loss (Code2Seq)

Figure 13: Input noise by method name leakage (MethodName, Java-Small).

11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%12000400060007100Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%12000400060007100Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%12000400060007656Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0F1-ScoreNoise level:0%25%50%75%100%12000400060007656Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

models are more prone to pick up on the identity cues of method
names in noisy training sets. For example, in Figures 12d and 13d
for Code2Seq, the training F1-Score of 100% or 75% noise is higher
than 0% noise. The use of identifier splitting, which reduces the rate
of unknown tokens, may explain why Code2Seq learns identity
cues more effectively than Code2Vec. Irrespective of the perfor-
mance during training, modelsâ€™ performance on the original test
set is highly dependent on the noise level; models trained with less
noisy data show higher test performance than models trained with
higher noisy data. However, as Code2Seq models more frequently
recognize method name cues, their test performance also suffers
more than Code2Vec models at higher noise levels.

Additionally, Figure 12ğ‘,ğ‘’ and Figure 13ğ‘,ğ‘’ suggest that the pre-
diction score distribution changes with an increasing noise level.
These behaviors are largely consistent with what was previously
observed from the prediction score distribution of output noise
(Figure 5). The confidence of trained models changes even at low
rates of added input noise. The change appears more significant in
the smaller-balanced Java-Top10 dataset than in the larger-skewed
Java-Small dataset.

Comparing the Gini coefficient values of input noise from Fig-
ure 12ğ‘,ğ‘“ and Figure 13ğ‘,ğ‘“ with those under output noise in Figure 7
may suggest a nearly reversed trend in the spread of training loss:
it decreases for output noise but increases for input noise with an
increasing noise level. This can be explained by the presence of
identity cues in the training set. In the case of output noise, we
replace a method name with a different randomly selected method
name. Thus, similar code snippets may have very different method
names, which makes the learning process more complicated. How-
ever, in the case of input noise, we add the target method name
inside the input code snippets, thus making the learning process
much simpler when such identify cues are present.

(cid:7)

Observation 6: Signal leaking input noise can help models
achieve high performance during training, but hurts their
ability to generalize on non-noisy test data.

(cid:6)

(cid:4)

(cid:5)

Injecting Repair Information. We next apply a similar form
5.2.3
of input noise to the VarMisuse tasks and dataset. As shown in
Figure 2ğ‘“ , we add noise to the training set by inserting repair data
into buggy methods by replacing the variable at the error location
and all occurrences of the correct variable that should be at that
location, and replacing the most frequent token in a correct sample
with a cue that no bug is present.

Figures 14 and 15 show the resulting impact of applying input
noise in the above manner on Transformer and GGNN models
trained on the VarMisuse dataset. Similar to the Code2Seq models
on the MethodName task, the Transformer and GGNN models
are highly capable of learning identity cues of the replacement
tokens (error location variable and correct target variable) in noisy
training sets, for both the localization and repair tasks: the training
F1-Score under 100% noise (full signal leakage) almost immediately
jump to perfect quality, as the models learn to predict based on the
presence of the inserted cues. The performance of the models on
the held-out data, where such cues are absent, consequently drops
to near-chance performance.

Moreover, the prediction score distribution generally skews
lower with increasing input noise level (Figure 14ğ‘,ğ‘’ and Fig-
ure 15ğ‘,ğ‘’ ), with the exception of the 100% noise-level, which
always predicts locations and repairs with complete confidence. It
seems, for the localization task, the role of our replacements as
identity cues for the models become most prominent when the
replacements are carried out for all the samples in the training set.
Finally, the Gini coefficient values in both the localization and
repair tasks (Figure 14ğ‘,ğ‘“ and Figure 15ğ‘,ğ‘“ ) show that the training
loss increases with the addition of input noise levels. These trends
are the reverse of the trends when we applied varying levels of
output noise, as seen in Figure 8; which can be explained by the
impact of identity cues, as described in Section 5.2.2.

In sum, VarMisuse models may achieve high performance on
noisy datasets during training, even on higher input noise levels,
but, as seen for MethodName models, always fail to generalize
during evaluation on the original non-noisy held-out data.

5.3 Analyses of Memorization with a Language

Model

In this section, we explore the impact of noise when fine-tuning the
CodeBERT language model on the Code-to-Text and CodeSearch
tasks, in order to characterize memorization in these settings. We
add output noise by replacing the target label with another label
as shown in Figure 2ğ‘–,ğ‘™ and add input noise by replacing frequent
tokens of the input code snippets with a mask or identity token
as shown in Figure 2â„,ğ‘˜ . Figure 16 shows the smoothed BLEU-4
score and accuracy (left column, solid lines are training and dashed
lines are validation), average probability score on the validation
set (center column), and the spread of training loss in each epoch
(right column) while fine-tuning the CodeBERT models with output
noise; Figure 17 shows the same results of CodeBERT models with
input noise.

Impact of Output Noise. As shown in Figure 16, the effect of
output noise on fine-tuning CodeBERT language models are con-
sistent with our previous findings on other neural code intelligence
models in Section 5.1. In the CodeSearch task, the accuracy of
CodeBERT models on the original training set and noisy training
sets converges towards the same point (Figure 16d), whereas on
the Code-to-Text task, the smoothed BLEU-4 score of models on
the original training set is very close to most of the noisy training
sets (Figure 16a), except for the 100% noise level where training is
significantly impaired (maybe that it is actually hard to memorize
when there is so much data to learn). However, their generalization
to original validation set (dashed lines) suffers with more added
noise. Similarly, the prediction score distribution (Figure 16ğ‘,ğ‘’ ) and
the spread of training loss (Figure 16ğ‘,ğ‘“ ) curves highlight that the
resulting trends significantly change after adding more noise. Note
that, the CodeSearch task has two output labels (negative-0 or
positive-1). As a result, in the 100% output noise level, all positive
labels change to negative labels and all negative labels change to
positive labels. Therefore, in Figure 16f, the training behaviors of
CodeBERT models on 100% output noise are actually the same as
0% output noise, and similarly, the 75% noise matches the 25% noise
level. However, as shown in Figure 16e, their confidence on the
(noise-free) validation data differs, as expected.

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

(a) Localization Accuracy

(b) Distribution of Localization Score

(c) Spread of Localization Loss

(d) Repair Accuracy

(e) Distribution of Repair Score

(f) Spread of Repair Loss

Figure 14: Input noise by injecting repair information (VarMisuse, Transformer).

(a) Localization Accuracy

(b) Distribution of Localization Score

(c) Spread of Localization Loss

(d) Repair Accuracy

(e) Distribution of Repair Score

(f) Spread of Repair Loss

Figure 15: Input noise by injecting repair information (VarMisuse, GGNN).

11020304050Steps0.00.20.40.60.81.0Localization accuracyNoise level:0%25%50%75%100%130006000900012387Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Repair accuracyNoise level:0%25%50%75%100%130006000900012387Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Localization accuracyNoise level:0%25%50%75%100%130006000900012387Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Repair accuracyNoise level:0%25%50%75%100%130006000900012387Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Steps0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

(a) Smoothed BLEU-4 Score (Code-to-Text)

(b) Distribution of Prediction Score
(Code-to-Text)

(c) Spread of Training Loss (Code-to-Text)

(d) Accuracy (CodeSearch)

(e) Distribution of Prediction Score
(CodeSearch)

(f) Spread of Training Loss (CodeSearch)

Figure 16: Impact of output noise in fine-tuning CodeBERT language model.

(a) Smoothed BLEU-4 Score (Code-to-Text)

(b) Distribution of Prediction Score
(Code-to-Text)

(c) Spread of Training Loss (Code-to-Text)

(d) Accuracy (CodeSearch)

(e) Distribution of Prediction Score
(CodeSearch)

(f) Spread of Training Loss (CodeSearch)

Figure 17: Impact of input noise in fine-tuning CodeBERT language model.

11020304050Epochs20406080100BLEU-4 ScoreNoise level:0%25%50%75%100%150010001400Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0AccuracyNoise level:100%75%50%25%0%15001500250035004417Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:100%75%50%25%0%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:100%75%50%25%0%11020304050Epochs020406080100BLEU-4 ScoreNoise level:0%25%50%75%100%150010001400Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0AccuracyNoise level:0%25%50%75%100%15001500250035004417Samples sorted by average score0.00.20.40.60.81.0Average predicted scoreNoise level:0%25%50%75%100%11020304050Epochs0.00.20.40.60.81.0Gini-coefficient of training lossNoise level:0%25%50%75%100%Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

Impact of Input Noise. The CodeBERT language models
experience a similar impact while fine-training with input noise
(Figure 17). Figures 17a and 17d show that the performance of
CodeBERT models on the original and noisy training sets achieve
high scores for both the Code-to-Text and CodeSearch tasks.
On the other side, their performance on the original validation
set decreases for added noise. Figures 17b and 17e show the
prediction score distribution for input noise on the Code-to-Text
and CodeSearch tasks, respectively. For 100% noise level in the
CodeSearch task, the CodeBERT models predict all validation
samples as positive in each epoch of fine-tuning. The dataset of
the CodeSearch task is balanced so that half the samples are
positive and the other half are negative. Therefore, the accuracy is
always 50% on the validation set (dashed red line in Figure 17d),
the average predicted scores are high on the original positive
samples and low on the original negative samples (vertical red
line in Figure 17e), and the spread of training loss between the
original positive and negative samples is significantly higher
(horizontal red line in Figure 17f). While the increase of noise level
considerably changes the spread of training loss in Figures 17c
and 17f, the resulting trends of Code-to-Text task (where 0%
is higher than 100%) are quite opposite to the CodeSearch
task (where 100% is higher than 0%); which relates to their
corresponding input noise generation techniques. As shown in
Figure 2ğ‘– for the Code-to-Text task, we add input noise by
replacing the tokens of input code snippets that appear in the
target docstring with the â€œMASKâ€ token. Therefore, it becomes
harder for CodeBERT models to generate the correct docstring
from masked code snippets. It relates to our experiments with
the output noise of Section 5.1 that exhibits a similar trend in the
spread of training loss. On the contrary, as shown in Figure 2ğ‘—
for the CodeSearch task, we add input noise by replacing the
most frequent tokens of code snippets and docstring of positive
(or negative) samples with the â€œPOSITIVEâ€ (or â€œNEGATIVEâ€) token.
Therefore, it becomes comparatively easier for CodeBERT models
to learn whether a given pair is matched, because the hints are
already added to input samples as identity cues. It relates to our
experiments with the input noise of Section 5.2.2 that exhibits a
similar trend in the spread of training loss.

(cid:11)

Observation 7: The impact of noise in fine-tuning
CodeBERT language model supports the findings with other
neural code intelligence models - the resulting trends of
CodeBERT model change considerably, even at a low rate of
added input/output noise.

(cid:10)

(cid:8)

(cid:9)

6 DISCUSSION AND FUTURE WORK
Neural networks are powerful tools for learning from very large
datasets. However, our work and others highlight that their per-
ceived performance may not reflect having learned useful insights.
To effectively and soundly use neural models in code intelligence
applications, our community needs to develop rigorous frameworks
for the evaluation and adoption of such models. We transplanted
and interpreted a rich suite of new metrics for studying this ques-
tion to the software engineering and code intelligence systems,
providing actionable uses for these.

def multiply(x, y):
return x + y

x, y = multiply1, multiply2
print(x, y)
I want x,y to be the sum of the first and

Figure 18: Continuation of Clippy auto-complete, with 1.3 billion
parameter, when cued with the first line of code.

6.1 Memorization and Network Architecture
Our results suggest that all models are susceptible to memorization,
and that the network architecture and hyper-parameterization can
influence their memorization behavior. In particular, we noticed
that a model that was over-parameterized for the complexity of the
task (i.e., Code2Vec on several datasets) was able to fit noise very
similar to real labels. This only translated into a small reduction of
test (or held-out) performance, suggesting that the latter is a poor
indicator for detecting when training data is noisy â€“ indeed, it is not
clear (but plausible) whether the original dataset already contained
noise of its own. In the future, we plan to evaluate the impact of
existing noise in the programs on the training characteristics of the
neural networks, as the programs can be buggy or incomplete in
the open-source datasets.

6.2 Are Current Models Memorizing?
In our analysis, we have primarily focused on injecting noise into
existing datasets, yet research suggests that these datasets are them-
selves noisy [15, 75]. More broadly, GitHub is a major data source
used for training neural models in code intelligence systems, and
unfortunately, it can be noisy [48, 60]. Given that we found some
metrics to be useful indicators of changes in training behavior as
the degree of noise was increased, are we now able to determine
whether the original data suffered from similar issues?

Answering the above question based on the results presented
in this paper is inherently cyclical, and deserving of further inves-
tigation. Nevertheless, several conclusions do seem plausible. For
one, that Code2Vec is using its excessive capacity to memorize
rather than generalize is clear from virtually all results. Secondly,
in most cases, the Code2Seq results often showed a rather small
gap between 0% and 25% noise than between 25% and 50% noise.
That this is not the case on the Java-Top10 dataset, or any other
model/dataset pair, suggests that the 25% noise situation is less
distinct from the original data than one might like. Finally, in the
localization predicted score distribution, both very low and high
degrees of noise produced much more similar curves than inter-
mediate levels of noise (Figure 6). The fact that making the labels
completely noisy yielded an increase in the proportion of samples
predicted confidently suggests that these models have close to the
capacity required to memorize this dataset, if needs be. At the same
time, studying the complete trend and dual phase transition from
0% to 100% strongly indicates that this is not what they are currently
doing. In the future, we plan to use such signals of memorization
to improve the generalization capability of models.

6.3 Memorization and Generative Models
Planning beyond the predictive tasks used in this study, very large
neural networks are increasingly often used in generative tasks,

Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

e.g. code completion. We should thus be aware of the strengths
and limits of such neural networks. Recent generative models in-
volve billions of trainable parameters, which allow a neural network
to memorize tremendous amounts of data, including noise, bugs,
and vulnerabilities [13]. In a sense, these models can morph into
opaque information retrieval systems. For instance, the snippet in
Figure 18 is the performance of Clippy5 auto-completion model on
a simple input â€œdef multiply(x, y):â€. Clippy uses 1.3 billion train-
able parameters and a similar causal language modeling as used in
Codex [13], the same technology powering GitHub Copilot6. The
output of the model, while syntactically correct is both semantically
wrong (it suggests x + y instead of x * y) and appears to contain
shards of different plausible continuation, including printing (some-
what) related identifiers and beginning to write a natural language
explanation. All this suggests that the model has clearly learned
to repeat common patterns surrounding such prompts, but at the
same time seriously lacks the required contextual and semantic
insight to produce a useful completion. Quantifying and reducing
the degree of repetition and noise in its training data may well
help overcome these obstacles. In the future, we plan to evaluate
the extent and impacts of memorization in the generative neural
models.

6.4 Is Memorization Always Unacceptable?
Perhaps the answer to this question requires considering two angles:
datasets and application domain. We showed that neural models
are capable of making wrong predictions with high confidence.
Therefore, when noise dominates the training datasets, memoriza-
tion should be contained and avoided. The Software Engineering
community has seen cases where problematic datasets have led to
training wrong models [73].

In cases where the inputs are redundant and the task mostly
depends on the similarity of input programs with instances in the
training data, memorization can be similar to caching the results in
static analysis techniques [34]. However, in cases where confiden-
tiality of training data is important or the input data is dissimilar
enough from the training dataset, perhaps a deeper understand-
ing of input programs is required and memorization in the models
should be evaluated, managed, and avoided when possible [11].

7 THREATS TO VALIDITY
Our work quantitatively studies code intelligence tools across a
series of metrics not previously used in software engineering field.
For the implementation of the models and metrics, we were able to
rely on the public implementation of the underlying models and
the description of the metrics in computer vision by Arpit et al. [8]
and Zhang et al. [77]. As such, the primary threat to our workâ€™s
validity is external: our results are based on the evaluation of a
selection of models used with their default capacities. As shown
by the Code2Vec results, these default configurations impact their
behavior in our study. We have opted to study these first since they
are most likely to be used in this form by practitioners; this already
reflected a couple of monthâ€™s worth of GPU utilization. Moreover,
we randomly add noise in the data to create noisy datasets. Our

5https://huggingface.co/flax-community/gpt-neo-125M-code-clippy
6https://github.com/CodedotAl/gpt-code-clippy#introduction

experiment may show varying results with a different run of ran-
domization. Taking an average of multiple runs may provide better
results, thus, we repeated the experiments multiple times for output
noise with a different random seed. Future studies may investigate
further variations on these, such as tuning the modelsâ€™ capacity,
denoising the noise-prone datasets, etc.

8 CONCLUSION
In this paper, we perform a large-scale study on the impact of noise
and memorization on the training behavior of neural models in
code intelligence systems. We do so by adding noise in the training
datasets of several popular neural code intelligence models and
measuring established metrics that characterize the training behav-
ior. To the best of our knowledge, this is the first such work in the
software engineering and code intelligence systems. We observe
that neural code intelligence models with excessive numbers of
trainable parameters can memorize datasets of code quite easily,
echoing findings from other communities. Across several metrics,
the models displayed consistent and identifiable changes in charac-
teristics as the role of memorization increases, including significant
changes in the typical confidence of model predictions. The con-
sistency of these trends suggests that they may be of great use in
identifying warning signs of memorization beyond the use of test
(or held-out) data alone.

REFERENCES
[1] Miltiadis Allamanis. 2019. The Adverse Effects of Code Duplication in Machine
Learning Models of Code. In Proceedings of the ACM SIGPLAN International
Symposium on New Ideas, New Paradigms, and Reflections on Programming and
Software (Onward! 2019). ACM New York, NY, USA, 143â€“153. https://doi.org/10.
1145/3359591.3359735

[2] Miltiadis Allamanis, Earl T. Barr, Christian Bird, and Charles Sutton. 2015. Sug-
gesting Accurate Method and Class Names. In Proceedings of the 10th Joint Meeting
on Foundations of Software Engineering (ESEC/FSE 2015). Association for Com-
puting Machinery, New York, NY, USA, 38â€“49. https://doi.org/10.1145/2786805.
2786849

[3] Miltiadis Allamanis, Earl T. Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A Survey of Machine Learning for Big Code and Naturalness. In ACM Computing
Surveys, Vol. 51. Association for Computing Machinery, New York, NY, USA,
Article 81, 37 pages. https://doi.org/10.1145/3212695

[4] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learn-
ing to Represent Programs with Graphs. International Conference on Learning
Representations (ICLR), OpenReview.net (2018). https://openreview.net/forum?
id=BJOFETxR-

[5] Miltiadis Allamanis, Hao Peng, and Charles A. Sutton. 2016. A Convolutional
Attention Network for Extreme Summarization of Source Code. In Proceedings
of the 33nd International Conference on Machine Learning (ICML 2016), Vol. 48.
Proceedings of Machine Learning Research (PMLR), Open Access, 2091â€“2100.
http://proceedings.mlr.press/v48/allamanis16.html

[6] Uri Alon, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences
from Structured Representations of Code. International Conference on Learning
Representations (ICLR), OpenReview.net (2019). https://openreview.net/forum?
id=H1gKYo09tX

[7] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn-
ing Distributed Representations of Code. In Proceedings of the ACM on Program-
ming Languages (PACMPL 2019), Vol. 3. Association for Computing Machinery,
New York, NY, USA, 40:1â€“40:29. https://doi.org/10.1145/3290353

[8] Devansh Arpit, StanisÅ‚aw Jastrzundefinedbski, Nicolas Ballas, David Krueger,
Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron
Courville, Yoshua Bengio, and Simon Lacoste-Julien. 2017. A Closer Look at
Memorization in Deep Networks. In Proceedings of the 34th International Con-
ference on Machine Learning (ICML 2017), Vol. 70. JMLR.org, 233â€“242. https:
//doi.org/10.5555/3305381.3305406

[9] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be
Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,
and Transparency (FAccT 2021). Association for Computing Machinery, New York,
NY, USA, 610â€“623. https://doi.org/10.1145/3442188.3445922

Memorization and Generalization in Neural Code Intelligence Models

Information and Software Technology, IST Journal 2022, Elsevier

[10] Pavol Bielik and Martin Vechev. 2020. Adversarial Robustness for Code. In
Proceedings of the 37th International Conference on Machine Learning (ICML 2020),
Vol. 119. Proceedings of Machine Learning Research (PMLR), Open Access, 896â€“
907. https://proceedings.mlr.press/v119/bielik20a.html

[11] Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.
The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural
Networks. In Proceedings of the 28th USENIX Conference on Security Symposium
(SEC 2019). USENIX Association, Berkeley, CA, United States, 267â€“284. https:
//dl.acm.org/doi/10.5555/3361338.3361358

[12] Casey Casalnuovo, Kenji Sagae, and Prem Devanbu. 2019. Studying the Difference
between Natural and Programming Language Corpora. In Empirical Software
Engineering, Volume 24, Issue 4. Kluwer Academic Publishers, Springer, USA,
1823â€“1868. https://doi.org/10.1007/s10664-018-9669-7

[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared
Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. 2021.
Evaluating Large Language Models Trained on Code. arXiv:cs.LG/2107.03374
https://arxiv.org/abs/2107.03374

[14] Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. 2019. Un-
derstanding and Utilizing Deep Neural Networks Trained with Noisy Labels. In
Proceedings of the 36th International Conference on Machine Learning (ICML 2019),
Vol. 97. PMLR, 1062â€“1070. https://proceedings.mlr.press/v97/chen19g.html
[15] Rhys Compton, Eibe Frank, Panos Patros, and Abigail Koay. 2020. Embedding Java
Classes with code2vec: Improvements from Variable Obfuscation. In Proceedings
of the 17th International Conference on Mining Software Repositories (MSR 2020).
Association for Computing Machinery, New York, NY, USA, 243â€“253. https:
//doi.org/10.1145/3379597.3387445

[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for Language Under-
standing. In Proceedings of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1. Associa-
tion for Computational Linguistics, Minneapolis, Minnesota, 4171â€“4186. https:
//doi.org/10.18653/v1/N19-1423

[17] Elizabeth Dinella, Hanjun Dai, Ziyang Li, Mayur Naik, Le Song, and Ke Wang.
2020. Hoppity: Learning Graph Transformations to Detect and Fix Bugs in
Programs. International Conference on Learning Representations (ICLR), OpenRe-
view.net (2020). https://openreview.net/forum?id=SJeqs6EFvB

[18] Aparna Elangovan, Jiayuan He, and Karin M. Verspoor. 2021. Memorization
vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation.
In Proceedings of the 16th Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL 2021). Association for Computational
Linguistics, Online, 1325â€“1335. https://doi.org/10.18653/v1/2021.eacl-main.113
[19] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:
A Pre-Trained Model for Programming and Natural Languages. In Findings of the
Association for Computational Linguistics: EMNLP. Association for Computational
Linguistics, Online, 1536â€“1547. https://doi.org/10.18653/v1/2020.findings-emnlp.
139

[20] Mark Gabel and Zhendong Su. 2010. A Study of the Uniqueness of Source Code.
In Proceedings of the 18th ACM SIGSOFT International Symposium on Foundations
of Software Engineering (FSE 2010). Association for Computing Machinery, New
York, NY, USA, 147â€“156. https://doi.org/10.1145/1882291.1882315

[21] Elizabeth Gibney et al. 2016. Google AI algorithm masters ancient game of Go.

Nature 529, 7587 (2016), 445â€“446. https://doi.org/10.1038/529445a

[22] Corrado Gini. 1936. On the Measure of Concentration with Special Reference to
Income and Statistics. Colorado College Publication, General Series 208, 1 (1936),
73â€“79.

[23] Jian Gu, Zimin Chen, and Martin Monperrus. 2021. Multimodal Representation
for Neural Code Search. In Proceedings of the 37th IEEE International Conference
on Software Maintenance and Evolution (ICSME) (ICSME 2021). IEEE, New York,
NY, USA, 483â€“494. https://doi.org/10.1109/ICSME52107.2021.00049

[24] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep Code Search. In
Proceedings of the 40th International Conference on Software Engineering (ICSE
2018). Association for Computing Machinery, New York, NY, USA, 933â€“944.
https://doi.org/10.1145/3180155.3180167

[25] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. 2016. Deep
API Learning. In Proceedings of the 24th ACM SIGSOFT International Symposium
on Foundations of Software Engineering (FSE 2016). Association for Computing Ma-
chinery, New York, NY, USA, 631â€“642. https://doi.org/10.1145/2950290.2950334
[26] Guy Hacohen, Leshem Choshen, and Daphna Weinshall. 2020. Letâ€™s Agree to
Agree: Neural Networks Share Classification Order on Real Datasets. In Proceed-
ings of the 37th International Conference on Machine Learning (ICML 2020), Vol. 119.
Proceedings of Machine Learning Research (PMLR), Open Access, 3950â€“3960.
http://proceedings.mlr.press/v119/hacohen20a.html

[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity Map-
pings in Deep Residual Networks. In European Conference on Computer Vision
(ECCV 2016). Springer, Cham, 630â€“645. https://doi.org/10.1007/978-3-319-46493-
0_38

[28] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and
International

David Bieber. 2020. Global Relational Models of Source Code.

Conference on Learning Representations (ICLR), OpenReview.net (2020). https:
//openreview.net/forum?id=B1lnbRNtwr

[29] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the Naturalness of Software. In Proceedings of the 34th International
Conference on Software Engineering (ICSE 2012). IEEE Press, New York, NY, USA,
837â€”-847. https://doi.org/10.1109/ICSE.2012.6227135

[30] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. arXiv:cs.LG/1909.09436 https://arxiv.org/abs/1909.09436

[31] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics, Volume 1.
Association for Computational Linguistics, Berlin, Germany, 2073â€“2083. https:
//doi.org/10.18653/v1/P16-1195

[32] Hong Jin Kang, TegawendÃ© F. BissyandÃ©, and David Lo. 2019. Assessing the Gen-
eralizability of Code2vec Token Embeddings. In Proceedings of the 34th IEEE/ACM
International Conference on Automated Software Engineering (ASE 2019). IEEE
Press, New York, NY, USA, 1â€“12. https://doi.org/10.1109/ASE.2019.00011
[33] Rafael-Michael Karampatsis and Charles Sutton. 2020. How Often Do Single-
Statement Bugs Occur? The ManySStuBs4J Dataset. In Proceedings of the 17th
International Conference on Mining Software Repositories (MSR 2020). Association
for Computing Machinery, New York, NY, USA, 573â€“577. https://doi.org/10.
1145/3379597.3387491

[34] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike
Lewis. 2020. Generalization through Memorization: Nearest Neighbor Language
Models. International Conference on Learning Representations (ICLR), OpenRe-
view.net (2020). https://openreview.net/forum?id=HklBjCEKvH

[35] Sunghun Kim, Hongyu Zhang, Rongxin Wu, and Liang Gong. 2011. Dealing with
Noise in Defect Prediction. In Proceedings of the 33rd International Conference on
Software Engineering (ICSE 2011). Association for Computing Machinery, New
York, NY, USA, 481â€“490. https://doi.org/10.1145/1985793.1985859

[36] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2017. ImageNet Classi-
fication with Deep Convolutional Neural Networks. Commun. ACM 60, 6 (2017),
84â€“90. https://doi.org/10.1145/3065386

[37] Triet H. M. Le, Hao Chen, and Muhammad Ali Babar. 2020. Deep Learning for
Source Code Modeling and Generation: Models, Applications, and Challenges.
Comput. Surveys 53, 3, Article 62 (2020), 38 pages. https://doi.org/10.1145/3383458
[38] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, Patrick Haffner, et al. 1998. Gradient-
based learning applied to document recognition. Proc. IEEE 86, 11 (1998), 2278â€“
2324. https://doi.org/10.1109/5.726791

[39] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankanhalli. 2019. Learning
to Learn From Noisy Labeled Data. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR 2019). 5046â€“5054. https://doi.org/10.1109/CVPR.
2019.00519

[40] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. 2016. Gated
Graph Sequence Neural Networks. International Conference on Learning Repre-
sentations (ICLR), OpenReview.net (2016). https://arxiv.org/abs/1511.05493
[41] Chin-Yew Lin and Franz Josef Och. 2004. ORANGE: a Method for Evaluating
Automatic Evaluation Metrics for Machine Translation. In Proceedings of the
20th International Conference on Computational Linguistics. COLING, Geneva,
Switzerland, 501â€“507. https://aclanthology.org/C04-1072

[42] Kui Liu, Dongsun Kim, TegawendÃ© F. BissyandÃ©, Taeyoung Kim, Kisub Kim,
Anil Koyuncu, Suntae Kim, and Yves Le Traon. 2019. Learning to Spot and
Refactor Inconsistent Method Names. In Proceedings of the 41st International
Conference on Software Engineering (ICSE 2019). IEEE Press, New York, NY, USA,
1â€“12. https://doi.org/10.1109/ICSE.2019.00019

[43] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. International Conference on
Learning Representations (ICLR), OpenReview.net (2020). https://openreview.net/
forum?id=SyxS0T4tvS

[44] Cristina V. Lopes, Petr Maj, Pedro Martins, Vaibhav Saini, Di Yang, Jakub Zitny,
Hitesh Sajnani, and Jan Vitek. 2017. DÃ©jÃ Vu: A Map of Code Duplicates on
GitHub. In Proceedings of the ACM on Programming Languages, Volume 1, OOPSLA
(PACMPL 2017). Association for Computing Machinery, New York, NY, USA,
Article 84, 28 pages. https://doi.org/10.1145/3133908

[45] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong
Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021.
CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understand-
ing and Generation. CoRR abs/2102.04664 (2021). https://microsoft.github.io/
CodeXGLUE/

[46] Paul W. McBurney and Collin McMillan. 2014. Automatic Documentation
Generation via Source Code Summarization of Method Context. In Proceed-
ings of the 22nd International Conference on Program Comprehension (ICPC
2014). Association for Computing Machinery, New York, NY, USA, 279â€“290.

Information and Software Technology, IST Journal 2022, Elsevier

Md Rafiqul Islam Rabin, Aftab Hussain, Mohammad Amin Alipour, and Vincent J. Hellendoorn

https://doi.org/10.1145/2597008.2597149

[47] Ari S. Morcos, Maithra Raghu, and Samy Bengio. 2018.

Insights on Repre-
sentational Similarity in Neural Networks with Canonical Correlation. In Pro-
ceedings of the 32nd International Conference on Neural Information Processing
Systems (NIPS 2018). Curran Associates Inc., Red Hook, NY, USA, 5732â€“5741.
https://dl.acm.org/doi/10.5555/3327345.3327475

[48] Nuthan Munaiah, Steven Kroh, Craig Cabrey, and Meiyappan Nagappan. 2017.
Curating GitHub for Engineered Software Projects. In Empirical Software Engi-
neering, Volume 22, Issue 6. Kluwer Academic Publishers, Springer, USA, 3219â€“
3253. https://doi.org/10.1007/s10664-017-9512-6

[49] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Sil-
vio Savarese, and Caiming Xiong. 2022. A Conversational Paradigm for Program
Synthesis. arXiv:cs.LG/2203.13474 https://arxiv.org/abs/2203.13474

[50] Curtis G Northcutt, Anish Athalye, and Jonas Mueller. 2021. Pervasive Label
Errors in Test Sets Destabilize Machine Learning Benchmarks. Proceedings of the
35th Conference on Neural Information Processing Systems - Track on Datasets and
Benchmarks (2021). https://openreview.net/forum?id=XccDXrDNLek

[51] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
Method for Automatic Evaluation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, Philadelphia, Pennsylvania, USA, 311â€“318. https:
//doi.org/10.3115/1073083.1073135

[52] Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2020. Evaluation of
Generalizability of Neural Program Analyzers under Semantic-Preserving Trans-
formations. arXiv:cs.SE/2004.07313 https://arxiv.org/abs/2004.07313

[53] Md Rafiqul Islam Rabin and Mohammad Amin Alipour. 2021. Code2Snapshot:
Using Code Snapshots for Learning Representations of Source Code.
arXiv:cs.SE/2111.01097 https://arxiv.org/abs/2111.01097

[54] Md Rafiqul Islam Rabin, Nghi D.Q. Bui, Ke Wang, Yijun Yu, Lingxiao Jiang, and
Mohammad Amin Alipour. 2021. On the generalizability of Neural Program Mod-
els with respect to semantic-preserving program transformations. In Information
and Software Technology (IST), Volume 135. Elsevier, Amsterdam, Netherlands,
106552. https://doi.org/10.1016/j.infsof.2021.106552

[55] Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, and Mohammad Amin Alipour.
2021. Understanding Neural Code Intelligence through Program Simplification.
In Proceedings of the 29th ACM Joint Meeting on European Software Engineering
Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE
2021). Association for Computing Machinery, New York, NY, USA, 441â€“452.
https://doi.org/10.1145/3468264.3468539

[56] Md Rafiqul Islam Rabin, Aftab Hussain, and Mohammad Amin Alipour. 2022.
Syntax-Guided Program Reduction for Understanding Neural Code Intelligence
Models. In Proceedings of the 6th ACM SIGPLAN International Symposium on
Machine Programming (MAPS 2022). Association for Computing Machinery, New
York, NY, USA, 70â€“79. https://doi.org/10.1145/3520312.3534869

[57] Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, and Moham-
mad Amin Alipour. 2020. Towards Demystifying Dimensions of Source Code
Embeddings. In Proceedings of the 1st ACM SIGSOFT International Workshop
on Representation Learning for Software Engineering and Program Languages
(RL+SE&PL 2020). Association for Computing Machinery, New York, NY, USA,
29â€“38. https://doi.org/10.1145/3416506.3423580

[58] Md Rafiqul Islam Rabin, Ke Wang, and Mohammad Amin Alipour. 2019. Testing
Neural Program Analyzers. 34th IEEE/ACM International Conference on Automated
Software Engineering (Late Breaking Results-Track) (2019). https://doi.org/10.
48550/arXiv.1908.10711

[59] Veselin Raychev, Pavol Bielik, and Martin Vechev. 2016. Probabilistic Model
for Code with Decision Trees. In ACM SIGPLAN Notices, Volume 51, Issue 10,
OOPSLA (OOPSLA 2016). Association for Computing Machinery, New York, NY,
USA, 731â€“747. https://doi.org/10.1145/3022671.2984041

[60] Veselin Raychev, Pavol Bielik, Martin Vechev, and Andreas Krause. 2016. Learning
Programs from Noisy Data. In Proceedings of the 43rd Annual ACM SIGPLAN-
SIGACT Symposium on Principles of Programming Languages (POPL 2016). As-
sociation for Computing Machinery, New York, NY, USA, 761â€“774.
https:
//doi.org/10.1145/2837614.2837671

[61] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019.
Do ImageNet Classifiers Generalize to ImageNet?. In Proceedings of the 36th
International Conference on Machine Learning (ICML 2019), Vol. 97. Proceed-
ings of Machine Learning Research (PMLR), Open Access, 5389â€“5400. https:
//proceedings.mlr.press/v97/recht19a.html

[62] Tushar Sharma, Maria Kechagia, Stefanos Georgiou, Rohit Tiwari, and Federica
Sarro. 2021. A Survey on Machine Learning Techniques for Source Code Analysis.

arXiv:cs.SE/2110.09610 https://arxiv.org/abs/2110.09610

[63] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929â€“1958.
https://dl.acm.org/doi/10.5555/2627435.2670313

[64] Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim A. Laredo, and Alessandro Morari.
2021. Probing Model Signal-Awareness via Prediction-Preserving Input Mini-
mization. In Proceedings of the 29th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineer-
ing (ESEC/FSE 2021). Association for Computing Machinery, New York, NY, USA,
945â€“955. https://doi.org/10.1145/3468264.3468545

[65] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Agha-
janyan. 2022. Memorization Without Overfitting: Analyzing the Training Dy-
namics of Large Language Models. arXiv:cs.CL/2205.10770 https://arxiv.org/
abs/2205.10770

[66] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh singh.
2019. Neural Program Repair by Jointly Learning to Localize and Repair. Inter-
national Conference on Learning Representations (ICLR), OpenReview.net (2019).
https://openreview.net/forum?id=ByloJ20qtm

[67] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Å ukasz Kaiser, and Illia Polosukhin. 2017. Attention is
All you Need. In Proceedings of the 31st International Conference on Neu-
ral
Informa-
tion Processing Systems, Volume 30 (NIPS 2017). Curran Associates Inc., Red
Hook, NY, USA, 5998â€“6008. https://proceedings.neurips.cc/paper/2017/hash/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

Information Processing Systems, Part of Advances in Neural

[68] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and
Philip S. Yu. 2018. Improving Automatic Source Code Summarization via Deep
Reinforcement Learning. In Proceedings of the 33rd ACM/IEEE International Con-
ference on Automated Software Engineering (ASE 2018). Association for Comput-
ing Machinery, New York, NY, USA, 397â€“407. https://doi.org/10.1145/3238147.
3238206

[69] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.
Bowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu-
ral Language Understanding. International Conference on Learning Representations
(ICLR), OpenReview.net (2019). https://openreview.net/forum?id=rJ4km2R5t7

[70] Ke Wang and Mihai Christodorescu. 2019. Coset: A benchmark for evaluating
neural program embeddings. arXiv:cs.LG/1905.11445 https://arxiv.org/abs/1905.
11445

[71] Yu Wang, Fengjuan Gao, and Linzhang Wang. 2021. Demystifying code summa-
rization models. arXiv:cs.LG/2102.04625 https://arxiv.org/abs/2102.04625
[72] Martin White, Christopher Vendome, Mario Linares-VÃ¡squez, and Denys Poshy-
vanyk. 2015. Toward Deep Learning Software Repositories. In Proceedings of the
12th Working Conference on Mining Software Repositories (MSR 2015). IEEE Press,
New York, NY, USA, 334â€“345. https://dl.acm.org/doi/10.5555/2820518.2820559
[73] Bowen Xu, Amirreza Shirani, David Lo, and Mohammad Amin Alipour. 2018.
Prediction of Relatedness in Stack Overflow: Deep Learning vs. SVM: A Re-
producibility Study. In Proceedings of the 12th ACM/IEEE International Sympo-
sium on Empirical Software Engineering and Measurement (ESEM 2018). Asso-
ciation for Computing Machinery, New York, NY, USA, Article 21, 10 pages.
https://doi.org/10.1145/3239235.3240503

[74] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. 2022.
A Systematic Evaluation of Large Language Models of Code. In Proceedings
of the 6th ACM SIGPLAN International Symposium on Machine Programming
(MAPS 2022). Association for Computing Machinery, New York, NY, USA, 1â€“10.
https://doi.org/10.1145/3520312.3534862

[75] Noam Yefet, Uri Alon, and Eran Yahav. 2020. Adversarial Examples for Models of
Code. In Proceedings of the ACM on Programming Languages, Volume 4, OOPSLA
(PACMPL 2020). Association for Computing Machinery, New York, NY, USA,
162:1â€“162:30. https://doi.org/10.1145/3428230

[76] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C. Mozer, and Yoram
Singer. 2020. Identity Crisis: Memorization and Generalization Under Extreme
Overparameterization.
International Conference on Learning Representations
(ICLR), OpenReview.net (2020). https://openreview.net/forum?id=B1l6y0VFPr
[77] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
2017. Understanding deep learning requires rethinking generalization. Inter-
national Conference on Learning Representations (ICLR), OpenReview.net (2017).
https://openreview.net/forum?id=Sy8gdB9xx

[78] Jie M. Zhang, Mark Harman, Lei Ma, and Yang Liu. 2022. Machine Learning Test-
ing: Survey, Landscapes and Horizons. IEEE Transactions on Software Engineering
48, 1 (2022), 1â€“36. https://doi.org/10.1109/TSE.2019.2962027

