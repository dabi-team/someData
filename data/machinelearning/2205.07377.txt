MIPT/TH-08/22
ITEP/TH-09/22
IITP/TH-11/22

Splendeurs et mis`eres of Heavisidisation

V.Dolotin and A.Morozov

MIPT, ITEP & IITP, Moscow, Russia

ABSTRACT

2
2
0
2

y
a
M
5
1

]
h
t
-
p
e
h
[

1
v
7
7
3
7
0
.
5
0
2
2
:
v
i
X
r
a

Machine Learning (ML) is applicable to scientiﬁc problems, i.e. to those which have a well deﬁned answer, only if this answer
can be brought to a peculiar form G : X −→ Z with G((cid:126)x) expressed as a combination of iterated Heaviside functions. At present

it is far from obvious, if and when such representations exist, what are the obstacles and, if they are absent, what are the ways to
convert the known formulas into this form. This gives rise to a program of reformulation of ordinary science in such terms – which
sounds like a strong enhancement of the constructive mathematics approach, only this time it concerns all natural sciences. We
describe the ﬁrst steps on this long way.

1

Introduction

Machine Learning (ML) is a powerful method to solve classiﬁcation problems, i.e. to construct the mappings
X −→ Z from a set of sample maps. This is used to ﬁnd the answer ”by analogy”, and has wide applications
to image recognition and decision taking problems, which cover most of needs of the everyday life.

It is natural that people try to extend the range of applications of ML and include the true scientiﬁc
problems in it, what could promote computer applications to science far beyond the ordinary big data analysis
and computer experiments, often designated as computer physics. See [1] for the latest publication in arXiv and
for the chain of references therein. However, there is a natural obstacle to this project, at least in the framework
of the steepest descent method, included into the current methodology of ML.

In [2] we formulated an applicability condition for ML to scientiﬁc problems, i.e. to those which have a
well deﬁned (”objective”) answer: this answer should have a representation G : X −→ Z with G((cid:126)x)
expressed as a combination of iterated Heaviside functions. Then the steepest descent method is
applied to the coeﬃcients/parameters w, which can be easily introduced into this formula: the lifting

G((cid:126)x) −→ G((cid:126)x|w)

(1)

is straightforward. It is not uniquely deﬁned and depends on the number of parameters w one wishes to introduce
and release. In general, there is a gauge invariance, acting on w in G((cid:126)x|w), and one needs to ﬁx the gauge in
order to eliminate the extra parameters Also in transition to G one substitutes the sharp Heaviside function by
smoothed σ-functions, what helps to reduce the gauge freedom and is more convenient for practical computer
calculations. ML algorithm is to minimize w.r.t. parameters w the functional

L(w) :=

1
2

(cid:88)

α

(cid:12)
(cid:12)Z (α) − G
(cid:12)

(cid:16) (cid:126)X (α)(cid:12)
(cid:12)
(cid:12) w

(cid:17)(cid:12)
(cid:12)
(cid:12)

2

(2)

for a given set of association-inspiring sample maps/plots
large-time asymptotics of the solution to

(cid:16) (cid:126)X (α), Z (α)(cid:17)

, enumerated by α, i.e.

taking the

˙w = −

∂L
∂w

Our exact G is then supposed to arise after the substitution of the optimal values ¯w:

G((cid:126)x) = lim
σ→θ

G((cid:126)x| ¯w)

(3)

(4)

Usually one does not care about the choice of kinetic term at the l.h.s. of the Langevin equation (3) and it
breaks gauge invariance. The stable point and the answer G((cid:126)x) are gauge invariant in any case.

1

 
 
 
 
 
 
Of course, to get a new knowledge, one can not start from a given function G, but our goal at this stage
is more limited and, in a sense, inverse – to understand, what kind of the already known answers can be
reproduced by the steepest descent ML method. A necessary step for doing this is expression of these answers
through iterations of Heaviside functions – for the purposes of this paper we use a word ”Heavisidisation” to
denote this reformulation in terms of ”acceptable” formulas.

In this paper we further elaborate on the elementary examples of [2] and attempt to extend them further.
One of the goals (not fully achieved yet) is Heavisidisation of algebraic numbers. The question is if one can
express such numbers in terms of the coeﬃcients of the underlying polynomial equations (like Cardano formulas
and less explicit constructions for roots and/or discriminants of more complicated systems of equations) by
using only the iterations of Heaviside functions.

In sec.2 we begin the systematic description of elementary operations – the building blocks for more com-
plicated constructions, like solution of quadratic equation in sec.3. After that in sec.4 we consider the lifting
(1), emerging freedom (gauge invariance) and the ways to ﬁx it. Sec.5 contains a brief description of a real
program and its (limited) relation to the true steepest descent method. It can help to appreciate the amount
of human art, which needs to be applied even in the nearly trivial situations. The concluding sec.6 returns to
the philosophic level and discusses the issues which still remain to be put in a clear form at the next steps of
the Heavisidisation program.

2 Elementary Heavisidisation

2.1 Heaviside as logical and & or

Since Heaviside function θ(x) is going to play the central role in our analysis, we list here its basic properties
and their interpretation in traditional terms of pure science.

We deﬁne Heaviside function

Then

θ =

(cid:26) 1
0

if x > 0
if x ≤ 0

θ◦2(x) := θ(cid:0)θ(x)(cid:1) = θ(x)

Note that we ﬁnd it convenient to deﬁne θ(0) = 0.

Further, this function realizes logical operations AND and OR:

∧(a, b) := θ(cid:0)θ(a) + θ(b) − 1(cid:1) = 1

only if

θ(a) = 1 AND θ(b) = 1

∨(a, b) := θ(cid:0)θ(a) + θ(b)(cid:1) = 1

<=>

θ(a) = 1 OR θ(b) = 1

(5)

(6)

(7)

2.2 Arithmetic operations

Together with the results of [2], by now we have the following set of network building blocks (which may be
used/combined as sub-networks):

• Logical operations ∧, ∨ on inputs

• δn(x): 1 iﬀ x = n, deﬁned similar to zero of (x − n), i.e. for integer x

δn(x) = θ(x − n + 1) − θ(x − n)

• Identity

x = I(x) :=

∞
(cid:88)

i=0

θ(x − i) −

∞
(cid:88)

i=0

θ(−x − i) =

(cid:88)

i∈Z

(cid:16)

(sgn i) · θ

(sgn i)(x − i)

(cid:17)

(8)

(9)

for integers, which is easily promoted to rationals i.e. to an arbitrarily good approximation for reals. It
can be used to construct addition and multiplication of inputs:1

1Note that we changed the deﬁnition of θ(x) from [2] to (5), which makes it more functorial – thus the small modiﬁcations of

formulas for (10) and (12) as compared to that paper.

2

• Addition

For positive integers addition looks simpler:

x + y = I(x) + I(y)

x + y = I(x) + I(y) =

∞
(cid:88)

i=0

θ(x − i) +

∞
(cid:88)

j=0

θ(y − j)

• Multiplication

(10)

(11)

x · y =

∞
(cid:88)

(cid:16)

θ

i,j

θ(x − i) + θ(y − j) − 1

(cid:17)

=

∞
(cid:88)

i,j

∧(x − i, y − j)

(12)

Using these blocks we can also deﬁne

• Subtraction

and approximate

• Division

x − y = I(x) − I(y)

x/y =

(cid:18)

∧

x − i,

(cid:19)

δj(y)
j

(cid:88)

i,j

(which is worth comparing to multiplication expression (12)).

• Square root and roots of other degrees:

x1/n =

∞
(cid:88)

i=0

θ(x − in)

(13)

(14)

(15)

The speciﬁcs (or value) of expressions (10)-(12) is that the numeric operations on the left-hand side are ap-
proximated by the network of nodes for which the individual node output in principal may not be proportional
to the input (the range of θ is [0, 1]).

One can be surprised: what is the true meaning of (10)? The point is that it expresses ordinary addition
through operational/network addition. Note that (12) uses the same operation add, there is no a priori network
multiplication.

Figure 1 illustrates the stability of (9) as a retraction domain for the steepest descent training procedure.
Blue dots show the position of initial bias values, and orange are the values after training, apparently tending
to the line corresponding to bias values in (cid:80)∞
i=0 θ(x − i) of (9). The corresponding gradients calculation is
discussed in Section 4.2.

2.3 Zeros detection

In [2] we suggested a formula for a zero of a function f (x): roughly,

zero of f =

(cid:90)

dx

(cid:16)

θ

x

d
dx

(cid:17)

f (x)

(16)

It is directly applicable to monotonically growing function with a single zero. In this section we present more
accurate formula without these restrictions and its generalization to many variables.

Formulas like (16) can be considered as Heavisidisation of the equation solving – but they are not the ”clever”
formulas for the answers, like formulas of Cardano type of, say, Koshul-complex constructions of [3, 4]. The
latter ones require there own Heavisidisation, which we begin to describe in sec.3 below.

3

Figure 1: The (node number, bias value) plot of descent of initial bias values (blue) to the line (orange) expected
in (9) via TensorFlow training with ’sigmoid’ activation function (smoothed version of θ).

2.3.1

1D

For a lattice approximation (fi) of a continuous function f denote

δi(f ) := ∨(θ(fi+1) − θ(fi), θ(fi) − θ(fi+1)) = θ (cid:0)θ(cid:0)θ(fi+1) − θ(fi)) + θ(θ(fi) − θ(fi+1)(cid:1)(cid:1)

(17)

the indicator function being 1 iﬀ f has (odd number of) 0 somewhere between the nodes i and i + 1. The union
operation is used to take care of the gradient direction and handle both decreasing and increasing behavior in
the vicinity of zero. Degenerate zeroes are not accurately described in this lattice approximation.

2.3.2

2D

The indicator function, which checks if a function f on 2D lattice has 0 within the square (i, j), (i + 1, j + 1) is

δi,j(f ) = ∨ (δi(f•j), δj(fi•)) = θ(θ(δi(f•j)) + θ(δj(fi•)))

(17)

= θ (cid:0)θ (cid:0)θ (cid:0)θ(cid:0)θ(fi+1,j) − θ(fi,j)) + θ(θ(fi,j) − θ(fi+1,j)(cid:1)(cid:1)(cid:1)
+ θ (cid:0)θ (cid:0)θ(cid:0)θ(fi,j+1) − θ(fi,j)) + θ(θ(fi,j) − θ(fi,j+1)(cid:1)(cid:1)(cid:1)(cid:1)

(18)

where f•j denotes the restriction of the two-variables function onto 1-dimensional sub-lattice with ﬁxed j (similar
to f (·, y)). The union is used to take care of the case when f is constant along one of the lattice axes, i or j.

The indicator function checking if the functions f, g on 2D lattice both have 0 within the square (i, j), (i +

1, j + 1) is

δi,j(f, g) = ∧(δij(f ), δij(g)) = θ(θ(δij(f )) + θ(δij(g)) − 1)

(19)

The position of a joint zero of f, g may be approximated by the following network with 2-dimensional output



(cid:88)



ij

i
N

δi,j(f, g),



δi,j(f, g)



(cid:88)

ij

j
N

(20)

2.3.3 Examples

Relations like (16) are trivial if we use θ(cid:48)(x) = δ(x) but get a little tricky in other terms, especially in the
diﬀerence form. In linear case F (x) = x − b we can apply integration by parts:

b =

(cid:90) Λ

−Λ

xθ(cid:48)(x − b)dx = xθ(x − b)|Λ

x=−Λ −

(cid:90) Λ

−Λ

4

s(x − b)dx = (Λ − 0) − (Λ − b) = b

(21)

for suﬃciently big ultraviolet cutoﬀ Λ > |b|. In terms of discrete summation

b =

Λ
(cid:88)

i=−Λ

i
N

(cid:26)

θ

(cid:18) i + 1
N

(cid:19)

− b

− θ

(cid:18) i
N

(cid:19)(cid:27)

− b

=

Λ
N

· θ

(cid:18) Λ + 1
N

(cid:19)

− b

+

Λ
(cid:88)

i=−Λ

(cid:18) i − 1
N

−

i
N

(cid:19)

(cid:18) i
N

θ

(cid:19)

− b

=

=

Λ
N

· θ

(cid:18) Λ + 1
N

(cid:19)

− b

−

1
N

Λ
(cid:88)

i=−Λ

(cid:18) i
N

θ

(cid:19)

− b

=

Λ
N

−

(cid:18) Λ
N

(cid:19)

− b

= b

(22)

(in fact there is a 1/N correction for this choice of discretisation).

Likewise for quadratic case we get

∞
(cid:88)

i=−∞

i
N

(cid:32)

(cid:40)
θ

·

i + 1
N

b + c +

(cid:18) i + 1
N

(cid:19)2(cid:33)

(cid:32)

i
N

− θ

b + c +

(cid:18) i
N

(cid:19)2(cid:33)(cid:41)

(23)

This corresponds to the network with 2 inputs, b and c. We can make further transforms to reproduce (15)
with n = 2:

= lim
Λ→∞

Λ
(cid:88)

i=−Λ

i
N

(cid:40)

(cid:32)

·

θ

c −

(cid:19)2

(cid:18) b
2

(cid:18) i + 1
N

+

b
2

(cid:19)2(cid:33)

(cid:32)

− θ

c −

=

Λ
N

θ|i=Λ −

−Λ
N

Λ
(cid:88)

θ|i=−Λ +

i − 1
N

· θ

−D/4 +

(cid:18) i
N

+

b
2

(cid:19)2(cid:33)

−

+

(cid:32)

(cid:19)2(cid:33)(cid:41)

D:=b2−4c
=

(cid:19)2

+

+

b
2

(cid:18) i
N
(cid:32)

(cid:18) b
2

Λ
(cid:88)

· θ

−D/4 +

i
N

(cid:18) i
N

+

b
2

=

2Λ
N

Λ
(cid:88)

θ|i=Λ +

−

1
N

i=−Λ
(cid:32)

θ

−D/4 +

(cid:18) i
N
√
=

r:=N

D/4

i=−Λ

(cid:32)

θ

−D/4 +

(cid:19)2(cid:33)

(cid:18) i(cid:48)
N

=

2Λ
N

θ|i=Λ −

1
N

Λ
(cid:88)

i=−Λ

=

2Λ
N

(cid:19)2(cid:33)

+

b
2

=

2Λ
N

θ|i=Λ +

(cid:32)

θ

−D/4 +

1
N

(cid:18) i + N b/2
N

i=−Λ
Λ
(cid:88)

−

i=−Λ

2Λ
N

θ|i=Λ −





1
N

−r
(cid:88)

r
(cid:88)

+

+

i=−Λ
(cid:18) i(cid:48)
N

i=−r
(cid:19)2(cid:33)

=


 θ

Λ
(cid:88)

i=r

(cid:32)

−D/4 +

(cid:32)

θ

D/4 −

1
N

r
(cid:88)

i=−r

(cid:18) i(cid:48)
N

(cid:18) i(cid:48)
N

θ|i=Λ −





1
N

0
(cid:88)

Λ
(cid:88)


 θ(1) +

+

i=−Λ

i=1

1
N

r
(cid:88)

θ

i=−r

(cid:32)

D/4 −

(cid:19)2(cid:33)

(cid:19)2(cid:33)

(cid:19)2(cid:33)

(cid:19)2(cid:33)

This is a ﬁnite sum, which is indeed (according to (15)) equal to

(24)
Here at the intermediate steps we introduced notations D := b2 − 4c, r := N (cid:112)D/4. For the steps 2 and 5 of
our transformations we use the translation of i.

b2 − 4c

= 2(cid:112)D/4 =

(cid:112)

2.4 Sector functions

2.4.1 Example: 1D

Take an example when X = R1 and Z = {0, 1}, i.e. we classify points of real line as belonging to 2 possible
classes. The mapping which we want to approximate by our network looks as:

g : X →






0, x < 2
1, 2 ≤ x ≤ 3
0, x > 3

i.e. points in the segment [2, 3] belong to the class 1, and the rest to 0.
Lets take a level-1 network. The output value has the general form

G =

(cid:88)

i

wi

1σ(wi

0x + ξi

0) + ξ1

(25)

(26)

If we take σ to be a Heaviside function, then we can indeed express the well known answer in the following
form:

This means that we can make tie network of layer-1 with 2 cells which is going to converge to the values:

G(x) = θ(x − 2) − θ(x − 3) + 0

¯w1

0 = ¯w2

0 = 1, ¯ξ1

0 = −2, ¯ξ2
¯w1

1 = − ¯w1

0 = −3
1 = 1

(27)

(28)

One can see that this way we can actually express any step function on the line, which solves the classiﬁcation

problem for the 1-dimensional conﬁguration space X.

5

2.4.2 Example: 2D

For simplicity consider not an arbitrary and limited domain, but just a sector on the plane X = R2. This means
that the mapping we want to describe is the characteristic function of a sector:

g : (x1, x2) (cid:55)→

(cid:26) 1
0

for x1, x2 > 0
otherwise

Solution is provided by the level-2 network. In general for such network

G =

(cid:88)

j

(cid:88)

wj

2θ(

i

wji

1 σ(wi1

0 x1 + wi2

0 x2 + ξi

0) + ξj

1) + ξ2

The approximation to our characteristic function gets exact for σ = θ:

G(x1, x2) = −θ

(cid:16)

θ(−x1) + θ(−x2) − 1

(cid:17)

+ 1

(29)

(30)

(31)

When both x1 and x2 are positive, the argument of external θ is −1, and y = 1. When any of x1 or x2 is
negative the argument is 0 or 1, and y = 0.

Thus the relevant network is layer-2 with 2+1 cells and converges to

¯w11

0 = ¯w22
0 = −1
¯w11
1 = ¯w12
1 = 1
2 = −1, ¯ξ2 = 1
¯w1

(32)

Combination of sector characteristic functions may provide an approximation (with precision depending on

the number of network cells) to any step function on the plane.

2.4.3 General classiﬁcation/approximation problem

Let us take a ”modiﬁed” Heaviside function with θ(0) = 0. Then it becomes a well-deﬁned operation with the
property

For approximation of the characteristic of (n + 1)-dimensional sector

θ ◦ θ = θ

we have the 2-layer network

x0, x1, . . . , xn > 0

G(x) = θ

(cid:33)

θ(xi) − n

(cid:32) n
(cid:88)

i=0

(33)

(34)

Note, that the property (33) allows us to re-write 1D example above as a particular case of 2-layer formula

(34). For 1-d sector x0 > 0, i.e. for n = 0:

G(x) = θ(θ(x0))

and then characteristic (27) of the segment [2, 3] looks as:

G(x) = θ(cid:0)θ(x − 2) − θ(x − 3)(cid:1)

Since a general multivariate step function may be approximated by (a linear combination of ) characteristics of
sectors and a any multivariate function may be approximated by (a linear combination of ) step functions then
2-layer networks provide a solution for a general approximation problem.

2.5 Non-Picassian networks

Using sectoral functions as building blocks for approximation makes it pretty adequate for approximating step
functions whose plots would look as a sample of spacial cubism art. This apparent limitation is due to the
fact that in the Heaviside operators of layer n + 1 we use linear combination of values of layer n. Having that
formulated immediately suggests some generalizations.

6

2.5.1 The role of bias

Lets add to each layer l of our network an extra node νl. And lets make out of those nodes a trivial ”identity”
sub-network

ν0 ≡ x0 = 1,

νl = θ◦l(ν0) = 1

Then our initial general form network with n-dimensional input (x1, . . . , xn)

Gjl+1(x) =

nl(cid:88)

jl=1





W jl+1jl
l

· θ

. . . θ



n1(cid:88)

W j2j1
1

· θ

j1=1

(cid:16) n
(cid:88)

i=1

W j1i

0 xi + ξj1

0

(cid:17)

+ ξj2
1


 · · · + ξjl+1

l





may be rewritten using the extension by the identity sub-network in homogeneous form as





W jl+1jl
l

· θ

. . . θ



nl(cid:88)

jl=0

n1(cid:88)

j1=0

W j2j1
1

· θ

(cid:16) n
(cid:88)

i=0

(cid:17)





 . . .



W j1i

0 xi

where all the bias terms are replaced by links to the identity sub-network

(35)

(36)

W j0

l νl = bj

l

After adding that extension to the layer, we may certainly omit the restrictions for the values of νl and W j0
l
when applying the ML process on computer, just remembering that our network with bias (35) is a particular
case of (36).

2.5.2 World getting smoother

The next generalization step would be to use a higher degree expressions of output Y(l) from layer l for the
input X(l+1) for the layer l + 1. Using more common tensor algebra notations and assuming summation for
repeated indexes:

xj = W (l),j
i1...id

yi1 . . . yid

(37)

Denote xI := xi1 · . . . · xid. Then using multi-index notations we can write our network in a rather symbolic

(but still recoverable to the original notations) form:

Gjl+1(x) = W (l),jl+1

Jl

· θ

(cid:16)

. . . W (2),J3
J2

· θ

(cid:16)

W (1),J2
J1

· θ

(cid:16)

W (0),J1
I

(cid:17)(cid:17)

xI )

(cid:17)

. . .

(38)

As in the case of bias, we can note that the network (36) is a specialization of (37) to the particular case of

y0 = νl = 1

(39)

and

W (l),j
i1...id
for more then 1 non-zero indexes in i1 . . . id. To go to the generalization it is enough just to omit this restriction
in concrete calculations.

= 0

The use of the above form of connection between layers dramatically changes the opportunities of approx-
imating step functions with smoother support boundaries. For instance to get an exact presentation of a
characteristic function of a 2D ellipse

ax2

1 + bx2

2 < 1

it is enough to use a 1-layer network with a single node

θ(ν2

0 − (ax2

1 + bx2

2))

(40)

(remember ν0 ≡ 1)while the approximation by sectoral functions would need a 2-layer network with tens or
hundreds of nodes (depending on the required precision).

7

2.5.3 Characteristic functions of bounded domains

In fuller generality, a domain F ((cid:126)x) < 1 has an acceptable characteristic function

If we have a domain, which is unit of two, F ((cid:126)x) < 1 and G((cid:126)x) < 1, then we can use (7) to claim that the
characteristic function is given by a two-layer expression

(cid:16)

θ

1 − F ((cid:126)x)

(cid:17)

(41)

(cid:16)

(cid:16)

θ

θ

F ((cid:126)x) + G((cid:126)x)

(cid:17)(cid:17)

if it is their intersection then the characteristic function is

(cid:16)

(cid:16)

θ

θ

F ((cid:126)x) + G((cid:126)x) − 1

(cid:17)(cid:17)

Can we also describe the boundary?

3 Towards algebraic numbers

3.1 Solving quadratic equation

(42)

(43)

Now we can attempt to solve the ﬁrst non-trivial problem: quadratic equation. The alternated sum of ze-
ros (cid:82) ∂θ(F (x))
zdz in case of F being quadratic polynomial has a concise analytic form (as a speciﬁc case of
Vandermonde determinant)

∂z

(cid:90) ∂θ(z2 + bz + c)
∂z

zdz =

(cid:112)

b2 − 4c

Hevisidization of both parts looks as

∞
(cid:88)

i=−∞

(cid:40)
θ

·

(cid:32)(cid:18) i + 1
N

i
N

(cid:19)2

+

i + 1
N

(cid:33)

b + c

− θ

(cid:112)

=

b2 − 4c

(15)
=

(cid:32)

θ

b2 − 4c −

1
N

∞
(cid:88)

i−0

(cid:18) i
N

(cid:33)(cid:41)

b + c

=

+

i
N

(cid:90) ∂θ(z2 + bz + c)
∂z

zdz

(cid:19)2

(cid:32)(cid:18) i
N
(cid:19)2(cid:33)

(12)
=

1
N

∞
(cid:88)

i−0

θ



(cid:88)



θ

j,k

(cid:16)

θ(b − j) + θ(b − k) − 1

(cid:17)

− 4c −

(cid:19)2


(cid:18) i
N

(44)

(45)

At the r.h.s. we consider −4c just as a number, to simplify the formula – it is easy the include it by the rules
of sec.2, but this will raise the number of layers, what will distract us from the main line of reasoning at this
moment.

When learning, the net can discover any side of the equation (45). However, one (left) is trivial deﬁnition
of the diﬀerence of zeroes, while the other (right) is an important scientiﬁc formula. How to teach the net to
distinguish? And to discover the r.h.s.?

Note, that without knowing the originating equation (44) we could prove the equality of both ends of (45)

following (24).

3.2 Ambiguity of Heavisidisation

In fact, already b2 can be deﬁned in diﬀerent ways, e.g.

b2 =

∞
(cid:88)

θ

i,j=0

or

(cid:16)

θ(b − i) + θ(b − j) − 1

(cid:17)

b2 =

∞
(cid:88)

i=0

√

i)

θ(b −

In this case it is more diﬃcult to say what is trivial, and what is the law of nature.

8

(46)

(47)

3.3 Architecture of the network

In computer science the diﬀerence between these formulas is associated with the ”architecture” of the network.
Thus we see, that the ”laws of nature” instead of ”trivialities” arise only for appropriate architecture choice(?!)
In other words, the diﬀerence has no sense for the computer(?!) – at least, it remains to understand,
what it is.

Another obvious question is what if we use architecture, sophisticated enough to incorporate both formulas?

3.4 Cardano formulas for cubic and quartic equations

The somewhat mysterious use of (16) in arriving to discriminant by means of Heaviside functions in Section
3.1 is essentially due to the fact that for quadratic polynomial the diﬀerence of roots (calculated by (16)) is a
particular case of Vandermonde determinant (discriminant). It is still a question whether we should expect any
algebraic outcome in case of degrees 3, 4.

3.5 Beyond Cardano

Even more interesting is what happens if we take polynomial of degree 5 and higher? The analogue of the l.h.s.
of (45) continues to exist, what if there is also a clever r.h.s. Perhaps, it exists(?), just ”inverse Hevisidization”
does not work in a simple way – the emerging combination of Heavisides (if any) can not be written in terms
of roots?

4 Steepest descent method for elementary examples

4.1 Linear function

According to (11) and (15), for

L =

(cid:88)

K

L2

K =

(cid:88)

K

(cid:32)

b1/n
K −

(cid:88)

i

(cid:33)2

Wi · θ(bK − ξn
i )

we need to get the answer (stable point)

¯Wi = 1,

¯ξi = in

These should come as solutions (limiting points) of the steepest descent equations

˙Wi = −

(cid:88)

LK · θ(bK − ξn
i )

K
LK · Wi δ(bK − ξn
i )

˙ξi = −

(cid:88)

K

(48)

(49)

(50)

Of course, LK = 0, which is an identity for (49), provides a solution – but we need to solve these equations
without knowing the answer. Already for n = 1 it is a non-trivial task.

Let us specify the problem: restrict it to n = 1 and to integer b. Then the maximally big teaching set
consists of pairs (K, bK = K). In this case further reduction trivializes the problem: if we ﬁx all ξi = i then Wi
will be deﬁned by minimization of

L =

1
2

(cid:88)

K

(cid:32)

K −

(cid:33)2

K−1
(cid:88)

i=0

Wi

(51)

with a sum over positive integers K (the sample set at n = 1, {bK = K}). This obviously implies ¯Wi = 1 –
this is what makes all the items in the sum of squares vanishing. Still the steepest descent equations (3) even
in this oversimpliﬁed case are rather complicated. Simple are combinations like

˙W0 − ˙W1 = 1 − W0
˙W1 − ˙W2 = 2 − W0 − W1
˙W2 − ˙W3 = 3 − W0 − W1 − W2
. . .

9

(52)

(what already calls for a more clever deﬁnition of the kinetic term at the l.h.s.)

At the same time, releasing ξi is not a truly good idea: clearly suﬃciently small deviations ξi from i are
allowed, it is enough that, say, i − 1 < ξi ≤ i for all i. One can ﬁx this ambiguity either by going to dense sets of
K (like rationals, with appropriate rescaling of i), or by smoothing the sharp Heaviside functions θ(x) −→ σ(x).
Thus one should be careful in precise formulation of the optimisation problem and deﬁning the
allowed type/range of adjusting parameters.???

Another point is that making the teaching set incomplete, breaks the validity of the answer irreversibly. For
example, if we omit just the single sample value at K = 2, this will eliminate W1 and the second line from (52).
This, in turn, will shift the solution to ¯W3 = 2, without changing all the other ¯W0 = ¯W2 = ¯W3 = . . . = 1. In
other words, in this way we teach that

K ≈ θ(K) + 2θ(K − 2) +

∞
(cid:88)

i=3

θ(K − i)

what is not true for K = 2, in variance of correct answer

K =

∞
(cid:88)

i=0

θ(K − i)

(53)

(54)

In other words, the net does not want to guess the omitted data, or, better to say, guesses it wrongly.
One can of course search for a better net, withe better associations, but this where science (exact knowledge)
stops and gives room to an art.

4.2 A more formal presentation

Let us train our 1-level network

(cid:88)

y =

θ(wix + ξi)

i

on 2 input-output (x, y) samples - (a, a) and (b, b), for a, b being positive integers.

The functional to minimize is:

L =

(cid:32)

(cid:88)

i

(cid:33)2

θ(awi + ξi) − a

+

(cid:32)

(cid:88)

i

(cid:33)2

θ(bwi + ξi) − b

= (y(a) − a)2 + (y(b) − b)2

Then

∂L
∂bi

= 2δ(ξi + awi)(y(a) − a) + 2δ(ξi + bwi)(y(b) − b)

Take the starting network state:

Then

w0

i = 1, ξ0

i = 0

∂L
∂ξi

(cid:12)
(cid:12)
(cid:12)
(cid:12)wi=1,ξi=0

= 2δ(a)

(cid:32)

(cid:88)

i

(cid:33)

θ(a) − a

+ 2δ(b)

(cid:33)

θ(b) − b

(cid:32)

(cid:88)

i

(55)

(56)

(57)

(58)

(59)

since for x > 0 we have (cid:80)
positive)

i θ(x) = +∞ > 0 then (assuming for the smoothed variant of δ() to be everywhere

∂L
∂bi

(cid:12)
(cid:12)
(cid:12)
(cid:12)wi=1,ξi=0

> 0

(60)

i.e. ξi should be diminishing.
Take the starting state:

Then

w0

i = 1, xi0

i = −2i

θ(a − 2i) − a =

a
2

− a = −

a
2

(cid:88)

i

10

and

∂L
∂bi

(cid:12)
(cid:12)
(cid:12)
(cid:12)wi=1,ξi=−2i

= 2δ(−2i + a)

(cid:16)

−

(cid:17)

a
2

+ 2δ(−2i + b)

−

(cid:18)

(cid:19)

b
2

= −(aδ(−2i + a) + bδ(−2i + b)) < 0

(61)

i.e. bi is supposed to be increasing.

Then it is not surprising that in general our network is going to converge to the state

¯wi = 1, ¯ξi = −i

which corresponds to the ”identity” transform I(x).

4.3 Powers

We can now switch from linear function to the simplest power. As a lifting of (47) we can take, say,

k2 =

∞
(cid:88)

i=0

Wiθ(k − ξi)

The desired stable point (47) is

However, there are many more. Even if one ﬁxes ξi =

¯Wi = 1,

√

i

¯ξi =
√

i, what we need to minimize is

L =

(cid:16)

1
2

(12 − W0)2 + (22 − W0 − W1 − W2 − W3)2 + . . .

(cid:17)

and the ﬁxed points are just

¯W0 = 1
¯W0 + ¯W1 + ¯W2 + ¯W3 = 4
. . .

(62)

(63)

(64)

(65)

what does not allow to ﬁx all the individual ¯Wi: we get ¯W1 + ¯W2 + ¯W3 = 3, not necessarily ¯W1 = ¯W2 = ¯W3 = 1.
Releasing ξi only worsens the situation.

Note that this is not the ambiguity, discussed in (3.2) – between the diﬀerent Hevisidisations of the answer.

Now we face the problem for a given answer (ﬁxed point) – and this is additional ambiguity of the lifting [?].

4.4 Continuous case and the gauge freedom

The simplest way to analyze this problem is to substitute our discrete examples by a continuous one with i
substitute by a continuous parameter z, say,

g(k) =

(cid:90) ∞

0

(cid:16)

W (z)θ

(cid:17)

k − ξ(z)

dz

(66)

Integral is invariant under change of variables. We can ﬁx this freedom by choosing either ξ(z) of W (z)/ or by
imposing any desired relation between them. In other words, there is always a 1-parametric set of equivalent
formulas of the type (66), and no one of them is truly distinguished. This makes the status of particular choice
like (15),

kα =

(cid:90) ∞

0

(cid:16)

k − z1/α(cid:17)

θ

dz

(67)

(proved by taking the k-derivative) somewhat unclear/shaky.

In other words, there is a gauge invariance, which need to be ﬁxed. The question is where is this step in the
standard formalism, and what are canonical??? ways of this gauge ﬁxing. Langevin equation (3) breaks this
gauge invariance, but it is restored at the ﬁxed points ¯w (extrema of L).

Additional problem is that the kernel of Heaviside transform has extra freedom,

θ(eu(z)z) = θ(z)

11

(68)

(just adding no extra zeros, besides z = 0) unlike the kernels of, say, Fourier or Mellin transforms. This
additional gauge freedom is ﬁxed by smoothing θ −→ σ, ??? but in the study of Heavisidisation it should be
also taken into account.

Iterated Heaviside transforms are less familiar:

(cid:90)

g(k) =

W2(z2) · θ

(cid:26)(cid:90)

W1(z1, z2) · θ

(cid:16)

k − ξ12(z1, z2)

(cid:17)

(cid:27)

dz1

dz2

(69)

and what is the way to ﬁnd and ﬁx the gauge freedom here?

One can also lift ordinary integrals to matrices (Heaviside matrix models).
Still another issue is that kinetic term at the l.h.s. of (3) does not respect gauge invariance. In fact, already
the simplest example (52) demonstrates that (3) can be not the most adequate choice even without this diﬃculty.
A clever choice of kinetic term is not needed for practical applications of ML, but it is clearly desired for a truly
scientiﬁc approach.

5 TensorFlow as a commonwealth of artiﬁcial and human

Romantically, one imagines artiﬁcial intelligence as a program, which gets a data set as input and produces an
answer in the output. In practice it is often far from this ideal – especially in application to ”scientiﬁc problems”,
which we discuss in this paper. Formally, the diﬃculty is to get onto the ”right” orbit, which converges to the
true answer – if at all exists in the suggested architecture. So far we concentrated on the last issue – emphasized
the need for the true answer to allow Heavisidisation. However, in the previous section we saw, that this is
not enough – even if the true answer can be expressed through iteration of Heaviside functions, the steepest
descent method does not obligatory allow to ﬁnd it, starting from generic initial conditions. In this section we
brieﬂy remind how the actually-existing programs work and explain why they are in fact inﬁnitely far from the
artiﬁcial-intelligence dream. The crucial watershed between human and artiﬁcial intelligence is not crossed yet
and the crucial decisions are still made by humans. Computer still only helps.

We take as an example the widespread TensorFlow program and illustrate the way it solves our elementary
problems, Heavisidised in the previous sections. Unlike mathematical formulation (2) assuming existence of
some minimizing subspace of network states, practical calculation at each step uses for computing the gradient
only one of training examples, makes a single timid step in the corresponding direction and then iterates with
the rest of samples set.
In other words, instead of making the step satisfying each of examples it makes a
sequence targeting samples successively.

Such procedure allows one not to be occupied with the question of compatibility of gradient descent condi-
tions, in particular with the fundamental question which size of the network is suitable for having a reasonable
solution. And as a payback we have no deﬁnite idea on when the training procedure should/may be ﬁnished
(delegating it to the ’epochs’ parameter), or even regarding its potential convergence, which makes the role of
human experience and a proper training scenario indispensable.

5.1 Example

For illustration we take a TensorFlow model (Python code is attached at the end of this section) which we
already used in getting the Fig.1. However, this time we describe the actual procedure in a more realistic way,
and explain that the nice picture in Fig.1 is in fact a result of a clever human work, not a blind application
of a well-deﬁned algorithm.
In our oversimpliﬁed example ”clever” is quite straightforward – the secret is
to use a homogeneous distribution of training examples. Here we just show what happens, if instead some
special/deformed distributions are taken. Still this illustrates the problem, which is quite general and requires
serious human insight in more sophisticated situations where the criteria of homogeneity are not that obvious.
Of course, this is not a surprise for people familiar with the true applications of ML, but it is not suﬃciently
well known and appreciated by the community of pure scientists, who did not (yet) use ML in their work.

So, we look at the simplest possible 1-layer network

with 10 nodes in the layer. Making use of above mentioned ”gauge invariance” we normalize the initial values
of W to 1 and take the initial state of bias as

10
(cid:88)

i=1

θ(x + bi)

bi =

(cid:26) i + 1, 1 ≤ i ≤ 5
i − 1, 6 ≤ i ≤ 10

12

Figure 2: The (node number, bias value) plot as a result of TensoFlow model.ﬁt() calculation (orange) with
the same initial state (blue) of {bi, i = 1 . . . 10} using training sequences [2, 5, 5, 5, 5] and [2, 2, 2, 2, 5]
correspondingly.

being a deformatiion of (9). We want to train our network on examples of identity map x (cid:55)→ y, using the set of
2 examples:

(x, y) = (2, 2) and (5, 5)

For the ﬁrst training session we submit those examples to our network in a sequence with inputs [2, 5, 5, 5, 5],
and for the second session we use the sequence [2, 2, 2, 2, 5]. In each session we iterate the submission 2000 (the
value of ’epochs’ parameter) times. The resulting values of bi are shown in Figure 2 as orange dots compared
to initial blue.

In case if TensorFlow was doing the descent following the system of equations corresponding to the set of
examples (as mathematical formulation suggests) we would certainly arrive in the deterministic way to the
same state of bi in both cases (making the same number of steps from the same initial state). But since each
elementary step is actually made based on the currently picked example from the sequence provided by a human,
it makes our network to travel in the space of states according to our training scenario.

To summarize, we can split the work of TensorFlow into the following steps:

1. choose a network conﬁguration, the set of layers

2. take initial state of w and b (from standard variants or manually)

3. pick an example according to the training set sequence (provided by human)

4. compute the descent gradient (for selected example) according to (2)-(3) (handled by TensorFlow with

possible custom activation function)

5. change w and b accordingly

6. iterate steps 3.-5.

Human interference is needed at the steps 1-3. Moreover, it may be unavoidable – if machine makes the
choice in 2, or we provide an inappropriate sequence of training example in 3, the process may never converge
to the desired values, training will not work.

5.2 Python listing

Below is the TensorFlow calculation from example of Section 5.

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

import math
import random
from matplotlib import pyplot

# NETWORK CONFIGURRATION/INITIALIZATIION

13

NCell = 10

binit = [None]*NCell
for i in range(NCell):

mv = 2 if i < NSample/2 else -2
binit[i] = -i + mv

binit = tf.keras.initializers.Constant(binit)

model = Sequential()
model.add(Dense(NCell, activation = ’sigmoid’, kernel_initializer = ’ones’, bias_initializer = binit))
model.add(Dense(1, kernel_initializer = ’ones’, bias_initializer = ’zeros’, trainable=False))
model.compile(loss=’mean_absolute_percentage_error’, optimizer=’adam’, metrics=["mse", "mae"])

# TRAINING
NStep = 2000
# training scenario 1
model.fit([2,5,5,5,5,5], [2,5,5,5,5,5], epochs=NStep, verbose=0)
# training scenario 2
#model.fit([2,2,2,2,2,5], [2,2,2,2,2,5], epochs=NStep, verbose=0)

# OUTPUT
abias = sbinit
pyplot.scatter(x, abias)

aweight=model.layers[0].weights[0].numpy()[0]
abias=model.layers[0].weights[1].numpy()
for i in range(NSample):

abias[i]=abias[i]/aweight[i]

abias.sort()
pyplot.scatter(x, abias)

5.3 TensorFlow and Heaviside

If we look at the list of standadard activation functions (analogues of θ) oﬀered by TensorFlow, we won’t see
Heaviside step function among them. Although it may be explained by diﬃculties for numeric diﬀerentiation,
there still remains a question of the impact of smoothering into results of training.

To study that we take a network with 2D input and 2 layers having 3 and 30 nodes correspondingly, and

custom sigmoid activation function

1
1 + exp(−20x)

being pretty close (in the uniform metric) to Heaviside.

We train it on 40 examples of the map:

[0, 2] × [0, 2] → R, (b, c) (cid:55)→ b2 + c

After 4000 iterations of passing through the set of training examples (epochs = 4000) it produces pretty

good matching (orange) with exact (blue) values on Figure 3.

Now we compute the output values produced by the network having the same values of w and b (which we
obtained after training) but instead of sigmoid having Heaviside as activiation function. The results (orange) are
compared to the output of our original network (blue) on Figure 4. We can see that even a slight smoothering
results in signiﬁcant discrepancy with Heaviside case already at the 2nd layer.

As a summary of experience with these examples, we can say that using Heaviside step function enables us
to approach to analytic calculations of the expected structure (values of W and b) of our network, in particular
- to estimate the required number of layers and nodes in there, deducing it from the algebraic structure of the
transform/mapping/function being approximated by the network. Numeric gradient descent calculations using
smoothed versions of Heaviside result in blurring of the structure, while in reward we are automatically (as a
mere consequence of smoothness) getting an extrapolation of training values to the whole regions in between
(which may look pretty impressive in cases when the approximated map really admits extrapolation).

14

Figure 3: (sample number, f -value) plot for 20 samples of testing (blue) vs. prediction (orange) values produced
by a 2-layer network trained on the map: f (b, c) = b2 + c

Figure 4: (sample number, f -value) plot for 20 samples of network output, trained for the map f (b, c) = b2 + c
with sigmoid activation (blue), compared to the output computed for the same weights and biases, but with
Heaviside activation (orange).

6 Conclusion

To conclude, in this paper we made a next step in Heavisidisation program, posed in [2]. We demonstrated
that it is much more natural and easy-going than it could originally seem – we explained how to get iterated-
Heaviside analogues of elementary operations and how to combine into answers for more serious problems. Also
we classiﬁed the ambiguities of Heavisidisation itself and additional gauge freedom in its lifting to ML problem.
Now we can revisit the original questions about the epistemology , raised in [2], and formulate them (just)

a little better. In short there are three groups of questions.

The ﬁrst is quite constructive:
A) Heavisidisation of various problems and answers to them, i.e. ﬁnding the analogues of the sides of (45)

for more interesting constructions and theorems.

The other two are still conceptual:
B) What it means to ﬁnd an algebraic formula (through coeﬃcients of F (x)) for above answer, i.e. the r.h.s.

of (45) for a given l.h.s.? Whether and what one needs to assume about the shape of this answer?

C) How to distinguish between reasonable (acceptably short) and unreasonable answers? Is there a true

diﬀerence between these two notions?

The main question after all that is what is the reasonable answer? One can hope to ﬁnd, say, an answer for
discriminant just as a polynomial in coeﬃcients – but it is inﬁnitely complicated, not the one which we really
wish. Humans want/need something like (51) of [5], not the 2000-item expression in the appendix to [4] (book
version).

But how can we distinguish between them without applying the human mind and taste?
And will a computer be ever able to distinguish?
Can the machine ever develop a scientiﬁc taste, similar to the human’s one?
Does this human taste have any ”objective” meaning or is just a pure reﬂection of the phyical/biological

structure of our brain?

Does this structure prevent us from discovering really deep laws of nature?
Or is it the one, exactly suited to complexity of these laws?

(In religious terms, if the human’s mind is

exactly of the same kind as Creator’s)?

This set of questions is a standard dilemma in computer physics, see, for example, a popular review in [6].

15

This paper teaches us that the answers to seemingly complicated questions can actually be nearly trivial.
Perhaps, this is true also for the level (quality) of scientiﬁc answers. Say, ”deeper” answers can be associated
with more layers (iterations of Heaviside functions) – at least this is an obvious diﬀerence between the two sides
of our sample example (45). In fact, for a given number of layers the attraction orbits modulo gauge invariance
are relatively few. They are invariants of the ”group” which reshuﬄes nodes in a given architecture. However
what expresses scientiﬁc knowledge in this paradigm, are connections between formulas at diﬀerent levels, and
they can be more sophisticated and interesting.

Acknowledgements

This work is partly supported by the Russian Science Foundation (Grant No.21-12-00400).

References

[1] Ch.Parka, Chi-Ok Hwangb†, K.Choa and Se-Jin Kim, arXiv:2205.04445

[2] V.Dolotin, A.Morozov and A.Popolitov, Machine Learning of the well known Things, arXiv:2204.11613

[3] I.Gelfand, M.Kapranov, and A.Zelevinsky, Discriminants, Resultants, and Multidimensional Determinants,

Birkh¨auser, Boston, MA, 1994

[4] V.Dolotin and A.Morozov, Introduction to Non-Linear Algebra, WS; hep-th/0609022

[5] A.Morozov and Sh.Shakirov, New and Old Results in Resultant Theory, Theor.Math.Phys. 163 (2010) 587,

arXiv:0911.5278

[6] D.Ruelle, Post-human Mathematics, arXiv:1308.4678

16

