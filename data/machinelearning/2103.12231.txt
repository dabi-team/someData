1
2
0
2

r
a

M
2
2

]
E
N
.
s
c
[

1
v
1
3
2
2
1
.
3
0
1
2
:
v
i
X
r
a

On the Role of System Software in Energy
Management of Neuromorphic Computing

Twisha Titirshaâˆ—
tt624@drexel.edu
Drexel University
Philadelphia, PA, USA

Adarsha Balajiâˆ—
ab3586@drexel.edu
Drexel University
Philadelphia, PA, USA

Shihao Songâˆ—
ss3695@drexel.edu
Drexel University
Philadelphia, PA, USA

Anup Das
anup.das@drexel.edu
Drexel University
Philadelphia, PA, USA

Abstract
Neuromorphic computing systems such as DYNAPs and
Loihi have recently been introduced to the computing com-
munity to improve performance and energy efficiency of
machine learning programs, especially those that are imple-
mented using Spiking Neural Network (SNN). The role of
a system software for neuromorphic systems is to cluster
a large machine learning model (e.g., with many neurons
and synapses) and map these clusters to the computing re-
sources of the hardware. In this work, we formulate the
energy consumption of a neuromorphic hardware, consid-
ering the power consumed by neurons and synapses, and
the energy consumed in communicating spikes on the inter-
connect. Based on such formulation, we first evaluate the
role of a system software in managing the energy consump-
tion of neuromorphic systems. Next, we formulate a simple
heuristic-based mapping approach to place the neurons and
synapses onto the computing resources to reduce energy
consumption. We evaluate our approach with 10 machine
learning applications and demonstrate that the proposed
mapping approach leads to a significant reduction of energy
consumption of neuromorphic computing systems.

CCS Concepts: â€¢ Hardware â†’ Neural systems; Emerg-
ing languages and compilers; Emerging tools and meth-
odologies; â€¢ Computer systems organization â†’ Data
flow architectures; Neural networks.

âˆ—Authors contributed equally to this research.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Â© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00
https://doi.org/10.1145/1122445.1122456

Keywords: Spiking Neural Network (SNN), Neuromorphic
Computing, Non Volatile Memory (NVM), Energy Consump-
tion, Static Power, Dynamic Power

ACM Reference Format:
Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das. 2018.
On the Role of System Software in Energy Management of Neuro-
morphic Computing. In Computing Frontiers â€™21: ACM International
Conference on Computing Frontiers, May 11â€“13, 2021, Virtual Confer-
ence. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/
1122445.1122456

1 Introduction
Neuromorphic computing describes the VLSI implemen-
tation of the neuro-biological architecture of the central
nervous system [6, 14, 49]. Neuromorphic systems are en-
ergy efficient in executing Spiking Neural Networks (SNNs),
which are considered as the third generation of neural net-
works [47]. SNNs use spike-based computations and bio-
inspired learning algorithms in solving machine learning
problems. In an SNN, pre-synaptic neurons communicate in-
formation encoded in spike trains to post-synaptic neurons,
via the synapses. Performance of an SNN-based application
can be assessed in terms of the inter-spike interval (ISI cod-
ing) or mean firing rate of the neurons (rate coding).

The hardware architecture of neuromorphic systems con-
sists of neurosynaptic cores, which are interconnected via a
shared interconnect [7]. Figure 1 illustrates the representa-
tive hardware architecture of many recent neuromorphic sys-
tems such as Loihi [30], TrueNorth [32], and DYNAPs [50].

Figure 1. Hardware architecture of neuromorphic systems.

Neurosynaptic cores are the computing resources of a
neuromorphic system. In many emerging architectures, a
neurosynaptic core is essentially a crossbar array that can

interconnectinterconnectneurosynaptic coreneurosynaptic coreneurosynaptic coreneurosynaptic coreinterconnectneurosynaptic coreneurosynaptic coreneurosynaptic coreneurosynaptic coreinterconnectneurosynaptic coreneurosynaptic coreneurosynaptic coreneurosynaptic core 
 
 
 
 
 
Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das

accommodate a fixed number of neurons and synapses. The
role of a system software for neuromorphic systems is there-
fore, to partition an SNN with many neurons and synapses
into clusters such that, the neurons and synapses of each
cluster can be mapped on to a neurosynaptic core of the
system. Therefore, the inter-cluster synapses are mapped on
the shared interconnect of the system.

Recently, energy consumption of neuromorphic systems
has come into spot light, specifically due to their applica-
tion in power-constrained environments such as Embedded
Systems and Internet-of-Things (IoT). To this end, several
low-power analog neuron designs are proposed to imple-
ment neurosynaptic cores for neuromorphic systems [41, 53,
55, 73, 74]. Another research direction is the shift from Static
RAM (SRAM)-based synapse designs to implementations
that use Non-Volatile Memory (NVM) â€“ Phase Change Mem-
ory (PCM) [13], Oxide-based Resistive RAM (OxRRAM) [48],
Ferroelectric RAM (FeRAM) [52], and Spin-Transfer Torque
Magnetic or Spin-Orbit-Torque RAM (STT/ SoT-MRAM) [72].1
In addition to the hardware-oriented energy reduction
techniques, we argue that the system software also plays
a pivotal role in the energy consumption of neuromorphic
systems. We show that the energy consumption of a neu-
romorphic hardware depends on 1) how an SNN model is
partitioned into clusters, 2) how the clusters are mapped
to the neurosynaptic cores, and 3) how the neurons and
synapses of a cluster are placed inside each core. Following
are our key contributions.

â€¢ We formulate the energy consumption of a neuromor-
phic hardware, considering the energy consumed inside
each neurosynaptic core and the energy consumed in
communicating spikes on the shared interconnect.
â€¢ We show that by not considering all the sources of
energy loss, existing system software approaches leave
a significant energy improvement opportunities.

â€¢ We propose a heuristic to minimize the total energy
consumption in neuromorphic computing without sig-
nificantly increasing the spike latency. This leads to
only a very marginal impact on performance.

â€¢ We evaluate our mapping approach with 10 machine
learning workloads on a cycle-accurate simulator of a
state-of-the-art neuromorphic hardware.

Results demonstrate that the current system software
frameworks for neuromorphic systems miss significant en-
ergy improvement opportunities. By explicitly incorporating
energy consumption of different hardware units, the pro-
posed mapping approach significantly minimizes the energy
consumption of neuromorphic systems.

1Beside neuromorphic computing, some of these memristor technologies
are also used as main memory in conventional computers to improve per-
formance and energy efficiency [61, 63, 65, 67, 68].

2 Background
There are many recent initiatives to map machine learn-
ing workloads to neuromorphic hardware. PACMAN [37] is
used to map SNNs to SpiNNaker hardware [36]. Corelet [1]
is used to map workloads to TrueNorth [32]. NEUTRAM [42]
is a mapping approach for digital neuromorphic chips such
as TIANJI [59]. PyNN [31], which started as a front-end to
many back-end SNN simulators such as Brian [39], NEU-
RON [40], and NEST [34], can now map SNN applications to
many neuromorphic hardware such as Loihi [30] and Neu-
rogrid [12]. A recent extension of PyNN, called PyCARL [8],
can simulate SNN applications using the back-end CARLsim
simulator [15], allowing mapping of these applications to
the DYNAPs neuromorphic hardware [50]. There are also
other propritary approaches to mapping SNN applications
to emerging neuromorphic chips such as BrainScaleS [56],
Braindrop [54], and ODIN [35].

DecomposedSNN [11] uses spatial decomposition tech-
nique to unroll each neuron with many fanin connections
into smaller atomic units that are connected sequentially.
This allows to densely pack each crossbar in a neuromor-
phic hardware leading to a significant improvement of re-
source utilization and a reduction of hardware area over-
head. PSOPART [29] and SpiNeMap [9] are mapping ap-
proaches that minimize spike communication energy on
the shared interconnect by lowering the spike volume and
spike latency, respectively. SPINERTM [10] is proposed to
remap SNN applications to neuromorphic hardware at tun-
time by monitoring the performance degradation. DFSyn-
thesizer [64] uses data flow models to analyze performance
of SNN workloads on crossbar-based neuromorphic hard-
ware. There are also other dataflow-based technique reported
in literature [2, 3, 18]. These approaches are demonstrated
with many SNN applications, such as the liquid state ma-
chine (LSM)-based heart-rate estimation [16]; spiking ResNet
architecture for ImageNet classification [57]; deep learn-
ing architecture for DNA sequence analysis [51]; heart-rate
classification using spiking CNN architecture using ECG
data [4, 28]; lateral inhibition-based digit recognition [33];
recurrent architecture-based predictive visual pursuit [43];
spiking architecture for seizure classification using EEG
data [38]; among others.

RENEU [66] is a recent technique proposed to map SNN
applications to hardware improving the circuit aging of the
peripheral circuitry in crossbars, which is caused due to
their high-voltage exposure. There are also other approaches
targeting circuit aging [5, 62]. ESPINE [71] is an approach
to map SNN applications to neuromorphic hardware, im-
proving the endurance of its Non-Volatile Memory synapses.
There are also other mapping approaches that target temper-
ature optimization [70] and releiability-performance trade-
offs [45, 69]. We compare our Hill Climbing approach against

On the Role of System Software in Energy Management of Neuromorphic Computing

Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

PyCARL and SpiNeMap, and found it to perform signifi-
cantly better in terms of energy consumption.

3 Problem Formulation
Unlike system software for conventional computers (e.g., the
Operating System), the role of the system software for neu-
romorphic hardware is to cluster a machine learning model
and map the clusters onto the crossbars of the neuromorphic
hardware. Figure 2 illustrates the mapping concept using an
example SNN shown in (â¶). The number on a link represents
the average number of spikes communicated between the
source and destination neurons for a representative training
data. We consider the mapping of this SNN to a hardware
with 2 Ã— 2 crossbars. Since a crossbar in this hardware can
only accommodate a maximum of 2 pre-synaptic connec-
tions, we partition the SNN of (â¶) into two clusters (shown
in two different colors) in (â·). These clusters can then be
mapped to the two crossbars as shown in (â¸), with an aver-
age 8 spikes communicated between the crossbars.

Figure 2. A typical clustering approach used by system soft-
ware to map SNNs to neuromorphic hardware.

In many neuromorphic applications, the number of pre-
synaptic connections per neuron can well exceed the cross-
bar input limit (which is typically 128 or 256). For those
applications, each neuron is first decomposed into smaller
units with fewer pre-synaptic connections before they are
clustered using the approach illustrated in Fig. 2 (see [11]).
Figure 3 illustrates the clusters 7 clusters of the LeNet
Convolutional Neural Network (CNN) obtained using the
clustering technique of SpiNeMap.

Formally, a clustered SNN graph is defined as follows.
Definition 1. (Clustered SNN) A clustered SNNs GCSNN = (C, L)
is a directed graph consisting of a finite set C of clusters and a
finite set L of connections between these clusters.

Each cluster ğ¶ğ‘– âˆˆ C is a tuple âŸ¨ğ¼ğ‘› (ğ¶ğ‘– ), ğ‘‚ğ‘¢ğ‘¡ (ğ¶ğ‘– ), ğ‘† (ğ¶ğ‘– ),ğ‘Š (ğ¶ğ‘– ) âŸ©,
where ğ¼ğ‘› (ğ¶ğ‘– ) is the number of pre-synaptic neurons of the
cluster, ğ‘‚ğ‘¢ğ‘¡ (ğ¶ğ‘– ) is the number of post-synaptic neurons of
the cluster, ğ‘† (ğ¶ğ‘– ) is the number of spikes generated inside the
cluster, and ğ‘Š (ğ¶ğ‘– ) is the set of synaptic weights of the cluster.
Each link ğ¿ğ‘– âˆˆ L of the graph has a value ğ‘†ğ‘ğ‘˜ (ğ¿ğ‘– ) attached to
it representing the number of spikes communicated on the
link between the source and the destination clusters.

Figure 3. The clusters generated from LeNet CNN.

The clusters of an SNN-based application are mapped to
the tiles of a neuromorphic hardware, where a tile consists
of a neurosynaptic core, e.g., a crossbar.

Formally, a neuromorphic hardware is defined as follows.
Definition 2. (Neuromorphic Hardware) A neuromorphic
hardware ğºğ‘ ğ» = (T, I) is a directed graph consisting of a finite
set T of tiles and a finite set I of interconnect links.

Each tile consists of a crossbar to map neurons and synapses,

and input and output buffers to receive and send spikes
over the interconnect, respectively. A tile ğ‘‡ğ‘– âˆˆ T is a tuple
âŸ¨ğ‘€, ğ¼ğ‘›ğµ (ğ‘‡ğ‘– ), ğ‘‚ğ‘¢ğ‘¡ğµ (ğ‘‡ğ‘– ) âŸ©, where ğ‘€ is the dimension of a crossbar
on the tile, i.e., the tile ğ‘‡ğ‘– can accommodate ğ‘€ pre-synaptic
neurons, ğ‘€ post-synaptic neurons, and ğ‘€ 2 synaptic connec-
tions, ğ¼ğ‘›ğµ (ğ‘‡ğ‘– ) is the input buffer size on the tile, and ğ‘‚ğ‘¢ğ‘¡ğµ (ğ‘‡ğ‘– )
is its output buffer size. Each interconnect link is bidirec-
tional, representing two-way communication between the
source and destination tiles with a fixed bandwidth ğµğ‘Š .

When mapping the clusters to the tiles of the hardware,
spikes from a tile (i.e., the cluster mapped to the tile) are
broadcasted on the interconnect. The network interface (NI)
logic on each tile ensures the delivery of the spikes to the
intended recipient neurons mapped to these tiles.

To formalize the energy consumption, we consider the
mapping of a clustered SNN GCSNN = (C, L) to the neuromor-
phic hardware ğºğ‘ ğ» = (T, I).

Mapping ğ‘€ : GCSNN = (C, L) â†’ ğºğ‘ ğ» = (T, I) is specified by a

logical matrix (ğ‘šğ‘– ğ‘— ) âˆˆ {0, 1}|C|Ã—|T|, where ğ‘šğ‘– ğ‘— is defined as

(cid:40)

ğ‘šğ‘– ğ‘— =

if cluster ğ¶ğ‘– âˆˆ C is mapped to tile ğ‘‡ğ‘— âˆˆ T

1
0 otherwise

(1)

To simplify the discussion, we consider a neuromorphic
hardware to have as many tiles as clusters of a given appli-
cation. The energy formulation also holds when the tiles are
time-multiplexed among the clusters [64].

4 Energy Modeling
In this section, we provide a comprehensive energy model for
neuromorphic hardware executing machine learning work-
loads. We consider the following energy components.

F-------------------,-------------------------T------------------ï¿½ Cluster 1 (......................... \ . . . . . . . . . . . . . . . . . . ..Â·Â· Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·"Â·Â·Â·Â·Â·Â·Â·Â· . â€¢â€¢â€¢â€¢â€¢ â€¢Â·â€¢â€¢â€¢Â·â€¢â€¢â€¢â€¢â€¢â€¢â€¢Â·â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢â€¢.. â€¢Â·Â·Â·Â·cÂ·Â·Â·Â·lÂ·uÂ·Â·sÂ·Â·tÂ·Â·eÂ·Â·rÂ·Â·Â·2Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â· <:..... ..â€¢Â·Â·Â·Â·Â·Â· .. â€¢Â·Â·Â·Â·Â·Â·Â· Â·Â· .... â€¢Â· a 9 OJ C: ï¿½ bCD 4 .... d Buffer 8 Interconnect d 8OJ C: ï¿½ 13 CD C .... e Buffer ------------------J cluster 4cluster 0cluster 6cluster 2cluster 1cluster 5cluster 334125687871414765676484834Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das

4.1 Spike Energy

This is the total energy consumed on the tiles to generate
all the spikes for a given SNN application working on a
representative data.

Using the formalism of Section 3, the spike energy is

ğ¸ğ‘ ğ‘ğ‘˜ =

|C|âˆ’1
âˆ‘ï¸

Â·

ğ‘–=0

ğ‘† (ğ¶ğ‘– )âˆ’1
âˆ‘ï¸

ğ‘— =0

ğ‘’ğ‘ ğ‘ğ‘˜ (ğ‘–, ğ‘—),

(2)

where ğ‘’ğ‘ ğ‘ğ‘˜ (ğ‘–, ğ‘—) is the energy of ğ‘— th spike on tile ğ¶ğ‘–. Since
each cluster is mapped to a tile of the hardware, the outer
summation is for all the clusters of an application, while the
inner summation is for all the spikes generated inside each
cluster. ğ‘’ğ‘ ğ‘ğ‘˜ (ğ‘–, ğ‘—) comprises of two components â€“ the energy
to generate a spike by a pre-synaptic neuron (ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›), which
remains the same for all the tiles (ignoring process variation
for the moment), and the energy consumed on a synaptic
cell due to the flow of current (ğ‘’ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ (ğ‘–, ğ‘—)). Therefore,

ğ‘’ğ‘ ğ‘ğ‘˜ (ğ‘–, ğ‘—) = ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ‘’ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ (ğ‘–, ğ‘—).

(3)

In all previous work, the energy per spike is typically as-
sumed to be constant. However, here we show this is not the
case. In general, the synaptic energy depends on the specific
NVM used to model the synaptic weights in a neurosynap-
tic core. We formulate this for the Phase-Change Memory
(PCM). The scope of this work is on the inference phase,
wherein a machine learning model is pre-trained offline, and
the trained model is programmed on the neuromorphic hard-
ware. Therefore, we focus on the energy to read the synaptic
weights stored in the PCM cells of a crossbar.

The energy consumed in propagating current through a

PCM cell is given by Joule Heating [21, 24â€“27, 71]

ğ‘’ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ = ğ¼ 2

ğ‘ğ‘Ÿğ‘œğ‘” Â· (cid:0)ğ‘…ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ + ğ‘…ğ‘‚ğ‘ (cid:1) Â· ğ‘¡ğ‘ ğ‘ğ‘˜,

(4)

where ğ¼ğ‘ğ‘Ÿğ‘œğ‘” is the current generated for the spike voltage
(â‰ˆ 50ğœ‡ğ´), ğ‘…ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ is the resistance of the PCM cell, ğ‘…ğ‘‚ğ‘ is
the ON resistance of the access transistor connecting the
PCM cell and ğ‘¡ğ‘ ğ‘ğ‘˜ is the spike duration (typically a few ms).
Considering ğ‘Š (ğ¶ğ‘– ) to be the synaptic weights of the cluster
ğ¶ğ‘–, which are programmed as conductances, Eq. 5 can be
written as

ğ‘’ğ‘ ğ‘¦ğ‘›ğ‘ğ‘ğ‘ ğ‘’ (ğ‘–, ğ‘—) = ğ¼ 2

ğ‘ğ‘Ÿğ‘œğ‘” Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

(cid:18)

ğ‘…ğ‘‚ğ‘ +

(cid:19)

,

1
ğ‘¤ (ğ‘–, ğ‘—)

(5)

where ğ‘¤ (ğ‘–, ğ‘—) âˆˆ ğ‘Š (ğ¶ğ‘– ) is the conductance of the PCM cell on
the path of the ğ‘— th spike in the cluster ğ¶ğ‘–.

Figure 4 shows a simple 2-input and 1-output SNN. The
neurons are shown as grayed circles, while the synaptic
weights are shown on each connection. The number on a
link represents the number of spikes for a given input.

(cid:104)
ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ¼ 2

The energy for the 5 spikes generated from the top pre-
synaptic neuron = 5 Â·
. The
energy for the 3 spikes from the bottom pre-synaptic neuron
ğ‘…ğ‘‚ğ‘ + 1
= 3 Â·
. Finally, the energy for
ğ‘¤2
the 2 spikes generated from the post-synaptic neuron is
2 Â· ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›. Therefore, the total spike energy is

(cid:104)
ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ¼ 2

ğ‘…ğ‘‚ğ‘ + 1
ğ‘¤1

ğ‘ğ‘Ÿğ‘œğ‘” Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

ğ‘ğ‘Ÿğ‘œğ‘” Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

(cid:17)(cid:105)

(cid:17)(cid:105)

(cid:16)

(cid:16)

Figure 4. Example of calculating ğ¸ğ‘ ğ‘ğ‘˜ for a simple SNN.

ğ¸ğ‘ ğ‘ğ‘˜

=

5 Â·

(cid:20)
ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ¼ 2

ğ‘ğ‘Ÿğ‘œğ‘” Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

(cid:20)
ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ¼ 2

ğ‘ğ‘Ÿğ‘œğ‘” Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

3 Â·

2 Â· ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›

(cid:18)

(cid:18)

ğ‘…ğ‘‚ğ‘ +

ğ‘…ğ‘‚ğ‘ +

(cid:19)(cid:21)

(cid:19)(cid:21)

+

+

1
ğ‘¤1
1
ğ‘¤2

From a crossbar perspective, the parasitic components on
the rows and columns create asymmetry in current propa-
gating through different NVM cells in the crossbar. Figure 5
shows the current variation in a 128x128 PCM crossbar.

Figure 5. Current map in a 128x128 crossbar.

Considering these current variations in a crossbar, the
programming current ğ¼ğ‘ğ‘Ÿğ‘œğ‘” is not a constant value for every
spike generated in a crossbar. In fact, the programming cur-
rent is higher for spikes propagating through a synaptic cell
located at the bottom left corner than through a synaptic cell
located at the top right corner (see Fig. 5). This is illustrated
in Figure 6, which shows two different ways of mapping the
SNN of Figure 6a to the crossbar. For Figure 6c, the program-
ming current is higher than the mapping of Figure 6b.

Therefore, the spike energy for an SNN application on a

neuromorphic hardware is

ğ¸ğ‘ ğ‘ğ‘˜ =

|C|âˆ’1
âˆ‘ï¸

Â·

ğ‘–=0

ğ‘† (ğ¶ğ‘– )âˆ’1
âˆ‘ï¸

ğ‘— =0

(cid:20)
ğ‘’ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘› + ğ¼ 2

ğ‘ğ‘Ÿğ‘œğ‘” (ğ‘–, ğ‘—) Â· ğ‘¡ğ‘ ğ‘ğ‘˜ Â·

(cid:18)

ğ‘…ğ‘‚ğ‘ +

(cid:19)(cid:21)

1
ğ‘¤ (ğ‘–, ğ‘—)

(6)
where ğ¼ğ‘ğ‘Ÿğ‘œğ‘” (ğ‘–, ğ‘—) is the current of the ğ‘— th spike in crossbar ğ¶ğ‘–
and it depends on where the corresponding synaptic connec-
tion is mapped within a crossbar.

0326496128Post-synapticneurons1289664320Pre-synapticneurons50556065707580PCMcurrent(ÂµA)On the Role of System Software in Energy Management of Neuromorphic Computing

Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Figure 6. (a) A simple spiking neural network, (b) Mapping
of the network to a crossbar, (c) A different mapping of the
network to the hardware.

4.2 Communication Energy

This is the total energy consumed by all spikes on the in-
terconnect of a neuromorphic hardware for a given SNN
application working on a representative data.

In mapping clusters to tiles, the inter-cluster spikes are
the ones that are communicated over the interconnect. Using
the formalism of Section 3, the communication energy is

ğ¸ğ‘ğ‘œğ‘šğ‘š =

|L|âˆ’1
âˆ‘ï¸

ğ‘˜=0

ğ‘†ğ‘ğ‘˜ (ğ¿ğ‘˜ ) Â· ğ‘’ğ‘ğ‘œğ‘šğ‘š (ğ¿ğ‘˜ )

(7)

where ğ‘’ğ‘ğ‘œğ‘šğ‘š (ğ¿ğ‘˜ ) is the energy to communicate a spike on
the link between the source and destination clusters of the
link ğ¿ğ‘˜ âˆˆ L. ğ‘’ğ‘ğ‘œğ‘šğ‘š (ğ¿ğ‘˜ ) depends on the hardware architecture
and how tiles are interconnected. In general,

ğ‘’ğ‘ğ‘œğ‘šğ‘š (ğ¿ğ‘˜ ) = ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ Â· (â„ğ‘˜ âˆ’ 1) + ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’ Â· â„ğ‘˜,

(8)

where â„ğ‘˜ is the hop distance between the source and destina-
tion tiles of the connection ğ¿ğ‘˜ , ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’ is the energy consumed
on the interconnect wires, and ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ is the energy consumed
on the switch [17, 19, 20, 22, 23, 58]. In the following, we
consider a mesh-based organization of tiles with X-Y routing
of spikes. For this interconnect architecture, the hop count
between two tiles is their Manhattan Distance. Specifically,
if the source cluster of the link ğ¿ğ‘˜ , represented as ğ‘†ğ‘Ÿğ‘ (ğ¿ğ‘˜ ),
(cid:17)
is placed on a tile located at coordinate
,
while the destination cluster, represented as ğ·ğ‘ ğ‘¡ (ğ¿ğ‘˜ ), is placed
on a tile located at coordinate

(cid:16)
ğ‘¥ğ‘‘ğ‘ ğ‘¡ (ğ¿ğ‘˜ ), ğ‘¦ğ‘‘ğ‘ ğ‘¡ (ğ¿ğ‘˜ )

(cid:16)
ğ‘¥ğ‘ ğ‘Ÿğ‘ (ğ¿ğ‘˜ ), ğ‘¦ğ‘ ğ‘Ÿğ‘ (ğ¿ğ‘˜ )

, then

(cid:17)

â„ğ‘˜

= ğ‘€ğ‘ğ‘›â„ğ‘ğ‘¡ğ‘¡ğ‘ğ‘›ğ·ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’
(cid:12)
(cid:12)
(cid:12)

(cid:12)
ğ‘¥ğ‘ ğ‘Ÿğ‘ (ğ¿ğ‘˜ ) âˆ’ ğ‘¥ğ‘‘ğ‘ ğ‘¡ (ğ¿ğ‘˜ )
(cid:12)
(cid:12)

=

(cid:16)
ğ‘†ğ‘Ÿğ‘ (ğ¿ğ‘˜ ), ğ·ğ‘ ğ‘¡ (ğ¿ğ‘˜ )

(cid:17)

(9)

+

(cid:12)
(cid:12)
(cid:12)

ğ‘¦ğ‘ ğ‘Ÿğ‘ (ğ¿ğ‘˜ ) âˆ’ ğ‘¦ğ‘‘ğ‘ ğ‘¡ (ğ¿ğ‘˜ )

(cid:12)
(cid:12)
(cid:12)

Figure 7 illustrates the placement of an example clustered
SNN to a mesh architecture. Cluster A in this example is
placed at coordinate (1,1), cluster B at (0,0), and cluster C
at (2,2). As can be seen from this figure, the hop distance
between A and B is 2, between B and C is 4, and between C
and A is 2. Therefore, the communication energy for spikes
, that
communicated between A and B = 3 Â·

(cid:104)
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 2 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’

(cid:105)

between B and C = 3 Â·

(cid:104)

A and C = 2 Â·

(cid:104)
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 2 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’

.

3 Â· ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 4 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’
(cid:105)

(cid:105)

, and that between

(a) Example clustered SNN.

(b) Placement of the SNN to a mesh
architecture..

Figure 7. Example of calculation ğ¸ğ‘ğ‘œğ‘šğ‘š for a clustered SNN
placed on a mesh architecture.

Therefore, the total communication energy is

ğ¸ğ‘ğ‘œğ‘šğ‘š =

3 Â·

3 Â·

2 Â·

(cid:104)
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 2 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’
(cid:104)

3 Â· ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 4 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’

(cid:105)

+

(cid:105)

+

(cid:104)
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 2 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’

(cid:105)

Overall, the communication energy for an SNN application

mapped to a neuromorphic hardware is

ğ¸ğ‘ğ‘œğ‘šğ‘š =

|L|âˆ’1
âˆ‘ï¸

ğ‘˜=0

ğ‘†ğ‘ğ‘˜ (ğ¿ğ‘˜ ) Â·

(cid:104)
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ Â· (â„ğ‘˜ âˆ’ 1) + ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’ Â· â„ğ‘˜

(cid:105)

(10)

4.3 Energy Dependencies and the Role of System

Software in Energy Consumption

From Equations 6 and 10, we can conclude that energy con-
sumption depends on 1) how an SNN model is partitioned
into clusters (determines the number of neurons and synapses
in each cluster), 2) how the clusters are mapped to the neu-
rosynaptic cores (determines the hop distances), and 3) how
the neurons and synapses of a cluster are placed inside each
core (determines the spikes propagation). All these factors
can be controlled via a system software such as SpiNeMap [9]
and DecomposeSNN [11].

Figure 8 compares SpiNeMap, which minimizes commu-
nication on the interconnect and DecomposeSNN, which
maximizes crossbar utilization for 10 machine learning ap-
plications (see our evaluation methodology in Section 6).

We observe that SpiNeMap has 24% lower communication
energy and 40% higher spike energy than DecomposeSNN.
This is because SpiNeMap explicitly minimizes spike com-
munication on the interconnect and therefore, has lower
communication energy. On the other hand, DecomposeSNN
maximizes crossbar utilization and therefore, generates fewer
clusters than SpiNeMap, resulting in lower spike energy.
These results are consistent with the reported results in the
two corresponding publications.

To highlight the importance of neuron and synapse place-
ment within each crossbar (see our motivation example in

Post-synaptic neurons123(a)(b)Pre-synaptic neuronsPost-synaptic neurons(c)Pre-synaptic neurons332ACB0, 0SSS1, 1SS2, 2SSSSBACComputing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das

this illustration we show the execution of AlexNet for Ima-
geNet classification. The hardware layer at the bottom con-
sists of the neuromorphic hardware such as TrueNorth [32],
Loihi [30], and DYNAPs [50]. At the middle is the system
software layer, which interacts with both the application
and hardware layers. The system software performs energy
optimization using the iterative approach shown to the right.

(a) Spike Energy (ğ¸ğ‘ ğ‘ğ‘˜ ) of SpiNeMap and DecomposeSNN.

(b) Communication Energy (ğ¸ğ‘ğ‘œğ‘šğ‘š) of SpiNeMap and DecomposeSNN.

Figure 8. Role of system software in energy management
of neuromorphic computing.

Figure 6), Figure 9 shows the variation between minimum
and maximum spike energy for SpiNeMap and Decompose-
SNN considering 100 random placements of synapses of
clusters to the crossbars of a neuromorphic hardware.

Figure 9. Variation in spike energy due to different synapse
placement strategies on crossbars.

We observe that the spike energy of SpiNeMap varies by
3.8% and that of DecomposeSNN varies by 5.9% depending
on how synapses are placed on crossbars.

We conclude that the system software of a neuromorphic
hardware can play a key role in managing the energy con-
sumption of neuromorphic computing.

5 Energy-Aware System Software
Using the motivation presented in Section 4.3, we now present
our energy-aware system software to map machine learning
applications to neuromorphic hardware.

Figure 10 shows a neuromorphic system comprising of
the application layer, the system software layer, and the
hardware layer. The application layer at the top consists
of the user space to run machine learning applications. In

Figure 10. Our energy-aware system software.
The workflow of the system software involves clustering
a machine learning application to generate clustered SNN
graph. Next, the clusters are mapped to the tiles of the hard-
ware using a mapping approach. Finally, the clusters are
placed to crossbars using the placement step. Although the
clustering step could potentially be incorporated inside the
iterative loop, we placed it outside to limit the complexity of
the design space exploration. In fact, clustering of applica-
tions is an NP-hard problem as shown in SpiNeMap [9]. Our
clustering approach uses the graph partitioning algorithm
of SpiNeMap, minimizing 1) inter-cluster communication
(similar to SpiNeMap, and 2) maximizing cluster utilization
(similar to DecomposeSNN).

We now describe the iterative approach of energy mini-
mization starting from a clustered SNN using Algorithm 1.
At the heart of this algorithm is the CalculateEnergy func-
tion, which calculates the total energy consumption using
Equations 6 and 10. The AssignCluster function is a greedy
heuristic to place a cluster to a crossbar. For this purpose,
we first sort (in descending order) the synapses of a cluster
in terms of their number of spikes. Next, the synapses are
allocated to the crossbars, ensuring that the most activated
one is placed towards the top right corner, where the spike
current is lower.

Algorithm 1 proceeds as follows. First, the clusters are
randomly allocated to tiles (line 2). Next, the energy con-
sumption is computed after assigning the synapses to the
crossbars (lines 2-3). Then, for every cluster pair, the algo-
rithm swaps their tile and compute the new energy (lines
6-10). If the energy improves, then the swapping is retained
and the algorithm proceeds to the next cluster pair (lines
11-13). The algorithm is iterated for ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ iterations, each
time starting from a new random allocation of clusters (lines
1-15). The minimum energy mapping is returned.

LeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE012NormalizedSpikeEnergySpiNeMapDecomposeSNNLeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE012NormalizedCommunicationEnergySpiNeMapDecomposeSNNLeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE0102030VariationinSpikeEnergy(%)SpiNeMapDecomposeSNNSNN-Based ApplicationClusteringClustered SNNMappingCluster-to-TileCluster Placementiterativesynapse to crossbarCompute EnergyApplicationSystem SoftwareNeuromorphic HardwareAlexNetOn the Role of System Software in Energy Management of Neuromorphic Computing

Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Algorithm 1: Place neuron and synapse to neuromor-
phic hardware, minimizing the energy consumption.

Input: ğºCSNS, ğºNH
Output: ğ‘€

1 for ğ‘– in ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ do
2

ğ‘€init = allocate clusters to tiles randomly;
AssignCluster();
ğ¸init = CalculateEnergy(ğ‘€init);
for ğ¶ğ‘¥ , ğ¶ğ‘¦ âˆˆ C do
ğ‘€ = ğ‘€init;
Find ğ‘‡ğ‘–,ğ‘‡ğ‘— such that ğ‘šğ‘¥,ğ‘– = ğ‘šğ‘¦,ğ‘— = 1 /* Find the tiles

to which ğ¶ğ‘¥ and ğ¶ğ‘¦ are mapped.

*/

Change ğ‘€ to set ğ‘šğ‘¥,ğ‘— = ğ‘šğ‘¦,ğ‘– = 1 and

ğ‘šğ‘¥,ğ‘– = ğ‘šğ‘¦,ğ‘— = 0/* Swap the tiles of ğ¶ğ‘¥ and
ğ¶ğ‘¦ .
AssignCluster();
ğ¸ = CalculateEnergy(ğ‘€)/* Calculate energy of the
*/

new mapping.

*/

if ğ¸ < ğ¸init then

ğ‘€init = ğ‘€ and ğ‘€min = ğ‘€/* If energy reduces

then retain the new mapping.

*/

end

3

4

5

6

7

8

9

10

11

12

13

end

14
15 end
16 Return ğ‘€min

In this algorithm, ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ is a user-defined parameter and
is used to explore the trade-offs between mapping time and
the solution quality (see Section 6).

6 Evaluation
We evaluated 10 machine learning applications that are
representative of three most commonly used neural net-
work classes â€” convolutional neural network (CNN), multi-
layer perceptron (MLP), and recurrent neural network (RNN).
These applications are summarized in Table 1. We simulate
these applications on an in-house cycle-accurate neuromor-
phic hardware simulator. We model the DYNAPs neuromor-
phic hardware [50] with the following configurations.

Table 1. Evaluated applications.

Synapses Neurons Topology
20,602
CNN
230,443 CNN
554,059 CNN
153,730 CNN

Class

CNN

MLP

RNN

Applications
LeNet [46]
AlexNet [44]
VGG16 [60]
HeartClass [4]
DigitRecogMLP
EdgeDet [15]
ImgSmooth [15]
HeartEstm [16]
VisualPursuit [43]
DigitRecogSTDP [33]

282,936
38,730,222
99,080,704
1,049,249
79,400
114,057
9,025
66,406
163,880
11,442

884
6,120
4,096
166
205
567

FeedForward (784, 100, 10)
FeedForward (4096, 1024, 1024, 1024)
FeedForward (4096, 1024)
Recurrent Reservoir
Recurrent Reservoir
Recurrent Reservoir

Accuracy
85.1%
90.7%
69.8 %
63.7%
91.6%
100%
100%
100%
47.3%
83.6%

Table 2 reports the hardware parameters of DYNAP-SE.

Table 2. Major simulation parameters extracted from [50]
and extrapolated for PCM technology.

Neuron technology

65nm CMOS

Synapse technology

Supply voltage
ğ¸ğ‘›ğ‘’ğ‘¢ğ‘Ÿğ‘œğ‘›
ğ‘’ğ‘ ğ‘¤ğ‘–ğ‘¡ğ‘â„ + 2 âˆ— ğ‘’ğ‘¤ğ‘–ğ‘Ÿğ‘’

PCM

1.0V

50pJ at 30Hz spike frequency

147pJ

Switch bandwidth

1.8G. Events/s

6.1 Energy Consumption

Figure 11 reports the total energy consumption for each appli-
cation for the evaluated approaches normalized to SpiNeMap.
We make the following two observations.

Figure 11. Total energy normalized to SpiNeMap.

First, between SpiNeMap and DecomposeSNN, the energy
consumption of SpiNeMap is lower than DecomposeSNN for
applications such as VGG16, HeartClass, and EdgeDet. For
these applications, there is a significant number of spikes
communicated on the interconnect. Therefore, by reduc-
ing inter-cluster communication, SpiNeMap also reduces
energy consumption. For other applications such as LeNet
and HeartEstm, the number of inter-cluster spikes is less,
so DecomposeSNN, which maximizes cluster utilization im-
proves on the total energy consumption. Second, compared
to both these approaches, the proposed approach results in
20% lower energy than SpiNeMap and 24% lower energy
than DecomposeSNN. The improvement over both these ap-
proaches is because the proposed approach explicitly incor-
porates both the spike and communication energy in finding
a suitable mapping of clusters to the hardware.

To give further insight, Figure 12 reports the total energy,
distributed into spike energy (ğ¸ğ‘ ğ‘ğ‘˜ ) and communication en-
ergy (ğ¸ğ‘ğ‘œğ‘šğ‘š). We observe that communication energy consti-
tute an average 58.8% of the total energy consumption and
it depends on the total spikes generated in a workload.

â€¢ A tiled array of 4 tiles, each with a 128x128 crossbar.

There are 65,536 synapses per crossbar.

â€¢ Spikes are digitized and communicated between cores
through a mesh routing network using the Address
Event Representation (AER) protocol.

â€¢ Each synaptic element is a PCM cell implementing

multi-bit synapse.

Figure 12. Total energy distributed into ğ¸ğ‘ ğ‘ğ‘˜ and ğ¸ğ‘ğ‘œğ‘šğ‘š.

LeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE0.51.01.5TotalEnergyNormalizedtoSpiNeMapSpiNeMapDecomposedSNNProposedLeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE050100150EnergyDistribution(%)EspkEcommComputing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das

6.2 Spike Latency and Model Performance

Figure 13 plots the spike latency for each evaluated applica-
tions for the evaluated approaches normalized to SpiNeMap.
We make the following two observations.

Figure 13. Spike latency normalized to SpiNeMap.
First, SpiNeMap has the lowest latency. This is because
SpiNeMap minimizes spike congestion on the interconnect,
which reduces spike delay. DecomposeSNN has the highest
latency because of its optimization objective, which is to
maximize utilization. Second, the proposed approach mini-
mizes spike communication to reduce the communication
energy. This also reduces the spike latency. Overall, the spike
latency of the proposed approach is only 6% higher than
SpiNeMap but 15% lower than DecomposeSNN. As described
in [9], spike latency can lead to a loss in model performance.
Therefore, by keeping the spike latency reasonably close
to SpiNeMap, the performance of the proposed approach is
similar to that reported in column 6 of Table 1.

6.3 Architecture Evaluation

Figure 14 report the energy consumption for three differ-
ent hardware configurations â€“ with 128x128, 256x256, and
512x512 crossbars, for the evaluated applications. Results
are normalized to the energy consumption with 128x128
crossbars. We observe that the energy consumption is 13%
and 28% lower when using 256x256 and 512x512 crossbars
compared to using 128x128 crossbars. Energy consumption
reduces when using larger crossbars because of the reduction
in the total number of spikes on the interconnect.

Figure 14. Energy consumption for different hardware con-
figurations.

6.4 Solution Quality Evaluation

Table 3 reports the mapping time and the normalized en-
ergy obtained for three different settings of the parameter
ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ . We observe that as ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ is increased, the en-
ergy consumption reduces for all applications. This is be-
cause with the increase in the number of iterations, Algo-
rithm 1 is able to find a better solution. However, the map-
ping time also increases. Finally, we observe that increasing

ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ from 100 to 1000 results in a significant increase
in mapping time with a minimal improvement of the total
energy. We conclude that setting ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ = 100 gives the
best trade-off in terms of mapping time and the solution
quality. Users can use this ğ‘€ğ‘ğ‘¥ğ¼ğ‘¡ğ‘’ğ‘Ÿ parameter to set a limit
on the time of their algorithm by analyzing the complexity
of their model against the ones we evaluate (see Table 1).

Table 3. Mapping time and solution trade-off.

MaxIter = 10

MaxIter = 100

MaxIter = 1000

Application

Mapping Total Mapping Total Mapping Total

Time (sec) Energy Time (sec) Energy Time (sec) Energy

LeNet

AlexNet

VGG16

HeartClass

DigitRecogMLP

EdgeDet

ImgSmooth

HeartEstm

VisualPursuit

DigitRecogSTDP

75

200

241

116

10

25

50

15

30

15

1.13

1.1

1.06

1.16

1.17

1.15

1.11

1.08

1.0

1.07

321

2189

2989

1008

160

200

400

156

324

164

1.0

1.0

1.0

1.0

1.0

1.0

1.0

1.0

1.0

1.0

2700

12008

34300

10178

1600

1898

1940

1344

3003

1615

0.99

1.0

1.0

1.03

0.97

0.98

0.99

0.97

1.0

0.9

7 Conclusion
In this work, we provide a comprehensive energy model
for executing machine learning applications on neuromor-
phic hardware. Using this model we show that the system
software for neuromorphic hardware plays a critical role in
energy management of neuromorphic computing by con-
trolling 1) how an SNN model is partitioned into clusters,
2) how the clusters are mapped to the neurosynaptic cores
of the hardware, and 3) how the neurons and synapses of
a cluster are placed inside each core. We then propose a
heuristic to perform energy-aware application mapping to
neuromorphic hardware, lowering the overall energy con-
sumption. Using this heuristic, we show that the energy
consumption can be reduced by an average 20% compared to
a state-of-the-art for typical machine learning applications.

8 Acknowledgments
This work is supported by 1) the National Science Founda-
tion Award CCF-1937419 (RTML: Small: Design of System
Software to Facilitate Real-Time Neuromorphic Computing)
and 2) the National Science Foundation Faculty Early Career
Development Award CCF-1942697 (CAREER: Facilitating De-
pendable Neuromorphic Computing: Vision, Architecture,
and Impact on Programmability).

References
[1] A. Amir, P. Datta, W. P. Risk, A. S. Cassidy, J. A. Kusnitz et al., â€œCognitive
computing programming paradigm: A corelet language for composing
networks of neurosynaptic cores,â€ in IJCNN, 2013.

[2] A. Balaji and A. Das, â€œCompiling spiking neural networks to mitigate
neuromorphic hardware constraints",â€ in IGSC Workshops, 2020.

LeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE0.51.01.5SpikeLatencyNormalizedtoSpiNeMapSpiNeMapDecomposedSNNProposedLeNetAlexNetVGG16HeartClassDigitRecogMLPEdgeDetImgSmoothHeartEstmVisualPursuitDigitRecogSTDPAVERAGE0.60.81.01.2NormalizedEnergy128x128256x256512x512On the Role of System Software in Energy Management of Neuromorphic Computing

Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

[3] A. Balaji and A. Das, â€œA framework for the analysis of throughput-
constraints of snns on neuromorphic hardware,â€ in ISVLSI, 2019.
[4] A. Balaji, F. Corradi, A. Das, S. Pande, S. Schaafsma et al., â€œPower-
accuracy trade-offs for heartbeat classification on neural networks
hardware,â€ Journal of Low Power Electronics (JOLPE), 2018.

[5] A. Balaji, S. Song, A. Das, N. Dutt, J. Krichmar et al., â€œA framework
to explore workload-specific performance and lifetime trade-offs in
neuromorphic computing,â€ CAL, 2019.

[6] A. Balaji, S. Ullah, A. Das, and A. Kumar, â€œDesign methodology for
embedded approximate artificial neural networks,â€ in GLSVLSI, 2019.
[7] A. Balaji, Y. Wu, A. Das, F. Catthoor, and S. Schaafsma, â€œExploration
of segmented bus as scalable global interconnect for neuromorphic
computing,â€ in GLSVLSI, 2019.

[8] A. Balaji, P. Adiraju, H. J. Kashyap, A. Das, J. L. Krichmar et al., â€œPy-
CARL: A PyNN interface for hardware-software co-simulation of spik-
ing neural network,â€ in IJCNN, 2020.

[9] A. Balaji, A. Das, Y. Wu, K. Huynh, F. G. Dellâ€™anna et al., â€œMapping
spiking neural networks to neuromorphic hardware,â€ TVLSI, 2020.
[10] A. Balaji, T. Marty, A. Das, and F. Catthoor, â€œRun-time mapping of
spiking neural networks to neuromorphic hardware,â€ JSPS, 2020.
[11] A. Balaji, S. Song, A. Das, J. Krichmar, N. Dutt et al., â€œEnabling resource-
aware mapping of spiking neural networks via spatial decomposition,â€
ESL, 2020.

[12] B. V. Benjamin, P. Gao, E. McQuinn, S. Choudhary, A. R. Chan-
drasekaran et al., â€œNeurogrid: A mixed-analog-digital multichip system
for large-scale neural simulations,â€ Proceedings of the IEEE, 2014.
[13] G. W. Burr, R. M. Shelby, A. Sebastian, S. Kim, S. Kim et al., â€œNeuro-
morphic computing using non-volatile memory,â€ Advances in Physics:
X, 2017.

[14] F. Catthoor, S. Mitra, A. Das, and S. Schaafsma, â€œVery large-scale neu-
romorphic systems for biological signal processing,â€ in CMOS Circuits
for Biological Sensing and Processing, 2018.

[15] T. Chou, H. Kashyap, J. Xing, S. Listopad, E. Rounds et al., â€œCARLsim
4: An open source library for large scale, biologically detailed spiking
neural network simulation using heterogeneous clusters,â€ in IJCNN,
2018.

[16] A. Das, P. Pradhapan, W. Groenendaal, P. Adiraju, R. Rajan et al.,
â€œUnsupervised heart-rate estimation in wearables with Liquid states
and a probabilistic readout,â€ Neural Networks, 2018.

[17] A. Das and A. Kumar, â€œFault-aware task re-mapping for throughput
constrained multimedia applications on NoC-based MPSoCs,â€ in RSP,
2012.

optimization for lifetime improvement of multicore systems,â€ in DAC,
2014.

[26] A. Das, A. Kumar, and B. Veeravalli, â€œReliability and energy-aware
mapping and scheduling of multimedia applications on multiprocessor
systems,â€ TPDS, 2015.

[27] A. Das, B. M. Al-Hashimi, and G. V. Merrett, â€œAdaptive and hierarchical
runtime manager for energy-aware thermal management of embedded
systems,â€ TECS, 2016.

[28] A. Das, F. Catthoor, and S. Schaafsma, â€œHeartbeat classification in
wearables using multi-layer perceptron and time-frequency joint dis-
tribution of ECG,â€ in CHASE, 2018.

[29] A. Das, Y. Wu, K. Huynh, F. Dellâ€™Anna, F. Catthoor et al., â€œMapping
of local and global synapses on spiking neuromorphic hardware,â€ in
DATE, 2018.

[30] M. Davies, N. Srinivasa, T. H. Lin et al., â€œLoihi: A neuromorphic many-

core processor with on-chip learning,â€ IEEE Micro, 2018.

[31] A. P. Davison, D. BrÃ¼derle, J. M. Eppler, J. Kremkow, E. Muller et al.,
â€œPyNN: a common interface for neuronal network simulators,â€ Frontiers
in Neuroinformatics, 2009.

[32] M. V. Debole, B. Taba, A. Amir et al., â€œTrueNorth: Accelerating from

zero to 64 million neurons in 10 years,â€ Computer, 2019.

[33] P. U. Diehl and M. Cook, â€œUnsupervised learning of digit recognition
using spike-timing-dependent plasticity,â€ Frontiers in Computational
Neuroscience, 2015.

[34] J. M. Eppler, M. Helias, E. Muller, M. Diesmann, and M.-O. Gewaltig,
â€œPynest: a convenient interface to the nest simulator,â€ Frontiers in Neu-
roinformatics, 2009.

[35] C. Frenkel, M. Lefebvre, J.-D. Legat, and D. Bol, â€œA 0.086-mm2 12.7-
pj/sop 64k-synapse 256-neuron online-learning digital spiking neuro-
morphic processor in 28-nm CMOS,â€ TBCAS, 2018.

[36] S. B. Furber, F. Galluppi, S. Temple, and L. A. Plana, â€œThe SpiNNaker

project,â€ Proceedings of the IEEE, 2014.

[37] F. Galluppi, X. Lagorce, E. Stromatias, M. Pfeiffer, L. A. Plana et al.,
â€œA framework for plasticity implementation on the SpiNNaker neural
architecture,â€ Frontiers in Neuroscience, 2015.

[38] S. Ghosh-Dastidar and H. Adeli, â€œImproved spiking neural networks
for EEG classification and epilepsy and seizure detection,â€ Integrated
Computer-Aided Engineering, 2007.

[39] D. F. Goodman and R. Brette, â€œThe brian simulator,â€ Frontiers in Neuro-

science, 2009.

[40] M. L. Hines and N. T. Carnevale, â€œThe NEURON simulation environ-

ment,â€ Neural Computation, 1997.

[18] A. Das and A. Kumar, â€œDataflow-based mapping of spiking neural

[41] G. Indiveri, â€œA low-power adaptive integrate-and-fire neuron circuit,â€

networks on neuromorphic hardware,â€ in GLSVLSI, 2018.

in ISCAS, 2003.

[19] A. Das, A. Kumar, and B. Veeravalli, â€œFault-tolerant network interface
for spatial division multiplexing based Network-on-Chip,â€ in ReCoSoC,
2012.

[20] A. Das, A. Kumar, and B. Veeravalli, â€œCommunication and migration
energy aware design space exploration for multicore systems with
intermittent faults,â€ in DATE, 2013.

[21] A. Das, A. Kumar, and B. Veeravalli, â€œReliability-driven task mapping
for lifetime extension of networks-on-chip based multiprocessor sys-
tems,â€ in DATE, 2013.

[22] A. Das, A. K. Singh, and A. Kumar, â€œEnergy-aware dynamic reconfigu-
ration of communication-centric applications for reliable MPSoCs,â€ in
ReCoSoC, 2013.

[23] A. Das, A. Kumar, and B. Veeravalli, â€œCommunication and migration
energy aware task mapping for reliable multiprocessor systems,â€ FGCS,
2014.

[24] A. Das, A. Kumar, and B. Veeravalli, â€œTemperature aware energy-
reliability trade-offs for mapping of throughput-constrained applica-
tions on multimedia MPSoCs,â€ in DATE, 2014.

[25] A. Das, R. A. Shafik, G. V. Merrett, B. M. Al-Hashimi, A. Kumar et al.,
â€œReinforcement learning-based inter-and intra-application thermal

[42] Y. Ji, Y. Zhang, S. Li, P. Chi, C. Jiang et al., â€œNEUTRAMS: Neural
network transformation and co-design under neuromorphic hardware
constraints,â€ in MICRO, 2016.

[43] H. J. Kashyap, G. Detorakis, N. Dutt, J. L. Krichmar, and E. Neftci, â€œA
recurrent neural network based model of predictive smooth pursuit
eye movement in primates,â€ in IJCNN, 2018.

[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classifica-
tion with deep convolutional neural networks,â€ in Advances in neural
information processing systems (NeurIPS), 2012.

[45] S. Kundu, K. Basu, M. Sadi, T. Titirsha, S. Song et al., â€œReliability

analysis for ML/AI hardware,â€ in VTS, 2021.

[46] Y. LeCun et al., â€œLenet-5, convolutional neural networks,â€ URL:

http://yann. lecun. com/exdb/lenet, 2015.

[47] W. Maass, â€œNetworks of spiking neurons: The third generation of

neural network models,â€ Neural Networks, 1997.

[48] A. Mallik, D. Garbin, A. Fantini, D. Rodopoulos, R. Degraeve et al.,
â€œDesign-technology co-optimization for oxrram-based synaptic pro-
cessing unit,â€ in VLSIT, 2017.

[49] C. Mead, â€œNeuromorphic electronic systems,â€ Proceedings of the IEEE,

1990.

Computing Frontiers â€™21, May 11â€“13, 2021, Virtual Conference

Twisha Titirsha, Shihao Song, Adarsha Balaji, and Anup Das

[50] S. Moradi, N. Qiao, F. Stefanini, and G. Indiveri, â€œA scalable multi-
core architecture with heterogeneous memory structures for dynamic
neuromorphic asynchronous processors (DYNAPs),â€ TBCAS, 2017.
[51] E. J. Moyer and A. Das, â€œMachine learning applications to DNA subse-

quence and restriction site analysis,â€ in SPMB, 2020.

[52] H. Mulaosmanovic, J. Ocker, S. MÃ¼ller, M. Noack, J. MÃ¼ller et al., â€œNovel
ferroelectric FET based synapse for neuromorphic systems,â€ in VLSIT,
2017.

[53] A. Natarajan and J. Hasler, â€œHodgkinâ€“huxley neuron and FPAA dy-

namics,â€ TBCAS, 2018.

[54] A. Neckar, S. Fok, B. V. Benjamin, T. C. Stewart, N. N. Oza et al., â€œBrain-
drop: A mixed-signal neuromorphic architecture with a dynamical
systems-based programming model,â€ Proceedings of the IEEE, 2018.
[55] A. Rubino, C. Livanelioglu, N. Qiao, M. Payvand, and G. Indiveri, â€œUltra-
low-power FDSOI neural circuits for extreme-edge neuromorphic
intelligence,â€ TCAS I: Regular Papers, 2020.

[56] J. Schemmel, A. GrÃ¼bl, S. Hartmann, A. Kononov, C. Mayr et al., â€œLive
demonstration: A scaled-down version of the brainscales wafer-scale
neuromorphic system,â€ in ISCAS, 2012.

[57] A. Sengupta, Y. Ye, R. Wang, C. Liu, and K. Roy, â€œGoing deeper in
spiking neural networks: VGG and residual architectures,â€ Frontiers in
Neuroscience, 2019.

[58] R. A. Shafik, A. Das, S. Yang, G. Merrett, and B. M. Al-Hashimi, â€œAdap-
tive energy minimization of openmp parallel applications on many-
core systems,â€ in PARMA-DITAM, 2015.

[59] L. Shi, J. Pei, N. Deng, D. Wang, L. Deng et al., â€œDevelopment of a

neuromorphic computing system,â€ in IEDM, 2015.

[60] K. Simonyan and A. Zisserman, â€œVery deep convolutional networks

for large-scale image recognition,â€ arXiv, 2014.

[61] S. Song and A. Das, â€œDesign methodologies for reliable and energy-

efficient PCM systems,â€ in IGSC Workshops, 2020.

[62] S. Song and A. Das, â€œA case for lifetime reliability-aware neuromorphic

computing,â€ in MWSCAS, 2020.

[63] S. Song, A. Das, O. Mutlu, and N. Kandasamy, â€œEnabling and exploiting
partition-level parallelism (PALP) in phase change memories,â€ ACM
Transactions on Embedded Computing (TECS), 2019.

[64] S. Song, A. Balaji, A. Das, N. Kandasamy, and J. Shackleford, â€œCompiling
spiking neural networks to neuromorphic hardware,â€ in LCTES, 2020.
[65] S. Song, A. Das, and N. Kandasamy, â€œExploiting inter- and intra-
memory asymmetries for data mapping in hybrid tiered-memories,â€
in ISMM, 2020.

[66] S. Song, A. Das, and N. Kandasamy, â€œImproving dependability of neu-
romorphic computing with non-volatile memory,â€ in EDCC, 2020.
[67] S. Song, A. Das, O. Mutlu, and N. Kandasamy, â€œImproving phase change
memory performance with data content aware access,â€ in ISMM, 2020.
[68] S. Song, A. Das, O. Mutlu, and N. Kandasamy, â€œAging-aware request
scheduling for non-volatile main memory,â€ in Asia and South Pacific
Design Automation Conference (ASP-DAC), 2021.

[69] T. Titirsha and A. Das, â€œReliability-performance trade-offs in neuro-

morphic computing,â€ in IGSC Workshops, 2020.

[70] T. Titirsha and A. Das, â€œThermal-aware compilation of spiking neural

networks to neuromorphic hardware,â€ in LCPC, 2020.

[71] T. Titirsha, S. Song, A. Das, J. Krichmar, N. Dutt et al., â€œEndurance-
aware mapping of spiking neural networks to neuromorphic hardware,â€
TPDS, 2021.

[72] A. F. Vincent, J. Larroque, N. Locatelli, N. B. Romdhane, O. Bichler et al.,
â€œSpin-transfer torque magnetic memory as a stochastic memristive
synapse for neuromorphic systems,â€ TBCAS, 2015.

[73] S. Woo, J. Cho, D. Lim, Y.-S. Park, K. Cho et al., â€œImplementation and
characterization of an integrate-and-fire neuron circuit using a silicon
nanowire feedback field-effect transistor,â€ TED, 2020.

[74] Z. Yang, Y. Huang, J. Zhu, and T. T. Ye, â€œAnalog circuit implementation
of LIF and STDP models for spiking neural networks,â€ in GLSVLSI,
2020.

