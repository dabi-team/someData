Transformer-Based Language Models for Software Vulnerability
Detection
Seung Ick Jang
CSIRO Data61, Sydney, Australia
seung.jang@data61.csiro.au

Chandra Thapa
CSIRO Data61, Sydney, Australia
chandra.thapa@data61.csiro.au

Muhammad Ejaz Ahmed
CSIRO Data61, Sydney, Australia
ejaz.ahmed@data61.csiro.au

2
2
0
2

p
e
S
6

]

R
C
.
s
c
[

2
v
4
1
2
3
0
.
4
0
2
2
:
v
i
X
r
a

Seyit Camtepe
CSIRO Data61, Sydney, Australia
seyit.camtepe@data61.csiro.au

Josef Pieprzyk
CSIRO Data61, Sydney, Australia &
Institute of Computer Science, Polish
Academy of Sciences, Warsaw, Poland
josef.pieprzyk@data61.csiro.au

Surya Nepal
CSIRO Data61, Sydney, Australia
surya.nepal@data61.csiro.au

ABSTRACT
The large transformer-based language models demonstrate excel-
lent performance in natural language processing. By considering
the transferability of the knowledge gained by these models in
one domain to other related domains, and the closeness of natural
languages to high-level programming languages, such as C/C++,
this work studies how to leverage (large) transformer-based lan-
guage models in detecting software vulnerabilities and how good
are these models for vulnerability detection tasks. In this regard,
firstly, a systematic (cohesive) framework that details source code
translation, model preparation, and inference is presented. Then, an
empirical analysis is performed with software vulnerability datasets
with C/C++ source codes having multiple vulnerabilities corre-
sponding to the library function call, pointer usage, array usage,
and arithmetic expression. Our empirical results demonstrate the
good performance of the language models in vulnerability detec-
tion. Moreover, these language models have better performance
metrics, such as F1-score, than the contemporary models, namely
bidirectional long short term memory and bidirectional gated re-
current unit. Experimenting with the language models is always
challenging due to the requirement of computing resources, plat-
forms, libraries, and dependencies. Thus, this paper also analyses
the popular platforms to efficiently fine-tune these models and
present recommendations while choosing the platforms.

1 INTRODUCTION
In Natural Language Processing (NLP), transformer-based models
outperform existing models, including recurrent neural network
(RNN) based architectures [5, 24, 27, 32]. Furthermore, transformer-
based language models are attractive and promising over RNN
because, unlike RNN, it allows parallelization in the model‚Äôs com-
putation for faster processing. This is essential to reduce the model
training/testing time if the model‚Äôs size is large, which is a usual
case for transformer-based models. Besides, their ability to remodel
from natural language processing tasks to related tasks, through
the process formally known as transfer learning, enables us to ex-
tend their usage in other domains. Thus, it is prudent to effectively
leverage these models beyond NLP, such as in software vulner-
ability detection, where most studies are limited to RNN-based
models [22, 23].

As software, including operating systems, is an integral part
of most computing devices, vulnerability detection at its source-
code level is a must-have mechanism for both proprietary and
open-source software to ensure protection from adversaries. They
can exploit the software vulnerabilities/weaknesses, allowing them
not only to control its execution but also to steal or modify its
data. For example, a simple buffer overflow bug in software such
as NVIDIA SHIELD TV (a popular streaming media device) [38],
macOS Catalina (an Apple operating system) [36] and WhatsApp
(a popular instant messaging application) [37] could lead to their
exploit.

There are additional benefits of using the transformer-based
models in software vulnerability detection. The benefits include
the following: (i) it automates the detection that is not possible
with the static analysis tools, which use heuristic methods to find
the code constructions from the known vulnerabilities requiring
extensive manual operations [40], and (ii) it removes the need
of extensive feature engineering requirements like in (classical)
machine learning.

Although Bidirectional Encoder Representations from Trans-
formers (BERT) [5], a transformer-based language model, is used re-
cently in vulnerability detection [44], it is still unclear how the other
models such as Generative Pre-trained Transformer (GPT) [27]
perform. Moreover, training/fine-tuning these models is always
challenging due to the computational requirements, libraries, and
dependencies. Thus, this paper aims to answer the following:

RQ1: What can be a systematic framework to leverage transformer-

based language models for software vulnerability detection?

RQ2: How well existing transformer-based language models per-
form in detecting software vulnerabilities compared to other
contemporary RNN-based models?

RQ3: Which platform is efficient to run these models?

In our studies, we choose software source codes written in a high-
level programming language, specifically C/C++, because of its
popularity [34], and it shares many characteristics with natural lan-
guages. Moreover, it inherits natural language grammar [8, 20, 23].
Besides, both have a well-defined structure (or syntax) and contex-
tual meaning (or semantics). Natural languages allow building long
sentences from words and shorter phrases. Likewise, programming
languages include a collection of instructions that can be used to
write complex programs. The semantics of natural languages de-
fines the meaning of sentences depending on the order and choice

1

 
 
 
 
 
 
of words and their context. For programming languages, seman-
tics refers to expected actions, their order, and results. Overall,
these similarities are additional factors to motivate and enable us
to leverage the transformer-based language models in software
vulnerabilities detection efficiently.

Transformer-based language models for software vulner-
ability detection: By considering (i) transformer-based (large)
models of various architectures and sizes, including BERT [5], Dis-
tilBERT [31], CodeBERT [8], GPT-2 [27], and Megatron language
model variants [32], and (ii) RNN-based models, namely bidirec-
tional long short term memory (BiLSTM) [23], and bidirectional
gated recurrent unit (BiGRU) [3], this work contributes the follow-
ing:

‚Ä¢ Systematic framework: To answer RQ1, we present a system-
atic framework for software vulnerability detection. The frame-
work details the translation of the source codes to vectorized
inputs for the models, description of the models, models‚Äô prepa-
ration, and inference.

‚Ä¢ Comparative performances of the models on software vul-
nerability detection in C/C++ source code databases: To an-
swer RQ2, firstly, comparative performances under binary and
multi-classification tasks are carried out to evaluate the models
with a vulnerability dataset having Buffer Error and Resource
Management Error [39]. Also, we demonstrate the need for data
cleaning in these tasks. Besides, we provide the fine-tuning time
to present an overall time cost of the models. Secondly, we further
extend the performance analysis on multiple C/C++ vulnerabil-
ities corresponding to the library function call, pointer usage,
array usage, and arithmetic expression that are related to more
than 341 Common Weakness Enumeration (CWE) IDs.

‚Ä¢ Platform analysis: It is always confusing and challenging to
handle a large model with billions of model parameters (e.g.,
GPT-2 with 1.5B parameters). These models can easily exceed the
capacity of available Graphics Processing Units (GPU) internal
RAM (e.g., 16GB) and usually require parallelisms, such as data
parallelism and model parallelism. Finding a suitable platform to
fine-tune and test these models effectively is as important as the
main problem, i.e., software vulnerability detection. Thus, as an
answer to RQ3, we provide a platform analysis of four popular
platforms, namely Horovod [9], Megatron framework [32], Deep-
Speed [25], and HuggingFace [7], along with empirical analysis
and our recommendations.

The remainder of this paper is structured as follows: Section 2
presents the details of our framework, including source code data
translation, a brief introduction of the models under investigation,
and the overall system flow to leverage transformer-based language
models for vulnerability detection. All our results, including the
performance of the models, are presented and discussed in Sec-
tion 3. Section 4 analyzes the popular platforms and presents the
challenges and recommendations for choosing the right platform to
tune the models. Section 5 presents the related works on machine
learning-based vulnerability detection. Finally, Section 6 concludes
the paper.

Table 1: Division of VulDeePecker dataset based on its type.

Dataset

Group 1

Group 2

Group 3

Type
Buffer Error (BE)
Non-vulnerable
Resource
Management Error (RME)
Non-vulnerable
BE+ RME
Combined Non-vulnerable

Original
10440
29313

Cleaned
7649
12262

7285

14600
17725
43913

2757

5010
10395
17197

Train
6161
9768

2214

4000
8368
13704

Test
1488
2494

543

1010
2027
3491

2 SYSTEMATIC FRAMEWORK
2.1 Data translation
The models require formatted data to capture important features
related to the vulnerabilities. Moreover, the model, including the
transformer-based language model, inputs the vectorized form of
the formatted data. The conversion of the data (e.g., C/C++ source
codes) to an appropriate format, which is further transformed into
vectorized inputs, is called data translation. In this process, the first
step is to change the source code into code gadgets.

2.1.1 Code Gadgets and its extraction. Code gadgets in software
vulnerability are first proposed by Li et al. [23]. It is generated as
follows:
‚Ä¢ Load all C/C++ files for analysis of relations between classes.
‚Ä¢ Normalize source codes by applying regular expressions. This

includes removing comments and non-ASCII characters.

‚Ä¢ Extract all function and variable definitions together with their

usages.

‚Ä¢ Work through all source codes and if there is a library/API func-

tion call, perform a back-track as follows:
‚Äì Extract all variable names from the function call.
‚Äì Stack up all lines which have relationships with the variables

remaining within the scope of the library/API function.

‚Äì If any variables are passed from a caller, perform another back-

track for the caller.

Overall, each code gadget can be seen as an assembled semanti-
cally related statement slices having data dependency or control
dependencies with each other. It can be associated either with a
vulnerability or without any vulnerability. In this work, we consider
the code gadgets formed based on data dependencies and labeled
‚Äú1‚Äù if they are vulnerable and ‚Äú0‚Äù otherwise. For an example of a
code gadget, refer to Figure 1.

2.1.2 Data preparation. Code gadgets are processed through

multiple stages before inputting into the model.

Data cleaning: As the code gadgets are extracted from multi-
ple sources, the dataset can have the following: (i) duplicate code
gadgets with the same label and (ii) duplicate code gadgets with
different labels (i.e., label conflict). For example, we discover these
two issues on the VulDeePecker dataset [23] (refer to Appendix A.1
for the details of this dataset). Duplicate gadgets with the same
label can leak data to the test set. On the other hand, duplicate
gadgets with different labels have a negative impact on model
training/testing. Thus, we have to clean the dataset. In this regard,
firstly, we find the duplicate code gadget by mapping all gadgets
into hash values using the SHA256 hashing algorithm provided
by python hashlib library. We choose hashing method for finding

2

Table 2: The list of the word embedding methods used for the models
considered in this paper. Refer to Huggingface‚Äôs site [15] for details on the
implementation of these word embedding methods and functions.

Word Embeddings

Tokenizer (our own) and Word2Vec
Huggingface‚Äôs Bert Tokenizer
(Tokenize based on WordPiece)
Huggingface‚Äôs DistilBert Tokenizer
(Runs end-to-end tokenization based
on punctuation splitting and WordPiece)
Huggingface‚Äôs Roberta Tokenizer
(Derived from GPT-2 tokenizer using
byte-level Byte-Pair-Encoding)
Huggingface‚Äôs GPT-2 Tokenizer
(Based on byte-level Byte-Pair-Encoding)
Huggingface‚Äôs GPT-2 Tokenizer
(Based on byte-level Byte-Pair-Encoding)

Embedding
size
512

Models

BiLSTM, BiGRU

512

512

BERT (BERTBase, MegatronBERT)

DistilBERT

514

RoBERTa, CodeBERT

1024

2048

GPT-2 (GPT-2 Base, GPT-2 Large,
GPT-2 XL, MegatronGPT-2), GPT-J

GPT-J

called word embedding. This process mainly has two associated
operations: tokenization and embedding. Tokenization converts
the input sentence into characters, words, or sub-words, which
are semantically-viable segments, and these smaller segments are
called tokens. In embedding, these tokens are mapped to relevant
vectors using various trained embedding methods, capturing the
context around the token in the sentence. In this regard, we use
Word2Vec for the BiLSTM and BiGRU models, a WordPiece-based
(subword-based) tokenizer for BERT models, and a byte-level Byte-
Pair-Encoding based tokenizer for GPT-2 models. The embedding
functions corresponding to the models investigated in this paper
are listed in Table 2.

2.2 Models
Our studies investigate a recurrent neural network-based model
and various transformer-based models for vulnerability detection.
These models are listed in Table 3. We briefly discuss these models
in the following (refer to Appendix B for details).

Under recurrent neural networks, we consider BiLSTM [14] and
BiGRU [3]. We use BiLSTM and BiGRU models that are close to
those used in software vulnerability detection as presented in [23]
(the model is not publicly available) and [22], respectively. These
models have two issues: (i) they cannot perform well when the
input sequence is too long, and (ii) hard to parallelize their opera-
tions at sequence level [35]. These shortcomings are removed in
transformers.

Under transformer-based models, we consider BERT [5], Dis-
tilBERT [31], Robustly optimized BERT approach (RoBERTa) [24],
CodeBERT [8], GPT-2 [27], and Megatron-Language Model vari-
ants [32]. In this paper, we consider the BERT model, called BERT-
Base, having 110M parameters with 12 layers, 768 hidden size, and
12 attention heads. DistilBERT is a smaller, faster, cheaper, and
lighter version of BERT, and it has 66M parameters with 6 layers,
768 hidden size, and 12 attention heads. RoBERTa has the BERTBase
architecture and is pre-trained towards better optimization, perfor-
mance, and robustness across NLP tasks. CodeBERT is a bimodal
pre-trained transformer model for both programming and natural
languages. It is trained on bimodal data (code & documents) [16],
with codes from Python, Java, JavaScript, Ruby, Go, and PHP. Its
architecture follows BERT and RoBERTa.

For GPT-based models, their architecture is based on decoder
blocks of transformer and masked self-attention [27]. GPT outper-
forms available models that are based on recursive neural networks,

Figure 1: An example of a code gadget of a non-vulnerable library/API
function. The first line of the gadget is a header, and the rest is its body.

the duplicates because it is much faster than Regex or naive string
comparison methods. For code gadgets with conflicting labels, we
remove all such code gadgets, and for same code gadgets with the
same labels, we removed their copies from the dataset. Refer to
Table 1 for the number of samples in cleaned and uncleaned (i.e.,
original) VulDeePecker dataset.

Data pre-processing: Firstly, if there are any comments in the
code gadget, those are removed. Secondly, user-defined names are
replaced by their symbolic equivalents. This is done by replacing (i)
user-defined function name by ‚ÄúFUNC‚Äù (or using consecutive natu-
ral numbers as postfix to ‚ÄúFUNC‚Äù, like ‚ÄúFUNC_1‚Äù and ‚ÄúFUNC_2‚Äù, if
multiple functions), and (ii) user assigned variable name by ‚ÄúVAR‚Äù
(or using consecutive natural numbers as postfix to ‚ÄúVAR‚Äù, like
‚ÄúVAR_1‚Äù and ‚ÄúVAR_2‚Äù, if multiple variables). This way, we normal-
ize the code gadget. Thirdly, we create subsets of data based on the
available vulnerabilities. For example, we make two sets of data
from the VulDeePecker dataset; one with Buffer Error (BE) and its
non-vulnerable versions, and the other with Resource Management
Error (RME) and its non-vulnerable versions. As we perform both
the binary classification and multi-class classification; we assign
the labels in the following ways:
‚Ä¢ For binary classification labeling, we perform experiments sepa-
rately for each of the vulnerabilities. For example, BE and RME
datasets of the VulDeePecker dataset. If code gadget has vulnera-
bility its label is ‚Äú1‚Äù, and ‚Äú0‚Äù otherwise.

‚Ä¢ For multi-class classification labeling, we perform experiments
on the union of the vulnerabilities, and we provide the label ‚Äú0‚Äù
for the clean data and ‚Äú1‚Äù onward in an increasing order based
on the available vulnerability types in the data. For example, the
code gadget with BE, RME and non-vulnerable are labelled ‚Äú1‚Äù,
‚Äú2‚Äù and ‚Äú0‚Äù, respectively, in VulDeePecker dataset.

Dataset partitioning: Following our data pre-processing step, we
divide the dataset into multiple groups for the experiments. For
example, the VulDeePecker dataset is divided into three groups;
Group 1 with BE and its non-vulnerable code gadgets, Group 2
with RME and its non-vulnerable code gadgets, and Group 3 with
combined BE and RME and their non-vulnerable code gadgets.
Dataset of Group 1 and Group 2 are used while performing binary
classification separately, and Group 3 is used while performing
three-class classification. The dataset is split into a train and test
set at a ratio of 80:20 (e.g., the number of samples in train and test
in the VulDeePecker dataset are as shown in Table 1). We perform
three-fold cross-validation and present the overall results for the
testing set.

Word embeddings: The learned representation for text data, such
as source code, where words are mapped to numeric data vectors
in a predefined vector space that encodes the word‚Äôs meaning, is

3

Table 3: Models considered in our studies and their sizes.

Provider

Nvidia

Hugging Face

Language Model
MegatronBERT
MegatronGPT-2
BERT

OpenAI

GPT-2

EleutherAI
Hugging Face
Microsoft
Hugging Face
VulDeePecker
SySeVR

GPT-J
DistilBERT
CodeBERT
RoBERTa
BiLSTM
BiGRU

Size
Standard
Standard
Base Model
Base Model
Large Model
XL Model
Standard
Standard
Standard
Standard
Standard
Standard

#Parameters
345M
345M
110M
117M
774M
1.5B
6B
66M
125M
125M
1.2M
1.6M

convolutional neural networks, and LSTMs [27]. In this paper, we
consider GPT-2 models of various sizes: (1) GPT-2 Base, which has
117M parameters with 12 layers, 768 hidden size, and 12 attention
heads, (2) GPT-2 Large, which has 774M parameters with 36 layers,
1280 hidden size, and 20 attention heads, (3) GPT-2 XL, which has
1.5B parameters with 48 layers, 1600 hidden size, and 25 attention
heads, and (4) GPT-J, which has 6B parameters with 28 layers, 4096
hidden size, and 16 attention heads, pre-trained on the dataset called
Pile having a diverse text data [10].

Megatron-LMs are transformer-based models developed by NVIDIA.

They are generated by enabling intra-layer model-parallelism on
the architecture level of the existing language models, such as BERT
and GPT-2 [32]. In this paper, we consider Megatron versions of
BERT and GPT-2 models provided by Nvidia; (1) MegatraonBERT
having 345M parameters with 24 layers, 1024 hidden size, and 16
attention heads, and (2) MegatronGPT-2 having 345M parameters
with 24 layers, 1024 hidden size, and 16 attention heads.

2.3 System flow
In this paper, we consider the pre-trained model approach of trans-
fer learning, where we first pick a pre-trained transformer-based
model, then a classification head is attached at the top of the final
layer of the model, and the resulting model is fine-tuned through
the software vulnerability dataset consisting of C/C++ source codes.
The overall systematic framework is illustrated in Figure 2, where
the process is divided into three main steps: pre-training, fine-
tuning, and inference.

2.3.1 Pre-training. In this work, we choose the transformer-
based models trained on a large corpus of English texts, except Code-
BERT, which is trained on source codes of various programming
languages. During the pre-training step, the models are trained in
an unsupervised fashion to understand the context of the English
words and sentences, including their syntax and semantics.

Except for CodeBERT, all BERT-based models, including RoBERTa,
and DistilBERT, are pre-trained using masked language modeling
that masked 15% of input token positions randomly, then those
masked tokens are predicted, and the model is optimized based
on the original masked tokens and the predicted tokens. More-
over, while training, the masked tokens are replaced (1) by token
‚Äú[MASK]" for 80%, (2) by a random token different than the replaced
one for 10%, and (3) by leaving the masked token as it is for 10%.
The models learn a bidirectional representation of the sentence
through masked language modeling. On the other hand, the Code-
BERT model uses two objectives; masked language modeling for the

4

Figure 2: Our systematic framework for software vulnerability detection:
pre-training, fine-tuning and inference.

Table 4: Architecture of the classification heads for our models‚Äô fine tuning.

DistilBERT

Language Model
BERT, MegatronBERT

Classification Head
Dropout Layer + Linear Layer (size = 3072)
Linear Layer (size = 3072) + ReLU + Dropout Layer +
Linear Layer (size = 3072)
Dropout Layer + Linear Layer (size = 3072) + tanh +
Dropout Layer + Linear Layer (size = 3072)
GPT-2, MegatronGPT-2 Dropout Layer + Linear Layer (size = 1024)
GPT-J

Linear Layer (size = 2048)

RoBERTa, CodeBERT

generator blocks and replaced token detection for the discriminator
block [8]. The replaced token detection enables the discriminator
block to learn effectively about the real and fake output tokens
from the generators instead of predicting masked tokens like in
masked language modeling. Unlike BERT, GPT-based models, in-
cluding MegatronGPT-2, use casual language modeling objective in
which the next word is predicted by providing all previous words
of an input sentence [28]. Thus, the learning in the GPT model is
unidirectional in nature; hence it is also called an autoregressive
model.

2.3.2

Fine-tuning and inference. Initially, all of our models, ex-
cept CodeBERT, are pre-trained on the natural language data. Now
they are specialized in the software vulnerability detection task,
which is a classification task, through fine-tuning. Usually, this is
performed with a small dataset than the pre-training dataset and
carried under supervised learning that requires the knowledge of
the data labels. For this task, a classification head is added to the
top of the pre-trained model. In this regard, we use the architecture
of the classification heads as depicted in Table 4 for the models. For
CodeBERT, only the discriminator block is used for fine-tuning and
inference.

For all the transformer-based models in this paper, we allow
the entire model architecture to update during the fine-tuning step,
which is performed with a low learning rate. This allows the model‚Äôs
pre-trained weights to be adjusted based on our software vulnerabil-
ity dataset (C/C++ source codes). Besides, all the transformer-based
models are fine-tuned for 10 epochs (whereas BiLSTM and BiGRU
are trained for 100 and 20 epochs, respectively, following the ex-
isting literature). Then, the resulting models are tested on the test

Training data:C/C++ source codesCode gadgetsTransformer-based modelsTransformer-based modelsClassification HeadClassification OutputsFine-tuning (supervised learning)Testing data:C/C++ source codesCode gadgetsDetectionTrained modelParametersModelInferenceEnglish texts or Source codesPre-training (unsupervised learning)Pre-processing, feature extractions, and vectorizationOutputsBERTEncoderEncoderEncoder‚Ä¶GPTDecoderDecoderDecoder‚Ä¶ORTransformer-based modelsdataset in the inference step, and various evaluation metrics, False
Positive Rate (FPR) and False Negative Rate (FNR) (refer to Appen-
dix C for details), are calculated.

3 EXPERIMENTS AND RESULTS
This section presents our empirical results and observations un-
der various experiments. All the transformer-based models in this
section are fine-tuned with the training dataset for 10 epochs with
learning_rate = 1.0e-05, weight_decay = 0.06, and warmup_steps
= 500. The weight decay and the warmup steps control the learn-
ing rate with the iterations while training. The total number of
iterations is calculated as follows:

Total iterations =

Total number of samples ‚àó Training epochs
Batch size

.

In all our experiments, the batch size equals 16, and a linear learning
rate scheduler is used. Now, when we set warmup_steps = 500, and
weight_decay = 0.06, then every 500 iterations (steps), our learning
rate will be decayed by 6%.

3.1 Need for clean data and testing our BiLSTM
In this paper, we consider a BiLSTM model similar to the one re-
ported by Li et al. [23]. We call the previously reported model
VulDeePecker Original model. Considering the VulDeePecker dataset
under binary classification, we experiment with our BiLSTM model
and BERTBase model on the clean and the original datasets as de-
picted in Table 1. In binary classification, the model considers only
one vulnerability; for other vulnerabilities, the model is trained
separately and independently. The BiLSTM model is trained for 100
epochs, and the BERT Base model is fine-tuned for 10 epochs. Our
results are depicted in Table 61, and they demonstrate the following:
‚Ä¢ The results of BiLSTM with the original data are better than
with the clean data for both Group 1 and Group 2 datasets. In
the original dataset, considering the high number of redundant
samples in the original dataset (see Table 5), we confirm a high
possibility of data leakage in the test dataset. Consequently, it
leads to apparently better performance. Thus, for fair results,
clean data is required for our experiments. We get the same
inference from the performance of BERTBase on the Group 2
dataset; however, there are improvements for the Buffer Error.
‚Ä¢ The performance of our BiLSTM on original data and VulDeeP-
ecker Original is similar for the Group 2 dataset but different
for the Group 1 dataset, specifically, FPR and FNR. Although we
cannot confirm the closeness of the original and our BiLSTM
model, we keep our BiLSTM model and its result for comparison.

3.2 Performance of the transformer-based
models on VulDeePecker dataset

In this paper, we perform experiments considering binary and multi-
class classifications separately. Moreover, binary classification en-
ables us to know the performance of our models on each specific
vulnerability independently. In contrast, multi-class classification

1These results are average results of three folds of the test sets. Let a test set is
represented by ùëáùëñ , for each fold ùëñ ‚àà {1, 2, 3}. If ùê∑ represents the clean VulDeePecker
dataset, then the corresponding fine-tuning train set is ùê∑ \ ùëáùëñ . The results for BiLSTM
are obtained at the end of the 100 epoch, whereas for all other models, it is obtained at
the end of the 10 epoch.

Table 5: Number of samples (code gadgets) having confliction and redun-
dancy in the uncleaned VulDeePecker dataset.

Confliction Redundancy Both Confliction & Redundancy

CWE119 - Buffer
CWE399 - Resource
Sub-total
Merged

645
86
731
741

18,989
13,992
32,981
33,050

208
40
248
257

Table 6: Test performance of BiLSTM and BERTBase for the binary classi-
fication on the original and clean dataset.

Dataset and
Vulnerability

Group 1,
Buffer Error

Group 2,
Resource
Management
Error

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score

VulDeePecker
Original [23]
2.90%
18.00%
82.00%
91.70%
86.60%
2.80%
4.70%
95.30%
94.60%
95.00%

BiLSTM
(Original Data)
24.99%
8.46%
78.57%
91.54%
84.55%
5.93%
5.76%
94.09%
94.24%
94.16%

BiLSTM
(Clean Data)
33.86%
15.27%
71.46%
84.73%
77.50%
16.10%
12.63%
84.50%
87.37%
85.86%

BERTBase
(Original Data)
2.49%
10.21%
92.76%
89.79%
91.25%
1.03%
3.48%
97.92%
96.52%
97.21%

BERTBase
(Clean Data)
4.05%
6.52%
93.56%
93.48%
93.52%
3.32%
5.82%
93.79%
94.18%
93.98%

allows us to see the model‚Äôs ability to deal with multiple vulnera-
bilities jointly. In our experiments, we find the differences in the
results, and they are presented in the following sections.

3.2.1 Binary classification. The test results for the various mod-
els in the binary classification task are presented in Table 7. Sepa-
rate experiments were performed for Group 1 and Group 2 datasets
(see Table 1), and all the models consider two output labels, viz., 0
(non-vulnerable), 1 (vulnerable: BE if Group 1 dataset and RME if
Group 2 dataset). We observe an overall improvement in results for
the transformer-based models over BiLSTM, including VulDeeP-
ecker Original model and BiGRU:
‚Ä¢ For Buffer Error, FPR is slightly improved; however, FNR re-
duction is significant. For example, GPT-2 XL has only 4.72%
compared to 18% reported originally for BiLSTM. Besides, im-
provements in Precision, Recall, and F1-scores are also significant.
For example, GPT-2 Large has an F1-score of 95.51% compared
to 86.6% reported originally for BiLSTM.

‚Ä¢ For Resource Management Error, considering the results reported
originally for BiLSTM, GPT-2 Large, GPT-2 XL, MegatronBERT,
MegatraonGPT-2, and GPT-J models have an improvement in all
metrics.

Among transformer-based models, GPT-2 Large and GPT-2 XL show
better vulnerability detection. Most interestingly, GPT-J, the largest
GPT-2-based model, is not better in detection than its smaller coun-
terparts, such as GPT-2 Large. The possible reason for this is that (1)
our dataset size might not be sufficient to fine-tune GPT-J, and (2)
GPT-J might need adjustment to its fine-tuning hyperparameters
such as learning rate and warm-up steps. We left further explo-
ration in this direction as future work. Now, analyzing the overall
trend in the improvements, the performance is in an increasing
trend (having some slight fall and rise) with the increase in the
model‚Äôs size. Refer to Figure 3(a) for an illustration.

3.2.2 Multi-class classification. The test results for the various
models in the multi-class classification are presented in Table 81.
All the experiments were performed using the Group 3 dataset (see
Table 1), and all the models consider three output labels, viz., 0
(non-vulnerable), 1 (BE), and 2 (RME). Like in binary classification,
we observe an overall improvement in results for the transformer-
based models over BiLSTM and BiGRU. Among transformer-based

5

Table 7: Average test performance1 of various models for the binary classification on clean VulDeePecker dataset Group 1 and Group 2.

Dataset and
Vulnerability

Group 1,
Buffer Error
(BE)

Group 2,
Resource
Management
Error
(RME)

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score

VulDeePecker
Original [23]
2.90%
18.00%
82.00%
91.70%
86.60%
2.80%
4.70%
95.30%
94.60%
95.00%

BiLSTM BiGRU BERTBase GPT-2 Base CodeBERT DistilBERT RoBERTa GPT-2 Large GPT-2 XL MegatronBERT MegatronGPT-2 GPT-J

33.86%
15.27%
71.46%
84.73%
77.50%
16.10%
12.63%
84.50%
87.37%
85.86%

15.19%
35.49%
73.04%
64.51%
68.37%
4.40%
10.34%
91.58%
89.66%
90.59%

4.05%
6.52%
93.56%
93.48%
93.52%
3.32%
5.82%
93.79%
94.18%
93.98%

4.20%
6.44%
93.35%
93.56%
93.45%
3.81%
5.01%
92.97%
94.99%
93.96%

2.97%
4.85%
95.27%
95.15%
95.21%
3.09%
4.71%
94.25%
95.29%
94.76%

3.85%
6.75%
93.86%
93.25%
93.55%
4.40%
7.12%
91.82%
92.88%
92.34%

4.48%
6.56%
92.95%
93.44%
93.19%
2.92%
5.20%
94.51%
94.80%
94.65%

2.67%
4.72%
95.74%
95.28%
95.51%
1.71%
3.10%
96.79%
96.90%
96.84%

2.66%
4.94%
95.75%
95.06%
95.40%
1.77%
3.28%
96.66%
96.72%
96.69%

3.25%
5.24%
94.84%
94.76%
94.80%
2.40%
3.53%
95.54%
96.47%
96.00%

2.81%
5.61%
95.49%
94.39%
94.94%
2.50%
3.03%
95.38%
96.97%
96.16%

2.74%
5.76%
95.61%
94.24%
94.90%
2.17%
3.96%
95.96%
96.04%
95.98%

Table 8: Average test performance1 of various models for the multi-class classification on clean VulDeePecker dataset Group 3.

Dataset and
Vulnerability

Group 3,
Buffer Error
(BE)

Group 3,
Resource
Management
Error
(RME)
Group 3,
BE + RME
(Global Avg.)
Group 3,
BE+ RME
(Macro Avg.)

Metrics

BiLSTM BiGRU BERTBase GPT-2 Base CodeBERT DistilBERT RoBERTa GPT-2 Large GPT-2 XL MegatronBERT MegatronGPT-2 GPT-J

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score
Precision
Recall
F1-score
Precision
Recall
F1-score

21.29%
13.95%
61.00%
86.05%
71.39%
3.40%
10.68%
74.48%
89.32%
81.20%
64.14%
86.92%
73.80%
67.74%
87.69%
76.42%

7.03%
38.64%
78.41%
61.36%
68.03%
0.66%
7.49%
93.99%
92.51%
93.23%
83.08%
69.57%
75.25%
86.20%
76.93%
81.08%

2.37%
4.85%
93.95%
95.15%
94.55%
0.66%
4.07%
94.12%
95.93%
95.02%
93.99%
95.36%
94.67%
94.04%
95.54%
94.78%

2.95%
5.41%
92.55%
94.59%
93.56%
1.02%
4.67%
91.20%
95.33%
93.21%
92.19%
94.79%
93.47%
91.88%
94.96%
93.39%

1.91%
4.83%
95.08%
95.17%
95.12%
0.64%
4.12%
94.34%
95.88%
95.10%
94.88%
95.36%
95.12%
94.71%
95.53%
95.11%

2.42%
5.83%
93.78%
94.17%
93.97%
0.73%
4.55%
93.54%
95.45%
94.48%
93.72%
94.51%
94.11%
93.66%
94.81%
94.23%

2.41%
5.68%
93.82%
94.32%
94.07%
0.75%
4.07%
93.40%
95.93%
94.65%
93.71%
94.74%
94.22%
93.61%
95.12%
94.36%

1.60%
4.96%
95.84%
95.04%
95.43%
0.42%
2.85%
96.23%
97.15%
96.68%
95.94%
95.59%
95.76%
96.03%
96.09%
96.06%

1.47%
4.77%
96.17%
95.23%
95.70%
0.42%
3.10%
96.27%
96.90%
96.58%
96.20%
95.68%
95.94%
96.22%
96.07%
96.15%

1.83%
4.61%
95.28%
95.39%
95.34%
0.50%
3.64%
95.52%
96.36%
95.93%
95.34%
95.65%
95.49%
95.40%
95.87%
95.63%

2.03%
5.08%
94.78%
94.92%
94.85%
0.56%
3.27%
95.02%
96.73%
95.86%
94.83%
95.39%
95.11%
94.90%
95.82%
95.36%

1.44%
5.22%
96.22%
94.78%
95.49%
0.25%
5.88%
97.68%
94.12%
95.87%
96.60%
94.61%
95.59%
96.95%
94.45%
95.68%

(a) Binary Classification.

(b) Multi-class Classification.

Figure 3: The overall trend of F1-score with the increasing size of the
Transformer-based models.

models, considering the global average results, GPT-2 XL shows
better Recall and F1-score; whereas GPT-J delivers better precision.
Unlike binary classification results, GPT-J, the largest GPT-2-based
model considered in this paper, has shown some better results than
its smaller counterparts for FPR and Precision. Now, analyzing
the overall trend in the improvements, the performance is in an
increasing trend (having some slight fall and rise) with the increase
in the model‚Äôs size. Refer to Figure 3(b) for an illustration.

3.2.3

Fine-tuning time. To give an idea of how long the fine-
tuning of these large models will take for the software vulnerability
detection, we present the fine-tuning time taken by our models for

Figure 4: Total time is taken in hours to fine-tune our models for 10 epochs
using one NVIDIA GPU RTX A6000 with 48GB GDDR6 GPU memory.

10 epochs with binary and multi-classification tasks. We ran our test
on our system with NVIDIA GPU RTX A6000 with 48GB GDDR6
GPU memory. All the models were fine-tuned using only one GPU
for the time measurement2. Our result is depicted in Figure 4. As per
expectation for multi-class classification tasks, among our models,
GPT-J being the largest model with 6B model parameters, took the
highest time, around 65 hours, to run for 10 epochs, and DistilBERT,
the smallest model with 66M parameters, took about 35 minutes
to run for 10 epochs. Also, we evaluated the models under the
test dataset at the end of each epoch in these runs. The pattern is
similar for the binary classification tasks with Group 1 and Group 2

2We fine-tuned all the models with HuggingFace, and we applied DeepSpeed

ZeRO Stage 2 on GPT-J only to resolve CUDA Error: out of memory issue.

6

Table 9: Division of SeVC dataset based on its categories. The number
of vulnerable and non-vulnerable samples are indicated by ‚ÄòV‚Äô and ‚ÄòNV‚Äô,
respectively, in the table.

Dataset

Group 4

Group 5

Group 6

Group 7

Group 8

Categories
API Function
Call (AFC)
Arithmetic
Expression (AE)
Array
Usage (AU)
Pointer
Usage (PU)
AFC + AE +
AU + PU

Original

Cleaned

64403

54181

22154

14454

42229

34166

291841

176883

420627

206376

Train
43344
(V: 10647, NV:32697)
11563
(V: 2642, NV: 8921)
27332
(V: 8237, NV: 19095)
141506
(V: 21189, NV: 120317 )
165100
(V: 274804, NV: 145823)

Test
10837
(V: 2611, NV: 8226)
2891
(V: 648, NV: 2243)
6834
(V: 2145, NV: 4689)
35377
(V: 5335, NV: 30042)
41276
(V: 4769 , NV: 36507)

datasets. For a model, the fine-tuning time is different for different
datasets because of the different sizes of the fine-tuning datasets
(see Table 1 where the training dataset is used for fine-tuning these
models).

Insight 1. While choosing the model for the vulnerability detec-
tion, if there is no time constraint for the model‚Äôs fine-tuning, then
we can pick one of the best performing models, e.g., GPT-2 Large
for the Group 1 dataset and F1-score. If there is a time constraint,
then we need to pick the model that has the best trade-off between
the performance and fine-tuning time, e.g., CodeBERT for Group 1
dataset and F1-score.

3.3 Performance of the transformer-based

models on the SeVC dataset

For the studies with more than two vulnerabilities, we consider the
Semantics-based Vulnerability Candidate (SeVC) dataset [22] hav-
ing 126 different vulnerabilities (refer to Appendix A.2 for details).
Broadly SeVC is divided into four categories based on the cause of
vulnerabilities; API Function Call, Arithmetic Expression, Array
Usage, and Pointer Usage. Overall, we divided the dataset into 5
groups for our studies. Refer to Table 9 for details.

We have performed binary and multi-class classification tasks for
the SeVC dataset considering BiLSTM, BiGRU, BERTBase, and GPT-
2 Base models. For the binary classification task, BERTBase and
GPT-2 Base have an improvement over BiLSTM in all metrics except
FPR and Precision for Group 4 and FPR for Group 6 & Group 7.
For example, the Group 4 dataset has an F1-score of 90.03% with
BERTBase compared to the previously reported results of 86.80%
and 78.30% with BiLSTMs. Refer to Table 10 for details. Besides,
except for the Group 6 dataset, BiGRU has better FNR compared
to other models. There is no result reported in the previous work
for multi-class classification tasks, so we compare BERTBase and
GPT-2 Base. Considering the global averaged results, BERTBase has
performed better than GPT-2 Base. It has 88.34% F1-score. Refer to
Table 11 for details. In contrast to binary classification, multi-class
classification with SeVC has a low F1-score. This is understandable
from the fact that SeVC has multiple vulnerabilities that increase
the complexity of machine learning.

4 PLATFORM ANALYSIS
There is a rising trend of releasing bigger models, usually transformer-
based models. For instance, the BERTBase model has 110 million pa-
rameters, and the BERTLarge has 340 million parameters [5, 29, 35],
which Google released in 2018. OpenAI released the GPT-2 language
model with 1.5 billion parameters in 2019, and it was followed by

7

Figure 5: Deep learning model parallelism approaches.

the GPT-3 model, which has 175 billion parameters [27, 28]. These
models are shown to be outperforming the existing models in terms
of accuracy or precision. These models are used in various fields via
the downstream task, where the model is further trained to adjust
to the new targeted dataset. On the flip side, handling these large
models requires substantial computational resources. Precisely, a
single GPU is not enough to train extra-large models with large-
scale data because these models have too many parameters to train,
and there are too many samples to process on a single GPU. Thus,
usually end up with a ‚ÄúCUDA out of memory‚Äù error. For example, in
our experiments, we could not run GPT-2 Large with 774M model
parameters on an NVIDIA V100 GPU with 16GB internal mem-
ory, even with a batch size of 1. In this regard, we consider deep
learning parallelism approaches (presented in [27]) to resolve the
challenge due to limited GPU memory. For an illustration of the
approaches, refer to Figure 5. There are multiple types of paral-
lelism methods that are defined based on how the data and models
are computed collaboratively with multiple GPUs. Usually, these
methods are collectively called a 3D Parallelism and are described
in the following:

‚Ä¢ Data parallelism: Data parallelism splits a large dataset into
smaller batches, and each GPU (or GPU group) holds an identical
copy of the model parameters. Then, each GPU (or GPU group)
sends the computed gradients to a parameter server. The param-
eter server aggregates the computed gradients and calculates the
updates. Afterward, it sends the updates to each GPU (or GPU
group) for updating, and each GPU (or GPU group) processes the
next batch.

‚Ä¢ Pipeline parallelism (vertical parallelism): Pipeline paral-
lelism enables model parallelism. This approach shares neural
network layers into stages with an equal number (desirably) of
layers on each stage. Each stage is processed by one GPU (or GPU
group), and the calculated output is forwarded to the next stage.
For example, splitting a model with 24 layers across 4 stages
would mean each stage gets 6 layers. Then, each GPU (or GPU
group) processes the assigned stage and passes the output to the
following GPU (or GPU group).

‚Ä¢ Model parallelism (tensor parallelism or horizontal paral-
lelism): Model parallelism splits the execution of a single layer
over multiple GPUs, while Pipeline parallelism splits multiple
layers across multiple GPUs. Each layer is split up into multi-
ple chunks, and each piece belongs to a designated GPU. The
processed results are synced at the end of the step.

GPU 16GPU 20GPU 24GPU 28GPU 0GPU 4GPU 8GPU 12Data ParallelismPipeline ParallelismLayer 1Layer 4Table 10: Test performance of various models for the binary classification on clean SeVC dataset. ‚ÄòNA‚Äô refers to ‚ÄòNot Available‚Äô.

Dataset and
Vulnerability
Group 4,
API
Function
Call
(AFC)

Group 5,
Arithmetic
Expression
(AE)

Group 6,
Array
Usage
(AU)

Group 7,
Pointer
Usage
(PU)

Metrics

FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score

VulDeePecker
(BiLSTM [22])
5.50%
22.50%
79.10%
77.52%
78.30%
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA
NA

SySeVR
(BiLSTM [22])
2.10%
17.50%
91.50%
82.56%
86.80%
3.80%
17.10%
88.30%
82.87%
85.50%
1.50%
18.30%
87.90%
81.72%
84.70%
1.30%
19.70%
87.30%
80.39%
83.70%

Our
BiLSTM
21.08%
8.91%
81.21%
91.09%
85.87%
15.43%
8.30%
85.60%
91.70%
88.55%
19.73%
14.26%
81.29%
85.74%
83.46%
15.66%
5.43%
85.80%
94.57%
89.97%

BiGRU

BERTBase

GPT-2 Base

15.50%
8.80%
85.47%
91.20%
88.24%
18.34%
6.55%
83.59%
93.45%
88.25%
18.15%
13.59%
82.64%
86.41%
84.49%
12.99%
4.78%
87.99%
95.22%
91.46%

3.63%
9.06%
89.14%
90.94%
90.03%
3.09%
9.32%
90.16%
90.68%
90.42%
3.58%
14.35%
91.30%
85.65%
88.38%
1.40%
7.96%
92.02%
92.04%
92.03%

3.28%
11.01%
89.87%
88.99%
89.43%
2.32%
11.21%
92.28%
88.79%
90.50%
4.32%
12.24%
89.92%
87.76%
88.82%
1.54%
8.25%
91.29%
91.75%
91.52%

Table 11: Test performance of various models for the multi-class classifi-
cation on clean SeVC dataset.

Dataset and
Vulnerability
Group 8,
API
Function
Call
(AFC)

Group 8,
Arithmetic
Expression
(AE)

Group 8,
Array
Usage
(AU)

Group 8,
Pointer
Usage
(PU)

Group 8,
AFC + AE + AU + PU
(Global Avg.)
Group 8,
AFC + AE + AU + PU
(Macro Avg.)

Metrics

BERTBase

GPT-2 Base

FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score
FPR
FNR
Precision
Recall
F1- score
Precision
Recall
F1 score
Precision
Recall
F1 score

0.11%
25.64%
83.45%
74.36%
78.64%
0.21%
9.96%
85.40%
90.04%
87.65%
0.39%
12.44%
85.16%
87.56%
86.34%
0.79%
9.60%
89.85%
90.40%
90.12%
87.95%
88.73%
88.34%
85.97%
85.59%
85.78%

0.05%
33.33%
90.83%
66.67%
76.89%
0.27%
9.60%
81.94%
90.40%
85.96%
0.44%
11.18%
83.70%
88.82%
86.19%
0.87%
11.35%
88.77%
88.65%
88.71%
86.88%
87.47%
87.18%
86.31%
83.63%
84.95%

Table 12: Summary of popular open-sourced machine learning platforms.

Description

HuggingFace
A machine learning
framework for Jax,
Pytorch and TensorFlow
‚úì

Data Parallelism
Pipeline Parallelism Partial (need customization)
Tensor Parallelism
Memory efficiency
Training speed

‚úó
Normal
Normal

Megatron

An implementation
of Transformer

‚úì
‚úì
‚úì
Normal
Good

Type

Can use Megatron-LM,
and all models

Dedicated only
for Megatron-LM

DeepSpeed
A deep learning
optimization library for
distributed training
‚úì
‚úì
‚úì
Excellent
Great
Just a library, supplement
tool for memory
efficiency and speed

Horovod

A python library
for data parallelism

‚úì
‚úó
‚úó
Normal
Normal

Dedicated only for
Data Parallelism

4.1 Platforms
To leverage the parallelism while model training/testing in our
studies, we have analyzed four popular open-sourced platforms.
Table 12 summarizes these platforms, and details are presented in
the following paragraphs.

Horovod Horovod [9] is a stand-alone Python library for data par-
allelism. It uses an optimized ring-all reduce algorithm to improve
both performance and usability. It supports TensorFlow, Keras, Py-
Torch, and Apache MXNet. It can achieve linear performance gain

8

provided that the portion of parameters in the dense layers to all pa-
rameters is small [13]. Horovod developers claimed that it achieved
90% scaling efficiency for Inception V3 and ResNet-101 models and
68% scaling efficiency for the VGG-16 model. Model parallelism
means models are split and can be evaluated concurrently. With this
definition, Horovod supports model partitioning for workload divi-
sion but does not support model or pipeline parallelism. Without
modification of Horovod implementation, it can train only mod-
els that fit into a single GPU. Consequently, we do not consider
Horovod as a development framework in this work.

Megatron Framework NVIDIA released Megatron language mod-
els and a PyTorch-based framework to train/test the models [32].
The framework and the model support not only model parallelism
(pipeline and tensor) but also data parallelism. NVIDIA trained
MegatronGPT-2 (8.3 billion parameters) with 8-way model paral-
lelism and 64-way data parallelism, trained on 512 GPUs (NVIDIA
Tesla V100) using mixed precision. This is 24√ó the size of BERT
and 5.6√ó the size of GPT-2 (the previous largest version). Mega-
tronBERT has 3.9 billion parameters, which is 12√ó the size of the
BERTLarge model [27, 28]. As the Megatron framework supports
all three parallelism approaches, we utilised it as one of the devel-
opment frameworks for this work. However, the Megatron frame-
work is model-specific, and hard to utilise other pre-trained models
within the framework. So, we only evaluated Megatron BERT 345M
model and the GPT-2 345M model by implementing our own model
providers, data providers, and metric function provider functions.

DeepSpeed DeepSpeed [25] is an open-source deep-learning op-
timization library released by Microsoft for PyTorch. It delivers
extreme-scale, extreme memory efficient, and extremely commu-
nication efficient model training by leveraging model parallelism
on existing computer resources. DeepSpeed introduces Zero Re-
dundancy Optimizer (ZeRO), and it enables 10√ó larger models, 10√ó
faster training, and minimal code change. As a specific example,
the memory consumption for training a 7.5B parameter model is
about 120GB of GPU RAM (the calculation is based on 64 GPUs).
The ZeRO Stage 1 partitions optimizer states across the GPUs and
requires 31.4GB of GPU RAM (the calculation is based on 64 GPUs).
The ZeRO Stage 2 reduces 32-bit gradients to 16-bit for updating

the model weights, and it requires 16.6GB of GPU RAM. In ZeRO
Stage 3, the 16-bit model parameters are partitioned across the GPU,
and it requires only 1.9GB of GPU RAM. Recently DeepSpeed intro-
duced ZeRO-Infinity, which leverages the total memory capacity of
a system, concurrently exploiting all heterogeneous memory (GPU,
CPU, and NVMe). They claimed that with a single NVIDIA DGX-2
node, a 1000 billion parameter model could be trained. The key idea
of ZeRO-Infinity is offloading data into other types of memory. It
splits a model into multiple states and stores into CPU or NVMe
device memory, which are usually much bigger than GPU memory,
and GPU holds only a small portion of states for computing. Due
to the overall features of DeepSpeed, we have used it to carry out
our experiments in this work.

HuggingFace HuggingFace [7] supports only data parallelism but
did not officially implement model parallelism (neither pipeline nor
tensor). However, HuggingFace integrates DeepSpeed, enabling
users to enjoy the benefits of DeepSpeed, such as ZeRO stages.
There are two options to integrate DeepSpeed:

‚Ä¢ Integration of the core DeepSpeed features via Trainer: Users
need to use Trainer and supply the DeepSpeed config file. The rest
of the things will be done automatically. We strongly recommend
this integration method.

‚Ä¢ Integrate DeepSpeed by ourself: Core functionality functions
must be implemented, such as from_pretrained and from_config.

4.2 Discussion on the platforms
In this section, firstly, we present the challenges that we faced, then
provide our recommendations.

4.2.1 Challenges. (1) No admin privilege: Institutions have
HPC clusters that consist of multiple nodes. For example, we have
an HPC cluster with 114 nodes. Each node has two Xeon 14-core E5-
2690 v4 CPUs and four NVIDIA Tesla P100 16 gigabytes GPUs. Due
to security and maintenance reasons, the service owner does not
provide admin privilege to HPC users, and this policy has created
many issues. For instance, we cannot install system-dependent
programs/libraries by ourselves, and due to the version of HPC OS
(SUSE Linux 12.4), we could not use the latest versions of NVIDIA
drivers and CUDA. We have used Anaconda [2] to set up virtual
environments, and within the virtual environments, we installed
pre-requisites and dependencies, which were not supported by the
HPC environment directly (in non-virtual environments).
(2) Model Parallelism: Some models in the HuggingFace frame-
work support a naive pipeline model parallelism, such as GPT-2 and
T5. Users need to define a dictionary that maps attention layers to
GPUs, and the embedding layer and LMHead are always automat-
ically mapped to the first device. This is an experimental feature
and has some GPU idling problems (it is hard to balance workload
between GPUs). Megatron supports tensor model parallelism and
pipeline model parallelism. However, NVIDIA does not provide
parallelised pre-trained models, and users need to write a program
to split/merge Megatron-LM models for model parallelism.
(3) Small GPU RAM: Many internal users share the HPC cluster
in an institution, and acquiring GPU computing resources is very
competitive. Moreover, each GPU can have less RAM size. For
example, our has only 16 gigabyte GPU RAM. These restrictions

9

Table 13: Fine-tuning performance comparison results of GPT-2 XL model
with/without DeepSpeed.

Model
Number of GPU
Applied DeepSpeed
Parallelization
Epoch
Batch Size / GPU
Train Samples
Train Runtime
Train Runtime / epoch
Train Samples / second
Train Samples / second / GPU
Train Runtime for 1 sample
Multi-GPU overhead
Average GPU RAM usage
(MB, batch size: 1)
DeepSpeed Runtime Gain
DeepSpeed Memory Gain

GPT-2 XL
1
No
-
2
16
22072
7H:38M:06S
3H:49M:03S
1.606
1.606
0.623
-

GPT-2 XL
2
No
Data
2
16
22072
3H:52M:09S
1H:56M:05S
3.169
1.584
0.631
1.36%

29633

35755

-
-

-
-

GPT-2 XL
1
Stage 2
-
2
16
22072
5H:38M:46S
2H:49M:23S
2.172
2.172
0.460
-

12515

26.05%
57.77%

GPT-2 XL
2
Stage 2
Data
2
16
22072
3H:23M:16S
1H:41M:38S
3.620
1.810
0.553
20.00%

12285

12.45%
65.64%

created issues, especially when we fine-tuned the large models. For
instance, to fine-tune the GPT-2 Extra Large model for the dataset,
we used 4 GPUs with the HuggingFace pipeline model parallelism.
Due to the overhead of parallelism and the GPU idling problems,
the fine-tuning task required a very long processing time, but we
could only acquire some of the required computing resources, and
it delayed our experiments.

4.2.2 Our recommendations. As a machine learning framework,
HuggingFace provides thousands of pre-trained models to perform
various tasks on text, image, and sound. The framework provides
not only easy-to-use APIs but also affordable memory efficiency and
training speed. However, it supports only data parallelism and is
very hard to train large models due to the lack of model parallelism
supports. Megatron framework can be an alternative platform to
perform tasks with large models as it supports 3D parallelism. Nev-
ertheless, the Megatron framework only provides a limited number
of pre-trained language models. Besides, the DeepSpeed framework
supports 3D parallelism with very high memory efficiency and high
training speed. Moreover, it can be easily integrated with Hugging-
Face models. We applied DeepSpeed ZeRO Stage 2 on HuggingFace
models to resolve our engineering challenges. With a single 16GB
RAM GPU, we could fine-tune the GPT-2 XL model. Even though
it was not practical, we could barely fine-tune the GPT-J model,
consisting of 6 billion parameters, only with a single GPU.

We evaluated the training performance of the DeepSpeed frame-
work in terms of processing speed and memory efficiency. We ran
our test on our system with two RTX A6000 48GB GPUs, and the
performance comparison results are depicted in Table 13. When
we were fine-tuning the GPT-2 XL model with a single GPU, the
average train runtime per epoch was 3H : 49M : 03S without
DeepSpeed. But with DeepSpeed, the average train runtime per
epoch was 2H : 49M : 23S, 26.05% of the performance gain. More
interestingly, when we applied DeepSpeed on the fine-tuning task,
GPU RAM usage was cut by 57.77%, making a large model fit into
a small GPU RAM. Similarly, we could get 12.45% processing speed
gain and 65.64% memory gain from the 2-GPU comparison.

Insight 2. We summarize our recommendations in the following:
‚Ä¢ Stick with data parallelism if the model fits inside one GPU.
‚Ä¢ If we could not accommodate the model inside one GPU, go with

Huggingface and DeepSpeed frameworks.

5 RELATED WORK
To automate the generation of program documents and facilitate the
natural language code search, a transformer-based language model,
called CodeBERT [8], is trained on pairs of natural languages and
programming languages; for example, a source code slice having
few comments in natural language regarding program description
followed by the code instructions. CodeBERT‚Äôs architecture consists
of a BERT model and two generators, one for code instructions and
the other for the natural language description. CodeBERT has a few
variants, such as (i) GraphCodeBERT [12] that uses semantic-level
data flow structure, (ii) BART [18], with both encoder and decoder
part in its model architecture, and its extension (iii) PLBART [1]. In
our work, we consider only CodeBERT because it is a base model
and investigate its usage in software vulnerability detection, which
has not been explored so far.

A BERTBase model is used for software vulnerability detec-
tion [44]. It was fined tuned with Software Assurance Reference
Dataset (SARD) database of 100K C/C++ source files and tested with
123 vulnerabilities. This work showed that the BERTBase model
and BERT with RNN heads of LSTM or BiLSTM models outperform
standard LSTM or BiLSTM models. The highest detection accuracy
reported was 93.49% for their dataset and model. In contrast to this
work, we consider a large dataset and multiple model architectures
like GPT-2 and MegatronBERT [32]. Besides, the data input form
is different; they used the source file after removing labels and
comments, whereas we leverage code gadgets.

Except for a few transformer-based models (aforementioned),
software vulnerability detection has been investigated through (i)
dynamic analysis, (ii) pattern matching, and (iii) machine learning
with Convolutional Neural Networks (CNN) and Recurrent Neu-
ral Networks (RNN)-based models. Dynamic analysis executes a
program looking for unusual or unexpected behavior. Fuzzing [33]
(or Atheris [11] for python codes) and the taint analysis [26] are
good examples of dynamic techniques. Unfortunately, they are
not efficient for a long code/program as their runtime increases
exponentially with the code/program length.

On the pattern matching side, (i) code similarity techniques,
(ii) rule-based techniques, and (iii) code-property graphs are used.
Code-similarity techniques find vulnerabilities by comparing a cur-
rently scanned code with signatures of identified vulnerable codes.
Those signatures are created by hashing, selection of substrings, or
abstract representation such as graphs and trees. VUDDY [17] and
Vulpecker [21] are examples of code-similarity techniques. For the
rule-based technique, the analyst decides on rules that identify vul-
nerabilities. Flawfinder [40] and Coverity [4] apply the rule-based
method. Unfortunately, both code-similarity and rule-based tech-
niques produce either high false negatives or high false positives,
or both. This is due to the fact that the decision about vulnerabil-
ity depends solely on historical data. Consequently, new unseen
vulnerabilities cannot be detected. Vulnerability detection works
in two steps for code-property graph techniques: First, for a given
source code, an appropriate graph is constructed by combining its
abstract syntax tree and program dependence graph. Then, the code-
property graph is traversed to detect vulnerabilities [41]. Sadly, this
technique still requires human intervention and supervision.

Machine learning automates the detection and learning process
with limited or no human intervention. However, classical machine
learning requires feature engineering, which is difficult, extensively
laborious, error-prone, and also needs human help to some extent.
Thus, a significant body of research has studied deep learning but is
limited to CNN and RNN-based models [19, 23, 30]. As these models
require formatted data to capture important features related to the
vulnerabilities, methods such as the lexed representation of C/C++
code [30], code gadget [23], code-property graphs [6], improved
code gadget with code attention and system dependency graph [45],
and a minimum intermediate representation learning [19] are pro-
posed. Besides deep learning language models, some works have
been done using graph neural networks in software vulnerability
detection [42]. The method is named Devign, and it captures com-
posite programming representations (abstract syntax tree, control
flow, and data flow) of source codes. Overall, all these methods
improve one over the other, but still, the results can be improved.
Our work focuses on the standard code gadget and its extraction
rather than its improved versions to see the relevance of the ap-
proach with the transformer-based language models. Moreover, any
improved versions of extraction techniques for the code represen-
tation can be easily transferred to our work by updating the code
gadget. Thus, we do not use minimum intermediate representation
learning and graph neural networks. However, in most cases, our
results with transformer-based models and standard code gadgets
are better than those with minimum intermediate representation
(see Section 3). We cannot compare our results with graph neural
network results due to their results in different datasets.

6 CONCLUSION
This paper studied transformer-based language models for software
vulnerability detection. Firstly, it presented a systematic framework
to use these models in the domain. Then, it presented the compar-
ative performances of these models along with recurrent neural
network (RNN)-based models under binary and multi-class classi-
fication tasks. For the dataset with two vulnerabilities, buffer and
resource management errors, related to the API function call, the
transformer-based language models outperformed BiLSTM and Bi-
GRU in all performance metrics (e.g., FPR, FNR, and F1-score). More
precisely, GPT-2 Large and GPT-2 XL have the best F1-score for
binary and multi-class classification (global average), respectively.
The overall trend for F1-score was increasing with the models‚Äô
increasing size. For a separate dataset with 126 types of different
vulnerabilities (related to 341 CWE IDs) falling under four broad
categories ‚Äì API function call, pointer usage, array usage, and arith-
metic expressions ‚Äì this paper studied the performance of BiLSTM,
BiGRU, BERTBase, and GPT-2 Base. Our results in binary classifi-
cation tasks demonstrated that BERTBase and GPT-2 have better
F1-score, but not good than BiGRU in FNR except for the array
usage category. Overall, the transformer-based language models
performed well in vulnerability detection. As these language mod-
els are difficult to run due to the challenges related to GPU memory
size, libraries to perform model parallelism, and installation of the
dependencies to the environment, this paper analyzed the popular
platforms and presented the best methods to run these models.

10

7 ACKNOWLEDGEMENT
This material is based upon work supported by the US Army Interna-
tional Technology Center Indo-Pacific (ITC-IPAC) under Contract
No. FA520921P0015. Any opinions, findings and conclusions, or rec-
ommendations expressed in this material are those of the author(s)
and do not necessarily reflect the views of the U.S. Army ITC-IPAC.
Besides, we acknowledge our vacation student Mr. Andrew Cain
for his help during the preliminary stage of this work. The work
of Josef Pieprzyk was supported in part by Polish National Science
Center (NCN) under Grant 2018/31/B/ST6/03003.

REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.
2021. Unified Pre-training for Program Understanding and Generation. CoRR
abs/2103.06333 (2021). arXiv:2103.06333 https://arxiv.org/abs/2103.06333

[2] Anaconda. Anaconda. https://www.anaconda.com.
[3] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder-Decoder for Statistical Machine Translation.
(2014). https://doi.org/10.48550/ARXIV.1406.1078

[4] Coverity. Coverity. https://scan.coverity.com/, last accessed on 01 July 2021.
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
In Proc. the 2019 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT. 4171‚Äì
4186. https://doi.org/10.18653/v1/n19-1423

[6] Xu Duan, Jingzheng Wu, Shouling Ji, Zhiqing Rui, Tianyue Luo, Mutian Yang, and
Yanjun Wu. 2019. VulSniper: Focus Your Attention to Shoot Fine-Grained Vul-
nerabilities. In Proceedings of the 28th International Joint Conference on Artificial
Intelligence (IJCAI‚Äô19). AAAI Press, 4665‚Äì4671.

[7] Hugging Face. Transformers: State-of-the-art Machine Learning for JAX, PyTorch

and TensorFlow. https://huggingface.co/docs/transformers/quicktour.

[8] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,
Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: A
Pre-Trained Model for Programming and Natural Languages. In Proc. of Findings
of the Association for Computational Linguistics: EMNLP 2020. 1536‚Äì1547. https:
//doi.org/10.18653/v1/2020.findings-emnlp.139

[9] The Linux Foundation. Horovod. https://horovod.ai.
[10] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and
Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language
Modeling. arXiv preprint arXiv:2101.00027 (2020).
[11] Google. 2020. Atheris. https://pypi.org/project/atheris/.
[12] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou,
Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng,
Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming
Zhou. 2020. GraphCodeBERT: Pre-training Code Representations with Data Flow.
CoRR abs/2009.08366 (2020). arXiv:2009.08366 https://arxiv.org/abs/2009.08366
[13] B. Hasheminezhad, S. Shirzad, N. Wu, P. Diehl, H. Schulz, and H. Kaiser. 2020.
Towards a Scalable and Distributed Infrastructure for Deep Learning Applications.
In 2020 IEEE/ACM Fifth Workshop on Deep Learning on Supercomputers (DLS).
IEEE Computer Society, Los Alamitos, CA, USA, 20‚Äì30. https://doi.org/10.1109/
DLS51937.2020.00008

[14] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural
Computation 9, 8 (1997), 1735‚Äì1780. https://direct.mit.edu/neco/article-abstract/
9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext

[15] Huggingface.co. Tokenizer summary. https://huggingface.co/transformers/v3.0.

2/tokenizer_summary.html.

[16] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc
Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of Semantic
Code Search. CoRR abs/1909.09436 (2019). arXiv:1909.09436 http://arxiv.org/abs/
1909.09436

[17] Seulbae Kim, Seunghoon Woo, Heejo Lee, and Hakjoo Oh. 2017. VUDDY: A
Scalable Approach for Vulnerable Code Clone Discovery. In 2017 IEEE Symposium
on Security and Privacy (SP). 595‚Äì614. https://doi.org/10.1109/SP.2017.62
[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2019. BART:
Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
Translation, and Comprehension. CoRR abs/1910.13461 (2019). arXiv:1910.13461
http://arxiv.org/abs/1910.13461

[19] Xin Li, Lu Wang, Yang Xin, Yixian Yang, and Yuling Chen. 2020. Automated Vul-
nerability Detection in Source Code Using Minimum Intermediate Representation
Learning. Applied Sciences 10, 5 (2020). https://doi.org/10.3390/app10051692

[20] Zhen Li, Deqing Zou, Shouhuai Xu, Zhaoxuan Chen, Yawei Zhu, and Hai Jin. 2021.
VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector. IEEE
Transactions on Dependable and Secure Computing (2021), 1‚Äì1. https://doi.org/10.
1109/TDSC.2021.3076142

[21] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Hanchao Qi, and Jie Hu. 2016.
VulPecker: an automated vulnerability detection system based on code similarity
analysis. In Proc. of the 32nd Annual Conference on Computer Security Applications.
201‚Äì213. https://doi.org/10.1145/2991079.2991102

[22] Zhen Li, Deqing Zou, Shouhuai Xu, Hai Jin, Yawei Zhu, and Zhaoxuan Chen. 2021.
SySeVR: A framework for using deep learning to detect software vulnerabilities.
IEEE Trans. Dependable Sec. Comput (2021). https://doi.org/abs/1807.06756
[23] Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang, Zhijun
Deng, and Yuyi Zhong. 2018. VulDeePecker: A Deep Learning-Based System
for Vulnerability Detection. In Proc. Network and Distributed System Security
Symposium (NDSS). Internet Society. https://doi.org/10.14722/ndss.2018.23158

[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A
Robustly Optimized BERT Pretraining Approach. CoRR abs/1907.11692 (2019).
arXiv:1907.11692 http://arxiv.org/abs/1907.11692

[25] Microsoft. 2022. DeepSpeed. https://www.deepspeed.ai.
[26] James Newsome and Dawn Song. 2005. Dynamic Taint Analysis for Automatic
Detection, Analysis, and Signature Generation of Exploits on Commodity Soft-
ware. In Proc. NDSS.

[27] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
Improving Language Understanding by Generative Pre-Training.

2018.
https://cdn.openai.com/research-covers/language-unsupervised/language_
understanding_paper.pdf.

[28] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019.
Language Models are Unsupervised Multitask Learners.
(2019). https://cdn.openai.com/better-language-models/language_models_are_
unsupervised_multitask_learners.pdf.

[29] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A Primer in BERTol-
ogy: What We Know About How BERT Works. Trans. of the Association for
Computational Linguistics 8 (2020), 842‚Äì866. https://doi.org/10.1162/tacl_a_00349
[30] Rebecca L. Russell, Louis Kim, Lei H. Hamilton, Tomo Lazovich, Jacob A. Harer,
Onur Ozdemir, Paul M. Ellingwood, and Marc W. McConley. 2018. Automated
Vulnerability Detection in Source Code Using Deep Representation Learning. In
Proc. ICMLA. 757‚Äì 762.

[31] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Dis-
tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. In Proc.
ùê∏ùëÄùê∂2

: 5th Edition Co-located with NeurIPS‚Äô19. 1‚Äì5.

[32] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro. 2020. Megatron-LM: Training Multi-Billion Parameter
Language Models Using Model Parallelism. arXiv:cs.CL/1909.08053

[33] Michael Sutton, Adam Greene, and Pedram Amini. 2007. Fuzzing: Brute Force
Vulnerability Discovery.
Pearson Education (2007). https://doi.org/books?
hl=en&lr=&id=DPAwwn7QDy8C&oi=fnd&pg=PT21&ots=4yt9E59Owq&sig=
-Ik4SyRTD9YTvmMnYcpKQMH2jz4&redir_esc=y#v=onepage&q&f=false

[34] the software quality company TIOBE. TIOBE Index for May 2022.

https:

//www.tiobe.com/tiobe-index/.

[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. Advances in neural information processing systems, Vol. 30.
Curran Associates, Inc., 5998‚Äì6008.

[36] VULDB. Apple macOS USD File buffer overflow. https://vuldb.com/?id.163591.
[37] VULDB. Facebook WhatsApp on Android Video Stream buffer overflow. https:

//vuldb.com/?id.160672.

[38] VULDB. NVIDIA Shield TV up to 8.2.1 NVDEC buffer overflow. https://vuldb.

com/?id.168508.

[39] VulDeePecker. Database of "VulDeePecker: A Deep Learning-Based System for
Vulnerability Detection". https://github.com/CGCL-codes/VulDeePecker.

[40] David A. Wheeler. 2018. Flawfinder. https://dwheeler.com/flawfinder/.
[41] Fabian Yamaguchi, Nico Golde, Daniel Arp, and Konrad Rieck. 2014. Modeling and
discovering vulnerabilities with code property graphs. In Proc. IEEE Symposium
on Security and Privacy. 590‚Äì604.

[42] Yaqin Zhou, Shangqing Liu, Jingkai Siow, Xiaoning Du, and Yang Liu. 2019. De-
vign: Effective Vulnerability Identification by Learning Comprehensive Program
Semantics via Graph Neural Networks. In Proc. NeurIPS.

[43] Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards
Story-like Visual Explanations by Watching Movies and Reading Books. CoRR
abs/1506.06724 (2015). arXiv:1506.06724 http://arxiv.org/abs/1506.06724
[44] Noah Ziems and Shaoen Wu. 2021. Security Vulnerability Detection Using Deep

Learning Natural Language Processing. arXiv:cs.CR/2105.02388

[45] Deqing Zou, Sujuan Wang, Shouhuai Xu, Zhen Li, and Hai Jin. 2019.
¬µVulDeePecker: A deep learning-based system for multiclass vulnerability detec-
tion. IEEE Trans. Dependable Sec. Comput (2019). https://doi.org/10.1109/TDSC.

11

is collected from 1591 open-source C/C++ programs from the Na-
tional Vulnerability Database (NVD) and 14000 programs from the
Software Assurance Reference Dataset (SARD). Moreover, it has
56395 and 364232 vulnerable and clean samples. The SeVC dataset
is divided into four major categories based on the cause of the
vulnerability in the following way:

(1) Library/API Function Call: This category has vulnerabilities

related to library/API function calls.

(2) Array Usage: This category has vulnerabilities related to
arrays, like address transfer as a function parameter and
improper array element access.

(3) Pointer Usage: This category has vulnerabilities related to
pointers, like improper use in pointer arithmetic and wrong
referencing.

(4) Arithmetic Expression: This category has vulnerabilities re-

lated to arithmetic expressions, like integer overflow.

B MODELS
B.1 Bidirectional Long Short Term Memory
Bidirectional Long Short Term Memory (BiLSTM) are recurrent
neural networks. It has two long short-term memory (LSTMs) [14] ‚Äì
one LSTM takes the input in the forward direction and the other in
the backward direction ‚Äì which enables BiLSTM to learn long-term
dependencies in the input sequence efficiently. BiLSTM architec-
ture is depicted in Figure 8. The LSTM block in the BiLSTM has
the cell state regulated by three gates, namely forget gate, update
gate, and output gate. Each LSTM cell has three inputs: (i) memory
component ùê∂ùë° ‚àí1 and activation component ùëéùë° ‚àí1 are the two inputs
from the previous cell, and (ii) word embedding ùë•ùë° at time ùë° of input
data. Computations are carried out as presented in the following:

Forget gate: ùëìùë° = ùúé (ùëäùëì ¬∑ [ùëéùë° ‚àí1, ùë•ùë° ] + ùëè ùëì ),
Update gate: ùëñùë° = ùúé (ùëäùë¢ ¬∑ [ùëéùë° ‚àí1, ùë•ùë° ] + ùëèùë¢ ),
Output gate: ùëúùë° = ùúé (ùëäùëú ¬∑ [ùëéùë° ‚àí1, ùë•ùë° ] + ùëèùëú ),

where ùëè is the bias,ùëä is the learning parameter (weight matrix), and
ùúé is the Sigmoid function. The memory component ùê∂ùë° is calculated
as:

ùê∂ùë° = ùëìùë° ‚àó ùê∂ùë° ‚àí1 + ùëñùë° ‚àó Àúùê∂ùë° ,
where Àúùê∂ùë° = ùë°ùëéùëõ‚Ñé(ùëäùëê [ùëéùë° ‚àí1, ùë•ùë° ] + ùëèùëê , and ‚Äò*‚Äò is elementwise multi-
plication. The activation component for the next cell is calculated
as:

ùëéùë° = ùëúùë° ‚àó tanh(ùê∂ùë° ).

B.2 Bidirectional Gated Recurrent Unit
Bidirectional Gated Recurrent Unit (BiGRU) [3] is a recurrent neural
network with gated recurrent units (GRUs). A GRU has a gating
mechanism, and it is similar to LSTM but has a forget gate, fewer
parameters, and no output gate. The architecture of BiGRU is the
same as in Figure 8 where each LSTM cell is replaced by a GRU cell.
Moreover, each GRU cell has two inputs; memory component ùê∂ùë° ‚àí1
from the previous cell and word embedding ùë•ùë° at time ùë° of input

(a) Vulnerable with Buffer Error

(b) Non-vulnerable

Figure 6: An example of a program with Buffer Error and its corrected
version.

(a) Vulnerable with RME Error

(b) Non-vulnerable

Figure 7: An example of a program with Resource Management Error
(RME) and its corrected version.

2019.2942930

A DATASETS
A.1 VulDeePecker Data
We consider the dataset published by CGCL/SCTS/BDTS Lab1 of
Huazhong University of Science and Technology [39]. We call this
dataset VulDeePecker dataset because it was released as a part of
their work that proposed a deep learning-based system for vulner-
ability detection called VulDeePecker [23]. The dataset contains
two common types of vulnerabilities collected from (i) syntactic
and academic security flaws and (ii) popular open-source projects,
including Linux kernel, Thunderbird, Wireshark, Apache HTTP
Server, Xen, OpenSSL, and VLC media player, mostly available on
the National Vulnerability Database (NVD) and the NIST Software
Assurance Reference Dataset (SARD). The vulnerabilities are the
following:
‚Ä¢ CWE-119 Buffer Error (BE): BE covers buffer vulnerabilities
caused by direct addressing of memory location without proper
validation of a referenced memory buffer. Refer to Figure 6 for
an example.

‚Ä¢ CWE-399 Resource Management Error (RME): RME includes
resource management vulnerabilities induced by improper han-
dling of resources, which may lead to a variety of errors such as
resource exhaustion, memory leakage, channel, and path excep-
tions. Refer to Figure 7 for an example.

A.2 SeVC Data
We consider the Semantics-based Vulnerability Candidate (SeVC)
dataset having 126 types of different vulnerabilities [22]. This dataset

12

Figure 10: Illustration of operations in self-attention; scaled dot-product
attention with four keys and four values associated with four tokens in an
input sequence (left figure) and its system block (right figure).

Figure 11: Multi-head attention mechanism.

Figure 8: BiLSTM architecture.

Figure 9: Transformer architecture.

data. Computations are carried out as follows:

Update gate: ùëñùë° = ùúé (ùëäùë¢ ¬∑ ùë•ùë° + ùëàùë¢ ¬∑ ùê∂ùë° ‚àí1 + ùëèùë¢ ),
Reset gate: ùëüùë° = ùúé (ùëäùëü ¬∑ ùë•ùë° + ùëàùëü ¬∑ ùê∂ùë° ‚àí1 + ùëèùëü ),
Current memory content: Àúùê∂ùë° = ùúô (ùëä‚Ñé ¬∑ ùë•ùë° + ùëà‚Ñé (ùëüùë° ‚äô ùê∂ùë° ‚àí1) + ùëè‚Ñé),
Final memory content: ùê∂ùë° = ùëñùë° ‚äô ùê∂ùë° ‚àí1 + (1 ‚àí ùëñùë° ) ‚äô Àúùê∂ùë° ,

where ùëè is the bias, ùëä and ùëà are the learning parameter (weight ma-
trix), ùúé is the Sigmoid function, ùúô is a hyperbolic tangent function,
and ‚äô is the Hadamard (element-wise) product.

B.3 Transformers
Transformers are deep neural network-based models based on an
attention mechanism that differentially weights the importance
and context of each token in the input sentence [35]. Refer to
Figure 9 for an illustration of transformer architecture. Its network
architecture has encoder and decoder blocks, and the core elements
in these blocks are positional encoding, multi-head attention, and
fully connected layers.

Positional encoding provides the relative or exact position of
the tokens (of data points) in the input sequence, and it is ap-
plied to the embeddings, which convert the input tokens (encoder
block) and output tokens (decoder block) to vectors of dimension
ùëëmodel. Sinusoidal functions can be used for the positional encod-
ings, for example, ùë†ùëñùëõ(ùëùùëúùë†/100002ùëñ/ùëëmodel ) for even positions, and
ùëêùëúùë† (ùëùùëúùë†/100002ùëñ/ùëëmodel ) for odd positions, where ùëñ and ùëùùëúùë† are the
dimension and position index, respectively. An attention mecha-
nism is applied to represent the input sequence by relating each
word/token to related words/tokens in the input sequence. This
is called self-attention. One of its examples is scaled dot-product
attention, whose calculation is illustrated in Figure 10. Firstly, each
output vector of the positional encoding is converted to three vec-
tors; a query ùëû, a key ùëò, and a value ùë£. The conversion is performed
by multiplying the output vector by matrices ùëä ùëÑ , ùëä ùêæ , and ùëä ùëâ ,
respectively. These matrices are continuously updated during train-
ing for better projections. Secondly, the dot products of query (a
vector ùëû) with all keys (all vectors ùëò) of the input sequence gener-
ates scores ùëÜùëñ , ùëñ ‚àà {1, 2, 3, . . . }. The scores are normalized, and then
softmax is calculated. This provides the weight of other parts of
the input sequence while encoding a word at a specific position.
Finally, each value vector is multiplied by their corresponding soft-
max scores and summed together to yield the attention value of
the token. The following equation represents all these operations
in matrix form:

Attention(ùëÑ, ùêæ, ùëâ ) = softmax(

ùëÑùêæùëá
‚àöÔ∏Åùëëùëò

)ùëâ ,

(1)

where ùëëùëò is the key vector‚Äôs dimension, and ùëÑ, ùêæ and ùëâ are matrices
packed with all (multi-head) queries, (multi-head) keys and (multi-
head) values, respectively.

The output of the attention function is further improved by col-
lectively attending to the information from different representation
subspaces. This is called multi-head attention, and performed as
follows: (i) The queries, values, and keys are linearly projected to
‚Ñé times, (ii) the attention function is calculated in each projection
in parallel, and (iii) all the outputs of the attention functions are
concatenated and projected through the linear layer as shown in
Figure 11. Its representation in matrix form is the following:
MultiHead(ùëÑ, ùêæ, ùëâ ) = Concat(head1, . . . , head‚Ñé)ùëä ùëÇ,
, ùêæùëä ùêæ
ùëñ

(2)
ùëñ ), and ùëä ùëÇ is a weight

where headùëñ = Attention(ùëÑùëä ùëÑ
ùëñ
matrix.

, ùëâùëä ùëâ

In transformer architecture, unlike in encoder block, the decoder
block has two layers of multi-head attention. The first one is masked

13

ùëéùëö=expùëÜùëöœÉùëó‡µØexp(ùëÜùëó234233****+++234ùëéùë°ùë°ùëíùëõùë°ùëñùëúùëõùë£ùëéùëôùë¢ùëí=‡∑çùëéùëöùë£ùëöùëò1ùëò2ùëò3ùëò4For eachùëñ‚àà1,2,3,4;ùëÜùëó=ùëòùëó‚ãÖùëûùëñ,ùëó‚àà1,2,3,4MatMulScaleSoftMaxQKVMatMulConcatenationLinearMulti-headAttentionScaled Dot productattention‚ÑéheadsLinearQLinearKLinearVRightshiftedoutputsInput EmbeddingMulti-head attentionInputsAdd & NormFeedforwardPositionalEncodingQKvAdd & NormùëÅ√óEncoderOutput EmbeddingMaskedMulti-head attentionAdd & NormPositionalEncodingQKvùëÅ√óDecoderMulti-head attentionFeedforwardAdd & NormAdd & NormKvQLinearSoftmaxOutputmulti-head attention that enables target sequences, i.e., its right-
shifted previous outputs, paying attention to itself, and ignoring
the future decoder‚Äôs input. Next is the multi-head attention that
enables the target sequence to pay attention to the input sequences.
Consequently, the attention scores of each target sequence, consid-
ering the attention scores of the input sequence (coming from the
encoder block), are generated, and these are transformed from their
embedding space to probability space by the final fully connected
layer and softmax layer.

Now we present various transformer-based models that are con-

sidered in this paper in the following paragraphs.

Bidirectional Encoder Representations from Transformers
Bidirectional Encoder Representations from Transformers (BERT)
is a transformer [35] whose architecture is solely based only on
its encoder blocks [5]. It scans the entire surrounding context of
its input all at once, processes it through encoder blocks, and col-
lectively fuses the left and right context in all of its layers to learn
syntactic information and acquires some semantic and world knowl-
edge [29]. It is a state-of-art language model. In this project, we
have utilised two BERT models pre-trained on lower-cased Eng-
lish datasets such as Wikipedia (2500M words) and BooksCorpus
dataset (800M words) [43]. The BERT Base model has 110M param-
eters with 12 layers, 768 hidden size, and 12 attention heads. The
BERT Large model has 340M parameters with 24 layers, 1024 hidden
size, 16 attention heads.

DistilBERT DistilBERT [31] is a smaller, faster, cheaper, and lighter
version of BERT while retaining the close performance (e.g., 95%)
of the original BERT‚Äôs performance. The size of DistilBERT is 40%
smaller than BERT, and it is obtained by leveraging the knowledge
distillation technique in the pre-training phase. Knowledge distil-
lation is a compression technique where a smaller model, called
student, is trained such that it reproduces the learning of the larger
model, called teacher. DistillBERT has 66M parameters with 6 layers,
768 hidden size, and 12 attention heads.

RoBERTa Robustly optimized BERT approach (RoBERTa) [24]
updates the key hyper-parameters during BERT‚Äôs pre-training to-
wards better optimization, performance, and robustness across NLP
tasks. The updates include (1) longer model training time with
more data and large mini-batch, (2) training on a longer sequence,
(3) removing the next sentence prediction (an objective method
applied for the BERT‚Äôs pre-training to predict if the two segments
are following each other or belong to different documents to create
a semantic inference), and (4) dynamic masking to the training
data. In this paper, we consider the RoBERTa with the BERT Base
architecture, and it has 12 layers, 768 hidden size, and 12 attention
heads.

CodeBERT CodeBERT [8] is a bimodal pre-trained transformer
model for both programming and natural languages. It is trained
on bimodal data (code & documents) [16], with codes from Python,
Java, JavaScript, Ruby, Go, and PHP. Its architecture follows BERT [5]
and RoBERTa [24]. During pre-training, its architecture has two gen-
erators, namely NL-generator and Code-generator, and one NL-code
discriminator. NL-generator and Code-generator generate plausi-
ble tokens for masked positions based on surrounding contexts

for natural language input and code input, respectively. NL-Code
discriminator is trained on the tokens sampled from NL-generator
and Code-generator. Its architecture is the same as RoBERTa, and it
is the targeted model used for the fine-tuning purpose. Moreover, it
has 125M parameters with 12 layers, 768 hidden size, and 12 attention
heads.

Generative Pre-trained Transformer The architecture of Gen-
erative Pre-trained Transformer (GPT) is based on decoder blocks
of transformer and masked self-attention [27]. GPT learns long-
range dependencies between words and sentence and world knowl-
edge through the generative pre-training, which is unsupervised
learning, and this learning are transferred to specific task through
fine-tuning, which is supervised learning. In contrast to BERT, GPT
outputs one token at a time, and that is added to its input sequence
for the next round. Consequently, each token in the input sentence
has a context of the previous words only. Thus, GPT is called an
auto-regressive model. GPT outperforms available models that are
based on recursive neural networks, convolutional neural networks,
and LSTMs [27]. Moreover, the GPT model is a powerful predictor
of the next token in a sequence, so it is popular in text generation
tasks. GPT models have been improved by increasing their model
parameters and rigorous pre-training on a large corpus of English
text datasets, called WebText [28]. More precisely, improved GPT
models consider task conditioning that enables the model to pro-
duce task-specific outputs for the same input. In this paper, we
consider GPT-2 models of various sizes: (1) GPT-2 Base, which has
117M parameters with 12 layers, 768 hidden size, and 12 attention
heads, (2) GPT-2 Large, which has 774M parameters with 36 layers,
1280 hidden size, and 20 attention heads, (3) GPT-2 XL, which has
1.5B parameters with 48 layers, 1600 hidden size, and 25 attention
heads, and (4) GPT-J, which has 6B parameters with 28 layers, 4096
hidden size, and 16 attention heads, pre-trained on the dataset called
Pile having a diverse text data [10].

Megatron-LM Megatron-LMs are transformer-based models de-
veloped by NVIDIA. They are generated by enabling intra-layer
model-parallelism on the architecture level of the existing language
models, such as BERT and GPT-2 [32]. The model parallelism in-
cludes two-dimensional model parallelism: tensor model parallelism
and pipeline model parallelism. The tensor model parallelism splits
a single transformer across multiple GPUs, while the pipeline model
parallelism splits transformer layers across multiple GPUs. Thus,
these models efficiently utilize multiple GPU environments to train
large models that can not be fitted inside one GPU. The resulting
models do not require changes in compilers and libraries, and they
enable an efficient way to scale up their model parameters to billions
(e.g., 8.3B) for an increased performance [32]. Moreover, Megatron‚Äôs
BERT version has rearranged normalization layers and residual con-
nections to allow the direct flow of gradients through the network
without going through the normalization layers. This enables in-
creasing in its performance with the increase in the model‚Äôs size.
In this paper, we consider Megatron versions of BERT and GPT-2
models provided by Nvidia; (1) MegatronBERT having 345M param-
eters with 24 layers, 1024 hidden size, and 16 attention heads, and
(2) Megatron-GPT2 having 345M parameters with 24 layers, 1024
hidden size, and 16 attention heads. These models are pre-trained on

14

False Negative Rate: False Negative Rate (FNR) specifies the pro-
portion of the positive class (i.e., vulnerable code gadgets) misclas-
sified as a negative class (i.e., non-vulnerable code gadgets) and
calculated as

FNR =

FN
FN + TP

.

Precision: Precision specifies the classifier‚Äôs resistance to misclas-
sifying negative class samples to positive class and calculated as

Precision =

TP
TP + FP

.

Recall: Recall specifies the classifier‚Äôs ability to correctly classify a
positive class, and is calculated as

Recall =

TP
TP + FN

.

F1-score: F1-score considers FP and FN together, and it is a har-
monic mean of Precision and Recall. It is calculated as

F1-score = 2 √ó

Precision √ó Recall
Precision + Recall

.

D THREE FOLDS RESULTS
The 3-fold individual results for the VulDeePecker dataset are pre-
sented in Table 14, Table 15, Table 16, and Table 17.

text data sourced from Wikipedia, RealNews, OpenWebText, and
CC-Stories [32].

C PERFORMANCE METRICS
Before defining the evaluation metrics presented in this paper, we
define the following terms, where the positive class samples refer
to vulnerable code gadgets and the negative class samples refer to
non-vulnerable code gadgets.

‚Ä¢ True Positive (TP) is the number of the positive class samples

that are correctly classified.

‚Ä¢ False Positive (FP) is the number of the negative class samples

that are misclassified as the positive class.

‚Ä¢ True Negative (TN) is the number of the negative class sam-

ples that are correctly classified.

‚Ä¢ False Negative (FN) is the number of the positive class sam-

ples that are misclassified as the negative class.

False Positive Rate: False Positive Rate (FPR) specifies the pro-
portion of the negative class (i.e., non-vulnerable code gadgets)
misclassified as a positive class (i.e., vulnerable code gadgets) and
calculated as

FPR =

FP
FP + TN

.

15

Table 14: Test performance of various models for the binary classification on clean VulDeePecker dataset.

Dataset and
Vulnerability

Group 1,
Buffer Error
(BE)

Group 2,
Resource
Management
Error
(RME)

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score

VulDeePecker
Original
2.90%
18.00%
82.00%
91.70%
86.60%
2.80%
4.70%
95.30%
94.60%
95.00%

BiLSTM
Fold 2

BiGRU
Fold 2

BERT Base
Fold 2
3.16%
5.71%

GPT-2 Base
Fold 2
3.82%
6.98%

Fold 1

Fold 3

Fold 1
4.80%
6.90%

Fold 3
Fold 1
Fold 3
3.87%
33.25% 36.36% 31.98% 18.41% 11.64% 15.52%
17.61% 10.85% 17.37% 31.54% 39.34% 35.58%
7.34%
71.25% 71.03% 72.10% 69.70% 77.35% 72.07% 92.31% 95.13% 93.24% 93.40% 94.09% 92.56% 95.34% 96.22% 94.26% 93.97% 93.90% 93.70%
82.39% 89.15% 82.63% 68.46% 60.66% 64.42% 93.10% 94.29% 93.05% 93.89% 93.02% 93.77% 95.53% 95.30% 94.63% 93.23% 93.85% 92.66%
76.42% 79.07% 77.01% 69.08% 67.99% 68.03% 92.71% 94.71% 93.15% 93.64% 93.55% 93.16% 95.44% 95.76% 94.44% 93.60% 93.87% 93.18%
3.36%
16.79% 18.18% 13.33%
11.94%
8.15%
9.46%
83.99% 83.28% 86.23% 91.95% 93.32% 89.47% 94.18% 92.60% 94.58% 92.04% 93.17% 93.72% 93.88% 93.69% 95.18% 91.09% 90.79% 93.58%
88.06% 90.54% 83.52% 89.55% 88.13% 91.30% 93.66% 95.18% 93.70% 94.96% 96.10% 93.89% 94.40% 96.47% 95.00% 93.47% 93.32% 91.85%
85.97% 86.76% 84.85% 90.74% 90.65% 90.38% 93.92% 93.87% 94.14% 93.48% 94.61% 93.80% 94.14% 95.06% 95.09% 92.27% 92.04% 92.71%

4.13%
3.35%
16.48% 10.45% 11.87%

Fold 3
4.20%
6.95%

Fold 1
4.11%
6.11%

Fold 1
2.89%
4.47%

Fold 1
3.70%
6.77%

Fold 3
3.58%
5.37%

Fold 3
4.68%
6.23%

5.03%
6.68%

5.73%
8.70%

3.05%
6.34%

2.86%
6.30%

3.75%
3.90%

4.04%
4.82%

4.82%
6.53%

4.33%
5.04%

3.24%
5.60%

3.36%
6.11%

3.45%
3.53%

2.57%
5.00%

DistilBERT
Fold 2
3.99%
6.15%

CodeBERT
Fold 2
2.45%
4.70%

Table 15: Test performance of various models for the binary classification on clean VulDeePecker dataset.

Dataset and
Vulnerability

Group 1,
Buffer Error
(BE)

Group 2,
Resource
Management
Error
(RME)

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score

RoBERTa
Fold 2
4.41%
8.25%

GPT-2 XL
Fold 2
2.33%
4.51%

GPT-2 Large
Fold 2
2.12%
4.63%

Fold 3
5.05%
5.77%

Fold 1
2.72%
4.66%

Fold 1
2.76%
4.99%

Fold 3
3.18%
4.85%

Fold 3
Fold 1
2.08%
3.98%
5.65%
8.32%
93.61% 93.17% 92.06% 95.59% 96.72% 94.90% 95.51% 96.41% 95.32% 94.52% 95.66% 94.34% 95.18% 96.02% 95.27% 94.22% 96.13% 96.48%
94.35% 91.75% 94.23% 95.34% 95.37% 95.15% 95.01% 95.49% 94.69% 95.14% 95.18% 93.97% 94.61% 94.92% 93.64% 96.39% 94.67% 91.68%
93.98% 92.46% 93.13% 95.46% 96.04% 95.03% 95.26% 95.95% 95.00% 94.83% 95.42% 94.16% 94.89% 95.47% 94.45% 95.29% 95.40% 94.02%
3.26%
3.34%
5.60%
2.96%
93.70% 94.15% 95.68% 95.93% 97.76% 96.67% 95.95% 97.03% 97.01% 94.21% 95.74% 96.65% 94.92% 96.30% 94.91% 95.91% 97.89% 94.08%
94.40% 95.55% 94.44% 96.64% 97.22% 96.85% 97.20% 96.85% 96.11% 97.20% 95.92% 96.30% 97.57% 96.66% 96.67% 96.27% 94.81% 97.04%
94.05% 94.84% 95.06% 96.28% 97.49% 96.76% 96.57% 96.94% 96.56% 95.68% 95.83% 96.47% 96.23% 96.48% 95.78% 96.09% 96.32% 95.53%

Fold 1
3.41%
4.86%

Fold 1
2.97%
5.39%

Fold 1
3.66%
3.61%

Fold 3
2.89%
5.31%

Fold 3
3.50%
6.03%

Fold 3
2.89%
6.36%

3.16%
4.45%

2.16%
3.36%

1.18%
2.78%

2.27%
5.56%

1.78%
3.70%

2.75%
2.43%

1.97%
3.34%

2.76%
3.33%

1.08%
5.19%

2.16%
3.73%

1.58%
3.15%

1.58%
3.89%

2.16%
2.80%

1.78%
3.15%

2.27%
4.08%

3.15%
2.80%

MegatronBERT
Fold 2
2.83%
4.82%

MegatronGPT2
Fold 2
2.58%
5.08%

GPT-J
Fold 2
2.49%
5.33%

Table 16: Test performance of various models for the multi-class classification on clean VulDeePecker dataset.

Dataset and
Vulnerability

Group 3,
Buffer Error
(BE)

Group 3,
Resource
Management
Error
(RME)
Group 3,
BE + RME
(Global Avg.)
Group 3,
BE + RME
(Macro Avg.)

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score
Precision
Recall
F1-score
Precision
Recall
F1-score

VulDeePecker
Original
2.90%
18.00%
82.00%
91.70%
86.60%
2.80%
4.70%
95.30%
94.60%
95.00%
N/A
N/A
N/A
88.65%
93.15%
90.80%

BiLSTM
Fold 2

BiGRU
Fold 2
4.97%

BERTBase
Fold 2
2.59%
4.83%

GPT-2 Base
Fold 2
3.30%
5.65%

CodeBERT
Fold 2
2.08%
5.02%

Fold 3

3.84%
7.82%

Fold1
2.23%
5.20%

Fold1
1.65%
5.46%

Fold1
2.58%
5.39%

Fold 3
2.98%
5.18%

Fold 3
2.30%
4.53%

Fold 3
Fold 3
Fold1
Fold1
2.58%
21.19% 20.80% 21.89% 11.96%
4.16%
12.89% 15.49% 13.45% 26.18% 43.56% 46.19%
5.18%
60.99% 61.88% 60.15% 70.13% 81.94% 83.16% 94.18% 93.63% 94.05% 93.32% 91.96% 92.39% 95.61% 94.80% 94.82% 94.65% 93.36% 93.35%
87.11% 84.51% 86.55% 73.82% 56.44% 53.81% 94.80% 95.17% 95.47% 94.61% 94.35% 94.82% 94.54% 94.98% 96.00% 93.03% 94.67% 94.82%
71.74% 71.44% 70.97% 71.92% 66.84% 65.34% 94.49% 94.40% 94.76% 93.96% 93.14% 93.59% 95.07% 94.89% 95.40% 93.83% 94.01% 94.08%
0.79%
3.20%
3.18%
11.65% 12.59%
3.37%
75.04% 75.20% 73.20% 92.74% 95.45% 93.80% 93.00% 94.90% 94.46% 90.96% 91.52% 91.12% 94.36% 94.22% 94.43% 93.49% 93.82% 93.31%
88.35% 87.41% 92.18% 94.45% 91.79% 91.30% 95.75% 95.07% 96.98% 94.82% 94.53% 96.63% 95.93% 95.26% 96.45% 95.56% 94.16% 96.63%
81.15% 80.84% 81.60% 93.59% 93.58% 92.53% 94.35% 94.99% 95.71% 92.85% 93.00% 93.79% 95.14% 94.74% 95.43% 94.52% 93.99% 94.94%
64.17% 64.92% 63.34% 75.92% 86.35% 86.96% 93.87% 93.95% 94.16% 92.68% 91.84% 92.04% 95.28% 94.65% 94.71% 94.33% 93.48% 93.34%
87.43% 85.26% 88.07% 79.23% 65.57% 63.92% 95.05% 95.15% 95.88% 94.66% 94.39% 95.30% 94.91% 95.05% 96.12% 93.69% 94.54% 95.30%
74.02% 73.71% 73.68% 77.54% 74.54% 73.68% 94.46% 94.55% 95.01% 93.66% 93.10% 93.64% 95.09% 94.85% 95.41% 94.01% 94.00% 94.31%
68.01% 68.54% 66.67% 81.43% 88.69% 88.48% 93.59% 94.26% 94.26% 92.14% 91.74% 91.76% 94.99% 94.51% 94.63% 94.07% 93.59% 93.33%
87.73% 85.96% 89.37% 84.14% 74.12% 72.55% 95.28% 95.12% 96.23% 94.71% 94.44% 95.72% 95.24% 95.12% 96.22% 94.30% 94.41% 95.72%
76.62% 76.27% 76.37% 82.76% 80.75% 79.73% 94.43% 94.69% 95.23% 93.41% 93.07% 93.70% 95.11% 94.82% 95.42% 94.18% 94.00% 94.51%

Fold 3
2.00%
4.00%

Fold1
2.00%
6.97%

0.68%
5.84%

0.69%
8.70%

0.97%
5.47%

0.72%
4.44%

0.78%
4.25%

0.62%
4.07%

0.80%
5.55%

0.48%
8.21%

0.65%
3.02%

0.56%
4.93%

1.02%
5.18%

0.65%
3.55%

1.07%
3.37%

0.64%
4.74%

DistilBERT
Fold 2
2.69%
5.33%

Table 17: Test performance of various models for the multi-class classification on clean VulDeePecker dataset.

Dataset and
Vulnerability

Group 3,
Buffer Error
(BE)

Group 3,
Resource
Management
Error
(RME)
Group 3,
BE + RME
(Global Avg.)
Group 3,
BE + RME
(Macro Avg.)

Metrics

FPR
FNR
Precision
Recall
F1-score
FPR
FNR
Precision
Recall
F1-score
Precision
Recall
F1-score
Precision
Recall
F1-score

RoBERTa
Fold 2
2.54%
6.54%

GPT-2 XL
Fold 2
1.50%
4.57%

GPT-2 Large
Fold 2
1.78%
4.38%

MegatronBERT
Fold 2
1.78%
4.76%

MegatronGPT-2
Fold 2
2.31%
5.52%

0.62%
4.01%

0.75%
3.20%

Fold 3
1.80%
3.87%

Fold 1
1.80%
5.33%

Fold 1
1.50%
4.80%

Fold 3
1.40%
4.92%

Fold 1
1.90%
5.20%

Fold 1
1.50%
5.13%

Fold 3
2.53%
4.86%

Fold 3
1.53%
5.38%

Fold 3
Fold 1
1.25%
2.15%
5.66%
5.84%
94.34% 93.64% 93.49% 96.01% 95.56% 95.94% 96.02% 96.22% 96.28% 94.99% 95.54% 95.32% 95.23% 94.24% 94.86% 95.76% 96.27% 96.63%
94.34% 93.46% 95.14% 94.87% 95.62% 94.62% 95.20% 95.43% 95.08% 94.80% 95.24% 96.13% 94.67% 94.48% 95.60% 95.00% 95.17% 94.16%
94.34% 93.55% 94.31% 95.43% 95.59% 95.28% 95.61% 95.82% 95.68% 94.90% 95.39% 95.72% 94.95% 94.36% 95.23% 95.38% 95.72% 95.38%
0.30%
0.88%
4.99%
5.15%
92.11% 94.43% 93.64% 96.30% 96.74% 95.65% 95.95% 96.54% 96.32% 95.75% 96.16% 94.65% 94.77% 95.81% 94.46% 97.90% 97.88% 97.27%
95.01% 95.99% 96.80% 96.30% 97.45% 97.69% 96.30% 96.72% 97.69% 95.75% 95.99% 97.34% 97.23% 95.99% 96.98% 94.82% 92.70% 94.85%
93.54% 95.20% 95.20% 96.30% 97.09% 96.66% 96.13% 96.63% 97.00% 95.75% 96.07% 95.97% 95.99% 95.90% 95.71% 96.34% 95.22% 96.04%
93.74% 93.85% 93.53% 96.08% 95.86% 95.86% 96.00% 96.31% 96.29% 95.19% 95.70% 95.13% 95.11% 94.64% 94.75% 96.31% 96.68% 96.80%
94.52% 94.11% 95.59% 95.25% 96.09% 95.45% 95.49% 95.76% 95.78% 95.05% 95.43% 96.45% 95.34% 94.87% 95.98% 94.95% 94.54% 94.35%
94.13% 93.98% 94.55% 95.66% 95.98% 95.65% 95.74% 96.03% 96.04% 95.12% 95.57% 95.79% 95.23% 94.75% 95.36% 95.63% 95.59% 95.56%
93.23% 94.04% 93.57% 96.15% 96.15% 95.80% 95.98% 96.38% 96.30% 95.37% 95.85% 94.98% 95.00% 95.02% 94.66% 96.83% 97.08% 96.95%
94.68% 94.72% 95.97% 95.59% 96.53% 96.16% 95.75% 96.07% 96.38% 95.28% 95.61% 96.73% 95.95% 95.23% 96.29% 94.91% 93.94% 94.50%
93.95% 94.38% 94.75% 95.87% 96.34% 95.98% 95.87% 96.23% 96.34% 95.32% 95.73% 95.85% 95.47% 95.13% 95.47% 95.86% 95.48% 95.71%

Fold 1
1.60%
5.00%

Fold 3
1.98%
4.40%

0.40%
3.70%

0.36%
2.55%

0.22%
7.30%

0.44%
3.70%

0.58%
2.77%

0.65%
3.02%

0.42%
2.31%

0.50%
2.31%

0.63%
2.66%

0.38%
3.28%

0.46%
4.25%

0.42%
4.01%

0.46%
4.01%

0.22%
5.18%

GPT-J
Fold 2
1.47%
4.83%

16

