PPL Bench: Evaluation Framework For Probabilistic
Programming Languages

0
2
0
2

t
c
O
7
1

]
L
P
.
s
c
[

1
v
6
8
8
8
0
.
0
1
0
2
:
v
i
X
r
a

SOURABH KULKARNI, KINJAL DIVESH SHAH, NIMAR ARORA, XIAOYAN WANG, YU-
CEN LILY LI, NAZANIN KHOSRAVANI TEHRANI, MICHAEL TINGLEY, DAVID NOURSI,
NARJES TORABI, SEPEHR AKHAVAN MASOULEH, ERIC LIPPERT, and ERIK MEIJER, Face-
book Inc, USA (@fb.com)

We introduce PPL Bench, a new benchmark for evaluating Probabilistic Programming Languages (PPLs) on a
variety of statistical models. The benchmark includes data generation and evaluation code for a number of
models as well as implementations in some common PPLs. All of the benchmark code and PPL implementations
are available on Github. We welcome contributions of new models and PPLs and as well as improvements in
existing PPL implementations. The purpose of the benchmark is two-fold. First, we want researchers as well as
conference reviewers to be able to evaluate improvements in PPLs in a standardized setting. Second, we want
end users to be able to pick the PPL that is most suited for their modeling application. In particular, we are
interested in evaluating the accuracy and speed of convergence of the inferred posterior. Each PPL only needs
to provide posterior samples given a model and observation data. The framework automatically computes and
plots growth in predictive log-likelihood on held out data in addition to reporting other common metrics such
as effective sample size and ˆ𝑟 .

1 INTRODUCTION
Probabilistic Programming Languages [Ghahramani 2015] allow statisticians to write probability
models in a formal language. These languages usually include a builtin inference algorithm that
allows practitioners to rapidly prototype and deploy new models. More formally, given a model
𝑃 (𝑋, 𝑍 ) defined over random variables 𝑋 and 𝑍 and given data 𝑋 = 𝑥, the inference problem is
to compute 𝑃 (𝑍 |𝑋 = 𝑥). Another variant of this inference problem is to compute the expected
value of some functional 𝑓 over this posterior, E[𝑓 (𝑍 )|𝑋 = 𝑥]. Most languages provide some
version of Markov Chain Monte Carlo [Brooks et al. 2011] inference. MCMC is a very flexible
inference technique that can conveniently represent high dimensional posterior distributions with
samples 𝑍1, . . . , 𝑍𝑚 that collectively approach the posterior asymptotically with 𝑚. Other inference
techniques that are often builtin include Variational Inference [Wainwright et al. 2008], which is
usually asymptotically inexact, and Importance Sampling [Glynn and Iglehart 1989], which is only
applicable to computing an expectation.

In roughly the last two decades there has been an explosive growth in the number of PPLs.
Starting with BUGS [Spiegelhalter et al. 1996], iBAL [Pfeffer 2001], BLOG [Milch et al. 2005], and
Church [Goodman et al. 2008] in the first decade, and followed by Stan [Carpenter et al. 2017],
Venture [Mansinghka et al. 2014], Gen [Cusumano-Towner and Mansinghka 2018], Turing [Ge
et al. 2018], to name some, in the next decade. Some of these PPLs restrict the range of models they
can handle whereas others are universal languages, i.e, they support any computable probability
distribution. There is a tradeoff between being a universal language and performing fast inference
because if a language only supports select probability distributions, its inference methods can
optimize for that. Different PPLs can be better suited for different use-cases and hence having
a benchmarking framework can prove to be extremely useful. We introduce PPL Bench as a
benchmarking framework for evaluating different PPLs on models. PPL Bench has already been

Authors’ address: Sourabh Kulkarni, skulkarni@umass.edu; Kinjal Divesh Shah, kshah97; Nimar Arora, nimarora; Xiaoyan
Wang, xiaoyan0; Yucen Lily Li, yucenli; Nazanin Khosravani Tehrani, nazaninkt; Michael Tingley, tingley; David Noursi,
dcalifornia; Narjes Torabi, ntorabi; Sepehr Akhavan Masouleh, sepehrakhavan; Eric Lippert, ericlippert; Erik Meijer,
erikm, Facebook Inc, Probability, 1 Hacker Way, Menlo Park, CA, 94025, USA (@fb.com).

 
 
 
 
 
 
Sourabh Kulkarni, Kinjal Divesh Shah, Nimar Arora, Xiaoyan Wang, Yucen Lily Li, Nazanin Khosravani Tehrani, Michael
Tingley, David Noursi, Narjes Torabi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer
2

Fig. 1. PPL Bench system overview

used to benchmark a new inference method, Newtonian Monte Carlo [Arora et al. 2019]. In the
rest of this paper, we provide an overview of PPL Bench1 and describe each of the initial statistical
models that are included before concluding.

2 SYSTEM OVERVIEW
PPL Bench is implemented in Python 3 [Team 2019]. It is designed to be modular, which makes it
very easy to add models and PPL implementations to the framework. The typical PPL Bench flow
is as follows:

• Model Instantiation and Data Generation: All the models in PPL Bench have various
parameters that need to be specified. A model with all it’s parameters set to certain values is
referred to as a model instance. We establish a model 𝑃𝜃 (𝑋, 𝑍 ) with ground-truth parameter
distributions(𝜃 ∼ 𝑃𝜙 ) where 𝜙 is the hyperprior (a prior distribution from which parameters
are sampled). In some models theta is sampled from 𝜙 while in others it can be specified
by users. PPL Bench is designed to accept parameters at both the PPL level, as well as the
model level. Therefore, any parameters such as number of samples, number of warmup
samples, inference method and so on can be configured with ease. Similarly, any model-
specific hyperparameters can also be passed as input through a JSON file. We can sample
parameter values form their distributions, which will instantiate the model:

We then simulate train and test data as follows:

𝑍1 ∼ 𝑃𝜃 (𝑍 )

𝑋𝑡𝑟𝑎𝑖𝑛

𝑋𝑡𝑒𝑠𝑡

𝑖𝑖𝑑
∼ 𝑃𝜃 (𝑋 |𝑍 = 𝑍1)
𝑖𝑖𝑑
∼ 𝑃𝜃 (𝑋 |𝑍 = 𝑍1)

Note that this process of data generation is performed independent of any PPL.

• PPL Implementation and Posterior Sampling: The training data is passed to various PPL
implementations which learn a model 𝑃𝜃 (𝑋 = 𝑋1, 𝑍 ); The output of this learning process
is obtained in the form of 𝑛 sets of posterior samples of parameters (one for each sampling
step).

𝑍 ∗
1...𝑛 ∼ 𝑃𝜃 (𝑍 |𝑋 = 𝑋𝑡𝑟𝑎𝑖𝑛)

1Open Source on Github: https://github.com/facebookresearch/pplbench

PPL Bench: Evaluation Framework For Probabilistic Programming Languages

3

The sampling is restricted to a single chain and any form of multi-threading is disabled.
However, we can run each benchmark multiple times, and we treat each trial as a chain. This
allows us to calculate metrics such as 𝑟ℎ𝑎𝑡 .

• Evaluation: Using the test data 𝑋𝑡𝑒𝑠𝑡 , posterior samples 𝑍 ∗

1...𝑛 and timing information, the

PPL implementations are evaluated on the following evaluation metrics:
Plot of the Predictive Log Likelihood w.r.t samples for each implemented PPL: PPL
Bench automatically generates a plot of predictive log likelihood against samples. This
provides a visual representation of how fast different PPLs converge to their final predictive
log likelihood value. This metric has been previously used by [Kucukelbir et al. 2017] to
compare convergence of different implementations of statistical models.
(cid:33)

(cid:32)

Predictive Log Likelihood(𝑛) = log

(𝑃 (𝑋𝑡𝑒𝑠𝑡 |𝑍 = 𝑍 ∗

𝑖 ))

1
𝑛

𝑛
∑︁

𝑖=1

After computing the predictive log likelihood w.r.t samples over each trial run, the min,
max and mean values over samples is obtained. These are plotted on a graph for each PPL
implementation so their convergence behavior can be visualized. This plot is designed to
capture both the relative performance and accuracy of these implementations.
Gelman-Rubin convergence statistic 𝑟ℎ𝑎𝑡 : We report 𝑟ℎ𝑎𝑡 for each PPL as a measure of
convergence by using samples generated across trials. We report this quantity for all queried
variables.
Effective sample size 𝑛𝑒 𝑓 𝑓 : The samples generated after convergence should ideally be
independent of each other. In reality, there is non-zero correlation between generated samples;
any positive correlation reduces the number of effective samples generated. Samples are
combined across trials and the 𝑛𝑒 𝑓 𝑓 as well as 𝑛𝑒 𝑓 𝑓 /𝑠 of each queried variable in each PPL
implementation is computed.
Inference time: Runtime is an important consideration for any practical use of probabilistic
models. The time taken to perform inference in each trial of a PPL is recorded.

The generated posterior samples and the benchmark configurations such as number of samples,
ground-truth model parameters etc. are stored along with the plot and evaluation metrics listed
above for each run. PPL Bench also allows for model-specific evaluation metrics to be added. Using
the PPL Bench framework, we establish four models to evaluate performance of PPLs:

(1) Bayesian Logistic Regression model[van Erp and Gelder 2013]
(2) Robust Regression model with Student-T errors[Gelman et al. 2013]
(3) Latent Keyphrase Index(LAKI) model, a Noisy-Or Topic Model[Liu et al. 2016]
(4) Crowdsourced Annotation model, estimating true label of an item given a set of labels

assigned by imperfect labelers[Passonneau and Carpenter 2014]

These models are chosen to cover a wide area of use cases for probabilistic modeling. Each of these
models is implemented in all or a subset of these PPLs: PyMC3 [Salvatier et al. 2016], JAGS [Plummer
et al. 2003], Stan [Carpenter et al. 2017] and BeanMachine [Tehrani et al. 2020]. The model and PPL
implementation portfolio is expected to be expanded with additional open-source contributions.
Subsequent sections describe these models and implementations in detail along with analysis of
observed results.

3 BAYESIAN LOGISTIC REGRESSION MODEL
Logistic Regression is the one of the simplest models we can benchmark PPLs on. Because its
posterior is log-concave, it is easy for PPLs to converge to the true posterior. For model definition,
see Appendix 8.1. Here, 𝑁 refers to number of data points and 𝐾 refers to the number of covariates.

Sourabh Kulkarni, Kinjal Divesh Shah, Nimar Arora, Xiaoyan Wang, Yucen Lily Li, Nazanin Khosravani Tehrani, Michael
Tingley, David Noursi, Narjes Torabi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer
4

PPL

N

Time(s)

𝑛𝑒 𝑓 𝑓 /s
min median max

PPL

N

Time(s)

BeanMachine
Stan
Jags
PyMC3
BeanMachine
Stan

298.64
20K
47.06
20K
491.22
20K
20K
48.37
200K 1167.13
200K 10119.02

21.74
21.62
0.17
23.65
0.003
0.06

232.85
25.78
0.18
27.10
0.004
0.06

416.98
108.63
4.04
67.27
0.01
0.34

BeanMachine
Stan
Jags
PyMC3
BeanMachine
Stan

146.79
20K
35.21
20K
742.40
20K
45.78
20K
200K 250.41
200K 140.58

𝑛𝑒 𝑓 𝑓 /s
min median max

3.11
87.71
0.59
92.50
1.87
0.77

8.48
114.75
1.40
99.14
4.00
1.52

41.85
361.16
2.87
137.45
10.33
2.40

Table 1. Runtime and 𝑛𝑒 𝑓 𝑓 /s for Bayesian Logis-
tic Regression.

Table 2. Runtime and 𝑛𝑒 𝑓 𝑓 /s for Robust Regres-
sion

Figure 2 shows the predictive log likelihood for 𝑁 = 20𝑘 and 𝑁 = 200𝑘. We use half of the data for
inference and the other half for evaluation. We run 1000 warm-up iterations for each of the PPLs
and plot the samples from the next 1000 iterations. We can see from Figure 2 that all the predictive
log likelihood (PLL) of different PPLs converge to around the same value. Thus, in addition to
convergence, the plot could also be used to check the accuracy of the model.

(a) N = 20K, K = 10

(b) N = 200K, K = 10

Fig. 2. Predictive log likelihood against samples for Bayesian Logistic Regression

4 ROBUST REGRESSION MODEL
Bayesian logistic regression with Gaussian errors, like ordinary least-square regression, is quite
sensitive to outliers [Rousseeuw and Leroy 2005]. To increase outlier robustness, a Bayesian
regression model with Student-T errors is more effective [Gelman et al. 2013]. For model definition,
see Appendix 8.2.

In Figure 3b, we include warmup samples in the plot. By doing so, we can also gain insights
into how quickly an algorithm adapts. We can see that Bean Machine adapts quicker than Stan.
Moreover, depending on the use-case, some metrics might be more informative to the end-users
than others. For example, we can see from Table 2 that Stan is faster than BeanMachine, but
BeanMachine generates more independent samples per time as 𝑛𝑒 𝑓 𝑓 /s is higher.

5 NOISY-OR TOPIC MODEL
Inferring topics from the words in a document is a crucial task in many natural language processing
tasks. The noisy-or topic model [Liu et al. 2016] is one such example. It is a Bayesian network
consisting of two types of nodes; one type are the domain key-phrases (topics) that are latent while
others are the content units (words) which are observed. Words have only topics as their parents.

PPL Bench: Evaluation Framework For Probabilistic Programming Languages

5

(a) N = 20K, K = 10

(b) N = 200K, K = 10 (warm up samples included)

Fig. 3. Predictive log likelihood against samples for Robust Regression

To capture hierarchical nature of topics, they can have other topics as parents. For model definition,
see Appendix 8.3.

For the experiment, we first sample the model structure. This consists of determining the children
of each node in the graph, and the corresponding weight associated with the edge. Next, the key-
phrase nodes are sampled once; and two instances of content units are sampled keeping the
key-phrase nodes fixed. One instance is passed to PPL implementation for inference, while the
other is used to compute posterior predictive of the obtained samples.

5.1 Stan Implementation
The model requires support for discrete latent variables, which Stan does not have. As a workaround,
we did a reparameterization; the discrete latent variables with values "True" and "False" which denote
whether a node is activated or not are instead represented as 1-hot encoded vectors[Maddison et al.
2016]. Each element of the encoded vector is now a real number substituting a boolean variable.
"True" is represented by [1, 0] and "False" by [0, 1] in the 1-hot encoding; a relaxed representation of
a true value, for example, might look like [0.9999, 0.0001]. The detailed reparameterization process
is as follows: We encode the probabilities of a node in a parameter vector 𝛼 = [𝛼true, 𝛼false], then
choose a temperature parameter 𝜏 = 0.1. For each key-phrase, we assign an intermediate random
variable 𝑋 𝑗 :

PPL Words Topics Time(s)

Stan
Jags
PyMC3
Stan
Jags
PyMC3

300
300
300
3000
3000
3000

30
30
30
100
100
100

37.58
0.17
38.05
438.74
0.68
298.79

min

17.50
15028.96
39.50
2.27
3773.66
3.96

𝑛𝑒 𝑓 𝑓 /s
median

79.83
17350.35
78.83
6.84
4403.28
10.04

max

80.39
18032.36
79.33
7.00
4504.26
14.72

Table 3. Runtime and 𝑛𝑒 𝑓 𝑓 /s for Noisy-Or Topic Model.

From Figure 4, we can see that all PPLs converge to the same predictive log likelihood. We see that
JAGS is extremely fast at this problem because it is written in C++ while PyMC3 is Python-based.
We suspect that Stan is slower then PyMC3 and JAGS because of the reparameterization which
introduces twice as many variables per node.

Sourabh Kulkarni, Kinjal Divesh Shah, Nimar Arora, Xiaoyan Wang, Yucen Lily Li, Nazanin Khosravani Tehrani, Michael
Tingley, David Noursi, Narjes Torabi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer
6

(a) 30 Topics, 300 Words

(b) 100 Topics, 3000 Words

Fig. 4. Predictive log likelihood against samples for Noisy-Or Topic Model

6 CROWDSOURCED ANNOTATION MODEL
There exist several applications where the complexity and volume of the task requires crowdsourcing
to a large set of human labelers. Inferring the true label of an item from the labels assigned by
several labelers is facilitated by this crowdsourced annotation model[Passonneau and Carpenter
2014]. The model takes into consideration an unknown prevalence of label categories, an unknown
per-item category, and an unknown confusion matrix per-labeler. For model definition, see 8.4

6.1 Results

PPL

Time(s)

𝑛𝑒 𝑓 𝑓 /s
min median max

Stan-MCMC
Stan-VI (meanfield)
Stan-VI (fullrank)

484.59
15.23
35.79

2.14
0.48
0.12

4.22
7.86
0.20

10.45
216.35
84.16

Table 4. Runtime and 𝑛𝑒 𝑓 𝑓 /s
for Crowd
Sourced Annotation Model with items=10K and
labelers=100

Fig. 5. Predictive log likelihood against samples
for Crowd Sourced Annotation Model

PPL Bench can also benchmark different inference techniques within a PPL. Here, we benchmark
Stan’s NUTS against Stan’s VI methods, using both meanfield as well as fullrank algorithm. We
notice that VI using meanfield algorithm is indistinguishable from NUTS. However, VI using
fullrank converges to a slightly different predictive log likelihood.

7 CONCLUSION
We have provided a modular benchmarking framework for evaluating PPLs. In its current form,
this is a very minimal benchmarking suite, but we hope that it will become more diverse with
community contributions. Our work to standardize the evaluation of inference should aid scientific
development in this field, and our focus on popular statistical models should encourage overall
industrial deployments of PPLs.

PPL Bench: Evaluation Framework For Probabilistic Programming Languages

7

REFERENCES
Nimar S. Arora, Nazanin Khosravani Tehrani, Kinjal Divesh Shah, Michael Tingley, Yucen Lily Li, Narjes Torabi, David
Noursi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer. 2019. Newtonian Monte Carlo: a second-order gradient
method for speeding up MCMC. In Association for Advancement in Approximate Bayesian Inference (AABI).

Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. 2011. Handbook of Markov Chain Monte Carlo. CRC press.
Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus Brubaker,
Jiqiang Guo, Peter Li, and Allen Riddell. 2017. Stan: A probabilistic programming language. Journal of statistical software
76, 1 (2017).

Marco Cusumano-Towner and Vikash K Mansinghka. 2018. A design proposal for Gen: probabilistic programming with
fast custom inference via code generation. In Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine
Learning and Programming Languages. ACM, 52–57.

Hong Ge, Kai Xu, and Zoubin Ghahramani. 2018. Turing: A language for flexible probabilistic inference. In International

Conference on Artificial Intelligence and Statistics. 1682–1690.

Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian data analysis.

Chapman and Hall/CRC.

Zoubin Ghahramani. 2015. Probabilistic Machine Learning and Artificial Intelligence. Nature 521, 7553 (2015), 452.
Peter W Glynn and Donald L Iglehart. 1989. Importance sampling for stochastic simulations. Management Science 35, 11

(1989), 1367–1392.

Noah Goodman, Vikash Mansinghka, Daniel M Roy, Keith Bonawitz, and Joshua Tenenbaum. 2008. Church: a language for
generative models with non-parametric memoization and approximate inference. In Uncertainty in Artificial Intelligence.
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David M Blei. 2017. Automatic differentiation

Variational Inference. The Journal of Machine Learning Research 18, 1 (2017), 430–474.

Jialu Liu, Xiang Ren, Jingbo Shang, Taylor Cassidy, Clare R Voss, and Jiawei Han. 2016. Representing Documents via Latent
Keyphrase Inference. In Proceedings of the 25th international conference on World Wide Web. International World Wide
Web Conferences Steering Committee, 1057–1067.

Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2016. The concrete distribution: A continuous relaxation of discrete

random variables. arXiv preprint arXiv:1611.00712 (2016).

Vikash Mansinghka, Daniel Selsam, and Yura Perov. 2014. Venture: a higher-order probabilistic programming platform with

programmable inference. arXiv preprint arXiv:1404.0099 (2014).

Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L. Ong, and Andrey Kolobov. 2005. BLOG: Probabilistic

models with unknown objects. In IJCAI International Joint Conference on Artificial Intelligence. 1352–1359.

Rebecca J Passonneau and Bob Carpenter. 2014. The benefits of a model of annotation. Transactions of the Association for

Computational Linguistics 2 (2014), 311–326.

Avi Pfeffer. 2001. IBAL: A probabilistic rational programming language. In IJCAI. Citeseer, 733–740.
Martyn Plummer et al. 2003. JAGS: A program for analysis of Bayesian graphical models using Gibbs sampling. In Proceedings

of the 3rd international workshop on distributed statistical computing, Vol. 124. Vienna, Austria., 10.

Peter J Rousseeuw and Annick M Leroy. 2005. Robust regression and outlier detection. Vol. 589. John wiley & sons.
John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. 2016. Probabilistic programming in Python using PyMC3.

PeerJ Computer Science 2 (apr 2016), e55. https://doi.org/10.7717/peerj-cs.55

David Spiegelhalter, Andrew Thomas, Nicky Best, and Wally Gilks. 1996. BUGS 0.5: Bayesian inference using Gibbs sampling

manual (version ii). MRC Biostatistics Unit, Institute of Public Health, Cambridge, UK (1996), 1–59.

Python Core Team. 2019. Python: A dynamic, open source programming language. Python Software Foundation. https:

//www.python.org/

Nazanin Tehrani, Nimar S. Arora, Yucen Lily Li, Kinjal Divesh Shah, David Noursi, Michael Tingley, Narjes Torabi, Sepehr
Masouleh, Eric Lippert, and Erik Meijer. 2020. Bean Machine: A Declarative Probabilistic Programming Language For
Efficient Programmable Inference. In Probabilistic Graphical Models (PGM).

Noel van Erp and P.H.A.J.M. Gelder. 2013. Bayesian logistic regression analysis. (08 2013), 147–154. https://doi.org/10.

1063/1.4819994

Martin J Wainwright, Michael I Jordan, et al. 2008. Graphical models, exponential families, and variational inference.

Foundations and Trends® in Machine Learning 1, 1–2 (2008), 1–305.

Sourabh Kulkarni, Kinjal Divesh Shah, Nimar Arora, Xiaoyan Wang, Yucen Lily Li, Nazanin Khosravani Tehrani, Michael
Tingley, David Noursi, Narjes Torabi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer
8

8 APPENDIX A - STATISTICAL MODEL DEFINITIONS

8.1 Bayesian Logistic Regression

8.2 Robust Regression

𝛼 ∼ Normal(0, 10, size = 1),
𝛽 ∼ Normal(0, 2.5, size = 𝐾),
𝑋𝑖 ∼ Normal(0, 10, size = 𝐾) ∀𝑖 ∈ 1 . . . 𝑁
𝑇 𝛽 ∀𝑖 ∈ 1..𝑁
𝜇𝑖 = 𝛼 + 𝑋𝑖
𝑌𝑖 ∼ Bernoulli(logit = 𝜇𝑖 ) ∀𝑖 ∈ 1..𝑁 .

𝜈 ∼ Gamma(2, 10)
𝜎 ∼ Exponential(1.0)
𝛼 ∼ Normal(0, 10)
𝛽 ∼ Normal(0, 2.5, size = 𝐾)
𝑋𝑖 ∼ Normal(0, 10, size = 𝐾) ∀𝑖 ∈ 1 . . . 𝑁
𝜇𝑖 = 𝛼 + 𝛽𝑇 𝑋𝑖 ∀𝑖 ∈ 1 . . . 𝑁
𝑌𝑖 ∼ Student-T(𝜈, 𝜇𝑖, 𝜎) ∀𝑖 ∈ 1 . . . 𝑁

8.3 Noisy-OR Topic Model
Let 𝑍 be the the set of all nodes in the network. Each model has a leak node 𝑂 which is the parent
of all other nodes. The set of keyphrases is denoted by 𝐾 and the set of content units is represented
as 𝑇 .

|Children(𝐾𝑖 )| ∼ Poisson(𝜆 = 3) ∀𝑖 ∈ 1 . . . 𝐾
W𝑜 𝑗 ∼ Exponential(0.1) ∀𝑗 ∈ 1 . . . 𝑍
W𝑖 𝑗 ∼ Exponential(1.0) ∀𝑖, 𝑗 where i ∈ Parents(𝑍 𝑗 )

𝑃 (𝑍 𝑗 = 1|Parents(𝑍 𝑗 )) = 1 − exp(−𝑊𝑜 𝑗 −

(𝑊𝑖 𝑗 ∗ 𝑍𝑖 ))

∑︁

𝑖

Z𝑗 ∼ Bernoulli(𝑃 (𝑍 𝑗 = 1|Parents(𝑍 𝑗 )))

8.4 Crowd Sourced Annotation Model
There are 𝑁 items, 𝐾 labelers, and each item could be one of 𝐶 categories. Each item 𝑖 is labeled
by a set 𝐽𝑖 of labelers. Such that the size of 𝐽𝑖 is sampled randomly, and each labeler in 𝐽𝑖 is drawn
uniformly without replacement from the set of all labelers. 𝑧𝑖 is the true label for item 𝑖 and 𝑦𝑖 𝑗 is
the label provided to item 𝑖 by labeler 𝑗. Each labeler 𝑙 has a confusion matrix 𝜃𝑙 such that 𝜃𝑙𝑚𝑛 is

PPL Bench: Evaluation Framework For Probabilistic Programming Languages

9

the probability that an item with true class 𝑚 is labeled 𝑛 by 𝑙.

𝜋 ∼ Dirichlet

(cid:18) 1
𝐶
𝑧𝑖 ∼ Categorical(𝜋) ∀𝑖 ∈ 1 . . . 𝑁

, . . . , 1
𝐶

(cid:19)

𝜃𝑙𝑚 ∼ Dirichlet(𝛼𝑚) ∀𝑙 ∈ 1 . . . 𝐾, 𝑚 ∈ 1 . . . 𝐶
|𝐽𝑖 | ∼ Poisson(𝐽loc)

𝑙 ∈ 𝐽𝑖 ∼ Uniform(1 . . . 𝐾) without replacement

𝑦𝑖𝑙 ∼ Categorical(𝜃𝑙𝑧𝑖 ) ∀𝑙 ∈ 𝐽𝑖

. We set 𝛼𝑚𝑛 = 𝛾 · 𝜌 if 𝑚 = 𝑛 and 𝛼𝑚𝑛 = 𝛾 · (1 − 𝜌) ·

Here 𝛼𝑚 ∈ R+𝐶
𝐶−1 if 𝑚 ≠ 𝑛. Where 𝛾 is the
concentration and 𝜌 is the a-priori correctness of the labelers. In this model, 𝑌𝑖𝑙 and 𝐽𝑖 are observed.
In our experiments, we fixed 𝐶 = 3, 𝐽loc = 2.5, 𝛾 = 10, and 𝜌 = 0.5.

1

