0
2
0
2

p
e
S
8
1

]

G
L
.
s
c
[

5
v
7
4
8
2
1
.
4
0
9
1
:
v
i
X
r
a

Optimal Sparse Decision Trees

Xiyang Hu1, Cynthia Rudin2, Margo Seltzer3∗
1Carnegie Mellon University, xiyanghu@cmu.edu
2Duke University, cynthia@cs.duke.edu
3The University of British Columbia, mseltzer@cs.ubc.ca

Abstract

Decision tree algorithms have been among the most popular algorithms for inter-
pretable (transparent) machine learning since the early 1980’s. The problem that
has plagued decision tree algorithms since their inception is their lack of optimality,
or lack of guarantees of closeness to optimality: decision tree algorithms are often
greedy or myopic, and sometimes produce unquestionably suboptimal models.
Hardness of decision tree optimization is both a theoretical and practical obstacle,
and even careful mathematical programming approaches have not been able to
solve these problems efﬁciently. This work introduces the ﬁrst practical algorithm
for optimal decision trees for binary variables. The algorithm is a co-design of
analytical bounds that reduce the search space and modern systems techniques,
including data structures and a custom bit-vector library. Our experiments highlight
advantages in scalability, speed, and proof of optimality.

1

Introduction

Interpretable machine learning has been growing in importance as society has begun to realize
the dangers of using black box models for high stakes decisions: complications with confounding
have haunted our medical machine learning models [22], bad predictions from black boxes have
announced to millions of people that their dangerous levels of air pollution were safe [15], high-stakes
credit risk decisions are being made without proper justiﬁcation, and black box risk predictions have
been wreaking havoc with the perception of fairness of our criminal justice system [10]. In all of
these applications – medical imaging, pollution modeling, recidivism risk, credit scoring – accurate
interpretable models have been created (by the Center for Disease Control and Prevention, Arnold
Foundation, and others). However, such interpretable-yet-accurate models are not generally easy to
construct. If we want people to replace their black box models with interpretable models, the tools to
build these interpretable models must ﬁrst exist.

Decision trees are one of the leading forms of interpretable models. Despite several attempts over the
last several decades to improve the optimality of decision tree algorithms, the CART [7] and C4.5
[19] decision tree algorithms (and other greedy tree-growing variants) have remained as dominant
methods in practice. CART and C4.5 grow decision trees from the top down without backtracking,
which means that if a suboptimal split was introduced near the top of the tree, the algorithm could
spend many extra splits trying to undo the mistake it made at the top, leading to less-accurate and
less-interpretable trees. Problems with greedy splitting and pruning have been known since the
early 1990’s, when mathematical programming tools had started to be used for creating optimal
binary-split decision trees [3, 4], in a line of work [5, 6, 16, 18] until the present [20]. However, these
techniques use all-purpose optimization toolboxes and tend not to scale to realistically-sized problems
unless simpliﬁed to trees of a speciﬁc form. Other works [11] make overly strong assumptions (e.g.,
independence of all variables) to ensure optimal trees are produced using greedy algorithms.

∗Authors are listed alphabetically.

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
We produce optimal sparse decision trees taking a different approach than mathematical programming,
greedy methods, or brute force. We ﬁnd optimal trees according to a regularized loss function that
balances accuracy and the number of leaves. Our algorithm is computationally efﬁcient due to a
collection of analytical bounds to perform massive pruning of the search space. Our implementation
uses specialized data structures to store intermediate computations and symmetries, a bit-vector
library to evaluate decision trees more quickly, fast search policies, and computational reuse. Despite
the hardness of ﬁnding optimal solutions, our algorithm is able to locate optimal trees and prove
optimality (or closeness of optimality) in reasonable amounts of time for datasets of the sizes used in
the criminal justice system (tens of thousands or millions of observations, tens of features).

Because we ﬁnd provably optimal trees, our experiments show where previous studies have claimed
to produce optimal models yet failed; we show speciﬁc cases where this happens. We test our method
on benchmark data sets, as well as criminal recidivism and credit risk data sets; these are two of
the high-stakes decision problems where interpretability is needed most in AI systems. We provide
ablation experiments to show which of our techniques is most inﬂuential at reducing computation
for various datasets. As a result of this analysis, we are able to pinpoint possible future paths to
improvement for scalability and computational speed. Our contributions are: (1) The ﬁrst practical
optimal binary-variable decision tree algorithm to achieve solutions for nontrivial problems. (2) A
series of analytical bounds to reduce the search space. (3) Algorithmic use of a tree representation
using only its leaves. (4) Implementation speedups saving 97% run time. (5) We present the ﬁrst
optimal sparse binary split trees ever published for the COMPAS and FICO datasets.

The code and the supplementary materials are available at https://github.com/xiyanghu/OSDT.

2 Related Work

Optimal decision trees have a quite a long history [3], so we focus on closely related techniques.
There are efﬁcient algorithms that claim to generate optimal sparse trees, but do not optimally balance
the criteria of optimality and sparsity; instead they pre-specify the topology of the tree (i.e., they know
a priori exactly what the structure of the splits and leaves are, even though they do not know which
variables are split) and only ﬁnd the optimal tree of the given topology [16]. This is not the problem
we address, as we do not know the topology of the optimal tree in advance. The most successful
algorithm of this variety is BinOCT [20], which searches for a complete binary tree of a given depth;
we discuss BinOCT shortly. Some exploration of learning optimal decision trees is based on boolean
satisﬁability (SAT) [17], but again, this work looks only for the optimal tree of a given number of
nodes. The DL8 algorithm [18] optimizes a ranking function to ﬁnd a decision tree under constraints
of size, depth, accuracy and leaves. DL8 creates trees from the bottom up, meaning that trees are
assembled out of all possible leaves, which are itemsets pre-mined from the data [similarly to 2]. DL8
does not have publicly available source code, and its authors warn about running out of memory when
storing all partially-constructed trees. Some works consider oblique trees [6], where splits involve
several variables; oblique trees are not addressed here, as they can be less interpretable.

The most recent mathematical programming algorithms are OCT [5] and BinOCT [20]. Example
ﬁgures from the OCT paper [5] show decision trees that are clearly suboptimal. However, as the code
was not made public, the work in the OCT paper [5] is not easily reproducible, so it is not clear where
the problem occurred. We discuss this in Section §4. Verwer and Zhang’s mathematical programming
formulation for BinOCT is much faster [20], and their experiments indicate that BinOCT outperforms
OCT, but since BinOCT is constrained to create complete binary trees of a given depth rather than
optimally sparse trees, it sometimes creates unnecessary leaves in order to complete a tree at a given
depth, as we show in Section §4. BinOCT solves a dramatically easier problem than the method
introduced in this work. As it turns out, the search space of perfect binary trees of a given depth is
much smaller than that of binary trees with the same number of leaves. For instance, the number of
different unlabeled binary trees with 8 leaves is Catalan(7) = 429, but the number of unlabeled
perfect binary trees with 8 leaves is only 1. In our setting, we penalize (but do not ﬁx) the number
of leaves, which means that our search space contains all trees, though we can bound the maximum
number of leaves based on the size of the regularization parameter. Therefore, our search space is
much larger than that of BinOCT.

Our work builds upon the CORELS algorithm [1, 2, 13] and its predecessors [14, 21], which create
optimal decision lists (rule lists). Applying those ideas to decision trees is nontrivial. The rule list

2

Search Space of CORELS and Decision Trees

p = 10

p = 20

d
1
2
3
4
5

Rule Lists
5.500 × 101
3.025 × 103
1.604 × 105
8.345 × 106
4.257 × 108

Trees
1.000 × 101
1.000 × 103
5.329 × 106
9.338 × 1020
“Inf”

Rule Lists
2.100 × 102
4.410 × 104
9.173 × 106
1.898 × 109
3.911 × 1011

Trees
2.000 × 101
8.000 × 103
9.411 × 108
9.204 × 1028
“Inf”

Table 1: Search spaces of rule lists and decision trees with number of variables p = 10, 20 and depth
d = 1, 2, 3, 4, 5. The search space of the trees explodes in comparison.

optimization problem is much easier, since the rules are pre-mined (there is no mining of rules in
our decision tree optimization). Rule list optimization involves selecting an optimal subset and an
optimal permutation of the rules in each subset. Decision tree optimization involves considering
every possible split of every possible variable and every possible shape and size of tree. This is
an exponentially harder optimization problem with a huge number of symmetries to consider. In
addition, in CORELS, the maximum number of clauses per rule is set to be c = 2. For a data set
with p binary features, there would be D = p + (cid:0)p
(cid:1) rules in total, and the number of distinct rule
lists with dr rules is P (D, dr), where P (m, k) is the number of k-permutations of m. Therefore, the
search space of CORELS is (cid:80)d−1
dr=1 P (D, dr). But, for a full binary tree with depth dt and data with
p binary features, the number of distinct trees is:

2

Ndt =

1
(cid:88)

2n0
(cid:88)

2ndt−2
(cid:88)

· · ·

n0=1

n1=1

ndt−1=1

p ×

(cid:18)2n0
n1

(cid:19)

(p − 1)n1 × . . . ×

(cid:18)2ndt−2
ndt−1

(cid:19)

(p − (dt − 1))ndt−1,

(1)

and the search space of decision trees up to depth d is (cid:80)d
dt=1 Ndt. Table 1 shows how the search
spaces of rule lists and decision trees grow as the tree depth increases. The search space of the trees
is massive compared to that of the rule lists.

Applying techniques from rule lists to decision trees necessitated new designs for the data structures,
splitting mechanisms and bounds. An important difference between rule lists and trees is that during
the growth of rule lists, we add only one new rule to the list each time, but for the growth of trees, we
need to split existing leaves and add a new pair of leaves for each. This leads to several bounds that
are quite different from those in CORELS, i.e., Theorem 3.4, Theorem 3.5 and Corollary E.1, which
consider a pair of leaves rather than a single leaf. In this paper, we introduce bounds only for the case
of one split at a time; however, in our implementation, we can split more than one leaf at a time, and
the bounds are adapted accordingly.

3 Optimal Sparse Decision Trees (OSDT)

n=1 and y = {yn}N

We focus on binary classiﬁcation, although it is possible to generalize this framework to multiclass
n=1, where xn ∈ {0, 1}M are binary features and
settings. We denote training data as {(xn, yn)}N
yn ∈ {0, 1} are labels. Let x = {xn}N
n=1, and let xn,m denote the m-th feature
of xn. For a decision tree, its leaves are conjunctions of predicates. Their order does not matter in
evaluating the accuracy of the tree, and a tree grows only at its leaves. Thus, within our algorithm,
we represent a tree as a collection of leaves. A leaf set d = (p1, p2, . . . , pH ) of length H ≥ 0 is an
H-tuple containing H distinct leaves, where pk is the classiﬁcation rule of the path from the root
to leaf k. Here, pk is a Boolean assertion, which evaluates to either true or false for each datum xn
indicating whether it is classiﬁed by leaf k. Here, ˆy(leaf)
We explore the search space by considering which leaves of the tree can be beneﬁcially split. The
leaf set d = (p1, p2, . . . , pK, pK+1, . . . , pH ) is the H-leaf tree, where the ﬁrst K leaves may not
to be split, and the remaining H − K leaves can be split. We alternately represent this leaf set as
d = (dun, δun, dsplit, δsplit, K, H), where dun = (p1, . . . , pK) are the unchanged leaves of d, δun =
(ˆy(leaf)
K ) ∈ {0, 1}K are the predicted labels of leaves dun, dsplit = (pK+1, . . . , pH ) are the
1
leaves we are going to split, and δsplit = (ˆy(leaf)
K+1, . . . , ˆy(leaf)
H ) ∈ {0, 1}H−K are the predicted labels of
leaves dsplit. We call dun a K-preﬁx of d, which means its leaves are a size-K unchanged subset of
(p1, . . . , pK, . . . , pH ). If we have a new preﬁx d(cid:48)
un ⊇ dun, then

un, which is a superset of dun, i.e., d(cid:48)

is the label for all points so classiﬁed.

, . . . , ˆy(leaf)

k

3

we say d(cid:48)

un starts with dun. We deﬁne σ(d) to be all descendents of d:

σ(d) = {(d(cid:48)

un, δ(cid:48)
If we have two trees d = (dun, δun, dsplit, δsplit, K, H) and d(cid:48) = (d(cid:48)
H (cid:48) = H + 1, d(cid:48) ⊃ d, d(cid:48)
we deﬁne d(cid:48) to be a child of d and d to be a parent of d(cid:48).

un ⊇ dun, d(cid:48) ⊃ d}.
split, K (cid:48), H (cid:48)), where
split, δ(cid:48)
un, d(cid:48)
un ⊇ dun, i.e., d(cid:48) contains one more leaf than d and d(cid:48)
un starts with dun, then

split, K (cid:48), Hd(cid:48)) : d(cid:48)

split, δ(cid:48)

un, d(cid:48)

un, δ(cid:48)

(2)

Note that two trees with identical leaf sets, but different assignments to dun and dsplit, are different
trees. Further, a child tree can only be generated through splitting leaves of its parent tree within dsplit.
A tree d classiﬁes datum xn by providing the label prediction ˆy(leaf)
of the leaf whose pk is true
k
for xn. Here, the leaf label ˆy(leaf)
is the majority label of data captured by the leaf k. If pk evaluates
to true for xn, we say the leaf k of leaf set dun captures xn . In our notation, all the data captured by
a preﬁx’s leaves are also captured by the preﬁx itself.

k

Let β be a set of leaves. We deﬁne cap(xn, β) = 1 if a leaf in β captures datum xn, and 0 otherwise.
For example, let d and d(cid:48) be leaf sets such that d(cid:48) ⊃ d, then d(cid:48) captures all the data that d captures:
{xn : cap(xn, d)} ⊆ {xn : cap(xn, d(cid:48))}.
The normalized support of β, denoted supp(β, x), is the fraction of data captured by β:

supp(β, x) =

1
N

N
(cid:88)

n=1

cap(xn, β).

(3)

3.1 Objective Function

For a tree d = (dun, δun, dsplit, δsplit, K, Hd), we deﬁne its objective function as a combination of the
misclassiﬁcation error and a sparsity penalty on the number of leaves:

R(d, x, y) = (cid:96)(d, x, y) + λHd.

(4)

R(d, x, y) is a regularized empirical risk. The loss (cid:96)(d, x, y) is the misclassiﬁcation error of d,
i.e., the fraction of training data with incorrectly predicted labels. Hd is the number of leaves in the
tree d. λHd is a regularization term that penalizes bigger trees. Statistical learning theory provides
guarantees for this problem; minimizing the loss subject to a (soft or hard) constraint on model size
leads to a low upper bound on test error from the Occham’s Razor Bound.

3.2 Optimization Framework

We minimize the objective function based on a branch-and-bound framework. We propose a series of
specialized bounds that work together to eliminate a large part of the search space. These bounds are
discussed in detail in the following paragraphs. Proofs are in the supplementary materials.

Some of our bounds could be adapted directly from CORELS [2], namely these two:
(Hierarchical objective lower bound) Lower bounds of a parent tree also hold for every child tree of
that parent (§3.3, Theorem 3.1). (Equivalent points bound) For a given dataset, if there are multiple
samples with exactly the same features but different labels, then no matter how we build our classiﬁer,
we will always make mistakes. The lower bound on the number of mistakes is therefore the number
of such samples with minority class labels (§B, Theorem B.2).

Some of our bounds adapt from CORELS [1] with minor changes: (Objective lower bound with
one-step lookahead) With respect to the number of leaves, if a tree does not achieve enough accuracy,
we can prune all child trees of it (§3.3, Lemma 3.2). (A priori bound on the number of leaves) For
an optimal decision tree, we provide an a priori upper bound on the maximum number of leaves (§C,
Theorem C.3). (Lower bound on node support) For an optimal tree, the support traversing through
each internal node must be at least 2λ (§3.4, Theorem 3.3).

Some of our bounds are distinct from CORELS, because they are only relevant to trees and not to
lists: (Lower bound on incremental classiﬁcation accuracy) Each split must result in sufﬁcient
reduction of the loss. Thus, if the loss reduction is less than or equal to the regularization term,
we should still split, and we have to further split at least one of the new child leaves to search for
the optimal tree (§3.4, Theorem 3.4). (Leaf permutation bound) We need to consider only one

4

permutation of leaves in a tree; we do not need to consider other permutations (explained in §E,
Corollary E.1). (Leaf accurate support bound) For each leaf in an optimal decision tree, the number
of correctly classiﬁed samples must be above a threshold. (§3.4, Theorem 3.5). The supplement
contains an additional set of bounds on the number of remaining tree evaluations.

3.3 Hierarchical Objective Lower Bound

, . . . , ˆy(leaf)
(cid:80)K

The loss can be decomposed into two parts corresponding to the unchanged leaves and the leaves
to be split: (cid:96)(d, x, y) ≡ (cid:96)p(dun, δun, x, y) + (cid:96)q(dsplit, δsplit, x, y), where dun = (p1, . . . , pK), δun =
(ˆy(leaf)
K+1, . . . , ˆy(leaf)
K ), dsplit = (pK+1, . . . , pHd ) and δsplit = (ˆy(leaf)
); (cid:96)p(dun, δun, x, y) =
1
Hd
(cid:80)N
k=1 cap(xn, pk) ∧ 1[ˆy(leaf)
1
(cid:54)= yn] is the proportion of data in the unchanged leaves that
N
are misclassiﬁed, and (cid:96)p(dsplit, δsplit, x, y) = 1
(cid:54)= yn] is
N
the proportion of data in the leaves we are going to split that are misclassiﬁed. We deﬁne a lower
bound b(dun, x, y) on the objective by leaving out the latter loss,

k=K+1 cap(xn, pk) ∧ 1[ˆy(leaf)

(cid:80)Hd

(cid:80)N

n=1

n=1

k

k

b(dun, x, y) ≡ (cid:96)p(dun, δun, x, y) + λHd ≤ R(d, x, y),
where the leaves dun are kept and the leaves dsplit are going to be split. Here, b(dun, x, y) gives a
lower bound on the objective of any child tree of d.
Theorem 3.1 (Hierarchical objective lower bound). Deﬁne b(dun, x, y) = (cid:96)p(dun, δun, x, y) +
λHd, as in (5). Deﬁne σ(d) to be the set of all d’s child trees whose unchanged leaves con-
tain dun, as in (2). For tree d = (dun, δun, dsplit, δsplit, K, Hd) with unchanged leaves dun, let
d(cid:48) = (d(cid:48)
un
contain dun and K (cid:48) ≥ K, Hd(cid:48) ≥ Hd, then b(dun, x, y) ≤ R(d(cid:48), x, y).

split, K (cid:48), Hd(cid:48)) ∈ σ(d) be any child tree such that its unchanged leaves d(cid:48)

split, δ(cid:48)

un, d(cid:48)

un, δ(cid:48)

(5)

Consider a sequence of trees, where each tree is the parent of the following tree. In this case, the lower
bounds of these trees increase monotonically, which is amenable to branch-and-bound. We illustrate
our framework in Algorithm 1 in Supplement A. According to Theorem 3.1, we can hierarchically
prune the search space. During the execution of the algorithm, we cache the current best (smallest)
objective Rc, which is dynamic and monotonically decreasing. In this process, when we generate a tree
whose unchanged leaves dun correspond to a lower bound satifying b(dun, x, y) ≥ Rc, according to
Theorem 3.1, we do not need to consider any child tree d(cid:48) ∈ σ(d) of this tree whose d(cid:48)
un contains dun.

Based on Theorem 3.1, we describe a consequence in Lemma 3.2.
Lemma 3.2 (Objective lower bound with one-step lookahead). Let d be a Hd-leaf tree with a
K-leaf preﬁx and let Rc be the current best objective. If b(dun, x, y) + λ ≥ Rc, then for any
child tree d(cid:48) ∈ σ(d), its preﬁx d(cid:48)
un starts with dun and K (cid:48) > K, Hd(cid:48) > Hd, and it follows that
R(d(cid:48), x, y) ≥ Rc.

This bound tends to be very powerful in practice in pruning the search space, because it states that
even though we might have a tree with unchanged leaves dun whose lower bound b(dun, x, y) ≤ Rc,
if b(dun, x, y) + λ ≥ Rc, we can still prune all of its child trees.

3.4 Lower Bounds on Node Support and Classiﬁcation Accuracy

We provide three lower bounds on the fraction of correctly classiﬁed data and the normalized support
of leaves in any optimal tree. All of them depend on λ.
Theorem 3.3 (Lower bound on node support). Let d∗ = (dun, δun, dsplit, δsplit, K, Hd∗ ) be any
optimal tree with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For an optimal tree, the support
traversing through each internal node must be at least 2λ. That is, for each child leaf pair pk, pk+1
of a split, the sum of normalized supports of pk, pk+1 should be no less than twice the regularization
parameter, i.e., 2λ,

2λ ≤ supp(pk, x) + supp(pk+1, x).

(6)

Therefore, for a tree d, if any of its internal nodes capture less than a fraction 2λ of the samples, it
cannot be an optimal tree, even if b(dun, x, y) < R∗. None of its child trees would be an optimal tree
either. Thus, after evaluating d, we can prune tree d.
Theorem 3.4 (Lower bound on incremental classiﬁcation accuracy). Let d∗ = (dun, δun,
dsplit, δsplit, K, Hd∗ ) be any optimal tree with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). Let d∗

5

Figure 1: Training accuracy of OSDT, CART, BinOCT on different datasets (time limit: 30 minutes).
Horizontal lines indicate the accuracy of the best OSDT tree. On most datasets, all trees of BinOCT
and CART are below this line.

have leaves dun = (p1, . . . , pHd∗ ) and labels δun = (ˆy(leaf)
with corresponding labels ˆy(leaf)
its label ˆy(leaf)

Hd∗ ). For each leaf pair pk, pk+1
k+1 in d∗ and their parent node (the leaf in the parent tree) pj and
, deﬁne ak to be the incremental classiﬁcation accuracy of splitting pj to get pk, pk+1:

, . . . , ˆy(leaf)

, ˆy(leaf)

k

1

j

ak ≡

1
N

N
(cid:88)

{cap(xn, pk) ∧ 1[ˆy(leaf)

k = yn] + cap(xn, pk+1) ∧ 1[ˆy(leaf)

k+1 = yn] − cap(xn, pj ) ∧ 1[ˆy(leaf)

j

= yn]}.

(7)

n=1

In this case, λ provides a lower bound, λ ≤ ak.

Thus, when we split a leaf of the parent tree, if the incremental fraction of data that are correctly
classiﬁed after this split is less than a fraction λ, we need to further split at least one of the two child
leaves to search for the optimal tree. Thus, we apply Theorem 3.3 when we split the leaves. We
need only split leaves whose normalized supports are no less than 2λ. We apply Theorem 3.4 when
constructing the trees. For every new split, we check the incremental accuracy for this split. If it is less
than λ, we further split at least one of the two child leaves. Both Theorem 3.3 and Theorem 3.4 are
bounds for pairs of leaves. We give a bound on a single leaf’s classiﬁcation accuracy in Theorem 3.5.
Theorem 3.5 (Lower bound on classiﬁcation accuracy). Let d∗ = (dun, δun, dsplit, δsplit, K, Hd∗ ) be
any optimal tree with objective R∗, i.e., d∗ ∈ argmind R(d, x, y). For each leaf (pk, ˆy(leaf)
) in d∗,
the fraction of correctly classiﬁed data in leaf k should be no less than λ,

k

λ ≤

1
N

N
(cid:88)

n=1

cap(xn, pk) ∧ 1[ˆy(leaf)

k

= yn].

(8)

Thus, in a leaf we consider extending by splitting on a particular feature, if that proposed split leads
to less than λ correctly classiﬁed data going to either side of the split, then this split can be excluded,
and we can exclude that feature anywhere further down the tree extending that leaf.

3.5

Incremental Computation

Much of our implementation effort revolves around exploiting incremental computation, designing
data structures and ordering of the worklist. Together, these ideas save >97% execution time. We
provide the details of our implementation in the supplement.

4 Experiments

We address the following questions through experimental analysis: (1) Do existing methods achieve
optimal solutions, and if not, how far are they from optimal? (2) How fast does our algorithm converge
given the hardness of the problem it is solving? (3) How much does each of the bounds contribute to
the performance of our algorithm? (4) What do optimal trees look like?

6

02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyClassification Accuracy of COMPAS Dataset OSDTCARTBinOCT048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyClassification Accuracy of FICO Dataset OSDTCARTBinOCT02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyClassification Accuracy of Tic-Tac-Toe Dataset OSDTCARTBinOCT02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyClassification Accuracy of Car Dataset OSDTCARTBinOCT02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyClassification Accuracy of Monk1 Dataset OSDTCARTBinOCT02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyClassification Accuracy of Monk2 Dataset OSDTCARTBinOCT02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyClassification Accuracy of Monk3 Dataset OSDTCARTBinOCTThe results of the per-bound performance and memory improvement experiment (Table 2 in the
supplement) were run on a m5a.4xlarge instance of AWS’s Elastic Compute Cloud (EC2). The
instance has 16 2.5GHz virtual CPUs (although we run single-threaded on a single core) and 64 GB
of RAM. All other results were run on a personal laptop with a 2.4GHz i5-8259U processor and
16GB of RAM.

We used 7 datasets: Five of them are from the UCI Machine Learning Repository [8], (Tic Tac Toe,
Car Evaluation, Monk1, Monk2, Monk3). The other two datasets are the ProPublica recidivism data
set [12] and the Fair Isaac (FICO) credit risk dataset [9]. We predict which individuals are arrested
within two years of release (N = 7, 215) on the recidivism data set and whether an individual will
default on a loan for the FICO dataset.

Accuracy and optimality: We tested the accuracy of our algorithm against baseline methods CART and
BinOCT [20]. BinOCT is the most recent publicly available method for learning optimal classiﬁcation
trees and was shown to outperform other previous methods. As far as we know, there is no public
code for most of the other relevant baselines, including [5, 6, 16]. One of these methods, OCT [5],
reports that CART often dominates their performance (see Fig. 4 and Fig. 5 in their paper). Our
models can never be worse than CART’s models even if we stop early, because in our implementation,
we use the objective value of CART’s solution as a warm start to the objective value of the current
best. Figure 1 shows the training accuracy on each dataset. The time limits for both BinOCT and our
algorithm are set to be 30 minutes.

Figure 2: Example OSDT execution traces (COM-
PAS data, λ = 0.005). Lines are the objective
value and dashes are the lower bound for OSDT.
For each scheduling policy, the time to optimum
and optimal objective value are marked with a star.

Main results: (i) We can now evaluate how close
to optimal other methods are (and they are often
close to optimal or optimal). (ii) Sometimes, the
baselines are not optimal. Recall that BinOCT
searches only for the optimal tree given the
topology of the complete binary tree of a cer-
tain depth. This restriction on the topology mas-
sively reduces the search space so that BinOCT
runs quickly, but in exchange, it misses optimal
sparse solutions that our method ﬁnds. (iii) Our
method is fast. Our method runs on only one
thread (we have not yet parallelized it) whereas
BinOCT is highly optimized; it makes use of
eight threads. Even with BinOCT’s 8-thread par-
allelism, our method is competitive.

Convergence: Figure 2 illustrates the behavior of
OSDT for the ProPublica COMPAS dataset with
λ = 0.005, for two different scheduling policies
(curiosity and lower bound, see supplement).
The charts show how the current best objective
value Rc and the lower bound b(dun, x, y) vary
as the algorithm progresses. When we schedule
using the lower bound, the lower bounds of eval-
uated trees increase monotonically, and OSDT
certiﬁes optimality only when the value of the
lower bound becomes large enough that we can
prune the remaining search space or when the
queue is empty, whichever is reached earlier. Us-
ing curiosity, OSDT ﬁnds the optimal tree much
more quickly than when using the lower bound.

(b) The 4 features are those
in Figure 4

(a) This is based on all the
12 features
Figure 3: Scalability with respect to number of
samples and number of features using (multiples
of) the ProPublica data set. (λ = 0.005). Note that
all these executions include the 4 features of the
optimal tree, and the data size are increased by
duplicating the whole data set multiple times.

Scalability: Figure 3 shows the scalability of
OSDT with respect to the number of samples
and the number of features. Runtime can theoretically grow exponentially with the number of features.
However, as we add extra features that differ from those in the optimal tree, we can reach the optimum
more quickly, because we are able to prune the search space more efﬁciently as the number of
extra features grows. For example, with 4 features, it spends about 75% of the runtime to reach the
optimum; with 12 features, it takes about 5% of the runtime to reach the optimum.

7

050000100000150000200000Number of Trees Evaluated0.000.050.100.150.200.250.300.35ValueScheduling Policy: Curiosity020000040000060000080000010000001200000Number of Trees Evaluated0.000.050.100.150.200.250.300.35ValueScheduling Policy: Lower Bound0.000.250.500.751.001.25Number of Samples(×107)050100150200250300Time (s)Time to optimumTotal time4681012Number of Features02468Time (s)Time to optimumTotal timeFigure 4: An optimal decision tree generated by OSDT on the COMPAS dataset. (λ = 0.005,
accuracy: 66.90%)

(a) BinOCT (accuracy: 76.722%)

(b) OSDT (accuracy: 82.881%)

Figure 5: Eight-leaf decision trees generated by BinOCT and OSDT on the Tic-Tac-Toe data. Trees
of BinOCT must be complete binary trees, while OSDT can generate binary trees of any shape.

(a) BinOCT (accuracy: 91.129%)

(b) OSDT (accuracy: 100%)

Figure 6: Decision trees generated by BinOCT and OSDT on the Monk1 dataset. The tree generated
by BinOCT includes two useless splits (the left and right splits), while OSDT can avoid this problem.
BinOCT is 91% accurate, OSDT is 100% accurate.

Ablation experiments: Appendix I shows that the lookahead and equivalent points bounds are, by far,
the most signiﬁcant of our bounds, reducing time to optimum by at least two orders of magnitude and
reducing memory consumption by more than one order of magnitude.

Trees: We provide illustrations of the trees produced by OSDT and the baseline methods in Figures 4,
5 and 6. OSDT generates trees of any shape, and our objective penalizes trees with more leaves,
thus it never introduces splits that produce a pair of leaves with the same label. In contrast, BinOCT
trees are always complete binary trees of a given depth. This limitation on the tree shape can prevent
BinOCT from ﬁnding the globally optimal tree. In fact, BinOCT often produces useless splits, leading
to trees with more leaves than necessary to achieve the same accuracy.

Additional experiments: It is well-established that simpler models such as small decision trees
generalize well; a set of cross-validation experiments is in the supplement demonstrating this.

Conclusion: Our work shows the possibility of optimal (or provably near-optimal) sparse decision
trees. It is the ﬁrst work to balance the accuracy and the number of leaves optimally in a practical
amount of time. We have reason to believe this framework can be extended to much larger datasets.
Theorem F.1 identiﬁes a key mechanism for scaling these algorithms up. It suggests a bound stating
that highly correlated features can substitute for each other, leading to similar model accuracies.
Applications of this bound allow for the elimination of features throughout the entire execution,
allowing for more aggressive pruning. Our experience to date shows that by supporting such bounds
with the right data structures can potentially lead to dramatic increases in performance and scalability.

8

priors>3age<26Nopriors:2-3juvenile-crimes=0YesNoYesYestop-left=omiddle-middle=obottom-right=x11top-right=x01bottom-right=xmiddle-middle=x01top-right=o10middle-middle=xtop-left=xbottom-right=x0bottom-left=xtop-right=x011bottom-left=xtop-right=x0111head=roundbody=roundbody=squareYesYesjacket=redNoYesbody=roundjacket=redNoYeshead=roundYesYesjacket=redbody=squarehead=squarehead=roundbody=roundYesNohead=squareNoYesNohead=squareNoYesYesReferences

[1] E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably optimal rule lists for
categorical data. In Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (KDD), 2017.

[2] E. Angelino, N. Larus-Stone, D. Alabi, M. Seltzer, and C. Rudin. Learning certiﬁably optimal rule lists for

categorical data. Journal of Machine Learning Research, 18(234):1–78, 2018.

[3] K. Bennett. Decision tree construction via linear programming. In Proceedings of the 4th Midwest Artiﬁcial

Intelligence and Cognitive Science Society Conference, Utica, Illinois, 1992.

[4] K. P. Bennett and J. A. Blue. Optimal decision trees. Technical report, R.P.I. Math Report No. 214,

Rensselaer Polytechnic Institute, 1996.

[5] D. Bertsimas and J. Dunn. Optimal classiﬁcation trees. Machine Learning, 106(7):1039–1082, 2017.

[6] R. Blanquero, E. Carrizosa, C. Molero-Rıo, and D. R. Morales. Optimal randomized classiﬁcation trees.

Aug. 2018.

[7] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression Trees. Wadsworth,

1984.

[8] D. Dheeru and E. Karra Taniskidou. UCI machine learning repository, 2017.

[9] FICO, Google, Imperial College London, MIT, University of Oxford, UC Irvine, and UC Berkeley.
Explainable Machine Learning Challenge. https://community.ﬁco.com/s/explainable-machine-learning-
challenge, 2018.

[10] A. W. Flores, C. T. Lowenkamp, and K. Bechtel. False positives, false negatives, and false analyses: A
rejoinder to “Machine bias: There’s software used across the country to predict future criminals". Federal
probation, 80(2), September 2016.

[11] A. R. Klivans and R. A. Servedio. Toward attribute efﬁcient learning of decision lists and parities. Journal

of Machine Learning Research, 7:587–602, 2006.

[12] J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the COMPAS recidivism algorithm.

ProPublica, 2016.

[13] N. Larus-Stone, E. Angelino, D. Alabi, M. Seltzer, V. Kaxiras, A. Saligrama, and C. Rudin. Systems
optimizations for learning certiﬁably optimal rule lists. In Proc. Conference on Systems and Machine
Learning (SysML), 2018.

[14] B. Letham, C. Rudin, T. H. McCormick, and D. Madigan. Interpretable classiﬁers using rules and Bayesian
analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350–1371,
2015.

[15] M. McGough. How bad is Sacramento’s air, exactly? Google results appear at odds with reality, some say.

Sacramento Bee, 2018.

[16] M. Menickelly, O. Günlük, J. Kalagnanam, and K. Scheinberg. Optimal decision trees for categorical data

via integer programming. Preprint at arXiv:1612.03225, Jan. 2018.

[17] N. Narodytska, A. Ignatiev, F. Pereira, and J. Marques-Silva. Learning optimal decision trees with SAT. In

Proc. International Joint Conferences on Artiﬁcial Intelligence (IJCAI), pages 1362–1368, 2018.

[18] S. Nijssen and E. Fromont. Mining optimal decision trees from itemset lattices. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 530–539.
ACM, 2007.

[19] J. R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1993.

[20] S. Verwer and Y. Zhang. Learning optimal classiﬁcation trees using a binary linear program formulation.

In 33rd AAAI Conference on Artiﬁcial Intelligence, 2019.

[21] H. Yang, C. Rudin, and M. Seltzer. Scalable Bayesian rule lists. In International Conference on Machine

Learning (ICML), 2017.

[22] J. R. Zech, M. A. Badgeley, M. Liu, A. B. Costa, J. J. Titano, and E. K. Oermann. Variable generalization
performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study.
PLoS Med., 15(e1002683), 2018.

9

Optimal Sparse Decision Trees: Supplementary Material

A Branch and Bound Algorithm

Algorithm 1 shows the structure of our approach.

B Equivalent Points Bound

When multiple observations captured by a leaf in dsplit have identical features but opposite labels, then no tree,
including those that extend dsplit, can correctly classify all of these observations. The number of misclassiﬁcations
must be at least the minority label of the equivalent points.
For data set {(xn, yn)}N
m=1, we deﬁne a set of samples to be equivalent if they
1[cap(xi, sm) =
have exactly the same feature values, i.e., (xi, yi) and (xj, yj) are equivalent if 1
M
cap(xj, sm)] = 1. Note that a data set consists of multiple sets of equivalent points; let {eu}U
u=1 enumerate
these sets. For each observation xi, it belongs to a equivalent points set eu. We denote the fraction of data with
the minority label in set eu as θ(eu), e.g., let eu = {xn : ∀m ∈ [M ], 1[cap(xn, sm) = cap(xi, sm)]}, and
let qu be the minority class label among points in eu, then

n=1 and a set of features {sm}M

(cid:80)M

m=1

θ(eu) =

1
N

N
(cid:88)

n=1

1[xn ∈ eu] 1[yn = qu].

(9)

We can combine the equivalent points bound with other bounds to get a tighter lower bound on the objective
function. As the experimental results demonstrate in §4, there is sometimes a substantial reduction of the
search space after incorporating the equivalent points bound. We propose a general equivalent points bound
in Proposition B.1. We incorporate it into our framework by proposing the speciﬁc equivalent points bound in
Theorem B.2.
Proposition B.1 (General equivalent points bound). Let d = (dun, δun, dsplit, δsplit, K, H) be a tree, then
R(d, x, y) ≥ (cid:80)U

u=1 θ(eu) + λH.

Algorithm 1 Branch-and-bound for learning optimal decision trees.

m=1, training data (x, y) = {(xn, yn)}N

Input: Objective function R(d, x, y), objective lower bound b(dun, x, y), set of features
n=1, initial best known tree d0 with objec-
S = {sm}M
tive R0 = R(d0, x, y); d0 could be obtained as output from another (approximate) algorithm,
otherwise, (d0, R0) = (null, 1) provides reasonable default values. The initial value of δsplit is the
majority label of the whole dataset.
Output: Provably optimal decision tree d∗ with minimum objective R∗

(dc, Rc) ← (d0, R0)
Q ← queue( [ ((), (), (), δsplit, 0, 0) ] )
while Q not empty do

d = (dun, δun, dsplit, δsplit, K, H) ← Q.pop( )
if b(dun, x, y) < Rc then
R ← R(d, x, y)
if R < Rc then

(dc, Rc) ← (d, R)

(cid:46) Initialize best tree and objective
(cid:46) Initialize queue with empty tree
(cid:46) Stop when queue is empty
(cid:46) Remove tree d from the queue
(cid:46) Bound: Apply Theorem 3.1
(cid:46) Compute objective of tree d
(cid:46) Update best tree and objective

end if
for every possible combination of features to split dsplit do

(cid:46) Branch: Enqueue dun’s children

split dsplit and get new leaves dnew
for each possible subset d(cid:48)
un = dun ∪ (dnew \ d(cid:48)
d(cid:48)
un, δ(cid:48)
Q.push( (d(cid:48)

split of dnew do
split)
split, δ(cid:48)

un, d(cid:48)

split, K (cid:48), H (cid:48)) )

end for

end for

end if
end while
(d∗, R∗) ← (dc, Rc)

(cid:46) Identify provably optimal solution

10

Recall that in our lower bound b(dun, x, y) in (5), we leave out the misclassiﬁcation errors of leaves we are
going to split (cid:96)0(dsplit, δsplit, x, y) from the objective R(d, x, y). Incorporating the equivalent points bound in
Theorem B.2, we obtain a tighter bound on our objective because we now have a tighter lower bound on the
misclassiﬁcation errors of leaves we are going to split, 0 ≤ b0(dsplit, x, y) ≤ (cid:96)0(dsplit, δsplit, x, y).
Theorem B.2 (Equivalent points bound). Let d be a tree with leaves dun, dsplit and lower bound b(dun, x, y),
then for any tree d(cid:48) ∈ σ(d) whose preﬁx d(cid:48)

un ⊇ dun,

R(d(cid:48), x, y) ≥ b(dun, x, y) + b0(dsplit, x, y), where

b0(dsplit, x, y) =

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

cap(xn, dsplit) ∧ 1[xn ∈ eu] 1[yn = qu].

(10)

(11)

C Upper Bounds on Number of Leaves

During the branch-and-bound execution, the current best objective Rc implies an upper bound on the maximum
number of leaves for those trees we still need to consider.

Theorem C.1 (Upper bound on the number of leaves). For a dataset with M features, consider a state space of
all trees. Let L(d) be the number of leaves of tree d and let Rc be the current best objective. For all optimal
trees d∗ ∈ argmind R(d, x, y)

L(d∗) ≤ min

(cid:16)

(cid:98)Rc/λ(cid:99) , 2M (cid:17)

,

(12)

where λ is the regularization parameter.
Corollary C.2 (A priori upper bound on the number of leaves). For all optimal trees d∗ ∈ argmind R(d, x, y),
(cid:98)1/2λ(cid:99) , 2M (cid:17)
(cid:16)

L(d∗) ≤ min

(13)

.

For any particular tree d with unchanged leaves dun, we can obtain potentially tighter upper bounds on the
number of leaves for all its child trees whose unchanged leaves include dun.
Theorem C.3 (Parent-speciﬁc upper bound on the number of leaves). Let d = (dun, δun, dsplit, δsplit, K, H) be
a tree, let d(cid:48) = (d(cid:48)
un ⊇ dun, and let Rc be the
un, d(cid:48)
current best objective. If d(cid:48)

split, K (cid:48), H (cid:48)) ∈ σ(d) be any child tree such that d(cid:48)

split, δ(cid:48)
un has lower bound b(d(cid:48)

un, δ(cid:48)

H (cid:48) < min

(cid:18)

H +

un, x, y) < Rc, then
(cid:23)
(cid:22) Rc − b(dun, x, y)
λ

(cid:19)

.

, 2M

(14)

Theorem C.3 can be viewed as a generalization of the one-step lookahead bound (Lemma 3.2). This is because
we can view (14) as a bound on H (cid:48) − H, which provides an upper bound on the number of remaining splits we
may need, based on the best tree we already have evaluated.

D Upper Bounds on Number of Tree Evaluations

In this section, based on the upper bounds on the number of leaves from §C, we give corresponding upper
bounds on the number of tree evaluations made by Algorithm 1. First, in Theorem D.1, based on information
about the state of Algorithm 1’s execution, we calculate, for any given execution state, upper bounds on the
number of additional tree evaluations needed for the execution to complete. We deﬁne the number of remaining
tree evaluations as the number of trees that are currently in, or will be inserted into, the queue. We evaluate the
number of tree evaluations based on current execution information of the current best objective Rc and the trees
in the queue Q of Algorithm 1.

Theorem D.1 (Upper bound on number of remaining tree evaluations). Consider the state space of all possible
leaves formed from a set of M features, and consider Algorithm 1 at a particular instant during execution.
Denote the current best objective as Rc, the queue as Q, and the size of preﬁx dun as L(dun). Denoting the
number of remaining preﬁx evaluations as Γ(Rc, Q), the bound is:

Γ(Rc, Q) ≤

(cid:88)

f (dun)
(cid:88)

dun∈Q

k=0

(3M − L(dun))!
(3M − L(dun) − k)!

,

where

f (dun) = min

, 3M − L(dun)

(cid:19)

.

(cid:18)(cid:22) Rc − b(dun, x, y)

(cid:23)

λ

11

(15)

(16)

The corollary below is a naïve upper bound on the total number of tree evaluations during the process of
Algorithm 1’s execution. It does not use algorithm execution state to bound the size of the search space like
Theorem D.1, and it relies only on the number of features and the regularization parameter λ.
Corollary D.2 (Upper bound on the total number of tree evaluations). Deﬁne Γtot(S) to be the total number of
trees evaluated by Algorithm 1, given the state space of all possible leaves formed from a set S of M features.
For any set S of all leaves formed of M features,

Γtot(S) ≤

K
(cid:88)

k=0

3M !
(3M − k)!

, where K = min((cid:98)1/2λ(cid:99), 2M ).

E Permutation Bound

If two trees are composed of the same leaves, i.e., they contain the same conjunctions of features up to a
permutation, then they classify all the data in the same way and their child trees are also permutations of each
other. Therefore, if we already have all children from one permutation of a tree, then there is no beneﬁt to
considering child trees generated from a different permutation.
Corollary E.1 (Leaf Permutation bound). Let π be any permutation of {1, . . . , H}, Let d =
(dun, δun, dsplit, δsplit, K, H) and D = (Dun, ∆un, Dsplit, ∆split, K, H) denote trees with leaves (p1, . . . , pH )
and Dun = (pπ(1), . . . , pπ(H)), respectively, i.e., the leaves in D correspond to a permutation of the leaves
in d. Then the objective lower bounds of d and D are the same and their child trees correspond to permutations
of each other.

Therefore, if two trees have the same leaves, up to a permutation, according to Corollary E.1, either of them
can be pruned. We call this symmetry-aware pruning. In Section §E.1, we demonstrate how this helps to save
computation.

E.1 Upper bound on tree evaluations with symmetry-aware pruning

Here we give an upper bound on the total number of tree evaluations based on symmetry-aware pruning (§E).
For every subset of K leaves, there are K! leaf sets equivalent up to permutation. Thus, symmetry-aware
pruning dramatically reduces the search space by considering only one of them. This effects the execution of
Algorithm 1’s breadth-ﬁrst search. With symmetry-aware pruning, when it evaluates trees of size K, for each set
of trees equivalent up to a permutation, it keeps only a single tree.
Theorem E.2 (Upper bound on tree evaluations with symmetry-aware pruning). Consider a state space of all
trees formed from a set S of 3M leaves where M is the number of features (the 3 options correspond to having
the feature’s value be 1, having its value be 0, or not including the feature), and consider the branch-and-bound
algorithm with symmetry-aware pruning. Deﬁne Γtot(S) to be the total number of preﬁxes evaluated. For any
set S of 3M leaves,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

Nk + C(M, k) − P (M, k),

(17)

where K = min((cid:98)1/2λ(cid:99), 2M ), Nk is deﬁned in (1).

(cid:80)2n0

Proof. By Corollary C.2, K ≡ min((cid:98)1/2λ(cid:99), 2M ) gives an upper bound on the number of leaves of any
optimal tree. The algorithm begins by evaluating the empty tree, followed by M trees of depth k = 1,
(cid:1)(M − 1)n1 trees of depth k = 2. Before proceeding to length k = 3,
then N2 = (cid:80)1
we keep only N2+ C(M, 2) − P (M, 2) trees of depth k = 2, where Nk is deﬁned in (1), P (M, k) denotes
the number of k-permutations of M and C(M, k) denotes the number of k-combinations of M . Now, the
number of length k = 3 preﬁxes we evaluate is N3 + C(M, 3) − P (M, 3). Propagating this forward gives
(17).

n1=1 M × (cid:0)2n0

n0=1

n1

Pruning based on permutation symmetries thus yields signiﬁcant computational savings of (cid:80)K
k=1 P (M, k) −
C(M, k). For example, when M = 10 and K = 5, the number reduced due to symmetry-aware pruning is
about 35463. If M = 20 and K = 10, the number of evaluations is reduced by about 7.36891 × 1011.

F Similar Support Bound

Here, we present the similar support bound to deal with similar trees. Let us say we are given two trees that are
the same except that one internal node is split by a different feature, where this second feature is similar to the
ﬁrst tree’s feature. By the similar support bound, if we know that one of these trees and all its child trees are
worse (beyond a margin) than the current best tree, we can prune the other one and all of its child trees.

12

support bound). Deﬁne d = (dun, δun, dsplit, δsplit, K, H) and D =
Theorem F.1 (Similar
(Dun, ∆un, Dsplit, ∆split, K, H) to be two trees which are exactly the same but one internal node split by
different features. Let f1, f2 be the features used to split that node in d and D respectively. Let t1, t2 be the left
subtree and the right subtree under the node f1 in d, and let T1, T2 be the left subtree and the right subtree
under the node f2 in D. Denote the normalized support of data captured by only one of t1 and T1 as ω, i.e.,

ω ≡

1
N

N
(cid:88)

[¬ cap(xn, t1) ∧ cap(xn, T1) + cap(xn, t1) ∧ ¬ cap(xn, T1)].

(18)

n=1

The difference between the two trees’ objectives is bounded by ω as the following:

ω ≥ R(d, x, y) − R(D, x, y) ≥ −ω,

where R(d, x, y) is objective of d and R(D, x, y) is the objective of D. Then, we have

ω ≥ min

d†∈σ(d)

R(d†, x, y) − min

D†∈σ(Dun)

R(D†, x, y) ≥ −ω.

(19)

(20)

Proof. The difference between the objectives of d and D is maximized when one of them correctly classiﬁes
all the data corresponding to ω but the other misclassiﬁes all of them. Therefore,

ω ≥ R(d, x, y) − R(D, x, y) ≥ −ω.

Let d∗ be the best child tree of d, i.e., R(d∗, x, y) = mind†∈σ(d) R(d†, x, y), and let D(cid:48) ∈ σ(Dun)
be its counterpart which is exactly the same but one internal node split by a different feature. Because
R(D(cid:48), x, y) ≥ minD†∈σ(Dun) R(D†, x, y),

min
d†∈σ(d)

R(d†, x, y) = R(d∗, x, y) ≥ R(D(cid:48), x, y) − ω

≥ min

D†∈σ(Dun)

R(D†, x, y) − ω.

(21)

Similarly, minD†∈σ(Dun) R(D†, x, y) + ω ≥ mind†∈σ(d) R(d†, x, y).

With the similar support bound, for two trees d and D as above, if we already know D and all its child
trees cannot achieve a better tree than the current optimal one, and in particular, we assume we know that
minD†∈σ(Dun) R(D†, x, y) ≥ Rc + ω, we can then also prune d and all of its child trees, because

min
d†∈σ(d)

R(d†, x, y) ≥ min

D†∈σ(Dun)

R(D†, x, y) − ω ≥ Rc.

(22)

G Implementation

We implement a series of data structures designed to support incremental computation.

G.1 Data Structure of Leaf and Tree

First, we store bounds and intermediate results for both full trees and the individual leaves to support the
incremental computation of the lower bound and the objective. As a global statistic, we maintain the best
(minimum) observed value of the objective function and the corresponding tree. As each leaf in a tree represents
a set of clauses, each leaf stores a bit-vector representing the set of samples captured by that clause and the
prediction accuracy for those samples. From these values in the leaves, we can efﬁciently compute both the
value of the objective for an entire tree and new leaf values for children formed from splitting a leaf.

Speciﬁcally, for the data structure of leaf l, we store:

• A set of clauses deﬁning the leaf.

• A binary vector of length N (number of data points) indicating whether or not each point is captured

by the leaf.

• The number of points captured by the leaf.

• A binary vector of length M (number of features) indicating the set of dead features. In a leaf, a

feature is dead if Theorem 3.5 does not hold.

• The lower bound on the leaf misclassiﬁcation error b0(l, x, y), which is deﬁned in (11)
• The label of the leaf.

• The loss of the leaf.

13

• A boolean indicating whether the leaf is dead. A leaf is dead if Theorem 3.3 does not hold; we never

split a dead leaf.

We store additional information for entire trees:

• A set of leaves in the tree.

• The objective.

• The lower bound of the objective.

• A binary vector of length nl (number of leaves) indicating whether the leaf can be split, that is, this
vector records split leaves dsplit and unchanged leaves dun. The unchanged leaves of a tree are also
unchanged leaves in its child trees.

G.2 Queue

Second, we use a priority queue to order the exploration of the search space. The queue serves as a worklist,
with each entry in the queue corresponding to a tree. When an entry is removed from the queue, we use it to
generate child trees, incrementally computing the information for the child trees. The ordering of the worklist
represents a scheduling policy. We evaluated both structural orderings, e.g., breadth ﬁrst search and depth
ﬁrst search, and metric-based orderings, e.g., objective function, and its lower bound. Each metric produces a
different scheduling policy. We achieve the best performance in runtime and memory consumption using the
curiosity metric from CORELS [2], which is the objective’s lower bound, divided by the normalized support of
its unchanged leaves. For example, relative to using the objective, curiosity reduces runtime by a factor of two
and memory consumption by a factor of four.

G.3 Symmetry-aware Map

Third, to support symmetry-aware pruning from Corollary E.1, we introduce two symmetry aware maps – a
LeafCache and a TreeCache. The LeafCache ensures that we only compute values for a particular leave once;
the TreeCache ensures we do not create trees equivalent to those we have already explored.

A leaf is a set of clauses, each of which corresponds to an attribute and the value (0,1) of that attribute. As the
leaves of a decision tree are mutually exclusive, the data captured by each leaf is insensitive to the order of the
leaf’s clauses. We encode leaves in a canonical order (sorted by attribute indexes) and use that canonical order
as the key into the LeafCache. Each entry in the LeafCache represents all permutations of a set of clauses. We
use a Python dictionary to map these keys to the leaf and its cached values. Before creating a leaf object, we
ﬁrst check if we already have that leaf in our map. If not, we create the leaf and insert it into the map. Otherwise,
the permutation already exists, so we use the cached copy in the tree we are constructing.

Next, we implement the permutation bound (Corollary E.1) using the TreeCache. The TreeCache contains
encodings of all the trees we have evaluated. Like we did for the clauses in the LeafCache, we introduce a
canonical order over the leaves in a tree and use that as the key to the TreeCache. If our algorithm produces a
tree that is a permutation of a tree we have already evaluated, we need not evaluate it again. Before evaluating a
tree, we look it up in the cache. If it’s in the cache, we do nothing; if it is not, we compute the bounds for the
tree and insert it into the cache.

G.4 Execution

Now, we illustrate how these data structures support execution of our algorithm. We initialize the algorithm
with the current best objective Rc and tree dc. For unexplored trees in the queue, the scheduling policy selects
the next tree d to split; we keep removing elements from the queue until the queue is empty. Then, for every
possible combination of features to split dsplit, we construct a new tree d(cid:48) with incremental calculation of the
lower bound b(d(cid:48)
un, x, y) and the objective R(d(cid:48), x, y). If we achieve a better objective R(d(cid:48), x, y), i.e., less
than the current best objective Rc, we update Rc and dc. If the lower bound of the new tree d(cid:48), combined with
the equivalent points bound (Theorem B.2) and the lookahead bound (Theorem 3.2), is less than the current best
objective, then we push it into the queue. Otherwise, according to the hierarchical lower bound (Theorem 3.1),
no child of d(cid:48) could possibly have an objective better than Rc, which means we do not push d(cid:48) queue. When
there are no more trees to explore, i.e., the queue is empty, we have ﬁnished the search of the whole space and
output the (provably) optimal tree.

14

H Proof of Theorems

H.1 Proof of Theorem C.1

Proof. For an optimal tree d∗ with objective R∗,

λL(d∗) ≤ R∗ = R(d∗, x, y) = (cid:96)(d∗, x, y) + λL(d∗) ≤ Rc.

The maximum possible number of leaves for d∗ occurs when (cid:96)(d∗, x, y) is minimized; therefore this gives
bound (12).
For the rest of the proof, let H ∗ = L(d∗) be the number of leaves of d∗. If the current best tree dc has zero
misclassiﬁcation error, then

λH ∗ ≤ (cid:96)(d∗, x, y) + λH ∗ = R(d∗, x, y) ≤ Rc = R(dc, x, y) = λH,
and thus H ∗ ≤ H. If the current best tree is suboptimal, i.e., dc /∈ argmind R(d, x, y), then
λH ∗ ≤ (cid:96)(d∗, x, y) + λH ∗ = R(d∗, x, y) < Rc = R(dc, x, y) = λH,

in which case H ∗ < H, i.e., H ∗ ≤ H − 1, since H is an integer.

H.2 Proof of Theorem C.3

Proof. First, note that H (cid:48) ≥ H. Now recall that

b(dun, x, y) = (cid:96)p(dun, δun, x, y) + λH

≤ (cid:96)p(d(cid:48)

un, δ(cid:48)

un, x, y) + λH (cid:48) = b(d(cid:48)

un, x, y),

and that (cid:96)p(dun, δun, x, y) ≤ (cid:96)p(d(cid:48)

un, δ(cid:48)
un, x, y) = (cid:96)p(d(cid:48)

b(d(cid:48)

un, δ(cid:48)

un, x, y). Combining these bounds and rearranging gives
un, x, y) + λH + λ(H (cid:48) − H)
≥ (cid:96)p(dun, δun, x, y) + λH + λ(H (cid:48) − H)
= b(dun, x, y) + λ(H (cid:48) − H).

(23)

Combining (23) with b(d(cid:48)

un, x, y) < Rc gives (14).

H.3 Proof of Theorem D.1

Proof. The number of remaining tree evaluations is equal to the number of trees that are currently in or will be
inserted into queue Q. For any such tree with unchanged leaves dun, Theorem C.3 gives an upper bound on the
number of leaves of a tree with unchanged leaves d(cid:48)

un that contains dun:

L(d(cid:48)

un) ≤ min

(cid:18)

L(dun) +

(cid:22) Rc − b(dun, x, y)
λ

(cid:23)

(cid:19)

, 2M

≡ U (dun).

This gives an upper bound on the remaining tree evaluations:

Γ(Rc, Q) ≤

(cid:88)

U (dun)−L(dun)
(cid:88)

P (3M − L(dun), k)

(24)

dun∈Q

k=0

(cid:88)

f (dun)
(cid:88)

=

dun∈Q

k=0

(3M − L(dun))!
(3M − L(dun) − k)!

,

where P (m, k) denotes the number of k-permutations of m.

H.4 Proof of Proposition D.2

Proof. By Corollary C.2, K ≡ min((cid:98)1/2λ(cid:99), 2M ) gives an upper bound on the number of leaves of any optimal
tree. Since we can think of our problem as ﬁnding the optimal selection and permutation of k out of 3M leaves,
over all k ≤ K,

Γtot(S) ≤ 1 +

K
(cid:88)

k=1

P (3M , k) =

K
(cid:88)

k=0

3M !
(3M − k)!

.

15

H.5 Proof of Theorem 3.3

, . . . , ˆy(leaf)

Proof. Let d∗ = (dun, δun, dsplit, δsplit, K, H) be an optimal
(ˆy(leaf)
split, δ(cid:48)
H ). Consider the tree d = (d(cid:48)
1
pair of sibling leaves pi → ˆy(leaf)
un = (p1, . . . , pi−1, pi+2, . . . , pH , pj) and δ(cid:48)
d(cid:48)
1
When d misclassiﬁes half of the data captured by pi, pi+1, while d∗ correctly classiﬁes them all, the difference
between d and d∗ would be maximized, which provides an upper bound:

un, d(cid:48)
and adding their parent leaf pj → ˆy(leaf)
).

tree with leaves (p1, . . . , pH ) and labels
split, K (cid:48), H (cid:48)) derived from d∗ by deleting a
, therefore

i+2 , . . . , ˆy(leaf)

, pi+1 → ˆy(leaf)
i+1

un = (ˆy(leaf)

H , ˆy(leaf)

i−1 , ˆy(leaf)

, . . . , ˆy(leaf)

un, δ(cid:48)

j

j

i

R(d, x, y) = (cid:96)(d, x, y) + λ(H − 1)

≤ (cid:96)(d∗, x, y) + supp(pi, x) + supp(pi+1, x)

−

1
2

[supp(pi, x) + supp(pi+1, x)] + λ(H − 1)

= R(d∗, x, y) +

1
2

[supp(pi, x) + supp(pi+1, x)] − λ

= R∗ +

1
2

[supp(pi, x) + supp(pi+1, x)] − λ

(25)

where supp(pi, x), supp(pi, x) is the normalized support of pi, pi+1, deﬁned in (3), and the regularization
‘bonus’ comes from the fact that d∗ has one more leaf than d.
Because d∗ is the optimal tree, we have R∗ ≤ R(d, x, y), which combined with (25) leads to (6). Therefore,
for each child leaf pair pk, pk+1 of a split, the sum of normalized supports of pk, pk+1 should be no less than
twice the regularization parameter, i.e., 2λ.

H.6 Proof of Theorem 3.4

Proof. Let d = (d(cid:48)
split, K (cid:48), H (cid:48)) be the tree derived from d∗ by deleting a pair of leaves, pi and
pi+1, and adding the their parent leaf, pj. The discrepancy between d∗ and d is the discrepancy between
(pi, pi+1) and pj: (cid:96)(d, x, y) − (cid:96)(d∗, x, y) = ai, where ai is deﬁned in (7). Therefore,

split, δ(cid:48)

un, d(cid:48)

un, δ(cid:48)

R(d, x, y) = (cid:96)(d, x, y) + λ(K − 1) = (cid:96)(d∗, x, y) + ai + λ(K − 1)

= R(d∗, x, y) + ai − λ = R∗ + ai − λ.

This combined with R∗ ≤ R(d, x, y) leads to λ ≤ ai.

H.7 Proof of Theorem 3.5

un, δ(cid:48)

un, d(cid:48)
and pi+1 with label ˆy(leaf)

split, K (cid:48), H (cid:48)) be the tree derived from d∗ by deleting a pair of leaves, pi with
Proof. Let d = (d(cid:48)
label ˆy(leaf)
i+1 , and adding the their parent leaf pj with label ˆy(leaf)
. The discrepancy
between d∗ and d is the discrepancy between pi, pi+1 and pj: (cid:96)(d, x, y) − (cid:96)(d∗, x, y) = ai, where we
deﬁned ai in (7). According to Theorem 3.4, λ ≤ ai and

split, δ(cid:48)

j

i

λ ≤

1
N

N
(cid:88)

{cap(xn, pi) ∧ 1[ˆy(leaf)

i

= yn]

n=1

+ cap(xn, pi+1) ∧ 1[ˆy(leaf)
− cap(xn, pj) ∧ 1[ˆy(leaf)
For any leaf j and its two child leaves i, i + 1, we always have

j

i+1 = yn]

= yn]}.

(24)

cap(xn, pi) ∧ 1[ˆy(leaf)

i

= yn] ≤

N
(cid:88)

cap(xn, pj) ∧ 1[ˆy(leaf)

j

= yn],

N
(cid:88)

n=1

N
(cid:88)

n=1

cap(xn, pi+1) ∧ 1[ˆy(leaf)

i+1 = yn] ≤

n=1

N
(cid:88)

n=1

cap(xn, pj) ∧ 1[ˆy(leaf)

j

= yn]

= yn] and ai ≤ 1
N

(cid:80)N

n=1 cap(xn, pi+1) ∧

which indicates that ai ≤ 1
N
1[ˆy(leaf)
i+1 = yn]. Therefore,

(cid:80)N

n=1 cap(xn, pi) ∧ 1[ˆy(leaf)

i

λ ≤

λ ≤

1
N

1
N

N
(cid:88)

n=1

N
(cid:88)

n=1

cap(xn, pi) ∧ 1[ˆy(leaf)

i

= yn],

cap(xn, pi+1) ∧ 1[ˆy(leaf)

i+1 = yn].

16

H.8 Proof of Proposition B.1

Proof. Recall that the objective is R(d, x, y) = (cid:96)(d, x, y) + λH, where the misclassiﬁcation error (cid:96)(d, x, y)
is given by

(cid:96)(d, x, y) =

1
N

N
(cid:88)

K
(cid:88)

n=1

k=1

cap(xn, pk) ∧ 1[ˆy(leaf)

k

(cid:54)= yn].

Any particular tree uses a speciﬁc leaf, and therefore a single class label, to classify all points within a set of
equivalent points. Thus, for a set of equivalent points u, the tree d correctly classiﬁes either points that have
the majority class label, or points that have the minority class label. It follows that d misclassiﬁes a number of
points in u at least as great as the number of points with the minority class label. To translate this into a lower
bound on (cid:96)(d, x, y), we ﬁrst sum over all sets of equivalent points, and then for each such set, count differences
between class labels and the minority class label of the set, instead of counting mistakes:

(cid:96)(d, x, y)

=

≥

1
N

1
N

U
(cid:88)

N
(cid:88)

K
(cid:88)

u=1

n=1

k=1

U
(cid:88)

N
(cid:88)

K
(cid:88)

u=1

n=1

k=1

cap(xn, pk) ∧ 1[ˆy(leaf)

k

(cid:54)= yn] ∧ 1[xn ∈ eu]

cap(xn, pk) ∧ 1[qu = yn] ∧ 1[xn ∈ eu].

Next, because every datum must be captured by a leaf in the tree d, (cid:80)K

k=1 cap(xn, pk) = 1.

(cid:96)(d, x, y) ≥

1
N

U
(cid:88)

N
(cid:88)

u=1

n=1

1[xn ∈ eu] 1[yn = qu] =

U
(cid:88)

u=1

θ(eu),

where the ﬁnal equality applies the deﬁnition of θ(eu) in (9). Therefore, R(d, x, y) = (cid:96)(d, x, y) + λK
≥ (cid:80)U

u=1 θ(eu) + λK.

I Ablation Experiments

We evaluate how much each of our bounds contributes to OSDT’s performance and what effect the scheduling
metric has on execution. Table 2 provides experimental statistics of total execution time, time to optimum, total
number of trees evaluated, number of trees evaluated to optimum, and memory consumption on the recidivism
data set. The ﬁrst row is the full OSDT implementation, and the others are variants, each of which removes a
speciﬁc bound. While all the optimizations reduce the search space, the lookahead and equivalent points bounds
are, by far, the most signiﬁcant, reducing time to optimum by at least two orders of magnitude and reducing
memory consumption by more than one order of magnitude. In our experiment, although the scheduling policy
has a smaller effect, it is still signiﬁcant – curiosity is a factor of two faster than the objective function and
consumes 25% of the memory consumed when using the objective function for scheduling. All other scheduling
policies, i.e., the lower bound and the entropy, are signiﬁcantly worse.

J Regularized BinOCT

Since BinOCT always produces complete binary trees of given depth, we add a regularization term to the
objective function of BinOCT. In this way, regularized BinOCT (RBinOCT) can generate the same trees as
OSDT. Following the notation of [20], we provide the formulation of regularized BinOCT:

min

(cid:88)

l,c

el,c + λ

(cid:88)

l

αl s.t.

17

(25)

Per-bound performance improvement (ProPublica data set)

Algorithm variant
All bounds
No support bound
No incremental accuracy bound
No accuracy bound
No lookahead bound
No equivalent points bound

Algorithm variant
All bounds
No support bound
No incremental accuracy bound
No accuracy bound
No lookahead bound
No equivalent points bound

Total time
(s)
14.70
17.11
30.16
31.83
31721
>12475
Total #trees
evaluated
232402
279763
546402
475691
284651888
>77000000

Slow-
down
—
1.16×
2.05×
2.17×
2157.89×
>848×

#trees
to optimum
16001
18880
21686
19676
3078274
—

Time to
optimum (s)
1.01
1.09
1.13
1.23
187.18
—
Mem
(GB)
.08
.08
.08
.09
10
>64

Table 2: Per-bound performance improvement, for the ProPublica data set (λ = 0.005, cold start,
using curiosity). The columns report the total execution time, time to optimum, total number of trees
evaluated, number of trees evaluated to optimum, and memory consumption. The ﬁrst row shows
our algorithm with all bounds; subsequent rows show variants that each remove a speciﬁc bound
(one bound at a time, not cumulative). All rows except the last one represent a complete execution,
i.e., until the queue is empty. For the last row (‘No equivalent points bound’), the algorithm was
terminated after running out of the memory (about ∼64GB RAM).

∀n

∀r

∀l

(cid:88)

f
(cid:88)

fn,f = 1

lr,l = 1

l
(cid:88)

c

pl,c = 1

∀n, f, b ∈ bin(f ) M · fn,f +

(cid:88)

(cid:88)

(cid:88)

lr,l +

M · tn,t −

(cid:88)

M ≤ M

∀n, f, b ∈ bin(f ) M (cid:48) · fn,f +

r∈lr(b)
(cid:88)

l∈ll(n)
(cid:88)

t∈tl(b)
(cid:88)

lr,l −

t∈tl(b)
M (cid:48) · tn,t ≤ M (cid:48)

∀n, f M (cid:48)(cid:48) · fn,f +

(cid:88)

r∈rr(b)
(cid:88)

l∈rl(n)

t∈tl(b)

lr,l +

(cid:88)

(cid:88)

lr,l ≤ M ”

maxt(f )<f (r)

l∈ll(n)

f (r)<mint(f )

l∈rl(n)

∀ l, c

(cid:88)

r:Cr =c

lr,l − M (cid:48)(cid:48)(cid:48) · pl ≤ el,c

(cid:88)

∀ l

r

lr,l ≤ R · αl,

(26)

where 1 ≤ n ≤ N , 1 ≤ f ≤ F , 1 ≤ r ≤ R, 1 ≤ l ≤ L, 1 ≤ c ≤ C. Variables αl, fn,f , tn,t, pl,c are binary,
and el,c and lr,l are continuous (see Table 3). Compared with BinOCT, we add a penalization term λ (cid:80)
l αl to
the objective function (25) and a new constraint (26), where λ is the same as that of OSDT, and αl = 1 if leaf l
is not empty and αl = 0 if leaf l is empty. All the rest of the constraints are the same as those of BinOCT. We
use the same notation as in the original BinOCT formulation [20].

Figure 7 shows the trees generated by regularized BinOCT and OSDT when using the same regularization
parameter λ = 0.007. Although the two algorithms produce the same optimal trees, regularized BinOCT is
much slower than OSDT. In our experiment, it took only 3.390 seconds to run the OSDT algorithm to optimality,
while the regularized BinOCT algorithm had not ﬁnished running after 1 hour.

In Figure 8, we provide execution traces of OSDT and RBinOCT. OSDT converges much faster than RBinOCT.
For some datasets, i.e., FICO and Monk1, the total execution time of RBinOCT was several times longer than
that of OSDT.

18

Notation
n
l
r
f
c
bin(f )
lr(b)
ur(b)
tl(b)
ll(n)
rl(n)
K
N = 2K − 1
L = 2K
F
C
R
T
Tf
Tmax
V f
r
Cr
mint(f )
maxt(f )
M
fn,f
tn,t
pl,c
αl
el,c
lr,l
λ

Type
index
index
index
index
index
set
set
set
set
set
set
constant
constant
constant
constant
constant
constant
constant
constant
constant
constant
constant
constant
constant
constant
binary
binary
binary
binary
continuous
continuous
continuous

Deﬁnition
internal (non-leaf) node in the tree, 1 ≤ n ≤ N
leaf of the tree, 1 ≤ l ≤ L
row in the training data, 1 ≤ r ≤ R
feature in the training data, 1 ≤ f ≤ F
class in the training data, 1 ≤ c ≤ C
feature f ’s binary encoding ranges
rows with values in b’s lower range, b ∈ bin(f )
rows with values in b’s upper range
tn,t variables for b’s range
node n’s leaves under the left branch
node n’s leaves under the right branch
the tree’s depth
the number of internal nodes (not leaves)
the number of leaf nodes
the number of features
the number of classes
the number of training data rows
the total number of threshold values
the number of threshold values for feature f
maximum of Tf over all features f
feature f ’s value in training data row r
class value in training data row r
feature f ’s minimum threshold value
feature f ’s maximum threshold value
minimized big-M value
node n’s selected feature f
node n’s selected threshold t
leaf l’s selected prediction class c
αl = 1 if leaf l is not empty
error for rows with class c in leaf l
row r reaches leaf l
the regularization parameter

Table 3: Summary of the notations used in RBinOCT.

(a) RBinOCT
(accuracy: 66.223%)

(b) OSDT
(accuracy: 66.223%)

Figure 7: The decision trees generated by Regularized BinOCT and OSDT on the Monk1 dataset.
The two trees are exactly the same, but regularized BinOCT is much slower in producing this tree.

K CART

We provide the trees generated by CART on COMPAS, Tic-Tac-Toe, and Monk1 datasets. With the same
number of leaves as their OSDT counterparts (Figure 4, Figure 6, Figure 5), these CART trees perform much
worse than the OSDT ones.

19

priors=0priors>3age<26NoYesYesNopriors=0priors>3age<26NoYesYesNoFigure 8: Execution traces of OSDT and regularized BinOCT. OSDT converges much more quickly
than RBinOCT.

L Cross-validation Experiments

The difference between training and test error is probabilistic and depends on the number of observations in
both training and test sets, as well as the complexity of the model class. The best learning-theoretic bound on
test error occurs when the training error is minimized for each model class, which in our case, is the maximum
number of leaves in the tree (the level of sparsity). By adjusting the regularization parameter throughout its full

20

0100200300400500600Time0.00.10.20.30.4ValueExecution Traces of OSDT, RBinOCT and CART (compas Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.00.10.20.30.40.5ValueExecution Traces of OSDT, RBinOCT and CART (fico Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.000.050.100.150.200.250.300.35ValueExecution Traces of OSDT, RBinOCT and CART (tictactoe Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.000.050.100.150.200.250.30ValueExecution Traces of OSDT, RBinOCT and CART (car Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.00.10.20.30.40.5ValueExecution Traces of OSDT, RBinOCT and CART (monk1 Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.000.050.100.150.200.250.300.350.40ValueExecution Traces of OSDT, RBinOCT and CART (monk2 Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART0100200300400500600Time0.00.10.20.30.40.5ValueExecution Traces of OSDT, RBinOCT and CART (monk3 Dataset)LowerBound-OSDTObjective-OSDTObjective-RBinOCTLowerBound-RBinOCTObjective-CART(a) COMPAS
(accuracy: 66.135%)

(b) MONK1
(accuracy: 91.935%)

(c) Tic-Tac-Toe
(accuracy: 76.513%)

Figure 9: Decision trees generated by CART on COMPAS, Tic-Tac-Toe, and Monk1 datasets. They
are all inferior to these trees produced by OSDT as shown in Section 4.

range, OSDT will ﬁnd the most accurate tree for each given sparsity level. Figures 10-16 show the training
and test results for each of the datasets and for each fold. As indicated by theory, higher training accuacy for
the same level of sparsity tends to yield higher test accuracy in general, but not always. There are some cases,
like the car dataset, where OSDT’s almost-uniformly-higher training accuracy leads to higher test accuracy,
and other cases where all methods perform the same. In the case where all methods perform the same, OSDT
provides a certiﬁcate of optimality showing that no better training performance is possible for the same level of
sparsity.

21

priors>3age<26priors:2-3NoNopriors:2-3NoYesYesjacket=redhead=roundbody=roundjacket=greenholding=balloonYesNobody=squareNoYesNobody=roundNoYesYesmiddle-middle=obottom-left=otop-right=o11top-left=obottom-right=o1bottom-middle=o10middle-left=o100Figure 10: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on COMPAS
dataset. Horizontal lines indicate the accuracy of the best OSDT tree in training.

22

02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.600.650.700.750.800.850.90AccuracyAccuracy of compas Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 11: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on FICO dataset.
Horizontal lines indicate the accuracy of the best OSDT tree in training.

23

048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test048121620242832Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of fico Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 12: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on Tic-Tac-Toe
dataset. Horizontal lines indicate the accuracy of the best OSDT tree in training.

24

02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.95AccuracyAccuracy of tictactoe Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 13: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on car dataset.
Horizontal lines indicate the accuracy of the best OSDT tree in training.

25

02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of car Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 14: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on Monk1 dataset.
Horizontal lines indicate the accuracy of the best OSDT tree in training.

26

02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.750.800.850.900.951.00AccuracyAccuracy of monk1 Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 15: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on Monk2 dataset.
Horizontal lines indicate the accuracy of the best OSDT tree in training.

27

02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.650.700.750.800.850.900.95AccuracyAccuracy of monk2 Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-TestFigure 16: 10-fold cross-validation experiment results of OSDT, CART, BinOCT on Monk3 dataset.
Horizontal lines indicate the accuracy of the best OSDT tree in training.

28

02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 1)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 2)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 3)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 4)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 5)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 6)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 7)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 8)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 9)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test02468101214161820Number of Leaves0.700.750.800.850.900.951.00AccuracyAccuracy of monk3 Dataset (Fold 10)OSDT-TrainOSDT-TestCART-TrainCART-TestBinOCT-TrainBinOCT-Test