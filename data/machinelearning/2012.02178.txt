1
2
0
2

t
c
O
3
2

]
I

A
.
s
c
[

2
v
8
7
1
2
0
.
2
1
0
2
:
v
i
X
r
a

Steady-State Planning in Expected Reward Multichain MDPs

Steady-State Planning in Expected Reward Multichain
MDPs

George K. Atia
Department of Electrical and Computer Engineering
Department of Computer Science
University of Central Florida, FL 32816, USA

Andre Beckus
Air Force Research Laboratory, NY 13441, USA

Ismail Alkhouri
Department of Electrical and Computer Engineering
University of Central Florida, FL 32816, USA

Alvaro Velasquez
Air Force Research Laboratory, NY 13441, USA

george.atia@ucf.edu

andre.beckus@us.af.mil

ialkhouri@knights.ucf.edu

alvaro.velasquez.1@us.af.mil

Abstract

The planning domain has experienced increased interest in the formal synthesis of
decision-making policies. This formal synthesis typically entails ﬁnding a policy which
satisﬁes formal speciﬁcations in the form of some well-deﬁned logic. While many such logics
have been proposed with varying degrees of expressiveness and complexity in their capacity
to capture desirable agent behavior, their value is limited when deriving decision-making
policies which satisfy certain types of asymptotic behavior in general system models. In
particular, we are interested in specifying constraints on the steady-state behavior of an
agent, which captures the proportion of time an agent spends in each state as it interacts for
an indeﬁnite period of time with its environment. This is sometimes called the average or
expected behavior of the agent and the associated planning problem is faced with signiﬁcant
challenges unless strong restrictions are imposed on the underlying model in terms of the
connectivity of its graph structure. In this paper, we explore this steady-state planning
problem that consists of deriving a decision-making policy for an agent such that constraints
on its steady-state behavior are satisﬁed. A linear programming solution for the general case
of multichain Markov Decision Processes (MDPs) is proposed and we prove that optimal
solutions to the proposed programs yield stationary policies with rigorous guarantees of
behavior.

1. Introduction

The proliferation and mass adoption of automated solutions in recent years has led to an
increased concern in the veriﬁcation, validation, and trust of the prescribed agent behavior
(Schwarting, Alonso-Mora, & Rus, 2018). This motivates the need for traditional techniques
which can yield guarantees of behavior. The study of such techniques has largely been
the focus of areas such as formal synthesis and planning, where it is common to derive
decision-making policies for agents acting in an environment such that some given formal
speciﬁcation is satisﬁed. The majority of prior art in this area entails the coupling of
some formal logic with the model of agent-environment dynamics in order to ﬁnd optimal

1

 
 
 
 
 
 
Atia et al.

policies which satisfy speciﬁcations expressed in said logic. Examples include planning
with Linear Temporal Logic (LTL) (Guo & Zavlanos, 2018), probabilistic LTL (PLTL)
(Kwiatkowska & Parker, 2013), LTL over ﬁnite traces (LTLf ) (Camacho & McIlraith, 2019),
Linear Dynamic Logic (LDLf ) (Brafman & De Giacomo, 2019), Computation Tree Logic
(CTL) (Pistore, Bettin, & Traverso, 2014), probabilistic CTL (PCTL) (Song, Feng, &
Zhang, 2015), Signal Temporal Logic (STL) (Lindemann & Dimarogonas, 2017), Chance-
Constrained Temporal Logic (C2TL) (Jha, Raman, Sadigh, & Seshia, 2018), Continuous
Stochastic Logic (CSL) (Ayala, Andersson, & Belta, 2014), µ-Calculus (De Giacomo, Felli,
Patrizi, & Sardina, 2010), Metric Temporal Logic (MTL) (Zhou, Maity, & Baras, 2016),
and logic fragments, such as the Rank-1 Generalized Reactivity (GR[1]) formulas of LTL
(Wongpiromsarn, Topcu, Ozay, Xu, & Murray, 2011). Formal multi-agent planning has
also been explored using Dynamic Epistemic Logic (Engesser, Bolander, & Nebel, 2017)
and Alternating-time Temporal Logic (ATL) (Jamroga, 2004).

The use of the foregoing logics has facilitated the growth of solutions to the aforemen-
tioned planning problems and are a good conduit for verifying, explaining, and yielding
provably correct agent behavior and, consequently, establishing a measure of trust. How-
ever, these logics are either insuﬃcient to reason about the asymptotic behavior that is
captured by the steady-state distribution of the agent as it follows some decision-making
policy, or existing solutions to the corresponding planning problems make strong assump-
tions on the underlying model of the system. Solutions to these challenges have gained
Indeed, there has been increased interest in what we refer to
traction in recent years.
as the steady-state planning problem of computing decision-making policies that satisfy
constraints on the resulting steady-state behavior. In particular, progress has been made
in easing the restrictions required on the agent-environment dynamics model, usually ex-
pressed in the form of a Markov Decision Process (MDP), in order to derive a solution
policy. In this paper, we advance the state-of-the-art in steady-state planning by establish-
ing the ﬁrst solution to steady-state planning in multichain MDPs such that the resulting
stationary policy satisﬁes constraints imposed on the steady-state distribution of the agent.
Our approach also dissolves assumptions of ergodicity or recurrence of the underlying MDP
which are often made in the literature when reasoning about steady-state distributions.

Steady-state planning has applications in several areas, such as deriving maintenance
plans for various systems, including aircraft maintenance, where the asymptotic failure rate
of components must be kept below some small threshold (Boussemart & Limnios, 2004;
Boussemart, Limnios, & Fillion, 2002). Optimal routing problems for communication net-
works have also been proposed in which data throughput must be maximized subject to
constraints on average delay and packet drop metrics (Lazar, 1983). This includes con-
straints on steady-state network behavior, which include steady-state network frequency
and steady-state phase or timing errors (Skwirzynski, 1981). There is also the potential
of leveraging solutions in the steady-state planning problem space to the design of intelli-
gent space satellites. Indeed, this is an area where the steady-state distribution of debris
following some orbit can be computed to reason about the probability of a satellite collid-
ing with said debris. Such information has been used to determine human-driven control
policies for tasks such as debris mitigation or debris removal (Tian, 2019), sometimes via
remote-controlled robots (Baiocchi, 2010) that are amenable to automated approaches.

2

Steady-State Planning in Expected Reward Multichain MDPs

The steady-state planning problem has been studied under various names, including
steady-state control (Akshay, Bertrand, Haddad, & H´elou¨et, 2013), average- or expected-
reward constrained MDPs (Altman, 1999), and steady-state policy synthesis (Velasquez,
2019). As pointed out by Altman, Boularouk, and Josselin (2019), solutions to this prob-
lem often require strong assumptions on the ergodicity of the underlying MDP. These as-
sumptions facilitate the search for eﬃcient algorithms by leveraging the one-to-one corre-
spondence between the optimality of solutions to various mathematical programs and the
optimality of policies derived thereof. This has been studied at length in the works of
Derman (1970), Kallenberg (1983), Puterman (1994), and Altman (1999), who have de-
rived mathematical programs for discounted, total, and expected reward formulations of
constrained MDPs. In particular, the work of Kallenberg laid the foundation for Marko-
vian control within the context of multichain constrained MDPs. However, it was noted
that deriving optimal policies for the expected-reward formulation was intractable by their
approach and there was no guarantee of agent behavior in terms of satisfying steady-state
constraints.

Summary of contributions. We make four main contributions. First, we introduce
the Steady-State Policy Synthesis (SSPS) problem of ﬁnding a policy from a predeﬁned
subset of stationary policies in a multichain MDP that maximizes an expected reward
signal while enforcing asymptotic behavior that is correct-by-construction (Nilsson, Hussien,
Balkan, Chen, Ames, Grizzle, Ozay, Peng, & Tabuada, 2015) – in the sense that our policies
yield provably correct behavior that satisﬁes the imposed speciﬁcations on the steady-state
distribution of the Markov chain induced by said policies. Our framework generalizes the
steady-state planning problems studied by Akshay et al. (2013) and Velasquez (2019), as
we do not impose any restrictions on the underlying MDP. In particular, we dispense with
the strong assumption made by Akshay et al. (2013) about the ergodicity of the MDP,
according to which every deterministic policy necessarily induces an ergodic Markov chain
(i.e., one that is recurrent and aperiodic). In sharp contrast to the work of Velasquez (2019),
we do not restrict our search to stochastic policies that induce an irreducible Markov chain
(i.e., one in which all states form one communicating class). In general, such a chain may
not even exist – normally, many states in a given MDP are inevitably transient. Our search
space consists of subsets of the stationary policies that we term edge- or class-preserving,
which, apart from a transient phase, restrict the long-term play in the terminal components
of the given MDP. We introduce two distinct notions for class preservation that yield policies
with diﬀerent characteristics. These notions will be made precise in Section 4.

As our second contribution, we develop a scalable approach to synthesize policies that
provably meet said asymptotic speciﬁcations through novel linear programming formula-
tions. While a tractable solution to the SSPS problem has heretofore remained elusive and
existing solutions require an enormous amount of calculations with no provable guarantees
(Kallenberg, 1983), two key ideas underlie our ability to tackle the associated combinato-
rial diﬃculties. The ﬁrst idea is the aforementioned restriction of the domain to edge- or
class-preserving policies, which can be provably obtained from solutions to simple linear
programs (LPs). The second idea is to encode constraints on the limiting distributions of
the corresponding Markov chains in formulated LPs, whose solutions yield optimal policies
maximizing the expected average reward while meeting desired asymptotic speciﬁcations on

3

Atia et al.

the limit points of the expected state-action frequencies. These LPs are crafted to capture
designated state classiﬁcations, absorption probabilities in closed communicating compo-
nents, and recurrence constraints within such components, along with the steady-state
speciﬁcations.

Our third contribution lies in deriving key theoretical results establishing provable per-
formance and behavior guarantees for the derived policies. Contracting or transient MDP
models that use the expected total reward as the optimality criterion are commonplace in
constrained MDPs since optimal stationary policies with regard to this criterion can always
be found via mathematical programming in view of a well-established one-to-one correspon-
dence between stationary policies and feasible solutions to such programs (Altman, 1998;
Feinberg, 2000; Wu & Durfee, 2010; Petrik & Zilberstein, 2009). The notoriously more
diﬃcult and equally important expected average reward criterion is much less understood
considering that such correspondence ceases to exist for general multichain MDPs. In this
paper, we tap into this long-standing dilemma and establish such one-to-one correspondence
for classes of stationary policies that are edge- or class-preserving. Theorems 1, 2, 3 and 4
establish the correctness of linear programs yielding optimal policies from said classes. The
proof of these theorems rest on few intermediate results. In particular, Lemma 3 character-
izes the Markov chains induced by the policies of interest, while Lemma 7 establishes the
feasibility of the steady-state distributions induced by these policies. Lemma 6 gives a suf-
ﬁcient condition for the existence of a one-to-one correspondence between feasible solutions
to the linear programs and the stationary policies derived from these solutions. Theorem 5
establishes an existence condition of policies found on a more relaxed notion of class preser-
vation, which inspires a constructive approach in Algorithm 1 to compute such policies.
Theorem 8 gives a generic suﬃcient condition for the existence of an optimal stationary
policy meeting the desired speciﬁcations beyond class-preserving ones.

As our fourth contribution, we introduce an alternative type of speciﬁcations applicable
in transient states. By augmenting our LPs with appropriate constraints, the synthesized
policies provably meet speciﬁcations on the expected number of visitations to transient
states simultaneously with the foregoing steady-state speciﬁcations on the asymptotic fre-
quency with which recurrent states are visited (Proposition 1).

We verify the theoretical ﬁndings of our work using a comprehensive set of numerical
experiments performed in various environments. The results demonstrate the correctness
of the proposed LPs in yielding policies with provably correct behavior and the scalability
of the proposed solutions to large problem sizes.

This article brings in and substantially extends the scope of our recent work (Atia
et al., 2020), which considered policy synthesis over edge-preserving policies. Such policies
constitute only a small subset of the policies considered herein. A particularly appealing
characteristic of the newly introduced policies is their greater ability to avert MDP transi-
tions of low return without violating the asymptotic constraints. In turn, they yield larger
expected rewards relative to their edge-preserving counterparts – in some cases, we show
that this gain can be substantial. Further, we derive general characterizations of optimality
over a larger class of policies obtained in terms of the MDP reward signal. In addition,
this article advances the aforementioned form of transient speciﬁcations that a policy can
provably meet together with the steady-state ones. We provide a complete presentation
of the steady-state planning problem through linear programming formulations over diﬀer-

4

Steady-State Planning in Expected Reward Multichain MDPs

ent families of policies, mathematical analyses establishing correctness of such formulations
with optimality guarantees, and a comprehensive set of numerical experiments in diverse
environments to support the theoretical ﬁndings.

To the best of our knowledge, this work is the ﬁrst to allow synthesis of stationary

policies with provably correct steady-state behavior in general multichain MDPs.

Organization: The paper is organized as follows. Notation and preliminaries are covered
in Section 2. Related work in steady-state planning is summarized in Section 3. The SSPS
problem is formalized in Section 4. We describe our linear programming approach and
present the results of our theoretical analysis in Section 5. Transient speciﬁcations and
extensions to a larger class of policies are presented in Section 6. Numerical experiments
are presented in Section 7 to validate our approach and demonstrate its scalability to large
problems. Concluding remarks are presented in Section 8.
In Appendix A, we present
statements and proof of technical lemmas. The proof of the main results are deferred to
Appendix B.

2. Preliminaries and Notation

We introduce some notation and preliminary deﬁnitions used throughout the paper. For
a matrix A, aij and A(i, j) are used interchangeably to denote the element in its ith row
and jth column. The vectors e and es denote the vectors (of appropriate dimension) of all
ones, and all zeros except for the sth entry, respectively. Given a vector x and index set
V , the vector xV is the vector with entries xv, v ∈ V , where xv is the entry corresponding
to index v. By |S|, we denote the cardinality of a set S. For an integer n > 0, the set
[n] := {1, . . . , n}, and A \ B denotes the set diﬀerence of sets A and B. The symbols ∃
and ∃! mean “there exists” and “there exists a unique”, respectively, and (cid:62) is the transpose
operator.

Deﬁnition 1 (Markov chain). A Markov chain is a stochastic model given by a tuple
M = (S, T, β), where S is the state space, T the transition function T : S × S → [0, 1] with
T (s(cid:48)|s) denoting the probability of transitioning from state s to state s(cid:48), and β : S → [0, 1]
the initial state distribution. With slight abuse of notation, the transition function can also
be thought of as a matrix T ∈ [0, 1]|S|×|S|, where T (s, s(cid:48)) = T (s(cid:48)|s). The use of T will be
clear from the context.

Classiﬁcation of states
(Norris, 1997; Privault, 2018): Given a ﬁnite Markov chain
M = (S, T, β), we say state s(cid:48) is accessible from state s if (T t)(s, s(cid:48)) > 0, for some t > 0,
where T t is the t−step transition matrix, i.e., if there is a positive probability of transitioning
to state s(cid:48) starting from state s in some number of steps. Two states are said to communicate
if they are both accessible from each other. Communication is an equivalence relation which
partitions the Markov chain M into communicating classes such that only members of the
same class communicate with each other. A class is closed if the probability of escaping
the class is zero. A state s ∈ S is said to be transient if, starting from s, there is a non-
zero probability of never returning to s. A set of transient states is termed a transient
set. Non-transient states are called recurrent, that is, state s is recurrent if, starting from
s, the probability of returning to state s after some number of steps is one. A Markov
chain for which there is only one communicating class consisting of the entire state space is

5

Atia et al.

called irreducible, whereas a Markov chain that has a single closed communicating class and
(possibly) some transient states is termed unichain. A state is periodic with period k if any
return to state s must occur in multiples of k time steps, where k is some integer greater
than 1. An example illustrating the classiﬁcation of states in a Markov chain is shown in
Figure 1.

Transience and recurrence describe the likelihood of returning to a state conditioned on
starting from that state, regardless of the initial state distribution β. Given β, we also
deﬁne an isolated component I as a maximal set of states in M that can never be visited,
that is, βI = (cid:80)
s∈I βs = 0, where βs is the initial probability of being in state s, and I
cannot be reached from any state in S \ I, i.e., (cid:80)
s(cid:48)∈I T (s(cid:48)|s) = 0, ∀s ∈ S \ I. In Figure
1, the set of states {s3, s4} is isolated. The term ‘reachable’ refers to states that are not
isolated.

Deﬁnition 2 (Markov decision process (MDP)). An MDP is a tuple M = (S, A, T, R, β),
in which S denotes the state space, A the set of actions, T : S × A × S → [0, 1] the transition
function with T (s(cid:48)|s, a) denoting the probability of transitioning from state s to state s(cid:48) under
action a, R : S × A × S → R a reward obtained when action a is taken in state s and we
end up in state s(cid:48), and β : S → [0, 1] the initial distribution. By A(s) ⊆ A, we denote the
set of actions available in state s.

Deﬁnition 3 (Transition graph). We deﬁne the transition graph of an MDP M =
(S, A, T, R, β) as the directed graph whose vertex set is the state space S, and in which
there is a directed edge from vertex s to vertex s(cid:48) if there exists an action a ∈ A(s) such that
T (s(cid:48)|s, a) > 0. The transition graph of a Markov chain M = (S, T, β) is the directed graph
with vertex set S, and which has a directed edge from vertex s to vertex s(cid:48) if T (s(cid:48)|s) > 0.

Deﬁnition 4 (Terminal strongly connected component (TSCC)). Consider the transition
graph of a Markov chain or MDP M with state space S and initial distribution β. A strongly
connected component (SCC) of the digraph is a maximal subset of vertices C, where for every
pair of vertices s, s(cid:48) ∈ C, there is a directed path1 from s to s(cid:48) and a directed path from s(cid:48) to
s (Tarjan, 1972). A Terminal Strongly Connected Component (TSCC) S(cid:48) ⊆ S is an SCC
reachable from some initial state s, βs > 0 and with no outgoing transitions to any state in
S \S(cid:48). A TSCC is also called a bottom SSC (Courcoubetis & Yannakakis, 1995). We denote
by rk(M) ⊆ S the set of states in the kth TSCC of M, and by r(M) = (cid:83)
k∈[m] rk(M) the
union of all such sets. The complement set is denoted ¯r(M) := S \ r(M), which in the case
of Markov chains is the set of transient or isolated states.

Figure 1 illustrates a Markov chain with two TSCCs (highlighted with two separate

colors).

Deﬁnition 5 (Stationary policy). Given MDP M = (S, A, T, R, β), a stationary policy
π : S → ∆A is a mapping of states to probability distributions over the space of actions A,
where ∆A is the probability simplex over A. The policy π speciﬁes the conditional probability
π(a|s) that action a is taken in state s. The set of all stationary policies is denoted ΠS.

1. There is a directed path from node v to node w if it is possible to reach w from v by traversing the

directed edges in the directions in which they point.

6

Steady-State Planning in Expected Reward Multichain MDPs

Figure 1: State classiﬁcation in a Markov chain with four communicating classes. The set
{s1, s2} is transient, and the sets {s3, s4}, {s5, s6, s7, s8} and {s9, s10, s11} are recurrent.
This Markov chain is not irreducible since the states do not belong to one communicating
class. States s9, s10, and s11 are periodic. The set {s3, s4} is isolated since it is not reachable
from states in S \ {s3, s4} and it has zero initial distribution. The components colored in
green and blue are the two TSCCs of the Markov chain, i.e., r1(M) = {s5, s6, s7, s8} and
r2(M) = {s9, s10, s11}.

Deﬁnition 6 (Markov chain induced by policy). The tuple Mπ = (S, Tπ, β) is the Markov
chain induced by a policy π in an underlying MDP M = (S, A, T, R, β), where

Tπ(s(cid:48)|s) =

(cid:88)

s∈A(s)

T (s(cid:48)|s, a)π(a|s)

(1)

Deﬁnition 7 (Unichain and multichain MDP). An MDP is called unichain (Puterman,
1994; Altman, 1999) if every stationary deterministic policy induces a Markov chain that is
unichain, that is, consists of exactly one recurrent set and possibly some transient states2.
An MDP is said to be multichain if it is not unichain. See Figure 2 and its caption for an
example.

Deﬁnition 8 (Stationary distribution). Given a Markov chain M = (S, T, β), a stationary
distribution Pr∞ : S → [0, 1] over the state space is any solution to the set of equations
(Norris, 1997)

Pr∞(s) =

(cid:88)

s(cid:48)∈S

Pr∞(s(cid:48))T (s|s(cid:48)), Pr∞(s) ≥ 0, ∀s ∈ S

Pr∞(s) = 1 .

(cid:88)

s∈S

(2)

(3)

According to the ergodic theorem of Markov chains, the solution to (2) and (3) is unique
if and only if T is the transition matrix of a unichain (Gallager, 2013, Chapter 4). If there are

2. This deﬁnition does not require the recurrent class to be ergodic (hence aperiodic). Our analysis dispenses

with the aperiodicity precondition as will be clear in the sequel.

7

s1s1s1s1s1s1s2s2s1s2s2s3s4s5s6s7s8s9s10s11Isolated (recurrent) componentTransient statesTSCCTSCCRecurrent statesβ   = β   = 0s3s4βs= 1/9, ∀s∈S∖{s3,s4}Recurrent statesAtia et al.

Figure 2: (a) Unichain MDP: every deterministic policy induces a Markov chain that has
exactly one recurrent component. (b) Multichain MDP: adding the self-loop to state s3
yields a multichain MDP. For example, the deterministic policy deﬁned by π(a1|s1) =
π(a2|s2) = π(a2|s3) = 1 induces the Markov chain in (c), which is multichain with two
recurrent components {s2} and {s3}.

multiple recurrent classes, then in general there will be many stationary distributions. For
example, for the Markov chain of Figure 2(c), one can verify that the distribution Pr∞(s1) =
Pr∞(s2) = 0, Pr∞(s3) = 1 and the distribution Pr∞(s1) = Pr∞(s3) = 0, Pr∞(s2) = 1 both
satisfy (2) and (3), thus they are both stationary distributions of the Markov chain. Note
that a stationary distribution may not be representative of the true steady-state behavior
of the system (c.f. Deﬁnition 10 and the following example).

Deﬁnition 9 (Stationary matrix of a Markov chain). Given a Markov chain M = (S, T, β),
the stationary matrix T ∞ is given by the Ces`aro limit3 (Puterman, 1994)

T ∞ = lim
n→∞

1
n

n
(cid:88)

t=1

T t .

(4)

Given a ﬁnite multichain Markov chain M = (S, T, β) with transient set F and recurrent
(i.e., non-transient) components Ek, k ∈ [m], the transition matrix T can be expressed in
the canonical form (Puterman, 1994, Appendix A)

T =










0
T1
T2
0
...
...
0
0
L1 L2

. . .

0
0
. . .
0
...
...
. . .
. . . Tm 0
. . . Lm Z










(5)

where the matrices Tk correspond to transitions between states in Ek, Lk to transitions from
states in F to states in Ek, k ∈ [m], and Z to transitions between states in F . Similarly,

3. The Ces`aro limit always exists and accounts for the non-convergence of powers of transition matrices
of periodic chains. Hence, we do not need a precondition about aperiodicity as our analysis does not
require that T ∞ = limn→∞ T n.

8

s1s3a1a2a2a1a1s2s1s3a1a2a2a2a1a1s2(a) Unichain MDP(b) Multichain MDPs1s3s2s2s3(c) Induced Markov chainπ(a1|s1) = π(a2|s2) = π(a2|s3) = 1Steady-State Planning in Expected Reward Multichain MDPs

we use Tπ,k, Lπ,k, k = 1, . . . , m, and Zπ to denote the corresponding submatrices of the
transition matrix Tπ of the Markov chain Mπ induced by policy π. Also (Puterman, 1994;
Kallenberg, 1983),

T ∞(s(cid:48), s) =




ηs,
ps(cid:48)kηs

0

s(cid:48), s ∈ Ek for some k ∈ [m]
s(cid:48) ∈ F, s ∈ Ek
otherwise

(6)

(cid:80)n

1
n

where, ηs = limn→∞
in s from initial states s(cid:48) ∈ Ek, (cid:80)
transient state s(cid:48) ∈ F into the recurrent class Ek, k ∈ [m], and (cid:80)m

t=1 T t(s(cid:48), s), is the long term proportion of time the chain spends
ηs = 1, ps(cid:48)k is the absorption probability from the

k=1 ps(cid:48)k = 1, ∀s(cid:48) ∈ F .

s∈Ek

In this paper, we are interested in the asymptotic behavior of an agent’s policy in an
MDP, as captured by the steady-state distribution of the induced Markov chain deﬁned
next.

Deﬁnition 10 (Steady-state distribution). Given an MDP M and policy π, the steady-
state distribution Pr∞
: S × A → [0, 1] over the state-action pairs, also known as the
π
occupation measure (Altman, 1999, Chapter 4), is the long-term proportion of time spent
in state-action pair (s, a) as the number of transitions approaches ∞, i.e.,

Pr∞

π (s, a) = lim
n→∞

1
n

n
(cid:88)

t=1

Pr(St = s, At = a|β, π),

s ∈ S, a ∈ A(s)

(7)

if the limit exists, where St and At are the state and action at time t. Also, Pr∞
(cid:80)

π (s) :=
π (s, a) is the steady-state probability of being in state s ∈ S. The steady-state

a∈A(s) Pr∞

distribution is a stationary distribution of the Markov chain induced by the policy π.

As an example, consider the MDP in Figure 2(b) with βs1 = 1, βs2 = βs3 = 0. The
steady-state distribution of the policy π(a1|s1) = π(a2|s2) = π(a2|s3) = 1, which induces
the Markov chain in Figure 2(c), has Pr∞

π (s2, a2) = 1 and 0 otherwise.

Deﬁnition 11. Given an MDP M = (S, A, T, R, β) and a set of policies Π ⊆ ΠS, we deﬁne

P ∞(Π) := {Pr∞

π |π ∈ Π}

as the set of occupation measures induced by policies in Π, where Pr∞

π is deﬁned in (7).

Deﬁnition 12 (Steady-state speciﬁcations and constraints (Velasquez, 2019)). Given an
MDP M = (S, A, T, R, β) and a set of labels L = {L1, . . . , LnL}, where Li ⊆ S, a set
of steady-state speciﬁcations is given by Φ∞
i=1. Given a policy π, the
speciﬁcation (Li, [li, ui]) ∈ Φ∞

L = {(Li, [li, ui])}nL
L is satisﬁed if and only if the steady-state constraint

li ≤

(cid:88)

s∈Li

Pr∞

π (s) ≤ ui

(8)

is satisﬁed; that is, if the steady-state probability of being in a state s ∈ Li in the Markov
chain Mπ falls within the interval [li, ui].

9

Atia et al.

Deﬁnition 13 (Labeled MDP (Velasquez, 2019)). An MDP M = (S, A, T, R, β, L, Φ∞
L )
augmented with the label set L and speciﬁcations Φ∞
L is termed a labeled MDP (LMDP).

Lemma 1. (Kallenberg, 1983, Theorem 4.3.2)(Krass & Vrieze, 2002) Given an MDP M =
(S, A, T, R, β) and policy π ∈ ΠS, the steady-state distribution Pr∞
π (s, a)}s,a of the
Markov chain Mπ is

π := {Pr∞

Pr∞

π (s, a) = (β(cid:62)T ∞

π )sπ(a|s), s ∈ S, a ∈ A(s)

(9)

where T ∞

π is the Ces`aro limit in (4), i.e., T ∞

π = limn→∞

1
n

(cid:80)n

t=1 T t
π.

Deﬁnition 14 (Expected average reward). Given an MDP M = (S, A, T, R, β), the ex-
pected average reward R∞

π (β) of a policy π is deﬁned as

R∞

π (β) = lim

inf
n→∞

1
n

n
(cid:88)

t=1

EAt∼π
S0∼β

[R(St, At)]

(10)

where R(s, a) := (cid:80)
the probability
measure induced by the initial distribution β and the policy π over the state-action trajec-
tories.

s(cid:48)∈S T (s(cid:48)|s, a)R(s, a, s(cid:48)), and the expectation is w.r.t.

It follows from the deﬁnition of the expected average reward in (10) and the steady-state

distribution (7) that for a stationary policy π (Krass & Vrieze, 2002; Altman, 1999)

R∞

π (β) =

(cid:88)

(cid:88)

s∈S

a∈A(s)

Pr∞

π (s, a)R(s, a)

(11)

where Pr∞

π (s, a) is given in (9).

The primary focus of this paper in the context of steady-state planning is to ﬁnd station-
ary policies that maximize the expected average reward (10) while satisfying speciﬁcations
Φ∞
L on the steady-state distribution (see Deﬁnition 12). We restrict the search to certain
classes of stationary policies which will be introduced and deﬁned precisely in Section 4.
Our solution approach to this constrained MDP problem is based on linear programming
formulations, which optimize a linear objective function capturing the expected reward, sub-
ject to linear equality and inequality constraints. Such constraints encode restrictions on
the steady-state distributions induced by policies of interest, as well as desired steady-state
speciﬁcations on the long-term frequencies for state-actions pairs. The decision variables
of the LPs correspond to the occupation measures, and policies are obtained from their
optimal solutions. We establish a one-to-one correspondence between optimal solutions of
said LPs and optimal policies of the constrained MDP problem.

3. Related Work

Research related to steady-state planning often comes from the ﬁeld of average- or expected-
reward constrained MDPs and has its roots in mathematical programming (Bertsekas,
2005). Many solutions proposed in this area utilize linear programming formulations to
derive policies (Altman, 1999). We illustrate these formulations in order of increasing com-
plexity and elucidate the key diﬀerences between the formulations in the literature and our

10

Steady-State Planning in Expected Reward Multichain MDPs

own. First, let us consider the simple problem of deriving a policy for an agent which seeks
to maximize expected reward without any constraints on its steady-state distribution.

In the unichain MDP case, one may synthesize a policy by solving a linear program (LP)

of the form (Manne, 1960; De Ghellinck, 1960)

max

(cid:88)

(cid:88)

s∈S
(cid:88)

a∈A(s)
(cid:88)

s∈S

a∈A(s)

xsa

(cid:88)

s(cid:48)∈S

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to

xsaT (s(cid:48) | s, a) =

(cid:88)

xs(cid:48)a

a∈A(s(cid:48))

xsa ∈ [0, 1]
(cid:88)

(cid:88)

xsa = 1 .

∀s(cid:48) ∈ S

(12)

∀s ∈ S, a ∈ A(s)

s∈S

a∈A(s)

The policy can be derived from the occupation measures given by xsa through a simple
calculation.
It is worth noting that this combination of occupation measures and linear
programming has enabled signiﬁcant progress in the area of planning within stochastic
shortest paths MDPs, where several occupation measure heuristics have been deﬁned to ﬁnd
decision-making policies that maximize the probability of reaching a set of goal states while
satisfying multiple cost constraints (Trevizan, Thi´ebaux, Santana, & Williams, 2016, 2017;
Trevizan, Thi´ebaux, & Haslum, 2017; Baumgartner, Thi´ebaux, & Trevizan, 2018). While
the LP in (12) always produces valid solutions for unichain MDPs, this is not necessarily
the case for multichain MDPs due to the fact that there may be more than one ergodic set
(Puterman, 1994). This issue is rectiﬁed by modifying LP (12) to obtain (Denardo & Fox,
1968; Kallenberg, 1983)

max

(cid:88)

(cid:88)

s∈S
(cid:88)

a∈A(s)
(cid:88)

s∈S
(cid:88)

a∈A(s)
(cid:88)

s∈S

a∈A(s)

xsa

(cid:88)

s(cid:48)∈S

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to

xsaT (s(cid:48) | s, a) =

(cid:88)

xs(cid:48)a

ysaT (s(cid:48) | s, a) =

a∈A(s(cid:48))
(cid:88)

(xs(cid:48)a + ys(cid:48)a) − βs(cid:48)

a∈A(s(cid:48))

∀s(cid:48) ∈ S

∀s(cid:48) ∈ S

(13)

xsa ∈ [0, 1], ysa ≥ 0

∀s ∈ S, a ∈ A(s).

The new ysa variables guide policy formation on the transient states. Both LP (12) and
LP (13) yield stationary stochastic policies. Furthermore, there always exists at least one
optimal deterministic policy, which can easily be derived from the stochastic policy solution
obtained from the LPs (Puterman, 1994).

For producing control policies with steady-state speciﬁcations, LPs (12) and (13) are
extended to include linear steady-state constraints on the occupation measures. When
applied to unichain MDPs, the constrained version of LP (12) encounters minor diﬃculties,
in that there may not be an optimal deterministic policy (Altman, 1999). Nonetheless, the
LP always produces an optimal stochastic stationary policy. In fact, there exists an optimal
policy having at most nL “randomizations”, i.e. having at most |S| + nL state-action pairs
with non-zero probability of being selected (Ross, 1989).

11

Atia et al.

On the other hand, serious issues arise when LP (13) is augmented with steady-state con-
straints and solved for multichain MDPs, as described in the pioneering work of Kallenberg
(1983). In particular, it was shown that there is not a one-to-one correspondence between
the feasible solutions of the augmented LP and the stationary policies. Instead, the space of
feasible solutions is partitioned into equivalence classes of various feasible solutions mapping
to the same policy. The key deﬁciency is that the steady-state distribution of the Markov
chain induced by the synthesized policy does not match the optimal solution to the LP in
general, and so the derived policy does not always meet the steady-state speciﬁcations (see
Example 1 in Section 4.1). This issue is not easily remedied, since the optimal solution
may not be achievable by any stationary policy, or identifying such a policy would generally
require combinatorial search. We refer the reader to the paper by Krass and Vrieze (2002)
for an overview.

In order to mitigate the preceding problem of integrating steady-state constraints, vari-
ous assumptions have been made in the literature on the structure of the underlying MDP.
Multichain MDPs are also frequently excluded from the conversation altogether. The as-
sumption that the MDP is ergodic, and therefore every policy induces an ergodic Markov
chain, has been used by Akshay et al. (2013) to ensure that steady-state equations and
constraints on the same are satisﬁed. This assumption is relaxed to some extent by Ross
(1989), Altman (1999), Feinberg (2009), where unichain MDPs are allowed. The assump-
tion of either an ergodic or a unichain MDP requires that no stationary deterministic policy
induce more than a single recurrent class, thus severely limiting the applicability of these
methods. These assumptions are removed in the recent work of Velasquez (2019), where
neither ergodic nor recurrence assumptions are made on the underlying MDP. However, the
solution proposed therein ﬁnds an irreducible Markov chain in the underlying MDP, if one
exists, and is therefore suitable for communicating MDPs where, for any two states s and
s(cid:48), there exists a deterministic stationary policy such that s can reach s(cid:48) in a ﬁnite number
of steps (Puterman, 1994). This solution, however, is too restrictive, thus not suitable for
reasoning over general multichain MDPs.

Another approach taken to address these challenges is to simply allow solutions to take
the form of non-stationary policies. In the work of Kallenberg (1983), this is accomplished
by a computationally expensive approach producing a potentially diﬀerent policy in each
time step. Another approach, proposed by Krass and Vrieze (2002), starts by using one
policy, and then switches to a second “tail” stationary policy. The time at which the switch
occurs is determined by a lottery performed at each time step, and once the switch occurs
the tail policy continues to be used indeﬁnitely (thus the policy is “ultimately” stationary
once the switch occurs). However, this approach has three key limitations. First, the
constraints must take the form of a target frequency vector, which imposes an equality
constraint on the steady-state distribution over all states. Second, the lottery system does
not guarantee that the switch will occur in a ﬁnite number of steps, thus meaning that the
policy is not guaranteed to be ultimately stationary. Third, the policy depends on a marker
to track whether or not the switch has occurred. This marker is not part of the MDP, and
therefore the MDP machinery must be modiﬁed to include a so-called marker-augmented
history. As an alternative, the authors also propose a way to extend the given MDP with
additional states, such that the problem can be solved using a stationary policy applied to

12

Steady-State Planning in Expected Reward Multichain MDPs

the extended MDP. However, this approach still cannot produce a stationary policy to solve
the original problem.

While most methods for solving constrained MDPs revolve around the use of math-
ematical programs, some reinforcement learning approaches have also been proposed for
optimizing the average-reward objective and, to a lesser extent, for solving constrained
instances of average-reward MDPs. Some noteworthy examples include the constrained
actor-critic method proposed by Bhatnagar and Lakshmanan (2012), wherein a Lagrangian
relaxation of the problem is used to incorporate steady-state costs into the objective function
being optimized by the constrained actor-critic algorithm. A similar Lagrangian Q-learning
approach is proposed by Lakshmanan and Bhatnagar (2012). Both of these reinforcement
learning methods assume that every Markov chain induced by a policy is irreducible, which
allows only a single recurrent class as with ergodic and unichain assumptions described
earlier. The Lagrangian approach has also been applied to speciﬁc stochastic policy linear
programming formulations relevant to aircraft maintenance problems where the asymptotic
failure is to be kept below some small threshold (Boussemart & Limnios, 2004; Boussemart
et al., 2002).

In contrast to the foregoing eﬀorts, our approach is computationally tractable, works
with the most general multichain MDPs, and always produces a stationary policy that
satisﬁes the given steady-state speciﬁcations, if one exists. Additionally, none of the afore-
mentioned methods consider constraints on the expected visits to transient states, as we
are considering in our work.

4. Steady-State Policy Synthesis: Problem Formulation

In this section, we introduce the Steady-State Policy Synthesis (SSPS) problem of ﬁnding
a stationary policy from predeﬁned classes of policies (edge- and class-preserving) that
maximizes the expected average reward subject to steady-state speciﬁcations. In contrast
to prior work, we do not impose restrictions on the underlying MDP. Before we present our
formulation, we brieﬂy discuss the challenges underlying policy synthesis under the average
reward optimality criterion and demonstrate the limitations of existing formulations in this
context. Subsequently, we specify our search domain of policies and deﬁne the SSPS problem
of synthesizing optimal policies from this domain.

4.1 Challenges and Limitations

We motivate this section with a simple example. Suppose an autonomous agent is marooned
on a set of three connected frozen islands as shown in Figure 3. The agent’s goal is to
maximize the amount of time it spends ﬁshing for sustenance while at the same time building
a canoe to escape the islands. The agent has an equal chance of starting in any state
belonging to the larger island of size n × n/2, i.e., we have βs = 2/n2 for each state s in
the island. Once the agent moves to one of the two smaller islands, it is unable to return
to the larger island. One quarter of the land in the small islands contains logs which can
be used to build a canoe, and each of these islands contains one ﬁshing site as well. For
the ﬁrst small island we have steady-state speciﬁcations (Llog1, [0.25, 1]), (Llog2, [0.25, 1]),
(Lcanoe1, [0.05, 1.0]) and reward R(·, ·, Lﬁsh1) = R(·, ·, Lﬁsh2) = 1. Likewise, the second small
island has steady-state speciﬁcations (Lcanoe2, [0.05, 1.0]), (Lﬁsh1, [0.1, 1.0]), (Lﬁsh2, [0.1, 1.0])

13

Atia et al.

Figure 3: LMDP M = (S, A, T, R, β, L, Φ∞
L ) with labels Llog1 = {s34, s36, s38, s43},
Llog2 = {s52, s55, s57, s61}, Lcanoe1 = {s33}, Lcanoe2 = {s49}, Lﬁsh1 = {s48}, Lﬁsh2 =
{s64}, steady-state speciﬁcations (Llog1, [0.25, 1]), (Llog2, [0.25, 1]), (Lcanoe1, [0.05, 1.0]),
(Lcanoe2, [0.05, 1.0]), (Lﬁsh1, [0.1, 1.0]), (Lﬁsh2, [0.1, 1.0]) ∈ Φ∞
L , and rewards R(·, ·, Lﬁsh1) =
R(·, ·, Lﬁsh2) = 1, R(·, ·, S \ (Lﬁsh1 ∪ Lﬁsh2)) = 0.

and reward R(·, ·, S \ (Lﬁsh1 ∪ Lﬁsh2)) = 0. Because the islands are covered in ice, the agent
has a chance of slipping in three possible directions whenever it moves. Speciﬁcally, if the
agent attempts to go right (left), it has a 90% chance of transitioning to the right (left),
and there is a 5% chance of transitioning instead to either of the states above or below it.
Similarly, if the agent tries to go up (down), it moves to the states above (below) it with 90%
chance, and to the states to the right and left of it with chance 5% each. This Frozen Island
scenario is motivated by that found in OpenAI Gym’s FrozenLake environment (Brockman
et al., 2016).

For this example, the LP by Velasquez (2019) is infeasible since there exists no policy
that induces an irreducible Markov chain, that is, one where all states in S belong to one
recurrent class. The LP by Kallenberg (1983) in (13) will return a solution (x, y), from
which the stationary policy π := π(x, y) is computed as follows




xsa
xs
ysa
ys
arbitrary

π(a|s) =


a∈A(s) xsa, ys = (cid:80)

s ∈ Ex, a ∈ A(s)
s ∈ Ey \ Ex, a ∈ A(s)
otherwise

(14)

where xs := (cid:80)
a∈A(s) ysa, Ex := {s ∈ S : xs > 0} and Ey := {s ∈ S : ys >
0}. However, in general the steady-state distribution induced by the policy (14) will not
satisfy the speciﬁed constraints. This deﬁciency is best demonstrated via a simple example.
The reader is also referred to Example 1 of Krass and Vrieze (2002).

Example 1. Consider the MDP in Figure 2(b) with initial probability βs1 = βs3 = 0, βs2 =
1. One feasible solution x of the LP in (13) (Kallenberg, 1983, Program 4.7.6) has xs2a2 =

14

Steady-State Planning in Expected Reward Multichain MDPs

Figure 4: (Left) One-to-one correspondence between the feasible LP solutions and the
stationary policies in unichain MDPs. (Right) Equivalence classes of feasible solutions map
to stationary policies. The steady-state distribution of the Markov chain induced by a policy
need not agree with the LP solution, and could hence fail to meet the LP constraints.

xs3a2 = 0.5. The policy π in (14) corresponding to x has π(a2|s2) = π(a2|s3) = 1, hence
Pr∞

π (s2, a2) = 1. Therefore, Pr∞

π (cid:54)= x.

The previous example underscores the main challenge underlying steady-state planning
in constrained Markov decision models with the average reward criterion: solutions to
formulated programs and stationary policies are not in one-to-one correspondence. In other
words, given a feasible LP solution (x, y), the steady-state distribution Pr∞
π induced by the
policy π(x, y) derived from that solution is not equal to x in general. As a result, unlike
unichain MDPs (Altman, 1999), steady-state speciﬁcations encoded as constraints on the
state-action variables are generally not met by π. Figure 5 (Left) illustrates the one-to-
one correspondence between LP solutions and stationary policies found in unichain MDPs.
Figure 5 (Right) illustrates the lack of such correspondence in multichain MDPs, where
instead, equivalence classes of feasible LP solutions (yellow circles) map to the same policy
(Kallenberg, 1983; Puterman, 1994).

4.2 Problem Setup

The previous example motivates the work of this paper in which we develop an approach
to synthesizing policies with provably correct asymptotic behavior based on the notions of
edge preservation and class equivalence. First, we will deﬁne sets of policies under which
certain class structures are preserved and give an example of such policies, then deﬁne the
SSPS problem of ﬁnding an optimal policy from such classes.

Deﬁnition 15 (Edge-preserving policies). Given an MDP M, we deﬁne the set of Edge-
Preserving (EP) policies ΠEP as the set of stationary policies that play every action available
at states in the TSCCs r(M) of M and for which r(Mπ) = r(M), i.e.,

ΠEP = (cid:8)π ∈ ΠS : r(Mπ) = r(M) ∧ π(a|s) > 0, ∀s ∈ r(M), a ∈ A(s)(cid:9) .

(15)

Hence, for every state s ∈ r(M) (see Deﬁnition 4), an EP policy assigns a non-zero
probability to every action in A(s), and every state in ¯r(M) is either transient or isolated
in the Markov chain induced by the policy. For example, the uniform policy which has

15

UnichainMDPMultichain MDPFeasiblesolutions𝑥!Pr"!#𝜋!𝜋$𝑥$Pr""#StationarypoliciesFeasiblesolutions𝑥!𝜋!𝜋$𝑥$StationarypoliciesAtia et al.

π(a|s) = 1/|A(s)|, ∀s ∈ S is in ΠEP. Note that other policies in ΠEP could assign a very
small probability (as long as it is non-zero) to non-rewarding transitions in r(M). Using an
open set deﬁnition in (15) simpliﬁes the exposition and the subsequent theoretical analysis,
however, it does not guarantee that an optimal policy from the set always exists. We discuss
and analyze variations of the problem formulation to address this issue at length in Section
5.4.4.

Next, we introduce two sets of policies whose deﬁnitions rest on two distinct notions of

class preservation.

Deﬁnition 16 (Class-preserving policies). Given an MDP M with TSCCs rk(M), k =
1, . . . , m, we deﬁne the set of Class-Preserving (CP) policies ΠCP as the set of stationary
policies that induce Markov chains with the same TSCCs as those of M, i.e.,

ΠCP = (cid:8)π ∈ ΠS : r(Mπ) = r(M) ∧ ∀k ∈ [m], rk(Mπ) = rk(M)(cid:9) .

(16)

Note that the condition r(Mπ) = r(M) in Deﬁnitions 15 and 16 implies that ¯r(M)
consists of transient or isolated states in Mπ for any π in ΠEP or ΠCP. Per (16), a CP policy
preserves the recurrence of all states in the TSCCs of the MDP but, unlike EP policies,
its support need not be the entire set of actions available at said states. Therefore, CP
policies can conceivably achieve larger rewards than EP policies by averting non-rewarding
transitions.

Deﬁnition 17 (Class-preserving up to unichain). Given an MDP M with TSCCs rk(M), k =
1, . . . , m, we deﬁne the set of Class-Preserving-up-to-Unichain (CPU) policies ΠCPU as

ΠCPU = (cid:8)π ∈ ΠS : r(Mπ) ⊆ r(M) ∧ ∀k ∈ [m], ∃! rk(Mπ) ⊆ rk(M)(cid:9) ,

(17)

that is, the set of stationary policies that induce Markov chains Mπ in which the TSCCs
of the MDP M are reachable and unichain (i.e., each contains exactly one non-isolated,
recurrent component) and the recurrent states are a subset of the recurrent states of M
(Recalling that the notation ∃! in (17) refers to the existence of a unique set).

This deﬁnition captures a more relaxed notion of class preservation than (16) for CP
policies in that it relaxes the requirement that all states in the TSCCs of M be recurrent
and reachable in the Markov chain Mπ induced by the policy π, to the milder requirement
that in Mπ there exists a unique reachable recurrent class in each of the TSCCs of M.

The aforementioned deﬁnitions are best illustrated by an example. Figure 5(b) illustrates
a Markov chain induced by an EP policy, i.e., one that plays every action available in the
TSCCs of the MDP of Figure 5(a) with non-zero probability. As shown, s1 is isolated and
s2 is transient – these would both be transient under the uniform policy. The TSCCs of the
induced chain are highlighted with two separate colors. Examples of Markov chains induced
by a CP and a CPU policy are shown in Figure 5(c) and (d), respectively. The Markov
chain of Figure 5(c) has the exact same TSCCs of the MDP and of the Markov chain of
Figure 5(b) induced by the EP policy, with the fundamental diﬀerence that the CP policy
is not supported on every action available in the TSCCs (e.g., see the recurrent component
highlighted in blue). By contrast, states s3, s4, s6 and s7 are transient in the Markov chain of
Figure 5(d). The set consisting of states s3, s4, s5 is unichain, having exactly one recurrent

16

Steady-State Planning in Expected Reward Multichain MDPs

Figure 5: (a) MDP and Markov chains induced by (b) EP, (c) CP, and (d) CPU policies.
For the MDP, all transitions are deterministic, i.e., T (s(cid:48)|s, a) ∈ {0, 1} indicating if there is
an outgoing edge from s to s(cid:48) under action a, the rewards are deﬁned such that R(s5, a3) =
R(s8, a1) = 1 and 0 otherwise, and the initial probabilities are βs1 = 0 and βs = 1/8, ∀s ∈ S\
{s1}. The numbers next to the edges of the Markov chains are the conditional probabilities
π(a|s) of the diﬀerent actions given the states specifying the policies.

17

s2s1s6s7s8s9s4s3s5a1a2a3a4a1a2a1a2a1a2a1a2a1a2a1a2a1a2a1a2a3a3a3β   = 0s1(a) MDP(c) Class-Preserving(b) Edge-Preserving(d) Class-Preserving up to Unichains2s1s6s7s8s9s4s3s511/31/31/311/23/41/21/21/21/21/211/31/31/31/411/31/31/31/211/21/211/21s5s3s4s9s8s7s61s1s21/32/51/41/23/41/22/33/52/31/32/3s2s6s7s8s9s4s3s51/61/31/31/31/61/31/31/31s1βs = 1/8, ∀s∈S ∖{s1}Atia et al.

component (state s5) and two transient states (s3 and s4). Similarly, the set composed of
states s6, s7, s8, s9 is unichain with one recurrent component (s8 and s9) and two transient
states (s6 and s7).

The classes of policies deﬁned in (15), (16) and (17) satisfy the following relations.

Lemma 2. ΠEP ⊆ ΠCP ⊆ ΠCPU .

We remark that the inclusions in Lemma 2 are generally strict, except for some special
MDPs. Speciﬁcally, given a general MDP M, there may exist a policy π ∈ ΠCP for which
r(Mπ) = r(M) and π(a|s) = 0 for some a ∈ A(s), s ∈ r(M), in which case π /∈ ΠEP.
Similarly, since a unichain may contain some transient states, ΠCP is generally a proper
subset of ΠCPU.

Problem Deﬁnition. We can readily deﬁne the class of problems SSPS(Π), parametrized
by a predeﬁned set of stationary policies Π, of ﬁnding a policy in the set Π that maximizes
the expected average reward while satisfying a given set of steady-state speciﬁcations.

(SSPS)). Given an LMDP M =
Deﬁnition 18 (Steady-state policy synthesis
(S, A, T, R, β, L, Φ∞
L ) and a domain of policies Π ⊆ ΠS, the SSPS(Π) problem is to ﬁnd
an optimal stochastic policy π ∈ Π that maximizes the expected average reward deﬁned in
(10) and satisﬁes the steady-state speciﬁcations Φ∞

L (Deﬁnition 12), i.e.,

max
π∈Π

(cid:88)

(cid:88)

(cid:88)

Pr∞

π (s, a)R(s, a) subject to

s∈S
(cid:88)

a∈A(s)
Pr∞

π (s, a) ∈ [l, u], ∀(Li, [l, u]) ∈ Φ∞
L

(18)

s∈Li

a∈A(s)

If the maximum in (18) cannot be attained over the domain Π, we deﬁne SSPS(Π) as the
problem of ﬁnding a policy π ∈ Π that satisﬁes the speciﬁcations Φ∞
L and whose expected
average reward R∞
π(cid:48) (β) − (cid:15), for some arbitrarily small (cid:15) > 0 (See Section
5.4.4).

π (β) ≥ supπ(cid:48)∈Π R∞

In this paper, we present solutions to SSPS(ΠEP), SSPS(ΠCP) and SSPS(ΠCPU), where
Π in (18) is set to ΠEP, ΠCP, and ΠCPU, respectively4. To this end, we ﬁrst determine the
TSCCs r(M) of M and the complement set ¯r(M) using standard techniques from graph
theory (Tarjan, 1972). These are then used to deﬁne an LP from which the solution policy
is derived.

5. Linear Programming Based Solutions

In this section, we present our linear-programming-based solution to the SSPS problem
(18) over edge- and class-preserving policies. We formulate linear programs that encode
constraints on the limiting distributions of said policies to solve SSPS(ΠEP) and SSPS(ΠCP).
The optimal solutions to the formulated programs provably yield optimal edge- and class-
preserving policies that meet the desired speciﬁcations. The encoded constraints are also
at the center of an iterative algorithm described in Section 5.3 to generate CPU policies

4. Our work (Atia et al., 2020) has presented preliminary results for the SSPS(ΠEP) problem.

18

Steady-State Planning in Expected Reward Multichain MDPs

for SSPS(ΠCPU). Our main results on SSPS in edge- and class-preserving policies are
presented in Sections 5.1, 5.2 and 5.3. To simplify the exposition, all proofs are deferred to
the appendix.

We present three main programs. The ﬁrst, used for the synthesis of optimal EP poli-
cies, is the most constrained as it encodes the requirement that every action in the terminal
components must be played with non-zero probability. While this condition is not neces-
sary in order to ensure one-to-one correspondence between the feasible solutions and the
induced steady-state distributions, it results in a simple program whose solution provably
yields an optimal EP policy that meets the speciﬁcations. The second program relaxes
this condition to the milder requirement that every state in the terminal components is
visited inﬁnitely often (which may not require playing every action available), but at the
expense of additional complexity. Speciﬁcally, its solution yields an optimal policy that
meets the speciﬁcations from the class of CP policies (a superset of EP policies) but uses
more complex ﬂow constraints to encode said requirement. The third program is the least
constrained and is used to synthesize a policy from the larger class of CPU policies. While
its solution is not guaranteed to yield a CPU policy, we derive a characterization of its op-
timal solution, which inspires a greedy algorithm to construct such policy. We augment the
program by iteratively adding constraints until convergence. The algorithm is guaranteed
to converge in a ﬁnite number of steps to a (possibly) suboptimal CPU policy that meets
the speciﬁcations.

By encoding constraints on the limiting distribution of the Markov chain induced by
a stationary policy derived from an LP solution, the policy is ultimately absorbed in the
TSCCs of the MDP. This restricts the long-term play to the TSCCs, which once reached,
cannot be escaped. By imposing strict positivity on state-action pairs or ﬂow constraints
in the TSCCs, we further ensure that these components are unichain, and in turn, the
long-term frequencies induced by the policy match the solution from which the policy is
generated.

5.1 SSPS(ΠEP) – Synthesis over Edge-Preserving Policies

In this section, we formulate a linear program to solve SSPS(ΠEP) deﬁned in (18), which
seeks to maximize the expected average reward subject to speciﬁcation constraints over the
class of policies ΠEP in (15).

Given MDP M, deﬁne Q0 to be the set of vectors x, y satisfying






(i) (cid:80)
(ii) (cid:80)
(iii) (cid:80)

s∈S

s∈S

(cid:80)

a∈A(s) xsaT (s(cid:48) | s, a) = (cid:80)
(cid:80)
a∈A(s) ysaT (s(cid:48) | s, a) = (cid:80)
(cid:80)
a∈A(f ) xf a = 0,

f ∈¯r(M)

a∈A(s(cid:48)) xs(cid:48)a, ∀s(cid:48) ∈ S
a∈A(s(cid:48))(xs(cid:48)a + ys(cid:48)a) − βs(cid:48), ∀s(cid:48) ∈ S

(19)

xsa ∈ [0, 1], ysa ≥ 0, ∀s ∈ S, a ∈ A(s), f ∈ ¯r(M), k ∈ [m]

19

Atia et al.

We can readily formulate LP1 (20) to synthesize optimal EP policies, which incorporates

two additional constraints beside the constraints in (19).

(cid:88)

(cid:88)

max

xsaR(s, a) subject to (x, y) ∈ Q0

s∈S

(LP1)

(iv)

li ≤

a∈A(s)
(cid:88)

(cid:88)

s∈Li

a∈A(s)

xsa ≤ ui, ∀(Li, [li, ui]) ∈ Φ∞
L

(20)

(v) xsa > 0, ∀s ∈ rk(M), k ∈ [m], a ∈ A(s)

Constraints (i ) – (iii ) constrain the limiting distributions of Markov chains induced by the
policies of interest, and are thus part of the constraint set of all programs we formulate in this
work. In particular, they capture the structure of the stationary matrix T ∞ corresponding
to the classiﬁcations r(M) and ¯r(M) (See Deﬁnition 4). Constraint (i ) ensures that x is a
stationary distribution (Altman, 1999; Puterman, 1994); constraint (ii ), which is described
in (Kallenberg, 1983, Chapter 4) and (Puterman, 1994, Sec 9.3), enforces consistency in
the expected average number of visits yf a for any transient state-action pair f ∈ ¯r(M), a ∈
A(f ); constraint (iii ) preserves the non-recurrence of the states f ∈ ¯r(M) by forcing zero
steady-state probability. Constraint (iv ) encodes the steady-state speciﬁcations. The strict
positivity constraint (v ) preserves the transitions in the TSCCs to yield EP policies. In
practice, we transform the strict inequalities to bounded ones by introducing an arbitrarily
small constant on the right-hand side, thereby ensuring an optimal solution always exists
(See Section 5.4.4). Enforcing constraints on the occupation measures ensures that, from
any state f ∈ ¯r(M), the process will be ultimately absorbed into the TSCCs rk(M), k ∈ [m].
The next theorem guarantees that every feasible solution to LP1 yields an EP policy.

Theorem 1. Given an LMDP M, let (x, y) ∈ Q1, where Q1 is the feasible set of solutions
to LP1 (20), and let π := π(x, y) be deﬁned as in (14). Then, π ∈ ΠEP.

We can readily state the following theorem establishing the correctness of LP1. It guarantees
that the policy synthesized from an optimal solution to (20) (if one exists) is not only in
ΠEP, but also is optimal among all such policies and meets the steady-state speciﬁcations,
i.e., solves SSPS(ΠEP).

Theorem 2. Given an LMDP M = (S, A, T, R, β, L, Φ∞
L ), LP1 in (20) is feasible iﬀ there
exists a policy π ∈ ΠEP such that the Markov chain Mπ = (S, Tπ, β) satisﬁes the speciﬁca-
tions Φ∞
L . Further, given an optimal solution x∗, y∗ of (20), the policy π∗ := π(x∗, y∗) as
deﬁned in (14) is optimal in the class of policies ΠEP and meets the speciﬁcations Φ∞
L .

5.2 SSPS(ΠCP) – Synthesis over Class-Preserving Policies

The strict positivity constraint (v ) of LP1 forces the policy to play every action in the
TSCCs of the MDP M (by assigning non-zero probability to every action available), which
may be restrictive and often unnecessary. Indeed, as we show, in order to ensure one-to-one
correspondence between the optimal solutions of a formulated LP and the optimal policies
of the constrained MDP derived from these solutions, it suﬃces to preserve the recurrence
or the unichain property of these components.

20

Steady-State Planning in Expected Reward Multichain MDPs

To address this restriction, we introduce ﬂow constraints in LP2 given in (21) (replacing
constraint (v )) to ensure the recurrence of the components rk(M), k ∈ [m], in the induced
chain. It helps to introduce some notation in order to express such constraints. We deﬁne
the transition relation of an MDP by T rel = {(s, s(cid:48)) ∈ S×S|s (cid:54)= s(cid:48)∧∃a ∈ A(s), T (s(cid:48)|s, a) > 0}
(Velasquez, 2019). This corresponds to the graph structure of the MDP. For each TSCC
rk(M), we further deﬁne its graph structure as T rel
k = T rel ∩ rk(M) × rk(M). We can now
add ﬂow constraints in order to ensure that, for the Markov chain induced by the solution
policy, each set rk(M) will remain a recurrent class without necessarily having to take every
action available in that set.

max

(cid:88)

(cid:88)

xsa

s∈S

(iv)

li ≤

a∈A(s)
(cid:88)

(cid:88)

xsa ≤ ui,

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) ∈ Q0,

(cid:88)

s(cid:48)∈S

s∈Li

(vi)

fsis(cid:48) =

a∈A(s)
(cid:88)

T (s(cid:48)|si, a)xsia

(vii)

f rev
sis(cid:48) =

(viii)

fss(cid:48) ≤

a∈A(si)
(cid:88)

T (si|s(cid:48), a)xs(cid:48)a

a∈A(s(cid:48))
(cid:88)

T (s(cid:48)|s, a)xsa

(LP2)

(ix)

f rev
ss(cid:48) ≤

a∈A(s)
(cid:88)

a∈A(s(cid:48))

T (s|s(cid:48), a)xs(cid:48)a

(x)

(xi)

(xii)

(xiii)

(xiv)

(cid:88)

fs(cid:48)s >

(cid:88)

fss(cid:48)

(s(cid:48),s)∈T rel
(cid:88)

(s,s(cid:48))∈T rel
(cid:88)

(s(cid:48),s)∈T rel
(cid:88)

f rev
s(cid:48)s >

(s,s(cid:48))∈T rel
(cid:88)

(s(cid:48),s)∈T rel

f rev
ss(cid:48)

fs(cid:48)s > 0

f rev
s(cid:48)s > 0

(s,s(cid:48))∈T rel
fss(cid:48), f rev

ss(cid:48) ∈ [0, 1]

∀(Li, [li, ui]) ∈ Φ∞
L

∀(si, s(cid:48)) ∈ T rel

k , k ∈ [m]

∀(s(cid:48), si) ∈ T rel

k , k ∈ [m]

∀(s, s(cid:48)) ∈ T rel

k , k ∈ [m]

∀(s(cid:48), s) ∈ T rel

k , k ∈ [m]

∀s ∈ r(M) \ {si}

∀s ∈ r(M) \ {si}

∀s ∈ r(M)

∀s ∈ r(M)

∀(s, s(cid:48)) ∈ T rel

(21)
The program LP2 in (21) is such that every state in rk(M) can reach and is reachable
from every other state in rk(M). For each k ∈ [m], constraint (vi ) induces an initial ﬂow
out of a randomly chosen state si ∈ rk(M) and into its neighbors s(cid:48), that is proportional
to the transition probability Tπ(s(cid:48)|si) in the Markov chain induced by the solution policy;
constraint (viii ) establishes the ﬂow capacity between states in a similar manner; (x ) ensures
that the incoming ﬂow into every state in rk(M) is greater than the outgoing ﬂow; ﬁnally,
constraint (xii ) ensures that there is incoming ﬂow into every state in rk(M). These
constraints ensure that every state in rk(M) is reachable from si, whereas constraints (vii ),
(ix ), (xi ), (xiii ) address the foregoing in the reverse graph structure of the MDP, thereby

21

Atia et al.

ensuring that si is reachable from all states in rk(M). We remark that the feasible set in
(21) is a superset of that in (20) since the ﬂow constraints (vi )–(xiv ) are implied by (v ),
hence LP2 is less-constrained than LP1.

We can readily state the following two theorems establishing the correctness of LP2,
which are the counterparts of Theorem 1 and 2. In particular, Theorem 3 guarantees that
the solution to LP2 is a CP policy, while Theorem 4 establishes that the policy (14) derived
from the optimal solution to LP2 in (21) solves SSPS(ΠCP), i.e., is optimal among the class
of CP policies and meets the steady-state speciﬁcations.

Theorem 3. Given an LMDP M, let (x, y) ∈ Q2 and π be deﬁned as in (14), where Q2 is
the feasible set of solutions to LP2 (21). Then, π ∈ ΠCP.
Theorem 4. Given an LMDP M = (S, A, T, R, β, L, Φ∞
L ), the LP in (21) is feasible iﬀ
there exists a policy π ∈ ΠCP, where ΠCP is deﬁned in (16), such that the Markov chain
Mπ = (S, Tπ, β) satisﬁes the speciﬁcations Φ∞
L . Further, given an optimal x∗, y∗ of (21),
the policy π(x∗, y∗) deﬁned in (14) is optimal in the class of policies ΠCP and meets the
speciﬁcations Φ∞
L .

5.3 SSPS(ΠCPU) – Synthesis over Class-Preserving up to Unichain Policies

In this section, we discuss policy synthesis over the larger set of policies ΠCPU. We provide
a suﬃcient condition under which we can identify an optimal policy π ∈ ΠCPU that meets
the speciﬁcations. Based on this result, we develop an iterative algorithm to construct a
policy in ΠCPU that provably meets the desired speciﬁcations.

Next, we give a suﬃcient condition for SSPS(ΠCPU), characterized in terms of the set

of optimal solutions to LP3 in (22).
(cid:88)

(cid:88)

(cid:88)

LP3 : max

xsa

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) ∈ Q0 and (iv)

(22)

s∈S

a∈A(s)

s(cid:48)∈S

Note that the feasible set of LP3 is the intersection of the set Q0 in (19) and the set of
variables satisfying the steady-state speciﬁcations, that is, without the positivity or ﬂow
constraints in (20) and (21), respectively.

Recall that a strongly connected digraph is one in which it is possible to reach any node
starting from any other node by traversing the directed edges in the directions in which
they point. Theorem 5 states that the policy π in (14), derived from an optimal solution to
LP3 in (22), solves SSPS(ΠCPU) if the directed subgraphs corresponding to the support of
the optimal solution in the TSCCs of M are strongly connected. In Theorem 5, we deﬁne
the digraph associated with the support of a given solution x as the graph whose vertices
are all states s with xs > 0 and whose edges correspond to actions a for which xsa > 0.
Theorem 5. Given LMDP M, let Q∗ be the set of optimal solutions of LP3 (22) and
X ∗ := {x : (x, y) ∈ Q∗ for some y}. Given x ∈ X ∗, let V +
k (x) := {s ∈ rk(M) : xs > 0}
and E+
k (x))
is strongly connected ∀k ∈ [m], then the policy π in (14) is optimal in the class of policies
ΠCPU and meets the speciﬁcations in Φ∞
L .
Corollary 1. If the condition in the statement of Theorem 5 holds for all x ∈ X ∗, then
LP3 (22) solves SSPS(ΠCPU).

k (x) := {(s, a) ∈ rk(M) × A(s) : xsa > 0}. If the directed subgraph (V +

k (x), E+

22

Steady-State Planning in Expected Reward Multichain MDPs

5.3.1 Generation of policies in ΠCPU

Inspired by Theorem 5, we devise a row-generation-based algorithm to search for a policy
π ∈ ΠCPU as shown in Algorithm 1. First, LP3 (22) is solved. If the digraph corresponding
to the support of the obtained solution is strongly connected for each of the TSCCs of
LMDP M, the policy in (14) is computed and the search stops. However, if the solution
does not correspond to a strongly connected digraph in some TSCC, then there must exist
a non-empty set of states with no outgoing edges to the rest of the states in that TSCC.
Hence, for some k ∈ [m] we ﬁnd a cut, that is, a set of states C that has no outgoing edges
to the complement set rk(M) \ C. The constraint in (23) corresponding to this cut is added
to include the edges in the support, where A(cid:48) = {a ∈ A(s) : T (s(cid:48)|s, a) > 0, s(cid:48) ∈ rk(M) \ C}.
The constraint forces the addition of missing edges across this cut in a greedy manner (by
forcing the sum of the state-action variables corresponding to these edges to be non-zero) to
eventually produce a strongly connected digraph. The process is repeated until a strongly
connected solution is found.

(cid:88)

(cid:88)

s∈C

a∈A(cid:48)

xsa > 0

(23)

Algorithm 1 is guaranteed to converge to a (possibly suboptimal) policy in ΠCPU in a ﬁnite
number of steps, since in the worst case (when all edges are included) it will yield a policy
in ΠEP ⊆ ΠCPU under which all edges in the TSCCs of M are retained. The ﬁniteness of
the number of steps is because the number of cuts in the ﬁnite MDP is bounded above by
O(maxk∈[m] 2|rk(M)|). Our experiments have shown that Algorithm 1 converges to a policy
in ΠCPU after a small number of iterations.

Algorithm 1 Generation of a policy π ∈ ΠCPU
Input: LMDP M with speciﬁcations Φ∞
L .
Output: Stationary policy π ∈ ΠCPU which satisﬁes Φ∞
L .

Determine the TSCCs rk(M), k ∈ m of M
isSConnected = F alse, C = {.}, A(cid:48) = {.}
while isSConnected = F alse do

Solve LP (22) with constraint (23) to get optimal values x∗
Compute the support E+
if digraph (V +

sa, ∀(s, a) ∈ S × A(s).
k (x∗) of each TSCC corresponding to x∗ (See Theorem 5).

k (x∗)) forms a SCC for every k ∈ [m] then

sa, y∗

k (x∗), E+
compute π using (14)
isSConnected = T rue

else

ﬁnd a cut and update C and A(cid:48)

end if
end while

5.4 Additional Insights

This section provides additional remarks and examples to shed more light on the linear
programming formulations. The section may be skipped without loss of continuity.

23

Atia et al.

5.4.1 Non-surjective mapping

All occupation measures induced by the policies of interest are elements of Q0 (19), that
is, Pr∞
π ∈ X0 := {x : (x, y) ∈ Q0 for some y} if π ∈ ΠCPU, which is aﬃrmed by Lemma 7
stated in Appendix A. However, in general, P ∞(ΠCPU) ⊂ X0, i.e., the set P ∞(ΠCPU) is a
proper subset of X0. In mathematical terms, the mapping (9) between the set of policies
ΠCPU and the set X0 is injective but non-surjective. In turn, there may exist elements of
X0 that are unpaired with policies in ΠCPU. This is illustrated by the following example.

Example 2. Consider the MDP in Figure 2(b). It is easy to see that x for which xs2a2 =
xs3a2 = 1/2, xs1a1 = xs2a1 = xs3a1 = 0 is in X0, i.e., x ∈ X0. However, the only policies in
ΠCPU that satisfy Pr∞
π (s3, a2) = 1 are the deterministic policies π1, π2, which
have π1(a2|s2) = 1, π1(a2|s3) = 0 (for which state s2 is recurrent and s3 is transient) and
π2(a2|s2) = 0, π2(a2|s3) = 1 (for which state s3 is recurrent and s2 is transient). However,
Pr∞

π2(s3, a2) = 1. Thus, x /∈ P ∞(ΠCPU).

π1(s2, a2) = Pr∞

π (s2, a2) + Pr∞

5.4.2 Insufficient constraint set

The set Q0 correctly encodes constraints on the limiting distributions of Markov chains
induced by policies in ΠCPU (with the state classiﬁcation corresponding to r(M) and ¯r(M)).
However, the lack of one-to-one correspondence between feasible solutions and policies (see
Section 4.1) is not fully resolved by the constraint set (19) without the additional constraints
in (20) or (21). In particular, consider the linear program LP0 with feasible set Q0

(LP0) : max

(cid:88)

(cid:88)

s∈S

a∈A(s)

xsa

(cid:88)

s(cid:48)∈S

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) ∈ Q0

(24)

The steady-state distribution of the policy (14) derived from an optimal solution (x∗, y∗)
to LP0 is generally not equal to x∗. In turn, speciﬁcations encoded as constraints on the
state-action variables as in (22) will not necessarily be met by the policy. This is best
illustrated via a simple example.

Example 3. Revisit the three-state example of Figure 2(b) and deﬁne the rewards R(s1, a1) =
R(s1, a2) = R(s2, a1) = R(s3, a1) = 0, R(s2, a2) = R(s3, a2) = 1 and initial distribu-
tion βs1 = 0, βs2 = βs3 = 1/2. The MDP has one TSCC such that, r(M) = r1(M) =
{s2, s3}, ¯r(M) = {s1}. The solution to LP0 in (24) which has x∗
s2a1 =
x∗
s3a1 = 0, x∗
s2a1 = 1/6, is optimal (albeit
not unique). However, the policy π := π(x∗, y∗) has Pr∞
π (s3, a2) = 1/2, hence
in general Pr∞

π (s2, a2) = Pr∞

s2a2 = 1/3, x∗

s3a2 = 2/3, y∗

s3a1 = 0, y∗

s1a2 = x∗

s1a1 = x∗

s1a2 = y∗

s1a1 = y∗

π (cid:54)= x∗ .

5.4.3 Remarks on SSPS(ΠCPU)

1) Note that, in the previous example, the derived policy π /∈ ΠCPU. However, if π :=
π(x∗, y∗) ∈ ΠCPU, where (x∗, y∗) is an optimal solution to (22), then π will be optimal
over ΠCPU ⊇ ΠCP i.e., solves SSPS(ΠCPU). This follows from the optimality of (x∗, y∗) and
Lemma 6 in the appendix, which gives a suﬃcient condition for the existence of a one-to-one
correspondence between the elements of Q0 and the steady-state distribution of policy (14).
2) In general, if we dispense with the ﬂow constraints in (21), we have no guarantee that
the TSCCs rk(M) will be unichain in Mπ under such π. For example, Mπ induced by the

24

Steady-State Planning in Expected Reward Multichain MDPs

policy π given in Example 3 has r1(Mπ) = {2}, r2(Mπ) = {3}, i.e., π /∈ ΠCPU. However, if
the rewards in this example are modiﬁed such that R(s1, a1) (cid:54)= R(s2, a1) while keeping all
other rewards unchanged, then π ∈ ΠCPU. Therefore, under certain suﬃcient conditions on
the reward vector, LP3 in (22) solves SSPS(ΠCPU).

5.4.4 Existence of Optimal Policies

Modiﬁed LP. The feasible set Q1 for LP1 is not compact given the strict inequalities of
constraint (v ) in (20). Therefore, the maximum in (20) may not always be attained on the
set. This can be easily remedied by replacing the strict inequalities with bounded ones via
introducing an arbitrarily small constant (cid:15) > 0 on the right-hand side. Even when such
requirement is not made explicit, a constant (cid:15) is dictated by the numerical precision of
the LP solvers. We deﬁne LP1((cid:15)) similar to (20), with constraints (v ) replaced with the
bounded inequalities in (v)(cid:48) for some (cid:15) > 0,

max

(cid:88)

(cid:88)

s∈S

a∈A(s)

LP1((cid:15))

xsaR(s, a) subject to (x, y) ∈ Q0, (iv),

(25)

(v)(cid:48) xsa ≥ (cid:15), ∀s ∈ rk(M), k ∈ [m], a ∈ A(s) .

Theorem 6 stated next is analogous to Theorem 2 with the modiﬁed program LP1((cid:15));
it establishes that every feasible solution of LP1((cid:15)) yields a policy that is in ΠEP, and
conversely, for every EP policy that meets the steady-state speciﬁcations, there exists an (cid:15) >
0 such that its steady-state distribution is LP1((cid:15))-feasible. Moreover, the policy obtained
from the optimal solution to LP1((cid:15)) solves SSPS(ΠEP), that is, its expected average reward
can be made arbitrarily close to the supremum over the set ΠEP as (cid:15) → 0.

Theorem 6. Given an LMDP M = (S, A, T, R, β, L, Φ∞

L ) and LP1((cid:15)) as in (25), then

(1) The policy π in (14) corresponding to a feasible solution of LP1((cid:15)) is in ΠEP.

(2) If ∃π ∈ ΠEP and π meets the speciﬁcations Φ∞

L , then ∃(cid:15) > 0 such that Pr∞

π is a

feasible solution of LP1((cid:15)).

(3) Let x∗, y∗ be an optimal solution to LP1((cid:15)) and π∗ := π(x∗, y∗) the corresponding

policy in (14). Then,

(cid:18)

lim
(cid:15)→0

sup
π∈ΠEP

R∞

π (β) − R∞

π∗(β)

(cid:19)

= 0

(26)

A similar result holds for CP policies if the maximum in (21) cannot be attained over
the feasible set by transforming constraints (x )–(xiii ) to bounded ones. The generalization
is straightforward, thus omitted for brevity.

Compact policy set – policies with bounded support. The foregoing existence issue
stems from the open set deﬁnition of ΠEP in (15), a result of which is that an optimal policy
from the set (i.e., one that maximizes the average reward) may not always exist. Therefore,
we introduce a slightly modiﬁed deﬁnition next, in which we force a lower bound on the

25

Atia et al.

values a policy assumes on its support, i.e., require that π(a|s) ≥ δ, for some arbitrarily
small constant δ > 0. We formally introduce the deﬁnition of the compact set of policies,
then state a result analogous to Theorem 2 based on this deﬁnition for completeness.

Deﬁnition 19. Given an MDP M and some small δ, where 0 < δ < 1/ maxs∈r(M) |A(s)|,
we deﬁne the set ΠEP(δ) ⊂ ΠEP of EP policies of bounded support as,

ΠEP(δ) = (cid:8)π ∈ ΠS : r(Mπ) = r(M) ∧ π(a|s) ≥ δ, ∀s ∈ r(M), a ∈ A(s)(cid:9) .

(27)

Theorem 7. Given an LMDP M = (S, A, T, R, β, L, Φ∞

L ) and the set ΠEP(δ) in (27),

(1) The policy π in (14) corresponding to a feasible solution of LP1(δ) is in ΠEP(δ).

(2) Let x∗, y∗ be an optimal solution to LP1(δ) and R∗(δ) the average reward of the cor-

responding policy in (14). Then,

lim
δ→0

max
π∈ΠEP(δ)

R∞

π (β) − R∗(δ) = 0.

(28)

According to Theorem 7, every feasible solution to LP1(δ) yields a policy in ΠEP (δ).
Also, the gap between the optimal expected average reward over the set ΠEP (δ) and the
optimal reward of LP1(δ) approaches zero as δ → 0.

6. Extensions

In this section, we explore extensions beyond class-preserving policies, as well as an alter-
native type of speciﬁcations applicable to transient states.

6.1 Beyond Class-Preserving Policies

In this section, we derive an alternative condition given in Theorem 8 under which LP3 in
(22) is guaranteed to yield a stationary policy whose steady-state distribution meets the
desired speciﬁcations. The policy generated need not be in ΠCPU. The proof of Theorem 8
follows from the suﬃcient and necessary optimality conditions of program (22) (Bertsimas
& Tsitsiklis, 1997). The condition is characterized in terms of the rewards vector R =
[R(s, a)], s ∈ S, a ∈ A(s). First, we introduce the following deﬁnition.

Deﬁnition 20 (Cone of feasible directions). The cone V (x, y), where (x, y) is any feasible
solution to LP (22), is deﬁned as

V (x, y) :=






a∈A(s(cid:48)) hs(cid:48)aT (s|s(cid:48), a),
s(cid:48)∈S zs(cid:48)aT (s|s(cid:48), a),

v = (h, z) ∈ R2|S||A| :
a∈A(s) hsa = (cid:80)
(cid:80)
(cid:80)
a∈A(s)(hsa + zsa) = (cid:80)
(cid:80)
(cid:80)
a∈A(s) hsa ≤ 0,
s∈Li
(cid:80)
a∈A(s) hsa ≥ 0,

(cid:80)
(cid:80)

s(cid:48)∈S

s∈Lj
hf a = 0,
hsa ≥ 0,
zsa ≥ 0,

∀s ∈ S,
∀s ∈ S,
i ∈ u(x),
j ∈ l(x),
∀f ∈ ¯r(M), a ∈ A(f ),
∀(s, a) ∈ n(x),
∀(s, a) ∈ m(y)






(29)

26

Steady-State Planning in Expected Reward Multichain MDPs

where u(x) := {i : (cid:80)
{(s, a) ∈ r(M) × A(s) : xsa = 0}, m(y) := {(s, a) ∈ S × A(s) : ysa = 0}.

a∈A(s) xsa = ui}, l(x) := {j : (cid:80)

(cid:80)

(cid:80)

Lj

Li

a∈A(s) xsa = lj}, n(x) :=

Theorem 8. Given LMDP M, let (x, y) be a feasible solution of (22). If R = [R(s, a)], s ∈
S, a ∈ A is an interior point of the dual cone

V ∗(x, y) := {u ∈ R2|S||A| : (cid:104)u, v(cid:105) ≤ 0, for every v ∈ V (x, y)} ,

where (cid:104), (cid:105) denotes the inner product, then the policy π in (14) meets the speciﬁcations Φ∞
L .
Further, π is the unique optimal policy in the class of policies for which ¯r(M) ⊆ ¯r(Mπ).

We remark that the policy could be outside of ΠCPU, but preserves the transience (or
isolation) of the states in ¯r(M). While the statement of Theorem 8 imposes a conservative
assumption on the rewards vector which may be generally hard to verify, it opens up pos-
sibilities for further research on steady-state planning over larger sets of policies (beyond
ΠEP, ΠCP and ΠCPU considered in this paper) – in this case, sets of policies that preserve
the transience of ¯r(M). Ultimately, one would hope to tackle SSPS(Π) for arbitrary sets
of stationary policies Π. These are directions for future investigation.

6.2 Transient Speciﬁcations

In Deﬁnition 12, we introduced speciﬁcations on the steady-state distribution. However,
such speciﬁcations are only useful in the recurrent sets where states are visited inﬁnitely
often. A transient state f ∈ ¯r(Mπ) on the other hand will only be visited a ﬁnite number
of times, i.e., Pr∞
π (f ) = 0 for any stationary policy π ∈ ΠS. In this section, we present an
alternative speciﬁcation type which can be applied to transient states.

We ﬁrst describe a suitable property of transient states against which speciﬁcations can

be applied. We then deﬁne transient speciﬁcations based on this property.

Deﬁnition 21 (Expected number of visits (Kemeny & Snell, 1963)). Given an MDP M
and policy π ∈ ΠS, the expected total number of times that state f ∈ ¯r(Mπ) is visited under
policy π is

ζπ(f ) = βT

¯r(Mπ)(I − Zπ)−1ef .

(30)

labels L =
Deﬁnition 22 (Transient speciﬁcation). Given an MDP and a set of
{L1, . . . , LnL}, where Li ⊆ ¯r(M), a set of transient speciﬁcations is given by Φtr
L =
{(Li, [li, ui])}nL
i=1. Given a policy π, the speciﬁcation (Li, [li, ui]) ∈ Φtr
L is satisﬁed if and
only if (cid:80)
ζπ(f ) ∈ [li, ui]; that is, if the expected number of visits to transient states
f ∈Li
f ∈ Li in the Markov chain Mπ falls within the interval [li, ui].

Suppose that we have a set of labels Ltr over transient states, and a set of transient
speciﬁcations Φtr
Ltr . We can augment the LMDP found in Deﬁnition 12 to incorporate these
transient speciﬁcations as follows. Let L∞ be the set of steady-state labels, and let Φ∞
L∞ be
corresponding steady-state speciﬁcations. We deﬁne a complete set of labels L = (L∞, Ltr)
and speciﬁcations ΦL = (Φ∞

Ltr ), and deﬁne an LMDP as M = (S, A, T, R, β, L, ΦL).

L∞, Φtr

Our next result regarding y and ζπ for the transient states gives a suﬃcient condition

for the policy to meet the transient speciﬁcation.

27

Atia et al.

Proposition 1. Given an MDP M, let (x, y) ∈ Q0 and π as in (14), where Q0 is deﬁned
in (19). If π ∈ ΠCPU, then yf = ζπ(f ) for any state f ∈ ¯r(M).

We remark that this is analogous to Lemma 6 stated in the appendix, which establishes

that if we have a point (x, y) ∈ Q0 for which (14) yields a CPU policy, then Pr∞

π = x.

Given this characterization, we can augment LP1 (20) and LP2 (21) with constraint (xv )

to synthesize policies subject to transient speciﬁcations.

(xv)

li ≤

(cid:88)

(cid:88)

s∈Li

a∈A(s)

ysa ≤ ui, ∀(Li, [li, ui]) ∈ Φtr
Ltr

(31)

7. Numerical Results

In this section, we present a set of numerical results to corroborate the ﬁndings of the
theoretical analysis.
In Section 7.1, we verify the steady-state behavior of the policies
derived from the proposed LPs using the Frozen Islands example of Figure 3, followed by a
study of their behavior in the presence of additional transient speciﬁcations in Section 7.2.
In Section 7.3, we present the results of a study which shows that the empirical steady-
state distributions and average number of state visitations induced by the derived policies
converge to values that meet the desired speciﬁcations.
In Section 7.4, we evaluate the
average reward achieved by said policies and examine the impact of various restrictions in
their respective LPs on the optimal values of the objective using the Toll Collector example
of Figure 13. A case study is also presented featuring the progress of the iterative Algorithm
1 for generating a CPU policy. A natural generalization of the speciﬁcations to the product
space of state-action pairs is presented in Section 7.5. We present two numerical experiments
to support the theoretical ﬁndings of Section 5.4.4 in Section 7.6. Finally, we examine the
scalability of the proposed formulations in Section 7.7, where we present the runtime results
for problems with increasing size conducted in various environments.

7.1 Steady-State Speciﬁcations

In this section, we demonstrate the correct-by-construction behavior of the policies pro-
posed. As an illustrative example, we ﬁrst examine the behavior of a policy in ΠEP using
the Frozen Island example shown in Figure 3. We run our proposed LP1 (20) to calculate
the steady-state distribution Pr∞
π (s), and show the values for the two TSCCs (the two small
islands) in Figure 6.

The heat map gives insight into the means by which the agent satisﬁes the speciﬁca-
tions. After the agent enters an island, it spends a large amount of time in states s33, s36,
s48, s49, s61, and s64, in the sense of asymptotic frequency of visits as given by Pr∞
π (s).
The agent also frequently visits states s36 and s61 to satisfy the steady-state speciﬁca-
tions (Llog1, [0.25, 1]) and (Llog2, [0.25, 1]), respectively. Likewise, to meet speciﬁcations
(Lcanoe1, [0.05, 1.0]), (Lcanoe2, [0.05, 1.0]) (Lﬁsh1, [0.1, 1.0]), (Lﬁsh2, [0.1, 1.0]) the agent often
visits states s33, s49, s48, and s64, respectively. In addition to visiting the aforementioned
states to satisfy the constraints, the agent also visits state s48 over 25% of the time to
maximize its expected reward (recall that R(·, ·, s48) = R(·, ·, s64) = 1).

The right three plots of Figure 7 show the values of Pr∞

π (s) along with the optimal
s obtained from LP1 (20) for SSPS(ΠEP), LP2 for SSPS(ΠCP), and Algorithm 1 for

values x∗

28

Steady-State Planning in Expected Reward Multichain MDPs

Figure 6: Heat maps showing the steady-state probabilities Pr∞
belonging to the two TSCCs of the Frozen Lakes example in Figure 3.

π (s) for states s ∈ r(M)

Figure 7: Example showing that Pr∞
s, s ∈ r(M) for policies in ΠEP, ΠCP and ΠCPU
derived from the proposed LP1, LP2 and Algorithm 1, but not for Kallenberg’s and the
discounted case (discount factor γ = 0.9999).

π (s) = x∗

SSPS(ΠCPU). In each of these, the steady-state distribution matches the one estimated by
the LP for every state. This in fact holds for all state-action pairs as well, i.e., Pr∞ = x∗.
This condition is essential to the proof of Theorems 2, 4 and 5 and ensures that the policy
is both optimal and satisﬁes the steady-state speciﬁcations. We calculate the policy cor-
responding to the optimal solution of LP (4.7.6) by Kallenberg (1983) given in (13) with
the additional speciﬁcation constraints for comparison. As shown in Figure 7 (left), the
derived policy fails to give a steady-state distribution equal to x∗. In addition, we obtain a
policy from the solution to LP (3.5) by Altman (1999) of a discounted reward MDP (with
the additional speciﬁcation constraints) using a discount factor γ = 0.9999. As observed
in the second from the left plot of Figure 7, the steady-state distribution of the derived
π (cid:54)= x∗. For
policy does not match x∗.
each speciﬁcation (Li, [li, ui]) ∈ Φ∞
x∗
s and
Pr∞
π (s), demonstrating that all of the speciﬁcations are met for the pro-
posed methods. For Kallenberg’s and the discounted formulations, however, although x∗
Llog1
satisfy the speciﬁcation, the policy yields steady-state distributions Pr∞
and x∗
π (Llog1)
and Pr∞
π (Lcanoe1) which violate the speciﬁcations (these violations are highlighted with
bold red text). In other words, e(cid:62)x∗
π (Llog1) for
the Kallenberg and discounted formulations. The table also shows the optimal reward R∗
given by our proposed methods, as well as the expected average reward yielded by the
π (s, a)R(s, a). While R∗ obtained by Kallenberg’s for-
policy, i.e., R∞
mulation is larger than that of the EP and CP methods, the proposed LPs produce policies

In Table 1, we show the ramiﬁcations when Pr∞
:= (cid:80)
L , Table 1 shows the values of e(cid:62)x∗
Li

π (Lcanoe1) and e(cid:62)x∗

π (Li) := (cid:80)

a∈A(s) Pr∞

π := (cid:80)

(cid:54)= Pr∞

(cid:54)= Pr∞

Pr∞

Lcanoe1

Lcanoe1

Llog1

s∈Li

s∈Li

(cid:80)

s∈S

29

Atia et al.

SS Speciﬁcations

Method

Logs (≥ 0.25)

Canoes (≥ 0.05)

Fish Rods (≥ 0.1)

Island 1

Island 2

Island 1

Island 2

Island 1

Island 2

x∗ Pr∞

x∗ Pr∞

x∗

Pr∞

x∗

Pr∞

x∗ Pr∞

x∗ Pr∞

Rewards

R∗

R∞
π

CPU
CP
EP
Kallenberg
Discounted (γ = 0.999)
Discounted (γ = 0.9999)

0.25
0.25
0.25
0.25
0.26
0.26
0.25
0.25
0.25
0.25 0.17 0.25
0.25 0.04 0.25
0.25 0.18 0.25 0.15 0.05

0.26
0.21
0.25
0.26
0 0.05 0.0037 0.05 0.0013 0.26
0.04 0.26

0.05
0.05
0.05
0.05
0.05
0.05
0.04 0.05

0.05
0.05
0.05
0.07

0.25
0.25
0.25
0.36

0.05
0.05
0.05
0.05

0.03 0.05

0.26
0.21
0.25
0.19
0.52
0.35

0.10
0.14
0.10
0.10
0.10
0.10

0.10
0.14
0.10
0.14
0.39
0.21

0.3621
0.3621
0.3605
0.3605
0.3547
0.3547
0.3621 0.3278
0.3576 0.9061
0.3617 0.5530

Table 1: Steady-state speciﬁcation comparison. Bold red text indicates violated steady-
state speciﬁcations. Constraints are speciﬁed in the header for each label type.

which yield larger values of R∞. Additionally, as the discount factor γ approaches 1, the
discounted reward does not converge to the expected reward. The policies obtained from
the discounted reward formulation achieve larger rewards R∞ by violating the steady-state
constraints and spending larger proportions of time in the rewarding ﬁshing sites.

7.2 Synthesis for Transient Speciﬁcations

In this section, we demonstrate the behavior of the policies derived subject to transient
speciﬁcations following the framework described in Section 6.2. We again compare the
policies derived from our proposed formulations to that of Kallenberg with regard to meeting
such speciﬁcations for the Frozen Islands example of Figure 3. The labels over states in
¯r(M) are set to Ltools = {s7, s13, s23}, Lgas = {s10, s16}, and Lsupplies = {s2, s15, s29} as
shown in Figure 9 (left). The agent sets out to collect some tools, ﬁll up enough gas, and
pick up the required ﬁshing supplies before transitioning to one of the smaller islands which
correspond to TSCCs. This is reﬂected in the transient speciﬁcations (Ltools, [10, Ntr]),
(Lgas, [12, Ntr]), (Lsupplies, [15, Ntr]) ∈ Φtr
L . These speciﬁcations bound the expected total
number of visitations to certain states in ¯r(M), where Ntr = 200. Figure 8 presents the
values of the expected total number of times a state s ∈ ¯r(M) is visited under policy π,
denoted by ζπ(s), along with the optimal values y∗
s , obtained from Kallenberg’s LP, LP1
(20), LP2 (21), and Algorithm 1. As shown, the results match the expected number of
visitations for the proposed methods for every state.
For each transient speciﬁcation (Li, [li, ui]) ∈ Φtr

y∗
f
and the expected total number of visitations achieved by the policy in corresponding states
ζπ(Li) := (cid:80)
ζπ(f ). As shown, LP1, LP2 and Algorithm 1 yield policies that satisfy the
given speciﬁcations, while the policy derived from the Kallenberg LP does not. The last
column shows the expected total number of visitations achieved by the policy on the larger
(transient) island, where ζπ(¯r(M)) = (cid:80)

f ∈¯r(M) ζπ(f ).
Figure 9 (right) shows a heat map for the expected number of visits to the transient
states, i.e.
In addition to
the constraints (Ltools, [10, Ntr]), (Lgas, [12, Ntr]), and (Lsupplies, [15, Ntr]), we also add the
constraint (¯r(M) \ (Ltools ∪ Lgas ∪ Lsupplies) , [0, 10]) to reduce the amount of time spent in
transient states with no resources. As shown, the agent meets the speciﬁcations largely by
visiting states s13, s16, and s29 to collect tools, gas, and supplies, respectively.

the large island. The policy is calculated using LP1 (20).

L , Table 2 shows e(cid:62)y∗
Li

:= (cid:80)

f ∈Li

f ∈Li

30

Steady-State Planning in Expected Reward Multichain MDPs

Figure 8: Example showing that ζπ(s) = y∗
for Kallenberg’s formulation.

s , s ∈ ¯r(M) for the proposed methods, but not

Figure 9: Distribution of labels on the large island, Ltools = {s7, s13, s23}, Lgas = {s10, s16},
and Lsupplies = {s2, s15, s29} (left). Heat map showing the expected number of visits ζπ(s)
for states s ∈ ¯r(M), i.e., the states belonging to the large island in Figure 3 (left).

Transient Speciﬁcations (Ntr = 200)

Results

Method

Tools (≥ 10) Gas (≥ 12) Supplies (≥ 15)

CPU
CP
EP
Kallenberg

y∗

19.22
18.97
19.32
19.34

ζπ

y∗

ζπ

y∗

15.51
19.22
15.26
18.97
19.32
15.51
5.55 15.53

21.15
15.51
20.67
15.26
15.51
21.15
3.95 21.17

ζπ

R∗

0.3621
21.15
0.3607
20.67
21.15
0.3547
5.82 0.3621

ζπ(¯r(M))
200
200
200
56.5

Table 2: Bold red text indicates violated transient speciﬁcations. Constraints are speciﬁed
in the header for each label type.

31

1234567891011121314151617181920212223242526272829303132Large Island (Transient)Atia et al.

Figure 10: Execution of policy, showing (left) average visits and (right) average reward up
to time n.

7.3 Empirical Study

In this section, we simulate the policies derived from our LPs to show the validity of our
formulations and to further demonstrate the failure of the Kallenberg formulation to yield
optimal rewards and meet speciﬁcations.

Let St and At denote the state and action, respectively, of the Frozen Island example at
time t assuming policy π and initial distribution β. The average number of visits fπ,n and
average reward gπ,n up to time n are deﬁned as

fπ,n(L) =

1
n

n
(cid:88)

t=1

gπ,n =

1L(St),

1L(s) =

(cid:26) 1 s ∈ L
0 s /∈ L

1
n

n
(cid:88)

t=1

R(St, At, St+1).

(32)

(33)

We take an ensemble average over 5000 paths.

First, we solve LP (4.7.6) of Kallenberg (1983). In Figure 10 (left), the solid green line
shows the average number of visits to the states in Llog1 = {s34, s36, s38, s43}, and the hori-
zontal dashed green line indicates the steady-state distribution. The square markers show
the lower bound of the speciﬁcation on the logs. While the value of fπ,n(Llog1) converges
to the steady-state distribution, the policy fails to meet the steady-state speciﬁcation. This
follows from the fact that Pr∞

π (cid:54)= x∗.

Next, we produce an EP policy by executing LP1 (20), using no transient speciﬁcations.
The average number of visits to the states in Llog1 is shown as a solid red line in Figure 10
(left). Not only does the average number of visits converge to the corresponding steady-state

32

𝜙𝜋𝑁tr=5𝑁tr=500No transientconstraintsProposed LP1Kallenberg𝑙𝑙𝑜𝑔1𝜙𝜋𝑁tr=5𝑁tr=500No transientconstraintsProposed LP1Kallenberg𝑅∗𝑔𝜋,𝑛Steady-State Planning in Expected Reward Multichain MDPs

distribution (dashed black line), but the speciﬁcation is met. We observe similar results for
CP and CPU policies as well.

In Figure 10 (right), the solid green and red lines show the average reward for the
Kallenberg LP and our proposed LP1, respectively. The dashed green line indicates R∗ for
Kallenberg’s formulation. As can be seen, for similar reasons as before, the average reward
converges to a reward other than that output by the LP. On the other hand, LP1 converges
to the corresponding LP reward R∗ (black dashed line).

The vertical green and red dashed lines in Figure 10 indicate the average time of entry
into r(M) for LP1 and Kallenberg’s LP, respectively, where the time of entry φπ is given
by

φπ = min{n | Sn ∈ r(M)}.

(34)

In both cases, the agent spends an unduly amount of time in the transient states before
transitioning to a recurrent set, which may be undesirable. To reduce the amount of time
spent in the transient states, we next introduce a transient speciﬁcation (¯r(M), [0, Ntr])
and rerun LP1. The constant Ntr is used to control the time of entry into the recurrent
sets. The results are shown for Ntr = 5 (blue lines) and Ntr = 500 (yellow lines). In both
cases, convergence of the average visits to Pr∞
π (Llog1) occurs at a much faster rate, leading
to a much faster accumulation of reward.

We now comment further on the simulation for Ntr = 5. The policy produced by LP1
separates the ﬁrst small island into two main subsets. The agent tends to visit state s33
repeatedly after entering the ﬁrst small island, leading to an above average number of visits
to log1 states. This results in an initial “overshoot” of Pr∞
π (Llog1). This eﬀect is not seen
for Ntr = 500 due to the averaging eﬀect of fπ,n(L). Likewise, the policy tends to delay the
entry of the agent into state s48 where rewards are accumulated. This delay is especially
noticeable in gπ,n for Ntr = 5 due to the logarithmic time scale.

In the same vein, we explore the simulated behavior of our policies in terms of the number
of visits to transient states, where the number of visits hπ,n(L) to states in L ⊆ ¯r(M) up
to time n is deﬁned as

hπ,n(L) =

n
(cid:88)

t=1

1L(St).

(35)

In Figure 11, we show the number of visits to the transient states for the same policies as
shown in Figure 10. For the policies produced by LP1, hπ,n(¯r(M)) converges to the optimal
y∗
¯r(M). On the other hand, as described in Section 7.2, for the Kallenberg formulation we
have ζπ (cid:54)= y∗ and so the derived policy fails to converge to y∗

¯r(M).

7.4 Comparison of Policies

Recall that policies in ΠEP exercise all transitions in the TSCCs of an MDP. By contrast,
policies in ΠCP and ΠCPU are less restrictive in that they only preserve the state classiﬁcation
and the unichain property of these components, respectively. In turn, they often yield larger
expected rewards while simultaneously satisfying desired speciﬁcations. In this section, we
verify the correctness of such policies and compare their optimal rewards.

33

Atia et al.

Figure 11: Execution of policy with transient speciﬁcations showing the number of visits to
transient states up to time n.

Figure 12 illustrates the Markov chains induced by policies in ΠEP, ΠCP, and ΠCPU,
respectively, for the MDP shown in its ﬁrst column. The policies are obtained from the
optimal solutions of the corresponding LPs. As observed, the Markov chain induced by the
EP policy (Column 2) contains all transitions in the TSCCs of the underlying MDP. The
self loops of states s3,s5, and s9 are missing in the Markov chain induced by the CP policy
without aﬀecting the recurrence of each of the TSCCs. In the case of the Markov chain
induced by the CPU policy, the state s3 is transient in the TSCC {s3, s4, s5}, but all TSCCs
of M remain unichain in the induced Markov chain. That is, each TSCC contains exactly
one recurrent component.

In order to compare the performance of our EP, CP, and CPU policies, we deﬁne the
Toll Collector example given by the LMDP M of Figure 13. In this problem, an agent must
choose one of m cities to visit, each of which corresponds to a TSCC of M. The k-th city
consists of nk, k ∈ [m] counties represented as vertices and roads connecting these counties
represented by edges. The roads with toll booths yield a positive reward for collecting a
toll. However, the agent needs to spend some time on roads without toll booths in order to
build them. We consider an instance of the Toll Collector problem for which m = 3 and the
number of states per TSCC nk = n, ∀k. To highlight the gap between the optimal rewards of
the diﬀerent policies, we deﬁne the labels Lk = {s ∈ rk(M) : R(s, a, s(cid:48)) = 0, ∀a ∈ A(s), s(cid:48) ∈
rk(M)} and Φ∞
L = (Lk, [l, 1]), respectively, ∀k ∈ [m]. As such, per Φ∞
L , the steady-state
probability of states with no rewardful transitions is forced to be bounded below by l. We
will use this steady-state speciﬁcation with various values of l to show that, for lower values
of l, there is a signiﬁcant gap in expected rewards observed by the various policies. As this
l value is increased, the gap can be shown to diminish.

34

𝜙𝜋𝑁tr=5𝑁tr=500No transientconstraintsProposed LP1Kallenbergℎ𝜋,𝑛ҧ𝑟Steady-State Planning in Expected Reward Multichain MDPs

Figure 12: Markov chains induced by the EP, CP, and CPU policies. The ﬁrst column
shows the underlying MDP M = (S, A, T, R, β). Transitions designated with circles have
unit reward, otherwise the reward is 0. The initial distribution β is uniform over S.

Figure 13: Toll Collector problem given by LMDP M = (S, A, T, R, β, L, Φ∞
L ) consisting of
m fully-connected TSCCs rk(M), k ∈ [m] and ¯r(M) = {s0}. The k-th TSCC consists of nk
states. State s0 has m actions, each of which leads to one of the m TSCCs with probability
1. For each state si in rk(M), there are nk − 1 actions, each of which causes a transition
to another state in rk(M) with probability 1. The reward function is deﬁned such that, in
each TSCC, there is a positive reward by taking the action that leads from some state si
to its neighbor si+1 and vice-versa. That is, R(si, ·, si+1) = R(si+1, ·, si) = 1 for some i.
These rewards are designated with red solid circles in each TSCC. All other rewards are 0.
The initial distribution β is uniform over S. The labels and steady-state speciﬁcations are
given by Lk = {s ∈ rk(M) : R(s, a, s(cid:48)) = 0, ∀a ∈ A(s), s(cid:48) ∈ rk(M)} and Φ∞
L = (Lk, [l, 1]),
respectively, for all k ∈ [m].

35

𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠1𝑠2𝑠1𝑠2MDPMarkov Chain -CPURewards𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠1𝑠2Markov Chain -EP𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠1𝑠2Markov Chain -CP𝑎3𝑎1𝑎2𝑎1𝑎4𝑎4𝑎3𝑎2𝑎2𝑎1𝑎2𝑎1𝑎3𝑎1𝑎2𝑎1𝑎1𝑎1𝑎1𝑎3𝑎2𝑎2𝑎2𝑎2𝑎1𝑎1𝑎1𝑎1𝑎1𝑎1𝑎2𝑎2𝑎2𝑎2𝑎2𝑎2𝑠!𝑠"𝑠#𝑠$!TSCC 1𝑠!TSCC 2𝑠$!%!𝑠$!%"𝑠$!%$"TSCC 𝑚RewardsAtia et al.

Figure 14: Comparison of EP, CP, and CPU policies for the Toll Collector example. (Left)
Expected reward as function of the total number of states in each TSCC when l = 0.
(Right) Expected reward as function of the lower bound l on the steady-state probability
when n = 25.

Figure 14 (left) compares the optimal rewards achieved by the diﬀerent policies as a
function of the total number of states in the TSCCs (i.e., 3n) when l = 0. As the number
of states increases, the gap between the average reward of the EP policies and their CP
and CPU counterparts increases. In this scenario, the EP policies incur a quadratic loss
relative to CPU policies since they are forced to exercise all existing transitions equally
and there are O(n2
k) such transitions in the k-th TSCC. On the other hand, an optimal
CPU policy preserves the unichain property while exercising exclusively the two transitions
with positive reward in each TSCC. A smaller loss is incurred by CP policies since they
are only required to preserve the recurrence of the TSCCs and can thus restrict themselves
to visiting the outer perimeter of each TSCC. In doing so, the CP policies incur a linear
loss when compared to the CPU policies because they must visit every state in a TSCC
inﬁnitely often in order to preserve the recurrent classiﬁcation of these states.

Figure 14 (right) illustrates the average rewards as a function of the lower bound l for
the three types of policies when the number of states in each TSCC is n = 25. When l
increases, the average reward gap between the diﬀerent policies diminishes since the agent
has to spend more time in states with no rewards to meet the desired speciﬁcations.

7.4.1 Operation of Algorithm 1

In this subsection, we present in detail the operation of the proposed Algorithm 1 to generate
a CPU policy. Consider the LMDP given in Figure 15. For each iteration, LP (22) is solved
and the digraph of the support of the solution in each TSCC is shown (colored nodes). In
the ﬁrst iteration, for the ﬁrst TSCC, states s4 and s5 form a SCC, while s3 does not belong
to the support of the solution. For both the second and third TSCCs, all respective states
belong to the support but they do not form a SCC, thus we can ﬁnd cut(s) (as can be seen

36

1020304050607080901000.750.80.850.90.95100.10.20.30.40.50.40.50.60.70.80.91Number of States in TSCCsLower Bound on Steady-State ProbabilityExpected RewardSteady-State Planning in Expected Reward Multichain MDPs

at the bottom of the ﬁgure of the second iteration). In the second iteration, the dotted
edges are added which results in one SCC for the second TSCC (no additional constraints
are needed) but not for the third TSCC. Thus, we consider additional cuts as shown at the
bottom of the ﬁgure of the third iteration. In the last iteration, the stopping criteria is met
(the digraph in each of the three TSCCs is strongly connected). The ﬁnal Markov chain
induced by the CPU policy derived from the solution to the LP of the third iteration is
shown on the right side of Figure 15. States {s3, s4, s5} form a unichain component, states
{s6, s7, s8, s9} form a recurrent component, and states {s10, s11, s12, s13, s14, s15} belong to
a TSCC where all edges are preserved.

Figure 15: Illustration of the progress of Algorithm 1 for generating a policy in ΠCPU. The
LMDP M = (S, A, T, R, β, L, Φ∞
L ), where S, A, and R are given in the MDP (ﬁrst column)
labels are Lgold1 = {s4, s5}, Lgold2 = {s6, s7}, Lgold3 = {s10, s11}, and
and β is uniform.
the steady-state speciﬁcations are (Lgold1, [0.20, 1]),(Lgold2, [0.10, 1]), (Lgold3, [0.15, 1]) (ﬁrst
column). In each iteration, we illustrate the support of the optimal solution for the TSCCs
of M (middle columns) along with the edges (state-action pairs) for a given cut. After
three iterations, the support of the optimal solution corresponds to SCCs in each of the
TSCCs. Every TSCC of M is a unichain component in the Markov chain Mπ induced by
the resulting policy (last column).

7.5 Speciﬁcations on State-Action Pairs

Up to this point, we have deﬁned steady-state and transient speciﬁcations over states.
However, the framework proposed can be used to synthesize policies with provably correct
behavior on the level of state-action pairs as well. As an example, consider the LMDP

37

𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠1𝑠2Final Markov ChainIteration -1-𝑠4𝑠5𝑠3𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7Iteration -2-Iteration -3-RewardsLabels𝐴𝑑𝑑𝑒𝑑𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡𝑠𝑇𝑆𝐶𝐶1:.𝑇𝑆𝐶𝐶2:.𝑇𝑆𝐶𝐶3:.𝐴𝑑𝑑𝑒𝑑𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡𝑠𝑇𝑆𝐶𝐶1:.,𝑇𝑆𝐶𝐶2:𝑠8,𝑎1,(𝑠9,𝑎2)𝑇𝑆𝐶𝐶3:𝑠14,𝑎1,(𝑠15,𝑎2)𝐴𝑑𝑑𝑒𝑑𝑐𝑜𝑛𝑠𝑡𝑟𝑎𝑖𝑛𝑡𝑠𝑇𝑆𝐶𝐶1:.,𝑇𝑆𝐶𝐶2:𝑠8,𝑎1,(𝑠9,𝑎2)𝑇𝑆𝐶𝐶3:𝑠14,𝑎1,𝑠15,𝑎2,𝑠11,𝑎1,(𝑠12,𝑎2)𝑠4𝑠5𝑠3𝑠9𝑠8𝑠6𝑠7𝑠11𝑠15𝑠10𝑠12𝑠13𝑠14𝑠1𝑠2MDP𝑎3𝑎1𝑎2𝑎1𝑎4𝑎4𝑎3𝑎2𝑎2𝑎1𝑎2𝑎1𝑎3𝑎1𝑎2𝑎1𝑎1𝑎1𝑎1𝑎3𝑎2𝑎2𝑎2𝑎2𝑎1𝑎1𝑎1𝑎1𝑎1𝑎1𝑎2𝑎2𝑎2𝑎2𝑎2𝑎2Atia et al.

Speciﬁcations

Steady-State

Transient

Pr∞

π (s4, a1) Pr∞
0.11
0.10
0.11

π (s6, a2) Pr∞
0.12
0.12
0.12

π (s10, a1)
0.20
0.20
0.20

ζπ(s2, a1)

ζπ(¯r(M))

26.2
31.03
28.82

50
50
50

Policy

EP
CP
CPU

Table 3: The policies meet transient and steady-state speciﬁcations on state-action pairs
for the MDP deﬁned in Figure 15.

tool = {(s2, a1)}, and L∞

M = (S, A, T, R, β, L, ΦL) deﬁned in Figure 15 (left). We deﬁne labels L = (L∞, Ltr) over
state-action pairs, i.e., Ltr
gold2 = {(s6, a2)}, and
gold3 = {(s10, a1)}. The speciﬁcations are given as ΦL = (Φ∞
L∞
Ltr ), where the steady-
state speciﬁcations are (L∞
gold3, [0.20, 1]), and the
transient speciﬁcations are given as (Ltr
tool, [20, 50]). We also set the average total number
of visitations Ntr = 50. Table 3 shows the steady-state distributions Pr∞
π (s, a) and the
expected number of visitations ζπ(s, a) for the labeled sets, as well as the total number of
visitations ζπ(¯r(M)) to the set ¯r(M) for EP, CP and CPU policies. As shown, the policies
meet both steady-state and transient speciﬁcations deﬁned over the product space S × A.

gold1 = {(s4, a1)}, L∞
L∞, Φtr
gold2, [0.12, 1]), and (L∞

gold1, [0.10, 1]), (L∞

7.6 Modiﬁed LP and Policy Set

Impact of (cid:15) in LP1((cid:15)). In this section, we use the MDP example of Figure 15 to inves-
tigate the impact of the parameter (cid:15) > 0 on the total reward induced by an optimal EP
policy. In particular, we solve LP1((cid:15)) with descending values of (cid:15) and compute the optimal
reward R∗((cid:15)). As shown in Figure 16, R∗((cid:15)) increases monotonically as we decrease (cid:15) with
diminishing return, and converges to nearly 0.36 as (cid:15) → 0. For values of (cid:15) below 10−4,
the change in average reward if we further decrease (cid:15) is insigniﬁcant. Therefore, in our
experiments we have set (cid:15) = 10−4.

Figure 16: Convergence of the average expected reward as we vary the parameter (cid:15) in (25)
for the MDP example of Figure 15.

Policies with bounded support. Here, we verify the result of Theorem 7. Consider
the example of Fig. 2 where R(s2, a1) = R(s3, a1) = R(s3, a2) = 0.1 and R(s2, a2) = 0.5.

38

10-510-410-30.26   0.28   0.3   0.32   0.34   0.36   Steady-State Planning in Expected Reward Multichain MDPs

Recalling that δ is a lower bound on the support of the policies in ΠEP(δ) in (27), we
can show that the optimal average reward over ΠEP(δ) is maxπ∈ΠEP(δ) R∞
π = 0.5(1 − δ)2 +
0.2δ(1 − δ) + 0.1δ2, achieved by the policy π∗ which has π∗(s2|a2) = π∗(s3|a1) = 1 − δ,
and π∗(a1|s2) = π∗(a2|s3) = δ. The reward R∗(δ) of the policy π in (14) obtained from
the optimal solution to LP1(δ) in (25) is R∗(δ) = 0.5 − 1.2δ, where π(a1|s2) = δ/(1 − 2δ),
π(a2|s2) = (1 − δ)/(1 − 2δ) and π(a1|s3) = π(a2|s3) = 1/2. Fig. 17 shows that the diﬀerence
R∞

π∗ − R∗(δ) → 0 as δ → 0 as per Theorem 7.

Figure 17: The diﬀerence R∞
R∗(δ) is the reward of the policy obtained from an optimal solution to LP1(δ).

π∗ − R∗(δ) → 0 as δ → 0, where π∗ = arg maxπ∈ΠEP(δ) R∞

π and

7.7 Scalability

We demonstrate the scalability of the proposed formulations using two sets of experiments.
The ﬁrst set of experiments are performed on a standard desktop with 16GB of RAM using
the Matlab CVX package for convex optimization (Grant & Boyd, 2014, 2008). We also
perform a second set of experiments on a standard desktop of 128GB of RAM using the
commercial CPLEX Optimizer, which provides a higher-precision mathematical solver for
large-scale linear programming.

For the ﬁrst set of experiments, we experiment with instances of increasing size of
the Toll Collector problem (Figure 13), the Frozen Islands environment (Figure 3) and
random partition graphs from the NetworkX library (Hagberg, Swart, & S Chult, 2008).
The Toll Collector problem uses an MDP with three TSCCs, each of size n, while the
Frozen Islands environment consists of an n × n grid. We also experiment with random
MDPs constructed from n-node directed Gaussian partition graphs generated using the
NetworkX toolbox (Hagberg et al., 2008). For such graphs, the cluster sizes are drawn from
a normal distribution with mean and variance n/5, and two nodes within the same cluster
are connected with probability pin, while two nodes in diﬀerent clusters are connected with
probability pout (Brandes, Gaertler, & Wagner, 2003). For these partition graphs, the state
space corresponds to the vertex set, the number of actions is equal to the maximum node
outdegree and the transitions are deterministic. The initial distribution is uniform over
the set ¯r(M) and the rewards are selected such that only the ﬁrst action from every state
yields a positive reward, i.e., R(s ∈ r(M), a1, ·) = 1 and 0 otherwise. An instance of an
MDP constructed from a 40-node Gaussian partition graph is illustrated in Figure 18. The
speciﬁcations for the three environments are given in the caption of Table 4.

39

10-410-310-210-1 0    0.02    0.04   Atia et al.

For each example, we generate EP, CP and CPU policies using LP1, LP2 and Algorithm
1, respectively, and report on the runtime as we increase n. All instances were veriﬁed to
meet the given speciﬁcations. The results are summarized in Table 4 demonstrating the
scalability of the proposed formulations. As shown, LP2 incurs the largest runtime as it
incorporates additional variables in the ﬂow constraints to enforce the recurrence of the
TSCCs.

Figure 18: A NetworkX random 40-node digraph used to generate a LMDP for the third
example of Table 4.

To further examine the scalability of the LPs underlying the diﬀerent policies to much
larger problem sizes, additional experiments are conducted using the CPLEX 12.8 solver.
We run simulations of the LP in (22), LP1 (with the positivity constraints) and LP2 (with
the ﬂow constraints) for random instances of the Frozen Islands problem. The runtime
results are reported in Table 5. For a 64 × 64 and a 128 × 128 grid, LP2 (the most complex)
is solved in about 20 seconds and 15 minutes, respectively, demonstrating the eﬀectiveness
of the developed formulations even for MDPs with over ten thousand states.

8. Conclusion

A framework for steady-state policy synthesis in general MDPs was developed to derive
policies that satisfy constraints on the steady-state behavior of an agent. Linear program-
ming solutions were proposed and their correctness proved for classes of edge-preserving and
class-preserving policies. The framework also enables policies that meet speciﬁed constraints
on the expected number of times the agent visits transient states. Numerical simulations of
the resulting policies demonstrate that our approach overcomes limitations in the literature.
The article provides the ﬁrst solution to the highly understudied problem of steady-
state planning over stationary policies in constrained expected average reward multichain
MDPs. The policies derived come with rigorous guarantees on the asymptotic long-term

40

States in ҧ𝑟()States in 𝑟()Steady-State Planning in Expected Reward Multichain MDPs

Example

Policy

Toll Collector, 3n

Frozen Lake, n × n

Random Gaussian, n

15

30

45

60

75

8 × 8

12 × 12

16 × 16

20 × 20

20

40

60

80

EP
CP
CPU

0.5
1.16
0.45

1.18
3.85
0.87

2.4
8.37
1.54

4.7
15.1
2.48

6.99
24.48
3.72

1.96
5.08
1.97

2.87
25.81
4.77

5.17
108.49
7.6

7.97
431.38
12.62

0.61
4.56
4.8

1.58
5.84
6.74

2.34
19.06
12.04

4.16
35.74
17.35

Table 4: Average runtime results (in seconds) for 20 instances of the Toll Collector, Frozen
Islands, and Gaussian partition graphs of increasing problem size n. The Toll Collector
MDP consists of three TSCCs, each of size n. The detailed LMDP parameters are given
in the caption of Figure 13 with a steady-state speciﬁcation lower bound l = 0.05. The
three-island problem described in Figure 3 forms an n × n grid.
In each of the smaller
islands, logs are randomly distributed over 1/4 of the states and a canoe (ﬁshing rod) is
placed in the top-left (bottom-right) tile. For these experiments, we have the constraints
(Llog1 ∪Llog2, [0.3, 1]), (Lcanoe1 ∪Lcanoe2, [0.05, 1]) and reward function R(·, ·, Lﬁsh1 ∪Lﬁsh2) =
1, R(·, ·, S \ Lﬁsh1 ∪ Lﬁsh2) = 0. For the Gaussian partition graphs, we deﬁne a steady-
state speciﬁcation (L, [0.05, 1]), where L = {si}, for some si ∈ r(M). The probability of
intra-cluster connection pin = 0.9 and the probability of inter-cluster connection pout is
0.05, 0.01, 0.01, 0.005 for the 20, 40, 60, 80 nodes, respectively.

LP

Frozen Islands Example

Size, n × n

8 × 8

16 × 16

32 × 32

64 × 64

128 × 128

LP1 (20)
LP2 (21)
LP3 (22)

0.0001
0.003
0.0001

0.0017
0.038
0.0018

0.0170
0.595
0.0168

0.1187
20.251
0.1553

20.306
933.821
5.425

Table 5: Average runtime (in seconds) of 20 instances per LP for the three-island problem
described in Figure 3. These islands combined form an n × n grid. In each of the smaller
islands, logs are randomly distributed over 1/4 of the states and a canoe (ﬁshing rod) is
placed in the top-left (bottom-right) tile. For these experiments, we have the constraints
(Llog1 ∪Llog2, [0.3, 1]), (Lcanoe1 ∪Lcanoe2, [0.05, 1]) and reward function R(·, ·, Lﬁsh1 ∪Lﬁsh2) =
1, R(·, ·, S \ Lﬁsh1 ∪ Lﬁsh2) = 0.

behavior of agents. The research ﬁndings have bearing on the ﬁelds of explainable, safe and
trustworthy AI, where there is increased concern about explaining AI decisions, ensuring
safety constraints are met, and building trust in the behavior of autonomous agents.

Acknowledgments

This research was supported in part by the Air Force Research Laboratory through the
Information Directorate’s Information Institute® contract number FA8750-20-3-1003 and

41

Atia et al.

FA8750-20-3-1004, the Air Force Oﬃce of Scientiﬁc Research through Award 20RICOR012,
and the National Science Foundation through CAREER Award CCF-1552497 and Award
CCF-2106339.

Appendix A. Technical Lemmas

Proof of Lemma 2

Let π ∈ ΠEP. Hence, r(Mπ) = r(M) according to (15). Consider the set of states rk(M)
in a TSCC of M, for some k ∈ [m]. We will show that this set is also a TSCC of Mπ.
To this end, we ﬁrst show that they form a SCC in the transition graph of Mπ. Since
π ∈ ΠEP, then π(a|s) > 0, ∀s ∈ r(M), a ∈ A(s). Hence, every action a ∈ A(s) available
in state s ∈ rk(M) is played with non-zero probability. From (1), for a pair of states
s, s(cid:48) ∈ rk(M), Tπ(s(cid:48)|s) > 0 if ∃a ∈ A(s) such that T (s(cid:48)|s, a) > 0. Thus, for every directed
path between a pair of nodes in rk(M) in the transition graph of M, there is a similar path
between the same nodes in the transition graph of Mπ. Therefore, the states rk(M) also
form a SCC in the transition graph of Mπ. Also, the set rk(M) is reachable in Mπ since
rk(M) ⊆ r(Mπ). Finally, there are no outgoing edges to states in S \ rk(Mπ) since the
edge set of the transition graph of Mπ is a subset of the edge set of the transition graph
of M. We conclude that rk(Mπ) = rk(M), ∀k ∈ [m]. From the deﬁnition of the set of CP
policies in (16), it follows that π ∈ ΠCP, proving that ΠEP ⊆ ΠCP. The two conditions in
(16) are special cases of the more general requirements in (17), hence ΠCP ⊆ ΠCPU.

The following lemma gives a characterization of the Markov chain state classiﬁcation

induced by a policy (14) derived from a feasible point of the constrained set Q0 in (19).

Lemma 3. Given an MDP M, let (x, y) ∈ Q0 deﬁned in (19) and π := π(x, y) as in (14).
The following holds for the Markov chain Mπ.

(a) If s ∈ ¯r(M), then s ∈ ¯r(Mπ), i.e., ¯r(M) ⊆ ¯r(Mπ).

(b) If s ∈ rk(M) ∩ Ex for some k ∈ [m], then s ∈ r(Mπ). As a consequence, if s ∈

rk(M) ∩ ¯r(Mπ), then s ∈ Ex, i.e., xs = 0.

Proof of Lemma 3

First, we show part (a), according to which every state in ¯r(M) is either transient or isolated
in the Markov chain Mπ induced by a policy of the form (14) derived from a point in Q0.
Consider f ∈ ¯r(M). From constraint (iii ), we have xf = 0. Thus, from constraint (ii ),
(14) and the fact that f is only reachable from states in ¯r(M),

yf = βf +

(cid:88)

(cid:88)

yf (cid:48)aT (f |f (cid:48), a)

f (cid:48)∈¯r(M)
(cid:88)

a∈A(f (cid:48))
(cid:88)

yf (cid:48)

= βf +

T (f |f (cid:48), a)π(a|f (cid:48))

f (cid:48)∈¯r(M)
(cid:88)

a∈A(f (cid:48))
yf (cid:48)Tπ(f |f (cid:48))

= βf +

f (cid:48)∈¯r(M)

42

(36)

Steady-State Planning in Expected Reward Multichain MDPs

Note that the second equality above follows from the deﬁnition of π in (14) for a state
f /∈ Ex. Two cases arise. If f /∈ Ey, then yf = 0. Hence, βf = 0 and Tπ(f |f (cid:48)) = 0, ∀f (cid:48) ∈ Ey.
Thus, we have shown that every state f in Ey ∩ ¯r(M) can only be reached from states in
Ey ∩ ¯r(M), and that all such states have zero initial probability. Thus, every such state is
either isolated or resides in an isolated component. Therefore, f ∈ ¯r(Mπ), where ¯r(Mπ)
consists of transient or isolated states. Now consider the other case where f ∈ Ey, i.e.,
yf > 0. Assume, for the sake of contradiction, that f ∈ r(Mπ). Hence, f ∈ F , for some
TSCC F (this subsumes the case where f is absorbing with |F | = 1). Then, it must be
that F ⊆ ¯r(M) since f is not reachable from states S \ ¯r(M) even under an EP policy.
Summing (36) over the set F , we have

(cid:88)

f (cid:48)∈F

yf (cid:48) =

=

(cid:88)

f (cid:48)∈F
(cid:88)

f (cid:48)∈F

(cid:88)

(cid:88)

βf (cid:48) +

yf (cid:48)Tπ(j|f (cid:48))

j∈F

f (cid:48)∈¯r(M)
(cid:88)

(cid:88)

Tπ(j|f (cid:48)) +

yf (cid:48)

βf (cid:48) +

f (cid:48)∈¯r(M)\F

j∈F

(cid:88)

f (cid:48)∈F

yf (cid:48) ,

(37)

where the second equality is due to the closure of the set F , implying that (cid:80)
j∈F Tπ(j|f (cid:48)) = 1
for f (cid:48) ∈ F . It follows that βf (cid:48) = 0, ∀f (cid:48) ∈ F , and Tπ(f |f (cid:48)) = 0, ∀f (cid:48) ∈ (¯r(M) \ F ) ∩ Ey.
Therefore, F ⊆ ¯r(Mπ), yielding a contradiction. Hence, f ∈ ¯r(Mπ). We conclude that
¯r(M) ⊆ ¯r(Mπ).

Next, we prove part (b) which states that every state s in a TSCC of M for which
xs > 0 is both recurrent and non-isolated in Mπ. Consider a state s ∈ rk(M) ∩ Ex for
some k ∈ [m], so xs > 0. Assume, for the sake of contradiction, that s ∈ ¯r(Mπ), i.e., the
state s is either transient or isolated. If s is transient, then the column of the matrix T ∞
π
corresponding to state s is zero. Therefore, from constraint (i ) in (19), we have xs = 0, i.e.,
s /∈ Ex, yielding a contradiction. If s ∈ F for some isolated component F , then

(cid:88)

s(cid:48)∈F

(xs(cid:48) + ys(cid:48)) =

(cid:88)

(cid:88)

(cid:88)

s(cid:48)∈F

f ∈F

a∈A(f )

yf aT (s(cid:48)|f, a)

by summing constraint (ii ) over states s(cid:48) ∈ F , and using the fact that βf = 0, ∀f ∈ F and
that s(cid:48) ∈ F is only reachable from states in the isolated set F . Since (cid:80)
s(cid:48)∈F T (s(cid:48)|f, a) =
1, ∀f ∈ F, a ∈ A(f ), by the closure of F , we get that (cid:80)
s(cid:48)∈F xs(cid:48) = 0 by interchanging the
order of the sums, i.e., s ∈ Ex, also yielding a contradiction. Hence, s ∈ r(Mπ).

The second clause of Lemma 3 (b) remains to be proved, i.e., s ∈ rk(M) ∩ ¯r(Mπ) =⇒
xs = 0. Consider s ∈ rk(M) ∩ ¯r(Mπ). Thus, s /∈ r(Mπ), so it follows from the result we
have just shown that s ∈ ¯r(M) ∪ Ex. However, since s ∈ rk(M) for some k, then s /∈ ¯r(M).
Hence, s ∈ Ex.

Next, we state and prove two lemmas that will be useful in the proof of Lemma 6, which
establishes a suﬃcient condition for the existence of a one-to-one correspondence between
a feasible point in Q0 and the steady-state distribution of the Markov chain induced by the
policy in (14) derived from this solution.

Lemma 4. Given MDP M, if (x, y) ∈ Q0, where Q0 is the set of points in (19), then x is
a stationary distribution of the Markov chain Mπ induced by the policy π in (14).

43

Atia et al.

Proof of Lemma 4

First, consider s(cid:48) ∈ ¯r(M), and deﬁne X0 := {x : (x, y) ∈ Q0 for some y}. Since x ∈ X0, we
have xs(cid:48) = 0 by constraint (iii ). Also,

xsTπ(s(cid:48)|s) =

(cid:88)

s∈S

(cid:88)

s∈¯r(M)

xsTπ(s(cid:48)|s) = 0 ,

(38)

where the ﬁrst equality holds since s(cid:48) ∈ ¯r(M) is only reachable from states s ∈ ¯r(M)
even when all edges deﬁning possible transitions in the MDP are preserved. Next, consider
s(cid:48) ∈ S \ ¯r(M). We have

xs(cid:48) :=

(cid:88)

xs(cid:48)a =

(cid:88)

(cid:88)

xsaT (s(cid:48)|s, a) =

(cid:88)

(cid:88)

xsπ(a|s)T (s(cid:48)|s, a) =

a∈A(s(cid:48))

s∈S

a∈A(s)

s∈Ex

a∈A(s)

xsTπ(s(cid:48)|s)

(cid:88)

s∈S

(39)

The ﬁrst equality follows from the fact that x ∈ X0, the second from the deﬁnition of π in
(14), and the last from the deﬁnition of Tπ in (1) and that xs = 0, ∀s ∈ S \ Ex. Finally,
x(cid:62)e = 1, by summing constraints (ii ) over all s(cid:48) ∈ S.

Lemma 5. Given an MDP M, let (x, y) ∈ Q0 and π := π(x, y) as in (14). If π ∈ ΠCPU,
then the subvector xrk(Mπ) of x must satisfy the following identity for all k ∈ [m]

rk(Mπ)e = β(cid:62)
x(cid:62)

rk(Mπ)e + β(cid:62)

¯r(Mπ)Pπ,k ,

(40)

where, Pπ,k = [pf k], f ∈ ¯r(Mπ), is the vector of absorption probabilities from ¯r(Mπ) into
rk(Mπ) under policy π.

Proof of Lemma 5

To show (40), note that, since π ∈ ΠCPU, we have that rk(Mπ) ⊆ rk(M), k ∈ [m], where
rk(Mπ) ⊂ r(Mπ) denotes the k-th TSCC of Mπ. Since (x, y) ∈ Q0, by summing constraints
(ii ) in (19) over the set rk(Mπ), we get

(cid:88)

βs =

(cid:88)

xs +

(cid:88)

ys −

(cid:88)

(cid:88)

(cid:88)

T (s|s(cid:48), a)ys(cid:48)a

s∈rk(Mπ)

s∈rk(Mπ)

s∈rk(Mπ)

s∈rk(Mπ)

s(cid:48)∈rk(Mπ)∪¯r(Mπ)

a∈A(s(cid:48))

(41)

where we used the fact that rk(Mπ) is only reachable from states in rk(Mπ) ∪ ¯r(Mπ). By
breaking the summation in the last term on the RHS of (41) over states s(cid:48) in the union of
the disjoint sets rk(Mπ) and ¯r(Mπ) and interchanging the order of the summations over s
and s(cid:48), the last term in (41) simpliﬁes to

(cid:88)

(cid:88)

(cid:88)

ys(cid:48)a

T (s|s(cid:48), a) +

(cid:88)

(cid:88)

(cid:88)

T (s|s(cid:48), a)ys(cid:48)a

s(cid:48)∈rk(Mπ)
(cid:88)

=

a∈A(s(cid:48))

ys(cid:48) +

s∈rk(Mπ)
(cid:88)

ys(cid:48)

s(cid:48)∈¯r(Mπ)

s∈rk(Mπ)

a∈A(s(cid:48))

(cid:88)

Tπ(s|s(cid:48)) ,

s(cid:48)∈rk(Mπ)

s(cid:48)∈¯r(Mπ)

s∈rk(Mπ)

(42)

44

Steady-State Planning in Expected Reward Multichain MDPs

where the ﬁrst term on the RHS of the equality (42) follows from the closure of rk(Mπ)
(which implies that (cid:80)
s∈rk(Mπ) T (s|s(cid:48), a) = 1 for s(cid:48) ∈ rk(Mπ)), and the second term from
the deﬁnition of the policy in (14) for states in Ex (noting that s(cid:48) ∈ ¯r(Mπ) implies xs(cid:48) = 0).
Replacing (42) in (41), we get that

(cid:88)

βs =

(cid:88)

xs −

(cid:88)

ys(cid:48)

(cid:88)

Tπ(s|s(cid:48))

(43)

s∈rk(Mπ)

s∈rk(Mπ)

s(cid:48)∈¯r(Mπ)

s∈rk(Mπ)

We proceed to further simplify the second term on the RHS of (43). Since xs = 0, ∀s ∈

¯r(Mπ), it follows from constraint (ii ) of (19) and (14) that

ys = βs +

(cid:88)

(cid:88)

ys(cid:48)

π(a|s(cid:48))T (s|s(cid:48), a) = βs +

(cid:88)

ys(cid:48)Tπ(s|s(cid:48)), ∀s ∈ ¯r(Mπ) . (44)

s(cid:48)∈¯r(Mπ)

a∈A(s(cid:48))

s(cid:48)∈¯r(Mπ)

In matrix form, this can be rewritten as

y¯r(Mπ) = (I − Z(cid:62)

π )−1β¯r(Mπ) ,

(45)

where Zπ = [zs(cid:48)s] ∈ [0, 1]|¯r(Mπ)|×|¯r(Mπ)|, with zs(cid:48)s := Tπ(s|s(cid:48)). Hence, the second summation
in (43) can be written as

(cid:88)

ys(cid:48)

(cid:88)

s(cid:48)∈¯r(Mπ)

s∈rk(Mπ)

Tπ(s|s(cid:48)) = y(cid:62)Lπ,ke = β(cid:62)

¯r(Mπ)(I − Zπ)−1Lπ,ke ,

(46)

where Lπ,k is the submatrix of Tπ of transitions from ¯r(Mπ) to rk(M) under policy π as
in (5). From (43) and (46),

(cid:88)

s∈rk(Mπ)

xs = β(cid:62)

rk(Mπ)e + β(cid:62)

¯r(Mπ)(I − Zπ)−1Lπ,ke .

(47)

Since the vector Pπ,k is the scaled (by the inverse of ηs) s-th column of the submatrix of the
matrix T ∞
π deﬁning transitions from ¯r(Mπ) to rk(Mπ), we have (Feller, 1968; Puterman,
1994),

Pπ,k = (I − Zπ)−1Lπ,ke ,

(48)

which proves the identity (40) of Lemma 5.

We can readily state the next Lemma which establishes the aforementioned suﬃciency

condition.

Lemma 6. Given an MDP M, let (x, y) ∈ Q0 and π := π(x, y) as in (14). If π ∈ ΠCPU,
then Pr∞

π = x.

Before we prove Lemma 6, we remark that this result also holds for policies in ΠEP and

ΠCP since these are subsets of ΠCPU per Lemma 2.

45

Atia et al.

Proof of Lemma 6

We seek to show that the steady-state distribution of the Markov chain Mπ induced by the
policy π (14) derived from a feasible point (x, y) ∈ Q0 matches x, provided that π is a CPU
policy, where Q0 is as deﬁned in (19).

First, we consider the states in ¯r(Mπ). We have that Pr∞

π (s) = 0, ∀s ∈ ¯r(Mπ) since such
states are either transient or isolated in the Markov chain Mπ induced by policy π. Next, we
argue that xs = 0 for all such states. From Lemma 3 (a), we have that ¯r(M) ⊆ ¯r(Mπ). For
states s ∈ ¯r(M), xs = 0 by constraint (iii ) in (19). Thus, we have shown that Pr∞
π (s) = xs
for every s ∈ ¯r(M). Now, consider a state s ∈ ¯r(Mπ) \ ¯r(M). The state s must belong to
rk(M) ∩ ¯r(Mπ) for some k ∈ [m], where m is the number of TSCCs in M. Hence, xs = 0
by Lemma 3 (b). Therefore, we have argued that xs = Pr∞

π (s) = 0, ∀s ∈ ¯r(Mπ).

Second, we consider states in r(Mπ). According to Lemma 4, x satisﬁes

rk(Mπ) = x(cid:62)
x(cid:62)

rk(Mπ)Tπ,k, ∀k ∈ [m] ,

(49)

where Tπ,k is the submatrix of Tπ of transitions between states in rk(Mπ). We have also
shown that xrk(Mπ) satisﬁes the identity (40) stated in Lemma 5.
π (s) = ηs

π (s) in Lemma 1 and (6), Pr∞

Given the deﬁnition of Pr∞

rk(M)e + β(cid:62)
β(cid:62)

¯r(M)Pπ,k

(cid:17)

(cid:16)

.

Hence,

(cid:88)

s∈rk(Mπ)

Pr∞

π (s) = β(cid:62)

rk(Mπ)e + β(cid:62)

¯r(Mπ)Pπ,k .

From (40) and (50), we conclude that

(cid:88)

Pr∞

π (s) =

(cid:88)

xs.

s∈rk(Mπ)

s∈rk(Mπ)

(50)

(51)

The ergodic theorem of Markov chains asserts that the solution to x(cid:62)T = x(cid:62), where x(cid:62)e =
1, x ≥ 0, is unique iﬀ T is the transition matrix of a unichain (Gallager, 2013; Altman,
1999). From (49), (50) and (51), we have shown that

k Tπ,k = x(cid:62)
x(cid:62)

k , where x(cid:62)

k e = ck, xk ≥ 0

for TSCCs k ∈ [m], where xk := xrk(Mπ), ck is the RHS of (50), and (cid:80)m
k=1 ck = 1. Further,
since π ∈ ΠCPU, every TSCC is a unichain. Hence, by the ergodic theorem, the solution
xrk(Mπ) to (49) and (40) is unique for each component rk(Mπ), k ∈ [m], thus x is equal to
the unique steady-state distribution, i.e., x = Pr∞
π .

We also make use of the following lemma in the proof of the converse part of Theorem
2. The lemma establishes that all occupation measures induced by the policies of interest
are Q0-feasible.

Lemma 7. Given MDP M, let X0 := {x : (x, y) ∈ Q0 for some y}, where Q0 is as deﬁned
in (19). Then, P ∞(ΠCPU) ⊆ X0.

46

Steady-State Planning in Expected Reward Multichain MDPs

Proof of Lemma 7

We show that the steady-state distribution induced by every CPU policy is in X0. To this
end, let x ∈ P ∞(ΠCPU), i.e., ∃π ∈ ΠCPU : Pr∞
π is as deﬁned in Lemma 1.
Therefore, x is a stationary distribution of the Markov chain Mπ, in which rk(Mπ), k ∈ [m]
are TSCCs and states ¯r(Mπ) are either transient or isolated. Hence, x(cid:62) = x(cid:62)Tπ. Therefore,

π = x, where Pr∞

xs(cid:48) :=

(cid:88)

(cid:88)

xs(cid:48)a =

xsTπ(s(cid:48)|s) =

(cid:88)

(cid:88)

xsπ(a|s)T (s(cid:48)|s, a)

a∈A(s(cid:48))
(cid:88)

(cid:88)

=

s∈S
xsaT (s(cid:48)|s, a) ,

s∈S

a∈A(s)

s∈S

a∈A(s)

(52)

where the last equality follows since Pr∞
π (s)π(a|s). Thus, the steady-state
distribution x satisﬁes constraint (i ) in (19). From the deﬁnition of ΠCPU in (17), every
f ∈ ¯r(M) is either transient or isolated under π. Thus, x¯r(M) := {Pr∞
π (f, a)}f ∈¯r(M),a = 0,
satisfying constraint (iii ).

π (s, a) = Pr∞

The variables yf a, f ∈ ¯r(Mπ), a ∈ A(f ), can be set as in (45), i.e., choose yf a =
β(cid:62)
¯r(Mπ)(I − Zπ)−1ef π(a|f ), f ∈ ¯r(Mπ), a ∈ A(f ), where Zπ is the submatrix of Tπ deﬁned
in (5), which satisﬁes the constraints (ii ) as we have already shown in (44). The remaining
variables ysa, s ∈ rk(Mπ), a ∈ A(s), can now be chosen in terms of xsa, yf a, T (s(cid:48)|s, a) and
β such that the corresponding constraints (ii ) are satisﬁed. Thus, for the given x, we have
shown the existence of a feasible y such that (x, y) ∈ Q0. Therefore, x ∈ X0.

Appendix B. Proof of Main Theorems

Proof of Theorem 1

Since (x, y) is a feasible point of LP1, we have that (x, y) ∈ Q0 per (20). From Lemma
3 (a), ¯r(M) ⊆ ¯r(Mπ), thus r(M) ⊇ r(Mπ). Consider a state s ∈ r(M). Then, s ∈
rk(M) for some k ∈ [m]. From the positivity constraint (v ) of LP1, we also have that
xs > 0, i.e., s ∈ Ex. Since s ∈ rk(M) ∩ Ex, it follows that s ∈ r(Mπ) by Lemma 3
(b). Therefore, r(M) ⊆ r(Mπ). We conclude that r(Mπ) = r(M). From constraint (v ),
xsa > 0, ∀s ∈ r(M), a ∈ A(s). It follows from the deﬁnition of π in (14) for states s ∈ Ex
that π(a|s) > 0, ∀s ∈ r(M), a ∈ A(s). We have shown that π satisﬁes both requirements in
(15), hence π ∈ ΠEP.

Proof of Theorem 2

( =⇒ ) First, we show that if (20) is feasible, then there exist an EP policy that meets the
speciﬁcations Φ∞
L . Let (x, y) ∈ Q1 denote a feasible solution to (20) and let π be deﬁned
as in (14). By Theorem 1, π ∈ ΠEP. By Lemma 2, we also have that π ∈ ΠCPU. Invoking
Lemma 6, we conclude that Pr∞
π (s, a) = xsa, s ∈ S, a ∈ A(s), i.e., x is equal to the steady-
state distribution of the Markov chain Mπ induced by policy π. Since x satisﬁes constraint
(iv ), this implies that Mπ meets the speciﬁcations Φ∞
L .
( ⇐= ) Now, we show the converse, that is, the existence of an EP policy that meets the
speciﬁcations implies that LP1 in (20) is feasible. Deﬁne V := {x : (iv) and (v) satisﬁed}.
Thus, we have that XLP1 = X0 ∩ V , where XLP1 = {x : (x, y) ∈ Q1 for some y}. Suppose

47

Atia et al.

π (s) := (β(cid:62)T ∞

π ∈ P ∞(ΠEP) is well-deﬁned as in Lemma 1. We have Pr∞

∃π ∈ ΠEP that satisﬁes the speciﬁcations Φ∞
L as in the statement of Theorem 2. Then
Pr∞
π )s > 0, ∀s ∈
r(Mπ), since all such states are recurrent in the Markov chain Mπ. Since π ∈ ΠEP,
π(a|s) > 0, ∀s ∈ r(M), a ∈ A(s), from (15). Hence, by Lemma 1, Pr∞
π (s, a) > 0, ∀s ∈
r(M), a ∈ A(s). Therefore, Pr∞
π ∈ V . Hence, P ∞(ΠEP) ∩ V is non-empty. Set xsa =
Pr∞
π (s, a), s ∈ S, a ∈ A(s). Recall that ¯r(Mπ) = ¯r(M) since π ∈ ΠEP, so we have xsa =
Pr∞
π (s, a) = 0, ∀s ∈ ¯r(M). From Lemma 7, P ∞(ΠEP) ⊆ X0, where we also use the fact that
P ∞(ΠEP) ⊆ P ∞(ΠCPU) as a consequence of Lemma 2. The variables ysa can be deﬁned
in terms of xsa, T (s(cid:48)|s, a) and β such that the constraints (ii ) are satisﬁed. Hence, XLP1,
and in turn Q1, is non-empty. The optimality of π∗ follows from the optimality of (x∗, y∗),
Theorem 1 and the established equality Pr∞

π∗ = x∗.

Proof of Theorem 3

Let f ∈ ¯r(M). From Lemma 3 (a), f ∈ ¯r(Mπ). Now consider s ∈ rk(M) for some k ∈ [m].
As argued earlier, every state in rk(M) is reachable from s given constraints (viii ), (x ), (xii )
of (21). In addition, s is reachable from all states in rk(M), which follows from constraints
(vii ), (ix ), (xi ), (xiii ). Hence, s ∈ r(Mπ). Therefore, r(M) ⊆ r(Mπ). Since we have
already shown that ¯r(M) ⊆ ¯r(Mπ), we conclude that r(Mπ) = r(M). Therefore, π ∈ ΠCP
deﬁned in (16).

Proof of Theorem 4

π (s, a) = xsa, s ∈ S, a ∈ A(s), which implies that Mπ meets the speciﬁcations Φ∞

The proof follows the same reasoning as that of Theorem 2.
( =⇒ ) Let (x, y, f, f rev) ∈ Q2 denote a feasible solution to (21) and let π be deﬁned
Invoking Lemma 6 and Lemma 2, we have that
as in (14). By Theorem 3, π ∈ ΠCP.
Pr∞
L per
constraint (iv ).
( ⇐= ) Deﬁne V := {x : (iv) and (vi) − (xiv) satisﬁed}. Thus, we have that XLP2 =
X0 ∩ V , where XLP2 = {x : (x, y, f, f rev) ∈ Q2}. Suppose ∃π ∈ ΠCP that satisﬁes the
speciﬁcations Φ∞
L as in the statement of Theorem 4. Hence, rk(Mπ), k ∈ [m] are the
recurrent components of Mπ. Then, Pr∞
π ∈ P ∞(ΠCP) is well-deﬁned as in Lemma 1. We
can set xsa = Pr∞
π (s) for every s ∈ S, a ∈ A(s). The ﬂow variables in
(vi ) - (vii ) can be deﬁned in terms of xsa and T (s(cid:48)|s, a) such that the constraints (x ) - (xiii )
are satisﬁed. Hence, x ∈ V , i.e., P ∞(ΠCP) ∩ V is non-empty. By Lemma 7, XLP2 and Q2
are non-empty. The optimality of π∗ follows from the optimality of (x∗, y∗), Theorem 3 and
the established equality Pr∞

π (s, a) = π(a|s)Pr∞

π∗ = x∗.

Proof of Theorem 5

Assume (x, y) ∈ Q∗. We have that V +
k (x) ⊆ rk(Mπ) for π in (14) by Lemma 3 (b).
Further, consider s ∈ rk(M) ∩ Ex. By constraint (i ) and the deﬁnition of π in (14),
Tπ(s|s(cid:48)) = 0, ∀s(cid:48) ∈ V +
k (x). For the sake of contradiction, assume s ∈ r(Mπ). Hence,
s ∈ F ⊆ rk(M) for some TSCC F of Mπ. Summing constraints (ii ) over the set F , we
get that βs = 0, ∀s ∈ F and Tπ(s|s(cid:48)) = 0, s(cid:48) ∈ ¯r(M), s ∈ F . Hence, s ∈ ¯r(Mπ), yielding a
contradiction. We conclude that V +
k (x) = r(Mπ) ∩ rk(M). Therefore, if for every k ∈ [m]

48

Steady-State Planning in Expected Reward Multichain MDPs

we have that the subgraph (V +
in Mπ, for π in (14). Hence, V +
π ∈ ΠCPU. The result now follows from Lemma 6.

k (x)) is strongly connected, then V +

k (x), E+
k (x) is a SCC
k (x) is the unique TSCC rk(Mπ) in the set rk(M), i.e.,

Proof of Theorem 6

(1) Every feasible solution of LP1((cid:15)) is also LP1-feasible. Hence, the result follows as an
immediate consequence of Theorem 1.
(2) The proof of part (2) follows the same reasoning as in the proof of the converse of The-
orem 2. Speciﬁcally, we have shown that, if π ∈ ΠEP, then Pr∞
π (s, a) > 0, ∀s ∈ r(Mπ), a ∈
A(s). Hence, ∃(cid:15) > 0 such that Pr∞
π ∈ V (cid:48), where V (cid:48) := {x : (iv) and (v)(cid:48) satisﬁed)}. There-
fore, XLP1((cid:15)) := X0 ∩ V (cid:48) is non-empty, and in turn LP1((cid:15)) is feasible.
(3) Let (cid:15)n → 0, n ∈ N, be a monotonically decreasing sequence, π∗
n the EP policy in
(14) corresponding to an optimal solution to LP1((cid:15)n), and Rn := R∞
(β). The sequence
π∗
n
(Rn)n∈N is monotonically non-decreasing since Rn ≥ Rm whenever (cid:15)n < (cid:15)m. Further,
from (11), we have that the sequence is bounded above since supπ∈ΠEP R∞
π (β) ≤ rmax,
where rmax := maxs∈S,a∈A(s) R(s, a). Since the sequence (Rn)n∈N is both increasing and
bounded, it converges to the limit supn Rn by the monotone convergence theorem (Royden
& Fitzpatrick, 2010). We are only left to show that supn Rn = supπ∈ΠEP R∞
π . To this
end, assume for the sake of contradiction that supn Rn < supπ∈ΠEP R∞
π . Since the RHS
of the inequality is the least upper bound on the average reward of EP policies, then for
any δ > 0, ∃π(cid:48) ∈ ΠEP : R∞
π − δ. We can choose δ small enough such that
π(cid:48) is LP1((cid:15)(cid:48))-feasible for all (cid:15)(cid:48) ≤ (cid:15).
R∞
Hence, from the deﬁnition of π∗
π(cid:48) , yielding a contradiction.

π(cid:48) > supn Rn. From part (2) above, ∃(cid:15) > 0, such that Pr∞
n, we get that supn Rn ≥ R∞

π(cid:48) > supπ∈ΠEP R∞

Proof of Theorem 7

(1) Let x be LP1(δ)-feasible. Since the feasible set for LP1(δ) is a subset of the feasible
set of LP1, then π ∈ ΠEP by Theorem 1. Therefore, we only need to verify the bounded
support requirement in (27). For s ∈ r(M), we have that xsa ≥ δ > 0, a ∈ A(s), from
constraint (v)(cid:48) in LP1(δ). Hence, π(a|s) = xsa/xs ≥ δ. Therefore, π ∈ ΠEP(δ).

(2) Assume π ∈ ΠEP(δ) and meets the speciﬁcations Φ∞
L . Noting that ΠEP(δ) ⊂ ΠEP,
then there exists an 0 < (cid:15) ≤ δ such that Pr∞
π is a feasible solution of LP1((cid:15)), which
follows from part (2) of Theorem 6. Hence, maxπ∈ΠEP(δ) R∞
π (β) ≤ R∗((cid:15)) since R∗((cid:15)) is the
optimal value of LP1((cid:15)), where (cid:15) ≤ δ is a function of δ. As δ → 0, the sequence of rewards
R∗(δ) is monotonically non-decreasing and bounded above. Hence, as δ → 0, the sequence
R∗(δ) converges to a limit. Every convergent sequence is a Cauchy sequence (Royden &
Fitzpatrick, 2010), i.e., the elements of the sequence become arbitrarily close to each other
as δ → 0. Hence, R∗((cid:15)) − R∗(δ) → 0, as δ → 0.

Proof of Theorem 8

The cone V (x, y) in (29) is the cone of feasible directions from a feasible point (x, y),
i.e., directions v = (h, z) along which ∃λ > 0 such that (x, y) + λ(h, z) is feasible. The
sets u(x), l(x), n(x) and m(y) denote the sets of active (upper and lower) speciﬁcation

49

Atia et al.

and non-negativity (of state-action variables x and y) constraints, respectively. Since the
rewards vector R is an interior point of the dual cone V ∗(x, y) designated in the statement of
Theorem 8, moving away from (x, y) along any feasible direction can only reduce the value
of the objective, i.e., (cid:80)
a∈A(s) R(s, a)hsa < 0. Hence, (x, y) is the unique optimal
solution to (22). We have already shown that the set of occupation measures induced by
policies for which ¯r(M) ⊆ ¯r(Mπ) is contained in the feasible set of (22). Since π in (14)
is one such policy by Lemma 3 (a), we have Pr∞
π = x and π meets the speciﬁcations Φ∞
L .
The uniqueness of π in this class of policies follows from the established uniqueness of the
optimal solution x.

(cid:80)

s∈S

Proof of Proposition 1

By Lemma 3, we have that f ∈ ¯r(Mπ). If π ∈ ΠCPU, then the condition of Lemma 6 is
met, and it follows from (45) that yf = β(cid:62)

¯r(Mπ)(I − Zπ)−1ef = ζπ(f ).

References

Akshay, S., Bertrand, N., Haddad, S., & H´elou¨et, L. (2013). The steady-state control
problem for Markov decision processes. In International Conference on Quantitative
Evaluation of Systems, pp. 290–304, Berlin Heidelberg. Springer.

Altman, E. (1998). Constrained Markov decision processes with total cost criteria: La-
grangian approach and dual linear program. Mathematical Methods of Operations
Research, 48 (3), 387–417.

Altman, E. (1999). Constrained Markov decision processes. CRC Press, Boca Raton.

Altman, E., Boularouk, S., & Josselin, D. (2019). Constrained Markov decision processes
with total expected cost criteria. In Proceedings of the 12th EAI International Con-
ference on Performance Evaluation Methodologies and Tools, pp. 191–192. ACM.

Atia, G., Beckus, A., Alkhouri, I., & Velasquez, A. (2020). Steady-state policy synthesis
in multichain Markov decision processes.
In Proceedings of the 29th International
Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 4069–4075. International Joint
Conferences on Artiﬁcial Intelligence Organization.

Ayala, A. M., Andersson, S. B., & Belta, C. (2014). Formal synthesis of control policies for
continuous time markov processes from time-bounded temporal logic speciﬁcations.
IEEE Transactions on Automatic Control, 59 (9), 2568–2573.

Baiocchi, D. (2010). Confronting Space Debris: Strategies and Warnings from Comparable

Examples Including Deepwater Horizon. Rand Corporation.

Baumgartner, P., Thi´ebaux, S., & Trevizan, F. (2018). Heuristic search planning with
multi-objective probabilistic LTL constraints. In Sixteenth International Conference
on Principles of Knowledge Representation and Reasoning, pp. 415–424.

Bertsekas, D. (2005). Dynamic programming and optimal control, Vol. 2. Athena Scientiﬁc,

Belmont, Mass.

Bertsimas, D., & Tsitsiklis, J. (1997). Introduction to Linear Optimization (1st edition).

Athena Scientiﬁc.

50

Steady-State Planning in Expected Reward Multichain MDPs

Bhatnagar, S., & Lakshmanan, K. (2012). An online actor–critic algorithm with function
approximation for constrained Markov decision processes. Journal of Optimization
Theory and Applications, 153 (3), 688–708.

Boussemart, M., & Limnios, N. (2004). Markov decision processes with asymptotic average
failure rate constraint. Communications in Statistics-Theory and Methods, 33 (7),
1689–1714.

Boussemart, M., Limnios, N., & Fillion, J. (2002). Non-ergodic Markov decision processes
with a constraint on the asymptotic failure rate: general class of policies. Stochastic
models, 18 (1), 173–191.

Brafman, R. I., & De Giacomo, G. (2019). Planning for LTLf /LDLf goals in non-Markovian
fully observable nondeterministic domains. In Proceedings of the Twenty-Eighth In-
ternational Joint Conference on Artiﬁcial Intelligence (IJCAI), pp. 1602–1608. Inter-
national Joint Conferences on Artiﬁcial Intelligence Organization.

Brandes, U., Gaertler, M., & Wagner, D. (2003). Experiments on graph clustering algo-

rithms. In European Symposium on Algorithms, pp. 568–579. Springer.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba,

W. (2016). OpenAI Gym. arXiv preprint arXiv:1606.01540.

Camacho, A., & McIlraith, S. A. (2019). Strong fully observable non-deterministic planning
with LTL and LTLf goals. In Proceedings of the Twenty-Eighth International Joint
Conference on Artiﬁcial Intelligence (IJCAI), pp. 5523–5531. International Joint Con-
ferences on Artiﬁcial Intelligence Organization.

Courcoubetis, C., & Yannakakis, M. (1995). The complexity of probabilistic veriﬁcation.

Journal of the ACM, 42 (4), 857–907.

De Ghellinck, G. (1960). Les probl`emes de d´ecisions s´equentielles. Cahiers du Centre

d’Etudes de Recherche Op´erationnelle, 2 (2), 161–179.

De Giacomo, G., Felli, P., Patrizi, F., & Sardina, S. (2010). Two-player game structures for
generalized planning and agent composition. In Twenty-Fourth AAAI Conference on
Artiﬁcial Intelligence.

Denardo, E. V., & Fox, B. L. (1968). Multichain Markov renewal programs. SIAM Journal

on Applied Mathematics, 16 (3), 468–487.

Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, Inc.,

Orlando, FL, USA.

Engesser, T., Bolander, T., & Nebel, B. (2017). Cooperative epistemic multi-agent planning
with implicit coordination. In Proceedings of the 3rd Workshop on Distributed and
Multi-Agent Planning (DMAP), p. 68.

Feinberg, E. A. (2009). Adaptive computation of optimal nonrandomized policies in con-
strained average-reward MDPs. In IEEE Symposium on Adaptive Dynamic Program-
ming and Reinforcement Learning, pp. 96–100.

Feinberg, E. A. (2000). Constrained discounted Markov decision processes and Hamiltonian

cycles. Mathematics of Operations Research, 25 (1), 130–140.

51

Atia et al.

Feller, W. (1968). An Introduction to Probability Theory and its Applications (3rd edition).,

Vol. 1. Wiley.

Gallager, R. G. (2013). Stochastic Processes: Theory for Applications. Cambridge University

Press, New York.

Grant, M., & Boyd, S. (2008). Graph implementations for nonsmooth convex programs.
In Blondel, V., Boyd, S., & Kimura, H. (Eds.), Recent Advances in Learning and
Control, Lecture Notes in Control and Information Sciences, pp. 95–110. Springer-
Verlag Limited.

Grant, M., & Boyd, S. (2014). CVX: Matlab software for disciplined convex programming,

version 2.1..

Guo, M., & Zavlanos, M. M. (2018). Probabilistic motion planning under temporal tasks

and soft constraints. IEEE Transactions on Automatic Control, 63 (12), 4051–4066.

Hagberg, A., Swart, P., & S Chult, D. (2008). Exploring network structure, dynamics, and
function using networkx. Tech. rep., Los Alamos National Lab.(LANL), Los Alamos,
NM (United States).

Jamroga, W. (2004). Strategic planning through model checking of atl formulae. In In-
ternational Conference on Artiﬁcial Intelligence and Soft Computing, pp. 879–884.
Springer.

Jha, S., Raman, V., Sadigh, D., & Seshia, S. A. (2018). Safe autonomy under perception un-
certainty using chance-constrained temporal logic. Journal of Automated Reasoning,
60 (1), 43–62.

Kallenberg, L. C. M. (1983). Linear programming and ﬁnite Markovian control problems.

Mathematisch Centrum, Amsterdam.

Kemeny, J., & Snell, J. L. (1963). Finite Markov chains. Springer-Verlag, New York.

Krass, D., & Vrieze, O. J. (2002). Achieving target state-action frequencies in multi-
chain average-reward Markov decision processes. Mathematics of Operations Research,
27 (3), 545–566.

Kwiatkowska, M., & Parker, D. (2013). Automated veriﬁcation and strategy synthesis for
In Automated Technology for Veriﬁcation and Analysis, pp.

probabilistic systems.
5–22. Springer.

Lakshmanan, K., & Bhatnagar, S. (2012). A novel Q-learning algorithm with function ap-
proximation for constrained Markov decision processes. In 50th Annual Allerton Con-
ference on Communication, Control, and Computing (Allerton), pp. 400–405. IEEE.

Lazar, A. (1983). Optimal ﬂow control of a class of queueing networks in equilibrium. IEEE

Transactions on Automatic Control, 28 (11), 1001–1007.

Lindemann, L., & Dimarogonas, D. V. (2017). Robust motion planning employing signal
temporal logic. In 2017 American Control Conference (ACC), pp. 2950–2955. IEEE.

Manne, A. S. (1960). Linear programming and sequential decisions. Management Science,

6 (3), 259–267.

52

Steady-State Planning in Expected Reward Multichain MDPs

Nilsson, P., Hussien, O., Balkan, A., Chen, Y., Ames, A. D., Grizzle, J. W., Ozay, N.,
Peng, H., & Tabuada, P. (2015). Correct-by-construction adaptive cruise control: Two
approaches. IEEE Transactions on Control Systems Technology, 24 (4), 1294–1307.

Norris, J. R. (1997). Markov Chains. Cambridge Series in Statistical and Probabilistic

Mathematics. Cambridge University Press.

Petrik, M., & Zilberstein, S. (2009). A bilinear programming approach for multiagent

planning. Journal of Artiﬁcial Intelligence Research, 35, 235–274.

Pistore, M., Bettin, R., & Traverso, P. (2014). Symbolic techniques for planning with
extended goals in non-deterministic domains. In Sixth European Conference on Plan-
ning.

Privault, N. (2018). Understanding Markov Chains: Examples and Applications. Springer

Singapore, Singapore.

Puterman, M. (1994). Markov decision processes : discrete stochastic dynamic programming.

Wiley, New York.

Ross, K. W. (1989). Randomized and past-dependent policies for Markov decision processes

with multiple constraints. Operations Research, 37 (3), 474–477.

Royden, H., & Fitzpatrick (2010). Real Analysis (4th edition). Pearson.

Schwarting, W., Alonso-Mora, J., & Rus, D. (2018). Planning and decision-making for
autonomous vehicles. Annual Review of Control, Robotics, and Autonomous Systems,
1 (1), 187–210.

Skwirzynski, J. K. (1981). New concepts in multi-user communication, Vol. 43. Springer

Science & Business Media.

Song, L., Feng, Y., & Zhang, L. (2015). Planning for stochastic games with co-safe objec-
tives. In Twenty-Fourth International Joint Conference on Artiﬁcial Intelligence.

Tarjan, R. E. (1972). Depth-ﬁrst search and linear graph algorithms. SIAM Journal on

Computing, 1, 146–160.

Tian, Z. (2019). United states law and policy on space debris. In Space Security and Legal

Aspects of Active Debris Removal, pp. 155–167. Springer.

Trevizan, F., Thi´ebaux, S., & Haslum, P. (2017). Occupation measure heuristics for proba-
bilistic planning. In Twenty-Seventh International Conference on Automated Planning
and Scheduling.

Trevizan, F., Thi´ebaux, S., Santana, P., & Williams, B. (2016). Heuristic search in dual
space for constrained stochastic shortest path problems. In Twenty-Sixth International
Conference on Automated Planning and Scheduling.

Trevizan, F., Thi´ebaux, S., Santana, P., & Williams, B. (2017). I-dual: solving constrained
ssps via heuristic search in the dual space. In Proceedings of the 26th International
Joint Conference on Artiﬁcial Intelligence, pp. 4954–4958.

Velasquez, A. (2019). Steady-state policy synthesis for veriﬁable control. In Proceedings
of the 28th International Joint Conference on Artiﬁcial Intelligence, pp. 5653–5661.
AAAI Press.

53

Atia et al.

Wongpiromsarn, T., Topcu, U., Ozay, N., Xu, H., & Murray, R. M. (2011). TuLiP: a
software toolbox for receding horizon temporal logic planning. In Proceedings of the
14th International Conference on Hybrid Systems: Computation and Control, pp. 313–
314. ACM.

Wu, J., & Durfee, E. H. (2010). Resource-driven mission-phasing techniques for constrained
agents in stochastic environments. Journal of Artiﬁcial Intelligence Research (JAIR),
38, 415–473.

Zhou, Y., Maity, D., & Baras, J. S. (2016). Timed automata approach for motion planning
using metric interval temporal logic. In 2016 European Control Conference (ECC),
pp. 690–695. IEEE.

54

