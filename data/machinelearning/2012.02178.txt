1
2
0
2

t
c
O
3
2

]
I

A
.
s
c
[

2
v
8
7
1
2
0
.
2
1
0
2
:
v
i
X
r
a

Steady-State Planning in Expected Reward Multichain MDPs

Steady-State Planning in Expected Reward Multichain
MDPs

George K. Atia
Department of Electrical and Computer Engineering
Department of Computer Science
University of Central Florida, FL 32816, USA

Andre Beckus
Air Force Research Laboratory, NY 13441, USA

Ismail Alkhouri
Department of Electrical and Computer Engineering
University of Central Florida, FL 32816, USA

Alvaro Velasquez
Air Force Research Laboratory, NY 13441, USA

george.atia@ucf.edu

andre.beckus@us.af.mil

ialkhouri@knights.ucf.edu

alvaro.velasquez.1@us.af.mil

Abstract

The planning domain has experienced increased interest in the formal synthesis of
decision-making policies. This formal synthesis typically entails ï¬nding a policy which
satisï¬es formal speciï¬cations in the form of some well-deï¬ned logic. While many such logics
have been proposed with varying degrees of expressiveness and complexity in their capacity
to capture desirable agent behavior, their value is limited when deriving decision-making
policies which satisfy certain types of asymptotic behavior in general system models. In
particular, we are interested in specifying constraints on the steady-state behavior of an
agent, which captures the proportion of time an agent spends in each state as it interacts for
an indeï¬nite period of time with its environment. This is sometimes called the average or
expected behavior of the agent and the associated planning problem is faced with signiï¬cant
challenges unless strong restrictions are imposed on the underlying model in terms of the
connectivity of its graph structure. In this paper, we explore this steady-state planning
problem that consists of deriving a decision-making policy for an agent such that constraints
on its steady-state behavior are satisï¬ed. A linear programming solution for the general case
of multichain Markov Decision Processes (MDPs) is proposed and we prove that optimal
solutions to the proposed programs yield stationary policies with rigorous guarantees of
behavior.

1. Introduction

The proliferation and mass adoption of automated solutions in recent years has led to an
increased concern in the veriï¬cation, validation, and trust of the prescribed agent behavior
(Schwarting, Alonso-Mora, & Rus, 2018). This motivates the need for traditional techniques
which can yield guarantees of behavior. The study of such techniques has largely been
the focus of areas such as formal synthesis and planning, where it is common to derive
decision-making policies for agents acting in an environment such that some given formal
speciï¬cation is satisï¬ed. The majority of prior art in this area entails the coupling of
some formal logic with the model of agent-environment dynamics in order to ï¬nd optimal

1

 
 
 
 
 
 
Atia et al.

policies which satisfy speciï¬cations expressed in said logic. Examples include planning
with Linear Temporal Logic (LTL) (Guo & Zavlanos, 2018), probabilistic LTL (PLTL)
(Kwiatkowska & Parker, 2013), LTL over ï¬nite traces (LTLf ) (Camacho & McIlraith, 2019),
Linear Dynamic Logic (LDLf ) (Brafman & De Giacomo, 2019), Computation Tree Logic
(CTL) (Pistore, Bettin, & Traverso, 2014), probabilistic CTL (PCTL) (Song, Feng, &
Zhang, 2015), Signal Temporal Logic (STL) (Lindemann & Dimarogonas, 2017), Chance-
Constrained Temporal Logic (C2TL) (Jha, Raman, Sadigh, & Seshia, 2018), Continuous
Stochastic Logic (CSL) (Ayala, Andersson, & Belta, 2014), Âµ-Calculus (De Giacomo, Felli,
Patrizi, & Sardina, 2010), Metric Temporal Logic (MTL) (Zhou, Maity, & Baras, 2016),
and logic fragments, such as the Rank-1 Generalized Reactivity (GR[1]) formulas of LTL
(Wongpiromsarn, Topcu, Ozay, Xu, & Murray, 2011). Formal multi-agent planning has
also been explored using Dynamic Epistemic Logic (Engesser, Bolander, & Nebel, 2017)
and Alternating-time Temporal Logic (ATL) (Jamroga, 2004).

The use of the foregoing logics has facilitated the growth of solutions to the aforemen-
tioned planning problems and are a good conduit for verifying, explaining, and yielding
provably correct agent behavior and, consequently, establishing a measure of trust. How-
ever, these logics are either insuï¬ƒcient to reason about the asymptotic behavior that is
captured by the steady-state distribution of the agent as it follows some decision-making
policy, or existing solutions to the corresponding planning problems make strong assump-
tions on the underlying model of the system. Solutions to these challenges have gained
Indeed, there has been increased interest in what we refer to
traction in recent years.
as the steady-state planning problem of computing decision-making policies that satisfy
constraints on the resulting steady-state behavior. In particular, progress has been made
in easing the restrictions required on the agent-environment dynamics model, usually ex-
pressed in the form of a Markov Decision Process (MDP), in order to derive a solution
policy. In this paper, we advance the state-of-the-art in steady-state planning by establish-
ing the ï¬rst solution to steady-state planning in multichain MDPs such that the resulting
stationary policy satisï¬es constraints imposed on the steady-state distribution of the agent.
Our approach also dissolves assumptions of ergodicity or recurrence of the underlying MDP
which are often made in the literature when reasoning about steady-state distributions.

Steady-state planning has applications in several areas, such as deriving maintenance
plans for various systems, including aircraft maintenance, where the asymptotic failure rate
of components must be kept below some small threshold (Boussemart & Limnios, 2004;
Boussemart, Limnios, & Fillion, 2002). Optimal routing problems for communication net-
works have also been proposed in which data throughput must be maximized subject to
constraints on average delay and packet drop metrics (Lazar, 1983). This includes con-
straints on steady-state network behavior, which include steady-state network frequency
and steady-state phase or timing errors (Skwirzynski, 1981). There is also the potential
of leveraging solutions in the steady-state planning problem space to the design of intelli-
gent space satellites. Indeed, this is an area where the steady-state distribution of debris
following some orbit can be computed to reason about the probability of a satellite collid-
ing with said debris. Such information has been used to determine human-driven control
policies for tasks such as debris mitigation or debris removal (Tian, 2019), sometimes via
remote-controlled robots (Baiocchi, 2010) that are amenable to automated approaches.

2

Steady-State Planning in Expected Reward Multichain MDPs

The steady-state planning problem has been studied under various names, including
steady-state control (Akshay, Bertrand, Haddad, & HÂ´elouÂ¨et, 2013), average- or expected-
reward constrained MDPs (Altman, 1999), and steady-state policy synthesis (Velasquez,
2019). As pointed out by Altman, Boularouk, and Josselin (2019), solutions to this prob-
lem often require strong assumptions on the ergodicity of the underlying MDP. These as-
sumptions facilitate the search for eï¬ƒcient algorithms by leveraging the one-to-one corre-
spondence between the optimality of solutions to various mathematical programs and the
optimality of policies derived thereof. This has been studied at length in the works of
Derman (1970), Kallenberg (1983), Puterman (1994), and Altman (1999), who have de-
rived mathematical programs for discounted, total, and expected reward formulations of
constrained MDPs. In particular, the work of Kallenberg laid the foundation for Marko-
vian control within the context of multichain constrained MDPs. However, it was noted
that deriving optimal policies for the expected-reward formulation was intractable by their
approach and there was no guarantee of agent behavior in terms of satisfying steady-state
constraints.

Summary of contributions. We make four main contributions. First, we introduce
the Steady-State Policy Synthesis (SSPS) problem of ï¬nding a policy from a predeï¬ned
subset of stationary policies in a multichain MDP that maximizes an expected reward
signal while enforcing asymptotic behavior that is correct-by-construction (Nilsson, Hussien,
Balkan, Chen, Ames, Grizzle, Ozay, Peng, & Tabuada, 2015) â€“ in the sense that our policies
yield provably correct behavior that satisï¬es the imposed speciï¬cations on the steady-state
distribution of the Markov chain induced by said policies. Our framework generalizes the
steady-state planning problems studied by Akshay et al. (2013) and Velasquez (2019), as
we do not impose any restrictions on the underlying MDP. In particular, we dispense with
the strong assumption made by Akshay et al. (2013) about the ergodicity of the MDP,
according to which every deterministic policy necessarily induces an ergodic Markov chain
(i.e., one that is recurrent and aperiodic). In sharp contrast to the work of Velasquez (2019),
we do not restrict our search to stochastic policies that induce an irreducible Markov chain
(i.e., one in which all states form one communicating class). In general, such a chain may
not even exist â€“ normally, many states in a given MDP are inevitably transient. Our search
space consists of subsets of the stationary policies that we term edge- or class-preserving,
which, apart from a transient phase, restrict the long-term play in the terminal components
of the given MDP. We introduce two distinct notions for class preservation that yield policies
with diï¬€erent characteristics. These notions will be made precise in Section 4.

As our second contribution, we develop a scalable approach to synthesize policies that
provably meet said asymptotic speciï¬cations through novel linear programming formula-
tions. While a tractable solution to the SSPS problem has heretofore remained elusive and
existing solutions require an enormous amount of calculations with no provable guarantees
(Kallenberg, 1983), two key ideas underlie our ability to tackle the associated combinato-
rial diï¬ƒculties. The ï¬rst idea is the aforementioned restriction of the domain to edge- or
class-preserving policies, which can be provably obtained from solutions to simple linear
programs (LPs). The second idea is to encode constraints on the limiting distributions of
the corresponding Markov chains in formulated LPs, whose solutions yield optimal policies
maximizing the expected average reward while meeting desired asymptotic speciï¬cations on

3

Atia et al.

the limit points of the expected state-action frequencies. These LPs are crafted to capture
designated state classiï¬cations, absorption probabilities in closed communicating compo-
nents, and recurrence constraints within such components, along with the steady-state
speciï¬cations.

Our third contribution lies in deriving key theoretical results establishing provable per-
formance and behavior guarantees for the derived policies. Contracting or transient MDP
models that use the expected total reward as the optimality criterion are commonplace in
constrained MDPs since optimal stationary policies with regard to this criterion can always
be found via mathematical programming in view of a well-established one-to-one correspon-
dence between stationary policies and feasible solutions to such programs (Altman, 1998;
Feinberg, 2000; Wu & Durfee, 2010; Petrik & Zilberstein, 2009). The notoriously more
diï¬ƒcult and equally important expected average reward criterion is much less understood
considering that such correspondence ceases to exist for general multichain MDPs. In this
paper, we tap into this long-standing dilemma and establish such one-to-one correspondence
for classes of stationary policies that are edge- or class-preserving. Theorems 1, 2, 3 and 4
establish the correctness of linear programs yielding optimal policies from said classes. The
proof of these theorems rest on few intermediate results. In particular, Lemma 3 character-
izes the Markov chains induced by the policies of interest, while Lemma 7 establishes the
feasibility of the steady-state distributions induced by these policies. Lemma 6 gives a suf-
ï¬cient condition for the existence of a one-to-one correspondence between feasible solutions
to the linear programs and the stationary policies derived from these solutions. Theorem 5
establishes an existence condition of policies found on a more relaxed notion of class preser-
vation, which inspires a constructive approach in Algorithm 1 to compute such policies.
Theorem 8 gives a generic suï¬ƒcient condition for the existence of an optimal stationary
policy meeting the desired speciï¬cations beyond class-preserving ones.

As our fourth contribution, we introduce an alternative type of speciï¬cations applicable
in transient states. By augmenting our LPs with appropriate constraints, the synthesized
policies provably meet speciï¬cations on the expected number of visitations to transient
states simultaneously with the foregoing steady-state speciï¬cations on the asymptotic fre-
quency with which recurrent states are visited (Proposition 1).

We verify the theoretical ï¬ndings of our work using a comprehensive set of numerical
experiments performed in various environments. The results demonstrate the correctness
of the proposed LPs in yielding policies with provably correct behavior and the scalability
of the proposed solutions to large problem sizes.

This article brings in and substantially extends the scope of our recent work (Atia
et al., 2020), which considered policy synthesis over edge-preserving policies. Such policies
constitute only a small subset of the policies considered herein. A particularly appealing
characteristic of the newly introduced policies is their greater ability to avert MDP transi-
tions of low return without violating the asymptotic constraints. In turn, they yield larger
expected rewards relative to their edge-preserving counterparts â€“ in some cases, we show
that this gain can be substantial. Further, we derive general characterizations of optimality
over a larger class of policies obtained in terms of the MDP reward signal. In addition,
this article advances the aforementioned form of transient speciï¬cations that a policy can
provably meet together with the steady-state ones. We provide a complete presentation
of the steady-state planning problem through linear programming formulations over diï¬€er-

4

Steady-State Planning in Expected Reward Multichain MDPs

ent families of policies, mathematical analyses establishing correctness of such formulations
with optimality guarantees, and a comprehensive set of numerical experiments in diverse
environments to support the theoretical ï¬ndings.

To the best of our knowledge, this work is the ï¬rst to allow synthesis of stationary

policies with provably correct steady-state behavior in general multichain MDPs.

Organization: The paper is organized as follows. Notation and preliminaries are covered
in Section 2. Related work in steady-state planning is summarized in Section 3. The SSPS
problem is formalized in Section 4. We describe our linear programming approach and
present the results of our theoretical analysis in Section 5. Transient speciï¬cations and
extensions to a larger class of policies are presented in Section 6. Numerical experiments
are presented in Section 7 to validate our approach and demonstrate its scalability to large
problems. Concluding remarks are presented in Section 8.
In Appendix A, we present
statements and proof of technical lemmas. The proof of the main results are deferred to
Appendix B.

2. Preliminaries and Notation

We introduce some notation and preliminary deï¬nitions used throughout the paper. For
a matrix A, aij and A(i, j) are used interchangeably to denote the element in its ith row
and jth column. The vectors e and es denote the vectors (of appropriate dimension) of all
ones, and all zeros except for the sth entry, respectively. Given a vector x and index set
V , the vector xV is the vector with entries xv, v âˆˆ V , where xv is the entry corresponding
to index v. By |S|, we denote the cardinality of a set S. For an integer n > 0, the set
[n] := {1, . . . , n}, and A \ B denotes the set diï¬€erence of sets A and B. The symbols âˆƒ
and âˆƒ! mean â€œthere existsâ€ and â€œthere exists a uniqueâ€, respectively, and (cid:62) is the transpose
operator.

Deï¬nition 1 (Markov chain). A Markov chain is a stochastic model given by a tuple
M = (S, T, Î²), where S is the state space, T the transition function T : S Ã— S â†’ [0, 1] with
T (s(cid:48)|s) denoting the probability of transitioning from state s to state s(cid:48), and Î² : S â†’ [0, 1]
the initial state distribution. With slight abuse of notation, the transition function can also
be thought of as a matrix T âˆˆ [0, 1]|S|Ã—|S|, where T (s, s(cid:48)) = T (s(cid:48)|s). The use of T will be
clear from the context.

Classiï¬cation of states
(Norris, 1997; Privault, 2018): Given a ï¬nite Markov chain
M = (S, T, Î²), we say state s(cid:48) is accessible from state s if (T t)(s, s(cid:48)) > 0, for some t > 0,
where T t is the tâˆ’step transition matrix, i.e., if there is a positive probability of transitioning
to state s(cid:48) starting from state s in some number of steps. Two states are said to communicate
if they are both accessible from each other. Communication is an equivalence relation which
partitions the Markov chain M into communicating classes such that only members of the
same class communicate with each other. A class is closed if the probability of escaping
the class is zero. A state s âˆˆ S is said to be transient if, starting from s, there is a non-
zero probability of never returning to s. A set of transient states is termed a transient
set. Non-transient states are called recurrent, that is, state s is recurrent if, starting from
s, the probability of returning to state s after some number of steps is one. A Markov
chain for which there is only one communicating class consisting of the entire state space is

5

Atia et al.

called irreducible, whereas a Markov chain that has a single closed communicating class and
(possibly) some transient states is termed unichain. A state is periodic with period k if any
return to state s must occur in multiples of k time steps, where k is some integer greater
than 1. An example illustrating the classiï¬cation of states in a Markov chain is shown in
Figure 1.

Transience and recurrence describe the likelihood of returning to a state conditioned on
starting from that state, regardless of the initial state distribution Î². Given Î², we also
deï¬ne an isolated component I as a maximal set of states in M that can never be visited,
that is, Î²I = (cid:80)
sâˆˆI Î²s = 0, where Î²s is the initial probability of being in state s, and I
cannot be reached from any state in S \ I, i.e., (cid:80)
s(cid:48)âˆˆI T (s(cid:48)|s) = 0, âˆ€s âˆˆ S \ I. In Figure
1, the set of states {s3, s4} is isolated. The term â€˜reachableâ€™ refers to states that are not
isolated.

Deï¬nition 2 (Markov decision process (MDP)). An MDP is a tuple M = (S, A, T, R, Î²),
in which S denotes the state space, A the set of actions, T : S Ã— A Ã— S â†’ [0, 1] the transition
function with T (s(cid:48)|s, a) denoting the probability of transitioning from state s to state s(cid:48) under
action a, R : S Ã— A Ã— S â†’ R a reward obtained when action a is taken in state s and we
end up in state s(cid:48), and Î² : S â†’ [0, 1] the initial distribution. By A(s) âŠ† A, we denote the
set of actions available in state s.

Deï¬nition 3 (Transition graph). We deï¬ne the transition graph of an MDP M =
(S, A, T, R, Î²) as the directed graph whose vertex set is the state space S, and in which
there is a directed edge from vertex s to vertex s(cid:48) if there exists an action a âˆˆ A(s) such that
T (s(cid:48)|s, a) > 0. The transition graph of a Markov chain M = (S, T, Î²) is the directed graph
with vertex set S, and which has a directed edge from vertex s to vertex s(cid:48) if T (s(cid:48)|s) > 0.

Deï¬nition 4 (Terminal strongly connected component (TSCC)). Consider the transition
graph of a Markov chain or MDP M with state space S and initial distribution Î². A strongly
connected component (SCC) of the digraph is a maximal subset of vertices C, where for every
pair of vertices s, s(cid:48) âˆˆ C, there is a directed path1 from s to s(cid:48) and a directed path from s(cid:48) to
s (Tarjan, 1972). A Terminal Strongly Connected Component (TSCC) S(cid:48) âŠ† S is an SCC
reachable from some initial state s, Î²s > 0 and with no outgoing transitions to any state in
S \S(cid:48). A TSCC is also called a bottom SSC (Courcoubetis & Yannakakis, 1995). We denote
by rk(M) âŠ† S the set of states in the kth TSCC of M, and by r(M) = (cid:83)
kâˆˆ[m] rk(M) the
union of all such sets. The complement set is denoted Â¯r(M) := S \ r(M), which in the case
of Markov chains is the set of transient or isolated states.

Figure 1 illustrates a Markov chain with two TSCCs (highlighted with two separate

colors).

Deï¬nition 5 (Stationary policy). Given MDP M = (S, A, T, R, Î²), a stationary policy
Ï€ : S â†’ âˆ†A is a mapping of states to probability distributions over the space of actions A,
where âˆ†A is the probability simplex over A. The policy Ï€ speciï¬es the conditional probability
Ï€(a|s) that action a is taken in state s. The set of all stationary policies is denoted Î S.

1. There is a directed path from node v to node w if it is possible to reach w from v by traversing the

directed edges in the directions in which they point.

6

Steady-State Planning in Expected Reward Multichain MDPs

Figure 1: State classiï¬cation in a Markov chain with four communicating classes. The set
{s1, s2} is transient, and the sets {s3, s4}, {s5, s6, s7, s8} and {s9, s10, s11} are recurrent.
This Markov chain is not irreducible since the states do not belong to one communicating
class. States s9, s10, and s11 are periodic. The set {s3, s4} is isolated since it is not reachable
from states in S \ {s3, s4} and it has zero initial distribution. The components colored in
green and blue are the two TSCCs of the Markov chain, i.e., r1(M) = {s5, s6, s7, s8} and
r2(M) = {s9, s10, s11}.

Deï¬nition 6 (Markov chain induced by policy). The tuple MÏ€ = (S, TÏ€, Î²) is the Markov
chain induced by a policy Ï€ in an underlying MDP M = (S, A, T, R, Î²), where

TÏ€(s(cid:48)|s) =

(cid:88)

sâˆˆA(s)

T (s(cid:48)|s, a)Ï€(a|s)

(1)

Deï¬nition 7 (Unichain and multichain MDP). An MDP is called unichain (Puterman,
1994; Altman, 1999) if every stationary deterministic policy induces a Markov chain that is
unichain, that is, consists of exactly one recurrent set and possibly some transient states2.
An MDP is said to be multichain if it is not unichain. See Figure 2 and its caption for an
example.

Deï¬nition 8 (Stationary distribution). Given a Markov chain M = (S, T, Î²), a stationary
distribution Prâˆ : S â†’ [0, 1] over the state space is any solution to the set of equations
(Norris, 1997)

Prâˆ(s) =

(cid:88)

s(cid:48)âˆˆS

Prâˆ(s(cid:48))T (s|s(cid:48)), Prâˆ(s) â‰¥ 0, âˆ€s âˆˆ S

Prâˆ(s) = 1 .

(cid:88)

sâˆˆS

(2)

(3)

According to the ergodic theorem of Markov chains, the solution to (2) and (3) is unique
if and only if T is the transition matrix of a unichain (Gallager, 2013, Chapter 4). If there are

2. This deï¬nition does not require the recurrent class to be ergodic (hence aperiodic). Our analysis dispenses

with the aperiodicity precondition as will be clear in the sequel.

7

s1s1s1s1s1s1s2s2s1s2s2s3s4s5s6s7s8s9s10s11Isolated (recurrent) componentTransient statesTSCCTSCCRecurrent statesÎ²   = Î²   = 0s3s4Î²s= 1/9, âˆ€sâˆˆSâˆ–{s3,s4}Recurrent statesAtia et al.

Figure 2: (a) Unichain MDP: every deterministic policy induces a Markov chain that has
exactly one recurrent component. (b) Multichain MDP: adding the self-loop to state s3
yields a multichain MDP. For example, the deterministic policy deï¬ned by Ï€(a1|s1) =
Ï€(a2|s2) = Ï€(a2|s3) = 1 induces the Markov chain in (c), which is multichain with two
recurrent components {s2} and {s3}.

multiple recurrent classes, then in general there will be many stationary distributions. For
example, for the Markov chain of Figure 2(c), one can verify that the distribution Prâˆ(s1) =
Prâˆ(s2) = 0, Prâˆ(s3) = 1 and the distribution Prâˆ(s1) = Prâˆ(s3) = 0, Prâˆ(s2) = 1 both
satisfy (2) and (3), thus they are both stationary distributions of the Markov chain. Note
that a stationary distribution may not be representative of the true steady-state behavior
of the system (c.f. Deï¬nition 10 and the following example).

Deï¬nition 9 (Stationary matrix of a Markov chain). Given a Markov chain M = (S, T, Î²),
the stationary matrix T âˆ is given by the Ces`aro limit3 (Puterman, 1994)

T âˆ = lim
nâ†’âˆ

1
n

n
(cid:88)

t=1

T t .

(4)

Given a ï¬nite multichain Markov chain M = (S, T, Î²) with transient set F and recurrent
(i.e., non-transient) components Ek, k âˆˆ [m], the transition matrix T can be expressed in
the canonical form (Puterman, 1994, Appendix A)

T =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£¯
ï£°

0
T1
T2
0
...
...
0
0
L1 L2

. . .

0
0
. . .
0
...
...
. . .
. . . Tm 0
. . . Lm Z

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

(5)

where the matrices Tk correspond to transitions between states in Ek, Lk to transitions from
states in F to states in Ek, k âˆˆ [m], and Z to transitions between states in F . Similarly,

3. The Ces`aro limit always exists and accounts for the non-convergence of powers of transition matrices
of periodic chains. Hence, we do not need a precondition about aperiodicity as our analysis does not
require that T âˆ = limnâ†’âˆ T n.

8

s1s3a1a2a2a1a1s2s1s3a1a2a2a2a1a1s2(a) Unichain MDP(b) Multichain MDPs1s3s2s2s3(c) Induced Markov chainÏ€(a1|s1) = Ï€(a2|s2) = Ï€(a2|s3) = 1Steady-State Planning in Expected Reward Multichain MDPs

we use TÏ€,k, LÏ€,k, k = 1, . . . , m, and ZÏ€ to denote the corresponding submatrices of the
transition matrix TÏ€ of the Markov chain MÏ€ induced by policy Ï€. Also (Puterman, 1994;
Kallenberg, 1983),

T âˆ(s(cid:48), s) =

ï£±
ï£²

Î·s,
ps(cid:48)kÎ·s
ï£³
0

s(cid:48), s âˆˆ Ek for some k âˆˆ [m]
s(cid:48) âˆˆ F, s âˆˆ Ek
otherwise

(6)

(cid:80)n

1
n

where, Î·s = limnâ†’âˆ
in s from initial states s(cid:48) âˆˆ Ek, (cid:80)
transient state s(cid:48) âˆˆ F into the recurrent class Ek, k âˆˆ [m], and (cid:80)m

t=1 T t(s(cid:48), s), is the long term proportion of time the chain spends
Î·s = 1, ps(cid:48)k is the absorption probability from the

k=1 ps(cid:48)k = 1, âˆ€s(cid:48) âˆˆ F .

sâˆˆEk

In this paper, we are interested in the asymptotic behavior of an agentâ€™s policy in an
MDP, as captured by the steady-state distribution of the induced Markov chain deï¬ned
next.

Deï¬nition 10 (Steady-state distribution). Given an MDP M and policy Ï€, the steady-
state distribution Prâˆ
: S Ã— A â†’ [0, 1] over the state-action pairs, also known as the
Ï€
occupation measure (Altman, 1999, Chapter 4), is the long-term proportion of time spent
in state-action pair (s, a) as the number of transitions approaches âˆ, i.e.,

Prâˆ

Ï€ (s, a) = lim
nâ†’âˆ

1
n

n
(cid:88)

t=1

Pr(St = s, At = a|Î², Ï€),

s âˆˆ S, a âˆˆ A(s)

(7)

if the limit exists, where St and At are the state and action at time t. Also, Prâˆ
(cid:80)

Ï€ (s) :=
Ï€ (s, a) is the steady-state probability of being in state s âˆˆ S. The steady-state

aâˆˆA(s) Prâˆ

distribution is a stationary distribution of the Markov chain induced by the policy Ï€.

As an example, consider the MDP in Figure 2(b) with Î²s1 = 1, Î²s2 = Î²s3 = 0. The
steady-state distribution of the policy Ï€(a1|s1) = Ï€(a2|s2) = Ï€(a2|s3) = 1, which induces
the Markov chain in Figure 2(c), has Prâˆ

Ï€ (s2, a2) = 1 and 0 otherwise.

Deï¬nition 11. Given an MDP M = (S, A, T, R, Î²) and a set of policies Î  âŠ† Î S, we deï¬ne

P âˆ(Î ) := {Prâˆ

Ï€ |Ï€ âˆˆ Î }

as the set of occupation measures induced by policies in Î , where Prâˆ

Ï€ is deï¬ned in (7).

Deï¬nition 12 (Steady-state speciï¬cations and constraints (Velasquez, 2019)). Given an
MDP M = (S, A, T, R, Î²) and a set of labels L = {L1, . . . , LnL}, where Li âŠ† S, a set
of steady-state speciï¬cations is given by Î¦âˆ
i=1. Given a policy Ï€, the
speciï¬cation (Li, [li, ui]) âˆˆ Î¦âˆ

L = {(Li, [li, ui])}nL
L is satisï¬ed if and only if the steady-state constraint

li â‰¤

(cid:88)

sâˆˆLi

Prâˆ

Ï€ (s) â‰¤ ui

(8)

is satisï¬ed; that is, if the steady-state probability of being in a state s âˆˆ Li in the Markov
chain MÏ€ falls within the interval [li, ui].

9

Atia et al.

Deï¬nition 13 (Labeled MDP (Velasquez, 2019)). An MDP M = (S, A, T, R, Î², L, Î¦âˆ
L )
augmented with the label set L and speciï¬cations Î¦âˆ
L is termed a labeled MDP (LMDP).

Lemma 1. (Kallenberg, 1983, Theorem 4.3.2)(Krass & Vrieze, 2002) Given an MDP M =
(S, A, T, R, Î²) and policy Ï€ âˆˆ Î S, the steady-state distribution Prâˆ
Ï€ (s, a)}s,a of the
Markov chain MÏ€ is

Ï€ := {Prâˆ

Prâˆ

Ï€ (s, a) = (Î²(cid:62)T âˆ

Ï€ )sÏ€(a|s), s âˆˆ S, a âˆˆ A(s)

(9)

where T âˆ

Ï€ is the Ces`aro limit in (4), i.e., T âˆ

Ï€ = limnâ†’âˆ

1
n

(cid:80)n

t=1 T t
Ï€.

Deï¬nition 14 (Expected average reward). Given an MDP M = (S, A, T, R, Î²), the ex-
pected average reward Râˆ

Ï€ (Î²) of a policy Ï€ is deï¬ned as

Râˆ

Ï€ (Î²) = lim

inf
nâ†’âˆ

1
n

n
(cid:88)

t=1

EAtâˆ¼Ï€
S0âˆ¼Î²

[R(St, At)]

(10)

where R(s, a) := (cid:80)
the probability
measure induced by the initial distribution Î² and the policy Ï€ over the state-action trajec-
tories.

s(cid:48)âˆˆS T (s(cid:48)|s, a)R(s, a, s(cid:48)), and the expectation is w.r.t.

It follows from the deï¬nition of the expected average reward in (10) and the steady-state

distribution (7) that for a stationary policy Ï€ (Krass & Vrieze, 2002; Altman, 1999)

Râˆ

Ï€ (Î²) =

(cid:88)

(cid:88)

sâˆˆS

aâˆˆA(s)

Prâˆ

Ï€ (s, a)R(s, a)

(11)

where Prâˆ

Ï€ (s, a) is given in (9).

The primary focus of this paper in the context of steady-state planning is to ï¬nd station-
ary policies that maximize the expected average reward (10) while satisfying speciï¬cations
Î¦âˆ
L on the steady-state distribution (see Deï¬nition 12). We restrict the search to certain
classes of stationary policies which will be introduced and deï¬ned precisely in Section 4.
Our solution approach to this constrained MDP problem is based on linear programming
formulations, which optimize a linear objective function capturing the expected reward, sub-
ject to linear equality and inequality constraints. Such constraints encode restrictions on
the steady-state distributions induced by policies of interest, as well as desired steady-state
speciï¬cations on the long-term frequencies for state-actions pairs. The decision variables
of the LPs correspond to the occupation measures, and policies are obtained from their
optimal solutions. We establish a one-to-one correspondence between optimal solutions of
said LPs and optimal policies of the constrained MDP problem.

3. Related Work

Research related to steady-state planning often comes from the ï¬eld of average- or expected-
reward constrained MDPs and has its roots in mathematical programming (Bertsekas,
2005). Many solutions proposed in this area utilize linear programming formulations to
derive policies (Altman, 1999). We illustrate these formulations in order of increasing com-
plexity and elucidate the key diï¬€erences between the formulations in the literature and our

10

Steady-State Planning in Expected Reward Multichain MDPs

own. First, let us consider the simple problem of deriving a policy for an agent which seeks
to maximize expected reward without any constraints on its steady-state distribution.

In the unichain MDP case, one may synthesize a policy by solving a linear program (LP)

of the form (Manne, 1960; De Ghellinck, 1960)

max

(cid:88)

(cid:88)

sâˆˆS
(cid:88)

aâˆˆA(s)
(cid:88)

sâˆˆS

aâˆˆA(s)

xsa

(cid:88)

s(cid:48)âˆˆS

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to

xsaT (s(cid:48) | s, a) =

(cid:88)

xs(cid:48)a

aâˆˆA(s(cid:48))

xsa âˆˆ [0, 1]
(cid:88)

(cid:88)

xsa = 1 .

âˆ€s(cid:48) âˆˆ S

(12)

âˆ€s âˆˆ S, a âˆˆ A(s)

sâˆˆS

aâˆˆA(s)

The policy can be derived from the occupation measures given by xsa through a simple
calculation.
It is worth noting that this combination of occupation measures and linear
programming has enabled signiï¬cant progress in the area of planning within stochastic
shortest paths MDPs, where several occupation measure heuristics have been deï¬ned to ï¬nd
decision-making policies that maximize the probability of reaching a set of goal states while
satisfying multiple cost constraints (Trevizan, ThiÂ´ebaux, Santana, & Williams, 2016, 2017;
Trevizan, ThiÂ´ebaux, & Haslum, 2017; Baumgartner, ThiÂ´ebaux, & Trevizan, 2018). While
the LP in (12) always produces valid solutions for unichain MDPs, this is not necessarily
the case for multichain MDPs due to the fact that there may be more than one ergodic set
(Puterman, 1994). This issue is rectiï¬ed by modifying LP (12) to obtain (Denardo & Fox,
1968; Kallenberg, 1983)

max

(cid:88)

(cid:88)

sâˆˆS
(cid:88)

aâˆˆA(s)
(cid:88)

sâˆˆS
(cid:88)

aâˆˆA(s)
(cid:88)

sâˆˆS

aâˆˆA(s)

xsa

(cid:88)

s(cid:48)âˆˆS

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to

xsaT (s(cid:48) | s, a) =

(cid:88)

xs(cid:48)a

ysaT (s(cid:48) | s, a) =

aâˆˆA(s(cid:48))
(cid:88)

(xs(cid:48)a + ys(cid:48)a) âˆ’ Î²s(cid:48)

aâˆˆA(s(cid:48))

âˆ€s(cid:48) âˆˆ S

âˆ€s(cid:48) âˆˆ S

(13)

xsa âˆˆ [0, 1], ysa â‰¥ 0

âˆ€s âˆˆ S, a âˆˆ A(s).

The new ysa variables guide policy formation on the transient states. Both LP (12) and
LP (13) yield stationary stochastic policies. Furthermore, there always exists at least one
optimal deterministic policy, which can easily be derived from the stochastic policy solution
obtained from the LPs (Puterman, 1994).

For producing control policies with steady-state speciï¬cations, LPs (12) and (13) are
extended to include linear steady-state constraints on the occupation measures. When
applied to unichain MDPs, the constrained version of LP (12) encounters minor diï¬ƒculties,
in that there may not be an optimal deterministic policy (Altman, 1999). Nonetheless, the
LP always produces an optimal stochastic stationary policy. In fact, there exists an optimal
policy having at most nL â€œrandomizationsâ€, i.e. having at most |S| + nL state-action pairs
with non-zero probability of being selected (Ross, 1989).

11

Atia et al.

On the other hand, serious issues arise when LP (13) is augmented with steady-state con-
straints and solved for multichain MDPs, as described in the pioneering work of Kallenberg
(1983). In particular, it was shown that there is not a one-to-one correspondence between
the feasible solutions of the augmented LP and the stationary policies. Instead, the space of
feasible solutions is partitioned into equivalence classes of various feasible solutions mapping
to the same policy. The key deï¬ciency is that the steady-state distribution of the Markov
chain induced by the synthesized policy does not match the optimal solution to the LP in
general, and so the derived policy does not always meet the steady-state speciï¬cations (see
Example 1 in Section 4.1). This issue is not easily remedied, since the optimal solution
may not be achievable by any stationary policy, or identifying such a policy would generally
require combinatorial search. We refer the reader to the paper by Krass and Vrieze (2002)
for an overview.

In order to mitigate the preceding problem of integrating steady-state constraints, vari-
ous assumptions have been made in the literature on the structure of the underlying MDP.
Multichain MDPs are also frequently excluded from the conversation altogether. The as-
sumption that the MDP is ergodic, and therefore every policy induces an ergodic Markov
chain, has been used by Akshay et al. (2013) to ensure that steady-state equations and
constraints on the same are satisï¬ed. This assumption is relaxed to some extent by Ross
(1989), Altman (1999), Feinberg (2009), where unichain MDPs are allowed. The assump-
tion of either an ergodic or a unichain MDP requires that no stationary deterministic policy
induce more than a single recurrent class, thus severely limiting the applicability of these
methods. These assumptions are removed in the recent work of Velasquez (2019), where
neither ergodic nor recurrence assumptions are made on the underlying MDP. However, the
solution proposed therein ï¬nds an irreducible Markov chain in the underlying MDP, if one
exists, and is therefore suitable for communicating MDPs where, for any two states s and
s(cid:48), there exists a deterministic stationary policy such that s can reach s(cid:48) in a ï¬nite number
of steps (Puterman, 1994). This solution, however, is too restrictive, thus not suitable for
reasoning over general multichain MDPs.

Another approach taken to address these challenges is to simply allow solutions to take
the form of non-stationary policies. In the work of Kallenberg (1983), this is accomplished
by a computationally expensive approach producing a potentially diï¬€erent policy in each
time step. Another approach, proposed by Krass and Vrieze (2002), starts by using one
policy, and then switches to a second â€œtailâ€ stationary policy. The time at which the switch
occurs is determined by a lottery performed at each time step, and once the switch occurs
the tail policy continues to be used indeï¬nitely (thus the policy is â€œultimatelyâ€ stationary
once the switch occurs). However, this approach has three key limitations. First, the
constraints must take the form of a target frequency vector, which imposes an equality
constraint on the steady-state distribution over all states. Second, the lottery system does
not guarantee that the switch will occur in a ï¬nite number of steps, thus meaning that the
policy is not guaranteed to be ultimately stationary. Third, the policy depends on a marker
to track whether or not the switch has occurred. This marker is not part of the MDP, and
therefore the MDP machinery must be modiï¬ed to include a so-called marker-augmented
history. As an alternative, the authors also propose a way to extend the given MDP with
additional states, such that the problem can be solved using a stationary policy applied to

12

Steady-State Planning in Expected Reward Multichain MDPs

the extended MDP. However, this approach still cannot produce a stationary policy to solve
the original problem.

While most methods for solving constrained MDPs revolve around the use of math-
ematical programs, some reinforcement learning approaches have also been proposed for
optimizing the average-reward objective and, to a lesser extent, for solving constrained
instances of average-reward MDPs. Some noteworthy examples include the constrained
actor-critic method proposed by Bhatnagar and Lakshmanan (2012), wherein a Lagrangian
relaxation of the problem is used to incorporate steady-state costs into the objective function
being optimized by the constrained actor-critic algorithm. A similar Lagrangian Q-learning
approach is proposed by Lakshmanan and Bhatnagar (2012). Both of these reinforcement
learning methods assume that every Markov chain induced by a policy is irreducible, which
allows only a single recurrent class as with ergodic and unichain assumptions described
earlier. The Lagrangian approach has also been applied to speciï¬c stochastic policy linear
programming formulations relevant to aircraft maintenance problems where the asymptotic
failure is to be kept below some small threshold (Boussemart & Limnios, 2004; Boussemart
et al., 2002).

In contrast to the foregoing eï¬€orts, our approach is computationally tractable, works
with the most general multichain MDPs, and always produces a stationary policy that
satisï¬es the given steady-state speciï¬cations, if one exists. Additionally, none of the afore-
mentioned methods consider constraints on the expected visits to transient states, as we
are considering in our work.

4. Steady-State Policy Synthesis: Problem Formulation

In this section, we introduce the Steady-State Policy Synthesis (SSPS) problem of ï¬nding
a stationary policy from predeï¬ned classes of policies (edge- and class-preserving) that
maximizes the expected average reward subject to steady-state speciï¬cations. In contrast
to prior work, we do not impose restrictions on the underlying MDP. Before we present our
formulation, we brieï¬‚y discuss the challenges underlying policy synthesis under the average
reward optimality criterion and demonstrate the limitations of existing formulations in this
context. Subsequently, we specify our search domain of policies and deï¬ne the SSPS problem
of synthesizing optimal policies from this domain.

4.1 Challenges and Limitations

We motivate this section with a simple example. Suppose an autonomous agent is marooned
on a set of three connected frozen islands as shown in Figure 3. The agentâ€™s goal is to
maximize the amount of time it spends ï¬shing for sustenance while at the same time building
a canoe to escape the islands. The agent has an equal chance of starting in any state
belonging to the larger island of size n Ã— n/2, i.e., we have Î²s = 2/n2 for each state s in
the island. Once the agent moves to one of the two smaller islands, it is unable to return
to the larger island. One quarter of the land in the small islands contains logs which can
be used to build a canoe, and each of these islands contains one ï¬shing site as well. For
the ï¬rst small island we have steady-state speciï¬cations (Llog1, [0.25, 1]), (Llog2, [0.25, 1]),
(Lcanoe1, [0.05, 1.0]) and reward R(Â·, Â·, Lï¬sh1) = R(Â·, Â·, Lï¬sh2) = 1. Likewise, the second small
island has steady-state speciï¬cations (Lcanoe2, [0.05, 1.0]), (Lï¬sh1, [0.1, 1.0]), (Lï¬sh2, [0.1, 1.0])

13

Atia et al.

Figure 3: LMDP M = (S, A, T, R, Î², L, Î¦âˆ
L ) with labels Llog1 = {s34, s36, s38, s43},
Llog2 = {s52, s55, s57, s61}, Lcanoe1 = {s33}, Lcanoe2 = {s49}, Lï¬sh1 = {s48}, Lï¬sh2 =
{s64}, steady-state speciï¬cations (Llog1, [0.25, 1]), (Llog2, [0.25, 1]), (Lcanoe1, [0.05, 1.0]),
(Lcanoe2, [0.05, 1.0]), (Lï¬sh1, [0.1, 1.0]), (Lï¬sh2, [0.1, 1.0]) âˆˆ Î¦âˆ
L , and rewards R(Â·, Â·, Lï¬sh1) =
R(Â·, Â·, Lï¬sh2) = 1, R(Â·, Â·, S \ (Lï¬sh1 âˆª Lï¬sh2)) = 0.

and reward R(Â·, Â·, S \ (Lï¬sh1 âˆª Lï¬sh2)) = 0. Because the islands are covered in ice, the agent
has a chance of slipping in three possible directions whenever it moves. Speciï¬cally, if the
agent attempts to go right (left), it has a 90% chance of transitioning to the right (left),
and there is a 5% chance of transitioning instead to either of the states above or below it.
Similarly, if the agent tries to go up (down), it moves to the states above (below) it with 90%
chance, and to the states to the right and left of it with chance 5% each. This Frozen Island
scenario is motivated by that found in OpenAI Gymâ€™s FrozenLake environment (Brockman
et al., 2016).

For this example, the LP by Velasquez (2019) is infeasible since there exists no policy
that induces an irreducible Markov chain, that is, one where all states in S belong to one
recurrent class. The LP by Kallenberg (1983) in (13) will return a solution (x, y), from
which the stationary policy Ï€ := Ï€(x, y) is computed as follows

ï£±
ï£²

xsa
xs
ysa
ys
arbitrary

Ï€(a|s) =

ï£³
aâˆˆA(s) xsa, ys = (cid:80)

s âˆˆ Ex, a âˆˆ A(s)
s âˆˆ Ey \ Ex, a âˆˆ A(s)
otherwise

(14)

where xs := (cid:80)
aâˆˆA(s) ysa, Ex := {s âˆˆ S : xs > 0} and Ey := {s âˆˆ S : ys >
0}. However, in general the steady-state distribution induced by the policy (14) will not
satisfy the speciï¬ed constraints. This deï¬ciency is best demonstrated via a simple example.
The reader is also referred to Example 1 of Krass and Vrieze (2002).

Example 1. Consider the MDP in Figure 2(b) with initial probability Î²s1 = Î²s3 = 0, Î²s2 =
1. One feasible solution x of the LP in (13) (Kallenberg, 1983, Program 4.7.6) has xs2a2 =

14

Steady-State Planning in Expected Reward Multichain MDPs

Figure 4: (Left) One-to-one correspondence between the feasible LP solutions and the
stationary policies in unichain MDPs. (Right) Equivalence classes of feasible solutions map
to stationary policies. The steady-state distribution of the Markov chain induced by a policy
need not agree with the LP solution, and could hence fail to meet the LP constraints.

xs3a2 = 0.5. The policy Ï€ in (14) corresponding to x has Ï€(a2|s2) = Ï€(a2|s3) = 1, hence
Prâˆ

Ï€ (s2, a2) = 1. Therefore, Prâˆ

Ï€ (cid:54)= x.

The previous example underscores the main challenge underlying steady-state planning
in constrained Markov decision models with the average reward criterion: solutions to
formulated programs and stationary policies are not in one-to-one correspondence. In other
words, given a feasible LP solution (x, y), the steady-state distribution Prâˆ
Ï€ induced by the
policy Ï€(x, y) derived from that solution is not equal to x in general. As a result, unlike
unichain MDPs (Altman, 1999), steady-state speciï¬cations encoded as constraints on the
state-action variables are generally not met by Ï€. Figure 5 (Left) illustrates the one-to-
one correspondence between LP solutions and stationary policies found in unichain MDPs.
Figure 5 (Right) illustrates the lack of such correspondence in multichain MDPs, where
instead, equivalence classes of feasible LP solutions (yellow circles) map to the same policy
(Kallenberg, 1983; Puterman, 1994).

4.2 Problem Setup

The previous example motivates the work of this paper in which we develop an approach
to synthesizing policies with provably correct asymptotic behavior based on the notions of
edge preservation and class equivalence. First, we will deï¬ne sets of policies under which
certain class structures are preserved and give an example of such policies, then deï¬ne the
SSPS problem of ï¬nding an optimal policy from such classes.

Deï¬nition 15 (Edge-preserving policies). Given an MDP M, we deï¬ne the set of Edge-
Preserving (EP) policies Î EP as the set of stationary policies that play every action available
at states in the TSCCs r(M) of M and for which r(MÏ€) = r(M), i.e.,

Î EP = (cid:8)Ï€ âˆˆ Î S : r(MÏ€) = r(M) âˆ§ Ï€(a|s) > 0, âˆ€s âˆˆ r(M), a âˆˆ A(s)(cid:9) .

(15)

Hence, for every state s âˆˆ r(M) (see Deï¬nition 4), an EP policy assigns a non-zero
probability to every action in A(s), and every state in Â¯r(M) is either transient or isolated
in the Markov chain induced by the policy. For example, the uniform policy which has

15

UnichainMDPMultichain MDPFeasiblesolutionsğ‘¥!Pr"!#ğœ‹!ğœ‹$ğ‘¥$Pr""#StationarypoliciesFeasiblesolutionsğ‘¥!ğœ‹!ğœ‹$ğ‘¥$StationarypoliciesAtia et al.

Ï€(a|s) = 1/|A(s)|, âˆ€s âˆˆ S is in Î EP. Note that other policies in Î EP could assign a very
small probability (as long as it is non-zero) to non-rewarding transitions in r(M). Using an
open set deï¬nition in (15) simpliï¬es the exposition and the subsequent theoretical analysis,
however, it does not guarantee that an optimal policy from the set always exists. We discuss
and analyze variations of the problem formulation to address this issue at length in Section
5.4.4.

Next, we introduce two sets of policies whose deï¬nitions rest on two distinct notions of

class preservation.

Deï¬nition 16 (Class-preserving policies). Given an MDP M with TSCCs rk(M), k =
1, . . . , m, we deï¬ne the set of Class-Preserving (CP) policies Î CP as the set of stationary
policies that induce Markov chains with the same TSCCs as those of M, i.e.,

Î CP = (cid:8)Ï€ âˆˆ Î S : r(MÏ€) = r(M) âˆ§ âˆ€k âˆˆ [m], rk(MÏ€) = rk(M)(cid:9) .

(16)

Note that the condition r(MÏ€) = r(M) in Deï¬nitions 15 and 16 implies that Â¯r(M)
consists of transient or isolated states in MÏ€ for any Ï€ in Î EP or Î CP. Per (16), a CP policy
preserves the recurrence of all states in the TSCCs of the MDP but, unlike EP policies,
its support need not be the entire set of actions available at said states. Therefore, CP
policies can conceivably achieve larger rewards than EP policies by averting non-rewarding
transitions.

Deï¬nition 17 (Class-preserving up to unichain). Given an MDP M with TSCCs rk(M), k =
1, . . . , m, we deï¬ne the set of Class-Preserving-up-to-Unichain (CPU) policies Î CPU as

Î CPU = (cid:8)Ï€ âˆˆ Î S : r(MÏ€) âŠ† r(M) âˆ§ âˆ€k âˆˆ [m], âˆƒ! rk(MÏ€) âŠ† rk(M)(cid:9) ,

(17)

that is, the set of stationary policies that induce Markov chains MÏ€ in which the TSCCs
of the MDP M are reachable and unichain (i.e., each contains exactly one non-isolated,
recurrent component) and the recurrent states are a subset of the recurrent states of M
(Recalling that the notation âˆƒ! in (17) refers to the existence of a unique set).

This deï¬nition captures a more relaxed notion of class preservation than (16) for CP
policies in that it relaxes the requirement that all states in the TSCCs of M be recurrent
and reachable in the Markov chain MÏ€ induced by the policy Ï€, to the milder requirement
that in MÏ€ there exists a unique reachable recurrent class in each of the TSCCs of M.

The aforementioned deï¬nitions are best illustrated by an example. Figure 5(b) illustrates
a Markov chain induced by an EP policy, i.e., one that plays every action available in the
TSCCs of the MDP of Figure 5(a) with non-zero probability. As shown, s1 is isolated and
s2 is transient â€“ these would both be transient under the uniform policy. The TSCCs of the
induced chain are highlighted with two separate colors. Examples of Markov chains induced
by a CP and a CPU policy are shown in Figure 5(c) and (d), respectively. The Markov
chain of Figure 5(c) has the exact same TSCCs of the MDP and of the Markov chain of
Figure 5(b) induced by the EP policy, with the fundamental diï¬€erence that the CP policy
is not supported on every action available in the TSCCs (e.g., see the recurrent component
highlighted in blue). By contrast, states s3, s4, s6 and s7 are transient in the Markov chain of
Figure 5(d). The set consisting of states s3, s4, s5 is unichain, having exactly one recurrent

16

Steady-State Planning in Expected Reward Multichain MDPs

Figure 5: (a) MDP and Markov chains induced by (b) EP, (c) CP, and (d) CPU policies.
For the MDP, all transitions are deterministic, i.e., T (s(cid:48)|s, a) âˆˆ {0, 1} indicating if there is
an outgoing edge from s to s(cid:48) under action a, the rewards are deï¬ned such that R(s5, a3) =
R(s8, a1) = 1 and 0 otherwise, and the initial probabilities are Î²s1 = 0 and Î²s = 1/8, âˆ€s âˆˆ S\
{s1}. The numbers next to the edges of the Markov chains are the conditional probabilities
Ï€(a|s) of the diï¬€erent actions given the states specifying the policies.

17

s2s1s6s7s8s9s4s3s5a1a2a3a4a1a2a1a2a1a2a1a2a1a2a1a2a1a2a1a2a3a3a3Î²   = 0s1(a) MDP(c) Class-Preserving(b) Edge-Preserving(d) Class-Preserving up to Unichains2s1s6s7s8s9s4s3s511/31/31/311/23/41/21/21/21/21/211/31/31/31/411/31/31/31/211/21/211/21s5s3s4s9s8s7s61s1s21/32/51/41/23/41/22/33/52/31/32/3s2s6s7s8s9s4s3s51/61/31/31/31/61/31/31/31s1Î²s = 1/8, âˆ€sâˆˆS âˆ–{s1}Atia et al.

component (state s5) and two transient states (s3 and s4). Similarly, the set composed of
states s6, s7, s8, s9 is unichain with one recurrent component (s8 and s9) and two transient
states (s6 and s7).

The classes of policies deï¬ned in (15), (16) and (17) satisfy the following relations.

Lemma 2. Î EP âŠ† Î CP âŠ† Î CPU .

We remark that the inclusions in Lemma 2 are generally strict, except for some special
MDPs. Speciï¬cally, given a general MDP M, there may exist a policy Ï€ âˆˆ Î CP for which
r(MÏ€) = r(M) and Ï€(a|s) = 0 for some a âˆˆ A(s), s âˆˆ r(M), in which case Ï€ /âˆˆ Î EP.
Similarly, since a unichain may contain some transient states, Î CP is generally a proper
subset of Î CPU.

Problem Deï¬nition. We can readily deï¬ne the class of problems SSPS(Î ), parametrized
by a predeï¬ned set of stationary policies Î , of ï¬nding a policy in the set Î  that maximizes
the expected average reward while satisfying a given set of steady-state speciï¬cations.

(SSPS)). Given an LMDP M =
Deï¬nition 18 (Steady-state policy synthesis
(S, A, T, R, Î², L, Î¦âˆ
L ) and a domain of policies Î  âŠ† Î S, the SSPS(Î ) problem is to ï¬nd
an optimal stochastic policy Ï€ âˆˆ Î  that maximizes the expected average reward deï¬ned in
(10) and satisï¬es the steady-state speciï¬cations Î¦âˆ

L (Deï¬nition 12), i.e.,

max
Ï€âˆˆÎ 

(cid:88)

(cid:88)

(cid:88)

Prâˆ

Ï€ (s, a)R(s, a) subject to

sâˆˆS
(cid:88)

aâˆˆA(s)
Prâˆ

Ï€ (s, a) âˆˆ [l, u], âˆ€(Li, [l, u]) âˆˆ Î¦âˆ
L

(18)

sâˆˆLi

aâˆˆA(s)

If the maximum in (18) cannot be attained over the domain Î , we deï¬ne SSPS(Î ) as the
problem of ï¬nding a policy Ï€ âˆˆ Î  that satisï¬es the speciï¬cations Î¦âˆ
L and whose expected
average reward Râˆ
Ï€(cid:48) (Î²) âˆ’ (cid:15), for some arbitrarily small (cid:15) > 0 (See Section
5.4.4).

Ï€ (Î²) â‰¥ supÏ€(cid:48)âˆˆÎ  Râˆ

In this paper, we present solutions to SSPS(Î EP), SSPS(Î CP) and SSPS(Î CPU), where
Î  in (18) is set to Î EP, Î CP, and Î CPU, respectively4. To this end, we ï¬rst determine the
TSCCs r(M) of M and the complement set Â¯r(M) using standard techniques from graph
theory (Tarjan, 1972). These are then used to deï¬ne an LP from which the solution policy
is derived.

5. Linear Programming Based Solutions

In this section, we present our linear-programming-based solution to the SSPS problem
(18) over edge- and class-preserving policies. We formulate linear programs that encode
constraints on the limiting distributions of said policies to solve SSPS(Î EP) and SSPS(Î CP).
The optimal solutions to the formulated programs provably yield optimal edge- and class-
preserving policies that meet the desired speciï¬cations. The encoded constraints are also
at the center of an iterative algorithm described in Section 5.3 to generate CPU policies

4. Our work (Atia et al., 2020) has presented preliminary results for the SSPS(Î EP) problem.

18

Steady-State Planning in Expected Reward Multichain MDPs

for SSPS(Î CPU). Our main results on SSPS in edge- and class-preserving policies are
presented in Sections 5.1, 5.2 and 5.3. To simplify the exposition, all proofs are deferred to
the appendix.

We present three main programs. The ï¬rst, used for the synthesis of optimal EP poli-
cies, is the most constrained as it encodes the requirement that every action in the terminal
components must be played with non-zero probability. While this condition is not neces-
sary in order to ensure one-to-one correspondence between the feasible solutions and the
induced steady-state distributions, it results in a simple program whose solution provably
yields an optimal EP policy that meets the speciï¬cations. The second program relaxes
this condition to the milder requirement that every state in the terminal components is
visited inï¬nitely often (which may not require playing every action available), but at the
expense of additional complexity. Speciï¬cally, its solution yields an optimal policy that
meets the speciï¬cations from the class of CP policies (a superset of EP policies) but uses
more complex ï¬‚ow constraints to encode said requirement. The third program is the least
constrained and is used to synthesize a policy from the larger class of CPU policies. While
its solution is not guaranteed to yield a CPU policy, we derive a characterization of its op-
timal solution, which inspires a greedy algorithm to construct such policy. We augment the
program by iteratively adding constraints until convergence. The algorithm is guaranteed
to converge in a ï¬nite number of steps to a (possibly) suboptimal CPU policy that meets
the speciï¬cations.

By encoding constraints on the limiting distribution of the Markov chain induced by
a stationary policy derived from an LP solution, the policy is ultimately absorbed in the
TSCCs of the MDP. This restricts the long-term play to the TSCCs, which once reached,
cannot be escaped. By imposing strict positivity on state-action pairs or ï¬‚ow constraints
in the TSCCs, we further ensure that these components are unichain, and in turn, the
long-term frequencies induced by the policy match the solution from which the policy is
generated.

5.1 SSPS(Î EP) â€“ Synthesis over Edge-Preserving Policies

In this section, we formulate a linear program to solve SSPS(Î EP) deï¬ned in (18), which
seeks to maximize the expected average reward subject to speciï¬cation constraints over the
class of policies Î EP in (15).

Given MDP M, deï¬ne Q0 to be the set of vectors x, y satisfying

ï£±

ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³

(i) (cid:80)
(ii) (cid:80)
(iii) (cid:80)

sâˆˆS

sâˆˆS

(cid:80)

aâˆˆA(s) xsaT (s(cid:48) | s, a) = (cid:80)
(cid:80)
aâˆˆA(s) ysaT (s(cid:48) | s, a) = (cid:80)
(cid:80)
aâˆˆA(f ) xf a = 0,

f âˆˆÂ¯r(M)

aâˆˆA(s(cid:48)) xs(cid:48)a, âˆ€s(cid:48) âˆˆ S
aâˆˆA(s(cid:48))(xs(cid:48)a + ys(cid:48)a) âˆ’ Î²s(cid:48), âˆ€s(cid:48) âˆˆ S

(19)

xsa âˆˆ [0, 1], ysa â‰¥ 0, âˆ€s âˆˆ S, a âˆˆ A(s), f âˆˆ Â¯r(M), k âˆˆ [m]

19

Atia et al.

We can readily formulate LP1 (20) to synthesize optimal EP policies, which incorporates

two additional constraints beside the constraints in (19).

(cid:88)

(cid:88)

max

xsaR(s, a) subject to (x, y) âˆˆ Q0

sâˆˆS

(LP1)

(iv)

li â‰¤

aâˆˆA(s)
(cid:88)

(cid:88)

sâˆˆLi

aâˆˆA(s)

xsa â‰¤ ui, âˆ€(Li, [li, ui]) âˆˆ Î¦âˆ
L

(20)

(v) xsa > 0, âˆ€s âˆˆ rk(M), k âˆˆ [m], a âˆˆ A(s)

Constraints (i ) â€“ (iii ) constrain the limiting distributions of Markov chains induced by the
policies of interest, and are thus part of the constraint set of all programs we formulate in this
work. In particular, they capture the structure of the stationary matrix T âˆ corresponding
to the classiï¬cations r(M) and Â¯r(M) (See Deï¬nition 4). Constraint (i ) ensures that x is a
stationary distribution (Altman, 1999; Puterman, 1994); constraint (ii ), which is described
in (Kallenberg, 1983, Chapter 4) and (Puterman, 1994, Sec 9.3), enforces consistency in
the expected average number of visits yf a for any transient state-action pair f âˆˆ Â¯r(M), a âˆˆ
A(f ); constraint (iii ) preserves the non-recurrence of the states f âˆˆ Â¯r(M) by forcing zero
steady-state probability. Constraint (iv ) encodes the steady-state speciï¬cations. The strict
positivity constraint (v ) preserves the transitions in the TSCCs to yield EP policies. In
practice, we transform the strict inequalities to bounded ones by introducing an arbitrarily
small constant on the right-hand side, thereby ensuring an optimal solution always exists
(See Section 5.4.4). Enforcing constraints on the occupation measures ensures that, from
any state f âˆˆ Â¯r(M), the process will be ultimately absorbed into the TSCCs rk(M), k âˆˆ [m].
The next theorem guarantees that every feasible solution to LP1 yields an EP policy.

Theorem 1. Given an LMDP M, let (x, y) âˆˆ Q1, where Q1 is the feasible set of solutions
to LP1 (20), and let Ï€ := Ï€(x, y) be deï¬ned as in (14). Then, Ï€ âˆˆ Î EP.

We can readily state the following theorem establishing the correctness of LP1. It guarantees
that the policy synthesized from an optimal solution to (20) (if one exists) is not only in
Î EP, but also is optimal among all such policies and meets the steady-state speciï¬cations,
i.e., solves SSPS(Î EP).

Theorem 2. Given an LMDP M = (S, A, T, R, Î², L, Î¦âˆ
L ), LP1 in (20) is feasible iï¬€ there
exists a policy Ï€ âˆˆ Î EP such that the Markov chain MÏ€ = (S, TÏ€, Î²) satisï¬es the speciï¬ca-
tions Î¦âˆ
L . Further, given an optimal solution xâˆ—, yâˆ— of (20), the policy Ï€âˆ— := Ï€(xâˆ—, yâˆ—) as
deï¬ned in (14) is optimal in the class of policies Î EP and meets the speciï¬cations Î¦âˆ
L .

5.2 SSPS(Î CP) â€“ Synthesis over Class-Preserving Policies

The strict positivity constraint (v ) of LP1 forces the policy to play every action in the
TSCCs of the MDP M (by assigning non-zero probability to every action available), which
may be restrictive and often unnecessary. Indeed, as we show, in order to ensure one-to-one
correspondence between the optimal solutions of a formulated LP and the optimal policies
of the constrained MDP derived from these solutions, it suï¬ƒces to preserve the recurrence
or the unichain property of these components.

20

Steady-State Planning in Expected Reward Multichain MDPs

To address this restriction, we introduce ï¬‚ow constraints in LP2 given in (21) (replacing
constraint (v )) to ensure the recurrence of the components rk(M), k âˆˆ [m], in the induced
chain. It helps to introduce some notation in order to express such constraints. We deï¬ne
the transition relation of an MDP by T rel = {(s, s(cid:48)) âˆˆ SÃ—S|s (cid:54)= s(cid:48)âˆ§âˆƒa âˆˆ A(s), T (s(cid:48)|s, a) > 0}
(Velasquez, 2019). This corresponds to the graph structure of the MDP. For each TSCC
rk(M), we further deï¬ne its graph structure as T rel
k = T rel âˆ© rk(M) Ã— rk(M). We can now
add ï¬‚ow constraints in order to ensure that, for the Markov chain induced by the solution
policy, each set rk(M) will remain a recurrent class without necessarily having to take every
action available in that set.

max

(cid:88)

(cid:88)

xsa

sâˆˆS

(iv)

li â‰¤

aâˆˆA(s)
(cid:88)

(cid:88)

xsa â‰¤ ui,

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) âˆˆ Q0,

(cid:88)

s(cid:48)âˆˆS

sâˆˆLi

(vi)

fsis(cid:48) =

aâˆˆA(s)
(cid:88)

T (s(cid:48)|si, a)xsia

(vii)

f rev
sis(cid:48) =

(viii)

fss(cid:48) â‰¤

aâˆˆA(si)
(cid:88)

T (si|s(cid:48), a)xs(cid:48)a

aâˆˆA(s(cid:48))
(cid:88)

T (s(cid:48)|s, a)xsa

(LP2)

(ix)

f rev
ss(cid:48) â‰¤

aâˆˆA(s)
(cid:88)

aâˆˆA(s(cid:48))

T (s|s(cid:48), a)xs(cid:48)a

(x)

(xi)

(xii)

(xiii)

(xiv)

(cid:88)

fs(cid:48)s >

(cid:88)

fss(cid:48)

(s(cid:48),s)âˆˆT rel
(cid:88)

(s,s(cid:48))âˆˆT rel
(cid:88)

(s(cid:48),s)âˆˆT rel
(cid:88)

f rev
s(cid:48)s >

(s,s(cid:48))âˆˆT rel
(cid:88)

(s(cid:48),s)âˆˆT rel

f rev
ss(cid:48)

fs(cid:48)s > 0

f rev
s(cid:48)s > 0

(s,s(cid:48))âˆˆT rel
fss(cid:48), f rev

ss(cid:48) âˆˆ [0, 1]

âˆ€(Li, [li, ui]) âˆˆ Î¦âˆ
L

âˆ€(si, s(cid:48)) âˆˆ T rel

k , k âˆˆ [m]

âˆ€(s(cid:48), si) âˆˆ T rel

k , k âˆˆ [m]

âˆ€(s, s(cid:48)) âˆˆ T rel

k , k âˆˆ [m]

âˆ€(s(cid:48), s) âˆˆ T rel

k , k âˆˆ [m]

âˆ€s âˆˆ r(M) \ {si}

âˆ€s âˆˆ r(M) \ {si}

âˆ€s âˆˆ r(M)

âˆ€s âˆˆ r(M)

âˆ€(s, s(cid:48)) âˆˆ T rel

(21)
The program LP2 in (21) is such that every state in rk(M) can reach and is reachable
from every other state in rk(M). For each k âˆˆ [m], constraint (vi ) induces an initial ï¬‚ow
out of a randomly chosen state si âˆˆ rk(M) and into its neighbors s(cid:48), that is proportional
to the transition probability TÏ€(s(cid:48)|si) in the Markov chain induced by the solution policy;
constraint (viii ) establishes the ï¬‚ow capacity between states in a similar manner; (x ) ensures
that the incoming ï¬‚ow into every state in rk(M) is greater than the outgoing ï¬‚ow; ï¬nally,
constraint (xii ) ensures that there is incoming ï¬‚ow into every state in rk(M). These
constraints ensure that every state in rk(M) is reachable from si, whereas constraints (vii ),
(ix ), (xi ), (xiii ) address the foregoing in the reverse graph structure of the MDP, thereby

21

Atia et al.

ensuring that si is reachable from all states in rk(M). We remark that the feasible set in
(21) is a superset of that in (20) since the ï¬‚ow constraints (vi )â€“(xiv ) are implied by (v ),
hence LP2 is less-constrained than LP1.

We can readily state the following two theorems establishing the correctness of LP2,
which are the counterparts of Theorem 1 and 2. In particular, Theorem 3 guarantees that
the solution to LP2 is a CP policy, while Theorem 4 establishes that the policy (14) derived
from the optimal solution to LP2 in (21) solves SSPS(Î CP), i.e., is optimal among the class
of CP policies and meets the steady-state speciï¬cations.

Theorem 3. Given an LMDP M, let (x, y) âˆˆ Q2 and Ï€ be deï¬ned as in (14), where Q2 is
the feasible set of solutions to LP2 (21). Then, Ï€ âˆˆ Î CP.
Theorem 4. Given an LMDP M = (S, A, T, R, Î², L, Î¦âˆ
L ), the LP in (21) is feasible iï¬€
there exists a policy Ï€ âˆˆ Î CP, where Î CP is deï¬ned in (16), such that the Markov chain
MÏ€ = (S, TÏ€, Î²) satisï¬es the speciï¬cations Î¦âˆ
L . Further, given an optimal xâˆ—, yâˆ— of (21),
the policy Ï€(xâˆ—, yâˆ—) deï¬ned in (14) is optimal in the class of policies Î CP and meets the
speciï¬cations Î¦âˆ
L .

5.3 SSPS(Î CPU) â€“ Synthesis over Class-Preserving up to Unichain Policies

In this section, we discuss policy synthesis over the larger set of policies Î CPU. We provide
a suï¬ƒcient condition under which we can identify an optimal policy Ï€ âˆˆ Î CPU that meets
the speciï¬cations. Based on this result, we develop an iterative algorithm to construct a
policy in Î CPU that provably meets the desired speciï¬cations.

Next, we give a suï¬ƒcient condition for SSPS(Î CPU), characterized in terms of the set

of optimal solutions to LP3 in (22).
(cid:88)

(cid:88)

(cid:88)

LP3 : max

xsa

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) âˆˆ Q0 and (iv)

(22)

sâˆˆS

aâˆˆA(s)

s(cid:48)âˆˆS

Note that the feasible set of LP3 is the intersection of the set Q0 in (19) and the set of
variables satisfying the steady-state speciï¬cations, that is, without the positivity or ï¬‚ow
constraints in (20) and (21), respectively.

Recall that a strongly connected digraph is one in which it is possible to reach any node
starting from any other node by traversing the directed edges in the directions in which
they point. Theorem 5 states that the policy Ï€ in (14), derived from an optimal solution to
LP3 in (22), solves SSPS(Î CPU) if the directed subgraphs corresponding to the support of
the optimal solution in the TSCCs of M are strongly connected. In Theorem 5, we deï¬ne
the digraph associated with the support of a given solution x as the graph whose vertices
are all states s with xs > 0 and whose edges correspond to actions a for which xsa > 0.
Theorem 5. Given LMDP M, let Qâˆ— be the set of optimal solutions of LP3 (22) and
X âˆ— := {x : (x, y) âˆˆ Qâˆ— for some y}. Given x âˆˆ X âˆ—, let V +
k (x) := {s âˆˆ rk(M) : xs > 0}
and E+
k (x))
is strongly connected âˆ€k âˆˆ [m], then the policy Ï€ in (14) is optimal in the class of policies
Î CPU and meets the speciï¬cations in Î¦âˆ
L .
Corollary 1. If the condition in the statement of Theorem 5 holds for all x âˆˆ X âˆ—, then
LP3 (22) solves SSPS(Î CPU).

k (x) := {(s, a) âˆˆ rk(M) Ã— A(s) : xsa > 0}. If the directed subgraph (V +

k (x), E+

22

Steady-State Planning in Expected Reward Multichain MDPs

5.3.1 Generation of policies in Î CPU

Inspired by Theorem 5, we devise a row-generation-based algorithm to search for a policy
Ï€ âˆˆ Î CPU as shown in Algorithm 1. First, LP3 (22) is solved. If the digraph corresponding
to the support of the obtained solution is strongly connected for each of the TSCCs of
LMDP M, the policy in (14) is computed and the search stops. However, if the solution
does not correspond to a strongly connected digraph in some TSCC, then there must exist
a non-empty set of states with no outgoing edges to the rest of the states in that TSCC.
Hence, for some k âˆˆ [m] we ï¬nd a cut, that is, a set of states C that has no outgoing edges
to the complement set rk(M) \ C. The constraint in (23) corresponding to this cut is added
to include the edges in the support, where A(cid:48) = {a âˆˆ A(s) : T (s(cid:48)|s, a) > 0, s(cid:48) âˆˆ rk(M) \ C}.
The constraint forces the addition of missing edges across this cut in a greedy manner (by
forcing the sum of the state-action variables corresponding to these edges to be non-zero) to
eventually produce a strongly connected digraph. The process is repeated until a strongly
connected solution is found.

(cid:88)

(cid:88)

sâˆˆC

aâˆˆA(cid:48)

xsa > 0

(23)

Algorithm 1 is guaranteed to converge to a (possibly suboptimal) policy in Î CPU in a ï¬nite
number of steps, since in the worst case (when all edges are included) it will yield a policy
in Î EP âŠ† Î CPU under which all edges in the TSCCs of M are retained. The ï¬niteness of
the number of steps is because the number of cuts in the ï¬nite MDP is bounded above by
O(maxkâˆˆ[m] 2|rk(M)|). Our experiments have shown that Algorithm 1 converges to a policy
in Î CPU after a small number of iterations.

Algorithm 1 Generation of a policy Ï€ âˆˆ Î CPU
Input: LMDP M with speciï¬cations Î¦âˆ
L .
Output: Stationary policy Ï€ âˆˆ Î CPU which satisï¬es Î¦âˆ
L .

Determine the TSCCs rk(M), k âˆˆ m of M
isSConnected = F alse, C = {.}, A(cid:48) = {.}
while isSConnected = F alse do

Solve LP (22) with constraint (23) to get optimal values xâˆ—
Compute the support E+
if digraph (V +

sa, âˆ€(s, a) âˆˆ S Ã— A(s).
k (xâˆ—) of each TSCC corresponding to xâˆ— (See Theorem 5).

k (xâˆ—)) forms a SCC for every k âˆˆ [m] then

sa, yâˆ—

k (xâˆ—), E+
compute Ï€ using (14)
isSConnected = T rue

else

ï¬nd a cut and update C and A(cid:48)

end if
end while

5.4 Additional Insights

This section provides additional remarks and examples to shed more light on the linear
programming formulations. The section may be skipped without loss of continuity.

23

Atia et al.

5.4.1 Non-surjective mapping

All occupation measures induced by the policies of interest are elements of Q0 (19), that
is, Prâˆ
Ï€ âˆˆ X0 := {x : (x, y) âˆˆ Q0 for some y} if Ï€ âˆˆ Î CPU, which is aï¬ƒrmed by Lemma 7
stated in Appendix A. However, in general, P âˆ(Î CPU) âŠ‚ X0, i.e., the set P âˆ(Î CPU) is a
proper subset of X0. In mathematical terms, the mapping (9) between the set of policies
Î CPU and the set X0 is injective but non-surjective. In turn, there may exist elements of
X0 that are unpaired with policies in Î CPU. This is illustrated by the following example.

Example 2. Consider the MDP in Figure 2(b). It is easy to see that x for which xs2a2 =
xs3a2 = 1/2, xs1a1 = xs2a1 = xs3a1 = 0 is in X0, i.e., x âˆˆ X0. However, the only policies in
Î CPU that satisfy Prâˆ
Ï€ (s3, a2) = 1 are the deterministic policies Ï€1, Ï€2, which
have Ï€1(a2|s2) = 1, Ï€1(a2|s3) = 0 (for which state s2 is recurrent and s3 is transient) and
Ï€2(a2|s2) = 0, Ï€2(a2|s3) = 1 (for which state s3 is recurrent and s2 is transient). However,
Prâˆ

Ï€2(s3, a2) = 1. Thus, x /âˆˆ P âˆ(Î CPU).

Ï€1(s2, a2) = Prâˆ

Ï€ (s2, a2) + Prâˆ

5.4.2 Insufficient constraint set

The set Q0 correctly encodes constraints on the limiting distributions of Markov chains
induced by policies in Î CPU (with the state classiï¬cation corresponding to r(M) and Â¯r(M)).
However, the lack of one-to-one correspondence between feasible solutions and policies (see
Section 4.1) is not fully resolved by the constraint set (19) without the additional constraints
in (20) or (21). In particular, consider the linear program LP0 with feasible set Q0

(LP0) : max

(cid:88)

(cid:88)

sâˆˆS

aâˆˆA(s)

xsa

(cid:88)

s(cid:48)âˆˆS

T (s(cid:48)|s, a)R(s, a, s(cid:48)) subject to (x, y) âˆˆ Q0

(24)

The steady-state distribution of the policy (14) derived from an optimal solution (xâˆ—, yâˆ—)
to LP0 is generally not equal to xâˆ—. In turn, speciï¬cations encoded as constraints on the
state-action variables as in (22) will not necessarily be met by the policy. This is best
illustrated via a simple example.

Example 3. Revisit the three-state example of Figure 2(b) and deï¬ne the rewards R(s1, a1) =
R(s1, a2) = R(s2, a1) = R(s3, a1) = 0, R(s2, a2) = R(s3, a2) = 1 and initial distribu-
tion Î²s1 = 0, Î²s2 = Î²s3 = 1/2. The MDP has one TSCC such that, r(M) = r1(M) =
{s2, s3}, Â¯r(M) = {s1}. The solution to LP0 in (24) which has xâˆ—
s2a1 =
xâˆ—
s3a1 = 0, xâˆ—
s2a1 = 1/6, is optimal (albeit
not unique). However, the policy Ï€ := Ï€(xâˆ—, yâˆ—) has Prâˆ
Ï€ (s3, a2) = 1/2, hence
in general Prâˆ

Ï€ (s2, a2) = Prâˆ

s2a2 = 1/3, xâˆ—

s3a2 = 2/3, yâˆ—

s3a1 = 0, yâˆ—

s1a2 = xâˆ—

s1a1 = xâˆ—

s1a2 = yâˆ—

s1a1 = yâˆ—

Ï€ (cid:54)= xâˆ— .

5.4.3 Remarks on SSPS(Î CPU)

1) Note that, in the previous example, the derived policy Ï€ /âˆˆ Î CPU. However, if Ï€ :=
Ï€(xâˆ—, yâˆ—) âˆˆ Î CPU, where (xâˆ—, yâˆ—) is an optimal solution to (22), then Ï€ will be optimal
over Î CPU âŠ‡ Î CP i.e., solves SSPS(Î CPU). This follows from the optimality of (xâˆ—, yâˆ—) and
Lemma 6 in the appendix, which gives a suï¬ƒcient condition for the existence of a one-to-one
correspondence between the elements of Q0 and the steady-state distribution of policy (14).
2) In general, if we dispense with the ï¬‚ow constraints in (21), we have no guarantee that
the TSCCs rk(M) will be unichain in MÏ€ under such Ï€. For example, MÏ€ induced by the

24

Steady-State Planning in Expected Reward Multichain MDPs

policy Ï€ given in Example 3 has r1(MÏ€) = {2}, r2(MÏ€) = {3}, i.e., Ï€ /âˆˆ Î CPU. However, if
the rewards in this example are modiï¬ed such that R(s1, a1) (cid:54)= R(s2, a1) while keeping all
other rewards unchanged, then Ï€ âˆˆ Î CPU. Therefore, under certain suï¬ƒcient conditions on
the reward vector, LP3 in (22) solves SSPS(Î CPU).

5.4.4 Existence of Optimal Policies

Modiï¬ed LP. The feasible set Q1 for LP1 is not compact given the strict inequalities of
constraint (v ) in (20). Therefore, the maximum in (20) may not always be attained on the
set. This can be easily remedied by replacing the strict inequalities with bounded ones via
introducing an arbitrarily small constant (cid:15) > 0 on the right-hand side. Even when such
requirement is not made explicit, a constant (cid:15) is dictated by the numerical precision of
the LP solvers. We deï¬ne LP1((cid:15)) similar to (20), with constraints (v ) replaced with the
bounded inequalities in (v)(cid:48) for some (cid:15) > 0,

max

(cid:88)

(cid:88)

sâˆˆS

aâˆˆA(s)

LP1((cid:15))

xsaR(s, a) subject to (x, y) âˆˆ Q0, (iv),

(25)

(v)(cid:48) xsa â‰¥ (cid:15), âˆ€s âˆˆ rk(M), k âˆˆ [m], a âˆˆ A(s) .

Theorem 6 stated next is analogous to Theorem 2 with the modiï¬ed program LP1((cid:15));
it establishes that every feasible solution of LP1((cid:15)) yields a policy that is in Î EP, and
conversely, for every EP policy that meets the steady-state speciï¬cations, there exists an (cid:15) >
0 such that its steady-state distribution is LP1((cid:15))-feasible. Moreover, the policy obtained
from the optimal solution to LP1((cid:15)) solves SSPS(Î EP), that is, its expected average reward
can be made arbitrarily close to the supremum over the set Î EP as (cid:15) â†’ 0.

Theorem 6. Given an LMDP M = (S, A, T, R, Î², L, Î¦âˆ

L ) and LP1((cid:15)) as in (25), then

(1) The policy Ï€ in (14) corresponding to a feasible solution of LP1((cid:15)) is in Î EP.

(2) If âˆƒÏ€ âˆˆ Î EP and Ï€ meets the speciï¬cations Î¦âˆ

L , then âˆƒ(cid:15) > 0 such that Prâˆ

Ï€ is a

feasible solution of LP1((cid:15)).

(3) Let xâˆ—, yâˆ— be an optimal solution to LP1((cid:15)) and Ï€âˆ— := Ï€(xâˆ—, yâˆ—) the corresponding

policy in (14). Then,

(cid:18)

lim
(cid:15)â†’0

sup
Ï€âˆˆÎ EP

Râˆ

Ï€ (Î²) âˆ’ Râˆ

Ï€âˆ—(Î²)

(cid:19)

= 0

(26)

A similar result holds for CP policies if the maximum in (21) cannot be attained over
the feasible set by transforming constraints (x )â€“(xiii ) to bounded ones. The generalization
is straightforward, thus omitted for brevity.

Compact policy set â€“ policies with bounded support. The foregoing existence issue
stems from the open set deï¬nition of Î EP in (15), a result of which is that an optimal policy
from the set (i.e., one that maximizes the average reward) may not always exist. Therefore,
we introduce a slightly modiï¬ed deï¬nition next, in which we force a lower bound on the

25

Atia et al.

values a policy assumes on its support, i.e., require that Ï€(a|s) â‰¥ Î´, for some arbitrarily
small constant Î´ > 0. We formally introduce the deï¬nition of the compact set of policies,
then state a result analogous to Theorem 2 based on this deï¬nition for completeness.

Deï¬nition 19. Given an MDP M and some small Î´, where 0 < Î´ < 1/ maxsâˆˆr(M) |A(s)|,
we deï¬ne the set Î EP(Î´) âŠ‚ Î EP of EP policies of bounded support as,

Î EP(Î´) = (cid:8)Ï€ âˆˆ Î S : r(MÏ€) = r(M) âˆ§ Ï€(a|s) â‰¥ Î´, âˆ€s âˆˆ r(M), a âˆˆ A(s)(cid:9) .

(27)

Theorem 7. Given an LMDP M = (S, A, T, R, Î², L, Î¦âˆ

L ) and the set Î EP(Î´) in (27),

(1) The policy Ï€ in (14) corresponding to a feasible solution of LP1(Î´) is in Î EP(Î´).

(2) Let xâˆ—, yâˆ— be an optimal solution to LP1(Î´) and Râˆ—(Î´) the average reward of the cor-

responding policy in (14). Then,

lim
Î´â†’0

max
Ï€âˆˆÎ EP(Î´)

Râˆ

Ï€ (Î²) âˆ’ Râˆ—(Î´) = 0.

(28)

According to Theorem 7, every feasible solution to LP1(Î´) yields a policy in Î EP (Î´).
Also, the gap between the optimal expected average reward over the set Î EP (Î´) and the
optimal reward of LP1(Î´) approaches zero as Î´ â†’ 0.

6. Extensions

In this section, we explore extensions beyond class-preserving policies, as well as an alter-
native type of speciï¬cations applicable to transient states.

6.1 Beyond Class-Preserving Policies

In this section, we derive an alternative condition given in Theorem 8 under which LP3 in
(22) is guaranteed to yield a stationary policy whose steady-state distribution meets the
desired speciï¬cations. The policy generated need not be in Î CPU. The proof of Theorem 8
follows from the suï¬ƒcient and necessary optimality conditions of program (22) (Bertsimas
& Tsitsiklis, 1997). The condition is characterized in terms of the rewards vector R =
[R(s, a)], s âˆˆ S, a âˆˆ A(s). First, we introduce the following deï¬nition.

Deï¬nition 20 (Cone of feasible directions). The cone V (x, y), where (x, y) is any feasible
solution to LP (22), is deï¬ned as

V (x, y) :=

ï£±

ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³

aâˆˆA(s(cid:48)) hs(cid:48)aT (s|s(cid:48), a),
s(cid:48)âˆˆS zs(cid:48)aT (s|s(cid:48), a),

v = (h, z) âˆˆ R2|S||A| :
aâˆˆA(s) hsa = (cid:80)
(cid:80)
(cid:80)
aâˆˆA(s)(hsa + zsa) = (cid:80)
(cid:80)
(cid:80)
aâˆˆA(s) hsa â‰¤ 0,
sâˆˆLi
(cid:80)
aâˆˆA(s) hsa â‰¥ 0,

(cid:80)
(cid:80)

s(cid:48)âˆˆS

sâˆˆLj
hf a = 0,
hsa â‰¥ 0,
zsa â‰¥ 0,

âˆ€s âˆˆ S,
âˆ€s âˆˆ S,
i âˆˆ u(x),
j âˆˆ l(x),
âˆ€f âˆˆ Â¯r(M), a âˆˆ A(f ),
âˆ€(s, a) âˆˆ n(x),
âˆ€(s, a) âˆˆ m(y)

ï£¼

ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£½
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£¾

(29)

26

Steady-State Planning in Expected Reward Multichain MDPs

where u(x) := {i : (cid:80)
{(s, a) âˆˆ r(M) Ã— A(s) : xsa = 0}, m(y) := {(s, a) âˆˆ S Ã— A(s) : ysa = 0}.

aâˆˆA(s) xsa = ui}, l(x) := {j : (cid:80)

(cid:80)

(cid:80)

Lj

Li

aâˆˆA(s) xsa = lj}, n(x) :=

Theorem 8. Given LMDP M, let (x, y) be a feasible solution of (22). If R = [R(s, a)], s âˆˆ
S, a âˆˆ A is an interior point of the dual cone

V âˆ—(x, y) := {u âˆˆ R2|S||A| : (cid:104)u, v(cid:105) â‰¤ 0, for every v âˆˆ V (x, y)} ,

where (cid:104), (cid:105) denotes the inner product, then the policy Ï€ in (14) meets the speciï¬cations Î¦âˆ
L .
Further, Ï€ is the unique optimal policy in the class of policies for which Â¯r(M) âŠ† Â¯r(MÏ€).

We remark that the policy could be outside of Î CPU, but preserves the transience (or
isolation) of the states in Â¯r(M). While the statement of Theorem 8 imposes a conservative
assumption on the rewards vector which may be generally hard to verify, it opens up pos-
sibilities for further research on steady-state planning over larger sets of policies (beyond
Î EP, Î CP and Î CPU considered in this paper) â€“ in this case, sets of policies that preserve
the transience of Â¯r(M). Ultimately, one would hope to tackle SSPS(Î ) for arbitrary sets
of stationary policies Î . These are directions for future investigation.

6.2 Transient Speciï¬cations

In Deï¬nition 12, we introduced speciï¬cations on the steady-state distribution. However,
such speciï¬cations are only useful in the recurrent sets where states are visited inï¬nitely
often. A transient state f âˆˆ Â¯r(MÏ€) on the other hand will only be visited a ï¬nite number
of times, i.e., Prâˆ
Ï€ (f ) = 0 for any stationary policy Ï€ âˆˆ Î S. In this section, we present an
alternative speciï¬cation type which can be applied to transient states.

We ï¬rst describe a suitable property of transient states against which speciï¬cations can

be applied. We then deï¬ne transient speciï¬cations based on this property.

Deï¬nition 21 (Expected number of visits (Kemeny & Snell, 1963)). Given an MDP M
and policy Ï€ âˆˆ Î S, the expected total number of times that state f âˆˆ Â¯r(MÏ€) is visited under
policy Ï€ is

Î¶Ï€(f ) = Î²T

Â¯r(MÏ€)(I âˆ’ ZÏ€)âˆ’1ef .

(30)

labels L =
Deï¬nition 22 (Transient speciï¬cation). Given an MDP and a set of
{L1, . . . , LnL}, where Li âŠ† Â¯r(M), a set of transient speciï¬cations is given by Î¦tr
L =
{(Li, [li, ui])}nL
i=1. Given a policy Ï€, the speciï¬cation (Li, [li, ui]) âˆˆ Î¦tr
L is satisï¬ed if and
only if (cid:80)
Î¶Ï€(f ) âˆˆ [li, ui]; that is, if the expected number of visits to transient states
f âˆˆLi
f âˆˆ Li in the Markov chain MÏ€ falls within the interval [li, ui].

Suppose that we have a set of labels Ltr over transient states, and a set of transient
speciï¬cations Î¦tr
Ltr . We can augment the LMDP found in Deï¬nition 12 to incorporate these
transient speciï¬cations as follows. Let Lâˆ be the set of steady-state labels, and let Î¦âˆ
Lâˆ be
corresponding steady-state speciï¬cations. We deï¬ne a complete set of labels L = (Lâˆ, Ltr)
and speciï¬cations Î¦L = (Î¦âˆ

Ltr ), and deï¬ne an LMDP as M = (S, A, T, R, Î², L, Î¦L).

Lâˆ, Î¦tr

Our next result regarding y and Î¶Ï€ for the transient states gives a suï¬ƒcient condition

for the policy to meet the transient speciï¬cation.

27

Atia et al.

Proposition 1. Given an MDP M, let (x, y) âˆˆ Q0 and Ï€ as in (14), where Q0 is deï¬ned
in (19). If Ï€ âˆˆ Î CPU, then yf = Î¶Ï€(f ) for any state f âˆˆ Â¯r(M).

We remark that this is analogous to Lemma 6 stated in the appendix, which establishes

that if we have a point (x, y) âˆˆ Q0 for which (14) yields a CPU policy, then Prâˆ

Ï€ = x.

Given this characterization, we can augment LP1 (20) and LP2 (21) with constraint (xv )

to synthesize policies subject to transient speciï¬cations.

(xv)

li â‰¤

(cid:88)

(cid:88)

sâˆˆLi

aâˆˆA(s)

ysa â‰¤ ui, âˆ€(Li, [li, ui]) âˆˆ Î¦tr
Ltr

(31)

7. Numerical Results

In this section, we present a set of numerical results to corroborate the ï¬ndings of the
theoretical analysis.
In Section 7.1, we verify the steady-state behavior of the policies
derived from the proposed LPs using the Frozen Islands example of Figure 3, followed by a
study of their behavior in the presence of additional transient speciï¬cations in Section 7.2.
In Section 7.3, we present the results of a study which shows that the empirical steady-
state distributions and average number of state visitations induced by the derived policies
converge to values that meet the desired speciï¬cations.
In Section 7.4, we evaluate the
average reward achieved by said policies and examine the impact of various restrictions in
their respective LPs on the optimal values of the objective using the Toll Collector example
of Figure 13. A case study is also presented featuring the progress of the iterative Algorithm
1 for generating a CPU policy. A natural generalization of the speciï¬cations to the product
space of state-action pairs is presented in Section 7.5. We present two numerical experiments
to support the theoretical ï¬ndings of Section 5.4.4 in Section 7.6. Finally, we examine the
scalability of the proposed formulations in Section 7.7, where we present the runtime results
for problems with increasing size conducted in various environments.

7.1 Steady-State Speciï¬cations

In this section, we demonstrate the correct-by-construction behavior of the policies pro-
posed. As an illustrative example, we ï¬rst examine the behavior of a policy in Î EP using
the Frozen Island example shown in Figure 3. We run our proposed LP1 (20) to calculate
the steady-state distribution Prâˆ
Ï€ (s), and show the values for the two TSCCs (the two small
islands) in Figure 6.

The heat map gives insight into the means by which the agent satisï¬es the speciï¬ca-
tions. After the agent enters an island, it spends a large amount of time in states s33, s36,
s48, s49, s61, and s64, in the sense of asymptotic frequency of visits as given by Prâˆ
Ï€ (s).
The agent also frequently visits states s36 and s61 to satisfy the steady-state speciï¬ca-
tions (Llog1, [0.25, 1]) and (Llog2, [0.25, 1]), respectively. Likewise, to meet speciï¬cations
(Lcanoe1, [0.05, 1.0]), (Lcanoe2, [0.05, 1.0]) (Lï¬sh1, [0.1, 1.0]), (Lï¬sh2, [0.1, 1.0]) the agent often
visits states s33, s49, s48, and s64, respectively. In addition to visiting the aforementioned
states to satisfy the constraints, the agent also visits state s48 over 25% of the time to
maximize its expected reward (recall that R(Â·, Â·, s48) = R(Â·, Â·, s64) = 1).

The right three plots of Figure 7 show the values of Prâˆ

Ï€ (s) along with the optimal
s obtained from LP1 (20) for SSPS(Î EP), LP2 for SSPS(Î CP), and Algorithm 1 for

values xâˆ—

28

Steady-State Planning in Expected Reward Multichain MDPs

Figure 6: Heat maps showing the steady-state probabilities Prâˆ
belonging to the two TSCCs of the Frozen Lakes example in Figure 3.

Ï€ (s) for states s âˆˆ r(M)

Figure 7: Example showing that Prâˆ
s, s âˆˆ r(M) for policies in Î EP, Î CP and Î CPU
derived from the proposed LP1, LP2 and Algorithm 1, but not for Kallenbergâ€™s and the
discounted case (discount factor Î³ = 0.9999).

Ï€ (s) = xâˆ—

SSPS(Î CPU). In each of these, the steady-state distribution matches the one estimated by
the LP for every state. This in fact holds for all state-action pairs as well, i.e., Prâˆ = xâˆ—.
This condition is essential to the proof of Theorems 2, 4 and 5 and ensures that the policy
is both optimal and satisï¬es the steady-state speciï¬cations. We calculate the policy cor-
responding to the optimal solution of LP (4.7.6) by Kallenberg (1983) given in (13) with
the additional speciï¬cation constraints for comparison. As shown in Figure 7 (left), the
derived policy fails to give a steady-state distribution equal to xâˆ—. In addition, we obtain a
policy from the solution to LP (3.5) by Altman (1999) of a discounted reward MDP (with
the additional speciï¬cation constraints) using a discount factor Î³ = 0.9999. As observed
in the second from the left plot of Figure 7, the steady-state distribution of the derived
Ï€ (cid:54)= xâˆ—. For
policy does not match xâˆ—.
each speciï¬cation (Li, [li, ui]) âˆˆ Î¦âˆ
xâˆ—
s and
Prâˆ
Ï€ (s), demonstrating that all of the speciï¬cations are met for the pro-
posed methods. For Kallenbergâ€™s and the discounted formulations, however, although xâˆ—
Llog1
satisfy the speciï¬cation, the policy yields steady-state distributions Prâˆ
and xâˆ—
Ï€ (Llog1)
and Prâˆ
Ï€ (Lcanoe1) which violate the speciï¬cations (these violations are highlighted with
bold red text). In other words, e(cid:62)xâˆ—
Ï€ (Llog1) for
the Kallenberg and discounted formulations. The table also shows the optimal reward Râˆ—
given by our proposed methods, as well as the expected average reward yielded by the
Ï€ (s, a)R(s, a). While Râˆ— obtained by Kallenbergâ€™s for-
policy, i.e., Râˆ
mulation is larger than that of the EP and CP methods, the proposed LPs produce policies

In Table 1, we show the ramiï¬cations when Prâˆ
:= (cid:80)
L , Table 1 shows the values of e(cid:62)xâˆ—
Li

Ï€ (Lcanoe1) and e(cid:62)xâˆ—

Ï€ (Li) := (cid:80)

aâˆˆA(s) Prâˆ

Ï€ := (cid:80)

(cid:54)= Prâˆ

(cid:54)= Prâˆ

Prâˆ

Lcanoe1

Lcanoe1

Llog1

sâˆˆLi

sâˆˆLi

(cid:80)

sâˆˆS

29

Atia et al.

SS Speciï¬cations

Method

Logs (â‰¥ 0.25)

Canoes (â‰¥ 0.05)

Fish Rods (â‰¥ 0.1)

Island 1

Island 2

Island 1

Island 2

Island 1

Island 2

xâˆ— Prâˆ

xâˆ— Prâˆ

xâˆ—

Prâˆ

xâˆ—

Prâˆ

xâˆ— Prâˆ

xâˆ— Prâˆ

Rewards

Râˆ—

Râˆ
Ï€

CPU
CP
EP
Kallenberg
Discounted (Î³ = 0.999)
Discounted (Î³ = 0.9999)

0.25
0.25
0.25
0.25
0.26
0.26
0.25
0.25
0.25
0.25 0.17 0.25
0.25 0.04 0.25
0.25 0.18 0.25 0.15 0.05

0.26
0.21
0.25
0.26
0 0.05 0.0037 0.05 0.0013 0.26
0.04 0.26

0.05
0.05
0.05
0.05
0.05
0.05
0.04 0.05

0.05
0.05
0.05
0.07

0.25
0.25
0.25
0.36

0.05
0.05
0.05
0.05

0.03 0.05

0.26
0.21
0.25
0.19
0.52
0.35

0.10
0.14
0.10
0.10
0.10
0.10

0.10
0.14
0.10
0.14
0.39
0.21

0.3621
0.3621
0.3605
0.3605
0.3547
0.3547
0.3621 0.3278
0.3576 0.9061
0.3617 0.5530

Table 1: Steady-state speciï¬cation comparison. Bold red text indicates violated steady-
state speciï¬cations. Constraints are speciï¬ed in the header for each label type.

which yield larger values of Râˆ. Additionally, as the discount factor Î³ approaches 1, the
discounted reward does not converge to the expected reward. The policies obtained from
the discounted reward formulation achieve larger rewards Râˆ by violating the steady-state
constraints and spending larger proportions of time in the rewarding ï¬shing sites.

7.2 Synthesis for Transient Speciï¬cations

In this section, we demonstrate the behavior of the policies derived subject to transient
speciï¬cations following the framework described in Section 6.2. We again compare the
policies derived from our proposed formulations to that of Kallenberg with regard to meeting
such speciï¬cations for the Frozen Islands example of Figure 3. The labels over states in
Â¯r(M) are set to Ltools = {s7, s13, s23}, Lgas = {s10, s16}, and Lsupplies = {s2, s15, s29} as
shown in Figure 9 (left). The agent sets out to collect some tools, ï¬ll up enough gas, and
pick up the required ï¬shing supplies before transitioning to one of the smaller islands which
correspond to TSCCs. This is reï¬‚ected in the transient speciï¬cations (Ltools, [10, Ntr]),
(Lgas, [12, Ntr]), (Lsupplies, [15, Ntr]) âˆˆ Î¦tr
L . These speciï¬cations bound the expected total
number of visitations to certain states in Â¯r(M), where Ntr = 200. Figure 8 presents the
values of the expected total number of times a state s âˆˆ Â¯r(M) is visited under policy Ï€,
denoted by Î¶Ï€(s), along with the optimal values yâˆ—
s , obtained from Kallenbergâ€™s LP, LP1
(20), LP2 (21), and Algorithm 1. As shown, the results match the expected number of
visitations for the proposed methods for every state.
For each transient speciï¬cation (Li, [li, ui]) âˆˆ Î¦tr

yâˆ—
f
and the expected total number of visitations achieved by the policy in corresponding states
Î¶Ï€(Li) := (cid:80)
Î¶Ï€(f ). As shown, LP1, LP2 and Algorithm 1 yield policies that satisfy the
given speciï¬cations, while the policy derived from the Kallenberg LP does not. The last
column shows the expected total number of visitations achieved by the policy on the larger
(transient) island, where Î¶Ï€(Â¯r(M)) = (cid:80)

f âˆˆÂ¯r(M) Î¶Ï€(f ).
Figure 9 (right) shows a heat map for the expected number of visits to the transient
states, i.e.
In addition to
the constraints (Ltools, [10, Ntr]), (Lgas, [12, Ntr]), and (Lsupplies, [15, Ntr]), we also add the
constraint (Â¯r(M) \ (Ltools âˆª Lgas âˆª Lsupplies) , [0, 10]) to reduce the amount of time spent in
transient states with no resources. As shown, the agent meets the speciï¬cations largely by
visiting states s13, s16, and s29 to collect tools, gas, and supplies, respectively.

the large island. The policy is calculated using LP1 (20).

L , Table 2 shows e(cid:62)yâˆ—
Li

:= (cid:80)

f âˆˆLi

f âˆˆLi

30

Steady-State Planning in Expected Reward Multichain MDPs

Figure 8: Example showing that Î¶Ï€(s) = yâˆ—
for Kallenbergâ€™s formulation.

s , s âˆˆ Â¯r(M) for the proposed methods, but not

Figure 9: Distribution of labels on the large island, Ltools = {s7, s13, s23}, Lgas = {s10, s16},
and Lsupplies = {s2, s15, s29} (left). Heat map showing the expected number of visits Î¶Ï€(s)
for states s âˆˆ Â¯r(M), i.e., the states belonging to the large island in Figure 3 (left).

Transient Speciï¬cations (Ntr = 200)

Results

Method

Tools (â‰¥ 10) Gas (â‰¥ 12) Supplies (â‰¥ 15)

CPU
CP
EP
Kallenberg

yâˆ—

19.22
18.97
19.32
19.34

Î¶Ï€

yâˆ—

Î¶Ï€

yâˆ—

15.51
19.22
15.26
18.97
19.32
15.51
5.55 15.53

21.15
15.51
20.67
15.26
15.51
21.15
3.95 21.17

Î¶Ï€

Râˆ—

0.3621
21.15
0.3607
20.67
21.15
0.3547
5.82 0.3621

Î¶Ï€(Â¯r(M))
200
200
200
56.5

Table 2: Bold red text indicates violated transient speciï¬cations. Constraints are speciï¬ed
in the header for each label type.

31

1234567891011121314151617181920212223242526272829303132Large Island (Transient)Atia et al.

Figure 10: Execution of policy, showing (left) average visits and (right) average reward up
to time n.

7.3 Empirical Study

In this section, we simulate the policies derived from our LPs to show the validity of our
formulations and to further demonstrate the failure of the Kallenberg formulation to yield
optimal rewards and meet speciï¬cations.

Let St and At denote the state and action, respectively, of the Frozen Island example at
time t assuming policy Ï€ and initial distribution Î². The average number of visits fÏ€,n and
average reward gÏ€,n up to time n are deï¬ned as

fÏ€,n(L) =

1
n

n
(cid:88)

t=1

gÏ€,n =

1L(St),

1L(s) =

(cid:26) 1 s âˆˆ L
0 s /âˆˆ L

1
n

n
(cid:88)

t=1

R(St, At, St+1).

(32)

(33)

We take an ensemble average over 5000 paths.

First, we solve LP (4.7.6) of Kallenberg (1983). In Figure 10 (left), the solid green line
shows the average number of visits to the states in Llog1 = {s34, s36, s38, s43}, and the hori-
zontal dashed green line indicates the steady-state distribution. The square markers show
the lower bound of the speciï¬cation on the logs. While the value of fÏ€,n(Llog1) converges
to the steady-state distribution, the policy fails to meet the steady-state speciï¬cation. This
follows from the fact that Prâˆ

Ï€ (cid:54)= xâˆ—.

Next, we produce an EP policy by executing LP1 (20), using no transient speciï¬cations.
The average number of visits to the states in Llog1 is shown as a solid red line in Figure 10
(left). Not only does the average number of visits converge to the corresponding steady-state

32

ğœ™ğœ‹ğ‘tr=5ğ‘tr=500No transientconstraintsProposed LP1Kallenbergğ‘™ğ‘™ğ‘œğ‘”1ğœ™ğœ‹ğ‘tr=5ğ‘tr=500No transientconstraintsProposed LP1Kallenbergğ‘…âˆ—ğ‘”ğœ‹,ğ‘›Steady-State Planning in Expected Reward Multichain MDPs

distribution (dashed black line), but the speciï¬cation is met. We observe similar results for
CP and CPU policies as well.

In Figure 10 (right), the solid green and red lines show the average reward for the
Kallenberg LP and our proposed LP1, respectively. The dashed green line indicates Râˆ— for
Kallenbergâ€™s formulation. As can be seen, for similar reasons as before, the average reward
converges to a reward other than that output by the LP. On the other hand, LP1 converges
to the corresponding LP reward Râˆ— (black dashed line).

The vertical green and red dashed lines in Figure 10 indicate the average time of entry
into r(M) for LP1 and Kallenbergâ€™s LP, respectively, where the time of entry Ï†Ï€ is given
by

Ï†Ï€ = min{n | Sn âˆˆ r(M)}.

(34)

In both cases, the agent spends an unduly amount of time in the transient states before
transitioning to a recurrent set, which may be undesirable. To reduce the amount of time
spent in the transient states, we next introduce a transient speciï¬cation (Â¯r(M), [0, Ntr])
and rerun LP1. The constant Ntr is used to control the time of entry into the recurrent
sets. The results are shown for Ntr = 5 (blue lines) and Ntr = 500 (yellow lines). In both
cases, convergence of the average visits to Prâˆ
Ï€ (Llog1) occurs at a much faster rate, leading
to a much faster accumulation of reward.

We now comment further on the simulation for Ntr = 5. The policy produced by LP1
separates the ï¬rst small island into two main subsets. The agent tends to visit state s33
repeatedly after entering the ï¬rst small island, leading to an above average number of visits
to log1 states. This results in an initial â€œovershootâ€ of Prâˆ
Ï€ (Llog1). This eï¬€ect is not seen
for Ntr = 500 due to the averaging eï¬€ect of fÏ€,n(L). Likewise, the policy tends to delay the
entry of the agent into state s48 where rewards are accumulated. This delay is especially
noticeable in gÏ€,n for Ntr = 5 due to the logarithmic time scale.

In the same vein, we explore the simulated behavior of our policies in terms of the number
of visits to transient states, where the number of visits hÏ€,n(L) to states in L âŠ† Â¯r(M) up
to time n is deï¬ned as

hÏ€,n(L) =

n
(cid:88)

t=1

1L(St).

(35)

In Figure 11, we show the number of visits to the transient states for the same policies as
shown in Figure 10. For the policies produced by LP1, hÏ€,n(Â¯r(M)) converges to the optimal
yâˆ—
Â¯r(M). On the other hand, as described in Section 7.2, for the Kallenberg formulation we
have Î¶Ï€ (cid:54)= yâˆ— and so the derived policy fails to converge to yâˆ—

Â¯r(M).

7.4 Comparison of Policies

Recall that policies in Î EP exercise all transitions in the TSCCs of an MDP. By contrast,
policies in Î CP and Î CPU are less restrictive in that they only preserve the state classiï¬cation
and the unichain property of these components, respectively. In turn, they often yield larger
expected rewards while simultaneously satisfying desired speciï¬cations. In this section, we
verify the correctness of such policies and compare their optimal rewards.

33

Atia et al.

Figure 11: Execution of policy with transient speciï¬cations showing the number of visits to
transient states up to time n.

Figure 12 illustrates the Markov chains induced by policies in Î EP, Î CP, and Î CPU,
respectively, for the MDP shown in its ï¬rst column. The policies are obtained from the
optimal solutions of the corresponding LPs. As observed, the Markov chain induced by the
EP policy (Column 2) contains all transitions in the TSCCs of the underlying MDP. The
self loops of states s3,s5, and s9 are missing in the Markov chain induced by the CP policy
without aï¬€ecting the recurrence of each of the TSCCs. In the case of the Markov chain
induced by the CPU policy, the state s3 is transient in the TSCC {s3, s4, s5}, but all TSCCs
of M remain unichain in the induced Markov chain. That is, each TSCC contains exactly
one recurrent component.

In order to compare the performance of our EP, CP, and CPU policies, we deï¬ne the
Toll Collector example given by the LMDP M of Figure 13. In this problem, an agent must
choose one of m cities to visit, each of which corresponds to a TSCC of M. The k-th city
consists of nk, k âˆˆ [m] counties represented as vertices and roads connecting these counties
represented by edges. The roads with toll booths yield a positive reward for collecting a
toll. However, the agent needs to spend some time on roads without toll booths in order to
build them. We consider an instance of the Toll Collector problem for which m = 3 and the
number of states per TSCC nk = n, âˆ€k. To highlight the gap between the optimal rewards of
the diï¬€erent policies, we deï¬ne the labels Lk = {s âˆˆ rk(M) : R(s, a, s(cid:48)) = 0, âˆ€a âˆˆ A(s), s(cid:48) âˆˆ
rk(M)} and Î¦âˆ
L = (Lk, [l, 1]), respectively, âˆ€k âˆˆ [m]. As such, per Î¦âˆ
L , the steady-state
probability of states with no rewardful transitions is forced to be bounded below by l. We
will use this steady-state speciï¬cation with various values of l to show that, for lower values
of l, there is a signiï¬cant gap in expected rewards observed by the various policies. As this
l value is increased, the gap can be shown to diminish.

34

ğœ™ğœ‹ğ‘tr=5ğ‘tr=500No transientconstraintsProposed LP1Kallenbergâ„ğœ‹,ğ‘›Ò§ğ‘ŸSteady-State Planning in Expected Reward Multichain MDPs

Figure 12: Markov chains induced by the EP, CP, and CPU policies. The ï¬rst column
shows the underlying MDP M = (S, A, T, R, Î²). Transitions designated with circles have
unit reward, otherwise the reward is 0. The initial distribution Î² is uniform over S.

Figure 13: Toll Collector problem given by LMDP M = (S, A, T, R, Î², L, Î¦âˆ
L ) consisting of
m fully-connected TSCCs rk(M), k âˆˆ [m] and Â¯r(M) = {s0}. The k-th TSCC consists of nk
states. State s0 has m actions, each of which leads to one of the m TSCCs with probability
1. For each state si in rk(M), there are nk âˆ’ 1 actions, each of which causes a transition
to another state in rk(M) with probability 1. The reward function is deï¬ned such that, in
each TSCC, there is a positive reward by taking the action that leads from some state si
to its neighbor si+1 and vice-versa. That is, R(si, Â·, si+1) = R(si+1, Â·, si) = 1 for some i.
These rewards are designated with red solid circles in each TSCC. All other rewards are 0.
The initial distribution Î² is uniform over S. The labels and steady-state speciï¬cations are
given by Lk = {s âˆˆ rk(M) : R(s, a, s(cid:48)) = 0, âˆ€a âˆˆ A(s), s(cid:48) âˆˆ rk(M)} and Î¦âˆ
L = (Lk, [l, 1]),
respectively, for all k âˆˆ [m].

35

ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 1ğ‘ 2ğ‘ 1ğ‘ 2MDPMarkov Chain -CPURewardsğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 1ğ‘ 2Markov Chain -EPğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 1ğ‘ 2Markov Chain -CPğ‘3ğ‘1ğ‘2ğ‘1ğ‘4ğ‘4ğ‘3ğ‘2ğ‘2ğ‘1ğ‘2ğ‘1ğ‘3ğ‘1ğ‘2ğ‘1ğ‘1ğ‘1ğ‘1ğ‘3ğ‘2ğ‘2ğ‘2ğ‘2ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘2ğ‘2ğ‘2ğ‘2ğ‘2ğ‘2ğ‘ !ğ‘ "ğ‘ #ğ‘ $!TSCC 1ğ‘ !TSCC 2ğ‘ $!%!ğ‘ $!%"ğ‘ $!%$"TSCC ğ‘šRewardsAtia et al.

Figure 14: Comparison of EP, CP, and CPU policies for the Toll Collector example. (Left)
Expected reward as function of the total number of states in each TSCC when l = 0.
(Right) Expected reward as function of the lower bound l on the steady-state probability
when n = 25.

Figure 14 (left) compares the optimal rewards achieved by the diï¬€erent policies as a
function of the total number of states in the TSCCs (i.e., 3n) when l = 0. As the number
of states increases, the gap between the average reward of the EP policies and their CP
and CPU counterparts increases. In this scenario, the EP policies incur a quadratic loss
relative to CPU policies since they are forced to exercise all existing transitions equally
and there are O(n2
k) such transitions in the k-th TSCC. On the other hand, an optimal
CPU policy preserves the unichain property while exercising exclusively the two transitions
with positive reward in each TSCC. A smaller loss is incurred by CP policies since they
are only required to preserve the recurrence of the TSCCs and can thus restrict themselves
to visiting the outer perimeter of each TSCC. In doing so, the CP policies incur a linear
loss when compared to the CPU policies because they must visit every state in a TSCC
inï¬nitely often in order to preserve the recurrent classiï¬cation of these states.

Figure 14 (right) illustrates the average rewards as a function of the lower bound l for
the three types of policies when the number of states in each TSCC is n = 25. When l
increases, the average reward gap between the diï¬€erent policies diminishes since the agent
has to spend more time in states with no rewards to meet the desired speciï¬cations.

7.4.1 Operation of Algorithm 1

In this subsection, we present in detail the operation of the proposed Algorithm 1 to generate
a CPU policy. Consider the LMDP given in Figure 15. For each iteration, LP (22) is solved
and the digraph of the support of the solution in each TSCC is shown (colored nodes). In
the ï¬rst iteration, for the ï¬rst TSCC, states s4 and s5 form a SCC, while s3 does not belong
to the support of the solution. For both the second and third TSCCs, all respective states
belong to the support but they do not form a SCC, thus we can ï¬nd cut(s) (as can be seen

36

1020304050607080901000.750.80.850.90.95100.10.20.30.40.50.40.50.60.70.80.91Number of States in TSCCsLower Bound on Steady-State ProbabilityExpected RewardSteady-State Planning in Expected Reward Multichain MDPs

at the bottom of the ï¬gure of the second iteration). In the second iteration, the dotted
edges are added which results in one SCC for the second TSCC (no additional constraints
are needed) but not for the third TSCC. Thus, we consider additional cuts as shown at the
bottom of the ï¬gure of the third iteration. In the last iteration, the stopping criteria is met
(the digraph in each of the three TSCCs is strongly connected). The ï¬nal Markov chain
induced by the CPU policy derived from the solution to the LP of the third iteration is
shown on the right side of Figure 15. States {s3, s4, s5} form a unichain component, states
{s6, s7, s8, s9} form a recurrent component, and states {s10, s11, s12, s13, s14, s15} belong to
a TSCC where all edges are preserved.

Figure 15: Illustration of the progress of Algorithm 1 for generating a policy in Î CPU. The
LMDP M = (S, A, T, R, Î², L, Î¦âˆ
L ), where S, A, and R are given in the MDP (ï¬rst column)
labels are Lgold1 = {s4, s5}, Lgold2 = {s6, s7}, Lgold3 = {s10, s11}, and
and Î² is uniform.
the steady-state speciï¬cations are (Lgold1, [0.20, 1]),(Lgold2, [0.10, 1]), (Lgold3, [0.15, 1]) (ï¬rst
column). In each iteration, we illustrate the support of the optimal solution for the TSCCs
of M (middle columns) along with the edges (state-action pairs) for a given cut. After
three iterations, the support of the optimal solution corresponds to SCCs in each of the
TSCCs. Every TSCC of M is a unichain component in the Markov chain MÏ€ induced by
the resulting policy (last column).

7.5 Speciï¬cations on State-Action Pairs

Up to this point, we have deï¬ned steady-state and transient speciï¬cations over states.
However, the framework proposed can be used to synthesize policies with provably correct
behavior on the level of state-action pairs as well. As an example, consider the LMDP

37

ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 1ğ‘ 2Final Markov ChainIteration -1-ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7Iteration -2-Iteration -3-RewardsLabelsğ´ğ‘‘ğ‘‘ğ‘’ğ‘‘ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¡ğ‘ ğ‘‡ğ‘†ğ¶ğ¶1:.ğ‘‡ğ‘†ğ¶ğ¶2:.ğ‘‡ğ‘†ğ¶ğ¶3:.ğ´ğ‘‘ğ‘‘ğ‘’ğ‘‘ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¡ğ‘ ğ‘‡ğ‘†ğ¶ğ¶1:.,ğ‘‡ğ‘†ğ¶ğ¶2:ğ‘ 8,ğ‘1,(ğ‘ 9,ğ‘2)ğ‘‡ğ‘†ğ¶ğ¶3:ğ‘ 14,ğ‘1,(ğ‘ 15,ğ‘2)ğ´ğ‘‘ğ‘‘ğ‘’ğ‘‘ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¡ğ‘ ğ‘‡ğ‘†ğ¶ğ¶1:.,ğ‘‡ğ‘†ğ¶ğ¶2:ğ‘ 8,ğ‘1,(ğ‘ 9,ğ‘2)ğ‘‡ğ‘†ğ¶ğ¶3:ğ‘ 14,ğ‘1,ğ‘ 15,ğ‘2,ğ‘ 11,ğ‘1,(ğ‘ 12,ğ‘2)ğ‘ 4ğ‘ 5ğ‘ 3ğ‘ 9ğ‘ 8ğ‘ 6ğ‘ 7ğ‘ 11ğ‘ 15ğ‘ 10ğ‘ 12ğ‘ 13ğ‘ 14ğ‘ 1ğ‘ 2MDPğ‘3ğ‘1ğ‘2ğ‘1ğ‘4ğ‘4ğ‘3ğ‘2ğ‘2ğ‘1ğ‘2ğ‘1ğ‘3ğ‘1ğ‘2ğ‘1ğ‘1ğ‘1ğ‘1ğ‘3ğ‘2ğ‘2ğ‘2ğ‘2ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘1ğ‘2ğ‘2ğ‘2ğ‘2ğ‘2ğ‘2Atia et al.

Speciï¬cations

Steady-State

Transient

Prâˆ

Ï€ (s4, a1) Prâˆ
0.11
0.10
0.11

Ï€ (s6, a2) Prâˆ
0.12
0.12
0.12

Ï€ (s10, a1)
0.20
0.20
0.20

Î¶Ï€(s2, a1)

Î¶Ï€(Â¯r(M))

26.2
31.03
28.82

50
50
50

Policy

EP
CP
CPU

Table 3: The policies meet transient and steady-state speciï¬cations on state-action pairs
for the MDP deï¬ned in Figure 15.

tool = {(s2, a1)}, and Lâˆ

M = (S, A, T, R, Î², L, Î¦L) deï¬ned in Figure 15 (left). We deï¬ne labels L = (Lâˆ, Ltr) over
state-action pairs, i.e., Ltr
gold2 = {(s6, a2)}, and
gold3 = {(s10, a1)}. The speciï¬cations are given as Î¦L = (Î¦âˆ
Lâˆ
Ltr ), where the steady-
state speciï¬cations are (Lâˆ
gold3, [0.20, 1]), and the
transient speciï¬cations are given as (Ltr
tool, [20, 50]). We also set the average total number
of visitations Ntr = 50. Table 3 shows the steady-state distributions Prâˆ
Ï€ (s, a) and the
expected number of visitations Î¶Ï€(s, a) for the labeled sets, as well as the total number of
visitations Î¶Ï€(Â¯r(M)) to the set Â¯r(M) for EP, CP and CPU policies. As shown, the policies
meet both steady-state and transient speciï¬cations deï¬ned over the product space S Ã— A.

gold1 = {(s4, a1)}, Lâˆ
Lâˆ, Î¦tr
gold2, [0.12, 1]), and (Lâˆ

gold1, [0.10, 1]), (Lâˆ

7.6 Modiï¬ed LP and Policy Set

Impact of (cid:15) in LP1((cid:15)). In this section, we use the MDP example of Figure 15 to inves-
tigate the impact of the parameter (cid:15) > 0 on the total reward induced by an optimal EP
policy. In particular, we solve LP1((cid:15)) with descending values of (cid:15) and compute the optimal
reward Râˆ—((cid:15)). As shown in Figure 16, Râˆ—((cid:15)) increases monotonically as we decrease (cid:15) with
diminishing return, and converges to nearly 0.36 as (cid:15) â†’ 0. For values of (cid:15) below 10âˆ’4,
the change in average reward if we further decrease (cid:15) is insigniï¬cant. Therefore, in our
experiments we have set (cid:15) = 10âˆ’4.

Figure 16: Convergence of the average expected reward as we vary the parameter (cid:15) in (25)
for the MDP example of Figure 15.

Policies with bounded support. Here, we verify the result of Theorem 7. Consider
the example of Fig. 2 where R(s2, a1) = R(s3, a1) = R(s3, a2) = 0.1 and R(s2, a2) = 0.5.

38

10-510-410-30.26   0.28   0.3   0.32   0.34   0.36   Steady-State Planning in Expected Reward Multichain MDPs

Recalling that Î´ is a lower bound on the support of the policies in Î EP(Î´) in (27), we
can show that the optimal average reward over Î EP(Î´) is maxÏ€âˆˆÎ EP(Î´) Râˆ
Ï€ = 0.5(1 âˆ’ Î´)2 +
0.2Î´(1 âˆ’ Î´) + 0.1Î´2, achieved by the policy Ï€âˆ— which has Ï€âˆ—(s2|a2) = Ï€âˆ—(s3|a1) = 1 âˆ’ Î´,
and Ï€âˆ—(a1|s2) = Ï€âˆ—(a2|s3) = Î´. The reward Râˆ—(Î´) of the policy Ï€ in (14) obtained from
the optimal solution to LP1(Î´) in (25) is Râˆ—(Î´) = 0.5 âˆ’ 1.2Î´, where Ï€(a1|s2) = Î´/(1 âˆ’ 2Î´),
Ï€(a2|s2) = (1 âˆ’ Î´)/(1 âˆ’ 2Î´) and Ï€(a1|s3) = Ï€(a2|s3) = 1/2. Fig. 17 shows that the diï¬€erence
Râˆ

Ï€âˆ— âˆ’ Râˆ—(Î´) â†’ 0 as Î´ â†’ 0 as per Theorem 7.

Figure 17: The diï¬€erence Râˆ
Râˆ—(Î´) is the reward of the policy obtained from an optimal solution to LP1(Î´).

Ï€âˆ— âˆ’ Râˆ—(Î´) â†’ 0 as Î´ â†’ 0, where Ï€âˆ— = arg maxÏ€âˆˆÎ EP(Î´) Râˆ

Ï€ and

7.7 Scalability

We demonstrate the scalability of the proposed formulations using two sets of experiments.
The ï¬rst set of experiments are performed on a standard desktop with 16GB of RAM using
the Matlab CVX package for convex optimization (Grant & Boyd, 2014, 2008). We also
perform a second set of experiments on a standard desktop of 128GB of RAM using the
commercial CPLEX Optimizer, which provides a higher-precision mathematical solver for
large-scale linear programming.

For the ï¬rst set of experiments, we experiment with instances of increasing size of
the Toll Collector problem (Figure 13), the Frozen Islands environment (Figure 3) and
random partition graphs from the NetworkX library (Hagberg, Swart, & S Chult, 2008).
The Toll Collector problem uses an MDP with three TSCCs, each of size n, while the
Frozen Islands environment consists of an n Ã— n grid. We also experiment with random
MDPs constructed from n-node directed Gaussian partition graphs generated using the
NetworkX toolbox (Hagberg et al., 2008). For such graphs, the cluster sizes are drawn from
a normal distribution with mean and variance n/5, and two nodes within the same cluster
are connected with probability pin, while two nodes in diï¬€erent clusters are connected with
probability pout (Brandes, Gaertler, & Wagner, 2003). For these partition graphs, the state
space corresponds to the vertex set, the number of actions is equal to the maximum node
outdegree and the transitions are deterministic. The initial distribution is uniform over
the set Â¯r(M) and the rewards are selected such that only the ï¬rst action from every state
yields a positive reward, i.e., R(s âˆˆ r(M), a1, Â·) = 1 and 0 otherwise. An instance of an
MDP constructed from a 40-node Gaussian partition graph is illustrated in Figure 18. The
speciï¬cations for the three environments are given in the caption of Table 4.

39

10-410-310-210-1 0    0.02    0.04   Atia et al.

For each example, we generate EP, CP and CPU policies using LP1, LP2 and Algorithm
1, respectively, and report on the runtime as we increase n. All instances were veriï¬ed to
meet the given speciï¬cations. The results are summarized in Table 4 demonstrating the
scalability of the proposed formulations. As shown, LP2 incurs the largest runtime as it
incorporates additional variables in the ï¬‚ow constraints to enforce the recurrence of the
TSCCs.

Figure 18: A NetworkX random 40-node digraph used to generate a LMDP for the third
example of Table 4.

To further examine the scalability of the LPs underlying the diï¬€erent policies to much
larger problem sizes, additional experiments are conducted using the CPLEX 12.8 solver.
We run simulations of the LP in (22), LP1 (with the positivity constraints) and LP2 (with
the ï¬‚ow constraints) for random instances of the Frozen Islands problem. The runtime
results are reported in Table 5. For a 64 Ã— 64 and a 128 Ã— 128 grid, LP2 (the most complex)
is solved in about 20 seconds and 15 minutes, respectively, demonstrating the eï¬€ectiveness
of the developed formulations even for MDPs with over ten thousand states.

8. Conclusion

A framework for steady-state policy synthesis in general MDPs was developed to derive
policies that satisfy constraints on the steady-state behavior of an agent. Linear program-
ming solutions were proposed and their correctness proved for classes of edge-preserving and
class-preserving policies. The framework also enables policies that meet speciï¬ed constraints
on the expected number of times the agent visits transient states. Numerical simulations of
the resulting policies demonstrate that our approach overcomes limitations in the literature.
The article provides the ï¬rst solution to the highly understudied problem of steady-
state planning over stationary policies in constrained expected average reward multichain
MDPs. The policies derived come with rigorous guarantees on the asymptotic long-term

40

States in Ò§ğ‘Ÿ()States in ğ‘Ÿ()Steady-State Planning in Expected Reward Multichain MDPs

Example

Policy

Toll Collector, 3n

Frozen Lake, n Ã— n

Random Gaussian, n

15

30

45

60

75

8 Ã— 8

12 Ã— 12

16 Ã— 16

20 Ã— 20

20

40

60

80

EP
CP
CPU

0.5
1.16
0.45

1.18
3.85
0.87

2.4
8.37
1.54

4.7
15.1
2.48

6.99
24.48
3.72

1.96
5.08
1.97

2.87
25.81
4.77

5.17
108.49
7.6

7.97
431.38
12.62

0.61
4.56
4.8

1.58
5.84
6.74

2.34
19.06
12.04

4.16
35.74
17.35

Table 4: Average runtime results (in seconds) for 20 instances of the Toll Collector, Frozen
Islands, and Gaussian partition graphs of increasing problem size n. The Toll Collector
MDP consists of three TSCCs, each of size n. The detailed LMDP parameters are given
in the caption of Figure 13 with a steady-state speciï¬cation lower bound l = 0.05. The
three-island problem described in Figure 3 forms an n Ã— n grid.
In each of the smaller
islands, logs are randomly distributed over 1/4 of the states and a canoe (ï¬shing rod) is
placed in the top-left (bottom-right) tile. For these experiments, we have the constraints
(Llog1 âˆªLlog2, [0.3, 1]), (Lcanoe1 âˆªLcanoe2, [0.05, 1]) and reward function R(Â·, Â·, Lï¬sh1 âˆªLï¬sh2) =
1, R(Â·, Â·, S \ Lï¬sh1 âˆª Lï¬sh2) = 0. For the Gaussian partition graphs, we deï¬ne a steady-
state speciï¬cation (L, [0.05, 1]), where L = {si}, for some si âˆˆ r(M). The probability of
intra-cluster connection pin = 0.9 and the probability of inter-cluster connection pout is
0.05, 0.01, 0.01, 0.005 for the 20, 40, 60, 80 nodes, respectively.

LP

Frozen Islands Example

Size, n Ã— n

8 Ã— 8

16 Ã— 16

32 Ã— 32

64 Ã— 64

128 Ã— 128

LP1 (20)
LP2 (21)
LP3 (22)

0.0001
0.003
0.0001

0.0017
0.038
0.0018

0.0170
0.595
0.0168

0.1187
20.251
0.1553

20.306
933.821
5.425

Table 5: Average runtime (in seconds) of 20 instances per LP for the three-island problem
described in Figure 3. These islands combined form an n Ã— n grid. In each of the smaller
islands, logs are randomly distributed over 1/4 of the states and a canoe (ï¬shing rod) is
placed in the top-left (bottom-right) tile. For these experiments, we have the constraints
(Llog1 âˆªLlog2, [0.3, 1]), (Lcanoe1 âˆªLcanoe2, [0.05, 1]) and reward function R(Â·, Â·, Lï¬sh1 âˆªLï¬sh2) =
1, R(Â·, Â·, S \ Lï¬sh1 âˆª Lï¬sh2) = 0.

behavior of agents. The research ï¬ndings have bearing on the ï¬elds of explainable, safe and
trustworthy AI, where there is increased concern about explaining AI decisions, ensuring
safety constraints are met, and building trust in the behavior of autonomous agents.

Acknowledgments

This research was supported in part by the Air Force Research Laboratory through the
Information Directorateâ€™s Information InstituteÂ® contract number FA8750-20-3-1003 and

41

Atia et al.

FA8750-20-3-1004, the Air Force Oï¬ƒce of Scientiï¬c Research through Award 20RICOR012,
and the National Science Foundation through CAREER Award CCF-1552497 and Award
CCF-2106339.

Appendix A. Technical Lemmas

Proof of Lemma 2

Let Ï€ âˆˆ Î EP. Hence, r(MÏ€) = r(M) according to (15). Consider the set of states rk(M)
in a TSCC of M, for some k âˆˆ [m]. We will show that this set is also a TSCC of MÏ€.
To this end, we ï¬rst show that they form a SCC in the transition graph of MÏ€. Since
Ï€ âˆˆ Î EP, then Ï€(a|s) > 0, âˆ€s âˆˆ r(M), a âˆˆ A(s). Hence, every action a âˆˆ A(s) available
in state s âˆˆ rk(M) is played with non-zero probability. From (1), for a pair of states
s, s(cid:48) âˆˆ rk(M), TÏ€(s(cid:48)|s) > 0 if âˆƒa âˆˆ A(s) such that T (s(cid:48)|s, a) > 0. Thus, for every directed
path between a pair of nodes in rk(M) in the transition graph of M, there is a similar path
between the same nodes in the transition graph of MÏ€. Therefore, the states rk(M) also
form a SCC in the transition graph of MÏ€. Also, the set rk(M) is reachable in MÏ€ since
rk(M) âŠ† r(MÏ€). Finally, there are no outgoing edges to states in S \ rk(MÏ€) since the
edge set of the transition graph of MÏ€ is a subset of the edge set of the transition graph
of M. We conclude that rk(MÏ€) = rk(M), âˆ€k âˆˆ [m]. From the deï¬nition of the set of CP
policies in (16), it follows that Ï€ âˆˆ Î CP, proving that Î EP âŠ† Î CP. The two conditions in
(16) are special cases of the more general requirements in (17), hence Î CP âŠ† Î CPU.

The following lemma gives a characterization of the Markov chain state classiï¬cation

induced by a policy (14) derived from a feasible point of the constrained set Q0 in (19).

Lemma 3. Given an MDP M, let (x, y) âˆˆ Q0 deï¬ned in (19) and Ï€ := Ï€(x, y) as in (14).
The following holds for the Markov chain MÏ€.

(a) If s âˆˆ Â¯r(M), then s âˆˆ Â¯r(MÏ€), i.e., Â¯r(M) âŠ† Â¯r(MÏ€).

(b) If s âˆˆ rk(M) âˆ© Ex for some k âˆˆ [m], then s âˆˆ r(MÏ€). As a consequence, if s âˆˆ

rk(M) âˆ© Â¯r(MÏ€), then s âˆˆ Ex, i.e., xs = 0.

Proof of Lemma 3

First, we show part (a), according to which every state in Â¯r(M) is either transient or isolated
in the Markov chain MÏ€ induced by a policy of the form (14) derived from a point in Q0.
Consider f âˆˆ Â¯r(M). From constraint (iii ), we have xf = 0. Thus, from constraint (ii ),
(14) and the fact that f is only reachable from states in Â¯r(M),

yf = Î²f +

(cid:88)

(cid:88)

yf (cid:48)aT (f |f (cid:48), a)

f (cid:48)âˆˆÂ¯r(M)
(cid:88)

aâˆˆA(f (cid:48))
(cid:88)

yf (cid:48)

= Î²f +

T (f |f (cid:48), a)Ï€(a|f (cid:48))

f (cid:48)âˆˆÂ¯r(M)
(cid:88)

aâˆˆA(f (cid:48))
yf (cid:48)TÏ€(f |f (cid:48))

= Î²f +

f (cid:48)âˆˆÂ¯r(M)

42

(36)

Steady-State Planning in Expected Reward Multichain MDPs

Note that the second equality above follows from the deï¬nition of Ï€ in (14) for a state
f /âˆˆ Ex. Two cases arise. If f /âˆˆ Ey, then yf = 0. Hence, Î²f = 0 and TÏ€(f |f (cid:48)) = 0, âˆ€f (cid:48) âˆˆ Ey.
Thus, we have shown that every state f in Ey âˆ© Â¯r(M) can only be reached from states in
Ey âˆ© Â¯r(M), and that all such states have zero initial probability. Thus, every such state is
either isolated or resides in an isolated component. Therefore, f âˆˆ Â¯r(MÏ€), where Â¯r(MÏ€)
consists of transient or isolated states. Now consider the other case where f âˆˆ Ey, i.e.,
yf > 0. Assume, for the sake of contradiction, that f âˆˆ r(MÏ€). Hence, f âˆˆ F , for some
TSCC F (this subsumes the case where f is absorbing with |F | = 1). Then, it must be
that F âŠ† Â¯r(M) since f is not reachable from states S \ Â¯r(M) even under an EP policy.
Summing (36) over the set F , we have

(cid:88)

f (cid:48)âˆˆF

yf (cid:48) =

=

(cid:88)

f (cid:48)âˆˆF
(cid:88)

f (cid:48)âˆˆF

(cid:88)

(cid:88)

Î²f (cid:48) +

yf (cid:48)TÏ€(j|f (cid:48))

jâˆˆF

f (cid:48)âˆˆÂ¯r(M)
(cid:88)

(cid:88)

TÏ€(j|f (cid:48)) +

yf (cid:48)

Î²f (cid:48) +

f (cid:48)âˆˆÂ¯r(M)\F

jâˆˆF

(cid:88)

f (cid:48)âˆˆF

yf (cid:48) ,

(37)

where the second equality is due to the closure of the set F , implying that (cid:80)
jâˆˆF TÏ€(j|f (cid:48)) = 1
for f (cid:48) âˆˆ F . It follows that Î²f (cid:48) = 0, âˆ€f (cid:48) âˆˆ F , and TÏ€(f |f (cid:48)) = 0, âˆ€f (cid:48) âˆˆ (Â¯r(M) \ F ) âˆ© Ey.
Therefore, F âŠ† Â¯r(MÏ€), yielding a contradiction. Hence, f âˆˆ Â¯r(MÏ€). We conclude that
Â¯r(M) âŠ† Â¯r(MÏ€).

Next, we prove part (b) which states that every state s in a TSCC of M for which
xs > 0 is both recurrent and non-isolated in MÏ€. Consider a state s âˆˆ rk(M) âˆ© Ex for
some k âˆˆ [m], so xs > 0. Assume, for the sake of contradiction, that s âˆˆ Â¯r(MÏ€), i.e., the
state s is either transient or isolated. If s is transient, then the column of the matrix T âˆ
Ï€
corresponding to state s is zero. Therefore, from constraint (i ) in (19), we have xs = 0, i.e.,
s /âˆˆ Ex, yielding a contradiction. If s âˆˆ F for some isolated component F , then

(cid:88)

s(cid:48)âˆˆF

(xs(cid:48) + ys(cid:48)) =

(cid:88)

(cid:88)

(cid:88)

s(cid:48)âˆˆF

f âˆˆF

aâˆˆA(f )

yf aT (s(cid:48)|f, a)

by summing constraint (ii ) over states s(cid:48) âˆˆ F , and using the fact that Î²f = 0, âˆ€f âˆˆ F and
that s(cid:48) âˆˆ F is only reachable from states in the isolated set F . Since (cid:80)
s(cid:48)âˆˆF T (s(cid:48)|f, a) =
1, âˆ€f âˆˆ F, a âˆˆ A(f ), by the closure of F , we get that (cid:80)
s(cid:48)âˆˆF xs(cid:48) = 0 by interchanging the
order of the sums, i.e., s âˆˆ Ex, also yielding a contradiction. Hence, s âˆˆ r(MÏ€).

The second clause of Lemma 3 (b) remains to be proved, i.e., s âˆˆ rk(M) âˆ© Â¯r(MÏ€) =â‡’
xs = 0. Consider s âˆˆ rk(M) âˆ© Â¯r(MÏ€). Thus, s /âˆˆ r(MÏ€), so it follows from the result we
have just shown that s âˆˆ Â¯r(M) âˆª Ex. However, since s âˆˆ rk(M) for some k, then s /âˆˆ Â¯r(M).
Hence, s âˆˆ Ex.

Next, we state and prove two lemmas that will be useful in the proof of Lemma 6, which
establishes a suï¬ƒcient condition for the existence of a one-to-one correspondence between
a feasible point in Q0 and the steady-state distribution of the Markov chain induced by the
policy in (14) derived from this solution.

Lemma 4. Given MDP M, if (x, y) âˆˆ Q0, where Q0 is the set of points in (19), then x is
a stationary distribution of the Markov chain MÏ€ induced by the policy Ï€ in (14).

43

Atia et al.

Proof of Lemma 4

First, consider s(cid:48) âˆˆ Â¯r(M), and deï¬ne X0 := {x : (x, y) âˆˆ Q0 for some y}. Since x âˆˆ X0, we
have xs(cid:48) = 0 by constraint (iii ). Also,

xsTÏ€(s(cid:48)|s) =

(cid:88)

sâˆˆS

(cid:88)

sâˆˆÂ¯r(M)

xsTÏ€(s(cid:48)|s) = 0 ,

(38)

where the ï¬rst equality holds since s(cid:48) âˆˆ Â¯r(M) is only reachable from states s âˆˆ Â¯r(M)
even when all edges deï¬ning possible transitions in the MDP are preserved. Next, consider
s(cid:48) âˆˆ S \ Â¯r(M). We have

xs(cid:48) :=

(cid:88)

xs(cid:48)a =

(cid:88)

(cid:88)

xsaT (s(cid:48)|s, a) =

(cid:88)

(cid:88)

xsÏ€(a|s)T (s(cid:48)|s, a) =

aâˆˆA(s(cid:48))

sâˆˆS

aâˆˆA(s)

sâˆˆEx

aâˆˆA(s)

xsTÏ€(s(cid:48)|s)

(cid:88)

sâˆˆS

(39)

The ï¬rst equality follows from the fact that x âˆˆ X0, the second from the deï¬nition of Ï€ in
(14), and the last from the deï¬nition of TÏ€ in (1) and that xs = 0, âˆ€s âˆˆ S \ Ex. Finally,
x(cid:62)e = 1, by summing constraints (ii ) over all s(cid:48) âˆˆ S.

Lemma 5. Given an MDP M, let (x, y) âˆˆ Q0 and Ï€ := Ï€(x, y) as in (14). If Ï€ âˆˆ Î CPU,
then the subvector xrk(MÏ€) of x must satisfy the following identity for all k âˆˆ [m]

rk(MÏ€)e = Î²(cid:62)
x(cid:62)

rk(MÏ€)e + Î²(cid:62)

Â¯r(MÏ€)PÏ€,k ,

(40)

where, PÏ€,k = [pf k], f âˆˆ Â¯r(MÏ€), is the vector of absorption probabilities from Â¯r(MÏ€) into
rk(MÏ€) under policy Ï€.

Proof of Lemma 5

To show (40), note that, since Ï€ âˆˆ Î CPU, we have that rk(MÏ€) âŠ† rk(M), k âˆˆ [m], where
rk(MÏ€) âŠ‚ r(MÏ€) denotes the k-th TSCC of MÏ€. Since (x, y) âˆˆ Q0, by summing constraints
(ii ) in (19) over the set rk(MÏ€), we get

(cid:88)

Î²s =

(cid:88)

xs +

(cid:88)

ys âˆ’

(cid:88)

(cid:88)

(cid:88)

T (s|s(cid:48), a)ys(cid:48)a

sâˆˆrk(MÏ€)

sâˆˆrk(MÏ€)

sâˆˆrk(MÏ€)

sâˆˆrk(MÏ€)

s(cid:48)âˆˆrk(MÏ€)âˆªÂ¯r(MÏ€)

aâˆˆA(s(cid:48))

(41)

where we used the fact that rk(MÏ€) is only reachable from states in rk(MÏ€) âˆª Â¯r(MÏ€). By
breaking the summation in the last term on the RHS of (41) over states s(cid:48) in the union of
the disjoint sets rk(MÏ€) and Â¯r(MÏ€) and interchanging the order of the summations over s
and s(cid:48), the last term in (41) simpliï¬es to

(cid:88)

(cid:88)

(cid:88)

ys(cid:48)a

T (s|s(cid:48), a) +

(cid:88)

(cid:88)

(cid:88)

T (s|s(cid:48), a)ys(cid:48)a

s(cid:48)âˆˆrk(MÏ€)
(cid:88)

=

aâˆˆA(s(cid:48))

ys(cid:48) +

sâˆˆrk(MÏ€)
(cid:88)

ys(cid:48)

s(cid:48)âˆˆÂ¯r(MÏ€)

sâˆˆrk(MÏ€)

aâˆˆA(s(cid:48))

(cid:88)

TÏ€(s|s(cid:48)) ,

s(cid:48)âˆˆrk(MÏ€)

s(cid:48)âˆˆÂ¯r(MÏ€)

sâˆˆrk(MÏ€)

(42)

44

Steady-State Planning in Expected Reward Multichain MDPs

where the ï¬rst term on the RHS of the equality (42) follows from the closure of rk(MÏ€)
(which implies that (cid:80)
sâˆˆrk(MÏ€) T (s|s(cid:48), a) = 1 for s(cid:48) âˆˆ rk(MÏ€)), and the second term from
the deï¬nition of the policy in (14) for states in Ex (noting that s(cid:48) âˆˆ Â¯r(MÏ€) implies xs(cid:48) = 0).
Replacing (42) in (41), we get that

(cid:88)

Î²s =

(cid:88)

xs âˆ’

(cid:88)

ys(cid:48)

(cid:88)

TÏ€(s|s(cid:48))

(43)

sâˆˆrk(MÏ€)

sâˆˆrk(MÏ€)

s(cid:48)âˆˆÂ¯r(MÏ€)

sâˆˆrk(MÏ€)

We proceed to further simplify the second term on the RHS of (43). Since xs = 0, âˆ€s âˆˆ

Â¯r(MÏ€), it follows from constraint (ii ) of (19) and (14) that

ys = Î²s +

(cid:88)

(cid:88)

ys(cid:48)

Ï€(a|s(cid:48))T (s|s(cid:48), a) = Î²s +

(cid:88)

ys(cid:48)TÏ€(s|s(cid:48)), âˆ€s âˆˆ Â¯r(MÏ€) . (44)

s(cid:48)âˆˆÂ¯r(MÏ€)

aâˆˆA(s(cid:48))

s(cid:48)âˆˆÂ¯r(MÏ€)

In matrix form, this can be rewritten as

yÂ¯r(MÏ€) = (I âˆ’ Z(cid:62)

Ï€ )âˆ’1Î²Â¯r(MÏ€) ,

(45)

where ZÏ€ = [zs(cid:48)s] âˆˆ [0, 1]|Â¯r(MÏ€)|Ã—|Â¯r(MÏ€)|, with zs(cid:48)s := TÏ€(s|s(cid:48)). Hence, the second summation
in (43) can be written as

(cid:88)

ys(cid:48)

(cid:88)

s(cid:48)âˆˆÂ¯r(MÏ€)

sâˆˆrk(MÏ€)

TÏ€(s|s(cid:48)) = y(cid:62)LÏ€,ke = Î²(cid:62)

Â¯r(MÏ€)(I âˆ’ ZÏ€)âˆ’1LÏ€,ke ,

(46)

where LÏ€,k is the submatrix of TÏ€ of transitions from Â¯r(MÏ€) to rk(M) under policy Ï€ as
in (5). From (43) and (46),

(cid:88)

sâˆˆrk(MÏ€)

xs = Î²(cid:62)

rk(MÏ€)e + Î²(cid:62)

Â¯r(MÏ€)(I âˆ’ ZÏ€)âˆ’1LÏ€,ke .

(47)

Since the vector PÏ€,k is the scaled (by the inverse of Î·s) s-th column of the submatrix of the
matrix T âˆ
Ï€ deï¬ning transitions from Â¯r(MÏ€) to rk(MÏ€), we have (Feller, 1968; Puterman,
1994),

PÏ€,k = (I âˆ’ ZÏ€)âˆ’1LÏ€,ke ,

(48)

which proves the identity (40) of Lemma 5.

We can readily state the next Lemma which establishes the aforementioned suï¬ƒciency

condition.

Lemma 6. Given an MDP M, let (x, y) âˆˆ Q0 and Ï€ := Ï€(x, y) as in (14). If Ï€ âˆˆ Î CPU,
then Prâˆ

Ï€ = x.

Before we prove Lemma 6, we remark that this result also holds for policies in Î EP and

Î CP since these are subsets of Î CPU per Lemma 2.

45

Atia et al.

Proof of Lemma 6

We seek to show that the steady-state distribution of the Markov chain MÏ€ induced by the
policy Ï€ (14) derived from a feasible point (x, y) âˆˆ Q0 matches x, provided that Ï€ is a CPU
policy, where Q0 is as deï¬ned in (19).

First, we consider the states in Â¯r(MÏ€). We have that Prâˆ

Ï€ (s) = 0, âˆ€s âˆˆ Â¯r(MÏ€) since such
states are either transient or isolated in the Markov chain MÏ€ induced by policy Ï€. Next, we
argue that xs = 0 for all such states. From Lemma 3 (a), we have that Â¯r(M) âŠ† Â¯r(MÏ€). For
states s âˆˆ Â¯r(M), xs = 0 by constraint (iii ) in (19). Thus, we have shown that Prâˆ
Ï€ (s) = xs
for every s âˆˆ Â¯r(M). Now, consider a state s âˆˆ Â¯r(MÏ€) \ Â¯r(M). The state s must belong to
rk(M) âˆ© Â¯r(MÏ€) for some k âˆˆ [m], where m is the number of TSCCs in M. Hence, xs = 0
by Lemma 3 (b). Therefore, we have argued that xs = Prâˆ

Ï€ (s) = 0, âˆ€s âˆˆ Â¯r(MÏ€).

Second, we consider states in r(MÏ€). According to Lemma 4, x satisï¬es

rk(MÏ€) = x(cid:62)
x(cid:62)

rk(MÏ€)TÏ€,k, âˆ€k âˆˆ [m] ,

(49)

where TÏ€,k is the submatrix of TÏ€ of transitions between states in rk(MÏ€). We have also
shown that xrk(MÏ€) satisï¬es the identity (40) stated in Lemma 5.
Ï€ (s) = Î·s

Ï€ (s) in Lemma 1 and (6), Prâˆ

Given the deï¬nition of Prâˆ

rk(M)e + Î²(cid:62)
Î²(cid:62)

Â¯r(M)PÏ€,k

(cid:17)

(cid:16)

.

Hence,

(cid:88)

sâˆˆrk(MÏ€)

Prâˆ

Ï€ (s) = Î²(cid:62)

rk(MÏ€)e + Î²(cid:62)

Â¯r(MÏ€)PÏ€,k .

From (40) and (50), we conclude that

(cid:88)

Prâˆ

Ï€ (s) =

(cid:88)

xs.

sâˆˆrk(MÏ€)

sâˆˆrk(MÏ€)

(50)

(51)

The ergodic theorem of Markov chains asserts that the solution to x(cid:62)T = x(cid:62), where x(cid:62)e =
1, x â‰¥ 0, is unique iï¬€ T is the transition matrix of a unichain (Gallager, 2013; Altman,
1999). From (49), (50) and (51), we have shown that

k TÏ€,k = x(cid:62)
x(cid:62)

k , where x(cid:62)

k e = ck, xk â‰¥ 0

for TSCCs k âˆˆ [m], where xk := xrk(MÏ€), ck is the RHS of (50), and (cid:80)m
k=1 ck = 1. Further,
since Ï€ âˆˆ Î CPU, every TSCC is a unichain. Hence, by the ergodic theorem, the solution
xrk(MÏ€) to (49) and (40) is unique for each component rk(MÏ€), k âˆˆ [m], thus x is equal to
the unique steady-state distribution, i.e., x = Prâˆ
Ï€ .

We also make use of the following lemma in the proof of the converse part of Theorem
2. The lemma establishes that all occupation measures induced by the policies of interest
are Q0-feasible.

Lemma 7. Given MDP M, let X0 := {x : (x, y) âˆˆ Q0 for some y}, where Q0 is as deï¬ned
in (19). Then, P âˆ(Î CPU) âŠ† X0.

46

Steady-State Planning in Expected Reward Multichain MDPs

Proof of Lemma 7

We show that the steady-state distribution induced by every CPU policy is in X0. To this
end, let x âˆˆ P âˆ(Î CPU), i.e., âˆƒÏ€ âˆˆ Î CPU : Prâˆ
Ï€ is as deï¬ned in Lemma 1.
Therefore, x is a stationary distribution of the Markov chain MÏ€, in which rk(MÏ€), k âˆˆ [m]
are TSCCs and states Â¯r(MÏ€) are either transient or isolated. Hence, x(cid:62) = x(cid:62)TÏ€. Therefore,

Ï€ = x, where Prâˆ

xs(cid:48) :=

(cid:88)

(cid:88)

xs(cid:48)a =

xsTÏ€(s(cid:48)|s) =

(cid:88)

(cid:88)

xsÏ€(a|s)T (s(cid:48)|s, a)

aâˆˆA(s(cid:48))
(cid:88)

(cid:88)

=

sâˆˆS
xsaT (s(cid:48)|s, a) ,

sâˆˆS

aâˆˆA(s)

sâˆˆS

aâˆˆA(s)

(52)

where the last equality follows since Prâˆ
Ï€ (s)Ï€(a|s). Thus, the steady-state
distribution x satisï¬es constraint (i ) in (19). From the deï¬nition of Î CPU in (17), every
f âˆˆ Â¯r(M) is either transient or isolated under Ï€. Thus, xÂ¯r(M) := {Prâˆ
Ï€ (f, a)}f âˆˆÂ¯r(M),a = 0,
satisfying constraint (iii ).

Ï€ (s, a) = Prâˆ

The variables yf a, f âˆˆ Â¯r(MÏ€), a âˆˆ A(f ), can be set as in (45), i.e., choose yf a =
Î²(cid:62)
Â¯r(MÏ€)(I âˆ’ ZÏ€)âˆ’1ef Ï€(a|f ), f âˆˆ Â¯r(MÏ€), a âˆˆ A(f ), where ZÏ€ is the submatrix of TÏ€ deï¬ned
in (5), which satisï¬es the constraints (ii ) as we have already shown in (44). The remaining
variables ysa, s âˆˆ rk(MÏ€), a âˆˆ A(s), can now be chosen in terms of xsa, yf a, T (s(cid:48)|s, a) and
Î² such that the corresponding constraints (ii ) are satisï¬ed. Thus, for the given x, we have
shown the existence of a feasible y such that (x, y) âˆˆ Q0. Therefore, x âˆˆ X0.

Appendix B. Proof of Main Theorems

Proof of Theorem 1

Since (x, y) is a feasible point of LP1, we have that (x, y) âˆˆ Q0 per (20). From Lemma
3 (a), Â¯r(M) âŠ† Â¯r(MÏ€), thus r(M) âŠ‡ r(MÏ€). Consider a state s âˆˆ r(M). Then, s âˆˆ
rk(M) for some k âˆˆ [m]. From the positivity constraint (v ) of LP1, we also have that
xs > 0, i.e., s âˆˆ Ex. Since s âˆˆ rk(M) âˆ© Ex, it follows that s âˆˆ r(MÏ€) by Lemma 3
(b). Therefore, r(M) âŠ† r(MÏ€). We conclude that r(MÏ€) = r(M). From constraint (v ),
xsa > 0, âˆ€s âˆˆ r(M), a âˆˆ A(s). It follows from the deï¬nition of Ï€ in (14) for states s âˆˆ Ex
that Ï€(a|s) > 0, âˆ€s âˆˆ r(M), a âˆˆ A(s). We have shown that Ï€ satisï¬es both requirements in
(15), hence Ï€ âˆˆ Î EP.

Proof of Theorem 2

( =â‡’ ) First, we show that if (20) is feasible, then there exist an EP policy that meets the
speciï¬cations Î¦âˆ
L . Let (x, y) âˆˆ Q1 denote a feasible solution to (20) and let Ï€ be deï¬ned
as in (14). By Theorem 1, Ï€ âˆˆ Î EP. By Lemma 2, we also have that Ï€ âˆˆ Î CPU. Invoking
Lemma 6, we conclude that Prâˆ
Ï€ (s, a) = xsa, s âˆˆ S, a âˆˆ A(s), i.e., x is equal to the steady-
state distribution of the Markov chain MÏ€ induced by policy Ï€. Since x satisï¬es constraint
(iv ), this implies that MÏ€ meets the speciï¬cations Î¦âˆ
L .
( â‡= ) Now, we show the converse, that is, the existence of an EP policy that meets the
speciï¬cations implies that LP1 in (20) is feasible. Deï¬ne V := {x : (iv) and (v) satisï¬ed}.
Thus, we have that XLP1 = X0 âˆ© V , where XLP1 = {x : (x, y) âˆˆ Q1 for some y}. Suppose

47

Atia et al.

Ï€ (s) := (Î²(cid:62)T âˆ

Ï€ âˆˆ P âˆ(Î EP) is well-deï¬ned as in Lemma 1. We have Prâˆ

âˆƒÏ€ âˆˆ Î EP that satisï¬es the speciï¬cations Î¦âˆ
L as in the statement of Theorem 2. Then
Prâˆ
Ï€ )s > 0, âˆ€s âˆˆ
r(MÏ€), since all such states are recurrent in the Markov chain MÏ€. Since Ï€ âˆˆ Î EP,
Ï€(a|s) > 0, âˆ€s âˆˆ r(M), a âˆˆ A(s), from (15). Hence, by Lemma 1, Prâˆ
Ï€ (s, a) > 0, âˆ€s âˆˆ
r(M), a âˆˆ A(s). Therefore, Prâˆ
Ï€ âˆˆ V . Hence, P âˆ(Î EP) âˆ© V is non-empty. Set xsa =
Prâˆ
Ï€ (s, a), s âˆˆ S, a âˆˆ A(s). Recall that Â¯r(MÏ€) = Â¯r(M) since Ï€ âˆˆ Î EP, so we have xsa =
Prâˆ
Ï€ (s, a) = 0, âˆ€s âˆˆ Â¯r(M). From Lemma 7, P âˆ(Î EP) âŠ† X0, where we also use the fact that
P âˆ(Î EP) âŠ† P âˆ(Î CPU) as a consequence of Lemma 2. The variables ysa can be deï¬ned
in terms of xsa, T (s(cid:48)|s, a) and Î² such that the constraints (ii ) are satisï¬ed. Hence, XLP1,
and in turn Q1, is non-empty. The optimality of Ï€âˆ— follows from the optimality of (xâˆ—, yâˆ—),
Theorem 1 and the established equality Prâˆ

Ï€âˆ— = xâˆ—.

Proof of Theorem 3

Let f âˆˆ Â¯r(M). From Lemma 3 (a), f âˆˆ Â¯r(MÏ€). Now consider s âˆˆ rk(M) for some k âˆˆ [m].
As argued earlier, every state in rk(M) is reachable from s given constraints (viii ), (x ), (xii )
of (21). In addition, s is reachable from all states in rk(M), which follows from constraints
(vii ), (ix ), (xi ), (xiii ). Hence, s âˆˆ r(MÏ€). Therefore, r(M) âŠ† r(MÏ€). Since we have
already shown that Â¯r(M) âŠ† Â¯r(MÏ€), we conclude that r(MÏ€) = r(M). Therefore, Ï€ âˆˆ Î CP
deï¬ned in (16).

Proof of Theorem 4

Ï€ (s, a) = xsa, s âˆˆ S, a âˆˆ A(s), which implies that MÏ€ meets the speciï¬cations Î¦âˆ

The proof follows the same reasoning as that of Theorem 2.
( =â‡’ ) Let (x, y, f, f rev) âˆˆ Q2 denote a feasible solution to (21) and let Ï€ be deï¬ned
Invoking Lemma 6 and Lemma 2, we have that
as in (14). By Theorem 3, Ï€ âˆˆ Î CP.
Prâˆ
L per
constraint (iv ).
( â‡= ) Deï¬ne V := {x : (iv) and (vi) âˆ’ (xiv) satisï¬ed}. Thus, we have that XLP2 =
X0 âˆ© V , where XLP2 = {x : (x, y, f, f rev) âˆˆ Q2}. Suppose âˆƒÏ€ âˆˆ Î CP that satisï¬es the
speciï¬cations Î¦âˆ
L as in the statement of Theorem 4. Hence, rk(MÏ€), k âˆˆ [m] are the
recurrent components of MÏ€. Then, Prâˆ
Ï€ âˆˆ P âˆ(Î CP) is well-deï¬ned as in Lemma 1. We
can set xsa = Prâˆ
Ï€ (s) for every s âˆˆ S, a âˆˆ A(s). The ï¬‚ow variables in
(vi ) - (vii ) can be deï¬ned in terms of xsa and T (s(cid:48)|s, a) such that the constraints (x ) - (xiii )
are satisï¬ed. Hence, x âˆˆ V , i.e., P âˆ(Î CP) âˆ© V is non-empty. By Lemma 7, XLP2 and Q2
are non-empty. The optimality of Ï€âˆ— follows from the optimality of (xâˆ—, yâˆ—), Theorem 3 and
the established equality Prâˆ

Ï€ (s, a) = Ï€(a|s)Prâˆ

Ï€âˆ— = xâˆ—.

Proof of Theorem 5

Assume (x, y) âˆˆ Qâˆ—. We have that V +
k (x) âŠ† rk(MÏ€) for Ï€ in (14) by Lemma 3 (b).
Further, consider s âˆˆ rk(M) âˆ© Ex. By constraint (i ) and the deï¬nition of Ï€ in (14),
TÏ€(s|s(cid:48)) = 0, âˆ€s(cid:48) âˆˆ V +
k (x). For the sake of contradiction, assume s âˆˆ r(MÏ€). Hence,
s âˆˆ F âŠ† rk(M) for some TSCC F of MÏ€. Summing constraints (ii ) over the set F , we
get that Î²s = 0, âˆ€s âˆˆ F and TÏ€(s|s(cid:48)) = 0, s(cid:48) âˆˆ Â¯r(M), s âˆˆ F . Hence, s âˆˆ Â¯r(MÏ€), yielding a
contradiction. We conclude that V +
k (x) = r(MÏ€) âˆ© rk(M). Therefore, if for every k âˆˆ [m]

48

Steady-State Planning in Expected Reward Multichain MDPs

we have that the subgraph (V +
in MÏ€, for Ï€ in (14). Hence, V +
Ï€ âˆˆ Î CPU. The result now follows from Lemma 6.

k (x)) is strongly connected, then V +

k (x), E+
k (x) is a SCC
k (x) is the unique TSCC rk(MÏ€) in the set rk(M), i.e.,

Proof of Theorem 6

(1) Every feasible solution of LP1((cid:15)) is also LP1-feasible. Hence, the result follows as an
immediate consequence of Theorem 1.
(2) The proof of part (2) follows the same reasoning as in the proof of the converse of The-
orem 2. Speciï¬cally, we have shown that, if Ï€ âˆˆ Î EP, then Prâˆ
Ï€ (s, a) > 0, âˆ€s âˆˆ r(MÏ€), a âˆˆ
A(s). Hence, âˆƒ(cid:15) > 0 such that Prâˆ
Ï€ âˆˆ V (cid:48), where V (cid:48) := {x : (iv) and (v)(cid:48) satisï¬ed)}. There-
fore, XLP1((cid:15)) := X0 âˆ© V (cid:48) is non-empty, and in turn LP1((cid:15)) is feasible.
(3) Let (cid:15)n â†’ 0, n âˆˆ N, be a monotonically decreasing sequence, Ï€âˆ—
n the EP policy in
(14) corresponding to an optimal solution to LP1((cid:15)n), and Rn := Râˆ
(Î²). The sequence
Ï€âˆ—
n
(Rn)nâˆˆN is monotonically non-decreasing since Rn â‰¥ Rm whenever (cid:15)n < (cid:15)m. Further,
from (11), we have that the sequence is bounded above since supÏ€âˆˆÎ EP Râˆ
Ï€ (Î²) â‰¤ rmax,
where rmax := maxsâˆˆS,aâˆˆA(s) R(s, a). Since the sequence (Rn)nâˆˆN is both increasing and
bounded, it converges to the limit supn Rn by the monotone convergence theorem (Royden
& Fitzpatrick, 2010). We are only left to show that supn Rn = supÏ€âˆˆÎ EP Râˆ
Ï€ . To this
end, assume for the sake of contradiction that supn Rn < supÏ€âˆˆÎ EP Râˆ
Ï€ . Since the RHS
of the inequality is the least upper bound on the average reward of EP policies, then for
any Î´ > 0, âˆƒÏ€(cid:48) âˆˆ Î EP : Râˆ
Ï€ âˆ’ Î´. We can choose Î´ small enough such that
Ï€(cid:48) is LP1((cid:15)(cid:48))-feasible for all (cid:15)(cid:48) â‰¤ (cid:15).
Râˆ
Hence, from the deï¬nition of Ï€âˆ—
Ï€(cid:48) , yielding a contradiction.

Ï€(cid:48) > supn Rn. From part (2) above, âˆƒ(cid:15) > 0, such that Prâˆ
n, we get that supn Rn â‰¥ Râˆ

Ï€(cid:48) > supÏ€âˆˆÎ EP Râˆ

Proof of Theorem 7

(1) Let x be LP1(Î´)-feasible. Since the feasible set for LP1(Î´) is a subset of the feasible
set of LP1, then Ï€ âˆˆ Î EP by Theorem 1. Therefore, we only need to verify the bounded
support requirement in (27). For s âˆˆ r(M), we have that xsa â‰¥ Î´ > 0, a âˆˆ A(s), from
constraint (v)(cid:48) in LP1(Î´). Hence, Ï€(a|s) = xsa/xs â‰¥ Î´. Therefore, Ï€ âˆˆ Î EP(Î´).

(2) Assume Ï€ âˆˆ Î EP(Î´) and meets the speciï¬cations Î¦âˆ
L . Noting that Î EP(Î´) âŠ‚ Î EP,
then there exists an 0 < (cid:15) â‰¤ Î´ such that Prâˆ
Ï€ is a feasible solution of LP1((cid:15)), which
follows from part (2) of Theorem 6. Hence, maxÏ€âˆˆÎ EP(Î´) Râˆ
Ï€ (Î²) â‰¤ Râˆ—((cid:15)) since Râˆ—((cid:15)) is the
optimal value of LP1((cid:15)), where (cid:15) â‰¤ Î´ is a function of Î´. As Î´ â†’ 0, the sequence of rewards
Râˆ—(Î´) is monotonically non-decreasing and bounded above. Hence, as Î´ â†’ 0, the sequence
Râˆ—(Î´) converges to a limit. Every convergent sequence is a Cauchy sequence (Royden &
Fitzpatrick, 2010), i.e., the elements of the sequence become arbitrarily close to each other
as Î´ â†’ 0. Hence, Râˆ—((cid:15)) âˆ’ Râˆ—(Î´) â†’ 0, as Î´ â†’ 0.

Proof of Theorem 8

The cone V (x, y) in (29) is the cone of feasible directions from a feasible point (x, y),
i.e., directions v = (h, z) along which âˆƒÎ» > 0 such that (x, y) + Î»(h, z) is feasible. The
sets u(x), l(x), n(x) and m(y) denote the sets of active (upper and lower) speciï¬cation

49

Atia et al.

and non-negativity (of state-action variables x and y) constraints, respectively. Since the
rewards vector R is an interior point of the dual cone V âˆ—(x, y) designated in the statement of
Theorem 8, moving away from (x, y) along any feasible direction can only reduce the value
of the objective, i.e., (cid:80)
aâˆˆA(s) R(s, a)hsa < 0. Hence, (x, y) is the unique optimal
solution to (22). We have already shown that the set of occupation measures induced by
policies for which Â¯r(M) âŠ† Â¯r(MÏ€) is contained in the feasible set of (22). Since Ï€ in (14)
is one such policy by Lemma 3 (a), we have Prâˆ
Ï€ = x and Ï€ meets the speciï¬cations Î¦âˆ
L .
The uniqueness of Ï€ in this class of policies follows from the established uniqueness of the
optimal solution x.

(cid:80)

sâˆˆS

Proof of Proposition 1

By Lemma 3, we have that f âˆˆ Â¯r(MÏ€). If Ï€ âˆˆ Î CPU, then the condition of Lemma 6 is
met, and it follows from (45) that yf = Î²(cid:62)

Â¯r(MÏ€)(I âˆ’ ZÏ€)âˆ’1ef = Î¶Ï€(f ).

References

Akshay, S., Bertrand, N., Haddad, S., & HÂ´elouÂ¨et, L. (2013). The steady-state control
problem for Markov decision processes. In International Conference on Quantitative
Evaluation of Systems, pp. 290â€“304, Berlin Heidelberg. Springer.

Altman, E. (1998). Constrained Markov decision processes with total cost criteria: La-
grangian approach and dual linear program. Mathematical Methods of Operations
Research, 48 (3), 387â€“417.

Altman, E. (1999). Constrained Markov decision processes. CRC Press, Boca Raton.

Altman, E., Boularouk, S., & Josselin, D. (2019). Constrained Markov decision processes
with total expected cost criteria. In Proceedings of the 12th EAI International Con-
ference on Performance Evaluation Methodologies and Tools, pp. 191â€“192. ACM.

Atia, G., Beckus, A., Alkhouri, I., & Velasquez, A. (2020). Steady-state policy synthesis
in multichain Markov decision processes.
In Proceedings of the 29th International
Joint Conference on Artiï¬cial Intelligence (IJCAI), pp. 4069â€“4075. International Joint
Conferences on Artiï¬cial Intelligence Organization.

Ayala, A. M., Andersson, S. B., & Belta, C. (2014). Formal synthesis of control policies for
continuous time markov processes from time-bounded temporal logic speciï¬cations.
IEEE Transactions on Automatic Control, 59 (9), 2568â€“2573.

Baiocchi, D. (2010). Confronting Space Debris: Strategies and Warnings from Comparable

Examples Including Deepwater Horizon. Rand Corporation.

Baumgartner, P., ThiÂ´ebaux, S., & Trevizan, F. (2018). Heuristic search planning with
multi-objective probabilistic LTL constraints. In Sixteenth International Conference
on Principles of Knowledge Representation and Reasoning, pp. 415â€“424.

Bertsekas, D. (2005). Dynamic programming and optimal control, Vol. 2. Athena Scientiï¬c,

Belmont, Mass.

Bertsimas, D., & Tsitsiklis, J. (1997). Introduction to Linear Optimization (1st edition).

Athena Scientiï¬c.

50

Steady-State Planning in Expected Reward Multichain MDPs

Bhatnagar, S., & Lakshmanan, K. (2012). An online actorâ€“critic algorithm with function
approximation for constrained Markov decision processes. Journal of Optimization
Theory and Applications, 153 (3), 688â€“708.

Boussemart, M., & Limnios, N. (2004). Markov decision processes with asymptotic average
failure rate constraint. Communications in Statistics-Theory and Methods, 33 (7),
1689â€“1714.

Boussemart, M., Limnios, N., & Fillion, J. (2002). Non-ergodic Markov decision processes
with a constraint on the asymptotic failure rate: general class of policies. Stochastic
models, 18 (1), 173â€“191.

Brafman, R. I., & De Giacomo, G. (2019). Planning for LTLf /LDLf goals in non-Markovian
fully observable nondeterministic domains. In Proceedings of the Twenty-Eighth In-
ternational Joint Conference on Artiï¬cial Intelligence (IJCAI), pp. 1602â€“1608. Inter-
national Joint Conferences on Artiï¬cial Intelligence Organization.

Brandes, U., Gaertler, M., & Wagner, D. (2003). Experiments on graph clustering algo-

rithms. In European Symposium on Algorithms, pp. 568â€“579. Springer.

Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba,

W. (2016). OpenAI Gym. arXiv preprint arXiv:1606.01540.

Camacho, A., & McIlraith, S. A. (2019). Strong fully observable non-deterministic planning
with LTL and LTLf goals. In Proceedings of the Twenty-Eighth International Joint
Conference on Artiï¬cial Intelligence (IJCAI), pp. 5523â€“5531. International Joint Con-
ferences on Artiï¬cial Intelligence Organization.

Courcoubetis, C., & Yannakakis, M. (1995). The complexity of probabilistic veriï¬cation.

Journal of the ACM, 42 (4), 857â€“907.

De Ghellinck, G. (1960). Les probl`emes de dÂ´ecisions sÂ´equentielles. Cahiers du Centre

dâ€™Etudes de Recherche OpÂ´erationnelle, 2 (2), 161â€“179.

De Giacomo, G., Felli, P., Patrizi, F., & Sardina, S. (2010). Two-player game structures for
generalized planning and agent composition. In Twenty-Fourth AAAI Conference on
Artiï¬cial Intelligence.

Denardo, E. V., & Fox, B. L. (1968). Multichain Markov renewal programs. SIAM Journal

on Applied Mathematics, 16 (3), 468â€“487.

Derman, C. (1970). Finite State Markovian Decision Processes. Academic Press, Inc.,

Orlando, FL, USA.

Engesser, T., Bolander, T., & Nebel, B. (2017). Cooperative epistemic multi-agent planning
with implicit coordination. In Proceedings of the 3rd Workshop on Distributed and
Multi-Agent Planning (DMAP), p. 68.

Feinberg, E. A. (2009). Adaptive computation of optimal nonrandomized policies in con-
strained average-reward MDPs. In IEEE Symposium on Adaptive Dynamic Program-
ming and Reinforcement Learning, pp. 96â€“100.

Feinberg, E. A. (2000). Constrained discounted Markov decision processes and Hamiltonian

cycles. Mathematics of Operations Research, 25 (1), 130â€“140.

51

Atia et al.

Feller, W. (1968). An Introduction to Probability Theory and its Applications (3rd edition).,

Vol. 1. Wiley.

Gallager, R. G. (2013). Stochastic Processes: Theory for Applications. Cambridge University

Press, New York.

Grant, M., & Boyd, S. (2008). Graph implementations for nonsmooth convex programs.
In Blondel, V., Boyd, S., & Kimura, H. (Eds.), Recent Advances in Learning and
Control, Lecture Notes in Control and Information Sciences, pp. 95â€“110. Springer-
Verlag Limited.

Grant, M., & Boyd, S. (2014). CVX: Matlab software for disciplined convex programming,

version 2.1..

Guo, M., & Zavlanos, M. M. (2018). Probabilistic motion planning under temporal tasks

and soft constraints. IEEE Transactions on Automatic Control, 63 (12), 4051â€“4066.

Hagberg, A., Swart, P., & S Chult, D. (2008). Exploring network structure, dynamics, and
function using networkx. Tech. rep., Los Alamos National Lab.(LANL), Los Alamos,
NM (United States).

Jamroga, W. (2004). Strategic planning through model checking of atl formulae. In In-
ternational Conference on Artiï¬cial Intelligence and Soft Computing, pp. 879â€“884.
Springer.

Jha, S., Raman, V., Sadigh, D., & Seshia, S. A. (2018). Safe autonomy under perception un-
certainty using chance-constrained temporal logic. Journal of Automated Reasoning,
60 (1), 43â€“62.

Kallenberg, L. C. M. (1983). Linear programming and ï¬nite Markovian control problems.

Mathematisch Centrum, Amsterdam.

Kemeny, J., & Snell, J. L. (1963). Finite Markov chains. Springer-Verlag, New York.

Krass, D., & Vrieze, O. J. (2002). Achieving target state-action frequencies in multi-
chain average-reward Markov decision processes. Mathematics of Operations Research,
27 (3), 545â€“566.

Kwiatkowska, M., & Parker, D. (2013). Automated veriï¬cation and strategy synthesis for
In Automated Technology for Veriï¬cation and Analysis, pp.

probabilistic systems.
5â€“22. Springer.

Lakshmanan, K., & Bhatnagar, S. (2012). A novel Q-learning algorithm with function ap-
proximation for constrained Markov decision processes. In 50th Annual Allerton Con-
ference on Communication, Control, and Computing (Allerton), pp. 400â€“405. IEEE.

Lazar, A. (1983). Optimal ï¬‚ow control of a class of queueing networks in equilibrium. IEEE

Transactions on Automatic Control, 28 (11), 1001â€“1007.

Lindemann, L., & Dimarogonas, D. V. (2017). Robust motion planning employing signal
temporal logic. In 2017 American Control Conference (ACC), pp. 2950â€“2955. IEEE.

Manne, A. S. (1960). Linear programming and sequential decisions. Management Science,

6 (3), 259â€“267.

52

Steady-State Planning in Expected Reward Multichain MDPs

Nilsson, P., Hussien, O., Balkan, A., Chen, Y., Ames, A. D., Grizzle, J. W., Ozay, N.,
Peng, H., & Tabuada, P. (2015). Correct-by-construction adaptive cruise control: Two
approaches. IEEE Transactions on Control Systems Technology, 24 (4), 1294â€“1307.

Norris, J. R. (1997). Markov Chains. Cambridge Series in Statistical and Probabilistic

Mathematics. Cambridge University Press.

Petrik, M., & Zilberstein, S. (2009). A bilinear programming approach for multiagent

planning. Journal of Artiï¬cial Intelligence Research, 35, 235â€“274.

Pistore, M., Bettin, R., & Traverso, P. (2014). Symbolic techniques for planning with
extended goals in non-deterministic domains. In Sixth European Conference on Plan-
ning.

Privault, N. (2018). Understanding Markov Chains: Examples and Applications. Springer

Singapore, Singapore.

Puterman, M. (1994). Markov decision processes : discrete stochastic dynamic programming.

Wiley, New York.

Ross, K. W. (1989). Randomized and past-dependent policies for Markov decision processes

with multiple constraints. Operations Research, 37 (3), 474â€“477.

Royden, H., & Fitzpatrick (2010). Real Analysis (4th edition). Pearson.

Schwarting, W., Alonso-Mora, J., & Rus, D. (2018). Planning and decision-making for
autonomous vehicles. Annual Review of Control, Robotics, and Autonomous Systems,
1 (1), 187â€“210.

Skwirzynski, J. K. (1981). New concepts in multi-user communication, Vol. 43. Springer

Science & Business Media.

Song, L., Feng, Y., & Zhang, L. (2015). Planning for stochastic games with co-safe objec-
tives. In Twenty-Fourth International Joint Conference on Artiï¬cial Intelligence.

Tarjan, R. E. (1972). Depth-ï¬rst search and linear graph algorithms. SIAM Journal on

Computing, 1, 146â€“160.

Tian, Z. (2019). United states law and policy on space debris. In Space Security and Legal

Aspects of Active Debris Removal, pp. 155â€“167. Springer.

Trevizan, F., ThiÂ´ebaux, S., & Haslum, P. (2017). Occupation measure heuristics for proba-
bilistic planning. In Twenty-Seventh International Conference on Automated Planning
and Scheduling.

Trevizan, F., ThiÂ´ebaux, S., Santana, P., & Williams, B. (2016). Heuristic search in dual
space for constrained stochastic shortest path problems. In Twenty-Sixth International
Conference on Automated Planning and Scheduling.

Trevizan, F., ThiÂ´ebaux, S., Santana, P., & Williams, B. (2017). I-dual: solving constrained
ssps via heuristic search in the dual space. In Proceedings of the 26th International
Joint Conference on Artiï¬cial Intelligence, pp. 4954â€“4958.

Velasquez, A. (2019). Steady-state policy synthesis for veriï¬able control. In Proceedings
of the 28th International Joint Conference on Artiï¬cial Intelligence, pp. 5653â€“5661.
AAAI Press.

53

Atia et al.

Wongpiromsarn, T., Topcu, U., Ozay, N., Xu, H., & Murray, R. M. (2011). TuLiP: a
software toolbox for receding horizon temporal logic planning. In Proceedings of the
14th International Conference on Hybrid Systems: Computation and Control, pp. 313â€“
314. ACM.

Wu, J., & Durfee, E. H. (2010). Resource-driven mission-phasing techniques for constrained
agents in stochastic environments. Journal of Artiï¬cial Intelligence Research (JAIR),
38, 415â€“473.

Zhou, Y., Maity, D., & Baras, J. S. (2016). Timed automata approach for motion planning
using metric interval temporal logic. In 2016 European Control Conference (ECC),
pp. 690â€“695. IEEE.

54

