Decomposable Non-Smooth Convex Optimization with
Nearly-Linear Gradient Oracle Complexity

Sally Dong
University of Washington, Seattle
sallyqd@uw.edu

Haotian Jiang
University of Washington, Seattle
jhtdavid@uw.edu

Yin Tat Lee†
University of Washington, Seattle
yintat@uw.edu

Swati Padmanabhan
University of Washington, Seattle
pswati@uw.edu

Guanghao Ye
Massachusetts Institute of Technology
ghye@mit.edu

August 9, 2022

Abstract

Many fundamental problems in machine learning can be formulated by the convex program

min
θ∈Rd

n

X
i=1

fi(θ),

where each fi is a convex, Lipschitz function supported on a subset of di coordinates of θ. One common
approach to this problem, exempliﬁed by stochastic gradient descent, involves sampling one fi term at
every iteration to make progress. This approach crucially relies on a notion of uniformity across the fi’s,
formally captured by their condition number. In this work, we give an algorithm that minimizes the above
n
i=1 di log(1/ǫ)) gradient computations, with no assumptions on
convex formulation to ǫ-accuracy in
the condition number. The previous best algorithm independent of the condition number is the standard
cutting plane method, which requires O(nd log(1/ǫ)) gradient computations. As a corollary, we improve
upon the evaluation oracle complexity for decomposable submodular minimization by [AKMSV21]. Our
main technical contribution is an adaptive procedure to select an fi term at every iteration via a novel
combination of cutting-plane and interior-point methods.

O(P
e

2
2
0
2

g
u
A
7

]

C
O
.
h
t
a
m

[

1
v
1
1
8
3
0
.
8
0
2
2
:
v
i
X
r
a

∗Author names are listed in alphabetical order.
†Supported by NSF awards CCF-1749609, DMS-1839116, DMS-2023166, CCF-2105772, a Microsoft Research Faculty Fel-

lowship, a Sloan Research Fellowship, and a Packard Fellowship

1

 
 
 
 
 
 
1 Introduction

Many fundamental problems in machine learning are abstractly captured by the convex optimization formu-
lation

minimizeθ

Rd

∈

n
i=1 fi(θ),

(1.1)

where each fi is a convex, Lipschitz function. For example, in empirical risk minimization, each fi measures
the loss incurred by the i-th data point from the training set. In generalized linear models, each fi represents
a link function applied to a linear predictor evaluated at the i-th data point.

P

The ubiquity of (1.1) in the setting with smooth fi’s has spurred the development of well-known variants
of stochastic gradient methods [RM51; BC03; Zha04; Bot12] such as [RSB12; SZ13b; JZ13; MZJ13; DBL14;
Mai15; AY16; HL16; SLB17]; almost universally, these algorithms leverage the “sum structure” of (1.1) by
sampling, in each iteration, one fi with which to make progress. These theoretical developments have in
turn powered tremendous empirical success in machine learning through widely used software packages such
as libSVM [CL11].

In many practical applications, (1.1) appears with non-smooth fi’s, as well as the additional structure
that each fi depends only on a subset of the problem parameters θ. One notable example is decomposable
submodular function minimization1 (SFM), which has proven to be expressive in diverse contexts such
as determinantal point processes [KT10], MAP inference in computer vision [KLT09; VKR09; FJPZ13],
hypergraph cuts [VBK20], and covering functions [SK10]. Another application is found in generalized linear
models when the data is high dimensional and sparse. In this setting, fi depends on a restricted subset of the
parameters θ that correspond to the features of the data point with non-zero value. Last but not least, the
case with each fi depending on a small subset of the parameters is also called sparse separable optimization
and has applications in sparse SVM and matrix completion [RRWN11].

In this work, we initiate a systematic study of algorithms for (1.1) without the smoothness assumption2.
Motivated by the aforementioned applications, we introduce the additional structure that each fi depends
on a subset of the coordinates of θ. As is standard in the black-box model for studying ﬁrst-order convex
optimization methods, we allow sub-gradient oracle access to each fi.
Problem 1.1. Let f1, f2, . . . , fn : Rd
each fi depends on di coordinates of θ, and is accessible via a (sub-)gradient oracle. Deﬁne m
be the “total eﬀective dimension” of the problem. Let θ⋆ def
and let θ(0) be an initial point such that

R be convex, L-Lipschitz, and possibly non-smooth functions, where
n
i=1 di to
n
i=1 fi(θ) be a minimizer of (1.1),

P
Rd satisfying

= arg minθ

θ(0)

def
=

θ⋆

7→

Rd

∈

k2 ≤

R. We want to compute a vector θ
P

∈

−

k
n

fi(θ)

≤

ǫLR +

i=1
X

fi(θ⋆).

n

i=1
X

(1.2)

Prior works. We focus on the weakly-polynomial regime and therefore restrict ourselves to algorithms with
polylog(1/ǫ) gradient oracle complexities. Table 1 summarizes the performance of all well-known algorithms
applied to Problem 1.1. Note that the variants of gradient descent each require bounded condition number.
The results of [Nes83; All17] and cutting plane methods are all complemented by matching lower bounds
[WS16; Nes04].

Even with smooth fi’s, ﬁrst-order methods perform poorly when the condition number is large, or when
there is a long chain of variable dependencies. These instances commonly arise in applications; an example
from signal processing is

minimizex

(x1 −

(

1)2 +

(xi −

xi+1)2 + x2
n

,

)

(1.3)

1

n

−

i=2
X

whose variables form an O(n)-length chain of dependencies, and has condition number κ = Θ(n2) and
¯κ = Θ(n3). Gradient descent algorithms such as [Nes83] and [All17] therefore require Ω(n2) gradient queries,
despite the problem’s total eﬀective dimension being only O(n).

1In decomposable submodular minimization, each fi corresponds to the Lovász extension of the individual submodular

function and is therefore generally non-smooth.

2A function f is said to be β-smooth if f (y) ≤ f (x) + h∇f (x), y − xi + β/2ky − xk2

2 for all x, y and α-strongly-convex if

f (y) ≥ f (x) + h∇f (x), y − xi + α/2ky − xk2

2 for all x, y. The condition number of f is deﬁned to be κ = β/α.

2

Authors

Algorithm Type

Gradient Queries

Non-smooth OK?

[Cau+47]
[Nes83]
[RSB12; SZ13b; JZ13]
[SZ13a; LMH15; FGKS15; ZL15; AB15]
[All17]
[KTE88; NN89; Vai89; BV02; LSW15; JLSW20]
[LV21; DLY21]

Gradient Descent (GD)
Accelerated (Acc.) GD
Stochastic (Stoch.) Variance-Reduced GD
Acc. Stoch. Variance-Reduced GD
Acc. Stoch. Variance-Reduced GD
Cutting-Plane Method (CPM)
Robust Interior-Point Method (IPM)

O(nκ log(1/ǫ))
O(n√κ log(1/ǫ))
O((n + κ) log(1/ǫ))
O((n + √nκ) log(κ) log(1/ǫ))
O((n + √nκ) log(1/ǫ))
O(nd log(1/ǫ))

O(

n
i=1 d3.5
i

log(1/ǫ))

X
X

Table 1: Gradient oracle complexities for solving (1.1) to ǫ-additive accuracy. κ denotes the condition number of
Pi fi, and κ is a variant of the condition number deﬁned to be the sum of smoothness of the fi’s divided by the
strong convexity of Pi fi.

P

On the other hand, cutting-plane methods (CPM) and robust interior-point methods (IPM) both trade

oﬀ the dependency on condition number for worse dependencies on the problem dimension.
These signiﬁcant gaps in the existing body of work motivate the following question:

Can we solve Problem 1.1 using a nearly-linear (in total eﬀective dimension) number of
subgradient oracle queries?

In this paper, we give an aﬃrmative answer to this question.

1.1 Our results

We present an algorithm to solve Problem 1.1 with gradient oracle complexity nearly-linear in the total
eﬀective dimension.

Theorem 1.2 (Main Result (Informal); see Theorem 4.11 for formal statement). We give an algorithm that

provably solves Problem 1.1 using O(m log(m/ǫ)) subgradient oracle queries, where m

def
=

n
i=1 di.

Intuitively, the number of gradient queries for each fi should be thought of as

O(di) in our algorithm,
which nearly matches that of the standard cutting-plane method for minimizing the individual function fi.
The nearly-linear dependence on m overall is obtained by leveraging the additional structure on the fi’s and
stands in stark contrast to the O(nd) query complexity of CPM, which is signiﬁcantly worse in the case
d. Furthermore, we improve over the current best gradient descent algorithms in the case
where each di ≪
where the fi’s have a large condition number.

e

P

Based on the query complexity of the standard cutting-plane method, we have the following lower bound

matching our algorithm’s query complexity up to a log m-factor:

Theorem 1.3. There exist functions f1, . . . , fn : Rd
are required to solve Problem 1.1.

7→

R for which a total of Ω(m log(1/ǫ)) gradient queries

An immediate application of Theorem 1.2 is to decomposable submodular function minimization:

n
i=1 Fi(S

Theorem 1.4 (Decomposable SFM). Let V =

, and let F : 2V
V with
approximate minimizer of F in O(nk2 log(nk/ǫ)) evaluation oracle calls.
P

{
R a submodular function on Vi ⊆

Vi), each Fi : 2Vi

1, 2, . . . , n

7→

∩

}

|

7→
Vi| ≤

[
1, 1] be given by F (S) =
k. We can ﬁnd an ǫ-additive

−

Theorem 1.4 signiﬁcantly improves over the evaluation oracle complexity of

O(nk6 log(1/ǫ)) given in
[AKMSV21] when the dimension k of each function Fi is large. For non-decomposable SFM, i.e. n = 1
= k, the current best weakly-polynomial time SFM algorithm3 ﬁnds an ǫ-approximate minimizer
and
in time O(k2 log(k/ǫ)) [LSW15]. Therefore, our result in Theorem 1.4 can be viewed as a generalization
of the evaluation oracle complexity for non-decomposable SFM in [LSW15], and the dependence on k in
Theorem 1.4 might be the best possible. We defer the details of decomposable SFM to Appendix A.

V1|

e

|

3Here, we focus on the weakly-polynomial regime, where the runtime dependence on ǫ is log(1/ǫ).

3

1.1.1 Limitations

Some limitations of our algorithm are as follows: When each fi depends on the entire d-dimensional vector
d, our gradient complexity simply matches that of
θ, as opposed to a subset of the coordinates of size di ≪
d. When the fi’s are
CPM. We would like to highlight, though, that our focus is in fact the regime di ≪
strongly-convex and smooth, our gradient complexity improves over Table 1 only when κ is large compared
to di. Finally, note that we consider only the gradient oracle complexity in our work; our algorithm’s
implementation requires sampling a Hessian matrix and a gradient vector at every iteration, which incur an
additional polynomial factor in the overall running time.

1.2 Technical challenges in prior works

We now describe the key technical challenges that barred existing algorithms from solving Problem 1.1 in
the desired nearly-linear gradient complexity.

Gradient descent and variants. As mentioned in Section 1, the family of gradient descent algorithms
presented in Table 1 are not applicable to Problem 1.1 without the smoothness assumption. When the ob-
jective in Problem 1.1 is smooth but has a large condition number, even the optimal deterministic algorithm,
Accelerated Gradient Descent (AGD) [Nes83] can perform poorly. For example, when applied to (1.3), AGD
updates only one coordinate in each step (thereby requiring n steps), with each step performing n gradient
queries (one on each term in the problem objective), yielding a total gradient complexity of Ω(n2) [Nes83].
For a similar reason, the fastest randomized algorithm, Katyusha [All17] also incurs a gradient complexity
of Ω(n2) [WS16].

Cutting-plane methods (CPM). Given a convex function f with its set
in the kth iteration, and iteratively shrinking
f by maintaining a convex search set
⊇ S
the subgradients of f . Speciﬁcally, this is achieved by noting that for any x(k) chosen from
gradient oracle indicates
∇
f (x(k)), y
x(k)
i ≤
(k), and diﬀerent choices of x(k) and

of minimizers, CPM minimizes
(k) using
E
(k), if the
E
(k) def=
S ⊆ H
(k+1)

), then the convexity of f guarantees
(k). The algorithm continues by choosing

(k) yield diﬀerent rates of shrinkage of

= 0, (i.e. x(k) /
(k)

(k) until a point in

f (x(k))
0

, and hence

∈ S
∩ E

y :
(k)

−

(k)

S

E

E

(cid:9)

S ⊆ H
E

⊇
S

E

h∇
(cid:8)
∩ H
E
is found.

Solving Problem 1.1 via the current fastest CPM [JLSW20] takes

O(d) iterations, each invoking the
gradient oracle on every fi to compute
O(nd) gradient queries
overall, which can be quadratic in n when d = Θ(n) even if each fi depends on only di = O(1) coordinates.
P
Similar to gradient descent and its variants, the poor performance of CPM on Problem 1.1 may therefore be
attributed to their inability to query the right fi required to make progress.

fi(x(k)). This results in

f (x(k)) =

n
i=1 ∇

∇

e

e

IPM solves the convex program minu

by solving a sequence
Interior-point methods (IPM).
of unconstrained problems minu Ψt(u) def=
is
a self-concordant barrier function that enforces feasibility by becoming unbounded as it approaches the
boundary of the feasible set
is known, and alternates between increasing t and updating to an approximate minimizer x⋆
via Newton’s method. For a suﬃciently large t, the minimizer x⋆
c, u
problem minu
i
barrier function used.

0 of ψ
S
t of the new Ψt
t also approximately optimizes the original
with sub-optimality gap O(ν/t), where ν is the self-concordance parameter of the

i
parametrized by increasing t, where ψ
S

. The algorithm starts at t = 0, for which an approximate minimizer x⋆

(u)
}

+ ψ
S

c, u

∈Sh

∈Sh

t
{

· h

S

i

c, u

∈

i
∀

To apply IPM to Problem 1.1, we may ﬁrst transform (1.1) to min(u,z)

is the feasible set. Using the universal barrier ψi for each
P
O(

i zi, where
(u, z) :
K
(ui, zi)
Ki [NN94a], the number
[n]
∈ Ki,
}
n
i=1 di), each requiring the computation of the Hessian and gradient of ψi for
of iterations of IPM is
O(n1.5) sub-gradient queries to fi’s even when all di = O(1). Even when
all i
leveraging the recent framework of robust IPM for linear programs [LV21], the computation of each Hessian
(by sampling the corresponding
),
far from the complexity we seek.

e
Ki [JLLV21]) yields a total sub-gradient oracle complexity of

[n], leading to a total of

n
i=1 d3.5
i

pP

O(

∈K

=

∈

e

{

e

4

P

6
1.3 Our algorithmic framework

We now give an overview of the techniques developed in this work to overcome the above barriers. First, we
transform (1.1) into a convex program over structured convex sets:

c, x
minimize
,
h
i
subject to xi ∈ Ki ⊆
Ax = b.

Rdi+1

[n]

i
∀

∈

(1.4)

where x is the concatenation of the vectors x1, . . . , xn, and notably the convex sets
this transformation we do not have explicit, closed-form expressions for each
oracle for fi can be transformed equivalently to a separation oracle

Ki are all disjoint. Under
Ki; however, the subgradient
× Kn.
K1 × K2 ×

def=

. . .

K

Ki. We deﬁne

Main idea: combining CPM and IPM. Recall that CPM maintains a convex set which initially
contains the feasible region and gradually shrinks around the minimizer, while IPM maintains a point inside
the feasible region that moves toward the minimizer. Our novel idea is to combine both methods and maintain
Kin,i ⊆ Ki ⊆ Kout,i.
an inner convex set
. When Inequality (3.4) and Inequality (3.3) are satisﬁed for all
Kin and
We deﬁne
[n], we make IPM-style updates without needing to make any oracle calls. When Inequality (3.3) is
i
violated for some i
out,i deﬁned as the centroid of
Kout,i
(c.f. Proposition 3.2). Based on the oracle’s response, we iteratively either grow
Kin)
Kin,i (and, thus,
around the optimum
Kout) inward, until ultimately they approximate
outward or shrink
K
point.

Kin,i as well as an outer convex set
Kout analogously to
[n], we query the separation oracle at the point x⋆

∈
Kout,i (and, thus,

Kout,i for each i

[n], such that

K

∈

∈

[n], we
First beneﬁt: large change in volume.
∈
query the separation oracle to see if x⋆
Kin,i, yielding
∈ Ki, the fact that it is the centroid of
in a large volume increase for
Kout,i
out,i. Thus, our
results in a large volume decrease for
algorithm witnesses a large change in volume of one of
Kout,i, regardless of the answer from the
oracle. Just like in standard CPM, this rapid change in volume is crucial to achieving the algorithm’s oracle
complexity.

If the point x⋆
out,i violates Inequality (3.3) for some i
out,i ∈ Ki or not. If x⋆
out,i ∈ Ki, then it is used to expand
Kin,i. On the other hand, if x⋆
out,i /
Kout,i when it is intersected with a halfspace through x⋆

Kin,i and

Second beneﬁt: making a smart choice about querying fi. Since the algorithm maintains both
Kout,i diﬀer signiﬁcantly (Inequality (3.3)
Kin,i and
, by checking if
an inner and outer set approximating
Ki is poorly approximated, and if so, improve the
essentially performs this function), we can determine if
inner and outer approximations of the true feasible set. Choosing the right
Ki translates to choosing the
right fi to make progress with at an iteration; thus, we address the central weakness of the gradient descent
variants in solving (1.1).

K

2 Notation and preliminaries

We lay out the notation used in our paper as well as the deﬁnitions and prior known results that we rely
on. We use lowercase boldface letters to denote (column) vectors and uppercase boldface letters to denote
matrices. We use xi to denote the ith block of coordinates in the vector x (the ordering of these blocks is
not important in our setup). We use

to denote the Loewner ordering of matrices.

and

i

We use

x, y
h

to mean the Euclidean inner product x⊤y. A subscript x in the inner product notation
means it is induced by the Hessian of some function (which is clear from context) at x; for example,
2
iiψ(x)v with ψ inferred from context. We deﬁne the local norm of v at x analogously:
u, v
h
∇
2ψ(x)
v

x = u⊤
i
v,
x =
k
h
k
We use ψ to represent barrier functions and Φ to represent potential functions, with appropriate subscripts
p

. We also deﬁne the norm

n
i=1 k

def=

xi.

∇

x,1

v

v

v

k

k

k

i

·

P

and superscripts to qualify them as needed.

(cid:23)

(cid:22)

5

2.1 Facts from convex analysis

In this section, we present some deﬁnitions and properties from convex analysis that are useful in our paper.
These results are standard and may be found in, for example, [Roc70; BBV04].

Deﬁnition 2.1. Let f : Rn

R. Then the function f ∗ : Rn

R deﬁned as

→

→

f ∗(y) = sup

dom(f )

x

∈

[

x, y
h

i −

f (x)]

is called the Fenchel conjugate of the function f. An immediate consequence of the deﬁnition (and by applying
the appropriate convexity-preserving property) is that f ∗ is convex, regardless of the convexity of f. We use
the superscript

on functions to denote their conjugates.

∗

Lemma 2.2 (Biconjugacy). For a closed, convex function f, we have f = f ∗∗.

Lemma 2.3 ([Roc70]). For a closed, convex diﬀerentiable function f, we have y =

f (x)

∇

⇐⇒

x =

f ∗(y).

∇
f (x)) =

2f ∗(

∇

∇

Lemma 2.4 ([Roc70]). For a strictly convex, twice-diﬀerentiable function f , we have
(
∇

2f (x))−

1.

Deﬁnition 2.5 (Polar of a Set [RW09]). Given a set

Rn, its polar is deﬁned as

S ⊆

def=

y
{

∈

◦

S

Rn :

y, x
h

i ≤

1,

x

∀

.

∈ S}

Lemma 2.6 ([Roc70]). Let

Rn be a closed, compact, convex set, and let y be a point. Then (conv

, where

◦

S

∩ H

H

S ⊆
is the halfspace deﬁned by

=

z
{

∈

H

Rn :

z, y
h

i ≤

1

.
}

, y

)◦

}

{S

⊆

2.2 Background on interior-point methods

Our work draws heavily upon geometric properties of self-concordant functions, which underpin the rich
theory of interior-point methods. We list below the formal results needed for our analysis, and refer the
reader to [NN94a; Ren01] for a detailed exposition of this function class. We begin with the deﬁnitions of
self-concordant functions and self-concordant barriers:

Deﬁnition 2.7 (Self-concordance [NN94a]). A function F : Q
set Q if for any x

Q and any h,

∈

R is a self-concordant function on a convex

7→

D3F (x)[h, h, h]

|

| ≤

2(D2F (x)[h, h])3/2,

where DkF (x)[h1, . . . , hk] denotes the k-th derivative of F at x along the directions h1, . . . , hk. We say F
is a ν-self-concordant barrier if F further satisﬁes

ν for any x

F (x)

2F (x))−

F (x)⊤(

Q.

1

∇

∇

∇

≤

∈

Theorem 2.8 (Theorem 2.3.3 from [Ren01]). If f is a self-concordant barrier, then for all x and y
we have

ν, where ν is the self-concordance of f .

f (x), y

x

h∇

−

i ≤

Theorem 2.9 (Theorem 2.3.4 from [Ren01]). If f is a ν-self-concordant barrier such that x, y
satisfy

x(x, 4ν + 1).

0, then y

f (x), y

x

h∇

∈ B
We now state the following result from self-concordance calculus.

i ≥

−

dom(f ),

dom(f )

∈

∈

Theorem 2.10 (Theorem 3.3.1 of [Ren01]). If f is a (strongly nondegenerate) self-concordant function,
then so is its Fenchel conjugate f ∗.

The following result gives a bound on the quadratic approximation of a function, with the distance
between two points measured in the local norm. The convergence of Newton’s method can be essentially
explained by this result.

6

Theorem 2.11 (Theorem 2.2.2 of [Ren01]). If f is a self-concordant function, x
then

dom(f ), and y

x(x, 1),

∈ B

f (y)

≤

f (x) +

f (x), y

h∇

x
i

−

+

1
2 k

y

x
k

−

2
x +

3(1

∈
3
x
x
k

x
k
−

y

−
y

k
− k

,

x)

where

2
x

k

y

x)
.
i
Finally, we need the following deﬁnitions of entropic barrier and universal barrier.

x
k

y
h

(y

x,

∇

−

−

−

·

2f (x)

def
=

Deﬁnition 2.12 ([BE15; Che21]). Given a convex body
dx
. Then the Fenchel conjugate f ∗ : int(
f (θ) = log
i
termed the entropic barrier. The entropic barrier is n-self-concordant.

x, θ
exp
h

K ⊆

∈K

x

K

→

Rn and some ﬁxed θ
)

Rn, deﬁne the function
R is a self-concordant barrier

∈

(cid:2)R

(cid:3)

Deﬁnition 2.13 ([NN94b; LY21]). Given a convex body
ψ : int(

R by

)

K

→

ψ(x) = log vol((

x)◦)

K −

Rn, the universal barrier of

is deﬁned as

K

K ⊆

x)◦ =
where (
barrier is n-self-concordant.

K −

∈

y

{

Rn : y⊤(z

x)

1,

z

∀

≤

−

∈ K}

is the polar set of

K

with respect to x. The universal

2.3 Facts from convex geometry

Since our analysis is contingent on the change in the volume of convex bodies when points are added to
them or when they are intersected with halfspaces, we invoke the classical result by Grünbaum several
times. We therefore state its relevant variants next: Theorem 2.14 applies to log-concave distributions,
and Corollary 2.16 is its speciﬁc case, since the indicator function of a convex set is a log-concave function
[BBV04].

Theorem 2.14 ([Grü60; BKLLS20]). Let f be a log-concave distribution on Rd with centroid cf . Let
u

=
H
t+, where
q
2 is the distance of the centroid to the halfspace scaled by the standard deviation along the

be a halfspace deﬁned by a normal vector v

Rd. Then,

f (z)dz

1
e −

≥

∈

H
R

Rd :
u, v
∈
h
cf ,v
q
t =
−h
(cid:8)
√Ey∼f h
v,y
−
normal vector v and t+ def

i ≥
i
cf i

(cid:9)

= max

0, t

.

}

{

Remark 2.15. A crucial special case of Theorem 2.14 is that cutting a convex set through its centroid yields
two parts, the smaller of which has volume at least 1/e times the original volume and the larger of which is
at most 1

1/e times the original total volume.

−

Corollary 2.16 ([Grü60]). Let
point x satisfying

x

µ

be a convex set with centroid µ and covariance matrix Σ. Then, for any
η).

K
η and a halfspace

, we have vol(

such that x

(1/e

vol(

)

)

H

∈ H

K ∩H

≥

K

·

−

−
Finally, we need the following facts.

≤

k

kΣ−1

Fact 2.17 (Volumes of standard objects). The volume of a q-dimensional Euclidean ball is given by vol(

πq/2
Γ(1+q/2)

¯Rq, and the volume of a q-dimensional cone = 1

q+1 ·

volume of base

height.

·

3 Our algorithm

Bq(0, ¯R)) =

We begin by reducing Problem 1.1 to the following slightly stronger formulation (see Theorem 4.11 for the
detailed reduction):

c, x
minimize
,
h
i
subject to xi ∈ Ki ⊆
Ax = b.

Rdi+1

[n]

i
∀

∈

(3.1)

where x is a concatenation of vectors xi’s, and the
Ki’s are disjoint convex sets. This formulation decouples
the overlapping support of the original fi’s by introducing additional variables tied together through the
linear system Ax = b. Each

Ki is constructed by applying a standard epigraph trick to the function fi.

7

Note that we do not have a closed-form expression for

Ki. Instead, the subgradient oracle for fi translates
Ki: on a point zi queried by the oracle, the oracle either asserts zi ∈ Ki, or returns

to a separation oracle for
a separating hyperplane that separates zi from

At the start of our algorithm, we have the following guarantee:

Ki.

Lemma 3.1. At the start of our algorithm, we are guaranteed the existence of the following.

def
=

Kin,1 × Kin,2 × · · · × Kin,n and

def
=

Kout

Kout,1 × Kout,2 × · · · × Kout,n such

that

1. Explicit convex sets
def
=

Kin
K1 × · · · × Kn ⊆ Kout,
Kin ⊆ K
2. An initial xinitial ∈ Kin such that Axinitial = b.
We show how to construct such a set

Section 5.2.

Kin in Section 5.1 and how to ﬁnd such a

Kout and xinitial in

3.1 Details of our algorithm

In this section, we explain our main algorithm (Algorithm 1).

Kout satisfying
Kin ⊆ K ⊆ Kout, an initial point
The inputs to Algorithm 1 are:
initial sets
x
Ki, the objective vector c, and scalar parameters
∈ Kin satisfying Ax = b, a separation oracle
m, n, R, r, and ǫ. All the parameters are set in the proof of Theorem 4.10.
Throughout the algorithm, we maintain a central path parameter t for IPM-inspired updates, the current
[n]. To run IPM-style

Kin and
Oi for each

solution x, and convex sets
updates, we choose the entropic barrier on

Kin,i and

Kout,i satisfying

Kout and the universal barrier on
Kout, the current t, and the entropic barrier ψout deﬁned on

Kin,i ⊆ Ki ⊆ Kout,i for each i
Kin.

∈

Given the current set

u : Au = b

{

, we deﬁne the point
}

Kout

b

def=

Kout ∩

(3.2)

x⋆

out

def= arg min
x
K
b

∈

out {

t

c, x
i
h

+ ψout(x)
}

.

over the set

explicitly and therefore must use its known proxies,

Per the IPM paradigm, for the current value of t, this point serves as a target to “chase” when optimizing
c, x
, we
h
i
}
do not know
Kout because
Kin or
Kout ⊇ K
out, we move the current solution x towards it by taking a Newton
step, provided certain conditions of feasibility and minimum progress are satisﬁed. If either one of these
conditions is violated, we update either

Kout. Although our overall goal in Problem 3.1 is to optimize over
b

ensures we do not miss a potential optimal point.

Having computed the current target x⋆

Kout; we choose

u : Au = b

K ∩ {

K

Kin,

Kout, or the parameter t.
out, we require two conditions to hold: x⋆

out ∈ Kin and

c, x
h

i ≥

In order to move x towards x⋆

Updating x.
c, x⋆
h

+ O(1/t).

outi
The ﬁrst condition implies x⋆

Newton step. To formally check this condition, we check if the following inequality is satisﬁed for all i
and for a ﬁxed constant η:

, which would in turn ensure feasibility of the resulting x after a
[n]

out ∈ K

∈

ψin,i(xi), x⋆

out,i −

+ η

xii

x⋆
out,i −

xik

· k

xi ≤

4di.

(3.3)

h∇

The intuition is that since any point within the domain of a self-concordant barrier satisﬁes the inequalities
in Theorem 2.8 and Theorem 2.9, violating Inequality (3.3) implies that x⋆
Kin,i, and as a
result, x⋆
The second condition we impose, one of “suﬃcient suboptimality”, ensures signiﬁcant progress in the

out is not a good candidate to move x towards.

out,i is far from

objective value can be made when updating x. Formally, we check if

If the inequality holds, then there is still “room for progress” to lower the value of
the inequality is violated, we update t instead.

c, x
i
h

by updating x; if

c⊤x⋆

out +

4m
t ≤

c⊤x.

(3.4)

8

Algorithm 1 Minimizing Decomposable Convex Function

1: ⊲ Solving Problem 3.1
2: Input. ǫ, A, b, c, R, r, m , n, x,
3: Initialization. t = m log m
4: while true do
5:
6:

Kin,
k2R and tend = 8m
k
+ 4m
t then

c, x⋆
outi
tend then

c, x
h
if t

√n

if

k

c

c

ǫ

i ≤ h
≥
return arg minx:x

7:

8:
9:

10:

11:
12:

13:
14:

15:

16:
17:

18:

19:
20:

end if
t
t
·
Update x⋆

←

1 + η
4m

(cid:2)

(cid:3)

end if
for all i
if

out and jump to Line 4

k

+ η

[n] do
∈
ψin,i(xi), x⋆
h∇
if x⋆

out,i −
out,i ∈ Ki then
Kin,i = conv(
Kout,i =
end if
Update x⋆

xii
Kin,i, x⋆
Kout,i ∩ Hi, where
out and jump to Line 4

out,i)

else

end if

Kout, and

k2R , η = 1

Oi for each i
∈
100 , and x⋆
out (via Equation (3.2))

[n].

⊲ Either update t or end the algorithm

in,Ax=b

∈K

t
{

c, x
i
h

+

n
i=1 ψin,i(xi)
}

.

⊲ End the algorithm

P

⊲ Update t
out computed as as per Equation (3.2)

⊲ x⋆

x⋆
out,i −

xik

xi ≥

4di then

⊲ Query

⊲ Update

Oi
Kin,i

Hi =

Oi(x⋆

out,i)

⊲ Update

Kout,i

⊲ x⋆

out computed as per Equation (3.2)

21:
22:

end for
Set δx
x
23:
24: end while

←

def= η
2 ·
x + δx

k

x

x⋆

x⋆

out−
x
k

out−

x,1

,where

u

k

k

x,1

def=

n
i=1 k

u

k

xi.

P

⊲ Move x towards x⋆

out

Given the two conditions hold, we move x towards x⋆

out in Line 23. The update step is normalized by the
distance between x and x⋆
(since by the deﬁnition of
self-concordance, the unit radius Dikin ball lies inside the domain of the self-concordance barrier), and also
helps bound certain ﬁrst-order error terms (Inequality (4.14) in Section 4.3).

out measured in the local norm, which enforces x

∈ K

The rest of this section details the procedure for when either of these conditions is violated.

out,i /

out,i into

Updating the inner and outer convex sets. Suppose Inequality (3.3) is violated for some i
x⋆
for i, we can either update

∈ Kin,i, which in turn means x⋆
Kin,i, or update
To decide which option to take, we query

out might not be in the feasible set
Kout,i and compute a new x⋆
Oi at the point x⋆
Kin,i, x⋆
Kin,i = conv(
∈ Ki, the oracle
out,i /

out,i ∈ Ki,
out,i) to be the convex hull of the
Oi will return a halfspace
Hi
Kout,i ∩ Hi (Line 17). After processing this update of the sets,
out and returns to the main loop since updating the sets does not necessarily

out,i: if the oracle indicates that x⋆

then we incorporate x⋆
Kin,i and x⋆
current
satisfying
the algorithm recomputes x⋆
imply that the new x⋆

Hi ⊇ Ki. Then we redeﬁne
out satisﬁes x⋆

[n]. Then
. To reestablish Inequality (3.3)

Kin,i by redeﬁning
Kout,i =
out ∈ Kin.

This update rule for the sets is exactly where our novelty lies: we do not arbitrarily update sets, rather,
we update one only after checking the very speciﬁc condition x⋆
∈ Kin,i. Since the separation oracle is
called only in this part of the algorithm, performing this check enables us to dramatically reduce the number
of calls we make to the separation oracle, thereby improving our oracle complexity.

out,i (Line 15). If, on the other hand, x⋆

out,i by Equation (3.2).

out,i /

K

∈

Further, this update rule shows that even when we cannot update the current x, we make progress by
Kin and
. To formally quantify the change in volume due to the above

using all the information from the oracles. Over the course of the algorithm, we gradually expand
shrink
operations, we consider the following alternative view of x⋆

Kout, until they well-approximate

K

out.

Proposition 3.2 (Section 3 in [BE15]; Section 3 of [Kla06]). Let θ

Rn, and let pθ be deﬁned as pθ(x)

def
=

∈

9

θ, x
exp(
h

i −

f (θ)), where f (θ)

def
= log

)du
i

θ, u
exp(
h
K
(cid:3)
(cid:2)R
pθ [x] = arg min
∼
int(
K

∈

x

Ex

. Then,

f ∗(x)

) {

θ, x

.

i}

− h

By this proposition, x⋆

out deﬁned in Equation (3.2) satisﬁes

x⋆

out

def
= E
x

expn−

t
h

∼

c,x

i−

loghR

Kout
b

exp(

c,u

t
h

−

i

)duio

[x],

(3.5)

out is the centroid of some exponential distribution over

that is, x⋆
Kout through x⋆
plane cutting
the query result in a large change in volume in either
bounded number of iterations.
b

out will yield a large decrease in volume of

Kout. As a result, if x⋆
b
Kout, allowing us to approximate

∈ Ki, the hyper-
out,i /
Kout, per Remark 2.15. Therefore,
with a
b

Kin or

K

Updating t.
parameter t. This could mean one of two things:

If Inequality (3.4) is violated, then the current x is “as optimal as one can get” for the current

The ﬁrst possibility is that we have already reached an approximate optimum, which we verify by checking
O(1/ǫ) in Line 6: If true, this indicates that we have attained our desired suboptimality, and

whether t
the algorithm terminates by returning

≥

xret = arg

x:x

min
in,Ax=b (

∈K

t

c, x
i

· h

+

n

i=1
X

ψin,i(xi)

)

.

The point xret is feasible because it is in
it is an approximate optimum for the original problem.

Kin by deﬁnition, and the suboptimality of O(1/tend) = O(ǫ) ensures
The second possibility is that we need to increase t to set the next “target suboptimality”. The value of
t is increased by a scaling factor of 1 + O(1/m) in Line 9. This scaling factor ensures, like in the standard
IPM framework, that the next optimum is not too far from the current one. Following the update to t, we
c, x⋆
recompute x⋆
+ O(1/t) is not guaranteed with the new t and
h
x⋆

out, the algorithm jumps back to the start of the main loop.

out by Equation (3.2). Since

c, x
i
h

outi

>

4 Our analysis

To analyze Algorithm 1, we deﬁne the following potential function that captures the changes in
t, and x in each iteration:

Kin,i,

Kout,i,

Φ def= t

c, x
i
h

+ log

exp(

−

out

(cid:20)Z
entropic terms

K
b

t

c, u
h

)du
i

+

(cid:21)

ψin,i(xi)
,

[n]
Xi
∈
universal terms

(4.1)

t

out

exp(

Kout (see Section 4.1) and ψin,i is the
where log
−
K
Ki. In the subsequent sections, we study the changes in each of these potential functions
universal barrier on
b
hR
b
along with obtaining bounds on the initial and ﬁnal potentials and combine them to bound the algorithm’s
separation oracle complexity.

|
is related to the entropic barrier on

c, u
h

{z

i

}

{z

}

|
)du
i

4.1 Potential change for the entropic terms

In this section, we study the changes in the entropic terms of Equation (4.1) upon updating the outer
convex set
Kout as well as t. These two changes are lumped together in this section because both updates
aﬀect the term log
Kout aﬀects it via
b
Grünbaum’s Theorem; the update in t aﬀects it via the fact that, by duality with respect to the entropic
b
is also self-concordant. We detail these two potential
barrier (Deﬁnition 2.12), log
changes below.

, albeit in diﬀerent ways: the update in

i
x, θ
exp(
h

) dx
c, x
i

)dx
i

K
b
hR

exp(

· h

−

K
b

out

out

∈

t

x
hR

i

10

Lemma 4.1 (Potential analysis for outer set). Let

t

x : xi ∈ Kout,i ∩ {
y : Ay = b
, and let Φ =
{
c, u
)du
Hi be the halfspace generated by the separa-
+ t
−
h
i
out,i as shown in Line 17 of Algorithm 1. Then the new potential Φ(new) =
Oi queried at x⋆
[n] ψin,i(xi) is bounded from above as follows.
)du
c, u
t
exp(
i
h

def
=
[n] ψin,i(xi). Let

c, x
i
h

c, x
i
h

Kout

P
+

+ t

}}

i
∈

+

−

i
∈

b

i

log

out exp(

K
b
hR
tion oracle

log

out

K
b
hR

∩Hi

i

P
Φ(new)

Φ + log(1

1/e).

−

≤

Proof. The change in potential is given by

Φ(new)

−

Φ = log

out

∩Hi

K
b

out

exp(

" R

exp(

t

) dx
c, x
i
· h
) dx #
c, x
i

.

−
t

−

· h

K
b
R

We now apply Theorem 2.14 to the right hand side, with the function f (x) = exp(
A(θ) = log

. Noting that each halfspace

exp(
out
Kout with respect to f (by the deﬁnition of x⋆
is the centroid of
and gives the claimed volume change.
b

)dx
θ, x
i

hR

−h

K
b

i

t

c, x
i −
out,i, where x⋆
Hi passes directly through x⋆
out
out in Equation (3.5)), Remark 2.15 applies

A(tc)), where

· h

−

To capture the change in potential due to the update in t, we recall the alternative perspective to the
given by Deﬁnition 2.12 and derive properties of self-concordant barriers.

exp(

function log

t

out

K
b
hR

)dx
c, x
i
h

−

i

Lemma 4.2. Consider a ν-self-concordant barrier ψ : int(
Deﬁne

R over the interior of a convex set

)

→

K

ξψ
t

def
= min

x

[t

c, x
i

· h

+ ψ(x)] and xt

def
= arg min

x

[t

c, x
i

· h

+ ψ(x)] .

Then for 0

h

≤

≤

1
√ν , we have

ξψ
t + th

xt, c

· h

i ≥

ξψ
t(1+h) ≥

ξψ
t + ht

c, xti −

· h

h2ν.

Proof. Note that here the ﬁrst inequality is fairly generic and holds for any function ψ. By deﬁnition of
ξψ
t(1+h) and ξψ
in Equation (4.2) and using the fact that the value on the right hand side of Equation (4.3)
is smaller than the expression evaluated at a ﬁxed x = xt,we have

t

Rd.

K ⊆

(4.2)

(4.3)

ξψ
t(1+h) = min
x

[t(1 + h)

t(1 + h)

≤
= ξψ

t + th

· h

· h
xt, c
i
xt, c
.
i

+ ψ(x)]

x, c
i
+ ψ(xt)

· h
We now prove the second inequality of the lemma. This one speciﬁcally uses the self-concordance of ψ.
Observe ﬁrst, by deﬁnition,

ξψ
t =

ψ∗(

−

−

tc).

(4.4)

Since ψ is a self-concordant barrier (and hence, a self-concordant function), Theorem 2.10 implies that ψ∗ is a
self-concordant function as well. Then, by applying Theorem 2.11 to ψ∗ under the assumption
1
yields

thc

k−

k−

≤

tc

ψ∗(

−

tc

−

thc)

ψ∗(

−

≤

tc) +

h∇

ψ∗(

−

tc),

thc
i

−

+

1
2 k −

(cid:20)

thc
k

2

−

tc +

3(1

3

thc
k
−
thc

k −
− k −

tc

k−

.

tc)

(cid:21)

(4.5)

By applying the ﬁrst-order optimality condition to the deﬁnition of xt in Equation (4.2), we see that

ψ(xt) =

∇

tc.

−

11

(4.6)

Next, evaluating a def=

tc to check the assumption

thc

tc

1, we get

thc

k−

k −
a2 = h2

tc,

h−

∇

2ψ∗(

tc)

(
−

tc)
i

·

−

k −
ψ(xt),
∇
ψ(xt), (

≤

k−
2ψ∗(
∇
2ψ(xt))−

ψ(xt))
1

∇

ψ(xt)
i

· ∇
ψ(xt)
i

· ∇

= h2
= h2
h2ν

h∇

h∇

≤

where we used Equation (4.6) and Lemma 2.4, in the ﬁrst two equations and Deﬁnition 2.7 and the complex-
ity value of ψ in the last step. Our range of h proves that a
1, which is what we need for Inequality (4.5)
to hold. We continue our computation to get

≤

1
2 k −

(cid:20)

thc
k

2

−

tc +

3(1

3

thc
k
−
thc

k −
− k −

tc

k−

1
2

h2ν +

1
3

h3ν3/2

1
2

≤

h2ν +

1
3

h2ν

≤

h2ν.

tc)

(cid:21)

≤

Applying Lemma 2.3 to Equation (4.6) gives

ψ∗(

tc) = xt.

∇

−

(4.7)

(4.8)

Plugging Equation (4.8) and Inequality (4.7) into the ﬁrst and second-order terms, respectively, of Inequality (4.5)
gives

−
Plugging in Equation (4.4) gives the desired inequality and completes the proof.

≤

−

−

−

ψ∗(

tc

thc)

ψ∗(

tc) +

xt,
h

thc
i

+ h2ν.

To ﬁnally compute the potential change due to t, we need the following result about the self-concordance
parameter of the entropic barrier. While [BE15] prove that this barrier on a set in Rd is (1 + ǫd)d-self-
concordant, the recent work of [Che21] remarkably improves this complexity to exactly d.

Theorem 4.3 ([Che21]). The entropic barrier on any convex body

Rd is a d-self-concordant barrier.

K ⊆

We may now compute the potential change due to change in t in Line 9.

Lemma 4.4. When t is updated to t
increases to Φ(new) as follows:

·

1 + η
4m

(cid:2)

(cid:3)
Φ(new)

in Line 9 of Algorithm 1, the potential Φ Equation (4.1)

Φ + η + η2.

≤

Proof. Recall that the barrier function we use for the set
and the deﬁnition of conjugate, we have

ξψout
t

−

= max

v

[

h−

tc, v

i −

Kout is the entropic barrier ψout. By Equation (4.2)
b
ψout(v)] = ψ∗out(

tc).

−

Applying Deﬁnition 2.12, taking the conjugate on both sides of the preceding equation, and using Lemma 2.2
then gives

From Equation (4.1), the change in potential by changing t to t

ξψout
t

= log

−

(cid:20)Z

K
b

exp(

−

t

c, u

· h

) du
i

.

(cid:21)

out

(4.9)

(1 + h) for some h > 0 may be expressed as

·

Φ(new)

Φ = log

−

exp

t(1 + h)c, v

h−

dv
i

log

−

exp

tc, v

h−

dv
i

+

th
h

·

c, x
.
i

(cid:21)
4m and ν = m (via a direct application of Theorem 4.3), we have h = η
By applying h = η
√ν ,
and so we may now apply Equation (4.9) and Lemma 4.2 in the preceding equation to obtain the following
bound.

√m = 1

4m ≤

(cid:20)Z

(cid:20)Z

K
b

K
b

(cid:21)

out

out

1

From Equation (3.2) and Equation (4.2), we see that xt for the entropic barrier satisﬁes the equation xt =
+ 4m
x⋆
t

out, and applying the guarantee

to this inequality, we obtain

c, x⋆

c, x
h

i ≤ h

outi

Φ(new)

Φ

th

c, x
h

i −

th

c, xti
h

≤

−

+ h2ν.

Φ(new)

Φ

−

≤

th

4m
t

·

+ h2ν = η +

η
4m

(cid:16)

2

ν

(cid:17)

≤

η + η2.

12

4.2 Potential change for the universal terms

In this section, we study the change in volume on growing the inner convex set
Kin,i in Line 15. As mentioned
in Section 3, our barrier of choice on this set is the universal barrier introduced in [NN94a] (see also [Gül97]).
This barrier was constructed to demonstrate that any convex body in Rn admits an O(n)-self-concordant
barrier, and its complexity parameter was improved to exactly n in [LY21].

Conceptually, we choose the universal barrier for the inner set because the operation we perform on the
inner set (i.e., generating its convex hull with an external point x⋆
out) is dual to the operation of intersecting
the outer set with the separating halfspace containing x⋆
out (see Lemma 2.6), which suggests the use of a
barrier dual to the entropic barrier used on the outer set. As explained in [BE15], for the special case of
convex cones, the universal barrier is precisely one such barrier.

We now state a technical property of the universal barrier, which we use in the potential argument for

this section.

Lemma 4.5 ([LY21, Lemma 1], [NN94a; Gül97]). Given a convex set
log vol(
K −
Rd
Σ
×
is the polar set of

d be the covariance matrix of the body (

x)◦ be the universal barrier deﬁned on

with respect to x. Let µ
x)◦ =
x)◦, where (

with respect to x. Then, we have that

K
K −

K −

K ∈
∈
y
{

∈

K

def
Rd and x
=
Rd be the center of gravity and
x)

Rn : y⊤(z

, let ψ

∈ K

(x)

1,

z

K

−

≤

∀

∈ K}

∈

ψ

(x) = (d + 1)µ,

2ψ

(x) = (d + 1)(d + 2)Σ + (d + 1)µµ⊤.

∇

K

Lemma 4.6. Given a convex set
barrier deﬁned on

with respect to x. Let η

K ⊆

K

def
= log vol(

x)◦ be the universal
. Let ψ
K −
be a point satisfying the following condition

K

∇

K
Rd and a point x
1/4 and y

≤
(x), y

∈ K
∈ K
x
y
k

−

ψ

h∇

K

x
i

−

+ η

k

4d,

x

≥

(4.10)

and construct the new set conv
respect to x satisﬁes the following inequality.

{K

}

, y

. Then, the value of the universal barrier deﬁned on this new set with

ψ

,new(x)

def
= ψconv

,y

}
Proof. By Lemma 2.6, we have that

{K

K

(x) = log vol(conv(

K

, y)

x)◦

ψ

K

≤

−

(x) + log(1

−

1/e + η).

(conv(

K

, y)

x)◦

(
K −

⊆

x)◦

,

∩ H

−

=

z
where
{
H
log vol(conv(
from vol(

K−

Rn :

∈
, y)

x

z, y
h
−
x)◦ from ψ

i ≤
K

]
∩ H
K
x)◦, for which it is immediate that one may apply an appropriate form of Grünbaum’s Theorem.

K −

≤

−

K

. Our strategy to computing the deviation of ψ
}

1
(x) is to compute the change in vol(conv(

, y)

,new(x) def= ψconv(
x)◦
vol [(

K
x)◦

K
−

,y)(x) =

Let µ be the center of gravity of the body (

x)◦. If µ /

, then Corollary 2.16 (with η = 0) gives

vol [(

x)◦

K −
]

vol(

∈ H
x)◦

(1

1/e),

≤
and taking the logarithm on both sides gives the claimed bound. We now consider the case in which µ
and the variance matrix of the body (

x, and consider the point

x)◦ is Σ. Deﬁne v = y

K −

K −

∩ H

−

·

,

∈ H

This point satisﬁes
show that z is suﬃciently close to µ, so that even though µ
halfspace
we may compute the following quantity.

is not too large. By applying Lemma 4.5 to compute

= 1, which implies z

v, z
i
h

∈ H

H

K −

1

z = µ +

−

v, µ
2
Σ

Σv.

·

i

− h
v
k
k
. Speciﬁcally, z lies on the separating hyperplane. We
x)◦ cut out by the
K −
2,
2
v
x = (d + 1)(d + 2)
i
k

, the subset of (
∈ H
v
k

2
v, µ
Σ + (d + 1)
h

k

k

z

k

µ

kΣ−1 =

−

=

1

v, µ

− h
2
v
x −

k

i

1
d+2 · h

1
(d+1)(d+2) k

(d + 1)(d + 2)

q

p

v, µ

2
i
1
x + 1
2
2 k

v

1
2 k

k

·

q

13

v, µ

− h
2
v
x −

k

i
v, µ
(d + 1)
h

.

2
i

(4.11)

Applying the expression for gradient from Lemma 4.5 in Equation (4.10), we have

v

η

k

x

k

≥

4d

v, µ
(d + 1)
h

−

i ≥

v, µ
2d
h

,
i

where we used the fact that µ
Plugging this in Equation (4.11) gives

∈ H

implies

v, µ
h

i ≤

1. Since η

≤

1/4, we have 1

2 k

v

2
x ≥

k

v, µ
(d + 1)
h

2.
i

z

k

−

µ

kA−1

≤

(d + 1)(d + 2)

1

·

p

4d

≤

1
4d(1

·

v, µ
v, µ

q

i
)/η ≤
i

− h
− h

v, µ

1

4d

≤

v, µ

− h
v
k

x

k

i

i
2
x

− h
1
2 k

v

k

η,

which implies Corollary 2.16 applies, giving us the desired volume reduction.

4.3 Potential change for the update of x

In this section, we quantify the amount of progress made in Line 22 of Algorithm 1 by computing the change
in the potential Φ as deﬁned in Equation (4.1).

Lemma 4.7. Consider the potential Φ Equation (4.1) and the update step δx = η
as in Line 22.
2 ·
Assume the guarantees in Inequality (3.3) and Inequality (3.4). Then the potential Φ incurs the following
minimum decrease.

−
x

x⋆

x,1

out

out

−

k

k

x⋆

x

Φ(new)

Φ

−

≤

η2
4

.

Proof. Taking the gradient of Φ with respect to x and rearranging the terms gives

By applying the expression for tc from the preceding equation, we get

tc =

xΦ

∇

−

n

i=1
X

ψin,i(xi).

∇

(4.12)

Φ(new)

Φ = t

c, x + δx
h

i

+

−

n

i=1
X
n

ψin,i(xi + δx,i)

t

c, x
h

i −

−

ψin,i(xi)

n

i=1
X

=

xΦ, δx

h∇

+

i

i=1
X

[ψin,i(xi + δx,i)

ψin,i(xi)

ψin,i(xi), δx,ii
]

− h∇

.

−

(4.13)

qψin,i (xi)

The term qψin,i(xi) measures the error due to ﬁrst-order approximation of ψin,i around xi. Since ψin,i(xi) is
self-concordant functions and

1/4, Theorem 2.11 shows that

η

δx

|

{z

}

k

δx,ik
ψin,i(xi + δx,i)

xi ≤ k

k

x,1 ≤
ψin,i(xi)

≤

−

ψin,i(xi), δx,ii ≤ k

− h∇

2
x,i.

δx,ik

Plugging in Inequality (4.14) into Equation (4.13), we get

Φ(new)

Φ

−

xΦ, δx

≤ h∇

+

δx

k

k

i

2
x,1.

(4.14)

(4.15)

We now bound the two terms on the right hand side one at a time. Using the deﬁnition of δx (as given in

14

the statement of the lemma) and of

xΦ from Equation (4.12) gives

∇
1

xΦ, δx

h∇

=

i

=

≤

=

≤

=

η
2

η
2

η
2

η
2
η
2

−

x⋆
out −
1

x⋆
out −
1

x⋆
out −
1

x⋆
out −
1

k

k

k

k

x⋆
out −
k
η2/2.

x
k

x
k

x
k

x
k

x
k

xΦ, x⋆

out −

x
i

x,1 h∇

tc, x⋆

out −

x,1 "h

+

x
i

n

i=1
X
n

ψin,i(xi), x⋆

h∇

out,i −

xii#

tc, x⋆

out −

+

x
i

x,1 "h

η

x⋆
out,i −

k

xik

xi

4ri −
(cid:0)

i=1
X
+ 4m

[

tc, x⋆
h

out −

x
i

x,1

η

k

x⋆
out −

x,1]

x
k

−

η

(
−

k

x⋆
out −

x
k

x,1)

x,1 ·

#
(cid:1)

(4.16)

where the third step follows from Inequality (3.3), the fourth step follows from
step follows from Inequality (3.4). To bound the second term, we note from Line 22 that

n
i=1 di = m, and the ﬁfth

P

x⋆
out −
x⋆
out −
Hence, we may plug in Inequality (4.16) and Equation (4.17) into Inequality (4.15) to get the desired result.

x,1
x,1 (cid:19)

= η2/4.

x
k
x
k

2
x,1 =

η
2 ·

(4.17)

k
k

δx

(cid:18)

k

k

2

4.4 Total oracle complexity

Before we bound the total oracle complexity of the algorithm, we ﬁrst bound the total potential change
throughout the algorithm.

t

+ log

out exp(

[n] ψin,i(xi) as
c, x
Lemma 4.8. Consider the potential function Φ = t
i
h
deﬁned in Equation (4.1) associated with Algorithm 1. Let Φinit be the potential at t = tinit of this algorithm,
P
and let Φend be the potential at t = tend. Suppose at t = tinit in Algorithm 1, we have
⊆ Kin
Kout ⊆ Bm(0, ¯R) with ¯R = O(√nR). Then we have, under the assumptions of
with ¯r = r/ poly(m) and
Theorem 4.10, that
mR
ǫr

Bm(x, ¯r)

(cid:19)(cid:19)
Proof. For this proof, we introduce the following notation: let volA(
·
x : Ax = b
subspace
starting with the entropic terms

) denote the volume restricted to the
. We also invoke Fact 2.17. We now bound the change in the potential term by term,

Φend ≤

Φinit −

c, u
h

)du
i

K
b
hR

m log

i
∈

−

+

O

(cid:18)

(cid:18)

}

{

i

.

t

c, x
i
h

+ log

exp(

−

t

c, u
h

)du
i

(cid:21)

out

(cid:20)Z

K
b

(4.18)

at t = tinit and a lower bound on it at t = tend. We start with bounding Equation (4.18) evaluated at
t = tend = 8m

. By optimality of ¯x, we know that ¯x
c, ¯x
c, x
i
h
i
(z, ¯r) restricted to the subspace
. Note that

x : Ax = b

and α =

out h

K
b

∈

∂

∈

Kout. Denote
A(z, ¯r). Consider the
b

Kout ⊇ B

{

}

k2R .

c
ǫ
k
Let ¯x = arg minx

BA(z, ¯r) to be
cone
C

and halfspace

B

deﬁned by

H

= ¯x +

λy : λ > 0, y
{

A(z

¯x, ¯r)
}

−

∈ B

and

H

C

15

b

def=

x :

(cid:26)

c, x
h

i ≤

α +

1
tend (cid:27)

.

Then, by a similarity argument, we note that

¯r
¯Rtend

k

c

k2

, which means

contains a cone with height

C ∩ H

1
tend

k

c

k2

and base radius

volA(

C ∩ H

)

≥

m

1
rank(A) ·

1
tendk

−

c

k2 ·

(cid:18)

¯r
¯Rtendk

c

m

−

rank(A)

1

−

k2 (cid:19)

vol(

Bm

·

−

rank(A)

−

1(0, 1)).

Then, we have

log

out

(cid:20)Z

K
b

exp(

−

c, u

tendh

)du
i

(cid:21)

+ tendh

c, x

i ≥

≥

≥

log

log

log

out

(cid:20)Z

K
b

(cid:20)ZC∩H

(cid:20)ZC∩H
1
e ·

(cid:20)

volA(

= log

= log

exp(

−

c, u

tendh

)du
i

(cid:21)

+ tend min
K
b

∈

x

outh

c, x
i

exp(

−

tendh

c, u

exp(

−

tendα

−

)du
i

(cid:21)
1)du

+ tendα

+ tendα

(cid:21)
tendα)
(cid:21)

+ tendα

−

volA(

) exp(

C ∩ H
1
e

)

·

(cid:20)
(m

C ∩ H
rank(A)

(cid:21)
1)

≥ −

−

+ log(vol(
log(m

−

·

−
rank(A)
Bm
rank(A))
−

−

−

−

log( ¯Rtendk
1(0, 1)))
log(tendk

c

k2/¯r))

c
k2)

1.

−

(4.19)

Next, to bound Equation (4.18) at t = tinit, we may express these terms as follows.

log

out

(cid:20)Z

K
b

exp(

−

tinit · h

c, u

)du
i

+ tinit · h

c, x
i

(cid:21)

log

volA(

≤

≤

≤

h
log(vol(

log(vol(
+ (m

−

c, x

max
u

+ tinit ·

out h
rank(A)(0, ¯R))) + tinit ·
rank(A)(0, 1)))

Kout)
i
b
Bm
−
Bm
rank(A)) log ¯R + O(m log m),

−
2 ¯R

K
b

−

∈

u

i

k

c
k2

(4.20)

where the second step is by

Kout ⊆ Kout ⊆ BPi∈[n] di(0, ¯R) (here, the second inclusion is by assumption),
Bq(0, ¯R)) = πq/2
b
We now compute the change in the entropic barrier

¯Rq and our choice of tinit

def= m log m
k2R .
√n
k

and the third step is by vol(

Γ(1+q/2)

c

[n] ψin,i(xi), where

P
ψin,i(xi) = log vol(

i
∈
◦in,i(xi)).

K
Bd(0, r) to be the d-dimensional Euclidean ball centred at the origin and with radius r. We note
Kin,i ⊆ Ki ⊆ Bdi(0, ¯R) throughout the algorithm. By the
Kin,i ⊇ Bdi(x, ¯r).

Deﬁne
by the radius assumption of Theorem 4.10 that
assumption made in this lemma’s statement, we have that at the start of Algorithm 1,
These give us the following bounds.

ψend

in,i (xi)

log(vol(

B

≥

◦di(0, ¯R)) and ψinit

in,i (xi)

log(vol(

B

≤

◦di(xi, ¯r))).

16

Applying the fact that vol(

Bd(0, r))
ψinit

∝
in,i (xi)

[n]
Xi
∈

(cid:2)

rd and summing over all i

[n] gives

∈

ψend

in,i (xi)

−

(cid:3)

log

vol(
vol(

Bdi(xi, 1/¯r))
Bdi(0, 1/ ¯R))

(cid:18)

(cid:19)

di log( ¯R/¯r) = m log( ¯R/¯r).

≤

=

[n]
Xi
∈

[n]
Xi
∈

(4.21)

Combining Inequality (4.20), Inequality (4.19), and Inequality (4.21), we have

Φinit −

Φend ≤

≤

m log(mR/r)

log(vol(

+
+ (m
(cid:2)

−
+ log(m

Bm
−
rank(A)

rank(A)(0, 1))) + (m
−
log( ¯Rtendk
c
1)
k2/¯r)
·
c
rank(A)) + log(tendk
k2) + 1

−

−

rank(A)) log ¯R + O(m log m)

log(vol(

Bm

rank(A)

−

1(0, 1)))
(cid:3)

−

−

m log(mR/ǫr)
+ O(m log m)

+ O((m

−

rank(A)) log(mR/ǫr))

O(m log(mR/ǫr)).

≤

Lemma 4.9. [Total oracle complexity] Suppose the inputs
Bm(0, ¯R) with ¯R = O(√nR) and
t
≥

tend, it outputs a solution x that satisﬁes

Kin ⊇ B

Kout ⊆
(z, ¯r) with ¯r = r/ poly(m). Then, when Algorithm 1 terminates at

Kout to Algorithm 1 satisfy

Kin and

using at most

Nsep = O

m log

mR
ǫr

c⊤x

≤

x

min
,Ax=b

∈K

c⊤x + ǫ

c
k2R

· k

separation oracle calls.

(cid:0)
Kout is updated;

Nt be the number of times t is updated;
(cid:0)

Proof. Let
Nout the number
of times
Ntotal the total number of iterations of
the while loop before termination of Algorithm 1. Then, combining Lemma 4.1, Lemma 4.4, Lemma 4.6,
and Lemma 4.7 gives

x the number of times x is updated, and

Nin the number of times

Kin is updated;

(cid:1)(cid:1)

N

Φend ≤

Φinit +

Nout ·

log(1

−

1/e) +

Nt ·

(η + η2) +

Nin ·

log(1

−

1/e + η) +

x
N

·

−

(cid:18)

η2
4

.

(cid:19)

(4.22)

The initialization step of Algorithm 1 chooses η = 1/100, tend = 8m
update t by a multiplicative factor of 1 + η

k

c

ǫ

k2R , and tinit = m log(m)

√n

c

k2R , and we always

k

4m (see Line 9); therefore, we have
Nt = O(m log(mR/(ǫr)).

From Algorithm 1, the only times the separation oracle is invoked is when updating
and Line 17, respectively. Therefore, the total separation oracle complexity is
Nsep =
we have

Kin or
Nin +

Kout in Line 15
Nout. Therefore,

This gives the claimed separation oracle complexity.

Nsep =

Nin +

Nout ≤

O(1)

[Φinit −

Φend +

Nt] = O(m log(mR/(ǫr))

·

We now prove the guarantee on approximation. Let xoutput be the output of Algorithm 1 and x be the
point which entered Line 5 right before termination. Note that the termination of Algorithm 1 implies, by
Line 5, that

c⊤xoutput ≤

c⊤x +

ν

tend ≤

c⊤x⋆

out +

4(n + m)
tend

≤

x

min
,Ax=b

∈K

c⊤x + ǫ

c
k2 ·

· k

R

where the ﬁrst step is by the second inequality in Lemma 5.8 (using the universal barrier) and the last step
follows by our choice of tend and the deﬁnition of x⋆
out and

.

Kout ⊇ K

17

Theorem 4.10 (Main theorem of Problem 3.1). Given the convex program

minimize
subject to

c, x
,
h
i
xi ∈ Ki ⊆
Ax = b.

Rdi+1

[n],

i
∀

∈

Denote

=

K

K1 × K2 ×

. . .

× Kn. Assuming we have

• outer radius R: For any xi ∈ Ki, we have
• inner radius r: There exists a z

xik2 ≤
Rd such that Az = b and

R, and

k

∈

(z, r)

B

,

⊂ K

then, for any 0 < ǫ < 1

2 , we can ﬁnd a point x

c, x
h

i ≤

xi∈Ki⊆

satisfying Ax = b and

c, x
i
h
[n],

+ ǫ

c
k2 ·

· k

R,

i
∈

∀

∈ K
min
Rdi+1
Ax=b

in O(poly(m log(mR/ǫr))) time and using

gradient oracle calls, where m =

n
i=1 di.

O(m log(mR/(ǫr))

P
Proof. We apply Theorem 5.1 for each
with ¯r = r
satisﬁes
s = 216 m2.5R

and obtaining the following:

Bm+n(z, ¯r)

⊂ K

rǫ

Ki separately to ﬁnd a solution zi. Then z = (z1, . . . , zn)

Rm+n
6m3.5 . Then, we modiﬁed convex problem as in Deﬁnition 5.5 with

∈

minimize
subject to

¯c, ¯x
h
i
¯A¯x = ¯b,
def=
¯
¯x
K

∈

Rm+n
≥

0 ×

Rm+n
0
≥

K ×

(4.23)

with

¯A = [A

A

|

| −

A], ¯b = b, ¯c = (c, k

c
k2s
√m + n ·

1, k

c
k2s
√m + n ·

1)⊤

We solve the linear system Ay = b

x(2)
i =

yi
0

(

−
if yi ≥
0,
otherwise.

and

x(3)
i =

yi

−
0

(

if yi < 0,
otherwise.

Az for y. Then, we construct the initial x by set x(1) = z,

ǫ
6√ns ,

{

∈

0}

and

x(1)

R2n
≥

Kin =

B(z, ¯r), (x(2), x(3))

∈
By our choice of tend, we have

Then, we run Algorithm 1 on the Problem 4.23, with initial x set above, ¯m = 3(m + n), ¯n = n + 2, ¯ǫ =
Kout =
8 ¯m
b
k2 ¯R ≤
¯c
ǫ
k
48¯ν¯tend√m + n R2
r k
m2.5R

B ¯m(0, √nR).
48m
c
k2R
c
k2, we note that
216 m2.5R
rǫ

First, we check the condition that s

48¯ν¯tend√m + n

R2
r k

c
k2 ≤

¯tend =

ǫr ≤

27648

= s.

¯ǫ
k

≥

.

Let ¯xoutput = (x(1)

output, x(2)

output, x(3)

output) be the output of Algorithm 1. Then, let xoutput = x(1)

output +

x(2)
output −

x(3)
output as deﬁned in Theorem 5.6. By Lemma 4.9, we have

where γ = ¯ǫ

¯c
k2 ·

· k

¯R.

¯c⊤x

min
x
in
∈P

min
x
∈P

≤

c⊤x + γ

18

Applying (3) of Theorem 5.6, we have

c⊤xoutput ≤

¯ν + 1
¯tend

+ γ + min
x

,Ax=b

c⊤x

min
,Ax=b

c⊤x + ǫ

c
k2 ·

· k

R.

≤

x

∈K
The last inequality follows by our choice of ¯ǫ and ¯tend, we have γ
in Lemma 4.9, it gives the claimed oracle complexity.

∈K

ǫ
2 k

k2R and ¯ν+1
c

¯tend ≤

ǫ
2 k

c
k2R. Plug this ¯ǫ

≤

Theorem 4.11 (Main Result). Given Problem 1.1 and θ(0) such that
R. Assuming all the
fi’s are L-Lipschitz, then there is an algorithm that in time poly(m log(1/ǫ)), using O(m log(m/ǫ)) gradient
oracle calls, outputs a vector θ

Rd such that

k2 ≤

θ(0)

θ⋆

−

k

∈

n

i=1
X

fi(θ)

≤

n

i=1
X

fi(θ⋆) + ǫ

LR.

·

Proof. First, we reformulate (1.1) using a change of variables and the epigraph trick. Suppose each fi depends
Rdi
on di coordinates of θ given by
for each i
dimensional. So we may deﬁne the convex set

[n]. Since each fi is convex and supported on di variables, its epigraph is convex and di + 1

[d]. Then, symbolically deﬁne xi = [x(i)

i1, . . . , idi} ⊆

i2 ; . . . ; x(i)
idi

i1 ; x(i)

∈

∈

{

]

Finally, we add linear constraints of the form x(i)
We denote these by the matrix constraint Ax = b. Then, Problem 1.1 is equivalent to

for all i, j, k where fi and fj both depend on θk.

unbounded
i
K

=

(xi, zi)

Rdi+1 : fi(xi)

(cid:8)

∈
k = x(j)

k

Lzi

.

≤

(cid:9)

n
minimize
i=1 Lzi
subject to Ax = b
P
(xi, zi)

unbounded
i
∈ K

for each i

[n].

∈
i = [θ(0)

i1

R, we deﬁne x(0)
to

; . . . , θ(0)
idi

] and z(0)

i = fi(θ(0))/L.

(4.24)

Since we are given θ(0) satisfying
k
Then, we can restrict the search space

θ(0)

−

θ∗
k2 ≤
unbounded
i
K
Rdi+1 :

Ki =

unbounded
i
K

∩ {

(xi, zi)

∈

xi −

k

x(0)
i k2 ≤

R and z(0)

i −

2R

≤

z(0)
i + 2R

.

}

It’s easy to check that
of radius R centered at (x(0)
Then, we apply Theorem 4.10 to (4.24) with
complexity directly.

, z(0)
i

i

Ki is contained in a ball of radius 5R centered at (x(0)

). The subgradient oracle for fi translates to a separation oracle for

i

), and contains a ball
Ki.
Ki to get the error guarantee and oracle

unbounded
i
K

replaced by

zi ≤
, z(0)
i

Finally, we have the matching lower bound.

Theorem 1.3. There exist functions f1, . . . , fn : Rd
are required to solve Problem 1.1.

7→

R for which a total of Ω(m log(1/ǫ)) gradient queries

R for which Ω(di log(1/ǫ)) total gradient
Proof. [Nes04] shows that for any di, there exists fi : Rdi
queries are required. We deﬁne f1, . . . , fn to be such functions on disjoint coordinates of θ. It follows that
Ω(

n
i=1 di log(1/ǫ)) = Ω(m log(1/ǫ)) gradient queries are required in total.

7→

P

5 Initialization

5.1 Constructing an initial

in,i

K

In this section, we discuss how to construct an initial set
particular, we will prove the following theorem.

Kin,i to serve as an input to Algorithm 1.

In

19

Theorem 5.1. Suppose we are given separation oracle access to a convex set

that satisﬁes

(z, r)

(0, R) for some z

Rd. Then, Algorithm 2, in O(d log(R/r)) separation oracle calls to

B

⊆ K ⊆
, outputs a point

K

K

B
x such that

∈
r
6d3.5

x,

B

(cid:0)

.

⊆ K

(cid:1)

Algorithm 2 Inner Ball Finding

Kout ←

B(0, R)
1:
2: while true do
3:
4:

Let v be the center of gravity of
Kout
Sample u from B(v, r/(6d)) uniformly
if u

∈ K
Let S =
if S

then
v
±
then

{
⊂ K

end if

r
6d3 ei : i

[d]

}

∈

return the inscribed ball of conv(S)

end if
Let

11:
12: end while

Kout ← Kout ∩ H

where

=

O

H

(u)

5:

6:
7:

8:
9:

10:

Before we prove the preceding theorem, we need the following facts about the self-concordant barrier and

convex sets.

Theorem 5.2 ([Nes04, Theorem 4.2.6]). Let ψ : int(
mizer x⋆

ψ. Then, for any x

) we have:

int(

∈

K

R be a ν-self-concordant barrier with the mini-

)

→

K

On the other hand, for any x

∈

Rd such that

k

x⋆
ψ −
x

k

x⋆
ψ ≤

x
k
x⋆
ψk

x⋆
ψ ≤

ν + 2√ν.

Theorem 5.3 ([KLS95, Theorem 4.1]). Let
matrix Σ. Then,

K ⊆

1, we have x

int(

).

−
Rd be a convex set with center of gravity µ and covariance

K

∈

k
Theorem 5.4 ([BGVV14, Section 1.4.2]). Let

p

−

≤

{

x :

x

µ

kΣ−1

(d + 2)/d

x :

x

} ⊆ K ⊆ {

kΣ−1
k
be a convex set with

−

µ

δ =

{

K−

x : B(x, δ)

⊂ K}

. Then, we have

K

d(d + 2)
}

.

≤

p
K ⊂

B(u, R) for some R. Let

vol

K−

δ ≥

vol

K −

(1

(1

−

−

δ
R

)d)

·

volB(u, R)

Proof of Theorem 5.1. We note that by the description of the Algorithm 2, the returned ball is the inscribed
ball of conv(S) and we have v
. We note that conv(S)
is a ℓ1 ball with ℓ1 radius

∈
r
6d3 , then the inscribed ball has ℓ2 radius

r
6d3.5 .
First, we prove the sample complexity of the algorithm above. We use

S. Then, we must have conv(S)

⊆ K
Kout at the t-th
Kt to denote the
Kt is obtained by intersection of halfspaces and

iteration. We ﬁrst observe that throughout the algorithm,
B(0, R). This implies

for each v

∈ K

Since

Kt contains a ball of radius r, let At be the covariance matrix of

Kt. By Theorem 5.3, we have

B(z, r)

⊆ K ⊆ Kt

t.

∀

At (cid:23)

r2
d(d + 2)

I.

Let
B(v, r/(6d)), so we have

Ht be the halfspace returned by the oracle at iteration t. We note that u is sampled uniform from
d(d + 2)

v

k

−

u

kA−1

≤ p

r

·

20

r
6d ≤

1
3

.

Apply the inequality above to Corollary 2.16, we have

vol(

Kt)

(1

1/e + 1/3)tvol(

(1

1/30)tvol(B(0, R)).

K0)

Then, since B(z, r)

−
⊆ Kt for all the t, this implies the algorithm at most takes O(d log(R/r)) many iterations.
Now, we consider the number of oracle calls within each iterations. There are three possible cases to

−

≤

≤

consider:

1. u

∈ K−

δ with δ = r

6d3 (see the deﬁnition of

K−

this is the last iteration. We can pay this O(d) oracle calls for the last iteration.

δ in Theorem 5.4). In this case, we have S

and

⊂ K

2. u

δ.

∈ K\K−

Since u is uniformly sampled from B(v, r/(6d)), Theorem 5.4 shows that u
at most

δ with probability

∈ K\K−

(1

1

−

−

δ
r/(6d)

)d

≤

1
d

.

Hence, this case only happens with probability only at most 1/d. Since the cost of checking S
takes O(d) oracle calls. The expected calls for this case is only O(1).

⊂ K

3. u /
∈

K. The cost is just 1 call.

Combining all the cases, the expected calls is O(1) per iteration.

5.2 Initial point reduction

In this section, we will show how to obtain an initial feasible point for the algorithm.

Deﬁnition 5.5. Given a convex program minAx=b,x
c
c3 = s
k2
k
√d ·
P
convex program by

, (x(2), x(3))

1 and

x(1)

∈ K

=

∈

{

We denote (c1, c2, c3) by c.

min

(x(1),x(2),x(3))

∈P

∈K⊆
0 : A(x(1) + x(2)

R2d
≥
c⊤1 x(1) + c⊤2 x(2) + c⊤3 x(3).

−

}

Rd c⊤x and some s > 0, we deﬁne c1 = c, c2 =
. We then deﬁne the modiﬁed

x(3)) = b

Theorem 5.6. Given a convex program minAx=b,x
Kin with
48νt√d
·

k2R. For an arbitrary t

Kin ⊆ K
R
c
r · k

R

∈

∈K⊆

0, we deﬁne the function

and inner radius r. For any modiﬁed convex program as in Deﬁnition 5.5 with s

Rd c⊤x with outer radius R and some convex set

≥

≥
in (x(1), x(2), x(3))
ft(x(1), x(2), x(3)) = t(c⊤1 x(1) + c⊤2 x(2) + c⊤3 x(3)) + ψ
P

where ψ

in is some ν self-concordant barrier for the set

P

Pin =
, x(3)
t

x(1)

{

∈ Kin, (x(2), x(3))

∈

R2d
≥

0 : A(x(1) + x(2)

x(3)) = b

.

−
in ft(x(1), x(2), x(3)), we denote xin = x(1)

}

def
= (x(1)

t

Given xt
Suppose minx

, x(2)
t
in ¯c⊤x

∈P

) = arg min(x(1),x(2),x(3))

¯c⊤x + γ, we have the following

∈P

minx

∈P

≤

t + x(2)

t −

x(3)
t

.

1. Axin = b,
2. xin ∈ Kin,
3. c⊤xin ≤
First, we show that x(1)

minx

∈K

t

,Ax=b c⊤x + ν+1

t + γ.

is not too close to the boundary. Before we proceed, we need the following

lemmas.

21

Lemma 5.7 (Theorem 4.2.5 [Nes04]). Let ψ be a ν-self-concordant barrier. Then, for any x
y

dom(ψ) such that

dom(ψ) and

∈

∈

we have

ψ′(x), y
h

−

x

i ≥

0,

y

x
k

x

ν + 2√ν.

≤
Lemma 5.8 (Theorem 2 of [ZLY22]). Given a convex set 4 Ω with a ν-self-concordant barrier ψΩ and inner
radius r. Let xt = arg minx t

c⊤x + ψΩ(x). Then, for any t > 0,

−

k

·

min

1
2t

,

(cid:26)

r

c
k
4ν + 4√ν

k2

c⊤xt −

≤

c⊤x

∞ ≤

ν
t

.

(cid:27)

Consider the optimization problem restricted in the subspace

as a direct corollary of theorem above we have the following:

{

(x(1), x(2), x(3)) : A(x(1) +x(2)

x(3)) = b

,

}

−

Corollary 5.9. Let ¯xt be as the same as deﬁned in Theorem 5.6. For t

2t

k

.

1
c
k2
Now, we are ready to show dist(x(1)

t

, ∂

Kin) is not too small.

4ν
c

r

≥

k

k2

, we have dist(x(1)

t

, x(1)
∞

)

≥

Theorem 5.10. Let ¯xt be the same as deﬁned in Theorem 5.6. For t

12νt

r
c

k

k2R .

4ν
c

r

≥

k

k2

, we have dist(x(1)

t

, ∂

Kin)

≥

Proof. We consider the domain restricted in the subspace
the optimality of ¯xt and Lemma 5.7, we have

{

(x(1), x(2), x(3)) : A(x(1) + x(2)

x(3)) = b

. By

}

−

KH ⊆ {

x :

x

k

−

x(1)
t kx(1)

t ≤

ν + 2√ν

,

}

where

=
Recall that

H

{

x : c⊤(x(1)

t −

x)

0

}

≥

and

KH

def=

H ∩ Kin.

a ball and a convex cone
ball of radius at least
We note that

Kin contains a ball of radius r, we denote it by B. We note that conv(x(1)
∞
with diameter at most 2R. We observe that the set conv(x(1)
, B)
∞
k2R since dist(x(1)
.
k
conv(x(1)
∞

∩ H ⊆ Kin ⊆ {

x(1)
t kx(1)

ν + 2√ν

1
c
k2

t ≤

, B)

C
r
c

x :

, ∂

H

−

≥

x

∞

}

k

2t

4t

)

k

,

, B) is a union of
contains a

∩ H

contains a ball of radius at least

}

r
c

k2R , and then by Theorem 5.2,

4t

k

x(1)
t kx(1)
r
c

ν +2√ν
t ≤
k2R )
⊆ Kin.
, x(3)
t )

k
, x(2)
t

∈

x :
this implies
we have B(x(1)

{

,

t

x

k

−

4(ν+2√ν)t

Lemma 5.11. Let (x(1)
x(2)
t −

x(3)
t k2 ≤

t
4√d
s R.

k
Proof. Let x⋆

in = arg minx

R3d be the same as deﬁned in Theorem 5.6. If t > ν
c

k2R , then we have

k

in,Ax=b c⊤x and x⋆

in = arg minx

∈K

in c⊤x. Since x⋆

∈P

∈ B

(0, R), we have

c⊤x⋆

in ≤ k

c
k2R.

Note that (x⋆

in, 0, 0)

∈ Pin, this means we have
c⊤x⋆

c
k2R.
Combining this with the second inequality in Lemma 5.8, we get

in ≤ k

in ≤

c⊤x⋆

c⊤xt ≤

c⊤x⋆

in +

ν
t ≤ k

c
k2R +

ν
t ≤

2

c
k2R.

k

4The original theorem is stated only for polytopes, but their proof works for general convex sets.

22

We further note that

This shows

Hence, we have

c⊤2 x(2)

t ≤

c⊤xt ≤

2

c
k

k

R.

max

{k

x(2)
t k2,

x(3)
t k2} ≤

k

2√d
c
k2R
k
c
k2s ≤

k

2√dR
s

.

x(2)
t −

x(3)
t k2 ≤

k

4√d
s

R.

Now, we are ready to prove Theorem 5.6.

Proof of Theorem 5.6. We note that xin satisﬁes (1), directly follows by deﬁnition of
have s

48νt√d

c

. By assumption, we

P

≥

R
r · k

·

k2R; using this in Lemma 5.11, we have
r
x(2)
t −
c
k2R

x(3)
t k2 ≤

12νt

k

.

This means xin = x(1)

t + x(2)

t ∈ Kin since dist(x(1)
x(3)

t

, ∂

t −

Now, we show c⊤xin is close to c⊤x⋆.
Let x⋆ = arg minx

,Ax=b c⊤x and x⋆ = arg minx

∈K

c⊤x. By Lemma 5.8, we have

∈P

k
Kin)

12νt

r
c

k

k2R .

≥

This implies

We have

c⊤xt −

ν
t ≤

c⊤x⋆

in ≤

c⊤x⋆ + γ

c⊤x(1)

t ≤

c⊤xt ≤

c⊤x⋆ +

c⊤x⋆ + γ.

+ γ.

≤

ν
t

c⊤xin = c⊤(x(1)

t + x(2)

t −

x(3)
t

)

≤

c⊤x⋆ +

ν
t

+

4
s k

c
k2R

≤

c⊤x⋆ +

ν + 1
t

+ γ.

6 Acknowledgements

We thank Ian Covert for helpful discussions about the problem applications and Ewin Tang for helpful
feedback on the paper.

23

References

[AB15]

Alekh Agarwal and Leon Bottou. “A lower bound for the optimization of ﬁnite sums”. In:
International conference on machine learning. PMLR. 2015, pp. 78–86.

[AKMSV21] Kyriakos Axiotis, Adam Karczmarz, Anish Mukherjee, Piotr Sankowski, and Adrian Vladu.
“Decomposable submodular function minimization via maximum ﬂow”. In: International Con-
ference on Machine Learning. PMLR. 2021, pp. 446–456.

[All17]

[AY16]

[BBV04]

[BC03]

[BE15]

[BGVV14]

[BKLLS20]

[Bot12]

[BV02]

Zeyuan Allen-Zhu. “Katyusha: The ﬁrst direct acceleration of stochastic gradient methods”.
In: The Journal of Machine Learning Research 18.1 (2017), pp. 8194–8244.

Zeyuan Allen-Zhu and Yang Yuan. “Improved SVRG for non-strongly-convex or sum-of-non-
convex objectives”. In: International conference on machine learning. PMLR. 2016, pp. 1080–
1089.

Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge
university press, 2004.

Léon Bottou and Yann Cun. “Large scale online learning”. In: Advances in neural information
processing systems 16 (2003).

Sébastien Bubeck and Ronen Eldan. “The entropic barrier: a simple and optimal universal
self-concordant barrier”. In: Conference on Learning Theory. 2015, pp. 279–279.

Silouanos Brazitikos, Apostolos Giannopoulos, Petros Valettas, and Beatrice-Helen Vritsiou.
Geometry of isotropic convex bodies. Vol. 196. American Mathematical Soc., 2014.

Sébastien Bubeck, Bo’az Klartag, Yin Tat Lee, Yuanzhi Li, and Mark Sellke. “Chasing nested
convex bodies nearly optimally”. In: Proceedings of the Thirty-First Annual ACM-SIAM Sym-
posium on Discrete Algorithms. 2020, pp. 1496–1508.

Léon Bottou. “Stochastic gradient descent tricks”. In: Neural networks: Tricks of the trade.
Springer, 2012, pp. 421–436.

Dimitris Bertsimas and Santosh Vempala. “Solving convex programs by random walks”. In:
Proceedings of the thiry-fourth annual ACM symposium on Theory of computing (STOC).
ACM. 2002, pp. 109–115.

[Cau+47]

Augustin Cauchy et al. “Méthode générale pour la résolution des systemes d’équations simul-
tanées”. In: Comp. Rend. Sci. Paris 25.1847 (1847), pp. 536–538.

[Che21]

[CL11]

[DBL14]

[DLY21]

[FGKS15]

[FJPZ13]

Sinho Chewi. “The entropic barrier is n-self-concordant”. In: arXiv preprint arXiv:2112.10947
(2021).

Chih-Chung Chang and Chih-Jen Lin. “LIBSVM: A library for support vector machines”. In:
ACM Transactions on Intelligent Systems and Technology 2 (3 2011). Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm, 27:1–27:27.

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. “SAGA: A fast incremental gradient
method with support for non-strongly convex composite objectives”. In: Advances in neural
information processing systems 27 (2014).

Sally Dong, Yin Tat Lee, and Guanghao Ye. “A nearly-linear time algorithm for linear pro-
grams with small treewidth: a multiscale representation of robust central path”. In: Proceedings
of the 53rd Annual ACM SIGACT Symposium on Theory of Computing. 2021, pp. 1784–1797.

Roy Frostig, Rong Ge, Sham Kakade, and Aaron Sidford. “Un-regularizing: approximate prox-
imal point and faster stochastic algorithms for empirical risk minimization”. In: International
Conference on Machine Learning. PMLR. 2015, pp. 2540–2548.

Alexander Fix, Thorsten Joachims, Sung Min Park, and Ramin Zabih. “Structured learning of
sum-of-submodular higher order energy functions”. In: Proceedings of the IEEE International
Conference on Computer Vision. 2013, pp. 3104–3111.

[GLS88]

Martin Grötschel, László Lovász, and Alexander Schrijver. Geometric algorithms and combi-
natorial optimization. Springer, 1988.

24

[Grü60]

[Gül97]

[HL16]

[JLLV21]

[JLSW20]

[JZ13]

[Kla06]

[KLS95]

[KLT09]

[KT10]

[KTE88]

[LMH15]

[LSW15]

[LV21]

[LY21]

[Mai15]

[MZJ13]

[Nes04]

[Nes83]

[NN89]

Branko Grünbaum. “Partitions of mass-distributions and of convex bodies by hyperplanes.”
In: Paciﬁc Journal of Mathematics 10.4 (1960), pp. 1257–1261.

Osman Güler. “On the self-concordance of the universal barrier function”. In: SIAM Journal
on Optimization 7.2 (1997), pp. 295–303.

Elad Hazan and Haipeng Luo. “Variance-reduced and projection-free stochastic optimization”.
In: International Conference on Machine Learning. PMLR. 2016, pp. 1263–1271.

He Jia, Aditi Laddha, Yin Tat Lee, and Santosh Vempala. “Reducing isotropy and volume to
KLS: an o*(n 3 ψ 2) volume algorithm”. In: Proceedings of the 53rd Annual ACM SIGACT
Symposium on Theory of Computing. 2021, pp. 961–974.

Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. “An improved cutting plane
method for convex optimization, convex-concave games, and its applications”. In: Proceedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing. 2020, pp. 944–953.

Rie Johnson and Tong Zhang. “Accelerating stochastic gradient descent using predictive vari-
ance reduction”. In: Advances in neural information processing systems 26 (2013).

Boas Klartag. “On convex perturbations with a bounded isotropic constant”. In: Geometric &
Functional Analysis GAFA 16.6 (2006), pp. 1274–1290.

Ravi Kannan, László Lovász, and Miklós Simonovits. “Isoperimetric problems for convex bod-
ies and a localization lemma”. In: Discrete & Computational Geometry 13.3 (1995), pp. 541–
559.

Pushmeet Kohli, Lubor Ladicky, and Philip H. S. Torr. “Robust higher order potentials for en-
forcing label consistency”. In: International Journal of Computer Vision 82.3 (2009), pp. 302–
324.

Alex Kulesza and Ben Taskar. “Structured determinantal point processes”. In: Advances in
neural information processing systems 23 (2010).

Leonid G Khachiyan, Sergei Pavlovich Tarasov, and I. I. Erlikh. “The method of inscribed
ellipsoids”. In: Soviet Math. Dokl. Vol. 37. 1988, pp. 226–230.

Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. “A universal catalyst for ﬁrst-order opti-
mization”. In: Advances in neural information processing systems 28 (2015).

Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. “A faster cutting plane method and its
implications for combinatorial and convex optimization”. In: 56th Annual IEEE Symposium
on Foundations of Computer Science (FOCS). 2015, pp. 1049–1065.

Yin Tat Lee and Santosh S Vempala. “Tutorial on the Robust Interior Point Method”. In:
arXiv preprint arXiv:2108.04734 (2021).

Yin Tat Lee and Man–Chung Yue. “Universal barrier is n-self-concordant”. In: Mathematics
of Operations Research 46.3 (2021), pp. 1129–1148.

Julien Mairal. “Incremental majorization-minimization optimization with application to large-
scale machine learning”. In: SIAM Journal on Optimization 25.2 (2015), pp. 829–855.

Mehrdad Mahdavi, Lijun Zhang, and Rong Jin. “Mixed optimization for smooth functions”.
In: Advances in neural information processing systems 26 (2013).

Yurii E. Nesterov. Introductory Lectures on Convex Optimization - A Basic Course. Vol. 87.
Applied Optimization. Springer, 2004. doi: 10.1007/978-1-4419-8853-9.
Yurii E Nesterov. “A method for solving the convex programming problem with convergence
rate O(1/k2)”. In: Dokl. akad. nauk Sssr. Vol. 269. 1983, pp. 543–547.

YE Nesterov and AS Nemirovskii. “Self-concordant functions and polynomial time methods
in convex programming. preprint, Central Economic & Mathematical Institute, USSR Acad”.
In: Sci. Moscow, USSR (1989).

[NN94a]

Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex pro-
gramming. SIAM, 1994.

25

[NN94b]

[Ren01]

[RM51]

Yurii E. Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex
programming. Vol. 13. Siam studies in applied mathematics. SIAM, 1994. isbn: 978-0-89871-
319-0. doi: 10.1137/1.9781611970791. url: https://doi.org/10.1137/1.9781611970791.
James Renegar. A mathematical view of interior-point methods in convex optimization. SIAM,
2001.

Herbert Robbins and Sutton Monro. “A stochastic approximation method”. In: The annals of
mathematical statistics (1951), pp. 400–407.

[Roc70]

R Tyrrell Rockafellar. Convex Analysis. Vol. 36. Princeton University Press, 1970.

[RRWN11]

Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. “Hogwild!: A lock-free ap-
proach to parallelizing stochastic gradient descent”. In: Advances in neural information pro-
cessing systems 24 (2011).

[RSB12]

[RW09]

[SK10]

[SLB17]

[SZ13a]

[SZ13b]

[Vai89]

[VBK20]

[VKR09]

[WS16]

[Zha04]

[ZL15]

Nicolas Roux, Mark Schmidt, and Francis Bach. “A stochastic gradient method with an expo-
nential convergence rate for ﬁnite training sets”. In: Advances in neural information processing
systems 25 (2012).

R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis. Vol. 317. Springer Science &
Business Media, 2009.

Peter Stobbe and Andreas Krause. “Eﬃcient minimization of decomposable submodular func-
tions”. In: Advances in Neural Information Processing Systems 23 (2010).

Mark Schmidt, Nicolas Le Roux, and Francis Bach. “Minimizing ﬁnite sums with the stochastic
average gradient”. In: Mathematical Programming 162.1 (2017), pp. 83–112.

Shai Shalev-Shwartz and Tong Zhang. “Accelerated mini-batch stochastic dual coordinate
ascent”. In: Advances in Neural Information Processing Systems 26 (2013).

Shai Shalev-Shwartz and Tong Zhang. “Stochastic dual coordinate ascent methods for regu-
larized loss minimization.” In: Journal of Machine Learning Research 14.2 (2013).

Pravin M Vaidya. “A new algorithm for minimizing convex functions over convex sets”. In:
30th Annual Symposium on Foundations of Computer Science. 1989, pp. 338–343.

Nate Veldt, Austin R Benson, and Jon Kleinberg. “Minimizing localized ratio cut objectives in
hypergraphs”. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining. 2020, pp. 1708–1718.

Sara Vicente, Vladimir Kolmogorov, and Carsten Rother. “Joint optimization of segmentation
and appearance models”. In: 2009 IEEE 12th international conference on computer vision.
IEEE. 2009, pp. 755–762.

Blake E Woodworth and Nati Srebro. “Tight complexity bounds for optimizing composite
objectives”. In: Advances in neural information processing systems 29 (2016).

Tong Zhang. “Solving large scale linear prediction problems using stochastic gradient descent
algorithms”. In: Proceedings of the twenty-ﬁrst international conference on Machine learning.
2004, p. 116.

Yuchen Zhang and Xiao Lin. “Stochastic primal-dual coordinate method for regularized em-
pirical risk minimization”. In: International Conference on Machine Learning. PMLR. 2015,
pp. 353–361.

[ZLY22]

Manru Zong, Yin Tat Lee, and Man-Chung Yue. “Short-step Methods Are Not Strongly
Polynomial-Time”. In: arXiv preprint arXiv:2201.02768 (2022).

26

A Decomposable submodular function minimization

A.1 Preliminaries

Throughout, V denotes the ground set of elements. A set function f : 2V
the following diminishing marginal diﬀerences property:

→

R is submodular if it satisﬁes

Deﬁnition A.1 (Submodularity). A function f : 2V
f (S), for any subsets S
T

V and i

T .

V

R is submodular if f (T

→

)

i

}

−

∪ {

f (T )

f (S

)

i

}

−

∪ {

≤

\

∈

⊆

⊆
). We assume
We may assume without loss of generality that f (
∅
that f is accessed by an evaluation oracle and use EO to denote the time to compute f (S) for a subset S.
Our algorithm for decomposable SFM is based on the Lovász extension [GLS88], a standard convex extension
of a submodular function.
Deﬁnition A.2 (Lovász extension [GLS88]). The Lovász extension ˆf : [0, 1]V
f is deﬁned as

R of a submodular function

) = 0 by replacing f (S) by f (S)

f (
∅

→

−

∈
[0, 1] is drawn uniformly at random from [0, 1].

ˆf (x) = E
t
∼

[0,1][f (
{

i

V : xi ≥

t

}

)],

where t

∼

The Lovász extension ˆf of a submodular function f has many desirable properties. In particular, ˆf is a

convex relaxation of f and it can be evaluated eﬃciently.

Theorem A.3 (Properties of Lovász extension [GLS88]). Let f : 2V
be its Lovász extension. Then,
(a) ˆf is convex and minx
(b) f (S) = ˆf (IS) for any subset S

[0,1]V ˆf (x) = minS

V f (S);

⊆

∈

V , where IS is the indicator vector for S;

⊆

(c) Suppose x

∈

[0, 1]V satisﬁes x1 ≥ · · · ≥

, then ˆf (x) =

V
i=1(f ([i])
|

|

x
|

V

|

f ([i

−

−

1]))xi.

R be a submodular function and ˆf

→

Property (c) in Theorem A.3 allows us to implement a sub-gradient oracle for ˆf by evaluating f .

P

Theorem A.4 (Sub-gradient oracle implementation for Lovász extension, Theorem 61 of [LSW15]). Let
R be a submodular function and ˆf be its Lovász extension. Then a sub-gradient for ˆf can be
f : 2V
implemented in time O(
|

EO +

2).

→

| ·

V

V

|

|

A.2 Decomposable submodular function minimization proofs

In this subsection, we prove the following more general version of Theorem 1.4.

n
i=1 Fi(S

[

|

→

→
−
V with

1, 1] be given by F (S) =
n
i=1 di and dmax := maxi
= di. Let m =
Vi|
∈

Theorem A.5 (Decomposable SFM). Let F : V
Vi), where each
R is a submodular function on Vi ⊆
Fi : 2Vi
[n] di.
Then we can ﬁnd an ǫ-approximate minimizer of f using at most O(dmaxm log(m/ǫ)) evaluation oracle calls.
ˆfi is the Lovász extension of f . Note
Proof. Let ˆfi be the Lovász extension of each fi, then ˆf =
that ˆf is 2-Lipschitz since the range of f is [
1, 1]. Also, the diameter of the range [0, 1]Vi for each Lovász
P
extension ˆfi is at most
[0, 1]V such
√dmax. Thus using Theorem 4.11, we can ﬁnd a vector x
that ˆf (x)
[0,1]V ˆf (x∗) + ǫ in poly(m log(1/ǫ)) time and O(m log(m√dmax/ǫ)) = O(m log(m/ǫ))
subgradients of the ˆfi’s. By Theorem A.4, each sub-gradient of ˆfi can be computed by making at most
dmax queries to the evaluation oracle for fi. Thus the total number of evaluation oracle calls we make
di ≤
in ﬁnding an ǫ-additive approximate minimizer x

[0, 1]V of ˆf is at most O(dmaxm log(m/ǫ)).

Vi| ≤

minx∗

|
p

n
i=1

P

P

−

≤

∩

∈

∈

Next we turn the ǫ-additive approximate minimizer x of ˆf into an ǫ-additive approximate minimizer
. Then by property (c) in Theorem A.3,
V for f . Without loss of generality, assume that x1 ≥ · · · ≥

x
|

V

|

∈

S
⊆
we have

ˆf (x) =

V

|

|

i=1
X

(f ([i])

f ([i

−

−

1]))xi = f (V )

x
|

V

|

·

+

V

|

1

|−

i=1
X

f ([i])

(xi −

·

xi+1).

27

xi+1 ≥

Since xi −
among f ([i]) for all i
S is an ǫ-additive approximate minimizer of f . This proves the theorem.

0, the above implies that mini
V

∈{
such that f (S)

f ([i])

∈ {

· · ·

1,...,

1,

|}

≤

≤

,

|

V
V
|
ˆf (x). Then by property (a) in Theorem A.3, the set

ˆf (x). Thus we can ﬁnd a subset S

⊆

|}

28

