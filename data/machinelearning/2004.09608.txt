2
2
0
2

b
e
F
2

]

G
L
.
s
c
[

3
v
8
0
6
9
0
.
4
0
0
2
:
v
i
X
r
a

Flow-based Algorithms for Improving Clusters: A Unifying
Framework, Software, and Performance

Kimon Fountoulakis∗ Meng Liu†

David F. Gleich‡

Michael W. Mahoney§

Abstract. Clustering points in a vector space or nodes in a graph is a ubiquitous primitive in
statistical data analysis, and it is commonly used for exploratory data analysis. In practice,
it is often of interest to “reﬁne” or “improve” a given cluster that has been obtained by some
other method. In this survey, we focus on principled algorithms for this cluster improvement
problem. Many such cluster improvement algorithms are ﬂow-based methods, by which we
mean that operationally they require the solution of a sequence of maximum ﬂow problems
on a (typically implicitly) modiﬁed data graph. These cluster improvement algorithms are
powerful, both in theory and in practice, but they have not been widely adopted for problems
such as community detection, local graph clustering, semi-supervised learning, etc. Possible
reasons for this are: the steep learning curve for these algorithms; the lack of eﬃcient and
easy to use software; and the lack of detailed numerical experiments on real-world data that
demonstrate their usefulness. Our objective here is to address these issues. To do so, we guide
the reader through the whole process of understanding how to implement and apply these
powerful algorithms. We present a unifying fractional programming optimization framework
that permits us to distill, in a simple way, the crucial components of all these algorithms. It
also makes apparent similarities and diﬀerences between related methods. Viewing these cluster
improvement algorithms via a fractional programming framework suggests directions for future
algorithm development. Finally, we develop eﬃcient implementations of these algorithms in
our LocalGraphClustering Python package, and we perform extensive numerical experiments
to demonstrate the performance of these methods on social networks and image-based data
graphs.

Part I. Introduction and Overview of Main Results

1

Introduction..........................................................................................
1.1 Cluster improvement: compared with graph clustering
1.2 Cluster improvement: compared with seeded graph diﬀusion
1.3 Cluster improvement: compared with image segmentation
1.4 Overview and Summary
1.5 Reproducible Software: the LocalGraphClustering package
1.6 Outline

6

6

4

9

7

9

3

2 Notation, Deﬁnitions, and Terminology........................................................ 10

2.1 Graph notation 10
2.2 Matrices and vectors for graphs 10
2.3 Vector norms 11
2.4 Graph cuts and volumes using set and matrix notation 11
2.5 Relative volume 12
2.6 Cluster quality metrics 12

∗School of Computer Science, University of Waterloo, Waterloo, ON, Canada. E-mail: kfountou@uwaterloo.ca.

KF would like to acknowledge DARPA and NSERC for providing partial support for this work.

†Department of Computer Science, Purdue University, West Lafayette, IN, USA. e-mail: liu1740@purdue.edu.
‡Department of Computer Science, Purdue University, West Lafayette, IN, USA. e-mail: dgleich@purdue.edu.
DFG would like to acknowledge NSF IIS-1546488, CCF-1909528, the NSF Center for Science of Information STC,
CCF-0939370, DOE DE-SC0014543, NASA, and the Sloan Foundation for partial support for this work.

§ICSI and Department of Statistics, University of California at Berkeley, Berkeley, CA, USA. E-mail: mma-
honey@stat.berkeley.edu. MWM would like to acknowledge ARO, DARPA, NSF, ONR, Cray, and Intel for
providing partial support for this work.

 
 
 
 
 
 
2

2.7

Strongly and weakly local graph algorithms 13

3 Main Theoretical Results: Flow-based Cluster Improvement and Fractional Pro-

gramming Framework.............................................................................. 14
3.1 Cluster improvement objectives and their properties 14
3.2 The basic fractional programming problem 17
3.3 Fractional programming for cluster improvement 17
3.4 Dinkelbach’s algorithm for fractional programming 18
3.5 A faster version of Dinkelbach’s algorithm via root ﬁnding 21
3.6 The algorithmic components of cluster improvement 22
3.7 Beyond conductance and degree weighted nodes 22

4 Cluster Improvement, Flow-based, and Other Related Methods......................... 23

4.1 Graph and mesh partitioning in scientiﬁc computing 24
4.2 The nature of clusters in sparse relational data and complex systems 24
4.3 Local graph clustering, community detection, and metadata inference 25
4.4 Conductance optimization 26
4.5 Network ﬂow-based computing 26
4.6 Recent progress on network ﬂow algorithms 27
4.7 Continuous and inﬁnite dimensional network ﬂow and cuts 27
4.8 Graph cuts and max ﬂow-based image segmentation 27

Part II. Technical Details Underlying the Main Theoretical Results

5 Minimum Cut and Maximum Flow Problems ................................................ 28

5.1 MinCut 28
5.2 Network Flow and MaxFlow 29
5.3 From MaxFlow to MinCut 31
5.4 MaxFlow solvers for weighted and unweighted graphs 32

6 The MQI Problem and Algorithm .............................................................. 32

Solving the MQI subproblem using MaxFlow algorithms 33
Iteration complexity 35

6.1
6.2
6.3 A faster version of the MQI algorithm 36

7 The FlowImprove Problem and Algorithm.................................................... 36

Iteration complexity 39

7.1 The FlowImprove subproblem 37
7.2
7.3 A faster version of the FlowImprove algorithm 39
7.4 Non-locality in FlowImprove 40
7.5 Relationship with PageRank 42

8 The LocalFlowImprove (and SimpleLocal) Problem and Algorithm..................... 42

Strongly Local Constructions of the Augmented Graph 45

8.1
8.2 Blocking Flow 47
8.3 The SimpleLocal subsolver 48
8.4 More sophisticated subproblem solvers 48

Part III. Empirical Performance and Conclusion

9 Empirical Evaluation ............................................................................... 49

9.1 Flow-based cluster improvement algorithms reduce conductance 50
9.2 Finding nearby targets by growing and shrinking 54
9.3 Using ﬂow-based algorithms for semi-supervised learning 55
9.4
Improving thousands of clusters on large scale data 58
9.5 Using ﬂow-based methods for local coordinates 60

10 Discussion and Conclusion ........................................................................ 61

Part IV. Replicability Appendices and References

A Replicability Details for Figures and Tables .................................................. 64
B Converting Images to Graphs .................................................................... 67

3

Part I. Introduction and Overview of Main Results

Introduction

1
Clustering is the process of taking a set of data as input and returning meaningful groups of
that data as output. The literature on clustering is tremendously and notoriously extensive (von
Luxburg, Williamson, and Guyon, 2012; Ben-David, 2018); see also comments by Hand in the
discussion of Friedman and Meulman (2004). It can seem that nearly every conceivable perspective
on the clustering problem—from statistical to algorithmic, from optimization-based to information
theoretic, from applications to formulations to implementations—that could be explored, has
been explored. Applications of clustering are far too numerous to discuss meaningfully, and they
are often of greatest practical interest for “soft” downstream objectives such as those common in
Exploratory Data Analysis. Yet, despite comprehensive research into the problem, there are still
useful and surprising new results on clustering discovered on a regular basis (Kleinberg, 2002;
Ackerman and Ben-David, 2008; Awasthi et al., 2015; Abbe, 2018).

Graph clustering is a special instance of the general clustering problem, where the input
is a graph, in this case, a set of nodes and edges, and the output is a meaningful grouping of
the graph’s nodes. The ubiquity of sparse relational data from internet-based applications to
biology, from complex engineered systems to neuroscience, as well as new problems inspired by
these domains (Newman, 2010; Easley and Jo, 2010; Brandes and Erlebach, 2005; Estrada and
Higham, 2010; Traud et al., 2011; Grindrod and Higham, 2013; Liberti et al., 2014; Bienstock,
Chertkov, and Harnett, 2014; Jia et al., 2015; Bertozzi and Flenner, 2016; Estrada and Hatano,
2016; Rombach et al., 2017; Fosdick et al., 2018; Fennell and Gleeson, 2019; Shi, Altaﬁni, and
Baras, 2019; Ehrhardt and Wolfe, 2019), has precipitated a recent surge of graph clustering
research (Newman, 2006; Leskovec et al., 2009; Eckles, Karrer, and Ugander, 2017).
For
instance, in graph and network models of complex systems, the community detection or module
detection problem is a speciﬁc instance of the graph clustering problem, in which one seeks to
identify clusters that exhibit relationships distinctly diﬀerent from other parts of the network.
Consequently, there are now a large number of tools and techniques that generate clusters from
graph data.

The tools and techniques we study in this survey arise from a diﬀerent and complementary
perspective. As such, they are designed to solve a diﬀerent and complementary problem. The
clustering problem itself is somewhat ill-deﬁned, but the way one often applies it in practice is
while performing exploratory data analysis. That is, one uses a clustering algorithm to “play
with” and “explore” the data, tweaking the clustering to see what insights about the data are
revealed. Motivated by this, and the well-known fact that the output of even the best clustering
algorithm is typically imperfectly suited to the downstream task of interest (for example Carrasco
et al. (2003) mentions “neither [. . . ] seems to yield really good [. . . ] clusterings of our dataset, so
we have resorted to hand-built combinations”), we are interested in tools and techniques that
seek to improve or reﬁne a given cluster—or more generally a representative set of vertices—in a
fashion that is computationally eﬃcient, that yields a result with strong optimality guarantees,
and that is useful in practice.

Somewhat more formally, here is the cluster improvement problem: given a graph G = (V, E)
and a subset of vertices R that serve as a reference cluster (or seed set), ﬁnd a nearby set S that
results in an improved cluster. That is,

when given as input a graph G = (V, E) and a set R
a cluster improvement algorithm returns a set S
where S is in some sense “better” than R.

⊂
⊂

V ,
V ,

A very important point here is that both G and R are regarded as input to the cluster improvement
problem. This is diﬀerent from more traditional graph clustering, which typically takes only G
as input, and it is a source of potential confusion. See Figure 1, which we explain in depth in
Section 1.1, for an illustration.

4

How to choose the set R, which is part of the input to a cluster improvement algorithm, is an
important practical problem (akin to how to construct the input graph in more traditional graph
clustering). It depends on the application of interest, and we will see several examples of it.

In the settings we will investigate in this survey, we will be (mainly) interested in graph
conductance (which we will deﬁne in Section 2.6 formally) as the cluster quality metric. Thus,
the optimization goal will be to produce a set S with smaller (i.e., better) conductance than R.
Generally speaking, a set of small conductance in a graph is a hint towards a bottleneck revealing
an underlying cluster. While we focus on conductance, the techniques we review are more general
and powerful. For example, these ideas, algorithms, and approaches can be adapted to other
graph clustering objectives such as ratio-cut (Lang and Rao, 2004), normalized-cut (Hochbaum,
2013), and other closely related “edge counting” objective functions and scenarios (Veldt, Klymko,
and Gleich, 2019; Veldt, Wirth, and Gleich, 2019). We return to the utility of conductance as an
objective function to improve clusters, even those output from related objectives and algorithms,
via an example in Section 1.1.

We deﬁne the precise improvement problems via optimization in subsequent sections. For now,
we treat them as black-box algorithms to explain how they might be used. These introductory
examples use one of two algorithms, MQI (Lang and Rao, 2004) and LocalFlowImprove (Orecchia
and Zhu, 2014), that we will study in depth. Both of these cluster improvement algorithms
execute an intricate sequence of max-ﬂow or min-cut computations on graphs derived from G
and R. A technical diﬀerence with important practical consequences is the following:

MQI always returns a set S of exactly optimal conductance
contained within the reference cluster R; whereas
LocalFlowImprove ﬁnds an improved cluster S with

conductance at least as good as that found by MQI,
by both omitting vertices of R and adding vertices outside R.

In addition to these two algorithms, we will also discuss in depth the FlowImprove (Andersen
and Lang, 2008) method.

1.1 Cluster improvement: compared with graph clustering
To start, consider Figure 1, in which we consider a synthetic graph model called a stochastic block
model. In our instance of the stochastic block model, we plant 5 clusters of 20 vertices. Edges
between vertices in the same cluster occur at random
with probability 0.3. Edges between vertices in diﬀerent
clusters occur at random with probability 0.0157. A
popular algorithm for graph clustering is the Louvain
method (Blondel et al., 2008). On this problem input
instance, running the Louvain method often produces a
clustering with a small number of errors (Aside 1). By
using the LocalFlowImprove algorithm on each cluster
returned by Louvain, we can directly reﬁne the clusters
output by the Louvain method (i.e., we can choose our input set R to be the output of some
other method). This example involves running the improvement algorithm one time for each
cluster returned by the Louvain method. Doing so results in a perfectly accurate clustering for
this instance. That said, the Louvain method is designed to partition the dataset and insists on a
cluster for each node, whereas improving each cluster may result in some vertices unassigned to a
cluster or assigned to multiple clusters. Although this does not occur in this instance on the block
model, it ought to be expected in general. There are a variety of ways to address this diﬀerence
in output given the domain speciﬁc usage. For instance, to reobtain a partition, one can create
clusters of unassigned vertices and pick a single assignment out of the multiple assignments based
on problem or application speciﬁc criteria.

Aside 1. For this particular example,
there are ways of getting a completely ac-
curate answer that involve re-running the
Louvain method or tweaking parameters.
Our point is simply that we can easily
improve existing clustering pipelines with
ﬂow-based improvement methods.

For this example, we’d like to highlight the diﬀerence in objective functions between the
modularity measure optimized by the Louvain algorithm and the conductance measure optimized

Graph clustering

5

(a) Input is a graph; this one has 5 planted
clusters.

(b) Output is a cluster for each node; we high-
light mistakes for the 5 groups.

Cluster improvement

(c) Input is a graph and a seed set of nodes.

(d) Output is an improved set of nodes.

Figure 1 – Graph clustering (known as community detection in some areas) is a problem where the input is a graph
and the output is a labeling or partition indicator for each node, indicating the group/cluster to which each node
belongs. This is illustrated in (a) and (b). Cluster improvement is diﬀerent problem. In cluster improvement
problems, the input is both a graph and a set of nodes, and the output is a set of nodes that is improved in some
sense. As an example, in (c), we show the input as the same graph from (a) along with one of the groups from (b)
that has a few mistakes. The result of cluster improvement in (d) has no mistakes. See replication details in the
appendix.

by LocalFlowImprove. Despite diﬀerences in these objectives (modularity compared with conduc-
tance), many clustering objective functions are related in their design to balance boundary and
size tradeoﬀs (i.e. isoperimetry). Consequently, exactly or optimally improving a related objective
is likely to result in beneﬁts to nearby measures. Moreover, conductance and modularity are
indeed close cousins as established either by how they make cut and volume tradeoﬀs (Gleich and
Mahoney, 2016) or by relationships with Markov stability (Delvenne, Yaliraki, and Barahona,
2010). Thus, it is not surprising that LocalFlowImprove is able to assist Louvain, despite the
diﬀerence in objectives. (Let us also note that ﬂow-based algorithms can be designed around a
variety of more general objective functions as well, see, Section 3.7.) Thus this example mixes
pieces that commonly arise in real-world uses: (i) the end goal (ﬁnd the hidden structures), (ii)
an objective function formulation of a related goal (optimize modularity), and (iii) an algorith-
mic procedure for that task (Louvain method). Given the output from (iii), the improvement
algorithms produce an exactly optimal solution to a nearby problem that (in this case) captures
exactly the true end goal (i).

6

1.2 Cluster improvement: compared with seeded graph diﬀusion
Another common scenario in applied work with graphs is what we will call a target identiﬁcation
problem. In this setting, there is a large graph and we are given only one, or a very small number
of vertices, from a hidden target set. See Figure 2(a) for an illustration. Seeded graph diﬀusions
are a common technique for this class of problems. In a seeded graph diﬀusion, the input is a
seed node s and the output is a set of nearby graph vertices related to s (Zhu, Ghahramani, and
Laﬀerty, 2003; Faloutsos, McCurley, and Tomkins, 2004; Zhou et al., 2004; Tong, Faloutsos, and
Pan, 2006; Kloumann and Kleinberg, 2014). Arguably, the most well-known and widely-applied
of these seeded graph methods is seeded PageRank (Andersen, Chung, and Lang, 2006; Gleich,
2015). In essence, seeded PageRank problems identify related vertices as places where a random
walk in the graph is likely to visit when it is frequently restarted at s.

Cluster improvement algorithms are diﬀerent than but closely related to seeded graph diﬀusion
problems. This relationship is both formal and applied. It is related in a formal (and obvious)
sense because seeded PageRank and its relatives correspond to an optimization problem that
will also provably identify sets of small conductance (Andersen, Chung, and Lang, 2006). It is
related in an applied sense for the following (important, but initially less obvious) reason: the
improvement methods we describe are excellent choices to reﬁne clusters produced by seeded
PageRank and related Laplacian-based spectral graph methods (Lang, 2005; Fountoulakis et al.,
2017; Veldt, Gleich, and Mahoney, 2016). The basic reason for this is that spectral methods often
exhibit a “leak” nearby a boundary. For instance, if a node at the boundary of an idealized target
cluster is visited with a non-trivial probability from a random walk, then neighbors will also be
visited with non-trivial probability. In particular, this means that such spectral methods tend to
output clusters with larger conductance, more false positives (in terms of the target set), and
sometimes fewer true positives as well.

An illustration of this leaking out of a spectral method is given in Figure 2. Here, we are
using the algorithms to study a graph with a planted target cluster of 72 vertices in the center of
a much larger 3000 node graph. If we run a seeded PageRank algorithm from a node nearby the
boundary of the target, then the result set expands too far beyond the target cluster (Figure 2(b)).
If we then run the MQI cluster improvement method on the output of seeded PageRank, then we
accurately identify the target cluster alone (Figure 2(c)). Likewise, if we simply expand the seed
node into a slightly larger set by adding all of the seed’s neighbors, and we then perform a single
run of the LocalFlowImprove method, then we will accurately identify this set.

1.3 Cluster improvement: compared with image segmentation
Our ﬁnal introductory example is given in Figure 3, and it illustrates these improvement algorithms
in the context of image segmentation. Here, an input image is translated into a weighted graph
through a standard technique. The goal of that technique is to ensure that similar regions of
the image appear as clusters in the resulting graph; this standard process is described formally
in Appendix B. On this graph representing an image, the target set identiﬁcation problem from
Section 1.2 yields an eﬀective image segmentation procedure, albeit with a much larger set of
seed nodes.

We focus on the face of the astronaut Eileen Marie
Collins (a retired NASA astronaut and United States
Air Force colonel) (Wikipedia, 2021) as our target set.
Figure 3(a) shows a superset of the face. When given as
the input set to the MQI cluster improvement method
(which, recall, always returns a subset of the input), the
result closely tracks the face, as is shown in Figure 3(b).
Note that there are still a small number of false positives
around the face—see the region left of the neck below the ear—but the number of false positives
decreases dramatically with respect to the input. Similarly, when given a subset of the face, we
can use LocalFlowImprove (which, recall, can expand or contract the input seed set) to ﬁnd most
of it. We present in Figure 3(c) the input cluster to LocalFlowImprove, which is clearly a subset

Aside 2. These image segmentation ex-
amples are used to illustrate properties
of the algorithms that are diﬃcult to vi-
sualize on natural graphs. They are not
intended to represent state of the art seg-
mentation procedures.

7

(a) Our target and a seed node (orange).

(b) The seeded PageRank result (red).

(c) MQI-based improvement (red) of the seeded PageR-
ank result set (inset orange nodes)

(d) LocalFlowImprove result (red) on a one-step neigh-
borhood of the seed (inset orange nodes)

Figure 2 – Cluster improvement with MQI (Lang and Rao, 2004) and LocalFlowImprove (Orecchia and Zhu, 2014)
on a large graph. We show a piece of a larger graph with a target cluster in the middle of (a) and an expanded view
of the target and seed in the inset of (a). If we run a seeded PageRank-based method to search for a cluster nearby
the seed, then the result leaks out into the rest of the graph and fails to capture the boundary of the cluster, as
shown in (b). If, using the seeded PageRank result as the reference set R (shown in orange in the inset of (c)), we
run MQI, then we accurately identify the target in (c) in red. Likewise, if, using the one-step neighborhood of the
seed as R (shown in orange in the inset of (d)), we run LocalFlowImprove, then we also accurately identify the
target (d) in red. See Appendix A for details.

of the face; and the output cluster for LocalFlowImprove is shown in Figure 3(d), which again
closely tracks the face with a few false negatives around the mouth.

1.4 Overview and Summary
One challenge with the ﬂow-based cluster improvement literature is that (so far) it has lacked the
simplicity of related spectral methods and seeded graph diﬀusion methods like PageRank (Gleich,
2015; Zhu, Ghahramani, and Laﬀerty, 2003; Faloutsos, McCurley, and Tomkins, 2004; Zhou et al.,
2004; Tong, Faloutsos, and Pan, 2006; Kloumann and Kleinberg, 2014). These spectral methods

8

(a) Input to MQI.

(b) Output of MQI.

(c) Input to LocalFlowImprove.

(d) Output of LocalFlowImprove.

Figure 3 – Illustration of cluster improvement with MQI (Lang and Rao, 2004) and LocalFlowImprove (Orecchia and
Zhu, 2014) on an image. In Figure 3(a), we show the input set of nodes to MQI. The set of nodes consists of the
pixels inside the yellow square. Note that MQI looks for good clusters within the input square, and the target cluster
is the face of Eileen Marie Collins (a retired NASA astronaut and United States Air Force colonel) (Wikipedia,
2021). In Figure 3(b), we show the output, which demonstrates that MQI-based cluster improvement decreases
the number of false positives. In Figure 3(c), we show the input set of nodes to LocalFlowImprove. The set of
nodes consists of the pixels inside the yellow square. Note that LocalFlowImprove looks for good clusters around the
region of the input square and the target cluster is the face of the Eileen Marie Collins. In Figure 3(d), we show
the output, which demonstrates that LocalFlowImprove-based cluster improvement increases the number of true
positives. See Appendix A for details.

are often easy to explain in terms of random walks, Markov chains, linear systems, and intuitive
notions of diﬀusion. Instead, the ﬂow-based literature involves complex and seemingly arbitrary
graph constructions that are then used, almost like magic (at least to researchers and downstream
scientists not deeply familiar with ﬂow-based algorithms), to show impressive theoretical results.
Our goal here is to pull back the curtain on these constructions and provide a uniﬁed framework
based on a class of optimization methods known as fractional programming.

The connection between ﬂow-based local graph clustering and fractional programming is
not new, e.g., Lang and Rao (2004) cite one relevant paper (Gallo, Grigoriadis, and Tarjan,
1989). Both Lang and Rao (2004) and Andersen and Lang (2008) mention binary search for
ﬁnding optimal ratios akin to root-ﬁnding. Hochbaum (2010) was the ﬁrst to develop a general
framework of root-ﬁnding algorithms for global ﬂow-based fractional programming problems.
However, specialization of these results to the FlowImprove problem require special treatment
which is not discussed in (Hochbaum, 2010). That said, our purpose in using these connections is

9

that they make the methods simpler to understand. Thus, we will make the connection extremely
clear, and we will demonstrate that our fractional programming optimization perspective uniﬁes
all existing ﬂow-based cluster improvement methods. Indeed, it is our hope that this perspective
will be used to develop new theoretically-principled and practically useful methodologies.

1.5 Reproducible Software: the LocalGraphClustering package
In addition to the detailed and uniﬁed explanation of the ﬂow-based improvement methods, we
have implemented these algorithms in a software package with a user-friendly Python interface.
The software is called LocalGraphClustering (Fountoulakis et al., 2019b) (which, in addition to
implementing ﬂow improvement methods that we review here we implement spectral diﬀusion
methods for clustering, methods for multi-label classiﬁcation, network community proﬁles and
network drawing methods). As an example of using this package, running the seeded PageRank
followed by MQI for the results shown in Figure 2 is as simple as:

import localgraphclustering as lgc
G = lgc.GraphLocal("geograph-example.edges")
seed = 305
R,cond = lgc.spectral_clustering(G,[seed],method=’l1reg’) # seeded PageRank
S,cond = lgc.flow_clustering(G,R,method=’mqi’) # improve with MQI

# load the package
# load the graph
# set the seed and compute

This software also enables us to explore a number of interesting applications of ﬂow-based cluster
improvement algorithms that demonstrate uses beyond simply improving the conductance of
sets. The implementation of the methods scales to graphs with billions of edges when used
appropriately. In this survey, we explore graphs with up to 117 million edges (Section 9.4).

This package is useful generally. For reproducibility we also provide code that reproduces all

the experiments that are presented in this survey.

1.6 Outline
There are three major parts to our survey; and these are designed to be relatively modular
to enable one to read parts (e.g., to focus on the theoretical results or the empirical results)
separately.

In the ﬁrst part, we introduce the fundamental concepts and techniques, both informally as
in this introduction and formally through our notation (Section 2) and fractional programming
sections (Section 3). In particular, we introduce graph cluster metrics such as conductance
in Section 2.6. We also introduce fundamental ideas related to local graph computations in
Section 2.7, which discusses the distinction between strongly and weakly local graph algorithms.
These ideas are then used to explain the precise objective functions and settings for ﬂow-based
cluster improvement algorithms in Section 3. This part continues with an overview of how these
methods ﬁt into the broader literature of graph-based algorithms (Section 4), and it includes a brief
discussion of other scenarios where max-ﬂow and min-cut algorithms are used as a fundamental
computational primitive (Section 4.5), as well as inﬁnite dimensional analogues to these ideas
(Section 4.7). We also include a number of ideas that show how the methods generalize beyond
using conductance.

In the second part, we provide the technical core of the survey. We begin our description of
the details of the methods with a review of concepts from minimum ﬂow and maximum cuts
(Section 5). In particular, this section has a careful derivation of these problems as duals in terms
of linear programs. The next three sections, Sections 6 to 8, cover the three algorithms that
we use in the experiments: MQI, FlowImprove, and LocalFlowImprove. For each algorithm, we
provide a thorough discussion on how to deﬁne each step of the algorithm. On a high level, these
algorithm require at each iteration the solution of a max-ﬂow problem. However, to actually
implement these methods one requires construction of a locally modiﬁed version of the given
graphs.

In the ﬁnal part, we provide an extensive empirical evaluation and demonstration of these
algorithms (Section 9). This is done in the context of a number of datasets where it is possible
to illustrate clearly and easily the beneﬁts of these techniques. Examples in this evaluation

10

include images, as we saw in the introduction, as well as road networks, social networks, and
nearest neighbor graphs that represent relationships among galaxies. This section also includes
experiments on graphs with up to 117 million edges. We also describe strategies to generate local
network visualizations from these local graph clustering methods that highlight characteristic
diﬀerences in how the ﬂow-based methods treat networks.

In addition, we provide an appendix with full reproducibility details for all of the ﬁgures
and tables (Appendices A and B). These include references to speciﬁc Python notebooks for
replication of the experiments.

2 Notation, Deﬁnitions, and Terminology
We begin by reviewing speciﬁc mathematical assumptions, notation, and terminology that we
will use. To start, we use the following standard notations:

Z
R
R+
Rn
Rn
Rn
+
Rn
+

n

×

n

×

denotes the set of integer numbers,
denotes the set of real-valued numbers,
denotes the set of real-valued non-negative numbers,
denotes the set of real-valued vectors of length n,
denotes the set of real-valued n
×
denotes the set of real-valued non-negative vectors of length n, and
denotes the set of real non-negative n

n matrices.

n matrices,

×

2.1 Graph notation
Given a graph G = (V, E), we let V denote the set of nodes and E denote the set of edges.
We assume an undirected, weighted graph throughout, although some of the constructions and
concepts involved in a ﬂow computation are often best characterized through directed graphs.
(See also Aside 3.) For an unweighted graph, everything we do will be equivalent to assigning an
edge weight of 1 to all edges. Also, we also assume that the given graphs have no self-loops.

1, 2, . . . , n
{

The cardinality of the set V is denoted by n, i.e.,
there are n nodes, and we assume that the nodes are
arbitrarily ordered from 1 to n. Therefore, we can write
. We use vi to denote node i, and
V :=
when it is clear, we will use i to denote that node. We
assume that the edges E in the graph are arbitrarily
ordered. The cardinality of the set E is denoted by m,
i.e., there are m edges. We will use eij to denote an edge.
Also, if a node j is a neighbor of node i, we denote this
relationship by j

i.

}

Aside 3. Our techniques would extend
to any clustering function on directed
graphs that deﬁnes a hypergraph using
techniques from Benson, Gleich, and
Leskovec (2016) based on motif enumera-
tion. For adaptations of these techniques
to hypergraphs, see Veldt, Benson, and
Kleinberg (2020a,b) for some examples.

∼

A path is a sequence of edges which connect a sequence of distinct vertices. A connected
component is a subset of nodes such that there exists a path between any pair of nodes in that
subset.

⊆

V , formally, ¯S =

We frequently work with subsets of vertices. Let S
V

V , for example. Then ¯S denotes the
complement of subset S
. The notation ∂S represents
v /
v
}
∈
{
the node-boundary of the set S; formally, it denotes the set of nodes that are in ¯S and are
connected with an edge to at least one node in S. In set notation, we have ∂S =
¯S, and there exists (u, v)
S
}
2.2 Matrices and vectors for graphs
Here, we deﬁne matrices that can be used to deﬁne models and objective functions on graph data.
They can also provide a compact way to understand and describe algorithms that operate on
graphs.

v, where v
{

E with u

⊆
|

∈

∈

∈

∈

S

.

The adjacency matrix A

if the graph is weighted) provides perhaps
Rn
+
the most simple representation of a graph using a matrix. In A, row i corresponds to node i in
the graph, and element Aij is non-zero if and only if nodes i and j are connected with an edge
in the given graph. The value of Aij is the edge weight for a weighted graph, or simply 1 for

0, 1
}

n (or

∈ {

∈

×

×

n

n

11

The diagonal weighted degree matrix D

an unweighted graph. Since we are working with undirected graphs, the adjacency matrix is
symmetric, i.e., Aij = Aji, where Aij is the element at the ith row and jth column of matrix A.
if the graph is weighted) is a
Zn
+
matrix that stores the degree information for every node. The element Dii is the sum of weights
of the edges of node i, i.e., Dii := (cid:80)
= j,
equal zero.

∈
i Aij; and oﬀ-diagonal elements, i.e., Dij, for i

Rn
+

(or

V :j

∈

×

×

∼

∈

n

n

j

The degree vector is deﬁned as d = diag(D), where diag(

) takes as input a vector or a matrix
·
and returns, respectively, a diagonal matrix with the vector in the diagonal or a vector with
diagonal elements of a matrix.

−

1, 1

∈ {

m
}

The edge-by-node incidence matrix B

n (where, recall, n is the number of
0,
×
nodes, and m is the number of edges) is often used to measure diﬀerences among nodes. Each
row of this matrix represents an edge, and each column represents a node. For example, row k
in B represents the kth edge in the graph (arbitrarily ordered) that corresponds (say) to nodes
i and j in the graph. Row k in B then has exactly two nonzero elements;
1 for the source of
the edge and 1 for the target of the edge, at the i and j position, respectively. If the graph is
undirected, then we can arbitrarily choose which node is the source and which node is the target
on an edge, without loss of generality. Note that because we assume no self-loops, the incidence
matrix contains the full information about the edges of the graph.

−

The diagonal edge-weight or edge-capacity matrix C

is a diagonal matrix where each
diagonal element corresponds to the weight of an edge in the graph. This matrix is the identity
for an unweighted graph. For example, the kth diagonal element corresponds to weight of the
kth edge in the graph.

Rm
+

∈

m

×

The Laplacian matrix L
or equivalently L = BT CB.

∈

∈

Zn

n (or

×

Rn

n if the graph is weighted) is deﬁned as L = D
×

A

−

Vectors of all-ones and all-zeros, denoted 1n and 0n, respectively, are column vectors of length
n. If the dimensions of each vector will be clear from the context, then we omit the subscript.
The indicator vector 1i is a column vector that is equal to 1 at the ith index and zero elsewhere.
If the indicator is used with a node, then the length of the vector 1i is n. For an edge, its length
is m.

x
(cid:107)

(cid:107)1 = (cid:80)
i |

If S is a subset of nodes or a subset of indices and A is any matrix, e.g., the adjacency matrix,
then AS is a submatrix of A that corresponds to the rows and columns with indices in S. Likewise,
1S is a column vector with ones in entries for S. These indicator vectors have length n.
2.3 Vector norms
We denote the vector 1-norm by
i(xi)2 We will
xi|
use these norms to measure diﬀerences among nodes that are represented in a vector x, i.e.,
(cid:107)1 = (cid:80)
is the
every node corresponds to an element in vector x. For example,
xi −
sum of diﬀerences among node representations in x. In the case of weighted graphs, this can be
generalized to
. For the 2-norm, we have
xj|
C,2 = (cid:80)
2
(cid:107)

(cid:107)
2.4 Graph cuts and volumes using set and matrix notation
Much of our discussion will ﬂuidly move between set-based descriptions and matrix-based
descriptions. Here, we give a simple example of how this works in terms of a graph cut and
volume of a set.

(cid:107)C,1 = (cid:80)
Bx
E Ceij (xi −

= (cid:80)
xj|
E Aij(xi −

E Ceij |
eij
xj)2 = (cid:80)

and the 2-norm by

(cid:107)2 = (cid:112)(cid:80)

(cid:107)
xi −

eij
∈
xj)2.

E Aij|

xi −

xj|

E |
∈

(cid:107)
eij

Bx

Bx

eij

eij

x

(cid:107)

∈

∈

∈

Graph cut We say that a pair of complement sets (S, ¯S), where S
V , is a global graph
partition of a given graph with node set V . Given a partition (S, ¯S), the cut of the partition is
the sum of weights of edges between S and ¯S, which can be denoted by either

⊆

cut(S, ¯S) =

(cid:88)

Aij,

or

cut(S) =

(cid:88)

Aij.

S,j

i

∈

∈

¯S

i

S,j

∈

¯S

∈

(2.1)

(cid:54)
12

Instead of using set notation to denote a partition of the graph, i.e., (S, ¯S), we can use indicator
vector notation x = 1S ∈ {

n to denote a partition. In this case, the cut of the partition is

0, 1
}

cut(S, ¯S) =

Aij|

xi −

xj|

=

B1S(cid:107)C,1.

(cid:107)

(2.2)

(cid:88)

i,j

Note that both expressions are symmetric in terms of S and ¯S.
Graph volume The volume of a set of nodes S is equal to the sum of the degrees of all nodes
in S, i.e.,

vol(S) =

(cid:88)

di.

(2.3)

i

S

∈

We will use the notation vol(G) to denote the volume of the graph, which is equal to vol(V ).
Using this deﬁnition and our matrix deﬁnitions above, we have that the volume of a subset of
nodes is vol(S) = 1T
2.5 Relative volume
FlowImprove and LocalFlowImprove formulations are simpler to explain by introducing the idea
of relative volume. The relative volume of S with respect to R and κ is

S d.

rvol(S; R, κ) = vol(S

R)

κ vol(S

∩

−

∩

¯R).

(2.4)

The relative volume is a very useful concept that we will use to deﬁne the objective functions of
the local ﬂow-based problems, MQI, FlowImprove and LocalFlowImprove. The purpose of the
relative volume is to measure the volume of the intersection of S with the input seed set nodes R,
while penalizing the volume of the intersection of S with the complement ¯R. This is important
when we deﬁne the objective functions of MQI, FlowImprove and LocalFlowImprove, since we
want to penalize sets S that have little intersection with R and high intersection with ¯R. This
makes sense, since in local ﬂow-based improve methods the goal is often to improve the input set
R, thus we want the output S of a method to be “related” to R more than ¯R.
2.6 Cluster quality metrics
Here, we discuss scores that we use to evaluate the quality of a cluster. For all of these measures,
smaller values correspond to better clusters, i.e., correspond to a cluster of higher quality.
Conductance The conductance function is deﬁned as the ratio between the number of edges
that connect the two sides of the partition (S, ¯S) and the minimum “volume” of S and ¯S:

φ(S) =

cut(S)
min(vol(S), vol( ¯S))

.

A set of minimal conductance is a fundamental bottleneck in a graph. For example, small
conductance in a set is often interpreted as an information bottleneck revealing community or
module structure, or (relatedly) as a bottleneck to the mixing of random walks on the graph.
Note that conductance values are always between 0 and 1, and they can be interpreted as a
probability. (Formally, this is the probability that random walk moves between S and ¯S in a
single prescribed step after the walk has fully mixed.)

Normalized Cuts The normalized cut function is a related notion that provides a score that
is often used in image segmentation problems (Shi and Malik, 2000), where a graph is constructed
from a given image and the objective is to partition the graph in two or more segments. In the
case of a bi-partition problem, the normalized cuts score reduces to:

ncut(S) =

cut(S)
vol(S)

+

cut( ¯S)
vol( ¯S)

.

The normalized cuts and conductance scores are related, in that φ(S)
2φ(S). There
is a related concept, called ncut’ (Sharon et al., 2006; Hochbaum, 2010) that just measures the
cut to volume ratio for a single set ncut’(S) = cut(S)/vol(S). Observe that this is equal to φ(S)
for any set with less than half of the volume.

ncut(S)

≤

≤

13

Expansion The expansion function or expansion score is deﬁned as the ratio between the
number of edges that connect the two sides of the partition (S, ¯S) and the minimum “size” of S
and ¯S:

˜φ(S) =

cut(S)
¯S
,
S
|
|

|

.

)

|

min(

Compared to the conductance score, which uses the vol-
ume (related to number of edges) of the sets S and ¯S in
the denominator, the expansion score counts the number
of nodes in S or ¯S. This has the property that the expan-
sion score is less aﬀected by high degree nodes. Similarly
to conductance, smaller expansion scores correspond to
better clusters. However, these values are not necessarily between 0 and 1.
Sparsity The sparsity measure of a set is a topic that arises often in theoretical computer
science. It is closely related to expansion, but measures the fraction of edges that exist in the cut
compared to the total possible number

Aside 4. Our deﬁnition of expansion used
here is sometimes used as the deﬁnition
for sparsity. The literature is not entirely
consistent on these terms.

cut(S)
¯S
S
||
|
This value is always between 0 and 1. Also, ˜φ(S)
cut( ¯S)
¯S
|

ψ(S) =

≤

|

|

.

. Hence, sparsity is a scaled measure akin to normalized cut.

nψ(S)

2 ˜φ(S) because nψ(S) = cut(S)
|

S

|

≤

+

Ratio cut The ratio cut function provides a score that is often used in data clustering problems,
where a graph is constructed by measuring similarities among the data, and the objective is to
partition the data into multiple clusters (Hagen and Kahng, 1992). In the case of the bi-partition
problem, the ratio cut score reduces to:

rcut(S) =

cut(S)
S

.

|

|

Observe that the ratio cut and expansion scores are related, in the sense that the latter is equal
to the former if the input set of nodes S has cardinality less than or equal to n/2. The ratio
cut was popularized due to its importance in image segmentation problems (Felzenszwalb and
Huttenlocher, 2004). Usually, this ratio is minimized by performing a spectral relaxation (von
Luxburg, 2007).

2.7 Strongly and weakly local graph algorithms
Local graph algorithms and locally-biased graph algorithms are the “right” setting to discuss
cluster improvement algorithms on large-scale data graphs. For the purposes of this survey, there
are two key types of (related but quite distinct) local graph algorithms:

• Strongly local graph algorithms. These algorithms take as input a graph G and a
reference cluster of vertices R; and they have a runtime and resource usage that only
depends on the size of the reference cluster R (or the output S, but not the size of the
entire graph G).

• Weakly local graph algorithms. These algorithms take as input a graph G and a
reference cluster of vertices R; and they return an answer whose size will depend on R, but
whose runtime and resource usage may depend on the size of the entire graph G (as well as
the size of R).

That is, in both cases, one wants to ﬁnd a good/better cluster near R, and in both cases one
outputs a small cluster S that is near R, but in one case the running time of the algorithm is
independent of the size of the graph G, while in the other case the running time depends on the
size of G. For more about local and locally-biased graph algorithms, we recommend Gleich and
Mahoney (2016); Fountoulakis, Gleich, and Mahoney (2017) and also Mahoney, Orecchia, and
Vishnoi (2012); Lawlor, Budavári, and Mahoney (2016b,a) for overviews.

It is easy to quantify the size of the output S being small; but, in general, the locality of an
algorithm, i.e., how many nodes/edges are touched at intermediate steps, may depend on how the
graph is represented. We typically assume something akin to an adjacency list representation
that enables:

14

• constant time access to a list of neighbors; and
• constant or nearly constant (e.g., O(log

V
|
Moreover, the cost of building this structure is not counted in the runtime of the algorithm, e.g.,
since it may be a one-time cost when the graph is stored. Note that, in addition to a reference
cluster R, these algorithms could take information about vertices in a reference set, such as a
vector of values, as well.

) time access to an arbitrary edge.

|

The importance of these characterizations and this discussion is the following:

for strongly local graph algorithms

the runtime is independent of the size of the graph.

In particular, this means that the algorithm does not even touch all of the nodes of the graph
G. This makes a strongly local graph algorithm an extremely useful tool for studying large data
graphs. For instance, in Figure 2, none of the algorithms used information from more than about
500 vertices of the the total 3000 vertices of the graph, and this result wouldn’t have changed at
all if the entire graph was 3 million vertices (or more as in Shun et al. (2016)).

To contrast with strongly-local graph algorithms, most graph and mesh partitioning tools—and
even the improved and reﬁned variations—are global in nature. In other words, the methods
take as input a graph, and the output of the methods is a global partitioning of the entire graph.
In particular, this means that the methods have running time which depends on the size of the
whole graph. This makes it very challenging to apply these methods to even moderately large
graphs.

3 Main Theoretical Results: Flow-based Cluster Improvement and Frac-

tional Programming Framework

In this section, we will introduce and discuss the fractional programming problem and its
relevance to ﬂow-based cluster improvement. The motivation is that work on cluster improvement
algorithms has thus far proceeded largely on a case-by-case basis; but as we will describe, fractional
programming is a class of optimization problems that provides a way to generalize and unify
existing cluster improvement algorithms.

3.1 Cluster improvement objectives and their properties
For the problem of conductance-based cluster improvement, the three methods we consider exactly
optimize the following objective functions:

MQI: minimize

S

V
subject to S

⊂

cut(S)
vol(S)
R

⊆

FlowImprove: minimize

S

V

⊂

subject to rvol(S; . . .) > 0

cut(S)
rvol(S; R, vol(R)/ vol( ¯R))

LocalFlowImprove(δ)
0
δ
≥

:

cut(S)
rvol(S; R, vol(R)/ vol( ¯R) + δ)

minimize
V
⊂
subject to rvol(S; . . .) > 0

S

The constraint rvol(S; . . .) > 0 simply means that we only consider sets where the denominator is
positive (we omit repeating all the parameters from the denominator for simplicity). Because
we are minimizing over discrete sets, there is not a closure problem with the resulting strict
inequality (rvol(S, . . .) > 0), so these are all well-posed.

Recall that rvol(S; R, κ) = vol(S
S such that rvol(S; R, vol(R)/ vol( ¯R))

κ vol(S

¯R). This deﬁnition implies that sets
R)
0 cannot be optimal solutions for FlowImprove, and

−

∩

∩
≤

15

that even fewer sets can be optimal for LocalFlowImprove. On the other hand, note that
LocalFlowImprove(δ) interpolates between the FlowImprove (δ = 0) and MQI (δ =
) because
¯R) that arises in rvol must be 0 in order for
when δ is suﬃciently large, then the term vol(S
1/ vol( ¯R)) then
the set S feasible for the non-negative rvol constraint. In fact, if δ > vol(R)(1
positive denominators alone will require S

R.

∞

−

∩

To understand better the connections between these three objectives, we begin by stating a
simple property of these objective functions. The following theorem states that conductance gets
smaller, i.e., better, as we move from MQI to LocalFlowImprove to FlowImprove.

⊂

≤

V have
THEOREM 3.1 Let G be an undirected, connected graph with non-negative weights. Let R
vol( ¯R), where ¯R is the complement of R. Let SMQI, SFI, SLFI be the optimal solution
vol(R)
of the MQI, FlowImprove, and LocalFlowImprove(δ) objectives, respectively. If the solutions of
vol( ¯SLFI) (that
FlowImprove and LocalFlowImprove satisfy vol(SFI)
is, the solution set is on the small side of the cut), then for any δ
0 in LocalFlowImprove, we
have that

vol( ¯SFI) and vol(SLFI)

⊂

≥

≤

≤

φ(SFI)

φ(SLFI)

φ(SMQI).

≤

≤

φ(SMQI), is a simple, useful exercise we repeat from Veldt,
Proof The ﬁrst piece, that φ(SLFI)
for any
Gleich, and Mahoney (2016, Theorem 4). Note that if S
κ. Now, note that for all rvol terms in the LocalFlowImprove(δ) objective with δ > 0, we have
vol(R)/ vol( ¯R). Moreover, solutions are constrained to only consider sets where rvol is
κ
positive. Thus, for the value of κ used in LocalFlowImprove, and also any positive κ, we have

R then φ(S) = cut(S)

rvol(S;R,κ)

≤

≥

⊆

φ(SLFI) =

cut(SLFI)
rvol(SLFI; R, κ)
vol(SLFI)
Next, note that for the chosen setting of κ, we have that rvol(S; R, κ) > 0 for all S
we have

cut(SLFI)
vol(SLFI) ≤

κ vol(SLFI

cut(SLFI)

¯R) ≤

−

∩

.

R. Thus,

⊆

φ(SLFI)

≤

minimum

S

R

⊆

cut(S)
rvol(S; R, κ)

= minimum
S
⊆

R

φ(S) = φ(SMQI).

This shows that both LocalFlowImprove and FlowImprove give better conductance sets than
MQI.

For the second piece, we use an alternative characterization of LocalFlowImprove as discussed in
Orecchia and Zhu (2014). LocalFlowImprove(δ) is equivalent to solving the following optimization
problem for some constant C:

minimize
V
⊂
subject to

S

cut(S)
rvol(S;R,vol(R)/vol( ¯R))
vol(S
R)
∩
vol(S) ≥

C, rvol(S; . . .) > 0

while FlowImprove solves the same problem without the constraint involving C. Then we have:

cut(SF I )
rvol(SF I ; R, vol(R)/vol( ¯R)) ≤

cut(SLF I )
rvol(SLF I ; R, vol(R)/vol( ¯R))

cut(SF I )
cut(SLF I ) ≤

rvol(SF I ; R, vol(R)/vol( ¯R))
rvol(SLF I ; R, vol(R)/vol( ¯R))

.

If φ(SF I ) > φ(SLF I ), we have

Thus,

cut(SF I )
cut(SLF I )

>

vol(SF I )
vol(SLF I )

.

rvol(SF I ; R, vol(R)/vol( ¯R))
rvol(SLF I ; R, vol(R)/vol( ¯R)) ≥
If we now substitute the deﬁnition of rvol and vol(S

cut(SF I )
cut(SLF I )

>

vol(SF I )
vol(SLF I )
vol(S

.

(1 + vol(R)/vol( ¯R))
(1 + vol(R)/vol( ¯R))

·

vol(SF I ∩
·
vol(SLF I ∩

R)
R)

−
−

¯R) = vol(S)

∩
vol(R)/vol( ¯R)
vol(R)/vol( ¯R)

−
vol(SF I )
vol(SLF I )

R),

∩

>

vol(SF I )
vol(SLF I )

·
·

Table 1 – Characteristics of the MQI, FlowImprove, and LocalFlowImprove methods.

Method

Strongly local

Explores
beyond R

Easy to
implement

MQI
FlowImprove
LocalFlowImprove (cid:88)

(cid:88)

(cid:88)
(cid:88)

(cid:88)
(cid:88)

Section

Section 6
Section 7
Section 8

16

vol(SF I ∩
vol(SF I )

R)

>

vol(SLF I ∩
vol(SLF I ) ≥

R)

C.

This means that SF I also satisﬁes the additional constraint in the optimization problem of LFI.
But SF I has smaller objective value, which is a contradiction to the fact that SLF I is the optimal
solution of LFI optimization problem.
(cid:4)

Theorem 3.1 would suggest that one should always use FlowImprove to minimize the conduc-
tance around a reference set R, but there are other aspects to implementations which should be
taken into account. The three most important, summarized in Table 1, are described here.

• Locality of algorithm. For strongly local algorithms, the output is a small cluster around
the reference set R and the running time depends only on the size of the output but is
independent of the size of the graph. Only the former is true for weakly local algorithms.
As we will show in the coming sections, both MQI and LocalFlowImprove are strongly local.
This enables both of them to be run quickly on very large graphs, assuming R is not too
large and δ is not too small.

• Exploration properties of algorithm. Some methods “shrink” the input, in the sense
that the output is a subset of the input, while other methods do not have this restriction,
i.e., they can (depending on the input graph and seed set) possibly shrink or expand the
input. This classiﬁcation is particularly useful when we view the methods as a way to
explore the graph around a given set of seed nodes. For example, MQI only explores the
region induced by R, and so it is not suitable for various tasks that involve ﬁnding new
nodes.

• Ease of implementation. A ﬁnal important property of methods regards how easy they
are to implement. MQI and FlowImprove are easy to implement because they rely on
standard primitives like simple MaxFlow computations. This means that one can black-box
max-ﬂow computations by calling existing eﬃcient software packages. For LocalFlowImprove,
however, getting a strongly local algorithm requires a more delicate algorithm. Therefore,
we consider it to be a more diﬃcult algorithm to implement.

As a simple and quick justiﬁcation of the locality property of the solution (which is distinct
from an algorithmic approach to achieve it), note the following simple-to-establish relationship
between δ and the size of the output set for LocalFlowImprove. This was originally used in Veldt,
Klymko, and Gleich (2019) as a small subset of a proof.

LEMMA 3.2 Let G be an undirected, connected graph with non-negative weights. Let S∗ be an
vol( ¯R). Then vol(S∗) <
optimal solution of the LocalFlowImprove objective with vol(R)
(cid:16)

≤

(cid:17)

1 +

vol( ¯R)
vol(R)+δ vol( ¯R)

vol(R).

Proof For simplicity, let σ = vol(R)/ vol( ¯R) + δ. Then because the denominator at any solution
must be positive, we have 0 < vol(S∗ ∩
R), so 0 < (1 + σ) vol(R
vol(S∗ ∩
S∗)
−
follows by substituting the deﬁnition for σ.

¯R). Note that vol(S∗ ∩
R)
−
σ vol(S∗). Thus, vol(S∗) < (1 + 1/σ) vol(R). The result
(cid:4)

¯R) = vol(S∗)

σ vol(S∗ ∩

−

∩

As we will show, all of the algorithms for these objectives ﬁt into a standard fractional
programming framework, which provides a useful way to reason about the opportunities and
trade-oﬀs. An even more general setting for such problems are quotient cut problems that we

discuss in Section 3.7. While they are often described in this literature on a case-by-case basis,
quotient cut problems are all instances of the more general fractional programming class of
problems.

3.2 The basic fractional programming problem
A fractional program is a ratio of two objective functions: N (x) for the numerator and D(x) for
the denominator. It is often deﬁned with respect to a subset S of Rn

17

minimize
x
subject to x

N (x)/D(x)

S

∈

(3.1)

S. Fractional programming is an important branch of nonlinear
where D(x) > 0 for all x
optimization (Frenk and Schaible, 2009). The key idea in fractional programming is to relate
(3.1) to the function

∈

f (δ) = minimize N (x)

δD(x) subject to x

S,

−
which captures the minimum value of the objective function for this minimization problem as a
function of δ. Below, we use “argmin” as the expression for an input or argument that minimizes
δ. Moreover, if N (x)
the problem. Note that f (δ)
and D(x) are linear functions and S is a set described by linear constraints, then f (δ) can be
easily computed by solving a linear program, for instance.

0 if there exists x such that N (x)/D(x)

≤

≤

∈

We now specialize this general framework for cluster improvement. Note that we will continue
to use δ as the ratio between the numerator and denominator instead of as the LocalFlowImprove
parameter until Section 9.

3.3 Fractional programming for cluster improvement
When we consider the objective functions from Sec-
tion 3.1, note that we can translate them into prob-
lems closely related to the fractional programming Prob-
lem (3.1). Let Q
V represent a subset of vertices. For
MQI, this is R itself and for the others, it is just V . Now
let g(S
R represent the denominator terms for
the MQI, FlowImprove, or LocalFlowImprove objectives from Section 3.1. Then, in a fractional
programming perspective on the problems, we are interested in solving the following problem

Aside 5. Most commonly fractional pro-
gramming is deﬁned for subsets of Rn as
the domain. In our case, we use set-based
domains.

Q)

→

⊆

⊆

minimize

φg(S) :=

(cid:40) cut(S)
g(S)

subject to S

∞

Q.

⊆

g(S) > 0
otherwise

(3.2)

Let us assume that there is at least one feasible set S
Q where g(S) > 0. This is satisﬁed for
all the examples above when S = R. Also note that if Q = V and if g(V ) > 0, the entire node
0, and
set V is immediately a solution. For FlowImprove and LocalFlowImprove, though, g(V )
so V is never a solution, and, in fact, the value of κ in FlowImprove is chosen exactly so that
g(V ) = 0.

⊆

≤

As discussed above, we will use a sequence of related parametric problems to ﬁnd the optimal

solution. Thus, we introduce the parametric function

where the parameter δ

R. We also deﬁne the function

z(S, δ) := cut(S)

δg(S),

−

∈
ˆz(δ) := minimize

S

z(S, δ) where S

Q, g(S) > 0.

⊆

(3.3)

Computing the value of ˆz(δ) is a key component that we will discuss in Section 3.6 and also
Sections 6 to 8. Given this, we can consider solving the following equation

ˆz(δ) = 0,

(3.4)

18

which is a simple root ﬁnding problem because ˆz(δ) is monotonically increasing as δ
also ˆz(0)
set S

0 and
0 (for our objectives). Note that ˆz(0) = 0 if cut(S) = 0 for some

≥
Q with g(S) > 0, which can happen for a disconnected graph.

0 and ˆz(φg(R))

→

≤

We now provide a theorem that establishes the relationship between the root ﬁnding Prob-
lem (3.4) and the basic fractional programming Problem (3.2). This theorem establishes that
by solving Problem (3.4) we solve Problem (3.2) as well. A similar theorem can be found
in Dinkelbach (1967).

⊆

THEOREM 3.3 Let G be an undirected, connected graph with non-negative weights. A set of nodes
S∗ is a solution of Problem (3.2) iﬀ

(cid:16) cut(S∗)
g(S∗)

ˆz

(cid:17)

= 0.

Proof For the ﬁrst part of the proof, let us assume that S∗ is a solution of Problem (3.2). This
implies that g(S∗) > 0. We have that

δ∗ :=

cut(S∗)
g(S∗) ≤

φg(S)

for all S

Q, g(S) > 0.

⊆

cut(S∗)

−

δ∗g(S∗) = 0,

Hence,

and

cut(S)

δ∗g(S)

−
≥
min z(S, δ∗)

0 for all S

Q, g(S) > 0.

⊆
Q, g(S) > 0)

Using the above we have that
{
this bound is achieved by S∗. Therefore, ˆz(δ∗) = 0, z(S∗, δ∗) = 0.

⊆

S

}

|

For the second part of the proof, assume that ˆz(δ∗) = 0 such that

is bounded below by zero, and

for some optimal S∗ of the minimization problem in ˆz. Then

δ∗ =

cut(S∗)
g(S∗)

,

(3.5)

cut(S∗)

δ∗g(S∗) = 0

cut(S, ¯S)

δ∗g(S)

for all S

Q, g(S) > 0.

−

−
From the second inequality, we have that φg(S)
Q, g(S) > 0. This means that the
optimal solution of Problem (3.2) is bounded below by δ∗. From the ﬁrst equation above, we get
that this bound is achieved by S∗. Therefore, S∗ solves Problem (3.2).
(cid:4)

S
δ∗ ∀

≤

⊆

⊆

≥

(3.6)

3.4 Dinkelbach’s algorithm for fractional programming
Based on Theorem 3.3, the root of Problem (3.4) will be the optimal value of the general cluster
improvement Problem (3.2). To ﬁnd the root of Problem (3.4), we will use a modiﬁed version of
Dinkelbach’s algorithm (Dinkelbach, 1967).

Dinkelbach’s algorithm Dinkelbach’s algorithm is given in Algorithm 3.1. Note that we had
to modify the original algorithm slightly since we do not assume that g(S) > 0,
Convergence of Dinkelbach’s algorithm We now provide a theorem that establishes that
the subproblem at Step 3 of Algorithm 3.1 does not output infeasible solutions, such as an S that
satisﬁes g(S)
0. Based on this, we can establish that the objective function of Problem (3.2) is
decreased at each iteration of Algorithm 3.1.

Q.

≤

⊆

S

∀

THEOREM 3.4 (Convergence) Let G be an undirected, connected graph with non-negative weights.
Let δ∗ be the optimal value of Problem (3.2). The subproblem in Step 3 of Algorithm 3.1 cannot
0 for δ > δ∗. Such solutions are in the solution set of the
have solutions that satisfy g(S)
subproblem if and only if δ
δ∗. Moreover, the sequence δk, which is set to be equal to φg(Sk),
decreases monotonically at each iteration. The algorithm returns a solution where g(Sk) > 0.

≤

≤

Algorithm 3.1 Dinkelbach’s Algorithm
1: Initialize k := 1, S1 := R and δ1 := φg(S1).
2: while we have not exited via the else clause do
Compute ˆz(δk) by solving Sk+1 := argmin
3:
if g(S)
if φg(Sk+1) < δk (Recall φg(S) =

4:

∞

≤

δk+1 := φg(Sk+1)

else

δk is optimal, return previous solution Sk.

k := k + 1

5:

6:

7:

8:

S z(S, δk) subject to S

0) then

19

Q

⊆

Proof For the ﬁrst part of the theorem, let δ
the sake of contradiction that g( ˆS)
0. Then

≤

0, ˆS

≥

argmin z(S, δ)
}

∈ {

; and let us assume for

Hence,

z(S, δ)

≥

z( ˆS, δ)

0

S

∀

≥

⊆

Q.

argmin z(S, δ)
}

∈ {

φg(S)
however, this can only be true if δ
this implies that g( ˆS) > 0. Therefore, a solution ˆS
δ

≥

≤

S

S

⊂

Q

∈ {

δ
,
∀
δ∗. Otherwise, for δ > δ∗ we have a contradiction, and
satisﬁes g( ˆS) > 0, unless

g(S) > 0
}

|

≤

δ∗.
For the second part of the theorem, let k be such that δk > δ∗. Then, we have that
z(Sk+1, δk) < 0, since z(Sk+1, δk) < z(Sk, δk) = 0 (where we get 0 by the deﬁnition of δk and
Sk). Because z(Sk+1, δk) < 0, we have that φg(Sk+1) = δk+1 < δk = φg(Sk). Note that because
g(Sk+1) > 0 for any δk > δ∗ then we must have δk+1 ≥
Note that because of the algorithm, δk can never be less than δ∗. Thus, the remaining case
is detecting that δk = δ∗. Suppose this is the case and also g(Sk+1) > 0, then δk+1 = δk = δ∗,
and based on Theorem 3.3 the algorithm terminates with an optimal solution because either
0, then the algorithm terminates (because
Sk+1 or Sk are solutions. If δk = δ∗ and g(Sk+1)
). Thus, Sk must have been optimal (if not, then g(Sk+1) must be larger than 0)
φg(Sk+1) =
and so the algorithm outputs an optimal solution.
(cid:4)

δ∗.

∞

≤

Iteration complexity of Dinkelbach’s algorithm The iteration complexity of a method
allows us to deduce a bound on the number of iterations necessary. We now provide an iteration
complexity result for Algorithm 3.1. This involves two results. We begin with Lemma 3.5. This
lemma describes several interesting properties of Algorithm 3.1 which have an important practical
implication. Speciﬁcally, it shows that g(Sk+1) < g(Sk). This result has important practical
implications, since it shows that Algorithm 3.1 is searching for subsets S that have a smaller
value of the function g. Lemma 3.5 will then allow us to prove an iteration complexity result of
Algorithm 3.1 in theorem 3.6. A similar result can be found in Gallo, Grigoriadis, and Tarjan
(1989, Lemma 4.3), but we repeat it in Lemma 3.5 for completeness. In Lemma 3.5, we also show
that the numerator of the objective function in Problem (3.2) decreases monotonically.

LEMMA 3.5 If Algorithm 3.1 proceeds to iteration k + 1, then it satisﬁes both g(Sk+1) < g(Sk)
and cut(Sk+1) < cut(Sk).

Proof Consider iterations k and k
iteration k

−
1, we have that z(Sk, δk

−

1 and assume that δk > δ∗. Then, from Theorem 3.4, in
1) < z(Sk

1) = 0. In iteration k, we have that
−

1, δk

−

−

z(Sk+1, δk) = cut(Sk+1)

δkg(Sk+1) < 0.

−

By adding and subtracting δk

1g(Sk+1) to the latter, we get
−

z(Sk+1, δk) = cut(Sk+1)

δk

−

−

1g(Sk+1) + δk

1g(Sk+1)

−

−

δkg(Sk+1) < 0.

20

Note that the ﬁrst two terms on the right side of the equality are the minimization problem for
that gave the solution Sk. Hence, we can lower-bound cut(Sk+1)

1g(Sk+1) via Sk to get

δk

cut(Sk)

δk

−

1g(Sk) + δk

−

1g(Sk+1)

−

−

δkg(Sk+1)

≤

−

−
z(Sk+1, δk) < 0.

Because z(Sk, δk) = 0, we get that cut(Sk) = δkg(Sk). Thus, using this in the latter inequality,
we get

δkg(Sk)
which is equivalent to

δk

−

−

1g(Sk) + δk

1g(Sk+1)

−

−

δkg(Sk+1)

≤

z(Sk+1, δk) < 0,

−
However, because the algorithm monotonically decreases δk, we have that δk
therefore we must have that

−

(g(Sk)

g(Sk+1))(δk −

δk

1) < 0.

δk < 0, and

1 −

−

g(Sk) > g(Sk+1).
This means that the denominator of the objective function in Problem (3.2) decreases monotoni-
cally. Additionally, from Theorem 3.4 we have that the objective function decreases monotonically.
These two imply that the numerator of the objective function, i.e., cut(S, ¯S), decreases monotoni-
cally.
(cid:4)

Given this result, we can establish the following theorem, which provides an iteration com-
plexity for Algorithm 3.1. This basic result can be improved, as we describe in Section 3.5,
next.

THEOREM 3.6 (Iteration complexity for Dinkelbach’s algorithm) Consider using Dinkelbach’s algorithm
Algorithm 3.1 for solving MQI, FlowImprove, or LocalFlowImprove on an undirected, connected
graph with non-negative integer weights when starting with the set R. Then the algorithm needs
at most cut(R)

vol(R) iterations to converge to a solution.

≤

Proof For all of the above programs, R is a feasible set and thus we can initialize our algorithms
with R. From Lemma 3.5, we have that cut(S) decreases monotonically at each iteration. Since
we assume that the graph is integer-weighted, then cut(S) is integer valued and so cut(R) gives
an upper bound on the number of iterations. Note that cut(R)
vol(R) for any set and so the
algorithms need at most cut(R)
(cid:4)

vol(R) iterations to converge to a solution S∗.

≤

≤

REMARK 3.7 A weakness of the previous result is that it does not give a complexity result for
graphs with non-integer weights. For weighted graphs with non-integer weights, if the weights
come from an ordered ﬁeld where the minimum relative spacing between elements is µ, such as
would exist for rational-valued weights or ﬂoating point weights, then the above argument gives
cut(R)/µ iterations. This is essentially tight as the following construction gives two sets whose
cut and volume diﬀer only by µ.

Here, S and S copy are duplicates of the same subgraph, so their cut ∂S is identical. Assume S is
small enough that we do not need to take into consideration the min term in conductance. Then
note that φ(S1) = cut(S1)
µ
µ . Furthermore, there is no obvious way to detect this scenario
−
−
as we have a set of well-spaced distinct edge weights (1, 2
µ). (Assuming all
µ, 3, 4
other edges in the graph have weight 1.)

vol(S1) = cut(S2)

µ, 5

vol(S2)

−

−

−

For this reason, we do not consider the iteration complexity of algorithms for graphs with non-
integer weights and we would recommend the algorithm in the next section to get an approximate
answer.

GrestScopySS1S2abcd2−µ534−µ2−µ13∂S∂S21

3.5 A faster version of Dinkelbach’s algorithm via root ﬁnding
Algorithm 3.1 requires at most vol(Q) iterations to converge to an exact solution for non-negative
integer-weighted graphs. If we are not interested in exact solutions, then we can improve the
iteration complexity of Algorithm 3.1 by performing a binary search on δ. This is possible because
it is easy to get bounds on the optimal range of δ. We have zero as a simple lower-bound and, for
the MQI, FlowImprove, and LocalFlowImprove objectives, φ(R)
1 is an easy-to-compute upper-
bound on the optimal δ. Algorithm 3.2 presents a modiﬁed version of Dinkelbach’s algorithm
that accomplishes this. In particular, the subproblem in Step 4 in Algorithm 3.2 is the same as
the subproblem in Step 3 of the original Algorithm 3.1.

≤

Algorithm 3.2 Fast Dinkelbach’s Algorithm for Problem (3.2)
1: Initialize k := 1, δmin := 0, δmax ≥
2: while δmax −

δmin > εδmin do

max φg(S)

p :=

⊆

S

{

|

Q
}

δk := (δmax + δmin)/2
Compute ˆz(δk) by solving Sk+1 := argmin
if g(Sk+1) > 0 (Then δk ≥
else

δ∗.) then

δmax := φg(Sk+1) and set Smax := Sk+1. (Note φg(Sk+1)

δk.)

≤

S z(S, δk) subject to S

Q

⊆

and ε

(0, 1]

∈

3:

4:

5:

6:

7:

8:

δmin := δk

k := k + 1

9:
10: Return

argminS

⊆

Q z(S, δmax) or Smax based on minimum φg

At Steps 5 to 8 of Algorithm 3.2, we make the decision to update δmax and δmin based on the
optimal value of the subproblem. We further store the best solution so far in Smax. In Step 10,
we test if another solve with δmax produces a solution with a better objective than Smax. This
test would allow us to certify that Smax was optimal if the subsequent objective was not lower.
In order to have a convergent algorithm, we have to guarantee that this decision results in a

well-deﬁned binary search. In the following lemma, we discuss this issue.

THEOREM 3.8 (Convergence of Algorithm 3.2 and iteration complexity) Let G be an undirected, con-
nected graph with non-negative weights. The binary search procedure in Algorithm 3.2 is well-
deﬁned, in the sense that the binary search interval includes the optimal solution, and condition in
Step 5 tells us on which side of the optimal solution the current solution is. Moreover, the sequence
δk of Algorithm 3.2 converges to an approximate solution
(log(δmax/ε))
iterations, where δ∗ = φg(S∗) and S∗ is an optimal solution to problem (3.2).

/δ∗ ≤

δ∗ −
|

δk|

ε in

O

|

}

S

⊆

Q

and δmax ≥

Proof Let p :=
p. Let S∗ be an optimal solution of Problem (3.2).
max φg(S)
{
From Theorem 3.3, we get that for (S∗, δ∗) we have that z(S∗, δ∗) = 0, which gives φg(S∗) = δ∗.
Therefore, δ∗ ∈
[0, δmax]. We will use this interval as our search space for the binary search.
Moreover, if g(Sk+1) > 0, then we get from Theorem 3.4 that δk > δ∗. Therefore, we can use δk
to update δmax in Step 6. In fact, because we have a speciﬁc set, we know that φg(Sk+1)
δk
and so we can use a slightly tighter update. However, if g(Sk+1)
0, then we get from Theorem
δ∗, and we can use δk to deﬁne δmin in Step 8. If the initial δmax is greater than p,
3.4 that δk ≤
then it is easy to see that Algorithm 3.2 converges to an optimal solution of Problem (3.2) in at
most log(δmax/ε) iterations, where ε > 0 is an accuracy parameter.

≤

≤

(cid:4)

Note that Theorem 3.8 is an improvement over Theorem 3.6. The former requires

(log(δmax/ε))
iterations in the worst-case, while the latter states that Dinkelbach’s algorithm requires vol(Q)
(number of edges) iterations. Similar results about binary search have been discussed in Lang
and Rao (2004); Andersen and Lang (2008); Hochbaum (2010). Among other details, what is
missing from these references is an exact quantiﬁcation of the value of ε necessary for an exact
solution which we provide in subsequent sections.

O

Table 2 – Speciﬁcs of MQI, FlowImprove and LocalFlowImprove, as special cases of Dinkelbach’s Algorithm 3.1 and
its binary search version Algorithm 3.2. In the table, R is the input seed set of nodes. The column Subproblem
refers to the specialized subsolver that is used to solve the subproblem at Step 3 of Algorithm 3.1 or Step 4 of
Algorithm 3.2. The Augmented Graph entry refers to an augmented graph construction that is used to understand
the subproblem that is solved at each iteration of Dinkelbach’s algorithm. Note that we omit all log-factors and
constants from the running times of the algorithms, more detailed running times can be found in the referenced
theorems. We use ˜O as O-notation without logarithmic factors.

22

Method

Dinkelbach
and Runtime

Binary Search
and Runtime

Subproblem
Construction, Runtime,
and Solvers

MQI

Algorithm 6.1

Algorithm 6.2

Problem (6.3)

O(cut(R) · subproblem)
Theorem 6.3
(Lang and Rao, 2004)

˜O(subproblem)
Theorem 6.5
(Lang and Rao, 2004)

Augmented Graph 1
MaxFlow with vol(R)
edges (§6.1)

FlowImprove Algorithm 7.1

Algorithm 7.2

Problem (7.3)

O(cut(R) · subproblem)
Theorem 7.3
(Andersen and Lang, 2008)

˜O(subproblem)
Theorem 7.5
(Andersen and Lang, 2008)

Augmented Graph 2
MaxFlow with vol(G)
edges (§7.1)

SimpleLocal

Algorithm 8.1

Problem (8.3)

O(cut(R) · subproblem)
Theorem 8.3, (Veldt,
Gleich, and Mahoney,
2016)

˜O(subproblem)
Theorem 8.3
(Orecchia and Zhu, 2014)

Augmented Graph 3
˜O((1+1/σ)2 vol(R)2) with
Alg 8.3 (§§8.1-8.3)

LocalFlow-
Improve(δ)
σ = δ + vol(R)
vol( ¯R)

3.6 The algorithmic components of cluster improvement
We have now shown how to solve cluster improvement problems in the form of Problem (3.2) via
either Dinkelbach’s algorithm or the bisection-based root ﬁnding variation. The last component
of the algorithmic framework is a solver for the subproblem (3.3) in the appropriate Step (3 or 4)
of each algorithm. Solving these subproblems is where the MinCut and MaxFlow-based algorithm
arises as they allow us to test ˆz(δ) < 0. In Sections 6 to 8 we work through how appropriate
MinCut and MaxFlow problems can be derived constructively.

At this point, we summarize the major results and show an overview of the running times of
the methods we will establish in these sections. In particular, in Table 2, we provide pointers
of algorithms and convergence theorems for each method. Also, in Table 2 we provide a short
summary of running times for each method where we make it clear that the subproblem solve
time is a dominant term.

3.7 Beyond conductance and degree weighted nodes
Our discussion and analysis of fractional programming for cluster improvement objectives has,
so far, focused on the MQI, FlowImprove and LocalFlowImprove problems as uniﬁed through
Problem 3.2. However, there is a broader class of objectives that generalizes beyond these speciﬁc
types of cuts and volume ratios. We will highlight a few deﬁnitions that are reasonably straight-
forward to understand, although we will return to the MQI, FlowImprove, and LocalFlowImprove
deﬁnitions above in the subsequent discussions.

As an instance of a more generalized setting, we can deﬁne a generalized volume of a set S,

which we call ν, with respect to an arbitrary vector of positive weights w,

ν(S; w) =

wi = 1T

S w.

(cid:88)

i

S

∈

Note that setting w to be the degree vector d gives the standard deﬁnition of volume, i.e.,
ν(S; d) = vol(S). Then we can seek solutions of

23

minimize
ν(S
V
⊂
subject to denominator > 0

R; w)

−

∩

S

κν(S

cut(S)

¯R; w)

∩

≥

as a generalized notion of MQI, FlowImprove, and LocalFlowImprove (where κ

ν(R; w)/ν( ¯R; w)).
A particularly useful instance is where w is simply the vector of all ones 1n. In which case
ν(S, 1n) is simply the cardinality of the set S. In this case cut(S)/ν(S, 1n) is the expansion or
ratio-cut value of a set (Section 2.6). This approach was used in the original MQI paper (Lang
and Rao, 2004), as that paper discussed ratio-cuts instead of conductance values. This more
general notion of volume also appeared in the FlowImprove paper (Andersen and Lang, 2008)
in order to unify the analysis of ratio-cuts and conductance objectives. While these two choices
have been explored, of course, the theory allows us to choose virtually any vector and this gives
a large amount of ﬂexibility. The MaxFlow and mincut constructions for the subproblems in
the subsequent sections would need to be adjusted to account for this type of arbitrary choice.
This is reasonably straightforward given our derivations. For example, we could set w = √d to
generate a hybrid objective between expansion and conductance.

As another example of how the framework can be even more general, we mention the ideas
from Veldt, Klymko, and Gleich (2019) that penalize excluding nodes from R in the solution set
S. These penalties can be set suﬃciently large such that we can solve variations of FlowImprove
and LocalFlowImprove where all the nodes in R must be in the result, for instance

minimize
V
⊂
subject to R

S

ν(S

⊂

cut(S)

R; w)

κν(S

∩
−
S, denominator > 0

∩

¯R; w)

.

They can also be set smaller, however, such that we wish to have most of R within the solution
S. This scenario is helpful when the element of R may have a conﬁdence associated with them.
All of the analysis in subsequent sections – including the locality of computations – applies
to these more general settings; however, the generalized details often obscure the simplicity and
connections among the methods. So we do not conduct the most general description possible.
We simply wish to emphasize that it is possible and useful to do so.

4 Cluster Improvement, Flow-based, and Other Related Methods
As we have already brieﬂy discussed, graph clustering is a well-established problem with an
extensive literature. Cluster improvement algorithms have received comparatively little attention.
In this section, we will discuss how the cluster improvement problem and algorithms for solving
this problem are similar to and diﬀerent than other related techniques in the literature. Our
goal is to draw a helpful distinction and explain the relationship between cluster improvement
problems/algorithms and a number of other (sometimes substantially but sometimes superﬁcially)
related topics.

For instance, we will discuss how the cluster improvement perspective yields the best results
on graph and mesh partitioning benchmark problems (Section 4.1). We will then highlight key
diﬀerences between the types of graphs arising in scientiﬁc and distributed computing and the
types of graphs based on sparse relational data and complex systems (Section 4.2), which strongly
motivates the use of local algorithms for these data. These local graph clustering algorithms, in
turn, have strong relationships with the community detection problem in networks as well as with
inferring metadata, which we will explore more concretely in the empirical sections.

Taking a step back, we explain our cluster improvement algorithms in terms of ﬁnding sets of
small conductance, and so we also brieﬂy survey the state of conductance optimization techniques
more generally (Section 4.4). Likewise, our algorithms are all based on using a network ﬂow
optimizer as a subroutine to accomplish something else. Since this scenario is surprisingly common,
e.g., because there are fast algorithms for network ﬂow computations, we highlight a few notable

24

applications of network-ﬂow based computing (Section 4.5) as well as the current state of the art
for computing network ﬂows (Section 4.6).

Finally, we conclude this section by relating our cluster improvement perspective to network
ﬂows in continuous domains (Section 4.7), total variation metrics, and a wide range of work in
using graph cuts and ﬂows in image segmentation (Section 4.8).

4.1 Graph and mesh partitioning in scientiﬁc computing
Graph and mesh partitioning are important tools in parallel and distributed computing, where
the goal is to partition a computation into many, large pieces that can be treated with minimal
dependencies among the pieces. This can then be used to maximize parallelism and minimize
communication in large scientiﬁc computing algorithms (Pothen, Simon, and Liou, 1990; Simon,
1991; Karypis and Kumar, 1998; Hendrickson and Leland, 1995a,b; Karypis and Kumar, 1999;
Hendrickson and Leland, 1994; Walshaw and Cross, 2007, 2000; Pellegrini and Roman, 1996;
Knight, Carson, and Demmel, 2014). The traditional inputs to graph partitioning for scientiﬁc
computing are graphs representing computational dependencies involved in solving a spatially
discretized partial diﬀerential equation. In these problems, there is often a strong underlying
geometry, where nodes are localized in space and edges are between nearby nodes. Furthermore,
one of the key goals (indeed, almost a constraint in this application) is that the partitions be
very well balanced so that no piece is much larger than the others.

In the context of this literature, our goal is not to produce an overall partitioning of the graph.
Rather, given a piece of a partition, our tools and algorithms would enable a user to improve that
partition in light of an objective function such as graph conductance or another related objective.
Indeed, work on improving and reﬁning the quality of an initial graph bisections can be found in
the Fiduccia-Mattheyses implementation of the Kernighan-Lin method (Fiduccia and Mattheyses,
1982). Given a quality score for a two-way partition of a graph and a desired balance size, this
algorithm searches among a class of local moves that could improve the quality of the partition.
This improvement technique is incorporated, for instance, into the SCOTCH (Pellegrini and
Roman, 1996), Chaco (Hendrickson and Leland, 1994), and METIS (Karypis and Kumar, 1998)
partitioners.

This strategy for partition-and-improvement is also a highly successful paradigm for generating
the best quality bisections and partitions on benchmark data. For example, on the Walshaw
collection of partitioning test cases (Soper, Walshaw, and Cross, 2004), around half of the current
best known results are the result of improving an existing partitioning using an improvement
algorithm (Henzinger, Noe, and Schulz, 2018). This has occurred a few times in the past as
well (Sanders and Schulz, 2010; Hein and Setzer, 2011; Lang and Rao, 2004). There are important
diﬀerences between the applications we consider (which are more motivated by machine learning
and data science) and those in mesh partitioning for scientiﬁc computing. Most notably, having
good balance among all the partitions is extremely important for eﬃcient parallel and distributed
computing, but it is much less so for social and information networks, as we discuss in the next
section.

4.2 The nature of clusters in sparse relational data and complex systems
Beyond the runtime diﬀerence between local and global graph analysis tools, there is another
important reason to consider local graph analysis for sparse relational data such as social and
information networks, machine learning, and complex systems. There is strong evidence that
large-scale graphs arising in these ﬁelds (Leskovec et al., 2009, 2008; Leskovec, Lang, and
Mahoney, 2010; Gargi et al., 2011; Jeub et al., 2015) have interesting small-scale structure, as
opposed to interesting and non-trivial large-scale global structure. Even aside from running time
considerations, this means that global graph methods tend to have trouble identifying these
small and good clusters and thus may not be well-applicable to many large graphs that arise
in large-scale data applications. As a simple example of the impact the diﬀerences of data may
have on a method, note that for graphs such as discretizations of a partial diﬀerential equation,
simply enlarging a spatially coherent set of vertices results in a set of better conductance (until

25

it is more than half the graph). On the other hand, the sets of small conductance in machine
learning and social network based graphs tend to be small, in which case enlarging them simply
makes them worse in terms of conductance. This has been quantiﬁed by the Network Community
Proﬁle (NCP) plot (Leskovec et al., 2009; Jeub et al., 2015).

4.3 Local graph clustering, community detection, and metadata inference
Local graph clustering is, by far, the most highly developed setting for local graph algorithms. A
local graph clustering method seeks a cluster nearby the reference set R, which can be as small
as a single node. Cluster improvement algorithms are, from this perspective, instances of local
graph clustering where the input is a good cluster R and the output is an even better cluster
S. Local graph clustering itself emerged simultaneously out of the study of partitioning graphs
for improvement in theoretical runtime of Laplacian solvers (Spielman and Teng, 2013) and the
limitations of global algorithms applied to machine learning and data analysis based graphs (Lang,
2005; Andersen and Lang, 2006; Andersen, Chung, and Lang, 2006). Subsequently, there have
been a large number of developments in both theory, practice, and applications. These include:
• improved theoretical bounds (Zhu, Lattanzi, and Mirrokni, 2013; Andersen et al., 2016),
• novel recovery scenarios (Kloumann and Kleinberg, 2014),
• optimization-based approaches and formulations (Gleich and Mahoney, 2014, 2015; Foun-

toulakis, Gleich, and Mahoney, 2017; Fountoulakis et al., 2017),

• heat kernel-based approaches (Chung, 2007a, 2009; Chung and Simpson, 2014; Kloster and

Gleich, 2014; Avron and Horesh, 2015),

• Krylov and Lanczos-based approaches (Li et al., 2015; Shi et al., 2017),
• local higher-order clustering based on triangles (Yin et al., 2017; Tsourakakis, Pachocki,

and Mitzenmacher, 2017),

• large-scale parallel approaches (Shun et al., 2016).
One reason for the diversity of methods in this area is that local graph clustering is a common
technique to study the community structure of a complex system or social network (Leskovec
et al., 2009, 2008; Leskovec, Lang, and Mahoney, 2010). The communities, or modules, of a
network represent a coarse-grained view of the underlying system (Newman, 2006; Palla et al.,
2005). In particular, local clustering, local improvement, and local reﬁnement algorithms are often
used to generate overlapping groups of communities from any community partition (Lancichinetti,
Fortunato, and Kertész, 2009; Xie, Kelley, and Szymanski, 2013; Whang, Gleich, and Dhillon,
2016). This is often called a local optimization and expansion methodology.

Another application of local graph clustering is metadata inference. The metadata inference
problem is closely related to semi-supervised learning, where the input is a graph and a set of
labels with many missing entries. The goal is to interpolate the labels around the remainder
of the graph. Hence, any local clustering method can also be used for semi-supervised learning
problems (Joachims, 2003; Zhou et al., 2004; Liu and Chang, 2009; Belkin, Niyogi, and Sindhwani,
2006; Zhu, Ghahramani, and Laﬀerty, 2003) (and thus, metadata inference). That said, the
metadata application raises a variety of statistical consistency questions (Ha, Fountoulakis, and
Mahoney, 2020), methodological questions due to a no-free-lunch theorem (Peel, Larremore, and
Clauset, 2016), as well as data suitability questions (Peel, 2017). We omit these discussions in
the interest of brevity and note that some caution with this approach is advisable.

Among the local graph clustering methods, the Andersen-Chung-Lang algorithm for seeded
PageRank computation (Andersen, Chung, and Lang, 2006) is often the de facto choice. This
method has both useful theoretical and empirical properties, namely, recovery guarantees in terms
of small conductance clusters (Andersen, Chung, and Lang, 2006; Zhu, Lattanzi, and Mirrokni,
2013) and extremely fast computation (Andersen, Chung, and Lang, 2006). It also has close
relationships to many other perspectives on graph problems (e.g. (Gleich and Mahoney, 2015;
Fountoulakis et al., 2017; Fountoulakis, Gleich, and Mahoney, 2017), including robust and 1-norm
regularized versions of these problems.

Cluster improvement algorithms are a natural ﬁt for both community detection and metadata
inference setting. Given any partition of the network, set of communities, set of overlapping

26

communities, or other set of vertex sets, we can study the results of improving each set individually.
This is exactly the setting of Figure 1, where we were able to ﬁnd a better partition of the network
given an initial partition. (Although, these techniques may not result in a partition.) Second, for
metadata inference, we simply seek to use a given label as a reference set that we improve. We
explore these applications from an empirical perspective in Section 9, where we compare them to
a relative of the Andersen-Chung-Lang method for these tasks.

4.4 Conductance optimization
Taking a step back, the cluster improvement algorithms we discuss improve the conductance
or ratio-cut scores. Finding the overall minimum conductance set in a graph is a well-known
NP-hard problem (Shahrokhi, 1990; Leighton and Rao, 1999). That said, there exist approxi-
mation algorithms based on linear programming (Leighton and Rao, 1988, 1999), semi-deﬁnite
programming (Arora, Rao, and Vazirani, 2009), and so-called cut-matching games (Khandekar,
Rao, and Vazirani, 2009; Orecchia et al., 2012). A full comparison and discussion of these ideas is
beyond the scope of this survey. We note that these techniques are not often implemented due to
complexities in the theory needed to get the sharpest possible bounds. However, these techniques
do inspire new scalable approaches, for instance (Lang, Mahoney, and Orecchia, 2009).

4.5 Network ﬂow-based computing
More broadly beyond conductance optimization, our work relates to the idea of using network
ﬂow as a fundamental computing primitive itself. By this, we mean that many other algorithms
can be cast as an instance of network ﬂow or a sequence of network ﬂow problems. When this
is possible, it enables us to use highly optimized solvers for this speciﬁc purpose that often
outperform more general methods. Bipartite matching is a well known, textbook example of this
scenario (Kleinberg and Tardos, 2005, Section 7.5). Other examples include ﬁnding the densest
subgraph of a network, which is the subset of vertices with highest average degree. Formally, if
we deﬁne

density(S) =

vol(S)

cut(S)

,

−
S
|

|

then the set S that maximizes this quantity is polynomial time computable via a sequence of
network ﬂow problems (Goldberg, 1984). Another instance is one of the many deﬁnitions of
communities on the web that can be solved exactly as a max-ﬂow problem (Flake, Lawrence, and
Giles, 2000). More relevant to our setting is the work of Hochbaum (2013), who showed that the
sets that minimize

minimize
S

cut(S)
vol(S)

and minimize

S

cut(S)
S
|

|

can be found in polynomial time through a sequence of max-ﬂow and min-cut computations.
Although feasible to compute, in general these sets are unlikely to be interesting on many machine
learning and data analysis based graphs, as they will tend to be very large sets that cut oﬀ a
small piece of the rest of the graph. (Formally, suppose there exists a node of degree 1 in an
unweighted graph, then the complement set of that node will be the solution.) Among other
reasons, this is the reason we use the objective functions that are symmetric in S and ¯S.

Four other interesting cases show the diversity of this technique. First, the semi-supervised
learning algorithm of Blum and Chawla (2001) uses the mincut algorithm to identify other vertices
likely to share the same label as those that are given. Second is the use of ﬂows to estimate a
gradient in an algorithm for ranking a set of data due to Osting, Darbon, and Osher (2013). Third,
there are useful connections between matching algorithms (which can be solved as ﬂow problems)
and semi-supervised learning problems (Jacobs, Merkurjev, and Esedo¯glu, 2018). Finally, there is
a recent set of research on total variation or TV norms in graphs and the connections to network
ﬂow (Jung et al., 2019). These were originally conceptualized for semi-supervised learning. They
can also be used to build local clustering mechanisms that optimize a combination of 2-norm and
1-norm objectives with max-ﬂow techniques (Jung and SarcheshmehPour, 2021).

27

(a) Original

(b) Boundary blur

(c) Blur and noise

(d) MQI-like
(δ = 0.11) solution

(e) MQI-like
(δ = 0.04) solution

Figure 4 – An example of using MQI-like procedures to reconstruct a binary image (a) from a blurry (b), noisy
sample (c). Here, the result set is a binary image, which is a set in the grid graph. The value δ is from the
fractional programming subproblem (3.3) with a custom denominator term as described in the reproduction details.
Using δ = 0.11 produces 209 error pixels around the boundary (d). Reducing δ to 0.04 (e) produces a convex shape
due to the conductance-like bias in this setting where convex shapes are optimal for isoperimetric-like objectives on
grids.

4.6 Recent progress on network ﬂow algorithms
Having ﬂow as a subroutine is useful because there is a large body of work in both theory and
practice at making ﬂow computations fast. For an excellent survey of the overall problem, the
challenges, and recent progress, we recommend Goldberg and Tarjan (2014). This overview
touches on the exciting line of work in theory that showed a connection between Laplacian linear
system solving and approximate maximum ﬂow computations (Christiano et al., 2011; Lee, Rao,
and Srivastava, 2013) as well as recent progress on the exact problem (Orlin, 2013). We refer
readers to Lee and Sidford (2013); Liu and Sidford (2019) as well. Also, we refer the reader to
software packages that compute maximum ﬂows fast (Dezso, Alpár, and Kovács, 2011).

(cid:107)C,1

Bx
(cid:107)

4.7 Continuous and inﬁnite dimensional network ﬂow and cuts
Our approach in this survey begins with a ﬁnite graph based on data and is entirely ﬁnite
dimensional. Alternative approaches seek to understand problems in the continuous or inﬁnite
dimensional setting. For instance, Strang (1983) posed a continuous maximum-ﬂow problem in a
domain, where the goal is to identify a function that satisﬁes continuous generalizations of the
ﬂow-conditions. As a quick example of these generalizations, recall that the cut of a set S can
be computed as
. The total variation of an indicator function for a set generalizes the
cut quantity to a continuous domain. This connection, and it’s relationship to sharp boundaries,
motivates total variation image denoising (Rudin, Osher, and Fatemi, 1992) as well as ideas of
continuous minimum cuts (Chan, Esedoglu, and Nikolova, 2006). Continued development of the
theory (Strang, 2010) has led to interesting new connections between the inﬁnite dimensional and
ﬁnite dimensional cases (Yuan, Bae, and Tai, 2010). There are strong connections in motivation
between our cluster improvement framework and ﬁnding optimal continuous functions in these
settings – e.g., we can think of sharpening a blurry, noisy image as improving a cluster (see
Figure 4) – but the details of the algorithms and data are markedly diﬀerent. In particular, we
largely think of the cluster improve routine as a strongly local operation. Understanding how
these ideas generalize to continuous or inﬁnite dimensional scenarios is an important problem
raised by our approach.

4.8 Graph cuts and max ﬂow-based image segmentation
One ﬁnal application of maximum ﬂows is graph cut-based image processing (Boykov and
Veksler, 2006; Marlet, 2017). The general setting in which these arise is an energy minimization
framework (Greig, Porteous, and Seheult, 1989; Kolmogorov and Zabih, 2004) with binary
variables. The goal is to identify a binary latent feature in an image as an exact or approximate
solution of an optimization problem. An extremely large and useful class of these energy functions
can be solved via a single or a sequence of max-ﬂow computations. The special properties of
the max-ﬂow problems on image-like data motivated the development of specialized max-ﬂow

0128

solvers that, empirically, have running time that scales linearly in the size of the data (Boykov
and Kolmogorov, 2004).

This methodology has a number of applications in image segmentation in 2d and 3d im-
ages (Boykov and Funka-Lea, 2006) such as MRIs. For instance, one task in medical imaging is
separating water from fat in an MRI, for which a graph cut based approach is highly success-
ful (Hernando et al., 2010). More recently, deep learning-based methods have often provided
a substantial boost in performance for image processing tasks. Even these, however, beneﬁt
from a cluster improvement perspective. Multiple papers have found that post-processing or
reﬁning the output of a convolutional neural net using a graph cut approach to yield improved
results in segmenting tumors (Ullah et al., 2018; Ma et al., 2018). These recent applications are
an extremely close ﬁt for our cluster improvement framework, where the goal is to ﬁnd a small
object in a big network starting from a good reference region. We often illustrate the beneﬁts and
diﬀerences between our methodologies with a closely related problem of reﬁning a local image
segmentation output, e.g, Figure 3.

Part II. Technical Details Underlying the Main Theoretical Results

5 Minimum Cut and Maximum Flow Problems
As a simple introduction to our presentation of the technical details of MQI, FlowImprove,
and LocalFlowImprove, we will start with the minimum cut and maximum ﬂow problems. We
will review the basics of these problems from an optimization and duality perspective. This is
because our technical discussions in subsequent sections will constitute related, but more intricate,
transformations, and will use maximum ﬂow problems as a subroutine. To simplify the text, we
use the names MinCut and MaxFlow to refer to the s-t minimum cut and s-t maximum ﬂow
problems, which are the fully descriptive terms for these problems.

5.1 MinCut
Given a graph G = (V, E), let s and t be two special nodes where s is commonly called the source
node and t is the sink node. The undirected MinCut problem is:

cut(S, ¯S)

minimize
S
subject to s

S, t

∈

∈

¯S, S

V.

⊆

(5.1)

The objective function of the MinCut problem measures the sum of the weights of edges between
the sets S and ¯S. The constraints encode the idea that we want to separate the source from
sink and so we want the source node s to be in S and the sink node t to be in ¯S. Putting the
objective function and the constraints together, we see that the purpose of the MinCut problem
is to ﬁnd a partition (S, ¯S) that minimizes the number of edges needed to separate node s from
node t. As an example, see Figure 5, where we demonstrate the optimal partition for the MinCut
Problem (5.1) on a toy graph.

We can express the MinCut problem in other equivalent ways, some of which are more
convenient for analysis and implementations. For example, we use indicator vector notation and
the incidence matrix from Section 2 to represent Problem (5.1) as

minimize
Bx
(cid:107)
x
subject to xs = 1, xt = 0, x

(cid:107)C,1

0, 1

∈ {

n.
}

(5.2)

Expressing the MinCut problem with this notation will be especially useful later when we develop
a uniﬁed framework for many cluster improvement algorithms. In practice, when implementing a
solver for this problem, we need not take the binary constraints into account. This is because we
can relax them without changing the objective value to obtain the following equivalent form of the
MinCut problem:

Bx
minimize
(cid:107)
x
subject to xs = 1, xt = 0, x

(cid:107)C,1

Rn.

∈

(5.3)

29

Figure 5 – Demonstration of the optimal MinCut solution of Problem (5.1). The numbers show the weight of each
edge. The red nodes (s, b, d) and the blue nodes (a, c, t) denote the optimal partition (S, ¯S), respectively, for
Problem (5.1). The black dashed line denotes the edges that are being cut, i.e., the edges that cross the partition S
and ¯S. The optimal objective value of Problem (5.1) for this example is equal to 9.

It can be shown that there exists a solution to (5.3) that has the same objective function value as
the optimal solution of (5.2). Given any solution to the relaxed problem, the integral solution can
be obtained by an exact rounding procedure. In that sense, the relaxed problem (5.3) and the
integral problem (5.2) are equivalent (Papadimitriou and Steiglitz, 1982). In the next subsection,
we will obtain a solution to (5.2) through the MaxFlow problem.

5.2 Network Flow and MaxFlow
We provide a basic deﬁnition of a network ﬂow, which is crucial for deﬁning MaxFlow. For more
details about network ﬂows we recommend reading the notes of Trevisan (2011).

Network ﬂows are commonly deﬁned on directed graphs. Given an undirected graph, we will
simply allow ﬂow to go in both directions of an edge. This means that instead of doubling the
number of edges, which is a common technique in the literature, we ﬁx an arbitrary direction
of the edges, encoded in the B matrix, and simply let ﬂow go in either direction by allowing
the ﬂow variables to be negative. Also, in the context of ﬂows, edge weights are usually called
edge capacities. We will use these terms interchangeably, but we tend to use capacities when
discussing ﬂow and weights when discussing cuts.

A network ﬂow is a mapping that assigns values to edges, i.e., a mapping f : E

R from the
set of edges E to R, which also satisﬁes capacity and ﬂow conservation constraints. We view f as
a vector that encodes this mapping for a ﬁxed ordering of the edges consistent with the incidence
matrix. The capacity constraints are easy to state. Let c = diag(C) be the capacity for each
edge, we need

→

c
−

≤

f

≤

c

so that the ﬂow along an edge is bounded by its respective capacity. The ﬂow preservation
constraints ensure that ﬂow is only created at the source and removed at the sink and that all
other nodes neither create nor destroy ﬂow. This can be evaluated using the incidence matrix
that, given a ﬂow f mapping, computes the changes via BT f . Consequently, ﬂow conservation is
written

R, pi = 0

where ps ∈
The maximum ﬂow problem is to compute a feasible network ﬂow with the maximum amount of
ﬂow that emerges from the source and gets to the sink. The corresponding MaxFlow optimization

and qt ∈

R, qi = 0

s
\{

t
\{

i
∀

i
∀

.
}

∈

∈

V

V

}

BT f = q

p

−

stacbd293819129problem can be expressed as:

pT 1s

maximize
f,p,q
subject to BT f = q

p
−
R, pi = 0
R, qi = 0
c.
f

≤

ps ∈
qt ∈
c
≤
−

30

(5.4)

i
∀
i
∀

∈
∈

V
V

s
}
\{
t
}
\{

See Figure 6 for a visual demonstration of the ﬂow variables and the optimal solution of Prob-
lem (5.4) for the same graph used in Figure 5.

(a) Demonstration of a (non-optimal) ﬂow

(b) Demonstration of the MaxFlow solution

Figure 6 – In this ﬁgure, all edges are undirected edges but each edge has an arrow in the middle indicating the
positive direction of ﬂow. A negative ﬂow value on an edge means that the ﬂow is ﬂowing against the positive
direction. The numerators in each expression show the ﬂow that passes through an edge, and the denominators in
each edge show the capacity of each edge. In Subﬁgure 6(a), we demonstrate a ﬂow that starts from the source
node s and ends to the sink node t and has value equal to 3. The path of that ﬂow is highlighted by gray dashed
arrows and includes nodes s, b, a, c and t. Note that the ﬂow in Subﬁgure 6(a) is not optimal since we can send
more ﬂow from the source to the sink while satisfying the constraints of Problem (5.4). The optimal solution of the
MaxFlow Problem (5.4) for this toy graph is shown in Subﬁgure 6(b). The optimal ﬂow that can be sent from the
source to the sink is equal to 9.

We will obtain the MaxFlow Problem (5.4) by computing the Lagrange dual of the relaxed
MinCut Problem (5.3). For basics about Lagrangian duality, we refer the reader to Chapter 5
in Boyd and Vandenberghe (2004). The process of obtaining the dual of a problem is important
for us, because it will allow us to understand how to implement ﬂow-based clustering methods in
subsequent sections. First, we will convert Problem (5.3) into an equivalent linear program

cT u + cT v

minimize
x,u,v
subject to Bx = u

v

−
xs = 1, xt = 0, x
0.
u, v

Rn

∈

(5.5)

≥
This can be done by starting with Problem (5.3) and following standard steps in conversion of a
linear program into standard form. Here, this involves introducing non-negative variables u and v
such that Bx = u
v and then writing the objective as above. (Note that due to the minimization,
at optimality, we will never have both u and v non-zero in the same index.) Consequently, the
Lagrangian function of Problem (5.5) is given by

−

L(u, v, x, f, s, g, p, q) = cT u + cT v
pT (x
−
s + c)T u + (
−
+ q)T x + pT 1s,

f T (Bx
−
−
1s) + qT x
f
−

= (f

−

−

u + v)

sT u

gT v

−

−

g + c)T v + (

BT f

−

p

−

(5.6)

stacbd0/23/9-3/33/80/10/90/20/13/9stacbd2/27/9-3/35/8-1/13/92/21/18/931

≥

0, f

Rm, ps ∈

where s, g
. The latter
V
}
constraints are important for Lagrangian duality because they guarantee that the dual function
(that we will derive below) will provide a lower bound for the optimal solution of the primal
Problem (5.3). See Chapter 5 in Boyd and Vandenberghe (2004). The dual function is

R and pi = 0

R and qi = 0

, qt ∈

s
}
\{

t
\{

i
∀

i
∀

∈

∈

∈

V

h(f, s, g, p, q) := min
u,v,x

L(u, v, x, f, s, g, p, q).

(5.7)

Note that the Lagrangian function L is a linear function with respect to u, v, x. Therefore, we
can obtain an analytic form for the dual function by requiring the partial derivatives of L with
respect to u, v and x to be zero. The following three equations arise from the latter process:

By substituting these conditions into (5.6), we have

BT f + p

−

q = 0n

f

s + c = 0n

−

f

−

−

g + c = 0n.

with domain that is deﬁned by the following constraints

h(f, s, g, p, q) = pT 1s,

(5.8)

BT f + p
q = 0n
−
R, pi = 0
V
i
∈
∈
∀
Thus, we obtain that the dual problem of Problem (5.5) is

s + c = 0n
V
i
∀

f
−
R, qi = 0

ps ∈

qt ∈

s
\{

}

f
−

t
\{

}

g + c = 0n
0.

−
s, g

≥

pT 1s = h(f, s, g, p, q)

maximize
f,s,g,p,q
subject to BT f = q

p
s + c = 0

−

f

−
f
g + c = 0
−
−
R, pi = 0
ps ∈
R, qi = 0
qt ∈
s, g
≥

i
∀
i
∀

V
V

s
}
\{
t
}
\{

∈
∈

(5.9)

0.
By eliminating the variables s and g we obtain the MaxFlow problem (5.4). (These correspond
to slack variables associated with

c.)

f

Both the primal (5.5) and dual (5.9) are feasible (with a trivial cut and a zero ﬂow, respectively)
and also have ﬁnite solutions (0 is a lower bound on the cut and vol(G) = 1T c is an upper bound
on the ﬂow). So, strong duality will hold between the two solutions at optimality, and the
optimal value of the MaxFlow Problem (5.4) is equal to the optimal value of the relaxed MinCut
Problem (5.3) (which is equal to the optimal value of (5.2)). This fact is often one component of
the so-called the MaxFlow-MinCut Theorem. Another important piece is discussed next.

c
−

≤

≤

5.3 From MaxFlow to MinCut
Assume that we have solved the MaxFlow problem to optimality and that we have obtained
the optimal ﬂow f . Then the MaxFlow-MinCut Theorem is a statement about the equivalence
between the objective function value of the optimal solution to the MinCut Problem (5.1) and
the objective function value of the optimal solution of the MaxFlow Problem (5.4). In many cases,
obtaining this quantity suﬃces; but, in some cases, we want to work with the actual solutions
themselves.

To obtain the optimal MinCut solution from an optimal MaxFlow solution, we deﬁne the
notion of a residual graph. A residual graph Gf of a given G has the same set of nodes as G,
but for each edge eij ∈
E, it has a forward edge ˜eij, i.e., from node i to node j, with capacity
fij, 0) and a backward edge ˆeji, i.e., from node j to node i, with capacity max(fij, 0),
max(cij −
where f is the optimal solution of the MaxFlow Problem (5.4). A demonstration of a residual
graph for a given ﬂow is shown in Figure 7.

Note that there cannot exist a path from s to t in the residual graph at a max-ﬂow solution.
(Otherwise, we would be able to increase the ﬂow!) Consequently, we can look at the set S of
vertices reachable starting from the source node s (this can be algorithmically identiﬁed using a
breadth-ﬁrst or depth-ﬁrst search starting from s). It is now a standard textbook argument that
the cut of the set S, which does not contain t, is equal to the maximum ﬂow.

32

(a) The residual graph of the non-optimal ﬂow Fig-
ure 6(a)

(b) The residual graph of the MaxFlow solution
Figure 6(b)

Figure 7 – The two subﬁgures show the directed residual graph for the ﬂows from Figure 6. The edge capacities are
removed for simplicity. Edges are only shown if they have positive capacity. Note that the ﬂow in Figure 6(a) is not
optimal since we can send more ﬂow from the source to the sink while satisfying the constraints of Problem (5.4),
this is equivalent to having a s to t path in the residual graph in Subﬁgure 7(a). In Subﬁgure 7(b), we show the
corresponding residual graph for the optimal ﬂow, and note that in the residual graph of the MaxFlow solution
there is no path from the source node to the sink node.

5.4 MaxFlow solvers for weighted and unweighted graphs
MaxFlow problems can be solved substantially faster than general linear programs. See our
discussion in Section 4.6 for more information on state-of-the-art solvers.

It is often assumed that the graphs are unweighted or have integer positive weights. All of
the MaxFlow problems we need to solve will be weighted with rational weights that depend on
the current estimate of the ratio in the fractional programming problem. Many of the same
algorithms can be applied for weighted problems as well. We explicitly mention both Dinic’s
algorithm (Dinitz, 1970) and the Push-Relabel algorithm (Goldberg and Rao, 1998), both of
which can be implemented for the types of weighted graphs we need. In our implementations, we
use Dinic’s algorithm. In these cases, however, the runtime becomes slightly tricky to state and is
fairly pessimistic. Consequently, when we have a runtime that depends on MaxFlow, we simply
state the number of edges involved in the computation as a proxy for the runtime.

6 The MQI Problem and Algorithm
In this section, we will describe the MaxFlow Quotient-Cut Improvement (MQI) algorithm, due
to Lang and Rao (2004). This cluster improvement method takes as input a graph G = (V, E)
and a reference set R
vol(G)/2, and it returns as output an “improved”
cluster, in the sense that the output is a subset of R of minimum conductance.

V , with vol(R)

⊂

≤

The basic MQI problem is

minimize
S
subject to S

cut(S)
vol(S)
R.

⊆

Due to the assumption that vol(R)

≤

vol(G)/2, this problem is equivalent to

minimize
S
subject to S

φ(S)

R.

⊆

(6.1)

(6.2)

≤

In the equivalence with conductance, this constraint
that vol(R)
vol(G)/2 is crucial because it makes this
problem polynomially solvable. Without this constraint,
the problem with conductance is intractable, however
we can still minimize the cut to volume ratio even when
vol(R) > vol(G)/2.

Aside 6. A curious implication of the
MQI objective is that it is NP-hard to ﬁnd
a set S with vol(S) ≤ vol(G)/2 that even
contains the set of minimum conductance.

stacbdstacbdRecall that this MQI problem is related to the frac-

tional programming Problem (3.2) by setting g(S) := vol(S) and Q = R. Lang and Rao (2004)
describe an algorithm to solve the MQI problem, which is equivalent to what is presented as
Algorithm 6.1. (They describe solving eq. (6.3) via the ﬂow procedure we will highlight shortly.)
It is easy to see that this algorithm is simply Algorithm 3.1 for fractional programming specialized
to this scenario. Consequently, we can apply our standard theory.

33

Algorithm 6.1 MQI (Lang and Rao, 2004)
1: Initialize k := 1, S1 := R and δ1 := φ(S1).
2: while we have not exited via else clause do
3:

cut(S)

Solve Sk+1 := argmin
R

δk vol(S)

−

S

if φ(Sk+1) < δk then
δk+1 := φ(Sk+1)

⊆

else

δk is optimal, return previous solution Sk.

k := k + 1

4:

5:

6:

7:

8:

The following theorem implies that MQI monotonically decreases the objective function in
Problem (6.1) at each iteration. It was ﬁrst shown by Lang and Rao (2004), but it is a corollary
of Theorem 3.4. Note that δk is equal to the objective function of Problem (6.1) evaluated at Sk.

THEOREM 6.1 (Convergence of MQI) Let G be an undirected, connected graph with non-negative
vol( ¯R). The sequence δk monotonically
weights. Let R be a subset of vertices with vol(R)
decreases at each iteration of MQI.

≤

6.1 Solving the MQI subproblem using MaxFlow algorithms
In this subsection, we will discuss how to solve eﬃciently the subproblem at Step 3 of MQI
Algorithm 6.1, namely

argminS
subject to S

cut(S)
R.

⊆

δ vol(S)

−

(6.3)

The summary of this subsection is that the subproblem corresponds to a MinCut-like problem and
by introducing a number of modiﬁcations, we can turn it into an instance of a MinCut problem.
This enables us to use MaxFlow solvers to compute a binary solution eﬃciently. The ﬁnal solver
will run a MaxFlow problem on the subgraph of G induced by R along with a few additional
edges.

By translating Problem (6.3) into indicator notation, we have

δxT d
¯R, x

0, 1

∈ {

n.
}

Bx
(cid:107)

minimize
x
subject to xi = 0

(cid:107)C,1 −
i
∈
∀
This is not a MinCut problem as stated, but there exists an equivalent problem that is a MinCut
problem. To generate this problem, we’ll go through two steps. First, we’ll shift the objective
to be non-negative. This is necessary because a MinCut problem always has a non-negative
objective. Second, we’ll introduce a source and sink to handle the terms that are not of the form
(cid:107)C,1 and the equality constraints. Again, this step is necessary because these problems must
For step 1, note that the maximum negative term is δ1T d. (It’s actually smaller due to the
equality constraints, but this overestimate will suﬃce.) Thus, we shift the objective by this value
and regroup terms

(cid:107)
have a source and sink.

(6.4)

Bx

minimize
x
subject to xi = 0

Bx
(cid:107)

(cid:107)C,1 + δ(1
−
¯R, x
i
∈
∀

x)T d
0, 1

∈ {

n.
}

(6.5)

Note that 1

x is simply an indicator for ¯S, the complement solution set. Consequently, we
want to introduce a penalty for each node placed in ¯S. To do so, we introduce a source node s

−

34

that will connect to each node of the graph with weight proportional to the degree of each node.
(A penalty for ¯S corresponds to an edge from the source s.) Since nothing in ¯R can be in the
solution, we can introduce a sink node t and connect it with inﬁnite weight edges to each node in
¯R. Thus, these edges will never be cut at optimality, as there is a ﬁnite-valued objective possible.
Also note that the inﬁnite weight can be replaced by a suﬃciently large graph-dependent weight
to achieve the same eﬀect.

This MinCut construction is given in Figure 8(b), although this omits the edges from s to
nodes in ¯R. This construction, however, is not amenable to a strongly local solution method, as
it naively involves the entire graph. Note that in practice, we can form the graph construction in
Figure 8(c) with the collapsed vertices without ever examining the whole graph.

To generate a strongly local method, note that we can collapse all the vertices in ¯R and t into
¯R into a
a single super-sink t. This simply involves rewiring all edges (u, v) where u /
∈
new edge (u, t) where we handle multiedges by summing their weights. This results in a number
of s to t edges, one for each node in ¯R, which we can further delete as they exert a constant
penalty of δ vol( ¯R) the ﬁnal objective. An illustration is given in Figure 8(c). Importantly, in
Figure 8(c), there are only a small number of nodes in ¯R that are collapsed into the sink node t,
but ¯R could have had thousands or millions or billions of nodes. In that case, the ﬁnal graph
would still have only a very small number of nodes, in which case strongly local algorithms would
be much faster.

¯R and v

∈

Augmented Graph 1 for the subproblem at Step 3 of MQI Algorithm 6.1
1: Extract the subgraph with nodes in R and the edges of these nodes, which we denote by

2: Add to the set of nodes R a source node s and a sink node t.
3: Add to the set of edges E(R) an edge from the source node s to every node in the seed set of

nodes R with weight the degree of that node times δ.

4: For any edge in G from R to ¯R, rewire it to node t and combine multiple edges by summing

E(R).

their weights.

To recap, see the Augmented Graph 1 procedure. We now give an explicit instance of the
MinCut problem to illustrate how it maps to our desired binary objective. Let B(R) and C(R)
be the incidence and weight matrix for the subgraph induced by the set R. Then consider the
incidence matrix and the diagonal edge-weight matrix of the modiﬁed graph, which are

˜B :=

(cid:34)

R
I
−

s
1
0 B(R)
0

I

(cid:35)

t
0
0
1

−

˜C :=

(cid:34)

δDR
0
0

0
C(R)
0

(cid:35)
,

0
0
Z

where DR is the submatrix of D corresponds to nodes in R (ordered conformally), and Z is a
diagonal matrix that stores the weights of the rewired edges from R to the sink t, i.e.,

Zii =

(cid:88)

e

ce, where e is an edge from i

R to any node in ¯R .

∈

(These weights can be zero if there are no edges leaving from a node i
R.) The ﬁrst column of
matrix ˜B corresponds to the source node, the last column corresponds to the sink node, and all
other columns in-between correspond to nodes in R. The ﬁrst block δDR in ˜C corresponds to
edges from the source to nodes in R, the second block CR in ˜C corresponds to edges from R to
R, and the third block Z in ˜C corresponds to edges from nodes in R to the sink node t. Let

∈

˜x :=





xs
xR
xt


 , so that ˜x1 = xs and ˜x

+2 = xt

R

|

|

35

(a) Graph and seed set R

(b) Non-local MinCut problem (c) Local MinCut problem

Figure 8 – Illustration of the augmented graph for solving the MQI subproblem. Subﬁgure 8(a) illustrates a small
graph and a seed set R denoted by the red ellipse. This set includes nodes with ID 1 to 5. Subﬁgure 8(b) demonstrates
the addition of a source node s and sink node t that involves the entire graph but solves the subproblem. Subﬁgure
8(c) illustrates the collapse of all nodes in ¯R into a single sink node t. Edges from R to ¯R are maintained with the
same weights but they are rewired to the sink node t. The ﬁnal MinCut problem in Subﬁgure 8(c) can be solved via
MaxFlow problem from the source to the sink.

then the MinCut problem with respect to the modiﬁed graph is

(cid:107) ˜C,1 =
minimize
˜x
subject to ˜x1 = 1, ˜x

˜B ˜x
(cid:107)

R

|

|

B(R)xR(cid:107)C(R),1 + δ1T DR(1R −
(cid:107)
.
0, 1
+2 = 0, ˜xi ∈ {
}

xR) + 1T ZxR

(6.6)

It is straightforward to verify that Problem (6.6) is equivalent to a shifted version of Problem (6.5)
where the objectives diﬀer by δ vol( ¯R). Finally, to get a solution of the original problem, we have
to further decrease the objective by the constant δ vol(R).

To solve this MinCut problem, we then simply use an undirected MaxFlow solver. The input

has

O

(vol(R)) edges and
|
Iteration complexity

R

|

+ 2 nodes.

6.2
We now specialize our general analysis in Section 3, and we present an iteration complexity result
for Algorithm 6.1. First, we present Lemma 6.2, which will be used in the iteration complexity
result in Theorem 6.3.

An interesting property of MQI that is shown in Lemma 6.2 is that the volume of Sk
monotonically decreases at each iteration, i.e., vol(Sk+1) < vol(Sk). This result has important
practical implications since it shows that MQI is searching for subsets S that have smaller volume
than the set S1. Moreover, Lemma 6.2 shows that the numerator of Problem (6.1) decreases
monotonically.

LEMMA 6.2 Let G be an undirected, connected graph with non-negative weights. If the MQI
algorithm proceeds to iteration k + 1 it satisﬁes both vol(Sk+1) < vol(Sk) and cut(Sk+1) < cut(Sk).

Proof The proof for this claim is given in the proof of Lemma 3.5.

(cid:4)

THEOREM 6.3 (Iteration complexity of MQI) Let G be a connected, undirected graph with non-negative
integer weights. Algorithm 6.1 has at most cut(R) iterations before converging to a solution.

Proof This is just an explicit specialization of Theorem 3.6.

(cid:4)

REMARK 6.4 (Time per iteration) At each iteration a weighted MaxFlow problem is being solved.
Therefore, the worst-case time of MQI will be its iteration complexity times the cost of computing
a MaxFlow on a graph of size vol(R). Here vol(R) is an upper bound on the number of edges
incident to vertices in R because the weights are integers.

12345678910SetR12345678910st2δ4δ7δ4δ4δ∞∞∞∞∞12345st2δ4δ7δ4δ4δ131236

6.3 A faster version of the MQI algorithm
The original MQI algorithm requires at most vol(R) iterations to converge to the optimal solution
for graphs with integer weights. After at most that many iterations the algorithm returns the
exact output. However, in the case that we are not interested in exact solutions we can improve
(cid:1) where ε > 0 is an accuracy parameter.
the iteration complexity of MQI to at most
O
To achieve this we will use binary search for the variable δ. It is true that for (S∗, δ∗) we have
[0, 1]. We will use this interval as our search space for
cut(S∗)/ vol(S∗) = δ∗. Therefore, δ∗ ∈
the binary search. The modiﬁed algorithm is shown in Algorithm 6.2. This algorithm is an
instance of Algorithm 3.2. Note that the subproblem in Step 4 in Algorithm 6.2 is the same as
the subproblem in Step 3 of the original Algorithm 6.1. The only part that changes is that we
introduced binary search for δ.

(cid:0)log 1
ε

Algorithm 6.2 Fast MQI
1: Initialize k := 1, δmin := 0, δmax := φ(R) and ε
δmin > εδmin do
2: while δmax −

(0, 1]

∈

cut(S)

−

δk vol(S) via MaxFlow on Augmented Graph 1.

if vol(Sk+1) > 0 (Then δk is above δ∗) then

δmax := φ(Sk+1), and set Smax = Sk+1 (Note φ(Sk+1)

δk)

≤

3:

4:

5:

6:

7:

8:

δk := (δmax + δmin)/2
Solve Sk+1 := argmin
R

S

⊆

else

δmin := δk

k := k + 1

9:
10: Return argmin

R cut(S)

S

⊆

−

δmax vol(S) or Smax based on minimum conductance.

Putting the iteration complexity of Fast MQI together with its per iteration computational

complexity we get the following theorem.

THEOREM 6.5 (Iteration complexity of the Fast MQI Algorithm 6.2) Let G be an undirected, connected,
vol( ¯R). The sequence
graph with non-negative weights. Let R be a subset of vertices with vol(R)
δk of Algorithm 6.2 converges to an approximate solution
(log 1/ε) iterations,
/δ∗ ≤
where δ∗ = φ(S∗) and S∗ is an optimal solution to problem (6.1). Moreover, if G has non-negative
integer weights then the algorithm will return the exact minimizer when ε < 1

δ∗ −
|

≤
ε in

δk|

O

vol(R)2 .

Proof The iteration complexity of MQI is an immediate consequence of Theorem 3.8. The exact
solution piece is a consequence of the smallest diﬀerence between values of conductance among
subsets of R for integer weighted graphs. Let S1 and S2 be arbitrary subsets of vertices in R with
φ(S1) > φ(S2). Then

φ(S1)

−

φ(S2) =

cut(S1) vol(S2)

cut(S2) vol(S1)

−
vol(S1) vol(S2)

(vol(R))−

2.

≥

The last piece occurs because if cut(S1) vol(S2)
possible diﬀerence is 1. At termination Fast MQI satisﬁes δmax −
diﬀerence bound, the next objective function value that is larger than δ∗ is at least δ∗ + 1
Therefore, setting ε < 1

cut(S2) vol(S1) is an integer, the smallest
εδmin. By the above
vol(R)2 .

vol(R)2 , we get that δmax < δ∗ + 1

δmin ≤

vol(R)2 .

−

(cid:4)

REMARK 6.6 (Time per iteration) Each iteration involves a weighted MaxFlow problem on a graph
with volume equal to O(vol(R)).

7 The FlowImprove Problem and Algorithm
In this section, we will describe the FlowImprove method, due to Andersen and Lang (2008). This
cluster improvement method was designed to address the issue that the MQI algorithm will always
return an output set that is strictly a subset of the reference set R. The FlowImprove method

also takes as input a graph G = (V, E) and a reference set R
vol(G)/2, and
it also returns as output an “improved” cluster. Here, the output is “improved” in the sense that
it is a set with conductance at least as good as R that is also highly correlated with R.
To state the FlowImprove method, consider the following variant of conductance:

V , with vol(R)

⊂

≤

37

φR(S) =




cut(S, ¯S)
rvol(S; R, θ)



∞

when the denominator is positive

(7.1)

otherwise

where θ = vol(R)/ vol( ¯R), and where the value is
if the denominator is negative. This particular
∞
¯R)
value of θ arises as the smallest value such that the rvol(S; R, θ) = vol(S
denominator is exactly zero when S = V and hence will rule out trivial solutions. (This idea is
equivalent to picking θ so that the total weight of edges connected to source is equal to the total
weight of edges connected to the sink in the coming ﬂow problem (Andersen and Lang, 2008).)
Note that this setup is also equivalent to the statement in Section 3.1 where the denominator
constraint is adjusted to be a positive inﬁnity value.

θ vol(S

R)

−

∩

∩

For any set S with vol(S)

that φR(S)
≥
true conductance score φ(
·
R, in the sense that the denominator penalizes sets S that are outside of the reference set R.

¯R), it holds
θ vol(S
) provides an upper-bound on the
·
) for sets that are not too big; but this objective provides a bias toward

φ(S). Thus, this modiﬁed conductance score φR(

vol( ¯S), since rvol(S; R, θ) = vol(S

R)

≤

−

∩

∩

Consequently, the FlowImprove problem is:

minimize
S
subject to S

φR(S)

V.

⊂

(7.2)

This FlowImprove problem is related to the fractional programming Problem (3.2) by setting
¯R) and Q = V . Andersen and Lang (2008) describe an algorithm
g(S) := vol(S
to solve the FlowImprove problem, which is equivalent to what we present as Algorithm 7.1.
It is easy to see that this algorithm is a special case of Algorithm 3.1 for general fractional
programming.

θ vol(S

R)

−

∩

∩

Algorithm 7.1 FlowImprove (Andersen and Lang, 2008)
1: Initialize k = 1, S1 := R and δ1 = φR(S1).
2: while we have not exited via else clause do
3:

(cid:0)vol(S

cut(S)

R)

Solve Sk+1 := argminS
if φR(Sk+1) < δk then
δk+1 := φR(Sk+1)

δk

−

θ vol(S

¯R)(cid:1)

∩

−

∩

else

δk is optimal, return previous solution Sk.

k := k + 1

4:

5:

6:

7:

8:

The following theorem implies that FlowImprove monotonically decreases the objective
function in Problem (7.2) at each iteration. It was ﬁrst shown by Andersen and Lang (2008), but
it is a corollary of Theorem 3.4. Note that δk is equal to the objective function of Problem (7.2)
evaluated at Sk.

THEOREM 7.1 (Convergence of FlowImprove) Let G be an undirected, connected graph with non-
vol( ¯R). The sequence δk monotoni-
negative weights. Let R be a subset of vertices with vol(R)
cally decreases at each iteration of FlowImprove (Algorithm 7.1).

≤

7.1 The FlowImprove subproblem
In this subsection, we will discuss how to solve eﬃciently the subproblem at Step 3 of FlowImprove.
We will follow similar steps as we did for MQI in Section 6.1. That is, we convert the MinCut-like

Augmented Graph 2 for the subproblem at Step 3 of FlowImprove Algorithm 7.1
1: Add to the set of nodes V a source node s and a sink node t.
2: Add to the set of edges E an edge from the source node s to every node in the seed set of

nodes R with weight the degree of that node times δ.

3: Add to the set of edges E an edge from the sink node t to every node in the set of nodes ¯R

with weight the degree of that node times δθ.

38

(a) Graph and seed set R

(b) MinCut graph for FlowImprove subproblem

Figure 9 – Illustration of the augmented graph for solving the FlowImprove subproblem. Subﬁgure 9(a) illustrates
the same graph and seed set from Figure 8. Subﬁgure 9(b) demonstrates the addition of a source node s and sink
node t, along with corresponding edges from s to nodes in R and node t to every node in ¯R. The MinCut problem
in Subﬁgure 9(b) can be solved to identify a set via a MaxFlow problem from the source to the sink.

problem into a true MinCut problem on an augmented graph, and then we use MaxFlow to ﬁnd
the set minimizing the objective. As a summary and overview, see the Augmented Graph 2
procedure and an example of this new modiﬁed graph in Figure 9. (Observe that here we do
not have a fourth step where we combine multiple edges, as we did in Augmented Graph 1 and
Figure 8(c)—thus, the FlowImprove Algorithm 7.1 will not be strongly local.)

Turning back towards the derivation of this formulation, the MinCut sub-problem at Step 3

of FlowImprove problem is equivalent to

Bx
minimize
(cid:107)
x
subject to x

δxT ˆdR + δθxT ˆd ¯R
(cid:107)C,1 −
n,
0, 1
}
where ˆdR is a n-dimensional vector that is equal to d for components with index in R and zero
elsewhere. Similarly for ˆd ¯R
where the two pieces have disjoint
support.

. Consequently, d = ˆdR + ˆd ¯R

(7.3)

∈ {

As with the previous case, we shift this and then add sources and sinks. First, the largest

possible negative value is at least δ1T ˆdR. Adding this yields

minimize
Bx
(cid:107)
x
subject to x

(cid:107)C,1 + δ(1
n.
0, 1
}

∈ {

x)T ˆdR + δθxT ˆd ¯R

−

(7.4)

Again, we have penalty terms associated with S (given by non-zero entries of x) and ¯S (given by
non-zero entries of 1
x). For these, we introduce a source and sink. The source connects to
penalties associated with ¯S and the sink connects to penalties associated with S. Note that these
penalties partition into two groups, associated with R and ¯R. Consequently, we add a source
node s and connect it to all nodes in R with weight δ ˆdR, and we also add a sink node t and
connect it to all nodes in ¯R with weight δθ ˆd ¯R

−

.

12345678910SetR12345678910st2δ4δ7δ4δ4δ7δθ3δθ2δθ4δθ3δθThe resulting MinCut problem is associated with the incidence matrix and the diagonal

edge-weight matrix of a modiﬁed problem as follows

39

˜B :=

(cid:34)

s
1
0
0

(cid:35)

V
t
IR
0
−
0
B
1
I ¯R −

˜C :=

(cid:34)

δDR
0
0

0
C
0

(cid:35)
.

0
0
δθD ¯R

Here, DR and D ¯R
Also, IR and I ¯R
respectively. These matrices give IRx = xR and I ¯Rx = x ¯R
was DR and D ¯R

are diagonal submatrices of D corresponding to nodes in R and ¯R, respectively.
are matrices where each row contains an indicator vector for a node in R and ¯R,
. They are ordered in the same way as

. Let

˜x :=





xs
x
xt


 , so that ˜x1 = xs and ˜x

+2 = xt

R

|

|

then the MinCut problem with respect to the modiﬁed graph is

˜B ˜x

(cid:107)C,1 + δ(1
minimize
(cid:107) ˜C,1 =
˜x
subject to ˜x1 = 1, ˜xn+2 = 0, ˜xi ∈ {
0, 1

Bx

(cid:107)

(cid:107)

−

} ∀

xR)T dR + δθx ¯Rd ¯R
i = 2 . . . n + 1.

(7.5)

Again, note that this objective corresponds to a constant shift with respect to Problem (7.3).
This problem can be solved via MaxFlow to give a set solution.

Iteration complexity

7.2
In Lemma 7.2 we show that when using FlowImprove the denominator of Problem (7.2), i.e.,
¯R), decreases monotonically at each iteration. Moreover, the numerator of
vol(S
Problem (7.2) decreases monotonically as well.

θ vol(S

R)

−

∩

∩

LEMMA 7.2 If the FlowImprove algorithm proceeds to iteration k + 1 it satisﬁes vol(Sk+1 ∩
vol(Sk ∩
vol(Sk ∩
Proof This result is a specialization of Lemma 3.5 and the proof is the same.

¯R)(cid:1) and cut(Sk+1) < cut(Sk).

R) < θ (cid:0)vol(Sk+1 ∩

¯R)

−

R)

−

(cid:4)

THEOREM 7.3 (Iteration complexity of the FlowImprove Algorithm 7.1) Let G be a connected, undirected
graph with non-negative integer weights. Then Algorithm 7.1 needs at most cut(R) iterations to
converge to a solution.

Proof This is just an explicit specialization of Theorem 3.6.

(cid:4)

REMARK 7.4 (Time per iteration) At each iteration a weighted MaxFlow problem is being solved, see
Section 7.1. The MaxFlow problem size is proportional to the whole graph.

7.3 A faster version of the FlowImprove algorithm
The original FlowImprove algorithm requires at most cut(R)
vol(R) iterations to converge to
the optimal solution. After at most that many iterations the algorithm returns the exact output.
However, in the case that we are not interested in exact solutions we can improve the iteration
(cid:1) where ε > 0 is an accuracy parameter. To
complexity of FlowImprove to at most
[0, 1]. Therefore,
achieve this we will use binary search for the variable δ. It is true that φR(R)
[0, 1]. We will use this interval as our search space for the binary search. The modiﬁed
δ∗ ∈
algorithm is shown in Algorithm 7.2. Note that the subproblem in Step 4 in Algorithm 7.2 is the
same as the subproblem in Step 3 of the original Algorithm 7.1. The only part that changes is
that we introduced binary search for δ.

(cid:0)log 1
ε

O

≤

∈

Putting the iteration complexity of Fast FlowImprove together with its per iteration compu-

tational complexity we get the following theorem.

Algorithm 7.2 Fast FlowImprove
1: Initialize k := 1, δmin := 0, δmax := 1 and ε
2: while δmax −

δmin > εδmin do

40

(0, 1]

∈

3:

4:

5:

6:

7:

8:

δk := (δmax + δmin)/2
Solve Sk+1 := argmin
if vol(Sk+1 ∩
else

−
∩
R) > θ vol(Sk+1 ∩
δmax := φR(Sk+1) and set Smax := Sk+1 (Note φR(Sk+1)

δk
−
¯R) (Then δk is above δ∗) then

S cut(S)

θ vol(S

R)

∩

(cid:0)vol(S

δk)

≤

¯R)(cid:1) via MaxFlow

δmin := δk

k := k + 1

9:
10: Return argmin

S cut(S)

−

δmax(vol(S

R)

∩

−

θ vol(S

∩

¯R)) or Smax based on min φR.

THEOREM 7.5 (Iteration complexity of the Fast FlowImprove Algorithm 7.2) Let G be an undirected, con-
vol( ¯R). The sequence
nected graph with non-negative weights. Let R be a subset of V with vol(R)
δk of Algorithm 7.2 converges to an approximate solution
(log 1/ε) iterations,
where δ∗ = φR(S∗) and S∗ is an optimal solution to problem (7.2). Moreover, if G has non-negative
integer weights, then the algorithm will return the exact minimizer when ε <

δ∗ −
|

δ∗ ≤

≤
ε in

δk|

O

.

1
vol(R)2 vol( ¯R)

Proof Iteration complexity of FlowImprove is an immediate consequence of Theorem 3.8. The exact
solution piece is a consequence of the smallest diﬀerence between values of relative conductance
for integer weighted graphs. Let S1 and S2 be arbitrary sets of vertices in the graph with
R) and k2 = cut(S1) vol(S2 ∩
φR(S1) > φR(S2). Let k1 = cut(S1) vol(S2 ∩
¯R)

−
¯R). Both are integers. Then

cut(S2) vol(S1 ∩

R)

cut(S2)
rvol(S2; R, θ)

=

k1 vol( ¯R)

k2 vol(R)

−
vol( ¯R)rvol(S1; R, θ)rvol(S2; R, θ) ≥

1
vol(R)2 vol( ¯R)

.

The last piece occurs because k1 and k2 are integers, and thus the smallest positive value of
k1 vol( ¯R)
k2 vol(R) is 1. The rest of the argument on the exact solution is the same as the proof
of Theorem 6.5.
(cid:4)

−

The subproblem is the same and so the cost per iteration is the same as discussed in Remark 7.4.

7.4 Non-locality in FlowImprove
The runtime bounds for FlowImprove assume that we may need to solve a MaxFlow problem
with size proportional to the entire graph. We now show that this is essentially tight and that
the solution of a FlowImprove problem, in general, is not strongly local. Indeed, the following
example shows that FlowImprove will return one fourth of the graph even when started with a
set R that is a singleton.

−

cut(S2) vol(S1 ∩
cut(S1)
rvol(S1; R, θ) −

41

LEMMA 7.6 Consider a cycle graph with some extra
edges connecting neighbors or neighbors in some parts
(illustrated on the right) with 4N + 8 nodes in 4 major
regions. Each set A and B has N nodes of degree 4
corresponding to a contiguous piece of the cycle graph
with neighbors and neighbors of neighbors connected.
Each set C and D has N degree 2 nodes. This intro-
duces two extra nodes, of degree 3, between each pair
of adjacent degree 2 and degree 4 regions. Consider
using any node of degree 4 as the seed node to Flow-
Improve algorithm. Then, at optimality, FlowImprove
will return a set with N + 4 nodes that is a continuous
degree 4 region plus the four adjacent degree 3 nodes.

Proof Without loss of generality, suppose we seed on a node from set A. According to Lemma 7.2,
when Dinkelbach’s algorithm for FlowImprove proceeds from iteration to iteration, it must return
a set with a strictly smaller cut value or the seed set R was optimal. This means FlowImprove
will only return one of the following sets. (Due to symmetry, there may be equivalent sets that
we don’t list.)

1. The seed node with cut 4.

2. A continuous subset of the A region, G0, G1, and a continuous subset of the set D, with

cut 3.

3. All of the A region, two adjacent degree 3 nodes (without loss of generality, G0 and G1) on

one end and one adjacency degree 3 node on the other edge (F1), with cut 3.

4. All of the A region and all adjacency degree 3 nodes (G0, G1, F0, F1), with cut 2.

5. All of the A region and all adjacency degree 3 nodes (G0, G1, F0, F1 and additional nodes

from sets C and D), with cut 2.

The goal is to show that case (4) is optimal, i.e., has the smallest objective value. Obviously, case
(5) cannot be optimal since it has the same cut value as case (4) but smaller relative volume.
Similarly, case (3) has the same cut value as case (2) but smaller relative volume. So case (3)
won’t be optimal either. So we only need to compare φR(S1), φR(S2) and φR(S4). Observe that
in this setting, θ = vol(R)
3 = 1

, so we can compute that

4
1)4+2N

vol( ¯R) =

3N +5

(2N

−

2+8
·

·

φR(S4) =

θ(4(N

4

−

2

−

1) + 3

4)

·

=

3N + 5
4N + 6

< 1 = φR(S1).

On the other hand, suppose in case (2), there are 1
from D, then we can write

≤

k < N nodes from A and m

0 nodes

≥

φR(S2) =

3

θ(4(k

4

−

−

1) + 3

·

2 + 2m) ≥

4

=

6θ

9N + 15
12N + 14

3

−

> φR(S4).

So case (4) is optimal.

(cid:4)

E0E1B0B1B2B3B4F1F0F0F1A0A1A2A3A4G0G1C0C1C2C3C4D0D1D2D3D4setC,Nnodesofdegree2setD,Nnodesofdegree2setB,Nnodesofdegree4setA,Nnodesofdegree47.5 Relationship with PageRank
The FlowImprove subproblem (7.5) is closely related to the PageRank problem if the 1-norm
objective is translated into a 2-norm objective and we relax to real-valued vectors (and make a
small perturbation to the resulting systems). This was originally observed, in slightly diﬀerent
ways, in our previous work (Gleich and Mahoney, 2014, 2015). For the same matrix ˜B, consider
the problem

42

minimize
˜x
subject to xs = ˜x1 = 0, xt = ˜xn+2 = 1, ˜xi ∈ {

(cid:107)

2
˜C,2

˜B ˜x
(cid:107)

0, 1

i = 2 . . . n + 1.

} ∀

(7.6)

Note that this problem, with the binary constraints, is exactly equivalent to the original problem.
However, if we relax the binary constraints to real-valued vectors and substitute in x1 = 1 and
xn+2 = 0, then this is a strongly-convex quadratic objective, which can be solved as the following
linear system:

(BT CB + δ diag( ˜dR) + θδ diag( ˜d ¯R))x = δ/2 ˜dR.

(7.7)

Here, BT CB = L = D
A is the Laplacian of the original graph. Also, if we had θ = 1 (or
simply assume this is true), then δ diag( ˜dR) + θδ diag( ˜d ¯R) = δD. This yields the linear system

−

(L + δD)x = δ/2

⇔

(I

−

1
1+δ AD−

1)Dx = δ/(2 + 2δ) ˜dR.

The second system is equivalent to a rescaled PageRank problem for an undirected graph
1)y = γv where y = Dx. This form, or a scaled version, is widely used in practice (Gleich,
(I
−
2015).

αAD−

8 The LocalFlowImprove (and SimpleLocal) Problem and Algorithm
In this section, we will describe the LocalFlowImprove method, due to Orecchia and Zhu (2014),
and the related SimpleLocal, due to Veldt, Gleich, and Mahoney (2016). This cluster improvement
method was designed to address the issue that FlowImprove is weakly (and not strongly) local,
i.e., that the FlowImprove method has a running time that depends on the size of the entire input
graph and not just on the size of the reference set R. The setup is the same: LocalFlowImprove
method takes as input a graph G = (V, E) and a reference set R
vol(G)/2,
and it returns as output an “improved” cluster.

V , with vol(R)

≤

⊂

To understand the LocalFlowImprove method, consider the following variant of conductance:

φR,σ(S) =




vol(S



∞

∩

cut(S, ¯S)
R)

σ vol(S

−

¯R)

∩

when the denominator is positive

(8.1)

otherwise

where σ
and allow it to vary. Given this, the basic LocalFlowImprove problem is:

). This is identical to FlowImprove (7.1), but we change θ into σ

[vol(R)/ vol( ¯R),

∞

∈

minimize
S
subject to S

φR,σ(S)

V.

⊂

On the surface, it is straightforward to adapt be-
tween FlowImprove and LocalFlowImprove. Simply “re-
peating” the entire previous section with σ instead of
θ will result in correct algorithms. For example, the
original algorithm proposed for LocalFlowImprove by
Orecchia and Zhu (2014) is presented in an equivalent
fashion Algorithm 8.1, which is simply an instance of the
bisection-based fractional programming Algorithm 3.2.
The key diﬀerence between FlowImprove and Lo-
calFlowImprove is that by setting σ larger than

(8.2)

Aside 7. For the theory in this section,
we parameterize the LocalFlowImprove ob-
jective with σ instead of θ + δ as in Sec-
tion 3.1 and Section 9. This choice re-
duces the number of constants in the state-
ment of theorems. The previous choice of
δ is designed to highlight the FlowImprove
to MQI spectrum.

43

vol(R)/ vol( ¯R) we will be able to show that the running time is independent of the size of
the input graph. Recall that we have already shown the output set has a graph-size independent
bound in Lemma 3.2.

This strongly-local aspect of LocalFlowImprove manifests in the subproblem solve step. Put
another way, we need to crack open the black-box ﬂow techniques in order to make them run in a
way that scales with the size of the output rather than the size of the input. As a simple example
of how we’ll need to look inside the black box, note that when σ =
, then LocalFlowImprove
corresponds to MQI, as discussed in Section 3.1, which has an extremely simple strongly local
algorithm. We want algorithms that will be able to take advantage of this property without
needing to be told this will happen. Consequently, in this section, we are going to discuss the
subproblem solver extensively.

∞

In particular, we will cover how to adapt a sequence of standard MaxFlow solves to be strongly
local, as in the SimpleLocal method of Veldt, Gleich, and Mahoney (2016) (Section 8.1), as well
as improvements that arise from using blocking ﬂows and adapting Dinic’s algorithm (Sections 8.2
and 8.3). We will also cover diﬀerences with solvers with diﬀerent types of theoretical tradeoﬀs
that were discussed in the original Orecchia and Zhu (2014) paper (Section 8.4).

Note that the SimpleLocal algorithm of Veldt, Gleich, and Mahoney (2016) did not use
binary search on δ as in Algorithm 8.1 (and nor do our implementations), instead it used the
original Dinkelbach’s algorithm. As we have pointed out a few times, binary search is not as
useful as it may seem for these problems, as a few iterations of Dinkelbach’s method is often
suﬃcient on real-world data. The point here is that the tradeoﬀ between bisection and the
greedy Dinkelbach’s method is independent of the subproblem solves that is the heart of what
diﬀerentiates LocalFlowImprove from FlowImprove. Finally, note that Algorithm 8.1 is also a
special instance of Algorithm 3.2.

Algorithm 8.1 LocalFlowImprove (Orecchia and Zhu, 2014)

1: Initialize k := 1, δmin := 0, δmax := 1, σ
2: while δmax −

δmin > εδmin do

(cid:104) vol(R)
vol( ¯R) ,

∈

∞

(cid:17)

, and ε

(0, 1]

∈

δk := (δmax + δmin)/2
Solve Sk+1 := argmin
if vol(Sk+1 ∩
else

−
R) > σ vol(Sk+1 ∩
δmax := φR,σ(Sk+1) and set Smax := Sk+1 (Note φR,σ(Sk+1)

(cid:0)vol(S
δk
R)
σ vol(S
−
¯R) (Then δk ≥
δ∗) then

S cut(S)

∩

∩

δk.)

≤

¯R)(cid:1) via MaxFlow

3:

4:

5:

6:

7:

8:

δmin := δk

k := k + 1

9:
10: Return argmin

S cut(S)

−

δmax(vol(S

R)

∩

−

σvol(S

∩

¯R)) or Smax based on min φR,σ

The iteration complexity of Algorithm 8.1 is now just a standard application of the fractional

programming theory.

THEOREM 8.1 (Iteration complexity of LocalFlowImprove) Let G be an undirected, connected graph
vol( ¯R). In Algorithm 8.1, the
with non-negative weights. Let R be a subset of nodes with vol(R)
sequence δk converges to an approximate optimal value
(log 1/ε) iterations,
δk|
where δ∗ = φR,σ(S∗) and S∗ is an optimal solution to problem (8.2). Moreover, if G has non-
negative integer weights and σ = (η + vol(R))/ vol( ¯R) for an integer value of η, then the algorithm
1
will return the exact minimizer when ε <
vol(R)2 vol( ¯R)

≤
/δ∗ ≤

δ∗ −
|

ε in

O

.

Proof The ﬁrst part is an immediate consequence of Theorem 3.8 with δmax = 1. The exact
solution piece is a consequence of the smallest diﬀerence between values of relative conductance for
integer weights. Let S1 and S2 be arbitrary sets of vertices in the graph with φR,σ(S1) > φR,σ(S2).
¯R),
Let k1 = cut(S1) vol(S2∩

R) and k2 = cut(S1) vol(S2∩

cut(S2) vol(S1∩

cut(S2) vol(S1∩

¯R)

R)

−

−

which are both integers. Then

cut(S1)
rvol(S1; R, σ) −

cut(S2)
rvol(S2; R, σ)

=

k1 vol( ¯R)

k2(η + vol(R))

vol( ¯R)rvol(S1; R, σ)rvol(S2; R, σ) ≥

−

44

1
vol(R)2 vol( ¯R)

.

The inequality follows because the integrality of k1 and k2 ensures tha the smallest positive value
of k1 vol( ¯R)
k2(η + vol(R)) is 1. The rest of the argument on the exact solution is the same as
−
the proof of Theorem 6.5.
(cid:4)

Augmented Graph 3 for the subproblem at Step 4 of LocalFlowImprove Algorithm 8.1. This
is identical to the FlowImprove procedure with σ instead of θ; for LocalFlowImprove we develop
algorithms to work with this problem implicitly.
1: Add to the set of nodes V a source node s and a sink node t.
2: Add to the set of edges E an edge from the source node s to every node in the seed set of

nodes R with weight the degree of that node times δ.

3: Add to the set of edges E an edge from the sink node t to every node in the set of nodes ¯R

with weight the degree of that node times δσ, where σ

[vol(R)/ vol( ¯R),

∈

).

∞

Moreover, the subproblem construction and augmented graph are identical to FlowImprove,
except with σ instead of θ. For the construction of the modiﬁed graph to use at the subproblem
step, see Augmented Graph 3. The MinCut problem for a speciﬁc value of δ = δk from the
algorithm is also equivalent with σ instead of θ,

minimize
Bx
(cid:107)
x
subject to x

(cid:107)C,1 + (1
n,
0, 1
}

∈ {

δ)xT ˆdR + δσxT ˆd ¯R

−

(8.3)

using the same notation from (7.3). (Here, we have not implemented the subsequent step of
associating terms with sources and sinks, as that follows an identical reasoning to FlowImprove
problem.)

However, in practice, we never explicitly build this augmented graph, as that would immediately
preclude a strongly local algorithm, where the runtime depends on vol(R) instead of n or m (the
number of vertices or edges). Instead, the algorithms seek to iteratively identify a local graph,
whose size is bounded by a function of vol(R) and σ that has all of R and just enough of the rest
of G to be able to guarantee a solution to (8.3).

As some quick intuition for why the LocalFlowImprove subproblem might have this property,
we recall Lemma 3.2, which showed that there is a bound on the output size that is independent of
the graph size. We further note the following sparsity-promoting intuition in the LocalFlowImprove
subproblem.

LEMMA 8.2 (Originally from Veldt, Gleich, and Mahoney (2016), Theorem 1) The subproblem solve in
LocalFlowImprove (8.3) corresponds to a degree-weighted 1-norm regularized variation on the
subproblem solve in FlowImprove (7.4). More speciﬁcally, the 1-norm regularized problem is

Bx
minimize
(cid:107)
x
subject to x

(cid:107)C,1 + ˆδ(1
0, 1
}
1+θ , where θ = vol(R)/ vol( ¯R).

∈ {

−

−

n

δ

with ˆδ = δ + κ and κ = δσ

x)T ˆdR + ˆδθxT ˆd ¯R + κ
(cid:107)

Dx

(cid:107)1

(8.4)

Proof The proof follows from expanding (8.4) using ˆδ and κ
Dx
(cid:107)
vectors. And then ignoring constant terms.

(cid:107)1 = κxT ˆdR +κxT ˆd ¯R

for indicator

(cid:4)

Given the rich literature on solving 1-norm regular-
ized problems in time much smaller than the ambient
problem space or with provably fewer samples (Tibshi-
rani, 1996; Efron et al., 2004; Candès, Romberg, and
Tao, 2006; Donoho and Tsaig, 2008), these results are
perhaps somewhat less surprising.

45

Aside 8. We also note that this idea of
adding a 1-norm penalty is a common de-
sign pattern to create strongly local algo-
rithms.

In the remainder of this section, we will explain two solution techniques for the subproblem solve
that will guarantee the following runtime for ﬁnding the set that minimizes the LocalFlowImprove
objective.

THEOREM 8.3 (Running time of LocalFlowImprove, based on Veldt, Gleich, and Mahoney (2016)) Let G be
a connected, undirected graph with non-negative integer weights. A LocalFlowImprove problem can
be solved via Dinkelbach’s Algorithm 3.1 or Algorithm 8.1. The algorithms terminate in worst-case
time

(cut(R)

·

O

subproblem) for Dinkelbach and

(log 1

ε ·

O

subproblem) for bisection.

Let γ = 1 + 1

σ . For solving the subproblem, we have the following possible runtimes,

Algorithm 8.2 γ vol(R) calls to MaxFlow on γ vol(R) edges
(cid:0)γ2 vol(R)2 log[γ vol(R)](cid:1)

Algorithm 8.3

(MaxFlow based)

(BlockingFlow based)

O

Proof This result can be obtained by combining the iteration complexity of Dinkelbach’s Theo-
rem 3.6 or LocalFlowImprove from Theorem 8.1 with either the running time of the MaxFlow-
based SimpleLocal subsolver Algorithm 8.2 or the running time of the blocking ﬂow algorithm,
Theorem 8.5.

(cid:4)

See Section 8.4 for details on faster algorithms from Orecchia and Zhu (2014).

8.1 Strongly Local Constructions of the Augmented Graph
Before we present algorithms for the LocalFlowImprove subproblem, we discuss a crucial result
from Orecchia and Zhu (2014) that reduces the Augmented Graph 3 for the MaxFlow problem to
a reduced modiﬁed graph that includes only nodes relevant to the optimal solution. The crux of
this section is an appreciation of the following statement:

An unsaturated edge in a ﬂow is an edge where the ﬂow value is strictly less than the
capacity. If, in a solution of MaxFlow on the augmented graph, there is an unsaturated
edge from a node in ¯R to t, then that node is not in the solution MinCut set.

This result is a fairly simple structural statement about how we might verify a solution to
such a MaxFlow problem. We will illustrate it ﬁrst with a simple example where the optimal
solution set is contained within R, akin to MQI but without that explicit constraint, and then
we move to the more general case, which will involve introducing the idea of a bottleneck set B.
Throughout these discussions, we will use ˆG to denote the full s, t augmented graph construction
for a LocalFlowImprove subproblem with R, δ, σ ﬁxed.

Consider what happens in solving a MaxFlow on ˆG where σ > vol(R). In this scenario,
LocalFlowImprove will always return S
R and this will be true on subproblem solve as well.
(See discussion in Section 3.1). We will show how we can locally certify a solution on ˆG—without
even creating the entire augmented graph. We ﬁrst note the structure of the ˆG partitioned
into the following sets: R, ∂R and everything else, i.e., ¯R
∂R. This results in a view of the
subproblem as follows:

−

⊆

full subproblem ˆG

or

edge subset.

stR1R2R3R4R5a1a2a3U1U2U3U4R¯RstR1R2R3R4R5a1a2a3U1U2U3U4R¯R∂R46

Suppose that we delete all the gray edges and solve the resulting MaxFlow problem (or we just
solve the problem for the teal-colored subset). This will result in a MaxFlow problem on a
subset of the edges of the augmented graph—and one that has size bounded by vol(R). In any
solution of the resulting MaxFlow problem, we have that all of the edges from ∂R to t will be
unsaturated, meaning that the ﬂow along those edges will be strictly smaller than the capacity.
This is straightforward to see because the total ﬂow out of the source is δ vol(R) and each edge
from ∂R to t has weight diδσ > diδ vol(R). Consequently the nodes in ∂R will always be on the
sink side of the MinCut solution.

This ability of unsaturated edges to provide a local guarantee that we have found a solution
arises from two aspects. First, we have a strict edge-subset of the true augmented graph, so any
ﬂow value we compute will be a lower-bound on the max ﬂow objective function on the entire
graph. Second, we have not removed any edges from the source. Consequently we can locally
certify this solution because none of the edges leading to t are saturated, so the bottleneck must
have been outside of the boundary of R. Put another way, since the edges from ∂R to t are
unsaturated, there is no way the omitted gray nodes and edges could have helped get more ﬂow
from the source to the sink.

Now, suppose that σ was smaller such that at least one node in the boundary of R has a
saturated edge to t. Then we lose the proof of optimality because it’s possible those missing
gray nodes and edges could have been used to increase the ﬂow. Suppose, however, we add those
bottleneck nodes in ∂R to a set B and solve for the MaxFlow where B is:

B is one node

or

B is two nodes.

That is, the subgraph of ˆG with all edges among s, t, R, B, ∂(R
B). As long as the bottleneck
B), then we have an optimal solution. The best way to think about
is not in the boundary ∂(R
this is to look at the missing edges in the picture. If all the edges to t from the boundary ∂(R
B)
are unsaturated, then we must have a solution, as the other edges could not have increased the
ﬂow.

∪

∪

∪

Of course, there may still be saturated edges in the boundary, but this suggests a simple

algorithm. To state it, let GR,σ,δ(B) be the s,t MaxFlow problem with
∂(R

B),

B

s, t
{

• all vertices
R
∪
∪
• all edges from the source s to nodes in set R,
• all edges with nodes in the set B
• all edges from nodes in R

∂(R
B to nodes in V .

} ∪

∪

B) to the sink node t,

∪

∪

∪
We iteratively grow B by nodes whose edges to t are saturated in a MaxFlow solve on GR,σ,δ(B),
starting with B empty. This procedure is described in Algorithm 8.2. It uses the idea of solving
MaxFlow problems consistently with previous iterations, to which we will return shortly. What
this means is that among multiple optimal solutions, we choose the one that would saturate edges
to B in the same way as previous solutions. There is a simple way to enforce this by using the
residual graph, and this really just means that once a node goes into B, it stays in B.

Locally ﬁnding the set B is, in a nutshell, the idea behind strongly local algorithms for
LocalFlowImprove. These strongly local algorithms construct the set B for each subproblem solve
by doing exactly what we describe here, along with a few small ideas to make them go fast. The
algorithms to accomplish this will always produce a set B whose size is bounded in terms of σ
and vol(R), as guaranteed by the following result.

LEMMA 8.4 (Lemma 4.3 (Orecchia and Zhu, 2014)) We have vol(B)
for the iteratively growing procedure in Algorithm 8.2 .

≤

1
σ vol(R) for every iteration

stR1R2R3R4R5a1a2a3U1U2U3U4R¯RB∂(R∪B)stR1R2R3R4R5a1a2a3U1U2U3U4R¯RB∂(R∪B)∅

Algorithm 8.2 MaxFlowSimpleLocal (Veldt, Gleich, and Mahoney, 2016)
1: Set B :=
2: while the following procedure has not yet returned do
3:

Solve the MaxFlow problem on GR,σ,δ(B) consistent with previous iterations.
Let J denote the vertices in ∂(R
if
J
|
else B

= 0 then return the MinCut set S as the solution

B) whose saturated edges to the sink t.

J and repeat

4:

5:

6:

B

∪

|

←

∪

47

Figure 10 – We demonstrate a ﬂow that starts from the source node s and ends to the sink node t. This ﬂow
includes the paths (s,b,d,t), (s,a,c,t) and (s,e,t). Note that this ﬂow is a blocking ﬂow since every path from s to t
includes at least one saturated edge. It is not a maximum ﬂow because there is a path from t to s with reversed
edges.

Proof The proof follows because each time a node v is added to B, we know there was a ﬂow
that saturated the edge with weight σδdv. Since the total ﬂow from s is δ vol(R), this implies
that if vol(B)
vol(R)/σ, then we have expanded enough edges to t to guarantee that the ﬂow
can be fully realized with no bottlenecks.
(cid:4)

≥

8.2 Blocking Flow
In each iteration of Algorithm 8.2, we need to identify the set J. We motivated this set with
a maximum ﬂow on the graph GR,σ,δ(B). It turns out that we do not actually need to solve a
MaxFlow problem. Instead, the concept of a blocking ﬂow suﬃces. The diﬀerence is subtle but
important. A blocking ﬂow is a ﬂow such that every path from the source to the sink contains
at least one saturated edge. For a demonstration of a blocking ﬂow, see Figure 10. See also the
helpful descriptions in Williamson (2019, Chapter 4). By this deﬁnition, a maximum ﬂow is
always a blocking ﬂow because the source and sink are disconnected in the residual graph.

The relevance of blocking ﬂows is that after ﬁnding a blocking ﬂow and looking at the residual
graph, then the distance from the source and sink increases by one. This is essentially what we
do with the set B and J. If B is not yet optimal and we ﬁnd nodes J in Algorithm 8.2 then the
length of the next path from the source to the sink found to become optimal must increase by
one. Blocking ﬂow algorithms use this property, along with other small properties of the residual
graph, to accelerate ﬂow computations. We defer additional details to Williamson (2019) in the
interest of space.

The best algorithm for computing blocking ﬂows has been suggested in Sleator and Tarjan
(1983). There, the authors proposed a link-cut tree data structure that is used to develop a strongly
polynomial time algorithm for weighted graphs that computes blocking ﬂows in
(m log n) time,
where m is the number of edges in the given graph. Blocking ﬂows are a major tool and subroutine
inside other solvers for MaxFlow problems. For example, Dinic’s algorithm (Dinitz, 1970), simply
runs successive blocking ﬂow computations on the residual graph to compute a maximum ﬂow
(see Williamson (2019, Algorithm 4.1) as well). This iteratively ﬁnds the maximum ﬂow up to a

O

steacbd4/43/31/120/24/60/60/20/23/53/30/34/61/148

distance d. Here, they serve the purpose of giving us a lower bound on the maximum ﬂow that
could saturate some edges of the graph.

8.3 The SimpleLocal subsolver
For the SimpleLocal subsolver, we will use the concept of local bottleneck graph GR(B) that
was introduced in Section 8.1. (We omit σ, δ for simplicity.) The only other idea involved is
that we can iteratively update the entire ﬂow itself using the residual graph. So, rather than
solving MaxFlow at each step, we compute a blocking ﬂow to ﬁnd new elements J and update
the residual graph. This ensures that the ﬂow between iterations is consistent in the fashion we
mentioned in Algorithm 8.2. The algorithm is presented in Algorithm 8.3. SimpleLocal is exactly
Dinic’s algorithm but specialized for our LocalFlowImprove problem.

Algorithm 8.3 SimpleLocal (Veldt, Gleich, and Mahoney, 2016)
1: Initialize the ﬂow variables f to zero and B :=
2: while True do
3:

∅
Compute a blocking ﬂow ˆf for the residual graph of G(B) with the ﬂow f , if ﬂow is zero,
then stop.
f + ˆf
f
Let J denote the vertices in ∂(R
the new ﬂow variables f .

B) whose edges to the sink node t get saturated using

4:

5:

←

∪

6: B
7: The current ﬂow variables f are optimal for the MaxFlow in G(B), return a MinCut set S

←

B

J

∪
on the source side s.

THEOREM 8.5 (Iteration complexity and running time for SimpleLocal) Let G be an undirected, connected
graph with non-negative weights. SimpleLocal requires (1 + 1
σ ) vol(R) iterations to converge to the
optimal solution of the MaxFlow subproblem and
σ ) vol(R))] running
time.

σ )2 log[(1 + 1

(vol(R)2(1 + 1

O

Proof Dinic’s algorithm converges in at most (1+ 1
σ ) vol(R) iterations (Proposition A.1 in Orecchia
and Zhu (2014) and Lemma 4.3 of Orecchia and Zhu (2014)). Each iteration requires a blocking
ﬂow operation which costs
σ ) vol(R)]) time (Lemma 4.2 of Orecchia
and Zhu (2014)). Hence, SimpleLocal requires

σ ) vol(R) log[(1 + 1

((1 + 1

O

(vol(R)2(1 + 1

σ )2 log[(1 + 1

σ ) vol(R)]) time.

(cid:4)

O

In (Veldt, Gleich, and Mahoney, 2016), SimpleLocal is described using MaxFlow to compute
the blocking ﬂows in Step 3 of Algorithm 8.3. We also used Dinkelbach’s algorithm instead of
binary search. Otherwise, however, the two algorithms are identical. In practice, both of those
modiﬁcations result in faster computations, although they are slower in theory.

8.4 More sophisticated subproblem solvers
There are more advanced solvers for the LocalFlowImprove algorithm possible in theory. For
instance, Orecchia and Zhu (2014) also presents a solver based on the Goldberg-Rao push relabel
method (Goldberg and Rao, 1998) that will yield a strongly local algorithm. Finally, note that
the goal in using these algorithms is often to minimize the conductance of a set S instead of
the relative conductance φR,σ(S), in which case relative conductance is just a computationally
useful proxy. In the analysis of Orecchia and Zhu (2014), they show that running algorithm
Algorithm 8.3 for a bounded number of iterations will either return a set S that minimizes the
relative conductance exactly, or ﬁnd an easy-to-identify bottleneck set S(cid:48) that has conductance
2δ. Using this second property, they are able to relate the runtime of the algorithm to
φ(S(cid:48))
the conductance of the set returned for a slightly diﬀerent type of guarantee than exactly solving
the LocalFlowImprove subproblem (Orecchia and Zhu, 2014, Theorem 1a).

≤

49

Part III. Empirical Performance and Conclusion

9 Empirical Evaluation
In this section, we provide a detailed empirical evaluation of the cluster improvement algorithms
we have been discussing. The focus of this evaluation is on illustrating how the methods behave
and how they might be incorporated into a wide range of use cases. The speciﬁc results we show
include the following.

1. Reducing conductance. (Section 9.1.) Flow-based cluster improvement algorithms are
eﬀective at ﬁnding sets of smaller conductance near the reference set—as the theory promises.
This is illustrated with examples from a road network, see Figure 12 and Table 3, where the
algorithm ﬁnds geographic features to make the conductance small, as well as on a data-deﬁned
graph from astronomy, see Figure 13 and Figure 14. We also illustrate empirically Theorem 3.1,
which states that FlowImprove and LocalFlowImprove always return smaller conductance
sets than MQI. In our experiments, these improvement algorithms commonly return sets of
nodes in which the conductance is cut in half, occasionally reducing it by up to one order of
magnitude or more.

2. Growing and shrinking. (Section 9.2.) Flow-based improvement algorithms are useful for
the target set recovery task (basically, the task of ﬁnding a desired set of vertices in a graph,
when given a nearby reference set of nodes), even when the conductance of the input is not
especially small. In particular, we show how these methods can grow and shrink input sets to
identify these hidden target sets when seeded nearby, by improving precision (the fraction of
correct results) or recall (the fraction of all possible results). In this case, we use a weighted
graph constructed from images, where the goal is to identify an object inside the image, see
Figure 15. We also use a social network, where the goal is to identify students with a speciﬁc
class year or major within the Johns Hopkins school community, see Figure 16.

3. Semi-supervised learning. (Section 9.3.) Going beyond simple unsupervised clustering
methods, semi-supervised learning is the task of predicting the labels of nodes in a graph,
when the nearby nodes share the same label and when given a set of true labels. Flow-based
improvement algorithms accurately reﬁne large collections of labeled data in semi-supervised
learning experiments. Our experiments show that ﬂow algorithms are eﬀective for this task,
moreso when one is given large collections of true labels, and somewhat less so when one is
given only a small number of true labels, see Figure 17.

4. Scalable implementations. (Section 9.4.) Our software implementations of these algorithms
can be used to ﬁnd thousands of clusters in a given graph in parallel. These computations
scale to large graphs, see Table 4. The implementations we have use Dinkelbach’s method and
Dinic’s algorithm for exact solutions of the MaxFlow problems.

5. Locally-biased ﬂow-based coordinates. (Section 9.5.) We can use our ﬂow improvement
algorithms to deﬁne locally-biased coordinates or embeddings, in a manner analogous to how
global spectral methods are often used to deﬁne global coordinates or embeddings for data,
see Figure 19 and Figure 20. This involves a novel ﬂow-based coordinate system that will
highlight subtle hidden structure in data that is distinctly diﬀerent from what is found by
spectral methods, as illustrated on road networks and in the spectra of galaxies.

To simplify and shorten the captions, throughout the
remainder of this section, we will use the abbreviations
mqi, fi (FlowImprove), and lfi (LocalFlowImprove). Be-
cause lfi depends on a parameter δ, we will simply write
lfi-δ, e.g., lfi-1.0. The formal interpretation of this pa-
rameter is LocalFlowImprove(R, σ = vol(R)/ vol( ¯R)+δ),
where δ is a non-negative real number. Recall that lfi-0.0
is equivalent to fi and lfi-

is equivalent to MQI.

∞

Aside 9. Large set results.
The
set found by fi and lfi may not have
vol(S) ≤ vol( ¯S). For instance, the fi re-
sult in Figure 12(d) has vol(S) > vol( ¯S).
In our computer codes, we always give
vol(S) ≤ vol( ¯S) and ﬂip S and ¯S to force
this property. Figure 12(d) reverses this
ﬂip to show the relationship with R.

50

Figure 11 – The US National Highway Network as a simpliﬁed graph has 51, 144 nodes and 86, 397 undirected edges.
Edges represent roads and are shown as orange lines, nodes are places where roads meet and are shown as black
dots. This display highlights major topographical features as well as major rivers. Mountain ranges, rivers, and
lakes create interesting ﬁne-scale features for our ﬂow algorithms to ﬁnd. There are also dense local regions around
cities akin to small well-connected pieces of social networks.

9.1 Flow-based cluster improvement algorithms
reduce conductance
The ﬁrst result we wish to illustrate is that the algorithms mqi, fi, and lfi reduce the conductance
of the input reference set, as dictated by our theory. For this purpose, we are going to study the
US highway network as a graph (see Figure 11). Edges in this network represent nationally funded
highways, and nodes represent intersections. There are ferry routes included, and there exist
other major roads that are not in this data. This network has substantial local and small-scale
structure that makes it a useful example. It has a natural large-scale geometry that makes it
easy to understand visually. And there are large (in terms of number of nodes) good (in terms of
conductance) partitions of this network.

We create a variety of reference sets for our ﬂow improvement methods to reﬁne. In Figure 12,

nodes in black show a set and purple edges with white interior show the cut.

• We start with two partitions, one horizontal and one vertical (Figures 12(a) and 12(c)) of
the network. These are simple-to-create sets based on using latitude and longitude, and
they roughly bisect the country into two pieces. They are also inherently good conductance
sets. This is due to the structure of roads on the surface of the Earth: they are tied
to the two-dimensional geometric structure, and thus they have good isoperimetric or
surface-to-volume properties

• Next, we consider a large region in the western US centered on Colorado (Figure 12(e)).
Again, this set is shown in black, and the purple edges (with white interior) highlight the
cut. The rest of the graph shown in orange.

• We further consider using the vertices visited in 200 random walks of length 60 around the
capital of Virginia (Figure 12(g)). This example will show our ability to reﬁne a set which,
due to the noise in the random walks, is of lower quality.

• Finally, we consider the result of the metis program for bisection, which represents our
ability to reﬁne a set that is already high quality. This is not shown because it looks visually

Input

Horiz.

cut

233

vol

size

cond.

85335

25054

0.0027

Vert.

131

72780

21552

0.0018

Colorado
region

195

23377

6982

0.0083

Virginia
random walks

112

1344

393

0.0833

metis

56

85926

25422

0.0007

51

Result

Alg.

mqi
fi

mqi
fi

mqi
lfi-1.0
lfi-0.1
fi

lfi-1.5
lfi-1.0

lfi-0.1

mqi
lfi-0.1
fi

cut

vol

size

cond.

ratio

12
29

29
42

9
97
101
42

23
24

26

42
42
42

9852
35189

35195
84582

1799
23617
26613
84204

1067
1212

2763
10471

10473
25030

506
7037
7941
24916

312
357

0.0012
0.0008

0.0008
0.0005

0.0050
0.0041
0.0038
0.0005

0.022
0.0198

225%
330%

220%
365%

167%
203%
220%
1672%

386%
420%

1938

572

0.0134

621%

84594
84594
84594

25034
25034
25034

0.0005
0.0005
0.0005

131%
131%
131%

Table 3 – The results of applying our algorithms to input sets of various quality for the graph of Figure 12. A few
of the sets and cuts are illustrated in Figure 12. All of the methods reduce the conductance score considerably, with
improvement ratios from 131% to 621%. The smallest improvements happen when the input is high-quality, such
as the output from metis.

indistinguishable from Figure 12(d), although the cut and volume are slightly diﬀerent, as
discussed below and in Table 3.

The conductance improvement results from a number of our algorithms are shown in Table 3
and Figure 12. This table shows additional results that are not present in the ﬁgures. We
make several observations. First, as given by mqi, the optimal subset of the horizontal split
of Figure 12(a) identiﬁes a region in the lower US, speciﬁcally, the southern California region
around Los Angeles, San Diego, and Santa Barbara (Figure 12(b)). The southern California
area is separated by mountains and deserts that are spanned by just 12 national highways to
connect to the rest of the country. Second, the result of fi on the vertical split of Figure 12(c)
of the US traces the Mississippi, Ohio, and Wabash rivers up to Lake Michigan (Figure 12),
splitting just 42 highways and ferry routes. Note that although we start with the reference on
the east coast, the set returned by the algorithm is entirely disjoint. This is because optimizing
the FlowImprove objective expanded the set to be larger than half the volume, which caused
the returned set to ﬂip to the other coast. Third, the region around Colorado in Figure 12(e) is
reﬁned by lfi-1.0 to include Dallas (which was split in the initial set) and follows the Missouri
river up into Montana. Finally, a set of random walks around the Virginia capital visit much of
the interior region of the state, albeit in a noisy fashion. Using lfi-1.0 (Figure 12(h)) reﬁnes the
edges of this region to reduce conductance. Reducing δ to 0.1 and using lfi-0.1 (Figure 12(i))
results in a bigger set that includes the nearby city (and dense region) of Norfolk. Note that, for
the high quality metis partition, all of our algorithms return exactly the same result. (Again,
these are not shown because the results are indistinguishable.) We also note that this set is the
overall smallest conductance result in the entire table because the volume is slightly larger than
vertical split experiments.

Overall, these results show the ability of our ﬂow improvement algorithms to improve conduc-
tance by up to a factor of 16 in the best case scenario and by a ratio of 1.31 on the high quality
metis partition. The most useful summary from these ﬁgures are as follows:

• Reducing the value of δ in lfi corresponds to ﬁnding smaller conductance sets compared to
mqi. We also observe that reducing δ in lfi results in larger clusters in terms of number of
nodes and volume.

• As predicted by Theorem 3.1, the results for lfi and fi are always better in terms of

conductance than mqi in terms of conductance.

52

(a) A simple horizontal split φ = 0.002 −→ (b) mqi ﬁnds Southern California φ = 0.0012

(c) A simple vertical split φ = 0.0018 −→ (d) fi (*) ﬁnds rivers and lakes φ = 0.0005

(e) A region around Colorado φ = 0.008 −→(f) lfi-1.0 tracks a river and adds Dallas φ =

0.004

(g) Random walks φ = 0.083

(h) lfi-1.0 gives φ = 0.020
Figure 12 – Our ﬂow-based cluster improvement algorithms reduce the conductance of simple input sets by ﬁnding
natural features including mountains, rivers, and cities. The purple edges highlight the boundary of the set shown
in black nodes, and φ is the conductance of the depicted set. Panel (a) shows an input that cuts horizontally the
map and (b) is the corresponding output of mqi. Panel (c) shows an input that cuts the map vertically and (d)
shows the output of fi. Panel (e) shows an input which corresponds to a large region in the western US centered
on Colorado and (f ) shows the output of lfi. Finally, panel (g) shows an input around the capital of Virginia,
which has been created using random walks, and (h) and (i) are the corresponding output of lfi-1.0 and lfi-0.1,
respectively.

(i) lfi-0.1 gives φ = 0.013

(*) See the large set results aside (Aside 9).

53

(a) The full graph

(b) Zoom into top-right

Figure 13 – The Main Galaxy Sample (MGS) dataset has 517,182 nodes and 32,229,812 edges. This display shows
an eigenvector embedding of the graph along with edges shown in light blue. The edges dominate the visualization
in parts and nodes are only shown where there is suﬃcient density. The node color is determined by the horizontal
coordinate (pink to orange). The right part of the visualization (dark orange to light orange coordinates in b) hints
at structure hidden within the upper band, which we will study in Section 9.5.

While visually useful to understand our algorithms, obtaining such results on a road network is
less useful and less interesting than obtaining similar results on graphs representing data with fewer
or diﬀerent strutural constraints. Thus, we now illustrate these same points in another, larger
dataset with a study of around 2500 improvement calls. This second dataset is a k = 32-nearest
neighbor graph constructed on the Main Galaxy Sample (MGS) in SDSS Data Release 7. We
brieﬂy review the details of this standard type of graph construction and provide further details
in Appendix A. This data begins with the emission spectra of 517,182 galaxies in 3841 bands. We
create a node for each galaxy and connect vertices if either is within the 16 closest vertices to the
other based on a Euclidean distance-like measure (see Appendix A). The graph is then weighted
proportional to this distance. The result is a weighted undirected graph with 517,182 nodes
and 15,856,315 edges (and 517,182 self-loops) representing nearest neighbor relationships among
galaxy spectra. Figure 13 provides a visualization of a global Laplacian eigenvector embedding of
this graph. For more details on this dataset, we refer readers to Lawlor, Budavári, and Mahoney
(2016b,a).

In this case, we compute reference sets using seeded PageRank using a random node, followed
by a sweepcut procedure by Andersen, Chung, and Lang (2006) to locally optimize the conductance
of the result. Consequently, the reference sets we start with are already fairly high quality. Then
we run mqi, lfi-1, lfi-0.1, and lfi-0.01 on the results. We repeat this experiment 2526 times.
The output to input conductance ratio is shown in Figure 14 with reference to the original
reference conductance from seeded PageRank (Figure 14(a)) and also with reference to the MQI
conductance (Figure 14(b)). Like the previous experiments with the road network, reducing δ
in this less easily visualizable data set results in improved conductance. Also like the previous
experiments, lfi always reduces the conductance more than mqi.

The point of these initial experiments is to demonstrate that these algorithms achieve their
primary goal of ﬁnding small conductance sets in a variety of scenarios. They can do so both in a
graph with an obviously geometric structure as well as in a graph without an obvious geometric
structure that was constructed from noisy observational data. In addition, they can do so starting

54

(a) Conductance improvement relative to seeded PageR-
ank

(b) Conductance improvement relative to MQI

(c) Conductance improvement relative to 2-hop BFS

(d) Conductance improvement relative to MQI

Figure 14 – A summary of 3102 (top row) and 2585 (bottom row) experiments in the MGS dataset that show (i) that
reducing δ in lfi produces sets of smaller conductance, when the input set is a of another conductance minimizing
procedure (top row) or 2-hop BFS set (bottom row), and also (ii) that lfi and fi always ﬁnd smaller conductance
sets than mqi. The inset ﬁgures shows the cumulative density function (CDF) of the probability density.

from higher or lower quality inputs. In the next section, we are going to evaluate our algorithms
on speciﬁc tasks where ﬁnding small conductance sets is not the end goal.

9.2 Finding nearby targets by growing and shrinking
Another use for cluster improvement methods is to recover a hidden target set of vertices from a
nearby reference set, e.g., a conjectured subregion of a graph, or a coherent section of an image.
The goal here is accuracy in returning the vertices of this set, and we can measure this in terms
of precision and recall. Let T be a target set we seek to ﬁnd and let S be the set returned by
, which is the fraction of results that were
the algorithm. Then the precision score is
S
|
|
correct, and the recall score is
, which is the fraction of all results that were obtained.
T
|
|
The ideal scenario is that both precision and recall are near 1.

S
|

/
|

/
|

∩

∩

S

T

T

|

We begin by looking at the simple scenario when the initial reference R is entirely contained
within T , and also a scenario when R is a strict superset of T . This setting allows us to see
how the ﬂow-based algorithms grow or shrink sets to ﬁnd these targets T , and it gives us a
useful comparison against simple greedy improvement algorithms as well as against spectral
graph-based approaches. For simplicity of illustration, we examine these algorithms on weighted
graphs constructed from images. The construction of a graph based on an image is explained in
Appendix B.

The results of the experiment are shown in Figure 15. We consider three distinct targets
within a large image, as shown in Figure 15(a) and Figure 15(b): the left dog, middle dog, and
right person. In our ﬁrst case, the reference is entirely contained within the target. In this case,
we can use either fi or lfi to attempt to enlarge to the target. (Note that we cannot use mqi, as
the target set is larger than the seed set.) For comparison, we use a seeded PageRank algorithm
as well. Our choice of this algorithm largely corresponds to replacing
in the ﬂow-based
(cid:107)
as discussed in Section 7.5. We use two seeded
objective with the minorant function

Bx

(cid:107)1

Bx

(cid:107)

2
2
(cid:107)

0.00.20.40.60.81.0(Improved) / (Seeded PR)0123456Probability density0.00.51.001CDFMQILFI (=0.01)LFI (=0.1)LFI (=1)0.00.20.40.60.81.0(LocalFlowImprove) / (MQI)0123456Probability density0.00.51.001CDFLFI (=0.01)LFI (=0.1)LFI (=1)0.00.20.40.60.81.0(Improved) / (2-hop BFS)0123456Probability density0.00.51.001CDFMQILFI (=0.01)LFI (=0.1)LFI (=1)0.00.20.40.60.81.0(LocalFlowImprove) / (MQI)0123456Probability density0.00.51.001CDFLFI (=0.01)LFI (=0.1)LFI (=1)55

PageRank scenarios that correspond to both fi and lfi, see Figure 15(c) to Figure 15(f). These
show that spectral methods that grow tend either ﬁnd a region that is too big or fail to grow
large enough to capture the entire region. This is quantiﬁed by a substantial drop in precision
compared with the ﬂow method. Second, we consider the case when the target is contained within
the reference set. This corresponds to the mqi setting as well as a variation of spectral clustering
that called Local Fiedler (Chung, 2007b) (because it uses the eigenvector with minimal eigenvalue
in a submatrix of the Laplacian). The results are in Figure 15(g) and Figure 15(h), and they
show a small precision advantage for the ﬂow-based methods (see the text below each image).
Finally, for reference, in Figure 15(i) and Figure 15(j), we also include the results of a purely
greedy strategy that grows or shrinks the reference set R to improve the conductance. This is
able to ﬁnd reasonably good results for only one of the test cases and shows that these sets are
not overly simple to identify, e.g., since they cannot be detected by algorithms that trivially grow
or expand the seed set. Let us also note that the results here measure

Next, we repeat these target set experiments using the Johns Hopkins network, a less
visualizable network, for which we see similar results. The data are a subset of the Facebook100
data-set from Traud et al. (2011); Traud, Mucha, and Porter (2012). The graph is unweighted. It
represents “friendship” ties and it has 5157 nodes and 186572 edges. This dataset comes along
with 6 features: major, second major, high school, gender, dorm, and year. We construct two
targets by using the features: students with a class year of 2009 and student with major id 217.
The visualization shows that major id 217 looks like it will be a fairly good cluster as the graph
visualization has moved the bulk away. However, the conductance of this set is 0.26. Indeed,
neither of these sets is particularly small conductance, which makes the target identiﬁcation
problem much harder than in the images. Both sets are illustrated in Figure 16(a).

Here, we use a simple breadth ﬁrst search (BFS) method to generate the input to mqi and
lfi to mirror the previous experiment with images. Given a single and arbitrary node in the
target cluster, we generate a seed set R by including its neighborhood within 2 hops. Like the
previous examples then, we use mqi to reﬁne precision and lfi to boost recall. We repeat this
generating and reﬁning procedure for 25 times for distributional statistics. The inputs as well as
results from mqi and lfi are shown in Figure 16(b) to Figure 16(h). The colors show the regions
that are excluded (black) or included (red or blue) by each input set or algorithm over 25 trials.
We ﬁrst summarize in Figure 16(b) the increase in precision for mqi and the increase in
both precision and recall for lfi. The remaining ﬁgures illustrate the sets on top of the graph
layout showing where the error occurs or regions missed over these 25 trials. In particular, in
Figure 16(c) and Figure 16(f) we illustrate the BFS input for the target clusters. These inputs
include a lot of false positives. In Figure 16(d) and Figure 16(g) we illustrate the corresponding
outputs of mqi. Note that mqi removes the most false positives by contracting the input set. But
the outputs of mqi can have low recall as mqi can only shrink the input set. In Figure 16(e) and
Figure 16(h) we show that lfi is able to both contract and expand the input set and it obtains a
good approximation to the target cluster.

This particular set of examples is designed to illustrate algorithm behavior in a simpliﬁed and
intuitive setting. In both cases we see results that reﬂect the interaction between the algorithm
and the data transformation. For the images, we create a graph designed to correlate with
segments, then run an algorithm designed to improve and ﬁnd additional structure. For the social
network, we simply take the data as given, without any attempt to change it to make algorithms
perform better. However, with more speciﬁc tasks in mind, we would suggest altering the graph
data in light of the targeted use as in (Gleich and Mahoney, 2015; Benson, Gleich, and Leskovec,
2016; Peel, 2017).

9.3 Using ﬂow-based algorithms for semi-supervised learning
Semi-supervised learning on graphs is the problem of inferring the value of a hidden label on all ver-
tices, when given a few vertices with known labels. Algorithms for this task need to assume that the
graph edges represent a high likelihood of sharing a label. For instance, one of the datasets we will
study

the MNIST

the MNIST

data.

Each

node

in

is

56

(a) The full image

(b) The targets

(c) fi

(d) Seeded PageRank, ρ = 10−12

(e) lfi-0.3

(f) Seeded PageRank, ρ = 10−6

(g) mqi

(h) LocalFiedler

(i) Greedy Grow

(j) Greedy Shrink

Figure 15 – Illustration of ﬁnding targets within an image (a) corresponding to the three low-conductance regions
shown in (b). The reference sets given to mqi, fi, and lfi are given by the yellow regions, which either need
to be grown or shrunk to ﬁnd the target. For growing, we compare against seeded PageRank, which is a spectral
analogue of fi and lfi; for shrinking, we compare against a local Fiedler vector, a spectral analogue of MQI, as
well as simple greedy approaches for both. The ﬂow-based methods capture the borders nicely and give high recall
for growing and high precision for shrinking. Among other things, in this case, fi grows too large on the right
person (c) whereas lfi (e) captures this target better. RC stands for recall and PR stands for precision.

graph represents an image of a handwritten digit, and
edges connect two images based on a nearest neighbor
relationships. The idea is that images that show similar
digits should share many edges. Hence, knowing a few
node labels would allow one to infer the hidden labels.
Note that this is a related, but distinct, problem to

Aside 10. If edges from the graph do not
represent a high likelihood of a shared at-
tributed, then there are scenarios where
the graph data itself can be transformed
such that this becomes the case (Peel,
2017).

right personleft dogmiddle dogPR=0.970, RC=0.930=0.0035PR=0.975, RC=0.898=0.0017PR=0.627, RC=0.937=0.0001PR=0.706, RC=0.916=0.0170PR=0.932, RC=0.836=0.0201PR=0.887, RC=0.917=0.0017PR=0.970, RC=0.930=0.0035PR=0.975, RC=0.898=0.0017PR=0.938, RC=0.913=0.0008PR=0.839, RC=0.794=0.0358PR=0.937, RC=0.796=0.0234PR=0.972, RC=0.664=0.0368PR=0.970, RC=0.930=0.0035PR=0.975, RC=0.898=0.0017PR=0.916, RC=0.933=0.0008PR=0.959, RC=0.944=0.0040PR=0.975, RC=0.899=0.0018PR=0.929, RC=0.923=0.0009PR=0.965, RC=0.502=0.0367PR=0.987, RC=0.407=0.0361PR=0.939, RC=0.772=0.0166PR=0.356, RC=0.999=0.0626PR=0.272, RC=0.999=0.0595PR=0.234, RC=0.999=0.036757

Feature

Vol.

Major-217
Class-2009

10696
32454

Size

200
886

Cond.

0.26
0.19

(a) Target sets for Johns Hopkins

Target

Set

Size

Cond.

Prec.

Rec.

Major-217 Input 1282
mqi Result 203
lfi Result 218

Class-2009 Input 1129
mqi Result 472
lfi Result 802

0.58
0.19
0.18

0.52
0.29
0.18

0.15
0.90
0.88

0.35
0.96
0.94

0.94
0.90
0.95

0.51
0.50
0.83

(b) Median statistics on input sets as well as mqi and lfi
results

(c) Class-2009 Input

(d) Class-2009 mqi

(e) Class-2009 lfi

(f) Major-217 Input

(g) Major-217 mqi

(h) Major-217 lfi

Figure 16 – The mqi and lfi-0.1 algorithms can also ﬁnd target sets in the Johns Hopkins Facebook social network,
even though they are fairly large conductance, which makes them more challenging. The algorithms use as the
reference set a simple 2-hop BFS sets with low precisions starting from a random target node. The layout for this
graph has been obtained using the force-directed algorithm, which is available from the graph-tool project (Peixoto,
2014). The colors show the regions that are excluded (black) or included (red or blue) by each input set or algorithm
over 25 trials.

the target set identiﬁcation problem (Section 9.2). The
major diﬀerence is that we need to handle multiple values of a label and produce a value of the
label for all vertices.

An early paper on this topic suggested that MinCut and ﬂow-based approaches should be
useful (Blum and Chawla, 2001). In our experiments, we compare ﬂow-based algorithms lfi and
fi with seeded PageRank, and we ﬁnd that the ﬂow-based algorithms are more sensitive to an
increase in the size of the set of known labels. In the coming experiments, this manifests as an
increase in the recall while keeping the precision ﬁxed. For these experiments, mqi is not a useful
strategy, as the purpose is to grow and generalize from a ﬁxed and known set of labels to the rest
of the graph.

There are three datasets we use to evaluate the algorithm for semi-supervised learning: a

synthetic stochastic block model, the MNIST digits data, and a citation network.

• SBM. SBM is a synthetic stochastic block model network. It consists of 6000 nodes in three
classes, where each class has 2000 nodes. The probability of having a link between nodes in
the same class is 0.005 while the probability of having a link between nodes in diﬀerent
classes is 0.0005. The one we use in the experiment has 36102 links. By our construction,
the edges preferentially indicate class similarity.

58

• MNIST. MNIST is a k-NN (nearest neighbor) network (Lecun et al., 1998). The raw data
consists of 60000 images. Each image represents a handwritten sample of one arabic digit.
Thus, there are 10 total classes. In the graph, each image is represented by a single node
and then connected to its 10 nearest neighbors based on Euclidean distance between the
images when represented as vectors of greyscale pixels. We assume that edges indicate class
similarity.

• PubMed. PubMed is a citation network (Namata et al., 2012).

It consists of 19717
scientiﬁc publications about diabetes with 44338 citations links. Each article is labeled with
one of three types. By our assumption, articles about one type of diabetes cite others about
the same type more often.

The experiment goes as follows: for each class, we randomly select a small subset of nodes,
and we ﬁx the labels of these nodes as known. We then run a spectral method or ﬂow method
where this set of nodes is the reference. We vary the number of labeled nodes included from 0.5%
to 15% of the class size. For each ﬁxed number of labeled nodes, we repeat this 30 times to get a
distribution of precision, recall, and F 1 scores (where F 1 is the harmonic mean of precision and
recall), and we represent an aggregate view of this. For the ﬂow methods, the output is a binary
vector with 1 suggesting the node belongs to the class of reference nodes. Thus, it’s possible that
some nodes are classiﬁed into multiple classes, while some other nodes remain unclassiﬁed. We
consider the ﬁrst case as false positives and the second case as false negatives when computing
precision and recall. For the spectral method, we use the real-valued solution vector to uniquely
assign a node to a class.

The results are in Figure 17 and show that the ﬂow-based methods have uniformly high
precision. As the set of known labels increases, the recall increases, yielding a higher overall F 1
score. Furthermore, the regularization in lfi-0.1 causes the set sizes to be smaller than fi, which
manifests as a decrease in recall compared with fi. In terms of why the ﬂow-based algorithms
have low-recall with small groups of known labels sizes, note Lemma 3.5, which requires that the
cut and denominator to reduce at each step. This makes it challenging for the algorithms and
objectives to produce high recall when started with small sets unless the sets are exceptionally
well-separated.

Improving thousands of clusters on large scale data

9.4
In practice, it is often the case that one might want to explore thousands of clusters in a single
large graph. For example, this is a common task in many computational topology pipelines (Lum
et al., 2013). Another example that requires thousands of clusters is computing the network
community proﬁle (Leskovec et al., 2009, 2008; Leskovec, Lang, and Mahoney, 2010), which shows
a conductance-vs-size result for a large number of sets, as a characteristic feature of a network.
In this section, we will explore the runtime of the ﬂow-based solvers on two small biological
networks and two large social networks, where nodes are individuals and edges represent declared
friendship relationships.

• sﬂd, has 232 nodes and 15570 edges in this graph (Brown et al., 2006).
• ppi, has 1096 nodes and 13221 edges (Pagel et al., 2004).
• orkut, has 3, 072, 441 nodes and 117, 185, 083 edges (Mislove et al., 2007). This data set

can be accessed via Leskovec and Krevl (2014).

• livej, has 4, 847, 571 nodes and 68, 993, 773 edges (Mislove et al., 2007). This data set can

be accessed via Leskovec and Krevl (2014).

The goal will be to enable studies such as those discussed above using instead the ﬂow-based
algorithms as a subroutine on these graphs. To generate seed sets to reﬁne, we use seeded
PageRank. Each input set is the result of a seeded PageRank algorithm on a random node with
a variety of settings to generate sets between a few nodes and up to around 10,000 nodes. For
each resulting set, we then run the mqi, lfi-0.9, lfi-0.6, and lfi-0.3 improvement methods. We
use our code (Fountoulakis et al., 2019b), which has a Python interface and methods that are
implemented using C++, for all of these experiments and runtimes. Our environment has a dual
CPU Intel E5-2670 (8 cores) CPU with 128 GB RAM. We parallelize over individual runs of the

59

Figure 17 – The horizontal axis shows the number of true labels included in the seeds and the plots are aligned with
the tables so you can read oﬀ the F1 score as well as the associated precision and recall for each choice. These
results on semi-supervised learning show that the ﬂow-based methods lfi-0.1 and fi are more sensitive to the
number of known true labels included in the reference seed sets compared with seeded PageRank.

Table 4 – Running times in seconds for generating and improving clusters on small scale biological networks and
large-scale social networks. The input cluster to the ﬂow-based improvement methods is the output of seeded
PageRank. It takes around 20 minutes to generate the input clusters for large scale social networks. Running
the ﬂow-based improvement algorithms takes around the same amount of time, except for lfi-0.3 on LiveJournal,
which takes roughly 30 minutes. The time measurements reﬂect the pleasingly parallel computation of results for
all clusters on a 16-core machine.

graph nodes

edges

sﬂd
ppi

orkut
livej

232
1096

3M
4.8M

16k
13k

117M
69M

Time

seeded
PR

18
46

1130
1057

(s)

mqi

0.5
1

171
105

clusters
found

342
1199

13799
31622

lfi-0.3

lfi-0.6

lfi-0.9

1.7
2.6

838
1940

1.6
2.5

701
1326

1.5
2.3

628
1094

seeded PageRank and ﬂow methods using the Python Multiprocessing module using a common
shared graph structure. Note that each individual run is independent.

In this way, we are able to explore tens of thousands of clusters in around 30-40 minutes, as
we demonstrate in Table 4. There, we present running times for producing the seeded PageRank
sets and then reﬁning it with the ﬂow-based methods. Note that the fastest method is mqi. It is
even faster than the seeded PageRank method that generates the input sets. This is because mqi
only explores the input subcluster, while LocalFlowImprove reaches outside of the input seed set
of nodes. Also, note the dependence of the runtime for lfi on the parameter δ. The larger the
parameter δ for lfi, the smaller the part of the graph that it explores outside of the input set of
nodes. This property is also captured in Table 4 by the running time of lfi.

0.20.40.60.81.0PrecisionPubMedMNISTSBM0.560.540.530.510.530.59SPR0.760.730.750.790.740.760.880.860.870.870.880.860.040.280.570.670.700.71LFIF1 Score0.010.880.960.960.960.960.100.360.770.980.990.990.040.400.670.700.710.71FI 0.240.860.870.870.870.870.100.360.980.990.990.990.51.534.567.50.00.20.40.60.81.0Recall0.51.534.567.513691215Ratio (%)9929659288711831479300900180027003600450060180360540720900No. labelsSPR (Seeded PageRank)LFI (LocalFlowImprove)FI (FlowImprove)60

Figure 18 – A spectral embedding of the US Highway Network corresponding to the ﬁrst and second non-trivial
eigenvectors of the Laplacian matrix. The embedded locations of major cities are labeled as well. Node color is
determined by the true longitude of a node, which shows that the ﬁrst eigenvector of the Laplacian correlates with
an east-west split of the network. This global embedding, however, compresses major regions of the northeastern
US (Washington, New York, Boston) as well as the Western US (Los Angeles, San Diego, Phoenix).

9.5 Using ﬂow-based methods for local coordinates
A common use case for global (but also local (Lawlor, Budavári, and Mahoney, 2016b,a)) spectral
methods on graphs and networks is using eigenvector information in order to deﬁne coordinates
for the vertices of a graph. This is often called a spectral
embedding or eigenvector embedding (Hall, 1970); it
may use two or more eigenvectors directly or with simple
transformations of them in order to deﬁne coordinates
for each node (Ng, Jordan, and Weiss, 2001). The ﬁnal
choice is typically made for aesthetic reasons. An exam-
ple of a spectral embedding for the US highway network
is shown in Figure 18. One of the problems with such global embeddings is that they often squash
interesting and relevant regions of the network into ﬁlamentary structures. For instance, notice
that both the extreme pieces of this embedding compress massive and interesting population
centers of the US on the east and west coast. Alternative eigenvectors show diﬀerent but similar
structure. A related problem is that they smooth out interesting features, making them diﬃcult
to use.

Aside 11. A bigger issue with spectral em-
beddings is that they often produce useless
results for many large networks; see Lang
(2005). Here, we use networks where
these techniques yield interesting results.

Semi-supervised eigenvectors are one way to address this aspect of global spectral embed-
dings (Hansen and Mahoney, 2014; Lawlor, Budavári, and Mahoney, 2016b,a). These seek
orthogonal vectors that minimize a constrained Rayleigh quotient. One challenge in using related
ideas to study ﬂow-based computation is that the solution of ﬂow-problem is fundamentally
discrete and binary. That is, a spectral solution produces a real-valued vector whose entries, e.g.,
for seeded PageRank, can be interpreted as probabilities. We can thus meaningfully discuss and
interpret sub-optimal, orthogonal solutions. Flow computations only gives 0 or 1 values, where
orthogonality implies disjoint sets.

In this section, we investigate how ﬂow-based methods can be used to compute real-valued
coordinates that can show diﬀerent types of structure within data compared with spectral methods.
In the interest of space, we are going to be entirely procedural with our description, justiﬁcation
is provided in Appendix A.

Given a reference set R, we randomly choose N subsets (we use 500-2500 subsets) of R with
exactly k entries; for each subset we add all nodes within distance d and call the resulting sets
called R1, . . . , RN . These serve as inputs to the ﬂow algorithms. For each subset, we compute
the result of a ﬂow-based improvement algorithm, which gives us sets Si. For each Si, we form
an indicator vector over the vertices, xi, where the entry is 1 if the vector is in the set and 0
otherwise. We assemble these vectors as columns of a matrix X, and we use the coordinates
of the dominant two left singular vectors as ﬂow-based coordinates. This procedure is given as
an algorithm in Algorithm 9.1. Note also that this procedure can be performed with spectral
algorithms as well. (See the appendix for additional details.)

Algorithm 9.1 The local ﬂow-based algorithm to generate ﬂow-based coordinates.
Input: A graph G, a set R and parameters
• N : the number of sets to sample
• k: the size of each subset
• d: the expansion distance
• c: the dimension of the ﬁnal embedding
• improve: a cluster improvement algorithm

61

Output: An embedding of the graph into c coordinates for each node
1: Let n be the number of vertices.
2: Allocate X, an n-by-N matrix of zeros.
3: for i in 1 to n do
4:

Let T be a sample of k entries from R at random without replacement
Let Ri be the set of T and also all vertices within distance d from T
Let Si be the set that results from improve(G, Ri)
Set X[v, i] = 1 for all v

7:
∈
8: Compute the rank-c truncated SVD of X and let U be the left singular vectors.
9: Return U , each row gives the c coordinates for a node

5:

6:

Si

The results of using Algorithm 9.1 (see parameters in Appendix A) to generate local coordinates
for a set of vertices on the west coast of the United States Highway Map is shown in Figure 19.
The set of vertices shown in Figure 19(a) is in a region where the spectral embedding compresses
substantial information. This region is shown on a map in Figure 19(b), and it includes major
population centers on the west coast. In Figure 19(c), we show the result of a local spectral
embedding that uses seeded PageRank in Algorithm 9.1, along with a few small changes that are
discussed in our reproducibility section. (Here, we note that these changes do not change the
character of our ﬁndings, they simply make the spectral embedding look better.) In the spectral
embedding, the region shows two key areas: 1. Seattle, Portland, and San Francisco and 2. Los
Angeles, San Diego, and Phoenix. In Figure 19(d), we show the result of the local ﬂow-based
embedding that uses lfi-0.1 as the algorithm. This embedding clearly and distinctly highlights
major population centers, and it does so in a way that is clearly qualitatively diﬀerent from
spectral methods.

We repeat this analysis on the Main Galaxy Sample (MGS) dataset to highlight the local
structure in a particularly dense region of the spectral embedding that was used for Figure 13.
The seed region we use is shown in Figure 20(a) and has 201,252 vertices, which represents almost
half the total graph. We use Algorithm 9.1 (see parameters in the Appendix A) again to get local
spectral (Figure 20(b)) and local ﬂow embeddings (Figure 20(c)). Again, we ﬁnd the the local
ﬂow embedding shows considerable substructure that is useful for future analysis.

As a simple validation that this substructure is real, we use the 2d embedding coordinates as
input to a k-means clustering procedure on both the local spectral and local ﬂow coordinates.
For each cluster that results from this procedure, we compute its conductance. Histograms of
conductance values are shown in Figure 21 for k = 50 and k = 100. Both of these histograms
show consistently smaller conductance values for the ﬂow-based embedding.

10 Discussion and Conclusion
Our goal with this survey is to highlight the advantages and wide utility of ﬂow-based algorithms
for improving clusters. The literature on these methods is much smaller compared with other
graph computation methodologies, despite attractive theoretical beneﬁts. For example, global
spectral methods based on random walks or eigenvectors are ubiquitous in computer science,
machine learning, and statistics. Here, we illustrated similar possibilities for ﬂow based methods.
We have also shown that these local ﬂow based improvement algorithms can scale to very large
graphs, often returning outputs without even touching most of the graph, and that many popular
machine learning and data analysis uses of ﬂow based methods can be applied to them. This is

62

(a) The subset of nodes (left) from the spec-
tral embedding of the US Highway Network
used to compute local embeddings

(b) The same subset shown on a map with
major cities labeled

(c) The local spectral embedding of the US
west coast cities

(d) The local ﬂow embedding of the US west
coast cities

Figure 19 – We select a subset of 7143 nodes that are compressed in the spectral embedding of the US Highway
network (shown in red and blue in ﬁgures (a) and (b)) that represent the majority of major cities on the west coast.
Note that interior cities such as Phoenix and Las Vegas are not included in the set. In (c) and (d) we show the
results of running the pipeline from Algorithm 9.1 to generate local spectral and local ﬂow based embeddings into two
dimensions. The color of a node is determined by its north-south latitude. Note that both include Phoenix and Las
Vegas. The local ﬂow embedding clearly and distinctly delineates clusters corresponding to major population centers
whereas the local spectral embedding shows a smooth view with only two major regions: 1. Northern California to
Seattle and 2. Southern California to Phoenix and Las Vegas.

the motivation behind our software package where these algorithms have been implemented (Foun-
toulakis et al., 2019b). An alternative implementation is available in Julia (Veldt, 2019). These
results and methods open the door for novel analyses of very large graphs.

As an example of these types of novel analyses, note that the fractional ratio δk inside MQI,
FlowImprove, and LocalFlowImprove (Algorithms 6.1, 7.1 and 8.1) can be interpreted as a ratio
between the numerator and denominator. This enables one to search for a value of δ that would
correspond to a given solution. See the ideas in Veldt, Wirth, and Gleich (2019) for how to use
search methods to choose δ for a speciﬁc application of clustering.

In our explanation of the theory behind the methods, we often encountered decisions where
we could have made more general, albeit more complex, statements. Our guiding principle was to
make it easy to appreciate the opportunities for these methods. As an example where there are

63

(a) The seed region

(b) Local spectral embedding

(c) Local ﬂow embedding

Figure 20 – Local spectral and local ﬂow embeddings of the large, 201,252 node, seed region – shown in green in (a) –
that is compressed in the global spectral embedding from Figure 13. In (b), the local spectral shows the nodes colored
with the same color as in Figure 13. Nodes that were not touched by the local embedding are shown with the big
node on the right hand side. In (c), the local ﬂow embedding with the same color scheme and same big node on the
right hand side. Note that the spectral embedding does not show any clear sub-structure besides a top-bottom split.
In contrast, the ﬂow embedding shows a number of pockets of structure indicative of small conductance subsets.

(a) k = 50

(b) k = 100

Figure 21 – A histogram of cluster conductance scores that come from using k-means on the two-dimensional local
spectral and local ﬂow embeddings from Figure 20. These show that the ﬂow embedding produces clusters with
smaller conductance, and they support the intuition from Figure 20 that the additional structure suggested by the
ﬂow embedding reﬂects meaningful sub-structure within the data.

vol( ¯R)
more general results, note that much of the theory of this survey holds where vol(R)
for the seed. For instance, the MQI, FI, and LFI procedures are all well deﬁned algorithms in
vol( ¯R).
this scenarios although our theory statements list the explicit requirement that vol(R)
What happens in these scenarios is that some of the details of runtime and other aspects change.
In terms of another generalization, the methods could have been stated in terms of a general
volume function as noted in Section 3.7. Again, however, this setting becomes more complex
to state for non-integer volume functions and a number of other subtle issues. In these cases,
we sought for the explanations that would make the underlying issues clear and focused on
conductance in order to do that.

≥

≤

There are a number of interesting directions that are worth further exploring. First, in
the theory from this survey, the binary search or bisection-based search methods have superior
worst-case time. However, in practice, these methods are rarely used. For instance our own
implementations always use the Dinkelbach greedy framework. This is because this strategy
commonly terminates in just a few iterations. This was noted in both the MQI and FlowImprove
papers (Lang and Rao, 2004; Andersen and Lang, 2008), yet there is still no theoretically satisfying

0.30.40.50.60.70.80.91.0(kmeans clusters)0246Probability densityLocal Flow EmbeddingLocal Spectral Embedding0.30.40.50.60.70.80.91.0(kmeans clusters)02468Probability densityLocal Flow EmbeddingLocal Spectral Embedding64

explanation. To provide some data, for the LocalFlowImprove experiments in Figure 14, we never
needed to evaluate more than 10 values of the fractional ratio to ﬁnd the optimal solution in
Dinkelbach’s algorithm. As evidence that this eﬀect is real, note that Lemma 3.5 actually shows
that cut(R) is a bound on the number of iterations for Dinkelbach’s algorithm. But for weighted
graphs, this becomes cut(R)/µ, where µ is the minimum spacing between elements (think of the
ﬂoating point machine value ε). A speciﬁc case where this type of insight would be helpful is in
terms of weighted graphs with non-negative weights. Dinkelbach’s algorithm does not appear to
be much slower on such problems, yet the worst-case theory bound is extremely bad and depends
on the minimum spacing between elements.

Another direction is a set of algorithms that span the divide between the fractional program
and the MinCut problem. For instance, it not necessary to completely solve the ﬂow problem
to optimality (until the last step) as all that is needed is a result that there is a better solution
available. This oﬀers a wide space to deploy recent advances in Laplacian-based solvers to handle
the problem – especially because the electrical ﬂow-based solution largely correspond to PageRank
problems. It seems optimistic, but reasonable, to expect good solutions in time that is more like
a random walk or spectral algorithm.

Finally, another direction for future research is to study these algorithms in hypergraphs (Veldt,
Benson, and Kleinberg, 2020b) and other types of higher-order structures. This was a part of
early work on graph-cuts in images that showed that problems where hyperedges had at most
three nodes could be solved exactly (Kolmogorov and Zabih, 2004). More recently, hypergraphs
have been used to identify reﬁned structure in networks based on motifs (Li and Milenkovic,
2017).

In closing, our hope is that this survey and the associated LocalGraphClustering package (Foun-
toulakis et al., 2019b) helps to make these powerful and useful methods—both basic ﬂow-based
analysis of smaller graphs, but especially local ﬂow-based analysis of very large data graphs—more
common in the future.

Part IV. Replicability Appendices and References

A Replicability Details for Figures and Tables
In the interest of reproducibility and replicability, we provide additional details on the methods
underlying the ﬁgures. To replicate these experiments, see our publicly-available code (Foun-
toulakis et al., 2019a). All of the seeded PageRank examples in this survey use an (cid:96)1-regularized
PageRank method (Fountoulakis et al., 2017). We use ρ to denote the regularization parameter
in (cid:96)1-regularized PageRank, α to denote the teleportation parameter in (cid:96)1-regularized PageRank
and δ to denote the parameter of LocalFlowImprove.

The implementations we have use Dinkelbach’s method, Algorithm 3.1, for the fractional
programming problem and Dinic’s algorithm for exact solutions of the weighted MaxFlow problems
at each step. Put another way, for MQI, we use Algorithm 6.1 and Dinic’s algorithm to solve the
MaxFlow problems. For FlowImprove and LocalFlowImprove, we use the Dinkelbach variation
on Algorithm 8.1 with Dinic’s algorithm used to compute blocking ﬂows in Algorithm 8.3. We
use the same implementation for LocalFlowImprove and FlowImprove and simply set δ = 0 for
FlowImprove. Using Dinkelbach’s method and Dinic’s MaxFlow has a worse runtime in theory,
but better performance in practice. The implementations always return the smallest connected
set that achieves the minimum of the objective functions. They also always return a set with less
than half the total volume of the graph.

Figure 1 We use the implementation of the Louvain algorithm in (Aynaud, 2018). We use our
own code to generate the SBM. The code for this experiment is in the notebook sbm_demo.ipynb
in the subdirectory ssl available in (Fountoulakis et al., 2019a).
Figure 2 This is a geometric-like stochastic block model “hybrid”. A short description of the
data-generation procedure follows but the code is the precise description. Create g groups of n
points. Each group is assigned the same random 2d spatial coordinate from a standard mean
0, variance 1 normal distribution. But each node within a group is also perturbed by a mean 0

65

O

(102) and σ2

random normally distributed amount with variance σ. Add p additional points with normally
distributed ρ (a “center” group). These determine the coordinates of all the nodes. Now, add
edges to k nearest neighbors and also within radius (cid:15). For this experiment, we set g = 25,
n = 100, σ = 0.05, p = 2000, ρ = 5, k = 5 and (cid:15) = 0.06. The code is in the Jupyter notebook
Geograph-Intro.ipynb (Fountoulakis et al., 2019a).
Figure 3 The image can be downloaded from van der Walt et al. (2014). The image is
turned into a graph using the procedure described in Appendix B. In particular, we set r = 80,
c = l/10, where l is the maximum between the row and column length of
σ2
p =
the image. The code for this experiment is in the Jupyter notebook astronaut.ipynb in the
subdirectory usroads available in (Fountoulakis et al., 2019a).
Figure 4 The original image is 100 by 100 pixels with a 13 pixel wide by 77 pixel tall vertical
strip and a 77 pixel wide by 25 pixel tall horizontal strip that intersect in a 13 by 25 pixel region.
This setup and intersection produces a rotated T-like shape centered in the 100 by 100 pixel
grid. To generate the noisy, blurred ﬁgure, we used a Moﬀat kernel (Moﬀat, 1969) that arises
from stellar photography (parameters α = 1.5, β = 1.2 and length scale 5) and added uniform
0.1, 0.1] noise (roughly 38% of max blurred value 0.261) for each pixel before scaling by 1/0.3
[
−
and clamping to [0, 1] range. This stellar photography scenario was chosen to simulate a binary
image reconstruction scenario. Let f be noisy, blurry image with values in the range [0, 1]. Let
G be the grid graph associated with the grid underlying the image f . Make sure to read the
Section 6 before reading the details of the reconstruction algorithm. Then we construct the
V with weight δfidi (where d is the degree); connect
following augmented graph: connect s to i
∈
t to i
. We show the mincut solution S for the two values
of δ explained in the problem. This corresponds to a minimization problem similar to (6.3),
namely minimize cut(S)
, which uses a biased notion of
}
|
volume ν(S) = (cid:80)
S difi (as in Section 3.7). In this case, if δ is 0.04, then we can no longer
i
continue improving the result and we end up with the convex set. For δ = 0.11, we have the rough
reconstruction of the original shape. For our own purposes, we used a Julia implementation (Veldt,
2019) of the ﬂow code that is available in the mqi-images subdirectory in (Fountoulakis et al.,
2019a).

∞
S difi subject to S

V where fi = 0 with weight

i
⊆ {

δ (cid:80)
i

fi > 0

−

∈

∈

∈

Figure 12 All details are given in the main text of the survey. The code is in the Jupyter
notebook usroads-figures.ipynb in the subdirectory usroads available in (Fountoulakis et al.,
2019a).

Table 3 This table provides additional details for the results of Figure 12. The code for this
experiment is in the Jupyter notebook usroads-figures.ipynb in the subdirectory usroads
available in (Fountoulakis et al., 2019a).

Figure 13 This dataset has been obtained from Lawlor, Budavári, and Mahoney (2016a). It is
a k = 32-nearest neighbor graph constructed on the Main Galaxy Sample (MGS) in SDSS Data
Release 7. Each galaxy is captured in a 3841-band spectral proﬁle. Each spectra is normalized
based on the median signal over 520 bands selected in Lawlor, Budavári, and Mahoney (2016a).
Since the results are sensitive to this set and it is not available elsewhere, the indices of the bands
were

856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,
882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907,
908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933,
934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 1251,
1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272,
1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293,
1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314,
1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335,
1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356,
1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377,
1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398,
1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419,
1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1948, 1949, 1950, 1951, 1952,
1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973,
1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994,
1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015,
2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114,
2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135,
2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156,

2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177,
2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198,
2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219,
2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240,
2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258

66

We create a node for each galaxy and connect vertices if either is within the 16 closest vertices
to the other based on a Euclidean after this median normalization. The graph is then weighted
proportional to this distance and the distance to the the 8th nearest neighbor based on a k-
nearest neighbor tuning procedure in manifold learning. (The results for spectral embeddings
are somewhat sensitive to this procedure.) Formally, let ρi be the distance to the 8th nearest
if all of these distances are 0). We add a weighted undirected edge based on
neighbor (or
node i to node j with distance di,j as Wi,j = exp(
If i and j are both nearest
neighbors, then we increment the weights, so the construction is symmetric. Each node also
has a self-loop with weight 1. The adjacency matrix of the graph has 32,229,812 non-zeros,
which is 15,856,315 edges and 517,182 self-loops. The code for this experiment is in the Jupyter
notebook hexbingraphplots_global.jl in the subdirectory flow_embedding/hexbin_plots
available in (Fountoulakis et al., 2019a). The full code to process the graph is available upon
request.

(di,j/ρi)).

∞

−

Figure 14 For this experiment we used seeded PageRank to ﬁnd the seed set for the ﬂow
algorithm mqi and lfi. We set the teleportation parameter of the seeded PageRank algorithm to
0.01. The code for this experiment is in the Jupyter notebook plot_cluster_improvement.ipynb
in the subdirectory cluster_improvement available in (Fountoulakis et al., 2019a).
Figure 15 In our experiments constructing the graph from the image, we follow Appendix B
and we set r = 80, σ2
c = l/10, where l is the maximum between the row
and column length of the image. The code for this experiment is in the Jupyter notebook
image_segmentation.ipynb in the subdirectory image_segmentation available in (Fountoulakis
et al., 2019a).

(102) and σ2

p =

O

Figure 16 The input is a 2-hop BFS set starting from a random target node. We independently
generate 25 such BFS sets. The transparency level of red or blue nodes is determined by the
ratio of including each node in the resulting sets. The code for this experiment is in the Jupyter
notebook social.ipynb in the subdirectory social available in (Fountoulakis et al., 2019a).
Speciﬁc details about tuning can also be found in the code.

Figure 17 For every class we randomly select a small percentage of labeled nodes, the exact
percentages are given in the main text. The nodes that are selected from each class are considered
a single seed set. For each seed set and for each class we use seeded PageRank with teleportation
parameter equal to 0.01. This procedure provides one PageRank vector per class. For each
unlabeled node in the graph we look at the corresponding coordinates in the PageRank vectors
and we give to each unlabeled node the label that corresponds to the largest value in the
PageRank vectors. For ﬂow methods, for every labelled node that is used, we run one step of
breadth-ﬁrst-search to expand the single seed node to a seed set. The expanded seed set is used
as input to the ﬂow methods. We ﬁnd a cluster and each node in the cluster is considered to
have the same label as the seed node. Based on this technique, it is possible that one node can
be allocated in more than one classes, we consider such node as false positives. The code for this
experiment is in the Jupyter notebook semisupervised_learning.ipynb in the subdirectory ssl
available in (Fountoulakis et al., 2019a). The MNIST graph was weighted for this experiment.
The distance between two images is computed by a radial basis function with width to be 2. To
robustify the process of rounding diﬀusion vector to class labels, we use a strategy from Gleich
and Mahoney (2015), which involves rounding to classes based on the node with the smallest
rank in the ranked-list of each diﬀusion vector.

Table 4 The code for this experiment is in the Jupyter notebooks in the subdirectory large_
scale available in (Fountoulakis et al., 2019a).
Figure 18 We use the eigenvector of the Laplacian matrix D
A associated with the smallest
non-zero eigenvalues to compute the vectors v1 and v2. The coordinates of the plot are generated

−

67

by assigning x and y based on the rank of a node in v1 and v2 in a sorted order. This has the eﬀect
of stretching out the eigenvector layout, which often compresses many nodes at similar point.
The color of the nodes is proportional to the east-west latitude. The code for this experiment is in
the notebook usroads-embed.ipynb in the subdirectory usroads in (Fountoulakis et al., 2019a).
Figure 19 We use Algorithm 9.1 with N = 500 sets, k = 1, d = 20, c = 2, along with lfi-0.1
as the improve algorithm. For the local spectral embedding, we use the same seeding parameters
with seeded PageRank with ρ = 1e-6. When we create the matrix X for seeded PageRank, we
take the base-10 logarithm of the result value (which is always between 0 and 1). For vertices
with 0 values, we assign them
10, which is lower than any other value. We found that this gave
a more useful embedding and helped the spectral show more structure. The node labeled “Rest
of graph” was manually placed in both because the embedding does not suggest a natural place
for this. Here, we also used the rank of the node in a sorted order, which helps to spread out
nodes that are all placed in exactly the same location. The code for this experiment is in the
Jupyter notebook usroads-embed.ipynb in the subdirectory usroads available in (Fountoulakis
et al., 2019a).

−

Figure 20 We use Algorithm 9.1 with N = 500 sets, k = 1, d = 3, c = 2, along with lfi-0.1
as the improve algorithm. We used the same local spectral methodology as in Figure 19. The
large red node represents the remainder of the graph and all “unembedded nodes,” which is
manually placed to highlight edges to the rest of the graph. Here, we also used the rank of
the node in a sorted order, which helps to spread out nodes that are all placed in exactly the
same location. The code for this experiment is in the Python script flow_embedding.py in the
subdirectory flow_embedding available in (Fountoulakis et al., 2019a) and Jupyter notebooks in
the subdirectory flow_embedding/hexbin_plots available in (Fountoulakis et al., 2019a). The
Python script needs to be run ﬁrst to generate data and then the notebook can be used to
generate the ﬁgures.

Figure 21 The code is in the Jupyter notebooks social.ipynb in the subdirectory flow_
embedding/cond_hists available in (Fountoulakis et al., 2019a). They both need the embedding
results from Figure 20 to generate the ﬁgures.

|

|

The rationale for the the local ﬂow embedding procedure We now brieﬂy justify the
motivation for the structure of the local ﬂow embedding algorithm. The key idea is that spectral
algorithms are based on linear operations: if we have any way of sampling the reference set R
1R, then if f is a linear function – such
with a normalized set indicator T such that E[T ] = 1
R
|
as an exact seeded PageRank computation – we have E[f (T )] = f ( 1
1R). This expectation
R
corresponds to the seeded PageRank result on the entire set. To include another dimension, we
|
would seek to ﬁnd an orthogonal direction to E[f (T )], such as is done with constrained eigenvector
computations. It is this linear function perspective that inspired our ﬂow-embedding algorithm:
collect samples of f (Ti) into a matrix and then use the SVD on the samples of T to approximate
E[f (T )] and the orthogonal component (given by the second singular vector). While some of
these arguments can be formalized and made rigorous for a linear function, that is an orthogonal
discussion (pun intended). Here, we simply use the observation that this perspective enables us
to use a nonlinear procedure f without any issue. This gave rise to the Algorithm 9.1, which
diﬀers only in that we grow the sets T
→
B Converting Images to Graphs
For illustration purposes, we use images to generate graphs in various examples throughout the
survey. The purpose of this construction is that visually distinct segments of the picture should
have small conductance. Given an image we create a weighted nearest neighbor graph using a
Gaussian kernel as described in Shi and Malik (2000). We create a node for each pixel. Then
we connect pixels with weighted edges. In particular, let wij denote the the weight of the edge
R3 is the color representation
R2 be the position of pixel i, ci ∈
between pixels i and j, let pi ∈
is the variance for the color. Then, we deﬁne the
of pixel i, σ2
d

Ri by including all vertices within graph distance d.

is the variance for the position, σ2
I

68

Figure 22 – We turn an image into a graph by adding a node for every pixel (b). Then we connect the nodes if
the associated pixels are close by (distance less than r) as well has have similar pixel values). We weight the edge
by the degree of similarity. The resulting graph has small conductance sets when there are regions with similarly
colored pixels.

edge weights as

wij :=




e−

0

(cid:107)pi−pj (cid:107)2
2
σ2
d

(cid:107)ci−cj (cid:107)2
2
σ2
I

−

if
pj(cid:107)
pi −
(cid:107)
otherwise

2

r

≤

Note that there is a region r that restricts the feasible edges, illustrated in Figure 22.
Acknowledgements

We would like to thank many individuals for discussions about these ideas over the years. We would also like to
especially thank Nate Veldt for a careful reading of an initial draft, Charles Colley for reviewing a later draft, both
Di Wang and Satish Rao for discussions on geometric aspects of ﬂow algorithms, and ﬁnally Kent Quanrud for many
helpful pointers.

References
E. Abbe. Community detection and stochastic block models: Recent developments. Journal of Machine Learning

Research, 18 (177), pp. 1–86, 2018. Cited on page 3.

M. Ackerman and S. Ben-David. Measures of clustering quality: A working set of axioms for clustering. In

Advances in Neural Information Processing Systems 21, pp. 121–128. Curran Associates, Inc., 2008. Cited on
page 3.

R. Andersen, F. Chung, and K. Lang. Local graph partitioning using pagerank vectors. FOCS ’06 Proceedings
of the 47th Annual IEEE Symposium on Foundations of Computer Science, pp. 475–486, 2006. Cited on pages
6, 25, and 53.

R. Andersen, S. O. Gharan, Y. Peres, and L. Trevisan. Almost optimal local graph clustering using evolving

sets. Journal of the ACM, 63 (2), 2016. Cited on page 25.

0.820.970.700.120.430.820.920.000.000.11(a) Input Image(b) Add nodes for each pixel(d) Each pixel adds edges to nearby pixels to reﬂect the similarity of intensity values(c) Add edges between pixelsDistance is smallIntensity and distance are closeAdd weighted edge (u,v) if d(u,v) is small and w(u,v) is large69

R. Andersen and K. J. Lang. Communities from seed sets. In Proceedings of the 15th international conference

on the World Wide Web, pp. 223–232. 2006. Cited on page 25.

———. An algorithm for improving graph partitions. In Proceedings of the nineteenth annual ACM-SIAM
symposium on Discrete algorithms, pp. 651–660. 2008. Cited on pages 4, 8, 21, 22, 23, 36, 37, and 63.

S. Arora, S. Rao, and U. Vazirani. Expander ﬂows, geometric embeddings and graph partitioning. Journal of

the ACM, 56 (2:5), 2009. Cited on page 26.

H. Avron and L. Horesh. Community detection using time-dependent personalized pagerank . In Proceedings of
the 32nd International Conference on International Conference on Machine Learning, pp. 1795–1803. 2015.
Cited on page 25.

P. Awasthi, A. S. Bandeira, M. Charikar, R. Krishnaswamy, S. Villar, and R. Ward. Relax, no need to

round: Integrality of clustering formulations. In Proceedings of the 2015 Conference on Innovations in
Theoretical Computer Science, pp. 191–200. 2015. Cited on page 3.

T. Aynaud. Python-Louvian. https://github.com/taynaud/python-louvain, 2018. Cited on page 64.
M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from

labeled and unlabeled examples. Journal of machine learning research, 7 (Nov), pp. 2399–2434, 2006. Cited on
page 25.

S. Ben-David. Clustering - what both theoreticians and practitioners are doing wrong. In Proceedings of the

Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), pp. 7962–7964. 2018. Cited on page 3.
A. Benson, D. F. Gleich, and J. Leskovec. Higher-order organization of complex networks. Science, 353 (6295),

pp. 163–166, 2016. doi:10.1126/science.aad9029. Cited on pages 10 and 55.

A. L. Bertozzi and A. Flenner. Diﬀuse interface models on graphs for classiﬁcation of high dimensional data.

SIAM Review, 58 (2), pp. 293–328, 2016. Cited on page 3.

D. Bienstock, M. Chertkov, and S. Harnett. Chance-constrained optimal power ﬂow: Risk-aware network

control under uncertainty. SIAM Review, 56 (3), pp. 461–495, 2014. Cited on page 3.

V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large

networks. Journal of Statistical Mechanics: Theory and Experiment, 2008 (10), p. P10008, 2008. Cited on page
4.

A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the

Eighteenth International Conference on Machine Learning, pp. 19–26. 2001. Cited on pages 26 and 57.
S. Boyd and L. Vandenberghe. Convex optimization, Cambridge university press, 2004. Cited on pages 30

and 31.

Y. Boykov and G. Funka-Lea. Graph cuts and eﬃcient n-d image segmentation. International Journal of

Computer Vision, 70 (2), pp. 109–131, 2006. Cited on page 28.

Y. Boykov and V. Kolmogorov. An experimental comparison of min-cut/max-ﬂow algorithms for energy

minimization in vision. IEEE Trans. Pattern Anal. Mach. Intell., 26 (9), pp. 1124–1137, 2004. Cited on page 28.

Y. Boykov and O. Veksler. Graph cuts in vision and graphics: Theories and applications. In Handbook of

Mathematical Models in Computer Vision, pp. 79–96. Springer-Verlag, Boston, MA, 2006. Cited on page 27.
U. Brandes and T. Erlebach, editors. Network Analysis: Methodological Foundations, Springer, 2005. Cited on

page 3.

S. D. Brown, J. A. Gerlt, J. L. Seffernick, and P. C. Babbitt. A gold standard set of mechanistically diverse

enzyme superfamilies. Genome biology, 7 (1), p. R8, 2006. Cited on page 58.

E. J. Candès, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly
incomplete frequency information. IEEE Transactions on Information Theory, 52 (2), pp. 489–509, 2006. Cited
on page 45.

J. J. Carrasco, D. C. Fain, K. J. Lang, and L. Zhukov. Clustering of bipartite advertiser-keyword graph. In
Proceedings of the Workshop on Clustering Large Data Sets at the 2003 International Conference on Data
Mining, pp. 72–79. 2003. Paper available from CiteSeer
http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.4.8969, and workshop proceedings available
from CiteSeer, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.377.3857. Cited on page 3.
T. F. Chan, S. Esedoglu, and M. Nikolova. Algorithms for ﬁnding global minimizers of image segmentation

and denoising models. SIAM Journal on Applied Mathematics, 66 (5), pp. 1632–1648, 2006. Cited on page 27.
P. Christiano, J. A. Kelner, A. Madry, D. A. Spielman, and S.-H. Teng. Electrical ﬂows, laplacian systems,
and faster approximation of maximum ﬂow in undirected graphs. In Proceedings of the Forty-third Annual ACM
Symposium on Theory of Computing, pp. 273–282. 2011. Cited on page 27.

F. Chung. The heat kernel as the PageRank of a graph. Proceedings of the National Academy of Sciences,

104 (50), pp. 19735–19740, 2007a. Cited on page 25.

———. Random walks and local cuts in graphs. Linear Algebra and its Applications, 423, pp. 22–32, 2007b. Cited

on page 55.

———. A local graph partitioning algorithm using heat kernel pagerank. Internet Mathematics, 6 (3), pp. 315–330,

2009. Cited on page 25.

F. Chung and O. Simpson. Computing heat kernel pagerank and a local clustering algorithm. 25th International

Workshop, IWOCA 2014, pp. 110–121, 2014. Cited on page 25.

J.-C. Delvenne, S. N. Yaliraki, and M. Barahona. Stability of graph communities across time scales.

Proceedings of the National Academy of Sciences, 107 (29), pp. 12755–12760, 2010.
doi:10.1073/pnas.0903215107. Cited on page 5.

70

B. Dezso, J. Alpár, and P. Kovács. LEMON - an open source c++ graph template library. Electronic Notes in

Theoretical Computer Science, 264, pp. 23–45, 2011. Cited on page 27.

E. Dinitz. Algorithm for solution of a problem of maximum ﬂow in a network with power estimation. Doklady

Akademii nauk SSSR, 11, pp. 1277–1280, 1970. Cited on pages 32 and 47.

W. Dinkelbach. On nonlinear fractional programming. Management Science, 13 (7), pp. 492–498, 1967. Cited on

page 18.

D. L. Donoho and Y. Tsaig. Fast solution of $\ell _{1}$-norm minimization problems when the solution may be

sparse. IEEE Transactions on Information Theory, 54 (11), pp. 4789–4812, 2008.
doi:10.1109/tit.2008.929958. Cited on page 45.

D. Easley and K. Jo. Networks, Crowds, and Markets: Reasoning About a Highly Connected World, Cambridge

University Press, New York, NY, USA, 2010. Cited on page 3.

D. Eckles, B. Karrer, and J. Ugander. Design and analysis of experiments in networks: Reducing bias from

interference. Journal of Causal Inference, 5 (1), 2017. Cited on page 3.

B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann. Statist., 32 (2), pp.

407–499, 2004. Cited on page 45.

B. Ehrhardt and P. J. Wolfe. Network modularity in the presence of covariates. SIAM Review, 61 (2), pp.

261–276, 2019. Cited on page 3.

E. Estrada and N. Hatano. Communicability angle and the spatial eﬃciency of networks. SIAM Review, 58 (4),

pp. 692–715, 2016. Cited on page 3.

E. Estrada and D. J. Higham. Network properties revealed through matrix functions. SIAM Review, 52 (4), pp.

696–714, 2010. Cited on page 3.

C. Faloutsos, K. S. McCurley, and A. Tomkins. Fast discovery of connection subgraphs. In Proceedings of the
Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 118–127. 2004.
Cited on pages 6 and 7.

P. F. Felzenszwalb and D. P. Huttenlocher. Eﬃcient graph-based image segmentation. International Journal

of Computer Vision, 59 (2), pp. 167–181, 2004. Cited on page 13.

P. G. Fennell and J. P. Gleeson. Multistate dynamical processes on networks: Analysis through degree-based

approximation frameworks. SIAM Review, 61 (1), pp. 92–118, 2019. Cited on page 3.

C. M. Fiduccia and R. M. Mattheyses. A linear-time heuristic for improving network partitions. In Proceedings

of the 19th Design Automation Conference, pp. 175–181. 1982. Cited on page 24.

G. W. Flake, S. Lawrence, and C. L. Giles. Eﬃcient identiﬁcation of web communities. In Proceedings of the
Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 150–160. 2000.
Cited on page 26.

B. K. Fosdick, D. B. Larremore, J. Nishimura, and J. Ugander. Conﬁguring random graph models with ﬁxed

degree sequences. SIAM Review, 60 (2), pp. 315–355, 2018. Cited on page 3.

K. Fountoulakis, D. F. Gleich, and M. W. Mahoney. An optimization approach to locally-biased graph

algorithms. Proceedings of the IEEE, 105 (2), pp. 256–272, 2017. Cited on pages 13 and 25.

K. Fountoulakis, M. Liu, D. Gleich, and M. W. Mahoney. Code for experiments of the present paper.

https://github.com/dgleich/flowpaper-code/tree/master/figures, 2019a. Cited on pages 64, 65, 66,
and 67.

———. Localgraphclustering API. https://github.com/kfoynt/LocalGraphClustering, 2019b. Cited on pages

9, 58, 62, and 64.

K. Fountoulakis, F. Roosta-Khorasani, J. Shun, X. Cheng, and M. W. Mahoney. Variational perspective

on local graph clustering. Mathematical Programming B, pp. 1–21, 2017. Cited on pages 6, 25, and 64.
H. Frenk and S. Schaible. Fractional programmingFractional Programming, pp. 1080–1091. Springer US,

Boston, MA, 2009. Cited on page 17.

J. H. Friedman and J. J. Meulman. Clustering objects on subsets of attributes (with discussion). Journal of the
Royal Statistical Society: Series B (Statistical Methodology), 66 (4), pp. 815–849, 2004. Cited on page 3.
G. Gallo, M. D. Grigoriadis, and R. E. Tarjan. A fast parametric maximum ﬂow algorithm and applications.

SIAM J. Comput., 18 (1), pp. 30–55, 1989. Cited on pages 8 and 19.

U. Gargi, W. Lu, V. Mirrokni, and S. Yoon. Large-scale community detection on youtube for topic discovery
and exploration. In Proceedings of Fifth International AAAI Conference on Weblogs and Social Media. 2011.
Cited on page 24.

D. F. Gleich. Pagerank beyond the web. SIAM Review, 57 (3), pp. 321–363, 2015. Cited on pages 6, 7, and 42.
D. F. Gleich and M. W. Mahoney. Anti-diﬀerentiating approximation algorithms: A case study with min-cuts,
spectral, and ﬂow. In Proceedings of the 31st International Conference on Machine Learning, pp. 1018–1025.
2014. Cited on pages 25 and 42.

———. Using local spectral methods to robustify graph-based learning algorithms. In Proceedings of the 21th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 359–368. 2015. Cited on
pages 25, 42, 55, and 66.

———. Mining large graphs. In Handbook of Big Data, pp. 191–220. CRC Press, 2016. doi:10.1201/b19567-17.

Cited on pages 5 and 13.

A. V. Goldberg. Finding a maximum density subgraph. M.S. Thesis CSD-84-171, University of California

Berkeley, 1984. Cited on page 26.

71

A. V. Goldberg and S. Rao. Beyond the ﬂow decomposition barrier . J. ACM, 45 (5), p. 783–797, 1998. Cited on

pages 32 and 48.

A. V. Goldberg and R. E. Tarjan. Eﬃcient maximum ﬂow algorithms. Commun. ACM, 57 (8), pp. 82–89,

2014. Cited on page 27.

D. M. Greig, B. T. Porteous, and A. H. Seheult. Exact maximum a posteriori estimation for binary images.
Journal of the Royal Statistical Society: Series B (Methodological), 51 (2), pp. 271–279, 1989. Cited on page 27.
P. Grindrod and D. J. Higham. A matrix iteration for dynamic network summaries. SIAM Review, 55 (1), pp.

118–128, 2013. Cited on page 3.

W. Ha, K. Fountoulakis, and M. W. Mahoney. Statistical guarantees for local graph clustering. The 23rd

International Conference on Artiﬁcial Intelligence and Statistics, 2020. Cited on page 25.

L. Hagen and A. B. Kahng. New spectral methods for ratio cut partitioning and clustering. IEEE Transactions

on computer-aided design, 11 (9), pp. 1074–1085, 1992. Cited on page 13.

K. M. Hall. An r-dimensional quadratic placement algorithm. Management Science, 17 (3), pp. 219–229, 1970.

Cited on page 60.

T. J. Hansen and M. W. Mahoney. Semi-supervised eigenvectors for large-scale locally-biased learning. J. Mach.

Learn. Res., 15 (1), pp. 3691–3734, 2014. Cited on page 60.

M. Hein and S. Setzer. Beyond spectral clustering - tight relaxations of balanced graph cuts. In Advances in
Neural Information Processing Systems 24, pp. 2366–2374. Curran Associates, Inc., 2011. Cited on page 24.
B. Hendrickson and R. Leland. The chaco user‘s guide. version 2.0 . Technical Report SAND94–2692, Sandia

National Labs, Albuquerque, NM, 1994. Cited on page 24.

———. An improved spectral graph partitioning algorithm for mapping parallel computations. SIAM Journal on

Scientiﬁc Computing, 16 (2), pp. 452–469, 1995a. Cited on page 24.

B. Hendrickson and R. W. Leland. A multi-level algorithm for partitioning graphs. SC, 95 (28), pp. 1–14,

1995b. Cited on page 24.

A. Henzinger, A. Noe, and C. Schulz. Ilp-based local search for graph partitioning. arX, cs.DS, p. 1802.07144,

2018. Cited on page 24.

D. Hernando, P. Kellman, J. P. Haldar, and Z.-P. Liang. Robust water/fat separation in the presence of large
ﬁeld inhomogeneities using a graph cut algorithm. Magnetic Resonance in Medicine, 63 (1), pp. 79–90, 2010.
Cited on page 28.

D. S. Hochbaum. Polynomial time algorithms for ratio regions and a variant of normalized cut. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 32 (5), pp. 889–898, 2010. Cited on pages 8, 12,
and 21.

———. A polynomial time algorithm for Rayleigh ratio on discrete variables: Replacing spectral techniques for

expander ratio, normalized cut and Cheeger constant. Operations Research, 61 (1), pp. 184–198, 2013. Cited on
pages 4 and 26.

M. Jacobs, E. Merkurjev, and S. Esedo ¯glu. Auction dynamics: A volume constrained MBO scheme. Journal
of Computational Physics, 354, pp. 288–310, 2018. doi:10.1016/j.jcp.2017.10.036. Cited on page 26.
L. G. S. Jeub, P. Balachandran, M. A. Porter, P. J. Mucha, and M. W. Mahoney. Think locally, act locally:
Detection of small, medium-sized, and large communities in large networks. Physical Review E, 91, p. 012821,
2015. Cited on pages 24 and 25.

P. Jia, A. MirTabatabaei, N. E. Friedkin, and F. Bullo. Opinion dynamics and the evolution of social power

in inﬂuence networks. SIAM Review, 57 (3), pp. 367–397, 2015. Cited on page 3.

T. Joachims. Transductive learning via spectral graph partitioning. In Proceedings of the 20th International

Conference on Machine Learning (ICML-03), pp. 290–297. 2003. Cited on page 25.

A. Jung, A. O. I. Hero, A. C. Mara, S. Jahromi, A. Heimowitz, and Y. C. Eldar. Semi-supervised learning
in network-structured data via total variation minimization. IEEE Transactions on Signal Processing, 67 (24),
pp. 6256–6269, 2019. doi:10.1109/tsp.2019.2953593. Cited on page 26.

A. Jung and Y. SarcheshmehPour. Local graph clustering with network lasso. IEEE Signal Processing Letters,

28, pp. 106–110, 2021. doi:10.1109/lsp.2020.3045832. Cited on page 26.

G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM J.

Sci. Comput., 20 (1), pp. 359–392, 1998. Cited on page 24.

———. Parallel multilevel series k-way partitioning scheme for irregular graphs. SIAM Review, 41 (2), pp.

278–300, 1999. Cited on page 24.

R. Khandekar, S. Rao, and U. Vazirani. Graph partitioning using single commodity ﬂows. J. ACM, 56 (4),

2009. Cited on page 26.

J. Kleinberg. An impossibility theorem for clustering. In Proceedings of the 15th International Conference on

Neural Information Processing Systems, pp. 463–470. 2002. Cited on page 3.

J. Kleinberg and E. Tardos. Algorithm Design, Addison-Wesley Longman Publishing Co., Inc., USA, 2005.

Cited on page 26.

K. Kloster and D. F. Gleich. Heat kernel based community detection. Proceedings of the 20th ACM SIGKDD
international conference on Knowledge discovery and data mining, pp. 1386–1395, 2014. Cited on page 25.
I. M. Kloumann and J. M. Kleinberg. Community membership identiﬁcation from small seed sets. Proceedings
of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 1366–1375,
2014. Cited on pages 6, 7, and 25.

72

N. Knight, E. Carson, and J. Demmel. Exploiting data sparsity in parallel matrix powers computations. In

Parallel Processing and Applied Mathematics, pp. 15–25. Springer Berlin Heidelberg, 2014. Cited on page 24.
V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? IEEE Transactions on

Pattern Analysis and Machine Intelligence, 26 (2), pp. 147–159, 2004. Cited on pages 27 and 64.

A. Lancichinetti, S. Fortunato, and J. Kertész. Detecting the overlapping and hierarchical community

structure in complex networks. New Journal of Physics, 11 (3), p. 033015, 2009.
doi:10.1088/1367-2630/11/3/033015. Cited on page 25.

K. Lang. Fixing two weaknesses of the spectral method . In Advances in Neural Information Processing Systems 18

(NIPS2005), pp. 715–722. 2005. Cited on pages 6, 25, and 60.

K. Lang and S. Rao. A ﬂow-based method for improving the expansion or conductance of graph cuts. In IPCO
2004: Integer Programming and Combinatorial Optimization, pp. 325–337. 2004. Cited on pages 4, 7, 8, 21, 22,
23, 24, 32, 33, and 63.

K. J. Lang, M. W. Mahoney, and L. Orecchia. Empirical evaluation of graph partitioning using spectral
embeddings and ﬂow. In Proceedings of the 8th International Symposium on Experimental Algorithms, pp.
197–208. 2009. Cited on page 26.

D. Lawlor, T. Budavári, and M. W. Mahoney. Mapping the similarities of spectra: Global and locally-biased
approaches to SDSS galaxies. The Astrophysical Journal, 833 (1), p. 26, 2016a. Cited on pages 13, 53, 60,
and 65.

———. Mapping the similarities of spectra: Global and locally-biased approaches to SDSS galaxy data. Technical

Report 1609.03932, arXiv, 2016b. Preprint with expanded information. Cited on pages 13, 53, and 60.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.

Proceedings of the IEEE, 86 (11), pp. 2278–2324, 1998. Cited on page 58.

Y. T. Lee, S. Rao, and N. Srivastava. A new approach to computing maximum ﬂows using electrical ﬂows. In
Proceedings of the Forty-ﬁfth Annual ACM Symposium on Theory of Computing, pp. 755–764. 2013. Cited on
page 27.

n) algorithm for the minimum cost ﬂow problem. arXiv,

Y. T. Lee and A. Sidford. Path Finding II: An ˜O(m
cs.DS, p. arXiv:1312.6713, 2013. Cited on page 27.

√

T. Leighton and S. Rao. An approximate max-ﬂow min-cut theorem for uniform multicommodity ﬂow problems

with applications to approximation algorithms. Foundations of Computer Science, 1988., 29th Annual
Symposium on, pp. 422–431, 1988. Cited on page 26.

———. Multicommodity max-ﬂow min-cut theorems and their use in designing approximation algorithms. Journal

of the ACM, 46 (6), pp. 787–832, 1999. Cited on page 26.

J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection.

http://snap.stanford.edu/data, 2014. Cited on page 58.

J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney. Statistical properties of community structure in large
social and information networks. In WWW ’08: Proceedings of the 17th International Conference on World
Wide Web, pp. 695–704. 2008. Cited on pages 24, 25, and 58.

J. Leskovec, K. Lang, and M. Mahoney. Empirical comparison of algorithms for network community detection.
In WWW ’10: Proceedings of the 19th International Conference on World Wide Web, pp. 631–640. 2010. Cited
on pages 24, 25, and 58.

J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. Mahoney. Community structure in large networks: Natural
cluster sizes and the absence of large well-deﬁned clusters. Internet of Mathematics, 6 (1), pp. 29–123, 2009.
Cited on pages 3, 24, 25, and 58.

P. Li and O. Milenkovic. Inhomogeneous hypergraph clustering with applications. In Advances in Neural
Information Processing Systems 30, pp. 2308–2318. Curran Associates, Inc., 2017. Cited on page 64.

Y. Li, K. He, D. Bindel, and J. E. Hopcroft. Uncovering the small community structure in large networks: A
local spectral approach. In Proceedings of the 24th International Conference on World Wide Web, pp. 658–668.
2015. Cited on page 25.

L. Liberti, C. Lavor, N. Maculan, and A. Mucherino. Euclidean distance geometry and applications. SIAM

Review, 56 (1), pp. 3–69, 2014. Cited on page 3.

W. Liu and S.-F. Chang. Robust multi-class transductive learning with graphs. In 2009 IEEE Conference on

Computer Vision and Pattern Recognition, pp. 381–388. 2009. Cited on page 25.

Y. P. Liu and A. Sidford. Faster energy maximization for faster maximum ﬂow . arXiv, cs.DS, p.

arXiv:1910.14276, 2019. Cited on page 27.

P. Y. Lum, G. Singh, A. Lehman, T. Ishkanov, M. Vejdemo-Johansson, M. Alagappan, J. Carlsson, and
G. Carlsson. Extracting insights from the shape of complex data using topology. Sci. Rep., 3, p. Online, 2013.
Cited on page 58.

Z. Ma, X. Wu, Q. Song, Y. Luo, Y. Wang, and J. Zhou. Automated nasopharyngeal carcinoma segmentation in
magnetic resonance images by combination of convolutional neural networks and graph cut. Experimental and
Therapeutic Medicine, 2018. Cited on page 28.

M. W. Mahoney, L. Orecchia, and N. K. Vishnoi. A local spectral method for graphs: with applications to

improving graph partitions and exploring data graphs locally. Journal of Machine Learning Research, 13, pp.
2339–2365, 2012. Cited on page 13.

R. Marlet. Graph cuts and applicaton to disparity map estmaton. Online, accessed July 2019, 2017. Cited on

page 27.

73

A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel, and B. Bhattacharjee. Measurement and analysis of
online social networks. In Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, pp.
29–42. 2007. Cited on page 58.

A. Moffat. A theoretical investigation of focal stellar images in the photographic emulsion and application to

photographic photometry. Astronomy and Astrophysics, 3 (1), pp. 455–461, 1969. Cited on page 65.

G. Namata, B. London, L. Getoor, and B. Huang. Query-driven active surveying for collective classiﬁcation.

In 10th International Workshop on Mining and Learning with Graphs, p. 8. 2012. Cited on page 58.

M. Newman. Networks: An Introduction, Oxford University Press, Inc., New York, NY, USA, 2010. Cited on

page 3.

M. E. J. Newman. Modularity and community structure in networks. Proceedings of the National Academy of

Sciences, 103 (23), pp. 8577–8582, 2006. Cited on pages 3 and 25.

A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Advances in

Neural Information Processing Systems 14, pp. 849–856. MIT Press, 2001. Cited on page 60.

L. Orecchia, L. J. Schulman, U. V. Vazirani, and N. K. Vishnoi. On partitioning graphs via single commodity
ﬂows. Proceedings of the forty-fourth annual ACM symposium on Theory of computing, pp. 1141–1160, 2012.
Cited on page 26.

L. Orecchia and Z. A. Zhu. Flow-based algorithms for local graph clustering. In Proceedings of the 25th Annual
ACM-SIAM Symposium on Discrete Algorithms, pp. 1267–1286. 2014. Cited on pages 4, 7, 8, 15, 22, 42, 43, 45,
46, and 48.

J. B. Orlin. Max ﬂows in o(nm) time, or better. In Proceedings of the Forty-ﬁfth Annual ACM Symposium on

Theory of Computing, pp. 765–774. 2013. Cited on page 27.

B. Osting, J. Darbon, and S. Osher. Statistical ranking using the $lˆ{1}$-norm on graphs. Inverse Problems

and Imaging, 7 (3), pp. 907–926, 2013. doi:10.3934/ipi.2013.7.907. Cited on page 26.

P. Pagel, S. Kovac, M. Oesterheld, B. Brauner, I. Dunger-Kaltenbach, G. Frishman, C. Montrone,

P. Mark, V. Stümpflen, H.-W. Mewes, et al. The MIPS mammalian protein–protein interaction database.
Bioinformatics, 21 (6), pp. 832–834, 2004. Cited on page 58.

G. Palla, I. Derényi, I. Farkas, and T. Vicsek. Uncovering the overlapping community structure of complex

networks in nature and society. Nature, 435, pp. 814–818, 2005. Cited on page 25.

C. Papadimitriou and K. Steiglitz. Combinatorial Optimization - Algorithms and Complexity, Prentice-Hall,

1982. Cited on page 29.

L. Peel. Graph-based semi-supervised learning for relational networks. In Proceedings of the 2017 SIAM

International Conference on Data Mining, pp. 435–443. 2017. Cited on pages 25, 55, and 56.

L. Peel, D. B. Larremore, and A. Clauset. The ground truth about metadata and community detection in

networks. arXiv, cs.SI, p. 1608.05878, 2016. Cited on page 25.

T. P. Peixoto. The graph-tool Python library. ﬁgshare, 2014. Cited on page 57.
F. Pellegrini and J. Roman. Scotch: A software package for static mapping by dual recursive bipartitioning of
process and architecture graphs. In International Conference on High-Performance Computing and Networking,
pp. 493–498. 1996. Cited on page 24.

A. Pothen, H. D. Simon, and K.-P. Liou. Partitioning sparse matrices with eigenvectors of graphs. SIAM J.

Matrix Anal. Appl., 11, pp. 430–452, 1990. Cited on page 24.

P. Rombach, M. A. Porter, J. H. Fowler, and P. J. Mucha. Core-periphery structure in networks (revisited).

SIAM Review, 59 (3), pp. 619–646, 2017. Cited on page 3.

L. I. Rudin, S. Osher, and E. Fatemi. Nonlinear total variation based noise removal algorithms. Phys. D,

60 (1-4), pp. 259–268, 1992. Cited on page 27.

P. Sanders and C. Schulz. Engineering multilevel graph partitioning algorithms. arXiv, cs.DS, p. 1012.0006,

2010. Cited on page 24.

F. Shahrokhi. The maximum concurrent ﬂow problem. Journal of the ACM (JACM), 37 (2), pp. 318–334, 1990.

Cited on page 26.

E. Sharon, M. Galun, D. Sharon, R. Basri, and A. Brandt. Hierarchy and adaptivity in segmenting visual

scenes. Nature, 442 (7104), pp. 810–813, 2006. Cited on page 12.

G. Shi, C. Altafini, and J. S. Baras. Dynamics over signed networks. SIAM Review, 61 (2), pp. 229–257, 2019.

Cited on page 3.

J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and

Machine Intelligence, 22 (8), pp. 888–905, 2000. Cited on pages 12 and 67.

P. Shi, K. He, D. Bindel, and J. Hopcroft. Local Lanczos spectral approximation for community detection. In

Proceedings of ECML-PKDD. 2017. Cited on page 25.

J. Shun, F. Roosta-Khorasani, K. Fountoulakis, and M. W. Mahoney. Parallel local graph clustering.

Proceedings of the VLDB Endowment, 9 (12), pp. 1041–1052, 2016. Cited on pages 14 and 25.

H. D. Simon. Partitioning of unstructured problems for parallel processing. Computing systems in engineering,

2 (2-3), pp. 135–148, 1991. Cited on page 24.

D. D. Sleator and R. E. Tarjan. A data structure for dynamic trees. Journal of Computer and System Sciences,

3, pp. 362–391, 1983. Cited on page 47.

A. J. Soper, C. Walshaw, and M. Cross. A combined evolutionary search and multilevel optimisation approach

to graph-partitioning. J. of Global Optimization, 29 (2), pp. 225–241, 2004. Cited on page 24.

74

D. A. Spielman and S. H. Teng. A local clustering algorithm for massive graphs and its application to nearly

linear time graph partitioning. SIAM Journal on Scientiﬁc Computing, 42 (1), pp. 1–26, 2013. Cited on page 25.
G. Strang. Maximal ﬂow through a domain. Mathematical Programming, 26 (2), pp. 123–143, 1983. Cited on

page 27.

———. Maximum ﬂows and minimum cuts in the plane. Journal of Global Optimization, 47 (3), pp. 527–535,

2010. Cited on page 27.

R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B

(Methodological), 58 (1), pp. 267–288, 1996. Cited on page 45.

H. Tong, C. Faloutsos, and J.-Y. Pan. Fast random walk with restart and its applications. In ICDM ’06:

Proceedings of the Sixth International Conference on Data Mining, pp. 613–622. 2006. Cited on pages 6 and 7.
A. L. Traud, E. D. Kelsic, P. J. Mucha, and M. A. Porter. Comparing community structure to characteristics

in online collegiate social networks. SIAM Review, 53 (3), pp. 526–543, 2011. Cited on pages 3 and 55.
A. L. Traud, P. J. Mucha, and M. A. Porter. Social structure of facebook networks. Physica A: Statistical

Mechanics and its Applications, 391 (16), pp. 4165–4180, 2012. Cited on page 55.

L. Trevisan. Combinatorial optimization: Exact and approximate algorithms.

http://theory.stanford.edu/~trevisan/books/cs261.pdf, 2011. Lecture notes for CS261 at Stanford
University. Cited on page 29.

C. E. Tsourakakis, J. Pachocki, and M. Mitzenmacher. Scalable motif-aware graph clustering. In Proceedings

of the 26th International Conference on World Wide Web, pp. 1451–1460. 2017. Cited on page 25.

M. Ullah, A. Iltaf, Q. Hou, F. Ali, and C. Liu. A foreground extraction approach using convolutional neural
network with graph cut. In 2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC).
2018. Cited on page 28.

S. van der Walt, J. L. Schönberger, J. Nunez-Iglesias, F. Boulogne, J. D. Warner, N. Yager,

E. Gouillart, T. Yu, and the scikit-image contributors. scikit-image: image processing in Python.
PeerJ, 2, p. e453, 2014. Cited on page 65.

L. N. Veldt, D. F. Gleich, and M. W. Mahoney. A simple and strongly-local ﬂow-based method for cut

improvement. In International Conference on Machine Learning, pp. 1938–1947. 2016. Cited on pages 6, 15, 22,
42, 43, 44, 45, 47, and 48.

N. Veldt. Pushrelabel local ﬂow algorithms. Github software,

https://github.com/nveldt/PushRelabelMaxFlow, 2019. Cited on pages 62 and 65.

N. Veldt, A. R. Benson, and J. Kleinberg. Hypergraph cuts with general splitting functions. arXiv, cs.DS, p.

2001.02817, 2020a. Cited on page 10.

———. Localized ﬂow-based clustering in hypergraphs. 2020b. arXiv:2002.09441. Cited on pages 10 and 64.
N. Veldt, C. Klymko, and D. F. Gleich. Flow-based local graph clustering with better seed set inclusion. In

Proceedings of the SIAM International Conference on Data Mining, pp. 378–386. 2019. Cited on pages 4, 16,
and 23.

N. Veldt, A. Wirth, and D. F. Gleich. Learning resolution parameters for graph clustering. In The World Wide

Web Conference, pp. 1909–1919. 2019. Cited on pages 4 and 62.

U. von Luxburg. A tutorial on spectral clustering. Statistics and Computing, 17 (4), pp. 395–416, 2007. Cited on

page 13.

U. von Luxburg, R. C. Williamson, and I. Guyon. Clustering: Science or art? In Proceedings of ICML

Workshop on Unsupervised and Transfer Learning, pp. 65–79. 2012. Cited on page 3.

C. Walshaw and M. Cross. Mesh partitioning: a multilevel balancing and reﬁnement algorithm. SIAM Journal

on Scientiﬁc Computing, 22 (1), pp. 63–80, 2000. Cited on page 24.

———. Jostle: Parallel multilevel graph-partitioning software – an overview. In Mesh Partitioning Techniques and

Domain Decomposition Techniques, pp. 27–58. Civil-Comp Ltd., 2007. Cited on page 24.

J. J. Whang, D. F. Gleich, and I. S. Dhillon. Overlapping community detection using neighborhood-inﬂated seed
expansion. Transactions on Knowledge and Data Engineering, 28 (5), pp. 1272–1284, 2016. Cited on page 25.

Wikipedia. Eileen Collins. 2021. [Online; accessed 16-September-2021]. Cited on pages 6 and 8.
D. P. Williamson. Network Flow Algorithms, Cambridge University Press, 2019. doi:10.1017/9781316888568.

Cited on page 47.

J. Xie, S. Kelley, and B. K. Szymanski. Overlapping community detection in networks: The state-of-the-art and

comparative study. ACM Comput. Surv., 45 (4), pp. 43:1–43:35, 2013. Cited on page 25.

H. Yin, A. R. Benson, J. Leskovec, and D. F. Gleich. Local higher-order graph clustering. Proceedings of the
23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 555–564, 2017.
Cited on page 25.

J. Yuan, E. Bae, and X.-C. Tai. A study on continuous max-ﬂow and min-cut approaches. In 2010 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition. 2010. Cited on page 27.

D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf. Learning with local and global consistency.
In Annual Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference, pp.
321–328. 2004. Cited on pages 6, 7, and 25.

X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic

functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pp. 912–919.
2003. Cited on pages 6, 7, and 25.

Z. A. Zhu, S. Lattanzi, and V. Mirrokni. A local algorithm for ﬁnding well-connected clusters. In Proceedings

of the 30th International Conference on Machine Learning, pp. 396–404. 2013. Cited on page 25.

75

