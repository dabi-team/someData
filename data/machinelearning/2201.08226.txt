Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for
K-means Clustering

Yubo Zhuang

Xiaohui Chen
University of Illinois at Urbana-Champaign

Yun Yang

2
2
0
2

b
e
F
9

]
L
M

.
t
a
t
s
[

2
v
6
2
2
8
0
.
1
0
2
2
:
v
i
X
r
a

Abstract

Semideﬁnite programming (SDP) is a pow-
erful tool for tackling a wide range of com-
putationally hard problems such as cluster-
ing. Despite the high accuracy, semideﬁ-
nite programs are often too slow in prac-
tice with poor scalability on large (or even
In this paper, we in-
moderate) datasets.
troduce a linear time complexity algorithm
for approximating an SDP relaxed K-means
clustering. The proposed sketch-and-lift (SL)
approach solves an SDP on a subsampled
dataset and then propagates the solution to
all data points by a nearest-centroid rounding
procedure. It is shown that the SL approach
enjoys a similar exact recovery threshold as
the K-means SDP on the full dataset, which
is known to be information-theoretically tight
under the Gaussian mixture model. The
SL method can be made adaptive with en-
hanced theoretic properties when the clus-
ter sizes are unbalanced. Our simulation
experiments demonstrate that the statisti-
cal accuracy of the proposed method out-
performs state-of-the-art fast clustering algo-
rithms without sacriﬁcing too much compu-
tational eﬃciency, and is comparable to the
original K-means SDP with substantially re-
duced runtime.

1

INTRODUCTION

Clustering is a widely explored unsupervised machine
learning task to partition data into a fewer number of
unknown groups. The K-means clustering is a clas-
sical clustering method with good empirical perfor-

Proceedings of the 25th International Conference on Artiﬁ-
cial Intelligence and Statistics (AISTATS) 2022, Valencia,
Spain. PMLR: Volume 151. Copyright 2022 by the au-
thor(s).

mance on recovering the cluster labels for Euclidean
data (MacQueen, 1967). Under the Gaussian mixture
model (GMM) with isotropic noise, K-means cluster-
ing is equivalent to the maximum likelihood estima-
tor (MLE) for cluster labels, which is known to be
worst-case NP-hard (Aloise et al., 2009). Fast ap-
proximation algorithms to solve the K-means such as
Lloyd’s algorithm (Lloyd, 1982; Lu and Zhou, 2016)
and spectral clustering (Meila and Shi, 2001; Ng et al.,
2001; Vempala and Wang, 2004; Achlioptas and Mc-
Sherry, 2005; von Luxburg, 2007; von Luxburg et al.,
2008) provably yield consistent recovery when diﬀer-
ent groups are well separated. Recently, semi-deﬁnite
programming (SDP) relaxations (Peng and Wei, 2007;
Mixon et al., 2016; Li et al., 2017; Fei and Chen,
2018; Chen and Yang, 2021a; Royer, 2017; Giraud and
Verzelen, 2018; Bunea et al., 2016) have emerged as
an important approach for clustering due to its su-
perior empirical performance (Peng and Wei, 2007),
robustness against outliers and adversarial attack (Fei
and Chen, 2018), and attainment of the information-
theoretic limit (Chen and Yang, 2021b). Despite hav-
ing polynomial time complexity, the SDP relaxed K-
means has notoriously poor scalability to large (or even
moderate) datasets for instance by interior point meth-
ods (Alizadeh, 1995; Jiang et al., 2020), as the typical
runtime complexity of an interior point algorithm for
solving the SDP is at least O(n3.5), where n is the
sample size. Hence the goal of this paper is to derive a
computationally cheap approximation to the SDP re-
laxed K-means formulation for reducing the time com-
plexity while maintaining statistical optimality.

Sketching, a popular numerical technique in random-
ized linear algebra to speed up matrix computations
via compressing a matrix to a much smaller one by
multiplying a random matrix (Drineas and Mahoney,
2017), has been deployed in recent years as a valuable
tool in many data science applications at scale. We re-
fer papers from Bluhm and Stilck Fran¸ca (2019); Yurt-
sever et al. (2017) for sketching semideﬁnite programs.
In this paper, we consider subsampling sketches, con-
structed by subsampling m out of the total n data
points (seen as a random projection with indepen-

 
 
 
 
 
 
Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

dent Bernoulli weights). Subsampling sketches per-
form sample size reduction to substantially reduce the
runtime complexity when m (cid:28) n by instead solving a
much smaller m-dimensional SDP.

In this paper, we propose a sketch-and-lift (SL) ap-
proach for fast and scalable clustering. The main goal
for the SL approach is to look for the smallest pos-
sible projected sample size m in order to maximally
reduce the computational cost without sacriﬁcing too
much statistical accuracy. Under the GMM, we show
that the subsampled size m can be made almost inde-
pendent of n to guarantee exact recovery on randomly
subsampled data points when the signal is above a
threshold depending on the down-sampling ratio m/n.
To reconstruct a solution to the full dataset, we need to
project back (or lift) from cluster labels estimated by
an m-dimensional SDP to cluster labels of the entire
n data points. This back projection step takes O(n)
complexity. Thus the proposed SL approach has an
overall linear time complexity as long as m = O(nc)
for some constant c ∈ (0, 1), which substantially miti-
gates the high polynomial runtime complexity of solv-
ing the original SDP relaxed K-means. For instance,
we can set c = 2/7 if the interior point method is used
to solve the SDP (Jiang et al., 2020).

The baseline SL procedure begins with a uniform sub-
sampling on the entire dataset. When the cluster sizes
are unequal, the single down-sampling parameter γ
creates a non-trivial bias on restricting the data di-
mension growth rate. Motivated from this observa-
tion, we propose two SL variants: one based on bias-
correction by equalizing the size of the estimated clus-
ters from the subsampled SDP, and the other based on
adaptively choosing the sampling weights on the input
data points. By doing so, we show that the constraint
on the data dimension is unnecessary after bias correc-
tion, and the bias-corrected SL and weighted SL boost
numeric performance for unequal cluster size case.

The rest of paper is structured as follows. In Section 2,
we describe some background on the K-means cluster-
ing and its SDP relaxation. In Section 3, we present
our SL approach and its variants. In Section 4, we de-
rive the guarantees for exact recovery of the SL meth-
ods under the standard Gaussian mixture model. In
Section 5, we show some statistical and computational
comparisons for the SL methods and the state-of-the-
art K-means algorithm in various settings.

2 BACKGROUND

We ﬁrst provide some background on the K-means
clustering. After that, we describe a matrix-lifting
semideﬁnite relaxation scheme which turns the mixed
integer program associated with the K-means into a

convex one by throwing away the integer constraints,
and review its theoretical properties.

2.1 K-means clustering

1, . . . , G∗

Let X1, . . . , Xn be a sequence of p-dimensional vectors
and X = (X1, . . . , Xn) ∈ Rp×n denote the data ma-
trix with n data points in Rp. Suppose that there is
a clustering structure G∗
K (i.e., a partition on
k = [n], where (cid:70) de-
[n] := {1, . . . , n} such that (cid:70)K
notes the disjoint union) on the n data point indices.
To recover the true clustering structure G∗
1, . . . , G∗
K
from data, we consider the K-means deﬁned as the fol-
lowing constrained combinatorial optimization prob-
lem:

k=1 G∗

max
G1,...,GK

K
(cid:88)

k=1

1
|Gk|

(cid:88)

i,j∈Gk

(cid:104)Xi, Xj(cid:105)

subject to

K
(cid:71)

k=1

Gk = [n],

(1)

where (cid:104)Xi, Xj(cid:105) = X (cid:62)
i Xj is the Euclidean inner prod-
uct in Rp that represents the similarity between two
vectors Xi and Xj. Note that the objective function
in (1) is proportional to the log-likelihood function
of cluster labels after proﬁling (maximizing) out the
cluster centers as nuisance parameters in the Gaus-
sian mixture model with constant and isotropic noise.
Therefore, solving for (1) is equivalent to computing
the maximum likelihood estimator.

The standard approach to ﬁnding an approximate so-
lution of the K-means problem is the Lloyd’s algo-
rithm, also known as Voronoi iteration or K-means
algorithm, which repeatedly ﬁnds the centroid of the
points within each cluster Gk and then re-assigns
points to the K clusters according to which of these
centroids is closest. To overcome some shortcomings
of the K-means algorithm that is unstable in both
its running time and approximation accuracy, the K-
means++ algorithm (Arthur and Vassilvitskii, 2007)
is proposed and becomes a state-of-the-art clustering
algorithm hereafter. The K-means++ algorithm ad-
dresses the unstability issue by specifying a careful ini-
tialization procedure to seed the K-means algorithm.
See the paper from Saxena et al. (2017) for a recent
review on diﬀerent clustering techniques and their de-
velopments.

We end this subsection with a brief review on theoret-
ical developments of the K-means clustering. Consis-
tency of the K-means estimation of the clustering cen-
ters is studied by Pollard (1981), without concerning
the computational complexity. Kumar and Kannan
(2010) and Awasthi and Sheﬀet (2012) show that if
the true cluster centers are suﬃciently well-separated
relative to their spreads, then the Lloyd’s algorithm

Yubo Zhuang, Xiaohui Chen, Yun Yang

initialized by spectral clustering achieves exact recov-
ery of the cluster labels. Partial recovery bounds of
Lloyd’s algorithm for local search solution to the K-
means are derived by Lu and Zhou (2016).

2.2 K-means as mixed integer program

Next, we describe an equivalent formulation of the K-
means optimization (1) that will be useful in moti-
vating its convex relaxation in the next subsection.
Note that we can express the cluster labels for each
partition G1, . . . , GK of [n] by their one-hot encoding:
we can associate each (Gk)K
k=1 with a binary assign-
ment matrix H = (hik) ∈ {0, 1}n×K, where hij = 1
indicates Xi belongs to cluster k, and hik = 0 oth-
erwise. Because each row of H contains exactly one
non-zero entry, there is one-to-one mapping (up to as-
signment labeling) between the partition and the as-
signment matrix. Thus recovery of the true clustering
structure is equivalent to recovery of the associated as-
signment matrix, and the K-mean clustering problem
can be re-expressed as a (non-convex) mixed integer
program:

(cid:104)A, HBH (cid:62)(cid:105)

max
H

subject to H ∈ {0, 1}n×K, H1K = 1n,

(2)

where A = X (cid:62)X is the n × n similarity matrix and 1n
denotes the n-dimensional vector of all ones.

2.3 SDP relaxed K-means

Relaxing the above mixed integer program (2) by
changing variable Z = HBH (cid:62), we arrive at the
SDP relaxed approximation of the K-means cluster-
ing problem:

ˆZ = arg max
Z∈Rn×n
subject to Z (cid:23) 0, tr(Z) = K, Z1n = 1n, Z ≥ 0,

(cid:104)A, Z(cid:105)

(3)

where Z ≥ 0 means each entry Zij ≥ 0 and Z (cid:23) 0
means the matrix Z is symmetric and positive semi-
deﬁnite. This SDP approximation relaxes the integer
constraint on H into two linear constraints tr(Z) = K
and Z ≥ 0 that are satisﬁed by any Z = HBH T as H
ranges over feasible solutions of problem (2). The SDP
in (3) was ﬁrst introduced by Peng and Wei (2007)
and it was shown that this SDP relaxing the integer
constraint is information-theoretically tight under the
standard Gaussian mixture model (Chen and Yang,
2021b) (see our brief review below).

1, . . . , G∗

Membership matrix Z ∗ corresponding to the true par-
tition G∗
K is a block diagonal matrix with K
blocks, each of which has size nk × nk with all entries
equal to n−1
k| is the size of cluster k.

k . Here nk = |G∗

Note that the SDP solution ˆZ of (3) is generally not
integral in the sense that ˆZ cannot be directly used
to recover a partition estimate of the data points. In
such case, we can apply some rounding technique to
project ˆZ back to yield a partition as the solution to
the original discrete optimization problem (1). For in-
stance, we may cluster the top K eigenvectors of ˆZ to
extract the estimated partition structure ˆG1, . . . , ˆGK.
On the other hand, it is known that rounding is not
necessary as the relaxed SDP solution can be directly
used to recover the K-means in (1), when the separa-
tion of cluster centers is large enough, a property often
referred in literature as the hidden integrality (Fei and
Chen, 2018; Chen and Yang, 2021b; Ndaoud, 2018;
Awasthi et al., 2015).

Formally, consider the standard GMM where nk obser-
vations from the k-th cluster follow i.i.d. N (µk, σ2Ip)
for k ∈ [K]. It is proved by Chen and Yang (2021b)
that for any α > 0, if the squared minimal separation
∆2 = min1≤k(cid:54)=l≤K (cid:107)µk −µl(cid:107)2 satisﬁes ∆2 ≥ (1+α) ¯∆2
∗,
where

¯∆2

∗ = 4σ2

(cid:18)

1 +

(cid:114)

1 +

(cid:19)

p
n∗ log n

log n,

(4)

with n∗ = min1≤k(cid:54)=l≤K 2nknl/(nk + nl) denoting the
smallest pairwise harmonic average and n = (cid:80)K
k=1 nk
the total sample size, then with probability at least
1 − c1K 2n−c2α for some constants c1, c2 > 0, the SDP
in (3) will produce the integral solution Z ∗ that cor-
responds to exact recovery (cf. Lemma A.1 for a pre-
cise statement). Regarding the lower bound, Chen and
Yang (2021b) shows that in the equal cluster size case
where nk = n/K for each k ∈ [K], if ∆2 ≤ (1 − α) ¯∆2
∗
holds for any α > 0, then with probability at least
1 − cKn−1, no clustering algorithm can achieve simul-
taneous exact recovery of all cluster labels. In other
words, ¯∆2
∗ is the cutoﬀ value for exact recovery of
GMM.

3 PROPOSED LINEAR TIME

APPROXIMATION
ALGORITHMS

Now we introduce our proposed linear time complex-
ity algorithm for approximating the SDP relaxed K-
means problem. We also discuss some variants that
signiﬁcantly boost the numerical performance and are
better suited to handle unequal cluster scenarios.

3.1 Sketch-and-lift for the K-means SDP

We ﬁrst provide some intuition before formally de-
scribing our sketch-and-lift (SL) approach for the K-
means SDP (3). As in the Lloyd’s algorithm, ﬁnding

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

a best clustering scheme consists of two intermediate
steps: 1. estimate the center of each cluster; 2. deter-
mine cluster labels based on which of these centers is
closest. It turns out that the loss of statistical accu-
racy in estimating the centers in the ﬁrst step due to
using fewer but correctly labeled samples is much less
severe than that due to using mislabeled samples. This
motivates us to apply stable and reliable but compu-
tationally more expensive clustering algorithms such
as the SDP (3) to a smaller subsample of size m to
extract correct cluster labels of the subsample. Based
on the cluster labels, we obtain an estimator of the
cluster centers (as within cluster averages) using the
subsample, and then apply the estimated centers for
clustering the entire data. As we will illustrate in the
theoretical analysis, the sample size m for estimating
the centers via the K-means SDP (3) in the ﬁrst step
can be as small as O(cid:0)(log n)2(cid:1) (see the remark after
Theorem 4.3) in order to guarantee the exact recovery
of entire data cluster labels in the second step under
suitable separation conditions. Due to this extremely
low sample size requirement on m, the overall com-
putational complexity will be dominated by the linear
O(n) complexity in the second step. Note that the sub-
sampling in the ﬁrst step corresponds to the “sketch”
operation; and the label recovery based on centers es-
timated from a subsample corresponds to the “lift”
operation. In principle, this SL idea can be incorpo-
rated with any accurate clustering method. We choose
the SDP relaxed K-means in this paper mainly due
to its theoretical optimality in cluster label recovery
(cf. Section 2.3).

Now we formally describe our SL approach. Let γ ∈
(0, 1) be a pre-speciﬁed subsampling factor which may
depend on the sample size n. We ﬁrst randomly sam-
ple an index subset T ⊂ [n] with i.i.d. Ber(γ). Denote
the subsampled data matrix as V = (Xi)i∈T , which
is of size p-by-m where m = |T | follows a Binomial
Bin(n, γ) distribution. Here assuming the i.i.d. sam-
pling is mainly for technical convenience, and in prac-
tice one can also uniformly sample a subset of [n] with
ﬁxed size (cid:98)nγ(cid:99), where (cid:98)x(cid:99) denotes the largest integer
not exceeding x. After the subsampling, we apply the
SDP relaxed K-means (3) to V :

ˆW = arg max

(cid:104)V (cid:62)V, W (cid:105)

Z∈Rm×m
subject to W (cid:23) 0, tr(W ) = K, W 1m = 1m, W ≥ 0.
(5)
Once we obtain a partition estimate ˆR1, . . . , ˆRK ⊂ T
from ˆW on the subset V (perhaps after a round-
ing procedure), we compute the centroids ¯Xk =
| ˆRk|−1 (cid:80)
Xj based on the estimated partition
T = (cid:70)K
ˆRk. Finally, we project back the cluster
labels to all data points in X \ V by mapping them to

j∈ ˆRk

k=1

the nearest centroid among ¯X1, . . . , ¯XK, i.e., for each
Xi ∈ X \ V , we assign i ∈ ˆGk when (cid:107)Xi − ¯Xk(cid:107) <
(cid:107)Xi − ¯Xl(cid:107) for all l (cid:54)= k and l ∈ [K], where (cid:107) · (cid:107) de-
notes the (cid:96)2-norm. The SL algorithm is summarized
in Algorithm 1 (steps 2-3 in subroutine Algorithm 2
correspond to rounding).

Algorithm 1: Sketch-and-lift algorithm for K-
means SDP with sampling weights (w1, . . . , wn).
Input: sampling weights (w1, . . . , wn) with
w1 = · · · = wn = γ ∈ (0, 1) being the
subsampling factor.

1 (Sketch) Independent sample an index subset

T ⊂ [n] via Ber(wi) and store the subsampled
data matrix V = (Xi)i∈T .

2 Run subroutine Algorithm 2 with input V to get

a partition estimate ˆR1, . . . , ˆRK for T .
3 Compute the centroids ¯Xk = | ˆRk|−1 (cid:80)

j∈ ˆRk

Xj for

k ∈ [K].

4 (Lift) For each i ∈ [n] \ T , assign i ∈ ˆGk if

(cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107),
Output: A partition estimate ˆG1, . . . , ˆGK for [n].

∀l (cid:54)= k, l ∈ [K].

Algorithm 2: Subroutine for solving K-means
SDP.
Input: Data matrix V ∈ Rp×m containing m

points.

1 Solve the SDP in (5) using V to get solution ˆW .
2 Perform the spectral decomposition of ˆW and
take the top K eigenvectors (ˆu1, . . . , ˆuk).
3 Run K-means clustering on (ˆu1, . . . , ˆuk) and
extract the cluster labels ˆR1, . . . , ˆRK as a
partition estimate for [m].
Output: A partition estimate ˆR1, . . . , ˆRK for

[m].

We highlight that the SL approach has a linear time
complexity in the sample size n if we choose m = O(nc)
for some small constant c ∈ (0, 1). Theoretically, it is
shown in Section 4 that the subsampling factor γ is
allowed to vanish to zero while retaining statistical va-
lidity of SL. Obviously, any clustering algorithm has
at least a linear time complexity since it should visit at
least one time for each data point. On the other hand,
it is shown that the SL enjoys a similar exact recov-
ery threshold as the original SDP on all data points,
which is known to achieve the information-theoretic
limit (Chen and Yang, 2021b). Empirically, we demon-
strate in Section 5 that around the sharp threshold of
exact recovery, the SL approach statistically outper-
forms the widely used K-means++ algorithm (Arthur
and Vassilvitskii, 2007) by a large margin in terms of
the error rates.

Yubo Zhuang, Xiaohui Chen, Yun Yang

Remark 3.1 (Multi-epoch with averaging). We can re-
peat the above SL procedure for multiple epochs to
enhance the empirical performance. A simple way to
achieve this is to randomly partition the data points
into (cid:98)n/m(cid:99) blocks, each of which is a sequence of in-
dependent Bernoulli trials of size n with success prob-
ability γ. Then we run (cid:98)n/m(cid:99) SL procedure in Algo-
rithm 1 on the independent data blocks and average
the centroids estimated from the multiple epochs be-
fore lifting. Such procedures can be easily paralleled
in a distributed system and therefore the computa-
tional burden for running multiple epochs is essentially
the same as one sketch-and-lift pass. In Section 5, we
present some numerical result for the multi-epoch SL
with averaging. This multi-epoch SL approach is sum-
marized in Algorithm 3 below.

Algorithm 3: Multi-epoch sketch-and-lift algo-
rithm for K-means SDP.
Input: Subsample size m.

1 Randomly partition data indices [n] into

S = (cid:98)n/m(cid:99) blocks T1, . . . , TS with size m.

2 for s = 1, . . . , S do

3

4

(Sketch) Run subroutine Algorithm 2 with
input Vs = (Xi)i∈Ts to get a partition
estimate ˆRs,1, . . . , ˆRs,K for T .
Compute the centroids
¯Xs,k = | ˆRs,k|−1 (cid:80)

Xj for k ∈ [K].

j∈ ˆRs,k

5 Compute the aggregated centroids
¯Xs,k for k ∈ [K].

¯Xk = S−1 (cid:80)S

s=1

6 (Lift) For each i ∈ [n] \ T , assign i ∈ ˆGk if

(cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107),
Output: A partition estimate ˆG1, . . . , ˆGK for [n].

∀l (cid:54)= k, l ∈ [K].

Remark 3.2 (Related work on stochastic block mod-
els). Mixon and Xie (2020) proposed a subsampled
SDP for the two-component stochastic block model
(SBM) with equal community size. The approach pre-
sented by Mixon and Xie (2020) ﬁrst randomly sub-
samples a small vertex set according a Bernoulli pro-
cess with rate γ ∈ (0, 1) and then solves the community
detection problem on the induced subgraph. The solu-
tion on the subgraph is ﬁnally projected by a majority
voting procedure to all nodes in the whole graph. It is
shown by Mixon and Xie (2020); Abdalla and Bandeira
(2021) that the subsampling factor γ > c, where c > 0
is a constant depending on the edge connecting prob-
abilities within-community and between-communities
in the graph, is needed for exact community recov-
ery with high probability. In our clustering problem,
we allow the subsampling ratio γ = o(1) (cf. The-
orem 4.1 below), so the computational cost can be
much further reduced than the subsampled SDP for
the SBM. In particular, we can choose very small γ

such that the reduced SDP problem size m = (cid:98)nγ(cid:99)
is nearly independent of n (up to some polylogarith-
mic factor logc n). Moreover, the subsampled SDP for
SBM proposed by Mixon and Xie (2020) works only
for two-component equal cluster size case, while our SL
approach works for unbalanced K-component clusters
and it can be further enhanced with bias-correction
(Section 3.2) and non-uniform sampling weights (Sec-
tion 3.3) to better handle the general unequal cluster
size scenario.

The SL approach performs the uniform subsampling
on n data points, which is natural for equal cluster
size case. If the cluster sizes are not equal, the esti-
mated centroids based on the partition given by the
sketched SDP in (5) have diﬀerent variances. Thus by
comparing the distances between data point in X \ V
with ¯Xk and ¯Xl will create a larger bias than that
in the equal cluster case. Theoretically, such an ex-
tra bias term will imposes the unnecessary constraint
of p = O(cid:0)(γn/K)2(cid:1) (cf. Theorem 4.1). To mitigate
this issue, we propose two procedures in the following
subsections.

3.2 Bias-corrected sketch-and-lift (BCSL)

Suppose we have obtained a partition ˆR1, . . . , ˆRK for
V by the sketched SDP and an estimate of the cluster
centers ¯Xk and ¯Xl. Let m := mink∈[K] | ˆRk| be the
smallest cluster size. To fairly compare the distances
(cid:107)Xi − ¯Xk(cid:107) and (cid:107)Xi − ¯Xl(cid:107) by matching the variance
of all cluster center estimates { ¯Xk : k ∈ [K]}, we
further down-sample ˆR1, . . . , ˆRK to have the same size
m. In particular, we can randomly sample a subset ˜Rk
with equal size m from each ˆRk. Then we apply the
lift step to propagate ˜R1, . . . , ˜RK to the original data
to obtain a partition ˆG1, . . . , ˆGK. As we will show
in Theorem 4.3, this bias correction scheme removes
the undesirable constraint on p as needed in the SL
approach. The bias-corrected SL (BCSL) algorithm is
summarized in Algorithm 4.

3.3 Weighted sketch-and-lift (WSL)

Another bias correcting method is to subsample
X1, . . . , Xn with non-uniform weights that convey the
cluster size information, so that in the sketched data
matrix V , all clusters have roughly the same number of
points. For example, we increase (decrease) the sam-
pling weights for those points from small (large) clus-
ters. Compared to the BCSL approach, this weighted
SL (WSL) approach has no waste of information when
estimating the cluster centers based on partition cen-
troids, given we know the ideal sampling weights.
However, the WSL appears to incur a chicken and egg
problem as the ideal sampling weights requires knowl-

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Algorithm 4: Bias-corrected sketch-and-lift algo-
rithm for K-means SDP.
Input: subsampling factor γ ∈ (0, 1).

1 Independently sample an index subset T ⊂ [n] via
Ber(γ) and make the subsampled data matrix
V = (Xi)i∈T .

2 Run subroutine Algorithm 2 with input V to get

a partition estimate ˆR1, . . . , ˆRK for T .

3 For each ˆRk, randomly sample a subset ˜Rk with

equal size m.

k ∈ [K].

4 Compute the centroids ¯Xk = | ˜Rk|−1 (cid:80)

j∈ ˜Rk

Xj for

5 For each i ∈ [n] \ T , assign i ∈ ˆGk if

(cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107),
Output: A partition estimate ˆG1, . . . , ˆGK for [n].

∀l (cid:54)= k, l ∈ [K].

edge on the cluster membership of each data point.
Fortunately, as we will discuss in Remark 3.3, a multi-
round extension of the WSL which iteratively applies
the WSL to reﬁne the sampling weights based on the
clustering labels in the previous round has surprisingly
good numerical performance in that the recovery er-
ror decays as the round increases (cf. Figure 18 in the
supplement). Now let us formally describe the WSL
approach. For each data point Xi for i ∈ G∗
k, we de-
note w∗
i = γn/(Knk) as the ideal sketch weight for
Xi. Suppose in practice we have a set of approximat-
ing weights wi ∈ [0, 1] such that there exists a subset
D ⊂ [n] which satisﬁes for some ((cid:15), δ) ∈ [0, ∞) × [0, 1]

|D| ≥ (1 − δ) n and

(cid:12)
(cid:12)
(cid:12)
(cid:12)

wi
w∗
i

(cid:12)
(cid:12)
− 1
(cid:12)
(cid:12)

≤ (cid:15).

(6)

Condition (6) requires the at least (1 − δ) proportion
of constructed sampling weights should be close to the
true sampling weights with at most (cid:15) distortion. We
call such weights a set of ((cid:15), δ)-weights. Ideal weights
are (0, 0)-weights.
In practice, a priori estimate for
the weights wi can be set through Lloyd’s algorithm
for the K-means. And the parameter γ can be chosen
as small as O(log(n)/n), which implicitly shows that
the weights wi’s would be as small as o(1). The rest
of the WSL is the same as Algorithm 1 with a general
non-uniform sampling weighs (w1, . . . , wn).
In addi-
tion, we can also combine the BCSL with the WSL
to enforce the equal cluster sizes when computing the
centroids ¯Xk’s. The WSL algorithm is summarized in
Algorithm 5.
Remark 3.3 (Multi-round WSL). The priori estimate
for the weights (e.g., by Lloyd’s or K-means++ al-
gorithm) can be viewed as a warm start of WSL. To
further boost the performance of the WSL, we can
iteratively apply WSL to reﬁne the recovered cluster
labels, which is summarized in Algorithm 6 below. In

Algorithm 5: Weighted sketch-and-lift algorithm
for K-means SDP.
Input: subsampling factor γ ∈ (0, 1).

1 Run Lloyd’s algorithm to obtain an initial

partition estimate ˜G1, . . . , ˜GK for [n], with sizes
˜n1, . . . , ˜nK.

2 For each i ∈ [n], set wi = γn/(K ˜nk) if i ∈ ˜Gk.
3 Run Algorithm 1 with weights (w1, . . . , wn).

Output: A partition estimate ˆG1, . . . , ˆGK for [n].

Section 5, we present some superior numerical result
for the multi-round WSL.

Algorithm 6: Multi-round weighted sketch-and-
lift algorithm for K-means SDP.
Input: subsampling factor γ ∈ (0, 1) and number

of rounds R.

1 Run Algorithm 5 to get partition estimate

˜G1, . . . , ˜GK for [n].
2 for r = 2, . . . , R do

3

4

For each i ∈ [n], update sampling weight
wi = γn/(K ˜nk) if i ∈ ˜Gk, where ˜nk = | ˜Gk|.
Run Algorithm 1 with weights (w1, . . . , wn) to
update ˜G1, . . . , ˜GK.

Output: A partition estimate ˜G1, . . . , ˜GK for [n].

4 EXACT RECOVERY

GUARANTEES

To study the theoretic properties of SL, we follow the
literature by using the standard GMM as our working
model. Speciﬁcally, we assume X1, . . . , Xn to be from
the following data generating model: if i ∈ G∗

k, then

Xi = µk + (cid:15)i,

(7)

where µ1, . . . , µK ∈ Rp are the (unobserved) cluster
centers and (cid:15)i are i.i.d. N (0, σ2Ip) noise. Recall that
∆2 = min1≤k(cid:54)=l≤K (cid:107)µk −µl(cid:107)2 denotes the squared min-
imal separation between cluster centers.
Theorem 4.1 (Separation bound for exact recovery by
SL). Let α > 0 and γ ∈ (0, 1) be the subsampling
ratio. Suppose that n1 = · · · = nK = n/K. If ∆2 ≥
(1 + α) ¯∆2

γ, where

(cid:32)

(cid:115)

¯∆2

γ = 4σ2

1 +

1 +

(cid:33)

Kp
γn log n

log n,

(8)

then the output ˆG1, . . . , ˆGK from the SL Algorithm 2
for the K-means SDP achieves exact recovery,
i.e.,
ˆGk = G∗
k for all k ∈ [K] with probability at least

Yubo Zhuang, Xiaohui Chen, Yun Yang

log(γn)
1 − C1(log(γn))−C2 , provided that K ≤ C3
log log(γn)
and p ≤ C4(γn/K)2, where Ci, i = 1, 2, 3, 4 are con-
stants depending only on α.

Theorem 4.1 considers the equal cluster case, and in-
cludes the exact recovery property for the original SDP
K-means (3) as a special case when γ = 1 (and no
lift step is needed). Compared to the separation cut-
oﬀ value (4) for exact recovery of the entire data, the
separation requirement in (8) has an extra factor of
γ−1 inside the square root—the larger Kp/(γn log n)
term corresponds to the statistical ﬂuctuation of using
only γn/K samples to estimate the p-dimensional clus-
ter centers instead of n/K samples in the entire data,
which appears to be inevitable for any single-epoch SL
method. Interestingly, as we empirically observed in
Section 5, the multi-epoch SL method summarized in
Algorithm 3 has a noticeable improvement over the SL
and appears to attain the optimal cutoﬀ value (4) due
to the usage of almost all data in estimating the clus-
ter centers (by averaging across subsamples). Based
on the numeric evidence, we pose the following con-
jecture as a future study.
Conjecture 4.2. Multi-epoch SL method with aver-
aging (Algorithm 3) attains the information-theoretic
threshold ¯∆2
∗ in (4) as the SDP (3) on the entire data.

For general subsampling ratio γ ∈ (0, 1), we note that
there is an additional constraint p (cid:46) (γn/K)2 to en-
sure the exact recovery. This constraint can be shown
even stricter p (cid:46) (γn/K) for unequal cluster size case.
This condition comes from the fact that when lift is
needed to obtain the full cluster labels on all data
points, we need to ensure that the estimated cluster
sizes from the subsampled SDP (5) are approximately
equal to match the variances.
In contrast, we shall
show that in Theorems 4.3 and 4.4 below that the
BCSL does not require this condition, and the WSL
still requires this condition p (cid:46) (γn/K)2, but both will
work for the unequal cluster size case as well.

Theorem 4.3 (Separation bound for exact recovery by
BCSL). Let α > 0, γ ∈ (0, 1) be the subsampling ratio
and n = mink∈[K] nk. If ∆2 ≥ (1 + α) ¯∆

(cid:48)2
γ , where

¯∆

(cid:48)2
γ = 4σ2

(cid:114)

(cid:18)

1 +

1 +

(cid:19)

p
γn log n

log n,

(9)

then the output ˆG1, . . . , ˆGK from the BCSL Al-
gorithm 4 achieves exact recovery with probabil-
ity at least 1 − C1(log γn)−C2, provided that K ≤
log(γn)
log log(γn) , n ≥ C4n/ log(γn), log n/n ≤ C5γ, where
C3
Ci, i = 1, 2, 3, 4, 5 are only depend on α.

According to Theorem 4.3, the subsampling factor γ
can be as small as O(cid:0)(log n)2/n(cid:1), corresponding to
a minimal subsample size m = O((log n)2). Conse-

quently, the overall time complexity is dominated by
the O(n) complexity of the lift step.

Theorem 4.4 (Separation bound for exact recovery by
WSL). Suppose we have a set of ((cid:15), δ)-weights wi ∈
[0, 1] satisfying (6). Let α > 0. If ∆2 ≥ (1 + α) ¯∆2
γ,
where ¯∆2
γ is deﬁned in (8), then the WSL Algorithm 5
achieves exact recovery with probability at least 1 −
C1(log γn)−C2, provided that

K ≤ C3(log γn)/(log log γn), p ≤ C4(γn/K)2,
δ ≤ C5(n/n) min (cid:8)1, (cid:112)γn/p (cid:9),
(cid:15) ≤ C6 min (cid:8)1, γn log n/(Kp)(cid:9),

where constants Ci, i = 1, . . . , 6, only depend on α.

We remark that the WSL by adjusting the bias with
adaptive (non-uniform) weights essentially reduces the
unequal sizes case to the equal size case. The cost of
choosing the adaptive weights is that we need impose
size conditions on ((cid:15), δ) (e.g., (cid:15), δ = o(p−c) for some
c > 0) such that they can absorb the eﬀect coming
from the growth of p. In Section B in the supplement,
we evaluate the eﬀect of initial weights by K-means++
algorithm. In addition, from the numerical results, we
conjecture that the multi-round WSL summarized in
Algorithm 6 can further relax the conditions to achieve
exact recovery, for example, by throwing away the p ≤
C4(γn/K)2 constraint. We leave its formal theoretical
study to a future direction.

5 NUMERICAL EXPERIMENTS

In this section, we test the numerical performance for
the SL method and its variants, and compare them
with the K-means++ algorithm on synthetic data.
MATLAB code implementing the SL approach and its
variants are available at: https://github.com/Yub
o02/Sketch-and-Lift-Scalable-Subsampled-Semi
definite-Program-for-K-means-Clustering

5.1 Setup

We generate data from a 4-component Gaussian mix-
ture model (7) parametrized by (p, n, λ∗), where pa-
rameter λ∗ > 0 characterizes the cluster center sep-
aration through ∆2 = (λ∗ ¯∆∗)2 and recall that ¯∆2
∗ is
the theoretical cutoﬀ in (4). We compare the following
clustering methods.

• M0 is the Matlab build-in K-means clustering im-
plementation (default algorithm is K-means++).
• M1 is the sketch-and-lift (SL) method described

in Algorithm 1.

• M2 is the bias-corrected sketch-and-lift (BCSL)

method described in Algorithm 4.

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

• M3 is the weighted sketch-and-lift (WSL) method

described in Algorithm 5.

• M4 is the multi-epoch sketch-and-lift (ME-SL)

with averaging described in Algorithm 3.

• M5 is the multi-round weighted sketch-and-lift
(MR-WSL) method described in Algorithm 6 with
output at the 4-th round.

For the SL methods (M1-M5), we vary the subsam-
pling factor γ. We choose round number as 4 in
M5 since according to the additional numerical re-
sults reported in the supplement, the MR-WSL typ-
ically reaches its best performance after 3-4 rounds.
We also compare with the original SDP relaxed K-
means method (3) (method O) on the entire data
points whenever it is feasible to run (in our case when
n ≤ 3000). We report the error rate in recovering clus-
ter labels and the running time for these algorithms
averaged over 100 simulations.

5.2 Results

Due to the space limit, we report simulation results
for equal cluster size case in this subsection. For com-
plete numerical experiment results including the un-
equal cluster size case, we refer to Section B in the
supplementary material.

The baseline setup is p = 1000, n = 2000, γ = 0.1
and λ∗ = 1.2, expect when γ is changing, we use
In each simulation setting, we vary one
n = 10000.
parameter and report the error rate, which is summa-
rized in Figure 1. Figure 2 compares the runtimes as n
changes. We observe that all SL methods have signiﬁ-
cantly and uniformly smaller error rate than the state-
of-the-art K-means++ method (blue solid curve) in
nearly all setups. We also note that runtime curves
of the SL-methods on the log-scale are parallel to the
K-means++ algorithm, indicating that SL methods
have the same linear O(n) complexity as the fast K-
means++ (with diﬀerence only occurring in the lead-
ing constant).
In comparison, the original SDP (O)
has super linear complexity as expected.

One interesting observation is that the multi-epoch
SL method (M4) is almost always the best method
across all four settings in Figure 1, and has compa-
rable performance as the original SDP method (O)
in the range when it is feasible to run. Note that
with sample size n = 2000 as the baseline, only mis-
specifying one cluster label in 1 out of 100 replicates
will incur an error rate of 5 × 10−6, meaning that an
average error rate (over 100 replicates) of order be-
low (or around) 10−5 can be viewed as perfect clus-
tering (due to the log-scale, we display 10−6 when the
actual error is zero). This empirical observation pro-
vides the numerical evidence for supporting Conjec-

Figure 1: Log-scale error rates (with error bars) when
one parameter varies. Zero error is displayed as 10−6
in the log-scale plot. Red vertical line in the lowest plot
indicates theoretical threshold ¯∆2

γ for SL methods.

100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesM1M2M3M4M5M020004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesM1M2M3M4M5M0O0.020.040.060.080.10.120.140.160.180.210-610-510-410-310-210-1100Error rate  Error rate when  changesM1M2M3M4M5M00.511.5*10-610-510-410-310-210-1100Error rate  Error rate when * changesM1M2M3M4M5M0Yubo Zhuang, Xiaohui Chen, Yun Yang

The ﬁrst one considers 32-dimensional
datasets.
mass cytometry (CyTOF) dataset, consisting of pro-
tein expression levels of healthy human bone marrow
mononuclear cells (BMMCs) from two healthy individ-
uals. Following Levine et al. (2015), we run clustering
analysis on individual H1, where n = 72463 cells were
assigned to populations and p = 32. We report clus-
tering results with K = 14 and γ = 0.02. The misclas-
siﬁcation error for kmeans++ (our Algorithm 6 with
1-st round using kmeans++ as initialization) is 0.5709
(0.4719) with time cost 0.8757 (226.5155).

The second one is for unbalanced synthetic 2-D Gaus-
sian clusters data presented by Rezaei and Fr¨anti
(2016), where n = 6500, p = 2, K = 8, γ = 0.01. The
misclassiﬁcation error for kmeans++ (our Algorithm 6
with 1-st round using kmeans++ as initialization) is
0.4301 (0.2213) and time cost is 0.0122 (0.5257). Thus,
the SL method is robust to the GMM assumption and
can improve the accuracy on top of kmeans++ with
similar scalability (both time costs increased by O(102)
times as n becomes O(102) times larger).

Acknowledgements

Xiaohui Chen was partially supported by NSF CA-
REER Award DMS-1752614. Yun Yang was partially
supported by NSF DMS-1907316.

References

P. Abdalla and A. S. Bandeira. Community detection
with a subsampled semideﬁnite program, 2021.

D. Achlioptas and F. McSherry. On spectral learning
of mixtures of distributions. In P. Auer and R. Meir,
editors, Learning Theory, pages 458–469, Berlin,
Heidelberg, 2005. Springer Berlin Heidelberg. ISBN
978-3-540-31892-7.

F. Alizadeh.

Interior point methods in semideﬁnite
programming with applications to combinatorial op-
timization. SIAM J. Optim., 5(1):13–51, 1995.

D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
Np-hardness of euclidean sum-of-squares clustering.
Machine learning, 75(2):245–248, 2009.

D. Arthur and S. Vassilvitskii. K-means++: The ad-
In Proceedings of the
vantages of careful seeding.
Eighteenth Annual ACM-SIAM Symposium on Dis-
crete Algorithms, SODA ’07, page 1027–1035, USA,
2007. Society for Industrial and Applied Mathemat-
ics. ISBN 9780898716245.

Improved spectral-norm
P. Awasthi and O. Sheﬀet.
bounds for clustering.
In Approximation, Ran-
domization, and Combinatorial Optimization. Algo-
rithms and Techniques, pages 37–49. Springer, 2012.

Figure 2: Log-scale runtime (with error bars) v.s. n.

ture 4.2 about the information-theoretic optimality of
the multi-epoch SL.

We also report the error decay for the MR-WSL with
K-means++ algorithm as the warm start in Section B
in the supplement, where we observed that the MR-
WSL has a surprisingly good recovery performance af-
In particular, from Figure 1 we can
ter 3-4 rounds.
see that the MR-WSL (M5) is the second best method
in most settings (with the best being M4), and has
signiﬁcant improvements over its single round coun-
terpart WSL (M3) due to progressive reﬁnements on
the estimated sampling weights (cf. Section B).

The plot at the very bottom in Figure 1 shows that
the error rates as we change the separation parameter
∆2 = min1≤k(cid:54)=l≤K (cid:107)µk − µl(cid:107)2, where the red verti-
cal line indicates the theoretical threshold ¯∆2
γ for SL
methods given in Theorem 4.1. Since error of order
10−5 is very close to perfect clustering, the numerical
results in the plot are consistent with our theory that
¯∆2
γ characterizes the cutoﬀ value for SL methods.

We also assess the impact of initialization (i.e., warm
start eﬀect) of WSL by K-means by looking at the es-
timated ((cid:15), δ) parameters (Table 1 for the 1-st round in
Section B). In particular, we ﬁnd that for ﬁxed (cid:15) = 0.2,
δ = 0.25373, 0.25598, 0.37208, 0.33282, 0.38929 for
p = 200, 500, 2000, 4000, 6000, respectively. Thus
for increasing p corresponding to more diﬃcult clus-
tering problems, the quality of initial weights deteri-
orates by the K-means. Still, our WLS (in particu-
lar, the MR-WLS reﬁnement) maintains good quality
of cluster label recovery. And δ = 0.000005, 0.00006,
0.028845, 0.088965, 0.15886, respectively when we per-
form the 2-nd round of WSL (Table 2 in Section B).
Finally we can see that δ = 0 uniformly for the 4-th
round WSL (Table 4 in Section B). This shows that the
multi-round WSL reﬁnes the clustering errors and can
eventually achieve the exact recovery as if we initialize
with the ideal weights.

Finally, we applied our method to two benchmark

20004000600080001000012000n10-1100101102Time cost (s)  Time cost when n changesM1M2M3M4M5M0OSketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

P. Awasthi, A. S. Bandeira, M. Charikar, R. Krish-
naswamy, S. Villar, and R. Ward. Relax, no need
to round: Integrality of clustering formulations. In
Proceedings of the 2015 Conference on Innovations
in Theoretical Computer Science, ITCS ’15, pages
191–200, New York, NY, USA, 2015. ACM. ISBN
978-1-4503-3333-7.

A. Bluhm and D. Stilck Fran¸ca. Dimensionality re-
duction of sdps through sketching. Linear Algebra
and its Applications, 563:461–475, 2019. ISSN 0024-
3795.

F. Bunea, C. Giraud, M. Royer, and N. Verzelen.
PECOK: a convex optimization approach to vari-
able clustering. arXiv:1606.05100, 2016.

X. Chen and Y. Yang. Diﬀusion k-means clustering on
manifolds: Provable exact recovery via semideﬁnite
relaxations. Applied and Computational Harmonic
Analysis, 52:303–347, 2021a. ISSN 1063-5203.

X. Chen and Y. Yang. Cutoﬀ for exact recovery of
gaussian mixture models. IEEE Transactions on In-
formation Theory, 67(6):4223–4238, 2021b.

P. Drineas and M. W. Mahoney. Lectures on random-

ized numerical linear algebra, 2017.

Y. Fei and Y. Chen.

Hidden integrality of
sdp relaxation for sub-gaussian mixture models.
arXiv:1803.06510, 2018.

C. Giraud and N. Verzelen.

recovery
bounds for clustering with the relaxed kmeans.
arXiv:1807.07547, 2018.

Partial

H. Jiang, T. Kathuria, Y. T. Lee, S. Padmanabhan,
and Z. Song. A faster interior point method for
semideﬁnite programming. In 2020 IEEE 61st An-
nual Symposium on Foundations of Computer Sci-
ence (FOCS), pages 910–918, 2020.

A. Kumar and R. Kannan. Clustering with spectral
norm and the k-means algorithm.
In 2010 IEEE
51st Annual Symposium on Foundations of Com-
puter Science, pages 299–308. IEEE, 2010.

B. Laurent and P. Massart. Adaptive estimation of a
quadratic functional by model selection. The Annals
of Statistics, 28(5):1302 – 1338, 2000.

J. H. Levine, E. F. Simonds, S. C. Bendall, K. L. Davis,
E. ad D. Amir, M. D. Tadmor, O. Litvin, H. G.
Fienberg, A. Jager, E. R. Zunder, R. Finck, A. L.
Gedman, I. Radtke, J. R. Downing, D. Pe’er, and
G. P. Nolan. Data-driven phenotypic dissection of
aml reveals progenitor-like cells that correlate with
prognosis. Cell, 162(1):184–197, 2015. ISSN 0092-
8674.

imity, and conic programming. arXiv:1710.06008,
2017.

S. Lloyd. Least squares quantization in pcm. IEEE
Transactions on Information Theory, 28:129–137,
1982.

Y. Lu and H. Zhou. Statistical and computational
guarantees of lloyd’s algorithm and its variants.
arXiv:1612.02099, 2016.

J. MacQueen. Some methods for classiﬁcation and
analysis of multivariate observations. Proc. Fifth
Berkeley Sympos. Math. Statist. and Probability,
pages 281–297, 1967.

M. Meila and J. Shi. Learning segmentation by ran-
dom walks.
In In Advances in Neural Informa-
tion Processing Systems, pages 873–879. MIT Press,
2001.

D. G. Mixon and K. Xie. Sketching semideﬁnite pro-

grams for faster clustering, 2020.

D. G. Mixon, S. Villar, and R. Ward. Clustering
subgaussian mixtures by semideﬁnite programming.
arXiv:1602.06612v2, 2016.

M. Ndaoud. Sharp optimal recovery in the two com-
ponent gaussian mixture model. arXiv:1812.08078,
2018.

A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clus-
tering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems, pages 849–
856. MIT Press, 2001.

J. Peng and Y. Wei. Approximating k-means-type
clustering via semideﬁnite programming. SIAM J.
OPTIM, 18(1):186–205, 2007.

D. Pollard. Strong consistency of k-means clustering.

Ann. Statist., 9(1):135–140, 1981.

M. Rezaei and P. Fr¨anti. Set-matching methods for
external cluster validity. IEEE Trans. on Knowledge
and Data Engineering, 28(8):2173–2186, 2016.

M. Royer. Adaptive clustering through semideﬁnite
programming. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 30, pages 1795–1803. Cur-
ran Associates, Inc., 2017.

A. Saxena, M. Prasad, A. Gupta, N. Bharill, O. P.
Patel, A. Tiwari, M. J. Er, W. Ding, and C.-T. Lin.
A review of clustering techniques and developments.
Neurocomputing, 267:664–681, 2017.

S. Vempala and G. Wang. A spectral algorithm for
learning mixture models. J. Comput. Syst. Sci, 68:
2004, 2004.

X. Li, Y. Li, S. Ling, T. Stohmer, and K. Wei. When
do birds of a feather ﬂock together? k-means, prox-

U. von Luxburg. A tutorial on spectral clustering.
Statistics and Computing, 17(4):395–416, 2007.

Yubo Zhuang, Xiaohui Chen, Yun Yang

U. von Luxburg, M. Belkin, and O. Bousquet. Consis-
tency of spectral clustering. Annals of Statistics, 36
(2):555–586, 2008.

A. Yurtsever, M. Udell, J. A. Tropp, and V. Cevher.
Sketchy decisions: Convex low-rank matrix opti-
mization with optimal storage, 2017.

Supplementary Material:
Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for
K-means Clustering

A PROOF OF MAIN RESULTS

In this section, we prove the main result of this paper.

A.1 Auxiliary lemmas

k| denotes the number of data points in k-th cluster and ∆2 = min1≤k(cid:54)=l≤K (cid:107)µk − µl(cid:107)2 is the

Recall that nk = |G∗
minimal separation between cluster centers. Set N = min1≤k(cid:54)=l≤K{ 2nknl
nk+nl
Lemma A.1 (Separation bound for exact recovery by the full SDP: general case). If there exist constants ˜δ > 0
and β ∈ (0, 1) such that

} and n = mink∈[K] nk.

and

with

log n ≥

(1 − β)2
β2

C1n
N

, ˜δ ≤

β2
(1 − β)2

C2
K

, N ≥

4(1 + ˜δ)2
˜δ2

,

∆2 ≥

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

p
N log n

(cid:33)

+ C3rn

log n

rn =

(1 − β)2
(1 + ˜δ) log n

(cid:18) √

p log n
n

+

(cid:19)

,

log n
n

then the SDP in (3) achieves exact recovery with probability at least 1 − C4 K 2 n−˜δ, where Ci, i = 1, 2, 3, 4, are
universal constants.

Lemma A.1 is proved by Chen and Yang (2021b) (Theorem II.1). Specializing Lemma A.1 to equal cluster case
n1 = · · · = nK = n/K, we have the following corollary.
2
Corollary A.2 (Separation bound for exact recovery by the full SDP: equal cluster case). Let α > 0 and ∆
1 be
deﬁned in (8). Suppose that the cluster sizes are equal and K ≤ C1 log(n)/ log log(n) for some small constant
2
C1 > 0 depending only on α. If ∆2 ≥ (1 + α)∆
1, then the SDP in (3) achieves exact recovery with probability
at least 1 − C2(log n)−c3 , where C2, c3 are constants depending only on α.

A.2 Proof outline for Theorem 4.1

Before presenting the rigorous proof, we ﬁrst discuss the overall strategy for proving Theorem 4.1 for equal cluster
size case, which can be divided into three steps. Proofs for Theorems 4.3 and 4.4 have the same architecture.
We deﬁne the events

A :=

B :=

(cid:110) ˆG1 = G∗
1, . . . , ˆGK = G∗
(cid:110) ˆR1 = R1, . . . , ˆRK = RK
(cid:110)

K

(cid:111)
,

(cid:111)
,

Bτ :=

(1 − τ )

≤ |Rk| ≤ (1 + τ )

nγ
K

, ∀k ∈ [K]

(cid:111)
,

nγ
K

where τ ∈ (0, 1) and Rk = G∗

k ∩ T . Since probability of wrong recovery satisﬁes
τ ) + P(Bc|Bτ ) + P(Ac ∩ B|Bτ ),

P(Ac) ≤ 2P(Bc

(10)

Yubo Zhuang, Xiaohui Chen, Yun Yang

it suﬃces to bound the three terms on the right-hand side of (10), where the ﬁrst term is due to the tolerance of
approximate equality of the cluster sizes on subsampled data (step 1), the second term is due to wrong recovery
using the subsampled SDP (step 2), and the third term is due to the lifting procedure (step 3) on the data points
that are not sampled in step 1.

In step 1, we reduce the exact recovery problem on the entire n data points to the subsampled m ≈ nγ data
points with K clusters with approximately equal size that resembles the problem structure of the original SDP
clustering problem. The violation probability of approximate cluster size in step 1 can be controlled by the
classical Chernoﬀ bound

P(Bc

τ ) ≤ 2n−1

for τ (cid:16) (cid:112)K log(n)/nγ.

In step 2, since the subsampling procedure is independent of the original data points, we can treat the subsampled
data points V = (Xi)i∈T as a new clustering problem. Based on this observation, we establish the exact
recovery guarantee on subsampled data using the separation upper bound proved for the general unbalanced
GMM by Chen and Yang (2021b), from which we show that if the minimal separation ∆2 ≥ (1 + α) ¯∆2
γ and
K (cid:46) log(γn)/ log log(γn), then

P(Bc|Bτ ) (cid:46) 1/ logc(γn).
In step 3, we use the nearest centroids ¯X1, . . . , ¯XK estimated from step 2 for propagating the solution from the
subsampled SDP to all data points that are not sampled in step 1. The key structure for the lift step to be
successful is the independence between V and X \V . In particular, the independence among ¯Xk, ¯Xl, Xi, i ∈ [n]/T
entails that under the minimal separation ∆2 ≥ (1+α) ¯∆2
γ, the error probability for the nearest-centroid procedure
assigning i ∈ ˆGk via (cid:107)Xi − ¯Xl(cid:107)2 > (cid:107)Xi − ¯Xk(cid:107)2 for all i ∈ G∗

k \ T vanishes

P(Ac ∩ B|Bτ ) ≤ K 2/nc.

Combining the above three steps with the master bound (10), we conclude that the probability of exact recovery
P(A) ≥ 1 − C log−c(γn) for large enough n, ensuring correctness of the SL approach.

A.3 Proof of Theorem 4.1

Step 1: reduction to subsampled data points. Let T ⊂ [n] be the subsampled data point indices so that
V = (Xi)i∈T ⊂ X and m = |T |. Let Rk = G∗
k ∩ T . Deﬁne the events
(cid:110) ˆG1 = G∗
1, . . . , ˆGK = G∗
(cid:110) ˆR1 = R1, . . . , ˆRK = RK
(cid:110)

B :=

A :=

(cid:111)
,

(cid:111)
,

K

Bτ :=

(1 − τ )

≤ |Rk| ≤ (1 + τ )

, ∀k ∈ [K]

nγ
K

(cid:111)
,

nγ
K

where τ ∈ (0, 1). Observe that

P(Ac) =P(Ac ∩ (B ∩ Bτ )) + P(Ac ∩ (Bc ∪ Bc

τ ))

≤P(Ac ∩ B|Bτ )P(Bτ ) + P(Ac ∩ Bc) + P(Ac ∩ Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc ∩ Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc|Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Bc|Bτ ) + 2P(Bc

τ ).

Thus to bound the error probability for exact recovery, it suﬃces to bound the three terms on the right-hand
side of the last inequality. Since the subsampled data points V from X = (X1, . . . , Xn) are drawn with i.i.d.
Ber(γ), we apply the Chernoﬀ bound and the union bound to get

Choosing τ = (cid:112)6K log(n)/nγ, we have

P(Bc

τ ) ≤ 2K exp

(cid:16)

−

τ 2nγ
3K

(cid:17)

.

P(Bc

τ ) ≤ 2n−1.

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Step 2: exact recovery for subsampled data. Since the subsampling procedure is independent of the
original Xi points, we can treat the Xi ∈ V as the new cluster problem to apply Lemma A.1 with T = (cid:83)K
k=1 Rk,
n = m and nk = mk, where mk = |Rk|. In particular, if there exist constants ˜δ > 0 and β ∈ (0, 1) such that

log m ≥

(1 − β)2
β2
C2
K

β2
(1 − β)2

C1m
M

,

, N ≥

˜δ ≤

4(1 + ˜δ)2
˜δ2

,

and

with

∆2 ≥

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

p
M log m

(cid:33)

+ C3rm

log m

rm =

(1 − β)2
(1 + ˜δ) log m

√

(cid:16)

p log m
m

+

log m
m

(cid:17)

,

, then the SDP achieves exact recovery, i.e., ˆRk = Rk, ∀k ∈
where m = mink∈[K] mk and M = min1≤k(cid:54)=l≤K
[K], with probability at least 1 − C4K 2m−˜δ, where Ci, i = 1, 2, 3, 4 are universal constants. Note that on event
Bτ , we have

2mlmk
ml+mk

(1 − τ )nγ ≤ m ≤ (1 + τ )nγ,
2mlmk
ml + mk

2
l + m−1

m−1

=

k

≥ (1 − τ )

nγ
K

.

Thus on the event Bτ , we can choose an upper bound ∆(cid:48)2 :

∆(cid:48)2 :=

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

pK/((1 − τ )γn)
log((1 + τ )γn)

(cid:33)

+ C3r(cid:48)
m

log((1 + τ )γn)

with

r(cid:48)
m =

(1 − β)2
(1 + ˜δ) log((1 + τ )γn)

(cid:32)

K(cid:112)p log((1 + τ )γn)
(1 − τ )γn

+

K log((1 + τ )γn)
(1 − τ )γn

(cid:33)

.

Note that τ = (cid:112)6K log(n)/nγ = o(1) under the assumption K log n
enough β and ˜δ that may also also depend on α, we have for large enough n, if K ≤ C1
constant C1 depending on α and ∆2 ≥ (1 + α) ¯∆2

n = o(γ). Fix an α > 0. By choosing small
log(γn)
log log(γn) for some

γ, where

(cid:32)

(cid:115)

¯∆2

γ = 4σ2

1 +

1 +

(cid:33)

Kp
γn log n

log n,

then SDP achieves exact recovery with probability at least 1 − C2(log(γn))−C3, where C2, C3 depend only on α.
Thus we conclude that

P(Bc|Bτ ) ≤ C2(log(γn))−C3 .

Remark A.3 (Lower bound for γ). It can be seen that the lower bound condition K log n
relaxed to K = o(nγ/ log log(γn)) given K ≤ C1
to interpret the lower bound for γ.

log(γn)
log log(γn) . i.e., we can think of K ≤ C1

n = o(γ) for γ can be
log(γn)
log log(γn) as another way

Step 3: lift solution from sketched SDP to all the data points. Recall that the lift solution to all n data
points is deﬁned as

ˆGk =

(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ ˆRk,

Yubo Zhuang, Xiaohui Chen, Yun Yang

where ¯Xk = (cid:80)
Xj/mk is the centroid of the k-th cluster output from the subsampled SDP. Since our goal
in this step is to bound P(Ac ∩ B|Bτ ), the subsequent analysis will be on the event B, that is ˆRk = Rk for all
k ∈ [K]. Then we have ¯Xk = (cid:80)

Xj/mk and

j∈ ˆRk

j∈Rk

ˆGk =

(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ Rk.

(cid:110)
(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ

kl =

Let A(i)
Recall that Xi = µk + (cid:15)i, ∀i ∈ G∗
¯(cid:15)k = (cid:80)

(cid:15)i/mk. For i ∈ G∗

(cid:111)

j∈Rk

, where i ∈ G∗

k\T, where ξ is some number to be determined.
k, where (cid:15)i are i.i.d. N (0, σ2Ip). Denote similarly ¯Xk = µk + ¯(cid:15)k, where

k\T , we note that (cid:15)i, ¯(cid:15)k, ¯(cid:15)l are independent. We can write

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2
=(cid:107)θ + (cid:15)i − ¯(cid:15)l(cid:107)2 − (cid:107)(cid:15)i − ¯(cid:15)k(cid:107)2
=(cid:107)θ(cid:107)2 + (cid:107)¯(cid:15)l(cid:107)2 − (cid:107)¯(cid:15)k(cid:107)2 − 2(cid:104)θ, ¯(cid:15)l(cid:105) + 2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105),

where θ = µk − µl. Set ζn = 2 log(Kn) and deﬁne

B(i)

kl,1 :=

(cid:110)

(cid:107)¯(cid:15)l(cid:107)2 ≥ m−1

l

(p − 2(cid:112)pζn),

(cid:107)¯(cid:15)k(cid:107)2 ≤ m−1

k (p + 2(cid:112)pζn + 2ζn),
(cid:113)
(cid:111)
2m−1

l ζn(cid:107)θ(cid:107)

(cid:104)θ, ¯(cid:15)l(cid:105) ≤

and

B(i)

kl,2 :=

(cid:110)

(cid:107)¯(cid:15)l − ¯(cid:15)k(cid:107)2 ≤ (m−1

l + m−1

k )(p + 2(cid:112)pζn + 2ζn),

(cid:104)θ, ¯(cid:15)k − ¯(cid:15)l(cid:105) ≤

(cid:113)

2(m−1

l + m−1

k )ζn(cid:107)θ(cid:107)

(cid:111)
.

kl,1 ∪ B(i)
kl = B(i)
Let B(i)
2000), we have P(B(i)c

kl,2. Using the standard tail probability bound for χ2 distribution (Laurent and Massart,
kl ) ≤ 5/(n2K 2). Since

we have on the event B(i)

kl that

(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105)|{¯(cid:15)l, ¯(cid:15)k} ∼ N (0, (cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2),

C∗ : = P

(cid:16)

2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105) ≤ −(1 − β)(cid:107)θ(cid:107)2(cid:12)
(cid:12)
(cid:12)¯(cid:15)k, ¯(cid:15)l

(cid:17)

(cid:33)

(cid:32)

(cid:32)

= 1 − Φ

≤ 1 − Φ

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)(cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2
(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

(cid:33)
,

n

where β ∈ (0, 1),

n = (cid:107)θ(cid:107)2 + 2
r(cid:48)(cid:48)

(cid:113)

2(m−1

l + m−1

k )ζn(cid:107)θ(cid:107) + (m−1

l + m−1

k )(p + 2(cid:112)pζn + 2ζn).

k )ζn ≤ (cid:107)θ(cid:107)(cid:112)2/M . Now we choose η > 0 such that
l + m−1
Note that (cid:107)θ(cid:107)2 ≥ ∆2 ≥ 8 log n, which implies
1+η ≥ 1 + 2(cid:112)2/M (i.e., M ≥ 8(1 + η−1)2). In order to have C∗ be bounded by n−(1+η), it is suﬃcient to require
1+2η
that

2(m−1

(cid:113)

(cid:32)

1 − Φ

(cid:33)

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

n

≤ 1 − Φ((cid:112)2(1 + η) log n).

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

or further

(1 − β)2
8(1 + η) log n

(cid:107)θ(cid:107)4 − (1 + 2(cid:112)2/M )(cid:107)θ(cid:107)2 − (p + 2(cid:112)pζn + 2ζn)(m−1

l + m−1

k ) ≥ 0.

A suﬃcient condition for the last display is

∆2 ≥

4σ2(1 + 2η)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + 2η)

p
M log n

(cid:33)

+ 2r(cid:48)(cid:48)(cid:48)
n

log n,

r(cid:48)(cid:48)(cid:48)
n =

(1 − β)2
(1 + 2η) log n

(cid:16) (cid:112)p log(nK)
m

+

log(nK)
m

(cid:17)

.

ξ =

mk − ml
mkml

(cid:115)

p + β(cid:107)θ(cid:107)2 − 4

log(nK)
ml

(cid:107)θ(cid:107) − 2

mk + ml
mkml

(cid:112)2p log(nK) − 4

log(nK)
mk

,

where

Now if we put

then we have

(cid:16)(cid:110)

P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ,

∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

(cid:16) n
(cid:91)

= P

(cid:91)

(cid:17)

A(i)c

kl

i=1

1≤k(cid:54)=l≤K

(cid:16)

P

A(i)c
kl ∩ B(i)

kl

(cid:17)

(cid:16)

+ P

B(i)c

kl

(cid:17)

E[C∗1B(i)

kl,2

] +

5
n

≤

≤

≤

n
(cid:88)

(cid:88)

i=1

1≤k(cid:54)=l≤K

n
(cid:88)

(cid:88)

1≤k(cid:54)=l≤K

i=1
K 2
nη +

7
n

.

Next we claim that ξ > 0. Recall that on the event Bτ , we have mk ∈ [(1 − τ )m∗, (1 + τ )m∗], 1/M ∈
[

], where m∗ = nγ/K. Then,

,

1
(1+τ )m∗

1
(1−τ )m∗

Note that

(cid:12)
(cid:12)
(cid:12)

mk − ml
mkml

(cid:12)
(cid:12)
(cid:12) ≤
p

2τ
(1 − τ )2

p
m∗

≤

√

6p

log n

(1 − τ 2)m3/2

∗

.

(cid:107)θ(cid:107)2 ≥ ¯∆2

γ ≥ 4σ2

(cid:18)

1 +

(cid:114)

1 +

(cid:19)

p
m∗ log n

log n.

So if p = O(γn/K)2), then

for large enough n. Similarly, we have

(cid:12)
(cid:12)
(cid:12)

mk − ml
mkml

(cid:12)
(cid:12)
(cid:12) ≤

p

β
5

(cid:107)θ(cid:107)2

4(cid:112)log(nK)/ml(cid:107)θ(cid:107) ≤
mk + ml
mkml

(cid:112)2p log(nK) ≤

2

β
5

(cid:107)θ(cid:107)2,

β
5

(cid:107)θ(cid:107)2,

4m−1

k log(nK) ≤

β
5

(cid:107)θ(cid:107)2.

Yubo Zhuang, Xiaohui Chen, Yun Yang

For α > 0, we can choose small enough β := β(α, σ) > 0 and η := η(α). Then for n large, we have if ∆2 ≥
(1 + α) ¯∆2

γ, then

P(Ac ∩ B|Bτ )
(cid:16)(cid:110)

=P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > 0, ∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

≤

K 2
nη .

Now, combining all pieces together, we conclude that, for all n large enough,

P( ˆG1 = G∗

1, . . . , ˆGK = G∗

K) ≥ 1 − C(log(γn))−c.

A.4 Proof outline for Theorem 4.3

The overall strategy for proving Theorem 4.3 for unequal cluster size case is identical to the proof of Theorem 4.1,
which can be divided into three steps. We will brieﬂy talk about the diﬀerence here. And all the details are
contained in the proof.

In step 1, we reduce the exact recovery problem on the entire n data points to the subsampled m ≈ nγ data
points with K clusters with approximately γnk many points for each cluster ˆGk that resembles the problem
structure of the original SDP clustering problem. The parameter τ in the classical Chernoﬀ bound now should
be

τ (cid:16) (cid:112)log(n)/nγ.

In step 2, since the subsampling procedure is independent of the original data points, we can treat the subsampled
data points V = (Xi)i∈T as a new clustering problem. Same as proof of Theorem 4.1, we establish the exact
recovery guarantee on subsampled data using the separation upper bound proved for the general unbalanced
(cid:48)2
γ and
GMM by Chen and Yang (2021b), from which we get the similar conditions.
K (cid:46) log(γn)/ log log(γn).

i.e., ∆2 ≥ (1 + α) ¯∆

In step 3, we ﬁrst get the subsampled clusters from step 2 and for each cluster, we randomly down-sample same
size (the minimum one) of points to make the new clusters have the same sample size. Then we use the nearest
centroids ¯X1, . . . , ¯XK of the new clusters for propagating the solution from the subsampled SDP to all data
points that are not sampled in step 1. The independence among ¯Xk, ¯Xl, Xi, i ∈ [n]/T entails that under the
γ, the error probability for the nearest-centroid procedure assigning i ∈ ˆGk
minimal separation ∆2 ≥ (1 + α) ¯∆2
via (cid:107)Xi − ¯Xl(cid:107)2 > (cid:107)Xi − ¯Xk(cid:107)2 for all i ∈ G∗

k \ T vanishes.

A.5 Proof of Theorem 4.3

Step 1: reduction to subsampled data points. Let T ⊂ [n] be the subsampled data point indices so that
V = (Xi)i∈T ⊂ X and m = |T |. Let nk = |G∗
(cid:110) ˆG1 = G∗
1, . . . , ˆGK = G∗
(cid:110) ˆR1 = R1, . . . , ˆRK = RK
(cid:110)

k ∩ T . Deﬁne the events

k|, Rk = G∗

B :=

A :=

(cid:111)
,

(cid:111)
,

K

Bτ :=

(1 − τ )nkγ ≤ |Rk| ≤ (1 + τ )nkγ, ∀k ∈ [K]

(cid:111)
,

where τ ∈ (0, 1). Observe that

P(Ac) =P(Ac ∩ (B ∩ Bτ )) + P(Ac ∩ (Bc ∪ Bc

τ ))

≤P(Ac ∩ B|Bτ )P(Bτ ) + P(Ac ∩ Bc) + P(Ac ∩ Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc ∩ Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc|Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Bc|Bτ ) + 2P(Bc

τ ).

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Thus to bound the error probability for exact recovery, it suﬃces to bound the three terms on the right-hand
side of the last inequality. Since the subsampled data points V from X = (X1, . . . , Xn) are drawn with i.i.d.
Ber(γ), we apply the Chernoﬀ bound and the union bound to get

Choosing τ = (cid:112)6 log(n)/nγ, we have

P(Bc

τ ) ≤ 2K exp

(cid:16)

−

τ 2nγ
3

(cid:17)

.

P(Bc

τ ) ≤ 2n−1.

Step 2: exact recovery for subsampled data. Since the subsampling procedure is independent of the
original Xi points, we can treat the Xi ∈ V as the new cluster problem to apply Lemma A.1 with T = (cid:83)K
k=1 Rk,
n = m and nk = mk, where mk = |Rk|. In particular, if there exist constants ˜δ > 0 and β ∈ (0, 1) such that

and

with

log m ≥

(1 − β)2
β2

C1m
M

,

˜δ ≤

β2
(1 − β)2

C2
K

, N ≥

4(1 + ˜δ)2
˜δ2

∆2 ≥

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

p
M log m

(cid:33)

+ C3rm

log m

rm =

(1 − β)2
(1 + ˜δ) log m

√

(cid:16)

p log m
m

+

log m
m

(cid:17)

,

, then the SDP achieves exact recovery, i.e., ˆRk = Rk, ∀k ∈
where m = mink∈[K] mk and M = min1≤k(cid:54)=l≤K
[K], with probability at least 1 − C4K 2m−˜δ, where Ci, i = 1, 2, 3, 4 are universal constants. Note that on event
Bτ , we have

2mlmk
ml+mk

(1 − τ )nγ ≤ m ≤ (1 + τ )nγ,
2mlmk
ml + mk

2
l + m−1

m−1

=

k

≥ (1 − τ )nγ.

Thus on the event Bτ , we can choose an upper bound ∆(cid:48)2 :

∆(cid:48)2 :=

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

p/((1 − τ )γn)
log((1 + τ )γn)

(cid:33)

+ C3r(cid:48)
m

log((1 + τ )γn)

with

r(cid:48)
m =

(1 − β)2
(1 + ˜δ) log((1 + τ )γn)

(cid:32) (cid:112)p log((1 + τ )γn)
(1 − τ )γn

+

log((1 + τ )γn)
(1 − τ )γn

(cid:33)

.

Note that τ = (cid:112)6 log(n)/nγ = o(1) under the assumption log n
β and ˜δ that may also also depend on α, we have for large enough n, if K ≤ C1
some constant C1, C2 depending on α and ∆2 ≥ (1 + α) ¯∆2

n = o(γ). Fix an α > 0. By choosing small enough
log(γn)
log log(γn) , n ≥ C2n/ log(γn) for

γ, where

¯∆2

γ = 4σ2

(cid:114)

(cid:18)

1 +

1 +

(cid:19)

p
γn log n

log n,

then SDP achieves exact recovery with probability at least 1 − C3(log(γn))−C4, where C3, C4 depend only on α.
Thus we conclude that

P(Bc|Bτ ) ≤ C2(log(γn))−C3 .

Yubo Zhuang, Xiaohui Chen, Yun Yang

Step 3: lift solution from sketched SDP to all the data points.

Recall that the lift solution to all n data points is deﬁned as

ˆGk =

(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ ˆRk,

j∈ ˜Rk

where ¯Xk = | ˜Rk|−1 (cid:80)
is the revised centroid of the k-th cluster output from the subsampled SDP. And ˜Rk
is a randomly sampled subset of ˆRk with equal size m. Since our goal in this step is to bound P(Ac ∩ B|Bτ ), the
subsequent analysis will be on the event B, that is ˆRk = Rk for all k ∈ [K].
Then we have ¯Xk = (cid:80)

Xj/m and

j∈ ˜Rk⊆Rk

ˆGk =

(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ Rk.

kl =

(cid:110)
(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ

Let A(i)
we further make the analysis on T (cid:48) ⊆ T, let A(i)
of T (cid:48). Recall that Xi = µk + (cid:15)i, ∀i ∈ G∗
¯(cid:15)k = (cid:80)
(cid:15)i/m. For i ∈ G∗

(cid:111)

j∈Rk

, where i ∈ G∗
kl,t(cid:48) = A(i)

k\T, where ξ is some number to be determined. If
k is any realization
k, where (cid:15)i are i.i.d. N (0, σ2Ip). Denote similarly ¯Xk = µk + ¯(cid:15)k, where

kl ∩ {T (cid:48) = t(cid:48)}, where t(cid:48) = (cid:70)K

k=1 R(cid:48)

k\T , we note that (cid:15)i, ¯(cid:15)k, ¯(cid:15)l are independent. We can write

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2
=(cid:107)θ + (cid:15)i − ¯(cid:15)l(cid:107)2 − (cid:107)(cid:15)i − ¯(cid:15)k(cid:107)2
=(cid:107)θ(cid:107)2 + (cid:107)¯(cid:15)l(cid:107)2 − (cid:107)¯(cid:15)k(cid:107)2 − 2(cid:104)θ, ¯(cid:15)l(cid:105) + 2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105),

where θ = µk − µl. Set ζn = 2 log(Kn) and deﬁne

B(i)

kl,1 :=

(cid:110)
(cid:107)¯(cid:15)l(cid:107)2 ≥ m−1(p − 2(cid:112)pζn),
(cid:107)¯(cid:15)k(cid:107)2 ≤ m−1(p + 2(cid:112)pζn + 2ζn),
2m−1ζn(cid:107)θ(cid:107)

(cid:104)θ, ¯(cid:15)l(cid:105) ≤

(cid:112)

(cid:111)

and

B(i)

kl,2 :=

(cid:110)
(cid:107)¯(cid:15)l − ¯(cid:15)k(cid:107)2 ≤ 2m−1)(p + 2(cid:112)pζn + 2ζn),
(cid:104)θ, ¯(cid:15)k − ¯(cid:15)l(cid:105) ≤ 2(cid:112)(m−1)ζn(cid:107)θ(cid:107)

(cid:111)
.

kl,1 ∪ B(i)
kl = B(i)
Let B(i)
2000), we have P(B(i)c

kl,2. Using the standard tail probability bound for χ2 distribution (Laurent and Massart,
kl ) ≤ 5/(n2K 2). Since

we have on the event B(i)

kl that

(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105)|{¯(cid:15)l, ¯(cid:15)k} ∼ N (0, (cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2),

C∗ : = P

(cid:16)

2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105) ≤ −(1 − β)(cid:107)θ(cid:107)2(cid:12)
(cid:12)
(cid:12)¯(cid:15)k, ¯(cid:15)l

(cid:17)

(cid:33)

(cid:32)

(cid:32)

= 1 − Φ

≤ 1 − Φ

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)(cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2
(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

(cid:33)
,

n

where β ∈ (0, 1),

n = (cid:107)θ(cid:107)2 + 4
r(cid:48)(cid:48)

(cid:112)

m−1ζn(cid:107)θ(cid:107) + 2m−1(p + 2(cid:112)pζn + 2ζn).

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Note that (cid:107)θ(cid:107)2 ≥ ∆2 ≥ 8 log n, which implies 2(cid:112)m−1ζn ≤ (cid:107)θ(cid:107)(cid:112)2/m. Now we choose η > 0 such that 1+2η
1+η ≥
1 + 2(cid:112)2/m (i.e., m ≥ 8(1 + η−1)2). In order to have C∗ be bounded by n−(1+η), it is suﬃcient to require that

(cid:32)

1 − Φ

(cid:33)

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

n

≤ 1 − Φ((cid:112)2(1 + η) log n).

or further

(1 − β)2
8(1 + η) log n

(cid:107)θ(cid:107)4 − (1 + 2(cid:112)2/M )(cid:107)θ(cid:107)2 − 2(p + 2(cid:112)pζn + 2ζn)m−1 ≥ 0.

A suﬃcient condition for the last display is

∆2 ≥

4σ2(1 + 2η)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + 2η)

p
m log n

(cid:33)

+ 2r(cid:48)(cid:48)(cid:48)
n

log n,

r(cid:48)(cid:48)(cid:48)
n =

(1 − β)2
(1 + 2η) log n

(cid:16) (cid:112)p log(nK)
m

+

log(nK)
m

(cid:17)

.

ξ = β(cid:107)θ(cid:107)2 − 4

(cid:115)

log(nK)
m

(cid:107)θ(cid:107) − 4m−1(cid:112)2p log(nK) − 4

log(nK)
m

,

where

Now if we put

notice that

(cid:16)

P

A(i)c

kl

(cid:17)

=

(cid:88)

(cid:16)

P

A(i)
kl,t(cid:48)

(cid:17)

(cid:16)

· P

T (cid:48) = t(cid:48)(cid:17)

t(cid:48)∈T (cid:48)

≤ max
t(cid:48)∈T (cid:48)

(cid:16)

P

A(i)
kl,t(cid:48)

(cid:17)

,

where T (cid:48) is all the possible subset of T s.t. | ˜Ri| = m. Then we have

(cid:16)(cid:110)

P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ,

∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

(cid:16) n
(cid:91)

= P

(cid:91)

(cid:17)

A(i)c

kl

i=1

1≤k(cid:54)=l≤K

(cid:16)

P

A(i)c

kl

(cid:17)

(cid:16)

P

max
t(cid:48)∈T (cid:48)

A(i)

kl,t(cid:48) ∩ B(i)

kl

(cid:17)

(cid:16)

+ P

B(i)c

kl

(cid:17)

E[C∗1B(i)

kl,2

] +

5
n

≤

≤

≤

≤

n
(cid:88)

(cid:88)

i=1

1≤k(cid:54)=l≤K

n
(cid:88)

(cid:88)

i=1

1≤k(cid:54)=l≤K

n
(cid:88)

(cid:88)

1≤k(cid:54)=l≤K

i=1
K 2
nη +

7
n

.

Next we claim that ξ > 0. Recall that on the event Bτ , we have mk ∈ [(1 − τ )γnk, (1 + τ )γnk], 1/m ∈
1
(1+τ )γn ,
[

(1−τ )γn ]. Note that

1

(cid:107)θ(cid:107)2 ≥ ¯∆2

γ ≥ 4σ2

(cid:18)

1 +

(cid:114)

1 +

(cid:19)

p
γn log n

log n.

Yubo Zhuang, Xiaohui Chen, Yun Yang

So if γn → ∞ as n → ∞, n large, then we have

4(cid:112)log(nK)/m(cid:107)θ(cid:107) ≤

4m−1(cid:112)2p log(nK) ≤

β
5
β
5

(cid:107)θ(cid:107)2,

(cid:107)θ(cid:107)2,

4m−1 log(nK) ≤

β
5

(cid:107)θ(cid:107)2.

For α > 0, we can choose small enough β := β(α, σ) > 0 and η := η(α). Then for n large, we have if ∆2 ≥
(1 + α) ¯∆2

γ, then

P(A ∩ B|Bτ )
(cid:16)(cid:110)

=P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > 0, ∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

≤

K 2
nη .

Now, combining all pieces together, we conclude that, for all n large enough,

P( ˆG1 = G∗

1, . . . , ˆGK = G∗

K) ≥ 1 − C(log(γn))−c.

A.6 Proof outline for Theorem 4.4

The overall strategy for proving Theorem 4.4 for unequal cluster size case is identical to the proof of Theorem 4.1,
which can be divided into three steps. Again we will brieﬂy talk about the diﬀerence here.

In step 1, we reduce the exact recovery problem on the entire n data points to the subsampled m ≈ nγ data
points with K clusters with approximately equal size that resembles the problem structure of the original SDP
clustering problem. Here we use the weighted sampling ratio wi for each point i ∈ [n], where majority of them
should be around the true weights w∗
i , i ∈ [n]. We will apply the classical Chernoﬀ bound by setting appropriate
τ through the deﬁnition of ((cid:15), δ) pairs. i.e.,

τ (cid:16) (cid:112)log(n)/γn + (cid:15) + δn/n.

In step 2, we assume τ = o(1) by setting each summand of τ to be o(1). And we set the same conditions for
minimal separation ∆2 ≥ (1 + α) ¯∆2
In step 3, we use the nearest centroids ¯X1, . . . , ¯XK estimated from step 2 for propagating the solution from the
subsampled SDP to all data points that are not sampled in step 1. The discussion here is similar to the proof of
Theorem 4.1. The only diﬀerence here is that now we bound the size diﬀerence by τ through ((cid:15), δ) pairs. i.e.,

γ and K (cid:46) log(γn)/ log log(γn).

|mk − ml| < 2τ (cid:16) (cid:112)log(n)/γn + (cid:15) + δn/n,

where k (cid:54)= l ∈ [n].

A.7 Proof of Theorem 4.4

Step 1: reduction to subsampled data points. Let T ⊂ [n] be the subsampled data point indices so that
V = (Xi)i∈T ⊂ X and m = |T |. Let Rk = G∗
k ∩ T . Deﬁne the events
(cid:110) ˆG1 = G∗
1, . . . , ˆGK = G∗
(cid:110) ˆR1 = R1, . . . , ˆRK = RK
(cid:110)

B :=

A :=

(cid:111)
,

(cid:111)
,

K

Bτ :=

Bτ1 :=

(1 − τ )

nγ
K
(1 − τ1)m∗

(cid:110)

≤ |Rk| ≤ (1 + τ )

, ∀k ∈ [K]

nγ
K

k ≤ |Rk| ≤ (1 + τ1)m∗

k, ∀k ∈ [K]

(cid:111)
,

(cid:111)
,

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

where m∗

k = (cid:80)

i∈G∗
k

wi, τ, τ1 ∈ (0, 1). Observe that

P(Ac) =P(Ac ∩ (B ∩ Bτ )) + P(Ac ∩ (Bc ∪ Bc

τ ))

≤P(Ac ∩ B|Bτ )P(Bτ ) + P(Ac ∩ Bc) + P(Ac ∩ Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc ∩ Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Ac ∩ Bc|Bτ ) + 2P(Bc
τ )
≤P(Ac ∩ B|Bτ ) + P(Bc|Bτ ) + 2P(Bc

τ ).

Thus to bound the error probability for exact recovery, it suﬃces to bound the three terms on the right-hand
side of the last inequality. Since the subsampled data points V from X = (X1, . . . , Xn) are drawn with i.i.d.
Ber(γ), we apply the Chernoﬀ bound and the union bound to get

P(Bc
τ1

) ≤ 2

(cid:88)

(cid:16)

−

exp

k∈[K]

τ 2m∗
k
3

(cid:17)

.

Choosing τ1 = (cid:112)6 log(n)/m∗, we have

If we choose τ = τ1 + (cid:15) + δ(1 + (cid:15)) ¯w∗K/γ, then

P(Bc
τ1

) ≤ 2n−1.

P(Bc

τ ) ≤ P(Bc
τ1

) ≤ 2n−1.

Step 2: exact recovery for subsampled data. Since the subsampling procedure is independent of the
original Xi points, we can treat the Xi ∈ V as the new cluster problem to apply Lemma A.1 with T = (cid:83)K
k=1 Rk,
n = m and nk = mk, where mk = |Rk|. In particular, if there exist constants ˜δ > 0 and β ∈ (0, 1) such that

and

with

log m ≥

(1 − β)2
β2

C1m
M

,

˜δ ≤

β2
(1 − β)2

C2
K

, N ≥

4(1 + ˜δ)2
˜δ2

∆2 ≥

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

p
M log m

(cid:33)

+ C3rm

log m

rm =

(1 − β)2
(1 + ˜δ) log m

√

(cid:16)

p log m
m

+

log m
m

(cid:17)

,

, then the SDP achieves exact recovery, i.e., ˆRk = Rk, ∀k ∈
where m = mink∈[K] mk and M = min1≤k(cid:54)=l≤K
[K], with probability at least 1 − C4K 2m−˜δ, where Ci, i = 1, 2, 3, 4 are universal constants. Note that on event
Bτ , we have

2mlmk
ml+mk

(1 − τ )nγ ≤ m ≤ (1 + τ )nγ,
2mlmk
ml + mk

2
l + m−1

m−1

=

k

≥ (1 − τ )

nγ
K

.

Thus on the event Bτ , we can choose an upper bound ∆(cid:48)2 :

∆(cid:48)2 :=

4σ2(1 + 2˜δ)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + ˜δ)

pK/((1 − τ )γn)
log((1 + τ )γn)

(cid:33)

+ C3r(cid:48)
m

log((1 + τ )γn)

with

r(cid:48)
m =

(1 − β)2
(1 + ˜δ) log((1 + τ )γn)

(cid:32)

K(cid:112)p log((1 + τ )γn)
(1 − τ )γn

+

K log((1 + τ )γn)
(1 − τ )γn

(cid:33)

.

Yubo Zhuang, Xiaohui Chen, Yun Yang

Note that τ = τ1 + (cid:15) + δ(1 + (cid:15)) ¯w∗K/γ = o(1) under the assumption τ1 = o(1), (cid:15) = o(1), δ(1 + (cid:15)) ¯w∗K/γ = o(1). i.e.
n = o(γ), δ = o(n/n), (cid:15) = o(1).Fix an α > 0. By choosing small enough β and ˜δ that may also also depend
K log n
log log(γn) for some constant C1 depending on α and ∆2 ≥ (1 + α) ¯∆2
γ,

log(γn)

on α, we have for large enough n, if K ≤ C1
where

(cid:32)

(cid:115)

¯∆2

γ = 4σ2

1 +

1 +

(cid:33)

Kp
γn log n

log n,

then SDP achieves exact recovery with probability at least 1 − C2(log(γn))−C3, where C2, C3 depend only on α.
Thus we conclude that

P(Bc|Bτ ) ≤ C2(log(γn))−C3 .
Step 3: lift solution from sketched SDP to all the data points. Recall that the lift solution to all n data
points is deﬁned as

ˆGk =

(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ ˆRk,

where ¯Xk = (cid:80)
Xj/mk is the centroid of the k-th cluster output from the subsampled SDP. Since our goal
in this step is to bound P(Ac ∩ B|Bτ ), the subsequent analysis will be on the event B, that is ˆRk = Rk for all
k ∈ [K]. Then we have ¯Xk = (cid:80)

Xj/mk and

j∈ ˆRk

j∈Rk
(cid:110)
i ∈ [n] \ T : (cid:107)Xi − ¯Xk(cid:107) < (cid:107)Xi − ¯Xl(cid:107), ∀l (cid:54)= k

(cid:111)

∪ Rk.

ˆGk =

(cid:110)
(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ

kl =

Let A(i)
Recall that Xi = µk + (cid:15)i, ∀i ∈ G∗
¯(cid:15)k = (cid:80)

(cid:15)i/mk. For i ∈ G∗

(cid:111)

j∈Rk

, where i ∈ G∗

k\T, where ξ is some number to be determined.
k, where (cid:15)i are i.i.d. N (0, σ2Ip). Denote similarly ¯Xk = µk + ¯(cid:15)k, where

k\T , we note that (cid:15)i, ¯(cid:15)k, ¯(cid:15)l are independent. We can write

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2
=(cid:107)θ + (cid:15)i − ¯(cid:15)l(cid:107)2 − (cid:107)(cid:15)i − ¯(cid:15)k(cid:107)2
=(cid:107)θ(cid:107)2 + (cid:107)¯(cid:15)l(cid:107)2 − (cid:107)¯(cid:15)k(cid:107)2 − 2(cid:104)θ, ¯(cid:15)l(cid:105) + 2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105),

where θ = µk − µl. Set ζn = 2 log(Kn) and deﬁne

B(i)

kl,1 :=

(cid:110)

(cid:107)¯(cid:15)l(cid:107)2 ≥ m−1

l

(p − 2(cid:112)pζn),

(cid:107)¯(cid:15)k(cid:107)2 ≤ m−1

k (p + 2(cid:112)pζn + 2ζn),
(cid:113)
(cid:111)
2m−1

l ζn(cid:107)θ(cid:107)

(cid:104)θ, ¯(cid:15)l(cid:105) ≤

and

B(i)

kl,2 :=

(cid:110)

(cid:107)¯(cid:15)l − ¯(cid:15)k(cid:107)2 ≤ (m−1

l + m−1

k )(p + 2(cid:112)pζn + 2ζn),

(cid:104)θ, ¯(cid:15)k − ¯(cid:15)l(cid:105) ≤

(cid:113)

2(m−1

l + m−1

k )ζn(cid:107)θ(cid:107)

(cid:111)
.

kl,1 ∪ B(i)
kl = B(i)
Let B(i)
2000), we have P(B(i)c

kl,2. Using the standard tail probability bound for χ2 distribution (Laurent and Massart,
kl ) ≤ 5/(n2K 2). Since

we have on the event B(i)

kl that

(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105)|{¯(cid:15)l, ¯(cid:15)k} ∼ N (0, (cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2),

C∗ : = P

(cid:16)

2(cid:104)θ − ¯(cid:15)l + ¯(cid:15)k, (cid:15)i(cid:105) ≤ −(1 − β)(cid:107)θ(cid:107)2(cid:12)
(cid:12)
(cid:12)¯(cid:15)k, ¯(cid:15)l

(cid:17)

(cid:33)

(cid:32)

(cid:32)

= 1 − Φ

≤ 1 − Φ

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)(cid:107)θ − ¯(cid:15)l + ¯(cid:15)k(cid:107)2
(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

(cid:33)
,

n

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

where β ∈ (0, 1),

n = (cid:107)θ(cid:107)2 + 2
r(cid:48)(cid:48)

(cid:113)

2(m−1

l + m−1

k )ζn(cid:107)θ(cid:107) + (m−1

l + m−1

k )(p + 2(cid:112)pζn + 2ζn).

k )ζn ≤ (cid:107)θ(cid:107)(cid:112)2/M . Now we choose η > 0 such that
l + m−1
Note that (cid:107)θ(cid:107)2 ≥ ∆2 ≥ 8 log n, which implies
1+η ≥ 1 + 2(cid:112)2/M (i.e., M ≥ 8(1 + η−1)2). In order to have C∗ be bounded by n−(1+η), it is suﬃcient to require
1+2η
that

2(m−1

(cid:113)

(cid:32)

1 − Φ

(cid:33)

(1 − β)(cid:107)θ(cid:107)2
2(cid:112)r(cid:48)(cid:48)

n

≤ 1 − Φ((cid:112)2(1 + η) log n).

or further

(1 − β)2
8(1 + η) log n

(cid:107)θ(cid:107)4 − (1 + 2(cid:112)2/M )(cid:107)θ(cid:107)2 − (p + 2(cid:112)pζn + 2ζn)(m−1

l + m−1

k ) ≥ 0.

A suﬃcient condition for the last display is

∆2 ≥

4σ2(1 + 2η)
(1 − β)2

(cid:32)

(cid:115)

1 +

1 +

(1 − β)2
(1 + 2η)

p
M log n

(cid:33)

+ 2r(cid:48)(cid:48)(cid:48)
n

log n,

r(cid:48)(cid:48)(cid:48)
n =

(1 − β)2
(1 + 2η) log n

(cid:16) (cid:112)p log(nK)
m

+

log(nK)
m

(cid:17)

.

ξ =

mk − ml
mkml

(cid:115)

p + β(cid:107)θ(cid:107)2 − 4

log(nK)
ml

(cid:107)θ(cid:107) − 2

mk + ml
mkml

(cid:112)2p log(nK) − 4

log(nK)
mk

,

where

Now if we put

then we have

(cid:16)(cid:110)

P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > ξ,

∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

(cid:16) n
(cid:91)

= P

(cid:91)

(cid:17)

A(i)c

kl

i=1

1≤k(cid:54)=l≤K

(cid:16)

P

A(i)c
kl ∩ B(i)

kl

(cid:17)

(cid:16)

+ P

B(i)c

kl

(cid:17)

E[C∗1B(i)

kl,2

] +

5
n

≤

≤

≤

n
(cid:88)

(cid:88)

i=1

1≤k(cid:54)=l≤K

n
(cid:88)

(cid:88)

1≤k(cid:54)=l≤K

i=1
K 2
nη +

7
n

.

Next we claim that ξ > 0. Recall that on the event Bτ , we have mk ∈ [(1 − τ )m∗, (1 + τ )m∗], 1/M ∈
[

], where m∗ = nγ/K. Then,

,

1
(1+τ )m∗

1
(1−τ )m∗

(cid:12)
(cid:12)
(cid:12)

g

mk − ml
mkml

(cid:12)
(cid:12)
(cid:12) ≤

p

2τ
(1 − τ )2

p
m∗

≤

Note that m∗ ≥ m∗(1 − (cid:15)) − δn(1 + (cid:15)) ¯w∗,

√

6p
(1 − τ )2m∗

log n
√

m∗ +

2(cid:15)
(1 − τ )2

p
m∗

+

2(λ(1 + (cid:15)) ¯w∗K/γ)
(1 − τ )2

p
m∗

.

(cid:107)θ(cid:107)2 ≥ ¯∆2

γ ≥ 4σ2

(cid:18)

1 +

(cid:114)

1 +

(cid:19)

p
m∗ log n

log n.

Yubo Zhuang, Xiaohui Chen, Yun Yang

So if p = O(γn/K)2), (cid:15) = O( γn

Kp log n), δ = O((cid:112)γn/p · n/n) then
(cid:12)
(cid:12)
(cid:12)

mk − ml
mkml

(cid:12)
(cid:12)
(cid:12) ≤

β
5

p

(cid:107)θ(cid:107)2

for large enough n. Similarly, we have

4(cid:112)log(nK)/ml(cid:107)θ(cid:107) ≤
mk + ml
mkml

(cid:112)2p log(nK) ≤

2

β
5

(cid:107)θ(cid:107)2,

β
5

(cid:107)θ(cid:107)2,

4m−1

k log(nK) ≤

β
5

(cid:107)θ(cid:107)2.

For α > 0, we can choose small enough β := β(α, σ) > 0 and η := η(α). Then for n large, we have if ∆2 ≥
(1 + α) ¯∆2

γ, then

P(A ∩ B|Bτ )
(cid:16)(cid:110)

=P

(cid:107)Xi − ¯Xl(cid:107)2 − (cid:107)Xi − ¯Xk(cid:107)2 > 0, ∀i ∈ G∗

k\T, ∀1 ≤ k (cid:54)= l ≤ K

(cid:111)c(cid:17)

≤

K 2
nη .

Now, combining all pieces together, we conclude that, for all n large enough,

P( ˆG1 = G∗

1, . . . , ˆGK = G∗

K) ≥ 1 − C(log(γn))−c.

B FULL NUMERICAL EXPERIMENT RESULTS

In this section, we show the full simulation results. The error rates and running times on log-scale are shown in
Figures 3 to 18 (with captions describing the simulation setup). The baseline setup is λ∗ = 1.2, p = 1000, K =
4, γ = 0.1, n = 2000 by default, expect when γ is changing, we use n = 10000. For simplicity, we set all the
distance between centers to be equal. i.e., (cid:107)µk − µl(cid:107)2 = ∆2, ∀1 ≤ k (cid:54)= l ≤ K. In particular, we arrange all the
centers on vertices of a regular simplex. i.e., µl = ∆/
2·el, ∀l ∈ [K], where el is a vector that the l-th component
is 1 and 0 otherwise. The variance of Gaussian distributions is chosen to be 1. The detailed explanation for
methods M0 − M5 and O can be found in Section 5.

√

∗ in (4); the separation for Figures 7 to 9 is the theoretical cutoﬀ ¯∆2

Figures 3 to 9 are under the setting of equal cluster size. Figures 10 to 16 are for the unequal cluster size case.
In particular, Figures 3 to 6, Figures 10 to 13 are under the separation condition from the theoretical cutoﬀ
¯∆2
γ for SL methods; and the separation for
(cid:48)2
Figures 14 to 16 is the theoretical cutoﬀ ¯∆
γ for BCSL methods. Figures 3 to 6 correspond to Figure 1 and
Figure 2 in Section 5. By comparing Figures 3 to 6 (equal cluster size) to Figures 10 to 13 (unequal cluster
size), we can see that when p changes, M2 and M3 which are the improved methods aiming at handling large p
unequal cluster size settings have better performance than M1, which are consistent with our theory. Moreover,
M5, the multi-round WSL (at 4-th round), is optimal among those methods in Figure 10. For the time cost, we
can still see that SL methods have the same linear O(n) complexity as the fast K-means++ (with diﬀerence only
occurring in the leading constant) for unequal cluster size from Figure 11 corresponding to Figure 2 in Section 5.
We consider two threshold settings (Figures 7 to 9 and Figures 14 to 16) in order to show that the thresholds
(cid:48)2
¯∆2
γ for BCSL methods are nearly optimal in the sense that they are neither too large
that all methods can achieve exact recovery easily nor too small that we can observe the exact recoveries in most
cases, which can be found in those ﬁgures.

γ for SL methods and ¯∆

We would like to make a remark on the initial weights estimated by the K-means++ algorithm, which is the
default K-means clustering algorithm in Matlab. From Table 1, we see that as dimension p increases, the fraction
(1 − δ) of “good” weights within the (cid:15)-distortion level [(1 − (cid:15))w∗
i ] decreases. This is also reﬂected in
Figure 17, where the error rate of cluster label recovery is almost constant (N2 purple curve) when the separation
signal is slightly above the cutoﬀ value of exact recovery. With such a warm-start given by K-means++, our
WSL can improve the recovery error rate in one iteration (N3 black curve). Moreover, if we use the ideal weights,

i , (1 + (cid:15))w∗

Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

then the WSL can achieve the exact recovery (N1 red curve in Figure 17). The one-iteration WSL can be viewed
as the multi-round WSL with one round, we can see from Figure 18 that after 3-4 rounds, the multi-round
WSL reﬁnes the clustering errors and can eventually achieve the exact recovery as if we initialize with the ideal
weights. This can be explained by comparing Table 1 with Table 2 to 4, where we can see the decreasing trends
for δ, which will become 0 eventually for all (cid:15) we set on the grid. This shows that the weights for R4 we get
from iteration are fairly close to or identical to 0. On the other hand it can be well explained by Figure 19,
which is the corresponding averaged relative weight diﬀerence of R1 to R5 in Figure 18. Averaged relative weight
diﬀerence is deﬁned by 1
j is deﬁned in Section 3.3. Ri stands for the i-th
n
round of MR-WSL corresponding to Figure 18. By comparing Figure 18 and Figure 19 we can see similar trends
between the relative weight diﬀerence for Ri+1 and the error rate for Ri when various parameters change. The
relative weight diﬀerence has a decreasing trend as we perform more rounds, which accounts for the improvement
of MR-WSL shown in Figure 18.

j . where wj, w∗

j∈[n] |wj − w∗

j |/w∗

(cid:80)

Figure 3: Log-scale error rates and runtime (with error bars) v.s. p under the setting of equal cluster size and
∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 4: Log-scale error rates and runtime (with error bars) v.s. n under the setting of equal cluster size and
∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesM1M2M3M4M5M0100020003000400050006000p10-1100101Time cost (s)  Time cost when p changesM1M2M3M4M5M020004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesM1M2M3M4M5M0O20004000600080001000012000n10-1100101102Time cost (s)  Time cost when n changesM1M2M3M4M5M0OYubo Zhuang, Xiaohui Chen, Yun Yang

Figure 5: Log-scale error rates and runtime (with error bars) v.s. γ under the setting of equal cluster size and
∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 6: Log-scale error rates and runtime (with error bars) v.s. λ∗ under the setting of equal cluster size and
∆2 = (λ∗ ¯∆∗)2. Red vertical line indicates theoretical threshold ¯∆2
γ for SL methods. Zero error is displayed as
10−6 in the log-scale plot.

Figure 7: Log-scale error rates and runtime (with error bars) v.s. p under the setting of equal cluster size and
∆2 = (λ∗ ¯∆γ)2. Zero error is displayed as 10−6 in the log-scale plot.

0.020.040.060.080.10.120.140.160.180.210-610-510-410-310-210-1100Error rate  Error rate when  changesM1M2M3M4M5M00.020.040.060.080.10.120.140.160.180.210-1100101102Time cost (s)  Time cost when  changesM1M2M3M4M5M00.511.5*10-610-510-410-310-210-1100Error rate  Error rate when * changesM1M2M3M4M5M00.511.5*10-1100101Time cost (s)  Time cost when * changesM1M2M3M4M5M0100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesM1M2M3M4M5M0100020003000400050006000p10-1100101Time cost (s)  Time cost when p changesM1M2M3M4M5M0Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Figure 8: Log-scale error rates and runtime (with error bars) v.s. n under the setting of equal cluster size and
∆2 = (λ∗ ¯∆γ)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 9: Log-scale error rates and runtime (with error bars) v.s. γ under the setting of equal cluster size and
∆2 = (λ∗ ¯∆γ)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 10: Log-scale error rates and runtime (with error bars) v.s. p under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

20004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesM1M2M3M4M5M020004000600080001000012000n10-1100101Time cost (s)  Time cost when n changesM1M2M3M4M5M00.020.040.060.080.10.120.140.160.180.210-610-510-410-310-210-1100Error rate  Error rate when  changesM1M2M3M4M5M00.020.040.060.080.10.120.140.160.180.2100101102Time cost (s)  Time cost when  changesM1M2M3M4M5M0100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesM1M2M3M5M0100020003000400050006000p10-1100101Time cost (s)  Time cost when p changesM1M2M3M5M0Yubo Zhuang, Xiaohui Chen, Yun Yang

Figure 11: Log-scale error rates and runtime (with error bars) v.s. n under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 12: Log-scale error rates and runtime (with error bars) v.s. γ under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆∗)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 13: Log-scale error rates and runtime (with error bars) v.s. λ∗ under the setting of unequal cluster size
(cid:48)2
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆∗)2. Red vertical line indicates theoretical threshold ¯∆
γ for
SL methods. Zero error is displayed as 10−6 in the log-scale plot.

20004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesM1M2M3M5M0O20004000600080001000012000n10-1100101102Time cost (s)  Time cost when n changesM1M2M3M5M0O0.020.040.060.080.10.120.140.160.180.210-610-510-410-310-210-1100Error rate  Error rate when  changesM1M2M3M5M00.020.040.060.080.10.120.140.160.180.210-1100101102Time cost (s)  Time cost when  changesM1M2M3M5M00.511.5*10-610-510-410-310-210-1100Error rate  Error rate when * changesM1M2M3M5M00.511.5*10-1100101Time cost (s)  Time cost when * changesM1M2M3M5M0Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

Figure 14: Log-scale error rates and runtime (with error bars) v.s. p under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆(cid:48)

γ)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 15: Log-scale error rates and runtime (with error bars) v.s. n under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆(cid:48)

γ)2. Zero error is displayed as 10−6 in the log-scale plot.

Figure 16: Log-scale error rates and runtime (with error bars) v.s. γ under the setting of unequal cluster size
(n1 = n2 = n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆(cid:48)

γ)2. Zero error is displayed as 10−6 in the log-scale plot.

100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesM1M2M3M5M0100020003000400050006000p10-1100101Time cost (s)  Time cost when p changesM1M2M3M5M020004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesM1M2M3M5M020004000600080001000012000n10-1100101102Time cost (s)  Time cost when n changesM1M2M3M5M00.020.040.060.080.10.120.140.160.180.210-610-510-410-310-210-1100Error rate  Error rate when  changesM1M2M3M5M00.020.040.060.080.10.120.140.160.180.2100101102Time cost (s)  Time cost when  changesM1M2M3M5M0Yubo Zhuang, Xiaohui Chen, Yun Yang

Figure 17: Log-scale error rates (with error bars) v.s. p under the setting of unequal cluster size (n1 = n2 =
n/8, n3 = n4 = 3n/8) and ∆2 = (λ∗ ¯∆(cid:48)
γ)2. N1 is WSL when we plug in the true weights. N2 is K-means++
method. N3 is WSL when we plug in the weights based on K-means++ method. Zero error is displayed as 10−6
in the log-scale plot.

Figure 18: Error rates of MR-WSL. R0 stands for the initial K-means++ as the warm start for our MR-WSL
and Ri stands for the i-th round of MR-WSL. Zero error is displayed as 10−6 in the log-scale plot. In particular,
we take the log-scale for x axis when γ changes. The settings for Figure 18 are the same as Figure 17. From the
plots we can see that after 3-4 rounds, the performance gets stable and achieved optimal.

100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesN1N2N310-310-210-110-610-510-410-310-210-1100Error rate  Error rate when  changesR0R1R2R3R4R50.511.5*10-610-510-410-310-210-1100Error rate  Error rate when * changesR0R1R2R3R4R520004000600080001000012000n10-610-510-410-310-210-1100Error rate  Error rate when n changesR0R1R2R3R4R5100020003000400050006000p10-610-510-410-310-210-1100Error rate  Error rate when p changesR0R1R2R3R4R5Sketch-and-Lift: Scalable Subsampled Semideﬁnite Program for K-means Clustering

(cid:80)

j∈[n] |wj − w∗

Figure 19: Averaged relative weight diﬀerence of MR-WSL. Averaged relative weight diﬀerence is deﬁned by
1
j is deﬁned in Section 3.3. Ri stands for the i-th round of MR-WSL corre-
n
sponding to Figure 18. Zero error is displayed as 10−6 in the log-scale plot. In particular, we take the log-scale
for x axis when γ changes. Figure 18 and Figure 19 have same settings.

j . where wj, w∗

j |/w∗

Table 1: Fraction of ((cid:15), δ)-weights initialized by K-means++ algorithm (1-st round of WSL, R1 for Figure 18).

(cid:15)
p = 200
p = 500
p = 2000
p = 4000
p = 6000

0
0.2602
0.2725
0.44123
0.40125
0.48375

0.2
0.25373
0.25598
0.37208
0.33282
0.38929

0.4
0.21828
0.18602
0.27103
0.24355
0.25716

0.6
0.10665
0.10836
0.12647
0.10086
0.13754

0.8
0.071097
0.06278
0.085495
0.068535
0.07653

1.0
0.054791
0.049165
0.065855
0.05453
0.06671

We ﬁx a grid of (cid:15) and report the estimated values of δ satisfying (6) for each (cid:15). Each δ is averaged over 100
simulations. The setting for Table 1 is the same as Figure 18.

10-310-210-110-610-510-410-310-210-1100Averaged relative weight difference  Averaged relative weight difference when  changesR1R2R3R4R50.511.5*10-610-510-410-310-210-1100Averaged relative weight difference  Averaged relative weight difference when * changesR1R2R3R4R520004000600080001000012000n10-610-510-410-310-210-1100Averaged relative weight difference  Averaged relative weight difference when n changesR1R2R3R4R5100020003000400050006000p10-610-510-410-310-210-1100Averaged relative weight difference  Averaged relative weight difference when p changesR1R2R3R4R5Yubo Zhuang, Xiaohui Chen, Yun Yang

Table 2: Fraction of ((cid:15), δ)-weights for the 2-nd round of WSL (R2 for Figure 18).

(cid:15)
p = 200
p = 500
p = 2000
p = 4000
p = 6000

0
0.005
0.02625
0.13125
0.3725
0.54875

0.2
0.000005
0.00006
0.028845
0.088965
0.15886

0.4
0.000005
0.00006
0.01318
0.04746
0.095185

0.6
0.000005
0.00006
0.012105
0.02566
0.057355

0.8
0.000005
0.00006
0.012105
0.02266
0.044775

1.0
0.000005
0.00006
0.00967
0.0198
0.03841

We ﬁx a grid of (cid:15) and report the estimated values of δ satisfying (6) for each (cid:15). Each δ is averaged over 100
simulations. The setting for Table 2 is the same as Figure 18.

Table 3: Fraction of ((cid:15), δ)-weights for the 3-rd round of WSL (R3 for Figure 18).

(cid:15)
p = 200
p = 500
p = 2000
p = 4000
p = 6000

0
0
0
0
0.00875
0.02125

0.2
0
0
0
0.005
0.00005

0.4
0
0
0
0.00125
0.00005

0.6
0
0
0
0.00125
0.00005

0.8
0
0
0
0.00125
0.00005

1.0
0
0
0
0.00125
0.00005

We ﬁx a grid of (cid:15) and report the estimated values of δ satisfying (6) for each (cid:15). Each δ is averaged over 100
simulations. The setting for Table 3 is the same as Figure 18.

Table 4: Fraction of ((cid:15), δ)-weights for the 4-th round of WSL (R4 for Figure 18).

(cid:15)
p = 200
p = 500
p = 2000
p = 4000
p = 6000

0
0
0
0
0
0

0.2
0
0
0
0
0

0.4
0
0
0
0
0

0.6
0
0
0
0
0

0.8
0
0
0
0
0

1.0
0
0
0
0
0

We ﬁx a grid of (cid:15) and report the estimated values of δ satisfying (6) for each (cid:15). Each δ is averaged over 100
simulations. The setting for Table 4 is the same as Figure 18.

