9
1
0
2

l
u
J

0
2

]
L
M

.
t
a
t
s
[

1
v
0
8
8
8
0
.
7
0
9
1
:
v
i
X
r
a

Spectral Graph Matching and Regularized Quadratic Relaxations I:
The Gaussian Model

Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu∗

July 23, 2019

Abstract

Graph matching aims at ﬁnding the vertex correspondence between two unlabeled graphs
that maximizes the total edge weight correlation. This amounts to solving a computationally
intractable quadratic assignment problem. In this paper we propose a new spectral method,
GRAph Matching by Pairwise eigen-Alignments (GRAMPA). Departing from prior spectral
approaches that only compare top eigenvectors, or eigenvectors of the same order, GRAMPA
ﬁrst constructs a similarity matrix as a weighted sum of outer products between all pairs of
eigenvectors of the two graphs, with weights given by a Cauchy kernel applied to the separation
of the corresponding eigenvalues, then outputs a matching by a simple rounding procedure. The
similarity matrix can also be interpreted as the solution to a regularized quadratic programming
relaxation of the quadratic assignment problem.

For the Gaussian Wigner model in which two complete graphs on n vertices have Gaussian
edge weights with correlation coeﬃcient 1 − σ2, we show that GRAMPA exactly recovers the
correct vertex correspondence with high probability when σ = O( 1
log n ). This matches the
state of the art of polynomial-time algorithms, and signiﬁcantly improves over existing spectral
methods which require σ to be polynomially small in n. The superiority of GRAMPA is also
demonstrated on a variety of synthetic and real datasets, in terms of both statistical accuracy
and computational eﬃciency. Universality results, including similar guarantees for dense and
sparse Erd˝os-R´enyi graphs, are deferred to the companion paper [FMWX19].

Contents

1 Introduction

2
3
1.1 Random weighted graph matching . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2 A new spectral method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3 Connections to regularized quadratic programming . . . . . . . . . . . . . . . . . . .
1.4 Diagonal dominance of the similarity matrix . . . . . . . . . . . . . . . . . . . . . . .
8
1.5 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

∗Z. Fan, C. Mao, and Y. Wu are with Department of Statistics and Data Science, Yale University, New Haven,
USA, {zhou.fan,cheng.mao,yihong.wu}@yale.edu. J. Xu is with The Fuqua School of Business, Duke University,
Durham, USA, jx77@duke.edu. Y. Wu is supported in part by the NSF Grants CCF-1527105, CCF-1900507, an NSF
CAREER award CCF-1651588, and an Alfred Sloan fellowship. J. Xu is supported by the NSF Grants IIS-1838124,
CCF-1850743, and CCF-1856424.

1

 
 
 
 
 
 
2 Main results

10
2.1 Gaussian Wigner model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.2 Proof outline for Theorem 2.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
2.3 Gaussian bipartite model

3 Proofs

14
3.1 Analysis in the noiseless setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.1.1 Properties of A and rotation by U . . . . . . . . . . . . . . . . . . . . . . . . 15
3.1.2 Proof of Lemma 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.1.3 Proof of Lemma 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.1.4 Proof of Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2 Bounding the eﬀect of the noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2.1 Vectorization and rotation by U . . . . . . . . . . . . . . . . . . . . . . . . . 23
3.2.2 Proof of Lemma 3.9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
3.2.3 Proof of Lemma 3.10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
1 by spectral method . . . . . . . . . . . . . . . . . . . . . . . . 30
. . . . . . . . . . . . . . . . . . . . . . . 32
2 by linear assignment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.3 Proof for the bipartite model
3.3.1 Recovery of π∗
3.3.2 Recovery of π∗
3.4 Gradient descent dynamics

4 Numerical experiments

34
4.1 Universality of GRAMPA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.2 Comparison of spectral methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
. . . . . . . . . . . . . 37
4.3 Comparison with quadratic programming and Degree Proﬁle
4.4 More graph ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
4.5 Networks of autonomous systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5 Conclusion

A Concentration inequalities for Gaussians

B Kronecker gymnastics

C Signal-to-noise heuristics

1

Introduction

41

42

43

44

Given a pair of graphs, the problem of graph matching or network alignment refers to ﬁnding a
bijection between the vertex sets so that the edge sets are maximally aligned [CFSV04, LR13,
ESDS16]. This is a ubiquitous problem arising in a variety of applications, including network
de-anonymization [NS08, NS09], pattern recognition [CFSV04, SS05], and computational biology
[SXB08, KHGM16]. Finding the best matching between two graphs with adjacency matrices A
and B may be formalized as the following combinatorial optimization problem over the set of

2

permutations Sn on {1, . . . , n}:

max
π∈Sn

n
(cid:88)

i,j=1

AijBπ(i),π(j).

(1)

This is an instance of the notoriously diﬃcult quadratic assignment problem (QAP) [PRW94,
BCPP98], which is NP-hard to solve or to approximate within a growing factor [MMS10].

As the worst-case computational hardness of the QAP (1) may not be representative of typical
graphs, various average-case models have been studied. For example, when the two graphs are
isomorphic, the resulting graph isomorphism problem can be solved for Erd˝os-R´enyi random graphs
in linear time whenever information-theoretically possible [Bol82, CP08], but remains open to be
solved in polynomial time for arbitrary graphs. In “noisy” settings where the graphs are not exactly
isomorphic, there is a recent surge of interest in computer science, information theory, and statistics
for studying random graph matching [YG13, LFP14, KHG15, FQRM+16, CK16, SGE17, CK17,
DCKG18, BCL+18, CKMP18, DMWX18, MX19].

1.1 Random weighted graph matching

In this work, we study the following random weighted graph matching problem: Consider two
weighted graphs with n vertices, and a latent permutation π∗ on {1, . . . , n} such that vertex i of
the ﬁrst graph corresponds to vertex π∗(i) of the second. Denoting by A and B their (symmetric)
weighted adjacency matrices, suppose that {(Aij, Bπ∗(i),π∗(j)) : 1 ≤ i < j ≤ n} are independent
pairs of positively correlated random variables. We wish to recover π∗ from A and B.

Notable special cases of this model include the following:

• Erd˝os-R´enyi graph model: (Aij, Bπ∗(i),π∗(j)) is a pair of correlated Bernoulli random variables.
Then A and B are Erd˝os-R´enyi graphs with correlated edges. This model has been extensively
studied in [PG11, LFP14, KL14, CK16, LS18, BCL+18, DCKG18, CKMP18, DMWX18].

• Gaussian Wigner model: (Aij, Bπ∗(i),π∗(j)) is a pair of correlated Gaussian variables, for ex-
ample, Bπ∗(i),π∗(j) = Aij +σZij where σ ≥ 0 and Aij, Zij are independent standard Gaussians.
Then A and B are complete graphs with correlated Gaussian edge weights. This model was
proposed in [DMWX18, Section 2] as a prototype of random graph matching due to its sim-
plicity, and certain results in the Gaussian model may be expected to carry over to dense
Erd˝os-R´enyi graphs.

Spectral methods have a long history in testing graph isomorphism [BGM82] and the graph
matching problem [Ume88]. In this paper, we introduce a new spectral method for graph matching,
for which we prove the following exact recovery guarantee in the Gaussian setting.

Theorem (Informal statement). Under the Gaussian Wigner model, if σ ≤ c/ log n for a suﬃ-
ciently small constant c > 0, then a spectral algorithm recovers π∗ exactly with high probability.

We describe the method in Section 1.2 below. The algorithm may also be interpreted as a
regularized convex relaxation of the QAP program (1), and we discuss this connection in Section
1.3. The performance guarantee matches the state of the art of polynomial-time algorithms, namely,
the degree proﬁle method proposed in [DMWX18], and exponentially improves the performance of

3

existing spectral matching algorithms, which require σ = 1
proposal.

poly(n) as opposed to σ = O( 1

log n ) for our

In a companion paper [FMWX19], we apply resolvent-based universality techniques to establish
similar guarantees for our spectral method on non-Gaussian models, including dense and sparse
Erd˝os-R´enyi graphs. Our proofs here in the Gaussian model are more direct and transparent, using
instead the rotational invariance of A and B and yielding slightly stronger guarantees.

A variant of our method may also be applied to match two bipartite random graphs, which we
discuss in Section 2.3. This is an extension of the analysis in the non-bipartite setting, which is our
primary focus.

1.2 A new spectral method

Write the spectral decompositions of the weighted adjacency matrices A and B as

A =

n
(cid:88)

i=1

λiuiu(cid:62)
i

and B =

n
(cid:88)

j=1

µjvjv(cid:62)
j

where the eigenvalues are ordered1 such that

λ1 ≥ · · · ≥ λn

and

µ1 ≥ · · · ≥ µn.

Algorithm 1 GRAMPA (GRAph Matching by Pairwise eigen-Alignments)

1: Input: Weighted adjacency matrices A and B on n vertices, a tuning parameter η > 0.
2: Output: A permutation (cid:98)π ∈ Sn.
3: Construct the similarity matrix

(cid:98)X =

n
(cid:88)

i,j=1

w(λi, µj) · uiu(cid:62)

i Jvjv(cid:62)

j ∈ Rn×n

where J ∈ Rn×n denotes the all-one matrix and w is the Cauchy kernel of bandwidth η:

w(x, y) =

1
(x − y)2 + η2 .

(2)

(3)

(4)

4: Output the permutation estimate (cid:98)π by “rounding” (cid:98)X to a permutation, for example, by solving

the linear assignment problem (LAP)

(cid:98)π = argmax

π∈Sn

n
(cid:88)

i=1

(cid:98)Xi,π(i).

(5)

Our new spectral method is given in Algorithm 1, which we refer to as graph matching by
pairwise eigen-alignments (GRAMPA). Therein, the linear assignment problem (5) may be cast as
a linear program (LP) over doubly stochastic matrices, i.e., the Birkhoﬀ polytope (see (13) below),

1This is in fact not needed for computing the similarity matrix (3).

4

or solved eﬃciently using the Hungarian algorithm [Kuh55]. We advocate this rounding approach in
practice, although our theoretical results apply equally to the simpler rounding procedure matching
each i to

(cid:98)π(i) = argmax

j

(cid:98)Xij.

(6)

We discuss the choice of the tuning parameter η further in Section 4, and ﬁnd in practice that the
performance is not too sensitive to this choice.

Let us remark that Algorithm 1 exhibits the following two elementary but desirable properties.

• Unlike previous proposals, our spectral method is insensitive to the choices of signs for indi-
vidual eigenvectors ui and vj in (2). More generally, it does not depend on the speciﬁc choice
of eigenvectors if certain eigenvalues have multiplicity greater than one. This is because the
similarity matrix (3) depends on the eigenvectors of A and B only through the projections
onto their distinct eigenspaces.

• Let (cid:98)π(A, B) denote the output of Algorithm 1 with inputs A and B. For any ﬁxed permutation
ij = Bπ(i),π(j), and by π ◦ (cid:98)π the composition

π, denote by Bπ the matrix with entries Bπ
(π ◦ (cid:98)π)(i) = π((cid:98)π(i)). Then we have the equivariance property

π ◦ (cid:98)π(A, Bπ) = (cid:98)π(A, B)
and similarly for Aπ. That is, the outputs given (A, Bπ) and given (A, B) represent the same
matching of the underlying graphs. This may be veriﬁed from (3) as a consequence of the
identity J = JΠ = ΠJ for any permutation matrix Π.

(7)

To further motivate the construction (3), we note that Algorithm 1 follows the same general
paradigm as several existing spectral methods for graph matching, which seek to recover π∗ by
rounding a similarity matrix (cid:98)X constructed to leverage correlations between eigenvectors of A and
B. These methods include:

• Low-rank methods that use a small number of eigenvectors of A and B. The simplest such

approach uses only the leading eigenvectors, taking as the similarity matrix

(cid:98)X = u1v(cid:62)
1 .

(8)

Then (cid:98)π which solves (5) orders the entries of v1 in the order of u1. Other rank-1 spectral
methods and low-rank generalizations have also been proposed and studied in [FQRM+16,
KG16].

• Full-rank methods that use all eigenvectors of A and B. A notable example is the popular

method of Umeyama [Ume88], which sets

(cid:98)X =

n
(cid:88)

i=1

siuiv(cid:62)
i

(9)

where si are signs in {−1, 1}; see also the related approach of [XK01]. The motivation is
that (9) is the solution to the orthogonal relaxation of the QAP (1), where the feasible set is

5

relaxed to the set the orthogonal matrices [FBR87]. As the correct choice of signs in (9) may
be diﬃcult to determine in practice, [Ume88] suggests also an alternative construction

(cid:98)X =

n
(cid:88)

i=1

|ui||vi|(cid:62)

(10)

where |ui| denotes the entrywise absolute value of ui.

Compared with these constructions, the proposed new spectral method has two important

features that we elaborate below:

“All pairs matter.” Departing from existing approaches, our proposal (cid:98)X in (3) uses a combi-
nation of uiv(cid:62)
for all n2 pairs i, j ∈ {1, . . . , n}, rather than only i = j. This renders our method
j
signiﬁcantly more resilient to noise. Indeed, while all of the above methods can succeed in recov-
ering π∗ in the noiseless case, methods based only on pairs (ui, vi) with i = j are brittle to noise
if ui and vi quickly decorrelate as the amount of noise increases—this may happen when λi is
not separated from other eigenvalues by a large spectral gap. When this decorrelation occurs, ui
becomes partially correlated with vj for neighboring indices j, and the construction (3) leverages
these partial correlations in a weighted manner to provide a more robust estimate of π∗.

The eigenvector alignment is quantitatively understood in certain regimes for the Gaussian
Wigner model B = A + σZ when A, Z are GOE or GUE: It is known that E[(cid:104)ui, vi(cid:105)2] = o(1) for the
leading eigenvector i = 1 as soon as σ2 (cid:29) n−1/3 [Cha14, Theorem 3.8], and for i in the bulk of the
Wigner semicircle spectrum as soon as σ2 (cid:29) n−1 [BY17, Ben17]. For i in the bulk, and the noise
regime n−1+ε (cid:28) σ2 (cid:28) n−ε, [Ben17, Theorem 1.3] further implies that (cid:104)ui, vj(cid:105) is approximately
Gaussian for each index j suﬃciently close to i, with zero mean and variance

E[(cid:104)ui, vj(cid:105)2] ≈

σ2/n
(λi − µj)2 + Cσ4 .

(11)

Here, the value of C (cid:16) 1 depends on the Wigner semicircle density near λi ≈ µj. Thus, for this
range of noise, the eigenvector ui of A is most aligned with O(nσ2) eigenvectors vj of B for which
|λi − µj| (cid:46) σ2, and each such alignment is of typical size E[(cid:104)ui, vj(cid:105)2] (cid:16) 1/(nσ2) (cid:28) 1. The signal
for π∗ in our proposal (3) arises from a weighted average of these alignments. As a result, while
poly(n) ,2 our new spectral
existing spectral approaches are only robust up to a noise level σ =
method is polynomially more robust and can tolerate σ = O( 1

1

log n ).

Cauchy spectral weights. The performance of the spectral method depends crucially on the
choice of the weight function w in (3). In fact, there are other methods that are also of the form
(3) but do not work equally well. For example, if we choose w(λ, µ) = λµ, then (3) simply reduces
to (cid:98)X = AJB = ab(cid:62), where a = A1 and b = B1 are the vectors of “degrees”. Rounding such a
similarity matrix is equivalent to matching by sorting the degree of the vertices, which is known to
fail when σ = Ω(n−1) due to the small spacing of the order statistics (cf. [DMWX18, Remark 1]).
The Cauchy spectral weight (4) is a particular instance of the more general form w(λ, µ) =
), where K is a monotonically decreasing kernel function and η is a bandwidth parameter.

K( |λ−µ|

η

2For the rank-one method (8) based on the top eigenvector pairs, a necessary condition for rounding to succeed is
that the two top eigenvectors are perfectly aligned, i.e., E[(cid:104)u1, v1(cid:105)2] = 1 − o(1). Thus (8) certainly fails for σ (cid:29) n−1/3.
For the Umeyama method (10), experiment shows that it fails when σ (cid:29) n−1/4; cf. Fig. 3(b) in Section 4.2.

6

Such a choice upweights the eigenvector pairs whose eigenvalues are close and signiﬁcantly penalizes
those whose eigenvalues are separated more than η. The speciﬁc choice of the Cauchy kernel matches
the form of E[(cid:104)ui, vj(cid:105)2] in (11), and is in a sense optimal as explained by a heuristic signal-to-noise
calculation in Appendix C. In addition, the Cauchy kernel has its genesis as a regularization term
in the associated convex relaxation, which we explain next.

1.3 Connections to regularized quadratic programming

Our new spectral method is also rooted in optimization, as the similarity matrix (cid:98)X in (3) cor-
responds to the solution to a convex relaxation of the QAP (1), regularized by an added ridge
penalty.

Denote the set of permutation matrices in Rn×n by Sn. Then (1) may be written in matrix

notation as one of the three equivalent optimization problems

max
Π∈Sn

(cid:104)A, ΠBΠ(cid:62)(cid:105) ⇐⇒ min
Π∈Sn

(cid:107)A − ΠBΠ(cid:62)(cid:107)2

F ⇐⇒ min
Π∈Sn

(cid:107)AΠ − ΠB(cid:107)2
F .

(12)

Note that the third objective (cid:107)AΠ − ΠB(cid:107)2
permutations to its convex hull (the Birkhoﬀ polytope of doubly stochastic matrices)

F above is a convex function in Π. Relaxing the set of

Bn (cid:44) {X ∈ Rn×n : X1 = 1, X (cid:62)1 = 1, Xij ≥ 0 for all i, j},

we arrive at the quadratic programming (QP) relaxation

min
X∈Bn

(cid:107)AX − XB(cid:107)2
F ,

(13)

(14)

which was proposed in [ZBV08, ABK15], following an earlier LP relaxation using the (cid:96)1-objective
proposed in [AD93]. Although this QP relaxation has achieved empirical success [ABK15, VCL+15,
LFF+16, DML17], understanding its performance theoretically is a challenging task yet to be
accomplished.

Our spectral method can be viewed as the solution of a regularized further relaxation of the
Indeed, we show in Corollary 2.2 that the matrix (cid:98)X in (3) is the

doubly stochastic QP (14).
minimizer of

min
X∈Rn×n

1
2

(cid:107)AX − XB(cid:107)2

F +

η2
2

(cid:107)X(cid:107)2

F − 1(cid:62)X1.

Equivalently, (cid:98)X is a positive scalar multiple of the solution (cid:101)X to

min
X∈Rn×n

(cid:107)AX − XB(cid:107)2

F + η2(cid:107)X(cid:107)2
F

s.t. 1(cid:62)X1 = n

(15)

(16)

which further relaxes (14) and adds a ridge penalty term η2(cid:107)X(cid:107)2
F . Note that (cid:98)X and (cid:101)X are equivalent
as far as the rounding step (5) or (6) is concerned. In contrast to (14), for which there is currently
limited theoretical understanding, we are able to provide an exact recovery analysis for the rounded
solutions to (15) and (16).

Note that the constraint in (16) is a signiﬁcant relaxation of the double stochasticity (14).
To make this further relaxed program work, the regularization term plays a key role. If η were

7

zero, the similarity matrix (cid:98)X in (3) would involve the eigengap |λi − µj| in the denominator which
can be polynomially small in n. Hence the regularization is crucial for reducing the variance of
the estimate and making (cid:98)X stable, a rationale reminiscent of the ridge regularization in high-
dimensional linear regression. In a companion paper [FMWX19], we analyze a tighter relaxation
than (16) which replaces 1(cid:62)X1 = n by the row-sum constraint X1 = 1, and there the ridge penalty
is still indispensable for achieving exact recovery up to noise level σ = O(1/ polylog(n)).

Viewing (cid:98)X as the minimizer of (15) provides not only an optimization perspective, but also an
associated gradient descent algorithm to compute (cid:98)X. More precisely, starting from the initial point
X (0) = 0 and ﬁxing a step size γ > 0, a straightforward computation veriﬁes that gradient descent
for optimizing (15) is given by the dynamics

X (t+1) = X (t) − γ(cid:0)A2X (t) + X (t)B2 − 2AX (t)B + η2X (t) − J(cid:1).

(17)

Corollary 2.2 below shows that running gradient descent for t = O(cid:0)(log n)3(cid:1) iterations suﬃces to
produce a similarity matrix, which, upon rounding, exactly recovers π∗ with high probability, using
X (t) in place of (cid:98)X in (5). Each iteration involves several matrix multiplication operations with
A and B, which may be more eﬃcient and parallelizable than performing spectral decompositions
when the graphs are large and/or sparse.

1.4 Diagonal dominance of the similarity matrix

Equipped with this optimization point of view, we now explain the typical structure of solutions
to the above quadratic programs including the spectral similarity matrix (3).
It is well known
that even the solution to the most stringent relaxation (14) is not the latent permutation matrix,
which has been shown in [LFF+16] by proving that the KKT conditions cannot be fulﬁlled with
high probability. In fact, a heuristic calculation explains why the solution to (14) is far from any
permutation matrix: Let us consider the “population version” of (14), where the objective function
is replaced by its expectation over the random instances A and B. Consider π∗ = id and the
Gaussian Wigner model B = A + σZ, where A and Z are independent GOE matrices with N (0, 1
n )
oﬀ-diagonal entries and N (0, 2
n ) diagonal entries. Then the expectation of the objective function is
F − 2E(cid:104)AX, XA(cid:105)
2
Tr(X)2 −
n

F + E(cid:107)XB(cid:107)2
n + 1
(cid:107)X(cid:107)2
n

F = E(cid:107)AX(cid:107)2
= (2 + σ2)

E(cid:107)AX − XB(cid:107)2

(cid:104)X, X (cid:62)(cid:105).

F −

2
n

Hence the population version of the quadratic program (14) is

min
X∈Bn

(2 + σ2)(n + 1)(cid:107)X(cid:107)2

F − 2 Tr(X)2 − 2(cid:104)X, X (cid:62)(cid:105),

whose solution3 is

X (cid:44) (cid:15)I + (1 − (cid:15))F,

(cid:15) =

2
2 + (n + 1)σ2 ≈

2
nσ2 .

(18)

(19)

This is a convex combination of the true permutation matrix and the center of the Birkhoﬀ polytope
F = 1
n J. Therefore, the population solution X is in fact a very “ﬂat” matrix, with each entry on
the order of 1
n , and is close to the center of the Birkhoﬀ polytope and far from any of its vertices.

3In fact, (19) is the solution to (18) even if the constraint is relaxed to 1(cid:62)X1 = n.

8

This calculation nevertheless provides us with important structural information about the so-
lution to such a QP relaxation: X is diagonally dominant for small σ, with diagonals about 2/σ2
times the oﬀ-diagonals. Although the actual solution of the relaxed program (14) or (15) is not
equal to the population solution X in expectation, it is reasonable to expect that it inherits the
diagonal dominance property in the sense that (cid:98)Xi,π∗(i) > (cid:98)Xij for all j (cid:54)= π∗(i), which enables
rounding procedures such as (5) to succeed.

With this intuition in mind, let us revisit the regularized quadratic program (15) whose solution
is the spectral similarity matrix (3). By a similar calculation, the solution to the population
version of (15) is given by αI + βJ, with α =
(η2+σ2)(η2+σ2+2) and
η2+σ2+2 , which is diagonally dominant for small σ and η. In turn, the basis
of our theoretical guarantee is to establish the diagonal dominance of the actual solution (cid:98)X; see
Fig. 1 for an empirical illustration.

2n2
(n(η2+σ2)+σ2)(n(η2+σ2+2)+σ2) ≈

n
n(η2+σ2+2)+σ2 ≈

β =

1

2

Although the ridge penalty η2(cid:107)X(cid:107)2

F guarantees the stability of the solution as discussed in
Section 1.3, it may seem counterintuitive since it moves the solution closer to the center of the
Birkhoﬀ polytope and further away from the vertices (permutation matrices). In fact, several works
in the literature [FJBd13, DML17] advocate adding a negative ridge penalty, in order to make the
solution closer to a permutation at the price of potentially making the optimization non-convex.
This consideration, however, is not necessary, as the ensuing rounding step can automatically map
the solution to the correct permutation, even if they are far away in the Euclidean distance.

(a) Histogram of diagonal (blue) and oﬀ-diagonal
(yellow with a normal ﬁt) entries of (cid:98)X.

(b) Heat map of (cid:98)X.

Figure 1: Diagonal dominance of the similarity matrix (cid:98)X deﬁned by (3) or (15) for the Gaussian
Wigner model B = A + σZ with n = 200, σ = 0.05 and η = 0.01.

It is worth noting that, in contrast to the prevalent analysis of convex relaxations in statistics
and machine learning (where the goal is to show that the solution to the relaxed program is close
to the ground truth in a certain distance) or optimization (where the goal is to bound the gap of
the objective value to the optimum), here our goal is not to show that the optimal solution per se
constitutes a good estimator, but to show that it exhibits a diagonal dominance structure, which
guarantees the success of the subsequent rounding procedure. For this reason, it is unclear from

9

-4000-200002000400060008000100001200001002003004005006007008005010015020020406080100120140160180200-4000-2000020004000600080001000012000ﬁrst principles that the guarantees obtained for one program, such as (16), automatically carry
over to a tighter program, such as (14). In the companion paper [FMWX19], we analyze a tighter
relaxation than (16), where the constraint is tightened to X1 = 1, and show that this has a similar
performance guarantee; however, this requires a diﬀerent analysis.

1.5 Notation

n=1 and {bn}∞

Let [n] (cid:44) {1, . . . , n}. We use C and c to denote universal constants that may change from line to
n=1 of real numbers, we write an (cid:46) bn if there is a universal
line. For two sequences {an}∞
constant C such that an ≤ Cbn for all n ≥ 1. The relation an (cid:38) bn is deﬁned analogously. We
write an (cid:16) bn if both the relations an (cid:46) bn and an (cid:38) bn hold. Let x ∨ y = max(x, y). Let id denote
the identity permutation, i.e. id(i) = i for every i. In a Euclidean space Rn or Cn, let ei be the i-th
standard basis vector, 1 = 1n the all-ones vector, J = Jn the n × n all-ones matrix, and I = In the
n×n identity matrix. We omit the subscripts when there is no ambiguity. Let (cid:107)·(cid:107) = (cid:107)·(cid:107)2 denote the
Euclidean vector norm on Rn or Cn. Let (cid:107)M (cid:107) = maxv(cid:54)=0 (cid:107)M v(cid:107)2/(cid:107)v(cid:107)2 denote the Euclidean operator
norm, (cid:107)M (cid:107)F = (Tr M ∗M )1/2 the Frobenius norm, and (cid:107)M (cid:107)∞ = maxij |Mij| the elementwise (cid:96)∞
norm of a matrix M . An eigenvector is always a unit column vector by convention. Denote by

(d)
= Y if random variables X and Y are equal in law.

X

2 Main results

In this section, we formulate the models, state more precisely our main results, provide an outline
of the proof, and discuss the extension to bipartite graphs.

2.1 Gaussian Wigner model

We say that A ∈ Rn×n is from the Gaussian Orthogonal Ensemble, or simply A ∼ GOE(n), if A is
symmetric, {Aij : i ≤ j} are independent, and Aij ∼ N (0, 1
n ) for i = j. We
say that the pair A, B ∈ Rn×n follows the Gaussian Wigner model for graph matching if
Bπ∗

n ) for i (cid:54)= j and N (0, 2

= A + σZ,

(20)

where π∗ is an unknown permutation (ground truth), Bπ∗ denotes the permuted matrix Bπ∗
ij =
Bπ∗(i),π∗(j), the matrices A, Z ∼ GOE(n) are independent, and σ ≥ 0 is the noise level. Our goal is
to recover the latent permutation π∗ from (A, B).

We may also consider the rescaled deﬁnition Bπ∗ =

1 − σ2A + σZ, so that A and B have the
same marginal law with correlation coeﬃcient 1 − σ2. Our proofs are easily adapted to this setup,
and we assume (20) for simplicity and a cleaner presentation.

√

We now formalize the exact recovery guarantee for Algorithm 1.

Theorem 2.1. Consider the model (20). There exist constants c, c(cid:48) > 0 such that if

then with probability at least 1 − n−4 for all large n, the matrix (cid:98)X in (3) satisﬁes

1/n0.1 ≤ η ≤ c/ log n

and

σ ≤ c(cid:48)η,

min
i∈[n]

(cid:98)Xi,π∗(i) >

max
i,j∈[n]: j(cid:54)=π∗(i)

(cid:98)Xij

(21)

and hence, Algorithm 1 recovers (cid:98)π = π∗.

10

Choosing η (cid:16) 1/ log n in Algorithm 1, we thus obtain exact recovery for σ (cid:46) 1/ log n. The
same exact recovery guarantee clearly also holds if rounding were performed by the simple scheme
(6), instead of solving the linear assignment (5). The probability n−4 is arbitrary and may be
strengthened to n−D for any constant D > 0, where c, c(cid:48) above depend on D.

Consider next the gradient descent iterates X (t) deﬁned by (17). We verify that these iterates

converge to (cid:98)X, and that the same guarantee holds for X (t) for suﬃciently large t.
Corollary 2.2. Let X (0) = 0, deﬁne recursively X (t) by the gradient descent dynamics (17), and
let (cid:98)X be the similarity matrix (3).

(a) The matrix (cid:98)X is the minimizer of the unconstrained program (15), and α (cid:98)X is the minimizer

of the constrained program (16) for some (random) scalar multiplier α > 0.

(b) In terms of the spectral decompositions of A and B in (2), each iterate X (t) is given by

X (t) =

n
(cid:88)

i,j=1

1 − [1 − γη2 − γ(λi − µj)2]t
η2 + (λi − µj)2

uiu(cid:62)

i Jvjv(cid:62)
j .

In particular, if the step size satisﬁes γ < 1/(η2 + (λi − µj)2) for all i, j, then X (t) → (cid:98)X as
t → ∞.

(c) In the setting of Theorem 2.1, for some constants C, c > 0, if γ < c and

t >

C log n
γη2

,

then the guarantees of Theorem 2.1 also hold with probability at least 1 − n−4 with X (t) in
place of (cid:98)X.

In particular, setting γ to be a small constant and η (cid:16) 1/ log n, we obtain the same diagonal
dominance property for X (t) as long as t (cid:38) (log n)3, and consequently either the rounding scheme
(5) or (6) applied to X (t) recovers the true matching π∗.

2.2 Proof outline for Theorem 2.1

We give an outline of the proof for Theorem 2.1. By the permutation invariance property (7) of
the algorithm, we may assume without loss of generality that π∗ = id, the identity permutation.
Then we must show in (21) that every diagonal entry of (cid:98)X is larger than every oﬀ-diagonal entry.

Denote the similarity matrix in (3) by

X = (cid:98)X(A, B) =

n
(cid:88)

i,j=1

1

(λi − µj)2 + η2 uiu(cid:62)

i Jvjv(cid:62)
j

(22)

and introduce X∗ as the similarity matrix constructed in the noiseless setting with A in place of
B. That is,

X∗ = (cid:98)X(A, A) =

1

(λi − λj)2 + η2 uiu(cid:62)

i Juju(cid:62)
j .

(23)

n
(cid:88)

i,j=1

We divide the proof into establishing the diagonal dominance of X∗, and then bounding the entry-
wise diﬀerence X − X∗.

11

Lemma 2.3. For some constants C, c > 0, if 1/n0.1 < η < c/ log n, then with probability at least
1 − 5n−5 for large n,

and

(X∗)ii > 1/(3η2)

min
i∈[n]

max
i,j∈[n]:i(cid:54)=j

(X∗)ij < C

(cid:18) √

log n
η3/2

+

(cid:19)

.

log n
η

(24)

Lemma 2.4. If η > 1/n0.1, then for a constant C > 0, with probability at least 1 − 2n−5 for large
n,

max
i,j∈[n]

|Xij − (X∗)ij| < Cσ

η3 +

(cid:18) 1

log n
η2

(cid:18)

1 +

σ
η

(cid:19)(cid:19)

.

(25)

Proof of Theorem 2.1. Assuming these lemmas, for some c, c(cid:48) > 0 suﬃciently small, setting η <
c/ log n and σ < c(cid:48)η ensures that the right sides of both (24) and (25) are at most 1/(12η2). Then
when π∗ = id, these lemmas combine to imply

min
i∈[n]

Xii >

1
4η2 >

1
6η2 > max

i,j∈[n]:i(cid:54)=j

Xij

with probability at least 1 − n−4. On this event, by deﬁnition, both the LAP rounding procedure
(5) and the simple greedy rounding (6) output (cid:98)π = id. The result for general π∗ follows from the
equivariance of the algorithm, applying this result to the inputs A and Bπ with π = (π∗)−1.

A large portion of the technical work will lie in establishing Lemma 2.3 for the noiseless setting.
We give here a short sketch of the intuition for this lemma, ignoring momentarily any factors that
are logarithmic in n and are hidden by the notations ≈ and (cid:47) below. Let us write

X∗ =

n
(cid:88)

i=1

1
η2 (u(cid:62)

i Jui)uiu(cid:62)

i +

(cid:88)

i(cid:54)=j

1
η2 + (λi − λj)2 (u(cid:62)

i Juj)uiu(cid:62)
j .

(26)

We explain why the ﬁrst term is diagonally dominant, while the second term is a perturbation
of smaller order. Central to our proof is the fact that A ∼ GOE(n) is rotationally invariant in
law, so that U = (u1, . . . , un) is uniformly distributed on the orthogonal group and independent of
λ1, . . . , λn. The coordinates of U are approximately independent with distribution N (0, 1

For the ﬁrst term in (26), with high probability u(cid:62)

i Jui = (cid:104)ui, 1(cid:105)2 ≈ 1 for every i. Then for each

n ).

k, the kth diagonal entry of the ﬁrst term satisﬁes

n
(cid:88)

i=1

1
η2 (u(cid:62)

i Jui)(ui)2

k ≈

n
(cid:88)

i=1

1
η2 (ui)2

k ≈

1
η2 .

Applying the heuristic (ui)k ∼ N (0, 1

n ), each (k, (cid:96))th oﬀ-diagonal entry satisﬁes

n
(cid:88)

i=1

1
η2 (u(cid:62)

i Jui)(ui)k(ui)(cid:96) (cid:47) 1
√
η2

.

n

For the second term in (26), each (k, (cid:96))th entry is

(cid:88)

i(cid:54)=j

1
η2 + (λi − λj)2 (u(cid:62)

i Juj)(ui)k(uj)(cid:96) = g(cid:62)Qh,

12

(27)

(28)

η2+(λi−λj )2 (u(cid:62)
where Q is deﬁned by Qii = 0 and Qij =
i Juj) for i (cid:54)= j, and the vectors g and h
are deﬁned by gi = (ui)k and hj = (uj)(cid:96). Applying the heuristic that g, h are approximately iid
N (0, 1

n I) and approximately independent of Q, we have a Hanson-Wright type bound
g(cid:62)Qh (cid:47) 1
n

(cid:107)Q(cid:107)F .

1

As n → ∞, the empirical spectral distribution n−1 (cid:80)n
law with density ρ. Then, applying also u(cid:62)

i Juj = (cid:104)ui, 1(cid:105)(cid:104)uj, 1(cid:105) (cid:47) 1, we obtain

i=1 δλi of A converges to the Wigner semicircle

1
n2 (cid:107)Q(cid:107)2

F

(cid:47) 1
n2

(cid:88)

(cid:16)

i(cid:54)=j

1
η2 + (λi − λj)2

(cid:17)2

≈

(cid:90) (cid:90) (cid:16)

1
η2 + (x − y)2

(cid:17)2

ρ(x)ρ(y)dxdy (cid:47) 1
η3 ,

where the last step is an elementary computation that holds for any bounded density ρ with bounded
support. As a result, each entry of the second term of (26) satisﬁes

(cid:88)

i(cid:54)=j

1
η2 + (λi − λj)2 (u(cid:62)

i Juj)(ui)k(uj)(cid:96) (cid:47) 1
η3/2

.

(29)

Combining (27)–(29) shows that the noiseless solution X∗ in (26) is indeed diagonally dominant,
with diagonals approximately η−2 and oﬀ-diagonals at most of the order η−3/2, omitting logarithmic
factors. We carry out this proof more rigorously in Section 3.1 to establish Lemma 2.3.

2.3 Gaussian bipartite model

Consider the following asymmetric variant of this problem: Let F, G ∈ Rn×m be adjacency ma-
trices of two weighted bipartite graphs on n left vertices and m right vertices, where m ≥ n is
assumed without loss of generality. Suppose, for latent permutations π∗
2 on [m], that
2 (j)) : 1 ≤ i ≤ n, 1 ≤ j ≤ m} are i.i.d. pairs of correlated random variables. We wish
{(Fij, Gπ∗
1 (i),π∗
1, π∗
to recover (π∗

2) from F and G.

1 on [n] and π∗

We propose to apply Algorithm 1 on the left singular values and singular vectors of F and G to
1, and then solve a second LAP to recover the (bigger)

ﬁrst recover the (smaller) row permutation π∗
column permutation π∗

2. This is summarized as follows:

Algorithm 2 Bi-GRAMPA (Bipartite GRAph Matching by Pairwise eigen-Alignments)
1: Input: F, G ∈ Rn×m, a tuning parameter η > 0.
2: Output: Row permutation (cid:98)π1 ∈ Sn and column permutation (cid:98)π2 ∈ Sm.
3: Construct the similarity matrix (cid:98)X as in (3), where now λ1 ≥ . . . ≥ λn and µ1 ≥ . . . ≥ µn are
the singular values of F and G, and ui and vj are the corresponding left singular vectors.
(cid:98)π1(i),j.

4: Let (cid:98)π1 be the estimate in (5), and denote by G(cid:98)π1,id ∈ Rn×m the matrix G(cid:98)π1,id
5: Find (cid:98)π2 by solving the linear assignment problem

ij = G

(F (cid:62)G(cid:98)π1,id)j,π(j).

(30)

(cid:98)π2 = argmax
π∈Sm

m
(cid:88)

j=1

13

We also establish an exact recovery guarantee for this method in a Gaussian setting: We say

that the pair F, G ∈ Rn×m follows the Gaussian bipartite model for graph matching if

Gπ∗

1 ,π∗

2 = F + σW,

(31)

1 ,π∗

where Gπ∗
dependent with i.i.d. N (0, 1
regime

2 denotes the permuted matrix Gπ∗

1 ,π∗
2
ij

2 (j), the matrices F and W are in-
m ) entries, and σ ≥ 0 is the noise level. We assume the asymptotic

= Gπ∗

1 (i),π∗

m = m(n)

and

n/m → κ ∈ (0, 1]

as

n → ∞.

(32)

Theorem 2.5. Consider the model (31), where n/m → κ ∈ (0, 1]. There exist κ-dependent
constants c, c(cid:48) > 0 such that if

1/n0.1 ≤ η ≤ c/ log n

and

σ log(1/σ) ≤ c(cid:48)η/ log n,

then with probability at least 1 − n−4 for all large n, Algorithm 2 recovers ((cid:98)π1, (cid:98)π2) = (π∗
Setting η (cid:16) 1/ log n, we obtain exact recovery under the condition σ (cid:46) (log n)−2(log log n)−1.
√

The proof is an extension of that of Theorem 2.1: Note that the ﬁrst step of Algorithm 2 is
GG(cid:62),
equivalent to applying Algorithm 1 on the symmetric polar parts A =
· denotes the symmetric matrix square root. In the Gaussian setting, A is still rotationally
where
invariant, and Lemma 2.3 will extend directly to X∗ constructed from this A. We will show a simpler
and slightly weaker version of Lemma 2.4 to establish exact recovery of the left permutation π∗
1,
under the stronger requirement for σ above. We will then analyze separately the linear assignment
for recovering π∗

2. Details of the argument are provided in Section 3.3.

F F (cid:62) and B =

1, π∗

2).

√

√

We conclude this section by discussing the assumption (32). The condition n → ∞ is information-
theoretically necessary to recover the right permutation π∗
2, unless σ is as small as 1/poly(n).
This can be seen by considering the oracle setting when π∗
1 is given, in which case the neces-
sary and suﬃcient condition for the maximal likelihood (linear assignment) to succeed is given
by n log (cid:0)1 + 1
(cid:1) − 4 log m → ∞ [DCK19]. The condition of ﬁnite aspect ratio n = Θ(m) is as-
σ2
sumed for the analysis of the Bi-GRAMPA algorithm; otherwise, if n = o(m), then the empirical
distribution of singular values of F converges to a point mass at 1, and it is unclear whether the
spectral similarity matrix in (22) or (23) continues to be diagonally dominant. We note that such
a condition is not information-theoretically necessary. In fact, as long as n and m are polynomially
related, running the degree proﬁle matching algorithm [DMWX18, Section 2] on the row-wise and
column-wise empirical distributions succeeds for σ = O( 1

log n ).

3 Proofs

We prove our main results in this section. Section 3.1 proves Lemma 2.3, which shows the diagonal
dominance of X∗ in the noiseless setting of A = B. Section 3.2 proves Lemma 2.4, which bounds
the diﬀerence X − X∗. Together with the argument in Section 2.2, these yield Theorem 2.1 on the
exact recovery in the Gaussian Wigner model.

Section 3.3 extends this analysis to establish Theorem 2.5 for the bipartite model. Finally,
Section 3.4 (which may be read independently) proves Corollary 2.2 relating (cid:98)X to the gradient
descent dynamics (17) and the optimization problems (15) and (16).

14

3.1 Analysis in the noiseless setting

We ﬁrst prove Lemma 2.3, showing diagonal dominance in the noiseless setting. Throughout, we
write the spectral decomposition

A = U ΛU (cid:62) where U = [u1 · · · un] and Λ = diag(λ1, . . . , λn).

(33)

3.1.1 Properties of A and rotation by U

In the proof, we will in fact only use the properties of the matrix A ∼ GOE(n) recorded in the
following proposition. The same proof will then apply to the bipartite case wherein the suitably
deﬁned A satisﬁes the same properties.

Proposition 3.1. Suppose A ∼ GOE(n). Then for constants C, c > 0,

(a) U is a uniform random orthogonal matrix independent of Λ.

(b) The empirical spectral distribution ρn = 1
n

i=1 δλi of A converges to a limiting law ρ, which
has a density function bounded by C and support contained in [−C, C]. Moreover, for all
large n,

(cid:80)n

(cid:110)

P

sup
x

|Fn(x) − F (x)| > Cn−1(log n)5(cid:111)

< n−c log log n,

where Fn and F are the cumulative distribution functions of ρn and ρ respectively.

(c) For all large n, P(cid:8)(cid:107)A(cid:107) > C} < e−cn.

Proof. Parts (a) and (c) are well-known, see for example [AGZ10, Corollary 2.5.4 and Lemma 2.6.7].
For (b), ρ is the Wigner semicircle law on [−2, 2], and the rate of convergence follows from [GT13,
Theorem 1.1].

Recall the deﬁnition

n
(cid:88)

1

η2 + (λi − λj)2 uiu(cid:62)
Our goal is to exhibit the diagonal dominance of this matrix. Without loss of generality, we analyze
(X∗)11 = e(cid:62)

1 X∗e1 and (X∗)12 = e(cid:62)

i Juju(cid:62)
j .

1 X∗e2.

X∗ =

i,j=1

Applying Proposition 3.1 (a) above, let us rotate by U to write the quantities of interest in a

more convenient form. Namely, we set

ϕ = U (cid:62)e1, ψ = U (cid:62)e2,

and ξ = U (cid:62)1.

(34)

These vectors satisfy

(cid:107)ϕ(cid:107)2 = (cid:107)ψ(cid:107)2 = 1,

(cid:107)ξ(cid:107)2 =

√

n,

(cid:104)ϕ, ξ(cid:105) = 1,

(cid:104)ψ, ξ(cid:105) = 1,

and (cid:104)ϕ, ψ(cid:105) = 0,

(35)

and are otherwise “uniformly random”. By this, we mean that (ϕ, ψ, ξ) is equal in law to (Oϕ, Oψ, Oξ)
for any orthogonal matrix O ∈ Rn×n, which follows from Proposition 3.1(a).

Deﬁne a symmetric matrix L ∈ Rn×n by

Lij =

1
η2 + (λi − λj)2 ,

15

(36)

and deﬁne (cid:101)L ∈ Rn×n such that (cid:101)Lii = 0 and (cid:101)Lij = Lij for i (cid:54)= j. Then

and

(X∗)12 =

n
(cid:88)

i,j=1

Lijϕiψjξiξj,

(X∗)11 =

1
η2

n
(cid:88)

i=1

i ξ2
ϕ2

i +

n
(cid:88)

i,j=1

(cid:101)Lijϕiϕjξiξj.

(37)

(38)

Importantly, by Proposition 3.1(a), the triple (ϕ, ψ, ξ) is independent of L and (cid:101)L. We will establish
the following technical lemmas.

Lemma 3.2. With probability at least 1 − 3n−7 for large n,

n
(cid:88)

i=1

i ξ2
ϕ2

i >

1
2

.

Lemma 3.3. For some constants C, c > 0, if 1/n0.1 < η < c, then with probability at least 1 − 2n−7
for large n,

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

Lijϕiψjξiξj

(cid:12)
(cid:12)
(cid:12) ∨

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

(cid:101)Lijϕiϕjξiξj

(cid:12)
(cid:12)
(cid:12) < C

(cid:18) √

log n
η3/2

+

(cid:19)

.

log n
η

(39)

Lemma 2.3 follows immediately from these results.

Indeed, for η < c/ log n and suﬃciently
small c > 0, these results and the forms (37–38) combine to yield (X∗)11 > 1/(3η2) and (X∗)12 <
log n/η3/2 + (log n)/η) with probability at least 1 − 5n−7. By symmetry, the same result holds
C(
for all (X∗)ii and (X∗)ij, and Lemma 2.3 follows from taking a union bound over i and j.

√

It remains to show Lemmas 3.2 and 3.3. The general strategy is to approximate the law of
(ϕ, ψ, ξ) by suitably deﬁned Gaussian random vectors, and then to apply Gaussian tail bounds
and concentration inequalities which are collected in Appendix A. As an intermediate step, we
will show the following estimates for the matrix L, using the convergence of the empirical spectral
distribution in Proposition 3.1(b).

Lemma 3.4. For constants C, c > 0, with probability at least 1 − n−10 for large n,

min
i,j∈[n]

Lij ≥ c, max
i,j∈[n]

Lij ≤

1
η2 ,

1
n

(cid:107)L(cid:107)F ≤

C
η3/2

,

1
n

max
i∈[n]

n
(cid:88)

j=1

L2

ij ≤

C
η3

and

1
n

max
i∈[n]

n
(cid:88)

j=1

Lij ≤

C
η

.

3.1.2 Proof of Lemma 3.2

Let z be a standard Gaussian vector in Rn independent of ϕ. First, we note that marginally ϕ is
equal to z/(cid:107)z(cid:107)2 in law. By standard bounds on maxj |zj| and (cid:107)z(cid:107)2 (see Lemmas A.1 and A.3), we
have that with probability at least 1 − 2n−7,

(cid:12)
(cid:12)ϕi

(cid:12)
(cid:12) ≤ 5

max
i∈[n]

(cid:18) log n
n

(cid:19)1/2

.

16

(40)

Next, the random vectors ϕ and ξ satisfy that (cid:107)ϕ(cid:107)2 = 1, (cid:107)ξ(cid:107)2 =

otherwise uniformly random. Hence if we let z be a standard Gaussian vector in Rn and deﬁne

√

n and (cid:104)ϕ, ξ(cid:105) = 1, and are

√

n − 1

(cid:101)ξ (cid:44)

z − (ϕ(cid:62)z)ϕ
(cid:13)z − (ϕ(cid:62)z)ϕ(cid:13)
(cid:13)
(cid:13)2

+ ϕ,

(d)
= (ϕ, (cid:101)ξ). Note that we can write (cid:101)ξ = αz + βϕ, where α and β = 1 − α(ϕ(cid:62)z) are
√
log n with probability at least 1 − 4n−8 by

then (ϕ, ξ)
random variables satisfying 0.9 ≤ α ≤ 1.1 and |β| ≤ 4
concentration of (cid:107)z(cid:107)2 and ϕ(cid:62)z (Lemmas A.1 and A.3). Therefore, we obtain

n
(cid:88)

i=1

i (cid:101)ξ2
ϕ2

i =

n
(cid:88)

i=1

ϕ2
i

(cid:0)α2z2

i + β2ϕ2

i + 2αβziϕi

(cid:1) ≥ 0.8

n
(cid:88)

i=1

i z2
ϕ2

i − 9(cid:112)log n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

ϕ3

i zi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(41)

For the ﬁrst term of (41), applying Lemma A.3 and then (40) yields

n
(cid:88)

i=1

i z2
ϕ2

i ≥ 1 − 22(log n)

(cid:33)1/2

(cid:32) n
(cid:88)

i=1

ϕ4
i

≥ 1 − 22(log n)

(cid:32) n
(cid:88)

i=1

54

(cid:18) log n
n

(cid:19)2(cid:33)1/2

≥ 0.9

with probability at least 1 − 3n−7. For the second term of (41), we once again apply Lemma A.1
and then (40) to obtain

9(cid:112)log n

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

ϕ3

i zi

(cid:12)
(cid:12)
(cid:12) ≤ 20 log n

(cid:33) 1
2

(cid:32) n
(cid:88)

i=1

ϕ6
i

≤ 0.1

with probability at least 1 − 3n−7. Combining the three terms ﬁnishes the proof.

3.1.3 Proof of Lemma 3.4

Let ρn be the empirical spectral distribution of A. For a large enough constant C1 > 0 where
[−C1, C1] contains the support of ρ, let E be the event where it also contains the support of ρn,
and

|Fn(x) − F (x)| < n−0.5.

sup
x

(42)

By Proposition 3.1, E holds with probability at least 1 − n−10.

The bound Lij ≤ 1/η2 holds by the deﬁnition (36). The bound n−1(cid:107)L(cid:107)F ≤ Cη−3/2 follows from
ij ≤ Cη−3 also over i and taking a square root. The bound c ≤ Lij also holds

summing n−1 (cid:80)
on E by the deﬁnition of L. It remains to prove the last two bounds on the rows of L.

j L2

For this, ﬁx a = 1 or a = 2. For each λ ∈ [−C1, C1], deﬁne a function

gλ(r) (cid:44)

(cid:18)

1
η2 + (r − λ)2

(cid:19)a

.

Then for each i,

1
n

n
(cid:88)

j=1

La

ij =

1
n

n
(cid:88)

j=1

gλi(λj) =

(cid:90) C1

−C1

gλi(r)dρn(r).

(43)

17

For some constants C, C(cid:48) > 0 and every λ ∈ [−C1, C1], replacing ρn by the limiting density ρ, we
have

(cid:90) C1

−C1

gλ(r)dρ(r) ≤ C

≤ C

(cid:90) C1

(cid:18)

−C1
(cid:32)(cid:90)

1
η2 + (r − λ)2
(cid:90)

1
η2a dr +

(cid:19)a

dr

|r−λ|≤η

η≤|r−λ|≤2C1

(cid:33)

1
(r − λ)2a dr

≤ C(cid:48)η1−2a.

(44)

To bound the diﬀerence between ρn and ρ, note that gλ(r) ≥ y for y ≥ 0 if and only if |r −λ| ≤ b
for some b = b(y) ≥ 0. Consider random variables Rn ∼ ρn and R ∼ ρ. Since gλ ≤ η−2a, we have

(cid:90) C1

gλ(r)dρn(r) −

(cid:12)
(cid:12)
(cid:12)

(cid:90) C1

−C1

gλ(r)dρ(r)

(cid:12)
(cid:12)
(cid:12)

(cid:90) η−2a

−C1
(cid:12)
(cid:12)
(cid:12)

0
(cid:90) η−2a

0

(cid:90) η−2a

0

=

≤

≤

(cid:0)P(cid:8)gλ(Rn) ≥ y(cid:9) − P(cid:8)gλ(R) ≥ y(cid:9)(cid:1) dy

(cid:12)
(cid:12)
(cid:12)

P(cid:8)|Rn − λ| ≤ b(y)(cid:9) − P(cid:8)|R − λ| ≤ b(y)(cid:9)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)dy
(cid:12)

2n−0.5dy = 2η−2an−0.5,

where the last inequality holds on the event E by (42). Combining the last display with (44), we
get that (43) is at most Cη1−2a for η > n−0.1. This gives the desired bounds for a = 1 and a = 2.

3.1.4 Proof of Lemma 3.3

We now use Lemma 3.4 to prove Lemma 3.3. Recall L deﬁned in (36), and (cid:101)L which sets its diagonal
to 0. We need to bound the quantities

(I) :

n
(cid:88)

i,j=1

Lijϕiψjξiξj

and

(II) :

n
(cid:88)

i,j=1

(cid:101)Lijϕiϕjξiξj.

(45)

The proof for (II) is almost the same as that for (I), so we focus on (I) and brieﬂy discuss the
diﬀerences for (II). Let us deﬁne a matrix K ∈ Rn×n by setting

Kij = Lijϕiψj.

(46)

Estimates for K. We translate the estimates for L in Lemma 3.4 to ones for K. Note that since
1√
ϕ and ψ are independent of L and uniform over the sphere with entries on the order of
n , it is
n (cid:107)L(cid:107)F and (cid:107)K(cid:107) (cid:46) 1
reasonable to expect that (cid:107)K(cid:107)F (cid:46) 1
n (cid:107)L(cid:107) with high probability; however, neither
statement is true in general, as shown by the counterexamples L = e1e(cid:62)
1 and L = I. Fortunately,
both statements hold for L deﬁned by (36) thanks to the structural properties established in Lemma
3.4.

Lemma 3.5. In the setting of Lemma 3.3, for the matrix K ∈ Rn×n deﬁned by (46), we have
(cid:107)K(cid:107)F (cid:46) 1

η3/2 with probability at least 1 − 2n−8.

18

Proof. It suﬃces to prove that conditional on L, with probability at least 1 − n−8, we have

This together with Lemma 3.4 yields that

(cid:107)K(cid:107)F (cid:46) 1
n

(cid:107)L(cid:107)F +

log n
n1/4

(cid:107)L(cid:107)∞.

(cid:107)K(cid:107)F (cid:46) 1
η3/2

+

log n
n1/4η2

(cid:46) 1
η3/2

,

where the last inequality holds since we choose η (cid:38) n−0.1.

Note that we have

(cid:107)K(cid:107)2

F =

n
(cid:88)

i,j=1

i ψ2
ϕ2

j L2

ij ≤

1
2

n
(cid:88)

i,j=1

i L2
ϕ4

ij +

1
2

n
(cid:88)

i,j=1

j L2
ψ4
ij.

It suﬃces to bound the ﬁrst term, as the second term has the same distribution. Let z be a standard
Gaussian vector in Rn. Then z/(cid:107)z(cid:107)2
it remains to prove that with probability at least 1 − n−10,

(d)
= ϕ. By the concentration of (cid:107)z(cid:107)2 around

n (Lemma A.3),

√

n
(cid:88)

i,j=1

i L2
z4

ij =

n
(cid:88)

i=1

i αi (cid:46) (cid:107)L(cid:107)2
z4

F + (cid:107)L(cid:107)2

∞n3/2(log n)2

where αi (cid:44) (cid:80)n

j=1 L2
ij.
To this end, we compute

(cid:34) n
(cid:88)

E

i=1

(cid:35)

z4
i αi

= 3(cid:107)L(cid:107)2
F

and moreover

Var

(cid:32) n
(cid:88)

i=1

(cid:33)

z4
i αi

=

n
(cid:88)

i=1

Var(z4

i )α2

i = 105

n
(cid:88)

i=1

α2
i

(cid:46) n3(cid:107)L(cid:107)4

∞.

Therefore, applying Theorem A.4 with d = 4 we obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

i α2
z4

i − 3(cid:107)L(cid:107)2
F

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46) (cid:107)L(cid:107)2

∞n3/2(log n)2

with probability at least 1 − n−10, which completes the proof.

Lemma 3.6. It holds with probability at least 1 − n−8 that for all j, k ∈ [n],

n
(cid:88)

i=1

i LijLik (cid:46) 1
ϕ2
n

n
(cid:88)

i=1

LijLik

and

n
(cid:88)

i=1

i Lij (cid:46) 1
ψ2
η

.

Proof. Since z/(cid:107)z(cid:107)2 has the same distribution as ϕ or ψ. By the concentration of (cid:107)z(cid:107)2 around
(Lemma A.3) and a union bound, it remains to prove that with probability at least 1 − n−10,

√

n

n
(cid:88)

i=1

z2
i LijLik (cid:46)

n
(cid:88)

i=1

LijLik

and

n
(cid:88)

i=1

i Lij (cid:46) n
z2
η

.

(47)

19

For the ﬁrst inequality, Lemma A.3 gives that with probability at least 1 − n−11,

n
(cid:88)

i=1

z2
i LijLik (cid:46)

n
(cid:88)

i=1

LijLik +

(cid:32) n
(cid:88)

i=1

ijL2
L2

ik log n

(cid:33)1/2

(cid:18)

+

max
i∈[n]

(cid:19)

LijLik

log n.

Note that 1 (cid:46) Lij ≤ 1/η2 by Lemma 3.4, so

n
(cid:88)

i=1

LijLik (cid:38) n

and

(cid:32) n
(cid:88)

i=1

ijL2
L2

ik log n

(cid:33)1/2

(cid:18)

+

max
i∈[n]

LijLik

√

(cid:19)

log n (cid:46)

n log n
η4

+

log n
η4 .

Therefore, if η (cid:38) n−0.1, then (cid:80)n
established.

i=1 LijLik is the dominating term. Hence the ﬁrst bound in (47) is

The same argument also works to yield

n
(cid:88)

i=1

z2
i Lij (cid:46)

n
(cid:88)

i=1

Lij.

Combining this with Lemma 3.4, we obtain the second bound in (47).

Lemma 3.7. For the matrix K ∈ Rn×n deﬁned by (46), we have (cid:107)K(cid:107) (cid:46) 1/η with probability at
least 1 − 2n−8.

Proof. Consider the event where the estimates of Lemmas 3.4 and 3.6 hold. Fix a unit vector
x ∈ Rn. We have

n
(cid:88)

(cid:107)Kx(cid:107)2

2 =





n
(cid:88)

i=1

j=1


2

ϕiψjLijxj



=

n
(cid:88)

(cid:32) n
(cid:88)

j,k=1

i=1

(cid:33)

ϕ2

i LijLik

ψjψkxjxk.

The ﬁrst bound in Lemma 3.6 then yields that

(cid:107)Kx(cid:107)2
2

(cid:46) 1
n

=

1
n

n
(cid:88)

(cid:32) n
(cid:88)

j,k=1

i=1

(cid:33)

LijLik

|ψjψkxjxk|

n
(cid:88)





n
(cid:88)

i=1

j=1



2

|ψj|Lij|xj|



=

1
n

(cid:13)M |x|(cid:13)
(cid:13)
2
2 ≤
(cid:13)

1
n

(cid:107)M (cid:107)2,

(48)

where |x| denotes the vector whose i-th entry is |xi|, and the matrix M is deﬁned by

Mij = |ψj|Lij.

Moreover, we have that

(cid:107)M (cid:62)x(cid:107)2

2 =



ψ2
i



n
(cid:88)

j=1

n
(cid:88)

i=1


2

Lijxj



≤

n
(cid:88)

i=1

ψ2
i





Lij





n
(cid:88)

j=1

Lijx2
j


 (cid:46) n
η

n
(cid:88)

(cid:32) n
(cid:88)

j=1

i=1

(cid:33)

ψ2

i Lij

x2
j ,





n
(cid:88)

j=1

20

where the ﬁrst inequality follows from the Cauchy-Schwarz inequality, and the second inequality
follows from the row sum bound in Lemma 3.4. In addition, by the second inequality in Lemma 3.6,

(cid:107)M (cid:62)x(cid:107)2
2

(cid:46) n
η2

n
(cid:88)

j=1

x2
j =

n
η2 .

It follows that (cid:107)M (cid:107)2 = (cid:107)M (cid:62)(cid:107)2 (cid:46) n/η2 which, combined with (48), yields (cid:107)Kx(cid:107)2
2
we conclude that (cid:107)K(cid:107) (cid:46) 1/η.

(cid:46) 1/η2. Therefore,

Bounding (I). We now bound (cid:80)n
i,j=1 Lijϕiψjξiξj. Recall that the vectors ϕ, ψ and ξ satisfy
the relations (35) and are otherwise uniform random. Let z be a standard Gaussian vector in Rn
independent of (ϕ, ψ) and deﬁne

√

(cid:101)ξ (cid:44)

n − 2

z − (ϕ(cid:62)z)ϕ − (ψ(cid:62)z)ψ
(cid:13)z − (ϕ(cid:62)z)ϕ − (ψ(cid:62)z)ψ(cid:13)
(cid:13)
(cid:13)2

+ ϕ + ψ.

(49)

Then the tuple (ϕ, ψ, (cid:101)ξ) is equal to (ϕ, ψ, ξ) in law. Thus it suﬃces to study

n
(cid:88)

i,j=1

Lijϕiψj (cid:101)ξi (cid:101)ξj.

Note that we can write (cid:101)ξ = αz + β1ϕ + β2ψ for random variables α, β1, β2 ∈ R, where β1 =
1 − α(ϕ(cid:62)z) and β2 = 1 − α(ψ(cid:62)z). By concentration inequalities for (cid:107)z(cid:107)2 and ϕ(cid:62)z (Lemmas A.1
log n with probability at least 1 − 4n−8.
and A.3), we have 0.9 ≤ α ≤ 1.1 and |β1| ∨ |β2| ≤ 5
Therefore, we obtain

√

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiψj (cid:101)ξi (cid:101)ξj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiψjzizj

+ (cid:112)log n

Lijϕiϕjψjzi

+ (cid:112)log n

Lijϕiψ2

j zi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

+ (cid:112)log n

Lijϕ2

i ψjzj

+ (cid:112)log n

+ (log n)

Lijϕ2

i ϕjψj

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

Lijϕiψiψjzj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiϕjψiψj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

+ (log n)

Lijϕ2

i ψ2
j

+ (log n)

+ (log n)

Lijϕiψiψ2
j

By the symmetry of ϕ and ψ, it suﬃces to study the following quantities

n
(cid:88)

(i) :

Lijϕiψjzizj,

(ii) :

i,j=1
n
(cid:88)

(iv) :

i,j=1

Lijϕ2

i ϕjψj,

(v) :

n
(cid:88)

i,j=1
n
(cid:88)

i,j=1

Lijϕiϕjψjzi,

(iii) :

n
(cid:88)

Lijϕiψ2

j zi,

Lijϕ2

i ψ2
j ,

(vi) :

i,j=1
n
(cid:88)

Lijϕiϕjψiψj.

i,j=1

We now bound each of these sums.

21

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

(50)

(51)

Bounding (i). For the matrix K deﬁned by (46), Lemma A.2 yields that

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiψjzizj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= |z(cid:62)Kz| (cid:46) | Tr(K)| + (cid:107)K(cid:107)F

(cid:112)log n + (cid:107)K(cid:107) log n,

with probability at least 1 − n−10. The trace vanishes because

Tr(K) =

n
(cid:88)

i=1

Liiϕiψi =

1
η2 (cid:104)ϕ, ψ(cid:105) =

1
η2 (cid:104)U (cid:62)e1, U (cid:62)e2(cid:105) = 0.

Moreover, Lemmas 3.5 and 3.7 imply that with probability at least 1−4n−8, we have (cid:107)K(cid:107)F (cid:46) 1/η3/2
and (cid:107)K(cid:107) (cid:46) 1/η. Therefore, we conclude that

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiψjzizj

(cid:46)

√

log n
η3/2

+

log n
η

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Bounding (ii) and (iii). For (ii) in (51), Lemma A.1 gives that with probability at least 1−n−10,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

Lijϕiϕjψjzi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46)





n
(cid:88)

i=1



ϕ2
i



n
(cid:88)

j=1



1/2

2

Lijϕjψj





(cid:112)log n.

Applying Lemmas 3.6 and 3.4, we obtain that with probability at least 1 − 3n−8,

(cid:12)
(cid:12)
n
(cid:88)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j=1

Lijϕjψj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
2

n
(cid:88)

j=1

Lijϕ2

j +

1
2

n
(cid:88)

j=1

Lijψ2
j

(cid:46) 1
n

n
(cid:88)

j=1

Lij (cid:46) 1
η

.

Combining the above two bounds yields

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

Lijϕiϕjψjzi

(cid:33)1/2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46) 1
η

(cid:32) n
(cid:88)

i=1

ϕ2
i

(cid:112)log n =

√

log n
η

.

The same argument also gives the same upper bound on (iii) in (51).

Bounding (iv), (v) and (vi). The proofs for quantities (iv), (v) and (vi) in (51) are similar, so

we only present a bound on (vi). Since ϕiϕjψiψj ≤ 1

2 (ϕ2

i + ψ2

i )|ϕjψj|, it holds that

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiϕjψiψj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤

1
2

n
(cid:88)

(cid:32) n
(cid:88)

j=1

i=1

(cid:33)

Lijϕ2
i

|ϕjψj| +

1
2

n
(cid:88)

(cid:32) n
(cid:88)

j=1

i=1

(cid:33)

Lijψ2
i

|ϕjψj|.

By the second bound in Lemma 3.6 and the symmetry of ϕ and ψ, we then obtain that with
probability at least 1 − 2n−8,

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

Lijϕiϕjψiψj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46) 1
η

n
(cid:88)

j=1

|ϕjψj| ≤

1
η

22

where the last step is by Cauchy-Schwarz. Similar arguments yield the same bound on (iv) and (v)
in (51).

Substituting the bounds on (i)–(vi) into (50), we obtain that with probability at least 1 − n−7,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

Lijϕiψj (cid:101)ξi (cid:101)ξj

(cid:46)

√

log n
η3/2

+

log n
η

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

which is the desired bound for quantity (I),

Bounding (II). The argument for establishing the same bound on (cid:80)n
i,j=1 (cid:101)Lijϕiϕjξiξj is similar,
so we brieﬂy sketch the proof. Analogous to (50), we may use a (simpler) Gaussian approximation
argument to obtain

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

(cid:101)Lijϕiϕjξiξj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:101)Lijϕiϕjzizj

+ (cid:112)log n

+ (log n)

(cid:101)Lijϕ2

i ϕ2
j

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

(cid:101)Lijϕ2

i ϕjzj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ (cid:112)log n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i,j=1

(cid:101)Lijϕiϕ2

j zi

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(52)

where z is a standard Gaussian vector independent of ϕ and (cid:101)L. Note that the matrix (cid:101)L is the same
as L except that its diagonal entries are set to zero. Hence the last three terms on the right-hand
side can be bounded in the same way as before.

For the ﬁrst term on the right-hand side of (52), we again apply Lemma A.2 to obtain

n
(cid:88)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
i,j=1

(cid:101)Lijϕiϕjzizj

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:46) |z(cid:62) (cid:101)Kz| (cid:46) | Tr( (cid:101)K)| + (cid:107) (cid:101)K(cid:107)F

(cid:112)log n + (cid:107) (cid:101)K(cid:107) log n,

where (cid:101)K is deﬁned by (cid:101)Kij = (cid:101)Lijϕiϕj. The trace term vanishes because the diagonal of (cid:101)L is zero
by deﬁnition. The proofs of Lemmas 3.5, 3.6 and 3.7 continue to hold with (cid:101)K and (cid:101)L in place of
K and L respectively, and hence the norms (cid:107) (cid:101)K(cid:107)F and (cid:107) (cid:101)K(cid:107) admit the same bounds as (cid:107)K(cid:107)F and
(cid:107)K(cid:107).

Combining the bounds on (I) and (II) completes the proof of Lemma 3.3, and hence also of

Lemma 2.3.

3.2 Bounding the eﬀect of the noise

We now prove Lemma 2.4, bounding the diﬀerence X −X∗ between the noisy and noiseless settings.
Again, π∗ = id is assumed without loss of generality.

3.2.1 Vectorization and rotation by U

Without loss of generality, we consider the entries X11 − (X∗)11 and X21 − (X∗)21. Writing the
spectral decomposition A = U ΛU (cid:62), we apply a vectorization followed by a rotation by U to ﬁrst
put these diﬀerences in a more convenient form.

23

Recall the notation

ϕ = U (cid:62)e1, ψ = U (cid:62)e2,

and ξ = U (cid:62)1,

where this triple (ϕ, ψ, ξ) satisﬁes (35). Set

(cid:101)Z = U (cid:62)ZU,

L∗ = In ⊗ Λ − Λ ⊗ In,

L = In ⊗ Λ − (Λ + σ (cid:101)Z) ⊗ In,

and introduce

√

where i =

H = (L − iη In2)−1 − (L∗ − iη In2)−1 ∈ Cn2×n2

−1. We review relevant Kronecker product identities in Appendix B.

We formalize the vectorization and rotation operations as the following lemma.

Lemma 3.8. When π∗ = id,

X11 − (X∗)11 =

X21 − (X∗)21 =

1
η
1
η

Im(ϕ ⊗ ϕ)(cid:62)H(ξ ⊗ ξ),

Im(ϕ ⊗ ψ)(cid:62)H(ξ ⊗ ξ),

where Im denotes the imaginary part. Furthermore, the triple (ϕ, ψ, ξ) is independent of H.

Proof. From the deﬁnitions, X∗ and X can be written in vectorized form as

x∗ (cid:44) vec(X∗) =

(cid:88)

x (cid:44) vec(X) =

ij
(cid:88)

ij

1

(λi − λj)2 + η2 (uj ⊗ ui)(uj ⊗ ui)(cid:62)1n2 ∈ Rn2
(λi − µj)2 + η2 (vj ⊗ ui)(vj ⊗ ui)(cid:62)1n2 ∈ Rn2

1

.

(53)

(54)

(55)

Since ui, vj, λi, µj, η are all real-valued, we may further write

x∗ =

x =

1
η

1
η

Im

Im

(cid:88)

ij
(cid:88)

ij

1
λi − λj − iη

1
λi − µj − iη

(uj ⊗ ui)(uj ⊗ ui)(cid:62)1n2

(vj ⊗ ui)(vj ⊗ ui)(cid:62)1n2.

Recall the spectral decomposition (2). Note that In ⊗ A − A ⊗ In is a real symmetric matrix
with orthonormal eigenvectors {uj ⊗ ui}i,j∈[n] and corresponding eigenvalues λi − λj. Similarly,
In ⊗ A − B ⊗ In is real symmetric with eigenvectors {vj ⊗ ui}i,j∈[n] and eigenvalues λi − µj. Thus,
using U (cid:62)AU = Λ, we have

x∗ =

=

=

1
η
1
η
1
η

Im(In ⊗ A − A ⊗ In − iη In2)−11n2

Im(U ⊗ U )(In ⊗ Λ − Λ ⊗ In − iη In2)−1(U (cid:62) ⊗ U (cid:62))(1n ⊗ 1n)

Im(U ⊗ U )(L∗ − iη In2)−1(ξ ⊗ ξ).

24

Similarly, using U (cid:62)BU = U (cid:62)(A + σZ)U = Λ + σ (cid:101)Z, we have

x =

1
η

Im(In ⊗ A − B ⊗ In − iη In2)−11n2 =

1
η

Im(U ⊗ U )(L − iη In2)−1(ξ ⊗ ξ).

Therefore, for both (k, (cid:96)) = (1, 1) and (2, 1),

Xk(cid:96) − (X∗)k(cid:96) = e(cid:62)

k (X − X∗)e(cid:96) = (e(cid:96) ⊗ ek)(cid:62)(x − x∗) =

1
η

Im((U (cid:62)e(cid:96)) ⊗ (U (cid:62)ek))H(ξ ⊗ ξ)

which gives the desired (54) and (55).

For the independence claim, note that H is a function of (Λ, (cid:101)Z), while (ϕ, ψ, ξ) is a function of
U . Crucially, since Z ∼ GOE(n), Proposition 3.1(a) implies that (cid:101)Z = U (cid:62)ZU ∼ GOE(n) also for
every ﬁxed orthogonal matrix U . This distribution does not depend on U , so (cid:101)Z is independent of
U , and hence (ϕ, ψ, ξ) is independent of H.

We divide the remainder of the proof into the following two results.

Lemma 3.9. For some constant C > 0 and any deterministic matrix H ∈ Cn2⊗n2, with probability
at least 1 − n−7 over (ϕ, ψ, ξ),

|(ϕ ⊗ ϕ)(cid:62)H(ξ ⊗ ξ)| ∨ |(ϕ ⊗ ψ)(cid:62)H(ξ ⊗ ξ)| ≤ C

(cid:18)

(cid:107)H(cid:107) + (cid:107)H(cid:107)F

(cid:19)

.

log n
n

Lemma 3.10. In the setting of Lemma 2.4, for some constant C > 0 and for H deﬁned by (53),
with probability at least 1 − n−7,

(cid:107)H(cid:107) ≤

Cσ
η2

and

(cid:107)H(cid:107)F ≤

Cσn
η

(cid:18)

1 +

(cid:19)

.

σ
η

Lemma 2.4 follows immediately from these results. Indeed, applying Lemma 3.9 conditional on

H, followed by the estimates for H in Lemma 3.10, we get for both (k, (cid:96)) = (1, 1) and (2, 1) that
(cid:18) σ

(cid:19)(cid:19)

(cid:18)

|Xk(cid:96) − (X∗)k(cid:96)| <

C
η

η2 +

σ log n
η

1 +

σ
η

with probability at least 1 − 2n−7. By symmetry, the same result also holds for all pairs (k, (cid:96)), and
Lemma 2.4 follows from a union bound over (k, (cid:96)).

3.2.2 Proof of Lemma 3.9

Introduce S, (cid:101)S ∈ Cn×n such that

vec(S)(cid:62) = (ϕ ⊗ ϕ)(cid:62)H and

vec( (cid:101)S)(cid:62) = (ϕ ⊗ ψ)(cid:62)H.

Then the quantities to be bounded are

(ϕ ⊗ ϕ)(cid:62)H(ξ ⊗ ξ) = ξ(cid:62)Sξ

and

(ϕ ⊗ ψ)(cid:62)H(ξ ⊗ ξ) = ξ(cid:62) (cid:101)Sξ.

(56)

We bound these in three steps: First, we bound (cid:107)S(cid:107)F and (cid:107) (cid:101)S(cid:107)F . Second, we bound | Tr S| and
| Tr (cid:101)S|. Finally, we make a Gaussian approximation for ξ and apply the Hanson-Wright inequality
to bound the quantities in (56).

25

Estimates for (cid:107)S(cid:107)F and (cid:107) (cid:101)S(cid:107)F . We show that with probability at least 1 − 5n−8,

(cid:107)S(cid:107)F ∨ (cid:107) (cid:101)S(cid:107)F (cid:46) 1
n

(cid:107)H(cid:107)F +

(cid:19)1/4

(cid:18) log n
n

(cid:107)H(cid:107).

(57)

Note that H (cid:62) = H, and

(cid:107)S(cid:107)F = (cid:107)H(ϕ ⊗ ϕ)(cid:107)2

and

(cid:107) (cid:101)S(cid:107)F = (cid:107)H(ϕ ⊗ ψ)(cid:107)2.

We give the argument for Gaussian vectors, and then apply a Gaussian approximation for (ϕ, ψ).
Lemma 3.11. Let x, y ∈ Rn be independent with N (0, 1) entries. Then for a constant C > 0 and
any deterministic H ∈ Cn2×n2, with probability at least 1 − 2n−10,

(cid:107)H(x ⊗ x)(cid:107)2 ∨ (cid:107)H(x ⊗ y)(cid:107)2 ≤ C(cid:2)(cid:107)H(cid:107)F + (n3 log n)1/4(cid:107)H(cid:107)(cid:3).
Proof. Let M = H ∗H. Then (cid:107)H(x ⊗ x)(cid:107)2 = (x ⊗ x)(cid:62)M (x ⊗ x). We bound the mean and then
apply Gaussian concentration of measure. Recall the Wick formula for Gaussian expectations: For
any numbers aijk(cid:96) ∈ C,

E[xixjxkx(cid:96)]aijk(cid:96) =

(cid:88)

i,j,k,(cid:96)

(cid:88)

(cid:16)

i,j,k,(cid:96)

1{i = j, k = l} + 1{i = k, j = l} + 1{i = l, j = k}

(cid:17)

aijk(cid:96).

Denoting the entry Mij,k(cid:96) = (ei ⊗ ej)(cid:62)M (ek ⊗ e(cid:96)) and applying this to aijk(cid:96) = Mij,k(cid:96), we get

E[(x ⊗ x)(cid:62)M (x ⊗ x)] =

(cid:88)

ij

(Mii,jj + Mij,ij + Mij,ji).

Introduce the involution Q ∈ Rn2×n2 deﬁned by Q(ei⊗ej) = ej ⊗ei, and note also that (cid:80)
vec(In). Then the above yields
(cid:12)
(cid:12)vec(In)(cid:62)M vec(In) + Tr M + Tr M Q
(cid:12)

(cid:12)
E[(x ⊗ x)(cid:62)M (x ⊗ x)]
(cid:12)
(cid:12) =
≤ (cid:107)M (cid:107) · (cid:107)vec(In)(cid:107)2

F + (cid:107)H(cid:107)F (cid:107)HQ(cid:107)F = n(cid:107)H(cid:107)2 + 2(cid:107)H(cid:107)2
F .

2 + (cid:107)H(cid:107)2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

i(ei⊗ei) =

(58)

To establish the concentration of F (x) (cid:44) (x ⊗ x)(cid:62)M (x ⊗ x) around its mean, we aim to apply

Lemma A.5 by bounding the Lipschitz constant of F on the ball

Note that for each i ∈ [n],

B = {x ∈ Rn : (cid:107)x(cid:107)2 ≤ 2n}.

[(x ⊗ x)(cid:62)M (x ⊗ x)]

∂
∂xi
= (ei ⊗ x)(cid:62)M (x ⊗ x) + (x ⊗ ei)(cid:62)M (x ⊗ x) + (x ⊗ x)(cid:62)M (ei ⊗ x) + (x ⊗ x)(cid:62)M (x ⊗ ei).

(59)

For x ∈ B, using (cid:80)

i eie(cid:62)

i = In and M = M ∗, we have

n
(cid:88)

i=1

|(ei ⊗ x)(cid:62)M (x ⊗ x)|2 =

n
(cid:88)

(x ⊗ x)(cid:62)M (ei ⊗ x)(ei ⊗ x)(cid:62)M (x ⊗ x)

i=1

= (x ⊗ x)(cid:62)M (In ⊗ xx(cid:62))M (x ⊗ x)
≤ (cid:107)x ⊗ x(cid:107)2
≤ (2n)3(cid:107)M (cid:107)2 = 8n3(cid:107)H(cid:107)4.

2 · (cid:107)M (cid:107)2 · (cid:107)I ⊗ xx(cid:62)(cid:107) = (cid:107)x(cid:107)6

2(cid:107)M (cid:107)2

26

The same bound holds for the other three terms in (59). Thus for all x ∈ B and some constant
C0 > 0, (cid:107)∇F (x) ≤ C0n3(cid:107)H(cid:107)4. Thus x (cid:55)→ (x ⊗ x)(cid:62)M (x ⊗ x) is L0-Lipschitz on B, where L0 (cid:44)
√
C0n3(cid:107)H(cid:107)2. Finally, note that F (0) = 0 and P {x /∈ B} ≤ e−cn by the χ2 tail bound of Lemma
log n, we conclude that

A.3. Applying Lemma A.5 with t (cid:16)

√

|(x ⊗ x)(cid:62)M (x ⊗ x)| ≤ |E[(x ⊗ x)(cid:62)M (x ⊗ x)]| + Cn2e−cn/2(cid:107)H(cid:107)2 + C(cid:48)(cid:112)

n3 log n(cid:107)H(cid:107)2

(58)
≤ C(cid:48)(cid:48)((cid:107)H(cid:107)2

F +

(cid:112)

n3 log n(cid:107)H(cid:107)2)

holds with probability at least 1 − n−10, where C, C(cid:48), C(cid:48)(cid:48) are absolute constants. Taking the square
root gives the desired result for (cid:107)H(x ⊗ x)(cid:107)2.

The bound for H(x ⊗ y) is similar: We have

E[(cid:107)H(x ⊗ y)(cid:107)2

2] = E[(x ⊗ y)(cid:62)M (x ⊗ y)] =

Mij,ij = Tr M = (cid:107)H(cid:107)2
F .

(cid:88)

ij

On B2 = {(x, y) ∈ R2n : (cid:107)x(cid:107)2
Cn3(cid:107)H(cid:107)2 as above. Applying Lemma A.5 as above yields the desired bound on (cid:107)H(x ⊗ y)(cid:107)2.

2 ≤ 2n}, we obtain (cid:107)∇x,y[(x ⊗ y)(cid:62)M (x ⊗ y)](cid:107)2 ≤ L2

2 ≤ 2n, (cid:107)y(cid:107)2

0 ≡

We now apply this and a Gaussian approximation to show (57). For (cid:107)S(cid:107)F , let x be a standard
Gaussian vector in Rn, so that x/(cid:107)x(cid:107)2 is equal to ϕ in law. Lemma A.3 shows with probability
1 − n−10 that (cid:107)x(cid:107)2

2 ≥ n/2, so

(cid:107)S(cid:107)F = (cid:107)H(ϕ ⊗ ϕ)(cid:107)2 ≤

2
n

(cid:107)H(x ⊗ x)(cid:107)2,

and the result follows from Lemma 3.11. For (cid:107) (cid:101)S(cid:107)F , let x, y be independent standard Gaussian
vectors in Rn. Since ϕ(cid:62)ψ = 0 and (ϕ, ψ) is rotationally invariant in law, this pair is equal in law to

(cid:16) x

(cid:107)x(cid:107)2

,

y − (y(cid:62)x/(cid:107)x(cid:107)2
(cid:107)y − (y(cid:62)x/(cid:107)x(cid:107)2

2)x
2)x(cid:107)2

(cid:17)

.

Standard concentration inequalities of Lemmas A.1 and A.3 then yield

(cid:107) (cid:101)S(cid:107)F = (cid:107)H(ϕ ⊗ ψ)(cid:107)2 ≤

2
n

(cid:107)H(x ⊗ y)(cid:107)2 +

√

5

log n

n3/2

(cid:107)H(x ⊗ x)(cid:107)2

(60)

with probability at least 1 − 4n−8, so the result also follows from Lemma 3.11.

Estimates for Tr S and Tr (cid:101)S. Next, we show that with probability at least 1 − 5n−8,

| Tr S| ∨ | Tr (cid:101)S| ≤ 2(cid:107)H(cid:107).

(61)

Note that

and similarly

Tr S = Tr SIn = (ϕ ⊗ ϕ)(cid:62)Hvec(In),

Tr (cid:101)S = (ϕ ⊗ ψ)(cid:62)Hvec(In).

27

We apply a Gaussian approximation. To bound Tr S, let x be a standard Gaussian vector,
so x/(cid:107)x(cid:107)2 is equal to ϕ in law. Deﬁne G ∈ Cn×n by vec(G) = Hvec(In). Then it follows from
Lemmas A.3 and A.2 that

| Tr S| = |ϕ(cid:62)Gϕ| =

|x(cid:62)Gx|
(cid:107)x(cid:107)2
2

≤

1
0.9n

(cid:0)| Tr G| + C(cid:107)G(cid:107)F log n(cid:1)

(62)

with probability at least 1 − n−10. We apply

| Tr G| = | Tr GIn| = |vec(In)(cid:62)Hvec(In)| ≤ (cid:107)H(cid:107)(cid:107)In(cid:107)2

F = n(cid:107)H(cid:107),

and

(cid:107)G(cid:107)F = (cid:107)Hvec(In)(cid:107)2 ≤ (cid:107)H(cid:107)(cid:107)In(cid:107)F =

√

n (cid:107)H(cid:107).

Combining these yields | Tr S| ≤ 2(cid:107)H(cid:107) for large n. For Tr (cid:101)S, introducing independent standard
Gaussian vectors x, y, the same arguments as leading to (60) give

|(ϕ ⊗ ψ)(cid:62)Hvec(In)| ≤

=

2
n
2
n

√

5

log n

n3/2

(cid:12)(x ⊗ x)(cid:62)Hvec(In)(cid:12)
(cid:12)
(cid:12)

(cid:12)(x ⊗ y)(cid:62)Hvec(In)(cid:12)
(cid:12)
√
log n
5

|x(cid:62)Gy| +

(cid:12) +

|x(cid:62)Gx|

n3/2

with probability at least 1 − 4n−8. Then | Tr (cid:101)S| ≤ 2(cid:107)H(cid:107) follows from the same arguments as above
by invoking Lemma A.2.

Quadratic form bounds. We now use (57) and (61) to bound the quadratic forms (56) in ξ.
Note that ξ is dependent on (ϕ, ψ) and hence on S and (cid:101)S, thus tools such as the Hanson-Wright
inequality is not directly applicable; nevertheless, thanks to the uniformity of U on the orthogonal
group, (ξ, ϕ, ψ) = U T (1, e1, e2) are only weakly dependent and well approximated by Gaussians.
Below we make this intuition precise.

Consider ξ(cid:62) (cid:101)Sξ. Let z be a standard Gaussian vector in Rn independent of (ϕ, ψ) (and hence of
(d)
(cid:101)S), and recall the Gaussian representation (49) so that (ϕ, ψ, (cid:101)ξ)
= (ϕ, ψ, ξ). Write (49) as (cid:101)ξ = αz+ (cid:101)ϕ,
where (cid:101)ϕ = (1 − α(ϕ(cid:62)z))ϕ + (1 − α(ψ(cid:62)z))ψ is a linear combination of ϕ and ψ. By concentration
inequalities for (cid:107)z(cid:107)2 and ϕ(cid:62)z in Lemmas A.1 and A.3, we have 0.9 ≤ α ≤ 1.1 and (cid:107) (cid:101)ϕ(cid:107)2 ≤ 8
log n
with probability 1 − 4n−8. On this event,

√

|ξ(cid:62) (cid:101)Sξ| ≤ 1.21|z(cid:62) (cid:101)Sz| + | (cid:101)ϕ(cid:62) (cid:101)S (cid:101)ϕ| + 1.1|z(cid:62) (cid:101)S (cid:101)ϕ| + 1.1| (cid:101)ϕ(cid:62) (cid:101)Sz|.

We bound these four terms separately conditional on (ϕ, ψ).

For the ﬁrst term, applying the Hanson-Wright inequality of Lemma A.2, we have

with probability at least 1 − n−10. For the second term, applying (cid:107) (cid:101)ϕ(cid:107)2 ≤ 8

√

log n,

|z(cid:62) (cid:101)Sz| ≤ | Tr (cid:101)S| + C(cid:107) (cid:101)S(cid:107)F log n

| (cid:101)ϕ(cid:62) (cid:101)S (cid:101)ϕ| ≤ (cid:107) (cid:101)S(cid:107)(cid:107) (cid:101)ϕ(cid:107)2

2 ≤ 64(cid:107) (cid:101)S(cid:107)F log n.

28

For the third term,

Applying again Lemma A.2, with probability at least 1 − n−10,

| (cid:101)ϕ(cid:62) (cid:101)Sz| ≤ (cid:107) (cid:101)ϕ(cid:107)2(cid:107) (cid:101)Sz(cid:107)2.

(cid:107) (cid:101)Sz(cid:107)2

2 ≤ Tr (cid:101)S∗ (cid:101)S + C(cid:107) (cid:101)S∗ (cid:101)S(cid:107)F log n ≤ (C + 1)(cid:107) (cid:101)S(cid:107)2

F log n,

so

| (cid:101)ϕ(cid:62) (cid:101)Sz| (cid:46) (cid:107) (cid:101)S(cid:107)F log n.
The fourth term is bounded similarly to the third, and combining these gives |ξ(cid:62) (cid:101)Sξ| (cid:46) | Tr (cid:101)S| +
(cid:107) (cid:101)S(cid:107)F log n with probability at least 1 − 6n−8. Applying (57) and (61), we get

|(ϕ ⊗ ψ)(cid:62)H(ξ ⊗ ξ)| = |ξ(cid:62) (cid:101)Sξ| (cid:46) (cid:107)H(cid:107) +

log n
n

(cid:107)H(cid:107)F

as desired.

The Gaussian approximation argument for (ϕ ⊗ ϕ)(cid:62)H(ξ ⊗ ξ) = ξ(cid:62)Sξ is simpler and omitted

for brevity. This concludes the proof of Lemma 3.9.

3.2.3 Proof of Lemma 3.10

Recalling the deﬁnition of H in (53) and applying

we get

A−1 − B−1 = A−1(B − A)B−1,

(63)

− H = (L∗ − iη In2)−1(σ (cid:101)Z ⊗ In)(L − iη In2)−1.
As L∗ − iη In2 is diagonal with each entry at least η in magnitude, we have the deterministic
bound (cid:107)(L∗ − iη In2)−1(cid:107) ≤ 1/η, and similarly (cid:107)(L − iη In2)−1(cid:107) ≤ 1/η. Applying Proposition 3.1(c),
(cid:107) (cid:101)Z(cid:107) ≤ C with probability at least 1 − n−10 for a constant C > 0. On this event, (cid:107)H(cid:107) ≤ Cσ/η2.

(64)

To bound (cid:107)H(cid:107)F , let us apply (63) again to (L − iη In2)−1 in (64), to write

−H = (L∗ − iηI)−1(σ (cid:101)Z ⊗ I)(L∗ − iηI)−1(cid:16)

I − (σ (cid:101)Z ⊗ I)(L − iηI)−1(cid:17)

.

Applying

we get with probability at least 1 − n−10 that

(cid:107)AB(cid:107)F ≤ (cid:107)A(cid:107)F (cid:107)B(cid:107),

(cid:107)H(cid:107)F ≤ (cid:107)(L∗ − iηI)−1(σ (cid:101)Z ⊗ I)(L∗ − iηI)−1(cid:107)F (1 + Cσ/η).

(65)

(66)

Note that here, L∗ is diagonal, and (cid:101)Z is independent of L∗. Let w ∈ Cn2 consist of the
λi−λk−iη . Let us also

diagonal entries of (L∗ − iηI)−1, indexed by the pair (i, k) ∈ [n]2, i.e., wik =
desymmetrize (cid:101)Z and write

1

(cid:101)Z =

1
√
2n

(W + W (cid:62)),

29

where W ∈ Rn×n has n2 independent N (0, 1) entries. Then

(cid:107)(L∗ − iηI)−1(σ (cid:101)Z ⊗ I)(L∗ − iηI)−1(cid:107)2

F =

=

σ2
2n
σ2
2n

(cid:13)
(cid:13)

(cid:13)(L∗ − iηI)−1(cid:0)(W + W (cid:62)) ⊗ I(cid:1)(L∗ − iηI)−1(cid:13)

2
(cid:13)
(cid:13)
F

n
(cid:88)

(Wij + Wji)2|wik|2|wjk|2.

i,j,k=1

Recall the symmetric matrix L ∈ Rn×n deﬁned by (36). We have

n
(cid:88)

k=1

|wik|2|wjk|2 =

n
(cid:88)

k=1

1
(λi − λk)2 + η2 ·

1
(λj − λk)2 + η2 =

n
(cid:88)

k=1

LikLjk = (L2)ij.

Introducing v = vec(L2) ∈ Rn2

+ indexed by (i, j), and applying Lemma A.3 conditional on v, we get

(cid:107)(L∗ − iηI)−1(σ (cid:101)Z ⊗ I)(L∗ − iηI)−1(cid:107)2

F ≤

2σ2
n

n
(cid:88)

i,j=1

W 2

ijvij ≤

2σ2
n

((cid:107)v(cid:107)1 + C(cid:107)v(cid:107)2 log n)

(67)

with probability at least 1 − n−10.

Finally, we apply Lemma 3.4 to bound (cid:107)v(cid:107)1 and (cid:107)v(cid:107)2. Note that (cid:107)v(cid:107)1 = 1L21 = (cid:107)L1(cid:107)2
2.

Applying maxi(L1)i ≤ Cn/η from Lemma 3.4, we get with probability at least 1 − n−8 that

(cid:107)v(cid:107)1 (cid:46) n3/η2.

By (65), we also have (cid:107)v(cid:107)2
(cid:107)L(cid:107)2

F ≤ Cn2/η3 from Lemma 3.4, we get

2 = (cid:107)L2(cid:107)2

F ≤ (cid:107)L(cid:107)2 · (cid:107)L(cid:107)2

F . Applying (cid:107)L(cid:107) ≤ maxi(L1)i ≤ Cn/η and

Applying this to (67) and then back to (66) yields

(cid:107)v(cid:107)2
2

(cid:46) n4/η5.

(cid:107)H(cid:107)2
F

(cid:46) σ2
n

(cid:18) n3

η2 +

n2 log n
η5/2

(cid:19) (cid:18)

1 +

(cid:19)2

σ
η

(cid:46) σ2n2
η2

(cid:18)

1 +

(cid:19)2

,

σ
η

where the second inequality holds for η > n−0.1. This is the desired bound on (cid:107)H(cid:107)F .

This concludes the proof of Lemma 3.10, and hence of Lemma 2.4.

3.3 Proof for the bipartite model

We now prove Theorem 2.5 for exact recovery in the bipartite model. We ﬁrst show that Algorithm 2
successfully recovers π∗
1. This extends the preceding argument in the symmetric case. We then
2 if (cid:98)π1 = π∗
show that the linear assignment subroutine recovers π∗
1.

3.3.1 Recovery of π∗

1 by spectral method

F F (cid:62),
The argument is a minor extension of that in the Gaussian Wigner model. Let us write A =
and introduce its spectral decomposition A = U ΛU (cid:62). Note that Λ and U then consist of the singular
values and left singular vectors of F .

√

30

√

(λ+−x)(x−λ−)

3.1 hold for A, where the constants C, C(cid:48), c may depend on κ = lim n/m.
the law of
√

To analyze the noiseless solution X∗ = (cid:98)X(A, A), note that all three claims of Proposition
Indeed, here, ρ is
λ when λ is distributed according to the Marcenko-Pastur distribution with density
κ)2. Then the density of ρ is 2xg(x2) (In the
g(x) =
case of κ = 1, ρ is the quarter-circle law.) Therefore, for any κ ∈ (0, 1], the density of ρ is supported
on on [1 −
κ] and bounded by some κ-dependent constant C. The rate of convergence
in (b) follows from [GT11, Theorem 1.1], and the claims in (a) and (c) are well-known. Thus the
proof of Lemma 2.3 applies, and we obtain with probability 1 − 5n−5 that

1{λ−≤x≤λ+} and λ± (cid:44) (1 ±

κ, 1 +

2πκx

√

√

√

(X∗)ii > η2/2,

min
i∈[n]

max
i,j∈[n]:i(cid:54)=j

(X∗)ij < C

Next, we analyze the noisy solution X (cid:44) (cid:98)X(A, B). Set B =

replacing Λ + σ (cid:101)Z in L with the general deﬁnition

L = In ⊗ Λ − U (cid:62)BU ⊗ In.

(cid:18) √

log n
η3/2
√

+

(cid:19)

.

log n
η

(68)

GG(cid:62), and deﬁne H by (53) but

Then the representations (54) and (55) of Lemma 3.8 continue to hold. Furthermore, write the
singular value decomposition F = U ΛV (cid:62), set (cid:102)W = U (cid:62)W , and note that U is uniformly random
and independent of (Λ, V, (cid:102)W ). Then

U (cid:62)BU = U (cid:62)(cid:113)

(F + σW )(F + σW )(cid:62)U =

(cid:113)

(ΛV (cid:62) + σ(cid:102)W )(ΛV (cid:62) + σ(cid:102)W )(cid:62)

which is independent of U , so that (ϕ, ψ, ξ) is still independent of H. Then applying Lemma 3.9
conditional on H, we get with probability at least 1 − n−7 that

|Xk(cid:96) − (X∗)k(cid:96)| (cid:46) 1
η

(cid:18)

(cid:107)H(cid:107) + (cid:107)H(cid:107)F

(cid:19)

log n
n

for both (k, (cid:96)) = (1, 1) and (2, 1).

To conclude the proof, we need a counterpart of Lemma 3.10 bounding the norms of H. Let us

simply use the fact that H has dimension n2 × n2 to bound (cid:107)H(cid:107)F ≤ n(cid:107)H(cid:107), and apply

(cid:107)H(cid:107) ≤ (cid:107)(L − iηI)−1(cid:107) · (cid:107)L − L∗(cid:107) · (cid:107)(L∗ − iηI)−1(cid:107) ≤ (cid:107)L − L∗(cid:107)/η2.

Then

(cid:107)L − L∗(cid:107) = (cid:107)U (cid:62)(A − B)U ⊗ In(cid:107) = (cid:107)A − B(cid:107) ≤

(cid:18)

(cid:107)F − G(cid:107)

2 + log

2
π

(cid:19)

(cid:107)F (cid:107) + (cid:107)G(cid:107)
(cid:107)F − G(cid:107)

where the last inequality follows from [Kat73, Proposition 1]. For a constant C > 0, this is at most
Cσ log(1/σ) with probability at least 1 − n−10 by the analogue of Proposition 3.1(c) applied to the
noise W = (G − F )/σ in this model. Combining these bounds yields for (k, (cid:96)) = (1, 1) and (2, 1)
that with probability 1 − 2n−7,

|Xk(cid:96) − (X∗)k(cid:96)| ≤

Cσ log(1/σ) log n
η3

.

31

This holds for all pairs (k, (cid:96)) with probability at least 1 − 2n−5 by a union bound. Thus for
η < c/ log n, σ log(1/σ) < c(cid:48)η/ log n, and suﬃciently small constants c, c(cid:48) > 0, we get from (68) that

min
i∈[n]

Xii > max

i,j∈[n]:i(cid:54)=j

Xij.

So Algorithm 2 recovers (cid:98)π1 = π∗
of the algorithm, this also holds for any π∗
1.

1 with probability at least 1 − 7n−5 when π∗

1 = id. By equivariance

3.3.2 Recovery of π∗
2 by linear assignment
We now show that on the event where (cid:98)π1 = π∗
step of Algorithm 2 recovers (cid:98)π2 = π∗
1 = id, and denote more simply π∗ = π∗
π∗
Theorem 3.12. Consider the single permutation model Gid,π∗ = F + σW where Gid,π∗
and F and W are as in Theorem 2.5. Let

2. We then formalize this claim as follows.

1, as long as n (cid:38) log m
log(1+ 1

4σ2 )

2 with high probability. Without loss of generality, let us take

, the linear assignment

ij = Gi,π∗(j),

(cid:98)π = argmax
π∈Sm

m
(cid:88)

j=1

(F (cid:62)G)j,π(j).

If n ≥ 24 log m
log(1+ 1
4σ2 )

, then (cid:98)π = π∗ with probability at least 1 − 2m−4.

Proof. Without loss of generality, assume that π∗ = id. Let us also rescale and consider G =
F + σW , where F and W are n × m random matrices with i.i.d. N (0, 1) entries. Our goal is to
show that

(cid:98)Π = argmax
Π∈Sm

(cid:104)F Π, G(cid:105)

coincides with the identity with probability at least 1 − m−4.

For any Π (cid:54)= I, we have (cid:104)F Π, G(cid:105)−(cid:104)F, G(cid:105) = σ(cid:104)F (Π−I), W (cid:105)−(cid:104)F (I−Π), F (cid:105), where (cid:104)F (I−Π), F (cid:105) =

1
2 (cid:107)F (I − Π)(cid:107)2

F . Then

P {(cid:104)F Π, G(cid:105) > (cid:104)F, G(cid:105)} = P

(cid:26)(cid:28)

(cid:29)

W,

F (Π − I)
(cid:107)F (I − Π)(cid:107)F
(cid:18) (cid:107)F (I − Π)(cid:107)F
2σ
(cid:107)F (I − Π)(cid:107)2
F
8σ2

(cid:19)(cid:21)

−

(cid:18)

(cid:20)

Q

= E

(cid:20)

(a)
≤ E

exp

≥

(cid:107)F (I − Π)(cid:107)F
2σ

(cid:27)

(cid:19)(cid:21)

(cid:26)

E

(b)
=

(cid:20)

(cid:18)

exp

−

(cid:107)z(cid:62)(I − Π)(cid:107)2
F
8σ2

(cid:19)(cid:21)(cid:27)n

,

where (a) follows from the Gaussian tail bound Q(x) (cid:44) (cid:82) ∞
x
because the n rows of F are i.i.d. copies of z ∼ N (0, Im).

1√

2π

e−t2/2dt ≤ e−x2/2 for x > 0; (b) is

32

Denote the number of non-ﬁxed points of Π by k ≥ 2, which is also the rank of I − Π. Denote its
F = 2k and maxi∈[k] σi ≤ (cid:107)I−Π(cid:107) ≤ 2.
i.i.d.∼ N (0, 1). Then
i , where w1, . . . , wk

singular values by σ1, . . . , σk. Then we have (cid:80)k
By rotational invariance, we have (cid:107)z(cid:62)(I − Π)(cid:107)2
F

i = (cid:107)I−Π(cid:107)2
i w2
i=1 σ2

i=1 σ2
(d)
= (cid:80)k

(cid:20)

E

(cid:18)

exp

−

(cid:107)z(cid:62)(I − Π)(cid:107)2
F
8σ2

(cid:19)(cid:21)

=

(cid:20)

E

k
(cid:89)

i=1

(cid:40)

i

σ2
8σ2 w2
i
(cid:18)

(cid:18)

exp

−

(cid:19)(cid:21)

(69)

= exp

−

log

1 +

1
2

k
(cid:88)

i=1

(cid:19)(cid:41)

σ2
i
4σ2

(cid:26)

≤ exp

−

(cid:18)

log

1 +

(cid:19)(cid:27)

,

1
4σ2

k
8

where the last step is due to (cid:80)k
the union bound over Π (cid:54)= I, we have

i=1 1{σ2

i ≥1} ≥ k/4.4 Combining the last two displays and applying

P

(cid:110)

(cid:98)Π (cid:54)= I

(cid:111)

≤

≤

(cid:88)

Π(cid:54)=I
m
(cid:88)

k=2

P {(cid:104)F Π, G(cid:105) > (cid:104)F, G(cid:105)}

(cid:19)

(cid:18)m
k

(cid:18)

k!

1 +

1
4σ2

(cid:19)−nk/8

≤

m
(cid:88)

k=2

(cid:18)

mk

1 +

1
4σ2

(cid:19)−nk/8

≤ 2m−4,

provided that m ≥ 2 and m (cid:0)1 + 1
4σ2

(cid:1)−n/8 ≤ m−2.

3.4 Gradient descent dynamics

Finally, we prove Corollary 2.2, which connects (cid:98)X in (3) to the gradient descent dynamics (17) and
the optimization problems (15) and (16).

To show that (cid:98)X solves (15), note that the objective function in (15) is quadratic, with ﬁrst

order optimality condition

A2X + XB2 − 2AXB + η2X = J.

Setting x = vec(X) and writing this in vectorized form

(cid:2)(In ⊗ A − B ⊗ In)2 + η2In2

(cid:3)x = 1n2,

we see that the vectorized solution to (15) is

(cid:98)x = (cid:2)(In ⊗ A − B ⊗ In)2 + η2In2

(cid:3)−11n2 ∈ Rn2

.

Applying the spectral decomposition (2), we get

(cid:98)x =

(cid:88)

ij

1

(λi − µj)2 + η2 (vj ⊗ ui)(vj ⊗ ui)(cid:62)1n2 =

u(cid:62)
i Jnvj

(λi − µj)2 + η2 vec(uiv(cid:62)
j ),

(cid:88)

ij

(70)

which is exactly the vectorization of (cid:98)X in (22).

4 The sharp condition n log (cid:0)1 + 1
σ2

exactly; cf. [DCK19].

(cid:1) − 4 log m → +∞ can be obtained by computing the singular values in (69)

33

Recall that (cid:101)X denotes the minimizer of (16). Introducing a Lagrange multiplier 2α ∈ R for the
constraint, the ﬁrst-order stationarity condition is A2X + XB2 − 2AXB + η2X = αJ, and hence
(cid:101)X = α (cid:98)X. To ﬁnd α, note that 1(cid:62) (cid:101)X1 = α1(cid:62) (cid:98)X1 = n. Furthermore, from (3) we have

1(cid:62) (cid:98)X1 =

(cid:104)ui, 1(cid:105)2(cid:104)vj, 1(cid:105)2
(λi − µj)2 + η2 > 0.

(cid:88)

ij

Hence α > 0. These claims together establish part (a).

For (b), let us consider the gradient descent dynamics also in its vectorized form. Namely,

deﬁne x(t) (cid:44) vec(X (t)). Then (17) can be written as

x(t+1) = (cid:2)(1 − γη2)In2 − γ(In ⊗ A − B ⊗ In)2(cid:3)x(t) + γ1n2.

For the initialization x(0) = 0, this gives

t−1
(cid:88)

x(t) = γ

(cid:2)(1 − γη2)In2 − γ(In ⊗ A − B ⊗ In)2(cid:3)s1n2

s=0
n
(cid:88)

= γ

i,j=1
n
(cid:88)

i,j=1

=

t−1
(cid:88)

s=0

(cid:2)1 − γη2 − γ(λi − µj)2(cid:3)s(vj ⊗ ui)(vj ⊗ ui)(cid:62)1n2

1 − [1 − γη2 − γ(λi − µj)2]t
η2 + (λi − µj)2

(vj ⊗ ui)(vj ⊗ ui)(cid:62)1n2.

(71)

Undoing the vectorization yields part (b).

For (c), note that η2 + (λi − µj)2 < C with probability at least 1 − n−10 by Proposition 3.1(c),
so that the convergence in part (b) holds provided that the step size γ ≤ c for some suﬃciently
small constant c. On this event, for all pairs (k, (cid:96)) we may apply the simple bound

|X (t)

k(cid:96) − (cid:98)Xk(cid:96)| ≤

(cid:88)

(1 − γ(λi − µj)2 − γη2)t
(λi − µj)2 + η2

|(u(cid:62)

i ek)(v(cid:62)

j e(cid:96))(u(cid:62)

i Jvj)|

ij
(1 − γη2)t
η2
(1 − γη2)t
η2

≤

≤

|(u(cid:62)

i ek)(v(cid:62)

j e(cid:96))(u(cid:62)

i Jvj)|

· n2 max

ij

· n3.

In particular, for t ≥ (C log n)/(γη2) and a suﬃciently large constant C > 0, this is at most 1/n.
Then the conclusion of Theorem 2.1 with X (t) in place of (cid:98)X still follows from Lemmas 2.3 and 2.4.

4 Numerical experiments

This section is devoted to comparing our spectral method to various methods for graph match-
ing, using both synthetic examples and real datasets. Let us ﬁrst make a few remarks regarding
implementation of graph matching algorithms.

Similar to the last step of Algorithm 1, many methods that we compare consist of a ﬁnal step
that rounds a similarity matrix to produce a permutation estimator. Throughout the experiments,

34

for the sake of comparison we always use the linear assignment (5) for rounding, which typically
yields noticeably better outcomes than the simple greedy rounding (6).

For GRAMPA, Theorem 2.1 suggests that the regularization parameter η needs to be chosen so
that σ ∨ n−0.1 (cid:46) η (cid:46) 1/ log n. In practice, one may compute estimates (cid:98)πη for diﬀerent values of
η and select the one with the minimum objective value (cid:107)A − B(cid:98)πη (cid:107)2
F . We ﬁnd in simulations that
the performance of GRAMPA is in fact not very sensitive to the choice of η, unless η is extremely
close to zero or larger than one. For simplicity and consistency, we apply GRAMPA to centered and
normalized adjacency matrices and ﬁx η = 0.2 for all synthetic experiments.

4.1 Universality of GRAMPA

Although the main theoretical result of this work, Theorem 2.1, is only proved for the Gaussian
Wigner model, the proposed spectral method in Algorithm 1 (denoted by GRAMPA) can in fact be
used to match any pair of weighted graphs. Particularly, in view of the universality results in
the companion paper [FMWX19], the performance of GRAMPA for the Gaussian Wigner model is
comparable to that for the suitably calibrated Erd˝os-R´enyi model. This is veriﬁed numerically in
Figure 2 which we now explain.

Figure 2: Universality of the performance of GRAMPA on three random graph models with 1000
vertices.

Given the latent permutation π∗, an edge density p ∈ (0, 1), and a noise parameter σ ∈ [0, 1],
we generate two correlated Erd˝os-R´enyi graphs on n vertices with adjacency matrices A and B,
such that (Aij, Bπ∗(i),π∗(j)) are i.i.d. pairs of correlated Bernoulli random variables with marginal
distribution Bern(p). Conditional on A,

Bπ∗(i),π∗(j) ∼

(cid:40)

Bern(cid:0)1 − σ2(1 − p)(cid:1)
Bern(cid:0)σ2p(cid:1)

if Aij = 1,
if Aij = 0.

(72)

Here the operational meaning of the parameter σ is that the fraction of edges which diﬀer between
the two graphs is approximately 2σ2(1 − p) ≈ 2σ2 for sparse graphs. In particular, the extreme
cases of σ = 0 and σ = 1 correspond to A and B which are perfectly correlated and independent,
respectively.

35

00.10.20.30.40.50.600.10.20.30.40.50.60.70.80.91Furthermore, the model parameters in (72) are calibrated to be directly comparable with the
Gaussian model, so that it is convenient to verify experimentally the universality of our spectral
algorithm. Indeed, denote the centered and normalized adjacency matrices by

A (cid:44) (p(1 − p)n)−1/2(A − E[A])

and B (cid:44) (p(1 − p)n)−1/2(B − E[B]).

(73)

Then it is easy to check that Aij and Bij both have mean zero and variance 1/n, and moreover,

E[(Aij − Bπ∗(i),π∗(j))2] = 2σ2/n.

(74)

√

Note that equation (74) also holds for the oﬀ-diagonal entries of the Gaussian Wigner model
B = A +
2σZ, where A and Z are independent GOE(n) matrices. In Figure 2, we implement
GRAMPA to match pairs of Gaussian Wigner, dense Erd˝os-R´enyi (p = 0.5) and sparse Erd˝os-R´enyi
(p = 0.01) graphs with 1000 vertices, and plot the fraction of correctly matched pairs of vertices
against the noise level σ, averaged over 10 independent repetitions. The performance of GRAMPA on
the three models is indeed similar, agreeing with the universality results proved in the companion
paper [FMWX19].

For this reason, in the sequel we primarily consider the Erd˝os-R´enyi model for synthetic ex-
periments. In addition, while GRAMPA is applicable for matching weighted graphs, many algorithms
in the literature were proposed speciﬁcally for unweighted graphs, so using the Erd˝os-R´enyi model
allows us to compare more methods in a consistent setup.

4.2 Comparison of spectral methods

We now compare the performance of GRAMPA to several existing spectral methods in the literature.
Besides the rank-1 method of rounding the outer product of top eigenvectors (8) (denoted by
TopEigenVec), we consider the IsoRank algorithm of [SXB08], the EigenAlign and LowRankAlign5
algorithms of [FQM+19], and Umeyama’s method [Ume88] which rounds the similarity matrix (10).
In Figure 3(a), we apply these algorithms to match Erd˝os-R´enyi graphs with 100 vertices6 and edge
density 0.5. For each spectral method, we plot the fraction of correctly matched pairs of vertices
of the two graphs versus the noise level σ, averaged over 10 independent repetitions. While all
estimators recover the exact matching in the noiseless case, it is clear that GRAMPA is more robust
to noise than all previous spectral methods by a wide margin.

A more precise comparison of GRAMPA with its closest competitor, Umeyama’s method, is given
1
in Fig. 3(b), which shows that Umeyama’s method is only robust to noise up to σ =
poly(n) ,
polylog(n) . Speciﬁcally,
whereas we prove in [FMWX19] that GRAMPA yields exact recovery up to σ =
we test these two methods on Erd˝os-R´enyi graphs with edge density 0.5 and sizes n = 400, 800
and 1600. The noise parameter σ is set to Cn−0.25 with varying values of C, where the exponent
−0.25 is empirically found to be the critical exponent above which Umeyama’s method fails. Out
of 100 independent trials, we record the fraction of times when the algorithm exactly recovers the
matching between the two graphs, and plot this quantity against C. From the lines U 400, U 800,
and U 1600 corresponding to Umeyama’s method on the three respective graph sizes, we see that
the performance of Umeyama’s method does not vary with n, supporting that σ (cid:16) n−0.25 is the

1

5We implement the rank-2 version of LowRankAlign here because a higher rank does not appear to improve its

performance in the experiments.

6This experiment is not run on larger graphs because IsoRank and EigenAlign involve taking Kronecker products

of graphs and are thus not as scalable as the other methods.

36

(a) Fraction of correctly matched vertices, on
Erd˝os-R´enyi graphs with n = 100 vertices,
averaged over 10 repetitions

(b) Rate of exact recovery for GRAMPA and
Umeyama’s method, on Erd˝os-R´enyi graphs with a
varying number of vertices, out of 100 repetitions

Figure 3: Comparison of six spectral methods for matching Erd˝os-R´enyi graphs with expected edge
density 0.5 and 100 vertices.

critical threshold for exact recovery by this method. Conversely, as n increases, the failure of
GRAMPA occurs at a larger value of C, as seen in the curves G 400, G 800, and G 1600. This aligns
with the theoretical result in [FMWX19] that GRAMPA succeeds for σ =

1
polylog(n) .

4.3 Comparison with quadratic programming and Degree Proﬁle

Next, we consider more competitive graph matching algorithms outside the spectral class. Since
our method admits an interpretation through the regularized QP (15) or (16), it is of interest
to compare its performance to (the algorithm that rounds the solution to) the full QP (14) with
full doubly stochastic constraints, denoted by QP-DS. Another recently proposed method for graph
matching is Degree Proﬁle [DMWX18], for which theoretical guarantees comparable to our results
have been established for the Gaussian Wigner and Erd˝os-R´enyi models.

Figure 4(a) plots the fraction of correctly matched vertex pairs by the three algorithms, on
Erd˝os-R´enyi graphs with 500 vertices and edge density 0.5, averaged over 10 independent repeti-
tions. GRAMPA outperforms DegreeProfile, while QP-DS is clearly the most robust, albeit at a much
higher computational cost. Since oﬀ-the-shelf QP solvers are extremely slow on instances with n
larger than several hundred, we resort to an alternating direction method of multipliers (ADMM)
procedure used in [DMWX18]. Still, solving (14) is more than 350 times slower than computing
the similarity matrix (3) for the instances in Figure 4(a). Moreover, DegreeProfile is about 15
times slower. We argue that GRAMPA achieves a desirable balance between speed and robustness
when implemented on large networks.

A closer inspection of the exact recovery threshold of GRAMPA and DegreeProfile is done in
Figure 4(b), in a similar way as Figure 3(b). Since Theorem 2.1 suggests that GRAMPA is robust
to noise up to σ (cid:46) 1
log n , we set the noise level to be σ = C/ log n, and plot the fraction of exact
recovery against C. The results for Erd˝os-R´enyi graphs of size n = 400, 800, 1600 and for the two
methods GRAMPA and DegreeProfile are represented by the curves G 400, G 800, G 1600 and D 400,

37

00.10.20.30.40.500.10.20.30.40.50.60.70.80.910.30.40.50.60.70.80.9100.10.20.30.40.50.60.70.80.91(a) Fraction of correctly matched vertices, on
Erd˝os-R´enyi graphs with n = 500 vertices,
averaged over 10 repetitions

(b) Rate of exact recovery for GRAMPA and
DegreeProfile, on Erd˝os-R´enyi graphs with a
varying number of vertices, out of 100 repetitions

Figure 4: Comparison of three competitive methods for matching Erd˝os-R´enyi graphs with expected
edge density 0.5.

D 800, D 1600 respectively. These results suggest that GRAMPA is more robust than DegreeProfile
by at least a constant factor.

4.4 More graph ensembles

To demonstrate the performance of GRAMPA on other models, we turn to sparser regimes of the
Erd˝os-R´enyi model, as well as other random graph ensembles. In Figure 5, we compare the per-
formance of GRAMPA and DegreeProfile (the two fast and robust methods from the preceding
experiments) on correlated pairs of sparse Erd˝os-R´enyi graphs, stochastic blockmodels, and power-
law graphs. Following [PG11], we generate these pairs by sampling a “mother graph” according to
such a model with edge density p/s for 0 < p < s ≤ 1, and then generating A and B by deleting
each edge independently with probability 1 − s. This yields marginal edge density p in both A and
B. We apply GRAMPA with the centered and normalized matrices A and B from (73) as input, with
p bing the marginal edge density. In each ﬁgure, the fraction of correctly matched pairs of vertices
is plotted against the eﬀective noise level

σ =

(cid:114) 1 − s
1 − p

,

for graphs with 1000 vertices, averaged over 10 independent repetitions. One may verify that the
above procedure of generating (A, B) and the deﬁnition of σ both agree with the previous deﬁnition
(72) in the Erd˝os-R´enyi setting.

In Figures 5(a) and 5(b), we consider Erd˝os-R´enyi graphs with edge density p = 0.01 and
0.005. Note that the sharp threshold of p for the connectedness of an Erd˝os-R´enyi graph with 1000
vertices is log n
n ≈ 0.0069 [ER60], below which the graphs contain isolated vertices whose matching
is non-identiﬁable. We see that the performance of GRAMPA is better than DegreeProfile in both
settings, and particularly in the sparser regime.

38

00.10.20.30.40.50.60.70.800.10.20.30.40.50.60.70.80.910.40.50.60.70.80.911.11.200.10.20.30.40.50.60.70.80.91(a) Erd˝os-R´enyi graph with
p = 0.01

(c) Stochastic blockmodel with
p = 0.01, pin = 0.016, and
pout = 0.004

(e) Power-law graph with
p = 0.01

(b) Erd˝os-R´enyi graph with
p = 0.005

(d) Stochastic blockmodel with
p = 0.005, pin = 0.008, and
pout = 0.002

(f) Power-law graph with
p = 0.005

Figure 5: Comparison of GRAMPA and DegreeProfile on synthetic networks

In Figures 5(c) and 5(d), we consider the stochastic blockmodel [HLL83] with two communities
each of size 500. The probability of an edge between vertices in the same (resp. diﬀerent) community
is denoted by pin (resp. pout). The values of pin and pout are chosen so that the overall expected
edge densities are p = 0.01 and 0.005 as in the Erd˝os-R´enyi case. We observe a similar comparison
of the two methods as in the Erd˝os-R´enyi setting.

Finally, in Figures 5(e) and 5(f), we consider power-law graphs that are generated according
to the following version of the Barab´asi-Albert preferential attachment model [BA99]: We start
with two vertices connected by one edge. Then at each step, denoting the degrees of the existing
k vertices by d1, . . . dk, we attach a new vertex to the graph by connecting it to the j-th existing
vertex independently with probability max(Cdj/((cid:80)k
i=1 di), 1) for each j = 1, . . . , k.7 This process
is iterated until the graph has n vertices. Here, C is a parameter that determines the ﬁnal edge
density.

As shown in Figure 5(e), for matching correlated power-law graphs with overall expected edge
density p = 0.01, GRAMPA is more noise resilient than DegreeProfile in terms of exact recovery.
As the noise grows, the performance of GRAMPA decays faster than DegreeProfile in terms of the
fraction of correctly matched pairs. In Figure 5(f), for sparser power-law graphs with expected

7Since a preferential attachment graph is connected by convention, we may repeat this step until the new vertex

is connected to at least one existing vertex.

39

00.10.20.30.40.500.10.20.30.40.50.60.70.80.9100.10.20.30.40.500.10.20.30.40.50.60.70.80.9100.10.20.30.40.500.10.20.30.40.50.60.70.80.9100.10.20.30.40.500.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.700.10.20.30.40.50.60.70.80.9100.10.20.30.40.50.60.700.10.20.30.40.50.60.70.80.91edge density p = 0.005, we again observe that GRAMPA has signiﬁcantly better performance than
DegreeProfile. Note that in this sparse regime, neither method can achieve exact recovery even in
the noiseless case due to the non-trivial symmetry of the graph arising from, for example, multiple
leaf vertices incident to a common parent. We revisit the issue of non-identiﬁability when studying
the real dataset in the next subsection.

4.5 Networks of autonomous systems

We corroborate the improvement of GRAMPA over DegreeProfile using quantitative benchmarks
on a time-evolving real-world network of n = 10000 vertices. Here, for simplicity, we apply both
methods to the unnormalized adjacency matrices, and set η = 1 for GRAMPA. We ﬁnd that the results
are not very sensitive to this choice of η. Although QP-DS yields better performance in Fig. 4, it is
extremely slow to run on such a large network, so we omit it from the comparison here.

We use a subset of the Autonomous Systems dataset from the University of Oregon Route
Views Project [Uni], available as part of the Stanford Network Analysis Project [LK14, LKF05].
The data consists of instances of a network of autonomous systems observed on nine days between
March 31, 2001 and May 26, 2001. Edges and (a small fraction of) vertices of the network were
added and deleted over time. In particular, the number of vertices of the network on the nine days
ranges from 10,670 to 11,174 and the number of edges from 22,002 to 23,409. The labels of the
vertices are known.

To test the graph matching methods, we consider 10,000 vertices of the network that are present
on all nine days. The resulting nine graphs can be seen as noisy versions of each other, with
correlation decaying over time. We apply GRAMPA and DegreeProfile to match each graph to that
on the ﬁrst day of March 31, with vertices randomly permuted.

In Figure 6(a), we plot the fraction of correctly matched pairs of vertices against the chronolog-
ically ordered dates. GRAMPA correctly matches many more pairs of vertices than DegreeProfile
for all nine days. As expected, the performance of both methods degrades over time as the network
becomes less correlated with the original one.

For the same reason as in the power-law graphs in Fig. 5(f), even the matching of the graph on
the ﬁrst day to itself is not exact. In fact, there are over 3,000 degree-one vertices in this graph,
and some of them are attached to the same high-degree vertices, so the exact matching is non-
identiﬁable. Thus for a given matching (cid:98)π, an arguably more relevant ﬁgure of merit is the number
of common edges, i.e. the (rescaled) objective value (cid:104)A, B(cid:98)π(cid:105)/2. We plot this in Figure 6(b) together
with the value for the ground-truth matching. The values of GRAMPA and of the ground-truth
matching are the same on the ﬁrst day, indicating that GRAMPA successfully ﬁnds an automorphism
of this graph, while DegreeProfile fails. Furthermore, GRAMPA consistently recovers a matching
with more common edges over the nine days.

As in many real-world networks, high-degree vertices here are hubs in the network of autonomous
systems and play a more important role. Therefore, we further evaluate the two methods by
comparing their performance on subgraphs induced by high-degree vertices. More precisely, we
consider the 1,000 vertices that have the highest degrees in the graph on the ﬁrst day, March 31.
In Figures 6(c) and 6(d), we still use the matchings between the entire networks that generated
Figures 6(a) and 6(b), but evaluate the correctness only on those top 1,000 high-degree vertices. We
observe that both methods succeed in exactly matching the subgraph from the ﬁrst day to itself,
and in general yield much better matchings for the high-degree vertices than for the remainder of
the graph. Again, GRAMPA produces better results than DegreeProfile on these subgraphs over

40

(a) Fraction of correctly matched vertices

(b) Number of common edges

(c) Fraction of correctly matched vertices, in
high-degree subgraph of 1000 vertices

(d) Number of common edges, in high-degree
subgraph of 1000 vertices

Figure 6: Comparison of GRAMPA and DegreeProfile for matching networks of autonomous systems
on nine days to that on the ﬁrst day

the nine days, in both measures of performance.

5 Conclusion

We have proposed a highly practical spectral method, GRAMPA, for matching a pair of edge-
correlated graphs. By using a similarity matrix that is a weighted combination of outer products
uiv(cid:62)
j across all pairs of eigenvectors of the two adjacency matrices, GRAMPA exhibits signiﬁcantly
improved noise resilience over previous spectral approaches. We showed in this work that GRAMPA
achieves exact recovery of the latent matching in a correlated Gaussian Wigner model, up to a noise
level σ (cid:46) 1
log n . In the companion paper [FMWX19], we establish a similar universal guarantee
for Erd˝os-R´enyi graphs and other correlated Wigner matrices, up to a noise level σ (cid:46)
1
polylog n .
GRAMPA exhibits improved recovery accuracy over previous spectral methods as well as the state-

41

03/3104/0704/1404/2104/2805/0505/1205/1905/260.10.150.20.250.30.350.40.450.50.5503/3104/0704/1404/2104/2805/0505/1205/1905/260.811.21.41.61.822.22.410403/3104/0704/1404/2104/2805/0505/1205/1905/260.40.50.60.70.80.9103/3104/0704/1404/2104/2805/0505/1205/1905/262500300035004000450050005500of-the-art degree proﬁle algorithm in [DMWX18], on a variety of synthetic graphs and also on a
real network example.

The similarity matrix (3) in GRAMPA can be interpreted as a ridge-regularized further re-
laxation of the well-known quadratic relaxation (14) of the QAP, where the doubly-stochastic
constraint is replaced by 1(cid:62)X1 = n. In the companion paper [FMWX19], we also analyze a tighter
relaxation with constraints X1 = 1, and establish similar guarantees. In synthetic experiments on
small graphs, we found that solving the full quadratic program (14), followed by the same rounding
procedure as used in GRAMPA, yields better recovery accuracy in noisy settings. However, unlike
GRAMPA, solving (14) does not scale to large networks, and the properties of its solution currently
lack theoretical understanding. We leave these as open problems for future work.

A Concentration inequalities for Gaussians

We collect auxiliary results on concentration of polynomials of Gaussian variables.

Lemma A.1. Let z be a standard Gaussian vector in Rn. For any ﬁxed v ∈ Rn and δ > 0, it holds
with probability at least 1 − δ that

|v(cid:62)z| ≤ (cid:107)v(cid:107)2

(cid:112)2 log(1/δ).

Lemma A.2 (Hanson-Wright inequality). Let z be a sub-Gaussian vector in Rn, and let M be a
ﬁxed matrix in Cn×n. Then we have with probability at least 1 − δ that

|z(cid:62)M z − Tr M | ≤ C(cid:107)z(cid:107)2
ψ2
≤ 2C(cid:107)z(cid:107)2
ψ2(cid:107)M (cid:107)F log(1/δ),

(cid:107)M (cid:107)F

(cid:16)

(cid:112)log(1/δ) + (cid:107)M (cid:107) log(1/δ)

(cid:17)

(75)

where C is a universal constant and (cid:107)z(cid:107)ψ2 is the sub-Gaussian norm of z.

See [RV13, Section 3.1] for the complex-valued version of the Hanson-Wright inequality. The

following lemma is a direct consequence of (75), by taking M to be a diagonal matrix.

Lemma A.3. Let z be a standard Gaussian vector in Rn. For an entrywise nonnegative vector
v ∈ Rn, it holds with probability at least 1 − δ that

(cid:12)
(cid:12)
(cid:12)

n
(cid:88)

i=1

viz2

i −

n
(cid:88)

i=1

(cid:16)

(cid:12)
(cid:12)
(cid:12) ≤ C

vi

(cid:17)
(cid:112)log(1/δ) + (cid:107)v(cid:107)∞ log(1/δ)

.

(cid:107)v(cid:107)2

In particular, it holds with probability at least 1 − δ that

(cid:12)
(cid:12)(cid:107)z(cid:107)2

2 − n(cid:12)

(cid:12) ≤ C

(cid:16)(cid:112)n log(1/δ) + log(1/δ)

(cid:17)

.

Theorem A.4 (Hypercontractivity concentration [SS12, Theorem 1.9]). Let z be a standard Gaus-
sian vector in Rn, and let f (z1, . . . , zn) be a degree-d polynomial of z. Then it holds that

P

(cid:110)(cid:12)
(cid:12)f (z) − E[f (z)](cid:12)

(cid:12) > t

(cid:111)

≤ e2 exp

(cid:20)

−

(cid:16)

t2
C Var[f (z)]

(cid:17)1/d(cid:21)

,

where Var[f (z)] denotes the variance of f (z) and C > 0 is a universal constant.

42

Finally, the following result gives a concentration inequality in terms of the restricted Lipschitz
constants, obtained from the usual Gaussian concentration of measure plus a Lipschitz extension
argument.

Lemma A.5. Let B ⊂ Rn be an arbitrary measurable subset. Let F : Rn → R such that F is
L-Lipschitz on B. Let X ∼ N (0, In). Then for any t > 0,

P {|F (X) − EF (X)| ≥ t + δ} ≤ exp

(cid:19)

(cid:18)

−

ct2
L2

+ (cid:15),

where c is a universal constant, (cid:15) = P {X /∈ B} and δ = 2(cid:112)(cid:15)(nL2 + F (0)2 + E[F (X)2]).

Proof. Let (cid:101)F : Rn → R be an L-Lipschitz extension of F , e.g., (cid:101)F (x) = inf y∈B F (y) + L(cid:107)x − y(cid:107).
Then by the Gaussian concentration inequality (cf. e.g. [Ver18, Theorem 5.2.2]), we have

P

(cid:111)
(cid:110)
| (cid:101)F (X) − E (cid:101)F (X)| ≥ t

(cid:18)

≤ exp

−

(cid:19)

.

ct2
L2

It remains to show that |EF (X) − E (cid:101)F (X)| ≤ δ. Indeed, by Cauchy-Schwarz, |EF (X) − E (cid:101)F (X)| ≤
E|F (X − (cid:101)F (X)|1{X /∈B} ≤
(cid:15)E[|F (X − (cid:101)F (X)|2]. Finally, noting that | (cid:101)F (X)| ≤ F (0) + L(cid:107)X(cid:107)2 and
E(cid:107)X(cid:107)2

2 = n completes the proof.

(cid:113)

B Kronecker gymnastics

Given A, B ∈ Cn×n, the Kronecker product A ⊗ B ∈ Cn2×n2 is deﬁned as

(cid:34) a11B ... a1nB
...
...

...

an1B ... annB

(cid:35)

. The

n ](cid:62) ∈ Cn⊗n. It is convenient to identify
vectorized form of A = [a1, . . . , an] is vec(A) = [a(cid:62)
[n2] with by [n]2 ordered as {(1, 1), . . . , (1, n), . . . , (n, n)}, in which case we have (A ⊗ B)ij,k(cid:96) =
AikBj(cid:96) and vec(A)ij = Aij.

1 , . . . , a(cid:62)

We collect some identities for Kronecker products and vectorizations of matrices used through-

out this paper:

(cid:104)A ⊗ A, B ⊗ B(cid:105) = (cid:104)A, B(cid:105)2,
(A ⊗ B)(U ⊗ V ) = AU ⊗ BV,

vec(AU B) = (B(cid:62) ⊗ A)vec(U ),

(X ⊗ Y )vec(U ) = vec(Y U X (cid:62)),

(A ⊗ B)(cid:62) = A(cid:62) ⊗ B(cid:62).

The third equality implies that

(cid:104)A ⊗ B, vec(U )vec(V )(cid:62)(cid:105) = (cid:104)vec(U ), (A ⊗ B)vec(V )(cid:105)

= (cid:104)vec(U ), vec(BV A(cid:62))(cid:105) = (cid:104)U, BV A(cid:62)(cid:105) = (cid:104)B, U AV (cid:62)(cid:105)

and hence

vec(U )(cid:62)(A ⊗ B)vec(V ) = (cid:104)A ⊗ B, vec(U )vec(V )(cid:62)(cid:105) = (cid:104)B, U AV (cid:62)(cid:105) = (cid:104)A, U (cid:62)BV (cid:105) = (cid:104)U A, BV (cid:105).

43

Applying the third equality to column vector z and noting that vec(z(cid:62)) = vec(z) = z, we have

In particular, it holds that

(X ⊗ y)z = vec(yz(cid:62)X (cid:62)),
(y ⊗ X)z = vec(Xzy(cid:62)).

(I ⊗ y)z = vec(yz(cid:62)) = z ⊗ y,
(y ⊗ I)z = vec(zy(cid:62)) = y ⊗ z.

C Signal-to-noise heuristics

We justify the choice of the Cauchy weight kernel in (4) by a heuristic signal-to-noise calculation
for (cid:98)X. We assume without loss of generality that π∗ is the identity, so that diagonal entries of (cid:98)X
indicate similarity between matching vertices of A and B. Then for the rounding procedure in (5),
we may interpret n−1 Tr (cid:98)X and (n−2 (cid:80)
ij)1/2 ≈ n−1(cid:107) (cid:98)X(cid:107)F as the average signal strength
and noise level in (cid:98)X. Let us deﬁne a corresponding signal-to-noise ratio as

i,j: i(cid:54)=j (cid:98)X 2

SNR =

E[Tr (cid:98)X]

E[(cid:107) (cid:98)X(cid:107)2

F ]1/2

and compute this quantity in the Gaussian Wigner model.

We abbreviate the spectral weights w(λi, µj) as wij. For (cid:98)X deﬁned by (3) with any weight

kernel w(x, y), we have

Tr (cid:98)X =

(cid:88)

ij

wij · u(cid:62)

i Jvj · u(cid:62)

i vj.

Applying that (A, B) is equal in law to (OAO(cid:62), OBO(cid:62)) for a rotation O such that O1 =
we obtain for every k that

√

nek,

E[Tr (cid:98)X] =

(cid:88)

ij

n · E[wij · u(cid:62)

i (eke(cid:62)

k )vj · u(cid:62)

i vj].

Then averaging over k = 1, . . . , n and applying (cid:80)

k eke(cid:62)

k = I yield that

E[Tr (cid:98)X] =

(cid:88)

ij

E[wij(u(cid:62)

i vj)2].

For the noise, we have

(cid:107) (cid:98)X(cid:107)2

F = Tr (cid:98)X (cid:98)X (cid:62) =

(cid:88)

i,j,k,l

wijwkl(u(cid:62)

i Jvj)(u(cid:62)

k Jv(cid:96)) Tr(uiv(cid:62)

j · v(cid:96)u(cid:62)

k ) =

w2

ij(u(cid:62)

i Jvj)2.

(cid:88)

ij

Applying the equality in law of (A, B) and (OAO(cid:62), OBO(cid:62)) for a uniform random orthogonal
matrix O, and writing r = O1/

n, we get

√

E[(cid:107) (cid:98)X(cid:107)2

F ] =

(cid:88)

ij

n2 · E[w2

ij(u(cid:62)

i r)2(v(cid:62)

j r)2].

44

Here, r = (r1, . . . , rn) is a uniform random vector on the unit sphere, independent of (A, B). For
any deterministic unit vectors u, v with u(cid:62)v = α, we may rotate to u = e1 and v = αe1 +
1 − α2e2
to get

√

E[(u(cid:62)r)2(v(cid:62)r)2] = E[r2

1 · (αr1 +

(cid:112)

1 − α2r2)2] = α2E[r4

1] + (1 − α2)E[r2

1r2

2] =

1 + 2α2
n(n + 2)

,

where the last equality applies an elementary computation. Bounding 1 + 2α2 ∈ [1, 3] and applying
this conditional on (A, B) above, we obtain

E[(cid:107) (cid:98)X(cid:107)2

F ] =

cn
n + 2

(cid:88)

ij

E[w2
ij]

for some value c ∈ [1, 3].

To summarize,

SNR (cid:16)

(cid:80)

E[w(λi, µj)(u(cid:62)

i vj)2]

.

E[w(λi, µj)2]

ij
(cid:113)(cid:80)
ij

The choice of weights which maximizes this SNR would satisfy w(λi, µj) ∝ (u(cid:62)
i vj)2. Recall that
for n−1+ε (cid:28) σ2 (cid:28) n−ε and i, j in the bulk of the spectrum, we have the approximation (11). Thus
this optimal choice of weights takes a Cauchy form, which motivates our choice in (4).

We note that this discussion is only heuristic, and maximizing this deﬁnition of SNR does not
automatically imply any rigorous guarantee for exact recovery of π∗. Our proposal in (4) is a
bit simpler than the optimal choice suggested by (11): The constant C in (11) depends on the
semicircle density near λi, but we do not incorporate this dependence in our deﬁnition. Also, while
(11) depends on the noise level σ, our main result in Theorem 2.1 shows that η need not be set
based on σ, which is usually unknown in practice. Instead, our result shows that the simpler choice
η = c/ log n is suﬃcient for exact recovery of π∗ over a range of noise levels σ (cid:46) η.

Acknowledgement

Y. Wu and J. Xu are deeply indebted to Zongming Ma for many fruitful discussions on the QP
relaxation (14) in the early stage of the project. Y. Wu and J. Xu thank Yuxin Chen for suggesting
the gradient descent dynamics which led to the initial version of the proof. Y. Wu is grateful to
Daniel Sussman for pointing out [LFF+16] and Joel Tropp for [ABK15].

References

[ABK15]

[AD93]

Yonathan Aﬂalo, Alexander Bronstein, and Ron Kimmel. On convex relaxation of
graph isomorphism. Proceedings of the National Academy of Sciences, 112(10):2942–
2947, 2015.

HA Almohamad and Salih O Duﬀuaa. A linear programming approach for the
weighted graph matching problem. IEEE Transactions on pattern analysis and ma-
chine intelligence, 15(5):522–525, 1993.

45

[AGZ10]

[BA99]

[BCL+18]

Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random
matrices, volume 118. Cambridge university press, 2010.

Albert-L´aszl´o Barab´asi and R´eka Albert. Emergence of scaling in random networks.
Science, 286(5439):509–512, 1999.

Boaz Barak, Chi-Ning Chou, Zhixian Lei, Tselil Schramm, and Yueqi Sheng. (Nearly)
eﬃcient algorithms for the graph matching problem on correlated random graphs.
arXiv preprint arXiv:1805.02349, 2018.

[BCPP98] Rainer E Burkard, Eranda Cela, Panos M Pardalos, and Leonidas S Pitsoulis. The
In Handbook of combinatorial optimization, pages

quadratic assignment problem.
1713–1809. Springer, 1998.

[Ben17]

Lucas Benigni. Eigenvectors distribution and quantum unique ergodicity for deformed
Wigner matrices. arXiv preprint arXiv:1711.07103, 2017.

[BGM82]

L´aszl´o Babai, D Yu Grigoryev, and David M Mount. Isomorphism of graphs with
bounded eigenvalue multiplicity. In Proceedings of the fourteenth annual ACM sym-
posium on Theory of computing, pages 310–324. ACM, 1982.

[Bol82]

[BY17]

B´ela Bollob´as. Distinguishing vertices of random graphs. North-Holland Mathematics
Studies, 62:33–49, 1982.

Paul Bourgade and H-T Yau. The eigenvector moment ﬂow and local quantum unique
ergodicity. Communications in Mathematical Physics, 350(1):231–278, 2017.

[CFSV04] Donatello Conte, Pasquale Foggia, Carlo Sansone, and Mario Vento. Thirty years of
graph matching in pattern recognition. International journal of pattern recognition
and artiﬁcial intelligence, 18(03):265–298, 2004.

[Cha14]

Sourav Chatterjee. Superconcentration and related topics, volume 15. Springer, 2014.

[CK16]

Improved achievability and converse bounds
Daniel Cullina and Negar Kiyavash.
for Erd¨os-R´enyi graph matching.
In Proceedings of the 2016 ACM SIGMETRICS
International Conference on Measurement and Modeling of Computer Science, pages
63–72. ACM, 2016.

[CK17]

Daniel Cullina and Negar Kiyavash. Exact alignment recovery for correlated Erd¨os-
R´enyi graphs. arXiv preprint arXiv:1711.06783, 2017.

[CKMP18] Daniel Cullina, Negar Kiyavash, Prateek Mittal, and H Vincent Poor. Partial
arXiv preprint

recovery of Erd˝os-R´enyi graph alignment via k-core alignment.
arXiv:1809.03553, Nov. 2018.

[CP08]

Tomek Czajka and Gopal Pandurangan. Improved random graph isomorphism. Jour-
nal of Discrete Algorithms, 6(1):85–92, 2008.

[DCK19]

Osman Emre Dai, Daniel Cullina, and Negar Kiyavash. Database alignment with
gaussian features. arXiv preprint arXiv:1903.01422, 2019.

46

[DCKG18] Osman Emre Dai, Daniel Cullina, Negar Kiyavash, and Matthias Grossglauser. On
the performance of a canonical labeling for matching correlated Erd˝os-R´enyi graphs.
arXiv preprint arXiv:1804.09758, 2018.

[DML17]

Nadav Dym, Haggai Maron, and Yaron Lipman. DS++: a ﬂexible, scalable and
provably tight relaxation for matching problems. ACM Transactions on Graphics
(TOG), 36(6):184, 2017.

[DMWX18] Jian Ding, Zongming Ma, Yihong Wu, and Jiaming Xu. Eﬃcient random graph
matching via degree proﬁles. arxiv preprint arxiv:1811.07821, Nov 2018.

[ER60]

P. Erd¨os and A. R´enyi. On the evolution of random graphs. Publ. Math. Inst. Hungar.
Acad. Sci, 5:17–61, 1960.

[ESDS16]

Frank Emmert-Streib, Matthias Dehmer, and Yongtang Shi. Fifty years of graph
matching, network alignment and network comparison.
Information Sciences,
346:180–197, 2016.

[FBR87]

Gerd Finke, Rainer E Burkard, and Franz Rendl. Quadratic assignment problems. In
North-Holland Mathematics Studies, volume 132, pages 61–82. Elsevier, 1987.

[FJBd13]

Fajwel Fogel, Rodolphe Jenatton, Francis Bach, and Alexandre d’Aspremont. Convex
relaxations for permutation problems. In Advances in Neural Information Processing
Systems, pages 1016–1024, 2013.

[FMWX19] Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu. Spectral graph matching and
regularized quadratic relaxations II: Erd˝os-R´enyi graphs and universality. preprint,
2019.

[FQM+19]

Soheil Feizi, Gerald Quon, Mariana Mendoza, Muriel Medard, Manolis Kellis, and Ali
Jadbabaie. Spectral alignment of graphs. IEEE Transactions on Network Science and
Engineering, 2019.

[FQRM+16] Soheil Feizi, Gerald Quon, Mariana Recamonde-Mendoza, Muriel M´edard, Mano-
arXiv preprint

Spectral alignment of networks.

lis Kellis, and Ali Jadbabaie.
arXiv:1602.04181, 2016.

[GT11]

[GT13]

[HLL83]

[Kat73]

Friedrich G¨otze and A Tikhomirov. On the rate of convergence to the marchenko–
pastur distribution. arXiv preprint arXiv:1110.1284, 2011.

Friedrich G¨otze and Alexandre Tikhomirov. On the rate of convergence to the semi-
circular law. In High Dimensional Probability VI, pages 139–165, Basel, 2013. Springer
Basel.

P. W. Holland, K. B. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps.
Social Networks, 5(2):109–137, 1983.

Tosio Kato. Continuity of the map s (cid:55)→ |s| for linear operators. Proceedings of the
Japan Academy, 49(3):157–160, 1973.

47

[KG16]

[KHG15]

Ehsan Kazemi and Matthias Grossglauser. On the structure and eﬃcient computation
of isorank node similarities. arXiv preprint arXiv:1602.00668, 2016.

Ehsan Kazemi, S Hamed Hassani, and Matthias Grossglauser. Growing a graph match-
ing from a handful of seeds. Proceedings of the VLDB Endowment, 8(10):1010–1021,
2015.

[KHGM16] Ehsan Kazemi, Hamed Hassani, Matthias Grossglauser, and Hassan Pezeshgi Modar-
res. Proper: global protein interaction network alignment through percolation match-
ing. BMC bioinformatics, 17(1):527, 2016.

[KL14]

[Kuh55]

[LFF+16]

[LFP14]

[LK14]

[LKF05]

[LR13]

[LS18]

[MMS10]

Nitish Korula and Silvio Lattanzi. An eﬃcient reconciliation algorithm for social
networks. Proceedings of the VLDB Endowment, 7(5):377–388, 2014.

Harold W Kuhn. The Hungarian method for the assignment problem. Naval research
logistics quarterly, 2(1-2):83–97, 1955.

Vince Lyzinski, Donniell Fishkind, Marcelo Fiori, Joshua Vogelstein, Carey Priebe,
and Guillermo Sapiro. Graph matching: Relax at your own risk. IEEE Transactions
on Pattern Analysis & Machine Intelligence, 38(1):60–73, 2016.

Vince Lyzinski, Donniell E Fishkind, and Carey E Priebe. Seeded graph matching for
correlated Erd¨os-R´enyi graphs. Journal of Machine Learning Research, 15(1):3513–
3540, 2014.

Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset
collection. http://snap.stanford.edu/data, June 2014.

Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: densiﬁ-
In Proceedings of the
cation laws, shrinking diameters and possible explanations.
eleventh ACM SIGKDD international conference on Knowledge discovery in data min-
ing, pages 177–187. ACM, 2005.

Lorenzo Livi and Antonello Rizzi. The graph matching problem. Pattern Analysis
and Applications, 16(3):253–283, 2013.

Joseph Lubars and R Srikant. Correcting the output of approximate graph matching
algorithms. In IEEE INFOCOM 2018-IEEE Conference on Computer Communica-
tions, pages 1745–1753. IEEE, 2018.

Konstantin Makarychev, Rajsekar Manokaran, and Maxim Sviridenko. Maximum
quadratic assignment problem: Reduction from maximum label cover and LP-based
approximation algorithm. Automata, Languages and Programming, pages 594–604,
2010.

[MX19]

Elchanan Mossel and Jiaming Xu. Seeded graph matching via large neighborhood
statistics. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 1005–1014. SIAM, 2019.

48

[NS08]

[NS09]

[PG11]

[PRW94]

[RV13]

[SGE17]

[SS05]

[SS12]

[SXB08]

[Ume88]

Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse
datasets. In Security and Privacy, 2008. SP 2008. IEEE Symposium on, pages 111–
125. IEEE, 2008.

Arvind Narayanan and Vitaly Shmatikov. De-anonymizing social networks. In Security
and Privacy, 2009 30th IEEE Symposium on, pages 173–187. IEEE, 2009.

Pedram Pedarsani and Matthias Grossglauser. On the privacy of anonymized net-
works. In ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pages 1235–1243, 2011.

Panos M. Pardalos, Franz Rendl, and Henry Wolkowicz. The quadratic assignment
problem: A survey and recent developments. In In Proceedings of the DIMACS Work-
shop on Quadratic Assignment Problems, volume 16 of DIMACS Series in Discrete
Mathematics and Theoretical Computer Science, pages 1–42. American Mathematical
Society, 1994.

Mark Rudelson and Roman Vershynin. Hanson-Wright inequality and sub-Gaussian
concentration. Electron. Commun. Probab., 18:no. 82, 9, 2013.

Farhad Shirani, Siddharth Garg, and Elza Erkip. Seeded graph matching: Eﬃcient
algorithms and theoretical guarantees. In 2017 51st Asilomar Conference on Signals,
Systems, and Computers, pages 253–257. IEEE, 2017.

Christian Schellewald and Christoph Schn¨orr. Probabilistic subgraph matching based
on convex relaxation. In International Workshop on Energy Minimization Methods in
Computer Vision and Pattern Recognition, pages 171–186. Springer, 2005.

Warren Schudy and Maxim Sviridenko. Concentration and moment inequalities for
polynomials of independent random variables.
In Proceedings of the Twenty-Third
Annual ACM-SIAM Symposium on Discrete Algorithms, pages 437–446. ACM, New
York, 2012.

Rohit Singh, Jinbo Xu, and Bonnie Berger. Global alignment of multiple protein
interaction networks with application to functional orthology detection. Proceedings
of the National Academy of Sciences, 105(35):12763–12768, 2008.

Shinji Umeyama. An eigendecomposition approach to weighted graph matching prob-
lems. IEEE transactions on pattern analysis and machine intelligence, 10(5):695–703,
1988.

[Uni]

University of Oregon Route Views Project. Autonomous Systems Peering Networks.
Online data and reports, http://www.routeviews.org/.

[VCL+15]

Joshua T Vogelstein, John M Conroy, Vince Lyzinski, Louis J Podrazik, Steven G
Kratzer, Eric T Harley, Donniell E Fishkind, R Jacob Vogelstein, and Carey E
Priebe. Fast approximate quadratic programming for graph matching. PLOS one,
10(4):e0121002, 2015.

49

[Ver18]

[XK01]

[YG13]

Roman Vershynin. High-Dimensional Probability: An Introduction with Applications
in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cam-
bridge University Press, 2018.

Lei Xu and Irwin King. A PCA approach for fast retrieval of structural patterns in
attributed graphs. IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), 31(5):812–817, 2001.

Lyudmila Yartseva and Matthias Grossglauser. On the performance of percolation
graph matching. In Proceedings of the ﬁrst ACM conference on Online social networks,
pages 119–130. ACM, 2013.

[ZBV08]

Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe Vert. A path following algorithm
for the graph matching problem. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 31(12):2227–2242, 2008.

50

