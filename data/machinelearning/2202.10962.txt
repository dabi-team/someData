ADAPTIVE CUT SELECTION IN MIXED-INTEGER LINEAR
PROGRAMMING

A PREPRINT

Mark Turner∗ †

turner@zib.de

Thorsten Koch∗ †
koch@zib.de

Felipe Serrano‡ †

serrano@zib.de

Michael Winkler§ †
winkler@gurobi.com

July 1, 2022

ABSTRACT

Cut selection is a subroutine used in all modern mixed-integer linear programming solvers with the
goal of selecting a subset of generated cuts that induce optimal solver performance. These solvers
have millions of parameter combinations, and so are excellent candidates for parameter tuning. Cut
selection scoring rules are usually weighted sums of different measurements, where the weights are
parameters. We present a parametric family of mixed-integer linear programs together with inﬁnitely
many family-wide valid cuts. Some of these cuts can induce integer optimal solutions directly after
being applied, while others fail to do so even if an inﬁnite amount are applied. We show for a
speciﬁc cut selection rule, that any ﬁnite grid search of the parameter space will always miss all
parameter values, which select integer optimal inducing cuts in an inﬁnite amount of our problems.
We propose a variation on the design of existing graph convolutional neural networks, adapting
them to learn cut selection rule parameters. We present a reinforcement learning framework for
selecting cuts, and train our design using said framework over MIPLIB 2017. Our framework and
design show that adaptive cut selection does substantially improve performance over a diverse set of
instances, but that ﬁnding a single function describing such a rule is difﬁcult. Code for reproducing
all experiments is available at https://github.com/Opt-Mucca/Adaptive-Cutsel-MILP.

1 Introduction

A Mixed-Integer Linear Program (MILP) is an optimisation problem that is classically deﬁned as:

argmin
{
x

c⊺x

Ax

b,

≤
Rn is the objective coefﬁcient vector, A

|

x

l

≤

Rm

×

u, x

Z|J |

Rn

−|J |

∈

≤
}
n is the constraint matrix, b

×

∈
n are the lower and upper variable bound vectors, and

∈

Here, c
∈
constraint vector, l, u
of indices of integer variables.

∈ {

R,

,
−∞

∞}

(1)

Rm is the right hand side
is the set

1, . . . , n

J ⊆ {

}

2
2
0
2

n
u
J

0
3

]

C
O
.
h
t
a
m

[

2
v
2
6
9
0
1
.
2
0
2
2
:
v
i
X
r
a

One of the main techniques for solving MILPs is the branch-and-cut algorithm, see [1] for an introduction. Generating
cutting planes, abbreviated as cuts, is a major part of this algorithm, and is one of the most powerful techniques
for quickly solving MILPs to optimality, see [2]. A cut is a constraint that does not remove any feasible solutions
of (1) when added to the formulation. We restrict ourselves to linear cuts in this paper, and denote a cut as α =
(α0,

Rn+1, and denote the set of feasible solutions as

, to formally deﬁne a cut in (2).

, αn)

· · ·

∈

IX

n

Xi=1

αixi

α0,

≤

x

∀

∈ IX

, where x = (x1,

, xn)

· · ·

(2)

∗Chair of Software and Algorithms for Discrete Optimization, Institute of Mathematics, Technische Universität Berlin, Straße

des 17. Juni 135, 10623 Berlin, Germany

†Zuse Institute Berlin, Department of Mathematical Optimization, Takustr. 7, 14195 Berlin
‡I2DAMO GmbH, Englerallee 19, 14195 Berlin, Germany
§Gurobi GmbH, Ulmenstr. 37-39, 60325 Frankfurt am Main, Germany

 
 
 
 
 
 
Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

The purpose of cuts is to tighten the linear programming (LP) relaxation of (1), where the LP relaxation is obtained
by removing all integrality requirements. Commonly, cuts are found that separate the current feasible solution to the
LP relaxation, referred to as xLP , from the tightened relaxation, and for this reason are often called separators. This
property is deﬁned as follows:

n

Xi=1

αixLP

i > α0, where xLP = (xLP
1

, . . . , xLP
n )

(3)

Within modern MILP solvers, the cut aspect of the branch-and-cut algorithm is divided into cut generation and cut
selection subproblems. The goal of cut generation is ﬁnding cuts that both tighten the LP relaxation at the current
node and improve overall solver performance. The cut selection subproblem is then concerned with deciding which
of the generated cuts to add to the formulation (1). That is, given the set of generated cuts
, ﬁnd
a subset

′ to add to the formulation (1).

α1,

|S ′|}

, α

′ =

· · ·

S

{

We focus on the cut selection subproblem in this paper, where we motivate the need for instance-dependent cut
selection rules as opposed to ﬁxed rules, and introduce a reinforcement learning (RL) framework for learning
parameters of such a rule. The cut selection subproblem is important, as adding either all or none of the generated
cuts to the LP usually results in poor solver performance. This is due to the large computational burden of solving
larger LPs at each node when all cuts are added, and the large increase in nodes needed to solve MILPs when no
cuts are added. For a summary on MILPs we refer readers to [1], for cutting planes [3], for cut selection [4], and for
reinforcement learning [5].

S ⊆ S

The rest of the paper is organised as follows. In Section 2, we summarise existing literature on learning cut selection.
In Section 3 we motivate the need for adaptive cut selection by showing worst case performance of ﬁxed cut selection
rules. This section was inspired by [6], which proved complexity results for ﬁxed branching rules. In Section 4 we
summarise how cut selection is performed in the MILP solver SCIP [7]. In Section 5 we show how to formulate cut
selection as a Markov decision process, and phrase cut selection as a reinforcement learning problem. This section
was motivated by [8], which presented variable selection as Markov decision process as well as experimental results
of an imitation learning approach. Finally, we present a thorough computational experiment on learning cut selector
parameters that improve root node performance over MIPLIB 2017 [9] in Section 6 using the MILP solver SCIP
version 8.0 [7].

2 Related Work

Several authors have proposed cut selection rules and performed several computational studies. The thesis [1] presents
a linear weighted sum cut selection rule, which drastically reduces solution time to optimality by selecting a reduced
number of good cuts. This cut selection rule and algorithm, see [7], can still be considered the basis of what we
use in this paper. A more in-depth guide to cutting plane management is given in [4]. Here, a large variety of cut
measures are summarised and additional computational results given that show how a reduced subset of good cuts can
drastically improve solution time. A further computational study, focusing on cut selection strategies for zero-half
cuts, is presented in [10]. They hypothesise that generating a large amount of cuts followed by heuristic selection
strategy is more effective than generating a few deep cuts. Note that the solver and cut selection algorithms used in
[1], [4], and [10] are different. More recently, [11] summarises the current state of separators and cut selection in
the literature, and poses questions aimed to better develop the science of cut selection. The ﬁnal remark of the paper
ponders whether machine learning can be used to answer some of the posed questions.

Recently, the intersection of mixed-integer programming and machine learning has received a lot of attention,
speciﬁcally when it comes to branching, see [6, 8, 12] for examples. To the best of our knowledge, however, there are
currently only four publications on the intersection of cut selection and machine learning. Firstly, [13] shows how cut
selection parameter spaces can be partitioned into regions, such that the highest ranking cut is invariant to parameter
changes within the regions. These results are extended to the class of Chvàtal-Gomory cuts applied at the root, with
a sample complexity guarantee of learning cut selection parameters w.r.t. the resultant branch and bound tree size.
Secondly, [14] presents a reinforcement learning approach using evolutionary strategies for ranking Gomory cuts via
neural networks. They show that their method outperforms standard measures, e.g. max violation, and generalises to
larger problem sizes within the same class. Thirdly, [15] train a neural network to rank linear cuts by expected objective
value improvement when applied to a semi-deﬁnite relaxation. Their experiments show that substantial computational
time can be saved when using their approximation, and that the gap after each cut selection round is very similar to
that found when using the true objective value improvement. Most recently, [16] proposes a multiple instance learning
approach for cut selection. They learn a scoring function parameterised as a neural network, which takes as input an
aggregated feature vector over a bag of cuts. Their features are mostly composed of measures normally used to score
cuts, e.g. norm violation. Cross entropy loss is used to train their network by labelling the bags of cuts before training
starts.

2

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Our contribution to the literature is three-fold. First, we provide motivation for instance dependent cut selection by
proving the existence of a family of parametric MILPs together with an inﬁnite amount of family-wide valid cuts.
Some of these cuts can induce integer optimal solutions directly after being applied, while others fail to do so even
if an inﬁnite amount are applied. Using a basic cut selection strategy and a pure cutting plane approach, we show
that any ﬁnite grid search of the cut selector’s parameter space, will miss all parameter values, which select integer
optimal inducing cuts in an inﬁnite amount of our instances. An interactive version of this constructive proof is
provided in Mathematica® [17], and instance creation algorithms are provided using SCIP’s Python API [7, 18].
Second, we introduce a RL framework for learning instance dependent cut selection rules, and present results on
learning parameters to SCIP’s default cut selection rule [1] over the data set MIPLIB 2017 [9]. Third and ﬁnally, we
implemented a new cut selector plugin, which is available in SCIP 8.0 [7], and enables users to include their own cut
selection algorithms in the larger MILP solving process.

3 Motivating Adaptive Cut Selection

We begin this section by introducing a simpliﬁed cut scoring rule and discussing how the parameters for such a score
are traditionally set in solvers. Following this, a series of lemmas and a theorem will be proven using a simulated pure
cutting plane approach.

simple_cut_score(λ, α, c) := λ

Consider the following simpliﬁed version of SCIP’s default cut scoring rule (see Section 4 for the default scoring rule):
(4)
−
Using the general MILP deﬁnition given in (1), we deﬁne the cut measures integer support (isp) and objective
parallelism (obp) as follows:

isp(α) + (1

obp(α, c),

Rn+1, c

[0, 1], α

Rn

λ)

∈

∈

∈

λ

∗

∗

nonzero(αi)
i
∈J
n
i=1 nonzero(αi)

, where nonzero(αi) =

0 if αi = 0
1 otherwise

(cid:26)

isp(α) :=

obp(α, c) :=

P
P

|

n
i=1 αici

n
P
i=1 α2
i

n
i=1 c2
i |

(5)

(6)

We now introduce Theorem 3.1, which refers to the λ parameter in (4).

pP

pP

Theorem 3.1. Given a ﬁnite discretisation of λ, an inﬁnite family of MILP instances together with an inﬁnite amount
of family-wide valid cuts can be constructed. Using a pure cutting plane approach and applying a single cut per
selection round, the inﬁnite family of instances do not solve to optimality for any value in the discretisation, but do
solve to optimality for an inﬁnite amount of alternative λ values.

The general purpose of Theorem 3.1 is to motivate the need for instance dependent parameters in the cut selection
subroutine. The typical approach for ﬁnding the best choice of cut selector parameters, see previous SCIP
computational studies [1, 19, 7], is to perform a parameter sweep, most often a grid search. A grid search, however,
leaves regions unexplored in the parameter space. In our simpliﬁed cut scoring rule (4), we have a single parameter,
namely λ, and these unexplored regions are simply intervals. We deﬁne Λ, the set of values in the ﬁnite grid search of
λ, as follows:

, n

1

,

}

−

N

Λ
|

| ∈

Λ :=

λ1, . . . , λ
|

1,
The unexplored intervals in the parameter space, denoted ˜Λ, is then deﬁned as:
(λ
[0, λ1)
|

λi < λi+1 ≤

, where 0

(λ1, λ2)

˜Λ :=

1, λn)

i
∀

(λn

∈ {

· · ·

|}

≤

{

1

Λ

, 1]
}

Λ
|

−

{

∪

∪ · · · ∪
Our goal is to show that for any Λ, we can construct an inﬁnite family of MILP instances from Theorem 3.1. Together
with our inﬁnite amount of family-wide valid cuts and speciﬁc cut selection rule, we will show that the solving process
˜Λ. In effect, this shows that using the
does not ﬁnitely terminate for any choice of λ outside of an interval (λlb, λub)
same ﬁxed λ value over all problems in a MILP solver, could result in incredibly poor performance for many problems.
This is somewhat expected, as a ﬁxed parameter cannot be expected to perform well on all possible instances, and
moreover, cut selection is only a small subroutine in the much larger MILP solving process. Additionally, the instance
space of MILPs is non-uniform, and good performance over certain problems may be highly desirable as they occur
more frequently in practice.

⊂

∪

3.1 Proof of Theorem
For the following theorem, we will simulate a pure cutting plane approach to solving MILPs using scoring rule (4).
We will use custom MILPs, cutting planes, and select exactly one cut per round. Each call to the selection subroutine
is called an iteration or round. The theorem is intended to show how a ﬁxed cut selection rule can consistently choose
“bad” cuts.

3

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Figure 1: The feasible region of the LP relaxation of P(a, d).

Theorem 3.1. Given a ﬁnite discretisation of λ, an inﬁnite family of MILP instances together with an inﬁnite amount
of family-wide valid cuts can be constructed. Using a pure cutting plane approach and applying a single cut per
selection round, the inﬁnite family of instances do not solve to optimality for any value in the discretisation, but do
solve to optimality for an inﬁnite amount of alternative λ values.

The parametric MILP we use to represent our inﬁnite family of instances is deﬁned as follows, where a
d

[0, 1]:

∈

R

≥

0 and

∈

(10 + d)x2 −
0

ax3

P(a, d) :=

min

s.t.






1

x1 −
2 x2 + 3x3 ≤
−
x3 ≤
0
−
2 x1 + 1
2 x2 −
−
2 x1 + 3
1
2 x3 ≤
Z,
x2 ∈
x1 ∈

1

1
2

0

7

2 x3 ≤
R,

x3 ∈ {

0, 1

}

The polytope of our MILPs LP relaxation is the convex hull of the following points:

:=

X

1
(0, 0, 0), (1, 0, 0), (1, 1, 0), ( −
2

{

, 3,

1
2

)
}

(7)

The convex hull of
region, we can exhaustively write out all integer feasible solutions:

X

is a 3-simplex, or alternatively a tetrahedron, see Figure 1 for a visualisation. For such a feasible

Lemma 3.2. The integer feasible set of P(a, d), (a, d)

R

∈

0 ×
≥
(1, x2, 0) :

[0, 1] is:

x2 ∈
∀

[0, 1]
}

(0, 0, 0)

} ∪ {

{

As we are dealing with linear constraints and objectives, we know that (1, x2, 0), where 0 < x2 < 1 cannot be optimal
without both (1, 1, 0) and (1, 0, 0) being also optimal. We therefore simplify the integer feasible set,

, to:

IX

:=

(0, 0, 0), (1, 0, 0), (1, 1, 0)
}

{

IX

(8)

At each iteration of adding cuts we will always present exactly three candidate cuts. We name these cuts as follows:

• The ‘good cut’, denoted

optimal.

GC

: Applying this cut immediately results in the next LP solution being integer

• The ‘integer support cut’, denoted

n: Applying this cut will result in a new LP solution barely better than
the previous iteration. The cut has very high integer support as the name suggests, and would be selected if λ
from (4) is set to a high value. The superscript n refers to the iteration number.

ISC

• The ‘objective parallelism cut’, denoted

n: Applying this cut will also result in a new LP solution barely
better than the previous iteration. The cut has very high objective parallelism, and would be selected if λ
from (4) is set to a low value. The superscript n refers to the iteration number.

OPC

4

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

GC

The cuts are deﬁned as follows, where

has an additional property of it being selected in the case of a scoring tie:

10x1 + 10x2 + x3 ≤
x1 + x3 ≤
1
x1 + 10x2 ≤
We use ǫn here to denote a small shift of the cut, with a greater ǫn resulting in a deeper cut. We deﬁne ǫn as follows:

ǫn
−
61
2 −

:
GC
n :

OPC

−
−

ISC

n :

ǫn

−

0

(9)
(10)

(11)

i
0 < ǫi < ǫi+1 ∀
ǫn = 0.1
lim
n
−→∞

N

∈

(12)

A better overview of the proof of Theorem 3.1 can now be imagined. At each cut selection round we present three
n is selected. As the scoring rule (4) is
cuts, where for high values of λ,
. Speciﬁcally, for
linear w.r.t. λ, our aim is to controllably sandwich the intermediate values of λ that will select
any given Λ, we aim to construct an inﬁnite amount of parameter values for a and d s.t. the intermediate values of λ
all belong to ˜Λ.

n is selected, and for low values

OPC

ISC

GC

Lemma 3.3. The vertex set of the LP relaxation of P(a, d), (a, d)
cuts (9), (10), or (11) are, respectively:

∈

R

0 ×

≥

[0, 1], after having individually applied

n

ISC

X

:=

IX ∪ {

1
( −
2

+

3ǫn
4

, 3

−

3ǫn
2

,

1
2 −

ǫn
4

1
), ( −
2

+

3ǫn
4

, 3

−

ǫn,

1
2 −

ǫn
4

1
), ( −
2

+

ǫn
2

, 3

−

3ǫn,

1
2 −

n

OPC

X

:=

IX ∪ {

1
( −
2

+

ǫn
21

, 3

−

2ǫn
21

,

1
2 −

ǫn
63

1
), ( −
2

+

3ǫn
43

, 3

−

4ǫn
43

,

1
2 −

ǫn
43

1
), ( −
2

+

ǫn
61

, 3

−

6ǫn
61

,

1
2 −

:=

GCX

IX ∪ {

(

61
91

,

60
91

,

10
91

)
}
(13)

ǫn
2

)
}
(14)

ǫn
61

)
}
(15)

Proof. Apply
LP relaxation.

,

GC

ISC

n, and

OPC

n to P(a, d) individually and then compute the vertices of the convex hull of the

ISC

OPC

n and

After applying a cut of one kind, e.g.
This is because

We note that because both integer support and objective parallelism do not depend on the current LP solution,
iteratively applying deeper cuts of the same kind would leave the cut’s scores unchanged. Thus, provided they do
not separate any integer points and continue to cut off the LP solution, deeper cuts of the same kind can be recursively
applied. This is why both

n have a superscript.
n, we cannot always simply increment n in the other cut, e.g.
n+1 does not guarantee separation of the now new LP solution in problem P(a, d)

n+1.
n for
OPC
\
n+1, which will always entirely
all sequences of
ǫn, ǫn+1}
OPC
n independent of how the series of ǫn values increase. Once again,
remove the facet of the LP created by adding
we note that as only the RHS values are changing, the score for all cuts within the same type remain unchanged,
and thus no two cuts from different types can be applied. We have proven our results using Mathematica [17], and
a complete notebook containing step-by-step instructions can be found at x. Below we will outline the necessary
cumulative lemmas to prove Theorem 3.1, and summarise the calculations we have taken to achieve each step.

. Instead, we create a variant of

OPC
∩ ISC

n+1, namely

OPC

ISC

ISC

{

Lemma 3.4. Having applied the cut

GC
OPC

or a deeper variant of
n+1 only by the RHS value is deﬁned as:
\

OPC

n to P(a, d), (a, d)

R

∈

0 ×

≥

ISC

n+1 cuts off that facet. The deeper variant, denoted

[0, 1], a new facet is created. Applying either
n+1, which differs from

\

OPC

n+1 :

x1 + 10x2 ≤

−

61
2 −

31ǫn

(16)

OPC

Proof. One can ﬁrst verify that the vertex set of the facet is
following cut is valid for all x

:

n
X \ IX

∈ ISC

n
X \ IX

ISC

. One can then ﬁnd the smallest ǫ′ s.t the

x1 + 10x2 ≤

−

61
2 −

ǫ′

5

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

The statement is valid for all ǫ′ > 61ǫn

n by seeing that it separates all vertices of

ISC
solution is cut off. We can verify this by checking that every x
ǫn < 0.1. Therefore ǫ′ = 31ǫn is valid, and we arrive at the cut

ISC

2 , and we arbitrarily select ǫ′ = 31ǫn. One can also check that
∈
∈ IX
\

dominates
N. Finally, we need to ensure that no integer
satisﬁes (16). This statement holds whenever

n
X \ IX

for all n

GC

Lemma 3.5. Having applied the cut
or

cuts off that facet.

GC

n to P(a, d), (a, d)

∈

OPC

[0, 1], a new facet is created. Applying

n+1

ISC

Proof. This follows the same structure as the proof of Lemma 3.4. We get that ǫ′ > 4ǫn
w.r.t. the integer constraints.

43 , and that ǫ′ = ǫn+1 is valid

n+1.

OPC
R

0 ×

≥

Using our deﬁnition of integer support and objective parallelism in (5)-(6), we derive the scores for each cut from the
simple cut selection scoring rule (4). We let cP(a,d) denote the vector of coefﬁcients from the objective of P(a, d),
[0, 1]. The integer support and objective parallelism values of each cut are as follows:
(a, d)

R

∈

0 ×

≥

isp(

GC

isp(

ISC

isp(

) =

2
3
) = 1
1
2

) =

OPC
, cP(a,d)) =

N

n

∀

∈

N

n

∀

∈
110 + a + 10d

obp(

GC

√201

1 + a2 + (10 + d)2

obp(

ISC

,cP(a,d)) =

√2

1 + a

p
1 + a2 + (10 + d)2

N

n

∀

∈

obp(

OPC

,cP(a,d)) =

p
√101

101 + 10d
1 + a2 + (10 + d)2

N

n

∀

∈

(17)

(18)

(19)

(20)

(21)

(22)

p
Using our simpliﬁed cut scoring rule as deﬁned in (4), we derive the necessary conditions deﬁning the λ values, which
assign

a score at least as large as the other cuts.

GC

Lemma 3.6.
used that satisﬁes the following conditions:

is selected and added to P(a, d), (a, d)

GC

λ

λ

∗

isp(

∗
isp(

GC

GC

) + (1

λ)

obp(

−
λ)

∗
obp(

−

∗

GC

) + (1

, cP(a,d))
GC
, cP(a,d))

≥
λ

≥

R

0 ×

≥

[0, 1], using scoring rule (4) if and only if a λ is

isp(

∗
isp(

ISC

OPC

n) + (1
n) + (1

λ)

λ)

obp(
∗
obp(

∗

−

−

ISC

OPC

n, cP(a,d))
n, cP(a,d))

(23)

(24)

∈

λ

∗

Proof. We know that the integer support and objective parallelism do not depend on ǫn as seen in equations (17)-(22).
Our cut selector rule also selects exactly one cut per iteration, namely the largest scoring cut. Therefore, whenever λ
n, and applied to P(a, d). If λ does
satisﬁes constraints (23) and (24),
not satisfy constraints (23) and (24), then

is not the largest scoring cut and will not be applied to P(a, d).

will be selected over both

n and

OPC

ISC

GC

GC

being the best scoring cut. The region,

The inequalities (23)-(24) deﬁne the region,
in
GC
R
(a, d)
ﬁxed (a, d) values.

0 ×

∈

≥

[0, 1], which maps any pairing of (a, d) to the set of λ values contained in

RGC

, which exactly contains all tuples (a, d, λ)

[0, 1]2 that result
0 ×
is visualised in Figure 2. We deﬁne the function r
(a, d) for all
for the corresponding

∈

GC

≥

RGC

R

RGC

r

: R

0 ×

[0, 1]

−→ P

([0, 1])

(25)

≥
refers to the power set. We are interested in

GC

R

P
≥
[0, 1], the values of a

Here
contains all (a, d, λ)
0 ×
d
∈
our value of a
∈
choosing different values of d
between any given ﬁnite discretisation of [0, 1].

∈
0, we aim to generate an inﬁnite amount of λ

as we believe that we can ﬁnd a continuous function that
[0, 1]2 pairings, which score all cuts equally. Using this function, we can ﬁnd for a ﬁxed
R
[0, 1] value that scores all cuts equally. By perturbing
the largest. Then, by
[0, 1] originally, we aim to ﬁnd such an inﬁnite set of λ values that can adaptively lie

RGC
0 that would result in a λ

[0, 1] values that score

GC

R

∈

∈

∈

∈

≥

≥

Lemma 3.7. There exists a closed form symbolic solution for the maximum value of a in terms of d over
denote this maximum value of a as a function over d, namely amax(d), d

[0, 1].

∈

. We

RGC

6

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Figure 2: The region,

, where

GC

RGC

is scored at least as large as both

n and

OPC

ISC

n under cut selection rule (4).

Proof. We verify this by solving the following optimisation problem, where amax(d) is printed in Appendix A:

argmax
a,λ {

a

|

(a, d, λ)

∈ RGC

, a

0, 0

d

≤

≤

≥

1, 0

λ

1

}

≤

≤

Lemma 3.8. Closed form symbolic solutions for the upper and lower bounds of λ can be found. These bounds are
continuous functions deﬁned over 0
amax(d), and we refer to them as λub(a, d) and λlb(a, d)
respectively.

1 and 0

≤

≤

≤

≤

a

d

∈

Proof. We know that the region respects inequalities (23) and (24) and that amax(d) is an upper bound on a for all
[0, 1]. Using this information, we can rearrange the inequalities to get λub(a, d) and λlb(a, d). The result for both
d
λub(a, d) and λlb(a, d) is a ratio of polynomials in terms of the parameters a and d. As the zeros of the denominators
lie outside of the domains 0
amax(d), we can conclude that both λub(a, d) and λlb(a, d) are
continuous and deﬁned over our entire domain. These bounds deﬁne the interval, [λlb(a, d), λub(a, d)], of λ values for
any ﬁxed a and d, which result in
1 and
0

being the largest scoring cut. Taken together with the bounds 0

amax(d) they make up

1 and 0

GC
.

≤

≤

≤

≤

≤

≤

a

a

d

d

≤

≤

RGC

Lemma 3.9. The lower and upper bounds for λ meet at a = amax(d), d
λlb(amax(d), d) for all 0
λ = λlb(amax(d), d)) would score all cuts equally.

1. This means that for all d

≤

≤

∈

d

[0, 1]. That is, λub(amax(d), d) =
[0, 1], λ = λub(amax(d), d) (identically

∈

Proof. This can be checked by substituting amax(d) into the equations of λub(amax(d), d) and λlb(amax(d), d), equating
both sides and rearranging. The result is that λub(amax(d), d) = λlb(amax(d), d).

Lemma 3.10. λub(amax(d), d) (identically: λlb(amax(d), d)), where 0
different valued end points.

d

≤

≤

1, is a continuous function and has

Proof. We know from Lemma 3.8 that λub(a, d) is continuous, and can conclude that λub(amax(d), d) is continuous.
The different valued endpoints can be derived by evaluating λub(amax(0), 0) and λub(amax(1), 1), which have the
relation λub(amax(1), 1) > λub(amax(0), 0).

Figure 3 visualises the function λub(amax(d), d) (identically λlb(amax(d), d)) for 0
1. For any d
functions alongside slight changes to amax(d), will be used to generate intervals of λ values, which score
and lie between a ﬁnite discretisation of [0, 1].

≤

≤

d

[0, 1], these
the largest

∈
GC

7

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

λ

0.520

0.518

0.516

0.514

0.512

0.510

0.2

0.4

0.6

0.8

d

1.0

Figure 3: Plot of λ values that scores all cuts equally for all d
λlb(amax(d), d))

∈

[0, 1]. That is, λub(amax(d), d), d

[0, 1] (identically:

∈

Lemma 3.11. λub(a, d) - λlb(a, d) > 0 for all 0
at which λub(a, d) = λlb(a, d) for 0

d
≤
amax(d).

a

≤

≤

1 and 0

≤

≤

a < amax(d). That is, a = amax(d) is the only time

d

Proof. We can verify this by setting the constraints (23)-(24) to hard equalities, and then solving over the domain
1. As
0
λub(0, d) > λlb(0, d), for all d
[0, 1], and both λub(a, d) and λlb(a, d) are continuous functions from Lemma 3.8,
we can conclude that λub(a, d) - λlb(a, d) > 0 for all 0

amax(d). Solving such a system gives the unique solution a = amax(d) for 0

a < amax(d).

1 and 0

1 and 0

≤

≤

≤

≤

≤

≤

∈

a

d

d

≤

≤

≤

Lemma 3.12. An interval [λlb(a′, d), λub(a′, d)]
for all 0

a′ < amax(d).

1 and 0

d

≤

≤

≤

r

GC

⊆

(a′, d) can be constructed, where λub(a′, d)

λlb(a′, d) > 0

−

Proof. We deﬁne following function,

amax(d, ˆǫ), representing amax(d) with a shift of ˆǫ:

amax(d, ˆǫ) := amax(d)

d

−

ˆǫ, where 0

d

≤

≤

1,

0 < ˆǫ

amax(d)

≤

(26)

We know from Lemma 3.11 that a = amax(d) is the only time at which λlb(a, d) = λub(a, d) for any d
also know that λub(a, d) and λlb(a, d) are deﬁned over all 0
holds for any d

[0, 1]. We
amax(d). Therefore the following

[0, 1] and ˆǫ

(0, amax(d)]:

1 and 0

d

≤

≤

≤

≤

∈

a

d

∈

∈

λub(

amax(d, ˆǫ), d)

−

λlb(

amax(d, ˆǫ), d) > 0

from the inequalities (23) - (24), we know that the following interval is
d

d

RGC

Additionally, by the deﬁnition of
connected:
I(

amax(d, ˆǫ), d) := [λlb(

amax(d, ˆǫ), d), λub(

amax(d, ˆǫ), d)], where 0

We therefore can construct a connected non-empty interval I(a′, d)
d
d
amax(d, ˆǫ) and 0

ˆǫ < amax(d).

d

≤

r

GC

⊆

d

1,

0 < ˆǫ

≤

≤
(a′, d) for all d

amax(d)

≤
[0, 1], where a′ =

∈

d
While we have shown the necessary methods to construct an interval of λ values, I(a, d), that result in
being
selected, we have yet to guarantee that at all stages of the solving process, the desired LP optimal solution is taken
amax(d). Speciﬁcally, we need to show that the originally optimal point is always
for all 0
n) a
1
2 , 3, 1
the integer solution (1, 1, 0) is optimal, and that after applying
( −
fractional solution from

N, is optimal.

≤
GC
(or

1 and 0

) for all n

n (or

OPC

ISC

a
≤
2 ), that after applying
n
ISC

GC

≤

≤

∈

d

X

n

OPC
X
1
Lemma 3.13. The fractional solution ( −

2 , 3, 1

2 ) is LP optimal for P(a, d) for all 0

d

≤

≤

1 and 0

a

≤

≤

amax(d).

Proof. This can be done done by substituting all points from
the objective is strictly less when evaluated at ( −

1

2 , 3, 1

1

X \

2 , 3, 1

( −
2 ). This shows that for all 0
1
( −
2

∈ X \ {

′ ∀

x=x

x′

≤

, 3,

≤
1
2

)
}

2 ) into the objective, and then showing that

d

1 and 0

a

≤

≤

amax(d) :

P(a, d)
(cid:12)
(cid:12)

x=( −

1

2 ,3, 1

2 ) < P(a, d)
(cid:12)
(cid:12)

Lemma 3.14. The integer solution (1, 1, 0) is LP optimal after applying
0

amax(d).

a

≤

≤

to P(a, d) for all 0

GC

d

≤

≤

1 and

8

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Proof. This can be done in an identical fashion to Lemma 3.13. That is, we show that for all 0
0

amax(d) :

a

d

≤

≤

1 and

≤

≤

P(a, d)
(cid:12)
(cid:12)

x=(1,1,0) < P(a, d)
(cid:12)
(cid:12)

x=x

x′

′ ∀

∈ GCX \ {

(1, 1, 0)
}

Lemma 3.15. Having applied the cut
0

amax(d).

a

≤

≤

n to P(a, d), a point from

n
X \IX

ISC

is LP optimal for all 0

d

≤

≤

1 and

ISC

Proof. This can be done by showing that for any choice of a
from
ISC
a
0
≤

n
X \ IX
amax(d):

≤

at which the objective is strictly less than at all integer points

[0, amax(d)] and d

IX

∈

[0, 1], there is at least one point
1 and

. Speciﬁcally, for all 0

∈

d

≤

≤

x′
∃

∈ ISC

n
X \ IX

s.t P(a, d)
(cid:12)
(cid:12)

x=x

′

< P(a, d)
(cid:12)
(cid:12)

x=x

x′′

′′ ∀

∈ IX

Lemma 3.16. Having applied the cut
and 0

amax(d).

a

≤

≤

n to P(a, d), a point from

n
X \IX

OPC

is LP optimal for all 0

d

1

≤

≤

OPC

Proof. This proof follows the same logic as that of Lemma 3.15.

We can now prove Theorem 3.1 using the Lemmas 3.2 - 3.16 that we have built up throughout this paper.

Theorem 3.1. Given a ﬁnite discretisation of λ, an inﬁnite family of MILP instances together with an inﬁnite amount
of family-wide valid cuts can be constructed. Using a pure cutting plane approach and applying a single cut per
selection round, the inﬁnite family of instances do not solve to optimality for any value in the discretisation, but do
solve to optimality for an inﬁnite amount of alternative λ values.

d

≤

≤

1 and 0

Proof. From Lemmas 3.2 - 3.5, we know the exact vertex set of our feasible region at each stage of the solving process,
as well as the exact set of cuts at each round. Furthermore, as at each round only the RHS value for each proposed
cut changes, the scoring of the cuts at each new round remains constant, and we can therefore completely describe the
be the set containing all cuts added during the solution process to
three scenarios of how cuts would be added. Let
an instance P(a, d), where 0
amax(d):
a

≤
n :
{ISC
,
{GC}
{OPC
From Lemma 3.6 we know the sufﬁcient conditions for a λ value that results in
the other cuts. Lemmas 3.7 - 3.11 show how these sufﬁcient conditions can be used to construct the region
Moreover, they show that
is bounded, and that a = amax(d), for all d
following occurs:

if λ > λub(a, d)
if λlb(a, d)
λ
if λ < λlb(a, d)

being scored at least as well as
GC
.
RGC
[0, 1], is the only time at which the

:= 


λub(a, d)

RGC

(27)

S
≤

n :



≤

≤

N

N

∈

∈

∈

S

n

n

∀

∀

}

}

,

,

We therefore conclude that
λlb(amax(d), d) are continuous, where d
value theorem, we then know the following:

RGC

∈

is connected. We know from Lemma 3.10 that both λub(amax(d), d) and
[0, 1], and that λub(amax(1), 1) > λub(amax(0), 0). From the intermediate

λlb(a, d) = λub(a, d)

d

∀

∈

[0, 1]

λ

∀

∈

[λub(amax(0), 0), λub(amax(1), 1)],

d′ s.t λ = λub(amax(d′), d′)

(28)

∃

From Lemma 3.12 we have shown an explicit way to construct an interval I(a, d)
[0, 1]. We can therefore construct the following intervals:
[0, amax(d))
I(

amax(d′, ˆǫ), d′)], where 0

amax(d′, ˆǫ), d′) = [λlb(

amax(d′, ˆǫ), d′), λub(

×

d′

r

GC

⊆

(a, d) for all (a, d)

∈

1,

0 < ˆǫ

≤

amax(d)

(29)

≤

≤

These intervals can be arbitrarily small as ˆǫ can be arbitrarily small. Moreover, as d′ values that satisfy (28) can be
d
amax(d, ˆǫ) are polynomials, we can generate inﬁnitely many disjoint intervals. We
used, and λlb(a, d), λub(a, d), and

d

d

d

9

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

can therefore conclude that for any ﬁnite discretisation of λ, Λ, an interval can be created that contains no values from
λ1, . . . , λ
Λ
|
I(

, but contains all values of λ for which P(a, d) solves to optimality.

|}
amax(d′, ˆǫ), d′)

, where 0 < ˆǫ

amax(d′)

(λ1, λ2)

[0, λ1)

1, λn)

(30)

(λn

{

(λ
|

Λ
|

, 1]
}

∪

≤

⊂ {

∪

∪ · · · ∪

−

Finally, Lemmas 3.13 - 3.16 ensure that each stage of the solving process, all cuts are valid for any fractional feasible
LP optimal solution for all P(a, d), where 0
amax(d). Moreover, the Lemmas guarantee that
only after applying

1 and 0
is an integer optimal solution found.

d

≤

≤

≤

≤

a

d

We therefore have shown how ﬁxing a global value of λ to a constant for use in the MILP solving process while
disregarding all instance information can result in inﬁnitely worse performance for inﬁnitely many instances.

GC

Corollary 3.1.1. There exists an inﬁnite family of MILP instances together with an inﬁnite amount of family-wide
valid cuts, which do not solve to integer optimality for any λ when using a pure cutting plane approach and applying
a single cut per selection round.

Proof. To show this we take the following function, where 0

d

≤

≤

1:

˜amax(d, ˜ǫ) := amax(d) + ˜ǫ,

0 < ˜ǫ

0.1

≤

Any such value of a retrieved from this function will lie outside of
λ value that results in ﬁnite termination, as

RGC

is never scored at least as high as the other cuts.

d

≤

≤

for all 0

1. There thus would exist no

Similar to the proof of Theorem 3.1, we need to ensure that the LP optimal point at all times during the solving process
is appropriate, and that the same integer optimal point stays integer optimal for all 0
˜amax(d, ˜ǫ). We therefore redo the proofs of Lemmas 3.13 - 3.16 but change the range of values of a.

1 and amax(d) < a

≤

≤

≤

d

GC

4 Cut Selection in SCIP

Until now we have motivated adaptive cut selection in a theoretical manner, by simulating poor performance of ﬁxed
cut selector rules in a pure cutting approach. Using this motivation, we now present results of how parameters of a cut
selection scoring rule can be learnt, and made to adapt with the input instance. We begin with an introduction to cut
selection in SCIP [7].

The ofﬁcial SCIP cut scoring rule (31) that has been used since SCIP 6.0 and is used for our experiments in Section 6,
is deﬁned as:
cut_score(λ, α, c, xLP , ˆx) := λ1 ∗

(31)

dcd(α, xLP , ˆx) + λ3 ∗
1, 2, 3, 4
i
∀

∈ {

obp(α, c)
isp(α) + λ4 ∗
, λ = [λ1, λ2, λ3, λ4]
}

eff(α, xLP ) + λ2 ∗
λ1 + λ2 + λ3 + λ4 = 1, λi

≥

0

The measures integer support (isp) and objective parallelism (obp) are deﬁned in (5) and (6). Using the general MILP
deﬁnition (1), letting xLP be the LP optimal solution of the current relaxation, and ˆx be the current best incumbent
solution, we deﬁne the cut measures efﬁcacy (eff) and directed cutoff distance (dcd) as follows:

dcd(α, xLP , ˆx) :=

P

eff(α, xLP ) :=

n
i=1 αixLP
i −
n
i=1 αiyi
|

|

P

α0

, where

α0

n
i=1 αixLP
α2

i −
1 + ... + α2
n
xLP
xLP

P
p
y =

ˆx
ˆx
||

−
−

||

(32)

(33)

We note that in SCIP the cut selector does not control how many times it itself is called, which candidate cuts are
provided, nor the maximum amount of cuts we can apply each round. We reiterate that each call to the selection
subroutine is called an iteration or round. Algorithm 1 gives an outline of the SCIP cut selection rule.

The SCIP cut selector rule in Algorithm 1 still follows the major principles presented in [1]. Cuts are greedily added
by the largest score according to the scoring rule (31). After a cut is added, all other candidate cuts that are deemed
too parallel to the added cut are ﬁltered out and can no longer be added to the formulation this round. Forced cuts,
which are always added to the formulation, preﬁlter all candidate cuts for parallelism, and are most commonly one-
dimensional cuts or user deﬁned cuts. We note that Algorithm 1 is a summarised version of the true algorithm, and
has abstracted some procedures.

Motivated by work from this paper, users can now deﬁne their own cut selection algorithms and include them in SCIP
since SCIP 8.0 [7]. We hope that this leads to additional research about machine learning cut selection algorithms in
modern MILP solvers.

10

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Algorithm 1: SCIP Default Cut Selector (Summarised)

Input : cuts
Return: Sorted array of selected cuts, the amount of cuts selected

n, forced_cuts

n, max_cuts

∈

∈

∈

≥

×

×

Rs2

Rs1

Z

0, (s1, s2)

Z2
≥

0

∈

←

cuts, n_cuts

s1 // Size of cuts array

1 n_cuts
2 for forced_cut in forced_cuts do
3
4 end
5 n_selected_cuts
6 selected_cuts
7 while n_cuts > 0 and max_cuts > n_selected_cuts do

←
← ∅

←

0

remove cuts from cuts too parallel to forced_cut

If no primal, efficacy replaces cutoff distance

8

9

10

select highest scoring cut remaining in cuts

// Scoring done with (31).
best_cut
selected_cuts
←
n_selected_cuts
cuts, n_cuts

selected_cuts

←

∪

n_selected_cuts + 1

best_cut

←
remove cuts from cuts too parallel to best_cut

11
12 end
13 return forced_cuts

←

selected_cuts, s2 + n_selected_cuts

∪

min
x

c1x1 +

· · · · · ·

+ cnxn

a1,1x1 +

· · · · · ·

+ a1,nxn

b1

≤

an,1x1 +

· · · · · ·

+ an,nxn

bm

≤

x1

xn

cons1

consm

Figure 4: A visualisation of the variable-constraint bipartite graph construction from a MILP.

5 Problem Representation and Solution Architecture

We now present our approach for learning cut selector parameters for MILPs. In Subsection 5.1 we describe our
encoding of a general MILP instance into a bipartire graph. Subsection 5.2 introduces a framework for posing cut
selection parameter choices as a RL problem, with Subsection 5.3 describing the GCNN architecture used as our
policy network. Subsection 5.4 outlines the training method to update our policy network.

5.1 Problem representation as a graph

The current standard for deep learning representation of a general MILP instance is the constraint-variable bipartite
graph as described in [8]. Some extensions to this design have been proposed, see [20], as well as alternative non graph
embeddings, see [9] and [21]. We use the embedding as introduced in [8] and the accompanying graph convolutional
neural network (GCNN) design, albeit with the removal of all LP solution speciﬁc features and a different interpretation
of the output. The construction process for the bipartite graph can be seen in Figure 4.

The bipartite graph representation can be written as G =
representations of MILP instances. V
correspond one-to-one with the variables (columns) in the MILP. C
other side, and correspond one-to-one with the constraints (rows) in the MILP. An edge (i, j)
variable represented by xi has non-zero coefﬁcient in constraint consj, where i
abuse notation slightly and say that E

is the set of all bipartite graph
7 is the feature matrix of nodes on one side of the graph, which
2 is the feature matrix of nodes on the
E exists when the
. We
1, . . . , m
1, where E is the edge feature tensor. Note that this representation

} ∈ G
Rm

1, . . . , n

, where

∈
and j

∈ {

∈ {

Rm

Rn

∈

∈

G

{

}

}

×

×

n

×

×

V, C, E

∈

11

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Tensor

V

C

E

Features
Normalised objective coefﬁcient
Normalised lower bound
|
Type: binary
Absolute objective parallelism (cosine similarity)
Normalised RHS per constraint
Type: linear
knapsack
Normalised coefﬁcients per constraint

upper bound

continuous

logicor

integer

setppc

|

|

|

|

|

|

|

implicit integer

varbound

2

Value Range
[-1, 1]
2, [

1, 1], 2

}

−

{−
one-hot encoding
[0,1]
[-1, 1]
one-hot encoding
[-1, 1]

Table 1: Feature descriptions of variable (column) feature matrix V, constraint (row) feature matrix C, and edge
feature tensor E.

would be extended once cuts are added as they then become constraints. The exact set of features can be seen in Table
1

5.2 Reinforcement Learning Framework
We formulate our problem as a single step Markov decision process. The initial state of our environment is s0 = G0 =
R, and deterministically transitions to
R4, resulting in an instant reward r(s0, a0)
G. An agent takes an action a0 ∈
Z. The action taken, a0, is dictated by a policy πθ(a0|
a terminal state s1 = GNr , Nr
s0) that maps any initial state
∈
to a distribution over our action space, i.e. a0 ∼
The MILP solver in this framework is our environment, and the cut selector our agent. Let Nr be the number of paired
separation and cut selection rounds we wish to apply, and Gi
after
i rounds have been applied. The action a0 ∈
followed by
Nr paired separation rounds. Applying action a0 to state s0 results in a deterministic transition to s1 = GNr , deﬁned
by the function f :

R4 is the choice of cut selector parameters

be the bipartite graph representation of G

λ1, λ2, λ3, λ4}

s0).

∈ G

∈ G

πθ(

R4

∈

·|

{

.

G −→

R, maps an initial state s0 to the gap of the LP solution of f(s0, a′)

The baseline function, b(s0) :
, where
R4, and some pre-loaded primal solution. The gap in this
the solver is run with standard cut selector parameters, a′
∈
experiment can be thought of as a normalised dual bound, as the pre-loaded primal cannot be improved upon without
a provable optimal solution itself. The pre-loaded primal also serves to make directed cutoff distance active from the
beginning of the solving process. We do note that this is different to the normal solve process and introduces some
bias. Let ga0 (s0) be the gap of the LP solution of f(s0, a0) if a0 are the cut selector parameter values used. The reward
r(s0, a0) can then be deﬁned as:

∈ G

G ×

−→ G

r(s0, a0) :=

b(s0)
−
b(s0)
|
|

ga0 (s0)
8
+ 10−

Let (s0, a0, s1)
is to maximise the expected reward over all trajectories. That is, we want to ﬁnd θ that parameterises:

be a trajectory, also called a roll out in the literature. The goal of reinforcement learning

∈ G ×

× G

R4

argmax
θ

E

(s0,a0,s1)

[r(s0, a0, s1)] = argmax

θ

Zs1

πθ

∼

Here, p(s0) is the density function on instances s
deﬁned as:

∈ G

Z(s0,a0)

f

−

1(s1)

p(s0)πθ(a0|

∈

∈G
evaluated at s = s0. The pre-image f−

s0)r(s0, a0) ds0da0ds1

(34)

1(s1) :

R4

is

× G

G −→

f−

1(
G

′) :=

{

(s0, a0)

∈ G ×

R4

|

f(s0, a0)

,

′

}

∈ G

′

G

⊆ G

We note that equation (34) varies from the standard deﬁnition as seen in [5], and those presented in similar research
[8, 14], as our action space is continuous. Additionally, as the set
is inﬁnite and we do not know the density function
p(s), we use sample average approximation, creating a uniform distribution around MIPLIB 2017 [9].

G

5.3 Policy Architecture
Our policy network, πθ(
), is parameterised as a graph convolutional neural network, and follows the general
design as in [8], where θ fully describes the complete set of weights and biases in the GCNN. The changes in
design are that we use 32 dimensional convolutions instead of 64 due to our lower dimensional input, and output
a 4 dimensional vector as we are interested in cut selector parameters. This technique of using the constraint-variable
graph as an embedding for graph neural networks has gained recent popularity, see [22] for an overview of applications
in combinatorial optimisation.

s0 ∈ G
·|

12

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

7

H1
V
32

×

n

H2
V
32

×

n

H3
V
4

n

×

µ
4

n

V

×
E
n
C

m

1

×

×

2

m

H1
C
32
×
Figure 5: The architecture of policy network πθ(a0|

H2
C
32

m

m

×

×

s0). H represent hidden layers of the network.

Our policy network takes as input the constraint-variable bipartite graph representation s0 =
. Two
staggered half-convolutions are then applied, with messages being passed from the embedding V to C and then
back. The result is a bipartite graph with the same topology but new feature matrices. Our policy is then obtained by
R4 represents
normalising feature values over all nodes and averaging the result into a vector µ
R. We note that having our policy network
the mean of a multivariate normal distribution,
N4(µ, γI), where γ
only output the mean was a design choice to simplify the learning process, and that our design can be extended to
also output γ or additional distribution information. Any sample from the distribution
N4(µ, γI) can be considered
with the non-negativity constraints relaxed. Figure 5 provides an
an action a0 ∈
λ1, λ2, λ3, λ4}
overview of this architecture.

R4, which represent

R4. This vector µ

∈

∈

∈

}

{

{

V, C, E

5.4 Training Method

To train our GCNN we use policy gradient methods, speciﬁcally the REINFORCE algorithm with baseline and
gaussian exploration, see [5] for an overview. An outline of the algorithm is given in Algorithm 2.

Algorithm 2: Batch REINFORCE

Input: Policy network πθ, MILP instances batch, nsamples

0

1
2 for s0 in batch do
3

πθ(

s0) // Note that πθ(

L ←
µ
·|
←
for i in
1, . . . , nsamples
{
sample
a0 ←
Apply Nr rounds of separation and cut selection to s0
s1 ←
Relative dual bound improvement of s1 to some baseline
r
←
L ← L

do
}
N4(µ, γI)

s0) is technically

log(πθ(a0|

+ (

4

5

6

7

8

×

−

·|

r

N4(µ, γI)

N, Nr

N

∈

∈

end

9
10 end
11 θ

θ +

θ
∇

L

// We use the Adam update rule in practice [23]

←

s0))) // Use log probability for numeric stability

Algorithm 2 is used to update the weights and biases, θ, of our GCNN, πθ(
instances by minimising
L
update rule, aside from a learning rate with value 5

). It does this for a batch of
, referred to as the loss function, see [24]. We used default parameter settings in the Adam

s0 ∈ G
·|

4.

10−

×

6 Experiments

We use MIPLIB 20175 [9] as our data set, and from now will simply refer to it as MIPLIB. For subsections 6.1 -
6.4, we run experiments on presolved instances. Each individual run on a presolved instance consists of a single
round of presolve (to remove ﬁxed variables), then solving the root node, using 50 separation rounds with a limit of
10 cuts per round, and a pre-loaded primal that is the best solution found within 600s when solved under standard
conditions. Propagation and heuristics for these runs are disabled, and after applying SCIP’s default cut selector
in Algorithm 1, the highest scoring cuts that were ﬁltered for parallelism are added until the 10 cuts per round
limit is reached, or no more cuts exist. We believe these conditions best represent a sandbox environment, which
allows cut selection to be the largest inﬂuence on solver performance. Additionally, all results are obtained by
. All code for reproducing experiments can be found at
averaging results over the SCIP random seeds
https://github.com/Opt-Mucca/Adaptive-Cutsel-MILP.

1, 2, 3

{

}

5MIPLIB 2017 – The Mixed Integer Programming Library https://miplib.zib.de/.

13

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Criteria
Tags: feasibility, numerics, infeasible, no solution
Unbounded objective, MIPLIB solution unavailable
Presolve longer than 300s under default conditions
No feasible solution found in 600s under default conditions
Solved to optimality at root
Too few cuts applied (< 250)
Root solve longer than 20s

% of instances removed
4.5%, 17.5%, 2.8%, 0.9%
0.9%, 2.6%
3.6%
10.9%
12.8%
7.1%
18.8%

Table 2: Percentage of instances removed from MIPLIB

For all experiments SCIP 8.0 [7] is used, with PySCIPOpt [18] as the API, and Gurobi 9.5.1 [25] as the LP solver.
PyTorch 1.7.0 [26] and PyTorch-Geometric 2.0.1 [27] are used to model the GCNN. All experiments are run on a
cluster equipped with Intel Xeon E5-2670 v2 CPUs with 2.50GHz and 64GB main memory.

For instance selection we discard instances from MIPLIB that satisfy any of the criteria in Table 2. We believe that
these conditions focus on instances where a good selection strategy of cuts can improve the dual bound in a reasonable
amount of time. We note that improving the dual bound is a proxy for overall solver performance, and does not
necessarily result in improved solution time.

6.1 Lower bounding potential improvement

To begin our experiments, we ﬁrst perform a grid search to give a lower bound on the potential improvement that
adaptive cut selection can provide. Given our training subset of MIPLIB, we generate all parameter scenarios satisfying
the following condition:

4

Xi=1

λi = 1, where λi =

βi
10

,

N,

βi

∈

i
∀

∈ {

1, 2, 3, 4

}

Recall that λi for all i
(dcd), efﬁcacy (eff), integer support (isp), and objective parallelism (obp).

1, 2, 3, 4

∈ {

}

are respectively multipliers of the cut scoring measures directed cutoff distance

s
e
c
n
a
t
s
n

i

f

o
n
o

i
t
c
a
r
F

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Median relative GAP improvement = 0.0958
Mean relative GAP improvement = 0.1866

0.0

0.2

0.4

0.6

0.8

1.0

Maximum relative GAP improvement (best parameters per instance in grid search)

Figure 6: Relative gap improvement from best choice parameters in Experiment 6.1.

14

 
 
Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

We solve the root node for all instances and parameter choices, and store the cut selector parameters that result in
the smallest gap, as well as their relative gap improvement compared to the default cut selector parameter values.
We remove all instances where the worst case parameter choice compared to the best case parameter choice differ
by a relative gap performance of less than 0.1%. Additionally, we remove instances where a quarter or more of the
parameter choices result in the identical best gap performance. These removals are made due to the sparse learning
opportunities provided by the instances, as the best case performance is minimally different from the worst, or the best
case performance is too common. This results in an additional 5.6% and 0.2% of instances being removed, leaving
125 (11.7%) instances remaining.

We conclude from the results presented in Figure 6 that there exists notable amounts of improvement potential per
instance from better cut selection rules. Speciﬁcally, we observe that the median instance can have a relative gap
improvement of 9.6% when only considering at most 500 cuts (50 rounds of 10 cuts), with this potential improvement
being only a lower bound as the results come from a grid search of the parameter space. Instance speciﬁc results are
available in Appendix B.

We draw attention to the interesting results on λ1 (multiplier of dcd) in Table 3. These results summarise that directed
cutoff distance is on average less important than the other measures in the context of cut scoring rule (31), which is
also reﬂected in the default settings of SCIP 8.0. These are aggregated results, however, and we note that there exist
instances where directed cutoff distance improves performance provided primal solutions of a reasonable quality exist.

Parameter Mean Median
0.100
0.179
λ1 (dcd)
0.200
0.241
λ2 (eff)
0.200
0.270
λ3 (isp)
0.300
0.310
λ4 (obp)

Std Deviation
0.216
0.242
0.248
0.260

Table 3: Statistics of best choice parameters per instance in Experiment 6.1.

6.2 Random Seed Initialisation
Let θi be the initialised weights and biases using random seed i, where i
policy with respect to λ = (λ1, λ2, λ3, λ4), the random seed that satisﬁes (35) is used throughout our experiments.

N. To minimise the bias of our initialised

∈

argmin
0,...,999

i
∈{

} Xs0

E[πθi (
·|

s0)]

k

−

[

1
4

,

1
4

,

1
4

,

1
4

]
k1

(35)

Median  elative GAP imp ovement = 0.005
Mean  elative GAP imp ovement = 0.0413

s
e
c
n
a
t
s
n

i

f
o
n
o
i
t
c
a

F

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

−0.9

−0.7

−0.5

−0.3

−0.1

0.1

0.3

0.5

0.7

0.9

Relative GAP imp ovement ( andomly initialised pa amete s pe  instance)

Figure 7: Relative gap improvement from random initialised parameters in Experiment 6.2.

15

 
 
 
Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

The performance of the randomly initialised GCNN can be seen in Figure 7 and Table 4, with instance speciﬁc results
available in Appendix B.

Parameter Mean Median
0.241
0.243
λ1 (dcd)
0.154
0.161
λ2 (eff)
0.283
0.279
λ3 (isp)
0.320
0.317
λ4 (obp)

Std Deviation
0.020
0.047
0.013
0.019

Table 4: Statistics of random initialised parameters per instance in Experiment 6.2.

6.3 Learning Capabilities
Before we attempt to generalise our result, we ﬁrst verify that our RL framework, policy architecture, and training
method, are capable of learning good cut selector parameters. To do so, we run 500 iterations of Algorithm 2 on each
N4(µ, γI), is deﬁned by the
instance individually, with nsamples set to 20. γ of the multivariate normal distribution,
following, where nepochs is the total amount of iterations of Algorithm 2 and iepoch is the current iteration:

γ(iepoch, nepochs) := 0.01

−

0.009

iepoch

∗
nepochs

,

iepoch, nepochs

N,

iepoch

∈

nepochs

≤

(36)

We note that γ represents one of many opportunities, such as the GCNN structural design and training algorithm,
where a substantial amount of additional effort could be invested to (over)tune the learning experiment. After training
GCNNs individually for each instance, we observe a median relative gap improvement of 4.18% in Figure 8 compared
to the 0.5% improvement of the random initialisation in Figure 7. Despite this performance by overﬁtting, the grid
search in Subsection 6.1 produced considerably better results. Nevertheless, we conclude from this experiment that
our RL framework, policy architecture, and training method are capable of learning good cut selector parameters,
albeit not globally optimal parameters. Instance speciﬁc results are available in Appendix B.

s
e
c
n
a
t
s
n

i

f

o

n
o

i
t
c
a
r
F

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Median relative GAP im rovement = 0.0418
Mean relative GAP im rovement = 0.0895

−0.9

−0.7

−0.5

−0.3

−0.1

0.1

0.3

0.5

0.7

0.9

Relative GAP im rovement (generated  arameters)

Figure 8: Relative gap improvement from generated parameters in Experiment 6.3.

Parameter Mean Median
0.172
0.214
λ1 (dcd)
0.147
0.219
λ2 (eff)
0.277
0.266
λ3 (isp)
0.314
0.301
λ4 (obp)

Std Deviation
0.137
0.239
0.158
0.171

Table 5: Statistics of generated parameters per instance in Experiment 6.3.

16

 
 
Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

6.4 Generalisation Capabilities

We now show the performance of our RL framework over MIPLIB itself instead of over individual instances. To do
so, we run 10000 iterations of Algorithm 2 (500 epochs), with nsamples set to 20, γ deﬁned as in (36), and allocate 5%
of remaining MIPLIB instances per batch.

The randomly initialised GCNN had a median relative gap improvement of 0.5% as seen in Figure 7 compared to
the 1.75% of our MIPLIB trained GCNN as seen in Figure 9. From this, and Experiment 6.3, which reveals a learnt
improvement potential of 4.2%, we conclude that our trained GCNN only partially generalised individual instance
performance onto MIPLIB as a whole.

As in Experiments 6.1 and 6.3, all cut selection parameters did not trend to 0 for the average instance. We believe
that this shows that all parameters are situationally useful depending on the instance. For speciﬁc instance results, see
Appendix B.

s
e
c
n
a
t
s
n

i

f

o
n
o

i
t
c
a
r
F

1.0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Median relative GAP im rovement = 0.0175
Mean relative GAP im rovement = 0.0604

−0.9

−0.7

−0.5

−0.3

−0.1

0.1

0.3

0.5

0.7

0.9

Relative GAP im rovement (generated  arameters)

Figure 9: Relative gap improvement from generated parameters in Experiment 6.4

Parameter Mean Median
0.286
0.294
λ1 (dcd)
0.120
0.232
λ2 (eff)
0.279
0.257
λ3 (isp)
0.238
0.216
λ4 (obp)

Std Deviation
0.122
0.274
0.088
0.146

Table 6: Statistics of generated parameters per instance in Experiment 6.4

7 Conclusion

We presented a parametric family of MILPs together with inﬁnitely many family-wide valid cuts. We showed for a
speciﬁc cut selection rule, that any ﬁnite grid search of the parameter space will always miss all parameter values,
which select integer optimal inducing cuts in an inﬁnite amount of our instances. We then presented a reinforcement
learning framework for learning cut selection parameters, and phrased cut selection in MILP as a Markov decision
process. By representing MILP instances as a bipartite graph, we used policy gradient methods to train a graph
convolutional neural network. The framework generates good performing, albeit sub-optimal, parameter values for
SCIP’s default cut scoring rule when trained on individual instances. Our design exhibits some generalisation loss
when extended to MIPLIB 2017 as a whole, however, it clearly outperforms the random initialisation, showing the
effectiveness of our design at learning instance dependent cut selector parameter values.

17

 
 
Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Results from our grid search experiments showed that there is a large amount of potential improvements to be made in
adaptive cut selection, with a median relative gap improvement of 9.6% with only 50 rounds of 10 cuts. We note that
this improvement does not necessarily translate to solution time.

A major contribution of this work is the new cut selector plugin for SCIP, which enables future research via easy
inclusion of custom cut selection algorithms in a modern MILP solver.

Acknowledgements

The work for this article has been conducted in the Research Campus MODAL funded by the German Federal Ministry
of Education and Research (BMBF) (fund numbers 05M14ZAM, 05M20ZBM). The described research activities are
funded by the Federal Ministry for Economic Affairs and Energy within the project UNSEEN (ID: 03EI1004-C).

References

[1] Tobias Achterberg. Constraint integer programming. PhD thesis, TU Berlin, 2007.
[2] Tobias Achterberg and Roland Wunderling. Mixed integer programming: Analyzing 12 years of progress. In

Facets of combinatorial optimization, pages 449–481. Springer, 2013.

[3] Hugues Marchand, Alexander Martin, Robert Weismantel, and Laurence Wolsey. Cutting planes in integer and

mixed integer programming. Discrete Applied Mathematics, 123(1-3):397–446, 2002.

[4] Franz Wesselmann and U Stuhl. Implementing cutting plane management and selection techniques. Technical

report, Technical report, University of Paderborn, 2012.

[5] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
[6] Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In International

conference on machine learning, pages 344–353. PMLR, 2018.

[7] Ksenia Bestuzheva, Mathieu Besançon, Wei-Kun Chen, Antonia Chmiela, Tim Donkiewicz, Jasper van
Doornmalen, Leon Eiﬂer, Oliver Gaul, Gerald Gamrath, Ambros Gleixner, Leona Gottwald, Christoph Graczyk,
Katrin Halbig, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Thorsten Koch, Marco Lübbecke,
Stephen J. Maher, Frederic Matter, Erik Mühmer, Benjamin Müller, Marc E. Pfetsch, Daniel Rehfeldt, Steffan
Schlein, Franziska Schlösser, Felipe Serrano, Yuji Shinano, Boro Sofranac, Mark Turner, Stefan Vigerske, Fabian
Wegscheider, Philipp Wellner, Dieter Weninger, and Jakob Witzig. The scip optimization suite 8.0, 2021.

[8] Maxime Gasse, Didier Chételat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial

optimization with graph convolutional neural networks. arXiv preprint arXiv:1906.01629, 2019.

[9] Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe, Timo Berthold,
Philipp Christophel, Kati Jarck, Thorsten Koch, Jeff Linderoth, et al. Miplib 2017: data-driven compilation
of the 6th mixed-integer programming library. Mathematical Programming Computation, pages 1–48, 2021.

[10] Giuseppe Andreello, Alberto Caprara, and Matteo Fischetti. Embedding

0, 1/2

-cuts in a branch-and-cut

framework: A computational study. INFORMS Journal on Computing, 19(2):229–238, 2007.

{

}

[11] Santanu S Dey and Marco Molinaro. Theoretical challenges towards cutting-plane selection. Mathematical

Programming, 170(1):237–266, 2018.

[12] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid von Glehn, Pawel Lichocki, Ivan Lobov, Brendan
O’Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja, Pengming Wang, et al. Solving mixed integer
programs using neural networks. arXiv preprint arXiv:2012.13349, 2020.

[13] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Sample complexity of tree
search conﬁguration: Cutting planes and beyond. Advances in Neural Information Processing Systems, 34,
2021.

[14] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer programming: Learning to

cut. In International Conference on Machine Learning, pages 9367–9376. PMLR, 2020.

[15] Radu Baltean-Lugojan, Pierre Bonami, Ruth Misener, and Andrea Tramontani. Scoring positive semideﬁnite
cutting planes for quadratic optimization via trained neural networks. optimization-online preprint 2018/11/6943,
2019.

[16] Zeren Huang, Kerong Wang, Furui Liu, Hui-ling Zhen, Weinan Zhang, Mingxuan Yuan, Jianye Hao, Yong
arXiv preprint

Learning to select cuts for efﬁcient mixed-integer programming.

Yu, and Jun Wang.
arXiv:2105.13645, 2021.

[17] Wolfram Research, Inc. Mathematica, Version 12.2. Champaign, IL, 2020.

18

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

[18] Stephen Maher, Matthias Miltenberger, Joao Pedro Pedroso, Daniel Rehfeldt, Robert Schwarz, and Felipe
Serrano. Pyscipopt: Mathematical programming in python with the scip optimization suite. In International
Congress on Mathematical Software, pages 301–307. Springer, 2016.

[19] Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eiﬂer, Maxime Gasse, Patrick
Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten
Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Miltenberger, Erik Mühmer, Benjamin
Müller, Marc E. Pfetsch, Franziska Schlösser, Felipe Serrano, Yuji Shinano, Christine Tawﬁk, Stefan Vigerske,
Fabian Wegscheider, Dieter Weninger, and Jakob Witzig. The SCIP Optimization Suite 7.0. ZIB-Report 20-10,
Zuse Institute Berlin, March 2020.

[20] Jian-Ya Ding, Chao Zhang, Lei Shen, Shengyin Li, Bing Wang, Yinghui Xu, and Le Song. Accelerating
primal solution ﬁndings for mixed integer programs based on solution prediction. In Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, volume 34, pages 1452–1459, 2020.

[21] Zachary Steever, Chase Murray, Junsong Yuan, Mark Karwan, and Marco Lübbecke. An image-based approach

to detecting structural similarity among mixed integer programs. Available at SSRN 3437981, 2020.

[22] Quentin Cappart, Didier Chételat, Elias Khalil, Andrea Lodi, Christopher Morris, and Petar Veliˇckovi´c.
Combinatorial optimization and reasoning with graph neural networks. arXiv preprint arXiv:2102.09544, 2021.
arXiv preprint

[23] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization.

arXiv:1412.6980, 2014.

[24] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
[25] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021.
[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary
DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

[27] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR Workshop

on Representation Learning on Graphs and Manifolds, 2019.

19

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

A Functions of Section 3

amax(d) := −

2680√101d+2020√201d

6767√2

27068√101+22220√201

−

6767√2

202√201

−

−

λlb(a, d)1 :=√20301√(a2+d(d+20)+101)
λlb(a, d)2 :=101a2+a(−
λlb(a, d)3 :=20d(−
λlb(a, d)4 :=

606a2+12a(10(√20301

−
151)d

10(√20301

20(√20301

−

−

101)d

−
λlb(a, d)5 :=120d(10(√20301

−

λub(a, d)6 :=5555a2+24a(10(√20301

−

110))

−

−

202(√20301
211√20301+31411)−
101)d+101(√20301

110))

−

−

22220√20301+3272501

151)d+211√20301

31411)+606(220√20301

32401)

−

−
101)d+101(√20301

110))

−

λlb(a, d)7 :=d((2400√20301

355633)d+50640√20301

7403300)+505(528√20301

76409)

−

−

−

λlb(a,d)1√λlb(a,d)2+λlb(a,d)3+λlb(a,d)4+λlb(a,d)5
λlb(a,d)6+λlb(a,d)7

λlb(a, d) :=

λub(a, d)1 :=

2
(cid:0)
−(a2+d(d+20)+101)

(cid:1)

λub(a, d)2 :=(2√402

λub(a, d)3 :=(6√402

λub(a, d)4 :=(6√402

−

−

−

203)a2+a(20(√402

−
609)a2+6a(10(√402

2)d+222√402

−
2)d+111√402

−

475)a2+6a(10(√402

−

2)d+111√402

842)+20d(−
421)+60d(−
421)+2d(−

−

−

10d+√402

−
10d+√402

−
233d+30√402

220)+220√402

−
220)+660√402

24401

73203

−

5260)+660√402

−

59669

−

λub(a, d) :=

√402√λub(a,d)1λub(a,d)2+λub(a,d)3
λub(a,d)4

B Per Instance Results and Statistics of Section 6

Variable Information
Num. vars
Num. bin. vars
Num. int. vars
Num. impl. int. vars
Num. cont. vars
Constraint Information
Num. cons
Num. linear cons
Num. logicor cons
Num. knapsack cons
Num. setppc cons
Num. varbound cons

Range
[63, 104000]
[0, 52640],
[0, 17528]
[0, 258]
[0, 103680]
Range
[20, 75466]
[0, 32428]
[0, 17323]
[0, 27920]
[0, 18631]
[0, 6241]

2218
584
0
0
393

Mean Median
5256.44
2338.88
287.67
2.01
2627.81
Mean Median
3493.51
1879.02
244.14
248.45
474.19
647.72

1537
346
0
0
0
21

Table 7: Instance statistics for Experiments 6.1, 6.2, 6.3, and 6.4

20

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Instance
22433
23588
50v-10
a2c1s1
app1-1
app3
b1c1s1
b2c1s1
beasleyC1
beasleyC2
berlin
bg512142
bienst1
bienst2
binkar10_1
bppc8-02
bppc8-09
brasil
cost266-UUE
danoint
dg012142
drayage-100-12
drayage-100-23
drayage-25-23
drayage-25-27
dws008-01
eil33-2
exp-1-500-5-5
f2gap201600
f2gap401600
f2gap801600
fhnw-schedule-paira100
g200x740
gmu-35-40
graphdraw-domain
graphdraw-gemcutter
h50x2450
h80x6320d
ic97_tension
icir97_tension
k16x240b
lectsched-5-obj
lotsize
mik-250-20-75-2
mik-250-20-75-3
mik-250-20-75-4
mik-250-20-75-5
milo-v12-6-r2-40-1
milo-v13-4-3d-4-0
misc07
mkc
mkc1
mtest4ma
n2seq36q
n3700
n3707
n370b
n5-3
n7-3
n9-3
neos-1337307
neos-1423785

GAP
0.1
0.01
0.04
0.24
0.39
0.16
0.32
0.25
0.17
0.11
0.93
0.01
0.28
0.22
0.16
0.92
0.02
0.94
0.01
0.0
0.01
0.4
0.4
0.46
0.01
0.07
0.0
0.5
0.03
0.15
0.01
0.01
0.27
0.01
0.04
0.04
0.1
0.13
0.22
0.23
0.1
0.07
0.25
0.11
0.07
0.09
0.04
0.05
0.06
0.02
0.3
0.64
0.89
0.33
0.01
0.01
0.01
0.14
0.14
0.09
0.01
0.3

λ1
0.0
0.0
0.1
0.0
0.0
0.2
0.0
0.0
0.3
0.3
0.1
0.4
0.0
0.0
0.2
0.1
0.1
0.2
0.0
0.0
0.0
0.1
0.2
0.1
0.0
0.0
0.6
0.1
0.0
0.9
0.1
0.6
0.3
0.1
0.3
0.1
0.1
0.4
0.0
0.6
0.0
0.0
0.1
0.1
0.9
0.0
0.0
0.0
0.0
0.1
0.4
0.2
0.1
0.3
0.2
0.6
0.1
0.2
0.2
0.0
0.0
0.1

λ2
0.0
0.5
0.0
0.0
0.4
0.2
0.3
0.3
0.3
0.0
0.4
0.1
0.2
0.0
0.1
0.1
0.4
0.0
0.2
0.6
0.1
0.4
0.4
0.0
0.4
0.8
0.1
0.0
0.3
0.1
0.7
0.1
0.6
0.2
0.0
0.0
0.0
0.0
0.0
0.1
0.3
0.1
0.0
0.5
0.0
0.1
1.0
0.3
0.0
0.5
0.1
0.0
0.1
0.2
0.1
0.0
0.1
0.3
0.0
0.4
0.7
0.1

λ3
1.0
0.2
0.1
0.9
0.0
0.5
0.3
0.2
0.0
0.7
0.3
0.0
0.8
0.7
0.2
0.3
0.5
0.3
0.7
0.2
0.0
0.2
0.1
0.1
0.3
0.1
0.1
0.6
0.7
0.0
0.1
0.1
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.0
0.4
0.0
0.7
0.4
0.1
0.2
0.0
0.7
0.1
0.3
0.0
0.3
0.3
0.4
0.5
0.0
0.4
0.2
0.1
0.1
0.1
0.0

λ4
0.0
0.3
0.8
0.1
0.6
0.1
0.4
0.5
0.4
0.0
0.2
0.5
0.0
0.3
0.5
0.5
0.0
0.5
0.1
0.2
0.9
0.3
0.3
0.8
0.3
0.1
0.2
0.3
0.0
0.0
0.1
0.2
0.0
0.4
0.2
0.2
0.0
0.5
0.7
0.3
0.3
0.9
0.2
0.0
0.0
0.7
0.0
0.0
0.9
0.1
0.5
0.5
0.5
0.1
0.2
0.4
0.4
0.3
0.7
0.5
0.2
0.8

#BP
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
4
1
1
1
1
1
1
1
1
1
1
1
15
1
2
1
1
1
1
1
1
1
1
1
1
2
6
1
1
1
1
1
1
1
1
1
1
5
1
1
1
1
1
1
1
1

Instance
neos-1456979
neos-2978205-isar
neos-3009394-lami
neos-3046601-motu
neos-3046615-murg
neos-3581454-haast
neos-3594536-henty
neos-3627168-kasai
neos-4333464-siret
neos-4333596-skien
neos-4343293-stony
neos-4387871-tavua
neos-4650160-yukon
neos-4724674-aorere
neos-4736745-arroux
neos-4738912-atrato
neos-4954672-berkel
neos-5076235-embley
neos-5079731-ﬂyers
neos-5093327-huahum
neos-5102383-irwell
neos-5107597-kakapo
neos-5140963-mincio
neos-5260764-orauea
neos-5261882-treska
neos-631517
neos-691058
neos-860300
neos16
neos17
neos5
newdano
nexp-150-20-1-5
nexp-150-20-8-5
ns2071214
p200x1188c
p500x2988
pg
pg5_34
physiciansched5-3
probportfolio
prod2
qnet1
r4l4-02-tree-bounds-50
r50x360
rocII-5-11
roll3000
set3-09
set3-10
set3-16
set3-20
seymour1
shipsched
sp150x300d
supportcase20
swath
tanglegram6
timtab1
timtab1CUTS
tokyometro
tr12-30
uct-subprob
usAbbrv-8-25_70

GAP
0.2
0.04
0.0
0.03
0.02
0.02
0.05
0.05
0.14
0.01
0.05
0.06
0.1
0.0
0.07
0.74
0.21
0.14
0.14
0.05
0.14
0.56
0.01
0.01
0.2
0.13
0.56
0.11
0.0
0.29
0.01
0.17
0.25
0.27
1.0
0.02
0.2
0.31
0.2
0.09
0.06
0.02
0.54
0.0
0.16
0.0
0.74
0.03
0.05
0.02
0.05
0.05
0.34
0.81
0.4
0.0
0.08
0.37
0.03
0.02
0.72
0.06
0.02

−

λ1
0.1
0.1
0.0
0.0
0.0
0.1
0.1
0.1
0.1
0.0
0.1
0.3
0.0
0.0
0.0
0.0
0.1
0.0
0.1
0.0
0.1
0.0
0.7
0.6
0.0
0.1
0.0
0.1
0.1
0.1
0.1
0.3
0.2
0.0
0.2
0.1
0.3
0.1
0.8
0.1
0.6
0.2
0.1
0.0
0.4
0.1
0.0
0.0
0.5
0.5
0.1
0.1
0.0
0.0
0.6
0.0
0.0
0.0
0.2
0.2
0.3
0.2
0.1

λ2
0.2
0.0
0.0
0.8
0.7
0.3
0.0
0.0
0.9
0.3
0.7
0.5
0.1
0.0
0.5
0.0
0.1
0.0
0.7
0.3
0.5
0.3
0.1
0.1
0.1
0.3
0.4
0.2
0.3
0.0
0.0
0.2
0.0
0.0
0.8
0.5
0.0
0.0
0.0
0.5
0.3
0.3
0.7
0.1
0.1
0.3
0.8
0.3
0.0
0.1
0.3
0.2
0.1
0.0
0.1
0.5
0.2
0.4
0.6
0.5
0.0
0.5
0.1

λ3
0.2
0.8
0.2
0.1
0.3
0.0
0.0
0.2
0.0
0.4
0.2
0.2
0.0
0.0
0.2
0.9
0.0
0.0
0.2
0.0
0.1
0.0
0.1
0.0
0.5
0.3
0.1
0.2
0.2
0.7
0.1
0.3
0.7
1.0
0.0
0.0
0.5
0.7
0.2
0.2
0.0
0.2
0.2
0.6
0.2
0.5
0.0
0.7
0.4
0.4
0.2
0.4
0.7
0.2
0.2
0.4
0.3
0.4
0.1
0.1
0.1
0.2
0.2

λ4
0.5
0.1
0.8
0.1
0.0
0.6
0.9
0.7
0.0
0.3
0.0
0.0
0.9
1.0
0.3
0.1
0.8
1.0
0.0
0.7
0.3
0.7
0.1
0.3
0.4
0.3
0.5
0.5
0.4
0.2
0.8
0.2
0.1
0.0
0.0
0.4
0.2
0.2
0.0
0.2
0.1
0.3
0.0
0.3
0.3
0.1
0.2
0.0
0.1
0.0
0.4
0.3
0.2
0.8
0.1
0.1
0.5
0.2
0.1
0.2
0.6
0.1
0.6

#BP
1
8
2
1
1
1
45
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
5
1
1
1
1
1
8
1
1
1
2
1
1
1
3
1
1
1
1
1
4
1
1
1
1
1
1
1
1
1
1
1
1
1
1

Table 8: Per instance results of Experiment 6.1 (Grid search). GAP refers to the relative gap improvement.
are the multipliers for dcd, eff, isp, and obp. #BP refers to the number of best parameter
λ1, λ2, λ3, λ4}
{
combinations. In the case of #BP > 1, a single best choice
λ1, λ2, λ3, λ4}

is provided.

{

21

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

λ3

λ2

λ4

λ1
0.23 0.13 0.29 0.35
0.24 0.12 0.29 0.36
0.26 0.1
0.29 0.34
0.23 0.19 0.28 0.3
0.22 0.2
0.27 0.3
0.24 0.19 0.28 0.3
0.23 0.18 0.28 0.3
0.23 0.18 0.28 0.3
0.23 0.17 0.28 0.31
0.24 0.16 0.28 0.32
0.24 0.16 0.29 0.32
0.24 0.17 0.28 0.31
0.24 0.18 0.28 0.3
0.24 0.18 0.28 0.31
0.24 0.18 0.28 0.3
0.26 0.09 0.29 0.35
0.27 0.08 0.3
0.35
0.24 0.15 0.29 0.32
0.24 0.18 0.28 0.3
0.23 0.19 0.28 0.3
0.23 0.18 0.27 0.31
0.24 0.15 0.29 0.32
0.24 0.15 0.29 0.32
0.24 0.15 0.29 0.32
0.24 0.15 0.29 0.32
0.25 0.14 0.29 0.33
0.21 0.27 0.25 0.28
0.25 0.14 0.29 0.32
0.22 0.26 0.23 0.29
0.22 0.26 0.23 0.29
0.22 0.26 0.23 0.29
0.28 0.08 0.29 0.35
0.24 0.14 0.29 0.33
0.24 0.14 0.29 0.33
0.27 0.11 0.28 0.34
0.27 0.12 0.28 0.34
0.25 0.13 0.29 0.33
0.24 0.15 0.29 0.32
0.26 0.13 0.29 0.32
0.28 0.1
0.29 0.33
0.24 0.14 0.29 0.33
0.28 0.09 0.29 0.35
0.26 0.13 0.29 0.32
0.24 0.15 0.28 0.33
0.24 0.16 0.28 0.33
0.24 0.15 0.28 0.33
0.24 0.15 0.28 0.33
0.24 0.16 0.28 0.32
0.24 0.17 0.28 0.31
0.19 0.28 0.24 0.29
0.23 0.14 0.28 0.35
0.24 0.16 0.28 0.32
0.25 0.13 0.29 0.33
0.19 0.26 0.26 0.29
0.25 0.14 0.29 0.33
0.25 0.14 0.29 0.33
0.25 0.14 0.29 0.33
0.23 0.19 0.28 0.29
0.23 0.19 0.28 0.29
0.23 0.2
0.28 0.29
0.19 0.23 0.28 0.31
0.24 0.18 0.28 0.3

−

−

−

−
−
−

GAP
Instance
0.09
neos-1456979
0.01
neos-2978205-isar
0.0
neos-3009394-lami
0.01
neos-3046601-motu
0.0
neos-3046615-murg
0.0
neos-3581454-haast
0.0
neos-3594536-henty
0.05
neos-3627168-kasai
0.08
neos-4333464-siret
0.01
neos-4333596-skien
0.0
neos-4343293-stony
0.0
neos-4387871-tavua
0.05
neos-4650160-yukon
0.01
neos-4724674-aorere
0.05
neos-4736745-arroux
0.58
neos-4738912-atrato
0.14
neos-4954672-berkel
0.01
neos-5076235-embley
0.01
neos-5079731-ﬂyers
neos-5093327-huahum 0.01
0.03
neos-5102383-irwell
1.0
neos-5107597-kakapo
0.0
neos-5140963-mincio
0.0
neos-5260764-orauea
0.05
neos-5261882-treska
0.11
neos-631517
0.28
neos-691058
0.0
neos-860300
0.0
neos16
0.07
neos17
0.03
neos5
0.1
newdano
0.2
nexp-150-20-1-5
0.15
nexp-150-20-8-5
0.0
ns2071214
0.01
p200x1188c
0.05
p500x2988
0.12
pg
0.02
pg5_34
0.01
physiciansched5-3
0.05
probportfolio
0.0
prod2
0.27
qnet1
0.0
r4l4-02-tree-bounds-50
0.11
r50x360
0.0
rocII-5-11
0.22
roll3000
0.0
set3-09
0.03
set3-10
0.0
set3-16
0.04
set3-20
0.01
seymour1
0.3
shipsched
0.48
sp150x300d
0.26
supportcase20
0.03
swath
0.02
tanglegram6
0.14
timtab1
0.01
timtab1CUTS
0.02
tokyometro
0.66
tr12-30
0.02
uct-subprob
0.0
usAbbrv-8-25_70

−
−

−
−

−
−

−

−

−

λ2

λ3

λ4

0.28 0.3
0.28 0.3
0.27 0.3
0.28 0.3

λ1
0.27 0.09 0.29 0.35
0.28 0.29
0.23 0.2
0.28 0.1
0.29 0.33
0.26 0.14 0.28 0.32
0.26 0.14 0.28 0.32
0.27 0.11 0.29 0.34
0.24 0.15 0.28 0.33
0.25 0.14 0.29 0.32
0.24 0.18 0.28 0.3
0.25 0.15 0.28 0.32
0.24 0.18 0.28 0.3
0.24 0.16 0.28 0.31
0.24 0.18 0.28 0.31
0.25 0.11 0.29 0.35
0.28 0.12 0.28 0.33
0.27 0.12 0.28 0.33
0.25 0.15 0.28 0.31
0.23 0.2
0.23 0.2
0.23 0.2
0.23 0.2
0.28 0.08 0.29 0.35
0.24 0.2
0.27 0.29
0.22 0.22 0.26 0.3
0.26 0.11 0.29 0.34
0.28 0.09 0.29 0.34
0.26 0.13 0.29 0.32
0.24 0.28
0.18 0.3
0.27 0.1
0.29 0.34
0.24 0.17 0.28 0.31
0.22 0.27 0.23 0.28
0.24 0.17 0.28 0.31
0.23 0.17 0.28 0.32
0.26 0.11 0.29 0.34
0.27 0.1
0.29 0.34
0.24 0.15 0.29 0.32
0.24 0.15 0.29 0.33
0.25 0.16 0.28 0.3
0.23 0.19 0.28 0.3
0.26 0.12 0.29 0.33
0.23 0.17 0.27 0.32
0.22 0.25 0.24 0.29
0.26 0.1
0.29 0.35
0.27 0.12 0.28 0.32
0.24 0.14 0.29 0.33
0.24 0.19 0.27 0.3
0.26 0.13 0.28 0.33
0.23 0.19 0.28 0.3
0.23 0.19 0.28 0.3
0.23 0.19 0.28 0.3
0.23 0.19 0.28 0.3
0.23 0.24 0.26 0.27
0.27 0.11 0.29 0.34
0.24 0.14 0.29 0.33
0.25 0.13 0.29 0.33
0.22 0.22 0.26 0.3
0.18 0.26 0.26 0.3
0.29 0.33
0.28 0.1
0.25 0.17 0.28 0.31
0.27 0.11 0.28 0.34
0.26 0.14 0.29 0.32
0.2
0.27 0.25 0.28
0.25 0.11 0.29 0.35

Instance
22433
23588
50v-10
a2c1s1
app1-1
app3
b1c1s1
b2c1s1
beasleyC1
beasleyC2
berlin
bg512142
bienst1
bienst2
binkar10_1
bppc8-02
bppc8-09
brasil
cost266-UUE
danoint
dg012142
drayage-100-12
drayage-100-23
drayage-25-23
drayage-25-27
dws008-01
eil33-2
exp-1-500-5-5
f2gap201600
f2gap401600
f2gap801600
fhnw-schedule-paira100
g200x740
gmu-35-40
graphdraw-domain
graphdraw-gemcutter
h50x2450
h80x6320d
ic97_tension
icir97_tension
k16x240b
lectsched-5-obj
lotsize
mik-250-20-75-2
mik-250-20-75-3
mik-250-20-75-4
mik-250-20-75-5
milo-v12-6-r2-40-1
milo-v13-4-3d-4-0
misc07
mkc
mkc1
mtest4ma
n2seq36q
n3700
n3707
n370b
n5-3
n7-3
n9-3
neos-1337307
neos-1423785

−
−

−

−

−
−

−
−

−
−
−

−

−

−
−
−

GAP
0.0
0.01
0.05
0.11
0.1
0.05
0.18
0.09
0.01
0.03
0.93
0.0
0.12
0.03
0.07
0.59
0.02
0.93
0.0
0.0
0.0
0.23
0.36
0.27
0.11
0.04
0.0
0.36
0.01
0.03
0.46
0.0
0.1
0.01
0.03
0.01
0.06
0.03
0.11
0.18
0.13
0.0
0.23
0.06
0.02
0.03
0.09
0.01
0.01
0.0
0.22
0.07
0.85
0.0
0.01
0.01
0.01
0.05
0.04
0.04
0.0
0.06

−

−

Table 9: Per instance results of Experiment 6.2 (Random seed). GAP refers to the relative gap improvement.
λ1, λ2, λ3, λ4}

are the multipliers for dcd, eff, isp, and obp.

{

22

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

λ3

λ2

0.6

0.08 0.3

λ1
λ4
0.78 0.14 0.09 0.0
0.35 0.42 0.11 0.13
0.15 0.21 0.23 0.42
0.11 0.15 0.74 0.0
0.19 0.19 0.01 0.61
0.14 0.23 0.51 0.12
0.16 0.15 0.33 0.36
0.23 0.15 0.05 0.57
0.2
0.43 0.15 0.22
0.08 0.23 0.33 0.35
0.21 0.05 0.32 0.41
0.38 0.46 0.06
0.1
0.16 0.26 0.38
0.2
0.38 0.33
0.29 0.0
0.26 0.29
0.15 0.3
0.3
0.09 0.0
0.35 0.0
0.29 0.36
0.17 0.05 0.34 0.44
0.19 0.44 0.05 0.32
0.45 0.47 0.08
0.0
0.57 0.23 0.0
0.2
0.17 0.1
0.33 0.4
0.32
0.3
0.17 0.0
0.39 0.45
0.31 0.23 0.23 0.23
0.16 0.32
0.32 0.2
0.18 0.23
0.59 0.0
0.44
0.4
0.16 0.0
0.0
0.0
1.0
0.0
0.0
1.0
0.0
0.0
0.07
0.0
0.93 0.0
0.08 0.24
0.47 0.2
0.14 0.37 0.28 0.22
0.35 0.0
0.29 0.36
0.32 0.18 0.45 0.05
0.05 0.42 0.24 0.29
0.05 0.41 0.44
0.1
0.35 0.53
0.12 0.0
0.19 0.24 0.26
0.3
0.33 0.51
0.17 0.0
0.18
0.08 0.54 0.2
0.2
0.19 0.0
0.61
0.38 0.49
0.14 0.0
0.02 0.24
0.44 0.3
0.19
0.37 0.15 0.3
0.08 0.01 0.52
0.4
0.49 0.0
0.17 0.34
0.13 0.57 0.3
0.15 0.0
0.41 0.44
0.14 0.39 0.16 0.31
0.33 0.08 0.19 0.4
0.0
0.34 0.4
0.12 0.16 0.33 0.4
0.39 0.04 0.38
0.2
0.49 0.27
0.14 0.1
0.19 0.0
0.47 0.34
0.11 0.12 0.45 0.31
0.22 0.24
0.14 0.4
0.18 0.23 0.3
0.29
0.15 0.14 0.32 0.4
0.31 0.15 0.14 0.41
0.18
0.82 0.0
0.0

0.25

0.0

−

−

GAP
Instance
0.13
neos-1456979
0.02
neos-2978205-isar
0.0
neos-3009394-lami
0.0
neos-3046601-motu
0.0
neos-3046615-murg
0.02
neos-3581454-haast
0.05
neos-3594536-henty
0.04
neos-3627168-kasai
0.11
neos-4333464-siret
0.01
neos-4333596-skien
0.02
neos-4343293-stony
0.04
neos-4387871-tavua
0.13
neos-4650160-yukon
0.01
neos-4724674-aorere
0.03
neos-4736745-arroux
0.35
neos-4738912-atrato
0.19
neos-4954672-berkel
0.06
neos-5076235-embley
0.12
neos-5079731-ﬂyers
neos-5093327-huahum 0.01
0.08
neos-5102383-irwell
1.0
neos-5107597-kakapo
0.01
neos-5140963-mincio
0.01
neos-5260764-orauea
0.16
neos-5261882-treska
0.11
neos-631517
0.22
neos-691058
0.04
neos-860300
0.0
neos16
0.08
neos17
0.02
neos5
0.16
newdano
0.25
nexp-150-20-1-5
0.18
nexp-150-20-8-5
0.33
ns2071214
0.0
p200x1188c
0.14
p500x2988
0.22
pg
0.1
pg5_34
0.02
physiciansched5-3
0.06
probportfolio
0.02
prod2
0.03
qnet1
0.0
r4l4-02-tree-bounds-50
0.13
r50x360
0.0
rocII-5-11
0.31
roll3000
0.01
set3-09
0.04
set3-10
0.0
set3-16
0.04
set3-20
0.03
seymour1
0.23
shipsched
0.11
sp150x300d
0.38
supportcase20
0.0
swath
0.06
tanglegram6
0.26
timtab1
0.01
timtab1CUTS
0.01
tokyometro
0.67
tr12-30
0.07
uct-subprob
0.01
usAbbrv-8-25_70

−

−

−

−

λ2

λ4

λ3

0.0

0.0

0.5

λ1
0.13 0.17 0.49 0.21
0.15 0.6
0.25
0.34 0.06 0.28 0.33
0.54 0.0
0.35 0.1
0.16 0.1
0.43 0.31
0.16 0.25 0.0
0.59
0.12 0.0
0.23 0.65
0.33 0.39
0.28 0.0
0.21 0.05 0.33 0.41
0.2
0.42 0.38
0.12 0.68 0.03 0.18
0.23 0.39 0.36 0.02
0.1
0.36 0.41 0.13
0.43 0.01 0.25 0.3
0.09 0.09 0.25 0.57
0.19 0.29 0.51
0.0
0.45
0.17 0.08 0.3
0.26 0.36
0.38 0.0
0.12 0.5
0.16 0.22
0.23 0.08 0.2
0.14 0.57 0.08 0.21
0.43 0.09 0.21 0.27
0.33 0.0
0.28 0.39
0.33 0.16 0.35 0.16
0.52 0.09
0.39 0.0
0.31 0.27
0.11 0.3
0.11 0.0
0.43 0.46
0.13 0.76 0.02 0.08
0.29 0.17 0.24 0.29
0.13 0.0
0.86 0.01
0.55 0.45 0.01 0.0
0.27 0.24 0.31 0.17
0.14 0.07 0.34 0.45
0.14 0.61 0.0
0.25
0.24 0.11 0.56 0.08
0.14 0.63 0.23 0.0
0.43 0.41
0.17 0.0
0.56
0.14 0.0
0.3
0.0
0.14 0.86 0.0
0.12 0.18 0.33 0.38
0.42 0.21
0.37 0.0
0.23 0.37 0.0
0.39
0.04 0.07 0.25 0.63
0.13 0.13 0.35 0.39
0.09 0.11 0.41 0.39
0.37 0.13
0.5
0.0
0.13 0.0
0.27 0.6
0.61 0.28 0.0
0.1
0.09 0.42 0.29
0.2
0.45 0.0
0.36 0.19
0.17 0.09 0.41 0.33
0.42 0.26 0.11 0.21
0.23 0.41
0.36 0.0
0.19 0.1
0.28 0.43
0.11 0.39 0.26 0.25
0.29 0.48 0.23 0.0
0.36
0.11 0.52 0.0
0.23 0.41
0.36 0.0
0.68
0.2
0.13 0.0
0.3
0.49 0.01 0.19
0.13 0.07 0.27 0.52
0.37 0.04 0.22 0.38
0.23 0.08 0.41 0.28

Instance
22433
23588
50v-10
a2c1s1
app1-1
app3
b1c1s1
b2c1s1
beasleyC1
beasleyC2
berlin
bg512142
bienst1
bienst2
binkar10_1
bppc8-02
bppc8-09
brasil
cost266-UUE
danoint
dg012142
drayage-100-12
drayage-100-23
drayage-25-23
drayage-25-27
dws008-01
eil33-2
exp-1-500-5-5
f2gap201600
f2gap401600
f2gap801600
fhnw-schedule-paira100
g200x740
gmu-35-40
graphdraw-domain
graphdraw-gemcutter
h50x2450
h80x6320d
ic97_tension
icir97_tension
k16x240b
lectsched-5-obj
lotsize
mik-250-20-75-2
mik-250-20-75-3
mik-250-20-75-4
mik-250-20-75-5
milo-v12-6-r2-40-1
milo-v13-4-3d-4-0
misc07
mkc
mkc1
mtest4ma
n2seq36q
n3700
n3707
n370b
n5-3
n7-3
n9-3
neos-1337307
neos-1423785

−

−

−

−
−

GAP
0.03
0.01
0.03
0.18
0.03
0.13
0.14
0.12
0.09
0.06
0.93
0.01
0.17
0.02
0.07
0.33
0.0
0.93
0.01
0.0
0.0
0.3
0.35
0.37
0.03
0.03
0.0
0.47
0.03
0.12
0.05
0.01
0.21
0.01
0.03
0.04
0.06
0.09
0.19
0.22
0.02
0.04
0.24
0.04
0.06
0.05
0.02
0.04
0.01
0.01
0.25
0.52
0.79
0.0
0.01
0.0
0.0
0.05
0.06
0.01
0.01
0.05

−

−

−

Table 10: Per instance results of Experiment 6.3 (Learning capabilities). GAP refers to the relative gap improvement.
λ1, λ2, λ3, λ4}

are the multipliers for dcd, eff, isp, and obp.

{

23

Adaptive Cut Selection in Mixed-Integer Linear Programming

A PREPRINT

Instance
22433
23588
50v-10
a2c1s1
app1-1
app3
b1c1s1
b2c1s1
beasleyC1
beasleyC2
berlin
bg512142
bienst1
bienst2
binkar10_1
bppc8-02
bppc8-09
brasil
cost266-UUE
danoint
dg012142
drayage-100-12
drayage-100-23
drayage-25-23
drayage-25-27
dws008-01
eil33-2
exp-1-500-5-5
f2gap201600
f2gap401600
f2gap801600
fhnw-schedule-paira100
g200x740
gmu-35-40
graphdraw-domain
graphdraw-gemcutter
h50x2450
h80x6320d
ic97_tension
icir97_tension
k16x240b
lectsched-5-obj
lotsize
mik-250-20-75-2
mik-250-20-75-3
mik-250-20-75-4
mik-250-20-75-5
milo-v12-6-r2-40-1
milo-v13-4-3d-4-0
misc07
mkc
mkc1
mtest4ma
n2seq36q
n3700
n3707
n370b
n5-3
n7-3
n9-3
neos-1337307
neos-1423785

−
−

−

GAP
0.03
0.01
0.08
0.11
0.1
0.04
0.14
0.09
0.04
0.05
0.93
0.01
0.0
0.02
0.07
0.37
0.02
0.93
0.0
0.0
0.01
0.2
0.21
0.06
0.09
0.06
0.0
0.47
0.02
0.15
0.05
0.0
0.17
0.01
0.03
0.0
0.08
0.05
0.04
0.17
0.1
0.04
0.24
0.08
0.04
0.0
0.11
0.02
0.07
0.02
0.24
0.14
0.75
0.33
0.01
0.0
0.01
0.02
0.03
0.04
0.01
0.04

−

−
−
−

−

−

−

−
−

−

−
−
−

λ3

λ2

0.0
0.0

λ1
λ4
0.59 0.05 0.36 0.0
0.58 0.01 0.39 0.02
0.53 0.07 0.35 0.04
0.23 0.38 0.25 0.14
0.26 0.16 0.23 0.35
0.25 0.28 0.28 0.19
0.23 0.32 0.25 0.2
0.23 0.32 0.25 0.2
0.18 0.37 0.21 0.24
0.18 0.36 0.21 0.25
0.18 0.35 0.21 0.25
0.21 0.28 0.21
0.3
0.02 0.32 0.35
0.3
0.29 0.02 0.32 0.37
0.28 0.32 0.27 0.14
0.41 0.0
0.59 0.0
0.57 0.0
0.42 0.01
0.18 0.35 0.21 0.25
0.22 0.42 0.24 0.12
0.28 0.24
0.28 0.2
0.26 0.17
0.27 0.3
0.33 0.31
0.36 0.0
0.33 0.32
0.36 0.0
0.33 0.31
0.36 0.0
0.36 0.0
0.33 0.31
0.43 0.13 0.36 0.07
0.35 0.27 0.25 0.13
0.0
0.3
0.29 0.41
0.99 0.0
0.0
0.0
0.99 0.0
0.01 0.98 0.01 0.01
0.26
0.44 0.0
0.25 0.14 0.26 0.36
0.07
0.52 0.01 0.4
0.35
0.29 0.06 0.3
0.29 0.06 0.3
0.35
0.35 0.02 0.31 0.32
0.07 0.29 0.34
0.3
0.33 0.05 0.3
0.32
0.29 0.4
0.31 0.0
0.29 0.44
0.27 0.0
0.3
0.32
0.37 0.0
0.29 0.45
0.26 0.0
0.27 0.4
0.13
0.2
0.13
0.29 0.38 0.2
0.32 0.34 0.21 0.13
0.31 0.35 0.21 0.13
0.33 0.24 0.29 0.14
0.28 0.17 0.28 0.28
0.17 0.78 0.05 0.01
0.38 0.03
0.58 0.0
0.35 0.15 0.29 0.21
0.28 0.09 0.27 0.36
0.43 0.22 0.22 0.13
0.29 0.46
0.25 0.0
0.29 0.46
0.26 0.0
0.29 0.46
0.26 0.0
0.12 0.69 0.18 0.01
0.12 0.69 0.18 0.01
0.12 0.7
0.18 0.01
0.48 0.08 0.28 0.16
0.25 0.34 0.27 0.15

0.3

Instance
neos-1456979
neos-2978205-isar
neos-3009394-lami
neos-3046601-motu
neos-3046615-murg
neos-3581454-haast
neos-3594536-henty
neos-3627168-kasai
neos-4333464-siret
neos-4333596-skien
neos-4343293-stony
neos-4387871-tavua
neos-4650160-yukon
neos-4724674-aorere
neos-4736745-arroux
neos-4738912-atrato
neos-4954672-berkel
neos-5076235-embley
neos-5079731-ﬂyers
neos-5093327-huahum
neos-5102383-irwell
neos-5107597-kakapo
neos-5140963-mincio
neos-5260764-orauea
neos-5261882-treska
neos-631517
neos-691058
neos-860300
neos16
neos17
neos5
newdano
nexp-150-20-1-5
nexp-150-20-8-5
ns2071214
p200x1188c
p500x2988
pg
pg5_34
physiciansched5-3
probportfolio
prod2
qnet1
r4l4-02-tree-bounds-50
r50x360
rocII-5-11
roll3000
set3-09
set3-10
set3-16
set3-20
seymour1
shipsched
sp150x300d
supportcase20
swath
tanglegram6
timtab1
timtab1CUTS
tokyometro
tr12-30
uct-subprob
usAbbrv-8-25_70

GAP
0.07
0.01
0.0
0.01
0.0
0.01
0.05
0.04
0.0
0.01
0.03
0.02
0.06
0.01
0.05
0.58
0.18
0.06
0.12
0.01
0.02
1.0
0.01
0.0
0.16
0.08
0.24
0.05
0.0
0.03
0.03
0.11
0.14
0.16
0.0
0.01
0.06
0.19
0.01
0.02
0.04
0.0
0.31
0.0
0.1
0.0
0.16
0.02
0.02
0.0
0.04
0.03
0.07
0.68
0.22
0.0
0.04
0.24
0.01
0.01
0.65
0.01
0.0

−

−
−
−

−

−

−

−

−

−

−

−

−

λ4

λ3
0.39 0.14
0.19 0.0
0.3

λ1
λ2
0.47 0.0
0.11 0.7
0.33 0.0
0.38
0.33 0.03 0.28 0.36
0.33 0.04 0.27 0.36
0.31 0.35
0.35 0.0
0.25 0.0
0.28 0.47
0.29 0.41
0.0
0.3
0.25 0.38 0.23 0.15
0.37 0.11 0.32 0.2
0.24 0.38 0.22 0.16
0.23 0.38 0.23 0.17
0.28 0.17 0.28 0.27
0.36 0.02 0.31 0.31
0.28 0.12 0.25 0.34
0.28 0.12 0.25 0.34
0.34 0.05 0.32 0.28
0.52 0.24 0.04
0.2
0.2
0.52 0.24 0.04
0.19 0.52 0.24 0.04
0.53 0.24 0.04
0.2
0.29 0.29
0.41 0.0
0.37 0.0
0.34
0.3
0.48 0.29 0.22 0.02
0.12
0.49 0.0
0.4
0.39
0.31 0.0
0.3
0.32 0.35
0.33 0.0
0.02
0.09 0.89 0.0
0.37 0.0
0.31 0.33
0.07 0.84 0.07 0.01
0.03 0.93 0.02 0.02
0.27 0.0
0.31 0.42
0.16 0.42 0.18 0.24
0.37 0.04
0.49 0.1
0.34 0.0
0.31 0.35
0.2
0.31 0.22 0.27
0.21 0.25 0.23 0.3
0.28 0.0
0.32 0.4
0.12 0.73 0.15 0.0
0.35 0.03 0.32 0.3
0.34 0.32 0.19 0.15
0.03
0.0
0.97 0.0
0.38 0.05
0.57 0.0
0.29 0.0
0.28 0.43
0.27 0.03 0.28 0.42
0.37 0.01 0.29 0.33
0.41 0.01 0.34 0.24
0.24 0.37 0.26 0.13
0.24 0.37 0.26 0.13
0.24 0.37 0.26 0.13
0.24 0.37 0.26 0.13
0.06 0.86 0.07 0.0
0.43 0.02 0.32 0.24
0.25 0.09 0.26 0.39
0.3
0.41
0.41 0.47 0.12 0.0
0.29 0.68 0.04 0.0
0.31 0.01 0.28 0.4
0.3
0.17 0.28 0.25
0.33 0.03 0.31 0.34
0.27 0.0
0.44
0.28 0.59 0.11 0.01
0.32
0.36 0.02 0.3

0.0

0.3

0.3

Table 11: Per instance results of Experiment 6.4 (Generalisation capabilities). GAP refers to the relative gap
improvement.

are the multipliers for dcd, eff, isp, and obp.

λ1, λ2, λ3, λ4}

{

24

