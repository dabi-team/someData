Online Planning for Decentralized Stochastic Control
with Partial History Sharing

Kaiqing Zhang, Erik Miehling, and Tamer Bas¸ar

9
1
0
2

g
u
A
6

]

G
L
.
s
c
[

1
v
7
5
3
2
0
.
8
0
9
1
:
v
i
X
r
a

Abstract— In decentralized stochastic control, standard ap-
proaches for sequential decision-making, e.g. dynamic program-
ming, quickly become intractable due to the need to maintain
a complex information state. Computational challenges are
further compounded if agents do not possess complete model
knowledge. In this paper, we take advantage of the fact that
in many problems agents share some common information, or
history, termed partial history sharing. Under this information
structure the policy search space is greatly reduced. We propose
a provably convergent, online tree-search based algorithm that
does not require a closed-form model or explicit communication
among agents. Interestingly, our algorithm can be viewed
as a generalization of several existing heuristic solvers for
decentralized partially observable Markov decision processes.
To demonstrate the applicability of the model, we propose a
novel collaborative intrusion response model, where multiple
agents (defenders) possessing asymmetric information aim to
collaboratively defend a computer network. Numerical results
demonstrate the performance of our algorithm.

I. INTRODUCTION

The lack of centralized information in decentralized con-
trol settings introduces signiﬁcant theoretical and computa-
tional challenges. The primary difﬁculty arises from the fact
that no agent knows all past system information (consisting
of past actions and observations, termed the history) that is
relevant to its decision making process, a property referred
to as perfect recall [1]. In centralized control settings, perfect
recall allows for the application of the one-way separation
between estimation and control [2] – the system state esti-
mate, given past system information, is independent of the
previous decision rules (the functions that map the available
information into control actions), i.e., policy-independence.
This results in the reduction of the history to a sufﬁcient
statistic1,
is
the probability distribution over states,
independent of the choice of past decision rules (see [3], [4])
and thus has a time-invariant domain. Importantly, the update
of the sufﬁcient statistic is a function of the control action,
and not the entire decision rule. In decentralized control
settings, no agent has perfect recall and thus the separation
principle cannot be applied. Asymmetric information among
agents results in a more complex sufﬁcient statistic and, due
to the lack of perfect recall, requires one to take into account
the inﬂuence of previous decision rules (instead of only the

that

This work was supported in part by the U.S. Ofﬁce of Naval Research
(ONR) MURI grant N00014-16-1-2710, and in part by the US Army
Research Ofﬁce (ARO) Grant W911NF-16-1-0485. The authors are with
the Coordinated Science Laboratory, University of Illinois at Urbana-
Champaign ({kzhang66, miehling, basar1}@illinois.edu).

1The information that is sufﬁcient for making optimal decisions, also

termed an information state or belief.

control action) on the update of the sufﬁcient statistic and,
in turn, on the computation of the optimal control.

The general problem of dynamic decision making when no
agent possesses complete system knowledge has been studied
in the literature spanning multiple ﬁelds. In the control
theory literature, the problem is referred to as decentralized
stochastic control, whereas in the computer science literature,
the problem is predominantly studied under the topic of
decentralized partially observable Markov decision processes
(Dec-POMDPs). Generally speaking, algorithms developed
to address the problem are subject to at least one of the
following two limitations. First, some algorithms are based
on the assumption that the knowledge of the model is known
either to all agents [5] or to a central coordinator that designs
true in
policies for all agents [6], which is usually not
practice. Second, many of the algorithms require planning to
be done ofﬂine, i.e., bottom-up approaches where the optimal
policies of all agents are calculated before runtime via
dynamic programming [7]–[10]. Unfortunately, these exact
approaches generate signiﬁcant computational issues even for
problems with a moderate dimension or planning horizon.
In response, other work has resorted to approximations that
rely on forward lookahead search (or top-down approaches).
Unfortunately, many of these algorithms are heuristic and are
not guaranteed to converge to the optimal2 policies [6].

With the goal of addressing these limitations, we propose
a provably convergent online planning algorithm for decen-
tralized stochastic control which can be implemented in a
decentralized fashion, without requiring any communication
among agents. Our proposed algorithm exactly addresses two
key challenges/drawbacks of decentralized decision making
[12]: the explosion in complexity as a function of the state
space, and the centralization of computation. In particular,
we ﬁrst take advantage of the fact that in many problems
decision makers share some common information, what is
referred to as partial history sharing (PHS) in the literature
[5]. For example, decision makers may observe each other’s
control actions (e.g. ﬂeet control of self-driving cars [13]),
or may share some common observations (e.g. cooperative
navigation of robots [14]). Following the structural results
developed in [5], utilizing common information allows for
the reformulation of the decentralized stochastic control
problem as an equivalent centralized partially observable
Markov decision process (POMDP). The state of the cen-
tralized POMDP includes both the underlying state of the

2The term optimal is taken to mean team-optimal [11], interpreted as
the joint policy that maximizes the total reward given the informational
constraints of the agents.

 
 
 
 
 
 
i.e.,

dynamic system as well as the local
information of the
controllers. The space of information states of the centralized
the space of joint probability distributions
POMDP,
over the system state and local information of agents, is
greatly reduced compared to the multi-agent belief state [7],
[8] commonly used in the Dec-POMDP literature. Such a
reduction further enables a compression of the policy search
space (a nice argument of this compression can be found
in Section I-A of [5]). To solve the centralized problem
efﬁciently, we adopt a top-down approach based on Monte-
Carlo tree search for solving large-scale POMDPs [15]. To
enable a decentralized implementation, all agents construct
a local copy of the search tree, and are assumed to share
a common source of randomness, e.g., a common random
number generator, so that the agents’ generated trees are
identical, obviating the need for explicit communication.

We demonstrate the performance of the proposed algo-
rithm in a computer security setting, focusing on how mul-
tiple agents (defenders) can collaboratively defend a cyber
network subject to the constraint that they cannot instanta-
neously share defense actions and intrusion information with
others. As will be shown, the setting is naturally addressed
by a form of PHS, termed the delayed sharing information
structure, allowing for direct application of the proposed
algorithm.

A. Related Work

Most existing solvers for decentralized decision-making
rely on a reformulation of the decentralized problem as a
centralized problem [5]–[7], [10]. One prominent approach in
the literature is the work of [5] in which the authors introduce
a common information approach, and an associated dynamic
programming decomposition, where agents are assumed to
share some of their history with others. Interestingly, the
reformulation of [5] includes several approaches for Dec-
POMDPs from the computer science community as special
cases, when the common information is absent. In particular,
the common information belief state in [5] generalizes the
deﬁnition of occupancy-state MDP in [10], [12], which was
shown to be the sufﬁcient statistic for solving general Dec-
POMDPs.

To solve the centralized problem efﬁciently, several top-
down algorithms have been proposed, e.g., [6] converted the
Dec-POMDP to a type of centralized sequential decision
problem, termed a non-observable MDP (NOMDP), which
was then solved by the heuristic MAA∗ tree search algorithm
[16]. Several sampling-based planning/learning algorithms
have also been proposed to improve the tractability of
Dec-POMDP solvers. In particular,
[17] developed solvers
that combined Monte-Carlo sampling with policy iteration
and the expectation-maximization algorithm, respectively. In
addition, Monte-Carlo tree search has been applied to special
classes of Dec-POMDPs, e.g., multi-agent POMDPs [18]
and multi-robot active perception [19]. Under PHS, the only
model-free learning algorithm that we are aware of is the
work of [20].

Most existing planning/learning procedures are designed
to be implemented in a centralized fashion, i.e., either the
coordinator designs optimal policies for all agents [6], [7],
[12], or each agent communicates with other agents to access
global information [14], [19], [21]–[23]. To enable a fully
decentralized implementation, we assume that all agents
use a common random number generator when sampling
distributions. This idea, termed a correlation device, has been
used in the past to avoid explicit communication between
agents [20], [24]. Such correlation devices are reminiscent
of similar devices used in generating correlated equilibria in
games [25].

B. Contribution

The contribution of our paper is three-fold: 1) We de-
velop a tractable online planning algorithm for decentralized
stochastic control problems with partial history sharing. The
algorithm does not need explicit knowledge3 of the model,
requires no explicit communication4 between agents, and
is provably convergent to the team-optimal solution. 2) We
offer a unifying perspective of existing algorithms through
the lens of decentralized stochastic control. Speciﬁcally, we
describe (in Section III-D) how some recently proposed
Dec-POMDP solvers can be viewed as special cases of
our algorithm. 3) The proposed algorithm is applied to a
novel computer security setting, which we term collaborative
intrusion response. To the best of our knowledge, this is the
ﬁrst decentralized approach to intrusion response.

We note that although such a common-information based
framework has been advocated in the control theory literature
[5],
it has not been fully investigated in the context of
developing planning/learning algorithms for Dec-POMDPs.
In this sense, our work offers a new perspective for improv-
ing the computational efﬁciency of solving Dec-POMDPs,
speciﬁcally for problems where common information exists.

II. PRELIMINARIES

In this section, we introduce the model for decentralized
stochastic control under partial history sharing, and review
the relevant structural results from [5].

A. Decentralized Stochastic Control with PHS

Consider a system of n cooperative decision makers
(hereafter referred to as agents) operating over a horizon
of length T . At each time t, the system takes on one of
ﬁnitely many states from the space X . The system state
evolves as a function of the control actions of the agents:
given a current system state and the collection of control
actions across agents, termed a joint control action, denoted
by u = (u1, . . . , un) ∈ U = U 1 × · · · × U n, the system
state obeys the following dynamics xt+1 = f (xt, ut, w0
t ),
where {w0
t=1 is a sequence of independent and identically
distributed (i.i.d.) random variables. Agents incur a common

t }T

3Instead, the existence of a simulator/generative model is assumed.
4As discussed in [26], no communication refers to no explicit information
transmission among agents. However, it is still possible for agents to share
information via modiﬁcation and observation of the underlying system.

reward (since they are cooperative), denoted by r(x, u),
which is a function of the underlying system state x and
the joint control action u.

Each agent lacks perfect information of the underlying
system state, instead receiving local (noisy) observations as
the system evolves. This leads to agents possessing asymmet-
ric information. Here, we focus on a special while prevalent
information structure, termed partial history sharing, where
agents’ local information may have some components in
common. In particular, at time t, three pieces of informa-
tion are available to each agent: a local observation, local
memory, and the common history.

• Local observations: Each agent i receives a local
t ∈ Y i, which is generated according to
observation yi
t), where {wi
t}T
t(xt, wi
t = hi
yi
t=1 is a sequence of i.i.d.
random variables. The collection of observations across
all agents is denoted by yt = (y1
t ) ∈ Y =
Y 1 × · · · × Y n.

t , · · · , yn

t ∈ Mi

t, a subset of {ui

• Local memory/information: Each agent i maintains a
local memory mi
1:t},
representing their (possibly limited) storage of the local
observations and actions up to and including time t.
• Common history/information: In addition to the local
memory, all agents possess a common history ct ⊆
{u1:t−1, y1:t}, which encodes any shared history of the
local observations and actions of all agents.

1:t−1, yi

Each agent i makes decisions based on its currently
available information (mi
t denote the
control law of agent i that maps agent i’s current information
(mi

t, ct) into a local control action ui

t, ct). Formally, let gi

t, that is

t = gi
ui

t(mi

t, ct).

(1)

The collection of agent i’s control laws over time is called
the control policy of agent i and is denoted by gi =
(gi
T ). The collection of control policies across all
the system and is
agents is called the control policy of
denoted by g = (g1, · · · , gn).

1, · · · , gi

t, yi

Z(mi

Z, that is zi

information {mi

The order of events in the model is now discussed. For a
given time-step t, each agent i takes an action, ui
t, receives a
t+1, and then shares a subset zi
local observation, yi
t+1 of their
t, ui
t+1} with all other
(updated) local
agents. The speciﬁc information that is shared is dictated by
t+1) ∈ Z i. We
t+1 = P i
t, ui
the function P i
term the quantity zi
t+1 the innovation, and deﬁne the joint
t+1) ∈ Z = Z 1×· · ·×Z n.
t+1, · · · , zn
innovation as zt+1 = (z1
Each agent then updates their local memory according to the
function P i
L, dictating what information is carried over to
t+1 = P i
the next iteration, that is mi
t+1) ∈
Mi
t+1.5 Finally, the common history is updated as ct+1 =
{ct, zt+1}. The functions P i
L are dictated by the
problem setting at hand (see Section IV for their deﬁnition
in the context of our computer security example).

Z and P i

t+1, zi

L(mi

t, ui

t, yi

t, yi

The objective of the problem is to determine the control
policy g that maximizes the expected total discounted reward,

5The dependency of mi

t+1 on zi

information and the common information are disjoint, mi

t+1 is to ensure that agent’s local
t+1 ∩ ct+1 = ∅.

deﬁned as

R(g) := Eg

(cid:35)

βtr(xt, ut)

(cid:34) T

(cid:88)

t=1

(2)

where r(x, u) is the instantaneous reward received when joint
action u is taken in system state x, β ∈ [0, 1] is the discount
factor, and Eg denotes the expectation with respect to the
probability measure induced by policy g.

B. Common Information Based Approach

In general, the decentralized problem with partial history
sharing is equivalent
to a centralized problem from the
perspective of some virtual coordinator [5]. The coordinator
is assumed to have access only to the information that is
in common among all agents, that is, the common history
described in Section II-A. The coordinator solves for func-
tions for each agent, termed prescriptions6, that map local
information (i.e. local memories) to control actions. It was
shown in [5] that the coordinator’s problem of determining
these prescriptions reduces to a POMDP with appropriately
deﬁned state, action, and observation spaces.

t : Mi

t , · · · , γn

t ), where γi

the local observations yi

In particular, consider a coordinator that can observe the
common history ct but not
t or
the local memories mi
t. The coordinator’s decision is the
set of prescriptions across all agents, denoted by γt =
t → U i is a mapping from the
(γ1
local memory to a local control action, that is, ui
t = γi
t).
The set of all prescriptions γi
t is denoted by Γi
t with Γt =
t ×· · ·×Γn
Γ1
t . By Lemma 1 of [5], the coordinator’s problem
can be viewed as a POMDP with state (xt, m1
t ) ∈
X × M1
t ) ∈ Γt,
and observations zt ∈ Z. By deﬁning the virtual history
of the coordinator7 as a sequence of joint prescriptions and
innovations, that is ht = {γ1, z2, γ2, · · · , γt−1, zt}, one can
deﬁne the information state, or belief state, for the centralized
POMDP as the distribution over (xt, m1
t ) given
virtual history ht, that is

t , actions γt = (γ1

t × · · · × Mn

t , · · · , mn

t , · · · , mn

t , · · · , γn

t(mi

πt = P(xt, m1

t , · · · , mn

t | ht).

(3)

The update of the information state πt follows the transition
πt+1 = φ(πt, γt, zt) for some function φ (cf. Appendix A in
[5]). Moreover, the reward of the centralized POMDP is
(cid:101)r(πt, γt) = E[r(xt, ut) | ct, γ1:t] = E[r(xt, ut) | ht, γt],

t(mi

t = γi

where ui
t). Note that the expectation is taken over
the probability distribution πt. The value function of the
POMDP is deﬁned as

V ∗
t (ht) = sup

µt,··· ,µT

Eµt,··· ,µT

(cid:34) T

(cid:88)

τ =t

(cid:35)
(cid:101)r(πτ , γτ )

βτ −t

(4)

6Note that in the control and computer science communities, both terms,
prescriptions and policies, have been used. We will use them interchange-
ably.

7To avoid confusion with the actual history of actions and observations
in the original decentralized stochastic control problem, we use the term
virtual history to represent the history of actions (γt) and observations (zt)
of the equivalent centralized POMDP (the virtual coordinator’s problem).

where µt, termed the coordination strategy, is a mapping
from the virtual history to prescriptions. The coordination
strategy is analogous to the notion of a policy in conventional
POMDPs. Note that V ∗
1 (h1) is exactly the maximum of the
expected total discounted reward deﬁned in (2).
Given any optimal coordination strategy µ∗

t , the optimal
t =
). Consequently, by Theorem 2 of [5], the

joint prescription is determined by γ∗
(γ1,∗
t
optimal control action for any agent i is determined by
t = γi,∗
ui,∗

t (ht) where γ∗

, · · · , γn,∗

t = µ∗

t (mi

t).

(5)

t

For ﬁnite action and memory spaces, the space of possible
prescriptions is also ﬁnite, that is |Γt| = (cid:81)n
, and
thus the coordinator’s functional optimization (of determin-
ing the optimal prescriptions) reduces to an optimization over
vector actions.

i=1 |U i||Mi
t|

The POMDP reformulation enables a sequential decompo-
sition of the problem and thus the construction of a backward
induction algorithm, via dynamic programming, for ﬁnding
an optimal control policy [5]. However, the backward in-
duction algorithm requires solving a sequence of one-stage
functional optimization problems for all realizations of πt,
which is computationally challenging given that πt lives in
an inﬁnite dimensional space. The aim of our algorithm
(introduced next) is to avoid this computational burden by
selecting prescriptions for the current information state πt
via online construction of search trees.

III. DECENTRALIZED ONLINE PLANNING
In this section, we outline our online algorithm for solving
the decentralized stochastic control problem with partial
history sharing. The algorithm is inspired by the single-
agent (centralized) POMDP algorithm known as partially
observable Monte-Carlo planning (POMCP) [15]. Using the
reduction of the decentralized control problem to a central-
ized POMDP shown in Section II-B, the development of a
decentralized algorithm based on a single-agent algorithm is
ﬁtting. Since the coordinator’s problem is based on common
information, all agents know this information and can indi-
vidually solve the coordinator’s problem, thus obtaining a
solution to the original decentralized control problem.

A. The Search Tree of Virtual Histories

As in POMCP [15], the basis of our algorithm is a search
tree constructed iteratively via Monte-Carlo simulations (de-
scribed in more detail in Section III-B). One key difference
here is that nodes in the search tree correspond to the
virtual histories ht, as deﬁned in Section II-B, instead of
the histories of actions and observations as in the POMCP
algorithm. The search tree, denoted by T , consists of nodes,
denoted by T (h), each of which encodes two quantities
T (h) = (N (h), V (h)): a count index N (h), describing how
many times node h has been visited in past simulations, and
an estimated value V (h), representing the mean value of
all simulations that began at virtual history h.8 For a given

8For brevity, we do not include a subscript t for the estimated value

V (h). The time index t will be self-evident from the argument h.

node h, each branch emanating from h is an alternating
sequence of joint prescriptions δ = (δ1, . . . , δn) ∈ Γ and
joint innovations z ∈ Z.

B. Algorithm: Decentralized Online Planning with PHS

We now describe our proposed algorithm, termed decen-
tralized online planning with partial history sharing. A fun-
damental characteristic of our algorithm is that computation
is decentralized, that is, we do not rely on a centralized entity
to compute agents’ control policies. To this end, each agent i
constructs its own copy of the search tree, denoted by T i. To
carry out simulations, we assume that each agent has access
to a generative model (a black-box simulator), G, that takes
as input a system state x and joint action (u1, . . . , un), and
outputs a successor system state x(cid:48), joint observation vector
(y1, . . . , yn), and reward r. The generate model avoids the
need for an explicit model representation.

(cid:113) log N (h)

The algorithm consists of two main stages: the search stage
and a belief update stage. In the search stage, each agent
begins their simulations from the same virtual history h.
Each agent draws a sample (x, m1, . . . , mn) from the current
belief, approximated by a set of particles B(h). Using this
sample, each agent expands the search tree from the root
node h using either a rollout simulation, in the case where
h does not have children nodes, or a selection rule (UCB1
[27]) if h already has children. The UCB1 selection rule
balances exploration and exploitation by maximizing the sum
of the current estimated value of prescription δ, V (hδ), and
an exploration term ρ
N (hδ) depending on the number
of times h has been visited, N (h), and the number of times
δ has been selected from h, N (hδ). Successive simulations
further expand the search tree and, due to the above selection
rule, allow for targeted search of the decision space and
efﬁcient convergence of estimates. The pseudocode of the
search stage, for a given agent i, is shown in Algorithm 1.
To enable a decentralized algorithm, we assume that
agents’ samples are correlated via a common source of
randomness. Practically, this means that any time in the
algorithm an agent draws a sample from a distribution, it
is done so using a pseudorandom number generator with a
common random seed. As a result, agents construct identical
search trees and compute the same optimal joint prescription.
Since agents are completely cooperative, they can agree upon
this common seed beforehand and, further, each agent i can
rely on every other agent j to follow the prescription function
that was computed during the search stage.

Under the computed joint prescription, each agent i uses
information to specify an action, and transmits
its local
their innovation zi
t+1 to all other agents using the function
P i
Z. Local memories are then updated via the function P i
L,
relevant branches of the search tree are identiﬁed, and the
virtual history is updated allowing for the next round of the
search algorithm to proceed.

Upon update of the virtual history, each agent must update
its belief π in order to reﬂect the new information (γt, zt+1).
In practice, it is not tractable to maintain an exact belief
representation. Instead, for a given virtual history ht, each

t × · · · × Mn

t ∈ X × M1

agent’s belief B(ht) is represented as a set of K particles
of the form Bj
t .9 Each agent
updates its belief by drawing a sample (x, m1, . . . , mn)
from the current belief approximation B(h) then, using the
computed prescription, speciﬁes a joint action (u1, . . . , un).
The generative model
is then called to obtain a sample
(x(cid:48), y1, . . . , yn, −) which is used to construct a joint innova-
tion and an updated set of local memories (m1(cid:48), . . . , mn(cid:48)). If
the sampled joint innovation matches the true joint innova-
tion, then the particle (x(cid:48), m1(cid:48), . . . , mn(cid:48)) is added to B(h(cid:48)).
The sampling repeats until K particles have been accepted
into B(h(cid:48)). The common source of randomness results in
agents possessing identical updated belief approximations.

C. Convergence

the
By virtue of the common source of randomness,
planning update, i.e., the construction of the search trees,
is identical and decoupled for all agents. As a result, the
convergence of our decentralized online planning algorithm
can be characterized by that of the single-agent POMCP
algorithm [15]. The convergence of our algorithm is given
as follows.

p
−→ V ∗

Lemma 1 (Theorem 2 [15]). Given the true belief state πτ ,
the value function constructed by Algorithm 1 converges in
probability to the value function, i.e., V (hτ )
τ (hτ ), for
any history hτ that are preﬁxed by ht with τ ≥ t, where
V ∗
τ (hτ ) is as deﬁned in (4). As the number of visits N (hτ )
approaches inﬁnity, the bias of the value function, E[V (hτ )−
V ∗
τ (hτ )], reduces on the order of log N (hτ )/N (hτ ).
Lemma 1 establishes that if the true belief state πτ is
available, then the optimal value function can be obtained
by Algorithm 1. Accordingly, the optimal prescription can
be approximated by ˆγ∗
t = argmaxδ∈Γ V (htδ), which yields
the approximate optimal control action ˆγi,∗

t (mi

t).

D. Connections to Existing Solvers

The common information approach advocated in the
control community has connections to some Dec-POMDP
solvers from the computer science community. For instance,
one popular heuristic algorithm for solving Dec-POMDPs is
the tree search algorithm MAA∗ [6]. In the context of MAA∗,
each node in the tree denotes a history of joint policies, which
can be interpreted as a state; and each edge represents a joint
decision rule, which corresponds to an action. The authors
formulate the problem as a NOMDP [6], deﬁning a sufﬁcient
statistic as the distribution over joint observation histories
and states. Drawing a connection to our approach,
this
reduction can be obtained via a special case of the common
information approach, where the common information is
empty and the local memory is the local observation history.
In this sense, our algorithm can be viewed as a generalization
of the Dec-POMDP solver presented in [6].

The common information approach is also related to the
reduction of Dec-POMDPs to occupancy state MDPs [10],

9The speciﬁc approximation is 1
K

(cid:80)K

j=1 δ

is the Kronecker delta function.

(x,m1,...,mn),Bj
t

, where δ·,·

where the occupancy state is deﬁned as the joint distribution
over states and the joint history of actions and observations.
As noticed in [10], the reduction to a NOMDP is a special
case of the occupancy state reduction when deterministic
policies are used. The latter approach is then included as a
special case of the common information approach when the
local memory is the history of local actions and observations
and the common information is null. While the occupancy
state reduction is amenable to sampling-based approaches,
[10] makes use of the piece-wise linearity and convexity of
the optimal value function to solve the problem.

IV. APPLICATION: COLLABORATIVE INTRUSION
RESPONSE IN CYBER NETWORKS

We consider the problem of collaborative intrusion re-
sponse describing how a collection of defenders can collab-
oratively achieve system-wide security under the constraint
that each agent can only prescribe localized defense actions
based on localized security alert information. Our setting
goes one step beyond collaborative intrusion detection sys-
tems [28] by addressing the question of not only attack
detection, but attack response.

j , N +

Following existing work, we model the cyber network
by a type of attack graph termed a condition dependency
graph [29], [30]. The dependency graph, denoted by G =
{S, E}, quantiﬁes the relationship between security condi-
tions (attacker capabilities), represented by nodes S, and
exploits, represented by hyperedges E. Speciﬁcally, each
node in S is assumed to either be enabled (attacker possesses
the capability) or disabled (attacker does not possess the
capability) whereas each edge ej ∈ E is an ordered pair of
sets, ej = (N −
j ), relating the conditions necessary for
the exploit to be attempted, termed preconditions N −
j ⊆ S,
to conditions that become enabled if the exploit succeeds,
termed postconditions N +
j ⊆ S. The system state, termed a
security state, is deﬁned as the set of currently enabled nodes.
We consider a simple probabilisitic threat model. For a given
security state, the attacker is attempts exploits with enabled
preconditions ej with a ﬁxed probability αj, where each
attempted exploit ej succeeds with a ﬁxed probability βj.
Defense actions induce system modiﬁcations that have the
effect of blocking certain exploits from succeeding (setting
βj = 0 for each blocked exploit). Each agent i is associated
an intrusion detection system which generates security alerts
from the set Ai. Attempt of an exploit ej ∈ E generates
alert k with probability of detection δjk. Additionally, each
alert ak is also subject to false alarms as dictated by the
(per time-step) probability ζk. Lastly, the attacker’s goals
(enabling speciﬁc nodes termed goal conditions) are encoded
by a cost function which takes into account the tradeoff
between security (keeping the attacker away from its goal
conditions) and availability (preserving network usability by
limiting system modiﬁcations).

The deﬁning feature of the security problem is that
no agent possesses system-wide knowledge. Agents have
asymmetric information over the factors that inﬂuence their
decision-making; the evolution of the security of the system,

Algorithm 1 Decentralized Online Planning with Partial History Sharing – Agent i
function SEARCH(h)

function SIMULATE(x, m1, . . . , mn, h, d)

repeat

if h = ∅ then: (x, m1, . . . , mn) ∼ B0
else: (x, m1, . . . , mn) ∼ B(h)
SIMULATE(x, m1, . . . , mn, h, 0)

until STOPPINGCONDITION()
return argmaxδ∈Γ V (hδ)

function ROLLOUT(x, m1, . . . , mn, h, d)

rollout(h))

if βd < ε then: return 0
rollout(h), . . . , Γn
γ = (γ1, . . . , γn) ∼ (Γ1
(u1, . . . , un) ← (γ1(m1), . . . , γn(mn))
(x(cid:48), y1, . . . , yn, r) ∼ G(x, u1, . . . , un)
zi ← P i
h(cid:48) ← hγz
(m1(cid:48), . . . , mn(cid:48)) ← (P 1

L(m1, u1, y1, z1), . . . ,
P n

L(mn, un, yn, zn))
R ← r + β · ROLLOUT(x(cid:48), m1(cid:48), . . . , mn(cid:48), h(cid:48), d + 1)
return R

Z (mi, ui, yi) and share zi with all other agents

if βd < ε then: return 0
if h (cid:54)∈ T i then

for all γ ∈ Γ do

T i(hγ) ← (N0(hγ), V0(hγ))
return ROLLOUT(x, m1, . . . , mn, h, d)

(cid:115)

V (hδ) + ρ

γ= (γ1, . . . , γn) ∈

Z (mi, ui, yi) and share zi with all other agents

argmax
(δ1,...,δn)∈Γ1×···×Γn
(u1, . . . , un) ← (γ1(m1), . . . , γn(mn))
(x(cid:48), y1, . . . , yn, r) ∼ G(x, u1, . . . , un)
zi ← P i
h(cid:48) ← hγz
(m1(cid:48), . . . , mn(cid:48)) ← (P 1
N (h) ← N (h) + 1
R ← r + β · SIMULATE(x(cid:48), m1(cid:48), . . . , mn(cid:48), h(cid:48), d + 1)
N (hγ) ← N (hγ) + 1
V (hγ) ← V (hγ) + R−V (hγ)
N (hγ)
return R

L(m1, u1, y1, z1), . . . , P n

log N (h)
N (hδ)

L(mn, un, yn, zn))

as well as the cost of an agent’s defense decision, depend
on all agents’ defense actions which are not known by other
agents. Furthermore, inference of the security status of the
system depends on security alerts from all IDSs; however,
each agent only receives local alerts from their own IDS.

We assume that agents can share such information (de-
fense decisions and security alerts) via a centralized database;
however, due to practical limitations, the updating of this
database cannot be done instantaneously. There is an inherent
delay in updating the information. The delayed sharing
information structure [2], [5], a special case of the general
model discussed in this paper, formalizes this interaction. As
described in Section II-B, the general form of each agent’s
belief is a joint distribution over the underlying state and the
local information of others (its unknown information). In this
example, under the delayed sharing information structure,
each agent’s belief is the joint distribution over the security
state and the information (joint defense actions and security
alerts) that is not yet available in the central database.

A. A Small Example

We study a small instance of the collaborative intrusion
response model consisting of n = 2 agents. The cyber
network is represented by the condition dependency graph
of Fig. 1. Each agent can control the status of a set of
exploits; agent 1 controls e1, e2, e3, e4, e6 and agent 2 con-
trols e5, e7, e8, e9, e10. Each agent’s action space is U i =
{0, 1}, where 1 (resp. 0) corresponds to all exploits under its
control being blocked (resp. not blocked). The threat model
is described by uniform probabilities of attack and success,
αj = βj = 0.5 for all ej ∈ E. Each agent’s IDS generates
a single alert, A1 = {a1}, A2 = {a2}, with A(ej) =
a1, j = 1, 2, . . . , 7, and A(ej) = a2, j = 4, 5, . . . , 10,
with probabilities of detection δj1 = 0.8 for j = 1, 2, 3;
δj1 = 0.1, δj2 = 0.3 for j = 4, 5, 6, 7; δj3 = 0.8 for

Fig. 1: The dependency graph of the example. The graph consists of
ten exploits (e.g. e1 = (∅, {s1}), e4 = ({s1, s2}, {s5}), etc.) and
nine security conditions, with goal conditions s8 and s9 represented
by double-encircled nodes.

j = 8, 9, 10, zero otherwise. Probabilities of false alarm are
ζ1 = ζ2 = 0.3. Each agent’s observation space is deﬁned
as Y i = {0, 1} designating the presence of an alert from
its IDS. The instantaneous cost function c : S × U → R
is c(s, u) = c(s) + c(u), where c(s) = 5 if s8, s9 ∈ s,
zero otherwise, and c(u) = 0 for u = (0, 0); c(u) = 1 for
u = (0, 1), u = (1, 0); and c(u) = 4 for u = (1, 1).

t = {ui

We assume that defense actions and security alerts are
subject to a 1-step delay before becoming available in the
central database (common history). Thus, the local memory
at time t of each agent i is given by mi
t} ∈
Mi
t = U i × Y i = {0, 1}2. The information in the database
at time t is ct = {u1:t−2, y1:t−1} whereas innovations are
given by zi
t, yi
t}, with
joint innovation z ∈ Z = U ×Y = {0, 1}2×{0, 1}2, resulting
in ct+1 = {ct, zt+1} = {u1:t−1, y1:t}. Local memories are
updated as mi
t, ui
t+1}.
The number of prescriptions is |Γt| = (cid:81)2
t| =
(cid:81)2
i=1 |U i||U i×Y i| = 24 · 24 = 256 for all t.

t, yi
i=1 |U i||Mi

t+1) = {ui

t+1 = P i

t+1) = mi

t+1 = P i

t = {ui

t−1, yi

t−1, yi

t+1, zi

Z(mi

L(mi

t, ui

t, yi

attacker’s progression[5] A. Nayyar, A. Mahajan, and D. Teneketzis, “Decentralized stochastic
control with partial history sharing: A common information approach,”
IEEE Trans. Automat. Contr., vol. 58, no. 7, pp. 1644–1658, 2013.
[6] F. A. Oliehoek and C. Amato, “Dec-POMDPs as non-observable

MDPs,” IAS Tech. Rep., no. IAS-UVA-14-01, 2014.

[7] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, “Dynamic program-
ming for partially observable stochastic games,” in Proc. AAAI Conf.
on Artiﬁcial Intell., vol. 4, 2004, pp. 709–715.

[8] S. Seuken and S. Zilberstein, “Improved memory-bounded dynamic
programming for decentralized POMDPs,” in Proc. Conf. on Uncer-
tainty in Artiﬁcial Intell., 2007.

[9] F. A. Oliehoek, “Sufﬁcient plan-time statistics for decentralized
POMDPs.” in Proc. Int. Joint Conf. on Artiﬁcial Intell., 2013, pp.
302–308.

[10] J. S. Dibangoye, C. Amato, O. Buffet, and F. Charpillet, “Optimally
solving Dec-POMDPs as continuous-state MDPs,” J. of Artiﬁcial
Intell. Res., vol. 55, pp. 443–497, 2016.

[11] S. Y¨uksel and T. Bas¸ar, Stochastic Networked Control Systems.

Birkh¨auser/Springer, 2013, vol. 10.

[12] J. Dibangoye and O. Buffet, “Learning to act in decentralized partially
observable MDPs,” in Proc. Int. Conf. on Mach. Learning, 2018, pp.
1233–1242.

[13] M. Gerla, E.-K. Lee, G. Pau, and U. Lee, “Internet of vehicles: From
intelligent grid to autonomous cars and vehicular clouds,” in Internet
of Things, IEEE World Forum on.

IEEE, 2014, pp. 241–246.

[14] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar, “Fully decen-
tralized multi-agent reinforcement learning with networked agents,”
in Proc. Int. Conf. on Mach. Learning, 2018, pp. 5872–5881.
[15] D. Silver and J. Veness, “Monte-Carlo planning in large POMDPs,”
in Advances in Neural Inf. Process. Syst., 2010, pp. 2164–2172.
[16] D. Szer, F. Charpillet, and S. Zilberstein, “MAA*: A heuristic search
algorithm for solving decentralized POMDPs,” in Proc. Conf. on
Uncertainty in Artiﬁcial Intell., 2005, pp. 576–583.

[17] F. Wu, S. Zilberstein, and N. R. Jennings, “Monte-carlo expectation
maximization for decentralized POMDPs.” in Proc. Int. Joint Conf. on
Artiﬁcial Intell., 2013, pp. 397–403.

[18] C. Amato, F. A. Oliehoek et al., “Scalable planning and learning for
multiagent POMDPs,” in Proc. AAAI Conf. on Artiﬁcial Intell., 2015,
pp. 1995–2002.

[19] G. Best, O. M. Cliff, T. Patten, R. R. Mettu, and R. Fitch, “Dec-MCTS:
Decentralized planning for multi-robot active perception,” The Int. J.
of Robotics Res., pp. 1–22, 2018.

[20] J. Arabneydi and A. Mahajan, “Reinforcement learning in decentral-
ized stochastic control systems with partial history sharing,” in Proc.
Amer. Contr. Conf.
IEEE, 2015, pp. 5449–5456.

[21] F. A. Oliehoek and M. T. Spaan, “Tree-based solution methods for
multiagent POMDPs with delayed communication,” in Proc. AAAI
Conf. on Artiﬁcial Intell., 2012.

[22] K. Zhang, Z. Yang, and T. Bas¸ar, “Networked multi-agent reinforce-
ment learning in continuous spaces,” in Proc. Conf. on Decision and
Contr.

IEEE, 2018.

[23] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar, “Finite-sample
analyses for fully decentralized multi-agent reinforcement learning,”
arXiv preprint arXiv:1812.02783, 2018.

[24] D. S. Bernstein, C. Amato, E. A. Hansen, and S. Zilberstein, “Policy
iteration for decentralized control of Markov decision processes,” J.
of Artiﬁcial Intell. Res., vol. 34, pp. 89–132, 2009.

[25] T. Bas¸ar and G. J. Olsder, Dynamic Noncooperative Game Theory.

SIAM, 1999, vol. 23.

[26] C. Amato, G. Chowdhary, A. Geramifard, N. K. Ure, and M. J.
Kochenderfer, “Decentralized control of partially observable Markov
Decision Processes,” in Proc. Conf. on Decision and Contr.
IEEE,
2013, pp. 2398–2405.

[27] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the
multiarmed bandit problem,” Mach. Learning, vol. 47, no. 2-3, pp.
235–256, 2002.

[28] E. Vasilomanolakis, S. Karuppayah, M. M¨uhlh¨auser, and M. Fischer,
“Taxonomy and survey of collaborative intrusion detection,” ACM
Comput. Surveys, vol. 47, no. 4, p. 55, 2015.

[29] P. Ammann, D. Wijesekera, and S. Kaushik, “Scalable, graph-based
network vulnerability analysis,” in Proc. Conf. on Comput. and Com-
mun. Security. ACM, 2002, pp. 217–224.

[30] E. Miehling, M. Rasouli, and D. Teneketzis, “A POMDP approach to
the dynamic defense of large-scale cyber networks,” IEEE Trans. Inf.
Forensics Security, vol. 13, no. 10, pp. 2490–2505, 2018.

Fig. 2: Discounted cost for a given time-step (t = 5) under various
simulation counts. Each point represents the cost at t for a distinct
sample path for the given value of nsim. The gray line represents
the mean discounted cost.

B. Numerical Results

We investigate the quality of the resulting policy, as
computed by Algorithm 1, as a function of the number of
simulations used to compute each action, denoted by nsim.
We assume a discount factor β = 0.8, discount horizon
threshold ε = 0.1, exploration constant ρ = 10, particle
count K = 400, and a uniform random rollout policy. The
performance of the computed policy is illustrated in Fig.
2. The ﬁgure empirically veriﬁes the convergence result in
Section III-C, i.e., the discounted cost decreases as nsim
increases.

V. DISCUSSION AND CONCLUDING REMARKS

In this paper, we have proposed an online tree-search
based algorithm for obtaining team-optimal policies in de-
centralized stochastic control problems when agents share
some of their history with others. Our algorithm enables
each agent to obtain approximately optimal control policies
without explicit communication or model knowledge. We
have also shown that two recent algorithms that solve Dec-
POMDPs [6], [10] can be viewed as special cases of our
algorithm. Lastly, we have demonstrated the performance of
our algorithm in a novel computer security setting.

We are not the ﬁrst to consider the applicability of tree-
search methods in multi-agent decision environments. In
fact, tree-search methods do not scale well for decentralized
problems due to the large joint action/observation spaces in
the multi-agent setting [18]. That said, by leveraging com-
mon information, our algorithm serves as an initial step to-
wards developing tractable sampling-based planning/learning
algorithms for decentralized stochastic control problems,
especially without full knowledge of the system model.

REFERENCES

[1] A. Nayyar, A. Mahajan, and D. Teneketzis, “Optimal control strategies
in delayed sharing information structures,” IEEE Trans. Automat.
Contr., vol. 56, no. 7, pp. 1606–1620, 2011.

[2] H. S. Witsenhausen, “Separation of estimation and control for discrete
time systems,” Proc. of the IEEE, vol. 59, no. 11, pp. 1557–1566, 1971.
[3] C. Striebel, “Sufﬁcient statistics in the optimum control of stochastic
systems,” J. of Math Anal. & Appl., vol. 12, no. 3, pp. 576–592, 1965.
[4] P. R. Kumar and P. Varaiya, Stochastic Systems: Estimation, Identiﬁ-

cation, and Adaptive Control. Prentice Hall, NJ, 1986.

400600800100012001400160000.511.52