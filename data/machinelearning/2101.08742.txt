Soft Genetic Programming Binary Classiﬁers

Ivan Gridin
ivan.gridin.pro@gmail.com

January 22, 2021

Abstract

The study of the classiﬁer’s design and it’s usage is one of the most important machine learning areas.
With the development of automatic machine learning methods, various approaches are used to build a robust
classiﬁer model. Due to some diﬃcult implementation and customization complexity, genetic programming
(GP) methods are not often used to construct classiﬁers. GP classiﬁers have several limitations and
disadvantages. However, the concept of "soft" genetic programming (SGP) has been developed, which allows
the logical operator tree to be more ﬂexible and ﬁnd dependencies in datasets, which gives promising results
in most cases. This article discusses a method for constructing binary classiﬁers using the SGP technique.
The test results are presented. Source code - https://github.com/survexman/sgp_classifier

1

Introduction

Genetic Programming (GP) is a promising machine learning technique based on the principles of Darwinian
evolution to automatically evolve computer programs to solve problems. GP is especially suitable for building
a classiﬁer of tree representation. GP is a soft computing search technique, which is used to evolve a
tree-structured program toward minimizing the ﬁtness value of it. The distinctive features of GP make it very
convenient for classiﬁcation, and the beneﬁt of it is the ﬂexibility, which allows the algorithm to be adapted
to the needs of each particular problem.

A special case of GP studies the logical tree’s development as a solution to a classiﬁcation problem [1].
Logical trees are composed of boolean, comparison, and arithmetic operators, and they output a boolean
value ( true or false). A solution presented as a logical tree is a very convenient way to analyze a dataset and
interpret a solution. Figure 1 shows an example of logical tree.

1
2
0
2

n
a
J

1
2

]

G
L
.
s
c
[

1
v
2
4
7
8
0
.
1
0
1
2
:
v
i
X
r
a

Figure 1: Logical tree example

1

 
 
 
 
 
 
This logical tree can be rewritten as the following system of inequalities:

x2 ≥ 7 ∨

y

y






≥ 3
≤ 8
z < x

Evolving logical trees using GP has the beneﬁt of being able to handle both numerical and categorical data
fairly simply. Signiﬁcant advantage of evolving logical trees is the fact that the trees are highly interpretable to
a researcher [4]. Logical trees are portable and can be easily implemented by various tools and programming
languages. Another signiﬁcant advantage is an ability of feature extraction [6].

However, one of the most critical problems in logical trees is a crucial logic change when a tree operator

changes [9][11]. Such operator changes occur as a result of crossover and mutation operations [2].

Figure 2: Operator change in Logical Tree

Figure 2 shows a simple change of addition operator to power operator in logical tree:

[x > 3 ⊕ sin(x(y + 15))) > 0.5]

.

−→ [x > 3 ⊕ sin(xy15))) > 0.5]

And below at Figure 3 we can see the eﬀect of such modiﬁcation:

2

Figure 3: Operator change eﬀect

We see that switching the operator +(y,15) to pow(y,15) changes the logic of the classiﬁer drastically.
The more complex the logical tree is, the more this feature is noticeable. This sensitivity of logical trees to
tiny changes makes it very diﬃcult to ﬁnd the appropriate logical tree in a smooth way.

2 Study Roadmap

SGP classiﬁer design is based on classical GP classiﬁer design. First, we will deﬁne the architecture of
GP classiﬁer. Next, we will introduce the Soft Genetic Programming approach, its design, and evolution
operations. Soft Genetic Programming’s main goal is to smoothen evolution improvement and increase the
probability of reaching local maxima. After, we will compare the behavior of classical GP Classiﬁer and SGP
Classiﬁer. And ﬁnally, we will present empirical evidence of the applicability of SGP approach.

3

3 Genetic Programming Classiﬁer Design

Genetic programming (GP) is a ﬂexible and powerful evolutionary technique with some special features that
are suitable for building a classiﬁer of tree representation. [2].

3.1 Operators

We will use the following operators for genetic programming trees [3]:

Boolean:

OR : {0, 1}2 → {0, 1}
AND : {0, 1}2 → {0, 1}
NOT : {0, 1} → {0, 1}

Comparison:

> : R2 → {0, 1}
< : R2 → {0, 1}

Mathematical:

+ : R2 → R
× : R2 → R
− : R → R

Terms:

Symbolic : → Xi

i ∈ 1, n, where n − number of f eatures

Constant : → R

3.2 Random Tree Generation

Random tree generation is limited by maximum and minimum operator type subchain [8]:

min max

Boolean

Comparison

Mathematical

Terms

1

1

1

1

3

1

4

1

4

3.3 Fitness Function

For ﬁtness function we use Balanced Accuracy:

Balanced Accuracy =

1
2

(

T P
T P + F N

+

T N
T N + F P

)

3.4 Crossover

The crossover operator creates two new oﬀspring which are formed by taking parts (genetic material) from
two parents. The operator selects two parents from the population based on a selection method. A crossover
point is then randomly selected in both trees, say point p1 and p2 , from tree t1 and t2 respectively. The
crossover then happens as follows: the subtree rooted at p1 is removed from t1 and inserted into the position
p2 in t2. The same logic applies to the point p2 ; the subtree root at the point is removed from t2 and inserted
into the place of p1 in t1 [4]. Figure 4 illustrates the crossover operation.

Figure 4: Crossover

5

3.5 Operator Mutation

An Operator Mutation works the following way: an operator is randomly selected and replaced with a random
tree. A new randomly generated subtree is inserted instead of selected operator [2]. Figure 5 illustrates the
operator mutation operation.

Figure 5: Mutation

3.6 Term Mutation

Term mutation never replaces a term by a random tree. The term mutation is deﬁned as follows:

if X is Symbolic, then X is replaced by random Xi i ∈ [1, n], where n is number of f eatures in dataset

if X is Constant, then X is replaced by X + r where r is normal random variable (0; 1)

6

3.7 Mutation Probabilities

During mutation each operator type is being randomly chosen with following probability table:

Operator Type Probability

Boolean

Comparison

Mathematical

Terms

0.1

0.2

0.3

0.5

3.8 Selection

Rank selection with elite size 1 used as the selection operation [2].

3.9 Evolution Algorithm

Evolution is implemented via canonical genetic algorithm method [5]:

Algorithm 1: GP Classiﬁer Evolution Algorithm

Result: Best individual
1 max_generation = 100;
2 population_size = 100;
3 cx_prob = 0.5;
4 mut_prob = 0.5;
5 population = random_population(population_size);
6 best_ind = select_best(population);
7 generation = 0;
8 while best_ind.ﬁtness < 1 or generation < max_generation do
9

10

11

12

selected_population = selection(population) ;
crossed_population = crossover(selected_population, cx_prob) ;
mutated_population = mutation(crossed_population, mut_prob) ;
population = mutated_population best_ind = select_best(population);
generation++;

13
14 end
15 return best_ind

7

4 Soft Genetic Programming Classiﬁer Design

The main idea of SGP is the usage of weighted continuous functions instead of discontinuous boolean and
comparison functions. Weight setting will allow calibrating each operator’s eﬀect in a pseudo logical tree,
leading to a more adaptive classiﬁer design.

4.1 Soft Operators

We change boolean and comparison operators to weighted continuous analogs [10] (we will call those function
sets pseudo boolean and pseudo comparison operators):

Pseudo Boolean:

OR:

[0, 1] × [0, 1]2
∈
(w; x, y)

[0, 1]
∈
w · max(x, y)

AN D:

[0, 1] × [0, 1]2
∈
(w; x, y)

[0, 1]
∈
w · min(x, y)

N OT :

[0, 1] × [0, 1]
∈
(w; x)

[0, 1]
∈
w(1 − x)

Also we will add additional operators:

AN D3:

[0, 1] × [0, 1]3
∈
(w; x, y, z)

[0, 1]
∈
w · min(x, y, z)

OR3:

[0, 1] × [0, 1]3
∈
(w; x, y, z)

[0, 1]
∈
w · max(x, y, z)

Pseudo Comparison:

>:

<:

[0, 1] × R2
∈
(w; x, y)

[0, 1] × R2
∈
(w; x, y)

[0, 1]
∈
w( x−y
|x−y|

)

[0, 1]
∈
w( y−x
|x−y|

)

Mathematical:

+: R2
∈
(x, y)

R
∈
x + y

8

: R2
∈
(x, y)

−: R
∈
x

R
∈
xy

R
∈
−x

Also we will add additional nonlinear operators:

sigm: R
∈
x

R
∈
1
1+e(−x)

lin2: R2 × R2

∈
(a, b; x, y)

lin3: R3 × R3

∈
(ai; xi)

R
∈
ax + by

R
∈

P aixi

Terms:

Symbolic : → Xi

i ∈ 1, n, where n − number of f eatures

Constant : → R

Weights:

W eight : → w , where w is random variable unif ormly distributed on [0, 1]

9

4.2 Soft Tree Representation

SGP tree is a tree where each pseudo boolean and comparison operator has its weight parameter, as is shown
on Figure 6:

Figure 6: SGP Tree

10

4.3 Random Tree Generation

See 3.2

4.4 Fitness Function

See 3.3

4.5 Weight Adjustment

In SGP we introduce weight adustment operation:

Algorithm 2: Weight Adjustment

procedure weight_adjustment(individual, max_tries);

for i in max_tries do

candidate = copy(individual);
coordinate, w = individual.get_random_weight();
new_w = w + random_shift();
candidate.set_weight(coordinate, new_w);
if candidate.ﬁtness > individual.ﬁtness then

return candidate

end

end
return individual

The main point of weight adjustment operation is to ﬁnd any positive improvement using weight calibration.

4.6 Fitness Driven Genetic Operations

The main problem of classiﬁer trees is that they have very fragile structures, and the probability of degradation
after canonical crossover and mutation operations is to high. In the SGP evolution algorithm, we will use
only positive improvement evolution operations.

Positive Crossover doesn’t accept an oﬀsrping which is worse then its parents:

Algorithm 3: Positive Crossover

procedure positive_crossover(ind1, ind2);
child1, child2 = crossover(ind1, ind2);
candidate1, candidate2 = max([ind1,ind2,child1,child2], by=ﬁtness, 2);
return [candidate1, candidate2]

Positive Mutation accepts only those mutations which improves individual’s genome:

Algorithm 4: Positive Mutation

procedure positive_mutation(ind1, max_tries);

for i in max_tries do

mutant = mutate(ind);
if mutant.ﬁtness > ind.ﬁtness then

return mutant

end

end
return ind

11

4.7 Extension Mutation

There is a handy technique for avoiding stucking an improvement of the population in some speciﬁc random
subspace. A good way of an individual improvement is adding OR operator as tree root with random subtree,
as it is shown on the Figure 7.

The Extension Mutation operation increases the probability of positive genome improvement.

Figure 7: Extension Mutation

12

4.8 Multiple Population

The usage of ﬁtness driven crossover and mutation operation provokes the problem of lacking gene variation.
This problem is solved using the multiple population technique. On each Nth generation the best individual
of each population is "thrown" to the sequent population [13].

Figure 8: Multiple Population

13

4.9 Evolution Algorithm

SGP evolution algorithm:

Algorithm 5: SGP Classiﬁer Evolution Algorithm

populations[i] = random_population(population_size);

Result: Best individual
1 max_generation = 100;
2 population_size = 100;
3 population_num = 4;
4 cx_prob = 0.5;
5 mut_prob = 0.5;
6 populations = [] ;
7 for i in population_num do
8
9 end
10 best_inds = [] ;
11 for i in population_num do
12
13 end
14 best_ind_ever = max(best_inds, key = ’ﬁtness’)
15 generation = 0;
16 while best_ind_ever.ﬁtness < 1 or generation < max_generation do
17

best_inds[i] = select_best(populations[i]);

for i in population_num do

18

19

20

21

22

23

24

25

26

27

28

29

30

31

selected_population = selection(populations[i]) ;
crossed_population = crossover(selected_population, cx_prob) ;
mutated_population = mutation(crossed_population, mut_prob) ;
weighted_population = weight_adjustment(mutated_population) ;
extended_population = weight_adjustment(weighted_population) ;
populations[i] = extended_population ;
best_inds[i] = select_best(populations[i]);

end
if generation mod 5 == 0 then
for i in population_num do

populations[i+1].append(best_inds[i]);

end

end
best_ind_ever = max(best_inds, key = ’ﬁtness’);
generation++;

32
33 end
34 return best_ind_ever

14

5 Visualization

Let’s compare the behavior of GP and SGP Classiﬁer on generated 2D datasets [7].

Figure 9: Linearly separable dataset

Figure 10: Large circle containing a smaller circle dataset

15

Figure 11: Two interleaving half circles dataset

We can see that both GP and SGP classiﬁers show conﬁdent behavior. As it could be expected, the
SGP Classiﬁer tends to use nonlinear dependencies. SGP classiﬁer highly likely have strict borders(i.e.
µ({SGP (x, y) ∈]0, 1[ }) ∼ 0), as it is common behavior for Decision Trees.

16

6 Experimental Results

We have tested SGP and GP classiﬁers using Large Benchmark Suite [12] for binary classiﬁcation problem.
As a classiﬁcation quality score we used balanced accuracy.

Test results were gathered by following testing algorithm:

Algorithm 6: Testing algorithm
1 for dataset in datasets do
for i in [1,20] do
2

3

4

5

6

7

shuﬄedDataset = shuﬄe(dataset) ;
train, test = split(shuﬄedDataset, .7) ;
for cls in classiﬁers do

cls.ﬁt(train) score = balancedAccuracy(cls, test)

end

end

8
9 end

17

Figure 12: Test Results. GP Classiﬁer - pink, SGP Classifer - blue

18

Below we provide a heatmap table view with mean balanced accuracy results:

prnn crabs heart h

crx

haberman breast

ﬂare

SGP Classiﬁer

0.978

0.7752

0.7752

0.6792

0.9559

0.7023

GP Classiﬁer

0.9724

0.7597

0.7597

0.6522

0.9464

0.6856

ADA

0.9095

0.7594

0.7594

0.5752

0.9358

0.5613

Decision Tree

0.8815

0.7312

0.7312

0.5585

0.9328

0.5654

Gausian Process

0.994

0.7925

0.7925

0.5981

0.953

0.534

Gausian NB

KNeighbors

0.632

0.748

0.745

0.5748

0.9605

0.6305

0.9079

0.7654

0.7654

0.5857

0.96

0.5727

Neural Network

0.9734

0.6429

0.6429

0.5

0.9517

0.5229

Random Forest

0.8249

0.769

0.769

0.5702

0.958

0.5196

pima

german heart c

credit g buddyCrx prnn synth

SGP Classiﬁer

0.7181

0.6791

0.7929

0.674

0.8559

0.8642

GP Classiﬁer

0.7176

0.6778

0.7938

0.6668

0.8537

0.8543

ADA

0.703

0.6667

0.782

0.6536

0.8427

0.8342

Decision Tree

0.7009

0.6394

0.7661

0.6151

0.8309

0.8368

Gausian Process

0.7288

0.6255

0.8304

0.6101

0.8614

0.8886

Gausian NB

KNeighbors

0.7218

0.6633

0.8127

0.6637

0.7866

0.844

0.6886

0.588

0.8191

0.5941

0.8517

0.8551

Neural Network

0.7168

0.6255

0.8206

0.6041

0.8598

0.8629

Random Forest

0.6714

0.5297

0.8146

0.5197

0.828

0.846

SGP provides very positive average results, and especially nice at haberman and ﬂare datasets. All results
can be regathered running script https://github.com/survexman/sgp_classifier/blob/main/soft/gp_
classification.py

19

7 Conclusion

This survey introduces the concept of Soft Genetic Programming. The robustness of the SGP Classiﬁer is
presented. This research aimed to deliver a new genetic programming design with pseudo boolean and pseudo
comparison operators and show its quality.

Never the less SGP has several drawbacks:

• Performance issue. SGP training stage takes much more time than classical classifers do.

• An incapability to use SGP as a feature selection tool. Due to the weighted operator design, each branch
and operator can have a very low weight. Thus the term that belongs to the weighted operator has
low signiﬁcance and cannot be used as a signiﬁcant feature. In classical GP Classiﬁer, each symbolic
variable highly likely has a high signiﬁcance degree and can thus be selected as meaningful feature.

Anyway, the improvement of the SGP technique for a particular task can provide excellent practical

results.

20

References

[1] Chan-Sheng Kuo and T. Hong and Chuen-Lung Chen Applying genetic programming technique in

classiﬁcation trees, 2007 Soft Computing, vol. 11, pages 1165-1172

[2] Koza, John R. Genetic Programming: On the Programming of Computers by Means of Natural Selection

1992 0-262-11170-5 MIT Press Cambridge, MA, USA

[3] Bhowan Urvesh, Zhang Mengjie and Johnston Mark Genetic Programming for Classiﬁcation with

Unbalanced Data 2010 Springer Berlin Heidelberg pages 1-13 ISBN: 978-3-642-12148-7

[4] Emmanuel. Dufourq Data classiﬁcation using genetic programming, 2015

[5] D. E. Goldberg Genetic Algorithms in Search, Optimization and Machine Learning Boston, MA, USA:

Addison-Wesley Longman Publishing Co., Inc., 1st ed., 1989.

[6] Suarez Ranyart, Valencia-Ramírez, José and Graﬀ Mario. Genetic programming as a feature selection
algorithm. 2014 IEEE International Autumn Meeting on Power, Electronics and Computing, ROPEC
2014. 1-5. 10.1109/ROPEC.2014.7036345. (2014)

[7] scikit-learn.org

Classiﬁer

comparison

https://scikit-learn.org/stable/auto_examples/

classification/plot_classifier_comparison.html

[8] Silva, Sara and Almeida, Jonas, Dynamic Maximum Tree Depth, Genetic and Evolutionary Computation

— GECCO 2003, 1776–1787 Springer Berlin Heidelberg ISSN 978-3-540-45110-5

[9] Hamed Hatami. Decision trees and inﬂuences of variables over product probability spaces, 2006;

arXiv:math/0612405.

[10] O’Donnell Ryan

Analysis of Boolean Functions, 2014

Cambridge University Press DOI:

10.1017/CBO9781139814782

[11] Moulinath Banerjee and Ian W. McKeague. Conﬁdence sets for split points in decision trees, 2007,
Annals of Statistics 2007, Vol. 35, No. 2, 543-574; arXiv:0708.1820. DOI: 10.1214/009053606000001415.

[12] Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz and Jason H. Moore. PMLB:

A Large Benchmark Suite for Machine Learning Evaluation and Comparison, 2017; arXiv:1703.00512.

[13] Søren B. Vilsen, Torben Tvedebrink and Poul Svante Eriksen. DNA mixture deconvolution using an evolu-
tionary algorithm with multiple populations, hill-climbing, and guided mutation, 2020; arXiv:2012.00513.

21

