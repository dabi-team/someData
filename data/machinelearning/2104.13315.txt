1

1
2
0
2

r
p
A
7
2

]
L
P
.
s
c
[

1
v
5
1
3
3
1
.
4
0
1
2
:
v
i
X
r
a

Inductive Program Synthesis over Noisy Datasets using
Abstraction Refinement Based Optimization

SHIVAM HANDA, Massachusetts Institute of Technology, USA
MARTIN RINARD, Massachusetts Institute of Technology, USA

We present a new synthesis algorithm to solve program synthesis over noisy datasets, i.e., data that may
contain incorrect/corrupted input-output examples. Our algorithm uses an abstraction refinement based
optimization process to synthesize programs which optimize the tradeoff between the loss over the noisy
dataset and the complexity of the synthesized program. The algorithm uses abstractions to divide the search
space of programs into subspaces by computing an abstract value that represents outputs for all programs
in a subspace. The abstract value allows our algorithm to compute, for each subspace, a sound approximate
lower bound of the loss over all programs in the subspace. It iteratively refines these abstractions to further
subdivide the space into smaller subspaces, prune subspaces that do not contain an optimal program, and
eventually synthesize an optimal program.

We implemented this algorithm in a tool called Rose. We compare Rose to a current state-of-the-art noisy
program synthesis system [Handa and Rinard 2020] using the SyGuS 2018 benchmark suite [Alur et al. 2013].
Our evaluation demonstrates that Rose significantly outperforms this previous system: on two noisy benchmark
program synthesis problem sets drawn from the SyGus 2018 benchmark suite, Rose delivers speedups of up to
1587 and 81.7, with median speedups of 20.5 and 81.7. Rose also terminates on 20 (out of 54) and 4 (out of 11)
more benchmark problems than the previous system. Both Rose and the previous system synthesize programs
that are optimal over the provided noisy data sets. For the majority of the problems in the benchmark sets
(272 out of 286), the synthesized programs also produce correct outputs for all inputs in the original (unseen)
noise-free data set. These results highlight the benefits that Rose can deliver for effective noisy program
synthesis.

Additional Key Words and Phrases: Program Synthesis, Machine Learning, Noisy datasets, Abstraction
Refinement

1 Introduction

Program synthesis has been successfully used to synthesize programs from examples, for domains
such as string transformations [Gulwani 2011; Singh and Gulwani 2016], data wrangling [Feng
et al. 2017], data completion [Wang et al. 2017b], and data structure manipulation [Feser et al. 2015;
Osera and Zdancewic 2015; Yaghmazadeh et al. 2016]. In recent years, there has been interest in
synthesizing programs from input-output examples in presence of noise/corruptions [Handa and
Rinard 2021, 2020; Peleg and Polikarpova 2020; Raychev et al. 2016]. This line of work aims to
tackle real world datasets which contain noise and corruptions. These techniques can synthesize
the correct programs, even in presence of substantial noise [Handa and Rinard 2020].

Noisy program synthesis has been formulated as an optimization problem over the program space
and the noisy dataset [Handa and Rinard 2021, 2020]. The optimization problem is parameterized
with three functions: a loss function, which measures by how much a program’s output differs from
the output in the given noisy data set, a complexity measure, which measures the complexity of a
candidate program, and an objective function, which combines these both of these scores to rank
programs [Handa and Rinard 2021, 2020]. Given a search space and a set of noisy input/output
pairs, the task of the noisy program synthesis is to synthesize a program which minimizes the
objective function over the noisy input/output pairs.

Authors’ addresses: Shivam Handa, EECS, Massachusetts Institute of Technology, USA, shivam@mit.edu; Martin Rinard,
EECS, Massachusetts Institute of Technology, USA, rinard@csail.mit.edu.

 
 
 
 
 
 
1:2

Shivam Handa and Martin Rinard

Similar to noise-free program synthesis, noisy program synthesis is effectively a search problem.
Working with noisy datasets further complicates the search: synthesizing a program which simply
maximizes the number of input-output examples it satisfies (is correct on) may not optimize the
objective function over the entire dataset. Moreover, recent research has demonstrated that noisy
program synthesis can often synthesize the correct program even when all input/output examples
are corrupted [Handa and Rinard 2020].

Current solutions to noisy program synthesis, either fall into enumeration based search techniques
[Peleg and Polikarpova 2020] or version space (finite tree automaton) based techniques [Handa
and Rinard 2020]. Both of these techniques reduce the search space by partitioning the space of
programs based on their execution behaviour on the given inputs. Both exploit the property that
programs which produce the same output values on the given input values will have the same
loss on a given dataset. If we restrict our search space to a single partition (instead of all programs
within the given program space) then the simplest program (based on the complexity measure)
will be the optimal solution to the noisy program synthesis problem. This is due to the fact that
all programs within this space have the same loss value. Therefore the program which minimizes
the complexity measure, minimizes the objective function. If the simplest program within this
partition is not the optimal program (i.e, there exists another program in the search space which
further reduces the value of the objective function) then we can safely conclude no other program
within this partition is the optimal program. With this knowledge, iterating over all partitions and
comparing their simplest program allows these techniques to synthesize the optimal program. The
performance of these techniques is determined by the number of partitions that are created, given
a dataset.

This partitioning approach has also been used in traditional noise-free program synthesis
settings [Gulwani 2011; Wang et al. 2017b] and horn-clause verification [Kafle and Gallagher
2015]. A potential solution to reducing the number of partitions was proposed by Kafle at al. in
the context of horn clause verification using tree automata [Kafle and Gallagher 2015]. Wang et al
introduced this technique to noise-free program synthesis [Wang et al. 2017a]. The technique uses
abstract output values to partition the partition the program space, instead of concrete output values.
An abstract value is a compact representation of a set of concrete values. Both of these techniques
associate partitions of their search space to abstract values. For example, [Wang et al. 2017a]
represents a partition with an array of abstract output values and contains all programs which,
given input values, maps these inputs to an array of concrete outputs, where each output value is
an element of the corresponding abstract output value. Abstract values allow these techniques to
reduce the number of partitions and hence decrease the running time of the synthesis algorithm.
Our work applies the abstract value based partitioning approach to create an abstraction
refinement based algorithm for solving the noisy program synthesis problem. Our technique
uses abstractions to partition the program space, with the abstract value of each partition enabling
us to soundly and approximately estimate a lower bound on the loss value of a partition, i.e.,
the minimum loss of any program within that partition. This lower bound on the loss value,
attached with the simplest program within a given partition, allows us to effectively synthesize
candidate optimal programs. Given a candidate optimal program, our technique can effectively
prune out partitions, which even with the minimum possible loss value, will fail to contain the
optimal program. The remaining partitions, based on their lower bound loss value, may contain
programs which better fit the noisy dataset compared to our candidate optimal program (i.e., the
remaining partitions may contain programs that the object function ranks above the candidate
optimal program). The synthesis algorithm then refines these abstractions in order to further refine
the abstraction-based partitioning and improve its estimate of the minimum possible loss value.
The algorithm guarantees that it will eventually synthesize the optimal program.

Noisy Program Synthesis using Abstractions

1:3

We have implemented our algorithm in the Rose synthesis tool. Rose can be instantiated to
work in different domains by providing suitable domain specific languages, abstract semantics, and
concrete semantics of functions within the language. Rose is parameterized over a large class of
objective functions, loss functions, and complexity measures. [Handa and Rinard 2020] highlights
that this flexibility is required to synthesize correct programs for datasets which contain a large
amount of noise.

We Rose to a current state-of-the-art noisy program synthesis system [Handa and Rinard 2020]
using the SyGuS 2018 benchmark suite [Alur et al. 2013]. Our evaluation demonstrates that
Rose significantly outperforms this previous system: on two noisy benchmark program synthesis
problem sets drawn from the SyGus 2018 benchmark suite, Rose delivers speedups of up to 1587
and 81.7, with median speedups of 20.5 and 81.7. Rose also terminates on 20 (out of 54) and 4 (out
of 11) more benchmark problems than the previous system. Both systems synthesize programs that
are optimal over the provided noisy data sets. For the majority of the problems in the benchmark
sets (272 out of 286), both systems also synthesize programs that produce correct outputs for all
inputs in the original (unseen) noise-free data set. These results highlight the significant benefits
that Rose can deliver for effective noisy program synthesis.
Contributions: This paper makes the following key contributions:

• New Abstraction Technique: It presents a new program synthesis technique for synthesizing
programs over noisy datasets. This technique uses the abstract semantics of DSL constructs
to partition the program search space. For each partition, the technique uses the abstract
semantics to compute an abstract value representing the outputs of all programs in that
partition. The abstract value also allows the technique to soundly estimate the minimum
possible loss value over programs all programs in each partition.

• New Refinement Technique: It presents a new refinement technique that works with
the sound approximation of the minimum loss values to refine the current partition, then
discard partitions that cannot possibly contain the optimal program. Iteratively applying this
refinement technique delivers the program with the optimal loss function over the given
input/output examples.

• Rose Evaluation: It presents the Rose synthesis system, which implements the new abstraction
and refinement techniques presented in this paper. Our experimental evaluation shows that
Rose delivers substantial performance improvements over a current state of the art noisy
program synthesis system [Handa and Rinard 2020] (Section 5). In comparison with this
previous system, these performance improvements result in substantially smaller overall
program synthesis times and many fewer synthesis timeouts.

2 Preliminaries
We first review the noisy program synthesis framework (introduced by [Handa and Rinard 2021,
2020]), the concepts associated with this framework, and the conditions that qualify a program
to be the correct solution to a synthesis problem. We also discuss the tree automata based noisy
program synthesis technique proposed by [Handa and Rinard 2020].

2.1 Finite Tree Automata
Finite Tree Automata are a type of state machine which accept trees rather than strings. They
generalize standard finite automata to describe a regular language over trees.

Definition 2.1 (FTA). A (bottom-up) Finite Tree Automaton (FTA) over alphabet 𝐹 is a tuple
A = (𝑄, 𝐹, 𝑄 𝑓 , Δ) where 𝑄 is a set of states, 𝑄 𝑓 ⊆ 𝑄 is the set of accepting states and Δ is a set of
transitions of the form 𝑓 (𝑞1, . . . , 𝑞𝑘 ) → 𝑞 where 𝑞, 𝑞1, . . . 𝑞𝑘 are states, 𝑓 ∈ 𝐹 .

1:4

Shivam Handa and Martin Rinard

𝑡 ∈ 𝑇𝐶
⟦𝑡⟧𝑥 𝑗 ⇒ 𝑣𝑡

(Constant)

⟦𝑥⟧𝑥 𝑗 ⇒ 𝑥 𝑗

(Variable)

⟦𝑒1⟧𝑥 𝑗 ⇒ 𝑣1

⟦𝑒2⟧𝑥 𝑗 ⇒ 𝑣2

. . .

⟦𝑒𝑘 ⟧𝑥 𝑗 ⇒ 𝑣𝑘

⟦𝑓 (𝑒1, 𝑒2, . . . 𝑒𝑘 )⟧𝑥 𝑗 ⇒ 𝑓 (𝑣1, 𝑣2, . . . 𝑣𝑘 )

(Function)

Fig. 1. Execution semantics for program 𝑝

Every symbol 𝑓 in the set 𝐹 has an associated arity. The set 𝐹𝑘 ⊆ 𝐹 is the set of all 𝑘-arity symbols
in 𝐹 . 0-arity terms 𝑡 in 𝐹 are viewed as single node trees (leaves of trees). 𝑡 is accepted by an FTA if
we can rewrite 𝑡 to some state 𝑞 ∈ 𝑄 𝑓 using rules in Δ. We use the notation L (A) to denote the
language of an FTA A, i.e., the set of all trees accepted by A.

Example 2.2. Consider the tree automaton A defined by states 𝑄 = {𝑞𝑇 , 𝑞𝐹 }, 𝐹0 = {True, False},

𝐹1 = not, 𝐹2 = {and}, final states 𝑄 𝑓 = {𝑞𝑇 } and the following transition rules Δ:

True → 𝑞𝑇
and(𝑞𝑇 , 𝑞𝑇 ) → 𝑞𝑇
or(𝑞𝑇 , 𝑞𝑇 ) → 𝑞𝑇

False → 𝑞𝐹
and(𝑞𝐹 , 𝑞𝑇 ) → 𝑞𝐹
or(𝑞𝐹 , 𝑞𝑇 ) → 𝑞𝑇

not(𝑞𝑇 ) → 𝑞𝐹
and(𝑞𝑇 , 𝑞𝐹 ) → 𝑞𝐹
or(𝑞𝑇 , 𝑞𝐹 ) → 𝑞𝑇

not(𝑞𝐹 ) → 𝑞𝑇
and(𝑞𝐹 , 𝑞𝐹 ) → 𝑞𝐹
or(𝑞𝐹 , 𝑞𝐹 ) → 𝑞𝐹

The above tree automaton accepts all propositional logic formulas over True and False which

evaluate to True.

2.2 Domain Specific Languages (DSLs)

The noisy program synthesis framework uses a Domain Specific Language (DSL) to specify to the
set of programs under consideration. Without loss of generality, we assume all programs 𝑝 are of
the form 𝜆 𝑥 .𝑒 (i.e., they take a single input 𝑥), where 𝑒 is a parse trees in a Context Free Grammar
𝐺. The internal nodes of this parse tree represent function and the leaves represent constants or
variable 𝑥. ⟦𝑝⟧𝑥 𝑗 denotes the output of 𝑝 on input value 𝑥 𝑗 (⟦.⟧ is defined in Figure 1).

All valid programs (which can be executed) are defined by a DSL grammar 𝐺 = (𝑇 , 𝑁 , 𝑃, 𝑠0)

where:
• 𝑇 is a set of terminal symbols. These include constants and input variable 𝑥. We use the notation

𝑇𝐶 to denote the set of constants in 𝑇 .

• 𝑁 is the set of non-terminals that represent subexpressions in our DSL.
• 𝑃 is the set of production rules of the form 𝑠 → 𝑓 (𝑠1, . . . , 𝑠𝑛), where 𝑓 is a built-in function in

the DSL and 𝑠, 𝑠1, . . . , 𝑠𝑛 are non-terminals in the grammar.

• 𝑠0 ∈ 𝑁 is the start non-terminal in the grammar.

We assume that we are given a black box implementation of each built-in function 𝑓 in the DSL.
In general, all techniques explored within this paper can be generalized to any DSL which can be
specified within the above framework.

Example 2.3. The following DSL defines expressions over input x, constants 2 and 3, and addition

and multiplication:

𝑛 := 𝑥 | 𝑛 + 𝑡
:= 2 | 3;
𝑡

| 𝑛 × 𝑡;

We use the notation 𝑝 [𝑥] to denote ⟦𝑝⟧𝑥. Given a vector of input values (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩, we use

the notation 𝑝 [ (cid:174)𝑥] to denote the vector ⟨𝑝 [𝑥1], . . . 𝑝 [𝑥𝑛]⟩.

Noisy Program Synthesis using Abstractions

1:5

2.3 Loss Functions
Given a noisy input-output example (𝑥, 𝑦) and a program 𝑝, a Loss Function 𝐿(𝑝 [𝑥], 𝑦) measures
how incorrect the program is with respect to the given example. The loss function only depends on
the noisy output 𝑦 and the output of the program 𝑝 on the input 𝑥. Given a noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦)
(where (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩ and (cid:174)𝑦 = ⟨𝑦1, . . . 𝑦𝑛⟩), we use the notation 𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦) to denote the sum of
loss of program 𝑝 on individual corrupted input-output example, i.e.,

𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦) =

𝑛
∑︁

𝐿(𝑝 [𝑥𝑖 ], 𝑦𝑖 )

𝑖=1
Definition 2.4. 0/1 Loss Function: The 0/1 loss function 𝐿0/1(𝑝 [𝑥], 𝑦) returns 0, if 𝑝 agrees with

an input-output example (𝑥, 𝑦), else it returns 1:

𝐿0/1(𝑝 [𝑥], 𝑦) = 1 if (𝑦 ≠ 𝑝 [𝑥]) else 0
Given a noisy dataset D, 𝐿0/1 counts the number of input-output examples where 𝑝 does not agree
within the data set.

Definition 2.5. 0/∞ Loss Function: The 0/∞ loss function 𝐿0/∞(𝑝 [𝑥], 𝑦) is 0 if 𝑝 matches the

output 𝑦 on input 𝑥 and ∞ otherwise:

𝐿0/∞ (𝑝 [𝑥], 𝑦) = 0 if (𝑦 = 𝑝 [𝑥]) else ∞
Definition 2.6. Damerau-Levenshtein (DL) Loss Function: The DL loss function 𝐿𝐷𝐿 (𝑝 [𝑥], 𝑦)
uses the Damerau-Levenshtein metric [Damerau 1964] to measure the distance between the output
from the synthesized program and the corresponding output in the noisy data set:

where, 𝐿𝑎,𝑏 (𝑖, 𝑗) is the Damerau-Levenshtein metric [Damerau 1964].

𝐿𝐷𝐿 (𝑝 [𝑥], 𝑦) = 𝐿𝑝 [𝑥 ],𝑦 (cid:0) |𝑝 [𝑥] | , |𝑦| (cid:1)

This distance metric counts the number of single character deletions, insertions, substitutions,
or transpositions required to convert one input string into another. We use this loss function to
work with input-output examples containing human-provided text, as more than 80% of all human
misspellings are reported to be captured by a single one of these four operations [Damerau 1964].

2.4 Complexity Measure
Given a program 𝑝, a Complexity Measure 𝐶 (𝑝) scores a program 𝑝 based on their complexity.
This score is independent of the noisy dataset D. Within the noisy program synthesis framework,
complexity measure is used to trade off performance on the noisy data set vs. complexity of the
synthesized program. Formally, a complexity measure 𝐶 (𝑝) maps each program 𝑝 to a real number.
As standard in noisy program synthesis literature [Handa and Rinard 2020], we work with
complexity measure of the form Cost(𝑝), which computes the complexity of given program 𝑝
represented as a parse tree recursively as follows:

Cost(𝑡) = cost(𝑡)

Cost(𝑓 (𝑒1, 𝑒2, . . . 𝑒𝑘 )) = cost(𝑓 ) +

𝑘
(cid:205)
𝑖=1

Cost(𝑒𝑖 )

where 𝑡 and 𝑓 are terminals and built-in functions in our DSL respectively. A popular example of a
complexity measure, expressed in this form, is Size(𝑝), which assigns the cost of a terminal and
function as 1 (cost(𝑡) = cost(𝑓 ) = 1).

Given an FTA A, we can synthesize the minimum complexity parse tree (as measured by
complexity measures of the form Cost(𝑝)) accepted by A using a dynamic programming based
algorithm proposed by [Gallo et al. 1993] (see Section 6 in [Wang et al. 2017a] for more details).

1:6

Shivam Handa and Martin Rinard

2.5 Objective Functions

An Objective Function, within the noisy program synthesis framework, combines the complexity
score of a program and it’s loss over a noisy dataset, to be a combined score. The objective function
is used to compare different programs in the DSL and synthesize the program which best-fits the
noisy dataset. Formally, an objective function 𝑈 maps loss and complexity tuples ⟨𝑙, 𝑐⟩ to a totally
ordered set, such that, for all 𝑙 and 𝑐, 𝑈 (⟨𝑙, 𝑐⟩) is monotonically non-decreasing with respect to
𝑐 and 𝑙.

Definition 2.7. Tradeoff Objective Function: Given a tradeoff parameter 𝜆 > 0, the tradeoff
objective function 𝑈𝜆 (⟨𝑙, 𝑐⟩) = 𝑙 + 𝜆𝑐. This objective function allows us to trade-off between
complexity and loss, directing the synthesis algorithm to search for a simpler program, in lieu of
increasing the loss over the dataset.

Definition 2.8. Lexicographic Objective Function: A lexicographic objective function 𝑈𝐿 (⟨𝑙, 𝑐⟩) =

⟨𝑙, 𝑐⟩ maps 𝑙 and 𝑐 into a lexicographically ordered space, i.e., ⟨𝑙1, 𝑐1⟩ < ⟨𝑙1, 𝑐2⟩ if and only if either
𝑙1 < 𝑙2 or 𝑙1 = 𝑙2 and 𝑐1 < 𝑐2. This objective function first minimizes the loss, then the complexity.

We will use the notation ⟨𝑙1, 𝑐1⟩ ≤𝑈 ⟨𝑙2, 𝑐2⟩ to denote 𝑈 (⟨𝑙1, 𝑐1⟩) ≤ 𝑈 (⟨𝑙2, 𝑐2⟩).

2.6 Program Synthesis over Noisy Datasets

Given a noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), the noisy programs synthesis aims to find a program 𝑝 which
best-fits the dataset D, where the best-fit is defined by the loss function 𝐿, complexity measure
𝐶, and objective function 𝑈 . Formally, given a DSL 𝐺, we wish to find a program 𝑝∗ ∈ 𝐺 which
minimizes the objective function 𝑈 (parameterized by 𝐿 and 𝐶), i.e.,

The above condition is equivalent to:

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝))

∀ 𝑝 ∈ 𝐺 . ⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

Concrete Finite Tree Automata: The noisy synthesis algorithm introduced by [Handa and Rinard
2020] builds upon the concept of a Concrete Finite Tree Automaton (CFTA). Given a domain specific
language 𝐺 and inputs (cid:174)𝑥, a Concrete Finite Tree Automaton is a tree automaton which accepts all
abstract syntax trees representing DSL programs. A CFTA is constructed using the rules in Figure 2.
The states of the CFTA is of the form 𝑞 (cid:174)𝑣
𝑠 , where 𝑠 is a symbol in 𝐺 and (cid:174)𝑣 is vector of values. The
existence of a state 𝑞 (cid:174)𝑣
𝑠 implies there exists a sub-expression in 𝐺, starting from symbol 𝑠, which
, . . . 𝑞 (cid:174)𝑣𝑘
maps inputs (cid:174)𝑥 to output (cid:174)𝑣. There exists a transition 𝑓 (𝑞 (cid:174)𝑣1
𝑠 in the CFTA, only if, for all
𝑠1
𝑖 = [1, |(cid:174)𝑣 |].𝑓 (𝑣1𝑖, . . . 𝑣𝑘𝑖 ) = 𝑣𝑖 .

𝑠𝑘 ) → 𝑞 (cid:174)𝑣

The Var Rule states that if we have an input symbol 𝑥, we construct a state 𝑞 (cid:174)𝑥

𝑥 ∈ 𝑄, where (cid:174)𝑥 is
the input vector. The Const Rule states that for any constant (non variable terminal), we construct a
state 𝑞 (cid:174)𝑣
𝑡 ∈ 𝑄, where (cid:174)𝑣 is a vector of size equal to (cid:174)𝑥 with each entry as ⟦𝑡⟧. The Final Rule states that,
given a start symbol 𝑠0, all states with symbol 𝑠0 are added to 𝑄 𝑓 . The Prod Rule states that if we
have a production 𝑠 → 𝑓 (𝑠1, . . . 𝑠𝑛) ∈ 𝑃, and there exists states 𝑞 (cid:174)𝑣1
𝑠𝑘 ∈ 𝑄, then we add a state
𝑠1
, . . . 𝑞 (cid:174)𝑣𝑘
𝑠 ∈ 𝑄, where for all 𝑖 = [1, |(cid:174)𝑣 |].𝑓 (𝑣1𝑖, . . . 𝑣𝑘𝑖 ) = 𝑣𝑖 . We also add a transition 𝑓 (𝑞 (cid:174)𝑣1
𝑞 (cid:174)𝑣
𝑠 in
𝑠1
the transition set Δ.

𝑠𝑘 ) → 𝑞 (cid:174)𝑣

, . . . 𝑞 (cid:174)𝑣𝑘

Given a CFTA (𝑄, 𝑄 𝑓 , Δ) (constructed using rules in Figure 2) and a state 𝑞 (cid:174)𝑣

𝑠0 ∈ 𝑄 𝑓 , a tree
𝑠0 }, Δ) if and only if 𝑝 [ (cid:174)𝑥] = (cid:174)𝑣 ([Handa and

representing program 𝑝 is accepted by automaton (𝑄, {𝑞 (cid:174)𝑣
Rinard 2020]).

Noisy Program Synthesis using Abstractions

1:7

(cid:174)𝑣 = ⟨𝑥1, . . . 𝑥𝑛⟩
𝑞 (cid:174)𝑣
𝑥 ∈ 𝑄

(Var)

𝑞 (cid:174)𝑣
𝑠0 ∈ 𝑄
𝑞 (cid:174)𝑣
𝑠0 ∈ 𝑄 𝑓

(Final)

𝑡 ∈ 𝑇𝐶, (cid:174)𝑣 = ⟨⟦𝑡⟧, . . . ⟦𝑡⟧⟩, |(cid:174)𝑣 | = 𝑛
𝑞 (cid:174)𝑣
𝑡 ∈ 𝑄

(Const)

, . . . , 𝑞 (cid:174)𝑣𝑘
𝑠 → 𝑓 (𝑠1, . . . , 𝑠𝑘 ) ∈ 𝑃, 𝑞 (cid:174)𝑣1
𝑠1
𝑠 ∈ 𝑄, 𝑓 (𝑞 (cid:174)𝑣1
𝑞 (cid:174)𝑣
𝑠1

, . . . , 𝑞 (cid:174)𝑣𝑘

𝑠𝑘 ) → 𝑞 (cid:174)𝑣

𝑠 ∈ Δ

𝑠𝑘 ∈ 𝑄, 𝑣 𝑗 = ⟦𝑓 ((cid:174)𝑣1𝑗, . . . , (cid:174)𝑣𝑘 𝑗 )⟧, (cid:174)𝑣 = ⟨𝑣1, . . . 𝑣𝑛⟩

(Prod)

Fig. 2. Rules for constructing FTA A = (𝑄, 𝑄 𝑓 , Δ) for inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩.

In general, the rules in Figure 2 may result in a FTA which has infinitely many states. [Handa
and Rinard 2020] handles this by only adding a new state within the constructed FTA if the height
of the smallest tree it will accept is less than some threshold height.
Synthesis Algorithm: Figure 3 presents a simplified version of the noisy synthesis algorithm
presented in [Handa and Rinard 2020]. The algorithm, given a noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), a DSL 𝐺,
an objective function 𝑈 , a loss function 𝐿, and a complexity measure 𝐶, synthesizes a program
which minimizes the objective function, i.e., the program 𝑝∗ returned by the algorithm satisfies the
following constraint:

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝))
The algorithm first constructs a Concrete Finite Tree Automaton (line 2) based on the rules presented
in Figure 2. Given this Concrete Finite Tree Automaton (𝑄, 𝑄 𝑓 , Δ), the algorithm finds the least
complex program (i.e., program which minimizes the complexity measure) for each accepting state
𝑞 ∈ 𝑄 𝑓 (line 3-4). Given an accepting state of the form 𝑞 (cid:174)𝑣
, a program 𝑝 ∈ 𝐺 is accepted by the
𝑠0
automaton (𝑄, {𝑞 (cid:174)𝑣
𝑠0] is the least complex
program which maps input vector (cid:174)𝑥 to outputs (cid:174)𝑣, i.e.,

𝑠0 }, Δ) if and only if 𝑝 [ (cid:174)𝑥] = (cid:174)𝑣. Given an accepting 𝑞 (cid:174)𝑣
𝑠0

, 𝑃 [𝑞 (cid:174)𝑣

𝑃 [𝑞 (cid:174)𝑣

𝑠0] ∈ argmin𝑝 ∈𝐺 [ (cid:174)𝑥→(cid:174)𝑣 ]

𝐶 (𝑝)

where 𝐺 [ (cid:174)𝑥 → (cid:174)𝑣] = {𝑝 |𝑝 ∈ 𝐺, 𝑝 [ (cid:174)𝑥] = (cid:174)𝑣 }.

The algorithm then finds an accepting state 𝑞 (cid:174)𝑣∗

𝑠0 ∈ 𝑄 𝑓 , such that, for all accepting states 𝑞 (cid:174)𝑣

𝑠0 ∈ 𝑄 𝑓 ,

⟨𝐿((cid:174)𝑣 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝑣∗

𝑠0 ])⟩ ≤𝑈 ⟨𝐿((cid:174)𝑣, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝑣
Since, for all programs 𝑝 ∈ 𝐺, there exists an accepting state 𝑞 (cid:174)𝑣
𝑠0
(𝑄, {𝑞 (cid:174)𝑣

𝑠0 }, Δ) ([Handa and Rinard 2020]), the following statement is true:

𝑠0])⟩

, such that, 𝑝 is accepted by

⟨𝐿((cid:174)𝑣 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝑣∗

𝑠0 ])⟩ ≤𝑈 ⟨𝐿((cid:174)𝑣, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝑣

𝑠0])⟩ ≤𝑈 ⟨𝐿((cid:174)𝑣, (cid:174)𝑦), 𝐶 (𝑝)⟩

Therefore,

𝑃 [𝑞 (cid:174)𝑣∗

𝑠0 ] ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝))

3 Noisy Program Synthesis using Abstraction Refinement Based Optimization

We next introduce our synthesis algorithm which builds upon the concept of a Finite Tree Automaton
which uses abstract values instead of concrete values, as used in Figure 3. The algorithm then
uses an abstraction refinement based optimization technique to direct the algorithm towards the
program which optimizes the objective function.

1:8

Shivam Handa and Martin Rinard

1: procedure Synthesize(D, 𝐺, 𝑈 , 𝐿, 𝐶)

input: Noisy Dataset D = ( (cid:174)𝑥, (cid:174)𝑦), DSL 𝐺.
input: Objective Function 𝑈 , Loss Function 𝐿, and Complexity metric 𝐶.
output: A program 𝑝∗, such that, ∀ 𝑝 ∈ 𝐺 .⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩.

(𝑄, 𝑄 𝑓 , Δ) := ConstructFTA( (cid:174)𝑥, 𝐺);
for 𝑞 ∈ 𝑄 𝑓 do

𝑃 [𝑞] := LeastComplex𝐶 ((𝑄, {𝑞}, Δ));

⊲ Least complex program for a given accepting state.

𝑞∗ := null; (cid:174)𝑣 ∗ := null;
for 𝑞 (cid:174)𝑣
𝑠0 ∈ 𝑄 𝑓 do
if 𝑞∗ = null or ⟨𝐿((cid:174)𝑣 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞∗])⟩ ≤𝑈 ⟨𝐿((cid:174)𝑣, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝑣

𝑠0 ])⟩ then

(cid:174)𝑣 ∗ = (cid:174)𝑣; 𝑞∗ := 𝑞 (cid:174)𝑣
𝑠0

;

⊲ 𝑞∗ is the accepting state which accepts a program which minimizes the Objective Function.

return 𝑃 [𝑞∗];

Fig. 3. Algorithm for noisy program synthesis using Finite Tree Automaton.

2:

3:
4:

5:

6:

7:

8:

9:

3.1 Abstractions

We construct an abstract version of the Finite Tree Automaton by associating abstract values with
each symbol. We assume that the abstract values are represented as conjunctions of predicates of
the form 𝑓 (𝑠) op 𝑐, where 𝑠 is a symbol in the given DSL, 𝑓 is a function, op is an operator, and 𝑐 is
a constant.
Universe of predicates: Given a DSL, our algorithm is parameterized by a universe U of predicates,
that our algorithm uses to construct abstractions for our synthesis algorithm. The universe U is
specified using a family of function F , a set of operators O, and a set of constants C, such that,
all predicate in the universe U can be written as 𝑓 (𝑠) op 𝑐, where 𝑓 ∈ F , op ∈ O, 𝑐 ∈ C, and 𝑠 is
a symbol in the DSL (except predicates true and false). We assume that F contains the identity
function, O contains equality, and C includes the set of all values that can be computed by any
sub-expression within the DSL 𝐺.
Notation: Given predicates P ⊆ U and an abstract value 𝜑 ∈ U, we use 𝛼 P (𝜑) to denote the
strongest conjunctions of predicates in P, such that 𝜑 =⇒ 𝛼 P (𝜑). Given a vector of abstract
values (cid:174)𝜑 = ⟨𝜑1, . . . 𝜑𝑛⟩, 𝛼 P ( (cid:174)𝜑) denotes the vector ⟨𝛼 P (𝜑1), . . . 𝛼 P (𝜑𝑛)⟩. As standard in abstract
interpretation literature [Cousot and Cousot 1977], we use the notation 𝛾 (𝜑) to denote the set of
concrete values represented by the abstract value 𝜑.
Abstract semantics: In addition to concrete semantics for each DSL construct, we are given
abstract semantics of each DSL construct in the form of symbolic post-conditions over the universe
of predicates U. Given a production 𝑠 → 𝑓 (𝑠1, . . . , 𝑠𝑛), we use the notation ⟦𝑓 (𝜑1, . . . 𝜑𝑘 )⟧# to
represent the abstract semantics of function 𝑓 , i.e., ⟦𝑓 (𝜑1, . . . , 𝜑𝑘 )⟧# = 𝜑 if the function 𝑓 returns 𝜑
(for symbol 𝑠), given abstract values 𝜑1, . . . , 𝜑𝑘 for arguments 𝑠1, . . . , 𝑠𝑘 . We assume that the abstract
semantics are sound, i.e.,

⟦𝑓 (𝜑1, . . . , 𝜑𝑘 )⟧# = 𝜑 and 𝑣1 ∈ 𝛾 (𝜑1), . . . 𝑣𝑘 ∈ 𝛾 (𝜑𝑘 ) =⇒ ⟦𝑓 (𝑣1, . . . 𝑣𝑘 )⟧ ∈ 𝛾 (𝜑)

However, we do not require the abstract semantics to be precise, i.e., formally:

⟦𝑓 (𝜑1, . . . , 𝜑𝑘 )⟧# = 𝜑, there may exist a 𝑣 ∈ 𝜑, 𝑠.𝑡 ., (cid:154) 𝑣1 ∈ 𝜑1, . . . 𝑣𝑘 ∈ 𝜑𝑘 .⟦𝑓 (𝑣1, . . . 𝑣𝑘 )⟧ = 𝑣
There may exist concrete value 𝑣 in the abstract output 𝜑, such that, no concrete input parameters
𝑣1, . . . 𝑣𝑘 in the abstract inputs 𝜑1, . . . 𝜑𝑘 exist, for which 𝑓 (𝑣1, . . . 𝑣𝑘 ) = 𝑣.

Noisy Program Synthesis using Abstractions

1:9

𝑡 ∈ 𝑇𝐶
⟦𝑡⟧P𝑥 𝑗 ⇒ 𝛼 P (𝑡 = ⟦𝑡⟧𝑥 𝑗 )

(Constant)

⟦𝑥⟧P𝑥 𝑗 ⇒ 𝛼 P (𝑥 = 𝑥 𝑗 )

(Variable)

⟦𝑒1⟧𝑥 𝑗 ⇒ 𝜑1

⟦𝑒2⟧𝑥 𝑗 ⇒ 𝜑2

. . .

⟦𝑒𝑘 ⟧𝑥 𝑗 ⇒ 𝜑𝑘

⟦𝑓 (𝑒1, 𝑒2, . . . 𝑒𝑘 )⟧𝑥 𝑗 ⇒ 𝛼 P (⟦𝑓 (𝜑1, 𝜑2, . . . 𝜑𝑘 )⟧#)

(Function)

Fig. 4. Abstract Execution semantics for program 𝑝.

But we require the abstract semantics to be precise if all of the input parameters are abstract
values representing a single concrete value, i.e., they are abstract values of the form 𝑠 = 𝑣. Formally:
⟦𝑓 (𝑠1 = 𝑣1, . . . 𝑠𝑘 = 𝑣𝑘 )⟧# = (𝑠 = ⟦𝑓 (𝑣1, . . . 𝑣𝑘 )⟧)
Given a program 𝑝, predicates P, and input 𝑥 𝑗 , ⟦𝑝⟧P𝑥 𝑗 denotes the abstract value of program 𝑝, if
the intermediate computed values are only represented via predicates in P. Figure 4 presents the
precise rules for computing ⟦𝑝⟧P𝑥 𝑗 . Given inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩, we use the notation ⟦𝑝⟧P (cid:174)𝑥 to
denote the vector (cid:174)𝜑 = ⟨⟦𝑝⟧P𝑥1, . . . ⟦𝑝⟧P𝑥𝑛⟩.
Abstract Loss Function: Given a loss function, the abstract semantics of a loss function allows
us to find the minimum possible loss value for a given abstract value, i.e., given a loss function 𝐿,
noisy output 𝑦, and an abstract value 𝜑:

𝐿(𝜑, 𝑦) = min
𝑧 ∈𝛾 (𝜑)

𝐿(𝑧, 𝑦)

3.2 Abstract Finite Tree Automaton

An Abstract Finite Tree Automata (AFTA) generalizes an Concrete Finite Tree Automaton by
replacing concrete values by abstract values while constructing automaton states. This allows us to
compress the size of an FTA, as multiple states with concrete values can be represented by a single
state with abstract value.

Given predicates P, DSL 𝐺, and inputs (cid:174)𝑥, Figure 5 presents the rules for constructing an AFTA
𝑠 , where 𝑠 is a symbol and (cid:174)𝜑 is a vector of abstract
𝑠 ∈ 𝑄, then there exists an expression 𝑒, starting from symbol 𝑠, such that ⟦𝑒⟧P (cid:174)𝑥 = (cid:174)𝜑. If

(𝑄, 𝑄 𝑓 , Δ). States in an AFTA are of the form 𝑞 (cid:174)𝜑
values. If 𝑞 (cid:174)𝜑
there is a transition 𝑓 (𝑞 (cid:174)𝜑1
𝑠1

, . . . 𝑞 (cid:174)𝜑𝑘

𝑠𝑘 ) → 𝑞 (cid:174)𝜑
𝑠 in the AFTA then
∀𝑗 = [1, | (cid:174)𝜑 |].⟦𝑓 (𝜑1𝑗, . . . 𝜑𝑘 𝑗 )⟧# =⇒ 𝜑 𝑗

The Var Rule states constructs a state 𝑞 (cid:174)𝜑
𝑥 ∈ 𝑄 for variable symbol 𝑥, where (cid:174)𝜑 = 𝛼 P ((𝑥 = 𝑥1), . . . (𝑥 =
𝑥𝑛)). The Const Rule constructs a state 𝑞 (cid:174)𝜑
𝑡 ∈ 𝑄 for each constant terminal 𝑡, where (cid:174)𝜑 is a vector
of size equal to (cid:174)𝑥 with each entry as 𝛼 P (𝑡 = ⟦𝑡⟧). The Final Rule adds all states with symbol
𝑠0 are added to 𝑄 𝑓 , where 𝑠0 is the start symbol. The Prod Rule constructs a state 𝑞 (cid:174)𝜑
𝑠 ∈ 𝑄, if
there exists a production 𝑠 → 𝑓 (𝑠1, . . . 𝑠𝑛) ∈ 𝑃 and there exists states 𝑞 (cid:174)𝜑1
𝑠𝑘 ∈ 𝑄 (where
𝑠1
𝑠𝑘 ) → 𝑞 (cid:174)𝜑
∀𝑖 = [1, |(cid:174)𝑣 |].𝑓 (𝜑1𝑖, . . . 𝜑𝑘𝑖 ) =⇒ 𝜑𝑖 ). We also add a transition 𝑓 (𝑞 (cid:174)𝜑1
𝑠1

, . . . 𝑞 (cid:174)𝜑𝑘

, . . . 𝑞 (cid:174)𝜑𝑘

𝑠 in Δ.

Theorem 3.1. (Structure of the Tree Automaton) Given a set of predicates P, input vector (cid:174)𝑥 =
⟨𝑥1, . . . 𝑥𝑛⟩, and DSL 𝐺, let A = (𝑄, 𝑄 𝑓 , Δ) be the AFTA returned by the function ConstructAFTA( (cid:174)𝑥, 𝐺, P).
Then for all symbols 𝑠 in 𝐺, for all expressions 𝑒 starting from symbol 𝑠 (and height less than
bound 𝑏), there exists a state 𝑞 (cid:174)𝜑
𝑠 }, Δ), where
(cid:174)𝜑 = ⟨⟦𝑒⟧P𝑥1, . . . ⟦𝑒⟧P𝑥𝑛⟩.

𝑠 ∈ 𝑄, such that, 𝑒 is accepted by the automaton (𝑄, {𝑞 (cid:174)𝜑

1:10

Shivam Handa and Martin Rinard

(cid:174)𝜑 = 𝛼 P (cid:0)⟨𝑥 = 𝑥1, . . . 𝑥 = 𝑥𝑛⟩(cid:1)

𝑞 (cid:174)𝜑
𝑥 ∈ 𝑄

(Var)

𝑞 (cid:174)𝜑
𝑠0 ∈ 𝑄
𝑞 (cid:174)𝜑
𝑠0 ∈ 𝑄 𝑓

(Final)

𝑡 ∈ 𝑇𝐶, (cid:174)𝜑 = 𝛼 P (cid:0)⟨𝑡 = ⟦𝑡⟧, . . . 𝑡 = ⟦𝑡⟧⟩(cid:1), | (cid:174)𝜑 | = 𝑛

𝑞 (cid:174)𝜑
𝑡 ∈ 𝑄

(Const)

𝑠 → 𝑓 (𝑠1, . . . , 𝑠𝑘 ) ∈ 𝑃, 𝑞 (cid:174)𝜑1
𝑠1

, . . . , 𝑞 (cid:174)𝜑𝑘
𝑠 ∈ 𝑄, 𝑓 (𝑞 (cid:174)𝜑1
𝑞 (cid:174)𝜑
𝑠1

𝑠𝑘 ∈ 𝑄, 𝜑 𝑗 = 𝛼 P (cid:0)⟦𝑓 ( (cid:174)𝜑1𝑗, . . . , (cid:174)𝜑𝑘 𝑗 )⟧#(cid:1), (cid:174)𝜑 = ⟨𝜑1, . . . 𝜑𝑛⟩
𝑠 ∈ Δ

𝑠𝑘 ) → 𝑞 (cid:174)𝜑

, . . . , 𝑞 (cid:174)𝜑𝑘

(Prod)

Fig. 5. Rules for constructing FTA A = (𝑄, 𝐹, 𝑄 𝑓 , Δ) with Abstract Values, for inputs (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩.

We present the proof of this theorem in the appendix A.1(Theorem A.3).

Corollary 3.2. Given a set of predicates P, input vector (cid:174)𝑥 = ⟨𝑥1, . . . 𝑥𝑛⟩, and DSL 𝐺, let A =
(𝑄, 𝑄 𝑓 , Δ) be the AFTA returned by the function ConstructAFTA( (cid:174)𝑥, 𝐺, P). All programs 𝑝 (of height
less than bound 𝑏) is accepted by A. For any accepting state 𝑞 (cid:174)𝜑
𝑠0 ∈ 𝑄 𝑓 , a program 𝑝 is accepted by the
automaton (𝑄, {𝑞 (cid:174)𝜑

𝑠0 }, Δ) if and only if ∀𝑖 ∈ [1, 𝑛], 𝑝 [𝑥𝑖 ] ∈ 𝛾 (𝜑𝑖 ).

3.3 Synthesis Algorithm
We present our synthesis algorithm in Figure 6. The Synthesize procedure takes a noisy dataset
D, a DSL 𝐺, a threshold 𝜖 ≥ 0, initial predicates P, a universe of possible predicates U, objective
function 𝑈 , loss function 𝐿, and a complexity measure 𝐶. We assume that true, false ∈ P.

The synthesis algorithm consists of a refinement loop (line 3-10). The loop first constructs a
Abstract Finite Tree Automaton (line 4) with the current set of predicates P using rules presented
in Figure 5. The algorithm then uses the MinCost function to generate a candidate program 𝑝∗
(line 5). The algorithm maintains the program 𝑝𝑟 , which is the best candidate program out of all the
candidate programs 𝑝∗ generated.

If the distance between the current program 𝑝∗ and the best possible program in the DSL 𝐺 is
less a tolerance level 𝜖 (Distance function, line 8), the algorithm returns the best candidate program
𝑝𝑟 . Note that

⟨𝐿(𝑝𝑟 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑟 )⟩ ≤𝑈 ⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩
Otherwise, the algorithm refine our AFTA to either improve our estimation of the best possible
program or synthesize a better candidate program. To refine our AFTA, the algorithm first picks
an input-output example (𝑥, 𝑦) from dataset D, on which we can improve the candidate program
𝑝∗ (line 9). Given an input-output example (𝑥, 𝑦), the procedure OptimizeAndBackPropogate
constructs the constraints required to improve the AFTA and then returns the set of predicates
which will allow the algorithm to build a more refined AFTA.
We discuss each of these sub-procedures in detail next.

3.4 Minimum Cost Candidate
We present the implementation of procedure MinCost in Figure 7. Given an AFTA (𝑄, 𝑄 𝑓 , Δ), noisy
dataset D = ( (cid:174)𝑥, (cid:174)𝑦), objective function 𝑈 , loss function 𝐿, and complexity measure 𝐶, MinCost
returns a program 𝑝𝑞∗ which minimizes the abstract objective function, where, given a program 𝑝,
the abstract objective function is defined as

𝑈 (𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝))

2:
3:
4:
5:
6:
7:

8:

9:

10:

2:
3:

4:

5:

6:

7:

8:

Noisy Program Synthesis using Abstractions

1:11

1: procedure Synthesize(D, 𝐺, 𝜖, P, U, 𝑈 , 𝐿, 𝐶)

input: Noisy Dataset D = ( (cid:174)𝑥, (cid:174)𝑦), DSL 𝐺, and tolerance 𝜖.
input: initial predicates P, and universe of predicates U.
input: Objective Function 𝑈 , Loss Function 𝐿, and Complexity metric 𝐶.
output: A program 𝑝∗, such that, 𝑝∗ satisfies the 𝜖-correctness condition.

𝑝𝑟 := null;
while true do

A := ConstructAFTA( (cid:174)𝑥, 𝐺, 𝑃);
𝑝∗ := MinCost(A, D, 𝑈 , 𝐿, 𝐶);
if 𝑝𝑟 = null or ⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝𝑟 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑟 )⟩ then

𝑝𝑟 := 𝑝∗;

if Distance(𝑝∗, D, P, 𝐿) ≤ 𝜖 then return 𝑝𝑟 ;
𝑥, 𝑦 := PickDimension(𝑝∗, D, P, 𝐿);
P := P (cid:208) OptimizeAndBackPropogate(𝑝∗, 𝑥, 𝑦, P, U);

Fig. 6. Algorithm for noisy program synthesis using Abstraction Refinement based Optimization.

1: procedure MinCost(A, D, 𝑈 , 𝐿, 𝐶)

input: AFTA A = (𝑄, 𝑄 𝑓 , Δ), Noisy Dataset D = ( (cid:174)𝑥, (cid:174)𝑦).
input: Objective Function 𝑈 , Loss Function 𝐿, and Complexity metric 𝐶.
output: A program 𝑝∗, such that, ∀ 𝑝 ∈ 𝐺 .⟨𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝)⟩.

for 𝑞 ∈ 𝑄 𝑓 do

𝑃 [𝑞] := LeastComplex𝐶 ((𝑄, {𝑞}, Δ));

⊲ Least complex program for a given accepting state.

𝑞∗ := null; (cid:174)𝜑 ∗ := null;
for 𝑞 (cid:174)𝜑
𝑠0 ∈ 𝑄 𝑓 do
if 𝑞∗ = null or ⟨𝐿( (cid:174)𝜑 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞∗])⟩ ≤𝑈 ⟨𝐿( (cid:174)𝜑, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑

𝑠0])⟩ then

(cid:174)𝜑 ∗ = (cid:174)𝜑; 𝑞∗ := 𝑞 (cid:174)𝜑
𝑠0

;

⊲ 𝑞∗ accepts a program which minimizes the Abstract Objective Function.

return 𝑃 [𝑞∗];

Fig. 7. Procedure for synthesizing the program which minimizes the Abstract Objective Function.

Therefore, for all programs 𝑝 ∈ 𝐺:

⟨𝐿(⟦𝑝𝑞∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝𝑞∗)⟩ ≤𝑈 ⟨𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝)⟩

Note that, since for all programs 𝑝, 𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦) ≤ 𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), the following statement is true:

⟨𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

Hence, for programs 𝑝 ∈ 𝐺:

⟨𝐿(⟦𝑝𝑞∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝𝑞∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

The procedure first finds the least complex program (i.e., program which minimizes the complexity
measure) for each accepting state 𝑞 ∈ 𝑄 𝑓 (line 2-3). Given an accepting state of the form 𝑞 (cid:174)𝜑
, a
𝑠0
program 𝑝 ∈ 𝐺 is accepted by the automaton (𝑄, {𝑞 (cid:174)𝜑
𝑠0 }, Δ) if and only if ⟦𝑝⟧P (cid:174)𝑥 = (cid:174)𝜑. Given an

1:12

Shivam Handa and Martin Rinard

accepting 𝑞 (cid:174)𝜑
𝑠0

, 𝑃 [𝑞 (cid:174)𝜑

𝑠0 ] is the least complex program which maps input vector (cid:174)𝑥 to outputs (cid:174)𝜑, i.e.,

𝑃 [𝑞 (cid:174)𝜑

𝑠0 ] ∈ argmin𝑝 ∈𝐺 [ (cid:174)𝑥→ (cid:174)𝜑 ]

𝐶 (𝑝)

where 𝐺 [ (cid:174)𝑥 → (cid:174)𝜑] = {𝑝 |𝑝 ∈ 𝐺, ⟦𝑝⟧P (cid:174)𝑥 = (cid:174)𝜑 }.

The algorithm then finds an accepting state 𝑞 (cid:174)𝜑 ∗

𝑠0 ∈ 𝑄 𝑓 , such that, for all accepting states 𝑞 (cid:174)𝜑

𝑠0 ∈ 𝑄 𝑓 ,

⟨𝐿( (cid:174)𝜑 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑 ∗

𝑠0 ])⟩ ≤𝑈 ⟨𝐿( (cid:174)𝜑, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑

𝑠0])⟩

Theorem 3.3. Given predicates P, DSL 𝐺, noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), objective function 𝑈 , loss
function 𝐿, complexity measure 𝐶, and A = ConstructAFTA( (cid:174)𝑥, 𝐺, P), if 𝑝∗ = MinCost(A, D, 𝑈 , 𝐿, 𝐶)
then

i.e., 𝑝∗ minimizes the abstract objective function.

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝))

We present the proof of this theorem in the appendix A.1 (Theorem A.4).

3.5 Termination Condition and Tolerance

Given a candidate program 𝑝∗, predicates P, noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), and a loss function 𝐿, the
Distance function returns the difference between the concrete loss of program 𝑝∗ over noisy dataset
D and the abstract loss (given predicates from P) over noisy dataset D. Formally:

Distance(cid:0)𝑝, ( (cid:174)𝑥, (cid:174)𝑦), P, 𝐿(cid:1) := 𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦) − 𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦)
The algorithm terminates if the distance is less than equal to the tolerance level 𝜖. Note that
if 𝜖 = 0, then the algorithm only terminates when 𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦) = 𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), and since for all
program 𝑝 ∈ 𝐺:

⟨𝐿(⟦𝑝𝑞∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝𝑞∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

is true, the following statement is also true:

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝))
In general, the previous work in noisy program synthesis have maintained a very strict version
of correctness, i.e., they generally synthesize a program 𝑝∗ which minimizes the objective function.
This leaves out any speedups which can be achieved to by relaxing the requirement to synthesizing
a program which is close to the optimal program but may not be one of the optimal programs.

To capture this relaxation, we introduce the concept of 𝜖-correctness. Given DSL 𝐺, a noisy
dataset D = ( (cid:174)𝑥, (cid:174)𝑦), an objective function 𝑈 , a loss function 𝐿, a complexity measure 𝐶, and set of
programs 𝐺𝑝 ⊆ 𝐺, let B𝜖 (𝐺𝑝 ) be a set of programs in 𝐺, such that, 𝑝∗ ∈ B𝜖 (𝐺𝑝 ) if and only if there
exists a program 𝑝 ∈ 𝐺𝑝 , such that,

⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦) + 𝜖, 𝐶 (𝑝)⟩

i.e., 𝑝∗ will be a better fit the dataset D compared to program 𝑝, if the loss of 𝑝 over dataset D was
increased by 𝜖.

Definition 3.4. (𝜖-correctness) Given a noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), a DSL 𝐺, an objective function

𝑈 , a loss Function 𝐿, and a complexity measure 𝐶, a program 𝑝𝑟 ∈ 𝐺 is 𝜖-correct if and only if:
𝑝𝑟 ∈ B𝜖 (argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝))

Noisy Program Synthesis using Abstractions

1:13

A program 𝑝𝑟 ∈ 𝐺 is 𝜖-correct if and only if there exists a 𝑝∗ ∈ 𝐺, such that,

⟨𝐿(𝑝𝑟 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑟 )⟩ ≤𝑈 ⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩
∀𝑝 ∈ 𝐺 . ⟨𝐿(𝑝∗ [ (cid:174)𝑥] − 𝜖, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

Note that, for 𝜖 = 0, the above condition reduces to

∀𝑝 ∈ 𝐺 . ⟨𝐿(𝑝𝑟 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑟 )⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

Therefore, for 𝜖 = 0, 𝑝𝑟 ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)).

Theorem 3.5. (Soundness) Given a dataset D, a DSL 𝐺, tolerance 𝜖 ≥ 0, universe of predicates U,
initial predicates P, objective function 𝑈 , loss function 𝐿, and the complexity measure 𝐶, if Algorithm 6
returns the program 𝑝∗, then 𝑝∗ satisfies the 𝜖-correctness condition (Definition 3.4).

Proof. Let us assume that the algorithm terminates on the 𝑖𝑡ℎ iteration. Let A𝑖 = (𝑄, 𝑄 𝑓 , Δ) be
the AFTA when the algorithm terminates. Let 𝑝𝑖 be the program returned by MinCost on the 𝑖𝑡ℎ
iteration.
From Theorem 3.3, for all programs 𝑝 ∈ 𝐺,

When the algorithm terminates, the following condition is true:

⟨𝐿(⟦𝑝𝑖 ⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝𝑖 )⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

which implies:

Distance(𝑝𝑖, D, P, 𝐿) ≤ 𝜖

𝐿(𝑝𝑖 [ (cid:174)𝑥], (cid:174)𝑦) − 𝐿(⟦𝑝𝑖 ⟧P𝑖 (cid:174)𝑥, (cid:174)𝑦) ≤ 𝜖

Therefore, for all programs 𝑝 ∈ 𝐺,

and the following is true for the synthesized program 𝑝𝑟 ,

⟨𝐿(𝑝𝑖 [ (cid:174)𝑥], (cid:174)𝑦) − 𝜖, 𝐶 (𝑝𝑖 )⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

⟨𝐿(𝑝𝑟 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑟 )⟩ ≤𝑈 ⟨𝐿(𝑝𝑖 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝𝑖 )⟩

Hence, if the algorithm 6 returns a program 𝑝𝑟 , then 𝑝𝑟 satisfies the 𝜖-correctness condition.

□

3.6 Abstraction Refinement based Optimization
Given a dataset D and predicates P, the program 𝑝∗ (returned by MinCost) minimizes the abstract
objective function, i.e., for all programs 𝑝 ∈ 𝐺:

⟨𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

Since, the algorithm did not terminate, Distance(𝑝∗, P, D, 𝐿) > 𝜖.

Let us consider the case when 𝜖 = 0. Since Distance(𝑝∗, P, D, 𝐿) > 0, the concrete loss of

program 𝑝∗ over dataset D is greater than the abstract loss of 𝑝∗ over D. Formally,

𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦) > 𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦)

This means that

⟨𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ <𝑈 ⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩
At this point, even though, for all programs 𝑝 ∈ 𝐺, the following is true:
⟨𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

We cannot prove that 𝑝∗ is the optimal function, i.e.,

⟨𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

And therefore, just using predicates P, we cannot prove that 𝑝𝑟 is the optimal function.

1:14

Shivam Handa and Martin Rinard

Similarly, if 𝜖 > 0, for all programs 𝑝 ∈ 𝐺, the following is true:

⟨𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

But we cannot prove that the following statement is true:

⟨𝐿(𝑝∗ [ (cid:174)𝑥] − 𝜖, (cid:174)𝑦), 𝐶 (𝑝∗)⟩ ≤𝑈 ⟨𝐿(𝑝 [ (cid:174)𝑥], (cid:174)𝑦), 𝐶 (𝑝)⟩

And therefore, just using predicates P, we cannot prove that 𝑝𝑟 is 𝜖-correct.

Therefore, in order to find the optimal program and prove it’s optimality, we have to expand the

set of predicates P.

To achieve this goal, the algorithm first selects an input-output example from the noisy dataset
D on which can improve the difference between the abstract loss and the concrete loss of programs
using procedure PickDimension. Given an input-output example (𝑥, 𝑦), the idea here is to expand
the set of predicates P to P ′, such that:

𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧P′𝑥, 𝑦) ≤ 𝐿(𝑝∗ [𝑥], 𝑦)

Thus improving our estimation of the abstract loss function for programs in 𝐺.

The algorithm allows us to plug any implementation of the procedure PickDimension, assuming

it satisfies the following constraint:

⟨𝑥𝑖, 𝑦𝑖 ⟩ = PickDimension(cid:0)𝑝, ( (cid:174)𝑥, (cid:174)𝑦), P, 𝐿(cid:1) =⇒ 𝐿(𝑝 [𝑥𝑖 ], 𝑦𝑖 ) > 𝐿(⟦𝑝⟧P𝑥𝑖, 𝑦𝑖 )

Since 𝐿(𝑝∗ [ (cid:174)𝑥], (cid:174)𝑦) − 𝐿(⟦𝑝∗⟧P (cid:174)𝑥, (cid:174)𝑦) > 𝜖 (as Distance(𝑝∗, D, P, 𝐿) > 𝜖), there exists at least one
𝑖 ∈ [1, 𝑛], such that,

𝐿(𝑝∗ [𝑥𝑖 ], 𝑦𝑖 ) > 𝐿(⟦𝑝∗⟧P𝑥𝑖, 𝑦𝑖 )
If multiple input-output examples exist for which the abstract loss is less than the concrete loss, an
implementation of PickDimension can return any one of them and our synthesis algorithm will
use that example to optimize the automaton.

Given the input-output example (𝑥, 𝑦), the algorithm uses the procedure OptimizeAndBackPropogate

to expand the set of predicates to P ′, such that

𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧P′𝑥, 𝑦) ≤ 𝐿(𝑝∗ [𝑥], 𝑦)

Figure 8 presents the OptimizeAndBackPropogate procedure. The procedure tries to find the

strongest formula 𝜓 ∗, such that, (𝑠0 = 𝑝 [𝑥]) =⇒ 𝜓 ∗ and:

𝐿((⟦𝑝∗⟧P𝑥), 𝑦) < 𝐿((⟦𝑝∗⟧P𝑥) ∧ 𝜓 ∗, 𝑦)

Note that since (𝑠0 = ⟦𝑝∗⟧𝑥) =⇒ 𝜓 ∗ (line 7):

𝐿((⟦𝑝∗⟧P𝑥) ∧ 𝜓 ∗, 𝑦) ≤ 𝐿(𝑝∗ [𝑥], 𝑦)

Theorem 3.6. Given expression 𝑒 = 𝑓 (𝑒1, . . . 𝑒𝑛), input 𝑥, abstract value 𝜓𝑝 (assuming (𝑠 =
⟦𝑒⟧𝑥) =⇒ 𝜓𝑝 , predicates P, and universe of predicates U, if the procedure BackPropogate(𝑒, 𝑥,𝜓𝑝, P, U)
returns predicate set P𝑟 then:

⟦𝑒⟧P∪P𝑟 𝑥 =⇒ 𝜓𝑝

We present the proof of this theorem in the appendix A.1 (Theorem A.5).

Theorem 3.7. Let P𝑟 = OptimizeAndBackPropogate(𝑝∗, 𝑥, 𝑦, P, U).

𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧𝑥, 𝑦) =⇒ 𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧( P (cid:208) P𝑟 )𝑥, 𝑦)

Noisy Program Synthesis using Abstractions

1:15

1: procedure OptimizeAndBackPropagate(𝑝, 𝑥 𝑗, 𝑦 𝑗, P, U, 𝐿)

input: Program 𝑝, input 𝑥 𝑗 , noisy output 𝑦 𝑗 , predicates P, universe of predicates U, and Loss
Function 𝐿.
output: A set of predicates P𝑟 .

2:

3:
4:
5:
6:

7:
8:
9:

2:

3:
4:
5:
6:

7:

8:

9:

10:
11:
12:

13:

14:

𝜑 := ⟦𝑝⟧P𝑥 𝑗 ; 𝜙 := (𝑠0 = ⟦𝑝⟧𝑥 𝑗 );
Φ := (cid:8)𝑞 ∈ U|𝜙 =⇒ 𝑞(cid:9);
Ψ := Φ;
for 𝑖 = 1 . . . 𝑚 do

Ψ := Ψ (cid:208) (cid:8)𝜓 ∧ 𝑞 | 𝜓 ∈ Ψ, 𝑞 ∈ Φ(cid:9);

⊲ Use a maximum of 𝑚 predictates.

𝜓 ∗ := 𝜙;
for 𝜓 ∈ Ψ do

if 𝜓 ∗ =⇒ 𝜓 and 𝐿(𝜑, 𝑦 𝑗 ) − 𝐿(𝜑 ∧ 𝜓, 𝑦 𝑗 ) ≤ 𝛿 > 0 then 𝜓 ∗ := 𝜓 ;

10:

return ExtractPredicates(𝜓 ∗) (cid:208) BackPropagate(𝑝, 𝑥 𝑗, 𝜑 ∧ 𝜓 ∗, P, U);

⊲ The abstract loss is increased by atleast 𝛿.

Fig. 8. Algorithm for extracting predicates P𝑟 to refine the abstract value of 𝑝, such that,
𝐿(⟦𝑝⟧P𝑥 𝑗 , 𝑦 𝑗 ) < 𝐿(⟦𝑝⟧( P (cid:208) P𝑟 )𝑥 𝑗 , 𝑦 𝑗 ).

1: procedure BackPropogate(𝑓 (𝑒1, . . . 𝑒𝑛), 𝑥 𝑗,𝜓𝑝, P, U)

output: A set of predicates 𝑃𝑟 , such that, ⟦𝑓 (𝑒1, . . . 𝑒𝑛)⟧( P∪P𝑟 )𝑥 𝑗 =⇒ 𝜓𝑝 .

(cid:174)𝜙 := ⟨⟦𝑒1⟧𝑥 𝑗, . . . ⟦𝑒𝑛⟧𝑥 𝑗 ⟩; (cid:174)𝜑 := ⟨⟦𝑒1⟧P𝑥 𝑗, . . . ⟦𝑒𝑛⟧P𝑥 𝑗 ⟩;
(cid:174)Φ := ⟨Φ1, . . . Φ𝑛⟩ where Φ𝑖 := (cid:8)𝑞 ∈ U|𝜙𝑖 =⇒ 𝑞(cid:9); (cid:174)Ψ := (cid:174)Φ;
for 𝑖 = 1 . . . 𝑚 do

for 𝑗 = 1, . . . , 𝑛 do

Ψ𝑗 := Ψ𝑗 (cid:208) (cid:8)𝜓 ∧ 𝑞 | 𝜓 ∈ Ψ𝑗, 𝑞 ∈ Φ𝑗 (cid:9);

⊲ Use a maximum of 𝑚 predicates.

(cid:174)𝜓 ∗ := (cid:174)𝜙;
for all (cid:174)𝜓 = ⟨𝜓1, . . . 𝜓𝑛⟩ | 𝜓𝑖 ∈ Ψ𝑖 do

if ∀𝑖 = 1, . . . 𝑛. 𝜓 ∗

𝑖 =⇒ 𝜓𝑖 and ⟦𝑓 (𝜑1 ∧ 𝜓1, . . . 𝜑𝑛 ∧ 𝜓𝑛)⟧# =⇒ 𝜓𝑝 then (cid:174)𝜓 ∗ := (cid:174)𝜓 ;

P𝑟 := ∅;
for 𝑖 = 1 . . . 𝑛 do

P𝑟 := P𝑟 (cid:208) ExtractPredicates(𝜓 ∗
if 𝑒𝑖 ∉ 𝑇 then P𝑟 := P𝑟 (cid:208) BackPropogate(𝑒𝑖, 𝑥 𝑗, 𝜑𝑖 ∧ 𝜓 ∗

𝑖 );

𝑖 , P, U);

return P𝑟 ;

Fig. 9. Algorithm to back propagate abstract value 𝜑 ∧ 𝜓 ∗ of expression 𝑒 = 𝑓 (𝑒1, . . . 𝑒𝑘 ), such that,
⟦𝑓 (𝜑1 ∧ 𝜓 ∗

1 , . . . 𝜑𝑘 ∧ 𝜓 ∗

𝑘 )⟧# =⇒ 𝜓𝑝 .

Proof. Let 𝜑 = ⟦𝑒⟧P𝑥 and 𝜓 ∗ be the abstract value from which predicates are extracted (line 10,

Figure 8). If 𝜓 ∗ was assigned by the if condition (line 9), then

However if 𝜓 ∗ was not assigned on line 9, then 𝜓 ∗ = (𝑠0 = 𝑝 [𝑥]), and the following is true:

𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(𝜑 ∧ 𝜓 ∗, 𝑦)

From Theorem 3.6,

𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧𝑥, 𝑦) = 𝐿(𝜑 ∧ 𝜓 ∗, 𝑦)

⟦𝑝∗⟧P∪P𝑟 =⇒ 𝜑 ∧ 𝜓 ∗

1:16

Shivam Handa and Martin Rinard

String expr 𝑒
Substring expr 𝑓
Position 𝑝
Direction 𝑑

:= Str(𝑓 ) | Concat(𝑓 , 𝑒);
:= ConstStr(𝑠) | SubStr(𝑥, 𝑝1, 𝑝2);
:= Pos(𝑥, 𝜏, 𝑘, 𝑑) | ConstPos(𝑘);
:= Start | End;

Fig. 10. DSL for string transformations where 𝜏 represents a token, 𝑘 is an integer, and 𝑠 is a string constant.

This implies 𝐿(𝜑 ∧ 𝜓 ∗, 𝑦) ≤ 𝐿(⟦𝑝∗⟧( P (cid:208) P𝑟 )𝑥, 𝑦) and therefore 𝐿(⟦𝑝∗⟧P𝑥, 𝑦) < 𝐿(⟦𝑝∗⟧( P (cid:208) P𝑟 )𝑥, 𝑦).
□

Theorem 3.8. (Completeness) Given a Dataset D, a DSL 𝐺, tolerance 𝜖 ≥ 0, universe of predicates
U, initial predicates P, objective function 𝑈 , loss function 𝐿, and the complexity measure 𝐶, the
algorithm 6 will eventually return some program 𝑝𝑟 .

Proof. Let A𝑖 be the FTA constructed in the 𝑖𝑡ℎ iteration of Algorithm 6. Let P𝑖 be the set
of predicates and let 𝑝𝑖 be the program returned by function MinCost on the 𝑖𝑡ℎ iteration. From
Theorem 3.7, if Distance(𝑝𝑖, D, P𝑖, 𝐿) > 𝜖), then for all 𝑘 > 𝑖:

Distance(𝑝𝑖, D, P𝑖+1, 𝐿) = 0 or Distance(𝑝𝑖, D, P𝑖, 𝐿) − Distance(𝑝𝑖, D, P𝑖+1, 𝐿) ≥ 𝛿 > 0
Since we restrict the size of the AFTA (by only considering programs of height less than equal to
some bound). We will reduce the Distance for some program in 𝐺 in every iteration. By induction,
we will eventually for some 𝑘,

Distance(𝑝𝑘, D, P𝑘, 𝐿) ≤ 𝜖

□

Theorem 3.9. (Correctness) Given a dataset D, a DSL 𝐺, tolerance 𝜖 ≥ 0, universe of predicates
U, initial predicates P, objective function 𝑈 , loss function 𝐿, and the complexity measure 𝐶, the
algorithm 6 will return a program 𝑝𝑟 which satisfies the 𝜖-correctness condition 3.4.

Proof. From Theorem 3.8, algorithm 6 will eventually terminate and return a program 𝑝𝑟 . From
□

Theorem 3.5, the returned program 𝑝𝑟 will satisfy the 𝜖-correctness condition.

4 Rose Implementation

We have implemented our synthesis algorithm in a tool called Rose. Rose is written in Java. The
implementation is modular and allows a user to plug-in different DSLs, abstract semantics, loss
functions, objective functions, and complexity measures. To support the experiments presented
in Section 5, we instantiate the Rose implementation with the string-processing domain-specific
language from[Handa and Rinard 2020; Wang et al. 2017a].
Domain Specific Language and Abstractions: We use the string processing domain specific
language from [Handa and Rinard 2020; Wang et al. 2017a] (Figure 10), which supports extracting
substrings (using the SubStr function) of the input string 𝑥 and concatenation of substrings (using
the Concat function). The function SubStr function extracts a substring using a start and an
end position. A position can either be a constant index (ConstPos) or the start or end of the 𝑘𝑡ℎ
occurrence of the match token 𝜏 in the input string (Pos).
Universe of Predicates: We construct a universe of predicates using predicates of the form
len(𝑠) = 𝑖, where 𝑠 is a symbol of a type of string and 𝑖 presents an integer. We also include
predicates of the form 𝑠 [𝑖] = 𝑐 indicating the 𝑖𝑡ℎ character of string 𝑠 is 𝑐. Besides these predicates,

Noisy Program Synthesis using Abstractions

1:17

⟦𝑓 (𝑠1 = 𝑐1, . . . , 𝑠𝑘 = 𝑐𝑘 )⟧#
⟦Concat(len(𝑓 ) = 𝑖1, len(𝑒) = 𝑖2)⟧#
⟦Concat(len(𝑓 ) = 𝑖1, 𝑒 [𝑖2] = 𝑐)⟧#
⟦Concat(len(𝑓 ) = 𝑖, 𝑒 = 𝑐)⟧#
⟦Concat(𝑓 [𝑖] = 𝑐, 𝑝)⟧#
⟦Concat(𝑓 = 𝑐, len(𝑒) = 𝑖)⟧#

:= (𝑠 = ⟦𝑓 (𝑐1, . . . 𝑐𝑘 )⟧)
:= (len(𝑒) = (𝑖1 + 𝑖2))
:= (𝑒 [𝑖1 + 𝑖2] = 𝑐)

:= (len(𝑒) = (𝑖 + len(𝑐)) ∧

:= (𝑒 [𝑖] = 𝑐)

len(𝑐)
(cid:211)
𝑗=1

𝑒 [𝑖 + 𝑗 − 1] = 𝑐 [ 𝑗 − 1]

:= (len(𝑒) = (len(𝑐) + 𝑖)) ∧

𝑒 [ 𝑗 − 1] = 𝑐 [ 𝑗 − 1]

⟦Concat(𝑓 = 𝑐1, 𝑒 [𝑖] = 𝑐2)⟧#
⟦Str(𝑝)⟧#

:= (𝑒 [len(𝑐1) + 𝑖] = 𝑐2) ∧

:= 𝑝

𝑒 [ 𝑗 − 1] = 𝑐1 [ 𝑗 − 1]

len(𝑐)
(cid:211)
𝑗=1
len(𝑐1)
(cid:211)
𝑗=1

Fig. 11. Abstract Semantics for String Transformation DSL.

we also include predicates of the form 𝑠 = 𝑐, where 𝑐 is a value which a symbol 𝑠 can take. We also
include both true and false. In summary, the universe of predicates, we are using, is:

U = (cid:8)len(𝑠) = 𝑖 | 𝑖 ∈ N(cid:9) ∪ (cid:8)𝑠 [𝑖] = 𝑐 | 𝑖 ∈ N, 𝑐 ∈ Char(cid:9) ∪ (cid:8)𝑠 = 𝑐 | 𝑐 ∈ Type(𝑠)(cid:9) ∪ (cid:8)true, false(cid:9)
Abstract Semantics: We define a generic transformer for conjunctions of predicates as follows:
(cid:19)

(cid:47)

(cid:47)

(cid:18)

(cid:219)

𝑓

(

𝑝𝑖1 ), . . . , (

𝑝𝑖𝑘 )

:=

(cid:219)

. . .

𝑓 (𝑝𝑖1, . . . 𝑝𝑖𝑘 )

𝑖1

𝑖𝑘

𝑖1

𝑖𝑘

This allows us to just define an abstract semantics for every possible combination of atomic
predicates, instead of abstract semantics for all possible abstract values. Figure 11 presents the
abstract semantics for functions in string processing DSL for all possible combinations of atomic
predicates.
Initial Abstraction: The initial abstraction set P includes predicates of form len(𝑠) = 𝑖, where 𝑠
is a symbol of type string and 𝑖 is an integer. It also includes true and false.
Abstractions and Loss Functions: We present the abstract version of the 0/∞ Loss Function and
0/1 Loss Function below:

𝐿0/∞(𝜑, 𝑦) = 0 if 𝑦 ∈ 𝛾 (𝜑), ∞ otherwise and 𝐿0/1(𝜑, 𝑦) = 0 if 𝑦 ∈ 𝛾 (𝜑), 1 otherwise
If 𝜑 ≠ false (𝐿𝐷𝐿 (false, 𝑦) = ∞), the abstract version of the Damerau-Levenshtein is 𝐿𝐷𝐿 (𝜑, 𝑦) =
𝑑𝑐,𝑦 (|𝑐 |, |𝑦|), where 𝑐 = ToStr(𝜑, 𝑦) and 𝑑 is defined below:

𝑑𝑐,𝑦 (𝑖, 𝑗) = min

𝑗
𝑖
𝑑𝑐,𝑦 (𝑖 − 1, 𝑗 − 1)
1 + 𝑑𝑐,𝑦 (𝑖 − 1, 𝑗 − 1)
1 + 𝑑𝑐,𝑦 (𝑖 − 1, 𝑗)
1 + 𝑑𝑐,𝑦 (𝑖, 𝑗 − 1)
𝑑𝑐,𝑦 (𝑖, 𝑗 − 1)
1 + 𝑑𝑐,𝑦 (𝑖 − 2, 𝑗 − 2)






𝑖 = 0
𝑗 = 0
𝑖, 𝑗 > 0 and (𝑐 [𝑖 − 1] = null or 𝑐 [𝑖 − 1] = 𝑦 [ 𝑗 − 1])
𝑖, 𝑗 > 0 and (𝑐 [𝑖 − 1] ≠ null and 𝑐 [𝑖 − 1] ≠ 𝑦 [ 𝑗 − 1])
𝑖 > 0
𝑗 > 0
𝑖 = |𝑦| and 𝜑 may contain strings of multiple lengths.
𝑖, 𝑗 > 1 and (𝑐 [𝑖 − 1] = null or 𝑐 [𝑖 − 1] = 𝑦 [𝑖 − 2])
and (𝑐 [𝑖 − 2] = null or 𝑐 [𝑖 − 2] = 𝑦 [𝑖 − 1])

1:18

Shivam Handa and Martin Rinard

Let P = ExtractPredicates(𝜑). The procedure ToStr returns an array 𝑐, such that, if len(𝑠) = 𝑖 ∈ P
then |𝑐 | = 𝑖, otherwise it is the maximum of the length of string 𝑦 or 𝑖 such that 𝑠 [𝑖] = 𝑐 ′ ∈ P. For
all 𝑠 [𝑖] = 𝑐𝑖 ∈ P, 𝑐 [𝑖] = 𝑐𝑖 , otherwise it is null.

In addition to the loss functions introduced in Section 2 use the following loss functions:

Definition 4.1. 1-Delete Loss Function: The 1-Delete loss function returns 0 if the outputs from
the synthesized program and the data set match exactly, 1 if a single deletion enables the output
from the synthesized program to match the output from the data set, and ∞ otherwise:

𝐿1𝐷 (𝑧, 𝑦) =

𝑧 = 𝑦
𝑎 · 𝑐 · 𝑏 = 𝑧 ∧ 𝑎 · 𝑏 = 𝑦 ∧ |𝑐 | = 1

0
1
∞ otherwise





The abstract version of the 1-Delete Loss Function:




0
1
∞ otherwise

𝐿1𝐷 (𝜑, 𝑦) =

𝑦 ∈ 𝛾 (𝜑)
𝑎 · 𝑏 = 𝑦 and (∃𝑐.𝑎 · 𝑐 · 𝑏 ∈ 𝛾 (𝜑) and |𝑐 | = 1)

Definition 4.2. 𝑛-Substitution Loss Function: The 𝑛-Substitution loss function counts the
number of positions where the noisy output does not agree with the output from the synthesized
program. If the synthesized program produces an output that is longer or shorter than the output
in the noisy data set, the loss function is ∞:

𝐿𝑛𝑆 (𝑧, 𝑦) =

|𝑧| ≠ |𝑦|

1 if 𝑧 [𝑖] ≠ 𝑦 [𝑖] else 0

|𝑧| = |𝑦|

∞
|𝑧 |
(cid:205)
𝑖=1





The abstract version of the 𝑛-Substitution loss function is:

𝐿𝑛𝑆 (𝜑, 𝑦) =

0

∞
𝐿𝑛𝑆 (𝑐, 𝑦)
∞
|𝑦 |
(cid:205)
𝑗=1

1(𝑐 [ 𝑗] ≠ null and 𝑐 [ 𝑗] ≠ 𝑦 [𝑖 𝑗 ])

𝜑 = true
𝜑 = false
𝜑 = (𝑠 = 𝑐)
𝑐𝜑 = ToStr(𝜑, 𝑦) and |𝑐𝜑 | ≠ |𝑦|

𝑐 = ToStr(𝜑, 𝑦) and |𝑐 | = |𝑦|






Incremental Automata Update: To avoid regenerating the entire FTA at every iteration of the
algorithm as in Algorithm 6, our Rose implementation applies on optimization that incrementally
updates parts of the FTA as appropriate at every iteration of the algorithm.

5 Experimental Results
We use the SyGuS 2018 benchmark suite [Alur et al. 2013] to evaluate Rose against the current
state-of-the-art noisy program synthesis system presented in [Handa and Rinard 2020]. The SyGus
2018 benchmark suite contains a range of string transformation problems, a class of problems
that has been extensively studied in past program synthesis projects [Gulwani 2011; Polozov and
Gulwani 2015; Singh and Gulwani 2016]. [Handa and Rinard 2020] use this benchmark suite to
benchmark their system by systematically introducing noise within these benchmarks. We recreated
the scenarios studied in [Handa and Rinard 2020] and report results for these scenarios.

We run all experiments on a 3.00 GHz Intel(R) Xeon(R) CPU E5-2690 v2 machine with 512GB
memory running Linux 4.15.0. We set a timeout limit of 10 minutes for each synthesis task. We

Noisy Program Synthesis using Abstractions

1:19

compare Rose with the noisy program synthesis system presented in [Handa and Rinard 2020]
(see Section 2), running with a bounded scope height threshold of 4 for all experiments ([Handa and
Rinard 2020], Section 9.1). We call this system CFTA.

5.1 Noisy Data Sets
Table 12 presents results for all SyGus 2018 benchmark problems which contain less than ten
input/output examples. We omit univ_1, univ_2, and univ_4-6 — these problems time out for both
Rose and CFTA (so the rows would contain all -). The first column (Benchmark) presents the name
of the SyGus 2018 benchmark. The second column (Number of Input/Output Examples) presents
the number of input/output examples in the benchmark problem. The remaining columns present
running times, in milliseconds, for Rose and CFTA running with different noise sources and loss
functions. A - indicates that the corresponding run timed out without synthesizing a program.

The objective function is the lexicographic objective function. The complexity measure is program
size. The noise source cyclically deletes a single character from outputs in the data set, starting
with the first character, then wrapping around when reaching the last position in the output. When
𝑛 = 1, the noise source corrupts the last output in the set of input/output examples. When 𝑛 = 3,
the noise source corrupts the last 3 input/output examples in the set of input/output examples.
For Rose , the table presents results for each of the 𝐿0/1, 𝐿𝐷𝐿, and 𝐿1𝐷 loss functions. For CFTA, we
report one running time for each benchmark problem — for CFTA, the running time is the same
for all noise source/loss function combinations.

For the benchmarks on which both terminate, Rose runs up to 1957 times faster than CFTA, with
a median speedup of 20.5 times over CFTA. This performance increase enables Rose to successfully
synthesize programs for 20 more benchmark problems than CFTA — Rose synthesizes programs for
44 of the 54 benchmark problems (timing out on the remaining 10), while CFTA can only synthesize
programs for 24 of the 54 benchmark problems (timing out on the remaining 30). These results
highlight the substantial performance benefits that Rose delivers.

Every synthesized program is guaranteed to minimize the objective function over the given
input/output examples. For 𝑛 = 1, all synthesized programs also have zero loss over the original
(unseen during synthesis) noise-free input/output examples (i.e., all synthesized programs generate
the correct output for each given input). For 𝑛 = 3, 34, 40, and 40 out of 44 synthesized programs, for
𝐿0/1, 𝐿𝐷𝐿, and 𝐿1𝐷 respectively, have zero loss over the original noise-free input/output examples.
For a given noise source/loss function combination, CFTA and Rose synthesize the same program
(unless one or both of the systems times out). These results highlight the ability of Rose to synthesize
correct programs even in the face of significant noise.

Table 13 presents results for the SyGus 2018 phone-*-long-repeat benchmarks running with
a noise source that cyclically and probabilistically replaces a single digit in each output string
with the next digit (wrapping back to 0 if the current digit is 9). The noise source iterates through
each output string in the data set in turn, probabilistically replacing the next character position in
each output string with another character, wrapping around to the first character position when
it reaches the last character position in the output string. The noise source corrupts 95% of the
input-output examples in each dataset.

We report results for two loss functions, 𝐿𝑛𝑆 (𝑛-substitution) and 𝐿𝐷𝐿 (Damerau-Levenshtien).
The objective function is the lexicographic objective function. The complexity measure is program
size. There is a row in the table for each phone-*-long-repeat benchmark; each entry presents
the running time (in milliseconds) for the corresponding synthesis algorithm running on the
corresponding benchmark problem.

For the benchmarks on which both terminate, Rose runs up to 81.7 times faster than CFTA, with
a median speedup of 39.0 times over CFTA. This performance increase enables Rose to successfully

1:20

Shivam Handa and Martin Rinard

Benchmark

No of examples

bikes
bikes_small
dr-name
dr-name_small
firstname
firstname_small
initials
initials_small
lastname
lastname_small
name-combine
name-combine_short
name-combine-2
name-combine-2_short
name-combine-3
name-combine-3_short
name-combine-4
name-combine-4_short
phone
phone_short
phone-1
phone-1_short
phone-2
phone-2_short
phone-3
phone-3_short
phone-4
phone-4_short
phone-5
phone-5_short
phone-6
phone-6_short
phone-7
phone-7_short
phone-8
phone-8_short
phone-9
phone-9_short
phone-10
phone-10_short
reverse-name
reverse-name_short
univ_3
univ_3_short

6
6
4
4
4
4
4
4
4
4
6
6
4
4
6
6
5
5
6
6
6
6
6
6
7
7
6
6
7
7
7
7
7
7
7
7
7
7
7
7
6
6
6
6

𝑛 = 1
𝐿𝐷𝐿
67
68
310
312
105
106
289
293
113
114
918
875
1721
1697
287
294
1683
1664
68
68
62
64
73
76
559
530
2067
2057
101
99
138
140
132
133
144
142
29672
29055
73379
77495
645
662
4117
4364

𝐿0/1
68
69
402
424
118
113
336
361
124
122
1288
1301
2100
2120
298
313
1863
1888
66
65
62
61
83
117
926
789
2571
2637
114
109
171
170
165
165
158
157
28815
28658
87772
85861
699
709
6258
6345

Rose

𝐿1𝐷
65
65
408
376
114
118
353
343
120
120
1609
1528
2162
2188
294
291
1917
1914
67
66
61
60
79
80
692
652
3054
2872
110
105
168
169
153
162
157
154
28029
28729
66170
75849
697
811
5994
6503

𝐿0/1
67
67
359
330
96
96
244
249
97
97
1330
1333
740
735
301
296
1875
1815
66
67
59
62
80
80
839
786
2678
2608
112
116
175
173
159
157
162
162
28576
29115
76963
86508
703
712
6260
6331

𝑛 = 3
𝐿𝐷𝐿
67
65
324
302
319
300
-
-
114
116
984
945
-
-
289
278
1584
1604
68
66
63
63
79
76
611
607
2202
2256
98
98
142
135
134
136
145
141
26465
27818
87778
81526
666
625
3499
3510

𝐿1𝐷
68
65
390
375
367
393
-
-
121
121
1477
1437
-
-
299
290
1921
1875
64
68
62
60
78
79
648
600
2963
3014
108
110
166
163
155
154
156
155
30941
28157
69130
65247
671
685
6068
5962

CFTA

19554
21210
-
-
4258
4220
36188
30920
175762
178825
-
-
-
-
547447
544044
-
-
943
963
933
942
953
943
-
-
-
-
122
127
3230
3327
2793
2762
3464
3223
-
-
-
-
-
-
-
-

Fig. 12. Runtime performance of Rose and CFTA on benchmarks with deletion based noise.

Noisy Program Synthesis using Abstractions

1:21

Benchmark

No of Examples

phone-long-repeat
phone-1-long-repeat
phone-2-long-repeat
phone-3-long-repeat
phone-4-long-repeat
phone-5-long-repeat
phone-6-long-repeat
phone-7-long-repeat
phone-8-long-repeat
phone-9-long-repeat
phone-10-long-repeat

400
400
400
400
400
400
400
400
400
400
400

Rose

𝐿𝑛𝑆
745
728
806
1919
3332
1145
1227
1253
1207
18795
41485

𝐿𝐷𝐿
749
749
784
2297
4683
1156
1315
1341
1290
39032
113397

CFTA

28836
28547
29766
-
-
2179
100241
89535
88932
-
-

Fig. 13. Rose and CFTA’s performance on dataset corrupted by substitution based noise.

synthesize programs for 4 more benchmark problems than CFTA — Rose synthesizes programs for
all 11 of the benchmark problems, while CFTA synthesizes programs for 7 of the 11 benchmark
problems (timing out on the remaining 4). Once again, these results highlight the substantial
performance benefits that Rose delivers.

Every synthesized program is guaranteed to minimize the objective function over the given
input/output examples. All synthesized programs have zero loss over the original (unseen during
synthesis) noise-free input/output examples (i.e., all synthesized programs generate the correct
output for each given input). Once again, these results highlight the ability of Rose to synthesize
correct programs even in the face of significant noise.
Noise-Free Data Sets We also evaluated the performance of Rose and CFTA by applying it to all
problems in the SyGuS 2018 benchmark suite [Alur et al. 2013]. For each problem we synthesize
the optimal program over clean (noise-free) datasets. We present the result of all SyGuS benchmark
suite in the appendix A.2.

For the benchmarks on which both terminate, Rose runs up to 1831 times faster than CFTA, with
the median speedup of 35.7 over CFTA. This enables Rose to successfully synthesize programs for
45 more benchmark problems that CFTA – Rose synthesizes programs for 90 of the 108 benchmark
problems (timing out on the remaining 18 problems), while CFTA can only synthesize the correct
program for 45 of the 108 benchmark problems (timing out of the remaining problems).

6 Related Work
We discuss related work in the following areas:
Programming-By-Example/Noise-Free Synthesis: Synthesizing programs from a set of input-
output examples has been a prominent topic of research for many years [Gulwani 2011; Shaw
1975; Singh and Gulwani 2016]. These techniques either require the entire dataset to be noise free,
or they try to remove corrupted input-output examples from the dataset before synthesizing the
correct program. Instead of removing corrupted examples from the dataset, our technique uses
the loss function to capture information from any corrupted examples and uses this information
during the synthesis. The experimental results show that, for our set of benchmarks, our approach
can synthesize the correct program even in the presence of substantial noise.
Neural Program Synthesis/Machine-Learning Approaches: Researchers have investigated
techniques that use machine learning/deep neural networks to synthesize programs [Balog et al.
2016; Devlin et al. 2017; Raychev et al. 2016]. The techniques primarily focus on synthesizing

1:22

Shivam Handa and Martin Rinard

programs over noise-free datasets. These techniques require a training phase and a differentable
loss function and provide no guarantees that the synthesized program will minimize the objective
function. Our technique, in contrast, does not require a training phase, can work with arbitrary
loss functions including, for example, the Damerau-Levenshtien loss function, and comes with a
guarantee that the synthesized program will minimize the objective function over the provided
(noisy) input/output examples.
Tree Automata/VSA based synthesis algorithms: [Polozov and Gulwani 2015; Singh and
Gulwani 2016; Wang et al. 2017b] all construct either version spaces or tree automata to represent
all programs (within a bounded search space) which satisfy a set of input-output examples. These
techniques build the version space in one shot before synthesis and require the dataset to be either
be correct or pruned to remove any corrupt input/output examples.

Our technique also works with version spaces (as represented in tree automata), but extends the
approach to work with noisy data sets. It also introduces an abstraction based optimization process
which can iteratively expand and improve the version space during synthesis (instead of building it
in one shot as in previous research).
Abstraction-Refinement based Synthesis Algorithms: There has been work done on using
abstraction refinement/refinement types to synthesize programs [Guo et al. 2019; Polikarpova et al.
2016; Wang et al. 2017b]. Given a noise free dataset and a program, checking if a program is correct
or incorrect simply checks if the synthesized programs satisfy all input/output examples. To refine
an abstraction, these techniques construct a proof of incorrectness. Each abstraction identifies a set
of programs, some of which may be correct and others of which may be incorrect. Refinement first
identifies a program that does not satisfy one or more of the input/output examples, then generates
constraints that refine the abstraction to eliminate this program. Iterative refinement eventually
produces the final program.

Abstraction in our noisy program synthesis framework, in contrast, works with an abstraction
that approximates the loss function over a set of programs. The refinement step selects a program
within the abstraction space, computes its loss, then uses this computed loss to refine the loss
approximation to bring this approximation closer to the actual loss. This refinement step, in
expectation, reduces the inaccuracy in the approximated loss function of the programs identified
by the abstraction. In contrast to previous approaches, which work with abstractions based on
program correctness and refinement steps that eliminate incorrect programs, our approach works
with abstractions that maintain a sound, conservative approximation of the minimum loss function
over the set of programs identified by the abstraction and refinement steps that eliminate programs
based on the loss of the programs.

One key difference is that refinement steps in previous techniques rely on the ability to identify
correct and incorrect programs. Because our technique works with noisy data sets, it can never
tell if a candidate program has minimal loss without comparing the program to all other current
candidate programs (unless the loss happens to be zero). It instead uses abstract minimum loss
values to bound how far off the optimal loss any candidate program may be. Instead of working
with correct or incorrect programs, our technique works by iteratively improving the accuracy of
the minimum loss function estimation captured by the abstraction.

Our technique therefore combines abstract tree automata with an abstraction-based optimization
process. Our approach, in contrast to previous approaches that use abstract tree automata, enables
us to synthesize programs that optimize an objective function over a set of noisy input/output
examples, including synthesizing correct programs that may disagree with one, some, or even all
of the provided input/output examples.

[Wang et al. 2017a] uses abstract tree automata and abstraction refinement for program synthesis.
Because their refinement strategy prunes any program that does not satisfy all of the provided

Noisy Program Synthesis using Abstractions

1:23

input/output examples, their algorithm requires the dataset to be noise free. This pruning is
necessary as this allows their technique to effectively capture constraints to prune large part of the
search space.
Noisy Program Synthesis: Handa and Rinard formalize a noisy program synthesis framework
and present a technique that uses finite tree automata to synthesize the program which best fits the
noisy dataset based on an objective function, a loss function, and a complexity measure [Handa and
Rinard 2020]. We present a simplified version of the presented technique in Section 2. This paper
presents a new abstraction/refinement technique to solve the noisy program synthesis problem.
Our experimental results show that this new technique significantly outperforms the technique
presented in [Handa and Rinard 2020].

Handa and Rinard also formalize a connection, in the context of noisy program synthesis, between
the characteristics of the noise source and the hidden program that together generate the noisy
data set and the characteristics of the loss function [Handa and Rinard 2021]. Specifically, the
paper identifies, given a noise model, a corresponding optimal loss function as well as properties of
combinations of noise models and loss functions that ensure that the presented noisy synthesis
algorithm will converge to the correct program given enough noisy input/output pairs. This work
is complementary to the work presented in this paper. All of the guarantees established in [Handa
and Rinard 2021] also apply to the algorithm presented in this paper.
Best Effort Synthesis: [Peleg and Polikarpova 2020] presents an enumeration-based technique to
synthesize programs from input-output datasets containing some incorrect outputs. Their technique
returns a ranked list of partially valid programs, removing programs which are observationally
equivalent. Their technique uses a fixed fitness function to order these partial results. [Peleg and
Polikarpova 2020] uses a specific loss function and a specific complexity measure to rank candidate
programs. Given this loss function and complexity measure, our technique will synthesize the exact
same program. Our technique also supports the use of a large class of loss functions, complexity
measures, and objective functions. [Handa and Rinard 2020] has showcased how crafting suitable
loss functions is essential and allows one to synthesize the correct program even when some or
even all input/output examples are corrupted.

7 Conclusion
We present a new technique to synthesize programs over noisy datasets. This technique uses an
abstraction refinement based optimization process to search for a program which best-fits a given
dataset, based on an objective function, a loss function, and a complexity measure. The algorithm
deploys an abstract semantics to soundly approximate the minimum loss function over abstracted
sets of programs in an underlying domain-specific language. Iterative refinement based on this
sound approximation produces a program whose loss value is within a specified tolerance level of
the program with optimal loss over the given noisy input/output data set. We provide a proof that
the technique is sound and complete and will always synthesize an 𝜖-correct program.

We have implemented our synthesis algorithm in the Rose noisy program synthesis system. Our
experimental results show that, on two noisy benchmark program synthesis problem sets drawn
from the SyGus 2018 benchmarks, Rose delivers speedups of up to 1587 and 81.7 over a previous
state-of-the art noisy synthesis system, with median speedups of 20.5 and 81.7 over this previous
system. Rose also terminates on 20 (out of 54) and 4 (out of 11) more benchmark problems than the
previous system. Both Rose and the previous system synthesize programs that are optimal over the
provided noisy data sets. For the majority of the problems in the benchmark sets (272 out of 286
for Rose), both systems also synthesize programs that produce correct outputs for all inputs in the
original (unseen) noise-free data set. These results highlight the significant benefits that Rose can
deliver for effective noisy program synthesis.

1:24

References

Shivam Handa and Martin Rinard

Rajeev Alur, Rastislav Bodik, Garvit Juniwal, Milo MK Martin, Mukund Raghothaman, Sanjit A Seshia, Rishabh Singh,
Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. 2013. Syntax-guided synthesis. In 2013 Formal Methods in
Computer-Aided Design. IEEE, 1–8.

Matej Balog, Alexander L Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. 2016. Deepcoder: Learning to

write programs. arXiv preprint arXiv:1611.01989 (2016).

Patrick Cousot and Radhia Cousot. 1977. Abstract interpretation: a unified lattice model for static analysis of programs by
construction or approximation of fixpoints. In Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of
programming languages. 238–252.

Fred J. Damerau. 1964. A Technique for Computer Detection and Correction of Spelling Errors. Commun. ACM 7, 3 (March

1964), 171–176. https://doi.org/10.1145/363958.363994

Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. 2017.
Robustfill: Neural program learning under noisy I/O. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70. JMLR. org, 990–998.

Yu Feng, Ruben Martins, Jacob Van Geffen, Isil Dillig, and Swarat Chaudhuri. 2017. Component-based synthesis of table

consolidation and transformation tasks from examples. In ACM SIGPLAN Notices, Vol. 52. ACM, 422–436.

John K Feser, Swarat Chaudhuri, and Isil Dillig. 2015. Synthesizing data structure transformations from input-output

examples. In ACM SIGPLAN Notices, Vol. 50. ACM, 229–239.

Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. 1993. Directed hypergraphs and applications. Discrete

applied mathematics 42, 2-3 (1993), 177–201.

Sumit Gulwani. 2011. Automating string processing in spreadsheets using input-output examples. In ACM Sigplan Notices,

Vol. 46. ACM, 317–330.

Zheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang, Ranjit Jhala, and Nadia Polikarpova. 2019. Program
synthesis by type-guided abstraction refinement. Proceedings of the ACM on Programming Languages 4, POPL (2019),
1–28.

Shivam Handa and Martin Rinard. 2021. Program Synthesis Over Noisy Data with Guarantees. arXiv:cs.PL/2103.05030
Shivam Handa and Martin C Rinard. 2020. Inductive program synthesis over noisy data. In Proceedings of the 28th ACM Joint
Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. 87–98.
Bishoksan Kafle and John P Gallagher. 2015. Tree automata-based refinement with application to Horn clause verification.

In International Workshop on Verification, Model Checking, and Abstract Interpretation. Springer, 209–226.

Peter-Michael Osera and Steve Zdancewic. 2015. Type-and-example-directed program synthesis. ACM SIGPLAN Notices 50,

6 (2015), 619–630.

Hila Peleg and Nadia Polikarpova. 2020. Perfect is the Enemy of Good: Best-Effort Program Synthesis. In 34th European

Conference on Object-Oriented Programming (ECOOP 2020). Schloss Dagstuhl-Leibniz-Zentrum für Informatik.

Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama. 2016. Program synthesis from polymorphic refinement types.

ACM SIGPLAN Notices 51, 6 (2016), 522–538.

Oleksandr Polozov and Sumit Gulwani. 2015. FlashMeta: a framework for inductive program synthesis. In ACM SIGPLAN

Notices, Vol. 50. ACM, 107–126.

Veselin Raychev, Pavol Bielik, Martin Vechev, and Andreas Krause. 2016. Learning programs from noisy data. In ACM

SIGPLAN Notices, Vol. 51. ACM, 761–774.

D Shaw. 1975. Inferring LISP Programs From Examples.
Rishabh Singh and Sumit Gulwani. 2016. Transforming spreadsheet data types using examples. In Acm Sigplan Notices,

Vol. 51. ACM, 343–356.

Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017a. Program synthesis using abstraction refinement. Proceedings of the ACM

on Programming Languages 2, POPL (2017), 63.

Xinyu Wang, Isil Dillig, and Rishabh Singh. 2017b. Synthesis of data completion scripts using finite tree automata. Proceedings

of the ACM on Programming Languages 1, OOPSLA (2017), 62.

Navid Yaghmazadeh, Christian Klinger, Isil Dillig, and Swarat Chaudhuri. 2016.
hierarchically structured data. In ACM SIGPLAN Notices, Vol. 51. ACM, 508–521.

Synthesizing transformations on

Noisy Program Synthesis using Abstractions

1:25

A Appendix

A.1 Appendix: Additional Theorems

Theorem A.1. Given P and P∗, such that, {true, false} ⊆ P ⊆ P∗ ⊆ U is true, then for any

abstract value 𝜑, the following statement is true:

𝛼 P∗

(𝜑) =⇒ 𝛼 P (𝜑)

Proof. Proof by contradiction. Assuming 𝛼 P∗ (𝜑) = 𝜑1 and 𝛼 P (𝜑) = 𝜑2, such that, 𝜑1

̸=⇒ 𝜑2.

Note that:

𝜑 =⇒ 𝜑1 ∧ 𝜑2, 𝜑1 ∧ 𝜑2 =⇒ 𝜑1, and 𝜑1 ∧ 𝜑2 =⇒ 𝜑2
𝜑1 ∧ 𝜑2 can be expressed by using predicates in P∗, and 𝜑1 ∧ 𝜑2 is stronger than 𝜑1. Hence
𝜑1 ≠ 𝛼 P∗ (𝜑). Therefore, by contradiction, the above theorem is true.
□

Theorem A.2. Given a set of predicates P ⊆ U and P∗ ⊆ U, such that {true} ⊆ P ⊆ P∗, then
for any expression 𝑒 (starting from symbol 𝑠) and any input value 𝑥𝑖 , the following statement is true:
(𝑠 = ⟦𝑒⟧𝑥𝑖 ) =⇒ ⟦𝑒⟧P∗𝑥𝑖 and ⟦𝑒⟧P∗𝑥𝑖 =⇒ ⟦𝑒⟧P𝑥𝑖

i.e., adding more predicates will make the abstract computation more precise.

Proof. We prove this using induction over height of expression 𝑒.

Base Case: The height of 𝑒 is 1, i.e., 𝑒 is a terminal 𝑡. Since true ∈ P, using definition of 𝛼 P:

From Theorem A.1:

(𝑠 = ⟦𝑡⟧𝑥𝑖 ) =⇒ 𝛼 P∗

(𝑠 = ⟦𝑡⟧𝑥𝑖 )

𝛼 P∗

(𝑠 = ⟦𝑡⟧𝑥𝑖 ) =⇒ 𝛼 P (𝑠 = ⟦𝑡⟧𝑥𝑖 )

Induction Hypothesis: For all expressions 𝑒 of height less than equal to 𝑛, the following statement is
true:

(𝑠 = ⟦𝑒⟧𝑥𝑖 ) =⇒ ⟦𝑒⟧P∗𝑥𝑖 and ⟦𝑒⟧P∗𝑥𝑖 =⇒ ⟦𝑒⟧P𝑥𝑖
Induction Step: Consider an expression 𝑒 of height 𝑛+1. Without loss of generality, we can assume 𝑒 is
of the form 𝑓 (𝑒1, . . . 𝑒𝑘 ), where height of sub-expressions 𝑒1, . . . 𝑒𝑘 is less than equal to 𝑛. Therefore,
for all 𝑗 ∈ [1, 𝑛]

(𝑠 = ⟦𝑒 𝑗 ⟧𝑥𝑖 ) =⇒ ⟦𝑒 𝑗 ⟧P∗𝑥𝑖 and ⟦𝑒 𝑗 ⟧P∗𝑥𝑖 =⇒ ⟦𝑒 𝑗 ⟧P𝑥𝑖

and for all 𝑗 ∈ [1, 𝑛]

⟦𝑒 𝑗 ⟧𝑥𝑖 ∈ 𝛾 (⟦𝑒 𝑗 ⟧P∗𝑥𝑖 ) ⊆ 𝛾 (⟦𝑒 𝑗 ⟧P∗𝑥𝑖 )
Let 𝛼 P (⟦𝑓 (⟦𝑒1⟧P∗𝑥𝑖, . . . ⟦𝑒𝑘 ⟧P∗𝑥𝑖 )⟧#) = 𝜑 ∗ and 𝛼 P (⟦𝑓 (⟦𝑒1⟧P𝑥𝑖, . . . ⟦𝑒𝑘 ⟧P𝑥𝑖 )⟧#) = 𝜑. Note that:
⟦𝑓 (⟦𝑒1⟧𝑥𝑖, . . . ⟦𝑒𝑘 ⟧𝑥𝑖 )⟧ ∈ 𝛾 (𝜑 ∗) ⊆ 𝛾 (𝜑)

Therefore,

(𝑠 = ⟦𝑒⟧𝑥𝑖 ) =⇒ 𝛼 P∗

(𝑠 = ⟦𝑒⟧𝑥𝑖 ) =⇒ 𝜑 ∗ and 𝜑 ∗ =⇒ 𝜑
(𝜑 ∗) and 𝛼 P∗

(𝜑 ∗) =⇒ 𝛼 P∗

(𝜑) and 𝛼 P∗

Hence, by induction, the above theorem is true.

(𝜑) =⇒ 𝛼 P (𝜑)

□

Theorem A.3. (Structure of the Tree Automaton) Given a set of predicates P, input vector (cid:174)𝑥 =
⟨𝑥1, . . . 𝑥𝑛⟩, and DSL 𝐺, let A = (𝑄, 𝑄 𝑓 , Δ) be the AFTA returned by the function ConstructAFTA( (cid:174)𝑥, 𝐺, P).
Then for all symbols 𝑠 in 𝐺, for all expressions 𝑒 starting from symbol 𝑠 (and height less than
bound 𝑏), there exists a state 𝑞 (cid:174)𝜑
𝑠 }, Δ), where
(cid:174)𝜑 = ⟨⟦𝑒⟧P𝑥1, . . . ⟦𝑒⟧P𝑥𝑛⟩.

𝑠 ∈ 𝑄, such that, 𝑒 is accepted by the automaton (𝑄, {𝑞 (cid:174)𝜑

1:26

Shivam Handa and Martin Rinard

Proof. We prove this theorem by using induction over height of the expression 𝑒.

𝑡 }, Δ).

𝑡 ∈ 𝑄 (for terminal 𝑡), where (cid:174)𝜑 = ⟨⟦𝑡⟧P𝑥1, . . . ⟦𝑡⟧P𝑥𝑛⟩

Base Case: Height of expression 𝑒 is 1. This implies the symbol is either 𝑥 or a constant. According to
Var and Const rules (Figure 5), there exists state 𝑞 (cid:174)𝜑
and 𝑡 is accepted by automaton (𝑄, {𝑞 (cid:174)𝜑
Inductive Hypothesis: For all symbols 𝑠 in 𝐺, for all expressions 𝑒 starting from symbol 𝑠 of height less
than equal to 𝑛, there exists a state 𝑞 (cid:174)𝜑
𝑠 }, Δ),
where (cid:174)𝜑 = ⟨⟦𝑒⟧P𝑥1, . . . ⟦𝑒⟧P𝑥𝑛⟩.
Induction Step: For any symbol 𝑠 in 𝐺, consider an expression 𝑒 = 𝑓 (𝑒1, . . . 𝑒𝑘 ) of height equal to
𝑛 + 1, created from production 𝑠 ← 𝑓 (𝑠1, . . . 𝑠𝑘 ). Note the height of expressions 𝑒1, . . . 𝑒𝑘 is less than
equal to 𝑛, therefore using induction hypothesis, there exists states 𝑞 (cid:174)𝜑1
𝑠𝑘 ∈ 𝑄, such that 𝑒𝑖 is
𝑠1
accepted by automaton (𝑄, {𝑞 (cid:174)𝜑𝑖
𝑠𝑖 }, Δ), where (cid:174)𝜑𝑖 = ⟨⟦𝑒𝑖 ⟧P𝑥1, . . . ⟦𝑒𝑖 ⟧P𝑥𝑛⟩. Note based on abstract
execution rules (Figure 4):

𝑠 ∈ 𝑄, such that, 𝑒 is accepted by the automaton (𝑄, {𝑞 (cid:174)𝜑

, . . . 𝑞 (cid:174)𝜑𝑘

⟦𝑒⟧P𝑥𝑖 = 𝛼 P (⟦𝑓 (⟦𝑒1⟧P𝑥𝑖, . . . ⟦𝑒𝑘 ⟧P𝑥𝑖 )⟧#)

According to Prod rule (Figure 5), there exists a state 𝑞 (cid:174)𝜑
𝑒 is accepted by (𝑄, {𝑞 (cid:174)𝜑

𝑠 }, Δ).

𝑠 ∈ 𝑄, where (cid:174)𝜑 = ⟨⟦𝑒⟧P𝑥1, . . . ⟦𝑒⟧P𝑥𝑛⟩, and

Therefore, by induction, for all symbols 𝑠 in 𝐺, for all expressions 𝑒 starting from symbol 𝑠 (and
𝑠 ∈ 𝑄, such that, 𝑒 is accepted by the automaton
□

height less than bound 𝑏), there exists a state 𝑞 (cid:174)𝜑
(𝑄, {𝑞 (cid:174)𝜑
𝑠 }, Δ), where (cid:174)𝜑 = ⟨⟦𝑒⟧P𝑥1, . . . ⟦𝑒⟧P𝑥𝑛⟩.

Theorem A.4. Given predicates P, DSL 𝐺, noisy dataset D = ( (cid:174)𝑥, (cid:174)𝑦), objective function 𝑈 , loss
function 𝐿, complexity measure 𝐶, and A = ConstructAFTA( (cid:174)𝑥, 𝐺, P), if 𝑝∗ = MinCost(A, D, 𝑈 , 𝐿, 𝐶)
then

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝))

i.e., 𝑝∗ minimizes the abstract objective function.

Proof. Let A = (𝑄, 𝑄 𝑓 , Δ). From corollary 3.2, for each program 𝑝 ∈ 𝐺, there exists a state
𝑠0 ∈ 𝑄 𝑓 , such that, ⟦𝑝⟧P (cid:174)𝑥 = (cid:174)𝜑. Since the algorithm finds an accepting state 𝑞 (cid:174)𝜑 ∗
𝑞 (cid:174)𝜑
𝑠0 ∈ 𝑄 𝑓 , such that,
for all accepting states 𝑞 (cid:174)𝜑

𝑠0 ∈ 𝑄 𝑓 ,

⟨𝐿( (cid:174)𝜑 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑 ∗

𝑠0 ])⟩ ≤𝑈 ⟨𝐿( (cid:174)𝜑, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑

𝑠0])⟩

for all 𝑝 ∈ 𝐺,

Since 𝑝∗ = 𝑃 [𝑞 (cid:174)𝜑 ∗
𝑠0 ],

⟨𝐿( (cid:174)𝜑 ∗, (cid:174)𝑦), 𝐶 (𝑃 [𝑞 (cid:174)𝜑 ∗

𝑠0 ])⟩ ≤𝑈 ⟨𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝)⟩

𝑝∗ ∈ argmin𝑝 ∈𝐺𝑈 (𝐿(⟦𝑝⟧P (cid:174)𝑥, (cid:174)𝑦), 𝐶 (𝑝))

□

Theorem A.5. Given expression 𝑒 = 𝑓 (𝑒1, . . . 𝑒𝑛), input 𝑥, abstract value 𝜓𝑝 (assuming (𝑠 =
⟦𝑒⟧𝑥) =⇒ 𝜓𝑝 , predicates P, and universe of predicates U, if the procedure BackPropogate(𝑒, 𝑥,𝜓𝑝, P, U)
returns predicate set P𝑟 then:

⟦𝑒⟧P∪P𝑟 𝑥 =⇒ 𝜓𝑝

Proof. We prove this theorem using induction over height of expression 𝑒.

Base Case: Height of 𝑒 is 2. This means all sub-expressions 𝑒1, . . . 𝑒𝑘 are terminals. Note that
P𝑟 ⊆ ExtractPredicates(𝜓 ∗

𝑖 ), for all 𝑖 ∈ [1, 𝑘].

⟦𝑒𝑖 ⟧P∪P𝑟 =⇒ 𝜑𝑖 ∧ 𝜓 ∗
𝑖

Noisy Program Synthesis using Abstractions

1:27

and

therefore

⟦𝑓 (𝜑1 ∧ 𝜓 ∗

1 , . . . 𝜑𝑘 ∧ 𝜓 ∗

𝑘 )⟧# =⇒ 𝜓𝑝

⟦𝑒⟧P∪P𝑟 𝑥 =⇒ 𝜓𝑝

Induction Hypothesis: For all expressions 𝑒 of height less than equal to 𝑛, the following is true:

⟦𝑒⟧P∪P𝑟 𝑥 =⇒ 𝜓𝑝
Induction Step: Let 𝑒 = 𝑓 (𝑒1, . . . 𝑒𝑘 ) be an expression of height equal to 𝑛+1. The height of expressions
𝑒1, . . . 𝑒𝑘 is less than equal to 𝑛.

Note that 𝜑𝑖 ∧𝜓 ∗

𝑖 =⇒ ⟦𝑒𝑖 ⟧𝑥 (line-7 and line-9). And since BackPropogate(𝑒𝑖, 𝑥, 𝜑𝑖 ∧𝜓 ∗

𝑖 , P, U) ⊆

P𝑟 , using induction hypothesis:

and

therefore

⟦𝑒𝑖 ⟧P∪P𝑟 =⇒ 𝜑𝑖 ∧ 𝜓 ∗
𝑖

⟦𝑓 (𝜑1 ∧ 𝜓 ∗

1 , . . . 𝜑𝑘 ∧ 𝜓 ∗

𝑘 )⟧# =⇒ 𝜓𝑝

⟦𝑒⟧P∪P𝑟 𝑥 =⇒ 𝜓𝑝

□

A.2 Appendix: Non Noisy Performance Comparison

1:28

Shivam Handa and Martin Rinard

Benchmark

No of Examples

bikes
bikes-long
bikes-long-repeat
bikes_small
dr-name
dr-name-long
dr-name-long-repeat
dr-name_small
firstname
firstname-long
firstname-long-repeat
firstname_small
initials
initials-long
initials-long-repeat
initials_small
lastname
lastname-long
lastname-long-repeat
lastname_small
name-combine
name-combine-2
name-combine-2-long
name-combine-2-long-repeat
name-combine-2_short
name-combine-3
name-combine-3-long
name-combine-3-long-repeat
name-combine-3_short
name-combine-4
name-combine-4-long
name-combine-4-long-repeat
name-combine-4_short
name-combine-long
name-combine-long-repeat
name-combine_short
phone
phone-1
phone-1-long
phone-1-long-repeat
phone-10
phone-10-long
phone-10-long-repeat
phone-10_short
phone-1_short
phone-2
phone-2-long
phone-2-long-repeat
phone-2_short
phone-3
phone-3-long
phone-3-long-repeat
phone-3_short
phone-4
phone-4-long
phone-4-long-repeat
phone-4_short

6
24
58
6
4
50
150
4
4
54
204
4
4
54
204
4
4
54
204
4
6
4
54
204
4
6
50
200
6
5
50
200
5
50
204
6
6
6
100
400
7
100
400
7
6
6
100
400
6
7
100
400
7
6
100
400
6

𝐿0/∞
67
115
198
67
376
444
788
381
121
334
823
120
364
626
1252
347
117
341
825
119
1336
2112
2514
3690
2101
303
475
982
298
1906
2286
3053
1964
1633
3590
1365
70
59
282
727
91284
90668
97173
78872
60
79
299
765
79
699
1229
2288
808
2779
3529
5514
2578

𝐿0/1
67
116
196
67
408
501
775
411
124
332
811
119
340
693
1199
344
123
345
833
120
1248
2140
2557
3615
2101
309
528
1126
330
1888
2280
3239
1868
1592
3562
1264
67
62
280
730
79267
72924
88929
76274
61
81
299
794
81
773
1291
2406
791
2511
3097
4487
2527

Rose
𝐿𝐷𝐿
69
125
230
73
349
659
1123
363
131
404
948
305
410
967
1637
736
132
383
970
139
1369
2119
1752
2567
1760
462
800
1163
345
2253
3235
3436
1779
2332
4844
1130
76
64
304
753
141671
115134
154668
117722
68
78
543
873
79
694
1639
2668
904
2251
4367
5921
2466

𝐿1𝐷
73
164
357
69
537
880
1012
531
135
735
1080
170
516
848
1627
422
1069
426
1120
127
2231
3756
4748
5456
2731
1382
665
1389
375
4029
2668
3856
3281
2867
5079
2542
110
72
322
811
134173
124326
133886
124704
81
157
430
994
89
883
1707
2674
1149
4568
5388
7886
5311

𝐿𝑛𝑆
67
113
204
67
378
446
845
406
118
333
819
118
199
488
1159
201
125
350
784
119
740
1730
2251
2990
1748
299
480
982
312
1882
2253
2915
2018
1119
2721
736
68
62
279
760
31920
35296
38224
28693
62
70
294
773
71
356
774
1954
363
1089
2228
3340
1108

CFTA
Threshold 4
19554
58187
127214
21210
-
-
-
-
4258
37946
148101
4220
36188
378070
-
30920
175762
565654
-
178825
-
-
-
-
-
547447
-
-
544044
-
-
-
-
-
-
-
943
933
8173
28547
-
-
-
-
942
953
6849
29766
943
-
-
-
-
-
-
-
-

Fig. 14. Runtime performance of Rose and CFTA over noise-free dataset

Noisy Program Synthesis using Abstractions

1:29

Benchmark

No of Examples

phone-5
phone-5-long
phone-5-long-repeat
phone-5_short
phone-6
phone-6-long
phone-6-long-repeat
phone-6_short
phone-7
phone-7-long
phone-7-long-repeat
phone-7_short
phone-8
phone-8-long
phone-8-long-repeat
phone-8_short
phone-9
phone-9-long
phone-9-long-repeat
phone-9_short
phone-long
phone-long-repeat
phone_short
reverse-name
reverse-name-long
reverse-name-long-repeat
reverse-name_short
univ_1
univ_1-long
univ_1-long-repeat
univ_1_short
univ_3
univ_3-long
univ_3-long-repeat
univ_3_short

7
100
400
7
7
100
400
7
7
100
400
7
7
100
400
7
7
100
400
7
100
400
6
6
50
200
6
6
20
30
6
6
20
30
6

𝐿0/∞
110
407
1156
109
168
493
1246
169
152
458
1245
162
155
460
1242
156
30087
31448
39881
27348
290
720
68
783
1075
1682
767
-
-
22101
-
6500
-
-
6697

𝐿0/1
116
407
1153
115
170
503
1295
178
163
485
1253
164
162
453
1265
156
28603
33337
41229
31304
285
752
66
732
1077
1659
774
-
-
22452
-
6362
-
-
6275

Rose
𝐿𝐷𝐿
100
485
1308
158
148
484
1353
183
370
502
1469
197
153
1490
1483
146
43063
48872
56789
43994
1092
806
68
764
2120
1912
823
203455
-
-
202296
5601
-
-
4506

𝐿1𝐷
267
466
1344
126
194
1047
1507
231
179
655
1921
199
266
538
1907
183
67994
59942
73652
62737
329
867
79
1186
1341
2590
987
-
-
54745
-
11083
-
-
9127

𝐿𝑛𝑆
98
386
1123
101
119
447
1234
121
123
438
1230
120
114
439
1229
119
12823
13498
18845
12140
285
766
68
463
764
1398
440
51541
64459
11928
50989
5770
-
-
5772

CFTA
Threshold 4
122
683
2179
127
3230
27566
100241
3327
2793
27770
89535
2762
3464
22961
88932
3223
-
-
-
-
8592
28836
963
-
-
-
-
-
-
-
-
-
-
-
-

Fig. 15. Runtime performance of Rose and CFTA over noise-free dataset

