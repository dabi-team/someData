2
2
0
2

g
u
A
4
1

]

G
L
.
s
c
[

1
v
8
6
8
6
0
.
8
0
2
2
:
v
i
X
r
a

Frouros: A Python library for drift detection in Machine Learning problems

Frouros: A Python library for drift detection in Machine
Learning problems

Jaime C´espedes Sisniega
´Alvaro L´opez Garc´ıa
Instituto de F´ısica de Cantabria (IFCA), CSIC-UC, Spain

cespedes@ifca.unican.es
aloga@ifca.unican.es

Abstract
Frouros is a Python library capable of detecting drift in machine learning problems.
It provides a combination of classical and more recent algorithms for drift detection:
both supervised and unsupervised, as well as some capable of acting in a semi-supervised
manner. We have designed it with the objective of being easily integrated with the
scikit-learn library, implementing the same application programming interface. The
library is developed following a set of best development and continuous integration prac-
tices to ensure ease of maintenance and extensibility. The source code is available at
https://github.com/IFCA/frouros.
Keywords: Machine learning, Drift detection, Concept drift, Data drift, Python

1. Introduction

When it comes to deploying machine learning models in real-world applications, there is
often a mistaken belief that models will be used in a stationary environment. This belief
assumes that the same concepts learned during the training phase will remain valid at in-
ference time Gama et al. (2004), or that training samples and production-time samples will
come from the same distribution Ackerman et al. (2021). However, in real-world scenarios
this is often not true, and both situations may result in some type of drift that can end
up aﬀecting model’s performance ˇZliobait˙e et al. (2016). In addition, due the high cost of
collecting and labeling samples, this performance loss can often not be conﬁrmed in many
real-world problems and other methods that only rely on distribution changes must be used.
In this paper we present Frouros, a Python library for drift detection in machine learn-
ing problems. The library tries to fulﬁll two main objectives: 1. to be able to easily integrate
in a machine learning model development pipeline with the scikit-learn Pedregosa et al.
(2011) library; 2. to unify in a single library the part of concept drift detection and adap-
tation (traditionally researched and used for streaming/evolving data streams and incre-
mental learning Khamassi et al. (2018)) with the research of change detection in the co-
variate distributions (also known as data shift, related to the ﬁeld of statistical two-sample
testing Rabanser et al. (2019) and methods that measure distance between distributions
Goldenberg and Webb (2019)).

2. Drift detection

Traditionally there has been little consensus on the terminology and deﬁnitions of the
diﬀerent types of drift, as stated in Moreno-Torres et al. (2012). In order to adopt some
clear deﬁnitions for the remainder of this paper, we apply those used in Gama et al. (2014)

1

 
 
 
 
 
 
C´espedes Sisniega and L´opez Garc´ıa

for the concept drift part, in combination with those used in Rabanser et al. (2019)’s work
for detecting dataset shift using only the covariates. Therefore, we set up the following
deﬁnitions assuming two diﬀerent time points, t0 and t1:

Concept drift. There is a change in the joint probability P (X, y) = P (y|X)P (X), with
or without a change in P (X). Thus, it can be deﬁned as Pt0 (X, y) 6= Pt1 (X, y). Known as
real concept drift when changes only aﬀect P (y|X) Gama et al. (2014).

Virtual drift. There is a change in P (X) but does not aﬀect the conditional probability
P (y|X) Gama et al. (2014); Ram´ırez-Gallego et al. (2017). Thus, Pt0(X) 6= Pt1(X) and
Pt0 (y|X) = Pt1 (y|X).

Data drift. As well as virtual drift, there is a change in P (X) but due the fact that
there is no labeled data y available, it cannot be veriﬁed that P (y|X) is being aﬀected or
not. Therefore, this type of drift only focuses in the distribution of the covariates P (X), so
Pt0 (X) 6= Pt1 (X). Hereafter, we rename dataset shift Rabanser et al. (2019) to data drift
in order to maintain consistency with the above deﬁnitions and with some of the related
software mentioned in Section 5 that also refer to it as data drift Van Looveren et al. (2019);
Leigh et al. (2022).

3. Overview and design

The design and implementation of the library has been carried out with the aim of making
it compatible with the use of scikit-learn, both through the use of its estimators, as well
as its pipeline. We also provide transformations that can be included in a pipeline and
allow to use multiple of the unsupervised methods, described in Section 3.2, at the same
time. Additionally, we provide prequential error metrics Gama et al. (2013) to evaluate the
performance of supervised methods described in Section 3.1.

Detection methods are divided in the supervised, unsupervised and semi-supervised
categories depending on the type of drift they can detect, according to the deﬁnitions given
in Section 2, and how they detect it. These categories are explained in what follows.

3.1 Supervised

These methods are aimed at detecting concept drift, so in order to update the detector they
require the ground-truth labels of the predictions that have previously been made.

In terms of implementation, the detector wraps the scikit-learn estimator and receives
through the update method the value that is used to perform the necessary steps (at least
update detector’s inner statistics and check if drift is occurring) on each iteration.
In
addition to receiving a scikit-learn estimator when the detector is instantiated, it receives
a conﬁguration class that contains a set of parameters that determine how it will behave.
Despite the fact that detectors can be updated by interacting with them directly (via
update method), we provide the following helper classes, so-called modes, to facilitate the
interaction with them.

IncrementalLearningMode works with methods that use warning and drift thresholds,
and scikit-learn estimators that support partial_fit method. It acts on an instance-
incremental manner by passing a tuple that contains a feature vector ~x, the prediction made

2

Frouros: A Python library for drift detection in Machine Learning problems

by the model and the ground-truth label, only when the user requires it. Additionally, a
value functions that uses that tuple needs to be provided to compute a value for the detector,
usually an error function. Therefore, this helper class allows to check the presence of drift
and add samples to the model’s decision.

NormalMode can be used by all the supervised methods without incrementally training
the wrapped model. It only checks if the detector has raised the drift ﬂag and resets its
inner statistics. Like IncrementalLearningMode, a value function must be provided, but
estimators that only support fit method are allowed, so it is not restricted only to those
that support partial_fit.

3.2 Unsupervised

The unsupervised methods are focused on detecting data drift by considering only the
covariates and regardless of the existence of labels or its lack thereof. Therefore, these
algorithms try to detect changes at a feature level by comparing new data distributions
against reference data distributions.

BaseEstimator and TransformerMixin classes from scikit-learn are used to imple-
ment these type of methods. The fit method stores the reference distribution and the
transform method applies the corresponding algorithm to compare new samples distribu-
tion with the reference distribution. In order to check if drift has happened after calling
transform method, distance or test attributes can be acceded, depending on the type
of algorithm used. All the implemented methods act in batch mode, expecting to receive
multiple samples each time a drift check is performed. Moreover, these detectors are im-
plemented considering the type of data that they are expected to work with: categorical or
numerical, and the number of features that can be considered: univariate or multivariate.

3.3 Semi-supervised

As with supervised methods, semi-supervised ones aim to detect concept drift by providing
ground-truth labels only when they are required. Therefore, when drift is suspected (equiv-
alent to the warning zone present in several of the supervised algorithms) it is necessary
to manually provide the detector with new labeled samples coming from an external entity
to verify whether drift is occurring or not. If drift is present, the model is replaced by one
trained with these new samples, otherwise the algorithm returns to a control state.

As far as implementation is concerned, their way of working is similar to supervised

methods with the exception of the use of the helper classes described in Section 3.1.

4. Development

With the intention of following a set of open source software development standards that al-
low the maintainability and extensibility of the library over time, we emphasize the following
areas:

Continuous integration. A Continuous integration workﬂow based on GitHub Actions
ensures that new modiﬁcations easily integrate with the existing code base and that they
are compatible with multiple Python versions.

3

C´espedes Sisniega and L´opez Garc´ıa

Documentation. An API documentation is provided using sphinx and hosted in Read
the Docs1 website. Some basic examples on the use of these detection methods are included
in the documentation.

Quality code.
In order to ensure minimum standards in terms of code quality, code
coverage is set to be greater than 90% and some Python quality and style tools that comply
with PEP8 standards are used, such as flake8, pylint, black and mypy.

In addition to the source code being available on GitHub2, Frouros package
Open source.
can be installed through the Python Package Index (PyPI)3. In terms of licensing, it is
distributed under the BSD-3-Clause license.

5. Comparison to related software

With regard to the concept drift detection part, MOA Bifet et al. (2010) has most of the
supervised methods that we are including, but they are implemented in Java. In Python,
River Montiel et al. (2021), that is focused on online machine learning and streaming data,
oﬀers some supervised algorithms but only a subset of those presented here. Another Python
library that contains this type of detectors is scikit-multiflow Montiel et al. (2018), but
it has not been taken into account due to the fact that it was merged with the online machine
learning library Creme Halford et al. (2019), resulting in the above-mentioned River library.
For the data drift part, Alibi-detect Van Looveren et al. (2019) has several algorithms
related to the ﬁeld of statistical two-sample hypothesis testing and some of them can act
both online (single sample) and oﬄine (batch sample). TorchDrift Viehmann et al. (2021)
also implements some statistical two-sample hypothesis testing methods but in this case
uses PyTorch for their implementation.

To the best of our knowledge, Menelaus Leigh et al. (2022) is the only open source
library that has both supervised and unsupervised methods, as well as a semi-supervised
algorithm, although they classify them in the following types: change detection, concept
drift and data drift. Supervised algorithms are implemented in such a way that the user
must necessarily be in charge of controlling each iteration of the sample without oﬀering
some helper functions or classes to interact with the detector, as Frouros does and it is
explained in Section 3.1.

Table 1 provides a more detailed view of the methods implemented in each of the libraries
mentioned above, as well as those included in Frouros. At the time of writing this paper,
Frouros is listed as the library with the highest number of methods available with 24.

Moreover, there are several other libraries and tools that have been excluded from
Table 1, due to the fact that they implement a more limited number of methods, or
that are more focused on building graphical dashboard and visual representations, such
as Deepchecks Bressler et al. (2022), Eurybia Roux et al. (2022), Evidently Dral (2020)
or NannyML Nuyttens and Perrakis (2022).

1. https://frouros.readthedocs.io
2. https://github.com/IFCA/frouros
3. https://pypi.org/project/frouros/

4

Frouros: A Python library for drift detection in Machine Learning problems

s
u
a
l
e
n
e
M

t
c
e
t
e
d
-
i
b

i
l

A

A
O
M

r
e
v
i
R

t
f
i
r
D
h
c
r
o
T

Supervised

X X X
X X
X X X
X
X X X
X
X X
X X
X

X
X X X
X
X
X
X
X X

Semi-supervised

X

Unsupervised

s
o
r
u
o
r
F

X
X
X
X
X
X
X
X
X

X
X

Reference

Bifet and Gavalda (2007)
Page (1954)
Gama et al. (2004)
Ross et al. (2012)
Baena-Garcıa et al. (2006)
Roberts (1959)
Frias-Blanco et al. (2014)
Frias-Blanco et al. (2014)
Raab et al. (2020)
Wang and Abraham (2015)
Page (1954)
Barros et al. (2017)
Huang et al. (2014)
Sakthithasan et al. (2013)
Pears et al. (2014)

X Nishida and Yamauchi (2007)

X
X

Sethi and Kantardzic (2017)
Sethi and Kantardzic (2017)

Method

ADWIN
CUSUM
DDM
ECDD-WT
EDDM
GMA
HDDM-A
HDDM-W
KSWIN
LFR
Page Hinkley
RDDM
SEED
SeqDrift1
SeqDrift2
STEPD

MD3-RS
MD3-SVM

C2ST
CDBD

X

X

Context-aware MMD X
X

CVM
EMD
FET
HDDDM
Histogram Intersection
JS Divergence
kdq-Tree
KL Divergence
KS

X
Learned Kernel MMD X
X
X

LSDD
MMD
Partial MMD
PCA-CD
PSI
Welch’s t-test
χ2
# Methods

X

9

X

X

X

X

Lopez-Paz and Oquab (2016)
Lindstrom et al. (2013)
Cobb and Van Looveren (2022)
Cram´er (1928)
Rubner et al. (2000)
Upton (1992)
Ditzler and Polikar (2011)
Swain and Ballard (1991)
Lin (1991)
Dasu et al. (2006)
Kullback and Leibler (1951)
Massey Jr (1951)
Liu et al. (2020)
Bu et al. (2016)
Gretton et al. (2012)
Viehmann (2021)
Qahtan et al. (2015)
Wu and Olson (2010)
Welch (1947)
Pearson (1900)

X
X X

X
X

X
X X

X X
X

X
X
X

12

14

7

4

24

Table 1: Drift detection methods by library

5

C´espedes Sisniega and L´opez Garc´ıa

6. Conclusion and future work

This papers presents Frouros, a Python library for drift detection in machine learning
problems that can be used with scikit-learn library, both for algorithms that aim to
detect concept drift and for those that try to detect data drift. Moreover, this library tries
to meet some of the open source software development standards that allow to extend it,
both in terms of adding new methods or modes to interact with the detectors as helper
functions or classes.
In view of future work, we plan to adapt supervised methods to
support batch-incremental learning, as long as the nature of each algorithm allows it. We
will also consider to extend the unsupervised part to include some methods that work with
individual instances (online) and not only in batch mode (oﬄine). Finally, adding new
modes, as described in Section 3.1, to interact with the detectors would make it possible to
adapt the library to handle more real-world use cases.

Acknowledgments

The authors acknowledge the funding from the Agencia Estatal de Investigaci´on, Unidad
de Excelencia Mar´ıa de Maeztu, ref. MDM-2017-0765.

6

Frouros: A Python library for drift detection in Machine Learning problems

References

Samuel Ackerman, Orna Raz, Marcel Zalmanovici, and Aviad Zlotnick. Automatically
detecting data drift in machine learning classiﬁers. arXiv preprint arXiv:2111.05672,
2021.

Manuel Baena-Garcıa, Jos´e del Campo- ´Avila, Ra´ul Fidalgo, Albert Bifet, R Gavalda, and
Rafael Morales-Bueno. Early drift detection method. In Fourth international workshop
on knowledge discovery from data streams, volume 6, pages 77–86, 2006.

Roberto SM Barros, Danilo RL Cabral, Paulo M Gon¸calves Jr, and Silas GTC Santos.
Rddm: Reactive drift detection method. Expert Systems with Applications, 90:344–355,
2017.

Albert Bifet and Ricard Gavalda. Learning from time-changing data with adaptive win-
dowing. In Proceedings of the 2007 SIAM international conference on data mining, pages
443–448. SIAM, 2007.

Albert Bifet, Geoﬀ Holmes, Bernhard Pfahringer, Philipp Kranen, Hardy Kremer, Timm
Jansen, and Thomas Seidl. Moa: Massive online analysis, a framework for stream clas-
siﬁcation and clustering. In Proceedings of the ﬁrst workshop on applications of pattern
analysis, pages 44–50. PMLR, 2010.

N. Bressler, I. Gabbay, S. Chorev, P. Tannor, M. Perlmutter, N. Hutnik, J. Liberman,
D. Ben Israel, and Y. Romanyshyn. Deepchecks: Comprehensive testing and validation
of ml models and data, 2022. URL https://www.deepchecks.com/. Software available
from https://github.com/deepchecks/deepchecks.

Li Bu, Cesare Alippi, and Dongbin Zhao. A pdf-free change detection test based on density
diﬀerence estimation. IEEE transactions on neural networks and learning systems, 29(2):
324–334, 2016.

Oliver Cobb and Arnaud Van Looveren. Context-aware drift detection. In International

Conference on Machine Learning, pages 4087–4111. PMLR, 2022.

Harald Cram´er. On the composition of elementary errors: First paper: Mathematical

deductions. Scandinavian Actuarial Journal, 1928(1):13–74, 1928.

Tamraparni Dasu, Shankar Krishnan, Suresh Venkatasubramanian, and Ke Yi. An
information-theoretic approach to detecting changes in multi-dimensional data streams.
In In Proc. Symp. on the Interface of Statistics, Computing Science, and Applications.
Citeseer, 2006.

Gregory Ditzler and Robi Polikar. Hellinger distance based drift detection for nonstationary
environments. In 2011 IEEE symposium on computational intelligence in dynamic and
uncertain environments (CIDUE), pages 41–48. IEEE, 2011.

Emeli Dral. An open-source framework to evaluate, test and monitor ml models in produc-
tion. https://github.com/evidentlyai/evidently, 2020. Online; accessed 14-8-2022.

7

C´espedes Sisniega and L´opez Garc´ıa

Isvani Frias-Blanco, Jos´e del Campo- ´Avila, Gonzalo Ramos-Jimenez, Rafael Morales-Bueno,
Agustin Ortiz-Diaz, and Yaile Caballero-Mota. Online and non-parametric drift detec-
tion methods based on hoeﬀding’s bounds. IEEE Transactions on Knowledge and Data
Engineering, 27(3):810–823, 2014.

Joao Gama, Pedro Medas, Gladys Castillo, and Pedro Rodrigues. Learning with drift
In Brazilian symposium on artiﬁcial intelligence, pages 286–295. Springer,

detection.
2004.

Joao Gama, Raquel Sebastiao, and Pedro Pereira Rodrigues. On evaluating stream learning

algorithms. Machine learning, 90(3):317–346, 2013.

Jo˜ao Gama,

Indr˙e ˇZliobait˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid
Bouchachia. A survey on concept drift adaptation. ACM computing surveys (CSUR), 46
(4):1–37, 2014.

Igor Goldenberg and Geoﬀrey I Webb. Survey of distance measures for quantifying concept
drift and shift in numeric data. Knowledge and Information Systems, 60(2):591–615,
2019.

Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexan-
der Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(25):
723–773, 2012. URL http://jmlr.org/papers/v13/gretton12a.html.

Max Halford, Geoﬀrey Bolmier, Raphael Sourty, Robin Vaysse, and Adil Zoui-
URL

a Python library for online machine

learning,

creme,

2019.

tine.
https://github.com/MaxHalford/creme.

David Tse Jung Huang, Yun Sing Koh, Gillian Dobbie, and Russel Pears. Detecting volatil-
ity shift in data streams. In 2014 IEEE International Conference on Data Mining, pages
863–868. IEEE, 2014.

Imen Khamassi, Moamar Sayed-Mouchaweh, Moez Hammami, and Khaled Gh´edira. Dis-
cussion and review on evolving data streams and concept drift adapting. Evolving systems,
9(1):1–23, 2018.

Solomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of

mathematical statistics, 22(1):79–86, 1951.

Nicholl Leigh, Thomas Schill, India Lindsay, Anmol Srivastava, Kodie P. McNamara, and
Shashank Jarmale. Menelaus. https://github.com/mitre/menelaus, 2022. Online;
accessed 14-8-2022.

Jianhua Lin. Divergence measures based on the shannon entropy. IEEE Transactions on

Information theory, 37(1):145–151, 1991.

Patrick Lindstrom, Brian Mac Namee, and Sarah Jane Delany. Drift detection using un-

certainty distribution divergence. Evolving Systems, 4(1):13–25, 2013.

8

Frouros: A Python library for drift detection in Machine Learning problems

Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J Sutherland.
Learning deep kernels for non-parametric two-sample tests. In International conference
on machine learning, pages 6316–6326. PMLR, 2020.

David Lopez-Paz and Maxime Oquab. Revisiting classiﬁer two-sample tests. arXiv preprint

arXiv:1610.06545, 2016.

Frank J Massey Jr. The kolmogorov-smirnov test for goodness of ﬁt. Journal of the Amer-

ican statistical Association, 46(253):68–78, 1951.

Jacob Montiel, Jesse Read, Albert Bifet, and Talel Abdessalem. Scikit-multiﬂow: A multi-
output streaming framework. Journal of Machine Learning Research, 19(72):1–5, 2018.
URL http://jmlr.org/papers/v19/18-251.html.

Jacob Montiel, Max Halford, Saulo Martiello Mastelini, Geoﬀrey Bolmier, Raphael
Sourty, Robin Vaysse, Adil Zouitine, Heitor Murilo Gomes, Jesse Read, Talel
streaming data
Abdessalem, and Albert Bifet.
in python.
URL
http://jmlr.org/papers/v22/20-1380.html.

Journal of Machine Learning Research, 22(110):1–8, 2021.

River: machine learning for

Jose G Moreno-Torres, Troy Raeder, Roc´ıo Alaiz-Rodr´ıguez, Nitesh V Chawla, and Fran-
cisco Herrera. A unifying view on dataset shift in classiﬁcation. Pattern recognition, 45
(1):521–530, 2012.

Kyosuke Nishida and Koichiro Yamauchi. Detecting concept drift using statistical testing.

In International conference on discovery science, pages 264–269. Springer, 2007.

Niels Nuyttens and Nikolaos Perrakis. Nannyml. https://github.com/NannyML/nannyml,

2022. Online; accessed 14-8-2022.

Ewan S Page. Continuous inspection schemes. Biometrika, 41(1/2):100–115, 1954.

Russel Pears, Sripirakas Sakthithasan, and Yun Sing Koh. Detecting concept change in

dynamic data streams. Machine Learning, 97(3):259–293, 2014.

Karl Pearson. X. on the criterion that a given system of deviations from the probable in
the case of a correlated system of variables is such that it can be reasonably supposed
to have arisen from random sampling. The London, Edinburgh, and Dublin Philosophical
Magazine and Journal of Science, 50(302):157–175, 1900.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

Abdulhakim A Qahtan, Basma Alharbi, Suojin Wang, and Xiangliang Zhang. A pca-based
change detection framework for multidimensional data streams: Change detection in
multidimensional data streams. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pages 935–944, 2015.

9

C´espedes Sisniega and L´opez Garc´ıa

Christoph Raab, Moritz Heusinger, and Frank-Michael Schleif. Reactive soft prototype

computing for concept drift streams. Neurocomputing, 416:340–351, 2020.

Stephan Rabanser, Stephan G¨unnemann, and Zachary Lipton. Failing loudly: An empirical
study of methods for detecting dataset shift. Advances in Neural Information Processing
Systems, 32, 2019.

Sergio Ram´ırez-Gallego, Bartosz Krawczyk, Salvador Garc´ıa, Micha l Wo´zniak, and Fran-
cisco Herrera. A survey on data preprocessing for data stream mining: Current status
and future directions. Neurocomputing, 239:39–57, 2017.

S. W. Roberts. Control chart tests based on geometric moving averages. Technometrics, 1
(3):239–250, 1959. ISSN 00401706. URL http://www.jstor.org/stable/1266443.

Gordon J Ross, Niall M Adams, Dimitris K Tasoulis, and David J Hand. Exponentially
weighted moving average charts for detecting concept drift. Pattern recognition letters,
33(2):191–198, 2012.

Nicolas

Roux,

Johann Martin,

and

Thomas

Bouch´e.

Eurybia.

https://github.com/MAIF/eurybia, 2022. Online; accessed 14-8-2022.

Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric

for image retrieval. International journal of computer vision, 40(2):99–121, 2000.

Sripirakas Sakthithasan, Russel Pears, and Yun Sing Koh. One pass concept change de-
In Paciﬁc-Asia conference on knowledge discovery and data

tection for data streams.
mining, pages 461–472. Springer, 2013.

Tegjyot Singh Sethi and Mehmed Kantardzic.

On the reliable detection of con-
cept drift from streaming unlabeled data. Expert Systems with Applications, 82:77–
doi: https://doi.org/10.1016/j.eswa.2017.04.008. URL
99, 2017.
https://www.sciencedirect.com/science/article/pii/S0957417417302439.

ISSN 0957-4174.

Michael J Swain and Dana H Ballard. Color indexing. International journal of computer

vision, 7(1):11–32, 1991.

Graham JG Upton. Fisher’s exact test. Journal of the Royal Statistical Society: Series A

(Statistics in Society), 155(3):395–402, 1992.

Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb, Ashley Scillitoe, Robert
Samoilescu, and Alex Athorne. Alibi detect: Algorithms for outlier, adversarial and drift
detection, 2019. URL https://github.com/SeldonIO/alibi-detect.

Thomas Viehmann.

for bridging the gap between outlier detection and drift detection.
arXiv:2106.12893, 2021.

Partial wasserstein and maximum mean discrepancy distances
arXiv preprint

Thomas Viehmann, Luca Antiga, Daniele Cortinovis, and Lisa Lozza.

Torchdrift.

https://github.com/torchdrift/torchdrift, 2021. Online; accessed 14-8-2022.

10

Frouros: A Python library for drift detection in Machine Learning problems

Heng Wang and Zubin Abraham. Concept drift detection for streaming data.

In 2015

international joint conference on neural networks (IJCNN), pages 1–9. IEEE, 2015.

Bernard L Welch. The generalization of ‘student’s’problem when several diﬀerent population

varlances are involved. Biometrika, 34(1-2):28–35, 1947.

Desheng Wu and David L Olson. Enterprise risk management: coping with model risk in a

large bank. Journal of the Operational Research Society, 61(2):179–190, 2010.

Indr˙e ˇZliobait˙e, Mykola Pechenizkiy, and Joao Gama. An overview of concept drift appli-

cations. Big data analysis: new algorithms for a new society, pages 91–114, 2016.

11

