1
2
0
2

y
a
M
5

]

G
L
.
s
c
[

2
v
0
2
4
1
0
.
5
0
1
2
:
v
i
X
r
a

Training Quantized Neural Networks to Global Optimality via
Semideﬁnite Programming

Burak Bartan
bbartan@stanford.edu

Mert Pilanci
pilanci@stanford.edu

Department of Electrical Engineering, Stanford University
May 6, 2021

Abstract

Neural networks (NNs) have been extremely successful across many tasks in machine learning.
Quantization of NN weights has become an important topic due to its impact on their energy
eﬃciency, inference time and deployment on hardware. Although post-training quantization
is well-studied, training optimal quantized NNs involves combinatorial non-convex optimization
problems which appear intractable. In this work, we introduce a convex optimization strategy to
train quantized NNs with polynomial activations. Our method leverages hidden convexity in two-
layer neural networks from the recent literature, semideﬁnite lifting, and Grothendieck’s identity.
Surprisingly, we show that certain quantized NN problems can be solved to global optimality in
polynomial-time in all relevant parameters via semideﬁnite relaxations. We present numerical
examples to illustrate the eﬀectiveness of our method.

1

Introduction

In this paper we focus on training quantized neural networks for eﬃcient machine learning models.
We consider the combinatorial and non-convex optimization of minimizing empirical error with
respect to quantized weights. We focus on polynomial activation functions, where the training
problem is still non-trivial to solve.

Recent work has shown that two-layer neural networks with ReLU [35, 36] and leaky ReLU
activations [25] can be trained via convex optimization in polynomial time with respect to the
number of samples and neurons. Moreover, degree-two polynomial activations can be trained
to global optimality in polynomial time with respect to all problem dimensions using semideﬁnite
programming [7]. In this work, we take a similar convex duality approach that involves semideﬁnite
programming. However, our method and theoretical analysis are substantially diﬀerent since we
consider quantized weights, which involves a discrete non-convex optimization problem. The fact
that the ﬁrst layer weights are discrete renders it a combinatorial NP-hard problem and thus we
cannot hope to obtain a similar result as in [7] or [35]. In contrast, in [7] it is shown that with the
constraint
ujk2 = 1 and ℓ1-norm regularization on the second layer weights, the global optimum
k
can be found in fully polynomial time and that the problem becomes NP-hard in the case of
quadratic regularization (i.e. weight decay).

The approach that we present in this paper for training quantized neural networks is diﬀerent
from other works in the quantized neural networks literature. In particular, our approach involves
deriving a semideﬁnite program (SDP) and designing a sampling algorithm based on the solution

1

 
 
 
 
 
 
of the SDP. Our techniques lead to a provable guarantee for the diﬀerence between the resulting
loss and the optimal non-convex loss for the ﬁrst time.

1.1 Prior work

Recently, there has been a lot of research eﬀort in the realm of compression and quantization of
neural networks for real-time implementations. In [41], the authors proposed a method that reduces
network weights into ternary values by performing training with ternary values. Experiments in [41]
show that their method does not suﬀer from performance degradation and achieve 16x compression
compared to the original model. The authors in [18] focus on compressing dense layers using
quantization to tackle model storage problems for large-scale models. The work presented in [21]
also aims to compress deep networks using a combination of pruning, quantization and Huﬀman
coding.
In [30], the authors present a quantization scheme where they use diﬀerent bit-widths
for diﬀerent layers (i.e., bit-width optimization). Other works that deal with ﬁxed point training
include [29], [19], [22]. Furthermore, [4] proposes layer-wise quantization based on ℓ2-norm error
minimization followed by retraining of the quantized weights. However, these studies do not address
optimal approximation. In comparison, our approach provides optimal quantized neural networks.
In [2], it was shown that the degree two polynomial activation functions perform comparably to
ReLU activation in practical deep networks. Speciﬁcally, it was reported in [2] that for deep neural
networks, ReLU activation achieves a classiﬁcation accuracy of 0.96 and a degree two polynomial
activation yields an accuracy of 0.95 on the Cifar-10 dataset. Similarly for the Cifar-100 dataset, it
is possible to obtain an accuracy of 0.81 for ReLU activation and 0.76 for the degree two polynomial
activation. These numerical results are obtained for the activation σ(t) = t + 0.1t2. Furthermore,
in encrypted computing, it is desirable to have low degree polynomials as activation functions.
For instance, homomorphic encryption methods can only support additions and multiplications in
a straightforward way. These constraints make low degree polynomial activations attractive for
encrypted networks. In [16], degree two polynomial approximations were shown to be eﬀective for
accurate neural network predictions with encryption. These results demonstrate that polynomial
activation neural networks are a promising direction for further exploration.

Convexity of inﬁnitely wide neural networks was ﬁrst considered in [8] and later in [5]. A
convex geometric characterization of ﬁnite width neural networks was developed in [12, 11, 6].
Exact convex optimization representations of ﬁnite width two-layer ReLU neural network problems
were developed ﬁrst in [35] and extended to leaky ReLU [25] and polynomial activation functions
[7]. These techniques were also extended to other network architectures including three-layer ReLU
[14], autoencoder [37], autoregressive [20], batch normalization [15] and deeper networks [13].

1.2 Notation

×

∈

Rn

d to denote the data matrix throughout the text, where its rows xi ∈
∈

Rd correspond
We use X
Rn denotes the target vector. We use ℓ(ˆy, y) for
to data samples and columns are the features. y
convex loss functions where ˆy is the vector of predictions and ℓ∗(v) = supz(vT z
ℓ(z, y)) denotes
) is the elementwise sign function. We use
its Fenchel conjugate. tr refers to matrix trace. sign(
·
for the Hadamard product
the notation Z
of vectors and matrices. The symbol
) to denote
denotes the Kronecker product. We use λmax(
·
the largest eigenvalue of its matrix argument. If the input to diag(
) is a vector, then the result
·
is a diagonal matrix with its diagonal entries equal to the entries of the input vector. If the input
) is a matrix, then the result is a vector with entries equal to the diagonal entries of the
to diag(
·
input matrix. ¯1 refers to the vector of 1’s. Sd
d)-dimensional symmetric
×

0 for positive semideﬁnite matrices (PSD). We use

d represents the set of (d

−

(cid:23)

⊗

◦

×

2

matrices.

2 Lifting Neural Network Parameters

We focus on two-layer neural networks with degree two polynomial activations σ(t) := at2 + bt + c.
Let f : Rd

R denote the neural network deﬁned as

→

m

f (x) =

σ(xT uj)αj

(1)

Xj=1
R are the ﬁrst and second layer weights,
Rd and αj ∈
Rd is the input sample, uj ∈
where x
respectively. This is a fully connected neural network with m neurons in the hidden layer. We
Rd, j = 1, . . . , m) in the hidden layer are
focus on the setting where the ﬁrst dm weights (i.e., uj ∈
constrained to be integers.

∈

The results are extended to neural networks with vector outputs, i.e., f : Rd

B of the Appendix.

2.1 Bilinear activation networks

RC, in Section

→

Now we introduce a simpler architecture with bilinear activations
zation given by

uT

X

X →

v and binary quanti-

m′

) =

f ′(

X

uT
j X

vjαj with uj, vj ∈ {−

d, αj ∈
1, +1
}

R,

j

∀

(2)

Xj=1

×

Rd

X ∈

d is the lifted version of the input x

Rd as will be deﬁned in the sequel. We
where
show that this architecture is suﬃcient to represent multi-level integer quantization and degree two
polynomial activations without any loss of generality. In addition, these networks can be mapped
to the standard network in (1) in a straightforward way as we formalize in this section. Hence,
some of our results leverage the above architecture for training and transform a bilinear activation
network into a polynomial activation network.

∈

Theorem 1 (Reduction to binary quantization and bilinear activation). The following multi-level
(i.e. M + 1 levels) quantized neural network

m

f (x) =

σ(xT uj)αj

d, αj ∈
}
can be represented as a binary quantized bilinear activation network

M + 2, . . . , 0, . . . , M

2, M

M,

−

−

Xj=1
where uj ∈ {−

R,

j

∀

) =

f ′(

X

m′

Xj=1

uT
j X

vjαj where uj, vj ∈ {−

dM +1,
1, +1
}

a˜x˜xT
b
2 ˜xT

b
2 ˜x
c

:=

X
f ′(

⊗

and ˜x := x

1M . Conversely, any binary quantized bilinear activation network

(cid:20)
) of this form can be represented as a multi-level quantized neural network f (x).

(cid:21)

X
In the remainder of this section, we provide a constructive proof of the above theorem by
showing the reduction in three steps: Reducing to binary quantization, lifting and reducing to
bilinear activation.

3

2.2 Reducing multi-level quantization to binary

In this section, we show that the two level binary quantization
model is suﬃcient to model
other quantization schemes with integer levels. Hence, we can focus on binary quantized neural
network models without loss of generality. Suppose that q represents a hidden neuron quantized to
M + 1 levels given by

1, 1
}

{−

Then we equivalently have

q

d
M :=
∈ Q

{−

M,

−

M + 2, . . . , 0, . . . , M

2, M

d .
}

−

d

d

M

qT x =

qixi =

uk+(i

−

1)M xi = uT ˜x ,

Xi=1
1M = [x1, x1, . . . , x2, x2, . . . , ]T

where ˜x = x
∈
stacking the input data x by replication as ˜x
represented as binary quantization.

Xi=1

⊗

∈

Xk=1

RdM since

i. Therefore,
RdM enables M + 1 level quantization to be

1)M ∈ QM ∀

−

M
k=1 uk+(i

P

(3)

(4)

2.3 Lifting dimensions

We ﬁrst show that binary quantized networks with degree two polynomial activations are equivalent
to binary quantized networks with quadratic activations. Note that the network output can be
expressed as

m

Xj=1
m

f (x) =

=

(cid:0)
˜uT
j

a(xT uj)2 + b(xT uj) + c

αj

axxT
b
2 xT

b
2 x
c

(cid:1)

˜ujαj

(5)

(cid:21)
j , 1]T . Consequently, we can safely relax
where we deﬁned the augmented weight vectors ˜uj := [uT
b
axxT
2 x
b
2 xT
c

axxT
this constraint to ˜uj ∈ {−
b
2 xT
we can assume (˜uj)d+1 = 1 without loss of generality.

d+1 since ˜uT
1, +1
j
}

˜uj) and

˜uj = (

b
2 x
c

˜uj)T

(
−

Xj=1

−

(cid:20)

(cid:20)

(cid:20)

(cid:21)

(cid:21)

2.4 Reduction to bilinear activation

Now we show that we can consider the network model

f (x) =

m

uT
j

Xj=1

(cid:20)

axxT
b
2 xT

b
2 x
c

(cid:21)

vjαj =

m

Xj=1

uT
j X

vjαj

(6)

where
out loss of generality. Using the symmetrization identity

uj, vj}
{

m
j=1 are the model parameters to represent networks with quadratic activations with-

}

|

X
{z

we can express the neural network output as

2uT Av = (u + v)T A(u + v)

uT Au

−

−

vT Av ,

(7)

m

2f (x) =

(uj + vj)T

Xj=1 (cid:18)

axxT
b
2 xT

(cid:20)

b
2 x
c

(cid:21)

(uj + vj)αj −

uT
j

axxT
b
2 xT

(cid:20)

b
2 x
c

(cid:21)

ujαj −

vT
j

axxT
b
2 xT

(cid:20)

b
2 x
c

vjαj

.

(cid:21)

(cid:19)

Note that 1
quadratic activation and 3m hidden neurons.

2 (uj + vj)

∈ {−

d and the above can be written as a quantized network with
1, 0, 1
}

4

3 Convex Duality of Quantized Neural Networks and SDP Relax-

ations

We consider the following non-convex training problem for the two-layer polynomial activation
network

p∗ =

s.t. uj

∈{−

min
d,αj
1,1
}

R j

∈

[m]

∈

ℓ



σ(Xuj)αj, y

+ βd



m

Xj=1

.

αj|
|

(8)

m

Xj=1





, y) is a convex and Lipschitz loss function, σ(t) := at2 + bt + c is a degree-two polynomial
Here, ℓ(
·
activation function, m is the number of neurons, β is the regularization parameter.

It is straightforward to show that this optimization problem is NP-hard even for the case when
y)2 is the squared loss via a reduction to

m = 1, σ(t) = t is the linear activation and ℓ(u, y) = (u
the MaxCut problem [17].

−

We scale the regularization term by d to account for the fact that the hidden neurons have
Euclidean norm √d, which simpliﬁes the notation in the sequel. Taking the convex dual with
m
j=1, the optimal value of the primal is lower bounded by
respect to the second layer weights

αj}
{
d∗ =

p∗

≥

|

vT σ(Xu)

max
βd ,
∀
|≤
max
maxu:u∈{−1,1}d |

u

=

vT σ(Xu)

|≤

1,1

d −
}

∈{−

v)

ℓ∗(

−

v).

ℓ∗(

−

βd −

(9)

Remarkably, the above dual problem is a convex program since the constraint set is an intersection
of linear constraints. However, the number of linear constraints is exponential due to the binary
quantization constraint.

We now describe an SDP relaxation which provides a lower-bounding and tractable dual convex
program. Our formulation is inspired by the SDP relaxation of MaxCut [17], which is analogous to
the constraint subproblem in (9). Let us assume that the activation is quadratic σ(u) = u2, since
we can reduce degree two polynomial activations to quadratics without loss of generality as shown
in the previous section. Then, we have

=

vT σ(Xu)
|
|
vT (Xu)2

.
i )u
|
βd can be equivalently stated as the following two

n
i=1 vixixT

The constraint maxu:u2

uT (
|

i =1,

i |
∀

| ≤

P

inequalities

The SDP relaxation for the maximization maxu:u2

n

vixixT
i

Xi=1
n

βd ,

u

≤

!

q∗1 = max
u:u2
i =1,

q∗2 = max
u:u2
i =1,

uT

uT

i
∀

i
∀

 −

vixixT
i

Xi=1
i =1,

i uT
∀
n

βd .

u

≤

!

n
i=1 vixixT
i

u is given by

ˆq1 =

max
0, Uii=1,

tr

i
∀

U

(cid:23)

Xi=1

(cid:0)P
vixixT

i U

(cid:1)

,

!

(10)

(11)

Sd

where U
the optimal value of the relaxation is an upper bound on the optimal solution, i.e., ˆq1 ≥
Consequently, the relaxation leads to the following lower bound:

d. This is a relaxation since we removed the constraint rank(U ) = 1. Hence,
q∗1.

∈

×

d∗

≥

q∗
1≤

max
βd, q∗

2 ≤

v)

ℓ∗(

−

βd −

≥

ˆq1

≤

max
βd, ˆq2

≤

v) .

ℓ∗(

−

βd −

(12)

5

 
 
More precisely, we arrive at d∗ ≥

dSDP where

dSDP := max

v −

v)

ℓ∗(

−

s.t.

max
0, Uii=1,

U

(cid:23)

max
0, Uii=1,

U

(cid:23)

tr

tr

i
∀

i
∀

n

vixixT

i U

βd

! ≤

Xi=1
n

 −

Xi=1

vixixT

i U

βd .

! ≤

(13)

The dual of the SDP in the constraint (11) is given by the dual of the MaxCut SDP relaxation,

which can be stated as

min
Rd
z
∈
s.t.

λmax

d

·

¯1T z = 0 .

n

Xi=1

vixixT

i + diag(z)

!

(14)

Since the primal problem is strictly feasible, it follows from Slater’s condition that the strong duality
holds between the primal SDP and the dual SDP. This allows us to reformulate the problem in (13)
as

max
v,z1,z2 −

ℓ∗(

−

v)

n

s.t. λmax

vixixT

i + diag(z1)

β

! ≤

Xi=1
n

vixixT

i + diag(z2)

λmax

 −

Xi=1
¯1T z1 = 0, ¯1T z2 = 0 ,

! ≤

β

(15)

where the variables have dimensions v
∈
Expressing the largest eigenvalue constraints as linear matrix inequalities yields

Rn, z1, z2 ∈

Rd and λmax denotes the largest eigenvalue.

dSDP := max

v,z1,z2 −
n

v)

ℓ∗(

−

s.t.

vixixT

i + diag(z1)

Xi=1

n

βId (cid:22)

0

−

vixixT

i + diag(z2)

−
¯1T z1 = 0, ¯1T z2 = 0 .

Xi=1

βId (cid:22)

0

−

(16)

Next, we will ﬁnd the dual of this problem. First we write the Lagrangian:

L(v, z1, z2, Z1, Z2, ρ1, ρ2) =

=

ℓ∗(

−

−

v)

−

n

Xi=1

vixT

i (Z1 −

Z2)xi + β tr(Z1 + Z2)

−

d

d

(Z1,jjz1,j + Z2,jjz2,j) +

(ρ1z1,j + ρ2z2,j)

Xj=1

Xj=1

(17)

6

 
 
 
where Z1, Z2 ∈
respect to v, z1, z2 leads to the following convex program

d and ρ1, ρ2 ∈

×

Sd

R are the Lagrange multipliers. Maximizing the Lagrangian with

min
Z1,Z2,ρ1,ρ2

ℓ 


, y



+ β tr(Z1 + Z2)

xT
1 (Z1 −
...
xT
n (Z1 −

Z2)x1

Z2)xn





Z1 (cid:23)

s.t. Z1,jj = ρ1, Z2,jj = ρ2, j = 1, . . . , d
0 .

0, Z2 (cid:23)







Equivalently,

p∗

≥

dSDP := min

Z1,Z2,ρ1,ρ2

ℓ (ˆy, y) + βd(ρ1 + ρ2)

s.t.

i (Z1 −

Z2)xi, i = 1, . . . , n

ˆyi = xT
Z1,jj = ρ1, Z2,jj = ρ2, j = 1, . . . , d
Z1 (cid:23)

0, Z2 (cid:23)

0 .

(18)

(19)

Remarkably, the above SDP is a polynomial time tractable lower bound for the combinatorial
non-convex problem p∗.

3.1 SDP relaxation for bilinear activation networks

m
j=1(xT uj)(xT vj)αj and provide an SDP relax-
Now we focus on the bilinear architecture f (x) =
ation for the corresponding non-convex training problem. It will be shown that the resulting SDP
relaxation is provably tight, where a matching upper bound can be obtained via randomization.
Moreover, the resulting feasible solutions can be transformed into a quantized neural network with
polynomial activations as we have shown in Section 2. Consider the non-convex training problem
for the two-layer network with the bilinear activation given by

P

p∗b =

s.t. uj,vj

min
d,αj
1,1
}

∈{−

R

∈

j
∀

[m]

∈

ℓ



(Xvj ))αj, y

+ βd



◦

m

((Xuj )

Xj=1



m

Xj=1

.

αj|
|

(20)



Here
denotes the Hadamard, i.e., direct product of two vectors. Repeating an analogous duality
analysis (see Section A.3 for the details), we obtain a tractable lower-bounding problem given by

◦

p∗b ≥

dbSDP := min
Q,ρ

ℓ (ˆy, y) + βdρ

s.t.

i Zxi, i = 1, . . . , n

ˆyi = 2xT
Qjj = ρ, j = 1, . . . , 2d

Q =

V
Z
Z T W

(cid:20)

0 .

(cid:23)

(cid:21)

(21)

The above optimization problem is a convex SDP, which can be solved eﬃciently in polynomial
time.

4 Main result: SDP Relaxation is Tight

We ﬁrst introduce an existence result on covariance matrices which will be used in our quantized
neural network construction.

7

Algorithm 1: Sampling algorithm for quantized neural networks
1. Solve the SDP in (21). Deﬁne the scaled matrix Z ∗s ←
2. Solve the problem

Z ∗/ρ∗.

Q∗ := arg

Q

(cid:23)

min
0,Qjj=1

j k
∀

Q(12) −

2
F .
sin(γZ ∗s )
k

(23)

3. Sample the ﬁrst layer weights u1, . . . , um, v1, . . . , vm from multivariate normal distribution

as

u
v

(cid:21)

(cid:20)

sign(

N

∼

(0, Q∗)) and set the second layer weights as αj = ρ∗

π
γm ,

j.

∀

4. (optional) Transform the quantized bilinear activation network to a quantized polynomial

activation network.

Theorem 2 (Trigonometric covariance shaping). Suppose that Z ∗ ∈
such that
(cid:23)
0 satisfying Qjj = 1

0 and Vjj = Wjj = 1

Z ∗
T W

V, W :

V
Z ∗

j and

∃
2d

R2d

Q

∀

(cid:21)

(cid:20)

×

∈

(cid:23)

∀

Rd

×

d is an arbitrary matrix

j. Then, there exists a PSD matrix

arcsin(Q(12)) = γZ ∗

(22)

where Q =

Q(11) Q(12)
Q(21) Q(22)(cid:21)

(cid:20)

, γ = ln(1 + √2), and arcsin is the elementwise inverse sine function.

Our construction is based on randomly generating quantized neurons whose empirical covariance
matrix is matched to the optimal solution of the convex SDP. The above theorem is an existence
result which will be crucial in our sampling algorithm. The important observation is that, if we

u
v

(cid:21)

(cid:20)

∼

N

let

sign(

(0, Q)) with some Q

π arcsin(Q), which
0, Qjj = 1
is referred to as Grothendieck’s Identity [3]. Therefore, E[uvT ] = arcsin Q(12) = γZ ∗, which is
proportional to the target covariance matrix. This algorithm is inspired by Krivine’s analysis of
the Grothendieck’s constant and its applications in approximating the cut norm using semideﬁnite
programming [3].

(cid:21) (cid:20)

(cid:23)

∀

(cid:20)

(cid:21)

j, then E

u
v

T

u
v

= 2

Proof of Theorem 2. Note that the condition

V, W :

∃

0 and Vjj = Wjj = 1

j

∀

(cid:23)

implies that there exists unit norm vectors x1, . . . , xd, y1, . . . , yd such that Z ∗ij = xT
applying Lemma 4.2 of [3] and Grothendieck’s Identity completes the proof.

i yj. Consequently,

V
Z ∗

Z ∗
T W

(cid:20)

(cid:21)

4.1 Sampling algorithm for approaching the global optimum

Now we present our sampling algorithm which generates quantized neural networks parameters
based on the solution of the lower-bounding convex SDP. The algorithm is listed in Algorithm 1.
We explain each step of the algorithm below.

1. Solve the SDP in (21) to minimize the training loss. Denote the optimal solution as Z ∗ and

ρ∗ and deﬁne the scaled matrix Z ∗s ←

Z ∗/ρ∗.

8

2. Find the 2d

2d covariance matrix Q∗ by solving (23) with Q =

×
d block matrix. γ = ln(1 + √2), and sin(
) is the element-wise
notation Q(ij) denotes a d
·
sine function. The objective value is guaranteed to be zero due to Theorem 2. Therefore we
have arcsin(Q∗(12)) = γZ ∗s and Q∗ (cid:23)

0, Q∗jj = 1

×

j.

∀

(cid:20)

where the

Q(11) Q(12)
Q(21) Q(22)(cid:21)

3. Sample u1, . . . , um, v1, . . . , vm via

2γ

N
π Z ∗s as a corollary of Theorem 2, we have E[ 1
m
to obtain E[
j=1 ujvT
predictions via ˆyi = 2xT

j
∀
j αj] = 2Z ∗s ρ∗ = 2Z ∗. This is as desired since the SDP computes the
i Zxi.

π Z ∗s . We set αj = ρ∗

m
j=1 ujvT

j ] = 2γ

π
γm ,

P

∼

(cid:20)

(cid:21)

m

sign(

(0, Q∗)). Since E[uvT ] = 2

π arcsin Q∗(12) =

P

u
v

4. This optional step can be performed as described in Section 2.

The extension of the sampling algorithm to the vector output networks is given in Section B.

4.2 Concentration around the mean

We establish a probabilistic bound on the convergence of the empirical sum 1
j in the step
m
3 of the sampling algorithm to its expectation. Our technique involves applying Matrix Bernstein
concentration bound for sums of i.i.d. rectangular matrices [38] to obtain:

m
j=1 ujvT

P

P



1
m

m

Xj=1

ujvT

j −

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)


for all ǫ > 0.

E[u1vT
1 ]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ǫ

≥



≤

exp

2



−

(2γ/π)2

(cid:18)

mǫ2
2
2 + d(c′ + 2ǫ/3)

Z ∗s k
k

+ log(2d)

.

(24)

(cid:19)

We summarize this analysis in the following theorem, which is our main result.

Theorem 3 (Main result). Suppose that the number of neurons satisﬁes m
. Let
≥
Lc denote the Lipschitz constant of the vectorized loss function under the ℓ-inﬁnity norm, i.e.
z′k∞
xik2. Then, Algorithm 1 generates a
ℓ(z)
|
, j = 1, . . . , m that
quantized neural network with weights ˆuj, ˆvj ∈ {−
achieve near optimal loss, i.e.,

, and deﬁne Rm := maxi
[n] k
∈
d and ˆαj =
1, +1
}

ρ∗π
m log(1+√2)

z
Lck

ℓ(z′)

| ≤

c1

−

−

ǫ2

c R4
L2

md log(d)

m

ℓ

((X ˆuj )

(X ˆvj ))ˆαj, y

◦

ℓ

−

(cid:1)

m

((Xu∗j )

Xj=1

(cid:0)

◦

Xj=1
with probability at least 1
−
ǫ
ﬁcient satisﬁes β
d min

(cid:12)
(cid:12)
(cid:12)

(cid:0)

. The weights u∗j , v∗j ∈ {−
are the optimal network parameters for the non-convex combinatorial problem in (20).
(cid:17)

α∗
j |

≤

ˆαj

(cid:16)

,

|

1
Pj |

c3ǫ2m/d for certain constants c1, c2, c3 when the regularization coef-
R, j = 1, . . . , m

c2e−
1
Pj |

≤

(cid:1)(cid:12)
(cid:12)
(cid:12)
d, α∗j ∈
1, +1
}

(Xv∗j ))α∗j , y

ǫ

(25)

Remark 1. For loss functions of the form ℓ(z) = 1
n
loss satisfying
s′|
φ(s′)
−
the inﬁnity norm. This fact follows from
z
Lck
which satisfy our assumption with Lc = 1.

n
) is a scalar Lc-Lipschitz
i=1 φ(zi), where φ(
·
, the vectorized loss function ℓ(z) is Lc-Lipschitz under
n
z′i| ≤
i=1 φ(z′i)
. Examples of 1-Lipschitz loss functions include hinge loss, logistic loss and ℓ1 loss,

P
n
i=1 φ(zi)

i=1 Lc|

φ(s)
|

z′k∞

zi −

s
Lc|

| ≤

P

P

P

−

≤

−

−

1
n

1
n

1
n

(cid:12)
(cid:12)

(cid:12)
(cid:12)

n

Remark 2. Our main result also holds when β
is always satisﬁed.

→

0. In this regime, the constraint β

ǫ
d min

≤

(cid:16)

1
Pj |

ˆαj

|

,

1
Pj |

α∗
j |

(cid:17)

9

The proof of Theorem 3 is provided in Section A.2. To the best of our knowledge, this is the ﬁrst
result on polynomial-time optimal trainability of quantized neural networks. We remark that one
can transform the near optimal quantized bilinear activation network to a near optimal quantized
polynomial activation network with the mapping shown in Section 2. Consequently, this result also
applies to approximating the solution of (8).

Additionally, note that the second layer weights are all identical, which allows us to represent
the sampled neural network using 2md bits and only one scalar ﬂoating point variable. One can
employ the reduction in Section 2.2 to train optimal multi-level quantized neural networks using
the above result in polynomial time.

Furthermore, it is interesting to note that, overparameterization is a key component in enabling
optimization over the combinatorial search space of quantized neural networks in polynomial time.
In contrast, the problems in (20) and (8) are NP-hard when m = 1.

5 Numerical Results

In this section, we present numerical results that verify our theoretical ﬁndings. Additional numer-
ical results can be found in the Appendix.

We compare the performance of the proposed SDP based method against a backpropagation
based method that we describe in the next subsection. We have used CVXPY [9, 1] for solving the
convex SDP. In particular, we have used the open source solver SCS (splitting conic solver) [31, 32]
in CVXPY, which is a scalable ﬁrst order solver for convex cone problems. Furthermore, in solving
the non-convex neural network training problems that we include for comparison, we have used the
stochastic gradient descent (SGD) algorithm with momentum in PyTorch [34].

The experiments have been carried out on a MacBook with 2.2 GHz 6-Core Intel Core i7

processor and 16 GB of RAM.

5.1 Planted dataset experiment

Figure 1 shows the cost as a function of the number of neurons m. The neural network architecture
m
j=1(xT uj)(xT vj)αj.
is a two-layer fully connected network with bilinear activation, i.e., f (x) =
This experiment has been done using a planted dataset. The plot compares the method described
in Section 4 against a backpropagation based quantization method.

P

The algorithm in Section 4 solves the relaxed SDP and then samples binary weights as described
previously. This procedure results in 2md binary weights for the ﬁrst layer. The second layer weights
are all equal to ρ∗π/(γm). This network requires storage of 2md bits and a single real number.
Furthermore, post-training quantization using backpropagation works as follows. First, we train a
two-layer neural network with bilinear activation in PyTorch [34] with m neurons using stochastic
gradient descent (SGD). We ﬁx the second layer weights to 1/m during training. After training,
m
we form the matrices ˆZ =
1
j=1 sign(uj) sign(vT
m . Then, the solution of
2
F is used to determine the second layer weights as c. The optimal
the problem minc
Z ∗k
P
−
ˆZ,Z ∗
solution is given by c = h
. This procedure results in 2md bits for the ﬁrst layer and a single
i
Z ∗,Z ∗
i
h
real number for the second layer and hence requires the same amount of storage as the SDP based
method.

j ) and Z ∗ =

m
j=1 ujvT
j

c ˆZ
k

P

R

∈

In addition to low storage requirements, this particular network is very eﬃcient in terms of
computation. This is very critical for many machine learning applications as this translates to
shorter inference times. For the two-layer neural network with bilinear activation, the hidden layer
and the bilinear activation layer
computations are 2md additions since the weights are

+1,
{

1
}

−

10

SDP + sampling
Backprop + quantization
Lower bound

3000

2000

t
s
o
C

1000

2000

1500

t
s
o
C

1000

500

SDP + sampling
Backprop + quantization

0

101

102
m

103

0

101

102
m

103

a) Training error

b) Test error

Figure 1: Objective against the number of neurons m. Dataset X has been synthetically generated
via sampling from standard Gaussian distribution and has dimensions n = 100, d = 20. The target
vector y has been computed via a planted model with 10 planted neurons. In the planted model,
the ﬁrst layer weights are binary and the second layer weights are real and non-negative. The
4. The lower bound is obtained by solving the SDP in Section
regularization coeﬃcient is β = 10−
3. Plots a and b show the cost on the training and test sets, respectively. The test set has been
generated synthetically by sampling from the same distribution as the training set.

performs m multiplications (i.e. (xT uj)(xT vj) j = 1, . . . , m). The second layer requires only m
additions and one multiplication since the second layer weights are the same.

Figure 1 shows that the SDP based method outperforms the backpropagation approach. Also,
we observe that the cost of the SDP based method approaches the lower bound rapidly as the
number of neurons m is increased. Furthermore, plot b shows that the test set performance for the
SDP based method is also superior to the backpropagation based method.

We note that another advantage of the SDP based sampling method over backpropagation is
that we do not need to solve the SDP for a ﬁxed number of neurons m. That is, the SDP does
not require the number of neurons m as an input. The number of neurons is used only during the
sampling process. This enables one to experiment with multiple values for the number of neurons
without re-solving the SDP.

5.2 Real dataset experiment

Figure 2 compares the backpropagation approach and the SDP based method on a real dataset
from UCI machine learning repository [10]. The dataset is the binary classiﬁcation ”breast-cancer”
dataset and has n = 228 training samples and 58 test samples and the samples are d = 9 dimensional.
Figure 2 shows the classiﬁcation accuracy against time for various methods which we describe below.
The regularization coeﬃcient β is picked for each method separately by searching the value that
yields the highest accuracy and the resulting β values are provided in the captions of the ﬁgures.
Figure 2 shows the training and test accuracy curves for backpropagation without quantization
by the blue solid curve. After the convergence of the backpropagation, we quantize the weights
as described in the previous subsection, and the timing and accuracy for the quantized model are
indicated by the cyan cross marker. The timing and accuracy of the SDP based method are shown
using the red cross marker. Figure 2 demonstrates that the SDP based method requires less time
to return its output. We observe that quantization reduces the accuracy of backpropagation to a
lower accuracy than the SDP based method’s accuracy.

It is important to note that in neural network training, since the optimization problems are

11

0.80

0.8

y
c
a
r
u
c
c
A

0.75

0.70

0.65

0

5

SDP + sampling
Backprop - unquantized
Backprop - quantized

10
15
Time (sec)

20

25

0.7

0.6

y
c
a
r
u
c
c
A

0.5

0.4

SDP + sampling
Backprop - unquantized
Backprop - quantized

0

5

10
15
Time (sec)

20

25

a) Training accuracy

b) Test accuracy

Figure 2: Classiﬁcation accuracy against wall-clock time. Breast cancer dataset with n = 228, d = 9.
The number of neurons is m = 250 and the regularization coeﬃcient is β = 0.1 for the SDP based
method and β = 0.1 for the backpropagation.

non-convex, it takes considerable eﬀort and computation time to determine the hyperparameters
that will achieve convergence and good performance. For instance, among the hyperparameters
that require tuning is the learning rate (i.e. step size). We have performed the learning rate tuning
for the backpropagation algorithm oﬄine and hence it is not reﬂected in Figure 2. Remarkably,
the proposed convex SDP based method does not require this step as it is readily handled by the
convex SDP solver.

Figure 3 shows results for the UCI repository dataset ”ionosphere”. This is a binary classiﬁcation
dataset with n = 280 training samples and 71 test samples. The samples are d = 33 dimensional.
The experiment setting is similar to Figure 2 with the main diﬀerence that the number of neurons
is 10 times higher (i.e., m = 2500). We observe that the SDP based method outperforms the
quantized network trained with backpropagation on both training and test sets.

1.0

0.8

y
c
a
r
u
c
c
A

0.8

0.6

0.4

SDP + sampling
Backprop - unquantized
Backprop - quantized

y
c
a
r
u
c
c
A

0.6

0.4

0

20

40
Time (sec)

60

80

0.2

0

SDP + sampling
Backprop - unquantized
Backprop - quantized

20

40
Time (sec)

60

80

a) Training accuracy

b) Test accuracy

Figure 3: Classiﬁcation accuracy against wall-clock time. Ionosphere dataset with n = 280, d = 33.
The number of neurons is m = 2500 and the regularization coeﬃcient is β = 10 for the SDP based
method, β = 10−

6 for backpropagation.

12

6 Conclusion

We introduced a convex duality based approach for training optimal quantized neural networks
with degree two polynomial activations. We ﬁrst presented a lower-bounding semideﬁnite program
which is tractable in polynomial time. We also introduced a bilinear activation architecture, and
the corresponding SDP lower-bound. We showed that bilinear architectures with binary quantiza-
tion are suﬃcient to train optimal multi-level quantized networks with polynomial activations. We
presented a sampling algorithm to generate quantized neural networks using the SDP by leveraging
Grothendieck’s identity and the connection to approximating the cut norm. Remarkably, we showed
that mild overparameterization is suﬃcient to obtain a near-optimal quantized neural network via
the SDP based sampling approach. Numerical experiments show that our method can generate
signiﬁcantly more accurate quantized neural networks compared to the standard post-training quan-
tization approach. Moreover, the convex optimization solvers are faster than backpropagation in
small to medium scale problems.

An immediate open question is to extend our results to deeper networks and diﬀerent archi-
tectures, such as ReLU networks. For instance, our algorithm can be applied with polynomial
approximations of ReLU. Moreover, one can apply our algorithm layerwise to optimally quantize a
pre-trained neural network by knowledge distillation.

We acknowledge that our current numerical results are limited to small and medium datasets due
to the memory constraints of standard SDP solvers. However, one can design custom optimization
methods to obtain approximate solutions of the SDP for larger dimensional instances. The SDPs
can also be deﬁned and solved in deep learning frameworks with appropriate parameterizations.
Random projection and sketching based optimizers for high-dimensional convex programs [39, 40,
24] and randomized preconditioning [23, 27, 26, 33, 28] can address these computational challenges.
We leave this as an important open problem.

From a complexity theoretical perspective, it is remarkable that overparameterization breaks
computational barriers in combinatorial and non-convex optimization. Speciﬁcally, it is straight-
forward to show that training a quantized neural network when m = 1, i.e., a single neuron is
NP-hard due to the connection to the MaxCut problem. However, allowing m =
(d log d) enables
optimization over a combinatorial search space in polynomial time. Exploring the other instances
and limits of this phenomenon is another interesting research direction.

O

Acknowledgments

This work was partially supported by the National Science Foundation under grants IIS-1838179
and ECCS-2037304, Facebook Research, Adobe Research and Stanford SystemX Alliance.

References

[1] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewriting system

for convex optimization problems. Journal of Control and Decision, 5(1):42–60, 2018.

[2] Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs

deep learning. arXiv preprint arXiv:2001.04413, 2020.

[3] Noga Alon and Assaf Naor. Approximating the cut-norm via grothendieck’s inequality.

In
Proceedings of the thirty-sixth annual ACM symposium on Theory of computing, pages 72–80,
2004.

13

[4] S. Anwar, K. Hwang, and W. Sung. Fixed point optimization of deep convolutional neural
networks for object recognition. In 2015 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), pages 1131–1135, April 2015.

[5] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal

of Machine Learning Research, 18(1):629–681, 2017.

[6] Burak Bartan and Mert Pilanci. Convex relaxations of convolutional neural nets.

In
ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 4928–4932. IEEE, 2019.

[7] Burak Bartan and Mert Pilanci. Neural spectrahedra and semideﬁnite lifts: Global convex
optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
arXiv:2101.02429, 2021.

[8] Yoshua Bengio, Nicolas Le Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte.
Convex neural networks. Advances in neural information processing systems, 18:123, 2006.

[9] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for

convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

[10] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[11] T. Ergen and M. Pilanci. Convex optimization for shallow neural networks.

In 2019 57th
Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages
79–83, 2019.

[12] Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoen-
coding and interpretable models. In International Conference on Artiﬁcial Intelligence and
Statistics, pages 4024–4033. PMLR, 2020.

[13] Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex

duality. arXiv preprint arXiv:2002.09773, 2020.

[14] Tolga Ergen and Mert Pilanci.

Implicit convex regularizers of cnn archi-tectures: Convex
optimization of two-and three-layer networks in polynomial time. International Conference on
Learning Representations (ICLR), arXiv preprint arXiv:2006.14798, 2021.

[15] Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci.
Demystifying batch normalization in relu networks: Equivalent convex optimization models
and implicit regularization. arXiv preprint arXiv:2103.01499, 2021.

[16] Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin Lauter, Michael Naehrig, and John
Wernsing. Cryptonets: Applying neural networks to encrypted data with high throughput and
accuracy. In International Conference on Machine Learning, pages 201–210, 2016.

[17] M. X. Goemans and D. P. Williamson.

Improved approximation algorithms for maximum
cut and satisﬁability problems using semideﬁnite programming. J. Assoc. Comput. Mach.,
42:1115–1145, 1995.

[18] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir D. Bourdev. Compressing deep convolutional

networks using vector quantization. CoRR, abs/1412.6115, 2014.

14

[19] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning

with limited numerical precision. CoRR, abs/1502.02551, 2015.

[20] Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci. Exact and relaxed convex for-
mulations for shallow neural autoregressive models. International Conference on Acoustics,
Speech, and Signal Processing, 2021.

[21] S. Han, H. Mao, and W.J. Dally. Deep compression: Compressing deep neural network with

pruning, trained quantization and huﬀman coding. CoRR, abs/1510.00149, 2015.

[22] K. Hwang and W. Sung. Fixed-point feedforward deep neural network design using weights
+1, 0, and -1. In 2014 IEEE Workshop on Signal Processing Systems (SiPS), pages 1–6, Oct
2014.

[23] Jonathan Lacotte, Sifan Liu, Edgar Dobriban, and Mert Pilanci. Limiting spectrum of
randomized hadamard transform and optimal iterative sketching methods. arXiv preprint
arXiv:2002.00864, 2020.

[24] Jonathan Lacotte and Mert Pilanci. Adaptive and oblivious randomized subspace meth-
ods for high-dimensional optimization: Sharp analysis and lower bounds. arXiv preprint
arXiv:2012.07054, 2020.

[25] Jonathan Lacotte and Mert Pilanci. All local minima are global for two-layer relu neural
networks: The hidden convex optimization landscape. arXiv preprint arXiv:2006.05900, 2020.

[26] Jonathan Lacotte and Mert Pilanci. Eﬀective dimension adaptive sketching methods for faster

regularized least-squares optimization. arXiv preprint arXiv:2006.05874, 2020.

[27] Jonathan Lacotte and Mert Pilanci. Optimal randomized ﬁrst-order methods for least-squares
problems. In International Conference on Machine Learning, pages 5587–5597. PMLR, 2020.

[28] Jonathan Lacotte and Mert Pilanci. Fast convex quadratic optimization solvers with adaptive

sketching-based preconditioners. arXiv preprint arXiv:2104.14101, 2021.

[29] Darryl Dexu Lin and Sachin S. Talathi. Overcoming challenges in ﬁxed point training of deep

convolutional networks. CoRR, abs/1607.02241, 2016.

[30] Darryl Dexu Lin, Sachin S. Talathi, and V. Sreekanth Annapureddy. Fixed point quantization

of deep convolutional networks. CoRR, abs/1511.06393, 2015.

[31] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting
and homogeneous self-dual embedding. Journal of Optimization Theory and Applications,
169(3):1042–1068, June 2016.

[32] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd. SCS: Splitting conic solver, version 2.1.2.

https://github.com/cvxgrp/scs, November 2019.

[33] Ibrahim Kurban Ozaslan, Mert Pilanci, and Orhan Arikan.

Iterative hessian sketch with
momentum. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 7470–7474. IEEE, 2019.

15

[34] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas
Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems
32, pages 8024–8035, 2019.

[35] Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-
time convex optimization formulations for two-layer networks. In International Conference on
Machine Learning, pages 7695–7705. PMLR, 2020.

[36] Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network
problems are copositive programs: Convex analysis of two layer networks and polynomial-time
algorithms.
International Conference on Learning Representations (ICLR), arXiv preprint
arXiv:2012.13329, 2021.

[37] Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regu-
larization behind neural reconstruction. International Conference on Learning Representations
(ICLR), arXiv preprint arXiv:2012.05169, 2021.

[38] Joel A Tropp. An introduction to matrix concentration inequalities.

arXiv preprint

arXiv:1501.01571, 2015.

[39] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. Scalable
semideﬁnite programming. SIAM Journal on Mathematics of Data Science, 3(1):171–200,
2021.

[40] Alp Yurtsever, Madeleine Udell, Joel Tropp, and Volkan Cevher. Sketchy decisions: Convex
In Artiﬁcial intelligence and statistics,

low-rank matrix optimization with optimal storage.
pages 1188–1196. PMLR, 2017.

[41] Chenzhuo Zhu, Song Han, Huizi Mao, and William J. Dally. Trained ternary quantization.

CoRR, abs/1612.01064, 2016.

16

A Proofs

A.1 Proof of Theorem 1

Rd by a multi-level quantized weight
Proof. First we show that the multiplication of the input x
d
vector q
M can be represented by the dot product of a function of the input, i.e., ˜x and a binary
∈ Q
quantized weight vector u, that is, qT x = uT ˜x. Here, u is a binary vector of size dM with entries
satisfying

∈

qi :=

M

Xk=1

uk+(i

−

1)M , i = 1, . . . , d .

(26)

For instance, for M = 4, we have q1 = u1 + u2 + u3 + u4. Note that because uj’s are from the
, which is equal to the set for (4 + 1 = 5)-level
set
2, 0, 2, 4
, we have that q1 ∈ {−
1, +1
}
}
quantization, i.e.,
Q4. The second entry of the q vector similarly satisﬁes q2 = u5 +u6+u7 +u8 ∈ Q4.
The same holds for all the entries q1, . . . , qd.

{−

−

4,

Next, plugging in (26) in the dot product qT x yields

d

d

M

qT x =

qixi =

Xi=1
d

M

Xi=1

Xk=1

uk+(i

−

1)M xi

uk+(i

−

1)M ˜xk+(i

1)M

−

=

Xi=1
= uT ˜x

Xk=1

(27)

RdM . This shows that
where we deﬁned ˜x :=
the dot product qT x is equal to the dot product uT ˜x where u is a dM -dimensional vector with
binary entries.

x1, x1, . . . , x1, x2, x2, . . . , x2, . . . , xd, xd, . . . , xd

∈

(cid:3)

(cid:2)

T

The input-output relationship for the two-layer fully connected neural network with polynomial
aqT
d
M and
R, j = 1, . . . , m. Using the fact that we can represent a dot product with multi-level quantized

activation is f (x) =
αj ∈
weights as a dot product with binary quantized weights, we equivalently have

αj where qj ∈ Q

m
j=1 σ(xT qj)αj =

j x + c
(cid:17)

j xxT qj + bqT

m
j=1

P

P

(cid:16)

f (x) =

m

Xj=1

(cid:0)

auT

j ˜x˜xT uj + buT

j ˜x + c

αj .

(28)

(cid:1)

We can rewrite this as a neural network with quadratic activation:

m

Xj=1
m

f (x) =

=

uT
j

1

(cid:20)

(cid:3)

˜ujαj

(cid:2)
˜uT
j X

a˜x˜xT
b
2 ˜xT

b
2 ˜x
c

αj

uj
1

(cid:21)

(cid:21) (cid:20)

(29)

where we have deﬁned ˜uj ∈ {−
j = 1, . . . , m. The proof of the converse follows from the symmetrization identity (7).

This representation can be seen as a bilinear activation network with u′j = uj and v′j = uj,

X ∈

×

R(dM +1)

(dM +1).

Xj=1
dM +1, j = 1, . . . , m, and
1, +1
}

17

A.2 Proof of Theorem 3

Proof. We begin by applying the matrix Bernstein concentration bound on the matrices (ujvT
E[ujvT
the following upper bound on the spectral norm of these matrices

j −
d)-dimensional zero-mean i.i.d. matrices. We obtain

j ]), j = 1, . . . , m, which we note are (d

×

ujvT
k

j −

E[ujvT
j ]

k ≤ k

E[ujvT
j ]
j k2 +
k2
k
j k2 + E[
ujvT
j k2]
k
vjk2 + E[
ujk2k
k

ujvT
ujvT
ujk2k
k
d + d = 2d ,

≤ k
=

≤

vjk2]

(30)

for j = 1, . . . , m where we use the triangle inequality in the ﬁrst line and Jensen’s inequality in the
m
second line. Next, we deﬁne Sj := ujvT
j=1 Sj, then the matrix variance of
the sum (which we will plug in the matrix concentration bound formula) is given by

j ] and S :=

E[ujvT

j −

P

σ2 = max

E[SST ]

k2,

k

{k

E[ST S]

k2}

= max

m

Xj=1

E[SjST
j ]
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m

Xj=1

,

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E[ST

j Sj]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)




2






(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

where the second equality follows because Sj’s are zero-mean.



E[SjST

j ] = E

ujvT
j −
h(cid:0)
= d E[ujuT
j ]
= d E[ujuT
j ]
= d E[ujuT
j ]

−

−

−

E[ujvT
j ]

T

i

(cid:1)

(cid:1) (cid:0)

E[ujvT
ujvT
j ]
j −
j ] E[vjuT
E[ujvT
j ]
T
(2γ/π)2Z ∗s Z ∗s
(2γ/πZ ∗s )2 .

Next, we bound the spectral norm of E[SST ] as

m

m

(31)

(32)

E[SST ]

k2 =

=

d E[ujuT
j ]

(2γ/πZ ∗s )2

E[SjST
j ]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:1)

k

−

=

Xj=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
m(2γ/πZ ∗s )2
(cid:13)
2
(cid:13)
2
Z ∗s k
(cid:13)
(cid:13)
2
k
(cid:13)
(cid:13)
k2 + m(2γ/π)2
= md(2γ/π)
(cid:13)
k
(cid:13)
The last line follows from the identity E[u1uT
1 ] = 2γ/π arcsin(Q(11)). We note that the upper
E[ST S]
bound for
k2. Hence, the matrix variance is upper
k2 is also an upper bound for
bounded by σ2
c′md+m(2γ/π)2
0 is a constant. Applying the matrix Bernstein
Z ∗s k
k
concentration bound yields

(cid:13)
(cid:13)
Xj=1
(cid:13)
(cid:0)
(cid:13)
md E[u1uT
1 ]
(cid:13)
(cid:13)
E[u1uT
1 ]
md
(cid:13)
≤
(cid:13)
E[u1uT
= md
1 ]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
arcsin(Q(11))
(cid:13)
(cid:13)

m(2γ/πZ ∗s )2
−
2 +
2 + m(2γ/π)2

k
2 where c′ ≥

E[SST ]

Z ∗s k
k

(33)

2
2 .

≤

k

2

P

m





(cid:13)
(cid:13)
Xj=1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(ujvT

j −

E[ujvT

j ])
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≥

2

mǫ



≤

2d exp

(cid:18)

m2ǫ2
σ2 + 2dmǫ/3

−

.

(cid:19)

(34)



18

Plugging in the expression for the variance, we obtain

1
m

m

Xj=1

ujvT

j −

P





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E[u1vT
1 ]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

ǫ

≥



≤

2d exp



= 2d exp

(cid:18)

(cid:18)

m2ǫ2

−

c′md + m(2γ/π)2

2
2 + 2dmǫ/3

Z ∗s k
k
mǫ2
2
2 + d(c′ + 2ǫ/3)

(cid:19)

(cid:19)

Z ∗s k
k
mǫ2
2
2 + d(c′ + 2ǫ/3)

+ log(2d)

.

(35)

(cid:19)

−

(2γ/π)2

= exp

−

(2γ/π)2

(cid:18)

Z ∗s k
k

m
j=1 u∗j (v∗j )T α∗j
Let us denote the optimal solution of the original non-convex problem as Z ∗nc =
R, j = 1, . . . , m are optimal network parameters for the
d, α∗j ∈
where the weights u∗j , v∗j ∈ {−
1, +1
}
non-convex combinatorial problem in (20) . Solving the SDP gives us an unquantized solution Z ∗
and via the sampling algorithm, we obtain the quantized solution given by ˆZ =

P

m
j=1 ˆuj ˆvT

j ˆαj.

We now introduce some notation. We will denote the loss term in the objective by L(Z) and

P

the regularization term by R(Z), that is,

L(Z) := ℓ 


, y



, R(Z) := d

xT
1 Zx1
...
xT
n Zxn













m

Xj=1

αj|
|

when Z =

m

Xj=1

ujvT

j αj.

(36)

We now bound the diﬀerence between the losses for the unquantized solution of the SDP, i.e., Z ∗,
and the quantized weights ˆZ =

m
j=1 ˆuj ˆvT

j ˆαj:

where we substituted ˆαj = ρ∗
in the SDP, i.e., ˆyi = 2xT

π
γm . The scaling factor of 2 in front of Z ∗ is due to the scaling factor
E[u1vT

(37)

P

xT
1 (

m
j=1 ˆuj ˆvT
j
...
m
j=1 ˆuj ˆvT
j

ρ∗π
γm −

ρ∗π
γm −

L( ˆZ)
|



−

P

| ≤

L(Z ∗)

Lc (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
i Zxi. Plugging in Z ∗/ρ∗ = Z ∗s = π
2γ

xT
n (





P

L( ˆZ)
|

−

L(Z ∗)

| ≤

1 ( 1
xT
m

ρ∗π
γ 


max
i=1,...,n

Lc (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ρ∗π
(cid:13)
(cid:13)
γ

n ( 1
xT
m

P

P
xT
i (

= Lc

Lc

ρ∗π
γ

≤

(cid:12)
(cid:12)
(ǫ

max
i=1,...,n

xik
k

m
j=1 ˆuj ˆvT
j −
...
m
j=1 ˆuj ˆvT
j −
m
1
ˆuj ˆvT
m

Xj=1
2
2) = Lc

2Z ∗)x1

2Z ∗)xn



(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1 ] yields





∞

E[u1vT

1 ])x1

E[u1vT

1 ])xn

E[u1vT

j −



(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
1 ])xi

∞

ρ∗π
γ

ǫR2
m

(cid:12)
(cid:12)

(38)

which holds with probability at least 1
as a result of the
matrix Bernstein concentration bound. Therefore, when the number of sampled neurons satisﬁes
the inequality

mǫ2
2+d(c′+2ǫ/3) + log(2d)
2
Z ∗
s k
k
(cid:17)

(2γ/π)2

exp

−

−

(cid:16)

(2γ/π)2

mǫ2
2
2 + d(c′ + 2ǫ/3) ≥

Z ∗k
k

2 log(2d) ,

19

this probability is at least 1
independent of d, m and ǫ.

−

exp(

−

log(2d)) = 1

exp(

−

−

Cǫ2m/d), where C > 0 is a constant

Next, we obtain upper and lower bounds on the non-convex optimal value. Since the SDP
solution provides a lower bound, and the sampled quantized network provides an upper bound, we
can bound the optimal value of the original non-convex problem as follows

L( ˆZ) + βR( ˆZ)

L(Z ∗nc) + βR(Z ∗nc)

L(Z ∗) + βR(Z ∗) .

(39)

We have already established that

mǫ with high probability. It follows

L( ˆZ)

−

≥
L( ˆZ)
|

−
L(Z ∗nc) = L( ˆZ)
ρ∗π
γ
ρ∗π
γ

≤

≤

≥
ρ∗π
γ LcR2

L(Z ∗)

| ≤

L(Z ∗) + L(Z ∗)

L(Z ∗nc)

−
LcR2

mǫ + L(Z ∗)

−

−
L(Z ∗nc)

LcR2

mǫ + βR(Z ∗nc)

(40)

where we have used (39) and that R(Z ∗)
implies that L(Z ∗nc)
ρ∗π
γ LcR2
R(Z ∗
nc)

βR( ˆZ).
ρ∗π
γ LcR2
mǫ
R( ˆZ)

satisﬁes β

L( ˆZ)

and β

−
mǫ

≤

≤

≤

0 to obtain the last inequality. Furthermore, (39)
If we pick the regularization coeﬃcient β such that it

≥

, we obtain the following approximation error bound

L(Z ∗nc)
|

−

L( ˆZ)

| ≤

2

ρ∗π
γ

LcR2

mǫ .

(41)

Rescaling ǫ by 2 ρ∗π

γ LcR2

m, i.e., replacing ǫ with

1
γ LcR2

2 ρ∗π

m

ǫ, we obtain the claimed approximation

result.

A.3 Duality Analysis for Bilinear Activation

This subsection has the details of the duality analysis that we have carried out to obtain the SDP in
(21) for the bilinear activation architecture. The derivations follow the same strategy as the duality
analysis in Section 3. The non-convex problem for training such a network is stated as follows:

p∗b =

s.t. uj,vj

min
d,αj
1,1
}

∈{−

R

∈

j
∀

[m]

∈

ℓ



(Xvj ))αj, y

+ βd



◦

.

αj|
|

(42)

Taking the convex dual with respect to the second layer weights
primal is lower bounded by

αj}
{


m
j=1, the optimal value of the

m

((Xuj )

Xj=1



m

Xj=1

p∗

≥

d∗ =

Rn is the dual variable.

maxu,v∈{−1,1}d |

max
νT ((Xu)
◦

(Xv))

|≤

ν)

ℓ∗(

−

βd −

(43)

The constraint maxu,v

1,1

d
}

∈{−

νT ((Xu)
|

◦

(Xv))

| ≤

βd can be equivalently stated as the following

where ν

∈

two inequalities

n

νixixT
i

βd ,

v

≤

!

q∗1 = max
i =v2
u2
i =1,

q∗2 = max
i =v2
u2
i =1,

uT

uT

i
∀

i
∀

Xi=1
n

 −

Xi=1

20

νixixT
i

βd.

v

≤

!

(44)

 
We note that the second constraint q∗2 ≤
the ﬁrst constraint leads to the second constraint:

βd is redundant since the change of variable u

u in

← −

q∗1 = max
i =v2
u2
i =1,

uT

i
∀

n

Xi=1

νixixT
i

v =

!

max
ui)2=v2

i =1,

i
∀

(
−

u)T

(
−

n

Xi=1

νixixT
i

v = max
i =v2
u2
i =1,

!

i
∀

uT

 −

n

νixixT
i

Xi=1

(45)

v = q∗2.

!

In the sequel, we remove the redundant constraint q∗2 ≤
tion maxu2

n
i=1 νixixT
i

v is given by (see, e.g., [3])

i =v2

i =1,

i uT
∀

βd. The SDP relaxation for the maximiza-

(cid:0)P

ˆq1 =

K=

(cid:1)

max
Z
V


Z T W




(cid:23)

tr

n

Xi=1

νixixT

i Z

.

!

0, Kjj=1,

j
∀

(46)

The dual of the above SDP relaxation can be derived via standard convex duality theory, and can
be stated as

Then, we arrive at

d∗

≥

min
z1,z2 s.t. ¯1T z1+¯1T z2=0

2d λmax

diag(z1)
n
i=1 νixixT
i

n
i=1 νixixT
i
diag(z2)

P

.

(cid:21)(cid:19)

(cid:18)(cid:20)

P

dSDP := max

ν,z1,z2 −

s.t.

ℓ∗(

ν)

−
diag(z1)
n
i=1 νixixT
i
(cid:20)
P
¯1T z1 + ¯1T z2 = 0 .
P

n
i=1 νixixT
i
diag(z2)

β
2

I

0

(cid:22)

−

(cid:21)

(47)

(48)

Next, we will ﬁnd the dual of the above problem. The Lagrangian is given by

L(ν, z1, z2, Q, ρ) =

=

ℓ∗(

−

−

ν)

−

tr

Q

(cid:18)

(cid:20)

diag(z1)
n
i=1 νixixT
i

n
i=1 νixixT
i
diag(z2)

P

+

β
2

(cid:21)(cid:19)

=

ℓ∗(

−

−

ν)

−

d

P

(Vjjz1,j + Wjjz2,j)

Xj=1

2

−

n

Xi=1

νixT

i Zxi +

β
2

d

tr(Q) + ρ

(z1,j + z2,j)

Xj=1
d

tr(Q) + ρ

(z1,j + z2,j)

(49)

Xj=1

Maximizing the Lagrangian with respect to ν, z1, z2 yields the problem

2xT

1 Zx1
...
n Zxn

2xT

min
Q,ρ

ℓ 



, y



+

β
2

tr(Q)













s.t. Vjj = ρ, Wjj = ρ, j = 1, . . . , d
V
Z
Z T W

Q =

0 .

(cid:23)

(cid:20)

(cid:21)

21

(50)

 
 
 
Finally, we obtain the following more concise form for the convex program

min
Q,ρ

s.t.

ℓ (ˆy, y) + βdρ

i Zxi, i = 1, . . . , n

ˆyi = 2xT
Qjj = ρ, j = 1, . . . , 2d

Q =

V
Z
Z T W

(cid:20)

0 .

(cid:23)

(cid:21)

(51)

B Vector Output Networks

We will assume the following vector output neural network architecture with bilinear activation

f (x) =

m

Xj=1

(xT uj)(xT vj)αT
j

(52)

where the second layer weights αj ∈
f (x) : Rd
×
concisely represented as ˆY = f (X)
training problem can be formulated as

R1

→

∈

RC, j = 1, . . . , m are C-dimensional vectors. We note that
C. The output of the neural network for all the samples in the dataset can be
C to denote the target matrix. The

C. We use Y

Rn

Rn

∈

×

×

p∗ =

uj ,vj

∈{−

min
d,αj
}

1,1

RC j

∈

[m]

∈

m

((Xuj )

Xj=1

ℓ





(Xvj))αT

j , Y

◦

+ βd

m

Xj=1





αjk1 .
k

(53)

Or,

p∗ =

uj,vj

min
d, j
1,1
}

∈{−

[m]

∈

αj

min

RC , j

[m], ˆY

∈

∈

ℓ

ˆY , Y

+ βd

(cid:16)

(cid:17)

m

Xj=1

αj k1
k

m

s.t.

ˆY =

((Xuj )

Xj=1

(Xvj ))αT
j .

◦

(54)

(55)

Rn, k =

(56)

The dual problem for the inner minimization problem is

max

ν −

ℓ∗(

−

ν)

s.t.

νT
k ((Xuj )
|

◦

(Xvj))

| ≤

βd,

j, k .

∀

We have introduced the dual variable ν
1, . . . , C. The optimal value of the primal is lower bounded by

C and its columns are denoted by νk ∈

∈

×

Rn

p∗

≥

d∗ =

maxu,v∈{−1,1}d |

max
νT
k ((Xu)
◦

(Xv))

ν) .

ℓ∗(

−

βd ,

k −
∀

|≤

The constraints of the above optimization problem can be equivalently stated as the following
inequalities

n

νk,ixixT
i

βd, k = 1, . . . , C,

v

≤

!

q∗1,k = max
i =v2
u2
i =1,

q∗2,k = max
i =v2
u2
i =1,

uT

uT

i
∀

i
∀

Xi=1
n

 −

Xi=1

βd, k = 1, . . . , C .

(57)

νk,ixixT
i

v

≤

!

22

 
As we have shown in Section A.3, the second set of inequalities q∗2,k ≤
hence we remove them. The SDP relaxation for the maximization maxu2
is given by

βd are implied by the ﬁrst and
v

n
i=1 νk,ixixT
i

i =v2

i =1,

i uT
∀

(cid:0)P

ˆq1,k =

max
V
Z
K=

Z T W




(cid:23)

tr

n

Xi=1

νk,ixixT

i Z

.

!

0, Kjj=1,

j
∀

We have previously given the dual of this problem as

min
zk,1,zk,2 s.t. ¯1T zk,1+¯1T zk,2=0

2d λmax

diag(zk,1)
n
i=1 νk,ixixT
i

n
i=1 νk,ixixT
i
diag(zk,2)

P

,

(cid:21)(cid:19)

(cid:18)(cid:20)

(cid:1)

(58)

(59)

P

Rd, k = 1, . . . , C. This allows us to establish the

where we deﬁne the variables zk,1 ∈
following lower bound

Rd, zk,2 ∈

d∗

≥

dSDP :=

max
zk,1,zk,2
{

ν,

C

k=1 −
}

ν)

ℓ∗(

−

s.t.

diag(zk,1)
n
i=1 νk,ixixT
i
(cid:20)
P
¯1T zk,1 + ¯1T zk,2 = 0,
P

n
i=1 νk,ixixT
i
diag(zk,2)

β
2

I

(cid:22)

−

(cid:21)

0,

k = 1, . . . , C

k = 1, . . . , C .

(60)

Next, we ﬁnd the dual of this problem. First, we write the Lagrangian:

L(ν,

zk,1, zk,2, Qk, ρk}
{

C
k=1) =

C

=

ℓ∗(

−

−

ν)

−

tr

Qk

(cid:18)

(cid:20)

Xk=1
C

P

=

ℓ∗(

−

−

ν)

−

Xk=1

(cid:0)
ρk(¯1T zk,1 + ¯1T zk,2) ,

C

+

Xk=1

diag(zk,1)
n
i=1 νk,ixixT
i

n
i=1 νk,ixixT
i
diag(zk,2)

P

β
2

+

n

(cid:21)(cid:19)
C

diag(Vk)T zk,1 + diag(Wk)T zk,2

2

−

(cid:1)

Xk=1

Xi=1

C

C

tr(Qk) +

ρk(¯1T zk,1 + ¯1T zk,2)

Xk=1
νk,ixT

i Zkxi +

Xk=1
C
β
2

Xk=1

tr(Qk)

(61)

where we have introduced Qk =

. Maximization of the Lagrangian with respect to

ν, zk,1, zk,2, k = 1, . . . , C leads to the dual problem given by

min
C
Qk,ρk}
k=1
{

s.t. Vk,jj = ρk, Wk,jj = ρk,

2xT

2xT

1 ZCx1
...
n ZC xn
k

∈

, Y 



+



[C], j



∈

[d]

β
2

C

Xk=1

tr(Qk)

Vk Zk
Z T
k Wk(cid:21)

(cid:20)

2xT

1 Z1x1
...
n Z1xn

2xT

. . .
. . .
. . .

ℓ 








Qk =

Vk Zk
Z T
k Wk(cid:21)

(cid:20)

0,

k

[C].

∈

(cid:23)

(62)

23

 
More concisely,

min
C
Qk,ρk}
k=1
{

s.t.

ℓ

+ βd

ˆY , Y
(cid:16)
(cid:17)
ˆYik = 2xT
i Zkxi,
k
Qk,jj = ρk,

C

ρk

Xk=1
i

∈

Qk =

(cid:20)

Vk Zk
Z T
k Wk(cid:21)

0,

(cid:23)

[n], k

[C]

∈
[C], j

∈
[2d]

∈

k

[C].

∈

(63)

where Vk, Zk, Wk are d

×

d-dimensional matrices.

B.1 Sampling Algorithm for Vector Output Networks

We now give the sampling algorithm:

1. Solve the SDP in (63) and deﬁne the matrices Z ∗s,k ←
2. Find Q∗k, k = 1, . . . , C by solving the problem

Z ∗k /ρ∗k, k = 1, . . . , C.

Q∗k := arg

Q

(cid:23)

min
0,Qjj=1

j k
∀

Q(12) −

2
F .
sin(γZ ∗s,k)
k

(64)

3. Carry out the following steps for each k = 1, . . . , C:

a. Sample m/C pairs of the ﬁrst layer weights uj, vj via

∼
N
b. Set the second layer weights for these neurons to αj = ρ∗kC π
γm ek where ek ∈

(0, Q∗k)).

sign(

(cid:20)

k’th unit vector.

uj
vj (cid:21)

RC is the

4. (optional) Transform the quantized bilinear activation network to a quantized polynomial

activation network as described in Section 2.

Figure 4 shows the classiﬁcation accuracy on a UCI machine learning repository with C = 4
classes. We perform one-hot encoding on the output and use the vector output SDP and sampling
method developed in this section. We observe that the accuracy of the sampling method approaches
the accuracy of the lower bounding SDP as m is increased.

C Further Details on Step 4 of the Sampling Algorithm

As stated in Step 4 of the sampling algorithm given in subsection 4.1, it is possible to transform the
bilinear activation architecture to a quadratic activation neural network with 3m neurons. The ﬁrst
layer weights of the quadratic activation network can be obtained, via the symmetrization identity,
d, j = 1, . . . , m. The second layer
as 1/2(uj + vj)
1, +1
}
weights are picked as stated in Step 3 for the ﬁrst m neurons and the remaining 2m neurons have
the opposite sign.

d, uj ∈ {−
1, 0, +1
}

d, vj ∈ {−
1, +1
}

∈ {−

24

0.9

0.8

0.8

0.7

0.6

y
c
a
r
u
c
c
A

0.7

0.6

y
c
a
r
u
c
c
A

0.5

0.4

SDP + sampling
SDP - not quantized

102

103

104
m

105

106

0.5

0.4

SDP + sampling
SDP - not quantized

102

103

104
m

105

106

a) Training accuracy

b) Test accuracy

Figure 4: Vector output network experiment showing multiclass classiﬁcation accuracy against the
number of sampled neurons m. The dataset is statlog vehicle multiclass with C = 4 classes and
dimensions n = 676, d = 18. The regularization coeﬃcient is β = 1. The blue solid line shows the
accuracy when we predict the labels using the lower bounding SDP in (63) without quantization.
The green curve with circle markers shows the accuracy for the quantized network when we use the
sampling method that we designed for the vector output case.

D Additional Numerical Results

Figure 5 shows the accuracy against time for the credit approval dataset. For this dataset, we
similarly observe shorter run times and better classiﬁcation accuracies for the SDP based sampling
method. Furthermore, increasing the number of neurons (plots c,d) improves the accuracy for both
methods, which is in consistency with the experiment result shown in Figure 1.

D.1 ReLU network comparison

Figure 6 compares the SDP based sampling method with a two-layer ReLU network. We train
the ReLU network using backpropagation and quantize the ﬁrst layer weights post-training. The
second layer weights are only scaled to account for the quantization of the ﬁrst layer weights and
not restricted to be identical. Thus, unlike the previous ﬁgures, the comparison in Figure 6 unfairly
favors the ReLU network. We observe that the SDP approach can still outperform SGD in this
case.

25

0.9

0.8

0.8

y
c
a
r
u
c
c
A

0.7

0.6

0.5

0.4

0.8

0.7

y
c
a
r
u
c
c
A

0.6

SDP + sampling
Backprop - unquantized
Backprop - quantized

y
c
a
r
u
c
c
A

0.7

0.6

0.5

SDP + sampling
Backprop - unquantized
Backprop - quantized

0

10

20

30

40

0

10

20

30

40

Time (sec)

Time (sec)

a) Training accuracy, m = 500

b) Test accuracy, m = 500

0.8

0.7

y
c
a
r
u
c
c
A

0.6

SDP + sampling
Backprop - unquantized
Backprop - quantized

SDP + sampling
Backprop - unquantized
Backprop - quantized

0

25

50

75

100

0

25

50

75

100

Time (sec)

Time (sec)

c) Training accuracy, m = 2500

d) Test accuracy, m = 2500

Figure 5: Classiﬁcation accuracy against wall-clock time. Credit approval dataset with n = 552, d =
15. The number of neurons m is speciﬁed in the sub-caption for each plot. The regularization
coeﬃcient is β = 10 for the SDP based method and β = 0.001 for backpropagation.

1.0

y
c
a
r
u
c
c
A

0.8

0.6

0.4

0.8

y
c
a
r
u
c
c
A

0.6

0.4

SDP + sampling
ReLU - unquantized
ReLU - quantized

SDP + sampling
ReLU - unquantized
ReLU - quantized

0

20

40
Time (sec)

60

80

0

20

40
Time (sec)

60

80

a) Training accuracy

b) Test accuracy

Figure 6: Classiﬁcation accuracy against wall-clock time showing comparison with a two-layer
ReLU network. Ionosphere dataset with n = 280, d = 33. For the SDP based sampling method,
m = 2500 and the regularization coeﬃcient is β = 10. For the ReLU network, m = 5000 and
β = 10−

7.

26

