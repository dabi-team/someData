GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

1

Compactly Restrictable Metric Policy Optimization Problems

Victor D. Dorobantu, Kamyar Azizzadenesheli, and Yisong Yue

2
2
0
2

l
u
J

2
1

]

C
O
.
h
t
a
m

[

1
v
0
5
8
5
0
.
7
0
2
2
:
v
i
X
r
a

Abstract— We study policy optimization problems for determin-
istic Markov decision processes (MDPs) with metric state and ac-
tion spaces, which we refer to as Metric Policy Optimization Prob-
lems (MPOPs). Our goal is to establish theoretical results on the
well-posedness of MPOPs that can characterize practically relevant
continuous control systems. To do so, we deﬁne a special class of
MPOPs called Compactly Restrictable MPOPs (CR-MPOPs), which
are ﬂexible enough to capture the complex behavior of robotic
systems but speciﬁc enough to admit solutions using dynamic
programming methods such as value iteration. We show how to
arrive at CR-MPOPs using forward-invariance. We further show that
our theoretical results on CR-MPOPs can be used to characterize
feedback linearizable control afﬁne systems.

Index Terms— Continuous Markov Decision Processes,
Reinforcement Learning, Optimal Control, Value Iteration,
Selection Theorems, Sampled-Data, Physical Systems

I. INTRODUCTION

P OLICY optimization is a cornerstone in planning, control and

reinforcement learning. Classic approaches include value itera-
tion and policy iteration [1]–[5]. From a theoretical standpoint, a key
step is establishing when policy optimization is well-posed, i.e., when
optimal policies exist, and when they can be found algorithmically.
While such results for value and policy iteration are well established
for discrete systems with ﬁnitely many states and actions (also known
as the tabular setting), relatively few foundational results have been
established for continuous control.

In this paper, we study Metric Policy Optimization Problems
(MPOPs), which come endowed with metric state and action spaces.
Compared to tabular MDPs, several new challenges arise in the
continuous setting. Even for deterministic problems, rewards may be
unbounded, maxima of functions (over actions) may not exist, and
the domains of value functions may be different for different policies.
Without addressing these challenges, optimal policies need not exist
and value iteration or policy iteration may be impossible.

In order to establish well-posedness of dynamic programming ap-
proaches, we deﬁne the class of Compactly Restrictable MPOPs (CR-
MPOPs). We show that CR-MPOPs arise naturally when imposing
forward-invariance constraints on the policy class one optimizes over.
As such, CR-MPOPs are well suited for characterizing many systems
which rely on forward-invariance for controller design.

Sampled-data design [6] allows us to synthesize policies for
continuous-time systems when inputs are passed through a zero-order
hold (held constant over ﬁxed frequency time intervals), as is realistic
for many physical systems. We leverage recent results [7], [8] to
certify the existence of a compact subset of the state space that can be
rendered forward-invariant through control. These results are readily
applicable to feedback linearizable control afﬁne systems, allowing
us to design CR-MPOPs for a wide array of complex systems.

Submitted May 15th, 2021. Resubmitted July 6th, 2022. This work
was supported in part by DARPA and Beyond Limits. Victor D. Dorobantu
was also supported in part by a Kortschak Fellowship.

Victor D. Dorobantu and Yisong Yue are with the Department of Com-
puting and Mathematical Sciences, California Institute of Technology,
Pasadena, CA 91125 USA. (e-mails: {vdoroban, yyue}@caltech.edu).
Yisong Yue is afﬁliated with Argo AI, Pittsburgh, PA. Kamyar Azizzade-
nesheli is with the Department of Computer Science, Purdue University,
West Lafayette, IN 47907 USA (e-mail: kamyar@purdue.edu).

Many frameworks extend the theory of discrete MDPs to general
state and action spaces. Most relevant for our work, [1] develops
semicontinuous MDPs with Borel measurable policy classes, [9]
and [10] develop results for analyticially and universally measurable
policy classes, respectively, and [11] reﬁnes conditions for the well-
posedness of value iteration. For a more complete summary, see [12].
Our work focuses on developing classes of policy optimization prob-
lems that can naturally connect theoretical results from reinforcement
learning with nonlinear continuous-time control, and can be directly
translated to checking certain properties in control systems.

Methods that maintain the forward-invariance of subsets of the
state space of interest have been well-studied [13]–[15]. These and
related methods have recently found applications in safe reinforce-
learning [16]–[19]. Physical systems and robotic platforms
ment
have been popular applications for reinforcement learning methods
recently, with the majority of methods employing function approx-
imation, discretizations (such as ﬁxed or adaptive gridding or state
aggregation), or direct policy search [20]–[22]. While policy search
methods generally have no guarantees of global optimality, they
have received much attention due to the scalability problems of
dynamic programming methods (especially after discretization) and
convergence problems of approximate dynamic programming (for a
more complete discussion of the relative merits of these methods,
see [20, §2.3]). In contrast, our goal is to identify settings that are
compatible with value iteration and to use control theoretic tools to
guide solution methods so we can expect good performance.

A. Contributions

We develop our results in the following phases: 1) We describe a
generic property (Assumption 1) that admits the well-posedness of
value iteration for MPOPs (Theorem 1). 2) We leverage the forward-
invariance of compact sets to design MPOPs and policy classes that
satisfy Assumption 1, allowing us to prove well-posedness of a large
class of policy optimization problems (Theorem 2). Such problem
settings are called CR-MPOPs. 3) We apply our results on CR-
MPOPs to analyze control afﬁne systems with time-sampled control
inputs. 4) We further apply our results to analyze robotic systems with
time-sampled control inputs, which comprise a large class of control
afﬁne systems. These results translate the generic requirements for
well-posedness to concrete requirements on the robotic system. 5) We
ﬁnally show that full-state feedback linearizable control afﬁne sys-
tems can render a compact subset of the state space forward invariant
with continuous controllers (thus satisfying conditions of Theorem 2),
demonstrating practically relevant instances of CR-MPOPs.

B. Notation, Conventions, and Deﬁnitions

We consider nonempty metric spaces throughout this paper and
endow each such space with the σ-algebra generated by its topology,
called the Borel σ-algebra. In this case, open sets and closed sets are
measurable sets and continuous functions are measurable functions.
A metric space is separable if it contains a countable dense subset.
Finite or countable sets can be regarded as separable metric spaces
when equipped with the discrete metric [23, §3.A]. For nonempty
metric spaces X and Y , the set of measurable functions from X to Y
is denoted L0(X; Y ). A bounded measurable function f : X → R is

 
 
 
 
 
 
2

GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

upper semicontinuous if for any c ∈ R, the preimage f −1([c, ∞)) =
{x ∈ X : f (x) ≥ c} is closed.

The set of all subsets of Y is the powerset of Y , denoted P(Y ).
A set-valued function C : X → P(Y ) is called a correspondence.
The graph of C is deﬁned as:

Graph(C) = {(x, y) ∈ X × Y : y ∈ C(x)}.

(1)

If rπ is bounded, then Sπ = S. If γ > 0, then fπ(Sπ) ⊆ Sπ, as
a convergent series still converges after the ﬁrst term is removed. If
Sπ (cid:54)= ∅, then the corresponding value function Vπ : Sπ → R is
deﬁned explicitly and implicitly, for all states s ∈ Sπ, as:

Vπ(s) =

∞
(cid:88)

t=0

γtrπ(f t

π(s)) = rπ(s) + γVπ(fπ(s)).

(6)

If C(x) (cid:54)= ∅ for all x ∈ X, then a selector of C is a function
f : X → Y satisfying f (x) ∈ C(x) for all x ∈ X. For any set
B ⊆ Y , the lower preimage of B under C is deﬁned as:

A set of policies Π admits a partial order if Sπ = S for all policies
π ∈ Π. In this case, for policies π, π(cid:48) ∈ Π, if Vπ(s) ≥ Vπ(cid:48) (s) for
all states s ∈ S, then π (cid:23) π(cid:48).

C(cid:96)(B) = {x ∈ X : C(x) ∩ B (cid:54)= ∅}.

(2)

The correspondence C is upper hemicontinuous1 if the lower preim-
age of each closed set is a closed set [24, Lemma 17.4].

A nonempty subset X0 ⊆ X is a metric space when equipped
with the restriction of the metric on X to X0 × X0. For any open set
U ⊆ X0, there is an open set V ⊆ X such that U = X0 ∩V . For any
continuous function f : X → Y , the restriction f |X0
: X0 → Y
is continuous. Similarly, for any measurable set A ⊆ X0, there is
a measurable set B ⊆ X such that A = X0 ∩ B, and for any
measurable function f : X → Y , the restriction f |X0
is measurable.
Consider a nonempty metric space Z and a function f : X × Y →
Z. For x ∈ X and y ∈ Y , the x-section fx : Y → Z and the
y-section f y : X → Z are deﬁned as:
fx(y(cid:48)) = f (x, y(cid:48)),

f y(x(cid:48)) = f (x(cid:48), y),

(3)

for all x(cid:48) ∈ X and y(cid:48) ∈ Y . If f is continuous,
then f has
continuous sections, and if f is measurable, then f has measurable
sections. We also deﬁne sections for functions on graphs. Consider a
correspondence C : X → P(Y ) with C(x) (cid:54)= ∅ for all x ∈ X, and
a function f : Graph(C) → Z. For x ∈ X, we deﬁne the x-section
fx : C(x) → Z as above.

N denotes the natural numbers, Z+ denotes the nonnegative
integers, R+ and R++ denote nonnegative and positive reals, and
+ and Sd
Sd
++ denote d×d positive semideﬁnite and deﬁnite matrices.

II. METRIC POLICY OPTIMIZATION PROBLEMS

In this paper, a deterministic Markov decision process (MDP) will
be characterized by a tuple (S, A, C, f, r, γ), with state space S,
action space A, action-admissibility correspondence C : S → P(A)
satisfying C(s) (cid:54)= ∅ for each state s ∈ S, transition map f :
Graph(C) → S, reward function r : Graph(C) → R, and discount
factor γ ∈ [0, 1). Note while r may be unbounded, r cannot assume
the values ±∞. We refer to an MDP as a metric MDP if S and A
are nonempty separable metric spaces and f and r are measurable
functions. In this paper, we focus solely on metric MDPs, and often
refer to them simply as MDPs for brevity.

We will limit our consideration to deterministic, Markovian, and
stationary (time-invariant) policies. In this case, a policy π : S → A
is a selector of C; that is, π(s) ∈ C(s) for all states s ∈ S. The
corresponding closed-loop transition map fπ : S → S and single-
step reward function rπ : S → R are deﬁned as:

fπ(s) = f (s, π(s)),

rπ(s) = r(s, π(s)).

(4)

for all states s ∈ S. For any t ∈ Z+, let f t
t-iterated composition of fπ. Deﬁne the subset Sπ ⊆ S as:

π : S → S denote the

(cid:40)

Sπ =

s ∈ S :

∞
(cid:88)

t=0

γtrπ(f t

π(s)) converges absolutely

.

(5)

(cid:41)

Deﬁnition 1 (Metric Policy Optimization Problem). We refer to the
pair of an MDP (S, A, C, f, r, γ) and a policy class Π admitting a
partial order as a metric policy optimization problem (MPOP). We
call an MPOP well-posed if there is an optimal policy π∗ ∈ Π with
π∗ (cid:23) π for all policies π ∈ Π.

Goals of this section. Our goal is to analyze well-posedness of
policy optimization for MDPs. We ﬁrst describe sufﬁcient conditions
for well-posedness of value iteration (Theorem 1). We then establish
conditions under which ill-posed MPOPs can be restricted to well-
posed problems by only considering policies that render the same
subset of the state space forward-invariant (Theorem 2), resulting in
CR-MPOPs. We will show how to apply our results on value iteration
to control afﬁne systems in Section III.

Throughout this section, we refer to and modify the following

example to ground our presentation:
Example 1. Consider the state space S = R, action space A =
R, and the constant action-admissibility correspondence C = A.
Additionally, consider the transition function f : Graph(C) → S
and reward function r : Graph(C) → R deﬁned as:

f (s, a) = s + tanh (a),

r(s, a) = −s2 − (tanh (a))2,

(7)

for all state-action pairs (s, a) ∈ Graph(C), and a discount factor
γ ∈ [0, 1). Note that |f (s, a)| ≤ |s| + | tanh (a)| ≤ |s| + 1 for all
state-action pairs (s, a) ∈ Graph(C). Consider any policy π : S →
A. By induction, we have |f t
π(s)| ≤ |s| + t for all states s ∈ S and
t ∈ Z+. For any state s ∈ S and T ∈ Z+, we have:
∞
(cid:88)

T
(cid:88)

γt((f t

π(s))2 + (tanh (π(f t

π(s))))2)

π(s)) ≥ −

γtrπ(f t

t=0

t=0
∞
(cid:88)

γt(s2 + 2|s|t + t2 + 1)

t=0
s2 + 1
1 − γ

−

2γ|s|
(1 − γ)2 −

γ(γ + 1)
(1 − γ)3 .

(8)

≥ −

= −

The sequence of partial sums as T → ∞ is monotone and bounded
below, so Vπ(s) is well-deﬁned. As s was arbitrary, we have Sπ = S.

A. Value Iteration

Consider a policy class Π with Sπ = S for each policy π ∈ Π and
supπ∈Π Vπ(s) < ∞ for each state s ∈ S. Accordingly, we deﬁne
the optimal (with respect to Π) value function V ∗ : S → R as:
V ∗(s) = sup
π∈Π
for all states s ∈ S. If a policy π∗ ∈ Π satisﬁes Vπ∗ = V ∗, then π∗
is optimal and the policy optimization problem is well-posed.

Vπ(s),

Value iteration generates a sequence of approximations of V ∗.
Given an initial guess V0 : S → R, we seek a sequence of real-
valued functions {Vn : n ∈ N} satisfying:

(9)

1Some authors call such correspondences upper semicontinuous; we use

hemicontinuous to distinguish correspondences from real-valued functions.

Vn+1(s) = sup

{r(s, a) + γVn(f (s, a))},

(10)

a∈C(s)

DOROBANTU et al.: COMPACTLY RESTRICTABLE METRIC POLICY OPTIMIZATION PROBLEMS

3

for all states s ∈ S and n ∈ Z+.

We now describe conditions under which MPOPs are well-posed
and can be solved with value iteration (Theorem 1). When assump-
tions are strengthened by requiring S and A to be Polish spaces,
we will make use of [11, Theorem 4.1] (the setting of this theorem
is called a semicontinuous model in [1, §6.7]). We provide a more
direct proof of well-posedness without these additional assumptions
in the appendix.

Assumption 1. The action admissibility correspondence C has
compact values and is upper hemicontinuous, the transition function
f is continuous, the reward function r is upper semicontinuous and
bounded, and the policy class Π is the set of all measurable selectors
of C. Moreover, for each state-action pair (s, a) ∈ Graph(C), there
is a corresponding policy πs,a ∈ Π satisfying πs,a(s) = a.

Remark 1. If the projection of Graph(C) onto the action space is
compact, then the assumption that for any state-action pair (s, a) ∈
Graph(C), there exists a policy πs,a ∈ Π with πs,a(s) = a can be
removed. See Lemma 1.

Remark 2 (Tabular MDP). If S and A are ﬁnite sets equipped with
discrete metrics and Π is the set of all selectors of C, then these
assumptions are immediately met. This follows since the discrete
metric renders all sets are open, closed, and compact, and all functions
deﬁned on such sets are continuous (and bounded if real-valued).

Example 2. Consider the MDP (S, A, C, f, r, γ) from Example 1.
While C is upper hemicontinuous, it is not compact-valued. While
f and r are continuous, r is unbounded. Consider the constant
correspondence C(cid:48) = [−1, 1], and the reward r(cid:48) : Graph(C(cid:48)) → R:

r(cid:48)(s) = e−s2

− (tanh (a))2,

(11)

state-action pairs

(s, a) ∈ Graph(C(cid:48)). Note that
for all
Graph(C(cid:48)) = S×[−1, 1], so r(cid:48) is bounded. Since f is continuous, its
restriction to Graph(C(cid:48)) is as well. With Π the set of all measurable
selectors of C(cid:48), note that for every pair (s, a) ∈ Graph(C(cid:48)), the
constant policy πs,a = a is a measurable selector of C(cid:48), so πs,a ∈ Π.
Thus, the MPOP deﬁned by the MDP (S, A, C(cid:48), f |Graph(C(cid:48)) , r(cid:48), γ)
and policy class Π satisﬁes Assumption 1.

We now demonstrate well-posedness under Assumption 1.

Theorem 1. Consider an MPOP characterized by an MDP
(S, A, C, f, r, γ) and a policy class Π that satisﬁes Assumption 1.
There is an optimal policy π∗ ∈ Π satisfying Vπ∗ = V ∗, and V ∗
is the limit of value iteration when the initial guess is bounded and
upper semicontinuous.

Proof. If S and A are Polish spaces, we can show that the MDP in
question satisﬁes the three conditions of Assumptions (W*) of [11],
in which case, there is an optimal policy and V ∗ is the limit of value
iteration when the initial condition is the zero function (a conclusion
of [11, Theorem 4.1]). The ﬁrst condition follows since r is upper
semicontinuous and bounded above (as it is bounded both above and
below), which corresponds to a lower semicontinuous cost function
that is bounded below. The second condition is satisﬁed by the upper
hemicontinuity of C (using the sequential deﬁnition in [24, Theorem
17.20]) since r is also bounded below. The third condition is satisﬁed
since the MDP is deterministic (for which transition probabilities are
represented by Dirac measures). In this case, we require:

lim
n→∞

ϕ(f (sn, an)) = ϕ(f (s, a)),

(12)

for any bounded continuous function ϕ : S → R and any convergent
sequence {(sn, an) ∈ Graph(C) : n ∈ N} converging to (s, a) ∈

Graph(C); equality follows since ϕ ◦ f is continuous (as the
composition of continuous functions).

If S and A are not both Polish spaces, we prove well-posedness in
the appendix, in which case, V ∗ is the limit of value iteration when
the initial guess is any bounded and upper semicontinuous function.

We conclude with a lemma mentioned in Remark 1, allowing

Assumption 1 to be relaxed slightly.

Lemma 1. Suppose an MDP (S, A, C, f, r, γ) and policy class Π
satisfy Assumption 1, suppose the projection of Graph(C) onto A
is compact, and let ρA : A × A → R+ denote the metric on A.
For any state-action pair (s, a) ∈ Graph(C), the function ϕs,a :
Graph(C) → R deﬁned as:

ϕs,a(s(cid:48), a(cid:48)) = −ρA(a(cid:48), a),
for all state-action pairs (s(cid:48), a(cid:48)) ∈ Graph(C) admits a maximizing
policy πs,a ∈ Π for which πs,a(s) = a.

(13)

Proof. Since ρA is continuous, so is ϕs,a. Since the projection of
Graph(C) onto A is compact, ϕs,a is bounded. If S and A are Polish
spaces, then the proof of [11, Lemma 3.4] uses the Arsenin-Kunugui
theorem to show the existence of a policy πs,a ∈ Π satisfying:

ϕs,a(s(cid:48), πs,a(s(cid:48))) = max

a(cid:48)∈C(s(cid:48))

ϕs,a(s(cid:48), a(cid:48))

= − min

a(cid:48)∈C(s(cid:48))

ρA(a(cid:48), a),

(14)

for all states s(cid:48) ∈ S. In particular, πs,a(s) = a as ϕs,a(s, πs,a(s)) =
− mina(cid:48)∈C(s) ρA(a(cid:48), a) = 0. This requires Assumptions (W*) of
[11] to be met, which is veriﬁed in the proof of Theorem 1.

We arrive at the same conclusion when S and A are not both

Polish spaces in the appendix.

B. Compactly Restrictable MPOPs

While Assumption 1 allows us to determine when MPOPs are
well-posed and can be solved using value iteration, it is not met for
many systems of interest. We now outline an alternative assumption
that not only mitigates theoretical shortcomings, but also better ﬁts
the problem settings for physical systems. Our main result here is
Theorem 2, showing how an MPOP satisfying these new assumptions
can generate an MPOP satisfying Assumption 1 through a systematic
transformation. We call the original MPOP a CR-MPOP.

1) Motivation: Consider

an MPOP deﬁned by an MDP
(S, A, C, f, r, γ) and a policy class Π that satisﬁes Assumption 1.
The proof of Theorem 1 requires a bounded reward function r and
a compact-valued action-admissibility correspondence C. For many
examples, these requirements are too strict, as shown in Example
2. If C can be modiﬁed such that its graph is compact and if r is
continuous, then r will be bounded over the graph of the modiﬁed
correspondence. This guides our systematic construction of a well-
posed policy optimization problem from an ill-posed one.

the
If the graph of the modiﬁed correspondence is compact,
projection of the graph onto the state space must also be compact.
Therefore, if the state space is not already compact, then the modiﬁed
correspondence must be deﬁned on a compact strict subset S0 ⊂ S;
that is, the modiﬁed correspondence must have the form C0 : S0 →
P(A), where C0(s) ⊆ C(s) is nonempty and compact for all states
s ∈ S0. Moreover, while restricting admissible actions to compact
sets is reasonable for physical systems (as forces, voltages, currents,
etc. are bounded by physical constraints), these restrictions must
be chosen carefully to enforce the nonemptiness condition. For the
restriction of f to Graph(C0) to be a well-deﬁned transition function,

4

GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

we require f (s, a) ∈ S0 for all pairs (s, a) ∈ Graph(C0). Finally,
we must ensure that C0 admits measurable selectors.

Restricting our attention to a compact subset S0 is also reasonable
for many physical systems, as dynamics models are often well-
understood only in a bounded set around an operating condition.

2) Compact Restriction: For this construction, we use policies
that render compact subsets of the state space forward-invariant. A
policy π ∈ Π renders a subset S0 ⊆ S forward-invariant if fπ(S0) ⊆
S0. By induction, we have f t

π(S0) ⊆ S0 for any t ∈ Z+.

Deﬁnition 2 (Compactly Restrictable Metric Policy Optimization
Problem). An MPOP characterized by an MDP (S, A, C, f, r, γ) and
a policy class Π is a compactly restrictable metric policy optimization
problem (CR-MPOP) if there is a nonempty compact subset S0 ⊆ S
and a continuous policy π0 ∈ Π rendering S0 forward-invariant.

Assumption 2. The action-admissibility correspondence C has
closed values and Graph(C) is closed, the transition function f and
reward function r are continuous, and the policy class Π is the set
of all measurable selectors of C.

To show how an CR-MPOP satisfying Assumption 2 can lead to

an MPOP satisfying Assumption 1, we need the following lemma.

Lemma 2. Consider an MDP (S, A, C, f, r, γ) and a policy class
Π satisfying Assumption 2. For any state s ∈ S, the s-section fs :
C(s) → S is continuous. Additionally, for any state s ∈ S and any
closed set G ⊆ S, the preimage (fs)−1(G) is a closed subset of A.

Proof. Let id : S ×A → S ×A denote the identity function on S ×A.
For any state s ∈ S, the s-section ids : A → S × A is continuous,
meaning the restriction ids|C(s) : C(s) → S × A is continuous. We
can write fs = f ◦ ( ids|C(s)), implying fs is continuous. For any
closed set G ⊆ S, there is a corresponding closed set Fs,G ⊆ A
satisfying (fs)−1(G) = C(s) ∩ Fs,G. Since C(s) is also a closed
subset of A, we conclude that (fs)−1(G) a closed subset of A.

We now present our guarantee of the existence of a well-posed

value iteration settings under Assumption 2.

Theorem 2. Consider a CR-MPOP characterized by an MDP
(S, A, C, f, r, γ) and a policy class Π that satisﬁes Assump-
tion 2. There exists an MPOP characterized by an MDP
(S0, A0, C0, f0, r0, γ) and a policy class Π0 satisfying Assumption
1, with S0 ⊆ S rendered forward-invariant by a continuous policy
in Π, A0 ⊆ A, C0 : S0 → P(A0) satisfying C0(s) ⊆ C(s) for all
states s ∈ S0, f0 = f |Graph(C0), r0 = r|Graph(C0), and:
: π ∈ Π, π(S0) ⊆ A0, and fπ(S0) ⊆ S0}.

(15)
Proof. For some n ∈ N, consider continuous policies π1, . . . , πn ∈
Π satisfying fπi (S0) ⊆ S0 for all i ∈ {1, . . . , n}. Such policies are
guaranteed to exist by the existence of the continuous policy π0 ∈ Π.
Consider the union of the images of S0 under each of these policies,
deﬁning the compact set A0 = (cid:83)n
i=1 πi(S0). Since S and A are
separable metric spaces, so are S0 and A0 [24, Corollary 3.5].

Π0 = { π|S0

Deﬁne the correspondence C0 : S0 → P(A0) as:
C0(s) = {a ∈ C(s) ∩ A0 : f (s, a) ∈ S0} = (fs)−1(S0) ∩ A0,

(16)
for all states s ∈ S0. Since π1(s), . . . , πn(s) ∈ C0(s), C0(s) (cid:54)= ∅
for all s ∈ S0. By Lemma 2 and since S0 is closed (it is compact in
a metric space), the preimage (fs)−1(S0) is a closed subset of A.
C0(s) is thus compact, as it is a closed subset of A0. For any closed
set G ⊆ A0, the lower preimage of G under C0 satisﬁes:

(C0)(cid:96)(G) = {s ∈ S0 : f (s, a) ∈ S0 for some a ∈ C(s) ∩ G}

= pS (f −1(S0) ∩ (S0 × G)),

(17)

where pS : S × A → S denotes the canonical projection onto S.
Since S0 is closed and f is continuous, there is a closed set F ⊆
S × A satisfying f −1(S0) = Graph(C) ∩ F . Since Graph(C) is
also closed, f −1(S0) is a closed subset of S ×A. Since G is a closed
subset of the compact set A0, G is compact, and since S0 is compact,
the product S0 × G is compact. As a closed subset of a compact set,
f −1(S0) ∩ (S0 × G) is compact. Since the projection operator pS is
continuous, (C0)(cid:96)(G) is compact. Therefore, (C0)(cid:96)(G) is a closed
subset of S0, and since G was arbitrary, C0 is upper hemicontinuous.
As restrictions of continuous functions, f0 and r0 are continuous,

implying r0 is upper semicontinuous. Note that:

Graph(C0) = {(s, a) ∈ S0 × A0 : f (s, a) ∈ S0}
= f −1(S0) ∩ (S0 × A0).

(18)

Since A0 is trivially a closed subset of A0, we have already shown
that f −1(S0) ∩ (S0 × A0) is compact, meaning Graph(C0) is
compact. Therefore, since r0 is continuous, it is bounded.

The policy class Π0 satisﬁes:

Π0 = { π|S0
= {π ∈ L0(S0; A0) : π(s) ∈ C0(s) for all states s ∈ S0}. (19)

: π ∈ Π, π(s) ∈ C0(s) for all states s ∈ S0}

To verify the last equality, ﬁrst consider a policy π ∈ Π satisfying
π(s) ∈ C0(s) for all states s ∈ S0. The restriction π|S0
is a
measurable function from S0 to A0 selecting from C0. Conversely,
consider a measurable function π : S0 → A0 selecting from C0.
Pick any policy πe ∈ Π and deﬁne the extension ¯π : S → A as π
on S0 and πe on S \ S0. For any measurable set B ⊆ A, we have:
¯π−1(B) = {s ∈ S0 : π(s) ∈ A0 ∩ B} ∪ {s ∈ S \ S0 : πe(s) ∈ B}
= π−1(A0 ∩ B) ∪ (π−1
(20)

e (B) \ S0),

which is a measurable set; therefore, ¯π is a measurable function.
Note that ¯π(s) = π(s) ∈ C0(s) ⊆ C(s) for all states s ∈ S0 and
¯π(s) = πe(s) ∈ C(s) for all states s /∈ S0. Since ¯π is a measurable
= π.
selector of C, we have ¯π ∈ Π, and inclusion follows since ¯π|S0
Finally, since Graph(C0) is compact, its projection onto A0 is
compact. Therefore, by Lemma 1, for every state-action pair (s, a) ∈
Graph(C0), there is a policy πs,a ∈ Π0 satisfying πs,a(s) = a.

Remark 3. An MPOP characterized by an MDP (S, A, C, f, r, γ)
and a policy class Π that satisﬁes Assumption 1 is trivially a CR-
MPOP satisfying Assumption 2 if the following sufﬁcient conditions
are met: 1) The state space S is compact, 2) The reward function r
is continuous, 3) The policy class Π contains a continuous policy.

Example 3. Consider the MDP (S, A, C, f, r, γ) deﬁned in Example
1 with a policy class Π comprised of measurable selectors of C, and
let S0 = [−1, 1]. Note that |f (0, a)| = | tanh (a)| < 1 for all actions
a ∈ R. Consider any state s ∈ [−1, 0). For any nonnegative action
a ∈ R+, we have −1 ≤ s ≤ f (s, a) = s + tanh (a) < s + 1 < 1,
and f (s, −a) = s + tanh (−a) ∈ [−1, s) if and only if −a ∈
[tanh−1 (−1 − s), 0) since tanh−1 is monotonically increasing. By
a similar argument for states in (0, 1], we determine that, for all
s ∈ S0 and a ∈ A, we have f (s, a) ∈ S0 if and only if:
[tanh−1 (−1 − s), ∞) −1 ≤ s < 0,
A
(−∞, tanh−1 (1 − s)] 0 < s ≤ 1.

s = 0,




(21)

a ∈



A correspondence coinciding with the requirements on actions
in (21) would not have compact values; this is illustrated by the
preimage f −1(S0) in Figure 1. Therefore, consider the continuous
policy π0 ∈ Π deﬁned as π0(s) = −s for all states s ∈ S. For any
state s ∈ [−1, 0), we have π0(s) = −s > 0 ≥ tanh−1(−1 − s).

DOROBANTU et al.: COMPACTLY RESTRICTABLE METRIC POLICY OPTIMIZATION PROBLEMS

5

including (projected) gradient ascent from several initial action seeds
or maximization over a ﬁnite but sufﬁciently dense sampling of
admissible actions. If the function approximation and optimization
approximation errors can be controlled, approximate convergence of
value iteration can be guaranteed [27, Proposition 2.3.2].

Any approximate optimization approach requires checking ad-
missibility of actions, which, in the case of a CR-MPOP, requires
checking whether or not a state-action pair is mapped to the correct
compact set under the transition map. This necessitates efﬁcient set
membership checking, and for nonconvex compact sets, proximity-
based membership approximations may be needed.

Finally, in Section III-C, we will use the closure of a reachable
set (under a speciﬁc policy) to restrict problems with feedback
linearizable control afﬁne systems to well-posed MPOPs. To sample
a state from the reachable set, we can sample a state from the
appropriate set of initial conditions and follow the policy for a
number of steps sampled from a geometric distribution (with success
probability 1 − γ). The resulting distribution is the discounted state
distribution under the policy [28], which is supported on the entirety
of the reachable set (thus, on a dense subset of its closure).

There are many possible combinations of computational ap-
proaches that may be suitable for well-posed MPOPs; we deem the
precise best choice outside the scope of this theoretical framing.

III. CONTROL AFFINE SYSTEMS
We now show how to apply the results from Section II to a general
class of control systems called control afﬁne systems [29, §13.2.3].
For state and action space dimensions d, m ∈ N, respectively,
consider a set D ⊆ Rd and vector ﬁelds f0, g1, . . . , gm : D → Rd.
Deﬁne the matrix-valued function G : D → Rd×m with columns
g1, . . . , gm. Deﬁne F : D × Rm → Rd as:

F (x, u) = f0(x) + G(x)u,
(25)
for all x ∈ D and u = (u1, . . . , um) ∈ Rm. An initial value problem
with constant control input u ∈ Rm is characterized by an initial
condition x ∈ D and an open time interval I ⊆ R with 0 ∈ I;
a corresponding solution is a differentiable function φ : I → D
satisfying φ(0) = x and ˙φ(t) = d
dt φ(t) = F (φ(t), u) = f0(φ(t)) +
G(φ(t))u for all times t ∈ I.

(Left) The preimage f −1(S0) is shown in blue, which contains
Fig. 1.
all state-action pairs mapped into S0 = [−1, 1] by f . For any state in
S0, the set of actions mapping that state into S0 is unbounded, therefore
not compact. (Right) The graph of π0 is shown in orange and the graph
of C0 is shown in blue. The graph of C0 is compact, and for every state
s ∈ S0, the set C0(s) is compact.

Similarly, for any state s ∈ (0, 1], we have π0(s) < tanh−1(1 − s).
Therefore, fπ0 (S0) ⊆ S0. The image of S0 under π0 is [−1, 1].
Therefore, we can deﬁne A0 = [−1, 1] and C0 : S0 → P(A0) as,
for all states s ∈ S0:

C0(s) =






[tanh−1(−1 − s), 1]
[−1, tanh−1(1 − s)]
[−1, 1]

s ≤ −1 − tanh (−1),
s ≥ 1 − tanh (1),
otherwise.

(22)

C. Policy Iteration

We brieﬂy mention the difﬁculties of policy iteration for MPOPs,
as noted in [12]. For a well-posed MPOP, policy iteration generates
a monotonically nondecreasing sequence of policies. Given an initial
policy π0 ∈ Π, we seek a sequence of policies {πn ∈ Π : n ∈ N}
satisfying, for all states s ∈ S and n ∈ Z+:

r(s, πn+1(s)) + γVπn (f (s, πn+1(s)))

= sup

a∈C(s)

{r(s, a) + γVπn (f (s, a))}.

(23)

If f , r, and π0 are continuous, then so are fπ0 and rπ0 which we can
show renders Vπ0 continuous. This implies r+γVπ0 ◦f is continuous.
If r is bounded, then so are Vπ0 and r +γVπ0 ◦f . Since r +γVπ0 ◦f
is upper semicontinuous and bounded, we can show (using [11] or
the appendix) that for any state s ∈ S, the optimization problem:

sup
a∈C(s)

{r(s, a) + γVπ0 (f (s, a))},

(24)

A. Time Sampling

is solved by the next policy π1, a measurable selector of C. However,
we cannot conclude that r + γVπ1 ◦ f is upper semicontinuous and
bounded, so the same argument cannot be applied iteratively.

Demonstrating that policy iteration can be applied requires special
knowledge that the policy improvement step in (23) continues to
produce policies with value functions that permit maximization and
appropriate selection (for the chosen policy class). Such knowledge
is available in linear-quadratic regulator (LQR) problems with linear
policy classes (see [25] for the discounted case).

D. Computation

There are 3 central difﬁculties in computationally implementing
value iteration for a well-posed MPOP: 1) Representing iterates
during value iteration, 2) Determining admissible actions, and 3) Ap-
proximating the update step (10). Typically, iterates are represented
using function approximators such as neural networks [5] or Gaussian
process regression models [26]. Training such approximators involves
sampling sufﬁciently many states from the state space region of
interest. For any given state, the maximization in the update step (10)
is generally a nonconvex optimization, with possible approximations

In contrast

to continuous-time control design, we consider
in which initial value
sampled-data control design (see [6]–[8]),
problems with constant control characterize the evolution of a system
over ﬁxed sample intervals, resulting in control input trajectories that
are piecewise constant in time. Such assumptions are realistic for
physical systems interacting with digital controllers, which measure
states and compute control inputs at nearly ﬁxed frequencies. This
setting requires the time intervals over which solutions are deﬁned to
be sufﬁciently long, uniformly for all initial conditions and control
inputs under consideration.

Speciﬁcally, ﬁx a sample period h ∈ R++, and deﬁne the subset
Sh ⊆ D and correspondence Ch : Sh → P(Rm) such that the
following properties are satisﬁed:

• For every initial condition x ∈ Sh, the corresponding set of

control inputs Ch(x) ⊆ Rm is nonempty,

• For every (x, u) ∈ Graph(Ch), there is a unique solution to
the initial value problem characterized by initial condition x,
control input u, and open time interval I ⊆ R with [0, h] ⊂ I,
with the solution denoted φx,u : I → D,

• Sh and Ch are maximal (cannot be contained in a superset and

containing correspondence)

−2−1012S−2−1012A−2−1012S−2−1012A6

GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

Deﬁne fh : Graph(Ch) → D for all pairs (x, u) ∈ Graph(Ch) as:

fh(x, u) = φx,u(h).
(26)
Example 4 (Linear System). Let D = Rd. For matrices A ∈ Rd×d
and B ∈ Rd×m, suppose f0(x) = Ax and G(x) = B for all initial
conditions x ∈ Rd. Then Sh = Rd and Ch(x) = Rm for all x ∈ Rd.
Moreover, fh can be expressed explicitly as a linear function in terms
of the matrix exponential of A.

Example 5 (Continuously Differentiable and Lipschitz Continuous
Vector Fields). Suppose D = Rd and f0, g1, . . . , gm are continu-
ously differentiable and (globally) Lipschitz continuous. For a control
input u ∈ Rm, since the u-section F u : Rd → Rd is a linear
combination of continuously differentiable and Lipschitz continuous
vector ﬁelds, it is continuously differentiable and Lipschitz contin-
uous. Therefore, for any initial condition x ∈ Rd the initial value
problem characterized by initial condition x, control input u, and
time interval R has a unique solution [30, Theorem 3.1.3]. Therefore,
Sh = Rd and Ch(x) = Rm for all initial conditions x ∈ Rd. In
general, fh does not have an closed-form representation.

Assumption 3. The vector ﬁelds f0, g1, . . . , gm are locally Lipschitz
continuous. Moreover, there exist subsets S ⊆ Sh and A ⊆ Rm and
a correspondence C : S → P(A) such that:

• For every initial condition x ∈ S, the set of control inputs

C(x) ⊆ A is nonempty with C(x) ⊆ Ch(x),

• Graph(C) is a closed subset of S × A,
• fh(Graph(C)) ⊆ S.

Both Examples 4 and 5 satisfy Assumption 3 with S = Sh,
A = Rm, and C(x) = A for all x ∈ S. The assumption of local
Lipschitz continuity in Assumption 3 ensures that the restriction of
fh to Graph(C) is continuous; this follows from [31, Theorem 3.5]
by treating control inputs as parameters.

B. Robotic Systems

Many robotic systems can be modeled as control afﬁne systems,
and we will demonstrate how to show that such systems can satisfy
Assumption 2, establishing well-posedness of value iteration for a
large class of practically relevant MPOPs. Examples of such systems
include manipulators, automobiles, aircraft, and spacecraft [32], [33].
1) Dynamics: An unconstrained robotic system with n ∈ N
degrees of freedom is characterized by an n-dimensional C2 manifold
Q called the conﬁguration manifold. In this work, we consider open
subsets Q ⊆ Rn for which we can identify the tangent bundle
T Q with Q × Rn. The inertia matrix function D : Q → Sn
++
characterizes the kinetic energy function T : Q × Rn → R++,
2 ˙q(cid:62)D(q) ˙q for all conﬁgurations q ∈ Q and
deﬁned as T (q, ˙q) = 1
velocities ˙q ∈ Rn. The assumption that D takes positive deﬁnite
values ensures that no conﬁgurations exist that admit arbitrarily large
velocities without affecting the kinetic energy of the system. We
require D to be differentiable, allowing us to express the matrix-
valued function C : Q × Rn → Rn×n of Coriolis and centrifugal
terms2 as:

(C(q, ˙q))ij =

n
(cid:88)

k=1

(cid:18) ∂
∂qj

(D(q))ik −

1
2

∂
∂qi

(cid:19)

(D(q))jk

˙qk,

(27)

˙q =
for all conﬁgurations q = (q1, . . . , qn) ∈ Q, velocities
( ˙q1, . . . , ˙qn) ∈ Rn, and indices i, j ∈ {1, . . . , n} [32, Equation
4.21]. We also require the potential energy function U : Q → Rn
to be differentiable, and we denote external nonconservative forces

2The use of C for this term is standard, but we will not use it in the same

context as an action-admissibility correspondence.

and torques applied to the system with the vector-valued function
Fext : Q × Rn → Rn. Finally, if the system is controlled with
m ∈ N actuators, then B : Q → Rn×m denotes the actuation
matrix function, converting control inputs to forces and torques. With
d = 2n and D = Q × Rn, we deﬁne F : D × Rm → Rd as:
(cid:21)
˙q
D(q)−1(Fext(q, ˙q) − C(q, ˙q) ˙q − ∇U (q))

F (x, u) =

(cid:20)

(cid:20)

+

0n×m
D(q)−1B(q)

(cid:21)

u,

(28)

for all x = (q, ˙q) ∈ Q × Rn and u ∈ Rm.

2) Regularity: We now establish regularity conditions that enable
us to deﬁne a valid transition function that can be used in an
MDP. We ﬁrst establish sufﬁcient conditions such that every initial
condition and control input under consideration correspond to initial
value problems with unique solutions. We then establish sufﬁcient
conditions for these solutions to exist for all nonnegative time.
Assumption 4. The conﬁguration manifold satisﬁes Q = Rn.
The functions D, C, ∇U , Fext, and B are each locally Lipschitz
continuous. There is a strictly positive lower bound λmin such that
λminIn (cid:22) D(q) for all conﬁgurations q ∈ Rn. There is some
closed set G ⊆ Rm such that for any control input u ∈ G there
are corresponding constants c0 ∈ R+ and c1 ∈ R++ such that:

(cid:107)Fext(q, ˙q) − ∇U (q) + B(q)u(cid:107)2 ≤ c0 + c1(cid:107) ˙q(cid:107)2,

(29)

for all conﬁgurations q ∈ Rn and velocities ˙q ∈ Rn.

Remark 4. If the derivative of D is locally Lipschitz continuous,
then so is D itself as it is continuously differentiable, and so is C
as it is bilinear in the derivative of D and the velocities. If D is
twice continuously differentiable, then the derivative of D is locally
Lipschitz continuous, implying D and C are as well. If U is twice
continuously differentiable, then ∇U is locally Lipschitz continuous.
If B is bounded by some M ∈ R+ and there are

Remark 5.
constants d0 ∈ R+ and d1 ∈ R++ such that:

(cid:107)Fext(q, ˙q) − ∇U (q)(cid:107)2 ≤ d0 + d1(cid:107) ˙q(cid:107)2,
(30)
for all conﬁgurations q ∈ Rn and velocities ˙q ∈ Rn, then for any
u ∈ Rm, we have:

(cid:107)Fext(q, ˙q) − ∇U (q) + B(q)u(cid:107)2 ≤ d0 + M (cid:107)u(cid:107)2 + d1(cid:107) ˙q(cid:107)2, (31)
for all conﬁgurations q ∈ Rn and velocities ˙q ∈ Rn. Therefore,
choose c0 = d0 + M (cid:107)u(cid:107)2 and c1 = d1, as well as G = Rm.

The proof of [34, Theorem 9.8b] shows that matrix inversion is
locally Lipschitz continuous. Fixing a control input u ∈ G, the
u-section F u is locally Lipschitz continuous as it is comprised of
sums and products of locally Lipschitz continuous functions. Fix an
initial conﬁguration q0 ∈ Rn and velocity ˙q0 ∈ Rn. There is a
corresponding maximal open interval Imax ⊆ R with 0 ∈ Imax
such that the initial value problem characterized by initial condition
(q0, ˙q0), control
input u, and time interval Imax has a unique
solution φ : Imax → R2n [30, Theorem 2.4.1] (the proof of this
theorem applies to locally Lipschitz continuous vector ﬁelds, not just
continuously differentiable vector ﬁelds). Let ψ : Imax → Rn satisfy
φ(t) = (ψ(t), ˙ψ(t)) for all times t ∈ Imax, and note that ψ(0) = q0
and ˙ψ(0) = ˙q0. Let I +
max = R+ ∩Imax. The kinetic energy satisﬁes:

T (ψ(t), ˙ψ(t)) − T (q0, ˙q0)

=

≤

(cid:90) t

0
(cid:90) t

0

˙ψ(s)(cid:62)(F (ψ(s), ˙ψ(s)) − ∇U (ψ(s)) + B(ψ(s))u) ds

(cid:107) ˙ψ(s)(cid:107)2(c0 + c1(cid:107) ˙ψ(s)(cid:107)2) ds,

(32)

DOROBANTU et al.: COMPACTLY RESTRICTABLE METRIC POLICY OPTIMIZATION PROBLEMS

7

(cid:112)

(

λmin(cid:107) ˙ψ(s)(cid:107)2) ds,

for all x = (q, ˙q) ∈ Rn × Rn and v ∈ Rn.

(33)

times t ∈ I +

for all
conﬁgurations q ∈ Rn and velocities ˙q ∈ Rn, we have:

max. Since T (q, ˙q) ≥ 1

2 λmin(cid:107) ˙q(cid:107)2

2 for all

(cid:16)(cid:112)

λmin(cid:107) ˙ψ(t)(cid:107)2
(cid:90) t

+

(cid:17)2

/2 ≤

(cid:16)(cid:112)2T (q0, ˙q0)

(cid:17)2

/2

c0 + c1(cid:107) ˙ψ(s)(cid:107)2
λmin

√

0
for all times t ∈ I +
max. For any upper bound tf ∈ I +
the functions φ1, φ2 : [0, tf ] → R+ deﬁned as:

max with tf > 0,

φ1(t) =

(cid:112)

λmin(cid:107) ˙ψ(t)(cid:107)2,

φ2(t) =

c0 + c1(cid:107) ˙ψ(t)(cid:107)2
λmin

√

,

(34)

for all t ∈ [0, tf ] are continuous and bounded, implying φ2 is
absolutely integrable. Therefore, from [35, Lemma 17] (originally
[36, Lemma A.5]), we have:

(cid:112)

λmin(cid:107) ˙ψ(t)(cid:107)2 ≤ (cid:112)2T (q0, ˙q0) +

(cid:90) t

0

c0 + c1(cid:107) ˙ψ(s)(cid:107)2
λmin

√

ds,

(35)

for all times t ∈ [0, tf ]. By the Gronwall-Bellman inequality [31,
Lemma A.1], we have:

(cid:107) ˙ψ(t)(cid:107)2 ≤ ((cid:112)2T (q0, ˙q0)/λmin + c0t/λmin)e

c1
λmin

t

,

(36)

for all times t ∈ [0, tf ]. Since tf was arbitrary, the bound (36) holds
for all times t ∈ I +

max.
We now use (36) to show that I +

max = R+. For contradiction,
assume there is some time tf ∈ R++ with tf /∈ I +
max. By upper
bounding the right-hand side of (36) by its value at the time tf , we
conclude that

˙ψ is bounded on I +

max. Also:

(cid:107)ψ(t) − q0(cid:107)2 ≤

(cid:90) t

0

(cid:107) ˙ψ(s)(cid:107)2 ds

c1
λmin

(cid:17)

≤ tf

(cid:16)(cid:112)2T (q0, ˙q0)/λmin + c0tf /λmin
max; that is, ψ is bounded on I +
for all times t ∈ I +
max. Therefore, φ
is bounded on I +
max, contradicting [30, Theorem 2.4.3] (again, the
proof of this theorem applies to locally Lipschitz continuous vector
ﬁelds). This implies I +
max = R+, meaning for any sample period
h ∈ R++, there is an open interval I ⊆ R with [0, h] ⊂ I.

tf ,

(37)

e

Thus, any robotic system satisfying Assumption 4 also satisﬁes

Assumption 3, no matter which sample period h is chosen.

C. Sampled-Data Control

We now determine sufﬁcient conditions for the existence of a
continuous policy rendering a compact subset of the state space
forward-invariant. This construction uses the methods of [7], [8],
based on the original work [6] in sampled-data control.

A control afﬁne system is full-state feedback linearizable if D is
open and there exist a function Φ : D → Rd that is a diffeomorphism
between D and an open subset of Rd, a continuous controller kfbl :
Rd × Rm → Rm that accepts an auxiliary input, and a controllable
pair (A, B) ∈ Rd×d × Rd×m such that:

∂Φ
∂x

F (x, kfbl(x, v)) = AΦ(x) + Bv,

(38)

for all x ∈ D and auxiliary control inputs v ∈ Rm.

Remark 6. Any robotic system satisfying Assumption 4 with n =
m and B(q) invertible for all conﬁgurations q ∈ Rn is full-state
feedback linearizable. The feedback linearizing controller kfbl : Rd ×
Rn → Rn is deﬁned as:
kfbl(x, v) = B(q)−1(C(q, ˙q) ˙q + ∇U (q) − Fext(q, ˙q) + D(q)v),
(39)

for all x = (q, ˙q) ∈ Rn × Rn and v ∈ Rn, and:

F (x, kfbl(x, v)) =

(cid:20)0n×n
0n×n

In
0n×n

(cid:21)

(cid:21) (cid:20)q
˙q

+

(cid:21)

(cid:20)0n×n
In

v,

(40)

Consider a full-state feedback linearizable control afﬁne system,
and suppose 0d ∈ D and Φ(0d) = 0d. Consider any gain matrix K ∈
Rm×d making A − BK Hurwitz stable with all eigenvalues having
strictly negative real parts. Suppose the system satisﬁes Assumption
3 and 0d ∈ int(S). Additionally, suppose π0 : S → Rm deﬁned as:

π0(x) = kfbl(x, −KΦ(x)),

(41)

for all states x ∈ S satisﬁes π0(x) ∈ C(x) for all states x ∈ S.
Since kfbl and Φ are continuous, π0 is continuous.

By [7, Lemmas 3, 4], there is a continuous (class KL) function
β : R+ × R+ → R+ with monotonically increasing s-sections βs
for all s ∈ R+ and monotonically nonincreasing r-sections βr for
all r ∈ R+, as well as some R ∈ R++ and a bounded open set
N ⊂ Rd with 0d ∈ N and:

{x ∈ Rd : (cid:107)x(cid:107)2 ≤ sup
x(cid:48)∈N

β((cid:107)x(cid:48)(cid:107)2, 0) + R} ⊆ S.

(42)

Additionally, for any R ≤ R, if the sample period is sufﬁciently
small, we have:

(cid:107)(fh)t

π0 (x)(cid:107)2 ≤ β((cid:107)x(cid:107)2, th) + R ≤ sup
x(cid:48)∈N

β((cid:107)x(cid:48)(cid:107)2, 0) + R, (43)

for all x ∈ N and t ∈ Z+. Let R ⊆ S denote the set of all states
reachable from N when following π0, deﬁned as R = {(fh)t
π0 (x) ∈
S : x ∈ N, t ∈ Z+}, and note that (fh)π0 (R) ⊆ R and cl(R) ⊆ S.
Since (fh)π0 (cl(R)) ⊆ cl(R) ⊆ S, we can set S0 = cl(R). The
set S0 ⊆ S is closed and bounded since:

sup
x∈N

β((cid:107)x(cid:107)2, 0) + R = β( sup
x∈N

(cid:107)x(cid:107)2, 0) + R < ∞.

(44)

Therefore, S0 is a compact set rendered forward-invariant by the
continuous policy π0, implying the system satisﬁes Assumption 2.

Finally, we can construct a compact subset of the state space in
a similar manner by appealing to [8, Theorem 3], employing the
notion of practical safety. In this case, if a compact 0-superlevel set
of a family of sampled-data control barrier functions [8, Deﬁnition
7] can be rendered forward-invariant when the transition map is
approximated by an appropriate Euler/Runge-Kutta scheme, then an
enlarged (but bounded) set can be rendered forward-invariant for the
exact transition map when the sample period is sufﬁciently small.
The enlargement can be controlled by decreasing the sample period.

IV. FUTURE WORK

There are many directions for future work. First, our results can be
viewed as “existence” results, and set the stage for studying compu-
tational aspects of policy optimization in metric MDPs. Second, our
results are applicable to inﬁnite-dimensional state and action spaces,
such as those encountered in the control of systems governed by
partial differential equations. This setting provides new challenges,
such as how to choose appropriate compact subsets of the state
space, as well as further computational considerations. Third, and
it would be interesting to also consider stochastic dynamics. Finally,
it is important to study the interaction between policy optimization
and model learning when a model must be derived from data.

8

GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

REFERENCES

[1] E. B. Dynkin and A. A. Yushkevich, Controlled markov processes.

Springer, 1979, vol. 235.

[2] M. L. Puterman, Markov decision processes: discrete stochastic dynamic

programming.

John Wiley & Sons, 2014.

[3] D. P. Bertsekas, Dynamic programming and optimal control 3rd edition,

volume II. Athena Scientiﬁc, 2011.

[4] ——, Reinforcement learning and optimal control. Athena Scientiﬁc,

2019.

[5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.

MIT press, 2018.

[6] D. Neˇsi´c, A. R. Teel, and P. V. Kokotovi´c, “Sufﬁcient conditions
for stabilization of sampled-data nonlinear systems via discrete-time
approximations,” Systems & Control Letters, vol. 38, no. 4-5, pp. 259–
270, 1999.

[7] A. J. Taylor, V. D. Dorobantu, Y. Yue, P. Tabuada, and A. D. Ames,
“Sampled-data stabilization with control lyapunov functions via quadrat-
ically constrained quadratic programs,” IEEE Control Systems Letters,
vol. 6, pp. 680–685, 2021.

[8] A. J. Taylor, V. D. Dorobantu, R. K. Cosner, Y. Yue, and A. D.
Ames, “Safety of sampled-data systems with control barrier functions
via approximate discrete time models,” arXiv preprint arXiv:2203.11470,
2022.

[9] D. Blackwell, D. Freedman, and M. Orkin, “The optimal reward operator

in dynamic programming,” The Annals of Probability, 1974.

[10] D. Bertsekas and S. E. Shreve, Stochastic optimal control: the discrete-

time case. Athena Scientiﬁc, 1996, vol. 5.

[11] E. A. Feinberg, P. O. Kasyanov, and N. V. Zadoianchuk, “Average cost
markov decision processes with weakly continuous transition probabili-
ties,” Mathematics of Operations Research, vol. 37, no. 4, pp. 591–607,
2012.

[12] H. Yu and D. P. Bertsekas, “A mixed value and policy iteration method
for stochastic control with universally measurable policies,” Mathematics
of Operations Research, vol. 40, no. 4, pp. 926–968, 2015.
[13] F. Blanchini, “Set invariance in control,” Automatica, 1999.
[14] S. Prajna, “Optimization-based methods for nonlinear and hybrid sys-
tems veriﬁcation,” Ph.D. dissertation, California Institute of Technology,
2005.

[15] A. D. Ames, X. Xu, J. W. Grizzle, and P. Tabuada, “Control barrier
function based quadratic programs for safety critical systems,” IEEE
Transactions on Automatic Control, vol. 62, no. 8, pp. 3861–3876, 2016.
[16] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, “Safe model-
based reinforcement learning with stability guarantees,” NeurIPS, 2017.
[17] U. Rosolia and F. Borrelli, “Learning model predictive control for
iterative tasks. a data-driven control framework,” IEEE Transactions on
Automatic Control, vol. 63, no. 7, pp. 1883–1896, 2017.

[18] R. Cheng, G. Orosz, R. M. Murray, and J. W. Burdick, “End-to-end
safe reinforcement learning through barrier functions for safety-critical
continuous control tasks,” in Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, vol. 33, 2019, pp. 3387–3395.

[19] J. Choi, F. Castaneda, C. J. Tomlin, and K. Sreenath, “Reinforcement
learning for safety-critical control under model uncertainty, using con-
trol lyapunov functions and control barrier functions,” arXiv preprint
arXiv:2004.07584, 2020.

[20] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement

learning in
robotics: A survey,” The International Journal of Robotics Research,
vol. 32, no. 11, pp. 1238–1274, 2013.

[21] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
deep visuomotor policies,” JMLR, vol. 17, no. 1, pp. 1334–1373, 2016.
[22] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” arXiv preprint arXiv:1509.02971, 2015.

[23] A. Kechris, Classical descriptive set
Business Media, 2012, vol. 156.

theory.

Springer Science &

[24] C. D. Aliprantis and K. C. Border, Inﬁnite Dimensional Analysis: A
Hitchhiker’s Guide. Springer Science & Business Media, 2006.
[25] S. L. Tu, “Sample complexity bounds for the linear quadratic regulator,”

Ph.D. dissertation, UC Berkeley, 2019.

[26] M. Kuss and C. Rasmussen, “Gaussian processes in reinforcement

learning,” NeurIPS, 2003.

[27] D. Bertsekas, Abstract dynamic programming. Athena Scientiﬁc, 2022.
[28] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,

“Deterministic policy gradient algorithms,” in ICML, 2014.

[29] S. M. LaValle, Planning algorithms. Cambridge university press, 2006.
[30] L. Perko, Differential equations and dynamical systems.
Springer

Science & Business Media, 2013, vol. 7.

[31] H. K. Khalil, Nonlinear systems. Prentice hall, 2002, vol. 3.
[32] R. M. Murray, Z. Li, and S. S. Sastry, A mathematical introduction to

robotic manipulation. CRC press, 1994.

[33] R. Olfati-Saber, “Nonlinear control of underactuated mechanical systems
with application to robotics and aerospace vehicles,” Ph.D. dissertation,
Massachusetts Institute of Technology, 2001.

[34] W. Rudin et al., Principles of mathematical analysis. McGraw-hill,

1976, vol. 3.

[35] P. Ballard, “The dynamics of discrete mechanical systems with perfect
unilateral constraints,” Archive for Rational Mechanics and Analysis,
vol. 154, no. 3, pp. 199–274, 2000.

[36] H. Brezis, Op´erateurs maximaux monotones et semi-groupes de contrac-

tions dans les espaces de Hilbert. Elsevier, 1973.

[37] P. Billingsley, Convergence of probability measures.

John Wiley &

Sons, 2013.
[38] M. R. K.

(https://mathoverﬂow.net/users/117299/m-reza k), “Tietze
extension theorem for lower semi continuous functions,” MathOverﬂow.
[Online]. Available: https://mathoverﬂow.net/q/295497

[39] Y. Askoura, “About extension of upper semicontinuous multi-valued

maps and applications,” arXiv preprint arXiv:0810.3127, 2008.

Victor D. Dorobantu was born in Stockholm,
Sweden in 1995. He received the B.S. degree
in CS from Cornell University in 2017, and is
currently pursuing the Ph.D. degree in comput-
ing and mathematical sciences at the California
Institute of Technology (Caltech). His research
interests are reinforcement learning theory, ge-
ometry, and nonlinear control.

Kamyar Azizzadenesheli was born in Iran, in
1992. He received the B.S. degree in EE from
the Sharif University of Technology, in 2014 and
the M.S. and Ph.D. degrees in EE from the
University of California, Irvine, CA, in 2019. From
2019 to 2020, he was a postdoctoral scholar
at the California Institute of Technology. Since
2020, he has been an Assistant Professor with
the Computer Science Department, Purdue Uni-
versity. He is the author of a digital book and
more than 38 articles. His research interests

include machine learning, from theory to practice.

Yisong Yue was born in Beijing in 1983. He
received a B.S. in CS from the University of
Illinois at Urbana-Champaign in 2005, and a
Ph.D in CS from Cornell Universitry in 2010.
From 2010-2013, he was a postdoctoral scholar
at Carnegie Mellon University. From 2013-2014,
he was a research scientist at Disney. Since
2014, he has been on the faculty at the California
Institute of Technology, where he is currently
a full professor. His research interests are in
machine learning.

APPENDIX
We offer alternative, direct proofs of Theorem 1 and Lemma 1.
We proceed as follows: 1) Introduce additional notation, conventions,
and deﬁnitions, 2) Show the measurability and boundedness of
value functions under measurable policies, 3) Introduce Bellman and
optimal Bellman operators, 4) Prove a selection theorem for bounded
and upper semicontinuous functions on graphs of correspondences,
5) Connect these results to proofs of Theorem 1 and Lemma 1.

DOROBANTU et al.: COMPACTLY RESTRICTABLE METRIC POLICY OPTIMIZATION PROBLEMS

9

A. Additional Notation, Conventions, and Deﬁnitions

For a nonempty metric spaces X and Y , the set of bounded
b (X), which is a
b (X) → R,

measurable functions from X to R is denoted L0
Banach space when equipped with the norm (cid:107) · (cid:107)sup : L0
deﬁned as:

(cid:107)f (cid:107)sup = sup
x∈X

|f (x)|,

(45)

for all bounded measurable functions f : X → R. The set of bounded
upper semicontinuous functions from X to R is denoted Cu
b (X), the
set of bounded continuous functions from X to R is denoted C0
b (X),
b (X) ⊆ L0
and note that C0

b (X) ⊆ Cu

b (X).

For another nonempty metric space Y , consider a correspondence
C : X → P(Y ). C is measurable if the lower preimage of each
is a measurable set, and C is weakly measurable if
closed set
the lower preimage of each open set is a measurable set. If C is
measurable, then it is weakly measurable [24, Lemma 18.2].

The product X × Y is a metric space, with a topology generated
by the basis {U × V : U ⊆ X and V ⊆ Y are open}, and,
if X and Y are separable, a σ-algebra generated by the collection
{A × B : A ⊆ X and B ⊆ Y are measurable sets} [37, Appendix
M].

For another nonempty metric space Z, consider a function f : X ×
Y → Z. If fx is continuous for each x ∈ X and f y is measurable
for each y ∈ Y , then f is called a Carath´eodory function; if Y is
also separable, then f is measurable [24, Lemma 4.51].

In what follows, consider an MPOP characterized by an MDP

(S, A, C, f, r, γ) and a policy class Π satisfying Assumption 1.

B. Measurability and Boundedness

To establish the measurability of the value function under a policy
π ∈ Π, we begin by establishing measurability of the corresponding
closed-loop transition map fπ and single-step reward function rπ. To
this end, we deﬁne z : S → S × A as:

z(s) = (s, π(s)),

(46)

for all states s ∈ S. Consider measurable sets A ⊆ S and B ⊆ A;
the preimage of the product A × B under z is z−1(A × B) = {s ∈
S : s ∈ A, π(s) ∈ B} = A ∩ π−1(B), which is a measurable set
since π is a measurable function. Since measurable products generate
the σ-algebra on S × A, z is measurable, implying the compositions
fπ = f ◦ z and rπ = r ◦ z are measurable functions. It follows
that for any t ∈ Z+, the t-iterated composition f t
π is a measurable
function, as is the composition rπ ◦ f t
π. Note that Sπ = S since rπ
is bounded (by the same bound on r); therefore, the corresponding
value function Vπ : S → R is well-deﬁned on all of S. Moreover, Vπ
is measurable since it is the pointwise limit of a sequence of partial
sums of measurable functions. Finally, Vπ is bounded by (1 − γ)−1
times the bound on r, so Vπ ∈ L0

b (S).

C. Bellman Operators

The Bellman operator Tπ : L0

b (S) under policy π
generalizes the implicit structure of the value function Vπ, satisfying:

b (S) → L0

[Tπ(g)](s) = rπ(s) + γg(fπ(s)),

(47)

for all states s ∈ S, for all bounded measurable functions g : S → R.
Indeed, for any bounded measurable function g : S → R, the function
Tπ(g) = rπ + γg ◦ fπ is bounded and measurable. The Bellman
operator Tπ is a γ-contraction [3, §1.4.1], so by the contraction
mapping principle [24, Theorem 3.48], Vπ is the only bounded
measurable function with Tπ(Vπ) = Vπ.

We use Bellman operators to characterize the optimal Bellman
operator, which is commonly used in value iteration. Since each value

function corresponding to a policy in Π is bounded by (1 − γ)−1
times the bound on r, the optimal value function V ∗ is well-deﬁned
and bounded by the same bound. The optimal Bellman operator T :
b (S) → Cu
Cu
[T (g)](s) = sup
π∈Π

(r(s, a) + γg(f (s, a))),

[Tπ(g)](s) = sup

b (S) satisﬁes:

a∈C(s)

(48)
for all states s ∈ S, for all bounded upper semicontinuous functions
g : S → R. We cannot deﬁne T on all of L0
b (S), and ensuring that T
is well-deﬁned (the suprema are ﬁnite and equal) and has codomain
Cu
b (S) is the subject of the following lemma.
Lemma 3. Suppose an MDP (S, A, C, f, r, γ) and policy class
Π satisfy Assumption 1. Let ϕ : Graph(C) → R be upper
semicontinuous and bounded. The function gϕ : S → R speciﬁed
as:

gϕ(s) = max
a∈C(s)

ϕ(s, a),

(49)

for all states s ∈ S is well-deﬁned, upper semicontinuous, and
bounded. Moreover, there is a policy πϕ ∈ Π satisfying:

ϕ(s, πϕ(s)) = gϕ(s),

(50)

for all states s ∈ S.

Proof. The function gϕ is well-deﬁned and upper semicontinuous by
[24, Lemma 17.30] and is bounded by the bound on ϕ.

Deﬁne the correspondence Cϕ : S → P(A) as:

Cϕ(s) = arg max
a∈C(s)

ϕ(s, a) ⊆ C(s),

(51)

for all states s ∈ S. For any state s ∈ S, since gϕ is well-
deﬁned, we have Cϕ(s) (cid:54)= ∅. The singleton set {s} and the set
of admissible actions C(s) are both compact, implying the product
{s} × C(s) is compact. Therefore, the set of maximizing state-action
pairs arg max(s(cid:48),a(cid:48))∈{s}×C(s) ϕ(s(cid:48), a(cid:48)) is nonempty and compact
[24, Theorem 2.43]. Since:
(cid:32)

(cid:33)

Cϕ(s) = pA

arg max
(s(cid:48),a(cid:48))∈{s}×C(s)

ϕ(s(cid:48), a(cid:48))

,

(52)

where pA : S × A → A is the continuous function projecting S × A
onto A, the set of maximizing actions Cϕ(s) is compact.

The remainder of the proof is a modiﬁcation of the proof of [1,
Theorem 2.5.A]. Since C is upper hemicontinuous, it is measurable.
Therefore, C is weakly measurable [24, Lemma 18.2]. Let ρA :
A × A → R+ denote the metric on A, and deﬁne the distance
function ρC : S × A → R+ as:

ρC (s, a) = inf

a(cid:48)∈C(s)

ρA(a, a(cid:48)),

(53)

for all states s ∈ S and actions a ∈ A. Since C is weakly measurable,
ρC is a Carath´eodory function [24, Theorem 18.5]; that is, for every
state s ∈ S, the s-section (ρC )s is continuous, and for every action
a ∈ A, the a-section (ρC )a is a measurable function.

Since C is upper hemicontinuous and compact valued and A is
a Hausdorff space (as it is a metric space), Graph(C) is a closed
subset of S × A [24, Theorem 17.10]. As discussed in [38] (and in
a similar manner to the proof of [39, Corollary 2]), let M ∈ R+
denote the bound on ϕ, and deﬁne the extension ¯ϕ : S × A → R as:

(cid:40)

¯ϕ(s, a) =

ϕ(s, a)
−M

(s, a) ∈ Graph(C),
(s, a) ∈ (S × A) \ Graph(C),

(54)

for all states s ∈ S and actions a ∈ A. To see that ¯ϕ is upper
semicontinuous, consider any real number c ∈ R. If c > −M , then
¯ϕ−1([c, ∞)) = ϕ−1([c, ∞)). Since ϕ is upper semicontinuous, there

10

GENERIC COLORIZED JOURNAL, VOL. XX, NO. XX, XXXX 2017

is a closed set Fc ⊆ S × A satisfying ϕ−1([c, ∞)) = Graph(C) ∩
Fc, making ¯ϕ−1([c, ∞)) a closed subset of S × A. Otherwise, if
c ≤ −M , then ¯ϕ−1([c, ∞)) = S × A, which is a closed set.

Consider a monotonically nonincreasing sequence of bounded real-
valued continuous functions {hn ∈ C0(S × A) : n ∈ N} converging
pointwise to ¯ϕ; such a sequence is guaranteed to exist [1, §2.4]. For
each n ∈ N, deﬁne the correspondence Cn : S → P(A) as:

Cn(s) = {a ∈ A : ρC (s, a) < 1/n

and gϕ(s) < hn(s, a) + 1/n},

(55)

for all states s ∈ S. Now, for any state s ∈ S, recall that the s-
section (ρC )s is continuous. For any n ∈ N, since hn is continuous,
the corresponding s-section (hn)s is continuous. Thus, Cn(s) is an
open subset of A, and can be represented as:

Cn(s) = {a ∈ A : (ρC )s(a) ∈ (−∞, 1/n)}

∩ {a ∈ A : (hn)s(a) ∈ (gϕ(s) − 1/n, ∞)}

= ((ρC )s)−1 ((−∞, 1/n))

∩ ((hn)s)−1 ((gϕ(s) − 1/n, ∞)) .

(56)

Since the sequence of functions {hn : n ∈ N} is monotonically
nonincreasing, these correspondences satisfy Cn+1(s) ⊆ Cn(s) for
all states s ∈ S and n ∈ N. For any state s ∈ S and any action
a ∈ Cϕ(s), we have ρC (s, a) = 0 < 1/n and gϕ(s) = ϕ(s, a) ≤
hn(s, a) < hn(s, a) + 1/n for all n ∈ N. This implies Cϕ(s) ⊆
(cid:84)∞

n=1 Cn(s) for all states s ∈ S.
Next, note that gϕ is a measurable function as it is bounded and
upper semicontinuous. For any action a ∈ A, recall that the a-
section (ρC )a is a measurable function. Additionally, for any n ∈ N,
the corresponding a-section (hn)a is a
since hn is continuous,
measurable function (as it is continuous). This means the following
set is measurable:

{s ∈ S : a ∈ Cn(s)}
= (cid:8)s ∈ S : (ρC )a(s) ∈ (−∞, 1/n)(cid:9)

∩ (cid:8)s ∈ S : gϕ(s) − ((hn)a)(s) ∈ (−∞, 1/n)(cid:9)

= ((ρC )a)−1 ((−∞, 1/n))

∩ (gϕ − (hn)a)−1 ((−∞, 1/n)) .

(57)

Lastly, for any state s ∈ S, consider a sequence of actions
{an ∈ A : n ∈ N} satisfying an ∈ Cn(s) for all n ∈ N. For
each n ∈ N, since ρC (s, an) < 1/n, there is an action a(cid:48)
n ∈ C(s)
satisfying ρA(a(cid:48)
n, an) < 2/n. Since C(s) is compact, the sequence
of actions {a(cid:48)
n ∈ C(s) : n ∈ N} has a limit point a∗ ∈ C(s). That
is, there is a monotonically increasing sequence {nk ∈ N : k ∈ N}
such that the subsequence {a(cid:48)
nk ∈ C(s) : k ∈ N} converges to a∗.
Since ρA(a(cid:48)
nk , ank ) < 2/nk ≤ 2/k for all k ∈ N, the subsequence
{ank ∈ A : k ∈ N} also converges to a∗. For any k, l ∈ N with
k > l, we have:

hnl (s, ank ) ≥ hnk (s, ank ) > gϕ(s) − 1/nk ≥ gϕ(s) − 1/k, (58)
is continuous, this means limk→∞ hnl (s, ank ) =
and since hnl
hnl (s, a∗) ≥ gϕ(s). Since the sequence of functions {hn : n ∈ N}
converges pointwise to ¯ϕ from above, we have liml→∞ hnl (s, a∗) =
¯ϕ(s, a∗) = ϕ(s, a∗) ≥ gϕ(s), implying a∗ ∈ Cϕ(s). That is, the
sequence of actions {an : n ∈ N} has a limit point in Cϕ(s).
By the measurability criterion in [1, §2.6], the correspondence Cϕ
satisﬁes the assumptions of [1, Theorem 2.6.B]; therefore, there exists
a measurable selection πϕ : S → A of Cϕ, implying πϕ ∈ Π and
ϕ(s, πϕ(s)) = gϕ(s) for all states s ∈ S.

Equipped with Lemma 3, we only need to show that the suprema
in (48) are equal. Fix a bounded measurable function g : S → R and

a state s ∈ S. For any policy π ∈ Π, we have:

[Tπ(g)](s) = r(s, π(s)) + γg(f (s, π(s)))

≤ sup

(r(s, a) + γg(f (s, a))).

(59)

a∈C(s)

Conversely, for any action a ∈ C(s), we have:

r(s, a) + γg(f (s, a)) = rπs,a (s) + γg(fπs,a (s))

= [Tπs,a (g)](s) ≤ sup
π∈Π

[Tπ(g)](s),

(60)

where πs,a ∈ Π satisﬁes πs,a(s) = a. Considering all policies in (59)
and all actions in (60) establishes the equality of the suprema. Since r
and g are bounded and upper semicontinuous and f is continuous, the
function r+γg ◦f is bounded and upper semicontinuous, so applying
Lemma 3 to this function allows us to conclude that T (g) ∈ Cu
b (S).
To show that the MPOP is well-posed, we show that the optimal
is T (V ∗) =
value function V ∗ is the ﬁxed point of T ;
V ∗. However, though T is a γ-contraction [3, §1.4.1], we cannot
immediately apply the contraction mapping principle [24, Theorem
3.48] to show the existence of a unique ﬁxed point of T since Cu
b (S)
is not a closed subset of L0

that

b (S).

Instead, we consider the sequence of bounded upper semicon-
tinuous function {Vn ∈ Cu
b (S) : n ∈ N} generated by value
iteration when the initial guess is chosen as the constant function
V0 = M/(1 − γ), where M ∈ R+ denotes the bound on r. To
verify that the functions in this sequence are bounded and upper
semicontinuous, note that the initial guess is bounded and upper
semicontinuous, and by induction, the value iteration update can be
written as:

Vn+1 = T (Vn),
(61)
for all n ∈ Z+. The sequence of functions is also monotonically
nonincreasing. To see this, note that:

V1(s) = [T (V0)](s) = sup

(r(s, a) + γV0(f (s, a)))

a∈C(s)

≤ M + γM/(1 − γ) = M/(1 − γ) ≤ V0(s),

(62)

for all states s ∈ S. Since T preserves ordering [3], if for some
n ∈ Z+ we have Vn+1(s) ≤ Vn(s) for all states s ∈ S, then
[T (Vn+1)](s) ≤ [T (Vn)](s) for all states s ∈ S. Equivalently,
Vn+2(s) ≤ Vn+1(s) for all states s ∈ S. By induction, it follows that
the sequence of functions is monotonically nonincreasing. Finally,
the sequence of functions is bounded by M/(1 − γ). By deﬁnition,
V0 satisﬁes this bound, and since the sequence of functions is
nonincreasing, it is upper bounded by M/(1 − γ). To show the lower
bound of −M/(1 − γ), note that:

V1(s) ≥ −M + γV0(f (s, a)) ≥ −M −

γ
1 − γ

M = −

M
1 − γ

, (63)

for all states s ∈ S. Similarly, if for some n ∈ Z+ we have Vn(s) ≥
−M/(1 − γ) for all states s ∈ S, then:

Vn+1(s) ≥ −M +γVn(f (s, a)) ≥ −M −

γ
1 − γ

= −

M
1 − γ

, (64)

for all states s ∈ S. Therefore, the sequence of functions is bounded
below by induction.

The pointwise limit of a monotonically nonincreasing sequence of
bounded upper semicontinuous functions is upper semicontinuous [1,
§2.4], and since the sequence is bounded, so is the limit. Denote the
b (S). To show that g∗ is a ﬁxed point of T , note
limit by g∗ ∈ Cu
that:

(cid:107)T (g∗) − g∗(cid:107)sup ≤ (cid:107)T (g∗) − Vn+1(cid:107)sup + (cid:107)Vn+1 − g∗(cid:107)sup
= (cid:107)T (g∗) − T (Vn)(cid:107)sup + (cid:107)Vn+1 − g∗(cid:107)sup
≤ γ(cid:107)g∗ − Vn(cid:107)sup + (cid:107)Vn+1 − g∗(cid:107)sup,

(65)

DOROBANTU et al.: COMPACTLY RESTRICTABLE METRIC POLICY OPTIMIZATION PROBLEMS

11

which implies:

(cid:107)Vπε − VNε+1(cid:107)sup ≤
γ
1 − γ
γ
1 − γ

γ
1 − γ
(cid:107)VNε+1 − g∗(cid:107)sup +
1 − γ
1 + γ

2γ
1 + γ

< 2

ε =

≤

ε

(cid:107)VNε+1 − VNε (cid:107)sup
γ
1 − γ

(cid:107)g∗ − VNε (cid:107)sup

(75)

Finally, we have:

(cid:107)Vπε − g∗(cid:107)sup ≤ (cid:107)Vπε − VNε+1(cid:107)sup + (cid:107)VNε+1 − g∗(cid:107)sup

<

2γ
1 + γ

ε +

1 − γ
1 + γ

ε = ε.

(76)

This means:

g∗(s) < Vπε (s) + ε ≤

(cid:18)

(cid:19)

Vπ(s)

sup
π∈Π

+ ε = V ∗(s) + ε,

(77)

for all states s ∈ S. Since ε was arbitrary, we have g∗(s) ≤ V ∗(s)
for all states s ∈ S.

Since V ∗ = g∗, we conclude that V ∗ is bounded and upper
semicontinuous. By Lemma 3, there is a policy π∗ ∈ Π satisfying:

Vπ∗ (s) = [Tπ∗ (Vπ∗ )](s)

= r(s, π∗(s)) + γV ∗(f (s, π∗(s)))
{r(s, a) + γV ∗(f (s, a))}
= sup

a∈C(s)

= [T (V ∗)](s) = V ∗(s),

(78)

for all states s ∈ S. This means π∗ is an optimal policy. Moreover,
since g∗ is the limit of value iteration from any bounded and upper
semicontinuous initial guess, V ∗ is as well.

Proof of Lemma 1. Even without the assumption that S and A are
Polish spaces, ϕs,a is continuous and bounded, so ϕs,a ∈ Cu
b (S).
Therefore, by Lemma 3, there is a policy πs,a ∈ Π satisfying (14),
with πs,a(s) = a.

γtrπ(f t

π(s)) + γT +1g∗(f T +1

π

(s)),

(70)

Finally, we conclude with a proof of Lemma 1.

for all n ∈ Z+. By choosing n sufﬁciently large, we make
(cid:107)T (g∗) − g∗(cid:107)sup arbitrarily small, implying (cid:107)T (g∗) − g∗(cid:107)sup = 0,
or T (g∗) = g∗. Now, consider any other sequence of bounded upper
semicontinuous functions {Wn ∈ Cu
b (S) : n ∈ N} generated by
value iteration from a bounded and upper semicontinuous initial guess
W0 ∈ Cu
(cid:107)Wn+1 − g∗(cid:107)sup = (cid:107)T (Wn) − T (g∗)(cid:107)sup ≤ γ(cid:107)Wn − g∗(cid:107)sup,
(66)

b (S). Since T is a γ-contraction, we have:

for all n ∈ Z+, or:

(cid:107)Wn − g∗(cid:107)sup ≤ γn(cid:107)W0 − g∗(cid:107)sup,

(67)

for all n ∈ Z+. This implies limn→∞ Wn = g∗.
We prove Theorem 1 by showing that V ∗ = g∗.

Proof of Theorem 1. First, for any policy π ∈ Π, we have:

g∗(s) = sup

(r(s, a) + γg∗(f (s, a)))

a∈C(s)

≥ rπ(s) + γg∗(fπ(s)),
for all states s ∈ S. For any T ∈ N, assume:

g∗(s) ≥

T −1
(cid:88)

t=0

γtrπ(f t

π(s)) + γT g∗(f T

π (s)),

(68)

(69)

for all states s ∈ S. Then:

g∗(s)

T −1
(cid:88)

γtrπ(f t

π(s)) + γT (rπ(f T

π (s)) + γg∗(fπ(f T

π (s))))

≥

=

t=0
T
(cid:88)

t=0

for all states s ∈ S. By induction, (69) holds for all T ∈ N, and
since g∗ is bounded, we have:

g∗(s) ≥

∞
(cid:88)

t=0

γtrπ(f t

π(s)) = Vπ(s),

(71)

for all states s ∈ S. Taking the supremum over all policies, we obtain
g∗(s) ≥ supπ∈Π Vπ(s) = V ∗(s) for all states s ∈ S.

We show the reverse inequality by following a modiﬁcation of
the proof of [2, Theorem 6.3.1]. Consider the sequence of bounded
upper semicontinuous functions {Vn ∈ Cu
b (S) : n ∈ N} generated
by value iteration from an arbitrary initial guess V0 ∈ Cu
b (S). Since
limn→∞ Vn = g∗, for any ε ∈ R++, there is a corresponding
Nε ∈ N such that:

(cid:107)Vn − g∗(cid:107)sup <

1 − γ
1 + γ

ε,

(72)

for all n ∈ N with n ≥ Nε. By Lemma 3, there is a policy πε ∈ Π
satisfying:

[Tπε (VNε+1)](s) = r(s, πε(s)) + γVNε+1(f (s, πε(s)))
(r(s, a) + γVNε+1(f (s, a)))

= max
a∈C(s)

= [T (VNε+1)](s),

(73)

for all states s ∈ S. Therefore:

(cid:107)Vπε − VNε+1(cid:107)sup

≤ (cid:107)Vπε − T (VNε+1)(cid:107)sup + (cid:107)T (VNε+1) − VNε+1(cid:107)sup
= (cid:107)Tπε (Vπε ) − Tπε (VNε+1)(cid:107)sup

+ (cid:107)T (VNε+1) − T (VNε )(cid:107)sup
≤ γ(cid:107)Vπε − VNε+1(cid:107)sup + γ(cid:107)VNε+1 − VNε (cid:107)sup,

(74)

