FamilySeer: Towards Optimized Tensor Codes by Exploiting
Computation Subgraph Similarity

Shanjun Zhang, Mingzhen Li, Hailong Yang, Yi Liu, Zhongzhi Luan and Depei Qian
School of Computer Science and Engineering
Beihang University, Beijing, China, 100191
{lu_cheung,lmzhhh,hailong.yang,yi.liu,07680,depeiq}@buaa.edu.cn

2
2
0
2

n
a
J

1

]

G
L
.
s
c
[

1
v
4
9
1
0
0
.
1
0
2
2
:
v
i
X
r
a

ABSTRACT
Deploying various deep learning (DL) models efficiently has boosted
the research on DL compilers. The difficulty of generating opti-
mized tensor codes drives DL compiler to ask for the auto-tuning
approaches, and the increasing demands require increasing auto-
tuning efficiency and quality. Currently, the DL compilers partition
the input DL models into several subgraphs and leverage the auto-
tuning to find the optimal tensor codes of these subgraphs. However,
existing auto-tuning approaches usually regard subgraphs as indi-
vidual ones and overlook the similarities across them, and thus fail
to exploit better tensor codes under limited time budgets.

We propose FamilySeer, an auto-tuning framework for DL com-
pilers that can generate better tensor codes even with limited time
budgets. FamilySeer exploits the similarities and differences among
subgraphs can organize them into subgraph families, where the
tuning of one subgraph can also improve other subgraphs within
the same family. The cost model of each family gets more purified
training samples generated by the family and becomes more ac-
curate so that the costly measurements on real hardware can be
replaced with the lightweight estimation through cost model. Our
experiments show that FamilySeer can generate model codes with
the same code performance more efficiently than state-of-the-art
auto-tuning frameworks.

KEYWORDS
Neural Networks, Deep Learning Compiler, Auto-tuning

1 INTRODUCTION
The performance of deep learning (DL) models is critical to address
the ever-increasing demands in the fields of computer vision [32],
natural language processing [28], auto-driving [18] and etc. How-
ever, manually optimizing the deep learning models is both error-
prone and hardly-portable due to the model diversity and hardware
complexity. Therefore, deep learning compilers play an important
role to enable high-performant code generation for various models
on different hardware automatically. Currently, several deep learn-
ing compilers have been proposed such as XLA [2], nGraph [8],
Tensor Comprehension [30], TVM [6] and etc. A compiler takes the
DL models from DL frameworks (e.g., Tensorflow [1], Mxnet [5],
Pytorch [22]) as input. It converts the model into multiple level of
intermediate representations (IRs), and then automatically applies
various performance optimizations regarding the model characteris-
tics and underlying hardware in order to generate high-performant
model codes [19]. Although different design philosophies have been
adopted in different compilers, the fundamental procedures to gen-
erate efficient model codes are similar. These procedures can be

commonly divided into two phases including: 1) integrating a col-
lection of code transformations (optimization techniques), and 2)
determining the optimal sequence to apply the code transforma-
tions (searching techniques).

During compilation, the deep learning model is first transformed
into a computation graph, where each node represents an operator
(e.g., convolution and ReLU) and each edge represents the data flow.
The computation graph is further divided into subgraphs, where
each subgraph may contain several operators that can be fused to-
gether (e.g., convolution, ReLU and pooling). The code generation
procedures are then applied to each subgraph to determine the op-
timal sequence of code transformations for high-performant codes.
The optimization techniques have commonly been adopted includ-
ing loop tiling, loop fusion, parallelization, vectorization and etc.
Whereas the searching techniques have commonly been adopted
including AutoTVM [7], Ansor [33], MetaTune [26] and etc. The
fundamental idea of searching techniques can be commonly divided
into three step: 1) explore massive code transformations and gener-
ate huge amount of code candidates, which forms a large search
space, 2) apply searching algorithms to identify the candidates with
high performance potential, and 3) evaluate these candidates on
real hardware and fine-tune the searching algorithms to identify
better code candidates. The above three steps are applied iteratively
until the time budget is expired. To maintain the search overhead
within acceptable time budget, the searching techniques usually
allocate the limited time slots proportionally to the subgraphs based
on their dominance in the model execution time.

With the optimization techniques well studied in existing deep
learning compilers, the searching techniques become critical in
generating high-performant model codes. A good searching tech-
nique needs to generate large enough search space (affecting search
quality) and explore the search space efficiently (affecting search
speed). For example, the recently proposed searching scheme An-
sor [33] adopts code sketch and parameter sampling to ensure good
search quality through a large search space. In addition, it adopts a
cost model to quickly filter out code transformation sequence with
high performance potential, and thus accelerates the search process.
Since the accuracy of cost model can directly affect the search effi-
ciency, several works [3, 17, 29] have been proposed to improve the
accuracy of cost model through either careful feature engineering
or relying on machine learning methods. However, the above works
universally adopt a single cost model for all subgraphs, that fails
to achieve accurate performance estimation for subgraphs with
diverse characteristics, and further constrains the search quality to
generate better codes. In addition, existing searching techniques
attempt to allocate more search time to the bottleneck subgraphs,

 
 
 
 
 
 
and thus the cost models are trained with bias towards these sub-
graphs. Such search policy neglects the optimization potential of
other subgraphs and slows down the convergence to optimal results
for all subgraphs of the entire model.

To address the above limitations, we propose FamilySeer, a new
search method to determine the optimal sequence of code trans-
formations during compilation. The fundamental idea of Famil-
ySeer is to exploit the similarity of subgraphs, and organize the
similar subgraphs into a collection of subgraph families. With the
subgraph families identified, FamilySeer constructs cost models at
subgraph family basis, which can effectively address the diverse
characteristics across different subgraph families and thus improve
the cost model accuracy. In addition, the subgraphs within a family
can share the search result during each tuning iteration without
applying the costly measurements on real hardware, which can
speedup the search process to converge to optimal results within
limited time budget. We implement FamilySeer within the deep
learning compiler TVM to enable model code generation across
different hardware platforms. Moreover, we implement auxiliary
optimizations such as parallelization of cost model training and
code measurement on GPUs, which can further speedup the search
process.

Specifically, this paper makes the following contributions:

• We propose FamilySeer, a new auto-tuning framework that
can be applied during model compilation to generate more
efficient model codes. FamilySeer exploits the subgraph simi-
larity to form a collection of subgraph families, and construct
cost models at subgraph family basis to improve cost model
accuracy.

• We re-design the search process to better utilize the advan-
tage of subgraph families. Particularly, we enable the sub-
graphs within each family to share the search results within
each tuning iteration, avoiding costly code measurements
on real hardware and thus accelerating the search process
to converge to optimal results.

• We evaluate the effectiveness of FamilySeer with representa-
tive models on both CPU and GPU platforms. The experimen-
tal results demonstrate that, compared to the state-of-the-art
search scheme Ansor, FamilySeer can generate model codes
more efficient with 2.49× and 3.04× performance speedup
on average within the same code performance.

The rest of the paper is organized as follows. Section 2 describes
the background of search-based auto-tuning framework in deep
learning compiler. Section 3 presents the drawback of auto-tuning
ignored by the deep learning compiler. Section 4 and Section 5
present the design overview and our detailed implementation of our
FamilySeer. Section 6 presents the evaluation results and compares
the search quality and efficiency with the state-of-the-art search
scheme. Section 8 concludes this paper.

2 BACKGROUND
The searching techniques of DL compilers are critical to generating
high-performance model codes. TVM is the state-of-the-art deep
learning compiler that applies many search-based optimization
techniques. Ansor is the second generation searching technique

2

Table 1: The number of the subgraphs partitioned by TVM
of state-of-the-art DL models.

DL Models
ResNetv1
ResNetv2
Mobilenet
Mobilenetv2
BERT
RoBERTa
GPT2
Vision Transformer

Number of Subgraph
25∼28
30∼32
22∼25
34∼38
11∼13
9∼11
10
13

of TVM. This work is built on top of the Ansor. To better under-
stand the searching procedure, we take Ansor as an example for
illustration.

Generating Subgraphs - Ansor is implemented within the
deep learning compiler TVM. TVM compiles a DL Model into a
computation graph, where a node represents an operator, and the
edges represent the data flow. TVM then fuses these operators
according to the pre-defined optimization rules. Ansor takes these
fused operators and regards them as a subgraph. As an operator can
take tensors with various shapes as the input (e.g., in ResNet50_v1,
the input shape of a convolution operator varies from 7 × 7 × 512 to
56×56×64), many subgraphs may have the same operator sequence
but with different input shapes. We have summarized the number
of subgraphs the state-of-the-art DL models in Table 1.

Scheduling Subgraphs - Ansor adopts the gradient descent
algorithm to schedule the evaluation process of subgraphs, which
allocates the limited time slots to the subgraphs, in order to improve
the search efficiency and the search quality. For example, GPT2
is divided into 10 subgraphs. But only three subgraphs contribute
to 80% of the overall execution time. Thus Ansor allocates more
time to these subgraphs than others. Specifically, Ansor passes each
subgraph’s performance information (execution time, GFlops, etc.)
into the gradient descent algorithm and allocates the next time
slot to the subgraph with lowest negative gradient. This algorithm
behaves well in the first few iterations since the bottleneck sub-
graphs have more possibility to acquire the time slots. But as the
bottleneck subgraph reaches its performance ceiling, their potential
of further performance improvement is minor. However, they are
still allocated with more time slots according to this algorithm,
which hinders the evaluation process of other subgraphs with more
potential.

Building the Cost Model - To reduce the overhead of evaluat-
ing the massive transformed codes for each subgraph on real hard-
ware, cost models have been adopted to estimate the performance
of the transformed codes. Ansor adopts an XGBoost cost model to
score each subgraph, which has been designed with heavy feature
engineering to correctly predict the performance of transformed
codes. The subgraphs with top K scores are selected to evaluate on
real hardware. These subgraph measurements are accumulated and
then used to train a new cost model. However, at the beginning of
code search, when there is no enough training data, the cost model
is hardly accurate to identify high-performant code candidates. In
addition, compounded with the subgraph scheduling strategy that

favors bottleneck subgraphs, the cost model is gradually trained
with biased data, which eventually constrains the model accuracy.
Evaluating Code Candidates - The potential code candidates
(transformed codes) of each subgraph selected by the cost model are
then evaluated on real hardware. The real measurements are used
to train the cost model as well as adjust the subgraph scheduling
decisions. One limitation of current Ansor implementation is that
it evaluates the code candidates sequentially, and fails to exploit
the independence among the code candidates for parallel evalua-
tion. Since the multi-GPU platforms become ubiquitous in deep
learning applications, evaluating the independent code candidates
simultaneously on multiple GPUs can significantly boost the search
process.

3 MOTIVATION
In this section, we present the observations of inefficiencies from
the existing auto-tuning frameworks for DL models, and we ana-
lyze the drawbacks of these frameworks to motivate the design of
FamilySeer.

3.1 Overlooking Similarities and Differences

Across Subgraphs

The auto-tuning frameworks usually adopt the cost model to esti-
mate the performance of the transformed programs. Specifically,
they build a monolithic cost model to estimate all transformed pro-
grams of all subgraphs during the auto-tuning [33, 34]. However,
due to the variety of subgraphs, the cost model may fail to preform
accurate estimations across all programs and introduce great bias
between estimated performance and real performance, which can
reduce the efficiency of the auto-tuning procedure. Figure 1 shows
the experiment of BERT-Large about the cost model accuracy. The
BERT-Large can be partitioned into 11 subgraphs, and we select
256 samples (candidates) for each subgraph to build the monolithic
cost model (256 × 11 samples in total). It is obvious that the predic-
tion accuracy (black bars) on subgraph_4 and subgraph_10 is much
lower than that of other nine subgraphs.

Since the cost model is trained from scratch, its accuracy is de-
pending on the quality of the training samples significantly. We
suspect that the training samples from different subgraphs can not
only cooperate with each other, which leads to improved accuracy
on some subgraphs but also interfere with each other, which leads
to decreased accuracy. To better understand this phenomenon, we
further equip each subgraph with a individual cost model, and train
the model with the subgraph’s samples. That is, each individual
model is trained with purified samples (i.e., 1
11 of the original sam-
ples). As shown in Figure 1 (gray bars), the accuracy on subgraph_4
and subgraph_10 returns to normal, which is similar with other
subgraphs. This is because the cost models are trained with puri-
fied samples without any biased sample. However, the accuracy on
subgraph_8 decreases because its cost model cannot leverage the
samples from similar subgraphs and thus suffers from decreased
samples.

Moreover, we conduct an experiment to explore the relationship
across training samples from different subgraphs. As shown in Fig-
ure 2, the cell in row 𝑋 and column 𝑌 represents the accuracy when

using the individual cost model of subgraph_X to predict the valida-
tion samples of subgraph_Y. It is obvious that the relationship across
the subgraphs is complicated enough. For example, the cost model
of subgraph_4 can predict the samples of subgraph_2/4/5/6/7/8/11
with an accuracy greater than 74.5%. However, only the cost model
of subgraph_6 can predict the samples subgraph_2 with an accu-
racy greater than 99.8%. Besides, the samples of subgraph_2/8 can
be accurately predicted by cost models of other subgraphs (expect
subgraph_11). Therefore, the similarities and differences between
subgraphs are complex enough, and the currently adopted mono-
lithic cost models overlook these features, missing the possibility
of extra performance improvement.

Figure 1: Accuracy difference between using monolithic cost
model and individual cost models.

Figure 2: Accuracy heatmap among difference subgraphs,
where the cell in row 𝑋 and column 𝑌 represents the accu-
racy when using the individual cost model of subgraph_X to
predict the validation samples of subgraph_Y.

3.2 Wasting Time on Subgraphs Without

Potential

The auto-tuning frameworks allocate a predefined time budget for
the tuning procedure. They consume the time budget to tune the
subgraphs in an iterative manner, till using up the time budget.
In each iteration, they tune single subgraph. They generate mul-
tiple program candidates for each subgraph, and then sort them

3

Subgraph_1Subgraph_2Subgraph_3Subgraph_4Subgraph_5Subgraph_6Subgraph_7Subgraph_8Subgraph_9Subgraph_10Subgraph_11Geomean0.000.250.500.751.001.25AccuracyMonolithic Cost ModelInvididual Cost Modelsubgraph_1subgraph_2subgraph_3subgraph_4subgraph_5subgraph_6subgraph_7subgraph_8subgraph_9subgraph_10subgraph_11subgraph_1subgraph_2subgraph_3subgraph_4subgraph_5subgraph_6subgraph_7subgraph_8subgraph_9subgraph_10subgraph_110.00.20.40.60.81.0in ascending order, according to their latency estimated by the
cost model. They select the top-k program candidates (e.g., top-64
in Ansor), and measure their latency in real hardware (e.g., V100
GPU), which is called measurements. Notably, each measurement
consumes a time budget. The results of the measurements are used
to train the cost models, at the same time, the results are normalized
to represent the real latency of the subgraph. These frameworks
adopt the greedy algorithm to allocate time budgets to the sub-
graphs with higher latency (i.e., bottleneck subgraphs). However,
if a bottleneck subgraph has trivial headroom of latency improve-
ment, the greedy algorithm still prioritizes this subgraph, which
wastes the budget and leaves other subgraphs far from the optimal.
We tune the BERT-Large model with Ansor on a V100 GPU,
where the time budget is set to 9900 measurements as Ansor rec-
ommended. As shown in Figure 3, subgraph_7 gets the most time
budget, 4288 measurements, and subgraph_4/5/6 gets 1024, 2048,
2304 measurements, respectively. While other subgraphs get much
fewer measurements. As shown in Figure 4, we further allocate
extra time budgets for these subgraphs, so that all subgraphs have
the same budget (i.e., 4288 measurements) as the subgraph_7. The
dashed lines indicate the latency improvement with the extra bud-
gets. Notably, subgraph_2/8 have limited number of program can-
didates (320 and 320, respectively) in total and all the candidates
have already been measured. We have several observations: 1) Sub-
graph_7 has no improvement during the 2944-4288 measurements.
2) Subgraph_5 has the highest improvement for 0.192 ms. 3) If we
just reallocate the wasted budgets to other subgraphs evenly, the
overall latency of the subgraphs can improve 0.049 ms.

To summarize, we believe that the existing auto-tuning frame-
works fail to utilize both the training samples and the time budgets
and leads to the unsatisfying tuning efficiency, which motivates us
to design an efficient auto-tuning framework.

Figure 4: The latency improvement of each subgraph when
allocating sufficient time budgets.

(e.g., rules provided by TVM). It clusters the subgraphs into several
families, where the subgraphs in a family can share the training
samples and time budgets. Then it tunes the subgraph families by
generating program candidates and select the candidates with less
latency, in an iterative manner. Notably, FamilySeer focuses on
the improvement of subgraph family rather than that of individual
subgraphs.Specifically, the design of FamilySeer primarily contains
two parts: the subgraph family and the family performance tuner
(i.e., foresee tuning), as shown in Figure 5.

Figure 3: Time budget allocation among different subgraphs.

4 DESIGN OVERVIEW
As stated above, the training samples and the time budgets are
underutilized by existing auto-tuning frameworks of deep learn-
ing compilers. We believe that the auto-tuning frameworks can
achieve better efficiency (i.e., less searching time) and better quality
(i.e., more inference throughput) even with the same amount of
samples and budgets. Therefore, we propose a new auto-tuning
framework, FamilySeer, which reschedules the training samples
and time budgets for superior performance to other frameworks.
FamilySeer takes the deep learning model as the input and ob-
tains a series of subgraphs according to the graph partition rules

Figure 5: The overall design of FamilySeer, including the sub-
graph family and the foresee tuning.

The subgraph family is the foundation for the improvement of
FamilySeer’s tuning efficiency. Forming the subgraph families needs
to cluster the subgraphs into the subgraph families, and construct
of the cost model to estimate the latency of generated programs
candidates within the family. In order to avoid introducing extra
overhead to the auto-tuning process we analyze the similarity of
subgraphs according to their attributes and form the families ac-
cordingly. During the auto-tuning, the search algorithm finds the

4

Subgraph_1Subgraph_2Subgraph_3Subgraph_4Subgraph_5Subgraph_6Subgraph_7Subgraph_8Subgraph_9Subgraph_10Subgraph_1101000200030004000# of measurements64128641024204823044288128320192128                                 8 V H G  W U L D O                                                        / D W H Q F \  L P S U R Y H P H Q W  G H O W D   P V  6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K   6 X E J U D S K    6 X E J U D S K   Deep learning NetworkGraph PartitionOptimalSubgraphsCompiledSubgraphs……S1SnGrouping andOptimizingSearch AlgorithmTunerGradient DescentGenerating CandidateTargetSubgraphCalculatePotentialHardwareCPUMultiple GPUExpectedCandidatesTrain Cost Modelwith EvaluatedCandidatesFamilySeer...                                           … Subgraph Attribute AnalysisFamily…Cost ModelFamily…Cost ModelFamily…Cost ModelFamily n…Cost ModelForesee tuningOperationHashOps countFindFamilyPotential AnalysisFast Tuningsubgraph family to which the current tuning subgraph belongs, and
then filter the various program candidates to find the candidates
with more improving potential with the help of the family’s cost
model.

The family performance tuner optimizes the tuning process of
the subgraph family. After the tuner figures out the bottleneck
subgraphs with higher latency, it generates program candidates
of the subgraphs based on the time budget. The tuner uses cost
model from each family to estimate the latency of each candidate,
and selects the candidates with higher improving potential. The
selected candidates are then sent to real hardware (e.g., GPUs) for
latency evaluation, and the results are returned to the subgraph
family. The evaluation results are forwarded to the cost model
inside the family, behaving as the training samples to help improve
the accuracy of the cost model. Since subgraphs within a family
share similar characteristics, we can use the cost model to foresee
other subgraphs, which replaces the costly code measurements on
real hardware and saves the time budgets, and thus improves the
search efficiency. This procedure is executed iteratively, till the time
budgets are used up.

5 METHODOLOGY
In this section, we describe the methods to explore the similarity of
subgraphs and construct subgraph families. Then we present the
algorithms of FamilySeer to apply subgraph families to optimize
the allocation of time budget so as to improve the tuning efficiency
and quality.

5.1 Identifying Similar Subgraphs
We find out that there are similarities between different subgraphs
(described in Section 3). To generate an accurate cost model for the
similar subgraphs, we need to cluster the subgraphs and reorder
into subgraph families. The similarity between subgraphs lies on the
accuracy across the individual cost models of individual subgraphs.
Our goal is to find several subgraph families so that the subgraphs
can benefit from the family cost models with higher estimation
accuracy. An intuitive approach is to apply the individual cost
model of each subgraph to estimate program candidates of other
subgraphs, and classify the subgraphs with impressive accuracy into
a family, during tuning. This approach ensures that each subgraph
can be placed on the most suitable subgraph family. However, it
requires continuous estimations on real hardware and redundant
training of cost models of all subgraphs, which introduces non-
trivial overhead. Moreover, due to the limited number of training
samples at the beginning, the cost models lack the training data
and usually fails to perform reliable estimations. This may result in
two dissimilar subgraphs being assigned together, which affects the
quality of the search. Therefore, FamilySeer adopts the approach
of static analyzing, which classifies the subgraphs based on their
attributes (e.g., operation sequence, input shape, etc.). There is no
overhead in the tuning process because the classification can be
done ahead of tuning.

We try to analyze the attributes of these subgraphs and find
out these subgraph families. A subgraph contains a variety of dif-
ferent operators with different input shapes, which form different
subgraphs based on a certain sequence. Most of the deep learning

Figure 6: Attributes of the subgraphs.

compiler generate many subgraph attributes to help to distinguish
different subgraphs. As shown in Figure 6, the operator sequence of
the subgraph is a description to the data flow of the subgraph, which
is unrelated to the size and shape of the subgraph. The compilers
tend to convert these sequences into strings using serialization algo-
rithms (e.g., hash) and using these serialized strings to distinguish
different subgraphs. The compilers also record the core operators
when performing operator fusion. Therefore, there are three algo-
rithms of static analyzing based on the subgraph attributes: 1) by
the number of operators, 2) by core operator, and 3) by the operator
sequence. We use these three algorithms to generate subgraph fami-
lies and analyze the accuracy of each subgraph using its cost model
inside the subgraph family. As shown in Table 2, using the algo-
rithm by the core operation to generate subgraph families achieves
the highest accuracy (99.4%). Therefore we construct subgraph fam-
ilies by the core operation of all subgraphs. Notably, Family still
reserves an interface for static analyzing. If better algorithms are
explored in the future, we can quickly replace the current algorithm
(by the core operation) with them.

Table 2: Cost model accuracy under different algorithms of
static analyzing.

Algorithm
By operation number
By core operation
By operation sequence

Accuracy
97.4%
99.4%
99.1%

5.2 Foresee tuning
As shown in Figure 5, subgraphs inside the same subgraph family
share the same cost model. We can use this cost model to foresee
better candidates for each subgraph and generate optimal program
candidates. The algorithm of foresee tuning is shown in Algorithm 1.
When tuning a deep learning model 𝑁 , users usually provide time
budget 𝐵 (i.e., number of measurements on real hardware) or an
expected latency of model inference. The foresee proportion 𝑝
determines the tuning opportunity that can be shared between
bottleneck subgraphs and less-improved subgraphs inside the same

5

SubgraphOperationSequenceInput ShapeOperation TypeFused Operation Number……Conv+AddGemm+Add+Addpool+Add128x128256x256x256…………Batch MatmulContribConv……subgraph family, which is described in detail later. The foresee
tuning algorithm takes 𝑁 , 𝐵, and 𝑝 as the inputs.

The operators of model 𝑁 is fused according the pre-defined
fusion rules of TVM, and each fused operators are considered as a
subgraph (line 2). Then the subgraph families are constructed by
identifying the similar subgraphs (line 3), as described in Section 5.1,
and an individual cost model is initialized for each subgraph family
(line 5). The number of generated candidates of each bottleneck
subgraph during the tuning iteration is defined in line 6, which is
similar with the mini-batch size. This number is restricted to 64
at most, in order to increase the tuning iterations and avoid the
over-fitting.

When the used budget 𝑏 (initialized to 0, line 1) is less than the
time budget 𝐵, FamilySeer conducts two tuning steps, one for the
bottleneck subgraph of all subgraphs and one for the bottleneck
graph of the subgraph family. This procedure is conducted in an
iterative manner until 𝑏 ≥ 𝐵. Firstly, FamilySeer focuses on the all
subgraphs, and calculates the improving potential of all subgraphs
(line 10). Specifically, the subgraphs with higher latency have higher
improving potential. Then the subgraphs are sorted by their poten-
tial in descending order (line 11), so that the top subgraph (denoted
as 𝑠𝑐𝑢𝑟 ) is the bottleneck subgraph with the highest latency (line 12).
Then the subgraph family (line 13) of the bottleneck subgraph and
the corresponding cost model (line 14) are figured out. FamilySeer
tunes this subgraph with the help of the family cost model (line 16).
The tuner inside 𝑡𝑢𝑛𝑒 function generates program candidates for
subgraph 𝑠𝑐𝑢𝑟 and then use the family cost model 𝑐𝑐𝑢𝑟 to estimate
the latency of the candidates. The candidates with lower latency
will be evaluated on the real hardware, and the number of candi-
dates is restricted to 𝑔. The evaluated candidates are used to update
the family cost model and help to improve accuracy of the cost
model for the next tuning iteration (line 17).

Then, FamilySeer focuses on the current subgraph family (𝑓 ) and
begins the foresee tuning. Subgraphs inside the same family share
similar code structures and therefore their program candidates tend
to be estimated accurately with the shared family cost model. If
the family contains only one subgraph (i.e., 𝑠𝑐𝑢𝑟 ), which mean the
current subgraph has no similar subgraph, the foresee tuning is
skipped. Otherwise, FamilySeer restricts the scope to the subgraph
family 𝑓 and repeats the core tuning steps from line 9 to line 17.
Notably, the number of generated program candidates is set to
𝑔 × 𝑝 (line 16). The adjustable foresee proportion 𝑝 determines
the proportion of tuning round shared between the bottleneck
subgraph and less-bottleneck subgraph inside the same subgraph
family, and FamilySeer appends 𝑔 × 𝑝 program candidates to the
subgraph family in the foresee tuning.

The value of 𝑝 should be greater than 0 but less than 1. A larger
𝑓 increases the time budget of tuning less-bottleneck subgraph,
but may cause spending too much time and thus trivializing the
bottleneck subgraphs. A smaller 𝑝 has less effect on the entire
tuning process, but it requires a highly accurate cost model to
estimate the program candidates. Therefore we recommended to
set a larger 𝑝 if users allows sufficient tuning time, and vice versa.
By default, FamilySeer sets 𝑝 to 0.25, which we think has acceptable
overhead on the tuning time while is enough to adapt to the family
cost model.

Algorithm 1 Foresee tuning
Input: DL model 𝑁 , time budgets 𝐵
Input: 𝑝 ← adjustable foresee proportion

⊲ # of used budgets

1: 𝑏 = 0
2: subgraphs = construct_subgraphs(𝑁 )
3: families = construct_family(subgraphs)
4: for 𝑓 in families do
5:
6: 𝑔 = min(64, 𝐵/# of subgraphs) ⊲ generated candidates in each

𝐶 [𝑓 ] = initialize_cost_model(𝑓 )

tuning iteration
7: while 𝑏 < 𝐵 do
8:

𝑠𝑐𝑜𝑝𝑒 = subgraphs
for 𝑠 in 𝑠𝑐𝑜𝑝𝑒 do

⊲ consider all subgraphs
⊲ core tuning steps

𝑃 [𝑠] = calculate_potential(𝑠)

⊲ get the family of 𝑠𝑐𝑢𝑟
⊲ get the cost model of 𝑠𝑐𝑢𝑟

sorted(𝑠𝑐𝑜𝑝𝑒, key=<potential>, order=desc)
𝑠𝑐𝑢𝑟 = 𝑠𝑐𝑜𝑝𝑒.pop()
𝑓 = find_family(𝑠𝑐𝑢𝑟 )
𝑐𝑐𝑢𝑟 = 𝐶 [𝑓 ]
𝑔 = 𝑔 if 𝑠𝑐𝑜𝑝𝑒 = all subgraphs, 𝑔 × 𝑝 if 𝑠𝑐𝑜𝑝𝑒 = family
𝑟𝑒𝑠𝑢𝑙𝑡 += 𝑡𝑢𝑛𝑒(𝑠𝑐𝑢𝑟 , 𝑐𝑐𝑢𝑟 , candidates=𝑔)
𝑐𝑐𝑢𝑟 = train_cost_model(𝑟𝑒𝑠𝑢𝑙𝑡, 𝑐𝑐𝑢𝑟 )
𝑏 += 𝑔
if len(𝑓 ) > 1 then
𝑠𝑐𝑜𝑝𝑒 = 𝑓
repeat line 9 → line 17
𝑏 += 𝑔 × 𝑝

⊲ core steps end
⊲ update 𝑏
⊲ begin foreseeing
⊲ consider current subgraph family
⊲ foresee the family
⊲ update 𝑏

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

5.3 Multi-GPU Acceleration
Modern servers are equipped with multiple GPUs, allowing multiple
tasks to be deployed simultaneously. During the tuning process, the
tuner generates several program candidates and evaluates them on
real hardware. Even if we optimize the tuner with foresee tuning,
evaluating the candidate still consumes most of the tuning time.
The candidates are transformed programs of the subgraphs and can
be considered as independent tasks, therefore we can evaluate them
on GPUs in parallel. We use remote procedure call (RPC) service
to utilize multiple GPUs in a single server. Each GPU with the
server is registered as a RPC device. As the tuning begins, the tuner
generates several candidates and let the compiler transform these
candidates into transformation codes (e.g., CUDA codes). The RPC
service allocates each code to the registered GPU and collect the its
real latency. The RPC service manages the allocation frequency to
avoid evaluating multiple tasks on a single GPU at the same time,
which helps to avoid the latency interference across the tasks.

We also optimize the training efficiency of the cost model. The
tuner relies on the cost model to estimate candidates during each
tuning iteration, and the training data for the cost model comes
from the evaluating result of each candidate. We keep training the
cost model during the tuning procedure in order to gain better
accuracy on the subgraphs. Training the machine learning cost
model (e.g., XGBoost) on CPU only takes less than a second even
under thousands of evaluated candidates [21]. However, most of
the deep learning models has more than ten subgraphs, which
generate more than ten thousands of the evaluated candidates.

6

Table 3: The DL models used for evaluation.

Table 4: The hardware platforms used for evaluation.

Model

Model

Task

CNN

Transformer

ResNet50_v1 [12]
ResNet152_v2 [13]
Mobilenet [14]
Mobilenetv2 [27]
ViT-Huge [10]
BERT-Large [9]
RoBERTa-Large [20]
GPT2-Small [23]

Image
Classfication

Language
Translation

GPU Platform
Intel E5 2680 V4
NVIDIA V100 32G

CPU Platform
Intel Sliver 4210
N/A

CPU
GPU
RAM DDR4 2666 512G DDR4 2400 384G
System
GCC
CUDA N/A
N/A
Driver

Ubuntu20.04
9.3

11.2
460.80

As the tuning continues, each cost model are trained with more
evaluated candidates, which consumes dozens of seconds.

To minimize the training time of cost models, we move the cost
model from CPU to GPU. This method will not affect the process
of evaluating candidates since the cost model is trained after the
candidates have been evaluated.

6 EVALUATION
In this section, we are trying to answer the following question: 1)
How does FamilySeer compare to Ansor improve search efficiency?
2) What is the actual performance curve during the tuning process?
3) Can FamilySeer further improve the search quality?

6.1 Experimental Setup
We choose several Deep learning models and benchmark them in
CPU and GPU platforms (shown in Table 4). The CPU platform
is a dual-socket CPU node, each socket is a 10-core Intel Xeon
Sliver 4210 with hyper-threading enabled. Each CPU applies 256 GB
of RAM in four memory channels. The GPU platform is a dual-
socket GPU cluster with two NVIDIA V100 32G, each socket is
a 14-core Intel Xeon E5 2680 V4 with hyper-threading disabled.
We compare the search quality and search efficiency against the
state-of-the-art auto-tuning framework: Ansor (Commit: 64𝑐1𝑏79).
The chosen models are shown in Table 3. ResNet and Mobilenet
represent traditional image classification. BERT, RoBERTa ,and
GPT2 represents language translation. Vision Transformer (ViT)
represents image classification based on transformer. The batch
size is set to 1 since most inferences use one batch.

We let Ansor run the searching on its recommended time budgets.
Ansor suggests 900 for each subgraph on GPU while 800 on CPU.
For example, the ResNet50_v1 has 28 subgraphs. The recommended
time budgets is set to 25200. We set the beginning of the tuning
where each subgraph has been tuned once so the cost model can
be built.

6.2 Search Efficiency Improvement
The search efficiency can be described as the tuning time to an
optimal end-to-end inference time. The higher the search efficiency
is, the lower end-to-end inference time is reached. To demonstrate
the convergence of end-to-end performance, we choose three end-
to-end inference times from Ansor when getting 80%, 90%, and
100% of the inference time as our baseline. The results are shown
in Figure 7.

7

Figure 7(a) demonstrates the tuning time speedup on CPU. The
search efficiency of Ansor (black bars) is normalized to 1. FamilySeer
(gray bars) reduces search time by up to 12.1× while still reaching
the same search quality. Overall, FamilySeer has an average of
1.53×, 1.66×, 2.49× speedup on 80%, 90% and 100% of performance,
respectively. Model likes Mobilenetv2 is partitioned into several
Conv2D subgraphs with relatively close execution time, Softmax
subgraph, and Pooling subgraph. FamilySeer forms these subgraphs
into several subgraph families to provide pure training data for the
cost model inside the families. The highly accurate cost model
helps improve search efficiency. We can also find that the search
efficiency of models (Mobilenet, Bert-Large, RoBERTa-Large, and
GPT2-Small) has better improvement as the search process continue.
This is because these models tend to have many similar subgraphs
and can easily benefit from FamilySeer. Model likes ResNet50_v1
has varied kinds of Conv2D such as Contrib Conv2D. The diversity
of the subgraphs requires more candidate evaluation before the cost
model in the family can search for better candidates, which results
to a slight performance lag (0.85× at 90%) before converging. As the
search process continues, the training data enrich and performance
regain. ViT-Huge also has a slight performance lag (0.82× at 90%)
before converging. This is because evaluating subgraphs of ViT-
Huge on CPU requires more time than other models. When the cost
model has low accuracy with limited evaluated candidates, many
non-improving candidates are measured, resulting in a longer time
to reach the same end-to-end performance.

Figure 7(b) shows the tuning time speedup on GPU. Because the
GPU platform has 2 GPUs, we can measure the candidates parallelly
and speed up the training of the cost model. The dark blue bar chart
represents FamilySeer accumulated with the speedup of training
cost model on GPU. The blue bar chart uses all the techniques. It
shows that FamilySeer can reduce up to 7.9× search time and has
an average of 1.89×, 2.36×, 3.04× speedup on 80%, 90% and 100% of
performance, respectively. FamilySeer has demonstrated a stable im-
provement compared to Ansor in most models except ResNet50_v1.
We find that Both methods have similar peak performance, but
FamilySeer reaches the peak performance of the Resnet50_v1 much
earlier than Ansor. Therefore, FamilySeer enlarges the gap between
Ansor at the beginning. But as the tuning continues, the gap shrinks
until Ansor runs out of its time budget.

When moving the cost model from CPU to GPU, we can also
find extra performance improvement from 1.01× to 1.09× because
training cost model on GPU is much faster than on CPU when
the training data is extensive. The improvement becomes more
significant as long as the model has more subgraphs. For example,

(a) On Xeon Sliver 4210

(b) On NVIDIA V100

Figure 7: Speedup comparison between Ansor and FamilySeer. We compare the used time of FamilySeer and Ansor when
reaching Ansor’s 80%, 90% and 100% performance of the inference time.

model likes MobileNetv2 has more than 30 subgraphs. Thus it
has more than 30000 candidates as training data, resulting in a
speedup of 1.09× and thousands of seconds being saved. Model
likes RoBERTa-Large have less than ten subgraphs. The speedup
is 1.01× and hundreds of seconds have been saved. Training cost
model on GPU can also benefit from a longer time budget, meaning
more candidates are measured and treated as training data.

The speedup of using parallel GPU is from 1.56× to 1.79× com-
pared to measuring candidates sequentially. Paralleling measure-
ment on GPU can benefit those subgraphs which have longer execu-
tion time. For example, most of the execution time of the subgraphs
in GPT2-Small are more than dozens of milliseconds and therefore
has a speedup of 1.72×. Having more subgraphs and evaluating can-
didates also benefit from parallel measurement such as MobileNetv2
(1.79×). The speedup of RoBERTa-Large is 1.56×, which is the low-
est among all the other models. This is because RoBERTa-Large has
less than ten subgraphs and the execution time of each subgraph is
less than a millisecond.

6.3 Search Quality Improvement
Although both Ansor and FamilySeer search on the same search
space, we explore the search space differently. We give both Ansor

8

and FamilySeer sufficient time budget and compare end-to-end
performance.

Table 5 and 6 show end-to-end performance on CPU and GPU
respectively. Models like Mobilenetv2 on GPU has shown an im-
provement of 8%. Other models have also shown an improvement
of 1-2%. The result on CPU shows better performance by up to
1.17× on Mobilenet. Note that as the searching continues, the peak
performance of the subgraphs is explored, and it makes even 1%
of the improvement difficult. Although we still search these candi-
dates on the same search space, the difference between Ansor and
FamilySeer is the time converges to the same performance. Ansor
may still reach the same performance as long as it has been given
more searching budget or explore the entire search space.

6.4 Turing Performance Curve
We analyze the tuning curve of Ansor and FamilySeer. We evaluate
and report the end-to-end latency change each time better candi-
dates have been generated to show the actual end-to-end inference
time during the tuning process. Figure 8 shows the tuning curve
of GPT2-Small on GPU. When the search begins, FamilySeer is
almost the same compared to Ansor. As the tuning continues, each
subgraph family gains sufficient training data and therefore, better
prediction accuracy. We can see many nearly vertical curves on

80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%1.01.52.02.53.0SpeedUp12.14.2ResNet50_v1ResNet152_v2MobilenetMobilenetv2ViT-HugeBERT-LargeRoBERTa-LargeGPT2-SmallGeomeanAnsorFamilySeer80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%80%90%100%12345SpeedUp7.95.5ResNet50_v1ResNet152_v2MobilenetMobilenetv2ViT-HugeBERT-LargeRoBERTa-LargeGPT2-SmallGeomeanAnsorFamilySeerFamilySeer+GPUFamilySeer+GPU+PARALLELTable 5: End-to-end performance comparison on CPU.

ResNet50_v1
ResNet152_v2
Mobilenet
Mobilenetv2
ViT-Huge
BERT-Large
RoBERTa-Large
GPT2-Small

Ansor
12.99 ms
36.17 ms
1.132 ms
1.79 ms
432.451 ms
149.1 ms
146.4 ms
5049 ms

FamilySeer
12.84 ms
36.08 ms
0.989 ms
1.53 ms
422.504 ms
146 ms
141.4 ms
4945 ms

Speedup
1.012x
1.002x
1.145x
1.170x
1.024x
1.021x
1.035x
1.021x

Table 6: End-to-end performance comparison on GPU.

ResNet50_v1
ResNet152_v2
Mobilenet
Mobilenetv2
ViT-Huge
BERT-Large
RoBERTa-Large
GPT2-Small

Ansor
1.8 ms
5.42 ms
0.244 ms
0.37 ms
50.76 ms
17.99 ms
17.67 ms
324.7 ms

FamilySeer
1.78 ms
5.24 ms
0.241 ms
0.34 ms
50.56 ms
17.85 ms
16.98 ms
320.7 ms

Speedup
1.011x
1.034x
1.011x
1.087x
1.004x
1.008x
1.041x
1.013x

FamilySeer. This is because FamilySeer utilizes foresee tuning and
subgraphs inside the same families can get better candidates under
a smaller time budget. Eventually, both Ansor and FamilySeer reach
the bottleneck of our tuning performance, and we can have better
performance compared to Ansor.

Figure 8: End-to-end latency curve of Ansor and FamilySeer
when tuning GPT2-Small on NVIDIA V100.

7 RELATED WORK
Deep learning compilers take deep learning Model as input and op-
timize the execution of deep learning model. The compilers describe
computation and scheduling using their own intermediate represen-
tation. There are many Well known deep learning compilers such as
XLA [2], nGraph [8], TVM [6], TACO, Tensor Comprehensions [30],
Halide [24], Tiramisu [4] and Glow [25].

9

Many techniques have been introduced to optimize deep learning
model on Deep learning compiler. AutoTVM [7] is a templated-
guided search framework using handwritten template to optimize
computation definition. The template defines the structure of the
tensor expression and provide several tunable parameters. The
compiler search and match these expressions with the template
and tuning for optimal parameter. The size of the search space is
defined by the template itself.

Ansor [33] is a search-based framework using cost model to
guide and optimize deep learning model. The compiler partition
model into multiple subgraphs and framework generate many trans-
formation codes according to the optimization rules. These trans-
formation codes are estimated by the cost model to find the possible
better code.

The large search space defined by the optimization rules mean
the accuracy of the cost model will affect the efficiency of the search-
based framework directly. Thus, many works focus on providing
highly accurate cost model like Tenset [34] use pretrained cost
model. Some works optimize the cost model itself [3, 17, 29]. Some
research focus on subgraph optimization such as fusing operation
and etc. [11, 15, 16, 31].

8 CONCLUSION
We propose FamilySeer, a new auto-tuning framework for deep
learning compilers to exploit the similarity of subgraphs and gen-
erate optimal code transformations. We evaluate the possibility of
forming a similar subgraph into a subgraph family to improve the
search quality and efficiency for the state-of-art auto-tuning frame-
works. Furthermore, we also utilize the advantage of the subgraph
families to accelerate the converge to optimal code. FamilySeer
outperforms the existing searching framework by up to 3.04× on
the search efficiency and further explores the search quality by up
to 1.17×. We hope that FamilySeer can help improve the search
efficiency of the auto-tuning in deep learning compiler to become
more efficient.

REFERENCES
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al.
2016. Tensorflow: A system for large-scale machine learning. In 12th {USENIX}
symposium on operating systems design and implementation ({OSDI} 16). 265–283.
[2] Martín Abadi, Michael Isard, and Derek G Murray. 2017. A computational
model for TensorFlow: an introduction. In Proceedings of the 1st ACM SIGPLAN
International Workshop on Machine Learning and Programming Languages. 1–7.
[3] Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel
Abdous, Taha Arbaoui, Karima Benatchba, et al. 2021. A Deep Learning Based
Cost Model for Automatic Code Optimization. Proceedings of Machine Learning
and Systems 3 (2021).

[4] Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo, Ab-
durrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, and Saman
Amarasinghe. 2019. Tiramisu: A polyhedral compiler for expressing fast and
portable code. In 2019 IEEE/ACM International Symposium on Code Generation
and Optimization (CGO). IEEE, 193–205.

[5] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun
Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A flexible and
efficient machine learning library for heterogeneous distributed systems. arXiv
preprint arXiv:1512.01274 (2015).

[6] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen
Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}:
An automated end-to-end optimizing compiler for deep learning. In 13th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}
18). 578–594.

[7] Tianqi Chen, Lianmin Zheng, Eddie Yan, Ziheng Jiang, Thierry Moreau, Luis
Ceze, Carlos Guestrin, and Arvind Krishnamurthy. 2018. Learning to optimize

                       8 V H G  7 L P H   V                                                         , Q I H U H Q F H  / D W H Q F \   P V  ) D P L O \ 6 H H U $ Q V R U[30] Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya Goyal,
Zachary DeVito, William S Moses, Sven Verdoolaege, Andrew Adams, and Albert
Cohen. 2018. Tensor comprehensions: Framework-agnostic high-performance
machine learning abstractions. arXiv preprint arXiv:1802.04730 (2018).

[31] Haojie Wang, Jidong Zhai, Mingyu Gao, Zixuan Ma, Shizhi Tang, Liyan Zheng,
Yuanzhi Li, Kaiyuan Rong, Yuanyong Chen, and Zhihao Jia. 2021. {PET}: Optimiz-
ing Tensor Programs with Partially Equivalent Transformations and Automated
Corrections. In 15th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 21). 37–54.

[32] Weihong Wang, Jie Yang, Jianwei Xiao, Sheng Li, and Dixin Zhou. 2014. Face
recognition based on deep learning. In International Conference on Human Cen-
tered Computing. Springer, 812–820.

[33] Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer
Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, et al. 2020. An-
sor: Generating high-performance tensor programs for deep learning. In 14th
{USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}
20). 863–879.

[34] Lianmin Zheng, Ruochen Liu, Junru Shao, Tianqi Chen, Joseph E Gonzalez, Ion
Stoica, and Ameer Haj Ali. 2021. TenSet: A Large-scale Program Performance
Dataset for Learned Tensor Compilers. In Thirty-fifth Conference on Neural Infor-
mation Processing Systems Datasets and Benchmarks Track (Round 1).

tensor programs. arXiv preprint arXiv:1805.08166 (2018).

[8] Scott Cyphers, Arjun K Bansal, Anahita Bhiwandiwalla, Jayaram Bobba, Matthew
Brookhart, Avijit Chakraborty, Will Constable, Christian Convey, Leona Cook,
Omar Kanawi, et al. 2018. Intel ngraph: An intermediate representation, compiler,
and executor for deep learning. arXiv preprint arXiv:1801.08058 (2018).

[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).

[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-
aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al. 2020. An Image is Worth 16x16 Words: Trans-
formers for Image Recognition at Scale. In International Conference on Learning
Representations.

[11] Jingzhi Fang, Yanyan Shen, Yue Wang, and Lei Chen. 2020. Optimizing DNN
computation graph using graph substitutions. Proceedings of the VLDB Endowment
13, 12 (2020), 2734–2746.

[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Identity mappings
in deep residual networks. In European conference on computer vision. Springer,
630–645.

[14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).

[15] Zhihao Jia, Oded Padon, James Thomas, Todd Warszawski, Matei Zaharia, and
Alex Aiken. 2019. TASO: optimizing deep learning computation with automatic
generation of graph substitutions. In Proceedings of the 27th ACM Symposium on
Operating Systems Principles. 47–62.

[16] Zhihao Jia, James Thomas, Tod Warszawski, Mingyu Gao, Matei Zaharia, and
Alex Aiken. 2019. Optimizing dnn computation with relaxed graph substitutions.
SysML 2019 (2019).

[17] Sam Kaufman, Phitchaya Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip
Roy, Amit Sabne, and Mike Burrows. 2021. A Learned Performance Model for
Tensor Processing Units. Proceedings of Machine Learning and Systems 3 (2021).
[18] Pritam Kore and Suchitra Khoje. 2019. Obstacle Detection for Auto-Driving Using
Convolutional Neural Network. In Proceedings of the 2nd International Conference
on Data Engineering and Communication Technology. Springer, 269–278.
[19] Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang, Zhongzhi
Luan, Lin Gan, Guangwen Yang, and Depei Qian. 2021. The Deep Learning
Compiler: A Comprehensive Survey. IEEE Transactions on Parallel and Distributed
Systems 32, 3 (2021), 708–727. https://doi.org/10.1109/TPDS.2020.3030548
[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).

[21] Rory Mitchell, Andrey Adinets, Thejaswi Rao, and Eibe Frank. 2018. Xgboost:
Scalable GPU accelerated learning. arXiv preprint arXiv:1806.11248 (2018).
[22] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019.
Pytorch: An imperative style, high-performance deep learning library. Advances
in neural information processing systems 32 (2019), 8026–8037.

[23] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. 2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.

[24] Jonathan Ragan-Kelley, Connelly Barnes, Andrew Adams, Sylvain Paris, Frédo
Durand, and Saman Amarasinghe. 2013. Halide: a language and compiler for
optimizing parallelism, locality, and recomputation in image processing pipelines.
Acm Sigplan Notices 48, 6 (2013), 519–530.

[25] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer Deng,
Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman Leven-
stein, et al. 2018. Glow: Graph lowering compiler techniques for neural networks.
arXiv preprint arXiv:1805.00907 (2018).

[26] Jaehun Ryu and Hyojin Sung. 2021. MetaTune: Meta-Learning Based Cost Model
for Fast and Efficient Auto-tuning Frameworks. arXiv preprint arXiv:2102.04199
(2021).

[27] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-
Chieh Chen. 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. In
Proceedings of the IEEE conference on computer vision and pattern recognition.
4510–4520.

[28] Shashi Pal Singh, Ajai Kumar, Hemant Darbari, Lenali Singh, Anshika Rastogi,
and Shikha Jain. 2017. Machine translation using deep learning: An overview.
In 2017 international conference on computer, communications and electronics
(comptelix). IEEE, 162–167.

[29] Benoit Steiner, Chris Cummins, Horace He, and Hugh Leather. 2021. Value
learning for throughput optimization of deep learning workloads. Proceedings of
Machine Learning and Systems 3 (2021).

10

