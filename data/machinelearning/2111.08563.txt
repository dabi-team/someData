Rank-Regret Minimization

Xingxing Xiao §‡, Jianzhong Li ‡§
§Department of Computer Science and Technology, Harbin Institute of Technology, Harbin, China
‡Faculty of Computer Science and Control Engineering, Shenzhen Institute of Advanced Technology
Chinese Academy of Sciences, Shenzhen, China
{xiaoxx, lijzh}@hit.edu.cn

2
2
0
2

r
a

M
9

]

G
L
.
s
c
[

2
v
3
6
5
8
0
.
1
1
1
2
:
v
i
X
r
a

Abstract—Multi-criteria decision-making often requires ﬁnd-
ing a small representative set from the database. A recently
proposed method is the regret minimization set (RMS) query.
RMS returns a size r subset S of dataset D that minimizes the
regret-ratio (the difference between the score of top-1 in S and
the score of top-1 in D, for any possible utility function). RMS
is not shift invariant, causing inconsistency in results. Further,
existing work showed that the regret-ratio is often a “made up”
number and users may mistake its absolute value. Instead, users
do understand the notion of rank. Thus it considered the problem
of ﬁnding the minimal set S with a rank-regret (the rank of top-1
tuple of S in the sorted list of D) at most k, called the rank-regret
representative (RRR) problem.

Corresponding to RMS, we focus on the min-error version of
RRR, called the rank-regret minimization (RRM) problem, which
ﬁnds a size r set to minimize the maximum rank-regret for all
utility functions. Further, we generalize RRM and propose the
restricted RRM (i.e., RRRM) problem to optimize the rank-regret
for functions restricted in a given space. Previous studies on both
RMS and RRR did not consider the restricted function space.
The solution for RRRM usually has a lower regret level and can
better serve the speciﬁc preferences of some users. Note that RRM
and RRRM are shift invariant. In 2D space, we design a dynamic
programming algorithm 2DRRM to return the optimal solution
for RRM. In HD space, we propose an algorithm HDRRM
that introduces a double approximation guarantee on rank-
regret. Both 2DRRM and HDRRM are applicable for RRRM.
Extensive experiments on the synthetic and real datasets verify
the efﬁciency and effectiveness of our algorithms. In particular,
HDRRM always has the best output quality in experiments.

Index Terms—Top-k query; Skyline; Multi-criteria decision-

making; Regret-ratio; Rank-regret

I. INTRODUCTION

It is a signiﬁcant problem to produce a representative tuple
set from the database for multi-criteria decision-making. The
problem is fundamental in many applications where the user is
only interested in some or even one tuple in a potentially huge
database. Consider the following example. Alice visits a large
car database where each car tuple has two attributes, horse
power (HP) and miles per gallon (MPG). Alice is looking for
a car with high MPG and high HP. There is a trade-off between
these two goals, since the increase in horse power comes at the
cost of reduced fuel economy. And it may be impossible for
Alice to browse every car tuple. How can the database provide
a representative set to best assist Alice in decision-making?

One approach is the top-k query [13], which requires a
predeﬁned utility function to model user preferences. The func-
tion assigns a utility/score to each tuple in database and the

query returns the k tuples with highest scores. A widely used
function is the weighted linear combination of tuple attributes,
i.e., (cid:80) wiAi. For example, Alice gives weights 70% and
30% to MPG and HP respectively, and the utility function is
0.7×MPG+0.3×HP. However, Alice may roughly know what
she is looking for, and it is difﬁcult to accurately determine
the function. Instead of asking users for their preferences,
Borzsony et. al. [4] proposed the skyline query which returns
the smallest set that contains the best tuple for any monotonic
utility function. However, it has the disadvantage of returning
an unbounded number of tuples.

Recently, the regret minimization set (RMS) query was
proposed by Nanongkai et al. [19] to address the issues of
previous methods. It needs no feedback from users and its
output size is controllable. RMS aims to ﬁnd a size r subset
S of the dataset D such that for any user, the top-1 tuple of S
is a good approximation of the top-1 of D. Nanongkai deﬁned
the regret-ratio to measure the “regret” level of a user if s/he
gets the best of S but not the best in D. Given a function f ,
let w be the highest utility of tuples in D and w(cid:48) be that of S.
Then the regret-ratio for f is (w − w(cid:48))/w. And RMS returns
a size r set that minimizes the maximum regret-ratio for all
possible utility functions.

Nevertheless, the RMS query and regret-ratio have some
shortcomings. First, RMS is not shift invariant (i.e., if we add a
ﬁxed constant to all values in some attribute, RMS may change
the result), which causes inconsistency in outputs. There are
attributes (e.g., temperature) that shift values when converting
between scales (e.g., °C, °F, and K). Similarly, the altitudes
and spatial coordinates are shifted as the reference point
changes. After shifting, the dataset is essentially unchanged
and query results should be the same. Paradoxically, RMS
may have different outputs. Further, RMS assumes a larger
value is preferable in each attribute and all values are non-
negative. To make the assumption satisﬁed, the data has to
be shifted. For smaller-preferred attributes (e.g., price), RMS
negates values in them and yields some negative ones. For
negative values (e.g., temperature), it shifts tuples to eliminate
them. However, the shift degree seriously affects the results
of RMS and introduces inconsistency.

Second, as shown in [3], the utility/regret-ratio is usually
an artiﬁcial number with no direct signiﬁcance, and users may
not understand its absolute value. For example, wine ratings
are on a 100-point scale. Wines rated below 80, which have

 
 
 
 
 
 
regret-ratios about 0.2 and seem attractive to novice drinkers,
are almost never ofﬁcially sold in stores. On the shopping
website Taobao, the logistics score of Adidas ﬂagship store
is 4.8 points (out of 5), which is the median but guarantees
a regret-ratio less than 0.04. The average logistics level on
Taobao is far worse than that on its competitor JD.com. It is
also shown that a small range in regret-ratio can include a
large fraction of dataset.

Relative to the regret-ratio, users do understand the notion
of rank. Consequently, Asudeh et al. [3] measured the user
regret by the rank. Given a function f , they deﬁned the rank-
regret of a set S to be the rank of the best tuple of S in the
sorted list of dataset D w.r.t. f . Obviously, if the rank-regret
of S for any possible utility function is at most k, then it
contains a top-k tuple for each user. For example, a subset of
wines with a rank-regret of 6 should contain one of the top-6
wines in the mind of any user, which is close to the top-1.

Specifying the output size is critical due to some consid-
erations such as the website display size, upload speed to
the cloud and communication bandwidth between hosts. In
this paper, we investigate the rank-regret minimization (RRM)
problem that ﬁnds a size r set to minimize the maximum rank-
regret for all utility functions. Further, we generalize RRM
and propose the restricted rank-regret minimization (RRRM)
problem with an additional input, the restricted function space
U. RRRM no longer pays attention to all possible functions
and aims to minimize the rank-regret for any function in U.
U may be the candidate space of function used by a user.
Speciﬁed directly by the user or mined by a learning algorithm
[14], [15], [21], [23],
the obtained function is inherently
inaccurate. However, as shown in [9], [16]–[18], it can be used
as a rough guide and expanded into a candidate space U. In
addition, U can be designated by experts and the functions not
in U are impossible for users to use. Under the same settings,
the solution of RRRM usually has a lower rank-regret than
RRM, owing to fewer functions in U. It can better serve the
speciﬁc preferences of some users. Contrary to RMS, both
RRM and RRRM are shift invariant.

Asudeh et al. [3] focused on the dual formulation of
RRM, the rank-regret representative (RRR) problem. Given a
threshold k, it ﬁnds the minimum set with a rank-regret at most
k. In 2D space, they proposed an O(n2 log n) time and O(n2)
space approximation algorithm 2DRRR that returns a set with
size and rank-regret at most rk and 2k respectively, where rk
is the minimal size of sets with rank-regrets at most k. In high-
dimensional (HD) space (i.e., dimension d > 2), based on the
combinatorial geometry notion of k-set [11], they presented
an O(|W |knLP(d, n)) time and O(|W |k) space algorithm
MDRRR with a logarithmic approximation-ratio on size and
a rank-regret of k, where LP(d, n) is the time for solving a
linear programming with d variables and n constrains, and
|W | is the number of k-sets (its best-known lower-bound
log n) [22]). As shown in [3], MDRRR is quite
is nd−1eΩ(
impractical and does not scale beyond a few hundred tuples.
Therefore,
they showed a randomized version, MDRRRr,
which reduces the time complexity to O(|W |(nd + k log k))

√

but no longer has a guaranteed rank-regret. Finally, they gave
a heuristic algorithm MDRC based on space partitioning.
Asudeh et al. [3] didn’t consider the restricted function space.
In this paper, we present several theoretical results as well
as practical advances for RRM and RRRM, in both 2D and
HD cases. In 2D space, we design a dynamic programming
algorithm 2DRRM to return the optimal solution for RRM,
indicating that RRM is in P for d = 2. It can be transformed
into an exact algorithm for RRR and is a huge improvement
over 2DRRR [3]. In HD space, we discretize the continuous
function space into a bounded size set of utility functions and
prove from two different perspectives that if a tuple set has a
small rank-regret for functions in the discretized set, then it
is approximately the same for the full space. We then convert
the problem into linear number of set-cover [5] instances. This
leads to the design of the algorithm HDRRM, which has a
double approximation guarantee on rank-regret. The output of
HDRRM always has the lowest rank-regret in experiments.
Both 2DRRM and HDRRM can be generalized to RRRM.
The main contributions of this paper are listed below.

• We generalize RRM and propose the RRRM problem,
which aims to minimize the regret level for any utility
function in a restricted space. Under the same settings, the
solution of RRRM usually has a better quality.

• We prove that RRM and RRRM are shift invariant. Second,
we provide a lower-bound Ω(n/r) on rank-regret, indicating
that there is no algorithm with a rank-regret upper-bound
independent of the data size n. In addition, we show that the
restricted skyline [9] is a set of candidate tuples for RRRM.
Relatively, skyline tuples [4] are candidates for RRM.
• In 2D space, we design an O(n2 log n) time and O(n2)
space algorithm 2DRRM to return the optimal solution for
RRM. It can be used to ﬁnd the optimal solution for RRR
and be applied for RRRM through some modiﬁcations.
• In HD space, we propose an O(n log2 n) time and
O(n log n) space algorithm HDRRM for RRM that returns
a size r set and approximates the minimal rank-regret for
ln ln n ) size set. It is the only HD algorithm that has
any O(
a rank-regret guarantee and is suitable for RRRM. HDRRM
always has the best output quality in experiments.

r

• Extensive experiments on the synthetic and real datasets
verify the efﬁciency and effectiveness of our algorithms.

i=1 t[i]2) 1

II. PROBLEM DEFINITION
Let D be a dataset containing n tuples with d numeric
attributes and Ai be the i-th attribute. Given a tuple t, its value
on Ai is denoted by t[i]. The L2-norm of t is abbreviated as
(cid:107)t(cid:107) = ((cid:80)d
2 . Assume that on each attribute, a larger
value is preferred and the range is normalized to [0, 1]. For
each Ai, there is a tuple t ∈ D with t[i] = 1, named as the
i-th dimensional boundary tuple. Deﬁne the basis [1] of D,
denoted by B, to be the set of all boundary tuples. Assume that
the dimensionality d is a ﬁxed constant, which is reasonable
in many scenarios and appears in many related works (e.g.,
[2], [3], [29]–[31]). Given an integer k ≥ 1, let [k] denote the
integer set {1, 2, . . . , k}.

Suppose the user preference is modeled by an unknown
utility function, which assigns a non-negative utility/score
f (t) to each tuple t ∈ D. To avoid several complicated but
uninteresting ”boundary cases”, assume that no two tuples
have the same utility in D. Following [1]–[3], [25], [26], [31],
focus on the popular-in-practice linear utility functions, shown
to effectively model the way users evaluate trade-offs in real-
life multi-objective decision-making [21]. A utility function f
is linear, if

f (t) = w(u, t) =

u[i]t[i]

d
(cid:88)

i=1

where u = (u[1], · · · , u[d]) is a d-dimensional non-negative
real vector and u[i] measures the importance of attribute Ai.
In the following, refer f by its utility vector u and use them
interchangeably. A tuple t outranks a tuple t(cid:48) based on u, if
w(u, t) > w(u, t(cid:48)). A user wants a tuple which maximizes the
utility w.r.t. his/her utility function. Given a utility vector u
and an integer k ≥ 1, let wk(u, D) be the k-th highest utility
of tuples in D. For brevity, w(u, D) = w1(u, D). The tuple
t ∈ D with w(u, t) = w(u, D) is called the highest utility
tuple of D for u. Set

Φk(u, D) = {t ∈ D | w(u, t) ≥ wk(u, D)}
to be the set of top-k tuples w.r.t. u. For each t ∈ D, ∇u(t)
denotes the rank of t in the sorted list of D in descending
order based on u. There are exactly ∇u(t) − 1 tuples in D
that outrank t according to u. Through ∇u(t), Φk(u, D) is
also deﬁned as the set {t ∈ D | ∇u(t) ≤ k}. Next, deﬁne the
rank-regret of a tuple set for a given utility vector.

Deﬁnition 1: Given a set S ⊆ D and a utility vector u, the
rank-regret of S for u, denoted by ∇u(S), is the minimum
rank of tuples in S based on u, i.e.,

∇u(S) = min
t∈S

∇u(t).

Intuitively, when the best tuple of S approaches that of D,
the rank-regret of S becomes smaller, indicating that the user
feels less regretful with S.

In reality, it is difﬁcult to accurately determine the utility
vector. Speciﬁed directly by the user or mined by a learning
algorithm, the obtained vector is inherently inaccurate. In the
former case, it is impossible for users to reasonably quantify
the relative importance of various attributes with absolute
precision. In the latter, the mined vector is a rough estimate but
not an exact representation of user preferences. A natural way
is to expand a single vector into a vector space. Accordingly,
deﬁne the rank-regret for a given set of utility vectors.

Deﬁnition 2: Given a set S ⊆ D and a set U of utility
vectors, the rank-regret of S for U, denoted by ∇U(S), is the
maximum rank-regret of S for all vectors in U, i.e.,
∇U(S) = max
u∈U

∇u(S).

Intuitively, ∇U(S) measures the rank of best tuple of S w.r.t
U in the worst case. A set S ⊆ D has ∇U(S) ≤ k, if ∀u ∈
U, w(u, S) ≥ wk(u, D). Notice that ∇U(S) is a monotonic
decreasing function. Given two sets R and S, if R ⊆ S ⊆ D,
then ∇U(R) ≥ ∇U(S).

With no knowledge about user preferences, our goal is
to ﬁnd a given size set with a small rank-regret for any
linear utility function. Intuitively, the class of linear functions
corresponds to the d-dimensional non-negative orthant

L = {(u[1], . . . , u[d]) | ∀i ∈ [d], u[i] ≥ 0}.
Consequently, we deﬁne the rank-regret minimization (RRM)
problem as follows.

Deﬁnition 3 (RRM Problem): Given a dataset D and an
integer r ≥ 1, compute a size r subset of D that minimizes
the rank-regret for L, i.e., return a set

S∗ = arg min
S⊆D:|S|≤r

∇L(S).

Through some prior knowledge about user preferences [9],
[16]–[18], assume that the utility vector lies in a restricted
space and propose the restricted RRM problem.

Deﬁnition 4 (RRRM Problem): Given a dataset D, a set of
utility vectors U ⊆ L and an integer r ≥ 1, compute a size r
subset of D that minimizes the rank-regret for U, i.e., return
a set

S∗ = arg min
S⊆D:|S|≤r

∇U(S).

When U = L, RRRM degenerates into RRM. The most
relevant researches on the restricted space are [9], [16]–[18].
U was assumed as a convex polytope [9], [18], a hyper-sphere
[17] or an axis-parallel hyper-rectangle [16]. They did not
consider minimizing rank-regret and controlling the output
size. In this paper, we assume that U can be any convex space,
which is more generalized than the previous researches.

The solution of RRM is close to that of RMS [19] when tu-
ples are uniformly distributed by utility. Table I shows a dataset
with 7 tuples over two attributes and Figure 1 shows them as
points in 2D space. When r = 1, the solutions for RRM and
RMS are {t3} and {t4} respectively. In terms of rank-regrets
and regret-ratios, the two are close. However, tuples may be
clustered in a small utility range. RMS pursues approximations
on the absolute utility (i.e., regret-ratio), possibly resulting in
a signiﬁcant increase on rank-regret. In Table I, add 4 to each
value on attribute A2 and obtain the shifted tuples shown in
Figure 2. Since the updated values are in the top range, RMS
directly ignores A2 and seeks the largest value on A1, i.e., the
result of RMS becomes {t7}, indicating that RMS is not shift
invariant. But t7 has the worst rank on A2. And the solution
of RRM is still {t3}. In short, minimizing regret-ratio does
not typically minimize rank-regret.

Asudeh et al. [3] focused on the dual version of RRM, called
RRR, i.e., ﬁnd the minimum set that fulﬁll a given rank-regret
threshold. Depending on the search needs of users, different
versions can be applied. A solver for RRM can be easily
adopted for RRR by a binary search with an additional log n
factor in the running time. The introduction shows several
situations where it is critical to specify the output size. The
output size also represents the effort required by the user to
make a decision. RRR cannot directly control the output size
and may return a huge set. Users may be unable or unwilling to
browse thousands of tuples. A large representative set provides

TABLE I
A 2D DATASET

A1
0
0.4
0.57
0.79
0.2
0.35
1

A2
1
0.95
0.75
0.6
0.5
0.3
0

Rank-Ratio
7
4
3
4
6
6
7

Regret-Ratio
100%
60%
43%
40%
80%
70%
100%

t1
t2
t3
t4
t5
t6
t7

Fig. 1. Tuples of Table I in 2D
space

Fig. 2. Shifted Tuples

Fig. 3. Proof Idea of Theorem
2

little help for decision-making. Therefore, we usually would
like to return only a limited number r of tuples.

For a huge dataset, due to the range [1, n], RMS/RRMS
may have a solution with a rank-regret that seems large. But
in fact the regret level of the user is not high. For datasets
of different sizes, the same rank-regret has different practical
meanings. Similar to regret-ratio, we can normalize the range
to [0, 1] (i.e., divide rank-regrets by n) and equivalently use
percentages to represent user regret. Expressing rank in the
form of percentage is much common. For example, highly
cited papers are usually deﬁned as those that rank in the top
1% by citations in the ﬁeld and publication year.

Complexity Analysis: Asudeh et al. [3] proved that RRM
is NP-Complete for d ≥ 3. Since RRM is the sub-problem
of RRRM, RRRM is NP-Hard for d ≥ 3. In this paper, we
propose a polynomial time exact algorithm for RRM when
d = 2, which is applicable for RRRM. It supplements the
previous analysis and shows that in 2D space, RRM and even
RRRM are in P.

III. THEORETICAL PROPERTY

In this section, we introduce a few theoretical properties
about the RRM and RRRM problems. First, we claim that
both problems are shift invariant. Second, by proving a lower-
bound on the rank-regret, we show that there is no algorithm
for RRM with an upper-bound guarantee independent of the
data size. Last, we clarify that the restricted skyline [9] is
the set of candidate tuples for RRRM. Correspondingly, the
skyline [27] is the candidate set for RRM.

A. Shift Invariance

Given two datasets D = {t1, ..., tn} and D(cid:48) = {t(cid:48)

n},
D(cid:48) is a shifting of D, if there are d non-negative reals λ1,
..., λd such that ∀i ∈ [n] and ∀j ∈ [d], t(cid:48)
i[j] = λj + ti[j].
Shift invariant means that for any dataset D, shifting does not
change the optimal solution of problem. As shown in Section
II, RMS is not shift invariant. We now claim that both RRM
and RRRM satisfy the property.

1, ..., t(cid:48)

Theorem 1: RRM and RRRM are shift invariant.

1, ..., t(cid:48)

Proof: Consider a dataset D = {t1, ..., tn}. Suppose that
n} is a shifting of D, i.e., there are positive reals
i[j] = λj +ti[j].
j=1 u[j]λj +
j=1 u[j]λj + w(u, ti). Thus the rank of t(cid:48)
i

D(cid:48) = {t(cid:48)
λ1, λ2, ..., λd such that ∀i ∈ [n] and ∀j ∈ [d], t(cid:48)
For any u ∈ U, w(u, t(cid:48)
j=1 u[j]ti[j] = (cid:80)d
(cid:80)d

i[j] = (cid:80)d

i) = (cid:80)d

j=1 u[j]t(cid:48)

in D(cid:48) w.r.t. u is the same as that of ti in D. And the rank-
regret of any set w.r.t. u remains the same after shifting. So
does the rank-regret w.r.t. U. Thus RRM (let U be L) and
RRRM are shift invariant.

For lack of space, the missing proofs in this paper can be

found in [28].

B. Lower-Bound

For RMS, Xie et al. [30] showed that there is a dataset over
d attributes such that the minimum regret-ratio for any size r
set is Ω(r−2/(d−1)). Correspondingly, we show a lower-bound
on the rank-regret.

Theorem 2: Given integers r ≥ 1 and d ≥ 2, there is a
dataset D of n d-dimensional tuples such that for any size r
set S ⊆ D, ∇L(S) is Ω(n/r).

Proof: First, assume d = 2. Our construction is inspired
by Nanongkai et al. [19]. We deﬁne the angle of a tuple/point
t to be arctan t[2]
t[1] . Let D be the size n set

{tθ = (cos θ, sin θ) | θ ∈ {0,

π
2(n − 1)

, · · · ,

(n − 1)π
2(n − 1)

}}.

π

All tuples (black dots in Figure 3) lie on a quarter-arc in R2
+
with radius 1 centered at the origin. Then the angle of tθ
is θ. Each tuple in D is the highest utility tuple for some
vectors in L. Consider a size r set S ⊆ D. Let the angles
of tuples in S are ϑ1 < ϑ2 < · · · < ϑr. Let ϑ0 = 0 and
ϑr+1 = π/2, i.e., tϑ0 = (1, 0) and tϑr+1 = (0, 1). Let p-O-q
be the angle subtended between lines obtained by joining the
origin O = (0, 0) with two points p and q. Consider angles
tϑi-O-tϑi+1 for all i ∈ {0, · · · , r}. Note that the sum of these
r + 1 angles is exactly π/2. Therefore, at least one angle is no
2(r+1) . Let the angle be tϑi-O-tϑi+1 . The difference
less than
between angles of two adjacent tuples in D is
2(n−1) . Thus
there are Ω(n/r) tuples in D whose angles are larger than
ϑi but less than ϑi+1. If i ∈ [r − 1], then set α = ϑi+ϑi+1
and consider the utility vector u = (cos α, sin α). Note that
the utility of each tuple t ∈ D based on u is the L2-distance
between the origin O and the projection of t onto line O-
(cos α, sin α). And the tuples in D whose angles lie between
ϑi and ϑi+1 have utilities larger than those of tϑi and tϑi+1,
i.e., outrank tϑi and tϑi+1 based on the utility vector u. See
Figure 3. In addition, tϑi and tϑi+1 are the top-2 tuples of S
for u. Thence the rank-regret of S for u is Ω(n/r). If i = 0,

π

2

then set α = 0. If i = r, set α = π/2. With the same analysis,
∇u(S) is also in Ω(n/r), and so is ∇L(S).

If d > 2, for each tuple in D, keep the values of ﬁrst two
attributes, and set those of the subsequent attributes to 1. Then
we are able to get the same result.

The difference between the bounds for RMS and RRM is
not just the difference in scale ([0, 1] for regret-ratio and [1, n]
for rank-regret). When d = 2, multiplying Ω(r−2/(d−1)) by
n results in a looser bound Ω(n/r2). For RMS, there are
several algorithms [1], [6], [19], [30] with an upper-bound
on the regret-ratio, independent of n. It is obvious that there
is no such algorithm for RRM and RRRM. However, the
bound in Theorem 2 is for the adversarial cases far from
practice. Algorithms proposed in this paper usually have small
rank-regrets in experiments. Further, the rank-regret can be
represented by a percentage as shown before.

C. Candidate Tuples

Based on the restricted space U, Ciaccia et al. [9] proposed
a new kind of dominance, through which they introduced the
restricted version of skyline.

Deﬁnition 5 (U-Dominance and U-Skyline): Given t, t(cid:48) ∈ D
and U ⊆ L, t U-dominates t(cid:48), written t ≺U t(cid:48), if ∀u ∈ U,
w(u, t) ≥ w(u, t(cid:48)) and ∃v ∈ U, w(v, t) > w(v, t(cid:48)). The U-
skyline of D is SkyU(D) = {t ∈ D | (cid:54) ∃t(cid:48) ∈ D, t(cid:48) ≺U t}.

When U = L, SkyU(D) is the skyline [27] of D, abbrevi-
ated as Sky(D). And for any U ⊆ L, SkyU(D) ⊆ Sky(D). We
show that SkyU(D) is the set of candidate tuples for RRRM.
Theorem 3: If there is a size r set S ⊆ D with ∇U(S) = k,

there is a set R ⊆ SkyU(D) with |R| ≤ r and ∇U(R) ≤ k.

Proof: For each tuple t ∈ S, if t /∈ SkyU(D), there must
be a tuple t(cid:48) ∈ SkyU(D) such that t(cid:48) U-dominates t. For each
u ∈ U, t(cid:48) outranks t. Then replace all such t in S with t(cid:48), and
obtain the set R. By deﬁnition, for each u ∈ U, ∇u(R) ≤
∇u(S). Thus we have ∇U(R) ≤ k. Further, R is no larger
than S.

It is sufﬁcient to focus on the subsets of SkyU(D) to ﬁnd
the solution of RRRM. Correspondingly, Sky(D) is the set of
candidate tuples for RRM.

IV. ALGORITHM IN 2D

In this section, we design a dynamic programming algorithm
to return the optimal solution for the RRM problem in 2D
space. And we generalize it to the RRRM problem.

A. Algorithm Preparation

Inspired by Mouratidis et al. [18], in this section, assume
that each utility vector u ∈ L is normalized such that u[1] +
u[2] = 1, i.e., u = (u[1], 1 − u[1]) for some u[1] ∈ [0, 1].
Plotting the utilities w(u, t) of tuples t = (t[1], t[2]) ∈ D as
functions of u[1], they are each mapped into a line l: y =
t[1] · x + t[2] · (1 − x). Let L(D), abbreviated as L, be the list
of corresponding lines of tuples in D. Figure 4 demonstrates
the dual representation L = {l1, l2, . . . , l7} for dataset D =
{t1, t2, . . . , t7} in Table I. In dual space, a utility vector u =
(c, 1 − c) (c ∈ [0, 1]) corresponds to the line x = c. And

tuple t ranks higher than tuple t(cid:48) based on u, if the line of t
is above that of t(cid:48) for x = c. In Figure 4, u = (0.25, 0.75)
is represented by the blue dashed line x = 0.25, on which
l2 is above l1, indicating t2 outranks t1. Given a line l ∈ L,
the rank of l for x = c, denoted by ∇c(l), is deﬁned as one
plus the number of lines in L above l for x = c, equal to
the rank of corresponding tuple t w.r.t. u = (c, 1 − c). In
Figure 4, the number of lines above l1 for x = 0.25 is 1, thus
∇0.25(l1) = ∇(0.25,0.75)(t1) = 2. The line of a skyline tuple
is called as the skyline line (e.g., l1, l2, l3, l4 and l7 in Figure
4). And L(Sky(D)) denotes the list of skyline lines. Note that
l ∈ L is a skyline line, if there is no line in L above l for all
x ∈ [0, 1].

Deﬁnition 6 (Convex Chain): A size r convex chain is a
sequence of line segments, {s1, . . . , sr}, where ∀i ∈ [r − 1],
si and si+1 have a common endpoint and the slope of si is
less than that of si+1.

Based on Theorem 3, our goal is to ﬁnd a size r subset of
Sky(D) that minimizes the rank-regret for L. In dual space,
the analog of such subset is a convex chain that begins on
y-axis and ends on line x = 1. Given a set S ⊆ Sky(D),
the corresponding convex chain of S, denoted by C(S), is
the upper envelop of skyline lines of S. Note that S and
C(S) may have different sizes. In Figure 4, C({t2, t3, t4})
is the chain {(b, d), (d, g)} (red curve). In the following, for
notational convenience, C(S) is equivalently represented by
a sequence of skyline lines, each of which contains a line
segment in Deﬁnition 6. For example, C({t1, t3, t7}) (green
curve) is represented by {l1, l3, l7}. The rank-regret of S for
u = (c, 1 − c) is equal to one plus the number of lines in L
above C(S) for x = c, which is deﬁned as the rank of C(S)
for x = c, denoted by ∇c(C(S)). Obviously, ∇c(C(S)) is the
minimum rank of lines of S for x = c, i.e.,
∇c(l).

∇c(C(S)) = min
l∈L(S)

Note that the rank-regret of S for L is equal to the maximum
rank of C(S) for x ∈ [0, 1], abbreviated as

∇[0,1](C(S)) = max
c∈[0,1]

∇c(C(S)).

In Figure 4, the rank of chain {l1, l3, l7} for x = 0.25 is 2,
and its maximum rank for x ∈ [0, 1] is 3.

B. Algorithm Description

We reformulate the RRM problem in 2D space and present
the algorithm 2DRRM to ﬁnd a size r convex chain within
L(Sky(D)) to minimize the maximum rank, which gives an
optimal solution for RRM in primal space. At a high-level,
2DRRM consists of two phases.

In the ﬁrst phase (line 1-2), 2DRRM sorts tuples of D in
descending order by the second attribute A2. Then the corre-
sponding lines, l1, l2, · · · , ln, is sorted by their intersections
with line x = 0 from top to bottom. Meantime, it calculates
lg(1), lg(2), · · · , lg(s),
Sky(D) and marks all skyline lines,
where g(i) is the initial position of i-th skyline line and s is the
number of skyline lines. Note that the skyline lines following
index order are sorted by their slopes in strictly ascending

TABLE II
UPDATES OF M

M [1, 1]
M [1, 2]
M [2, 1]
M [2, 2]
M [3, 1]
M [3, 2]

Initial
{l1},1
{l1},1
{l2},2
{l2},2
{l3},3
{l3},3

(l1, l2)
-,2
-,2
-,-
{l1, l2},1
-,-
-,-

(l1, l3)
-,3
-,3
-,-
-,-
-,-
{l1, l3},2

(l2, l3)
-,-
-,-
-,-
-,2
-,-
-,-

Fig. 4. Dual Representation for
Tuples in Figure 1

Fig. 5.
Lines

Intersections between

Fig. 6. Render the Scene.

order. In Figure 4, l7 is the ﬁfth skyline line and g(5) = 7.
Skyline lines l1, l2, l3, l4, l7 are sorted by their slopes. The ﬁrst
phase takes O(n log n) time.

In the second phase, 2DRRM makes a vertical line L move
horizontally from y-axis to x = 1 (black dashed line in Figure
5), stopping at the intersections of lines in L (dots in Figure
5). During the movement, 2DRRM traces out and evaluates
the best convex chains encountered up to date, stored in a
matrix M . When L reaches x = 1, the best chain with a size
bound r is the optimal solution. Horizontal movement of L is
achieved through two data structures: a sorted list of L and a
min-heap H, which maintains the unprocessed intersections.
Sorted List L. The lines in L are always sorted by
their intersections with L from top to bottom. In Figure 5,
when L reaches x = 0.25 (blue dashed line), L is the list
{l2, l1, l3, l4, l5, l7, l6}. Only after L passes through one of the
intersections between lines in L, the sort order changes and
L is updated. And the order change is always between two
adjacent lines in L. In Figure 5, the intersection of line l1
and l2 is on the line x = 1
9 (green dashed line). When L is
prior to x = 1
9 , lines in L maintain the original order, but just
afterwards, the order of l1 and l2 ﬂips.

Min-Heap H. H stores discovered but unprocessed in-
tersections, sorted by x-coordinate in ascending order. The
intersection of li and lj, denoted by (li, lj), is discovered
exactly when li and lj are immediate neighbors in L. In Figure
5, when L passes through x = 1
9 , l1 and l3 become immediate
neighbors and the intersection (l1, l3) is inserted into H. To
prevent repeated insertion, H is implemented by a binary
search tree. Before inserting an intersection, the algorithm ﬁrst
judges whether it is already in H.

Matrix M. M is a s × r matrix and maintains the best
the cell M [i, j]
seen solutions. When L reaches x = c,
stores the convex chain within L(Sky(D)) that (i) ends
in skyline line lg(i), (ii) contains at most j segments, and
(iii) minimizes the maximum rank for x ∈ [0, c], stored in
M [i, j].rank. In Figure 5, when L reaches x = 0.25, M [2, 2]
contains the chain {l1, l2} (blue curve) with M [2, 2].rank =
∇[0,0.25]({l1, l2}) = 1.

Afterwards, show the speciﬁc execution process. 2DRRM
ﬁrst initializes H (line 4-6) and M (line 7-8). For each pair of
neighbors in L, if the intersection lies between y-axis and line
x = 1, push it into H. For each i ∈ [s] and j ∈ [r], M [i, j]
and M [i, j].rank are initialized to the chain {lg(i)} and the

rank of lg(i) in L. Next, 2DRRM processes the intersections in
H. When H is not empty, it pops the top intersection (li, lj)
off H (line 9-10), indicating L stops at (li, lj). Let c be the
x-coordinate of (li, lj). li and lj are immediately adjacent in
L. And li is above lj for x ∈ [0, c) and below lj for x ∈ (c, 1].
Therefore, directly swap li and lj in L (line 11). Then li and
lj have new neighbors lp and lq in L respectively. Potentially,
two new intersections are pushed into H provided that they
are between L and line x = 1 (line 12-13). Updating M is
more complicated. When L passes through (li, lj), only the
ranks of li and lj change. Moreover, only the convex chains
ends in line li or lj may be updated. According to whether li
and lj are skyline lines, there are three cases:
(1) (Red and green dots in Figure 5) li is the i(cid:48)-th skyline
line, i.e., g(i(cid:48)) = i. For each h ∈ [r], the rank of chain
M [i(cid:48), h] is increased by one, and the maximum rank, i.e.,
M [i(cid:48), h].rank, may also increase (line 14-16). (Red dots)
Further, if lj is also a skyline line and g(j(cid:48)) = j, the
rank of M [j(cid:48), h] is reduced by one, but its maximum rank
remains unchanged. In addition, the old chain M [j(cid:48), h]
may no longer be the best convex chain that meets the
requirements. M [i(cid:48), h − 1] and lj may form a better chain
(line 17-19).

(2) (White dots) lj is a skyline line (g(j(cid:48)) = j), but li is not.
For all skyline lines, only the rank of lj changes. For each
h ∈ [r], the rank of M [j(cid:48), h] is reduced by one, but the
maximum rank remains unchanged. No update.

(3) (Black dot) Both are not skyline lines. The ranks of skyline

lines don’t change. No update.

Finally, for all i ∈ [s], 2DRRM ﬁnds the cell M [i, r] with the
lowest M [i, r].rank (line 20). For each skyline line in M [i, r],
it adds corresponding tuple to the result set S (line 21).

Brieﬂy illustrate the process of handling intersections. As-
sume r = 2 and D contains only t1, t2 and t3 in Table I. L and
H are ﬁrst set to {l1, l2, l3} and {(l1, l2), (l2, l3)} respectively.
And M is initialized as shown in the second column of Table
II where ”-” means no update. Then 2DRRM takes the ﬁrst
intersection (l1, l2) from H, and L is changed into {l2, l1, l3}.
The new intersection (l1, l3) is inserted into H. Since l1 is the
1-st skyline line (i.e., g(1) = 1), M [1, 2].rank is updated as 2.
Further, l2 is the 2-nd skyline line, and M [1, 1] with l2 forms a
better convex chain, i.e., M [2, 2].rank > M [1, 1].rank. Then
M [2, 2] and M [2, 2].rank are modiﬁed to {l1, l2} and 1. Sub-
sequently, M [1, 1].rank is changed into 2. After processing

Algorithm 1: 2DRRM
Input: D, r;
Output: a size r set S that minimizes the rank-regret;

1 Sort tuples in D and calculate Sky(D);
2 Transform tuples in D to lines in L;
3 S is an empty set, H is a min-heap and M is a matrix;
4 foreach i ∈ [n − 1] do
5

if (li, li+1) lies between x = 0 and x = 1 then

6

push (li, li+1) into H;

7 foreach i ∈ [s] and j ∈ [r] do
8

M [i, j] = {lg(i)}, M [i, j].rank = Rank(lg(i));

9 while H is not empty do
10

Pop top intersection (li, lj) off H;
Swap li and lj in L; // lp and lq are new
neighbors of li and lj in L

if (lq, lj) lies between L and x = 1, push it into H;
if (li, lp) lies between L and x = 1, push it into H;
if li is a skyline line (and i = g(i(cid:48))) then

for h = r, r − 1, · · · , 2, 1 do

M [i(cid:48), h].rank = max(M [i(cid:48), h].rank,
Rank(li));
if lj is a skyline line (and j = g(j(cid:48))) and
M [j(cid:48), h].rank > M [i(cid:48), h − 1].rank then
M [j(cid:48), h].rank = M [i(cid:48), h − 1].rank;
M [j(cid:48), h] = M [i(cid:48), h − 1] sufﬁxed with lj;

11

12

13

14

15

16

17

18

19

20 Find M [i, r] for i ∈ [s] with lowest M [i, r].rank;
21 Add corresponding tuples of lines in M [i, r] to S;
22 return S;

(l1, l3) and (l2, l3) in turn, 2DRRM returns {t1, t2} or {t1, t3}.
Theorem 4: In 2D space, 2DRRM returns an optimal

solution for the RRM problem.

Proof: Based on Theorem 3, there must be a size r subset
of Sky(D) that is the optimal solution for the RRM problem.
Let k∗ be its rank-regret for L. Let C∗ be its correspond-
ing convex chain, denoted by a sequence of skyline lines
{lg(i1), lg(i2), · · · , lg(ir)}, where 1 ≤ i1 < i2 < · · · < ir ≤ s
and lg(i) is the i-th skyline line. We prove that for any integer
j ∈ [r − 1], when L passes intersection (lg(ij ), lg(ij+1)) and
M has been updated, M [ij+1, j + 1].rank ≤ k∗.

We ﬁrst prove that it holds for j = 1. Let ch be the x-
coordinate of intersection (lg(ih), lg(ih+1)). Since the maximum
rank of convex chain C∗ is k∗, the rank of lg(i1) is at most
k∗ for x ∈ [0, c1]. Further, M [i1, 1]
is {lg(i1)}. Thence,
before L reaches (lg(i1), lg(i2)), M [i1, 1].rank is at most k∗.
When L passes (lg(i1), lg(i2)), M [i2, 2].rank is updated as the
smaller of its old value and unupdated M [i1, 1].rank. Thus
M [i2, 2].rank ≤ k∗.

We prove if it holds for j = h (h ∈ [r−2]), then it also holds
for j = h + 1. Assume the maximum rank of M [ih+1, h +
1] for x ∈ [0, ch] is at most k∗. When L moves from line
x = ch to line x = ch+1, the cell M [ih+1, h + 1] may be
updated. However, the rank of lg(ih+1) is at most k∗ for x ∈

[ch, ch+1]. Therefore, the maximum rank of M [ih+1, h+1] for
x ∈ [0, ch+1], i.e., the value of M [ih+1, h + 1].rank before
L passes (lg(ih+1), lg(ih+2)), still does not exceed k∗. Same as
before, M [ih+2, h + 2].rank ≤ k∗.

Thence, when L passes (lg(ir−1), lg(ir)) and M has been
updated, the maximum rank of M [ir, r] for x ∈ [0, cr−1]
is at most k∗. Further, the rank of lg(ir) is at most k∗ for
x ∈ [cr−1, 1]. Therefore, when the algorithm terminates, the
maximum rank of M [ir, r] for x ∈ [0, 1] is at most k∗. And
the corresponding set of skyline tuples has a rank-regret of k∗
for L.

Theorem 5: 2DRRM takes O(n2 log n) time and uses

O(n2) space.

Proof: The ﬁrst phase (line 1-2) takes O(n log n) time. It
takes O(n log n + sr) time to initialize H and M (line 3-8).
There are at most n(n − 1)/2 intersections. It takes O(log n)
time to pop or push an intersection to H. It takes O(1) time to
swap two adjacent lines in L. If the intersection corresponds
to case (2) or case (3), the algorithm dose not perform any
operation in line 14-19. Otherwise, it has to update O(r) cells
in M . There are at most sn intersections that ﬁt case (1).
Therefore, updates to L, H and M for all intersections take
O(n2 log n + snr) time, which dominates the running time of
the algorithm.

The space comes from the min-heap H and the matrix M .
There are at most O(n2) intersections in H and O(sr) cells
in M .

2DRRM can be easily adopted for RRR [3] by a binary
search with an additional log n factor in the running time,
and returns the optimal solution. 2DRRM is inspired by the
2D algorithm proposed by Chester et al. [7] for the kRMS
problem, but there are many differences between them. First,
the dual transformation used in 2DRRM is more practical and
understandable. Secondly, based on Theorem 3, 2DRRM uses
only the skyline tuples as candidates, not all tuples in D.
Thirdly, 2DRRM can be generalized to the restricted function
space, shown in the following subsection. Last but not least,
the goal of 2DRRM is to minimize rank-regret, rather than
k-regret-ratio [7]. And there is no need to pre-calculate the
top-k rank contour [7].

C. Modiﬁcations for RRRM

Through some modiﬁcations, 2DRRM can return the opti-
mal solution for the RRRM problem in 2D space. For RRRM,
utility vectors are bounded in a speciﬁc convex region U ⊆ L.
Earlier, it is assumed that each u ∈ L is normalized such that
u[1] + u[2] = 1. Similarly, normalize the vectors in U. Then
they lie on a line segment

P = {(u[1], 1 − u[1]) | ∃u(cid:48) ∈ U, u[1] =

u(cid:48)[1]
u(cid:48)[1] + u(cid:48)[2]

}.

For example, in Figure 6, U is the shaded region, and then P is
the red line segment. Let c0 and c1 be the x-coordinates of two
endpoints. The above conversion process is called rendering
the scene [10] in computational geometry and takes O(1) time.
In addition to the conversion process, 2DRRM requires three
additional modiﬁcations. Firstly, it focuses on the subsets of

v ∈ Db close to u. Note that D = Da ∪ Db. From two different
perspectives, it is proved that if a set has a small rank-regret
for D, then it is approximately the same for S.

Inspired by Asudeh et al. [2], Db is obtained with the
help of polar coordinate system. Each vector (u[1], · · · , u[d])
in S corresponds to a (d − 1)-dimensional angle vector
(θ[1], . . . , θ[d − 1]) in polar (since (cid:107)u(cid:107) = 1, by default, the
magnitude is 1), where θ[0] is set to 0 and ∀i ∈ [d],
u[i] = sin(θ[d − 1]) sin(θ[d − 2]) × · · · × sin(θ[i]) cos(θ[i − 1]).
In polar, divide the range of each angle dimension, i.e., [0, π
2 ],
into γ equal-width segments. The width of each segment is π
2γ .
Keep the γ + 1 boundaries of segments on each dimension,
and then get a size (γ + 1)d−1 set of angle vectors, i.e.,

{(

z1π
2γ

, · · · ,

zd−1π
2γ

) | ∀i ∈ [d − 1], zi ∈ {0, 1, . . . , γ}}.

As a example, Figure 8 illustrates the above set of angle
vectors (red dots) in polar when d = 3 and γ = 3. Based
on polar coordinate conversion, transform the set to a subset
Db of S.

Next, prove that the discrete set D can approximate S. Given
a tuple t ∈ D and an integer k ∈ [n], let VS,k(t) denote the
set of vectors in S for which t is ranked in top-k of D, i.e.,
VS,k(t) = {u ∈ S | w(u, t) ≥ wk(u, D)}.
If k is obvious from the context, use VS(t) to denote VS,k(t).
Given a set S ⊆ D, ∪t∈SVS(t) is a partial spherical surface
of S. Intuitively, if there are more vectors in ∪t∈SVS(t), S is
more representative. But the number of vectors in ∪t∈SVS(t)
is uncountable. Deﬁne k-ratio of S, denoted by Ratk(S), as
the ratio of surface area of ∪t∈SVS(t) to that of S, i.e.,

Ratk(S) =

Area(∪t∈SVS(t))
Area(S)

where Area(·) indicates the surface area. Ratk(S) is the
proportion of vectors in S, for which S has a rank-regret at
most k. Further, we have the following lemma.

Lemma 1: For any S ⊆ D, ∇S(S) ≤ k, iff Ratk(S) = 1.
Proof: Assume S has a rank-regret no more than k for
S. Then ∀u ∈ S, there is a tuple t ∈ S such that w(u, t) ≥
wk(u, D), i.e., u ∈ VS(t). Thus we have ∪t∈SVS(t) = S and
Ratk(S) = 1.

Assume Ratk(S) = 1. Then ∪t∈SVS(t) = S and ∀u ∈ S,
there is a tuple t ∈ S such that u ∈ VS(t), which means that
∇u(S) ≤ k. Thus we have ∇S(S) ≤ k.

Following the setting in [31], assume each vector in S has
the same probability of being used by a user, i.e., for any user,
his/her utility vector is a random variable and obeys a uniform
distribution on S. We relax the assumption later and show that
the analysis can be applied for any other distribution. Under
the assumption, for any user, Ratk(S) is the probability that
S contains a top-k tuple w.r.t. his/her utility function.

Further, VDa,k(t), abbreviated as VDa (t), is the set of vectors

in Da based on which t is ranked in the top-k of D, i.e.,

VDa,k(t) = {u ∈ Da|w(u, t) ≥ wk(u, D)}.

Since Da is a size m unbiased sample of S, |∪t∈S VDa (t)|
is an
unbiased estimate of Ratk(S). Due to Da ⊆ D, if ∇D(S) ≤ k,

m

Fig. 7. L (Shaded), S (Green),
and D (Red) in 2D space.

Fig. 8.
Illustration of space par-
tition in polar when d = 3 and
γ = 3

SkyU(D) to ﬁnd the solution of RRRM, rather than Sky(D).
In 2D space, SkyU(D) can be computed in O(n log n) time
[16]. Secondly, in the ﬁrst phase, tuples are sorted based on
utilities w.r.t. u = (c0, 1 − c0), rather than the values on A2.
Thirdly, L moves horizontally from line x = c0 to line x = c1,
rather than from y-axis to x = 1. Therefore, H only stores the
intersections that lie between L and x = c1, and M maintains
the convex chains that minimize the maximum rank for x ∈
[c0, c] (L is line x = c and c ∈ [c0, c1]).

V. ALGORITHM IN HD

In this section, discuss the problems in HD space. Both
RRM and RRRM are NP-hard for d ≥ 3. Consider designing
an approximation algorithm.

In HD space, the geometric shapes become complex and
the problem conversion in 2DRRM is inefﬁcient. With the
help of sampling and the polar coordinate system, we take
an alternative route for solving RRM by transforming the full
space L into a discrete set D. From two different perspectives,
it is proved that if a representative set has a small rank-regret
for D, then it is almost the same for L. Based on the ﬁnite size
of D, consider ﬁnding a given size set to minimize the rank-
regret w.r.t. D, which is an approximate solution for RRM.
The problem is converted into linear number of set-cover [5]
instances, and then the algorithm HDRRM is proposed, which
introduces a double approximation guarantee for rank-regret.
Finally, it is shown that HDRRM is suitable for RRRM.

A. Algorithm Preparation

Without loss of generality, assume that each u in L is
normalized such that (cid:107)u(cid:107) = 1 in this section. Then the vectors
lie on the surface S of a sphere with radius 1 centered at the
origin in the d-dimensional non-negative orthant, i.e.,

S = {u | u ∈ Rd

+ ∧ (cid:107)u(cid:107) = 1}.

Note that for any S ⊆ D, ∇L(S) = ∇S(S). There is an
inﬁnite number of vectors in S. To make ”inﬁnite” become
”ﬁnite”, construct a discrete set D ⊆ S, e.g., Figure 7 shows
L (shaded region), S (green quarter arc) and D (set of red dots)
in 2D space. D contains two parts: Da, a size m set of samples
generated from S by a d-dimensional uniform distribution, to
estimate the area of some subregions of S; Db, a size (γ+1)d−1
set obtained by discretization in polar coordinate space with
a parameter γ, to guarantee that for any u ∈ S, there is a

1
|S||K|n ,
(cid:114)

then ∇Da (S) ≤ k and |∪t∈S VDa (t)|
following theorem.

m

= 1. And we have the

Theorem 6: Given a collection S ⊆ 2D, a set K ⊆ [n] and
), with probability at least

a sample size m = O( ln |S|+ln n
1 − 1

n , ∀S ∈ S and ∀k ∈ K, if ∇D(S) ≤ k, then

ε2

Ratk(S) ≥ 1 − ε.

Proof: Let S be a set in S and k be an integer in K.
For each i ∈ [m], let ui be the i-th vector in Da and Xi
be a random variable where Xi = 1 if ui ∈ ∪t∈SVS(t),
otherwise Xi = 0. Then probability Pr(Xi = 1) is equal
to Ratk(S). And | ∪t∈S VDa (t)| = (cid:80)m
i=1 Xi. According to
the well-known Chernoff-Hoeffding Inequality [20], we know
that with a probability at most

− Ratk(S) >

| ∪t∈S VDa (t)|
m

ln |S| + ln |K| + ln n
2m
Therefore, with a probability at least 1 − 1
n , ∀S ∈ S and
∀k ∈ K, inequality (1) does not hold. If ∇D(S) ≤ k, then
|∪t∈S VDa (t)|
= 1. Set m to ln |S|+ln |K|+ln n
, and then the
m

(1)

2ε2

.

theorem is proved.

Theorem 6 shows that if a set S satisﬁes ∇D ≤ k, the
value of Ratk(S) is close to 1, i.e., for any user, with high
probability, S has a rank-regret at most k w.r.t. his/her utility
function. In other words, if a set has a small rank-regret for D,
it is approximately the same for S. Next, prove it from another
perspective.

parameter γ = O( 1

Theorem 7: Given k ∈ [n], a set S ⊆ D and a discretization
(cid:15) ), if ∇D(S) ≤ k and B ⊆ S, then ∀u ∈ S,
w(u, S) ≥ (1 − (cid:15))wk(u, D),

where B is the basis of D.

Proof: Given a utility vector u ∈ S, we have to prove that
there is a tuple t ∈ S such that w(u, t) ≥ (1 − (cid:15))wk(u, D).
Discuss two cases for u separately. Firstly, consider the case
. Given a utility vector u ∈ S, since
that wk(u, D) ≤
(cid:107)u(cid:107) = 1, there must be an attribute Ai such that u[i] ≥ 1/
d.
Based on the deﬁnition of basis, for Ai, there is also a tuple
t in B such that t[i] = 1. And then w(u, t) ≥ 1/
d. Due to
B ⊆ S, the theorem holds at this time.

1
(1−(cid:15))

√

√

√

d

Secondly, consider the case that wk(u, D) >

1
√
(1−(cid:15))
shown in [2], there is a utility vector v ∈ Db such that

d

. As

(cid:114) 1
2
(cid:114) 1
2

√

−

−

1
2
1
2

(cid:107)u − v(cid:107) ≤

=

≤

cosd−1(

π
2γ

)

(1 − 2 sin2 π
4γ

)d−1

√

d − 1 sin

≤

d − 1

π
4γ

π
4γ

= σ

The range of tuples in D on each attribute is normalized to
[0, 1]. Thus ∀t ∈ D, (cid:107)t(cid:107) ≤

d and

√

|w(u, t) − w(v, t)| = (cid:104)u − v, t(cid:105) ≤ (cid:107)u − v(cid:107) · (cid:107)t(cid:107) ≤ σ

d.

For each t in the top-k of D w.r.t. u, we have

w(v, t) ≥ w(u, t) − σ

d ≥ wk(u, D) − σ

√

√

√

d.

Algorithm 2: ASMS
Input: D, k, B, D;
Output: a superset Q of B with ∇D(Q) ≤ k;
1 Q = B, V = ∅, Dk = ∅, and ∀t ∈ D, VDk (t) = ∅;
2 foreach u ∈ D do
3

Compute Φk(u, D);
if Φk(u, D) contains no boundary tuple then

Add u to Dk;
∀t ∈ Φk(u, D), add u to VDk (t);

4

5

6

7 ∀t ∈ D, add non-empty VDk (t) to V;
8 C = Set-Cover(Dk, V);
9 Add corresponding tuples of C to Q;
10 return Q;

For utility vector v, there are at least k tuples in D whose
utilities are no less than wk(u, D) − σ
d. It means that
wk(v, D), i.e., the k-th highest utility w.r.t. v, is at least
d. Since Db ⊆ D, ∇Db (S) ≤ ∇D(S) ≤ k
wk(u, D) − σ
and there is a tuple t(cid:48) in S such that w(v, t(cid:48)) ≥ wk(v, D).
Then we have

√

√

w(u, t(cid:48)) ≥ w(v, t(cid:48)) − σ

d ≥ wk(u, D) − 2σ

d.

√

√

Due to wk(u, D) >

1
(1−(cid:15))

√

d

, we have

w(u, t(cid:48)) > (1 − 2dσ(1 − (cid:15)))wk(u, D) > (1 − 2dσ)wk(u, D).

√

. Then w(u, t(cid:48)) > (1 − (cid:15))wk(u, D) and the

d−1π
Let γ be d
2(cid:15)
theorem holds at this time.
Note that S has a rank-regret at most k for S, if ∀u ∈ S,
w(u, S) ≥ wk(u, D). Theorem 7 shows from another per-
spective that D can approximate S.

B. Algorithm for RRM

Based on Theorem 6 and 7, we propose an approximation
algorithm HDRRM to ﬁnd a size r superset of basis B with
the minimum rank-regret for D, which approaches the optimal
solution of RRM. The rank-regret of a subset of D is in set [n].
Consider each value in [n] as a possible rank-regret threshold
of the following problem.

Deﬁnition 7 (MS Problem): Given a dataset D, a threshold
k ∈ [n], the basis B of D and the utility vector set D, ﬁnd
the minimum superset of B with a rank-regret at most k for
D, i.e., return a set

Q∗ =

arg min
B⊆Q⊆D:∇D(Q)≤k

|Q|.

Assume that there is a solver for MS. Given the size bound
r, a naive algorithm for RRM applies binary search to explore
the values of k in [n] and for each value, calls the solver.

1) Approximate Solver for MS: First, convert the MS prob-
lem into a set-cover instance [5]. Note that given a set Q ⊆ D,
∇D(Q) ≤ k, if ∪t∈QVD,k(t) = D, where VD,k(t) is the set of
vectors in D for which t is ranked in top-k of D. VD,k(t) can
be viewed as the set of vectors ”covered” by t, and MS aims to
ﬁnd a small tuple set that ”covers” all vectors in D. Since the
target set must contain all boundary tuples, there is no need to

take vectors ”covered” by them into consideration. Let Dk be
the set of vectors in D ”uncovered” by boundary tuples, i.e.,
Dk = D\(∪t∈BVD,k(t)).
Similarly, VDk (t) denotes the set of vectors in Dk ”covered”
by tuple t, i.e.,

VDk (t) = {u ∈ Dk | w(u, t) ≥ wk(u, D)}.

Lemma 2: For any Q ⊆ D with B ⊆ Q, ∇D(Q) ≤ k, iff

∪t∈QVDk (t) = Dk.

Proof: Assume Q has a rank-regret no more than k for
D. Then for each vector u in Dk ⊆ D, there is a tuple t ∈ Q
such that w(u, t) ≥ wk(u, D), i.e., u ∈ VDk (t). Thus we have
∪t∈QVDk (t) = Dk.

Assume ∪t∈QVDk (t) = Dk. Since VDk (t) ⊆ VD(t), we have
Dk ⊆ ∪t∈QVD(t). Due to B ⊆ Q, ∪t∈BVD(t) ⊆ ∪t∈QVD(t).
Further, we know that Dk ∪ (∪t∈BVD(t)) = D. Thence D ⊆
∪t∈QVD(t) ⊆ D, and ∇D(Q) ≤ k.

Lemma 2 indicates that the goal set just needs to ”cover”
the vectors in Dk. Then MS is formulated as a set-cover [5]
instance, and its solution is obtained by running a set-cover
algorithm on the set system Σ = (Dk, V), where
V = {VDk (t) | t ∈ D ∧ VDk (t) (cid:54)= ∅}.
It aims to ﬁnd the minimum set-cover of Σ, where a collection
C ⊆ V is a set-cover if ∪V ∈CV = Dk. A set-cover C of Σ
corresponds to a set S ⊆ D such that ∪t∈SVDk (t) = Dk. And
Q = S ∪ B has a rank-regret at most k for D.

Based on the above analysis, we propose an approximate
solver for MS, called ASMS, whose pseudo-code is shown
in Algorithm 2. In ASMS, Set-Cover(·, ·) (line 8) is a naive
greedy algorithm proposed by Chvatal [8] for the set-cover
problem. Starting from C = ∅, it always adds the set in V that
contains the largest number of uncovered vectors, to C at each
iteration until ∪V ∈CV = Dk. Theoretically, Set-Cover(·, ·) has
a time complexity of O(|Dk| · |V|), and its result achieves an
approximation ratio of 1 + ln |Dk| on size.

Theorem 8: ASMS is an O(|D|(nd + k log k)) time solver.
Proof: Since there are n tuples in D, line 1 takes O(n)
time. For each u ∈ D, Φk(u, D) is computed in O(nd +
k log k) time. Thus line 2-7 takes O(|D|(nd + k log k)) time
totally. There are at most |D| vectors in Dk and n sets in V, and
then the time complexity of the invoked set-cover algorithm is
O(|D|n). Therefore, ASMS takes O(|D|(nd + k log k)) time.

2) Improved Binary Search: Theorem 8 shows that ASMS
has a time complexity related to the threshold k. To make
the value of k as small as possible in ASMS, HDRRM turns
the original binary search into two stages. In the ﬁrst stage,
it repeatedly doubles the value of k and calls ASMS, until
a set no larger than r is obtained (line 3-6). Then the search
space is {k/2 + 1, k/2 + 2, . . . , k} (line 7), on which HDRRM
performs a binary search in the second stage (line 8-12).

3) Theoretical Results:
Theorem 9: Given a integer k ≥ 1, ASMS returns a set Q

such that ∇D(Q) ≤ k and

Algorithm 3: HDRRM
Input: D, r, γ, m;
Output: a representative set R ⊆ D no larger than r;
1 Construct the vector set D and compute the basis B;
2 k = 1;
3 while k ≤ n do
4

Q = ASMS(D, k, B, D);
if |Q| ≤ r, then R = Q and break;
k = 2k;

5

6

7 low = k/2 + 1, high = k;
8 while low < high do
9

k = (cid:98)(low + high)/2(cid:99);
Q = ASMS(D, k, B, D);
if |Q| ≤ r, then high = k and R = Q;
else low = k + 1;

10

11

12

13 return R;

where |D| ≤ (γ + 1)d−1 + m and r∗ is the minimum size of
sets with a rank-regret at most k for S.

Proof: Let S∗ be the minimum subset of D with a rank-

regret at most k for S, i.e.,

S∗ = arg

min
S⊆D:∇S(S)≤k

|S|.

The size of S∗ is r∗. Correspondingly, Q∗ is the minimum set
with a rank-regret at most k for Dk. Since Dk is a subset of
S, we have ∇Dk (S∗) ≤ ∇S(S∗) ≤ k, and then |Q∗| ≤ |S∗|.
Q\B is a approximate solution for Dk with a (1 + ln |Dk|)
approximate ratio on size, and

|Q\B| ≤ (1 + ln |Dk|)|Q∗| ≤ (1 + ln |Dk|)r∗.

Further, the size of B is d and Dk ⊆ D. In short, |Q| ≤
(1 + ln |D|)r∗ + d.

δ2 ) and a discretization parameter γ = O( 1

Theorem 10: Given a size bound r ≥ 1, a sample size m =
(cid:15) ), HDRRM

O( r ln n
returns a set R such that
(1) ∇D(R) ≤ k∗ and |R| ≤ r;
(2) for any user, with probability at least 1 − δ, R has a
rank-regret at most k∗ w.r.t. his/her utility function;

(3) ∀u ∈ S, w(u, R) ≥ (1 − (cid:15))wk∗ (u, D),
where |D| ≤ (γ + 1)d−1 + m and k∗ is the minimum rank-
regret for S of size

r−d

1+ln |D| sets.

r−d

Proof: Based on the deﬁnition of k∗, there is a set S ⊆
D such that |S| = r−d
1+ln |D| and ∇S(S) = k∗. And then the
minimum size of sets with rank-regrets at most k∗ for S is
1+ln |D| . Based on Theorem 9 and its proof, we
no more than
know that if input parameter k = k∗, then ASMS returns a
set Q ⊆ D such that |Q| ≤ r. Further, R is the result of the
call to ASMS, in which parameter k is equal to the smallest
value k(cid:48) in [n] that makes ASMS output a set no greater than
r. There is a trade-off between the input parameter k and the
size of output in ASMS. Therefore, we have

|Q| ≤ (1 + ln |D|)r∗ + d

∇D(R) ≤ k(cid:48) ≤ k∗.

And condition (1) holds. Based on Theorem 7, since B ⊆ R,
condition (3) holds.

TABLE III
COMPARISON FOR ALGORITHMS IN HD

2(δ− 1

Let K be the set

[n − r + 1]. Let S be the collec-
tion of supersets of B no larger than r. R has a rank-
regret at most k∗ ∈ K for D and R ⊆ S. Set m to
(r−d) ln(n−d)+ln(n−r+1)+ln n
. Based on Theorem 6, with prob-
n )2
n , Ratk(Q) ≥ 1−δ+ 1
ability at least 1− 1
n . Under the previous
assumption, each utility vector in S is equally probable to be
used by a user. Totally, for any user, with a probability at least
(1 − δ + 1
n ) > 1 − δ, there is a tuple t in Q ranked in
the top-k of T . Condition (2) holds.

n )(1 − 1

Theorem 11: HDRRM is an O(n(γd−1 + m)(log n + d))

time and O(n(γd−1 + m)) space algorithm.

r−d

Proof: k∗ ≤ n is the minimum rank-regret for S of all
1+ln |D| sets. It takes O(n) and O(|D|) time to calculate B
size
and D, respectively. For each u ∈ D, consider precomputing
the utilities of tuples w.r.t. u and sorting them. The above
process totally takes O(|D|n(d + log n)) time. Then each call
of ASMS only takes O(|D|n) time (line 3 takes O(k) time).
HDRRM calls ASMS O(log k∗) times. Thus HDRRM takes
O(|D|n(d + log n + log k∗)) time in total. The space overhead
mainly comes from the precomputed total of |D| ordered utility
lists and at most n sets in V. Each list requires O(n) space,
and each set requires at most O(D) space.

r

|W |

the size bound r is a constant

in practice.
Note that
Assuming both 1
δ in Theorem 10 are constants, then
HDRRM takes O(n log2 n) time and O(n log n) space. And
it approximates the best rank-regret for O(

(cid:15) and 1

ln ln n ) size sets.

space (the lower-bound of

4) Comparison: Asudeh et al. [3] proposed three approxi-
mation algorithms for RRR: MDRRR, an O(|W |knLP(d, n))
time and O(|W |k)
is
√
nd−1eΩ(
log n) [22]) algorithm that guarantees a logarithmic
approximation-ratio on size and a rank-regret of k; MDRRRr,
the randomized version of MDRRR with O(|W |(nd+k log k))
time and O(|W |k) space; MDRC, a heuristic algorithm based
on function space partitioning. Combined with a binary search,
the above algorithms can be applied for RRM. However,
compared with HDRRM, the algorithms have some serious
ﬂaws. MDRRR is a theoretical algorithm and does not scale
beyond a few hundred tuples. MDRRRr runs faster but still
does not handle more than 105 tuples or 5 attributes. As
a sacriﬁce for speed, both MDRRRr and MDRC have no
guarantee on rank-regret. In addition, MDRRR and MDRC
are not applicable for RRRM. HDRRM does not have these
defects and always has the minimal rank-regret in experiments.
In contrast, MDRC often has the maximum one. In some cases,
the output rank-regret of MDRC is more than 2 orders of
magnitude worse than HDRRM, which is unacceptable. Table
III summarizes the above comparisons.

C. Modiﬁcations for Other Distribution or RRRM

Similar to [31], although it is assumed that vectors in S
have the same probability of being used by a user, HDRRM
can generalize to any other distribution through some mod-
iﬁcations. First, the samples in Da are generated based on
the speciﬁc distribution of S instead of a uniform distribution.

Algorithms

Criteria
Guarantee on rank-regret
Suitable for RRRM
Scalable for large n, d
Acceptable rank-regret

MDRRR MDRRRr MDRC HDRRM

Yes
No
No
Yes

No
Yes
No
Yes

No
No
Yes
No

Yes
Yes
Yes
Yes

Second, Ratk(·) in Theorem 6 is deﬁned by integral on S w.r.t.
probability density, rather than the surface area.

To generalize to RRRM, HDRRM requires the following
modiﬁcations. Firstly, the m vectors in Da are sampled from
U. Correspondingly, Ratk(·) is deﬁned by integral over U.
Secondly, the algorithm has to discard some of vectors in Db.
Speciﬁcally, for each u in Db, it keeps u in D, if there is a
vector in U with the same direction as u, i.e., ∃c > 0, cu ∈ U.
Here, the algorithm no longer calculates a normalized space
of U, like P in Section IV.

VI. EXPERIMENTS

All algorithms were implemented in C++ and run on a Core-
I7 machine running Ubuntu 18.04 with 128 GB of RAM. Most
experimental settings follow those in [2], [3]. Some results are
plotted in log-scale for better visualization.

There are six datasets used in experiments, three synthetic
datasets (independent, correlated and anti-correlated) and three
real datasets (Island, NBA, and Weather). We generate the
synthetic datasets by a generator proposed by Borzsony et. al.
[4]. In general, the more correlated the attributes, the smaller
the output rank-regrets. The real data includes three publicly
available datasets commonly used in the existing researches,
the Island dataset [24], [31], the NBA dataset [25] and the
Weather dataset [24]. Island contains 63,383 2-dimensional
tuples, which characterize geographic positions. NBA is a
basketball dataset with 21,961 tuples, each of which repre-
sents one player/season combination on 5 attributes. Weather
includes 178,080 tuples described by 4 attributes. For all
datasets, the range of each attribute is normalized to [0, 1].

In 2D space, we evaluated the performance of our algorithm
2DRRM and compared it with 2DRRR proposed in [3].
2DRRM returns the optimal solution for RRM. And 2DRRR
is an approximation algorithm for RRR [3]. Given a threshold
k, 2DRRR relaxes the rank-regret bound to 2k for the optimal
output size. It is adapted to RRM by performing a binary
search on k in [n] to ﬁnd the smallest value that guarantees the
size bound r. Due to the low output rank-regrets, speciﬁcally,
the improved binary search in Section V is adopted to reduce
the number of rounds. For lack of space, the evaluations are
focused on the running time. Our algorithm 2DRRM always
guarantees optimality on rank-regret.

In HD space, we compared the performance of our algo-
rithm HDRRM with the HD algorithms proposed in [3] for
RRR, namely MDRRR, MDRRRr and MDRC. Similar to
2DRRR, they are applied to RRM with a binary search. Due
to the sensitivity to k, MDRRR and MDRRRr are enhanced
by the improved binary search. The results of MDRRR are

2) Impact of the output size (r): We varied the output size
from 5 to 10. Figure 10 shows that as the output size increases,
2DRRM is basically unaffected but 2DRRR runs faster than
before. The time complexity of 2DRRM has nothing to do
with r. The increase in r leads to a decrease in the output
rank-regret, and then 2DRRR requires less rounds. Further,
2DRRM is 5 times faster than 2DRRR on the anti-correlated
dataset with r = 5.

3) Real datasets: Figures 11 and 12 show the execution
time of 2DRRM and 2DRRR over two real datasets, Island
and NBA, respectively. We varied the dataset size from 10K
to 60K for Island, and from 5K to 20K for NBA. Figure
11 shows that 2DRRM outperforms 2DRRR by one order of
magnitude on Island. While in Figure 12 the two algorithm
are almost indistinguishable. This is because the output rank-
regrets remain 1 on NBA. And the two algorithms have similar
structure and running time.

B. HD Experimental Result

,

n )2

m =

The performance of HD algorithms was studied on the three
synthetic datasets (independent, correlated and anti-correlated)
and two real datasets (NBA and Weather). The default values
for the dataset size, number of attributes and output size are
set to n = 10K, d = 4 and r = 10, respectively. Based on the
proof of Theorem 10 (shown in [28]), the sample size is set
to

(r − d) ln(n − d) + ln(n − r + 1) + ln n
2(δ − 1
where we set δ = 0.03 by default. Following the setting
in [2], the discretization parameter γ is set to 6. In Figures
13 to 28, the rectangle, circle, triangle, and inverted triangle
markers represent algorithms HDRRM, MDRRRr, MDRC and
MDRMS respectively, while the black doted lines show the
execution time and the red solid lines represent the output
rank-regrets. Note that the output of HDRRM is produced by
a call to solver ASMS, whose input parameter k (equal to
the output rank-regret of HDRRM for D) is denoted by a red
cross dashed line in each ﬁgure. The red rectangle solid line
represents the rank-regret of HDRRM for L. Figures 13 to 28
show that the two lines basically ﬁt, indicating that the ﬁnite
vector set D approximates the inﬁnite vector space L well.

Combined with a binary search for solving RRM, both
MDRRRr and MDRC have no theoretical guarantee on rank-
regret. In addition, MDRRRr is not scalable beyond 100K
tuples or 5 attributes, as we shall show. And MDRC cannot be
applied to the RRRM problem. The output of HDRRM always
has the minimal rank-regret. Instead, for all experiments, either
MDRMS or MDRC has the worst output quality. And in some
cases, MDRMS and MDRC have rank-regrets more than one
and two orders of magnitude larger than HDRRM respectively.
Since a different optimization objective (i.e., the regret-ratio),
MDRMS fails to have a reasonable output rank-regret.

1) Impact of the dataset size (n): We varied the dataset
size from 1K to 1000K and evaluated the performance of
HDRRM, MDRRRr, MDRC and MDRMS. Figures 13, 14
and 15 show the results for the independent, correlated and

Fig. 9. 2D, impact of dataset size
on three synthetic datasets

Fig. 10. 2D, impact of output size
on three synthetic datasets

Fig. 11. 2D, varied the dataset size
on Island

Fig. 12. 2D, varied the dataset size
on NBA

not included here, since it is a theoretical algorithm and does
not scale beyond a few hundred tuples. As shown in Section
II, the existing algorithms proposed for RMS [19] pursue
the optimization on the regret-ratio, possibly resulting in a
signiﬁcant increase on rank-regret. To verify this, following the
setting in [3], compare HDRRM with the MDRMS algorithm
proposed in [2]. Both the rank-regret and execution time of an
algorithm are used to measure its performance. Computing the
exact rank-regret of a set is not scalable to the large settings.
Similar to [3], draw 100,000 functions uniformly at random
and consider them for estimating the rank-regret.

A. 2D Experimental Result

Figures 9, 10, 11 and 12 show the performance of our
algorithm 2DRRM and the algorithm 2DRRR proposed in
[3]. In Figures 9 and 10, black, red, and blue bars are used
for the independent, correlated and anti-correlated datasets
respectively. The default values for the dataset size and output
size are set to n = 10K (K = 103) and r = 5. Note that
2DRRM is an exact algorithm and 2DRRR is an approximae
algorithm.

1) Impact of the dataset size (n): We tested each algorithm
over the independent, correlated and anti-correlated datasets.
We varied the dataset size from 100 to 100K. Figure 9
shows the execution time of both algorithms. As the amount
of data increases, it becomes more and more obvious that
2DRRM runs faster than 2DRRR. This is because increasing
the dataset size leads to an increase in the output rank-regret,
and then 2DRRR requires more rounds of binary search.
Compared to 2DRRR, 2DRRM is less sensitive to the corre-
lations of attributes, since it mainly considers all intersections
between lines in dual space. 2DRRR performs much worse for
the anti-correlated dataset, because anti-correlations cause a
higher rank-regret of output. In addition, 2DRRM outperforms
2DRRR by 4 times on the anti-correlated dataset with n =
100K.

Fig. 13. HD, impact of dataset size
on independent dataset

Fig. 14. HD, impact of dataset size
on correlated dataset

Fig. 15. HD, impact of dataset size
on anti-correlated dataset

Fig. 16. HD, impact of dimension
on independent dataset

Fig. 17. HD, impact of dimension
on correlated dataset

Fig. 18. HD, impact of dimension
on anti-correlated dataset

Fig. 19. HD, impact of output size
on independent dataset

Fig. 20. HD, impact of output size
on correlated dataset

anti-correlated datasets. As the amount of data increases, the
running time and output rank-regrets of the four algorithms
increase signiﬁcantly. MDRRRr does not scale beyond 10K
tuples for the anti-correlated dataset, and 100K tuples for
the independent and correlated datasets. The reason is that
MDRRRr needs the collection of k-sets, whose size grows
super linearly as the data volume increases. Correspondingly,
the time complexity of HDRRM is nearly linearly related to
the amount of data, thus it is scalable for large n. On all
three datasets, MDRC and MDRMS perform well in terms
of running time. However, their outstanding performance is at
the expense of result quality. For all settings, either MDRC
or MDRMS has the highest output rank-regret. Relatively,
our algorithm HDRRM always has the lowest one. On the
anti-correlated dataset with n = 1000K, the rank-regret of
HDRRM is 160, but that of MDRC is an astonishing 7231,
which is unacceptable. MDRC cannot guarantee the output
quality. For the correlated dataset and the same n, the rank-
regret of MDRMS is 1439 (that of HDRRM is 94). MDRMS
outputs a set with a high rank-regret.

2) Impact of the number of attributes (d): We varied the
number of attributes from 2 to 6. Figures 16, 17 and 18
show the results for the independent, correlated and anti-
correlated datasets. As the number of attributes increases,
the execution time and output rank-regret of each algorithm
increase. However, compared to the others, the running time
of MDRRRr increases more drastically, and it does not scale
beyond 5 attributes for the independent and correlated datasets.
Further, MDRRRr is sensitive to anti-correlations between
attributes and cannot handle more than 4 attributes for the anti-
correlated dataset. As expected, for all three datasets, HDRRM
always has the minimal rank-regret. On the contrary, either
MDRC or MDRMS has the worst output quality.

3) Impact of the output size (r): The output size is varied
from 10 to 15. Figures 19, 20 and 21 show the results for
the independent, correlated and anti-correlated datasets. The
execution time of HDRRM, MDRC and MDRMS is basically

unaffected by the output size. But as the output size increases,
MDRRR runs signiﬁcantly faster. This is due to the fact that
an increase in the output size leads to a decrease in rank-
regret, thereby reducing the number of k-sets. However, not
for all algorithms, the rank-regret decreases as output size
increases. The rank-regret of MDRMS ﬂuctuates up and down
in the independent and anti-correlated datasets. This is because
MDRMS has a different optimization objective, the regret-
ratio. HDRRM always has the lowest rank-regret. And either
MDRC or MDRMS has the highest one.

4) Impact of the sample size: Since the sample size m is
closely related to the error parameter δ, we varied δ from
0.01 to 0.1. We studied the effect of the value of δ on
the performance of HDRRM. Figures 22, 23 and 24 show
the results for the independent, correlated and anti-correlated
datasets respectively. Increasing δ decreases the sample size
and causes a rapid decrease in running time. On the other hand,
it leads to an increase of the output rank-regret for HDRRM.
Looking at the ﬁgures, setting δ = 0.03 seems appropriate,
as it seems to reach a point of saturation where decreasing δ
beneﬁts little in terms of rank-regret but causes a signiﬁcant
increase in running time.

5) Generalization to the RRRM problem: We follow the

setting in [9] and let the restricted vector space be

U = {u ∈ Rd

+ | ∀i ∈ [c], u[i] ≥ u[i + 1]},

which represents one of the most common types of constraints
on utility vectors: weak rankings [12]. And we set c to 2. For
lack of space, we only show the results on the anti-correlated
dataset, which is the most interesting synthetic dataset [29]. In
Figure 25, we varied the dataset size from 1K to 1000K and
evaluated the performance of HDRRM and MDRRRr. Since
the vector space is severely restricted, the number of k-sets
decreases and MDRRRr runs faster than before. However, as
the increase of dataset size, the gap between the rank-regret
of MDRRRr and that of HDRRM becomes more obvious. In
Figure 26, we varied the number of attributes from 3 to 6. As

Fig. 21. HD, impact of output size
on anti-correlated dataset

Fig. 22. HD, impact of δ on inde-
pendent dataset

Fig. 23. HD, impact of δ on corre-
lated dataset

Fig. 24. HD, impact of δ on anti-
correlated dataset

Fig. 25. HD, RRRM, varied dataset
size on anti-correlated dataset

Fig. 26. HD, RRRM, varied dimen-
sion on anti-correlated dataset

Fig. 27. HD, varied dataset size on
NBA

Fig. 28. HD, varied dataset size on
Weather

the dimension increases, the execution time and output rank-
regret of each algorithm increase. However, compared with
HDRRM, MDRRRr is more sensitive to the dimension and
does not scale beyond 5 attributes. Further, HDRRM always
has the minimal rank-regret.

6) Real datasets: We also evaluated the performance of HD
algorithms over the NBA and Weather datasets. In Figure 27,
we varied the dataset size from 5K to 20K and set the number
of attributes to 5 for NBA. MDRC and MDRMS perform well
in terms of running time. And the execution time of MDRRRr
is most affected by the amount of data. When n = 20K,
HDRRM and MDRRRr are equally efﬁcient. HDRRM and
MDRC have the lowest and highest rank-regrets, respectively.
In Figure 28, we varied the dataset size from 40K to 160K and
set the dimension to 4 for Weather. The results are basically the
same as before, but the output quality of MDRC is much more
terrible. When n = 120K, the output rank-regret of HDRRM
is 9, but that of MDRC is an unbelievable 1610. There is
a difference of more than two orders of magnitude between
them. Even for other data volumes, the difference is more than
one order. MDRC is not adapted to the Weather dataset.

C. Summary of results

To summarize, the experiments veriﬁed the effectiveness
and efﬁciency of our algorithms 2DRRM and HDRRM.
2DRRM returns the optimal solution and runs faster than the
approximate algorithm 2DRRR. Our HD algorithm HDRRM
also outperforms the other algorithms. HDRRM always has the
lowest output rank-regret. Further, HDRRM is applicable for
the RRRM problem and is scalable for large data volumes and
high dimensions. While MDRC runs faster, its output quality is
often the worst and in some cases, is even unacceptable, which
is more than 2 orders of magnitude worse than HDRRM.
In addition, MDRC is not suitable for the RRRM problem.
Although MDRRRr can be applied to RRRM, it does not scale
beyond 100K tuples or 5 attributes. Further, both MDRRRr
and MDRC have no guarantee on rank-regret. MDRMS per-

forms well in terms of execution time, but it pursues the
optimization on the regret-ratio [19], possibly resulting in a
signiﬁcant increase on rank-regret. Sometimes MDRMS fails
to have a reasonable output rank-regret. Experiments veriﬁed
that it is feasible to approximate the full vector space L with
the discrete vector set D.

VII. CONCLUSION

In this paper, we generalize RRM and propose the RRRM
problem to ﬁnd a set, limited to only r tuples, that minimizes
the rank-regret for utility functions in a restricted space. The
solution of RRRM usually has a lower regret level and can
better serve the speciﬁc preferences of some users. We make
several theoretical results as well as practical advances for
RRM and RRRM. We prove that both RRM and RRRM
are shift invariant. With a theoretical lower-bound, we show
that there is no algorithm with a upper-bound, independent
of the data volume, on the rank-regret. In 2D space, we
design the algorithm 2DRRM to ﬁnd the optimal solution
for RRM. 2DRRM can be applied to the RRRM problem. In
HD space, we propose the algorithm HDRRM, the only HD
algorithm that has a theoretical guarantee on rank-regret and is
applicable for RRRM at the same time. HDRRM always has
the best output quality in experiments. The comprehensive set
of experiments on the three synthetic datasets and the three real
datasets verify the efﬁciency, output quality, and scalability of
our algorithms.

ACKNOWLEDGMENTS

This work is supported by the National Natural Sci-
ence Foundation of China (NSFC) Grant NOs. 61732003,
61832003, 61972110, U1811461 and U19A2059,
and
the National Key R&D Program of China Grant NO.
2019YFB2101900.

[24] Weicheng Wang, Raymond Chi-Wing Wong, and Min Xie. Interactive
search for one of the top-k. In Proceedings of the 2021 International
Conference on Management of Data, pages 1920–1932, 2021.

[25] Yanhao Wang, Yuchen Li, Raymond Chi-Wing Wong, and Kian-Lee
Tan. A fully dynamic algorithm for k-regret minimizing sets. In 2021
IEEE 37th International Conference on Data Engineering (ICDE), pages
1631–1642. IEEE, 2021.

[26] Yanhao Wang, Michael Mathioudakis, Yuchen Li, and Kian-Lee Tan.
Minimum coresets for maxima representation of multidimensional
data. In ACM SIGACT-SIGMOD-SIGART Symposium on Principles of
Database Systems (PODS), 2021.

[27] Xingxing Xiao and Jianzhong Li. Sampling-based approximate skyline
calculation on big data. In International Conference on Combinatorial
Optimization and Applications, pages 32–46. Springer, 2020.

[28] Xingxing Xiao and Jianzhong Li. Rank-regret minimization. CoRR,

abs/2111.08563, 2021.

[29] Min Xie, Raymond Chi-Wing Wong, and Ashwin Lall. An experimental
survey of regret minimization query and variants: bridging the best
worlds between top-k query and skyline query. The VLDB Journal,
29(1):147–175, 2020.

[30] Min Xie, Raymond Chi-Wing Wong, Jian Li, Cheng Long, and Ashwin
Lall. Efﬁcient k-regret query algorithm with restriction-free bound for
any dimensionality. In Proceedings of the 2018 international conference
on management of data, pages 959–974, 2018.

[31] Min Xie, Raymond Chi-Wing Wong, Peng Peng, and Vassilis J Tsotras.
Being happy with the least: Achieving α-happiness with minimum
number of tuples. In 2020 IEEE 36th International Conference on Data
Engineering (ICDE), pages 1009–1020. IEEE, 2020.

REFERENCES

[1] Pankaj K. Agarwal, Nirman Kumar, Stavros Sintos, and Subhash Suri.
Efﬁcient algorithms for k-regret minimizing sets. In 16th International
Symposium on Experimental Algorithms, June 21-23, 2017, London, UK,
volume 75 of LIPIcs, pages 7:1–7:23, 2017.

[2] Abolfazl Asudeh, Azade Nazi, Nan Zhang, and Gautam Das. Efﬁcient
computation of regret-ratio minimizing set: A compact maxima repre-
sentative. In Proceedings of the 2017 ACM International Conference on
Management of Data, pages 821–834, 2017.

[3] Abolfazl Asudeh, Azade Nazi, Nan Zhang, Gautam Das, and HV Ja-
In Proceedings of the 2019
gadish. Rrr: Rank-regret representative.
International Conference on Management of Data, pages 263–280, 2019.
[4] Stephan Borzsony, Donald Kossmann, and Konrad Stocker. The skyline
In Proceedings 17th international conference on data engi-

operator.
neering, pages 421–430. IEEE, 2001.

[5] Herv´e Br¨onnimann and Michael T Goodrich. Almost optimal set covers
in ﬁnite vc-dimension. Discrete & Computational Geometry, 14(4):463–
479, 1995.

[6] Wei Cao, Jian Li, Haitao Wang, Kangning Wang, Ruosong Wang,
Raymond Chi-Wing Wong, and Wei Zhan. k-regret minimizing set:
Efﬁcient algorithms and hardness. In 20th International Conference on
Database Theory (ICDT 2017). Schloss Dagstuhl-Leibniz-Zentrum fuer
Informatik, 2017.

[7] Sean Chester, Alex Thomo, S Venkatesh, and Sue Whitesides. Com-
puting k-regret minimizing sets. Proceedings of the VLDB Endowment,
7(5):389–400, 2014.

[8] Vasek Chvatal. A greedy heuristic for the set-covering problem.

Mathematics of operations research, 4(3):233–235, 1979.

[9] Paolo Ciaccia and Davide Martinenghi. Reconciling skyline and ranking
queries. Proceedings of the VLDB Endowment, 10(11):1454–1465, 2017.
[10] Mark De Berg, Marc Van Kreveld, Mark Overmars, and Otfried
In Computational geometry,

Schwarzkopf. Computational geometry.
pages 1–17. Springer, 1997.

[11] Herbert Edelsbrunner.

Algorithms in combinatorial geometry, vol-

ume 10. Springer Science & Business Media, 2012.

[12] Yun Seong Eum, Kyung Sam Park, and Soung Hie Kim. Establishing
dominance and potential optimality in multi-criteria analysis with impre-
cise weight and value. Computers & Operations Research, 28(5):397–
409, 2001.

[13] Ihab F Ilyas, George Beskales, and Mohamed A Soliman. A survey of
top-k query processing techniques in relational database systems. ACM
Computing Surveys (CSUR), 40(4):1–58, 2008.

[14] Kevin G Jamieson and Robert D Nowak. Active ranking using pairwise

comparisons. arXiv preprint arXiv:1109.3701, 2011.

[15] Thorsten Joachims. Optimizing search engines using clickthrough data.
In Proceedings of the eighth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 133–142, 2002.

[16] Jinfei Liu, Li Xiong, Qiuchen Zhang, Jian Pei, and Jun Luo. Eclipse:
Generalizing knn and skyline. In 2021 IEEE 37th International Confer-
ence on Data Engineering (ICDE), pages 972–983. IEEE, 2021.
[17] Kyriakos Mouratidis, Keming Li, and Bo Tang. Marrying top-k with
skyline queries: Relaxing the preference input while producing output of
controllable size. In Proceedings of the 2021 International Conference
on Management of Data, pages 1317–1330, 2021.

[18] Kyriakos Mouratidis and Bo Tang. Exact processing of uncertain top-k
queries in multi-criteria settings. Proceedings of the VLDB Endowment,
11(8):866–879, 2018.

[19] Danupon Nanongkai, Atish Das Sarma, Ashwin Lall, Richard J Lipton,
and Jun Xu. Regret-minimizing representative databases. Proceedings
of the VLDB Endowment, 3(1-2):1114–1124, 2010.

[20] Jeff M Phillips. Chernoff-hoeffding inequality and applications. arXiv

preprint arXiv:1209.6396, 2012.

[21] Li Qian, Jinyang Gao, and HV Jagadish. Learning user preferences by
adaptive pairwise comparison. Proceedings of the VLDB Endowment,
8(11):1322–1333, 2015.

[22] G´eza T´oth. Point sets with many k-sets. In Proceedings of the sixteenth
annual symposium on Computational geometry, pages 37–42, 2000.
[23] Hongning Wang, Yue Lu, and ChengXiang Zhai. Latent aspect rating
In Proceedings of the
analysis without aspect keyword supervision.
17th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 618–626, 2011.

