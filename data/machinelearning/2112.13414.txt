REINFORCEMENT LEARNING WITH
DYNAMIC CONVEX RISK MEASURES

2
2
0
2

n
a
J

0
3

]

G
L
.
s
c
[

2
v
4
1
4
3
1
.
2
1
1
2
:
v
i
X
r
a

Anthony Coache∗
Department of Statistical Sciences
University of Toronto
anthony.coache@mail.utoronto.ca
https://anthonycoache.ca/

Sebastian Jaimungal∗
Department of Statistical Sciences
University of Toronto
sebastian.jaimungal@utoronto.ca
http://sebastian.statistics.utoronto.ca/

February 1, 2022

ABSTRACT

We develop an approach for solving time-consistent risk-sensitive stochastic optimization problems
using model-free reinforcement learning (RL). Speciﬁcally, we assume agents assess the risk of a
sequence of random variables using dynamic convex risk measures. We employ a time-consistent
dynamic programming principle to determine the value of a particular policy, and develop policy
gradient update rules that aid in obtaining optimal policies. We further develop an actor-critic style
algorithm using neural networks to optimize over policies. Finally, we demonstrate the performance
and ﬂexibility of our approach by applying it to three optimization problems: statistical arbitrage
trading strategies, obstacle avoidance robot control, and ﬁnancial hedging.

Keywords Reinforcement Learning · Dynamic Risk Measures · Policy Gradient · Actor-Critic Algorithm ·
Time-Consistency · Trading Strategies · Robot Control · Financial Hedging

1

Introduction

Reinforcement learning (RL) provides a (model-free) framework for learning-based control. RL problems aim at
learning dynamics in the underlying data and ﬁnding optimal behaviors while collecting data via an interactive process.
It differs from supervised learning that attempts to learn classiﬁcation functions from labeled data, and unsupervised
learning that seeks hidden patterns in unlabeled data. The agent makes a sequence of decisions while interacting with
the data-generating process and observing feedback in the form of costs. The agent aims to discover the best possible
actions by interacting with the environment and consistently updating their actions based on their experience, while
often, also taking random actions to assist in exploring the state space – the classic exploration-exploitation trade-off
(Sutton and Barto, 2018).

In RL, uncertainty in the data-generating process can have substantial effects on performance. Indeed, the environmental
randomness may result in algorithms optimized for “on-average” performance to have large variance across scenarios.
For example, consider a portfolio selection problem: a risk-neutral optimal strategy (where one optimizes the expected

∗The authors acknowledge support from the Natural Sciences and Engineering Research Council of Canada (grants RGPIN-2018-

05705, RGPAS-2018-522715, and CGSD3-2019-534435).

 
 
 
 
 
 
RL with Dynamic Convex Risk

terminal return) focuses on assets with the highest returns while ignoring the risks associated with them. As a second
example, consider an autonomous car which should account for environmental uncertainties such as the weather
and road conditions when learning the optimal strategy. Such an agent may wish to account for variability in the
environment and the results of its actions to avoid large potential “losses”. For an overview and outlook on RL in
ﬁnancial mathematics see, e.g., Jaimungal (2022).

In the extant literature, there are numerous proposals for accounting for risk sensitivity, where most of them replace
the expectation in the optimization problem by risk measures – we provide an overview in Section 2. Risk-awareness
or risk-sensitivity in RL offers a remedy to the data-generating process uncertainty by quantifying low-probability
but high-cost outcomes, provides strategies that are more robust to the environment, and allows more ﬂexibility than
traditional approaches as it is tuned to the agent’s risk preference. The speciﬁc choice of risk measure is a decision
the agent makes considering their goals and risk tolerances. We do not address here how the agent makes this speciﬁc
choice and instead refer the reader to, e.g., Dhaene et al. (2006) for an overview.

An interesting approach to risk-aware learning stems from Tamar et al. (2015; 2016), where they provide policy search
algorithms to solve risk-aware RL problems in a dynamic framework. Both studies investigate stationary policies,
restrict themselves to coherent risk measures, and apply their methodology to simple ﬁnancial engineering applications.
More speciﬁcally, they evaluate their algorithms when learning policies for (perpetual) American options and trading in
static portfolio optimization problems. Several real-world applications require a level of ﬂexibility that these limitations
preclude.

In this paper, we develop a model-free approach to solve a wide class of (non-stationary) risk-aware RL problems in a
time-consistent manner. Our contributions may be summarized as follows: (i) we extend Tamar et al. (2015; 2016);
Ahmadi et al. (2020); Kose and Ruszczynski (2021) by focusing on the broad class of dynamic convex risk measures
and consider ﬁnite-horizon problems with non-stationary policies; (ii) we devise an actor-critic algorithm to solve this
class of RL problems using neural networks to allow continuous state-action spaces; (iii) we derive a recursive formula
for efﬁciently computing the policy gradients; and (iv) we demonstrate the performance and ﬂexibility of our proposed
approach on three important applications: optimal trading for statistical arbitrage, obstacle avoidance in robot control,
and hedging ﬁnancial options. We demonstrate that our approach appropriately accounts for uncertainty and leads to
strategies that mitigate risk.

The remainder of the paper is structured as follows. In Section 2, we discuss related work and introduce our RL notation
in Section 3. Section 4 formalizes the evaluation of risk in both static and dynamic frameworks. We introduce the class
of sequential decision making problems with dynamic convex risk measures in Section 5 and devise an actor-critic style
algorithm to solve them in Section 6. Finally, Section 7 illustrates the performance of our proposed algorithm, and we
discuss our work’s limitations and possible extensions in Section 8.

2 Related Work

The literature in risk evaluation for sequential decision making can be divided between those that apply a risk measure
to a single cost random variable, and those that apply risk measures recursively to a sequence of cost random variables.
The former approach optimizes the risk of a single random variable, which does not account for the temporal structure
in what generates it, while the latter optimizes the risk of sequences of random variables in a dynamic framework as
additional information becomes available.

Several authors address sequential decision making problems by minimizing the risk of a cost over a whole episode.
For example, Prashanth and Ghavamzadeh (2013) focus on objective functions for variance related criteria, while Chow
et al. (2017) take a risk-constrained approach with an objective function that includes a penalty on the conditional
value-at-risk (CVaR). Some lines of research look at risk-sensitive RL problems using a broader classes of risk measures,
such as comonotonic (Petrik and Subramanian, 2012), entropic (Nass et al., 2019), or spectral (Bäuerle and Glauner,

2

RL with Dynamic Convex Risk

2020a) risk measures. Di Castro et al. (2019) consider a risk-neutral objective function, but includes an additional
constraint on the risk of the cumulative discounted cost.

Other works extend optimization in Markov decision processes (MDPs) by evaluating the risk at each period. For
instance, Ruszczy´nski (2010) evaluates the risk at each period using dynamic Markov coherent risk measures, while
Chu and Zhang (2014) and Bäuerle and Glauner (2021) propose iterated coherent risk measures, where they both
derive risk-aware dynamic programming (DP) equations and provide policy iteration algorithms. While they focus on
coherent risk measures, various classes of risk measures have already been extended to a dynamic framework, such
as distribution-invariant risk measures (Weber, 2006), coherent risk measures (Riedel, 2004), convex risk measures
(Frittelli and Gianin, 2004; Detlefsen and Scandolo, 2005), and dynamic assessment indices (Bielecki et al., 2016),
among others – however, these works do not look at how to perform model free optimization, i.e., they do not look at
RL. For an overview of dynamic risk measures, see, e.g., Acciaio and Penner (2011).

Another way to account for uncertainty in the data-generating process is by allowing the parameters of the model, or the
entire distribution itself, to be unknown. The class of robust MDPs (Delage and Mannor, 2010) focuses on optimizing
the worst-case expectation when the parameters vary within a certain set. There exists relationships between risk-aware
and robust MDPs, as shown in Osogami (2012) and Bäuerle and Glauner (2020b). Indeed, minimizing a Markov
coherent risk measure in a risk-aware context is equivalent to minimizing a certain worst-case expectation where the
uncertainty set is characterized by a concave function. Several researchers have developed algorithms to solve robust
MDPs, for an overview see, e.g., Rahimian and Mehrotra (2019).

While we consider a model-free approach, several researchers tackle related, but distinct, problems in a model-based
framework. For instance, Weinan et al. (2017) and Han et al. (2018) use deep learning methods to solve non-linear
partial differential equations (PDEs). Using the non-linear Feynman-Kac representation, they reformulate the PDEs as
backward stochastic differential equations (BSDEs), they then parametrize the co-adjoint process and initial condition
using an ensemble of neural networks, and use the mean squared error in the terminal condition as the loss function.
Despite there being an equivalence between BSDEs and dynamic risk measures (Peng, 1997; Drapeau et al., 2016), the
dual representation does not directly help when we aim to optimize a dynamic risk measure in a model-free manner.

Other types of algorithms exist in the literature to ﬁnd a solution to risk-aware RL problems. Among others, Galichet
et al. (2013) use a multi-armed bandit approach, Shen et al. (2014) devise a risk-sensitive Q-learning method when
optimizing utility functions, Bellemare et al. (2017) use a distributional perspective to learn the whole distribution of the
value function, Yu et al. (2018) employ an approximate DP approach to devise a value iteration algorithm for Markov
risk measures, and Kalogerias et al. (2020) address risk-aware problems from a Bayesian perspective. The shortcomings
of these approaches are that they apply to a speciﬁc class of risk measures and the developed methodologies are tuned
to them.

3 Reinforcement Learning

In this section, we introduce the necessary theoretical background for the RL problems we study. We describe
each problem as an agent who tries to learn an optimal behavior, or agent’s policy, by interacting with a certain
data-generating process.

Let S and A be arbitrary state and action spaces respec-
tively, and let C ⊂ R be a cost space. The data-generating
process is often represented as a MDP with the tuple
(S, A, c, P), where c(s, a, s0) ∈ C is a deterministic, state-
action dependent cost function and P characterizes the
transition probabilities P(st+1 = s0|st = s, at = a).
The transition probability is assumed stationary, although

ct

ct+1

at

at+1

. . .

st

st+1

st+2

. . .

Figure 1: Directed graph representation of an MDP.

3

RL with Dynamic Convex Risk

time may be a component of the state, e.g. we usually
assume that time is embedded in the state space without loss of generality. A single episode consists of T periods,
where T ∈ N is known and ﬁnite. At each period, the agent begins in state st ∈ S, takes an action at ∈ A, moves to
the next state st+1 ∈ S, and receives a cost ct = c(st, at, st+1) ∈ C. A directed graph representation of the described
MDP is shown in Fig. 1. A trajectory consists of all states, actions and costs that occur during a single episode and we
denote it by the tuple

τ := (s0, a0, c0, . . . , sT −1, aT −1, cT −1, sT ).

The agent follows a strategy described by a randomized (also known as exploratory control) policy π : S → P(A),
where P(A) is the space of probability measures on σ(A). More speciﬁcally when in state s at time t, the agent selects
the action a with probability π(a|st = s). This also allows for non-stationary policies when dealing with ﬁnite-horizon
problems.

Standard RL usually deals with risk-neutral objective functions of the (γ-discounted) cost of a trajectory induced by a
MDP with a policy π, for instance

" T −1
X

#
γtc(st, at, st+1)

E

min
π

t=0

,

(1)

where γ ∈ (0, 1] is a discount factor. At period T , there is no action and hence any cost based on the terminal state is
encapsulated in cT −1. Contrastingly, risk-sensitive RL considers problems in which the objective takes into account the
variability of the cost with a risk measure ρ, whether optimizing

 T −1
X

t=0

ρ

min
π

!

γtc(st, at, st+1)

,

(2)

or Eq. (1) with an additional constraint on the risk measure of the trajectory cost. We discuss thoroughly risk measures
and their properties in Section 4 next.

In all cases, the goal of RL is to learn the optimal policy π that attains the minimum in the corresponding objective
function, and do so in a manner that makes no explicit assumptions on the data-generating process.

4 Risk Measures

In this section, we formalize how we quantify the risk of random variables. We start by providing a review of static risk
measures, and then continue by reviewing the framework of Ruszczy´nski (2010) for building time-consistent dynamic
risk measures. Static risk measures evaluate the immediate risk of a ﬁnancial position, while dynamic risk measures
allow its monitoring at different times, and lead to time-consistent optimal strategies.

4.1 Static Setting

Let (Ω, F, P ) be a probability space, and deﬁne Z := Lp(Ω, F, P ) and Z ∗ := Lq(Ω, F, P ) with p, q ∈ [1, ∞]. They
represent the space of p-integrable, respectively q-integrable, F-measurable random variables. A risk measure is a
mapping ρ : X → R, where X is a space of random variables. In what follows we assume that X = Z and Z ∈ Z is
interpreted as a random cost. We next enumerate some properties of various risk measures.
Deﬁnition 1. Let m ∈ R, β > 0 and λ ∈ [0, 1]. A risk measure ρ is said to be

d= Z2;

1. law-invariant if ρ(Z1) = ρ(Z2) if Z1
2. monotone if Z1 ≤ Z2 implies ρ(Z1) ≤ ρ(Z2);
3. translation invariant if ρ(Z + m) = ρ(Z) + m;
4. positive homogeneous if ρ(βZ) = β ρ(Z);
5. comonotonic additive if ρ(Z1 + Z2) = ρ(Z1) + ρ(Z2) for all comonotonic pairs (Z1, Z2);

4

RL with Dynamic Convex Risk

6. subadditive if ρ(Z1 + Z2) ≤ ρ(Z1) + ρ(Z2);
7. convex if ρ(λZ1 + (1 − λ)Z2) ≤ λρ(Z1) + (1 − λ)ρ(Z2).

There is a consensus in the literature that any risk measure should satisfy the monotonicity and translation invariance
properties. More speciﬁcally, a portfolio with a higher cost for every possible scenario is indeed riskier, and the
deterministic part of a portfolio does not contribute to its risk.

Deﬁnition 2. A risk measure is said to be monetary (Föllmer et al., 2004) if and only if it is monotone and translation
invariant.

In addition to being monetary, additional requirements may be assumed so that the risk measure reﬂects observed
investor behavior.

Deﬁnition 3. A risk measure ρ is said to be coherent (Artzner et al., 1999) if and only if it is monotone, translation
invariant, positive homogeneous, and subadditive.

The additional properties for coherent risk measures guarantees that doubling a position doubles its risk, and diversifying
a portfolio reduces its risk. The CVaR (Rockafellar et al., 2000) is a well-known example of a coherent risk measure
commonly used in the literature. Criticisms of positive homogeneity and subadditivity led to the study of a broader
class of risk measures, where these axioms are weakened and replaced by the notion of convexity. Indeed, the risk
might increase in a nonlinear way, which is not permitted under coherent risk measures.

Deﬁnition 4. A risk measure ρ is said to be convex (Föllmer and Schied, 2002) if and only if it is monotone, translation
invariant and convex.

Another advantage of convex risk measures is that we can combine several risk measures into a linear combination
to create a trade-off between different risk-aware objectives. Indeed, one can easily show that given two convex risk
measures ρ1, ρ2 and nonnegative coefﬁcients β1, β2 > 0, then ρ := β1 ρ1 + β2 ρ2 is also convex. The set of coherent
risk measures is a strict subset of the set of convex risk measures.

A dual representation of convex (and, as a special case, coherent) risk measures provides us with a key result for
developing our practical algorithm. The dual representation requires us to introduce the expectation under what is
effectively a distorted probability measure and denote Eξ[Z] := P

ω Z(ω)ξ(ω)P (ω) with Z ∈ Z and ξ ∈ Z ∗.

Deﬁnition 5. The conjugate (Shapiro et al., 2014) of the risk measure ρ, denoted ρ∗ : Z ∗ → R, is given by

Deﬁnition 6. The biconjugate (Shapiro et al., 2014), or conjugate of the conjugate, is a mapping ρ∗∗ : Z → R with

ρ∗(ξ) = sup
Z∈Z

(cid:8)Eξ[Z] − ρ(Z)(cid:9) .

(3)

ρ∗∗(Z) = sup
ξ∈Z ∗

(cid:8)Eξ [Z] − ρ∗(ξ)(cid:9) .

(4)

The dual representation of the risk measure is given in the following theorem.

Theorem 7 (Representation Theorem (Shapiro et al., 2014)). A risk measure ρ is convex, proper (i.e. ρ(Z) > −∞ and
its domain is nonempty) and lower semicontinuous (i.e. ρ(W ) ≤ lim inf Z→W ρ(Z)) iff there exists a set

U(P ) ⊂

n
ξ ∈ Z ∗ : P

ω ξ(ω)P (ω) = 1, ξ ≥ 0

o
,

often referred to as the risk envelope, such that

ρ(Z) = sup

(cid:8)Eξ [Z] − ρ∗(ξ)(cid:9) .

ξ∈U (P )

(5)

5

RL with Dynamic Convex Risk

Moreover, we have that ρ is coherent (e.g. satisﬁes also the positive homogeneity) iff

ρ(Z) = sup

(cid:8)Eξ [Z](cid:9) .

ξ∈U (P )

If we assume ρ is a convex, proper, lower semicontinuous risk measure, then by Theorem 7, it may be written as an
optimization problem where the distortion ξ is chosen adversarially from a subset of the set of all probability densities.
Moreover, the notable difference between coherent and convex risk measures is the conjugate term that appears in its
dual representation. Coherent risk measures are uniquely characterized by their risk envelope.

4.2 Dynamic Setting

Optimizing a controlled performance criteria using static risk measures is known to lead to time-inconsistent solutions
– we discuss the precise deﬁnition below. Adapting risk measures to properly account for the ﬂow of information
requires additional care to ensure that the risk evaluation is done in a time-consistent manner, especially with DP
models for MDPs. There are multiple extensions of static risk measure to the dynamic case. Indeed, various classes
of risk measures have already been extended to a dynamic framework, such as distribution-invariant (Weber, 2006),
coherent (Riedel, 2004), convex risk measures (Frittelli and Gianin, 2004; Detlefsen and Scandolo, 2005), and dynamic
assessment indices Bielecki et al. (2016), among others. Here we closely follow the work of Ruszczy´nski (2010).

To this end, let T := {0, . . . , T } denote the sequence of periods in an episode. Consider a ﬁltration F0 ⊆ F1 ⊆ . . . ⊆
FT ⊆ F on a probability space (Ω, F, P ) and (Zt)t∈T with Zt = Lp(Ω, Ft, P ). Deﬁne Zt,T := Zt × · · · × ZT .

Deﬁnition 8. A conditional risk measure is a map ρt,T : Zt,T → Zt which satisﬁes the monotonicity property, i.e.
ρt,T (Z) ≤ ρt,T (W ) for all Z, W ∈ Zt,T such that Z ≤ W a.s..

Deﬁnition 9. A dynamic risk measure is a sequence of conditional risk measures {ρt,T }t∈T .

We may interpret ρt,T (Z), for Z ∈ Zt,T , as a Ft-measurable charge the agent would be willing to incur at time t
instead of the sequence of costs Z. Developing a dynamic programming principle (DPP) for dynamic risk measures
crucially depends on the property of time-consistency. One wishes to evaluate the risk of future outcomes, but it must
not lead to inconsistencies at different points in time.

Deﬁnition 10 (Time-consistency (Ruszczy´nski, 2010)). {ρt,T }t∈T is said to be time-consistent iff for any sequence
Z, W ∈ Zt1,T and any t1, t2 ∈ T such that (0 ≤ t1 < t2 ≤ T ),

Zk = Wk, ∀k = t1, . . . , t2 and ρt2,T (Zt2, . . . , ZT ) ≤ ρt2,T (Wt2 , . . . , WT )

implies that ρt1,T (Zt1 , . . . , ZT ) ≤ ρt1,T (Wt1 , . . . , WT ).

Deﬁnition 10 may be interpreted as follows: if Z will be at least as good as W at time t2 and they are identical between
t1 and t2, then Z should not be worse than W at time t1.

Furthermore, we introduce one additional concept that aids in developing a recursive relationship for dynamic risk
measures.

Deﬁnition 11. A one-step conditional risk measure is a map ρt : Zt+1 → Zt which satisﬁes ρt(Z) = ρt,t+1(0, Z) for
any Z ∈ Zt+1.

One may assume even stronger properties for conditional risk measures, e.g., one may assume the risk measures are
static (across time) and convex. In the next section, we do precisely this. Therefore, the one-step conditional risk
measure ρt(· | Ft) outputs an Ft-measurable random variable obtained when conditioning on Ft. As a consequence of
Deﬁnitions 10 and 11, we have the following recursive relationship for time-consistent dynamic risk measures.

Theorem 12 (Recursive relationship (Ruszczy´nski, 2010)). Let {ρt,T }t∈T be a time-consistent, dynamic risk measure.
Suppose that it satisﬁes ρt,T (Zt, Zt+1, . . . , ZT ) = Zt + ρt,T (0, Zt+1, . . . , ZT ), ρt,T (0, . . . , 0) = 0, and the local

6

RL with Dynamic Convex Risk

property ρt1,t2(1AZ) = 1Aρt1,t2 (Z), for any Z ∈ Zt,T , t ∈ T , A ∈ Ft1. Then {ρt,T }t∈T can be expressed as

ρt,T (Zt, . . . , ZT ) = Zt + ρt

Zt+1 + ρt+1

Zt+2 + · · · + ρT −2

(cid:18)

(cid:16)

ZT −1 + ρT −1(ZT )

(cid:17)

· · ·

(cid:19)!
.

(6)

We also deﬁne dynamic Markov risk measures where ρt are Markovian for any t ∈ T , i.e. only allow dependence on
the current σ-algebra. Those are obtained from risk transition mappings, a Markovian version of one-step conditional
risk measures – for a thorough exploration, see Ruszczy´nski (2010).

5 Problem Setup

In this section, we formally introduce the optimization problems that we face. Brieﬂy, we are interested in RL problems
where the agent wants to minimize a dynamic convex risk measure in order to obtain a time-consistent solution.

Let (S, A, c, P) be a MDP, T := {0, . . . , T − 1} be the sequence of periods in an episode, and suppose that the agent’s
policy π is parametrized by some parameters θ ∈ Θ. We consider a time-consistent, Markov, dynamic convex risk
measure {ρt,T }t∈T . Using Eq. (6) from Theorem 12, we aim to solve the following T -period risk-sensitive RL problem:

min
θ

ρ0,T (Z θ) = min

θ

ρ0

cθ
0 + ρ1

(cid:18)

cθ
1 + · · · + ρT −2

(cid:16)

T −2 + ρT −1(cθ
cθ

T −1))

(cid:17)

· · ·

(cid:19)!
,

(P1)

t , sθ

t = c(st, aθ

where cθ
t+1) is a Ft+1-measurable random cost and the trajectory may be modulated by the policy πθ –
that is why we include a θ index to actions and states. In the sequel, we may omit the superscript θ when obvious for
readability purposes.

Let us denote transition probabilities by Pθ(a, s0|st = s) := P(s0|s, a)πθ(a|st = s) and ξ-weighted conditional
expectations by Eξ
(a,s0) ξ(a, s0)Pθ(a, s0|st)Z(a, s0) for any t ∈ T . Using the dual representation from
Theorem 7, the problem in Eq. (P1) may be written equivalently as

t [Z] := P

min
θ

max
ξ0∈U (Pθ(·,·|s0=s0))

(

"

Eξ0
0

cθ
0 +

max
ξ1∈U (Pθ(·,·|s1=s1))

(cid:26)

(cid:20)
cθ
1 +

Eξ1
1

+

max
ξT −1∈U (Pθ(·,·|sT −1=sT −1))

n

EξT −1
T −1

i

h

cθ
T −1

− ρ∗

T −1(ξT −1)

o

· · ·

(cid:21)

· · ·

(cid:27)#

− ρ∗

1(ξ1)

)

(P2)

− ρ∗

0(ξ0)

.

Assumption 13. We restrict to convex risk measures ρ such that the risk envelope U may be written as

(

U(Pθ(·, ·|s)) =

ξ :

X

(a,s0)

ξ(a, s0)Pθ(a, s0|s) = 1, ξ ≥ 0, ge(ξ, Pθ) = 0, ∀e ∈ E, fi(ξ, Pθ) ≤ 0, ∀i ∈ I

)
,

(7)

where ge(ξ, P) are afﬁne functions wrt ξ, fi(ξ, P) are convex functions wrt ξ, and E (resp. I) denotes the ﬁnite set of
equality (resp. inequality) constraints. Furthermore, for any given ξ ∈ {ξ : P
(a,s0) ξ(a, s0) = 1, ξ ≥ 0}, ge(ξ, p) and
fi(ξ, p) are twice differentiable in p, and there exists M > 0 such that for all (a, s0) ∈ A × S we have

(

max

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d fi(ξ, p)
dp(a, s0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
i∈I

, max
e∈E

(cid:12)
(cid:12)
(cid:12)
(cid:12)

d ge(ξ, p)
dp(a, s0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

≤ M.

As noted by Tamar et al. (2016), “all coherent risk measures we are aware of in the literature are already captured
by [that] risk envelope”, hence, we view the restrictions of Assumption 13 on the explicit form of U as being not

7

 
 
RL with Dynamic Convex Risk

too restrictive. Note, however, here we use convex (which subsumes coherent) risk measures, while still keeping the
structure of the risk envelope induced by this observation for coherent risk measures.

We now wish to derive DP equations with a view of solving problems of the form (P2). To this end, deﬁne the value
function V as the running risk-to-go

Vt(s; θ) := ρt

cθ
t + ρt+1

(cid:18)

t+1 + · · · + ρT −1(cθ
cθ

T −1)

(cid:19)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

!
,

st = s

(8)

for all s ∈ S and t ∈ T . The value function represents the risk at a certain time when being in a speciﬁc state and
following the policy πθ. The DP equations (DPE) for a speciﬁc policy πθ are

VT −1(s; θ) = ρT −1

(cid:16)

cθ
T −1

Vt(s; θ) = ρt

(cid:16)

t + Vt+1(sθ
cθ

,

(cid:12)
(cid:17)
(cid:12)
(cid:12)sT −1 = s
and
(cid:12)
(cid:17)
(cid:12)
(cid:12)st = s

t+1; θ)

,

(9a)

(9b)

for any s ∈ S and t ∈ T \ {T − 1}. Using the dual representation in Theorem 7, the DPE in Eqs. (9a) and (9b) may be
written as

VT −1(s; θ) =

max
ξ∈U (Pθ(·,·|sT −1=s))
(

Vt(s; θ) =

max
ξ∈U (Pθ(·,·|st=s))

Eξ

t,s

(

Eξ

T −1,s

i

h

cθ
T −1

− ρ∗

T −1(ξ)

)
,

and

h

t + Vt+1(sθ
cθ

t+1; θ)

i

− ρ∗

t (ξ)

)
,

(10a)

(10b)

where Eξ
t,s[·] denotes the conditional expectation Eξ[· |st = s]. The DPE allows us to recursively assess the full extant
of the risk associated with a ﬁxed policy πθ, and in particular the recursion can be seen to include the risk associated
with the current (random) cost and the one-step ahead running risk-to-go, both of which depend explicitly on the next
state.

We aim to optimize the value function V over policies πθ using a policy gradient approach (Sutton et al., 2000). Policy
gradient proposes to optimize by updating parameters of the policy using the update rule θ ← θ + η∇θV (·; θ), which
requires an estimation of the gradient. In order to obtain the gradient of V , we need an additional assumption on the
transition probabilities, more speciﬁcally on the policy.

Assumption 14. We suppose the logarithm of transition probabilities log Pθ(a, s0|s) is a differentiable function in θ
when Pθ(a, s0|s) 6= 0, and its gradient wrt θ is bounded for any (a, s) ∈ A × S.

Theorem 15 provides the gradient formulae in our proposed methodology.

Theorem 15 (Gradient of V ). Let Assumptions 13 and 14 hold. For any state s ∈ S, the gradient of the value function
at period T − 1 is then

∇θVT −1(s; θ) = Eξ∗

T −1

"

(cid:0)c(s, aθ

T −1, sθ

T ) − λ∗(cid:1) ∇θ log πθ(aθ

#
T −1|sT −1 = s)

− ∇θρ∗

T −1(ξ∗) −

X

e∈E

λ∗,E (e)∇θge(ξ∗, Pθ)

−

!

X

i∈I

λ∗,I(i)∇θfi(ξ∗, Pθ)

!
,

(11a)

8

 
 
 
RL with Dynamic Convex Risk

and the gradient of the value function at periods t ∈ T \ {T − 1} is

∇θVt(s; θ) = Eξ∗

t

"

(cid:0)c(s, aθ

t , sθ

t+1) + Vt+1(sθ

t+1; θ) − λ∗(cid:1) ∇θ log πθ(aθ

t |st = s) + ∇θVt+1(sθ

#
t+1; θ)

− ∇θρ∗

t (ξ∗) −

X

e∈E

λ∗,E (e)∇θge(ξ∗, Pθ)

−

!

X

i∈I

λ∗,I(i)∇θfi(ξ∗, Pθ)

!
,

(11b)

where (ξ∗, λ∗, λ∗,E , λ∗,I) is any saddle-point of the Lagrangian function of Eqs. (10a) and (10b) respectively.

Proof of Theorem 15. In order to have an expression for the gradient of the value function, we ﬁrst compute the gradient
of the last period and then obtain the recursive relation for subsequent periods. Using Theorem 7 and Assumption 13,
the Lagrangian of the maximization problem in Eq. (10a) (with t = T − 1) for any state s ∈ S can be written as

Lθ(ξ, λ, λE , λI) =

X

(a,s0)

ξ(a, s0)Pθ(a, s0|sT −1 = s)cT −1(s, a, s0) − ρ∗

T −1(ξ)



− λ



X

(a,s0)

ξ(a, s0)Pθ(a, s0|sT −1 = s) − 1





(cid:0)λE (e)ge(ξ, Pθ)(cid:1) −

−

X

e∈E

(cid:0)λI(i)fi(ξ, Pθ)(cid:1) .

X

i∈I

(12)

By the convexity of Eq. (10a) and Assumption 13, Lθ in Eq. (12) has at least one saddle-point. We emphasize here that
the saddle-points (ξ∗, λ∗, λ∗,E , λ∗,I) depend on the state s.

We next recall a what is known in the ML literature as the “likelihood-ratio” trick, which states that

Then, the gradient of the log-probability of a transition ∇θ log Pθ may be represented as

p(x; θ)∇θ log (p(x; θ)) = ∇θp(x; θ).

∇θ log Pθ(a, s0|sT −1 = s) = ∇θ

(cid:0)log P(s0|s, a) + log πθ(a|st = s)(cid:1)

= ∇θ log πθ(a|st = s).

(13)

(14)

Next, we apply the Envelop theorem for saddle-point problems (see Theorem 4 and Corollary 5 in Milgrom and
Segal, 2002) – which relies on the equidifferentiability of the objective function and the absolute continuity of its
gradient. These properties are satisﬁed as we work under Assumption 13. Under these assumptions, the gradient of the
optimization problem equals to the gradient of the Lagrangian evaluated at one of its saddle-points.

Using Assumption 14, the Envelop theorem, the Lagrangian in Eq. (12), the “likelihood-ratio” trick in Eq. (13) and the
gradient in Eq. (14), we obtain that

(cid:12)
∇θVT −1(s; θ) = ∇θLθ(ξ, λ, λE , λI)
(cid:12)
(cid:12)ξ∗,λ∗,λ∗,E ,λ∗,I

X

=

(a,s0)

ξ∗(a, s0)cT −1(s, a, s0)∇θPθ(a, s0|sT −1 = s) − ∇θρ∗

T −1(ξ∗)

−

−

X

λ∗ξ∗(a, s0)∇θPθ(a, s0|sT −1 = s)

(a,s0)
X

(cid:0)λ∗,E (e)∇θge(ξ∗, Pθ)(cid:1) −

e∈E

(cid:0)λ∗,I(i)∇θfi(ξ∗, Pθ)(cid:1)

X

i∈I

9

 
 
RL with Dynamic Convex Risk

"

= Eξ∗

T −1

(cid:0)cT −1(s, aθ

T −1, sθ

T ) − λ∗(cid:1) ∇θ log πθ(aθ

#
T −1|sT −1 = s)

− ∇θρ∗

T −1(ξ∗) −

X

e∈E

λ∗,E (e)∇θge(ξ∗, Pθ)

−

!

X

i∈I

λ∗,I(i)∇θfi(ξ∗, Pθ)

!
.

(15)

The gradient for others periods is obtained in a similar manner. The Lagrangian of the problem in Eq. (10b) (with
t = T − 2, · · · , 0) is

Lθ(ξ, λ, λE , λI) =

X

(a,s0)

ξ(a, s0)Pθ(a, s0|st = s) (ct(s, a, s0) + Vt+1(s0; θ)) − ρ∗

t (ξ)



X

− λ



(a,s0)

ξ(a, s0)Pθ(a, s0|st = s) − 1





(cid:0)λE (e)ge(ξ, Pθ)(cid:1) −

−

X

e∈E

(cid:0)λI(i)fi(ξ, Pθ)(cid:1) .

X

i∈I

(16)

As the value function depends on the policy πθ, the gradient formula will have an additional term. We obtain

(cid:12)
∇θVt(s; θ) = ∇θLθ(ξ, λ, λE , λI)
(cid:12)
(cid:12)ξ∗,λ∗,λ∗,E ,λ∗,I
ξ∗(a, s0)Pθ(a, s0|st = s)∇θVt+1(s0; θ) −

X

=

(a,s0)

X

+

ξ∗(a, s0) (ct(s, a, s0) + Vt+1(s0; θ)) ∇θPθ(a, s0|st = s)

λ∗ξ∗(a, s0)∇θPθ(a, s0|st = s)

X

(a,s0)

(a,s0)
− ∇θρ∗

t (ξ∗) −

X

e∈E

(cid:0)λ∗,E (e)∇θge(ξ∗, Pθ)(cid:1) −

(cid:0)λ∗,I(i)∇θfi(ξ∗, Pθ)(cid:1)

X

i∈I

"

= Eξ∗
t

(cid:0)ct(s, aθ

t , sθ

t+1) + Vt+1(sθ

t+1; θ) − λ∗(cid:1) ∇θ log πθ(aθ

t |st = s) + ∇θVt+1(sθ

t+1; θ)

− ∇θρ∗

t (ξ∗). −

X

e∈E

λ∗,E (e)∇θge(ξ∗, Pθ)

−

!

X

i∈I

λ∗,I(i)∇θfi(ξ∗, Pθ)

!
.

This concludes the proof.

#

(17)

t (ξ∗) appears at ﬁrst not to depend on the policy, and therefore the term ∇θρ∗

t (ξ∗) in Theorem 15
While the term ρ∗
t (ξ) = Eξ
appears to vanish, this is not necessarily so. To see why, let us consider convex penalties of the form ρ∗
t [ft(ξ)]
for convex functions ft : Z ∗ → R. In this case, using the Envelope theorem (Milgrom and Segal, 2002) and the
“likelihood-ratio” trick – the derivation is similar to the proof of Theorem 15 – the gradient wrt the policy is given by

∇θρ∗

t (ξ∗) = Eξ∗

t [ft(ξ∗)∇θ log πθ(aθ

t |st = s)].

While we do not restrict to the above speciﬁc form for ρ∗
gradient cannot be ignored.

t , this result illustrates why, in general, this contribution to the

6 Actor-Critic Algorithm

In this section, we provide details on our proposed algorithm and the architecture of the implemented objects. Our policy
gradient algorithm has an actor-critic style (Konda and Tsitsiklis, 2000), in the sense that we must learn two functions,
and we do so in an alternating fashion: (i) a value function V , which plays the role of the critic; and (ii) the policy π,

10

 
 
 
 
RL with Dynamic Convex Risk

which plays the role of the actor. Actor-critic algorithms are on-policy policy search methods that maintain an estimate
of a value function, which is then used to update the agent’s policy parameters. Such algorithms have been developed in
the RL community for their ability to ﬁnd optimal policies using low variance gradient estimates (Grondman et al.,
2012). To obtain an approximation of the optimal policy, we perform the steps described in Algorithm 1.

Algorithm 1: Main steps

Input: Value function V φ, policy πθ
1 Initialize environment and optimizers ;
2 for each epoch κ = 1, . . . , K do

We propose to use function approximations, more specif-
ically neural network structures, as they are useful
when dealing with continuous state-action spaces and
are known to be universal approximators. In recent years,
deep neural network modeling has shown remarkable suc-
cess in approximating complex functions (see e.g. LeCun
et al., 2015; Silver et al., 2016; Goodfellow et al., 2016),
especially in ﬁnancial mathematics (see e.g. Al-Aradi
et al., 2018; Hu, 2019; Casgrain et al., 2019; Cuchiero
et al., 2020; Horvath et al., 2021; Campbell et al., 2021;
Carmona and Laurière, 2021; Ning et al., 2021; Hambly
et al., 2021). The use of compositions of simple functions
(usually referred to as propagation and activation func-
tions) through several layers approximates complicated functions to arbitrary accuracy (with arbitrarily large structures).
Neural networks also avoid the curse of dimensionality issue of representing nonlinear functions in high dimensions.

Simulate trajectories under πθ ;
Freeze ˜π = πθ ;
Critic: Estimate V φ using ˜π, Algorithm 2 ;
Freeze ˜V = V φ ;
Actor: Update πθ using ˜V , Algorithm 3 ;
Store results ;

Output: Optimal πθ ≈ πθ∗

and V φ ≈ V (s; θ∗)

3

6

7

4

8

5

There are several approaches for modeling the policy and value function with neural network structures. We consider a
policy π characterized by a single (fully-connected, multi-layered feed forward) artiﬁcial neural network (ANN) with
parameters θ, which takes a state s and time t as inputs and outputs a distribution over the space of actions A. We also
suppose that the value function V is characterized by another single (fully-connected, multi-layered feed forward) ANN
with parameters φ. V φ
t (s; θ) approximates the value function when the system is in state s during the period t under
policy πθ, previously denoted Vt(s; θ) in Eqs. (10a) and (10b). We therefore refer to the policy and value function
respectively by πθ and V φ.

Our proposed algorithm uses a nested simulation or simulation upon
simulation approach, where we simulate transitions at each visited
state. This simulation upon simulation approach results in full (outer)
episodes and batches of (inner) transitions for every state, as illus-
trated in Fig. 2. This allows us to easily compute and estimate
quantities of interest for each state, e.g. saddle-points or one-step
conditional risk measures. Such nested simulation approaches are
computationally expensive – part of our future work aims to develop
a more efﬁcient algorithm when simulations are costly.

We next provide additional details on our actor-critic algorithm in
Algorithm 1, i.e. how to (i) estimate the value function in Subsec-
tion 6.1 (step 5), and (ii) update the policy in Subsection 6.2 (step 7).
Additional implementation details are provided in Appendix A.

6.1 Value Function Estimation

Figure 2: Representation of (outer) episodes
and (inner) transitions for the simulation upon
simulation framework.

The value function V φ may be estimated by using the recursion in Eqs. (10a) and (10b). We use the simulation upon
simulation approach mentioned previously. More precisely, when sampling a certain state st, we also generate M
(inner) transitions from the policy πθ to obtain the tuples (st, a(m)
), m = 1, . . . , M . We can then estimate
the one-step conditional risk measures for any given state using those additional transitions. To update the value

t+1, c(m)

, s(m)

t

t

11

00.20.40.60.81-2-1012RL with Dynamic Convex Risk

function, we perform this simulation process for a mini-batch of states, compute the predicted (i.e. output of V φ) and
target values (i.e. Eqs. (10a) and (10b)), and calculate the expected square loss between these values. We update the
parameters using the Adam optimizer (Kingma and Ba, 2014) and repeat this process for several epochs in order to
provide a good approximation of the value function. We recall that the policy πθ is ﬁxed while we optimize V φ. The
algorithm is provided in Algorithm 2.

Algorithm 2: Estimation of the value function V

Input: V φ, πθ, N episodes, M transitions, number of epochs K, batch size B

1 for each epoch k = 1, . . . , K do
2

Set the gradients to zero ;
Sample B states s(b)
Obtain from πθ the associated transitions (a(b,m)

, b = 1, . . . , B, t ∈ T ;

t

, s(b,m)

t+1 , c(b,m)

t

), m = 1, . . . , M ;

t

for each state b = 1, . . . , B, t ∈ T do
Compute the predicted values ˆvb
if t = T − 1 then

t = V φ

t (s(b)

t

; θ) ;

Set the target value as

vb
T −1 =

max

ξ∈U (Pθ (·,·|sT −1=s

(b)
T −1

))

else

Set the target value as

(cid:26)

Eξ

T −1,s

(b)
T −1

h
c(b,m)
T −1

i

+ ρ∗

T −1(ξ)

(cid:27)

;

vb
t =

max

ξ∈U (Pθ (·,·|st=s

(b)
t

))

(cid:26)

Eξ

t,s

(b)
t

h

c(b,m)
t

Compute the expected square loss between vb
Update φ by performing an Adam optimizer step ;

t and ˆvb
t ;

Output: An estimate of the value function V φ

t (s; θ) ≈ Vt(s; θ)

+ V φ

t+1(s(b,m)

t+1 ; θ)

i

(cid:27)

+ ρ∗

t (ξ)

;

3

4

5

6

7

8

9

10

11

12

A powerful result for neural networks structures is the universal approximation theorem – see e.g. Cybenko (1989);
Hornik (1991); Leshno et al. (1993); Pinkus (1999).
Theorem 16 (Universal Approximation (Cybenko, 1989)). Let d1, d2 ∈ N and σ be an activation function. Then σ is
not a polynomial iff for any continuous function f : Rd1 → Rd2 , any compact subset K ⊂ Rd1 and any (cid:15) > 0, there
exists a neural network ˆf(cid:15) : Rd1 → Rd2 with representation ˆf(cid:15) = W2 ◦ σ ◦ W1 such that supx∈Kkf (x) − ˆf(cid:15)(x)k< (cid:15).

We prove next that for a ﬁxed policy πθ, we can approximate its corresponding value function Vt(s; θ) with an ANN
using the procedure devised in Algorithm 2. Theorem 17 follows from the universal approximation theorem.

Theorem 17 (Approximation of V ). Let πθ denote a ﬁxed policy, with corresponding value function as deﬁned in
Eq. (8), which we denote Vt(s; θ). Then for any (cid:15)∗ > 0, there exists an artiﬁcial neural network V φ : S → R such that
ess sups∈S kVt(s; θ) − V φ

t (s; θ)k< (cid:15)∗, for any t ∈ T .

Proof of Theorem 17. First, we prove a lemma that states convex risk measures are continuous since they are in fact
monetary.

Lemma 18. Convex one-step conditional risk measures ρt are absolutely continuous a.s..

Proof of Lemma 18. Indeed, starting from the inequality Z ≤ W + kZ − W k∞, where Z, W ∈ Zt+1, and using the
monotonicity and translation invariance properties, we have

ρt(Z) ≤ ρt(W + kZ − W k∞) =⇒ ρt(Z) − ρt(W ) ≤ kZ − W k∞ .

(18)

12

RL with Dynamic Convex Risk

Repeating this with W ≤ Z + kZ − W k∞ yields to

ess sup kρt(Z) − ρt(W )k ≤ kZ − W k∞ .

(19)

Therefore convex risk measures are Lipschitz continuous a.s. wrt the essential supremum norm, and hence they are
absolutely continuous a.s..

Next, recall that the value function given in Eq. (8) is a dynamic convex risk measure, and therefore may be written
recursively with the DPE in Eqs. (9a) and (9b) as

Vt(s; θ) = ρt

(cid:16)

t + Vt+1(sθ
cθ

t+1; θ)

(cid:12)
(cid:17)
(cid:12)
(cid:12)st = s

.

(20)

Without loss of generality, let us consider the case where the ﬁrst dimension of the state space S corresponds to the time
t ∈ T . At the period T − 1, we have that

VT −1(s; θ) = ρT −1

(cid:16)

cθ
T −1

(cid:12)
(cid:17)
(cid:12)
(cid:12)sT −1 = s

.

(21)

This is a convex risk measure which is absolutely continuous a.s. by Lemma 18. Using the universal approximation
theorem given in Theorem 16, we obtain that there exists a neural net φ such that

(cid:13)
(cid:13)VT −1(s; θ) − V φ
(cid:13)

T −1(s; θ)

(cid:13)
(cid:13)
(cid:13) < (cid:15)T −1.

ess sup
s∈S

(22)

For other periods, the ANN approximates the value function at period t as long as the value function at the next period
t + 1 is adequately approximated. Using the translation invariance, and once again the universal approximation theorem,
we have

ess sup
s∈S

(cid:13)
(cid:13)Vt(s; θ) − V φ
(cid:13)

(cid:13)
(cid:13)
(cid:13) = ess sup
t (s; θ)

s∈S

< ess sup

s∈S

(cid:13)
(cid:13)
(cid:13)ρt
(cid:13)
(cid:13)
(cid:13)ρt

(cid:16)

(cid:16)

t + Vt+1(sθ
cθ

ct + V φ

t+1(sθ

(cid:12)
(cid:17)
(cid:12)
(cid:12)st = s
t+1; θ)
(cid:12)
(cid:17)
(cid:12)
(cid:12)st = s
t+1)

− V φ

(cid:13)
(cid:13)
t (s; θ)
(cid:13)

− V φ

(cid:13)
(cid:13)
(cid:13) + (cid:15)t+1
t (s; θ)

< (cid:15)t + (cid:15)t+1.

Applying this argument recursively for any period t ∈ T , and as Theorem 16 is valid for any (cid:15) > 0, we can perform the
training procedure in order to construct a sequence of (cid:15)t, t ∈ T that satisﬁes a global error (cid:15)∗, such as P (cid:15)t < (cid:15)∗.

6.2 Update of the Policy

The update of the policy is done using the gradients provided in Eqs. (11a) and (11b) of Theorem 15. Some points worth
mentioning concern the policy, the saddle-points, and the gradient formula. When implementing the algorithm, we
ensure the policy uses the so-called reparametrization trick, which allows the existence of pathwise gradient estimators
from random samples. Usually there are three basic approaches to perform a reparametrization:

(i) Use a location-scale transformation – we can view the standard random variable as an auxiliary variable Z (such

as N (0, 1)) and simulate µθ + Zσθ (distributed as N (µθ, σθ));

(ii) Use the inverse cumulative distribution function – if it is tractable, we can use the inverse transform sampling

method to simulate realizations from uniform random variables;

(iii) Use a transformation of auxiliary variables – common examples are the log-normal distribution, which can be
expressed by exponentiation of a Gaussian distribution, and the gamma distribution, which can be rewritten as a
sum of exponentially distributed random variables.

Also since we assume the form of the risk envelope is known in an explicit form in Assumption 13, we can obtain a
saddle-point (ξ∗, λ∗, λ∗,E , λ∗,I) of the Lagrangian of Eqs. (10a) and (10b) for any given risk measure, either analytically

13

RL with Dynamic Convex Risk

or using a sample average approximation (Shapiro et al., 2014). The approach to obtain these saddle-points is illustrated
for common risk measures in Section 7.

We recall that the value function V φ is ﬁxed while we optimize πθ. When we compute the gradient of the value function
to optimize the policy, we ﬁx the parameters of the value function φ. This can be interpreted as taking a copy of the
ANN structure, which implies that the value function used in the actor part of the algorithm does not depend explicitly
on θ. Therefore, the additional expectation of the gradient of the value function at t + 1 in Eq. (11b) vanishes. The
value function gradient is then estimated averaging over a batch of states and different periods. The algorithm is given
in Algorithm 3.

Algorithm 3: Update of the policy π

Input: πθ, V φ, N episodes, M transitions, number of epochs K, batch size B

3

4

5

6

7

8

9

10

11

12

13

14

15

16

1 for each epoch k = 1, . . . , K do
2

Set the gradients to zero ;
Sample B states s(b)
Obtain from πθ the associated transitions (a(b,m)

, b = 1, . . . , B, t ∈ T ;

t

t

|s(b)

t+1 = V φ

for each state b = 1, . . . , B, t ∈ T do
= ∇θ log πθ(a(b,m)
t
t+1(s(b,m)
t+1 ; θ) ;

Obtain ˆz(b,m)
Obtain ˆv(b,m)
Get a saddle-point (ξ∗, λ∗, λ∗,E , λ∗,I) ;
Obtain ˆg(b)
Obtain ˆρ(b)
if t = T − 1 then

e,t = ∇θge(ξ∗, Pθ) and ˆf (b)
t (ξ∗) ;
t = ∇θρ∗

i,t = ∇θfi(ξ∗, Pθ) ;

, s(b,m)

t+1 , c(b,m)

t

), m = 1, . . . , M ;

t

t ) with the reparametrization trick ;

Calculate the gradient ∇θVt(s(b)

t

; θ) from Eq. (11a)

‘(b)
t =

1
M

M
X

m=1

(cid:16)

c(b,m)
t

− λ∗(cid:17)

ˆz(b,m)
t

− ˆρ(b)

t −

λ∗,E (e)ˆg(b)

e,t −

X

e∈E

λ∗,I(i) ˆf (b)
i,t

!
;

X

i∈I

else

Calculate the gradient ∇θVt(s(b)

t

; θ) from Eq. (11b)

‘(b)
t =

1
M

M
X

(cid:16)

m=1

c(b,m)
t

+ ˆv(b,m)

t+1 − λ∗(cid:17)

ˆz(b,m)
t

− ˆρ(b)

t −

λ∗,E (e)ˆg(b)

e,t −

X

e∈E

λ∗,I(i) ˆf (b)
i,t

!
;

X

i∈I

Take the average ‘ = 1
BT
Update θ by performing an Adam optimizer step ;

t=0 ‘(b)

b=1

;

t

PT −1

PB

Output: An updated policy πθ

7 Experiments

In this section, we provide three illustrative examples to understand the potential gain of using dynamic risk measures
in RL, and more speciﬁcally the advantages of our proposed approach on several examples. In our experiments, we
consider several risk measures in order to compare their performance and highlight their differences.2

The ﬁrst risk measure we consider is the expectation ρE(Z) = E[Z], which serves as a benchmark for the risk-neutral
approach. It is a convex risk measure, and its saddle-point (ξ∗, λ∗) is given by ξ∗(ω) = 1 and λ∗ = 0.

2We implemented more dynamic convex risk measures in our code available on Github, and users can easily add their own in the

Python ﬁles – see Appendix A for a description of the code architecture.

14

 
 
RL with Dynamic Convex Risk

The second risk measure is the conditional value-at-risk (CVaR) with threshold α ∈ (0, 1)

where

ρCVaR(Z; α) = sup
ξ∈U (P)

(cid:8)Eξ [Z](cid:9) ,

(

U(P) =

ξ :

X

ω

ξ(ω)P(ω) = 1, ξ ∈

(cid:21))

.

(cid:20)

0,

1
α

(23)

(24)

The CVaR is a coherent risk measure widely used in the ﬁnancial mathematics literature (Rockafellar et al., 2000). As
shown in Shapiro et al. (2014), any saddle-point (ξ∗, λ∗) satisﬁes ξ∗(ω) = 1
α if Z(ω) > λ∗ and ξ∗(ω) = 0 otherwise,
where λ∗ is any (1 − α)-quantile of Z.

The third risk measure is a penalized CVaR where we add a relative entropy term with respect to the uniform distribution.
This is a convex but not coherent risk measure. It is given by

ρCVaR−p(Z; α, β) = sup
ξ∈U (P)

(cid:8)Eξ [Z] − β Eξ [log ξ](cid:9) ,

β > 0,

(25)

with the same risk envelope given in Eq. (24). Obtaining saddle-points is not as straightforward as with the CVaR, since
it requires solving a convex optimization problem. The Lagrangian with the risk envelope constraints is

L(ξ, λ, η) =

X

ω

ξ(ω)P(ω) (Z(ω) − β log ξ(ω)) − λ

!

ξ(ω)P(ω) − 1

−

X

ω

(cid:18)

η(ω)

ξ(ω) −

(cid:19)

,

1
α

X

ω

with λ ∈ R and η(ω) > 0 for all ω. Setting the derivative of Eq. (26) wrt ξ(ωi) to zero leads to

ξ(ωi) = exp

(cid:18) Z(ωi) − λ − β
β

−

η(ωi)
β P(ωi)

(cid:19)

.

When imposing the constraint on the η’s on Eq. (27), we obtain the following expression

ξ(ωi) =






exp

(cid:16) Z(ωi)−λ−β
β

(cid:17)

1
α

if η(ωi) = 0

if η(ωi) > 0

,

(26)

(27)

(28)

and that constraint is active when Z(ωi) > −β log(α) + β + λ. We combine Eq. (28) with the constraint on λ to get

X

i : Z(ωi)≤−β log(α)+β+λ

P(ωi)

(cid:18)

exp

(cid:18) Z(ωi) − λ − β
β

(cid:19)

−

(cid:19)

1
α

= 1 −

1
α

.

(29)

Any saddle-point (ξ∗, λ∗) then satisﬁes ξ∗(ω) = max(1/α, e(Z(ωi)−λ∗−β)/β), where λ∗ is a root of Eq. (29).

We note here that the penalized CVaR contains both risk measures as special cases. Indeed, for β = 0, it reduces to the
CVaR, while we recover the expectation as β tends to ∞.

7.1 Statistical Arbitrage Example

This collection of experiments is performed on an algorithmic trading environment problem. The agent begins each
episode with zero inventory, and on each period the agent wishes to trade quantities of an asset, whose price ﬂuctuates
according to some data-generating processes. For each period t ∈ T , the agent observes the asset’s price St ∈ R+ and
their inventory qt ∈ (−qmax, qmax), performs a trade aθ



t ∈ (−amax, amax), resulting in wealth yt ∈ R according to

t = 1, . . . , T − 1

,

(30)

y0 = 0,
yt = yt−1 − aθ
yT = yT −1 − aθ



t−1St−1 − ϕ(aθ
T −1ST −1 − ϕ(aθ

t−1)2,

T −1)2 + qT ST − ψq2
T ,

15

 
RL with Dynamic Convex Risk

with coefﬁcients ϕ = 0.005 and ψ = 0.5 for the cost transactions and terminal penalty imposed by the market
respectively. We suppose that T = 5, qmax = 5, amax = 2, and the asset price follows an Ornstein-Uhlenbeck process,
and hence mean-reverts:

dSt = κ(µ − St)dt + σdWt,
where κ = 2, µ = 1, σ = 0.2 and Wt is a standard P-Brownian motion.3 In our RL notation, for all periods t ∈ T , the
actions are determined by the trades at, the costs by the differences in wealth ct = yt−1 − yt, and the states by the
tuples (t, St, qt).

(31)

(a) Mean, t = 1

(b) Mean, t = 3

(c) Mean, t = 4

(d) Mean, t = 5

(e) CVaR, t = 1

(f) CVaR, t = 3

(g) CVaR, t = 4

(h) CVaR, t = 5

(i) CVaR-p, t = 1

(j) CVaR-p, t = 3

(k) CVaR-p, t = 4

(l) CVaR-p, t = 5

Figure 3: Learned policy by the actor-critic algorithm as a function of time (from left to right) when optimizing the
expectation (top), the CVaR with α = 0.2 (middle) and the penalized CVaR with α = 0.2 and β = 0.1 (bottom) in the
statistical arbitrage example.

Figure 3 shows a comparison of the learned policy between the mean, the CVaR and the penalized CVaR. When
optimizing a risk-neutral objective function (see Figs. 3a to 3d), in the beginning of the episode, the agent aims to sell
quantities of the asset when its price is higher than the mean-reversion level, and buy it when its price is lower. As
periods evolve, the pattern shifts to ensure that the agent concludes the episode with zero inventory to avoid the terminal
penalty. With other dynamic risk measures (see Figs. 3e to 3l), we observe that the agent is less aggressive, and instead
waits until there are more signiﬁcant price deviations from the mean-reversion level before taking actions that would
beneﬁt from the price reverting back to its mean. This reﬂects the risk-sensitive behavior of the agent.

The distribution of the terminal reward when the agent follows the learned policy for the mean, CVaR, and penalized
CVaR is illustrated in Fig. 4. In general, risk-sensitive approaches lead to a distribution with a smaller variance and
fewer large losses. The agent’s tolerance to risk can be adjusted by modifying the threshold α of the dynamic CVaR

3Our approach is model-free, which implies that we can easily replace the asset price dynamics with more complex models, for

instance including a stochastic volatility.

16

42024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:1-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:3-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:4-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:5-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:1-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:3-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:4-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:5-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:1-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:3-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:4-2-101242024Inventory0.500.751.001.251.50PriceLearned Policy; Time step:5-2-1012RL with Dynamic Convex Risk

(see Fig. 4a) or the relative entropy penalty constant β (see Fig. 4b). When increasing α or β, the distribution of the
terminal wealth converges to the distribution for the risk-neutral objective function, as expected.

(a) CVaR with various α’s

(b) Penalized CVaR with α = 0.2 and various β’s

Figure 4: Estimated distribution of the terminal wealth when following the learned policy in the statistical arbitrage
example for several risk measures over 30, 000 episodes. Vertical dashed lines indicate the 0.1 and 0.9 quantiles.

7.2 Cliff Walking Example

4

3

2

x

This set of experiments is performed on a continuous
version of the cliff walking problem (Sutton and Barto,
2018), illustrated in Fig. 5. Consider an autonomous
rover exploring the land of a new planet, represented as
a Cartesian coordinate system. The rover starts at (0, 0)
and aims to get to (T, 0), with T = 9, while avoiding all
coordinates where x ≤ C = 1.0, illustrated as the cliff.
All allowed movements at any period t ∈ T , i.e. moving
from (t, x1) to (t + 1, x2), incur a cost of 1 + (x2 − x1)2.
Stepping into the cliff region induces an additional cost of
100, while landing further from the goal at (T, x) induces
a penalty of size x2. Actions taken by the rover are drawn
t ∼ πθ = N (µθ, σ), with
from a Gaussian distribution aθ
µθ ∈ (−amax, amax), amax = 4.0 and σ = 1.5. This
represents the rover’s desire to move in a certain direction µθ, but its movements are altered by the unknown terrain (e.g.
slipping on sand, crossing shallow water, etc.).4 This introduces randomness in the RL problem that the autonomous
rover must account for while making decisions. In our RL notation, for all periods t ∈ T , the costs c ∈ C are determined
by the rover’s movements, and the states by the tuples (t, xt).

Figure 5: Illustration of the modiﬁed cliff walking problem
with T = 10 and the cliff (in orange). One sample path
(in blue) of the rover is drawn where it falls into the cliff at
t = 6.

Cliff

1
0

−1

10

t

2

1

7

4

5

8

9

6

3

We next explore the agent’s sensitivity to risk – whether the rover should reach the goal as quickly as possible by staying
close to the cliff, or take a more circuitous route to avoid inadvertently falling into the cliff. Fig. 6 shows the 10%,
50%, and 90% quantiles of the region visited by the rover when following the optimal strategy induced by the four
dynamic risk measures. The ﬁgure illustrates that, indeed, the agent takes into account the uncertainty of the unknown
terrain by staying further and further away from the cliff as they become more risk-averse. Notice, the optimal policy
when optimizing the risk-neutral expectation induces the agent to stay close to the cliff to avoid the costs of vertical

4The environment can be modiﬁed in order to place different obstacles on the coordinate system, and the policy parametrized

with atypical distributions (e.g. one-sided or skewed) to illustrate different terrains.

17

0.40.20.00.20.40.6Terminal wealth05101520253035DensityDistribution of the terminal wealthmeanCVaR0.1CVaR0.2CVaR0.50.40.20.00.20.40.6Terminal wealth0.02.55.07.510.012.515.017.5DensityDistribution of the terminal wealthmeanCVaR0.2CVaR0.2-pen0.1CVaR0.2-pen0.5RL with Dynamic Convex Risk

movements. Ultimately, using a risk-sensitive approach gives a reward distribution with mitigated tail risk for the
autonomous rover, as shown in Fig. 7.

Figure 6: Estimated 0.1, 0.5 and 0.9 quantiles of the
region visited by the rover when following the learned
policy over 30, 000 episodes

Figure 7: Estimated distribution of the terminal reward
when following the learned policy in the cliff walking
example for several risk measures over 30, 000 episodes.

7.3 Hedging with Friction Example

A corner stone problem of mathematical ﬁnance is the question of how to hedge the exposure to ﬁnancial options.
In this section, we illustrate how our approach may be applied to hedging a call option in an environment where the
underlying asset dynamics follows the Heston (1993) model in a market with trading frictions. In principle, one can
swap out the speciﬁc stochastic volatility model for other models and/or use historical sample paths.

We denote the price of an underlying asset by (St)t∈T and the call option’s strike price by K = 10. We consider the
case where an agent sells the call option and aims to dynamically hedge it trading solely in the underlying asset and
the bank account. We assume there are T = 10 periods (corresponding to one month). Hence, the agent must pay
(ST − K)+ at the terminal time.

We denote the stochastic variance process by (νt)t∈T , and, as a reminder, in the Heston model the price evolution of
the underlying asset is given by

dSt = µ Stdt +

√

dνt = κ (ϑ − νt) dt + ς

νt St dW S
t ,
νt dW ν
t ,

√

(32a)

(32b)

where µ = 0.1 is the drift, κ = 9 the mean-reversion rate, ϑ = (0.25)2 the mean-reversion level, ς = 1 the volatility
t )t∈T are two P-Brownian motions with correlation
of the volatility (often referred to as vol-vol), and (W S
t )t∈T , (W ν
ρ = −0.5 (i.e. d[W S, W V ]t = ρdt).

For generating sample paths, we use the Milstein discretization scheme (Mil’shtejn, 1975) to simulate the dynamics of
the stock price and volatility, where for each t ∈ T ,

(cid:26)(cid:18)

St+1 = St exp

µ −

(cid:19)

1
2

ν+
t

q

ν+
t ∆tW S
t

(cid:27)

,

∆t +

νt+1 = νt + κ(ϑ − ν+

t )∆t + ς

q

ν+
t ∆tW ν

t + 1(νt ≥ 0)

ς 2∆t((W ν

t )2 − 1)

(cid:19)

,

(cid:18) 1
4

(33a)

(33b)

with ν+

t = max(νt, 0), ∆t = 1/(12T ), and the initial price and volatility respectively of S0 = 10 and ν0 = (0.2)2.

18

02468Time02468PositionmeanCVaR0.5CVaR0.2CVaR0.125020015010050Terminal reward0.0000.0050.0100.0150.020DensityDistribution of the terminal rewardmeanCVaR0.5CVaR0.2CVaR0.1RL with Dynamic Convex Risk

At each period t ∈ T , the agent’s wealth yt is determined by its hedges at and bank account Bt. We assume: (i) there
are market frictions in the form of transaction costs of (cid:15) = 0.005 (per share); (ii) the interest rate of the bank account r
is zero; and (iii) the agent starts with an initial wealth of y0 = B0.

We next describe the dynamic hedging procedure we employ and the cash-ﬂow it induces. For each t ∈ T , the agent
takes an action aθ
t , which corresponds to the number of shares to hold over the next time interval, based on its policy,
and this action inﬂuences its bank account and wealth in the following manner:

Bt+ = Bt − (cid:0)aθ
yt+ = Bt+ + aθ

t − aθ
t St.

t−1

(cid:1) St − (cid:12)

(cid:12)aθ

t − aθ

t−1

(cid:12)
(cid:12) (cid:15),

(34a)

(34b)

The second term in Bt+ represents the rebalancing costs, while the third term represents a cost due to trading frictions.
The asset price, and bank account, evolves over the next period and induces a change in the agent’s wealth process.
Thus, for each t ∈ T \ {T − 1}, we have the following relationships:

Bt+1 = er∆tBt+,
yt+1 = Bt+1 + aθ

t St+1.

(35a)

(35b)

At the end of the investment horizon, the agent must pay the call option and liquidate its inventory. Therefore, we have
T −1ST − (cid:12)

(cid:12)
(cid:12) (cid:15) − (ST − K)+,

(cid:12)aθ

(36a)

T −1

BT = er∆tB(T −1)+ + aθ
yT = BT .

(36b)

In our RL notation, for all periods t ∈ T , the costs c ∈ C are given by the change in the agent ’s wealth process
ct = yt−1 − yt, and the states represent all the information the agent possesses before making its hedging decision.
Depending on the option and the market assumptions, one may easily include several features into the state space,
such as more asset price history, market volatility, inventories of other assets the agent holds, and so on. To keep the
experiments brief, here, we focus on just a few features.

We consider the case where the states that determine the agent’s policy are tuples of the form (t, St, at−1). Typically,
in continuous time ﬁnancial modeling for the Heston model, one assumes that the volatility process is observed and
used as a feature in obtaining the optimal hedge. Here, however, we exclude it from the policy as in real-world trading,
volatility itself is not observable. The initial wealth B0, is obtained by forcing the dynamic CVaR at level α to be 0.2.
This speciﬁc choice is up to the agent to choose and represents the minimal reservation value the agent is willing to take.
In implementation, we achieve this by initially setting the price to the Black-Scholes price using the long-run volatility
√
ϑ, and then adjusting the price until the dynamic CVaR at level α is 0.2. As the CVaR is translation invariant (see

Deﬁnition 4), the learned policy is not affected by this price adjustment.

In Fig. 8, we illustrate the P&L distribution of the optimal strategies for three different conﬁdence levels of dynamic
CVaR. Analogous to the other experiments, the ﬁgure suggests that risk-sensitive policies lead to reward distributions
that become less variable as the conﬁdence level of CVaR increases. In Fig. 9, we generate a scatter plot of the
underlying asset’s price versus the agent’s terminal bank account (corresponding to their terminal wealth). In a situation
where the agent can perfectly hedge, we expect to see the “hockey stick” payoff. In the current context, however, while
we do see the general “hockey stick” shape, there is additional convexity induced by the agent aiming to hedge in a
discrete trading environment with trading frictions and (unobserved) stochastic volatility – which is far from being a
complete market.

19

RL with Dynamic Convex Risk

Figure 8: Estimated distribution of the terminal wealth
when following the learned policy in the hedging example
over 30, 000 episodes.

Figure 9: Scatter plot of the bank account before paying
the call option against the terminal price of the asset when
following the learned policy in the hedging example over
3, 000 episodes.

8 Discussion

In this work, we extend risk-aware RL procedures found in the literature by developing a methodology for a wide class
of sequential decision making problems. Our approach broadens to the whole class of dynamic convex risk measures
and allows non-stationary behaviors from the agent. Our work performs well on three benchmark RL and ﬁnancial
mathematical problems, which opens doors for several applications in credit risk, portfolio optimization and optimal
control among others.

There are still some future directions that we could explore to provide even more ﬂexibility to our proposed approach.
For instance, we could devise a computationally efﬁcient methodology for large-scale problems when simulations can
be costly, propose a robust framework for time-consistent dynamic risk measures, and develop a generalization for
deterministic policies with dynamic risk measures in a similar manner to deep deterministic policy gradient. More
recently, efforts were put to verify the theoretical convergence of policy gradient methods. Indeed, Agarwal et al. (2021)
show that the policy gradient approach with a risk-neutral expectation has global convergence guarantees. On the other
hand, it may not converge to a global optimum when the objective function is a dynamic risk measure (Huang et al.,
2021). It thus remains an open challenge to prove that actor-critic algorithms with dynamic convex risk measures
converge to an optimal policy when both the value function and policy are characterized by ANNs.

References

Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

Sebastian Jaimungal. Reinforcement learning and stochastic optimisation. Finance and Stochastics, 26(1):103–129,

2022.

Jan Dhaene, Steven Vanduffel, Marc J Goovaerts, Rob Kaas, Qihe Tang, and David Vyncke. Risk measures and

comonotonicity: a review. Stochastic models, 22(4):573–606, 2006.

Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy gradient for coherent risk measures.

Advances in Neural Information Processing Systems, 28:1468–1476, 2015.

Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Sequential decision making with coherent

risk. IEEE Transactions on Automatic Control, 62(7):3323–3338, 2016.

20

0.40.20.00.20.40.6Terminal wealth01234DensityDistribution of the terminal wealthCVaR0.5CVaR0.2CVaR0.189101112Price of the asset0.50.00.51.01.52.02.5Bank accountCVaR0.5CVaR0.2CVaR0.1RL with Dynamic Convex Risk

Mohamadreza Ahmadi, Ugo Rosolia, Michel D Ingham, Richard M Murray, and Aaron D Ames. Constrained risk-averse

markov decision processes. arXiv preprint arXiv:2012.02423, 2020.

Umit Kose and Andrzej Ruszczynski. Risk-averse learning by temporal difference methods with markov risk measures.

J. Mach. Learn. Res., 22:38–1, 2021.

LA Prashanth and Mohammad Ghavamzadeh. Actor-critic algorithms for risk-sensitive mdps. 2013.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning

with percentile risk criteria. The Journal of Machine Learning Research, 18(1):6070–6120, 2017.

Marek Petrik and Dharmashankar Subramanian. An approximate solution method for large risk-averse markov decision

processes. arXiv preprint arXiv:1210.4901, 2012.

David Nass, Boris Belousov, and Jan Peters. Entropic risk measure in policy search. arXiv preprint arXiv:1906.09090,

2019.

Nicole Bäuerle and Alexander Glauner. Minimizing spectral risk measures applied to markov decision processes. arXiv

preprint arXiv:2012.04521, 2020a.

Dotan Di Castro, Joel Oren, and Shie Mannor. Practical risk measures in reinforcement learning. arXiv preprint

arXiv:1908.08379, 2019.

Andrzej Ruszczy´nski. Risk-averse dynamic programming for markov decision processes. Mathematical programming,

125(2):235–261, 2010.

Shanyun Chu and Yi Zhang. Markov decision processes with iterated coherent risk measures. International Journal of

Control, 87(11):2286–2293, 2014.

Nicole Bäuerle and Alexander Glauner. Markov decision processes with recursive risk measures. European Journal of

Operational Research, 2021.

Stefan Weber. Distribution-invariant risk measures, information, and dynamic consistency. Mathematical Finance: An

International Journal of Mathematics, Statistics and Financial Economics, 16(2):419–441, 2006.

Frank Riedel. Dynamic coherent risk measures. Stochastic processes and their applications, 112(2):185–200, 2004.

Marco Frittelli and E Rosazza Gianin. Dynamic convex risk measures. Risk measures for the 21st century, pages

227–248, 2004.

Kai Detlefsen and Giacomo Scandolo. Conditional and dynamic convex risk measures. Finance and stochastics, 9(4):

539–561, 2005.

Tomasz R Bielecki, Igor Cialenco, Samuel Drapeau, and Martin Karliczek. Dynamic assessment indices. Stochastics,

88(1):1–44, 2016.

Beatrice Acciaio and Irina Penner. Dynamic risk measures. In Advanced mathematical methods for ﬁnance, pages

1–34. Springer, 2011.

Erick Delage and Shie Mannor. Percentile optimization for markov decision processes with parameter uncertainty.

Operations research, 58(1):203–213, 2010.

Takayuki Osogami. Robustness and risk-sensitivity in markov decision processes. Advances in Neural Information

Processing Systems, 25:233–241, 2012.

21

RL with Dynamic Convex Risk

Nicole Bäuerle and Alexander Glauner. Distributionally robust markov decision processes and their connection to risk

measures. arXiv preprint arXiv:2007.13103, 2020b.

Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review.

arXiv preprint

arXiv:1908.05659, 2019.

E Weinan, Jiequn Han, and Arnulf Jentzen. Deep learning-based numerical methods for high-dimensional parabolic
partial differential equations and backward stochastic differential equations. Communications in Mathematics and
Statistics, 5(4):349–380, 2017.

Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations using deep learning.

Proceedings of the National Academy of Sciences, 115(34):8505–8510, 2018.

Shige Peng. Backward sde and related g-expectation. Pitman research notes in mathematics series, pages 141–160,

1997.

Samuel Drapeau, Michael Kupper, Emanuela Rosazza Gianin, and Ludovic Tangpi. Dual representation of minimal
supersolutions of convex BSDEs. In Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, volume 52,
pages 868–887. Institut Henri Poincaré, 2016.

Nicolas Galichet, Michele Sebag, and Olivier Teytaud. Exploration vs exploitation vs safety: Risk-aware multi-armed

bandits. In Asian Conference on Machine Learning, pages 245–260. PMLR, 2013.

Yun Shen, Michael J Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement learning. Neural

computation, 26(7):1298–1328, 2014.

Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning.

In

International Conference on Machine Learning, pages 449–458. PMLR, 2017.

Pengqian Yu, William B Haskell, and Huan Xu. Approximate value iteration for risk-aware markov decision processes.

IEEE Transactions on Automatic Control, 63(9):3135–3142, 2018.

Dionysios S Kalogerias, Luiz FO Chamon, George J Pappas, and Alejandro Ribeiro. Better safe than sorry: Risk-aware
nonlinear bayesian estimation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 5480–5484. IEEE, 2020.

Hans Föllmer, Alexander Schied, and Terry J Lyons. Stochastic ﬁnance. an introduction in discrete time. The

Mathematical Intelligencer, 26(4):67–68, 2004.

Philippe Artzner, Freddy Delbaen, Jean-Marc Eber, and David Heath. Coherent measures of risk. Mathematical ﬁnance,

9(3):203–228, 1999.

R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of risk, 2:21–42,

2000.

Hans Föllmer and Alexander Schied. Convex measures of risk and trading constraints. Finance and stochastics, 6(4):

429–447, 2002.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic programming: modeling and

theory. SIAM, 2014.

Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. In Advances in neural information processing systems, pages 1057–1063,
2000.

22

RL with Dynamic Convex Risk

Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. Econometrica, 70(2):583–601, 2002.

Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information processing systems,

pages 1008–1014. Citeseer, 2000.

Ivo Grondman, Lucian Busoniu, Gabriel AD Lopes, and Robert Babuska. A survey of actor-critic reinforcement
learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), 42(6):1291–1307, 2012.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser,
Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks
and tree search. nature, 529(7587):484–489, 2016.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1. MIT press Cambridge,

2016.

Ali Al-Aradi, Adolfo Correia, Danilo Naiff, Gabriel Jardim, and Yuri Saporito. Solving nonlinear and high-dimensional

partial differential equations via deep learning. arXiv preprint arXiv:1811.08782, 2018.

Ruimeng Hu. Deep ﬁctitious play for stochastic differential games. arXiv preprint arXiv:1903.09376, 2019.

Philippe Casgrain, Brian Ning, and Sebastian Jaimungal. Deep q-learning for nash equilibria: Nash-dqn. arXiv preprint

arXiv:1904.10554, 2019.

Christa Cuchiero, Wahid Khosrawi, and Josef Teichmann. A generative adversarial network approach to calibration of

local stochastic volatility models. Risks, 8(4):101, 2020.

Blanka Horvath, Aitor Muguruza, and Mehdi Tomas. Deep learning volatility: a deep neural network perspective on

pricing and calibration in (rough) volatility models. Quantitative Finance, 21(1):11–27, 2021.

Steven Campbell, Yichao Chen, Arvind Shrivats, and Sebastian Jaimungal. Deep learning for principal-agent mean

ﬁeld games. arXiv preprint arXiv:2110.01127, 2021.

René Carmona and Mathieu Laurière. Deep learning for mean ﬁeld games and mean ﬁeld control with applications to

ﬁnance. arXiv preprint arXiv:2107.04568, 2021.

Brian Ning, Sebastian Jaimungal, Xiaorong Zhang, and Maxime Bergeron. Arbitrage-free implied volatility surface

generation with variational autoencoders. arXiv preprint arXiv:2108.04941, 2021.

Ben Hambly, Renyuan Xu, and Huining Yang. Policy gradient methods for the noisy linear quadratic regulator over a

ﬁnite horizon. SIAM Journal on Control and Optimization, 59(5):3359–3391, 2021.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,

2014.

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control, signals and

systems, 2(4):303–314, 1989.

Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks, 4(2):251–257, 1991.

Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward networks with a

nonpolynomial activation function can approximate any function. Neural networks, 6(6):861–867, 1993.

Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8:143–195, 1999.

23

RL with Dynamic Convex Risk

Steven L Heston. A closed-form solution for options with stochastic volatility with applications to bond and currency

options. The review of ﬁnancial studies, 6(2):327–343, 1993.

GN Mil’shtejn. Approximate integration of stochastic differential equations. Theory of Probability & Its Applications,

19(3):557–562, 1975.

Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradient methods:

Optimality, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1–76, 2021.

Audrey Huang, Liu Leqi, Zachary C Lipton, and Kamyar Azizzadenesheli. On the convergence and optimality of policy

gradient for markov coherent risk. arXiv preprint arXiv:2103.02827, 2021.

A Implementation

In this section, we expand on the experimental setup by giving additional details on the implementation of al-
gorithms given in Section 6. All the code written in Python is available at https://github.com/acoache/
RL-DynamicConvexRisk.

The structure of the Python ﬁles is similar between all sets of experiments. The envs.py ﬁle contains the environment
class for RL problem, as well as functions to interact with it. It has both the PyTorch and NumPy versions of the
simulation engine. The risk_measure.py ﬁle has the class that creates an instance of a risk measure, with functions
to compute the risk and calculate the gradient. Risk measures currently implemented are the expectation, the CVaR,
the penalized CVaR, the mean-semideviation and a linear combination between the mean and CVaR. There is also a
utils.py ﬁle which contains some useful functions and variables, such as a function to create new directories and
colors for the visualizations.

Models are regrouped under the models.py ﬁle with classes to build ANN structures using the PyTorch library. In
our experiments presented in Section 7, the value function V φ has four layers of 16 hidden nodes each with SiLU
activation functions, and no activation function for its output layer. The ANN for the policy πθ is composed of ﬁve
layers with 16 hidden nodes each with SiLU activation function, but with an output layer speciﬁc to the application –
e.g. linear transformation of a sigmoid activation function that maps to (−amax, amax) for the set of experiments in
Subsection 7.1. The learning rates for V φ and πθ are of the order of respectively 1 × 10−3 and 5 × 10−4. Both V φ and
πθ are updated with mini batches of 300 (outer) episodes and 1, 000 (inner) transitions during the training phase.5

The whole algorithm is wrapped into a single class named ActorCriticPG, where input arguments specify which
problem the agent faces. The user needs to give an environment, a (convex) risk measure, as well as two neural network
structures that play the role of the value function and agent’s policy. Each instance of that class has functions to select
actions from the policy, whether at random or using the best behavior found thus far, and give the set of invalid actions.
There is also a function to simulate (outer) episodes and (inner) transitions using the simulation upon simulation
approach discussed in Section 6. Algorithm 2 is wrapped in a function which takes as inputs the mini-batch size B,
number of epochs K and characteristics of the value function neural network structure, such as the learning rate and the
number of hidden nodes. Similarly, another function implements Algorithm 3 and takes as inputs the mini-batch size B
and number of epochs K.

The main.py ﬁle contains the program to run the training phase. The ﬁrst part concerns the importation of libraries and
initialization of all parameters, either for the environment, neural networks or risk measure. Some notable parameters
that need to be speciﬁed by the user in the hyperparams.py ﬁle are the numbers of epochs, learning rates, size of
the neural networks and number of episodes/transitions among others. The next section is the training phase and its
skeleton is given in Algorithm 1. It uses mostly functions from the actor_critic.py ﬁle. Finally, the models for the

5Hyperparameters depend on the speciﬁc experiment and are chosen to accelerate the learning procedure.

24

RL with Dynamic Convex Risk

policy and value function are saved in a folder, along with diagnostic plots. Since the nested simulation approach is
computationally expensive, running this Python program can take up to 12 hours depending on the application and the
hyperparameters, especially the number of periods and (inner) transitions .

This main_plot.py ﬁle contains the program to run the testing phase. The ﬁrst part concerns the importation of
libraries and initialization of all parameters. Note that parameters must be identical to the ones used in main.py. The
next section evaluates the policy found by the algorithm. It runs several simulations using the best behavior found by
the actor-critic algorithm. Finally it outputs graphics to assess the performance of the procedure, such as the preferred
action in any possible state and the estimated distribution of the total cost when following the best policy.

25

