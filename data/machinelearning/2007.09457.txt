2
2
0
2

r
p
A
7
2

]

A
N
.
h
t
a
m

[

2
v
7
5
4
9
0
.
7
0
0
2
:
v
i
X
r
a

Compressed sensing of low-rank plus sparse matrices

Jared Tannera,b, Simon Varya,

∗

aMathematical Institute, University of Oxford, Oxford OX2 6GG, UK
bThe Alan Turing Institute, The British Library, London NW1 2DB, UK

Abstract

×

Expressing a matrix as the sum of a low-rank matrix plus a sparse matrix is a ﬂexible model cap-
turing global and local features in data. This model is the foundation of robust principle component
analysis [1, 2], and popularized by dynamic-foreground/static-background separation [3]. Compressed
sensing, matrix completion, and their variants [4, 5] have established that data satisfying low complex-
ity models can be eﬃciently measured and recovered from a number of measurements proportional to
the model complexity rather than the ambient dimension. This manuscript develops similar guaran-
n matrices that can be expressed as the sum of a rank-r matrix and a s-sparse
tees showing that m
matrix can be recovered by computationally tractable methods from
r) + s) log(mn/s)
linear measurements. More speciﬁcally, we establish that the low-rank plus sparse matrix set is closed
provided the incoherence of the low-rank component is upper bounded as µ < √mn/(r√s), and
subsequently, the restricted isometry constants for the aforementioned matrices remain bounded in-
dependent of problem size provided p/mn, s/p, and r(m + n
r)/p remain ﬁxed. Additionally, we
show that semideﬁnite programming and two hard threshold gradient descent algorithms, NIHT and
NAHT, converge to the measured matrix provided the measurement operator’s RIC’s are suﬃciently
small. These results also provably solve convex and non-convex formulation of Robust PCA with the
(1/(µr)), where s = α2mn, and improve the
asymptotically optimal fraction of corruptions α =
previously best known guarantees by not requiring that the fraction of corruptions is spread in every
column and row by being upper bounded by α. Numerical experiments illustrating these results are
shown for synthetic problems, dynamic-foreground/static-background separation, and multispectral
imaging.

(r(m + n

O

O

−

−

Keywords: matrix sensing, low-rank plus sparse matrix, robust PCA, restricted isometry property,
non-convex methods
2010 MSC: 15A29, 41A29, 62H25, 65F10, 65J20, 68Q25, 90C22, 90C26

1. Introduction

Data with a known underlying low-dimensional structure can often be estimated from a number
of measurements proportional to the degrees of freedom of the underlying model, rather than what
its ambient dimension would suggests. Examples of such low-dimensional structures for which the
aforementioned is true include: compressed sensing [6, 7, 8], matrix completion [9, 10, 11], sparse
measures [12, 13, 14], and atomic decompositions [15] more generally. Our work extends these results
to the matrices which are formed as the sum of a low-rank matrix and a sparse matrix, a model

∗Corresponding author
Email addresses: tanner@maths.ox.ac.uk (Jared Tanner), vary.simon@gmail.com (Simon Vary)
URL: https://people.maths.ox.ac.uk/tanner/ (Jared Tanner), https://simonvary.github.io (Simon Vary)
1This publication is based on work partially supported by: the EPSRC I-CASE studentship (voucher 15220165) in
partnership with Leonardo, The Alan Turing Institute through EPSRC (EP/N510129/1) and the Turing Seed Funding
grant SF019.

April 28, 2022

 
 
 
 
 
 
Rm
S

popularized by the work on robust principle component anaysis (Robust PCA) [1, 2]. Speciﬁcally, we
n of the form X = L + S, where L is of rank at most r, and S has at
consider matrices X
×
∈
most s non-zero entries,
s. The low-rank plus sparse model is a rich model with the low rank
k0 ≤
component modeling global correlations, while the additive sparse component allows a ﬁxed number
of entries to deviate from this global model in an arbitrary way. Among applications of this model are
image restoration [16], hyperspectral image denoising [17, 18, 19], face detection [20, 21], acceleration
of dynamic MRI data acquisition [22], analysis of medical imagery [23], separation of moving objects
in at otherwise static scene [3], and target detection [24].

k

(
·
A

), where

Unlike Robust PCA where X is directly available, we consider the compressed sensing setting where
X is measured through a linear operator
mn.
Our contributions extend existing results on restricted isometry constants (RIC) for Gaussian and
other measurement operators for sparse vectors [25] or low-rank matrices [11] to the sets of low-rank
plus sparse matrices. For the set of matrices which are the sum of a low-rank plus a sparse matrix the
results diﬀer subtly due to the space not being closed, in that there are matrices X for which there
does not exist a nearest projection to the set of low-rank plus sparse matrices [26]. To overcome this,
we introduce the set of low-rank plus sparse matrices with the incoherence constraint on the singular
vectors of the low-rank component, see Deﬁnition 1.1

Rp and typically p

Rp, b

: Rm

≪

→

A

∈

×

n

Deﬁnition 1.1 (Low-rank plus sparse set LSm,n(r, s, µ)). Denote the set of m
are the sum of a rank r matrix and a s sparse matrix as

×

n real matrices that

LSm,n(r, s, µ) =

L + S




∈

Rm

n : rank(L)

×

r,

S

k

≤

U T ei

2 ≤

µr/m

s,

max
i
[m]
∈
max
[n]
i
∈

k0 ≤

p

V T fi
(cid:13)
(cid:13)
(cid:13)
(cid:13)
[1, √mn/r] controls the incoherence of L.

µr/n 


(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 ≤

p



,

(1)

Rn

×

Rm

r, V

×


∈

where U
ei ∈

r are the ﬁrst r left and the right singular vectors of L respectively,

Rn are the canonical basis vectors, and µ

∈
Rm, fj ∈
The parameter µ is referred to as the incoherence of the low-rank component [1, 2] and it controls
correlation between the low-rank component and the sparse component. We show that LSm,n(r, s, µ)
sets are closed when the incoherence is suﬃciently upper bounded as µ < √mn/(r√s), see Lemma 1.1.
This bound is equivalent to the asymptotically optimal scaling in terms of r, s and µ in the recov-
ery guarantees independently achieved in Robust PCA using convex relaxation [27] or in nonconvex
methods [28], but without the need for the assumption that the fraction of corruptions in each column
and row is upper bounded.

∈

The natural generalization of the RIC deﬁnition from sparse vectors and low-rank matrices to the

space LSm,n(r, s, µ) is given in Deﬁnition 1.2.

Deﬁnition 1.2 (RIC for LSm,n(r, s, µ)). Let
integers (r, s) and every µ
∆r,s,µ(

) > 0 such that

≥

A

Rp be a linear map. For every pair of
1, deﬁne the (r, s, µ)-restricted isometry constant to be the smallest

: Rm

→

A

×

n

for all matrices X

∈

(1

∆r,s,µ(

))

−

A
LSm,n(r, s, µ).

X

2
F ≤ k A

(X)
k

2
2 ≤

k

k

(1 + ∆r,s,µ(

A

))

X

k

k

2
F ,

(2)

Random linear maps

which have a suﬃcient concentration of measure phenomenon can overcome
the dimensionality of LSm,n(r, s, µ) to achieve ∆r,s,µ which is bounded by a ﬁxed value independent
of dimension size provided the number of measurements p is proportional to the degrees of freedom of
a rank-r plus sparsity-s matrix r(m + n
r) + s. A suitable class of random linear maps is captured
in the following deﬁnition.

A

−

Deﬁnition 1.3 (Nearly isometrically distributed map). Let
in linear maps Rm

Rp. We say that

n

×

A

is nearly isometrically distributed if, for

be a random variable that takes values

→

A

Rm

X

∀

∈

n,

×

(3)

E

2
(X)
k

kA
h

i

=

X

2
F

k

k

2

,
(cid:1)(cid:17)

(4)

(5)

and for all ε

∈

(0, 1), we have

Pr

(X)
k

k A

2
2 − k

X

2
F

k

X

ε

k

k

2
F

≥

≤

2 exp

(cid:0)
and there exists some constant γ > 0 such that for all t > 0, we have

(cid:1)

(cid:0)(cid:12)
(cid:12)

(cid:12)
(cid:12)

p
2

ε2/2

−

ε3/3

−

(cid:16)

Pr

k A k ≥

(cid:18)

1 +

r

mn
p

+ t

exp

≤

(cid:19)

γpt2

.

−
(cid:0)

(cid:1)

There are two crucial properties for a random map to be nearly isometric. Firstly, it needs to
be isometric in expectation as in (3), and exponentially concentrated around the expected value as
in (4). Secondly, the probability of large distortions of length must be exponentially small as in (5).
This ensures that even after taking a union bound over an exponentially large covering number for
LSm,n(r, s, µ), see Lemma 2.3, the probability of distortion remains small [25, 11].

In addition to developing RIC bounds as in Deﬁnition 1.2 we also show that the RIC of an operator
implies uniqueness of the decomposition and that exact recovery is possible with computationally
eﬃcient algorithms such as convex relaxations or gradient descent methods. The following subsection
summarizes our main contributions. The rest of the paper is organized as

• In Section 2, we prove that the RICs of LSm,n(r, s, µ) for Gaussian and fast Johnson-Lindenstrauss
transform (FJLT) measurement operators remain bounded independent of problem size provided
the number of measurements p is proportional to

• In Section 3, we prove that when the RICs of

O
) are suitably bounded then the solution to a
linear system
(X0) = b has a unique decomposition in LSm,n(r, s, µ) that can be recovered using
computationally tractable convex optimization solvers and hard thresholding gradient descent
algorithms which are natural extensions of algorithms developed for compressed sensing [29] and
matrix completion [30]. These results also provably solve Robust PCA with the asymptotically
(1/(µr)), where s = α2mn, and improve the previously
optimal fraction of corruptions α =
known guarantees by not requiring the fraction of the sparse corruptions in every column and
row is bounded by some α

(r(m + n

r) + s).

(0, 1).

(
·
A

A

O

−

∈

• In Section 4, we empirically study the average case of recovery on synthetic data by solving
convex optimization and by the proposed gradient descent methods and observe a phase transi-
tion in the space of parameters for which the methods succeed. We also give an example of two
practical applications of the low-rank plus sparse matrix recovery in the form of a subsampled
dynamic-foreground/static-background video separation and robust recovery of multispectral
imagery.

1.1. Main contribution

We show that for suﬃciently incoherent matrices the LSm,n(r, s, µ) set is a closed set, which is es-
(√mn/(r√s)).

sential in developing the recovery guarantees with asymptotically optimal scaling µ =

O

Lemma 1.1 (LSm,n(r, s, µ) is a closed set). Let µ < √mn/(r√s) and X = L + S
Then the following holds

∈

LSm,n(r, s, µ).

(1)

(2)

(3)

L, S

|h

µ r√s

√mn k
µ2 r2s
mn

L

S

kF k
1/2
−

kF ,
X

i| ≤
1

kF ≤

k
LSm,n(r, s, µ) is a closed set.

−

(cid:16)

(cid:17)

k

L

kF and

S
k

kF ≤

(cid:16)

µ2 r2s
mn

1

−

(cid:17)

1/2

−

X
k

kF ,

3

The proof, given in Appendix B on page 30, is a consequence of an upper bound on the magnitude
of the inner product beteween a suﬃciently incoherent low-rank matrix and a sparse matrix and then
employing this bound to show that the Frobenius norm of the two components is upper bounded by
the Frobenius norm of their sum, which also makes LSm,n(r, s, µ) a closed set.

The foundational analytical tool for our recovery results is the RIC for LSm,n(r, s, µ), which as
for other RICs [25, 11], follows from balancing a covering number for the set LSm,n(r, s, µ) and the
measurement operator being a near isometry as deﬁned in Deﬁnition 1.3.

Theorem 1 (RIC for LSm,n (r, s, µ)). For a given m, n, p
(0, 1), and
Rp satisfying the concentration of measure inequalities in
a random linear transform
Deﬁnition 1.3, there exist constants c0, c1 > 0 such that the RIC for LSm,n(r, s, µ) is upper bounded
with ∆r,s,µ(

∆ provided

N, µ < √mn/(r√s), ∆

: Rm

→

A

∈

∈

×

n

)
A

≤

p > c0 (r(m + n

r) + s) log

−

1
 (cid:18)

−

µ2 r2s
mn

1/2 mn

−

(cid:19)

s !

,

(6)

with probability at least 1

exp (

−

−

c1p), where c0, c1 are constants that depend only on ∆.

Theorem 1 establishes that for random ensembles of linear transformations that satisfy the con-
centration of measure inequalities in Deﬁnition 1.3, the RIC for LSm,n(r, s, µ) is upper bounded in
the asymptotic regime as m, n and p approach inﬁnity at appropriate rates and the incoherence µ is
suﬃcently upper bounded ensuring the set is closed; see Lemma 1.1. Speciﬁcally, the RIC remains
bounded independent of the problem dimensions m and n provided p to be taken proportional to the
order of degrees of freedom of the rank-r plus sparsity-s matrices times a logarithmic factor as in (6).
which satisfy the conditions of Deﬁnition 1.3 include random
Gaussian ensemble which acquires the information about the matrix X through p linear measurements
of the form

Examples of random ensembles of

A

bℓ :=

A(ℓ), X
h
Rm
where the p distinct sensing matrices A(ℓ)
entries sampled from the Gaussian distribution as A(ℓ)
symmetric Bernoulli ensembles, and Fast Johnson-Lindenstrauss Transform (FJLT) [31, 32].

i
n are the sensing operators deﬁning
×

and have
(0, 1/p). Other notable examples include

ℓ = 1, 2, . . . , p,

i,j ∼ N

(X)ℓ =

(7)

for

A

A

∈

For a linear transform

which has RIC suitably upper bounded and a given vector of samples
(X0), the matrix X0 is the only matrix in the set LSm,n(r, s, µ) that satisﬁes the linear constraint.

A

b =

A

Theorem 2 (Existence of a unique solution for
some integers r, s
LSm,n(r, s, µ) satisfying

1 and µ < √mn/(r√s). Let b =

(X) = b.

A

≥

A

A

with RIC). Suppose that ∆2r,2s,µ(

) < 1 for
(X0), then X0 is the only matrix in the set

A

= X0. Then Z := X0 −

(X) = b
Proof. Assume, on the contrary, that there exists a matrix X
and X
LSm,n(2r, 2s, µ)
by the subadditivity property of LSm,n(r, s, µ) sets in Lemma Appendix B.1. But then by the RIC
2
F > 0, which is a contradiction.
we would have 0 =
k

∈
X is a non-zero matrix for which

LSm,n(r, s, µ) such that

2
(Z)
2 ≥
k

(Z) = 0 and Z

∆2r,2s,µ)

kA

(1

A

A

−

Z

∈

k

As in compressed sensing and matrix completion, it is in general NP-hard to recover X0 = L0+S0 ∈
LSm,n(r, s, µ) from
mn. This follows from the non-convexity of the
≪
has suﬃciently small
feasible set LSm,n(r, s, µ). However, we show that if the linear transformation
RIC over the set LSm,n(r, s, µ), which requires µ < √mn/(r√s), then the solution can be obtained
with computationally tractable methods such as by solving the semideﬁnite program

(X0) for minimal r, s when p

A

A

X=L+S

Rm×n k

min
∈

L

k∗

+ λ

S

k1 ,

k

s.t.

(X)

kA

b

−

k2 ≤

εb,

(8)

4

6
Algorithm 1 Normalized Iterative Hard Thresholding (NIHT) for LS recovery
, r, s, and termination criteria
∗(b); r, s, µ, ε) , X 0 = L0 + S0, j = 0

Input: b =
Set: (L0, S0) =

A

Ω0 = supp(S0) and U 0 as the top r left singular vectors of L0

(X0),
A
(
A

P

1: while not converged do
2:

Compute the residual Rj =

3:

4:

5:
6:

∗

(X j)

b

A

A
(cid:0)

−
Proj(U j ,Ωj )
(cid:13)
(cid:13)
(cid:13)

2

Rj
(cid:1)

Compute the stepsize: αj =
2
Set W j = X j
(cid:1)(cid:17)(cid:13)
(cid:13)
Compute (Lj+1, Sj+1) = RPCAr,s,µ(W j, εp) and set X j+1 = Lj+1 + Sj+1
(cid:13)
Let Ωj+1 = supp(Sj+1) and U j+1 be the top r left singular vectors of Lj+1
j = j + 1

Proj(U j ,Ωj )

F
(cid:1)(cid:13)
(cid:13)
(cid:13)

αj Rj

(cid:13)
(cid:13)
(cid:13)

A

−

(cid:16)

/

(cid:0)

(cid:0)

Rj

2

7:
8: end while

Output: X j

where
k · k∗
is the model misﬁt.

is the Schatten 1-norm and

k · k1 is the sum of the absolute value of the entries2 and εb

√mn . Let X ∗ = L∗ + S∗ be the solution of (8) with λ =

Theorem 3 (Guaranteed recovery by the convex relaxation). Let b =
and µ < √mn/(4r√2s) are such that the restricted isometry constant ∆4r,2s,2µ(
γ := µ 4r√2s
X ∗

1
7 −
≤
X0kF ≤
(X0) by iterative gradient
descent methods that are guaranteed to converge to a global minimizer of the non-convex optimization
problem

(X0) and suppose that r, s
)
A
−

Alternatively, X0 can be obtained from its compressed measurements

1
2γ where
42 εb.

2r/s, then

p

A

A

≥

k

X=L+S

Rm×n kA

min
∈

(X)

b

k2 ,

−

s.t. X

∈

LSm,n(r, s, µ).

(9)

We introduce two natural extensions of the simple yet eﬀective Normalized Iterative Hard Thresholding
(NIHT) for compressed sensing [29] and matrix completion [30] algorithms, here called NIHT and
Normalized Alternative Hard Thresholding (NAHT) for low-rank plus sparse matrices, Algorithms 1
and 2 respectively. In both cases we establish that if the measurement operator has suitably small
RICs then NIHT and NAHT provably converge to the global minimum of the non-convex problem
LSm,n(r, s, µ) for which b =
formulated in (9) and recover X0 ∈
Theorem 4 (Guaranteed recovery by NIHT). Suppose that r, s
that the restricted isometry constant ∆3 := ∆3r,3s,µ(
A
described in Algorithm 1 will linearly converge to X0 as

(X0).
N and µ < √mn
3r√3s
5 . Then NIHT applied to b =

are such
(X0) as

) < 1

(cid:14) (cid:0)

A
(cid:1)

A

∈

(10)

X j+1

X0

−

4∆3

F ≤

1

∆3

X j

X0

−

F + εp,
(cid:13)
(cid:13)

−
within the precision of εp, where εp is the accuracy of the Robust PCA oblique projection that performs
projection on the set of incoherent low-rank plus sparse matrices LSm,n(r, s, µ).

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

Theorem 5 (Guaranteed recovery by NAHT). Suppose that r, s
) < 1
such that the restricted isometry constant ∆3 := ∆3r,3s,µ(
NAHT applied to b =

A

∈
9 −

(X0) as described in Algorithm 2 will linearly converge to X0 = L0 + S0 as

N and µ < √mn
γ2 where γ2 := µ 2r√2s

3r√3s
are
√mn . Then

(cid:14) (cid:0)

(cid:1)

A

Lj+1

L0

−

(cid:13)
(cid:13)

F +
(cid:13)
(cid:13)

Sj+1

S0

−

F ≤

1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

6∆3 + 9
3∆3 −

8 γ2
9
8 γ2

−

Lj

−

L0

F +

Sj

−

S0

F

.

(cid:0)(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

(11)

2Our use of k · k1 as the sum of the modulus of the entries of a matrix diﬀers from the vector induced 1-norm of a

matrix.

5

Algorithm 2 Normalized Alternating Hard Thresholding (NAHT) for LS recovery

Input: b =
Set: (L0, S0) =

A

(X0),
A
(
A

P

, r, s, and termination criteria
∗(b); r, s, τ, ε) , X 0 = L0 + S0, j = 0

Ω0 = supp(S0) and U 0 as the top r left singular vectors of L0

1: while not converged do
2:

Compute the residual Rj

Compute the stepsize αL

b

−

Rj
(cid:1)

2

L =
j =

(X j)

∗
A
A
Proj(U j ,Ωj )
(cid:0)
(cid:13)
(cid:13)
(cid:13)

Proj(U j ,Ωj )

Rj

A

2

j Rj
αL

2
(cid:1)(cid:17)(cid:13)
Set V j = Lj
(cid:13)
(cid:13)
Set Lj+1 = HT(V j; r, µ) and let U j+1 be the left singular vectors of Lj+1
Set X j+ 1
Compute the residual Rj

2 = Lj+1 + Sj

(X j+ 1
2 )

. (cid:13)
(cid:13)
(cid:13)

(cid:1)(cid:13)
(cid:13)
(cid:13)

−

(cid:16)

(cid:0)

(cid:0)

b

L

F

S =

−
Rj

(cid:16)

A

∗
A
Proj(U j+1,Ωj )
(cid:13)
(cid:13)
(cid:13)

2

(cid:17)

F
(cid:1)(cid:13)
(cid:13)
(cid:13)

Proj(U j+1,Ωj )

Rj

(cid:16)

(cid:0)

2

2
(cid:1)(cid:17)(cid:13)
(cid:13)
(cid:13)

A

. (cid:13)
(cid:13)
(cid:13)

Compute the stepsize αS

j =

−

j Rj
αS

Set W j = Sj
Set Sj+1 = HT(W j ; s) and let Ωj+1 = supp(Sj+1)
Set X j+1 = Lj+1 + Sj+1
j = j + 1

(cid:0)

S

3:

4:

5:

6:

7:

8:

9:

10:
11:

12:
13: end while

Output: X j = Lj + Sj

◦

−

(Rj

PU j Rj), where PU j := U j

Note that the projection used in computing the stepsize is deﬁned as Proj(U j ,Ωj )

:= PU j Rj +
denotes
1Ωj
the entry-wise Hadamard product. This corresponds to ﬁrst projecting the left singular vectors of
Rj on the subspace spanned by columns of U j, which makes the incoherence of the resulting matrix
bounded by µ, and then setting entries at indices Ωj to be equal to the entries of Rj at indices Ωj.
One can repeat this process to achieve better more precise projection of Rj in the low-rank plus sparse
matrix set deﬁned by

∗, 1Ωj is a matrix with ones at indices Ωj, and

U j, Ωj

U j

Rj

◦

(cid:1)

(cid:0)

(cid:1)

(cid:0)

.

The hard thresholding projection in Algorithm 1 is performed by computing Robust PCA which
n on

is solved to an accuracy proportional to εp. The Robust PCA projection of a matrix W
LSm,n(r, s, µ) such that
the set of LSm,n(r, s, µ) with precision εp returns a matrix X

Rm

∈

(cid:1)

(cid:0)

×

∈

X

←

RPCAr,s,µ(W, εp)

s.t.

X

k

WrpcakF ≤

−

εp,

(12)

where Wrpca := arg minY
kF is the optimal projection of the matrix W on the set
LSm,n(r, s, µ), which can be computed by a number of eﬃcient Robust PCA algorithms, such as the
Alternating Projection algorithm (AltProj) [28] or the Accelerated Alternating Projection algorithm
(AccAltProj) by [33], which have high robustness in practice and provable global linear convergence
when α =

respectively, where s = α2mn.

(1/ (µr)) and α =

LSm,n(r,s,µ) k

µr2

1/

W

−

Y

∈

O

O

1.2. Relation to prior work

(cid:0)

(cid:0)

(cid:1)(cid:1)

It is well known that the low-rank plus sparse matrix decomposition solved by Robust PCA does
not need to have a unique solution without further constraints, such as the singular vectors of the
low-rank component being uncorrelated with the canonical basis as quantiﬁed by the incoherence
condition [9, 11] with parameter µ

max
1,...,m

U ∗eik2 ≤

k

i
∈{

µr
m

,

r

max
1,...,n

V ∗fik2 ≤

k

i
∈{

µr
n

r

}
r are the ﬁrst r left and the right singular vectors of L respectively,
×
Rn are the canonical basis vectors. The incoherence condition for small values of µ

r, V

Rn

∈

×

}

,

(13)

where U
ei ∈

Rm
∈
Rm, fi ∈

6

ensures that the left and the right singular vectors are well spread out and not sparse. It is therefore
LSm,n(r, s, µ) from subsampled measurements
sensible to expect that the problem of recovering X0 ∈
should obey the same conditions. The incoherence assumption is directly assumed in the convergence
(√mn/(r√s)) which is
analysis of NAHT and the convex recovery is assumed where we require µ <
equivalent to the best known recovery bounds in Robust PCA [27, 28]. The incoherence assumption
is also implicitly used in the convergence analysis of NIHT in the Robust PCA projection step in
Algorithm 1, Line 5, the solution of which is dependent on the incoherence of L.

O

The results presented here extend the well developed literature on compressed sensing and matrix
completion/sensing [4, 5] to the setting of low-rank plus sparse matrices as deﬁned in Deﬁnition 1.1.
These foundational RIC bound results allow for further extension to other non-convex algorithms,
such as [34], further model based constraints as in [35] and other additive models.

The recovery result by convex relaxation in Theorem 3 controls the measurement error and/or
model mismatch εb. In the proof of NIHT convergence in Theorem 4 we consider exact measurements
but we control the error of the Robust PCA projection which is assumed to be solved only within
prescribed precision εp. The convergence result of NAHT in Theorem 5 alternates between projecting
of the low-rank and the sparse component. The non-convex algorithms are also stable to error εb, but
we omit the stability analysis for clarity in the proofs.

Theorem 3, 4, and 5 also provably solve Robust PCA when

is chosen to be the identity and
µ < √mn/(r√s) which translates to the optimal scaling in terms of the number of corruptions
(1/(µr)), where s = α2mn, but without the need of requiring that the fraction of the sparse
α =
corruptions in every column and row is bounded by α.

A

O

2. Restricted Isometry Constants for LSm,n (r, s, µ)

This section presents a proof of Theorem 1, that linear maps

Rp sampled from
a class of probability distributions obeying concentration of measure and large deviation inequali-
ties, have bounded RIC for sets of low-rank plus sparse matrices with bounded energy as deﬁned in
Deﬁnition 1.2. More precisely, that the RIC of
remains bounded independent of dimension once
A
1/2 mn
. Examples of linear maps which satisfy these
p
s
bounds include random Gaussian matrices and the Fast Johnson-Lindenstrauss transform (FJLT)
[31, 32]. We extend the method of proof used in the context of sparse vectors by [25] and its alteration
for the low-rank matrix recovery by [11].

r) + s) log

(r(m + n

µ2 r2s

mn )−

: Rm

≥ O

(cid:17)(cid:17)

→

(1

A

−

−

(cid:16)

(cid:16)

×

n

Our proof of Theorem 1 follows from proving the alternative form of (2) deﬁned without the

squared norms by

1

−

¯∆r,s,µ(

A

)

X

k

kF ≤ k A

(X)

k2 ≤

1 + ¯∆r,s,µ(

)
A

X

kF ,

k

(14)

which we denote as ¯∆. The discrepancy between (14) and (2) is due to (14) being more direct to
derive and (2) allowing for more concise derivation of Theorem 3, 4, and 5, but the two deﬁnitions
are related up to a multiplicative constant3.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

The proof of Theorem 1 begins with the derivation of an RIC for a single subspace Σm,n(V, W, T, µ)
(LT )

(L) is restricted in the subspace V , the row space

of LSm,n(r, s, µ) when the column space of
in the subspace W , and the sparse component S is in the subspace T ,

C

C

X = L + S

Σm,n (V, W, T, µ) = 


3The constant ¯∆ satisﬁying the inequalities in (14) also implies (cid:0)1 − ¯∆(cid:1)

kXk2
which in turn ensures that ∆ in Deﬁnition 2 is ∆ = 2 ¯∆ − ¯∆2 ∈ [0, 1] when ¯∆ ∈ [0, 1].

[m] :
[n] :

i
∀
i
∀

∈
∈

∈

×

2

Rm

n :

µr
m ,
µr
n

,




2
2 ≤ (cid:0)1 + ¯∆(cid:1)

(15)

kXk2

F ,

p
p

F ≤ k A(X)k2

(L)

C

W,

(LT )

V,
C
⊆
⊆
supp (S)
T,
⊆
PV eik2 ≤
k
PW fik2 ≤
k

7

(
·
A
LSm,n(r, s, µ) =

Rn are the canonical basis vectors.

Rm and fi ∈
Following this, we show that the isometry constant of

where PV and PW denote the orthogonal projection on the subspace V and W respectively, and
ei ∈
is robust to a perturbation of the column
and the row subspaces (V, W ) of the low-rank component. Finally, we use a covering argument over
all possible column and row subspaces (V, W ) of the low-rank component and count over all possible
sparsity subspaces T of the sparse component to derive an exponentially small probability bound for
the event that

) satisﬁes RIC with constant ¯∆ for sets

A

where

Σm,n(V, W, T, µ) : V
{

(16)
(m, r) is the Grassmannian manifold – the set of all r-dimensional subspaces of Rm, and
n matrix that has s elements. Thus proving
V
RIC for sets of low rank plus sparse matrices given the energy bound on the low-rank component L.
when constrained to a single ﬁxed column and a

(mn, s) is the set of all possible supports sets of an m

(mn, s)
}

(m, r), W

(n, r), T

∈ V

∈ G

∈ G

×

G

,

The following result describes the behavior of
row space (V, W ) and a single sparse matrix space T .

A

Rp be a nearly
Lemma 2.1 (RIC for a ﬁxed LS subspace Σm,n(V, W, T, µ)). Let
isometric random linear map from Deﬁnition 1.3 and Σm,n(V, W, T, µ) as deﬁned in (15) is ﬁxed for
some (V, W ), T and µ < √mn/(r√s). Then for any ¯∆

: Rm

(0, 1)

→

A

×

n

X

∀

∈

Σm,n (V, W, T, µ) :

(1

¯∆)
k

X

−

with probability at least

∈

kF ≤ k A

(X)

(1 + ¯∆)
k

X

kF ,

k ≤

24
¯∆

dim V

dim W

·

dim T

24
¯∆

τ

(cid:19)

(cid:18)

exp

p
2

(cid:18)

−

(cid:18)

¯∆2
8 −

¯∆3
24

,

(cid:19)(cid:19)

τ

(cid:19)

1

2

−
µ2 r2s

mn )−

(cid:18)
1/2.

where τ := (1

−

The proof follows the same argument as the one for sparse vectors [25, Lemma 5.1] and for low-rank
matrices in [11, Lemma 4.3] with the exception of appropriately scaling the Frobenius norm of the
two components in relation to the Frobenius norm of their sum. Our variant of the proof for low-rank
plus sparse matrices is presented in Appendix B on page 32.

To establish the impact of a perturbation of the spaces (U, V ) on the ¯∆ in Lemma 2.1 we deﬁne a

metric ρ(
·

,

·

) on

G

(D, d) as follows

PU2 k
) as in (19) deﬁnes a metric space
)), where PU denotes an orthogonal projection associated with the subspace U . Let us

The Grassmannian manifold
(
,
G
also denote a set of matrices whose column and row space is a subspace of V and W respectively

PU1 −
,
(D, d) combined with distance ρ(
·
·

(D, d) , ρ (
·

U1, U2 ∈ G

ρ(U1, U2) :=

(D, d) :

(19)

G

k

·

.

(V, W ) =

X :

(X)

V,

C

⊆

C

(X T )

W

,

⊆

(20)

and P(V,W ) is an orthogonal projection that ensures that the column space and row space of P(V,W )X
lies within V and W . The distance between Σ1 := Σm,n (V1, W1, T, µ) and Σ2 := Σm,n (V2, W2, T, µ)
that have a ﬁxed T is given by

(cid:9)

(cid:8)

(17)

(18)

ρ ((V1, W1) , (V2, W2)) =

P(V1,W1) −

P(V2,W2)k

.

k
Lemma 2.2 (Variation of ¯∆ in RIC in respect to a perturbation of (V, W )). Let Σ1 := Σm,n(V1, W1, T, µ)
and Σ2 := Σm,n(V2, W2, T, µ) be two low-rank plus sparse subspaces with the same ﬁxed subspace T
and µ < √mn/(r√s). Suppose that for ¯∆ > 0, the linear operator

(21)

X

∀

∈

Σ1 :

(1

¯∆)
k

X

−

kF ≤ k A

(X)

k ≤

A
(1 + ¯∆)
k

satisﬁes
kF .

X

Then

(1
with ¯∆′ := ¯∆+τ ρ ((V1, W1) , (V2, W2))

Σ2 :

∈

Y

∀

¯∆′)
Y
−
k
1 + ¯∆ +

kF ≤ k A

(Y )

(1 + ¯∆′)
k
with ρ as deﬁned in (19) and τ := (1

kF ,

k ≤

Y

(cid:0)

k A k

(cid:1)

8

(22)

(23)

µ2 r2s

mn )−

1/2.

−

The proof is similar to the line of argument made in [11, Lemma 4.4], see Appendix B on page 35.
1/2 appearing in the expression for ¯∆′, which is
The notable exception is the term τ := (1
mn )−
a result of the set LSm,n(r, s) not being closed, as shown in [26, Theorem 1.1], without the constraint
L

kF ≤
To establish the proof of Theorem 1 we combine Lemma 2.1 and Lemma 2.2 with an ε-covering
of LSm,n(r, s, µ), where ε will be picked to control the maximal allowed perturbation between the
subspaces ρ ((V1, W1) , (V2, W2)). The covering number R(ε) of LSm,n(r, s, µ) at resolution ε is the
smallest number of subspaces (Vi, Wi, Ti) such that, for any triple of V

kF from Lemma 1.1.

(m, r), W

(n, r), T

µ2 r2s

X

−

k

k

τ

∈
∈ G
ε and T = Ti. The following Lemma gives an upper

∈ G

(mn, s) there exists i with ρ ((V, W ) , (Vi, Wi))

V
bound on the cardinality of ε-covering.

≤

Lemma 2.3 (Covering number of LSm,n(r, s)). The covering number R(ε) of the set LSm,n(r, s) is
bounded above by

R(ε)

mn
s

(cid:18)

≤

(cid:19) (cid:18)

4π
ε

r(m+n

−

2r)

.

(cid:19)

(24)

The proof comes by counting the possible support sets with cardinality s and by the work of
Szarek on ε-covering of the Grassmannian [36, Theorem 8], for completeness the proof is given in
Appendix B, page 34.

Bounds on the RIC for the set of low-rank plus sparse matrices then follow a proof technique that
uses the covering number argument in combination with the concentration of measure inequalities as
was done before for sparse vectors [25] and subsequently for low-rank matrices [11].
Proof of Theorem 1 (RIC for LSm,n(r, s, µ)), stated on page 4.

Proof. By linearity of
kF = 1
1/2 by Lemma 1.1 and
and consequently also
by µ < √mn/(r√s). Let (Vi, Wi, Ti) be an ε-covering of LSm,n(r, s, µ) whose covering number is
bounded by Lemma 2.3 since LSm,n(r, s, µ)
LSm,n(r, s). For every triple (Vi, Wi, Ti) deﬁne a subset
of matrices

and conicity of LSm,n(r, s, µ) assume without loss of generality
τ with τ := (1
L

kF ≤

kF ≤

µ2 r2s

mn )−

τ and

A
k

X

⊂

−

S

k

k

{
By (Vi, Wi, Ti) being an ε-covering we have LSm,n(r, s, µ)

∈

Bi =

X

Σm,n (V, W, Ti, µ) : ρ ((V, W ) , (Vi, Wi))

.

}

ε
≤
i Bi. Therefore, if for all
S
X
k ≤

(1 + ¯∆)
k

kF

⊆
(X)

Bi

X

(
∀

∈ Bi) :

(1

¯∆)
k

X

−
¯∆, proving that

kF ≤ k A

holds, then necessarily ¯∆r,s,µ ≤
X

¯∆) = Pr

Pr( ¯∆r,s,µ ≤

∀

Pr

≥

(
∀

(cid:16)
i), (

X

∀

(cid:16)

LSm,n(r, s, µ) : (1

¯∆)
k

−

X

∈
∈ Bi) : (1

¯∆)
k

X

−

kF ≤ k A

(X)

kF ≤ k A
(X)

k ≤

k ≤
X

(1 + ¯∆)
k

(1 + ¯∆)
k

X

kF

(cid:17)

kF

,

(cid:17)

where the inequality comes from the fact that LSm,n(r, s, µ) is a subset of the ε-covering
i Bi and
therefore the statement holds with less or equal probability. It remains to derive a lower bound on
S
the probability in the equation (28) which in turn proves the theorem.
1

¯∆
2 , which we show later in (37) occurs with probability expo-

In the case that

k A k ≤

¯∆
2τ ε −

−

nentially converging to 1, rearranging the terms yields

τ ε(1 + ¯∆/2 +

)

¯∆/2.

k A k
If the RIC holds for a ﬁxed (Vi, Wi, Ti) with ¯∆/2, then by Lemma 2.2 in combination with (29)

≤

yields

X

(
∀

∈ Bi) : (1

−

¯∆)
k

X

kF ≤ k A

(X)

(1 + ¯∆)
k

X

kF .

(30)

k ≤

9

(25)

(26)

(27)

(28)

(29)

Therefore, using the probability union bound on (28) over all i’s and the probability of
¯∆/ (2τ (1 +
the bound ε

)) by (29) and ¯∆

¯∆/ (2τ (1 +

0.

))

≤

k A k

k A k

Pr

(
∀

i), (

X

∀

(cid:16)

i
X
Pr

1

−

≥

−

Pr

Y

∃

(cid:18)

kAk ≥

(cid:16)

≤
∈ Bi) : (1

¯∆)
k

X

−

kF ≤ k A

(X)

k ≤

X

≥
(1 + ¯∆)
kF
k
(cid:17)
¯∆/2)
> (1 + ¯∆/2)

< (1

−

(cid:19)

kA

or

(Y )
k
(Y )
k

kA

Σm,n(Vi, Wi, Ti, µ) :

∈
¯∆
2τ ε −

¯∆
2

1

−

.

(cid:17)

The probability in (32) is bounded from above as

Σm,n(Vi, Wi, Ti, µ) :

or

∈

Pr

i
X

Y

∃

(cid:18)

2 R(ε)

≤

48
¯∆

τ

(cid:19)

(cid:18)

2

≤

mn
s

(cid:18)

(cid:19) (cid:18)

4π
ε

(cid:19)

r2

48
¯∆
(cid:18)
r(m+n

s

τ

exp

(cid:19)
2r)

−

p
2
(cid:18)
r2+s

−

(cid:18)

48
¯∆

τ

(cid:19)

(cid:18)

kA

kA

(Y )
k
(Y )
k
¯∆3
192

¯∆2
32 −

exp

p
2

−

(cid:18)

< (1

¯∆/2)
> (1 + ¯∆/2)

−

(cid:19)

(cid:19)(cid:19)
¯∆2
32 −

(cid:18)

¯∆3
192

,

(cid:19)(cid:19)

satisfying

k A k

(31)

(32)

(33)

(34)

(35)

(36)

where in the ﬁrst inequality we used Lemma 2.1 and in the second inequality the bound on the
ε-covering of the subspaces by Lemma 2.3.

In order to complete the lower bound in (31) it remains to upper bound (33) which we obtain by

selecting the covering resolution ε suﬃciently small so that the Pr

k A k ≥

¯∆
2τ ε −

1

−

¯∆
2

is exponen-

tially small with the exponent proportional to the bound in (36). From condition (5) of Deﬁnition 1.3
we have that the random linear map satisﬁes

(cid:16)

(cid:17)

γ > 0) : Pr

(
∃

k A k ≥

(cid:18)

1 +

r

mn
p

+ t

exp

≤

(cid:19)

−
(cid:0)

γpt2

,

(cid:1)

in particular

Pr

k A k ≥

(cid:18)

¯∆
2τ ε −

1

−

¯∆
2

exp

γp

 −

(cid:18)

≤

(cid:19)

¯∆
2τ ε −

¯∆
2 −

mn
p −

2

2

.

!

(cid:19)

r

Selecting the covering resolution ε

ε <

¯∆

4τ

mn/p + 1 + ¯∆/4

obtains the following exponentially small upper bound

(cid:16)p

,

(cid:17)

Pr

k A k ≥

(cid:18)

¯∆
2τ ε −

1

−

¯∆
2

exp (

−

γmn) .

≤

(cid:19)

(37)

(38)

(39)

(40)

Returning to the inequality (31), combined with the bound on the ﬁrst term in (36), and setting

ε = ¯∆

4τ

mn/p + 1 + ¯∆/4

in the second term of (36), such that (39) is satisﬁed, we have

. (cid:16)

(cid:16)p

(cid:17)(cid:17)

10

that

emn
s

2

(cid:16)

(cid:17)

s

16π(

mn/p + 1 + ¯∆/4)

¯∆

p

τ

r(m+n

−

2r)

r2+s

48
¯∆

τ

(cid:19)

(cid:18)

exp

·

−

(cid:18)

p
2

¯∆3
192

(cid:19)(cid:19)

!
¯∆2
32 −
¯∆
4

+ 1 +

(cid:18)

(41)

(42)

= exp

  −

pa( ¯∆) + r (m + n

2r) log

−

mn
p

(cid:18)r

+ r (m + n

2r) log

−

(cid:19)

16π
¯∆

τ

(cid:19)

(cid:18)

+ (r2 + s) log

48
¯∆

τ

(cid:19)

(cid:18)

+ s log

+ log(2)

,

emn
s

!

(cid:17)

(cid:16)
and we deﬁne a( ¯∆) := ¯∆2/64

where we used the inequality
and 4th terms in (42) can be bounded as

≤

mn
s

emn
s

(cid:0)

(cid:1)

(cid:0)

s

(cid:1)

¯∆3/384. The 2nd, 3rd

−

c2 > 0) :

(
∃

2nd + 3rd + 4th

≤

c2/a( ¯∆)

r(m + n

and the 5th and 6th term of (42) as

(cid:0)

(cid:1)

r) log

−

mn
p

τ

,

(cid:19)

(cid:18)

(43)

c3 > 0) :

(
∃

5th + 6th

c3/a( ¯∆)

s log

≤
(cid:0)
where c2 and c3 are dependent only on ¯∆. Therefore there exists positive constants c0, c1 that
depend4 only on ¯∆ such that if p
, then RICs are upper bounded
c0 (r(m + n
by the constant ¯∆ with probability at least e−
c1p. By the constant ¯∆ in (14) being related to the
RIC with squared norms, the result also implies an upper bound on RICs with the squared norms in
Deﬁnition 1.2.

r) + s) log

mn
s τ

≥

−

(cid:16)

(cid:17)

(cid:1)

(cid:0)

(cid:1)

(44)

mn
s

τ

,

3. Provable recovery guarantees using computationally eﬃcient algorithms

This section contains the proofs of our main algorithmic contributions that a low-rank plus sparse
LSm,n(r, s, µ) can be eﬃciently recovered from subsampled measurements taken by a
matrix X0 ∈
) which satisﬁes given bounds on its RIC. These algorithms also provably solve
linear mapping
(
·
A
Robust PCA when
which is the optimal scaling
A
in terms of the number of corruptions, rank, and the incoherence. Subsection 3.1 presents the proof
of Theorem 3 which shows that the convex relaxation (8) of (9) robustly recovers X0. Subsection 3.2
states the proofs of Theorem 4 and Theorem 5 for the simple yet eﬃcient hard thresholding algorithms
NIHT and NAHT, described in Alg. 1 and Alg. 2 respectively.

is chosen to be the identity and s =

mn/(µ2r2)

O

(cid:1)

(cid:0)

3.1. Recovery of X0 ∈

LSm,n(r, s, µ) using the convex relaxation (8).

Let X ∗ = L∗ + S∗ be the solution of the convex optimization problem formulated in (8). Here it
is shown that if the RICs of the measurement operator
) are suﬃcient small, then X ∗ = X0 when
the linear constraint in the convex optimization problem (8) is satisﬁed exactly, or alternatively that
X ∗
k
Proof of Theorem 3 (Guaranteed recovery by the convex relaxation (8)), stated on page 5.

X0kF is proportional to

(
·
A

(X ∗)

k2.

kA

−

−

b

4We have that c1 = (1 + γ)a( ¯∆)−1 and c0 = 16π/( ¯∆ a( ¯∆)).

11

 
(47)

(48)

(49)

(50)

−

L0) + (S∗

X0 = (L∗

S0) = RL + RS be the residual split into the low-
Proof. Let R = X ∗
rank component RL = L∗
S0. We treat RL and RS
L0 and the sparse component RS = S∗
separately, combining the method of proof used in the context of compressed sensing by [37] and
its extension for the low-rank matrix recovery by [11] with the important exception of needing to
decompose RL into a sum of incoherent low-rank matrices using Lemma Appendix B.6 and carefully
treat its correlation with RS.

−

−

−

−

By Lemma Appendix B.4 on page 35 there exist matrices RL

0 , RL

c ∈

Rm

n such that RL = RL

0 +RL
c

×

and

RL
0 ∈
c )T = 0m

LSm,n(2r, 0, µ)
0 RL
m and LT

L0(RL

×

c = 0n

×

n.

(45)

(46)

Similarly, by the argument made in the proof of [37, Theorem 1], which we state in Lemma Appendix B.5,
there exist matrices RS

n such that RS = RS

Rm

0 + RS

c and

0 , RS

×

c ∈

0 ≤
supp
(cid:13)
(cid:13)
By (L∗, S∗) being a minimum and X0 being feasible of the convex optimization problem (8)

supp (S0)
(cid:13)
(cid:13)

=

∩

∅

(cid:0)

(cid:1)

.

RS
0

s
RS
c

+ λ

S0 + RS

0 + RS
c

1

L0k∗
k

+ λ

k

S0k1 ≥ k
=

S∗
k1
k
0 + RL
c

+ λ
L∗
k∗
L0 + RL
L0 + RL
(cid:13)
c
(cid:13)
L0k∗
(cid:13)
k
(cid:13)

1

+

≥
=

RS
0

+ λ
(cid:13)
(cid:13)

S0 + RS
c

∗ −
RL
(cid:13)
c
(cid:13)
(cid:13)
(cid:13)
−

∗
RL
(cid:13)
(cid:13)
0
1 −
(cid:13)
(cid:13)
∗
RL
S0k1 + λ
+ λ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
0
∗ −
k
(cid:13)
(cid:13)
(cid:13)
(cid:13)
∗
(cid:13)
(cid:13)
(cid:13)
0 + RL
L0 = RL
c , the inequality
c and S∗
where the second line comes from L∗
(cid:13)
(cid:13)
(cid:13)
in the third line comes from the reverse triangle inequality, and the fourth line comes from the
construction of RL
c combined with [11, Lemma 2.3], restated as Corollary Appendix B.1, and
by supp(RS
S0k1 from both sides of (52) and rearranging
c )
k
terms yields
RL
(53)
0

1 .
c and RS
c as sums of matrices with de-
(cid:13)
(cid:13)
(cid:13)
(cid:13)
creasing energy as was done by [11] for low-rank matrices and by [37] for sparse vectors. By
1 + RL
Lemma Appendix B.6 there exists a decomposition RL

We proceed by decomposing the remainder terms RL
(cid:13)
(cid:13)

λ
RS
(cid:13)
(cid:13)
c
1 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
0 + RS
S0 = RS
(cid:13)
(cid:13)

(cid:13)
(cid:13)
c = RL

c and RS
0 ) =

L0k∗
k
RS
c

2 + . . . such that

. Subtracting

1 ,
(cid:13)
(cid:13)

supp(RS

∗
(cid:13)
(cid:13)

RL
c

(52)

(51)

RS
0

RS
0

and

1 ≤

+ λ

+ λ

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

∩

λ

∅

∗

RL
i

RL
j

T

= 0m

(cid:0)

(cid:1)

LSm,n(Mr, 0, µ)

RL

i ∈
m and

×
RL

i+1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

2
F ≤

T

RL

j = 0n
2

×

(cid:1)

RL
i

.

RL
i
1
(cid:0)
Mr

∗
(cid:13)
(cid:13)

(cid:13)
(cid:13)

n,

= j

i
∀

(54)

(55)

(56)

To decompose the residual of the sparse component order the indices of RS

[n] in decreasing order of magnitude of the entries of RS

c as v1, v2, . . . , vmn ∈
c and split the indices of the entries into

[m]
sets of size Ms as

×

Constructing RS
i

:=

RS
c

Ti

(cid:0)

(cid:1)

vℓ : (i

Ti :=
decomposes RS

{

1)Ms ≤

ℓ
iMs}
≤
c = RS
c into a sum RS

,
1 + RS

−

2 + . . . such that

(cid:13)
∅
(cid:13)

RS
i
0 ≤
= Ti ∩
(cid:13)
(cid:13)
1
Ms

Ms,

Tj,

RS
i

i
∀
i
∀
(j) ,

1

≥
= j

v
∀

∈

Ti+1

RS
c

(cid:12)
(cid:12)

(v) ≤
(cid:12)
(cid:12)

Ti (cid:12)
Xj
∈
(cid:12)

(cid:12)
(cid:12)

12

(57)

(58)

(59)

(60)

6
6
RS
where the inequality (60) implies that
in LSmn(Mr, Ms, µ) by construction. Combining the two decompositions of RL
(cid:13)
(cid:13)
following bound

1. We denote Ri = RL
(cid:13)
(cid:13)

RS
i

i+1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1
Ms

i + RS
c and RS

i which are
c gives the

2
F ≤

2

(61)

(62)

(63)

(64)

(65)

(66)

(67)

RjkF ≤
k

2
Xj
≥

2
Xj
≥

RL
j

+

F

RS
j

F

(cid:13)
(cid:13)

1
Ms

r

1
Ms

1
Xj
≥

RS
c

1

RS
j

(cid:13)
(cid:13)

1

(cid:13)
(cid:13)

(cid:13)
Mr
(cid:13)
Ms

(cid:13)
(cid:13)
RS
0

1

!

(cid:13)
(cid:13)
+

2
Xj
≥

∗
(cid:13)
(cid:13)
+

r

+

(cid:13)
(cid:13)
1
Mr

1
Mr

(cid:13)
(cid:13)

RL
j

Xj
1
(cid:13)
≥
(cid:13)
RL
c

∗

(cid:13)
(cid:13)
RL
0

(cid:13)
1
(cid:13)
Mr  
2r
Mr

(cid:13)
(cid:13)
RL
0

∗
(cid:13)
(cid:13)
F +

r
s
Ms

r

≤

=

r

r

≤ r

≤

r

(cid:13)
(cid:13)
RS
0

(cid:13)
(cid:13)
F ,
(cid:13)
(cid:13)

(cid:13)
(cid:13)
where the inequality in the ﬁrst line comes from the triangle inequality, the second inequality comes as
a consequence of (56) and (60), the third line comes from (55) combined with [11, Lemma 2.3], restated
Mr/Ms,
as Corollary Appendix B.1, and from (59), the fourth inequality comes from (53) with λ =
and the last ﬁfth line is a property of ℓ1 and Schatten-1 norms. Choosing Mr = 2r and Ms = s in
(65) gives

p

(cid:13)
(cid:13)

(cid:13)
(cid:13)

RjkF ≤

k

RL
0

F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

RS
0

F ,
(cid:13)
(cid:13)

2
Xj
≥
2r/s as stated in the theorem.
we have

(cid:13)
(cid:13)

and also that λ =

By feasibility of X ∗ and linearity of

p

A
b

Rj)

2
Xj
≥

F +
(cid:13)
(cid:13)
F + 2
(cid:13)
(cid:13)

k2 =
Let ∆ := ∆4r,2s,µ be the RIC with squared norms for LSm,n(4r, 2s, µ) and γ := µ 4r√2s

εb ≥ kA

k2 =

k2 .

(X ∗)

X0)

(X ∗

(R)

kA

kA

−

−

√mn < 1. Then

(68)

(69)

(70)

2 kA

(R)

k2 ,

(71)

(1

−

∆)
k

RL
0 k
(RL

2
F ≤
0 ),

RL
0

A
(RL
(cid:0)

(cid:13)
A
(cid:13)

0 −

(RL
0 ),
(RL
0 ),

(RL

0 −

A

(R)

A

R + R)

(cid:11)(cid:12)
(cid:12)

2
2 =
A
+
R)
(cid:12)
(cid:1)(cid:13)
(cid:10)
(cid:12)
(cid:13)

A
(cid:11)
(cid:10)
R1 −
0 −

RL
0

,

(
−
A

RS

(cid:1)

(RL

0 ),

(R)

A

(cid:11)(cid:12)
(cid:12)

(cid:11)(cid:12)
(cid:12)

+

A
(cid:10)

+(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
R1kF +
k

≤

1

(cid:18)

γ2 + ∆
(cid:19)

RL
0

F 

RS
0

(cid:13)
(cid:13)

(cid:13)
(cid:13)
RL
0



(cid:13)
(cid:13)
RL
0

A
(cid:10)
*A

=

(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:0)
2γ

−
2γ

k

2
Xj
≥

RL
0

+

A

RjkF 
(cid:0)

+ (1 + ∆)
k

(cid:13)
(cid:13)

(cid:1)(cid:13)
(cid:13)
RL
0 kF εb

RS
0

F

1

(cid:18)

≤

−

γ2 + ∆
F +
(cid:19)
(cid:13)
(cid:13)
where the inequality in the ﬁrst line comes from RL
LSm,n(4r, 2s, µ) satisfying the RICs, the second
0 ∈
line is a consequence of feasibility in (67), the third line comes from Lemma Appendix B.7 and by
sums of individual pairs in the inner product being in LSm,n(4r, 2s, µ) by Lemma Appendix B.1, and
the last inequality follows from the optimality condition in (66). After dividing both sides of (72) by
(1

R1kF
k

(72)

(cid:0)(cid:13)
(cid:13)

∆)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:1)

−

RL
0

(cid:13)
(cid:13)

F gives
(cid:13)
(cid:13)

RL
0

(cid:13)
(cid:13)

(cid:13)
(cid:13)

1

2γ

F ≤

1

∆

1

(cid:18)

−

−

γ2 + ∆
(cid:19)

RL
0

F + 2
(cid:13)
(cid:13)
13

(cid:0)(cid:13)
(cid:13)

RS
0

(cid:13)
(cid:13)

F +
(cid:13)
(cid:13)

R1kF
k

+ εb

(cid:1)

1 + ∆
∆
1

−

.

(73)

Mutatis mutandis, the same argument applies to

RS
0

F

and similarly to

(cid:13)
(cid:13)

RS
0

F ≤

1

−
(cid:13)
(cid:13)
R1kF as
k

1

2γ

∆

1

(cid:18)

−

γ2 + ∆
(cid:19)

2

RL
0

(cid:0)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

RS
0

F +
(cid:13)
(cid:13)

R1kF

k

+ εb

(cid:1)

1 + ∆
∆
1

−

,

R1kF ≤

k

1

1

2γ

∆

1

(cid:18)

−

−

γ2 + ∆
(cid:19)

Adding (73), (75), and (75) together gives

2

RS
0

(cid:0)

(cid:13)
(cid:13)

F + 2
(cid:13)
(cid:13)

RL
0

F

+ εb

(cid:13)
(cid:13)

(cid:1)

(cid:13)
(cid:13)

1 + ∆
∆
1

−

.

(74)

(75)

RL
0

F +
(cid:13)
(cid:13)

RS
0

F +

R1kF ≤

k

1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)
7 −
(76) being upper bounded as

For ∆ < 1

2γ the prefactor

1

2γ

∆

1

(cid:18)

−

−

γ2 + ∆
(cid:19)

1

∆

1

−

∆ + 2γ
γ2
−
(cid:16)

1

(cid:17)

5

RL
0

F + 5
(cid:13)
(cid:13)

RS
0

F + 2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:0)
(cid:13)
(cid:13)
< 1
6 and therefore also ∆

1

R1kF
k

+ 3 εb

1 + ∆
∆
1
(76)
(cid:1)
∆ < 1
6 , resulting into

−

,

−

RL
0

RS
0

F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

R1kF ≤
k

5
6

F +
(cid:13)
(cid:13)

(cid:0)(cid:13)
(cid:13)

(cid:13)
(cid:13)

which after rearranging yields

Applying the triangle inequality on R = RL

RL
0

F +
(cid:13)
(cid:13)

RS
0

F +

(cid:13)
(cid:13)

(cid:13)
0 + RS
(cid:13)

(cid:13)
(cid:13)

RS
0

F +

R1kF

k

+

7
2

εb,

(cid:1)

(cid:13)
(cid:13)

21 εb .

RL
0

F +
(cid:13)
(cid:13)
(cid:13)
(cid:13)
R1kF ≤
0 + R1 +

k

(77)

(78)

and using the bounds in (66)

2 Rj

≥

(cid:17)

and (78) concludes the proof

R
k

kF ≤

j
(cid:16)P
R1kF +

Rjk
k

42 εb .

2
Xj
≥
R1kF ≤
k

RS
0

F +

k

RL
0

F +
(cid:13)
(cid:13)
RL
0

(cid:13)
(cid:13)
2

≤

(cid:13)
(cid:13)

(cid:13)
(cid:13)
F + 2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
RS
0

(cid:13)
(cid:13)

F +
(cid:13)
(cid:13)

3.2. Recovery of X0 ∈

LSm,n(r, s, µ) by Alg. 1 and Alg. 2.

This section presents the proofs of Theorem 4 and 5, that NIHT and NAHT respectively recover
) are suﬃciently

(X0) and knowledge of (r, s, µ) provided the RICs of

LSm,n(r, s, µ) from

X0 ∈
bounded.

A

(
·
A

The proof of NIHT follows the same line of thought as the one for low-rank matrix completion
[30], with the only diﬀerence of the hard thresholding projection, in the form of RPCA, being an
imprecise projection with accuracy εp as stated in (12). The proof consists of deriving an inequality
where
X0kF , and then showing that this
multiplicative factor is strictly less then one if
A
Proof of Theorem 4 (Guaranteed recovery by NIHT, Alg. 1).

X0kF is bounded by a factor multiplying

−
satisﬁes RIC with ∆3 := ∆r,s,µ(

) < 1/5.

X j+1

X j

A

−

k

k

(X0) be the vector of measurements of the matrix X0 ∈
A
(X j)

LSm,n(r, s, µ) and W j =
Proof. Let b =
X j
to be the update of X j before the oblique Robust PCA projection step
αj A
X j+1 = RPCAr,s,µ(W j, εp). By X j+1 being within an εp distance in the Frobenius norm of the
optimal RPCA projection X j+1

rpca := RPCAr,s,µ(W j , 0) deﬁned in (12)

A
(cid:0)

−

−

(cid:1)

b

∗

W j

−

X j+1

(cid:13)
(cid:13)

2
F =
(cid:13)
(cid:13)

≤

≤

W j

−
W j

(cid:13)
(cid:13)
(cid:16)(cid:13)
W j
(cid:13)

(cid:0)(cid:13)
(cid:13)

X j+1

rpca + X j+1

X j+1

2

F

rpca −
+

X j+1

X j+1
rpca

F

−

−

2
(cid:13)
(cid:13)

,

(cid:1)

X0

(cid:13)
F + εp
(cid:13)
(cid:13)
(cid:13)
14

−

2

(cid:13)
X j+1
(cid:13)
rpca

F

(cid:17)

(cid:13)
(cid:13)

(79)

(80)

(81)

(cid:10)
X j

(cid:13)
(cid:13)
αj A
A
(cid:0)
(cid:0)
X0, X j+1

∗

b

(cid:1)

−
(cid:1)
X0

−

2

W j

−
X j
h
X j

= 2
(cid:10)
= 2

−

−
X0, X j+1
−
X0, X j+1

where in the second line we used the triangle inequality, and the third line comes from X j+1
rpca being
the optimal projection thus being the closest matrix in LSm,n(r, s, µ) to W j in the Frobenius norm
and by X j+1 being within εp distance of X j+1

rpca. By expansion of the left hand side of (79)

W j

−

X j+1

(cid:13)
(cid:13)

W j

W j
(cid:13)
(cid:13)
W j

−

−

2
F =
(cid:13)
=
(cid:13)
=

(cid:13)
(cid:13)
(cid:0)(cid:13)
(cid:13)

−
X0

X0 + X0 −
2
X0 −
F +
(cid:13)
(cid:13)
F + εp
X0
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:1)

2

X j+1

2
F
2
X j+1
(cid:13)
F + 2
(cid:13)
W j
(cid:13)
X0
(cid:13)

−

≤

(cid:13)
(cid:13)

(82)

(83)

(84)

2

W j
−
2
F + 2 εp
(cid:10)
(cid:13)
(cid:13)

X0, X0 −
W j

−

(cid:13)
(cid:13)
k

W j

−

X j+1

X0

F + εp
(cid:11)
(cid:13)
X0k
(cid:13)

2
F from both sides

where the last line (84) follows from the inequality in (81). Subtracting
of (84) gives

X j+1

X0

2

W j

X0, X j+1

X0

+ 2 εp

W j

X0

F + εp

2 .

−
The matrix W j in the inner product on the right hand side of (85) can be expressed using the

−

−

−

(cid:11)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(85)

2
F ≤

update rule W j = X j

2 αj

∗
A
2 αj
(cid:10)

X0i −
(cid:11)
X0
−
−
X j
X0

A
X j
(cid:0)
A
X j+1
(cid:0)
(cid:10)

X j

X0

−
X0

, X j+1
−
X j+1

X0

2

,
(cid:1)
A
F ,
(cid:1)
(cid:13)
(cid:13)
(cid:13)
(X0) is the vector of measurements5 and linearity of
where in the ﬁrst line we use that b =
(cid:13)
(cid:13)
(cid:13)
A
second line we split the inner product into two inner products by linearity of
the third line is a consequence of Lemma Appendix B.8.

−
αj A∗QAQ

X0
(cid:11)

X0

(cid:11)
−

(cid:1)(cid:11)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

−

≤

−

(cid:10)

(cid:0)

I

F

2

, in the
, and the inequality in

A

(86)

(87)

(88)

The matrix W j can be expressed using the update rule W j = X j

second term of the right hand side of (85) and upper bounded by Lemma Appendix B.8
(cid:1)

(cid:0)

A
αj A

∗

−

X j

A
(cid:0)

By Lemma Appendix B.8, the eigenvalues of

I

W j

X0

−

(cid:13)
(cid:13)

F =
(cid:13)
(cid:13)

≤

X j

−

I
(cid:13)
(cid:13)
(cid:13)
(cid:13)

X0 −
−
αj A∗QAQ

αj A

∗

A
X j
(cid:0)

2
(cid:13)
(cid:13)
αj A∗QAQ
(cid:13)
(cid:13)

−
αj A∗QAQ

X j

−
X0

(cid:0)
−

X0
F .
(cid:13)
(cid:13)

2

(cid:1)(cid:1)(cid:13)
(cid:13)

are bounded by

αj (1 + ∆3)

1

−

λ

I

≤

(cid:0)
−

(cid:1)
1
≤

−

αj (1

∆3) ,

−

where ∆3 := ∆3r,3s,µ.

(cid:0)

(cid:1)

Consider the stepsize computed in Algorithm 1, Line 3 inspired by the previous work on NIHT in

the context of compressed sensing [29] and low-rank matrix sensing [30]

b

in the

−

(cid:1)

(89)

(90)

(91)

(92)

2

Proj(U j ,Ωj )

Rj

2

(cid:0)

F
(cid:1)(cid:13)
(cid:13)
Proj(U j ,Ωj ) (Rj)
(cid:13)
2
(cid:17)(cid:13)
(cid:13)
(cid:13)

αj = (cid:13)
(cid:13)
(cid:13)
A

(cid:16)

as

(cid:13)
(cid:13)
(cid:13)

A
1

where the projection Proj(U j ,Ωj )
Then we can bound αj using the RIC of

(cid:0)

(cid:1)

Rj

ensures that the residual Rj is projected onto the set LSm,n(r, s, µ).

1 + ∆1 ≤

αj ≤

1

1
∆1

−

,

(93)

5Here it would be possible to extend the result to be stable under measurement error εb as done in Theorem 3 by

adding an error term in (86).

15

where ∆1 := ∆r,s,µ. Combining (91) with (93) gives

1

−

1 + ∆3
1

∆1 ≤

−

λ

I

−

αj A∗QAQ

1

−

≤

1
∆3
−
1 + ∆1

.

(94)

∆1, the magnitude of the lower bound in (94) is greater than the upper bound. Therefore

(cid:0)

(cid:1)

Since ∆3 ≥

where the constant η is strictly smaller than one if ∆3 < 1/5.

(cid:13)
(cid:13)

η := 2

(cid:18)

1 + ∆3
1

∆1 −

−

1

≥

(cid:19)

2

I

−

αj A∗QAQ

,

2
(cid:13)
(cid:13)

(95)

Finally, the error in (85) can be upper bounded by (88) combined with (90) with η being the upper

bound on the operator norm in (95)

X j+1

X0

−

2
F ≤

η

X j

X0

F

−

X j+1

X0

−

F + η εp
(cid:13)
(cid:13)

X j

X0

−

F + εp
(cid:13)
(cid:13)

2 .

(96)

(cid:13)
(cid:13)

It remains to show the inequality (96) implies the update rule contracts the error and the iterates
X j converge to a matrix within the precision εp of the RPCA. For the ease of notation we rewrite
(96) using the notation ej :=

X0kF and arrange the inequality into a squared form

X j

−

k

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

ej+1

η ej ej+1 + η εp ej + εp

2

ej+1

1
(cid:0)
2

−

(cid:1)

ηej

ηej + εp

2

.

1
2

2

2

≤

≤

(cid:18)
Since the right hand side of (97) is positive, we have ej+1
an upper bound on the convergence rate

(cid:18)

(cid:19)

(cid:19)
ηej + εp, which by 1/5 > ∆3 ≥

≤

(97)

∆1 gives

X j+1

X0

−

F ≤

1

(cid:13)
(cid:13)

(cid:13)
(cid:13)

4∆3

∆3

−

X j

X0

−

(cid:13)
(cid:13)

F + εp .
(cid:13)
(cid:13)

Proof of Theorem 5 (Guaranteed recovery of NAHT, Alg. 2).

(X0) be the vector of measurements6 of the matrix X0 ∈
Proof. Let b =
A
Lj
αL
(X j)
−
a consequence of Lj+1 being the closest rank r matrix to V j in the Frobenius norm we have that

LSm,n(r, s, µ) and V j =
to be the update of Lj before the rank r projection Lj+1 = HT(V j; r, µ). As

∗
j A

−

b

A
(cid:0)

(cid:1)
L0

V j

−

Subtracting

(cid:13)
(cid:13)
V j

(cid:13)
(cid:13)

−
L0 −
(cid:13)
(cid:13)

L0

(cid:13)
Lj+1
(cid:13)

V j

−

2
F ≥
=

2
Lj+1
F =
2
(cid:13)
F +
(cid:13)
(cid:13)
(cid:13)

V j

−
Lj+1

L0 + L0 −
2
F + 2
(cid:13)
(cid:13)

L0

(cid:13)
(cid:13)
(cid:13)
L0, L0 −
L0 −
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
F from both sides of (98) and rearranging terms gives
(cid:13)

V j
(cid:13)
(cid:13)
(cid:13)
(cid:13)

V j

−

−

(cid:10)

Lj+1

.

(cid:11)

Lj+1

2
F

2
F ≤
=2
(cid:13)
(cid:13)
=2

V j

2
Lj
(cid:10)
−
Lj
(cid:10)
Lj
(cid:10)

=2

L0, Lj+1

L0

−
X j

−

−
αL
∗
j A
A
αL
L0 −
(cid:0)
(cid:0)
∗
j A
L0, Lj+1
2 αL
j
αL
−
+ 2 αL

A
j A∗QAQ
(cid:0)
(cid:10)
Sj
j ρ2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−
Sj

−

−

−

X0
(cid:11)
Lj

A
L0
(cid:0)

(cid:0)
−
S0
(cid:11)

,

L0
(cid:1)
−
S0

F

−
Lj

−

(cid:10)

2

I

≤

(cid:13)
(cid:13)

L0, Lj+1
−
S0

L0
, Lj+1
(cid:11)

−
Lj+1

L0

L0
(cid:11)

−

L0

,
(cid:1)(cid:1)

A

(cid:1)

(cid:0)

(cid:1)(cid:11)

(cid:1)(cid:1)
−
2 αL
j

−
L0 + Sj
Lj

A
Lj+1
(cid:10)
(cid:0)
−
Lj+1

−

−
L0

A

(cid:0)
F
Lj+1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−
L0

−

F

L0
(cid:1)(cid:11)
F ,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(98)

(99)

(100)

(101)

(102)

(103)

(cid:13)
(cid:13)
6Again, it is possible to extend the result to the case when there is a measurement error εb as done in Theorem 3

(cid:13)
(cid:13)

by having b = A(X0) + e, with kek2 ≤ εb.

16

A

where in the second line we expanded V j using the update rule V j = Lj
and
(X0), in the third line we expanded X j = Lj + Sj, in the fourth line we split the inner product
b =
into two inner products by linearity of
, and in the last line the inequality comes from Lemma
Appendix B.8 bounding the ﬁrst two terms and Lemma Appendix B.7 bounding the third term
with ρ2 :=
(cid:17)
LSm,n(2r, 2s, µ). Dividing both sides of (103) by

∆2 + 2γ2
γ2
2
−

L0 + Sj

A
(cid:0)

(X j)

Lj+1

j A

αL

S0

A

−

−

−

−

∈

(cid:16)

(cid:1)

b

1

(cid:1)

where ∆2 := ∆2r,2s,µ and γ2 := µ 2r√2s
√mn since
F gives
(cid:13)
F + 2 αL
(cid:13)
(cid:13)
(cid:13)

L0 −
(cid:13)
Lj
(cid:13)

F ≤
(cid:13)
(X j+ 1
(cid:13)
2 )

j A∗QAQ

Lj+1

j ρ2

αL

L0

−

−

2

I

b

(cid:0)

(cid:13)
(cid:13)
be the subsequent update of Sj before the s-sparse projection
(cid:13)
(cid:13)
Sj+1 = HTs(W j). By Sj+1 being the closest s sparse matrix to W j in the Frobenius norm and by
S0k0 ≤

(cid:16)
s, it follows that

∗
j A

(cid:13)
(cid:13)
A

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

−

(cid:17)

k

Lj+1

L0 −
(cid:13)
αS
Let W j = Sj
(cid:13)

Sj

S0

F .

−

(104)

Subtracting

(cid:13)
(cid:13)

W j

−
S0 −

(cid:13)
(cid:13)

(cid:13)
(cid:13)

W j

S0

−

2
F ≥
=

W j

−

2
Sj+1
F =
2
(cid:13)
F +
(cid:13)

S0

W j

−
Sj+1

−

W j
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
S0, S0 −
(cid:13)
(cid:13)
2
F from both sides in (105) and rearranging terms gives
(cid:13)
Sj+1
(cid:13)

(cid:13)
S0 −
(cid:13)
(cid:13)
(cid:13)

S0, Sj+1

W j

W j

S0

(cid:13)
(cid:13)

−

2

S0 + S0 −
2
F + 2
(cid:13)
(cid:13)

(cid:10)

Sj+1

2
F

2
F ≤
(cid:13)
=2
(cid:13)
=2

=2

2
≤

−

−

A

−
αS

S0
−
X j+ 1
∗
j A
(cid:16)
(cid:16)
αS
S0 −
∗
j A
S0, Sj+1
2 αS
j
αS
−
+ 2 αS

A
S0
(cid:0)
−
Lj+1
Sj
j A∗QAQ
(cid:0)
(cid:10)
Lj+1
j ρ2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:11)
−

A

−

−

−

−

Sj
(cid:10)

D
Sj
Sj
(cid:10)

(cid:10)

I

(cid:13)
(cid:13)

2

X0

(cid:11)
−
Lj+1
−
2 αS
j
,

(cid:0)
−
L0

S0
(cid:1)

F

L0

(cid:13)
F
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)

S0, Sj+1

S0

−
E
, Sj+1
Sj+1

S0

−
S0

S0
(cid:1)

,

(cid:1)(cid:1)
A

(cid:0)

−

−
(cid:17)(cid:17)
L0 + Sj
Sj
A
Sj+1
(cid:10)
(cid:0)
A
Sj+1
(cid:0)
Sj+1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

−

−

−
S0

S0

(cid:1)(cid:11)
F
F ,
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Sj+1

.

(105)

(cid:11)

S0

S0

(cid:11)

−

−

(cid:1)(cid:11)

(106)

(107)

(108)

(109)

(110)

where in the second line we express W j using the update rule W j = Sj

A

−
(X0), in the third line we expanded X j+ 1
2 = Lj+1 + Sj, in the fourth line we split the inner
b =
, and the inequality in the last line comes from
product into two inner products by linearity of
A
Lemma Appendix B.8 bounding the ﬁrst two terms and Lemma Appendix B.7 bounding the third
∆2 + 2γ2
term with ρ2 :=
γ2
1
2
−
LSm,n(2r, 2s, µ). Dividing both sides of (110) by

L0 + Sj+1

Lj+1

S0

A

−

−

−

(cid:16)

(cid:17)

(cid:16)

(cid:17)

∈

αS

j A

b

and

(X j+ 1
2 )

where ∆2 := ∆2r,2s,µ and γ2 := γ2r,2s,µ since
S0 −
(cid:13)
Sj
(cid:13)

j AT
αS

QAQ

Sj+1

j ρ2

S0

2

I

(cid:0)

Lj+1

(cid:1)

F gives
(cid:13)
F + 2 αS
(cid:13)
(cid:13)
(cid:13)

−

S0 −
(cid:13)
Adding together (104) and (111)
(cid:13)

F ≤

Sj+1

−

(cid:13)
(cid:13)
Lj+1

L0 −
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

F ≤

F +
2
(cid:13)
(cid:13)
+2

(cid:13)
(cid:13)
Sj+1
S0 −
αL
j AT
I
(cid:13)
−
(cid:13)
j AT
αS
I
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2 αS

−

j ρ2
j ρ2

−

S0

L0

QAQ
(cid:13)
(cid:13)
QAQ

F + 2 αL
Lj
F + 2 αS
Sj
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
which after rearranging terms in (112) becomes
(cid:13)
(cid:13)
(cid:13)
Lj+1
F +
S0 −
j AT
αL
Lj
QAQ
(cid:13)
(cid:13)
−
(cid:13)
(cid:13)
+ αL
j AT
αS
j ρ2
QAQ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

L0 −
I
2
(cid:1) (cid:13)
−
≤
(cid:13)
+2
I

(cid:13)
(cid:13)
(cid:0)(cid:13)
(cid:13)

Sj+1

j ρ2

L0

−

−

−

1

(cid:0)

F

(cid:13)
F
(cid:13)
Sj
(cid:13)
(cid:13)
(cid:1) (cid:13)
(cid:13)

17

L0

−

F .
(cid:13)
(cid:13)

S0

−

F
L0
(cid:13)
(cid:13)

F ,
(cid:13)
(cid:13)

(cid:13)
(cid:13)

Sj
−
Lj+1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

S0

F

−

(cid:13)
(cid:13)

(111)

(112)

(113)

j , αL

and because αS
j , ∆2 ≥
increase the left hand side while adding 2 αL
hand side of (113), therefore

0 and γ2 ∈

(0, 1), subtracting 2 αS
Lj

j ρ2

1

−

(cid:0)

2 αS

j ρ2

L0 −
2
I
(cid:1) (cid:0)(cid:13)
−
(cid:13)
(cid:0)(cid:13)
= max
(cid:13)

≤

Lj+1
αj AT

F +
QAQ
(cid:13)
(cid:13)
(cid:13)
j AT
αL
QAQ
(cid:13)

I

−

αj AT

QAQ
simpliﬁes to

where
1

−

I
−
2 αS
j ρ2
(cid:13)
(cid:13)

(cid:0)

(cid:1)

(cid:13)
(cid:13)
L0 −
(cid:13)
(cid:13)

Lj+1

(cid:8)(cid:13)
(cid:13)
F +
I
(cid:13)
(cid:13)
≤

Sj+1

F

S0 −
αj AT
(cid:13)
−
(cid:13)
1

QAQ
(cid:13)
(cid:13)
2 αS
j ρ2
(cid:13)
(cid:13)
By Lemma Appendix B.8, the eigenvalues of

(cid:13)
(cid:13)

−

2

I

(cid:13)
(cid:13)
S0 −
+ αL
j ρ2
(cid:13)
(cid:13)

−

,

I

(cid:13)
(cid:13)

(cid:13)
(cid:13)

+ αL

j ρ2

−

Sj+1

L0

S0 −

j ρ2k

kF on the left does not
F on the right does not decrease the right
(cid:13)
(cid:13)
Sj+1

F
Lj
(cid:13)
(cid:13)
(cid:1) (cid:0)(cid:13)
j AT
αS
(cid:13)

L0

(cid:1)
−

QAQ

F +
(cid:13)
(cid:13)

Sj

−

S0

F

,

(114)

(cid:1)
. Dividing both sides of (114) by

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:9)

(cid:13)
(cid:13)

Lj

L0

−

Sj

−

S0

F

.

(cid:0)(cid:13)
(cid:13)
αj AT
QAQ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
can be bounded as

(cid:1)

F +
(cid:13)
(cid:13)

αj (1 + ∆3)

1

−

(cid:1)
1
≤

−

αj (1

∆3) ,

−

−
αj AT

I

λ

(cid:0)
−
≤
(cid:0)
j and αS
. By αL

QAQ

(cid:1)

j being the normalized stepsizes as introduced

with ∆3 := ∆3r,3s,µ being the RIC of
in [29, 30]

A

Proj(U j ,Ωj )

Rj

F
(cid:1)(cid:13)
(cid:13)
Proj(U j ,Ωj ) (Rj)
(cid:13)
(cid:16)

(cid:0)

2

2

2
(cid:17)(cid:13)
(cid:13)
(cid:13)

αL
j = (cid:13)
(cid:13)
(cid:13)
A

(cid:13)
(cid:13)
(cid:13)

and αS

2

Proj(U j+1,Ωj )

Rj

(cid:0)

F
(cid:1)(cid:13)
(cid:13)
Proj(U j+1,Ωj ) (Rj)
(cid:13)
2
(cid:17)(cid:13)
(cid:16)
(cid:13)
(cid:13)

2

j = (cid:13)
(cid:13)
(cid:13)
A

(cid:13)
(cid:13)
(cid:13)

where the projection Proj(U j ,Ωj )
is projected into the set LSm,n(r, s, µ). Then, it follows from the RIC for
(cid:1)
can be bounded as

, Proj(U j+1,Ωj )

(cid:17)

(cid:0)

2

Rj+ 1
(cid:16)

ensures that the residual Rj and Rj+ 1
j , αS
j

that the stepsizes αL

2

Rj

A

αL/S
j ≤
where ∆1 := ∆r,s,µ. Putting (116) and (118) together

1 + ∆1 ≤

1

1
∆1

,

1

−

1

−

1 + ∆3
1

∆1 ≤

−

αL/S
j AT

QAQ

λ

I

(cid:16)

−

1

−

≤

1
∆3
−
1 + ∆1

.

(cid:17)

Since ∆3 ≥
bound. Therefore

∆1 we have that the magnitude of the lower bound in (119) is greater than the upper

Finally, the constant on the right hand side of (115) can be upper bounded

1 + ∆3
1

∆1 −

1

≥

−

I

αL/S
j AT

QAQ

−

(cid:13)
(cid:13)
(cid:13)

.

2
(cid:13)
(cid:13)
(cid:13)

η := 2

I

−

j AT
αS
1

QAQ
2 αS

+ αL

j ρ2

j ρ2
(cid:13)
(cid:13)
∆2 + 2γ2
+ 1
γ2
1
∆1
2
−
−
(cid:16)
∆2 + 2γ2
γ2
2
−

∆1

1

1

(cid:16)

(cid:17)

(cid:13)
(cid:13)
1+∆3
1

−
1

2

≤

≤

1

(cid:16)

−

∆1 −
1

(cid:17)
2 1
1
−
−
6∆3 + 4γ2
γ2
2
−
4γ2
3∆3 −
γ2
1
2

1

−

−

18

= 2

(cid:17)

∆3 + ∆1 + ∆2 + 2γ2
γ2
1
2
−
4γ2
γ2
1
2

2∆2 −

∆1 −

−

1

−

(115)

(116)

(117)

(118)

(119)

(120)

(121)

(122)

(123)

where the inequality in the second line in (122) comes from upper bounds in (120) and in (118), and
the third line in (123) follows from ∆2 ≥
which translates to

To ensure that η < 1, it suﬃces to show that the right-hand side in (123) is smaller than one,

∆1.

∆3 ≤

1
9

8

1

−

1
(cid:18)

γ2

γ2
2 (cid:19)

γ2. For ∆3r,3s,µ < 1

9 −

,

(124)

−
γ2 the inequality in (115) implies contraction

which is satisﬁed when ∆3 ≤
of the error
Lj+1

1
9 −

F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

F ≤
because η < 1, which guarantees linear convergence of iterates Lj and Sj to L0 and S0 respectively.

−

−

(cid:1)

F

(cid:13)
(cid:13)

(cid:0)(cid:13)
(cid:13)

F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

η

Lj

L0

Sj

S0

,

(125)

L0 −
(cid:13)
(cid:13)

Sj+1

S0 −

4. Numerical experiments

A

LSm,n(r, s, µ) from its undersampled values

This section demonstrates the computational eﬃcacy of recoverying a low-rank plus sparse matrix
(X0). Section 4.1 considers synthetic examples
X0 ∈
LSm,n(r, s, µ) are created, and recovery from their undersampled values at-
where matrices in X0 ∈
tempted for the following algorithms: NIHT (Alg. 1), NAHT (Alg. 2), SpaRCS [34], and the convex
relaxation (8). Figure 1 presents empirically observed phase transitions, which indicate the values of
model complexity r, s, and measurements p for which recovery is possible. Figure 2 and 3 gives ex-
amples of convergence rates for NIHT, NAHT, and SpaRCS, including contrasting diﬀerent methods
to implement the projection NIHT, step 5 of Alg. 1. Section 4.2 presents applications to dynamic-
foreground/static-background and computational multispectal imaging. An additional phase transi-
tion simulation for the convex relaxation is given in Appendix Appendix A. Software to reproduce
the experiments in this section is publicly available7.

4.1. Empirical average case performance on synthetic data

×

×

∈

∈

Rn

Rm

r, V

Synthetic matrices X0 = L0 + S0 ∈

LSm,n(r, s, µ) are generated using the experimental setup
proposed in the Robust PCA literature [28, 38, 33]. The low-rank component is formed as L0 = U V T ,
r are two random matrices having their entries drawn i.i.d. from the
where U
standard Gaussian distribution. The support set of the sparse component S0 is generated by sampling
[n] indices of size s and each non-zero entry (S0)i,j is drawn from
a uniformly random subset of [m]
)]. Each synthetic matrix is measured using
(L0)i,j|
the uniform distribution over [
−
Rp. The random Gaussian measurement operators are constructed by p
linear operators
matrices A(ℓ)
(0, 1/p) where
p is the number of measurements. The Fast Johnson-Lindenstrauss Transform is implemented as

(L0)i,j|
n whose entries are sampled from Gaussian distribution A(ℓ)

) , E (
|

i,j ∼ N

: Rm

A
Rm

×
E (

→

∈

×

×

n

|

Rp

AFJLT (X) = RHD vec (X) ,
mn is a restriction matrix constructed from a mn

where R
∈
randomly selected, H
matrix whose entries are sampled independently randomly from
vectorized matrix X

∈
Rm

Rmn

n.

×

×

×

mn is discrete cosine transform matrix, D

mn identity matrix with p rows
mn is a diagonal
Rmn is the

Rmn
, and vec (X)
}

1, 1

∈

∈

×

×

{−

(126)

Theorems 1, 3, 4, and 5 indicate that recovery of X0 from

(X0) depends on the problem dimen-
sions through the ratios of the number of measurements p with the ambient dimension mn, and the
r)+ s, through an undersampling and two oversampling
minimum number of measurements, r(m+ n
ratios

A

−

∈

δ =

p
mn

and ρr =

−

,

ρs =

.

(127)

r(m + n
p

r)

s
p

7https://github.com/SimonVary/lrps-recovery

19

}

∈ {

0.02, 0.04, . . . , 1

0, 0.02, 0.04, . . . , 1

The matrix dimensions m and n are held ﬁxed, while p, r and s are chosen according to varying
1, with the
parameters δ, ρr and ρs. For each pair of ρr, ρs ∈ {
where ρr + ρs ≤
sampling ratio restricted to values δ
, 20 simulated recovery tests are conducted
}
and we compute the critical subsampling ratio δ∗ above which more than half of the experiments
drawn from the (dense) Gaussian distribution, the highest per
succeeded. For the linear transform
iteration cost in NIHT and NAHT comes from applying
to the residual matrix, which requires pmn
scalar multiplications which scales proportionally to (mn)2. For this reason, our tests are restricted to
the matrix size of m = n = 100 in the case of NIHT and NAHT, and to a smaller size m = n = 30 for
testing the recovery by solving the convex relaxation (8) with semideﬁnite programming [39] that has
variables which is more computationally demanding8 compared to the hard thresholding
O
gradient descent methods. Algorithms are terminated at iteration ℓ when either: the relative residual
k2, or the relative decrease in
error is smaller than 10−
the objective is small

6, that is when

(mn)2

k2 ≤

k2/

(X ℓ)

10−

k A

A

A

−

k

k

(cid:1)

(cid:0)

b

b

b

6

(X ℓ)

k A

(X ℓ

−

−
15)

b

k2
b

−

k2 (cid:19)

(cid:18)

k A

1/15

> 0.999,

(128)

2

k

k

∈

−

X ℓ

10−

X0kF ≤

LSm,n(r, s, µ) that is within 10−

or the maximum of 300 iterations is reached. An algorithm is considered to have successfully recovered
2 of X0 in the relative
X0 ∈
Frobenius error,

LSm,n(r, s, µ) if it returns a matrix X ℓ
X0kF .

Figure 1 depicts the phase transitions of δ above which NIHT and NAHT successfully recovers
X0 in more than half of the experiments. For example, the level curve 0.4 in Fig. 1 denotes the
values of ρr and ρs below which recovery is possible for at least half of the experiments for p = 0.4mn
and ρr, ρs as given by (127). Note that the bottom left portion of Fig. 1 corresponds to smaller
values of model complexity (r, s) and are correspondingly easier to recover than larger values of (r, s).
0.6, even from
Both algorithms are observed to recover matrices with prevalent rank structure, ρr ≤
very few measurements as opposed to matrices with prevalent sparse structure requiring in general
more measurements for a successful recovery. Phase transitions corresponding to the sparse-only
(ρr = 0) and to the rank-only (ρs = 0) cases are roughly in agreement with phase transitions that
have been observed for non-convex algorithms in compressed sensing [40] and matrix completion
literature [30, 41]. We observe that NAHT achieves almost identical performance to NIHT in terms
of possible recovery despite not requiring the computationally expensive Robust PCA projection in
every iteration. For both algorithms we see that the successful recovery is possible for matrices with
higher ranks and sparsities in the case of FJLT measurements compared to Gaussian measurements.
Equivalent experiments are conducted for the convex relaxation (8), but with smaller matrix size
30
30 and limited to 10 simulations for each set of parameters due to the added computational
demands. The convex optimization is formulated using CVX modeling framework [42] and solved in
Matlab by the semideﬁnite programming optimization package SDPT3 [39]. We observe that recovery
by solving the convex relaxation is successful for somewhat lower ranks and sparsities and requiring
larger sampling ratio δ compared to the non-convex algorithms. The observed phase transitions of the
convex relaxation alongside phase transitions for m = n = 30 experiments with NIHT and NAHT are
depicted in Figure A.6 in Appendix A. Comparing the phase transitions of the non-convex algorithms
in Fig. 1 and Fig. A.6 show that with the increased problem size, the phase transition are independent
of the dimension with only small diﬀerences due to ﬁnite dimensional eﬀects of the smaller problem
size in the case of m = n = 30.

×

Figure 2 presents convergence timings of Matlab implementations of the three non-convex algo-
rithms used for recovery of matrices with m = n = 100 from p = (1/2)102 (δ = 1/2) measurements
and three values of ρr = ρs =
. The convergence results are presented for two variants
}
of NIHT with diﬀerent Robust PCA algorithms Accelerated Alternating Projection (AccAltProj) [33]

0.05, 0.1, 0.2
{

8As an example, a low-rank plus sparse matrix with m = n = 100 with ρr = ρs = 0.1 undersampled and measured
with Gaussian matrix with δ = 0.5 takes 2.5 seconds and 2.3 seconds to recover using NIHT and NAHT respectively,
while the recovery using the convex relaxation takes over 7 hours.

20

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(a) NIHT (Gaussian measurements)
1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.2

0.4

0.6

0.8

1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(b) NIHT (FJLT measurements)

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(c) NAHT (Gaussian measurements)

(d) NAHT (FJLT measurements)

Figure 1: Phase transition level curves denoting the value of δ∗ for which values of ρr and ρs below
which are recovered for at least half of the experiments for δ, ρr, and ρs as given by (127). NIHT is
observed to recover matrices of higher ranks and sparsities from FJLT than from Gaussian measure-
ments, while the phase transitions for NIHT and NAHT are comparable. The Robust PCA projection
in NIHT, step 5 in Alg. 1, is performed by AccAltProj [33].

and Go Decomposition (GoDec) [43] in the projection step 5 of Alg. 1. Both NIHT and NAHT con-
verge at a much faster rate than the existing non-convex algorithm for low-rank plus sparse matrix
recovery SpaRCS [34]. All the algorithms take longer to recover a matrix for increased rank r and/or
sparsity s.

The computational eﬃcacy of NIHT compared to NAHT depends on the cost of computing the
Robust PCA calculation in comparison to the cost of applying
. NAHT computes two step sizes
twice per iteration in comparison to just one such
in each iteration which results into computing
computation per iteration in the case of NIHT. On the other hand, NIHT involves solving Robust PCA
in every iteration for the projection step whereas NAHT performs computationally cheaper singular
value decomposition (SVD) and sparse hard thresholding projection.

A

A

Figure 3 illustrates the convergence of the individual low-rank and sparse components

L0kF
S0kF as a function of time. The algorithms are observed to approximate the the low-rank

and

Sℓ

k

−

−

k

Lℓ

21

100

10-1

10-2

10-3

10-4

100

10-1

10-2

10-3

10-4

100

10-1

10-2

10-3

10-4

0

5

10

15

20

25

30

0

5

10

15

20

25

30

0

5

10

15

20

25

30

(a) ρr = ρs = 0.05

(b) ρr = ρs = 0.1

(c) ρr = ρs = 0.2

Figure 2: Relative error in the approximate X ℓ as a function of time for synthetic problems with
m = n = 100 and p = (1/2)1002, δ = 1/2, for Gaussian linear measurements
. In (b), SpaRCS
converged in 171 sec. (45 iterations), and in (c), SpaRCS did not converge.

A

100

10-2

10-4

0

1

2

3

4

5

6

7

8

100

10-2

10-4

0

5

10

15

20

(a) δ = 0.5, ρr = 0.05, ρs = 0.15

(b) δ = 0.5, ρr = 0.05, ρs = 0.25

Figure 3: Error between between the approximate recovered low-rank and sparse components Lℓ and
Sℓ and the true low-rank and sparse components L0 and S0. Error is plotted as a function of recovery
time for synthetic problems with m = n = 100 and p = (1/2)1002, δ = 1/2, for Gaussian linear
measurements

.

A

factor more accurately than the sparse component and that the computational time increases for
larger values of sparsity fraction ρs. Moreover, for both NIHT and NAHT the relative error of both
components decreases together.

4.2. Applications
4.2.1. Dynamic-foreground/static-background video separation

Background/foreground separation is the task of distinguishing moving objects from the static-
background in a time series, e.g. a video recording. A widely used approach is to arrange frames of the
video sequence into an m
n matrix, where m is the number of pixels and n is the number of frames
of the recording and apply Robust PCA to decompose the matrix into the sum of a low-rank and a
sparse component which model the static background and dynamic foreground respectively [3]. Herein
we consider the same problem but with the additional challenge of recovering the video sequence from
subsampled information [34] analogous to compressed sensing.

×

We apply NIHT, Alg. 1, to the well studied shopping mall surveillance [44] which is 256

150
video sequence. The video sequence is rearranged into a matrix of size 26 600
150 and measured using
subsampled FJLT (126) with one third as many meausrements as the ambient dimension, δ = 0.33.
The static-background is modeled with a rank-r matrix with r = 1 and the dynamic-foreground by an
s-sparse matrix with s = 197 505 (ρr = 0.02, ρs = 0.15). Figure 4 displays the reconstructed image
Xniht and its sparse component Sniht alongside the results obtained from applying Robust PCA
(AccAltProj [33]) which makes use of the fully sampled video sequence rather than the one-third

256

×

×

×

22

(a) Xrpca

(b) Xniht

(c) Srpca

(d) Sniht

256

150 video sequence compared to the approximation of
Figure 4: NIHT recovery results of a 256
the complete video sequence by Robust PCA (AccAltProj [33]). The video sequence is reshaped into
a 26 600
150 matrix and either recovered from FJLT measurements with δ = 0.33 using rank r = 1
and sparsity s = 197 505 or approximated from the full video sequence by computing Robust PCA
by AccAltProj with the same rank and sparsity parameters. Recovery by NIHT from subsampled
information achieves PSNR of 34.5 dB whereas the Robust PCA approximation from the full video
sequence achieves PSNR of 35.5 dB.

×

×

×

measurements available to NIHT. NIHT accurately estimates the video sequence achieving PSNR of
34.5 dB while also separating the low-rank background from the sparse foreground. The results are
of a similar visual quality to the case of Robust PCA that achieves PSNR of 35.5 dB which requires
access to the full video sequence.

4.2.2. Computational multispectral imaging

A multispectral image captures a wide range of light spectra generating a vector of spectral re-
sponses at each image pixel thus acquiring information in the form of a third order tensor. Low-rank
model has a vital role in multispectral imaging in the form of a linear spectral mixing models that
assume the spectral responses of the imaged scene are well approximated as a linear combination of
spectral responses of only few core materials referred to as endmembers [45]. As such, the low-rank
structure can be exploited by computational imaging systems which acquire the image in a compressed
from and use computational methods to recover a high-resolutional image [46, 47, 48]. However, when
diﬀerent materials are in close proximity the resulting spectrum can be highly nonlinear combination
of the endmembers resulting in anomalies of the model [49]. Herein we propose the low-rank plus
sparse matrix recovery as a way to model the spectral anomalies in the low-rank structure.

512

We employ NIHT on a 512

48 airborne hyperspectral image from the GRSS 2018 Data
×
Fusion contest [50] that is rearranged into a matrix of size 262 144
48 and subsampled using FJLT
with δ = 0.33. Figure 5 demonstrates recovery by NIHT using rank r = 3 and sparsity s = 150 995
(ρr = 0.25, ρs = 0.05) in comparison with the the low-rank model with rank r = 3 and s = 0
(ρr = 0.25, ρs = 0). Both methods recover the image well but the low-rank plus sparse recovery
achieves slightly higher PSNR of 39.1 dB compared to the low-rank recovery that has PSNR of

×

×

23

(a) Groundtruth Xtrue

(b) Low-rank plus sparse Xniht

(c) Low-rank Xmc

(d) PSNR (low-rank)

Xtrue

Xniht

Xmc

(e) PSNR (low-rank plus sparse)

(f) Detail 1 (694 nm)

(g) Detail 2 (694 nm)

Figure 5: Recovery by NIHT from FJLT measurements with δ = 0.33 using low-rank model (ρr =
0.25, ρs = 0) compared to the low-rank plus sparse model (ρr = 0.25, ρs = 0.05). Figure 5a - 5b
show the color renderings of the original multispectral image and the two recovered images. Figure 5d
and Figure 5e show the spatial PSNR of the recovery from the low-rank only model (overall PSNR of
38.9 dB) and the low-rank plus sparse model (overall PSNR of 39.1 dB) respectively. Figure 5f and
Figure 5g show two details of size 128

128 in the 694 nm band.

×

38.9 dB and slightly better ﬁne details. Figure 5d and Figure 5e depict the localization of the error in
terms of PSNR and shows that adding the sparse component improves PSNR of a few localized parts.
Although the overall gain in the PSNR is small compared to the low-rank model, the diﬀerences in
the localized regions of the image can be potentially impactful when further analyzed in practical
applications such as semantic segmentation [51].

5. Conclusion

The main theorems, Theorems 1, 2, 3, 4, and 5, are the natural extension of analogous results
in the compressed sensing and matrix completion literature to the space of low-rank plus sparse ma-
trices, Deﬁnition 1.1, see [4, 5] and references therein. They establish the foundational theory and
provide examples of algorithms for recovery of matrices that can be expressed as a sum of a low-rank

24

and a sparse matrix from under sampled measurements. While these results could be anticipated,
with [34] being an early non-convex algorithm for this setting, these advancements had not yet been
proven. We prove that the restricted isometry constants of random linear operators obeying concen-
tration of measure inequalities, such as Gaussian measurements or the Fast Johnson-Lindenstrauss
Transform, can be upper bounded when the number of measurements are of the order depending
on the degrees of freedom of the low-rank plus sparse matrix. Making use of these RICs, we show
that low-rank plus sparse matrices can be provably recovered by computationally eﬃcient methods,
e.g. by solving semideﬁnite programming or by two gradient descent algorithms, when the restricted
isometry constants of the measurement operator are suﬃciently bounded. These results also provably
solve Robust PCA with the asymptotically optimal number of corruptions and improve the previously
known guarantees by not requiring an assumption on the support of the sparse matrix. Numerical
experiments on synthetic data empirically demonstrate phase transitions in the parameter space for
which the recovery is possible. Experiments for dynamic-foreground/static-background video separa-
tion show that the segmentation of moving objects can be obtains with similar error from only one
third as many measurement as compared to the entire video sequence. The contributions here open
up the possibility of other algorithms in compressed sensing and low-rank matrix completion/sensing
to be extended to the case of low-rank plus sparse matrix recovery, e.g. more eﬃcient algorithms such
as those employing momentum [52, 53] or minimising over increasingly larger subspaces [41]. These
results also illustrate how RICs can be developed for more complex additive data models, provided it
is possible to control the correlation between them, and one can expect that similar results would be
possible for new data models.

Acknowledgement

We would like to thank Robert A. Lamb and David Humphreys for useful discussions around the

applications of low-rank plus sparse model to multispectral imaging.

References

[1] E. J. Cand`es, X. Li, Y. Ma, J. Wright, Robust principal component analysis?, Journal of the

ACM 58 (3) (2011) 1–37. doi:10.1145/1970392.1970395.

[2] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, A. S. Willsky, Rank-sparsity incoher-
ence for matrix decomposition, SIAM Journal on Optimization 21 (2) (2011) 572–596.
doi:10.1137/090761793.

[3] T. Bouwmans, A. Sobral, S. Javed, S. K. Jung, E.-H. Zahzah, Decomposition into low-
rank plus additive matrices for background/foreground separation: A review for a com-
parative evaluation with a large-scale dataset, Computer Science Review 23 (2017) 1–71.
doi:10.1016/j.cosrev.2016.11.001.

[4] Y. C. Eldar, G. Kutyniok, Compressed sensing: Theory and applications, Cambridge University

Press, 2012. doi:10.1017/CBO9780511794308.

[5] S. Foucart, H. Rauhut, A Mathematical

Introduction to Compressive Sensing, Ap-
plied and Numerical Harmonic Analysis, Springer New York, New York, NY, 2013.
doi:10.1007/978-0-8176-4948-7.

[6] D. Donoho, Compressed sensing, IEEE Transactions on Information Theory 52 (4) (2006) 1289–

1306. doi:10.1109/TIT.2006.871582.

[7] E. Candes, J. Romberg, T. Tao, Robust uncertainty principles: exact signal reconstruction from
highly incomplete frequency information, IEEE Transactions on Information Theory 52 (2) (2006)
489–509. arXiv:0409186, doi:10.1109/TIT.2005.862083.

25

[8] E. Candes, T. Tao, Decoding by Linear Programming, IEEE Transactions on Information Theory

51 (12) (2005) 4203–4215. doi:10.1109/TIT.2005.858979.

[9] E. J. Cand`es, B. Recht, Exact matrix completion via convex optimization, Foundations of Com-

putational Mathematics 9 (6) (2009) 717–772. doi:10.1007/s10208-009-9045-5.

[10] E. J. Candes, T. Tao, The power of convex relaxation: near-optimal matrix completion, IEEE
Transactions on Information Theory 56 (5) (2010) 2053–2080. doi:10.1109/TIT.2010.2044061.

[11] B. Recht, M. Fazel, P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix equations
via nuclear norm minimization, SIAM Review 52 (3) (2010) 471–501. doi:10.1137/070697835.

[12] E. J. Cand`es, C. Fernandez-Granda, Towards a mathematical theory of super-resolution, Com-
munications on Pure and Applied Mathematics 67 (6) (2014) 906–956. doi:10.1002/cpa.21455.

[13] V. Duval, G. Peyr´e, Exact support recovery for sparse spikes Deconvolution, Foundations of
Computational Mathematics 15 (5) (2015) 1315–1355. doi:10.1007/s10208-014-9228-6.

[14] A. Eftekhari, J. Tanner, A. Thompson, B. Toader, H. Tyagi, Sparse non-negative super-resolution
— simpliﬁed and stabilised, Applied and Computational Harmonic Analysis 1 (2019) 1–65.
doi:10.1016/j.acha.2019.08.004.

[15] Y. Chi, M. Ferreira Da Costa, Harnessing sparsity over the continuum:

atomic norm
minimization for superresolution, IEEE Signal Processing Magazine 37 (2) (2020) 39–57.
doi:10.1109/MSP.2019.2962209.

[16] S. Gu, L. Zhang, W. Zuo, X. Feng, Weighted nuclear norm minimization with application to
image denoising, in: 2014 IEEE Conference on Computer Vision and Pattern Recognition, no. 2,
IEEE, 2014, pp. 2862–2869. doi:10.1109/CVPR.2014.366.

[17] A. Gogna, A. Shukla, H. K. Agarwal, A. Majumdar, Split Bregman algorithms for sparse /
joint-sparse and low-rank signal recovery: application in compressive hyperspectral imaging, in:
2014 IEEE International Conference on Image Processing (ICIP), IEEE, 2014, pp. 1302–1306.
doi:10.1109/ICIP.2014.7025260.

[18] Y. Chen, Y. Guo, Y. Wang, D. Wang, C. Peng, G. He, Denoising of hyperspectral images using
nonconvex low rank matrix approximation, IEEE Transactions on Geoscience and Remote Sensing
55 (9) (2017) 5366–5380. doi:10.1109/TGRS.2017.2706326.

[19] W. Wei, L. Zhang, Y. Zhang, C. Wang, C. Tian, Hyperspectral image denoising from an in-
complete observation, in: 2015 International Conference on Orange Technologies (ICOT), IEEE,
2015, pp. 177–180. doi:10.1109/ICOT.2015.7498517.

[20] X. Luan, B. Fang, L. Liu, W. Yang, J. Qian, Extracting sparse error of robust PCA for face
recognition in the presence of varying illumination and occlusion, Pattern Recognition 47 (2)
(2014) 495–508. doi:10.1016/j.patcog.2013.06.031.

[21] J. Wright, A. Yang, A. Ganesh, S. Sastry, Yi Ma, Robust face recognition via sparse represen-
tation, IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (2) (2009) 210–227.
doi:10.1109/TPAMI.2008.79.

[22] F. Xu, J. Han, Y. Wang, M. Chen, Y. Chen, G. He, Y. Hu, Dynamic magnetic resonance
imaging via nonconvex low-rank matrix approximation, IEEE Access 5 (2017) 1958–1966.
doi:10.1109/ACCESS.2017.2657645.

[23] H. Gao, J.-F. Cai, Z. Shen, H. Zhao, Robust principal component analysis-based four-
dimensional computed tomography, Physics in Medicine and Biology 56 (11) (2011) 3181–3198.
doi:10.1088/0031-9155/56/11/002.

26

[24] O. Oreifej, X. Li, M. Shah, Simultaneous video stabilization and moving object detection in
turbulence, IEEE Transactions on Pattern Analysis and Machine Intelligence 35 (2) (2013) 450–
462. doi:10.1109/TPAMI.2012.97.

[25] R. Baraniuk, M. Davenport, R. DeVore, M. Wakin, A simple proof of the restricted isom-
etry property for random matrices, Constructive Approximation 28 (3) (2008) 253–263.
doi:10.1007/s00365-007-9003-x.

[26] J. Tanner, A. Thompson, S. Vary, Matrix Rigidity and the Ill-Posedness of Robust PCA and
Matrix Completion, SIAM Journal on Mathematics of Data Science 1 (3) (2019) 537–554.
doi:10.1137/18M1227846.

[27] D. Hsu, S. M. Kakade, T. Zhang, Sparse Corruptions, IEEE Transactions on Information Theory

57 (11) (2011) 7221–7234.

[28] P. Netrapalli, U. N. Niranjan, S. Sanghavi, A. Anandkumar, P. Jain, Non-convex robust PCA,

in: Advances in Neural Information Processing Systems 27 (NIPS 2014), 2014.

[29] T. Blumensath, M. Davies, Normalized iterative hard thresholding: guaranteed stability and
performance, IEEE Journal of Selected Topics in Signal Processing 4 (2) (2010) 298–309.
doi:10.1109/JSTSP.2010.2042411.

[30] J. Tanner, K. Wei, Normalized iterative hard thresholding for matrix completion, SIAM Journal

on Scientiﬁc Computing 35 (5) (2013) S104–S125. doi:10.1137/120876459.

[31] N. Ailon, B. Chazelle, The fast Johnson–Lindenstrauss transform and approximate nearest neigh-

bors, SIAM Journal on Computing 39 (1) (2009) 302–322. doi:10.1137/060673096.

[32] F. Krahmer, R. Ward, New and improved Johnson-Lindenstrauss embeddings via the re-
stricted isometry property, SIAM Journal on Mathematical Analysis 43 (3) (2011) 1269–1281.
doi:10.1137/100810447.

[33] H. Cai, J.-F. Cai, K. Wei, Accelerated alternating projections for robust principal component

analysis, Journal of Machine Learning Research 20 (1) (2019) 685—-717.

[34] A. E. Waters, A. C. Sankaranarayanan, R. G. Baraniuk, SpaRCS: recovering low-rank and sparse
matrices from compressive measurements, in: Advances in Neural Information Processing Systems
24 (NIPS 2011), no. 2, 2011, pp. 1089—-1097.

[35] R. G. Baraniuk, V. Cevher, M. F. Duarte, C. Hegde, Model-based compressive sensing, IEEE
Transactions on Information Theory 56 (4) (2010) 1982–2001. doi:10.1109/TIT.2010.2040894.

[36] S. Szarek, Metric entropy of homogeneous spaces, Banach Center Publications 43 (1) (1998)

395–410. doi:10.4064/-43-1-395-410.

[37] E. J. Cand`es, J. K. Romberg, T. Tao, Stable signal recovery from incomplete and inaccurate
measurements, Communications on Pure and Applied Mathematics 59 (8) (2006) 1207–1223.
doi:10.1002/cpa.20124.

[38] X. Yi, D. Park, Y. Chen, C. Caramanis, Fast algorithms for robust PCA via gradient descent,

in: Advances in Neural Information Processing Systems 29 (NIPS 2016), 2016.

[39] K. C. Toh, M. J. Todd, R. H. T¨ut¨unc¨u, SDPT3 — A Matlab software package for semideﬁ-
nite programming, Version 1.3, Optimization Methods and Software 11 (1-4) (1999) 545–581.
doi:10.1080/10556789908805762.

[40] J. Blanchard, J. Tanner, Performance comparisons of greedy algorithms in compressed sensing,
Numerical Linear Algebra with Applications 22 (2) (2015) 254–282. doi:10.1002/nla.1948.

27

[41] J. D. Blanchard, J. Tanner, K. Wei, CGIHT: Conjugate gradient iterative hard thresh-
Information and Inference (2015)

olding for compressed sensing and matrix completion,
iav01doi:10.1093/imaiai/iav011.

[42] G. Stephen, M. Boyd, CVX: Matlab Software for Disciplined Convex Programming, version 2.1

(2014).

[43] T. Zhou, D. Tao, GoDec: Randomized low-rank & sparse matrix decomposition in noisy case,
Proceedings of the 28th International Conference on Machine Learning 35 (1) (2011) 33–40.

[44] L. Li, W. Huang, I.-H. Gu, Q. Tian, Statistical modeling of complex backgrounds for fore-
ground object detection, IEEE Transactions on Image Processing 13 (11) (2004) 1459–1472.
doi:10.1109/TIP.2004.836169.

[45] M. Dimitris, D. Marden, G. Shaw A., Hyperspectral

target detection applications, Lincoln Laboratory Journal 14 (1)
doi:10.1039/C4RA04655B.

image processing for automatic
(2003) 79 —- 116.

[46] X. Cao, T. Yue, X. Lin, S. Lin, X. Yuan, Q. Dai, L. Carin, D. J. Brady, Computational snapshot
multispectral cameras: toward dynamic capture of the spectral world, IEEE Signal Processing
Magazine 33 (5) (2016) 95–108. doi:10.1109/MSP.2016.2582378.

[47] K. Degraux, V. Cambareri, L. Jacques, B. Geelen, C. Blanch, G. Lafruit, General-
2015 IEEE International
IEEE, 2015, pp. 315–319.

ized inpainting method for hyperspectral
Conference on Image Processing (ICIP), Vol. 2015-Decem,
doi:10.1109/ICIP.2015.7350811.

image acquisition,

in:

[48] G. A. Antonucci, S. Vary, D. Humphreys, R. A. Lamb, J. Piper, J. Tanner, Multispectral snapshot
demosaicing via non-convex matrix completion, in: 2019 IEEE Data Science Workshop (DSW),
IEEE, 2019, pp. 227–231. doi:10.1109/DSW.2019.8755561.

[49] D. Stein, S. Beaven, L. Hoﬀ, E. Winter, A. Schaum, A. Stocker, Anomaly detec-
imagery, IEEE Signal Processing Magazine 19 (1) (2002) 58–69.

tion from hyperspectral
doi:10.1109/79.974730.

[50] Y. Xu, B. Du, L. Zhang, D. Cerra, M. Pato, E. Carmona, S. Prasad, N. Yokoya, R. Han-
sch, B. Le Saux, Advanced multi-sensor optical remote sensing for urban land use and land
cover classiﬁcation: outcome of the 2018 IEEE GRSS data fusion contest, IEEE Journal of
Selected Topics in Applied Earth Observations and Remote Sensing 12 (6) (2019) 1709–1724.
doi:10.1109/JSTARS.2019.2911113.

[51] R. Kemker, C. Salvaggio, C. Kanan, Algorithms for semantic segmentation of multispectral re-
mote sensing imagery using deep learning, ISPRS Journal of Photogrammetry and Remote Sens-
ing 145 (June 2017) (2018) 60–77. doi:10.1016/j.isprsjprs.2018.04.014.

[52] A. Kyrillidis, V. Cevher, Matrix recipes for hard thresholding methods, Journal of Mathematical

Imaging and Vision 48 (2) (2014) 235–265. doi:10.1007/s10851-013-0434-7.

[53] K. Wei, Fast iterative hard thresholding for compressed sensing, IEEE Signal Processing Letters

22 (5) (2015) 593–597. doi:10.1109/LSP.2014.2364851.

[54] G. G. Lorentz, M. V. Golitschek, Y. Makovoz, Constructive approximation: Advanced problems,

Springer-Verlag Berlin Heidelberg, 1996.

28

Appendix A. Phase transitions for synthetic problem of size m = n = 30

Figure A.6 depicts the phase transitions of δ above which NIHT, NAHT and solving the convex
relaxation problem in (8) successfully recovers X0 in more than half of the experiments. Comparing
Fig. A.6 to Fig. 1 we see that the phase transitions roughly occur for the same parameters ρr, ρs with
only small diﬀerences due to the ﬁnite dimensional eﬀects of the smaller problem size being more
pronounced when m = n = 30. We also observe that non-convex algorithms perform better than the
convex relaxation in that they are able to recover higher ranks and sparsities from fewer samples in
addition to also taking less time to converge.

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(a) Convex relaxation (Gaussian measure-
ments)

(b) Convex relaxation (FJLT measure-
ments)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(c) NIHT (Gaussian measurements)

(d) NIHT (FJLT measurements)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

(e) NAHT (Gaussian measurements)

(f) NAHT (FJLT measurements)

Figure A.6: Phase transition level curves denoting the value of δ∗ for which values of ρr and ρs below
which are recovered for at least 5 out of 10 experiments for δ, ρr, and ρs as given by (127). The convex
optimization problem is solved by SDPT3 [39]. NIHT and NAHT are observed to recover matrices of
higher ranks and sparsities compared to solving the convex relaxation.

29

Appendix B. Supporting lemmata

The following lemma reveals the usefulness of incoherence in controlling the correlation between

incoherent low-rank and sparse matrices.

Lemma Appendix B.1 (Subadditivity of the LSm,n(r, s, µ) set). The sum of two incoherent low-
rank plus sparse matrices X1, X2 ∈
LSm,n(r, s, µ) is also an incoherent low-rank plus sparse matrix
X1 + X2 ∈

LSm,n(2r, 2s, µ), and consequently

LSm,n(r, s, µ) + LSm,n(r, s, µ) = LSm,n(2r, 2s, µ),

where the plus sign denotes the Minkowski sum of two sets.

Proof. Let X1, X2 ∈
left and the right singular vectors of L1 and L2 respectively.

LSm,n(r, s, µ) with X1 = L1 + S1, X2 = L2 + S2, and U1, U2 and V1, V2 being the

Construct the sum X = L + S, where L = L1 + L2, S = S1 + S2, and U, V are the left and right
singular vectors of the newly constructed L. Since the column space of U is a subspace of the column
space of the concatenated matrix [U1 U2] we have that the projection on U must have a smaller or
equal norm than the projection on [U1 U2]

2

2
2 ≤

(cid:13)
(cid:13)

U T ei

[U1 U2]T ei
2
(cid:13)
(cid:13)
i [U1 U2] [U1 U2]T ei
= eT
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
U T
U T
2 +
(cid:13)
where in the third line we use the deﬁnition of incoherence. Since the rank of the matrix doubled, the
(cid:13)
µ2r
m . The argument can be followed mutatis mutandis

inequality yields the desired result
for the upper bound on the right singular vectors V .

(cid:13)
(cid:13)
U T eik ≤

2
2 ≤

µr
m

2 ei

1 ei

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

=

k

2

,

q

Proof of Lemma 1.1 (LSm,n(r, s, µ) is a closed set), stated on page 3.

Proof. The ﬁrst part (1) of the statement is proved as part of Lemma Appendix B.2.

To prove the second part (2) of the statement, let X = L+S

LSm,n(r, s, µ) and denote γ = µ r√s
√mn
for which we have γ < 1 by µ < √mn/(r√s). By conicity of LSm,n(r, s, µ) we can assume without
loss of generality

kF = 1. The bound on the correlation in (1) states

X

∈

k

L, S
i|
S
kF
which combined with the rearranged terms of the identity

|h
L
kF k

≥

γ

k

,

X

k

2
F = 1 =

2
F +

L

k

S

k

k

2
F + 2

L, S
h

i

yields

.

(B.1)

k
S
L

k
k

kF
kF −

k
k

k
L
S

kF
kF (cid:12)
(cid:12)
(cid:12)
(cid:12)

γ

≥

L, S
i|
S
kF

|h
L
kF k

=

1
2

1

L

kF k

S

kF −

k

S
k
by

k
The proof follows by showing that the inequality in (B.1) implies an upper bound on
kF . For ease of notation, we denote x :=
kF and y :=
k
L
k
x2
y2
2γxy
−
S
kF are strictly positive.
0 implies that x

kF and
kF , and multiply the inequality in (B.1)
(B.2)

1, and thus concludes the proof. The other

where we used that
The case of 1

L
k
x2

1 and y

kF k

≥ |

kF

−

L

S

S

1

k

k

|

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
L

kF ,
y2
−
≤
0 in (B.2) is equivalent to

k
≥

≤

case of 1

x2

−

−

−
y2

≤

2γxy

≥

x2 + y2

1,

−

30

√c2x2
which has two roots y =
respectively, we seek only the real roots, for which to exist we need c2x2
c < 1, we can rearrange the terms as

x2 + 1. Since x, y denote the Frobenius norm of L and S
0, and because

x2 + 1

cx

−

≥

−

−

±

which is equivalent to

x

≤

1

,

γ2

1

−

p

L

k

kF ≤

1

γ2

1

−

=

1
(cid:18)

−

µ2 r2s
mn

1/2

−

,

(cid:19)

prooving the second statement (2) in Lemma 1.1. Applying the same arguments to
bound on the Frobenius norm of the sparse component.

kF yields the
Finally, to prove the third part of the statement (3), consider a sequence Xi = Li + Si ∈
kF , we

XikF → k

. Since, also

n as i

→ ∞

Rm

X

∈

S

k

k

p

LSm,n(r, s, µ) that converges to a matrix X
have that for any εp > 0, there exists i0 ∈
X

×
N such that

∀
which, combined with
LikF ≤
k
µ r2s
1/2 by the second part of the statement (2).
mn )−
where τ := (1

X
XikF ≤ k
XikF , implies that for all i

εp ≤ k

i > i0 :

kF −

≥

k

k

τ

kF + εp,
i0 we have

LikF ≤

k

τ

X

kF + τ εp

k

Denote the closed set of rank-r matrices whose Frobenius norm is bounded by γ > 0 as

−

Lm,n (r, γ) =
(cid:8)
which is also compact by being closed and bounded.
kF + τ εp) for all i

X

∈

Y

k

×

Rm

n :

rank (Y )

r,

≤

Y

k

kF ≤

γ

,

(cid:9)

can assume, by passing to a subsequence, that Li

We have that Li ∈ Lm,n (r,

i
→∞
−−−→
Additionally, since τ > 0 is ﬁxed, the upper bound of the Frobenius norm of the low-rank compo-
L
k

i0. Since the set is compact and closed, we
∈ Lm,n (r, τ
L
kF .
X
τ
k

XikF must also hold in the limit

kF + τ εp) as i

By the set of s-sparse matrices being closed, we have that the limit point

LikF ≤
k

kF ≤

nent

i0.

X

≥

≥

k

k

τ

is also an s-sparse matrix, thus

Si = Xi −

Li →

X

−

L,

X = L + (X

L)

−

∈

LSm,n(r, s, µ),

proving that LSm,n(r, s, µ) is closed.

Lemma Appendix B.2 (The rank-sparsity correlation bound). Let L, S
be the singular value decomposition of L, then

∈

Rm

n and L = U ΣV T

×

L, S

|h

i| ≤

abs (U ) abs

V T

σmax (L)

S

k1 ,

k

∞

(B.3)

(cid:1)(cid:13)
(cid:0)
where abs(
) denotes the entry-wise absolute value of a matrix, the matrix norms are vectorised entry-
(cid:13)
·
wise ℓp-norms, and σmax (L) is the largest singular value of L. As a consequence, if L is a rank-r
matrix that is µ-incoherent and S is an s-sparse matrix

(cid:13)
(cid:13)

L, S

|h

i| ≤

µ

r√s
√mn k

L

S

kF k

kF .

(B.4)

31

Proof. For L, S

Rm

×

∈

n and L = U ΣV T being the singular value decomposition of L, we have

L, S

|h

=

i|

≤

=

≤

≤

≤

[m]
X(i,j)
∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
[m]
X(i,j)
∈

[n]

×

[n]

×

[m]
X(i,j)
∈

×

[n]

[n]

X(i,j)
[m]
∈
×
σmax(L)

Si,j eT

i U ΣV T fj (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

Si,j Li,j (cid:12)
(cid:12)
(cid:12)
(cid:12)
U T ei
(cid:12)
(cid:12)

Si,j|

|

|

|

r

(cid:12)
(cid:0)
(cid:12)
(cid:12)
Si,j| (cid:12)
Xk=1
(cid:12)
(cid:12)
r
(cid:12)
(cid:12)
Xk=1

Si,j|

[n]

×
V T fj

(cid:12)
(cid:12)
X(i,j)
[m]
(cid:12)
∈
(cid:12)
T
(cid:12)
Σ
(cid:12)
(cid:1)

(cid:0)
U T ei

k

(cid:1)(cid:12)
(cid:12)
(cid:12)
V T fj

σk

(cid:0)

σk abs

(cid:1)
(cid:0)
U T ei

(cid:0)
abs

Si,j|

|

k(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
(cid:12)
V T fj

k abs
(cid:1)
U T ei

T

(cid:0)
abs

k

(cid:1)
V T fj

σmax(L)

[m]
X(i,j)
∈
S
k1

k

[n]

×
abs (U ) abs

(cid:1)

(cid:0)

(cid:1)

(cid:0)
V T

∞

(B.5)

(B.6)

(B.7)

(B.8)

(B.9)

(B.10)

(cid:13)
(cid:13)

(cid:1)(cid:13)
(cid:13)

(cid:0)
Rn to be the canonical basis vectors of Rm
Rm, fi ∈
where in the ﬁrst line in (B.5) we denote ei ∈
and Rn, the inequality in the second line (B.6) comes from the subadditivity of the absolute value,
in the third line (B.7) we write out the inner product as a sum, in the fourth line (B.8) we use the
subadditivity and multiplicativity of the abslolute value and denote abs(
) as the entry-wise absolute
·
value of a vector, the ﬁfth line (B.9) comes from σmax(L) being the largest singular value of L, and
the ﬁnal line in (B.10) comes from the entry-wise ℓ
-norm bounding the absolute value of all entries
of

V T

abs (U ) abs
If the low-rank component L is also µ-incoherent, we further have
(cid:0)

(cid:1)(cid:1)

∞

(cid:0)

.

abs (U ) abs

V T

(cid:13)
(cid:13)

(cid:0)

∞

(cid:1)(cid:13)
(cid:13)

=

≤

≤

max
(i,j)
[m]
∈
U T ei
r
(cid:13)
,
(cid:13)
√mn

(cid:13)
µ
(cid:13)

×

2

abs

U T ei

T

abs

V T fj

[n]

(cid:1)

(cid:0)

(cid:1)

(cid:0)
V T fj

2

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(B.11)

(B.12)

(B.13)

where the ﬁrst upper bound comes from the Cauchy-Schwarz inequality on the entry-wise absolute
values of U T ei and V T fj, and the second upper bound comes from the deﬁnition of incoherence in
(13) Combining (B.13) with (B.10), the fact that
kF for s-sparse matrices, and that
kF yields the result in (B.4)
σmax ≤ k
Proof of Lemma 2.1 (RIC for a ﬁxed LS subspace), stated on page 8.
The proof uses similar to arguments as [25, Lemma 5.1] and [11, Lemma 4.3] with the exception that
here we consider two subsets, one for the low rank and another for the sparse component.

k1 ≤

S
k

√s

L

S

k

Proof. By the linearity of
ity that

A
kF = 1. By Lemma 1.1 with

) and conicity of Σm,n(V, W, T, µ) we can assume without loss of general-
r√s , we can bound the Frobenius norm
1/2

(
·

X

X

k

k

kF = 1 and µ < √mn
S

of the low-rank and the sparse component as

1
There exist two ﬁnite ( ¯∆/8)-coverings of the two matrix sets with bounded norms
(cid:16)

τ , where τ :=

kF ≤

kF ≤

τ and

L

k

k

−

µ2 r2s
mn

−

.

L

∈

(cid:8)

(L)

V,
⊆
n : S

×

C
Rm

C

(LT )

S

T,

k

⊆

W,

⊆
kF ≤

Rm

×

n :

S

∈

(cid:8)

τ

L

k
τ

kF ≤
,

(cid:9)

(cid:9)

32

(cid:17)

(B.14)

(B.15)

(B.17)

(B.18)

(B.19)

(B.20)

(B.21)

(B.22)

that we denote ΛL, ΛS and by [54, Chapter 13] they are subsets of the two sets in (B.14) and (B.15),
and their covering numbers are upper bounded as

ΛL

≤

24
¯∆

τ

(cid:18)

(cid:12)
(cid:12)
ΛL, QS

(cid:19)
ΛS

(cid:12)
(cid:12)
QL + QS : QL

dim V

dim W

·

ΛS

dim T

.

24
¯∆

τ

(cid:19)

≤

(cid:18)

(B.16)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

Let Λ :=
∈
The set Λ is a ( ¯∆/4)-covering of the set Σm,n (V, W, T, µ) since for all X
exists a pair Q

be the set of sums of all possible pairs of the two coverings.
Σm,n (V, W, T, µ) there

Λ such that

∈

∈

(cid:8)

(cid:9)

∈

X

k

Q

kF =

−

L + S

QL + QS

F

(cid:13)
(cid:13)

L

−

≤

(cid:13)
(cid:13)

−
QL

(cid:0)
F +
(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:1)(cid:13)
QS
(cid:13)

S

−

F ≤

¯∆
8

+

¯∆
8

,

(cid:13)
(cid:13)

where in the ﬁrst line we used the fact that X can be expressed as L + S, and in the second line we
applied the triangular inequality combined with the QL, QS being ( ¯∆/8)-coverings of the matrix sets
for the low-rank component and the sparse component respectively.

Applying the probability union bound on concentration of measure of

as in (4) with εp = ¯∆/2

A

gives that

1
(cid:18)
holds with the probability at least

Λ) :

(
∀

Q

∈

¯∆
2

−

Q
k

(cid:19)

kF ≤ kA

k2 ≤

(Q)

¯∆
2

(cid:19)

1 +

(cid:18)

Q

kF ,

k

2

1

−

(cid:18)

24
¯∆

τ

(cid:19)

dim V

dim W

·

dim T

24
¯∆

τ

(cid:19)

(cid:18)

exp

p
2

(cid:18)

−

(cid:18)

¯∆2
8 −

¯∆3
24

.

(cid:19)(cid:19)

By Σm,n(V, W, T, µ) being a closed set by Lemma 1.1, the maximum

M =

max

Σm,n(V,W,T,µ),

Y

∈

Y

k

k

is attained. Then there exists Q

Λ such that

∈

F =1 k A

(Y )

k2,

¯∆
2

¯∆
4

Q)

(X

kA

kA

(X)

(X)

k2 ≤ kA

k2 +
where the ﬁrst inequality comes from applying the triangle inequality to X and Q
second inequality we used (B.19) to upper bound
Lemma Appendix B.1 and the upper bound of
X
a ( ¯∆/4)-covering. Note that the inequality (B.22) holds for all X
norm
inequality in (B.22) applied to the matrix that attains the maximum

X and in the
Q)
Σm,n(V, W, T, µ) by
Λ combined with Λ being
Σm,n(V, W, T, µ) whose Frobenius
X for which the maximum in (B.21) is attained. The
X yields

(X)
k2 since (X
−
kF comes from Q
∈
∈

kF = 1 and thus also for a matrix

k A
Q
−

k2 ≤

+ M

1 +

X

−

−

∈

k

k

,

¯∆
2

+ M

b
¯∆
4

M

≤

1 +

=

⇒

M

≤

b
1 + ¯∆.

The lower bound follows from the reverse triangle inequality

(X)

k2 ≥ k A

k2 − k A

(Q)

(X

k A

Q)

k2 ≥

−

1
(cid:18)

−

¯∆
2

−

(cid:19)

(1 + ¯∆)

¯∆
4 ≥

¯∆

1

−

(B.23)

(B.24)

where the second inequality comes from
Q)
bined with Q being an element of a ( ¯∆/4)-covering.

kA

(X

−

k2 ≤

M

X

k

−

Q

kF ≤

1 + ¯∆

¯∆
4 by (B.21) com-

Combining (B.22) with the bound on M in (B.23) gives the upper bound and (B.24) gives the

(cid:0)

(cid:1)

lower bound on

(X)

k2 completing the proof.

kA

33

Lemma Appendix B.3 (ε-covering of the Grassmannian [36, Theorem 8]). Let (
a metric space on a Grassmannian manifold
exists ε-covering

)) be
(D, d) with the metric ρ as deﬁned in (19). Then there
(D, d) such that

(D, d), ρ(
·

(D, d) with Λ =

G

,

·

G

ρ(U,

U )

ε,

≤

(B.25)

Ui}
{
U

∀

∈ G

G
N
i=1 ⊂ G
(D, d) : min
Λ

b
U

∈

and N

≤

d(D

d)

−

C0
ε

with C0 independent of ε, bounded by C0 ≤

b

2π.

The above bound on the covering number of the Grassmannian is used in the following lemma to

(cid:1)

(cid:0)

bound the covering number of the set LSm,n(r, s, τ ).
Proof of Lemma 2.3 (Covering number of LSm,n(r, s)), stated on page 9.

Proof. By Lemma Appendix B.3 there exist two ﬁnite (ε/2)-coverings Λ1 :=
Λ2 :=

(n, r), with their covering numbers upper bounded as

Wi}
{

Λ2
|
|
i=1 ⊆ G

Vi}
{

Λ1
|
|
i=1 ⊆ G

(m, r) and

Λ1| ≤

|

4π
ε

r(m

r)

−

Λ2| ≤

|

4π
ε

r(n

−

r)

,

(cid:19)
as given in [11, (4.18)] that uses [36, Theorem 8]. By Λ1, Λ2 being (ε/2)-coverings

(cid:18)

(cid:19)

(cid:18)

V

∀

W

∈ G

(m, r) :

(n, r) :

Vi ∈
∃
Wi ∈
∃

Λ1,
Λ2,

ρ(V, V1)
≤
ρ(W, W1)

ε/2,

ε/2.

∈ G

∀
(mn, s) is the set of all possible support sets of an m

≤

Let Λ3 =
s elements. Thus the cardinality of Λ3 is

(mn, s) where

V

V
Construct Λ = (Λ1 ×
(n, r) and T
∈ G
V ,

(V, W ) ,

W

(m, r), W

G
ρ

Λ2 ×
∈ V

mn
s
Λ3) where

(cid:0)

.

(cid:1)
×

denotes the Cartesian product. Choose any V

(mn, s) for which we now show there exists

V ,

W ,

T

n matrix that has

×

∈
Λ such that

∈

ε and T = ˆT , thus showing that the set Λ is an ε-covering of LSm,n(r, s, τ ).

(cid:17)

(cid:16)

b

c

b

≤

(cid:17)(cid:17)

(cid:16)
Satisfying T =
c

(cid:16)
V
The projection operator onto the pair (V, W ) can be written as P(V,W ) = PV ⊗
pairs of subspaces (V, W ) and (

(mn, s) containing all support sets with at most s entries.
PW , so for the two

W ) we have the following

T comes from Λ3 =

V ,

b

b

(B.26)

(B.27)

(B.28)

ρ

(V, W ) ,

V ,
b

W
c

(cid:16)

(cid:16)

b

c

(cid:17)(cid:17)

=

=

=

k

k

k

b
V ,

c
W )k

P b

P(
PW −
P b
V

P(V,W ) −
PV ⊗
PV −
P b
PV −
(cid:0)
V
V,

⊗
PW k
(cid:1)
V kk
W,
+ ρ

Pc

V ⊗
W k
PW + P b
V
P b

+

W

.

≤ k
= ρ

PW −
PW −
(cid:0)
V kk

k

Pc
W
Pc

(cid:1)
W k

(B.29)

(B.30)

(B.31)

(B.32)

(B.33)

k

(cid:17)

c

By Λ1 and Λ2 being (ε/2)-coverings, we have that for any V, W exist
ρ
+ ρ

Λ2, such that
ε. Using the bounds on the cardinality of Λ1, Λ2 in

(V, W ) ,

Λ1 and

W,

V ,

W

W

W

V,

∈

∈

V

V

(cid:16)
(B.26) combined with

(cid:17)(cid:17)

(cid:16)

yields that the cardinality of Λ is bounded above by

(cid:16)

(cid:17)

b

c

(cid:16)

(cid:17)

(cid:16)

b

≤

b

c

c

Λ1| |

Λ2| |

Λ3| ≤

|

mn
s

(cid:18)

(cid:19) (cid:18)

4π
ε

r(m+n

−

2r)

.

(cid:19)

(B.34)

≤

ρ
(cid:16)
Λ3|

|

=

mn
(cid:17)
s
b
(cid:1)
(cid:0)
R(ε) =

Proof of Lemma 2.2 (Variation of ¯∆ in RIC in respect to a perturbation of (V, W )), stated on
page 8.

34

Proof. Recall the notation used in Lemma 2.2 that there are sets Σ1 := Σm,n (V1, W1, T, µ) and
Σ2 := Σm,n (V2, W2, T, µ) which have a shared support T of the sparse component.
Σ2, so we can write Y = L + S such that supp(S) = T,
V2,
µ2 r2s
kF for τ := (1
kF = 1 and therefore

W2 and
Let Y
⊆
τ
assume without loss of
kF ≤
k
generality
τ . Denote U1 = (V1, W1) and U2 = (V2, W2) and let PUi
be an orthogonal projection onto the space of matrices whose column and row space is deﬁned by
Vi, Wi such that left and right singular vectors of PUi Y lie in Vi respectively Wi. Then

1/2 by Lemma 1.1. By linearity of
kF ≤

mn )−
L

∈
Y
k
Y
k

(LT )

(L)

A

⊆

−

L

C

C

k

(Y )
k

k A

PU2 L))

k

−

=

kA

(L + S)
=
k
(PU1 L + S)
k

(PU1 L
(PU1 L + S
−
kA
PU2 ] L)
+
([PU1 −
k
kA
≤ kA
(1 + ¯∆)
PU1 L + S
ρ (U1, U2)
+
k
k
k A k
≤
= (1 + ¯∆)
+
PU2 ] L
PU2 L + S + [PU1 −
k
(1 + ¯∆) (
) +
L
kF + ρ(U1, U2)
Y
k
k
1 + ¯∆ +
1 + ¯∆ + τ ρ(U1, U2)
Y

k
k A k

≤

k

k

≤ k

kF

L

k

,

k A k

ρ (U1, U2)

k A k
ρ (U1, U2)

L

k

k

(B.35)

(B.36)
(B.37)

(B.38)
(B.39)

(B.40)

L
k

k

(cid:0)

where in the ﬁrst line (B.35) we use the fact that PU2 L = L, the second line (B.36) follows by
the triangle inequality and linearity of
on
combined with the deﬁnition of ρ in (19). We proceed in (B.38) and
(PU1 L + S) using the RICs of
on (PU2 L + S). Finally, in
(B.39) by projecting L to space U2 and again bounding the eﬀect of
(B.40) we use

(cid:1)(cid:1)
, and in the third inequality we bound the eﬀect of

τ . We obtain a similar lower bound using the reverse triangular inequality

A

A

A

A

L

(cid:0)

k

kF ≤
(Y )
k

k A

(PU1 L + S
(PU1 L + S)

=

kA
≥ kA
1

≥
=

≥

1
(cid:0)
1
(cid:0)

Y

(cid:0)
≥ k

¯∆
¯∆
(cid:1)
¯∆
(cid:1)
1
(cid:1)

−

−

−
kF

(PU1 L

PU2 L))

k
PU2 ] L)
k
L

ρ(U1, U2)
k

PU1 L + S

−
k − kA

−
([PU1 −
k − k A k
PU1 ] L
[PU2 −
kF )
L
ρ(U1, U2)
k
¯∆ +
τ ρ(U1, U2)(1
−

PU2 L + S
Y
kF −
¯∆
−

−

− k A k
)

k A k

kF
k − k A k

.

k

k
(
k

−

L

ρ(U1, U2)
k
L
kF

ρ(U1, U2)
k

kF

(B.41)
(B.42)

(B.43)

(B.44)

(B.45)

(B.46)

(B.47)

Combining (B.40) and (B.46) yields

(cid:0)

Y

∀

with ¯∆′ = ¯∆ + τ ρ(U1, U2)

Σ2 :

∈
1 + ¯∆ +

(1

¯∆′)
k

Y

−

.

k A k

(cid:1)

(1 + ¯∆′)
k

Y

kF ,

kF ≤ k A

(Y )

k ≤

In the proof of Theorem 3 we make use of the following Lemma Appendix B.4 and Corol-
lary Appendix B.1 from [11] which we restate here for completeness with the small addition of the
incoherence property in (2).

(cid:0)

(cid:1)

Lemma Appendix B.4 ([11, Lemma 3.4]). Let A
exist matrices B1 and B2 such that

∈

LSm,n(r, 0, µ) and B

Rm

n. Then there

×

∈

(1) B = B1 + B2,
(2) B1 ∈
(3) ABT
(4)

= 0.

B1, B2i
h

LSm,n(2r, 0, µ),
2 = 0 and AT B2 = 0,

Proof. Consider a full singular value decomposition of A,

A = U

Σ 0
0
0

(cid:20)

V T ,

(cid:21)

35

(B.48)

and let ˆB := U T BV . Partition ˆB as

ˆB =

ˆB11
ˆB21

(cid:20)

ˆB12
ˆB22 (cid:21)

.

Deﬁning now

B1 := U

ˆB11
ˆB21

ˆB12
0

(cid:20)

V T ,

(cid:21)

B2 := U

0
0

(cid:20)

0
ˆB22 (cid:21)

V T ,

it can be veriﬁed that B1 and B2 satisfy the conditions of the lemma.

(B.49)

(B.50)

Corollary Appendix B.1 ([11, Lemma 2.3]). Let A and B be matrices of the same dimensions. If
ABT = 0 and AT B = 0, then
+

A + B

=

B

A

.

k∗
k
Lemma Appendix B.5 (Decomposing RS = RS
0 + RS
RS

0 that has the entries of RS at indices Ω0

k∗

k∗

k

k

c ). Let supp S0 = Ω0 and construct a matrix

if
if

(RS

(RS)i,j
0

0 )i,j =

(i, j)
∈
(i, j) /
∈
0 that has the entries of RS at the indices of the complement of Ω0. Then
(by

Ω0,
Ω0,

(B.51)

= s),

(

RS

and a matrix RS

c = RS
−
S0k0 = s
k

Ω0|
|
RS
S0k1 +
c k1
k
(by supp(RS
0 )

(1)
(2)
(3)

RS
0 k0 ≤ k
k
S0 + RS
c k1 =
k
0 , RS
RS
= 0
c i
h

∩
0 and RS
Proof. It can be easily veriﬁed that RS

(by supp(RS
0 )
supp(RS
c ) =

∩
).

∅

supp(RS

c ) =

),

∅

c constructed as in (B.51) satisfy the conditions (1)-(3).

Lemma Appendix B.6 (Decomposing RL
Rm
n be an arbitrary matrix and Mr ∈
RL
×
mn
i=1 RL
decomposition RL

i such that

c ∈

c =

c into a sequence of incoherent low-rank matrices). Let
N be a ﬁxed rank of the decomposition. There exists a

P

RL
i

RL
j

T

= 0m

(cid:0)

(cid:1)

RL

i ∈
m and

×
RL

i+1

LSm,n(Mr, 0, 1)

T

RL

j = 0n
2

×

(cid:1)

RL
i

.

RL
i
1
(cid:0)
Mr

2
F ≤

Rm

(cid:13)
(cid:13)
m and Z = [z1, z2, . . . , zn]

(cid:13)
(cid:13)

(cid:13)
(cid:13)

∗
(cid:13)
(cid:13)

n,

= j

i
∀

(B.52)

(B.53)

(B.54)

Proof. Let Y = [y1, y2, . . . , ym]
are maximally incoherent with the canonical basis

∈

×

Rn

×

n be two bases whose vectors

∈

i, j

∀

∈

[m]

i, j

∀

∈

[n]

k

k

yT
j eik2 =
zT
j eik2 =

1
√m
1
√n

,

(B.55)

(B.56)

which can be constructed by taking m columns and the same rows of a Hadamard matrix and rescaling
it such that it forms an orthonormal basis.
yizT
j

n. Since E is a basis, there are coeﬃcients c1, c2, . . . , cmn ∈

m,n
i,j=1 ⊂

Denote E =

Rm

R

×

such that

(cid:8)

(cid:9)

mn

RL

c =

ck yk zT
k .

(B.57)

Xk=1

36

6
Since the columns of Y and Z can be arbitrarily permutated, we can assume without loss of generality
that

for all i. We split the indices of

into sets of size Mr as

ck| ≥ |

ck+1|

|

.

(B.58)

{

1, . . . , mn
1)Mr + 1, . . . , iMr}

}

Ii :=

(i
{

−

:=

Constructing RL
k results into the decomposition with desirable properties. The
i
ﬁrst property (B.52) follows from the subadditivity of the incoherence in Lemma Appendix B.1, the
second property in (B.53) follows from E being an orthogonal basis, and ﬁnally, the last property in
(B.54) comes from the ck being the singular values of each constructed RL
i .

ck yk zT

P

Ii

∈

k

Lemma Appendix B.7 (Upper bound on
(
) whose RICs are
hA
·
upper bounded by ∆2 := ∆2r,2s,µ and two incoherent low-rank plus sparse matrices X1 = L1 + S1 ∈
=
LSm,n(r, s, µ), X2 = L2+S2 ∈
0 and have bounded the rank-sparsity coeﬃcient γ2 := γ2r,2s,µ < 1, we have that

LSm,n(r, s, µ) that have orthogonal components

). For an operator
)
i

(
A
·
L1, L2i
h

S1, S2i
h

= 0,

(
·

A

),

2γ2

∆2 +

1

≤

(cid:18)

γ2
2 (cid:19)

X1kF k

X2kF ,

k

(X1),

A

(X2)
i
(cid:12)
(cid:12)
(cid:12)

hA
(cid:12)
(cid:12)
(cid:12)

−
√mn is the rank-sparsity correlation coeﬃcient as deﬁned in Lemma B.4 on page31.
) being a linear transform, bilinearity of the inner-product, and conicity of LSm,n(r, s, µ),
X2kF = 1. The parallelogram law
X1kF = 1 and
k

k

where γ2 = µ 2r√2s

(
·
A

Proof. By
we can assume without loss of generality that
k2 and
applied to
(X1)
k

k2 yields
=
(X2)
k

kA
2
2 +

(X2)

(X1)

kA

kA

2
2

2

Subtracting 2

(X1) +

(X2)
k

2
2 +

(X1)

kA

(X2)
k

− A

2
2 .

(cid:16)
(X1)
kA

(cid:17)

A

kA
kA
2
2 from both sides of (B.60)
2
(X2)
2 − kA
k

(X2)
k
(X2)
i

(X1) +

kA

A

A

=

− A
(X1),

4

hA

(X1)

(X2)
k

− A

2
2 .

We can expand the equality in (B.61) to bound its right-hand side using the RICs as

(X1),

(X2)

i|

A

|hA

1
4
1
4
1
4

=

≤

≤

2
F − k A

(X1 + X2)
k
X1 + X2k
2
F + 2
X1k
k

k

2
F −

(X1 −
(1

−
X1, X2i
h

2
F

X2)
k

∆2)

(cid:12)
X1 −
(cid:12)
k
X2k

2
F

k

2
F

X2k

(cid:12)
(cid:12)
(cid:12)

+

(cid:16)
∆2)

X1k
k

2
F −

2

X1, X2i
h

+

(cid:17)
X2k
k

k A
(cid:12)
(1 + ∆2)
(cid:12)
(cid:12)
(cid:12)
(1 + ∆2)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(1

−

−

2
F

(B.64)

=

2
F +

∆2
2

X1k
k

(cid:16)
X2k
k

(cid:17) (cid:12)
(cid:12)
(cid:12)
X1, X2i
(cid:12)
h
(cid:12)
(cid:12)
) and by X1 + X2 and
where the inequality in the second line in (B.63) comes from the RICs of
(cid:12)
X2 being in the set LSm,n(2r, 2s, µ) combined with Lemma Appendix B.1, the equality in the
X1 −
third line in (B.64) is the result of expanding the inner products, and ﬁnally, the last equality in
(B.64) comes from elementary operations and using the fact that
Moreover, by X1 and X2 being component-wise orthogonal

X1k
k
L1, L2i
h
upper-bound the magnitude of the correlation between X1 and X2 as

X1, X2i
h
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X2k
k
S1, S2i
h

∆2 +
(cid:12)
(cid:12)
(cid:12)

= 1 and
= 0 and

= 0, we can

(B.65)

(
·
A

= 1.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

=

2
F

(cid:17)

(cid:16)

(B.59)

(B.60)

(B.61)

(B.62)

(B.63)

X1, X2i|
|h

=
=

≤

+
+

L1, L2i
|h
L1, S2i
|h
γ2

L1, S2i
h
L2, S1i|
h
S2kF +
L1kF k
k
(cid:16)
2γ2
,

+

L2, S1i
h

+

S1, S2i|
h

L2kF k

S1kF

k

(cid:17)

(B.66)
(B.67)

(B.68)

(B.69)

≤

1

γ2
2

−

37

where in the ﬁrst equality in (B.66) we expanded the inner-product, the second equality in (B.67) is
the consequence of the components being orthogonal, the inequality in the third line in (B.68) is the
consequence of Lemma Appendix B.2, and the last inequality in (B.69) comes from the upper-bound
of the norms

S2kF from Lemma 1.1 and by
k
We can now further upper bound (B.65) using the bound in (B.65) combined with the triangle on

X1kF = 1 and
k

X2kF = 1.

L2kF ,
k

S1kF ,
k

L1kF ,

k

k

the absolute value

when

X1kF = 1 and
k

k

(X1),

hA

(X2)
i

A

≤

∆2 +

2γ2

γ2
2

1

−

,

(B.70)

X2kF = 1 which translates into the bound in (B.59) in the general case

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
X2
X2kF (cid:19)(cid:29)(cid:12)
(cid:18)
k
(cid:12)
2γ2
(cid:12)
∆2 +
(cid:12)
γ2
2 (cid:19)

1

−

A

(cid:18)

X1
X1kF (cid:19)
k

,

A

(cid:18)

≤

(cid:12)
(cid:28)
(cid:12)
(cid:12)
(cid:12)

) and the inner product.

X1kF k

X2kF

k

X1kF k

X2kF ,

k

(B.71)

by linearity of

(
·
A

Note that the bound can be lowered for speciﬁc matrices X1, X2 such that the matrices of their

X2 are in LSm,n(r, s, µ) sets with smaller ranks or sparsities.

sums X1 + X2 and X1 −
Lemma Appendix B.8. Let X j, X j+1, X0 be any matrices in the set LSm,n(r, s, µ) with µ <
√mn
) be an operator whose RICs are suﬃciently upper bounded, then the
, αj ≥
following two inequalities hold
(cid:1)

3r√3s

0, and

(
·
A

(cid:14) (cid:0)

X j
h

−

X0, X j+1

−

X0i −
≤ k

αjhA
I
−

(X j
X0),
−
A
αj AT
QAQk2k

(X j+1
X j

−

X0)
−
i
X j+1
X0kF k

X0kF ,

−

X0kF ,

−

(B.72)

(B.73)

(B.74)

and

αj A
k
where the spectrum of the matrix

X0 −

−

∗

X j

X0

X j
−
αj AT
QAQ

(cid:1)(cid:1)

I

kF ≤ k
Rmn
×

αj AT

QAQk2k
−
mn is bounded as

X j

A
I
(cid:0)

(cid:0)
−
αj (1 + ∆3r,3s,µ)

(cid:0)

1

∈
αj AT
(cid:1)

≤
−
which gives an upper bound on the norm
in (B.74) is larger then the upper bound.

λ

I

I

(cid:0)
−

k

−
αj AT

1

QAQ
≤
1
QAQk2 ≤ |

(cid:1)

−

αj (1

∆3r,3s,µ) ,

−

−
αj (1 + ∆3r,3s,µ)

as the lower bound

|

Proof. We vectorize the matrices on the left hand side of (B.72) using a mapping vec(
·
that stacks columns of a given matrix into a vector and a mapping mat(
·
operators

Rp to the space of matrices of size p

: Rm

mn

n

×

Rmn
) : Rm
) from the space of linear

→

×

n

A

→
x0 = vec (X0) , xj = vec

×

, xj+1 = vec

X j+1

Rmn

∈

A = mat (

) = 

A

(cid:1)
mn.

Rp

×

(cid:0)



∈

(B.75)

(cid:0)

(cid:1)

X j
vec (A1)T
...
vec (Ap)T







Let X0 = U 0Σ0V 0 + S0, X j = U jΣjV j + Sj, X j+1 = U j+1Σj+1V j+1 + Sj+1 be the singular value
Rm
decompositions where the matrices of the left singular vectors are U j
r and their sparse
components are supported at indices Ωj = supp
. Consider the union of the index sets Ω :=
and construct the following frame

Ω0, Ωj, Ωj+1

Sj

∈

×

(cid:8)

(cid:9)

Q = [In ⊗

U
0n,3r
...
0n,3r

U E] = 





0n,3r
U

. . .

(cid:1)

(cid:0)
. . . 0n,3r
. . . 0n,3r
. . .
. . .

U

38

Rmn

×

3(nr+s),

(B.76)

eΩ1

. . .

eΩ3s

∈








Rm

×

∈

3r is formed by concatenating U 0, U j, U j+1 and eΩi

is a vector corresponding to
where U
QT is an
QT Q
a vectorized matrix with a single entry 1 at the index Ωi. Note that PQ = Q
orthogonal projection matrix on the low-rank plus sparse subspace deﬁned by the matrix U and the
index set Ω. Note that by Q being formed by the low-rank plus sparse bases of X0, X j, X j+1 we have
that the projection does not change the vectorized matrices

(cid:1)

(cid:0)

−

1

PQx0 = x0, PQxj = xj, PQxj+1 = xj+1.

To establish the bound in (B.72) we write the left hand side in its vectorized form

T

x0

xj+1

xj

−

x0

−

−

αj

A(xj

−

T

x0)

A(xj+1

x0)

,

−

and replacing A with AQ = APQ in (B.78) using the identities in (B.77) simpliﬁes the term as

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:0)

(cid:1)

(cid:1)

(cid:0)

T

T

(cid:0)

(cid:1)

−

−

−

=
(cid:0)

x0

x0

αj

xj

xj

x0)

xj+1

T
(cid:1)
T
(cid:1)

−
x0

AQ(xj

AQ(xj+1
x0)
−
(xj+1
αj A∗QAQ(xj+1
(cid:0)
(cid:0)
−
−
αj A∗QAQ)(xj+1
xj
xj+1
x0k2 k
X j
X0kF k
−
−
αj A∗QAQk2 is the ℓ2 operator norm of an mn

x0)
x0k2
(cid:1)
X0kF ,
−
mn matrix.

(I
x0
(cid:0)
−
−
αj A∗QAQk2 k
(cid:0)
αj A∗QAQk2 k

−
X j+1

(cid:0)
≤ k
=

xj
(cid:0)

−

−

−

×

−

=

k

(cid:1)

(cid:1)

I

I

x0)

−

x0)

(cid:1)

(cid:1)

where

I

k

−

Similarly we now establish the bound in (B.73)

X j

X0 −

αj A

∗

−

(cid:13)
(cid:13)

X j

X0

−

(cid:0)

A
(cid:0)

F =
=

(cid:1)(cid:1)(cid:13)
(cid:13)

≤

xj

−

x0 + αj AT A
xj
αj AT A
−
αj A∗QAQ

(cid:1) (cid:0)
2

x0 −
x0
(cid:0)
−
X j

2
X0
(cid:1)(cid:13)
(cid:13)

−

−

2

(cid:1)(cid:13)
(cid:13)
F ,

xj

I

(cid:13)
(cid:13)
I
(cid:13)
(cid:0)
(cid:13)
(cid:13)
(cid:13)

where we just vectorized the matrices and the linear operator
using ℓ2-operator norm
self-adjoint, as such its eigenvalues can be bounded using the RICs as done by [30] and by [41]

(cid:13)
(
) and upper bounded the expression
(cid:13)
·
A
αj A∗QAQk2. Matrix AQ acts on a subspace of LSm,n(3r, 3s, µ) and is

(cid:13)
(cid:13)

(cid:13)
(cid:13)

−

k

I

(B.77)

(B.78)

(B.79)

(B.80)

(B.81)

(B.82)

(B.83)

(B.84)

(B.85)

(B.86)

αj (1 + ∆3r,3s,2µ)

1

−

λ

I

≤

−

αj A∗QAQ

1

−

≤

αj (1

−

∆3r,3s,2µ) .

(B.87)

(cid:0)

(cid:1)

39

