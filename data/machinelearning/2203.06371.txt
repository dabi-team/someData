Varying Coeﬃcient Linear Discriminant Analysis for
Dynamic Data

Yajie Bao∗1 and Yuyang Liu†1

1School of Mathematical Sciences, Shanghai Jiao Tong University

October 11, 2022

Abstract

Linear discriminant analysis (LDA) is an important classiﬁcation tool in statistics and ma-
chine learning. This paper investigates the varying coeﬃcient LDA model for dynamic data,
with Bayes’ discriminant direction being a function of some exposure variable to address the
heterogeneity. We propose a new least-square estimation method based on the B-spline ap-
proximation. The data-driven discriminant procedure is more computationally eﬃcient than
the dynamic linear programming rule (Jiang et al., 2020). We also establish the convergence
rates for the corresponding estimation error bound and the excess misclassiﬁcation risk. The
estimation error in L2 distance is optimal for the low-dimensional regime and is near optimal
for the high-dimensional regime. Numerical experiments on synthetic data and real data both
corroborate the superiority of our proposed classiﬁcation method.

1

Introduction

∈

Rp is the covariate and Y

Classiﬁcation is one of the most essential topics in statistics and machine learning, and widely
applied in many scientiﬁc and industrial ﬁelds. Consider a pair of random variables (X, Y ), where
is the label. If Y = 1, the covariate X follows the p-
X
(µ2, Σ).
dimensional multivariate normal distribution
We assume the prior probabilities of two classes are equal, that is P(Y = 1) = P(Y = 0) = 1/2.
For a new random covariate X new, we aim to predict its unknown label Ynew according to some
discriminant rule. If we know the parameters µ1, µ2 and Σ in advance, let µ = (µ1 + µ2)/2, then
the well-known Bayes’ linear discriminant rule is given by

(µ1, Σ), otherwise X is distributed as

0, 1
}

∈ {

N

N

2
2
0
2

t
c
O
0
1

]
E
M

.
t
a
t
s
[

3
v
1
7
3
6
0
.
3
0
2
2
:
v
i
X
r
a

ψ(X new) = I

(X new −
(cid:16)

µ)⊤Σ−

1(µ1 −

µ2)

≥

0

(cid:17)

(1.1)

1(µ1 −

µ2) is called Bayes’ discriminant direction. In real data analysis, a data-driven
µ2 and pooled sample covariance
Σ in (1.1), which is asymptotically optimal when the dimensionality p is ﬁxed (Anderson,

where Σ−
Bayes’ classiﬁcation rule is given by plugging sample means
matrix
1958).

µ1,

b

b

b

Driven by contemporary measurement technologies, high-dimensional data sets have been broadly
collected in classiﬁcation problems. Classical LDA has been proved to perform poorly (no better

∗Email: baoyajie2019stat@sjtu.edu.cn
†Email: d0408x@sjtu.edu.cn

1

 
 
 
 
 
 
1 and µ1 −

than random guessing) in the high-dimensional setting, especially when the dimension is much
larger than the sample size (Bickel and Levina, 2004). To address high-dimensional issue, the
1
sparsity assumption is introduced to LDA. Several proposed methods assumed that both Σ−
µ2 have sparse structures. For example, Shao et al. (2011) used the thresholding pro-
and µ1 −
cedure to estimate Σ−
µ2 separately, then constructed a plug-in sparse Bayes’ linear
discriminant rule. Similar regularized methods can also be found in Guo et al. (2007); Wu et al.
(2009); Witten and Tibshirani (2009), etc.
In addition, some works only assumed Bayes’ linear
discriminant direction Σ−
µ2) is sparse. Cai and Liu (2011) proposed the linear program-
ming discriminant (LPD) rule by directly estimating the product Σ−
µ2) through con-
strained ℓ1 minimization. Recently, Cai and Zhang (2019) proposed an adaptive LPD procedure
that achieved the minimax optimal convergence rate of estimation error and excess misclassiﬁca-
tion risk in high-dimensional case. Mai et al. (2012) estimated the sparse discriminant direction
via a sparse penalized least squares formulation. Mai and Zou (2015) studied high-dimensional
sparse semiparametric discriminant analysis and relaxed the Gaussian assumption. For multiclass
problem, Mai et al. (2019) proposed a sparse discriminant procedure by estimating all discriminant
directions simultaneously.

1(µ1 −

1(µ1 −

Heterogeneous data is widespread in many modern scientiﬁc ﬁelds, such as ﬁnance, biology, and
astronomy (Fan et al., 2014). The prevalent statistical approach to address the heterogeneity is
imposing the dynamic or varying coeﬃcient assumption, where the population means and covari-
ance matrix may vary with some observable exposure variable.
In speciﬁc, Chen et al. (2019);
Chen and Leng (2016); Wang et al. (2021) investigated the dynamic covariance model in the high-
dimensional regime. Under the dynamic setting, Bayes’ discriminant direction is a function of
the exposure variable. Consequently, classical plug-in Bayes’ discriminant rule will deteriorate in
analyzing non-static data, and thus leads to unsatisfactory performance. To address the dynamic
data, Jiang et al. (2020) proposed the dynamic linear programming discriminant (DLPD) rule by
assuming µ1, µ2 and Σ are functions of some q-dimensional random covariate U . To estimate the
sparse Fisher’s linear discriminant direction function β∗(u) = Σ−
µ2(u)) given U = u,
Σ(u). Then
they ﬁrst used the Nadaraya-Watson method to obtain estimators
they estimated β∗(u) using the linear programming approach Cai and Liu (2011); Candes and Tao
(2007):

1(u)(µ1(u)
µ1(u),

−
µ2(u) and

b

β(u) = arg min
Rp

β

∈

β
k
n

k1 subject to

Σ(u)β
|

−

µ1(u)
(

−

λn

,

(1.2)

o

b

µ1(U new),

where λn is a tuning parameter. However, this classiﬁcation procedure is computationally expensive
b
for large scale prediction problem. For each new observation (X new, U new), DLPD method needs
Σ(U new) and re-solve the corresponding large scale linear
to re-estimate
In addition, the support set of discriminant direction β∗(u) decides which
programming (1.2).
variable contributes to classiﬁcation but (1.2) can not provide a invariant support set since it is a
point-wise estimator. In some real applications, the varying support set of discriminant direction
in DLPD method may lack interpretability.

µ2(U new) and

b

b

b

b

b

b

b
µ2(u))

|∞ ≤

The dynamic discriminant analysis shares the same semi-parametric spirit with the classical varying
coeﬃcient model (Hastie and Tibshirani, 1993), where the unknown parameters are assumed to be
a smooth function of the exposure variable. In the past decades, the varying coeﬃcient method has
been applied to a variety of statistical models, such as linear regression model (Hoover et al., 1998;
Fan et al., 1999), generalized linear model (Cai et al., 2000; Fan and Zhang, 2008), quantile regres-
sion (Honda, 2004; Wang et al., 2009) and support vector machine (Lu et al., 2018), etc. Motivated
by the least square form of Bayes’ discriminant direction, we propose a new estimation method for
the discriminant direction function based on B-spline approximation, which can be applied in the

2

classiﬁcation for dynamic data. In high-dimensional regime, we can estimate the approximation
coeﬃcient by solving a penalized least square problem. The computational drawback of the DLPD
rule (Jiang et al., 2020) is circumvented in our developed varying coeﬃcient discriminant proce-
dure. For each new observation, we only need to re-compute the B-spline basis vector. Hence it
has a signiﬁcant computational advantage over the DLPD rule. In the high-dimensional case, the
support set of our proposed estimator is irrelevant with the value of exposure variable, which is
indeed helpful to select important features contributing to classiﬁcation.

The remainder of this paper is organized as follows. In section 2, we propose a new discriminant di-
rection function and its varying coeﬃcient estimators in both low-dimensional and high-dimensional
regimes. In section 3, we establish the upper bounds for the estimation error and uniform excess
misclassiﬁcation risk for our proposed varying coeﬃcient LDA procedure. In section 5 and 6, we
verify the performance of our method through simulations on synthetic data and real data respec-
tively.

j

∈

|∞

P

p |

k1 =

p
j=1 |

xj|
⊆ {

k2 = (

2)1/2 and
1, 2, ..., p

to denote the ℓ1, ℓ2 and ℓ

≤
≤
P
Rpq, we write sub-vector b(j) = (b(j

x
|
, we use xS to denote the sub-vector (xj : j
}

Notations. We deﬁne some notations that will be used throughout the paper. For two real
positive sequences an and bn, we write an . bn if there exists some positive constant m such that
Rp, we use
bn if an . bn and bn . an. For a real-valued vector x
mbn. And we write an ≍
an ≤
p
x
x
= max1
,
xj|
j=1 |
k
k
norm respectively. For a subset S
Specially, for a vector b
And for a subset S
⊆ {
real-valued matrix A
∈
maximal entry in absolute value is denoted by
}
B to
and T
denote the Kronecker product on two matrices A and B with proper sizes. Specially, for a matrix
, we write the group sub-matrix as A(ST ) = (Aij)
A
}
∈
for i
. For a sequence of real
T
(k
∈ {
}
−
∈ {
−
random variables Xn, we write Xn = OP(an) if for any ǫ > 0, there exists some constant C > 0
such that P(
Xn|
|

q, ℓ2 (spectral) norm is deﬁned by
A
k
Aij|
= maxi,j |
, we write sub-matrix AST = (Aij) for i
∈
}

∞
S).
, bjq)⊤ for j = 1, 2, ..., p.
S). For a
, the
|
1, 2, ..., p

, we use b(S) to denote the group sub-vector (b(j) : j
}

k2 = sup
x
2=1,
k
. For two subsets S
S and j

pq and two subsets S, T
1)q + 1, ..., kq : k

∈
x⊤Ay
2=1 |
k

xj|
1)q+1,

1, 2, ..., p
Rp

1)q + 1, ..., kq : k

T . We use A

> Can) < ǫ.

⊆ {
S
}

1, 2, ..., p

1, 2, ..., q

and j

A
|

×
(k

⊆ {

⊆ {

Rpq

· · ·

|∞

⊗

∈

∈

∈

∈

∈

×

−

y

k

k

2 Varying coeﬃcient LDA via B-spline approximation

−

∼ N

[0, 1], we assume X

In this section, we provide a detailed description of the varying coeﬃcient linear discriminant
(µ1(u), Σ(u))
rule. Given the univariate exposure variable U = u
(µ2(u), Σ(u)) if Y = 0, then Bayes’ discriminant direction is β∗(u) =
if Y = 1 and X
1(u)(µ1(u)
Σ−
µ2(u)). We also denote the pooled mean as µ(u) = π1µ1(u) + π2µ2(u), where
π1 = P(Y = 1) and π2 = P(Y = 0). To introduce our new discriminant direction function, we
deﬁne a new response variable as Z = π2 if Y = 1 and Z =
In addition, the
exposure variable U is assumed to be independent with the label Y . Motivated by the least square
µ2) in static setting (Anderson, 1958;
form of the plug-in Bayes’ discriminant direction
Mai et al., 2012), we propose a new discriminant direction function θ∗(U ) = (θ∗1(U ),
, θ∗p(U ))⊤
as the minimizer of the following population least square problem

π1 if Y = 0.

µ1 −
(

∼ N

Σ−

· · ·

−

∈

b

1

b

b

θj(U )

min
∈L

2(

P

p

Xj=1

E

)

Z





−





2

θj(U )(Xj −

µj(U ))



U

(cid:12)
(cid:12)



,





3

(2.1)

P

is the joint distribution of (X, Z, U ) and

.
where
P
It is worthwhile noting that the representation (2.1) is similar to the approximation of coeﬃcient
function in the varying coeﬃcient linear model. Then the discriminant direction function θ∗(U )
satisﬁes

) denotes the L2 space under measure

2(

P

L

−
A further computation gives rise to the following closed form,

−

−

n

µ(U ))⊤θ∗(U ))
U
|

= 0.

o

E

(X

µ(U ))(Z

(X

θ∗(U ) = π1π2Σ−

1(U )(µ1(U )

µ2(U ))[1

(µ1(U )

−

−

−

µ2(U ))⊤θ∗(U )].

If the population covariance matrix Σ(U ) is positive deﬁnite and µ1(U )
guaranteed that (µ1(U )
function satisﬁes that

= 0, we are
(0, 1). As a consequence, Bayes’ discriminant direction

µ2(U ))⊤θ∗(U )

µ2(U )

−

−

∈

β∗(U ) = Σ−

1(U )(µ1(U )

µ2(U )) = c∗(U )θ∗(U ),

−

(2.2)

where c∗(U ) = 1/[π1π2−
new observation (X new, Unew), we deﬁne the oracle varying coeﬃcient discriminant rule as

µ2(U ))⊤θ∗(U )]. For the equal-prior case (π1 = π2), given any

π1π2(µ1(U )

−

ψ(X new, Unew) = I

(X new −

µ(Unew))⊤θ∗(Unew)

≥

0

.

(2.3)

(cid:16)

(cid:17)

Recall that c∗(u) > 0, the classiﬁcation result of (2.3) is consistent with using Bayes’ discriminant
direction β∗(Unew).

2.1 Approximation of discriminant direction function

Let B(
), ..., BLn (
) = (B1(
)) be the scaled B-spline basis of the polynomial splines space, which
·
·
·
Ln
) = √Ln. According to the B-spline approximation theory
k=1 Bk(
satisﬁes that Bk(
0 and
)
·
·
(De Boor, 1978), under some regular conditions, each coordinate of discriminant direction θ∗(u)
P
RLn is the approximation coeﬃcient. If
can be approximated by θ∗j (u)
µ1(u) and µ2(u) are known, the “best” approximation coeﬃcients in population form is deﬁned
as

γ⊤(j)B(u), where γ(j) ∈

≈

≥

2

γ(1),
(

,

· · ·

γ(p)) = arg min

E

Z

−





(Xj −

µj(U ))γ⊤j B(U )



.



(2.4)

Let

γ = (

γ⊤(1),

e
γ⊤(p))⊤ and

,

e

· · ·

B(U ) = (X

−



µ(U ))

⊗


B(U ), it is easy to show that



e

e

e

e
γ =

B(U )

B(U )⊤]

1

−

E[

B(U )Z].

For any u

∈

[0, 1], we may write the approximated discriminant direction as

e

e

e

θ(u) =

γ⊤(1)B(u),
(cid:16)

,

· · ·

γ⊤(p)B(u)
(cid:17)

⊤ .

Therefore, the data-driven discriminant procedure boils down to estimate the approximation co-
eﬃcient
In the following
subsections, we consider the equal-prior case, that is π1 = π2 = 1/2. And we provide the extension
of our method to unbalanced case in Section 4.1.

γ and the mean functions µ1(u), µ2(u) based on collected samples.

e

e

e

RLn ,
γ(j)∈
p
j
1
≤

≤

E[
(cid:16)

e

e

p

Xj=1

(cid:17)

4

6
2.2 Data-driven discriminant procedure

(X i, Ui, Yi) : i = 1, 2, ..., 2n
{

be an i.i.d. sample set. We denote the pseudo response variable
Let
by Zi = I(Yi = 1)
1
2 for i = 1, 2, ..., 2n and denote the value of B-spline basis taken at Ui by
Bi = (B1(Ui), ..., BLn (Ui))⊤. Without loss of generality, we assume the sample size of two classes
are equal. The sample index sets of two classes are
with
|I1|

i : Yi = 1
}
{

i : Yi = 0
}
{

I1 =

I2 =

|I2|

= n.

and

−

=

}

2.2.1 Classical low-dimensional regime.

To construct the sample form of problem (2.4), we start with estimating the mean functions
µ1(u) and µ2(u). By the B-spline theory, we may estimate the mean functions by
µl(u) =
α⊤l1B(u),
(

α⊤lpB(u))⊤ for l = 1, 2, where

,

· · ·

1

−

BiXij,

for j = 1, 2, ..., p.

b

b

b

αlj =

µ1(u) +

µ(u) = (

Let
following least-square problem
b

b

b



BiB⊤i 


Xi
∈Il

Xi
∈Il
b
µ2(u))/2, the estimators (



γ(1),

,

γ(p)) can be obtained by solving the

· · ·

b

p

b

2

1
2n

2n

Xi=1

min

RLn ,
γ(j)∈
p
j
1
≤

≤

Zi −





(Xij −

Xj=1

.

µj(Ui))B⊤i γ(j)

Bi = (X i −
e

b
,

(2.5)

µ(Ui))

Bi. In

⊗

b

(2.6)

γ⊤(p))⊤ and
With slightly abusing notations, we denote
low-dimensional regime, the problem (2.5) has a closed form solution

γ = (

γ⊤(1),

· · ·

b

2n

b

−

1 2n

b

e
2.2.2 Sparse high-dimensional regime.

b

e

γ =

Bi

B⊤
i

!

Xi=1

BiZi.

Xi=1

e

In the high-dimensional case, we assume Bayes’ discriminant function β∗(u) is sparse with the sup-
port set S :=

= s. Without loss of generality, let S =

and

j : E[
2] > 0
β∗j (U )
}
|
|
{

S
|

|

1, ..., s
{

.
}

Since θ∗(u) has the same support set with β∗(u), the “best” coeﬃcients for approximating θ∗j (u)
for j

S are deﬁned as

∈

γ(1),
(

,

· · ·

e

Consequently, for any u
by

e

∈

γ(s)) = arg min

RLn ,
γ(j)∈
s
j
1
≤

≤

s

Xj=1

Z

−

E









(Xj −

µj(U ))γ ⊤(j)B

2

.









(2.7)

[0, 1], we shall approximate the discriminant direction function θ∗(u)

Let D = E[
B(U )
alently written as

e

θ(u) =

γ⊤1 B(u),
(cid:16)
B(U )⊤] and b = E[
e
e
γ⊤s , 0⊤,
γ⊤1 ,
γ = (
e
e

· · ·

· · ·

e

e

e

,

,

γ⊤s B(u), 0,

, 0

⊤ .

· · ·

· · ·

(cid:17)
B(U )Z], the approximation coeﬃcient vector can be equiv-
1
(SS)b(S) and
γ⊤(Sc))⊤ where

e
, 0⊤)⊤ = (

γ(S) = D−

γ⊤(S),

5

e

e

e

 
s)Ln. It means that the estimator of approximation coeﬃcient

γ(Sc) = 0(p
γ should have group
sparsity structure. Therefore, we add the group lasso penalty (Yuan and Lin, 2006) to the objective
function in (2.5), and then obtain the estimators (
e

γ(p)) by solving

γ(1),

e

−

,

· · ·

min

RLn ,
γ(j)∈
p
j
1
≤

≤

1
2n

2n

Xi=1

p

Xj=1

Zi −





(Xij −

b

b

µj(Ui))B(Ui)⊤γ(j)


b

2

+ λn

p

Xj=1

γ(j)k2
k

(2.8)

where λn is a tuning parameter. After some simpliﬁcations, the problem (2.8) is equivalent to the
following quadratic programming form

γ = arg min

γ

∈

RpLn 


1
2

γ⊤Dnγ

b⊤n γ + λn

−

p

Xj=1

,

γ(j)k2
k


(2.9)

where

b

Dn =

1
2n


2n

Xi=1

Bi

B⊤
i ,

bn =

1
2n



2n

BiZi.

Xi=1

e

The problem (2.9) can be eﬃciently solved by several well studied optimization methods, such as
group coordinate descent algorithm and iterative shrinkage thresholding algorithm (ISTA) (Beck and Teboulle,
2009). We provide a detailed description about ISTA to solve (2.9) in Appendix E.

e

e

2.2.3 Discriminant rule and asymptotic optimality.

After obtaining

µ(u) and

γ, the estimator of discriminant direction function is given by

b

b

θ(u) =

γ⊤(1)B(u),
(cid:16)

,

· · ·

γ⊤(p)B(u)
(cid:17)

⊤ .

(2.10)

For any new observation (X new, Unew), the data-driven varying coeﬃcient linear discriminant rule
is

b

b

b

ψ(X new, Unew) = I

µ(Unew))⊤

θ(Unew)

For any u

[0, 1], the optimal misclassiﬁcation risk of oracle rule (2.3) is R(u) = Φ(

b

∈

where ∆(u) =
function of a standard normal random variable. Given the samples and u
misclassiﬁcation risk of data-driven rule (2.11) is

µ2(u))⊤Σ−

(µ1(u)

1(u)(µ1(u)

q

−

−

∈

(X new −
(cid:16)

≥

0

.

(cid:17)

(2.11)

b

b
µ2(u)) and Φ(
) is the cumulative distribution
·
[0, 1], the conditional

∆(u)/2),

−

Rn(u) :=

1
2

Φ



µ(u)
(

−

µ1(u))⊤

θ(u)

θ⊤(u)Σ(u)

θ(u)
b



b
q
µ2(u)) /2 and ¯Φ(
) = 1
b
b
·

+



¯Φ

1
2



µ(u)
(

−

µ2(u))⊤

θ(u)

θ⊤(u)Σ(u)

θ(u)
b





b
q

,





µ(u) = (

µ1(u) +

where
). Through utilizing the technique developed in
Φ(
·
Cai and Zhang (2019), we have the following proposition to provide an upper bound for the excess
misclassiﬁcation risk.

−

b

b

b

b

b

Proposition 2.1. Suppose that for any u
k2 is uniformly upper bounded from inﬁnity
µ1(u)
k2 = o(1),
and ∆(u) is uniformly lower bounded away from zero. In addition, if
k
µ2(u)
[0, 1]
k2 = o(1), we have for any u
∈
k
b
2
µ(u)
θ∗(u)
(
2 +
k

k2 = o(1) and
Rn(u)
|

2.
µ(u))⊤ β∗(u)
|

θ(u)
k
R(u)
b
|

Σ(u)
k

θ(u)
k

µ1(u)

µ2(u)

θ∗(u)

(2.12)

[0, 1],

−

−

−

−

−

−

.

∈

b

|

b

6

b

3 Theoretical results

In this section, we will present the estimation error bounds and the convergence rates of excess
misclassiﬁcation risk of our proposed varying coeﬃcient LDA procedure in both low-dimensional
regime and high-dimensional regime. Specially, for two function vectors ν(
))⊤
), ..., νm(
) = (ν1(
·
·
·
)) mapping from [0, 1] to Rm, we deﬁne the L2 distance between ν(
and ξ(
) and
), ..., ξm(
) = (ξ1(
·
·
·
·
ξ(
) as
·

ν
k

ξ

kL2 =

−

1

ν(u)

0 k

(cid:18)Z

2
ξ(u)
2du
k

−

1
2

.

(cid:19)

3.1 Classical low-dimensional regime

Before presenting the convergence rates of our proposed estimator, we introduce the following
necessary technical assumptions for the clarity of ensuing theoretical results.
(C1) There exist two constant 0 < λ0 ≤

such that for any u

λ1 <

[0, 1]

∈

∞
λmin (Σ(u))

λ0 ≤

λmax (Σ(u))

λ1,

≤
where λmin (Σ(u)) and λmax (Σ(u)) are respectively the minimum and maximum eigenvalues
of Σ(u).

≤

(C2) The density function h of U satisﬁes that 0 < D1 ≤
∈
(C3) Each entry of functions µ1(u), µ2(u) and Σ(u)−

D1 and D2 and any u

[0, 1].

h(u)

≤

D2 <

∞

for two positive constants

1(µ1(u)

µ2(u)) belongs to the following

−

function space

d([0, 1]) :=

W

f : [0, 1]
{

→

R, sup
x |

f (ℓ)(x)

| ≤

D for ℓ = 0, 1, ..., t and

f (t)(x)

sup
′ |
x,x

′

f (t)(x

)
| ≤

L

x
|

−

′

x

r
|

}

−

where d = r + t

(C4) Assume supu

≥
[0,1] max
∈
In addition, p = o(n(2d

µ1(u)
−
1)/(2d+1)).

{k
−

1 and f (s) denotes the s-th derivative of function f and f (0) = f .

µ2(u)

k2,

θ∗(u)
k

k2}

= δp ≤

M for some large constant M .

Assumption (C1) is very common in high-dimensional linear discriminant analysis literature (Mai et al.,
2012; Cai and Liu, 2011; Jiang et al., 2020). Assumption (C2) and (C3) are regular conditions in
B-spline approximation theory, similar assumptions also appeared in (Xue and Qu, 2012; Fan et al.,
θ∗(u)
2014). For the simplicity of convergence rates, we assume
k2 are both
k2 and
k
uniformly bounded in (C4). The condition on the dimensionality ensures that Ln
p log n/n = o(1)
n1/(2d+1), which guarantees the optimality of our
under the optimal length of B-spline basis Ln ≍
proposed estimator.

µ1(u)
k

µ2(u)

p

−

θ∗

θ
k

Note that L2 error of our proposed estimator can be decomposed into two parts: the approximation
kL2. Our ﬁrst result shows that the approximation
error
error shrinks as the length of spline basis vector Ln grows, which also attains the optimal conver-
e
gence rate of classical B-spine approximation error (see Huang (2003); Schumaker (2007)). The
proof of Theorem 3.1 is given in Appendix B.2.

kL2 and the estimation error

θ
k

−

−

θ

e

b

7

Theorem 3.1. Assume the assumptions (C1)-(C4) hold, then the approximation error in L2 dis-
tance is bounded by

θ
k

−

θ∗

d
kL2 . √pL−
n .

(3.1)

The following theorem provides the upper bound of estimation error for the discriminant direction
function estimator (2.10). Compared with the analysis in the varying coeﬃcient linear model, the
theoretical development in this paper is more challenging. The reason is two-fold:

e

• There is no direct relation between the pseudo response variable Zi and the covariate X i. The
D
empirical processes in the proof are established upon ﬁne-grained decomposition to Dn −
(see Appendix C.3).

• The estimator for approximation coeﬃcient

γ in (2.5) involves the mean function estimators
µ1 and
µ2 computed from the same samples. We utilize chaining technique to establish
several concentration inequalities on the operator norm of matrices and ℓ2 norm of matrix-
b
vector-products (see Lemma C.4-C.5).
b

b

The proof of Theorem 3.2 is deferred to Appendix B.3.

Theorem 3.2. Assume conditions (C1)-(C4) hold. Let an =
error in L2 distance is bounded by

p

Ln log n/n + L−

n , the estimation

d

θ
k

θ

kL2 = OP

−

pLn log n
n

 r

+ anpLn

log n
n

r

+ √pL−
n

d

.

!

(3.2)

b

e

Remark 3.1. Together with the approximation error in Theorem 3.1 and assumption (C4), if we
take the length of B-spline vector as Ln ≍
2(d+1) , it is easy to see that the L2 error can be
bounded by

(n/ log n)

1

θ
k

θ∗

kL2 = OP

−

√p

log n
n

(cid:19)

(cid:18)

d
2d+1

.

!

(3.3)

According to Stone (1982), the minimax convergence rate for one-dimensional function in function
d/(2d+1). Apparently, our proposed estimation procedure is optimal up to a
space
logarithmic factor.

d([0, 1]) is n−

W

b

From assumptions (C1) and (C4), we know
sition A.1, we establish the uniform bound for the mean function estimator, that is

2 .
µ(u))⊤ β∗(u)
|

µ(u)
k

µ(u)

−

−

(

|

2
µ(u)
2. In Propo-
k

µ(u)

sup
[0,1] k
u
∈

−

b
k2 = OP

µ(u)

pLn log n
n

 r

+ √pL−
n

b
d

!

.

b
In addition, we also have supu
same bound with (3.2) (see Appendix B.3) and
obatin the L2 bound of the excess misclassiﬁcation risk in the following corollary.
b

γ
k2 shares the
k
√Ln. In conjunction with (3.3), we can
p

k2 = OP(Ln
B(u)
k2 ≤
k

p log n/n) since

[0,1] k
∈

θ(u)

θ(u)

−

−

γ

e

b

e

Corollary 3.1. Under the same settings of Theorem 3.2, we assume ∆(u)
1
constant c and take Ln ≍

2(d+1) , then it holds

(n/ log n)

c > 0 for some

≥

Rn −
k

R

kL2 = OP

log n
n

p

(cid:18)

(cid:19)

2d
2d+1

.

!

8

 
 
3.2 Sparse high-dimensional regime

The following assumption plays a similar role as condition (C4) in low-dimensional regime.
µ2(u)S k2,

(C5) Assume supu

= δs ≤

θ∗(u)
k

M for some large constant M .

k2}

[0,1] max
∈
In addition, s = o(n(2d

(µ1(u)
−
1)/4(d+1)).

{k
−

The approximation error bound under sparse setting is presented in the following theorem, which
) =
can be easily obtained by tracing the proof of Theorem 3.1 since θ∗j (
·
Theorem 3.3. Assume the assumptions (C1)-(C3) and (C5) hold, then the approximation error
in high-dimensional case is

) = 0 for j
θj(
·

Sc.

∈

e

θ
k

−

θ∗

d
kL2 . √sL−
n .

Below we provide the estimation error bound for the group-sparse estimator in (2.9), and the proof
is deferred to Appendix B.4.

e

Theorem 3.4. Assume conditions (C1), (C2), (C3) and (C5) hold, let an =
for any ϑ > 0, if we take

λn ≥

C

 r

Ln log p
n

+ anLns

log p
n

r

+ √sL−
n

d

!

for some suﬃciently large positive constant C, then

Ln log n/n + L−

d
n ,

p

(3.4)

holds with probability at least 1

θ
k
ϑ

b
−

Lnp−

−

θ

kL2 . √sλn
ϑs.

−
Lnp−
e

Remark 3.2. To interpret the orders in (3.4), we introduce the crucial quantity in the proof of
Theorem 3.4: max1

(Dn)(jS)

j

≤

≤

p k
(Dn)(jS)
k

(bn)(j)k2, which can be bounded by
(bn −
k

γ(S) −
(Dn −
(bn)(j)k2 ≤ k
e
D(jS)
+
k

D)(jS)
γ(S) −

γ(S)k2 +
b(j)k2.
e

γ(S) −

e

b)(j)k2

(3.5)

6∈

≤

≤

S, despite the fact D(ScS)

p, the ﬁrst two terms in (3.5) can be bounded by

For any 1
j
through concentration. For j
For j
θ(u))S k2 (see Appendix B.4), which is exactly the last term √sL−
e
If we set the length of B-spline basis vector as Ln ≍
e
estimator

γ(S) −
b(Sc) 6
e

S, D(jS)
γ(S) −

(ns/ log p)

Ln log p/n + anLns

log p/n
b(j) = 0 holds due to the deﬁnition of
γ in (2.7).
p
p
= 0, we can still show that it is bounded by
(θ∗(u)
k
d
e

n in (3.4).

2d+1 , the L2 error of the group-sparse

−

∈

e

1

θ(
) will be
·

b

θ
k

θ∗

kL2 = OP

−

d+1
2d+1

s

log p
n

(cid:18)

(cid:19)

d
2d+1

.

!

(3.6)

b

2d

Compared with the oracle minimax rate √sn−
due to the bias D(ScS)
risk, it suﬃces to control the upper bound of
µ1(u)
e
with condition (C5), we have
corollary is a direct result of (3.6) and Proposition A.1.

2d+1 , there is an additional factor s1/(2d+1) in (3.6)
b(Sc). To obtain the convergence rate for the excess misclassiﬁcation
2. Recall the fact Σ(u)β∗(u) =
µ(u))⊤β∗(u)
|
1(µ1(u)
µ2(u), then simple algebra shows that β∗S(u) = (ΣSS(u))−
µ2(u))S . Combining
−
2
µ(u)S k
2. Then the following

2 .
µ(u))⊤β∗(u)
b
|

µ(u)
(
k

γ(S) −

µ(u)
(
|

µ(u)
(
|

−

−

−

−

b

b

9

 
Corollary 3.2. With the same conditions and choice of λn in Theorem 3.4, if we take Ln ≍
(ns/ log p)

2d+1 , the excess misclassiﬁcation risk of

θ satisﬁes that

1

Rn −
k

R

kL2 = OP

b
2d+2
2d+1

s

log p
n

(cid:19)

(cid:18)

2d
2d+1

.

!

4 Extensions

This section will generalize our approach to more general classiﬁcation problems in dynamic
data.

4.1 Binary classiﬁcation with unequal prior

For general static binary classiﬁcation problem, Bayes’ discriminant rule is given by

ψ(X) = I

(X

(cid:18)

µ)⊤Σ−

1(µ1 −

−

µ2) + log

π1
π2 ≥

0
(cid:19)

(4.1)

where π1 = P(Y = 1), π2 = P(Y = 0) and µ = π1µ1 +π2µ2. In varying coeﬃcient regime, according
to (2.2), (4.1) can be generalized to the following form

ψ(X, U ) = I

(X

(cid:18)

µ(U ))⊤c∗(U )θ∗(U ) + log

−

π1
π2 ≥

,

0
(cid:19)

(4.2)

N
i=1

I(Yi = 1)/N and

µ2(U ))⊤θ∗(U )]. The prior probabilities can be estimated by
where c∗(U ) = 1/[π1π2 −
I(Yi = 0)/N , where N is the total sample size. To estimate
π1 =
θ∗(u), we only need to set Zi =
π1 if Yi = 0 in (2.8). As a consequence, for
any new observation (X new, Unew), we can perform varying coeﬃcient discriminant rule by plugging
b
in corresponding estimators into (4.2).

π1π2(µ1(U )
−
N
π2 =
i=1

π2 if Yi = 1 and Zi =

P

P

−

b

b

b

4.2 Multivariate Exposure Variable

∈

Rm, we may consider the following single-index extension. Specially, given

For multivariate U
U = u, we assume the covariate X
if Y = 0. Then Bayes’s discriminant direction is also a function of u⊤ϕ∗, that is θ∗j (u) = g∗j (u⊤ϕ∗)
for j = 1, ..., p, where g∗j (
) is a smooth univariate function. Similar to (2.1), g∗j s are deﬁned as the
·
solution of the following least-square problem

(µ1(u⊤ϕ∗), Σ(u⊤ϕ∗)) if Y = 1 and X

∼ N

∼ N

(µ2(u⊤ϕ∗), Σ(u⊤ϕ∗))

min
P

gj

∈L

2(

),j=1,...,p

p

−

Xj=1

gj(U⊤ϕ∗)(Xj −

µj(U⊤ϕ∗))



2



E

Z









U

.





(cid:12)
(cid:12)
(cid:12)

If ϕ∗ is known, we can approximate the function g∗j (U⊤ϕ∗) by γ⊤j B(U⊤ϕ∗). And the optimal
approximation coeﬃcients are deﬁned as

min

RLn ,j=1,...,p

γj ∈

E

Z





−





p

(Xj −

Xj=1

10

2

µj(U⊤ϕ∗))γ⊤j B(U⊤ϕ∗)



.







 
As for the initial estimator of ϕ, according to our assumption, we can equivalently write the
covariate X i as the form of the standard single index model

1/2

X i = µ1(U ⊤i ϕ∗) +

Σ(U ⊤i ϕ∗)
(cid:16)
(cid:17)
Σ(U ⊤i ϕ∗)
(cid:17)
(cid:16)
(0, Ip). We may utilize the method proposed in Xia (2006) to obtain the estimator of
ϕ, the estimators of univariate functions g∗j s can be estimated

X i = µ2(U ⊤i ϕ∗) +

if Yi = 0,

if Yi = 1,

1/2

ǫi

ǫi

where ǫi ∼ N
ϕ∗, denoted by
by the B-spline procedure in our paper.

ϕ. By plugging in

b

b

5 Numerical experiments

This section investigates the numerical performance of the proposed varying coeﬃcient discriminant
procedure. In our simulation study, we only consider the balanced case where the sample sizes of
the two classes are equal.

The exposure variable Ui for i = 1, 2, ..., 2n are generated independently from uniform distribution
on [0, 1] in the following experiments. After generating Ui, we sample the covariate X i with Yi = 1
(µ2(Ui), Σ(Ui)) for
from
i = n + 1, ..., 2n, where µ1(u) = 0 and µ2(u) = Σ(u)β(u). Several combinations of β(u) and Σ(u)
are considered in our simulation. Each entry of Bayes’ discriminant direction function take values
as:

(µ1(Ui), Σ(Ui)) for i = 1, ..., n and sample X i with Yi = 0 from

N

N

• Direction 1: β(1)

j

(u) = 1 for 1

• Direction 2: β(2)

j

(u) = u for 1

j

j

≤

≤

≤

≤

p (or s);

p (or s);

• Direction 3: β(2)

j

(u) = sin(4u) for 1

j

≤

≤

p (or s);

• Direction 4: β(4)

(u) = eu for 1

j

≤
In high-dimensional case, we set βj(
) = 0 for s + 1
·
considered in our simulations:
• Covariance matrix 1, σ(1)

|, for 1

i,j (u) = 0.5|

≤

−

j

i

j

p (or s).

i, j

p;

≤

≤

j

≤

≤

p. Three covariance matrices are

• Covariance matrix 2. σ(2)

i,j (u) = u|

j

i

−

|, for 1

i, j

p;

≤

≤

• Covariance matrix 3. σ(3)

i,j (u) = uI(i

= j) + I(i = j) for 1

i, j

p.

≤

≤

The combination of Direction 1 and Covariance matrix 1 is a classical static setting, where each
entry of the mean vector and covariance matrix is a constant value. The other combinations are
dynamic settings. We use the cubic spline in our simulation, and select the number of spline basis
functions by 5-fold cross-validation. We compute the misclassiﬁcation risk based on an indepen-
dently generated test set with size 200.

5.1 Low-dimensional case

For low-dimensional case, the sample size of each class is ﬁxed as n = 100 and the dimensionality
. The proposed method in this paper (abbreviated as VCLDA) is
5, 10, 20
p is varying from
}
{

11

6
Table 1: Misclassiﬁcation risk and its standard error (in parentheses) of each method in low-
dimensional case.

p Σ Orcale

VCLDA

LDA

Orcale

VCLDA

LDA

5

10

20

5

10

20

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

1
2
3

0.048
0.055
0.039

0.005
0.014
0.004

0.000
0.002
0.000

0.194
0.192
0.173

0.125
0.117
0.092

0.083
0.077
0.054

β(1)

0.075(0.021)
0.078(0.021)
0.058(0.020)

0.028(0.014)
0.038(0.015)
0.027(0.012)

0.020(0.012)
0.041(0.019)
0.042(0.017)

β(3)

0.234(0.034)
0.234(0.033)
0.209(0.028)

0.186(0.027)
0.183(0.030)
0.154(0.031)

0.189(0.031)
0.200(0.037)
0.176(0.037)

0.050(0.016)
0.119(0.028)
0.093(0.023)

0.006(0.006)
0.152(0.025)
0.104(0.023)

0.000(0.001)
0.215(0.034)
0.117(0.025)

0.317(0.034)
0.382(0.047)
0.351(0.047)

0.281(0.038)
0.437(0.046)
0.353(0.051)

0.286(0.039)
0.476(0.041)
0.391(0.059)

0.227
0.221
0.202

0.155
0.163
0.126

0.108
0.125
0.081

0.010
0.025
0.018

0.001
0.007
0.003

0.000
0.001
0.000

β(2)

0.259(0.039)
0.249(0.034)
0.221(0.032)

0.193(0.033)
0.198(0.029)
0.155(0.031)

0.157(0.032)
0.182(0.034)
0.128(0.024)

β(4)

0.024(0.012)
0.040(0.015)
0.033(0.014)

0.011(0.008)
0.027(0.014)
0.022(0.013)

0.014(0.009)
0.041(0.023)
0.044(0.022)

0.255(0.032)
0.272(0.032)
0.245(0.030)

0.197(0.029)
0.270(0.035)
0.212(0.026)

0.166(0.028)
0.312(0.035)
0.222(0.029)

0.029(0.013)
0.156(0.025)
0.137(0.023)

0.016(0.009)
0.195(0.029)
0.157(0.023)

0.011(0.008)
0.246(0.035)
0.172(0.029)

deployed to the generated data. For comparison, we also conduct the following two classiﬁcation
rules:

1. Oracle: use the population Bayes’ discriminant direction Σ(u)−

classiﬁcation.

1(µ1(u)

−

µ2(u)) to conduct

2. LDA: use the static estimators of mean vectors and covariance matrix, i.e., the sample means

and sample covariance matrix, to compute discriminant direction.

We report the averaged misclassiﬁcation risks computed from the test set in Table 1. The oracle
classiﬁcation rule is the most accurate among all procedures. In a static setting, we can see that LDA
achieves nearly oracle performance. As we expected, the performance of LDA procedure degrades
drastically in the dynamic case. Meanwhile, the misclassiﬁcation risk of VCLDA is signiﬁcantly
lower than LDA, and very close to the oracle procedure in all dynamic settings.

5.2 High-dimensional case

In high-dimensional simulation, we ﬁx the sample size of each class as n = 100 and consider the
dimensionality p = 100 and p = 200. Moreover, the sparsity under each dimensionality varies in
. For comparison, we also conduct the oracle rule, the static LPD rule (Cai and Liu,
5, 10, 20
{
}
2011) and DLPD rule (Jiang et al., 2020) in the test set. The misclassiﬁcation risks and their stan-
dard errors under four discriminant direction functions are summarized in Table 2-5 respectively.

12

Table 2: Misclassiﬁcation risk and its standard error (in parenthesis) of each method under Direction
1 in high-dimensional case.

s Σ Orcale

VCLDA

LPD

DLPD

Orcale

VCLDA

LPD

DLPD

p = 100

p = 200

5

10

20

1
2
3

1
2
3

1
2
3

0.048
0.055
0.039

0.005
0.014
0.004

0.000
0.002
0.000

0.076(0.019)
0.070(0.017)
0.060(0.017)

0.015(0.009)
0.041(0.017)
0.012(0.009)

0.009(0.006)
0.018(0.010)
0.009(0.007)

0.053(0.012)
0.195(0.154)
0.332(0.192)

0.103(0.193)
0.152(0.028)
0.123(0.026)

0.055(0.157)
0.208(0.067)
0.123(0.022)

0.053(0.015)
0.202(0.035)
0.085(0.018)

0.101(0.187)
0.168(0.034)
0.040(0.019)

0.055(0.157)
0.176(0.030)
0.026(0.013)

0.048
0.056
0.039

0.005
0.014
0.004

0.000
0.002
0.000

0.071(0.018)
0.067(0.017)
0.060(0.017)

0.025(0.012)
0.043(0.019)
0.016(0.011)

0.004(0.005)
0.014(0.009)
0.009(0.008)

0.057(0.016)
0.153(0.106)
0.150(0.058)

0.007(0.006)
0.148(0.023)
0.128(0.021)

0.000(0.001)
0.198(0.031)
0.125(0.021)

0.057(0.016)
0.134(0.040)
0.100(0.029)

0.007(0.006)
0.160(0.030)
0.040(0.015)

0.000(0.001)
0.164(0.026)
0.029(0.018)

Table 3: Misclassiﬁcation risk and its standard error (in parentheses) of each method under Direc-
tion 2 in high-dimensional case.

s Σ Orcale

VCLDA

LPD

DLPD

Orcale

VCLDA

LPD

DLPD

p = 100

p = 200

5

10

20

1
2
3

1
2
3

1
2
3

0.225
0.217
0.199

0.158
0.164
0.126

0.107
0.126
0.081

0.243(0.031)
0.252(0.028)
0.241(0.031)

0.173(0.027)
0.208(0.031)
0.156(0.026)

0.126(0.024)
0.184(0.028)
0.099(0.021)

0.371(0.109)
0.274(0.032)
0.268(0.030)

0.204(0.029)
0.260(0.050)
0.212(0.025)

0.182(0.032)
0.280(0.064)
0.207(0.056)

0.248(0.031)
0.338(0.046)
0.251(0.027)

0.210(0.030)
0.347(0.033)
0.166(0.026)

0.146(0.030)
0.336(0.028)
0.116(0.020)

0.227
0.220
0.204

0.160
0.165
0.127

0.108
0.124
0.081

0.244(0.032)
0.237(0.031)
0.232(0.031)

0.189(0.028)
0.185(0.026)
0.140(0.024)

0.132(0.025)
0.179(0.028)
0.093(0.020)

0.281(0.045)
0.280(0.042)
0.272(0.054)

0.206(0.031)
0.258(0.026)
0.212(0.030)

0.166(0.025)
0.270(0.027)
0.202(0.026)

0.300(0.042)
0.370(0.043)
0.239(0.032)

0.210(0.031)
0.345(0.034)
0.174(0.028)

0.146(0.027)
0.329(0.029)
0.124(0.023)

Undoubtedly, the oracle classiﬁcation rule is the most accurate among all procedures. In a static
setting, it can be seen that the DLPD rule almost achieves the same performance as the LPD rule
in static settings (see Table 2). As we expected, the performance of the classical LDA procedure
degrades drastically in the dynamic case, which performs like random guessing in a highly dynamic
setting. Except for the static setting, we can see that the misclassiﬁcation risk of our proposed
VCLDA rule is signiﬁcantly lower than the DLPD rule, especially for the setting with Covariance
matrix 2. In addition, the results indicate that the performance of VCLDA is most close to the
oracle procedure.

In fact, VCLDA fully uses the information that the discrimination direction varies with diﬀerent
values of U while the active set of the discrimination coeﬃcient will not change in our simulation
settings. The former leads to a lower misclassiﬁcation risk than the static LPD rule, and the latter
leads to better performance over the DLPD rule.

6 Real Data Analysis

Diﬀuse large B-cell lymphoma (DLBCL) is a heterogeneous disease with recognized variability
in clinical outcome, genetic features, and cells of origin.
It is of vital importance for precision
medicine if we can predict DLBCL in advance. Using the data provided in Monti and S. (2005),

13

Table 4: Misclassiﬁcation risk and its standard error (in parenthesis) of each method under Direction
3 in high-dimensional case.

s Σ Orcale

VCLDA

LPD

DLPD

Orcale

VCLDA

LPD

DLPD

p = 100

p = 200

5

10

20

1
2
3

1
2
3

1
2
3

0.193
0.193
0.172

0.123
0.119
0.090

0.080
0.080
0.050

0.244(0.033)
0.211(0.029)
0.203(0.033)

0.147(0.024)
0.148(0.026)
0.115(0.025)

0.110(0.022)
0.114(0.023)
0.084(0.023)

0.337(0.055)
0.412(0.068)
0.334(0.048)

0.280(0.038)
0.450(0.075)
0.282(0.046)

0.249(0.036)
0.487(0.045)
0.218(0.037)

0.291(0.04)
0.302(0.032)
0.298(0.029)

0.232(0.028)
0.293(0.05)
0.214(0.03)

0.201(0.027)
0.234(0.027)
0.184(0.026)

0.194
0.192
0.172

0.125
0.122
0.091

0.084
0.079
0.052

0.221(0.030)
0.214(0.028)
0.209(0.028)

0.152(0.025)
0.145(0.028)
0.126(0.025)

0.109(0.021)
0.114(0.023)
0.071(0.019)

0.335(0.053)
0.395(0.068)
0.341(0.051)

0.278(0.039)
0.442(0.075)
0.274(0.036)

0.260(0.042)
0.498(0.020)
0.218(0.031)

0.286(0.031)
0.298(0.031)
0.291(0.032)

0.237(0.027)
0.270(0.032)
0.226(0.035)

0.201(0.031)
0.229(0.027)
0.221(0.033)

Table 5: Misclassiﬁcation risk and its standard error (in parentheses) of each method under Direc-
tion 4 in high-dimensional case.

s Σ Orcale

VCLDA

LPD

DLPD

Orcale

VCLDA

LPD

DLPD

p = 100

p = 200

5

10

20

1
2
3

1
2
3

1
2
3

0.010
0.025
0.018

0.001
0.007
0.003

0.000
0.001
0.000

0.018(0.009)
0.049(0.016)
0.037(0.014)

0.010(0.007)
0.025(0.011)
0.022(0.012)

0.007(0.006)
0.011(0.009)
0.008(0.005)

0.032(0.014)
0.174(0.087)
0.299(0.166)

0.020(0.011)
0.408(0.141)
0.179(0.041)

0.012(0.008)
0.426(0.116)
0.170(0.026)

0.015(0.010)
0.207(0.035)
0.056(0.021)

0.005(0.009)
0.195(0.031)
0.033(0.014)

0.000(0.001)
0.189(0.029)
0.027(0.016)

0.010
0.025
0.018

0.001
0.006
0.003

0.000
0.001
0.000

0.022(0.012)
0.035(0.013)
0.031(0.014)

0.006(0.006)
0.020(0.011)
0.016(0.010)

0.007(0.007)
0.015(0.009)
0.009(0.008)

0.033(0.012)
0.271(0.163)
0.175(0.021)

0.020(0.011)
0.319(0.144)
0.186(0.060)

0.011(0.008)
0.479(0.063)
0.178(0.022)

0.019(0.010)
0.177(0.029)
0.060(0.020)

0.003(0.004)
0.174(0.028)
0.037(0.014)

0.000(0.003)
0.179(0.027)
0.027(0.015)

we establish the model to predict DLBCL according to the gene expression. It is mentioned in
Monti and S. (2005) that tumors had less frequent genetic abnormalities in younger patients. Thus,
our proposed method VCLDA seems suitable for setting up the prediction model by setting the age
as the exposure variable U .

The original data has 124 patients and 44972 gene expression levels. The binary response means
whether a germinal center B-cell is normal or not, which is the signiﬁcant signal of DLBCL. We
screen out 150 gene expression levels to build a model according to the t test on the binary response.
We conduct the following four procedures: LPD (exclude age as a covariate), LPD (include age as
a covariate), DLPD (regard age as U ), and VCLDA (regard age as U ). We randomly choose ten
patients as the test sample in each trial and regard the remaining samples as the training set to run
the classiﬁcation procedure. The average results of misclassiﬁcation risks on the test sample over
100 trials are reported in Table 6. It shows that the contribution of U is negligible as a covariate
in the static LPD rule.
In contrast, it improves the classiﬁcation accuracy tremendously as an
exposure variable in the dynamic model.

Additionally, the active sets selected by the DLPD method under diﬀerent ages are highly coin-
cident. It means that the genes inﬂuencing DLBCL will not change signiﬁcantly with age, which
is also reasonable in the gene analysis. Two genes are excluded from the active set by the DLPD
method during a very short age interval, which may be confusing and misleading to the relative

14

Table 6: The average misclassiﬁcation risk and its standard error of each method in DLBCL dataset.

Method LPD (exclude age) LPD (include age) DLPD (U = age) VCLDA (U = age)

Avg
SE

0.432
0.211

0.432
0.211

0.192
0.167

0.171
0.122

researchers. Nearly all active genes selected by the DLPD method are also selected by the VCLDA
method. Besides, as we ﬁnd in coeﬃcients estimated by VCLDA, most of the genes have a weak in-
ﬂuence on DLBCL when U is small, which collaborates with the conclusion in Monti and S. (2005)
that tumors have less frequent genetic abnormalities in younger patients.

7 Discussion

This paper investigates the LDA model for dynamic data and proposes a new varying coeﬃcient
discriminant rule. The proposed classiﬁcation procedure is more eﬃcient than the dynamic linear
programming rule (Jiang et al., 2020). We also establish the upper bounds for estimation error and
uniform excess misclassiﬁcation risk. The synthetic and real data experiments also demonstrate a
better classiﬁcation performance of our varying coeﬃcient LDA method.

The Gaussian graphical model (GGM) is an essential formalism to infer dependence structures
of contemporary data sets, whose structure is equivalent to the support of the precision matrix.
Recently, Qiao et al. (2020) proposed the functional graphical model and assumed the covariate
is a p-dimensional functional data. The authors proposed an estimator of the precision matrix
function based on kernel smoothing and CLIME (Cai et al., 2011). Therefore, studying the high-
dimensional, varying coeﬃcient GGM under a dynamic setting will be of great interest.

References

T. W. Anderson. An introduction to multivariate statistical analysis. Wiley, New York, 1958.

A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.

P. J. Bickel and E. Levina. Some theory for ﬁsher’s linear discriminant function,naive bayes’,
and some alternatives when there are many more variables than observations. Bernoulli, 10(6):
989–1010, 2004.

T. Cai and W. Liu. A direct estimation approach to sparse linear discriminant analysis. Journal

of the American Statistical Association, 106(496):1566–1577, 2011.

T. Cai and L. Zhang. High dimensional linear discriminant analysis: optimality, adaptive algorithm
and missing data. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 81
(4):675–705, 2019.

T. Cai, W. Liu, and X. Luo. A constrained ℓ1 minimization approach to sparse precision matrix

estimation. Journal of the American Statistical Association, 106(494):594–607, 2011.

Z. Cai, J. Fan, and R. Li. Eﬃcient estimation and inferences for varying-coeﬃcient models. Journal

of the American Statistical Association, 95(451):888–902, 2000.

15

E. Candes and T. Tao. The dantzig selector: Statistical estimation when p is much larger than n.

The Annals of Statistics, 35(6):2313–2351, 12 2007.

J. Chen, D. Li, and O. Linton. A new semiparametric estimation approach for large dynamic
covariance matrices with multiple conditioning variables. Journal of Econometrics, 212(1):155–
176, 2019.

Z. Chen and C. Leng. Dynamic covariance models. Journal of the American Statistical Association,

111(515):1196–1207, 2016.

C. De Boor. A practical guide to splines, volume 27. springer-verlag New York, 1978.

J. Fan and W. Zhang. Statistical methods with varying coeﬃcient models. Statistics and its

Interface, 1(1):179, 2008.

J. Fan, W. Zhang, et al. Statistical estimation in varying coeﬃcient models. The Annals of

Statistics, 27(5):1491–1518, 1999.

J. Fan, F. Han, and H. Liu. Challenges of big data analysis. National Science Review, 1(2):293–314,

2014.

J. Fan, Y. Ma, and W. Dai. Nonparametric independence screening in sparse ultra-high-dimensional
varying coeﬃcient models. Journal of the American Statistical Association, 109(507):1270–1284,
2014.

Y. Guo, T. Hastie, and R. Tibshirani. Regularized linear discriminant analysis and its application

in microarrays. Biostatistics, 8(1):86–100, 2007.

T. Hastie and R. Tibshirani. Varying-coeﬃcient models. Journal of the Royal Statistical Society:

Series B (Methodological), 55(4):757–779, 1993.

T. Honda. Quantile regression in varying coeﬃcient models. Journal of Statistical Planning and

Inference, 121(1):113–125, 2004.

D. R. Hoover, J. A. Rice, C. O. Wu, and L.-P. Yang. Nonparametric smoothing estimates of

time-varying coeﬃcient models with longitudinal data. Biometrika, 85(4):809–822, 1998.

J. Z. Huang. Local asymptotics for polynomial spline regression. The Annals of Statistics, 31(5):

1600–1635, 2003.

B. Jiang, Z. Chen, and C. Leng. Dynamic linear discriminant analysis in high dimensional space.

Bernoulli, 26(2):1234–1268, 2020.

X. Lu, F. Dong, X. Liu, and X. Chang. Varying coeﬃcient support vector machines. Statistics &

Probability Letters, 132:107–115, 2018.

Q. Mai and H. Zou. Sparse semiparametric discriminant analysis. Journal of Multivariate Analysis,

135:175–188, 2015.

Q. Mai, H. Zou, and M. Yuan. A direct approach to sparse discriminant analysis in ultra-high

dimensions. Biometrika, 99(1):29–42, 2012.

Q. Mai, Y. Yang, and H. Zou. Multiclass sparse discriminant analysis. Statistica Sinica, 29(1):

97–111, 2019.

Monti and S. Molecular proﬁling of diﬀuse large b-cell lymphoma identiﬁes robust subtypes includ-

ing one characterized by host inﬂammatory response. Blood, 105(5):1851–1861, 2005.

16

X. Qiao, C. Qian, G. M. James, and S. Guo. Doubly functional graphical models in high dimensions.

Biometrika, 107(2):415–431, 02 2020.

Z. Qin, K. Scheinberg, and D. Goldfarb. Eﬃcient block-coordinate descent algorithms for the group

lasso. Mathematical Programming Computation, 5(2):143–169, 2013.

L. Schumaker. Spline functions: basic theory. Cambridge University Press, 2007.

J. Shao, Y. Wang, X. Deng, S. Wang, et al. Sparse linear discriminant analysis by thresholding for

high dimensional data. The Annals of Statistics, 39(2):1241–1265, 2011.

C. J. Stone. Optimal global rates of convergence for nonparametric regression. The Annals of

Statistics, 10(4):1040–1053, 1982.

H. Wang, B. Peng, D. Li, and C. Leng. Nonparametric estimation of large covariance matrices with

conditional sparsity. Journal of Econometrics, 223(1):53–72, 2021.

H. J. Wang, Z. Zhu, and J. Zhou. Quantile regression in partially linear varying coeﬃcient models.

The Annals of Statistics, 37(6B):3841 – 3866, 2009.

D. M. Witten and R. Tibshirani. Covariance-regularized regression and classiﬁcation for high
dimensional problems. Journal of the Royal Statistical Society: Series B (Statistical Methodology),
71(3):615–636, 2009.

M. C. Wu, L. Zhang, Z. Wang, D. C. Christiani, and X. Lin. Sparse linear discriminant analysis for
simultaneous testing for the signiﬁcance of a gene set/pathway and gene selection. Bioinformatics,
25(9):1145–1151, 2009.

Y. Xia. Asymptotic distributions for two estimators of the single-index model. Econometric Theory,

22:1112—-1137, 2006.

L. Xue and A. Qu. Variable selection in high-dimensional varying-coeﬃcient models with global

optimality. Journal of Machine Learning Research, 13(63):1973–1998, 2012.

M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal

of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006.

Appendix A Preliminaries

A.1 Background of B-spline approximation

B(U ) and write B and
From now on, we will omit the argument in random vector B(U ) and
B respectively whenever the context is clear. We introduce the following facts about standard
B-spline basis B∗(u) = (B∗1(u), ..., B∗Ln (u))⊤ (see (De Boor, 1978; Fan et al., 2014)), which will be
e
used in our proof:

e

1. For any u

∈
2. For any ηk ∈

[0, 1], both 0

max1

k

Ln B∗k(u)

≤
R, k = 1, 2, ..., Ln, we have

≤

≤

1 and

≤

Ln
k=1 B∗k(u) = 1 hold.

Ln

Ln

1

L−
n

η2
k .

Xk=1

Z  

Xk=1

ηkB∗k(w)

!

17

P

2

Ln

dw . L−
n

1

η2
k.

Xk=1

(A.1)

From the facts displayed above, for any r

1, we also have

≥
E [
r]
B∗k(U )
|
|

L−

1
n ,

≍

and

k2 = sup
Similarly, we can also obtain that

E[B∗]
k

2=1 |
k

ν

k

E[ν⊤B∗]

| ≤

sup
ν

2=1
k

k

E[ν⊤B∗]2

1/2

= O(L−
n

1/2

).

(cid:16)

(cid:17)

n . λmin(E[B∗B∗⊤])
1
L−

≤

λmax(E[B∗B∗⊤]) . L−

1
n .

Writing the scaled B-spline basis as B(u) = √LnB∗(u), we have

E[B]
k

k2 = O(1),

and

λmin(E[BB⊤]) = O(1),

λmax(E[BB⊤]) = O(1).

A.2 Concentration inequality

The following concentration inequality will be used throughout the proof.

Lemma A.1 (Lemma 1, Cai and Liu (2011)). Let ξ1, ..., ξn be independent random variables with
s2
mean 0. Suppose that there exists some φ > 0 and sn such that
n. Then for
0 < x < s2
n,

E[ξ2

i eφ

n
i=1

≤

|]

ξi

|

n

P

where Cφ = φ + φ−

1.

P

Xi=1

ξi ≥

Cφsnx

! ≤

x2)

exp(

−

Next lemma gives the moment inequalities for normal random variable.

Lemma A.2. Let X

∼ N

(0, σ2), then for any 0

E[X 2eφ

|

X

|]

≤

and for any φ

0 and k

1

≥

≥

≤
e
φ2

φ

≤

1
√2σ

,

1
2φ2σ2

−

;

1

p

E[X keφ

φ2σ2
2

|

e

X

|]

E[X k
≤
−
(cid:16)
φσ2, σ2) and Xφσ2

φσ2] + E[X k

φσ2]
(cid:17)

(φσ2, σ2).

∼ N

where X

−

φσ2

(
−

∼ N

holds for any φ

0,

≥

Proof of Lemma A.2. Using the basic inequality s2es

e2s for any s

≤

0, we have

≥

E[X 2eφ

|

X

|]

≤

≤

=

=

φ−

φ−

e
φ2
e
φ2

X

2E

2E[e2φ

|]
|
e1+φ2X 2
h
+
1
√2πσ
Z
1
2φ2σ2

−∞

1

.

−

p

18

i
∞
e−

x2
2σ2 +φ2x2

dx

 
In addition, we also have

E[X keφ

|

X

|] =

=

≤

−∞
0

Z

1
√2πσ
1
√2πσ (cid:18)Z
e
√2πσ (cid:18)Z

φ2σ2
2

+

∞

xke−

x2
2σ2 +φ

x

|

|dx

xke−

x2
2σ2 −

φxdx +

+

∞

xke−

x2
2σ2 +φxdx

−∞
+

∞

−∞

0
Z

(x+φσ2)2
2σ2

xke−

dx +

+

∞

(x−φσ2)2
2σ2

xke−

Z

−∞

(cid:19)

dx

.

(cid:19)

A.3 Estimation error bound for mean functions

The estimation error bounds for mean function vectors in the following proposition contribute to
establish the convergence rates of discriminant direction estimator and the excess misclassiﬁcation
risk.

Proposition A.1. Denote the estimator of the mean functions by
and

,

µ2(u) = (

α⊤21B(u),

α⊤2pB(u))⊤, then under condition (C1)-(C4), for any ϑ > 0 we have

µ1(u) = (

α⊤11B(u),

,

α⊤1pB(u))⊤

· · ·

· · ·

b

b

b

b

b

and

b
sup
[0,1] |
u
∈

µ1(u)

b

µ1(u)

|∞

−

.

Ln log n
n

r

+ L−

d
n ,

µ2(u)

sup
[0,1] |
u
∈

µ2(u)

|∞

−

.

Ln log n
n

r

+ L−

d
n ,

hold with probability at least 1

b
3pLnn−

ϑ respectively.

−

Lemma A.3. For any ϑ > 0, there exists some positive constant C such that

1
n

n

Xi=1

BiB⊤i −

P

 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E[BB⊤]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

CLn

≥

2

log n

r

n ! ≤

n−

ϑLn.

The proof of Lemma A.3 is deferred to Appendix C.1.

Remark A.1. It is worthwhile noting that Lemma A.3 is more tight than the results Lemma A.7
in Fan et al. (2014), where the authors established the following bound

n

1
n

B∗i (B∗i )⊤

−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
By the fact that Bi = √LnB∗i and B = √LnB∗, using the relation above, we can only obtain the
following worse bound

 r

Xi=1

2

E[B∗(B∗)⊤]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

= OP

Ln log n

.

n !

n

Xi=1

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

BiB⊤i −

E[BB⊤]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
19

2

= OP

L3/2
n

log n

r

n !

.

 
Proof of Proposition A.1. First we introduce the population form of the approximation coeﬃcient,

α1j = arg min
RLn

α

∈

Xj −

E

(cid:16)

(cid:17)

(cid:12)
(cid:12)

α⊤B(U )

Y = 1

2

,

(A.2)

e
α⊤11B,
and denote
(De Boor, 1978; Huang, 2003), we are guaranteed that

µ1(u) = (

· · ·

,

α⊤1pB)⊤. According to the splines’ approximation property

e

e

In addition, we have

e
sup
[0,1] |
u
∈

µ1(u)

e

µ1(u)

|∞

−

. L−

d
n .

α1j −

α1j =

b

e

=

1
n

1
n







1

−



1



−



1
n

1
n

Xi
∈I

1

Xi
∈I

1

BiB⊤i 

BiB⊤i 


Xi
∈I

1

Xi
∈I

1

Rp

p and

×

Bi(Xij −

B⊤i

α1j)



Bi[Xij −

B⊤i

e
α1j]





e




∆A
k2 = o(1),
k
(I + A−
k2k
A−
k2
k
2
1
∆A
(cid:0)
2k
k

A−
k2 ≤ k
A−
≤ k
A−
2
k

1∆A
k2.

≤

1

1

I
k2

1

1∆A)−
−
k2 + o(1)
(cid:1)

.

(A.3)

(A.4)


For any positive deﬁnite matrix A

(A + ∆A)−
k

1

∈

−

1

A−

Now let A = E[BB⊤] and ∆A =
∆A
k

k2 = o(1) almost surely, (A.4) results in

P

n
i=1 BiB⊤i /n

E[BB⊤]. Since Lemma A.3 claims that

−

1

−

BiB⊤i 


E[BB⊤]−

−

2

1

1
n

Xi
∈I

1

2

≤





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

E[BB⊤]−

1

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
BiB⊤i −

1
n

n

Xi=1

≥

(cid:13)
(cid:13)
(cid:13)

2 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

.

2

E[BB⊤]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

In conjunction with Lemma A.3 and λmin(E[BB⊤])

M1 we have

1

−

1
n

BiB⊤i 


Xi
∈I

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)





(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
M1

+ C

r

Ln log n
n

≤

2

holds with probability at least 1

ϑ. Substituting (A.5) into (A.3) yields

Lnn−

−

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi
∈I

1

α1j −
k

α1jk2 .

b

e

Bi[Xij −

µ1j(Ui) + µ1j(Ui)

B⊤i

−

20

.

α1j]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

2

(A.5)

(A.6)

On the other hand, we note that given Yi = 1 and Ui, Xij ∼ N
η > 0 and using Lemma A.2, we have

(µ1j(Ui), Σjj(Ui)). By choosing any

E

E

(B∗k(Ui))2 (Xij −
h
(B∗k(Ui))2E
n
2E

(B∗k(Ui))2e

h
η2Σ

(Xij −

jj (Ui)
2

≤

µ1j(Ui))2 exp (η

µ1j(Ui))2 exp (η

Xij −
B∗k(Ui)
||
|
)
µ1j(Ui)
Xij −
|
|
jj(Ui) + Σjj(Ui)

η2Σ2

i

io

Ui

(cid:12)
(cid:12)

)
µ1j(Ui)
|

≤
.E[(B∗k(Ui))2]

(cid:20)

L−

1
n ,

≤

(cid:0)

(cid:21)

(cid:1)

where the ﬁrst inequality follows from 0
Xij given Ui. Applying Lemma A.1 and uniform bound, we can guarantee

B∗k(Ui)

≤

≤

1 and Σjj(Ui) is the conditional variance of

P


1

max
k
≤
≤

1
n

Xi
∈I

1

(Xij −

µ1j(Ui))Bk(Ui)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

log n

n 

≤

.

r

Lnn−

ϑ.



Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

It yields that



holds with probability at least 1

1
n

Xi
∈I

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Bi(Xij −

Ln log n
n

µ1j(Ui))

.

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
ϑ. In addition, we note that
(cid:13)

r

2

2Lnn−

−

,

(A.7)

Bi[µ1j(Ui)

B⊤i

−

E[B[µ1j(U )

B⊤

−

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi
∈I

1

+

2

α1j]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
B⊤i

−

≤

(cid:13)
(cid:13)
(cid:13)
α1j]

e

e
Bi[µ1j(Ui)

1
n

Xi
∈I

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

E[Bi[µ1j(Ui)

−

−

2

e
B⊤i

α1j]]
(cid:13)
(cid:13)
(cid:13)
α1j]]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

.

2

Using Lemma A.1 again, we may show that

1
n

Xi
∈I

1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Bi[µ1j(Ui)

B⊤i

α1j]

−

−

E[Bi[µ1j(Ui)

B⊤i

−

d

. L−
n

Ln log n
n

,

r

2

α1j]]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

holds with probability at least 1

e
Lnn−

−

e
ϑ. Combining (A.7) and the following fact

E[B[µ1j(U )
k

−

B⊤

α1j]]

k2 . L−

d
n k

E[B]

d
k2 . L−
n ,

we have with probability at least 1

ϑ
3Lnn−
e

−

1
n

Xi
∈I

1

Bi[Xij −

µ1j(Ui) + µ1j(Ui)

B⊤i

−

Ln log n
n

.

.

r

α1j]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

2

(A.8)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Together with (A.6), we have proved the ﬁrst assertion.

21

For each ﬁxed u

[0, 1], we denote η(u) =

E[BB⊤]

1

−

B∗(u). It holds that

∈
µ1(u)
|

−

µ1(u)

|∞

= max

j

(cid:0)
B(u)⊤( ˆα1j −
|

(cid:1)
α1j)
|

b

e

= max

j

1

−

B(u)⊤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
j

1
n

e

1
n





1

Xi
∈I

BiB⊤i 

η(u)⊤B∗i [Xij −

1
n

Xi
∈I

1

Bi[Xij −

B⊤i

α1j]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

Xi
∈I
where the last inequality comes from Lemma A.3 and (A.8). It follows from (A.1) that

e

1

. Ln max

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B⊤i

,

α1j]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E

η(u)⊤B∗i

2

= E

(cid:20)(cid:16)

(cid:21)

(cid:17)

Ln

Xk=1





ηk(u)B∗(U )

!

2





Ln

1

. L−
n

η2
k(u) . L−

1
n .

Xk=1

Further we have

E

η(u)⊤B∗i

2

(cid:20)(cid:16)

(cid:17)

Then applying Lemma A.1, we can show that

(Xij −

µ1j(Ui))2eη

|

η(u)⊤B∗
i |

Xij

−

µ1j (Ui)

||

. L−

1
n .

(cid:21)

max
j

1
n

Xi
∈I

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

η(u)⊤B∗i [Xij −

µ1j(Ui) + µ1j(Ui)

B⊤i

−

log n
nLn

.

r

α1j]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

holds with probability at least 1

pn−

−

e
ϑ. With the same probability, for any ﬁxed u, it holds that

µ1(u)
|

−

µ1(u)

|∞

.

Ln log n
n

.

r

(A.9)

Next we use chaining technique to prove the uniform result. Notice that, we may divide the interval
[0, 1] to nM sub-intervals with end points 0 = u0 ≤
[0, 1], there
nM such that
exists some 0

u1 ≤ · · ·
M . Thus we have

unM = 1. Then for any u

∈

b

e

ℓ

≤

≤
µ1(u)

|

−

b

e

µ1(u)

−

−

n−

u
|

uℓ| ≤
µ1(uℓ)
|∞ ≤ |
µ1(u)
+
{|
b
µ1(uℓ)
|
b

|∞
|∞

µ1(uℓ)
µ1(uℓ)
e
µ1(uℓ)
e
b
µ are Lipschitz continuous. It follows that
e
. max
ℓ
≤

µ1(u)
|
M ,
+ n−
e

µ1(uℓ)

µ1(uℓ)

µ1(u)

nM |

|∞

|∞

−

−

−

−

+

.

−

b

≤

|∞

0

µ1(uℓ)

|∞}

+ n−

M .

where we used both

µ1 and

sup
b
[0,1] |
u
∈

µ1(u)
e

By choosing M = 1 and large ϑ, the second assertion follows from (A.9) immediately.

b

e

b

e

Appendix B Proofs of main results

B.1 Proof of Proposition 2.1

The proof is adapted from Cai and Zhang (2019), here we provide it for completeness.

22

 
Lemma B.1 (Lemma 7, Cai and Zhang (2019)). For two vectors θn and ˆθn, if
as n

c for some constant c, then when n

, and

,

θn −
k

ˆθnk2 = o(1)

→ ∞

θ
k

k2 ≥

θnk2k
k

ˆθnk2 −
∆(u) = c∗(u)(
1(u)δ(u) = c∗(u)θ∗(u) for c∗(u)

θ⊤n

−

ˆθn ≍ k

→ ∞
ˆθnk
2
θn −
2.
θ(u)⊤Σ(u)

Proof. Let δ(u) = µ1(u)
[0, 1]. Recall the
relation β∗(u) = Σ−
(0, 1). To simplify notations, we will use f
to denote f (u) for any function of u. Following the proof technique in Cai and Zhang (2019), we
deﬁne a intermediate quantity

µ2(u) and

θ(u))1/2 for u

∈
b

∈

b

b

R(u) =

1
2

Φ

 −

c∗δ⊤

θ/2
∆ !

+

¯Φ

1
2

b

c∗δ⊤

θ/2
∆ !

.

b

Note that, c∗δ⊤

θ=δ⊤Σ−

e

1/2Σ1/2(c∗

θ), then using Lemma B.1

b

b

Using the fact that c∗θ∗ = β∗ = Σ−

b

∆

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c∗δ⊤

θ

b

∆ (cid:12)
(cid:12)
b
(cid:12)
(cid:12)
(cid:12)

b

=

δ⊤Σ−
(cid:12)
k
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
. (cid:13)
(cid:13)
(cid:13)
1δ and

Σ−

1/2δ

Σ
k

1/2

k2 −

δ⊤Σ−

1/2Σ1/2(c∗
θ)

Σ1/2(c∗
(cid:13)
2
(cid:13)
Σ1/2(c∗
θ)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

b

θ)

b

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

2

.

∆

(cid:13)
(cid:13)
b
(cid:13)
k2 is bounded,
Σ1/2(c∗
1/2δ

θ)

Σ−

1/2δ

−

(cid:13)
(cid:13)
(cid:13)

Σ1/2(c∗

θ)

(cid:13)
(cid:13)
(cid:13)

b

2

2 ≤

=

≤

Σ−

−

Σ−

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(c∗)2
(cid:13)
(cid:13)

1/2δ

−
Σ1/2
k

2

2

(cid:13)
(cid:13)
(cid:13)
−
2
2.
k

Σ1/2(c∗

θ
b

c∗θ∗)

2
θ
2k
k

θ∗
b

−

Σ1/2c∗θ∗

−

2

2
(cid:13)
(cid:13)
(cid:13)

(B.1)

(B.2)

Then take Taylor expansion to the two terrms of

R around

b

∆/2 and ∆/2 respectively, we have

−

R =

−

1
2  −

R

e

θ/2

c∗δ⊤
∆
b
′′

+

′

Φ

∆
2 !

e
∆
2

(cid:19)

−

(cid:18)

+

1
2  

∆
2 −

c∗δ⊤

′

Φ

∆
2

(cid:18)

(cid:19)

θ/2
∆ !
2
b


b

∆
2 !

+

1
4 

(cid:16)


∆

=

1
√2π  

b
Φ

(b1n) + Φ

′′

(b2n)

c∗δ⊤

θ

−

∆ !
b

exp

(cid:18)

θ/2

c∗δ⊤
∆
b

−

b

(cid:19)

(cid:17)
∆2
8

−

+

1
4 



′′

Φ

b

(b1n) + Φ

′′

(b2n)

(cid:16)

(cid:17)

θ/2

c∗δ⊤
∆
b

2

∆
2 !

−



,





where b1n is some point between
Hence it holds that

−

∆
2 and

c∗δ⊤bθ/2
b
∆

−

b

and b2n is some point between ∆

2 and c∗δ⊤bθ/2

b
∆

.

′′

Φ

(b1n)

′′

Φ

(b2n)

∆
2

≍

exp

≍

23

∆2
8

.

(cid:19)

−

(cid:18)

 
 
 
Together with (B.1) and (B.2), we can obtain

2

c∗δ⊤

θ

R
|

R

|

−

.

e

.

.

∆
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:13)
θ
(cid:13)
(cid:13)
k

c∗δ⊤

θ

−

−

+

∆
∆ (cid:12)
(cid:12)
(cid:12)
(cid:12)
b
(cid:12)
(cid:12)
Σ1/2(c∗
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Σ−

1/2δ
b
−
2
2.
k

θ∗

−

θ)

(cid:13)
(cid:13)
(cid:13)

b

∆ (cid:12)
(cid:12)
b
(cid:12)
(cid:12)
(cid:12)

b

2

2

(B.3)

Next we bound

R

Rn −
|
θ/2
c∗δ⊤
e
∆
b

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

, note that
|

b

µ2)⊤ (c∗

θ)

µ
(

−

−

∆

b
µ + µ2)⊤Σ−
b

b
−

b
=

.

(δ/2

(cid:12)
(cid:12)
(cid:12)
1
∆
(

|
(cid:16)
µ

. |

−

(cid:12)
(cid:12)
(cid:12)
1δ + (δ/2
(cid:12)
(cid:12)

−

∆

b

(

µ

µ)⊤ β∗

−
µ)⊤ β∗
b
∆

+

θ
k

|

−

θ

k2k
b

µ

|

,

b

b

−

c∗θ∗)
(cid:12)
(cid:12)
(cid:12)

µ + µ2)⊤(c∗

θ

b

b

−

µ

k2

(cid:17)

where the ﬁrst inequality follows from that ∆ is bounded. Take Taylor expansion on the two terms
respectively, we have
of Rn(t) around

b
and c∗δ⊤bθ/2

c∗δ⊤bθ/2
b∆

−

b∆

Rn(t)

R =

−

µ
(

−

1
2  

∆

µ1)⊤ (c∗

θ)

b
µ

(

1
2  

−

µ2)⊤ (c∗
b
∆

b
θ)

b

b
(b3n)

µ
(

−

b
′′

1
4 

Φ

b
µ
(

−

′′

(b4n)



1
4 

Φ

c∗δ⊤

θ/2
∆ !

b
c∗δ⊤
θ/2
b
∆ !

′

Φ

′

Φ

+

−

c∗δ⊤

θ/2
∆ !

 −

c∗δ⊤

b
θ/2
b
∆ !

b
2

b
µ1)⊤ (c∗
θ)
b

∆

µ2)⊤ (c∗

b

b
θ)

∆

c∗δ⊤

θ/2
b
∆ !

b
c∗δ⊤
θ/2
b
∆ !





2



+

−

e

−

+

+

=

′′

(b3n) + Φ

Φ

′′

4

(c∗bθ)

⊤

µ1)
b
∆

−

b

θ)

b
(b4n)

µ
(
b
−

b
µ)⊤ (c∗
∆
b
and c∗δ⊤bθ/2
and b4n is some point between (bµ
b



b

b

,

b
∆

(B.4)

(c∗bθ)

⊤

µ1)
b
∆

−

where b3n is some point between (bµ
and
, then

c∗δ⊤bθ/2
b
∆

−

′′

Φ

(b3n)

′′

Φ

(b4n)

∆
2

≍

exp

≍

∆2
8

−

.

(cid:19)

(cid:18)

In fact, we also used µ

µ1 =

−

−

δ/2 and µ

−

µ2 = δ/2 to obtain (B.4). Then (B.4) implies

Rn(t)
|

R

|

−

.

µ
(

.

e

µ)⊤ (c∗θ∗)
(cid:12)
(cid:12)
(cid:12)

µ)⊤ β∗

2.
|

−

−

2

+

µ
(

(cid:12)
(cid:12)
(cid:12)

b

µ)⊤ (

θ

−

−

b

2

θ∗)
(cid:12)
(cid:12)
(cid:12)

(B.5)

24

(cid:12)
(cid:12)
|
(cid:12)

µ
(
b

b

 
 
 
Combining (B.3) and (B.5), for any u

[0, 1], it holds that

∈

Rn(u)
|

−

R(u)

| ≤ |
.

Rn(u)

θ(u)
k

−

−

+

R(u)
|

R(u)
|
2
θ∗(u)
2 +
e
k

−
µ(u)
(
e
(cid:12)
(cid:12)
(cid:12)

b

R(u)
|

−

µ(u))⊤ β∗(u)
(cid:12)
(cid:12)
(cid:12)

2

.

b

B.2 Proof of Theorem 3.1 and 3.3

Here we only prove Theorem 3.1, and the proof of Theorem 3.1 can be easily obtained through
the similar analysis. The following lemma provides the lower bound and upper bound for the
eigenvalues of E[

B⊤], and the proof is deferred to Appendix C.2.

B

Lemma B.2. Assume the assumptions hold, then there exist two positive constant M1 and M2
such that

e

e

M1λ0 ≤

λmin(E[

B

B⊤])

≤

λmax(E[

B

B⊤])

M2(λ1 + δp/4).

≤

Proof of Theorem 3.1. There exists ¯θj(U ) = ¯γ⊤j B(U ) for j = 0, 1, ..., p such that
e

e

e

e

θ∗j (u)

sup
[0,1] |
u
∈

¯θj(u)

| ≤

−

M0L−

d
n .

Let ¯γ = (¯γ⊤1 ,

· · ·

, ¯γ⊤p )⊤, then note that

The optimality condition of θ∗j (U ) implies that

e

e

e

¯γ =

E

B

B⊤

1

−

E

B

Z

γ

−

h

(cid:16)

(cid:17)i

−

B⊤ ¯γ

,

(cid:17)i

e

(cid:16)

h

e

E

(Xj −
"

µj(U ))

Z

p

−

Xl=1

(Xj −

which means

µj(U ))θ∗j (U )

U

! (cid:12)
(cid:12)
(cid:12)
(cid:12)

(B.6)

(B.7)

= 0,

#

E

(Xj −

"

µj(U ))B(U )

Z

p

−

(Xj −

Xl=1

µj(U ))θ∗j (U )

!#

= 0.

Recall

B = (X

µ)

−

⊗

B, then we can get

B⊤ ¯γ

−

= E

B





θ∗j (U )(Xj −

µj(U ))

−

e
E

B

Z

(cid:16)

h

e

Let C(U ) = E

(X

(cid:2)

p

Xj=1

(cid:17)i

= E

 e

B(X

e

−

µ(U ))(X

h

−

C(U ) = Σ(U ) +

e
µ(U ))⊤|
U
1
4

(cid:3)
(µ1(U )

µ)⊤(θ∗(U )

−

−

.

¯θ(U ))
i

, then simple calculation yields that

p

Xj=1

¯θj(U )(Xj −

µj(U ))









µ2(U ))(µ1(U )

µ2(U ))⊤.

−

−

25

 
 
For ν = (ν⊤(1), ..., ν ⊤(p))⊤, we denote

ν(U ) = (ν⊤(1)B(U ), ..., ν ⊤(p)B(U ))⊤. Then we have

E[

B(Z

B⊤ ¯γ)]

e

2

(cid:13)
(cid:13)
(cid:13)
−

−
E[ν⊤
B(Z
e
(cid:12)
(cid:12)
E[
(cid:12)
(cid:12)
(cid:12)
E
(cid:12)

ν(U )
e
k

e
ν(U )⊤(X

k2k

(cid:13)
(cid:13)
= sup
e
(cid:13)
ν
2=1
k
k
= sup
ν

2=1
k

k
= sup
ν

k

2=1
k
(cid:12)
d
.√pL−
(cid:12)
n

(cid:2)
sup
e
ν
2=1
k

k

E[
ν(U )
k

k2],

B⊤ ¯γ)]

(cid:12)
(cid:12)
µ(U ))(X
(cid:12)

e
−

−

µ(U ))⊤(θ∗(U )

C(U )

θ∗(U )

k2k

¯θ(U )

k2

−

(cid:3)(cid:12)
(cid:12)

−

¯θ(U ))]
(cid:12)
(cid:12)
(cid:12)

(B.8)

where the last inequality follows from (B.6) and
(A.1) and

ν
k

k2 = 1, we have

e

C(u)
k

k2 ≤ k

Σ(u)

k2 + δp. Using the inequality

E[
ν
k

k2]

≤

E[
ν
k

2
2]
k

1/2

(cid:0)

(cid:1)

= Ln 


Xj=1

e

e
Combining (B.7) and (B.8), we are guaranteed that
γ⊤j B(u), together with (A.1), we can have

p

1/2

p

1/2

E[(ν⊤(j)B∗)2]



.



ν(j)k
k

2
2

.

Xj=1


¯γ

−

γ
k


k2 . √pL−


n . Recall that

d

θj(u) =

e

θ
k

−

¯θ

2
L2 =
k

e

1

0

Z

θ(u)

(cid:13)
(cid:13)
(cid:13)e

−

2

2

¯θ(u)
(cid:13)
(cid:13)
(cid:13)

du = Ln

p

e

1

p

e

2

du

γ(j) −

¯θ(j)

⊤ B∗(u)

(cid:17)

(cid:19)

0 (cid:18)(cid:16)

Xj=1 Z
γ(j) −
k

e
¯θ(j)k

2
2

.

=

Xj=1
γ
k

¯γ
e

2
2.
k

−

It yields that

e

θ∗
k

−

θ

e

kL2 ≤ k

θ∗

¯θ

kL2 +

θ
k

−

¯θ

d
kL2 . √pL−
n .

−

e

B.3 Proof of Theorem 3.2

2n
i=1

Bi

B⊤

i and bn = 1
2n

Let Dn = 1
2n
b = E[
error

B⊤] and
BZ]. The following two lemmas give the concentration bounds for two terms in estimation
γ
k
e
b

P
k2. We defer the proofs in Section C.
Lemma B.3. Under the conditions of Theorem 3.2, we have

BiZi. Correspondingly, we write D = E[

P

2n
i=1

B

−

γ

e

e

e

e

e

e

Dn −

k

D

k2 . Ln

p log n
n

r

+ pL3/2

n an

log n
n

,

r

(B.9)

holds with probability at least 1

ϑLnp

n−

−

−

pLnn−

ϑ.

26

Lemma B.4. Under the conditions of Theorem 3.2, we have

Dn

γ

k

D

γ

k2 .

−

pLn log n
n

r

+ pLnan

log n
n

,

r

e
holds with probability at least 1
−

n−

ϑLnp
e

−

pLnn−

ϑ.

Lemma B.5. Under the conditions of Theorem 3.2, we have

bn −
|

b

|∞

.

log n
n

r

+ L−

1
2
n an,

holds with probability 1

pLnn−

ϑ.

−

Proof of Theorem 3.2. From the deﬁnition of

γ and

γ, we have

Now let us recall the optimal condition of

b

e

γ = D−

1

γ

−

1

γ = D−
n (bn −
n bn −
e
b
γ,

e

Dn

γ) .

e

In addition, notice that

0 = E

B

h

e

Z
e
(cid:16)

−

B⊤

γ

= b

(cid:17)i

e

e

D

γ.

−

e

(B.10)

(B.11)

(B.12)

(B.13)

µ(U ))

BZ]

b = E [(X
1
2

−
E [(µ1(U )

=
= E [(µ1(U )

⊗
µ(U ))

−
µ2(U ))

−
RpLn, we denote

B]

⊗

−

1
2

E [(µ2(U )

µ(U ))

B]

⊗

−

B] .

⊗

ν(U ) = (ν⊤(1)B(U ), ..., ν ⊤(p)B(U ))⊤. By the deﬁnition

For ν = (ν⊤(1), ..., ν ⊤(p))⊤ ∈
k · k2 and (A.1), we have
of
b
k

k2 = sup

SpLn−1

ν

∈
= sup

ν

SpLn−1

∈
= sup

SpLn−1

e
ν⊤b
(cid:12)
(cid:12)
(cid:12)
(cid:12)
E
ν⊤ ((µ1(U )
(cid:12)
(cid:12)
(cid:12)
(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
sup
(cid:12)
SpLn−1

h
E [
ν(U )
e
k

k2]

h

µ2(U ))

−

⊗

ν(U )⊤ (µ1(U )

µ2(U ))

−

B)

i(cid:12)
(cid:12)
(cid:12)

i(cid:12)
(cid:12)
(cid:12)

ν
∈
δp

δp

≤

≤

= δp

. δp

ν

ν

ν

ν

∈

sup
SpLn−1

∈

(cid:0)

E[
2
ν(U )
2]
e
k
k

1/2

p

e

E

(cid:1)

1/2

(ν⊤(j)B(U ))2

h

1/2

i





sup
SpLn−1 

∈



sup
SpLn−1 

∈



Xj=1
p

Xj=1

27

ν(j)k
k

2
2

= δp,



(B.14)

where δp = supu
[0,1] k
∈
Using the inequality (A.4), we get

µ1(u)

µ2(u)

−

k2. According to Lemma B.3, we know that

Dn −
k

D

k2 = oP(1).

D−

1
n −

1

D−

(cid:13)
(cid:13)

2 ≤
(cid:13)
(cid:13)

≤

1

2

D−
2 k
1
1
1 λ−
(cid:13)
0 k
(cid:13)

Dn −
Dn −

2M −
(cid:13)
(cid:13)

D

D

k2
k2 ,

where the second inequality follows from Lemma B.2. Hence we can guarantee that
n k2 = O(1)
with high probability. By plugging the bounds in Lemma B.4 and B.5, together with (B.13), we
have

D−
k

1

γ
k

−

γ

b

e

γ

1
n k2k
1
n k2k
b

D−
Dn
bn −
k2 ≤ k
D−
Dn
bn −
γ
=
k
e
Dn
bn −
γ
.
k2 +
k
k
e
pLn log n
n

.

+ anpLn
e

k2
−

−

r

γ

b + D
D
γ

k2

k2
e
log n
.
e
n

r
γ(1), ..., B(u)⊤

(B.15)

γ(p))⊤. Applying (A.1),

θ = (B(u)⊤

γ(1), ..., B(u)⊤

γ(p))⊤ and

θ(u) = (B(u)⊤

Recall
we have
b

b

1

θ(u)

0 k

−

Z

e
b
2
θ(u)
2du = Ln
k

b

e

p

.

p

1

e

B∗(u)⊤(

γ(j) −

e
2

γ(j))
(cid:17)

du

γ(j)k

2
2 =

b
γ
k

−

γ

e

2
2.
k

(cid:16)

0
Xj=1 Z
γ(j) −
k

Xj=1

Then we have ﬁnished the proof of Theorem 3.2 by plugging (B.15).

b

e

b

e

B.4 Proof of Theorem 3.4

The following lemma provides the ℓ2 error bound for general quadratic group lasso problem. We
defer the proof of Lemma B.6 to Appendix C.6.

Lemma B.6. For general quadratic group lasso problem

γ = arg min
RpLn

γ

∈

if the following two conditions hold

b

1
2

γ⊤Aγ

−

b⊤γ + λ

p

Xj=1

γjk2,
k

1. A satisﬁes the restrictive eigenvalue condition with parameter ζ: for any ξ

ξ
k

k1 ≤

4√sLnk
ξ

k2, it holds that

ξ⊤Aξ

2. for any ˇγ

∈

RpLn such that ˇγ(j) = 0 for j

∈

ζ

ξ
k

k2.

≥
Sc and

RpLn such that

∈

(Aˇγ

max
j
1
≤
≤

p k

b)(j)k2 ≤

−

λ
2

.

(B.16)

then we have

γ
k

−

ˇγ

k2 ≤

12√sλ
ζ

and

γ
k

−

ˇγ

k1 ≤

48s√Lnλ
ζ

.

b

b

28

Lemma B.7. Under conditions (C1)-(C5), let ν
any ϑ > 0 we have

∈

RpLn be a ﬁxed vector with ν(Sc) = 0, then for

(Dn

max
j
1
≤
≤

p k

γ

D

γ)(j)k2 .

ν
k

k2

−

Ln log p
n

 r

+ anLns

log p

r

n !

,

holds with probability at least 1

e

e
Lnp−

ϑ

−

Lnsp−

ϑ

−

−

sp−

ϑLn.

Proof of Theorem 3.4. According to Lemma B.6, it suﬃces to show the restrictive eigenvalue con-
dition of Dn and the inequality (B.16). For any ξ
k2, we
have

RpLn such that

4√sLnk
ξ

k1 ≤

ξ
k

∈
ξ⊤Dnξ = ξ⊤E[
B⊤]ξ + ξ⊤(Dn −
B
2
(Dn −
ξ
ξ
M1λ0k
k1|
2 − k
k
e
e
2
2
D
Dn −
ξ
ξ
M1λ0k
1 |
2 − k
k
k
D
Dn −
4sLn |
(M1λ0 −
|∞
Dn −
By tracing the proof of Lemma B.7, we can guarantee sLn|
exists some positive constant ζ such that,

≥

≥

≥

D)ξ

D)ξ

|∞

2
2.
k

|∞
ξ
)
k
D

|∞

= oP(1). It implies that there

Hence we have veriﬁed the restrictive eigenvalue condition. Recall the approximation coeﬃcient
γ(S) = D−

γ(Sc) = 0, then we have

1
(SS)b(S) and

ξ⊤Dnξ

ζ

ξ
k

2
2.
k

≥

e

and

e

(Dn

γ

−

bn)(S) = (Dn)(SS)D−

1

(SS)b(S) −

e
γ

(Dn

−

bn)(Sc) = (Dn)(ScS)D−

1

(SS)bS −

(bn)(S),

(bn)(Sc).

(B.17)

(B.18)

In addition, similar to (B.14), we can verify

e

b(S)k2 . δs. Together with Lemma B.2, we have
k
D−
γ(S)k2 ≤ k
k

1
(SS)k2k

b(S)k2 . δs.
S

Then from (B.17), Lemma B.7 and B.5, for any j

e

∈

(Dn
k

γ

bn)(j)k2 ≤

−

e

.

.

max
S
j
∈
(cid:13)
(cid:2)
(cid:13)
Ln log p
(cid:13)
n
Ln log p
n

r

r

+ anLns

+ anLns

(cid:3)
log p
e
n
log p
n

r

r

2
(cid:13)
(cid:13)
(cid:13)
r

+

+ an,

(Dn)(j,S) −

D(j,S)

γ(S)

+

Ln

b(S) −

(bn)(S)

p
Ln log p
n

(cid:12)
(cid:12)
+ an

∞
(cid:12)
(cid:12)

holds with probability at least 1
j

−
Sc. From (B.18), we claim that for any j

Lnp−

−

Lnsp−
Sc

ϑ

sp−

ϑLn. Next we will derive the bound for

ϑ

−

∈

(Dn
k

γ

−

e

bn)(j)k2 ≤
+

∈
γ(S))(j) −

(D(ScS)
(cid:13)
(cid:13)
(cid:13)
p

Ln

b(Sc) −
e
(cid:12)
(cid:12)

b(j)

2
(cid:13)
.
(cid:13)
(cid:13)
∞
(cid:12)
(cid:12)

(bn)(Sc)

29

+

(cid:13)
(cid:2)
(cid:13)
(cid:13)

(Dn)(j,S) −

D(j,S)

γ(S)

(cid:3)

e

2

(cid:13)
(cid:13)
(cid:13)

Using the optimality of θ∗(U ), we have

Together with

B(Sc)Z] and

µj(U ))B(U ), we also have

E

Z

B(Sc) 
S
Xj
∈
e

γ(j), b(Sc) = E[
θj(U ) = B(U )⊤





−

D(ScS)
e

γ(S) = E

e

= E

B(Sc)
e
n

e
B(Sc)

e

B⊤

(S)

γ(S)
e
o
e
(Xj −




S
Xj
∈

(Xj −

µj(U ))θ∗j (U )

= 0.






B(j) = (Xj −

e

µj(U ))B(U )⊤

γ(j)


µj(U ))

θj(U )

e

−


Z


e
θj(U )
µj(U ))(

+ b(Sc)






θ∗j (U ))



−



Sc, it holds that




e
U ], then for j
∈

(Xj −

(Xj −

S
Xj
∈

= E

= E





e
B(Sc) 
e


B(Sc) 

S
Xj

∈
e

µ(U ))S |
µj(U ))(X

−
b(j) = E
γ(S))(j) −

B(j)(X

−

µ(U ))⊤S (θ∗(U )

−

e

θ(U ))S

.

i

e

h

+ b(Sc).

θ(U ))S

−
µ(U ))⊤S (θ∗(U )

e

i(cid:12)
(cid:12)
(cid:12)

B(j)(X

µ(U ))⊤S (θ∗(U )

−
ν⊤B(Xj −
e
ν⊤Bcj,S(U )⊤(θ∗(U )

µj(U ))(X

−

−

cj,S(u)
k

k2k

(θ∗(u)

n

θ(U ))S

−

i(cid:12)
θ(u))S k2
(cid:12)
e
(cid:12)
1/2
e

E

o

h

ν⊤B
|

|
i

θ(U ))S

−

e

i(cid:12)
(cid:12)
(cid:12)

Let cj,S(U ) = E[(Xj −
(D(ScS)

For any ν

SLn

1,

−

∈

E

(cid:12)
(cid:12)
=
(cid:12)

=

≤

e
ν⊤

h
E

h

(cid:12)
E
(cid:12)
(cid:12)
(cid:12)
h
(cid:12)
sup
(cid:12)
[0,1]
u

∈

. δs√sL−
n

d

E

ν⊤B

2

. √sL−

d
n ,

(cid:18)

(cid:20)(cid:16)

(cid:21)(cid:19)

(cid:17)

where we used Theorem 3.3 and

cj,S(U )

sup
[0,1] k
u
∈

u

k2 = sup
[0,1] k
∈
. sup
u

[0,1] k
∈

(µ1j (u)

(µ1(u)

µj(u))(µ1(u)

−
µ2(u))S k2 ≤

−

δs.

µ(u))S k2

−

Hence we have for any j

Sc,

∈
γ(S))(j) −
(cid:16)
Then applying Lemma B.7 and B.5, we claim that

sup
SLn−1

(D(ScS)

b(j)

2 ≤

∈

ν

e

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

(Dn

max
j
∈

Sc k

γ

−

bn)(j)k2 .

r
ϑ.
10Lnp−

holds with probability at least 1

e

−

ν⊤
(cid:12)
(cid:12)
(cid:12)
Ln log p
n

30

(D(ScS)

. δs√sL−

d
n .

γ(S))(j) −

b(j)

e

(cid:17)(cid:12)
(cid:12)
(cid:12)

+ anLns

log p
n

r

+ an + √sL−
n

d

Appendix C Deferred proofs of Section A and B

C.1 Proof of Lemma A.3

Proof. Let SLn
with K

−

17Ln . Let Q =

≤

1 be the unit sphere in RLn, we denote the 1

n
i=1 BiB⊤i /n

E[BB⊤], then we have

8 -covering of SLn

1 by

−

ν1, ..., ν K }
{

P

Q
k

−
k2 = sup

ν

SLn−1 |

∈

ν⊤Qν

.
|

Based on the deﬁnition of covering set, for any ν
ν
k

1/8. It follows that

νkk2 ≤

−

SLn

−

1, there exists some 1

∈

k

≤

≤

K such that

ν⊤Qν
|

| ≤ |

ν⊤k Qνk|
ν⊤k Qνk|
ν⊤k Qνk|

+

ν ⊤k Q(νk −
+ 2
|
1
1
Q
k2 +
4 k
64 k
1
k2.
2 k

Q

+

ν)
|
Q

k2

≤ |

≤ |

+

(νk −
|

ν)⊤Q(νk −

ν)
|

Thus we have

Since

B∗i k
k

2
2 =

Q
k

k2 ≤

2 max
k
≤
≤

1

K |

Ln
k=1(B∗k(Ui))2

P

E

(ν⊤k B∗i )4 exp

≤
P
η(ν ⊤k B∗i )2
{

n

1
n

ν⊤k Qνk|

k

= 2 max
1
≤
≤

E[(ν⊤k B)2]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ln
k=1 B∗k(Ui) = 1, together with (A.1), we have
(cid:12)

(ν⊤k Bi)2

K (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi=1

−

eηE

(ν⊤k B∗i )2

h

i

≤

}
i

. L−

1
n k

νkk

2
2 = L−

1
n ,

.

(C.1)

together with Lemma A.1 we claim that

P

max
k
≤
≤

1

1
n

n

Xi=1

(ν⊤k Bi)2

−

E[(ν⊤k B)2]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Then the conclusion follows immediately.

h

K (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

CLn

≥

r

Ln log n

nLn ! ≤

ϑL3
n

Kn−

n−

ϑLn.

≤

C.2 Proof of Lemma B.2

Proof of Lemma B.2. Let C(u) = E[(X

µ(u))(X

−

C(u) =

E

(X

1
2

(cid:16)
h
= Σ(u) +

1
4

µ(u))(X

µ(u))⊤

−

−

Y = 1
|

(µ1(u)

−

µ2(u))(µ1(u)

−

−
+ E

i
µ2(u))⊤.

µ(u))⊤], and then it holds

(X

h

−

µ(u))(X

µ(u))⊤

−

Y = 0
|

i(cid:17)

From conditions (C1) and (C4) we have

λ0 ≤

λmin(C(u))

≤

λmax(C(u))

λ1 +

≤

1
4

δp,

(C.2)

holds for any u

∈

[0, 1]. In addition, we notice that

E[

B

B⊤] = E

(X

µ(U ))(X

e

e

= E

h(cid:16)
C(U )

−

⊗

h

BB⊤

(cid:16)

(cid:17)i

31

µ(U ))⊤

BB⊤

(cid:16)

(cid:17)i

⊗

(cid:17)

−
.

 
Then for any η = (η⊤(1),

, η⊤(p))⊤ ∈

· · ·

RLnp with

η
k

k2 = 1, we have

p

2

η⊤E[

B

B⊤]η = E

e

e

= E

(Xj −

Xj=1
B⊤η(1),

µj(U ))B⊤ηj

C(U )

, B⊤η(p)

· · ·





(cid:20)(cid:16)

≥

≥

Similarly, we have

u

λmin (C(u)) E

inf
[0,1]
∈
λ0λmin(E[BB⊤]).

· · ·

, B⊤η(p)

⊤

(cid:21)

(cid:17)

B⊤η(1),
(cid:17)
(cid:16)
η⊤(j)BB⊤η(j)


p





Xj=1

η⊤E[

B

B⊤]η

≤

λ1 +

(cid:18)

δp
4

(cid:19)

λmax(E[BB⊤]).

Then the result follows from λmin(E[BB⊤]) = O(1) and λmax(E[BB⊤]) = O(1) (see Section
A.1).

e

e

C.3 Proof of Lemma B.3

To prove Lemma B.3, we impose the following ﬁve lemmas on the concentration inequalities of
random matrices. The proofs can be found in Appendix D.1 - D.4.

Lemma C.1. Let Zi = Z(Ui) = (X i −
for any ϑ > 0 and k = 1, 2

µk(Ui))

⊗

Bi, then under condition (C1)-(C4), we have

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Z iZ⊤i −

E

Yi = k

Z iZ ⊤i |
h

Xi
∈Ik n

. Ln

p log n
n

,

r

io

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

and

1
n

ZiZ⊤i −

E

h

Xi
∈Ik n

Z iZ⊤i |

Yi = k

pLn log n
n

,

.

r

io

γ

e

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

hold with probability at least 1

n−

ϑpLn.

−

Lemma C.2. Under conditions (C1)-(C4), for k = 1, 2 and any ϑ > 0, we have

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi
∈Ik nh

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(X i −

µk(Ui))(µ1(Ui)

µ2(Ui))⊤

−

BiB⊤i
(cid:16)

⊗

i

Xi
∈Ik h

(cid:17)

(X i −

µk(Ui))(µ1(Ui)

µ2(Ui))⊤

−

BiB⊤i

⊗

i

(cid:16)

γ

e

(cid:17)o

. Ln

p log n
n

,

r

pLn log n
n

,

.

r

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

hold with probability at least 1

n−

ϑLnp.

−

32

Lemma C.3. Under conditions (C1)-(C4), let Ai = [(µ1(Ui)
(BiB⊤i ), then for any ϑ > 0,

−

µ2(Ui))(µ1(Ui)

µ2(Ui))⊤]

⊗

−

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
hold with probability at least 1
(cid:13)
−

1
n

n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Xi=1
n−

n

Ai −

Xi=1
(Ai −

E[Ai])

2

E[Ai]
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
γ
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

ϑLnp.

. Ln

p log n
n

,

r

pLn log n
n

.

r

Lemma C.4. Under conditions (C1)-(C4), then for k = 1, 2 and any ϑ > 0,

1
n

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi
∈Ik

[(X i −

µk(Ui))(

µ(Ui)

µ(Ui))⊤]

⊗

−

. pL3/2

n

log n

b
n  r

r

Ln log n
n

d

+ L−
n

,

!

(BiB⊤i )
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

[(X i −

µk(Ui))(

µ(Ui)

µ(Ui))⊤]

⊗

−

Xi
∈Ik n

log n

. pLn

r

b

Ln log n
n

d

+ L−
n

,

!

n  r
ϑpLn

−

pLnn−

ϑ.

(BiB⊤i )

o

γ

e

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

hold with probability at least 1

−

n−

Lemma C.5. Under conditions (C1)-(C4), let G(Ui) = [(µ1(Ui)
(BiB⊤i ), we have for any ϑ > 0

−

µ2(Ui))(

µ(Ui)

µ(Ui))⊤]

⊗

−

G(Ui)

1
n

−

1
n

Xi
∈I

1

Xi
∈I

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
n

G(Ui)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
G(Ui)
γ

. pL3/2

n

log n

r

n  r

Ln log n
n

d

+ L−
n

b

. pLn

log n

r

n  r

Ln log n
n

d

+ L−
n

,

,

!

!

G(Ui)

γ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
hold with probability at least 1
(cid:13)

Xi
∈I

e

1

−

2

Xi
∈I
n−

ϑpLn

−

−

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
pLnn−
(cid:13)

e

ϑ.

1
n

Proof of Lemma B.3. Note that we can rewrite E[

B

B⊤] and

2n
i=1

Bi

B⊤

i /2n as

E

B

B⊤

=

h

e

e

i

E

1
2

nh

(X

µ1(U )) (X
e
e
−

−

P

µ1(U ))⊤

I∗
1

⊗

i

(BB⊤)
e

e

Y = 1

(cid:12)
(cid:12)

(BB⊤)

Y = 0

o

}

+

E

1
|
2

+

E

1
|
4

nh

nh

|

(X

−

µ2(U )) (X

−

{z
µ2(U ))⊤

I∗
2

⊗

i

(µ1(U )

−

µ2(U ))(µ1(U )

{z

µ2(U ))⊤

−

I∗
3

{z

33

o

(cid:12)
(cid:12)

}
(BB⊤)

o

}

⊗

i

and

1
2n

2n

Xi=1

Bi

B⊤

i =

1
2n

e

e

Xi
∈I

1 nh

(X i −

µ(Ui))(X i −

µ(Ui))⊤

(BiB⊤i )

o

⊗

i

b

I1

b

+

1
|
2n

|

Xi
∈I

2 nh

(X i −

µ(Ui))(X i −

{z

µ(Ui))⊤

⊗

i

b

b

I2

{z

(BiB⊤i )

}

.

o

}

To upper bound
and I2,

k

P

2n
i=1

Bi

B⊤

i /2n

E[

B

−

B⊤]

k2, we begin with the following decompositions for I1

e
I1 =

e

1
2n

e
e
(X i −

Xi
∈I

1 h

µ(Ui))(X i −

µ(Ui))⊤

(BiB⊤i )

⊗

i

+

1
2n

|

Xi
∈I

1 nh

(X i −

I1
1

{z
µ(Ui))(µ(Ui)

µ(Ui))⊤

−

I1
2

b

Xi
∈I

1 nh

(µ(Ui)

{z
µ(Ui))(X i −

−

µ(Ui))⊤

b

I1
3

}
(BiB⊤i )

o

(BiB⊤i )

}

o

⊗

i

⊗

i

Xi
∈I

1 h

(µ(Ui)

−

µ(Ui))(µ(Ui)

{z

b

I1
4

{z

}
(BiB⊤i )

µ(Ui))⊤

−

⊗

i

b

}

and

I2 =

1
2n

Xi
∈I

2 h

(X i −

µ(Ui))(X i −

µ(Ui))⊤

(BiB⊤i )

⊗

i

I2
1

+

1
|
2n

Xi
∈I

2 nh

Xi
∈I

2 nh

(X i −

µ(Ui))(µ(Ui)

{z

µ(Ui))⊤

−

I2
2

b

(µ(Ui)

{z
µ(Ui))(X i −

−

µ(Ui))⊤

}

(BiB⊤i )

o

(BiB⊤i )

}

o

⊗

i

⊗

i

b

I2
3

Xi
∈I

2 h

(µ(Ui)

−

µ(Ui))(µ(Ui)

{z

b

I2
4

{z
34

}
(BiB⊤i )

µ(Ui))⊤

−

⊗

i

b

}

+

1
|
2n

+

1
|
2n

|

+

1
|
2n

+

1
|
2n

|

Step 1.1. upper bounding

I1
1 −
k

I∗1 + I2

1 −

I∗2 −

I∗3k2. First, we decompose I1

1 as

I1
1 =

1
2n

Xi
∈I

1 h

(X i −

µ1(Ui))(X i −

µ1(Ui))⊤

(BiB⊤i )

⊗

i

+

1
|
4n

+

1
|
4n

+

1
|
8n

By Lemma C.1, we have

|

By Lemma C.2, we have

and

By Lemma C.3, we have

(X i −

Xi
∈I

1 h

I1
11

µ1(Ui))(µ1(Ui)

{z

µ2(Ui))⊤

−

I1
12

(µ1(Ui)

Xi
∈I

1 h

{z
µ2(Ui))(X i −

−

µ1(Ui))⊤

I1
13

}
(BiB⊤i )

}
(BiB⊤i )

⊗

i

⊗

i

(µ1(Ui)

Xi
∈I

1 h

{z
µ2(Ui))(µ1(Ui)

−

µ2(Ui))⊤

−

}
(BiB⊤i )

⊗

i

I1
14

{z

}

P

I1
11 −

I∗1

(cid:13)
(cid:13)

2 ≥
(cid:13)
(cid:13)

CLn

p log n

r

n ! ≤

n−

ϑLnp.

P

I1
12

2 . Ln

(cid:13)
(cid:13)

(cid:13)
(cid:13)

p log n

r

n ! ≥

n−

ϑLnp,

1

−

P

I1
13

2 . CLn

p log n

r

n ! ≥

n−

ϑLnp.

1

−

(cid:13)
(cid:13)

P

I1
14 −

 k

(cid:13)
(cid:13)

1
2

I∗3k2 . Ln

p log n

r

n ! ≥

pLnn−

ϑ.

1

−

Combining the results displayed above, it follows that

P

I1
1 −

 k

I∗1 −

Similarly, we also have

P

I2
1 −

 k

I∗2 −

1
2

1
2

I∗3k2 . Ln

p log n

r

n ! ≥

ϑLnp

3n−

1

−

−

pLnn−

ϑ.

I∗3k2 . Ln

p log n

r

n ! ≥

ϑLnp

3n−

1

−

−

pLnn−

ϑ.

35

(C.3)

(C.4)

 
 
 
Step 1.2. upper bounding

2 + I2
I1
k

2k2,

3 + I2
I1
k

3k2 and

I1
4k2 +
k

I2
4k2. Note that
k

I1
2 + I2
k

Xi
∈I

1 h

Xi
∈I

2 h

1
2n

+

1
2n

2k2 ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
µ2(Ui))(

1
4n

+

Xi
∈I

1

(X i −

µ1(Ui))(

µ(Ui)

b
µ(Ui)

µ2(Ui))(

(X i −

−

−

µ(Ui))⊤

µ(Ui))⊤

G(Ui)

1
4n

−

Xi
∈I

2

,

b
G(Ui)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

⊗

⊗

i

i

(BiB⊤i )
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(BiB⊤i )
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

2

where G(Ui) = [(µ1(Ui)
we have

−

µ(Ui)

µ(Ui))⊤]

−

⊗

(BiB⊤i ). By invoking Lemma C.5 and C.3,

P

b
2k2 . pL3/2
2 + I2
I1

n an

 k

log n

r

n ! ≥

n−

ϑLnp,

1

−

where an =

p

Ln log n/n + L−

n . Similarly, we can obtain

d

P

3 + I2
I1

3k2 . pL3/2

n an

 k

log n

r

n ! ≥

n−

ϑLnp.

1

−

In addition, by Proposition A.1 we have

I1
4k2 +
k

I2
4k2 ≤
k

≤

Combining (C.3)-(C.7), we have

µ(Ui)

2
µ(Ui)
2k
k

Bik

2
2

−

1

2 max
n k
i
≤
≤
2pLna2
n

b

Bi

B⊤

i −

E[

B

B⊤]

. Ln

p log n
n

r

+ pL3/2

n an

log n
n

r

1
2n

2n

Xi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

e

e

e

ϑLnp

n−

−

−

(cid:13)
2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
pLnn−

ϑ.

holds with probability at least 1

(C.5)

(C.6)

(C.7)

(C.8)

C.4 Proof of Lemma B.4

Proof. By replacing the bounds for the operator norm of matrices with those for ℓ2-norm of matrix-
θj(Ui) is
vector-products in Section C.3, we can ﬁnish the proof Due to the fact that
bounded, we can drop a √Ln factor for matrix-vector-product bounds in Lemma C.1- C.3.
e

γ(j) =

B⊤
i

e

e

36

C.5 Proof of Lemma B.5

Proof. Note that for any j = 1, 2, ..., p and k = 1, 2, ..., Ln, we ﬁnd that

1
2n

1
4n

=

=

1
4

2n

Xi=1

Xi
1
∈I
2n
1
2n

Xi=1

Bk(Ui)(Xij −

µj(Ui))Zi

Bk(Ui)(Xij −

b
µj(Ui))

−

Bk(Ui)(Xij −

µj(Ui))

1
4n

2n

Xi
∈I

2

Bk(Ui)

b
µ1j(Ui)

1
4

1
2n

−

I1

b

Bk(Ui)

µ2j(Ui)

,

Xi=1

I2

b

b

where we used

|

µ1j = B⊤i

α1j,

{z

µ2j = B⊤i

}

{z
α2j and the optimal condition for

}

|

α1j and

α2j such that

b

Xi
∈I

1

Moreover, note that

Bi(Xij −
b
b

B⊤i

α1j) = 0,

b

b

Xi
∈I

2

Bi(Xij −

B⊤i

α2j) = 0.

b

b

b

µj(U ))Z]

E[Bk(U )(Xj −
1
E[Bk(U )(Xj −
=
4
1
E[Bk(U )µ1j (U )]
4
I ∗
1

=

1
4

−

Y = 1]
µj(U ))
|
E[Bk(U )µ2j(U )]
I ∗
2

1
4

−

E[Bk(U )(Xj −

Y = 0]
µj(U ))
|

|
The remaining detail is to upper bound
we have

{z

}

|
I1 −
|

{z
. According to Proposition A.1 and Bk = √LnB∗k,
I ∗1 |

}

I1 −
|

I ∗1 | ≤

Ln

p

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
2n

Ln

2n

Xi=1
1
2n

B∗k(Ui)[

µ1j(Ui)

−

2n

b

B∗k(Ui)µ1j(Ui)

µ1j(Ui)]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

p

Ln

(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
2n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ln

an

≤

p

+

Xi=1
2n

Xi=1
2n
1
2n

Xi=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B∗k(Ui)
|

| −

E[
]
B∗k(Ui)
|
|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

−

B∗k(Ui)µ1j(Ui)

p
Using Lemma A.1, we can verify

E[B∗k(U )µ1j(U )]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
LnE[
(cid:12)
]
B∗k(U )
|
|

+ an

p

.

E[B∗k(U )µ1j(U )]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

log n
Lnn ! ≥

ϑ

n−

1

−

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

and

1
2n

2n

Xi=1

B∗k(Ui)µ1j(Ui)

−

1
2n

2n

Xi=1

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

B∗k(Ui)
|

| −

.

r

E[B∗k(U )µ1j(U )]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
log n
Lnn ! ≥

.

r

E[
]
B∗k(Ui)
|
|
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

37

n−

ϑ.

1

−

In addition, recall the fact that E

B∗k(U )
|

| ≤

M2L−

n , which yields that

1

P

I1 −

 |

.

I ∗1 |

r

log n
n

+ L−
n

1/2

an

n−

ϑ.

1

−

! ≥

Thus we are guaranteed that

1
2n

2n

Xi=1

BiZi −
e

P

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

E[

BZ]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

C.6 Proof of Lemma B.6

Proof. By the optimality of

γ, we have

log n
n

.

r

+ L−
n

1/2

an

1

−

! ≥

4pLnn−

ϑ.

(C.9)

1
2

γ
(

−

ˇγ)⊤ A (

γ

b
ˇγ) + λn

−

γ(j)k2 ≤
k

(Aˇγ

−

b)⊤ (ˇγ

−

γ) + λn

p

Xj=1

ˇγ(j)k2.
k

(C.10)

b

b

Using the condition (B.16) and dropping the ﬁrst non-negative term in the left hand side of (C.10),
we are guaranteed that

p

Xj=1

b

λn

p

Xj=1

γ(j)k2 ≤
k

b

≤

=

p

Xj=1
λn
2

λn
2

(Aˇγ
k

b)(j)k2k

(ˇγ

−

γ)(j)k2 + λn

−

ˇγ(j)k2
k

p

Xj=1

S
Xj
∈

(ˇγ
k

−

(ˇγ
k

−

p

Xj=1

b
γ)(j)k2 + λn
λn
2

b
γ)(j)k2 +

Sc k
Xj
∈
Sc that

b

∈

It follows from the assumption ˇγ(j) = 0 for j

(ˇγ

γ)(j)k2 + λn

−

b

ˇγ(j)k2.
k

S
Xj
∈

b

p

Xj=1

ˇγ(j)k2
k

1
2

(ˇγ

Sc k
Xj
∈

γ)(j)k2 ≤

−

b

≤

1
2

1
2

S
Xj
∈

S
Xj
∈

(ˇγ
k

γ)(j)k2 +

−

(ˇγ
k

−

b
γ)(j)k2 +

(ˇγ
k

b
γ)(j)k2,

−

S
Xj
∈

S
Xj
∈

ˇγ(j)k2 −
k

Xj
S
∈
γ)(j)k2

(ˇγ
k

−

γ(j)k2
k

b

b

(C.11)

where we used the fact

ˇγ(j)k2 − k
k

(ˇγ

b
γ)(j)k2. From (C.10), we can also obtain that

−

1
2

γ
(

−

ˇγ)⊤ A (

γ

ˇγ)

−

≤

λn
b
2

(ˇγ
k

−

b
γ)(j)k2 + λn

b

b

≤

3λn
2

p

Xj=1

ˇγ(j)k2 −
k

λn

p

Xj=1

γ(j)k2
k

b

(C.12)

3
2

≤

S
Xj
∈
γ(j)k2 ≤ k

p

Xj=1
p

Xj=1

b
γ)(j)k2.

−

(ˇγ
k

b
38

By the restrictive eigenvalue condition of A, we know

1
2

γ
(

−

ˇγ)⊤ A (

γ

ˇγ)

−

≥

ζ
2 k

ˇγ

γ

2
2.
k

−

Together with (C.12) and (C.11), we further have

b

b

b

(ˇγ

Sc k
Xj
∈

−

γ)(j)k2


b

ζ

ˇγ
k

−

γ

2
2 ≤
k

3λn

b

p

Xj=1

(ˇγ
k

γ)(j)k2

−

b

−

(ˇγ
k

γ)(j)k2 +

(ˇγ
k

b
γ)(j)k2

−

γ)(S)k2
b
−
γ
k2,
b

b

= 3λn 


12λn

S
Xj
∈

≤

≤

S
Xj
∈
12λn√s
(ˇγ
k
12λn√s
ˇγ
k

−
which yields the ﬁrst conclusion in Lemma B.6. In fact, we also used the following relation

≤

2

γ)(j)k2


And the second conclusion holds since

(ˇγ
k

S
Xj
∈



−

b

s

≤

S
Xj
∈

(ˇγ
k

−

γ)(j)k

2
2 = s

(ˇγ
k

−

2
γ)(S)k
2.

b

b

(ˇγ
k

−

γ)(j)k1 +

(ˇγ

γ)(j)k1

−

ˇγ
k

γ

k1 =

−

b

≤

≤

≤

S
Xj
∈

Ln

p
4

S
Xj
∈

Ln

p
4

Xj
S
∈
ˇγ
sLnk

p

Sc k
Xj
∈
γ)(j)k2 +

b
(ˇγ
k

−

p

(ˇγ
k

b
γ)(j)k2
−

γ

b
k2.

−

b

Ln

(ˇγ

b
Sc k
Xj
∈

γ)(j)k2

−

b

C.7 Proof of Lemma B.7

Proof of Lemma B.7. Recall that

γ(Sc) = 0, thus

(Dn

max
j
1
≤
≤

p k

γ

e
−

D

γ)(j)k2 = max

1

j

p k

(Dn −

≤

≤

Then it suﬃces to show that

e

e

D)(j,S)

γ(S)k2.

e

max
j
1
≤
≤

(Dn −

p k

D)(j,S)

γ(S)k2 .

r

log p
n

+

Ln log p
n

+ L−
n

2d

,

(C.13)

with high probability. We use the same decomposition for Dn −
D in the Section C.3 and only
. Correspondingly, the expectation E[
] means conditional
i : Yi = 1
prove the counterpart to
·
}
{
expectation E[
Yi = 1]. We split the proof into the following two main steps.
·|

e

39

Step 1. upper bounding max1

≤

j

≤

(I1

1 −

p k

I∗1 + I2

1 −

I∗2 −

I∗3)(j,S)

γ(S)k2. Recall

(I1

1 −

I∗1 −

1
2

I∗3)(j,S)

γ(S)

e

(cid:13)
(cid:13)
(cid:13)
(cid:13)

2 ≤
+

(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

(I1

11 −

I∗1)(j,S)

γ(S)

12)(j,S)

γ(S)

e
2
(cid:13)
(cid:13)
I∗3)(j,S)
e
(cid:13)

1
2

14 −

13)(j,S)

γ(S)

e

2
(cid:13)
(cid:13)
(cid:13)

e

+

2
(cid:13)
(cid:13)
(I1
(cid:13)
(cid:13)
(cid:13)
γ(S)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

.

(cid:13)
(cid:13)
(I1
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(I1

Notice that

(I1

11)(j,S)

γ(S)

2
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
e
(cid:13)
θS(Ui)⊤ (X
Given Yi = 1 and Ui,
θ∗(u)
our assumption supu
[0,1] k
∈
e

p

=

Ln

n

1
n

(Xij −

µ1j(Ui))

θS(Ui)⊤ (X

µ1(Ui))S B∗i

.

Xi=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
µ1(Ui))S is a normal random variable with mean-zero. Due to
−
δs, together with Theorem 3.3, we have
k2 ≤

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

2

−

E1

θS(Ui)⊤ (X

(cid:20)(cid:16)

e

µ1(Ui))S

−

2

Ui

=

θS(Ui)⊤ΣSS(Ui)

θS(Ui)

(cid:17)

(cid:12)
(cid:12)

(cid:21)

≤

λ1k
e
2λ1

≤
. 2λ1

e

2
θS(Ui)
2
k
2
θS(Ui)
2 +
k
k
d
δs + sL−
e
n

e
(cid:16)

θS(Ui)
k
= O(1).
e

2
θ∗S(Ui)
2
k

−

(cid:17)

Let Ti,jk = B∗k(Ui)
have

θS(Ui)⊤ (X

µ1(Ui))S (Xij −

−

µ1j(Ui)) for 1

(cid:16)

(cid:17)

≤

j

≤

p and 1

k

≤

≤

Ln, then we

e

(cid:13)
(cid:13)
(cid:13)

(I1

11 −

I∗1)(j,S)

γ(S)

e

2 ≤

(cid:13)
(cid:13)
(cid:13)

Ln max
1
≤
≤

k

Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

Xi
∈I

1

Ti,jk −

.

E1[Ti,jk]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(C.14)

In addition,

≤

E1
E[Ti,jk])2 exp [η
(Ti,jk −
E1
T 2
Ti,jk −
i,jk exp [η
(cid:8)
|
E1
E1[Ti,jk]
)
exp(η
(cid:8)
|
|

E1[Ti,jk]
Ti,jk −
]
|
|
E1[Ti,jk]
+ [E1[Ti,jk]]2E1 {
]
|
T 2
]
Ti,jk|
i,jk exp [η
(cid:9)
|

E1[Ti,jk]
exp [η
Ti,jk −
]
|
|
+ [E1[Ti,jk]]2E1 {
.
]
Ti,jk|
exp [η
}
|
Ui] = Σj,S(Ui), we have
µ1(Ui))
|

(cid:9)

(cid:9)

(cid:1)

}

≤
Recall that E1[(Xij −

(cid:0)

(cid:8)
µ1j(Ui))(X
−

E1[Ti,jk]

E1

≤

B∗k(Ui)

θS(Ui)⊤ (X

µ1(Ui))S (Xij −

−

E

h
B∗k(Ui)

≤

≤

h
λ1 sup
u

[0,1] k
∈
1
n .

.L−

θS(Ui)⊤Σj,S(Ui)
e
(cid:12)
(cid:12)
k2E [B∗k(Ui)]
θS(u)
(cid:12)
(cid:12)
(cid:12)e
(cid:12)

i

e

40

µ1j(Ui))
i

Denote Hi =
for suﬃciently large C > 0, we have

θS(Ui)⊤ (X

−

µ1(Ui))S. Applying the second assertion of Lemma A.2, for any η >

E1

≤

e
T 2
i,jk exp [η
]
Ti,jk|
|
E1
(B∗k(Ui))2(Xij −
(cid:9)
(cid:8)
(cid:8)
(B∗k(Ui))2

E1

E

≤
. E

(cid:20)
(B∗k(Ui))2

(Xij −
h
(cid:16)
1
. L−
n .

µ1j(Ui))2H 2

i exp [η

µ1j(Ui))4e2η

|

(Xij −
|
Xij

µ1j (Ui)
|

−

µ1j(Ui))Hi|
]
(cid:9)
E
i e2η
H 4

Ui

1/2

Hi

|

Ui

|

i(cid:17)

(cid:21)

(cid:12)
(cid:12)

i

h

(cid:12)
(cid:12)

In fact, we also used the fact E[H 2(Ui)
Ui] and E[(Xij −
|
for η = 1/(2Cλ1) for suﬃciently large C > 0, it holds

(cid:2)

(cid:3)

µ1j(Ui))2] are both bounded. Moreover,

E1

eη

Ti,jk|

|

h

E1

E1

i

≤

≤

|

eη
h
eη(
|

(Xij

µ1j (Ui))Hi

|

−

Xij

µ1j (Ui)
|

−

i
2+H 2
i |

)

Xij

µ1j (Ui)
|

−

2

|

e2η
h

i
Ui

(cid:12)
(cid:12)

E1

e2η

|

Hi

2

|

Ui

i

h

(cid:12)
(cid:12)

1/2

i(cid:17)

(cid:21)

h

E1

E

≤
(cid:20)(cid:16)
= O(1).

Combing the results above, we conclude that

E1

(Ti,jk −

E[Ti,jk])2 exp [η

Ti,jk −

|

E[Ti,jk]
|

]

. L−

1
n .

According to Lemma A.1, we are guaranteed that

(cid:8)

(cid:9)

P





max
j,k (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

Xi
∈I

1

Ti,jk −

log p
nLn 

1

−

≥

.

r

Lnp−

ϑ.



E1[Ti,jk]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

In conjunction with (C.14), it follows that

P

max
p
j
1
≤
≤

For I1

12 and I1

13, we have

11 −

(I1
(cid:13)
(cid:13)
(cid:13)

I∗1)(j,S)

γ(S)

Ln log p

n ! ≥

.

r

Lnp−

ϑ.

1

−

2

(cid:13)
(cid:13)
(cid:13)

and

where

and

e

2 ≤

(cid:13)
(cid:13)
(cid:13)

(I1

12)(j,S)

γ(S)

e

(I1

13)(j,S)

γ(S)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

1
n

Ln max
1
≤
≤

k

Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n

Xi=1

n

Ei,jk

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

Fi,jk

Ln max
1
≤
≤

k

2 ≤

(cid:13)
(cid:13)
(cid:13)
µ1j(Ui))

Xi=1

Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
θS(Ui)⊤ (µ1(Ui)

,

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
µ2(Ui))S ,

−

e

Ei,jk = B∗k(Ui)(Xij −

Fi,jk = B∗k(Ui)(Xij −

e
µ1j(Ui))

θS(Ui)⊤ (X

µ1(Ui))S .

−

e

41

(C.15)

(C.16)

(C.17)

 
Due to the fact that

s
l=1(

ν⊤(l)B∗i )2

≤

1, we can verify that

P

max

E1

e
(cid:26)

(cid:16)

E2

i,jkeη

Ei,jk|

|

2

, E1

2

Fi,jk|

|

. L−

1
n .

i,jkeη
F 2
(cid:16)

(cid:17)

(cid:27)

(cid:17)

Combining with Lemma A.1, (C.16) and (C.17), we are guaranteed that

P

max
p
j
1
≤
≤

For

n(cid:13)
(cid:13)
(cid:13)

(I1

12)(j,S)

γ(S)

+

(I1

13)(j,S)

γ(S)

(cid:13)
(cid:13)
(cid:13)

e

o

2
(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

Ln log p

≤ r

n ! ≥

Lnp−

ϑ.

1

−

(C.18)

I1
14 −

I∗3/2 =

−

it is easy to obtain that

e

1
8n

E

1
8

Xi
∈I

1 h
(µ1(U )

nh

(µ1(Ui)

µ2(Ui))(µ1(Ui)

−

−

µ2(Ui))⊤

(BiB⊤i )

µ2(U ))(µ1(U )

µ2(U ))⊤

−

−

⊗

i

⊗

i
(BB⊤)

,

o

P

max
p
j
1
≤
≤

1
2

14 −

(I1
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)

I∗3)(j,S)

γ(S)

.

Ln log p

r

n ! ≥

Lnp−

ϑ.

1

−

Combining (C.16), (C.18) and (C.19), we have

e

P

max
p
j
1
≤
≤

Similarly, we also have

(I1

1 −

I∗1 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

P

max
p
j
1
≤
≤

(I2

1 −

I∗2 −

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Step 2. upper bounding max1

2 + I2
I1

2 =

+

1
2n

1
2n

then we have

1
2

1
2

I∗3)(j,S)

γ(S)

.

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)

e

I∗3)(j,S)

γ(S)

.

r

2

(cid:13)
(cid:13)
(cid:13)
e
(cid:13)
3 + I2
2 + I1
2 + I2

µ(Ui))(µ(Ui)

j

≤

≤

(I1

p k
(X i −

Xi
∈I

1 nh

(X i −

µ(Ui))(µ(Ui)

Xi
∈I

2 nh

Ln log p

r

n ! ≥

Lnp−

ϑ.

1

−

Ln log p

n ! ≥

Lnp−

ϑ.

1

−

3 + I1

4 + I2

4)(j,S)

−

−

µ(Ui))⊤

i

i

b
µ(Ui))⊤

b

⊗

⊗

γ(S)k2. Recall that
(BiB⊤i )
e

o

(BiB⊤i )

,

o

(C.19)

(C.20)

(C.21)

2 + I2

2)(j,S)

(I1
k

≤

Ln max
1
≤
≤

k

+ Ln max
1
≤

≤

k

+ Ln max
1
≤

≤

k

Xi
∈I

1

γ(S)k2
1
e
Ln (cid:12)
2n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
Ln (cid:12)
2n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
4n

Xi
∈I

2

Xi
∈I

1

(Xij −

µ1j(Ui))B∗k(Ui)

θ(Ui)⊤S (

µ(Ui)

µ2j(Ui))B∗k(Ui)

e
θ(Ui)⊤S (

b
µ(Ui)

(Xij −

Gi,jk(Ui)

1
4n

−

42

b

,

e
Gi,jk(Ui)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

2

−

−

µ(Ui))S(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
µ(Ui))S(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

 
 
 
 
where Gi,jk(Ui) = (µ1j(Ui)

−

µ2j(Ui))B∗k(Ui)

θ(Ui)⊤S (

µ(Ui)

µ(Ui))S. Notice that

−

θ(Ui)⊤S (

µ(Ui)

µ(Ui))S =

−

e

b

+

=

b
αl)⊤Bi

θl(Ui)

e

µl(Ui))

θl(Ui)

s

e
αl −
(

Xl=1
s
b
e
α⊤l Bi −
(

Xl=1
e
θ(Ui)⊤S

where

M.S = (

α1, ...,

αs) and

M.S = (

e
α1, ...,

αs). Then we have

f

M.S

e

⊤ Bi +
(cid:17)

M.S −
c

(cid:16)

θ(Ui)⊤S

e

M⊤.SBi
(cid:16)

f

,

(cid:17)

c

b

f
(Xij −

e

e

µ1j(Ui))B∗k(Ui)

s

Xl=1

[ˆµl(Ui)

−

µl(Ui)]

1

Xi
∈I
1
2n

b
1
2n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
1
(cid:12)
(cid:12)
2n
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

e
M.S

θl(Ui)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
⊤ Bi(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

f

(cid:16)

M.S −
c
M⊤.SBi

(Xij −

µ1j(Ui))B∗k(Ui)

θ(Ui)⊤S

µ1j(Ui))B∗k(Ui)

e
θ(Ui)⊤S

Xi
∈I

1

(Xij −

Xi
∈I

1

e

(cid:16)

f

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

From Proposition A.1, we know that
technique in Section D.3 to

M.S, we can show that

M.S −
k
c

M.SkF . √san. By utilizing the same chaining
f

P

max
p
j
1
≤
≤

Recall that

M.S −
c
(I1
(cid:13)
(cid:13)
(cid:13)

f
2 + I2
2)(j,S)

. anLns

log p

r

n ! ≥

p−

ϑsLn.

1

−

(C.22)

γ(S)

e

2

(cid:13)
(cid:13)
(cid:13)

3 + I2
I1

3 =

+

1
2n

1
2n

Xi
∈I

1 nh

(µ(Ui)

Xi
∈I

2 nh

(µ(Ui)

µ(Ui))(X i −

−

µ(Ui))⊤

b
µ(Ui))(X i −

−

µ(Ui))⊤

b

⊗

⊗

(BiB⊤i )

o

(BiB⊤i )

.

o

i

i

Similarly, we can verify

P

max
p
j
1
≤
≤

For

3 + I2

3)(j,S)

(I1
(cid:13)
(cid:13)
(cid:13)

4 + I2
I1

4 =

+

1
2n

1
2n

(µ(Ui)

Xi
∈I

1 h

(µ(Ui)

Xi
∈I

2 h

e

−

−

γ(S)

. anLn

log p

r

n ! ≥

p−

ϑLn.

1

−

(C.23)

2
(cid:13)
(cid:13)
(cid:13)

µ(Ui))(µ(Ui)

b
µ(Ui))(µ(Ui)

b

43

(BiB⊤i )

(BiB⊤i ),

⊗

⊗

−

−

µ(Ui))⊤

i

i

b
µ(Ui))⊤

b

 
 
it follows from Proposition A.1 that

4 + I2

4)(j,S)

γ(S)

(I1
(cid:13)
(cid:13)
(cid:13)
≤

Ln

1

k

i

≤

≤
Lnan max
≤
2n
1
≤
. Lnan max
1
i
≤
≤
√sLna2
n,

≤

≤

2
(cid:13)
(cid:13)
(cid:13)
2n

b

max
e
Ln,1
≤

i

−

B∗k(Ui)[ˆµj(Ui)
≤
(cid:12)
(cid:12)
θ(Ui)⊤S (
(cid:12)
(cid:12)
(cid:12)
µ(Ui)
(
(cid:12)e
2n k

−
µ(Ui))Sk2 k

µ(Ui)

b
−

(cid:12)
(cid:12)
θ(Ui)Sk2
(cid:12)
e

µj(Ui)]

θ(Ui)⊤S (

µ(Ui)

µ(Ui))S

e

b

µ(Ui))S

−

(cid:12)
(cid:12)
(cid:12)

(C.24)

holds with probability at least 1

−

sLnp−

ϑ. Combining (C.20)-(C.24), we have

max
j
1
≤
≤

(Dn −

p k

D)(j,S)

γ(S)k2 .

Ln log p
n

r

+ anLns

log p
n

,

r

holds with probability at least 1

e
ϑ
sLnp−

−

Lnp−

ϑsLn

Lnp−

ϑ.

−

−

Appendix D Proofs of auxiliary lemmas in Section C

D.1 Proof of Lemma C.1

Proof of Lemma C.1. We only prove the bound for
] to denote the conditional expectation E[
E1[
·|
·
1 be the unit sphere in RpLn, we denote the 1/8-covering set of SpLn
Let SpLn
17(pLn). It follows that for any ν
with N
≤
∈
E1[Z iZ ⊤i ], then we have
Let Qi = ZiZ⊤i −

−
1, there exist some νl such that

I1, and the case in

Y = 1].

SpLn

−

−

I2 is similar. Here we use

1 by
ν
k

ν 1, ..., ν N }
{
νlk2 ≤
1/8.
−

1
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Xi
∈I

1

≤

Qi(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

Note that

2 max
l
≤
≤

1

N (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

Xi
∈I

1

.

ν⊤l Qiνl(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

L−

1
n ν⊤l Qiνl =

ν⊤l [(X i −
(cid:16)
ν⊤li (X i −
= [

µ1(Ui))
µ1(Ui))]2

2

E1
B∗i )]
−
(cid:17)
E1[
ν⊤li (X i −

⊗

−

ν⊤l [(X i −
(cid:16)
µ1(Ui))]2

µ1(Ui))

2

⊗

B∗i )]
(cid:17)

νl(Ui) = ((νl)⊤(1)B∗i , ..., (ν l)⊤(p)B∗i )⊤ for νl = ((ν l)⊤(1), ..., (ν l)⊤(p))⊤. Let Rli =

e

νl(Ui)) given Ui and Yi = 1. Let σ2

li =

where
µ1(Ui)), then Rli ∼ N
then we have σ2
e
λ2

li ≤

e
ν l(Ui)⊤Σ(Ui)

(0,

νl(Ui)⊤(X i −
νl(Ui),

νl(Ui)⊤Σ(Ui)
e

e

e

e
E1[σ2
li]

E1

λ1

≤

≤

e
2
νl(Ui)
2k
k
k
p

Σ(Ui)

E1

ν⊤(j)B∗i

(cid:2)

e
Xj=1

k2
2

λ1E

≤

(cid:3)
. L−
n

1

2
νl(Ui)
2
k
k
p
(cid:2)
ν(j)k
e
k

(cid:3)
2
2 = L−

1
n .

Xj=1

(cid:20)(cid:16)

(cid:21)

(cid:17)

44

For η = 1/(8λ2M2), simple calculation gives E1[eηR2
have

li ] = O(1). Using H¨older’s inequality, we also

E1

R2

E1[R2
li]

2

exp

h(cid:0)
2 exp

≤
.E1

(cid:0)
σ4
li

li −
ηE1[R2
li]
R4
li
σ4
li

E1

(cid:1) (cid:16)
eηR2

li

h
Ui
|

η

R2
|
lieηR2
(cid:0)

li

li −
+

E1[R2
li]
|
(cid:1)i
R2
li

E1

2 E1

eηR2

li

(cid:1)
E1

R4

i
E1

(cid:0)
σ2
li

+

(cid:2)
2

(cid:3)(cid:1)

h

i(cid:17)

(cid:21)(cid:21)

Ui
|

(cid:0)
E1

(cid:3)

h

(cid:2)
e2ηR2

(cid:3)(cid:1)
li

Ui
|

1/2

2

+ L−
n

i(cid:17)

(cid:21)

(cid:20)

(cid:20)
E1
(cid:16)
(cid:2)
n . L−
+ L−
Invoking Lemma A.1, we can obtain that

≤
.E1

(cid:20)
σ2
li

σ2
li

E1

(Rli/σli)8

1
n .

2

(cid:2)

(cid:3)

P

max
j
1
≤
≤

N (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

n

Xi=1

ν⊤l Qiνl

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

. Ln

p log n

r

n ! ≥

n−

ϑpLn.

1

−

Next we prove the conclusion for matrix-vector-product.
ν

RpLn

1,

−

∈

It suﬃces to show that for any ﬁxed

i(cid:17)
ν(Ui) = (ν⊤(1)B∗i , ..., ν ⊤(p)B∗i )⊤, then notice that

h

1

Xi
∈I

Let

ν⊤

Z iZ ⊤i −
(cid:16)

E1

ZiZ⊤i

P

1
n

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

Lnp log n
n

.

r



≤



γ

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

n−

ϑpLn.

(D.1)

e

ν⊤Z iZ ⊤i

Denote Ri = (X i −

µ1(Ui))⊤

Ri|
(cid:16)
Also, we know that E1[R2
i |

Ui ∼ N

0,

µ1(Ui))
µ1(Ui))⊤

Bi)

⊗
θ(Ui).

e

=

γ = ν⊤ ((X i −
Ln (X i −

γ⊤ ((X i −

µ1(Ui))
Bi)
⊗
µ1(Ui))⊤
ν(Ui) (X i −
e
p
ν(Ui) and Si = (X i −
µ1(Ui))⊤
e
Ui ∼ N
, Si|
(cid:17)
λ1 and E1[S2
Ui] . λ1 since
i |
θ∗(u)
k

0,
e
(cid:16)
ν(u)
e
k2 ≤
k
d
n = O(1),
k2 + √pL−
e

ν(Ui)⊤Σ(Ui)

e
k2 .

ν(Ui)

θ(u)

Ui]
e
≤
sup
[0,1] k
u
∈

e

θ(Ui), then

e

θ(Ui)⊤Σ(Ui)

θ(Ui)

.

1 and

e

(cid:17)

which is true due to the assumptions supu
inequality, we have

e

[0,1] k
∈

θ∗(u)

k2 = O(1) and √pL−

d

n = o(1). Using H¨older’s

E1[RiSi])2eη

|

RiSi

−

E1[RiSi]

|

E1

(RiSi −

h
2 exp (η

E1[RiSi]
)
|
|
i eη
i S2

RiSi

|

|

R2

E1
h
(cid:16)
+ E1[R2

R2

≤
.E1

i S2

i eη

RiSi

|

|

i
+ (E1 [RiSi])2 E1

RiSi

eη

|

|

i ]E1[S2

i ]E1

i
eη

|

RiSi

.

|

h

i(cid:17)

(D.2)

Denote σ2
i =
distribution, it holds that

h
ν(Ui)⊤Σ(Ui)

i

h

i

ν(Ui). Then applying Lemma A.2 and moment formula of normal

e

E1

R4

e
i e2η

|

Ri

Ui

|

h

i

(cid:12)
(cid:12)

≤
. σ4

i =

32e2η2σ2

i

(ησ2

i )4 + 6(ησ2

i + 3σ4
i )2σ2
i
2

(cid:1)

(cid:0)

ν(Ui)⊤Σ(Ui)

ν(Ui)

,

(cid:16)

e
45

(cid:17)

e

 
where the second inequality holds since σi is bounded. Similarly, we can also verify that

E1

i e2η
S4

|

Si

|

Ui

.

θ(Ui)⊤Σ(Ui)

θ(Ui)

2

.

It follows from

θ(Ui)⊤Σ(Ui)

h

(cid:12)
θ(Ui) is bounded that
e
(cid:12)

i

(cid:16)

R2

i S2

i eη

RiSi
e

|

|

= E1

E1

R2

i S2

i eη

E1
e

h

(cid:17)

e

RiSi

|

Ui

|

i

E1

≤
. E1

. E1
. E1

h

h
E1

R4

i e2η

|

Ri

|

(cid:12)
(cid:12)
Ui

ii
E1

i e2η
S4

|

Si

|

Ui

1/2

(cid:20)(cid:16)

h

i
(cid:12)
ν(Ui)⊤Σ(Ui)
ν(Ui)
(cid:12)

h

i(cid:17)
θ(Ui)⊤Σ(Ui)

(cid:12)
(cid:12)

(cid:21)
θ(Ui)

h(cid:16)
ν(Ui)⊤Σ(Ui)

e
h
2
ν(Ui)
2
k
k
e

ν(Ui)
e
i
1
. L−
n .
e

(cid:17) (cid:16)

e

(cid:17)i

e

Then we take η = 1/(Cλ1) for suﬃciently large C > 0, it holds that

(cid:3)

(cid:2)

E1

RiSi

eη

|

|

≤
Substituting (D.3) and (D.4) into (D.2), we have

i(cid:17)

≤

h

i

h

E1
(cid:16)

h

e2ηR2

i

E1

e2ηS2

i

1/2

= O(1).

i

h

i(cid:17)

e
i +S2
i )

eη(R2

Applying Lemma A.1 again, we can prove (D.1) immediately.

E1

(RiSi −

E1[RiSi])2eη

|

RiSi

−

E1[RiSi]

. L−

1
n .

|

i

E1
(cid:16)

h

(D.3)

(D.4)

D.2 Proof of Lemma C.2 and C.3

Proof. The proofs of Lemma C.2 and C.3 are similar, here we only prove Lemma C.2.

For the ﬁrst assertion for operator norm of matrix, it suﬃces to show for any ﬁxed ν
that

∈

SpLn

1 such

−

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1

Xi
∈I
holds with probability at least 1
−
(ν⊤(1)B∗i , ..., ν ⊤(p)B∗i )⊤. Then we have

n

n−

ν⊤

[(X i −

µ1(Ui))(µ1(Ui)

µ2(Ui))⊤]

(BiB⊤i )

ν

. Ln

−

⊗

o
ϑpLn. For ν = (ν⊤(1), ..., ν ⊤(p))⊤ ∈

(cid:12)
(cid:12)
(cid:12)
(cid:12)
SpLn
(cid:12)
(cid:12)

p log n
n

,

r

1, we write

−

ν(Ui) =

e

Let

and σ2

i =

ν⊤

n

[(X i −
= Ln(

µ1(Ui))(µ1(Ui)

ν(Ui)⊤(µ1(Ui)

µ2(Ui))⊤]

−
µ2(Ui)))[

ν

⊗

(BiB⊤i )
o
µ1(Ui))].
ν(Ui)⊤(X i −

−

e

e

Ti = (

ν(Ui)⊤(µ1(Ui)

µ2(Ui)))[

ν(Ui)⊤(X i −

−

µ1(Ui))],

ν(Ui)⊤Σ(Ui)

ν(Ui), then for η > 0 it holds that

e

e

E1[T 2

i eη
e

|

Ti

e
|] . E1

2E

≤
. E

2

eη

|

eν(Ui)⊤(X i

µ1(Ui))
|

−

(cid:21)

ν(Ui)⊤(X i −
i + φ2σ4
σ2
i

η2σ2
i
e
2

(cid:20)(cid:16)
e

µ1(Ui))
(cid:17)

(cid:20)
(cid:0)
2
ν(Ui)
2
k
k

(cid:21)

(cid:1)

. L−

1
n ,

(cid:2)

e

(cid:3)

46

where the ﬁrst inequality follows from
A.1, we are guaranteed that

ν(u)⊤(µ1(u)
|

−

µ2(u))
|

is uniformly bounded. By Lemma

P

1
n

p log n

.

1

n−

ϑpLn.

(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
For the assertion for matrix-vector-product, it suﬃces to show

n 

Xi
∈I

r



≥

−

1

e
Ti(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ν⊤

[(X i −
n

Xi
∈I
holds with probability at least 1

1

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ1(Ui))(µ1(Ui)

µ2(Ui))⊤]

⊗

−

n−

ϑpLn. Notice that

−
µ1(Ui))(µ1(Ui)

pLn log n
n

.

r

(BiB⊤i )

γ

o

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

and

=

p

(cid:16)

e

ν⊤

[(X i −
ν(Ui)⊤(X i −

n
Ln

µ2(Ui))⊤]

−

⊗

(BiB⊤i )

γ

µ1(Ui))

θ(Ui)⊤(µ1(Ui)

(cid:17) (cid:16)

e

o
µ2(Ui))
e
(cid:17)

,

−

Denote Ri =

θ(Ui)⊤(µ1(Ui)
(cid:12)
(cid:12)
(cid:12)e
ν(Ui)⊤(X i −

µ1(Ui))

−

µ2(Ui))
(cid:12)
(cid:12)
(cid:12)

θ(Ui)⊤(µ1(Ui)

(cid:0)

e

E1

R2

i eη

|

Ri

|

. E1

(cid:1) (cid:16)
e
ν(Ui)⊤(X i −

h

i

(cid:20)(cid:16)
1
n .

. L−

e

≤

δp sup
[0,1] k
u
∈

θ(u)

k2 = O(1).

−

e
µ2(Ui))
(cid:17)
eν(Ui)⊤(X i
eη

2

|

µ1(Ui))
(cid:17)

µ1(Ui))
|

−

(cid:21)

. Then we can get

Applying Lemma A.1, we can prove the desired result.

D.3 Proof of Lemma C.4

Proof. Let

M = (

M1, ...,

Mp)

RLn

p where the j-th column Mj = 1
2 (

α1j +

×

α2j) and

f

f
α1j =

E[BB⊤]
f
(cid:17)
(cid:16)
M1, ...,
Similarly, we denote

M = (

E[BXj|
Mp)

−

e

Y = 1],

α1j =

1

−

E[BB⊤]
(cid:17)
(cid:16)

E[BXj|
Y = 0].
e
e
Mj = 1
2 (

α1j +

α2j) and

RLn

×

p, where the j-th column

e

∈

∈
1

α1j =

BiB⊤i 

Recalling the approximation error bound in the proof of Proposition A.1:

BiXij,

α2j =

Xi
∈I

Xi
∈I

Xi
∈I









b

b

1

1

2

c

1
n

−

1
c
1
n

c
BiB⊤i 


1
n

1
−
c

b
b
BiXij.

1
n

Xi
∈I

2

α⊤1jB(u)

sup
[0,1] |
u
∈

µ1j(u)
|

−

. L−

d
n ,

α⊤2jB(u)

sup
[0,1] |
u
∈

µ2j(u)
|

−

. L−

d
n .

It means that

e
M⊤Bi −
k
f

µ(Ui)

k2 . √pL−
Mj −
:=
k
c

n

A

d

n . In addition, we deﬁne the good event:

e

Mjk2 . an :
f

47

for 1

j

≤

≤

p

,

o

Ln log n

where an =
Lemma C.3 under the good event

n + L−

q

.

A

n . By Proposition A.1, we know that P(
d

c)

≤

Lnn−

ϑ. Next we prove

A

We ﬁrst prove the bound for the operator norm of the matrix. Let Ai = [(
µ1(Ui))⊤]
⊗
SpLn
ν, ξ
−

µ(Ui))(X i −
(BiB⊤i ). According to the proof of Lemma C.1, it suﬃces to show that for any ﬁxed
1 such that

µ(Ui)

−

b

∈

holds with probability at least 1

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

ν⊤Aiξ

. anL3/2
n p

1

Xi
∈I
n−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ϑpLn. Notice that
(cid:12)
(cid:12)

log n
n

r

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

1

≤

1
n

= Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

1

e
ξ(Ui)⊤(X i −
[
e

ν⊤i (
[

µ(Ui)

µ(Ui))][

i (X i −
ξ⊤

−

b
µ1(Ui))][

ν(Ui)⊤(

e
M⊤Bi −
f
ν(Ui)⊤(

M

−

e
µ1(Ui))][

ξ(Ui)⊤(X i −
[
e

Xi
∈I

1

ν⊤Aiξ

1
n

1

Xi
∈I
1
n

Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
+ Ln (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ1(Ui))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ(Ui))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

c
ξ(Ui) = (ξ⊤(1)B∗i , ..., ξ⊤(p)B∗i )⊤ for ν = (ν⊤(1), ..., ν ⊤(p))⊤ and

f

e

(D.5)

,

ν(Ui) = (ν⊤(1)B∗i , ..., ν ⊤(p)B∗i )⊤ and
where
ξ = (ξ⊤(1), ..., ξ⊤(p))⊤. Also, we know that

e

e

ξ(Ui)⊤(X i −
e

µ1(Ui))
Ui ∼ N
|

0,

ξ(Ui)⊤Σ(Ui)

ξ(Ui)

.

e

(cid:16)
(cid:17)
M⊤Bi −
µ(Ui))]. Apply the second assertion in Lemma
f
µ1(Ui))
|

e

µ1(Ui))][
ξ(Ui)⊤(X i −
Denote Ti = [
A.2 with any constant η > 0, we get
e
e
i eη
T 2

i eη
T 2

E1

E1

Ti

|

|

ν(Ui)⊤(

h
. pL−
n

2d

i
E1

2pL−
n

2d

E

≤

. pL−
n

2d

E

eξ(Ui)⊤(X i
|

−

h

≤
ξ(Ui)⊤(X i −
(cid:20)(cid:16)
⊤Σ(Ui)
eξ(Ui)
η2eξ(Ui)
e
e
2

µ1(Ui))
(cid:17)

(cid:18)

(cid:20)
ξ(Ui)⊤Σ(Ui)
h
e

ξ(Ui)
i

e

where we also used the fact

2

i
eξ(Ui)⊤(X i
|

eη

µ1(Ui))
|

−

ξ(Ui)⊤Σ(Ui)

ξ(Ui) + η2

e
. pL−

2d
n L−

1
e
n ,

(cid:21)
ξ(Ui)⊤Σ(Ui)

ξ(Ui)

2

(cid:16)

e

(cid:19)(cid:21)

(cid:17)

e

ξ(Ui)⊤Σ(Ui)

ξ(Ui)

≤

2
ξ(Ui)
2 ≤
k

λ1k
e

Applying Lemma A.1, we can verify that

e

e

λ1k

B∗i k

2
2

p

Xj=1

ν(j)k
k

2
2 ≤

λ1.

ξ(Ui)⊤(X i −
[
e

Xi
∈I

1

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

µ1(Ui))][

ν(Ui)⊤(

M⊤Bi −
f

. pL−
n

d

Ln log n
n

,

r

(D.6)

µ(Ui))]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

48

holds with probability at least 1
We deﬁne a matrix set as

−

n−

ϑpLn. Next we proceed to bound the second term in (D.5).

where an = O(
RLn, 1
ζ ℓ
a set
p
{
∈
ζℓ
Mj −
that
k2 ≤
k
Ξ. And for any M
It follows that for any M

(cid:8)
Ln log n/n+L−
nM Ln :
an}
≤
M √Lnan
Ξ, there exists some 1

ℓ
≤
n−

k2 ≤

ζℓ
k

∈

Ξ

∈

Ξ =

M

RLn

p :

×

∈
d
n ) and Mj is the j-th column of M. For each 1

(cid:9)

an for j = 1, 2, ..., p

,

Mjk2 ≤
k

such that there exists some 1

1. It means that we can ﬁnd a subset Ξ
nM pLn such that

ℓ

≤

≤

j

′

≤
ℓ
≤
≤
Mℓ : 1
=
ℓ
{
≤
Mℓ
Mjk2 ≤
j −
k

p, we may ﬁnd
≤
nM Ln satisfying
nM pLn
≤
an√Lnn−

} ⊆
M .

µ1(Ui))][

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

1

ξ(Ui)⊤(X i −
[
e
1
n

≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ξ(Ui)⊤(X i −
[
e

1

Xi
∈I
1
n

1

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

ξ(Ui)⊤(X i −
[
e
ξ(Ui)⊤(X i −
[
Xi
∈I
e
+ an√pLnn−

1

1
n

M max
i
∈I

1

µ1(Ui))][

e

ν(Ui)⊤M⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ν(Ui)⊤(Mℓ)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

e
µ1(Ui))][

ν(Ui)⊤(Mℓ

e

µ1(Ui))][

ν(Ui)⊤(Mℓ)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

e
ξ(Ui)⊤(X i −
(cid:12)
(cid:12)
(cid:12)e

µ1(Ui))
(cid:12)
(cid:12)
(cid:12)

p

1/2

where the last inequality is true since

M)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

ν(Ui)⊤(Mℓ)⊤Bi

ν(Ui)

k2 

((Mℓ

Mj)⊤Bi)2



≤

√pLnann−

M .

j −

≤ k

(cid:12)
(cid:12)
(cid:12)
µ1(Ui))][

Xj=1


e
ν(Ui)⊤(Mℓ)⊤B∗i ]. Then we have



Let Vℓ,i = [

(cid:12)
(cid:12)
(cid:12)e
ξ(Ui)⊤(X i −
e

e
ξ(Ui)⊤(X i −
[
e

1
n

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
M

Xi
∈I
1
n

≤

≤

1

1

∈

≤

Xi
∈I

ξ(Ui)⊤(X i −
[
e
1
n

Ξ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
max
nM pLn (cid:12)
ℓ
(cid:12)
≤
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ξ(Ui)⊤(X i −
µ1(Ui))
ξ(Ui)⊤(X i −
Ui ∼ N
e
|
e

Vℓ,i(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

1

49

µ1(Ui))][

ν(Ui)⊤(

M

−

c

e
µ1(Ui))][

M)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

f

e
M
+ an√pLnn−

ν(Ui)⊤M⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

log n,

p

0,

ξ(Ui)⊤Σ(Ui)

ξ(Ui)

,

(cid:16)

e

(cid:17)

e

where the last inequality follows from the bound for the maximal of n independent Gaussian random
variables and the variance of

µ1(Ui)) is bounded almost surely. Notice that

1For each coordinate of Mj , we can divide the interval [−an, an] into nM small intervals with equal length 2an/N M .

(D.7)

ξ(Ui)⊤Σ(Ui)

where E[
second assertion of Lemma A.2 with any η > 0, together with
√pan, we can verify that

ξ(Ui)] . L−
n

ξ(Ui)⊤Σ(Ui)

and

e

e

e

e

1

ξ(Ui) is almost surely bounded. Using the
k2 ≤
k2 ≤ k

Mℓ
k

ν(Ui)

ν(Ui)

kF k

Mℓ

V 2
i eη
h
Applying Lemma A.1, it is easy to show

E

Vi

|

|

i

. pa2

nL−

1
n ,

e

e

1
n

max
ℓ
≤

1

≤

nM pLn (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xi
∈I

1

ξ(Ui)⊤(X i −
[
e
n−

with probability at least 1
into (D.5), we can ﬁnish the proof.

−

µ1(Ui))][

ν(Ui)⊤(Mℓ)⊤Bi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

. pan

Ln log n
n

r

(D.8)

ϑLnp. By choosing suﬃciently large M and substituting (D.6)-(D.8)

For the case in the matrix-vector-product, we notice that

γ =

ν⊤Ai

θ(Ui)⊤(X i −
θ(Ui)⊤(X i −
e
By utilizing the same chaining technique and applying Lemma A.1, we can prove
e

M⊤Bi −
M
f
−

µ1(Ui))][
µ1(Ui))][

ν(Ui)⊤(
e

M)⊤Bi].

ν(Ui)⊤(

µ(Ui))]

Ln[

Ln[

p

p

+

f

c

e

e

ν⊤Ai

γ

. anLnp

log n
n

,

r

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

Xi
∈I

1

n−

ϑpLn. Then we can ﬁnish the proof.

holds with probability at least 1

D.4 Proof of Lemma C.5

Proof. We prove Lemma C.5 under the good event
deﬁned in Section D.4. We prove the bound
for the operator norm of the matrix, and ℓ2 norm bound for the matrix-vector-product is similar.
It suﬃces to show that for any ﬁxed ν, ξ
−

1 such that

SpLn

A

∈

1
n

ν⊤G(Ui)ξ

1

−

Xi
∈I

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
holds with probability at least 1
(cid:12)
µ(u) =
and write
deﬁnition of G(Ui), we have

1
n

−

ν⊤G(Ui)ξ

Xi
∈I

2

CL3/2
n p

≤

log n

r

n  r

Ln log n
n

d

+ L−
n

,

!

n−

ϑpLn. To simplify notations, we denote δ(Ui) = µ1(Ui)

M⊤B(u), where the j-th column of

M equals to (

α1j +

µ2(Ui)
α2j)/2. Recall the

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

b

c
G(Ui) =

δ(Ui) (

µ(Ui)

c

b

b

µ(Ui))⊤

−

⊗

(cid:17)

BiB⊤i
(cid:16)

⊤

M⊤B(Ui)

(cid:17)

⊗

BiB⊤i
(cid:16)

(cid:19)

(cid:17)

=

(cid:16)

(cid:18)

δ(Ui)

M⊤B(Ui)
b
(cid:16)

c

−

δ(Ui)

M⊤B(Ui)

+

|

(cid:18)

|

(cid:16)

f

G1(Ui)
f

{z
µ(Ui)

⊤

(cid:17)

−

G2(Ui)

{z

50

(cid:17)

}

⊗

(cid:19)

BiB⊤i
(cid:16)

.

(cid:17)

}

It yields that

Xi
∈I

1

Xi
∈I

1

Xi
∈I

1

1
n

1
n

1
n

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
|

ν⊤G(Ui)ξ

−

1
n

ν⊤G1(Ui)ξ

−

Π1

2

Xi
∈I
1
n

Xi
∈I

2

ν⊤G(Ui)ξ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
ν⊤G1(Ui)ξ

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
}
ν⊤G1(Ui)ξ

ν⊤G2(Ui)ξ

{z

−

1
n

Π2

{z

Xi
∈I

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
}

(D.9)

.

Denote

ν(Ui) = (ν⊤(1)B∗i , ..., ν ⊤(p)B∗i )⊤ and

ξ(Ui) = (ξ⊤(1)B∗i , ..., ξ⊤(p)B∗i )⊤. Then we have

e

e

ν⊤G1(Ui)ξ =

ν(Ui)⊤δ(Ui)

ξ(Ui)⊤

M

(cid:16)

e

(cid:17) (cid:18)
e

(cid:16)

c

f

M

−

⊤ Bi
(cid:17)

(cid:19)

(cid:16)

=: Ti

M

M

.

−

(cid:17)

c

f

Here we use the same notation Ξ in Section D.3. From Proposition A.1, we know that

1
n

−

Π1 =

≤

1
n

1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
M

Xi
∈I
1
n

∈

Ti

M

M

−

(cid:16)

(cid:17)

c
f
Ti (M)

1
n

−

Xi
∈I

2

Ti

M

Xi
∈I

2

(cid:16)

c
Ti (M)

−

M

f

(cid:12)
(cid:12)
(cid:17)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(Ti (M)

−

E [Ti (M)])

(Ti (M)

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
−

1
n

2

Xi
∈I
1
n

Xi
∈I

1

Xi
∈I

1

= sup
M

∈

sup
M

Ξ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ξ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
Ξ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

1
n

Ti(M)

E [Ti(M)]

+ sup
M

Ti(M)

1

∈

≤

−

Xi
∈I

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
}
In fact, the second equality holds since the randomness of Ti(M) is from Ui and Ui is independent
Bik2 ≤
1, and following
δp,
of Yi. Using the facts
k2 ≤
k
the chaining arguments in Section D.3, we have

ν(Ui)
k

δ(Ui)
k

ξ(Ui)
k

Ξ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k2 ≤

k2 ≤

√Ln,

1 and

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
}

Xi
∈I

Π11

{z

{z

−

|

|

∈

2

e

e

Π11 ≤

1

≤

max
ℓ.nM pLn (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
n

Ti

Mℓ

Xi
∈I

1

(cid:16)

(cid:17)

−

E

Ti

Mℓ

h

(cid:16)

(cid:17)i

51

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ an√pLnn−

M ,

(D.11)

(D.10)

E [Ti (M)])

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
E [Ti(M)]

.

where an = O(

Ln log n/n + L−

n ). In addition, notice that

d

p

E

Ti

Mℓ

2

(cid:16)

h

(cid:16)

(cid:17)i(cid:17)

≤

≤

≤

≤

E[(

ν(Ui)⊤δ(Ui))2]E[(

ξ(Ui)⊤(Mℓ)⊤B∗i )2]

E

2
δ(Ui)
2
k

2
ν(Ui)
2k
e
k
k
2
pa2
pδ2
ν(Ui)
(cid:2)
2
n
k
k
e

E

Mℓ
k

E
e

h

(cid:3)

2
ξ(Ui)
2k
k

B∗i k

2
2

e

i

pδ2

pa2
n

. pδ2

pa2

(cid:2)

E

p
e

Xj=1

1
n ,
nL−

(cid:3)
2
νjk
2k
k

B∗i k

2
2



and

E

T 2
i

Mℓ

= E

h

(cid:16)

(cid:17)i

2

ν(Ui)⊤δ(Ui)

ξ(Ui)⊤(Mℓ)⊤B∗i
(cid:16)
ν(Ui)⊤δ(Ui)

2
e

(cid:17)

2

(cid:21)

(cid:17)

(cid:21)

(cid:17)

(cid:20)(cid:16)
E
pa2
e
n
(cid:20)(cid:16)
1
n .
nL−
e

pa2

≤
. pδ2

In fact, we used
Thus we have

Mℓ
k

2
ξ(Ui)
2 =
k

Mℓ
k

2
F k
k

2
ξ(Ui)
2 ≤
k

pan and

Bik
k

2
2 ≤

Ln in the relations above.

e

2

E

Ti

Mℓ

Ti(Mℓ)

eη

|

−

E[Ti(Mℓ)]

|

h
T 2
i
h
E

(cid:16)
Mℓ

(cid:17)i(cid:17)
eη
|

Ti(Mℓ)
|

+

E

(cid:16)
Ti

(cid:17)
Mℓ

2

i

(cid:16)

(cid:21)
Mℓ

(cid:16)

(cid:17)i(cid:17)

Ti
h

2

E[eη

|

E[Ti(Mℓ)]

|]

(cid:27)

e
Mℓ

E

Ti

(cid:20)(cid:16)
2eη
|

(cid:17)
(cid:16)
E[Ti(Mℓ)]
|

−

E

(cid:26)
+

Mℓ

≤

.E

T 2
i
h
2E

≤

"

(cid:16)
e
E
.pLna2
n

pLnδ2

pa2
n

≤

2

#

(cid:19)

(cid:16)
(cid:17)i
(cid:16)
ν(Ui)⊤δ(Ui)

h
2

(cid:17)i(cid:17)

(cid:16)
ξ(Ui)⊤

Mℓ

⊤ Bi

(cid:18)

(cid:17)

e
ν(Ui)⊤δ(Ui)

2

(cid:20)(cid:16)
p

(cid:21)
2

(cid:17)
ν⊤(j)B∗i

e

E

Xj=1

(cid:20)(cid:16)

(cid:21)

(cid:17)

(cid:17)
(cid:16)
pa2
pLnδ2
n

E

≤

. pδ2

pa2
n

p

Xj=1

2
ν(Ui)
2
k
k
(cid:2)

e
νjk
k

(cid:3)
2 = pδ2
2

pa2
n.

where the second inequality holds since Ti
obtain that

Mℓ

is bounded. In accordance with Lemma A.1, we

P

Π11 . panL3/2

n

r

(cid:0)
log n
n

(cid:1)

+ 2an√pLnn−

M

1

−

! ≥

n−

ϑpLn.

(D.12)

Similar bound also holds for Π12, in conjunction with (D.10)-(D.12) and proper choice for M , we
are guaranteed that

P

Π1 . panL3/2

n

log n

r

n ! ≥

n−

ϑpLn.

1

−

(D.13)

52

 
 
Algorithm 1: ISTA with backtracking line-search

RpLn, number of iterations T , shrinking rate ρ

Input: Initial point γ0 ∈
(0, 1).
size η0 ∈
1 do
for t = 0, 1, ..., T
g(γt) = Dnγt
Compute the gradient:
Find the smallest nonnegative integer it such that with η = ρitηt
−

bn.

∇

−

−

1

(0, 1), initial step

∈

g(Qη(γt))

≤

g(γ t) + (Qη(γt)

γt)⊤

∇

−

g(γt) +

1
2η k

Qη(γt)

γt

2
2.
k

−

Set ηt = ρitηt
−

1 and update γt+1 = Qηt (γt).

end for
Output: The ﬁnal solution γ⊤.

Now let Wi =
tween Ui and Yi that

(cid:0)

e

ν(Ui)⊤δ(Ui)

ξ(Ui)⊤[

M⊤Bi −
f

(cid:1) (cid:16)
e

µ(Ui)]

, then it follows from the dependence be-

(cid:17)

1
n

Π2 ≤ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
M⊤j B(u)
Since Wi is bounded and supu |

Xi
∈I

1

Wi −

−

1
n

+

E[Wi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
. L−

µj(u)
|

Wi −

Xi
∈I

2

.

E[Wi]
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

d
n ≤

an, we have

(Wi −

E

h

f
E[Wi])2 eη
|

Wi

−

E[Wi]

|

i

. E[W 2

i ] + (E[Wi])2
E[
M⊤Bi −
k
E[sup
f
u k
1
n .
nL−
f

M⊤B(u)

ppa2

. δ2
p
δ2
p

≤
. δ2

2
2
µ(Ui)
ξ(Ui)
2]
2k
k
k
2
2
µ(u)
ξ(Ui)
2]
2k
e
k
k

−

e

Applying Lemma A.1, we get

P

Π2 . pan

log n

r

n ! ≥

n−

ϑpLn.

1

−

(D.14)

Plugging (D.14) and (D.13) into (D.9), we ﬁnish the proof Lemma C.3.

Appendix E Iterative Shrinkage Thresholding Algorithm

Next we take ISTA as an example to illustrate the optimization procedure to solve (2.9). Denote
RpLn, ISTA approach updates the solution through
g(γ) = 1

b⊤n γ. Given a point γ

2 γ⊤Dnγ

−

∈

53

 
solving the following subproblem

γ+ = Qη(γ)

= arg min
z

RpLn 


∈

g(γ) + (z

γ)⊤

∇

−

g(γ) +

1
2η k

z

γ

2
2 + λn
k

−

p

Xj=1

z(j)k2
k


= arg min
z


RpLn 


∈

1
2η

p

Xj=1

z(j) −
k

(γ

η

∇

−

2

g(γ))(j)k

2 + λnk

z(j)k2




,

where η is the step size. The solution of the subproblem is given by the soft-thresholding operator,
that is





(γ+)(j) =

(γ
(γ
k

η
η

g(γ))(j)
∇
g(γ))(j)k2
∇

−
−

max

0,

(γ
k

η

g(γ))(j)k2 −

∇

−

ηλn

.

(cid:8)

(cid:9)

The step size η is determined by a backtracking line-search (Beck and Teboulle, 2009) such that

g(γ +)

g(γ) + (γ+ −

≤

γ)⊤

∇

g(γ) +

1
2η k

γ+ −

γ

2
2.
k

Dnk2, whereas it usually leads a very small step sizes and slow
Another simple choice for η is 1/
k
convergence (Qin et al., 2013). For the ease of reference, we provide the detailed procedure in
Algorithm 1.

54

