The Adaptive Dynamic Programming Toolbox

Xiaowei Xing and Dong Eui Chang

1

0
2
0
2

c
e
D
9
2

]

C
O
.
h
t
a
m

[

1
v
4
5
6
4
1
.
2
1
0
2
:
v
i
X
r
a

Abstract—The paper develops the Adaptive Dynamic Program-
ming Toolbox (ADPT), which solves optimal control problems
for continuous-time nonlinear systems. Based on the adaptive
dynamic programming technique, the ADPT computes optimal
feedback controls from the system dynamics in the model-
based working mode, or from measurements of trajectories
of the system in the model-free working mode without the
requirement of knowledge of the system model. Multiple options
are provided such that the ADPT can accommodate various
customized circumstances. Compared to other popular software
toolboxes for optimal control, the ADPT enjoys its computational
precision and speed, which is illustrated with its applications to
a satellite attitude control problem.

Index Terms—Optimal control, adaptive dynamic program-

ming, software toolbox.

I. INTRODUCTION

O PTIMAL control

is an important branch in control
engineering. For continuous-time dynamical systems,
ﬁnding an optimal feedback control involves solving the so-
called Hamilton-Jacobi-Bellman (HJB) equation [1]. For linear
systems, the HJB equation becomes the well-known Riccati
equation which results in the linear quadratic regulator [2].
For nonlinear systems, solving the HJB equation is generally
a formidable task due to its inherently nonlinear nature. As
a result, there has been a great deal of research devoted to
approximately solving the HJB equation. Al’brekht proposed
a power series method for smooth systems to solve the HJB
equation [3]. With the assumption that the optimal control and
the optimal cost function can be represented in Taylor series,
by plugging the series expansions of the dynamics, the cost
integrand function, the optimal control and the optimal cost
function into the HJB equation and collecting terms degree
by degree, the Taylor expansions of the optimal control and
the optimal cost function can be recursively obtained. Similar
ideas can be found in [4] and [5]. An approach to approxi-
mately solve the HJB based on the grid-based discretization of
the state and time space is studied in [6]. A recursive algorithm
is developed to sequentially improve the control law which
converges to the optimal one by starting with an admissible
control in [7]. This recursive algorithm is commonly referred
to as policy iteration (PI) and can be also found in [8]–[10].
The common limitation of these methods is that the complete
knowledge of the system is necessary.

In the past few decades, reinforcement learning (RL) [11]
has provided a means to design optimal controllers in an adap-
tive manner from the point of learning. Adaptive/approximate
dynamic programming (ADP), which is an iterative RL-based
adaptive optimal control design method, has been proposed in

The authors are with the School of Electrical Engineering, Korea Advanced
Institute of Science and Technology, Daejeon 34141, Republic of Korea
(email: xwxing@kaist.ac.kr; dechang@kaist.ac.kr).

[12]–[14]. An ADP strategy is presented for nonlinear systems
with partially unknown dynamics in [12], and the necessity of
the knowledge of system model is fully relaxed in [13] and
[14].

Together with the growth of optimal control theory and
methods, several software tools for optimal control have been
developed. Notable examples are Nonlinear Systems Toolbox
[15], Control Toolbox [16], ACADO [17], its successor ACA-
DOS [18], and GPSOP-II [19]. A common feature of these
packages is that system equations are used in them. Besides,
optimal controls generated by [16]–[19] are open-loop such
that an optimal control is computed for each initial state.
So, if the initial state changes, optimal controls should be
computed again. In contrast, the Nonlinear Systems Toolbox
[15] produces an optimal feedback control by solving the HJB
equation.

this paper

The primary objective of

is to develop a
MATLAB-based toolbox that solves optimal feedback con-
trol problems computationally for nonlinear systems in the
continuous-time domain. More speciﬁcally, employing the
adaptive dynamic programming technique, we derive a com-
putational methodology to compute approximate optimal feed-
back controls, based on which we develop the Adaptive
Dynamic Programming Toolbox (ADPT). The ADPT supports
two working modes: the model-based mode and the model-free
mode. The knowledge of system equations is required in the
model-based working mode. In the model-free working mode,
the ADPT produces the approximate optimal feedback con-
trol from measurements of system trajectories, removing the
requirement of the knowledge of system equations. Moreover,
multiple options are provided such that the user can use the
toolbox with much ﬂexibility.

The remainder of the paper is organized as follows. Sec-
tion II reviews the standard optimal control problem for a
class of continuous-time nonlinear systems and the model-free
adaptive dynamic programming technique. Section III provides
implementation details and software features of the ADPT. In
Section IV, the ADPT is applied to a satellite attitude control
problem in both the model-based working mode and the
model-free working mode. Conclusions and potential future
extensions are given in Section V. The codes of the ADPT are
available at https://github.com/Everglow0214/The Adaptive
Dynamic Programming Toolbox.

II. REVIEW OF ADAPTIVE DYNAMIC PROGRAMMING

We review the adaptive dynamic programming (ADP) tech-
nique to solve optimal control problems [20]. Consider a
continuous-time nonlinear system given by

˙x = f (x) + g(x)u,

(1)

 
 
 
 
 
 
where x ∈ Rn is the state, u ∈ Rm is the control, f : Rn →
Rn and g : Rn → Rn×m are locally Lipschitz continuous
mappings with f (0) = 0. It is assumed that the system (1)
is stabilizable at x = 0 in the sense that the system can
be locally asymptotically stabilized by a continuous feedback
control. To quantify the performance of a control, an integral
cost associated with the system (1) is given by
∞

J(x0, u) =

Z
0

(q(x(t)) + u(t)T Ru(t)) dt,

(2)

where x0 = x(0) is the initial state, q : Rn → R≥0 is a
positive deﬁnite function and R ∈ Rm×m is a symmetric,
positive deﬁnite matrix. A feedback control u : Rn → Rm is
said to be admissible if it stabilizes the system (1) at the origin,
and makes the cost J(x0, u) ﬁnite for all x0 in a neighborhood
of x = 0.

The objective is to ﬁnd a control policy u that minimizes
J(x0, u) given x0. Deﬁne the optimal cost function V ∗ :
Rn → R by

V ∗(x) = min
u

J(x, u)

for x ∈ Rn. Then, V ∗ satisﬁes the HJB equation

0 = min

{∇V ∗(x)T (f (x) + g(x)u) + q(x) + uT Ru},

u

and the minimizer in the HJB equation is the optimal control
which is expressed in terms of V ∗ as

2

Algorithm 1 Policy Iteration
Input: An initial admissible control u0(x), and a threshold

ǫ > 0.

Output: The approximate optimal control ui+1(x) and the

approximate optimal cost function Vi(x).

1: Set i ← 0.
2: while i ≥ 0 do
3:

Policy evaluation: solve for the continuously differen-
tiable cost function Vi(x) with Vi(0) = 0 from
∇Vi(x)T (f (x) + g(x)ui(x)) + q(x)
+ ui(x)T Rui(x) = 0.

(3)

4:

Policy improvement: update the control policy by

ui+1(x) = −

1
2

R−1g(x)T ∇Vi(x).

(4)

5:
6:

7:

8:
9:

if kui+1(x) − ui(x)k ≤ ǫ for all x then

break

else

Set i ← i + 1.
continue

end if

10:
11: end while

u∗(x) = −

1
2

R−1g(x)T ∇V ∗(x).

where

Moreover, the state feedback u∗ locally asymptotically sta-
bilizes (1) at the origin and minimizes (2) over all admis-
sible controls [2]. Solving the HJB equation analytically is
extremely difﬁcult in general except for linear cases. Hence,
approximate or iterative methods are needed to solve the
HJB, and a well-known policy iteration algorithm, which is
called policy iteration [7], is reviewed in Algorithm 1. Let
{Vi(x)}i≥0 and {ui+1(x)}i≥0 be the sequences of functions
generated by the policy iteration algorithm in Algorithm 1. It
is shown in [7] that Vi+1(x) ≤ Vi(x) for i ≥ 0, and the limit
functions V (x) = limi→∞ Vi(x) and u(x) = limi→∞ ui(x)
are equal to the optimal cost function V ∗ and the optimal
control u∗.

Consider approximating the solutions to (3) and (4) by
ADP instead of obtaining them exactly. For this purpose,
choose an admissible feedback control u0 : Rn → Rm for
(1) and let {Vi(x)}i≥0 and {ui+1(x)}i≥0 be the sequences
of functions generated by the policy iteration algorithm in
Algorithm 1 starting with the control u0(x). Choose a bounded
time-varying exploration signal η : R → Rm, and apply the
sum u0(x) + η(t) to (1) as follows:

˙x = f (x) + g(x)(u0(x) + η(t)).

(5)

Assume that solutions to (5) are well deﬁned for all positive
time. Let T (x, u0, η, [r, s]) = {(x(t), u0(x(t)), η(t)) | r ≤
t ≤ s} denote the trajectory x(t) of the system (5) with the
input u0 + η over the time interval [r, s] with 0 ≤ r < s. The
system (5) can be rewritten as

˙x = f (x) + g(x)ui(x) + g(x)νi(x, t),

(6)

νi(x, t) = u0(x) − ui(x) + η(t).

Combined with (3) and (4), the time derivative of Vi(x) along
the trajectory x(t) of (6) is obtained as

˙Vi(x) = −q(x) − ui(x)T Rui(x) − 2ui+1(x)T Rνi(x, t) (7)

for i ≥ 0. By integrating both sides of (7) over any time
interval [r, s] with 0 ≤ r < s, one gets

Vi(x(s)) − Vi(x(r))
s

= −

Z
r

(q(x) + ui(x)T Rui(x) + 2ui+1(x)T Rνi(x, t)) dτ.

(8)

Let φj : Rn → R and ϕj : Rn → Rm, with j = 1, 2, . . . ,
be two inﬁnite sequences of continuous basis functions on a
compact set in Rn containing the origin as an interior point
that vanish at the origin [20]. Then, Vi(x) and ui+1(x) for each
i ≥ 0 can be expressed as inﬁnite series of the basis functions.
For each i ≥ 0 let ˆVi(x) and ˆui+1(x) be approximations of
Vi(x) and ui+1(x) given by

ˆVi(x) =

ˆui+1(x) =

N1

Xj=1
N2

Xj=1

ci,jφj(x),

wi,jϕj (x),

(9)

(10)

where N1 > 0 and N2 > 0 are integers and ci,j, wi,j are
coefﬁcients to be found for each i ≥ 0. Then, equation (8) is
approximated by ˆVi(x) and ˆui+1(x) as follows:

N1

Xj=1

ci,j (φj(x(s)) − φj (x(r)))

s

N2

+

Z
r

(2

Xj=1
s

wi,j ϕj(x)T Rˆνi) dτ

(q(x) + ˆui(x)T Rˆui(x)) dτ,

= −

Z
r

where

ˆu0 = u0,

ˆνi = u0 − ˆui + η.

(11)

(12)

have

that we

available K trajectories
Suppose
T (x, u0, η, [rk, sk]), k = 1, . . . , K, where x(t), u0(t),
and η(t) satisfy (6) over the K time intervals [rk, sk],
k = 1, . . . , K. Then, we have K equations of the form (11)
for each i ≥ 0, which can be written as

ei,k = 0,

k = 1, . . . , K,

(13)

where

N1

ei,k :=

ci,j(φj(x(sk)) − φj (x(rk)))

Xj=1

sk

N2

(2

Xj=1

+

+

Z

rk

sk

Z

rk

wi,jϕj (x)T Rˆνi) dτ

(q(x) + ˆui(x)T Rˆui(x)) dτ.

Then, the coefﬁcients {ci,j}N1
by minimizing

j=1 and {wi,j}N2

j=1 are obtained

K

Xk=1

kei,kk2.

In other words, the K equations in (13) are solved in the least
j=1 and {wi,j}N2
squares sense for the coefﬁcients, {ci,j}N1
j=1.
Thus two sequences { ˆVi(x)}∞
i=0 can be
generated from (11). According to [20, Cor. 3.2.4], for any
arbitrary ǫ > 0, there exist integers i∗ > 0, N ∗∗
1 > 0 and
N ∗∗

i=0 and {ˆui+1(x)}∞

2 > 0 such that

N1

Xj=1

N2

Xj=1

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

ci∗,jφj(x) − V ∗(x)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
wi∗,jϕj(x) − u∗(x)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

≤ ǫ,

≤ ǫ

for all x, if N1 > N ∗∗
1

and N2 > N ∗∗
2 .
Remark 1: The ADP algorithm relies on only the measure-
ments of states, the initial control policy and the exploration
signal, lifting the requirement of knowing the precise system
model, while the conventional policy iteration algorithm in
Algorithm 1 requires the knowledge of the exact system
model. Hence, the ADP algorithm is 100% data-based and
model-free.

3

Remark 2: Equation (11) depends on the initial control u0,
the exploration signal η, the time interval [r, s] as well as the
index i, where the ﬁrst three u0, η, and [r, s] are together
equivalent to the trajectory T (x, u0, η, [r, s]) if the initial state
x(r) at t = r is given. Hence, we can generate more diverse
trajectories by changing u0 and η as well as [r, s], and enrich
the ADP algorithm accordingly, as follows. Suppose that we
have available K trajectories T (xk, uk
0, ηk, [rk, sk]), 1 ≤ k ≤
K, where xk, uk

0 and ηk satisfy (6), i.e.,

˙xk(t) = f (xk(t)) + g(xk(t))(uk

0 (xk(t)) + ηk(t))

for rk ≤ t ≤ sk. Then, we have K equations of the form
(11) for each i ≥ 0, which can be written as ei,k = 0, k =
1, . . . , K, where

ei,k :=

N1

Xj=1

ci,j(φj (xk(sk)) − φj(xk(rk)))

sk

N2

(2

Xj=1

+

+

Z

rk

sk

Z

rk

wi,j ϕj(xk)T Rˆνk

i ) dτ

(q(xk) + ˆui(xk)T Rˆui(xk)) dτ

with ˆu0 = uk
cients {ci,j}N1

0 and ˆνk = uk
j=1 and {wi,j}N2

0 + ηk − ˆui. Then, the coefﬁ-
j=1 are obtained by minimizing
K
k=1 kei,kk2. For the sake of simplicity of presentation,
however, in this paper we will ﬁx u0 and η and vary only
P
the time intervals to generate trajectory data.

III. IMPLEMENTATION DETAILS AND SOFTWARE
FEATURES
We now discuss implementation details and features of
the Adaptive Dynamic Programming Toolbox (ADPT). We
provide two modes to generate approximate optimal feedback
controls; one mode requires the knowledge of system model,
but the other eliminates this requirement, giving rise to the
ADPT’s unique capability of handling model-free cases.

A. Implementation of Computational Adaptive Dynamic Pro-
gramming

To approximate Vi(x) and ui+1(x) in (3) and (4), monomi-
als composed of state variables are selected as basis functions.
For a pre-ﬁxed number d ≥ 1, deﬁne a column vector Φd(x)
by ordering monomials in graded reverse lexicographic order
as
n) ∈ RN ×1,
Φd(x) = (x1, . . . , xn; x2
1, x1x2, . . . , x2
where x = (x1, x2, . . . , xn) ∈ Rn is the state, d ≥ 1 is the
highest degree of the monomials, and N is given by
d

1, . . . , xd

n; x3

N =

(cid:18)

Xi=1

i + n − 1

.
n − 1 (cid:19)

For example, if n = 3 and d = 3, the corresponding ordered
monomials are

x1, x2, x3;
1, x1x2, x1x3, x2
x2
1x2, x2
1, x2
x3

1x3, x1x2

2, x2x3, x2
3;
2, x1x2x3, x1x2

3, x3

2, x2

2x3, x2x2

3, x3
3.

According to (9) and (10), the cost function Vi(x) and the
control ui+1(x) are approximated by ˆVi(x) and ˆui+1(x) which
are deﬁned as

ˆVi(x) = ciΦd+1(x),
ˆui+1(x) = WiΦd(x),

(14)

(15)

where d ≥ 1 is the approximation degree, and ci ∈ R1×N1
and Wi ∈ Rm×N2 are composed of coefﬁcients corresponding
to the monomials in Φd+1(x) and Φd(x) with

N1 =

d+1

i + n − 1

(cid:18)

Xi=1

n − 1 (cid:19)

, N2 =

d

(cid:18)

Xi=1

i + n − 1

.
n − 1 (cid:19)

We take the highest degree of monomials to approximate Vi
greater by one than the approximation degree since ui+1 is
obtained by taking the gradient of Vi
in (4) and g(x) is
constant in most cases.

Theorem 1: Let a set of trajectories be deﬁned as ST =

{T (x, u0, η, [rk, sk]), k = 1, 2, . . . , K} with K ≥ 1, and let

α(x) = RηΦd(x)T ,
β(x) = R(u0(x) + η)Φd(x)T ,
γ(x) = Φd−1(x)Φd(x)T .

Then the coefﬁcients ci and Wi satisfy

Ai

cT
i
vec(Wi)(cid:21)

(cid:20)

= bi,

(16)

where

A0 =







sK



(x)

(x)

s1
2vec(
r1
R

α(x) dt)T
...

rK α(x) dt)T
2vec(
R

Φ[r1,s1]
d+1
...

Φ[rK ,sK ]

d+1

∈ RK×(N1+mN2),
s1
r1
...
R
rK (q(x) + u0(x)T Ru0(x)) dt
R

(q(x) + u0(x)T Ru0(x)) dt

−

sK

b0 = 


and for i = 1, 2, . . . ,

−



∈ RK×1,




Ai =

sK



(x)

(x)

s1
2vec(
r1
R

Φ[r1,s1]
d+1
...

Φ[rK ,sK ]

d+1

∈ RK×(N1+mN2),
s1
r1
R
bi = 
rK q(x) dt − hW T
−


R
∈ RK×1,

q(x) dt − hW T

−

sK

(β(x) − RWi−1γ(x)) dt)T

...

rK (β(x) − RWi−1γ(x)) dt)T
2vec(
R

i−1RWi−1,
...
i−1RWi−1,

s1
r1

γ(x) dti



sK
rK γ(x) dti




R

R

where

Φ[rk,sk]
d+1

(x) = Φd+1(x(sk))T − Φd+1(x(rk))T

for k = 1, 2, . . . , K, and the operator vec(·) is deﬁned as

4





vec(Z) =

z1
z2
...
zn
with zj ∈ Rm×1 being the jth column of a matrix Z ∈ Rm×n
for j = 1, . . . , n.

∈ Rmn×1











Proof: See Appendix A.

We now give the computational adaptive dynamic program-
ming algorithm in Algorithm 2 for practical implementation.
To solve the least squares problem in line 5 in the algorithm,
we need to have a sufﬁciently large number K of trajectories
such that the minimization problem can be well solved nu-
merically. Then the approximate optimal feedback control is
generated by the algorithm as ˆui+1 = WiΦd(x).

Algorithm 2 Computational Adaptive Dynamic Programming
Input: An approximation degree d ≥ 1, an initial admissible
control u0(x), an exploration signal η(t), and a threshold
ǫ > 0.

Output: The approximate optimal control ˆui+1(x) and the

approximate optimal cost function ˆVi(x).

1: Apply u = u0 + η as the input during a sufﬁciently long

period and collect necessary data.

2: Set i ← 0.
3: while i ≥ 0 do
4:

Generate Ai and bi.
Obtain ci and Wi by solving the minimization problem

Ai

cT
i
vec(Wi)(cid:21)

(cid:20)

min
ci,Wi (cid:13)
(cid:13)
(cid:13)
(cid:13)

2

.

− bi(cid:13)
(cid:13)
(cid:13)
(cid:13)

if i ≥ 1 and kci − ci−1k2 + kWi − Wi−1k2 ≤ ǫ2 then

break

else

Set i ← i + 1.
continue

end if

10:
11:
12: end while
13: return ˆui+1(x) = WiΦd(x) and ˆVi(x) = ciΦd+1(x)

5:

6:

7:

8:
9:







Remark 3: As in the statement of Theorem 1, several
integral terms are included in Ai and bi for i ≥ 0. As in
(12), u0 does not get approximated by the basis functions, so
the matrices A0 and b0 in Theorem 1 are obtained with x(rk),
sk
sk
x(sk),
rk α(x) dt,
rk q(x) dt,
1 ≤ k ≤ K. For i ≥ 1, the matrices Ai and bi in Theorem 1
R
R
sk
need, in addition,
rk γ(x) dt, 1 ≤ k ≤ K,
as well as Wi−1.
R

rk u0(x)T Ru0(x) dt and
R
sk
rk β(x) dt and
R

sk

Remark 4: In the situation where the system dynamic
equations are known, the ADPT uses the Runge-Kutta method
to simultaneously compute the trajectory points x(rk) and
x(sk) and the integral terms that appear in Ai and bi. In
the case when system equations are not known but trajectory
data are available, the ADPT applies the trapezoidal method
to evaluate these integrals numerically. In this case, each
trajectory T (x, u0, η, [rk, sk]) is represented by a set of its

sample points {x(tk,ℓ), u0(tk,ℓ), η(tk,ℓ)}Lk
ℓ=1, where {tk,ℓ}Lk
ℓ=1
is a ﬁnite sequence that satisﬁes rk = tk,1 < tk,2 < . . . <
tk,Lk−1 < tk,Lk = sk, and then the trapezoidal method is
applied on these sample points to numerically evaluate the
integrals over the time interval [rk, sk]. If intermediate points
in the interval [rk, sk] are not available so that partitioning the
interval [rk, sk] is impossible, then we just use the two end
points rk and sk to evaluate the integral by the trapezoidal
method as

sk

Z

rk

h(t)dt ≈

(sk − rk)(h(sk) + h(rk))
2

(17)

for a function h(t).

B. Software Features

The codes of the ADPT are available at https://github.

com/Everglow0214/The Adaptive Dynamic Programming
Toolbox.

1) Symbolic Expressions: It is of great importance for an
optimal control package that the user can describe functions
such as system equations, cost functions, etc., in a convenient
manner. The idea of the ADPT is to use symbolic expressions.
Consider a optimal control problem, where the system model
is in the form (1) with

0
1
k4





f (x) = 

x2

−k1x1 − k2x3

1 − k3x2



,

g(x) = 



,

(18)



k4



where x = (x1, x2) ∈ R2 is the state, u ∈ R is the control, and
k1, k2, k3, k4 ∈ R are system parameters. The cost function is
in the form (2) with

q(x) = 5x2

1 + 3x2
Then in the ADPT the system dynamics and the cost function
can be deﬁned in lines 1 – 17 in Listing 1.

2, R = 2.

(19)

Listing 1
AN EXAMPLE OF THE MODEL-BASED MODE.

1 n = 2; % state dimension
2 m = 1; % control dimension
3 %% Symbolic variables.
4 syms x [n,1] real
5 syms u [m,1] real
6 syms t real
7
8 %% Define the system.
9 k1 = 3; k2 = 2; k3 = 2; k4 = 5;
10 f = [x2;
11
12 g = [0;
13

(-k1*x1-k2*x1ˆ3-k3*x2)/k4];

1/k4];

14
15 %% Define the cost function.
16 q = 5*x1ˆ2+3*x2ˆ2;
17 R = 2;
18
19 %% Execute ADP iterations.
20 d = 3; % approximation degree

5

21 [w,c] = adpModelBased(f,g,x,n,u,m,q,...
22

R,t,d);

2) Working Modes: Two working modes are provided in
the ADPT; the model-based mode and the model-free mode.
The model-based mode deals with the situation where the
system model is given while the model-free mode addresses
the situation where the system model is not known but only
trajectory data are available. An example of the model-based
mode is given in Listing 1, where after deﬁning the system
the cost function (19) and the approximation
model (18),
degree d in lines 1 – 20, the function, adpModelBased,
returns the coefﬁcients Wi and ci for the control ˆui+1 and the
cost function ˆVi, respectively, in lines 21 – 22.

An example of the model-free mode is shown in Listing 2,
where the system model (18) is assumed to be unknown. The
initial control u0 is in the form of u0(x) = −F x with the
feedback control gain F deﬁned in line 18. The exploration
signal η is composed of four sinusoidal signals as shown in
lines 21 – 22. A list of two initial states x(0) = (−3, 2) and
x(0) = (2.2, 3) is given in lines 29 – 30, and a list of the
corresponding total time span for simulation is given lines 31
– 32, where the time interval [0, 6] is divided into sub-intervals
of size 0.002 so that trajectory data are recorded every 0.002
sec in lines 38 – 44. The time stamps are saved in the column
vector t_save in line 42, and the values of states are saved
in the matrix x_save in line 43, with each row in x_save
corresponding to the same row in t_save. Similarly, the
values of the initial control u0 and the exploration signal
η are saved in vectors u0_save and eta_save in lines
46 – 47. These measurements are passed to the function,
adpModelFree, in lines 51 – 52 to compute the optimal
control and the optimal cost function approximately.

In the both model-based and model-free modes the approx-
imate control is saved in the ﬁle, uAdp.p, that is generated
automatically and can be applied by calling u=uAdp(x)
without dependence on other ﬁles. Similarly, the user may
also check the approximate cost through the ﬁle, VAdp.p.

3) Options: Multiple options are provided such that the
user may customize optimal control problems in a convenient
way. In the model-based mode, the user may set option values
through the function, adpSetModelBased,
in a name-
value manner before calling adpModelBased. That is, the
speciﬁed values may be assigned to the named options. An
example is shown in Listing 3, where two sets of initial states,
time intervals and exploration signals are speciﬁed in lines 1 –
9. Then, in lines 16 – 17 the output of adpSetModelBased
should be passed to adpModelBased for the options to take
effect. Otherwise, the default values would be used for the
options as in lines 21 – 22 in Listing 1. The options supported
by adpSetModelBased are listed in Table II in Appendix
B.

Listing 3
A DEMONSTRATION OF CALLING THE FUNCTION adpSetModelBased.

1 %% The user may specify settings.
2 xInit = [-3, 2;
3
4 tSpan = [0, 10;

2.2, 3];

Listing 2
AN EXAMPLE OF THE MODEL-FREE MODE.

1 n = 2; % state dimension
2 m = 1; % control dimension
3
4 %% Define the cost function.
5 q = @(x) 5*x(1)ˆ2+3*x(2)ˆ2;
6 R = 2;
7
8 %% Generate data.
9 syms x [n,1] real
10 syms t real
11 k1 = 3; k2 = 2; k3 = 2; k4 = 5;
12 % System dynamics.
13 f = [x2;
14
15 g = [0;
16

(-k1*x1-k2*x1ˆ3-k3*x2)/k4];

1/k4];

17
18 F = [1, 1] % feedback gain
19
20 % Exploration signal.
21 eta = 0.8*(sin(7*t)+sin(1.1*t)+...
sin(sqrt(3)*t)+sin(sqrt(6)*t));
22
23 e = matlabFunction(eta,’Vars’,t);
24
25 % To be used in the function ode45.
26 dx = matlabFunction(f+g*(-F*x+eta),...
27

’Vars’,{t,x});

2.2, 3];

28
29 xInit = [-3, 2;
30
31 tSpan = [0:0.002:6;
32
33 odeOpts = odeSet(’RelTol’,1e-6,...
34

’AbsTol’,1e-6);

0:0.002:6];

35
36 t_save = [];
37 x_save = [];
38 for i = 1:size(xInit,1)
39

[time, states] = ode45(...

40

41
42

@(t,x)dx(t,x),tSpan(i,:),...
xInit(i,:),odeOpts);
t_save = [t_save; time];
x_save = [x_save; states];

43
44 end
45
46 u0_save = -x_save*F;
47 eta_save = e(t_save);
48
49 %% Execute ADP iterations.
50 d = 3; % approximation degree
51 [w,c] = adpModelFree(t_save,x_save,n,...
52

u0_save,m,eta_save,d,q,R);

6

5

0, 8];

6
7 syms t real
8 eta = [0.8*sin(7*t)+sin(3*t);
9

sin(1.1*t)+sin(pi*t)];

10
11 adpOpt = adpSetModelBased(...
12

’xInit’,xInit,’tSpan’,tSpan,...
’explSymb’,eta);

13

14
15 %% Execute ADP iterations.
16 [w,c] = adpModelBased(f,g,x,n,u,m,q,...
17

R,t,d,adpOpt);

For the command, adpModelFree, option values can
function, adpSetModelFree,
be modiﬁed with the
in the name-value manner. The options
supported by
adpSetModelFree are listed in Table III in Appendix B.
Among these options, ‘stride’ enables the user to record values
of states, initial controls and exploration signals in a high
frequency for a long time, while using only a portion of them
in the iteration process inside adpModelFree. To illustrate
it, let each trajectory in the set ST of trajectories in the
statement of Theorem 1 be represented by two sample points
at time rk and sk, that is, the trapezoidal method evaluates
integrals over [rk, sk] by taking values at rk and sk as in
(17). Suppose that trajectories in ST are consecutive, that is,
sk = rk+1 for k = 1, 2, . . . , K − 1. By setting ‘stride’ to
δ with δ = 1, 2, . . . , K, the data used to generate Ai and
bi in Algorithm 2 become {T (x, u0, η, [r1+iδ, s(i+1)δ]), i ∈
N, (i + 1)δ ≤ K}. For example, consider 3 consecutive
trajectories T (x, u0, η, [rk, rk+1]) with k = 1, 2, 3. If ‘stride’
is set to 1, one will have three equations from (11) as follows:

N1

Xj=1

ci,j(φj (x(rk+1)) − φj (x(rk)))

rk+1

N2

+

Z

rk

(2

Xj=1

wi,jϕj (x)T Rˆνi) dτ

rk+1

= −

Z

rk

(q(x) + ˆui(x)T Rˆui(x)) dτ

for k = 1, 2, 3. These three equations contribute to three rows
of Ai and three rows of bi as in Theorem 1. If ‘stride’ is set to
3, then one will have only one equation from (11) as follows:

N1

Xj=1

ci,j(φj (x(r4)) − φj(x(r1)))

r4

N2

+

Z

r1

(2

Xj=1
r4

= −

Z

r1

wi,j ϕj(x)T Rˆνi) dτ

(q(x) + ˆui(x)T Rˆui(x)) dτ,

(20)

where the integrals over [r1, r4] are evaluated by the trape-
zoidal method with the interval [r1, r4] partitioned into the
three sub-intervals [r1, r2]∪[r2, r3]∪[r3, r4], i.e, with the points
at r1, r2, r3 and r4. Equation (20) will contribute to one row

of Ai and one row of bi as in Theorem 1. With the assumption
that Ai has full rank with ‘stride’ set to 3, by setting ‘stride’
to 3, the number of equations in the minimization problem in
Algorithm 2 is two less than that with ‘stride’ set to 1, and
as a result, the computation load is reduced in the numerical
minimization. It is remarked that with ‘stride’ equal to 3,
all the four points at r1, . . . , r4 are used by the trapezoidal
method to evaluate the integrals over the interval [r1, r4] in
(20), producing a more precise value of integral than the one
that would be obtained with the two end points at r1 and r4
only. An example of calling adpSetModelFree is shown
in Listing 4. Similarly, adpModelFree takes the output of
adpSetModelFree as an argument to validate the options
speciﬁed.

Listing 4
A DEMONSTRATION OF CALLING THE FUNCTION adpSetModelFree.

1 %% The user may specify settings.
2 adpOpt = adpSetModelFree(’stride’,2);
3
4 %% Execute ADP iterations.
5 [w,c] = adpModelFree(t_save,x_save,n,...
u0_save,m,eta_save,d,q,R,adpOpt);
6

IV. APPLICATIONS TO THE SATELLITE ATTITUDE
STABILIZING PROBLEM

In this section we apply the ADPT to the satellite at-
titude stabilizing problem because a stabilization problem
can be formulated as an optimal control problem. In the
ﬁrst example, the system model is given and the controller
is computed by the function adpModelBased. The same
problem is solved again in the second example by the function
adpModelFree when the system dynamics is totally un-
known. The source codes for these two examples are available
at https://github.com/Everglow0214/The Adaptive Dynamic
Programming Toolbox.

A. Model-Based Case

Let H denote the set of quaternions and S3 = {q ∈ H |
kqk = 1}. The equations of motion of the continuous-time
fully-actuated satellite system are given by

˙q =

qΩ,

1
2

˙Ω = I−1((IΩ) × Ω) + I−1u,

(21)

(22)

where q ∈ S3 represents the attitude of the satellite, Ω ∈ R3 is
the body angular velocity vector, I ∈ R3×3 is the moment of
inertial matrix and u ∈ R3 is the control input. The quaternion
multiplication is carried out for qΩ on the right-hand side of
(21) where Ω is treated as a pure quaternion. By the stable
embedding technique [21], the system (21) and (22) deﬁned
on S3 × R3 is extended to the Euclidean space H × R3 [22],
[23] as

7

where q ∈ H, Ω ∈ R3 and α > 0.

Consider the problem of stabilizing the system (23) and (24)
at the equilibrium point (qe, Ωe) = ((1, 0, 0, 0), (0, 0, 0)). The
error dynamics is given by

1
2

(eq + qe)eΩ − α(|eq + qe|2 − 1)(eq + qe),

˙eq =
˙eΩ = I−1((IeΩ) × eΩ) + I−1u,

where eq = q − qe and eΩ = Ω − Ωe are state errors. Since the
problem of designing a stabilizing controller can be solved by
designing an optimal controller, we pose an optimal control
problem with the cost integral (2) with q(x) = xT Qx, where
x = (eq, eΩ) ∈ R7 and Q = 2I7×7, and R = I3×3. The
inertia matrix I is set to I = diag(0.1029, 0.1263, 0.0292).
The parameter α that appears in the above error dynamics is
set to α = 1.

We set the option ‘xInit’ with three different initial states.
For each initial state, the option ‘tSpan’ is set to [0, 15]. We use
the option ‘explSymb’ to set exploration signals; see Table II in
Appendix B for the use of the option ‘explSysb’. For the initial
control u0, the default initial control is used, which is an LQR
controller computed for the linearization of the error dynamics
around the origin with the weight matrices Q = 2I7×7 and
R = I3×3. We then call the function, adpModelBased, to
generate controllers of degree d = 1, 2, 3. The computation
time taken by the function, adpModelBased, to produce
the controllers are recorded in Table I. For the purpose
of comparison, we also apply Al’brekht’s method with the
Nonlinear Systems Toolbox (NST) [15] to produce controllers
of degree d = 1, 2, 3 for the same optimal control problem,
and record their respective computation time in Table I. For
comparison in terms of optimality, we apply the controllers
to the system (23) and (24) for the initial error state x0 =
((cos(θ/2) − 1, sin(θ/2), 0, 0), (0, 0, 0)) with θ = 1.99999π
and compute their corresponding values of the cost integral
in Table I. Since we do not know the exact optimal value of
the cost integral J(x0, u) for this initial state, we employ the
software package called ACADO [17] to numerically produce
the optimal control for this optimal control problem with the
given initial state. We note that both NST and ACADO are
model-based.

We can see in Table I that ADPT in the model-based mode
is superior to NST in terms of optimality, and ADPT (model-
based) for d = 2, 3 is on par with ACADO in terms of
optimality. Notice however that ACADO produces an open-
loop optimal control for each given initial state, which is an
inferior point of ACADO, while ADPT produces a feedback
optimal control that is independent of initial states. Moreover,
even for the given initial state ACADO takes a tremendous
amount of time to compute the open-loop optimal controller.
From these observations, we can say that ADPT in the model-
based mode is superior to NST and ACADO in terms of
optimality, speed and usefulness all taken into account.

1
2

˙q =

qΩ − α(|q|2 − 1)q,

˙Ω = I−1((IΩ) × Ω) + I−1u,

(23)

(24)

B. Model-Free Case

Consider solving the same optimal problem as in Section
IV-A, but the system dynamics in (21) and (22), or equivalently

TABLE I
COSTS AT x0 AND COMPUTATION TIME BY ADPT,
NST AND ACADO.

ADPT
(model-based)

ADPT
(model-free)

NST

ACADO

d = 1
d = 2
d = 3
d = 1
d = 2
d = 3
d = 1
d = 2
d = 3
-

J(x0, u)a
37.8259
33.6035
33.4986
43.8308
36.8319
37.4111
208.9259
94.6868
64.0721
32.6000

Time [s]b
1.6572
2.5878
11.6869
0.8923
3.5327
90.6225
0.2702
0.6211
3.6201
2359.67

a J(x0, u) denotes the integral cost of the correspond-

ing control u.

b ‘Time [s]’ denotes the computation time taken by the

method to obtain the controller.

the error dynamics are not available. Since we do not have
real trajectory data available, for the purpose of demonstration
we make up some trajectories with four initial states for
the error dynamics, where the same initial control u0 and
exploration signals η are used as the model-based case in
Section IV-A. The simulation for data collection is run over
the time interval [0, 20] with the recording period being 0.002
sec, producing 10, 000 = 20/0.002 sampled points for each
run. For the function adpModelFree, the option of ‘stride’
is set to 4. Then, the function, adpModelFree, is called to
generate controllers of degree d = 1, 2, 3, the computation
time taken for each of which is recorded in Table I. For
the purpose of comparison in terms of optimality, we apply
the controllers generated by adpModelFree to the system
(23) and (24) with the initial error state x0 = ((cos(θ/2) −
1, sin(θ/2), 0, 0), (0, 0, 0)) with θ = 1.99999π and compute
the corresponding values of the cost integral; see Table I for
the values.

From Table I, we can see that ADPT in the model-free
mode takes more computation time than ADPT in the model-
based mode, and the cost integrals by ADPT in the model-
free working mode is slightly higher than those in the model-
based working mode, since the integrals in the iteration process
are evaluated less accurately. However, ADPT in the model-
free mode is superior to NST in terms of optimality and to
ACADO in terms of computation time. More importantly, it is
noticeable that the result by model-free ADPT is comparable
to model-based ADPT, which shows the power of data-based
adaptive dynamic programming and the ADP toolbox.

To see how the computed optimal controller works in terms
of stabilization, the norm of the state error under the control
with d = 3 generated by ADPT in the model-free mode is
plotted in Fig. 1 together with the norm of state error by the
NST controller with degree 3. We can see that the convergence
to the origin is faster with the model-free ADP controller
than with the controller by NST that is model-based. This
comparison result is consistent with the comparison of the
two in terms of optimality.

8

Fig. 1. The state errors kx(t)k with the controllers of degree 3 generated by
ADPT in the model-free working mode and by NST.

V. CONCLUSIONS AND FUTURE WORK

The Adaptive Dynamic Programming Toolbox, a MATLAB-
based package for optimal control for continuous-time nonlin-
ear systems, has been presented. We propose a computational
methodology to approximately produce the optimal control
and the optimal cost function by employing the adaptive
dynamic programming technique. The ADPT can work in the
model-based mode or in the model-free mode. The model-
based mode deals with the situation where the system model
is given while the model-free mode handles the situation where
the system dynamics are unknown but only system trajectory
data are available. Multiple options are provided for the both
modes such that the ADPT can be easily customized. The
optimality, the running speed and the utility of the ADPT are
illustrated with a satellite attitude stabilizing problem.

Currently control policies and cost functions are approxi-
mated by polynomials in the ADPT. As mathematical prin-
ciples of neural networks are being revealed [24], [25], we
plan to use deep neural networks in addition to polynomials
in the ADPT to approximately represent optimal controls and
optimal cost functions to provide users of the ADPT more
options.

APPENDIX A
PROOF OF THEOREM 1

Proof: Combining (11), (14) and (15), one has

c0(Φd+1(x(sk)) − Φd+1(x(rk))) + 2

sk

Z

rk

Φd(x)T W T

0 Rη dt

sk

= −

Z

rk

(q(x) + u0(x)T Ru0(x)) dt,

(25)

and for i = 1, 2, . . . ,

ci(Φd+1(x(sk)) − Φd+1(x(rk)))

+ 2

− 2

sk

Z

rk

sk

Z

rk

Φd(x)T W T

i R(u0(x) + η) dt

Φd(x)T W T

i RWi−1Φd(x) dt

sk

= −

Z

rk

(q(x) + Φd(x)T W T

i−1RWi−1Φd(x)) dt.

(26)

By applying the property

hA, BCi = hACT , Bi = hBT A, Ci

TABLE II
OPTIONS SUPPORTED BY THE FUNCTION adpSetModelBased.

9

of

the Euclidean inner product deﬁned by hE, F i =
ij Eij Fij for matrices E = [Eij ] and F = [Fij ] of equal

size, one may rewrite (25) and (26) as
P

c0(Φd+1(x(sk)) − Φd+1(x(rk)))

+ 2

W0,

D

rk

Z
sk

sk

RηΦd(x)T dt

E

(q(x) + u0(x)T Ru0(x)) dt,

(27)

= −

Z

rk
and for i = 1, 2, . . . ,

ci(Φd+1(x(sk)) − Φd+1(x(rk)))

sk

+ 2

Wi,

D

Z

rk

− 2

Wi, RWi−1

D

R(u0(x) + η)Φd(x)T dt

sk

Z

rk

E
Φd(x)Φd(x)T dt

E

= −

i−1RWi−1,

W T
D

sk

= −

Z

rk

q(x) dt.

sk

Z

rk

Φd(x)Φd(x)T dt

E

(28)

Then, the system of linear equations in (16) readily follows
from (27) and (28).

APPENDIX B

Here we list options supported by adpSetModelBased
and adpSetModelFree in Table II and Table III, respec-
tively.

ACKNOWLEDGMENT

This work was conducted by Center for Applied Research
in Artiﬁcial Intelligence(CARAI) grant funded by Defense
Acquisition Program Administration(DAPA) and Agency for
Defense Development(ADD) (UD190031RD).

REFERENCES

[1] D. E. Kirk, Optimal Control Theory: An Introduction.

Englewood

Cliffs, NJ, USA: Prentice-Hall, 1970.

[2] F. L. Lewis, D. L. Vrabie, and V. L. Syrmos, Optimal Control. Hoboken,

NJ, USA: John Wiley & Sons, Inc., 2012.

[3] E. G. Al’brekht, “On the optimal stabilization of nonlinear systems,” J.

Appl. Math. Mech., vol. 25, no. 5, pp. 1254–1266, 1961.

[4] W. L. Garrard and J. M. Jordan, “Design of nonlinear automatic ﬂight
control system,” Automatica, vol. 13, no. 5, pp. 497–505, Sep. 1977.
[5] Y. Nishikawa, N. Sannomiya, and H. Itakura, “A method for suboptimal
design of nonlinear feedback systems,” Automatica, vol. 7, no. 6, pp.
703–712, Nov. 1971.

[6] J. K. Peterson, “On-line estimation of the optimal value function: HJB-
estimators,” in Adv. Neural Inf. Process. Syst. 5 (NIPS), Denver, CO,
USA, Nov. 1992, pp. 319–326.

[7] G. N. Saridis and C.-S. G. Lee, “An approximation theory of optimal
control for trainable manipulators,” IEEE Trans. Syst., Man, Cybern.,
vol. 9, no. 3, pp. 152–159, Mar. 1979.

[8] R. W. Beard, G. N. Saridis, and J. T. Wen, “Galerkin approximations
of the generalized Hamilton-Jacobi-Bellman equation,” Automatica,
vol. 33, no. 12, pp. 2159–2177, Dec. 1997.

[9] ——, “Approximate solutions to the time-invariant Hamilton-Jacobi-
Bellman equation,” J. Optim. Theory Appl., vol. 96, pp. 589–626, Mar.
1998.

Name
(default)
‘xInit’
([ ])

‘xInitNum’
(2)
‘xInitMin’
(0.3)

‘xInitMax’
(0.9)

‘tSpan’
([0 8])

‘odeOpt’

‘u0Symb’
([ ])

‘explSymb’
([ ])
‘explAmpl’
(0.8)

‘numFreq’
(4)

‘basisOpt’
(‘mono’)
‘stride’
(1)

‘crit’
(1)

‘epsilon’
(0.001)

‘maxIter’
(100)

Description

of

the

value

‘odeOpt’:

Two default initial states are generated randomly, with
the value of each initial state in an open interval
(‘xInitMin’, ‘xInitMax’) or (−‘xInitMax’, −‘xInitMin’).
The user may choose the number (‘xInitNum’) and the
range (‘xInitMin’ and ‘xInitMax’) of initial states.
The user may also directly specify initial states (‘xInit’)
by a row vector or a matrix with each row indicating one
initial condition.
Trajectories will be calculated starting from these initial
states for a time interval (the corresponding row of
‘tSpan’). The number of rows of ‘tSpan’ automatically
matches with ‘xInitNum’ by default.
Trajectories are calculated by the Runge-Kutta mathod.
Settings of the Runge-Kutta mathod (‘odeOpt’) can be
modiﬁed by calling the function odeSet.
Default
option
odeset(’RelTol’,1e-6,’AbsTol’,1e-6).
The default initial control is calculated from the linear
quadratic method after linearization around the origin.
The user may also directly specify the initial control
(‘u0Symb’) in a symbolic form.
Default exploration signals are sums of four sinusoidal
signals with different frequencies.
The frequencies in the default exploration signals con-
sist of rational numbers and irrational numbers and are
chosen randomly from a pre-deﬁned set.
The user may specify the number (‘numFreq’) and the
amplitude (‘explAmpl’) of sinusoidal signals in one ex-
ploration signal.
The number of default exploration signals automatically
matches with the control dimension and the number of
initial states.
All of these default exploration signals are different.
The user may also directly specify exploration signals
(‘explSymb’) in a symbolic form.
Supported basis functions (‘basisOpt’):
monomials (‘mono’).
The user may specify data to be used in the iteration
process by choosing the stride (‘stride’) in each trajectory.
The number of rows of ‘stride’ automatically matches
with ‘xInitNum’ by default.
The stop criterion (‘crit’) can be:
0 : kci − ci−1k ≤ ‘epsilon’ (the criterion used in [20]),
1 : kci − ci−1k2 + kWi − Wi−1k2 ≤ ‘epsilon’2,
2 : kci − ci−1k ≤ ‘epsilon’ ·kci−1k,
3 : kci − ci−1k2 + kWi − Wi−1k2 ≤ ‘epsilon’2 ·
(kci−1k2 + kWi−1k2).
The maximum number of iterations (‘maxIter’) can also
be speciﬁed.

[10] M. Abu-Khalaf and F. L. Lewis, “Nearly optimal control

laws for
nonlinear systems with saturating actuators using a neural network HJB
approach,” Automatica, vol. 41, no. 5, pp. 779–791, May 2005.
[11] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

Cambridge, MA, USA: MIT Press, 1998.

[12] D. L. Vrabie and F. L. Lewis, “Neural network approach to continuous-
time direct adaptive optimal control for partially unknown nonlinear
systems,” Neural Netw., vol. 22, no. 3, pp. 237–246, Apr. 2009.
[13] Y. Jiang and Z.-P. Jiang, “Robust adaptive dynamic programming and
feedback stabilization of nonlinear systems,” IEEE Trans. Neural Netw.
Learn. Syst., vol. 25, no. 5, pp. 882–893, May 2014.

[14] J. Y. Lee, J. B. Park, and Y. H. Choi, “Integral reinforcement learning
for continuous-time input-afﬁne nonlinear systems with simultaneous
invariant explorations,” IEEE Trans. Neural Netw. Learn. Syst., vol. 26,

10

TABLE III
OPTIONS SUPPORTED BY THE FUNCTION adpSetModelFree.

Name
(default)
‘basisOpt’
(‘mono’)
‘stride’
(1)

‘crit’
(1)

‘epsilon’
(0.001)

‘maxIter’
(100)

Description

Supported basis functions (‘basisOpt’):
monomials (‘mono’).
The user may specify data to be used in the iteration
process by choosing the stride (‘stride’) in each trajectory.
The number of rows of ‘stride’ automatically matches
with ‘xInitNum’ by default.
The stop criterion (‘crit’) can be:
0 : kci − ci−1k ≤ ‘epsilon’ (the criterion used in [20]),
1 : kci − ci−1k2 + kWi − Wi−1k2 ≤ ‘epsilon’2,
2 : kci − ci−1k ≤ ‘epsilon’ ·kci−1k,
3 : kci − ci−1k2 + kWi − Wi−1k2 ≤ ‘epsilon’2 ·
(kci−1k2 + kWi−1k2).
The maximum number of iterations (‘maxIter’) can also
be speciﬁed.

no. 5, pp. 916–932, May 2015.

[15] A. J. Krener, “The Nonlinear Systems Toolbox.” [Online]. Available:

https://www.math.ucdavis.edu/%7Ekrener/

[16] M. Giftthaler, M. Neunert, M. St¨auble, and J. Buchli, “The Control
Toolbox - an open-source C++ library for robotics, optimal and model
predictive control,” in 2018 IEEE Int. Conf. Simul., Model., Program.
Auton. Robots (SIMPAR), Brisbane, QLD, Australia, May 2018, pp. 123–
129.

[17] B. Houska, H. J. Ferreau, and M. Diehl, “ACADO Toolkit - an open
source framework for automatic control and dynamic optimization,”
Optim. Contr. Appl. Methods, vol. 32, no. 5, pp. 298–312, May 2011.
[18] R. Verschueren et al., “ACADOS - a modular open-source framework
for fast embedded optimal control,” 2019. [Online]. Available: https://
arxiv.org/abs/1910.13753

[19] M. A. Patterson and A. V. Rao, “GPOPS-II: a MATLAB software
for solving multiple-phase optimal control problems using hp-adaptive
Gaussian quadrature collocation methods and sparse nonlinear program-
ming,” ACM Trans. Math. Softw., vol. 41, no. 1, Oct. 2014.

[20] Y. Jiang and Z.-P. Jiang, Robust Adaptive Dynamic Programming.

Hoboken, NJ, USA: John Wiley & Sons, Inc., 2017.

[21] D. E. Chang, “On controller design for systems on manifolds in
Euclidean space,” Int. J. Robust Nonlinear Contr., vol. 28, no. 16, pp.
4981–4998, Nov. 2018.

[22] W. Ko, “A stable embedding technique for control of satellite attitude
represented in unit quaternions,” Master’s thesis, Korea Adv. Inst.
Science Techn., Republic of Korea, 2020.

[23] W. Ko, K. S. Phogat, N. Petit, and D. E. Chang, “Tracking controller
design for satellite attitude under unknown constant disturbance using
stable embedding,” Journal of Electrical Engineering & Technology, In
Press. DOI: 10.1007/s42835-020-00622-3

[24] K. Gurney, An Introduction to Neural Networks. UCL, London, UK:

UCL Press, 1997.

[25] A. L. Caterini and D. E. Chang, Deep Neural Networks in a Mathemat-

ical Framework. New York City, NY, USA: Springer, 2018.

