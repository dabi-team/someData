1
2
0
2

l
u
J

2
1

]

G
L
.
s
c
[

1
v
6
6
1
5
0
.
7
0
1
2
:
v
i
X
r
a

STATEFUL DETECTION OF MODEL EXTRACTION ATTACKS

A PREPRINT

Soham Pal
Indian Institute of Science, Bangalore
sohampal@iisc.ac.in

Yash Gupta∗
nference
yash@nference.net

Aditya Kanade
Indian Institute of Science, Bangalore
kanade@iisc.ac.in

Shirish Shevade
Indian Institute of Science, Bangalore
shirish@iisc.ac.in

ABSTRACT

Machine-Learning-as-a-Service providers expose machine learning (ML) models through application
programming interfaces (APIs) to developers. Recent work has shown that attackers can exploit
these APIs to extract good approximations of such ML models, by querying them with samples of
their choosing. We propose VarDetect, a stateful monitor that tracks the distribution of queries made
by users of such a service, to detect model extraction attacks. Harnessing the latent distributions
learned by a modiﬁed variational autoencoder, VarDetect robustly separates three types of attacker
samples from benign samples, and successfully raises an alarm for each. Further, with VarDetect
deployed as an automated defense mechanism, the extracted substitute models are found to exhibit
poor performance and transferability, as intended. Finally, we demonstrate that even adaptive attackers
with prior knowledge of the deployment of VarDetect, are detected by it.

Keywords machine-learning-as-a-service · security and privacy · model extraction

1

Introduction

The growing popularity of machine learning (ML) models has led to the rise of Machine-Learning-as-a-Service (MLaaS)
offerings. Typically, MLaaS providers expose cloud ML models through black-box request-response APIs, allowing
users to query the MLaaS model g with an input x of their choosing, and obtain the predicted output label or probability
vector g(x). As many proprietary MLaaS models bill users on a pro rata basis, model architecture and weights are often
withheld as a trade secret. The security and privacy of their black-box models is thus a key concern of MLaaS service
providers.

Recent work by Tramèr et al. [2016] has shown that attackers with only black-box access to MLaaS models can perform
model extraction attacks to obtain a close approximation ˜g ≈ g. For this, an attacker generates a set of labeled pairs
{(x, g(x))} by querying the MLaaS model g with samples x of its choosing. By training a new model on these pairs,
the attacker obtains the extracted substitute model ˜g.

Besides the obvious threat to the pay-per-query business model of black-box MLaaS models, gradients of ˜g can be
used to generate adversarial examples by adding human-imperceptible noise to samples so that they are misclassiﬁed
by g, as in Papernot et al. [2017]; or to speed up model inversion, as in Tramèr et al. [2016], which reveals part of the
conﬁdential dataset used to train g. Detecting and preventing model extraction attacks is thus key to building secure
MLaaS systems.

Cloud providers of MLaaS and other services deploy various monitoring tools to detect performance and security
issues. In this paper, we propose VarDetect as a stateful monitor for detecting model extraction attacks. By tracking
the distribution of queries made by each user to an ML model, VarDetect raises an alarm if the distribution of user

∗Work done while a graduate student at Indian Institute of Science, Bangalore.

 
 
 
 
 
 
Stateful Detection of Model Extraction Attacks

Train

DC

MLaaS model

g(·)

Allow

o

Raise
alarm

VarDetect

Prediction

g(x)

Query

x

Figure 1: Overview of our model extraction defense

queries deviates from the expected inputs (see Figure 1). The security team of the MLaaS provider can set various
thresholds for detection, and map them to alarms of increasing severity. They could then inspect the user activity to
take further necessary action. To summarize, we propose VarDetect – a monitor for the stateful detection of model
extraction attacks. VarDetect has the following advantages over prior work:

• VarDetect successfully detects all three classes of attackers proposed in the literature, while allowing access to

benign users.

• We demonstrate the effectiveness of VarDetect experimentally, across three diverse image classiﬁcation tasks,

wherein it reduces the accuracy and transferability of extracted models, as intended.

• VarDetect does not require access to attacker data.
• We demonstrate that VarDetect is effective against two classes of adaptive attackers, which are aware of its

deployment to safeguard the MLaaS model.

We make our source code available at https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect.
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect
https://github.com/vardetect/vardetect

2 Related Work

Lee et al. [2019], Orekondy et al. [2019a] propose algorithms intended to reduce the performance of model extraction by
perturbing the output probabilities returned by MLaaS models. While effective in reducing damage, it is not applicable
when the model returns only output labels. Zheng et al. [2019] introduce BDPL, a Boundary Differentially Private
Layer that generalizes this to binary classiﬁers that only return output labels, but the resulting approach causes the
MLaaS provider to deliberately returning incorrect labels with a low probability. Kesarwani et al. [2018] propose a
model extraction monitor that is speciﬁcally applicable to decision tree classiﬁers, and cannot be extended to neural
network classiﬁers. Juuti et al. [2019] propose PRADA, a defense against model extraction that is applicable only to
attackers that synthesize attacker queries either through perturbation or by taking linear combinations of samples from
the problem domain. Their algorithm cannot detect the most potent non-problem domain Orekondy et al. [2019b],
Pal et al. [2020] class of attacks. While Atli et al. [2019] can defend against such attacks, they assume access to the
attacker’s dataset, which is unrealistic in practice.

Our work is also closely related to the domain of anomaly detection. Prior work by Meng and Chen [2017] and
Santhanam and Grnarova [2018], Samangouei et al. [2018] have leveraged autoencoders (AEs) and generative adversarial
networks (GANs) respectively to protect models against adversarial examples. Andrews et al. [2016] propose the use
of AEs in combination with classic outlier detection methods (one-class support vector machine) for hybrid anomaly
detection. Deep generative models, including variation autoencoders (VAEs) have been used in myriad other ways
for outlier detection, see Chalapathy and Chawla [2019] for further details. While anomaly detection mechanisms
are typically harnessed to detect out-of-distribution samples for which the ML model may fallaciously predict a high
conﬁdence score for one of its labels, we are instead interested in safeguarding MLaaS models against model extraction.

3 Threat Model

Consider a k-category image classiﬁcation dataset DC. Let g denote the MLaaS model trained on this conﬁdential
dataset and g(x) denote the k-dimensional probability vector obtained by applying g to the input x ∈ [0, 1]d. We begin
by deﬁning the threat posed to g by model extraction attackers.

Attack surface Users of the MLaaS model may query the model with a sample x of their choosing. The MLaaS
API, in turn, responds by returning the prediction g(x) to the user. Users have no access to the weights or architecture

2

Stateful Detection of Model Extraction Attacks

DC

Syn

AdvPD

NPD

PD

AltPD

Figure 2: Representative comparison of attacker and benign samples

of g. For the purposes of this paper, each user corresponds to a single user account of an MLaaS service, used by an
individual.

Benign user capabilities As we expect benign users to have access to problem domain data, we model benign users
as users that query g with samples from either a:

1. Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set: This set of samples mimics the distribution of training data used to train g.
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
Problem-domain (PD) test set
2. Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set: This contains samples belonging to classes that are not part of
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
Alternative problem-domain (AltPD) test set
the training set, but are of similar nature (e.g., trafﬁc signs that are not part of the training set, as in Figure 2).
This is constructed using a set of held-out classes.

Attacker capabilities Attackers, much like benign users, may also query the model with inputs x of their choosing.
We assume, as in prior work such as Juuti et al. [2019], that the attacker has no or limited access to DC, but instead
draw samples from an attacker dataset DA composed of one or more of:

1. Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn): sampled from a multivariate uniform distribution, as in Tramèr et al.
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
Synthetically generated samples (Syn)
[2016],
2. Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD): by adding noise to a limited number of PD samples, as
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
Adversarially perturbed Problem Domain (AdvPD)
in Papernot et al. [2017], Juuti et al. [2019]
3. Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD): obtained by, e.g., crawling the public web for images, as in Correia-Silva
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
Non-Problem Domain data (NPD)
et al. [2018], Orekondy et al. [2019b], Pal et al. [2020].

Figure 2 visualizes samples for each of the 3 attacker and 2 benign datasets for an MLaaS model trained on the German
Trafﬁc Sign Recognition Benchmark of Houben et al. [2013].

4 Background

4.1 Variational Autoencoders

Variational autoencoders (VAEs), proposed originally by Kingma and Welling [2014] are a class of encoder-decoder
generative models. Much like autoencoders (AEs), VAEs reconstruct their input x at the output ˜x in a three step
process:

1. VAEs take as input a sample x, map it through an encoder to obtain parameters of a distribution, say:

where fµ, fσ are neural networks.

µ(x) = fµ(x), σ(x) = fσ(x)

3

Stateful Detection of Model Extraction Attacks

Input

x

Query
x

E
A
V

r
e
d
o
c
n
E

Enqueue

z

When the
buffer is full

Compute
MMD

Check

MMD >
δ

VarDetect

DC

δ

Else
Allow

Model

g

Output

g(x)

Yes
Raise alarm

o

Figure 3: The VarDetect framework: obtaining embeddings, MMD computation and thresholding. Conditional paths
not drawn lead to Else.

2. Using µ, σ, a latent variable z ∼ N (µ, σ2) is drawn.

When x is drawn from the same distribution as the training dataset for the VAE, z shall be incentivized to
follow N (µ0, σ2

0), where µ0, σ0 are user deﬁned parameters.

3. Finally, a decoder neural network is used to obtain the reconstruction, ˆx = fdec(z), desiring that ˆx ≈ x.

The training loss of the VAE is L = Llatent + ρ · Lrecon.

Llatent = Ex∼Dtrain [KL(N (µ(x), σ(x)2)||N (µ0, σ2
Lrecon = Ex∼Dtrain [(cid:107)x − ˆx(cid:107)2]

0))]

where Dtrain is the dataset on which the VAE is trained. Lrecon is the standard autoencoder reconstruction loss. As
described in step 2 above, the other loss term Llatent constrains z to follow the required distribution N (µ0, σ2

0).

4.2 Maximum Mean Discrepancy

The maximum mean discrepancy between generating distributions of datasets A and B is computed as:

(cid:13)
(cid:13)
(cid:13)

1
|A|

(cid:80)
a∈A

φ(a) − 1
|B|

(cid:80)
b∈B

φ(b)

(cid:13)
(cid:13)
(cid:13)2

We use the kernel trick (Gaussian kernel, with σg ∈ {1, 5, 10, 15, 20}) to replace the explicit dot product:

φ(z)T φ(z(cid:48)) = K(z, z(cid:48)) = (cid:80)
g

(cid:16)

exp

(cid:17)

− (cid:107)z−z(cid:48)(cid:107)2
2σ2
g

Gretton et al. [2012] showed that, under certain conditions, the MMD tends to zero asymptotically if the generating
distributions of A and B are the same. As shown in Algorithm 1, subsampling may be used to reduce the computation
cost, drawing M samples each of size N from A and B. In our experiments, we set M = 100 and subsample size
N = 20.

5 The Proposed VarDetect Monitor

We design VarDetect to continuously monitor the distribution of queries to g from each user. As shown in Figure 3,
VarDetect buffers incoming queries, and stores them in a queue of some ﬁxed size, m. Each query is added to this
queue. If a user’s queue is full, the oldest query is removed and the buffered queries are checked against the distribution
of training queries by computing the MMD between their latent distributions. Whenever the observed MMD exceeds a
speciﬁed threshold δ, an alarm is raised, pointing the security team to the suspicious user and their activity. Algorithm 1
details the procedure for outlier dataset generation and detection.

1. In lieu of requiring access to attacker samples as in Atli et al. [2019], we construct an outlier dataset DO to
contrast DC against. An immediate possibility is to use adversarial perturbations on the conﬁdential dataset
to construct the outlier dataset; however, ﬁrst, it has been shown by Carlini and Wagner [2017] that it is
easy to fool a network trained to detect adversarial examples; and second, we wish to avoid mimicking the
AdvPD attacker speciﬁcally. We demonstrate experimentally that by building DO by simply adding noise to
PD samples, not only does the resulting VAE learn to separate out all (Syn, AdvPD and NPD) attackers, but
that it is also more resilient to evasion attacks (as we shall show in Section 7.3.2). Future work may explore
other constructions of DO.

4

Stateful Detection of Model Extraction Attacks

Algorithm 1: VarDetect Framework

1 Procedure mmd(A, B)
33

for i = 1, 2, . . . M do

Ai ← {a ∼ A : j = 1, 2, . . . N } ;
Bi ← {b ∼ B : j = 1, 2, . . . N };

end

(cid:46) Subsample from both sets uniformly at random

(cid:46) Create outlier dataset

4

5

6

88

1010

3

4

3

4

5

6

7

8

9

J ←

M
(cid:83)
i=1
µ ← 1
|Ji|
return µ;

(cid:110)(cid:13)
(cid:13)
(cid:13)
(cid:80)

φ(a) − 1
|Bi|

(cid:80)
b∈Bi

φ(b)

(cid:13)
(cid:13)
(cid:13)2

(cid:111)
;

(cid:80)
a∈Ai

1
|Ai|

i Ji;

1212
1 Algorithm VarDetect-Precompute
2

for xi ∈ DC do

νi ∼ U (0, 1) ;
ni ∼ N (νi, 1)d;

77

5

99

end
DO ← ∪i{min(max(τ i · xi + (1 − τ i) · ni, 0), 1)};
fµ, fσ ← Train modiﬁed VAE on DC and DO;
u = {fµ(x) : x ∈ Dtrain
C };
1111
1 Algorithm VarDetect-Process-User
2

H ← QUEUE();
for input x from user do
H.ENQUEUE({x});
if H.LENGTH > m then
H.DEQUEUE();
if mmd(u, H) > δ then raise alarm ;

end

end

2. VarDetect trains a class-conditional variational autoencoder to map DC and DO to distinct regions in latent

space, by modifying the objective function:

Llatent = Ex∼DC
+ Ex∼DO

(cid:2)KL(cid:0)N (µ(x), σ(x)2)||N (µC, σ2
(cid:2)KL(cid:0)N (µ(x), σ(x)2)||N (µO, σ2

C)(cid:1)(cid:3)
O)(cid:1)(cid:3)

where µC, σC and µO, σO are chosen appropriately to separate the mappings of conﬁdential and outlier
samples in latent space. We note that the 2 classes modeled by this VAE are benign and outlier (at test time,
attacker) samples, and do not correspond to the classes of the original dataset used for training the model g.

3. VarDetect is stateful by design: at test time, VarDetect matches distribution by computing the MMD between
the latent mapping of a user’s query history and those of DC training samples. We expect benign users to
generate a lower test-time MMD than attackers, as their samples should more closely resemble DC. An alarm
is raised when the provider-speciﬁed threshold δ is crossed.

Note that unlike in the work of Atli et al. [2019] or the anomaly detection works discussed in Section 2, we are not
interested in detecting individual suspicious queries for two reasons: First, raising alarms for each such query can
overwhelm the security team of the MLaaS provider. Second, even benign users may occassionally make queries that
can be judged to be outliers. Such single outliers are not a security threat from the perspective of model extraction – we
therefore look for sustained malicious behaviors instead.

If and when an alarm is determined to be false by the security team (e.g., due to data drift), the provider may consider
adding the user’s samples to the VAE training dataset and retraining it, to further reduce the incidence of false alarms.

6 Experimental Setup

A brief summary of our experimental setup follows, with further details made available in our public repository.

5

Stateful Detection of Model Extraction Attacks

6.1 Network Architectures

Image Classiﬁers We use a convolutional neural network for the MLaaS and substitute models, having 3 blocks of
conv→batch_norm→conv→batch_norm→pool layers. The conv and pool kernels are 3 × 3 and 2 × 2. The number of
ﬁlters in the 3 blocks are 32, 64 and 128, and all activations are ReLU. The ﬁnal volume is ﬂattened and passed through
an output projection layer with a softmax activation.

VAE Encoder The VAE encoder passes the input x through 4 conv layers with 32, 64, 128 and 256 ﬁlters of size
4 × 4. The resulting volume is ﬂattened, and projected through a dense layer to a 512-dimensional vector. This vector
is passed through 2 separate feedforward networks to produce the mean µ and standard deviation σ vectors. All
activations are ReLU. Finally, z is sampled from N (µ, σ2).

VAE Decoder z is passed through a dense layer of size 512. The resulting vector is reshaped into a one-dimensional
volume, and passed through 4 deconv layers, with 256, 128, 64 and 32 ﬁlters of size 4 × 4. All activations are ReLU.
The ﬁnal volume is passed through a similar deconvolution layer (producing the required number of channels) with a
sigmoid activation to obtain the ﬁnal reconstructed image.

6.2 Datasets

We use a wide range of image classiﬁcation datasets as conﬁdential datasets, namely: the simple grayscale 10-class
Fashion-MNIST (F-MNIST) of Xiao et al. [2017], the color 10-class Street View House Numbers (SVHN) of Netzer
et al. [2011] and the color 43-class German Trafﬁc Sign Recognition (GTSR) benchmark of Houben et al. [2013]. Our
NPD attacker uses ImageNet samples of Deng et al. [2009] as a proxy for non-problem domain data, as in Pal et al.
[2020].

6.3 Hyperparameters

Network and Loss Dropout is applied at a rate of 0.2 on all layers with ReLU activations. The VAEs are trained with
a 32-dimensional latent variable. The loss is conﬁgured with a reconstruction loss multiplier ρ = 0.5. We use the means
µC = 0, µO = 5 · 1 and uncorrelated unit-variance σC = σO = 1, where 0 and 1 are zero and all-ones vectors, as
before. The Adam optimizer of Kingma and Ba [2015] is used.

Training Hyperparameters Convolutional layers in the classiﬁers use the He initializer, and all other layers use a
Glorot initializer. The VAE is trained for up to 500 epochs, until convergence. For the Syn and AdvPD attacks, the
substitute model is trained for 50 and 100 epochs respectively. For NPD attacks, training is performed up to 1000
epochs, employing early stopping with a patience of 10 epochs, validating on the F1 measure on the validation set.

7 Experimental Results

We extensively evaluate VarDetect against a suite of 12 attackers. In the main paper, we select 3 representative attacks:
uniform retraining of Tramèr et al. [2016] (Syn), the JSMA attack of Papernot et al. [2017] (AdvPD) and the ensemble
strategy of Pal et al. [2020] (NPD). Extended results for the remaining attacks are presented in Appendix A.

7.1 Comparison of Attackers and Benign Users

We ﬁrst study how the MMD values evolve for attackers and benign users. The AltPD case (as deﬁned in Section 3)
requires that the attacker has access to a set of classes that are not used during training. To ensure uniformity, we hold
out half of the classes, and train the classiﬁers and VAEs on the remaining half. All the attackers and benign users are
then evaluated on the same models. The same setting is used for plotting learned latent representations below. Other
than these experiments, we use the original splits of the datasets.

MMD over Time As shown in Figure 4, VarDetect maps both PD and AltPD benign user datasets to low MMD
values. Thus, VarDetect admits both PD and AltPD benign users; the latter contains classes not seen during training.

We also observe that the MMD values for Syn, AdvPD and NPD attackers are clearly well-separated, and much higher
than that for benign users. Thus, using an appropriate threshold, VarDetect can be deployed to detect the attackers
without unnecessarily ﬂagging benign users to the security team.

6

Stateful Detection of Model Extraction Attacks

0.25

0.25

0.25

100

200

300

(a) F-MNIST dataset

200

400

(b) GTSR dataset

100

200

300

(c) SVHN dataset

, Syn

Figure 4: The change in MMD (Y axis), as computed by VarDetect over time: for the PD test set

, AltPD test set
attackers. The X-axis indicates the cumulative number of queries ﬁred by
each user at the time of MMD computation. The process begins only after the query buffer for each user is full, i.e.,
H.LENGTH > m (we use m = 100 in our experiments).

and NPD

, AdvPD

DC

DO

Syn DA

(a) Syn Attacker

DC

DO

AdvPD DA

DC

DO

NPD DA

DC

DO

AltPD

(b) AdvPD Attacker

(c) NPD Attacker

(d) AltPD Attacker

Figure 5: Projection of the latent vectors z into 3D (via a PCA projection) for F-MNIST conﬁdential dataset DC, outlier
dataset DO and various attacker dataset DA samples – revealing that encodings of attacker samples lie closer to outlier
samples.

In Table 1, we summarize whether an alarm is raised or not for each of the benign and attacker datasets for different
threshold values. In general, by increasing the threshold, VarDetect is made gradually more forgiving: in the order PD,
then AltPD, then NPD, then AdvPD and ﬁnally Syn. Thus, Syn data is rejected with extreme ease, while NPD is the
hardest to reject. Using a threshold of 0.5 or greater allows us to admit PD and AltPD, while detecting all the attacker
cases. A threshold of 0.25 is sufﬁcient to allow PD benign users.

Learned Latent Space Representations To understand the success of VarDetect in distiguishing between benign
users and attackers, we inspect their latent representations. In Figure 5, we show PCA projections of VAE encodings of
the conﬁdential dataset DC and outlier dataset DO, along with the three attackers and the AltPD benign user. These are
shown for the F-MNIST dataset.

We observe that our modiﬁed VAE learns to clearly separate out DC and DO datasets. Further, each of the attacker
datasets are mapped away from DC and towards DO. The AltPD benign data points however get mapped closer to DC
than DO. This results in the MMD values for attackers being higher than MMD values for benign users.

7.2 Performance of Extracted Models (Accuracy and Transferability)

We now deploy VarDetect as an automated defense mechanism for the detection of all three attacks, using the original
splits of the datasets (PD benign users only). We set the threshold value to δ = 0.25, and block attackers once they
cross this threshold. The accuracy and transferability (of adversarial examples crafted using the test set from ˜g on to g)
for each dataset is tabulated in Tables 2 and 3 for attackers with a budget of 100K queries. We make extended results for
other budgets and attacks available in Appendix A. The deployment of VarDetect reduces both metrics of the extracted
model, demonstrating that the attacks are foiled.

7

Stateful Detection of Model Extraction Attacks

Table 1: Detection at different thresholds: (cid:55) indicates that an alarm has not been raised.

Threshold

Syn

AdvPD

NPD

AltPD

PD

0.00
0.25
0.50
1.00
1.50
2.50

Alarm Alarm Alarm Alarm Alarm
Alarm Alarm Alarm Alarm
Alarm Alarm Alarm
Alarm Alarm Alarm
Alarm Alarm Alarm
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)

(cid:55)

(a) F-MNIST

Threshold

Syn

AdvPD

NPD

AltPD

PD

0.00
0.25
0.50
1.00
1.50
2.50

Alarm Alarm Alarm Alarm Alarm
Alarm Alarm Alarm Alarm
Alarm Alarm Alarm
Alarm Alarm
Alarm
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)

(b) GTSR

Threshold

Syn

AdvPD

NPD

AltPD

PD

0.00
0.25
0.50
1.00
1.50
2.50

Alarm Alarm Alarm Alarm Alarm
Alarm Alarm Alarm
Alarm Alarm Alarm
Alarm Alarm
Alarm
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)
(cid:55)

(cid:55)
(cid:55)

(c) SVHN

Table 2: The test set accuracy (%) of substitute models obtained by model extraction attackers, when the MLaaS model
is defended using VarDetect, compared to when it is not (lower is better), for attackers with a budget of 100,000 queries.

Undefended

Defended

Syn

AdvPD NPD

Syn

AdvPD NPD

F-MNIST 25.33
69.10
GTSR
50.71
SVHN

84.66
90.44
71.12

81.10
93.96
92.57

7.56
5.19
14.57

75.50
54.07
41.38

12.24
6.74
15.67

7.3 Adaptive Attackers

Carlini et al. [2019] demonstrate that defenses which are evaluated only against known attacks could fail with simple
adaptations that exploit knowledge about the defense. For a comprehensive evaluation of VarDetect in situations where
insider knowledge has been compromised, we consider two adaptive attackers:

7.3.1 Spaced-out Adaptive Attackers

We ﬁrst consider an attacker that is aware of the stateful nature of VarDetect, and its query buffering strategy. Such an
attacker can intelligently space out its DA queries over time, and, in the interim, ﬁre innocuous queries from a benign

8

Stateful Detection of Model Extraction Attacks

Table 3: The test set transferability (%) of substitute models obtained by model extraction attackers, when the MLaaS
model is defended using VarDetect, compared to when it is not (lower is better), for attackers with a budget of 100,000
queries.

Undefended

Defended

Syn

AdvPD NPD

Syn

AdvPD NPD

F-MNIST 60.74
84.51
GTSR
82.51
SVHN

68.73
80.81
82.30

78.74
94.56
91.30

42.35
13.61
31.54

63.13
68.53
53.52

55.79
37.94
26.87

DC

Syn

Perturbed

DC

AdvPD

Perturbed

DC

NPD

Perturbed

(a) Syn attacker

(b) AdvPD attacker

(c) NPD Attacker

Figure 6: Projection of the latent vectors z into 3D (via a PCA projection) for samples x drawn from various attacker
datasets and those of their corresponding perturbed counterparts (generated by white-box gradient adaptive attackers),
obtained after 500 iterations of FGSM with a step size of (cid:15) = 0.001, compared to latent vectors of conﬁdential dataset
samples. Plots are generated for the MNIST dataset.

distribution, e.g., from a limited dataset of PD samples. Let the dilution factor of an attacker be the rate at which it ﬁres
malicious queries, e.g., an attacker with dilution factor 10% ﬁres 10 attacker queries, followed by 90 benign queries.
Note that our spaced-out attacker also addresses the scenario in which one or more individuals who share a single
MLaaS user account are malicious (but not all).

We consider two sets of such attackers extracting F-MNIST at dilution factors of 5% and 15% respectively, and plot the
corresponding MMD over time in Figure 7. The attackers with a dilution factor of 15% are detected using a threshold
of δ = 0.25, while the attackers using a dilution factor of 5% are not. By titrating the threshold value δ, the security
team can make VarDetect sensitive to various values of MMD over time. As the threshold is lowered, the attacker is
forced to decrease their dilution factor, at the cost of increased query complexity.

We present extended results for the spaced-out attacker in Appendix A, for dilution factors of 5%, 15% and 25% across
all three conﬁdential datasets. We summarize our ﬁndings as follows: using a threshold of δ = 0.25 allows us to detect
all attackers at a dilution factor of 15%, except for the NPD attacker on the GTSR dataset. This, too, is detected at a
dilution factor of 25%. Consequently, the overhead of having to ﬁre innocuous queries is increased by 4×-6.67×.

7.3.2 White-Box Gradients Attackers

Next, we consider an adaptive attacker that is motivated to perturb attacker samples x ∈ DA to form modiﬁed ˜x = x+∆
such that they are more likely to pass for benign samples, i.e., the encoding of ˜x is closer to the encodings of DC
samples than it is to the encodings of DO samples. We assume that this adaptive attacker has white-box access to the
VAE encoder fµ, as well as knowledge of the values of µO, µC, σO and σC chosen by the MLaaS service provider.

Attack Method Starting with a sample x ∈ DA, our attacker uses iterative FGSM to nudge its latent encoding in the
direction µC, with the intent of avoiding detection:

Repeatedly: x ← x + (cid:15) sgn(∇x(cid:107)fµ(x) − µC(cid:107))

9

Stateful Detection of Model Extraction Attacks

0.25

0.25

200

400

600

800

(a) Spaced-out attacker: 5%

200

400

600

800

(b) Spaced-out attacker: 15%

Figure 7: MMD computed by VarDetect over time (F-MNIST dataset) for spaced-out attackers at different dilutions.
(PD

, AdvPD

, NPD

, Syn

)

We perform this experiment for all three sets of attackers against a classiﬁcation model trained on the MNIST dataset of
Lecun et al. [1998], and plot 3D projections of the initial and ﬁnal latent space encodings after the attack in Figure 6
(after 500 iterations using (cid:15) = 0.001). As is evident from these ﬁgures, our adaptive attacker fails to use iterative FGSM
method to avoid detection, and the perturbed samples are detected at a threshold of δ = 0.25.
We perform a grid search by varying (cid:15) in the range (cid:15) ∈ {1, 10−1, 10−2, 10−3, 10−4}, and run each instance up to 5000
iterations of FGSM. Note that (cid:15) may be viewed either as the step size (holding a constant learning rate of 1), or as the
learning rate (holding step size constant as 1). In no case does the attack succeed.

8 Conclusion

In this work, we design VarDetect: a framework to detect model extraction attacks targeted at MLaaS providers, by
continuously monitoring the queries made by each user to it. VarDetect requires no access to attacker data, and it is
the ﬁrst detection mechanism that raises an alarm for all three types of model extraction attacks in the literature. We
demonstrate that with VarDetect deployed as an automated defense mechanism, the task accuracy of extracted substitute
models is reduced. Finally, VarDetect is demonstrated to hold up against two types of adaptive attackers: either halting
extraction altogether, or increasing the overhead of innocuous queries.

10

Stateful Detection of Model Extraction Attacks

References

Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. Stealing machine learning models via

prediction APIs. In USENIX Security 16, 2016.

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. Practical
black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and
Communications Security, ASIA CCS ’17. ACM, 2017.

T. Lee, B. Edwards, I. Molloy, and D. Su. Defending against neural network model stealing attacks using deceptive

perturbations. In 2019 IEEE Security and Privacy Workshops (SPW), 2019.

Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Utility-constrained defenses against

model stealing attacks. CoRR, abs/1906.10908, 2019a. URL http://arxiv.org/abs/1906.10908.

Huadi Zheng, Qingqing Ye, Haibo Hu, Chengfang Fang, and Jie Shi. BDPL: A boundary differentially private layer
against machine learning model extraction attacks. In European Symposium on Research in Computer Security,
pages 66–83. Springer, 2019.

Manish Kesarwani, Bhaskar Mukhoty, Vijay Arya, and Sameep Mehta. Model extraction warning in MLaaS paradigm.

In Proceedings of the 34th Annual Computer Security Applications Conference. ACM, 2018.

Mika Juuti, Sebastian Szyller, Alexey Dmitrenko, Samuel Marchal, and N. Asokan. PRADA: Protecting against DNN

model stealing attacks. In 2019 IEEE European Symposium on Security and Privacy (EuroS&P), 2019.

Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff Nets: Stealing functionality of black-box models. In

Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4954–4963, 2019b.

Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish K. Shevade, and Vinod Ganapathy. ActiveThief: Model
extraction using active learning and unannotated public data. In Proceedings of the Thirty-Fourth AAAI Conference
on Artiﬁcial Intelligence, AAAI’20. AAAI Press, 2020.

Buse Gul Atli, Sebastian Szyller, Mika Juuti, Samuel Marchal, and N. Asokan. Extraction of complex DNN models:
Real threat or boogeyman? In AAAI-20 Workshop on Engineering Dependable and Secure Machine Learning
Systems, 2019.

Dongyu Meng and Hao Chen. MagNet: A two-pronged defense against adversarial examples. In Proceedings of the
2017 ACM SIGSAC Conference on Computer and Communications Security, CCS ’17. Association for Computing
Machinery, 2017.

Gokula Krishnan Santhanam and Paulina Grnarova. Defending against adversarial attacks by leveraging an entire GAN.

CoRR, abs/1805.10652, 2018. URL http://arxiv.org/abs/1805.10652.

Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-GAN: Protecting classiﬁers against adversarial
attacks using generative models. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=BkJ3ibb0-.

Jerone Andrews, Edward Morton, and Lewis Grifﬁn. Detecting anomalous data using auto-encoders. International

Journal of Machine Learning and Computing, 6:21, 2016.

Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. CoRR, abs/1901.03407,

2019. URL http://arxiv.org/abs/1901.03407.

Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue, Alberto F. de Souza, and Thiago Oliveira-Santos.
Copycat CNN: Stealing knowledge by persuading confession with random non-labeled data. In 2018 International
Joint Conference on Neural Networks (IJCNN), 2018.

Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of trafﬁc signs in
real-world images: The german trafﬁc sign detection benchmark. In 2013 International Joint Conference on Neural
Networks (IJCNN). IEEE, 2013.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference on Learning

Representations, 2014. URL https://openreview.net/forum?id=33X9fd2-9FyZd.

A Gretton, K. Borgwardt, Malte Rasch, B. Schölkopf, and AJ Smola. A kernel two-sample test. The Journal of Machine

Learning Research, 13:723–773, 2012.

Nicholas Carlini and David A. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods.

CoRR, abs/1705.07263, 2017. URL http://arxiv.org/abs/1705.07263.

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking machine

learning algorithms. CoRR, abs/1708.07747, 2017. URL http://arxiv.org/abs/1708.07747.

11

Stateful Detection of Model Extraction Attacks

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural
images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,
2011.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image

database. In 2009 IEEE conference on computer vision and pattern recognition. IEEE, 2009.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on

Learning Representations, 2015. URL https://openreview.net/forum?id=8gmWwjFyLj.

Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian J. Goodfellow,
Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. CoRR, abs/1902.06705, 2019. URL
http://arxiv.org/abs/1902.06705.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings

of the IEEE, 1998.

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
International Conference on Learning Representations, 2015. URL https://arxiv.org/abs/1412.6572.

In

Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. CoRR,

abs/1607.02533, 2016.

David D. Lewis and William A. Gale. A sequential algorithm for training text classiﬁers. In Proceedings of the 17th

Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1994.

Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to
fool deep neural networks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer
Society, 2016.

Melanie Ducoffe and Frédéric Precioso. Adversarial active learning for deep networks: a margin based approach.

CoRR, abs/1802.09841, 2018. URL http://arxiv.org/abs/1802.09841.

Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. In International

Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1aIuk-RW.

Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry
Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg,
Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems,
2015. URL http://tensorflow.org/. Software available from tensorﬂow.org.

12

Stateful Detection of Model Extraction Attacks

A Additional Model Extraction Attacks

In this section, we demonstrate the efﬁcacy of VarDetect against a larger suite of attacks. We consider the following
classes of attacks:

A.1 Synthetic Attacks (Syn)

In addition to the Uniform Retraining attack of Tramèr et al. [2016], we additionally evaluate their Line Search
Retraining attack, using the publicly available implementation2. As no implementation of Adaptive Retraining is
available for their experiments on neural networks, we omit this attack.

A.2 Adversarial Problem Domain Attacks (AdvPD)

In addition to the JSMA attack of Papernot et al. [2017], we also evaluate the following additional attacks introduced by
Juuti et al. [2019]:

• Non-targeted FGSM (N FGSM), which uses the FGSM method of adversarial example generation proposed
by Goodfellow et al. [2015], to perturb x away from its original label y (L being the classiﬁer loss function):

˜x ← x + (cid:15) · sgn(∇xL(y, S(x)))

• Non-targeted iterative FGSM (N I-FGSM), where iterative FGSM of Kurakin et al. [2016] is used in lieu

of FGSM (the objective remaining unchanged from the NF attack)

• Targeted FGSM (T-RND FGSM), which uses FGSM to perturb x towards a target class yr chosen uniformly

at random from among all possible classes:

˜x ← x − (cid:15) · sgn(∇xL(yr, S(x)))

• Targeted iterative FGSM (T-RND I-FGSM), where iterative FGSM is used in lieu of FGSM (the objective

remaining the same as that of the TF attack)

We implement these attacks, using these update rules, as described in the original paper by Papernot et al. [2017].

A.3 Non-Problem Domain Attacks (NPD)

In addition to the Adversarial + K-Center strategy outlined in the paper, we consider with the following additional
strategies proposed by Pal et al. [2020]:

• Random, where the samples x to be queried are chosen uniformly at random

• Uncertainty, where samples x with the highest entropy of the predicted probability vector S(x) are chosen,

as in Lewis and Gale [1994], where entropy is calculated as:

H(cid:0)S(x)(cid:1) = (cid:80)

log Sj(x)

j

where Sj(x) is the jth component of the vector S(x).

• DeepFool-based Active Learning (DFAL), where samples which lie close to the decision boundary are
chosen. The DeepFool technique of Moosavi-Dezfooli et al. [2016] is used on samples x to obtain adversarial
˜x. α = (cid:107)x − ˜x(cid:107) is computed, and samples with the lowest α values are chosen, following the method outlined
by Ducoffe and Precioso [2018].

• k-Center, where diverse samples are selected by choosing samples that lie farthest apart in an Euclidean

distance sense, as in Sener and Savarese [2018].

Our implementations of these attacks are based on their public implementations 3.

2https://github.com/ftramer/Steal-ML/
3https://bitbucket.org/iiscseal/activethief/

13

Stateful Detection of Model Extraction Attacks

0.25

0.25

0.25

200

400

600

800

(a) 5% dilution, F-MNIST

200

400

600

800

200

400

600

800

(b) 15% dilution, F-MNIST

(c) 25% dilution, F-MNIST

0.25

0.25

200

400

600

800

(d) 5% dilution, GTSR

0.25

0.25

0.25

200

400

600

800

200

400

600

800

(e) 15% dilution, GTSR

(f) 25% dilution, GTSR

0.25

200

400

600

800

200

400

600

800

200

400

600

800

(g) 5% dilution, SVHN

(h) 15% dilution, SVHN

(i) 25% dilution, SVHN

Figure 8: Change in MMD for different datasets when an adaptive attacker ﬁres queries at different dilution fractions
and NPD
(the plot colors and patterns follow the same conventions as before, i.e., PD

, AdvPD

, Syn

).

A.4 Model Accuracy and Transferability

We calculate the test accuracy (%) and transferability success rate (%) of adversarial examples when using a threshold
of δ = 0.25 to automatically block attackers, and present the results in Tables 4 and 5 respectively. As before,
the substitute model task accuracy for models extracted from a defended model is lower than those extracted from
undefended models and the transferability success rate is almost always lowered when crafting adversarial examples
using the extracted model. Thus, our observations from Section 7.2 are consistent with a far broader range of attacks
and query budgets.

A.5 Spaced-out Adaptive Attacker

In Figure 8, we present extended results for the spaced-out attacker we present in Section 7.3.1. A threshold of δ = 0.5
is adequate to detect all three types of attacks at dilutions of 15% or above, with the sole exception of the NPD attack
on the GTSR dataset. The NPD attacker for the GTSR dataset is detected at a threshold of 25%, as shown.

B Conﬁguration and Reproducibility

Our deep learning models are implemented in Python 2.7.17, using the TensorFlow 1.14 framework of Abadi et al.
[2015], and are executed on an NVIDIA GPU using CUDA 10.0 and NVIDIA cuDNN 7.6.4. We additionally use a
number of Python packages speciﬁed in the requirements.txt ﬁle of our public code repository.

We perform our experiments on a server with a 20-core Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz processor, 32
GB of system memory and equipped with a Titan X (Pascal) GPU accelerator with 12GB of vRAM, running on the
Ubuntu 18.04.5 LTS (Bionic Beaver) operating system.

In all of our experiments, we use a program-wide seeds to ensure reproducibility. However, due to the underlying
nature of cuDNN-reliant operations such as tf.reduce_sum, GPU non-determinism may cause the weights of the
trained models to change across multiple runs, even on the same system. To this end, we apply the TensorFlow
Determinism patch made available by NVIDIA4 to reduce GPU non-determinism and aid in reproducibility. We tested
our experiments across a number of different servers, and found the results to be consistent within a margin of error.
Further information about our seeds is available in the public repository we release as part of this work.

4https://github.com/NVIDIA/framework-determinism

14

Stateful Detection of Model Extraction Attacks

Table 4: Test set accuracy (%) of substitute models obtained by model extraction attackers, when the MLaaS model is
defended using VarDetect (blocking attacks that cross a MMD threshold of δ = 0.5), compared to when it is not. The
results clearly indicate that VarDetect reduces the accuracy of the extracted model, as desired.

Attacker query budget →

∞

10K

15K

20K

25K

30K

100K

Defended

Undefended

FASHION-MNIST of Xiao et al. [2017]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

7.56
6.78
75.50
74.37
76.61
76.68
74.78
9.96
9.99
10.02
10.00
12.24

15.64
11.91
80.86
79.27
80.93
79.74
82.61
65.06
70.33
60.04
77.71
71.81

13.97
14.24
79.65
79.11
81.72
79.08
82.80
72.89
77.94
76.39
74.89
75.81

21.34
14.31
82.61
78.57
82.90
80.69
84.55
70.92
73.99
75.87
79.96
81.39

14.06
12.94
81.53
81.15
84.07
78.15
85.20
72.07
76.36
78.97
78.80
79.92

GERMAN TRAFFIC SIGN RECOGNITION of Houben et al. [2013]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

5.19
3.28
54.07
58.02
60.16
57.43
55.75
7.40
6.02
7.59
5.76
6.74

17.58
10.15
73.99
71.86
65.86
69.02
67.34
53.02
54.45
59.39
53.94
56.20

29.89
12.60
79.80
74.27
67.93
76.57
72.59
63.48
61.33
65.58
62.31
64.39

33.50
18.61
82.32
75.87
72.71
75.91
71.48
64.92
69.02
68.95
66.04
68.83

33.59
29.88
81.00
77.17
68.27
80.45
75.90
68.44
74.03
71.59
69.75
70.10

STREETVIEW HOUSE NUMBERS of Netzer et al. [2011]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

14.57
11.05
41.38
42.77
36.71
36.24
40.19
15.42
12.50
11.90
15.33
15.67

16.43
15.15
57.17
60.14
43.62
63.17
48.01
65.30
64.75
68.55
67.54
67.93

37.91
24.17
59.84
63.84
46.72
65.21
48.81
74.34
66.79
68.73
67.22
70.09

38.56
27.47
59.82
66.97
42.02
65.80
46.53
71.78
69.59
74.70
73.82
72.55

39.45
40.64
65.23
65.62
48.32
65.91
54.45
74.32
69.31
71.42
76.79
74.84

15.85
15.28
83.92
82.92
84.35
78.72
84.63
69.67
77.76
78.03
81.99
80.40

44.21
33.78
85.83
77.23
64.75
79.93
84.80
68.08
74.98
70.44
68.02
71.76

45.22
45.02
65.03
66.86
48.08
68.40
53.90
74.18
73.71
77.25
77.47
77.22

25.33
20.47
84.66
83.63
86.19
82.87
87.34
79.64
83.15
79.30
82.25
80.77

69.10
72.67
90.44
83.60
80.72
85.07
91.11
85.69
83.93
85.42
86.60
84.45

50.71
54.38
71.12
67.83
53.15
68.82
59.75
81.93
82.43
79.07
82.76
82.49

15

Stateful Detection of Model Extraction Attacks

Table 5: The transferability success rate (%) of substitute models obtained by model extraction attackers, when the
MLaaS model is defended using VarDetect (blocking attacks that cross a MMD threshold of δ = 0.5), compared to
when it is not. The results clearly indicate that in most cases, VarDetect reduces the transferability success rate of the
extracted model, as desired.

Attacker query budget →

∞

10K

15K

20K

25K

30K

100K

Defended

Undefended

FASHION-MNIST of Xiao et al. [2017]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

42.35
46.12
63.13
58.92
62.36
64.53
64.86
56.73
54.20
54.21
56.83
55.79

59.56
61.48
62.22
62.02
57.00
72.14
62.28
72.29
79.97
75.93
75.50
79.63

59.14
60.30
63.38
68.91
58.72
64.37
61.02
79.94
78.58
72.69
79.17
77.91

59.31
61.92
63.41
67.20
60.47
66.72
61.79
71.34
80.81
78.42
82.05
74.93

56.67
63.35
65.74
69.61
59.46
65.66
61.12
68.38
81.63
82.10
83.71
76.99

GERMAN TRAFFIC SIGN RECOGNITION of Houben et al. [2013]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

13.61
33.16
68.53
67.79
56.96
62.26
62.83
31.16
29.28
35.95
24.74
37.94

64.10
54.24
72.43
77.64
49.98
80.16
62.33
83.50
82.43
82.34
85.23
84.90

68.79
59.41
74.28
79.28
57.14
79.26
68.31
85.76
86.67
86.48
87.80
80.51

72.32
66.85
73.45
78.27
43.79
80.10
62.14
88.67
87.58
86.19
84.38
88.89

73.14
73.20
76.94
81.20
41.99
80.11
69.86
87.88
88.32
89.68
89.83
90.72

STREETVIEW HOUSE NUMBERS of Netzer et al. [2011]

Syn-Uniform Retraining
Syn-Line Search Retraining
AdvPD JSMA
AdvPD N FGSM
AdvPD N I-FGSM
AdvPD T-RND FGSM
AdvPD T-RND I-FGSM
NPD-ActiveThief (Random)
NPD-ActiveThief (Uncertainty)
NPD-ActiveThief (DFAL)
NPD-ActiveThief (k-Center)
NPD-ActiveThief (DFAL + k-Center)

31.54
28.12
53.52
57.92
50.02
52.31
54.61
24.22
24.82
25.49
41.81
26.87

33.55
31.71
67.69
77.21
28.68
77.44
46.44
81.54
84.30
84.76
83.39
84.57

74.85
42.29
65.74
79.73
39.56
80.06
42.23
87.35
84.64
87.27
86.02
85.19

76.64
51.03
70.96
78.57
22.55
80.54
39.34
88.28
86.44
87.77
87.27
84.42

76.02
71.08
73.90
78.87
30.56
80.29
53.49
88.24
87.24
85.97
88.64
88.89

58.17
64.49
62.22
68.66
61.36
66.30
60.59
73.60
79.20
77.94
84.47
71.16

75.25
74.69
78.24
80.10
59.78
80.38
73.64
89.61
89.93
89.41
90.68
90.40

77.02
79.35
75.09
82.20
44.28
82.07
55.56
89.95
88.51
88.76
87.95
86.52

60.74
58.96
68.73
72.05
64.19
71.88
65.67
78.74
80.74
83.11
80.35
81.10

84.51
88.08
80.81
85.57
66.00
84.62
77.90
94.56
94.28
94.42
92.97
93.96

82.51
85.70
82.30
83.22
39.75
82.58
57.27
91.30
92.52
92.01
92.11
92.57

16

