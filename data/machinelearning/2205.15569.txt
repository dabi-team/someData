2
2
0
2

y
a
M
1
3

]

G
L
.
s
c
[

1
v
9
6
5
5
1
.
5
0
2
2
:
v
i
X
r
a

GSR: A Generalized Symbolic Regression Approach

Tony Tohme1,2
tohme@mit.edu

Dehong Liu2,
†
liudh@merl.com

Kamal Youcef-Toumi1
youcef@mit.edu

1Massachusetts Institute of Technology

2Mitsubishi Electric Research Laboratories

Abstract

Identifying the mathematical relationships that best describe a dataset remains a
very challenging problem in machine learning, and is known as Symbolic Regres-
sion (SR). In contrast to neural networks which are often treated as black boxes,
SR attempts to gain insight into the underlying relationships between the indepen-
dent variables and the target variable of a given dataset by assembling analytical
functions.
In this paper, we present GSR, a Generalized Symbolic Regression
approach, by modifying the conventional SR optimization problem formulation,
while keeping the main SR objective intact. In GSR, we infer mathematical rela-
tionships between the independent variables and some transformation of the target
variable. We constrain our search space to a weighted sum of basis functions, and
propose a genetic programming approach with a matrix-based encoding scheme.
We show that our GSR method outperforms several state-of-the-art methods on the
well-known SR benchmark problem sets. Finally, we highlight the strengths of
GSR by introducing SymSet, a new SR benchmark set which is more challenging
relative to the existing benchmarks.

1 Introduction

Symbolic regression (SR) aims to ﬁnd a mathematical expression that best describes the relation-
ship between the independent variables and the target (or dependent) variable based on a given
dataset. By inspecting the resulting expression, we may be able to identify nontrivial relations
and/or physical laws which can provide more insight into the system represented by the given
dataset. SR has gained tremendous interest and attention from researchers over the years for many
reasons. First, many rules and laws in natural sciences (e.g.
in physical and dynamical systems
[36, 38]) are accurately represented by simple analytical equations (which can be explicit [6] or im-
plicit [16, 26]). Second, in contrast to neural networks that involve complex input-output mapping,
and hence are often treated as black boxes which are difﬁcult to interpret, SR is very concise and
interpretable. Finally, symbolic equations may outperform neural networks in out-of-distribution
generalization (especially for physical problems) [11].

SR does not require a priori speciﬁcation of a model. Conventional regression methods such as least
squares [50], likelihood-based [14, 34, 42], and Bayesian regression techniques [21, 22, 40, 41, 48]
use ﬁxed-form parametric models and optimize for the model parameters only. SR seeks to ﬁnd both
a model structure and its associated parameters simultaneously.

Related Work. The SR problem has been widely studied in the literature [19, 33]. SR can be a
very challenging problem and is thought to be NP-hard [23, 35, 45], particularly when the search
space of analytical functions grows exponentially with the dimension of the input feature vector
(i.e. the number of independent variables). Several approaches have been suggested over the years.
Most of the methods use genetic (or evolutionary) algorithms [3, 18, 38, 49]. Some more recent
methods are Bayesian in nature [15], some are physics-inspired [45], and others use divide-and-
conquer [24] and block building algorithms [8, 9, 10]. Lately, researchers proposed using machine
learning algorithms and neural networks [1, 2, 4, 17, 27, 30, 35, 37, 44, 47] to solve the SR prob-
lem. Furthermore, some works suggested constraining the search space of functions to generalized

†Corresponding author.

 
 
 
 
 
 
linear space [31] (e.g. Fast Function eXtraction [28], Elite Bases Regression [7], etc.) which
proved to accelerate the convergence of genetic algorithms signiﬁcantly (at the expense of sometimes
losing the generality of the solution [24]).

Many SR methods (especially the ones relying on genetic programming) use a tree-based implemen-
tation, where analytical functions are represented by expression trees. However, these methods are
difﬁcult to code and can be computationally expensive as the search space can be very complex con-
taining expressions of any size and length [12]. Some approaches suggested encoding functions as
an integer string [32], others proposed representing them using matrices [7, 12, 13, 25]. These ap-
proaches made the coding and implementation very simple noting that the search space can still be
large depending on the size of the strings or matrices representing the functions.

Our Contribution. We present Generalized Symbolic Regression (GSR), by modifying the conven-
tional SR optimization problem formulation, while keeping the main SR objective intact. In GSR, we
identify mathematical relationships between the independent variables (or features) and some trans-
formation of the target variable. In other words, we learn the mapping from the feature space to a
transformed target space (where the transformation applied to the target variable is also learned dur-
ing this process). To ﬁnd the appropriate functions (or transformations) to be applied to the features as
well as to the targets, we constrain our search space to a weighted sum of basis functions. In contrast
to conventional tree-based genetic programming approaches, we propose a matrix-based encoding
scheme to represent the basis functions (and hence the full mathematical expressions). We run a se-
ries of numerical experiments on the well-known SR benchmark datasets and show that our proposed
method is competitive with many state-of-the-art methods. Finally, we introduce SymSet, a new SR
benchmark problem set that is more challenging than existing benchmarks.

2 Notation and Problem Formulation

N
i=1 consisting of N
Consider the following regression task. We are given a dataset
D
R
Rd denotes the ith d-dimensional input feature vector and yi ∈
i.i.d. paired examples, where xi ∈
represents the corresponding continuous target variable. The goal of SR is to search the space of all
deﬁned by a set of given mathematical functions (e.g., exp, ln,
possible mathematical expressions
), along with the following optimization problem:
sin, cos) and arithmetic operations (e.g., +,

xi, yi}

=

S

{

,

−

×
f ∗ = argmin
∈S

f

,
÷
N

i=1
X

(cid:2)

f (xi)

yi

−

2

(cid:3)

(1)

where f is the model function and f ∗ is the optimal model.

3 Generalized Symbolic Regression (GSR)

In this section, we introduce our Generalized Symbolic Regression (GSR) approach. We present its
problem formulation, and discuss its solution and implementation.

3.1 Modifying the goal of symbolic regression

As highlighted in Section 2, the goal of SR is to search the function space to ﬁnd the model that best
ﬁts the mapping between the independent variables and the target variable (i.e. the mapping between
xi and yi, for all i). In our proposed GSR approach, we tweak the goal of SR; we instead search the
function space to ﬁnd the model that best describes the mapping between the independent variables
and a transformation of the target variable (i.e. the mapping between xi and some transformation or
function of yi, for all i). Formally, we propose modifying the goal of SR to search for appropriate
deﬁned by a set of given
(model) functions from a space of all possible mathematical expressions
mathematical functions (e.g., exp, ln, sin, cos) and arithmetic operations (e.g., +,
), which
,
can be described by the following optimization problem:

−

÷

×

S

,

f ∗,g∗ = arg min
∈S

f,g

N

f (xi)

−

2

g(yi)

(2)

i=1
X
(cid:2)
where f ∗ and g∗ are the optimal analytical functions.
In our proposed GSR method, instead of learning a mathematical expression of the form y = f (x),
we learn an expression of the form g(y) = f (x). By doing so, we may be able to i) ﬁnd (or converge
to) a mathematical expression much faster, and ii) apply many basis functions to the target variable

(cid:3)

2

(which is a scalar) without much increasing the complexity of the expression (and hence we may be
able to avoid some of the complicated basis functions applied to the input variables by applying their
corresponding inverse transformations to the target variable). We illustrate this concept in Table 1.

Table 1: GSR ﬁnds analytical expressions of the form g(y) = f (x) instead of y = f (x).

Ground Truth Expression Learned Expression

y = √x+5
y = 1/(3x1+x3
2)
y = (2x1 +x2)−
y = ln(x3
y = ex3

1 +4x1x2)
1+2x2+cos(x3)

2
3

1 = 3x1+x3
2

y2 = x+5
y−
ln(y) =
ey = x3
ln(y) = x3

−

1 +4x1x2

2
3 ln(2x1 +x2)

1 +2x2+cos(x3)

Although the main goal of GSR is to ﬁnd expressions of the form g(y) = f (x), we may encounter
situations where it is best to simply learn expressions of the form y = f (x) (i.e. g(y) = y). For
instance, consider the ground truth expression y = sin(x1)+ 2x2. In this case, we expect to learn the
expression exactly as is (i.e. g(y) = y and f (x) = sin(x1) + 2x2) as long as the right basis functions
(i.e. sin(x) and x in this case) are within the search space, as we will see in the next sections.

3.2 A new problem formulation for symbolic regression

Now that we have presented the goal of our proposed GSR approach (summarized by Equation (2)),
to reduce the computational challenges and
we need to constrain the search space of functions
to functions that can
accelerate the convergence of our algorithm. Inspired by [7, 28], we conﬁne
be expressed as a linear combination (or as a weighted sum) of basis functions (which can be linear
or nonlinear). In mathematical terms, for a given input feature vector xi and a corresponding target
variable yi, the search space

is constrained to model functions of the form:

S

S

Mφ

Mψ

αjφj (xi),

g(yi) =

βjψj(yi)

(3)

) and ψj(
·

) are the basis functions applied to the feature vector xi and the target variable yi,
where φj (
·
respectively, and Mφ and Mψ are the corresponding number of basis functions involved, respectively.
In matrix form, the minimization problem described in Equation (2) is equivalent to ﬁnding the vectors
of coefﬁcients α = [α1···

αMφ]T and β = [β1···

j=1
X

α∗,β∗ = argmin

βMψ ]T such that:
Yβ
Xα

α,β ||

−

2

||

where

X = 


Xα

φ1(x1)
...

φ2(x1)
...

φ1(xN ) φ2(xN )

φMφ (x1)
...
φMφ (xN )

···

···
···

, Y = 






ψ1(y1)
...

ψ2(y1)
...

ψ1(yN ) ψ2(yN )

···

···
···

ψMψ (y1)
...
ψMψ (yN )






||

−

Yβ

Note that if we examine the minimization problem as expressed in Equation (4), we can indeed mini-
2 by simply setting α∗ = 0 and β∗ = 0 which will not lead to a meaningful solution
mize
||
to our GSR problem. In addition, to avoid reaching overly complex mathematical expressions for f (
)
·
), we are interested in ﬁnding sparse solutions for the weight vectors α∗ and β∗ consisting
and g(
·
mainly of zeros which results in simple analytical functions containing only the surviving basis func-
tions (i.e. whose corresponding weights are nonzero). To this end, we apply L1 regularization, also
known as Lasso regression [39], by adding a penalty on the L1 norm of the weights vector (i.e. the
sum of its absolute values) which leads to sparse solutions with few nonzero coefﬁcients. In terms of
our GSR method, Lasso regression automatically performs basis functions selection from the set of
basis functions that are under consideration.

Putting the pieces together, we reformulate the minimization problem in Equation (4) as a constrained
Lasso regression optimization problem deﬁned as

(4)

.

(5)

S
f (xi) =

j=1
X

w ||
s.t.
where λ > 0 is the regularization parameter, and

||

w∗ = argmin

A = [X

Y ],

−

w =

(cid:20)

αMφ β1···

w

||1

w

Aw

2
2 +λ
||
||
||2 = 1
α
β

= [α1···

(cid:21)

3

(6)

(7)

βMψ ]T .

3.3 Solving the GSR problem

To solve the GSR problem, we ﬁrst present our approach for solving the constrained Lasso prob-
lem in Equation (6), assuming some particular sets of basis functions are given. We then outline
our genetic programming (GP) procedure for ﬁnding the appropriate (or optimal) sets of these basis
functions, before discussing our matrix-based encoding scheme (to represent the basis functions) that
we will use in our GP algorithm.

3.3.1 Solving the Lasso optimization problem given particular sets of basis functions

{

=

ψj(yi)
}

φj(xi)
}

N
i=1, we are also given the sets of basis
We assume for now that, in addition to the dataset
Mψ
Mφ
j=1 used with the input feature vector xi and its corresponding
functions
j=1 and
target variable yi, respectively, for all i. In other words, we assume for now that the matrix A in
Equation (7) is formed based on particular sets of basis functions
,
ψj(yi)
}
and we are mainly interested in solving the constrained optimization problem in Equation (6).

(cid:1)
Applying the alternating direction method of multipliers (ADMM) [5], the optimization problem in
Equation (6) can be written as

φj(xi)
}

Mφ
j=1 and

Mψ
j=1

i.e.

xi,yi}

D

{

{

{

{

(cid:0)

w∗ = argmin

w ||
s.t.

w

Aw

2
2 +λ
||
||
||2 = 1
z = 0
−

||
w

z

||1

where λ > 0 is the regularization parameter.
The scaled form of ADMM (see [5] for details) for this problem is

wk ←
zk ←
uk ←

where u is the scaled dual vector, and

argmin
w

||2=1Lρ(w,zk

1,uk

1)

−

−

||
Sλ/ρ
uk

−

wk +uk
1 +wk −
(cid:0)

−
zk

1

(cid:1)

Lρ(w,z,u) =

||

Aw

2
2 +

||

ρ
2

z+u

w

−

2
2

where ρ > 0 is the penalty parameter and the soft thresholding operator S is deﬁned as

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
κ a > κ
a

a
0
a+κ a <

−

|

κ
κ

| ≤
−

Sκ(a) =




w

Lρ(w,zk
1,uk
0 =
∇
=2AT Awk +ρ(wk −

−

−
zk
(cid:12)
(cid:12)
−

1)

w=wk
1 +uk

1)

−

wk =
wk = wk/
(cid:0)

2AT A+ρI
wk||2

||

1

−

ρ(zk

1 −

−

uk

1)

−

·

To ﬁnd the minimizer wk in the ﬁrst step of the ADMM algorithm above (in Equation (9)), we ﬁrst
1) with respect to w, set it to zero, and then
1,uk
compute the gradient of the function
normalize the resulting vector solution:


Lρ(w,zk

−

−

It follows that

(cid:1)
Algorithm 1 outlines the overall process for solving the constrained Lasso optimization problem
in Equation (6), for a given matrix A, regularization parameter λ, penalty parameter ρ, and initial
guesses w0, z0, u0.

3.3.2 Finding the appropriate sets of basis functions using genetic programming

Now that we have presented Algorithm 1 that solves the constrained Lasso optimization problem in
Equation (6) for particular sets of basis functions, we go through our procedure for ﬁnding the optimal
sets of basis functions

and hence, the optimal analytical functions f (
·

) and g(
·

)

.

(cid:0)

(cid:1)

4

(8)

(9)

(10)

(11)

(12)

(13)

Algorithm 1: Solving the constrained Lasso optimization problem using ADMM
Input: A, λ, ρ, w0, z0, u0
Output: w
function SOLVEADMM(A, λ, ρ, w0, z0, u0)

w0,z

z0,u

u0;

←

←

1

−

ρ(z

·

−

u);

←

Initialization: w
while Not Converge do
2AT A+ρI
w
w/
||2;
(cid:0)
||
w+u
Sλ/ρ
z;
u+w
(cid:0)

←
←
←
←
end
end function

w
w
z
u

−

(cid:1)

(cid:1)
;

,

Encoding Scheme
Most of the SR methods rely on expression trees in their implementation. That is, each mathematical
expression is represented by a tree where nodes (including the root) encode arithmetic operations (e.g.
+,
) or mathematical functions (e.g. cos, sin, exp, ln), and leaves contain the independent
variables (i.e. x1,...,xd) or constants. However, coding these tree-based methods is often not very
straightforward since they deal with graph implementations. Inspired by [25, 7], we use matrices
to represent the basis functions. However, we propose our own encoding scheme that we believe is
general enough to handle/recover a wide range of expressions.

÷

×

−

,

...

···
...

We introduce the basis matrices Bφ and Bψ to represent the basis functions φ(
) used with
·
the feature vector x and the target variable y, respectively. The basis matrices Bφ and Bψ are of sizes
nBφ

1 respectively (i.e. Bψ is a column vector), and take the form

mBφ and nBψ

) and ψ(
·

×

×

bφ
1,1
...

Bφ = 

bφ
1,mBφ

...



,

Bψ = 

bψ
1,1
...



,

(14)

bψ
nBψ ,1

bφ
nBφ ,1

•









bφ

nBφ ,mBφ


i,1 are all integers. The ﬁrst column bφ
i,j and bψ




where the entries bφ
,1 of Bφ and the ﬁrst (and only)
column bψ
,1 of Bψ indicate the mathematical function (or transformation) to be applied (on the input
•
feature vector x and the target variable y, respectively). In Bφ, the second column bφ
,2 speciﬁes the
•
,bφ
type of argument (see Table 2), and the remaining nv = mBφ
,mBφ indicate
•
which independent variables (or features) are involved (i.e. the active operands). The quantity nv
represents the maximum total multiplicity of all the independent variables included in the argument.
Note that nBφ and nBψ specify the number of transformations to be multiplied together (i.e. each
basis function φ(
·

) will be a product of nBφ and nBψ transformations, respectively).

2 columns bφ
•

) and ψ(
·

The encoding/decoding process happens according to a table of mapping rules that is very straightfor-
ward to understand and employ. For instance, consider the mapping rules outlined in Table 2, where d
is the dimension of the input feature vector. As we will see in our numerical experiments in Section 4,
we will adopt this table for many SR benchmark problems. Other mapping tables are deﬁned accord-
ing to different benchmark problems1 (more details about the SR benchmark problem speciﬁcations
can be found in Appendix C). The encoding from the analytical form of a basis function to the basis ma-
trix is straightforward. For example, for d = 3, nBφ = 4 and mBφ = 5 (i.e. nv = 3), the basis function
φ(x) = x2cos(x2
1x2)ln(x1 +x3) can be generated according to the encoding steps shown in Table 3.
Based on the mapping rules in Table 2 and the encoding steps in Table 3, the basis function
φ(x) = x2cos(x2

1x2)ln(x1 +x3) can be described by a 4

5 matrix as follows:

,3,

···

−

1 0
2 2
Bφ = 
5 1
0



•

×

2
•
1 1
1 3

• •

•
2

0


•


(15)

1Each SR benchmark problem uses a speciﬁc set (or library) of allowable mathematical functions (e.g. cos,

sin, exp, log), and hence, we mainly modify the ﬁrst two rows of the mapping tables.

5

Table 2: Example table of mapping rules for a basis
function. The identity operator is denoted by

1.
•

Table 3: Encoding steps corresponding to the basis
function φ(x) = x2cos(x2

1x2)ln(x1 +x3).

0
1

2

1
5
3
1 cos sin exp ln

4

b•,1
Transformation (T )

b•,2
Argument Type (arg)

,b•,mB

b•,3,
···
Variable (v)

•
2
1
0
x P Q
2
1
0

3
skip x1 x2 x3

d
xd

···
···

Step T arg v1 v2

v3 Update

1

x

x2 — — T1(x) = x2

1
2
3
4
Final Update: φ(x) = T1(x)

•
cos Q x1 x1 x2 T2(x) = cos(x2
1x2)
ln P x1 x3 skip T3(x) = ln(x1 +x3)
1 — — — — T4(x) = 1
T3(x)

T4(x)

T2(x)

·

·

·

•

Remark 3.1. In Table 3, — denotes entries that are ignored during the construction of the basis func-
tion. The argument type in Step 1 is x which implies that we only select the ﬁrst variable (encoded
by b
,3) out of the nv variables as an argument, and hence the entries corresponding to v2 and v3 are
ignored. Similarly, the transformation in Step 4 is T = 1 which implies that the argument type and the
nv variables are all ignored. These are the only two cases where some entries are ignored during the
construction process. The same ignored entries are reﬂected in the matrix Bφ using
. More encoding
examples can be found in Appendix B.

•

,3 ∈
•
P

Remark 3.2. The term ‘skip’ can be thought of as 0 or 1 when the argument type is summation
or multiplication

respectively. To account for the case where the argument type is x, we let b

Q

1,...,d
}

(i.e. we exclude 0) as b

,3 is the only entry considered in this case (see Remark 3.1).
•

{
Remark 3.3. The same basis function can be represented by several matrices for three reasons:
i) Each basis function is a product of transformations where each transformation is represented by
a row in the basis matrix. Hence, a new basis matrix for the same basis function is formed by
simply swapping rows.
ii) When the argument type is
, the order of the nv variables (including ‘skip’) starting from
the third column of the matrix Bφ does not affect the expression. Hence a new basis matrix for the
P
same basis function is formed by simply swapping these columns.
iii) As mentioned in Remark 3.1, some entries are ignored in some cases. Hence a new basis matrix
for the same basis function is formed by simply modifying these entries.
Remark 3.4. In the example above,we showed how we can produce the matrix Bφ to represent a basis
function φ(x). A similar (and even simpler) procedure can be applied to produce the matrix Bψ that
represents a basis function ψ(y); we only need a mapping table corresponding to the set of allowable
transformations (e.g. the ﬁrst two rows of Table 2).

Q

or

Note that the decoding from the basis matrix to the expression of a basis function is trivial; we go
through the rows of the basis matrix and convert them into transformations according to a mapping
table (e.g. Table 2), before ﬁnally multiplying them together. Also note that the search space of basis
functions
is huge in general which makes enumeration impractical, and hence, we will
rely on GP for effective search process.

mainly φ(
·

)

(cid:0)

(cid:1)

Genetic Programming (Evolutionary Algorithm)
The SR problem has been extensively studied in the literature, and a wide variety of methods has
been suggested over the years to tackle it. Most of these methods are based on genetic programming
(GP) [3, 18, 38, 49]. This is a heuristic search technique that tries to ﬁnd the optimal mathematical
expression (in the SR context) among all possible expressions within the search space. The optimal
(or best) expression is found by minimizing some objective function, known as the ﬁtness function.

GP is an evolutionary algorithm that solves the SR problem. It starts with an initial population (or ﬁrst
generation) of Np randomly generated individuals (i.e. mathematical expressions), then recursively
applies the selection, reproduction (crossover), and mutation operations until termination. During
the selection operation, the ﬁtness of each of the Np individuals of the current generation is evaluated
(according to the ﬁtness function), and the np ﬁttest (or best) individuals are selected for reproduction
and mutation (the selected individuals are part of the new generation and can be thought of as parents).
The reproduction (crossover) operation generates new individuals (offsprings) by combining random
parts of two parent individuals. The mutation operation produces a new individual by changing a ran-
dom part of some parent individual. Finally, the recursion terminates, when some individual reaches
a predeﬁned ﬁtness level (i.e. until some stopping criterion is satisﬁed).

In our GSR approach, we use a slightly modiﬁed version of the GP algorithm described above. Each
individual in the population initially consists of two sets of Mφ and Mψ randomly generated basis

6

) and g(
·

) to be used
functions encoded by basis matrices. Such matrices will form the functions f (
·
with the input feature vector x and the target variable y, respectively. This is different from the GP
algorithm described above where individuals typically represent the full mathematical expression or
function as a whole. In addition, the Np individuals in the population of a new generation consist
np individuals generated
of the np ﬁttest individuals of the current generation in addition to Np −
as follows. With probability 1
4 , a new individual is generated (reproduced) by randomly combining
basis functions (i.e. basis matrices) from two parent individuals (i.e. crossover) selected from the np
surviving individuals. With probability 1
4 , a new individual is generated by randomly choosing one of
the np surviving individuals, and replacing (mutating) some of its basis functions (i.e. basis matrices)
with completely new ones (i.e. randomly generated). With probability 1
2 , a completely new individual
is randomly generated (in the same way we generate the individuals of the initial population). Ran-
domly generating individuals enhances diversity in the basis functions and avoids reaching a plateau.
Indeed, this is just one of many ways that can be followed to apply some sort of crossover/mutation
on individuals deﬁned by their sets of basis functions instead of their full mathematical expression. A
pseudocode of our proposed GSR algorithm is provided in Appendix A.

4 Experimental Results

We evaluate our proposed GSR method through a series of numerical experiments on a number of
common SR benchmark datasets. In particular, we compare our approach to existing state-of-the-art
methods using three popular SR benchmark problem sets: Nguyen [46], Jin [15], and Neat [43]. In
addition, we demonstrate the beneﬁts of our proposed method on the recently introduced SR bench-
mark dataset called Livermore [30], which covers problems with a wider range of difﬁculty compared
to the other benchmarks. Finally, we introduce a new and more challenging set of SR benchmark
problems, which we call SymSet, mainly for two reasons: i) Our GSR algorithm achieves perfect
scores on Nguyen, and almost perfect scores on Jin, Neat, and Livermore, and hence we introduced a
benchmark problem set that is more challenging, ii) The existing SR benchmark problem sets do not
really reﬂect the strengths of our proposed method, and thus we designed SymSet to explicitly high-
light the beneﬁts we gain from using our proposed approach. Each SR benchmark problem consists of
a ground truth expression, a training and test dataset, and a set (or libary) of allowable arithmetic op-
erations and mathematical functions. Speciﬁcations of all the SR benchmark problems are described
in Appendix C. Hyperparameters and additional experiment details are provided in Appendix A.

Across our experiments, we mainly compare our proposed approach against four strong SR methods:
Neural-guided genetic programming population seeding (NGGPPS): A hybrid approach of
neural-guided search and GP, which uses a recurrent neural network (RNN) to seed the starting popu-
lation for GP [30]. NGGPPS achieves state-of-the-art results on the well-known SR benchmarks.
Deep Symbolic Regression (DSR): A reinforcement learning method that proposes a risk-seeking
policy gradient to train an RNN to produce better-ﬁtting expressions [35]. DSR is the “RNN only"
version of NGGPPS, and is also considered a top performer on the common SR benchmarks.
Bayesian Symbolic Regression (BSR): A Bayesian framework which carefully designs prior dis-
tributions to incorporate domain knowledge (e.g. preference of basis functions or tree structure),
and which employs efﬁcient Markov Chain Monte Carlo (MCMC) methods to sample symbolic
trees from the posterior distributions [15].
Neat-GP: a GP approach which uses the NeuroEvolution of Augmenting Topologies (NEAT) algo-
rithm that greatly reduces the effects of bloat (i.e. controls the growth in program size) [43].

We ﬁrst compare GSR against NGGPPS, DSR, as well as Eureqa (a popular GP-based commercial
software proposed in [38]) on the Nguyen benchmarks. We follow their experimental procedure and
report the results in Table 4. We use recovery rate as our performance metric, deﬁned as the fraction
of independent training runs in which an algorithm’s resulting expression achieves exact symbolic
equivalence compared to the ground truth expression (as veriﬁed using a computer algebra system
such as SymPy [29]). Table 4 shows that GSR signiﬁcantly outperforms DSR and Eureqa in exactly
recovering the Nguyen benchmark expressions. As NGGPPS achieves nearly perfect scores on the
Nguyen benchmarks, GSR shows only a slight improvement (on Nguyen-7) compared to NGGPPS.
However, GSR exhibits faster runtime than NGGPPS; by running each benchmark problem on a single
core, GSR takes an average of 2.5 minutes per run on the Nguyen benchmarks compared to 3.2 minutes
for NGGPPS. Runtimes on individual Nguyen benchmark problems are shown in Appendix Table 11.

We next evaluate GSR on the Jin and Neat benchmark sets. The results are reported in Tables 5 and 6
respectively. A RMSE value of 0 indicates exact symbolic equivalence. From Table 5, we can clearly

7

Table 4: Recovery rate comparison of GSR against several algorithms on the Nguyen benchmark set
over 100 independent runs. The formulas for these benchmarks are shown in Appendix Table 19.

Benchmark

Expression

GSR NGGPPS

DSR

Eureqa

Recovery Rate (%)

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11

y = x3+x2 +x
y = x4+x3 +x2 +x
y = x5+x4 +x3 +x2 +x
y = x6+x5 +x4 +x3 +x2 +x
y = sin(x2)cos(x)
y = sin(x)+sin(x+x2)
y = ln(x+1)+ln(x2 +1)
y = √x
y = sin(x1)+sin(x2
2)
y = 2sin(x1)cos(x2)
y = xx2
1

−

1

Average

100
100
100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
97
100
100
100
100

100
100
100
100
72
100
35
96
100
100
100

100
100
95
70
73
100
85
0
100
64
100

99.73

91.18

80.64

observe that GSR outperforms DSR and BSR and performs nearly as good as NGGPPS recov-
ering all the Jin problems (accross all independent runs) except Jin-6. Table 6 shows that GSR
outperforms all other methods (NGGPPS, DSR, and Neat-GP) on the Neat benchmarks. Note
that expressions containing divisions (i.e. Neat-6, Neat-8, and Neat-9) are not exactly recovered
by GSR (i.e. only approximations are recovered) since the division operator is not included in
our scheme (see Appendix E for details).

We then run experiments on the Livermore benchmark set which contains problems with a large range
of difﬁculty. In addition to NGGPPS and DSR, we compare against NGGPPS using the soft length
prior (SLP) and hierarchical entropy regularizer (HER) recently introduced in [20]. We also compare
against a recently proposed method, known by genetic expert-guided learning (GEGL) [1], which
trains a molecule-generating deep neural network (DNN) guided with genetic exploration. Table 7
shows that our GSR method outperforms all other methods on both the Nguyen and Livermore bench-
mark sets, beating NGGPPS+SLP/HER which was the top performer on these two benchmark sets.

We highlight the strengths of GSR on the new SymSet benchmark problem set, and show the beneﬁts
of searching for expressions of the form g(y) = f (x) instead of y = f (x). Typical expressions, with
exact symbolic equivalence, recovered by GSR are shown in Appendix Table 26. The key feature
of GSR lies in its ability to recover expressions of the form g(y) = f (x). To better highlight the
beneﬁts offered by this feature, we disable it by constraining the search space in GSR to expressions
of the form y = f (x) (which is the most critical ablation). We refer to this special version of GSR
as s-GSR. Note that most of the SymSet expressions cannot be exactly recovered by s-GSR (i.e. they
can only be approximated). We compare the performance of GSR against s-GSR on the SymSet
benchmarks in terms of accuracy and runtime (see Table 8). The results clearly show that GSR is

Table 5: Comparison of mean root-mean-
square error (RMSE) for GSR against
several methods on the Jin benchmark
problem set over 50 independent runs.
The formulas for these benchmarks are
shown in Appendix Table 19.

Benchmark GSR NGGPPS DSR BSR

Mean RMSE

Jin-1
Jin-2
Jin-3
Jin-4
Jin-5
Jin-6

0
0
0
0
0
0.018

Average

0.0030

0
0
0
0
0
0

0

0.46
0

2.04
6.84
0.00052 0.21
0.00014 0.16
0.66
4.63

0
2.23

Table 6: Comparison of median RMSE for GSR
against several methods on the Neat benchmark prob-
lem set over 30 independent runs. The formulas for
these benchmarks are shown in Appendix Table 19.

Benchmark

GSR

Neat-1
Neat-2
Neat-3
Neat-4
Neat-5
Neat-6
Neat-7
Neat-8
Neat-9

0
0
0
0
0
2.0×10−4
0.0521
4.0×10−4
8.1×10−9

Median RMSE
NGGPPS

DSR Neat-GP

0
0
0
0
0

0
0
0.0041
0.0189
0

6.1×10−6 0.2378
1.0606
1.0028
0.1076
0.0228
0.1511
0

0.0779
0.0576
0.0065
0.0253
0.0023
0.2855
1.0541
0.1498
0.1202

0.45

2.42

Average

0.0059

0.1139

0.1756

0.1977

8

Table 7: Recovery rate comparison of GSR
against several algorithms on the Nguyen and
Livermore benchmark sets over 25 independent
runs. Recovery rates on individual benchmark
problems are shown in Appendix Table 10.

Recovery Rate (cid:0)%(cid:1)
All Nguyen Livermore

GSR
NGGPPS+SLP/HER 82.59
78.59
NGGPPS
66.82
GEGL
49.18
DSR

90.59 100.00
92.00
92.33
86.00
83.58

85.45
77.45
71.09
56.36
30.41

Table 8: Average performance in mean RMSE
and runtime (on a single core), along with their
standard errors, for GSR and its most critical
ablation (denoted by s-GSR) on the SymSet
benchmark problem sets over 25 independent
runs. Mean RMSE and runtime values on
individual benchmark problems are shown
in Appendix Table 12.

SymSet Average

Mean RMSE

Runtime (sec)
GSR 2.66×10−4±1.59×10−4 120.84±4.22
136.19±4.56

s-GSR 2.56×10−2 ±5.27×10−3

faster than s-GSR, averaging around 2 minutes per run on the SymSet benchmarks compared to 2.27
minutes for s-GSR (i.e.
11% runtime improvement). In addition, GSR is more accurate than
s-GSR by two orders of magnitude. This is due to the fact that GSR exactly recovers the SymSet
expressions across most of the runs, while s-GSR only recovers approximations for most of these
expressions. This reﬂects the superiority of GSR over s-GSR, which demonstrates the beneﬁts of
learning expressions of the form g(y) = f (x) in SR tasks.

∼

5 Discussion

Making predictions. Given a new input feature vector x
of solving the equation g(y) = f (x
∗
∗
known quantity and y is the only unknown. If g(
·
using y
f (x
(cid:1)
∗
not, we might end up with many solutions for y
lead to more than one solution). In this case, we choose y
of y which can be determined from the training dataset.

) is not invertible, then y

. If g(
·

f (x
∗

= g−

)

∗

∗

1

∗

) for y, or equivalently, g(y)

, predicting y

f (x
∗
∗

with GSR is simply a matter
) = 0. Note that f (x
) is a
∗
can be easily found

−

) is an invertible function, then y

∗

∗

will be the root of the function h(y) = g(y)

−
) is invertible or
(an invertible function, which is not one-to-one, can
to be the solution that belongs to the range

). Root-ﬁnding algorithms include Newton’s method. Whether the function g(
·

(cid:0)

Limitations. GSR, including state-of-the-art methods, have difﬁculty with expressions containing
divisions. For GSR, this is due to the way we deﬁne our encoding scheme. Other methods fail even
though the division is included in their framework. GSR can overcome this issue by modifying its
encoding scheme to include divisions within the basis functions (at the expense of signiﬁcantly in-
creasing the complexity of the search space). Another limiting factor to GSR is that it cannot recover
expressions containing composition of functions, such as y = ecos(x) + ln(x). Another challenging
task for GSR is to reach, although expressible, expressions containing multiple complex basis func-
tions simultaneously. This can be due to the choice of the hyperparameters or the GP search process.
A more elaborate discussion about the limitations of GSR can be found in Appendix E. These limita-
tions will be addressed in a future paper. Indeed, there are plenty of expressions that still cannot be
fully recovered by GSR. This is the case for all other SR methods as well.

6 Conclusion

We introduce GSR, a Generalized Symbolic Regression approach by modifying the formulation of
the conventional SR optimization problem.
In GSR, we identify mathematical relationships be-
tween the input features and some transformation of the target variable. That is, we infer the map-
ping from the feature space to a transformed target space, by searching for expressions of the form
g(y) = f (x) instead of y = f (x). We conﬁne our search space to a weighted sum of basis func-
tions and use genetic programming with a matrix-based encoding scheme to extract their expressions.
We perform several numerical experiments on well-known SR benchmark datasets and show that
our GSR approach outperforms many state-of-the-art methods. We further highlight the strengths
of GSR by introducing SymSet, a new SR benchmark set which is more challenging relative to
the existing benchmarks. We believe that GSR’s concept of ﬁtting g(y) = f (x) can be applied to
other SR methods and can boost their performance.

9

References

[1] Sungsoo Ahn, Junsu Kim, Hankook Lee, and Jinwoo Shin. Guiding deep molecular optimization
with genetic exploration. Advances in neural information processing systems, 33:12008–12021,
2020.

[2] Ali R Al-Roomi and Mohamed E El-Hawary. Universal functions originator. Applied Soft

Computing, 94:106417, 2020.

[3] Thomas Bäck, David B Fogel, and Zbigniew Michalewicz. Evolutionary computation 1: Basic

algorithms and operators. CRC press, 2018.

[4] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Paras-
In International Conference on Machine

candolo. Neural symbolic regression that scales.
Learning, 2021.

[5] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via

the alternating direction method of multipliers. Now Publishers Inc, 2011.

[6] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations
from data by sparse identiﬁcation of nonlinear dynamical systems. Proceedings of the national
academy of sciences, 113(15):3932–3937, 2016.

[7] Chen Chen, Changtong Luo, and Zonglin Jiang. Elite bases regression: A real-time algorithm
for symbolic regression. In 2017 13th International conference on natural computation, fuzzy
systems and knowledge discovery (ICNC-FSKD), pages 529–535. IEEE, 2017.

[8] Chen Chen, Changtong Luo, and Zonglin Jiang. Fast modeling methods for complex system
with separable features. In 2017 10th International Symposium on Computational Intelligence
and Design (ISCID), volume 1, pages 201–204. IEEE, 2017.

[9] Chen Chen, Changtong Luo, and Zonglin Jiang. Block building programming for symbolic

regression. Neurocomputing, 275:1973–1980, 2018.

[10] Chen Chen, Changtong Luo, and Zonglin Jiang. A multilevel block building algorithm for fast
modeling generalized separable systems. Expert Systems with Applications, 109:25–34, 2018.

[11] Miles Cranmer, Alvaro Sanchez-Gonzalez, Peter Battaglia, Rui Xu, Kyle Cranmer, David
Spergel, and Shirley Ho. Discovering symbolic models from deep learning with inductive biases.
arXiv preprint arXiv:2006.11287, 2020.

[12] Fabrício Olivetti de França. A greedy search tree heuristic for symbolic regression. Information

Sciences, 442:18–32, 2018.

[13] Fabricio Olivetti de Franca and Guilherme Seidyo Imai Aldeia. Interaction-transformation evo-
lutionary algorithm for symbolic regression. Evolutionary Computation, pages 1–25, 2020.

[14] Anthony William Fairbank Edwards. Likelihood. CUP Archive, 1984.

[15] Ying Jin, Weilin Fu, Jian Kang, Jiadong Guo, and Jian Guo. Bayesian symbolic regression.

arXiv preprint arXiv:1910.08892, 2019.

[16] Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for
parallel implicit sparse identiﬁcation of nonlinear dynamics. Proceedings of the Royal Society
A, 476(2242):20200279, 2020.

[17] Samuel Kim, Peter Y Lu, Srijon Mukherjee, Michael Gilbert, Li Jing, Vladimir ˇCeperi´c, and
Marin Soljaˇci´c. Integration of neural network-based symbolic regression in deep learning for
scientiﬁc discovery. IEEE Transactions on Neural Networks and Learning Systems, 2020.

[18] John R Koza and John R Koza. Genetic programming: on the programming of computers by

means of natural selection, volume 1. MIT press, 1992.

[19] William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabrício Olivetti de França, Marco
Virgolin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary symbolic regression
methods and their relative performance. arXiv preprint arXiv:2107.14351, 2021.

10

[20] Mikel Landajuela Larma, Brenden K Petersen, Soo K Kim, Claudio P Santiago, Ruben Glatt,
T Nathan Mundhenk, Jacob F Pettit, and Daniel M Faissol. Improving exploration in policy
gradient search: Application to symbolic optimization. arXiv preprint arXiv:2107.09158, 2021.

[21] Peter M Lee. Bayesian statistics. Arnold Publication, 1997.

[22] Thomas Leonard and John SJ Hsu. Bayesian methods: an analysis for statisticians and interdis-

ciplinary researchers, volume 5. Cambridge University Press, 2001.

[23] Qiang Lu, Jun Ren, and Zhiguang Wang. Using genetic programming with prior formula knowl-
edge to solve symbolic regression problem. Computational intelligence and neuroscience,2016,
2016.

[24] Changtong Luo, Chen Chen, and Zonglin Jiang. A divide and conquer method for symbolic

regression. arXiv preprint arXiv:1705.08061, 2017.

[25] Changtong Luo and Shao-Liang Zhang. Parse-matrix evolution for symbolic regression. Engi-

neering Applications of Artiﬁcial Intelligence, 25(6):1182–1193, 2012.

[26] Niall M Mangan, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Inferring biologi-
cal networks by sparse identiﬁcation of nonlinear dynamics. IEEE Transactions on Molecular,
Biological and Multi-Scale Communications, 2(1):52–63, 2016.

[27] Georg Martius and Christoph H Lampert. Extrapolation and learning equations. arXiv preprint

arXiv:1610.02995, 2016.

[28] Trent McConaghy. Ffx: Fast, scalable, deterministic symbolic regression technology. In Genetic

Programming Theory and Practice IX, pages 235–260. Springer, 2011.

[29] Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondˇrej ˇCertík, Sergey B Kirpichev,
Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy:
symbolic computing in python. PeerJ Computer Science, 3:e103, 2017.

[30] T Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P Santiago, Daniel M Faissol, and
Brenden K Petersen. Symbolic regression via neural-guided genetic programming population
seeding. arXiv preprint arXiv:2111.00053, 2021.

[31] John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the

Royal Statistical Society: Series A (General), 135(3):370–384, 1972.

[32] Michael O’Neill and Conor Ryan. Grammatical evolution. IEEE Transactions on Evolutionary

Computation, 5(4):349–358, 2001.

[33] Patryk Orzechowski, William La Cava, and Jason H Moore. Where are we now? a large bench-
mark study of recent symbolic regression methods. In Proceedings of the Genetic and Evolu-
tionary Computation Conference, pages 1183–1190, 2018.

[34] Yudi Pawitan. In all likelihood: statistical modelling and inference using likelihood. Oxford

University Press, 2001.

[35] Brenden K Petersen, Mikel Landajuela Larma, Terrell N. Mundhenk, Claudio Prata Santiago,
Soo Kyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathemat-
ical expressions from data via risk-seeking policy gradients. In International Conference on
Learning Representations, 2021.

[36] Markus Quade, Markus Abel, Kamran Shaﬁ, Robert K Niven, and Bernd R Noack. Prediction
of dynamical systems by symbolic regression. Physical Review E, 94(1):012214, 2016.

[37] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation
and control. In International Conference on Machine Learning,pages 4442–4450. PMLR, 2018.

[38] Michael Schmidt and Hod Lipson. Distilling free-form natural laws from experimental data.

science, 324(5923):81–85, 2009.

[39] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statis-

tical Society: Series B (Methodological), 58(1):267–288, 1996.

11

[40] Tony Tohme. The Bayesian validation metric: a framework for probabilistic model calibration

and validation. PhD thesis, Massachusetts Institute of Technology, 2020.

[41] Tony Tohme, Kevin Vanslette, and Kamal Youcef-Toumi. A generalized bayesian approach to

model calibration. Reliability Engineering & System Safety, 204:107141, 2020.

[42] Tony Tohme, Kevin Vanslette, and Kamal Youcef-Toumi. Improving regression uncertainty

estimation under statistical change. arXiv preprint arXiv:2109.08213, 2021.

[43] Leonardo Trujillo, Luis Muñoz, Edgar Galván-López, and Sara Silva. neat genetic programming:

Controlling bloat naturally. Information Sciences, 333:21–43, 2016.

[44] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max Tegmark.
Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity. arXiv preprint
arXiv:2006.10782, 2020.

[45] Silviu-Marian Udrescu and Max Tegmark. Ai feynman: A physics-inspired method for symbolic

regression. Science Advances, 6(16):eaay2631, 2020.

[46] Nguyen Quang Uy, Nguyen Xuan Hoai, Michael O’Neill, Robert I McKay, and Edgar Galván-
López. Semantically-based crossover in genetic programming: application to real-valued sym-
bolic regression. Genetic Programming and Evolvable Machines, 12(2):91–119, 2011.

[47] Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. Symbolicgpt: A generative

transformer model for symbolic regression. arXiv preprint arXiv:2106.14131, 2021.

[48] Kevin Vanslette, Tony Tohme, and Kamal Youcef-Toumi. A general model validation and testing

tool. Reliability Engineering & System Safety, 195:106684, 2020.

[49] Marco Virgolin, Tanja Alderliesten, and Peter AN Bosman. Linear scaling with and within
semantic backpropagation-based genetic programming for symbolic regression. In Proceedings
of the genetic and evolutionary computation conference, pages 1084–1092, 2019.

[50] CJ Wild and GAF Seber. Nonlinear regression. New York: Wiley, 1989.

12

A Implementation, Hyperparameters, and Additional Experiment Details

∈

) and ψ(
·

) and ψ(
(cid:0)
·

namely φ(
·
) using basis matrices Bφ and Bψ of sizes nBφ

R,
Implementation. Our GSR method discovers expressions of the form g(y) = f (x) where y
Rd, and where the search space for f (
x
) is constrained to a weighted sum of Mφ and
) and g(
·
·
, respectively. We use a matrix-based encoding scheme to
Mψ basis functions
)
represent φ(
1, respec-
·
tively (where mBφ = nv + 2 and nv is deﬁned in Section 3.3.2). Hence, in addition to the number of
basis functions Mφ and Mψ, the parameters nBφ, nv, and mBφ affect the complexity of the evolved
expressions, and hence can be controlled to conﬁne the search space (although d also affects the com-
plexity of the expression, it is given by the problem and cannot be controlled). Although more than one
basis matrix can lead to the same basis function (see Remark 3.3), the search space of basis functions
is still huge in general, and thus, enumerating all the possible basis functions is not prac-
mainly φ(
·
tical. Hence, we will rely on genetic programming (GP) for effective search process. A pseudocode
(cid:0)
of our GP-based GSR algorithm is outlined in Algorithm 2.

mBφ and nBψ

×

×

∈

(cid:1)

(cid:1)

)

Algorithm 2: GP Procedure for GSR

Input: Np, np, Mφ, Mψ,
Output:
function SOLVEGSR(Np, np, Mφ, Mψ,

x,

L

L

I

∗

y

x,

y)

Initialize population:
1(k),

// I(k)
0;

← {I
k
for i = 1 to Np do
i(k)

←

I

2(k),...,

L

L
Np (k)
I
// Initialize the generation (or iteration) counter

}

I
/* Each individual

GENERATERANDOMINDIVIDUAL(Mφ,Mψ,

x,
L
i(k) contains two randomly generated sets of Mφ and

y);

←

L

I
Mψ basis matrices respectively

end
Evaluate each individual
/* For each individual

I

i(k) with respect to the ﬁtness function;

i(k), form the matrix Ai(k), solve for the optimal

coefficients vector wi(k)

SolveADMM(Ai(k),

I

I(k)
while Stopping Criterion not Satisﬁed do

SORTED(I(k));

←

←

), then compute its fitness */
···
// in ascending order of fitness

←

k+1;

k
UPDATECRITERION();
/* Start with a strict stopping criterion (e.g. a very low error

// Increment the generation (or iteration) counter

threshold) and slowly relax it (e.g. gradually increase the error
threshold)

s
x,

s
y ←

CHOOSESUBLIBRARY(

L
/* Choose sublibraries

x,
y);
L
L
x and
used with x and y respectively

L
⊆ L

L

s
x

s
y ⊆ L

L

y of allowable operations to be

I [1:np](k)
/* The np fittest individuals of the previous generation are copied to

I[1:np](k

1);

←

−

the

current new one

for i = np +1 to Np do

GENERATERANDOMINTEGER(1,4);

u
if u = 1 then

←

i(k)

REPRODUCE(I [1:np](k),

s
x,

s
y);

I
/* Crossover based on the surviving individuals

←

L

L

else if u = 2 then
i(k)

MUTATE(I [1:np](k),

s
x,

s
y);

I
/* Mutation based on the surviving individuals

←

L

L

i(k)

s
GENERATERANDOMINDIVIDUAL(Mφ,Mψ,
y);
I
/* Randomly generate a completely new individual

←

s
x,

L

L

else

end

end

end
Evaluate each individual
SORTED(I(k));
I(k)
∗
1(k);

←
← I

I

end function

I

i(k) with respect to the ﬁtness function;

// in ascending order of fitness
// return the fittest individual

13

*/

*/

*/

*/

*/

*/

*/

,

,

,

{

{

L

L

÷

×

1,

x =

x and

x, and

Ly =

1,exp,ln
}
•

,cos,sin,exp,ln
}

−
(resulting in the mapping Table 2) and

+,
×}
{
−
+,
L0 =
{
1,cos,sin,exp,ln
}
•

The main inputs to our GP-based algorithm are Np, np, Mφ, Mψ,
Ly. Recall that Np is
the population size and np is the number of surviving individuals per generation.
Ly are the
libraries of allowable transformations that can be used with x and y, respectively. These libraries form
the ﬁrst two rows of mapping tables, e.g. Table 13, and are deﬁned by the benchmark problem. Note
that the division operator is not part of our GSR architecture. That is, the main arithmetic operations
. For example, for the Nguyen benchmark dataset, the library of allow-
used by GSR are
as shown in Table 19. In this case, we deﬁne
able operations is
. Regarding
1,
L
the stopping criterion, common terminating conditions for GP include: i) a solution reaches minimum
criterion (e.g. error threshold), ii) the algorithm reaches a ﬁxed number of generations (or iterations),
iii) the algorithm generates a ﬁxed number of individuals (or candidates expressions), iv) the algorithm
reaches a plateau such that new generations no longer improve results, v) combinations of the above
conditions. In our case, the algorithm terminates when the solution hits a minimum root-mean-square
error (RMSE) threshold. To accelerate termination, we slowly relax the error threshold by gradually in-
creasing it. To avoid reaching a plateau and since we are dealing with a small population size as shown
in Table 9, we enhance diversity (in the basis functions) by producing completely new individuals
with probability 1/2 per generation (while performing crossover and mutation with probability 1/4
s
each per generation). To speed up the search process, we employ sublibraries
y ⊆ Ly
L
of allowable transformations, used when generating completely new individuals (or completely new
basis functions in the case of mutation). For x, we mainly rely on three sublibraries which are the
trig, and the original library
most common: a polynomial sublibrary
1, cos, sin
1
.
1,
L
}
}
•
•
3 would be included in these sublibraries if they were part of
Note that power operators such as
the original library deﬁned by the benchmark problem. The function CHOOSESUBLIBRARY() works
1,500,
according to some cycle. For example, assuming k is the generation (or iteration) counter,if k
each cycle consists of 70 iterations broken into three stages, the ﬁrst stage consists of 15 iterations and
s
assigns
poly, and the third
x
L
trig. This cycle repeats until
and ﬁnal stage consists of the remaining 35 iterations and assigns
k = 1,500, after which the cycle’s size becomes 1,500 iterations broken into three equal stages (i.e.
500 iterations per sublibrary). For y, the cycle consists of 20 iterations, in which we equally alternate
between the polynomial sublibrary
Ly itself (i.e. 10 iterations for each
poly and the original library
sublibrary). Indeed, the use of sublibraries is only possible when the corresponding operations are
included in the original library deﬁned by the benchmark problem (e.g. Neat-6 and Neat-8 cannot use
are not included in their corresponding original libraries,
trigonometric sublibraries since
as shown in Table 19). In addition, it is up to the user to specify the cycle’s size and how to alternate
between sublibraries, or even decide whether to use sublibraries in the ﬁrst place.

x, the second stage consists of 25 iterations and assigns

x itself. For the Nguyen benchmark example above,

poly, a trigonometric sublibrary

cos,sin
}

poly =

L
and

x and

2,
•

trig =

s
x
L

s
x
L

s
x
L

← L

← L

← L

⊆ L

1,

≤

L

L

L

L

{

{

•

{

Hyperparameters. Throughout our experiments, we adopt the following hyperparameter values.
For GP, we use a population size Np = 30, and we allow for np = 10 surviving individuals per
generation. We perform crossover with probability 1
4 and allow for only 2 parents to be involved in
the process (i.e. new individuals are formed by combing basis functions from two randomly chosen
parent individuals). We apply mutation with probability 1
4 and allow for 3 basis functions (randomly
selected from an individual) to be mutated (i.e. to be discarded and replaced by completely new basis
functions). We generate a (completely new) random individual with probability 1
2 . For ADMM, we
use a regularizer λ = 0.4, a penalty ρ = 0.1. The algorithm terminates when the ℓ2-norm of the
5.
difference between the weight vectors from two consecutive iterations falls below a threshold of 10−
2 = [ 1
Regarding initial conditions, we use w0 =
" denotes a normalized vector),
√ 1
z0 = 1 = [1
0]T . For GSR, we allow for a maximum of Mφ = 15 basis
b
functions φ(
) (this is the maximum number since some of the Mφ basis
·
functions will be multiplied by 0, i.e. at most we get Mφ nonzero coefﬁcients multiplying the basis
fcuntions). To avoid overﬁtting and overly complex expressions, we allow for a maximum of Mψ = 1
) (in this case the maximum and minimum are both 1
basis function ψ(
·
) will consist of a single basis function). It is worth noting that we use Mψ = 2 for SymSet-11.
and g(
·
) will be a product of
Each basis ψ(
·
Nt transformations, where Nt is a random integer between 1 and 3, i.e. nBφ
. For each of
}
these Nt transformations, the maximum total multiplicity of all the independent variables (or features)
. GSR terminates when a
in an argument is a random integer between 2 and 5, i.e. nv ∈ {
6 (recall
candidate expression achieves a RMSE lower than a threshold with a starting value of 10−
that this threshold is slowly relaxed during the process, e.g. by progressively multiplying it by a factor
of √10 for every 1,500 iterations). All hyperparameter values are summarized in Table 9.

) for each expression of g(
·
) will consist of a single transformation nBψ = 1. Each basis φ(
·
∈ {

1]T , u0 = 0 = [0
···
) for each expression of f (
·

(where “

2,3,4,5

1,2,3

2 ···
4 +

2 ]T

+ 1
4

···

b

···

}

1

1

14

Table 9: Hyperparameter values for GSR for all experiments, unless otherwise speciﬁed.

Hyperparameter

GP Parameters

Population size
Number of survivors per generation
Crossover probability
Number of parents involved in crossover
Mutation probability
Number of bases to mutate
Randomly generated individual probability

ADMM Parameters

Regularization parameter
Penalty parameter
Tolerance on the solution error
Initial guesses

GSR Parameters

Symbol

Value

Np
np
—
—
—
—
—

30
10
1/4
2
1/4
3
1/2

λ
ρ
—
w0, z0, u0

0.4
0.1
5
10−
1
2 ,1,0

Maximum number of basis functions φ(
·
Maximum number of basis functions ψ(
·
Maximum total multiplicity of all features in an argument
Number of tranformations multiplied together per basis φ(
·
Number of tranformations multiplied together per basis ψ(
·
Tolerance on the solution error (RMSE)

) for each expression of f (
·
) for each expression of g(
·

)
)

)
)

Mφ
Mψ
nv
nBφ
nBψ
—

b

15
1
2,3,4,5
{
1,2,3
{
1
10−

6

}
}

−

Additional experiment details. For all benchmark problems, we run GSR for multiple indepen-
dent trials using different random seeds (following the experimental procedure in [30, 35]). Table
10 shows the recovery rates of GSR against literature-reported values from several algorithms on the
Nguyen and Livermore individual benchmark problems. We ﬁrst note that, due to the wide domain
of sampled input points imposed by Livermore-1 (i.e. [
10,10]), we observed some instabilities in
the solution due to the presence of the exponential function, which we decided to exclude from the
library of allowable operations during the search process for this benchmark. In what follows, we
provide explanations for the results shown in Table 10. Note that Livermore-5 is difﬁcult to recover
by NGGPPS+SLP/HER and the remaining methods as it contains subtractions. Subtraction is more
difﬁcult than addition since it is not cumulative. This is not an issue for GSR since both additions
and subtractions are equally recovered through the sign of the optimal coefﬁcients multiplying the
basis functions. Livermore-10 and Livermore-17 are more challenging than Nguyen-10 since they
require adding the same basis function many more times (which is apparent through the poor recovery
rates of the different methods). Fortunately, this is also not a problem for GSR since it can be easily
solved by ﬁnding the right coefﬁcient multiplying the basis function. Livermore-18 is more challeng-
ing than Livermore-2 and Nguyen-5 since it requires recovering the constant 5 without a constant
optimizer
. For GSR, this can be recovered by naturally
solving for the real-valued coefﬁcient. The problem of the different methods on Livermore-22 lies in
the constant 0.5, which requires ﬁnding x
x+x compared to GSR which simply solves for the optimal
parameter multiplying x2. We observe that GSR performs poorly on Livermore-9 and Livermore-
21 compared to NGGPPS+SLP/HER. This can be due to the choice of hyperparameters (e.g. Mφ,
nBφ, and nv) as well as the GP-based search process. These two benchmarks require ﬁnding the
ﬁrst 9 and 8 powers of x simultaneously, respectively, which can be difﬁcult to achieve by GSR, es-
pecially that we only consider Mφ = 15 basis functions φ(
), as mentioned
·
earlier. Note that polynomials were not an issue for GSR up to the 6th order (i.e. Nguyen-4). We
also tried experimenting with a 7th order polynomial (i.e. y = x7 + x6 + x5 + x4 + x3 + x2 + x)
and GSR achieved 100% recovery rate. We started observing a decline in the recovery rate when we
added the 8th power of x. In other words, Livermore-21 (the 8th order polynomial) seems to be the
limit that GSR can reach with polynomials while Livermore-9 (the 9th order polynomial) becomes
very difﬁcult to recover. It is worth noting that if the libraries for the Livermore-9 and Livermore-21
problems contained the square and cube operators
(as is the case for the Jin benchmarks
described in Table 19), then GSR would easily recover these two problems. Finally, GSR is not able
since both benchmarks require
to recover Livermore-7

which can be recovered as x+x+x+x+x

{•
and Livermore-8

) per expression of f (
·

•
y = cosh(x)

y = sinh(x)

2,

}

(cid:1)

(cid:0)

x

3

(cid:0)

(cid:1)

(cid:0)

(cid:1)

15

y

⇐⇒

≈

≈

≈

Livermore-8:

0.8616y =

−

y

⇐⇒

≈

≈

≈

ﬁnding the basis function e−•,2 which cannot be expressed using our current encoding scheme unless
it is available as a transformation by itself. That is, the exponential operator e• is not enough to recover
1
e• = e−• using our current encoding scheme. Had the negative exponential operator e−• been part
of the library of allowable operations deﬁned by Livermore-7 and Livermore-8, GSR would easily re-
, we added the operator
cover these two benchmarks. As we can see for SymSet-1
e−• to the library of allowable operations (see Table 20), which made GSR’s mission much simpler
and it was able to recover the corresponding ground truth expression as shown in Table 26. It is worth
mentioning that, although ground truth expressions are not expressible, GSR was naturally able to
recover the best approximations possible for Livermore-7 and Livermore-8, which turned out to be
their Taylor expansions around 0. GSR’s typical output expressions were as follows:

y = xsinh(x)

−

4
5

(cid:1)

(cid:0)

Livermore-7:

0.51655y = +0.51655x+0.48165x(x

x)+0.48165x(x

x)

×

×

0.048738(x+x+x)(x+x+x)(x+x)+0.0022335(x+x)(x)(x

x

×

×

x)

−
x+0.166x3+0.00865x5

x+

x3
3!

+

x5
5!

sinh(x)

the ﬁrst three terms of the Taylor series of sinh(x) around 0

(cid:0)

(cid:1)

x

−

0.2154
0.0012368(x

0.042956(x+x)x
−
x
x)(x
×

−
−
×
1+0.5x2+0.0416x4+0.00144x6
x6
6!

x4
4!

1+

×

×

+

+

x2
2!
cosh(x)

−

0.035877(x
x)

x)
×
0.042956x(x+x)

x)(x

×

0.2154

0.2154
0.25898x(x)

−

−
−

0.2154

−

the ﬁrst four terms of the Taylor series of cosh(x) around 0

(cid:0)

(cid:1)

We next perform a runtime comparison between GSR and NGGPPS on the Nguyen benchmark prob-
lem set. We run each benchmark problem on a single core and report the results in Table 11. We ﬁnd
that GSR exhibits faster runtime than NGGPPS, averaging 2.5 minutes per run on the Nguyen bench-
marks compared to 3.2 minutes for NGGPPS. It is worth noting that although GSR is, on average,
faster than NGGPPS, it still exhibits slower runtime on some problems (e.g. Nguyen-6 and Nguyen-9
in Table 11). This is due to the randomness of the search process as well as the use of sublibraries as
mentioned earlier in the Appendix. Indeed, the runtime depends on the stopping criterion or condition.
For example, one can shorten the runtime further if the interest is just in an approximation rather than
an exact recovery. Our GSR method recovers exact expressions in the order of few minutes.

We further highlight the strengths of GSR on the new SymSet benchmark problem set, and show the
beneﬁts of searching for expressions of the form g(y) = f (x) instead of y = f (x). Typical expressions,
with exact symbolic equivalence, recovered by GSR are shown in Table 26. The key feature of GSR
lies in its ability to recover expressions of the form g(y) = f (x). To better highlight the beneﬁts
offered by this feature, we disable it by constraining the search space in GSR to expressions of the
form y = f (x) (which is the most critical ablation). We refer to this special version of GSR as s-GSR.
Note that most of the SymSet expressions cannot be exactly recovered by s-GSR (i.e. they can only
be approximated). We compare the performance of GSR against s-GSR on the SymSet benchmarks
in terms of accuracy and runtime (see Table 12). The results clearly show that GSR is faster than
s-GSR, averaging around 2 minutes per run on the SymSet benchmarks compared to 2.27 minutes
for s-GSR (i.e.
11% runtime improvement). In addition, GSR is more accurate than s-GSR by
two orders of magnitude. This is due to the fact that GSR exactly recovers the SymSet expressions
across most of the runs, while s-GSR only recovers approximations for most of these expressions. It is
worth mentioning that on SymSet-1, SymSet-4, SymSet-5, SymSet-10, and SymSet-12, we observe
mean RMSE values of the same order of magnitude between GSR and s-GSR, since these expressions
can be exactly recovered by simply learning expressions of the form y = f (x). As GSR has to
perform a search to discover that g(y) is simply y for these expressions, it exhibits slower runtime
than s-GSR in recovering these expressions (see Table 12).

∼

2Livermore-7 and Livermore-8 can be expressed as sinh(x) = ex −e

−x

and cosh = ex+e

2

−x

respectively.

2

16

Table 10: Recovery rate comparison of GSR against literature-reported values from several algorithms
on the Nguyen and Livermore benchmark problem sets over 25 independent runs. The ground truth
expressions for these benchmarks are shown in Tables 19 and 20.

Benchmark

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11
Nguyen-12⋆

Nguyen Average

Livermore-1
Livermore-2
Livermore-3
Livermore-4
Livermore-5
Livermore-6
Livermore-7
Livermore-8
Livermore-9
Livermore-10
Livermore-11
Livermore-12
Livermore-13
Livermore-14
Livermore-15
Livermore-16
Livermore-17
Livermore-18
Livermore-19
Livermore-20
Livermore-21
Livermore-22

Livermore Average

All Average

GSR

NGGPPS + SLP/HER NGGPPS GEGL DSR

Recovery Rate (%)

100
100
100
100
100
100
100
100
100
100
100
100
100

100
100
100
100
100
100
0
0
4
100
100
100
100
100
100
100
100
100
100
100
76
100
85.45
90.59

100
100
100
100
100
100
100
100
100
100
100
4

100
100
100
100
100
100
96
100
100
100
100
12

100
100
100
100
92
100
48
100
100
92
100
0

100
100
100
100
72
100
35
96
100
100
100
0

92.00

92.33

86.00

83.58

100
100
100
100
40
100
4
0
88
8
100
100
100
100
100
100
36
48
100
100
88
92

100
100
100
100
4
88
0
0
24
24
100
100
100
100
100
92
68
56
100
100
24
84

100
44
100
100
0
64
0
0
12
0
92
100
84
100
96
12
4
0
100
100
64
68

3
87
66
76
0
97
0
0
0
0
17
61
55
0
0
4
0
0
100
98
2
3

77.45

82.59

71.09

78.59

56.36

66.82

30.41

49.18

17

Table 11: Single-core runtimes of GSR vs. NGGPPS on the Nguyen benchmarks. The ground truth
expressions for these benchmarks are shown in Table 19.

Benchmark

Nguyen-1
Nguyen-2
Nguyen-3
Nguyen-4
Nguyen-5
Nguyen-6
Nguyen-7
Nguyen-8
Nguyen-9
Nguyen-10
Nguyen-11

Average

Runtime (sec)
GSR NGGPPS

18.66
25.96
38.77
63.82
447.01
465.79
33.35
93.89
391.79
68.05
38.86
153.27

27.05
59.79
151.06
268.88
501.65
43.96
752.32
123.21
31.17
103.72
66.50

193.57

Table 12: Average performance in mean RMSE and runtime (on a single core), along with their
standard errors, for GSR and its most critical ablation (denoted by s-GSR) on the SymSet bench-
mark problem sets over 25 independent runs. The ground truth expressions for these benchmarks
are shown in Table 20.

Benchmark

GSR

s-GSR

Mean RMSE

Runtime (sec)

GSR

s-GSR

SymSet-1
SymSet-2
SymSet-3
SymSet-4
SymSet-5
SymSet-6
SymSet-7
SymSet-8
SymSet-9
SymSet-10
SymSet-11
SymSet-12
SymSet-13
SymSet-14
SymSet-15
SymSet-16
SymSet-17

Average

2.52
4.28
6.58
7.94
3.63
4.38
2.39
6.83
3.57
2.12
1.43
1.22
2.31
2.18
1.81
7.24
2.57
2.66

×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×

10−5
10−7
10−6
10−4
10−5
10−5
10−4
10−4
10−6
10−4
10−3
10−5
10−5
10−5
10−6
10−4
10−4
10−4

×

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

±

9.29
3.35
1.79
5.19
9.92
8.09
1.71
1.95
3.41
4.43
9.97
5.37
1.83
9.47
4.75
3.58
7.03
1.59

×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×

10−6
10−8
10−6
10−4
10−5
10−6
10−5
10−5
10−5
10−4
10−4
10−5
10−5
10−6
10−5
10−4
10−5
10−4

×

10−5
49.94
10−3
75.24
10−4
66.34
10−3
428.23
10−5
71.86
10−2
76.59
10−3
93.61
10−3
68.07
10−3
57.36
10−5
393.62
10−3
124.38
10−4
78.53
10−2
96.14
10−3
112.32
10−4
46.97
10−3
98.37
10−3
116.78
10−3 120.84

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

2.61
2.83
2.51
8.46
4.12
3.72
2.54
2.47
2.68
5.47
9.65
4.02
5.48
4.57
2.33
3.71
4.49
4.22

±

43.81
91.95
88.14
405.54
64.19
94.18
109.94
90.61
83.18
379.56
187.49
69.72
114.85
137.61
85.07
126.16
143.31

136.19

1.95
4.26
3.49
6.04
3.82
4.16
4.98
3.06
2.09
6.86
8.35
2.97
4.61
4.53
4.14
5.94
6.28

4.56

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

±

3.69
2.34
1.22
1.37
4.98
8.12
6.83
7.88
6.32
3.24
9.39
7.09
9.13
6.14
6.71
9.86
3.41

2.56

×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×

×

10−5
10−3
10−3
10−4
10−5
10−2
10−2
10−3
10−3
10−4
10−2
10−5
10−2
10−2
10−3
10−3
10−3
10−2

1.62
1.21
8.17
1.46
7.37
1.93
6.56
3.88
1.39
7.58
6.78
2.88
3.28
7.78
3.57
2.19
4.54

5.27

×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×

×

±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±
±

±

18

B More Examples on Our Matrix-Based Encoding Scheme

The encoding process happens according to a table of mapping rules that is very straightforward to
understand and use. For example, consider the mapping rules shown in Table 13 below, where d is the
dimension of the input feature vector. Note that Table 13 involves more transformations than Table 2.

Table 13: An example table of mapping rules for a basis function. Placeholder operands are denoted
by

2 corresponds to the square operator. The identity operator is denoted by
•

, e.g.
•

1.

•

,1

b
•
Transformation (T )

,2

b
•
Argument Type (arg)

0
1

0
x

1
1

•
1

1

2

−
•
2

,b

b
,3,
···
•
Variable (v)

•

,mB

0
skip

2
1
P Q
x2
x1

3
2

•

3
x3

4
3

•

···
···

5
cos

6
sin

7
exp

8
9
ln √

•

d
xd

Example 1. For d = 2, nBφ = 2 and mBφ = 4 (i.e. nv = 2), the basis function φ(x) = x2
generated according to the encoding steps shown in Table 14.

1ex1x2 can be

Table 14: Encoding steps corresponding to the basis function φ(x) = x2

1ex1x2 .

Step

T

arg

v2 Update
v1
x1 — T1(x) = x2
1
x1

T2(x) = ex1x2

x

2
•
exp

1
2
x2
Final Update: φ(x) = T1(x)

Q

T2(x)

·

1ex1x2 can be encoded by a 2

Based on the mapping rules in Table 13 and the encoding steps in Table 14, the basis function
φ(x) = x2
4 matrix as follows:
×
3 0
Bφ =
7 2

(16)

1
1

(cid:20)

•
2
(cid:21)

Example 2. For d = 3, nBφ = 5 and mBφ = 5 (i.e. nv = 3), the basis function
φ(x) = x3

can be generated according to the encoding steps shown in Table 15.

2sin(x2x3)√x2+2x3
2x1+x2

Table 15: Encoding steps corresponding to the basis function φ(x) = x3

2sin(x2x3)√x2+2x3
2x1+x2

.

Step

T

arg

v1

v2

v3 Update

1

x1

x
P

1
2
3
4
5
Final Update: φ(x) = T1(x)

T1(x) = (2x1 +x2)−
x1
x2
x2 — — T2(x) = x3
2
x2
x2

−
•
3
•
x3 — T3(x) = sin(x2x3)
sin
T4(x) = √x2 +2x3
√
x3
•
1 — — — — T5(x) = 1
T4(x)

T5(x)

T2(x)

T3(x)

Q
P

x3

·

·

·

·

1

Based on the mapping rules in Table 13 and the encoding steps in Table 15, the basis function
φ(x) = x3

2sin(x2x3)√x2+2x3
2x1+x2

(17)

×

can be encoded by a 5
2 1
4 0
6 2
9 1

0



Bφ = 

•

5 matrix as follows:
1 1
2
2
•
2 3
2 3

•

•
3


•


• •

19

Example 3. For nBψ = 2, the basis function ψ(y) = y3√y can be generated according to the encoding
steps shown in Table 16.

Table 16: Encoding steps corresponding to the basis function ψ(y) = y3√y.

Step

1
2

T

3

•
√

•

Update
T1(y) = y3
T2(y) = √y

Final Update: ψ(y) = T1(y)

T2(y)

·

Based on the mapping rules in Table 13 and the encoding steps in Table 17, the basis function
ψ(y) = y3√y can be encoded by a 2

1 matrix as follows:

Bψ =

4
9
(cid:21)

(cid:20)

(18)

Example 4. For nBψ = 3, the basis function ψ(y) = ln(y) can be generated according to the encoding
steps shown in Table 17.

Table 17: Encoding steps corresponding to the basis function ψ(y) = ln(y).

Step

T Update

1
2
3

1
ln
1

T1(y) = 1
T2(y) = ln(y)
T3(y) = 1

Final Update: ψ(y) = T1(y)

T2(y)

T3(y)

·

·

Based on the mapping rules in Table 13 and the encoding steps in Table 17, the basis function
ψ(y) = ln(y) can be encoded by a 3

1 matrix as follows:

×

×

Bψ =

0
8
0#
"

(19)

Example 5. For nBψ = 1, the basis function ψ(y) = ey can be generated according to the encoding
steps shown in Table 18.

Table 18: Encoding steps corresponding to the basis function ψ(y) = ey.

Step

T

1

exp

Update
T1(y) = ey

Final Update: ψ(y) = T1(y)

Based on the mapping rules in Table 13and the encoding steps in Table 18, the basis function ψ(y) = ey
can be encoded by a 1

1 matrix as follows:

×

Bψ = [7]

(20)

20

C Symbolic Regression Benchmark Problem Sets

Table 19: Speciﬁcations of the Symbolic Regression (SR) benchmark problems. Input variables are
denoted by x for 1-dimensional problems, and by (x1, x2) for 2-dimensional problems. U (a, b, c)
indicates c random points uniformly sampled between a and b for every input variable; different
random seeds are used for the training and test sets. E(a,b,c) indicates c evenly spaced points between
except Neat-
a and b for every input variable; the same points are used for the training and test sets
6, which uses E(1, 120, 120) as test set, and the Jin tests, which use U (
. To
(cid:0)
simplify the notation, libraries (of allowable arithmetic operators and mathematical functions) are
. Placeholder operands are
deﬁned relative to a ‘base’ library
denoted by

L0 =
÷
2 corresponds to the square operator.

, cos, sin, exp, ln
}

3, 3, 30) as test set

, e.g.

+,

−

−

×

{

(cid:1)

,

,

•

•

Benchmark Expression

Dataset

Library

1

−

1+ 1
x3
1+ 1
x3

y = x3 +x2+x
Nguyen-1
y = x4 +x3+x2+x
Nguyen-2
y = x5 +x4+x3+x2 +x
Nguyen-3
y = x6 +x5+x4+x3 +x2+x
Nguyen-4
y = sin(x2)cos(x)
Nguyen-5
y = sin(x)+sin(x+x2)
Nguyen-6
y = ln(x+1)+ln(x2 +1)
Nguyen-7
Nguyen-8
y = √x
y = sin(x1)+sin(x2
Nguyen-9
2)
Nguyen-10
y = 2sin(x1)cos(x2)
y = xx2
Nguyen-11
1
y = x4
2 x2
Nguyen-12
x2
1 −
2 −
Nguyen-12⋆ y = x4
2 x2
x2
1 −
2 −
1+0.5x2
1.3x3
y = 2.5x4
1−
y = 8x2
1+8x3
15
2−
1+0.5x3
y = 0.2x3
2−
y = 1.5ex1 +5cos(x2)
y = 6sin(x1)cos(x2)
y = 1.35x1x2 +5.5sin((x1 −
y = x4 +x3+x2+x
y = x5 +x4+x3+x2 +x
y = sin(x2)cos(x)
−
y = ln(x+1)+ln(x2 +1)
y = 2sin(x1)cos(x2)
x
1
y =
k=1
k
y = 2
2.1cos(9.8x1)sin(1.3x2)
−
P
−(x1
y = e
1.2+(x2−
y = 1
−4
1+x
1

Neat-1
Neat-2
Neat-3
Neat-4
Neat-5
Neat-6
Neat-7

Jin-1
Jin-2
Jin-3
Jin-4
Jin-5
Jin-6

2−
1.2x2−

Neat-8
Neat-9

2.5)2
+ 1
1+x

−1)2

−4
2

1

1.7x2

0.5x1

1)(x2−

−
−
−
−
−
−

1,1,20)
1,1,20)
1,1,20)
1,1,20)
1,1,20)
1,1,20)

U (
U (
U (
U (
U (
U (
U (0,2,20)
U (0,4,20)
U (0,1,20)
U (0,1,20)
U (0,1,20)
U (0,1,20)
U (0,10,20)

U (
U (
U (
U (
U (
1)) U (

−
−
−
−
−
−

3,3,100)
3,3,100)
3,3,100)
3,3,100)
3,3,100)
3,3,100)

−
−
−

1,1,20)
1,1,20)
1,1,20)

U (
U (
U (
U (0,2,20)
U (
E(1,50,50)
E(

−

1,1,100)

50,50,105)

U (0.3,4,100)
E(

5,5,21)

−

−

L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0 −{
L0 −{
L0 −{
L0 −{
L0 −{
L0 −{
1
L0 ∪{
1
L0 ∪{
1
L0 ∪{
1
L0 ∪{
L0
,√
,
+,
−
•}
−•
•
÷
×
{
3,√
2,
tan,tanh,
L0 ∪{
•
•
2
,exp,e−•,
,
+,
{
•
−
L0

}∪{•
}∪{•
}∪{•
}∪{•
}∪{•
}∪{•

3,const
}
3,const
}
3,const
}
3,const
}
3,const
}
3,const
}

ln
ln
ln
ln
ln
ln

2,
2,
2,
2,
2,
2,

•
•
•
•
•
•

}
}
}
}

1,

÷

×

}

,

,

•}

21

Table 20: Speciﬁcations of the Symbolic Regression (SR) benchmark problems. Input variables are
denoted by x for 1-dimensional problems, by (x1,x2) for 2-dimensional problems, and by (x1,x2,x3)
for 3-dimensional problems. U (a,b,c) indicates c random points uniformly sampled between a and b
for every input variable; different random seeds are used for the training and test sets. To simplify the
notation, libraries (of allowable arithmetic operators and mathematical functions) are deﬁned relative
to ‘base’ libraries
. Placeholder operands are
denoted by
, e.g.

c
0 =
L0 =
L
2 corresponds to the square operator.
•

,cos,sin,exp,ln
}

const
}

L0 ∪ {

+,

or

×

−

÷

•

{

,

,

Benchmark

Livermore-1
Livermore-2
Livermore-3
Livermore-4
Livermore-5
Livermore-6
Livermore-7
Livermore-8
Livermore-9
Livermore-10
Livermore-11

Livermore-12

Livermore-13
Livermore-14
Livermore-15
Livermore-16
Livermore-17
Livermore-18
Livermore-19
Livermore-20
Livermore-21
Livermore-22

SymSet-1
SymSet-2
SymSet-3
SymSet-4
SymSet-5
SymSet-6
SymSet-7

SymSet-8

SymSet-9
SymSet-10

SymSet-11
SymSet-12
SymSet-13
SymSet-14
SymSet-15
SymSet-16
SymSet-17

10,10,1000)
1,1,20)
1,1,20)

x2

2
1

−
−
−

Dataset

−
−
−
−

1x2
1
x1+x2

Expression
y = 1
3 +x+sin(x2)
U (
y = sin(x2)cos(x)
U (
−
y = sin(x3)cos(x2)
U (
−
y = ln(x+1)+ln(x2 +1)+ln(x)
U (0,2,20)
y = x4
1 +x2
x3
U (0,1,20)
1 −
1−
y = 4x4 +3x3+2x2+x
U (
U (
y = sinh(x)
y = cosh(x)
U (
y = x9 +x8 +x7+x6 +x5+x4 +x3+x2 +x U (
y = 6sin(x1)cos(x2)
y = x2
y = x5
1
x3
2
1
y = x
3
y = x3 +x2 +x+sin(x)+sin(x2)
y = x
y = x
y = 4sin(x1)cos(x2)
y = sin(x2)cos(x)
5
−
y = x5 +x4 +x2+x
y = e−
y = x8 +x7 +x6+x5 +x4+x3 +x2+x
y = e−

U (0,4,20)
U (
U (0,4,20)
U (0,4,20)
U (0,1,20)
U (
U (
U (
U (
U (

U (0,1,20)
U (

0.5x2

U (

1
5
2
5

−

−

−

x2

1,1,20)
1,1,20)
1,1,20)
1,1,20)

1,1,50)

1,1,50)

1,1,20)
1,1,20)
1,1,20)
1,1,20)
1,1,20)

−
−
−
−
−

1,1,20)

1,1,20)
1,1,20)
1,1,20)
3,3,20)
1,1,20)
1,1,20)

U (
U (
U (
U (
U (
U (
U (0,2,20)

−
−
−
−
−
−

U (

1,1,20)

−

U (0,2,20)
U (0,1,20)

1,1,20)
1,1,20)

−
−

U (
U (
U (0,1,20)
U (0,2,20)
U (0,1,20)
U (0,1,20)
U (0,1,20)

1

1
3

2 −

−
−

2x2+7

1.7x2−

4
5
2.8x+5)−

y = xsinh(x)
3x4
y = (x5
−
1.2x2+11.5)
y = (x4
−
cos(x)+4.2exsin(x2)
y = 0.8
−
y = 4.5x2
1 +x1x3
3.1
y = 5
x3
3x1−
2
y = ln(x3
1 +4x1x2)
1x4
1 +14x3
5x5
y =
y = (2x1 +x2)−
y = 1.5cos(x1)ln(x1x2)
y =
y = 0.4x4
p
y = 2x2
x1+x3
y = x1x2x3
x1+x2+x3
y = (x1 +x2)x3
y = e2.6x1−
y = ln

ln(x2)+9.8cos(x3)
0.2ex1+x2 +0.5cos(x2
3)

−
2cos(x1)+30ex2 +4

1 +6.2x2−

3.5x1x3 −

2 −

2.5

p

2
3

4.5

(cid:0)

(cid:1)

22

Library

L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
L0
c
0 −{
L
c
0 ∪{•
L
c
0 ∪{•
L
c
0
L
c
0
L
c
0 ∪{•
L
c
0
L
c
0 ∪{•
L
c
0
L
c
0
L
2
c
0 ∪{•
L
c
0
L
c
0
L
c
0
L
c
0
L
c
0
L
c
0
L

ln

}∪{
1
−
}
3
2,
•

}

−

1

}

2,

3
•

}

}

e−•

}

D Typical Recovered Expressions

Table 21: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Nguyen
benchmark set. Note that the coefﬁcients in the GSR expressions form a unit vector due to the normal-
ization constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by simpliﬁcation)
that the GSR expressions are symbolically equivalent to the ground truth expressions.

Benchmark

Expression

Truth y = x3 +x2 +x

Nguyen-1

0.558y =

GSR

−

x)

0.1395(x

x
0.1395(x
×
×
×
−
×
0.2697x+0.0651(x+x+x+x)+0.0651(x+x+x+x)
0.2697x
−
0.558(x
x
0.1395(x
×

0.1395(x

0.2697x

0.2697x

x)

x)

x)

×

−

−

−

×

−

×

×

x

x

−
−
−

x)

Truth y = x4 +x3 +x2+x

Nguyen-2

0.58554y =

GSR

−

Nguyen-3

GSR

Truth y = x5 +x4 +x3+x2 +x
x
0.5y = +0.125x+0.25(x
×
x
+0.125x+0.5x(x
×
+0.125(x)x+0.125(x
×
Truth y = x6 +x5 +x4+x3 +x2+x

×
×

Nguyen-4

GSR

0.48318y =

−

0.09759x(x+x)
0.19518(x
×
0.09759(x+x)x

×

x

0.58554x

0.29277x(x

−
−
0.19518(x

×
x)x

0.19518(x
x)
x

×

x)x
−
0.29277x(x

0.09759(x+x)x
x

x)

×

×

×
−

−
x)

−

×

−
−
−

x)+0.125(x)x+0.5(x
x)+0.125x+0.125(x

x
×
x
×
x)+0.25x(x

×
×

x

×

×

x)

x)x
x)+0.125x

0.096636x
0.24159(x
0.24159(x
0.096636x

x)(x

0.48318x(x
−
x)x
−
×
x)
x
×
0.096636x
−

0.096636x

×
−
0.24159(x
×
0.16106(x

×

−

−

×

−
−
−
−

x)

×

x)

0.16106(x
−
×
x
0.48318(x
x)(x+x)
x

0.16106(x
x)
×
−
x)x
x)(x
×
0.096636x

×

x)

×

×
−

Nguyen-5

Nguyen-6

1

−

Truth y = sin(x2)cos(x)
GSR 0.63246y = 0.63246cos(x)sin(x
Truth y = sin(x)+sin(x+x2)
GSR 0.5y = 0.5cos(x)sin(x
×
Truth y = ln(x+1)+ln(x2 +1)
0.70956ey = +0.1095x(x

x

x)

−

×

0.31623

−

0.31623

x)+0.5sin(x)+0.5cos(x

x)sin(x)

×

Nguyen-7

GSR

x)+0.1095(x

x

x

×
+0.17082x+0.12702(x
×
+0.22776x(x
+0.23652(x+x+x)x+0.17082x+0.01314(x+x)

×
×
x)+0.23652+0.22776(x

×
x)(x+x)+0.17082x

×

×

−

×

x)x+0.23652

x)+0.17082x+0.23652
0.1095x(x+x)(x

x)

×

Nguyen-8

Nguyen-9

Nguyen-10

Nguyen-11

Nguyen-12

Truth y = √x
GSR 0.83654ln(y) =
Truth y = sin(x1)+sin(x2
2)
GSR

0.57735y =

−

−

0.032175ln(x

x

x

×

×

×

x)+0.54697ln(x)

0.57735sin(x1)

0.57735sin(x2 ×

−

x2)

−

Truth y = 2sin(x1)cos(x2)
GSR 0.44721y = 0.89442sin(x1)cos(x2)
Truth y = xx2
1
GSR 0.70711ln(y) = 0.70711x2ln(x1)
1 + 1
2 x2
Truth y = x4
x3
x2
2 −
GSR
0.2(x2×
x2)
−
0.4x1(x1 ×
−
1 + 1
x3
2 x2
x2
2 −
0.1(x2+x2+x2)x2 −
−
x1)+0.6x2
+0.3x1(x1 ×

0.4x1−
x1)

Truth y = x4

1 −
0.6y =

1 −
0.4y =

−
x1×

−

−

0.4x1+0.4(x1+x2 +x1)+0.4(x1×

x1)x1

Nguyen-12⋆ GSR

0.3(x1+x1)(x1 ×

x1 ×

x1)+0.3(x1×

x1 ×

x1)

23

Table 22: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Jin bench-
mark set. Note that the coefﬁcients in the GSR expressions form a unit vector due to the normalization
constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by simpliﬁcation) that
the GSR expressions are symbolically equivalent to the ground truth expressions. Although GSR does
not exactly recover Jin-6, it recovers approximations with very low RMSE, as shown in Table 5.

Benchmark

Expression

Jin-1

Jin-2

Jin-3

Jin-4

Jin-5

Jin-6

15

1.7x2
0.398632x3

1.3x3

1+0.5x2
Truth y = 2.5x4
2−
1−
GSR 0.30664y = +0.15332x2
2−
Truth y = 8x2
1+8x3
GSR 0.06909y =
Truth y = 0.2x3
GSR
0.7943y =
Truth y = 1.5ex1 +5cos(x2)
GSR
0.25198y =

2−
0.518175+0.55272x3
−
1+0.5x3
1.2x2−
2−
0.11914x2−
0.37797ex1

0.5x1
0.15886x3

2−

−

−

−

−

Truth y = 6sin(x1)cos(x2)
GSR 0.13484y =
Truth y = 1.35x1x2 +5.5sin((x1 −
GSR Not exactly recovered

−

1)(x2 −

1))

0.260644x2−

0.260644x2+0.7666x1x3
1

1−

0.518175+0.27636x2

1+0.27636x2
1

1+0.39715(x2+x2 +x2+x1)

0.11915x2−

−

0.39715x3
2

0.62995cos(x2)

0.62995cos(x2)

−

−

0.80904sin(x2)cos(x1)+0.40452sin(x2 +x1)+0.40452sin(x2 +x1)

Table 23: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Neat
benchmark set. Note that the coefﬁcients in the GSR expressions form a unit vector due to the normal-
ization constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by simpliﬁcation)
that the GSR expressions are symbolically equivalent to the ground truth expressions. Although GSR
does not exactly recover Neat-6,Neat-7, Neat-8, and Neat-9, it recovers approximations with very low
RMSE, as shown in Table 6.

Benchmark

Expression

Truth y = x4 +x3 +x2+x

Neat-1

0.64952y =

GSR

−

0.32476x(x+x)(x
0.32476(x
−
0.064952(x+x)

x)
−
0.2129x

×
−
0.2129x

x)

×

0.064952(x+x)+0.082996(x+x+x)
0.18764x(x+x)x
x

0.32476(x
0.2129x

x)
×
−
0.27424(x
−

x)

×

×

−

−

−
−
−

Truth y = x5 +x4 +x3+x2 +x
0.3914y = +0.3914x(x

Neat-2

GSR

x

×

x
×
+0.02794(x+x)+0.3914x(x
+0.18274x+0.27676(x
x)
×
0.12682(x+x+x)+0.18274x
+0.18274x+0.18274x
−

x)+0.18274x+0.27676x(x)+0.18274x
0.02702(x+x+x)(x+x)
x)
×
0.12682(x+x+x)+0.3914(x
−

−

×

×

x)x

x

×

1

Truth y = sin(x2)cos(x)
GSR
0.57735y =
Truth y = ln(x+1)+ln(x2 +1)
GSR

−

−

−

0.57735sin(x

x)cos(x)+0.57735

×

−

0.25(x

0.5ey =
×
Truth y = 2sin(x1)cos(x2)
GSR 0.57735y = 0.57735cos(x2)sin(x1)+0.57735cos(x2)sin(x1)

0.5x(x

0.25x

0.25x

0.5

x)

x)

−

−

−

−

×

−

−

0.25(x

x)

×

Neat-3

Neat-4

Neat-5

Neat-6

Neat-7

Neat-8

Neat-9

1
k

x
Truth y =
k=1
GSR Not exactly recovered

P
Truth y = 2
−
GSR Not exactly recovered

2.1cos(9.8x1)sin(1.3x2)

−1)2

2.5)2

−(x1
Truth y = e
1.2+(x2−
GSR Not exactly recovered
Truth y = 1
−4
1+x
1
GSR Not exactly recovered

+ 1
1+x

−4
2

24

Table 24: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Livermore
benchmark set. Note that the coefﬁcients in the GSR expressions form a unit vector due to the normal-
ization constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by simpliﬁcation)
that the GSR expressions are symbolically equivalent to the ground truth expressions. Although GSR
does not exactly recover Livermore-7 and Livermore-8, it naturally recovers their Taylor approxima-
tions, as discussed in Appendix A.

0.21064x

0.022175(x+x)

−

0.50998x(x

x)

−

×

0.21064x
−
0.022175(x+x)

−

0.25499(x

x)

×

x1)

0.44721(x1×

x1)x1 +0.44721(x1×

x1)(x1 ×

−

x1)

Benchmark
Livermore-1 Truth y = 1

3 +x+sin(x2)

GSR 0.65079y = 0.21693+0.65079sin(x

Expression

x)+0.325395(x+x)

×

Livermore-2 Truth y = sin(x2)cos(x)

2

−

GSR

0.40825y =

0.40825cos(x)sin(x

−

−

x)+0.8165

×

Livermore-3 Truth y = sin(x3)cos(x2)

x

×

×

x)cos(x

x)

×

Livermore-4 GSR

Livermore-5 Truth y = x4

−

−

−

0.57735sin(x

0.50998ey =

1
−
GSR
0.57735y = 0.57735
Truth y = ln(x+1)+ln(x2 +1)+ln(x)
x)
−
x)(x

−
−
1+x2
x3
1 −
GSR 0.44721y =
0.44721x2+0.44721(x1×
−
Truth y = 4x4+3x3+2x2+x
0.15763y =

0.25499(x
0.50998(x

×
×

1 −

x2

x)

×

−

Livermore-6

GSR

−

0.26677x

0.26677x
−
0.47289(x
0.03804(x+x)x
0.093216(x+x)+0.253886(x+x+x+x)

0.093216(x+x)

0.63052x(x)(x

×
x
×
0.26677x

0.23918(x

x)

−

−

−

−

×

x)

x)

×

−

−
−
−

Livermore-7 Truth y = sinh(x)

GSR Not exactly recovered

Livermore-8 Truth y = cosh(x)

Livermore-9

0.30767y =

GSR Not exactly recovered
Truth y = x9 +x8+x7 +x6+x5 +x4+x3 +x2+x
GSR
x)
x)(x
×
×
−
−
x)(x)(x
x
−
×
×
x)(x
x
x
×
×
×
−
+0.056037(x+x+x+x)(x+x+x+x)
x

0.30767x(x
0.30767(x
0.30767(x

0.30767(x
x)
x

0.30767(x

x)(x)(x

×
x
x

×
×

×
×

x)

−

×

x

x

x

−

×

×

×

×

×

−

x

x)x

0.266671(x

x)

0.30767(x

−

×
×
0.153835x
−
−
0.30767(x
x
−
×
0.266671(x

x)

×
−
−
0.22364x(x+x+x)

x
×
x)
×

x

×
x)
×
×
x)x
x
0.153835x

×

Livermore-10 Truth y = 6sin(x1)cos(x2)

GSR 0.22942y = 0.68826cos(x2)sin(x1)+0.68826sin(x1)cos(x2)

Livermore-11 Truth y = x2

1x2
1
x1+x2

GSR

−

0.40825ln(y) =

0.8165ln(x1 ×

−

x1)+0.40825ln(x2 +x1)

25

Table 25: Typical expressions (with exact symbolic equivalence) recovered by GSR for the Livermore
benchmark set (cont’d). Note that the coefﬁcients in the GSR expressions form a unit vector due to
the normalization constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by
simpliﬁcation) that the GSR expressions are symbolically equivalent to the ground truth expressions.

Benchmark
Livermore-12 Truth y = x5
1
x3
2

GSR 0.16903ln(y) = 0.84515ln(x1)

Livermore-13 Truth y = x

1
3

Expression

0.50709ln(x2)

−

GSR 0.97332ln(y) = 0.16222ln(x)+0.16222ln(x)

Livermore-14 Truth y = x3 +x2+x+sin(x)+sin(x2)

GSR 0.40825y = 0.40825x(x

Livermore-15 Truth y = x

1
5

x)+0.40825sin(x)+0.40825(x

x)+0.40825sin(x

x)+0.40825x

×

×

×

GSR 0.99015ln(y) = 0.099015ln(x)+0.099015ln(x)

Livermore-16 Truth y = x

2
5

GSR 0.99504ln(y) = 0.099504ln(x

Livermore-17 Truth y = 4sin(x1)cos(x2)

x

x

×

×

×

x)

GSR 0.17408y = 0.69632sin(x2 +x1)

0.69632cos(x1)sin(x2)

−

Livermore-18 Truth y = sin(x2)cos(x)

5
0.19245y = 0.96225

GSR
Truth y = x5 +x4+x2 +x

−

−

0.19245cos(x)sin(x

x)

×

−

Livermore-19 GSR

0.46202y =

−

Livermore-20 Truth y = e−

x2

−
−

0.46202x(x
0.290313x

x)x
×
0.15297(x+x)
−

−

−

0.015609(x+x+x)+

0.46202

0.15297(x+x)+0.12175(x+x+x+x)

−

(x1

x1)

∗

∗

−

0.46202x(x

x)(x

x)

×

×

−

0.89442ln(y) = 0.44721(x+x)x

GSR
Truth y = x8 +x7+x6 +x5+x4 +x3+x2 +x
0.027357(x+x+x)x

0.38914y =

Livermore-21 GSR

−

−
+0.0714425x(x+x)(x+x)

−

Livermore-22 Truth y = e−

0.5x2

0.19457(x
0.22497(x

−
−

×
×

0.38914(x

x
0.38914x

x
x
x)(x
x)
×
×
−
×
0.22497(x
0.38914(x
−
×
−
0.22497x(x
×
−
0.097285x(x+x+x+x)(x
−

x)(x
0.38914x(x
×
×
x
x
x)
x
×
×
×
0.027357x(x+x+x)
x)
−
x

×
x)

×

×

×

×

x

×
x)
×
x)
×

x

x
×
x)x

x)(x+x)(x
×
0.224998(x

−

−

x)

x
×
x)

GSR 0.8165ln(y) =

−

0.40825(x+x)x+0.40825(x

x)

×

26

Table 26: Typical expressions (with exact symbolic equivalence) recovered by GSR for the SymSet
benchmark set. Note that the coefﬁcients in the GSR expressions form a unit vector due to the normal-
ization constraint imposed by the Lasso problem in Equation (6). It is easy to verify (by simpliﬁcation)
that the GSR expressions are symbolically equivalent to the ground truth expressions.

Benchmark

SymSet-1

Expression

0.17612xe−
1

x

−

0.563584exe−

x

0.17612xe−

x

−

Truth y = xsinh(x)
GSR 0.70448y = 0.35224exx
Truth y = (x5

−
2.8x+5)−

−

4
5

3x4
−
0.11335y−

−
1 =

−
−
−
1.2x2+11.5)

1
3

SymSet-2

GSR

−

0.23188x
−
0.11335
0.11335
0.11335+0.34005x(x

0.11335+0.50651(x+x)
0.23188x
x)
x

−

−

−

−
0.11335(x

0.11335
x

×

0.23188x
−
x)x
x
×

×

Truth y = (x4
GSR 0.1173y3 = +0.26576x2+0.26576x2+0.26576(x

−

x)+0.44965+0.44965

+0.02346(x+x+x+x+x)x3

0.23451(x+x+x+x)x+0.44965

×

×

×

−

SymSet-3

SymSet-4

SymSet-5

SymSet-6

SymSet-7

SymSet-8

SymSet-9

−
1+x1x3
−

0.16964y =

−

Truth y = 0.8
GSR 0.22485y =
Truth y = 4.5x2
GSR
−
Truth y = 5
x3
3x1−
2
GSR 0.84515y−
Truth y = ln(x3
GSR

1 +4x1x2)

cos(x)+4.2exsin(x2)

0.112425cos(x)+0.94437sin(x

x)ex

×

−

0.112425cos(x)+0.17988

1.7x2−
3.1
2 −
0.16964(x2×

x2×

x2 ×

x1)

0.76338(x1×

−

x1)+0.525884+0.288388x2

1 =

0.16903(x2×

−

x2)x2 +0.50709x1

−

0.482715ey =

Truth y =
GSR 0.060634y2 = 0.30317(x2

5x5

0.64362(x1+x1 +x1)x2 +0.21883x2−
−
1x4
1 +14x3
2 −

2x2+7
1)+0.848876x2
x3
1×

2(x1 ×

1x3

0.482715(x1×

x1 ×

x1)

−

0.21883x2

x2)+0.424438

0.060634(x2+x2)

−

p
Truth y = (2x1 +x2)−
GSR 0.83205ln(y) =

2
3

0.5547ln(x1 +x2+x1)

−

SymSet-10 Truth y = 1.5cos(x1)ln(x1x2)
GSR 0.32444y =

2.5
−
0.8111+0.48666ln(x1 ×

−

x2)cos(x1)

SymSet-11 Truth y =

GSR
−
Truth y = 0.4x4

p

2cos(x1)+30ex2 +4
0.228568y +0.028571y2=
0.457136+0.85713ex2+0.057142cos(x1)
−
4.5
3.5x1x3 −
1+6.2x2−
0.18324+0.065152x1(x1)(x1 ×
−
0.18324+0.504928x2
0.18324
−

0.57008(x3×

x1)

−

−

x1)+0.504928x2−

0.18324

SymSet-12 GSR 0.16288y =

SymSet-13 Truth y = 2x2
x1+x3

SymSet-14 Truth y = x1x2x3

x1+x2+x3

GSR 0.57735ln(y) = 0.57735ln(x2 +x2)

0.57735ln(x1 +x3)

−

GSR 0.57735ln(y) = 0.57735ln(x1 ×

x3)

x2×

−

0.57735ln(x2 +x3+x1)

SymSet-15 Truth y = (x1 +x2)x3

GSR 0.70711ln(y) = 0.70711x3ln(x1 +x2)

SymSet-16 Truth y = e2.6x1−

ln(x2)+9.8cos(x3)

GSR 0.098035ln(y) = 0.960743cos(x3)

SymSet-17 Truth y = ln

0.2ex1+x2 +0.5cos(x2
3)

GSR 0.88045ey = 0.17609ex1+x2 +0.440225cos(x3 ×

(cid:1)

(cid:0)

x3)

0.0490175ln(x2 ×

−

x2)+0.254891x1

27

E Limitations

Although GSR achieves great results whether by recovering exact expressions or approximations with
low errors, it still has several limitations:

Absence of division operations. The primary limiting factor to our GSR method is that it still can-
not handle divisions. This is due to the way we deﬁne our encoding scheme. In this current ver-
sion, we only consider a weighted sum of basis functions where the basis functions are a product of
transformations; no divisions are involved. We can overcome this issue by modifying the encoding
scheme to include divisions within the basis functions
e.g. a negative integer in the ﬁrst column
of the basis functions implies a division by the corresponding transformation, i.e. using Table 13, a
ﬁrst-column entry of
. However, this will signiﬁcantly increase the total
number of possible combinations in which we can form basis matrices. Due to the lack of divisions
(cid:1)
in its current version, GSR suffers on some benchmarks such as Neat-6, Neat-8, Neat-9, Livermore-
7, Livermore-8. It is worth noting that GSR can recover some divisions with the help of the ln or
1 operators (see Livermore-11, Livermore-12, Livermore-20, Livermore-22, SymSet-2, SymSet-6,
•
SymSet-9, SymSet-13, and SymSet-14). This is only possible when the original function consists of
only one term (not a sum of terms).

8 encodes the division 1
ln

−

(cid:0)

−

Composition of tranformations. Another limiting factor to the current version of GSR is that it
cannot recover expressions containing composite functions, such as y = ecos(x) + ln(x).
In this
example, the basis function ecos(x) cannot be recovered by GSR due to our encoding scheme. Again,
if ln(x) was not there, that is, if the function contained the ﬁrst term only, i.e. y = ecos(x), then GSR
can handle the situation by recovering ln(y) = cos(x). The beneﬁts of using g(y) = f (x) can be
clearly observed on the SymSet benchmark problems (especially SymSet-16, and SymSet-17).

Choice of hyperparameters and search process. Throughout our experiments, we have observed
that, for some benchmarks (such as Jin-6 and Neat-7), althought they are expressible by GSR, they
were not fully recovered. GSR only recovered approximations for these benchmarks with very low
errors. This can be explained by two reasons: i) The choice of hyperparameters affects the search
process, ii) Our matrix-based GP search process may not be very effective on these benchmarks,
given the complexity of their corresponding basis functions, and thus they may require a huge number
of iterations to be recovered. That is, if we keep our code running for a very long time, we may be able
to recover these benchmarks. This can be veriﬁed by expanding Jin-6 and Neat-7 as follows:

Jin-6:

y = 1.35x1x2 +5.5sin((x1 −
= 1.35x1x2 +5.5sin(x1x2 −
= 1.35x1x2 +5.5
x1 −
= 1.35x1x2 −

sin(

−

(cid:2)

1))
x2+1)

1)(x2−
x1 −
x2)cos(x1x2 +1)+cos(

5.5cos(1)sin(x1 +x2)cos(x1x2)

+5.5sin(1)sin(x1 +x2)sin(x1x2)

(cid:3)

φ1(x)

φ2(x)

x1−

−

x2)sin(x1x2 +1)

+5.5cos(1)cos(x1 +x2)sin(x1x2)
}

|

{z
φ3(x)

+5.5sin(1)cos(x1 +x2)cos(x1x2)
}

|

{z
φ4(x)

Neat-7:

|

{z

}

|

{z

}

y = 2

= 2

= 2

−

−

−

−

2.1cos(9.8x1)sin(1.3x2)
2.1
cos(9.8)cos(x1)
2.1cos(9.8)sin(1.3)cos(x1)cos(x2)

sin(9.8)sin(x1)

−

(cid:0)

sin(1.3)cos(x2)+cos(1.3)sin(x2)

+2.1sin(9.8)sin(1.3)sin(x1)cos(x2)

(cid:1)(cid:0)

(cid:1)

φ1(x)

φ2(x)

+2.1sin(9.8)cos(1.3)sin(x1)sin(x2)
2.1cos(9.8)cos(1.3)cos(x1)sin(x2)
}
}

|

|

{z
φ3(x)

{z
φ4(x)

}
As we can see, GSR has to ﬁnd the four corresponding basis functions simultaneously in order to
recover the expressions.

{z

{z

|

}

|

Indeed, there are plenty of expressions that still cannot be fully recovered by our GSR method. This
is the case for all the other methods as well.

28

