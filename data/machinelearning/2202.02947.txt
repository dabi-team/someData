Parallel Successive Learning for Dynamic Distributed
Model Training over Heterogeneous Wireless Networks

Seyyedali Hosseinalipour, Member, IEEE, Su Wang, Student Member, IEEE, Nicol`o
Michelusi, Senior Member, IEEE, Vaneet Aggarwal, Senior Member, IEEE, Christopher G.
Brinton, Senior Member, IEEE, David J. Love, Fellow, IEEE, and Mung Chiang, Fellow, IEEE

1

2
2
0
2

b
e
F
7
1

]

G
L
.
s
c
[

4
v
7
4
9
2
0
.
2
0
2
2
:
v
i
X
r
a

Abstract‚ÄîFederated learning (FedL) has emerged as a popular
technique for distributing model training over a set of wireless
devices, via iterative local updates (at devices) and global aggrega-
tions (at the server). In this paper, we develop parallel successive
learning (PSL), which expands the FedL architecture along
three dimensions: (i) Network, allowing decentralized cooperation
among the devices via device-to-device (D2D) communications. (ii)
Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL
considers heterogeneous number of stochastic gradient descent
iterations with different mini-batch sizes at the devices; (ii-b)
Data: PSL presumes a dynamic environment with data arrival
and departure, where the distributions of local datasets evolve
over time, captured via a new metric for model/concept drift.
(ii-c) Device: PSL considers devices with different computation
and communication capabilities. (iii) Proximity, where devices
have different distances to each other and the access point. PSL
considers the realistic scenario where global aggregations are
conducted with idle times in-between them for resource efÔ¨Åciency
improvements, and incorporates data dispersion and model disper-
sion with local model condensation into FedL. Our analysis sheds
light on the notion of cold vs. warmed up models, and model
inertia in distributed machine learning. We then propose network-
aware dynamic model tracking to optimize the model learning
vs. resource efÔ¨Åciency tradeoff, which we show is an NP-hard
signomial programming problem. We Ô¨Ånally solve this problem
through proposing a general optimization solver. Our numerical
results reveal new Ô¨Åndings on the interdependencies between the
idle times in-between the global aggregations, model/concept drift,
and D2D cooperation conÔ¨Åguration.

Index Terms‚ÄîParallel successive learning, federated learning,
device-to-device communications, network optimization, peer-to-
peer learning, dynamic machine learning, cooperative learning.

I. INTRODUCTION

Distributed machine learning (ML) over wireless networks has
attracted signiÔ¨Åcant attention recently, for applications ranging
from keyboard next word prediction to autonomous driving [1],
[2]. Distributed ML is an alternative to centralized ML, which
requires a central node, e.g., a server, coexisting with the
dataset. This alternative is of particular interest since in many
applications the dataset is collected in a distributed fashion
across a set of wireless devices, e.g., through their sensing
equipment or the users‚Äô input, where the transfer of data to the
cloud incurs signiÔ¨Åcant energy consumption and long delays.
Federated learning (FedL) is the most recognized distributed
ML technique, with the premise of keeping the devices‚Äô datasets
local [3], [4]. Its conventional architecture resembles a star
topology of device-server interactions (Fig. 1). Each model
training round of FedL consists of (i) local updating, where
devices update their local models based on their datasets and

Fig. 1: ‚ÄúStar‚Äù learning topology architecture of conventional FedL.

the global model, e.g., via stochastic gradient descent (SGD),
and (ii) global aggregation, where the server aggregates the
local models to a new global model and broadcasts it.

A. Related Work

Researchers have considered the effects of limited/imperfect
communications in wireless networks (e.g., channel fading,
packet loss, and limited bandwidth) on the performance of
FedL [5]‚Äì[9]. Also, quantization [10] and sparsiÔ¨Åcation [11]
techniques have been studied to facilitate FedL implementation.
Researchers have also considered the computation aspects of
implementing FedL over wireless networks [6], [7], [12], [13].
Part of this literature has focused on the impact of straggler
nodes, i.e., when some nodes have low computation capability,
on model training [6], [13], [14]. Another emphasis has been
reducing the computation requirements of model training, e.g.,
through coding techniques [12], intelligent data ofÔ¨Çoading
between devices [13], and efÔ¨Åcient device sampling [15].

Other research has focused on extending the star topology of
FedL. Hierarchical FedL has proposed a tree structure between
edge devices and the main server, e.g., [16]. This literature
has mostly focused on speciÔ¨Åc use cases of two-tiered network
structures above wireless cellular devices, e.g., with edge clouds
at base stations. Additionally, there is a literature on fully
decentralized FedL over mesh network architectures without
a centralized server, where device-server communications are
replaced with device-to-device (D2D) communications [17],
[18]. Building upon this, semi-decentralized architectures for
FedL have also been proposed, where D2D communications
are exploited in conjunction with device-server interactions
to improve the model training performance [19], [20]. In this
literature, D2D communications are solely used for distributed
model parameter aggregation across the nodes.

Different from existing works, we propose a new architecture
for dynamic/online distributed ML, which is motivated by the
shortcomings of FedL, discussed next.

B. FedL Shortcomings and Solution Overview

1) Limitations of FedL over heterogeneous wireless networks:
Consider a wireless network consisting of a set of battery
limited devices, e.g., smart phones or sensors. Assume that a

Local DatasetDeviceMain Server‚Ä¶Local LearningWWWùë§1ùë§2ùë§ùëÅW=‚àë!"#$|ùíü!|‚àë!"#$|ùíü"|ùë§ùëóùíü2ùíü1ùíüùëÅ‚Ä¶Global Aggregation 
 
 
 
 
 
base station (BS) aims to train an ML model using the data
gathered by the devices. There are multiple challenges faced
in utilizing conventional FedL in this environment:

1) There might exist some devices with high data quality/quan-
tity that suffer from low computation capability, resulting
in their data being neglected during training.

2) There might exist some devices with high computation
capability suffering from low data quantity/quality, resulting
in idle processing resources.

3) There might exist some devices with high computation
capability and good data quality/quantity, but poor channel
conditions to the BS, making their participation in the model
aggregation step challenging.

4) There might exist some devices with low computation
capability and low data quality/quantity that have good
channel conditions to the BS, which will be unused.

Hence, there are several conditions under which conventional
FedL will result in poor performance. Roughly speaking, FedL
models are biased towards devices with good channel conditions
that have large amounts of data, which might encompass only
a small portion of the overall network.

2) Enabling Data Sharing in FedL: FedL and almost all
the subsequent literature built on top of it to date assume that
the users have strict data privacy concerns and never share
their data. Although this might hold in some applications,
e.g., healthcare systems, there are also applications where the
data privacy may not be strictly regulated, e.g., model training
over sensor networks to detect abrupt environmental changes.
Also, economic incentives (e.g., rewards, gas credit, or cash
back) can also facilitate data sharing in many applications
such as autonomous driving. Further, there exists emerging
research on privacy preserving representation learning, which
aims to obfuscate the sensitive attributes of data, making the
data shareable without privacy concerns [21], [22]. These will
make the data sharing a viable method in the near future.

3) Parallel successive learning: Motivated by the above
challenges, we propose a novel methodology for distributing
ML over heterogeneous wireless networks that leverages the
following properties:

1) Modern wireless devices are capable of device-to-device
(D2D) communications [23]‚Äì[25], which are often consider-
ably low power consuming. D2D communications can also
be carried out in the out-band mode that does not occupy
the licensed spectrum. There exists a literature on D2D
communications for various ad-hoc networks [26]‚Äì[28].
2) Devices with high data quantity/quality and low computation
capability can transfer a portion of their data to those
with better computation resources via D2D communications.
Then, the devices with more computation resources will train
on larger datasets, while the rest train on smaller datasets.
3) Devices with high computation capability and bad channel
conditions to the BS can execute model training and ofÔ¨Çoad
their trained models to those with better channels.

4) Devices with good channel conditions to the BS and low
computation capability can act as aggregators, which receive
models from neighboring devices and conduct a local
aggregation followed by uplink transmission to the BS.

2

The aforementioned concepts constitute the pillars of parallel
successive learning (PSL). PSL is a foremost realization of
fog learning paradigm introduced by us in [24], which enables
both parameter and data ofÔ¨Çoading among the devices.

C. Summary of Contributions

‚Ä¢

Our main contributions can be summarized as follows:
We develop PSL, a distributed ML technique that introduces
a new degree of freedom into model training, which is idle
times in between global aggregations. PSL further extends
FedL in the following three dimensions:
I. Network: PSL considers the local D2D network among
the devices and allows direct device cooperation for
data and parameter dispersion. This migrates away from
the star topology of FedL and paves the road to more
decentralized distributed ML architectures. This approach
is complementary to recent works that utilize D2D in
distributed ML to conduct model consensus [17]‚Äì[20].
II. Heterogeneity: PSL considers and addresses three types

of heterogeneity in wireless distributed ML:

‚Äì Device: PSL assumes different computation and commu-
nication capabilities across devices. This is reÔ¨Çected in
different CPU cycles ranges, chip-set coefÔ¨Åcients, and
transmit powers in D2D vs. uplink transmissions.

‚Äì Learning: PSL adapts device participation in model
training according to their capabilities. In particular, it
considers heterogeneous number of local SGD iterations
with various mini-batch sizes at the devices.

‚Äì Data: PSL considers a dynamic environment with data
arrival and departure at the devices, where the distribution
of the local datasets are non-i.i.d and evolving over time.
We interpret this as the drift of the local loss over time.
III. Proximity: PSL considers D2D and device-to-server
proximities to determine efÔ¨Åcient transfers of both ML
models and data across the network.

‚Ä¢

‚Ä¢

‚Ä¢

We introduce resource pooling, where device resources are
cooperatively orchestrated to facilitate ML model training.
This is realized through a sequence of steps we develop and
analyze: data dispersion, local computation, and model/gra-
dient dispersion with local condensation.
We analytically characterize the convergence behavior of
PSL, through which we quantify the joint effect of (i) SGD
with non-uniform local data sampling at each device, (ii)
non-uniform numbers of local SGD iterations across the
devices, and (iii) dynamic data at the devices captured
via model/concept drift. We leverage this to formulate the
network-aware PSL problem, which optimizes over the
tradeoffs between energy consumption, delay, and model
performance for heterogeneous wireless networks.
We show that network-aware the PSL problem is highly
non-convex and NP-hard. We then propose a tractable
approach based on posynomial approximation and constraint
correction to solve the problem through a sequence of
convex problems, which enjoys convergence guarantees.
The proposed optimization transformation technique, given
the generality of the analysis and the problem formulation,
sheds light on the solution of a broader range of problems
under the umbrella of network-aware distributed learning.

II. SYSTEM MODEL

PSL in a nutshell. PSL is a general distributed ML paradigm,
with its cornerstone built on FedL. It conducts distributed
model training via resource pooling and coordinates device
resources to operate in a cooperative manner. It conducts
each round of model training via Ô¨Åve steps: (i) global model
broadcasting, (ii) data dispersion, (iii) local computation, (iv)
model/gradient dispersion with local condensation, and (v)
global model aggregation, which are illustrated in Fig. 2.

Henceforth, we present the PSL model tracking in Sec. II-A,
describe the data management of PSL in Sec. II-B, and discuss
the local model training of PSL in Sec. II-C. We then introduce
model training phases experienced through PSL in Sec. II-D,
and model the device orchestration in PSL in Sec. II-E.

A. Dynamic/Online Model Tracking Problem in PSL

, N

=

1, 2,
{

We consider a network of N devices

‚àà

N

¬∑ ¬∑ ¬∑

}
coexisting with a server located at a BS, where ML model
training is conducted through a series of global aggregations
N. In contrast to most existing works in
indexed by k
FedL that assume a stationary data distribution, PSL considers
dynamic ML characterized by data variations. In particular, in
PSL, the size and distribution of devices‚Äô datasets are assumed
to be time-varying, i.e., changing across global aggregations.
This is more realistic for real-world applications of distributed
ML. For instance, in online product recommendation systems,
user preferences may change from day to night and from season
to season [29], and in keyword next word prediction, word
choices are affected by trending news [30].

n , which has D(k)
(k)

At global iteration k, each device n

is associated with
‚àà N
(k)
data points. Each
a dataset
n
|
D
contains a feature vector, denoted by d,
data point d
and a label. For example, in image classiÔ¨Åcation, the feature
may be the RGB colors of all pixels in the image, and the
label may be the location where the image was taken.

(k)
n
‚àà D

n (cid:44)

|D

In PSL, the model training is started with an initial global
RM , where M
model broadcast among the nodes, i.e., w(0)
‚àà
(k)
is the model dimension. For each data point d
n , the ML
model is associated with a loss function f (w, d) that quantiÔ¨Åes
RM . We refer to Table 1 in [7]
the error of parameter w
for a list of ML loss functions. For an arbitrary w, during the
(k)
k-th global aggregation, let F (k)
n ) denote the
f (w, d)/D(k)
local loss at node n, Fn(w
n .
Then, the global loss of the ML model is given by
n F (k)
(k)) =

n (w) (cid:44) Fn(w

F (k)(w) (cid:44) F (w

n (w),

(k)
n ) =

D(k)

|D
(k)
n

‚àà D

(1)

(cid:80)

|D

‚ààD

‚àà

d

|

(k)

D

|D

(cid:88)n
‚ààN
where D(k) (cid:44)
(k), the collection
is the cardinality of
of data points across devices. We model the evolution of data
through a new deÔ¨Ånition of model drift in Sec. III (DeÔ¨Ånition 2).
Due to the temporal variations in the distributions of local
datasets, the optimal global model is time varying. In particular,
for a training duration of K global iterations, the optimal global
models can be represented as a sequence (cid:8)w(k)(cid:63) (cid:9)K
k=1, where
(2)
= arg min
RM
w

F (k)(w),

w(k)(cid:63)

k,

‚àÄ

which may not be unique in the case of loss functions which
are not strongly convex, e.g., neural networks.

‚àà

1
D(k)

|D

3

Fig. 2: A schematic of the learning architecture of PSL with Ô¨Åve steps: (i)
global model broadcasting, where the devices receive the global parameter
from the server, (ii) data dispersion, where the devices conduct partial dataset
ofÔ¨Çoading, (iii) local computation, where the devices compute their local
models, (iv) model/gradient dispersion with local condensation, where devices
conduct partial model/gradient transfer and perform local aggregations, (v)
uplink transmission, where some devices transmit their models to the BS.
B. Data Heterogeneity and Management in PSL

n,j |. We also let (cid:101)œÉ(k)
d/S(k)

We propose partitioning the dataset of each device into a
disjoint set of sub-datasets, called stratum (see Fig. 3).1 At
global iteration k, we assume that dataset of device n consists of
set S (k)
n | strata, each with size
S(k)
1
n,j
(k)
n,j ‚àí1

n,2, ¬∑ ¬∑ ¬∑ } of S(k)
(cid:114)
n,j =

n (cid:44) {S (k)
(cid:44) |S (k)

(cid:107)d ‚àí (cid:101)¬µ(k)

n (cid:44) |S (k)

n,1, S (k)

n,j(cid:107)2

(k)
n,j

(cid:80)

d‚ààS

S

2

¬µ(k)
n,j =

d

(cid:101)

‚ààS

(cid:80)

(k)
n,j

and

n,j denote the sample (total) standard
(k)
deviation and the mean of data inside stratum
n,j . We
S
will exploit the means of the strata for data management as
explained in the following, and the variance for optimal local
data sampling for SGD in Sec. III (Proposition 1). We assume
that data points in each stratum possess the same label.

This data partitioning has three advantages: (i) it provides
effective data management upon data arrival/departure, (ii) it
leads to tractable techniques to track the local dataset evolution,
(iii) it opens the door to effective non-uniform data sampling to
reduce the noise of SGD. We describe the Ô¨Årst two advantages
below and defer the explanation of the third one to Sec. II-C.
PSL considers two types of data movement: (i) inter-node
data arrival/departure and (ii) intra-node data arrival/departure.
In case (i), the arrival/departure of data at the devices is due
to local data ofÔ¨Çoading, facilitated by device-to-device (D2D)
communications. In case (ii), arrival/departure is caused by
devices collecting and abandoning data. When new data arrives,
the device has control on how to assign it to the existing strata.
Also, in case (i), the devices have control on which data points
from which strata to transfer (see Fig. 3).

Data arrival. If data points with new labels arrive, the
device will form new strata and assign the respective data
points to them. Otherwise, the device assigns each arriving
data point to the stratum with the closest mean among those
with the same label. In particular, given the current means of
the strata at device n, (cid:101)¬µ(k)
j, the arriving data point d, with
n,j,
feature vector d, gets assigned to stratum S (k)
n,j(cid:63) =
n,j ‚àíd(cid:107). This promotes homogeneity among
arg min
the data points within each stratum. After the size of a stratum
reaches a predeÔ¨Åned maximum size smax, the stratum is further
partitioned into two strata with equal sizes.2 In addition, if the
size of the stratum falls bellow a threshold smin, the stratum

n,j(cid:63) , where S (k)

(cid:107)(cid:101)¬µ(k)

(k)
n,j ‚ààS

(k)
n

‚àÄ

S

1We use ‚Äústratum‚Äù as singular form and ‚Äústrata‚Äù as plural form.
2smax is assumed to be an even number without loss of generality.

Global Model  BroadcastingData DispersionLocal ComputationModel Dispersion &LocalCondensationUplink TransmissionMain ServerMain ServerMain ServerMain ServerMain Serveris assumed to merged with the strata containing data with the
data points with the same label if such strata exists.

4

Data ofÔ¨Çoading. When a device ofÔ¨Çoads data, we can
imagine two potential strategies for data point selection: (i)
choosing from strata with higher variances, which results in
a smaller sampling error for SGD, and (ii) choosing from
strata with more datapoints, which reduces local model bias.
Since data across the devices is non-iid in PSL, strategy (i)
can increase the divergence across local datasets, which can
signiÔ¨Åcantly reduce the performance of the global model. We
thus rely on strategy (ii), where for ofÔ¨Çoading a data point,
(k)
device n chooses the stratum
n,j(cid:63) with largest size and
S
ofÔ¨Çoads the data point d that has the closest distance to the
mean of strata, i.e., d = arg mind
. In this
(cid:107)
way, we minimize the impact an ofÔ¨Çoading device experiences
on its local data distribution.
Remark 1. In general, the optimal data ofÔ¨Çoading strategy
may be a hybrid of the two mentioned strategies, since the
impact of data ofÔ¨Çoading on the divergence of the local models
from the global model is difÔ¨Åcult to quantify in environments
with unknown local data distributions. In this work, we propose
the Ô¨Årst steps towards smart data management and leave further
investigations to future work.

¬µ(k)
n,j ‚àí

(k)
n,j(cid:63) (cid:107)

‚ààS

d

(cid:101)

Tracking of local data statistics. To cope with the dynamics
of local datasets from data arrival/departure, we exploit an
online data statistics tracking technique at each device. We
assume that each device has computed or otherwise gained
knowledge of the initial mean and variance of each of its
local data strata. Upon each data point arrival/departure, the
node actively tracks the mean and variance of its strata as
described by the following lemma, which eliminates the need
for recomputing these quantities from scratch.
Lemma 1 (Online Tracking of Strata Mean and Variance). Let
(vector) data points with mean ¬µold and
denote a set of new datapoints
and œÉ2
,
with mean and variance of ¬µ
A
denote a set of datapoints departing from
, respectively. Then,

denote a set of
|S|
S
sample variance œÉ2
old. Let
that are added to
respectively, and

S
D
with mean and variance of ¬µ

A

A

S
D
the new mean and variance of
S
¬µold +
|A|
+

¬µnew = |S|

and œÉ2
D
are given by
¬µ

¬µ

A ‚àí |D|

D

,

(3)

|S|

old + (
|A| ‚àí
+

|A| ‚àí |D|
1)œÉ2
(
A ‚àí
|D| ‚àí
1
|S| ‚àí |D| ‚àí

1)œÉ2

D

¬µ

2
A (cid:107)
+

‚àí

|A|

(cid:16)
|S| ‚àí |D| ‚àí

|S||D|
+

|S|‚àí|D|
1

¬µold ‚àí
(cid:107)
(cid:17)

œÉ2
new =

(
|S| ‚àí

1)œÉ2

|A||S|
+

|A|

|S|‚àí|D|

+

(cid:16)

|A|

|A||D|
+

|S|‚àí|D|
+

|A|
¬µold ‚àí
(cid:107)
(cid:17)
|A|
¬µA ‚àí
(cid:107)
|S| ‚àí |D| ‚àí

(cid:17)

¬µ

2
D (cid:107)
1

.

‚àí (cid:16)
Proof. The proof is provided in Appendix A.

|A|

¬µ

2
D (cid:107)

(4)

(cid:4)

C. Local Data Sampling and Model Training Iterations

As compared to SGD with uniform sampling, commonly
used in FedL literature, we exploit SGD with non-uniform
sampling. Our technique, inspired by stratiÔ¨Åed sampling in
statistics [31], is advantageous to uniform sampling techniques
when the distribution of data in each stratum is homogeneous

Fig. 3: A schematic of the data stratiÔ¨Åcation for PSL. The Ô¨Ågure represents
the dataset at a device. The device faces two dilemmas: (i) how to assign the
arriving data to the strata, and (ii) which data points to choose to ofÔ¨Çoad.

while between strata is heterogeneous. The initial allocation of
data points across the strata in each device at the beginning of
training can be conducted as in centralized SGD [32]; we are
focused on the beneÔ¨Åts of this technique to distributed ML.

To solve (2) in a distributed manner, within each global
training round, devices conduct local model training through
successive mini-batch SGD updates. However, device hetero-
geneity leads to varying contributions to model training. PSL
assumes that devices utilize SGD with (i) different numbers
of local iterations, (ii) different mini-batch sizes, and (iii) non-
uniform data sampling. Formally, at global iteration k, device
n performs e(k)
n iterations of SGD over its local dataset. The
evolution of local model parameters is then given by

w(k),e

n = w(k),e
‚àí

n

1

Œ∑k
D(k)
n

‚àí

S(k)
n

S(k)
n,j ‚àá

j=1
(cid:88)

(k),e
n,j

(cid:88)d
‚ààB

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

, (5)

, e(k)
n

}

B

1,

‚àà {

(k),e
n,j

where e
denotes the local iteration index, Œ∑k
¬∑ ¬∑ ¬∑
denotes the step-size, and w(k),0
= w(k) is the previously
i
received global parameter from the BS. The nested sum in (5)
indicates the overall gradient is computed by calculating the
denotes
gradient over samples from strata. In particular,
(k)
n,j .
the data batch, sampled at the e-th iteration from stratum
S
We conduct data sampling uniformly at random within each
stratum, with the number of samples collected varying by
strata, leading to an overall non-uniform sampling procedure
(see Proposition 1). We assume that the number of sampled
data points from each stratum does not change during each
training interval, i.e., B(k)
e. We also deÔ¨Åne
,
|
S(k)
n,j as the SGD mini-batch size. Both B(k)
j=1 B(k)
B(k)
n =
n
and B(k)
n,j are design variables that should be tuned according to
device capability and dataset heterogeneity, discussed in Sec. III
and IV. Furthermore, the bounds in Sec. III and the subsequent
optimization problem in Sec. IV will differentiate between
conducting several SGD iterations with small mini-batches vs.
a single SGD with a large mini-batch, and further reveal the
advantage of conducting multiple local SGD iterations.

n,j =

(k),e
n,j

(cid:80)

|B

‚àÄ

n

After each device n performs its last iteration of local
n , it computes the accumulated gradient
(cid:1)(cid:14)Œ∑k , ofÔ¨Çoaded either to its neighbors

model training, i.e., e(k)
n = (cid:0)w(k) ‚àíw(k),e
‚àáF
or to the BS. The global update is carried out at the BS as

(k)
n

(k)

n

w(k+1) = w(k)

(6)
Œ∑k ‚àá
(k) is the normalized accumulated gradient of the

‚àí

F

,

(k)

where ‚àáF

..................‚Ä¶‚Ä¶‚Ä¶·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†Å·†ÅData Arrival??Data Offloading??DatasetStratum·†Å??Datapoint AssignmentDatapoint Selectiondevices, factoring in the heterogeneous number of local SGD
iterations [33], [34], given by

(k)

=

F

‚àá

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

D(k)
n
D(k)e(k)

n ‚àá

F

(k)
n .

(7)

(cid:88)n(cid:48)‚ààN

(cid:88)n
‚ààN

Global model w(k+1) will be then used to synchronize the
devices for the next round of local updates. The process of
recovering
at the server through gradient dispersion and
local condensation among the devices is discussed in Sec. II-E3.

‚àá

(k)

F

D. Dynamic Model Training with Idle Times: ‚ÄúCold‚Äù vs.
‚ÄúWarmed up‚Äù Model, and Model ‚ÄúInertia‚Äù

}

‚àà

‚àí

Z+

0
‚à™{

Many distributed ML applications (e.g., keyboard next word
prediction or online cloth recommendation system) call for
model training to be executed across long time periods (e.g.,
multiple weeks or seasons). In such settings, it is unrealistic to
assume that the global aggregations are conducted continuously
back to back, where the devices are always engaged in
local model training, since this will require prohibitively high
resource consumption. Thus, as compared to current art, PSL
further introduces a new design parameter ‚Ñ¶(k)
that
captures the idle time between the end of global aggregation
1 and the beginning of k, during which the devices are not
k
engaged in model training (‚Ñ¶(0) (cid:44) 0). This parameter captures
the frequency of engagement of the devices in model training.
Initially, when the global model is launched from a ‚Äúcold
start‚Äù, i.e., it is not well-trained, conducting model training
results in signiÔ¨Åcant improvements in model performance,
calling for rapid global training rounds, i.e., small ‚Ñ¶(k)-s.
After several global training rounds, the model training at the
devices starts with a ‚Äúwarmed up‚Äù model, which marginalizes
the reward in terms of model performance gains. In this regime,
if the data at the devices changes rapidly, to track the changes,
fast global rounds are required; otherwise, model training can
be delayed, i.e., large ‚Ñ¶(k)-s, to save energy and network
resources. In particular, given a warmed up global model, the
model training should be triggered when sufÔ¨Åcient changes in
the local data distributions is occurred. We call this phenomenon
the inertia of the global model, since it resembles the same
notion in physics. Initially, the model has a lower inertia. During
model training, the inertia of the global model increases (i.e.,
it becomes reluctant to changes) and it takes large shifts in
the data distribution to trigger a new model training round. A
key contribution of our work is in characterizing this notion
of inertia in distributed ML.

E. Cooperative Resource Pooling in PSL

In PSL, the resource of the devices are utilized cooperatively,
where both data and model parameters can be transferred
in D2D mode, to improve model training. Optimizing these
transfers requires consideration of their resource requirements.
, we let h(k)
n
‚àà N
denote the channel gain of the device to the BS. Consequently,
the data rate of device n to the BS is given by3

At global aggregation k, for each device n

n = BU log
r(k)

1 +

h(k)
n |
|

2pU
n

N U

,

(8)

(cid:16)
3log(.) denotes logarithm with base 2 unless otherwise stated.

(cid:17)

(cid:14)

5

where BU denotes the uplink bandwidth given to each device,
pU
n is the uplink transmit power of the device, and N U = N0BU
is the noise power with N0 denoting the noise spectral density.
Similarly, with D2D communications, for two devices
, we deÔ¨Åne the data rate at device m achieved

n, m
via transmission from device n in D2D mode as follows:

‚àà N

n,m = BD log
r(k)

1 +

h(k)
n,m|
|

2pD
n

N D

,

(9)

(cid:17)

(cid:16)

(cid:14)

where BD denotes the D2D bandwidth given to each user pair,
h(k)
n,m denotes the channel gain among the respective nodes, pD
n
denotes the D2D transmit power of device n, and N D = N0BD.
Remark 2. Since the physical layer details are abstracted
in this paper, for simplicity, it is assumed that the devices
utilize a multi-access protocol such as OFDMA [35] that avoids
interference of the devices in uplink transmissions. The same
holds for D2D communications. Investigating the effect of
interference is left as a topic for future works.

We next formalize the data dispersion, local computations,
and local parameter aggregations with local model condensation
processes in PSL visualized in Fig. 2:

n,m ‚àà [0, 1], where (cid:80)

1) Data Dispersion: In PSL, each device can ofÔ¨Çoad a
portion of its data in D2D mode. We deÔ¨Åne a continuous
variable (cid:37)(k)
n, k, as the
fraction of data of node n ofÔ¨Çoaded to node m at global
iteration k, with œÅ(k)
n,n being the amount of data kept locally.
We refer to (cid:37)(k) = (cid:2)(cid:37)(k)
1‚â§n,m‚â§N as the data dispersion matrix.
The data reception time at device m from n is thus given by

m‚ààN (cid:37)(k)

n,m = 1,

n,m

‚àÄ

(cid:3)

n,m = (cid:37)(k)
T DR,(k)

n,mD(k)
n
where
Œ≤ denotes the number of bits representing one data point.
Assuming data dispersion occurs in parallel (see Remark 2),
the total data reception time at node m is

r(k)
n,m,

(10)

(cid:14)

Œ≤

(cid:98)

(cid:98)

T DR,(k)
n,m

.

(11)

T DR,(k)
m

= max
‚ààN \{

n

(cid:111)

} (cid:110)

m
2) Local Computation: We assume that the devices are
equipped with different processing units. Let us deÔ¨Åne an as
the number of CPU cycles that are used to process one data
sample at device n. Subsequently, the computation time at
device n to execute e(k)
n local SGD iterations based on (5) at
global iteration k is given by
= e(k)

(12)

n anB(k)
n

T C,(k)
n

f (k)
n ,

m,nD(k)
(cid:37)(k)
m .

m

where f (k)
mini-batch size satisÔ¨Åes B(k)
n

n

denotes the CPU frequency of the device, and the
(cid:14)

‚ààN

(cid:80)

‚â§
3) Model Parameter/Gradient Dispersion and Local Conden-
sation: PSL introduces a scenario in which after performing
local computations, each device can partition its vector of
local model/accumulated gradient into different chunks and
disperse the chunks among the neighboring devices in D2D
mode, which we call model/gradient dispersion (see Fig. 4).
Considering the update rule in (6)&(7), at each device n, this
can be carried out via either dispersing the (normalized) latest
local model D
n , or via dispersing the (normalized

w(k),e

(k)

(k)
n
D(k)e

(k)
n

(k)
n
D(k)e

(k)
accumulated) gradient D
n , both of which are vectors
‚àáF
with size M . In the following, we focus on the latter approach
without loss of generality. In this work, we consider that in each
aggregation period, a device either (i) completely disperses its

(k)
n

6

Fig. 5: A schematic of PSL operations during global round k. There is an
idle time in between global aggregations. During the device acquisition time,
a series of synchronization signals are broadcast by the BS.

EC,(k)
n

= (Œ±n/2)ane(k)

EGD,(k)
n

=

2

f (k)
n B(k)
n
n
(cid:17)
nœï(k)
pD
n,mM Œ≤/r(k)
n,m,

(cid:16)

,

(18)

(19)

‚àà N

= pU

EGT,(k)
n

}
n,n/r(k)
n .

(cid:88)m
n
‚ààN \{
nM Œ≤œï(k)
In (18), Œ±n/2 is the effective chipset capacitance of n

(20)
[6].
5) PSL under Synchronization Signaling: To synchronize the
processes involved in PSL, we assume that the BS ochestrates
the devices through a sequence of signals. In particular,
the BS starts each global training round via broadcasting
signal SD, which begins the data dispersion phase among
the devices. Afterward, the BS dictates the start of local
model training via broadcasting signal SL. Then, it starts the
model/gradient dispersion phase with a signal SM. Finally, the
global aggregation round ends with the BS broadcasting a
signal SU, at which time the devices start uplink transmissions.
Let T D,(k), T L,(k), T M,(k), T U,(k) denote the corresponding
time interval associated with data dispersion, local model
training, model/gradient dispersion, and uplink transmissions,
respectively. The total delay of global training round k is
T Tot,(k) = T D,(k) + T L,(k) + T M,(k) + T U,(k), which we refer
to as the device acquisition time. Fig. 5 depicts a schematic of
the timeline of PSL during a global training round.
Remark 4. PSL can be extended to an asynchronous regime,
which is left as future work. For example, devices which Ô¨Ånish
the data dispersion faster can start the local computation and
upload their resulting models/gradients to neighboring devices,
while some others are still conducting data dispersion.

III. CONVERGENCE ANALYSIS OF PSL

We make two standard assumptions to conduct convergence
analysis. Henceforth, notation
.
(cid:107)
(cid:107)
Assumption 1 (Smoothness of the Global and Local Loss
Functions). Local loss function F (k)
, k:
‚àà N
RM,(23)
F (k)
Œ≤
(cid:107)‚àá
which implies the Œ≤-smoothness of the global loss function F .

denotes the 2-norm.

‚àÄ
w, w(cid:48)

is Œ≤-smooth,

n (w(cid:48))

n (w)

w
(cid:107)

F (k)

‚àí ‚àá

(cid:107) ‚â§

,
(cid:107)

w(cid:48)

‚àí

‚àà

n

‚àÄ

n

Assumption 2 (Bounded Dissimilarity of Local Loss Func-
tions). There exist Ô¨Ånite constants Œ∂1 ‚â•
0, for any set
an = 1, such that
of coefÔ¨Åcients
0
an ‚â•
}n
{
‚ààN
2
F (k)
Œ∂1
n (cid:107)

n
‚ààN
F (k)
an‚àá
(cid:80)
n

(cid:88)n
‚ààN
We then deÔ¨Åne local data variability to further measure the

(cid:13)
(cid:13)
(cid:13)
level of heterogeneity at the devices‚Äô local datasets:

1, Œ∂2 ‚â•

an(cid:107)‚àá

, where

+ Œ∂2,

(cid:88)n
‚ààN

(24)

(cid:13)
(cid:13)
(cid:13)

k.

‚â§

‚àÄ

2

DeÔ¨Ånition 1 (Local Data Variability). The local data variability
w, k satisÔ¨Åes
at each device n is measured via Œòn ‚â•
(k)
n .(25)
d
Œòn(cid:107)
‚àí
(cid:107)‚àá

fn(w, d(cid:48))

fn(w, d)

‚àÄ
d, d(cid:48)

0, which

‚àí ‚àá

‚ààD

(cid:107) ‚â§

d(cid:48)

‚àÄ

(cid:107)

,

Fig. 4: A schematic of gradient dispersion with local condensation for a network
(k)
of 5 nodes. Each node n computes
n (with size M ), and normalizes it
F
‚àá
with respect to its number of data points and SGD iterations. Nodes 4 and
5 partition the resulting vector to multiple chunks and disperse the chunks
across their neighbors. The recipients sum the received vectors with their own
vectors and transmit the results (vector of size M ) to the main server, which
computes

(k) (see (7)) and conducts global aggregation (see (6)).

F

‚àá

local gradient across its neighbors in D2D mode or (ii) keeps
its local gradient, receives gradient chucks from its neighbors,
and engages in uplink transmission.
Remark 3. One can imagine a hybrid framework where each
device disperses a fraction of its model/gradient parameters in
D2D mode and uploads the remaining portion to the BS. In this
regime, to reduce the communication burden, the dispersion of
chunks to different neighbors should be conducted to maximize
the overlap between the chunks that a device receives and
those it retains locally, studying of which is left to future work.
To alleviate uplink transmission of multiple gradients, when a
device receives gradient chunks, it conducts a local aggregation,
i.e., summing the received chunks with its local gradient,
and only sends the resulting vector to the BS. We call this
local model/gradient condensation since it results in uplink
transmission of a vector with size M at each node regardless of
the number of received chunks from its neighbors (see Fig. 4).
To model this process, we deÔ¨Åne a continuous variable
œï(k)
n,m ‚àà [0, 1] to denote the fraction of gradient parameters (i.e.,
the fraction of indices of the gradient vector) ofÔ¨Çoaded from
device n to m during global iteration k, where (cid:80)
n,m = 1,
n, k. If node n does not share its gradient with any neighbor,
‚àÄ
œï(k)
n,n = 1. We deÔ¨Åne œï(k) = [œï(k)
n,m]1‚â§n,m‚â§N as the gradient
dispersion matrix. The time required for the reception of
gradient elements at node m from node n is given by
n,m = œï(k)
T RG,(k)

(13)
where Œ≤ denotes the number of bits required to represent one
element of the gradient vector with size M . Since the reception
of gradient occurs in a parallel manner, the total time required
for the reception of gradient parameters at node m is given by

n,mM Œ≤/r(k)
n,m,

m‚ààN œï(k)

= max
‚ààN \{
Finally, the uplink transmission time at node n is given by

(cid:111)

m

n

.

T RG,(k)
m

T RG,(k)
n,m

(14)

T GT,(k)
n

(15)
As explained earlier, each node either disperses its gradient or
keeps it entirely local, and thus œï(k)
n.
Thus, (15) can be expressed as T GT,(k)

n,n is a binary variable,
= M Œ≤œï(k)
n

n,n/r(k)
n .

‚àÄ

} (cid:110)
= M Œ≤/r(k)
n .

4) Energy Consumption: At global iteration k, the total

at each device n is given by

energy consumption E(k)
n
E(k)
+ EC,(k)
n = EDD,(k)
n
n
where EDD,(k)
, EGD,(k)
, EC,(k)
denote the energy
n
n
used for data dispersion, local computation, gradient dispersion,
and gradient transmission to the BS, given as follows:

+ EGD,(k)
n
, EGT,(k)
n

+ EGT,(k)
n

(16)

n

,

EDD,(k)

n

=

n(cid:37)(k)
pD

n,mD(k)
n

Œ≤/r(k)

n,m,

(17)

(cid:88)m
‚ààN \{

n

}

(cid:98)

+++++12345'(k)4,2M<latexit sha1_base64="Ksl3Mqm14BTr2wQs1VAGtOmpXdU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSlJKeiy6MaNUME+oI1hMp20QyeTMDMp1BD8FTcuFHHrf7jzb5w+Ftp64MLhnHu59x4/ZlQq2/42ciura+sb+c3C1vbO7p65f9CUUSIwaeCIRaLtI0kY5aShqGKkHQuCQp+Rlj+8nvitERGSRvxejWPihqjPaUAxUlryzKPuCIl4QB/S0vAs89LqeSW79cyiXbansJaJMydFmKPumV/dXoSTkHCFGZKy49ixclMkFMWMZIVuIkmM8BD1SUdTjkIi3XR6fWadaqVnBZHQxZU1VX9PpCiUchz6ujNEaiAXvYn4n9dJVHDpppTHiSIczxYFCbNUZE2isHpUEKzYWBOEBdW3WniABMJKB1bQITiLLy+TZqXs2GXnrlqsXc3jyMMxnEAJHLiAGtxAHRqA4RGe4RXejCfjxXg3PmatOWM+cwh/YHz+AJJIlKE=</latexit><latexit sha1_base64="Ksl3Mqm14BTr2wQs1VAGtOmpXdU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSlJKeiy6MaNUME+oI1hMp20QyeTMDMp1BD8FTcuFHHrf7jzb5w+Ftp64MLhnHu59x4/ZlQq2/42ciura+sb+c3C1vbO7p65f9CUUSIwaeCIRaLtI0kY5aShqGKkHQuCQp+Rlj+8nvitERGSRvxejWPihqjPaUAxUlryzKPuCIl4QB/S0vAs89LqeSW79cyiXbansJaJMydFmKPumV/dXoSTkHCFGZKy49ixclMkFMWMZIVuIkmM8BD1SUdTjkIi3XR6fWadaqVnBZHQxZU1VX9PpCiUchz6ujNEaiAXvYn4n9dJVHDpppTHiSIczxYFCbNUZE2isHpUEKzYWBOEBdW3WniABMJKB1bQITiLLy+TZqXs2GXnrlqsXc3jyMMxnEAJHLiAGtxAHRqA4RGe4RXejCfjxXg3PmatOWM+cwh/YHz+AJJIlKE=</latexit><latexit sha1_base64="Ksl3Mqm14BTr2wQs1VAGtOmpXdU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSlJKeiy6MaNUME+oI1hMp20QyeTMDMp1BD8FTcuFHHrf7jzb5w+Ftp64MLhnHu59x4/ZlQq2/42ciura+sb+c3C1vbO7p65f9CUUSIwaeCIRaLtI0kY5aShqGKkHQuCQp+Rlj+8nvitERGSRvxejWPihqjPaUAxUlryzKPuCIl4QB/S0vAs89LqeSW79cyiXbansJaJMydFmKPumV/dXoSTkHCFGZKy49ixclMkFMWMZIVuIkmM8BD1SUdTjkIi3XR6fWadaqVnBZHQxZU1VX9PpCiUchz6ujNEaiAXvYn4n9dJVHDpppTHiSIczxYFCbNUZE2isHpUEKzYWBOEBdW3WniABMJKB1bQITiLLy+TZqXs2GXnrlqsXc3jyMMxnEAJHLiAGtxAHRqA4RGe4RXejCfjxXg3PmatOWM+cwh/YHz+AJJIlKE=</latexit>'(k)4,1M<latexit sha1_base64="b5RtEDtI+/DGOcJuYSy1bcyk8JU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISKeiy6MaNUME+oI1hMp20QycPZiaFGoK/4saFIm79D3f+jdM2C60euHA4517uvceLOZPKsr6MwtLyyupacb20sbm1vWPu7rVklAhCmyTikeh4WFLOQtpUTHHaiQXFgcdp2xtdTf32mArJovBOTWLqBHgQMp8RrLTkmge9MRbxkN2nldFJ5qa1Uzu7cc2yVbVmQH+JnZMy5Gi45mevH5EkoKEiHEvZta1YOSkWihFOs1IvkTTGZIQHtKtpiAMqnXR2fYaOtdJHfiR0hQrN1J8TKQ6knASe7gywGspFbyr+53UT5V84KQvjRNGQzBf5CUcqQtMoUJ8JShSfaIKJYPpWRIZYYKJ0YCUdgr348l/SOqvaVtW+rZXrl3kcRTiEI6iADedQh2toQBMIPMATvMCr8Wg8G2/G+7y1YOQz+/ALxsc3kMKUoA==</latexit><latexit sha1_base64="b5RtEDtI+/DGOcJuYSy1bcyk8JU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISKeiy6MaNUME+oI1hMp20QycPZiaFGoK/4saFIm79D3f+jdM2C60euHA4517uvceLOZPKsr6MwtLyyupacb20sbm1vWPu7rVklAhCmyTikeh4WFLOQtpUTHHaiQXFgcdp2xtdTf32mArJovBOTWLqBHgQMp8RrLTkmge9MRbxkN2nldFJ5qa1Uzu7cc2yVbVmQH+JnZMy5Gi45mevH5EkoKEiHEvZta1YOSkWihFOs1IvkTTGZIQHtKtpiAMqnXR2fYaOtdJHfiR0hQrN1J8TKQ6knASe7gywGspFbyr+53UT5V84KQvjRNGQzBf5CUcqQtMoUJ8JShSfaIKJYPpWRIZYYKJ0YCUdgr348l/SOqvaVtW+rZXrl3kcRTiEI6iADedQh2toQBMIPMATvMCr8Wg8G2/G+7y1YOQz+/ALxsc3kMKUoA==</latexit><latexit sha1_base64="b5RtEDtI+/DGOcJuYSy1bcyk8JU=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISKeiy6MaNUME+oI1hMp20QycPZiaFGoK/4saFIm79D3f+jdM2C60euHA4517uvceLOZPKsr6MwtLyyupacb20sbm1vWPu7rVklAhCmyTikeh4WFLOQtpUTHHaiQXFgcdp2xtdTf32mArJovBOTWLqBHgQMp8RrLTkmge9MRbxkN2nldFJ5qa1Uzu7cc2yVbVmQH+JnZMy5Gi45mevH5EkoKEiHEvZta1YOSkWihFOs1IvkTTGZIQHtKtpiAMqnXR2fYaOtdJHfiR0hQrN1J8TKQ6knASe7gywGspFbyr+53UT5V84KQvjRNGQzBf5CUcqQtMoUJ8JShSfaIKJYPpWRIZYYKJ0YCUdgr348l/SOqvaVtW+rZXrl3kcRTiEI6iADedQh2toQBMIPMATvMCr8Wg8G2/G+7y1YOQz+/ALxsc3kMKUoA==</latexit>'(k)5,3M<latexit sha1_base64="7xsVZub3b60xu+8wT/AXDhINOZ4=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSmJD3RZdONGqGAf0MYymU7aoZNJmJkUagj+ihsXirj1P9z5N07bLLT1wIXDOfdy7z1exKhUtv1t5BYWl5ZX8quFtfWNzS1ze6cuw1hgUsMhC0XTQ5IwyklNUcVIMxIEBR4jDW9wPfYbQyIkDfm9GkXEDVCPU59ipLTUMffaQySiPn1ISoOjtJOcH5+mtx2zaJftCax54mSkCBmqHfOr3Q1xHBCuMENSthw7Um6ChKKYkbTQjiWJEB6gHmlpylFApJtMrk+tQ610LT8UuriyJurviQQFUo4CT3cGSPXlrDcW//NasfIv3YTyKFaE4+kiP2aWCq1xFFaXCoIVG2mCsKD6Vgv3kUBY6cAKOgRn9uV5Uj8pO3bZuTsrVq6yOPKwDwdQAgcuoAI3UIUaYHiEZ3iFN+PJeDHejY9pa87IZnbhD4zPH5VWlKM=</latexit><latexit sha1_base64="7xsVZub3b60xu+8wT/AXDhINOZ4=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSmJD3RZdONGqGAf0MYymU7aoZNJmJkUagj+ihsXirj1P9z5N07bLLT1wIXDOfdy7z1exKhUtv1t5BYWl5ZX8quFtfWNzS1ze6cuw1hgUsMhC0XTQ5IwyklNUcVIMxIEBR4jDW9wPfYbQyIkDfm9GkXEDVCPU59ipLTUMffaQySiPn1ISoOjtJOcH5+mtx2zaJftCax54mSkCBmqHfOr3Q1xHBCuMENSthw7Um6ChKKYkbTQjiWJEB6gHmlpylFApJtMrk+tQ610LT8UuriyJurviQQFUo4CT3cGSPXlrDcW//NasfIv3YTyKFaE4+kiP2aWCq1xFFaXCoIVG2mCsKD6Vgv3kUBY6cAKOgRn9uV5Uj8pO3bZuTsrVq6yOPKwDwdQAgcuoAI3UIUaYHiEZ3iFN+PJeDHejY9pa87IZnbhD4zPH5VWlKM=</latexit><latexit sha1_base64="7xsVZub3b60xu+8wT/AXDhINOZ4=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyxCBSmJD3RZdONGqGAf0MYymU7aoZNJmJkUagj+ihsXirj1P9z5N07bLLT1wIXDOfdy7z1exKhUtv1t5BYWl5ZX8quFtfWNzS1ze6cuw1hgUsMhC0XTQ5IwyklNUcVIMxIEBR4jDW9wPfYbQyIkDfm9GkXEDVCPU59ipLTUMffaQySiPn1ISoOjtJOcH5+mtx2zaJftCax54mSkCBmqHfOr3Q1xHBCuMENSthw7Um6ChKKYkbTQjiWJEB6gHmlpylFApJtMrk+tQ610LT8UuriyJurviQQFUo4CT3cGSPXlrDcW//NasfIv3YTyKFaE4+kiP2aWCq1xFFaXCoIVG2mCsKD6Vgv3kUBY6cAKOgRn9uV5Uj8pO3bZuTsrVq6yOPKwDwdQAgcuoAI3UIUaYHiEZ3iFN+PJeDHejY9pa87IZnbhD4zPH5VWlKM=</latexit>'(k)5,2M<latexit sha1_base64="G/ot9cC1xZtgSG9PWtBgSdqkN6I=">AAAB/XicbVDJSgNBEK2JW4zbuNy8NAYhgoSZoOgx6MWLEMEskIxDT6eTNOlZ6O4JxGHwV7x4UMSr/+HNv7GTzEETHxQ83quiqp4XcSaVZX0buaXlldW1/HphY3Nre8fc3WvIMBaE1knIQ9HysKScBbSumOK0FQmKfY/Tpje8nvjNERWShcG9GkfU8XE/YD1GsNKSax50RlhEA/aQlIYnqZucn1bSW9csWmVrCrRI7IwUIUPNNb863ZDEPg0U4VjKtm1FykmwUIxwmhY6saQRJkPcp21NA+xT6STT61N0rJUu6oVCV6DQVP09kWBfyrHv6U4fq4Gc9ybif147Vr1LJ2FBFCsakNmiXsyRCtEkCtRlghLFx5pgIpi+FZEBFpgoHVhBh2DPv7xIGpWybZXtu7Ni9SqLIw+HcAQlsOECqnADNagDgUd4hld4M56MF+Pd+Ji15oxsZh/+wPj8AZPQlKI=</latexit><latexit sha1_base64="G/ot9cC1xZtgSG9PWtBgSdqkN6I=">AAAB/XicbVDJSgNBEK2JW4zbuNy8NAYhgoSZoOgx6MWLEMEskIxDT6eTNOlZ6O4JxGHwV7x4UMSr/+HNv7GTzEETHxQ83quiqp4XcSaVZX0buaXlldW1/HphY3Nre8fc3WvIMBaE1knIQ9HysKScBbSumOK0FQmKfY/Tpje8nvjNERWShcG9GkfU8XE/YD1GsNKSax50RlhEA/aQlIYnqZucn1bSW9csWmVrCrRI7IwUIUPNNb863ZDEPg0U4VjKtm1FykmwUIxwmhY6saQRJkPcp21NA+xT6STT61N0rJUu6oVCV6DQVP09kWBfyrHv6U4fq4Gc9ybif147Vr1LJ2FBFCsakNmiXsyRCtEkCtRlghLFx5pgIpi+FZEBFpgoHVhBh2DPv7xIGpWybZXtu7Ni9SqLIw+HcAQlsOECqnADNagDgUd4hld4M56MF+Pd+Ji15oxsZh/+wPj8AZPQlKI=</latexit><latexit sha1_base64="G/ot9cC1xZtgSG9PWtBgSdqkN6I=">AAAB/XicbVDJSgNBEK2JW4zbuNy8NAYhgoSZoOgx6MWLEMEskIxDT6eTNOlZ6O4JxGHwV7x4UMSr/+HNv7GTzEETHxQ83quiqp4XcSaVZX0buaXlldW1/HphY3Nre8fc3WvIMBaE1knIQ9HysKScBbSumOK0FQmKfY/Tpje8nvjNERWShcG9GkfU8XE/YD1GsNKSax50RlhEA/aQlIYnqZucn1bSW9csWmVrCrRI7IwUIUPNNb863ZDEPg0U4VjKtm1FykmwUIxwmhY6saQRJkPcp21NA+xT6STT61N0rJUu6oVCV6DQVP09kWBfyrHv6U4fq4Gc9ybif147Vr1LJ2FBFCsakNmiXsyRCtEkCtRlghLFx5pgIpi+FZEBFpgoHVhBh2DPv7xIGpWybZXtu7Ni9SqLIw+HcAQlsOECqnADNagDgUd4hld4M56MF+Pd+Ji15oxsZh/+wPj8AZPQlKI=</latexit>'(k)5,1M<latexit sha1_base64="BBk4+pq2B5Ty8dLDhwIL6KmAf90=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISUXRZdONGqGAf0MYwmU7aoZMHM5NCDcFfceNCEbf+hzv/xmmbhVYPXDiccy/33uPFnEllWV9GYWFxaXmluFpaW9/Y3DK3d5oySgShDRLxSLQ9LClnIW0opjhtx4LiwOO05Q2vJn5rRIVkUXinxjF1AtwPmc8IVlpyzb3uCIt4wO7TyvAoc9OzYzu7cc2yVbWmQH+JnZMy5Ki75me3F5EkoKEiHEvZsa1YOSkWihFOs1I3kTTGZIj7tKNpiAMqnXR6fYYOtdJDfiR0hQpN1Z8TKQ6kHAee7gywGsh5byL+53US5V84KQvjRNGQzBb5CUcqQpMoUI8JShQfa4KJYPpWRAZYYKJ0YCUdgj3/8l/SPKnaVtW+PS3XLvM4irAPB1ABG86hBtdQhwYQeIAneIFX49F4Nt6M91lrwchnduEXjI9vkkqUoQ==</latexit><latexit sha1_base64="BBk4+pq2B5Ty8dLDhwIL6KmAf90=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISUXRZdONGqGAf0MYwmU7aoZMHM5NCDcFfceNCEbf+hzv/xmmbhVYPXDiccy/33uPFnEllWV9GYWFxaXmluFpaW9/Y3DK3d5oySgShDRLxSLQ9LClnIW0opjhtx4LiwOO05Q2vJn5rRIVkUXinxjF1AtwPmc8IVlpyzb3uCIt4wO7TyvAoc9OzYzu7cc2yVbWmQH+JnZMy5Ki75me3F5EkoKEiHEvZsa1YOSkWihFOs1I3kTTGZIj7tKNpiAMqnXR6fYYOtdJDfiR0hQpN1Z8TKQ6kHAee7gywGsh5byL+53US5V84KQvjRNGQzBb5CUcqQpMoUI8JShQfa4KJYPpWRAZYYKJ0YCUdgj3/8l/SPKnaVtW+PS3XLvM4irAPB1ABG86hBtdQhwYQeIAneIFX49F4Nt6M91lrwchnduEXjI9vkkqUoQ==</latexit><latexit sha1_base64="BBk4+pq2B5Ty8dLDhwIL6KmAf90=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1iEClISUXRZdONGqGAf0MYwmU7aoZMHM5NCDcFfceNCEbf+hzv/xmmbhVYPXDiccy/33uPFnEllWV9GYWFxaXmluFpaW9/Y3DK3d5oySgShDRLxSLQ9LClnIW0opjhtx4LiwOO05Q2vJn5rRIVkUXinxjF1AtwPmc8IVlpyzb3uCIt4wO7TyvAoc9OzYzu7cc2yVbWmQH+JnZMy5Ki75me3F5EkoKEiHEvZsa1YOSkWihFOs1I3kTTGZIj7tKNpiAMqnXR6fYYOtdJDfiR0hQpN1Z8TKQ6kHAee7gywGsh5byL+53US5V84KQvjRNGQzBb5CUcqQpMoUI8JShQfa4KJYPpWRAZYYKJ0YCUdgj3/8l/SPKnaVtW+PS3XLvM4irAPB1ABG86hBtdQhwYQeIAneIFX49F4Nt6M91lrwchnduEXjI9vkkqUoQ==</latexit>|D(k)5|D(k)e(k)5rF(k)5<latexit sha1_base64="YgAgj2or+ToA9+p9+Lhu8YhzzRk=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOKLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa3d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2LncrBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98gbqwy</latexit><latexit sha1_base64="YgAgj2or+ToA9+p9+Lhu8YhzzRk=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOKLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa3d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2LncrBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98gbqwy</latexit><latexit sha1_base64="YgAgj2or+ToA9+p9+Lhu8YhzzRk=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOKLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa3d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2LncrBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98gbqwy</latexit>|D(k)4|D(k)e(k)4rF(k)4<latexit sha1_base64="/bqPri5/6jSZpxQOHZlTdl2FVZs=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiMFXRYVceFCwVqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey6sAFOVM0IZmmtOrRFKIAk6bQe8w95t3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFujrJrs93byTq1QWaORhzTby3zYzstX2Z8AQEHbI4zqxd+ueJW3SHwOPEKUkEFzjrlR78bkzSiQhMOSrU8N9FtA1IzwmlW8lNFEyA9uKEtSwVEVLXN8OoMb1mli8NY2ic0Hqo/OwxESvWjwFbmx6m/Xi7+57VSHe63DRNJqqkgo0VhyrGOcR4h7jJJieZ9S4BIZv+KyS3YGLUNumRD8P6ePE4ud6ueW/XOa5X6QRHHHNpAm2gbeWgP1dEJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8bjKwv</latexit><latexit sha1_base64="/bqPri5/6jSZpxQOHZlTdl2FVZs=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiMFXRYVceFCwVqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey6sAFOVM0IZmmtOrRFKIAk6bQe8w95t3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFujrJrs93byTq1QWaORhzTby3zYzstX2Z8AQEHbI4zqxd+ueJW3SHwOPEKUkEFzjrlR78bkzSiQhMOSrU8N9FtA1IzwmlW8lNFEyA9uKEtSwVEVLXN8OoMb1mli8NY2ic0Hqo/OwxESvWjwFbmx6m/Xi7+57VSHe63DRNJqqkgo0VhyrGOcR4h7jJJieZ9S4BIZv+KyS3YGLUNumRD8P6ePE4ud6ueW/XOa5X6QRHHHNpAm2gbeWgP1dEJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8bjKwv</latexit><latexit sha1_base64="/bqPri5/6jSZpxQOHZlTdl2FVZs=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiMFXRYVceFCwVqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey6sAFOVM0IZmmtOrRFKIAk6bQe8w95t3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFujrJrs93byTq1QWaORhzTby3zYzstX2Z8AQEHbI4zqxd+ueJW3SHwOPEKUkEFzjrlR78bkzSiQhMOSrU8N9FtA1IzwmlW8lNFEyA9uKEtSwVEVLXN8OoMb1mli8NY2ic0Hqo/OwxESvWjwFbmx6m/Xi7+57VSHe63DRNJqqkgo0VhyrGOcR4h7jJJieZ9S4BIZv+KyS3YGLUNumRD8P6ePE4ud6ueW/XOa5X6QRHHHNpAm2gbeWgP1dEJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8bjKwv</latexit>|D(k)3|D(k)e(k)3rF(k)3<latexit sha1_base64="DCrj3v1dhmZ6VdrLwoILZbtOYFQ=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMqOCLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa2d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2L3crBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98Wqqws</latexit><latexit sha1_base64="DCrj3v1dhmZ6VdrLwoILZbtOYFQ=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMqOCLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa2d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2L3crBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98Wqqws</latexit><latexit sha1_base64="DCrj3v1dhmZ6VdrLwoILZbtOYFQ=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMqOCLkVFXLhQsFbo1HInzdTQTGZIMkKJ81Nu/A9XunChiFt/wUw7grYeCBzOuY/cEyScKe26L87Y+MTk1PTMbGlufmFxqby8cqXiVBJaIzGP5XUAinImaE0zzel1IilEAaf1oHuU+/U7KhWLxaXuJbQZQUewkBHQVmqVz/xQAjH3fgT6lgA3x9mN2exuZa2d+8wcDzimP1rmx3Zavsz4AgIO2JxkVi/8csWtun3gUeIVpIIKnLfKT347JmlEhSYclGp4bqKbBqRmhNOs5KeKJkC60KENSwVEVDVN/+oMb1iljcNY2ic07qu/OwxESvWiwFbmx6lhLxf/8xqpDvebhokk1VSQwaIw5VjHOI8Qt5mkRPOeJUAks3/F5BZsjNoGXbIheMMnj5Kr7arnVr2L3crBYRHHDFpD62gTeWgPHaBTdI5qiKAH9Ize0Lvz6Lw6H87noHTMKXpW0R84X98Wqqws</latexit>|D(k)2|D(k)e(k)2rF(k)2<latexit sha1_base64="k5ivnEyVrzMFa4uX4rLGwMv1vGc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZJ3dQWbqI47pt5b5sZ2WLzO+gIADNseZ1Qu/XHGr7hB4nHgFqaACZ53yo9+NSRpRoQkHpVqem+i2AakZ4TQr+amiCZAe3NCWpQIiqtpmeHWGt6zSxWEs7RMaD9WfHQYipfpRYCvz49RfLxf/81qpDg/ahokk1VSQ0aIw5VjHOI8Qd5mkRPO+JUAks3/F5BZsjNoGXbIheH9PHieXu1XPrXrne5XaYRHHHNpAm2gbeWgf1dAJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8RyKwp</latexit><latexit sha1_base64="k5ivnEyVrzMFa4uX4rLGwMv1vGc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZJ3dQWbqI47pt5b5sZ2WLzO+gIADNseZ1Qu/XHGr7hB4nHgFqaACZ53yo9+NSRpRoQkHpVqem+i2AakZ4TQr+amiCZAe3NCWpQIiqtpmeHWGt6zSxWEs7RMaD9WfHQYipfpRYCvz49RfLxf/81qpDg/ahokk1VSQ0aIw5VjHOI8Qd5mkRPO+JUAks3/F5BZsjNoGXbIheH9PHieXu1XPrXrne5XaYRHHHNpAm2gbeWgf1dAJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8RyKwp</latexit><latexit sha1_base64="k5ivnEyVrzMFa4uX4rLGwMv1vGc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZJ3dQWbqI47pt5b5sZ2WLzO+gIADNseZ1Qu/XHGr7hB4nHgFqaACZ53yo9+NSRpRoQkHpVqem+i2AakZ4TQr+amiCZAe3NCWpQIiqtpmeHWGt6zSxWEs7RMaD9WfHQYipfpRYCvz49RfLxf/81qpDg/ahokk1VSQ0aIw5VjHOI8Qd5mkRPO+JUAks3/F5BZsjNoGXbIheH9PHieXu1XPrXrne5XaYRHHHNpAm2gbeWgf1dAJOkMNRNA9ekKv6M15cF6cd+djVDrhFD3r6Beczy8RyKwp</latexit>|D(k)1|D(k)e(k)1rF(k)1<latexit sha1_base64="6aZpJAT2NjF8fhzC9+Lw2AKG+cc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZB1vkJn6iGP6rWV+bKfly4wvIOCAzXFm9cIvV9yqOwQeJ15BKqjAWaf86HdjkkZUaMJBqZbnJrptQGpGOM1KfqpoAqQHN7RlqYCIqrYZXp3hLat0cRhL+4TGQ/Vnh4FIqX4U2Mr8OPXXy8X/vFaqw4O2YSJJNRVktChMOdYxziPEXSYp0bxvCRDJ7F8xuQUbo7ZBl2wI3t+Tx8nlbtVzq975XqV2WMQxhzbQJtpGHtpHNXSCzlADEXSPntArenMenBfn3fkYlU44Rc86+gXn8wsM5qwm</latexit><latexit sha1_base64="6aZpJAT2NjF8fhzC9+Lw2AKG+cc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZB1vkJn6iGP6rWV+bKfly4wvIOCAzXFm9cIvV9yqOwQeJ15BKqjAWaf86HdjkkZUaMJBqZbnJrptQGpGOM1KfqpoAqQHN7RlqYCIqrYZXp3hLat0cRhL+4TGQ/Vnh4FIqX4U2Mr8OPXXy8X/vFaqw4O2YSJJNRVktChMOdYxziPEXSYp0bxvCRDJ7F8xuQUbo7ZBl2wI3t+Tx8nlbtVzq975XqV2WMQxhzbQJtpGHtpHNXSCzlADEXSPntArenMenBfn3fkYlU44Rc86+gXn8wsM5qwm</latexit><latexit sha1_base64="6aZpJAT2NjF8fhzC9+Lw2AKG+cc=">AAACNXicbVDLSgMxFM34tr6qLt0Ei6CbMiOCLosWceFCwWqhU8udNKOhmcyQZISSzk+58T9c6cKFIm79BTPtCGo9EDiccx+5J0g4U9p1n52Jyanpmdm5+dLC4tLySnl17VLFqSS0QWIey2YAinImaEMzzWkzkRSigNOroHeU+1d3VCoWiwvdT2g7ghvBQkZAW6lTPvVDCcQM/Aj0LQFu6tm12e7tZB1vkJn6iGP6rWV+bKfly4wvIOCAzXFm9cIvV9yqOwQeJ15BKqjAWaf86HdjkkZUaMJBqZbnJrptQGpGOM1KfqpoAqQHN7RlqYCIqrYZXp3hLat0cRhL+4TGQ/Vnh4FIqX4U2Mr8OPXXy8X/vFaqw4O2YSJJNRVktChMOdYxziPEXSYp0bxvCRDJ7F8xuQUbo7ZBl2wI3t+Tx8nlbtVzq975XqV2WMQxhzbQJtpGHtpHNXSCzlADEXSPntArenMenBfn3fkYlU44Rc86+gXn8wsM5qwm</latexit>rF(k)=Xn02N|D(k)n0|e(k)n0D(k)Xn2N|D(k)n|D(k)e(k)nrF(k)n<latexit sha1_base64="5wsVTrV6ctsQd4zS7G1/UexnIQI=">AAACunicjVFLaxsxENZu08ZxX25zzEXElLoXs1sK6SEF05rSU3Agjg3erZmVtY6wVrtI2oKR9SObW/5NtPbm6RwyIPjm+2ZG80gKzpQOgivPf7Hz8tVuY6/5+s3bd+9bHz6eq7yUhA5JznM5TkBRzgQdaqY5HReSQpZwOkoWvyp99I9KxXJxppcFjTOYC5YyAtpR09b/KHdylW0iAQkHbH5b+9d0Fl/sj0iV2dSIzxETOMpAXxDg5sTaKJVAzOqW6tcJVaxd0XuONf2Nh20TO6srPrugWN1VuKkr7JM9T0Wtt9pBN1gb3gZhDdqotsG0dRnNclJmVGjCQalJGBQ6NiA1I5zaZlQqWgBZwJxOHBSQURWb9eot/uSYGU5z6Z7QeM3ezzCQKbXMEhdZDaceaxX5lDYpdfo9NkwUpaaCbD5KS451jqs74hmTlGi+dACIZK5XTC7ArVG7azfdEsLHI2+D86/dMOiGp9/avZ/1OhroAB2iDgrREeqhP2iAhoh4R17spd7cP/YTn/mLTajv1Tn76IH5+hrT9ds7</latexit><latexit sha1_base64="5wsVTrV6ctsQd4zS7G1/UexnIQI=">AAACunicjVFLaxsxENZu08ZxX25zzEXElLoXs1sK6SEF05rSU3Agjg3erZmVtY6wVrtI2oKR9SObW/5NtPbm6RwyIPjm+2ZG80gKzpQOgivPf7Hz8tVuY6/5+s3bd+9bHz6eq7yUhA5JznM5TkBRzgQdaqY5HReSQpZwOkoWvyp99I9KxXJxppcFjTOYC5YyAtpR09b/KHdylW0iAQkHbH5b+9d0Fl/sj0iV2dSIzxETOMpAXxDg5sTaKJVAzOqW6tcJVaxd0XuONf2Nh20TO6srPrugWN1VuKkr7JM9T0Wtt9pBN1gb3gZhDdqotsG0dRnNclJmVGjCQalJGBQ6NiA1I5zaZlQqWgBZwJxOHBSQURWb9eot/uSYGU5z6Z7QeM3ezzCQKbXMEhdZDaceaxX5lDYpdfo9NkwUpaaCbD5KS451jqs74hmTlGi+dACIZK5XTC7ArVG7azfdEsLHI2+D86/dMOiGp9/avZ/1OhroAB2iDgrREeqhP2iAhoh4R17spd7cP/YTn/mLTajv1Tn76IH5+hrT9ds7</latexit><latexit sha1_base64="5wsVTrV6ctsQd4zS7G1/UexnIQI=">AAACunicjVFLaxsxENZu08ZxX25zzEXElLoXs1sK6SEF05rSU3Agjg3erZmVtY6wVrtI2oKR9SObW/5NtPbm6RwyIPjm+2ZG80gKzpQOgivPf7Hz8tVuY6/5+s3bd+9bHz6eq7yUhA5JznM5TkBRzgQdaqY5HReSQpZwOkoWvyp99I9KxXJxppcFjTOYC5YyAtpR09b/KHdylW0iAQkHbH5b+9d0Fl/sj0iV2dSIzxETOMpAXxDg5sTaKJVAzOqW6tcJVaxd0XuONf2Nh20TO6srPrugWN1VuKkr7JM9T0Wtt9pBN1gb3gZhDdqotsG0dRnNclJmVGjCQalJGBQ6NiA1I5zaZlQqWgBZwJxOHBSQURWb9eot/uSYGU5z6Z7QeM3ezzCQKbXMEhdZDaceaxX5lDYpdfo9NkwUpaaCbD5KS451jqs74hmTlGi+dACIZK5XTC7ArVG7azfdEsLHI2+D86/dMOiGp9/avZ/1OhroAB2iDgrREeqhP2iAhoh4R17spd7cP/YTn/mLTajv1Tn76IH5+hrT9ds7</latexit>Device Acquisition time:Data DispersionLocal ComputationModel Dispersion &Local CondensationUplinkTransmissionIdle Time<latexit sha1_base64="5fd1FL6LLCb1cC6wxy9wbb24Q0M=">AAAB6HicbVBNS8NAEJ34WetX1aOXxSJ4KomKeix68diC/YA2lM12067dbMLuRCihv8CLB0W8+pO8+W/ctjlo64OBx3szzMwLEikMuu63s7K6tr6xWdgqbu/s7u2XDg6bJk414w0Wy1i3A2q4FIo3UKDk7URzGgWSt4LR3dRvPXFtRKwecJxwP6IDJULBKFqpjr1S2a24M5Bl4uWkDDlqvdJXtx+zNOIKmaTGdDw3QT+jGgWTfFLspoYnlI3ogHcsVTTixs9mh07IqVX6JIy1LYVkpv6eyGhkzDgKbGdEcWgWvan4n9dJMbzxM6GSFLli80VhKgnGZPo16QvNGcqxJZRpYW8lbEg1ZWizKdoQvMWXl0nzvOJdVS7ql+XqbR5HAY7hBM7Ag2uowj3UoAEMODzDK7w5j86L8+58zFtXnHzmCP7A+fwB4sWNAA==</latexit>tEnding of Global Aggregation ùëò‚àí1Beginning of Global Aggregation ùëò<latexit sha1_base64="GMBU0oKPpI8u1cbWwT3ekxwM30I=">AAAB+3icdVDLSgMxFM3UV62vsS7dBItQQcqklD52RTcuXFToC9qxZNJMG5p5kGTEMsyvuHGhiFt/xJ1/Y6atoKIHAodz7uWeHCfkTCrL+jAya+sbm1vZ7dzO7t7+gXmY78ogEoR2SMAD0XewpJz5tKOY4rQfCoo9h9OeM7tM/d4dFZIFflvNQ2p7eOIzlxGstDQy8+3beOhhNZVufJ2cF2dnycgsWCXLshBCMCWoVrU0aTTqZVSHKLU0CmCF1sh8H44DEnnUV4RjKQfICpUdY6EY4TTJDSNJQ0xmeEIHmvrYo9KOF9kTeKqVMXQDoZ+v4EL9vhFjT8q55+jJRczfXir+5Q0i5dbtmPlhpKhPlofciEMVwLQIOGaCEsXnmmAimM4KyRQLTJSuK6dL+Pop/J90yyVULVVuKoXmxaqOLDgGJ6AIEKiBJrgCLdABBNyDB/AEno3EeDRejNflaMZY7RyBHzDePgHTZJRT</latexit>TL,(k)<latexit sha1_base64="LXagkssK9dSoFUdjbwdwk74rqBk=">AAAB+3icdVDLSgMxFM3UV62vsS7dBItQQcpMbW1nV9SFywp9QTuWTJppQzMPkoxYhvkVNy4UceuPuPNvzLQVVPRA4HDOvdyT44SMCmkYH1pmZXVtfSO7mdva3tnd0/fzHRFEHJM2DljAew4ShFGftCWVjPRCTpDnMNJ1ppep370jXNDAb8lZSGwPjX3qUoykkoZ6vnUbDzwkJ8KNr5LT4vQkGeoFo2TVq9WzOjRKhmGVrZoilmWZNROaSklRAEs0h/r7YBTgyCO+xAwJ0TeNUNox4pJiRpLcIBIkRHiKxqSvqI88Iux4nj2Bx0oZQTfg6vkSztXvGzHyhJh5jpqcx/ztpeJfXj+Sbt2OqR9Gkvh4cciNGJQBTIuAI8oJlmymCMKcqqwQTxBHWKq6cqqEr5/C/0mnXDLPS5WbSqFxsawjCw7BESgCE9RAA1yDJmgDDO7BA3gCz1qiPWov2utiNKMtdw7AD2hvnwx1lHo=</latexit>TD,(k)<latexit sha1_base64="+Qg9ZbFjSr7yqTMHDIM0o9v+JuY=">AAAB+3icdVDLSsNAFJ3UV62vWJduBotQQUIi9bEsunEjVOgL2lgm00k7dDIJMxOxhPyKGxeKuPVH3Pk3TtoUVPTAwOGce7lnjhcxKpVtfxqFpeWV1bXiemljc2t7x9wtt2UYC0xaOGSh6HpIEkY5aSmqGOlGgqDAY6TjTa4yv3NPhKQhb6ppRNwAjTj1KUZKSwOz3LxL+gFSY+knN+lxdXKUDsyKbZ3aGaBt2QuSK06uVECOxsD86A9DHAeEK8yQlD3HjpSbIKEoZiQt9WNJIoQnaER6mnIUEOkms+wpPNTKEPqh0I8rOFO/byQokHIaeHpyFvO3l4l/eb1Y+RduQnkUK8Lx/JAfM6hCmBUBh1QQrNhUE4QF1VkhHiOBsNJ1lXQJi5/C/0n7xHLOrNptrVK/zOsogn1wAKrAAeegDq5BA7QABg/gETyDFyM1noxX420+WjDynT3wA8b7F5aKlCg=</latexit>TM,(k)<latexit sha1_base64="38ZKQjS1t9fJ2coPeVIxKAOVd1E=">AAAB+3icdVDLSsNAFJ34rPUV69LNYBEqSEliaOuu6MZlhaYttLFMppN26OTBzEQsIb/ixoUibv0Rd/6Nk7aCih4YOJxzL/fM8WJGhTSMD21ldW19Y7OwVdze2d3b1w9KHRElHBMHRyziPQ8JwmhIHEklI72YExR4jHS96VXud+8IFzQK23IWEzdA45D6FCOppKFeat+mgwDJifBTJzurTE+zoV42qheNmmXXoFE1jLppmTmx6va5DU2l5CiDJVpD/X0winASkFBihoTom0Ys3RRxSTEjWXGQCBIjPEVj0lc0RAERbjrPnsETpYygH3H1Qgnn6veNFAVCzAJPTc5j/vZy8S+vn0i/4aY0jBNJQrw45CcMygjmRcAR5QRLNlMEYU5VVogniCMsVV1FVcLXT+H/pGNVzVrVvrHLzctlHQVwBI5BBZigDprgGrSAAzC4Bw/gCTxrmfaovWivi9EVbblzCH5Ae/sE9FeUaA==</latexit>TU,(k)<latexit sha1_base64="OwljbccwR22yBlQypUYyC6Fn4G0=">AAAB83icdVDLSsNAFL3xWeur6tLNYBHqJiSlVJdFN+6sYB/QxDKZTtqhM0mYmQgl9DfcuFDErT/jzr9x0geo6IHLPZxzL3PnBAlnSjvOp7Wyura+sVnYKm7v7O7tlw4O2ypOJaEtEvNYdgOsKGcRbWmmOe0mkmIRcNoJxle533mgUrE4utOThPoCDyMWMoK1kTzvRtAhvs8q47Npv1R27LqTAzl2bUmqc+Las+6UYYFmv/ThDWKSChppwrFSPddJtJ9hqRnhdFr0UkUTTMZ4SHuGRlhQ5Wezm6fo1CgDFMbSVKTRTP2+kWGh1EQEZlJgPVK/vVz8y+ulOrzwMxYlqaYRmT8UphzpGOUBoAGTlGg+MQQTycytiIywxESbmIomhOVP0f+kXbXdul27rZUbl4s4CnAMJ1ABF86hAdfQhBYQSOARnuHFSq0n69V6m4+uWIudI/gB6/0Lh8yRXg==</latexit>‚å¶(k)Ending of Global Aggregation ùëò‚Ä¶‚Ä¶Synchronization Signals<latexit sha1_base64="9OzxxI8B1fLHWKmgeW4dnvL2hr4=">AAACAHicdVC7SgNBFJ31GeNr1cLCZjAIVsskLkm2C2phGdE8IIlhdjKbDJl9MDMrhGUbf8XGQhFbP8POv3GySUBFD1w4nHMv997jRpxJhdCnsbS8srq2ntvIb25t7+yae/tNGcaC0AYJeSjaLpaUs4A2FFOctiNBse9y2nLHF1O/dU+FZGFwqyYR7fl4GDCPEay01DcPuz5WI+klN+ldsuCXado3C8hyqlUH2RBZNkJ2paTJmeM4ZQSLFspQAHPU++ZHdxCS2KeBIhxL2SmiSPUSLBQjnKb5bixphMkYD2lH0wD7VPaS7IEUnmhlAL1Q6AoUzNTvEwn2pZz4ru7MTvztTcW/vE6svGovYUEUKxqQ2SIv5lCFcJoGHDBBieITTTARTN8KyQgLTJTOLK9DWHwK/yfNklUsW/a1Xaidz+PIgSNwDE5BEVRADVyBOmgAAlLwCJ7Bi/FgPBmvxtusdcmYzxyAHzDevwBVyJeQ</latexit>SD<latexit sha1_base64="s4Su5frFgSZwRk1ANb9zkM/vA8Y=">AAACAHicdVC7SgNBFJ31GeNr1cLCZjAIVsskLkm2C9pYWEQ0D0himJ3MJkNmH8zMCmHZxl+xsVDE1s+w82+cbBJQ0QMXDufcy733uBFnUiH0aSwtr6yurec28ptb2zu75t5+U4axILRBQh6Ktosl5SygDcUUp+1IUOy7nLbc8cXUb91TIVkY3KpJRHs+HgbMYwQrLfXNw66P1Uh6yU16lyz4VZr2zQKynGrVQTZElo2QXSlpcuY4ThnBooUyFMAc9b750R2EJPZpoAjHUnaKKFK9BAvFCKdpvhtLGmEyxkPa0TTAPpW9JHsghSdaGUAvFLoCBTP1+0SCfSknvqs7sxN/e1PxL68TK6/aS1gQxYoGZLbIizlUIZymAQdMUKL4RBNMBNO3QjLCAhOlM8vrEBafwv9Js2QVy5Z9bRdq5/M4cuAIHINTUAQVUAOXoA4agIAUPIJn8GI8GE/Gq/E2a10y5jMH4AeM9y9h+JeY</latexit>SL<latexit sha1_base64="Gukv6tcV0PHL5izG3UC1d6TK2yQ=">AAACAHicdVC7SgNBFJ31GeNr1cLCZjAIVsskLkm2C9rYCBHNA5IYZiezyZDZBzOzQli28VdsLBSx9TPs/BsnmwRU9MCFwzn3cu89bsSZVAh9GkvLK6tr67mN/ObW9s6uubfflGEsCG2QkIei7WJJOQtoQzHFaTsSFPsupy13fDH1W/dUSBYGt2oS0Z6PhwHzGMFKS33zsOtjNZJecpPeJQt+laZ9s4Asp1p1kA2RZSNkV0qanDmOU0awaKEMBTBHvW9+dAchiX0aKMKxlJ0iilQvwUIxwmma78aSRpiM8ZB2NA2wT2UvyR5I4YlWBtALha5AwUz9PpFgX8qJ7+rO7MTf3lT8y+vEyqv2EhZEsaIBmS3yYg5VCKdpwAETlCg+0QQTwfStkIywwETpzPI6hMWn8H/SLFnFsmVf24Xa+TyOHDgCx+AUFEEF1MAlqIMGICAFj+AZvBgPxpPxarzNWpeM+cwB+AHj/QtjfpeZ</latexit>SM<latexit sha1_base64="Yl/Lt3tHNU8YE0QZ2NvuwP0lOyI=">AAACAHicdVC7SgNBFJ2NrxhfUQsLm8EgWC2TuCTZLmhjGdE8IFnD7GQ2GTL7YGZWCMs2/oqNhSK2foadf+Nkk4CKHrhwOOde7r3HjTiTCqFPI7eyura+kd8sbG3v7O4V9w/aMowFoS0S8lB0XSwpZwFtKaY47UaCYt/ltONOLmd+554KycLgVk0j6vh4FDCPEay0NCge9X2sxtJLbtK7ZMlbaToolpBp1+s2siAyLYSsWkWTc9u2qwiWTZShBBZoDoof/WFIYp8GinAsZa+MIuUkWChGOE0L/VjSCJMJHtGepgH2qXSS7IEUnmplCL1Q6AoUzNTvEwn2pZz6ru7MTvztzcS/vF6svLqTsCCKFQ3IfJEXc6hCOEsDDpmgRPGpJpgIpm+FZIwFJkpnVtAhLD+F/5N2xSxXTevaKjUuFnHkwTE4AWegDGqgAa5AE7QAASl4BM/gxXgwnoxX423emjMWM4fgB4z3L2+ul6E=</latexit>SU<latexit sha1_base64="gOmIyaIiyoLykMzvTJdxqgIpM40=">AAAB/XicdVDLSgMxFM3UV62v+ti5CRahggwZO7ZdFt24rNAXtLVk0kwbmnmQZIQ6DP6KGxeKuPU/3Pk3ZtoKKnogcDjnXu7JcULOpELow8gsLa+srmXXcxubW9s7+d29lgwiQWiTBDwQHQdLyplPm4opTjuhoNhzOG07k8vUb99SIVngN9Q0pH0Pj3zmMoKVlgb5g8ZN3POwGks3bgQqOS1OTpJBvoBMZFulyjlEZgmVbDslyK5Y1TK0TDRDASxQH+Tfe8OARB71FeFYyq6FQtWPsVCMcJrkepGkISYTPKJdTX3sUdmPZ+kTeKyVIXQDoZ+v4Ez9vhFjT8qp5+jJWdDfXir+5XUj5Vb7MfPDSFGfzA+5EYcqgGkVcMgEJYpPNcFEMJ0VkjEWmChdWE6X8PVT+D9pnZlW2bSv7ULtYlFHFhyCI1AEFqiAGrgCddAEBNyBB/AEno1749F4MV7noxljsbMPfsB4+wSiWJVd</latexit>TTot,(k)7

(21)

(22)

1
K

K‚àí1
(cid:88)

k=0

E

(cid:104)(cid:13)
(cid:13)‚àáF (k)(w(k))(cid:13)
(cid:13)

2(cid:105)

‚â§

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

1
K

K‚àí1
(cid:88)

k=0

E (cid:2)F (k‚àí1)(w(k)) ‚àí F (k)(w(k+1))(cid:3)
Œì(k)(1 ‚àí Œõ(k))
(cid:123)(cid:122)
(a)

(cid:125)

(cid:124)

+

K‚àí1
(cid:88)

k=0

+

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõ(k))

(cid:32)

8Œ≤2Œò2Œ∑2
k

(cid:98)D(k)
n
D(k)

(cid:88)

n‚ààN

(e(k)

n ‚àí 1)

Ô£´
Ô£≠1 ‚àí

(k)
S
n
(cid:88)

j=1

Ô£∂

Ô£∏

B(k)
n,j
S(k)
n,j

S(k)
n,j
(cid:16)
D(k)
n

(cid:17)2

(S(k)

n,j ‚àí 1)
B(k)
n,j

‚Ñ¶(k+1)‚àÜ(k+1)
Œì(k)(1 ‚àí Œõ(k))
(cid:123)(cid:122)
(cid:125)
(cid:124)
(b)
(cid:16)

(cid:17)2

(cid:101)œÉ(k)

n,j

(cid:124)

(cid:123)(cid:122)
(c)

(cid:125)

kŒ≤2 (cid:16)

+ 8Œ∂2Œ∑2
(cid:124)

e(k)
max
(cid:123)(cid:122)
(d)

(cid:17) (cid:16)

e(k)
max ‚àí 1

(cid:17)

+ 8Œò2Œ≤Œì(k) (cid:88)

(cid:125)

(cid:124)

n‚ààN

Ô£´

Ô£¨
Ô£≠

(cid:98)D(k)
n
(cid:113)

D(k)

e(k)
n

Ô£∂

2

Ô£∑
Ô£∏

(k)
S
n
(cid:88)

j=1

Ô£´
Ô£≠1 ‚àí

Ô£∂

Ô£∏

B(k)
n,j
S(k)
n,j

S(k)
n,j
D(k)
n

(cid:17)2

(cid:16)

(cid:16)

(S(k)

n,j ‚àí 1)
B(k)
n,j

(cid:17)2

(cid:33)

(cid:101)œÉ(k)

n,j

Ô£π

Ô£∫
Ô£∫
Ô£ª

(cid:125)

(cid:123)(cid:122)
(e)

K‚àí1
(cid:88)

1
K

K‚àí1
(cid:88)

k=0

E

(cid:104)(cid:13)
(cid:13)‚àáF (k)(w(k))(cid:13)
(cid:13)

2(cid:105)

(cid:112)

‚â§ 2

(cid:98)emax

F (‚àí1)(w(0)) ‚àí F (K)(cid:63)
N K(1 ‚àí Œõmax)
eminŒ±

‚àö

(cid:88)

√ó

n‚ààN

(cid:98)D(k)
n
D(k)

(e(k)

n ‚àí 1)

Ô£´
Ô£≠1 ‚àí

(k)
S
n
(cid:88)

j=1

B(k)
n,j
S(k)
n,j

+

Ô£∂

Ô£∏

‚àö

2
eminŒ±

(cid:98)emax
‚àö

N K

S(k)
n,j
(cid:98)D(k)
n

(cid:17)2

(cid:16)

‚Ñ¶(k+1)‚àÜ(k+1)
1 ‚àí Œõmax
(cid:17)2
(cid:16)
(cid:101)œÉ(k)

n,j

+

k=0
(S(k)

n,j ‚àí 1)
B(k)
n,j

+

1
K

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõmax)

(cid:32)

8Œ≤2Œò2Œ±2N
Ke(k)
sum

8Œ∂2Œ±2Œ≤2N
Ke(k)
sum

(cid:17) (cid:16)

(cid:16)
e(k)
max

(cid:33)

(cid:17)

e(k)
max ‚àí 1

+

1
K

K‚àí1
(cid:88)

k=0

4emaxŒ±Œò2Œ≤
(cid:113)

(1 ‚àí Œõmax)

‚àö

N
‚àö

Ô£´

Ô£¨
Ô£≠

(cid:88)

K

n‚ààN

(cid:98)D(k)
n
(cid:113)

D(k)

e(k)
n

Ô£∂
2

Ô£∑
Ô£∏

(k)
S
n
(cid:88)

j=1

Ô£´
Ô£≠1 ‚àí

Ô£∂

Ô£∏

B(k)
n,j
S(k)
n,j

S(k)
n,j
(cid:98)D(k)
n

(cid:17)2

(cid:16)

(cid:16)

(cid:17)2

(cid:101)œÉ(k)

n,j

(S(k)

n,j ‚àí 1)
B(k)
n,j

e(k)
sum

.
Œòn}

We further deÔ¨Åne Œò = maxn

‚àà

‚àí

w

‚ààN {
We next quantify the dynamics of data variations at the
devices via introducing a versatile measure that connects the
variation in the data to the performance of the ML model:
DeÔ¨Ånition 2 (Model/Concept Drift). Let Dn(t) and D(t)
denote the instantaneous number of datapoints at device n and
total number of data points at wall clock time t (measured
in seconds) during PSL. For each device n, we measure the
online model/concept drift for two consecutive time instances
1 and t during which the device does not conduct ML
t
R, which captures the variation of
model training by ‚àÜn(t)
the local loss for any model parameter,
1)
Dn(t)
D(t) Fn
1) Fn
‚àí
‚àí
0,
A larger value for the model/concept drift, i.e., ‚àÜn (cid:29)
implies a larger local loss variation and a harder tracking of the
optimal model parameters for the ML training. Also, the above
deÔ¨Ånition encompasses the case where due to model drift the
old model becomes more Ô¨Åt to the current data (when ‚àÜn <
0). Our deÔ¨Ånition of model drift is different from other few
deÔ¨Ånitions in current art [33], [36] from two aspects. First, our
deÔ¨Ånition connects the data variations to the model performance
and can be used in scenarios where major variations in some
dimensions of data (i.e., some of the features) does not affect
the performance. Second, our proposed drift is estimable (i.e.,
it is not deÔ¨Åned with respect to the variations in the optimal
model as in [33], [36] which is by itself unknown a priori).

Dn(t)
(cid:1)

‚àÄ
Dn(t

‚àÜn(t).(26)

Dn(t
D(t

RM :

‚àà
1)

w

‚àí

‚àí

‚â§

w

(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:1)

(cid:0)

(cid:0)

We next obtain the convergence behavior of PSL for non-
(k)
to denote the set of data
convex loss functions. We use
n
D
points at device n used during global round k obtained after
n | = (cid:80)
the model dispersion phase, (cid:98)D(k)
m,nD(k)
n = | (cid:98)D(k)
m‚ààN (cid:37)(k)
m .
(cid:98)
Also, with slight abuse of notations, we use S(k)
n,j in the bounds
to denote the number of data points in respective stratum and
(cid:101)œÉ(k)
n,j to denote its variance during local model training of global
round k which are obtained via Lemma 1.
Theorem 1 (General Convergence Behavior of PSL). Assume

Œ∑k ‚â§

min

1
2Œ≤

(cid:110)

(cid:114)

Œ∂1(Œ≤2+Œõ(k))

Œõ(k)
(cid:16)
e(k)
max

(cid:16)

e(k)
1
max‚àí

(cid:17)(cid:17) ,

2Œ≤

(cid:16)

n

‚ààN

(cid:80)

D(k)

n e(k)
n
D(k)

1

‚àí

,

(cid:17)

(cid:111)

where Œõ(k) < 1 is a constant and e(k)
. Also,
}
‚àÜ(k)
deÔ¨Åne Œì(k) =
n ,
‚ààN
where
the cumulative
average of the gradient of the global loss function over the
training period of PSL satisÔ¨Åes the upper bound in (21).

Œ∑k
n
2
n = maxt
‚àà

max = maxn
, and ‚àÜ(k) =

T Idle,(k) ‚àÜn(t). Then,

n e(k)
n
D(k)

‚àÜ(k)

e(k)
n

‚ààN {

D(k)

(cid:80)

(cid:80)

‚ààN

(cid:98)

n

(cid:98)

‚àí

(k

In (21), F (k

Proof. Refer to Appendix B for the detailed proof.
1)(w(k)) = F (w(k)

(cid:4)
1)) denotes the loss
under which the k-th global model training ends where w(k)
is obtained and F (k)(w(k+1)) = F (w(k+1)
(k)) denotes the
loss under which the k + 1-th global model training ends where
w(k+1) is obtained. Also, F (
1)(w(0)) denotes the initial loss
of the algorithm before the start of any model training, i.e.,
‚Ñ¶(1) seconds before the Ô¨Årst model training round.

|D

|D

‚àí

‚àí

Main Takeaways. The bound in (21) captures the effect
of the ML-related parameters on the performance of PSL.
Term (a) captures the effect of consecutive loss function gains
during ML training. Term (b) captures the effect of model
drift (via ‚àÜ(k+1)). Terms (c) and (e) are both concerned with
the data stratiÔ¨Åcation and mini-batch sizes used at the devices.
Term (d) captures the effect of model dissimilarity (via Œ∂2)
and the number of SGD iterations (via e(k)
max). Larger model
dissimilarity leads to smaller step size choices (Œ∂1 incorporated
in the condition on Œ∑k ) and larger upper bound (Œ∂2 in (d)). Also,
larger local data variability implies a larger bound (Œò in (c)
and (e)). Further, terms (a) and (b) are inversely proportional
to Œ∑k (incorporated in Œì(k)), terms (c) and (d) are quadratically
proportional to Œ∑k, and term (e) is linearly proportional to it
(incorporated in Œì(k)). The bound also provides further insights:
(i) upon having e(k)
n, k, terms (c) and (d) become zero
and the bound demonstrate 1-epoch distributed ML with non-
uniform SGD sampling; (ii) upon sampling all the datapoints,
i.e., B(k)
j, n, k, terms (c) and (e) become zero
and the bound reveals the convergence of full-batch local
gradient descents, (iii) upon having uniform sampling across
strata, B(k)
n , the bound demonstrates the
convergence upon uniform data sampling using SGD with mini-
batch size B(k)
at each device n; (iv) the effect of ofÔ¨Çoading
n
is reÔ¨Çected in S(k)
n . In particular, considering terms
n,j and

n,j = S(k)
n,j ,

n,j = B(k)

n,j /D(k)

n S(k)

n = 1,

D(k)

‚àÄ

‚àÄ

(cid:98)

(c) and (e), increasing S(k)
n,j -s (i.e., data reception at device
n), with everything else being constant, needs to be met via
increasing B(k)
n,j (i.e., increasing the mini-batch size) to keep
the bound value Ô¨Åxed. Similarly, upon data ofÔ¨Çoading, the
device can use a smaller mini-batch size. Thus, the bound
promotes ofÔ¨Çoading data from devices with low computation
capability to those with higher computation capability.

The bound in (21) reveals that reaching convergence is
attainable under certain circumstances, which we aim to obtain:
Corollary 1 (Convergence under Proper Choice of Step Size
and Bounded Local Iterations). In addition to the conditions
in Theorem 1, further assume that (i) Œ∑k = Œ±(cid:14)(cid:113)
sum/N with
a Ô¨Ånite positive constant Œ± chosen to satisfy the condition
on Œ∑k in Theorem 1, where e(k)
emin ‚â§
e(k)
emax,
sum
(cid:80)
Œõmax < 1, (iv) emin ‚â§
k, (iii) maxk
emax
‚àÄ
‚â§
for two Ô¨Ånite positive constants emin and emax,
k, where
(cid:98)
(cid:98)
(cid:98)
(cid:9)
e(k)
n e(k)
n /D(k). Then, the cumulative average
avg =
of the global loss function gradient for PSL satisÔ¨Åes (22).
Proof. Refer to Appendix C for the detailed proof.

e(k)
n , (ii)
emin and
e(k)
(cid:98)
avg
‚â§

emax for two Ô¨Ånite positive constants

(cid:8)
D(k)

sum =

Ke(k)

Œõ(k)

(cid:80)

‚ààN

‚ààN

(cid:4)

‚â§

‚àÄ

n

n

Bound (22) no longer depends on consecutive loss gains ((a)

‚àí
. We next obtain the conditions under which the cumu-

in (21)), rather it depends on the initial error F (
F (K)(cid:63)
lative average of the global gradient converges to zero:
Corollary 2 (Convergence under UniÔ¨Åed Upperbounds on the
Sampling Noise). Under the conditions speciÔ¨Åed in Corol-
lary 1, further assume (i) bounded stratiÔ¨Åed sampling noise

1)(w(0))

‚àí

(k)

Ô£±
Ô£¥Ô£≤
S
n(cid:88)

Ô£¥Ô£≥

j=1

max
k,n

(cid:32)

1 ‚àí

(cid:33)

B(k)
n,j
S(k)
n,j

S(k)
n,j
D(k)
n

(cid:17)2

(cid:16)

(S(k)

n,j ‚àí 1)
B(k)
n,j

(cid:17)2

(cid:16)
(cid:101)œÉ(k)

n,j

Ô£º
Ô£¥Ô£Ω
Ô£¥Ô£æ ‚â§

e(k)
max

2

(cid:2)

‚àÄ

‚àà

(cid:4)

‚àá

(cid:13)
(cid:13)

‚â§ O

} ‚â§

K
1
‚àí
k=0

(1/‚àöK).

F (k)(w(k))

+,
Œ≥
K‚àÜ(k)
,
a, 0
(cid:3)
}

iterations maxk{

n, k, (ii) bounded local
emax
‚àÄ
for positive constant emax, and (iii) bounded idle period as
‚Ñ¶(k)
k, for a Ô¨Ånite non-negative Œ≥, where
‚àÄ
‚â§
[a]+ (cid:44) max
R. Then, the cumulative average of
a
{
the global loss function gradient for PSL satisÔ¨Åes (27), implying
1
K
Proof. Refer to Appendix D for the detailed proof.
(cid:13)
(cid:13)

(cid:80)
One of the key Ô¨Åndings of Corollary 2 is that, to have
a guaranteed convergence behavior, the idle times must be
+,
inversely proportional to the model drift, i.e., ‚Ñ¶(k)
k. This implies that larger model drift requires rapid global
‚àÄ
aggregations (i.e., small idle times), while smaller model drift
requires less frequent global aggregations (i.e., large idle times).
We next obtain the data sampling technique that needs to be
utilized under a certain data sampling budget at each device:
Proposition 1 (PSL under Optimal Local Data Sampling).
Considering the assumptions made in Theorem 1. For a given
mini-batch size at each device n, i.e., B(k)
n , tuning the number
of sampled points from strata according to Neyman‚Äôs sampling
technique represented with respect to the variance of the
(cid:17)‚àí1
(cid:16)
i=1 (cid:101)œÉ(k)
n (cid:101)œÉ(k)
B(k)
data described by B(k)
minimizes the bound in (21). Further, if Œ∑k = Œ±(cid:14)(cid:113)
sum/N
with a Ô¨Ånite positive constant Œ± chosen to satisfy the condition
on Œ∑k in Theorem 1 the cumulative average of global gradient
under PSL satisÔ¨Åes the bound in (28).

n,i S(k)
n,i
Ke(k)

Œ≥
K‚àÜ(k)

n,jS(k)

(cid:17) (cid:16) (cid:80)S

n,j =

(k)
n

‚â§

n,j

(cid:3)

(cid:2)

Proof. Refer to Appendix E for the detailed proof.

8

(cid:4)

The choice of B(k)

n,j in Proposition 1 advocates sampling
more data points from those strata with higher variance to
reduces the SGD noise. This technique is particularly effective
when the local datasets are non-i.i.d., e.g., devices possess
unbalanced number of data points from different labels.

The bound obtained in Proposition 1 is rather general.
In particular, the bound is not obtained under conditions in
Corollary 2, which were considered to prove the convergence
to the stationary point. This is intentionally done to give a
generalized bound that describes the PSL convergence under
arbitrary choice of idle times and sampling errors. We thus
build our optimization with respect to this bound, making our
optimization solver general and applicable to a wide variety
of scenarios. We obtain the convergence of PSL when the
conditions of Corollaries 1 and 2 are imposed on Proposition 1
in Appendix F (Corollary 3 and 4), which are omitted here for
brevity since the results are qualitatively similar.

IV. NETWORK-AWARE PARALLEL SUCCESSIVE LEARNING

In network-aware PSL, we aim to jointly optimize the macro
decisions of the system, e.g., timing of the synchronization
signals and idle times between global aggregations, and the
micro decisions of the system, e.g., local mini-batch sizes and
data/parameter ofÔ¨Çoading ratios, which is among the most
general formulations in literature. We formulate the network-
aware PSL as the following optimization problem:

œÉmax,

(P) : min

K

1

‚àí

c1ETot,(k) + c2T Tot,(k)

(cid:35)

1
K (cid:34)
1
K

+ c3

(cid:88)k=0
K
‚àí

1

E

(cid:88)k=0
(cid:99)D

(k)

‚àá

(cid:104)(cid:13)
(cid:13)

F (k)(w(k))

2

(cid:16)

=Œû

,B(k),‚Ñ¶(k),‚àÜ(k)(cid:17)

(cid:105)

(cid:13)
(cid:13)
given by (28)

s.t.
T Tot,(k) = T D,(k) + T L,(k) + T M,(k) + T U,(k),
ETot,(k) =

(cid:123)(cid:122)

(cid:124)

(cid:125)

E(k)
n ,

K

(cid:88)n
‚ààN

T Tot,(k) + ‚Ñ¶(k) = T ML,

(cid:88)k=1
max
n
‚ààN (cid:110)
max
n
‚ààN (cid:110)
max
n
‚ààN (cid:110)
max
n
‚ààN (cid:110)

T DR,(k)
n

(cid:111)
‚â§

T C,(k)
n

(cid:111)
T RG,(k)
n

‚â§

(cid:111)

T GT,(k)
n

‚â§
(cid:37)(k)
n,m = 1, n

(cid:111)

T D,(k),

‚â§
T L,(k),

T M,(k),

T U,(k),

,

‚àà N

œï(k)

n,m = 1, n

,

‚àà N

(cid:88)m
‚ààN

(cid:88)m
‚ààN
œï(k)
n,n

(1

‚àí

œï(k)

n,m ‚â§

0, n

,

‚àà N

n

}

(cid:88)m
‚ààN \{
œï(k)
n,n)

(cid:88)m
‚ààN \{

n

}

œï(k)

m,n ‚â§

0, n

,

‚àà N

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

(38)

(39)

(40)

1
K

K‚àí1
(cid:88)

k=0

E

(cid:104)(cid:13)
(cid:13)‚àáF (k)(w(k))(cid:13)
(cid:13)

2(cid:105)

‚â§2

(cid:112)

(cid:98)emax

F (‚àí1)(w(0)) ‚àí F (K)(cid:63)
N K(1 ‚àí Œõmax)
eminŒ±

‚àö

‚àö

(cid:98)emaxŒ≥

2
‚àö
N K(1 ‚àí Œõmax)

+

+

eminŒ±

4emaxŒ±Œò2Œ≤
‚àö

(1 ‚àí Œõmax)

‚àö

N
‚àö
(cid:98)emin

9

œÉmax

K

(cid:33)

(27)

8Œ≤2Œò2Œ±2N (emax ‚àí 1)œÉmax/(cid:98)emin + 8Œ∂2Œ±2N Œ≤2 (emax) (emax ‚àí 1)/(cid:98)emin

(cid:32)

+

1
K

1
(1 ‚àí Œõmax)

1
K

K‚àí1
(cid:88)

k=0

(cid:104)

(cid:107)‚àáF (k)(w(k))(cid:107)2(cid:105)

E

‚â§ Œû

(cid:16)
(cid:98)D

(k)

, B(k), ‚Ñ¶(k), ‚àÜ(k)(cid:17)

(cid:44)

‚àö

2

(cid:98)emax

F (‚àí1)(w(0)) ‚àí F (K)(cid:63)(cid:17)
(cid:16)
‚àö

+

Œ±emin

N K(1 ‚àí Œõmax)

K‚àí1
(cid:88)

k=0

(cid:113)

2
Œ±e(k)
avg

e(k)
sum‚Ñ¶(k+1)‚àÜ(k+1)
‚àö
N K(1 ‚àí Œõ(k))

+

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõ(k))

Ô£´

Ô£¨
Ô£¨
Ô£≠

8Œ≤2Œò2Œ±2N
e(k)
sumK2

(cid:88)

n‚ààN

(cid:98)D(k)
n
D(k)

(e(k)

n ‚àí 1)

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

1
B(k)
n

1
(cid:98)D(k)
n

(cid:16)

(cid:17)2

Ô£´

Ô£¨
Ô£≠

(k)
S
n
(cid:88)

j=1

Ô£∂

2

n,j S(k)
(cid:101)œÉ(k)

n,j

‚àí

Ô£∑
Ô£∏

(cid:124)

(cid:123)(cid:122)
(a)

+

8Œ∂2Œ±2Œ≤2N
e(k)
sumK2

(cid:17)(cid:16)

(cid:16)

e(k)
max

e(k)
max ‚àí 1

(cid:17)

Ô£∂

Ô£∑
Ô£∑
Ô£∏

K‚àí1
(cid:88)

+

k=0

2

‚àö

N

(cid:113)

4e(k)

avg Œ±Œò2Œ≤
‚àö

e(k)
sumK

K(1 ‚àí Œõ(k))

(cid:88)

n‚ààN

(cid:18)

1
(cid:113)

(cid:19)2

e(k)
n

D(k)

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

1
B(k)
n

(cid:125)

(k)
S
n
(cid:88)

j=1
(cid:124)

S(k)
n,j

(cid:16)

(cid:101)œÉ(k)

n,j

(cid:17)2

Ô£π

Ô£∫
Ô£∫
Ô£ª

(cid:123)(cid:122)
(b)

Ô£´

Ô£¨
Ô£≠

(k)
S
n
(cid:88)

j=1

(cid:125)

Ô£∂

2

n,j S(k)
(cid:101)œÉ(k)

n,j

‚àí

Ô£∑
Ô£∏

(28)

Ô£π

Ô£∫
Ô£∫
Ô£ª

(k)
S
n
(cid:88)

j=1
(cid:124)

S(k)
n,j

(cid:16)

(cid:101)œÉ(k)

n,j

(cid:17)2

(cid:123)(cid:122)
(c)

(cid:125)

(42)

, (41)

B(k)

f max
n , 1

f (k)
n ‚â§

f min
n ‚â§
(cid:37)(k)
n,m, œï(k)
Variables:
K, (cid:8)e(k), f (k), B(k), (cid:37)(k), œï(k), T D,(k), T L,(k), T M,(k), T U,(k), ‚Ñ¶(k)(cid:9)K

‚â§
0, n, m

m,nD(k)
(cid:37)(k)

n ‚â§
,

n,m ‚â•

m , n

(cid:88)m
‚ààN

‚àà N

‚àà N

n ]n‚ààN ,

k=1
Objective and variables. P aims to identify the number
of global aggregations K, and the value of the following
variables at each global aggregation k: the number of local
SGD iterations e(k) = [e(k)
n ]n‚ààN , the frequency cycles of
the devices f (k) = [f (k)
the local mini-batch sizes
B(k) = [B(k)
n ]n‚ààN (given the mini-bath size, the sample size
of strata, i.e., [B(k)
n,j]n‚ààN , ‚àÄj, is given by Proposition 1), the data
ofÔ¨Çoading ratios (cid:37)(k) = [(cid:37)(k)
n,m]n,m‚ààN , the model parameter
ofÔ¨Çoading ratios œï(k) = [œï(k)
n,m]n,m‚ààN , synchronization periods
(i.e., T D,(k), T L,(k), T M,(k), T U,(k) deÔ¨Åned in Sec. II-E5), and the
idle times between the global aggregations ‚Ñ¶(k). The objective
function of P draws a tradeoff between the total energy
consumption, device acquisition time/cost, and the accuracy of
the global model captured via function Œû, which is characterized
by the bound in (28). These (possibly) competing objectives
are weighted via non-negative coefÔ¨Åcients c1, c2, c3.4

Constraints. T Tot,(k) in (30) denotes the device acquisition
time and ETot,(k) in (31) denotes the total energy consumption
during global round k, where E(k)
is given by (16). Con-
n
straint (32) ensures that the accumulated time spent during
the model training and the idle times equals to the ML model
training time T ML. Constraints (33), (34), (35), (36) ensure
that the time interval used for data dispersion (see (11)), local
computation (see (12)), model dispersion (see (14)), and uplink
transmission (see (15)) are chosen to ensure the operation
of the system without conÔ¨Çict. Constraints (37) and (38)
ensure the proper dispersion of the data and model parameters.
Constraints (39) and (40) together ensure that each device either
disperses its model parameter or keeps it local and engages in
model condensation, i.e., œÜn,n is binary and takes the value
of 0 if any portion of the local model is ofÔ¨Çoaded; and 1
otherwise. Finally, (41), (42) are the feasibility constraints.

Main takeaways. In P, if c3 = 0, and c1, c2 > 0, model
training never occurs (K = 0) and devices always remain
in the idle state. As c3 increases, the solution favors a lower
model loss via increasing the number of global rounds K and
decreasing the idle times ‚Ñ¶(k)-s. Œû is a function of concept drift

4We trivially deÔ¨Åned ETot,(0) = T Tot,(0) = 0.

(‚àÜ(k)
n -s), especially upon having small concept drifts, once the
global model reaches a relatively low loss (i.e., it is warmed
up and has high inertia), performing global aggregations result
in marginal performance gains, and thus the frequency of
global rounds will be decreased. However, upon having large
concept drifts, since the global model obsoletes fast, to track
the model variations, more global rounds with lower idle
times are preferred. Also, considering Œû behavior in (28),
the optimization favors larger mini-batch sizes at the devices
with higher number of datapoints and larger data variance.
Also, the data is often ofÔ¨Çoaded from the devices with limited
computation resource to those with abundant resources, while
the model parameters are often transferred from the devices
with poor BS channel conditions to those with better channels.
Behavior of P. Except integer K, all the variables are
continuous. Given that 1 ‚â§ K ‚â§ T ML, with all the rest
of the variables known as a function of K, K can be
obtained with an exhaustive search. We thus focus on obtaining
the rest of variables for a given K. In P, multiplication
between optimization variables appear in multiple places. For
, encapsulated in E(k)
example, in EC,(k)
(see (16)) in (31),
n
n
n , B(k)
multiplication between e(k)
exist (see (18)).
Similar phenomenon exist in T C,(k)
(see (12)) in (34). More
importantly, the deÔ¨Ånition of Œû in (28) consists of multiple
terms with multiplication of variables some of which with
negative coefÔ¨Åcients. In particular, the problem belongs to the
category of Signomial Programming, which are highly non-
convex and NP-hard [37]. This is expected given the generality
of the formulation and the complex behavior of bound (28).

n , and f (k)

n

n

In the following, we provide a tractable technique to solve P.
Our approach relies on a set of approximations and constraint
modiÔ¨Åcations to solve the problem in an iterative manner, which
enjoys strong convergence guarantees.5 Although our approach
is developed for P, it can be applied to a broader category of
problems that we call network-aware distributed ML, where
the formulations are mostly concerned with optimizing the
performance of the ML training under network constraints.
We are among the Ô¨Årst to introduce these highly versatile
optimization techniques to distributed ML literature.

5To be able to solve the problem, we rely on strict positive optimization
variables and replace constraint (42) with (cid:37)(k)
n,m > 0, ‚àÄn, m. Accord-
ingly, inequalities in (39) and (40) in the form of A(x) ‚â§ 0 are replaced with
A(x) < œë, where œë > 0 is an optimization variable. œë is then added to the
objective function with a penalty term to ensure œë ‚Üì 0 at the Ô¨Ånal solution.

n,m, œï(k)

H(x) =

K
(cid:88)

k=1

T Tot,(k) + ‚Ñ¶(k)

(cid:98)H(x; (cid:96)) (cid:44)

‚â•

(cid:32)

T M,(k)H([x](cid:96)
(cid:2)T M,(k)(cid:3)(cid:96)

‚àí

‚àí
1

1)

(cid:33)

(cid:32)

K
(cid:89)

k=1

1

T M,(k)(cid:105)(cid:96)
(cid:104)
H([x](cid:96)

‚àí
1)

‚àí

T D,(k)H([x](cid:96)
(cid:2)T D,(k)(cid:3)(cid:96)

‚àí

‚àí
1

1)

(cid:33)

1

T D,(k)(cid:105)(cid:96)
(cid:104)
H([x](cid:96)

‚àí
1)

‚àí

(cid:32)

(cid:104)

(cid:33)

1

T L,(k)(cid:105)(cid:96)
H([x](cid:96)

‚àí
1)

‚àí

T L,(k)H([x](cid:96)
(cid:2)T L,(k)(cid:3)(cid:96)

‚àí

‚àí
1

1)

(cid:32)

T U,(k)H([x](cid:96)
(cid:2)T U,(k)(cid:3)(cid:96)

‚àí

‚àí
1

(cid:104)

(cid:33)

1)

1

T U,(k)(cid:105)(cid:96)
H([x](cid:96)

‚àí
1)

‚àí

(cid:32)

‚Ñ¶(k)H([x](cid:96)
(cid:2)‚Ñ¶(k)(cid:3)(cid:96)

‚àí

‚àí
1

(cid:33)

1)

(cid:104)

‚Ñ¶(k)(cid:105)(cid:96)
H([x](cid:96)

1

‚àí
1)

‚àí

G(x) =

(cid:88)

m

‚ààN

(cid:37)(k)
n,m ‚â•

(cid:98)G(x; (cid:96)) (cid:44) (cid:89)

m

‚ààN

Ô£´

Ô£¨
Ô£≠

1)

‚àí

(cid:37)(k)
n,mG([x](cid:96)
(cid:105)(cid:96)
(cid:104)
(cid:37)(k)
n,m

‚àí

1

Ô£∂

Ô£∑
Ô£∏

(cid:21)(cid:96)

1

‚àí

(cid:20)

(cid:37)

(k)
n,m
G([x](cid:96)

1)

‚àí

(44)

J(x) =

(cid:88)

m

‚ààN

œï(k)

n,m ‚â•

(cid:98)J(x; (cid:96)) (cid:44) (cid:89)

m

‚ààN

Ô£´

Ô£¨
Ô£≠

1)

‚àí

œï(k)
n,mJ([x](cid:96)
(cid:105)(cid:96)
(cid:104)
œï(k)
n,m

‚àí

1

Ô£∂

Ô£∑
Ô£∏

10

(43)

1

‚àí

(cid:21)(cid:96)

(cid:20)

œï

(k)
n,m
J([x](cid:96)

1)

‚àí

(45)

I(x) =

(cid:88)

m,nD(k)
(cid:37)(k)

m ‚â•

m

‚ààN

(cid:98)I(x; (cid:96)) (cid:44) (cid:89)

(cid:32)

m

‚ààN

D

(cid:33)

1)

‚àí

m,nI([x](cid:96)
(cid:37)(k)
[(cid:37)(k)
m,n](cid:96)

‚àí

1

(k)
m [(cid:37)

I([x](cid:96)

(k)
m,n ](cid:96)
1 )
‚àí

1

‚àí

(46)

R(x) =

(cid:88)

n

‚ààN

e(k)
n ‚â•

(cid:98)R(x; (cid:96)) (cid:44) (cid:89)

(cid:32)

n

‚ààN

(cid:33) [e

(k)
n ][(cid:96)
R([x](cid:96)
‚àí

‚àí

1]
1 )

1)

‚àí

n R([x](cid:96)
e(k)
[e(k)
n ][(cid:96)

‚àí

1]

V (x) =

(cid:88)

n

‚ààN

(cid:98)D(k)

n e(k)

n ‚â•

(cid:98)V (x; (cid:96)) (cid:44) (cid:89)

(cid:89)

n

‚ààN

m

‚ààN

Ô£´

Ô£¨
Ô£≠

m,ne(k)
(cid:37)(k)
(cid:104)
m,ne(k)
(cid:37)(k)

n V ([x](cid:96)
(cid:105)[(cid:96)

n

‚àí

‚àí

1)
1]

D

(k)
m

Ô£∂

Ô£∑
Ô£∏

(cid:21)[(cid:96)

1]

‚àí

(k)
n

(cid:20)

(cid:37)

(k)
m,n e
V ([x](cid:96)

1)

‚àí

(47)

(48)

A related problem format in literature to P is geometric
programming (GP), to understand which the knowledge of
monomial and posynomials is necessary.
DeÔ¨Ånition 3. A monomial is deÔ¨Åned as a function f : Rn
R: f (y) = zyŒ±1
Œ±j ‚àà R,
monomials: g(y) = (cid:80)M

++ ‚Üí
n , where z ‚â• 0, y = [y1, ¬∑ ¬∑ ¬∑ , yn], and
j. Further, a posynomial g is deÔ¨Åned as a sum of
yŒ±
2

n , zm‚â• 0, ‚àÄm.

m=1 zmyŒ±

1 yŒ±2

¬∑ ¬∑ ¬∑ yŒ±n

¬∑ ¬∑ ¬∑ yŒ±

(n)
m

(1)
m

(2)
m

‚àÄ

2

1

A GP in its standard form admits a posynomials objective
function subject to inequality constraints on posynomials and
equality constraints on monomials (see Appendix G-A). With
a logarithmic change of variables, GP in its standard form
can be transformed into a convex optimization that can be
efÔ¨Åciently solved using well-known software, e.g., CVX [38].
Nevertheless, P does not admit the format of GP. In particular,
the bound in (28) appearing in the objective function consists
of terms with negative sign that violate the deÔ¨Ånition of
posynomials. Furthermore, constraints (32), (37) and (38) are
equalities on posynomials, which GP does not admit. To tackle
these violating terms, we Ô¨Årst consider the constraints in the
form of equality on posynomials, and use the method of
penalty functions and auxiliary variables [39]. To this end,
we consider each equality on a posynomial in the format of
g(x) = c as two inequality constraints: (c-i) g(x)
c, and
(c-ii) 1/(Ag(x))
1 is an auxiliary variable,
c, where A
which will later be forced A
1 via being added to the
objective function with a penalty coefÔ¨Åcient.6 Inequality (c-i)
is an inequality on a posynomial, which GP admits. However,
(c-ii) is an inequality on a non-posynomial (division of a
monomial/posynomial by a posynomial is not a posynomial).
One way to transform (c-ii) to an inequality on a posynomial
is to approximate its denominator by a monomial (division of
a posynomial by a monomial is a posynomial). To this end, we
exploit the arithmetic-geometric mean inequality, which upper
bounds a posynomial with a larger monomial in value.
Lemma 2 (Arithmetic-geometric mean inequality [40]).
i(cid:48)
Consider a posynomial function g(y) =
i=1 ui(y), where
ui(y) is a monomial,

i. The following inequality holds:

‚â•
‚Üì

‚â§

‚â§

‚àÄ
ÀÜg(y) (cid:44)

i(cid:48)

g(y)

‚â•

(cid:80)

(ui(y)/Œ±i(z))Œ±i(z) ,

(49)

where Œ±i(z) = ui(z)/g(z),

i, and z > 0 is a Ô¨Åxed point.

i=1
(cid:89)
‚àÄ

6A ‚Üì 1 is an equivalent representation of A ‚Üí 1+.

Algorithm 1: Optimization solver for problem P

input

: Convergence criterion, model training duration T ML.

1 for K = 1 to T ML do
2

Set the iteration count (cid:96) = 0.
Choose a feasible point x[0].
Obtain the monomial approximations (43)-(48),(50) given x[(cid:96)].
Replace the results in the approximation of Problem P (i.e., P (cid:48)

given by (186)-(210) in Appendix G).

With logarithmic change of variables, transform the resulting GP
problem to a convex problem (as described in Appendix G-A).

(cid:96) = (cid:96) + 1
Obtain the solution of the convex problem using current art

solvers (e.g., CVX [38]) to determine x[(cid:96)].

if two consecutive solutions x[(cid:96)‚àí1] and x[(cid:96)] do not meet the

speciÔ¨Åed convergence criterion then

Go to line 4 and redo the steps using x[(cid:96)].
else

Set the solution of the iterations as x(cid:63)

K = x[(cid:96)].

3

4

5

6

7

8

9

10

11

12

13 x(cid:63) = min{x(cid:63)

K }
1

K

‚â§

{Objective of P evaluated at x(cid:63)

K }

T ML

‚â§

In Appendix G, we explain all the steps taken to solve the
optimization problem P. Due to space limitations, we provide
a high level discussion in the following.

D(k)

n , e(k)

sum and e(k)

Our technique to solve P is an iterative approach, where
at each iteration (cid:96), after using the aforementioned method
of penalty functions based on (c-i) and (c-ii) mentioned
above. The corresponding posynomials in (c-ii) for (32), (37)
and (38) are approximated using (43), (44), and (45), re-
spectively. Furthermore, since
avg appear
in multiple places in (28), for tractability, we treat them as
optimization variables and add the following constraints to P:
D(k)
,
(
sum =
avgD(k)) = 1, which are all equality
1,
(cid:98)
constraint on posynomials. We thus use the method of penalty
(cid:80)
functions with approximations given in (46), (47) and (48) to
transform them. It is easy to verify that (43)-(48) are in fact
the best local monomial approximations to their corresponding
posynomials near Ô¨Åxed point x[(cid:96)
in terms of the Ô¨Årst-
order Taylor approximation (vector x encapsulates all the
optimization variables in all the expressions).

(cid:37)(k)
m,nD(k)
n /(e(k)

(cid:98)
m = 1, n

m
‚ààN
n e(k)
D(k)
(cid:80)

n /e(k)
e(k)

n )‚àí

‚àà N

(cid:80)

‚ààN

‚ààN

(cid:98)

‚àí

1]

n

n

1

We next tackle the complex term Œû in the objective function
F (K)(cid:63)
of P, i.e., (28). In (28), we Ô¨Årst upper bound F (w(0))
‚àí
(inside the term in the Ô¨Årst line) with F (w(0)) and e(k)
1
n
(inside the term in the second line), and e(k)
with e(k)
1
n
max

‚àí
‚àí

W (x) = œá(k) +

1
(1 ‚àí Œõ(k))

8Œ≤2Œò2Œ±2N
K2

(e(k)

sum)‚àí1 (cid:88)

n‚ààN

e(k)
n
D(k) (cid:98)Z(k)

n +

K

‚àö

N

4Œò2Œ≤Œ±
‚àö

K(1 ‚àí Œõ(k))

(cid:16)
e(k)
avg

(cid:17) (cid:16)

e(k)
sum

(cid:17)‚àí1/2 (cid:88)

n‚ààN

(cid:98)D(k)
n
(cid:0)D(k)(cid:1)2 e(k)
n

(cid:98)Z(k)
n

11

‚â• (cid:99)W (x; (cid:96)) (cid:44)

(cid:32)

œá(k)W ([x](cid:96)‚àí1)
[œá(k)](cid:96)‚àí1

(cid:33) [œá(k) ](cid:96)
W ([x](cid:96)

1
1)

‚àí

‚àí

(cid:89)

2
(cid:89)

√ó

n‚ààN

q=1

Œ¥1(x, n) =

1
(1 ‚àí Œõ(k))

8Œ≤2Œò2Œ±2N
K2

e(k)
n
e(k)
sumD(k)

(cid:98)Z(k)
n , Œ¥2(x, n) =

K

(cid:18) Œ¥q(x, n)W ([x](cid:96)‚àí1)
Œ¥q([x](cid:96)‚àí1, n)

‚àö

N

4Œò2Œ≤Œ±
‚àö

K(1 ‚àí Œõ(k))

(cid:19)

Œ¥q ([x](cid:96)
‚àí
W ([x](cid:96)

1 ,n)
1 )

‚àí

,

(50)

e(k)
avg (cid:98)D(k)
n
(cid:17)1/2 (cid:0)D(k)(cid:1)2 e(k)

n

(cid:16)

e(k)
sum

n , (cid:98)Z(k)
(cid:98)Z(k)

n =

(k)
S
n
(cid:88)

j=1

sn,j

(cid:17)2

(cid:16)
(cid:101)œÉ(k)

n,j

‚àÄ

(cid:98)

‚àÄ

sn,j,

D(k)
n

‚â§
œÉ(k)
n,j,

with e(k)
max (inside the term in the third line) since these do
not impose a notable difference in the optimization solution.
Also, to have a tractable solution, we assume that (i) the
relative size of the strata to the size of the local dataset is
upper bounded throughout the learning period, and let sn,j ‚â§
1
(k)
denote the upper bound of the relative size of stratum
n,j
S
to the local dataset, i.e., S(k)
k, and (ii) the
n,j /
optimizer optimizes the ML bound for an upper bound on
the variance of the local strata (i.e.,
k, n, j, is upper
bounded via historical data for the optimizer); however, during
the PSL model training each node uses Lemma 1 to track the
variance of its strata, based on which it conducts non-uniform
data sampling according to the rule obtained in Proposition 1.
These upper bounds and assumptions are inherently assumed
in Proposition 2. Note that in (28), all the terms contain the
K
summation over global aggregation index k, i.e.,
k=1, expect
the Ô¨Årst term (term (a)), which in turn can be upper bounded
as (cid:80)K
. We thus consider Œû in (28) as Œû =
K(1‚àíŒõmax)
(cid:80)K
‚àí,1(x) ‚àí œÉ(k)
+ (x) contains all
the terms with positive coefÔ¨Åcients, while œÉ(k)
‚àí,1(x), œÉ(k)
‚àí,2(x)
are two terms with negative sign (terms (b) and (c) and their
coefÔ¨Åcients). We next replace the term Œ≥Œû in the objective
function of P with Œ≥ (cid:80)K‚àí1
k=0 œá(k), which auxiliary variable œá(k)
is the upperbound of summand in Œû, which we add it to the
‚àí,1(x) ‚àí œÉ(k)
constraints as œÉ(k)
‚àí,2(x) ‚â§ œá(k), ‚àÄk. Now we
focus on this constraint, which can be written as follows:

2
Œ±eminK
+ (x) ‚àí œÉ(k)

‚àí,2(x), where œÉ(k)

+ (x) ‚àí œÉ(k)

k=1 œÉ(k)

(cid:98)emaxF (w(0))

(cid:80)

k=1

(cid:101)

‚àö

‚àö

œÉ(k)
+ (x)

œá(k) + œÉ(k)
‚àí

,1(x) + œÉ(k)

1.

(51)

‚àí

,2(x)
(cid:17)

‚â§

(cid:46) (cid:16)

Considering the fraction in (51), all the terms encapsulated in
œÉ(k)
+ (x) are posynomials, making its numerator a posynomial.
However, its denominator is also a posynomial, making it a non-
posynomial fraction. We thus focus on the approximation of the
denominator with a monomial, for which we exploit Lemma 2
(see Appendix G for the detailed steps), the result of which is
given by (50). After the conducted approximations, we arrive at
a problem in which the objective function is a posynomial, and
all the constraints are inequalities on posynomials admitting the
GP format (see the problem in (186)-(210) in Appendix G). It is
easy to verify that with a logarithmic change of the optimization
variables, the problem then becomes a convex optimization,
which is easy to solve using existing software. We provide the
pseudo-code of our optimization solver in Algorithm 1. We
next show the convergence guarantees of our solver.
Proposition 2 (Convergence of the Optimization Solver). For
each K, Algorithm 1 generates a sequence of solutions for P (cid:48)
using the approximations (43)-(48),(50) that converge to x(cid:63)
K
satisfying the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions of P.

Fig. 6: Accuracy obtained using PSL vs. the baseline method (Fed Nova [34]).
Left subplot: time varying local datasets across the global aggregations. Right
subplot: static local datasets across the global aggregations.

prove the convergence, which can be done using the methods
(cid:4)
in [39], [42] and omitted for brevity.

V. SIMULATION RESULTS

(cid:113)

n =

n ‚àº CN (0, 1) captures Rayleigh fading, and Œ≤(k)

We next evaluate the effectiveness of our PSL methodology.
n , h(k)
We incorporate the effect of fading in channel gains h(k)
n,m
n u(k)
Œ≤(k)
(in (8) and (9)). For the uplink channel h(k)
n ,
where u(k)
n =
Œ≤0 ‚àí 10(cid:101)Œ± log10(d(k)
n /d0) [43]. Here, Œ≤0 = ‚àí30dB, d0 = 1m,
(cid:101)Œ± = 3, and d(k)
n is the instantaneous distance between node n
and the BS . We use the same formula to describe the D2D
Œ± = 3.2 since D2D are more prone to
channels but choose
excessive loss. The devices are randomly placed in a circle
area with radius of 25m with a BS in the center.
A. Dynamic Model Training Under PSL: Proof-of-Concept

(cid:101)

We compare the ML performance of PSL against the baseline
method which also considers various SGD iterations across the
devices (FedNova [34]) to classify MNIST [44] and Fashion-
MNIST [45] datasets for a network of 10 devices in Fig. 6.
Both datasets consist of 60K training samples and 10k testing
sampling, where each datum belongs to one of 10 labels. We
consider both time-varying and static device datasets. In the
dynamic case, the devices obtain a new dataset of size drawn
(1000, 125) after each global
from a normal distribution
N
aggregation. For the static case, the devices use a Ô¨Åxed dataset
with size drawn from
(1000, 125) for the entirety of the
experiment. Device datasets are only composed of data from
3 distinct labels. To have a fair comparison, we isolate the
ML performance and consider both methods with no data
ofÔ¨Çoading. The maximum and minimum number of local SGDs
are considered 25 and 1 respectively. Fig. 6 shows that PSL
outperforms the baseline method in both cases owed to its
local data management and non-uniform data sampling. We
quantify the resource savings of PSL in Table I.

N

Proof. It can be shown that P (cid:48) (given by (186)-(210)) is an
inner approximation [41] of P. Thus, it is sufÔ¨Åcient to examine
the three characteristics mentioned in [41] for Algorithm 1 to

B. Network-Aware PSL: Ablation Study

Direct examination of P is difÔ¨Åcult due to the complexity
and entanglement of the optimization variables and elements of

                                                * O R E D O  $ J J U H J D W L R Q   N  7 H V W L Q J  $ F F X U D F \     ) 0 1 , 6 7   ) H G  1 R Y D ) 0 1 , 6 7   3 6 / ) 0 1 , 6 7   ) H G  1 R Y D ) 0 1 , 6 7   3 6 / 0 1 , 6 7   ) H G  1 R Y D 0 1 , 6 7   3 6 /Table I: PSL network savings vs the baseline method (Fed Nova [34]). We
measure network savings to reach accuracy thresholds by the combination of
energy and device acquisition time (DAT).

Acc

Static Datasets

Time Varying Datasets

Energy (kJ)

T 40% 11.7 (50%)
S
50% 11.7 (29%)
I
N
M
60% 40.1 (32%)
F
T 60% 40.8 (87%)
70% 64.1 (92%)
80% 134 (92%)

S
I
N
M

DAT (s)
730 (50%)
730 (29%)
2557 (32%)
2556 (87%)
4017 (92%)
8400 (92%)

Energy (kJ)
17.5 (60%)
11.7 (25%)
46.6 (32%)
35.0 (86%)
69.9 (92%)
134 (92%)

DAT (s)
1095 (60%)
730 (25%)
2921 (32%)
2191 (86%)
4383 (92%)
8400 (92%)

12

Fig. 8: Demonstration of the variation of the idle time in between the
consecutive global aggregations (right y-axis) for a given conÔ¨Åguration of
model/concept drift (left y-axis). The higher value of concept drift leads
to smaller idle times in between the global aggregations, i.e., the global
aggregations are triggered faster with higher concept drifts.

Fig. 7: Convergence of the objective function of P upon having varying
number of devices N for T ML = 1000s, where the ML performance term
in the objective function is prioritized over the energy and delay terms. For
a Ô¨Åxed training period, PSL achieves a lower objective function value upon
having higher number of devices due to achieving a lower ML loss value.
the bound in (28). As a result, we perform an ablation study ‚Äì
systematically evaluating the impacts of important optimization
and scaling variables in isolation ‚Äì to characterize P in detail.
We use the set of network characteristics in Table II.

1) Optimization Solver and Network Size: We Ô¨Årst inves-
tigate the convergence of our optimization solver for various
network sizes in Fig. 7, where T ML = 1000s, K = 2, and
N ‚àà {5, 10, 15, 20}. As can be seen, larger number of devices
leads to slower convergence but a better Ô¨Ånal solution since it
causes (i) processing higher number of datapoints across the
devices that leads to a better ML performance, and (ii) more
efÔ¨Åcient D2D data/model transfer opportunity. Furthermore,
Fig. 7 reveals the diminishing rewards of increasing the number
of devices, where an initial increase from N = 5 to N = 10
results in a notable performance improvement; however, this
effect is less notable as the number of devices increase. This
implies that engaging more devices may allow for more
efÔ¨Åcient model training but there is a point after which the
network energy consumption (due to data processing and model
aggregations) overshadows the ML performance gains.

2) Model/Concept Drift vs. Idle Time: We investigate the
effect of the model drift ‚àÜ(k) on the system idle time in Fig. 8,
where T ML = 5000s, and K = 10. The Ô¨Ågure shows that our
solution promotes rapid global aggregations when the value of
model drift increases. Further, when the value of concept drift
Table II: Network characteristics used for ablation study. The experiments all
use the network values herein, unless indicated otherwise.

Param Value
(cid:98)Œ≤
Œ≤
f max
f min
cn

32 √ó 32 √ó 4
50000 √ó 16
2.3GHz
100KHz
2e ‚àí 13

Param Value
pU
n
pD
n
N0
BU
BD

250mW
100mW
‚àí174dBm/Hz
1MHz
100kHz

Param Value
‚àÜ
Œõ
Œ±
Œ∂2
Œò

0.1
0.9
0.1
1e ‚àí 5
3

Fig. 9: Average local SGD iterations for different devices through the model
training period upon having different degree of model dissimilarity Œ∂2. Upon
having smaller Œ∂2, the variance across the local number of SGD iterations
across the devices is preferable, where the devices with lower processing
cost have higher number of local SGD iterations. Upon increasing the model
dissimilarity, both the variance and the mean of the SGD iterations across the
devices is decreased. For larger values of Œ∂2, i.e., extreme non-iid data, all
the devices conduct the same number of local SGD iterations.

is low, the global aggregations are carried out via higher idle
times: since the data variations at devices is small, they can
stay in idle state to save energy and device acquisition cost.
3) Model Dissimilarity vs. Local SGD Iterations: Œ∂2 in (28)
quantiÔ¨Åes data dissimilarity, where higher data dissimilarity
increases the chance of local model bias with increased local
SGD iterations. In Fig. 9, we vary dissimilarity and depict the
number of SGD iterations across the devices, where T ML = 100s.
Each device n requires an CPU cycles to process each datum,
thus a large an indicates a higher data processing cost. Fig. 9
shows that when Œ∂2 is small, i.e., data is homogeneous across
the devices, PSL maximizes ML performance by having more
local SGD iterations at devices with small an (n ‚àà {3, 4, 5}).
As Œ∂2 increases, uneven local SGD iterations can favor local
models at devices with higher SGD iterations, so PSL reduces
the variance of the number SGD iterations among the devices.
4) Cold vs. Warmed Up Model, and Model Inertia: Model
training interval T ML limits the time for all aspects of PSL (i.e.,
data/model transfer, local processing, and uplink transmissions).
In Fig. 10, we depict the optimal number of global aggregations
for a wide range of T ML values. The left subplot shows the
objective function value as a function of the number of global
aggregations K. It can be seen that increasing the number of
global aggregations K is not suitable for all scenarios. Large K
implies that the system must spend more time on the data and
model transmission processes and therefore has less time to run
model training. As a result, for smaller T ML, K (cid:63) is smaller, and

                     $ S S U R [ L P D W L R Q  , W H U D W L R Q                2 E M H F W L Y H  ) X Q F W L R Q  9 D O X H√ó    1      1               √ó    1       1              * O R E D O  $ J J U H J D W L R Q   N           0 R G H O  & R Q F H S W  ' U L I W  9 D O X H           (k)   , G H D O  7 L P H   V H F   H    H    H    H    H    H   H 2           $ Y H U D J H  / R F D O  6 * '  , W H U D W L R Q V Q       an         Q       an        Q       an        Q       an         Q       an        13

Fig. 10: Optimal number of global aggregations K(cid:63) for different period of
model training intervals T ML. Global aggregations trigger faster for a cold
model. During model training, the inertia of the model increases and it becomes
harder to trigger a new global aggregation once the model is warmed up.

Fig. 12: Device acquisition time (right y-axis) and device idle times (left y-axis)
for varying values of cost of device utilization c2 in P. As c2 increases the
device utilization time decreases and devices remain idle for longer times.

Fig. 11: Top row: the average mini-batch size through model training period.
Bottom row: the net number of received data at the devices (negative values
imply data ofÔ¨Çoading) through model training period. Different ranges of
energy importance c1 in P are considered. The devices with higher processing
costs have smaller mini-batch sizes and ofÔ¨Çoad their data. As c1 increases,
the system ofÔ¨Çoads the data of devices with high processing costs, and, to
compensate for the impact on the ML loss, the mini-batch at the device with
the cheapest processing cost (n = 5) is increased. The right subplot shows
the extreme case, where the importance of the energy overshadows the ML
performance and the mini-batch size even at the cheapest device decreases.
K (cid:63) increases as T ML increases. In the right subplot of Fig. 10,
we analyze the relationship between T ML and K (cid:63). Initially,
i.e., having a cold model, K (cid:63) increases rapidly as a function
of T ML, however, as the model gets warmed up increasing K (cid:63)
calls for larger increments in T ML which signiÔ¨Åes the model
inertia (i.e., for a warmed up model, to trigger a new model
training round larger data variations are needed so that ML
model gains outweighs the network costs).

We next sequentially focus on the scaling of energy, acqui-

sition time, and ML bound terms in problem P.

5) Energy Scaling in Optimization Objective: c1 in the
objective of P controls the importance of the energy com-
ponents. To demonstrate the effect of c1, we focus on the
data processing (measured via mini-batch size) and ofÔ¨Çoading
(measured via net data received) in Fig. 11, where T ML = 100s
and Œ∂2 = 1e ‚àí 3. In the Ô¨Årst column of Fig. 11, c1 increases
while is still small, which leads to less data processing at
devices with high processing cost (i.e., n = 2) and more
data ofÔ¨Çoading from them. The middle column accelerates the
increment of c1, showing rapid increase of data reception and
processing at device n = 5. Here, even lower processing cost
devices (n = 4) begin ofÔ¨Çoading data and processing less data.
In the last column, data processing becomes almost prohibitive,
and consequently, even device n = 5 processes only a subset
of its data. Even though device n = 5 processes only a subset

Fig. 13: Average mini-batch size of the devices for various values of ML
model performance importance c3 in the objective function of P. As c3
increases, the device start processing more data locally, where more data is
being processed in the cheapest devices in terms of computation cost.
of its data, it is still beneÔ¨Åcial for all devices to transfer data to
device n = 5 in order to reduce their SGD noise (they choose
mini-batch sizes of 1 so lower number of local data implies a
lower SGD noise) and thereby reduce the ML term Œû.

6) Temporal Importance in Optimization Objective: The
cost of device acquisition, c2, determines the balance of device
acquisition and idle times. We investigate its inÔ¨Çuence in
Fig. 12, where T ML = 1000s. It can be seen that a decrease in
the device acquisition/active times corresponds to an increase in
the idle times. This trade-off occurs in three regimes, segmented
into three subplots. Initially, increasing c2 has no effect until a
network dependent threshold (in this case c2 = 40) is reached.
After this threshold, we enter a regime in which incremental
increase of c2 results in a notable increase in idle times and
a proportional decrease in active times. Finally, in the third
regime, the marginal reward of reducing device acquisition,
which translates to processing less data in the ML bound,
become prominent. To outweigh this effect, c2 must increase
by orders of magnitude to reduce the system active time.

7) ML Performance in Optimization Objective: Finally, we
characterize the ML model training importance c3 by its effect
on the devices mini-batch size in Fig. 13 for T ML = 1000s.
With increasing c3, the importance of the model performance
increases, and the solution prioritizes the third term in the
objective by increasing the mini-batch size at the cost-efÔ¨Åcient
devices: we see that n = 5 begins processing more data, and,
as c3 increases, other devices also begin processing more data
based on a combination of data ofÔ¨Çoading and processing cost.

VI. CONCLUSION

We introduced dynamic distributed learning via PSL. PSL
extends federated learning through network, heterogeneity, and

       .             2 E M H F W L Y H  ) X Q F W L R Q  9 D O X H√ó   K*K*K*K*K*TML      H TML      H TML      H TML      H TML      H                               TML√ó         K*          0 L Q L  % D W F K  6 L ] H                      1 H W  ' D W D  5 H F H L Y H G            c1 Q       an         Q       an        Q       an        Q       an         Q       an        Q       an        Q       an         Q       an                                  6 X P  R I  , G O H  7 L P H V   V H F                       H   H   H   H                   6 X P  R I  $ F W L Y H  7 L P H V   V H F  F   H    H    H    H    H    H    H    H    H    H         $ Y H U D J H  0 L Q L  % D W F K  6 L ] Hc3 Q       an         Q       an        Q       an        Q       an         Q       an        Q       an        Q       an         Q       an        proximity. Also, it considers a cooperative distributed learning
paradigm via incorporating data dispersion and model/gradient
dispersion with local condensation across the devices to battle
the innate shortcomings of federated learning. PSL further
considers a realistic scenario in which the model training rounds
are conducted with idle times in between, during which the
data evolves across the devices. We modeled the processes
conducted during PSL, introduced a new deÔ¨Ånition for con-
cept/model drift, obtained the convergence characteristics of
PSL, and formulated the network-aware PSL problem. We then
proposed a general optimization solver to solve the problem
with convergence guarantee. Multiple future work directions
are also discussed in the paper.

REFERENCES

[1] C. Jiang, H. Zhang, Y. Ren, Z. Han, K.-C. Chen, and L. Hanzo, ‚ÄúMachine
learning paradigms for next-generation wireless networks,‚Äù IEEE Wireless
Commu., vol. 24, no. 2, pp. 98‚Äì105, 2016.

[2] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, ‚ÄúWireless network
intelligence at the edge,‚Äù Proc. IEEE, vol. 107, no. 11, pp. 2204‚Äì2239,
2019.

[3] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
‚ÄúCommunication-efÔ¨Åcient learning of deep networks from decentralized
data,‚Äù in Proc. Artif. Intell. Stat., 2017, pp. 1273‚Äì1282.

[4] G. Zhu, D. Liu, Y. Du, C. You, J. Zhang, and K. Huang, ‚ÄúToward an
intelligent edge: wireless communication meets machine learning,‚Äù IEEE
Commun. Mag., vol. 58, no. 1, pp. 19‚Äì25, 2020.

[5] M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, ‚ÄúA joint
learning and communications framework for federated learning over
wireless networks,‚Äù arXiv preprint arXiv:1909.07972, 2019.

[6] N. H. Tran, W. Bao, A. Zomaya, M. N. H. Nguyen, and C. S. Hong,
‚ÄúFederated learning over wireless networks: Optimization model design
and analysis,‚Äù in Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM),
2019, pp. 1387‚Äì1395.

[7] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and
K. Chan, ‚ÄúAdaptive federated learning in resource constrained edge
computing systems,‚Äù IEEE J. Sel. Areas Commun. (JSAC), vol. 37, no. 6,
pp. 1205‚Äì1221, 2019.

[8] G. Zhu, Y. Wang, and K. Huang, ‚ÄúBroadband analog aggregation for
low-latency federated edge learning,‚Äù IEEE Trans. Wireless Commun.,
vol. 19, no. 1, pp. 491‚Äì506, 2020.

[9] M. M. Amiri and D. G¬®und¬®uz, ‚ÄúFederated learning over wireless fading
channels,‚Äù IEEE Trans. Wireless Commun., vol. 19, pp. 3546‚Äì3557, 2020.
[10] N. Shlezinger, M. Chen, Y. C. Eldar, H. V. Poor, and S. Cui, ‚ÄúFederated
learning with quantization constraints,‚Äù in Proc. IEEE Int. Conf. Acoustics,
Speech, Signal Process. (ICASSP), 2020, pp. 8851‚Äì8855.

[11] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, and T. Hoe-
Ô¨Çer, ‚ÄúSparCML: High-performance sparse communication for machine
learning,‚Äù in Conf. High Perf. Comput. Netw. Stor. Anal., 2019, pp. 1‚Äì15.
[12] S. Dhakal, S. Prakash, Y. Yona, S. Talwar, and N. Himayat, ‚ÄúCoded
federated learning,‚Äù in IEEE Glob. Commun. Conf., 2019, pp. 1‚Äì6.
[13] Y. Tu, Y. Ruan, S. Wang, S. Wagle, C. G. Brinton, and C. Joe-Wang,
‚ÄúNetwork-aware optimization of distributed learning for fog computing,‚Äù
in Proc. IEEE Int. Conf. Comput. Commun. (INFOCOM), 2020.
[14] W. Wang, Y. Sun, B. Eriksson, W. Wang, and V. Aggarwal, ‚ÄúWide
compression: Tensor ring nets,‚Äù in Proc. IEEE Conf. Comput. Vision
Pattern Recog. (CVPR), 2018, pp. 9329‚Äì9338.

[15] S. Ji, W. Jiang, A. Walid, and X. Li, ‚ÄúDynamic sampling and selective
masking for communication-efÔ¨Åcient federated learning,‚Äù arXiv preprint
arXiv:2003.09603, 2020.

[16] L. Liu, J. Zhang, S. Song, and K. B. Letaief, ‚ÄúClient-edge-cloud
hierarchical federated learning,‚Äù in Proc. IEEE Int. Conf. Commun. (ICC),
2020, pp. 1‚Äì6.

[17] S. Savazzi, M. Nicoli, and V. Rampa, ‚ÄúFederated learning with cooper-
ating devices: A consensus approach for massive IoT networks,‚Äù IEEE
Internet Things J., vol. 7, no. 5, pp. 4641‚Äì4654, 2020.

[18] H. Xing, O. Simeone, and S. Bi, ‚ÄúDecentralized federated learning via
SGD over wireless D2D networks,‚Äù in IEEE Int. Workshop Signal Process.
Adv. Wireless Commun., 2020, pp. 1‚Äì5.

[19] S. Hosseinalipour, S. S. Azam, C. G. Brinton, N. Michelusi, V. Aggarwal,
D. J. Love, and H. Dai, ‚ÄúMulti-stage hybrid federated learning over large-
scale D2D-enabled fog networks,‚Äù IEEE/ACM Trans. Netw., pp. 1‚Äì16,
2022.

14

[20] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and
N. Michelusi, ‚ÄúSemi-decentralized federated learning with cooperative
D2D local model aggregations,‚Äù IEEE J. Sel. Areas Commun., 2021.

[21] M. Bertran, N. Martinez, A. Papadaki, Q. Qiu, M. Rodrigues, G. Reeves,
and G. Sapiro, ‚ÄúAdversarially learned representations for information
obfuscation and inference,‚Äù in Proc. Int. Conf. Machine Learn. (ICML),
2019, pp. 614‚Äì623.

[22] S. S. Azam, T. Kim, S. Hosseinalipour, C. Brinton, C. Joe-Wong, and
S. Bagchi, ‚ÄúTowards generalized and distributed privacy-preserving
representation learning,‚Äù arXiv preprint arXiv:2010.01792, 2020.
[23] M. N. Tehrani, M. Uysal, and H. Yanikomeroglu, ‚ÄúDevice-to-device
communication in 5G cellular networks: challenges, solutions, and future
directions,‚Äù IEEE Commun. Mag., vol. 52, no. 5, pp. 86‚Äì92, 2014.
[24] S. Hosseinalipour, C. G. Brinton, V. Aggarwal, H. Dai, and M. Chiang,
‚ÄúFrom federated to fog learning: Distributed machine learning over
heterogeneous wireless networks,‚Äù IEEE Commun. Mag., vol. 58, no. 12,
pp. 41‚Äì47, 2020.

[25] M. Chen, H. V. Poor, W. Saad, and S. Cui, ‚ÄúWireless communications for
collaborative federated learning,‚Äù IEEE Commun. Mag., vol. 58, no. 12,
pp. 48‚Äì54, 2020.

[26] M. Abolhasan, T. Wysocki, and E. Dutkiewicz, ‚ÄúA review of routing
protocols for mobile ad hoc networks,‚Äù Ad hoc Netw., vol. 2, no. 1, pp.
1‚Äì22, 2004.

[27] S. Zeadally, R. Hunt, Y.-S. Chen, A. Irwin, and A. Hassan, ‚ÄúVehicular ad
hoc networks (VANETS): status, results, and challenges,‚Äù Telecommun.
Syst., vol. 50, no. 4, pp. 217‚Äì241, 2012.

[28] I. Bekmezci, O. K. Sahingoz, and S¬∏ . Temel, ‚ÄúFlying ad-hoc networks
(FANETs): A survey,‚Äù Ad hoc Netw., vol. 11, no. 3, pp. 1254‚Äì1270,
2013.

[29] S. Zhang, L. Yao, A. Sun, and Y. Tay, ‚ÄúDeep learning based recommender
system: A survey and new perspectives,‚Äù ACM Comput. Surv. (CSUR),
vol. 52, no. 1, pp. 1‚Äì38, 2019.

[30] D. Netburn. From covid to curbside, 2020 changed our vocabulary too.
[Online]. Available: https://www.latimes.com/science/story/2020-12-28/
from-covid-to-curbside-2020-changed-our-vocabulary-too

[31] S. L. Lohr, Sampling: Design and Analysis: Design And Analysis. CRC

Press, 2019.

[32] P. Zhao and T. Zhang, ‚ÄúAccelerating minibatch stochastic gradient descent
using stratiÔ¨Åed sampling,‚Äù arXiv preprint arXiv:1405.3080, 2014.
[33] E. Rizk, S. Vlaski, and A. H. Sayed, ‚ÄúDynamic federated learning,‚Äù arXiv

preprint arXiv:2002.08782, 2020.

[34] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, ‚ÄúTackling the
objective inconsistency problem in heterogeneous federated optimization,‚Äù
arXiv preprint arXiv:2007.07481, 2020.

[35] H. Holma and A. Toskala, LTE for UMTS: OFDMA and SC-FDMA

based radio access.

John Wiley & Sons, 2009.

[36] Y. Ruan, X. Zhang, S.-C. Liang, and C. Joe-Wong, ‚ÄúTowards Ô¨Çexible

device participation in federated learning,‚Äù arXiv:2006.06954, 2021.
[37] M. Chiang, ‚ÄúGeometric programming for communication systems,‚Äù Found.
Trends¬Æ Commun. Inf. Theory, vol. 2, no. 1‚Äì2, pp. 1‚Äì154, 2005.
[38] M. Grant and S. Boyd, ‚ÄúCVX: Matlab software for disciplined convex

programming, version 2.1,‚Äù 2014.

[39] G. Xu, ‚ÄúGlobal optimization of signomial geometric programming
problems,‚Äù Eur. J. Oper. Res., vol. 233, no. 3, pp. 500‚Äì510, 2014.
[40] R. J. DufÔ¨Ån and E. L. Peterson, ‚ÄúReversed geometric programs treated
by harmonic means,‚Äù Indiana Univ. Math J., vol. 22, pp. 531‚Äì550, 1972.
[41] B. R. Marks and G. P. Wright, ‚ÄúA general inner approximation algorithm
for nonconvex mathematical programs,‚Äù Oper. Res., vol. 26, no. 4, pp.
681‚Äì683, 1978.

[42] S. Wang, S. Hosseinalipour, M. Gorlatova, C. G. Brinton, and M. Chi-
ang, ‚ÄúUAV-assisted online machine learning over multi-tiered net-
works: A hierarchical nested personalized federated learning approach,‚Äù
arXiv:2106.15734, 2021.

[43] D. Tse and P. Viswanath, Fundamentals of wireless communication.

Cambridge university press, 2005.

[44] L. Yan, C. Corinna, and C. J. Burges. The MNIST dataset of handwritten

digits. [Online]. Available: http://yann.lecun.com/exdb/mnist/

[45] H. Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST. [Online]. Available:

https://github.com/zalandoresearch/fashion-mnist

[46] J. Neyman, ‚ÄúOn the two different aspects of the representative method:
the method of stratiÔ¨Åed sampling and the method of purposive selection,‚Äù
in Breakthroughs Stat. Springer, 1992, pp. 123‚Äì150.

[47] S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi, ‚ÄúA tutorial on
geometric programming,‚Äù Opt. Eng., vol. 8, no. 1, p. 67, 2007.

APPENDIX A
PROOF OF LEMMA 1

15

denote the set of new datapoints that are added to stratum

=
2, with d denoting the feature vector of datapoint d. It is straightforward to verify that the new

, where ¬µ

d,

‚ààA

S

d

A

A

= 1
|A|

œÉ2

(cid:80)

Letting
1

A
d

d
|A|‚àí
mean of the stratum is given by

‚ààA

‚àí

¬µ

1

A

(cid:80)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

¬µnew = |S|

¬µ

¬µold +
|A|
+

|S|

|A| ‚àí |D|

¬µ

A ‚àí |D|

D

.

(52)

Also, the new variance of the stratum is given by

œÉ2
new =

d

d
‚ààA (cid:107)

¬µnew(cid:107)

2 +

‚àí

(cid:80)

d
‚ààA(cid:107)

d
(cid:107)

2 +

d

d
‚ààS (cid:107)

2
(cid:107)

(cid:80)
|A|

d

‚àí

d

d
‚ààS (cid:107)
+

‚àí
+
|S|
d
‚ààD(cid:107)

(cid:80)
d
‚ààA (cid:107)

d

(cid:80)

2 +
(cid:107)

d

d
‚ààS (cid:107)

(cid:80)

‚àí

2
(cid:107)

d

(cid:80)

(cid:80)

(cid:80)

‚ààD (cid:107)

2

d

‚àí

d
‚ààD(cid:107)

¬µnew(cid:107)
1
(cid:80)
|D| ‚àí
2 +
2 +
¬µnew(cid:107)
|A|(cid:107)
(cid:107)
|S|(cid:107)
+
|S|
|A|
2 +
¬µnew(cid:107)
+
|A|

2 +
d
(cid:107)

|A|(cid:107)

|S|

2

¬µnew(cid:107)

‚àí

2

¬µnew(cid:107)
+

|S|(cid:107)
+

1
|D| ‚àí
2
¬µnew(cid:107)
|D| ‚àí
|S|

1
¬µold+

d

d
‚ààA (cid:107)

2 +
(cid:107)

d

d
‚ààS (cid:107)

2
(cid:107)

‚àí

d

‚ààD (cid:107)

2 + (
d
(cid:107)

|A|

+

(cid:80)

2

|S|

‚àí

(cid:16)

(cid:80)
¬µ

¬µold+

|A|
+

A ‚àí|D|

|S|

|A|‚àí|D|

d

d
‚ààA (cid:107)

2 +
(cid:107)

(cid:80)

(cid:80)

|A|
d

d
‚ààS (cid:107)

¬µ

(cid:80)
(cid:62) (

+

+

|A|

|S|

|A|
¬µold +
1
|D| ‚àí
2 +
d
(cid:107)

‚ààD (cid:107)

d

|S|
+

D

+

2
(cid:107)

(cid:17)
|S|

‚àí

|S| ‚àí |D|
1

|D| ‚àí
¬µ

A ‚àí |D|

¬µ

)

D

(cid:13)
(cid:13)
(cid:13)

)

2

=

=

=

=

‚àí|D|(cid:107)

¬µnew(cid:107)

2

‚àí

2¬µ(cid:62)new(

d+

d

‚ààS

d

‚àí

d

‚ààA

d)

d

‚ààD

(cid:80)
2¬µ(cid:62)new(

(cid:80)
¬µold +

(cid:80)

A ‚àí |D|

¬µ

|A|

¬µ

)

D

|S|

‚àí

2

‚àí |D|(cid:107)

2

¬µnew(cid:107)

¬µ

|A|
+

¬µ

A ‚àí|D|

|S|

|A|‚àí|D|

D

(cid:13)
(cid:13)
(cid:13)

+

¬µ

2
A (cid:107)

(cid:107)

2
‚àí|A|
+

|S|‚àí|D|

(cid:17)

|A|

(cid:16)

+

2

¬µD(cid:107)

(cid:107)

2
‚àí|D|
+

|S|‚àí|D|

(cid:17)

|A|

(cid:16)

(cid:80)
+

|A|
1

(cid:17)

+

2

¬µD(cid:107)
(cid:107)

(cid:16)

2
‚àí|D|
+

|A|

|S|‚àí|D| ‚àí |D|

(53)

(cid:17)

2¬µold¬µ(cid:62)

A

+ ‚àí

(cid:16)

|A||S|
+

|A|

|S|‚àí|D|

d

d
‚ààA (cid:107)

2
(cid:107)

‚àí |A|(cid:107)

=

¬µ

A (cid:107)

(cid:17)
2 +

(cid:80)

2

¬µold(cid:107)

+ (cid:107)

(cid:16)

2

+

‚àí|S|
+

|A|

|S|‚àí|D|

(cid:80)

|S|

(cid:17)

2

¬µold(cid:107)
(cid:107)
|A|

(cid:16)
+

|S||D|
+

|A|

|S|‚àí|D|

‚àí|S|
+

|A|

|S|‚àí|D|

(cid:17)
1
|S| ‚àí |D| ‚àí
¬µ(cid:62)
+ 2¬µ

(cid:80)
+ 2¬µold¬µ(cid:62)

D

(cid:17)

‚àí

A

D

d

d
‚ààD (cid:107)

|A|

(cid:16)
2 +
(cid:107)

2

|A||D|
+

|S|‚àí|D|

(cid:17)
2
D (cid:107)

¬µ

|D|(cid:107)

(cid:16)

d

+
|S| ‚àí |D| ‚àí
|A|
2
d
‚ààS (cid:107)
‚àí |S|(cid:107)
(cid:107)
+
|A|
|S| ‚àí |D| ‚àí
+

1
¬µold(cid:107)
1

2
‚àí|A|
+

¬µ

2
A (cid:107)

(cid:107)

|A|

|S|‚àí|D|

(cid:16)
+

|A|

|S| ‚àí |D| ‚àí
|S||D|
+

|S|‚àí|D|

1

(cid:17)

+

D

|A|

(cid:16)
|S| ‚àí |D| ‚àí
1)œÉ2

D

2¬µold¬µ(cid:62)

A

+ ‚àí

|A||S|
+

|A|

|S|‚àí|D|

+ 2¬µold¬µ(cid:62)

(cid:17)

(cid:16)
+ (

|S| ‚àí
+

1)œÉ2

|A|
(
old ‚àí
|D| ‚àí
1
|S| ‚àí |D| ‚àí
+

2

|A|
|A||S|‚àí|S||D|
|S|‚àí|D|
|A|

+

¬µ
(cid:107)

A (cid:107)

(
|A| ‚àí

1)œÉ2

A

=

2

¬µold(cid:107)

+ (cid:107)

(cid:16)

2¬µold¬µ(cid:62)

A

+ ‚àí

|A||S|
+

|A|

|S|‚àí|D|

(cid:16)
old + (
|A| ‚àí
+

+

(cid:17)
1)œÉ2

|A|
(
A ‚àí
|D| ‚àí
1
|S| ‚àí |D| ‚àí
¬µold ‚àí
(cid:107)

2
A (cid:107)

‚àí

¬µ

|A|

(
|S| ‚àí

=

1)œÉ2

|A||S|
+

|S|‚àí|D|

|A|

(cid:17)

+

(cid:16)

|A||S|‚àí|A||D|
|S|‚àí|D|
|A|

+

(cid:17)

|A|
+ 2¬µold¬µ(cid:62)

(cid:16)
+
|S| ‚àí |D| ‚àí
|S||D|
+

(cid:17)
1

D

|A|

|S|‚àí|D|

(cid:16)
|S| ‚àí |D| ‚àí
1)œÉ2

1

D

+ 2¬µ

¬µ(cid:62)

A

D

(cid:16)

|A||D|
+

|A|

|S|‚àí|D|

(cid:17)

+

2

¬µD(cid:107)
(cid:107)

‚àí|A||D|‚àí|S||D|

+

|S|‚àí|D|

|A|

(cid:16)

(cid:17)

+ 2¬µ

¬µ(cid:62)

A

D

(cid:17)

(cid:16)

|A||D|
+

|A|

|S|‚àí|D|

With minor rearranging of the terms, the result of the lemma is obtained.

|S||D|
+

|A|

(cid:16)

|S|‚àí|D|
+

(cid:107)

¬µold ‚àí
|S| ‚àí |D| ‚àí

(cid:17)

|A|

¬µ

2
D (cid:107)

‚àí

1

(cid:16)

|A||D|
+

|S|‚àí|D|

|A|

(cid:17)

(cid:17)

¬µA ‚àí
(cid:107)

¬µ

2
D (cid:107)

.

APPENDIX B
PROOF OF THEOREM 1

16

Revisiting the parameter updating rule of the method, the evolution of the global parameter can be described as follows:

Œ∑k‚àá
is the normalized accumulated gradient of the devices with respect to their number of local descents given by

‚àí

F

,

w(k+1) = w(k)

(54)

(k)

where

(k)

F

‚àá

(k)

=

F

‚àá

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n ,

(55)

where we have used the facts that (i) the model training is conducted on the Ô¨Ånal datasets of the nodes at each global model
, which is obtained after data arrival and departure at the nodes prior to the start of local
training round, i.e.,
model training, and (ii) during data ofÔ¨Çoading, no data points disappears, and thus

D(k)
n ,

‚àà N

n = D(k).

D(k)

D(k)

n =

n

‚àÄ

Using the Œ≤-smoothness of the global loss function (Assumption 1), we have

(cid:98)

F (k)(w(k+1))

F (k)(w(k)) +

F (k)(w(k))(cid:62)

w(k+1)

w(k)

‚â§

‚àá

‚àí

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Remark 5. Throughout, the super index on the loss functions, e.g., F (k)(w) or F (k)
(cid:13)
(cid:13)
n (w) stands for the time index under
which the losses are measured and the corresponding dataset at the network after model dispersion phase, i.e., F (k)(w) =
F (w

(k)) or F (k)

n (w) = Fn(w

(k)) = Fn(w

(k)
n ),

w.

(cid:16)

(cid:17)

|D

|

D

(cid:98)

‚àÄ

D
|

(cid:98)

Replacing the updating rule for w(k+1) and taking the conditional expectation (with respect to randomized data sampling at

n

‚ààN

n

‚ààN

(cid:80)
Œ≤
2

+

(cid:98)
w(k+1)

‚àí

(cid:80)

w(k)

2

.

(56)

the last aggregation of the PSL) from both hand sides yields

Ek

F (k)(w(k+1))

(cid:104)

(cid:105)

F (k)(w(k))

‚â§

‚àí ‚àá

F (k)(w(k))(cid:62)Ek

(cid:34)

Œ∑k

e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n

(cid:35)

(cid:88)n
‚ààN
2

(57)

+

Œ≤
2

Œ∑k

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:88)n
‚ààN

(cid:88)n(cid:48)‚ààN (cid:98)
D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n

.

Ô£π

Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(k)) is the loss of the Ô¨Ånal model obtained at
Remark 6. Note that as mentioned in Remark 5, F (k)(w(k+1)) = F (w(k+1)
(k). This loss is different
global iteration k based on conducting SGD across the devices on the global data distribution
than the loss under which the global iteration k + 1 starts, i.e., F (k+1)(w(k+1)) = F (w(k+1)
(k+1)) due to the existence of
model/concept drift. In the later parts of the proof, we will reveal the connection between F (k)(w(k+1)) and F (k+1)(w(k+1)).

|D

|D

D

Since

F

(k)
n =

‚àá

‚àí

(cid:16)

w(k),e(k)

n

n

‚àí

w(k)

Œ∑k, via recursive expansion of the updating rule in (5), we get

(cid:17) (cid:46)
(k)
n =

F

‚àá

1
D(k)
n

e(k)
n

S(k)
n

S(k)
n,j ‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

.

(58)

(cid:88)d
‚ààB
Taking the expectation from both hand sides of (58), using the law of total expectation (across the local mini-batches) and the
fact that data points selection inside each stratum is conducted uniformly at random without replacement, we get

(cid:98)

e=1
(cid:88)

j=1
(cid:88)

(k),e
n,j

F

(k)
n

Ek

‚àá

(cid:104)

(cid:105)

=

1
D(k)
n

e(k)
n

S(k)
n

S(k)
n,j

e=1
(cid:88)

j=1
(cid:88)
e(k)
n

S(k)
n

Ek Ô£Æ
Ô£Ø
Ô£∞

(k),e
n,j

(cid:88)d
‚ààB

S(k)
n,j

‚àá

1

, d)

‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

fn(w(k),e
‚àí
n
S(k)
n,j

(cid:98)
=

1
D(k)
n

Ô£π

Ô£∫
Ô£ª

(59)

e=1
(cid:88)

j=1
(cid:88)

e(k)
(cid:98)
n

S(k)
n

=

‚àá

e=1
(cid:88)

j=1
(cid:88)

(k)
n,j

(cid:88)d
‚ààS

(k)
n,j

(cid:88)d
‚ààS
fn(w(k),e
‚àí
n
D(k)
n

(cid:98)

1

, d)

=

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

).

17

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

‚àá

e=1
(cid:88)

(cid:88)n
‚ààN
2

n (w(k),e
F (k)
‚àí
n

1

)

Ô£π

Ô£ª

(60)

By replacing (59) back in (57), we get

Ek

F (k)(w(k+1))

(cid:104)

(cid:105)

F (k)(w(k))

‚â§

‚àí ‚àá

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

F (k)(w(k))(cid:62)Ek Ô£Æ
Œ∑k
Ô£∞

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:88)n
‚ààN

(cid:88)n(cid:48)‚ààN (cid:98)
D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n

.

Ô£π

Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+

Œ≤
2

Œ∑k

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

Using the fact that for any two real valued vectors a and b with the same length, we have: 2a(cid:62)b =
we further get

2 +
a
(cid:107)

b
(cid:107)

2
(cid:107)

(cid:107)

a

b

2,
(cid:107)

‚àí

‚àí (cid:107)

Ek

F (k)(w(k+1))

(cid:104)

(cid:105)

F (k)(w(k))

‚â§

Œ∑k
2

‚àí

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

+

2

F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:88)n
(cid:13)
‚ààN
(cid:13)
(cid:13)
2
(cid:13)

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

)

EkÔ£Æ

‚àá

(cid:13)
Ô£Ø
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
e(k)
n

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(61)

‚àá

‚àí (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Œ≤Œ∑2
k
2

+

F (k)(w(k))

D(k)
n
D(k)e(k)
n
(cid:98)

‚àí

(cid:88)n
‚ààN

n (w(k),e
F (k)
‚àí
n

1

)

‚àá

e=1
(cid:88)

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n

(cid:88)n
‚ààN

Ô£π

Ô£∫
Ô£∫
Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

.

Ô£π

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£ª
We bound the last term on the right hand side of above inequality as follows:

Ek Ô£Æ
(cid:13)
(cid:88)n(cid:48)‚ààN (cid:98)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
D(k)
n
D(k)e(k)
(cid:98)

n ‚àá

F

(k)
n

2

Ô£π

Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
D(k)
n

e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n
‚ààN
e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

Œ≤Œ∑2
k
2

Ek Ô£Æ
(cid:13)
(cid:88)n(cid:48)‚ààN (cid:98)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
Ek Ô£Æ
(cid:13)
(cid:13)
(cid:88)n(cid:48)‚ààN (cid:98)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
(cid:13)
(cid:13)
(cid:34)(cid:13)
(cid:88)n(cid:48)‚ààN (cid:98)
(cid:13)
(cid:13)
(cid:13)
D(k)
e(k)
(cid:13)
n(cid:48)
n(cid:48)
D(k)

Ek

Œ≤Œ∑2
k
2

D(k)
n
D(k)e(k)
n
(cid:98)
D(k)
n
D(k)e(k)
(cid:98)

(cid:88)n
‚ààN

(cid:88)n
‚ààN

Ô£´

Ô£¨
Ô£≠

(cid:98)
1
D(k)
n

n (cid:32)

e(k)
n

S(k)
n

e=1
(cid:88)
e(k)
n

j=1
(cid:88)
S(k)
n

S(k)
n,j ‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

(k),e
n,j

(cid:88)d
‚ààB

S(k)
n,j ‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

e=1
(cid:88)

j=1
(cid:88)

(k),e
n,j

(cid:88)d
‚ààB

2

Ô£π

Ô£∫
Ô£ª

Ô£∂

Ô£∑
Ô£∏

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

e(k)
n

(cid:98)
n (w(k),e
F (k)
‚àí
n

1

) +

(cid:88)n(cid:48)‚ààN (cid:98)
S(k)
n

e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

D(k)
n
D(k)e(k)
n
(cid:98)

(cid:88)n
‚ààN
fn(w(k),e
‚àí
n
B(k)
n,j

e=1
(cid:88)
, d)

1

(cid:33)

S(k)
n,j ‚àá

e(k)
n

‚àá

e=1
(cid:88)

D(k)
n
D(k)e(k)
n
(cid:88)n
(cid:98)
‚ààN
D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

Œ≤Œ∑2
k

Ek

(cid:34)(cid:13)
(cid:88)n(cid:48)‚ààN (cid:98)
(cid:13)
(cid:13)
(cid:13)
e(k)
D(k)
(cid:13)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

= Œ≤Œ∑2
k

(cid:32)

(cid:88)n
‚ààN
D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

D(k)
n
D(k)e(k)
(cid:98)

n (cid:32)

1
D(k)
n

e(k)
n

e=1
(cid:88)

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
n
(cid:98)
2

e(k)
n

‚àá

e=1
(cid:88)

Ek

(cid:88)n
‚ààN

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:33)

2

(cid:98)
n (w(k),e
F (k)
‚àí
n

D(k)
n
D(k)e(k)
(cid:98)

n (cid:32)

1

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
(cid:13)
D(k)
n

(cid:98)
e(k)
n

n (w(k),e
F (k)
‚àí
n

1

)

‚àá

2

(cid:35)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(62)

2

n (w(k),e
F (k)
‚àí
n

1

(k),e
n,j

(cid:88)d
‚ààB

j=1
(cid:88)
2

+ Œ≤Œ∑2
k

(cid:35)
e(k)
n

S(k)
n

Ek

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e=1
(cid:88)

j=1
(cid:88)

(k),e
n,j

(cid:88)d
‚ààB

(a)

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)
S(k)
n,j ‚àá

(cid:88)n
‚ààN
fn(w(k),e
1
‚àí
n
B(k)
n,j

e(k)
n

‚àá

e=1
(cid:88)

D(k)
n
D(k)e(k)
n
(cid:98)
e(k)
n

, d)

n (w(k),e
F (k)
‚àí
n

1

)

‚àí

‚àá

e=1
(cid:88)

(cid:35)

)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:35)

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ Œ≤Œ∑2
k

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:124)
Ek

(cid:32)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
where in inequality (i) we have used the Cauchy-Schwarz inequality
(cid:13)

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:88)n
‚ààN

e=1
(cid:88)

(cid:33)

D(k)
n
D(k)e(k)
n
(cid:98)

n (w(k),e
F (k)
‚àí
n

1

‚àá

2

(cid:123)(cid:122)

,

(cid:35)

)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
a + b
(cid:13)
(cid:107)
(cid:107)

(cid:125)

2

a
(cid:107)
(cid:107)

‚â§

2 + 2

2.
b
(cid:107)
(cid:107)

Œ≤Œ∑2
k
2

=

=

‚àí

(i)

‚â§

‚àí

Bounding term (a) in (62): Using (59), i.e., each local gradient estimation is unbiased, combined with the fact that the

noise of gradient estimation is assumed to be independent across the nodes we get

(a)

(i)
=

2

Ek

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

(cid:32)

(cid:88)n
‚ààN

=

(cid:32)

(cid:88)n
‚ààN

2

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

(ii)
=

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

(cid:32)

(cid:88)n
‚ààN

(cid:32)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ek

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2 e(k)

n

e=1
(cid:88)

1
D(k)
n

(cid:98)

1
D(k)
n

(cid:32)

Ek

(cid:124)

(cid:34)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

e(k)
n

S(k)
n

e=1
(cid:88)

j=1
(cid:88)

(k),e
n,j

(cid:88)d
‚ààB
S(k)
n

1
D(k)
n

e(k)
(cid:98)
n

e=1 (cid:32)
(cid:88)

j=1
(cid:88)

(k),e
n,j

(cid:88)d
‚ààB

S(k)
n,j ‚àá

fn(w(k),e
n
B(k)
n,j

1

‚àí

, d)

e(k)
n

‚àí

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

)

S(k)
n,j ‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

n (w(k),e
F (k)
‚àí
n

1

‚àí ‚àá

(b)

S(k)
n

S(k)
n,j ‚àá

j=1
(cid:88)

(k),e
n,j

1

, d)

fn(w(k),e
(cid:123)(cid:122)
‚àí
n
B(k)
n,j

n (w(k),e
F (k)
‚àí
n

1

‚àí ‚àá

(cid:88)d
‚ààB
where to obtain equality (i) we expanded (a) in (62) and used the fact that the noise of gradient estimation across the
mini-batches are independent and zero mean, and to obtain (ii) we used the fact that each individual term in (b) is zero mean.
To further bound (63), we consider the result of [31] (Chapter 3, Eq. (3.5)), based on which the above equality can be further
simpliÔ¨Åed as follows:

(cid:98)

18

(63)

.

2

(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

)
(cid:33)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:125)
)
(cid:33)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:35)

(cid:35)

2

(cid:35)

(a) =

(cid:32)

(cid:88)n
‚ààN

=

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

(cid:32)

S(k)
n

j=1
(cid:88)

2 e(k)

n

e=1
(cid:88)

2 e(k)

n

1
D(k)
n

Ô£Æ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
S(k)
(cid:13)
n
(cid:13)

(cid:98)

1
j=1 (cid:32)
(cid:88)

‚àí

e=1
(cid:88)

S(k)
n,j ‚àá

fn(w(k),e
‚àí
n
B(k)
n,j

1

, d)

1
D(k)
n

‚àí

2

S(k)
n,j

(cid:17)
D(k)
n

2 (cid:16)

œÉ(k),e
‚àí
n,j
B(k)
n,j

2

1

,

(cid:17)

(cid:98)

(k),e
n,j

(cid:88)d
‚ààB
B(k)
n,j
n,j (cid:33) (cid:16)
S(k)
(cid:16)

(cid:17)

(cid:88)n
‚ààN
1

where œÉ(k),e
‚àí
n,j
realization w(k),e
‚àí

n

denotes the variance of the gradients evaluated at the particular local descent iteration e

(cid:98)

1 for the parameter

‚àí

1

. We further bound this quantity as follows:

fn(w(k),e
‚àí
n

1

‚àá

(k)
n

(cid:88)d
(cid:98)
D
‚àà

2

, d)(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£π

Ô£∫
Ô£ª

(64)

‚àá

fn(w(k),e
n
S(k)
n,j

(k)
n,j

d(cid:48)‚ààS

1

‚àí

,d(cid:48))

2

1

, d)

‚àí

d(cid:48)‚ààS

(k)
n,j ‚àá

(cid:80)

(cid:13)
(cid:13)
(cid:13)
fn(w(k),e
‚àí
n

2

1

, d(cid:48))

1

‚àí

, d)

‚àí ‚àá

fn(w(k),e
‚àí
n

1

, d(cid:48))

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

œÉ(k),e
‚àí
n,j

1

2

=

(cid:16)

(cid:17)

(k)
n,j

d

‚ààS

(cid:80)

‚àá

(cid:13)
(cid:13)
(cid:13)

fn(w(k),e
‚àí
n

1

, d)

(cid:80)
1

‚àí
S(k)
n,j ‚àí
fn(w(k),e
‚àí
n

(k)
n,j

‚ààS

(cid:16)

1
S(k)
n,j

(cid:17)2

d

= (cid:80)

S(k)
n,j ‚àá
(cid:13)
(cid:13)
(cid:13)

S(k)
n,j ‚àí
(cid:16)
S(k)
n,j

1
(cid:17)2

(k)
n,j

‚ààS

d
(i)
‚â§ (cid:80)

(k)
n,j

d(cid:48)‚ààS

(cid:80)

‚àá

(cid:13)
(cid:13)
(cid:13)

d
‚â§ (cid:80)

(S(k)
n,j ‚àí
(cid:16)
S(k)
n,j

1)Œò2
(cid:17)2

(k)
n,j

‚ààS

(k)
n,j

d(cid:48)‚ààS

(cid:80)
S(k)
n,j ‚àí

1

(cid:13)
(cid:13)
(cid:13)

1

S(k)
n,j ‚àí
fn(w(k),e
n

S(k)
n,j ‚àí
d(cid:48)
d

1

2

‚àí

=

(S(k)

n,j ‚àí
S(k)
n,j

1)Œò2

(k)
n,j

d

‚ààS

(k)
n,j

d(cid:48)‚ààS

2 (cid:80)

(cid:80)

d
(cid:20)(cid:13)
(cid:13)
(cid:13)

(ii)
=

(cid:17)
(cid:16)
(S(k)
1)Œò2
n,j ‚àí
2
S(k)
n,j

=

(cid:17)
1)Œò2

(cid:16)
2(S(k)
n,j ‚àí
S(k)
n,j

S(k)
n,j

(k)
n,j

d

‚ààS

(cid:80)

(k)
n,j

Œª

‚àí

(cid:101)

d

(cid:13)
(cid:13)
(cid:13)

œÉ(k)
n,j

2

,

(cid:16)

(cid:17)

(cid:13)
(cid:13)
(cid:13)

‚àí

(S(k)

n,j ‚àí
S(k)
n,j

‚â§

1)Œò2
d
2 (cid:80)

‚ààS

(k)
n,j

d(cid:48)‚ààS

(cid:80)

d

(k)
n,j

‚àí

(cid:13)
S(k)
(cid:13)
n,j ‚àí
(cid:13)

(k)
n,j

Œª

2

(cid:16)
+

(cid:17)
d(cid:48)

2

(k)
n,j

Œª

(cid:101)

2

(cid:13)
(cid:13)
(cid:13)

+ S(k)
n,j

‚àí

(cid:13)
S(k)
(cid:13)
(cid:13)
n,j ‚àí

1
(cid:101)

(k)
n,j

d(cid:48)‚ààS

(cid:13)
S(k)
(cid:13)
n,j ‚àí
(cid:13)

1

(cid:80)

2(d

‚àí

‚àí

(k)
n,j)(cid:62)(d(cid:48)

Œª

(cid:101)

‚àí

2

(k)
n,j

Œª

(cid:101)

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

d(cid:48)

(cid:13)
(cid:13)
(cid:13)

2

(cid:13)
(cid:13)
(cid:13)

d(cid:48) +

Œª

(k)
n,j ‚àí

(k)
n,j

Œª

1

‚àí

(cid:101)

(cid:101)

(k)
n,j)
(cid:21)

Œª

(cid:101)

(65)

where d denotes the feature vector of data point d, and

(cid:101)

(k)
n,j and

Œª

œÉ(k)
n,j denote the mean and sample variance of data points in

(cid:101)

(cid:101)

(66)

2

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(67)

stratum

(k)
n,j , which are gradient independent. Also, in inequality (i) of (65), we used the Cauchy-Schwarz inequality, and in
S
(k)
n,j) = 0. Replacing the above result in (64), gives us

(d

Œª

(ii) we used the fact that

(k)
n,j

d

‚ààS

‚àí

19

(cid:80)

2Œò2

(a)

‚â§

(cid:32)

(cid:88)n
‚ààN

(cid:101)
D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

2 e(k)

n

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

e=1
(cid:88)

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

.

(cid:17)

(cid:17)
Replacing the above result back in (62), inequality (61) can be written as follows:

(cid:16)

(cid:98)

EkÔ£Æ

Ô£Ø
Ô£Ø
Ô£∞
2

‚àá

(cid:13)
(cid:13)
(cid:13)

F (k)(w(k))

2

+

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:88)n
(cid:13)
‚ààN
(cid:13)
(cid:13)
(cid:13)

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

Ek

F (k)(w(k+1))

(cid:104)

(cid:105)

F (k)(w(k))

‚â§

Œ∑k
2

‚àí

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

F (k)(w(k))

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

‚àá

e=1
(cid:88)

‚àí

(cid:88)n
‚ààN

n (w(k),e
F (k)
‚àí
n

1

‚àá

‚àí (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 2Œò2Œ≤Œ∑2
k

(cid:32)

(cid:88)n(cid:48)‚ààN (cid:98)
D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

2

(cid:33)

+ Œ≤Œ∑2
k

(cid:32)

(cid:88)n(cid:48)‚ààN (cid:98)

n (cid:33)

D(k)
n
D(k)e(k)
(cid:98)
D(k)
n
D(k)e(k)
n
(cid:98)

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

(cid:88)n
‚ààN

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
S(k)
(cid:13)
n

Ô£π

Ô£∫
Ô£∫
Ô£ª

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

2

(cid:33)

(cid:32)

(cid:88)n
‚ààN

2 e(k)

n

1
j=1 (cid:32)
(cid:88)

‚àí

e=1
(cid:88)

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

2

(cid:17)

(cid:98)

(cid:16)
.

Ô£π

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£ª

F (k)(w(k))

2

Performing some algebraic manipulations on the above inequality and gathering the terms yields

Ek

F (k)(w(k+1))

F (k)(w(k))

‚â§

(cid:105)
e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

2

‚àí

(cid:33)

Œ∑k
2

Œ∑k
2

‚àí

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)
e(k)
D(k)
n(cid:48)
n(cid:48)
D(k) Ô£∂

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:104)

+

Œ≤Œ∑2
k

Ô£´

Ô£≠

(cid:124)
Œ∑k
2

+

(cid:32)

(cid:88)n(cid:48)‚ààN (cid:98)

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

(cid:88)n(cid:48)‚ààN (cid:98)

‚àá

F (k)(w(k))

‚àí

(cid:88)n
‚ààN

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
(cid:13)
(cid:13)
(cid:124)
D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

+ 2Œò2Œ≤Œ∑2
k

(cid:32)

(cid:88)n(cid:48)‚ààN (cid:98)

2

(cid:33)

(cid:32)

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

‚àá

(cid:13)
(cid:13)
(cid:13)
Ek Ô£Æ
(cid:13)
(cid:13)
(cid:88)n
(cid:13)
‚ààN
Ô£Ø
(cid:13)
Ô£∞
(cid:13)
(cid:13)
e(k)
n

Ô£∏
(c)

(cid:123)(cid:122)
D(k)
n
D(k)e(k)
n
(cid:98)

(d)

2 e(k)
(cid:123)(cid:122)
n

S(k)
n

‚àá

e=1
(cid:88)

1
j=1 (cid:32)
(cid:88)

‚àí

e=1
(cid:88)

(cid:13)
(cid:13)
(cid:13)

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
‚àí
n

1

2

2

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£π

Ô£∫
Ô£ª

(cid:125)

n (w(k),e
F (k)
‚àí
n

1

)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
S(k)
n,j
D(k)
n

Ô£π

Ô£∫
Ô£ª

(S(k)
(cid:125)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

2

.

(cid:17)

(68)

B(k)
n,j
S(k)
n,j (cid:33)

(cid:16)

(cid:17)

(cid:98)

Assuming Œ∑k ‚â§
n(cid:48)‚ààN
bound. In the following, we aim to bound term (d):

2Œ≤

(cid:18)

(cid:19)

(cid:80)

1

‚àí

(cid:98)D(k)
n(cid:48)
D(k)

e(k)
n(cid:48)

makes the coefÔ¨Åcient in term (c) negative and thus it can be removed from the upper

(d)

(i)

‚â§

D(k)
n
D(k)

(cid:88)n
‚ààN (cid:98)

‚àá

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
e(k)
(cid:13)
n
(cid:13)

(ii)

‚â§

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
n
(cid:98)

n (w(k))
F (k)

1
e(k)
n

‚àí

e(k)
n

‚àá

e=1
(cid:88)

n (w(k),e
F (k)
n

1

‚àí

n (w(k))
F (k)

‚àí ‚àá

n (w(k),e
F (k)
n

1

‚àí

Ek

‚àá

(cid:20)(cid:13)
(cid:13)
(cid:13)

e=1
(cid:88)

2

)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)

)

(cid:13)
(cid:13)
(cid:13)

Ô£π

Ô£∫
Ô£ª

Œ≤2

‚â§

(cid:21)

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
n
(cid:98)

e(k)
n

e=1
(cid:88)

Ek

w(k)

(cid:20)(cid:13)
(cid:13)
(cid:13)

‚àí

w(k),e
n

‚àí

1

2

.

(cid:21)
(69)

(cid:13)
(cid:13)
(cid:13)

w(k),e
n

‚àí

1

2

, we take the following steps:

To bound Ek

w(k)

(cid:20)(cid:13)
(cid:13)
(cid:13)
w(k)

Ek

(cid:20)(cid:13)
(cid:13)
(cid:13)

‚àí

‚àí

= Œ∑2
k

EkÔ£Æ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£Ø
Ô£Ø
Ô£∞

1
D(k)
n

(cid:98)

(cid:21)

(cid:13)
(cid:13)
(cid:13)
2

(iii)
= Œ∑2
k

w(k),e
n

‚àí

1

(cid:21)

(cid:13)
(cid:13)
(cid:13)
S(k)
n

1

e

‚àí

(cid:88)e(cid:48)=1

j=1
(cid:88)

(k),e(cid:48)
n,j

(cid:88)d
‚ààB

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
(cid:13)
S(k)
(cid:13)
n,j ‚àá

1
D(k)
n

S(k)
n

e

1

‚àí

(cid:88)e(cid:48)=1

j=1
(cid:88)

(k),e(cid:48)
n,j

(cid:88)d
‚ààB

S(k)
n,j ‚àá

fn(w(k),e(cid:48)
n
B(k)
n,j

1

‚àí

, d)

2

Ô£π

Ô£∫
Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

1

‚àí

, d)

(cid:98)
fn(w(k),e(cid:48)
n
B(k)
n,j

(iv)

‚â§

2Œ∑2
k

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
(cid:13)

1
D(k)
n

(cid:98)

(v)
= 2Œ∑2
k

1

e

‚àí

(cid:88)e(cid:48)=1

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£Ø
(cid:13)
Ô£∞
(cid:13)

1
D(k)
n

(cid:98)

(cid:124)

2

Ô£π

Ô£∫
Ô£∫
Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

1
D(k)
n

‚àí

1

e

‚àí

‚àá

(cid:88)e(cid:48)=1 (cid:88)d
(cid:98)
D
‚àà

(k)
n

fn(w(k),e(cid:48)
n

‚àí

1

, d) +

1
D(k)
n

1

e

‚àí

(cid:88)e(cid:48)=1 (cid:88)d
(cid:98)
D
‚àà

(k)
n

fn(w(k),e(cid:48)
n

‚àí

1

, d)

‚àá

S(k)
n

1

(cid:98)

e

‚àí

(cid:88)e(cid:48)=1

j=1
(cid:88)

(k),e(cid:48)
n,j

(cid:88)d
‚ààB

S(k)
n,j ‚àá

fn(w(k),e(cid:48)
n
B(k)
n,j

1

‚àí

, d)

‚àí

(cid:98)
e

1

‚àí

1
D(k)
n

+ 2Œ∑2
k

S(k)
n

1
D(k)
n

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
S(k)
(cid:13)
(cid:98)
n,j ‚àá

j=1
(cid:88)

(k),e(cid:48)
n,j

(cid:88)d
‚ààB

e

1

‚àí

(k)
n

‚àí

(cid:88)e(cid:48)=1 (cid:88)d
(cid:98)
D
‚àà
fn(w(k),e(cid:48)
n
B(k)
n,j

(cid:98)
fn(w(k),e(cid:48)
n

‚àá

1

, d)

1
D(k)
n

‚àí

(k)
n

(cid:88)e(cid:48)=1 (cid:88)d
(cid:98)
‚àà
D
2

Ô£π

1

‚àí

, d)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(k)
n

(cid:88)d
(cid:98)
D
‚àà

Ô£ª
fn(w(k),e(cid:48)
n

‚àá

fn(w(k),e(cid:48)
n

1

‚àí

‚àá

, d)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£π

Ô£∫
Ô£ª

1

‚àí

, d)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£π

Ô£∫
Ô£ª

(cid:125)

+ 2Œ∑2
k

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

1
D(k)
n

(cid:98)

(cid:98)

fn(w(k),e(cid:48)
n

1

‚àí

‚àá

(e)

e

1

‚àí

(cid:123)(cid:122)

(k)
n

(cid:88)e(cid:48)=1 (cid:88)d
(cid:98)
D
‚àà
(f )

2

,
Ô£π

Ô£ª

, d)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

20

(70)

where in inequalities (i) and (ii) in (69) we used Jenson‚Äôs inequality; and to obtain (70), in equality (iii) we used (5), in
(cid:123)(cid:122)
inequality (iv) we used Cauchy‚ÄìSchwarz inequality, and (v) uses the fact that each local gradient estimation is unbiased (i.e.,
zero mean) conditioned on its own local parameter and the law of total expectation (across the mini-batches e(cid:48)).

(cid:125)

(cid:124)

Similar to (64), using (65) we upper bound term (e) in (70) as follows:

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

2

.

(cid:17)

(71)

(e)

‚â§

4Œò2Œ∑2
k

e

1

‚àí

(cid:88)e(cid:48)=1

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

2

S(k)
n,j
D(k)
n
(cid:16)

(cid:98)

(cid:17)

(cid:101)

2

n (w(k))
F (k)

Also, for term (f ), we have

(f )

(i)

‚â§

2Œ∑2

k(e

1)

‚àí

(ii)

‚â§

4Œ∑2

k(e

‚àí

1)

4Œ∑2

k(e

1)

‚àí

‚â§

4Œ∑2

kŒ≤2(e

‚àí

‚â§

1

e

‚àí

(cid:88)e(cid:48)=1
1
e

‚àí

(cid:88)e(cid:48)=1
1
e
‚àí

(cid:88)e(cid:48)=1
e
‚àí

1)

(cid:88)e(cid:48)=1

(cid:98)
‚àá

1
D(k)
n

Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
Ek Ô£Æ
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
Ek Ô£Æ
‚àá
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
Ek Ô£Æ
w(k),e(cid:48)
n
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

1

fn(w(k),e(cid:48)
n

1

‚àí

, d)

n (w(k)) +
F (k)

‚àá

‚àí ‚àá

‚àá

2

(k)
n

(cid:88)d
(cid:98)
D
‚àà

n (w(k),e(cid:48)
F (k)
n

1

)

‚àí

‚àí ‚àá

n (w(k))
F (k)

Ô£π

+ 4Œ∑2

k(e

1)

1

e

‚àí

e

1

‚àí

‚àí

‚àí

1)

‚àá
(cid:88)e(cid:48)=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
‚àá
(cid:88)e(cid:48)=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

,

(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

Ô£ª
+ 4Œ∑2

k(e

1

e

‚àí

‚àá
(cid:88)e(cid:48)=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

n (w(k),e(cid:48)
F (k)
n

1

‚àí

)

‚àí ‚àá

2

n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
‚àí

+ 4Œ∑2

k(e

Ô£π

Ô£π

Ô£ª
1)

w(k)

1

‚àí

‚àí

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£π

(cid:13)
(cid:13)
(cid:13)
Ô£ª
(cid:13)
(cid:13)
n (w(k))
F (k)

n (w(k))
F (k)

2

(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(72)

Ô£ª
where inequalities (i) and (ii) are obtained via Cauchy-Schwarz inequality. Replacing the result of (71) and (72) back in (70)

21

,

(73)

2

n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

we have

w(k),e
n

‚àí

1

‚àí

Ek

w(k)

(cid:20)(cid:13)
(cid:13)
(cid:13)

2

(cid:21)

(cid:13)
(cid:13)
(cid:13)

4Œò2Œ∑2
k

‚â§

S(k)
n

e

1

‚àí

(cid:88)e(cid:48)=1

j=1 (cid:32)
(cid:88)

1

‚àí

+ 4Œ∑2

kŒ≤2(e

1)

‚àí

e

1

‚àí

(cid:88)e(cid:48)=1

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n
(cid:16)
(cid:98)
Ek Ô£Æ
w(k),e(cid:48)
n
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)

‚àí

(cid:17)
1

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

2

(cid:17)

w(k)

‚àí

2

Ô£π

Ô£ª

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

+ 4Œ∑2

k(e

1)

‚àí

which implies

e(k)
n

e=1
(cid:88)

Ek

w(k)

(cid:20)(cid:13)
(cid:13)
(cid:13)
e(k)
n
kŒ≤2

(e

+ 4Œ∑2

e=1
(cid:88)

‚â§

4Œò2Œ∑2
k

e(k)
n

(cid:16)

2

w(k),e
n

‚àí

1

‚àí

1)

‚àí

1

e

‚àí

(cid:88)e(cid:48)=1

e(k)
n ‚àí

(cid:17) (cid:16)

‚â§

(cid:21)

(cid:13)
(cid:13)
(cid:13)
Ek Ô£Æ
w(k),e(cid:48)
n
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
S(k)
(cid:13)
n

1

1
j=1 (cid:32)
(cid:88)
e(k)
n

(cid:17)

e(k)
n

e

‚àí

4Œò2Œ∑2
k

S(k)
n

1

e=1
(cid:88)

(cid:88)e(cid:48)=1

1
j=1 (cid:32)
(cid:88)
2

w(k)

1

‚àí

‚àí

+ 4Œ∑2
k

B(k)
n,j
S(k)
n,j (cid:33)

‚àí

Ô£π

(cid:13)
(cid:13)
(cid:13)
Ô£ª
(cid:13)
S(k)
(cid:13)
n,j
D(k)
n

2

2

B(k)
n,j
S(k)
n,j (cid:33)

‚àí

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

e

(cid:17)
1

‚àí

(cid:16)
(cid:98)
1)

‚àí

e(k)
n

(e

e=1
(cid:88)

‚àá
(cid:88)e(cid:48)=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

2

œÉ(k)
n,j

n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(S(k)

n,j ‚àí

1)

(cid:16)
B(k)
n,j

(cid:17)

(cid:101)

1

e

‚àí

‚àá
(cid:88)e(cid:48)=1 (cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
œÉ(k)
n,j

1)

(cid:16)
B(k)
n,j

(cid:17)

(cid:101)

2

+ 4Œ∑2

kŒ≤2

e(k)
n

(cid:16)

e(k)
n ‚àí

(cid:17) (cid:16)

1

(cid:17)

Assuming Œ∑k ‚â§

2Œ≤

n (e(k)
e(k)
n

(cid:18)

(cid:113)

1)

‚àí

e(k)
n

e=1
(cid:88)

Ek

w(k)

(cid:20)(cid:13)
(cid:13)
(cid:13)

‚àí

w(k),e
n

‚àí

1

(cid:16)
1

(cid:17)
(cid:98)
w(k)
‚àí

‚àí

Ek Ô£Æ
w(k),e
n
(cid:13)
(cid:13)
(cid:13)
Ô£∞
(cid:13)
(cid:13)
n, the above inequality implies

+ 4Œ∑2
k

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

Ô£ª

Ô£π

,

e=1
(cid:88)
1

‚àí

e(k)
n

(cid:16)

e(k)
n ‚àí

(cid:17) (cid:16)

1

(cid:17)

e(k)
n

‚àá

e=1 (cid:13)
(cid:13)
(cid:88)
(cid:13)
(cid:13)
(cid:13)

n (w(k))
F (k)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

,

(74)

‚àÄ

(cid:19)

2

‚â§

1

(cid:21)

‚àí

(cid:13)
(cid:13)
(cid:13)

+

4Œò2Œ∑2

n

ke(k)
(cid:16)
kŒ≤2e(k)

n

4Œ∑2

4Œ∑2
k

e(k)
n

e(k)
n

1

S(k)
n

(cid:17)
1

‚àí
e(k)
n
(cid:16)
2

‚àí
e(k)
n

(cid:17)
1

‚àí
e(k)
n

‚àí

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

(cid:17)

(cid:98)
.

(cid:16)
2

n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

‚àá

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)
1

(cid:17)

Replacing this result back in (69) we have

1

‚àí

(cid:17)
(cid:16)
(cid:16)
kŒ≤2e(k)
4Œ∑2
n

(cid:16)

(d)

‚â§

4Œ≤2Œò2Œ∑2
k

(cid:88)n
‚ààN

1

‚àí

(cid:16)
4Œ∑2

(cid:17) (cid:16)
kŒ≤2e(k)
n

+ 4Œ∑2

kŒ≤2

(cid:88)n
‚ààN

D(k)
n
D(k)e(k)
n
(cid:98)

= 4Œ≤2Œò2Œ∑2
k

(cid:88)n
‚ààN

1

‚àí

(cid:16)
4Œ∑2

(cid:17) (cid:16)
kŒ≤2e(k)
n

e(k)
n

e(k)
n

1

‚àí
e(k)
n
(cid:16)
e(k)
n

2

(cid:17)
‚àí

1

(cid:17)
e(k)
n

(cid:16)
(cid:17)
kŒ≤2e(k)
n

1

(cid:16)
4Œ∑2
‚àí
e(k)
n

e(k)
n

(cid:16)

1

D(k)
n
D(k)e(k)
n
(cid:98)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

1

‚àí
e(k)
n

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
B(k)
(cid:13)
n,j
S(k)
n,j (cid:33)

‚àá

(cid:17)
‚àí

1

(cid:13)
(cid:13)
(cid:13)
(cid:17)
(cid:13)
(cid:13)
D(k)
n
D(k)e(k)
n
(cid:98)

n (w(k))
F (k)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

2

1

‚àí

n (w(k))
F (k)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

‚àá

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:17)

‚àí
e(k)
n

(cid:17)
‚àí

1

(cid:17)

1

‚àí
(cid:17)
e(k)
n

(cid:16)

D(k)
n
D(k)

e(k)
n

(cid:16)
e(k)
n

4Œ∑2

(cid:16)
kŒ≤2e(k)

n

1

‚àí

+ 4Œ∑2

kŒ≤2

Assuming e(k)
we get

max = maxn

(cid:88)n
‚ààN (cid:98)
e(k)
n

‚ààN {

}

(75)

2

(cid:17)

2

(cid:17)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

2

S(k)
n,j
D(k)
n
(cid:16)

(cid:98)

(cid:17)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(cid:101)

(76)

, and using the bounded dissimilarity assumption among the local gradients (Assumption 2),

(d)

‚â§

4Œ≤2Œò2Œ∑2
k

(cid:88)n
‚ààN

1

‚àí

(cid:16)
4Œ∑2

(cid:17) (cid:16)
kŒ≤2e(k)
n

e(k)
n

e(k)
n

1

‚àí
e(k)
n
(cid:16)

(cid:17)
‚àí

1

(cid:17)

D(k)
n
D(k)e(k)
n
(cid:98)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

2

S(k)
n,j
D(k)
n
(cid:16)

(cid:98)

(cid:17)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

22

(77)

2

(cid:17)

(78)

(79)

(80)

(81)

4Œ∑2

kŒ≤2

e(k)
max

e(k)
max

+

1

‚àí

(cid:16)
kŒ≤2e(k)
4Œ∑2

max

(cid:17) (cid:16)

(cid:16)

‚àí

e(k)
max

‚àí
e(k)
n

1

(cid:17)
1

Œ∂1

Ô£´

(cid:17)

Ô£≠
e(k)
n

= 4Œ≤2Œò2Œ∑2
k

1

(cid:16)
4Œ∑2

(cid:17) (cid:16)
kŒ≤2e(k)
n

D(k)
n
D(k) ‚àá

n (w(k))
F (k)

2

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
‚àí
e(k)
n

(cid:88)n
‚ààN (cid:98)
1

(cid:17)
‚àí

1

D(k)
n
D(k)e(k)
n
(cid:98)

S(k)
n

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
1
j=1 (cid:32)
(cid:88)

‚àí

(cid:88)n
‚ààN
e(k)
max

4Œ∑2

kŒ≤2

‚àí
e(k)
max

+

1

(cid:13)
(cid:13)
(cid:13)
Replacing this result back in (68) and gathering the terms leads to

(cid:13)
(cid:13)
(cid:13)

‚àí

‚àí

(cid:17)

(cid:16)

(cid:16)
kŒ≤2e(k)
4Œ∑2

max

(cid:17) (cid:16)

e(k)
max

(cid:17)
1

(cid:18)

‚àá

1

‚àí

(cid:16)

Œ∂1

(cid:17)
F (k)(w(k))

2

+ Œ∂2

.

(cid:19)

+ Œ∂2Ô£∂
Ô£∏
B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

Ek

F (k)(w(k+1))

(cid:104)

(cid:105)

F (k)(w(k)) +

‚â§

Œ∑k
2

(cid:88)n
‚ààN (cid:98)

D(k)

n e(k)
n
D(k) Ô£´

4Œ∑2

kŒ≤2

e(k)
max

e(k)
max

(cid:16)
kŒ≤2e(k)
4Œ∑2

max

(cid:17) (cid:16)

e(k)
max

1

‚àí

Ô£≠

‚àí

‚àí

1

(cid:17)
1

(cid:17)

Œ∂1 ‚àí

1
Ô£∂

Ô£∏

2

F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)

‚àá

(cid:13)
(cid:13)
(cid:13)

e(k)
n

e(k)
n

(cid:88)n
‚ààN

1

‚àí

(cid:16)
4Œ∑2

(cid:17) (cid:16)
kŒ≤2e(k)
n

(cid:16)

1

(cid:124)
‚àí
e(k)
n

(cid:17)
‚àí

1

(cid:17)

D(k)
n
D(k)e(k)
n
(cid:98)

(cid:16)
(g)

(cid:123)(cid:122)
S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

(cid:125)
S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

+

Œ∑k
2

D(k)
e(k)
n(cid:48)
n(cid:48)
D(k) Ô£´
Ô£¨
Ô£¨
Ô£≠

(cid:88)n(cid:48)‚ààN (cid:98)

4Œ≤2Œò2Œ∑2
k

4Œ∂2Œ∑2

kŒ≤2

e(k)
max

e(k)
max

+

4Œ∑2

(cid:16)
kŒ≤2e(k)

1

‚àí

1

‚àí
1

(cid:17)

‚àí

2

(cid:17)

Ô£∂

Ô£∑
Ô£∑
Ô£∏

(cid:17) (cid:16)

max

e(k)
max
(cid:16)
e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

(cid:32)

(cid:88)n(cid:48)‚ààN (cid:98)

(cid:33)

(cid:32)

(cid:88)n
‚ààN

+ 2Œò2Œ≤Œ∑2
k

D(k)
n
D(k)e(k)
(cid:98)

n (cid:33)

2

S(k)
n

e(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

.

(cid:17)

With a proper choice of step size, we aim to make term (g) negative, for which we Ô¨Årst impose the following condition on the de-
nominator of the fraction in (g) (the following condition is equivalent to our prior condition: Œ∑k ‚â§

n (e(k)
e(k)
n

n):

2Œ≤

1)

‚àí

(cid:98)

‚àÄ

‚àí

1

,

(cid:16)

(cid:17)

(cid:18)

(cid:113)

(cid:19)

4Œ∑2

kŒ≤2e(k)
max

1

‚àí

e(k)
max ‚àí

1

(cid:17)

(cid:16)

0

Œ∑k ‚â§

‚áí

‚â•

1
e(k)
max

1
2Œ≤ (cid:118)
(cid:117)
(cid:117)
(cid:116)

e(k)
max

(cid:16)

.

1

‚àí

(cid:17)

We also assume that there exist a set of constants Œõ(k),

k, where

‚àÄ

4Œ∑2

kŒ≤2

e(k)
max

e(k)
max

(cid:16)
kŒ≤2e(k)
4Œ∑2

max

(cid:17) (cid:16)

e(k)
max

1

‚àí

‚àí

‚àí

1

(cid:17)
1

Œõ(k) < 1

Œ∂1 ‚â§

‚áí

1

1
kŒ≤2e(k)

max

4Œ∑2

‚àí

Œ∂1 + Œõ(k)
Œ∂1

,

‚â§

e(k)
max

1

‚àí

(cid:16)

(cid:17)

(cid:16)
which can be obtained under the following condition on the step size:

(cid:17)

Œ∑k ‚â§

1
2Œ≤ (cid:118)
(cid:117)
(cid:117)
(cid:116)

(Œ∂1 + Œõ(k))

Œõ(k)
e(k)
max

,

e(k)
max

1

‚àí

(cid:16)

(cid:16)

(cid:17)(cid:17)

which is a tighter condition on the step size as compared to (79). Let us further deÔ¨Åne Œì(k) = Œ∑k
2
conditions/assumptions, we simplify (78) as follows:
Ek
‚àí
Œì(k)(1

F (k)(w(k+1))
Œõ(k))

F (k)(w(k))

F (k)(w(k))

‚àá

‚â§

2

(cid:98)D(k)

n e(k)
n
D(k)

. Under these

n

‚ààN

(cid:80)

(cid:13)
(cid:13)
(cid:13)

1
Œõ(k)) (cid:32)

+

(1

‚àí

(cid:13)
(cid:13)
(cid:13)
4Œ≤2Œò2Œ∑2
k

(cid:2)
‚àí

(cid:3)

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

Œ∂1 + Œõ(k)
Œ∂1

1)

(cid:18)

S(k)
n

1
j=1 (cid:32)
(cid:88)

(cid:19)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

23

(82)

2

(cid:17)

+ 4Œ∂2Œ∑2

kŒ≤2

e(k)
max

(cid:16)

+

4Œò2Œ≤Œ∑k
(1

Œõ(k)) (cid:32)

‚àí

(cid:88)n(cid:48)‚ààN (cid:98)

e(k)
max ‚àí

(cid:17) (cid:16)
D(k)
e(k)
n(cid:48)
n(cid:48)
D(k)

Œ∂1 + Œõ(k)
Œ∂1

1

(cid:17)

D(k)
n
D(k)
(cid:98)

(cid:33)

(cid:88)n
‚ààN

(cid:32)

(cid:33)

2

(cid:33)

1
e(k)
n

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

.

(cid:17)

Taking total expectation and averaging across global aggregations, we have:

1
K

K

1

‚àí

E

(cid:88)k=0

‚àá

(cid:13)
(cid:13)
(cid:13)

F (k)(w(k))

1
K

‚â§

2

(cid:13)
(cid:13)
(cid:13)

(cid:88)k=0 (cid:34)

(cid:2)

K

1

‚àí

E

F (k)(w(k))

E

‚àí
Œì(k)(1
(cid:3)

‚àí

(cid:16)

(cid:98)
F (k)(w(k+1))
Œõ(k))
(cid:2)

(cid:3)

(cid:17)

(cid:35)

(cid:124)
4Œ≤2Œò2Œ∑2
k

1
Œõ(k)) (cid:32)

D(k)
n
D(k) (e(k)

n ‚àí

+

1
K

K

1

‚àí

Ô£Æ

(1

‚àí

(cid:88)k=0

Ô£Ø
Ô£Ø
Ô£∞
kŒ≤2
+ 4Œ∂2Œ∑2

(cid:88)n
‚ààN (cid:98)
Œ∂1 + Œõ(k)
Œ∂1

e(k)
max

(cid:16)

e(k)
max ‚àí

1

(cid:17)

(cid:17) (cid:16)
e(k)
D(k)
n(cid:48)
n(cid:48)
D(k)

+

4Œò2Œ≤Œ∑k
(1

Œõ(k)) (cid:32)

‚àí

(cid:88)n(cid:48)‚ààN (cid:98)

D(k)
n
D(k)
(cid:98)

(cid:33)

(cid:88)n
‚ààN

(cid:32)

(a)

1)

(cid:123)(cid:122)
Œ∂1 + Œõ(k)
Œ∂1

(cid:18)

S(k)
n

1
j=1 (cid:32)
(cid:88)

(cid:19)

‚àí

(cid:125)
B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

(cid:33)

2

(cid:33)

1
e(k)
n

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

(cid:16)

(cid:17)

Ô£∫
Ô£∫
Ô£ª

2

.
Ô£π

(cid:17)

(83)

‚àà

We next focus on the Ô¨Årst term on the right hand side of (82) and aim to upper bound it. Let tk denotes the wall-clock time
index (in seconds) in which global aggregation k begins and t(cid:48)k = tk + T Tot,(k) denotes the time in which global aggregation k
[t(cid:48)k, tk+1] is the idle time in between global model training rounds of k and k + 1 and the length
concludes. In particular, t
of the idle time is given by ‚Ñ¶(k+1) = tk+1 ‚àí
t(cid:48)k. To avoid confusion between wall-clock time index and global aggregation
index, we let
Dn(t) denotes the dataset of device n at wall-clock time t after all the data arrivals and departures occur (and
(k)
n denotes the respective dataset possessed by node n used to obtain the model for global
for global dataset
(k)). Note that we assume that the data arrival/departures only happen during
(cid:98)
aggregation k + 1 (and for the global dataset
the idle time in between the global aggregations, so during the period of each device acquisition time, the loss functions are
stationary. Noting that F (k)(w(k)) is the loss function under which global aggregation k + 1 starts from (at tk+1)7, we bound
it in terms of F (k

1)(w(k)), i.e., the loss under which global aggregation k concludes as follows:

(t)), while

D

D

D

(cid:98)

(cid:98)

(cid:98)

(cid:98)

‚àí

F (k)(w(k)) = F (w(k)

D
|

(cid:98)

(tk+1))

F (w(k)

(tk+1 ‚àí

|

D

1)) + F (w(k)

(tk+1 ‚àí

|

D

1))

‚àí
(tk+1 ‚àí
|

‚àí

1))

1)

|

2))
‚àí ¬∑ ¬∑ ¬∑
(cid:98)
Dn(tk+1 ‚àí
1)
(tk+1 ‚àí
D
|
(cid:98)
+ |

(cid:98)
‚àí ¬∑ ¬∑ ¬∑

+ F (w(k)

(t(cid:48)k))

|
Fn(w(k)

D
(cid:98)
Dn(tk+1 ‚àí
(cid:98)
|
(cid:98)
Fn(w(k)

|

1))

|

Dn(t(cid:48)k))
(cid:35)
(cid:98)

D
|

2)) + F (w(k)

|

‚àí

=

(k)) = F (w(k)
(tk+1)) = F (w(k)
D
|
F (w(k)
(tk+1 ‚àí
D
|
(cid:98)
Dn(tk+1)
|
(cid:98)
(tk+1)
|
D
|
(cid:98)
1)
Dn(tk+1 ‚àí
(cid:98)
|
1)
(tk+1 ‚àí
|
D
|
(cid:98)
t(cid:48)k)
(tk+1 ‚àí
(cid:98)

(cid:88)n
‚ààN
+ |

Fn(w(k)

Fn(w(k)

‚àÜ(k+1)
n

+

‚â§

(cid:34)

|

|
D
(cid:98)
Dn(tk+1))
(cid:98)
|
(cid:98)
Dn(tk+1 ‚àí
(cid:98)

= ‚Ñ¶(k+1)

(cid:88)n
‚ààN
‚àÜ(k+1)
n

+

= ‚Ñ¶(k+1)

(cid:88)n
‚ààN

‚àÜ(k+1)
n

|

(cid:88)n
D
(cid:98)
‚ààN
+ F (w(k)
(cid:98)

|

D

‚àí

(k

|
‚àí

Fn(w(k)

Dn(t(cid:48)k)
|
|
(t(cid:48)k)
|
D
|
(cid:98)
1)
‚àí
Fn(w(k)
(cid:98)
1)

|

(cid:88)n
‚ààN
(k
n
D
|

(k

(k
n
D
|

|
Dn(t(cid:48)k)
(t(cid:48)k)
|
|
D
(cid:98)
Dn(t(cid:48)k))
(cid:98)
(cid:98)
1)
‚àí

)

|

1)) = ‚Ñ¶(k+1)‚àÜ(k+1) + F (k

(cid:98)

1)(w(k)),

‚àí

(84)

n

(cid:88)n
‚ààN
where ‚àÜ(k+1)
n (T Idle,(k+1) denotes the set of idle time instances between global aggregations k + 1
T Idle,(k+1) ‚àÜn(t),
= maxt
‚àÄ
‚àà
‚àÜ(k+1)
1)(w(0)) denote the initial loss of the algorithm before model training starts,
. Let F (
and k), and ‚àÜ(k+1) =
n
using the above upper bound we have F (0)(w(0))
1)(w(0)) + ‚Ñ¶(1)‚àÜ(1), where F (0)(w(0)) is the initial loss that the Ô¨Årst
(cid:80)
global model training round starts from after having the idle period of ‚Ñ¶1. Also using the above bound recursively in the term
(a) of (83), we get

F (

‚ààN

‚â§

(cid:98)

‚àí

‚àí

n

7Note that we have assumed that to obtain the local model parameters to conduct the k + 1-th global aggregation, we have assumed that each device n uses
(cid:98)D(k)
n .

Using (82) and (84), applying Œ∂1+Œõ(k)

Œ∂1

< 2 yields

1
K

+

K

1

‚àí

(cid:88)k=0

K

1

‚àí

(cid:88)k=0

E

(cid:107)‚àá

F (k)(w(k))

2
(cid:107)

‚â§

1
K

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

1
Œõ(k)) (cid:32)

8Œ≤2Œò2Œ∑2
k

(1

‚àí

(cid:88)n
‚ààN (cid:98)

K

1

‚àí

E

F (k

‚àí

1)(w(k))

(cid:88)k=0

(cid:2)

Œì(k)(1
(cid:3)

E

F (k)(w(k+1))

Œõ(k))
(cid:2)

‚àí
‚àí

+

(cid:3)

K

1

‚àí

(cid:88)k=0

‚Ñ¶(k+1)‚àÜ(k+1)
Œõ(k))
Œì(k)(1

‚àí

D(k)
n
D(k) (e(k)

n ‚àí

S(k)
n

1)

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

+ 8Œ∂2Œ∑2

kŒ≤2

e(k)
max

(cid:16)

(cid:17) (cid:16)

e(k)
max ‚àí

1

+

K

1

‚àí

(cid:88)k=0

8Œò2Œ≤Œì(k)
Œõ(k))
(1

‚àí

(cid:32)

(cid:88)n
‚ààN

which concludes the proof.

(cid:17)
D(k)
n
D(k)
(cid:98)

(cid:33)

2

(cid:33)

1
e(k)
n

S(k)
n

j=1 (cid:32)
(cid:88)

B(k)
n,j
S(k)
n,j (cid:33)

1

‚àí

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

,
Ô£π

Ô£∫
Ô£∫
Ô£ª

24

(85)

25

(86)

‚Ñ¶(k+1)‚àÜ(k+1)

Œ∑ke(k)

avg/2

(1

Œõ(k))

‚àí

(cid:16)
1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:17)
2

(cid:17)

(cid:101)

2

.
Ô£π

(cid:17)

APPENDIX C
PROOF OF COROLLARY 1

1
K

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

K

1

E

‚àí

F (k

‚àí

(cid:88)k=0

(cid:2)

D(k)
n
D(k) (e(k)

n ‚àí

1)(w(k))
Œ∑ke(k)

avg/2
(cid:3)

(cid:16)

S(k)
n

(cid:17)

1)

1
j=1 (cid:32)
(cid:88)

‚àí

(cid:88)n
‚ààN (cid:98)

+

K

1

‚àí

(cid:88)k=0

(S(k)

n,j ‚àí

(cid:3)

2

E

F (k)(w(k+1))

‚àí

Œõ(k))

(1

(cid:2)
‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

(cid:17)

(cid:16)

(cid:98)

Considering (85), we have

1
K

+

K

1

‚àí

(cid:88)k=0

K

1

‚àí

(cid:88)k=0

E

(cid:107)‚àá

2
F (k)(w(k))
(cid:107)

‚â§

1
Œõ(k)) (cid:32)

8Œ≤2Œò2Œ∑2
k

(1

‚àí

e(k)
max ‚àí

1

(cid:33)

(cid:17)

+ 8Œ∂2Œ∑2

kŒ≤2

e(k)
max

(cid:16)
8Œò2Œ≤

(1

+

K

1

‚àí

(cid:88)k=0

(cid:16)
‚àí

(cid:17) (cid:16)
Œ∑ke(k)
Œõ(k))

avg/2

2

(cid:33)

1
e(k)
n

D(k)
n
D(k)
(cid:98)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

(cid:17)

(cid:32)

(cid:88)n
‚ààN

Assuming Œ∑k = Œ±

where e(k)

avg =

n

‚ààN

1
‚àöe(k)

sum K/N
n e(k)
(cid:98)D(k)
n
D(k)

with a Ô¨Ånite positive constant Œ±, maxk

, and

emin ‚â§

e(k)
sum

‚â§

emax, we get

(cid:17)
Œõmax < 1, (emax)‚àí

‚â§

(cid:16)
(cid:98)
Œõ(k)

(cid:8)

(cid:9)

Ô£∫
Ô£∫
Ô£ª

‚â§

1

1

‚àí

‚â§

e(k)
avg
(cid:16)

(cid:17)

(emin)‚àí

1,

(cid:80)
1
K

+

K

1

‚àí

E

(cid:104)
1

‚àí

(cid:88)k=0
K
1
K

(cid:88)k=0
Œ±2N
Ke(k)
sum

+ 8Œ∂2

(cid:98)

F (k)(w(k))

(cid:98)
emax

2

2
(cid:107)

‚â§

(cid:107)‚àá

(cid:105)
(cid:112)
(cid:98)
8Œ≤2Œò2 Œ±2N
Ke(k)
sum

1
Œõmax) (cid:32)

(1

‚àí

‚àí

F (
1)(w(0))
‚àí
eminŒ±‚àöN K(1
‚àí

F (K)(cid:63)
Œõmax)

+

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

S(k)
n

1)

1
j=1 (cid:32)
(cid:88)

2‚àö

emax

K

1

‚àí

‚Ñ¶(k+1)‚àÜ(k+1)

eminŒ±‚àöN K
(cid:98)
B(k)
n,j
S(k)
n,j (cid:33)

‚àí

(cid:88)k=0
S(k)
n,j
D(k)
n

2

Œõmax

1

‚àí
(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

(cid:17)

Œ≤2

e(k)
max

e(k)
max ‚àí

(cid:16)
(cid:17) (cid:16)
4emaxŒ±Œò2Œ≤‚àöN
e(k)
sum‚àöK (cid:88)n

Œõmax)

‚ààN

‚àí

(cid:113)

+

1
K

K

1

‚àí

(cid:88)k=0

(1

1

(cid:17)

(cid:16)

(cid:98)

(cid:33)

(cid:17)

D(k)
n
D(k)
(cid:98)

(cid:32)

2

(cid:33)

1
e(k)
n

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(cid:17)

(cid:16)

(cid:98)

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

2

,

(cid:17)

(87)

which concludes the proof.

Considering (87), assuming a bounded stratiÔ¨Åed sampling noise: maxk,n

APPENDIX D
PROOF OF COROLLARY 2

œÉmax,
n, k, bounded local iterations, maxk{
‚àÄ
for a Ô¨Ånite non-negative constant Œ≥, we have

e(k)
max

emax,

emin ‚â§

} ‚â§

e(k)
sum

‚â§

S(k)
n
j=1

1

(cid:40)

(cid:18)
(cid:80)
emax, and ‚àÜ(k)

B(k)
n,j
S(k)
n,j (cid:19)
Œ≥
K‚Ñ¶(k)

(cid:16)

S(k)
n,j
(cid:98)D(k)
+,

n

‚àí

‚â§

(S(k)

n,j ‚àí

(cid:17)2

k, maxk{
‚àÄ

(cid:16)

(cid:101)œÉ(k)

n,j

1)
B(k)
n,j
e(k)
max

(cid:41) ‚â§
emax

} ‚â§

26

(cid:17)2

1
K

+

+

‚â§

+

E

‚àí

K

1

‚àí

(cid:88)k=0
K
1
K

(cid:88)k=0
1
K
‚àí

(cid:88)k=0
emax

1
K

2

1

(cid:112)
K
1
‚àí
(cid:98)
K

(cid:107)‚àá
(cid:104)
1

2

2

(1

F (k)(w(k))
‚â§
(cid:107)
(cid:105)
(cid:112)
8Œ≤2Œò2 Œ±2N
(cid:98)
eminK

1
Œõmax) (cid:32)
‚àí
4emaxŒ±Œò2Œ≤‚àöN
(cid:98)
emin‚àöK
Œõmax)‚àö
(1
‚àí
F (K)(cid:63)
F (
1)(w(0))
‚àí
‚àí
(cid:98)
eminŒ±‚àöN K(1
Œõmax)
‚àí
8Œ≤2Œò2 Œ±2N
eminK

œÉmax

+

(1

œÉmax

(cid:88)k=0
1
K
‚àí

(cid:88)k=0
emax

1
Œõmax) (cid:32)
‚àí
4emaxŒ±Œò2Œ≤‚àöN
Œõmax)‚àö
(1
‚àí
F (
1)(w(0))
‚àí
‚àí
(cid:98)
eminŒ±‚àöN K(1
‚àí
1
8Œ≤2Œò2Œ±2N (emax ‚àí
Œõmax) (cid:32)
4emaxŒ±Œò2Œ≤‚àöN
Œõmax)‚àö

(cid:98)
emin‚àöK
F (K)(cid:63)
Œõmax)

emin‚àöK

œÉmax,

(cid:98)
(1

‚àí

+

+

1
K

= 2

+

+

(cid:112)
1
K

(1

‚àí

(cid:2)
emax

2‚àö

(cid:3)
1

K

‚àí

eminŒ±‚àöN K
(cid:88)k=0
(cid:98)
Œ≤2 (emax) (emax ‚àí

K(1

1)

(cid:33)

Œ≥

‚àí

Œõmax)

emax

(cid:98)
F (
1)(w(0))
‚àí
‚àí
eminŒ±‚àöN K(1
‚àí

(cid:98)
F (K)(cid:63)
Œõmax)

+

(emax ‚àí

1)œÉmax + 8Œ∂2

Œ±2N
eminK

(cid:98)

2‚àö

emaxŒ≥

eminŒ±‚àöN K(1

Œõmax)

‚àí

(cid:98)

(emax ‚àí

1)œÉmax + 8Œ∂2

Œ≤2 (emax) (emax ‚àí

1)

(cid:33)

Œ±2N
eminK

(cid:98)

2‚àö

emaxŒ≥

eminŒ±‚àöN K(1

Œõmax)

‚àí

1)œÉmax/

(cid:98)
emin + 8Œ∂2Œ±2Œ≤2N (emax) (emax ‚àí

(cid:98)

1)/

emin

(cid:33)

(cid:98)

(88)

which concludes the proof.

(cid:98)

APPENDIX E
PROOF OF PROPOSITION 1

We focus on the term described in (71), and bound the following summation:

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:101)

27

(89)

2

.

(cid:17)

Since the dataset is Ô¨Åxed during each round of local model training and the sampling of the mini-batches from different strata
is conducted with respect to the variance of the data at each minibatch, we can omit the index of local SGD iteration e(cid:48) from
the above expression to get

(cid:98)

(cid:16)

(cid:17)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
S(k)
n,j (cid:33)

S(k)
n,j
D(k)
n

2

(S(k)

n,j ‚àí

2

1)

œÉ(k)
n,j

(cid:16)
B(k)
n,j

(cid:17)

‚â§

(cid:101)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

2

S(k)
n,j

(cid:17)
D(k)
n

2 (cid:16)

2

.

œÉ(k)
n,j
(cid:17)
B(k)
n,j
(cid:101)

(90)

B(k)
n,j
n,j (cid:33) (cid:16)
S(k)
(cid:16)

(cid:17)

Exploiting a similar technique used in [46] (see Eq. (39) of [46]), it can be shown that the the right hand side of (90) can be
written as

(cid:98)

(cid:98)

(cid:16)

(cid:17)

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
n,j (cid:33) (cid:16)
S(k)
(cid:16)

2

S(k)
n,j

(cid:17)
D(k)
n

2 (cid:16)

2

œÉ(k)
n,j
(cid:17)
B(k)
n,j
(cid:101)

=

D(k)
n
B(k)
n
(cid:98)

B(k)
n
‚àí
2
D(k)
n

S(k)
n

j=1
(cid:88)

S(k)
n,j

2

œÉ(k)
n,j
(cid:16)

(cid:17)

(cid:17)

(cid:98)

+

(cid:16)
S(k)
n

j=1
(cid:88)

(cid:17)

(cid:98)
B(k)

n,j Ô£´

S(k)
n,j

(cid:101)
œÉ(k)
n,j
D(k)
n
(cid:101)

S
2 ‚àí (cid:80)

1
D(k)
B(k)
n
n

‚àí

B(k)
n,j

Ô£¨
Ô£≠

S(k)
n

j=1
(cid:88)

(cid:16)
(cid:98)
S(k)
n,j Ô£´

(cid:17)
œÉ(k)
S
n,j ‚àí (cid:80)

Ô£¨
Ô£≠(cid:101)

(k)
n,j(cid:48) ‚ààS
B(k)
n

(k)
n,j(cid:48) ‚ààS

S(k)
n,j(cid:48)

œÉ(k)
n,j(cid:48)

(k)
n

2
(cid:101)

D(k)
n
(cid:17)
(cid:16)
S(k)
(cid:98)
(k)
n,j(cid:48)
n

D(k)
n

œÉ(k)
n,j(cid:48)

(cid:101)

2

Ô£∂

Ô£∑
Ô£∏
2

.

Ô£∂

(91)

Ô£∑
Ô£∏

Considering the fact that only the term on the second line is dependent on the mini-batch size (B(k)
expression can be achieved with the choice of mini-batch size B(k)

n,j ), minimization of this
n, j. We subsequently simplify

n (cid:101)œÉ(k)

B(k)

(cid:98)

(cid:98)

,

n,j =

(cid:80)

n,j

n,j S(k)
n (cid:101)œÉ(k)

(k)

n,j(cid:48) |S

(k)
n,j(cid:48) |

‚àÄ

(k)
n,j(cid:48) ‚ààS

the above expression as follows:

S

S(k)
n

1
j=1 (cid:32)
(cid:88)

‚àí

B(k)
n,j
n,j (cid:33) (cid:16)
S(k)
(cid:16)

2

S(k)
n,j

(cid:17)
D(k)
n

2 (cid:16)

2

œÉ(k)
n,j
(cid:17)
B(k)
n,j
(cid:101)

S(k)
n

=

1
B(k)
n,j

‚àí

(cid:98)
Ô£´

(cid:80)

2 Ô£Æ

S

(cid:17)
(k)
n,j(cid:48) ‚ààS
B(k)

(k)
n

|S
n S(k)
n,j

(k)
n,j(cid:48)|
œÉ(k)
n,j

j=1 (cid:32)
(cid:88)
œÉ(k)
n,j(cid:48)

(cid:101)

1
n,j (cid:33) (cid:16)
S(k)
(cid:16)
œÉ(k)
n,j
(cid:16)

(cid:17)

2

S(k)
n,j

(cid:17)
D(k)
n

2

2

2

œÉ(k)
n,j
(cid:16)

(cid:17)

(cid:101)
(cid:17)
S(k)
n,j

‚àí

œÉ(k)
n,j
(cid:16)

(cid:98)
2

(cid:17)

S(k)
n

j=1
(cid:88)
S(k)
n

j=1
(cid:88)

1
D(k)
n

(cid:16)

(cid:98)

1
D(k)
n

(i)
=

=

=

Ô£Ø
Ô£∞

Ô£¨
Ô£≠

(cid:17)

(k)
n,j(cid:48) ‚ààS

2 Ô£Æ

Ô£´

S
(cid:80)

(k)
n

œÉ(k)
(cid:101)
n,j(cid:48)|S

(k)
n,j(cid:48)|

B(k)
n
(cid:101)

2

S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

(cid:98)

(cid:16)
1
D(k)
n

(cid:17)

Ô£¨
Ô£≠

Ô£Ø
Ô£∞
1
B(k)
n Ô£´
Ô£≠

Ô£Ø
Ô£∞

S(k)
n

j=1
(cid:88)

2 Ô£Æ

S(k)
n,j

Ô£∂

(cid:16)

Ô£∑
Ô£∏
S(k)
n,j

Ô£∂

(cid:101)
S(k)
n,j

œÉ(k)
n,j ‚àí

Ô£∑
Ô£∏
S(k)
n,j

(cid:101)
œÉ(k)
n,j

(cid:16)

(cid:17)

B(k)

(cid:101)
n,j S(k)
n (cid:101)œÉ(k)
n (cid:101)œÉ(k)

n,j

(k)

n,j(cid:48) |S

2

Ô£π

Ô£∫
Ô£ª

(k)
n,j(cid:48) |

j=1
(cid:88)

(cid:80)

(k)
n,j(cid:48) ‚ààS

S

(cid:101)

2

œÉ(k)
n,j

(cid:17)

(cid:16)

(cid:101)

Ô£π

Ô£∫
Ô£ª

2

(cid:17)

Ô£π

Ô£∫
Ô£ª

(92)

Ô£∏
where in equality (i) we have used the sampling rule: B(k)
(cid:98)
n,j =

(cid:17)

(cid:16)

(cid:101)

parameter in the above inequality is the mini-batch size B(k)
n
parameters are Ô¨Åxed.

Replacing the above result in (85), we get

. It is worth noting that the only control

appearing in the Ô¨Årst term inside the bracket, while the rest of the

1
K

K‚àí1
(cid:88)

k=0

(cid:104)

(cid:107)‚àáF (k)(w(k))(cid:107)2(cid:105)

E

‚â§

Ô£Æ

Ô£Ø
Ô£Ø
Ô£Ø
Ô£Ø
Ô£∞

1
K

K‚àí1
(cid:88)

k=0

(cid:104)

E

F (k‚àí1)(w(k))

(cid:105)

‚àí E

(cid:104)

(cid:105)
F (k)(w(k+1))

Œì(k)(1 ‚àí Œõ(k))

+

K‚àí1
(cid:88)

k=0

‚Ñ¶(k+1)‚àÜ(k+1)
Œì(k)(1 ‚àí Œõ(k))

28

Ô£´

+

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõ(k))

8Œ≤2Œò2Œ∑2
k

Ô£¨
Ô£¨
Ô£≠

(cid:88)

n‚ààN

n

(cid:98)D(k)
D(k) (e(k)

n ‚àí 1)

‚àí

(k)

S
n(cid:88)

j=1

S(k)
n,j

(cid:17)2

(cid:16)
(cid:101)œÉ(k)

n,j

Ô£π

Ô£∫
Ô£∫
Ô£ª

+ 8Œ∂2Œ∑2

kŒ≤2 (cid:16)

e(k)
max

(cid:17) (cid:16)

e(k)
max ‚àí 1

(cid:17)

Ô£∑
Ô£∑
Ô£∏

Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(k)

S
n(cid:88)

Ô£´

Ô£≠

j=1

1
B(k)
n

2

Ô£∂

(cid:101)œÉ(k)
n,jS(k)

n,j

Ô£∏

(cid:17)2

1
(cid:16)
(cid:98)D(k)
Ô£∂

n

+

K‚àí1
(cid:88)

k=0

8Œò2Œ≤Œì(k)
(1 ‚àí Œõ(k))

(cid:88)

n‚ààN

(cid:33)2

(cid:32)

(cid:98)D(k)
n
D(k)

1
e(k)
n

1
(cid:98)D(k)

n

(cid:16)

(cid:17)2

Ô£Æ

Ô£Ø
Ô£∞

1
B(k)
n

(k)

S
n(cid:88)

Ô£´

Ô£≠

j=1

2

Ô£∂

(cid:101)œÉ(k)
n,jS(k)

n,j

Ô£∏

‚àí

(k)

S
n(cid:88)

j=1

S(k)
n,j

(cid:17)2

(cid:16)

(cid:101)œÉ(k)

n,j

Ô£π

Ô£∫
Ô£ª

Ô£π

Ô£∫
Ô£∫
Ô£∫
Ô£∫
Ô£ª

.

Considering Œ∑k = Œ±

1
‚àöe(k)

sum K/N

, deÔ¨Åning e(k)

avg =

n

‚ààN

(cid:98)D(k)

n e(k)
n
D(k)

since Œì(k) = Œ∑k
2

n

‚ààN

(cid:98)D(k)
n e(k)
D(k) = Œ∑k

2 e(k)

n

avg, we get

(cid:80)
K
‚àí

1

(cid:88)k=0

2
(cid:113)

e(k)
sum

E

(cid:0)

(cid:2)

F (k
1)(w(k))
‚àí
Œ±e(k)
avg‚àöN K(1
(cid:3)

‚àí

‚àí

(cid:80)
E

F (k)(w(k+1))

Œõ(k))
(cid:2)

(cid:3)(cid:1)

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

1)

1
D(k)
n

2

S(k)
n

1
B(k)
n Ô£´
Ô£≠

n,jS(k)
œÉ(k)

n,j Ô£∂

(cid:101)

Ô£∏

j=1
(cid:88)

2 Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(cid:17)

F (k)(w(k))

2
(cid:107)

‚â§

(cid:107)‚àá
(cid:104)

(cid:105)
e(k)
sum‚Ñ¶(k+1)‚àÜ(k+1)
2
(cid:113)
Œ±e(k)
avg‚àöN K(1

Œõ(k))

‚àí
8Œ≤2Œò2 Œ±2N
e(k)
sumK 2

1
Œõ(k))

(1

‚àí

Ô£´

Ô£¨
Ô£¨
Ô£≠
2

S(k)
n,j

œÉ(k)
n,j

Ô£π

+ 8Œ∂2

(cid:16)

(cid:17)

Ô£∫
Ô£∫
Ô£ª
avgŒ±Œò2Œ≤‚àöN

(cid:101)
4e(k)

K

1

‚àí

E

1
K

+

+

‚àí

+

(cid:88)k=0
1
K
‚àí

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

S(k)
n

j=1
(cid:88)

K

1

‚àí

(cid:88)k=0

e(k)
sumK‚àöK(1

Œõ(k)) (cid:88)n

‚ààN

‚àí

(cid:113)

(cid:16)

(cid:98)
Ô£∂

Ô£∑
Ô£∑
Ô£∏

Œ±2N
e(k)
sumK 2

Œ≤2

e(k)
max

(cid:16)

e(k)
max ‚àí

(cid:17) (cid:16)

1

(cid:17)

2

(cid:33)

1
e(k)
n

D(k)
n
D(k)
(cid:98)

(cid:32)

1
D(k)
n

2

To remove the dependency of the bound on F (k)(w(k+1)), 1
further upper bound the Ô¨Årst term on the right hand side of the above inequality and obtain the following upper bound

K, which requires non-causal information to optimize, we

‚â§

(cid:16)
(cid:98)
1
B(k)
n Ô£´
Ô£≠

(cid:17)
S(k)
n

j=1
(cid:88)

2

S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

(cid:101)

Ô£∏

j=1
(cid:88)

S(k)
n,j

œÉ(k)
n,j

2

.

Ô£π

(93)

(cid:16)

(cid:17)

(cid:101)

Ô£∫
Ô£ª

√ó Ô£Æ
Ô£Ø
Ô£∞
‚â§

k

1
K

+

K

1

‚àí

E

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

S(k)
n

F (k)(w(k))

2
(cid:107)

‚â§

(cid:107)‚àá
(cid:104)

(cid:105)
8Œ≤2Œò2 Œ±2N
e(k)
sumK 2

1
Œõ(k))

(1

‚àí

2‚àö

1)(w(0))

F (
emax
‚àí
Œ±emin‚àöN K(1
(cid:98)

(cid:0)

‚àí

F (K)(cid:63)

‚àí
Œõmax)

+

(cid:1)

K

1

‚àí

(cid:88)k=0

e(k)
sum‚Ñ¶(k+1)‚àÜ(k+1)
2
(cid:113)
Œ±e(k)
avg‚àöN K(1

Œõ(k))

‚àí

2

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

1)

1
D(k)
n

S(k)
n

1
B(k)
n Ô£´
Ô£≠

n,jS(k)
œÉ(k)

n,j Ô£∂

(cid:101)

Ô£∏

j=1
(cid:88)

2 Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(cid:17)

‚àí

j=1
(cid:88)

S(k)
n,j

œÉ(k)
n,j

(cid:17)

(cid:16)

(cid:101)

+ 8Œ∂2

Œ±2N
e(k)
sumK 2

Œ≤2

e(k)
max

(cid:16)

e(k)
max ‚àí

(cid:17) (cid:16)

(cid:16)

(cid:98)
Ô£∂

1

(cid:17)

Ô£∑
Ô£∑
Ô£∏

Ô£´

Ô£¨
Ô£¨
Ô£≠
2

Ô£π

Ô£∫
Ô£∫
Ô£ª

+

K

1

‚àí

(cid:88)k=0

(cid:113)

4e(k)

avgŒ±Œò2Œ≤‚àöN

e(k)
sumK‚àöK(1

Œõ(k)) (cid:88)n

‚ààN

‚àí

2

(cid:33)

1
e(k)
n

D(k)
n
D(k)
(cid:98)

(cid:32)

1
D(k)
n

2

which concludes the proof.

√ó Ô£Æ
Ô£Ø
Ô£∞

2

S(k)
n

(cid:17)
S(k)
n

(cid:16)
(cid:98)
1
B(k)
n Ô£´
Ô£≠

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

(cid:101)

Ô£∏

S(k)
n,j

œÉ(k)
n,j

j=1
(cid:88)

(cid:16)

(cid:101)

j=1
(cid:88)

29

(94)

2

(cid:17)

,

Ô£π

Ô£∫
Ô£ª

APPENDIX F
FURTHER CONVERGENCE RESULTS UNDER OPTIMAL SAMPLING

30

Corollary 3 (Convergence of PSL with Optimal Local Sampling under Bounded Local Iterations). In addition to the conditions
emax,
described in Proposition 1, further assume that maxk
‚â§
k. Then, the convergence behavior of the cumulative average of gradient of the global loss functions for PSL is described by

Œõmax < 1, emin ‚â§

emin ‚â§

k, and

emax,

e(k)
sum

e(k)
avg

Œõ(k)

‚â§

‚â§

‚àÄ

‚àÄ

2‚àö

(cid:9)
1)(w(0))

(cid:8)
F (
emax
‚àí
Œ±emin‚àöN K(1
(cid:98)

‚àí

(cid:0)

F (K)(cid:63)

‚àí
Œõmax)

+

(cid:1)

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

1)

1
D(k)
n

2‚àö

emax

K

1

‚àí

‚Ñ¶(k+1)‚àÜ(k+1)

(cid:98)

(cid:98)

Œ±emin‚àöN K
(cid:98)

(cid:88)k=0

Œõmax)

(1

‚àí
2

S(k)
n

1
B(k)
n Ô£´
Ô£≠

2 Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(cid:17)

n,jS(k)
œÉ(k)

n,j Ô£∂

(cid:101)

Ô£∏

j=1
(cid:88)

(95)

Proof. Considering (94), assuming maxk
k, we get

‚àÄ

1
Œõmax < 1, (emax)‚àí

Œõ(k)

‚â§

(cid:8)

(cid:9)

K

1

‚àí

E

2
F (k)(w(k))
(cid:107)

‚â§

(cid:107)‚àá

(cid:104)

1
Œõmax)

(1

‚àí

(cid:105)
8Œ≤2Œò2 Œ±2N
Ô£´
eminK 2
Ô£¨
Ô£¨
Ô£≠
2

(cid:98)
Œ±2N
eminK 2 Œ≤2

S(k)
n,j

œÉ(k)
n,j

Ô£π

+ 8Œ∂2

(cid:16)

(cid:17)

Ô£∫
Ô£∫
(cid:101)
Ô£ª
4emaxŒ±Œò2Œ≤‚àöN

‚àö

eminK‚àöK(1

1
K

+

‚àí

+

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

S(k)
n

j=1
(cid:88)

K

1

‚àí

(cid:88)k=0

e(k)
max

(cid:16)

e(k)
max ‚àí

(cid:17) (cid:16)

(cid:16)

(cid:98)
Ô£∂

1

(cid:17)

Ô£∑
Ô£∑
Ô£∏
1
B(k)
n Ô£´
Ô£≠

2

Ô£∂

Ô£∏

Ô£Æ

Ô£Ø
Ô£∞

(cid:98)

1

Œõmax)

‚àí

(cid:88)n
‚ààN

Ô£´

Ô£≠

D(k)

e(k)
n

(cid:113)

(cid:98)

K

1

‚àí

(cid:88)k=0
1
K
‚àí

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

S(k)
n

j=1
(cid:88)

K

1

‚àí

(cid:88)k=0

1
K

+

+

‚àí

+

2‚àö

F (0)(w(0))

emax
Œ±emin‚àöN K(1
(cid:98)

(cid:0)

‚àí

2
(cid:107)

E

F (k)(w(k))

‚â§

(cid:107)‚àá
(cid:104)
2‚àö
Œ±emin‚àöN K(1

(cid:105)
emax‚Ñ¶(k+1)‚àÜ(k+1)
Œõmax)

‚àí

(cid:98)

1
Œõmax)

Ô£´

8Œ≤2Œò2 Œ±2N
eminK 2

(1

‚àí

D(k)
n
D(k) (e(k)

n ‚àí

(cid:88)n
‚ààN (cid:98)

1)

1
D(k)
n

Ô£¨
Ô£¨
Ô£≠
2

Ô£π

(cid:98)
Œ±2N
eminK 2 Œ≤2

+ 8Œ∂2

S(k)
n,j

œÉ(k)
n,j

(cid:16)

(cid:17)

Ô£∫
Ô£∫
(cid:101)
Ô£ª
4emaxŒ±Œò2Œ≤‚àöN

(cid:98)

‚àö

eminK‚àöK(1

Œõmax)

‚àí

(cid:32)

(cid:88)n
‚ààN

(cid:98)

e(k)
max

(cid:16)

e(k)
max ‚àí

(cid:17) (cid:16)

2

(cid:33)

1
e(k)
n

D(k)
n
D(k)
(cid:98)

(cid:16)

(cid:98)
Ô£∂

1

(cid:17)

Ô£∑
Ô£∑
Ô£∏

1
D(k)
n

2

(cid:17)
S(k)
n

S(k)
n,j

œÉ(k)
n,j

2

.

Ô£π

S(k)
n

2

S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

j=1
(cid:88)

(cid:101)
e(k)
avg

1

‚àí

j=1
(cid:88)

Ô£∏

(emin)‚àí

1,

‚â§

‚àÄ

(cid:16)

(cid:101)
k and

(cid:17)

Ô£∫
Ô£ª
emin ‚â§

(cid:98)

e(k)
sum

emax,

‚â§

(cid:98)

‚â§

(cid:16)
F (K)(cid:63)

‚àí
Œõmax)

(cid:17)

(cid:1)

2

S(k)
n

j=1
(cid:88)

1
B(k)
n Ô£´
Ô£≠

2 Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(cid:17)

n,jS(k)
œÉ(k)

n,j Ô£∂

(cid:101)

Ô£∏

Rearranging the terms concludes the proof.

√ó Ô£Æ
Ô£Ø
Ô£∞

(cid:16)
(cid:98)
1
B(k)
n Ô£´
Ô£≠

2

S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

(cid:101)

Ô£∏

S(k)
n,j

œÉ(k)
n,j

j=1
(cid:88)

(cid:16)

(cid:101)

j=1
(cid:88)

2

(cid:17)

.

Ô£π

Ô£∫
Ô£ª

(96)

(cid:4)

Corollary 4 (Convergence of PSL with Optimal Local Sampling under UniÔ¨Åed Upperbounds on the Sam-
further assume a bounded stratiÔ¨Åed sampling noise
pling Noise). Under the conditions speciÔ¨Åed in Corollary 3,

maxk,n

1
(cid:16)
(cid:98)D(k)

(cid:17)2

1
B(k)
n

n

(cid:40)

} ‚â§
(cid:20)
emax, and bounded idle period as ‚Ñ¶(k)
k, for a Ô¨Ånite non-negative constant Œ≥. Then, the convergence behavior
of the cumulative average of gradient of the global loss functions across the global aggregation instances of PSL is described

‚àí
(cid:80)
Œ≥
K‚àÜ(k)

(cid:21)(cid:41) ‚â§

(cid:16)(cid:80)

‚â§

(cid:16)

(cid:17)

(cid:17)

‚àÄ

(cid:101)

(cid:101)

œÉmax,

n, k, bounded local iterations maxk{
‚àÄ

e(k)
max

2

œÉ(k)
n,j

n,j

S(k)
j=1 S(k)
n
+,

S(k)
n
j=1

n,jS(k)
œÉ(k)

n,j

2

(cid:2)

(cid:3)

by the following upper bound, which implies 1
K
F (0)(w(0)) ‚àí F (K)(cid:63) (cid:17)
(cid:16)
(cid:2)
(cid:80)
‚àö

(cid:107)‚àáF (k)(w(k))(cid:107)2 ‚â§

K‚àí1
(cid:88)

(cid:107)‚àá

‚àö

2

K
1
‚àí
k=0

E

F (k)(w(k))

2
(cid:107)
‚àö

(cid:98)emax
Œ±emin

N K(1 ‚àí Œõmax)

1
K

+

2
Œ±emin

(cid:3)
(cid:98)emax
‚àö

N K

Œ≥
(1 ‚àí Œõmax)

+

‚àö

(1/‚àöK):

‚â§ O

k=0

Ô£´

+

1
K(1 ‚àí Œõmax)

Ô£¨
Ô£¨
Ô£≠

8Œ≤2Œò2Œ±2N
(cid:98)emin

(emax ‚àí 1)œÉmax + 8Œ∂2

Œ±2N
(cid:98)emin

Œ≤2 (emax) (emax ‚àí 1)

Ô£∂

.

Ô£∑
Ô£∑
Ô£∏

31

(97)

œÉmax

‚àö

N

4emaxŒ±Œò2Œ≤
‚àö
(cid:98)emin

K(1 ‚àí Œõmax)

Proof. Considering (95), assuming a bounded stratiÔ¨Åed sampling noise:
n,jS(k)
œÉ(k)

S(k)
j=1 S(k)
n

maxk,n

œÉ(k)
n,j

S(k)
n
j=1

n,j

n,j

(cid:17)2

2

2

1
(cid:16)
(cid:98)D(k)
(cid:40)
emax, and ‚àÜ(k)

n

1
B(k)
n

(cid:20)

(cid:16)(cid:80)
+,

Œ≥
K‚Ñ¶(k)

(cid:101)

‚àÄ

k, for a Ô¨Ånite non-negative constant Œ≥, we have

‚àí

(cid:17)

(cid:80)

(cid:21)(cid:41) ‚â§

(cid:17)

(cid:16)

(cid:101)

œÉmax,

n, k, bounded local iterations, maxk{
‚àÄ

e(k)
max

} ‚â§

‚â§
K

1
(cid:2)
‚àí

(cid:88)k=0
1
K

‚àí

(cid:88)k=0
1
K

‚àí

1
K

+

+

‚â§

+

+

(cid:3)
2
F (k)(w(k))
(cid:107)

2‚àö

‚â§

E

(cid:107)‚àá

(1

‚àí

1
Œõmax)

Ô£´

8Œ≤2Œò2Œ±2N
eminK 2

Ô£¨
Ô£¨
Ô£≠
4emaxŒ±Œò2Œ≤‚àöN

(cid:98)

‚àö

eminK‚àöK(1
‚àí
F (0)(w(0))

Œõmax)
F (K)(cid:63)

(cid:88)k=0
2‚àö

œÉmax,

emax
(cid:98)
Œ±emin‚àöN K(1
(cid:98)
1

(cid:0)

‚àí
Œõmax)

+

(cid:1)

‚àí

K

‚àí

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

(1

‚àí

1
Œõmax)

Ô£´

8Œ≤2Œò2Œ±2N
eminK 2

Ô£¨
Ô£¨
Ô£≠
4emaxŒ±Œò2Œ≤‚àöN

(cid:98)

‚àö

eminK‚àöK(1

Œõmax)

‚àí

1)(w(0))

F (
emax
‚àí
Œ±emin‚àöN K(1
(cid:98)

(cid:0)

‚àí

F (K)(cid:63)

‚àí
Œõmax)

+

(cid:1)

D(k)
n
D(k) (emax ‚àí

(cid:88)n
‚ààN (cid:98)

1)œÉmax + 8Œ∂2

2‚àö

emax

K

1

‚àí

Œ≥

K(1

(cid:88)k=0

Œ±emin‚àöN K
(cid:98)
Œ±2N
eminK 2 Œ≤2 (emax) (emax ‚àí

‚àí

Œõmax)

1)Ô£∂

Ô£∑
Ô£∑
Ô£∏

(cid:98)

Œ≥
Œõmax)

‚àí

2‚àö

emax

(1

Œ±emin‚àöN K
(cid:98)
D(k)
n
D(k) (emax ‚àí

(cid:88)n
‚ààN (cid:98)

œÉmax,

(cid:98)

1)œÉmax + 8Œ∂2

Œ±2N
eminK 2 Œ≤2 (emax) (emax ‚àí

1)Ô£∂

2‚àö

=

F (0)(w(0))

emax
(cid:98)
Œ±emin‚àöN K(1
(cid:98)

(cid:0)

‚àí

F (K)(cid:63)

‚àí
Œõmax)

+

(cid:1)

2‚àö

emax

Œ±emin‚àöN K
(cid:98)

(1

‚àí

Œ≥
Œõmax)

+

1
Œõmax)

(1

‚àí

+ 8Œ∂2

Œ±2N
eminK

Œ≤2 (emax) (emax ‚àí

1)Ô£∂

+

=

F (0)(w(0))

2‚àö
emax
(cid:98)
Œ±emin‚àöN K(1
(cid:98)

(cid:0)

‚àí

F (K)(cid:63)

‚àí
Œõmax)

Ô£∑
Ô£∑
Ô£∏

+

(cid:1)

K

1

‚àí

(cid:88)k=0
2‚àö

emax
(cid:98)

Œ±emin‚àöN K
(cid:98)

(1

‚àí

Œ≥
Œõmax)

4emaxŒ±Œò2Œ≤‚àöN

‚àö

eminK‚àöK(1

Œõmax)

‚àí

œÉmax,

Ô£∑
Ô£∑
Ô£∏

8Œ≤2Œò2Œ±2N
eminK

(emax ‚àí

1)

(98)

(cid:98)

Ô£´

Ô£¨
Ô£¨
Ô£≠

+

K(1

1

‚àí

Ô£´

8Œ≤2Œò2Œ±2N
emin

(emax ‚àí

Œõmax)

1)œÉmax + 8Œ∂2

Ô£¨
Ô£¨
Ô£≠

4emaxŒ±Œò2Œ≤‚àöN
emin‚àöK(1
where concludes the proof.

(cid:98)
Œõmax)

‚àö

‚àí

+

œÉmax,

(cid:98)

Œ±2N
emin

(cid:98)

Œ≤2 (emax) (emax ‚àí

1)Ô£∂

Ô£∑
Ô£∑
Ô£∏

(99)

(cid:4)

APPENDIX G
TRANSFORMING THE NETWORK-AWARE OPTIMIZATION OF PSL PROBLEM

A. Geometric Programming

A basic knowledge of monomials and posynomials, which is given below, is a prerequisite to understand the GP.

32

M

DeÔ¨Ånition 4. A monomial is a function f : Rn
and Œ±j ‚àà

, yn],
j. Based on the deÔ¨Ånition of monomials, a posynomial g is deÔ¨Åned as a sum of monomials: g(y) =
‚àÄ
yŒ±(2)
2

R,
m=1 dmyŒ±(1)
A standard GP is a non-convex optimization problem deÔ¨Åned as minimizing a posynomial subject to inequality constraints

R:8 f (y) = dyŒ±1

yŒ±n
n , where d

0, y = [y1,

(cid:80)
on posynomials (and monomials) and equality constraints on monomials [37], [47]:

++ ‚Üí

1 yŒ±2

yŒ±(n)

2 ¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

‚â•

n

m

m

m

1

.

min
y
s.t.

f0(y)

fi(y)
1,
hl(y) = 1,

‚â§

i = 1,

l = 1,

, I,

, L,

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

(100)

where fi(y) =
function f (y) = log
bi,k = log(di,k), bl = log(dl) the GP can be converted into the following convex format

Œ±(1)
Œ±(2)
Mi
i,m
l. Due to the fact that the log-sum-exp
m=1 di,my
y
l
1
2
n
j=1 eyj is convex (log denotes the natural logarithm) with the following change of variables zi = log(yi),

Œ±(1)
i, and hl(y) = dly
l
1
‚àÄ

Œ±(2)
i,m
y
2

Œ±(n)
y
l
n

Œ±(n)
i,m
n

¬∑ ¬∑ ¬∑

¬∑ ¬∑ ¬∑

(cid:80)

‚àÄ

y

,

,

(cid:80)

M0

m=1
(cid:88)
Mi

min
z

log

s.t.

log

e(Œ±(cid:62)0,mz+b0,m)

e(Œ±(cid:62)i,mz+bi,m)

m=1
(cid:88)

Œ±(cid:62)l z + bl = 0 l = 1,

0 i = 1,

, I,

¬∑ ¬∑ ¬∑

, L,

‚â§

¬∑ ¬∑ ¬∑

(101)

where z = [z1,

¬∑ ¬∑ ¬∑

, zn](cid:62), Œ±i,k =

i,k , Œ±(2)
Œ±(1)

i,k ¬∑ ¬∑ ¬∑

B. Optimization Problem Transformation

(cid:104)

, Œ±(n)
i,k

(cid:62),

(cid:105)

i, k, and Œ±l =
‚àÄ

, a(2)
l

¬∑ ¬∑ ¬∑

, Œ±(n)
l

Œ±(1)
l
(cid:104)

(cid:62),

l.

‚àÄ

(cid:105)

Let us revisit Problem P in the following, where we use the explicit expressions for the constraints w.r.t the optimization

variables:

(P) : min

K

1

‚àí

c1ETot,(k) + c2T Tot,(k)

1
K (cid:34)

(cid:88)k=0

+ c3

(cid:35)

1
K

K

1

‚àí

E

(cid:88)k=0

F (k)(w(k))
(cid:13)
(cid:13)
(cid:13)
given by (28)

‚àá

(cid:13)
(cid:13)
(cid:13)

2

(cid:125)

(cid:16)

=Œû

(k)

(cid:98)D

,B(k),‚Ñ¶(k),‚àÜ(k)(cid:17)
(cid:123)(cid:122)

s.t.
T Tot,(k) = T D,(k) + T L,(k) + T M,(k) + T U,(k),
ETot,(k) =

E(k)
n ,

(cid:124)

(cid:88)n
‚ààN

K

T Tot,(k) + ‚Ñ¶(k) = T ML,

(cid:88)k=1
max
n
‚ààN (cid:26)

max
m

‚ààN (cid:110)

m,nD(k)
(cid:37)(k)
m

Œ≤/rm,n

T D,(k),

‚â§

(cid:111)(cid:27)

e(k)
n

max
n
‚ààN (cid:40)

anB(k)
n
f (k)
n (cid:41) ‚â§

(cid:98)
T L,(k),

œïm,nM Œ≤/rm,n

T M,(k),

‚â§

(cid:27)

(cid:9)

max
n

‚ààN (cid:26)

max
m
‚ààN

max
n

‚ààN (cid:26)

M Œ≤œïn,n

(cid:8)
rn (cid:27)
(cid:37)(k)
n,m = 1, n

(cid:88)m
‚ààN

T U,(k),

‚â§

,

‚àà N

8Rn

++ denotes the strictly positive quadrant of n-dimensional Euclidean space.

(102)

(103)

(104)

(105)

(106)

(107)

(108)

(109)

(110)

œï(k)

n,m = 1, n

,

‚àà N

(cid:88)m
‚ààN
œï(k)
n,n

(1

‚àí

n

}

(cid:88)m
‚ààN \{
œï(k)
n,n)

œï(k)

n,m ‚â§

0, n

,

‚àà N

œï(k)

m,n ‚â§

0, n

,

‚àà N

(cid:88)m
‚ààN \{

n
}
f max
n , 1
0, n, m

B(k)

f min
f (k)
n ‚â§
n ‚â§
n,m, œï(k)
(cid:37)(k)
n,m ‚â•
Variables:
K, (cid:8)f (k), B(k), (cid:37)(k), œï(k), T D,(k), T L,(k), T M,(k), T U,(k), ‚Ñ¶(k)(cid:9)K

n ‚â§
,

n , n

D(k)

‚àà N

‚àà N

‚â§

(cid:98)

,

k=1

33

(111)

(112)

(113)

(114)

(115)

In the following, we aim to transform the problem into GP format:
Constraint (106): We transform this constraint via expressing it as an normalized inequality on monomials as follows:

Œ≤/rm,n ‚â§
(cid:98)
Constraint (107): We transform this constraint via expressing it as an normalized inequality on posynomials as follows:

1, m, n

‚àà N

(cid:17)

(cid:16)

(116)

.

m,nD(k)
(cid:37)(k)
m

T D,(k)

1

‚àí

T L,(k)

1

‚àí

e(k)
n

(cid:16)

(cid:17)

anB(k)
n
f (k)
n

1, n

.

‚àà N

‚â§

(117)

Constraint (108): We transform this constraint via expressing it as an normalized inequality on monomials as follows:

T M,(k)

1

‚àí

(cid:16)

(cid:17)

œïm,nM Œ≤/rm,n ‚â§

1, m, n

.

‚àà N

(118)

Constraint (109): We transform this constraint via expressing it as an normalized inequality on monomials as follows:

T U,(k)

‚àí

(cid:16)

(cid:17)

1 M Œ≤œïn,n
rn

1, n

.

‚àà N

‚â§

(119)

Note that (103), (104) are solely deÔ¨Ånition of the terms in the objective function which are posynomials. We next focus on
the violating constraints (105), (110), (111) since they impose equality constraints on posynomials which are unacceptable in
GP. We transform these violating constraints in the following:

Constraint (105): We Ô¨Årst revisit this constraints:

We write this constraint via introducing an auxiliary variable and then write the constraint as two inequalities:

(cid:88)k=1

K

[T Tot,(k) + ‚Ñ¶(k)] = T ML.

(T ML)‚àí

1

K

(cid:32)

(cid:88)k=1

1
(T ML)‚àí
AML

T D,(k) + T L,(k) + T M,(k) + T U,(k) + ‚Ñ¶(k)

1,

(cid:33) ‚â§

(AML)‚àí

1

K
k=1 T D,(k) + T L,(k) + T M,(k) + T U,(k) + ‚Ñ¶(k)

1,

‚â§

1,

(cid:16)(cid:80)
where AML is added with a large penalty term to the objective function to force AML
1 at the optimal point. It can be seen
that the fraction in (122) is not in the format of GP since it is an inequality with a posynomial in the denominator, which is
not a posynomial. We thus exploit arithmetic-geometric mean inequality (Lemma 2) to approximate the denominator with a
monomial:

(123)

‚â•

(cid:17)

‚Üì

K

H(x) =

T Tot,(k) + ‚Ñ¶(k)

(cid:88)k=1

H(x; (cid:96)) (cid:44)

‚â•

(cid:98)

K

(cid:89)k=1 (cid:32)

T D,(k)H([x](cid:96)
‚àí
1

T D,(k)

(cid:96)
‚àí

1

‚àí

[T D,(k)](cid:96)
H([x](cid:96)

1 )

‚àí

1)

(cid:33)

(cid:32)

T L,(k)H([x](cid:96)
‚àí
1

(cid:96)

T L,(k)

‚àí

1

‚àí

[T L,(k)](cid:96)
H([x](cid:96)

1)

‚àí

1)

(cid:33)

(cid:2)

(cid:3)

(cid:2)

(cid:3)

(120)

(121)

(122)

T M,(k)H([x](cid:96)
‚àí
1

1)

T M,(k)

(cid:96)
‚àí

(cid:32)

1

[T M,(k)](cid:96)
1)
H([x](cid:96)

‚àí

‚àí

(cid:33)

(cid:32)

T U,(k)H([x](cid:96)
‚àí
1

(cid:96)

T U,(k)

‚àí

[T U,(k)](cid:96)
H([x](cid:96)

1

‚àí

1)

‚àí

1)

(cid:33)

(cid:32)

‚Ñ¶(k)H([x](cid:96)
‚àí
1

‚Ñ¶(k)

(cid:96)
‚àí

We Ô¨Ånally approximate the constraint as follows:

(cid:3)

(cid:2)

(cid:2)

(cid:3)

K

(cid:2)

(cid:3)

1
(T ML)‚àí

T Tot,(k) + ‚Ñ¶(k)

1,

(cid:33) ‚â§

(cid:88)k=1
1

(cid:32)
(AML)‚àí
1

1,

H(x; (cid:96)) ‚â§

(T ML)‚àí
AML

1.

‚â•

(cid:98)

1)

(cid:33)

34

1

‚àí

[‚Ñ¶(k)](cid:96)
H([x](cid:96)

1)

‚àí

.

(124)

(125)

(126)

(127)

Constraint (110): Considering this constraint,

ofÔ¨Çoading) as two inequalities:

m

‚ààN

(cid:80)

(cid:37)(k)
n,m = 1, we write it via introducing an auxiliary variable (for data

(cid:37)(k)
n,m ‚â§

1,

(cid:88)m
‚ààN
(ADO)‚àí

1

(cid:37)(k)
n,m ‚â§
1,

m
ADO
(cid:80)

‚ààN

‚â•

1,

(128)

(129)

(130)

where ADO is added with a large penalty term to the objective function to force ADO
exploit arithmetic-geometric mean inequality (Lemma 2) to approximate the denominator of (129):

‚Üì

1 at the optimal point. We further

G(x) =

(cid:88)m
‚ààN

(cid:37)(k)
n,m ‚â•

G(x; (cid:96)) (cid:44)

(cid:98)

We Ô¨Ånally approximate this constraint as follows:

(cid:37)(k)
n,mG([x](cid:96)
(cid:96)
(cid:37)(k)
n,m
(cid:104)

(cid:105)

‚àí

1)
‚àí
1 Ô£∂

Ô£∑
Ô£∏

(cid:89)m
‚ààN

Ô£´

Ô£¨
Ô£≠

(cid:37)(k)
n,m ‚â§

1,

(cid:88)m
‚ààN
1
(ADO)‚àí
G(x; (cid:96)) ‚â§
ADO
(cid:98)

‚â•

1.

1,

1

‚àí

(k)

n,m](cid:96)
[(cid:37)
G([x](cid:96)

‚àí

1)

.

(131)

(132)

(133)

(134)

Constraint (111): Considering this constraint,

(for model ofÔ¨Çoading) as two inequalities:

m

‚ààN

(cid:80)

œï(k)

n,m = 1, n

‚àà N

, we write it via introducing an auxiliary variable

œï(k)

n,m ‚â§

1,

(cid:88)m
‚ààN
(AMO,(k))‚àí
1
œï(k)
n,m ‚â§
1,

m
‚ààN
AMO,(k)
(cid:80)

‚â•

1,

(135)

(136)

(137)

where AMO,(k) is added with a large penalty term to the objective function to force AMO,(k)
exploit arithmetic-geometric mean inequality (Lemma 2) to approximate the denominator of (136):

‚Üì

1 at the optimal point. We further

J(x) =

(cid:88)m
‚ààN

œï(k)

n,m ‚â•

J(x; (cid:96)) (cid:44)

(cid:98)

Finally, we transform this constraint as follows:

(cid:89)m
‚ààN

Ô£´

Ô£¨
Ô£≠

œï(k)

n,mJ([x](cid:96)
(cid:96)
œï(k)
n,m

‚àí

1)
‚àí
1 Ô£∂

(cid:104)

(cid:105)

Ô£∑
Ô£∏

(k)

n,m](cid:96)
[œï
J([x](cid:96)

‚àí

1

‚àí

1 )

.

(138)

1,

35

(139)

(140)

(141)

1,

œï(k)

n,m ‚â§

(cid:88)m
‚ààN
1
(AMO,(k))‚àí

J(x; (cid:96)) ‚â§

AMO,(k)
(cid:98)

1.

‚â•

Constraints (112), (113): These two constraints are not in the standard format of GP, since the right hand sides are 0. We

Ô¨Årst focus on (112) and write it as follows:

œï(k)
n,n

n

(cid:88)m
‚ààN \{
1,

AB1,(k)

‚â•

œï(k)

n,m + 1

‚â§

AB1,(k),

}

(142)

(143)

where AB1,(k) is added with a large penalty term to the objective function to force AB1,(k)
‚Üì
written as an inequality on a posynomial, and thus the constraint can be written as follows:

1. Note that (142) can be readily

AB1,(k)

1

‚àí

(cid:16)
AB1,(k)

(cid:17)

1.

‚â•

œï(k)
n,n

Ô£´

Ô£≠

(cid:88)m
‚ààN \{

n

}

œï(k)

n,m + 1

1

Ô£∂

‚â§

Ô£∏

We next focus on (113) and perform the following algebraic steps:

œï(k)

n,n)

(1

‚àí

œï(k)

m,n + 1

‚â§

AB2,(k)

n

}

œï(k)
n,n

œï(k)

m,n + 1

(cid:88)m
‚ààN \{
œï(k)

m,n ‚àí

(cid:88)m
n
‚ààN \{
AB2,(k) + œï(k)
n,n

}

œï(k)

m,n + 1

‚â§

AB2,(k)

‚â§

œï(k)
m,n

(cid:88)m
‚ààN \{

n

}

(cid:88)m
‚ààN \{

n

}

‚áí

‚áí

‚áí

m

‚ààN \{
AB2,(k) + œï(k)
(cid:80)
n,n

n

}

(cid:88)m
‚ààN \{

n

}

1,

œï(k)

m,n ‚â§

œï(k)

m,n + 1

m

‚ààN \{

n

}

(144)

(145)

(146)

(147)

(148)

where AB2,(k) is added with a penalty term to the objective function to ensure AB2,(k)
Arithmetic-geometric mean inequality to approximate the denominator of (148) with a monomial as follows:

(cid:80)

‚Üì

1 at the optimal point. We next use

L(x) = 1 + œï(k)
n,n

œï(k)

m,n ‚â•

L(x; (cid:96)) (cid:44)

(cid:88)m
‚ààN \{
Finally, we write this constraint as follows:

(cid:98)

n

}

n,nœï(k)
œï(k)
m,nL([x](cid:96)
(cid:96)
n,nœï(k)
œï(k)
‚àí
m,n

1)
‚àí
1 Ô£∂

(cid:104)

(cid:105)

Ô£∑
Ô£∏

1
L([x](cid:96)

1 )

‚àí

1)

‚àí

L([x](cid:96)
1

(cid:19)

(cid:18)

(cid:89)m
‚ààN \{

n

}

Ô£´

Ô£¨
Ô£≠

œï(k)

m,n + 1

m

‚ààN \{

(cid:80)
AB2,(k)

n
}
L(x; (cid:96))
1.

‚â•
(cid:98)

1,

‚â§

[œï

(k)

(k)
n,n œï
L([x](cid:96)

m,n](cid:96)
1 )

‚àí

1

‚àí

. (149)

(150)

(151)

The ML bound in the objective function: We now turn our attention to the term associated with the ML performance in

36

(152)

2

the objective function:

1
K

K

1

‚àí

E

(cid:88)k=0

(cid:107)‚àá
(cid:104)

F (k)(w(k))

2
(cid:107)

‚â§

F (0)(w(0))

2‚àö
emax
Œ±emin‚àöN K(1

(cid:0)

Œõmax)
(cid:1)

‚àí

+

K

1

‚àí

(cid:88)k=0

e(k)
sum‚Ñ¶(k+1)‚àÜ(k+1)
2
(cid:113)
Œ±e(k)
avg‚àöN K(1

Œõ(k))

1
Œõ(k))

(1

‚àí

K

1

‚àí

(cid:88)k=0

S(k)
n

S(k)
n,j

œÉ(k)
n,j

(cid:105)
(cid:98)
8Œ≤2Œò2 Œ±2N
Ô£´
e(k)
sumK 2
Ô£¨
Ô£¨
Ô£≠
2

Ô£π

+ 8Œ∂2

Œ±2N
e(k)
sumK 2

(cid:16)

(cid:17)

Ô£∫
Ô£∫
Ô£ª
avgŒ±Œò2Œ≤‚àöN

(cid:101)
4e(k)

+

‚àí

+

j=1
(cid:88)

K

1

‚àí

(cid:88)k=0

e(k)
sumK‚àöK(1

Œõ(k)) (cid:88)n

‚ààN (cid:18)

‚àí

(cid:113)

D(k)
n
D(k) (e(k)
n )

(cid:88)n
‚ààN (cid:98)

Œ≤2

e(k)
max

2

Ô£∂

1
D(k)
n

(cid:16)

(cid:98)

1
B(k)
n Ô£´
Ô£≠

2 Ô£Æ

Ô£Ø
Ô£Ø
Ô£∞

(cid:17)

‚àí
S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

(cid:101)

Ô£∏

j=1
(cid:88)

(cid:16)

(cid:17)

1
D(k)

(cid:19)

Ô£∑
Ô£∑
Ô£∏
2 1
e(k)
n

Ô£Æ

S(k)
n

j=1
(cid:88)

1
B(k)
n Ô£´
Ô£≠

Ô£Ø
Ô£∞

2

S(k)
n

n,jS(k)
œÉ(k)

n,j Ô£∂

‚àí

(cid:101)

Ô£∏

S(k)
n,j

œÉ(k)
n,j

2

.

Ô£π

(cid:16)

(cid:17)

(cid:101)

Ô£∫
Ô£ª

j=1
(cid:88)

To have a tractable solution, we assume that the relative size of the strata to the size of the local dataset is upper bounded
(k)
n,j to the local dataset, i.e.,
throughout the learning period, and let sn,j denote the upper bound of the relative size of stratum
S
S(k)
n,j
(cid:98)D(k)
n ‚â§

k. Accordingly, we have

sn,j,

‚àÄ

S(k)
n

j=1
(cid:88)

S(k)
n,j

œÉ(k)
n,j

(cid:16)

(cid:101)

S(k)
n

‚â§

j=1
(cid:88)

2

(cid:17)

2

S(k)
n

sn,j

D(k)
n

œÉ(k)
n,j

2

=

D(k)
n

(cid:98)

(cid:16)

(cid:101)

(cid:98)

(cid:17)

2

n,jS(k)
œÉ(k)

n,j Ô£∂

‚â§ Ô£´

(cid:101)

Ô£∏

Ô£≠

œÉ(k)
n,jsn,j

D(k)

n Ô£∂

(cid:101)

(cid:98)

Ô£∏

j=1
(cid:88)

=

D(k)
n

(cid:16)

(cid:98)

S(k)
n

Ô£´

Ô£≠

j=1
(cid:88)

Thus the expression in (152) can be written as follows:

S(k)
n

j=1
(cid:88)

2

,

(cid:17)

sn,j

œÉ(k)
n,j

(cid:16)
(cid:101)
(cid:44) (cid:98)Z(k)

n

(153)

.

(154)

(cid:124)
2

(cid:17)

Ô£´

Ô£≠

(cid:124)

S(k)
n

j=1
(cid:88)

2

(cid:123)(cid:122)
(cid:125)
œÉ(k)
n,jsn,jÔ£∂
Ô£∏

(cid:101)
(cid:44) (cid:101)Z(k)

n

(cid:123)(cid:122)

(cid:125)

F (k)(w(k))
(cid:107)

2

‚â§

(cid:105)

(cid:107)‚àá
(cid:104)

F (

emax

2‚àö
‚àí
Œ±emin‚àöN K(1

1)(w(0))
Œõmax)
(cid:1)

(cid:0)

‚àí

+

K

1

‚àí

(cid:88)k=0

e(k)
sum‚Ñ¶(k+1)‚àÜ(k+1)
2
(cid:113)
Œ±e(k)
avg‚àöN K(1

Œõ(k))

‚àí

K

1

‚àí

E

1
K

+

+

(cid:88)k=0
1
K

‚àí

(cid:88)k=0
1
K

‚àí

(cid:88)k=0

(1

‚àí

1
Œõ(k))

Ô£´

8Œ≤2Œò2Œ±2N
e(k)
sumK 2

Ô£¨
Ô£¨
Ô£≠
avgŒ±Œò2Œ≤‚àöN

4e(k)

(cid:98)

(cid:88)n
‚ààN

e(k)
sumK‚àöK(1

Œõ(k)) (cid:88)n

‚ààN (cid:18)

‚àí

(cid:113)

e(k)
n
D(k)
D(k)

n (cid:34)

1
B(k)

D(k)
n

n (cid:16)

(cid:98)

2

(cid:17)

Z (k)

n ‚àí

D(k)
n

Z (k)
n

(cid:35)

+ 8Œ∂2

(cid:101)

(cid:98)

(cid:98)

Œ±2N
e(k)
sumK 2

Œ≤2

e(k)
max

(cid:16)

(cid:17)

(cid:98)

1
D(k)

(cid:19)

2 1
e(k)
n (cid:34)

1
B(k)

D(k)
n

2

(cid:17)

Z (k)

n ‚àí

D(k)
n

Z (k)
n

,

(cid:35)

(cid:101)

(cid:98)

(cid:98)

n (cid:16)

(cid:98)

2

Ô£∂

Ô£∑
Ô£∑
Ô£∏

(155)

1/2

(cid:125)

2 (cid:0)F (‚àí1)(w(0))(cid:1)
‚àö
N K(1 ‚àí Œõmax)
Œ±
(cid:125)
(cid:124)

(cid:123)(cid:122)
(b)

37

(156)

which can be written as follows:

Œû (cid:44)

K‚àí1
(cid:88)

k=0

(cid:104)

(cid:107)‚àáF (k)(w(k))(cid:107)2(cid:105)

E

‚â§

Ô£´
Ô£≠ min

1‚â§k‚â§K

Ô£±
Ô£≤

Ô£≥

(cid:88)

n‚ààN

(cid:98)D(k)

n e(k)
n
D(k)

Ô£º
Ô£Ω

Ô£æ

Ô£∂

‚àí1 Ô£´

Ô£∏

Ô£≠ max
1‚â§k‚â§K

Ô£±
Ô£≤

Ô£≥

(cid:88)

e(k)
n

n‚ààN

(cid:124)

(cid:123)(cid:122)
(a)

K‚àí1
(cid:88)

+

Ô£´

Ô£≠

(cid:88)

k=0

n‚ààN

(cid:98)D(k)

n e(k)
n
D(k)

Ô£∂

‚àí1 Ô£´

Ô£∏

Ô£≠

Ô£∂

1/2

e(k)
n

Ô£∏

(cid:88)

n‚ààN

(cid:124)

(cid:123)(cid:122)
(c)

(cid:125)

2‚Ñ¶(k+1)‚àÜ(k+1)
‚àö
Œ±
(cid:124)

N K(1 ‚àí Œõ(k))
(cid:125)

(cid:123)(cid:122)
(d)

Ô£∂

Ô£∏

Ô£º
Ô£Ω

Ô£æ

+

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõ(k))
(cid:123)(cid:122)
(cid:125)
(cid:124)
(e)

Ô£´

Ô£¨
Ô£¨
Ô£≠

(cid:124)

8Œ≤2Œò2Œ±2N
K2

Ô£´

Ô£≠

(cid:88)

n‚ààN

Ô£∂

‚àí1

e(k)
n

Ô£∏

(cid:88)

n‚ààN

n (cid:98)D(k)
e(k)
n
D(k)

1
B(k)
n

(cid:101)Z(k)
n

(cid:125)

(cid:123)(cid:122)
(f )
Ô£∂

Ô£∑
Ô£∑
Ô£∏

+

8Œ∂2Œ≤2Œ±2N
K2

Ô£´

Ô£≠

(cid:88)

n‚ààN

Ô£∂

‚àí1

e(k)
n

Ô£∏

(cid:18)

max
n‚ààN

(cid:19)2

{e(k)
n }

(cid:124)

+

K‚àí1
(cid:88)

k=0

‚àö

(cid:124)

(cid:123)(cid:122)
(g)

‚àö

4Œò2Œ≤Œ±
KK(1 ‚àí Œõ(k))

N

Ô£´

Ô£≠

(cid:88)

n‚ààN

(cid:98)D(k)

n e(k)
n
D(k)

(cid:125)

Ô£∂

Ô£´

Ô£∏

Ô£≠

Ô£∂

‚àí1/2

e(k)
n

Ô£∏

(cid:88)

n‚ààN

(cid:88)

n‚ààN

(cid:125)

(cid:124)

‚àí

K‚àí1
(cid:88)

k=0

1
(1 ‚àí Œõ(k))

8Œ≤2Œò2Œ±2N
K2

(cid:123)(cid:122)
(h)

Ô£´

Ô£≠

(cid:88)

n‚ààN

Ô£∂

‚àí1

e(k)
n

Ô£∏

(cid:88)

n‚ààN
(cid:124)

(cid:125)

(cid:123)(cid:122)
(j)

e(k)
n
D(k) (cid:98)Z(k)

n

(cid:123)(cid:122)
(m)

(cid:125)

Ô£∂

‚àí1/2

(cid:124)

‚àö

(cid:124)

‚àí

K‚àí1
(cid:88)

k=0

‚àö

4Œò2Œ≤Œ±
KK(1 ‚àí Œõ(k))

N

Ô£´

Ô£≠

(cid:88)

n‚ààN

(cid:98)D(k)

n e(k)
n
D(k)

Ô£∂

Ô£´

Ô£∏

Ô£≠

(cid:123)(cid:122)
(n)

e(k)
n

Ô£∏

(cid:88)

n‚ààN

(cid:88)

n‚ààN

(cid:125)

(cid:124)

Ô£´

Ô£¨
Ô£≠

Ô£´

Ô£¨
Ô£≠

(cid:98)D(k)
n
(cid:113)

D(k)

e(k)
n
(cid:123)(cid:122)
(i)

(cid:113)

(cid:98)D(k)
n
(cid:113)

D(k)

e(k)
n

Ô£∂
2

Ô£∑
Ô£∏

1
B(k)
n

(cid:101)Z(k)
n

(cid:125)

Ô£∂
2

Ô£∑
Ô£∏

(cid:98)Z(k)
n

,

(cid:123)(cid:122)
(o)

(cid:125)

which is explicitly written as the optimization variables. In the following, we break down the expression into different parts and
carefully investigate the characteristics of each part with respect to the optimization variables.

‚Ä¢

‚Ä¢

‚Ä¢

‚Ä¢

‚Ä¢

‚Ä¢

Term (a) is in the form of a fraction with a non-posynomial denominator and a posynomial in the denominator, which is
not a posynomial. Term (b) is a constant and thus a posynomial. Thus, term (a) and (b) combined are in the form of a
ratio with a posynomial denominator which is not a posynomial.
Term (c) has a non-posynomial denominator and a posynomial numerator, and thus is not a posynomial. Term (d) is a
posynomial. Thus, term (c) and (d) combined are in the form of a ratio with a posynomial in the denominator which is
not a posynomial.
Term (e) is a constant and thus a posynomial. Term (f ) is a ratio between a sum of monomials which is a posynomial and
a posynomial, and thus it is not a posynomial. Also, term (g) is a ratio between a max of monomials and a posynomial,
which is not a posynomial. Thus, both combinations of term (e) with (f ) and term (e) with (g) are not posynomials.
Term (h) is a ratio between a posynomial and square root of a posynomial, which is not a posynomial. Term (i) is a sum
of monomials which is a posynomial. Thus, combination of term (h) with (i) is not a posynomial.
Term (j) is a ratio between two posynomials, which is not a posynomial. Term (m) is a sum of monomials which is a
posynomial. Thus, combination of term (j) and (m) is a posynomial.
Term (n) a ratio between a posynomial and square root of a posynomial, which is not a posynomial. Term (o) is a sum of
monomials which is a posynomial. Thus, combination of term (n) and (o) is not a posynomial.

Note that combination of two terms (j)&(m) is multiplied with a negative sign which makes the term non-posynomial even

if the terms inside were posynomials. The same holds for the combination of terms (n)&(o).

In summary, we are faced with two challenges: Ô¨Årst, the existence of division by posynomials and square root of posynomials
and maximum of monomials inside the expressions; second, the existence of negative sign in the expression. In the following,
we aim to alleviate these issues via introducing new constraints, using arithmetic-geometric mean to approximate the posynomial
denominators with monomials, and some algebraic manipulations.

Introducing new constrains and exploiting the arithmetic-geometric mean: We Ô¨Årst incorporate e(k)

avg as the
optimization variables, which makes the expressions simpler and more tractable. Then, to ensure that they take their desired

sum and e(k)

= 1,

38

(157)

values in the solution, we re-write their deÔ¨Ånitions as follows:

e(k)
sum

n e(k)
n
D(k)
However, the above two expressions are as the ratio of a monomial and a posynomial, which is not a posynomial. We thus
exploit the arithmetic-geometric mean inequality in Lemma 2 and approximate the denominator of the above two constraints as
below:

(cid:80)

‚ààN

n

(158)

‚ààN

n

(cid:80)

e(k)
n
e(k)
avg
(cid:98)D(k)

= 1.

R(x) =

(cid:88)n
‚ààN

V (x) =

e(k)
n ‚â•

R(x; (cid:96)) (cid:44)

(cid:98)

D(k)

n e(k)

n =

e(k)
n R([x](cid:96)
[(cid:96)

1)
‚àí
1] Ô£∂

Ô£´

(cid:89)n
‚ààN

‚àí

e(k)
n
Ô£¨
Ô£≠
(cid:104)
(cid:105)
m,ne(k)
m (cid:37)(k)
D(k)
n

Ô£∑
Ô£∏

1]

‚àí

(k)

n ][(cid:96)
[e
R([x](cid:96)

‚àí

1)

,

(cid:88)n
‚ààN

(cid:98)

‚ààN (cid:88)m
(cid:88)n
‚ààN

(k)
m [(cid:37)

D

(k)
m,n e
V ([x](cid:96)

(k)

n ][(cid:96)
1)

‚àí

1]

‚àí

.

(cid:98)
(cid:105)
As a result, constraints (157) and (158) can be expressed as follows:

V (x; (cid:96)) (cid:44)

‚â•

‚ààN (cid:89)m
(cid:89)n
‚ààN

n V ([x](cid:96)
[(cid:96)

m,ne(k)
(cid:37)(k)
m,ne(k)
(cid:37)(k)
n
(cid:104)

‚àí

Ô£´

Ô£¨
Ô£≠

1)
‚àí
1] Ô£∂

Ô£∑
Ô£∏

1,

1

‚àí

e(k)
sum

e(k)
n ‚â§

(cid:16)

(cid:17)

(cid:88)n
‚ààN
1
(ASum,(k))‚àí
1
e(k)
sum

‚àí

R(x; (cid:96))

(cid:16)
(cid:17)
ASum,(k)

1,
(cid:98)

‚â•
1

e(k)
avg

‚àí

(cid:17)

(cid:34)
(cid:88)n
(cid:16)
‚ààN (cid:98)
D(k)(AAvg,(k))‚àí
1
1
e(k)
avg

V (x; (cid:96))

‚àí

(cid:16)
(cid:17)
AAvg,(k)

1,
(cid:98)
‚â•

1,

‚â§

1,

‚â§

D(k)

n e(k)
n
D(k)

1,

(cid:35) ‚â§

(159)

(160)

(161)

(162)

(163)

(164)

(165)

(166)

where ASum,(k) and AAvg,(k) are added with penalty terms to the objective function to ensure ASum,(k)
As a result, we have translated both (157) and (158) to inequalities on posynomials, which is acceptable in GP.

‚Üì

1 and AAvg,(k)

1.

‚Üì

We also Ô¨Ånd it useful to add

D(k)

n as optimization variable to have tractable expressions. Considering

D(k)

n =

we take a similar approach as above and exploit the arithmetic-geometric mean inequality as follows:
(cid:98)

(cid:98)

1

m,nD(k)
(cid:37)(k)
m ,

m

‚ààN

(cid:80)

D

(k)
m [(cid:37)

(k)
m,n ](cid:96)
1 )

‚àí

‚àí

I([x](cid:96)

.

(167)

I(x) =

(cid:88)m
‚ààN
We Ô¨Ånally transform this constraint as:

m,nD(k)
(cid:37)(k)

m ‚â•

I(x; (cid:96)) (cid:44)

(cid:98)

(cid:32)

(cid:89)m
‚ààN

(cid:37)(k)
m,nI([x](cid:96)
[(cid:37)(k)
m,n](cid:96)

‚àí

1)
‚àí
1 (cid:33)

m,nD(k)
(cid:37)(k)
m

1,

(cid:35) ‚â§

D(k)
(

n )‚àí

1

(cid:34)

(cid:88)m
‚ààN
1

(AD,(k))‚àí
1
n )‚àí

(cid:98)
D(k)
(
AD,(k)
(cid:98)

‚â•

I(x; (cid:96)) ‚â§
1,
(cid:98)

1,

39

(168)

(169)

(170)

1.

where AD,(k) is added with a penalty terms to the objective function to ensure AD,(k)
Then, we introduce

emax and emin (appeared in term (a) in (156)) into the optimization variables and impose the following

‚Üì

constraints:

(cid:98)

e(k)
sum(
emin(e(k)

1
emax)‚àí
1
avg)‚àí
(cid:98)

‚â§

1,

1,

‚â§

both of which are inequality on monomials.

Furthermore, we introduce e(k)

max as another optimization variable and then introduce the following constraint:

which is an inequality on a posynomial accepted in GP.

e(k)
n
e(k)
max ‚â§

1, n

,

‚àà N

(171)

(172)

(173)

It can be readily validated that with the procedure taken above, i.e., extending the optimization variables space to contain
e(k)
sum, e(k)
max, and introducing the above constraints, all the terms which are recognized by under-braces
avg,
in (156) are transformed to posynomials. We then, focus on the two summations in the last two lines of (156) which have
negative coefÔ¨Åcients.

emax emin, and e(k)

D(k)
n ,

(cid:98)

(cid:98)

Handling the terms with negative coefÔ¨Åcients: We write down Œû deÔ¨Åned in (156) as follows:

K

1

‚àí

Œû =

1 (x) + œÉ(k)
œÉ(k)

2 (x) + œÉ(k)

3 (x) + œÉ(k)

4 (x) + œÉ(k)

5 (x)

(cid:88)k=0 (cid:104)

œÉ(k)
6 (x)

‚àí

‚àí

,

œÉ(k)
7 (x)
(cid:105)

(174)

where œÉ1 is the Ô¨Årst line of (156) (i.e., (a)&(b)) after the above variables are introduced as optimization variables, œÉ2 is the
second line of (156) (i.e., (c)&(d)) after the above variables are introduced as optimization variables, and so forth. Note that
œÉ6, œÉ7 are the last two lines of (156) which appear with negative sign. We next deÔ¨Åne:

3 (x) + œÉ(k)

4 (x) + œÉ(k)

5 (x),

2 (x) + œÉ(k)

1 (x) + œÉ(k)
6 (x),
7 (x).

+ (x) (cid:44) œÉ(k)
œÉ(k)
,1(x) (cid:44) œÉ(k)
œÉ(k)
‚àí
,2(x) (cid:44) œÉ(k)
œÉ(k)
‚àí
œÉ(k)
œÉ(k)
+ (x)
‚àí
function of P. We then transform the term Œ≥Œû in the objective function as Œ≥
(cid:104)
is the upperbound of the summand in Œû (i.e., œÉ(k)
,2(x)
follows:

,2(x)
(cid:105)
œÉ(k)
‚àí

Note that Œû =

œÉ(k)
‚àí

œÉ(k)
‚àí

+ (x)

,1(x)

,1(x)

K
1
‚àí
k=0

(cid:80)

‚àí

‚àí

‚àí

‚àí

corresponds to the term with the weight coefÔ¨Åcient Œ≥ in the objective
K
k=0 œá(k), in which the auxiliary variable œá(k)
‚àí
œá(k),
k), which is added to the constraints as

1

(cid:80)
‚â§

‚àÄ

(175)

(176)

(177)

œÉ(k)
‚àí

œÉ(k)
+ (x)
‚àí
œÉ(k)
+ (x)

œÉ(k)
,1(x)
‚àí
‚àí
œá(k) + œÉ(k)
‚â§
‚àí
œÉ(k)
+ (x)
,1(x) + œÉ(k)
To transform this constraint to an upperbound inequality on a posynomial, we need to make the numerator of the fraction
on the left hand side of (180) a posynomial and its denominator a monomial. Note that all the terms in the numerator are
posynomials, which makes the numerator a posynomial. Thus, it is sufÔ¨Åcient to focus on the approximation of the denominator,
for which we exploit the arithmetic-geometric mean inequality as follows:

œá(k)
,2(x)
‚â§
,1(x) + œÉ(k)

œá(k) + œÉ(k)
‚àí

,2(x) ‚â§

,2(x)

(180)

(179)

(178)

‚â°

‚â°

1.

‚àí

‚àí

W (x) =œá(k) +

1
Œõ(k))

8Œ≤2Œò2Œ±2N
K 2

(e(k)

1
sum)‚àí

(1

‚àí

e(k)
n
D(k)

Z (k)
n

(cid:98)

(cid:88)n
‚ààN

+

‚â•

where

4Œò2Œ≤Œ±‚àöN

‚àöKK(1

‚àí

W (x; (cid:96)) (cid:44)

(cid:18)

(cid:99)

Œõ(k))

(cid:16)
(cid:17) (cid:16)
œá(k)W ([x](cid:96)
‚àí
1
[œá(k)](cid:96)

‚àí

e(k)
avg

e(k)
sum

1/2

‚àí

(cid:17)
[œá(k)](cid:96)
W ([x](cid:96)

(cid:88)n
‚ààN
1
1)

‚àí

‚àí

1)

(cid:19)

√ó

(cid:89)n
‚ààN

q=1 (cid:18)
(cid:89)

D(k)
n
2

D(k)
(cid:98)

(cid:0)

(cid:1)
2

Z (k)
n

e(k)
n

(cid:98)

Œ¥q(x, n)W ([x](cid:96)
‚àí
1, n)
Œ¥q([x](cid:96)

‚àí

1)

(cid:19)

Œ¥q ([x](cid:96)
‚àí
W ([x](cid:96)

1 ,n)
1 )

‚àí

Œ¥1(x, n) =

Œ¥2(x, n) =

(1

8Œ≤2Œò2Œ±2N
K 2

1
Œõ(k))
‚àí
4Œò2Œ≤Œ±‚àöN

K‚àöK(1

Œõ(k))

‚àí

e(k)
sum

Z (k)
n ,

e(k)
n
e(k)
sumD(k)
e(k)
D(k)
(cid:98)
n
avg
1/2

2

e(k)
n

Z (k)
n ,

(cid:98)

D(k)
(cid:98)
(cid:0)

(cid:1)

S(k)
n

Z (k)

n =

sn,j

œÉ(k)
n,j

(cid:16)
(cid:101)
We Ô¨Ånally convert (180) to the following constraint:

(cid:98)

j=1
(cid:88)

(cid:16)

(cid:17)

2

.

(cid:17)

œÉ(k)
+ (x)
W (x; (cid:96)) ‚â§

1,

40

,

(181)

(182)

(183)

(184)

(185)

which is an inequality on the ratio between a posynomial and a monomial and thus a posynomial.
We Ô¨Ånally obtain the optimization problem presented on the next page as our Ô¨Ånal formulation, which is an iterative-based
1 in

optimization, where at each iteration, the optimization admits the GP format (coefÔ¨Åcients p1, p2, p3, p4, p5, p6, p7, p8 (cid:29)
the objective function are constant penalty coefÔ¨Åcients):

(cid:99)

(P (cid:48)) : min

1
K

K‚àí1
(cid:88)

Ô£Æ
Ô£∞c1

Ô£´

Ô£≠

(cid:88)

E(k)
n

Ô£∂
Ô£∏ + c2

k=0

n‚ààN

(cid:16)

T D,(k) + T L,(k) + T M,(k) + T U,(k)(cid:17)

+ c3œá(k)

Ô£π

Ô£ª

+ p1AML + p2ADO +

K‚àí1
(cid:88)

k=0

‚â§ 1,

(cid:104)

p3AMO,(k) + p4AD,(k) + p5ASum,(k) + p6AAvg,(k) + p7AB1,(k) + p8AB2,(k)(cid:105)

s.t.
œÉ(k)
+ (x)
(cid:99)W (x; (cid:96))
(cid:16)
e(k)
sum

(cid:17)‚àí1 (cid:88)

n‚ààN
(ASum,(k))‚àí1
(cid:17)‚àí1
(cid:16)
e(k)
sum

e(k)
n ‚â§ 1,

‚â§ 1,

(cid:16)

(cid:98)D(k)
n

(cid:17)‚àí1

(AD,(k))‚àí1
(cid:17)‚àí1

(cid:16)
(cid:98)D(k)
n

(cid:17)‚àí1

(cid:16)

e(k)
avg

(cid:98)R(x; (cid:96))
Ô£Æ

(cid:88)

Ô£∞

m‚ààN

(cid:98)I(x; (cid:96))
Ô£Æ

(cid:88)

Ô£∞

n‚ààN

m,nD(k)
(cid:37)(k)
m

Ô£π
Ô£ª ‚â§ 1

‚â§ 1,

(cid:98)D(k)

n e(k)
n
D(k)

Ô£π
Ô£ª ‚â§ 1,

‚â§ 1,

D(k)(AAvg,(k))‚àí1
(cid:17)‚àí1
(cid:16)
e(k)
avg
(cid:16)
e(k)
max

(cid:98)V (x; (cid:96))
(cid:17)‚àí1

e(k)
n

‚â§ 1, n ‚àà N ,

e(k)
sum ((cid:98)emax)‚àí1 ‚â§ 1,
‚â§ 1,
emin

e(k)
avg

(cid:17)‚àí1

(cid:16)

(cid:16)

T ML(cid:17)‚àí1

(cid:32) K
(cid:88)

k=1

(cid:33)

T Tot,(k) + ‚Ñ¶(k)

‚â§ 1,

(cid:16)

‚â§ 1,

(AML)‚àí1
(T ML)‚àí1 (cid:98)H(x; (cid:96))
(cid:16)
T D,(k)(cid:17)‚àí1
(cid:37)(k)
m,nD(k)
T L,(k)(cid:17)‚àí1
T M,(k)(cid:17)‚àí1
(cid:16)
T U,(k)(cid:17)‚àí1
(cid:88)
(cid:37)(k)
n,m ‚â§ 1,

(cid:16)

m (cid:98)Œ≤/rm,n ‚â§ 1, m, n ‚àà N ,

n anB(k)
e(k)

n /f (k)

n ‚â§ 1, n ‚àà N ,

œïm,nM Œ≤/rm,n ‚â§ 1, m, n ‚àà N ,

M Œ≤œïn,n/rn ‚â§ 1, n ‚àà N ,

m‚ààN
(ADO)‚àí1

(cid:98)G(x; (cid:96))
(cid:88)

‚â§ 1,

œï(k)

n,m ‚â§ 1,

m‚ààN
(AMO,(k))‚àí1

(cid:98)J(x; (cid:96))
AB1,(k)(cid:17)‚àí1

(cid:16)

‚â§ 1,

Ô£´
Ô£≠œï(k)

n,n

(cid:88)

œï(k)

n,m + 1

Ô£∂
Ô£∏ ‚â§ 1, n ‚àà N ,

m‚ààN \{n}

(cid:80)

m‚ààN \{n} œï(k)
(cid:98)L(x; (cid:96))

m,n + 1

‚â§ 1, n ‚àà N ,

AMO,(k), AAvg,(k), AD,(k), ASum,(k), AB1,(k), AB2,(k), ADO, AML ‚â• 1,

‚â§ 1, 1 ‚â§ B(k)

n ‚â§ (cid:98)D(k)

n , n ‚àà N

(cid:98)D(k)

n , e(k)

n , f (k)

n , T D,(k), T L,(k), T M,(k), T U,(k), ‚Ñ¶(k), e(k)

sum, e(k)

avg, e(k)

n,m > 0, n, m ‚àà N ,

‚â§ 1,

f min
n
f (k)
n

f (k)
n
f max
n
n,m, œï(k)
max, œá(k), emin, (cid:98)emax, (cid:37)(k)

41

(186)

(187)

(188)

(189)

(190)

(191)

(192)

(193)

(194)

(195)

(196)

(197)

(198)

(199)

(200)

(201)

(202)

(203)

(204)

(205)

(206)

(207)

(208)

(209)

(210)

Variables:
K, (cid:8)
(k)
ADO, AML, emin, (cid:98)emax

(cid:98)D

, e(k), f (k), B(k), (cid:37)(k), œï(k), T D,(k), T L,(k), T M,(k), T U,(k), ‚Ñ¶(k), e(k)

sum, e(k)

avg , e(k)

max, œá(k), AMO,(k), AAvg,(k), AD,(k), ASum,(k), AB1 ,(k), AB2 ,(k)(cid:9)K

k=1,

