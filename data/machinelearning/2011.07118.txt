0
2
0
2

v
o
N
3
1

]

V
C
.
s
c
[

1
v
8
1
1
7
0
.
1
1
0
2
:
v
i
X
r
a

Deep Multi-view Image Fusion for Soybean Yield Estimation

in Breeding Applications

Luis G Riera1†, Matthew E. Carroll2†, Zhisheng Zhang1, Johnathon M. Shook2,
Sambuddha Ghosal1, Tianshuang Gao3, Arti Singh2, Sourabh Bhattacharya1, Baskar
Ganapathysubramanian1, Asheesh K. Singh*2, and Soumik Sarkar*1

1Department of Mechanical Engineering, Iowa State University, Ames, Iowa, USA
2Department of Agronomy, Iowa State University, Ames, Iowa, USA
3Department of Computer Science, Iowa State University, Ames, Iowa, USA
*Corresponding authors. Email: singhak@iastate.edu & soumiks@iastate.edu
†These authors contributed equally to this work.

Abstract

Reliable seed yield estimation is an indispensable step in plant breeding programs geared towards cul-

tivar development in major row crops. The objective of this study is to develop a machine learning (ML)

approach adept at soybean [Glycine max L. (Merr.)] pod counting to enable genotype seed yield rank pre-

diction from in-ﬁeld video data collected by a ground robot. To meet this goal, we developed a multi-view

image-based yield estimation framework utilizing deep learning architectures. Plant images captured from

different angles were fused to estimate the yield and subsequently to rank soybean genotypes for application

in breeding decisions. We used data from controlled imaging environment in ﬁeld, as well as from plant

breeding test plots in ﬁeld to demonstrate the efﬁcacy of our framework via comparing performance with

manual pod counting and yield estimation. Our results demonstrate the promise of ML models in making

breeding decisions with signiﬁcant reduction of time and human effort, and opening new breeding methods

avenues to develop cultivars.

1 Introduction

Plant breeding programs worldwide rely on yield testing to make selections and advancement decisions to-
wards the development of new varieties. A vital component in this process is the growing and harvesting of

an inordinate number of plots at several locations each year, incurring substantial costs and resource alloca-
tions burdening the economics of a breeding program. The need to assess tens of thousands of genotypes in

a program is necessitated by the inherent requirement to work with a desired level of and expand the genetic
variance for a higher response to selection [1]. Therefore, the need to accurately measure or predict yield has

motivated researchers to constantly develop modern tools in genomics [2], [3] and phenomics [4], [5], [6] .

1

 
 
 
 
 
 
One of the avenues to yield prediction is through the fusion of high dimensional phenotypic trait data

using machine learning (ML) approaches to provide plant breeders the tools to do in-season seed yield (SY)
prediction [7], and fusing ML and optimization techniques to identify a suite of in-season phenotypic traits

collected from multiple sensors that decrease the dependence on resource-intensive end-season phenotyping
in breeding programs [8]. Other avenues have been through integrating weather and genetic information in

conjunction with deep time series attention models for crop SY prediction [9], [10].

These advances in ML methods and earnest effort to collect large data sets is commendable, and has a

positive role in numerous scenarios; however, these approaches do not work for plant breeding programs of
all sizes, geographical regions and crops. One less explored approach is using simple camera (tri-band digital)

to estimate plant reproductive organs and estimate SY. If imaging is coupled with automated ground robotic
systems, breeders can compute plot SY to make breeding decisions in an efﬁcient manner. Gao et al. [11]

deployed a low cost lightweight distributed multiple robot system for soybean phenotypic data collection,
which demonstrates a usable platform to meet yield estimation requirements. We envision that an automated

data collection platform (i.e., ground robot) with sensors (i.e., digital camera) creates a framework to estimate
SY in ﬁeld conditions from breeding plots where genotypes are assessed for their merit and commercialization

potential. The motivation for this challenge is to provide a timely and cost effective solution for SY estimation

in ﬁeld plots. This framework can be deployed in a breeding pipeline to improve the capability to obtain high
quality SY data without the need to machine harvest all plots, one of the most time and resource intensive

steps in plant breeding. To provide a context of the time and resource investment, 50-100 soybean yield
plots can be combined in one hour at each testing ﬁeld site depending on plot sizes etc, by a 1-2 person work

crew. Plant breeding programs are also at the mercy of weather events. For example, excessive rainfall in fall
season in North America soybean growing regions often complicates machine harvest, bringing the entire

program to a halt causing signiﬁcant delay in data analysis and advancement decisions for winter nursery
operations or for the next season. Also, breeding decisions for making selections and advancements need to

wait for machine harvest data delaying breeding cycles and turnaround times.

We suggest a balanced and strategic approach by using small weight autonomous ground robots. This is

applicable to early generation testing, progeny row stage, and also for preliminary and unreplicated advanced
replicated yield trial stage, where genotypes can be grown in multiple locations but only one location needs

to be harvested to obtain seed source for next season planting. Additionally, in replicated tests, only one
replication will be harvested for seed source. Furthermore, such a breeding pipeline will empower breeding

programs to operate in wet soil conditions where machine harvest is not possible. In all these above scenarios,

except the harvest plot, all other plots will be imaged using the ground robot and yield estimated using the
application of ML models by obtaining information on plant reproductive organs of economic importance,

such as SY. These situations and the integration of yield estimation in breeding methods and pipelines using
ground robots, can remove the need to harvest all plots from all locations, to save time and resources as

described earlier.

Computer vision models for crop yield estimation have been proposed in the past. However, such models

are primarily built for larger fruit trees, with potentially less background clutter and occlusion compared to
soybean plants and pods [12], [13], [14], [15], and [16]. Recently proposed TasselNet [17] attempts to count

maize tassels based on a single aerial image.

In this context, we develop a deep learning framework for soybean pod detection to predict the number of

2

pods in each plot using multiple views of plants taken by simple RGB cameras. This information is then used

as a proxy to estimate seed yield. We also test the usefulness of this model on images collected by a ground
robot capable of taking RGB images that are processed by the deep learning model. While the proposed pod

counting and yield estimation approach can be used in an ofﬂine manner using single or multi-view images of
plots, we further develop a plant detector and tracker (from soybean plots), enabling our approach to also be

used in an online manner by a ground robot using on-board processors and edge computing. The rest of the
paper is organized as follows. Section 2 (Materials and Methods) presents the data collection and processing

steps along with the ML framework for pod counting and plant detection/tracking from plots. In Section 3
(Results and Discussion), we present the performance of our proposed framework and discuss the feasibility

and promise of our approach in breeding programs by comparing performance with manual ground truth.
Finally, the paper is summarized and concluded with directions of future work.

2 Materials and Methods

In this section, we describe data collection and pre-processing steps essential for this project, followed by

details on the deep learning framework for pod detection and counting, a framework for plant detection and
tracking from soybean plots that can enable real-time pod counting (and yield estimation) using a ground

robot with on-board processing.

2.1 Data acquisition and pre-processing

In this paper, we used two different data sets for ML model development as well as validation. The ﬁrst one
is a control data set acquired in an outdoor (i.e., ﬁeld) environment with an effort to use optimal lighting and
other imaging environmental settings. The second in-ﬁeld data set is a more realistic one, collected in the

ﬁeld with soybean crops with diverse environmental variability. Details of these data sets are provided below
along with the speciﬁc pre-processing steps used in this study.

Control Data Set: The images in this set were collected from random 30.5cm sub-sections with soybean
plants from matured soybean plots in 2014 (145 sub-sections) and 2015 (154 sub-sections). Three images

were taken for each sub-section using a tri-fold black background, which was used to remove background
artifacts from the images as seen in Fig. 1. Images were taken with a Canon EOS Rebel T5 in the RAW 18

mega pixel format, and were converted to jpg for processing. The focus and white balance were set to auto,
and were adjusted as per the prevailing conditions. Upon the completion of imaging, plants from the 30.5cm

sections were cut from the ground level using a sharp clipper, and bundled in a bag for pod counting. Care
was taken to ensure no plant part loss occurred, enabling accurate pod counting. One sub-section in 2015 had

four images that were taken instead of three images, so we did not use information from this sub-section.

Expert raters labeled images for the 298 sub-sections using the VIA (VGG Image Annotator) image
labeling software [18] to create the bounding boxes for pods. This data was split into two subsets for training

(247 plots) and testing (51 plots) the ML models. Statistical characteristics of these data sets are provided in
Table 1 and in Fig. 2a.

In-ﬁeld Data Set: This data set consisted of images taken from a ground robot [19]. The ground robot
was outﬁtted with a wooden frame with mounted webcam (Logitech C920) to capture images, ensuring that

3

(a) Left view

(b) Front view

(c) Right view

Figure 1: Sample images from the control data set along the background trifold used to remove background
noise. These three views correspond to the same plot with multiple plants.

full length of the soybean plants in each plot were imaged in each frame (Fig .3a). Videos were captured at
a frame rate of 30 frames per second and at 720p resolution. The camera conﬁguration permitted ﬁlming the

two sides of the soybean plots with fewer passes (Fig .3b). This data was collected from the USDA GRIN
mini core collection [20] genotypes grown in a ﬁeld near Ames, IA, in 15.24cm length and 76.2cm plot to

plot distance. All plots were hand harvested at the R8 growth stage [21], after they were imaged with the
ground robot. The plant height of genotypes from these plots ranged from 25cm to 108cm with a median

height of 70cm and a standard deviation of 15.99 cm. All plots were also rated for lodging on a scale of
1-5 with a score of 1 being all plants being erect, and a score of 5 being prostrate. In this study, 46% of the

plots were scored as a 1, 24% were scored as a 2, 13% were scored as a 3, 10% were scored as a 4, and 7%
were scored as a 5. The genotypes varied in pod and pubescence colors, with 60% of the genotypes having

a brown pod color and 40% had a tan pod color. 26% of the lines had light tawny pubescence, 32% had
tawny pubescence, and 42% had gray pubescence. Genotypes could be further separated into elite, diverse

and PI types representing commercial varieties to unimproved introductions [22]. The overall mean of the
elite genotypes was 641 pods per plot (range of 313 to 1038 pods), diverse genotypes had a mean of 623

(range of 142 to 1058 pods), and PI had a mean of 466 pods (range of 150 to 805 pods).

Overall, we selected 124 plots in this data set. An expert rater determined the start and end of the frame
sequences for each plot in each pass. This is to ensure that the frames accurately correspond to the plots

for which manual pod counting was performed to obtain ground truth. The number of video frames per plot
ranged from 11 to 98, with a median of 38 frames per plot. As we will consider only a few frames (1 to 3 in

this study) per side of a plot to estimate yield, different sets of frames corresponding to a plot can be taken
as different samples. We use this logic to perform data augmentation and form training and test sets with 167

and 43 plot samples, respectively (i.e., total of 210 samples). Statistical characteristics of these data sets are
provided in Table 1 and in Fig. 2b.

4

(a) Pod count distribution for training and test
subsets within the control data set

(b) Pod count distribution for training and test
subsets within the in-ﬁeld data set

Figure 2: Pod count distributions for control and in-ﬁeld data sets

(a) Sensor setup on the ground robot

(b) Diagram demonstrating how the ground robot typically
traverse a soybean ﬁeld test

Figure 3: The top row illustrate the ground robots’ sensors setup and how it traverse the ﬁeld, while images
from (c) to (h) in bottom two rows are typical video frames images, taken by the ground robot, viewing the
same plot from the North and South sides.

5

Table 1: Descriptive statistics of the datasets include the control and in-ﬁeld data sets

Datasets

No. Plots Annotated

Number of Pods

Minimum

Maximum

Mean

Control set

In-ﬁeld set

Train

247

144

704

Test

51

257

831

396.2

423.9

Train

178

142

1058

599.9

Test

44

142

1038

597.2

Standard deviation

99.89

106.61

196.60

198.17

2.2 Machine learning framework for yield estimation

Soybean SY estimation through automated detection and counting of soybean pods is a challenging computer

vision task. Complexity of this problem arises from various factors such as cluttered visual environment,
occlusion of pods in a video frame and lighting variations. One possible approach to address these issues

at least in part is to consider multiple video frames for a plot from different viewing angles. Therefore,
we propose a deep learning-based multi-view image fusion framework that builds on a core model for pod

detection and localization. In addition, to deploy this yield estimation framework on board a robotic platform,
we need to detect and keep track of individual plots in real-time. In this regard, we also build a plot detection

and tracking framework that can provide the necessary video frames for all plants in a speciﬁc plot to the
yield estimation module. The proposed machine learning frameworks are described below.

2.2.1 Pod detection and yield estimation

Our pod count (as measurement of seed yield) estimation model takes multiple RGB images of the same
plot, with multiple plants in commercial planting density, from different viewing angles. The idea is that

by having multiple images from different perspectives, the model could learn to overcome pods occlusion
problems, mitigate possible image quality issues encountered during the automated images selection process

from videos (i.e., sequence of frames), and data heterogeneity encountered in real-life soybean experiment,
breeding and production ﬁelds. The model architecture is presented below:

Model architecture and training process: Our deep learning framework for multi-image fusion has two
primary tasks - (i) pod detection on a soybean plot (with multiple plants) based on an individual image frame

and (ii) estimating an overall pod count per plot based on multiple image frames. We choose a RetinaNet

model architecture [23] with a VGG19 backbone to execute the ﬁrst task. During the ﬁrst phase of training,
we train the RetinaNet model to detect and isolate pods on soybean plants per plot as shown in Fig. 5. For
the control data set, 99 images were randomly selected and annotated using a single class ’Pod’ to train such
a model. On the other hand, we use 513 randomly selected images from the in-ﬁeld data set for training a

RetinaNet model for pod detection in a ﬁeld setting.

After training and validation of a RetinaNet model, we focus on the next task of fusing information from

multiple images to estimate the pod count for a soybean plot. The crux of the idea here is to leverage the
features extracted by the pod detection model from the different images and map them to the overall pod

6

Figure 4: Yield (pod count) estimation model architecture consisting of a feature extraction module (FM) and
a regression module (RM) diagram

count. To implement this, we use the lower 16 convolution layers of the VGG19 backbone of the trained

RetinaNet model as the feature extractors (see [24] for the detailed structure of VGG19). We call this part of

the model the feature extraction module (FM) as shown in Fig. 4. The features extracted by the FM layers
from multiple images are concatenated and are used as inputs in a regression module (RM). The RM has three

consecutive convolution layers, with their respective max-pooling and batch normalization layers, except for
the last convolution layer. These convolution layers are followed by a ﬂatten layer, and three fully connected

layers as shown in Fig. 4. The output of RM is the pod count for the plot consisting of multiple plants. We
freeze the FM layers (taken from a well-trained RetinaNet model for pod detection) in order to train the RM
layers. For the control data set, we started with only the front view image of a plot as the input (to FM) and
then added two side views for the multi-view version of the model. On the other hand, for the in-ﬁeld data
set, input images come in pairs, taken from the opposite sides by the robotic platform. We experiment with
one image from each side (for the single view model) as well as with three images from each side (for the

multi-view model) of the plot. The training and test data distributions are already discussed in Section 2.1
and Table 1. All model variations were trained and validated using a PC Workstation (OS: Ubuntu 18.04,

CPU: Intel Xeon Silver 4108, GPU: Nvidia TITAN XP, RAM: 72 GB).

2.2.2 Plant Detection and Tracking in Plots

In order to deploy the proposed yield estimation framework in an on-board, real-time fashion, we need to

isolate the image frames corresponding to individual plots from the streaming video sequence collected by

7

Figure 5: Plot samples with their respective annotated (left) and detected (right) pods. Detection IoU thresh-
old set to 0.5. Four different examples here show the data diversity in the in-ﬁeld data set

8

a camera on the robotic platform. However, in addition to isolating the image frames, if the plant area can

be isolated in the image frame then that part of the image can be used for pod detection. This can help in
reducing the negative effects of other plants (from other neighboring plots) in the background as well as other

background clutters. Therefore, we ﬁrst focus on detection and isolation of plants from a plot in video frames.
Model architecture and training process: Similar to the pod detection model, we use a RetinaNet
model with VGG19 backbone to detect and isolate the primary plot in an image frame. To train this model,
we annotate about 1000 images from the in-ﬁeld data set with diverse background conditions as well as

diverse shapes and sizes of soybean plants. We use 90% of the annotated samples for training and rest for
validation.

Upon detection, we track the plants in a plot through the video frames with a unique ID tag such that the
pod count estimation process can extract multiple frames for a speciﬁc plot and does not end up over-counting

pods. There are two main aspects in a tracking algorithm. First, we detect and locate the targeted object in
a frame, in our case, a soybean plot with multiple plants. Second, we decide whether the targeted object is

present in subsequent frames. In our speciﬁc implementation, when the detector detects a plot, we save the
information as central point of the bounding box. Each of the new central point is offered a unique ID and the

location of those points are compared with the points in the subsequent frame, using Euclidean distance in a

pair-wise manner. Based on the minimum distance, two central points (i.e., bounding boxes) are assigned to
the same plot. If a new plot appears, the central point of that plot is isolated, and a new unique ID is assigned.

If the current plot disappears or does not get detected, the corresponding central point is saved as an existing
point. The ID is removed if the corresponding central point does not appear for several frames (ﬁve frames

in our implementation).

3 Results and Discussion

In this section, we present the performance of our proposed framework in the context of soybean pod detection

and pod count estimation. We also evaluate the usefulness of our pod count or yield estimation outcomes in
breeding practices. Finally, for applicability of these outcomes in varied plots and images to ensure utilization

in ﬁeld breeding, we show some anecdotal performance of our plant detection and tracking framework.

3.1 Pod detection performance

The pod detection model was validated using the Mean Average Precision (mAP) metric, by setting the
Intersection over Union (IoU) threshold at 0.55. Details of (mAP) and IoU can be found in the supplementary
material. The mAP score for the control data set and in-ﬁeld data set were 0.59 and 0.71, respectively.
However, it is important to note that the control data set had signiﬁcantly lower pod annotations compared to
its in-ﬁeld counterpart. The rationale of having a smaller set of pod annotations in the control data set was

that even with such a small training set, the overall pod count estimation performance was acceptable (see
3.2). Few anecdotal results for pod detection and isolation are presented in Fig. 5.

9

(a) Correlation for the one image input model with
the control data set

(b) Correlation for the three images input model with
control data set

(c) Correlation for the one image (per side) input
model with the in-ﬁeld data set

(d) Correlation for the three images (per side) input
model with the in-ﬁeld data set

Figure 6: Correlations between ground truth and estimated pod counts using one and three images for the
control and in-ﬁeld data sets

3.2 Pod count estimation results

The correlation between the ground truth and the predicted pod counts for both the control data set and the
in-ﬁeld data set are provided in Fig. 6. We observed that fusing multi-view images does help in improving
the correlation between ground truth and prediction for both data sets. However, as expected, the perfor-
mance is better for the control data set (Fig 6 a,b), which can be attributed to the less occlusion and clean
background with sharp color contrast with the foreground objects (plant and pods in this case). Interestingly,
an improved pod count estimation performance for the control data set was noted, despite using a feature
extraction module that shows a lower mAP value for pod detection and localization (due to smaller size of

training data). Although, these moderate correlations may be acceptable for breeding purposes and applica-
tions, the predicted pod counts had narrower ranges compared to the ground truth pod count ranges. This can

be attributed to the training data distribution shown in Fig. 2 where most data points lie close to the mean
value and we end up with unbalanced data sets with less representations from extreme pod count values.

10

3.3 Genotype ranking performance

While the correlation between pod count (i.e., proxy for yield estimation) prediction and the ground truth is
an important metric for our proposed framework; from a breeding practice perspective, it is also important

to make sure that the yield estimation framework is useful to downselect the top performing genotypes. For
example, a 30% selection cut-off means that only those plots that rank among the top 30% (in terms of yield)

are selected to advance to the next generation and subsequent testing (next season or year) in the breeding
program. In this study, we use both 20% and 30% selection criteria to validate our framework, as these are

reasonable downselect (i.e., culling) levels in a breeding program. Standard ML metrics such as accuracy,
sensitivity and speciﬁcity are presented for evaluation [Fig. 7 and Table 2]. However, as our test data set sizes
are rather small (51 test samples for control data set and 44 test samples for in-ﬁeld data set), we also provide
the actual numbers of True Positive, True Negative, False Positive and False Negative samples in Table 2.

Figure 7: Ranking scores for the one and three images per side models on the in-ﬁeld test data set

Table 2: Model predicted ranking results for the top 20% and 30% plots from the in-ﬁeld data set using one
and three images (per side) for pod counting

Ranking
True Positive
True Negative
False Positive
False Negative
Accuracy
Sensitivity
Speciﬁcity

1-img - Control Set

3-imgs - Control Set

1-img - In Field Set

3-imgs - In Field Set

Top 20% Top 30% Top 20% Top 30% Top 20% Top 30% Top 20% Top 30%

7
37
3
4
0.86
0.64
0.93

12
33
3
3
0.88
0.80
0.92

7
37
3
4
0.86
0.64
0.93

10
31
5
5
0.80
0.67
0.86

3
31
5
5
0.77
0.38
0.86

8
26
5
5
0.77
0.62
0.84

3
31
5
5
0.77
0.38
0.86

9
27
4
4
0.82
0.69
0.87

From the results, it is clear that performance for the control data set is slightly better compared to that
for the in-ﬁeld data set, which conforms with our earlier correlation results, and is also true from the do-
main experience. However, we do not observe a signiﬁcant improvement in performance with the usage of

11

multi-view images as opposed to only single view images. For the in-ﬁeld data set, we noted that single

view images from both sides of the plots are still needed [shown in Fig. 3(c-h)]. Overall, our results show
that the proposed framework could be quite useful for selecting top performing genotypes especially when

using a 30% selection criteria compared to a 20% selection criteria as evidenced with a higher sensitivity
score. However, if a plant breeder is more interested in discarding the bottom performers, they can achieve

reasonable success at 20% selection level too due to high speciﬁcity and a lower sensitivity scores. This is
particularly of importance in early stages of yield testing, where breeders are more concerned about ”dis-

carding” unworthy entries rather than ”select” the top performers as the tests do not have sufﬁcient statistical
power to separate mean performance corresponding to the phenotypic and breeding values. With an improve-

ment in test data size it is possible that model performance (i.e., sensitivity) may also improve enabling high
conﬁdence using more stringent downselection or culling levels. With small test data sets, it is difﬁcult to

draw strong conclusions in such a discrete classiﬁcation setting (where, even one or two samples can change
the overall statistics). Therefore, future work will focus on substantially increasing the test data set size to

arrive at statistically more signiﬁcant conclusions.

3.4 Plot detection and tracking performance

We provide few anecdotal results of our plot detection and tracking models in Fig. 8. We report that plants

of various sizes and shapes can be detected and sufﬁciently isolated despite a cluttered background with very
low contrast. Although in practice we see that our plot detection and tracking system is mostly reliable,

performance can suffer in low-light conditions as well as in severe occlusion scenarios, speciﬁcally due to
large and lodged (non-upright) plants. Our future work will go beyond this anecdotal study to generate

statistically signiﬁcant quantitative results for a fully end-to-end on-board real-time soybean yield estimation
system. However, we clearly show the feasibility of such a system in this paper.

4 Conclusion and Future work

In this paper, we propose a method capable of reducing overhead in yield testing trial. This was achieved

with minimum human intervention by properly estimating yield, using pod count estimation, and ranking
soybean genotypes for making breeding decisions. Our proposed method uses a deep learning framework

that performs soybean pod detection and yield estimation using single or (fusing) multiple RGB images of a
plant collected from a mobile motorized ground phenotyping unit. Our experiment showed relevant accurate

results in a controlled outdoor environment; although, model performance was lower for the outdoor in-ﬁeld
image set, the results are still quite promising. We attribute the degradation of the model prediction to the

lower quality of the data we had from the outdoor image set in comparison to the control one. The bottleneck

of our experiments was not the ability to image plots, but the time and effort it took to manually count pods
for every plot at a very high level of ﬁdelity. One of the beneﬁts of this type of yield estimation, is that

it focuses on physically quantifying every pod in a plot, which has a direct correlation with the yield. We
observed a correlation of 0.76 and 0.82 between manual pod count and seed yield for the in-ﬁeld and control

sets respectively. An average of 2.0 and 2.2 seed/pod were noted for in-ﬁeld and control set, respectively.

12

Plot 1

Plot 2

Plot 3

Plot 4

(a) Samples of plots detected while the robot travels on the same row

Frame No. 132

Frame No. 134

Frame No. 154

Frame No. 162

(b) Sequences of images frames while detecting and tracking a single soybean plot from an input video

Figure 8: Samples of plot detection and tracking from recorded videos by the ground robot system.

13

The results of ML based pod counting method are now being integrated in our soybean breeding program

at Iowa State University for breeding applications, while we continue to expand our research. First, we are
exploring to improve the image gathering quality using an online feedback control system that interact with

the robot to navigate the ﬁelds in an automated manner. Second, we are developing algorithms capable of
matching video frames to accurate plant locations and determine the best frames to use for a plot. Both data

sets were relatively small in size compared to many other use cases of deep learning frameworks. Therefore,
increasing the data set size may show better results than what we were able to obtain. We note that active

learning algorithms will also be useful to reduce the amount of labeling needed by deep learning models to
achieve good predictive performance ([25]).

Although, pod estimation is a proxy or surrogate trait for yield estimation, it is one of the important yield
component of overall seed yield [26]. Most other yield prediction methodologies in plant science have fo-

cused on above canopy measurements such as reﬂectance measurements [8], [7] , and canopy coverage [27].
Future work should focus on the above problems, as well as moving integration of automated ways to obtain-

ing yield component traits with indirect estimation of physiological traits and indices. These can be deployed
at all ﬁeld and controlled environment growing conditions, allowing for larger sampling sizes without the

needs for labor intensive pod counting tasks and/or machine harvest of all plots. If high ﬁdelity rankings

can be achieved in full breeding plot tests, this methodology could help to greatly reduce the labor and time
required for harvest operations in a breeding program in any give year. This will allow for less issues related

to timely harvesting of plots, as well as faster decision making in a breeding program, with data available to
a breeder sooner than would typically be available with traditional harvest methods. One important thing to

consider however would be to appropriately analyze the data when it is in the ranking format instead of actual
yield. In this case, it will be more difﬁcult for a breeder to identify outliers, and perform spatial adjustments

to a breeding trial when the plots are not set to yield, but rather to ranks. Although, with continual model
advancements, actual yield prediction and not just rankings is not inconceivable. We are continuing to deploy

these ML based methods for trait phenotpying including root nodule count - Soybean Nodule Acquisition
Pipeline (SNAP) that quantiﬁes nodule by combining RetinaNet and UNet deep learning architectures for

object (i.e., nodule) detection and segmentation [28] as well as for microscopic nematode egg detection in
cluttered images [29] . ML and more speciﬁcally DL methods continue to open previously inconceivable

phenotyping doors for breeding and research application.

14

Supplementary Material

The mAP is calculated according to Eq. (1), where TP, FP, TN, and FN denote the true positive, false positive,

true negative and false negative, respectively. The thresholds for in Eq. (1) was set to 0.5, this means that any
predicted object is considered a TP if its’ IoU with respect to the ground truth is greater than 0.5.

mAP =

1

|threshold| X

t

T P (t)
T P (t) + F P (t) + F N (t)

Intersection over union (IoU) criteria is computed by Eq. (2).

IoU (A, B) =

A ∩ B
A ∪ B

(1)

(2)

We calculated the model Speciﬁcity Eq. (3) to measure its ability to correctly identify those plots that
did not meet the Top ranking criterium and the Sensitivity Eq. (4) measure that ability to identify those plots

meet the selecting criterium correctly. At the same time, Accuracy Eq. (5) measured the closeness of the
predictions to a speciﬁc value.

Specif icity =

T N
T N + F P

Recall(Sensitivity) =

T P
T P + F N

Accuracy =

T P + T N
T P + T N + F P + F N

(3)

(4)

(5)

The correlation Eq. (6) was used as an indicator to measure the linear relationship between the model

predictions and the ground truth values.

ρX,Y = corr(X, Y ) =

cov(X, Y )
σX σY

=

E[(X − E(X))(Y − E(Y )]
σX σY

(6)

15

Feature Module architecture Summary
Output Shape

Layer (type)

input 1 (InputLayer)

(None, None, None, 3)

block1 conv1 (Conv2D)

(None, None, None, 64)

block1 conv2 (Conv2D)

(None, None, None, 64)

No. Param

0

1792

36,928

block1 pool (MaxPooling2D)

(None, None, None, 64)

0

block2 conv1 (Conv2D)

(None, None, None, 128)

73,856

block2 conv2 (Conv2D)

(None, None, None, 128)

14,7584

block2 pool (MaxPooling2D)

(None, None, None, 128)

0

block3 conv1 (Conv2D)

(None, None, None, 256)

block3 conv2 (Conv2D)

(None, None, None, 256)

block3 conv3 (Conv2D)

(None, None, None, 256)

block3 conv4 (Conv2D)

(None, None, None, 256)

29,5168

590,080

590,080

590,080

block3 pool (MaxPooling2D)

(None, None, None, 256)

0

block4 conv1 (Conv2D)

(None, None, None, 512)

1,180,160

block4 conv2 (Conv2D)

(None, None, None, 512)

2,359,808

block4 conv3 (Conv2D)

(None, None, None, 512)

2,359,808

block4 conv4 (Conv2D)

(None, None, None, 512)

2,359,808

block4 pool (MaxPooling2D)

(None, None, None, 512)

0

block5 conv1 (Conv2D)

(None, None, None, 512)

2,359,808

block5 conv2 (Conv2D)

(None, None, None, 512)

2,359,808

block5 conv3 (Conv2D)

(None, None, None, 512)

2,359,808

block5 conv4 (Conv2D)

(None, None, None, 512)

2,359,808

block5 pool (MaxPooling2D)

(None, None, None, 512)

0

C5 reduced (Conv2D)

(None, None, None, 256)

131,328

Total params: 20,155,712
Trainable params: 20,155,712

Estimator Module architecture summary,

Layer (type)

add 1 (Add)

Output Shape

No. Param

(None, 7, 7, 256)

0

e cov2 (Conv2D)

(None, 5, 5, 256)

590,080

e cov2 max (MaxPooling2D)

(None, 2, 2, 256)

ﬂatten 1 (Flatten)

(None, 1024)

e den1 (Dense)

e den2 (Dense)

e den3 (Dense)

(None, 16)

(None, 8)

(None, 1)

Total params: 20,762,337
Trainable params: 606,625

Non-trainable params: 20,155,712

0

0

164

16

9

16

References

[1] L. Hazel and J. L. Lush, “The efﬁciency of three methods of selection,” Journal of Heredity, vol. 33,

no. 11, pp. 393–399, 1942.

[2] A. Xavier, W. M. Muir, and K. M. Rainey, “Assessing predictive properties of genome-wide selection

in soybeans,” G3: Genes, Genomes, Genetics, vol. 6, no. 8, pp. 2611–2616, 2016.

[3]

J. Zhang, Q. Song, P. B. Cregan, and G.-L. Jiang, “Genome-wide association study, genomic predic-
tion and marker-assisted selection for seed weight in soybean (glycinemax),” Theoretical and Applied
Genetics, vol. 129, no. 1, pp. 117–130, 2016.

[4] A. L. Harfouche, D. A. Jacobson, D. Kainer, J. C. Romero, A. H. Harfouche, G. S. Mugnozza, M.
Moshelion, G. A. Tuskan, J. J. Keurentjes, and A. Altman, “Accelerating climate resilient plant breed-
ing by applying next-generation artiﬁcial intelligence,” Trends in biotechnology, 2019.

[5] G. Rebetzke, J. Jimenez-Berni, R. Fischer, D. Deery, and D. Smith, “High-throughput phenotyping to

enhance the use of crop genetic resources,” Plant Science, vol. 282, pp. 40–48, 2019.

[6] Y. Zhang, C. Zhao, J. Du, X. Guo, W. Wen, S. Gu, J. Wang, and J. Fan, “Crop phenomics: Current

status and perspectives,” Frontiers in Plant Science, vol. 10, p. 714, 2019.

[7] K. A. Parmley, R. H. Higgins, B. Ganapathysubramanian, S. Sarkar, and A. K. Singh, “Machine learn-

ing approach for prescriptive plant breeding,” Scientiﬁc reports, vol. 9, no. 1, pp. 1–12, 2019.

[8] K. Parmley, K. Nagasubramanian, S. Sarkar, B. Ganapathysubramanian, A. K. Singh, et al., “Develop-

ment of optimized phenomic predictors for efﬁcient plant breeding decisions using phenomic-assisted
selection in soybean,” Plant Phenomics, vol. 2019, p. 5 809 404, 2019.

[9] Z. Jiang, C. Liu, B. Ganapathysubramanian, D. J. Hayes, and S. Sarkar, “Predicting county-scale maize

yields with publicly available data,” Scientiﬁc Reports, vol. 10, no. 1, pp. 1–12, 2020.

[10] T. Gangopadhyay, J. Shook, A. K. Singh, and S. Sarkar, “Deep time series attention models for crop
yield prediction and insights,” in NeurIPS Workshop on Machine Learning and the Physical Sciences,

Vancouver, Canada, 2019.

[11] T. Gao, H. Emadi, H. Saha, J. Zhang, A. Lofquist, A. Singh, B. Ganapathysubramanian, S. Sarkar,
A. K. Singh, and S. Bhattacharya, “A novel multirobot system for plant phenotyping,” Robotics, vol. 7,
no. 4, p. 61, 2018.

[12] R. Fern´andez, C. Salinas, H. Montes, and J. Sarria, “Multisensory system for fruit harvesting robots.
experimental testing in natural scenarios and with different kinds of crops,” Sensors, vol. 14, no. 12,
pp. 23 885–23 904, 2014.

[13] S. Nuske, S. Achar, T. Bates, S. Narasimhan, and S. Singh, “Yield estimation in vineyards by visual
grape detection,” in 2011 IEEE/RSJ International Conference on Intelligent Robots and Systems, IEEE,
2011, pp. 2352–2358.

[14] D. Bulanon, T. Burks, and V. Alchanatis, “Image fusion of visible and thermal images for fruit detec-

tion,” Biosystems engineering, vol. 103, no. 1, pp. 12–22, 2009.

17

[15] A. Gongal, A. Silwal, S. Amatya, M. Karkee, Q. Zhang, and K. Lewis, “Apple crop-load estimation
with over-the-row machine vision system,” Computers and Electronics in Agriculture, vol. 120, pp. 26–
35, 2016.

[16] R. Linker, “Machine learning based analysis of night-time images for yield prediction in apple or-

chard,” Biosystems Engineering, vol. 167, pp. 114–125, 2018.

[17] H. Lu, Z. Cao, Y. Xiao, B. Zhuang, and C. Shen, “Tasselnet: Counting maize tassels in the wild via

local counts regression network,” Plant methods, vol. 13, no. 1, p. 79, 2017.

[18] A. Dutta, A. Gupta, and A. Zissermann, “Vgg image annotator (via)(2016),” URL http://www. robots.

ox. ac. uk/˜ vgg/software/via, 2018.

[19] S. Robots, Inc.; superdroid robts; robots, parts and custom solutions; lt-f data sheet.

[20] M. F. Oliveira, R. L. Nelson, I. O. Geraldi, C. D. Cruz, and J. F. F. de Toledo, “Establishing a soybean

germplasm core collection,” Field Crops Research, vol. 119, no. 2-3, pp. 277–289, 2010.

[21] W. R. Fehr and C. E. Caviness, “Stages of soybean development,” 1977.

[22] Q. Song, L. Yan, C. Quigley, B. D. Jordan, E. Fickus, S. Schroeder, B.-H. Song, Y.-Q. Charles An,
D. Hyten, R. Nelson, et al., “Genetic characterization of the soybean nested association mapping pop-
ulation,” The Plant Genome, vol. 10, no. 2, pp. 1–14, 2017.

[23] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss for dense object detection,” in
Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980–2988.

[24] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,”

arXiv preprint arXiv:1409.1556, 2014.

[25] K. Nagasubramanian, T. Z. Jubery, F. F. Ardakani, S. V. Mirnezami, A. K. Singh, A. Singh, S. Sarkar,
and B. Ganapathysubramanian, “How useful is active learning for image-based plant phenotyping?”
arXiv preprint arXiv:2006.04255, 2020.

[26]

J. Pandey and J. Torrie, “Path coefﬁcient analysis of seed yield components in soybeans (glycine max
(l.) merr.) 1,” Crop Science, vol. 13, no. 5, pp. 505–507, 1973.

[27] T. Z. Jubery, J. Shook, K. Parmley, J. Zhang, H. S. Naik, R. Higgins, S. Sarkar, A. Singh, A. K. Singh,
and B. Ganapathysubramanian, “Deploying fourier coefﬁcients to unravel soybean canopy diversity,”
Frontiers in plant science, vol. 7, p. 2066, 2017.

[28] T. Z. Jubery, C. Carley, S. Sarkar, A. Singh, B. Ganapathysubramanian, and A. K. Singh, “Using
machine learning to develop a fully automated soybean nodule acquisition pipeline (snap),” bioRxiv,

2020.

[29] A. Akintayo, G. L. Tylka, A. K. Singh, B. Ganapathysubramanian, A. Singh, and S. Sarkar, “A deep
learning framework to discern and count microscopic nematode eggs,” Scientiﬁc reports, vol. 8, no. 1,

pp. 1–11, 2018.

18

