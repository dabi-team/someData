Robust Value Iteration for Continuous Control Tasks

Michael Lutter1,2, Shie Mannor1,3, Jan Peters2, Dieter Fox1,4, Animesh Garg1,5
1Nvidia, 2TU Darmstadt, 3 Technion, 4 University of Washington, 5University of Toronto & Vector Institute

1
2
0
2

y
a
M
5
2

]

G
L
.
s
c
[

1
v
9
8
1
2
1
.
5
0
1
2
:
v
i
X
r
a

Abstract—When transferring a control policy from simulation
to a physical system, the policy needs to be robust to variations
in the dynamics to perform well. Commonly, the optimal policy
overﬁts to the approximate model and the corresponding state-
distribution, often resulting in failure to trasnfer underlying
distributional shifts. In this paper, we present Robust Fitted
Value Iteration, which uses dynamic programming to compute
the optimal value function on the compact state domain and
incorporates adversarial perturbations of the system dynamics.
The adversarial perturbations encourage a optimal policy that
is robust to changes in the dynamics. Utilizing the continuous-
time perspective of reinforcement learning, we derive the optimal
perturbations for the states, actions, observations and model
parameters in closed-form. Notably, the resulting algorithm does
not require discretization of states or actions. Therefore, the
optimal adversarial perturbations can be eﬃciently incorporated
in the min-max value function update. We apply the resulting
algorithm to the physical Furuta pendulum and cartpole. By
changing the masses of the systems we evaluate the quantitative
and qualitative performance across diﬀerent model parameters.
We show that robust value iteration is more robust compared
to deep reinforcement learning algorithm and the non-robust
version of the algorithm. Videos of the experiments are shown
at https://sites.google.com/view/rfvi

I. Introduction

To avoid the laborious and potentially dangerous training of
control policies on the physical system, Simulation to reality
transfer (sim2real) learns a policy in simulation and evaluates
the policy on the physical system. When transferred to the
real world, the policy should solve the task and obtain a
comparable reward to the simulation. Therefore, the goal of
sim2real is to learn a policy that is robust to changes in
the dynamics and successfully bridges the simulation-reality
gap. Naive reinforcement learning (RL) methods usually do
not succeed for sim2real as the resulting policies overﬁt to
the approximate simulation model. Therefore, the resulting
policies are not robust to changes in the dynamics and fail to
solve the task in the real world. In contrast, sim2real methods
extending RL with domain randomization [1–4] or adversarial
disturbances [5–8] have shown the successful transfer to the
physical world [9].

In this paper, we focus on adversarial disturbances to bridge
the simulation-reality gap. In this paradigm the RL problem
is formulated as a two player zero-sum game [6]. The optimal
policy wants to maximize the reward while the adversary
wants to minimize the reward. For control tasks, the policy
controls the system input u and the adversary ξ controls
the dynamics. For example, the adversary can change the
state, action, observation, model parameters or all of them
within limits. Therefore, this problem formulation optimizes

The control chart of robust ﬁtted value iteration (rFVI) for
Figure 1.
continuous states, actions and time. The deterministic optimal policy and
deterministic adversaries, which add an bias to the system dynamics, only
depend on shared value function. While the optimal policy performs hill-
climbing the adversaries perform steepest descent following the value function
gradient.

the worst-case performance and not the expected reward as
standard RL. The resulting policy is robust to changes in the
dynamics as the planning in simulation uses the worst-case
approximation which includes the physical system [10].

In this adversarial RL setting,
(1) we show that the optimal action and optimal adversary
can be directly computed using the value function estimate if
the reward is separable into state and action reward and the
continuous-time dynamics are non-linear and control-aﬃne.
We derive the solution for the state, action, observation and
this paper extends the
model bias analytically. Therefore,
existing analytic solutions from continuous-time RL [11–14].
(2) we propose robust ﬁtted value iteration (rFVI). This al-
gorithm solves the adversarial RL problem with continuous
states, actions and disturbances by leveraging the analytic
expressions and value iteration. Using this approach,
the
continuous states and actions do not need to be discretized as
in classical methods [15–17] or require multiple optimizations
as the modern actor-critic approaches [7, 18].
(3) we provide an in-depth evaluation of rFVI on the physical
system. We benchmark this algorithm on the real-world Furuta
pendulum and cartpole. To test the robustness, we perturb
the model parameters by applying additional weights. The
performance is compared to standard deep RL algorithms with
and without domain randomization.

Therefore the contributions of this paper are the derivation
of the analytic adversarial actions, the introduction of robust
ﬁtted value iteration and the extensive evaluation on multiple
physical systems. In the evaluation we focus on two under-
actuated systems to provide an in-depth qualitative and quan-

∫!(#)̇"#"$(";')!!%#;'+)#;'*"+!"+!#+!$ 
 
 
 
 
 
Figure 2. The optimal Value function 𝑉 ∗ and policy 𝜋∗ of rFVI, cFVI and four diﬀerent variations of SAC. All policies achieve nearly identical reward
on the nominal dynamics model. The variations of SAC demonstrate the change of the policy when increasing the entropy of the state distribution during
training. The entropy is increased by enlarging the initial state distribution 𝜇 and using domain randomization. For SAC and 𝜇 = N (± 𝜋, 𝜎) the optimal
policy is only valid on the optimal trajectory. For SAC UDR and 𝜇 = U (− 𝜋, + 𝑝𝑖), the policy is applicable on the complete state domain. rFVI and cFVI
perform value iteration on the compact state domain and naturally obtain an optimal policy applicable on the complete state-domain. rFVI adapts 𝑉 ∗ and 𝜋∗
to have a smaller ridge leading up to the upright pendulum and exerts higher actions when deviating from the optimal trajectory.

titative analysis of the various algorithms in the physical world.

In the following we introduce the continuous-time formulation
of adversarial RL (Section II). Section III derives the optimal
adversaries and introduces robust ﬁtted value iteration. Finally,
we describe the experimental setup and report the performance
of the algorithms on the physical system in Section IV.

II. Problem Statement

The inﬁnite horizon continuous-time RL optimization with the
adversary ξ is described by

𝜋∗(x) = arg max

𝜋

inf
ξ∈Ω

𝑉 ∗(x) = max
u

inf
ξ∈Ω

x(𝑡) = x0 +

∫ ∞

0
∫ ∞

0
∫ 𝑡

0

exp(−𝜌𝑡) 𝑟𝑐 (x𝑡 , u𝑡 ) 𝑑𝑡,

exp(−𝜌𝑡) 𝑟𝑐 (x𝑡 , u𝑡 ) 𝑑𝑡,

𝑓𝑐 (x𝜏, u𝜏, ξ𝜏) 𝑑𝜏,

(1)

(2)

(3)

with the state x, action u, admissible set Ω, discounting
constant 𝜌 ∈ (0, ∞], reward 𝑟𝑐, dynamics 𝑓𝑐, optimal value
function 𝑉 ∗ and policy 𝜋∗ [6, 13]. The order of the optimiza-
tions can be switched as the optimal actions and disturbance
remain identical [6]. The adversary ξ must be constrained to
be in the set of admissible disturbances Ω as otherwise the
adversary is too powerful and would prevent the policy from
learning the task.

The deterministic continuous-time dynamics 𝑓𝑐 is assumed to
be non-linear w.r.t. the system state x but aﬃne w.r.t. the
action u. Such dynamics are described by

(cid:164)x = a(x; θ) + B(x; θ)u,

(4)

with the non-linear drift a, the non-linear control matrix B
and the system parameters θ. We assume that the approximate
parameters ˆθ and the approximate equation of motions ˆa,
ˆB
are known. For most rigid-body systems this assumptions is
feasible as the equations of motions can be derived analytically
and the system parameters can be measured. The resulting
model is only approximate due to the idealized assumption of

rigid bodies, measurement error of the system parameters and
neglecting the actuator dynamics.

The optimal policy and adversary is modeled as a stationary
and Markovian policy that applies a state-dependent distur-
bance. In this case, the worst-case action is deterministic if
the dynamics are deterministic. If the adversary would be
stochastic, the optimal policies are non-stationary and non-
Markovian [19]. This assumption is used in most of the
existing literature on adversarial RL [8, 13, 19, 20]. We
consider four diﬀerent adversaries that alter the state [20–
22], action [7, 13, 18, 23–25], observation [8, 19] and model
parameters [8]. The diﬀerent adversaries address potential
causes of the simulation gap. The state adversary ξ𝑥 incorpo-
rates unmodeled physical phenomena in the simulation. The
action adversary ξ𝑢 addresses the non-ideal actuators. The
observation adversary ξ𝑜 introduces the non-ideal observations
caused by sensors. The model adversary ξ𝜃 introduces a bias
to the system parameters. All adversaries could be subsumed
via a single adversary with large admissible set. However, the
resulting dynamics would not capture underlying structure of
the simulation gap [8] and the optimal policy would be too
conservative [26]. Therefore, we disambiguate between the
diﬀerent adversaries to capture this structure. Mathematically
the models are described by

State ξ𝑥 :
Action ξ𝑢 :
Observation ξ𝑜 :
Model ξ𝜃 :

(cid:164)x = a(x; θ) + B(x; θ)u + ξx,
(cid:164)x = a(x; θ) + B(x; θ) (u + ξu) ,
(cid:164)x = a(x + ξo; θ) + B(x + ξo; θ) u,
(7)
(cid:164)x = a(x; θ + ξ𝜃 ) + B(x; θ + ξ𝜃 ) u. (8)

(5)

(6)

Instead of disturbing the observation, Equation 7 disturbs
the simulation state of the drift and control matrix. This
disturbance is identical to changing the observed system state.

The deterministic perturbation is in contrast to the standard
RL approaches, which describe ξ𝑖 as a stochastic variable.
However, this diﬀerence is due to the worst-case perspective,
where one always selects the worst sample at each state.
This worst case sample is deterministic if the policies are

Entropy of state distribution during training7.50.07.5 [rad/s]V*(x)SAC - =SAC & UDR - =SAC - =USAC & UDR - =UcFVIrFVI±/20+/2 [rad]7.50.07.5 [rad/s]*(x)±/20+/2 [rad]±/20+/2 [rad]±/20+/2 [rad]±/20+/2 [rad]±/20+/2 [rad]Table I
The optimal actions u𝑘 and adversarial actions 𝜉 𝑘 for the state-, action-, model- and observation bias with the admissible set Ω.

State Perturbation

Action Perturbation

Model Perturbation

Observation Perturbation

Dynamics 𝑓𝑐 (x, u, 𝜉 )
Optimal Action u𝑘

Optimal Disturbance 𝜉 𝑘

a(x) + B (x)u + 𝜉
∇ ˜𝑔 (B (x)𝑇 ∇𝑥 𝑉 𝑘 )
(cid:16)

∇𝑥 𝑉 𝑘 (cid:17)

−ℎΩ

a(x) + B (x) (u + 𝜉 )
∇ ˜𝑔 (B (x)𝑇 ∇𝑥 𝑉 𝑘 )
B𝑇 ∇𝑥 𝑉 𝑘 (cid:17)

−ℎΩ

(cid:16)

stationary and Markovian. The ﬁltering approaches of state-
estimation are not applicable to this problem formulation as
these approaches cannot infer a state-dependent bias.

i.e.,

The reward is separable into state reward 𝑞𝑐 and action reward
𝑔𝑐 described by

𝑟𝑐 (x, u) = 𝑞𝑐 (x) − 𝑔𝑐 (u).

(9)

The action cost is non-linear, positive deﬁnite and strictly
convex. The assumptions on the action cost are not limiting
as these resulting properties are desirable. The convexity of
the action cost enforces that the optimal action is unique. The
positive deﬁniteness of 𝑔𝑐 penalizes non-zero actions, which
prevents the bang-bang controller to be optimal.

III. Robust Fitted Value Iteration

In the following, we summarize continuous ﬁtted value itera-
tion (cFVI) [27]. Afterwards, we derive the analytic solutions
for the optimal adversary and present robust ﬁtted value itera-
tion (rFVI). This algorithm solves the adversarial RL problem
and obtains a robust optimal policy. In contrast, cFVI only
solves the deterministic RL problem and obtains an optimal
policy that overﬁts to the approximate dynamics model.

A. Preliminaries - Continuous Fitted Value Iteration

Continuous ﬁtted value iteration (cFVI) [27] extends the
classical value iteration approach to compute the optimal value
function for continuous action and state spaces. By showing
that the optimal policy can be computed analytically, the value
iteration update can be computed eﬃciently. For non-linear
control-aﬃne dynamics (Equation 4) and separable reward
(Equation 9), the optimal action is described by

𝜋𝑘 (x) = ∇ ˜𝑔𝑐

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

(10)

with current value function ∇𝑥𝑉 𝑘 and the convex conjugate of
the action cost ˜𝑔𝑐 [11, 12, 14]. The convex conjugate is deﬁned
as ∇ ˜𝑔(w) = [∇𝑔(w)]−1. For a quadratic action cost, ∇ ˜𝑔 is a
linear transformation. For barrier shaped action cost, ∇ ˜𝑔 re-
scales and limits the action range. This solution is intuitive as
the optimal policy performs hill climbing on the value function
manifold and action cost determines the step size.

Substituting the analytic policy into the value iteration update,
the classical algorithm can be extended to continuous actions,

a( 𝜃 + 𝜉 ) + B ( 𝜃 + 𝜉 )u
∇ ˜𝑔 (B (x)𝑇 ∇𝑥 𝑉 𝑘 )
(cid:18) (cid:16) 𝜕a
𝜕x + 𝜕B

𝜕𝜃 u𝑘 (cid:17)𝑇

∇𝑥 𝑉 𝑘

−ℎΩ

a(x + 𝜉 ) + B (x + 𝜉 )u
∇ ˜𝑔 (B (x)𝑇 ∇𝑥 𝑉 𝑘 )
(cid:18) (cid:16) 𝜕a
𝜕x u𝑘 (cid:17)𝑇
𝜕x + 𝜕B

∇𝑥 𝑉 𝑘

−ℎΩ

(cid:19)

(cid:19)

𝑉 𝑘+1
tar = max
u
(cid:16)
= 𝑟

𝑟 (x, u) + 𝛾𝑉 𝑘 ( 𝑓 (x𝑡 , u))
+ 𝛾𝑉 𝑘 (cid:16)

x𝑡 , 𝜋𝑘 (x𝑡 )

(cid:17)

(cid:16)

𝑓

x𝑡 , 𝜋𝑘 (x𝑡 )

(11)

(12)

(cid:17)(cid:17)

.

Therefore, the closed-form policy enables the eﬃcient com-
putation of the target. Combined with function approximation
for the value function [28–31], classical value iteration can
be extended to continuous state and action spaces without
discretization. The ﬁtting of the value function is described
by,

𝜓𝑘+1 = arg min

𝜓

∑︁

x

(cid:107)𝑉 𝑘+1
tar

(x) − 𝑉 (x; 𝜓)(cid:107)

𝑝
𝑝,

(13)

with (cid:107) · (cid:107) 𝑝 being the ℓ𝑝 norm. Iterating between computing
𝑉tar and ﬁtting the value function, learns the optimal value
function 𝑉 ∗ and policy 𝜋∗.

B. Deriving the Optimal Disturbances

For the adversarial RL formulation, the value function target
contains a max-min optimization described by

𝑉 𝑘+1
tar

(x) = max
u

inf
ξ∈ Ω

𝑟 (x, u) + 𝛾𝑉 𝑘 (cid:0) 𝑓 (x, u, ξ)(cid:1).

(14)

To eﬃciently obtain the value function update, the optimal
action and the optimal disturbance need to be computed in
closed form. We show that this optimization problem can be
solved analytically for the described dynamics and disturbance
models (Section II). Therefore, the adversarial RL problem can
be solved by value iteration.

𝑖 have a
The resulting optimal actions u∗ and disturbances ξ∗
coherent intuitive interpretation. The optimal actions perform
steepest ascent by following the gradient of the value func-
tion ∇𝑥𝑉. The optimal perturbations perform steepest descent
by following the negative gradient of the value function. The
step-size of policy and adversary is determined by the action
cost 𝑔 or the admissible set Ω. The optimal policy and the
optimal adversary is described by

u𝑘 = ∇ ˜𝑔

𝑇

(cid:18) 𝜕 𝑓𝑐 (.)
𝜕u

(cid:19)

∇𝑥𝑉 𝑘

, ξ𝑘

𝑖 = −ℎΩ

𝑇

(cid:18) 𝜕 𝑓𝑐 (.)
𝜕ξ𝑖

(cid:19)

∇𝑥𝑉 𝑘

. (15)

In the following we abbreviate [𝜕 𝑓𝑐 (.)/𝜕y]𝑇 ∇𝑥𝑉, as z𝑦. For
the adversarial policy, ℎΩ rescales z 𝜉 to be on the boundary
of the admissible set. If the admissible set bounds the signal

energy to be smaller than 𝛼, the disturbance is rescaled to have
the length 𝛼. Therefore, the adversary is described by

Ω𝐸 = {ξ ∈ R𝑛 | (cid:107)ξ(cid:107)2 ≤ 𝛼} ⇒ ℎ𝐸 (z 𝜉 ) = 𝛼

z 𝜉
(cid:107)z 𝜉 (cid:107)2

.

(16)

If the amplitude of the disturbance is bounded, the disturbance
performs bang-bang control. In this case the adversarial policy
is described by

Ω𝐴 = {ξ ∈ R𝑛 | νmin ≤ ξ ≤ νmax}

⇒ ℎ 𝐴(z 𝜉 ) = 𝚫 sign (cid:0)z 𝜉 (cid:1) + µ,

(17)

with µ = (νmax + νmin) /2 and 𝚫 = (νmax − νmin) /2.
The following theorems derive Equation 15, 16 and 17 for
the optimal policy and the diﬀerent disturbances. Theorem 1
describes the state adversary, Theorem 2 describes action ad-
versary, Theorem 3 the observation adversary and Theorem 4
describes the model adversary. Following the theorems, we
provide sketches of the proofs for the state and model dis-
turbance. The remaining proofs are analogous. The complete
proofs for all theorems are provided in the appendix. All
solutions are summarized in Table I.

Theorem 1. For the adversarial state disturbance (Equa-
tion 5) with bounded signal energy (Equation 16), the optimal
continuous-time policy 𝜋 and state disturbance ξ𝑥 is described
by

𝜋(x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉

(cid:17)

,

ξ𝑥 = −𝛼

∇𝑥𝑉
(cid:107)∇𝑥𝑉 (cid:107)2

.

Theorem 2. For the adversarial action disturbance (Equa-
tion 6) with bounded signal energy (Equation 16), the opti-
mal continuous-time policy 𝜋 and action disturbance ξ𝑢 is
described by

𝜋(x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉

(cid:17)

,

ξ𝑢 = −𝛼 B(x)𝑇 ∇𝑥𝑉
(cid:107)B(x)𝑇 ∇𝑥𝑉 (cid:107)2

.

Proof Sketch Theorem 1 For the admissible set Ω𝐸 , Equa-
tion 14 can be written with the explicit constraint. This
optimization is described by

𝑉tar = max
u

min
ξ𝑥

𝑟 (x, u) + 𝛾𝑉 (cid:0) 𝑓 (x, u, ξ𝑥)(cid:1)

s.t. ξ𝑇

𝑥 ξ𝑥 ≤ 𝛼2.

Substituting the Taylor expansion for 𝑉 (x𝑡+1), the dynamics
model and the reward, the optimization is described by

𝑉tar = max
u

min
ξ𝑥

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾O (x, u, Δ𝑡)Δ𝑡

with the higher order terms O (x, u, Δ𝑡). In the continuous-
time limit, the higher-order terms and the discounting disap-
pear, i.e., limΔ𝑡→0 O (x, u, Δ𝑡)=0 and limΔ𝑡→0 exp(−𝜌Δ𝑡)=1.
Therefore, the optimal action is described by

u𝑡 = arg max

∇𝑥𝑉𝑇 B u − 𝑔𝑐 (u) ⇒ u𝑡 = ∇ ˜𝑔𝑐 (cid:0)B𝑇 ∇𝑥𝑉 (cid:1).

u

The optimal state disturbance is described by

ξ∗
𝑥 = arg min

∇𝑥𝑉𝑇 ξ𝑥

s.t.

ξ𝑥

1
2

(cid:2)ξ𝑇

𝑥 ξ𝑥 − 𝛼2(cid:3) ≤ 0.

Algorithm 1 Robust Fitted Value Iteration (rFVI)

Input: Model 𝑓𝑐 (x, u), Dataset D & Admissible Set Ω 𝜉
Result: Value Function 𝑉 ∗(x; 𝜓∗)
while not converged do

// Compute Value Target for x ∈ D:
x𝜏 = x𝑖 + ∫ 𝜏
𝑡 , ξ 𝜃
𝑅𝑡 = ∫ 𝑡
𝑉tar (x𝑖) = ∫ 𝑇

exp(−𝜌𝜏) 𝑟𝑐 (x𝜏, u𝜏)𝑑𝜏 + exp(−𝜌𝑡)𝑉 𝑘 (x𝑡 )
𝛽 exp(−𝛽𝑡) 𝑅𝑡 𝑑𝑡 + exp(−𝛽𝑇)𝑅𝑇

𝑓𝑐 (x𝑡 , u𝑡 , ξ 𝑥

𝑡 , ξ𝑜

𝑡 , ξ𝑢

𝑡 )𝑑𝑡

0

0

0

// Fit Value Function:
𝜓𝑘+1 = arg min𝜓 (cid:205)

x∈ D (cid:107)𝑉tar (x) − 𝑉 (x; 𝜓)(cid:107) 𝑝

if RTDP rFVI then

// Add samples from 𝜋𝑘+1 to FIFO buﬀer D
D 𝑘+1 = ℎ(D 𝑘 , {x𝑘+1

. . . x𝑘+1

𝑁 })

0

end if
end while

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions. The resulting optimal adver-
sarial state perturbation is described by

ξ𝑥 = −𝛼

∇𝑥𝑉
(cid:107)∇𝑥𝑉 (cid:107)2

.

(cid:3)

Theorem 3. For the adversarial model disturbance (Equa-
tion 8) with element-wise bounded amplitude (Equation 17),
smooth drift and control matrix (i.e., a, B ∈ 𝐶1) and
B(θ + ξ𝜃 ) ≈ B(θ), the optimal continuous-time policy 𝜋 and
model disturbance ξ𝜃 is described by

𝜋(x) = ∇ ˜𝑔

(cid:16)

with z𝜃 =

(cid:17)

B(x)𝑇 ∇𝑥𝑉
(cid:18) 𝜕a(x; θ)
𝜕θ

,

ξ𝜃 = −𝚫𝜈 sign (zθ) + µ𝜈

+

𝜕B(x; θ)
𝜕θ

(cid:19)𝑇

𝜋(x)

∇𝑥𝑉,

parameter mean µν = (νmax + νmin) /2 and parameter range
𝚫ν = (νmax − νmin) /2.

Theorem 4. For the adversarial observation disturbance
(Equation 7) with bounded signal energy (Equation 16),
smooth drift and control matrix (i.e., a, B ∈ 𝐶1) and
B(x + ξ𝑜) ≈ B(x), the optimal continuous-time policy 𝜋 and
observation disturbance ξ𝑜 is described by

𝜋(x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉

(cid:17)

,

with z𝑜 =

(cid:18) 𝜕a(x; θ)
𝜕x

+

𝜕B(x; θ)
𝜕x

ξ𝑜 = −𝛼 z𝑜
(cid:107)z𝑜 (cid:107)2
(cid:19)𝑇

𝜋(x)

∇𝑥𝑉 .

Proof Sketch Theorem 3 Equation 14 can be written as
𝑟 (x, u) + 𝛾𝑉 (cid:0) 𝑓 (.)(cid:1) s.t.

(ξ𝜃 − µ𝜈)2 ≤ 𝚫2
𝜈

𝑉tar = max
u

min
ξ𝜃

by replacing the admissible set Ω𝐴 with an explicit constraint.
In the following we abbreviate B(x; θ + ξ𝜃 ) as B 𝜉 and

Figure 3. The learning curves for DP rFVI, DP cFVI, RTDP cFVI and RTDP rFVI averaged over 5 seeds. The shaded area displays the min/max range
between seeds. DP rFVI learns slower compared to DP cFVI on the carpole and Furuta pendulum as the adversary prevents learning. RTDP rFVI does not
learn the task as the adversary is too strong for the online variant of rFVI despite using the identical admissible set as the oﬄine variant DP rFVI.

a(x; θ + ξ𝜃 ) as a 𝜉 . Substituting the Taylor expansion for
𝑉 (x𝑡+1), the dynamics models and reward yields
𝑉tar − 𝛾𝑉
Δ𝑡

(cid:2)𝛾∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) + 𝛾O (.) − 𝑔𝑐(cid:3)

= 𝑞𝑐 + max
u

min
ξ

that is robust to changes in the dynamics. Therefore, the value
function target is computed using
x, u𝑘 (cid:17)

x, u𝑘 , ξ𝑘

+ 𝛾𝑉 𝑘 (cid:16)

𝑉 𝑘+1
tar = 𝑟

𝑢 , ξ𝑘

𝑥 , ξ𝑘

𝑜 , ξ𝑘
𝜃

(18)

(cid:17)(cid:17)

(cid:16)

(cid:16)

𝑓

In the continuous-time limit the optimal action and disturbance
is determined by

u∗, ξ∗

𝜃 = max
u

min
ξ

(cid:2)∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) − 𝑔𝑐 (u)(cid:3) .

This nested max-min optimization can be solved by ﬁrst
solving the inner optimization w.r.t. to u and substituting this
solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

ξ∗ = arg min

∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) +

ξ

1

2 λ𝑇 (cid:16)

(ξ𝜃 − µ𝜈)2 − 𝚫2
𝜈

(cid:17)

.

where actions u𝑘 and disturbances ξ𝑖 are determined ac-
cording to Table I. Iterating between computing the target
and ﬁtting the value function network (Equation 13) enables
the learning of the robust optimal value function and robust
optimal policy.

N-Step Value Function Target The learning can be acceler-
ated by using the exponentially weighted 𝑛-step value target
instead of the 1-step target, as shown by the classical eligibility
traces [17], generalized advantage estimation [33, 34] or model
based value-expansion [35]. In the continuous limit this target
is described by

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

𝑉tar (x0) =

z𝜃 + λ𝑇 (ξ𝜃 − µ𝜈) (cid:66) 0 ⇒ ξ∗

𝜃 = −z𝜃 (cid:11) λ + µ𝜈

with the elementwise division (cid:11). Using the primal feasibility
and the complementary slackness, the optimal λ∗ can be com-
puted. The resulting optimal model disturbance is described by

𝜃 (u) = −𝚫𝜈 sign (cid:0)z𝜃 (u)(cid:1) + µ𝜈
ξ∗
as z𝜃 (cid:11) (cid:107)z𝜃 (cid:107)1 = sign(z𝜃 ). The action can be computed by
𝜃 (u)(cid:1) u(cid:3) − 𝑔𝑐 (u).

𝜃 (u)) + B (cid:0)ξ∗

∇𝑥𝑉𝑇 (cid:2)a(ξ∗

u∗ = arg max

u

Due to the envelope theorem [32], the extrema is described by

B(x; θ + ξ∗

𝜃 (u))𝑇 ∇𝑥𝑉 − 𝑔𝑐 (u) (cid:66) 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. θ. Approximating
B(x; θ + ξ∗(u)) ≈ B(x; θ), lets one solve for u. In this case
the optimal action u∗ is described by u∗=∇ ˜𝑔(B(x; θ)𝑇 ∇𝑥𝑉).
This approximation implies that neither agent or the adversary
can react to the action of the other and must choose simulta-
(cid:3)
neously. This assumption is common in prior works [5].

C. Algorithm
Using the theoretical insights from the previous section, robust
ﬁtted value iteration can be derived. Instead of computing the
value function target using only the optimal action as in cFVI,
rFVI includes the four adversaries to learn a optimal policy

∫ 𝑇

0
∫ 𝑡

0
∫ 𝑡

0

𝛽 exp(−𝛽𝑡) 𝑅𝑡 𝑑𝑡 + exp(−𝛽𝑇)𝑅𝑇 ,

exp(−𝜌𝜏) 𝑟𝑐 (x𝜏, u𝜏)𝑑𝜏 + exp(−𝜌𝑡)𝑉 𝑘 (x𝑡 ),

(cid:16)

𝑓𝑐

x, u𝑘 , ξ𝑘

𝑥 , ξ𝑘

𝑢 , ξ𝑘

𝑜 , ξ𝑘
𝜃

(cid:17)

𝑑𝜏 + x0,

𝑅𝑡 =

x𝑡 =

where 𝛽 is the exponential decay factor. In practice we treat
𝛽 as the hyperparameter and select 𝑇 such that the weight of
the 𝑅𝑇 is exp (−𝛽𝑇) (cid:66) 10−4.

Admissible Set For the state, action and observation adversary
the signal energy is bounded. We limit the energy of ξ𝑥, ξ𝑢 and
ξ𝑜 as the non-adversarial disturbances are commonly modeled
as multivariate Gaussian distribution. Therefore, the average
energy is determined by the noise covariance matrix. For the
model parameters θ a common practice is to assume that
the approximate model parameters have an model error of up
to ±15% [1, 2]. Hence, we bound the amplitude of each com-
ponent. To not overﬁt to the deterministic worst case system
of 𝑉 𝑘 and enable the discovery of good actions, the amplitude
of the adversarial actions of 𝜉𝑥, 𝜉𝑢, 𝜉𝑜 is modulated using a
Wiener process. This random process allows a continuous-time
formulation that is agnostic to the sampling frequency.

Oﬄine and Online rFVI The proposed approach is oﬀ-policy
as the samples in the replay memory do not need to originate
from the current policy 𝜋𝑘 . Therefore, the dataset can either
consist of a ﬁxed dataset or be updated within each iteration.
In the oﬄine dynamic programming case, the dataset contains
samples from the compact state domain X. We refer to the

0255075100Episode−40−200RewardPendulumDPcFVI-λ=0.85DPrFVI-λ=0.85RTDPcFVI-λ=0.85RTDPrFVI-λ=0.85050100150200250Episode−150−100−500RewardCartpoleDPcFVI-λ=0.90DPrFVI-λ=0.85RTDPcFVI-λ=0.45RTDPrFVI-λ=0.45050100150200250Episode−400−300−200−1000RewardFurutaPendulumDPcFVI-λ=0.95DPrFVI-λ=0.70RTDPcFVI-λ=0.40RTDPrFVI-λ=0.25Figure 4.
The tracked trajectories of DP rFVI and DP cFVI on the physical cartpole with varied pendulum masses. DP rFVI is capable to perform the
swing-up for the varying pendulum mass. The qualitative performance does not change if the weight is added or reduced. DP cFVI can swing up and balance
all varied pendulums but it requires more pre-swings for all conﬁgurations. During balancing the cart is not centered and the cart oscillates around the center
for DP cFVI. The pendulum must signiﬁcantly deviate from the target position before the DP cFVI policy breaks the stiction of the linear actuator. In contrast,
the higher actions of DP rFVI break the stiction and balance the pendulum with the cart centered.

Figure 5. The tracked trajectories for DP rFVI and DP cFVI on the Furuta
pendulum for diﬀerent pendulum weights. The trajectories of rFVI do not
signiﬁcantly change when the pendulum mass altered. For DP cFVI the
trajectories start to change when an additional weight is added. For these
system dynamics, DP cFVI requires some failed swing-ups until the policy
can balance the pendulum.

oﬄine variant as DP rFVI. In the online case, the replay
memory is updated with samples generated by the current
policy 𝜋𝑘 . Every iteration the states of the previous 𝑛-rollouts
are added to the data and replace the oldest samples. This
online update of state distribution performs real-time dynamic
programming (RTDP) [36]. We refer to the online variant as
RTDP rFVI. The pseudo code of DP cFVI and RTDP rFVI is
summarized in Algorithm 1.

IV. Experiments
In the following non-linear control experiments we want to
answer the following questions:
Q1: Does rFVI learn a robust policy that can be successfully
transferred to the physical systems with diﬀerent model pa-
rameters?
Q2: How does the policy obtained by rFVI diﬀer qualitatively
compared to cFVI and the deep RL baselines?
To answer these questions, we apply the algorithms to perform
the swing-up task of the under-actuated cartpole (Fig. 4)
and Furuta pendulum (Fig. 5). Both systems are standard
environments for benchmarking non-linear control policies.
We focus only on these two systems to perform extensive
robustness experiments on the actual physical systems. To test
the robustness with respect to model parameters, we attach
small weights to the passive pendulum.

A. Experimental Setup
Systems The physical cartpole and Furuta pendulum are
manufactured by Quanser [37] and voltage controlled. For the

approximate simulation model we use the rigid-body dynamics
model with the parameters supplied by the manufacturer. If we
add negative weights to the pendulum, we attach the weights
to the opposite lever of the pendulum. This moves the center
of mass of the pendulum closer to the rotary axis. Therefore,
this shift reduces the downward force and is equivalent to a
lower pendulum mass.

Baselines The performance is compared to the actor-critic
deep RL methods: DDPG [38], SAC [39] and PPO [34]. The
robustness evaluation is only performed for the best performing
baselines on the nominal physical system. The performance of
all baselines on the nominal system is summarized in Table
IV (Appendix). The initial state distribution is abbreviated
by {SAC, PPO, DDPG}-U for a uniform distribution of the
pendulum angle and {SAC, PPO, DDPG}-N for a Gaussian
distribution. The baselines with Gaussian initial state distri-
bution did not achieve robust performance on the nominal
system. If the baseline uses uniform domain randomization
the acronym is appended with UDR.

Evaluation To evaluate rFVI and the baselines we separately
compare the state and action reward as these algorithms
optimize a diﬀerent objectives. Hence, these algorithms trade-
oﬀ state and action associated rewards diﬀerently. It is expected
that the worst-case optimization uses higher actions to prevent
deviation from the optimal trajectory. On the physical system,
the performance is evaluated using the 25th, 50th and 75th re-
ward percentile as the reward distribution is multi-modal.

B. Experimental Results
The learning curves averaged over 5 seeds of DP rFVI and
RTDP rFVI are shown in Figure 3. The results in simulation
are summarized in Table II. DP rFVI learns a policy that
obtains slightly lower reward compared to cFVI and the deep
RL baselines. This lower reward is expected as the worst-
case optimization yields conservative policies [26]. DP rFVI
exhibits low variance between seeds but learns slower than
DP cFVI. This slower learning is caused by the adversary
which counteracts the learning progress. RTDP rFVI does
not learn to successfully swing-up the Furuta pendulum and
cartpole. Despite using the same admissible set for both
variants, the adversary is too powerful for RTDP rFVI and

DPrFVI(ours)∆mp=−20g/−16%∆mp=−10g/−08%∆mp=+00g/−00%∆mp=+10g/+08%∆mp=+20g/+16%∆mp=+50g/+39%DPcFVIDPrFVI(ours)∆mp=−5g∆mp=−2g∆mp=+0g∆mp=+1g∆mp=+2g∆mp=+5gDPcFVIFigure 6. The 25th, 50th and 75th reward percentile for the physical Furuta pendulum and cartpole with varied pendulum weights. DP rFVI achieves higher
state reward for real-world systems compared to the baselines. For the diﬀerent weights the reward remains nearly constant. For the Furuta pendulum the
action cost is signiﬁcantly higher compared to the baselines as the DP rFVI causes a chattering during balancing due to the high actions and minor time
delays in the control loop. If only the swing-up phase is considered the rewards are comparable.

Table II
The average rewards in simulation.

Algorithm

State Rwd

Action Rwd

State Rwd

Action Rwd

State Rwd

Action Rwd

Sim Pendulum

Sim Cartpole

Sim Furuta Pendulum

SAC-U

DP cFVI

DP rFVI (ours)

SAC-U & UDR

−24.5 ± 00.1 −08.3 ± 00.4 −15.5 ± 05.4 −11.6 ± 02.0 −37.0 ± 13.2 −04.8 ± 02.7
−22.3 ± 03.0 −08.3 ± 03.2 −14.4 ± 02.8 −09.9 ± 02.2 −22.3 ± 02.5 −05.9 ± 01.3
−21.1 ± 05.2 −09.4 ± 04.7 −13.9 ± 02.4 −10.4 ± 02.3 −22.6 ± 02.9 −05.9 ± 01.5
−22.6 ± 02.6 −08.9 ± 01.9 −13.9 ± 02.6 −10.3 ± 02.5 −22.1 ± 02.6 −06.1 ± 01.4
−20.9 ± 01.2 −10.6 ± 01.3 −16.4 ± 04.7 −11.8 ± 04.9 −24.6 ± 03.6 −05.5 ± 01.9
DDPG-U
DDPG-U & UDR −21.0 ± 04.8 −11.5 ± 01.6 −14.5 ± 03.5 −12.8 ± 03.5 −26.1 ± 02.4 −06.2 ± 01.9
−24.5 ± 04.4 −09.0 ± 02.4 −82.2 ± 83.1 −04.9 ± 01.1 −34.9 ± 12.6 −03.4 ± 01.1
PPO-U
−24.5 ± 00.2 −11.1 ± 03.0 −55.9 ± 23.1 −09.8 ± 05.2 −42.9 ± 04.6 −06.1 ± 02.4

PPO-U & UDR

prevents learning. In this case policy does not discover the
positive reward at the top as the adversary prevents balancing.
Therefore, the policy is too pessimistic and converges to a bad
local optima. The ablation study in the appendix shows, that
RTDP rFVI learns a successful policy if the admissible sets
are reduced. To overcome this problem one would need to bias
the exploration to be optimistic.

The performance on the physical systems is summarized in
Figure 4, 5 and 7.1 Across the diﬀerent parameters of both
systems, rFVI achieves the highest state reward compared
to cFVI and the deep RL baselines. The best performing
trajectories between diﬀerent conﬁgurations are nearly iden-
tical. Only for the cartpole the failure rates slightly increases
when positive weights are added. In this case the pendulum
is swung-up but cannot be stabilized, due to the backslash of
the linear actuator. For cFVI and the deep RL baselines even
the best trajectories deteriorate when weights are added to the
pendulum. This is especially notable in Figure 7, where the
deep RL baselines start to explore the complete state space
when additional weights are added. The high state rewards
of rFVI are obtained at the expense of higher action cost,
which can be higher compared to some baselines on the
physical system. To summarize rFVI obtains a robust policy
that performs the swing-up with low state reward and more
consistency than the baselines but uses higher actions.

Compared to the policies obtained by DP cFVI and the deep
RL baselines, DP rFVI policy exerts higher actions. Therefore,
DP rFVI achieves the robustness compared to the baselines
by utilizing a stiﬀer feedback policy approaching bang-bang
control. The higher actions are only observed on the physical

1Videos of the experiments at https://sites.google.com/view/rfvi.

system where the deviation from the optimal trajectory is
inevitable. In simulation the action cost are comparable to
the baselines. Therefore, the high actions originate from the
feedback-term of the non-linear policy that tries to compensate
the tracking error. The stiﬀer feedback policy is expected as
traditional robust control approaches yield high feedback gains
[40]. The stiﬀness of the rFVI policy is clearly visible in
Figure 2. On the ridge leading up to the balancing point, the
policy directly applies maximum action, if one deviates from
the center of the ridge. In contrast, DP cFVI has a gradient
that slightly increases the action when one deviates from the
optimal trajectory.

For the cartpole the higher actions achieve much better per-
formance. DP rFVI achieves stabilization of the cart in the
center of the track (Figure 4). The higher actions break the
stiction and overcome the backslash of the geared cogwheel
actuator during the balancing. For DP cFVI and the baselines,
the cart oscillates around the center. The pendulum angle has
to deviate signiﬁcantly until the actions become large and
break the stiction. For the Furuta pendulum the high actions
are more robust during the swing-up phase. However, during
the balancing the lower link starts to chatter due to the high-
frequency switching between high actions. This switching is
caused by minor delays in the control
loop and the very
low friction of the pendulum. About 90% of the action cost
of DP rFVI for the Furuta pendulum is incurred during the
stabilization. DP cFVI incurs only 10% of the action cost
during stabilization. If one would only consider the swing-
up phase, the reward of DP rFVI is higher compared to the
baselines. To summarize, high-stiﬀness feedback policies are
robust to changes in dynamics. However, this robustness can
also make the resulting policies more sensitive to other sources
error not included in the speciﬁcation, e.g., control delays.

Besides the performance evaluation of DP rFVI, the experi-
ments show that DP cFVI achieves comparable performance
than the deep RL baselines with domain randomization.
Despite overﬁtting to a deterministic approximate model,
DP cFVI is able to be robust against some variations in
model parameters. This result suggests that for these two
physical systems, preventing the distribution shift by solving

-5g-21%-2g-8%0g0%1g4%3g12%5g21%0.0-50.0-100.0-150.0-200.0-250.0RewardFuruta PendulumState Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%State Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-5g-21%-2g-8%0g0%1g4%3g12%5g21%Action Rwd-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp0.0-50.0-100.0-150.0RewardCartpoleDP rFVI (ours)DP cFVIPPO-U & UDRSAC-USAC-U & UDRDDPG-U & UDRMedian-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mp-20g-16%-10g-8%0g0%10g8%20g16%50g39%Change in Mass - mpFigure 7. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Furuta pendulum. The diﬀerent columns
correspond to diﬀerent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up
for the diﬀerent pendulum masses. In contrast to DP rFVI, the baselines start to deviate strongly from trajectories on the nominal system. When weights are
added the baselines start to cover the complete state-space. A ﬁgure displaying the roll-outs per algorithm is provided in the appendix.

for the policy on the compact state domain obtains a policy
with comparable robustness as uniform domain randomization.
Furthermore, the deep RL performance increases with larger
state distribution and using the maximum-entropy formulation
on the physical system. Therefore, the experiments suggest that
the state-distribution of the policy aﬀects the policy robustness.
This correlation should be investigated in-depth in future work.

V. Related Work

Robust Policies Learning robust policies has been approached
by (1) changing the optimization objective that balances risk
and reward [41–43], (2) introducing a adversary to optimize
the worst-case performance [5–7, 44, 45] and (3) randomizing
the dynamics model to be robust to the model parameters [2,
2–4, 9, 46]. In robotics, domain randomization is most widely
used to achieve successful sim2real
transfer. For example
domain randomization was used for in-hand manipulation [46],
ball-in-a-cup [2], locomotion [9], manipulation [3, 4].

In this paper we focus on the adversarial formulation. This
approach has been extensively used for continuous control
tasks [7, 8, 13, 18, 23]. For example, Pinto et. al. [7, 18]
used a separate agent as adversary controlling an additive
control input. This adversary maximized the negative reward
using a standard actor-critic learning algorithm. The agent
and adversary do not share any information. Therefore, an
additional optimization is required to optimize the adversary. A
diﬀerent approach by Mandlekar et. al. [8] used auxiliary loss
to maximize the policy actions. Both approaches are model-
free. In contrast to these approaches, our approach derives
the adversarial perturbations using analytic expressions de-
rived directly from the Hamilton-Jacobi-Isaacs (HJI) equation.
Therefore, our approach shares knowledge between the actor
and adversary due to a shared value function and requires
no additional optimization. However, our approach requires
knowledge of the model to compute the actions and pertur-
bations analytically. The approach is similar to Morimoto and
Doya [13]. In contrast to this work, we extend the analytic
solutions to state, action, observation and model disturbances,

do not require a control-aﬃne disturbance model, and use the
constrained formulation rather than the penalized formulation.

Continuous-Time Reinforcement Learning Various ap-
proaches have been proposed to solve the Hamilton-Jacobi-
Bellman (HJB) diﬀerential equation with the machine learning
toolset. These methods can be divided into trajectory and
state-space based methods. Trajectory based methods solve the
stochastic HJB along a trajectory using path integral control
[47–49] or forward-backward stochastic diﬀerential equations
[50, 51]. State-space based methods solve the HJB globally
to obtain a optimal non-linear controller applicable on the
complete state domain. Classical approaches discretize the
problem and solve the HJB or HJI using a PDE solver [5].
To overcome the curse of dimensionality of the grid based
methods, machine learning methods have proposed to use vari-
ous function approximators ranging from polynomial functions
[52, 53], kernels [54] to deep networks [12, 14, 55, 56]. In this
paper, we utilize the state-space based approach solve the HJI
via dynamic programming. The value function is approximated
using a deep network and optimal value function is solved via
value iteration.

VI. Conclusion and Future Work

We proposed robust ﬁtted value iteration (rFVI). This al-
gorithm solves the adversarial continuous-time reinforcement
learning problem for continuous states, action and adversary
via value iteration. To enable the eﬃcient usage of value
iteration, we presented analytic expressions for the adversar-
ial disturbances for the state, action, observation and model
adversary. Therefore, our derivations extend existing analytic
expressions for continuous time RL from literature [11–14].
The non-linear control experiments using the physical cartpole
and Furuta pendulum showed that rFVI is robust to varia-
tions in model parameters and obtains higher state-rewards
compared to the deep RL baselines with uniform domain
randomization. The robustness of rFVI is achieved by utilizing
a stiﬀer feedback policy that exerts higher actions compared
to the baselines.

In future work we plan to learn the admissible sets from
data from the physical system. In domain randomization, the
automatic tuning of the parameter distributions has been very
successful [2, 4]. However, these approaches are not directly
transferable as one would also need to estimate the admissible
set of the action, state and observation and not only the system
parameter distribution as in domain randomization.

Acknowledgements

M. Lutter was an intern at Nvidia during this project. A. Garg
to
was partially supported by CIFAR AI Chair. We also want
thank Fabio Muratore, Joe Watson and the RSS reviewers for
their feedback. Furthermore, we want
to thank the open-source
projects SimuRLacra [57], MushroomRL [58], NumPy [59] and
PyTorch [60].

References

[1] F. Muratore, F. Treede, M. Gienger, and J. Peters, “Do-
main randomization for simulation-based policy opti-
mization with transferability assessment,” in Conference
on Robot Learning (CoRL), 2018.

[2] F. Muratore, C. Eilers, M. Gienger, and J. Peters, “Data-
eﬃcient domain randomization with bayesian optimiza-
tion,” IEEE Robotics and Automation Letters (RAL),
2021.

[3] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin,
J. Issac, N. Ratliﬀ, and D. Fox, “Closing the sim-to-real
loop: Adapting simulation randomization with real world
experience,” 2019.

[4] F. Ramos, R. C. Possas, and D. Fox, “Bayessim: adap-
tive domain randomization via probabilistic inference for
robotics simulators,” 2019.

[5] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin,
“Hamilton-Jacobi reachability: A brief overview and re-
cent advances,” 2017.

[6] R. Isaacs, Diﬀerential games: a mathematical

theory
with applications to warfare and pursuit, control and
optimization. Courier Corporation, 1999.

[7] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, “Ro-
bust adversarial reinforcement learning,” in International
Conference on Machine Learning (ICML), 2017.

[8] A. Mandlekar, Y. Zhu, A. Garg, L. Fei-Fei, and
S. Savarese, “Adversarially robust policy learning: Active
construction of physically-plausible perturbations,” in In-
ternational Conference on Intelligent Robots and Systems
(IROS), 2017.

[9] Z. Xie, X. Da, M. van de Panne, B. Babich, and A. Garg,
“Dynamics Randomization Revisited: A Case Study for
Quadrupedal Locomotion,” in IEEE International Con-
ference on Robotics and Automation (ICRA), 2021.
[10] J. Garcıa and F. Fernández, “A comprehensive survey
learning,” Journal of Machine

on safe reinforcement
Learning Research, 2015.

[11] S. E. Lyshevski,

control of nonlinear
continuous-time systems: design of bounded controllers
via generalized nonquadratic functionals,” in American

“Optimal

Control Conference (ACC), vol. 1, pp. 205–209, IEEE,
1998.

[12] K. Doya, “Reinforcement learning in continuous time and
space,” Neural computation, vol. 12, no. 1, pp. 219–245,
2000.

[13] J. Morimoto and K. Doya, “Robust reinforcement learn-

ing,” Neural computation, 2005.

[14] M. Lutter, B. Belousov, K. Listmann, D. Clever, and
J. Peters, “HJB optimal feedback control with deep
diﬀerential value functions and action constraints,” in
Conference on Robot Learning (CoRL), 2019.

[15] R. Bellman, Dynamic Programming. USA: Princeton

University Press, 1957.

[16] M. L. Puterman, Markov decision processes: discrete
stochastic dynamic programming. John Wiley & Sons,
1994.

[17] R. S. Sutton, A. G. Barto, et al., Introduction to rein-
forcement learning. MIT press Cambridge, 1998.
[18] L. Pinto, J. Davidson, and A. Gupta, “Supervision via
competition: Robot adversaries for learning tasks,” in
International Conference on Robotics and Automation
(ICRA), 2017.

[19] H. Zhang, H. Chen, C. Xiao, B. Li, D. Boning, and C.-
J. Hsieh, “Robust deep reinforcement learning against
adversarial perturbations on observations,” arXiv preprint
arXiv:2003.08938, 2020.

[20] M. Heger, “Consideration of risk in reinforcement learn-
ing,” in Machine Learning Proceedings, Elsevier, 1994.
[21] M. L. Littman, “Markov games as a framework for
multi-agent reinforcement learning,” in Machine learning
proceedings, Elsevier, 1994.

[22] A. Nilim and L. El Ghaoui, “Robust control of markov
decision processes with uncertain transition matrices,”
Operations Research, 2005.

[23] C. Tessler, Y. Efroni, and S. Mannor, “Action robust
reinforcement learning and applications in continuous
control,” in International Conference on Machine Learn-
ing (ICML), 2019.

[24] A. Pattanaik, Z. Tang, S. Liu, G. Bommannan, and
G. Chowdhary, “Robust deep reinforcement learning with
adversarial attacks,” arXiv preprint arXiv:1712.03632,
2017.

[25] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine,
and S. Russell, “Adversarial policies: Attacking deep re-
inforcement learning,” arXiv preprint arXiv:1905.10615,
2019.

[26] H. Xu and S. Mannor, “Robustness and generalization,”

Machine learning, 2012.

[27] M. Lutter, S. Mannor, J. Peters, D. Fox, and A. Garg,
“Value Iteration in Continuous Actions, States and
Time,” in International Conference on Machine Learning
(ICML), 2021.

[28] D. Ernst, P. Geurts, and L. Wehenkel, “Tree-based
batch mode reinforcement learning,” Journal of Machine
Learning Research, 2005.

[29] A.

massoud

Farahmand, M.

Ghavamzadeh,

C. Szepesvári, and S. Mannor, “Regularized ﬁtted
q-iteration for planning in continuous-space markovian
decision problems,” in American Control Conference
(ACC), IEEE, 2009.

[30] M. Riedmiller, “Neural ﬁtted q iteration–ﬁrst experi-
ences with a data eﬃcient neural reinforcement learning
method,” in European Conference on Machine Learning,
Springer, 2005.

[31] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Ve-
ness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K.
Fidjeland, G. Ostrovski, et al., “Human-level control
through deep reinforcement learning,” Nature, vol. 518,
no. 7540, p. 529, 2015.

[32] M. Carter, Foundations of mathematical economics. MIT

press, 2001.

[33] J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and
P. Moritz, “Trust region policy optimization.,” in Icml,
vol. 37, pp. 1889–1897, 2015.

[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and
O. Klimov, “Proximal policy optimization algorithms,”
arXiv preprint arXiv:1707.06347, 2017.

[35] V. Feinberg, A. Wan, I. Stoica, M. I. Jordan, J. E.
Gonzalez, and S. Levine, “Model-based value estimation
for eﬃcient model-free reinforcement learning,” arXiv
preprint arXiv:1803.00101, 2018.

[36] A. G. Barto, S. J. Bradtke, and S. P. Singh, “Learning
to act using real-time dynamic programming,” Artiﬁcial
intelligence, 1995.

[37] Quanser, “Quanser courseware and resources.” https:
//www.quanser.com/solution/control-systems/, 2018.
[38] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
Y. Tassa, D. Silver, and D. Wierstra, “Continuous con-
trol with deep reinforcement learning,” arXiv preprint
arXiv:1509.02971, 2015.

[39] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft
actor-critic: Oﬀ-policy maximum entropy deep reinforce-
ment learning with a stochastic actor,” in International
Conference on Machine Learning, 2018.

[40] K. J. Åström, L. Neumann, and P.-O. Gutman, “A com-
parison between robust and adaptive control of uncertain
systems,” IFAC Proceedings Volumes, 1987.

[41] V. S. Borkar, “A sensitivity formula for risk-sensitive
cost and the actor–critic algorithm,” Systems & Control
Letters, 2001.

[42] Y. Chow, A. Tamar, S. Mannor, and M. Pavone, “Risk-
sensitive and robust decision-making: a cvar optimization
approach,” arXiv preprint arXiv:1506.02188, 2015.
[43] H. Bharadhwaj, A. Kumar, N. Rhinehart, S. Levine,
F. Shkurti, and A. Garg, “Conservative safety critics for
exploration,” in International Conference on Learning
Representations (ICLR), 2021.

[44] A. Tamar, H. Xu, and S. Mannor, “Scaling up ro-
bust mdps by reinforcement learning,” arXiv preprint
arXiv:1306.6189, 2013.

[45] J. Harrison*, A. Garg*, B. Ivanovic, Y. Zhu, S. Savarese,
L. Fei-Fei, and M. Pavone (* equal contribution),

“AdaPT: Zero-Shot Adaptive Policy Transfer for Stochas-
tic Dynamical Systems,” in International Symposium on
Robotics Research (ISRR), 2017.

[46] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefow-
icz, B. McGrew, J. Pachocki, A. Petron, M. Plappert,
G. Powell, A. Ray, et al., “Learning dexterous in-hand
manipulation,” The International Journal of Robotics
Research (ĲRR), 2020.

[47] H. J. Kappen, “Linear theory for control of nonlinear
stochastic systems,” Physical review letters, 2005.
[48] E. Todorov, “Linearly-solvable markov decision prob-
information processing

lems,” in Advances in neural
systems, 2007.

[49] E. Theodorou, J. Buchli, and S. Schaal, “Reinforcement
learning of motor skills in high dimensions: A path inte-
gral approach,” in International Conference on Robotics
and Automation (ICRA), IEEE, 2010.

[50] M. Pereira, Z. Wang, I. Exarchos, and E. Theodorou,
“Learning deep stochastic optimal control policies using
forward-backward sdes,” in Robotics: science and sys-
tems, 2019.

[51] M. A. Pereira, Z. Wang,

I. Exarchos, and E. A.
Theodorou, “Safe optimal control using stochastic bar-
rier functions and deep forward-backward sdes,” arXiv
preprint arXiv:2009.01196, 2020.

[52] X. Yang, D. Liu, and D. Wang, “Reinforcement learning
for adaptive optimal control of unknown continuous-time
nonlinear systems with input constraints,” International
Journal of Control, 2014.

[53] D. Liu, D. Wang, F.-Y. Wang, H. Li, and X. Yang,
“Neural-network-based online hjb solution for optimal ro-
bust guaranteed cost control of continuous-time uncertain
nonlinear systems,” IEEE transactions on cybernetics,
vol. 44, no. 12, pp. 2834–2847, 2014.

[54] P. Hennig, “Optimal reinforcement learning for gaussian
systems,” in Advances in Neural Information Processing
Systems, 2011.

[55] Y. Tassa and T. Erez, “Least squares solutions of the
HJB equation with neural network value-function approx-
imators,” IEEE transactions on neural networks, vol. 18,
no. 4, pp. 1031–1041, 2007.

[56] J. Kim, J. Shin, and I. Yang, “Hamilton-jacobi deep
q-learning for deterministic continuous-time systems
with lipschitz continuous controls,” arXiv preprint
arXiv:2010.14087, 2020.

[57] F. Muratore, “Simurlacra - a framework for reinforcement
learning from randomized simulations.” https://github.
com/famura/SimuRLacra, 2020.

[58] C. D’Eramo, D. Tateo, A. Bonarini, M. Restelli, and
“Mushroomrl: Simplifying reinforcement
https://github.com/MushroomRL/

J. Peters,
learning
research.”
mushroom-rl, 2020.

[59] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gom-
mers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,
S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer,
M. H. van Kerkwĳk, M. Brett, A. Haldane, J. F. del Río,

M. Wiebe, P. Peterson, P. Gérard-Marchant, K. Sheppard,
T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E.
Oliphant, “Array programming with NumPy,” Nature,
2020.

[60] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury,
G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Rai-
son, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,
J. Bai, and S. Chintala, “Pytorch: An imperative style,
high-performance deep learning library,” in Advances in
Neural Information Processing Systems 32, 2019.
[61] H. K. Khalil and J. W. Grizzle, Nonlinear systems, vol. 3.

Prentice hall Upper Saddle River, NJ, 2002.
and
[62] F. Berkenkamp, M. Turchetta, A. Schoellig,
learning
A. Krause, “Safe model-based reinforcement
with stability guarantees,” in Advances in neural infor-
mation processing systems, 2017.

[63] S. M. Richards, F. Berkenkamp, and A. Krause, “The
lyapunov neural network: Adaptive stability certiﬁcation
for safe learning of dynamical systems,” arXiv preprint
arXiv:1808.00924, 2018.

[64] J. Z. Kolter and G. Manek, “Learning stable deep
dynamics models,” in Advances in Neural Information
Processing Systems (NeurIPS), 2019.

[65] Y.-C. Chang, N. Roohi, and S. Gao, “Neural lyapunov
control,” in Advances in Neural Information Processing
Systems (NeurIPS), 2019.

Next Page

Appendix

A. State Disturbance Proof

Theorem. For the adversarial state disturbance (Equation 5)
with bounded in signal energy (Equation 16), the optimal con-
tinuous time policy 𝜋𝑘 and state disturbance ξ𝑘
𝑥 is described
by

𝜋𝑘 (x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

ξ𝑘 = −𝛼

∇𝑥𝑉 𝑘
(cid:107)∇𝑥𝑉 𝑘 (cid:107)2

.

This solution is intuitive as the adversary wants to minimize
the reward and this disturbance performs steepest descent using
the largest step-size. Therefore, the perturbation is always on
(cid:3)
the constraint.

B. Action Disturbance Proof
Theorem. For the adversarial action disturbance (Equation 5)
with bounded in signal energy (Equation 16), the optimal con-
tinuous time policy 𝜋𝑘 and action disturbance ξ𝑘
𝑢 is described
by

Proof. Equation 14 can be formulated with the explicit con-
straint, i.e.,

𝜋𝑘 (x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

𝑢 = −𝛼 B(x)𝑇 ∇𝑥𝑉 𝑘
ξ𝑘
(cid:107)B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:107)2

.

𝑟 (x𝑡 , u) + 𝛾𝑉 (cid:0) 𝑓 (x𝑡 , u, ξ𝑥)(cid:1)

𝑉tar = max
u

min
ξ𝑥
with ξ𝑇

𝑥 ξ𝑥 − 𝛼2 ≤ 0.

Substituting the Taylor expansion, the dynamics model and the
reward yields

𝑉tar = max
u

min
ξ𝑥

𝑉tar − 𝛾𝑉
Δ𝑡

= 𝑞𝑐 + max
u

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾OΔ𝑡

(cid:2)∇𝑥𝑉𝑇 (a + Bu) + 𝛾O − 𝑔𝑐(cid:3)
(cid:2)∇𝑥𝑉𝑇 ξ𝑥 (cid:3)

+ min
ξ

with the higher order terms O (x, u, Δ𝑡). In the continuous
time limit, the higher-order terms disappear, i.e., lim𝑡→0 O = 0.
Therefore, the optimal action is described by

u𝑡 = arg max

∇𝑥𝑉𝑇 B(x𝑡 ) u − 𝑔𝑐 (u)

u

⇒ u𝑡 = ∇ ˜𝑔𝑐 (B(x𝑡 )∇𝑥𝑉) .

The optimal state disturbance is described by

ξ∗
𝑥 = arg min

∇𝑥𝑉𝑇 ξ𝑥 with ξ𝑇

𝑥 ξ𝑥 − 𝛼2 ≤ 0.

ξ𝑥

Proof. Equation 14 can be formulated with the explicit con-
straint, i.e.,

𝑉tar = max
u

min
ξ𝑢

𝑟 (x, u) + 𝛾𝑉 (cid:0) 𝑓 (x, u, ξ𝑢)(cid:1) with ξ𝑇

𝑢 ξ𝑢 ≤ 𝛼2.

Substituting the Taylor expansion, the dynamics model and the
reward yields

𝑉tar = max
u

min
ξ𝑢

𝑉tar − 𝛾𝑉
Δ𝑡

= 𝑞𝑐 + max
u

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾OΔ𝑡

(cid:2)∇𝑥𝑉𝑇 (a + Bu) + 𝛾O − 𝑔𝑐(cid:3)
(cid:2)∇𝑥𝑉𝑇 B ξ𝑢(cid:3)

+ min
ξ

with the higher order terms O (x, u, Δ𝑡). In the continuous
time limit, the higher-order terms disappear, i.e., lim𝑡→0 O = 0.
Therefore, the optimal action is described by

u𝑡 = arg max

∇𝑥𝑉𝑇 B(x) u − 𝑔𝑐 (u)

u

⇒ u𝑡 = ∇ ˜𝑔𝑐 (B(x)∇𝑥𝑉) .
The optimal state disturbance is described by
∇𝑥𝑉𝑇 B(x) ξ𝑢 with ξ𝑇

ξ∗
𝑢 = arg min

𝑢 ξ𝑢 ≤ 𝛼2.

ξ𝑢

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions, i.e.,

This constrained optimization can be solved using the Karush-
Kuhn-Tucker (KKT) conditions, i.e.,

∇𝑥𝑉 + 2𝜆 ξ𝑥 = 0 ⇒ ξ∗

𝑥 = −

1
2𝜆

∇𝑥𝑉

with the Lagrangian multiplier 𝜆 ≥ 0. From primal feasibil-
ity and the complementary slackness condition of the KKT
conditions follows that

1
4𝜆2 ∇𝑥𝑉𝑇 ∇𝑥𝑉 − 𝛼2 ≤ 0

𝜆

(cid:18) 1
4𝜆2 ∇𝑥𝑉𝑇 ∇𝑥𝑉 − 𝛼2

(cid:19)

= 0

⇒ 𝜆 ≥

√︁

1
2𝛼

∇𝑥𝑉𝑇 ∇𝑥𝑉

⇒ 𝜆0 = 0, 𝜆1 =

√︁

1
2𝛼

∇𝑥𝑉𝑇 ∇𝑥𝑉 .

Therefore, the optimal adversarial state perturbation is de-
scribed by

ξ𝑘 = −𝛼

𝑉 𝑘
𝑥
(cid:107)𝑉 𝑘
𝑥 (cid:107)2

.

B(x𝑡 )𝑇 ∇𝑥𝑉 + 2𝜆 ξ𝑢 = 0 ⇒ ξ∗

𝑢 = −

1
2𝜆

B(x𝑡 )𝑇 ∇𝑥𝑉

with the Lagrangian multiplier 𝜆 ≥ 0. From primal feasibil-
ity and the complementary slackness condition of the KKT
conditions follows that

1
4𝜆2 ∇𝑥𝑉𝑇 BB𝑇 ∇𝑥𝑉 − 𝛼2 ≤ 0

⇒ 𝜆 ≥

𝜆

(cid:18) 1
4𝜆2 ∇𝑥𝑉𝑇 BB𝑇 ∇𝑥𝑉 − 𝛼2

(cid:19)

= 0

√︁

1
2𝛼

∇𝑥𝑉𝑇 BB𝑇 ∇𝑥𝑉

⇒ 𝜆0 = 0, 𝜆1 =

√︁

1
2𝛼

∇𝑥𝑉𝑇 BB𝑇 ∇𝑥𝑉 .

Therefore, the optimal adversarial state perturbation is de-
scribed by

ξ𝑘
𝑢 = −𝛼

B(x𝑡 )𝑇 𝑉 𝑘
𝑥
(cid:107)B(x𝑡 )𝑇 𝑉 𝑘
𝑥 (cid:107)2

.

(cid:3)

C. Model Disturbance Proof

Theorem. For the adversarial model disturbance (Equation 8)
with element-wise bounded amplitude (Equation 17), smooth
drift and control matrix (i.e., a, B ∈ 𝐶1) and B(𝜃 + ξ𝜃 ) ≈
the optimal continuous time policy 𝜋𝑘 and model
B(𝜃),
disturbance ξ𝑘
𝜃 is described by
B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)
(cid:16)
(cid:18) 𝜕B
𝜕𝜃

ξ𝑘
𝜃 = −𝚫𝜈 sign (z𝜃 ) + µ𝜈
𝜕a
𝜕𝜃

𝜋𝑘 (x) = ∇ ˜𝑔

with z𝜃 =

u∗ +

𝑉 ∗
𝑥 ,

(cid:19)𝑇

mean µν = (νmax + νmin) /2 and range 𝚫ν = (νmax − νmin) /2.

Proof. Equation 14 can be written with the explicit constraint
instead of the inﬁmum with the admissible set Ω𝐴, i.e.,

𝑉tar = max
u

with

min
ξ𝜃
1
(cid:16)
2

𝑟 (x, u) + 𝛾𝑉 (cid:0) 𝑓 (x, u, ξ𝜃 )(cid:1)

(ξ𝜃 − µ𝜈)2 − 𝚫2
𝜈

(cid:17)

≤ 0.

Substituting the Taylor expansion for 𝑉 (x𝑡+1), the dynamics
models and reward as well as abbreviating B(x; 𝜃 + ξ𝜃 ) as
B 𝜉 and a(x; 𝜃 + ξ𝜃 ) as a 𝜉 yields

𝑉tar = max
u

min
ξ𝜃

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾O (.)Δ𝑡

𝑉tar − 𝛾𝑉
Δ𝑡

= 𝑞𝑐 + max
u

min
ξ

(cid:2)𝛾∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) + 𝛾O (.) − 𝑔𝑐(cid:3)

In the continuous time limit the optimal action and disturbance
is determined by

u∗, ξ∗

𝜃 = max
u

min
ξ

(cid:2)∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) − 𝑔𝑐 (u)(cid:3) .

This nested max-min optimization can be solved by ﬁrst
solving the inner optimization w.r.t. to u and substituting this
solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

ξ∗ = arg min

∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) +

ξ

1

2 λ𝑇 (cid:16)

(ξ𝜃 − µ𝜈)2 − 𝚫2
𝜈

(cid:17)

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

z𝜃 + λ𝑇 (ξ𝜃 − µ𝜈) = 0 ⇒ ξ∗
𝜃 = −z𝜃 (cid:11) λ + µ𝜈
(cid:21)𝑇

with z𝜃 =

(cid:20) 𝜕a
𝜕𝜃

+

𝜕B
𝜕𝜃

u

∇𝑥𝑉

and the elementwise division (cid:11). The primal feasibility and the
complementary slackness yields

(cid:16)

1
2

−z2

𝜃 (cid:11) λ2 − 𝚫2
𝜈

(cid:17)

≤ 0 ⇒ λ ≥ (cid:107)z𝜃 (cid:107)1 (cid:11) 𝚫𝜈

1

2 λ𝑇 (cid:16)

−z2

𝜃 (cid:11) λ2 − 𝚫2
𝜈

(cid:17)

= 0 ⇒ λ0 = 0, λ1 = (cid:107)z𝜃 (cid:107)1 (cid:11) 𝚫𝜈.

Therefore, the optimal model disturbance is described by

𝜃 (u) = −𝚫𝜈 sign (cid:0)z𝜃 (u)(cid:1) + µ𝜈
ξ∗

as z𝜃 (cid:11) (cid:107)z𝜃 (cid:107)1 = sign(z𝜃 ). Then the optimal action can be
computed by

u∗ = arg max

u

∇𝑥𝑉𝑇 (cid:2)a(ξ∗

𝜃 (u)) + B (cid:0)ξ∗

𝜃 (u)(cid:1) u(cid:3) − 𝑔𝑐 (u).

Due to the envelope theorem, the extrema is described by

B(x; 𝜃 + ξ∗ (u))𝑇 ∇𝑥𝑉 − 𝑔𝑐 (u) = 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. 𝜃. Approximating
B(x; 𝜃 + ξ∗(u)) ≈ B(x; 𝜃), lets one solve for u. In this case
the optimal action u∗ is described by u∗ = ∇ ˜𝑔(B(x; 𝜃)𝑇 ∇𝑥𝑉).
This approximation is feasible for two reasons. First of all, if
the adversary can signiﬁcantly alter the dynamics in each step,
the system would not be controllable and the optimal policy
would not be able to solve the task. Second, this approximation
implies that neither agent or the adversary can react to the
action of the other and must choose simultaneously. This
assumption is common in prior works [5]. The order of the
minimization and maximization is interchangeable. For both
cases the optimal action as well as optimal model disturbance
are identical and require the same approximation during the
(cid:3)
derivation.

D. Observation Disturbance Proof

Theorem. For the adversarial observation disturbance (Equa-
tion 7) with bounded signal energy (Equation 16), smooth drift
and control matrix (i.e., a, B ∈ 𝐶1) and B(x + ξ𝑜) ≈ B(x),
the optimal continuous time policy 𝜋𝑘 and observation distur-
bance ξ𝑘

𝑜 is described by

𝜋𝑘 (x) = ∇ ˜𝑔

(cid:16)

B(x)𝑇 ∇𝑥𝑉 𝑘 (cid:17)

with z𝑜 =

(cid:18) 𝜕a(x; 𝜃)
𝜕x

+

𝑜 = −𝛼 z𝑜
ξ𝑘
(cid:107)z𝑜 (cid:107)2
(cid:19)𝑇

u∗

𝑉 ∗
𝑥 .

𝜕B(x; 𝜃)
𝜕x

Proof. Equation 14 can be written with the explicit constraint
instead of the inﬁmum with the admissible set Ω𝐴, i.e.,

𝑉tar = max
u

min
ξ𝑜
1
2

with

𝑟 (x, u) + 𝛾𝑉 (cid:0) 𝑓 (x, u, ξ𝑜)(cid:1)

(cid:16)

𝑜 ξ𝑜 − 𝛼2(cid:17)

(ξ𝑇

≤ 0.

Substituting the Taylor expansion for 𝑉 (x𝑡+1), the dynamics
models and reward as well as abbreviating B(x + ξ𝑜; 𝜃) as
B 𝜉 yields

𝑉tar = max
u

min
ξ𝑜

𝑟 + 𝛾𝑉 + 𝛾∇𝑥𝑉𝑇 𝑓𝑐Δ𝑡 + 𝛾O (.)Δ𝑡

𝑉tar − 𝛾𝑉
Δ𝑡

= 𝑞𝑐 + max
u

min
ξ

(cid:2)𝛾∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) + 𝛾O (.) − 𝑔𝑐(cid:3)

In the continuous time limit the optimal action and disturbance
is determined by

u∗, ξ∗

𝑜 = max
u

min
ξ

(cid:2)∇𝑥𝑉𝑇 (cid:0)a 𝜉 + B 𝜉 u(cid:1) − 𝑔𝑐 (u)(cid:3) .

This nested max-min optimization can be solved by ﬁrst
solving the inner optimization w.r.t. to 𝜉 and substituting this

Table III
Average rewards on the simulated and physical systems. The ranking describes the decrease in reward compared to the best result averaged on all systems.
The initial state distribution during training is noted by 𝜇. The dynamics are either deterministic model 𝜃 ∼ 𝛿 ( 𝜃) or sampled using uniform domain
randomization 𝜃 ∼ U ( 𝜃). During evaluation the roll outs start with the pendulum pointing downwards.

Algorithm

DP rFVI (ours)
DP cFVI
RTDP cFVI (ours)

SAC
SAC & UDR
SAC
SAC & UDR

DDPG
DDPG & UDR
DDPG
DDPG & UDR

PPO
PPO & UDR
PPO
PPO & UDR

𝜇

−
−
U

N
N
U
U

N
N
U
U

N
N
U
U

𝜃

𝛿 ( 𝜃 )
𝛿 ( 𝜃 )
𝛿 ( 𝜃 )

U ( 𝜃 )
𝛿 ( 𝜃 ) )
U ( 𝜃 )
U ( 𝜃 )

U ( 𝜃 )
𝛿 ( 𝜃 ) )
U ( 𝜃 )
U ( 𝜃 )

U ( 𝜃 )
𝛿 ( 𝜃 ) )
U ( 𝜃 )
U ( 𝜃 )

Simulated Pendulum
Reward
[𝜇 ± 2𝜎]

Success
[%]

Simulated Cartpole
Reward
[𝜇 ± 2𝜎]

Success
[%]

Simulated Furuta Pendulum
Success
[%]

Reward
[𝜇 ± 2𝜎]

Physical Cartpole

Physical Furuta Pendulum

Success
[%]

Reward
[𝜇 ± 2𝜎]

Success
[%]

Reward
[𝜇 ± 2𝜎]

Average
Ranking
[%]

100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

−032.7 ± 000.3
−030.5 ± 000.8
−031.1 ± 001.4

−031.1 ± 000.1
−032.9 ± 000.6
−030.6 ± 001.4
−031.4 ± 002.5

−031.1 ± 000.4
−032.5 ± 000.5
−031.5 ± 000.7
−032.5 ± 003.6

−032.0 ± 000.2
−032.3 ± 000.6
−033.4 ± 004.7
−035.6 ± 003.1

100.0
100.0
100.0

100.0
100.0
100.0
100.0

98.0
100.0
100.0
100.0

100.0
100.0
99.0
100.0

−027.1 ± 004.8
−024.2 ± 002.1
−024.9 ± 001.6

−026.9 ± 003.2
−029.7 ± 004.6
−024.2 ± 001.4
−024.2 ± 001.3

−050.4 ± 285.6
−027.4 ± 002.3
−028.2 ± 005.5
−027.2 ± 001.0

−031.5 ± 007.2
−084.0 ± 007.8
−039.7 ± 045.7
−044.8 ± 021.4

100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

100.0
100.0
100.0
100.0

−041.3 ± 010.8
−027.7 ± 001.6
−040.1 ± 002.7

−029.3 ± 001.5
−032.0 ± 001.1
−028.1 ± 002.0
−028.1 ± 001.3

−030.5 ± 003.5
−034.6 ± 009.8
−030.0 ± 001.7
−032.1 ± 001.5

−081.1 ± 018.3
−040.9 ± 004.6
−038.2 ± 013.1
−048.5 ± 006.2

100.0
73.3
100.0

00.0
100.0
53.3
40.0

06.7
00.0
06.7
00.0

00.0
00.0
00.0
40.0

−074.1 ± 040.3
−143.7 ± 210.4
−101.1 ± 029.0

−518.6 ± 028.1
−394.8 ± 382.8
−144.5 ± 204.0
−296.4 ± 418.9

−536.7 ± 262.7
−517.9 ± 117.6
−459.4 ± 248.3
−318.1 ± 063.4

−287.9 ± 068.8
−435.4 ± 111.9
−183.8 ± 018.0
−143.8 ± 016.1

100.0
100.0
00.0

86.7
100.0
100.0
100.0

46.7
86.7
100.0
100.0

33.3
46.7
60.0
100.0

−278.0 ± 034.3
−082.1 ± 007.6
−1009.9 ± 004.5

−330.7 ± 799.0
−181.4 ± 157.9
−350.8 ± 433.3
−092.3 ± 064.1

−614.1 ± 597.8
−192.7 ± 404.8
−146.6 ± 218.3
−156.7 ± 246.4

−718.7 ± 456.1
−935.7 ± 711.6
−755.3 ± 811.0
−080.6 ± 010.8

−062.7
−019.2
−247.7

−185.8
−120.8
−086.5
−063.8

−281.4
−156.6
−126.0
−091.7

−261.7
−370.0
−219.4
−054.4

solution into the outer maximization. The Lagrangian for the
optimal model disturbance is described by

ξ∗
𝑜 = arg min

∇𝑥𝑉𝑇 (cid:0)a(ξ𝑜) + B(ξ𝑜)u(cid:1) +

ξ𝑜

𝜆
2

(cid:16)

𝑜 ξ𝑜 − 𝛼2(cid:17)
ξ𝑇

Using the KKT conditions this optimization can be solved.
The stationarity condition yields

z𝑜 + 𝜆 ξ𝑜 = 0 ⇒ ξ∗

𝑜 = −

with z𝑜 =

(cid:20) 𝜕a(x; 𝜃)
𝜕x

+

𝜕B(x; 𝜃)
𝜕x

u∗

∇𝑥𝑉 .

z𝑜

1
𝜆
(cid:21)𝑇

The primal feasibility and the complementary slackness yield

1
2
𝜆
2

(cid:18) 1
𝜆2 z𝑇
(cid:18) 1
𝜆2 z𝑇

𝑜 z𝑜 − 𝛼2

(cid:19)

≤ 0 ⇒ 𝜆 ≥

1
𝛼

(cid:107)z𝜃 (cid:107)2

𝑜 z𝑜 − 𝛼2

(cid:19)

= 0 ⇒ 𝜆0 = 0, 𝜆1 =

1
𝛼

(cid:107)z𝑜 (cid:107)2.

Therefore, the optimal observation disturbance is described by
𝑜 (u) = −𝛼 z𝑜
ξ∗
(cid:107)z𝑜 (cid:107)2

.

Then the optimal action can be computed by
∇𝑥𝑉𝑇 (cid:2)a(cid:0)ξ∗

𝑜 (u)(cid:1) + B (cid:0)ξ∗

u∗ = arg max

𝑜 (u)(cid:1) u(cid:3) − 𝑔𝑐 (u).

u

Due to the envelope theorem, the extrema is described by

B(x; 𝜃 + ξ∗

𝑜 (u))𝑇 ∇𝑥𝑉 − 𝑔𝑐 (u) = 0.

This expression cannot be solved without approximation as
B does not necessarily be invertible w.r.t. x. Approximating
𝑜 (u); 𝜃) ≈ B(x; 𝜃), lets one solve for u. In this case
B(x + ξ∗
the optimal action u∗ is described by u∗ = ∇ ˜𝑔(B(x; 𝜃)𝑇 ∇𝑥𝑉).
This approximation is feasible for two reasons. First of all, if
the adversary can signiﬁcantly alter the dynamics in each step,
the system would not be controllable and the optimal policy
would not be able to solve the task. Second, this approximation
implies that neither agent or the adversary can react to the
action of the other and must choose simultaneously. This
assumption is common in prior works [5]. The order of the
minimization and maximization is interchangeable. For both

cases the optimal action as well as optimal model disturbance
are identical and require the same approximation during the
(cid:3)
derivation.

Value Function Representation

For the value function we are using a locally quadratic deep
network as described in [27]. This architecture assumes that
the state cost is a negative distance measure between x𝑡 and the
desired state xdes. Hence, 𝑞𝑐 is negative deﬁnite, i.e., 𝑞(x) <
0 ∀ x ≠ xdes and 𝑞(xdes) = 0. These properties imply that
𝑉 ∗ is a negative Lyapunov function, as 𝑉 ∗ is negative deﬁnite,
𝑉 ∗ (xdes) = 0 and ∇𝑥𝑉 ∗(xdes) = 0 [61]. With a deep network
a similar representation can be achieved by

𝑉 (x; 𝜓) = − (x − xdes)𝑇 L(x; 𝜓)L(x; 𝜓)𝑇 (x − xdes)
with L being a lower triangular matrix with positive diagonal.
This positive diagonal ensures that LL𝑇 is positive deﬁnite.
Simply applying a ReLu activation to the last layer of a deep
network is not suﬃcient as this would also zero the actions for
the positive values and ∇𝑥𝑉 ∗ (xdes) = 0 cannot be guaranteed.
The local quadratic representation guarantees that the gradient
and hence, the action, is zero at the desired state. However, this
representation can also not guarantee that the value function
has only a single extrema at xdes as required by the Lyapunov
theory. In practice, the local regularization of the quadratic
structure to avoid high curvature approximations is suﬃcient
as the global structure is deﬁned by the value function target.
L is the mean of a deep network ensemble with 𝑁 independent
parameters 𝜓𝑖. The ensemble mean smoothes the initial value
function and is diﬀerentiable. Similar representations have
been used by prior works in the safe reinforcement learning
community [62–65].

VII. Detailed Experimental Setup

Systems The performance of the algorithms is evaluated
using the swing-up the torque-limited pendulum, cartpole and
Furuta pendulum. The physical cartpole (Figure 4) and Furuta
pendulum (Figure 5) are manufactured by Quanser [37]. For
simulation, we use the equations of motion and physical
parameters of the supplier. Both systems have very diﬀerent

Figure 8. The learning curves for DP rFVI and RTDP rFVI with diﬀerent adversary amplitudes averaged over 5 seeds. The shaded area displays the min/max
range between seeds. The 𝛼 corresponds of the percentage of the admissible set for all adversaries, i.e., with increasing 𝛼 the adversary becomes more
powerful. For DP rFVI the stronger adversaries do aﬀect the ﬁnal performance only marginally. For RTDP rFVI the adversaries become too powerful for small
𝛼 and prevent learning of the optimal policy. This eﬀect is especially distinct for the Furuta pendulum as this system is very sensible due to the low masses.
Therefore, DP rFVI can learn a good optimal policy despite very strong adversaries.

rFVI starts to fail at 𝛼 > 0.1. The Furuta pendulum starts to
fail earlier as this system is much more sensitive and smaller
actions cause large changes in dynamics compared to the cart-
pole. This ablation study shows that the dynamic programming
variant can learn the optimal policy despite strong adversaries.
In contrast the real-time dynamic programming variant fails
to learn the optimal policy for comparable admissible set.
This failure is caused by the missing positive feedback during
exploration. The adversary prevents the policy from discov-
ering the positive reward during exploration. Therefore, the
policy converges to a too pessimistic policy. In contrast the
dynamic programming variant does not rely on exploration and
covers the compact state domain. Therefore, the optimal policy
discovers the optimal policy despite the strong adversary.

A. Physical Experiments
The rewards of all baselines on the nominal physical system
are reported in Table III. The robustness experiments were only
performed for the best performing baselines. On the nominal
system rFVI outperforms all baselines on the cartpole. The
domain randomization baselines do not necessarily outperform
the deterministic deep RL baselines as the main source of
simulation gap is the backslash and stiction of the actuator
which is not randomized. For the Furuta pendulum, DP rFVI
has a lower reward compared to DP cFVI. However, this
lower reward is only caused by the chattering during the
balancing that causes very high actions costs. If one only
considers the swing-up phase, DP rFVI outperforms both
PPO-U UDR and DP cFVI. For the Furuta pendulum the
deep RL & UDR baselines outperform the deep RL baselines
without UDR. This is expected as the main uncertainty for the
Furuta pendulum is caused by the uncertainty of the system
parameters. In general a trend is observable that the algorithms
with larger state domain during training achieve the better
sim2real transfer.

characteristics. The Furuta pendulum consists of a small and
light pendulum (24g, 12.9cm) with a strong direct-drive motor.
Even minor diﬀerences in the action cause large changes
in acceleration due to the large ampliﬁcation of the mass-
matrix inverse. Therefore, the main source of uncertainty for
this system is the uncertainty of the model parameters. The
cartpole has a longer and heavier pendulum (127g, 33.6cm).
The cart is actuated by a geared cogwheel drive. Due to the
larger masses the cartpole is not so sensitive to the model
parameters. The main source of uncertainty for this system is
the friction and the backlash of the linear actuator. The systems
are simulated and observed with 500Hz. The control frequency
varies between algorithm and is treated as hyperparameter.

Reward Function The desired state for all tasks is the upward
pointing pendulum at xdes = 0. The state reward is described
by 𝑞𝑐 (x) = −(z − zdes)𝑇 Q(z − zdes) with the positive deﬁnite
matrix Q and the transformed state z. For continuous joints the
joint state is transformed to 𝑧𝑖 = 𝜋2 sin(𝑥𝑖). The action cost is
described by 𝑔𝑐 (u) = −2 β umax/𝜋 log cos(𝜋 u/(2 umax)) with
the actuation limit umax and the positive constant β. This bar-
rier shaped cost bounds the optimal actions. The corresponding
policy is shaped by ∇ ˜𝑔(w) = 2 umax/𝜋 tan−1 (w/β). For the
experiments, the reward parameters are

Pendulum: Qdiag = [01.0, 0.1] ,
Cartpole: Qdiag = [25.0, 1.0, 0.5, 0.1] ,
Furuta Pendulum: Qdiag = [01.0, 5.0, 0.1, 0.1] ,

𝛽 = 0.5
𝛽 = 0.1
𝛽 = 0.1

Additional Experimental Results

This section summarizes the additional experiments omitted
in the main paper. In the following we perform an ablation
study varying the admissible set and report the performance
of additional baselines on the nominal physical system.

Ablation Study - Admissible Set
The ablation study highlighting the diﬀerences in learning
curves for diﬀerent admissible sets is shown in Figure 8. In
this plot we vary the admissible set of each adversary from 0
to the 1, where 𝛼 = 1 corresponds to the admissible set used
for the robustness experiments. With increasing admissible set
the ﬁnal performance of the adversary decreases marginally for
DP rFVI. For RTDP rFVI the optimal policy does not learn
the task for stronger adversaries. RTDP rFVI starts to fail for
𝛼 > 0.3 on the cartpole. On the Furuta pendulum, RTDP

050100150200250Episode−150−100−500RewardCartpole-DPrFVI050100150200250Episode−400−300−200−1000RewardFurutaPendulum-DPrFVI050100150200250Episode−150−100−500RewardCartpole-RTDPrFVI050100150200250Episode−400−300−200−1000RewardFurutaPendulum-RTDPrFVIα=0.0α=0.1α=0.2α=0.3α=0.4α=0.5α=0.6α=0.7α=0.8α=0.9α=1.0Figure 9. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Furuta pendulum. The diﬀerent columns
correspond to diﬀerent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up
for the diﬀerent pendulum masses. In contrast to DP rFVI, the baselines start to deviate strongly from trajectories on the nominal system. When weights are
added the baselines start to cover the complete state-space.

Figure 10. The roll-outs of DP rFVI, DP cFVI and the deep RL baselines with domain randomization the physical Cartpole. The diﬀerent columns correspond
to diﬀerent pendulum masses. The deviation from the dashed center line corresponds to the joint velocity. DP rFVI achieves a consistent swing-up for the
diﬀerent pendulum masses but the failure rate slighlty increases when weights are added to the pendulum. In contrast to DP rFVI, the baselines start to deviate
stronger from trajectories on the nominal system when the system dynamics are altered.

