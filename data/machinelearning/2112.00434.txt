2
2
0
2

r
a

M
9
1

]

G
L
.
s
c
[

2
v
4
3
4
0
0
.
2
1
1
2
:
v
i
X
r
a

Training Experimentally Robust and Interpretable
Binarized Regression Models Using Mixed-Integer
Programming

Sanjana Tule
Monash University
Melbourne, Australia
stul0004@student.monash.edu

Nhi Ha Lan Le
Monash University
Melbourne, Australia
hlee0047@student.monash.edu

Buser Say
Monash University
Melbourne, Australia
buser.say@monash.edu

Abstract—In this paper, we explore model-based approach to
training robust and interpretable binarized regression models for
multiclass classiﬁcation tasks using Mixed-Integer Programming
(MIP). Our MIP model balances the optimization of prediction
margin and model size by using a weighted objective that:
minimizes the total margin of incorrectly classiﬁed training in-
stances, maximizes the total margin of correctly classiﬁed training
instances, and maximizes the overall model regularization. We
conduct two sets of experiments to test the classiﬁcation accuracy
of our MIP model over standard and corrupted versions of
multiple classiﬁcation datasets, respectively. In the ﬁrst set of
experiments, we show that our MIP model outperforms an equiv-
alent Pseudo-Boolean Optimization (PBO) model and achieves
competitive results to Logistic Regression (LR) and Gradient
Descent (GD) in terms of classiﬁcation accuracy over the standard
datasets. In the second set of experiments, we show that our
MIP model outperforms the other models (i.e., GD and LR) in
terms of classiﬁcation accuracy over majority of the corrupted
datasets. Finally, we visually demonstrate the interpretability of
our MIP model in terms of its learned parameters over the
MNIST dataset. Overall, we show the effectiveness of training
robust and interpretable binarized regression models using MIP.

Index Terms—robust machine learning, interpretable machine

learning, mixed-integer programming

I. INTRODUCTION

Machine learning models have signiﬁcantly improved the
ability of autonomous systems to solve challenging tasks, such
as image recognition [1], speech recognition [2] and natural
language processing [3]. The rapid deployment of such models
in safety critical systems resulted in an increased interest in
the development of machine learning models that are robust
and interpretable [4].

In the context of machine learning, robustness refers to the
performance stability of the model in the presence of natural
and/or adversarial changes [5]. For example, random noise in
the training dataset, or random changes in the environment can
both signiﬁcantly degrade the testing accuracy of a machine
learning model when they are not accounted for [6]. Orthogo-
nal to robustness, interpretability is concerned with the insights

that the learned model can provide to its users about the
relationships that are present in the dataset or the model [7].
Interpretable models are desirable as they often drive actions
and further discoveries. Interpretable machine learning aims
to construct models that are low in complexity (i.e., typically
measured by the size of the model), highly simulatable (i.e.,
how easy for the user to simulate the prediction of the model),
and highly modular (i.e., can each subcomponent of the model
be evaluated separately). Interpretable machine learning can be
thought of as a set of criteria that should be considered prior
to the model selection.

In this paper, we train interpretable machine learning models
that are experimentally robust to random corruptions in the
dataset. Speciﬁcally, we focus on performing the supervised
multiclass classiﬁcation task, and limit our scope to random
corruption of labels. Based on the interpretability criteria (i.e.,
complexity, simulatability and modularity), we select binarized
Linear Regression as the appropriate machine learning model,
and optimally learn its parameters by solving an equivalent
Mixed-Integer Programming (MIP) model. We experimentally
demonstrate the performance beneﬁt of training a binarized
Linear Regression model using MIP over both standard and
corrupted datasets, and visually validate the interpretability
of the learned model using the MNIST dataset. Overall, we
contribute to the interpretable machine learning literature with
an experimentally robust training methodology using MIP.

II. PRELIMINARIES

We begin this section by presenting the notation that is
used throughout the paper, proceed with the deﬁnition of the
supervised multiclass classiﬁcation task, and conclude with
a brief introduction of the notions of data corruption and
interpretability in machine learning.

A. Notation

• F = {1, . . . , n} is the set of features for positive integer

n ∈ Z+.

Work supported by the Faculty of Information Technology at Monash

University through the 2021 Early Career Researcher Seed Grant.

• C = {1, . . . , m} is the set of classes for positive integer

m ∈ Z+.

 
 
 
 
 
 
• I = {1, . . . , k} and J = {k + 1, . . . , k + l} are the sets of
training and test instances for positive integers k ∈ Z+
and l ∈ Z+, respectively.

• Document description (i.e., input) (cid:126)xi is a vector of values
for instance i ∈ I ∪ J over the set of features F such
that (cid:126)xi = (x1,i, . . . , x|F |,i) ∈ (cid:81)|F |
f =1 Df where Df is the
domain of input for feature f ∈ F . In this paper, we
assume the domains of input for all features f ∈ F are
binary (i.e., Df = {0, 1} for all f ∈ F ).
• li ∈ C is the label for instance i ∈ I ∪ J.
• D = {d1, . . . , d|I∪J|} is the dataset such that di =

(cid:104)(cid:126)xi, li(cid:105) for instance i ∈ I ∪ J.

B. Multiclass Classiﬁcation

Given dataset D, (supervised) multiclass classiﬁcation is the
task of constructing a function (cid:102) : {0, 1}|F | → C using the
training dataset {d1, . . . , d|I|} to correctly predict the label
lj ∈ C of input (cid:126)xj for test instances j ∈ J such that (cid:102)((cid:126)xj) =
lj.

|I|

Multiclass classiﬁcation task is commonly solved using
models that optimize primarily the training accuracy of func-
tion (cid:102) that can be deﬁned as |{i|(cid:102)((cid:126)xi)=li,i∈I}|
and secondarily
the complexity of function (cid:102) that is typically measured by the
number of (non-zero) parameters of function (cid:102). Intuitively,
maximization of training accuracy aims to learn the relation-
ship between input (cid:126)xi and label li ∈ C while the minimization
of complexity aims to select the simplest function (cid:102). It has
been experimentally shown that the combined optimization of
these two objectives can learn function (cid:102) with higher test
accuracy (i.e., compared to the optimization of the training
accuracy by itself) by avoiding overﬁtting of function (cid:102) to
the training instances [8].

In many settings, however, the training dataset might be
corrupted which can lower the test accuracy of function (cid:102).
In this paper, we focus on learning accurate function (cid:102) that
is experimentally robust to corrupted training datasets, and
brieﬂy cover data corruption in machine learning next.

C. Data Corruption in Machine Learning

In the context of machine learning, data corruption refers
to modiﬁcations to input (cid:126)xi and/or label li ∈ C of training
instance i ∈ I. The nature of such modiﬁcations can be
due to random noise and/or adversarial attacks, which can
signiﬁcantly lower the test accuracy of function (cid:102) [9]. In
this paper, we focus on learning accurate function (cid:102) that is
experimentally robust to random corruption of labels.

In machine learning, experimental accuracy and experimen-
tal robustness are both considered as important objectives to
measure the performance of function (cid:102), and have mainly
driven the research to solve challenging tasks, such as image
recognition [1], speech recognition [2] and natural language
processing [3]. However, such performance increase often
came at a price of interpretability of the learned function (cid:102).
Orthogonal to the previously covered performance-related ob-
jectives, interpretability is concerned with the insights function
(cid:102) provides to its users about relationships that are present in

the dataset I ∪ J or the learned function (cid:102) [7]. In this paper,
we focus on learning an interpretable function (cid:102), and brieﬂy
cover interpretable machine learning next.

D. Interpretable Machine Learning

Interpretable machine learning is the extraction of knowl-
edge from function (cid:102) regarding relationships that are present
in the dataset I ∪ J or learned by the function (cid:102) [7]. The
knowledge extracted by function (cid:102) is deemed to be relevant
if it provides insight for its users about the underlying problem
it solves. These insights often drive actions and discovery, and
can be communicated through visualization, natural language,
or mathematical equations as we will demonstrate in Sec-
tion IV-D. In this paper, the following interpretability criteria
are applied in the modeling stage of our approach to construct
function (cid:102) [7]:

1) Complexity: Interestingly, complexity has a dual role
in both improving the test accuracy (as previously
discussed in Section II-B) and the interpretability of
function (cid:102). That is, when the learned function (cid:102) has
sufﬁciently small number of non-zero parameters, the
user can understand how the learned parameters relate
the input (cid:126)xi to the label li ∈ C for instance i ∈ I ∪ J.
2) Simulatability: A function (cid:102) is considered to be simu-
latable if the user can internalize the entire prediction
process of function (cid:102) (e.g., if the user can predict the
label li ∈ C given the input (cid:126)xi for function (cid:102)). Since
simulatability requires the entire prediction process of
function (cid:102) to be internalized by the user, the computa-
tions required to make a prediction must be both simple
and small in size.

3) Modularity: A function (cid:102) is considered to be modular
if the subcomponents of function (cid:102) can be interpreted
independently.

III. TRAINING ROBUST AND INTERPRETABLE BINARIZED
REGRESSION MODELS

In this section, we ﬁrst describe our model assumptions
and choices, and then present two equivalent models based on
Mixed-Integer Programming (MIP) and Pseudo-Boolean Opti-
mization (PBO) for training interpretable binarized regression
models to perform the multiclass classiﬁcation task.

A. Model Assumptions and Choices

In this section, we describe our model assumptions and
choices to perform the multiclass classiﬁcation task. One
fundamental assumption we make in this paper is the existence
of a function ð : {0, 1}|F | → Z|C| such that (cid:102)((cid:126)xi) =
arg max ð((cid:126)xi) holds for all instances i ∈ I ∪ J [5]. This
assumption has two important beneﬁts for effectively modeling
the multiclass classiﬁcation task.

The ﬁrst beneﬁt is that it allows us to learn function ð as a

binarized Linear Regression model of the form:
(cid:88)

(cid:126)wf xf,i + (cid:126)b ∀i ∈ I

ð((cid:126)xi) =

(1)

f ∈F

where (cid:126)wf ∈ {−1, 0, 1}|C| and (cid:126)b ∈ Z|C|. Based on the inter-
pretability criteria that are previously listed in Section II-D, the
binarized Linear Regression model can be considered one of
the most interpretable machine learning models. The binarized
Linear Regression model has:

1) low complexity because it has no latent parameters
(c.f., Deep Neural Networks [10]) and can easily be
regularized,

2) high simulatability due to the binarization of the domain
of the weight parameters (cid:126)wf ∈ {−1, 0, 1}|C| which
gives each value an intuitive semantic meaning (i.e.,
negative impact: -1, no impact: 0 and positive impact:
1) for the prediction of a given class c ∈ C.1 Moreover,
it utilizes only simple additive features that require at
most linear number of addition operations (in the size
of |F |) per class c ∈ C to make its predictions2, and
3) high modularity due to the independence of computa-

tions carried for each output of function ð.

The second beneﬁt is that it allows us to optimize the total
margin by which the training instances I are correctly and
incorrectly classiﬁed. We will experimentally demonstrate in
Section IV-C that this optimization allows us to learn function
(cid:102) that is robust to corrupted training datasets.

Next, we present two equivalent models based on Mixed-
Integer Programming (MIP) and Pseudo-Boolean Optimization
(PBO) for training robust and interpretable binarized regres-
sion models, that are based on our modeling assumptions and
choices, to perform the multiclass classiﬁcation task.

B. Mixed-Integer Programming Model

In this section, we present the Mixed-Integer Programming
(MIP) model to train a binarized regression model to perform
the multiclass classiﬁcation task.

Hyperparameters: The MIP model uses the following

hyperparameters:

• α ∈ R+ is the hyperparameter representing the im-
portance of maximizing the total margin of correctly
classiﬁed training instances.

• β ∈ R+ is the hyperparameter representing the im-
portance of minimizing the total margin of incorrectly
classiﬁed training instances.
Decision Variables: The MIP model uses the following

decision variables:

• w+

f,c ∈ {0, 1} and w−

f,c ∈ {0, 1} together encode the
value of the weight between feature f ∈ F and class
c ∈ C.

• bc ∈ Z encodes the value of the bias for class c ∈ C.

1Note that while letting the domain of the weight parameter (cid:126)wf to be
integers could improve the test accuracy of function ð, it would also decrease
its interpretability since the user does not have a prior knowledge on the
meaning of the values of function ð. That is, arbitrarily high or low values
of the weight parameters contradicts with the simulatability criterion as such
values do not have a natural semantic meaning to the user.

2Regularization can decrease this value signiﬁcantly in practice as we

experimentally show in Table II.

• yc,i ∈ Z encodes the value of the prediction for class

• e+

c ∈ C and training instance i ∈ I.
i ∈ Z+ encode the value of the margin
i ∈ Z+ and e−
for (i) correctly and (ii) incorrectly classifying training
instance i ∈ I, respectively.
Constraints: The MIP model uses the following con-

straints:

w+
f,c + w−
(cid:88)
(w+

f,c ≤ 1
f,c − w−

f,c)xf,i + bc = yc,i

∀f ∈ F, c ∈ C (2)

∀c ∈ C, i ∈ I

(3)

i − e−

i

f ∈F
yli,i ≥ yc,i + e+
i ≥ 0, e−
e+
i ≥ 0
w+
f,c, w−
f,c ∈ {0, 1}
e+
i , e−
yc,i ∈ Z
bc ∈ Z

i ∈ Z

(4)

∀i ∈ I, c ∈ C \ li
∀i ∈ I

(5)
∀f ∈ F, c ∈ C (6)

∀i ∈ I

(7)

∀c ∈ C, i ∈ I

(8)
∀c ∈ C (9)

where constraint (2) ensures that the weight between feature
f ∈ F and class c ∈ C cannot be both positive and negative,
constraint (3) computes the prediction as the weighted sum of
its inputs plus the bias for class c ∈ C and training instance
i ∈ I, constraint (4) relates the prediction of all classes C
to the margin of correctly and incorrectly classifying training
instance i ∈ I, constraint (5) enforces the margin of correctly
and incorrectly classifying training instance i ∈ I to be non-
negative and constraints (6-9) deﬁne the domain of all decision
variables.

Objective Function: The MIP model uses the following

objective function:

min −α

e+
i + β

(cid:88)

i∈I

e−
i +

(cid:88)

i∈I

(cid:88)

f ∈F,c∈C

(w+

f,c + w−

f,c)

(10)

which balances the optimization of prediction margin and
model size by (i) maximizing the total margin of correctly
classiﬁed training instances, (ii) minimizing the total margin
of incorrectly classiﬁed training instances and (iii) minimizing
the total model size.

C. Pseudo-Boolean Optimization Model

In this section, we present the Pseudo-Boolean Optimization
(PBO) model to train a binarized regression model to perform
the multiclass classiﬁcation task.

Hyperparameters: The PBO model uses the same sets
of hyperparameters as the previously presented MIP model
in addition to hyperparameter Q that is used to quantize the
domains of integer-valued decision variables.

f,c ∈ {0, 1} and w−

Decision Variables, Constraints and Objective Function:
The PBO model uses the same sets of decision variables as
the previously presented MIP model to encode the weights
w+
f,c ∈ {0, 1}. Given PBO is restricted to
have decision variables with only binary domains, the PBO
model uses a set of decision variables bc,q ∈ {0, 1}, yc,i,q ∈
{0, 1}, e+
i,q ∈ {0, 1} to equally represent the
i ∈ Z+
domains of decision variables bc ∈ Z, yc,i ∈ Z, e+

i,q ∈ {0, 1} and e−

i ∈ Z+, respectively. The domain of some (bounded)
and e−
decision variable x ∈ Z is represented using the following
formula3:

Dataset
MNIST

|F|, |C|
784, 10

x = xLB +

Q
(cid:88)

q=1

2q−1xq

together with the following constraint:

(11)

Flags

43, 5

Ubuntu

48
93
153

α, β
5, 10

2, 5

2, 5

|I|
20, 60,
100, 200,
300, 500,
700, 1000

10, 25,
35, 50
10
23
43

Brief Description
Image classiﬁcation task
with grayscale images of
handwritten digits
represented by 784 pixels
(i.e., in 28 by 28 format).
Describes the attributes
of the ﬂags of countries.
Natural language processing
task to classify the intention
of the user behind the
question asked.

xLB +

Q
(cid:88)

q=1

2q−1xq ≤ xU B

(12)

where xLB and xU B represent the lower bound and the upper
bound on decision variable x, and the value of hyperparameter
Q is calculated according to the following formula:

Q = (cid:100)log2(xU B − xLB + 1)(cid:101)

(13)

Finally, the PBO model uses the same sets of constraints and
the objective function as the previously presented MIP model
with the minor modiﬁcation that encodes the set of decision
variables bc ∈ Z, yc,i ∈ Z, e+
i ∈ Z+ according
to formula (11), formula (13) and constraint (12) using the
set of decision variables bc,q ∈ {0, 1}, yc,i,q ∈ {0, 1}, e+
i,q ∈
{0, 1} and e−

i ∈ Z+ and e−

i,q ∈ {0, 1}.

IV. EXPERIMENTAL RESULTS

In this section, we begin by presenting the results of two sets
of computational experiments. In the ﬁrst set of experiments,
we test the effectiveness of training a binarized regression
model to perform the multiclass classiﬁcation task using our
MIP and PBO models against Logistic Regression (LR) with
real-valued weights and biases, and Gradient Descent (GD)
with binarized weights and integer-valued biases over three
standard datasets, namely: MNIST [12], Flags [13] and Ask
Ubuntu [14]. We provide detailed comparative results on both
the training and test accuracy of all models, and further report
the model sizes, runtimes and optimality gaps of our MIP and
PBO models. Our detailed results for the ﬁrst set of experi-
ments show that the MIP model achieves competitive test ac-
curacy results to the real-valued LR model over three datasets
and outperforms the LR model in one dataset, while achieving
upto around 70% model size reduction. In the second set of
experiments, we test the effectiveness of training a binarized
regression model to perform the multiclass classiﬁcation task
using our MIP model against the previously described LR and
GD models over three corrupted datasets. We provide detailed
comparative results on the test accuracy of all models. Our
results for the second set of experiments show that our MIP
model outperforms the LR model in majority of the corrupted
datasets without experiencing signiﬁcant degradation in test
accuracy. We conclude this section by visually demonstrating
the interpretability of our MIP model using the interpretability
criteria discussed previously in Section II-D.

3While we do not work with real-valued decision variables in this paper,
it is important to note that a similar quantization formula can be used to
approximate the domains of real-valued decision variables [11].

TABLE I: Summary of the datasets including feature and
class sizes, number of training instances used for each training
problem, the values of the hyperparameters α and β, and the
brief descriptions of the datasets.

A. Experimental Domains and Setup

The domains and the setup used throughout this section is

as follows.

Experimental Domains: The MNIST [12] dataset consists
of 784 features and 10 classes with 70,000 instances. The
dataset is of grayscale images of handwritten digits represented
by 784 pixels (i.e., in 28 by 28 pixel format) where each
pixel has a value between 0 and 255. We have binarized each
pixel such that the pixel is set to 1 if its value is greater
than 255
2 , and 0 otherwise. We trained our models over 20,
60, 100, 200, 300, 500, 700 and 1000 training instances, and
reported test accuracy results over the remaining dataset. The
Flags [13] dataset consists of 43 features and 5 classes with
143 instances. The dataset describes the attributes of the ﬂags
of various countries. We trained our models over 10, 25, 35 and
50 training instances, and reported test accuracy results over
the remaining dataset. Finally, the Ask Ubuntu [14] dataset
consists of 5 classes with 162 instances. The dataset has ﬁve
classes which refer to the user’s intent behind the questions
asked. Natural language preprocessing is performed to extract
the features of this dataset. We trained our models over
10, 23 and 43 training instances, and reported test accuracy
results over the remaining dataset. Finally in the second set
of experiments, 10% of labels are randomly assigned to a
different class, and everything else remained the same. Table I
provides a summary of the datasets.

Experimental Setup: All experiments were run on Intel
Core i5-8250U CPU 1.60GHz with 8.00 GB memory, using
a single thread with one hour total time limit per training
problem. The MIP model is optimized using Gurobi [15],
the PBO model is optimized using RoundingSat [16], the LR
model is optimized using Scikit-learn [17] and the GD model
is created using LARQ [18] and trained with a learning rate
of 0.001 over 200 epochs using Adam [19]. The value of
hyperparameter α is selected using grid search where the value
of hyperparameter β is set to 2α. Both MIP and PBO models
used the same selected values of hyperparameters α and β as
detailed in Table I.

Model
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PB
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PBO
MIP
PB
MIP
PBO
MIP
PBO
MIP
PBO

Dataset, |I|
MNIST, 20
MNIST, 20
MNIST, 60
MNIST, 60
MNIST, 100
MNIST, 100
MNIST, 200
MNIST, 200
MNIST, 300
MNIST, 300
MNIST, 500
MNIST, 500
MNIST, 700
MNIST, 700
MNIST, 1000
MNIST, 1000
Flags, 10
Flags, 10
Flags, 25
Flags, 25
Flags, 35
Flags, 35
Flags, 50
Flags, 50
Ubuntu, 10
Ubuntu, 10
Ubuntu, 23
Ubuntu, 23
Ubuntu, 43
Ubuntu, 43

Gap
0.00
0.34
0.00
0.52
0.00034
0.51
0.00063
0.85
0.00068
1.28
0.00078
9.76
0.0010
12.60
0.00081
140.89
0.00
0.00
0.00
96.25
0.00
72.03
0.00
119.56
0.00
0.00
0.00
0.00
0.00
251.08

Time (sec.)
3.99
x ≥ T.O.
304.87
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
T.O.
0.13
22.00
0.71
T.O.
0.58
T.O.
2.43
T.O.
0.11
13.16
0.35
1192.20
0.83
T.O.

Reduction (%)
62.19
3.48
55.23
11.28
49.72
17.48
44.87
34.22
43.52
31.98
40.47
19.97
39.31
15.30
37.33
6.54
69.77
53.48
48.37
51.62
41.40
47.44
44.19
37.67
69.58
28.75
68.17
33.54
65.88
31.50

TABLE II: Comparison of the MIP model and the PBO
model in terms of their duality gaps, runtimes and model size
reductions in Experiment 1 where T.O. means timeout.

model in terms of testing accuracy, and perform competitively
with the real-valued LR model.

The inspection of Table II highlights the beneﬁt of using
the MIP model over the PBO model for training binarized re-
gression models. Across all training problems, the MIP model
outperforms the PBO model in terms of runtime performance
where the PBO model almost always runs out of the one hour
time limit. As a result, the PBO model can return feasible
solutions with large duality gaps that can yield low testing
accuracy (e.g., as visualized in subﬁgure 1b).

C. Experiment 2: Training with Corrupted Datasets

In the second set of experiments, we compare the effec-
tiveness of performing the multiclass classiﬁcation task using
the MIP model against LR and GD models over corrupted
datasets. Figure 2 visualizes the test accuracy of all three
models over each corrupted dataset (i.e., represented by in-
dividual rows) as well as the decrease in test accuracy that is
due to data corruption, which is measured by the difference
between the test accuracy of a given model in Experiment 1
and Experiment 2.

In Figure 2, the inspection of subﬁgures 2a, 2b and 2c
demonstrates the experimental robustness of the MIP model
three corrupted
over both LR and GD models across all
datasets. In the corrupted MNIST and Flags datasets, the MIP
model outperforms both LR and GD models with minimal
decrease in its testing accuracy. Similarly in the corrupted Ask

(a) Standard MNIST

(b) Standard MNIST

(c) Standard Flags

(d) Standard Flags

(e) Standard Ask Ubuntu

(f) Standard Ask Ubuntu

Fig. 1: Visualization of Experiment 1 which compares the
training accuracy (left) and the test accuracy (right) of the
MIP model (red) and the PBO model (green) against the LR
model (blue) and the GD model (orange) over three standard
datasets and different number of training instances within one
hour time limit.

B. Experiment 1: Training with Standard Datasets

In the ﬁrst set of experiments, we compare the effectiveness
of performing the multiclass classiﬁcation task using the MIP
model and the PBO model against LR and GD over the
standard datasets. Figure 1 visualizes the training accuracy
(i.e., the left column) and the test accuracy (i.e., the right
column) of all models over each dataset (i.e., represented by
individual rows). Table II compares the model sizes, runtimes
and optimality gaps of both MIP and PBO models.

In Figure 1, the inspection of subﬁgures 1a and 1b highlights
to scale with the
the clear capability of the MIP model
increasing size of the training datasets. In the MNIST dataset,
the MIP model outperforms the binarized GD model in 7 out of
8 training problems in terms of testing accuracy, and performs
competitively with the real-valued LR model. In contrast to
the MIP model, the performance of the PBO model degrades
with the increasing number of training instances. Moreover,
the inspection of subﬁgures 1c, 1d, 1e and 1f demonstrates
the ability of both the MIP model and the PBO model to train
models with competitive test accuracies over small training
problems. In Flags and Ask Ubuntu datasets, the MIP model
and the PBO model consistently outperform the binarized GD

Simulatability: In order to demonstrate the simulatability
of the learned binarized regression model, we simply visualize
the learned non-zero weights of the learned binarized regres-
sion model as follows. For each class c ∈ C = {0, 1, . . . , 9},
we color pixel f ∈ F = {0, 1, . . . , 783} to: black if the value
of decision variable w+
f,c that is obtained from solving the
MIP model is 1, white if the value of decision variable w−
f,c
that is obtained from solving the MIP model is 1, and gray
otherwise. Given the visualization of the learned weights in
Figure 3, the users can take any input image and simulate
the prediction of the learned model by visually comparing
the input image to the subﬁgures 3a to 3j. Intuitively, each
subﬁgure c visually represents what
the learned binarized
regression model predicts number c to look like (and not to
look like) based on its learned weights.

V. DISCUSSION, RELATED WORK AND FUTURE WORK

In this section, we discuss our assumptions, choices, con-
tributions and experimental results in relation to the literature
with the goal of opening new areas for future work.

In Section III-A, we detailed our model assumptions and
choices for training accurate and interpretable binarized mod-
els using MIP and PBO. Similar to Rosenfeld et al. [5], we
made the assumption on the existence of function ð which
allowed us to train linear regression models for the multiclass
classiﬁcation task. In Section IV-B, we experimentally demon-
strated that modeling ð instead of (cid:102) signiﬁcantly improves
the test performance of our MIP model. Our results suggest
that similar works on training Binarized Neural Networks [20]
using MIP models [21] might also beneﬁt from similar explicit
modeling of function ð.

In Section IV-C, we experimentally demonstrated that our
learned binarized regression model is robust to random label
corruption. Under adversarial settings,
the data corruption
problem can be formulated as a bilevel optimization problem
where the attacker tries to optimally corrupt the dataset in
order to degrade the test accuracy of the machine learning
model [22]. Under such adversarial settings, the ability to
provide formal robustness guarantees [6] presents an important
venue for future work.4 Similar to this paper, the robustness
study of other interpretable machine learning models (e.g.,
decision lists [23], decision sets [24], decision trees [25]) can
potentially yield important ideas for future work.

In Section IV-D, we experimentally demonstrated the in-
terpretability of our learned binarized regression model on a
challenging visual task (i.e., MNIST). The effective visual-
ization of our learned models on non-visual tasks (e.g., as
visualized in subﬁgures 4a-4c and 4d-4e for Flags and Ask
Ubuntu datasets) in order to derive valuable insights remains
an important area for future work.

4We have experimented with a modiﬁed version of our MIP model that uses
big-M constraints to remove outliers, and found that this model performed
worse than our original MIP model. We conjecture that this is because (i) the
balanced optimization of prediction margin and model size is sufﬁcient for
good experimental performance under our experimental settings and (ii) the
modiﬁed MIP model is harder to optimize due to the big-M constraints.

(a) Corrupted MNIST

(b) Corrupted Flags

(c) Corrupted Ask Ubuntu

Fig. 2: Visualization of Experiment 2 which compares the test
accuracy of the MIP model (red) against the LR model (blue)
and the GD model (orange) over three corrupted datasets and
different number of training instances within one hour time
limit. For each model, the black region represents the decrease
in test accuracy due to random corruption of labels.

Ubuntu dataset, the MIP model outperforms the GD model
while performing competitively with the LR model.

Overall, our results highlight the clear performance beneﬁt
of training binarized regression models for the multiclass
classiﬁcation task using MIP. Next, we turn our attention to
the interpretability our MIP model.

D. Interpretability Results

In this section, we focus on the interpretability of our MIP
model based on the criteria that are previously discussed in
Section II-D, namely: complexity, simulatability and modular-
ity. Given the binarized regression model is highly modular
by deﬁnition (i.e., each output of the regression model can
be computed independently from the others), we focus on the
remaining two criteria.

Complexity: As evident from Table II, our MIP model
often learns binarized regression models with signiﬁcantly
reduced complexity through near optimal regularization; by
setting both decision variables w+
f,c ∈ {0, 1}
equal to 0. Given the learned binarized regression model is
often optimally regularized and has no latent parameters by
deﬁnition, we conclude that the binarized regression model
learned by solving our MIP model has low complexity.

f,c ∈ {0, 1} and w−

(a) Number 0

(b) Number 1

(c) Number 2

(d) Number 3

(a) Flags 0

(b) Flags 1

(e) Number 4

(f) Number 5

(c) Flags 2

(d) Ubuntu 0

(g) Number 6

(h) Number 7

(i) Number 8

(j) Number 9

Fig. 3: Visualization of interpretability results for MNIST over
each number c ∈ C = {0, . . . , 9}. A pixel is colored to: black
if the value of decision variable w+
f,c is 1, white if the value
of decision variable w−
f,c is 1, and gray otherwise. Intuitively,
each subﬁgure c visually represents what the learned binarized
regression model predicts number c to look like (and not to
look like) based on its learned weights.

(e) Ubuntu 1

(f) Ubuntu 2

Fig. 4: Visualization of interpretability results for Flags and
Ask Ubuntu datasets over different classes using the same
coloration methodology that is described in Figure 3 where
each vertex represents a feature and two vertices are connected
by an edge based on their Hamming distance to each other.
Here, we have used the Continuous k-Nearest Neighbours
method [26] such that an edge is deﬁned for a pair of features
x, y ∈ F if d(x, y) < δ ∗ (cid:112)d(x, xk)d(y, yk) where d(x, y)
denotes the Hamming distance between features x and y,
δ ∈ R+ is a parameter controlling the sparsity of the graph,
and features xk, yk ∈ F are the k-th nearest neighbours of
features x and y, respectively.

[22] Z. Suvak, M. F. Anjos, L. Brotcorne, and D. Cattaruzza, “Design of
poisoning attacks on linear regression using bilevel optimization,” 2021.
[23] R. L. Rivest, “Learning decision lists,” Mach. Learn., vol. 2, no. 3, p.

229–246, Nov. 1987.

[24] P. Clark and T. Niblett, “The cn2 induction algorithm,” Mach. Learn.,

vol. 3, no. 4, p. 261–283, 1989.

[25] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classiﬁcation
and Regression Trees. Monterey, CA: Wadsworth and Brooks, 1984.
[26] T. Berry and T. Sauer, “Consistent manifold representation for topologi-
cal data analysis,” Foundations of Data Science, vol. 1, no. 1, pp. 1–38,
2019.

REFERENCES

[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” in 25th NIPS, 2012, pp. 1097–
1105.

[2] L. Deng, G. E. Hinton, and B. Kingsbury, “New types of deep neural
network learning for speech recognition and related applications: an
overview,” in IEEE International Conference on Acoustics, Speech and
Signal Processing, 2013, pp. 8599–8603.

[3] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
P. Kuksa, “Natural language processing (almost) from scratch,” JMLR,
vol. 12, pp. 2493–2537, 2011.

[4] C. Rudin, “Stop explaining black box machine learning models for high
stakes decisions and use interpretable models instead,” Nature Machine
Intelligence, vol. 1, pp. 206–215, 05 2019.

[5] E. Rosenfeld, E. Winston, P. Ravikumar, and Z. Kolter, “Certiﬁed
robustness to label-ﬂipping attacks via randomized smoothing,” in
Proceedings of the 37th International Conference on Machine Learning,
ser. Proceedings of Machine Learning Research, H. D. III and A. Singh,
Eds., vol. 119. PMLR, 13–18 Jul 2020, pp. 8230–8241.

[6] N. Natarajan, I. S. Dhillon, P. Ravikumar, and A. Tewari, “Learning with
noisy labels,” in Proceedings of the 26th International Conference on
Neural Information Processing Systems - Volume 1, ser. NIPS’13. Red
Hook, NY, USA: Curran Associates Inc., 2013, p. 1196–1204.

[7] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu, “Def-
initions, methods, and applications in interpretable machine learning,”
Proceedings of the National Academy of Sciences, vol. 116, no. 44, pp.
22 071–22 080, 2019.

[8] P. Bhlmann and S. van de Geer, Statistics for High-Dimensional Data:
Springer Publishing Company,

Methods, Theory and Applications.
Incorporated, 2011.

[9] B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I. Tsang, and
M. Sugiyama, “Co-teaching: Robust training of deep neural networks
with extremely noisy labels,” in NeurIPS, 2018, pp. 8535–8545.
[10] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent

in nervous activity,” pp. 115–133, 1943.

[11] B. Say, “Optimal planning with learned neural network transition mod-
els,” Ph.D. dissertation, University of Toronto, Toronto, ON, Canada,
2020.

[12] Y. LeCun and C. Cortes, “MNIST handwritten digit database,” 2010.
[13] J. D. Romano, T. T. Le, W. La Cava, J. T. Gregg, D. J. Goldberg,
P. Chakraborty, N. L. Ray, D. Himmelstein, W. Fu, and J. H. Moore,
“Pmlb v1.0: an open source dataset collection for benchmarking machine
learning methods,” arXiv preprint arXiv:2012.00058v2, 2021.

[14] D. Braun, A. Hernandez-Mendez, F. Matthes, and M. Langen, “Evaluat-
ing natural language understanding services for conversational question
answering systems,” in Proceedings of the 18th Annual SIGdial Meeting
on Discourse and Dialogue.
Saarbr¨ucken, Germany: Association for
Computational Linguistics, 2017, pp. 174–185.

[15] L. Gurobi Optimization, “Gurobi optimizer reference manual,” 2021.
[16] J. Elffers and J. Nordstr¨om, “Divide and conquer: Towards faster pseudo-
boolean solving,” in Proceedings of the Twenty-Seventh International
Joint Conference on Artiﬁcial Intelligence, ser. IJCAI’18, 2018, pp.
1291–1299.

[17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg et al.,
“Scikit-learn: Machine learning in python,” Journal of machine learning
research, vol. 12, pp. 2825–2830, 2011.

[18] L. Geiger and P. Team, “Larq: An open-source library for training
binarized neural networks,” Journal of Open Source Software, vol. 5,
no. 45, p. 1746, Jan. 2020.

[19] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
2014, cite arxiv:1412.6980Comment: Published as a conference paper
at the 3rd International Conference for Learning Representations, San
Diego, 2015.

[20] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Bi-
narized neural networks,” in Proceedings of the Thirtieth International
Conference on Neural Information Processing Systems, ser. NIPS’16,
USA, 2016, pp. 4114–4122.

[21] R. Toro Icarte, L. Illanes, M. P. Castro, A. A. Cire, S. A. McIlraith, and
J. C. Beck, “Training binarized neural networks using MIP and CP,”
in Principles and Practice of Constraint Programming, T. Schiex and
S. de Givry, Eds. Cham: Springer International Publishing, 2019, pp.
401–417.

