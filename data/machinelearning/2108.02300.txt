ONLINE STOCHASTIC DCA WITH APPLICATIONS TO PRINCIPAL
COMPONENT ANALYSIS ∗

1
2
0
2

g
u
A
4

]

C
O
.
h
t
a
m

[

1
v
0
0
3
2
0
.
8
0
1
2
:
v
i
X
r
a

Hoai An Le Thi
Université de Lorraine, LGIPM
F-57000 Metz, France
hoai-an.le-thi@univ-lorraine.fr

Hoang Phuc Hau Luu
Université de Lorraine, LGIPM
F-57000 Metz, France
hoang-phuc-hau.luu@univ-lorraine.fr

Tao Pham Dinh
Laboratory of Mathematics, INSA-Rouen
University of Normandie
76801 Saint-Étienne-du-Rouvray Cedex, France
pham@insa-rouen.fr

ABSTRACT

Stochastic algorithms are well-known for their performance in the era of big data. In convex opti-
mization, stochastic algorithms have been studied in depth and breadth. However, the current body
of research on stochastic algorithms for nonsmooth, nonconvex optimization is relatively limited.
In this paper, we propose new stochastic algorithms based on DC (Difference of Convex functions)
programming and DCA (DC Algorithm) - the backbone of nonconvex, nonsmooth optimization.
Since most real-world nonconvex programs fall into the framework of DC programming, our pro-
posed methods can be employed in various situations, in which they confront stochastic nature and
nonconvexity simultaneously. The convergence analysis of the proposed algorithms is studied in-
tensively with the help of tools from modern convex analysis and martingale theory. Finally, we
study several aspects of the proposed algorithms on an important problem in machine learning: the
expected problem in Principal Component Analysis.

Keywords DC programming, DCA, nonconvex optimization, online stochastic DCA, Principal Component Analysis

1 Introduction

We consider the following optimization problem

F (w) = E(g(w, Z))

min
w∈S{

E(h(w, Z))
}

,

, P) such that Z : Ω

−
Rm is a nonempty, compact, and convex set, Z is a random vector determined in some complete probability
Rn and g, h are functions satisfying some conditions described later. Broadly, g

where S
⊂
space (Ω,
M
and h are those that make G(w) = E(g(w, Z)) and H(w) = E(h(w, Z)) convex, lower semi-continuous.
The framework of the problem (1) is very general in two aspects. Firstly, the underlying distribution of Z is arbitrary,
which makes it able to treat any random variable involved. As a special case, when Z is uniformly distributed over a
ﬁnite set, we obtain a large-sum problem,

→

(1)

F (w) =

min
w∈S (

1
N

g(w, zi)

1
N

−

N

i=1
X

N

i=1
X

h(w, zi)

)

.

(2)

∗This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which

this version may no longer be accessible.

 
 
 
 
 
 
Online Stochastic DCA with applications to Principal Component Analysis

Secondly, in our setting, g and h are allowed to be nonsmooth, resulting in a very large class of stochastic nonsmooth,
nonconvex DC programs which comprises most real-world problems [29]. Various learning problems possess DC
structures, here we name a few: robust learning [47, 11], robust phase retrieval [12], Positive Unlabeled (PU) learning
with convex loss [17], Difference of Log-sum-exp neural networks [10, 8], principal component analysis [36], etc.

Having said that, the main challenge of the problem (1) also comes from the nonconvex structure of F and the unknown
underlying distribution of Z. So far, there is very few algorithms for stochastic nonconvex and nonsmooth problems
of the general setting (1).

In the literature, stochastic optimization has been investigated thoroughly for convex problems since the seminal
work [43]. In this work, the authors introduced a novel idea of using stochastic approximations (SA) that results
in Stochastic Gradient Descent (SGD). Thanks to its inexpensive computation cost, the SGD really opened a door
in numerical optimization for large-scale problems [5, 2]. Hitherto, many variants of the SGD have been studied
including stochastic subgradient descent [13, 44], incorporating Nesterov’s acceleration technique [15], using second-
order information [4, 3, 9].
In nonconvex optimization, stochastic algorithms remain rare. Most of them require
the objective to be smooth or partially smooth (some components of the objective are smooth). We list here some
main approach to tackle nonconvex stochastic problems. Inspired by the aforementioned SGD, the ﬁrst approach is
stochastic (proximal) (sub)gradient-based methods which are mainly developed for smooth or weakly convex objective
functions [16, 1, 12].
In this approach, a gradient-like update is performed at each iteration where the proximal
operator can be employed. The second is stochastic MM (Majorization-Minimization) for partially smooth objective
[33, 42], in which the stochastic convex surrogate is constructed at each iteration and is minimized to obtain an updated
optimization variable. The third is stochastic Successive Convex Approximation [45, 48] (mainly for smooth objective
functions) that is similar to stochastic MM where the sequence of approximation functions are convex but need not
be the upper bound of sample objective functions. The fourth is stochastic DCA that aims to deal with stochastic
DC programs - a substantially large class to cover almost all real-world nonconvex optimization problems [29]. Initial
works in this approach include [22, 23, 32, 37, 46] that consider some special classes of DC problems such as large-sum
and/or (partially) smooth, as well as [21] working on a very general class of stochastic nonsmooth DC programs. To
extend beyond the DC programming framework, [34] used Moreau envelope which is a DC function to approximate a
nonsmooth, nonconvex regularizer, and then developed a stochastic DCA for solving the resulting problem. It is worth
mentioning that in [33, 42], the authors also consider DC surrogates whose the second DC component is differentiable.
It should be further noted that, as indicated in [29], while the (stochastic) MM proposes a general idea to majorize the
objective function, (stochastic) DCA gives the simplest and the most closed convex surrogate thanks to DC structures
of the objective. Futhermore, usual choices of surrogates of MM result in DCA versions [29].

In deterministic optimization context, DC programming and DCA constitute a quite logical and natural extension
of modern convex analysis/programming to nonsmooth nonconvex analysis/programming, sufﬁciently large to cover
most real-world nonsmooth nonconvex programs, but not too broad in order to explore/exploit the powerful arsenal of
convex analysis/programming. This theoretical and algorithmic philosophy was ﬁrst introduced in 1985 by Pham Dinh
Tao, and widely developed by Le Thi Hoai An and Pham Dinh Tao since 1993 to become now classic and increasingly
popular (see [27, 41, 39, 40] and a comprehensible review on thirty years of developments of DC programming and
DCA in [29]). It is widely recognized that DCA is one of rare algorithms to efﬁciently solve large-scale nonconvex
and nonsmooth programs [29]. Thanks to the pervasiveness of DC programming and the ﬂexible principle of DC re-
formulations, DCA recover almost all standard methods in convex and nonconvex programming. Also, the ﬂexibility
and simplicity of DCA make the method a powerful tool to be employed in various applications in applied sciences
including transport logistic, ﬁnance, computational biology, computational chemistry, robotics, data mining and ma-
chine learning, image processing and computer vision, cryptology, inverse problems and ill-posed problems, etc., see
e.g., [20, 25, 26, 27, 18, 24, 28, 19, 30, 31, 38, 41, 29] and the list of references in [29].

To our knowledge, the paper [21] is the ﬁrst work dealing with the general setting (1) where both DC components
are allowed to be nonsmooth. In that article, the authors proposed several stochastic DCA schemes in the aggregated
update style. That is, all past information (sample realizations) is used to construct subproblems. These algorithms
therefore need to store all samples in the computer memory during the computational process.
In this work, we
investigate online stochastic DCA for the general problem (1) to deal with fast streaming data where we do not need to
store samples all the time. Furthermore, thanks to the online mechanism, our proposed algorithms have the adaptive
ability which is a great advantage over the SDCA schemes proposed in [21]. Numerical experiments will justify this
claim.

Paper’s contribution. We design three new online stochastic DCA schemes for solving the generic problem (1) (which
will be described in more details in section 2.3). The problem is very general in such a way that both DC components
are nonsmooth. Besides, we will see that the assumptions used are mild that make the considered problem cover
a very large class of real-world applications. Since the update steps of the proposed algorithms require new fresh

2

Online Stochastic DCA with applications to Principal Component Analysis

samples from the distribution of Z, we refer to our algorithms as online stochastic DCA (osDCA in short). The
ﬁrst osDCA scheme constructs stochastic approximations (SA) for both values of G and subgradients of H. The
convergence analysis of the proposed algorithm is rigorously studied. It turns out that the subsequential convergence
to critical points with probability one is guaranteed. Although we only consider the same random vector inside both
DC components for simplicity of presentation, the proposed algorithm and its convergence analysis can be extended
E(h(w, ˜Z)), where Z and ˜Z are two different random
to a more general setting which is F (w) = E(g(w, Z))
−
vectors. The extension aims to handle optimization problems involving with two parallel streams of data. Next, in the
second and the third algorithms, we consider two scenarios where the values of G and the subgradients of H can be
directly computed, respectively. The subsequential convergence to DC critical points is also established with milder
assumptions than those of the ﬁrst osDCA scheme. In three proposed algorithms, we require the number of samples
used at each iteration to increase at a certain rate. This rate in the latter two algorithms is better than the ﬁrst one. In
addition, in the second scheme, this rate can be speciﬁed in advance without the knowledge on the complexity of a
family of functions associated with g. The proposed osDCA schemes enjoy a double beneﬁt of an online algorithms:
they are suitable to perform streaming data which come from an unknown distribution. Moreover, we discuss several
contexts where one can formulate some classes of stochastic as well as deterministic programs into the form of (1).

Finally, based on our proposed algorithms, we design two speciﬁc schemes for solving the expected problem of
principal component analysis. Numerical experiments have been conducted carefully to study the proposed algorithms’
behaviors in different aspects.

2 Preliminaries

2.1 Outline of DC programming and DCA

In this subsection, we brieﬂy introduce DC programming and DCA. Let Γ0(Rn) denote the convex cone of all lower
semicontinuous proper convex functions on Rn. The standard DC program takes the form

f (x) = g(x)
Γ0(Rn). Such a function f is called DC, g

α := inf

{

h(x) : x

−

Rn

}

∈

(Pdc),

∈

where g, h
of f . Note that, a DC program with closed convex constraint x
program in such a way that f = (g + χC )
For a convex function θ deﬁned on Rn and a convex set C, the modulus of strong convexity of θ on C, denoted by
ρ(θ, C) or ρ(θ) if C = Rn, is given by

h is DC decomposition, while g and h are DC components
C can be equivalently written as a standard DC

∈
h, where χC is the indicator function of C.

−

−

0 : θ
−
Moreover, a function θ is said to be strongly convex on C if ρ(θ, C) > 0. The subdifferential of θ at x0 ∈
denoted by ∂θ(x0), is deﬁned by

ρ(θ, C) = sup
{

(µ/2)

k · k

≥

µ

}

.

2 is convex on C

dom θ,

Rn : θ(x)

y

{

∈

∈

≥

θ(x0) +

∂θ(x0) =

x
h
Γ0(Rn) is deﬁned by θ∗(y) = sup
The conjugate function θ∗ of θ
A point x∗ is called a critical point, or a generalized Karush-Kuhn-Tucker (KKT) point of (Pdc) if ∂g(x∗)
∩
= ∂h(x∗)
, or equivalently 0
⊂

=
∂g(x∗).
∂h(x∗), while it is called a strongly critical point of g
∅
DCA is based on local optimality conditions and duality in DC programming, which introduces the nice and elegant
concept of approximating a DC program by a sequence of convex ones: at each iteration k, DCA approximates the
second DC component h by its afﬁne minorization hk(x) = h(xk) +
∂h(xk), and then solves
the resulting convex subprogram to get xk+1. The standard DCA is formally described as follows.

, with yk
i

∀
∈
θ(x) : x

∂h(x∗)

∂g(x∗)

xk, yk

x0, y

x
h

x, y

h if

i −

Rn

,
i

{h

−

−

−

−

∅ 6

∈

∈

∈

x

}

}

.

.

Rn

Standard DCA.
Initialization: Let x0

dom ∂h and k = 0.

∈

repeat
Step 1: Compute the subgradient yk

∂h(xk).

∈

Step 2: Solve the following convex program

xk+1

arg min
{

∈

g(x)

−

hk(x) : x

X

.

}

∈

3

6
Online Stochastic DCA with applications to Principal Component Analysis

Step 3: k = k + 1.

until Stopping criterion.

Convergences properties of the standard DCA and its complete theoretical foundation in the DC programming frame-
generated by
work can be found in [27, 41, 39]. For instance, it is especially worth mentioning that the sequence
DCA has the following properties:

xk

}

{

1. The sequence

is decreasing.

(g
{
h)(xk+1) = (g

−

h)(xk)
}

−

2. If (g

−

iteration.

h)(xk), then xk and xk+1 are critical points of (Pdc) and DCA terminates at k-th

3. If ρ(g) + ρ(h) > 0 then the series

∞
k=1 k

xk+1

xk

k

−

2 converges.

4. If the optimal value α of the problem (Pdc) is ﬁnite and the sequences
P

every limit point ˜x of

is a critical point of g

xk

{

}

h.

−

xk

{

}

and

yk

{

}

are bounded, then

2.2 Some notions in probability theory

2.2.1 History of a stochastic process

Given a stochastic process
}
where σ(X1, X2, . . . , Xk) is the sigma algebra generated by random variables
increasing sigma algebras

∞
k=1, we deﬁne the history up to time k of
{

is called a ﬁltration.

Xk

=

X

{

by

X

P
X1, X2, . . . , Xk

k = σ(X1, X2, . . . , Xk),
. The sequence of

}

k

{P

}

2.2.2 Rademacher average

For a set of points

z1, z2, . . . , zl

{

}

:= zl in Ξ, the Rademacher average Rl(g, zl) is deﬁned as

where σ′
average of a family of functions

is are i.i.d. random numbers such that σi
Ξ
∈
}
Rl(g, Ξ) =

, z) : z

g(
·

{

l

1
l

Rl(g, zl) = Eσ sup
w∈S (cid:12)
(cid:12)
(cid:12)
with P(σi = 1) = P(σi =
(cid:12)
(cid:12)

σig(w, zi)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

∈ {±
, denoted by Rl(g, Ξ), is deﬁned as

i=1
X

}

1

,

sup
z1∈Ξ,z2∈Ξ,...,zl∈Ξ

Rl(g, zl).

1) = 1/2. The Rademacher

−

2.3 Online Stochastic DCA for solving (1)

This subsection develops osDCA schemes for solving the problem (1) which can be described as follows.

2.3.1 Problem setting

∈

Let PZ be the probability distribution of Z on Rn and Ξ = supp(PZ ) be the support of PZ . By deﬁnition, a point
Rn is in supp(PZ ) if PZ (Nx) > 0, for all neighborhood Nx of x. Since a measure “lives" in its support, we only
x
need to work in Ξ instead of Rn. For instance, a Dirac measure δa concentrating at a single point a admits a support
. A basic
containing only one point a; a discrete measure µ =
property of Ξ is that it is closed in Rn. Moreover, PZ (Ξ∁) = 0 since Rn is the topological Hausdoff space and PZ is
a Radon measure in Rn. Therefore, only the values of g and h on S
Ξ matter. For simplicity of presentation, we
. Here we use the
assume that dom g = dom h = S
∞
. Moreover, g and h are assumed to be Borel measurable. It is noted that the Borel
convention +
) = +
∞
∞ −
sigma algebra on R
. We assume that g(w, Z), h(w, Z) are
+
∪ {
∞}
S. Let G(w) = E(g(w, Z)) and H(w) = E(h(w, Z)), it follows that dom G = dom H = S.
integrable for all w
Besides, we assume that g(
Ξ, G and H are lower
·
semicontinuous, so that the problem (1) is DC. Moreover, we need some mild additional assumptions as follows.

∞
i=1 βiδai with βi > 0 admits a support

∞
is generated by the order topology of R

, z) are convex, lower semicontinuous, for all z

Ξ. That is, the value of g and h outside S

, z) and h(
·

a1, a2, . . .
}

Ξ is set to +

∪ {
∈

∞}

(+

P

+

×

×

×

∈

{

Assumption 1.

i. For all z

Ξ, dom ∂h(
·

∈

, z) = S.

ii. ¯ρ := ρH + inf z∈Ξ ρ(g(
·

, z)) > 0.

4

Online Stochastic DCA with applications to Principal Component Analysis

iii. There exists a Borel measurable selector τ such that

where τ is L2 uniformly bounded in the sense that there exists a Borel measurable function ˜τ such that ˜τ (Z)2
is integrable and

τ (w, z)

˜τ (z).

S, z

Ξ,

w

w

∀

∈

∈

S, z

Ξ, τ (w, z)

∂wh(w, z),

∈

iv. supw∈S |

F (w)
|

∀
< +

∈

.
∞

∈

k

k ≤

Remark 1. It is observed that the assumptions i), iii) and iv) are mild. On another hand, thanks to the regulariza-
tion technique introduced in [41], the assumption 1-(ii) is easily fulﬁlled by adding an L2 regularizer to both DC
components.

Assumption 2.

i. There exists a Borel measurable function ˜g : Rn

R such that ˜g(Z) is integrable and

→

ii. Rk(g, Ξ)

≤

Ng/kα with Ng > 0 and α > 0.

It is noteworthy that the assumption 2-(ii) holds for various cases described as follows [14].

g(w, z)

| ≤

|

˜g(z),

w

∀

∈

S, z

∈

Ξ.

Case 1. Holder functions g(
Ξ.
·
Let D be the length of a cube in Rm containing the compact set S. Suppose that

, z), z

∈

M, L > 0 and γ

∃

∈

(0, 1] such that

g(w, z)

g(x, z)

1.

2.

|

|

M,

w

| ≤

∀
g(y, z)

∈

S, z

L

x

−

−
| ≤
(0, 1/2), Rk(g, Ξ)

k

∈

Ξ,

y

γ,

x, y

∀

∈

k
Ng/kα, where

S, z

Ξ.

∈

≤

Then, for any α

∈

Ng = LDγm

γ
2 +

M √m
γ(1

−

2α)e

.

Case 2. Holder functions g(w,
Suppose that Ξ is compact, let D be the length of a cube in Rn that contains Ξ. Suppose that there exists M, L, γ > 0
such that

), w

S.

∈

·

p

g(w, z)

M,

w

S, z

Ξ.

| ≤

∀

∈

g(w, u)

g(w, v)

∈
u

L

1.

2.

|

|

| ≤

k
−
Ng/kα where Ng = M + LDγn

−

∈

∀

k

v

γ,

w

S, u, v

Ξ.

∈

γ

2 and α = γ/(2γ + n).

Then Rk(g, Ξ)

≤

Case 3. Discrete set Ξ.

Suppose that the number of elements of Ξ is ﬁnite, say
that

Ξ. Then, Rk(g, Ξ)

g(w, z)

S, z

M,

w

Ξ
|
|
M

= NΞ. Furthermore, assume that there exists M > 0 such

NΞ/k, hence, α = 1/2.

|

| ≤

∀

∈

∈

≤

It turns out that assumption 2 is not strong; hence a class of functions meeting the criteria is wide to cover many
p
problems arising in practice.
In three cases of Rademacher complexity presented above, though α in case 2 can
be very small in the high-dimension regime, which makes our next algorithm impractical, the other two cases have
α = 1/2 or arbitrarily near to 1/2, which are appropriate sample rates in practice.

It should be stressed that the Rademacher complexity measures the richness of a class of functions. Therefore, roughly
speaking, the function g must be quite “simple" in this Rademacher sense. This criterion naturally fulﬁlls our demand
as we want to control the variability of stochastic approximations made on g.

2.3.2 Online Stochastic DCA schemes

We now introduce an osDCA scheme described in algorithm 1.

The algorithm 1 is well deﬁned with probability 1. To be more speciﬁc, the set of events that makes algo-
rithm 1 work is
k =
∈
∞
σ(Z0, Z1, . . . , Zk−1, w0, w1, . . . , wk). We observe that
k=0 is an adapted
∞
k=0. The convergence results of algorithm 1 are presented in theorem 1.
process with respect to the ﬁltration

) = 1. We denote Zk = Zk,1:nk and

∞
k=0 is a predictable process and

Ξ) and hence P(
V

nk
i=1 (Zk,i

∞
k=1 ∩

wk

tk

=

P

∩

V

}

{

}

{

{P

k+1}

5

Online Stochastic DCA with applications to Principal Component Analysis

Algorithm 1 Online Stochastic DCA

Initialization: Choose w0
repeat

∈

S and a sequence of sample sizes

, set k = 0.

nk

{

}

1. Draw independently nk samples Zk,1, . . . , Zk,nk from the distribution of Z in such a way that they are also

independent of the past.
1
2. Compute tk =
nk

nk
i=1 τ (wk, Zk,i).

3. Solve the following convex program to get wk+1,

P

wk+1

∈

arg minw∈Rm

1
nk

(

nk

i=1
X

g(w, Zk,i)

tk, w

− h

.

i)

4. Set k = k + 1.
until Stopping criterion.

, if the sequence of sample sizes

α, 1

}

nk

{

}

satisﬁes

Theorem 1. Under assumptions 1 and 2, let β = min
{

, the iterations of algorithm 1 satisfy:

∞
k=1 n

−β
k < +

1. There exists F ∞ integrable such that F (wk)
P

F ∞ a.s.

→

∞

2.

∞
k=1 k

wk+1

wk

2 < +

∞
3. There exists a measurable set
critical point of F = G

H.

P

−

k

−

a.s.

L ⊂

Ω with P(

) = 1 such that for each ω

L

, every limit point of

∈ L

wk(ω)
}

{

is a

Proof. 1. Let ν(w) := E(τ (w, Z)). It follows from ν(wk)

H(wk+1)

H(wk) +

≥

∈
ν(wk), wk+1
h

∂H(wk) that
ρH
2 k

wk

−

+

i

wk+1

wk

2.

k

−

On the other hand, it follows from deﬁnition of wk+1 that

nk

g(wk, Zk,i)

1
nk

i=1
X
From (3) and (4), we obtain

1
nk

≥

nk

i=1
X

g(wk+1, Zk,i) +

tk, wk
h

−

wk+1

+

i

inf z∈Ξ ρ(g(
·

2

, z))

wk+1

k

wk

2.

k

−

with ¯ρ = ρH + inf z∈Ξ ρ(g(
·
E(F (wk+1)

1
nk

P

nk
i=1 g(wk+1, Zk,i)

≤

H(wk+1) +

1
nk
¯ρ
2 k

nk
i=1 g(wk, Zk,i)
tk
wk
h

2 +

H(wk)

wk+1
P

−

−

−

−
, z)). By taking conditional expectation with respect to
E(
tk
≤
h
g(wk+1, Zk,i)

ν(wk), wk+1

i|P
wk+1

G(wk+1)

F (wk)

k)
nk

wk

|P

−

−

k

k

P
k)

−
E(
k

¯ρ
2

|P

! −

1
nk

−

i=1
X

+ E

wk

2

k

|P

k).

−

By applying Schwartz inequality and Holder inequality,

E(
tk
−
h
By using AM-GM inequality, we obtain

ν(wk), wk+1

−

wk

k)

tk

E(
k

2

ν(wk)
k

|P

k)

−

≤

i|P

1

2 E(
k

wk+1

wk

2

k

|P

−

1
2 .

k)

ν(wk), wk+1

wk

,
i

−

k both sides of (5), we obtain

tk

E(
k

2

ν(wk)
k

k)

1

2 E(
k

−

≤
It follows from the independence of Zk,i and Zk,j for all i
nk

|P

|P

−

k

wk+1

wk

2

1
2

k)

1
2 ¯ρ

tk

E(
k
= j that

2

ν(wk)
k

|P

−

k) +

¯ρ
2

E(
k

wk+1

wk

2

k

|P

−

k).

(8)

E

tk

k

2

ν(wk)
k

k

|P

−

=

(cid:0)

(cid:1)

1
n2
k

τ (wk, Zk,i)

2

ν(wk)
k

k

|P

−

.

(cid:1)

E

k
(cid:0)

i=1
X

6

(3)

(4)

(5)

(6)

(7)

 
6
Online Stochastic DCA with applications to Principal Component Analysis

We observe that

Therefore,

E

τ (wk, Zk,i)

k
= EZ (
(cid:0)
k
= EZ

−
τ (wk, Z)
k
τ (wk, Z)
k

2

ν(wk)
k
k
|P
ν(wk)
2) +
(cid:1)
k
ν(wk)
2
k

− k

k

2

k
(cid:0)

2

E(τ (wk, Zk,i)
−
h
2 = VZ(τ (wk, Z)).

k), ν(wk)
i

|P

(cid:1)

tk

2

ν(wk)
k

k

|P

−

E

k
(cid:0)

=

1
nk

(cid:1)

VZ (τ (wk, Z)).

(9)

From (6), (7), (8), and (9) we obtain

E(F (wk+1)

F (wk)

|P

k)

≤

−

VZ(τ (wk, Z))

2 ¯ρ

×

nk

+ E

G(wk+1)

1
nk

−

nk

i=1
X

g(wk+1, Zk,i)

|P

.

k

!

(10)

Next, we make an upper bound on the right-hand side of (10). Firstly, the (nonnegative) term VZ(τ (wk, Z)) is bounded
above by E(˜τ (Z)2). Secondly, we show that

i=1
X
To prove (11), let us ﬁrst introduce “ghost samples" Z ′
independent of all Zk,i and identically distributed with Z. By Jensen’s inequality, we get

k,1, Z ′

k,nk

nk

G(w)

1
nk

−

E

sup
w∈S (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

! ≤

g(w, Zk,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k,2, . . . , Z ′
(cid:12)

2Rnk (g, Ξ).

(11)

(similar to the arguments in [6]) that are

1
nk

nk

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Therefore,

g(w, Zk,i)

−

E

≤

E(g(w, Z))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
nk

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nk

g(w, Zk,i)

i=1
X

(cid:0)

−

g(w, Z ′

k,i)

(cid:12)
(cid:12)
(cid:1)
(cid:12)
(cid:12)
(cid:12)

Zk,i, i = 1, nk

|

.

!

nk

1
nk

E

g(w, Zk,i)

sup
w∈S (cid:12)
(cid:12)
(cid:12)
(cid:12)
Now let σ1, σ2, . . . , σnk be independent random variables with P(σi = 1) = P(σi =
(cid:12)
are also independent of Zk,i and Z ′

E(g(w, Z))
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
w∈S (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

g(w, Zk,i)

! ≤

i=1
X

i=1
X

−

E

k,i. Then,

nk

1
nk

nk

1
nk

.

g(w, Z ′

k,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
2 in such a way that they

!

i=1
X
1) = 1

−

−

E

1
nk

sup
w∈S (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
w∈S (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
sup
w∈S

2E

= E

≤

= 2E

Eσ

nk

i=1
X
1
nk

i=1
X
nk

1
nk (cid:12)
i=1
(cid:12)
X
(cid:12)
1
(cid:12)
(cid:12)
sup
nk (cid:12)
w∈S
(cid:12)
(cid:12)
(cid:12)
≤
(cid:12)

= 2E(Rnk (g, Z nk ))

nk

g(w, Zk,i)

1
nk

−

nk

i=1
X

g(w, Z ′

σi(g(w, Zk,i)

g(w, Z ′

−

!

k,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
k,i))
(cid:12)
!
(cid:12)
(cid:12)
(cid:12)
(cid:12)

σig(w, Zk,i)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

nk

!

σig(w, Zk,i)
(cid:12)
i=1
(cid:12)
X
(cid:12)
2E(Rnk (g, Ξ)) = 2Rnk (g, Ξ).
(cid:12)
(cid:12)

!!

Now, we establish the almost sure convergence of the sequence
that there exists R such that F (w)

S. Let D(w) = F (w)

R,

w

{

≥

∀

∈

F (wk)
}
R
≥

−

as follows. The assumption 1-(iv) implies
0 and Sk = [E(D(wk+1)
k) >

D(wk)

−

|P

7

 
 
 
 
 
 
 
 
 
Online Stochastic DCA with applications to Principal Component Analysis

0]. Since Sk is

P

k-measurable and by using (10), (11) , we obtain

∞

E(1Sk (D(wk+1)

k=1
X

∞

D(wk)))

−

=

≤

≤

(cid:0)
∞

k=1
X
1
2 ¯ρ

k=1
X
E(˜τ (Z)2)
2 ¯ρ

E

E(1Sk (D(wk+1)

D(wk))

|P

k)

−

E(VZ (τ (wk, Z)))
nk

∞

(cid:1)
Rnk (g, Ξ)

+ 2

∞

k=1
X

1
nk

+ 2Ng

k=1
X
1
nα
k

< +

.
∞

∞

k=1
X

It follows from semimartingale convergence theorem [35] that there exists D∞ integrable such that D(wk)
a.s., which implies F (wk)

F ∞ = D∞ + R a.s.

D∞

→

2. By applying AM-GM inequality, we obtain

→

tk
h

−

ν(wk), wk+1

wk

−

i ≤

1
¯ρ k

tk

ν(wk)
k

2 +

−

¯ρ
4 k

wk+1

wk

2.

k

−

Combining this inequality with (5), we get

¯ρ
4 k

1
nk

−

nk

i=1
X

wk+1

wk

2

k

≤

F (wk)

−

F (wk+1) + G(wk+1)

G(wk)

−

g(wk+1, Zk,i) +

nk

g(wk, Zk,i) +

1
¯ρ k

tk

ν(wk)
k

2.

−

−
1
nk

i=1
X

By applying Lebesgue dominated convergence theorem (theorem 4.2, [7]) and noticing that

we get

E

E

1
nk

nk

i=1
X

g(wk, Zk,i)

G(wk)

|P

−

k

!!

= 0,

∞

k=1
X
∞

E

¯ρ
4

+

M
¯ρ

k=1
X

wk

k

−

wk+1

2

k

! ≤

E(F (w1))

E(F ∞)

−

1
nk

+ 2Ng

∞

k=1
X

1
nα
k

<

.
∞

Therefore,

−
3. We denote Gk(w) = 1
P
nk

∞
k=1 k

wk

wk+1

2 < +

a.s.

∞

k
nk
i=1 g(w, Zk,i), it follows from tk

wk+1, tk
i
h
ν(wk), wk
h
Together with the following inequalities

P

= Gk(wk+1) + G∗

k(tk),

= H(wk) + H ∗(ν(wk)).

i

we obtain

H(wk+1)
Gk(wk+1)

≥

H(wk) +
tk, wk+1

ν(wk), wk+1
h
Gk(wk)

−

wk
,
i
tk, wk

− h

i ≤

− h

,
i

∂Gk(wk+1) and ν(wk)

∂H(wk) that

∈

∈

Gk(wk)

H(wk)

−
Gk(wk+1)

≥

H ∗(ν(wk))
tk
h

H(wk+1) +

−

−

−

G∗
k(tk) +
ν(wk), wk

ν(wk), wk

tk
h

−

−
wk+1

,
i

i

which implies

≥

Gk(wk)

H(wk)

−

−

H ∗(ν(wk)) + G∗

k(tk)

0,

→

(12)

8

 
 
 
Online Stochastic DCA with applications to Principal Component Analysis

H(wk)

since Gk(wk)
−
Hence, (12) implies G(wk) + G∗
It is observed that

→

F ∞, tk

−
k(tk)

− h

ν(wk)

0, and Gk(wk+1)

→
wk, ν(wk)

0.

i →

H(wk+1)

F ∞.

→

−

(cid:12)
(cid:12)
(cid:12)
(cid:12)

sup
x∈S{h
(cid:12)
(cid:12)
(cid:12)
(cid:12)

G∗

k(tk)

|

G∗(tk)
|

−

=

x, tk

Gk(x)

} −

i −

x, tk

sup
x∈S{h

G(x)
}

i −

sup
x∈S |

≤

Gk(x)

−

G(x)

| →

0.

L

− h

i →

, we have

0 a.s. Now let

wk(ω)
, there exists a subsequence
{
}
if necessary, we can assume that ν(wkj (ω))
w∗, ν∗

Hence, we obtain G(wk) + G∗(tk)
wk, ν(wk)
1 gained from all almost surely true statements from the beginning of the proof, we have P(
at most countably ﬁnite statements. Let ω
limit point of
ν(wkj (ω))
}
{
G∗(tkj (ω))
. By letting j
→
i
→ h
. On the other hand, according to Young’s inequality, G(w∗) + G∗(ν∗)
w∗, ν∗
obtain G(w∗) + G∗(ν∗)
i
. Thefore, G(w∗) + G∗(ν∗) =
w∗, ν∗
h
i
it follows from ν(wkj (ω))
ν∗, w
H(w∗) +
H(w)
h
≥
∂H(w∗)
since ∂G(w∗)

be an intersection of sets with probability
) = 1 since there are
S be a
w∗. By extracting a subsequence of
ν∗. Therefore, G(wkj (ω))+
and noting that θ(w, z) = G(w) + G∗(z) is lower semicontinuous, we

∂G(w∗). Furthermore, for each w
. In other words, ν∗
i
ν(wkj (ω)), w
h
∂H(w∗), and we conclude that w∗ is a critical point of F = G

∞
w∗, ν∗
h
∂H(wkj (ω)) that H(w)
. Therefore, ν∗
i

≥
S,
wkj (ω)
, which implies
i
H

ν(wk(ω))
and
}
{
such that wkj (ω)
ν∗, which implies tkj (ω)

{
wkj (ω)
}
→

are bounded. Let w∗

∈
H(wkj (ω)) +

wk(ω)
}

∈
w∗
.

∈ L

≤ h

−
=

→

→

−

+

≥

−

L

∈

∈

∈

{

∩

∅

Remark 2. (i) The algorithm only uses samples at the current time to update the solution (past samples are no longer
used). Therefore, even if the distribution of Z changes at a certain time (suppose that, due to some real-world events,
Z becomes Z ′ at the iteration k), the algorithm will automatically solve the problem (1) with Z being replaced by Z ′.
Indeed, the current solution wk can be considered as the initial point for restart, the algorithm continues operating
based on new samples from the distribution of Z ′. The theorem 1 is still valid, and the subsequential convergence with
probability one to DC critical points of the DC problem associated with the new distribution is guaranteed. This is
indeed an advantage of the osDCA. In contrast, intuitively, stochastic algorithms using aggregated update (still using
old samples to compute the current solution) barely have this kind of adaptivity. We will conduct numerical experi-
ments to study this aspect.
(ii) Our algorithm and the convergence analysis can be extended to deal with the more general problem whose
the random variables inside the ﬁrst and the second DC components are not necessarily the same, i.e., F (w) =
E(h(w, ˜Z)). With this new setting, at the iteration k, we approximate values of G and the subgradients
E(g(w, Z))
of H by using nk independent random samples obtained from the distribution of Z and ˜nk independent random sam-
ples obtained from the distribution of ˜Z, respectively. The sample size sequences
need to increase in
∞
such a way that

and

and

˜nk

nk

−

∞

{

}

{

}

k=1 n−α

k <

∞

k=1 ˜n−1

k <

.
∞

P

Next, we will discuss two scenarios where one can directly compute (without stochastically approximation) values of
G or subgradients of H. Since the information of G (resp. subgradient of H) can be achieved, we will modify the
algorithm 1 to exploit this advantage. Note that these two schemes are not special cases of the algorithm 1, but they
will coincide with the algorithm 1 in some cases.

P

The values of G can be directly computed without approximation In this case, G does not need to be stochas-
tically approximated, we replace the approximation of G in step 3 of algorithm 1 by its true value, which results in
algorithm 2.

Algorithm 2 Online Stochastic DCA with exact G
Similar to algorithm 1, where step 3 of algorithm 1 is replaced by the following step:

3. Solve the following convex program to get wk+1,

wk+1

∈

arg minw∈Rm

G(w)

(cid:8)

tk, w

− h

.

i
(cid:9)

With this algorithm, we obtain stronger convergence results since G is computed exactly. Note that, in the convergence
results of algorithm 1, we impose the assumption 2 in order to control the variance of the stochastic estimator of G.
To study the convergence of algorithm 2, we do not need such an assumption. Furthermore, in the assumption 1, we
replace the convexity condition ρH + inf z∈Ξ ρ(g(
, z)) > 0 by the weaker one ρH + ρG > 0, which gives rise to a
·
milder assumption called the assumption 1’. We obtain the convergence theorem 2 whose proof is similar to the proof
of theorem 1.

9

6
Online Stochastic DCA with applications to Principal Component Analysis

Theorem 2. Under the assumption 1’, if the sequence of sample sizes
iterations of algorithm 2 satisfy:
1. There exists F ∞ integrable such that F (wk)

F ∞ a.s.

→

2.

∞
k=1 k

wk+1

wk

2 < +

∞
3. There exists a measurable set
critical point of F = G

H.

P

−

k

−

a.s.

L ⊂

Ω with P(

) = 1 such that for each ω

L

nk

{

}

satisﬁes

∞

k=1 n−1

k < +

, then the

∞

P

, every limit point of

∈ L

wk(ω)
}

{

is a

The subgradients of H can be directly computed without approximation In this case, we replace the stochastic
estimator of the subgradient of H in the algorithm 1 by the true subgradient of H to obtain the following algorithm.

Algorithm 3 Online Stochastic DCA with exact subgradients of H
Similar to algorithm 1, where step 2 of algorithm 1 is replaced by the following step:

2. Compute tk

∂H(wk).

∈

Since we work directly on H, we replace assumption 1-(i) by dom ∂H = S. Likewise, the assumption 1-(iii) is
replaced by the following:

there exist M > 0 such that

w

∀

S,

t
∀

∈

∈

∂H(w) :

t

k

k ≤

M.

These modiﬁcations bring about a new set of assumptions called assumption 1”. We obtain the following convergence
results whose proof is similar to the proof of algorithm 1.

Theorem 3. Under assumptions 1” and 2, if the sequence of sample sizes
iterations of algorithm 3 satisfy:
1. There exists F ∞ integrable such that F (wk)

F ∞ a.s.

→

nk

{

}

satisﬁes

P

∞

k=1 n−α

k < +

, the

∞

2.

∞
k=1 k

wk+1

wk

2 < +

a.s.

k

−

P

H.

L ⊂

Ω with P(

∞
3. There exists a measurable set
critical point of F = G
Remark 3. (i) When g(w, z) does not depend on z, algorithm 2 coincides with algorithm 1; likewise, when h(w, z)
does not depend on z, algorithm 3 and algorithm 1 coincide. It is worth noting that, in practice, thanks to the ﬂexibility
of DC decompositions, one can usually formulate the given stochastic problem as a stochastic DC program with one
stochastic DC component and one deterministic DC component. For example, we consider F (w) = E(f (w, Z)). If the
Ξ. Then, F has the following DC decomposition:
functions f (
·

, z) are L-smooth with the same constant L for all z

) = 1 such that for each ω

, every limit point of

wk(ω)
}

∈ L

is a

−

L

∈

{

F (w) =

L
2 k

w

2

k

−

E

G(w)

L
2 k

w

2

k

−

(cid:18)

f (w, Z)

.

(cid:19)

H(w)

In another case, suppose that there exists a convex function ϕ(w) such that functions f (w, z)+ ϕ(w) are convex for all
z

, z) are weakly convex), F has the following DC decomposition:

w

{z

}

|

Ξ (in particular, when ϕ(w) = (κ/2)
k

∈

| {z }
2, f (
·

k

F (w) = E (f (w, Z) + ϕ(w))

ϕ(w)

.

−

H(w)

G(w)

{z
(ii) In big data analytics, large-sum problems play a key role. We consider the following large-sum objective function

| {z }

}

|

N

N

N

F (w) =

αifi(w) =

αigi(w)

αihi(w),

−

i=1
X
0 for all i = 1, N and

i=1
X
N
where gi, hi are convex, αi
i=1 αi = 1. The function F can be rewritten as F (w) =
E(gI (w))
E(hI (w)), where I is a random index with P(I = i) = αi. In this case, the distribution of I is known
P
completely. However, as N can be very large, we may still need to apply osDCA schemes. Furthermore, since I is
known, we have full freedom to choose algorithm 1, algorithm 2, or algorithm 3 to apply, which leads to - in general
- three distinctive algorithms. The practical trade-off between these algorithms would be which DC component (or
none of them) is cheaper to be computed directly.

i=1
X

≥

−

10

Online Stochastic DCA with applications to Principal Component Analysis

3 Applications: solving the Expected PCA

Principal component analysis (PCA) is arguably one of the most successful tools for dimensionality reduction. In this
section, we will apply osDCA schemes to the expected problem of PCA to study the generalization capacity of the
proposed methods.

3.1 osDCA schemes for solving Expected PCA

We consider the following expected problem of PCA (denoted by E-PCA) as follows [36],

E(
w, Z
h
where Z is a normalized random vector, i.e.
interested is that the data obtained online.

min

−

k

2),
i
Z

k

1
2

subject to

w

k

k ≤

1,

(E-PCA)

= 1, with unknown distribution. The situation in which we are

The problem (E-PCA) can be considered as the theoretical problem of the classic PCA (and - vice versa - the classic
PCA is the empirical problem of (E-PCA)). In other words, the problem (E-PCA) aims to generalize the compressing
capacity of the classical PCA on unseen data.

Firstly, we observe that the problem (E-PCA) is nonconvex and it can be formulated as a DC problem,

minimize
w∈S

G(w)

−

H(w),

(13)

w

2, H(w) = E

where G(w) = λ
and λ > 0. Although we
w
have a very natural DC decomposition with G(w) = 0, H(w) = E
2 to both DC
(cid:1)
components to fulﬁll to assumption 1-(ii). Since the values G are directly obtained without approximation, algorithm
1 coincides with algorithm 2. We call this scheme osDCA-1, where the k-th iteration is described as follows.

k ≤
}
, here we add λ

Rm :
1
w, Z
2 h

2 + 1
2 h

w
k
2
i

, S =

λ
2 k

w, Z

2
i

2 k

2 k

w

w

∈

k

k

{

k

1

(cid:0)

(cid:1)

(cid:0)

1. Receive nk samples Zk,1, . . . , Zk,nk .
2. Compute tk = λwk + 1
nk
λ−1tk if
P
tk
tk/

3. Update wk+1 =

nk
i=1 h
tk

λ
k
k ≤
otherwise.

wk, Zk,i

(cid:26)

k

k

Zk,i.
i

Secondly, it is well-known that if a function θ has L-Lipschitz continuous gradient, then (L/2)
−
2 + θ are convex. Therefore, we have another DC decomposition for the problem (E-PCA) as follows,

k · k

2

θ and (L/2)

k ·

k

minimize
w∈S

G(w)

−

H(w).

(14)

where

G(w) = E

H(w) = E

(cid:18)

L
2 k
L
2 k

w

2

k

−

2 +

w

k

1
2 h
1
2 h

w, Z

w, Z

2
i

2
i

(cid:19)

,

.

(cid:19)

(cid:18)

Since G, H remains unknown, we apply algorithm 1 for this DC problem. Obviously the family
= 1
}
is uniformly Lipschitz and uniformly bounded by a constant, therefore, the rate α in assumption 2 can be chosen
arbitrarily in (0, 1/2). With this setup, we obtain a second scheme called osDCA-2 whose the k-th iteration is described
as follows.

g(
·

, z) :

k

k

{

z

1. Receive nk samples Zk,1, Zk,2, . . . , Zk,nk .
2. Compute the stochastic gradient

tk = Lwk +

1
nk

nk

i=1
X

wk, Zk,i
h

Zk,i.
i

3. Solve the following convex program to get wk+1,

minimize

w∈S (

L
2 k

w

2

k

−

1
2nk

nk

i=1
X

w, Zk,i
h

2
i

tk, w

− h

.

i)

(15)

11

Online Stochastic DCA with applications to Principal Component Analysis

The problem (15) is convex and can be solved by existing convex optimization packages. However, we solve it by
DCA since it has the following “false" DC decomposition

˜g(w) =

L
2 k

w

k

2, ˜h(w) =

1
2nk

nk

i=1
X

w, Zk,i
h

2 +
i

tk, w
h

,
i

which results in a simple DCA scheme where convex subproblems have closed-form solutions. The (deterministic)
DCA takes the current solution u0 = wk as the initial point, then operates until the stopping criterion which is
ul+1

< ǫ is met, where ǫ > 0 is the error tolerance.

ul

k

−

k

3.2 Numerical experiments

3.2.1 Datasets

The numerical experiments are conducted on standard machine learning datasets on LIBSVM 2. The information of
the used datasets is described in Table 1. The samples of each dataset are normalized as

= 1.

zi

k

k

Dataset

# Features

letter
YearPredictionMSD
SensIT Vehicle
shuttle

16
90
100
9

# Train
set
15000
463715
78823
43500

# Validation

set
5000
51630
19705
14500

Table 1: Datasets’ information

Furthermore, to test the adaptive ability of osDCA schemes, we generate a synthetic dataset that consists of two
500), validation
subdatasets (training set (200000
set (200000

500)), in which the generating mechanism is described in subsection 3.2.3.

500)) and (training set (200000

500), validation set (500000

×

×

×

×

3.2.2 Comparative algorithms

We compare our algorithms with two versions of Projected Stochastic Subgradient method (PSS) [12] - an online algo-
rithm for weakly convex objective functions, and four Stochastic DCA schemes (SDCA) [21] proposed for nonconvex,
nonsmooth DC programs.

3.2.3 Experiment setup and results

The numerical experiments comprise of four parts. The ﬁrst experiment is the comparative experiment between the
proposed algorithms with two versions of PSS and four SDCA schemes, the second experiment studies our algorithms’
behaviors when the DC decomposition of the problem varies, the third experiment compares between convex solvers
for solving subproblems, and the fourth experiment studies the adaptive capacity of osDCA schemes.

{

}

0.001, 0.005, 0.01, 0.015, 0.02

In the ﬁrst experiment, we compare osDCA schemes with two versions of PSS (constant stepsize policy and dimin-
ishing stepsize policy) and four SDCA schemes. Firstly, we ran the PSS with many different stepsizes and observed
its performance in order to choose a proper range to ﬁnd a good stepsize. We then ran the PSS with the constant
and found that the stepsize 0.005 consistently gives good performance on
stepsize in
four validation sets. About the diminishing stepsize αk = c/k, we ran PSS with c being chosen in
4, 5, . . . , 11, 12
}
and found that c = 8 achieves good performance on all four datasets. For the four SDCA schemes, it should be
stressed that SDCA1 and SDCA3 require the ﬁrst DC component of the objective to be explicitly deﬁned, meanwhile,
SDCA2 and SDCA4 can handle the unknown ﬁrst DC component. Therefore, we apply SDCA1 and SDCA3 to (13)
with λ = 10−6 that yields good results; meanwhile, SDCA2 and SDCA4 are applied to (14) where L = 1. We use
the sequence of equal weights for all four SDCA schemes. On the other hand, based on the theoretical analysis, the
parameters of osDCA schemes are chosen as follows. For the osDCA-1, we choose the sequence of sample sizes as
nk = k2, and λ = 1 which is a neutral number and results in a good performance over four datasets. For the osDCA-2,
the sequence of sample sizes is chosen as nk = k3, the Lipschitz smoothness constant L = 1 and the tolerance error
in solving subproblems ǫ = 10−5.

{

2The datasets can be downloaded from https://www.csie.ntu.edu.tw/~cjlin/libsvm/.

12

Online Stochastic DCA with applications to Principal Component Analysis

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

osDCA-1
SDCA1
SDCA3
PSS with diminishing stepsize
PSS with constant stepsize

2

4

6

8

10

Time (s)

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

osDCA-1
SDCA1
SDCA3
PSS with diminishing stepsize
PSS with constant stepsize

0.1

0.2

0.3
Time (s)

0.4

0.5

0.6

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

10-5

0

osDCA-1
SDCA1
SDCA3
PSS with diminishing stepsize
PSS with constant stepsize

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

osDCA-1
SDCA1
SDCA3
PSS with diminishing stepsize
PSS with constant stepsize

0.05

0.1
Time (s)

0.15

0.2

10-5

0

5

10

15

20

25

Time (s)

(a) SensIT Vehicle

(b) shuttle

(c) letter

(d) YearPredictionMSD

Figure 1: The performance of osDCA-1 compared with SDCA1, SDCA3 and two versions of PSS.

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

osDCA-2
SDCA2
SDCA4
PSS with diminishing stepsize
PSS with constant stepsize

5

10

15

Time (s)

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

(a) SensIT Vehicle

osDCA-2
SDCA2
SDCA4
PSS with diminishing stepsize
PSS with constant stepsize

0.1

0.2

0.3

0.4

0.5

Time (s)

(b) shuttle

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

10-5

0

osDCA-2
SDCA2
SDCA4
PSS with diminishing stepsize
PSS with constant stepsize

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

osDCA-2
SDCA2
SDCA4
PSS with diminishing stepsize
PSS with constant stepsize

0.05

0.1
Time (s)

0.15

0.2

10-5

0

5

10

15
Time (s)

20

25

30

(c) letter

(d) YearPredictionMSD

Figure 2: The performance of osDCA-2 compared with SDCA2, SDCA4 and two versions of PSS.

As a preprocessing step, each training dataset is randomly shufﬂed before each run. Then, the mentioned algorithms
perform one pass through each training dataset and automatically terminate when the training dataset is used up. The
starting points are also randomly initialized in S. The performance of our algorithms are measured on the validation
set to guarantee their generalization capability. To enhance visualization, we ﬁrst ﬁnd the “optimal solution" w∗ on
F (w∗) (under the
the validation set by running deterministic DCA. We then report the suboptimality graph F (wn)
log-scale) averaging over 20 runs. Furthermore, we classify osDCA-1, SDCA1, SDCA3 in one group and osDCA-2,
SDCA2, SDCA4 in another group (since the former three use the DC decomposition (13) and the latter three use (14))
to plot them in two different ﬁgures.

−

All experiments are performed on a PC Intel(R) Core(TM) i7-8700 CPU @3.20GHz of 16 GB RAM.

Figures 1 and 2 illustrate the performance of osDCA schemes compared with SDCA schemes and the PSS with
constant stepsize and diminishing stepsize.

Comparisons between osDCA schemes and PSS. Our algorithms take a very short amount of time to pass through
the training sets while obtaining really small suboptimality values, say 10−4
10−5. In contrast, the PSS with
constant stepsize struggles to reach the optimal solution and exhibits the well-known ﬂuctuation behavior with the
10−4. On the other hand, PSS with diminishing stepsize performs very well
suboptimality varying around 10−3
and obtains similar suboptimality as osDCA schemes, where the differences (i.e., Fval(wpss)
Fval(wosdca), where
Fval is the objective function measured on the validation set, wpss and wosdca are solutions found by PSS and osDCA,
10−6 (resp. from
respectively) between this PSS and osDCA-1 (resp. osDCA-2) range from
32.3)

10−6 to
10−6). To obtain this result, osDCA-1 (resp. osDCA-2) is 2.7

1.45
18.4 (resp. 1.7

10−7 to 5.89

3.84

6.18

∼

∼

−

×

×

−

−
∼

∼

−
×
times faster the PSS with diminishing stepsize.

×

Comparisons between osDCA and SDCA. The differences (i.e., Fval(wsdca)
Fval(wosdca)) between SDCA1 (resp.
10−6). Wall-clock
10−5 to
SDCA3) and osDCA-1 vary from
2.23
2.44
−
×
×
time for osDCA-1 to perform one pass through the training datasets is 2.7
14.5) times shorter than
18.5 (6.1
SDCA1 (resp. SDCA3). The differences (i.e., Fval(wsdca)
Fval(wosdca)) between SDCA2 (resp. SDCA4) and osDCA-
10−7 to 5.53
10−7). Moreover, osDCA-2 makes one pass
2 are from
7.81
×
through the training datasets 5.5
17.6) times faster than SDCA2 (resp. SDCA4). We also observe
that, at the time osDCA schemes terminate, they usually obtain smaller optimality values than SDCA schemes.

10−4 (resp.
−
24.3 (resp. 4.3

10−6 (resp.

−
10−5 to

10−5 to

−
2.25

2.44

8.95

1.04

×
∼

−
∼

×

×

−

−

×

∼

−

−

∼

×

Furthermore, it is well-known that there are two main factors needed to be carefully considered when designing any
DCA (or its variants), namely the DC decomposition of the problem and the convex solver for subproblems. There-
fore, we consider the following experiments to study our proposed algorithms’ behaviors within these two mentioned
perspectives.

13

Online Stochastic DCA with applications to Principal Component Analysis

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

 =20

 =15

 =10

 =5

 =1

 =0.5

 =0.001

=0

1

2
Time (s)

3

4

Figure 3: Performance (one run) of osDCA-1 when λ > 0 varies and when λ = 0

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

10-5

0

osDCA-2 with DCA
osDCA-2 with CPLEX

0.5

1

1.5

Time (s)

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

(a) SensIT Vehicle

osDCA-2 with DCA
osDCA-2 with CPLEX

0.05

0.1

0.15

0.2

0.25

Time (s)

(b) shuttle

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

10-5

0

osDCA-2 with DCA
osDCA-2 with CPLEX

osDCA-2 with DCA
osDCA-2 with CPLEX

100

10-1

10-2

10-3

10-4

y
t
i
l

a
m

i
t
p
o
b
u
S

0.1

0.2
Time (s)

0.3

0.4

10-5

0

1

2

3

4

5

6

Time (s)

(c) letter

(d) YearPredictionMSD

Figure 4: The performance (one run) of osDCA-2 with two different convex solvers: the DCA and CPLEX

k · k

In the second experiment, our aim is to study the behavior of osDCA-1 when λ varies (change the DC decomposition of
the problem). It is observed that, to surely fulﬁll the strong convexity condition ρG +ρH > 0, we add the regularization
2 to both G and H components. A natural question raised is that: suppose H is already strongly convex,
term λ
will we obtain some “optimal" performance if we do not use this regularization term? This curiosity motivates us
to perform the osDCA-1 scheme with DC decomposition g(w, z) = 0, h(w, z) = 1
2. Before presenting the
2 h
i
experimental results, let us discuss a little bit about the condition ρH > 0 in this case. We know that this condition
does not always hold and it is equivalent to E(ZZ ⊤) being positive deﬁnite. By deﬁnition, the positive deﬁniteness of
E(ZZ ⊤) is equivalent to E
= 0 such
= 0. Therefore, this condition is violated if there exists w0 6
> 0,
that E((w⊤
0 Z = 0 almost surely. In other words, the condition ρH > 0 does not hold if
there is a perfectly linear dependence between features of the random vector Z.

0 Z)2) = 0, or equivalently wT

(w⊤Z)2

w, z

w

∀

(cid:0)

(cid:1)

Figure 3 shows the behaviors of osDCA-1 with different λ > 0 and an extreme case where λ = 0 on the
YearPredictionMSD dataset. We observe that, the optimal performance of osDCA-1 is achieved at some moder-
ate values of λ, say, from 1 to 5. Besides, the quality of the performance is not monotone with respect to λ. With large
value of λ, osDCA-1 somehow gets stuck at the beginning. The performance of osDCA-1 is gradually improved as λ
decreases up to a certain value, and then the performance slightly deteriorates as λ continues to approach 0.

In the third experiment, we study the performance of osDCA-2 with different convex solvers for subproblems. To
be speciﬁc, beside the (deterministic) DCA used in the osDCA-2 scheme, we want to use the industrial CPLEX for
solving the convex subproblems. Figure 4 shows the difference between osDCA-2 using deterministic DCA and
CPLEX for solving convex subproblems. It is observed from the ﬁgure that while the suboptimality values of these
two algorithms are similar, osDCA-2 using DCA for the convex subproblem is faster than osDCA-2 with CPLEX.

In the last experiment, we study the adaptive capacity of osDCA schemes compared with SDCA schemes when there
is an abrupt change in the distribution of Z. We describe the context of the problem as follows. We are receiving
streaming data from an unknown distribution (the data is - in fact - realizations of Z). At a certain time, suppose
that there is a real-world event that makes the distribution of Z change (Z becomes some Z ′). We do not know this
event (and hence, the change of Z is also unknown to us) and continue to receive streaming data from the changed
distribution. From that time, we want to solve (1) with Z being replaced by Z ′ since the new random variable Z ′ is
more relevant than Z.

To this end, we generate a synthetic dataset as follows. The dataset consists of two subdatasets representing data
500) and a validation
collected before and after the abrupt change. The ﬁrst subdataset includes a training set (200000

×

14

6
Online Stochastic DCA with applications to Principal Component Analysis

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

10-6

0

osDCA-1
osDCA-2
SDCA1

100

y
t
i
l

a
m

i
t
p
o
b
u
S

10-2

10-4

SDCA2
SDCA3
SDCA4

1

2
Time (s)

3

4

10-6

0

200

400

600
Time (s)

800

1000

1200

Figure 5: The adaptive ability of osDCA schemes over SDCA schemes

×

×

500) and a validation set (200000

set (500000
500) that are generated from multivariate normal distribution with a mean vector 0 and a positive deﬁnite
covariance matrix. Then, we change the covariance matrix and generate the second subdataset consisting a training
set (200000
= 1. We concatenate two
training sets to create one uniﬁed training set in order to feed to the algorithms. Before the change, we measure the
performance of each algorithm on the ﬁrst validation set, and after the change, we use the second validation set. Figure
5 shows the average results of 20 runs, here we separate the results into two subﬁgures because the running times
of SDCA2, SDCA3, SDCA4 are remarkably longer than osDCA-1, osDCA-2, and SDCA1. The numerical results
conﬁrm the adaptive capacity of osDCA schemes over SDCA schemes.
Indeed, after the abrupt change, osDCA
schemes quickly regain suboptimality values that are as good as the ones obtained before the change. Meanwhile,
SDCA schemes barely adapt to the change and decrease the suboptimality slowly.

500). All data is then normalized as

zi

×

k

k

4 Conclusion

We have designed three online stochastic algorithms based on DCA to handle stochastic nonsmooth, nonconvex DC
programs. The ﬁrst scheme stochastically approximates both DC components; meanwhile, the other two are designed
for the context that one of two DC components can be directly computed. The theoretical properties of the proposed
algorithms are rigorously studied, and the almost sure convergence to critical points is established. As online stochastic
algorithms, the osDCA schemes gain a competitive edge when dealing with streaming data. The beneﬁts of osDCA
schemes include remedying storage burden and the ability to adapt to new changes of data distribution. On the other
hand, it is well-known that the variance of stochastic estimators of online stochastic algorithms is high, which creates
difﬁculties in the convergence analysis, especially in nonconvex and nonsmooth settings. Our algorithms’ convergence
results hold thanks to the increase of sample sizes. Moreover, the rate of this increase is determined based on the
Rademacher complexity of the family of functions
. Nevertheless, such complexity is not always
easy to compute. In future works, we would like to improve this condition and provide a better rate.

, z) : z

g(
·

Ξ
}

∈

{

On the other hand, to study the practical behaviors of the proposed algorithms, we conduct numerical experiments on
the expected problem of PCA. We consider streaming data that comes from an unknown distribution. The numerical
experiments justify the proposed algorithms’ efﬁciency. Indeed, the proposed osDCA schemes obtain good solutions
within a short time. In addition, the adaptive capacity of osDCA schemes have been conﬁrmed: after a change of
the data distribution, our algorithms quickly adapt to the new distribution. As a comparison, SDCA schemes do not
have this ability. Further experimental insights conﬁrm the importance of choosing the DC decomposition for the
considered problem and the convex solver for subproblems. It has been shown that the (deterministic) DCA is a very
efﬁcient and robust convex solver in our experiments.

References

[1] Dimitri P Bertsekas and John N Tsitsiklis. Gradient convergence in gradient methods with errors. SIAM J. Optim.,

10(3):627–642, 2000.

[2] Léon Bottou. Large-scale machine learning with stochastic gradient descent.

In Proceedings of COMP-

STAT’2010, pages 177–186. Springer, 2010.

[3] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information Processing Systems, volume 20, pages 161–168. Curran
Associates, Inc., 2008.

15

Online Stochastic DCA with applications to Principal Component Analysis

[4] Léon Bottou and Yann Cun. Large scale online learning.

In S. Thrun, L. Saul, and B. Schölkopf, editors,

Advances in Neural Information Processing Systems, volume 16, pages 217–224. MIT Press, 2004.

[5] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. SIAM

Rev., 60(2):223–311, 2018.

[6] Stéphane Boucheron, Olivier Bousquet, and Gábor Lugosi. Theory of classiﬁcation: a survey of some recent

advances. ESAIM Probab. Stat., 9:323–375, 2005.

[7] Haim Brezis. Functional analysis, Sobolev spaces and partial differential equations. Springer Science & Busi-

ness Media, 2010.

[8] Sven Brüggemann and Corrado Possieri. On the use of difference of log-sum-exp neural networks to solve

data-driven model predictive control tracking problems. IEEE Contr. Syst. Lett., 5(4):1267–1272, 2020.

[9] Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-Newton method for

large-scale optimization. SIAM J. Optim., 26(2):1008–1031, 2016.

[10] Giuseppe C Calaﬁore, Stéphane Gaubert, and Corrado Possieri. A universal approximation result for difference

of log-sum-exp neural networks. IEEE Trans. Neural Netw. Learn. Syst, 2020.

[11] Ronan Collobert, Fabian Sinz, Jason Weston, and Léon Bottou. Trading convexity for scalability. In Proceedings

of the 23rd international conference on Machine learning, pages 201–208, 2006.

[12] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions.

SIAM J. Optim., 29(1):207–239, 2019.

[13] Yuri Ermoliev. Stochastic quasigradient methods and their application to system optimization. Stochastics, 9:1–

36, 1983.

[14] Yuri M Ermoliev and Vladimir I Norkin. Sample average approximation method for compound stochastic opti-

mization problems. SIAM J. Optim., 23(4):2231–2263, 2013.

[15] Saeed Ghadimi and Guanghui Lan. Optimal stochastic approximation algorithms for strongly convex stochastic

composite optimization i: A generic algorithmic framework. SIAM J. Optim., 22(4):1469–1492, 2012.

[16] Saeed Ghadimi and Guanghui Lan. Stochastic ﬁrst-and zeroth-order methods for nonconvex stochastic program-

ming. SIAM J. Optim., 23(4):2341–2368, 2013.

[17] Ryuichi Kiryo, Gang Niu, Marthinus C du Plessis, and Masashi Sugiyama. Positive-unlabeled learning with
non-negative risk estimator. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30, pages 1675–1685. Curran
Associates, Inc., 2017.

[18] H. A. Le Thi, H. M. Le, V. V. Nguyen, and T. Pham Dinh. A DC programming approach for feature selection in

support vector machines learning. Adv Data Anal Classif, 2(3):259–278, 2008.

[19] H. A. Le Thi, M. Moeini, T. Pham Dinh, and J. Judice. A DC programming approach for solving the symmetric
eigenvalue complementarity problem. Computational Optimization and Applications, 51:21097–1117, 2012.

[20] Hoai An Le Thi. An efﬁcient algorithm for globally minimizing a quadratic function under convex quadratic

constraints. Math. Program., 87:401–426, 2000.

[21] Hoai An Le Thi, Van Ngai Huynh, Tao Pham Dinh, and Hoang Phuc Hau Luu. Stochastic difference-of-convex

algorithms for solving nonconvex optimization problems. arXiv preprint arXiv:1911.04334v2, 2020.

[22] Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, and Bach Tran. Stochastic DCA for the large-sum of non-
convex functions problem and its application to group variable selection in classiﬁcation. In Proceedings of the
34th International Conference on Machine Learning, pages 3394–3403, Sydney, Australia, 2017. PMLR.

[23] Hoai An Le Thi, Hoai Minh Le, Duy Nhat Phan, and Bach Tran. Stochastic DCA for minimizing a large sum of

DC functions with application to multi-class logistic regression. Neural Netw, 132:220–231, 2020.

[24] Hoai An Le Thi, Van Vinh Nguyen, and Samir Ouchani. Gene selection for cancer classiﬁcation using DCA.

Journal of Frontiers of Computer Science and Technology, 3(6):612–620, 2009.

[25] Hoai An Le Thi and Tao Pham Dinh. D.C. programming approach to the multidimensional scaling problem. In
Athanasios Migdalas, Panos M. Pardalos, and Peter Värbrand, editors, From Local to Global Optimization, pages
231–276. Springer US, 2001.

[26] Hoai An Le Thi and Tao Pham Dinh. Large-scale molecular optimization from distance matrices by a DC

optimization approach. SIAM J. Optim., 14(1):77–114, 2003.

16

Online Stochastic DCA with applications to Principal Component Analysis

[27] Hoai An Le Thi and Tao Pham Dinh. The DC (difference of convex functions) programming and DCA revisited
with DC models of real world nonconvex optimization problems. Ann. Oper. Res., 133(1-4):23–46, 2005.
[28] Hoai An Le Thi and Tao Pham Dinh. On solving linear complementarity problems by DC programming and

DCA. Comput. Optim. Appl., 50(3):507–524, 2011.

[29] Hoai An Le Thi and Tao Pham Dinh. DC programming and DCA: thirty years of developments. Math. Program.,
Special Issue dedicated to : DC Programming - Theory, Algorithms and Applications, 169(1):5–68, 2018.
[30] Hoai An Le Thi, Tao Pham Dinh, Hoai Minh Le, and Xuan Thanh Vo. DC approximation approaches for sparse

optimization. European J. Oper. Res., 244(1):26–46, 2015.

[31] Hoai An Le Thi and Duy Nhat Phan. DC programming and DCA for sparse ﬁsher linear discriminant analysis.

Neural Computing and Applications, 28(9):2809–2822, 2017.

[32] Junyi Liu, Ying Cui, Jong-Shi Pang, and Suvrajeet Sen. Two-stage stochastic programming with linearly bi-

parameterized quadratic recourse. SIAM J. Optim., 30(3):2530–2558, 2020.

[33] Julien Mairal. Stochastic majorization-minimization algorithms for large-scale optimization. Advances in Neural

Information Processing Systems, 26:2283–2291, 2013.

[34] Michael Metel and Akiko Takeda. Simple stochastic gradient methods for non-smooth non-convex regularized
In Proceedings of the 36th International Conference on Machine Learning, pages 4537–4545.

optimization.
PMLR, 2019.

[35] Michel Metivier. Semimartingales. Walter de Gruyter, 2011.
[36] Andrea Montanari and Emile Richard. Non-negative principal component analysis: Message passing algorithms

and sharp asymptotics. IEEE Trans. Inf. Theory, 62(3):1458–1484, 2015.

[37] Atsushi Nitanda and Taiji Suzuki. Stochastic Difference of Convex Algorithm and its Application to Training
Deep Boltzmann Machines. In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and
Statistics, pages 470–478, Fort Lauderdale, FL, USA, 2017. PMLR.

[38] Jong-Shi Pang, Meisam Razaviyayn, and Alberth Alvarado. Computing b-stationary points of nonsmooth DC

programs. Math. Oper. Res., 42(1):95–118, 2017.

[39] T. Pham Dinh and H. A. Le Thi. A DC optimization algorithm for solving the trust-region subproblem. SIAM

Journal of Optimization, 8(2):476–505, 1998.

[40] T. Pham Dinh and H. A. Le Thi. Recent advances in DC programming and DCA. Transactions on Computational

Collective Intelligence, 8342:1–37, 2014.

[41] Tao Pham Dinh and Hoai An Le Thi. Convex analysis approach to DC programming: theory, algorithms and

applications. Acta Math. Vietnam., 22(1):289–355, 1997.

[42] Meisam Razaviyayn, Maziar Sanjabi, and Zhi-Quan Luo. A stochastic successive minimization method for
nonsmooth nonconvex optimization with applications to transceiver design in wireless communication networks.
Math. Program., 157(2):515–545, 2016.

[43] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics,

pages 400–407, 1951.

[44] Andrzej Ruszczy´nski and Wojciech Syski. On convergence of the stochastic subgradient method with on-line

stepsize rules. J. Math. Anal. Appl., 114(2):512–527, 1986.

[45] Gesualdo Scutari, Francisco Facchinei, Peiran Song, Daniel P Palomar, and Jong-Shi Pang. Decomposition by
partial linearization: Parallel optimization of multi-agent systems. IEEE Trans. Signal Process., 62(3):641–656,
2013.

[46] Yi Xu, Qi Qi, Qihang Lin, Rong Jin, and Tianbao Yang. Stochastic optimization for DC functions and non-
In Proceedings of the 36th International

smooth non-convex regularizers with non-asymptotic convergence.
Conference on Machine Learning, pages 6942–6951. PMLR, 2019.

[47] Yi Xu, Shenghuo Zhu, Sen Yang, Chi Zhang, Rong Jin, and Tianbao Yang. Learning with non-convex truncated
losses by sgd. In Proceedings of The 35th Uncertainty in Artiﬁcial Intelligence Conference, pages 701–711, Tel
Aviv, Israel, 2020. PMLR.

[48] Yang Yang, Gesualdo Scutari, Daniel P Palomar, and Marius Pesavento. A parallel decomposition method for
nonconvex stochastic multi-agent optimization problems. IEEE Trans. Signal Process., 64(11):2949–2964, 2016.

17

