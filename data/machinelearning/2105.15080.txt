1
2
0
2

y
a
M
1
3

]

Y
C
.
s
c
[

1
v
0
8
0
5
1
.
5
0
1
2
:
v
i
X
r
a

PREDICTING CHRONIC HOMELESSNESS: THE IMPORTANCE OF
COMPARING ALGORITHMS USING CLIENT HISTORIES

A PREPRINT

Geoffrey Messier
University of Calgary
2500 University Dr. NW, Calgary, AB, Canada, T2N 1N4
gmessier@ucalgary.ca
Caleb John
University of Calgary
2500 University Dr. NW, Calgary, AB, Canada, T2N 1N4
ctjohn@ucalgary.ca
Ayush Malik
University of Calgary
2500 University Dr. NW, Calgary, AB, Canada, T2N 1N4
ayushmalik118@gmail.com

ABSTRACT

This paper investigates how to best compare algorithms for predicting chronic homelessness for the
purpose of identifying good candidates for housing programs. Predictive methods can rapidly refer
potentially chronic shelter users to housing but also sometimes incorrectly identify individuals who
will not become chronic (false positives). We use shelter access histories to demonstrate that these
false positives are often still good candidates for housing. Using this approach, we compare a simple
threshold method for predicting chronic homelessness to the more complex logistic regression and
neural network algorithms. While traditional binary classiﬁcation performance metrics show that
the machine learning algorithms perform better than the threshold technique, an examination of
the shelter access histories of the cohorts identiﬁed by the three algorithms show that they select
groups with very similar characteristics. This has important implications for resource constrained
not-for-proﬁt organizations since the threshold technique can be implemented using much simpler
information technology infrastructure than the machine learning algorithms.

1 Introduction

Data science has the potential to be a powerful tool for helping to end homelessness. To date, there has been consid-
erable activity aimed at using data analysis and machine learning algorithms to assess the risk that an individual will
experience a homelessness related event. Most algorithms applied to homelessness problems are binary classiﬁers that
seek to determine whether an event will or will not occur. Evaluating the risk of entering or re-entering homelessness
has been well explored (DiGuiseppi et al. 2020; Chan et al. 2017; Kube et al. 2019; Greer et al. 2016; Hong et al. 2018;
Shinn, Greer, et al. 2013; Gao et al. 2017; Byrne, Montgomery, et al. 2019). These studies evaluate their algorithms

 
 
 
 
 
 
Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

using classiﬁcation metrics such as true positive rate (sensitivity), false positive rate (speciﬁcity), false negative rate
(miss rate) and true negative rate (selectivity) (Powers 2008).

Housing First programming is another important aspect of addressing homelessness. The goal of Housing First is
to quickly connect people experiencing homelessness with permanent housing without pre-conditions (Goering et al.
2014; United States Interagency Council on Homelessness 2015). It is common to prioritize people experiencing
chronic homelessness for Housing First programs (Gaetz et al. 2014; Aubry et al. 2015; Toros and Flaming 2018).
These individuals can be identiﬁed using one of several chronic homelessness deﬁnitions (Byrne and Culhane 2015;
Calgary Drop-In Centre 2019; Snyder et al. 2008; Employment and Social Development Canada 2019). However,
these deﬁnitions require a minimum of 6-12 months of homelessness before identifying an individual as chronic.

Clearly, forcing a new entrant to homelessness to accumulate many months of shelter use before being prioritized for
housing support is undesirable. Predictive data analysis algorithms have the potential to identify good candidates for
Housing First programs much more quickly (VanBerlo et al. 2020; Toros and Flaming 2018). These authors demon-
strate the efﬁcacy of their methods using classiﬁcation metrics with (Toros and Flaming 2018) going a step further
to also estimate system cost savings. Both papers demonstrate good results but the complexity of their methods are
beyond the reach of the information technology (IT) capabilities of many resource constrained not-for-proﬁt organi-
zations.

The contribution of this paper is to demonstrate the importance of going further than classiﬁcation metrics when
evaluating the best method to identify individuals at risk of chronic homelessness. In particular, our approach will
reveal that it is not necessary to use a high complexity machine learning algorithm to select a good cohort to prioritize
for housing services. Using classiﬁcation metrics alone can be misleading due to how chronic homelessness is deﬁned.
Unlike a ﬁrst entry into homelessness, chronic homelessness is not a single observable event. Rather, it is a pattern
of events that evolve over time to eventually satisfy a deﬁnition. When a classiﬁer accidentally selects a person who
does not meet the chosen deﬁnition for chronic homelessness, it is penalized by an increase in its false positive rate.
However, the false positive individual may still be an excellent candidate for housing support.

For example, consider two algorithms attempting to predict whether an individual will eventually meet the chronic
deﬁnition criterion of 180 days of homelessness within a year (Employment and Social Development Canada 2019).
The ﬁrst algorithm selects a non-chronic individual with 179 days of homelessness and the second a non-chronic
individual with only 1 day of homelessness. Both selections would result in the same increase in false positive rate but
the ﬁrst algorithm has clearly done a better job of choosing a person in need.

To illustrate our point, we will compare three different binary classiﬁers for predicting chronic homelessness: logistic
regression, a neural network and a simple shelter stay threshold test. While we will evaluate them using classiﬁcation
metrics, we will also perform a statistical analysis of the shelter access histories of the cohorts identiﬁed by the
algorithms. These cohorts contain individuals who do eventually experience chronic homelessness (the true positives)
and those who do not (the false positives). Comparing the shelter access histories of the cohorts identiﬁed for support
reveals that the simple threshold test provides comparable and, in some ways, better performance than the more
complex algorithms.

This has important implications. Implementing a sophisticated machine learning triage tool, like a neural network,
for front-line use would require a shelter to make a considerable investment in IT infrastructure and expertise. This
investment is unnecessary if a simple threshold test implemented in a spreadsheet by non-technical staff could achieve
the same result.

While the focus of this paper is using emergency shelter client data to identify good candidates for housing, we should
emphasize that we do not feel this is the only tool that should be used to triage clients for support programs (Aubry
et al. 2015). Predictive data-based triage techniques should be incorporated into a housing and support program that
includes counsellors and case workers who are reaching out to clients through a variety of mechanisms. Data analysis

2

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

adds value in this type of environment by helping to spot the “under-the-radar” clients who are not engaging with
staff and may fall through the cracks, particularly in very busy shelters. Any individual identiﬁed by the techniques
discussed in this paper should not be immediately placed into housing but should instead be referred to a case manager
for a comprehensive consultation.

The techniques we consider are evaluated using 13 years of anonymized client data from a major North American
shelter. The characteristics of this dataset, the pre-processing of the data and the data features used to predict chronic
shelter use are described in Section 2. The three predictive techniques are presented in Section 3 and their performance
is compared using classiﬁcation metrics and cohort shelter access histories in Section 4. Concluding remarks are made
in Section 5.

2 Description of Data

This is a secondary data analysis performed on anonymized shelter stay records collected at the Calgary Drop-In
Centre (DI) between July 1, 2007 and January 20, 2020. The data anonymization and client privacy protocol used
during this study was approved by the University of Calgary Conjoint Faculties Research Ethics Board. The dataset
consists of 5,431,521 entries for 41,935 unique client proﬁles. Each entry records a single timestamped interaction
with a client that could be accessing sleep services, accessing counselling services, a security incident resulting in a
ban from shelter or a log note. To mitigate left censoring of the data, we exclude all clients who have their ﬁrst sleep
date appear prior to July 1, 2009 which removes a total of 19,967 clients. To mitigate right censoring of the data, we
also exclude all clients who have their ﬁrst sleep date appear after January 20, 2018 which removes a total of 3,570
clients. As a result, 18,398 individuals are retained in the dataset (43.9% of the original dataset population).

The objective of this study is to use this dataset to train and test techniques for predicting whether a new shelter client
will eventually become a chronic shelter user. The Canadian federal deﬁnition of chronic homelessness is used to
classify which individuals in the dataset develop into chronic shelter users. This deﬁnition includes “individuals who
are currently experiencing homelessness and who meet at least 1 of the following criteria: they have a total of at least 6
months (180 days) of homelessness over the past year or they have recurrent experiences of homelessness over the past
3 years, with a cumulative duration of at least 18 months (546 days)” (Employment and Social Development Canada
2019). A total of 1,549 of the 18,398 clients (8.4%) satisfy this deﬁnition and are designated as chronic in the dataset.

Many of the entries in the original DI dataset contain comments entered by DI staff related to their interaction with
the client. To maintain client privacy, these comment ﬁelds are stripped by DI IT staff as part of the anonymization
process. The DI IT staff replaced each comment ﬁeld by a count of how many keywords occur in each comment. The
keywords are divided into the following categories: Police, Emergency Medical Services (EMS), Health, Violence and
Addiction. For example, a common Police word was “CPS” (Calgary Police Services) and a common Violence word
was “ﬁght”. A comment ﬁeld with one occurrence of “CPS” and two occurrences of “ﬁght” would have a count of 1
for Police, a count of 2 for Violence and zero counts for all other keyword categories.

The techniques used in this paper to identify likely chronic shelter users will generate those predictions using data
collected in a 90 day time window that starts from the ﬁrst day a client sleeps in shelter. This time window was
chosen as a compromise between collecting enough data for a reliable prediction while still identifying individuals
for housing supports as early in their shelter timeline as possible. The data features generated for each client are
summarized in Table 1 including a column indicating whether the feature is a client’s static characteristic or a count
of the total number of each type of record over the 90 day period. These features serve as the input to the prediction
algorithms described in Section 3.

We note that one of the limitations of our dataset is that it is biased towards clients who interact with emergency
shelters. An important cohort of individuals experiencing chronic homelessness are the “rough sleepers” who prefer to
sleep outdoors rather than in shelter. As we will see in Section 4, our algorithms rely heavily on the number of shelter

3

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

Field
Age
Bar
Sleep
Log

Description
Operation
Client age.
Static
Records noting a ban from shelter services.
Total Count
Sleep records.
Total Count
General log entries.
Total Count
Counselling entries.
Counsellor Total Count
Records with one or more Police keywords.
Total Count
Records with one or more EMS keywords.
Total Count
Records with one or more Health keywords.
Total Count
Total Count
Records with one or more Violence keywords.
Total Count Records with one or more Addiction keywords.

Police
EMS
Health
Violence
Addiction

Table 1: Client data features.

stays in a client’s record with predicting chronic shelter use. A rough sleeper who satisﬁes the chronic homelessness
deﬁnition may be overlooked due to being under-represented in the emergency shelter data. A way to mitigate this
bias would be for outreach teams to survey the rough sleeper population and include them in the shelter database.

3 Prediction Techniques

Logistic regression is selected as our ﬁrst algorithm due to its computational simplicity (Hastie et al. 2017) and as a
useful benchmark due to its widespread use in the social sector (DiGuiseppi et al. 2020; Chan et al. 2017; Hong et al.
2018; Gao et al. 2017; Byrne, Montgomery, et al. 2019). Since it is often a challenge for a linear algorithm to converge
when operating on multiple features, the logistic regression algorithm utilizes the Sleep and Age data features only.
Both features are normalized to have zero mean and unit variance prior to training and testing. No regularization
algorithm is used on the logistic regression coefﬁcients.

Our second algorithm is a multi-layer perceptron neural network classiﬁer (Goodfellow et al. 2016) that operates on
all the data features listed in Table 1. The settings for the network are selected using a grid search where predictor
accuracy is used as the metric to choose the best setting combination. The rectiﬁed linear function was the best
performing hidden layer activation function when combined with an L2 regularization penalty of 0.05, 100 hidden
layers and a stochastic gradient descent solver with an adaptive learning rate. Note that this is a relatively standard
neural network design. There are many additional ways to optimize neural network performance (ibid.) that we do not
explore since the focus of this paper is algorithm evaluation rather than implementation.

The ﬁnal algorithm is a simple threshold test that indicates a client will likely become chronic if they sleep in shelter
for more than 75% of the time in the 90 day window. This corresponds to 67 or more 24 hour periods where a client
has accessed sleep services at least once.

4 Results

The algorithms described in Section 3 are evaluated using k-fold stratiﬁed cross-validation (Diamantidis et al. 2000).
Cross-validation is a technique where the dataset is divided into a training set and a testing set. The machine learning
algorithm is trained on the training set but its accuracy is evaluated on the testing set. Evaluating model accuracy on
data excluded from the training prevents the model from over-ﬁtting the data and generating overly optimistic results
(Hastie et al. 2017). The data is randomly divided into 10 equal sized groups (10 folds) where 9 folds are used for
training and 1 for testing (Valmarska et al. 2017). The algorithms are then evaluated on all 10 possible training/testing
fold combinations. Note that the threshold test described in Section 3 does not require training. When evaluating the
threshold test as part of the k-fold cycles, it is tested on the fold designated for testing and the training data is ignored.

4

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

Section 4.1 presents a comparison of the three predictive techniques using classiﬁcation metrics. Section 4.2 compares
the algorithms based on a statistical analysis of the shelter access histories of the cohorts identiﬁed by each algorithm.

4.1 Classiﬁcation Metrics

Let P denote the 1,549 clients in the dataset ﬂagged as chronic by the Canadian federal deﬁnition and N denote
the 18,398 - 1,549 = 16,849 clients who are not. Let the ˆ· operator denote the client group that a test predicts will
become chronic. Using this notation, the true positives (chronic clients that an algorithm correctly predicts will become
chronic) are denoted ˆP and the false positives (non-chronic clients that an algorithm incorrectly predicts will become
chronic) are denoted ˆN .

The algorithms are compared using ﬁve classiﬁcation metrics calculated when they are applied to the k-fold cross-
validation testing data. True positive rate or sensitivity ( ˆP /P ) is the proportion of chronic clients that we identify
correctly. High sensitivity is important since we do not want to miss anyone in need of help. False positive rate or
false alarm rate ( ˆN /N ) is the proportion of non-chronic clients identiﬁed as chronic. A high false positive rate will
reduce the efﬁciency of a housing program since false positives result in referring individuals for support that they may
not need. Conﬁdence or precision is the proportion of clients identiﬁed by the algorithm who actually become chronic
( ˆP /( ˆN + ˆP )). Since ˆN + ˆP represents the total number of housing referrals, precision is a useful measure since it
reﬂects the fraction of these referrals who would actually become chronic. Finally, accuracy is deﬁned as the number of
correct classiﬁcations (true positives plus true negatives) divided by the total population size ( ˆP + (N − ˆN ))/(P + N ).
The classiﬁcation metric values for our three classiﬁers are summarized in Table 2, where the absolute number of true
positive and false positives are provided in brackets.

The results in Table 2 suggest that machine learning will lead to the most efﬁcient housing program. Efﬁciency is
deﬁned as directing resources towards those who need them most (Shinn and Cohen 2019) and Table 2 indicates that
machine learning algorithms are the best choice for achieving this goal. For both logistic regression and the neural
network, approximately 64% of clients recommended for housing actually become chronic in the dataset compared to
60% for the threshold test. In terms of absolute numbers, the threshold test will refer almost 1000 non-chronic clients
for housing while the machine learning algorithms keep this number below 300. The true positive rates reveal that the
threshold test does do a better job of referring a higher proportion of chronic clients for housing. This is because the
threshold test essentially casts a much wider net. It selects a higher number of true positives but also a higher number
of false positives.

Table 2 includes accuracy since it is a commonly cited performance metric. However, we will not use it as a means to
compare algorithms for this application. Accuracy can be very misleading when dealing with an imbalanced data set
where the condition we are predicting only occurs in a minority of cases. For example, we could devise a prediction
algorithm that simply says no client will ever become chronic. Since N = 16,849 and P = 1,549, this test would result
in 0 true positives and 16,849 true negatives. The result is an accuracy of (0 + 16, 849)/18, 398 = 91.6%. So, accuracy
can sometimes lend credibility to an algorithm that has little or no practical use.

While the focus of this paper is algorithm evaluation and not algorithm design, we can use the results in Table 2 to
make some comments on machine learning algorithm behavior. The table shows that logistic regression using two
data features performs almost identically to the neural network which uses all ten features. This indicates that Age and
Sleeps are the features with the strongest correlation to chronic shelter use and that the more general neural network
algorithm converges to the same linear separation of the data feature space as logistic regression. It is also instructive
to compare the results in Table 2 to (VanBerlo et al. 2020). The authors achieve a very similar conﬁdence performance
but a superior true positive rate. This is most likely due to their use of dynamic time data in addition to summary data
as well as a prediction time window that potentially allows their algorithm to use a longer data time window than 90
days.

5

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

We should note that any machine learning algorithm tends to ﬁnd data features that are correlated with an outcome of
interest and that correlation does not also imply causation. The causes of chronic homelessness are many and varied.
Even though our analysis indicates that a larger number of shelter sleeps in the ﬁrst 90 days is correlated with chronic
homelessness, it would be circular to argue that many shelter stays is a cause of chronic homelessness which is deﬁned
by a large number of shelter stays. The data feature correlations we have identiﬁed are useful for our application of
using emergency shelter data to predict an outcome. However, taking the additional step of speculating whether these
correlations represent some of the underlying causes of chronic homelessness is beyond the scope of this paper.

Logistic Regression
Neural Network
Threshold

(False Alarm Rate) Conﬁdence Accuracy

False Pos. Rate

True Pos. Rate
(Sensitivity)
0.316 (490)
0.352 (546)
0.985 (1526)
Table 2: Binary classiﬁcation metrics.

0.016 (270)
0.018 (296)
0.058 (982)

0.645
0.648
0.608

0.928
0.929
0.945

4.2 Shelter Access History Comparison

In this section, the three binary classiﬁers are compared based on the shelter access histories of the cohorts identiﬁed
by each classiﬁer as chronic. Each cohort is the union of an algorithm’s false positives and true positives (ie. ˆN + ˆP ).
In the following, a stay is a 24 hour period where sleep services are accessed at least once. An episode of shelter
use is a series of stays where the separation between consecutive stay dates is less than 30 days (Byrne and Culhane
2015). Statistical results are presented for the total shelter stays per client, total episodes per client, shelter tenure per
client (number of days between ﬁrst and last stay), shelter use percentage (stays divided by tenure) and the average
gap length between episodes of shelter use. Since shelter client characteristics are often exponentially distributed, we
show median, 10th percentile and 90th percentile in addition to average. The results for logistic regression, the neural
network and the threshold test are provided in Tables 3, 4 and 5, respectively.

Total Stays
Total Episodes
Tenure (days)
Usage Percentage
Average Gap Length (days)

Average Median
412.5
3.0
1055.0
60.8
1.5

672.7
3.8
1273.1
60.8
3.1

10th Pctl.
113.0
1.0
199.0
13.7
0.9

90th Pctl.
1691.0
8.0
2662.0
100.9
7.3

Group Size: 760/18398 (4.1%)
Table 3: Logistic regression cohort characteristics.

Total Stays
Total Episodes
Tenure (days)
Usage Percentage
Average Gap Length (days)

Average Median
394.0
3.0
1091.0
59.9
1.6

661.8
3.7
1289.0
59.6
3.3

10th Pctl.
108.0
1.0
175.0
13.0
1.0

90th Pctl.
1652.0
8.0
2677.0
100.0
7.7

Group Size: 842/18398 (4.6%)
Table 4: Neural network cohort characteristics.

6

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

Total Stays
Total Episodes
Tenure (days)
Usage Percentage
Average Gap Length (days)

Average Median
362.0
4.0
1397.0
36.7
2.7

563.3
5.4
1526.9
44.6
4.2

10th Pctl.
120.0
1.0
288.0
11.1
1.0

90th Pctl.
1271.0
11.0
2961.0
93.8
9.0

Group Size: 2508/18398 (13.6%)
Table 5: Threshold test group cohort characteristics.

These results show that all three classiﬁers select groups of clients who would be considered high priority for housing
support. The implication of these results is that the false positives in Section 4.1 are mostly people who were “near
misses” and almost but not quite satisﬁed the chronic homelessness deﬁnition.

While the threshold test appeared to be inferior to the machine learning algorithms in Section 4.1, its performance is
much stronger when evaluated using clients’ shelter access histories. The median number of stays in the threshold test
population is approximately equivalent to the machine learning algorithms and the median shelter tenure is actually
signiﬁcantly longer for the threshold test group. Usage percentage is lower for the threshold population which is most
likely due to including longer tenure clients. The number of episodes of shelter access and the average length of gaps
between episodes of shelter use are also both higher for the threshold group.

This suggests that the non-chronic false positive clients selected by the threshold test exhibit episodic shelter access
characteristics. Originally discovered by the cluster analysis in (Kuhn and Culhane 1998), episodic shelter clients
are characterized by more sporadic periods of shelter access over a long period of time. While perhaps not strictly
“chronic”, we argue that any client group with 362 median stays and a median period of shelter interaction of 1397
days is clearly in need of housing support.

One possible drawback of the threshold test is that it identiﬁes a greater number of clients (13.6%) compared to
the machine learning algorithms (4.1% and 4.6%). This may raise concerns that its use may overwhelm a housing
program. To put this in perspective, the 2,508 clients identiﬁed by the threshold test are stretched over 101 months
of data. This means the test would identify an average of approximately 24.8 clients per month. The current housing
programs at the Calgary Drop-In Centre have successfully been able to house approximately twice this many clients
(Calgary Drop-In Centre 2019). Therefore, we believe that the clients identiﬁed by the threshold test would fall within
the capacity of most housing programs.

5 Concluding Remarks

Data science has the potential to be a very powerful tool for delivering efﬁcient programs to support people experienc-
ing homelessness. This exciting area of research is a true inter-disciplinary collaboration where ideas and techniques
from statistics, engineering and computer science are applied to problems in the social science domain. However, it
is important to evaluate the beneﬁt of these techniques using an approach that recognizes that any group of people
will have characteristics that cannot be easily summarized by a binary label like “chronic” or “not chronic”. This
has important implications not just for understanding the nature of a group identiﬁed for support but for choosing the
technique used to identify that group in the ﬁrst place.

For data science to have beneﬁt, it must make the transition from the research domain to application in a front-line
shelter or social program setting. Implementing many machine learning algorithms comes with technical barriers that
are not trivial to overcome, particularly in the not-for-proﬁt sector. If a simple technique offers similar performance to
a complex one, the simple technique should be chosen every time.

7

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

The broad contribution of this paper is to highlight the importance of including a client shelter access history analysis
when using data science techniques to support homelessness services. By applying this philosophy to the speciﬁc
problem of predicting chronic homelessness, our second contribution is to show that a simple threshold test offers
similar performance to more sophisticated machine learning algorithms. This test could be implemented today by
most shelters and housing programs using IT infrastructure already in place. This is an example of how proper
evaluation of data science can actually remove some of the barriers to using these techniques in a real world setting.

6 Acknowledgments

The authors would like to acknowledge the support of the Natural Sciences and Engineering Research Council of
Canada (NSERC), the Calgary Drop-In Centre and the Government of Alberta. This study is based in part on data
provided by Alberta Community and Social Services. The interpretation and conclusions contained herein are those
of the researchers and do not necessarily represent the views of the Government of Alberta. Neither the Government
of Alberta nor Alberta Community and Social Services express any opinion related to this study.

References

Aubry, T., Bell, M., Ecker, J., Gaetz, S., Goering, P., Hume, C., and Turner, A. (2015). Screening for housing ﬁrst.

Canadian Observatory on Homelessness and the Mental Health Commission of Canada.

Byrne, T. and Culhane, D. P. (2015). Testing alternative deﬁnitions of chronic homelessness. Psychiatric Services 66

(9), pp. 996–999.

Byrne, T., Montgomery, A. E., and Fargo, J. D. (2019). Predictive modeling of housing instability and homelessness

in the Veterans Health Administration. Health services research 54 (1), pp. 75–85.

Calgary Drop-In Centre (2019). More than Emergency Shelter: 2018-19 Report

to Community. URL:

https://www.calgarydropin.ca/cms/wp-content/uploads/2019/11/Report-to-Community-final.pdf.

Chan, H., Rice, E., Vayanos, P., Tambe, M., and Morton, M. (2017). Evidence From the Past: AI Decision Aids to

Improve Housing Systems for Homeless Youth. AAAI Fall Symposia, pp. 149–157.

Diamantidis, N. A., Karlis, D., and Giakoumakis, E. A. (2000). Unsupervised stratiﬁcation of cross-validation for

accuracy estimation. Artiﬁcial Intelligence 116 (1-2), pp. 1–16.

DiGuiseppi, G. T., Davis, J. P., Leightley, D., and Rice, E. (2020). Predictors of Adolescents’ First Episode of Home-

lessness Following Substance Use Treatment. Journal of Adolescent Health 66, pp. 408–415.

Employment and Social Development Canada (Dec. 2019). Reaching Home: Canada’s Homelessness Strategy Direc-

tives. URL: https://www.canada.ca/en/employment-social-development/programs/homelessness/directives.html.

Gaetz, S., Gulliver, T., and Richter, T. (2014). The State of Homelessness in Canada 2014. Toronto, Ontario, Canada:

Canadian Homelessness Research Network.

Gao, Y., Das, S., and Fowler, P. (2017). Homelessness service provision: a data science perspective. Workshops at the

Thirty-First AAAI Conference on Artiﬁcial Intelligence.

Goering, P., Veldhuizen, S., Watson, A., Adair, C., Kopp, B., Latimer, E., and Aubry, T. (2014). National ﬁnal report:

Cross-site at home/chez soi project. Mental Health Commission of Canada 48.
Goodfellow, I., Bengio, Y., and Courville, A. (2016). Deep Learning. MIT Press.
Greer, A. L., Shinn, M., Kwon, J., and Zuiderveen, S. (2016). Targeting services to individuals most likely to enter

shelter: Evaluating the efﬁciency of homelessness prevention. Social Service Review 90 (1), pp. 130–155.

Hastie, T., Tibshirani, R., and Friedman, J. (2017). The Elements of Statistical Learning: Data Mining, Inference and

Prediction. 2nd ed. Springer.

Hong, B., Malik, A., Lundquist, J., Bellach, I., and Kontokosta, C. E. (2018). Applications of machine learning meth-
ods to predict readmission and length-of-stay for homeless families: the case of WIN shelters in New York City.
Journal of Technology in Human Services 36 (1), pp. 89–104.

8

Predicting Chronic Homelessness: The Importance of Comparing Algorithms using Client Histories

Kube, A., Das, S., and Fowler, P. J. (2019). Allocating interventions based on predicted outcomes: A case study on

homelessness services. Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Vol. 33, pp. 622–629.

Kuhn, R. and Culhane, D. P. (1998). Applying cluster analysis to test a typology of homelessness by pattern of shelter
utilization: Results from the analysis of administrative data. American journal of community psychology 26 (2),
pp. 207–232.

Powers, D. M. W. (2008). Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness and

Correlation. Journal of Machine Learning Technologies 2 (1), pp. 37–63.

Shinn, M. and Cohen, R. (2019). Homelessness prevention: A review of the literature. Center for Evidence-Based

Solutions to Homelessness.

Shinn, M., Greer, A. L., Bainbridge, J., Kwon, J., and Zuiderveen, S. (2013). Efﬁcient targeting of homelessness

prevention services for families. American journal of public health 103 (S2), S324–S330.

Snyder, S. G., Wilkinson, J., Blank, L., Blumenthal, L., Keen, G. R., Manning, J., Prokosch, M., Ralston, P., Rogers,
G., Thompson, P., Weasel Head, C., and Wigston, R. (2008). A Plan for Alberta: Ending Homelessness in 10 Years.
The Alberta Secretariat for Action on Homelessness.

Toros, H. and Flaming, D. (2018). Prioritizing Homeless Assistance Using Predictive Algorithms: An Evidence-Based

Approach. Cityscape 20 (1), pp. 117–146.

United States Interagency Council on Homelessness (2015). Opening doors: Federal strategic plan to prevent and end

homelessness. US Interagency Council on Homelessness.

Valmarska, A., Lavraˇc, N., F¨urnkranz, J., and Robnik- ˇSikonja, M. (2017). Reﬁnement and selection heuristics in

subgroup discovery and classiﬁcation rule learning. Expert Systems with Applications 81, pp. 147–162.

VanBerlo, B., Ross, M. A., Rivard, J., and Booker, R. (2020). Interpretable Machine Learning Approaches to Prediction

of Chronic Homelessness. arXiv preprint arXiv:2009.09072.

9

