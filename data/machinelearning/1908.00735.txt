0
2
0
2

n
a
J

7
2

]

G
L
.
s
c
[

2
v
5
3
7
0
0
.
8
0
9
1
:
v
i
X
r
a

Eﬃcient computation of counterfactual
explanations of LVQ models

Andr´e Artelt∗ and Barbara Hammer †

CITEC - Cognitive Interaction Technology
Bielefeld University - Faculty of Technology
Inspiration 1, 33619 Bielefeld - Germany

Abstract. The increasing use of machine learning in practice and legal
regulations like EU’s GDPR cause the necessity to be able to explain the
prediction and behavior of machine learning models. A prominent exam-
ple of particularly intuitive explanations of AI models in the context of
decision making are counterfactual explanations. Yet, it is still an open
research problem how to eﬃciently compute counterfactual explanations
for many models. We investigate how to eﬃciently compute counterfactual
explanations for an important class of models, prototype-based classiﬁers
such as learning vector quantization models. In particular, we derive spe-
ciﬁc convex and non-convex programs depending on the used metric.

1

Introduction

Due to the recent advances in machine learning (ML), ML models are being more
and more used in practice and applied to real-world scenarios for decision mak-
ing [1–3]. Essential demands for user acceptance as well as legal regulations like
the EU’s ”General Data Protection Right” (GDPR) [4], that contains a ”right
to an explanation”, make it indispensable to explain the output and behavior of
ML models in a comprehensible way.

As a consequence, many research approaches focused on the question how to
realize explainability and transparency in machine learning in recent years [5–8].
There exist diverse methods for explaining ML models [6, 9]. One family of
methods are model-agnostic methods [6,10]. Model-agnostic methods are ﬂexible
in the sense that they are not tailored to a particular model or representation.
This makes model-agnostic methods (in theory) applicable to many diﬀerent
types of ML models. In particular, ”truly” model-agnostic methods do not need
access to the training data or model internals. It is suﬃcient to have an interface
for passing data points to the model and obtaining the output/predictions of the
model - the underlying model is viewed as a black-box.

Examples of model-agnostic methods are feature interaction methods [11],
feature importance methods [12], partial dependency plots [13] and local meth-
ods that approximates the model locally by an explainable model (e.g. a decision
tree) [14,15]. These methods explain the models by using features as vocabulary.

∗corresponding author: aartelt@techfak.uni-bielefeld.de
†We gratefully acknowledge funding from the VW-Foundation for the project IMPACT

funded in the frame of the funding line AI and its Implications for Future Society.

 
 
 
 
 
 
A diﬀerent class of model-agnostic explanations are example-based explana-
tions where a prediction or behavior is explained by a (set of) data points [16].
Instances of example-based explanations are prototypes & criticisms [17] and
inﬂuential instances [18]. Another instance of example-based explanations are
counterfactual explanations [19]. A counterfactual explanation is a change of
the original input that leads to a diﬀerent (speciﬁc) prediction/behavior of the
ML model - what has to be diﬀerent in order to change the prediction of the
model? Such an explanation is considered to be fairly intuitive, human-friendly
and useful because it tells people what to do in order to achieve a desired out-
come [9, 19].

Existing counterfactual explanations are mostly model agnostic methods,
which are universally applicable but computationally expensive [15, 20, 21]. In
this work, we address the question how to eﬃciently compute counterfactuals
for a popular class of models by referring to its speciﬁc structure.

Prototype based models such as learning vector quantization (LVQ) represent
data by a set of representative samples [22]. LVQ models can be combined with
metric learning and thereby increase the eﬀectiveness of the model in case of few
prototypes [23, 24]. Furthermore, LVQ models can be used in many settings like
life-long learning [25].

Here, we will consider the question how to eﬃciently compute counterfac-
tual explanations of prototype-based classiﬁers, in particular LVQ models. By
exploiting the special structure of such models, we are able to

1. propose model- and regularization-dependent methods for eﬃciently com-

puting counterfactual explanations of LVQ models,

2. and empirically demonstrate the eﬃciency of the modeling as regards
speed-up as well as required amount of change in comparison to standard
techniques.

Further, the framework enables a straightforward incorporation of domain knowl-
edge, which can be phrased as additional constraints. Such domain knowledge
could be used for generating more plausible and feasible counterfactual expla-
nations.

The remainder of this paper is structured as follows: First, we brieﬂy review
counterfactual explanations (section 2) and learning vector quantization models
(section 3). Then, in section 4 we introduce our convex and non-convex pro-
gramming framework for eﬃciently computing counterfactual explanations of
diﬀerent types of LVQ models - note that all derivations can be found in the
appendix (section 6). Furthermore, we empirically evaluate the eﬃciency of our
methods. Finally, section 5 summarizes the results of this paper.

2 Counterfactual explanations

Counterfactual explanations [19] (often just called counterfactuals) are an in-
stance of example-based explanations [16]. Other instances of example-based

explanations [9] are inﬂuential instances [18] and prototypes & criticisms [17].

A counterfactual states a change to some features of a given input such
that the resulting data point (called counterfactual) has a diﬀerent (speciﬁed)
prediction than the original input. The rational is considered to be intuitive,
human-friendly and useful because it tells practitioners which minimum changes
can lead to a desired outcome [9, 19].

A classical use case of counterfactual explanations is loan application [3, 9]:
Imagine you applied for a credit at a bank. Unfortunately, the bank rejects your
application. Now, you would like to know why. In particular, you would like to
know what would have to be diﬀerent so that your application would have been
accepted. A possible explanation might be that you would have been accepted if
you would earn 500$ more per month and if you would not have a second credit
card.

Although counterfactuals constitute very intuitive explanation mechanisms,

there do exist a couple of problems.

One problem is that there often exist more than one counterfactual - this
is called Rashomon eﬀect [9]. If there are more than one possible explanation
(counterfactual), it is not clear which one should be selected.

In our opinion, the main issue is feasibility and plausibility - in the sense that
the counterfactual data point should be valid and plausible in the given domain.
There exist ﬁrst approaches like [26,27] but in general it is still an open research
question how to eﬃciently compute feasible and plausible counterfactuals.

An alternative - but very similar in the spirit - to counterfactuals [19] is the
Growing Spheres method [21]. However, this method suﬀers from the curse of
dimensionality because it has to draw samples from the input space, which can
become diﬃcult if the input space is high-dimensional.

Deﬁnition 1 (Counterfactual explanation). Assume a prediction function h is
given. Computing a counterfactual ~x′ ∈ Rd for a given input ~x ∈ Rd is phrased
as an optimization problem [19]:

arg min
~x′ ∈ Rd

ℓ

h(~x′), yc
(cid:0)

(cid:1)

+ C · θ(~x′, ~x)

(1)

where ℓ(·) denotes the loss function, yc the requested prediction, and θ(·) a
penalty term for deviations of ~x′ from the original input ~x. C > 0 denotes
the regularization strength.

Two common regularizations are the weighted Manhattan distance and the

generalized L2 distance.

The weighted Manhattan distance is deﬁned as

θ(~x′, ~x) =

αj · |(~x)j − (~x′)j|

(2)

Xj

where αj > 0 denote the feature wise weights. A popular choice [19] for αj is
the inverse median absolute deviation of the j-th feature median in the training

data set D:

αj =

1
MADj

(3)

where MADj = median

(~x)j
(~x)j − median
~x ∈ D (cid:18)(cid:12)
(cid:12)
(cid:0)
(cid:12)
(cid:12)

~x ∈ D

(cid:12)
(cid:19)
(cid:12)
(cid:1)
(cid:12)
(cid:12)

The beneﬁt of this choice is that it takes the (potentially) diﬀerent variability of
the features into account. However, because we need access to the training data
set D, this regularization is not a truly model-agnostic method - it is not usable
if we only have access to a prediction interface of a black-box model.

The generalized L2 distance1 is deﬁned as

θ(~x′, ~x) = k~x − ~x′k2

Ω = (~x − ~x′)⊤ Ω(~x − ~x′)

(4)

where Ω denotes a symmetric positive semi-deﬁnite (s.psd) matrix. Note that
the L2 distance can be recovered by setting Ω = I. The generalized L2 distance
can be interpreted as the Euclidean distance in a linearly distorted space.

Depending on the model and the choice of ℓ(·) and θ(·), the ﬁnal optimization
problem might be diﬀerentiable or not.
If it is diﬀerentiable, we can use a
gradient-based optimization algorithm like conjugate gradients, gradient descent
or (L-)BFGS. Otherwise, we have to use a black-box optimization algorithm like
Downhill-Simplex (also called Nelder-Mead) or Powell. However, using a model
and regularization speciﬁc optimization technique has the best chances to enable
a simpliﬁcation and computationally eﬃcient solution. Unfortunately, there exit
model speciﬁc optimizers for counterfactual explanations for few ML models
only.

3 Learning vector quantization

In learning vector quantization (LVQ) models [22] we compute a set of labeled
prototypes {(~pi, oi)} from a training data set of labeled real-valued vectors - we
refer to the i-th prototype as ~pi and the corresponding label as oi. A new data
point is classiﬁed according to the winner-takes-it-all scheme:

h(~x) = oi

s.t. ~pi = arg min

~pj

d(~x, ~pj)

(5)

where d(·) denotes a function for computing the distance between a data point
and a prototype - in vanillas LVQ, this is independent chosen globally as the
squared Euclidean distance:

d(~x, ~p) = (~x − ~p)⊤I(~x − ~p)

(6)

There exist matrix-LVQ models like GMLVQ, LGMLVQ [23], MRSLVQ and
LMRSLVQ [24] that learn a custom (class or prototype speciﬁc) distance matrix

1also called Mahalanobis distance

Ωp that is used instead of the identity I when computing the distance between
a data point and a prototype. This gives rise to the generalized L2 distance
Eq. (4):

d(~x, ~p) = (~x − ~p)⊤Ωp(~x − ~p)
Because Ωp must be a s.psd matrix, instead of learning Ωp directly, these LVQ
variants learn a matrix ˜Ωp and compute the ﬁnal distance matrix as:

(7)

Ωp = ˜Ω

⊤
p

˜Ωp

(8)

By this, we can guarantee that the matrix Ωp is s.psd, whereas the model only
has to learn an arbitrary matrix ˜Ωp - which is much easier than making some
restriction on the deﬁniteness. Training usually takes place by optimizing suit-
able cost functions as regards prototype locations and metric parameters. For
counterfactual reasoning, the speciﬁc training method [23, 24] is irrelevant and
we refer to the ﬁnal model only.

4 Counterfactual explanations of LVQ models

We aim for an eﬃcient explicit formulation how to ﬁnd counterfactuals, for
diverse LVQ models.

4.1 General approach

Because a LVQ model assigns the label of the nearest prototype to a given input,
we know that the nearest prototype of a counterfactual ~x′ must be a prototype
~pi with oi = yc. In order to compute a counterfactual ~x′ of a given input ~x, it is
suﬃcient to solve the following optimization problem for each prototype ~pi with
oi = yc and select the counterfactual ~x′ yielding the smallest value of θ(~x′, ~x):

θ(~x′, ~x)

arg min
~x′ ∈ Rd
s.t. d(~x′, ~pi) + ǫ ≤ d(~x′, ~pj) ∀ ~pj ∈ P(yc)
where P(yc) denotes the set of all prototypes not labeled as yc and ǫ > 0 is
a small value preventing that the counterfactual lies exactly on the decision
boundary. Note that the feasible region of Eq. (9) is always non-empty - the
prototype ~pi is always a feasible solution. Furthermore, in contrast to Eq. (1),
the formulation does not include hyperparameters.

(9b)

(9a)

The pseudocode for computing a counterfactual of a LVQ model is described
in Algorithm 1. Note that the for loop in line 3 of Algorithm 1 can be easily
parallelized.

For the weighted Manhattan distance as a regularization θ(·), the objective
Eq. (9a) becomes linear in ~x′, where Υ is the diagonal matrix with entries αj
and ~β is an auxiliary variable that can be discarded afterwards:

~1⊤ ~β

min
~x′,~β ∈ Rd
s.t. Υ~x′ − Υ~x ≤ ~β, −Υ~x′ + Υ~x ≤ ~β,

~β ≥ ~0

(10)

Algorithm 1 Computing a counterfactual of a LVQ model
Input: Original input ~x, requested prediction yc of the counterfactual, the LVQ
model
Output: Counterfactual ~x′
1: ~x′ = ~0
2: z = ∞
3: for ~pi with oi = yc do
4:

⊲ Try each prototype with the correct label

⊲ Initialize dummy solution

Solving Eq. (9) yields a counterfactual ~x′
∗
if θ(~x′

∗, ~x) < z then ⊲ Keep this counterfactual if it deviates less from

5:

6:

7:

the original input then the currently ”best” counterfactual

z = θ(~x′
~x′ = ~x′
∗

∗, ~x)

end if

8:
9: end for

For the Euclidean distance Eq. (4) as regularization θ(·), the objective Eq. (9a)
can be written in a convex quadratic form:

min
~x′ ∈ Rd

1
2

~x′⊤~x′ − ~x′⊤~x

(11)

In the subsequel we explore Eq. (9) for diﬀerent regularizations θ(·) and LVQ
models, and investigate how to solve it eﬃciently. Note that for the purpose
of better readability and due to space constraints, we put all derivations in the
appendix (section 6).

4.2 (Generalized matrix) LVQ

When using (generalized matrix) LVQ models - no class or prototype speciﬁc
distance matrices - the optimization problem Eq. (9) becomes a linear program
(LP) when using the weighted Manhattan distance as a regularizer, and a convex
quadratic program (QP) problem when using the Euclidean distance. Both
programs can be solved eﬃciently and (up to equivalence) uniquely [28]. More
precisely, the constraints Eq. (9b) can be written as a set of linear inequality
constraints:

~x′⊤~qij + rij + ǫ ≤ 0 ∀ ~pj ∈ P(yc)

where

~qij = Ω(~pj − ~pi)

rij =

1
~p
2 (cid:16)

⊤
i Ω ~pi − ~p

⊤
j Ω ~pj

(12)

(13)

(cid:17)

We set Ω = I if the LVQ model uses the Euclidean distance.

Another beneﬁt is that we can easily add additional convex quadratic con-
straints like box constraints, restricting the value range of linear interaction of
features or freezing some features - these features are not allowed to be diﬀerent
in the counterfactual. We can use such additional constraints for computing a
more plausible and feasible counterfactual.

4.3 (Localized generalized matrix) LVQ

In case of local matrix-LVQ models that learn a class or prototype speciﬁc dis-
tance matrix Ωp, the optimization problem Eq. (9) becomes a quadratically
constrained quadratic program (QCQP) for Manhattan and Euclidean regular-
ization, since the constraints Eq. (9b) become a set of quadratic constraints:

1
2

~x′⊤Qij~x′ + ~x′⊤~qij + rij + ǫ ≤ 0 ∀ ~pj ∈ P(yc)

where

Qij = Ωi − Ωj

(14)

(15)

Unfortunately, we can not make any statement about the deﬁniteness of Qij.
Because Qij is the diﬀerence of two s.psd matrices, all we know is that it is a
symmetric matrix. Therefore, Eq. (14) yields a non-convex QCQP and solving a
non-convex QCQP is known to be NP-hard [29]. However, there exist methods
like the Suggest-Improve framework [29] that can approximately solve a non-
convex QCQP very eﬃciently - more details on how to apply this to Eq. (14)
can be found in the appendix (section 6).

4.4 Experiments

We empirically conﬁrm the eﬃciency of our proposed methods in comparison to
black-box mechanisms by means of the following experiments: We use GLVQ,
GMLVQ and LGMLVQ models with 3 prototypes per class for the ”Breast Can-
cer Wisconsin (Diagnostic) Data Set” [30], the ”Optical Recognition of Hand-
written Digits Data Set” [31] and the ”Ames Housing dataset” [32]. Thereby,
we use PCA-preprocessing 2 to reduce the dimensionality of the digit data set
to 10 and of the breast cancer data set to 5. We standardize the house data
set and turned it into a binary classiﬁcation problem3 by setting the target to
1 if the price is greater or equal to 160k$ and 0 otherwise. We use the Suggest-
Improve framework [29] for solving the non-convex QCQPs, where we pick the
target prototype as initial solution in the Suggest-step and we use the penalty
convex-concave procedure (CCP) in the Improve-step [29]. For comparison, we
use the optimizer for computing counterfactual explanations of LVQ models as
implemented in ceml [33] - where the distance to the nearest prototype with the
requested label yc is minimized by Downhill-Simplex search or CMA-ES.

We report results for the Manhattan distance as regularizer - we used the
Manhattan distance for enforcing a sparse solution. For each possible combina-
tion of model, data set and method, a 4-fold cross validation is conducted and

2This is for the purpose of better stability and better semantic meaning, since in the original
domain already a small perturbation is suﬃcient for changing the class, since adversarial
attacks exist even for linear functions in high dimensions if feature correlations are neglected.
Since PCA can be approximately inverted, counterfactuals in PCA space can be lifted to the
original data space.

3In addition, we select the following features: TotalBsmt, 1stFlr, 2ndFlr, GrLivA, Wood-
Deck, OpenP, 3SsnP, ScreenP and PoolA - When computing counterfactuals, we ﬁx the last
ﬁve features.

Data set
Method
GLVQ
GMLVQ
LGMLVQ

Breast cancer

DS
3.26
2.71
2.00

CMA Ours
1.96
3.28
2.46
6.49
1.57
1.61

Handwritten digits
DS
6.51
21.34
8.12

CMA Ours
3.99
6.53
4.40
11.63
7.53
7.88

House prices

DS
3.81
5.06
12.74

CMA Ours
3.32
3.85
3.78
8.63
8.20
12.59

Table 1: Mean Manhattan distance between the counterfactual and the original
data point - best (smallest) values are highlighted. For LGMLVQ with DS or
CMA-ES (marked italic), in 5% to 60% of the cases no solution was found.

the mean distance is reported. The results are listed in Table 1. In all cases,
our method yields counterfactuals that are closer to the original data point than
the one found by minimizing the original cost function Eq. (1) with Downhill-
Simplex search (DS) or CMA-ES. In addition, our method is between 1.5 and
158.0 faster in comparison to DS/CMA-ES method. Furthermore, Downhill-
Simplex and CMA-ES did not always ﬁnd a counterfactual when dealing with
LGMLVQ models. We would like to remark that our formulation can easily be
extended by linear/quadratic constraints which can incorporate prior knowledge
such as a maximum possible change of speciﬁc input features - see Table 2 for an
example. Such extensions do not change the form of the optimization problem
hence its complexity.

Data point
Original
Counterfactual
Constrained Counterfactual

TotalBsmt
0
0
373

1stFlr
1120
366
1454

2ndFlr GrLivA

468
1824
1454

1588
2225
3125

Label
1
0
0

Table 2: House prices, we obtain a ”plausible” counterfactual by adding con-
strains, here the constraint ”2ndFlr ≤ 1stFlr” is added.

The implementation of our proposed method for computing counterfactual
explanations is available online4. All experiments were implemented in Python
3.6 [34] using the packages cvxpy [35] cvx-qcqp [36], sklearn-lvq [37], numpy [38],
scipy [39], scikit-learn [40] and ceml [33].

5 Conclusion

We proposed, and empirically evaluated, model- and regularization-dependent
convex and non-convex programs for eﬃciently computing counterfactual expla-
nations of LVQ models. We found that in many cases we get either a set of
linear or convex quadratic programs which both can be solved eﬃciently. Only
in the case of localized matrix-LVQ models we have to solve a set of non-convex
quadratically constrained quadratic programs - we found that they can be eﬃ-
ciently approximately solved by using the Suggest-Improve framework.

4https://github.com/andreArtelt/efficient_computation_counterfactuals_lvq

6 Appendix

6.1 Minimizing the Euclidean distance

First, we expand the Euclidean distance Eq. (4):

k~x′ − ~xk2

2 = (~x′ − ~x)⊤(~x′ − ~x)

= ~x′⊤~x′ − ~x′⊤~x − ~x⊤~x′ + ~x⊤~x
= ~x′⊤~x′ − 2~x⊤~x′ + ~x⊤~x

(16)

Next, we note that that we can drop the constant ~x⊤~x when optimizing with
respect to ~x′:

min
~x′ ∈ Rd
⇔

min
~x′ ∈ Rd

k~x′ − ~xk2
2

~x′⊤~x′ − ~x⊤~x′

1
2

(17)

6.2 Minimizing the weighted Manhattan distance

First, we transform the problem of minimizing the weighted Manhattan distance
Eq. (2) into epigraph form:

αj · |(~x′)j − (~x)j|

min
~x′ ∈ Rd

⇔

Xj

min
~x′ ∈ Rd,β ∈ R

β

s.t.

αj · |(~x′)j − (~x)j| ≤ β

Xj
β ≥ 0

Next, we separate the dimensions:

min
~x′ ∈ Rd,β ∈ R

β

s.t.

αj · |(~x′)j − (~x)j| ≤ β

Xj
β ≥ 0

⇔

min

~x′,~β ∈ Rd Xj

(~β)j

s.t. αj · |(~x′)j − (~x)j | ≤ (~β)j ∀ j

(~β)j ≥ 0 ∀ j

(18)

(19)

After that, we remove the absolute value function:

min

~x′,~β ∈ Rd Xj

(~β)j

s.t. αj · |(~x′)j − (~x)j| ≤ (~β)j ∀ j

(~β)j ≥ 0 ∀ j

⇔

min

~x′,~β ∈ Rd Xj

(~β)j

s.t. αj(~x′)j − αj(~x)j ≤ (~β)j ∀ j

− αj (~x′)j + αj(~x)j ≤ (~β)j ∀ j
(~β)j ≥ 0 ∀ j

Finally, we rewrite everything in matrix-vector notation:

min
~x′,~β ∈ Rd

~1⊤ ~β

s.t.
Υ~x′ − Υ~x ≤ ~β
− Υ~x′ + Υ~x ≤ ~β
~β ≥ ~0

Υ = diag(αj )

where

6.3 Enforcing a speciﬁc prototype as the nearest neighbor

(20)

(21)

(22)

By using the following set of strict inequalities, we can force the prototype ~pi
to be the nearest neighbor of the counterfactual ~x′ - which would cause ~x′ to be
classiﬁed as oi:

d(~x′, ~pi) < d(~x′, ~pj) ∀ ~pj ∈ P(yc)

(23)

We consider a ﬁxed pair of i and j:

Ωi < k~x′ − ~pjk2
Ωj

d(~x′, ~pi) < d(~x′, ~pj)
⇔ k~x′ − ~pik2
⇔ (~x′ − ~pi)⊤Ωi(~x′ − ~pi) < (~x′ − ~pj)⊤Ωj(~x′ − ~pj)
⇔ ~x′⊤Ωi~x′ − 2~x′⊤Ωi~pi + ~p
⇔ ~x′⊤Ωi~x′ − ~x′⊤Ωj~x′ − 2~x′⊤Ωi~pi + 2~x′⊤Ωj~pj + ~p
⇔ ~x′⊤(Ωi − Ωj)~x′ + ~x′⊤(−2Ωi~pi + 2Ωj~pj) + (~p
~x′⊤(Ωi − Ωj)~x′ + ~x′⊤(Ωj~pj − Ωi~pi) +

⇔

(~p

⊤

i Ωi~pi < ~x′⊤Ωj~x′ − 2~x′⊤Ωj~pj + ~p
⊤
i Ωi~pi − ~p
⊤
j Ωi~pj) < 0

⊤
j Ωi~pj
⊤
j Ωi~pj < 0

⊤
i Ωi~pi − ~p

1
2

⊤
i Ωi~pi − ~p

⊤
j Ωi~pj) < 0

⇔

~x′⊤Qij~x′ + ~x′⊤~qij + rij < 0

1
2
1
2

where

Qij = Ωi − Ωj

~qij = Ωj~pj − Ωi~pi

rij =

1
2

~pi⊤Ωi~pi − ~pj⊤Ωj~pj
(cid:0)

(cid:1)

(24)

(25)

(26)

(27)

If we only have a global distance matrix Ω, we ﬁnd that Qij = 0 and the
inequality Eq. (24) simpliﬁes:

d(~x, ~pi) < d(~x, ~pj)
⇔ ~x′⊤~qij + rij < 0

where

~qij = Ω

~pj − ~pi
(cid:0)

⊤
i Ω ~pi − ~p

(cid:1)
⊤
j Ω ~pj

rij =

1
~p
2 (cid:16)

(28)

(29)

(30)

(cid:17)

If we do not use a custom distance matrix, we have Ω = I and Eq. (24) becomes:

where

d(~x, ~pi) < d(~x, ~pj)
⇔ ~x′⊤~qij + rij < 0

~qij = ~pj − ~pi

rij =

1
~p
2 (cid:16)

⊤
i ~pi − ~p

⊤
j ~pj

(cid:17)

(31)

(32)

(33)

1
2
g(~x′) =

6.4 Approximately solving the non-convex QCQP

Recall the non-convex quadratic constraint from Eq. (24):

~x′⊤Qij~x′ + ~x′⊤~qij + rij + ǫ ≤ 0

1
2

where the matrix Qij is deﬁned as the diﬀerence of two s.psd matrices:

By making use of Eq. (35), we can rewrite Eq. (34) as:

Qij = Ωi − Ωj

1
~x′⊤Ωi~x′ + ~x′⊤~qij + rij + ǫ −
2
⇔ f (~x′) − g(~x′) ≤ 0

1
2

~x′⊤Ωj~x′ ≤ 0

where

f (~x′) =

~x′⊤Ωi~x′ + ~x′⊤~qij + rij + ǫ

(34)

(35)

(36)

(37)

1
2
Under the assumption that the regularization function θ(·) is a convex function5,
we can rewrite a generic version of the non-convex QCQP (induced by Eq. (14))
as follows:

~x′⊤Ωj~x′

(38)

min
~x′ ∈ Rd

θ(~x′, ~x)

s.t.
f (~x′) − g(~x′) ≤ 0

(39)

Because Ωi and Ωj are s.psd matrices, we know that f (~x′) and g(~x′) are convex
functions. Therefore Eq. (39) is a diﬀerence-of-convex program (DCP).

This allows us to use the penalty convex-concave procedure (CCP) [29] for
computing an approximate solution of Eq. (39) that is equivalent to the original
non-convex QCQP induced by Eq. (14). For using the penalty CCP, we need
the ﬁrst order Taylor approximation of g(~x′) around a current point ~xk:
ˆg(~x′)~xk = g(~xk) + (∇~x′ g)(~xk)⊤(~x′ − ~xk)
k Ωj~xk + (Ωj~xk)⊤(~x′ − ~xk)
~x⊤

=

1
2

= (Ωj~xk)⊤~x′ +
jk~x′ + ˜rjk
= ~ρ⊤

1
2

k Ωj~xk − (Ωj~xk)⊤~xk
~x⊤

where

~ρjk = Ωj~xk
1
2
5The weighted Manhattan distance and the Euclidean distance are convex functions!

~x⊤
k Ωj~xk

˜rjk = −

(40)

(41)

(42)

References

[1] Sharad Goel, Justin M. Rao, and Ravi Shroﬀ. Precinct or prejudice? understanding racial

disparities in new york city’s stop-and-frisk policy. 2016.

[2] Kaveh Waddell. How algorithms can bring down minorities’ credit scores. The Atlantic,

2016.

[3] Amir E. Khandani, Adlar J. Kim, and Andrew Lo. Consumer credit-risk models via
machine-learning algorithms. Journal of Banking & Finance, 34(11):2767–2787, 2010.

[4] European

parliament

and

council.

General

data

protection

regulation.

https://eur-lex.europa.eu/eli/reg/2016/679/oj , 2016.

[5] Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana
Kagal. Explaining explanations: An overview of interpretability of machine learning.
In 5th IEEE International Conference on Data Science and Advanced Analytics, DSAA
2018, Turin, Italy, October 1-3, 2018 , pages 80–89, 2018.

[6] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti,
and Dino Pedreschi. A survey of methods for explaining black box models. ACM Comput.
Surv., 51(5):93:1–93:42, August 2018.

[7] Erico Tjoa and Cuntai Guan. A survey on explainable artiﬁcial intelligence (XAI): towards

medical XAI. CoRR, abs/1907.07374, 2019.

[8] Wojciech Samek, Thomas Wiegand, and Klaus-Robert M¨uller. Explainable artiﬁcial in-
telligence: Understanding, visualizing and interpreting deep learning models. CoRR,
abs/1708.08296, 2017.

[9] Christoph Molnar.

Machine
https://christophm.github.io/interpretable-ml-book/ .

Interpretable

Learning.

2019.

[10] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Model-agnostic interpretability
of machine learning. In ICML Workshop on Human Interpretability in Machine Learning
(WHI), 2016.

[11] Brandon M. Greenwell, Bradley C. Boehmke, and Andrew J. McCarthy. A simple and
eﬀective model-based variable importance measure. CoRR, abs/1805.04755, 2018.

[12] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All Models are Wrong but many
are Useful: Variable Importance for Black-Box, Proprietary, or Misspeciﬁed Prediction
Models, using Model Class Reliance. arXiv e-prints, page arXiv:1801.01489, Jan 2018.

[13] Qingyuan Zhao and Trevor Hastie. Causal interpretations of black-box models. Journal

of Business & Economic Statistics, 0(ja):1–19, 2019.

[14] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ”why should i trust you?”:
Explaining the predictions of any classiﬁer. In Proceedings of the 22Nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, KDD ’16, pages
1135–1144, New York, NY, USA, 2016. ACM.

[15] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Dino Pedreschi, Franco Turini,
and Fosca Giannotti. Local rule-based explanations of black box decision systems. CoRR,
abs/1805.10820, 2018.

[16] A. Aamodt and E. Plaza. Case-based reasoning: Foundational issues, methodological

variations, and systemapproaches. AI communications, 1994.

[17] Been Kim, Oluwasanmi Koyejo, and Rajiv Khanna. Examples are not enough, learn to
In Advances in Neural Information Processing
criticize! criticism for interpretability.
Systems 29: Annual Conference on Neural Information Processing Systems 2016,
December 5-10, 2016, Barcelona, Spain, pages 2280–2288, 2016.

[18] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence func-
tions. In Proceedings of the 34th International Conference on Machine Learning, ICML
2017, Sydney, NSW, Australia, 6-11 August 2017 , pages 1885–1894, 2017.

[19] Sandra Wachter, Brent D. Mittelstadt, and Chris Russell. Counterfactual explana-
tions without opening the black box: Automated decisions and the GDPR. CoRR,
abs/1711.00399, 2017.

[20] Shubham Sharma, Jette Henderson, and Joydeep Ghosh. CERTIFAI: counterfactual
explanations for robustness, transparency, interpretability, and fairness of artiﬁcial intel-
ligence models. CoRR, abs/1905.07857, 2019.

[21] Thibault Laugel, Marie-Jeanne Lesot, Christophe Marsala, Xavier Renard, and Marcin
Detyniecki. Comparison-based inverse classiﬁcation for interpretability in machine learn-
In Information Processing and Management of Uncertainty in Knowledge-Based
ing.
Systems. Theory and Foundations - 17th International Conference, IPMU 2018, C´adiz,
Spain, June 11-15, 2018, Proceedings, Part I, pages 100–111, 2018.

[22] David Nova and Pablo A. Est´evez. A review of learning vector quantization classiﬁers.

Neural Comput. Appl., 25(3-4):511–524, September 2014.

[23] Petra Schneider, Michael Biehl, and Barbara Hammer. Adaptive relevance matrices
in learning vector quantization. Neural Computation, 21(12):3532–3561, 2009. PMID:
19764875.

[24] Petra Schneider, Michael Biehl, and Barbara Hammer. Distance learning in discriminative
vector quantization. Neural Computation, 21(10):2942–2969, 2009. PMID: 19635012.

[25] Stephan Kirstein, Heiko Wersing, Horst-Michael Gross, and Edgar K¨orner. A life-long
learning vector quantization approach for interactive learning of multiple categories.
Neural networks : the oﬃcial journal of the International Neural Network Society , 28:90–
105, 04 2012.

[26] Arnaud Van Looveren and Janis Klaise. Interpretable counterfactual explanations guided

by prototypes. CoRR, abs/1907.02584, 2019.

[27] Rafael Poyiadzi, Kacper Sokol, Ra´ul Santos-Rodriguez, Tijl De Bie, and Peter A. Flach.
FACE: feasible and actionable counterfactual explanations. CoRR, abs/1909.09369, 2019.

[28] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University

Press, New York, NY, USA, 2004.

[29] Jaehyun Park and Stephen Boyd. General heuristics for nonconvex quadratically con-

strained quadratic programming. arXiv preprint arXiv:1703.07870, 2017.

[30] Olvi L. Mangasarian William H. Wolberg, W. Nick Street. Breast cancer wisconsin (diag-

nostic) data set. https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic),
1995.

[31] E. Alpaydin and C. Kaynak. Optical recognition of handwritten digits data set.

https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits,
1998.

[32] Dean De Cock. Ames, iowa: Alternative to the boston housing data as an end of semester

regression project. Journal of Statistics Education, 19(3), 2011.

[33] Andr´e Artelt. Ceml: Counterfactuals for explaining machine learning models - a python

toolbox. https://www.github.com/andreArtelt/ceml, 2019.

[34] Guido Van Rossum and Fred L Drake Jr. Python tutorial. Centrum voor Wiskunde en

Informatica Amsterdam, The Netherlands, 1995.

[35] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language
for convex optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

[36] Jaehyun Park and Stephen Boyd. A cvxpy extension for handling nonconvex qcqp via

suggest-and-improve framework. https://github.com/cvxgrp/qcqp, 2017.

[37] Joris Jensen. sklearn-lvq. https://github.com/MrNuggelz/sklearn-lvq, 2017.

[38] St´efan van der Walt, S. Chris Colbert, and Ga¨el Varoquaux. The numpy array: A
structure for eﬃcient numerical computation. Computing in Science and Engineering,
13(2):22–30, 2011.

[39] Eric Jones, Travis Oliphant, Pearu Peterson, et al. SciPy: Open source scientiﬁc tools

for Python, 2001–.

[40] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blon-
del, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825–2830, 2011.

