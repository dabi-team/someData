1
2
0
2

g
u
A
0
3

]
I

A
.
s
c
[

1
v
1
8
3
3
1
.
8
0
1
2
:
v
i
X
r
a

©2021 the authors. This is a preprint version. The ﬁnal version of this
paper is available at https://dl.acm.org/doi/10.1145/3449726.3463131
(DOI:10.1145/3449726.3463131).

Trustworthy AI for Process Automation on a
Chylla-Haase Polymerization Reactor

Daniel Hein(cid:63) and Daniel Labisch(cid:63)(cid:63)

(cid:63)Siemens AG, Technology, Munich, Germany (e-mail: hein.daniel@siemens.com).
(cid:63)(cid:63)Siemens AG, Digital Industries, Karlsruhe, Germany (e-mail:
daniel.labisch@siemens.com).

August 31, 2021

Abstract

In this paper, genetic programming reinforcement learning (GPRL) is
utilized to generate human-interpretable control policies for a Chylla-
Haase polymerization reactor. Such continuously stirred tank reactors
(CSTRs) with jacket cooling are widely used in the chemical industry,
in the production of ﬁne chemicals, pigments, polymers, and medi-
cal products. Despite appearing rather simple, controlling CSTRs in
real-world applications is quite a challenging problem to tackle. GPRL
utilizes already existing data from the reactor and generates fully au-
tomatically a set of optimized simplistic control strategies, so-called
policies, the domain expert can choose from. Note that these policies
are white-box models of low complexity, which makes them easy to val-
idate and implement in the target control system, e.g., SIMATIC PCS
7. However, despite its low complexity the automatically-generated
policy yields a high performance in terms of reactor temperature con-
trol deviation, which we empirically evaluate on the original reactor
template.

1

Introduction

Utilizing AI-based methods in real-world industrial applications requires a
certain amount of trust in the automatically-generated solution. One way to

1

 
 
 
 
 
 
Figure 1: SIMATIC PCS 7 Chylla-Haase polymerization reactor template [1]

build trust, is to have machine learning (ML) methods present the reasons for
their decisions in a human-understandable way [11]. On the one hand, one
can attempt to interpret any useful system, even if it inherently is a black-
box system, e.g., by explanation techniques for classiﬁers [24] or visualization
methods for neural networks (NNs) [4]. On the other hand, interpretability
can be evaluated via a quantiﬁable proxy, where at ﬁrst, a model class, e.g.,
algebraic equations, is claimed to be interpretable and then algorithms are
developed in order to optimize within this class [6, 23, 27].

For the latter, recently a new method called genetic programming rein-
forcement learning (GPRL) has been proposed [17] and evaluated on chal-
lenging benchmark problems, like a hardware cart-pole demonstrator [18] or a
real-world inspired industrial benchmark [16]. In the present paper, we apply
GPRL for the ﬁrst time on a SIMATIC PCS 7 Chylla-Haase polymerization
reactor template (see Fig. 1) [1], to improve the process automation con-
troller while keeping the policy solution interpretable and thus trustworthy.
The template exempliﬁes the conventional control strategy using a realistic
but simple simulation model directly built in SIMATIC PCS 7. It aims as a
template for real applications.

2

If a process plant runs safely and reliably during operation, the operator
always strives to maximize the economic yield. Existing degrees of freedom
are used to design the operation as optimally as possible without violating
existing limitations and boundary conditions. This can be done through
manual intervention by the operator based on experience, or through the use
of various optimization methods.

Optimization methods can basically be divided into two classes, simulation-

based and data-based tools, although mixed forms are also possible. Strictly
speaking, the optimization works with a simulation model in the ﬁrst case
and with a data-based model in the second case. An optimization based
on a general PROcess Modelling System (gPROMS) is an example for the
ﬁrst case. Most data-based approaches create black-box results like neural
networks. These models can show impressive results, but most of them are
too complex to be interpreted or validated.

Hence, available optimization methods for process industries are either
interpretable expert-generated process controllers or non-interpretable AI-
generated process controllers. Herein, we show how the ML method genetic
programming can be utilized together with supervised AI-based regression
models to automatically generate interpretable process controllers. By doing
so, the manual eﬀort to generate interpretable process controllers can be
reduced signiﬁcantly. Furthermore, novel unknown control strategies can be
found, which improve the quality key performance indicators (KPIs) of the
controller.

In contrast to simulation-based approaches, this method signiﬁcantly re-
duces the modeling eﬀorts. Manually creating a simulation model is very
costly in general. Even if a model already exists, e.g., from the planning
phase, the adaption and parametrization of the model to represent the real
behavior is still very time-consuming. Furthermore, many simulation models
from the planning phase are just static ones which couldn’t be easily extended
to a dynamic version. The data-based approach GPRL does not require all
these eﬀorts.

Furthermore, generating black-box controllers by ML can be prone to
overﬁt to some process model inaccuracies. Black-box controllers normally
have hundreds of parameters which are automatically tuned to yield a high
performance with respect to the predicted control quality. However, in many
cases model inaccuracies and loopholes can be exploited by the ML process
which result in controllers that only performing well on the imperfect data-
based models but not on the real process. Interpretable controllers by genetic
programming, formulated using the established controller building blocks
are more robust to process model inaccuracies. Furthermore, experts of the
respective process are able to identify contradictions and implausibilities in

3

interpretable controllers.

The resulting process controllers can either directly replace existing con-
trol structures or can also be used as a superimposed optimization strategy.
This ﬂexibility combined with the interpretable controller function enables
the process owner to balance the conﬂicting goals of optimizing performance
and guaranteeing safety and reliability of the process.

In Sections 2 and 3, the interface of the reactor template and the GPRL
In Section 4, we show how GPRL
method are introduced, respectively.
uses available simulation data from a Chylla-Haase reactor to generate inter-
pretable policies. In the evaluation phase of the experiment, we show that
the newly found policies reduce the control error by up to 37 %.

2 Chylla-Haase polymerization reactor

Continuously stirred tank reactors (CSTRs) with jacket cooling are widely
used in the chemical industry, in the production of ﬁne chemicals, pigments,
polymers, and medical products. CSTRs can be run in batch, semi-batch or
continuous operation. [1]

To perform an accurate control, online optimization and monitoring of
processes, precise knowledge of the status of the respective process is required
at all times. However, only in rare cases it is possible to measure all the states
of the reactor using real sensor equipment. Often, some process variables can
not be determined at all or only with great technical eﬀort.

Simulating the dynamic process model in a soft sensor in parallel to the
real process, allows the automation system to observe all modeled inner states
of the reactor even if they cannot be registered by measurement equipment.
The Chylla-Haase reactor, considered in our experiments, contains a uni-
versal stirred tank reactor that is employed for the production of diverse
polymers with diﬀerent recipes. Here, an extended Kalman ﬁlter serves as a
soft sensor for monitoring the chemical reaction.

The values calculated online by the soft sensor can be used in various

ways [1]:

• Calculation of the current speed of reaction and the heat released by

the exothermic reaction,

• Calculation of the monomer mass remaining in the reactor, and

• Calculation of heat transfer from reactor to cooling jacket in order to

detect the build-up of deposition.

4

A frequently considered control problem statement for Chylla-Haase re-
actors, is to maintain reaction temperature of a semi-batch polymerization
reactor at a setpoint in spite of a variety of conditions ranging from raw
material impurity, product variety, repeated batching and varying ambient
temperature. Although the reactor conﬁguration and control requirements
appear rather simple, in real-world applications it is quite a challenging prob-
lem to tackle. [5]

The rector template (SIMATIC PCS 7 project and documentation) we
used in our experiments is available online1. The production of polymer is
maintained by an sequential function chart (SFC) recipe, where the simula-
tion of one batch takes between 10 to 40 minutes, depending on the process
parameters. The operator screen as user interface of the template is depicted
in Fig. 1.

The reactor template can be inﬂuenced by two variables either given by
the operator or using a superimposed optimization strategy during the batch:

• Reactor temperature setpoint ˆT , and

• Monomer feed setpoint ˆM .

These variables are called decision variables or actions.

The task is to process a certain amount of monomer into polymer, given
a certain reactor setpoint. According to the recipe provided, the reactor is
heated up to a certain start temperature. If this temperature is reached, the
monomer is added to the stirred tank reactor at a certain monomer feed rate
ˆM . As soon as a pre-deﬁned amount of monomer is processed , the batch
is ﬁnished and the tank gets drained. Note that the reactor temperature
has to be controlled constantly by heating and cooling, since the reaction
temperature is highly dependent on the amount of polymer inside the tank.
The default method to control the temperature is to use a proportional–
integral–derivative (PID) controller. Today PID control is still one of the
most common control strategies, since it is very often applied at the lowest
level of the control hierarchy in industrial systems [3]. In 2002, Desborough
and Miller conducted a survey of more than 11,000 controllers in the reﬁning,
chemicals, and pulp and paper industries which revealed that 97 % of regula-
tory controllers had a PID structure [10]. However, PID controllers can have
severe drawbacks. They use only limited process information and the design
criterion sometimes yields closed loop systems with poor robustness [2].

Hence, it is expected that an RL-based controller, which is potentially
non-linear and can utilize all available process variables, can outperform

1https://support.industry.siemens.com/cs/de/de/view/109756215

5

a

s

Variable Description
ˆT
ˆM

Reactor temperature setpoint K
Monomer feed setpoint

kg/s

352-365
0.005-0.015

Unit

Range

S
T
M
P
U A
Q

Intended setpoint
Reactor temperature
Monomer mass
Polymer mass
Heat loss coeﬃcient
Reaction heat ﬂow

352-365
-
-
-

K
K
kg
kg
kW/K 0-1
kW

-

Table 1: Reactor variables of action a and state s

standard PID regulation on many real-world use cases. The action and state
variables of the reactor template, which are available for an RL controller,
are given in Table 1. The action variables in a are the decision variables of
the reactor template, whereas the state variables in s contain the intended
setpoint S as well as sensor measurements from the reactor. Note that S is
not a variable modeled by the reactor simulation, but it represents the true
intended temperature setpoint of the operator. The default controller sets
ˆT = S, i.e., the operator’s intended setpoint is directly used as input for the
underlying PID regulator.

3 Genetic programming reinforcement learn-

ing

The GPRL approach learns policy representations which are basic algebraic
equations of low complexity [17]. Given that GPRL can ﬁnd rather short
(non-complex) equations, it is expected to reveal substantial knowledge about
underlying coherencies between available state variables and well-performing
control policies for a certain RL problem. By the term complexity of a policy,
we refer to its human-interpretable form, e.g., a simple algebraic equation
can be easily understood by a domain expert, hence it is of low complexity,
whereas a non-linear system of equations, like a neural network, cannot be
validated by a human expert, yielding a high complexity.

Note that GPRL is not only optimizing a certain trade-oﬀ between pol-
icy complexity and performance, but rather it evolves a whole optimized
Pareto front from which the domain expert can choose a trustworthy and

6

well-performing solution.

3.1 Population-based reinforcement learning

Generally, reinforcement learning (RL) is distinguished from other computa-
tional approaches by its emphasis on an agent learning from direct interaction
with its environment. This approach, which does not rely on exemplary su-
pervision or complete models of the environment, is referred to as online
learning. However, for many real-world problems online learning is prohib-
ited for safety reasons. For example, it is not advisable to deploy an online
RL agent, who starts by applying an arbitrary initial policy, which is subse-
quently improved by exploitation and exploration, on safety-critical domains
like process industries or power plants. For this reason, oﬄine learning is a
more suitable approach for applying RL methods on already collected train-
ing data and system models, to yield RL policies.

Oﬄine RL is often referred to as batch learning because it is based solely
on a previously generated batch of transition samples from the environment.
The batch data set D contains transition tuples of the form (st, at, st+1, rt+1),
where the application of action at in state st resulted in a transition to state
st+1 and yielded a reward rt+1, where t denotes a discrete time step.

Herein, we use model-based value estimation to estimate the performance
of a policy. In the ﬁrst step, supervised ML is applied to learn approximate
models of the underlying environment from transition tuples (st, at, st+1, rt+1)
as follows:

˜g(st, at) ← st+1,

˜r(st, at, st+1) ← rt+1.

(1)

Using models ˜g and ˜r, the value for policy π of each state s in the data
batch can be estimated by computing the value function ˜vπ(s). Hence, using
model-based value estimation means performing trajectory rollouts on system
models for diﬀerent starting states:

˜vπ(st) =

∞
(cid:88)

k=0

γk ˜r(st+k, at+k, st+k+1),

with st+k+1 = ˜g(st+k, at+k), at+k = π(st+k),

(2)

(3)

and discount factor 0 ≤ γ ≤ 1.

Since we are searching for interpretable solutions, the resulting policies
have to be represented in an explicit form. Policy search yields explicit
policies which can be enforced to be of an interpretable form. Furthermore,
policy search is inherently well-suited for being used with population-based
optimization techniques, like genetic programming (GP).

7

Figure 2: Population-based policy search iterating policy evaluation (E) and
policy improvement (I)

The goal of using policy search for learning interpretable RL policies is to
ﬁnd the best policy among a set of policies that is spanned by a parameter
vector x ∈ X , where X is the space of all interpretable policy parameters for
this RL task. Herein, a policy corresponding to a particular parameter value
x is denoted by π[x]. The policy’s performance, when starting from st is
measured by its value function given in Eq. (2). Furthermore, including only
a ﬁnite number of T > 1 future rewards for the model-based value estimation
from Eq. (2) yields

˜vπ[x](st) =

T −1
(cid:88)

k=0

γk ˜r(st+k, at+k, st+k+1),

(4)

with st+k+1 = ˜g(st+k, at+k) and at+k = π[x](st+k).

To rate the performance of policy π[x], the value function is used as

follows:

Rπ[x] =

1
|S|

(cid:88)

s∈S

˜vπ[x](s),

(5)

with Rπ[x] being the average discounted return of π[x] on a representative
set of test states S.

In population-based RL, the interest lies in populations of policies, which
means that multiple diﬀerent policies exist at the same time in one policy
iteration step. Fig. 2 depicts how the return values of all population members
drive the improvement step (I) of the whole population. The evaluation step
(E) is performed individually for every member. Applying population-based
evolutionary methods to policy search has already been successfully achieved
in the past [8, 9, 15, 28].

3.2 Genetic programming

GP has been utilized for creating system controllers since its introduction [21].
Since then, the ﬁeld of GP has grown signiﬁcantly and has produced numer-

8

<latexit sha1_base64="c2+IruHlk400z420Qh0I89XlCrU=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICWRgh4LPvAiVOhL2lI26TYuTZOwSQtt6UG8evOqP0x/iwe/rFHQIt2wmdlvvvl2ZscKXBFGhvGW0paWV1bX0pns+sbm1nZuZ7cW+kNp86rtu75sWCzkrvB4NRKRyxuB5Gxgubxu9c/jeH3EZSh8rxKNA94eMMcTPWGzCNBdKxCdqXFszjq5vFEw1NLnHTNx8pSssp97pxZ1ySebhjQgTh5F8F1iFOJrkkkGBcDaNAUm4QkV5zSjLHKHYHEwGNA+/g5OzQT1cI41Q5Vt4xYXWyJTp0PsK6VogR3fyuGHsB/YE4U5/94wVcpxhWNYC4oZpXgDPKJ7MBZlDhLmdy2LM+OuIurRmepGoL5AIXGf9o/OBSISWF9FdLpUTAcaljqP8AIebBUVxK/8raCrjruwTFmuVLxEkUFPwsavj3owZvPvUOed2knBNArmbTFfKiYDT9M+HdARpnpKJbqmMuqwUckzvdCrVtEm2oP2+EXVUknOHv1a2tMnkwqR7A==</latexit><latexit sha1_base64="CoNJRUr+gisPcTdWr2HtD+vGU3I=">AAAC7XichVFNa9tAEH1RkjZx01ZJj70Ym0IOwUjFkB4NbUIvhTTEHxAHs1I29mJ9Ia0DqfC9f6C30GtvvbZ/pf0tOeTtRgmkpnjFat6+mXk7sxNkkSq05/1ZcVbX1p883disPdt6/uKlu73TK9JZHspumEZpPghEISOVyK5WOpKDLJciDiLZD6bvjb9/KfNCpcmJvsrkWSzGibpQodCkRm5jmNJtssthLPQkFFF5PJ+PymGmRqW35xO7Ta/l2VVfBH4FmqjWUer+xRDnSBFihhgSCTRxBIGC3yl8eMjInaEklxMp65eYo8bcGaMkIwTZKf9jnk4rNuHZaBY2O+QtEXfOzDrecB9axYDR5lZJXNDecH+x3Pi/N5RW2VR4RRtQcdMqfiKvMWHEssy4iryvZXmm6UrjAu9sN4r1ZZYxfYYPOh/oyclNraeOAxs5pkZgz5d8gYS2ywrMK98r1G3H57TCWmlVkkpRUC+nNa/Pejhm/9+hLoLe25bvtfzP7WanXQ18A6/RwC6nuo8OPuKIdYT4ip/4hd9O6nxzrp3vd6HOSpXzCo+W8+MWDBWdpg==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="bU2aoiD1hG+bww6/5l2U5Ni1Ny4=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjQihehon1AWyRJtzU0LzbbYi0F8erNq/4x/S0e/HZNBS3SDZuZ/eabb2d2nNj3EmGa7xltYXFpeSWby6+urW9sFra2a0k04C6rupEf8YZjJ8z3QlYVnvBZI+bMDhyf1Z3+qYzXh4wnXhTeiFHM2oHdC72u59oCUKMl2L0Ylye3haJpmGrps46VOkVKVyUqfFCLOhSRSwMKiFFIAr5PNiX4mmSRSTGwNo2BcXieijOaUB65A7AYGDbQPv49nJopGuIsNROV7eIWH5sjU6d97HOl6IAtb2XwE9hP7AeF9f69YayUZYUjWAeKOaV4CVzQHRjzMoOUOa1lfqbsSlCXTlQ3HuqLFSL7dH90zhDhwPoqolNZMXvQcNR5iBcIYauoQL7yVEFXHXdgbWWZUglTRRt6HFa+PurBmK2/Q511aoeGZRrW1VGxZKQDz9Iu7dEBpnpMJbqgCuqQ03yhV3rTrrWR9qg9fVO1TJqzQ7+W9vwFD1uSIQ==</latexit><latexit sha1_base64="e42w+peHwDYlJrr1/t4WluWSwoQ=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjwgR6EivYBbZEk3dbQvNhsi7UUxKs3r/rH9Ld48Ns1FbRIN2xm9ptvvp3ZcWLfS4Rpvme0ufmFxaVsLr+8srq2XtjYrCZRn7us4kZ+xOuOnTDfC1lFeMJn9ZgzO3B8VnN6xzJeGzCeeFF4I4YxawV2N/Q6nmsLQPWmYPdidDG+LRRNw1RLn3as1ClSuspR4YOa1KaIXOpTQIxCEvB9sinB1yCLTIqBtWgEjMPzVJzRmPLI7YPFwLCB9vDv4tRI0RBnqZmobBe3+NgcmTrtYp8pRQdseSuDn8B+Yj8orPvvDSOlLCscwjpQzCnFS+CC7sCYlRmkzEktszNlV4I6dKS68VBfrBDZp/ujc4IIB9ZTEZ1OFbMLDUedB3iBELaCCuQrTxR01XEb1laWKZUwVbShx2Hl66MejNn6O9Rpp7pvWKZhXR0US0Y68Cxt0w7tYaqHVKJzKqMOOc0XeqU37Vobao/a0zdVy6Q5W/Rrac9fGPeSJQ==</latexit><latexit sha1_base64="Gi37WHsFfe/OxrEzyk0dZSxAjxM=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICUpBT0WfOBFqNCXtKUk6TYuzYtNWmhLD+LVm1f9YfpbPPhlTQUt0g2bmf3mm29ndszA4WGkae8pZWV1bX0jnclubm3v7Ob29uuhPxQWq1m+44umaYTM4R6rRTxyWDMQzHBNhzXMwUUcb4yYCLnvVaNxwDquYXu8zy0jAnTfDnh3qp0WZ91cXitocqmLjp44eUpWxc99UJt65JNFQ3KJkUcRfIcMCvG1SCeNAmAdmgIT8LiMM5pRFrlDsBgYBtAB/jZOrQT1cI41Q5lt4RYHWyBTpWPsa6logh3fyuCHsJ/YE4nZ/94wlcpxhWNYE4oZqXgLPKIHMJZluglzXsvyzLiriPp0LrvhqC+QSNyn9aNziYgANpARla4k04aGKc8jvIAHW0MF8SvPFVTZcQ/WkJZJFS9RNKAnYOPXRz0Ys/53qItOvVjQtYJ+V8qXS8nA03RIR3SCqZ5RmW6ogjosVPJCr/SmVJWJ8qg8fVOVVJJzQL+W8vwFlXGR7Q==</latexit><latexit sha1_base64="ArHKTagTro+F+vcryTkSzxddgJw=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICXRgh4LPvAiVOhL2lI26TYuzYskLbSlB/Hqzav+MP0tHvyypoIW6YbNzH7zzbczO4ZvizDStPeUsrS8srqWzmTXNza3tnM7u7XQGwQmr5qe7QUNg4XcFi6vRiKyecMPOHMMm9eN/kUcrw95EArPrUQjn7cdZrmiJ0wWAbpv+aIz0Y5Pp51cXitocqnzjp44eUpW2ct9UIu65JFJA3KIk0sRfJsYhfiapJNGPrA2TYAF8ISMc5pSFrkDsDgYDGgffwunZoK6OMeaocw2cYuNHSBTpUPsa6logB3fyuGHsJ/YY4lZ/94wkcpxhSNYA4oZqXgLPKIHMBZlOglzVsvizLiriHp0LrsRqM+XSNyn+aNziUgArC8jKl1JpgUNQ56HeAEXtooK4leeKaiy4y4sk5ZLFTdRZNALYOPXRz0Ys/53qPNO7aSgawX9rpgvFZOBp2mfDugIUz2jEt1QGXWYqOSFXulNqShj5VF5+qYqqSRnj34t5fkLl9iR7g==</latexit><latexit sha1_base64="sj5HG0SxNAxALhBOubmqE6P2PU0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICWRgh4LPvCiVOhL2lKSdBuX5kWyLbSlB/Hqzav+MP0tHvyypoIW6YbNzH7zzbczO2bg8Eho2ntKWVpeWV1LZ7LrG5tb27md3VrkD0KLVS3f8cOGaUTM4R6rCi4c1ghCZrimw+pm/zyO14csjLjvVcQoYG3XsD3e45YhAN23At6ZaMe3004urxU0udR5R0+cPCWr7Oc+qEVd8smiAbnEyCMB3yGDInxN0kmjAFibJsBCeFzGGU0pi9wBWAwMA2gffxunZoJ6OMeakcy2cIuDHSJTpUPsK6logh3fyuBHsJ/YY4nZ/94wkcpxhSNYE4oZqXgDXNADGIsy3YQ5q2VxZtyVoB6dyW446gskEvdp/ehcIBIC68uISpeSaUPDlOchXsCDraKC+JVnCqrsuAtrSMukipcoGtALYePXRz0Ys/53qPNO7aSgawX9rpgvFZOBp2mfDugIUz2lEl1TGXVYqOSFXulNqShj5VF5+qYqqSRnj34t5fkL2LWSCQ==</latexit><latexit sha1_base64="Fbb/nlneYPY4rZpxPd6eWhxhc54=">AAAC7XichVFNSyNBEH0ZdY3uupt1j15CgrCHJcyIsB4F18WLEMWYgJHQM7axyXwx0xF0yN0/sDfx6s3r+lf0t3jwdTsKuyL20FOvX1W9rury01Dl2nXvKs7U9MyH2erc/MdPC5+/1L4u7ufJOAtkJ0jCJOv5IpehimVHKx3KXppJEfmh7PqjDePvnsosV0m8p89SeRiJYayOVSA0qUGt0U/oNtlFPxL6JBBhsTuZDIp+qgaF+2OFuNZ0W65d9dfAK0ET5WontXv0cYQEAcaIIBFDE4cQyPkdwIOLlNwhCnIZkbJ+iQnmmTtmlGSEIDvif8jTQcnGPBvN3GYHvCXkzphZxzL3b6voM9rcKolz2gfuc8sN37yhsMqmwjNan4pzVnGbvMYJI97LjMrI51rezzRdaRxjzXajWF9qGdNn8KLzi56M3Mh66ti0kUNq+PZ8yheIaTuswLzys0LddnxEK6yVViUuFQX1Mlrz+qyHY/b+H+prsL/S8tyWt7PaXF8tB17FEhr4zqn+xDq20GYdAS5wg7+4dRLnj3PpXD2FOpUy5xv+Wc71Iw59nac=</latexit><latexit sha1_base64="BQJxzXifRem5IW/A8iQeJgOXw6w=">AAAC7XichVFNS+RAEH1m/f4c9ehlcBA8yJDowO5RcF28CCqOCo4MndiOzeSLpEfQMPf9A3tbvHrzqn9Ff4sHX7dRUBE7dOr1q6rXVV1+Gqpcu+7DgPNjcGh4ZHRsfGJyanqmMjt3kCe9LJDNIAmT7MgXuQxVLJta6VAepZkUkR/KQ7+7YfyHFzLLVRLv68tUnkSiE6szFQhNql1ZbCV0m+yiFQl9Hoiw2Ov320UrVe3CXVkjrtTcumtX9TPwSlBDuXaSyiNaOEWCAD1EkIihiUMI5PyO4cFFSu4EBbmMSFm/RB/jzO0xSjJCkO3y3+HpuGRjno1mbrMD3hJyZ8ysYon7j1X0GW1ulcQ57RP3leU6X95QWGVT4SWtT8Uxq7hNXuOcEd9lRmXkay3fZ5quNM7wy3ajWF9qGdNn8Kbzm56MXNd6qti0kR1q+PZ8wReIaZuswLzyq0LVdnxKK6yVViUuFQX1Mlrz+qyHY/Y+DvUzOFite27d223U1hvlwEexgEUsc6o/sY4t7LCOAH9xizvcO4nzz/nvXL+EOgNlzjzeLefmGRDlnag=</latexit><latexit sha1_base64="vbsof7IKzTRwc4ppEXp+c1skZtw=">AAAC7XichVHPaxNBFP6y1tqm2kY99hISCh4k7EqgHgOt4kWppWkCTQiz20kyZH+xOwnUJXf/AW/i1ZtX/Vfq3+LBb6abQg3SWWbfN99775v35vlpqHLtutcV58HGw81HW9vVncdPdvdqT5+d58k8C2Q3SMIk6/sil6GKZVcrHcp+mkkR+aHs+bMj4+8tZJarJD7TV6kcRmISq7EKhCY1qjUGCd0muxhEQk8DERany+WoGKRqVLgvPxDXmm7Ltau+DrwSNFGuk6T2GwNcIkGAOSJIxNDEIQRyfhfw4CIlN0RBLiNS1i+xRJW5c0ZJRgiyM/4nPF2UbMyz0cxtdsBbQu6MmXUccL+1ij6jza2SOKf9w/3JcpP/3lBYZVPhFa1PxW2r+J68xpQR92VGZeSqlvszTVcaY7y23SjWl1rG9Bnc6hzTk5GbWU8db2zkhBq+PS/4AjFtlxWYV14p1G3Hl7TCWmlV4lJRUC+jNa/Pejhm79+hroPzVy3PbXkf281Ouxz4FvbRwAtO9RAdvMMJ6wjwGT/wE7+cxPnifHW+3YQ6lTLnOe4s5/tfUd2dww==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="51NKWXtK0QWXqt2dG54BYI5I8Jk=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICWRgh4LPvAiVOhL2lKSdBuXpklItoW29CBevXnVH6a/xYPfrlHQIt2wmdlvvvl2ZscOPR4Lw3hLaUvLK6tr6Ux2fWNzazu3s1uLg2HksKoTeEHUsK2YedxnVcGFxxphxKyB7bG63T+X8fqIRTEP/IoYh6w9sFyf97hjCUB3rZB3puaxOevk8kbBUEufd8zEyVOyykHunVrUpYAcGtKAGPkk4HtkUYyvSSYZFAJr0xRYBI+rOKMZZZE7BIuBYQHt4+/i1ExQH2epGatsB7d42BEydTrEvlKKNtjyVgY/hv3AnijM/feGqVKWFY5hbShmlOINcEH3YCzKHCTM71oWZ8quBPXoTHXDUV+oENmn86NzgUgErK8iOl0qpgsNW51HeAEftooK5Ct/K+iq4y6spSxTKn6iaEEvgpWvj3owZvPvUOed2knBNArmbTFfKiYDT9M+HdARpnpKJbqmMupwUMkzvdCrVtEm2oP2+EXVUknOHv1a2tMnlXOR7Q==</latexit><latexit sha1_base64="9aJ/QYQiaMnKaN/mimpRjNEPDlc=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICUpBT0WfOBFqNCXtKUk6TYuzYtNWmhLD+LVm1f9YfpbPPhlTQUt0g2bmf3mm29ndszA4WGkae8pZWV1bX0jnclubm3v7Ob29uuhPxQWq1m+44umaYTM4R6rRTxyWDMQzHBNhzXMwUUcb4yYCLnvVaNxwDquYXu8zy0jAnTfDnh3qp8WZ91cXitocqmLjp44eUpWxc99UJt65JNFQ3KJkUcRfIcMCvG1SCeNAmAdmgIT8LiMM5pRFrlDsBgYBtAB/jZOrQT1cI41Q5lt4RYHWyBTpWPsa6logh3fyuCHsJ/YE4nZ/94wlcpxhWNYE4oZqXgLPKIHMJZluglzXsvyzLiriPp0LrvhqC+QSNyn9aNziYgANpARla4k04aGKc8jvIAHW0MF8SvPFVTZcQ/WkJZJFS9RNKAnYOPXRz0Ys/53qItOvVjQtYJ+V8qXS8nA03RIR3SCqZ5RmW6ogjosVPJCr/SmVJWJ8qg8fVOVVJJzQL+W8vwFl9qR7g==</latexit><latexit sha1_base64="dvRKB5jdkaiTkXTXuNNRC/rfZMY=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICXRgh4LPvAiVOhL2lI26TYuzYskLbSlB/Hqzav+MP0tHvyypoIW6YbNzH7zzbczO4ZvizDStPeUsrS8srqWzmTXNza3tnM7u7XQGwQmr5qe7QUNg4XcFi6vRiKyecMPOHMMm9eN/kUcrw95EArPrUQjn7cdZrmiJ0wWAbpv+aIz0Y9Pp51cXitocqnzjp44eUpW2ct9UIu65JFJA3KIk0sRfJsYhfiapJNGPrA2TYAF8ISMc5pSFrkDsDgYDGgffwunZoK6OMeaocw2cYuNHSBTpUPsa6logB3fyuGHsJ/YY4lZ/94wkcpxhSNYA4oZqXgLPKIHMBZlOglzVsvizLiriHp0LrsRqM+XSNyn+aNziUgArC8jKl1JpgUNQ56HeAEXtooK4leeKaiy4y4sk5ZLFTdRZNALYOPXRz0Ys/53qPNO7aSgawX9rpgvFZOBp2mfDugIUz2jEt1QGXWYqOSFXulNqShj5VF5+qYqqSRnj34t5fkLmkGR7w==</latexit><latexit sha1_base64="Zn9EyRxiJbR7oqgW9/UHvQMKokc=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICWRgh4LPvCiVOhL2lKSdBuX5kWyLbSlB/Hqzav+MP0tHvyypoIW6YbNzH7zzbczO2bg8Eho2ntKWVpeWV1LZ7LrG5tb27md3VrkD0KLVS3f8cOGaUTM4R6rCi4c1ghCZrimw+pm/zyO14csjLjvVcQoYG3XsD3e45YhAN23At6Z6Me3004urxU0udR5R0+cPCWr7Oc+qEVd8smiAbnEyCMB3yGDInxN0kmjAFibJsBCeFzGGU0pi9wBWAwMA2gffxunZoJ6OMeakcy2cIuDHSJTpUPsK6logh3fyuBHsJ/YY4nZ/94wkcpxhSNYE4oZqXgDXNADGIsy3YQ5q2VxZtyVoB6dyW446gskEvdp/ehcIBIC68uISpeSaUPDlOchXsCDraKC+JVnCqrsuAtrSMukipcoGtALYePXRz0Ys/53qPNO7aSgawX9rpgvFZOBp2mfDugIUz2lEl1TGXVYqOSFXulNqShj5VF5+qYqqSRnj34t5fkL2x6SCg==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="bU2aoiD1hG+bww6/5l2U5Ni1Ny4=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjQihehon1AWyRJtzU0LzbbYi0F8erNq/4x/S0e/HZNBS3SDZuZ/eabb2d2nNj3EmGa7xltYXFpeSWby6+urW9sFra2a0k04C6rupEf8YZjJ8z3QlYVnvBZI+bMDhyf1Z3+qYzXh4wnXhTeiFHM2oHdC72u59oCUKMl2L0Ylye3haJpmGrps46VOkVKVyUqfFCLOhSRSwMKiFFIAr5PNiX4mmSRSTGwNo2BcXieijOaUB65A7AYGDbQPv49nJopGuIsNROV7eIWH5sjU6d97HOl6IAtb2XwE9hP7AeF9f69YayUZYUjWAeKOaV4CVzQHRjzMoOUOa1lfqbsSlCXTlQ3HuqLFSL7dH90zhDhwPoqolNZMXvQcNR5iBcIYauoQL7yVEFXHXdgbWWZUglTRRt6HFa+PurBmK2/Q511aoeGZRrW1VGxZKQDz9Iu7dEBpnpMJbqgCuqQ03yhV3rTrrWR9qg9fVO1TJqzQ7+W9vwFD1uSIQ==</latexit><latexit sha1_base64="e42w+peHwDYlJrr1/t4WluWSwoQ=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjwgR6EivYBbZEk3dbQvNhsi7UUxKs3r/rH9Ld48Ns1FbRIN2xm9ptvvp3ZcWLfS4Rpvme0ufmFxaVsLr+8srq2XtjYrCZRn7us4kZ+xOuOnTDfC1lFeMJn9ZgzO3B8VnN6xzJeGzCeeFF4I4YxawV2N/Q6nmsLQPWmYPdidDG+LRRNw1RLn3as1ClSuspR4YOa1KaIXOpTQIxCEvB9sinB1yCLTIqBtWgEjMPzVJzRmPLI7YPFwLCB9vDv4tRI0RBnqZmobBe3+NgcmTrtYp8pRQdseSuDn8B+Yj8orPvvDSOlLCscwjpQzCnFS+CC7sCYlRmkzEktszNlV4I6dKS68VBfrBDZp/ujc4IIB9ZTEZ1OFbMLDUedB3iBELaCCuQrTxR01XEb1laWKZUwVbShx2Hl66MejNn6O9Rpp7pvWKZhXR0US0Y68Cxt0w7tYaqHVKJzKqMOOc0XeqU37Vobao/a0zdVy6Q5W/Rrac9fGPeSJQ==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="KUJbB/akks5kM6qJ6V0KBP0YKbc=">AAAC7XichVFNa9tAEH1WkjZ209ZJj70Ym0IOwUglkB4DbUIvhTTEdsA2ZiWvncX6QlobXOF7/0Bvodfeem3/SvtbcsjbjRJIQ8mK1bx9M/N2ZsdPQ5Vr1/1TcdbWN5483azWnm09f/Gyvr3TzZN5FshOkIRJdu6LXIYqlh2tdCjP00yKyA9lz5+9N/7eQma5SuIzvUzlMBLTWE1UIDSpUb05SOg22cUgEvoiEGFxulqNikGqRoW35xHXW27btavxEHglaKFcJ0n9LwYYI0GAOSJIxNDEIQRyfn14cJGSG6IglxEp65dYocbcOaMkIwTZGf9TnvolG/NsNHObHfCWkDtjZgNvuI+tos9oc6skzmmvuL9YbvrfGwqrbCpc0vpUrFrFT+Q1LhjxWGZURt7W8nim6Upjgne2G8X6UsuYPoM7nQ/0ZORm1tPAkY2cUsO35wVfIKbtsALzyrcKDdvxmFZYK61KXCoK6mW05vVZD8fs/TvUh6D7tu25be/zfutwvxz4Jl6jiV1O9QCH+IgT1hHgK37iF347ifPNuXS+34Q6lTLnFe4t58c1Dn+dpw==</latexit><latexit sha1_base64="8pGKKGwbVJAsKytKWGl9423OQuc=">AAAC7XichVFNa9tAEH1W0zZOv9z2mIuxKfRQjBQC7TGQtuQSSEIcG2JjVsraXqwvpLXBEb73D/RWes0t1+avJL8lh7zdyIHWlKxYzds3M29ndvw0VLl23euK82Tt6bPn69WNFy9fvX5Te/vuJE+mWSDbQRImWdcXuQxVLNta6VB200yKyA9lx5/sGn9nJrNcJfGxnqeyH4lRrIYqEJrUoNboJXSb7KIXCT0ORFgcLRaDopeqQeF92iKuNd2Wa1d9FXglaKJcB0ntBj2cIUGAKSJIxNDEIQRyfqfw4CIl10dBLiNS1i+xwAZzp4ySjBBkJ/yPeDot2Zhno5nb7IC3hNwZM+v4wP3dKvqMNrdK4pz2lvvccqP/3lBYZVPhnNanYtUq7pPXGDPiscyojFzW8nim6UpjiC+2G8X6UsuYPoMHna/0ZOQm1lPHNxs5ooZvzzO+QEzbZgXmlZcKddvxGa2wVlqVuFQU1MtozeuzHo7Z+3eoq+Bkq+W5Le9wu7mzXQ58HZto4COn+hk72MMB6wjwA5f4gysncX46v5zf96FOpcx5j7+Wc3EHEOedqA==</latexit><latexit sha1_base64="g804vR+Zy1N80G3c0Ib1HjIhTTQ=">AAAC7XichVHPaxNBFP6ytjZpq6b16CU0FDxI2NWAHgu14kWIYtJCUsLsZpIM2V/sTgrpkrv/gDfptbde67+if4sHv5luBC2SWWbfN99775v35vlpqHLtuj8qzoONzYdb1dr2zu6jx0/qe/u9PJlngewGSZhkZ77IZahi2dVKh/IszaSI/FCe+rNj4z+9kFmukvizXqTyPBKTWI1VIDSpYf1gkNBtsotBJPQ0EGHxabkcFoNUDQvvxSvietNtuXY17gOvBE2Uq5PUf2KAERIEmCOCRAxNHEIg59eHBxcpuXMU5DIiZf0SS2wzd84oyQhBdsb/hKd+ycY8G83cZge8JeTOmNnAIfc7q+gz2twqiXPaX9yXlpv894bCKpsKF7Q+FWtW8QN5jSkj1mVGZeSqlvWZpiuNMd7YbhTrSy1j+gz+6LylJyM3s54GTmzkhBq+PV/wBWLaLiswr7xSaNiOR7TCWmlV4lJRUC+jNa/Pejhm79+h3ge9ly3PbXkf282jdjnwKp7hAM851dc4wnt0WEeAL7jBLb47ifPV+eZc3YU6lTLnKf5azvVvE0+dqQ==</latexit><latexit sha1_base64="XNPUgPlX2k1acpdnflcbLw1egBI=">AAAC7XichVHPaxNBFP6ytprEVqMeewkJggcJu6Wgx0Bb8aLE0rSFpoTZ7SQZsr/YnQTiknv/AW/i1ZtX/Vfq3+LBb6abQltKZ5l933zvvW/em+enocq1615WnEdr64+fVGv1pxubz543Xrw8ypNZFsh+kIRJduKLXIYqln2tdChP0kyKyA/lsT/dNf7jucxylcSHepHKs0iMYzVSgdCkho3WIKHbZBeDSOhJIMLiYLkcFoNUDQvv7WfiRtvtuHY17wKvBG2Uq5c0/mKAcyQIMEMEiRiaOIRAzu8UHlyk5M5QkMuIlPVLLFFn7oxRkhGC7JT/MU+nJRvzbDRzmx3wlpA7Y2YTr7k/WEWf0eZWSZzT/uP+arnxvTcUVtlUuKD1qVizip/Ia0wY8VBmVEauank403SlMcJ7241ifallTJ/Btc4ePRm5qfU0sW8jx9Tw7XnOF4hp+6zAvPJKoWk7PqcV1kqrEpeKgnoZrXl91sMxe7eHehccbXc8t+N92Wl3d8qBV7GFFt5wqu/QxUf0WEeAC/zCb/xxEueb8935cRXqVMqcV7ixnJ//AVRHncQ=</latexit><latexit sha1_base64="PdKhvUaNfE6D002IPRe2pS+5hcw=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICUpBT0WfOBFqNCXtKUk6TYuzYtNWmhLD+LVm1f9YfpbPPhlTQUt0g2bmf3mm29ndszA4WGkae8pZWV1bX0jnclubm3v7Ob29uuhPxQWq1m+44umaYTM4R6rRTxyWDMQzHBNhzXMwUUcb4yYCLnvVaNxwDquYXu8zy0jAnTfDnh3WjzVZ91cXitocqmLjp44eUpWxc99UJt65JNFQ3KJkUcRfIcMCvG1SCeNAmAdmgIT8LiMM5pRFrlDsBgYBtAB/jZOrQT1cI41Q5lt4RYHWyBTpWPsa6logh3fyuCHsJ/YE4nZ/94wlcpxhWNYE4oZqXgLPKIHMJZluglzXsvyzLiriPp0LrvhqC+QSNyn9aNziYgANpARla4k04aGKc8jvIAHW0MF8SvPFVTZcQ/WkJZJFS9RNKAnYOPXRz0Ys/53qItOvVjQtYJ+V8qXS8nA03RIR3SCqZ5RmW6ogjosVPJCr/SmVJWJ8qg8fVOVVJJzQL+W8vwFl9yR7g==</latexit><latexit sha1_base64="E66nj/wNHLRSp4ZTJ51qZvpYras=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICUpBT0WfOBFqNCXtKUk6TYuzYskLbSlB/Hqzav+MP0tHvx2TQUt0g2bmf3mm29ndszA4VGsae8pZWV1bX0jnclubm3v7Ob29uuRPwwtVrN8xw+bphExh3usFvPYYc0gZIZrOqxhDi5EvDFiYcR9rxqPA9ZxDdvjfW4ZMaD7dsC70+JpcdbN5bWCJpe66OiJk6dkVfzcB7WpRz5ZNCSXGHkUw3fIoAhfi3TSKADWoSmwEB6XcUYzyiJ3CBYDwwA6wN/GqZWgHs5CM5LZFm5xsENkqnSMfS0VTbDFrQx+BPuJPZGY/e8NU6ksKhzDmlDMSMVb4DE9gLEs002Y81qWZ4quYurTueyGo75AIqJP60fnEpEQ2EBGVLqSTBsapjyP8AIebA0ViFeeK6iy4x6sIS2TKl6iaEAvhBWvj3owZv3vUBederGgawX9rpQvl5KBp+mQjugEUz2jMt1QBXVYqOSFXulNqSoT5VF5+qYqqSTngH4t5fkLmkOR7w==</latexit><latexit sha1_base64="JJR075LuOWu33xLg7aVxFaFVPw8=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICWpBT0WfOBFqNCXtKVs0m1cmhdJWmhLD+LVm1f9YfpbPPhljYIW6YbNzH7zzbczO4ZvizDStLeUsrS8srqWzmTXNza3tnM7u/XQGwYmr5me7QVNg4XcFi6vRSKyedMPOHMMmzeMwXkcb4x4EArPrUZjn3ccZrmiL0wWAbpr+6I7LR6fzLq5vFbQ5FLnHT1x8pSsipd7pzb1yCOThuQQJ5ci+DYxCvG1SCeNfGAdmgIL4AkZ5zSjLHKHYHEwGNAB/hZOrQR1cY41Q5lt4hYbO0CmSofYV1LRADu+lcMPYT+wJxKz/r1hKpXjCsewBhQzUvEGeET3YCzKdBLmdy2LM+OuIurTmexGoD5fInGf5o/OBSIBsIGMqHQpmRY0DHke4QVc2BoqiF/5W0GVHfdgmbRcqriJIoNeABu/PurBmPW/Q5136sWCrhX021K+XEoGnqZ9OqAjTPWUynRNFdRhopJneqFXpapMlAfl8YuqpJKcPfq1lKdPnKqR8A==</latexit><latexit sha1_base64="iFnvrIGRqpETUkVF8T4XFP3Eaac=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwICUpBT0WfOBFqdCXtKUk6TYuzYtkW2hLD+LVm1f9YfpbPPhljYIW6YbNzH7zzbczO2bg8Eho2ltKWVpeWV1LZ7LrG5tb27md3XrkD0OL1Szf8cOmaUTM4R6rCS4c1gxCZrimwxrm4CyON0YsjLjvVcU4YB3XsD3e55YhAN21A96dFo9vZt1cXitocqnzjp44eUpWxc+9U5t65JNFQ3KJkUcCvkMGRfhapJNGAbAOTYGF8LiMM5pRFrlDsBgYBtAB/jZOrQT1cI41I5lt4RYHO0SmSofYl1LRBDu+lcGPYD+wJxKz/71hKpXjCsewJhQzUvEauKB7MBZlugnzu5bFmXFXgvp0KrvhqC+QSNyn9aNzjkgIbCAjKl1Ipg0NU55HeAEPtoYK4lf+VlBlxz1YQ1omVbxE0YBeCBu/PurBmPW/Q5136sWCrhX021K+XEoGnqZ9OqAjTPWEynRFFdRhoZJneqFXpapMlAfl8YuqpJKcPfq1lKdP3YeSCw==</latexit><latexit sha1_base64="bU2aoiD1hG+bww6/5l2U5Ni1Ny4=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjQihehon1AWyRJtzU0LzbbYi0F8erNq/4x/S0e/HZNBS3SDZuZ/eabb2d2nNj3EmGa7xltYXFpeSWby6+urW9sFra2a0k04C6rupEf8YZjJ8z3QlYVnvBZI+bMDhyf1Z3+qYzXh4wnXhTeiFHM2oHdC72u59oCUKMl2L0Ylye3haJpmGrps46VOkVKVyUqfFCLOhSRSwMKiFFIAr5PNiX4mmSRSTGwNo2BcXieijOaUB65A7AYGDbQPv49nJopGuIsNROV7eIWH5sjU6d97HOl6IAtb2XwE9hP7AeF9f69YayUZYUjWAeKOaV4CVzQHRjzMoOUOa1lfqbsSlCXTlQ3HuqLFSL7dH90zhDhwPoqolNZMXvQcNR5iBcIYauoQL7yVEFXHXdgbWWZUglTRRt6HFa+PurBmK2/Q511aoeGZRrW1VGxZKQDz9Iu7dEBpnpMJbqgCuqQ03yhV3rTrrWR9qg9fVO1TJqzQ7+W9vwFD1uSIQ==</latexit><latexit sha1_base64="e42w+peHwDYlJrr1/t4WluWSwoQ=">AAAC0XichVFLS8NAEJ7GV1tfVY9egkXwFBIR9FjwgR6EivYBbZEk3dbQvNhsi7UUxKs3r/rH9Ld48Ns1FbRIN2xm9ptvvp3ZcWLfS4Rpvme0ufmFxaVsLr+8srq2XtjYrCZRn7us4kZ+xOuOnTDfC1lFeMJn9ZgzO3B8VnN6xzJeGzCeeFF4I4YxawV2N/Q6nmsLQPWmYPdidDG+LRRNw1RLn3as1ClSuspR4YOa1KaIXOpTQIxCEvB9sinB1yCLTIqBtWgEjMPzVJzRmPLI7YPFwLCB9vDv4tRI0RBnqZmobBe3+NgcmTrtYp8pRQdseSuDn8B+Yj8orPvvDSOlLCscwjpQzCnFS+CC7sCYlRmkzEktszNlV4I6dKS68VBfrBDZp/ujc4IIB9ZTEZ1OFbMLDUedB3iBELaCCuQrTxR01XEb1laWKZUwVbShx2Hl66MejNn6O9Rpp7pvWKZhXR0US0Y68Cxt0w7tYaqHVKJzKqMOOc0XeqU37Vobao/a0zdVy6Q5W/Rrac9fGPeSJQ==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="rLoE5Bujxtl/2z+Sk6r1j8ziL3s=">AAAC7XichVFNa9tAEH1W0zZOv9z2mIuxKfRQjBQC7TGQtuQSSEIcG2JjVsraXqwvpLXBEb73D/RWes0t1+avJL8lh7zdyIHWlKxYzds3M29ndvw0VLl23euK82Tt6bPn69WNFy9fvX5Te/vuJE+mWSDbQRImWdcXuQxVLNta6VB200yKyA9lx5/sGn9nJrNcJfGxnqeyH4lRrIYqEJrUoNboJXSb7KIXCT0ORFgcLRaDopeqQbH1ySOuNd2Wa1d9FXglaKJcB0ntBj2cIUGAKSJIxNDEIQRyfqfw4CIl10dBLiNS1i+xwAZzp4ySjBBkJ/yPeDot2Zhno5nb7IC3hNwZM+v4wP3dKvqMNrdK4pz2lvvccqP/3lBYZVPhnNanYtUq7pPXGDPiscyojFzW8nim6UpjiC+2G8X6UsuYPoMHna/0ZOQm1lPHNxs5ooZvzzO+QEzbZgXmlZcKddvxGa2wVlqVuFQU1MtozeuzHo7Z+3eoq+Bkq+W5Le9wu7mzXQ58HZto4COn+hk72MMB6wjwA5f4gysncX46v5zf96FOpcx5j7+Wc3EHEOmdqA==</latexit><latexit sha1_base64="K/Zx1oNTs5xVBYC0aG4FTnLFgQM=">AAAC7XichVHPSxtBFP5crSa21VSPvQRDoYcSdkNAj0Jb8VKIxRjBSJjdTOKQ/cXuREiX3PsPeCu9euvV/iv1b+mh34yroKE4y+z75nvvffPePD8NVa5d98+Ss7zyYnWtUl1/+er1xmbtzdZJnkyzQHaDJEyyU1/kMlSx7GqlQ3maZlJEfih7/uSj8fcuZZarJD7Ws1SeR2Icq5EKhCY1qO30E7pNdtGPhL4IRFh8nc8HRT9Vg6L1oUVca7hN1676IvBK0EC5OkntFn0MkSDAFBEkYmjiEAI5vzN4cJGSO0dBLiNS1i8xxzpzp4ySjBBkJ/yPeTor2Zhno5nb7IC3hNwZM+t4x31gFX1Gm1slcU77l/ub5cb/vaGwyqbCGa1PxapV/EJe44IRz2VGZeR9Lc9nmq40Rtiz3SjWl1rG9Bk86HyiJyM3sZ46PtvIMTV8e77kC8S0XVZgXvleoW47HtIKa6VViUtFQb2M1rw+6+GYvadDXQQnrabnNr2jdmO/XQ68grfYwXtOdRf7OESHdQT4jl+4wW8nca6cH87Pu1BnqczZxqPlXP8DE1GdqQ==</latexit><latexit sha1_base64="z37xcO4SwF7vPdAE3vrO7JWqJHs=">AAAC7XichVHPSxtBFP7captorWl79BIMBQ8l7EbBHoW24kVIxaiQSJhdx2TI/mJ3IqRL7v0HvIlXb73qv6J/Sw9+M65CG4qzzL5vvvfeN+/N89NQ5dp17+acV/MLr99UqotLb5ffrdTefzjMk3EWyE6QhEl27ItchiqWHa10KI/TTIrID+WRP/pq/EfnMstVEh/oSSpPIjGI1ZkKhCbVr631ErpNdtGLhB4GIiz2p9N+0UtVv2h93iCuNdyma1d9FnglaKBc7aR2jx5OkSDAGBEkYmjiEAI5vy48uEjJnaAglxEp65eYYpG5Y0ZJRgiyI/4HPHVLNubZaOY2O+AtIXfGzDo+ce9YRZ/R5lZJnNP+4f5pucF/byissqlwQutTsWoV98hrDBnxUmZURj7V8nKm6UrjDF9sN4r1pZYxfQbPOt/oyciNrKeO7zZyQA3fns/5AjFthxWYV35SqNuOT2mFtdKqxKWioF5Ga16f9XDM3r9DnQWHrabnNr0fm43tzXLgFaxiDeuc6ha2sYs26wjwC79xg1sncS6cS+fqMdSZK3M+4q/lXD8AFbmdqg==</latexit><latexit sha1_base64="11PboPdyJDsAnRLWeU8CFBpwwzI=">AAAC7XichVHPSxtBFP5ca5torakevQRDwUMJu0HQo9BWvLREMSokEmbXMRmyv9idCOmSu/+AN/Ham9f2X6l/iwe/GddCG4qzzL5vvvfeN+/N89NQ5dp1f885868WXr+pVBeX3i6/W6m9Xz3Ok3EWyE6QhEl26otchiqWHa10KE/TTIrID+WJP/pk/CeXMstVEh/pSSrPIjGI1YUKhCbVr230ErpNdtGLhB4GIiwOp9N+0UtVv2h9/EZca7hN1676LPBK0EC52kntHj2cI0GAMSJIxNDEIQRyfl14cJGSO0NBLiNS1i8xxSJzx4ySjBBkR/wPeOqWbMyz0cxtdsBbQu6MmXV84N6zij6jza2SOKd94P5uucF/byissqlwQutTsWoVv5LXGDLipcyojHyu5eVM05XGBXZsN4r1pZYxfQZ/dD7Tk5EbWU8dX2zkgBq+PV/yBWLaDiswr/ysULcdn9MKa6VViUtFQb2M1rw+6+GYvX+HOguOW03PbXoHW43drXLgFaxjA5uc6jZ2sY826whwhTv8xC8nca6dG+f2KdSZK3PW8NdyfjwCVrGdxQ==</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit><latexit sha1_base64="h6y2ZmFOnY2ox2tebxYybKrqYx0=">AAAC0nichVFLS8NAEJ7GV1tfVY9egkXwVBIp6LHgAy9Chb6kLbJJtzV08yDZlj7wIF69edUfpr/Fg1/WVNAi3bCZ2W+++XZmxwqEE0nDeE9pS8srq2vpTHZ9Y3NrO7ezW4v8QWjzqu0LP2xYLOLC8XhVOlLwRhBy5lqC163+WRyvD3kYOb5XkeOAt13W85yuYzMJ6HbUEh1fRvroLpc3CoZa+rxjJk6eklX2cx/Uog75ZNOAXOLkkYQviFGEr0kmGRQAa9MUWAjPUXFOD5RF7gAsDgYD2se/h1MzQT2cY81IZdu4RWCHyNTpEPtSKVpgx7dy+BHsJ/ZEYb1/b5gq5bjCMawFxYxSvAYu6R6MRZluwpzVsjgz7kpSl05VNw7qCxQS92n/6JwjEgLrq4hOF4rZg4alzkO8gAdbRQXxK88UdNVxB5Ypy5WKlygy6IWw8eujHozZ/DvUead2XDCNgnlTzJeKycDTtE8HdISpnlCJrqiMOmxU8kKv9KZVtIn2qD19U7VUkrNHv5b2/AW255Jl</latexit>ous results that can compete with human-produced solutions [22], including
system controllers [20, 26], game playing [14, 28], and robotics [12, 19].

GP encodes computer programs as sets of genes and then modiﬁes (evolves)
them using a so-called genetic algorithm (GA) to drive the optimization of
the population, by applying selection and reproduction to the population.
The basis for both concepts is a ﬁtness value, which represents the quality
of performing the predeﬁned task for each individual. Selection means, that
only the best portion of the current generation will survive each iteration
and continue existing in the next generation. Analogous to biological sex-
ual breeding, two individuals are selected for reproduction based on their
ﬁtness, and two oﬀspring individuals are created by crossing their chromo-
somes. Technically, this is realized by selecting compatible cutting points in
the function trees and subsequently interchanging the subtrees beneath these
cuts. The two resulting individuals are introduced to the population of the
next generation. Herein, we applied tournament selection [7] for selecting
the individuals to be crossed.

To rate the quality of each policy candidate, a ﬁtness value has to be
provided in order for the GP algorithm to advance. For GPRL, the ﬁtness of
each individual is calculated by generating trajectories using the model-based
return estimation from Eq. (5).

The overall GA used in the experiments is given as follows:

1. Randomly initialize the population of size N

2. Determine ﬁtness value of each individual using Eq. (5) (in parallel)

3. Evolve next generation

(a) Crossover (depending on crossover ratio rc)

i. Select individuals by tournament selection
ii. Cross two tournament winners
iii. Add resulting individuals to new population

(b) Reproduction (depending on reproduction ratio rr)

i. Select individuals by tournament selection
ii. Add tournament winner to new population

(c) Automatic cancelation and terminal mutation (depending on

auto cancel ratio ra and terminal mutation ration rm)
i. Apply automatic cancelation on all individuals
ii. Add canceled individuals according to ra

9

iii. Select best individuals for each complexity level of old popu-

lation

iv. Randomly mutate ﬂoat terminals using normal distribution
N : z(cid:48) ∼ z + 0.1z · N (0, 1), where z and z(cid:48) are the original and
the mutated ﬂoat terminals, respectively; and create N · ra
adjusted individuals from each best

v. Determine ﬁtness value of each individual (in parallel)
vi. Add best adjusted individuals to new population according

to rm

(d) Fill new population with new randomly generated individuals

(new individuals ratio rn)

(e) Determine ﬁtness value of each individual using Eq. (5) (in

parallel)

(f) If none of the stopping criteria is met

i. Go back to 3.

4. Return best individual found so far for each complexity level

For the experiments described in Section 4, the following new population
ratios have been used: rc = 0.45, rr = 0.05, rm = 0.1, ra = 0.1, rn =
0.3. Stopping criteria can be a maximum number of iterations, a certain
algorithm runtime, a pre-deﬁned performance value, etc. In our experiments,
we stopped the GA after 100 iterations.

GPRL has been implemented using the open source evolutionary com-
putation framework DEAP2 which provides an excellent and widely-used
Python implementation of genetic programing using preﬁx trees [13]. A de-
tailed explaination of the GA utilized in GPRL, including a description of
automatic cancelation and parameter motivation, can be found in [16].

4

Interpretable process controllers for a Chylla-
Haase polymerization reactor

The framework of learning interpretable process controllers contains two ML
steps. Firstly, available data from the process is used to generate a process
model. Here, we used supervised ML to train weights of a recurrent neural
network as system identiﬁcation. Secondly, GPRL is performed on this re-
current model to produce several Pareto-optimized process controllers. The

2https://github.com/DEAP/deap

10

Figure 3: EPC of generating interpretable process controllers using GPRL.
Note that the Domain expert is integrated into the optimization loop. How-
ever, generating interpretable control strategies is fully automatized.

controllers are optimized towards the process KPIs utilizing the platform-
speciﬁc process control building blocks. An event-driven process chain (EPC)
ﬂowchart of applying GPRL for process optimization is depicted in Fig. 3.

4.1 Recurrent system identiﬁcation

To generate a batch data set D with suﬃcient experience, 100 diﬀerent explo-
ration process recipes have been used. In the classic control strategy, using
a PID temperature controller, the controller receives a constant setpoint. As
the controller needs to handle both, heating and cooling during the produc-
tion, the performance is not optimal and signiﬁcant control deviations could
be observed. This will be shown in Section 4.4 together with the results. For
safety and acceptance reasons, GPRL is not intended to replace the PID con-
troller but to be superimposed on it. Therefore, the setpoint temperature ˆT

11

ProcessProcess dataSupervised ML regressionProcess surrogate modelGPRLProcess KPIsController building blocksControl strategiesControllerDomain expertValidation and selectionSelected strategyof the PID controller becomes the action of GPRL and a new overall setpoint
S is introduced (see Table 1). Modifying ˆT reveals new degrees of freedom
not used before in the operation of the reactor. Therefore, new training data
needs to be created wherein ˆT is changed. For our experiments, the set-
point has been changed once per recipe between 352 and 365 K at a random
batch runtime between 100 and 600 s. In process optimization, such step
attempts are a common method of safely exploring in a bounded subspace
of the control problem.

The response of the process on these changes is then recorded and ex-
ported to the data batch. In the next step, the data is subsampled on a 10 s
grid and then normalized, i.e., average 0 and standard deviation 1.

The system model ˜g is a recurrent neural network (RNN) with the task
to predict the values of st+1 = (Tt+1, Mt+1, Pt+1, U At+1, Qt+1), st+2, . . . , st+F
from the inputs st = (Tt, Mt, Pt, U At, Qt), st−1, . . ., st−H+1 and at = ( ˆTt, ˆMt),
at−1, . . . , at−H+1. The RNN is unfolded H = 10 time steps into the past and
F = 10 time steps into the future. In each time step, the observable variables
of the past and present are inputs. Whereas, in the future branch of the RNN
only the control variables ˆT and ˆM are used as input. The topology of the
RNN is described in [25] as Markov decision process extraction network. The
model was implemented as a tensorﬂow graph using RNN cells with two
hidden layers and 20 tanh activation neurons on each layer.

Note that an F = 10 overshooting is used, which means that the regres-
sion error used to optimize the network weights, is not only computed on the
next time step, but as an average error over the next 10 time steps. In our
experience, this generally helps to yield more robust predictions for models
which are used in closed loop simulations, where predictions have to be made
on the models own, possibly inaccurate, previous predictions.

After 10,000 training episodes, the quadratic loss of the prediction has
fallen to 0.0017 on the trainings set, 0.0022 on the validations set, and 0.0021
on the generalization set (see Fig. 4). The sets contained 70, 20, and 10 step
attempt time series for training, validation, and generalization, respectively.
Fig. 5 depicts the average absolute loss for the predicted reactor temperature
T over the 10 overshooting time steps. It is shown, how the prediction quality
is decreasing for more distant time steps. However, a validation error increase
from 0.019 to 0.026 on a prediction horizon of 100 seconds (10 time steps)
still yields an adequate system model.

4.2 Learning an interpretable process controller

The RNN, trained in the previous step, can now be used as model ˜g together
with the reward function ˜r to rate the performance of policy candidates. In

12

Figure 4: Learning curve of the RNN surrogate model training

Figure 5: Prediction error of the trained model RNN surrogate model over
ten future time steps

13

case of the temperature control task we consider here, the reward is computed
by ˜r(st) = rt = −(S − Tt)2, where St is the desired setpoint and Tt is the
predicted reactor temperature at time step t. According to Eq. (4), the
controller parameters x are tested on 100 starting states S drawn from the
data batch D. The trajectories produced by closed loop evaluation of policy
π with parameters x and system model ˜g, have a length of T = 10. Hence,
the performance of x can be estimated by Eq. (5).

The building blocks of the genetic programming algorithm are basic alge-
braic functions {+, −, ∗, /}, state and action variables {s, a}, and dead time
blocks storing past values of the variables up to 50 seconds {(cid:3)t, (cid:3)t−10, . . . ,
(cid:3)t−50}, with (cid:3) an arbitrary variable from s or a. Here, we are searching for a
policy that serves as a setpoint process controller which dynamically changes
setpoint ˆT to minimize the distance between the desired setpoint S and the
actual reactor temperature T . The monomer setpoint ˆM has been ﬁxed to
the value 0.015. Note that the policy can incorporate state and action values
from the past and is evaluated in a closed loop manor for ten time steps
during training. Consequently, it will not greedily try to reach the desired
setpoint in the next step, but in the long run try to avoid overshooting and
maximize the reward instead.

After 100 iterations of GPRL with 500 individuals in each generation, the
performance of the best policies found are visualized in Fig. 6. It is easy to
see that individuals of lower complexity have a higher penalty (lower reward)
compared to individuals of higher complexity. However, the estimated per-
formance seems to not improve signiﬁcantly for individuals of complexity 10
or higher. After discussion with domain experts, an individual of complexity
9 and an estimated penalty of around 392 has been selected (highlighted in
red). The selected individual can be represented as the following equation:

ˆTt = Tt−30 − 2Tt + 2S − 1.

(6)

This equation can easily be interpreted thanks to its simple structure.
The reactor temperature setpoint is dynamically changed with respect to
the reactor temperature 30 s ago, the current temperature, and the intended
setpoint. Note that all variables (T , ˆT , and S) in the equation have been
normalized by average 359.12 and standard deviation 6.47.

4.3 SIMATIC PCS 7 integration

The policy found by GPRL and represented by Eq. (6), can easily be imple-
mented in SIMATIC PCS 7, since GPRL utilized only components from the
pre-deﬁned building blocks which are available in the target system. Fig. 7

14

Figure 6: Pareto front generated by GPRL. According to the domain expert,
the selected policy (red dot) is a good compromise between setpoint penalty
(392) and complexity (9).

shows a screenshot of the respective structured control language (SCL) code.
Note, that since this is the native language of the system at hand, the new
policy is easy to integrate and fast in execution. Domain experts are able
to fully understand the code and can easily make adjustments. All of this
would be impossible if we were to use a black-box policy.

After generating the SCL code, the new control strategy can be loaded
into a standard continuous function chart (CFC) building block. Fig. 8 shows
the CFC diagram with our new policy block (FB10) and the additional dead
time block (DeadTime) with 30 s below. The policy is now fully integrated
into the automation system and can be evaluated.

Note that for applying the function to a real production plant, some
safety logic and switching between diﬀerent modes like manual and automatic
would be required. Integration of this is straight forward and neglected for
the following evaluation using the simulation model.

4.4 Evaluation

To evaluate and visualize the performance of the policy, we tested it on
four exemplary intended setpoints, i.e., 352, 358, 362, 365 K. The intended
setpoint S is plotted in red. The action of the policy ˆT is plotted in blue.
The resulting reactor temperature T , using the default PID regulator ˆT = S,
is plotted in orange. The resulting reactor temperature T , using the new RL

15

05101520253035complexity380390400410420430440450setpoint penaltyFigure 7: SCL script with the implemented GPRL control strategy

16

Figure 8:
GPRL policy

CFC diagram with the building block (FB10) containing the

policy given in Eq. (6), is plotted in green.

Fig. 9 depicts the results for S = 352 K. In the beginning the RL policy
changes the temperature setpoint to the maximum value of 365 K, to speed
up the heating process of the reactor. After approximately 80 s, it reduces
the setpoint to the minimum value of 352 K, to keep the reactor temperature
as close as possible to the intended setpoint. However, since the setpoint can
not be lowered below the minimum of 352 K, the reactor is constantly above
the intended setpoint.
In this example, the new RL policy could slightly
reduce control deviation by 0.3 %.

A real improvement can be seen for intended setpoint S = 358 K. Fig. 10
shows that the default controller produces huge overshooting, whereas the
new RL policy stays close to the intended setpoint over the whole batch
time. Again, the RL policy is increasing the setpoint to the maximum at the
beginning just to reduce it to the minimum after 100 s. After 400 s we can
see that the RL policy is starting to slightly increase the setpoint again to
counteract the decrease in reactor temperature at the end of the batch. The
control deviation can be reduced by 33.5 %.

In Fig. 11, we see a similar behavior for intended setpoint S = 362 K.
The new RL policy changes the setpoint dynamically over the whole batch
time to reduce control deviation by 37.7 %.

17

Figure 9: Setpoint controls and resulting trajectories for default and GPRL
policies. Results are shown for an intended reactor setpoint of 352 K.

Figure 10: Setpoint controls and resulting trajectories for default and GPRL
policies. Results are shown for an intended reactor setpoint of 358 K.

18

Figure 11: Setpoint controls and resulting trajectories for default and GPRL
policies. Results are shown for an intended reactor setpoint of 362 K.

The ﬁnal example is intended setpoint S = 365 K in Fig. 12. Here,
the new RL policy reduces control deviation by 30.1 %. Note that in this
example, it is very important not to overshoot too much, since depending on
the product in the reactor, too high temperatures can spoil the process and
ruin the quality of the ﬁnal product.

To conclude, we can say that the control deviation has been reduced for all
intended setpoints tested. The policy was automatically learned from safely
generated batch data, easy to implement, and reduced control deviation by
up to 37 %.

5 Conclusion

The experiments using GPRL on a SIMATIC PCS 7 polymerization reactor
template show that it is possible to generate human-interpretable policies
from existing process data fully automatically. In contrast to widely-known
RL methods, which produce black-box value functions and/or policies, with
GPRL, the domain experts can stay in the optimization loop, since the re-
sults are convenient to validate and implement. Utilizing such trustworthy
AI methods will help bring state-of-the-art ML algorithms into real-world
industry applications, to leverage the optimization potentials which are to
be expected in many domains.

19

Figure 12: Setpoint controls and resulting trajectories for default and GPRL
policies. Results are shown for an intended reactor setpoint of 365 K.

Acknowledgments

The project this report is based on was supported with funds from the
German Federal Ministry of Education and Research under project num-
ber 01IS18049A. The sole responsibility for the report’s contents lies with
the authors.

References

[1] Siemens AG. Pcs 7 unit template ”stirred tank reactor with kalman
ﬁlter” using the example of the chemical industry, 2018. URL https:
//support.industry.siemens.com/cs/de/de/view/109756215.

[2] K.J. ˚Astr¨om and T. H¨agglund. The future of pid control. Control

engineering practice, 9(11):1163–1175, 2001.

[3] K.J. ˚Astr¨om and T. H¨agglund. Revisiting the ziegler–nichols step re-
sponse method for pid control. Journal of process control, 14(6):635–650,
2004.

[4] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, and
W. Samek. On pixel-wise explanations for non-linear classiﬁer decisions
by layer-wise relevance propagation. PloS one, 10(7):e0130140, 2015.

20

[5] A. Bhat and R.N. Banavar. The chylla-haase problem: a neural network
controller. In Proceedings of the 1998 IEEE International Conference on
Control Applications (Cat. No. 98CH36104), volume 1, pages 192–196.
IEEE, 1998.

[6] B. Bischoﬀ, D. Nguyen-Tuong, T. Koller, H. Markert, and A. Knoll.
Learning throttle valve control using policy search.
In Joint Euro-
pean Conference on Machine Learning and Knowledge Discovery in
Databases, pages 49–64. Springer, 2013.

[7] T. Blickle and L. Thiele. A mathematical analysis of tournament selec-

tion. In ICGA, pages 9–16, 1995.

[8] H.S. Chang, J. Hu, M.C. Fu, and S.I. Marcus. Population-based evolu-
tionary approaches. In Simulation-Based Algorithms for Markov Deci-
sion Processes, chapter 3, pages 61–87. Springer, 2007.

[9] H.H. Chin and A.A. Jafari. Genetic algorithm methods for solving the
best stationary policy of ﬁnite Markov decision processes.
In System
Theory, 1998. Proceedings of the Thirtieth Southeastern Symposium on,
pages 538–543. IEEE, 1998.

[10] L. Desborough and R. Miller. Increasing customer value of industrial
control performance monitoring - Honeywell’s experience.
In AIChE
symposium series, number 326, pages 169–189. New York; American
Institute of Chemical Engineers; 1998, 2002.

[11] F. Doshi-Velez and B. Kim. Towards a rigorous science of interpretable

machine learning. arXiv preprint arXiv:1702.08608, 2017.

[12] K.L. Downing. Adaptive genetic programs via reinforcement learning. In
Proceedings of the 3rd Annual Conference on Genetic and Evolutionary
Computation, GECCO’01, pages 19–26, San Francisco, CA, USA, 2001.
Morgan Kaufmann Publishers Inc.

[13] F.-A. Fortin, F.-M. De Rainville, M.-A. Gardner, M. Parizeau, and
C. Gagn´e. Deap: Evolutionary algorithms made easy. The Journal
of Machine Learning Research, 13(1):2171–2175, 2012.

[14] C. Gearhart. Genetic programming as policy search in Markov deci-
sion processes. In J. R. Koza, editor, Genetic Algorithms and Genetic
Programming at Stanford, pages 61–67. Stanford Bookstore, Stanford,
USA, 2003.

21

[15] F. Gomez, J. Schmidhuber, and R. Miikkulainen. Eﬃcient non-linear
In European Conference on Machine

control through neuroevolution.
Learning, pages 654–662. Springer, 2006.

[16] D. Hein. Interpretable Reinforcement Learning Policies by Evolutionary
Computation. PhD thesis, Technische Universit¨at M¨unchen, 2019.

[17] D. Hein, S. Udluft, and T.A. Runkler. Interpretable policies for rein-
forcement learning by genetic programming. Engineering Applications
of Artiﬁcial Intelligence, 76:158–169, 2018.

[18] D. Hein, S. Limmer, and T.A. Runkler. Interpretable control by rein-
forcement learning. IFAC-PapersOnLine, 53(2):8082–8089, 2020. 21th
IFAC World Congress.

[19] S. Kamio and H. Iba. Adaptation technique for integrating genetic
programming and reinforcement learning for real robots. Trans. Evol.
Comp, 9(3):318–333, June 2005.

[20] M.A. Keane, J.R. Koza, and M.J. Streeter. Automatic synthesis using
genetic programming of an improved general-purpose controller for in-
dustrially representative plants. In Proceedings of the 2002 NASA/DoD
Conference on Evolvable Hardware (EH’02), EH ’02, pages 113–123,
Washington, DC, USA, 2002. IEEE.

[21] J.R. Koza. Genetic Programming: On the Programming of Computers
by Means of Natural Selection. MIT Press, Cambridge, MA, USA, 1992.

[22] J.R. Koza. Human-competitive results produced by genetic program-
ming. Genetic Programming and Evolvable Machines, 11(3):251–284,
2010.

[23] F. Maes, R. Fonteneau, L. Wehenkel, and D. Ernst. Policy search in a
space of simple closed-form formulas: Towards interpretability of rein-
forcement learning. Discovery Science, pages 37–50, 2012.

[24] M.T. Ribeiro, S. Singh, and C. Guestrin. Why should I trust you?:
Explaining the predictions of any classiﬁer. In Proceedings of the 22nd
ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pages 1135–1144. ACM, 2016.

[25] A.M. Sch¨afer and S. Udluft. Solving partially observable reinforcement
learning problems with recurrent neural networks. In Workshop Proc.
of the European Conf. on Machine Learning, pages 71–81, 2005.

22

[26] H. Shimooka and Y. Fujimoto. Generating equations with genetic pro-
gramming for control of a movable inverted pendulum. In Selected Pa-
pers from the Second Asia-Paciﬁc Conference on Simulated Evolution
and Learning on Simulated Evolution and Learning, SEAL’98, pages
179–186, London, UK, 1999. Springer-Verlag.

[27] T. Wang, C. Rudin, F. Velez-Doshi, Y. Liu, E. Klampﬂ, and P. Mac-
Neille. Bayesian rule sets for interpretable classiﬁcation. In Data Min-
ing (ICDM), 2016 IEEE 16th International Conference on, pages 1269–
1274. IEEE, 2016.

[28] D.G. Wilson, S. Cussat-Blanc, H. Luga, and J.F. Miller. Evolving sim-
ple programs for playing Atari games.
In Proceedings of the Genetic
and Evolutionary Computation Conference, GECCO ’18, pages 229–236,
New York, NY, USA, 2018. ACM.

23

