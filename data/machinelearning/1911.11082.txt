0
2
0
2

y
a
M
4

]
L
M

.
t
a
t
s
[

2
v
2
8
0
1
1
.
1
1
9
1
:
v
i
X
r
a

A New Distribution-Free Concept for
Representing, Comparing, and Propagating
Uncertainty in Dynamical Systems with
Kernel Probabilistic Programming (cid:63)

Jia-Jie Zhu ∗ Krikamol Muandet ∗ Moritz Diehl ∗∗
Bernhard Sch¨olkopf ∗

∗ Empirical Inference Department,
Max Planck Institute for Intelligent Systems, T¨ubingen, Germany.
(e-mail: {jzhu, krikamol, bs}@tuebingen.mpg.de)
∗∗ Department of Microsystems Engineering,
University of Freiburg, Freiburg, Germany.
(e-mail: moritz.diehl@imtek.uni-freiburg.de)

Abstract: This work presents the concept of kernel mean embedding and kernel probabilistic
programming in the context of stochastic systems. We propose formulations to represent,
compare, and propagate uncertainties for fairly general stochastic dynamics in a distribution-free
manner. The new tools enjoy sound theory rooted in functional analysis and wide applicability
as demonstrated in distinct numerical examples. The implication of this new concept is a new
mode of thinking about the statistical nature of uncertainty in dynamical systems.

1. INTRODUCTION

Classic stochastic control methods such as LQG hinge
on the mathematical fact that the family of Gaussian
distributions is closed under an aﬃne transformation.
This allows uncertainty to be propagated in a tractable
manner under the Gaussianity assumption. Robust control
methods, such as the classic tube model predictive control,
also rely on the linearity of dynamics to propagate the
polytopic uncertainty. However, when we move beyond
those assumptions to the territories of nonlinear dynamics
and non-Gaussian noise, uncertainty propagation becomes
diﬃcult or even intractable.

A central component in robust and stochastic control is
how to represent the system uncertainty. To this end, point
estimate, ellipsoidal uncertainty set, Gaussian distribu-
tion, or randomized computer simulations have been used
in various applications. Along with those, a few families
of uncertainty propagation methods have been proposed,
e.g., generalized polynomial chaos approximation, Gaus-
sian processes. The representation and propagation aﬀect
control as well as system identiﬁcation. For example, a
parameter estimation problem typically uses the likelihood
function as its criterion for goodness-of-ﬁt, which requires
assuming the distribution family.

Consider an illustrative sketch in Figure 1. How can we
measure the diﬀerences between system models? If the
dynamical system of interest is deterministic (left), then

(cid:63) This project has received funding from the European Unions
Horizon 2020 research and innovation programme under the Marie
Skodowska-Curie grant agreement No 798321, the German Federal
Ministry for Economic Aﬀairs and Energy (BMWi) via eco4wind
(0324125B) and DyConPV (0324166B), and by DFG via Research
Unit FOR 2401.

Fig. 1. An illustrative example of deterministic (left)
and stochastic (right) dynamical systems. Notice the
system in (right, red) is stochastic and bi-modal.

we can simply compare the solutions of the systems in
the sense of Euclidean distance. Now we consider Figure 1
(right), which illustrates the evolution of two stochastic dy-
namical systems (with arbitrary distribution). Due to the
stochasticity, the solutions are distributions. Comparison
in Euclidean distance is no longer feasible. How do we in-
corporate statistical information without imposing strong
distributional assumptions? The question we address is
how to represent, compare, and propagate the randomness
in dynamical systems in a quantitative way.

This work proposes to embed uncertain state distributions
into the reproducing kernel Hilbert spaces (RKHS), en-
abling quantiﬁcation and algebraic operations therein. The
main contributions of this work are the following.

• We introduce the RKHS embedding method of kernel
probabilistic programming in the context of uncertain
dynamical systems. The resulting formulations of rep-
resentation, comparison, and propagation methods
are our main contributions.

 
 
 
 
 
 
• Through distinct numerical examples, we demon-
strate the ﬂexible and distribution-free nature of our
method as a unifying tool for dynamical systems.
• We propose a recursive reduced set method to prop-
agate the system uncertainty by iteratively solving
an optimization problem. This forms ﬁxed-size kernel
expansions rather than allowing the number of uncer-
tainty realizations to explode over time.

Notation:
In this work, the state of uncertain dynamical
systems are denoted by x(τ, ξ), where τ is time and ξ is
the uncertainty in the system, e.g., a disturbance process.
The Stieltjes integral (cid:82) · dP denotes the integration w.r.t.
either a continuous or a discrete probability measure
P . Symbol X often denotes a nonempty set and H a
reproducing kernel Hilbert space (RKHS). We write ξ ∼ P
to denote that the random variable (RV) ξ follows the
distribution law P .

Another well-developed methodology in Bayesian statistics
is the Gaussian process (GP). Intuitively, GP generalizes
the Gaussian distribution to the distribution of functions.
For example, dynamics described by a diﬀerence equation
can be modeled by a GP prior, i.e., xt+1 − xt = f (xt) ∼
GP(µ(xt), σ(xt)) where xt is a shorthand for x(t, ξ). Given
data samples, it can be shown that the posterior predictive
distribution for an unseen state is also Gaussian. This
allows us to quantify the distribution of solution x(t, ξ). We
refer to Rasmussen and Williams (2006) for an accessible
introduction.

Mathematically speaking, gPC and GP are both surrogate
function classes enabled by function approximation theory.
This paper does not focus on analyzing the function
approximation aspect. Instead, the embedding method we
shall propose in Section 3 calls for a shift in our ways of
thinking about statistical distributions.

2. BACKGROUND & RELATED WORK

2.2 Goodness-of-ﬁt measure for system identiﬁcation

2.1 Uncertainty quantiﬁcation in dynamical systems

Stochastic dynamical system refers to a system whose
evolution is aﬀected by non-deterministic disturbances.
To illustrate our idea concretely, let us consider a simple
example of stochastic ODE that was studied in Xiu and
Karniadakis (2002):

˙x(t) = ξ x,

x(0) = x0,

(1)
where the parameter ξ is uncertain. This uncertainty may
enter as (a) time-invariant, or, more generally (b) time-
varying stochastic process. For a moment, let us consider
case (a) and the parameter follows a certain distribution
law ξ ∼ Pξ. This is hence an initial value problem (IVP)
with uncertain parameter. The problem of quantifying
the distribution of the solution x(t, ξ) to the IVP (1) is
typically referred to as the uncertainty quantiﬁcation in
dynamical systems.

One somewhat trivial approach of quantifying the solution
is the Monte Carlo sampling that samples {ξ1, ξ2, . . . , ξN }
from the underlying distribution. Then we deterministi-
cally integrate the ODE with those realizations of the
uncertain parameter to obtain solutions {x(t, ξi)}N
i=1, ∀t.
We may later extract the moment statistics of x(t, ξ) by
its Monte Carlo estimation.

In numerical analysis, one representative thread of works
in uncertainty quantiﬁcation is the generalized polynomial
chaos (gPC) method. It expands the solution of IVP
(1) as a series in orthogonal polynomial basis functions
{φi(ξ)}N

i=1,

x(t, ξ) =

∞
(cid:88)

αiφi(ξ).

i=1
This expansion is mathematically elegant in that the basis
φi(ξ)’s account for the stochasticity and the expansion
coeﬃcients αi’s are deterministic. From this point onward,
we can either follow Galerkin’s method propagate the
coeﬃcients αi through the dynamical systems as in Xiu
and Karniadakis (2002), or, use the so-called stochastic
collocation to numerically integrate the IVP (1) at certain
collocation nodes {ξ1, ξ2, . . . , ξN } as in Xiu (2009).

System identiﬁcation studies how to construct mathemat-
ical models of dynamical systems from observed data.
While this paper does not directly propose a system identi-
ﬁcation algorithm, we show how the proposed concept can
impact how we analyze and compare the models in terms
of goodness-of-ﬁt. To give readers a concrete example of
this new concept, we revisit the parameter and variability
estimation (PVE) proposed by Mohammadi et al. (2015)
in Section 4.2. This may be thought of as an alternative
to the least square (LSQ) estimation, whose underlying
assumption is that the system is following an additive
Gaussian distribution with ﬁxed variance, y = f (x) +
ξ, ξ ∼ N ( ¯ξLSQ, σLSQ). In contrast, PVE is based on the
robust optimization idea that the disturbance should lie
within an ellipsoidal uncertainty set ξ ∈ E( ¯ξPVE, ZPVE)
but not making any assumptions on the distribution fam-
ily. The PVE method then formulates the identiﬁcation
of the uncertain ellipsoid as a semideﬁnite programming
(SDP) problem. More details are provided in that paper.

It can be relatively diﬃcult to compare an LSQ point
estimate (or MLE in general) with e.g. PVE because they
do not share the same likelihood . This paper proposes
such a unifying framework of comparing diﬀerent system
models’ goodness-of-ﬁt.

2.3 Reproducing kernel Hilbert space (RKHS) embedding

A kernel is a real-valued bivariate function k(·, ·) : X ×X →
R. It is said to be positive deﬁnite if (cid:80)n
i,j=1 αiαjk(xi, xj) ≥
0 for any n ∈ N, (α1, . . . , αn) ∈ Rn, and (x1, . . . , xn) ∈ X n.
In addition, it is a reproducing kernel of an RKHS H
if (i) ∀x ∈ X , k(x, ·) ∈ H and (ii) ∀x ∈ X , f ∈ H,
f (x) = (cid:104)f, k(x, ·)(cid:105)H. The latter is known as the reproducing
property of H. Choosing f = k(x(cid:48), ·) for some x(cid:48) ∈ X and
applying the reproducing property yield the kernel trick
k(x, x(cid:48)) = (cid:104)k(x, ·), k(x(cid:48), ·)(cid:105)H = (cid:104)φ(x), φ(x(cid:48))(cid:105)H.
(2)
That is, we can view the kernel evaluation k(x, x(cid:48)) as
a generalized similarity measure between x and x(cid:48) after
mapping them into the feature space H. We refer to φ
deﬁned above as a canonical feature map associated with

the kernel k. One of the most common kernels on Rd is the
Gaussian kernel

k(x, x(cid:48)) = exp

(cid:18)

−

1
2σ2 (cid:107)x − x(cid:48)(cid:107)2

2

(cid:19)

,

x, x(cid:48) ∈ Rd,

(3)

where σ > 0 is a bandwidth parameter.

An important application of kernel methods is in repre-
senting probability measures via kernel mean embedding
(KME) [Smola et al. (2007)]. This line of work can be
thought of as a systematic way of endowing unstructured
data with representations in a Hilbert space to provide
the ability to perform algebraic operations therein. Math-
ematically, we deﬁne the KME of a random variable X as
follows.
Deﬁnition 2.1. (Kernel mean embedding) Given random
(cid:112)k(X, X) < ∞,
variable X ∼ P and kernel k for which EX
we deﬁne the kernel mean embedding of X as a function

(cid:90)

µk

X (·) =

k(x, ·) dP (x).

This function is a member of the RKHS, µk
associated with the kernel k.

X ∈ H

In the rest of the paper, we follow the convention in
kernel machine learning to simply write the function µk
X (·)
as µX to emphasize that it is an element of the RKHS
H. It has been shown that if k belongs to a class of
kernels known as characteristic kernels, then µX uniquely
determines the distribution P ; see, e.g., Fukumizu et al.
(2004); Sriperumbudur et al. (2010).

To help readers get a concrete understanding, we outline
common kernels and the information their KME preserve
in Table 1. We then give examples of the explicit forms of
the KMEs.

Table 1. Common kernels and what statistical
information their KME preserve. More exam-
ples can be found in Muandet et al. (2017)

Linear
Polynomial

k(x, x(cid:48)) = x(cid:62)x(cid:48)
k(x, x(cid:48)) = (x(cid:62)x(cid:48) + 1)p

Gaussian

k(x, x(cid:48)) = exp

Exponential

k(x, x(cid:48)) = exp(x(cid:62)x(cid:48))

(cid:16)

−

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2

(cid:17)

Mean of distribution
Up to p-th moments

All information

All information

Example (Second-order polynomial kernel embedding)
Suppose the kernel function in question is the polynomial
kernel of order two k(x, x(cid:48)) = (x(cid:62)x(cid:48) + 1)2, the KME is
given by

(cid:90)

µX =

k(x, ·)dP (x) =

(cid:90)

(cid:0)x(cid:62)(·) + 1(cid:1)2

dP (x)

= (·)(cid:62)Exx(cid:62)(·) + 2Ex(cid:62)(·) + 1.

in that it does not seek to use ﬁnite-order truncation
for approximating functions. Rather, we make use of the
kernel trick (2) to represent similarity in data even in
inﬁnite-dimensional feature space. This gives rise to the
power of kernel machine learning.

Example (Exponential kernel embedding) Given the ex-
ponential kernel k(x, x(cid:48)) = e(cid:104)x,x(cid:48)(cid:105), the KME is

(cid:90)

µX =

e(cid:104)x,·(cid:105)dP (x).

This is the moment-generating function. Notably, replac-
ing x by −x yields the Laplace transform.

The following result in Song (2008) gives the consistency
of a sample-based estimator ˆµX for µX .
Lemma 2.1. (Smola et al. (2007); Song (2008); estimator
for KME) Let us denote by X a random variable and
{xi}N

i=1 its i.i.d samples. Then,

ˆµX :=

1
N

N
(cid:88)

i=1

k(xi, ·) → µX

(5)

as N → ∞ with probability 1.

Furthermore, Sch¨olkopf et al. (2015); Simon-Gabriel et al.
(2016) proved the estimation consistency for KME of
transformations of RVs in more general conditions. They
term their approach kernel probabilistic programming
(KPP).
Lemma 2.2. (KME consistency) Suppose f : X → Z is
a continuous function, kx, kz are continuous kernels on X
and Z. Under mild conditions [cf. Theorem 1 of Simon-
Gabriel et al. (2016)] , the following is true.
f (X) → µkz

X , then ˆµkz

X → µkx

If ˆµkx

f (X).

Notably, we do not require samples to be i.i.d. This result
equips us with algebraic tools to learn an embedding of
f (X) directly from that of X. In the rest of the paper,
by KPP we mean the RKHS embedding method that per-
forms algebraic operations on KMEs via transformations
of random samples.

In the context of reinforcement learning, e.g., in Nishiyama
et al. (2012); Gr¨unew¨alder et al. (2012); Boots et al.
(2013), RKHS embeddings of conditional distributions,
which is diﬀerent from ours, were used to learn dynamics.
We share the common thread of using RKHS embeddings
while diﬀer in a few important aspects, e.g., our use of
KPP, numerical integration for continuous-time systems.
See Song et al. (2013) for more details and references.

(4)

3. APPROACH

This shows that the RKHS associated with this KME
consists of quadratic functions whose coeﬃcients preserve
statistical information up to the second order (mean and
variance), but not higher. In general, p-th order polyno-
mial kernel embeddings preserve information up to the p-
th order. A richer kernel embedding, e.g. Gaussian kernel
embedding, may preserve information up to inﬁnite order.

Remark Readers familiar with polynomial approximation
may recognize that the integrand in (4) can be expanded
in certain Wiener-Askey polynomial bases. However, our
method diﬀers from the philosophy of gPC expansion

3.1 Representing uncertainty with RKHS embeddings

KPP introduced in the last section gives us a powerful
tool to represent distributions without any parametric
assumption. In the context of dynamical systems, if we
view the evolution of the system uncertainty as transfor-
mations of random variables (RV), then KPP naturally
becomes a tool to propagate system uncertainty. An im-
portant motivation of our methodology is that it shall be
distribution-free, i.e., it shall not impose assumptions on
the uncertainty distributions (e.g., Gaussianity).

In a nutshell, we represent the distribution of x(τ, ξ), the
state of the dynamical systems (continuous or discrete
time), by its KME and the corresponding sample-based
estimator given by

(cid:90)

µx(τ,ξ) =

k(x(τ, ξ), ·)dP (ξ),

ˆµx(τ,ξ) =

N
(cid:88)

i=1

αik(x(τ, ξi), ·),

(6)

where a simple choice is αi = 1
N . As discussed in Section
2.3, KME with second-order polynomial kernel preserves
(nominal state) and second (variance) order information
commonly used in stochastic control. In this light, we
may view our method as a generalization of Monte Carlo
moment estimation.

3.2 Goodness-of-ﬁt measure for uncertain system models

As suggested in Figure 1 (right), it may be diﬃcult to
quantitatively compare stochastic system models directly.
For example, say we have identiﬁed an LSQ point estimate
ˆξ1 and another estimation described by a distribution
in uncertain parameter ˆξ2 ∼ P2 based on two diﬀerent
system identiﬁcation methods. We cannot simply compare
the goodness-of-ﬁt by comparing the likelihood objectives
as they might diﬀer for diﬀerent identiﬁcation methods.
Furthermore, the parameter descriptions may also diﬀer
(e.g. point estimation vs. set description) In addition,
can we be certain, in a quantitative manner, that the
systems behave diﬀerently after the propagation through
dynamics? 1

We summon the strength of KME to endow almost ar-
bitrary data types the meaning of distance through the
Hilbert space embedding. This allows us to compare sys-
tems by performing statistical two-sample tests [cf. Gret-
ton et al. (2012)] using the simulation samples.

Given the state distribution embeddings of two diﬀerent
systems µxt, µyt computed as in (6), we may measure how
diﬀerent two stochastic systems are by straightforwardly
computing their distance in the embedding Hilbert space,
(cid:107)µxt − µyt(cid:107)H. This quantity is also known as a maximum
mean discrepancy (MMD). Using kernel trick (2) and the
estimator in (6), we obtain the following.
Lemma 3.1. (Sample-based estimator for RKHS distance;
MMD) Given two sets of samples {xi}M
i=1
from simulations of two dynamical systems, a sample-
based estimator for (cid:107)µxt − µyt(cid:107)H is given by

i=1 and {yi}N

(cid:107)ˆµx−ˆµy(cid:107)2

H =

1
M 2

M
(cid:88)

i,j=1

k(xi, xj)

−

2
M N

M
(cid:88)

N
(cid:88)

i=1

j=1

k(xi, yj) +

1
N 2

N
(cid:88)

i,j=1

k(yi, yj),

(7)

where we omit time index t for conciseness. More details
can be found in Sch¨olkopf et al. (2002).

We propose that the goodness-of-ﬁt may again be straight-
forwardly measured by the RKHS distance (cid:107)µx(τ, ˆξ1) −
1 We note the Kullback-Leibler (KL) divergence is not distribution-
free: we need distribution functions to calculate its estimate.

Algorithm 1 Direct KPP for uncertainty propagation

1: Given: initial state x(0), (uncertain) dynamical sys-

tem f (x, t, ξ).

2: Output: KME estimate ˆµx(τ,ξ) at time τ .
3: Choose realization of the uncertain variable nodal set
{ξ1, ξ2, . . . , ξN } either via collocation or sampling.
4: Evolve the deterministic system forward, either via
diﬀerence equation or numerical integration, obtain
the states {x(τ, ξi)}N
i=1 at time τ .

5: Compute KME estimate ˆµx(τ,ξ) by (6).

µx(τ, ˆξ2)(cid:107)H, where x(τ, ˆξ1) and x(τ, ˆξ2) are two state dis-
tributions under two uncertain parameter descriptions.

This is powerful in that it can compare arbitrary (un-
known) uncertain systems. We demonstrate this ﬂexibility
in Section 4.2.

3.3 Uncertainty propagation via KPP

We have thus far discussed the use of KME as repre-
sentation and goodness-of-ﬁt measure for uncertainty in
stochastic systems. In this section, we propose to use KPP
for uncertainty propagation, which is at the core of many
stochastic control algorithms.

We ﬁrst present two diﬀerent views of uncertainty prop-
agation in systems. From a statistical standpoint, they
correspond to the diagonal and U-statistics estimation. In
particular, the U-statistic estimator is known to have lower
variance than the diagonal estimator but to require more
samples. We then show a novel recursive application of
the so-called reduced set method in propagating stochastic
dynamics forward.

Direct propagation via KPP: The main idea of this al-
gorithm is simple: sample a realization of the uncertainty,
and evolve the system as it is deterministic. The steps
are presented in Algorithm 1. By doing so, we view the
deterministic evolution as algebraic operations performed
on the uncertainty.

In Step 4, if the underlying system model is continuous-
time, we rely on numerical integration to propagate the
samples forward. Let us consider the integral of the dy-
namics function (deterministic or random) over the time
period [0, t], x(t, ξ) = x(0) + (cid:82) t
0 f (τ, ξ)dτ. In practice,
this integral is often intractable. Numerical integration is
performed to approximate its value, ˆx(t, ξ, h) ≈ x(t, ξ),
where h may denote the step size of an one-step numerical
integration rule.

One immediate question is, how will the integration error
aﬀect the embedding estimate? I.e., is the following true?
(8)
ˆµˆx(t,ξ,h) → µx(t,ξ), ∀t.
By virtue of Lemma 2.2, we obtain the following result.
Lemma 3.2. (Consistency 2 of KPP estimation with nu-
merical integration) Suppose k is a continuous positive-
deﬁnite kernel, {ξ1, ξ2, . . . , ξN } is chosen either via i.i.d

2 With a slight overload of terminology, we note the term consistent
is used in both statistics and numerical analysis community. In
both ﬁelds, they refer to the asymptotic convergence of statistical
estimator and numerical integration respectively.

sampling or collocation rules. The KPP estimator ˆµˆx(t,ξ)
produced by a one-step numerical integration rule with
step size h in Algorithm 1 is consistent, i.e.,

ˆµˆx(t,ξ,h) → µx(t,ξ), ∀t, as N → ∞, h → 0.

(9)

The proof (given in the appendix) is a direct consequence
of the consistency of numerical integration and that of
the KPP estimator. Similar propagation methods are used
in stochastic collocation in conjunction with gPC [Xiu
(2009)]. In the above algorithm, the propagated samples
are used to represent the distributions via the RKHS
embeddings. In the next section, we shall see a non-trivial
generalization of the above algorithm.

Recursive reduced set KPP for uncertainty propagation:
One nuance is encountered when considering more general
descriptions of uncertainty other than the simple parame-
ter uncertainty. For simplicity, let us restrict the discussion
to discrete-time dynamics and assume that the uncertainty
ξ enters as discrete realizations {ξi
t} of stochastic processes
at each time t. In this case, the system state at time t is a
function of all previous-step uncertainties,

X(t) = G(x0, ξ1, ξ2, . . . , ξt−1).
From a statistical standpoint, this is a transformation of
multiple RVs. To estimate the KME, one can either use
the diagonal estimator which corresponds to the already-
discussed Algorithm 1,

µd
X(t) =

1
N

N
(cid:88)

i=1

k(G(x0, ξi

1, ξi

2, . . . , ξi

t−1), ·),

or the U-statistics estimator which delivers smaller vari-
ance

µU
X(t) =

1
N t

N
(cid:88)

N
(cid:88)

· · ·

i1=1

it=1

k(G(x0, ξi1

1 , ξi2

2 , . . . , ξit

t ), ·). (10)

The downside of the U-statistics estimator is that it
may involve exponentially many samples of the uncertain
random variable (disturbances). To relieve this sample
complexity while still capturing the statistical distribu-
tion, Sch¨olkopf et al. (2015) proposed to use the re-
duced set method to compute multi-step transformations
of RV with only a subset of samples. Intuitively, the re-
duced set method seeks to ﬁnd a (small) set of expansion
points and weights {(xi, αi)}NR
i=1 such that the expansion
x = (cid:80)NR
ˆµR
i=1 αik(xi, ·) approximates a U-statistics estima-
tion such as in (10).

We propose the recursive reduced set kernel probabilistic
programming for uncertainty propagation in Algorithm 2.

Intuitively, at every time step, we look for a subset of
all samples (of the U-statistics samples) to serve as new
expansion points. One step of our recursion is similar to
the basic idea of eﬃcient quadrature rule [Gauss (1815)].
The optimization problem in Step 5 has two main tasks,
ﬁnding the expansion points and weights {(xi
i=1
simultaneously. There is a wide range of techniques for
treating this problem (see Chapter 18 of Sch¨olkopf et al.
(2002)) In Section 4, we give a numerical example as a
proof of concept for this procedure.

t+1)}N

t+1, αi

Algorithm 2 Recursive reduced set KPP for uncertainty
propagation

1: Given: initial state x(0), (uncertain) dynamical sys-
tem x+ = f (x, t, ξ), desired size for reduced-set NR
Output: Reduced set expansion for KME at time T

ˆµR
xT

=

N
(cid:88)

i=1

T k(xi
αi

T , ·).

2: loop
3: At time t, given the reduced set expansion for
embedding the current state distribution, sample Nξ
realizations of the uncertain process
ξj
t ∼ Pξt, j = 1, . . . , Nξ.

4:

5:

Compute KME of next state with U-statistics
NR(cid:88)

Nξ
(cid:88)

tk(f (xi
αi

t, ξj), ·).

(11)

ˆµxt+1 =

1
N

i=1

j=1

Construct the reduced set {(xi
next time step

t+1, αi

t+1)}NR

i=1 for the

ˆµR

xt+1

=

NR(cid:88)

i=1

t+1k(xi
αi

t+1, ·),

(12)

by minimizing the following optimization criterion
(13)

− ˆµxt+1 (cid:107)H.

(cid:107)ˆµR

xt+1

6: end loop

4. NUMERICAL EXPERIMENTS

We present numerical examples that vary in their uncer-
tain system descriptions and types of tasks, showcasing
the ﬂexibility of the proposed framework.

4.1 Uncertain ODE

Let us revisit the example of stochastic ODE in (1),

˙x(t) = ξ x,

x(0) = x0,

In this experiment, the uncertain ξ is drawn from a
mixture-of-Gaussian distribution. We then draw another
ξ(cid:48) from a unimodal Gaussian. Its mean and variance are
chosen such that the ﬁrst two moments match those of ξ,
i.e.,

ξ ∼ GMM, ξ(cid:48) ∼ N(m, σ)
Eξ = Eξ(cid:48), Eξ2 = Eξ(cid:48)2.

(14)

Their distributions are illustrated in Figure 2 (top).

We wish to answer: can we quantify the diﬀerent behav-
iors of those systems without imposing distribution as-
sumptions? To this end, we apply Algorithm 1 to obtain
two sets of propagated system states {x(t, ξi)}N
i=1 and
{x(t, ξ(cid:48)i)}N
are
computed using (6).

i=1. The state KME estimator ˆµXt and ˆµX (cid:48)

t

As illustrated in Figure 2 (bottom), the two moment-
matched parameter distributions result in distinct system
behaviors over time. To quantitatively compare the behav-
iors, we compute the RKHS-distance over time between
those two embeddings (cid:107)ˆµXt − ˆµX (cid:48)
To understand how diﬀerent kernels preserve statistical
properties of the system, we also plotted the embedding

(cid:107)H using (7).

t

F
D
P

e
t
a
t
s

m
e
t
s
y
s

2
w

e
t
a
t
s

parameter value

True

PVE

LSQ

time

Fig. 2. (top). Histogram approximating the uncertain pa-
rameter density for IVP example. (Red) the histogram
represents the initial parameter ξ ∼ GMM, (Gray) the
histogram represents ξ(cid:48) ∼ N (m, σ). The two distri-
butions match up to the second moment. However,
the GMM is skewed. (bottom). Two stochastic ODE
states with parameter ξ (red) and ξ(cid:48) (gray). The states
are obtained as a result of forward Euler integration.

e
c
n
a
t
s
i
d

S
H
K
R

time

Fig. 3. (top): RKHS distance over time between two
(cid:107)H associated with Gaussian
embeddings (cid:107)ˆµXt − ˆµX (cid:48)
kernel of diﬀerent bandwidth. (bottom): Embedding
distances associated with polynomial kernel of order
1 − 4.

t

distances associated with polynomial kernel of order 1 − 4
in Figure 3 (bottom). We clearly observe that diﬀerent
polynomial kernels capture diﬀerent orders of moment
information. Notably, the two system states seem to have
similar means and therefore the ﬁrst order polynomial ker-
nel does not diﬀerentiate the two systems. As we increase
the order of the polynomial kernel, the diﬀerence becomes
evident. We then show the Gaussian kernel embeddings in
Figure 3 (top), where we vary the bandwidth parameter
σ. It can be shown that the Gaussian kernel keeps track
of statistical moments up to inﬁnite order. If bandwidth
is large, the kernel treats everything the same so RKHS
distance is small. On the other hand, if bandwidth is small,

w1

time

Fig. 4. (left) plots the true uncertainty set (blue), es-
timated ellipsoidal uncertainty set (red), as well as
the LSQ point estimate (black). (right) plots the re-
sult of evolving the system for 600 steps using true
ellipsoid (blue), estimated variability ellipsoid using
PVE (red), Gaussian distributed parameter variabil-
ity from LSQ (black).

the kernel treats everything as diﬀerent. In the limiting
case as the bandwidth → 0, it can be shown the KME
estimation reduces to the usual Monte Carlo estimation
[cf. Section 3.3 in Sch¨olkopf et al. (2015)].

4.2 Distribution-free goodness-of-ﬁt for identiﬁcation

To demonstrate the proposed technique is a good measure
of goodness-of-ﬁt for arbitrary uncertainty distributions,
we apply it to PVE example in Mohammadi et al. (2015).
Diﬀerent from the uncertainty description in Section 4.1,
this is typically a “worst-case” scenario where the model
needs to account for the worst case realization of distur-
bances, described by ellipsoidal uncertainty sets.

In this example, the model to be identiﬁed is a time-
varying autoregressive exogenous-input model (ARX) with
variable parameter.

yk = a1yk−1 + a2yk−2 + b1uk−1.
The uncertain time-varying parameters are w = (a2, b1).
To generate the data, we follow the setup of Mohammadi
et al. (2015) to choose the true uncertain parameters wi
uniformly randomly 3 within the true ellipsoidal uncer-
tainty set E( ¯w, Z). The identiﬁcation was performed ac-
cording to that paper. As a result, we obtain the estimated
ellipsoid E(wPVE, ZPVE) and the LSQ parameter wLSQ , as
illustrated in Figure 4 (left).

We evolve the system for 600 steps using the true model
and those two diﬀerent uncertain parameter models (PVE
vs. Gaussian distributed parameter w ∼ N (wLSQ, ZLSQ)
resulting from the LSQ assumption, where ZLSQ is es-
timated by the sample variance as in that paper). The
resulting state trajectories are shown in Figure 4 (right).
We apply Algorithm 1 and compute the RKHS distance
(cid:107)ˆµX(t, ˆξPVE/LSQ) − ˆµX(t,ξ∗)(cid:107)H between the embeddings of
the estimated model and the true model. Figure 5 demon-
strates that the PVE model (red) matches the true model

3 We do not use this information during our measuring goodness-of-
ﬁt. In addition, we note the original data generation procedure may
be extended beyond uniform sampling as robust optimization only
concerns the distribution support.

e
c
n
a
t
s
i
d

S
H
K
R

(cid:107)PVE − True(cid:107)H
(cid:107)LSQ − True(cid:107)H

e
t
a
t
s

alg. 2
alg. 1

r
o
r
r
e

.
x
o
r
p
p
a

time

Fig. 5. We show goodness-of-ﬁt using (cid:107)ˆµX(t, ˆξPVE) −
ˆµX(t,ξ∗)(cid:107)H for estimated parameter variability using
PVE (red), and (cid:107)ˆµX(t, ˆξLSQ) − ˆµX(t,ξ∗)(cid:107)H for Gaussian-
distributed parameter from LSQ estimation (black).
They correspond to the system evolution over time in
Figure 4 (right).

better than the LSQ model (black) in the sense of RKHS
metric. This comparison is performed under no distribu-
tional assumptions.

In that paper, it is obvious that LSQ did not deliver
the parameter variability that ﬁts the true generating
distribution. However, one may ask, is the LSQ estimated
parameter ξLSQ nonetheless able to deliver similar system
behaviors? But there was no uniﬁed way to compare them.
This paper provides a unifying framework to quantita-
tively answer this question.

4.3 Recursive reduced set KPP for uncertainty propagation

Relying on the statistical consistency results in Section 2,
uncertainty propagation can be performed straightfor-
wardly using Algorithm 1. In this section, we demonstrate
the use of Algorithm 2. As a proof of concept, let us
consider a simple discrete-time stochastic system
(cid:19)
1
2

x(t + 1) = x(t) + w(t), w(t) ∼ Uniform

+ 0.1t.

1
2

−

(cid:18)

,

We emphasize the distribution of w(t) could be made fairly
arbitrary (and non-stationary)— the proposed propaga-
tion method does not place restrictions on this distribu-
tion. The system evolution is illustrated in Figure 6 (left).

i∈R αik(xi

At every time step, Algorithm 2 is applied to ﬁnd an
embedding of the current state µ˜xt = (cid:80)
t, ·),
where R denotes the reduced-set indices. Then the dy-
namics is propagated forward again. Note we use the naive
reduced set method following Simon-Gabriel et al. (2016)
which simply samples expansion points {xi
t} from the full
set and then solves a quadratic minimization problem for
coeﬃcients {αi} as in Step 5. A more sophisticated method
will be introduced in future work. As illustrated in Figure
6 (left), This procedure is repeated for 10 time steps. We
compare the RKHS approximation error (cid:107)ˆµ˜xt − µxt(cid:107)2
H of
the recursive reduced set method in Algorithm 2, against
that of the diagonal estimate of Algorithm 1. While we
do not know the true embedding µxt, we approximate
it with a large-sample estimate using 500 samples. The
approximation error in RKHS metric are illustrated in
Figure 6 (right). We observe a faster convergence in both
mean and variance by the recursive reduced set method in
Algorithm 2.

time

number of samples

Fig. 6. (left) Uncertainty propagation using reduced set
method. The uncertainty is forward propagated for 10
steps, using the reduced set methods of size parameter
10. The red lines are Monte Carlo simulations of the
system in question. The black dots are chosen reduced
set to represent the state distribution at each time
step. (right) Comparing approximation errors of the
recursive reduced set algorithm (red, Alg. 2), (cid:107)ˆµ˜xt −
µxt(cid:107)2
H, with that of the diagonal estimate of KME
(blue, Alg. 1)). The dynamics are run for 10 steps.
Each method is run 10 times to produce the error bar
plot. The state is sampled at time step t = 10. In
calculating the embedding , we use a Gaussian kernel
with bandwidth 0.5.

5. DISCUSSION

This paper proposed to use kernel probabilistic program-
ming as a unifying tool for treating uncertainties in dy-
namical systems. We demonstrated concrete numerical
procedures of propagating the uncertainty in dynamics. It
is our on-going endeavor to investigate more sophisticated
optimization procedures as well as mathematically rigor-
ous analysis of convergence in Algorithm 2 with more gen-
eral numerical integration. Another important direction is
distributionally robust control design and state estimation
using RKHS embeddings.

Compared with existing popular methods such as gPC or
GP, RKHS embedding methods for dynamical systems are
still in the early development. This paper serves as a call to
action for their wider applications to robust and stochastic
control.

ACKNOWLEDGEMENTS

We would like to thank Mario Zanon for providing the
code to reproduce the PVE experiment.

REFERENCES

Boots, B., Gordon, G., and Gretton, A. (2013). Hilbert
space embeddings of predictive state representations.
arXiv preprint arXiv:1309.6819.

Campi, M.C., Garatti, S., and Prandini, M. (2019). Sce-
In Handbook of Model

nario optimization for mpc.
Predictive Control, 445–463. Springer.

Fukumizu, K., Bach, F., and Jordan, M. (2004). Di-
mensionality reduction for supervised learning with re-
producing kernel Hilbert spaces. Journal of Machine
Learning Research, 5, 73–99.

Gauss, C.F. (1815). Methodus nova integralium valores per
approximationem inveniendi. apvd Henricvm Dieterich.

Gretton, A., Borgwardt, K., Rasch, M., Sch¨olkopf, B., and
Smola, A. (2012). A kernel two-sample test. Journal of
Machine Learning Research, 13, 723–773.

Gr¨unew¨alder, S., Lever, G., Baldassarre, L., Pontil, M.,
and Gretton, A. (2012). Modelling transition dynamics
In Proceedings of
in MDPs with RKHS embeddings.
the 29th International Conference on Machine Learning,
535–542. Omnipress.

Mohammadi, A., Diehl, M., and Zanon, M. (2015).
Estimation of uncertain ARX models with ellip-
2015 European Con-
soidal parameter variability.
trol Conference, ECC 2015, (1), 1766–1771.
doi:
10.1109/ECC.2015.7330793.

Muandet, K., Fukumizu, K., Sriperumbudur, B., and
Sch¨olkopf, B. (2017). Kernel mean embedding of distri-
butions: A review and beyond. Foundations and Trends
in Machine Learning, 10(1-2), 1–141.

Nishiyama, Y., Boularias, A., Gretton, A., and Fukumizu,
K. (2012). Hilbert space embeddings of POMDPs. In
Proceedings of the 28th Conference on Uncertainty in
Artiﬁcial Intelligence, 644–653.

Rasmussen, C.E. and Williams, C.K. (2006). Gaussian
processes for machine learning. Gaussian Processes
for Machine Learning, by CE Rasmussen and CKI
Williams. ISBN-13 978-0-262-18253-9.

Sch¨olkopf, B., Muandet, K., Fukumizu, K., Harmeling,
S., and Peters, J. (2015). Computing functions of
random variables via reproducing kernel Hilbert space
representations. Statistics and Computing, 25(4), 755–
766. doi:10.1007/s11222-015-9558-5.

Sch¨olkopf, B., Smola, A.J., Bach, F., et al. (2002). Learn-
ing with kernels: support vector machines, regulariza-
tion, optimization, and beyond. MIT press.

physics, 5(2-4), 242–272.

Xiu, D. and Karniadakis, G.E. (2002). The wiener–askey
polynomial chaos for stochastic diﬀerential equations.
SIAM journal on scientiﬁc computing, 24(2), 619–644.

Appendix A. PROOF OF LEMMA 3.2

Proof. We provide a proof for the consistency of Algo-
rithm 1.

(cid:107)ˆµ ˆF (x,ξ,t) − µF (x,ξ,t)(cid:107) = (cid:107)ˆµ ˆF (x,ξ,t) − ˆµF (x,ξ,t)
+ˆµF (x,ξ,t) − µF (x,ξ,t)(cid:107)
≤ (cid:107)ˆµ ˆF (x,ξ,t) − ˆµF (x,ξ,t)(cid:107) + (cid:107)ˆµF (x,ξ,t) − µF (x,ξ,t)(cid:107)
≤ C · (cid:107) ˆF (x, ξ, t) − F (x, ξ, t)(cid:107) + (cid:107)ˆµF (x,ξ,t) − µF (x,ξ,t)(cid:107)
→ 0

(A.1)
In the last inequality, The ﬁrst term is due to the continu-
ity of kernel k. In the last line, the ﬁrst term converges to
0 due to the consistency of numerical integration whereas
the second term converges due to the consistency of KPP
in Proposition 3.2.
The convergence rate is O(hp) + O( 1√
). The ﬁrst term is
N
the result of p-th order one-step numerical integration rule
(e.g. RK-p) with step size h. The second term is triggered
by The KPP estimator ﬁnite-sample convergence. N is the
sample size used by the KPP algorithm.

Scokaert, P.O. and Mayne, D.Q. (1998). Min-max feedback
model predictive control for constrained linear systems.
IEEE Transactions on Automatic control, 43(8), 1136–
1142.

Simon-Gabriel, C.J., ´Scibior, A., Tolstikhin,

I., and
Sch¨olkopf, B. (2016). Consistent kernel mean estimation
for functions of random variables. Advances in Neural
Information Processing Systems, 1(i), 1740–1748.

Smola, A.J., Gretton, A., Song, L., and Sch¨olkopf, B.
(2007). A Hilbert space embedding for distributions.
In Proceedings of the 18th International Conference on
Algorithmic Learning Theory (ALT), 13–31. Springer-
Verlag.

Song, L. (2008). Learning via Hilbert Space Embedding of
Distributions. Ph.D. thesis, The University of Sydney.
Song, L., Fukumizu, K., and Gretton, A. (2013). Kernel
embeddings of conditional distributions: A uniﬁed ker-
nel framework for nonparametric inference in graphical
models. IEEE Signal Processing Magazine, 30(4), 98–
111.

Sriperumbudur, B., Gretton, A., Fukumizu, K., Sch¨olkopf,
B., and Lanckriet, G. (2010). Hilbert space embeddings
and metrics on probability measures. Journal of Ma-
chine Learning Research, 99, 1517–1561.

Tempo, R., Calaﬁore, G., and Dabbene, F. (2012). Ran-
domized algorithms for analysis and control of uncertain
systems: with applications. Springer Science & Business
Media.

Xiu, D. (2009). Fast numerical methods for stochastic com-
putations: a review. Communications in computational

