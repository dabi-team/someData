Graph Neural Networks for Maximum
Constraint Satisfaction

Jan Toenshoﬀ

Martin Ritzert
Martin Grohe

Hinrikus Wolf

February 12, 2020
RWTH Aachen University

Many combinatorial optimization problems can be phrased in the language
of constraint satisfaction problems. We introduce a graph neural network
architecture for solving such optimization problems. The architecture is
generic; it works for all binary constraint satisfaction problems. Training is
unsupervised, and it is suﬃcient to train on relatively small instances; the
resulting networks perform well on much larger instances (at least 10-times
larger). We experimentally evaluate our approach for a variety of problems,
including Maximum Cut and Maximum Independent Set. Despite being
generic, we show that our approach matches or surpasses most greedy and
semi-deﬁnite programming based algorithms and sometimes even outperforms
state-of-the-art heuristics for the speciﬁc problems.

1. Introduction

Constraint satisfaction is a general framework for casting combinatorial search and opti-
mization problems; many well-known NP-complete problems, for example, k-colorability,
Boolean satisﬁability and maximum cut can be modeled as constraint satisfaction prob-
lems (CSPs). Our focus is on the optimization version of constraint satisfaction, usually
referred to as maximum constraint satisfaction (Max-CSP), where the objective is to
satisfy as many constraints of a given instance as possible. There is a long tradition of
designing exact and heuristic algorithms for all kinds of CSPs. Our work should be
seen in the context of a recently renewed interest in heuristics for NP-hard combinatorial
problems based on neural networks, mostly graph neural networks (for example, (Lemos
et al. 2019; Prates et al. 2019; Selsam et al. 2019)).

We present a generic graph neural network (GNN) based architecture called RUN-CSP
(Recurrent Unsupervised Neural Network for Constraint Satisfaction Problems) with the
following key features:

1

0
2
0
2

b
e
F
0
1

]
I

A
.
s
c
[

3
v
7
8
3
8
0
.
9
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
Unsupervised: Training is completely unsupervised and just requires a set of instances

of the problem.

Scalable: Networks trained on small instances achieve good results on much larger inputs.

Generic: The architecture is generic and can learn to ﬁnd approximate solutions for any

binary Max-CSP.

We focus on binary CSPs, where each constraint involves only two variables. This is no
severe restriction, because every CSP can be transformed into an equivalent binary CSP
(see (Dechter 2003)).

To solve Max-CSPs, we train a graph neural network which we view as a message
passing protocol. The protocol is executed on a graph with nodes for all variables of
the instance and edges for all constraints. Associated with each node are two states, a
short-term state and a long-term state, which are both vectors of some ﬁxed length. The
exchanged messages are linear functions of the short-term states. We update the internal
states using an LSTM (Long Short-Term Memory) cell for each variable and share the
parameters of the internal functions over all variables. Finally, we extract probabilities
for the possible values of each variable from its short-term state through a linear function
combined with softmax activation. The parameters of the LSTM, message generation,
and readout function are learned. Since all parameters are shared over all variables, we
can apply the model to instances of arbitrary size1.

For training, we design a loss function that rewards solutions with many satisﬁed
constraints. Eﬀectively, through the choice of the loss function, we train our networks
to satisfy the maximum number of constraints. Our focus on the optimization problem
Max-CSP rather than the decision problem allows us to train unsupervised. This is a
major point of distinction between our work and recent neural approaches to Boolean
satisﬁability (Selsam et al. 2019) and the coloring problem (Lemos et al. 2019). Both
approaches require supervised training and output a prediction for satisﬁability or
coloring number. In contrast, our framework returns an (approximately optimal) variable
assignment.

We experimentally evaluate our approach on the following NP-hard problems: the
maximum 2-satisﬁability problem (Max-2-Sat), which asks for an assignment maximizing
the number of satisﬁed clauses for a given Boolean formula in 2-conjunctive normal form;
the maximum cut problem (Max-Cut), which asks for a partition of a graph in two
parts such that the number of edges between the parts is maximal (see Figure 1); the
3-colorability problem (3-Col), which asks for a 3-coloring of the vertices of a given
graph such that the two endvertices of each edge have distinct colors. We also consider
the maximum independent set problem Max-IS, which asks for an independent set of
maximum cardinality in a given graph. Strictly speaking, Max-IS is not a maximum
constraint satisfaction problem, because its objective is not to maximize the number
of satisﬁed constraints, but to satisfy all constraints while maximizing the number of
variables with a certain value. We include this problem to demonstrate that our approach
can easily be adapted to such related problems.

1Our Tensorﬂow implementation of RUN-CSP is available at https://github.com/RUNCSP/RUN-CSP.

2

t=1

t=2

t=3

t=4

t=5

t=6

t=7

t=8

Figure 1: A maximum cut for a graph found by RUN-CSP in seven iterations. Edges
not part of the cut are shown in red.

We demonstrate that our simple generic approach works well for all four problems and
matches competitive baselines. Since our approach is generic for all Max-CSPs, those
baselines include other general approaches such as greedy algorithms and semi-deﬁnite
programming (SDP). The latter is particularly relevant, because it is known (under
certain complexity theoretic assumptions) that SDP achieves optimal approximation
ratios for all Max-CSPs (Raghavendra 2008). For Max-2-Sat, our approach even
manages to surpass a state-of-the-art heuristic.

Almost all models are trained on quite small training sets consisting of small random
instances. We evaluate those models on unstructured random instances as well as highly
structured benchmark instances.
Instance sizes vary from small instances with 100
variables and 200 constraints to medium sized instances with more than 1,000 variables
and over 10,000 constraints. We observe that RUN-CSP is able to generalize well from
small instances to instances both smaller and much larger. The largest (benchmark)
instance we evaluate on has approximately 120,000 constraints, but that instance required
the use of large training graphs. Computations with RUN-CSP are very fast in comparison
to many heuristics and proﬁts from modern hardware like GPUs. For medium-sized
instances with 10,000 constraints the computation takes less than ﬁve seconds.

We do not claim that our method is in general competitive with state-of-the-art
heuristics for speciﬁc problems. Furthermore, it has to be distinguished from solvers
which in addition to a solution return a guarantee that no better solution exists. However,
we demonstrate that our approach clearly improves on the state-of-the-art for neural
methods on small and medium-sized binary CSP instances, while still being completely
generic.

3

1.1. Related Work

An early group of papers dates back to the 1980’s and uses Hopﬁeld Networks (Hopﬁeld
and Tank 1985) to approximate TSP and other discrete problems using neural networks.
Hopﬁeld and Tank use a single-layer neural network with sigmoid activation and apply
gradient descent to come up with an approximative solution. The loss function adopts soft
assignments and uses the length of the TSP tour and a term penalizing incorrect tours as
loss, hence being unsupervised. This approach has been extended to k-colorability (Dahl
1987; Gassen and Carothers 1993; Harmanani, Hannouche, and Khoury 2010; Takefuji
and Lee 1991) and other CSPs (Adorf and Johnston 1990). The loss functions used in
some of these approaches are similar to ours.

Newer approaches involve modern machine learning techniques and are usually based
on graph neural networks (GNNs). NeuroSAT (Selsam et al. 2019), a learned message
passing network for predicting satisﬁability, reignited the interest in solving NP-complete
problems with neural networks. Prates et al. (2019) use GNNs to learn TSP trained
on instances of the form (G, ‘±ε) where ‘ is the length of an optimal tour on G. They
achieved good results on graphs with up to 40 nodes. Using the same idea, Lemos et
al. (2019) learned to predict k-colorability of graphs scaling to larger graphs and chromatic
numbers than seen during training. Yao, Bandeira, and Villar (2019) evaluated the
performance of unsupervised GNNs for the Max-Cut problem. They adapted a GNN
architecture by Chen, Li, and Bruna (2019) to Max-Cut and trained two versions of their
network, one through policy gradient descent and the other via a diﬀerentiable relaxation
of the loss function which both achieved similar results. Amizadeh, Matusevych, and
Weimer (2019) proposed an unsupervised architecture for Circuit-SAT, which predicts
satisfying variable assignments for a given formula. For the #P-hard weighted model
counting problem for DNF formulas, Abboud, Ceylan, and Lukasiewicz (2019) applied
a GNN-based message passing approach. For large instances with more than 100,000
nodes, Li, Chen, and Koltun (2018) use a GNN to guide a tree search for Max-IS and
Khalil et al. (2017) choose the best heuristic for TSP through reinforcement learning.

2. Method

In this section, we describe our RUN-CSP architecture for Max-CSPs. Formally, a
CSP-instance is a triple I = (X, D, C), where X is a set of variables, D is a domain,
and C is a set of constraints of the form (x1, . . . , x‘, R) for some R ⊆ D‘. We only
consider binary constraints (with ‘ = 2) in this paper. A constraint language is a ﬁnite
set Γ of relations over some ﬁxed domain D, and I is a Γ-instance if R ∈ Γ for all
constraints (x1, x2, R) ∈ C. An assignment α : X → D satisﬁes a constraint (x1, x2, R)
if (α(x1), α(x2)) ∈ R, and it satisﬁes the instance I if it satisﬁes all constraints in C.
CSP(Γ) is the problem of deciding whether a given Γ-instance has a satisfying assignment
and ﬁnding such an assignment if there is one. Max-CSP(Γ) is the problem of ﬁnding
an assignment that satisﬁes the maximum number of constraints.

For example, an instance of 3-Col has a variable xv for each vertex v of the input
6=) for each edge vw of the graph.

graph, domain D = {1, 2, 3}, and a constraint (v, w, R3

4

6= = {(1, 2), (2, 1), (1, 3), (3, 1), (2, 3), (3, 2)} is the inequality relation on {1, 2, 3}.

Here R3
Thus 3-Col is CSP({R3

6=}).

2.1. Architecture

We use a randomized recurrent graph neural network architecture to evaluate a given prob-
lem instance using message passing. For any binary constraint language Γ a RUN-CSP
network can be trained to approximate Max-CSP(Γ). Intuitively, our network can be
viewed as a trainable communication protocol through which the variables of a given
instance can negotiate a value assignment. With every variable x ∈ X we associate
x ∈ Rk and a hidden (long-term) state h(t)
a short-term state s(t)
x ∈ Rk which change
throughout the message passing iterations t ∈ {0, . . . , tmax}. The short-term state vector
s(0)
for every variable x is initialized by sampling each value independently from a normal
x
distribution with zero mean and unit variance. All hidden states h(0)
x are initialized as
zero vectors.

Every message passing step uses the same weights and thus we are free to choose the
number tmax ∈ N of iterations for which RUN-CSP runs on a given problem instance.
This number may or may not be identical to the number of iterations used for training.
The state size k and the number of iterations used for training ttr
are the main hyperparameters of our network.

max and evaluation tev

max

x , s(t)

x , h(t)

Variables x and y that co-occur in a constraint c = (x, y, R) can exchange messages.
Each message depends on the states s(t)
y , the relation R and the order of x and y
in the constraint, but not the internal long-term states h(t)
y . The dependence on R
allows the network to send diﬀerent messages whenever the states of x and y correspond
to a satisfying or unsatisfying assignment for each constraint. This dependence implies
that we have independent message generation functions for every relation R in the
constraint language Γ. We therefore require Γ to be ﬁxed. The process of message
passing and updating the internal states is repeated tmax times. We use linear functions
to compute the messages. (Experiments showed that more complicated functions did
not improve performance while being less stable and less eﬃcient during training.) Thus,
the messaging function for every relation R is deﬁned by a trainable weight matrix
MR ∈ R2k×2k as

(cid:16)

SR

x , s(t)
s(t)
y

(cid:17)

= MR

!

s(t)
x
s(t)
y

.

(1)

The output of SR are two stacked k-dimensional vectors, which represent the messages to x
and y, respectively. Since MR is in general not symmetric, the generated messages depend
on the order of the variables in the constraint. This behavior is desirable for asymmetric
relations. For symmetric relations we modify SR to produce messages independently
from the order of variables in c. In this case we use a smaller weight matrix MR ∈ R2k×k
to generate both messages:

SR(s(t)

x , s(t)

y ) =

MR

!

s(t)
x
s(t)
y

, MR

!!

s(t)
y
s(t)
x

5

 
 
 
 
The two messages can still be diﬀerent, but the content of each message depends only on
the states of the endpoints.

The internal states hx and sx are updated by an LSTM cell based on the mean of
the received messages. For a variable x which received the messages m1, . . . , m‘ the new
states are thus computed by

h(t+1)
x

, s(t+1)
x

= LSTM

x , s(t)
h(t)
x ,

(cid:18)

(cid:19)

.

mi

1
‘

‘
X

i=1

(2)

x . In our architecture we use ϕ(t)(x) = softmax(W s(t)

For every variable x and iteration t ∈ {1, . . . , tmax} the network produces a soft assignment
ϕ(t)(x) from the state s(t)
x ) with
W ∈ Rd×k trainable and d = |D|. In ϕ, the linear function reduces the dimensionality
while the softmax function enforces stochasticity. The soft assignments ϕ(t)(x) can be
interpreted as probabilities of a variable x receiving a certain value v ∈ D. If the domain
D contains only two values, we compute a ‘probability’ p(t)(x) = σ(W s(t)
x ) for each node
with W ∈ R1×k. The soft assignment is then given by ϕ(t)(x) =
. To
obtain a hard variable assignment α(t) : X → D, we assign the value with the highest
estimated probability in ϕ(t)(x) for each variable x ∈ X. We select the hard assignment
with the most satisﬁed constraints as the ﬁnal prediction of the network. This is not
necessarily α(tev

p(t)(x), 1 − p(t)(x)

max).

(cid:16)

(cid:17)

Algorithm 1 describes the architecture in pseudocode. Note that the network’s output
depends on the random initialization of the short-term states s(0)
x . Those states are the
basis for all messages sent during inference and thus for the solution found by RUN-CSP.
By applying the network multiple times to the same input and choosing the best solution,
we can therefore boost the performance.

2.2. Loss Function

In the following we derive our loss function used for unsupervised training. Let I =
(X, D, C) be a CSP-instance. Assume without loss of generality that D = {1, . . . , d} for
a positive integer d. Given I, in every iteration our network will produce a soft variable
assignment ϕ : X → [0, 1]d, where ϕ(x) is stochastic for every x ∈ X. Instead of choosing
the value with the maximum probability in ϕ(x), we could obtain a hard assignment
α : X → D by independently sampling a value for each x ∈ X from the distribution
speciﬁed by ϕ(x). In this case, the probability that any given constraint (x, y, R) ∈ C is
satisﬁed by α can be expressed by

Pr
α∼ϕ

[(α(x), α(y)) ∈ R] = ϕ(x)T AR ϕ(y)

(3)

where AR ∈ {0, 1}d×d is the characteristic matrix of the relation R with (AR)i,j =
1 ⇐⇒ (i, j) ∈ R. We then aim to minimize the combined negative log-likelihood over
all constraints:

LCSP (ϕ, I) :=

1
|C|

(cid:16)

· X
(x,y,R)∈C

− log

ϕ(x)T AR ϕ(y)

(cid:17)

(4)

6

Algorithm 1: Network Architecture
Input: Instance (X, C), tmax ∈ N
Output: (ϕ(1), . . . , ϕ(tmax)), ϕ(t) : X → [0, 1]d
for x ∈ X do

// random initialization
s(0)
x ∼ N (0, 1)k
h(0)
:= 0 ∈ Rk
x
for t ∈ {1, ..., tmax} do

for c := (x, y, R) ∈ C do
// generate messages
(m(t)

c,y) := SR(s(t−1)

c,x, m(t)
for x ∈ X do

x

, s(t−1)
y

)

P

// combine messages and update
c∈C,x∈c m(t)
r(t)
x := 1
c,x
deg(x)
(h(t)
x ) := LSTM(h(t−1)
x , s(t)
ϕ(t)(x) := softmax(W · s(t)
x )

, s(t−1)
x

x

, r(t)
x )

We combine the loss function LCSP throughout all iterations with a discount factor
λ ∈ [0, 1] to get our training objective:

L({ϕt}t≤ttr

max

, I) :=

ttr
maxX

t=1

λttr

max−t · LCSP

(cid:16)

ϕ(t), I

(cid:17)

(5)

This loss function allows us to train unsupervised since it does not depend on any ground
truth assignments. In general, computing optimal solutions for supervised training can
easily turn out to be prohibitive; our approach completely avoids such computations.

Remarks:
(1) In this paper, we focus on binary CSPs. For the extension to ‘-ary CSPs for
some ‘ ≥ 3 (for example 3-SAT), we note that the generalization of the loss function is
straightforward. Exploring network architectures that can process constraints of higher
arity remains future work.

(2) It is also possible to extend the framework to the weighted Max-CSPs where a
weight is associated with each constraint. To achieve this, we can replace the averages in
the loss function and message collection steps by weighted averages. Early experiments
in that direction show promising results.

3. Experiments

To validate our method empirically, we performed experiments for Max-2-Sat, Max-
Cut, 3-Col and Max-IS. For all experiments, we used internal states of size k = 128;
state sizes up to k = 1024 did not increase performance for the tested instances. We

7

Figure 2: Percentage of satisﬁed clauses of random 2-CNF formulas for RUN-CSP,
Loandra and WalkSAT. Each data point is the average of 100 formulas; the ratio of
clauses per variable increases in steps of 0.1.

max. In contrast, choosing ttr

empirically chose to use ttr
max = 30 iterations during training and, unless stated otherwise,
tev
max = 100 for evaluation. Especially for larger instances it proved beneﬁcial to use a
relatively high tev
max > 50)
resulted in unstable training. During evaluation, we use 64 parallel runs for each instance
and use the best result. Further increasing this number mainly increases the runtime
but has no real eﬀect on the quality of solutions. We trained most models with 4,000
instances split into in 400 batches. Training is performed for 25 epochs using the Adam
optimizer with default parameters and gradient clipping at a norm of 1.0. The decay
over time in our loss function was set to λ = 0.95. We provide a more detailed overview
of our implementation and training conﬁguration in the appendix.

max too large during training (ttr

We ran our experiments on machines with two Intel Xeon 8160 CPUs and one NVIDIA
Tesla V100 GPU but got very similar runtimes on consumer hardware. Evaluating 64
runs on an instance with 1,000 variables and 1,000 constraints takes about 1.5 seconds,
10,000 constraints about 5 seconds, and 20,000 constraints about 8 seconds. Training a
model takes less than 30 minutes. Thus, the computational cost of RUN-CSP is relatively
low.

8

87.590.092.595.097.5100.0% of satisfied Clauses100 Variables400 VariablesRUN-CSPLoandraWalkSAT123456Clauses per Variable87.590.092.595.097.5100.0% of satisfied Clauses800 Variables123456Clauses per Variable1600 VariablesTable 1: Number of unsatisﬁed constraints for Max-2-Sat benchmark instances derived
from the Ising spin glass problem. Best solutions are printed in bold.

Instance |V |

|C| Opt. RUN-CSP WalkSAT Loandra

t3pm3
t4pm3
t5pm3
t6pm3
t7pm3

17
162
27
38
64
384
78
125 750
216 1269 136
343 2058 209

17
40
78
136
216

17
38
78
142
227

17
38
78
142
225

3.1. Max-2-SAT

We view Max-2-Sat as a binary CSP with domain D = {0, 1} and a constraint language
consisting of three relations R00 (for clauses with two negated literals), R01 (one negated
literal) and R11 (no negated literals). For example, R01 = {(0, 0), (0, 1), (1, 1)} is the
set of satisfying assignments for a clause (¬x ∨ y). For training a RUN-CSP model we
used 4,000 random 2-CNF formulas with 100 variables each. The number of clauses was
sampled uniformly between 100 and 600 for every formula and each clause was generated
by sampling two distinct variables and then independently negating the literals with
probability 0.5.

For the evaluation of RUN-CSP in Max-2-Sat we start with random instances and
compare it to a number of problem-speciﬁc heuristics. All baselines can solve Max-
Sat for arbitrary arities, not only Max-2-Sat, while RUN-CSP can solve a variety of
binary Max-CSPs. The state-of-the-art Max-Sat Solver Loandra (Berg, Demirovi´c,
and Stuckey 2019) won the unweighted track for incomplete solvers in the Max-Sat
Evaluation 2019 (Bacchus, J¨arvisalo, and Martins 2019). We ran Loandra in its default
conﬁguration with a timeout of 20 minutes on each formula. To put this into context, on
the largest evaluation instance used here (9,600 constraints) RUN-CSP takes less than
seven minutes on a single CPU core and about ﬁve seconds using the GPU. WalkSAT
(Kautz 2019; Selman, Kautz, Cohen, et al. 1993) is a stochastic local search algorithm
for approximating Max-Sat. We allowed WalkSAT to perform 10 million ﬂips on each
formula using its ‘noise’ strategy with parameters n = 2 and m = 2000. Its performance
was boosted similarly to RUN-CSP by performing 64 runs and selecting the best result.
For evaluation we generated random formulas with 100, 400, 800 and 1,600 variables.
The ratio between clauses and variables was varied in steps of 0.1 from 1 to 6. Figure 2
shows the average percentage of satisﬁed clauses in the solutions found by each method
over 100 formulas for each size and density. The methods yield virtually identical results
for formulas with less than 2 clauses per variable. For denser instances, RUN-CSP yields
slightly worse results than both baselines when only 100 variables are present. However,
it matches the results of Loandra for formulas with 400 variables and outperforms the
solver for instances with 800 and 1,600 variables. The performance of WalkSAT degrades
on these formulas and is signiﬁcantly worse than RUN-CSP.

For more structured formulas, we use Max-2-Sat benchmark instances from the

9

Table 2: P -values of graph cuts produced by RUN-CSP, Yao, SDP, and EO for regular
graphs with 500 nodes and varying degrees. We report the mean across 1000 random
graphs for each degree.

d RUN-CSP Yao Rel. Yao Pol.

SDP

EO

3
5
10
15
20

0.714
0.726
0.710
0.697
0.685

0.707
0.701
0.670
0.607
0.614

0.693
0.668
0.599
0.629
0.626

0.702
0.690
0.682
0.678
0.674

0.727
0.737
0.735
0.736
0.732

unweighted track of the Max-Sat Evaluation 2016 (Argelich 2016) based on the Ising
spin glass problem (De Simone et al. 1995; Heras et al. 2008). We used the same general
setup as in the previous experiment but increased the timeout for Loandra to 60 minutes.
In particular we use the same RUN-CSP model trained entirely on random formulas.
Table 1 contains the achieved numbers of unsatisﬁed constraints across the benchmark
instances. All methods produced optimal results on the ﬁrst and the third instance.
RUN-CSP slightly deviates from the optimum on the second instance. For the fourth
instance RUN-CSP found an optimal solution while both WalkSAT and Loandra did not.
On the largest benchmark formula, RUN-CSP again produced the best result.

Thus, RUN-CSP is competitive for random as well as spin-glass-based structured
Max-2-Sat instances. Especially on larger instances it also outperforms conventional
methods. Furthermore, training on random instances generalized well to the structured
spin-glass instances.

3.2. Max Cut

Max-Cut is a classical Max-CSP with only one relation {(0, 1), (1, 0)} used in the
constraints. In this section we evaluate RUN-CSP’s performance on this problem. Yao,
Bandeira, and Villar (2019) proposed two unsupervised GNN architectures for Max-Cut.
One was trained through policy gradient descent on a non-diﬀerentiable loss function while
the other used a diﬀerentiable relaxation of this loss. They evaluated their architectures
on random regular graphs, where the asymptotic Max-Cut optimum is known. We use
their results as well as their baseline results for Extremal Optimization (EO) (Boettcher
and Percus 2001) and a classical approach based on semi-deﬁnite programming (SDP)
(Goemans and Williamson 1995) as baselines for RUN-CSP. To evaluate the sizes of
graph cuts Yao, Bandeira, and Villar (2019) introduced a relative performance measure
called P-value given by P (z) = z/n−d/4√
where z is the predicted cut size for a d-regular
d/4

graph with n nodes. Based on results of Dembo, Montanari, Sen, et al. (2017), they
showed that the expected P -value of d-regular graphs approaches P ∗ ≈ 0.7632 as n → ∞.
P -values close to P ∗ indicate a cut where the size is close to the expected optimum and
larger values are better. While Yao, Bandeira, and Villar trained one instance of their

10

Table 3: Achieved cut sizes on Gset instances for RUN-CSP, DSDP and BLS.

Graph

|V |

|E|

RUN-CSP DSDP

BLS

G14
G15
G22
G49
G50
G55

800
800
2000
3000
3000
5000

4694
4661
19990
6000
6000
12468

2943
2928
13028
6000
5880
10116

2922
2938
12960
6000
5880
9960

3064
3050
13359
6000
5880
10294

GNN for each tested degree, we trained one network model on 4,000 Erd˝os-R´enyi graphs
and applied it to all graphs. For training, each graph had a node count of n = 100
and a uniformly sampled number of edges m ∼ U (100, 2000). Thus, the model was not
trained speciﬁcally for regular graphs. Table 2 reports the mean P -values across 1,000
random regular graphs with 500 nodes for diﬀerent degrees. For every method other
than RUN-CSP, we provide the values as reported by Yao, Bandeira, and Villar. While
RUN-CSP does not match the cut sizes produced by extremal optimization, it clearly
outperforms both versions of the GNN as well as the classical SDP-based approach.

We performed additional experiments on standard Max-Cut benchmark instances.
The Gset dataset (Ye 2003) is a set of 71 weighted and unweighted graphs that are
commonly used for testing Max-Cut algorithms. The dataset contains three diﬀerent
types of random graphs. Those graphs are Erd˝os-R´enyi graphs with uniform edge
probability, graphs where the connectivity gradually decays from node 1 to n, and
4-regular toroidal graphs. Here, we use two unweighted graphs for each type from this
dataset.

We reused the RUN-CSP model from the previous experiment but increased the number
of iterations for evaluation to tev
max = 500. Our ﬁrst baseline by Choi and Ye (2000) uses
an SDP solver based on dual scaling (DSDP) and a reduction based on the approach
of Goemans and Williamson (1995). Our second baseline Breakout Local Search (BLS)
is based on the combination of local search and adaptive perturbation (Benlic and Hao
2013). Its results are among the best known solutions for the Gset dataset. For DSDP
and BLS we report the values as provided in the literature. Table 8 reports the achieved
cut sizes for RUN-CSP, DSDP, and BLS. On G14 and G15, which are random graphs
with decaying node degree, the graph cuts produced by RUN-CSP are similar in size to
those reported for DSDP. For the Erd˝os-R´enyi graphs G22 and G55 RUN-CSP performs
better than DSDP but worse than BLS. Lastly, on the toroidal graphs G49 and G50
all three methods achieved the best known cut size. This reaﬃrms the observation that
our architecture works particularly well for regular graphs. Although RUN-CSP did not
outperform the state-of-the-art heuristic in this experiment it performed at least as well
as the SDP based approach DSDP.

11

Table 4: Percentages of hard instances classiﬁed correctly by RUN-CSP, Greedy, Hy-
bridEA, and GNN-GCP. We evaluate on 1,000 instances for each size. We provide mean
and standard deviation across ﬁve diﬀerent RUN-CSP models.

Nodes RUN-CSP Greedy HybridEA GNN-GCP
Pos. Neg.

Pos.

Pos.

Pos.

50
100
150
200
300
400

98.4 ±0.3
62.5 ±2.7
15.5 ±2.3
2.6 ±0.4
0.1 ±0.0
0.0 ±0.0

34.0
6.7
1.5
0.5
0.0
0.0

100.0
100.0
98.7
88.9
39.9
15.3

77.6
64.5
57.7
51.9
48.8
46.3

27.0
37.8
43.5
45.3
52.7
54.7

3.3. Coloring

Within coloring we focus on the case of 3 colors, i.e. we consider CSPs over the domain
{1, 2, 3} with one constraint relation {(i, j); i, j ∈ D, i 6= j}.
In general, RUN-CSP
aims to satisfy as many constraints as possible and therefore approximates Max-3-Col.
Instead of evaluating on Max-3-Col, we evaluate on its practically more relevant decision
variant 3-Col which asks whether a given graph is 3-colorable without conﬂicts. We
turn RUN-CSP into a classiﬁer by predicting that a given input graph is 3-colorable if
and only if it is able to ﬁnd a conﬂict-free vertex coloring.

We evaluate RUN-CSP on so-called ‘hard’ random instances, similar to those deﬁned
by Lemos et al. (2019). These instances are a special subclass of Erd˝os-R´enyi graphs
where an additional edge can make the graph no longer 3-colorable. We describe our exact
generation procedure in the appendix. We trained ﬁve RUN-CSP models on 4,000 hard
3-colorable instances with 100 nodes each. In Table 10 we present results for RUN-CSP,
a greedy heuristic with DSatur strategy (Br´elaz 1979), the state-of-the-art heuristic
HybridEA (Galinier and Hao 1999; Lewis 2015; Lewis et al. 2012), and GNN-GCP
(Lemos et al. 2019). We trained a GNN-GCP network on the training instances of Lemos
et al. (2019) and allowed HybridEA to make 500 million constraint checks on each graph.
All algorithms but GNN-GCP report a graph a 3-colorable only if they found a valid
3-coloring on it and thus never produce false positives. Thus, we additionally report
values for GNN-GCP on the non-3-colorable counterparts of the evaluation instances.
For GNN-GCP we observe that it roughly gets the chromatic number right and only
predicts 3 or 4 on our test graphs. Nevertheless, it seems unable to correctly classify
the test instances as high accuracy on positive instances comes with low accuracy on
negative instances and vice versa. For the other three algorithms we observe a clear
hierarchy. HybridEA expectedly performs best and ﬁnds solutions even for some of the
largest graphs. RUN-CSP correctly classiﬁes most of the graphs up to 100 nodes and
clearly outperforms GNN-GCP. The weakest algorithm is DSatur which even fails on
most of the small 50 node graphs.

Using a RUN-CSP model trained on a mixture of random graphs performs slightly

12

Figure 3: Independent set sizes on random graphs produced by RUN-CSP, ReduMIS
and a greedy heuristic. The sizes are given as the percentage of nodes contained in the
independent set. Every data point is the average for 100 graphs; the degree increases in
steps of 0.2.

worse than models trained on hard random graphs only (shown in the appendix).

Overall, we see that despite being designed for maximization tasks, RUN-CSP outper-
forms greedy heuristics and neural baselines on the decision variant of 3-Col for hard
random instances.

3.4. Independent Set

Finally, we experimented with the maximum independent set problem Max-IS. The
independence condition can be modeled through a constraint language ΓIS with one
binary relation RIS = {(0, 0), (0, 1), (1, 0)}. Here, assigning the value 1 to a variable is
equivalent to including the corresponding node in the independent set. Max-IS is not
simply Max-CSP(ΓIS), since the empty set will trivially satisfy all constraints. Instead,
Max-IS is equivalent to ﬁnding an assignment which satisﬁes RIS at all edges while
maximizing an additional objective function that measures the size of the independent set.
To model this in our framework, we extend the loss function to reward assignments with
many variables set to 1. For a graph G = (V, E) and a soft assignment ϕ : V → [0, 1], we
deﬁne

LMIS(ϕ, G) = (cid:0)κ+LCSP(ϕ, G)(cid:1) · (cid:0)1+Lsize(ϕ, G)(cid:1),
Lsize(ϕ, G(cid:1) =

(1 − ϕ(v)).

X

1
|V |

v∈V

13

(6)

30405060% of Nodes in IS100 Nodes400 NodesRUN-CSPReduMISGreedy24681012Average Degree30405060% of Nodes in IS800 Nodes24681012Average Degree1600 NodesHere, LCSP is the standard RUN-CSP loss for ΓIS and κ adjusts the relative importance
of LCSP and Lsize. Intuitively, smaller values for κ decrease the importance of Lsize which
favors larger independent sets. A naive weighted sum of both terms turned out to be
unstable during training and yielded poor results, whereas the product in Equation (6)
worked well. For training, LMIS is combined across iterations with a discount factor λ as
in the standard RUN-CSP architecture.

We start by evaluating the performance on random graphs. We trained a network on
4,000 random Erd˝os-R´enyi graphs with 100 nodes and m ∼ U (100, 600) edges each and
with κ = 1. For evaluation we use random graphs with 100, 400, 800 and 1,600 nodes and
a varying number of edges. For roughly 6% of all predictions, the predicted set contained
induced edges (just a single edge in most cases), meaning the predicted sets where not
independent. We corrected these predictions by removing one of the endpoints of each
induced edge from the set and only report results after this correction. We compare
RUN-CSP against two baselines: ReduMIS, a state-of-the-art Max-IS solver (Akiba and
Iwata 2016; Lamm et al. 2017) and a greedy heuristic, which we implemented ourselves.
The greedy procedure iteratively adds the node with lowest degree to the set and removes
the node and its neighbors from the graph until the graph is empty. Figure 3 shows the
achieved independent set sizes, each data point is the mean IS size across 100 random
graphs. For graphs with 100 nodes, RUN-CSP achieves similar sizes as ReduMIS and
clearly outperforms the greedy heuristic. On larger graphs our network produces smaller
sets than ReduMIS. However, RUN-CSP’s performance remains similar to the greedy
baseline and, especially on denser graphs, outperforms it.

For more structured instances, we use a set of benchmark graphs from a collection of
hard instances for combinatorial problems (Xu 2005). The instances are divided into ﬁve
sets with ﬁve graphs each. These graphs were generated through the RB Model (Xu
and Li 2003; Xu et al. 2005), a model for generating hard CSP instances. A graph of
the class frbc-k consists of c interconnected k-cliques and the maximum independent
set has a forced size of c. The previous model trained on Erd˝os-R´enyi graphs did not
perform well on these instances and produced sets with many induced edges. Thus, we
trained a new network on 2,000 instances we generated ourselves through the RB Model.
The exact generation procedure of this dataset is provided in the appendix. Training
used a batch size of 5 for 25 epochs. We set κ = 0.1 to increase the importance of the
independence condition. The predictions of the new model contained no induced edges for
all benchmark instances. Table 5 contains the achieved IS sizes. RUN-CSP yields similar
results as the greedy heuristic. While our network does not match the state-of-the-art
heuristic, it still beats the greedy approach on instances with over 100,000 edges.

4. Conclusions

We have presented a universal approach for approximating Max-CSPs with recurrent
neural networks. Its key feature is the ability to train without supervision on any available
data. Our experiments on the optimization problems Max-2-Sat, Max-Cut, 3-Col
and Max-IS show that RUN-CSP produces high quality approximations for all four

14

Table 5: Achieved IS sizes for the benchmark graphs. We report the mean and std.
deviation for the 5 graphs in each group.

Graphs

|V |

|E| RUN-CSP Greedy ReduMIS

frb30-15 450 18k 25.8 ±0.8
frb40-19 790 41k 33.6 ±0.5
frb50-23 1150 80k 42.2 ±0.4
frb59-26 1478 126k 49.4 ±0.5

24.6 ±0.5
33.0 ±1.2
42.2 ±0.8
48.0 ±0.7

30 ±0.0
39.4 ±0.5
48.8 ±0.4
57.4 ±0.9

problems. Our network can compete with traditional approaches like greedy heuristics
or semi-deﬁnite programming on random data as well as benchmark instances. For
Max-2-Sat, RUN-CSP was able to outperform a state-of-the-art Max-Sat Solver. Our
approach also achieved better results than neural baselines, where those were available.
RUN-CSP networks trained on small random instances generalize well to other instances
with larger size and diﬀerent structure. Our approach is very eﬃcient and inference takes
a few seconds, even for larger instances with over 10,000 constraints. The runtime scales
linearly in the number of constraints and our approach can fully utilize modern hardware,
like GPUs.

Overall, RUN-CSP seems like a promising approach for approximating Max-CSPs
with neural networks. The strong results are somewhat surprising, considering that our
networks consist of just one LSTM Cell and a few linear functions. We believe that
our observations point towards a great potential of machine learning in combinatorial
optimization.

Future Work We plan to extend RUN-CSP to CSPs of arbitrary arity and to weighted
CSPs. It will be interesting to see, for example, how it performs on 3-SAT and its
maximization variant. Another possible future extension could combine RUN-CSP with
traditional local search methods, similar to the approach by Li, Chen, and Koltun (2018)
for Max-IS. The soft assignments can be used to guide a tree search and the randomness
can be exploited to generate a large pool of initial solutions for traditional reﬁnement
methods.

15

References

[1] Mart´ın Abadi et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Systems. Software available from tensorﬂow.org. 2015. url: http://tensorflow.
org/.

[2] Ralph Abboud, Ismail Ilkan Ceylan, and Thomas Lukasiewicz. “Learning to Reason:
Leveraging Neural Networks for Approximate DNF Counting”. In: arXiv preprint
arXiv:1904.02688 (2019).

[3] Hans-Martin Adorf and Mark D Johnston. “A discrete stochastic neural network
algorithm for constraint satisfaction problems”. In: 1990 IJCNN International Joint
Conference on Neural Networks. IEEE. 1990, pp. 917–924.

[4] Takuya Akiba and Yoichi Iwata. “Branch-and-reduce exponential/FPT algorithms
in practice: A case study of vertex cover”. In: Theoretical Computer Science 609
(2016), pp. 211–225.

[5] Saeed Amizadeh, Sergiy Matusevych, and Markus Weimer. “Learning to solve
circuit-SAT: An unsupervised diﬀerentiable approach”. In: International Conference
on Learning Representations (2019). url: https://openreview.net/forum?id=
BJxgz2R9t7.

[6] Josep Argelich. Eleventh Evaluation of Max-SAT Solvers (Max-SAT-2016). 2016.

url: http://maxsat.ia.udl.cat/introduction/.

[7] Fahiem Bacchus, Matti J¨arvisalo, and Ruben Martins. “MaxSAT Evaluation 2019:

Solver and Benchmark Descriptions”. In: (2019).

[8] Una Benlic and Jin-Kao Hao. “Breakout Local Search for the Max-Cut problem”. In:
Engineering Applications of Artiﬁcial Intelligence 26.3 (2013), pp. 1162 –1173. issn:
0952-1976. doi: https://doi.org/10.1016/j.engappai.2012.09.001. url:
http://www.sciencedirect.com/science/article/pii/S0952197612002175.

[9] Jeremias Berg, Emir Demirovi´c, and Peter J Stuckey. “Core-boosted linear search
for incomplete maxSAT”. In: International Conference on Integration of Constraint
Programming, Artiﬁcial Intelligence, and Operations Research. Springer. 2019,
pp. 39–56.

[10] Stefan Boettcher and Allon G Percus. “Extremal optimization for graph partition-

ing”. In: Physical Review E 64.2 (2001), p. 026114.

[11] Daniel Br´elaz. “New methods to color the vertices of a graph”. In: Communications

of the ACM 22.4 (1979), pp. 251–256.

[12] Zhengdao Chen, Lisha Li, and Joan Bruna. “Supervised Community Detection
with Line Graph Neural Networks”. In: International Conference on Learning
Representations. 2019. url: https://openreview.net/forum?id=H1g0Z3A9Fm.

[13] Changhui Choi and Yinyu Ye. “Solving sparse semideﬁnite programs using the
dual scaling algorithm with an iterative solver”. In: Manuscript, Department of
Management Sciences, University of Iowa, Iowa City, IA 52242 (2000).

16

[14] Edward Dahl. “Neural network algorithms for an np-complete problem: map and
graph coloring”. In: Proc. First Int. Conf. Neural Networks III. 1987, pp. 113–120.

[15] Caterina De Simone et al. “Exact ground states of Ising spin glasses: New experi-
mental results with a branch-and-cut algorithm”. In: Journal of Statistical Physics
80.1-2 (1995), pp. 487–496.

[16] R. Dechter. Constraint Processing. Morgan Kaufmann, 2003.

[17] Amir Dembo, Andrea Montanari, Subhabrata Sen, et al. “Extremal cuts of sparse
random graphs”. In: The Annals of Probability 45.2 (2017), pp. 1190–1217.

[18] Philippe Galinier and Jin-Kao Hao. “Hybrid evolutionary algorithms for graph
coloring”. In: Journal of combinatorial optimization 3.4 (1999), pp. 379–397.

[19] David W Gassen and Jo Dale Carothers. “Graph color minimization using neural
networks”. In: Proceedings of 1993 International Conference on Neural Networks
(IJCNN-93-Nagoya, Japan). Vol. 2. IEEE. 1993, pp. 1541–1544.

[20] Michel X Goemans and David P Williamson. “Improved approximation algorithms
for maximum cut and satisﬁability problems using semideﬁnite programming”. In:
Journal of the ACM (JACM) 42.6 (1995), pp. 1115–1145.

[21] Haidar Harmanani, Jean Hannouche, and Nancy Khoury. “A neural networks
algorithm for the minimum colouring problem using FPGAs”. In: International
Journal of Modelling and Simulation 30.4 (2010), pp. 506–513.

[22] Federico Heras et al. “2006 and 2007 Max-SAT evaluations: Contributed instances”.
In: Journal on Satisﬁability, Boolean Modeling and Computation 4 (2008), pp. 239–
250.

[23] Demian Hespe, Christian Schulz, and Darren Strash. “Scalable kernelization for
maximum independent sets”. In: Journal of Experimental Algorithmics (JEA) 24.1
(2019), pp. 1–22.

[24] Petter Holme and Beom Jun Kim. “Growing scale-free networks with tunable

clustering”. In: Physical review E 65.2 (2002), p. 026107.

[25] John J Hopﬁeld and David W Tank. ““Neural” computation of decisions in opti-

mization problems”. In: Biological cybernetics 52.3 (1985), pp. 141–152.

[26] Selman Kautz. Walksat Home Page. 2019. url: https://www.cs.rochester.edu/

u/kautz/walksat/.

[27] Elias Khalil et al. “Learning combinatorial optimization algorithms over graphs”.
In: Advances in Neural Information Processing Systems. 2017, pp. 6348–6358.

[28] Sebastian Lamm et al. “Finding near-optimal independent sets at scale”. In: Journal

of Heuristics 23.4 (2017), pp. 207–229.

[29] Henrique Lemos et al. “Graph Colouring Meets Deep Learning: Eﬀective Graph Neu-

ral Network Models for Combinatorial Problems”. In: arXiv preprint arXiv:1903.04598
(2019).

[30] Rhyd Lewis. A guide to graph colouring. Vol. 7. Springer, 2015.

17

[31] Rhyd Lewis et al. “A wide-ranging computational comparison of high-performance
graph colouring algorithms”. In: Computers & Operations Research 39.9 (2012),
pp. 1933–1950.

[32] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. “Combinatorial optimization with
graph convolutional networks and guided tree search”. In: Advances in Neural
Information Processing Systems. 2018, pp. 539–548.

[33] NetworkX developer team. NetworkX. 2014. url: https://networkx.github.io/.

[34] Marcelo Prates et al. “Learning to Solve NP-Complete Problems: A Graph Neural
Network for Decision TSP”. In: Proceedings of the AAAI Conference on Artiﬁcial
Intelligence. Vol. 33. 2019, pp. 4731–4738.

[35] P. Raghavendra. “Optimal algorithms and inapproximability results for every
CSP?” In: Proceedings of the 40th ACM Symposium on Theory of Computing. 2008,
pp. 245–254.

[36] Bart Selman, Henry A Kautz, Bram Cohen, et al. “Local search strategies for
satisﬁability testing.” In: Cliques, coloring, and satisﬁability 26 (1993), pp. 521–532.

[37] Daniel Selsam et al. “Learning a SAT Solver from Single-Bit Supervision”. In: Inter-
national Conference on Learning Representations. 2019. url: https://openreview.
net/forum?id=HJMC_iA5tm.

[38] Yoshiyasu Takefuji and Kuo Chun Lee. “Artiﬁcial neural networks for four-coloring
map problems and K-colorability problems”. In: IEEE Transactions on Circuits
and Systems 38.3 (1991), pp. 326–333.

[39] Duncan J Watts. “Networks, dynamics, and the small-world phenomenon”. In:

American Journal of sociology 105.2 (1999), pp. 493–527.

[40] Ke Xu. BHOSLIB: Benchmarks with Hidden Optimum Solutions for Graph Problems
(Maximum Clique, Maximum Independent Set, Minimum Vertex Cover and Vertex
Coloring). 2005. url: http://sites.nlsde.buaa.edu.cn/˜kexu/benchmarks/
graph-benchmarks.htm.

[41] Ke Xu and Wei Li. “Many hard examples in exact phase transitions with application

to generating hard satisﬁable instances”. In: arXiv preprint cs/0302001 (2003).

[42] Ke Xu et al. “A Simple Model to Generate Hard Satisﬁable Instances”. In: IJCAI-
05, Proceedings of the Nineteenth International Joint Conference on Artiﬁcial
Intelligence, Edinburgh, Scotland, UK, July 30 - August 5, 2005. 2005, pp. 337–342.
url: http://ijcai.org/Proceedings/05/Papers/0989.pdf.

[43] Weichi Yao, Afonso S Bandeira, and Soledad Villar. “Experimental performance
of graph neural networks on random instances of max-cut”. In: arXiv preprint
arXiv:1908.05767 (2019).

[44] Yinyu Ye. The Gset Dataset. 2003. url: https://web.stanford.edu/˜yyye/

yyye/Gset/.

18

A. External Software and Data

Table 6 lists the versions and sources of all the external software used in our paper. Table
7 lists the sources of all the evaluation instances that we did not generate ourselves.
All evaluation instances are also provided in our repository at https://github.com/
RUNCSP/RUN-CSP

Table 6: Used Software

Software

Version Author / Source

Tensorﬂow

1.15.2

(Abadi et al. 2015)
https://tensorflow.org

NetworkX

2.4

(NetworkX developer team 2014)
https://networkx.github.io

Loandra

2019

(Berg, Demirovi´c, and Stuckey 2019)
https://maxsat-evaluations.github.io/2019/descriptions

MaxWalkSAT 20

ReduMIS

1.1

(Selman, Kautz, Cohen, et al. 1993)
https://www.cs.rochester.edu/u/kautz/walksat/

(Hespe, Schulz, and Strash 2019)
http://algo2.iti.kit.edu/kamis/

HybridEA

2015

(Galinier and Hao 1999), (Lewis et al. 2012), (Lewis 2015)
http://rhydlewis.eu/resources/gCol.zip

GNN-GCP

2020

(Lemos et al. 2019)
https://github.com/machine-reasoning-ufrgs/GNN-GCP

Table 7: Used Data

Instances Author / Source

Spinglass
2-CNF

(Heras et al. 2008)
http://maxsat.ia.udl.cat/benchmarks/ ,
Unweighted Crafted Benchmarks

Gset

Max-IS
Graphs

(Ye 2003)
https://www.cise.ufl.edu/research/sparse/matrices/Gset/

(Xu 2005)
http://sites.nlsde.buaa.edu.cn/˜kexu/benchmarks/graph-benchmarks.htm

19

B. Reproducibility

This section summarizes the necessary information for replicating our implementation
and exact training procedure. The information can also be found in training scripts
provided in the source code. All trainable matrices in the messaging functions SR were
initialized with uniform Glorot initialization. For activation, bias, and initialization of
the LSTM cell we used the default values provided by TensorFlow 1.14.0. All trainable
parameters were regularized with the ‘2-norm with a weight of 0.01. The pooled vectors
of received messages r(t)
x were normalized with an additional batch normalization layer,
before being passed into the LSTM cell. The discount factor in our loss function was set
to λ = 0.95 in all experiments. The state size of all networks was set to k = 128.

All networks were trained with ttr

max = 30. Training was performed with the Adam
optimizer using the default parameters β1 = 0.9, β2 = 0.999, and (cid:15) = 1 × 10−7. The
learning rate was initialized as 0.001 and decayed with a factor of 0.1 every 5 epochs.
The gradients were clipped at a norm of 1.0.

B.1. Parallel execution

We perform 64 runs on each instance to boost the performance during evaluation. Our
implementation performs these runs in parallel with a single forward pass of the network.
To achieve this, we copy the graph 64 times and combine the disjoint copies into one
larger instance. This instance is then processed by the RUN-CSP network. Since there
are no messages exchanged between connected components, this is equivalent to executing
the network 64 times on the same graph. Evaluating on a single large graph is, especially
for small instances, faster on a GPU than multiple executions on the small graphs.

For the largest IS benchmark graphs the parallel inference with 64 copies exceeds the
memory limit of our GPU. Instead, we performed 8 consecutive runs with 8 copies each
and selected the best assignment afterwards.

C. Hard 3-Col Instances

Here, we describe how the ‘hard’ 3-col graphs used in our vertex coloring experiment are
generated. These instances are 3-colorable Erd˝os-R´enyi graphs and there is (at least)
one edge e such that when adding e to the graph, it is non-3-colorable. To generate such
a graph, we ﬁrst initialize a graph with n nodes and no edges. We then iteratively add
individual edges which are uniformly sampled at random. After adding each edge, we use
a conventional SAT solver to check whether the graph is still 3-colorable. (We use the
pycosat Python package for this task.) This process is stopped once the graph is found to
be non-3-colorable. The graphs with and without the last added edge are then returned
as negative and positive instances, respectively. Our graph generation procedure stops
adding edges when the next randomly chosen edge makes the graph non-3-colorable. In
contrast, Lemos et al. (2019) stopped adding edges when it was possible to add such an
edge that makes the graph non-3-colorable. The graphs generated by Lemos et al. are
thus less dense than ours.

20

D. Max-IS Benchmarks

The RUN-CSP network that was evaluated on the Max-IS benchmark graphs was
trained on our own synthetic benchmark instances. Here, we will describe the generation
procedure of these instances. (Xu and Li 2003; Xu et al. 2005) proposed the RB Model,
which is a general model for generating hard random CSP instances close to the phase
change of satisﬁability. Furthermore, they described how to generate hard instances for
graph problems, including Max-IS instances, by reducing SAT benchmarks of the RB
Model to these problems. We used their generation procedure for Max-IS benchmarks
as described in Xu (2005). Given c ∈ N, p ∈ [0, 1] and α, r > 0, the procedure generates
a graph as follows:

1. Generate c disjoint cliques with k = cα vertices each.

2. Select two random cliques and generate pc2α random edges between them (without

repetition).

3. Run Step 2 for another rc ln c − 1 times (with repetition).

To enforce an optimal independent set of size c, one can exclude one node of each clique
from the process of adding random edges.

We used this procedure to generate 2,000 training instances. For each graph we

uniformly sampled c ∼ U (10, 25), k ∼ U (5, 20) and p ∼ U (0.3, 1.0). We then chose

α =

ln(k)
ln(c)

and r = −

α
ln(1 − p)

(7)

This choice for r is expected to yield ‘hard’ instances according to the RB Model (Xu
et al. 2005). We then used the algorithm described above to generate a graph with the
chosen parameters.

Training was performed with a batch size of 5. Note that we reduced the batch size in
comparison to all other experiments due to the relatively large size of the graphs. The
constant κ that distributes the importance of the losses LCSP and Lsize was reduced to
κ = 0.1 to emphasize the independence condition. Without this reduction the computed
solutions contained multiple edges violating independence and not just one or two as in
the other IS experiment.

E. Additional Experiments

E.1. Detailed Gset Results

Table 8 provides the achieved cut sizes for additional Gset instances. As for the subset
of instances provided in the Experiments Section, the RUN-CSP network performed 64
parallel evaluation runs with tev

max = 500 iterations.

21

Table 8: Achieved cut sizes on Gset instances for RUN-CSP, DSDP and BLS. The
values for DSDP and BLS taken from the literature. DSDP values were only available
for a subset of GSET.

Graph

type

|V |

|E|

RUN-CSP DSDP

BLS

G1
G2
G3
G14
G15
G16
G22
G23
G24
G35
G36
G37
G48
G49
G50
G51
G52
G53
G55

random 800
random 800
random 800
800
decay
decay
800
800
decay
random 2000
random 2000
random 2000
2000
decay
2000
decay
2000
decay
3000
torus
3000
torus
3000
torus
1000
decay
1000
decay
decay
1000
random 5000

19176
19176
19176
4694
4661
4672
19990
19990
19990
11778
11766
11785
6000
6000
6000
5909
5916
5914
12468

11369
11367
11390
2943
2928
2921
13028
13006
13001
7339
7325
7317
6000
6000
5880
3690
3681
3695
10116

-
-
-
2922
2938
-
12960
13006
12933
-
-
-
6000
6000
5880
-
-
-
9960

11624
11620
11622
3064
3050
3052
13359
13344
13337
7684
7678
7689
6000
6000
5880
3848
3851
3850
10294

E.2. Structure Speciﬁc Performance for Max-3-Col

A key feature of RUN-CSP is the unsupervised training procedure, which allows us to
train a network on arbitrary data without knowledge of any optimal solutions. Intuitively,
we would expect a network trained on instances of a particular structure to adapt toward
this class of instances and perform poorer for diﬀerent structures. We will brieﬂy evaluate
this hypothesis for four diﬀerent classes of graphs using the Max-3-Col problem.

Erd˝os-R´enyi Graphs: Graphs are generated by uniformly sampling m distinct edges

between n nodes.

Geometric Graphs: A graph is generated by ﬁrst assigning random positions within a
1 × 1 square to n distinct nodes. Then an edge is added for every pair of points
with a distance less than r.

Powerlaw-Cluster Graphs: This graph model was introduced by Holme and Kim (2002).
Each graph is generated by iteratively adding n nodes. Every new node is connected
to m random nodes that were already inserted. After each edge is added, a triangle
is closed with probability p, i.e. an additional edge is added between the inserted
node and a random neighbor of the other endpoint of the edge.

22

Regular Graphs: We consider random 5-regular graphs as an example for graphs with

very speciﬁc structure class.

For each graph class we generated a training dataset with 4,000 random instances. The
number of nodes was sampled uniformly between 50 and 100 for each graph of all four
classes. For Erd˝os-R´enyi graphs, the edge count m was chosen randomly between 100 and
400. The parameter r of each geometric graph was sampled uniformly from the interval
[0.1, 0.2]. For Powerlaw-Cluster graphs, the parameter m was uniformly sampled from
{1, 2, 3} and p was uniformly drawn from the interval [0, 1]. Five RUN-CSP models were
trained on each dataset. We refer to these groups of models as MER, MGeo, MPow and
MReg. Additionally, 5 models MMixwere trained on a mixed dataset with 1,000 random
instances of each graph class.

For evaluation, we generated 1,000 instances of each class. Table 9 contains the
percentage of unsatisﬁed constraints over the models and graph classes. In general, all
models perform well on the class of structures they were trained on. MGeo and MPow
outperform MER on Erd˝os-R´enyi graphs while MER outperforms MGeo on Powerlaw-
Cluster graphs and MPow on geometric graphs. The networks trained on 5-regular graphs
only perform well on the same class and yield poor results for other structures. Overall,
MMix produced the best results when averaged over all four classes, despite not achieving
the best results for any particular class.

E.3. Detailed 3-Col Results

In Section 3.3 of the main paper we evaluated RUN-CSP on the 3-Col problem for hard
random instances. There, the models were trained on hard 3-colorable instances. To
further evaluate how the training data eﬀects RUN-CSP models, we also applied networks
trained on diﬀerent datasets to the same hard evaluation instances. We trained ﬁve
networks on the negative non-3-colorable counterparts of the positive training instances
used earlier. Furthermore, we applied the ﬁve models of MMix from the previous section
to the hard evaluation instances. The achieved percentages of optimally 3-colored graphs
are provided in Table 10.

Table 9: Percentages of unsatisﬁed constraints for each graph class under the diﬀerent
RUN-CSP models. Values are averaged over 1,000 graphs and the standard deviation is
computed with respect to the ﬁve RUN-CSP models.

Graphs

Erd˝os-R´enyi
Geometric
Pow. Cluster
Regular

MER
(%)

MGeo
(%)

MPow
(%)

MReg
(%)

MMix
(%)

4.75 ±0.01

4.73 ±0.02
10.33 ±0.07 10.16 ±0.04
1.96 ±0.01
2.41 ±0.03

1.89 ±0.00
2.33 ±0.01

4.72 ±0.02
11.39 ±0.66
1.87 ±0.00
2.33 ±0.02

6.69 ±1.60
18.99 ±3.32
2.44 ±0.67
2.32 ±0.00

4.73 ±0.01
10.18 ±0.03
1.89 ±0.00
2.33 ±0.00

Mean

4.83 ±0.02

4.82 ±0.03

5.08 ±0.18

7.61 ±1.40

4.78 ±0.01

23

Table 10: Percentages of hard positive instances for which optimal 3-colorings were
found by diﬀerent RUN-CSP models, Greedy Search (DSatur), and HybridEA. The
RUN-CSP models were trained on only positive (Pos.), only negative instances (Neg.), or
the mixed random graphs from the previous experiment. We report mean and standard
deviation across ﬁve RUN-CSP models that where trained on each dataset.

Nodes RUN-CSP (Pos.) RUN-CSP (Neg.) RUN-CSP (MMix) Greedy HybridEA

50
100
150
200
300
400

98.4 ±0.3
62.5 ±2.7
15.5 ±2.3
2.6 ±0.4
0.1 ±0.0
0.0 ±0.0

98.7 ±0.3
58.9 ±3.0
14.1 ±1.7
1.9 ±0.3
0.2 ±0.1
0.0 ±0.0

97.4 ±0.5
43.2 ±2.5
6.7 ±0.4
0.5 ±0.3
0.1 ±0.1
0.0 ±0.0

34.0
6.7
1.5
0.5
0.0
0.0

100.0
100.0
98.7
88.9
39.9
15.3

On graphs with over 50 nodes the models trained on negative hard instance perform
marginally worse than those trained on the positive counterparts. The models of MMix
perform worse than those trained on hard instances, especially for the larger graphs.
However, they do still outperform the traditional greedy heuristic by a signiﬁcant margin.

E.4. Convergence of RUN-CSP

We now use the Max-3-Col problem to illustrate the convergence behavior of RUN-CSP
networks. We generated a random Erd˝os-R´enyi graph with 500 nodes and 2,000 edges.
One of the RUN-CSP models of MMix from the previous experiment was used to predict
a maximum 3-coloring for this graph. We performed 64 parallel evaluation runs for
tev
max = 100 iterations. For each iteration we obtain 64 values for the numbers of correctly
colored edges, one for each parallel run. Figure 4 plots the distributions of these values
across all iterations as boxplots. In the ﬁrst 20 time steps, the predictions improve quickly,
as the network moves away from its random initialization towards a better solution. The
network continues to ﬁnd better color assignments far past iteration 30, which is the
number iterations used during training, the absolute maximum was reached in iteration
67.

E.5. Coloring Benchmark Instances

We evaluated RUN-CSP on a number of k-COL benchmark instances, similar to Lemos
et al. (2019). We obtained the 20 graphs from the COLOR02 Workshop2 that were
also used to evaluate GNN-GCP to enable a direct comparison. Any single RUN-CSP
network is bound to a ﬁxed domain size d. A network cannot use more than d colors and
even if a given graph can be colored with less than d colors, RUN-CSP will still use all d
colors. Thus, in order to compute a chromatic number, we trained 14 distinct networks
with domain sizes ranging from 4 to 17. We apply each network to a given graph and

2The graphs can be downloaded at https://mat.tepper.cmu.edu/COLOR02/

24

Figure 4: The distribution of satisﬁed edges for 64 parallel evaluation runs on a random
Erd˝os-R´enyi graph. For each iteration we show a boxplot that represents the 64 numbers
of satisﬁed edges at a given time step. The whiskers of the boxplots include the minimum
and maximum values. The limits of the box are given by the upper and lower quartile.
The red line represents the median in each iteration. The overall maximum (reached
after 67 steps) is shown as a horizontal blue line.

choose the output that achieved a conﬂict free coloring with the fewest colors as our ﬁnal
result.

Unlike our experiments for Max-Cut, we found that RUN-CSP networks trained
purely in Erd˝os-R´enyi graphs performed poorly in the given setup. Instead, we generated
mixed datasets that consist of 30% Erd˝os-R´enyi graphs, 30% Geometric graphs, 30%
Powerlaw-Cluster graphs and 10% Connected Caveman graphs (Watts 1999). Connected
Caveman graphs are a graph model introduced by Watts (1999) that depends on two
numbers l, k ∈ N. All other graph classes were introduced in the Section 5.2.

The training datasets were adapted to the number c ∈ {4, . . . , 17} of colors available
such that networks with more colors were trained on denser graphs. Each training set
contained 4,000 random graphs generated according to the following parameters:

Erd˝os-R´enyi: n ∼ U (50, 100), m ∼ U (2n, n · c)

Geometric: n ∼ U (80, 28c), r ∼ U (0.1, 0.2)

Powerlaw-Cluster: n ∼ U (20, 20c), m ∼ U (1, 4), r ∼ U (1, 2)

Connected Caveman: l ∼ U (10, 20), k ∼ U (max(4, c − 2), c + 2)

We compare RUN-CSP to the classical methods used in our previous vertex coloring
experiment, namely HybridEA and the greedy DSatur strategy. As before, HybridEA

25

Table 11: Results for k-Col on benchmarks instances. We provide the number of
colors needed for an optimal coloring by RUN-CSP, Greedy (DSatur) and HybridEA. For
GNN-GCP we provide the prediction as reported by Lemos et al. (2019). This method
only predicts the chromatic number and can therefore underestimate the true value.

Benchmark

|V | Opt RUN-CSP GNN-GCP DSatur HybridEA

Queen5 5
Queen6 6
myciel5
Queen7 7
Queen8 8
1-Insertions 4
huck
jean
Queen9 9
david
Mug88 1
myciel6
Queen8 12
games120
Queen11 11
anna
2-Insertions4
Queen13 13
myciel7
homer

25
36
47
49
64
67
74
80
81
87
88
95
96
120
121
138
149
169
191
561

5
7
6
7
9
4
11
10
10
11
4
7
12
9
11
11
4
13
8
13

5
8
6
10
11
5
11
10
17
11
4
8
17
9
> 17
11
5
> 17
9
17

6
7
5
8
8
4
8
7
9
9
3
7
10
6
12
11
4
14
NA
14

5
8
6
9
10
5
11
10
12
11
4
7
13
9
15
11
5
17
8
13

5
7
6
7
9
5
11
10
10
11
4
7
12
9
12
11
5
14
8
13

was allowed to perform 500 million constraint checks on each graph. Table 11 provides
the number of colors that each method needed to color the graphs without conﬂict. For
comparison, we provide the predicted chromatic number of GNN-GCP as reported by
Lemos et al. (2019). On most benchmark instances HybridEA ﬁnds the optimal chromatic
number and otherwise uses one additional color. For instances with a chromatic number
of up to 6 the performance of our network is identical to DSatur and HybridEA. In
general, RUN-CSP performs slightly worse than the greedy DSatur algorithm. The
number of instances for which RUN-CSP found optimal solutions is larger than the
number of graphs for which GNN-GCP predicted the correct chromatic number. We
point out that the focus of our architecture is a maximization task associated with a
domain of ﬁxed size. Despite this, RUN-CSP was able to outperform GNN-GCP on this
task, while also predicting color assignments.

26

