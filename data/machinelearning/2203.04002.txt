2
2
0
2

r
a

M
8

]
S
D
.
s
c
[

1
v
2
0
0
4
0
.
3
0
2
2
:
v
i
X
r
a

Semi-Random Sparse Recovery in Nearly-Linear Time

Jonathan A. Kelner∗

Jerry Li†

Allen Liu‡

Aaron Sidford§

Kevin Tian¶

Abstract

Sparse recovery is one of the most fundamental and well-studied inverse problems. Standard
statistical formulations of the problem are provably solved by general convex programming
techniques and more practical, fast (nearly-linear time) iterative methods. However, these latter
“fast algorithms” have previously been observed to be brittle in various real-world settings.

∈

let A

We investigate the brittleness of fast sparse recovery algorithms to generative model changes
through the lens of studying their robustness to a “helpful” semi-random adversary, a framework
which tests whether an algorithm overﬁts to input assumptions. We consider the following
Rn×d be a measurement matrix which contains an unknown subset of
basic model:
Rm×d which are bounded and satisfy the restricted isometry property (RIP), but is
rows G
Rd be s-sparse, and given either exact measurements b = Ax⋆
otherwise arbitrary. Letting x⋆
or noisy measurements b = Ax⋆+ξ, we design algorithms recovering x⋆ information-theoretically
optimally in nearly-linear time. We extend our algorithm to hold for weaker generative models
relaxing our planted RIP row subset assumption to a natural weighted variant, and show that
our method’s guarantees naturally interpolate the quality of the measurement matrix to, in
some parameter regimes, run in sublinear time.

∈

∈

Our approach diﬀers from that of prior fast iterative methods with provable guarantees
under semi-random generative models [CG18, LSTZ20], which typically separate the problem
of learning the planted instance from the estimation problem, i.e. they attempt to ﬁrst learn
the planted “good” instance (in our case, the matrix G). However, natural conditions on a
submatrix which make sparse recovery tractable, such as RIP, are NP-hard to verify and hence
ﬁrst learning a suﬃcient row reweighting appears challenging. We eschew this approach and
design a new iterative method, tailored to the geometry of sparse recovery, which is provably
robust to our semi-random model. Our hope is that our approach opens the door to new robust,
eﬃcient algorithms for other natural statistical inverse problems.

∗MIT, kelner@mit.edu. Supported in part by NSF awards CCF-1955217, CCF-1565235, and DMS-2022448.
†Microsoft Research, jerrl@microsoft.com. This work was partially done while visiting the Simons Institute for

the Theory of Computing.

‡MIT, cliu568@mit.edu. This work was partially done while working as an intern at Microsoft Research, and was

supported in part by an NSF Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.

§Stanford University, sidford@stanford.edu. Supported in part by a Microsoft Research Faculty Fellowship,
NSF CAREER Award CCF-1844855, NSF Grant CCF-1955039, a PayPal research award, and a Sloan Research
Fellowship.

¶Stanford University, kjtian@stanford.edu. This work was partially done while visiting the Simons Institute for
the Theory of Computing, and was supported in part by a Google Ph.D. Fellowship, a Simons-Berkeley VMware
Research Fellowship, a Microsoft Research Faculty Fellowship, NSF CAREER Award CCF-1844855, NSF Grant
CCF-1955039, and a PayPal research award.

 
 
 
 
 
 
Contents

1 Introduction

1.1 Our techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2 Preliminaries

1
5
7

9

3 Exact recovery

9
3.1 Radius contraction using step oracles . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
3.2 Designing a step oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.3 Equivalence between Assumption 1 and RIP . . . . . . . . . . . . . . . . . . . . . . . 18
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.4 Putting it all together

4 Noisy recovery

20
4.1 Radius contraction above the noise ﬂoor using step oracles . . . . . . . . . . . . . . . 21
4.2 Designing a strong step oracle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
4.3 Deterministic assumptions for noisy regression . . . . . . . . . . . . . . . . . . . . . 28
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.4 Putting it all together

A Greedy and non-convex methods fail in the semi-random setting

A.1 Iterative hard thresholding
A.2 Orthogonal matching pursuit
A.3 Convex methods

36
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

B Deferred proofs

37

1

Introduction

Sparse recovery is one of the most fundamental and well-studied inverse problems, with numer-
ous applications in prevalent real-world settings [EK12]. In its most basic form, we are given an
Rm
d and measurements b = Gx⋆ for an unknown
entrywise Gaussian measurement matrix G
Rd; the goal of the problem is to recover x⋆. Seminal works by Cand`es, Romberg,
s-sparse x⋆
and Tao [CR05, CT06, CRT06] showed that even when the linear system in G is extremely under-
constrained, recovery is tractable so long as m = Ω(s log d). Further they gave a polynomial-time
algorithm known as basis pursuit based on linear programming recovering x⋆ in this regime.

∈

∈

×

Unfortunately, the runtime of linear programming solvers, while polynomial in the size of the
input, can still be prohibitive in many high-dimensional real-world settings. Correspondingly, a
number of alternative approaches which may broadly be considered ﬁrst-order methods have been
developed. These methods provably achieve similar recovery guarantees under standard generative
models such as Gaussian measurements, with improved runtimes compared to the aforementioned
convex programming methods. We refer to these ﬁrst-order methods through as “fast” algorithms
throughout and they may roughly be placed in the following (potentially non-disjoint) categories.
• Greedy algorithms, e.g. [MZ93, PRK93, NV10], seek to greedily ﬁnd elements in the support

of the true x⋆ using diﬀerent combinatorial search criteria.

• Non-convex iterative algorithms, e.g. [NT09, BD09, BD10, MD10, Fou11], directly opti-

mize a (potentially non-convex) objective over a non-convex domain.

• Convex ﬁrst-order methods, e.g. [FN03, DDDM04, CW05, BT09, BBC11, NRWY12,
ANW12] quickly solve the convex objective underlying basis pursuit using ﬁrst-order methods.

We also note that theoretically, when n is suﬃciently large, recent advances by [vdBLSS20,
vdBLL+21], also obtain fast runtimes for the relevant linear programming objective. The fastest
IPM for the noiseless sparse recovery objective runs in time1
O(nd + n2.5) which is nearly-linear
d2/3. For a range of (superlogarithmic, but sublinear) n, these runtimes
when A is dense and n
are no longer nearly-linear; furthermore, these IPMs are second-order and our focus is on designing
ﬁrst-order sparse recovery methods, which are potentially more practical.2

≪

e

It has often been observed empirically that fast ﬁrst-order methods can have large error, or fail
to converge, in real-world settings [DNW13, JTK14, PCBVB14] where convex programming-based
algorithms (while potentially computationally cumbersome) perform well statistically [ZWW+16,
AP17]. This may be surprising, given that in theory, fast algorithms essentially match the statistical
performance of the convex programming-based algorithms under standard generative assumptions.
While there have been many proposed explanations for this behavior, one compelling argument is
that fast iterative methods used in practice are more brittle to changes in modeling assumptions.
We adopt this viewpoint in this paper, and develop fast sparse recovery algorithms which achieve
optimal statistical rates under a semi-random adversarial model [BS95, FK01], a popular framework
for investigating the robustness of learning algorithms under changes to the data distribution.

Semi-random adversaries. Semi-random adversaries are a framework for reasoning about algo-
rithmic robustness to distributional shift. They are deﬁned in statistical settings, and one common
type of semi-random adversary is one which corresponds to generative models where data has been
corrupted in a “helpful” or “monotone” way. Such a monotone semi-random adversary takes a
dataset from which learning is information-theoretically tractable, and augments it with additional
information; this additional information may not break the problem more challenging from an
eO to hide polylogarithmic factors in problem parameters for brevity of exposition throughout the paper.
1We use
2We also note that these IPM results do not immediately apply to natural (nonlinear) convex programs for sparse

recovery under noisy observations, see Appendix A.

1

information-theoretic perspective,3 but may aﬀect the performance of algorithms in other ways.
In this paper, we consider a semi-random adversary which makes the computational problem more
diﬃcult without aﬀecting the problem information-theoretically, by returning a consistent super-
set of the unaugmented observations. This contrasts with other adversarial models such as gross
corruption [Ans60, Tuk60, Hub64, Tuk75], where corruptions may be arbitrary, and the corrupted
measurements incorrect. It may be surprising that a “helpful” adversary has any implications what-
soever on a learning problem, from either an information-theoretic or computational standpoint.

Typically, convex programming methods for statistical recovery problems are robust to these
sorts of perturbations — in brief, this is because constraints to a convex program that are met
by an optimum point does not change the optimality of that point. However, greedy and non-
convex methods — such as popular practical algorithms for sparse linear regression — can be
susceptible to semi-random adversaries. Variants of this phenomenon have been reported in many
common statistical estimation problems, such as stochastic block models and broadcast tree mod-
els [MPW16], PAC learning [Blu03], matrix completion [Moi17, CG18], and principal component
regression [BRW21]. This can be quite troubling, as semi-random noise can be thought of as a rel-
atively mild form of generative model misspeciﬁcation: in practice, the true distribution is almost
always diﬀerent from the models considered in theory. Consequently, an algorithm’s non-robustness
to semi-random noise is suggestive that the algorithm may be more unreliable in real-world settings.
We consider a natural semi-random adversarial model for sparse recovery (see e.g. page 284
of [AV18]), which extends the standard restricted isometry property (RIP) assumption, which
states that applying matrix A approximately preserves the ℓ2 norm of sparse vectors. Concretely,
throughout the paper we say matrix A satisﬁes the (s, c)-restricted isometry (RIP) property if for
all s-sparse vectors v,

1
v
c k

2
2 ≤ k

k

Av

2
2 ≤

k

2
2 .

c

v

k

k

We state a basic version of our adversarial model here, and defer the statement of the fully general
version to Deﬁnition 2.4 We defer the introduction of notation used in the paper to Section 2.

Deﬁnition 1 (pRIP matrix). Let m, n, d
(planted RIP) if there is an (unknown) G

G is (Θ(s), Θ(1))-RIP for appropriate constants, and

1
√m
we say A is pRIP.

N be known with n
Rm

d is ρ-pRIP
m. We say A
d such that each row of G is also a row of A and
O(1) for brevity

ρ. When ρ =

Rn

≥

∈

×

×

∈
∈

G
k

kmax ≤

Under the problem parameterizations used in this paper, standard RIP matrix constructions
O(1) with high probability. For example, when G is entrywise Gaussian and m =
satisfy ρ =
Θ(s log d), a tail bound shows that with high probability we may set ρ = O(√log d) to be compatible
with the assumptions in Deﬁnition 1.

e

e

∈

Rm

pRIP matrices can naturally be thought of as arising from a semi-random adversarial model as
d is generated, for example from a standard ensemble (e.g.
follows. First, an RIP matrix G
×
d by reshuﬄing
Gaussian or subsampled Hadamard). An adversary inspects G, and forms A
and arbitrarily augmenting rows of G. Whenever we refer to a “semi-random adversary” in the
remainder of the introduction, we mean the adversary provides us a pRIP measurement matrix A.
The key recovery problem we consider in this paper is recovering an unknown s-sparse vector
Rn through A. We consider both the noiseless or exact setting

Rd given measurements b

∈
3There are notable exceptions, e.g. the semi-random stochastic block model of [MPW16].
4When clear from context, as it will be throughout the main sections of the paper, s will always refer to the
Rn×d. For example, the parameter

Rd in an exact or noisy recovery problem through A

sparsity of a vector x⋆
s in Deﬁnition 1 is the sparsity of the vector in an associated sparse recovery problem.

Rn

x⋆

∈

∈

∈

∈

×

2

where b = Ax⋆ and the noisy setting where b = Ax⋆ + ξ for bounded ξ.
In the noiseless set-
ting in particular, the semi-random adversary hence only gives the algorithm additional consistent
measurements of the unknown s-sparse vector x⋆. In this sense, the adversary is only “helpful,”
as it returns a superset of information which is suﬃcient for sparse recovery (formally, this ad-
versary cannot break the standard restricted nullspace condition which underlies the successful
performance of convex programming methods). We note n may be much larger than m, i.e. we
impose no constraint on how many measurements the adversary adds.

Semi-random sparse recovery in nearly-linear time. We devise algorithms which match the
nearly-linear runtimes and optimal recovery guarantees of faster algorithms on fully random data,
but which retain both their runtime and the robust statistical performances of convex programming
methods against semi-random adversaries. In this sense, our algorithms obtain the “best of both
worlds.” We discuss and compare more extensively to existing sparse recovery algorithms under
Deﬁnition 1 in the following section. We ﬁrst state our result in the noiseless observation setting.

Theorem 1 (informal, see Theorem 3). Let x⋆
be pRIP. There is an algorithm, which given A and b = Ax⋆, runs in time
with high probability.

∈

Rd be an unknown s-sparse vector. Let A

d
×
O(nd), and outputs x⋆

∈

Rn

Since our problem input is of size nd, our runtime in Theorem 1 is nearly-linear in the problem
size. We also extend our algorithm to handle the noisy observation setting, where we are given
perturbed linear measurements of x⋆ from a pRIP matrix.

e

Theorem 2 (informal, see Theorem 4). Let x⋆
Rn
be arbitrary. Let A
in time

∈

×

O(nd), and with high probability outputs x satisfying

Rn
d be pRIP. There is an algorithm, which given A and b = Ax⋆ + ξ, runs

Rd be an unknown s-sparse vector, and let ξ

∈

∈

e

x⋆

x
k

−

k2 ≤

O

1
√m k

ξ

(cid:18)

,

k2,(m)

(cid:19)

where

ξ
k

k2,(m) denotes the ℓ2 norm of the largest m entries of ξ by absolute value.

The error scaling of Theorem 2 is optimal in the semi-random setting. Indeed, when there is no
semi-random noise, the guarantees of Theorem 2 exactly match the standard statistical guarantees
in the fully-random setting for sparse recovery, up to constants; for example, when A = √mI (which
is clearly RIP, in fact an exact isometry, after rescaling), it is information-theoretically impossible
to obtain a better ℓ2 error.5 The error bound of Theorem 2 is similarly optimal in the semi-random
setting because in the worst case, the largest entries of ξ may correspond to the rows of the RIP
matrix from which recovery is information-theoretically possible.

Performance of existing algorithms. To contextualize Theorems 1 and 2, we discuss the
performance of existing algorithms for sparse recovery under the semi-random adversarial model
of Deﬁnition 1. First, it can be easily veriﬁed that our semi-random adversary never changes
the information-theoretic tractability of sparse recovery. In the noiseless setting for example, the
performance of the minimizer to the classical convex program based on ℓ1 minimization,

min
Ax=b k

x

k1 ,

is unchanged in the presence of pRIP matrices (as x⋆ is still consistent with the constraint set, and
in particular a RIP constraint set), and hence the semi-random problem can be solved in polynomial

5In the literature it is often standard to scale down the sensing matrix A by √m; this is why our error bound is
similarly scaled. However, this scaling is more convenient for our analysis, especially when stating weighted results.

3

time via convex programming. This suggests the main question we address: can we design a near-
linear time algorithm obtaining optimal statistical guarantees under pRIP measurements?

As alluded to previously, standard greedy and non-convex methods we have discussed may fail
to converge to the true solution against appropriate semi-random adversaries generating pRIP ma-
trices. We give explicit counterexamples to several popular methods such as orthogonal matching
pursuit and iterative hard thresholding in Appendix A. Further, it seems likely that similar coun-
terexamples also break other, more complex methods commonly used in practice, such as matching
pursuit [MZ93] and CoSaMP [NT09].

Additionally, while fast “convex” iterative algorithms (e.g. ﬁrst-order methods for solving ob-
jectives underlying polynomial-time convex programming approaches) will never fail to converge to
the correct solution given pRIP measurements, the analyses which yield fast runtimes for these al-
gorithms [NRWY12, ANW12] rely on properties such as restricted smoothness and strong convexity
(a specialization of standard conditioning assumptions to numerically sparse vectors). These hold
under standard generative models but again can be broken by pRIP measurements; consequently,
standard convergence analyses of “convex” ﬁrst-order methods may yield arbitrarily poor rates.

One intuitive explanation for why faster methods fail is that they depend on conditions such as
incoherence [DS89] or the restricted isometry property [CT06], which can be destroyed by a semi-
random adversary. For instance, RIP states that if S is any subset of m = Θ(s) columns of A, and
AS is the submatrix formed by taking those columns of A, then A⊤S AS is an approximate isometry
(i.e. it is well-conditioned). While it is well-known that RIP is satisﬁed with high probability
when A consists of Θ(s log d) Gaussian rows, it is not too hard to see that augmenting A with
additional rows can easily ruin the condition number of submatrices of this form. In contrast, convex
methods work under weaker assumptions such as the restricted nullspace condition, which cannot
be destroyed by the augmentation used by pRIP matrices. Though these weaker conditions (e.g.
the restricted nullspace condition) suﬃce for algorithms based on convex programming primitives,
known analyses of near-linear time “fast” algorithms require additional instance structure, such as
incoherence or RIP. Thus, it is plausible that fast algorithms for sparse recovery are less robust to
the sorts of distributional changes that may occur in practice.

Beyond submatrices. Our methods naturally extend to a more general setting (see Deﬁnition 2,
wherein we deﬁne “weighted RIP” (wRIP) matrices, a generalization of Deﬁnition 1). Rather than
assuming there is a RIP submatrix G, we only assume that there is a (nonnegative) reweighting of
the rows of A so that the reweighted matrix is “nice,” i.e. it satisﬁes RIP. Deﬁnition 1 corresponds
to the special case of this assumption where the weights are constrained to be either 0 or 1 (and
hence must indicate a subset of rows). In our technical sections (Sections 3 and 4), our results are
stated for this more general semi-random model, i.e. sparse recovery from wRIP measurements.
For simplicity of exposition, throughout the introduction, we mainly focus on the simpler pRIP
sparse recovery setting described following Deﬁnition 1.

Towards instance-optimal guarantees. While the performance of the algorithms in Theo-
rems 1 and 2 is already nearly-optimal in the worst case semi-random setting, one can still hope to
improve our runtime and error bounds in certain scenarios. Our formal results, Theorems 3 and 4,
provide these types of ﬁne-grained instance-optimal guarantees in several senses.

In the noiseless setting (Theorem 3), if it happens to be that the entire matrix A is RIP (and
not just G), then standard techniques based on subsampling the matrix can be used to solve the
O(sd) with high probability. For example, if A is pRIP where, following the
problem in time
notation of Deﬁnition 1, G is entrywise Gaussian, and the adversary chose to give us additional
Gaussian rows, one could hope for a runtime improvement (simply by ignoring the additional
measurements given). Theorem 3 obtains a runtime which smoothly interpolates between the two

e

4

e

≈

O(sd) when m

Θ(s) rows which is RIP, then we show that our algorithm runs in sublinear time

regimes of a worst-case adversary and an adversary which gives us additional random measurements
from an RIP ensemble. Roughly speaking, if there exists a (a priori unknown) submatrix of A of
s
m ),
m
≫
n. We show this holds in our weighted semi-random model (under wRIP
which is
e
measurements, Deﬁnition 2) as well, where the runtime depends on the ratio of the ℓ1 norm of the
norm, a continuous proxy for the number of RIP rows under pRIP.
(best) weight vector to its ℓ
We show a similar interpolation holds in the noisy measurement setting, both in the runtime
sense discussed previously, and also in a statistical sense. In particular, Theorem 4 achieves (up to
logarithmic factors) the same interpolating runtime guarantee of Theorem 3, but further attains a
squared ℓ2 error which is roughly the average of the m largest elements of the squared noise vector
ξ (see the informal statement in Theorem 2). This bound thus improves as m
Θ(s); we show
it extends to weighted RIP matrices (Deﬁnition 2, our generalization of Deﬁnition 1) in a natural
way depending on the ℓ

-ℓ1 ratio of the weights.

O(nd

≫

∞

e

e

·

∞

1.1 Our techniques

Our overall approach for semi-random sparse recovery is fairly diﬀerent from two recent works in
the literature which designed fast iterative methods succeeding under a semi-random adversarial
model [CG18, LSTZ20].
In particular, these two algorithms were both based on the following
natural framework, which separates the “planted learning” problem (e.g. identifying the planted
benign matrix) from the “estimation” task (e.g. solving a linear system or regression problem).

1. Compute a set of weights for the data (in linear regression for example, these are weights on
each of the rows of a measurement matrix A), such that after re-weighting, the data ﬁts the
input assumptions of a fast iterative method which performs well on a fully random instance.

2. Apply said fast iterative algorithm on the reweighted data in a black-box manner.

[LSTZ20] studied the standard problem of overdetermined linear
To give a concrete example,
regression with a semi-random adversary, where a measurement matrix A is received with the
promise that A contains a “well-conditioned core” G. The algorithm of [LSTZ20] ﬁrst learned a
re-weighting of the rows of A by a diagonal matrix W
2 , such that the resulting system in A⊤WA
is well-conditioned and hence can be solved using standard ﬁrst-order methods.

1

In the case of semi-random sparse recovery, there appear to be signiﬁcant barriers to reweighting
approaches (which we will shortly elaborate on). We take a novel direction that involves designing a
new nearly-linear time iterative method for sparse recovery tailored to the geometry of the problem.

Why not reweight the rows? There are several diﬃculties which are immediately encountered
when one tries to use the aforementioned reweighting framework for sparse recovery. First of all,
there is no eﬀective analog of condition number for an underdetermined linear system. The standard
assumption on the measurement matrix A to make sparse recovery tractable for fast iterative
methods is that A satisﬁes RIP, i.e. A is roughly an isometry when restricted to O(s)-sparse vectors.
However, RIP is NP-hard to verify [BDMS13] and this may suggest that it is computationally hard
to try, say, learning a reweighting of the rows of A such that the resulting reweighted matrix is
guaranteed to be RIP (though it would be very interesting if this were achievable). More broadly,
almost all explicit conditions (e.g. RIP, incoherence etc.) which make sparse recovery tractable
for fast algorithms are conditions about subsets of the columns of A. Thus, any approach which
reweights rows of A such that column subsets of the reweighted matrix satisfy an appropriate
condition results in optimization problems that seems challenging to solve in nearly-linear time.

The geometry of sparse recovery. We now explain our new approach, and how we derive
deterministic conditions on the steps of an iterative method which certify progress by exploiting

5

the geometry of sparse recovery. We focus on the clean observation setting in this technical overview.
Suppose that we wish to solve a sparse regression problem Ax⋆ = b where x⋆ is s-sparse, and we
are given A and b. To ﬁx a scale, suppose for simplicity that we know
k2 = 1.
Also, assume for the purpose of conveying intuition that A is pRIP, and that the planted matrix
G in Deﬁnition 1 is an entrywise random Gaussian matrix.

k1 = √s and

x⋆
k

x⋆

k

We next consider a natural family of iterative algorithms for solving the system Ax⋆ = b. For
simplicity, assume that our current iterate is xt = 0. Inspired by standard gradient methods for
solving regression, we consider “ﬁrst-order” algorithms of the following form:

yt ←
xt+1 ←

xt + A⊤u
Π(yt)

(1)

∈

Rn are coeﬃcients to be computed (for example, a natural attempt could be to set u to
where u
2
be a multiple of Axt −
2 at xt) and Π
b, resulting in the step being a scaled gradient of
denotes projection onto the (convex) ℓ1 ball of radius √s. Our goal will be to make constant-factor
O(1) iteration method. Note
k2 in each application of (1), to yield a
progress in terms of
that xt+1 must at minimum make constant factor progress in the direction x⋆
xt (in terms of
decreasing the projection onto this direction) if we hope to make constant factor progress in overall
distance to x⋆. In other words, we must have

Ax
k

xt −

x⋆

−

−

e

k

k

b

xt+1 −
h

xt, x⋆

xti

= Ω(
k

−

x⋆

−

2
2) = Ω(1) .

xtk

First, observe that by the deﬁnition of our step and shifting so that xt = 0, the point yt satisﬁes

A⊤u, x⋆
h
so to obtain a corresponding progress lower bound for the move to yt, we require

Axt) = u⊤b,

yt −
h

= u⊤(b

xt, x⋆

xti

xti

−

−

=

−

u⊤b = Ω(1).

(2)

Of course, this condition alone is not enough, for two reasons: the step also moves in directions
orthogonal to x⋆
xt, and we have not accounted for the projection step Π. If all of the rows of A
n
were random, then standard Gaussian concentration implies that we expect
d , and thus

−

b

k2 =

k

to satisfy (2) we need

w
k

k2 ≥

q

d
n . This also implies

p

d
n

,

2 ≈ r

yt −
k

xtk2 =

A⊤w

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

since for Gaussian A, we expect that its rows are roughly orthogonal. Moreover, since
1 in
the typical underconstrained setting, almost all of the step from xt to yt is actually orthogonal to
the desired “progress direction” parallel to x⋆
xt. This appears to be a serious problem, because
in order to argue that our algorithm makes progress, we need to argue that the ℓ1 projection step
xt+1 = Π(yt) “undoes” this huge ℓ2 movement orthogonal to the progress direction (see Figure 1).
Our key geometric insight is that the ℓ1 ball is very thin in most directions, but is thick in
directions that correspond to “numerically sparse” vectors, namely vectors with bounded ℓ1 to ℓ2
ratios. Crucially, the movement of our step in the progress direction parallel to x⋆
xt is numerically
sparse because x⋆
xt is itself O(s)-numerically sparse by assumption. However, for Gaussian A,
the motion in the subspace orthogonal to the progress direction ends up being essentially random
-bounded). Formally, we leverage this decomposition to
(and thus is both not sparse, and is ℓ

q

−

−

−

d
n ≫

∞

6

yt

d
n

q

xt+1

1

xt

x⋆

Figure 1: The eﬀect of ℓ1 projection on iterate progress. The dashed line represents a facet of the
ℓ1-ball around xt of radius

x⋆

xt −
k

k1.

show that the ℓ1 projection keeps most of the forward movement in the progress direction x⋆
−
but eﬀectively ﬁlters out much of the orthogonal motion, as demonstrated by the ﬁgure below.

xt

This geometric intuition is the basis for the deterministic conditions we require of the steps of
our iterative method, to guarantee that it is making progress. More precisely, the main condition
we require of our step in each iteration, parameterized by the coeﬃcients u used to induce (1), is
that yt −

xt has a “short-ﬂat” decomposition into two vectors p + e where

p

k2 = O(1) is “short” and

k

e
k∞

k

= O

1
√s

(cid:18)

(cid:19)

is “ﬂat”.

The above bounds are rescaled appropriately in our actual method. We state these requirements
formally (in the clean observation case, for example) in Deﬁnition 3, where we deﬁne a “step oracle”
which is guaranteed to make constant-factor progress towards x⋆. By combining the above short-
ﬂat decomposition requirement with a progress requirement such as (2), we can show that as long
as we can repeatedly implement a satisfactory step, our method is guaranteed to converge rapidly.
This framework for sparse recovery eﬀectively reduces the learning problem to the problem of
implementing an appropriate step oracle. Note that a valid step always exists by only looking
at the Gaussian rows. To complete our algorithm, we give an implementation of this step oracle
(i.e. actually solving for a valid step) which runs in nearly-linear time even when the data is
augmented by a semi-random adversary (that is, our measurement matrix is pRIP or wRIP rather
than RIP), and demonstrate that our framework readily extends to the noisy observation setting.
Our step oracle is motivated by stochastic gradient methods. In particular, we track potentials
corresponding to the progress made by our iterative method and the short-ﬂat decomposition, and
show that uniformly sampling rows of a pRIP (or wRIP) matrix A and taking steps in the direction
of these rows which maximally improve our potentials rapidly implements a step oracle.

1.2 Related work

Sparse recovery. Sparse recovery, and variants thereof, are fundamental statistical and algo-
rithmic problems which have been studied in many of settings, including signal processing [LF81,

7

SS86, DS89, BD09, BCDH10], and compressed sensing [CR05, CT06, CRT06, Don06, RV06]. A
full review of the extensive literature on sparse recovery is out of the scope of the present paper;
we refer the reader to e.g. [EK12, DDEK12, Kut13, Sch18] for more extensive surveys.

Within the literature on sparse recovery, arguably the closest line of work to ours is the line
of work which attempts to design eﬃcient algorithms which work when the restricted condition
number of the sensing or measurement matrix is large. Indeed, it is known that many nonconvex
methods fail when the restricted condition number of the sensing matrix is far from 1, which is
often the case in applications [JTK14]. To address this, several works [JTK14, Sch18] have designed
novel non-convex methods which still converge, when the restricted condition number of the matrix
is much larger than 1. However, these methods still require that the restricted condition number is
constant or bounded, whereas in our setting, the restricted condition number could be arbitrarily
large due to the generality of the semi-random adversary assumption.

Another related line of work considers the setting where, instead of having a sensing matrix
(0, Σ), for some
with rows which are drawn from an isotropic Gaussian, have rows drawn from
potentially ill-conditioned Σ [BRT09, RWY10, VdGL13, JTK14, KM14, DHL17, ZWJ17, Bel18,
KKMR21]. This setting is related to our semi-random adversarial model, in that the information-
theoretic content of the problem does not change, but obtaining eﬃcient algorithms which match
the optimal statistical rates is very challenging. However, there does not appear to be any further
concrete connection between this “ill-conditioned covariance” setting and the semi-random model
Indeed, the ill-conditioned setting appears to be qualitatively much
we consider in this paper.
in particular, [KKMR21] shows evidence that there are in fact no
more diﬃcult for algorithms:
eﬃcient algorithms that achieve the optimal statistical rates, without additional assumptions on Σ.
In contrast in the semi-random setting, polynomial-time convex programming approaches, while
having potentially undesirable superlinear runtimes, still obtain optimal statistical guarantees.

N

Finally as discussed earlier in the introduction, there is a large body of work on eﬃcient algo-
rithms for sparse recovery in an RIP matrix (or a matrix satisfying weaker or stronger analogous
properties). These works e.g. [CR05, CT06, CRT06, MZ93, PRK93, NV10, NT09, BD09, BD10,
MD10, Fou11, FN03, DDDM04, CW05, BT09, BBC11, NRWY12, ANW12] are typically based on
convex programming or diﬀerent iterative ﬁrst-order procedures.

Semi-random models. Semi-random models were originally introduced in a sequence of in-
novative papers [BS95, FK01] in the context of graph coloring.
In theoretical computer sci-
ence, semi-random models have been explored in many settings, for instance, for various graph-
structured [FK00, FK01, CSX12, MMV12, MMV14] and constraint satisfaction problems [KMM11].
More recently, they have also been studied for learning tasks such as clustering problems and com-
munity detection [ES09, MS10, MMV13, CJSX14, GRSY14, MMV15, MMV16, MPW16], matrix
completion [CG18], and linear regression [LSTZ20]. We refer the reader to [Rou21] for a more thor-
ough overview of this vast literature. Finally, we remark that our investigation of the semi-random
sparse recovery problem is heavily motivated by two recent works [CG18, LSTZ20] which studied
the robustness of fast iterative methods to semi-random modeling assumptions.

We also note that the well-studied Massart noise model in PAC learning [MN06] can be thought
of as a semi-random variant of the random classiﬁcation noise model. However, this setting appears
to be quite diﬀerent from ours: in particular, it was not until quite recently that polynomial-time
algorithms were even known to be achievable for a number of fundamental learning problems under
Massart noise [DGT19, DKTZ20, CKMY20, DK20, DKK+21, DKT21, DPT21, DIK+21, ZL21].

8

2 Preliminaries

N, 1

i

v

k

n

∈

∈

≤

≤

i
{

[d], we let

General notation. We let [n] :=
k·kp,
Rd
and the sparsity (number of nonzero entries) of a vector is denoted
∈
k2,(k) be the ℓ2 norm of the largest k entries of v in absolute value (with
and k
other elements zeroed out). The all-zeroes vector of dimension n is denoted 0n. The nonnegative
probability simplex in dimension n (i.e.

. The ℓp norm of a vector is denoted
}
k·k0. For a vector v

p
k
Rd and positive semideﬁnite covariance Σ

0) is denoted ∆n.
Rd
(µ, Σ) denotes the cor-
∼unif. S denotes a uniform random sample from set S. For
∆n we we use Multinom(N, p) to denote the probability distribution corresponding

∈
responding multivariate Gaussian. i
N
to N independent draws from [n] as speciﬁed by p.

k1 = 1, p

For mean µ

N and p

Rn
≥

d,

N

∈

∈

∈

∈

×

Sparsity. We say v is s-sparse if
NS(v) :=
v

2
2. Note that from the Cauchy-Schwarz inequality, if

s. We deﬁne the numerical sparsity of a vector by
s.

s, then NS(v)

k0 ≤

v
k

v

v
k

k0 ≤

≤

2
1 /
k

k

k

k

Matrices. Matrices are in boldface throughout. The zero and identity matrix of appropriate
dimension from context are 0 and I. For a matrix A
[n] and
d symmetric matrices is Sd, and its positive deﬁnite
its columns be A:j, j
[d]. The set of d
×
0 and Sd
and positive semideﬁnite restrictions are Sd
on Sd.
0. We use the Loewner partial order
(cid:22)
≻
(cid:23)
d is denoted
The largest entry of a matrix A
A
. When a matrix
[n],j
k
d is clear from context, we refer to its rows as
A

d, we let its rows be Ai:, i

Aij|

[d] |
∈

Rn

Rn

Rn

∈

∈

∈

∈

×

×

∈

kmax := maxi
[n].
ai}i
{
∈

×

∈

C2 and p

Rd has a (C2, C
∈
Rd with

Short-ﬂat decompositions. Throughout we frequently use the notion of “short-ﬂat decompo-
Rd with
sitions.” We say v
∈
0 to denote
p
e
k2 ≤
k
k
∈
the vector which coordinatewise [trunc(v, c)]i = sgn(vi) max(
c, 0) (i.e. the result of adding or
vi| −
|
subtracting at most c from each coordinate to decrease the coordinate’s magnitude). Note that
v
C2 (in which case
trunc(v, C
p = trunc(v, C

) short-ﬂat decomposition if v = p + e for some e

) short-ﬂat decomposition if and only if

. Further, we use trunc(v, c)

p is such a decomposition).

Rd has a (C2, C

) and e = v

Rd for c

)
k2 ≤

k∞ ≤

∞
C

R

∈

∈

∈

∞

∞

∞

k

≥

Restricted isometry property. We say that matrix A
isometry property (RIP) or (more concisely) A is (s, c)-RIP, if for all s-sparse vectors v

∈

×

d satisﬁes the (s, c)-restricted

Rn

∞

−

Rd,

∈

1
v
c k

2
2 ≤ k

k

Av

2
2 ≤

k

2
2 .

c

v

k

k

3 Exact recovery
In this section, we give an algorithm for solving the underconstrained linear system Ax⋆ = b given
Rn (i.e. noiseless or “exact”
the measurement matrix A
regression), and x⋆ is s-sparse. Our algorithm succeeds when A is weighted RIP (wRIP), i.e. it
satisﬁes Deﬁnition 2, a weighted generalization of Deﬁnition 1.

d) and responses b

d (for n

Rn

≤

∈

∈

×

Deﬁnition 2 (wRIP matrix). Let w⋆
[0, 1]. We say A
and there exists a weight vector w⋆
∆n satisfying
(Θ(s), Θ(1))-RIP for appropriate constants. When ρ =

∞ ∈
∈

Rn

×

d is (ρ, w⋆
∞

)-wRIP if
, such that diag (w⋆)

∈
w⋆
k
k∞ ≤
O(1) for brevity we say A is w⋆
∞

w⋆
∞

A

k

kmax ≤

ρ,
1
2 A is

-wRIP.

As discussed after Deﬁnition 1, a wRIP matrix can be thought of as arising from a “semi-random
model” because it strictly generalizes our previously-deﬁned pRIP matrix notion in Deﬁnition 1
with w⋆
m times the zero-one indicator vector of rows of G. The main
∞
result of this section is the following theorem regarding sparse recovery with wRIP matrices.

m , by setting w⋆ to be 1

= 1

e

9

Theorem 3. Let δ
probability at least 1
wRIP matrix A

∈
−
Rn
×

∈

x⋆
(0, 1), r > 0, and suppose R0 ≥ k
δ, Algorithm 1 using Algorithm 2 as a step oracle takes as input a (ρ, w⋆
∞
d and b = Ax⋆, and computes ˆx satisfying

k2 for s-sparse x⋆
x⋆

Rd. Then with
)-

r in time

∈

ˆx

k

−

k2 ≤

O

nd log3(ndρ) log

1
δ ·

log

R0
r

log

R0
r

w⋆
∞

·

sρ2 log d

.

(cid:18)(cid:18)

(cid:19)
Under the wRIP assumption, Theorem 3 provides a natural interpolation between the fully
random and semi-random generative models. To build intuition, if a pRIP matrix contains a
O(s) rows (the information-theoretically minimum size), then by setting
planted RIP matrix with
w⋆
O(nd). However, in the fully random regime where

, we obtain a near-linear runtime of

(cid:19)(cid:19)

(cid:18)

(cid:19)

(cid:18)

(cid:1)

(cid:0)

w⋆

∞ ≈

∞ ≈

1
e
O(s)
1
n (i.e. all of A is RIP), the runtime improves

e

e

The roadmap of our algorithm and its analysis are as follows.

O(sd) which is sublinear for n

s.

≫

1. In Section 3.1, we give an algorithm (Algorithm 1) which iteratively halves an upper bound
on the radius to x⋆, assuming that either an appropriate step oracle (see Deﬁnition 3) based
on short-ﬂat decompositions can be implemented for each iteration, or we can certify that the
input radius bound is now too loose. This algorithm is analyzed in Lemma 3.

e

2. We state in Assumption 1 a set of conditions on a matrix-vector pair (A, ∆) centered around
the notion of short-ﬂat decompositions, which suﬃce to provide a suﬃcient step oracle imple-
mentation with high probability in nearly-linear time. In Section 3.2 we analyze this imple-
mentation (Algorithm 2) in the proof of Lemma 1 assuming the inputs satisfy Assumption 1.

3. In Section 3.3, we show Assumption 1, with appropriate parameters, follows from A being
wRIP. This is a byproduct of a general equivalence we demonstrate between RIP, restricted
conditioning measures used in prior work [ANW10], and short-ﬂat decompositions.

3.1 Radius contraction using step oracles

In this section, we provide and analyze the main loop of our overall algorithm for proving Theorem 3.
This procedure, HalfRadiusSparse, takes as input an s-sparse vector xin and a radius bound R
xin −
k
subroutine, it requires access to a “step oracle”
certain assumptions on the matrix A.

≥
1
xout −
2 R. As a
k
Ostep, which we implement in Section 3.2 under

k2 and returns an s-sparse vector xout with the guarantee

k2 ≤

x⋆

x⋆

d, if the following holds. Whenever there is v

Rn

Deﬁnition 3 (Step oracle). We say that
A
such that ∆ = Av, with probability
conditions hold. First,

≥

∈

1

×

−

Ostep is a (Cprog, C2, δ)-step oracle for ∆
Rd with 1
v
v
4 ≤ k
k
Rn
δ,
≥

Rn and
2√2s
0 such that the following two

∈
Ostep returns w

∈
k1 ≤

k2 ≤

1 and

∈

Second, there exists a (C2, Cprog

6√s ) short-ﬂat decomposition of A⊤diag (w) ∆:

wi∆2

i ≥

Cprog.

[n]
Xi
∈

trunc

A⊤diag (w) ∆,

C2.

Cprog
6√s

2 ≤
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

Intuitively, (4) guarantees that we can write γ = p + e where p denotes a “progress” term which
we require to be suﬃciently short in the ℓ2 norm, and e denotes an “error” term which we require to
. We prove that under certain assumptions on the input A (stated in Assumption 1
be small in ℓ
below), we can always implement a step oracle with appropriate parameters.

∞

(3)

(4)

10

[n] w⋆
∈

i ∆iai:

i

P

(6)

Assumption 1. The matrix A
w⋆
satisfying
k
that for all v

w⋆
∞
Rd with 1

∈
, a constant L, ρ
1 and

v

×

Rn

k∞ ≤
∈

4 ≤ k

k2 ≤

1. A is entrywise bounded by

ρ, i.e.

±

≥
v
k
A

k

k1 ≤
kmax ≤

ρ.

2.

d satisﬁes the following. There is a weight vector w⋆

∆n
1, and a constant K (which may depend on L) such

∈

2√2s we have, deﬁning ∆ = Av:

1
L ≤

[n]
Xi
∈

w⋆

i ∆2

i ≤

L.

(5)

3. For W⋆ := diag (w⋆), there is a (L,

1

K√s ) short-ﬂat decomposition of A⊤W⋆∆ =

trunc

A⊤W⋆∆,

(cid:18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
K√s

L.

2 ≤
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

Our Assumption 1 may also be stated in a scale-invariant way (i.e. with (5), (6) scaling with
k2), but it is convenient in our analysis to impose a norm bound on v. Roughly, the second prop-
v
k
erty in Assumption 1 is (up to constant factors) equivalent to the “restricted strong convexity”
and “restricted smoothness” assumptions of [ANW10], which were previously shown for speciﬁc
measurement matrix constructions such as random Gaussian matrices. The use of the third prop-
erty in Assumption 1 (the existence of short-ﬂat decompositions for numerically sparse vectors)
Interestingly, we show in
in designing an eﬃcient algorithm is a key contribution of our work.
Section 3.3 that these assumptions are up to constant factors equivalent to RIP.

−

R (x

More speciﬁcally, we show that when A is wRIP, we can implement a step oracle for ∆ = Av
where v = 1
x⋆) for some iterate x of Algorithm 1, which either makes enough progress to
advance the algorithm or certiﬁes that v is suﬃciently short, by using numerical sparsity properties
In Lemma 1, we show that Assumption 1 suﬃces
of v. We break this proof into two parts.
to implement an appropriate step oracle; this is proven in Section 3.2.
In Lemma 2, we then
demonstrate the wRIP assumption with appropriate parameters implies our measurement matrix
satisﬁes Assumption 1, which we prove by way of a more general equivalence in Section 3.3.

Lemma 1. Suppose A satisﬁes Assumption 1. Algorithm 2 is a (Cprog, C2, δ) step oracle StepOracle
for (∆, A) with Cprog = Ω(1), C2 = O(1) running in time

O

nd log3(ndρ) log

(cid:18)(cid:18)

1
δ

·

(cid:19)

w⋆
∞

(cid:0)

sρ2 log d

.

(cid:19)

(cid:1)

Lemma 2. Suppose A
parameters in Deﬁnition 2. Then, A also satisﬁes Assumption 1.

d is (ρ, w⋆
∞

∈

×

Rn

)-wRIP with a suitable choice of constants in the RIP

We now give our main algorithm HalfRadiusSparse, assuming access to the step oracle

from Section 3.2 with appropriate parameters, and that A obeys Assumption 1.

Ostep

Lemma 3. Assume A satisﬁes Assumption 1. With probability at least 1
(i.e.

xout

x⋆

1
2 R).

k

−

k2 ≤

T δ, Algorithm 1 succeeds

−

Proof. Throughout this proof, condition on the event that all step oracles succeed (which provides
the failure probability via a union bound). We ﬁrst observe that x⋆
because of Cauchy-Schwarz,
R.
the 2s-sparsity of xin −

x⋆, and the assumption

xin −

k2 ≤

∈ X

Next, we show that in every iteration t of Algorithm 1,

x⋆

k

xt+1 −
k

x⋆

2
2 ≤

k

1

−

(cid:18)

C 2
2
2C 2

prog (cid:19)

xt −
k

x⋆

2
2 .

k

11

(7)

Algorithm 1: HalfRadiusSparse(xin, R,
1 Input: s-sparse xin ∈
xin −
Rn, δ
∈
2 Output: s-sparse vector xout that satisﬁes

Rd, R
Ostep for all (∆, A) with ∆

≥ k
∈

Ostep, δ, A, b)
x⋆
k2 for s-sparse x⋆
Rn
(0, 1), A
×
∈
x⋆
xout −
k
√2sR
T

k2 ≤

}

←

xink1 ≤

−

Rd

i

6

x

x

∈

≤

−

| k

xin,
T

3 Set x0 ←
4 for 0
t
≤
5

X ← {
1 do
wt ← Ostep(∆t, A) for ∆t ←
[n][wt]i[∆t]2
if
i < Cprog or
∈
Return: xout ←
P
end
else xt+1 ←
argminx
10 end
11 Return: xout ←

∈X k

x

8

9

7

−

> C2 then
xt truncated to its s largest coordinates

2

1

R (Axt −

b), γt ←
trunc(γt, Cprog
6√s )
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
xt −

ηRγtk2 ;

xt truncated to its s largest coordinates

Rd, (Cprog, C2, δ)-step oracle

∈
Rn ;
d, b = Ax⋆
1
2 R with probability

∈

6C2
2
C2

prog

, η

←

Cprog
2C2
2

;

l

m

1

−

≥

T δ ;

A⊤diag (wt) ∆t =

i

[n][wt]i[∆t]iai ;
∈

P

As x⋆

∈ X

, the optimality conditions of xt+1 as minimizing

x

ηRγt)
k

2
2 over

X

imply

=

⇒ k

xt −

x⋆

k

2

h
2
2 − k

xt+1 −
xt+1 −

xt + ηRγt, xt+1 −
x⋆
2ηR

2
2 ≥

k

xt −

xt+1k

2
2 .

(8)

k

−

k
x⋆
i ≤
γt, xt+1 −
h

(xt −
0
x⋆

+

i

Hence, it suﬃces to lower bound the right-hand side of the above expression. Let γt = pt + et
denote the (C2, Cprog
6√s ) short-ﬂat decomposition of γt which exists by Deﬁnition 3 assuming the step
oracle succeeded. We begin by observing

2ηR

γt, xt+1 −
h

xti

+

xt −

k

xt+1k

2
2 = 2ηR

et, xt+1 −
h
2ηR
etk∞ k
k
ηR2Cprog −

+ 2ηR

pt, xt+1 −
xti
h
η2R2
xtk1 −
xt+1 −
ptk
k
η2R2C 2
2 .

xti

2
2

≥ −

≥ −

+

xt −
k

xt+1k

2
2

(9)

The ﬁrst inequality followed from H¨older on the ﬁrst term and Cauchy-Schwarz on the latter two
, and the bounds on et
terms in the preceding line. The second followed from the ℓ1 radius of
and pt from (4). Next, from Deﬁnition 3, for ∆ = ∆t = 1
x⋆),

b) and v = 1

X

R (Axt −

R (xt −
2ηR2Cprog.

(10)

2ηR

γt, xt −
h

x⋆

i

= 2ηR

wi∆i h

ai, v

i

= 2ηR2

wi∆2

i ≥

[n]
Xi
∈

[n]
Xi
∈

Finally, (7) follows from combining (8), (9), and (10), with our choice of η, and the fact that
inducting on this lemma implies the ℓ2 distance to x⋆ of the iterates is monotone decreasing.

1

k

k2 ≤

xt −

4 R. Note that the vector v = 1

Next, we claim that regardless of whether Algorithm 1 terminates on Line 7 or Line 11, we
b). By
), and upon iterating (7) on our
1 is met. Hence, if the algorithm
1
4 R, as otherwise the termination

x⋆
have
assumption the condition
v
radius bound assumption, this implies that the condition
terminated on Line 7, we must have
xt −
k2 ≤
condition would have been false. On the other hand, by (7), after T steps we have

2√2s is met (since xt, x⋆
v
k
x⋆

x⋆) satisﬁes Av = ∆ := 1

∈ X
k2 ≤
k2 ≤

R (Axt −

R (xt −

1
4 R =

k1 ≤

⇒ k

v
k

k

xT −

k

x⋆

2
2 ≤

k

exp

T C 2
2
2C 2

prog (cid:19)

−

(cid:18)

x0 −
k

x⋆

2
2 ≤

k

1
16

R2.

12

We conclude that at termination,
xout = argmin

xt −
k2 ≤
k
xtk2 imply the desired

x
s k

k0≤

−

x

k

x⋆

1
4 R. Now, s-sparsity of x⋆ and the deﬁnition of

xout −

k

x⋆

k2 ≤ k

xout −

xtk2 +

x⋆
k

xtk2 ≤

2

k

−

x⋆

xtk2 ≤

−

1
2

R.

(11)

3.2 Designing a step oracle

In this section, we design a step oracle
Rn
input matrix A
convenient to deﬁne

∈

×

d. Our step oracle iteratively builds a weight vector ¯w

Ostep(∆, A) (see Deﬁnition 3) under Assumption 1 on the
0. It will be

Rn
≥

∈

γ ¯w :=

¯wi∆iai.

(12)

[n]
Xi
∈
Note that a valid step oracle always exists (although it is unclear how to implement the follow-
ing solution): namely, setting ¯w = w⋆ satisﬁes the oracle assumptions by the second and third
conditions in Assumption 1. In order to ensure Algorithm 5 is indeed a step oracle, we track two
potentials for some µ, C we will deﬁne in Algorithm 2:

Φ2( ¯w) :=

¯wi∆2

i and Φsqmax( ¯w) :=

[n]
Xi
∈

p

k

(cid:18)

min
L
k

k2≤

¯w

sqmaxµ (γ ¯w −

p)

(cid:19)

k1

+ k

¯w
k1
4CLs

,

(13)

where sqmaxµ(x) := µ2 log

exp

x2
j
µ2

.

!






[d]
Xj
∈

Intuitively, Φ2( ¯w) corresponds to progress on (3), and Φsqmax( ¯w) is intended to track the bounds
(4). We note the following fact about the sqmax function which follows from direct calculation.

Fact 1. For all x

Rd,

x

2
k
∞ ≤

k

∈

sqmaxµ(x), and sqmaxµ(x)

µ2 log(d).

≥

Also it will be important to note that Φsqmax( ¯w) can be computed to high precision eﬃciently.
We state this claim in the following and defer a full proof to Appendix B; we give a subroutine
which performs a binary search on a Lagrange multiplier on the ℓ2 constraint on p, and then solves
for each optimal pj using another binary search based on the Lagrange multiplier value.

Lemma 4. Let δ > 0 and θ

0. For any vector γ

≥

∈

Rd, we can solve the optimization problem

to additive accuracy δ in time

min
p
k2≤

k

θ

sqmaxµ(γ

p)

−

O

d log2

2
γ
2
k
k
µ√δ !!

.

We state the full implementation of our step oracle as Algorithm 2 below.
Our main helper lemma bounds the expected increase in Φsqmax from choosing a row of A
uniformly at random, and choosing a step size according to w⋆. We do not know w⋆, but we argue
that our algorithm makes at least this much expected progress. Deﬁne the decomposition promised
by (6):

p⋆ := trunc

A⊤W⋆∆,

(cid:18)

1
K√s

(cid:19)

13

, e⋆ := A⊤W⋆∆

p⋆.

−

 
 
 
(0, 1) ;
1 and

v

∈
k2 ≤

v
k

k1 ≤

2√2s such that

4 ≤ k

1

∈
δ, (3), (4) are satisﬁed with Cp = 1, C2 = O(1).
−
1
∞sρ2 log d , N ′ ← ⌈
Kw⋆

log2

1
δ ⌉

;

≥

←

Algorithm 2: StepOracle(∆, A, δ)
Rn, A
1 Input: ∆
2 Output: w such that if there is v

Rn

∈

∈

×

d satisfying Assumption 1, δ

Rd with 1

∆ = Av, with probability
1
√Cs log d , η

200, µ

←
N ′ do

3 C
←
4 for 0
5

k
≤
w0 ←
for 0

≤

≤

5Ln
;
η ⌉

← ⌈
N do

≤
0n, N
t
if Φ2(wt)
≥
∼unif. [n] ;
Sample i
Compute (using Lemma 4) dt ∈

1 then Return: w

6

7

8

9

wt ;

←

[0, ηw⋆
∞

] maximizing to additive O( η
n )

Γt(d) := Φ2(wt + dei)

CsΦsqmax(wt + dei)

−

(14)

wt+1 ←

wt + dtei ;

10

end

11 end
12 Return: w

0n ;

←

Furthermore, deﬁne for all i

[n],

∈

where p⋆ is given by (6). We use

z(i)
{

}i

[n] as certiﬁcates of Φsqmax’s growth in the following.
∈

z(i) := ηw⋆

i (∆iai −

p⋆),

(15)

Lemma 5. Assume that the constant K in Assumption 1 is suﬃciently large, and that ∆ =
Av where v satisﬁes the norm conditions in Assumption 1. Then for any ¯w
0 such that
1
Φsqmax( ¯w)

C 2µ2 log d, and η

Rn
≥

∈

∞sρ2 log d , we have

Kw⋆

≤

≤

Ei

∼unif.[n] [Φsqmax( ¯w + ηw⋆
i )]

≤

Φsqmax( ¯w) +

1
2CLs ·

η
n

.

max(2√2, L) and (5)
Proof. We assume for simplicity L
remains true. Let p ¯w be the minimizing argument in the deﬁnition of Φsqmax( ¯w) in (13). For any
i )p⋆ is a valid argument for the optimization problem deﬁning
i
Φsqmax( ¯w + ηw⋆

[n], it is clear that p ¯w + (ηw⋆

2√2 as otherwise we may set L

L, and since

i . Next, deﬁne

i ) by

←

≥

w

∈

k1 grows by ηw⋆

k

p⋆
k

k2 ≤

F (x) :=

exp

[d]
Xj
∈

x2
j
µ2

!

(16)

¯w
such that Φsqmax( ¯w) = µ2 log F (x)+ k
p ¯wk2 + ηw⋆
i L, we conclude
k

k1

4CLs for x = γ ¯w−

p ¯w. As discussed earlier, since

p ¯w + (ηw⋆
k

i )p⋆

k2 ≤

Φsqmax( ¯w + ηw⋆
i )

≤

µ2 log F (x + z(i)) + k

¯w + ηw⋆
4CLs

i k1

.

(17)

14

 
We next compute

F (x + z(i)) =

1
n

[n]
Xi
∈

≤

1
n

1
n

exp

x2
j
µ2

[d]
Xj
∈

F (x) max
j

[d] 

∈

[n]
Xi
∈



[n]
Xi
∈

! 

exp

exp

2xjz(i)

j )2

j + (z(i)
µ2

2xjz(i)

j )2

j + (z(i)
µ2

!


∈

∈

!


.

(18)

We now bound the right-hand side of this expression. For any i

[n] and j

[d], recalling (15),

ηw⋆
∆i| k
i (
|

aik∞

+

p⋆

k2)

k

ηw⋆
∞

≤

≤

L(√sρ2 + 1).

(19)

The second inequality used our bounds from Assumption 1; note that for ∆ = Av where v satisﬁes
2√2sρ. Hence, if we choose a suﬃciently
the norm conditions in Assumption 1,
v
k
large constant K in Assumption 1, we have

∆i| ≤
|

k1 ≤

ρ

Also by the assumption that Φsqmax( ¯w)

√C
K√s log dρ2 ·

≤

L(√sρ2 + 1)

1
4C√log d

.

≤

(cid:0)

(cid:1)
C 2µ2 log d we must have that for all j

[d],

∈

z(i)
j
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1
µ

z(i)
j
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

≤
xj|
|
µ ≤

C

log d.

p

Now, using exp(c)

1 + c + c2 for

c
| ≤
|

1, we get

≤
j + (z(i)
2xjz(i)
µ2

j )2

exp

[n]
Xi
∈

1 +

2xjz(i)
j

µ2 +

(z(i)
j )2
µ2 +

2xjz(i)

j )2

j + (z(i)
µ2

2xjz(i)
j
µ2 + 10C 2 log d

·

(z(i)
j )2
µ2

.

!

! ≤

[n]
Xi
∈





1 +

≤

[n]  
Xi
∈

2

!





(20)

We control the ﬁrst-order term via the observation that
(6), so taking the constant K in Assumption 1 suﬃciently large, we have

[n] z(i) = ηe⋆ which is ℓ
∈

i

P

(cid:12)
(cid:12)
Xi
[n]
(cid:12)
∈
(cid:12)
2xjz(i)
(cid:12)
j
(cid:12)
µ2

≤

e⋆

η
µ k

z(i)
j
µ (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
≥
L, and (5):

2C

p

≤

=

⇒ (cid:12)
(cid:12)
[n]
Xi
(cid:12)
∈
(cid:12)
(cid:12)
(cid:12)

η√C log d
K

k∞ ≤

log d

η√C log d
K

·

≤

η log d
8L

.

In the last inequality we assumed K
(a + b)2

2a2 + 2b2,

p⋆

p⋆

k

k∞ ≤ k

k2 ≤

≤

16C 1.5L. We control the second-order term by using

2

z(i)
j

[n] (cid:16)
Xi
∈

(cid:17)

≤

2η2w⋆

∞ 

[n]
Xi
∈



w⋆

i [p⋆]2

j +

[n]
Xi
∈

15

w⋆

i ∆2

i ρ2

2η2w⋆
∞



≤

(Lρ2 + L2).

(22)



-bounded from

∞

(21)

 
 
 
 
 


≤

≤

Putting together (20), (21), and (22), with the deﬁnition of µ, we conclude for suﬃciently large K,

exp

1
n

[n]
Xi
∈

2xjz(i)

j )2

j + (z(i)
µ2

1 +

η log d
8Ln

2η2w⋆
∞

+

(Lρ2 + L2)
nµ2

! ≤

1 +

≤

η log d
4Ln

.

Hence, combining the above with (18), and using log(1 + c)

c for all c,

≤

µ2 log

1
n



F (x + z(i))



≤

µ2 log F (x) +

µ2η log d
4Ln

= µ2 log F (x) +

1
Cs ·

η
4Ln

.

(23)

[n]
Xi
∈



Finally, we compute via (17) and concavity of log,

Ei

∼unif.[n][Φsqmax( ¯w + ηw⋆
i )]

≤

µ2
n

[n]
Xi
∈

log F (x + z(i)) + k

¯w
k1
4CLs

+

1

4CLs 

1
n

ηw⋆

i 



[n]
Xi
∈
η
n


1
4CLs ·

µ2 log

1
n



F (x + z(i))



+ k

¯w
k1
4CLs

+

[n]
Xi
∈


µ2 log F (x) + k

¯w
k1
4CLs

+



1
Cs ·

η
2Ln

= B( ¯w) +

1
Cs ·

η
2Ln

.

In the last line, we used the bound (23).

Finally, we can complete the analysis of Algorithm 2.

Lemma 1. Suppose A satisﬁes Assumption 1. Algorithm 2 is a (Cprog, C2, δ) step oracle StepOracle
for (∆, A) with Cprog = Ω(1), C2 = O(1) running in time

O

nd log3(ndρ) log

(cid:18)(cid:18)

1
δ

·

(cid:19)

w⋆
∞

(cid:0)

sρ2 log d

.

(cid:19)

(cid:1)

Proof. It suﬃces to prove Algorithm 2 meets its output guarantees in this time. Throughout this
proof, we consider one run of Lines 5-10 of the algorithm, and prove that it successfully terminates
1
2 assuming A satisﬁes Assumption 1 and that ∆ = Av for v satisfying
on Line 7 with probability
the norm bounds in Assumption 1. This yields the failure probability upon repeating N ′ times.

≥

For the ﬁrst part of this proof, we assume we can exactly compute ∆t, and carry out the proof
accordingly. We discuss issues of approximation tolerance at the end, when bounding the runtime.
Correctness. We use the notation At := Φ2(wt), Bt := Φsqmax(wt), and Φt := At −
CsBt. We
ﬁrst observe that At is 1-Lipschitz, meaning it can only increase by 1 in any given iteration; this
follows from ηw⋆
i =
∞
Suppose some run of Lines 5-13 terminates by returning on Line 8 in iteration T , for 0

N .
1 = Cprog, so to show that the algorithm satisﬁes
The termination condition implies that AT ≥
Deﬁnition 3, it suﬃces to show existence of a short-ﬂat decomposition in the sense of (4). Clearly, Φt
is monotone non-decreasing in t, since we may always force Γt = 0 by choosing dt = 0. Moreover,
2, since
Φ0 =
AT

Csµ2 log d =
CsB0 =
1 by the termination condition; hence,

1. The above Lipschitz bound implies that AT ≤

1, since ∆2

-ℓ1 H¨older.

8sρ2 by ℓ

8sρ2 ∆2

ai, v
h

i ≤

i ≤

∆2

≤

≤

−

−

≤

T

∞

i

1

2

−
1 ≤

−

AT −

CsBT = ΦT ≥

Φ0 =

1 =

⇒

−

BT ≤

AT + 1

Cs ≤

3
Cs ≤

C 2µ2 log d.

16

 
i , and hence
η
Ln −

≥

Note that the above inequality and nonnegativity of sqmaxµ imply that k
wT k1 ≤
k
12L. For the given value of C = 200, and the ﬁrst inequality in Fact 1, the deﬁnition of the ﬁrst
wT k1 = O(1).
summand in B implies there is a short-ﬂat decomposition meeting (4) with C2 = L
k
Hence, we have shown that Deﬁnition 3 is satisﬁed whenever the algorithm returns on Line 7.
0, the algorithm will terminate. This follows

We make one additional observation: whenever Φt ≥
since on such an iteration,

wT
k1
4LCs ≤

3
Cs , so

At ≥

CsBt ≥

CsB0 = Csµ2 log d = 1,

since clearly the function B is minimized by the all-zeroes weight vector, attaining value µ2 log d.
Success probability. We next show that with probability at least 1
terminate. Fix an iteration t. When sampling i
at least that attained by setting dt = ηw⋆

[n], the maximum gain in Φt for dt ∈

2 , the loop in Lines 5-10 will
] is

[0, ηw⋆
∞

∈

η
2Ln

η
2Ln

.

(24)

i over a uniformly sampled i

[n]

E[Φt+1 −
Here, we used that the expected gain in At by choosing dt = ηw⋆
is lower bounded by η

At ≤

Φt |

=

1]

Let Zt be the random variable equal to Φt −

∈
Ln via (5), and the expected gain in CsBt is upper bounded by Lemma 5.
Φ0, where we freeze the value of wt′ for all t′ ≥
2 always: whenever Zt ≥

t if
1,
0 so the algorithm will terminate, and Zt is 1-Lipschitz because At is. Moreover,

the algorithm ever returns on Line 8 in an iteration t. Notice that Zt ≤
we have Φt ≥
whenever we are in an iteration t where Pr[At ≥
E[Zt+1 −
Zt |

≤
At ≤
1] is a monotone non-decreasing function of t, since At is monotone. After N

Clearly, Pr[At ≥
iterations, if we still have Pr[At ≥
1]
display yields E[ZN ] > 2. This yields the desired success probability.
Runtime. The cost of each iteration is dominated by the following computation in Line 9: we
wish to ﬁnd d

5Ln
η
1
2 , we would obtain a contradiction since recursing the above

] maximizing to additive O( η

1
2 , applying (24) implies

Zt] = E[Zt+1 −

1] Pr[At ≤

η
4Ln

n ) the following objective:

≥

≤

≥

1]

1]

.

[0, ηw⋆
∞

∈

Φ2(w + dei)

−

CsΦsqmax(w + dei).

We claim the above function is a concave function of d. First, we show Φsqmax is convex (and
the result will then follow from linearity of Φ2). To see this, for two values wi and w′i, let the
corresponding maximizing arguments in the deﬁnition of Φsqmax( ¯w + wi) and Φsqmax( ¯w + w′i) be
denoted p and p′. Then, 1
2 (wi + w′i), and by convexity of
sqmaxµ and linearity of the ℓ1 portion, we have the conclusion.

2 (p + p′) is a valid argument for ¯w + 1

∆i|
|

Next, note that all

are bounded by 2√2sρ (proven after (19)) and all aij are bounded by ρ
by assumption. It follows that the restriction of Φ2 to a coordinate is 8sρ2-Lipschitz. Moreover the
linear portion of Φsqmax is clearly
4CLs -Lipschitz in any coordinate. Finally we bound the Lipschitz
constant of the sqmax part of Φsqmax. It suﬃces to bound Lipschitzness for any ﬁxed p of
sqmaxµ (γ ¯w −

p + di∆iai)

1

p+d∆iai) and sqmax(γ ¯w −
because performing the minimization over p involved in two sqmax(γ ¯w −
p + d′∆iai) can only bring the function values closer together. By direct computation the derivative
of the above quantity with respect to di is

∆iaij

2 [γ ¯w −
(cid:16)

[d]
Xj
∈

p + di∆iai]j

qj

(cid:17)

17

for some probability density vector q

∈

∆d. Further we have

√sρ2

(ηw⋆
∞

·

).

+ O(1) + 2√2sρ2

(cid:0)

(cid:1)

O

k
∆i|

γ ¯w −
|

∈
Hence, we may evaluate to the desired O( η

p + di∆iai|j ≤
Here we used our earlier proof that we must only consider values of
algorithm (since
than (maxi
[d] |
[n] |
∈
∈
linear portions this shows Φ2 and Φsqmax are poly(ndρ)-Lipschitz.

k2 = O(1) throughout the
k
wtk1 = O(1) throughout) and this also implies no coordinate of γ ¯w can be larger
k1 by deﬁnition of γ ¯w. Combined with our bounds on
)(maxi
n ) accuracy by approximate minimization of a Lips-
chitz convex function over an interval (Lemma 33, [CLM+16]) with a total cost of O(d log3(ndρ)).
Here we use the subroutine of Lemma 4 in Lemma 33 of [CLM+16], with evaluation time O(d log2(ndρ)).
The algorithm then runs in N N ′ iterations, each bottlenecked by the cost of approximating
Γt; combining these multiplicative factors yields the runtime. We note that we do not precompute
∆ = Av; we can compute coordinates of ∆ in time O(d) as they are required by Algorithm 2.

)
aij|

[n],j

¯w

p

k

3.3 Equivalence between Assumption 1 and RIP

The main result of this section is an equivalence between Assumption 1 and the weighted restricted
isometry property, which requires two helper tools to prove. The ﬁrst is a “shelling decomposition.”
[k] v(l) where v(1) is obtained by
Lemma 6. Let v
σ. Then if we write v =
∈
taking the s largest coordinates of v, v(2) is obtained by taking the next s largest coordinates and so
on (breaking ties arbitrarily so that the supports are disjoint), we have

Rd have NS(v)

P

≤

∈

l

v(l)

σ
v
s k

k2 .

2 ≤

r

l
X2
≤
≤

k (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Proof. Note that the decomposition greedily sets v(l) to be the s largest coordinates (by absolute
1] v(l′), zeroing all other coordinates and breaking ties arbitrarily. This satisﬁes
value) of v
1
√s

v(l+1)

√s

P

−

2 ≤

≤

−

l′

∈

[l

1

.

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

v(l+1)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

∞

v(l)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

The last inequality follows since every entry of v(l) is larger than the largest of v(l+1) in absolute
value. Finally, summing the above equation and using disjointness of supports yields

1
v
√s k

2 ≤

σ
v
s k

k2 .

k1 ≤

r

v(l)

l
X2
≤
≤

k (cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

The second bounds the largest entries of image vectors from the transpose of an RIP matrix.

Lemma 7. Let A

∈

Rn

×

d be (s, c)-RIP, and let u

Rn. Then,

∈

A⊤u

(cid:13)
(cid:13)
Proof. Let v
(cid:13)
of A⊤u. The lemma is equivalent to showing

k2 .
u
Rd be the s-sparse vector obtained by zeroing out all but the s largest coordinates
k2. Note that
u
k2 .
u
v
k2 k

u
k2 ≤
k2 k

k2 ≤
Av

v, A⊤u

2,(s) ≤

2
2 =

v
k

v
k

≤ k

(cid:13)
(cid:13)
(cid:13)

∈

k

k

k

k

c

c

c

The ﬁrst inequality used Cauchy-Schwarz, and the second applied the RIP property of A to v,
which is s-sparse by construction. The conclusion follows via dividing by

D

E

v
k

k2.

18

Using these helper tools, we now prove the main result of this section.

Lemma 8. The following statements are true.

1. If A satisﬁes Assumption 1 with weight vector w⋆, then (W⋆)
2. If the matrix (W⋆)

1
2 A is RIP with parameters

1
2 A is (s, L)-RIP.

12800L3K 2s,

√L
2 !

for L

A
k
Proof. We prove each equivalence in turn.

kmax ≤

1, and

≥

ρ, then A satisﬁes Assumption 1.

Assumption 1 implies RIP. The statement of RIP is scale-invariant, so we will prove it for
all s-sparse unit vectors v without loss of generality. Note that such v satisﬁes the condition in
√s by Cauchy-Schwarz. Then, the second condition of
Assumption 1, since
k1 ≤
Assumption 1 implies that for ∆ = Av, we have the desired norm preservation:

k2 = 1 and

v
k

k

v

1
L ≤

(W∗)

1
2 Av

2

2

=

[n]
Xi
∈

w⋆

i ∆2

i ≤

L.

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

Boundedness and RIP imply Assumption 1. Let v
k1 ≤
2√2s, and deﬁne ∆ := Av. The ﬁrst condition in Assumption 1 is immediate from our assumed
entrywise boundedness on A, so we begin by demonstrating the lower bound in (5). Let

v
4 ≤ k

k2 ≤

1 and

∈

k

v

Rd satisfy 1

and let v(1), . . . , v(k) be the shelling decomposition into s′-sparse vectors given by Lemma 6, where
σ = 128s from the ℓ1 and ℓ2 norm bounds on v. By Lemma 6, we have

s′ = 12800L3K 2s

In particular, the triangle inequality then implies 0.9

v

k
. By applying the triangle inequality and since (W⋆)

k2 ≤

v

k2. Next, recall that
1
2 A is RIP,

[n] w⋆
∈

i ∆2

i =

i

P

(W⋆)
(cid:13)
(cid:13)
(cid:13)

(W⋆)

1
2 Av

(cid:13)
(cid:13)
(cid:13)
1
2 Av

2

2

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

v(2)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

+

2

· · ·

+

v(k)

0.1
v
L k

2 ≤

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

k2 .
v(1)

(cid:13)
(cid:13)

2 ≤ k
(cid:13)
(cid:13)

2

2

2 ≥  

≥  

k

(W⋆)

1

2 Av(l)

(W⋆)

1

2 Av(1)

2 −

(cid:13)
(cid:13)
(cid:13)
5
√L ·

0.9

v

k

(cid:13)
(cid:13)
(cid:13)
k2 −

Xl=2 (cid:13)
(cid:13)
(cid:13)
1
v
L k

√L
2 ·

2

k2

!

≥

2!

(cid:13)
(cid:13)
(cid:13)
16
v
L k

2
2 ≥

k

1
L

.

In the second inequality, we applied the RIP assumption to each individual term, since all the
vectors are s′-sparse. Similarly, to show the upper bound in (5), we have

1
2 Av

(W⋆)
(cid:13)
(cid:13)
(cid:13)

2
2 ≤  
(cid:13)
(cid:13)
(cid:13)

≤  

(W⋆)
(cid:13)
(cid:13)
(cid:13)
√L
2 · k

1

2 Av(1)

k

+

(W⋆)

1

2 Av(l)

2

(cid:13)
(cid:13)
(cid:13)
√L
2 ·

Xl=2 (cid:13)
(cid:13)
(cid:13)
1
v
k2
L k

2

L.

≤

!

v

k2 +

19

2

2!
(cid:13)
(cid:13)
(cid:13)

 
It remains to verify the ﬁnal condition of Assumption 1. First, for u := W

1
2 Av, by applying

the shelling decomposition to v into s′-sparse vectors

v(l)
{
2 Av(l)

1

}l

[k],
∈
√L

2 ≤

u
k2 ≤
k

(W⋆)

[k] (cid:13)
Xl
∈
(cid:13)
(cid:13)

v

k2 .

(25)

k

(cid:13)
(cid:13)
(cid:13)

≤

Here, we used our earlier proof to bound the contribution of all terms but v(1). Applying Lemma 7
to the matrix (W⋆)

1
2 A and vector u, we have for ∆ = Av,

1
2 u

2,(s′)

=

A⊤W⋆∆

L.

2,(s′) ≤

A⊤(W⋆)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
By setting the ℓ2 bounded component in the short-ﬂat decomposition of A⊤W⋆∆ to be the top
(cid:13)
1
s′ entries by magnitude, it remains to show the remaining coordinates are ℓ
K√s .
This follows from the deﬁnition of s′ and (25), which imply that the s′ + 1th largest coordinate (in
magnitude) cannot have squared value larger than L2
s′

1
K 2s without contradicting (25).

bounded by

(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)

∞

Finally, it is immediate that Lemma 2 follows from Lemma 8.

3.4 Putting it all together

At this point, we have assembled the tools to prove our main result on exact recovery.

Theorem 3. Let δ
probability at least 1
wRIP matrix A

∈
−
Rn
×

∈

x⋆
(0, 1), r > 0, and suppose R0 ≥ k
δ, Algorithm 1 using Algorithm 2 as a step oracle takes as input a (ρ, w⋆
∞
d and b = Ax⋆, and computes ˆx satisfying

k2 for s-sparse x⋆
x⋆

Rd. Then with
)-

r in time

∈

ˆx

k

−

k2 ≤

O

nd log3(ndρ) log

(cid:18)(cid:18)

1
δ ·

log

R0
r

(cid:18)

log

R0
r

(cid:19)

(cid:18)

(cid:19)(cid:19)

w⋆
∞

·

sρ2 log d

.

(cid:19)

Rd where 1

(cid:0)
δ, combining Lemma 1 and Lemma 2 implies that Assumption 1
Proof. With probability at least 1
holds for all v
r ), we can
v
implement a step oracle for N runs of Algorithm 1 in the allotted time, each with failure probability
δ
N . Moreover, Algorithm 1 returns in O(1) iterations, and allows us to halve our radius upper
1
bound. By taking a union bound on failure probabilities and repeatedly running Algorithm 1 N
times, we obtain a radius upper bound of r with probability

2√2s, and that for N = O(log R0

−
v
k2 ≤
4 ≤ k

k1 ≤

1 and

−

δ.

∈

1

k

(cid:1)

≥

−

4 Noisy recovery

In this section, we give an algorithm for solving a noisy sparse recovery problem in a wRIP matrix
A

d (where we recall Deﬁnition 2). In particular, we assume that we receive

Rn

×

∈

b = Ax⋆ + ξ⋆,

(26)

for an arbitrary unknown ξ⋆

Rn, and x⋆

Rd is s-sparse. Throughout this section, we will deﬁne

∈

∈

1
w⋆
∞
is an entrywise bound on w in Deﬁnition 2. We deﬁne the (unknown) “noise ﬂoor”

m :=

,

(27)

where w⋆
∞

1
√m k
k·k2,(m) in Section 2. Our goal will be to return x such that

k2,(m) ,

Rξ :=

where we deﬁned
We now formally state the main result of this section here.

ξ⋆

x⋆

x
k

−

k2 = O(Rξ).

20

Theorem 4. Let δ
d is (ρ, w⋆
A
×
∞
at least 1

Rn

∈

x⋆
(0, 1), and suppose R0 ≥ k
∈
)-wRIP and b = Ax⋆ + ξ⋆, and R1 ≥

k2 for s-sparse x⋆

Rξ := 1

ξ⋆

Rd. Further, suppose
k2,(m). Then with probability

∈

√m k

δ, Algorithm 3 using Algorithm 3 as a noisy step oracle computes ˆx satisfying

−

x⋆

ˆx

k

−

k2 ≤

Rﬁnal = Θ(Rξ),

in time

O

ndw⋆
∞

s log4(ndρ) log2

d
δ ·

log

R0
Rﬁnal (cid:19)

log

R1
Rﬁnal (cid:19)(cid:19)(cid:19)

·

ρ2 log

R0
Rﬁnal (cid:19)

log

R1
Rﬁnal (cid:19)(cid:19)

.

(cid:18)

(cid:18)

(cid:18)(cid:18)

(cid:18)
Similarly to Theorem 3, Theorem 4 provides a runtime guarantee which interpolates between
the fully random and semi-random settings, and runs in sublinear time when e.g. the entire mea-
surement matrix A satisﬁes RIP. Theorem 4 further provides a reﬁned error guarantee as a function
of the noise vector ξ, which again interpolates based on the “quality” of the weights w. This is
captured through the parameter m = 1
ξ scales as the
w⋆
∞
average squared entry of ξ, and more generally it scales as the average of the largest m entries.

n, the squared error bound R2

: when m

≈

(cid:18)

(cid:18)

We solve the noisy variant by essentially following the same steps as Section 3 and making minor
modiﬁcations to the analysis; we give an outline of the section here. In Section 4.1, we generalize
the framework of Section 3.1 to the setting where we only receive noisy observations (26), while
our current radius is substantially above the noise ﬂoor. We then implement an appropriate step
oracle for this outer loop in Section 4.2, and prove that the relevant Assumption 2 used in our step
oracle implementation holds when A is wRIP in Section 4.3.

4.1 Radius contraction above the noise ﬂoor using step oracles

In this section, we give the main loop of our overall noise-tolerant algorithm, HalfRadiusSparseNoisy,
which takes as input s-sparse xin and a radius bound R
k2. It then returns an s-sparse
1
2 R, as long as R is larger than an appropriate
vector xout with the guarantee
multiple of Rξ. We give the analog of Deﬁnition 3 in this setting, termed a “noisy step oracle.”

xout −
k

xin −

k2 ≤

≥ k

x⋆

x⋆

Deﬁnition 4 (Noisy step oracle). We say that
∆

Rn and A
×
∆ = Av + ξ where

∈
that
e
the following two conditions hold. First,

k2,(m) ≤

√m
Cξ

ξ
k

Rn

∈

, with probability

Onstep is a (Cprog, C2, Cξ, δ)-noisy step oracle for
Rd with 1
1 such
k2 ≤
0 such that
Onstep returns w

12 ≤ k
∈

v
Rn
≥

δ,

≥

−

∈

1

d if the following holds. Whenever there is v

e

Second, there exists a (C2, Cprog

6√s ) short-ﬂat decomposition of A⊤diag (w) ∆:

Cprog.

wi

∆i∆i ≥
e

[n]
Xi
∈

(28)

trunc

A⊤diag (w) ∆,

(cid:18)

Cprog
6√s

C2.

2 ≤

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)

We next characterize how a strengthened step oracle with appropriate parameters also is a noisy

step oracle. First, we will need a deﬁnition.

Deﬁnition 5. For distributions A, B on Rn, we say A stochastically dominates B if there is a
random variable C on Rn whose coordinates are always nonnegative such that the distribution of A
is the same as the distribution of B + C (where C may depend on the realization of B).

We now formalize the properties of the strengthened step oracle that we will construct.

21

Rn and A

Deﬁnition 6 (Strong step oracle). We say that
∆
×
well as the following additional guarantees.
e

1. For the output weights w, we have

Rn

∈

∈

Ostep is a (Cprog, C2, Cξ, δ)-strong step oracle for
d if it satisﬁes all the properties of a standard step oracle (Deﬁnition 3), as

2. The distribution of w output by the oracle is stochastically dominated by the distribution

w
k

k1 ≤

CprogC 2
ξ
4

δ.

·

(29)

δ
4sρ2 log d
δ

Multinom 
&

CprogC 2

ξ nsρ2 log d
δ
m

, 

1
n

'

1.







|

{z

, . . . ,

n

1
n










}

for some ρ

≥

3. Compared to Deﬁnition 3 (the step oracle deﬁnition), we have the stronger guarantees that
24√s ) short-ﬂat decomposition in (4), and obtains its guarantees
1 (instead of a lower bound of 1
4 ).

A⊤diag (w) ∆ admits a (C2, Cprog
using the bounds 1

v
12 ≤ k

k2 ≤

We next demonstrate that a strong step oracle is a noisy step oracle.

Lemma 9. Suppose
Then,

Ostep is also a ( 1

Ostep is a (Cprog, C2, Cξ, δ)-strong step oracle for
∆, A).
4 Cprog, C2, Cξ, 2δ)-noisy step oracle for (

Rn and A

Rn

d.

×

∈

∈

∆

e

Proof. In the deﬁnition of a noisy step oracle, we only need to check the condition that
1
4 Cprog for an arbitrary ∆ =
from Deﬁnition 6. Note that

k2,(m) ≤

√mC −
ξ

ξ where

∆

−

e

k

ξ

1

P

, as all other conditions are immediate

∆i∆i ≥
e

i

[n] wi
∈

e

[n]
Xi
∈

wi

∆i∆i =

wi

∆i(

ξi)

e

Xi
[n]
∈
1
2

≥

∆i −
e
1
∆2
i −
2

e
wi

wiξ2
i .

Xi
[n]
∈
2 b2. The ﬁrst sum above is at least 1
where we used a2
2 Cprog by assumption. To upper
bound the second sum, we will use the second property in the deﬁnition of a strong step oracle. Let
S
[n] be the set consisting of the m largest coordinates of ξ (with ties broken lexicographically).
Let α be drawn from the distribution

[n]
Xi
∈

1
2 a2

ab

≥

−

−

⊂

e

1

δ
4sρ2 log d
δ

Multinom 

&

CprogC 2

ξ nsρ2 log d
δ
m

, 

1
n

'

, . . . ,

1
n

.





1
5 δCprogC 2
Note that with 1
0.1δ probability, by a Chernoﬀ bound, we have that
this happens, then since S consists of the largest coordinates of ξ, any vector β such that β
≤
entrywise and

P

{z

−

ξ . If
α

|

∈

i

1
4 δCprogC 2

ξ must have

β
k

k1 ≤




n






}



S αi ≥

βiξ2

i ≤

5
4

αiξ2
i .

S
Xi
∈

[n]
Xi
∈

22

Now note that for any S with

= m,

S
|

|

E

"
S
Xi
∈

αiξ2
i

# ≤

δCprogC 2
ξ
4m · k

ξ

2
2,(m) ≤
k

δCprog
4

.

Combining the above two inequalities and Markov’s inequality and the fact that the distribution
δ probability,
of α stochastically dominates the distribution of w, we deduce that with at least 1

−

wiξ2

i ≤

1
0.9δ ·

E



[n]
Xi
∈

max
β
α
≤
1
4 δCprogC2

[n]
ξ Xi
∈

β

k1≤


k


βiξ2

i 

≤

Cprog
2

.




Putting everything together, we conclude that we have

with failure probability at most 2δ, completing the proof.

Cprog
4

wi

[n]
Xi
∈

∆i∆i ≥
e

In Section 4.2, we prove that if A satisﬁes Assumption 2 (a slightly diﬀerent assumption than
Assumption 1) then with high probability we can implement a strong step oracle with appropriate
parameters. This is stated more formally in the following; recall m is deﬁned in (27).

Assumption 2. The matrix A
w⋆
w⋆
with
k
∞
Rd, ξ
that for all v

Rn with

= 1

m , a constants L, ρ

Rn

×

∈

≥

k∞ ≤
∈

∈

d satisﬁes the following. There is a weight vector w⋆

∆n
1, and constants K, Cξ (which may depend on L) such

∈

1
4 ≤ k

v

k2 ≤

1,

v

k

k1 ≤

2√2s,

ξ

k

k2,(m) ≤

√m
Cξ

we have, deﬁning

∆ = Av + ξ:

1. A is entrywise bounded by

ρ, i.e.

±

A

k

kmax ≤

ρ.

e

2.

3. There is a (L,

1

K√s ) short-ﬂat decomposition of A⊤W⋆

1
L ≤

[n]
Xi
∈

w⋆
i

∆2

i ≤

L.

e

1
K√s

∆:

e

2 ≤
(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)

trunc

A⊤W⋆

∆,

(cid:18)

e

(cid:13)
(cid:13)
(cid:13)
(cid:13)

(30)

(31)

L.

Lemma 10. Suppose A satisﬁes Assumption 2. Algorithm 5 is a (Cprog, C2, Cξ, δ) strong step
oracle StrongStepOracle for (

∆, A) with

Cprog = Ω(1), C2 = O (1) , Cξ = O(1), δ =

e

1
2

C2
105Cprog (cid:19)

(cid:18)

2

,

running in time

O

nd log3(ndρ) log

(cid:18)(cid:18)

1
δ

·

(cid:19)

(cid:18)

w⋆
∞

sρ2 log2 d
δ

.

(cid:19)(cid:19)

23

Here, in contrast to the noiseless setting, we can only guarantee that the strong step oracle (and
thus also the noisy step oracle) succeeds with constant probability. In our full algorithm, we boost
the success probability of the oracle by running a logarithmic number of independent trials and
aggregating the outputs. We also show that for an appropriate choice of constants in Deﬁnition 2,
Assumption 2 is also satisﬁed, stated in Lemma 11 and proven in Section 4.3.

Lemma 11. Suppose A
parameters in Deﬁnition 2. Then, A also satisﬁes Assumption 2.

d is (ρ, w⋆
∞

∈

×

Rn

)-wRIP with a suitable choice of constants in the RIP

Rd, (Cprog, C2, Cξ, δ′)-noisy

Onstep, δ, A, b)
∈
4 Cprog
C2
CξRξ ;
1
2 R with probability

)2, A

Rn

d,

∈

×

1

δ

−

≥

k2 ≤

l

x

x

∈

prog

−

Rd

≥ k

| k
;

step oracle
b = Ax⋆ + ξ⋆

Algorithm 3: HalfRadiusSparseNoisy(xin, R, Rξ,
1 Input: s-sparse xin ∈

xin −

Rd, R
x⋆
Onstep for all (∆, A) with ∆
ξ⋆
k

k2,(m) ≤

k2 for s-sparse x⋆
Rn, δ′ ≤
(10−
∈
Rξ√m, with R
≥
x⋆
xout −
k
√2sR
;
}

∈
Cprog
2C2
2

xink1 ≤

xin,
X ← {
100C2
2
, η
C2
←
10 log d
m
δ ;
Ntrials do

Rn for
2 Output: s-sparse vector xout that satisﬁes
3 x0 ←
4 T
←
5 Ntrials ←
6 for 1
j
≤
xj
7
0 ←
for 0
1 do
t ← Onstep(∆j
wj
γj
A⊤diag
t ←
if (wj
then
xj
xj
t truncated to its s largest coordinates ;
T ←
Break: ;

t , A) for ∆j
∆j
[wj
t =
t ]
(cid:17)
(cid:16)

R (Axj
[n][wj

b),
t −
t ]i[∆j

≤
x0 ;
t

t , γj

t ←

P

−

≤

≤

T

∈

9

8

1

end
xj
t+1 ←
end

argminx

∈X

x

xj
t −

−

ηRγt

;

15

(cid:13)
(cid:13)
(cid:13)
16 end
T , . . . , xNtrials
x1
Aggregate(
17 xT ←
{
18 Return: xout ←
xT truncated to its s largest coordinates ;

, R
}

2 ) ;

T

2
(cid:13)
(cid:13)
(cid:13)

10

11

12

13

14

t ]iai ;
t ) do not meet all of (3), (4) and the additional criteria in Deﬁnition 6

i

, R)
S
Rd, R
z

Algorithm 4: Aggregate(
1 Input:

=
yi}i
[k] ⊂
S
{
∈
yi −
have
points yi ∈ S
k
2 Output:
z
z
−
3 for 1
i
4
5 end

k
if at least 0.51k points yj ∈ S
e

z with
k do

≥
k2 ≤
R ;

k2 ≤

≤
e

≤

0 such that for some unknown z
R
3 ;

∈

Rd, at least 0.51k

satisfy

yi −
k

yjk2 ≤

2R
3 then Return:

z

yi ;

←

e

Next, we give a guarantee regarding our geometric post-processing step, Algorithm 4.

24

Claim 1. Aggregate(

S

, R) runs in O(k2d) time and meets its output guarantees.

Proof. Let T be the subset of indices i
yi for some i
returned by the algorithm. The ball of radius 2R
z, since otherwise it can only contain at most 0.49k points. Thus,
k2 ≤
dominated by the time it takes to do k2 distance comparisons of points in Rd.

R
3 . Whenever the algorithm tests
yi −
T , it will be returned and satisﬁes the desired properties. Now consider any yi
3 around
R. The runtime is

3 around yi intersects the ball of radius R
yi −
k

[k] such that

k ≤

∈

∈

k

z

z

We remark that is possible that for k = Ω(log 1

δ ) as is the case in our applications, the runtime
of Claim 1 can be improved to have a better dependence on k by subsampling the points and using
low-rank projections for distance comparisons.

Lemma 12. Assume A satisﬁes Assumption 2. Then, Algorithm 3 meets its output guarantees in
time

O

nd log3(ndρ)

w⋆
∞

·

sρ2 log d

log2 d
δ

·

.

(cid:18)

(cid:0)

(cid:1)

(cid:0)

(cid:19)
[Ntrials], except with probability 1

(cid:1)

Proof. We claim that for each independent trial j
output xj
trials satisfy

T δ′, the
R
x⋆
6 . Once we prove this, by Chernoﬀ at least 0.51Ntrials of the
R
6 except with probability at most δ, and then we are done by Claim 1.
It remains to prove the above claim. Fix a trial j, and drop the superscript j for notational
Onstep. Since b = Ax⋆ + ξ⋆, we have

convenience. In every iteration t,

T satisﬁes
xj
T −
k

xj
T −
k
x⋆
k2 ≤

R (Axt −

b) is given to

∆ := 1

k2 ≤

−

∈

e
∆ =

1
R

(A(x

−

x⋆) + ξ⋆) = Av + ξ,

for

v
k

k2 ≤

1,

v

k

k1 ≤

2√2s, and

e
ξ
k

k2,(m) ≤

√m
Cξ

, where the last inequality used the assumed bounds

ξ⋆

k

k2,(m) ≤

Rξ√m,

Rξ
R ≤

1
Cξ

.

Hence, by the assumptions on
except with probability
δ′. If the check in Line 10 fails, then except with probability
≤
R
6 follows analogously to Lemma 3, since v = 1
conclusion
k2 ≤

Onstep, it will not fail for such inputs unless
R (x

xT −
k

k2 ≥

x⋆).

v
k

x⋆

−

The other case’s correctness follows identically to the proof of Lemma 3, except for one diﬀer-

1
12 is violated,
δ′, the

≤

ence: to lower bound the progress term (10), we use the assumption (28) which shows

2ηR

γt, xt −

h

x⋆

i

= 2ηR

wi

[n]
Xi
∈

ai, v

i

= 2ηR2

wi

[n]
Xi
∈

∆i h
e

∆i∆i ≥
e

2ηR2Cprog.

Hence, following the proof of Lemma 3 (and adjusting for constants), whenever the algorithm does
not terminate we make at least a 50
T fraction of the progress towards x⋆, so in T iterations (assuming
xT −
no step oracle failed) we will have
k

R
6 .
Finally, the runtime follows from combining Lemma 10 (with constant failure probability) with
Ntrials due to the number of calls to the step oracle, contributing
δ term to

a multiplicative overhead of T
one additional logarithmic factor. We adjusted one of the log d terms to become a log d
account for the runtime of Aggregate (see Claim 1).

k2 ≤

x⋆

·

25

4.2 Designing a strong step oracle

In this section, we design a strong step oracle
our oracle iteratively builds a weight vector ¯w, and sets

Ostep(

∆, A) under Assumption 2. As in Section 3.2,

γ ¯w :=

e
¯wi
∆iai.

We will use essentially the same potentials as in (13), deﬁned in the following:

[n]
Xi
∈

e

Φ2( ¯w) :=

¯wi

∆2
i ,

Φsqmax( ¯w) :=

e

[n]
Xi
∈

e

e

p

k

(cid:18)

min
L
k

k2≤

¯w

sqmaxµ(γ ¯w −

p)

(cid:19)

k1

+ k

¯w
k1
4CLs

.

(32)

Algorithm 5: StrongStepOracle(
Rn, A

1 Input:
2 Output: (w, γ) such that γ =

Rn

∆

∈

∈

×

e

∆, A, δ)

d satisfying Assumption 1, δ

(0, 1) ;
∆iai, and if there is v

∈

e
such that
satisﬁed with

∆ = Av + ξ where

e

i

[n] wi
∈
k2,(m) ≤
e

ξ
P
k

√m
Cξ

, with probability

∈

Rd with 1

v

1

12 ≤ k
δ, (3), (4) are

k2 ≤

1

−

≥

Cprog = 1, C2 = O (1) .

Furthermore, the second condition in (4) is satisﬁed with the constant 24 rather than 6,
and there is Cξ = O(1) such that (29) is also satisﬁed.
2
log2
δ ⌉

∞sρ2 log d , N ′ ← ⌈

1
√Cs log d , η

3200, µ

Kw⋆

←

1

;

3 C
←
4 for 0
5

6

7

8

9

←
N ′ do

;

5Ln
η ⌉

← ⌈
N do

k
≤
≤
0n, N
w0 ←
for 0
t
≤
≤
if
Φ2(wt)
≥
Sample i
∼unif. [n] ;
e
Compute (using Lemma 4) dt ∈

1 then Return: γ

←

i

[n][wt]i
∈

∆iai, w

wt ;

←

P
[0, ηw⋆
∞

] maximizing to additive O( η
n )

e

Γt(d) :=

Φ2(wt + dei)

Cs

Φsqmax(wt + dei)

−

e

e

wt+1 ←

wt + dtei ;

10

end

11 end
12 Return: γ

0d, w

0n

←

←

Algorithm 5 is essentially identical to Algorithm 2 except for changes in constants. We further

have the following which veriﬁes the second property in the deﬁnition of strong step oracle.

Fact 2. The distribution of w returned by Algorithm 5 is stochastically dominated by the distribution

ηw∗
∞

Multinom 

5Ln
η

, 

1
n

, . . . ,

1
n







Proof. Every time we inspect a row, we change the corresponding entry of w by at most ηw∗
∞
result follows from the number of iterations in the algorithm and uniformity of sampling rows.



}







{z

|

n

. The

26

To analyze Algorithm 5, we provide appropriate analogs of Lemmas 5 and 1. Because Algo-
rithm 5 is very similar to Algorithm 2, we will largely omit the proof of the following statement,
which follows essentially identically to the proof of Lemma 5 up to adjusting constants.

Lemma 13. Assume that the constant K in Assumption 2 is suﬃciently large, and that
Av + ξ where v, ξ satisfy the norm conditions in Assumption 2. Then for any ¯w
B( ¯w)

C 2µ2 log d, we have

Rn
≥

∆ =
0 such that

∈

e

≤

Ei

∼unif.[n][B( ¯w + ηw⋆
i )]

≤

B( ¯w) +

1
2CLs ·

η
n

.

Proof. The analysis is essentially identical to that of Lemma 5; we discuss only the main diﬀerence.
To apply the Taylor expansion of the exponential, Lemma 5 required a bound that

1
µ

= O

z(i)
j
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
for all i
∈
Here, we will take z(i)
j = ηw⋆
i (
∆iaij −
Lemma 5, except that
ai, v
∆i =
i
h
e

[n] and j

(cid:18)

∈

1
√log d

=

⇐

(cid:19)

= O

1
√s log d

,

(cid:19)

(cid:18)

z(i)
j
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

[d]. Note that in the setting of Lemma 5, we took z(i)

p⋆
j ).
p⋆
j ); bounds on all of these terms follow identically to in

i (∆iaij −

j = ηw⋆

+ ξi, so we need to show

e

ηw⋆
ξi|
i ρ
|

= O

1
√s log d

.

(cid:19)

(cid:18)

ξi|
This follows since η
|
m, this is equivalent to m = Ω(sρ2), an explicit assumption we make.

= O(√m) by assumption. Hence, as w⋆

1
log d and

≤

i ≤

1
m by deﬁnition of

We now give a full analysis of Algorithm 5, patterned oﬀ of Lemma 1.

Lemma 10. Suppose A satisﬁes Assumption 2. Algorithm 5 is a (Cprog, C2, Cξ, δ) strong step
oracle StrongStepOracle for (

∆, A) with

running in time

Cprog = Ω(1), C2 = O (1) , Cξ = O(1), δ =

e

1
2

C2
105Cprog (cid:19)

(cid:18)

2

,

O

nd log3(ndρ) log

(cid:18)(cid:18)

1
δ

·

(cid:19)

(cid:18)

w⋆
∞

sρ2 log2 d
δ

.

(cid:19)(cid:19)

Proof. The analysis is essentially identical to that of Algorithm 2 in Lemma 1; we discuss diﬀerences
here. First, the stochastic domination condition follows from Fact 2 for suﬃciently large Cξ, K.

For the remaining properties, since the algorithm runs N ′ ≥

suﬃces to show each run meets Deﬁnition 6 with probability
assuming there exists the desired decomposition
bounding with the failure probability in Fact 2 yields the overall failure probability.

2
δ times independently, it
1
2 under the events of Assumption 2,
∆ = Av + ξ in the sense of Assumption 2. Union

log2

≥

Correctness. As in Lemma 1, it is straightforward to see that
Φ2 is 1-Lipschitz, since the value
of η is smaller than that used in Algorithm 2. The termination condition in iteration T then again
3
implies
Cs . For C = 3200, this implies the short-ﬂat decomposition
with stronger parameters required by Deﬁnition 6, as well as the

Φsqmax(wT )

Φ2(wT )

1, and

≤

≥

e
wT k1 bound.

k

Success probability. As in Lemma 1, the expected growth in Φt in any iteration where Pr[
1]

η
4Ln . Hence, running for

1
2 is

5Ln
η

iterations and using Φt −

Φ0 ≤

≥

≥

≤

Φ2(wt)

≥

2 yields the claim.
e

e

e

e

27

Runtime. This follows identically to the analysis in Lemma 1.

4.3 Deterministic assumptions for noisy regression

In this section, we prove Lemma 11, restated here for completeness. The proof will build heavily
on our previous developments in the noiseless case, as shown in Section 3.3.

Lemma 11. Suppose A
parameters in Deﬁnition 2. Then, A also satisﬁes Assumption 2.

d is (ρ, w⋆
∞

∈

×

Rn

)-wRIP with a suitable choice of constants in the RIP

Proof. The analysis is largely similar to the analysis of Lemma 8; we will now discuss the diﬀerences
here, which are introduced by the presence of the noise term ξ. There are three components to
discuss: the upper and lower bounds in (30), and the decomposition (31).

Regarding the bounds in (5), by changing constants appropriately in Deﬁnition 2, we can assume
4 . In particular,

that A satisﬁes the second property in Assumption 1 with the parameters 4
for ∆ = Av, we then have

L and L

Recall that

∆ = ∆ + ξ for some

ξ
k

k2,(m) ≤

. Hence,

4
L ≤

w⋆

i ∆2

i ≤

L
4

.

[n]
Xi
∈
√m
Cξ

e

[n]
Xi
∈

w⋆
i

∆2

i ≤

e

≤

2

w⋆

i ∆2

i + 2

[n]
Xi
∈
L
+ 2
2

1
m k

(cid:18)

w⋆

i ξ2
i

[n]
Xi
∈

ξ

2
2,(m)
k

L,

≤

(cid:19)

2a2 + 2b2, and the
for an appropriately large C 2
1
w⋆
i ξ2
second inequality used that the largest
m
k
is attained by greedily choosing the m largest coordinates of ξ by their magnitude, and setting
w⋆
m for those coordinates. This gives the upper bound in Assumption 2, and the lower bound
follows similarly: for appropriately large C 2

4
L . Here the ﬁrst inequality used (a + b)2
w⋆
k

i can be subject to

k1 = 1 and

[n] w⋆
∈

i = 1

k∞ ≤

ξ ≥

P

≤

i

L
2 ,

ξ ≥
1
2

Xi
[n]
∈
1
2

2
L −

w⋆

i ∆2

i −

w⋆

i ξ2
i

1
2

[n]
Xi
∈

1
m k

ξ

2
2,(m)
k

1
L

.

≥

w⋆
i

∆2

i ≥

e

[n]
Xi
∈

≥

(cid:18)
Lastly, for the decomposition required by (31), we will use the decomposition of Lemma 2 for the
component due to
i ∆iai; in particular, assume by adjusting constants that this component
has a ( L
2 ,

2K√s ) short-ﬂat decomposition. It remains to show that

[n] w⋆
∈

(cid:19)

1

i

P

w⋆

i ξiai = A⊤W⋆ξ.

[n]
Xi
∈

also admits a ( L
2 ,
inequality. Let u = (W⋆)

1

1
2 ξ; from earlier, we bounded

2K√s ) short-ﬂat decomposition, at which point we may conclude by the triangle

u
k
k

2
2 ≤

1
m k

ξ

2
2,(m) =
k

u
k2 ≤

⇒ k

1
Cξ

.

28

1
Hence, applying Lemma 7 using the RIP matrix (W⋆)
2 A with appropriate parameters yields the
conclusion, for large enough Cξ. In particular, the ℓ2-bounded part of the decomposition follows
from Lemma 7, and the proof of the ℓ

-bounded part is identical to the proof in Lemma 8.

∞

4.4 Putting it all together

We now prove our main result on noisy recovery.

Theorem 4. Let δ
d is (ρ, w⋆
A
×
∞
at least 1

Rn

∈

x⋆
(0, 1), and suppose R0 ≥ k
∈
)-wRIP and b = Ax⋆ + ξ⋆, and R1 ≥

k2 for s-sparse x⋆

Rξ := 1

ξ⋆

Rd. Further, suppose
k2,(m). Then with probability

∈

√m k

δ, Algorithm 3 using Algorithm 3 as a noisy step oracle computes ˆx satisfying

−

in time

x⋆

ˆx

k

−

k2 ≤

Rﬁnal = Θ(Rξ),

(cid:18)

O

log

log

(cid:18)(cid:18)

d
δ ·

ndw⋆
∞

s log4(ndρ) log2

R0
Rﬁnal (cid:19)

R1
Rﬁnal (cid:19)(cid:19)
k2,(m), initialized
Proof. Our algorithm will iteratively maintain a guess Rguess on the value of
Rξ, the hypothesis of Algorithm 3 is satisﬁed, and hence
at Rguess ←
using a strategy similar to the proof of Theorem 3 (but terminating at accuracy R = O(Rguess)
where the constant is large enough to satisfy the assumption R
CξRguess) results in an estimate
at distance R with probability at least 1

R1. For each value of Rguess ≥

R0
Rﬁnal (cid:19)
ξ⋆

R1
Rﬁnal (cid:19)(cid:19)(cid:19)

δ, with runtime

1
√m k

ρ2 log

log

≥

(cid:18)

(cid:18)

(cid:18)

(cid:18)

.

·

−

ndw⋆
∞

O

(cid:18)(cid:18)

sρ2 log4(ndρ) log2

d
δ ·

log

R0
Rﬁnal (cid:19)(cid:19)(cid:19)

·

(cid:18)

(cid:18)

ρ2 log

R0
Rﬁnal (cid:19)(cid:19)

.

(cid:18)

The runtime above follows from Lemma 12.

Our overall algorithm repeatedly halves Rguess, and outputs the last point returned by a run of
the algorithm where it can certify a distance bound to x⋆ of R = CξRguess. We use Rﬁnal to denote
Rξ this certiﬁcation will succeed, so we at most
CξRguess on the last run. Clearly for any Rguess ≥
2CξRξ. The ﬁnal runtime follows
lose a factor of 2 in the error guarantee as we will have Rﬁnal ≤
from adjusting δ by a factor of O(log R1
Rﬁnal

) to account for the multiple runs of the algorithm.

29

References

[Ans60]

Frank J Anscombe. Rejection of outliers. Technometrics, 2(2):123–146, 1960. 1

[ANW10]

Alekh Agarwal, Sahand N. Negahban, and Martin J. Wainwright. Fast global conver-
gence rates of gradient methods for high-dimensional statistical recovery. In Advances
in Neural Information Processing Systems 23: 24th Annual Conference on Neural In-
formation Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010,
Vancouver, British Columbia, Canada, pages 37–45, 2010. 3, 3.1, A.3

[ANW12]

Alekh Agarwal, Sahand Negahban, and Martin J Wainwright. Fast global convergence
of gradient methods for high-dimensional statistical recovery. The Annals of Statistics,
pages 2452–2482, 2012. 1, 1, 1.2

[AP17]

[AV18]

Abhishek Aich and P Palanisamy. On application of omp and cosamp algorithms for
doa estimation problem. In 2017 International Conference on Communication and
Signal Processing (ICCSP), pages 1983–1987. IEEE, 2017. 1

Pranjal Awasthi and Aravindan Vijayaraghavan. Towards learning sparsely used dic-
tionaries with arbitrary supports. In Mikkel Thorup, editor, 59th IEEE Annual Sym-
posium on Foundations of Computer Science, FOCS 2018, Paris, France, October 7-9,
2018, pages 283–296. IEEE Computer Society, 2018. 1

[BBC11]

Stephen Becker, J´erˆome Bobin, and Emmanuel J Cand`es. Nesta: A fast and accurate
ﬁrst-order method for sparse recovery. SIAM Journal on Imaging Sciences, 4(1):1–39,
2011. 1, 1.2

[BCDH10] Richard G Baraniuk, Volkan Cevher, Marco F Duarte, and Chinmay Hegde. Model-
based compressive sensing. IEEE Transactions on information theory, 56(4):1982–
2001, 2010. 1.2

[BD09]

[BD10]

Thomas Blumensath and Mike E Davies. Iterative hard thresholding for compressed
sensing. Applied and computational harmonic analysis, 27(3):265–274, 2009. 1, 1.2,
A, A.1

Thomas Blumensath and Mike E Davies. Normalized iterative hard thresholding:
IEEE Journal of selected topics in signal
Guaranteed stability and performance.
processing, 4(2):298–309, 2010. 1, 1.2

[BDMS13] Afonso S Bandeira, Edgar Dobriban, Dustin G Mixon, and William F Sawin. Cer-
tifying the restricted isometry property is hard. IEEE transactions on information
theory, 59(6):3448–3450, 2013. 1.1

[Bel18]

[Blu03]

[BRT09]

[BRW21]

Pierre C Bellec. The noise barrier and the large signal bias of the lasso and other
convex estimators. arXiv preprint arXiv:1804.01230, 2018. 1.2

Avrim Blum. Machine learning: My favorite results, directions, and open problems.
In 44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Pro-
ceedings., pages 2–2. IEEE, 2003. 1

Peter J Bickel, Ya’acov Ritov, and Alexandre B Tsybakov. Simultaneous analysis of
lasso and dantzig selector. The Annals of statistics, 37(4):1705–1732, 2009. 1.2

Aditya Bhaskara, Aravinda Kanchana Ruwanpathirana, and Maheshakya Wijewar-
dena. Principal component regression with semirandom observations via matrix com-
pletion.
In International Conference on Artiﬁcial Intelligence and Statistics, pages
2665–2673. PMLR, 2021. 1

30

[BS95]

[BT09]

[CG18]

Avrim Blum and Joel Spencer. Coloring random and semi-random k-colorable graphs.
Journal of Algorithms, 19(2):204–234, 1995. 1, 1.2

Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for
linear inverse problems. SIAM journal on imaging sciences, 2(1):183–202, 2009. 1,
1.2

Yu Cheng and Rong Ge. Non-convex matrix completion against a semi-random adver-
sary. In Conference On Learning Theory, pages 1362–1394. PMLR, 2018. (document),
1, 1.1, 1.2

[CJSX14]

Yudong Chen, Ali Jalali, Sujay Sanghavi, and Huan Xu. Clustering partially ob-
served graphs via convex optimization. The Journal of Machine Learning Research,
15(1):2213–2238, 2014. 1.2

[CKMY20] Sitan Chen, Frederic Koehler, Ankur Moitra, and Morris Yau. Classiﬁcation under
misspeciﬁcation: Halfspaces, generalized linear models, and connections to evolvabil-
ity. arXiv preprint arXiv:2006.04787, 2020. 1.2

[CLM+16] Michael B. Cohen, Yin Tat Lee, Gary L. Miller, Jakub Pachocki, and Aaron Sidford.
Geometric median in nearly linear time. In Daniel Wichs and Yishay Mansour, editors,
Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2016, Cambridge, MA, USA, June 18-21, 2016, pages 9–21. ACM, 2016. 12

[CR05]

[CRT06]

[CSX12]

[CT06]

Emmanuel J Candes and Justin K Romberg. Signal recovery from random projections.
In Computational Imaging III, volume 5674, pages 76–86. International Society for
Optics and Photonics, 2005. 1, 1.2

Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery
from incomplete and inaccurate measurements. Communications on Pure and Applied
Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences,
59(8):1207–1223, 2006. 1, 1.2

Yudong Chen, Sujay Sanghavi, and Huan Xu. Clustering sparse graphs. arXiv preprint
arXiv:1210.3335, 2(5), 2012. 1.2

Emmanuel J Candes and Terence Tao. Near-optimal signal recovery from random
projections: Universal encoding strategies? IEEE transactions on information theory,
52(12):5406–5425, 2006. 1, 1, 1.2

[CW05]

Patrick L Combettes and Val´erie R Wajs. Signal recovery by proximal forward-
backward splitting. Multiscale Modeling & Simulation, 4(4):1168–1200, 2005. 1, 1.2

[DDDM04]

Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding
algorithm for linear inverse problems with a sparsity constraint. Communications
on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences, 57(11):1413–1457, 2004. 1, 1.2

[DDEK12] Mark A Davenport, Marco F Duarte, Yonina C Eldar, and Gitta Kutyniok. Introduc-

tion to compressed sensing., 2012. 1.2

[DGT19]

Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos. Distribution-independent
pac learning of halfspaces with massart noise. arXiv preprint arXiv:1906.10075, 2019.
1.2

[DHL17]

Arnak S Dalalyan, Mohamed Hebiri, and Johannes Lederer. On the prediction per-
formance of the lasso. Bernoulli, 23(1):552–581, 2017. 1.2

31

[DIK+21]

[DK20]

[DKK+21]

[DKT21]

[DKTZ20]

Ilias Diakonikolas, Russell Impagliazzo, Daniel Kane, Rex Lei, Jessica Sorrell, and
Christos Tzamos. Boosting in the presence of massart noise.
arXiv preprint
arXiv:2106.07779, 2021. 1.2

Ilias Diakonikolas and Daniel M Kane. Hardness of learning halfspaces with massart
noise. arXiv preprint arXiv:2012.09720, 2020. 1.2

Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos
Zariﬁs. Threshold phenomena in learning halfspaces with massart noise. arXiv preprint
arXiv:2108.08767, 2021. 1.2

Ilias Diakonikolas, Daniel M Kane, and Christos Tzamos. Forster decomposition and
learning halfspaces with noise. arXiv preprint arXiv:2107.05582, 2021. 1.2

Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zariﬁs. Learning half-
spaces with massart noise under structured distributions. In Conference on Learning
Theory, pages 1486–1513. PMLR, 2020. 1.2

[DNW13] Mark A Davenport, Deanna Needell, and Michael B Wakin. Signal space cosamp
for sparse recovery with redundant dictionaries. IEEE Transactions on Information
Theory, 59(10):6820–6829, 2013. 1

[Don06]

[DPT21]

[DS89]

[EK12]

[ES09]

[FK00]

[FK01]

[FN03]

[Fou11]

[GLS88]

David L Donoho. Compressed sensing. IEEE Transactions on information theory,
52(4):1289–1306, 2006. 1.2

Ilias Diakonikolas, Jongho Park, and Christos Tzamos. Relu regression with massart
noise. arXiv preprint arXiv:2109.04623, 2021. 1.2

David L Donoho and Philip B Stark. Uncertainty principles and signal recovery. SIAM
Journal on Applied Mathematics, 49(3):906–931, 1989. 1, 1.2

Yonina C Eldar and Gitta Kutyniok. Compressed sensing: theory and applications.
Cambridge university press, 2012. 1, 1.2

Micha Elsner and Warren Schudy. Bounding and comparing methods for correlation
clustering beyond ilp. In Proceedings of the Workshop on Integer Linear Programming
for Natural Language Processing, pages 19–27, 2009. 1.2

Uriel Feige and Robert Krauthgamer. Finding and certifying a large hidden clique in
a semirandom graph. Random Structures & Algorithms, 16(2):195–208, 2000. 1.2

Uriel Feige and Joe Kilian. Heuristics for semirandom graph problems. Journal of
Computer and System Sciences, 63(4):639–671, 2001. 1, 1.2

M´ario AT Figueiredo and Robert D Nowak. An em algorithm for wavelet-based image
restoration. IEEE Transactions on Image Processing, 12(8):906–916, 2003. 1, 1.2

Simon Foucart. Hard thresholding pursuit: an algorithm for compressive sensing.
SIAM Journal on Numerical Analysis, 49(6):2543–2563, 2011. 1, 1.2

Martin Gr¨otschel, L´aszl´o Lov´asz, and Alexander Schrijver. Geometric Algorithms and
Combinatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer,
1988. A.3

[GRSY14] Amir Globerson, Tim Roughgarden, David Sontag, and Cafer Yildirim. Tight error

bounds for structured prediction. arXiv preprint arXiv:1409.5834, 2014. 1.2

[Hub64]

Peter J Huber. Robust estimation of a location parameter. The Annals of Mathemat-
ical Statistics, pages 73–101, 1964. 1

32

[JTK14]

Prateek Jain, Ambuj Tewari, and Purushottam Kar. On iterative hard thresholding
methods for high-dimensional m-estimation. In NIPS, 2014. 1, 1.2

[KKMR21] Jonathan Kelner, Frederic Koehler, Raghu Meka, and Dhruv Rohatgi. On the power
of preconditioning in sparse linear regression. arXiv preprint arXiv:2106.09207, 2021.
1.2

[KM14]

[KMM11]

[Kut13]

[LF81]

[LSTZ20]

[MD10]

[MMV12]

[MMV13]

[MMV14]

[MMV15]

[MMV16]

[MN06]

[Moi17]

Vladimir Koltchinskii and Stanislav Minsker. l1-penalization in functional linear re-
gression with subgaussian design. Journal de l’Ecole polytechnique-Math´ematiques,
1:269–330, 2014. 1.2

Alexandra Kolla, Konstantin Makarychev, and Yury Makarychev. How to play unique
games against a semi-random adversary: Study of semi-random models of unique
games. In 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,
pages 443–452. IEEE, 2011. 1.2

Gitta Kutyniok. Theory and applications of compressed sensing. GAMM-Mitteilungen,
36(1):79–101, 2013. 1.2

Shlomo Levy and Peter K Fullagar. Reconstruction of a sparse spike train from a
portion of its spectrum and application to high-resolution deconvolution. Geophysics,
46(9):1235–1243, 1981. 1.2

Jerry Li, Aaron Sidford, Kevin Tian, and Huishuai Zhang. Well-conditioned meth-
ods for ill-conditioned systems: Linear regression with semi-random noise, 2020.
(document), 1.1, 1.1, 1.2

Arian Maleki and David L Donoho. Optimally tuned iterative reconstruction algo-
rithms for compressed sensing. IEEE Journal of Selected Topics in Signal Processing,
4(2):330–341, 2010. 1, 1.2

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Approx-
imation algorithms for semi-random partitioning problems.
In Proceedings of the
forty-fourth annual ACM symposium on Theory of computing, pages 367–384, 2012.
1.2

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Sorting
noisy data with partial information. In Proceedings of the 4th conference on Innova-
tions in Theoretical Computer Science, pages 515–528, 2013. 1.2

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Constant
factor approximation for balanced cut in the pie model. In Proceedings of the forty-
sixth annual ACM symposium on Theory of computing, pages 41–49, 2014. 1.2

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Corre-
lation clustering with noisy partial information. In Conference on Learning Theory,
pages 1321–1342. PMLR, 2015. 1.2

Konstantin Makarychev, Yury Makarychev, and Aravindan Vijayaraghavan. Learning
communities in the presence of errors. In Conference on learning theory, pages 1258–
1291. PMLR, 2016. 1.2
Pascal Massart and ´Elodie N´ed´elec. Risk bounds for statistical learning. The Annals
of Statistics, 34(5):2326–2366, 2006. 1.2

Ankur Moitra. What does robustness say about algorithms. ICML ’17 Tutorial, 2017.
1

33

[MS10]

[MZ93]

[MPW16]

Ankur Moitra, William Perry, and Alexander S Wein. How robust are reconstruction
thresholds for community detection? In Proceedings of the forty-eighth annual ACM
symposium on Theory of Computing, pages 828–841, 2016. 1, 3, 1.2
In
Claire Mathieu and Warren Schudy. Correlation clustering with noisy input.
Proceedings of the twenty-ﬁrst annual ACM-SIAM symposium on Discrete Algorithms,
pages 712–728. SIAM, 2010. 1.2
St´ephane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dic-
tionaries. IEEE Transactions on signal processing, 41(12):3397–3415, 1993. 1, 1, 1.2
[NRWY12] Sahand N Negahban, Pradeep Ravikumar, Martin J Wainwright, and Bin Yu. A
uniﬁed framework for high-dimensional analysis of m-estimators with decomposable
regularizers. Statistical science, 27(4):538–557, 2012. 1, 1, 1.2
Deanna Needell and Joel A Tropp. Cosamp: Iterative signal recovery from incomplete
and inaccurate samples. Applied and computational harmonic analysis, 26(3):301–321,
2009. 1, 1, 1.2, A
Deanna Needell and Roman Vershynin. Signal recovery from incomplete and inac-
curate measurements via regularized orthogonal matching pursuit. IEEE Journal of
selected topics in signal processing, 4(2):310–316, 2010. 1, 1.2

[NV10]

[NT09]

[RV06]

[Rou21]

[PRK93]

[RWY10]

[PCBVB14] Luisa F Polania, Rafael E Carrillo, Manuel Blanco-Velasco, and Kenneth E Barner.
Exploiting prior knowledge in compressed sensing wireless ecg systems. IEEE journal
of Biomedical and Health Informatics, 19(2):508–519, 2014. 1
Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krish-
naprasad. Orthogonal matching pursuit: Recursive function approximation with ap-
plications to wavelet decomposition. In Proceedings of 27th Asilomar conference on
signals, systems and computers, pages 40–44. IEEE, 1993. 1, 1.2
Tim Roughgarden. Beyond the Worst-Case Analysis of Algorithms. Cambridge Uni-
versity Press, 2021. 1.2
Mark Rudelson and Roman Vershynin. Sparse reconstruction by convex relaxation:
Fourier and gaussian measurements. In 2006 40th Annual Conference on Information
Sciences and Systems, pages 207–212. IEEE, 2006. 1.2
Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties
for correlated gaussian designs. The Journal of Machine Learning Research, 11:2241–
2259, 2010. 1.2
Ludwig Schmidt. Algorithms above the noise ﬂoor. PhD thesis, Massachusetts Insti-
tute of Technology, 2018. 1.2
Fadil Santosa and William W Symes. Linear inversion of band-limited reﬂection
seismograms. SIAM Journal on Scientiﬁc and Statistical Computing, 7(4):1307–1330,
1986. 1.2
Joel A Tropp and Anna C Gilbert. Signal recovery from random measurements via
orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):4655–
4666, 2007. A, A.2
John W Tukey. A survey of sampling from contaminated distributions. Contributions
to probability and statistics, pages 448–485, 1960. 1
John W Tukey. Mathematics and the picturing of data. In Proceedings of the Interna-
tional Congress of Mathematicians, Vancouver, 1975, volume 2, pages 523–531, 1975.
1

[Tuk60]

[Tuk75]

[Sch18]

[TG07]

[SS86]

34

[vdBLL+21] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sidford,
Zhao Song, and Di Wang. Minimum cost ﬂows, mdps, and 1-regression in nearly linear
time for dense instances. In STOC ’21: 53rd Annual ACM SIGACT Symposium on
Theory of Computing, Virtual Event, Italy, June 21-25, 2021, pages 859–869, 2021. 1

[vdBLSS20] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense lin-
ear programs in nearly linear time. In Proccedings of the 52nd Annual ACM SIGACT
Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26,
2020, pages 775–788, 2020. 1

[VdGL13]

[ZL21]

[ZWJ17]

Sara Van de Geer and Johannes Lederer. The lasso, correlated design, and improved
oracle inequalities.
In From Probability to Statistics and Back: High-Dimensional
Models and Processes–A Festschrift in Honor of Jon A. Wellner, pages 303–316. In-
stitute of Mathematical Statistics, 2013. 1.2

Chicheng Zhang and Yinan Li. Improved algorithms for eﬃcient active learning half-
spaces with massart and tsybakov noise. arXiv preprint arXiv:2102.05312, 2021. 1.2

Yuchen Zhang, Martin J Wainwright, and Michael I Jordan. Optimal prediction for
sparse linear models? lower bounds for coordinate-separable m-estimators. Electronic
Journal of Statistics, 11(1):752–799, 2017. 1.2

[ZWW+16] Zhimin Zhang, Shoushui Wei, Dingwen Wei, Liping Li, Feng Liu, and Chengyu Liu.
Comparison of four recovery algorithms used in compressed sensing for ecg signal
processing.
In 2016 Computing in Cardiology Conference (CinC), pages 401–404.
IEEE, 2016. 1

35

A Greedy and non-convex methods fail in the semi-random setting

In this section, we show how a few standard, commonly-used non-convex or greedy methods can
fail (potentially quite drastically) in the semi-random adversary setting. The two algorithms that
we examine are Iterative Hard Thresholding and Orthogonal Matching Pursuit [BD09, TG07].
We believe it is likely that similar counterexamples can be constructed for other, more complex
algorithms such as CoSaMP [NT09]. For simplicity in this section, we will only discuss the speciﬁc
semi-random model introduced in Deﬁnition 1, where A is pRIP, i.e. it contains an unknown RIP
matrix G as a subset of its rows.

A.1

Iterative hard thresholding

The iterative hard thresholding algorithm [BD09] involves initializing x0 = 0 and taking

xt+1 = Hs

xt −

(cid:18)

1
n

A⊤(b

−

Axt)

(cid:19)

where Hs zeroes out all but the s largest entries in magnitude (ties broken lexicographically). We
can break this algorithm in the semi-random setting by simply duplicating one row many times.

Hard semi-random adversary. Let n = Cm for some suﬃciently large constant C. The ﬁrst m
rows of A are drawn independently from
(0, I), except set the ﬁrst entry
∼ N
1)m rows of A all equal to v. We will set the sparsity parameter
of v to 1. We set the last (C
s = 1 and let x⋆ = (1, 0, . . . , 0). We let b = Ax⋆.

(0, I). Now draw v

N

−

Proposition 1. With A, b generated as above, with high probability, iterative hard thresholding
does not converge.

Proof. With high probability, some coordinate of v is Ω(√log d). We then have that some entry of
A⊤b has magnitude at least Ω(n√log d) with high probability. Thus, the next iterate x1 must have
exactly one nonzero entry that has magnitude at least Ω(√log d) and furthermore, this entry must
correspond to some coordinate of v that has magnitude at least Ω(√log d). However, this means
that the residuals in all of the rows that are copies of v are at least Ω(log d). In the next step, by
the same argument, we get that the residuals blow up even more and clearly this algorithm will
never converge. In fact, xt will never have the right support because its support will always be on
one of the entries where v is large.

A.2 Orthogonal matching pursuit

The orthogonal matching pursuit algorithm [TG07] involves initializing x0 = 0 and keeping track
of a set S (that corresponds to our guess of the support of x⋆). Each iteration, we choose a column
cj of A that maximizes |h
b is the residual). We then
k
add j to S and project the residual onto the orthogonal complement of all coordinates in S. We
show that we can again very easily break this algorithm in the semi-random setting.

and then add j to S (where rt = Axt −

cj,rt
i|
2
cj
2
k

Hard semi-random adversary. Let n = 3m. First, we draw all rows of A independently from
(0, I). Next, we modify some of the entries in the last 2m rows. Let s be the sparsity parameter.
N
2 , 0, . . . , 0) be supported on the ﬁrst s coordinates and set b = Ax⋆. Now we
Let x⋆ = (s−
modify the columns of A (aside from the ﬁrst s so Ax⋆ is not aﬀected). We set the last 2m entries
of one of these columns cj to match those of b.

1
2 , . . . , s−

1

Proposition 2. With A, b generated as above, with high probability, orthogonal matching pursuit
does not recover x⋆.

36

cj,b
Proof. With high probability (as long as s
i|
2
cj
2
k
because its last 2m entries exactly match those of b. However, j is not in the support of x⋆ so the
algorithm has already failed.

10), the column cj is the one that maximizes |h
k

≥

We further make the following observation.

Remark 1. By modifying other columns of A as well, the semi-random adversary can actually
make the algorithm pick all of the wrong columns in the support.

A.3 Convex methods

Now we brieﬂy comment on how convex methods are robust, in the sense that they can still be used
in the semi-random setting (but may have substantially slower rates than their fast counterparts).
In the noiseless observations case, this is clear because the additional rows of A are simply additional
constraints that are added to the standard ℓ1 minimization convex program.

In the noisy case, let the target error be θ =

ξ∗k2,(m). We then solve the modiﬁed problem

k

min

x
k1
k
subject to

Ax

k

b

−

k2,(m) ≤

θ.

Note that the above is a convex program and thus can be solved in polynomial time by e.g. cutting
plane methods [GLS88]. Also, note that x⋆ is indeed feasible for the second constraint. Now for
the solution

x that we obtain, we must have

x⋆

x

b

A(x⋆
k

−

k

k1 ≤ k
x)
b

k2,(m) ≤

k1 and
2θ.

Let G be the set of m randomly generated rows of A under our semi-random adversarial model.
The previous two conditions imply

b

•

•

x⋆
x
k
−
G(x⋆
k
b

k1 ≤
x)
−

2√s

x⋆
k
2θ

k2 ≤

x

k2

−

b

which now by restricted strong convexity of G (see [ANW10]) implies that
We can furthermore round
worsens by a factor of 2 for x′ (see Lemma 3 for this argument).
B Deferred proofs

√m ).
x to s-sparse to obtain the sparse vector x′, and the above bound only

k2 = O( θ

x⋆

−

x

b

b

b

k

Lemma 4. Let δ > 0 and θ

0. For any vector γ

≥

∈

Rd, we can solve the optimization problem

to additive accuracy δ in time

min
p
k2≤

k

θ

sqmaxµ(γ

p)

−

O

d log2

2
γ
2
k
k
µ√δ !!

.

Proof. Let
all j

Rd be the set of p such that p has the same sign as γ entrywise and
γj|
pj| ≤ |
|
[d]. By symmetry of the sqmax and the ℓ2 norm under negation, the optimal p lies in
P

P ⊂

∈

for
.

Next we claim that the function smaxµ(γ

p) is 2

γ

k2-Lipschitz in the ℓ2 norm as a function

k

−

. To see this, the gradient is directly computable as

of p, over

P

2(p

γ)

◦

−

x where x

∈

∆d with xi =

j

P
37

pi]2/µ2)

exp([γi −
[n] exp([γj −

∈

pj]2/µ2)

for all i

[n]

∈

 
 
denotes entrywise multiplication. Thus, the ℓ2 norm of the derivative is bounded by 2

where
over
optimal, which implies by Lipschitzness that the function value is within additive δ of optimal.

◦
. In the remainder of the proof, we show how to ﬁnd p
P

which has ℓ2 error

∈ P

γ
k2
k
to the

k2

δ
γ

2

k

Next, since 0

∈ P

, we may assume without loss of generality that

θ >

δ
γ

k

2

.

k2

(33)

else we may just output 0, which achieves optimality gap at most 2
k
Now, by monotonicity of ln it suﬃces to approximately minimize

γ

k2 θ.

[γ

p]2
j

−
µ2

.

!

exp

[d]
Xj
∈

The sum above is always at least d. First we check if
can set p so that all entries of γ

θ + √δ. If this is true then clearly we
p have magnitude at most √δ. This gives a solution such that

k2 ≤

γ
k

−

sqmaxµ(γ

p)

−

≤

µ2 log

d exp

(cid:18)

(cid:18)

δ
µ2

(cid:19)(cid:19)

= µ2 log d + δ

and since the value of sqmax is always at least µ2 log d, this solution is optimal up to additive error
θ + √δ in the remainder of the proof. We also assume all entries
δ. Thus, we can assume
of γ are nonzero since if an entry of γ is 0 then the corresponding entry of p should also be 0.
Finally by symmetry of the problem under negation we will assume all entries of γ are positive in
the remainder of the proof, such that each entry of p is also positive.

k2 ≥

γ
k

γ

that
some scalar ζ and all j,

θ + √δ, the optimal solution must have

By monotonicity of sqmax in each coordinate (as long as signs are preserved) and the assumption
k2 = θ. By using Lagrange multipliers, for
[γ

k2 ≥

p
k

k

pj = exp(ζ)

[γ

p]j exp

·

−

p]2
j

−
µ2

.

!

(34)

For the optimal ζ by taking ℓ2 norms of the quantity above, we have

θ =

p

k2 = ζ

k

γ

k

−

p

k2 ·

C for some C

0, exp

∈ "

2
γ
2
k
k
µ2

.

!#

Hence taking logarithms of both sides and using both the bounds (33) and
optimum, which follows from the previous discussion, we obtain

γ

k

−

p

k2 ≥

√δ at the

log

θ

γ

k

p

k2 −

−

ζ

0, k

∈ "

2
γ
2
k
µ2

=

⇒

ζ

#

∈ "−

2
γ
2
k
k
µ2 −

log

2

2
γ
2
k
k
δ !

, log

γ
k2
k
√δ (cid:19)#

.

(cid:18)

We next show how to compute p to high accuracy given a guess on ζ. Observe that if γj > 0, then
the right-hand side of (34) is decreasing in pj and hence by the intermediate value theorem, there
is a unique solution strictly between 0 and γj for any ζ. Also, note that the location of this solution
increases with ζ. Let p(ζ) be the solution obtained by exactly solving (34) for some given ζ. We
have shown for all ζ that 0

k2 for all ζ.
γ
β ). To see this, ﬁx
k2
some ζ, µ, and γj, and consider solving (34) for the ﬁxed point pj. We can discretize [0, γj] into

For a ﬁxed ζ, we claim we can estimate p(ζ) to ℓ2 error β in time O(d log k

γj entrywise and hence

[p(ζ)]j ≤

γ
k2 ≤ k

p(ζ)

≤

k

38

 
 
 
 
k

and perform a binary search. The right-hand side is decreasing in pj and

intervals of length γj β
γ
k2
the left-hand side is increasing so the binary search yields some interval of length γjβ
containing
γ
k2
the ﬁxed point pj via the intermediate value theorem. The resulting ℓ2 error along all coordinates is
then β. We also round this approximate p(ζ) entrywise down in the above search to form a vector
˜p(ζ, β) such that ˜p(ζ, β)
β. We use this notation and it
is well-deﬁned as the search is deterministic.

p(ζ) entrywise and

˜p(ζ, β)
k

k2 ≤

p(ζ)

−

≤

k

In the remainder of the proof we choose the constants

α :=

δ2
γ

k

192

, β := min

4
2

k

δ2
γ

k

192

δ
γ

k

,

4

3
2

k

.

k2 !

γ
γ
γ
]
We deﬁne ˜p(ζ) := ˜p(ζ, β) for short as β will be ﬁxed. Discretize the range [
k2
k
k
k
µ2 −
δ
√δ
ζ ⋆ < ζ + α. Because
into a grid of uniform intervals of length α. Consider the ζ such that ζ
p(ζ ⋆) is entrywise larger than p(ζ) and hence the logarithmic term on the right-hand side of (34)
is smaller for p(ζ ⋆) than p(ζ), we have

log 2
k

, log k

−

≤

2
2

2
2

Moreover the optimal p(ζ ⋆) has ℓ2 norm θ, so

[p(ζ)]j ≤

[p (ζ ⋆)]j ≤
ζ
|

−

exp (α) [p (ζ)]j .

ζ ⋆

| ≤

α and exp(α)

1

−

≤

2α imply

p(ζ)

k

−

p(ζ ⋆)

k2 ≤

2α

k

p(ζ ⋆)

k2 ≤

2α

γ

k

k2 ≤

∆ :=

96

δ2
γ

k

.

3
2
k

θ
Consider the algorithm which returns the ζalg on the search grid which minimizes
|
(we will discuss computational issues at the end of the proof). As we have argued above, there is
a choice which yields

∆, θ + ∆] and hence

˜p(ζalg)

k2 −

|k

[θ

p(ζ)
k

k2 ∈

−
˜p(ζalg)
k

k2 ∈

∆

[θ

−

−

β, θ + ∆ + β] .

We next claim that

p(ζalg)
k

−

p(ζ ⋆)

k2 ≤

4

.

δ
γ

k

k2

Suppose (36) is false and ζalg > ζ ⋆. Then letting u := p(ζalg) and v := p(ζ ⋆), note that u, v, and
u

v are all entrywise nonnegative and hence

−

u
k
k

2
v
2 ≥ k

k

2
2 +

Hence, we have by

x2 + y2

≥

2ui(ui −

Xi
[n]
∈
x + y2

3x for 0

vi) + (ui −

vi)2 >

2
2 +

v
k

k

2

.

δ
γ

k

4

(cid:18)

k2 (cid:19)

y

≤

≤

x, (33), and θ

γ

k2,

≤ k

p

p(ζalg)
k

k2 =

2

4
k
3

δ
γ
k2
(cid:17)
v
k2 ≥

δ2
γ

k2 >
u
k

v
k

k2 +

θ +

θ + ∆ + 2β.

(cid:16)

3
2 ≥
k
k
k2 > θ + ∆ + β and hence we reach a contradiction with (35).

48

k

k

Similarly, suppose (36) is false and ζalg < ζ ⋆. Then for the same deﬁnitions of u, v, and using

So, by triangle inequality

˜p(ζalg)

the inequality

x2

y2

x

−

≤

−

y2
3x for 0

y

≤

≤

x, we conclude

p
2
2 >

k

v
k

2
u
2 +
k

k

2

δ
γ

k

4

(cid:18)

k2 (cid:19)

=

⇒ k

u
k2 ≤ sk

2

v

2
2 −
k

δ
γ

k

4

(cid:18)

k2 (cid:19)

< θ

∆

−

−

2β.

39

(35)

(36)

 
So we reach a contradiction with (35) in this case as well.

In conclusion, (36) is true and we obtain by triangle inequality the desired

˜p(ζalg)
k

−

p(ζ ⋆)

k2 ≤

4

+ β√d

≤

2

δ
γ

k2

δ
γ

.

k2

k
The complexity of the algorithm is bottlenecked by the cost of ﬁnding ˜p(ζalg). For each ζ on the grid
γ
the cost of evaluating ˜p(ζ) induces a multiplicative d log( k
δ ) overhead. The cost of performing
k
2
γ
2
) overhead; note that a binary search
k
µ√δ
k2 is monotonic by our consistent choice of rounding down, and hence

the binary search on the ζ grid is a multiplicative log( k
suﬃces because

k

2
2

˜p(ζalg)
k
is unimodal.

˜p(ζalg)

k2 −

θ

|

| k

40

