9
1
0
2

c
e
D
4
2

]
L
M

.
t
a
t
s
[

1
v
4
5
5
1
1
.
2
1
9
1
:
v
i
X
r
a

Composable Effects for Flexible and Accelerated
Probabilistic Programming in NumPyro

Du Phan ∗
phandu@postech.ac.kr

Neeraj Pradhan ∗
npradhan@uber.com
Uber AI

Martin Jankowiak
jankowiak@uber.com

Abstract

NumPyro is a lightweight library that provides an alternate NumPy backend to
the Pyro probabilistic programming language with the same modeling interface,
language primitives and effect handling abstractions. Effect handlers allow Pyro’s
modeling API to be extended to NumPyro despite its being built atop a funda-
mentally different JAX-based functional backend. In this work, we demonstrate
the power of composing Pyro’s effect handlers with the program transformations
that enable hardware acceleration, automatic differentiation, and vectorization in
JAX. In particular, NumPyro provides an iterative formulation of the No-U-Turn
Sampler (NUTS) that can be end-to-end JIT compiled, yielding an implementation
that is much faster than existing alternatives in both the small and large dataset
regimes.

1

Introduction

Many probabilistic programming languages (PPLs) [1] are embedded as a DSL within a host language.
The advantage of embedding is availability of host language infrastructure along with frameworks for
automatic differentiation and hardware acceleration. Within the Python community, some examples
of embedded PPLs include Pyro [2] and ProbTorch [3] based on PyTorch [4], TensorFlow Probability
[5] and Edward2 [6] based on TensorFlow, and PyMC3 [7] based on Theano. NumPyro is a package
for probabilistic programming built atop JAX [8, 9], which is a high-level tracing library for program
transformations (e.g. automatic differentiation, vectorization and JIT compilation) of Python and
NumPy functions. Thus NumPyro enables users to write probabilistic programs using familiar
NumPy arrays and operations.

NumPyro is built around the same effect handling abstraction as Pyro. Effect handlers provide a way
to inject effectful computation into primitive statements in a probabilistic program, e.g. recording
the random choices made in an execution trace. In NumPyro these effects can be easily composed
with the JAX tracer that operates at the level of NumPy operations and its own set of primitives
for control ﬂow. This allows us to expose a modeling language that is the same as in Pyro. Under
the hood, inference algorithms can use effect handlers to inspect and modify program behavior and
freely compose with JAX transformations to speed up critical subroutines via parallelization and
JIT compilation. As an example (see Sec. 3.1), we implement an iterative version of the No-U-Turn
Sampler that can leverage JAX’s jit transformation for end-to-end compilation and optimization by
the XLA compiler [10].

∗Equal contribution

Preprint.

 
 
 
 
 
 
Primitives
Affected
Description

Usage

seed

trace

condition

sample

sample, param

sample

Seeds fn with a PRNGKey.
Every call to sample inside
fn results in splitting of
PRNGKey to generate a fresh
seed for subsequent calls.
seed(fn, rng_key)(...)

Records the input, output,
and function calls inside
of sample, param
statements in fn.

Conditions unobserved
sample sites in fn to
values in param.

trace(fn).get_trace(...)

condition(fn, param)(...)

Examples of effect handlers: the primitive statements affected by each handler, the effect added, and
usage w.r.t. the original function fn with its arguments denoted by ellipsis.

2 Support for Pyro’s Modeling Interface

NumPyro retains the same language primitives and modeling and inference interface as in Pyro.2 In
particular, NumPyro supports sample and param statements that allow users to designate random
variables and learnable parameters, respectively. It also has effect handlers like trace, replay and
condition to provide nonstandard interpretations to these statements. Table 1 lists some commonly
used effect handlers.

Effect handlers have emerged as a composable abstraction for program transformations in PPLs
[2, 11, 12]. We have found the effect handling abstraction to be particularly useful in designing a
common interface to probabilistic programming, despite fundamental differences in the underlying
backend. While PyTorch tries to accommodate much of Python’s dynamism and facilities for object-
oriented programming with mutable objects (e.g. PyTorch optimizers update parameters in-place),
JAX encourages a functional style of programming as required by the tracer. As an example, unlike
PyTorch, JAX uses a functional pseudo-random number generator [13], which mandates passing an
explicit random number generator key (PRNGKey) to distribution samplers. In practice, NumPyro
inference algorithms take a single PRNGKey from the user that is split to generate new keys when
passed to downstream functions. This does not result, however, in any change to probabilistic
programs formulated in NumPyro, as this splitting mechanism is abstracted into a seed handler that
operates on sample statements (see Table 1).

3 Leveraging JAX Transformations in Inference Subroutines

JAX [9] is a Python library that provides a high-level tracer for implementing transformations of
programs in Python and NumPy. Currently, three main transformations are available: i) automatic
differentiation (grad); ii) JIT compilation (jit) to multiple backends using XLA; and iii) automatic
vectorization (vmap). Frostig et al. [9] note that ML workloads are composed of many pure-and-
statically-composed (PSC) subroutines that are good candidates for acceleration. This is also true
of the inference subroutines that lie at the core of NumPyro. While NumPyro’s frontend—i.e. its
modeling and inference API—is close to Pyro, we have taken care to ensure that the core inference
algorithms and utilities are purely functional so that we can make extensive use of JAX transformations
like jit and vmap. This allows us to implement highly optimized and parallelizable subroutines. We
provide two such examples: i) composing jit and grad to implement an iterative version of the
NUTS sampler that can be end-to-end JIT compiled by JAX; and ii) composing vmap with effect
handlers like trace or condition to implement vectorized subroutines.

3.1

Iterative No-U-Turn Sampler (NUTS)

The No-U-Turn Sampler (NUTS) [14] is an extension of the Hamiltonian Monte Carlo (HMC)
algorithm [15, 16], which can efﬁciently sample from high-dimensional continuous probability
distributions. NUTS adaptively sets the trajectory length parameter in HMC, which along with the
adaptation of the step size and mass matrix parameters, ensures that HMC runs efﬁciently on a variety

2A generic API for modeling and inference for dispatch to different Pyro backends can be found at https:

//github.com/pyro-ppl/pyro-api

2

of models without extensive hand-tuning. This provides a highly attractive black-box inference
algorithm for PPLs to have in their toolkit.

To JIT compile NUTS sampling, we need to jit transform a key component of the algorithm, namely
the BuildTree subroutine that recursively builds an implicit balanced binary tree by running the
LeapFrog integrator (Appendix A). While this can be written as a PSC subroutine, tracing it is hard
for two reasons. First, the form of the LeapFrog integrator requires us to JIT through a gradient
computation. JAX can handle this, since transformations like jit and grad are composable.3
Second—and more problematically—the complex control ﬂow of the recursive formulation cannot
be traced for JIT compilation in JAX.4
An alternative would be to JIT compile a single LeapFrog step or the potential energy function.5
However, drawing a single sample involves many LeapFrog steps, and the overhead in terms
of Python function dispatch calls is signiﬁcant. In addition, this approach signiﬁcantly reduces
opportunities for operator fusion in XLA compilation. To overcome these limitations, we propose
an iterative version of the NUTS algorithm that can be fully JIT compiled.
In particular, this
involves converting the BuildTree procedure into an iterative procedure, paving the way for a
NUTS implementation that can take full advantage of XLA acceleration. As we demonstrate in
benchmarking experiments in Sec. 4, the result is an algorithm that is much faster than existing
implementations. More details on the algorithm are available in Appendix A.

3.2 Vectorizing Subroutines with vmap

Many utilities and subroutines for inference, e.g. model prediction, Monte Carlo estimation, or
running MCMC chains, can be batched to make use of SIMD vectorization. In many frameworks
this kind of batching requires laborious manual threading and/or signiﬁcant cognitive overhead in
managing explicit batch dimensions. JAX provides a vectorizing map (vmap) transformation that
makes it easy to represent batched computations as mapping over function arguments along an
outermost axis. This requires no changes to the underlying code but maintains the efﬁciency of
manual batching.

Since JAX transformations are fully composable with Pyro’s effect handlers like seed, trace, and
condition, and since the latter are implemented within the Python runtime and thus traceable, vmap
becomes very powerful. As an example, Fig. 1 shows how we can use vmap to batch three common
computations: i) sampling from the prior; ii) sampling from the posterior predictive distribution;
iii) and computing log-likelihoods. Note that without vmap we would need to explicitly handle
an additional batch dimension within the logistic_regression model and the utility functions
in Fig. 1b, which is particularly cumbersome for more involved models. As a ﬁnal example, in
Stochastic Variational Inference (SVI) [17], we optimize a loss function that is a Monte Carlo estimate
of the Evidence Lower Bound (ELBO). This requires running the model as well as the inference
network multiple times, all of which can be elegantly parallelized using vmap (see Appendix D).

4 Experiments

We compare the performance of NumPyro’s NUTS implementation with that of other frameworks
(Stan [18, 19] and Pyro) in both the small and large data regimes. Recall that NumPyro’s NUTS
implementation is end-to-end JIT compiled, while in Pyro only the potential energy computation
is compiled. We use three benchmark models: i) a Hidden Markov Model (HMM) on a small
synthetic dataset; ii) logistic regression on the Forest CoverType dataset [20]; and iii) a sparse
kernel interaction model (SKIM) [21] on synthetic datasets with varying dimensionalities. Refer to
Appendix C for details on the benchmarking experiments.

Hidden Markov Model Since we use a small dataset for this experiment, we expect poor per-
formance on the GPU; consequently we limit ourselves to a CPU-only comparison. Note that,
although the dataset is small, the potential energy computation involves a loop that can be expensive
to differentiate through. From Table 2a, we see that for the HMM, NumPyro is around 340X faster

3Note, however, that for example PyTorch’s tracing JIT does not allow for this.
4This obstacle was also noted in Tran et al. [6].
5This is the approach adopted by the NUTS implementation in Pyro.

3

1 def logistic_regression(x, y=None):
2
3
4
5

ndims = np.shape(x)[-1]
m = sample('m', Normal(0., np.ones(ndims)))
b = sample('b', Normal(0., 1.))
return sample('y', Bernoulli(logits=x @ m +

b), obs=y)

conditioned_model = condition(logistic_regression, param)
return seed(conditioned_model, rng_key)(*args)

1 def predict_fn(rng_key, param, *args):
2
3
4
5 def ll_fn(rng_key, param, *args):
f_traced = trace(predict_fn)
6
obs = f_traced.get_trace(rng_key, param, *args)['y']
7
return np.sum(obs_node['fn'].log_prob(obs['value']))
8

(a) A model for logistic regression in NumPyro

(b) Utilities for prediction and log-likelihood computation using
seed, trace and condition handlers

1 # x -> input dataset
2 # samples -> a dict of samples from the posterior distribution
3 # rng_keys.. -> batch of PRNGKeys
4
5 prior_predictive = vmap(lambda rng_key: seed(logistic_regression, rng_key)(x))(rng_keys_sim)
6 posterior_predictive = vmap(lambda rng_key, param: predict_fn(rng_key, param, x))(rng_keys_pred, samples)
7 log_likelihood = vmap(lambda rng_key, param: ll_fn(rng_key, param, x, y))(rng_keys_pred, samples)
8 exp_log_likelihood = logsumexp(log_likelihood) - np.log(num_samples)

(c) Vectorized sampling from the prior and posterior predictive with log-likelihood computation using vmap. A
complete code listing (including inference) is provided in Appendix B.

Figure 1: A simple logistic regression model. The modeling language is the same as in Pyro.

Framework

HMM6

COVTYPE

Stan (64-bit CPU)
Pyro (32-bit CPU)
Pyro (GPU)

NumPyro (32-bit CPU)
NumPyro (64-bit CPU)
NumPyro (GPU)

0.53
30.51
-

0.09
0.15
-

135.94
32.76
3.36

30.11
71.18
1.46

(a) Time (ms) per leapfrog step in different frame-
works.

(b) Time (ms) per effective sample for SKIM as
the dimensionality of the dataset (p) is varied.

Figure 2: Empirical evaluation of time taken by NumPyro’s Iterative NUTS with respect to other
frameworks.

than Pyro and 6X faster than Stan. The iterative procedure in Algorithm 2 introduces insigniﬁcant
overhead, and the end-to-end compilation allows XLA to output highly optimized code.

Logistic Regression For this dataset, which contains more than half a million datapoints, GPU
acceleration signiﬁcantly outperforms the CPU, as expected. The time spent in computing gradients
in the LeapFrog integrator exceeds the time spent building the tree or computing the terminating
condition. Since the bottleneck primarily lies in large tensor operations, we expect the difference
between the various GPU implementations to be narrower. Nevertheless, on this problem NumPyro
is about 2X faster than Pyro.

Sparse Kernel Interaction Model SKIM is Bayesian model for sparse regression that can be
used to discover pairwise interactions in high dimensional data. Since the sparsity-inducing prior
introduces a latent variable for each of the p input dimensions, this represents a difﬁcult inference
problem when p is large. Fig. 2b shows how the time per effective sample using NUTS scales with
the dimensionality of the dataset for Stan and NumPyro. We observe that NumPyro has consistently
lower overhead as compared to Stan. NumPyro offers the ﬂexibility to run inference in single or
double precision as well as on different backends such as CPU, GPU, or TPU. While inference with

6 By conducting 5 runs with different random seeds, 1000 warmup steps and 1000 samples for each run, the
average effective sample size for Stan, NumPyro 32-bit, and NumPyro 64-bit are 652, 556, and 788 respectively.

4

101102dimension (p)101102103time per effective sample (ms)Stan (64-bit CPU)NumPyro (64-bit CPU)NumPyro (32-bit CPU)NumPyro (GPU)double precision yields a higher effective sample size on average, it is not enough to compensate
for the higher time taken to run inference, and hence the time per effective sample is lower for
single precision. This model also particularly beneﬁts from GPU acceleration, resulting in a major
improvement in execution time when compared to the CPU backend.

5 Summary

We describe NumPyro, a package for probabilistic programming using Python and NumPy that
uses JAX transformations under the hood for hardware acceleration, automatic differentiation, and
vectorization. NumPyro has a functional core, where inference subroutines are pure-and-statically
composed functions that can be traced by JAX for parallelization and JIT compilation. These
subroutines also make use of effect handlers to inspect and transform probabilistic programs. Effect
handlers operate on core language primitives within the Python runtime, are transparent to the JAX
tracer, and are, therefore, fully composable with JAX’s transformations. This composability allows
us to offer the same modeling language as Pyro, and at the same time leverage JAX tranformations to
parallelize and JIT compile inference subroutines for signiﬁcant speed ups. In particular we show
that the judicious application of these program transformations allows us to implement an iterative
version of the NUTS algorithm that offers strong performance on both the CPU (for small models)
and the GPU (for larger models).

6 Acknowledgments

We would like to thank Noah Goodman for feedback, and the JAX development team—in particular
Matthew Johnson and Peter Hawkins—for their invaluable help with many JAX issues and feature
requests.

7 References

[1] Jan-Willem van de Meent, Brooks Paige, Hongseok Yang, and Frank Wood. An introduction to probabilistic

programming. arXiv preprint arXiv:1809.10756, 2018.

[2] Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos,
Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep universal probabilistic
programming. Journal of Machine Learning Research, 20(28):1–6, 2019. URL http://jmlr.org/
papers/v20/18-403.html.

[3] Siddharth Narayanaswamy, T Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah Goodman,
Pushmeet Kohli, Frank Wood, and Philip Torr. Learning disentangled representations with semi-supervised
deep generative models. In Advances in Neural Information Processing Systems, pages 5925–5935, 2017.

[4] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming
Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.

[5] Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian
Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorﬂow distributions. arXiv preprint
arXiv:1711.10604, 2017.

[6] Dustin Tran, Matthew W Hoffman, Dave Moore, Christopher Suter, Srinivas Vasudevan, and Alexey
Radul. Simple, distributed, and accelerated probabilistic programming. In Advances in Neural Information
Processing Systems, pages 7598–7609, 2018.

[7] John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. Probabilistic programming in python
using PyMC3. PeerJ Computer Science, 2:e55, apr 2016. doi: 10.7717/peerj-cs.55. URL https:
//doi.org/10.7717/peerj-cs.55.

[8] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.

[9] Roy Frostig, Matthew Johnson, and Chris Leary. Compiling machine learning programs via high-level

tracing. 2018. URL http://www.sysml.cc/doc/2018/146.pdf.

5

[10] XLA: Optimizing Compiler for Machine Learning. https://www.tensorflow.org/xla/.

[11] Dave Moore and Maria I. Gorinova. Effect handling for composable program transformations in edward2.

CoRR, abs/1811.06150, 2018. URL http://arxiv.org/abs/1811.06150.

[12] Gordon Plotkin and Matija Pretnar. Handlers of algebraic effects. In Giuseppe Castagna, editor, Program-
ming Languages and Systems, pages 80–94, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN
978-3-642-00590-9.

[13] The JAX Team. JAX PRNG Design. https://github.com/google/jax/blob/master/design_

notes/prng.md, 2019.

[14] Matthew D. Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths in
hamiltonian monte carlo. Journal of Machine Learning Research, 15:1593–1623, 2014. URL http:
//jmlr.org/papers/v15/hoffman14a.html.

[15] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics

letters B, 195(2):216–222, 1987.

[16] Radford Neal. MCMC Using Hamiltonian Dynamics. CRC Press, May 2011. doi: 10.1201/b10905-6.

URL http://dx.doi.org/10.1201/b10905-6.

[17] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The

Journal of Machine Learning Research, 14(1):1303–1347, 2013.

[18] Bob Carpenter, Andrew Gelman, Matthew D Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt,
Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell. Stan: A probabilistic programming language.
Journal of statistical software, 76(1), 2017.

[19] Allen Riddell, Ari Hartikainen, Daniel Lee, riddell stan, Marco Inacio, Daniel Chen, Kenneth C. Arnold,
Dougal J. Sutherland, Aki Vehtari, Shinya SUZUKI, Takahiro Kubo, Todd Small, Tobias Erhardt, Stephen
Hoover, Stephan Hoyer, Richard C Gerkin, Joerg Rings, Jackie, J. J. Ramsey, Aaron Darling, seantalts,
Skipper Seabold, Max Shron, Liam Brannigan, Kyle Foreman, Juan Manuel Veron, Jeremy Salwen, Dave
H, and Alexander Rudiuk. stan-dev/pystan: v2.18.0.0, October 2018. URL https://doi.org/10.5281/
zenodo.1456206.

[20] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.

edu/ml.

[21] Raj Agrawal, Brian Trippe, Jonathan Huggins, and Tamara Broderick. The kernel interaction trick: Fast
Bayesian discovery of pairwise interactions in high dimensions. In Kamalika Chaudhuri and Ruslan
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pages 141–150, Long Beach, California, USA, 09–15 Jun
2019. PMLR. URL http://proceedings.mlr.press/v97/agrawal19a.html.

[22] Stan Development Team. Stan Modeling Language User’s Guide and Reference Manual, Version 2.18.0.

2018. URL http://mc-stan.org.

6

Figure 3: A graphical representation of how binary trees are constructed in ITERATIVEBUILDTREE.
The orange node is the leaf generated at the current step. Blue nodes are the leaves stored in memory
for the purpose of checking the U-Turn condition. White nodes are past leaves that have been removed
from memory. Dashed white nodes have not been generated yet. Thick black lines link the left and
right leaves of subtrees where we need to check the U-Turn condition.

A Iterative NUTS - Algorithm Details

Computing a trajectory in NUTS involves a doubling procedure where at each iteration we run the LeapFrog
integrator for twice the number of steps taken in the previous iteration, with the direction (forward or reverse)
chosen randomly. This has the effect of building an implicit balanced binary tree. The doubling process is
terminated when a subtrajectory from the leftmost to the rightmost node of any balanced subtree begins to double
back on itself.

Existing NUTS implementations use a recursive tree building formulation, the BuildTree subroutine, to double
the trajectory length (see Hoffman and Gelman [14, Algorithm 6]). A simpliﬁed version of this subroutine is
presented in Algorithm 1. To build a tree at depth d, it builds two subtrees at depth d − 1 and combines them.
This recursive procedure also takes care to ensure that memory usage scales as O(log N ) (where N = 2d) rather
than O(N ) by only storing O(1) data per subtree. This is important because storing all N momentum-position
pairs might be prohibitive for large models. The key to JIT compiling NUTS sampling is the ability to convert
the recursive BuildTree subroutine into an iterative implementation.

The iterative version of BuildTree takes an initial node (position-momentum pair) z−1 and a tree depth d
argument, and runs the LeapFrog integrator for N = 2d steps. We will be using 0-based indexing in the
following discussion.

Checking the U-Turn Condition For node zn, we need to check the U-Turn condition with respect to
the leftmost nodes of any binary subtree for which zn is the rightmost node. Let us denote the indices for
these candidate nodes by C(n), and the binary representation of n by b(n). Indices in C(n) have the same
binary representation as b(n) except that trailing contiguous 1s in b(n) are progressively masked by 0; e.g.
for n = 11, b(11) = (1011)2, and the set of candidate nodes for checking the U-Turn condition are indexed
by C(11) = {(1010)2, (1000)2} = {8, 10}. Note that this implies that we only need to check the U-Turn
condition at odd-numbered nodes against a subset of previous even-numbered nodes.
This allows us to iteratively build the binary tree by running the LeapFrog integrator for 2d steps, and terminating
early if the U-Turn condition stated above is satisﬁed. However, in a naive implementation we would still need
O(N ) memory, since we would need to store the position-momentum pairs at each step of the integrator, which
would be an unacceptable regression from the O(log N ) memory requirement of the recursive algorithm.

Memory Efﬁciency To tackle the issue of memory efﬁciency, we will use an array S to store only even
numbered nodes zk at index i = BitCount(k). As we iteratively build the tree, nodes in S may be overwritten
so that at step n, index i stores the largest even node zk such that k < n and BitCount(k) = i. Note that the
maximum size of the array S is d because the largest bit count of even numbers less than 2d is d − 1.

At an odd step n, the data for the candidate nodes indexed by C(n) must be present in S because the masking
procedure ensures that these candidates are the largest even nodes less than n for their corresponding bit counts.
Figure 3 illustrates the iterative procedure at step n = 11, and full details of the algorithm are provided in
Algorithm 2.

7

index12340depth0123456789101112131415Algorithm 1 BUILDTREE
Input initial node z, tree depth d
if d = 0 then

z(cid:48) ← LEAPFROG(z)
return TREE(z(cid:48), z(cid:48), F alse)

else

TL ← BUILDTREE(z, d − 1)
if TL.turning then
return TL

else

z ← TL.right
TR ← BUILDTREE(z, d − 1)
zL ← TL.lef t
zR ← TR.right
if TR.turning then

turning ← T rue

else

turning ← ISUTURN(zL, zR)

return TREE(zL, zR, turning)

Algorithm 2 ITERATIVEBUILDTREE
Input initial node z, tree depth d
Initialize storage S[0], S[1], ..., S[d − 1]
for n ← 0 to 2d − 1 do
z ← LEAPFROG(z)
if n is even then

i ← BITCOUNT(n)
S[i] ← z

else

// gets the number of candidate nodes
l ← TRAILINGBIT(n)
imax ← BITCOUNT(n − 1)
imin ← imax − l + 1
for k ← imax to imin do

turning ← ISUTURN(S[k], z)
if turning then

return TREE(S[0], z, T rue)

return TREE(S[0], z, F alse)

Figure 4: Comparing the recursive (left) and iterative (right) versions of the tree building algorithm in
NUTS. Note that these are high-level speciﬁcations of the full algorithms and ignore details about
additional metadata in Tree such as the proposal candidate. Other details like the step size and
choosing between forward and reverse directions are also omitted, since they are not relevant to the
proposed changes.

8

B Code for Vectorized Sampling - Logistic Regression

conditioned_model = condition(logistic_regression, param)
return seed(conditioned_model, rng_key)(*args)

tr = trace(predict_fn).get_trace(rng_key, params, *args)
obs_node = tr['y']
return np.sum(obs_node['fn'].log_prob(obs_node['value']))

1 from jax import random, vmap
2 import jax.numpy as np
3 from jax.scipy.special import logsumexp
4
5 import numpyro
6 import numpyro.distributions as dist
7 from numpyro.handlers import condition, seed, trace
8 from numpyro.infer import MCMC, NUTS
9
10
11 def logistic_regression(x, y=None):
ndims = np.shape(x)[-1]
12
m = numpyro.sample('m', dist.Normal(0., np.ones(ndims)))
13
b = numpyro.sample('b', dist.Normal(0., 1.))
14
return numpyro.sample('y', dist.Bernoulli(logits=x @ m + b), obs=y)
15
16
17
18 def predict_fn(rng_key, param, *args):
19
20
21
22
23 def loglik_fn(rng_key, params, *args):
24
25
26
27
28
29 # Generate random data
30 true_coefs = np.array([1., 2., 3.])
31 x = random.normal(random.PRNGKey(0), (100, 3))
32 dim = 3
33 y = dist.Bernoulli(logits=x @ true_coefs).sample(random.PRNGKey(3))
34
35 # Run inference to generate samples from the posterior
36 num_warmup, num_samples = 500, 500
37 kernel = NUTS(model=logistic_regression)
38 mcmc = MCMC(kernel, num_warmup, num_samples)
39 mcmc.run(random.PRNGKey(1), x, y=y)
40 samples = mcmc.get_samples()
41
42 # Generate batch of PRNGKeys
43 rngs_sim = random.split(random.PRNGKey(2), num_samples)
44 rngs_pred = random.split(random.PRNGKey(3), num_samples)
45
46 # Prediction and log likelihood
47 prior_predictive = vmap(lambda rng_key: seed(logistic_regression, rng_key)(x))(rng_keys_sim)
48 posterior_predictive = vmap(lambda rng_key, param: predict_fn(rng_key, param, x))(rng_keys_pred, samples)
49 log_likelihood = vmap(lambda rng_key, param: loglik_fn(rng_key, param, x, y))(rng_keys_pred, samples)
50 expected_log_likelihood = logsumexp(log_likelihood) - np.log(num_samples)

Listing 1: Using vmap to vectorize three common inference subroutines: i) sampling from the prior;
ii) sampling from the posterior predictive distribution; and iii) computing log-likelihoods.

C Experimental Details

All experiments are conducted on a system using an AMD Ryzen Threadripper 1920X processor and an NVIDIA
GeForce RTX 2080 Ti graphics card. Framework versions: PyStan 2.19.1.1, Pyro 1.0 (with PyTorch 1.3.1),
NumPyro 0.2.3 (with JAX 0.1.53 and jaxlib 0.1.36). For each experiment, we conduct 5 runs with different
random seeds and report the average across 5 runs. Code used to benchmark all experiments can be found on the
benchmarks branch of the NumPyro GitHub repository.7

Hidden Markov Model To test performance in the small dataset regime, we use an HMM. Following [22,
Section 2.6], we construct a semi-supervised HMM model with 3-dimensional latent states and 10-dimensional
observations. Using ﬁxed transition and emission matrices, we sample 600 data points and treat the ﬁrst 100
latent states as observed. For benchmarking we take 1000 warmup steps and draw 1000 NUTS samples for both
Stan and NumPyro. Because Pyro’s NUTS implementation is extremely slow on this problem, we ﬁx the step
size to 0.1 and only draw 40 samples for each run.

7https://github.com/pyro-ppl/numpyro/tree/benchmarks-20191222/benchmarks

9

Logistic Regression We consider a logistic regression model on the Forest CoverType dataset, which
has 581, 012 datapoints and 54 features. The prior on the weights is a unit normal distribution. Following [6],
we normalize all features and transform the multi-class problem into a binary class problem by merging all the
classes except for the most frequent one. For benchmarking we ﬁx the step size in all frameworks to 0.0015 and
draw 40 samples. That step size value was obtained by running 150 warmup adaptation steps in NumPyro.

Sparse Kernel Interaction Model (SKIM) For each value of dimensionality p, we produce an artiﬁcial
dataset with N = 200 data points that contains 3 randomly selected pairwise interaction terms amongst the p
covariates. For each of the frameworks, we adapt the step size and mass matrix using 1000 warmup adaptation
steps and then compute the time per effective sample based on the next 1000 drawn samples averaged over 5
random runs.

D Vectorized Estimation of the Evidence Lower Bound (ELBO) in SVI

def __init__(self, num_particles):

1 from numpyro import optim
2 from numpyro.infer import ELBO, SVI
3
4 # model, guide defined externally for the inference problem
5
6 # vectorize elbo estimate over `num_particles`
7 class VectorizedELBO:
8
9
10
11
12
13
14
15
16 optimizer = optim.Adam(1e-3)
17 svi = SVI(model, guide, optimizer, VectorizedELBO(num_particles=100))

# args are arguments passed by SVI to `ELBO().loss`
def loss(self, rng_key, *args):

rng_keys = random.split(rng_key, self.num_particles)
return np.mean(vmap(lambda rng_: ELBO().loss(rng_, *args))(rng_keys))

self.num_particles = num_particles

Listing 2: Using vmap to vectorize the computation of the Evidence Lower Bound (ELBO) in SVI.

10

