REINFORCED OPTIMAL CONTROL

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

Abstract. Least squares Monte Carlo methods are a popular numerical approximation method
for solving stochastic control problems. Based on dynamic programming, their key feature is
the approximation of the conditional expectation of future rewards by linear least squares re-
gression. Hence, the choice of basis functions is crucial for the accuracy of the method. Earlier
work by some of us [Belomestny, Schoenmakers, Spokoiny, Zharkynbay. Commun. Math. Sci.,
18(1):109–121, 2020] proposes to reinforce the basis functions in the case of optimal stopping
problems by already computed value functions for later times, thereby considerably improving
the accuracy with limited additional computational cost. We extend the reinforced regression
method to a general class of stochastic control problems, while considerably improving the
method’s eﬃciency, as demonstrated by substantial numerical examples as well as theoretical
analysis.

1. Introduction

Stochastic control problems form an important class of stochastic optimization problems that
ﬁnd applications in a wide variety of ﬁelds, see [Pha09] for an overview. The general problem can
be formulated as follows: How should a decision-maker control a system with a stochastic com-
ponent to maximize the expected reward? In the theory of stochastic control, one distinguishes
between problems with continuous and discrete sets of possible control values. While the ﬁrst
class of control problems contains, for example, energy storage problems, the second one includes
stopping and multiple stopping problems. Furthermore one diﬀerentiates between discrete-time
and continuous-time optimal control problems. (Neither of these distinctions is fundamental: for
instance, many numerical methods will replace optimal control problems with a continuous set of
control values in continuous time by a surrogate problem with discrete control values in discrete
time. Moreover, many discrete optimal control problems may well be analyzed as continuous
ones, if the number of possible control values or time-steps is ﬁnite but very high).

The range of applications of stochastic control problems is very wide. Originally, optimal
stochastic continuous control problems were inspired by engineering problems in the continuous
control of a dynamic system in the presence of random noise, see [Åst12] and references therein.
In the last decades, problems in mathematical ﬁnance (portfolio optimization, options with
variable exercise possibilities) and economics inspired many new developments, see [BR11] for
some recent developments. Let us also mention a closely connected area of reinforcement learning
with plethora of applications in robotics, data science, and engineering, see [SB18].

As a canonical general approach for solving a discrete-time optimal control problem one may
consider all possible future evolutions of the process at each time that a control choice is to be
made, see [Åst12]. This method is well developed and may be eﬀective in some special cases but
for more general problems such as optimal control of diﬀusion in high dimensions, this approach

2020 Mathematics Subject Classiﬁcation. 91G20,93E24.
Key words and phrases. Reinforced regression, least squares Monte Carlo, stochastic optimal control.
C.B., P.H, P.P. J.S. and V.S. were supported by the MATH+ project AA4-2 Optimal control in energy markets
using rough analysis and deep networks. The authors are also thankful to the associate editor and anonymous
referees for there feedback and suggestions.

1

2
2
0
2

r
a

M
5
2

]

C
O
.
h
t
a
m

[

2
v
2
8
3
2
1
.
1
1
0
2
:
v
i
X
r
a

 
 
 
 
 
 
2

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

is impractical. In [BKS09] a generic Monte Carlo approach combined with linear regression was
proposed and studied, see also [BS18] for an overview. However, as an important disadvantage,
there may be not enough ﬂexibility when modeling highly non-linear behavior of optimal value
functions. For instance, a regression based on higher-degree polynomials or local polynomials
(splines) may contain too many parameters and, therefore, may over-ﬁt the Monte Carlo sample
or even prohibit parameter estimation because the number of parameters is too large. As an
alternative to the polynomial bases, nonlinear approximation structures (e.g., artiﬁcial neural
networks) can be used instead (see, e.g. [HE16],[BCJ19] and [BCJW21]).

In [BSSZ20] a Monte Carlo based reinforced regression approach is developed for building
sparse regression models at each backward step of the dynamic programming algorithm in the
case of optimal stopping problems. In a nutshell, the idea is to start with a generic set of basis
functions, which is systematically enlarged with highly problem-dependent additional functions.
The additional basis functions are constructed for the optimal stopping problem at hand with-
out using a ﬁxed predeﬁned ﬁnite dictionary. The new basis functions are learned during the
backward induction via incorporating information from the preceding backward induction step.
More speciﬁcally, the (computed, hence approximate) value function at time ti+1 is used as an
additional basis function at time ti. Thereby, basis functions highly speciﬁc to the problem at
hand are constructed in a completely automatic way. Indeed, the continuation function at time
ti can often be observed to be very close to the value function at time ti+1, especially when the
time-step ti+1 − ti is small – alluding to continuity in time of the solution to some continuous
time version of the optimal stopping problem. [BSSZ20] report that the reinforced basis leads to
increased precision over the starting set of basis functions, comparable to the standard regression
algorithm based on a substantially increased set of basis functions. This improvement is obtained
with a limited increase of the computational cost.

In this work, we carry over the approach of [BSSZ20] to a general class of discrete-time optimal
control problems including multiple stopping problems (thus allowing pricing of swing options)
and a gas storage problem. This generalization turns out to be rather challenging as the com-
plexity of using the previously constructed value function in regression basis at each step of the
backward procedure becomes prohibitive when applying the original approach of [BSSZ20]. We
overcome this computational bottleneck by introducing a novel version of the original reinforced
regression algorithm where one uses a hierarchy of ﬁxed time-depth approximation of the opti-
mal value function instead of a full-depth approximation employed in [BSSZ20]. As a result, we
regain eﬃciency and are able to improve upon the standard linear regression algorithm in terms
of achievable precision for a given computational budget.

More precisely, we construct a hierarchy v(i), i = 0, . . . , I, of (approximate) value functions
with depth I > 0. Here, v(0) denotes the value functions obtained from the classical Monte Carlo
regression algorithm. The higher levels v(i) are computed by regression based on a set of basis
functions reinforced by the value function v(i−1) one level lower. This way, the added computa-
tional cost incurred from reinforcing the basis can be further decreased with minimal sacriﬁces
of accuracy already for small values of I. In fact, we propose two versions of the algorithm. In
the ﬁrst version, the levels of the hierarchy of value functions are trained consecutively, allowing
for an adaptive choice of the depth I of the hierarchy. In the second version, all the levels are
trained concurrently, thereby improving the accuracy at each individual level. As a consequence,
I needs to be ﬁxed in advance and cannot be chosen adaptively in the second variant.

Outline of the paper. In Section 2 we describe a rather general setting for discrete stochastic
control problems which we are going to use in this paper. The setting is based on [GHW11]. We
recall the reinforced regression algorithm for optimal stopping problems by [BSSZ20] in detail in
Section 3. There we also motivate the hierarchical construction of the new reinforced regression

REINFORCED OPTIMAL CONTROL

3

algorithm as restricted to the optimal stopping problem. The full algorithm – including both
variants – is introduced in Section 4. A detailed analysis of computational costs is provided in
Section 5. The next Section 6 provides a detailed convergence analysis for the standard and
reinforced regression algorithms in the current setting. Extensive numerical examples including
optimal stopping problems, multiple stopping problems and a gas storage optimization problem
are provided in Section 7.

2. Setting

First, we present a proper setting for the construction and analysis of reinforced regression
algorithms. The setting will be largely based on [GHW11]. We will consider stochastic control
problems in discrete time with ﬁnite action sets. We note that extensions to continuous action
sets are certainly possible, but are left to future research.

We consider a ﬁltration Fj, j = 0, . . . , J, which is extended by F−1 := { ∅, Ω }, FJ+1 := FJ
for convenience. Let X be a Markov process with values in X adapted to (Fj)j=0,...,J . Note that
we assume that the dynamics of the underlying process X does not depend on the control.

At time 0 ≤ j ≤ J we are given a control Yj, which is Fj−1-measurable, and an Fj-measurable
cash-ﬂow Zj = Hj(a, Yj, Xj) for some deterministic, measurable function Hj, where a is an action
that we may choose at time j in some ﬁnite action space K. Note that cash-ﬂows may be positive
or negative. We assume that the control Yj takes values in a ﬁnite set L.
Remark 2.1. The assumption that actions a and controls y take values in ﬁnite sets K and L,
respectively, is a weaker assumption than it may seem at ﬁrst sight. Many important control
problems naturally fall into this class, see examples below. Even more importantly, it is a well-
known fact that many optimal control problems with genuinely continuous action and control
spaces have solutions of bang-bang type, i.e., all optimal controls consist of actions taken from
a ﬁnite set, usually at the boundaries of the (continuous) action sets. Hence, such control
problems can eﬀectively be reduced to control problems with ﬁnite actions sets. Extensions of
the reinforced regression algorithm to inﬁnite action spaces will be studied in future work.

For a given value of the control y ∈ L and a given value x of the underlying process Xj, we

are given a set of admissible actions

(2.1)

Kj(y, x) ⊂ K,

j = 0, . . . , J,

i.e., a is admissible iﬀ a ∈ Kj(x, y). Finally, if we apply a ∈ Kj(Yj, Xj), then the control is
updated by

(2.2)

Yj+1 := ϕj+1(a, Yj), ϕj+1 : K × L → L.
Suppose that the control and the underlying state process take values Yj and Xj at time 0 ≤
j ≤ J, respectively. For a := (aj, . . . , aJ ) ∈ KJ−j+1 and j ≤ (cid:96) ≤ J − 1, we deﬁne
(2.3)
noting that Y(cid:96)(a; j, Yj) only depends on aj, . . . , a(cid:96)−1. Additionally, we deﬁne Fj,J (K) to be the set
(cid:96)=j-adapted processes taking values in K indexed by j, . . . , J. Clearly, if A := (A(cid:96))J
of (F(cid:96))J
(cid:96)=j ∈
Fj,J (K) and Yj ∈ Fj−1, then the process Y·(A; j, Yj) is previsible. The set of admissible strategies
or admissible policies Aj is deﬁned as follows:

Y(cid:96)+1(a; j, Yj) := ϕ(cid:96)+1(a(cid:96), Y(cid:96)(a; j, Yj)),

Yj(a; j, Yj) := Yj,

(2.4)

Aj(Yj, X≥j) :=

(cid:110)

A = (A(cid:96))J

(cid:96)=j ∈ Fj,J (K)

(cid:12)
(cid:12)
(cid:12) A(cid:96) ∈ K(cid:96)(Y(cid:96)(A; j, Yj), X(cid:96)),

(cid:96) = j, . . . , J

(cid:111)
.

Now the central issue is the optimal control problem

(2.5)

Vj :=

sup

Ej

A=(A(cid:96))J

(cid:96)=j ∈Aj (Yj ,X≥j )





J
(cid:88)



H(cid:96)(A(cid:96), Y(cid:96)(A; j, Yj), X(cid:96))

 ,

(cid:96)=j

4

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

at a generic time 0 ≤ j ≤ J, where Ej denotes the conditional expectation w.r.t. Fj.

Taking advantage of the Markov property, we introduce the notation Aj(y, x) := Aj(y, X x
≥j),
where X j,x denotes the Markov process X conditioned on Xj = x, and is deﬁned for j ≤ (cid:96) ≤ J.
We may then deﬁne the value function as

(2.6)

v∗
j (y, x) :=

sup
(cid:96)=j ∈Aj (y,x)

A=(A(cid:96))J

E





J
(cid:88)

(cid:16)

H(cid:96)

A(cid:96), Y(cid:96)(A; j, y), X j,x

(cid:96)

(cid:17)



 ,

(cid:96)=j

which satisﬁes the dynamic programming principle:

(2.7)

v∗
j (y, x) = sup

(cid:16)

Hj(a, y, x) + E

(cid:104)

j+1(ϕj+1(a, y), X j,x
v∗

j+1)

(cid:105)(cid:17)

.

for j = 0, . . . , J (with v∗

a∈Kj (y,x)
J+1(y, x) := 0).

Let us now give a few examples for classical stopping and control problems which fall into the

above setup.

Example 2.2. For a single optimal stopping problem with payoﬀ gj ≥ 0 at time j, the set of
possible control values is L = { 0, 1 }, where a control state y denotes the number of remaining
exercise opportunities. The action a takes the value 1 if we stop at the current time and 0
otherwise. Hence, we have

Kj(y, x) = K(y) :=

(cid:40)

{ 0, 1 } ,
{ 0 } ,

y = 1,
y = 0,

implying that K = { 0, 1 }. The cash-ﬂow is deﬁned by

Hj(a, y, x) := a gj(x),
independent of the value of the control y. Finally, the update function of the control is deﬁned
by ϕj+1(a, y) := max(y − a, 0). Note that the value function v∗
j (0, ·) ≡ 0, and, hence, the optimal
stopping literature usually only considers (j, x) (cid:55)→ v∗
Example 2.3. Let us now suppose that we have a multiple stopping problem with L ∈ N
exercise rights. Again, the control state y signiﬁes the remaining exercise opportunities, leading
to L = { 0, 1, . . . , L }. The admissible action set is now deﬁned as

j (1, x).

Kj(y, x) = K(y) :=

(cid:40)

{ 0, 1 } ,
{ 0 } ,

y ≥ 1,
y = 0.

Again, K = { 0, 1 }. The cash-ﬂow Hj and the update function ϕj+1 are deﬁned as in Example 2.2.
Example 2.4. Consider a simple gas storage problem: given N ∈ N and ∆ = 1/N , we assume
that the volume of gas in a storage can only be increased and decreased by a fraction ∆ over
a given time increment. Let the control y denote the status (ﬁll level) of the storage at time
j. Hence, we deﬁne L := { 0, ∆, 2∆, . . . , 1 } . At time j, we may either sell ∆ (volume of gas;
a = −1), buy ∆ (a = +1) – at the current market price Xj – or do nothing (a = 0). Hence, the
admissible policy set is

Kj(y) :=






y = 0,

{ 0, 1 } ,
{ −1, 0, 1 } , ∆ ≤ y ≤ 1 − ∆,
{ −1, 0 } ,

y = 1,

with K = { −1, 0, 1 }, while the cash-ﬂow is given by

Hj(a, y, x) := −a∆x.

The update function in given by ϕj+1(a, y) := ((y + a∆) ∧ 1) ∨ 0.

REINFORCED OPTIMAL CONTROL

5

Remark 2.5. While we do not allow the actions to have an eﬀect on the dynamics of the state
process X, a large class of more general control problems could be incorporated by a simple
modiﬁcation of our setting. If we allow updates of the control variable y to depend on the state
x as well as on the previous control and the action, i.e., Yj+1 = ϕj(a, Yj, Xj), then our theoretical
analysis remains intact. However, it now becomes possible to control the dynamics of the state
process X, provided that the law of the controlled process remains absolutely continuous w.r.t. the
law of the original process (Xj)j=0,...,J . We refer to the discussion of the optimal liquidation
example in [GHW11, Section 2] for more details. Note, however, allowing Y to depend on X
in such a way might require us to use regression in (x, y) rather than just x for most practical
problems.

3. Reinforced regression for optimal stopping

In this section, we recall the standard regression algorithm as well as the reinforced regres-
sion algorithm introduced in [BSSZ20] for optimal stopping problems. We will point out the
drawbacks of the latter algorithm for more general control problems, and propose and motivate
several modiﬁcations. However, for the purpose of a clear illustration, we will restrict ourselves
in this section to the optimal stopping case.

Let us recall the optimal stopping setup from Example 2.2 and denote by v∗

j (x) the value
function at j ∈ {0, ..., J} evaluated in x ∈ Rd and y = 1. Further recall that the dynamic
programming principle is given by

v∗
j (x) = max(gj(x), c∗

j (x)), 0 ≤ j ≤ J − 1,

v∗
J (x) = gJ (x),

x ∈ Rd,

j (x) = Ej[v∗

where the continuation function is given by c∗
j+1)]. Fix a set of basis function
{ψ1, ..., ψK} with ψk : Rd → R, k = 1, . . . , K, and sample trajectories (X (m)
)0≤j≤J,1≤m≤M
from the underlying Markov chain, i.e., (X (m)
)0≤j≤J are i.i.d. samples from the distribution of
(Xj)0≤j≤J , m = 1, . . . , M . Then the regression method due to Tsitsiklis-van Roy [TVR01], which
we will refer to as the standard regression method, inductively constructs an approximation v =
(vj)j=0,...,J to the value function v∗ as follows: For j = J initialize vJ = gJ . For j ∈ {J − 1, ..., 0}
set

j+1(X j,x

j

j

(3.1)

vj(x) := max(gj(x), cj(x)),

cj(x) =

K
(cid:88)

k=1

γj,kψk(x),

where the regression coeﬃcients are given by the solution to the least squares problem

(3.2)

γj,1, ..., γj,K := arg min
γ1,...,γK

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vj+1(X (m)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

2

(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

.

The procedure is illustrated in Figure 1. Note that the costs of this algorithm are of the order
M · J · K 2 (see, e.g., [BSSZ20] or Section 5).

One problem of the standard regression algorithm is that its performance strongly depends on
the choice of basis functions. Indeed, while standard classes such as polynomials or splines usually
form the backbone of the construction of basis functions, practitioners usually add customized
basis functions, for instance the payoﬀ function gj and some functionals applied to it.

As a more systematic approach, the authors of [BSSZ20] proposed a reinforced regression
In this procedure the regression basis at each step of the backward induction is
algorithm.
reinforced with the approximate value function from the previous step of the induction. The

6

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

Figure 1. Illustration of standard regression approach due to Tsitsiklis-van
Roy [TVR01]. The solid arrows indicate the dependencies in the (feed forward)
evaluation of cj and vj in (3.1). The dashed arrows start from the regression
data vj+1 and symbolize the regression procedure (3.2).

approximate continuation function at j ∈ {0, ..., J − 1} is then given by

K
(cid:88)

cj(x) :=

γj,kψk(x) + γj,K+1vj+1(x),

k=1
where the regression coeﬃcient are the solution to the least squares problem

γj,1, ..., γj,K+1 := arg min
γ1,...,γK+1

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vj+1(X (m)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

) − γK+1vj+1(X (m)

j

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

.

Note that this procedure induces a recursion whenever an approximate value function is evalu-
ated: in order to evaluate vj(x) we need to evaluate cj(x), which in turn requires an evaluation
of vj+1(x) and so forth, until vJ (x) = gJ (x) terminates the recursion. Figure 2 illustrates this
procedure. The costs of the reinforced regression method are proportional to M ·J ·K 2 +M ·J 2 ·K
(see [BSSZ20]).

Figure 2. Illustration of the reinforced regression approach. Evaluation of vj
in the reinforced regression algorithm leads to a recursion with J − j steps.

Despite the increased computational cost compared to the standard regression algorithm with
the same set of basis functions ψ1, . . . , ψK, the reinforced regression algorithm can improve the
overall computational cost for a ﬁxed error tolerance drastically. As a rule of thumb, [BSSZ20] re-
port that the reinforced regression algorithm with a standard basis consisting of polynomials of a
given degree leads to similar accuracy as the standard regression algorithm based on polynomials
of one degree higher. In particular, the reinforced regression algorithm already outperforms the
standard regression algorithm for small dimensions d > 1, as long as the number J of time-steps
is not too large.

A direct generalization of the reinforced regression algorithm to more general control problems
is certainly possible. The main diﬀerence to the optimal stopping problem is that for ﬁxed time j
we have to choose from many potential candidates to reinforce with, namely any vj+1(y, ·), y ∈ L

v(i)j(x)gj(x)c(i)j(x)ψ·(x)γ(i)j,·j=J−4J−3J−2J−1J1v(i)j(x)gj(x)c(i)j(x)ψ·(x)γ(i)j,·j=J−4J−3J−2J−1J1REINFORCED OPTIMAL CONTROL

7

is a candidate. Additionally, the dynamic programming principle (2.7) now entails a possibly
non-trivial optimization problem in terms of the policy a. Especially the second point makes
the recursion at the heart of the reinforced regression algorithm untenable for general control
problems.

One solutions immediately comes to mind: If performing the recursion all the way to terminal
time J is too costly, why not truncate at a certain recursion depth? This idea is, in principle,
sound, and is the basis of the adaptations suggested below. However, some care is needed in the
implementation of this idea. Indeed, if “truncation” simply were to mean “replace the reinforcing
basis functions by 0 after a certain truncation step”, this would introduce a structural error in the
procedure, as regression coeﬃcients formerly computed in the presence of these basis functions
would suddenly be incorrect. Instead, we propose to compute a hierarchy of reinforced regression
solutions, corresponding to diﬀerent “cut-oﬀ depths” of the recursion. This way, we can make
sure that the coeﬃcients are always consistent, that is, an error as mentioned above can be
avoided. We introduce two versions, which both adhere to the same general idea, but diﬀer in
an important implementation detail.

The hierarchical reinforced regression algorithm A iteratively constructs approximations

(v(i))i=0,1,... to the true value function as follows: For i = 0 we construct (v(0)
)0≤j≤J using
the standard regression method described above. Then for any i ≥ 1, given that v(l) is already
constructed for 0 ≤ l ≤ i−1, deﬁne v(i) with the usual backwards induction, where the regression
basis at step j ∈ {J − 1, ..., 0} is reinforced with v(i−1)
. The approximate continuation function
of the ith iteration is given by

j

j

(3.3)

c(i)
j (x) :=

K
(cid:88)

k=1

j,kψk(x) + γ(i)
γ(i)

j,K+1v(i−1)

j+1 (x),

where the regression coeﬃcients are the solutions to the least squares problem

j,1, ..., γ(i)
γ(i)

j,K+1 := arg min
γ1,...,γK+1

M
(cid:88)

m=1

(cid:12)
(cid:12)
j+1(X (m)
v(i)
(cid:12)
(cid:12)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

) − γK+1v(i−1)

j+1 (X (m)

j

2

(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

.

J

The procedure may be stopped after a ﬁxed number of iterations, or using an adaptive criterion.
An illustration of the method can be found in Figure 3. Note that the recursion that is started
when evaluating v(i)
j+i(x) for
i ≤ J − j or in v(i−J−j)

j (x) always terminates after at most i steps in the evaluation of v(0)

(x) = gJ (x) for J − j ≤ i.

For a ﬁxed number of iterations i ∈ {0, ..., I} we can modify the structure of the previous
method so that the primary iteration is the backwards induction over j ∈ {J, J − 1, ..., 0} and
the secondary iteration is over i ∈ {0, ..., I}. In this case we can further modify the algorithm
by using v(I)
for all i ∈ {0, ..., I}. We
name the resulting algorithm the hierarchical reinforced regression algorithm B. The approximate
continuation function at step j and iteration i is then still given by (3.3) and the least squares
problem is given by

j+1 as the regression target for the continuation functions c(i)

j

j,1, ..., γ(i)
γ(i)

j,K+1 := arg min
γ1,...,γK+1

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

j+1(X (m)
v(I)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

) − γK+1v(i−1)

j+1 (X (m)

j

2

(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

.

Also in this algorithm, the recursion that is started when evaluating v(I)
steps. The costs of the algorithm are discussed in Section 5.

j

stops after at most I

8

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

Figure 3. Illustration of the hierarchical reinforced regression algorithm A, for
three iterations. In the lower right part of the diagram, the vertical lines indicate
the equality v(i)

for J − j ≤ i.

j ≡ v(l)

j

4. Iterated reinforced regression for optimal control

Following the ideas and motivations of Section 3 we now present hierarchical reinforced re-
gression algorithms for optimal control based on the Bellman equation (2.7). The algorithms
are based on M sample trajectories (X (m)
)j=0,...,J,m=1,...,M from the underlying Markov chain
: Rd → R. For each y ∈ L we
X. and some initial set {ψ1, ..., ψK} of basis functions ψi
will deﬁne a subset Ly ⊂ L of cardinality Ry := |Ly| and reinforce the basis {ψ1, . . . , ψK} by
{vj+1(z, ·)|z ∈ Ly}. The respective algorithms iteratively construct sequences of approximations
to the value function

j

v(i) = (v(i)

j )j=0,...,J with v(i)

j

: L × Rd → R,

for i = {0, 1, ...} until the iteration is terminated.

4.1. Hierarchical reinforced regression algorithm A. For i = 0 construct v(0) using the
standard regression method inductively as follows: At the terminal time J initialize v(0)
:= vJ
J
where

(4.1)

vJ (y, x) = max

a∈KJ (y,x)

HJ (a, y, x),

for all

y ∈ L, x ∈ Rd.

For a j ∈ {0, ..., J − 1}, assume that v(0)
for each y ∈ L deﬁne the regression coeﬃcients by solving the following least squares problem
(cid:12)
(cid:12)
j+1(y, X (m)
v(0)
(cid:12)
(cid:12)
(cid:12)

is already constructed for all l ∈ {j + 1, ..., J}. Then

j,K := arg min
γ1,...,γK

γkψk(X (m)

, ..., γ(0),y

j+1) −

γ(0),y
j,1

M
(cid:88)

K
(cid:88)

(4.2)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

)

2

.

j

l

m=1

k=1

Next deﬁne the continuation function by

(4.3)

c(0)
j (y, x) :=

K
(cid:88)

k=1

γ(0),y
j,k ψk(x),

for all

y ∈ L, x ∈ Rd

v(i)j(x)gj(x)c(i)j(x)ψ·(x)γ(i)j,·j=J−5J−4J−3J−2J−1Ji=0i=1i=2i=31REINFORCED OPTIMAL CONTROL

9

and the approximate value function v(0)

j

(4.4)

v(0)
j

(y, x) := max

a∈Kj (y,x)

(cid:16)

through the dynamic programming principle
(cid:17)
j (ϕj(a, y), x)

for all

y ∈ L, x ∈ Rd.

Hj(a, y, x) + c(0)

Given the approximation v(i) for some i ≥ 0 we construct a new approximation v(i+1) using
:= vJ . For
is already constructed for l ∈ {j + 1, ..., J}. Then for each

reinforced regression inductively as follows: Initialize at the terminal time v(i+1)
j ∈ {0, ..., J − 1} assume that v(i+1)
y ∈ L deﬁne the regression coeﬃcients by solving the following least squares problem

J

l

(4.5)

γ(i+1),y
j,1

, ..., γ(i+1),y

j,K+Ry := arg min
γ1,...,γK+Ry

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

j+1 (y, X (m)
v(i+1)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

)

−

Ry
(cid:88)

k=1

γK+kv(i)

j+1(yk, X (m)

j

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

,

)

where {yk}k=1,...,Ry = Ly, and deﬁne the continuation function c(i+1)

j

by

(4.6)

c(i+1)
j

(y, x) :=

K
(cid:88)

k=1

γ(i+1),y
j,k

ψk(x) +

Ry
(cid:88)

k=1

j,K+k v(i)
γ(i+1),y

j+1(yk, x),

for all y ∈ L and x ∈ Rd. Finally deﬁne the approximation v(i+1)
ming principle by

j

through the dynamic program-

(4.7)

v(i+1)
j

(y, x) := max

a∈Kj (y,x)

(cid:16)

Hj(a, y, x) + c(i+1)

j

(ϕj(a, y), x)

(cid:17)

,

for all y ∈ L and x ∈ Rd.

The iteration over i ∈ {0, 1, ...} can be terminated after I ∈ N steps, yielding v(I) as an ap-
proximation to the true value function. Alternatively one can introduce an adaptive termination
criterion, for example by comparing the relative change in the error of the least squares problem
(4.5), terminating after the change falls under a given threshold.

Remark 4.1. Recall that in the initialization we have v(i)
follows inductively that

J = v(0)

J

for all i ∈ {1, ..., I}. It then

(4.8)

j ≡ v(l)
v(i)
j ,

for all

J − j ≤ i ≤ I,

l ≥ i.

This identity can be used to reduce the costs of the algorithm, since the regression problem only
needs to be solved for all (j, i) with 0 ≤ j ≤ J − 1 and 0 ≤ i ≤ (J − j) ∧ I.

Remark 4.2. More general or other forms of reinforced basis functions are certainly possible.
The essential point is that they are based on the regression result from the preceding step in
the backwards induction and the preceding iteration. Our speciﬁc choice may be seen as a
natural primal choice. We left ﬂexibility in the choice of the sets Ly, for which depending on
the cardinality of the set L, possible choices are the trivial Ly = L and Ly = {y}, or Ly = L(cid:48)
for some set L(cid:48) independent of y, or more elaborately Ly
j = {ϕ(a, y) | a ∈ Kj(y, xj)} for some
xj ∈ Rd. Note that the use of a step dependent set Ly
j in the above method is straightforward.

4.2. Hierarchical reinforced regression algorithm B. Note that (4.2) and (4.5) are based
on the approximate value functions v(0)
j+1 , respectively, even though the more accurate
approximation v(I)
j+1 is already available at this point. Hence, we can potentially improve the
algorithm’s accuracy by always considering the most accurate approximation of the value function
vj+1 in the Bellman equation.

j+1 and v(i+1)

10

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

Fix a number of iterations I ∈ N and initialize the approximate value functions at the terminal
J ≡ vJ for all i ∈ {0, ..., I}, where vJ is given by (4.1). The approximate value functions

time by v(i)
at times previous to J are deﬁned inductively as follows:

Let j ∈ {0, ..., J − 1} and assume that v(i)

is already deﬁned for all l ∈ {j + 1, ..., J} and
i ∈ {0, ..., I}. For i = 0 and each y ∈ L determine the coeﬃcients for the regression basis by
solving the least squares problem

l

(4.9)

γ(0),y
j,1

, ..., γ(0),y

j,K := arg min
γ1,...,γK

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

j+1(y, X (m)
v(I)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

2

(cid:12)
(cid:12)
(cid:12)
)
(cid:12)
(cid:12)

and deﬁne the approximate continuation function c(0) by (4.3). For i ∈ {1, ..., I} and each y ∈ L
determine the regression coeﬃcients by solving the least squares problem

(4.10) γ(i),y

j,1 , ..., γ(i),y

j,K+Ry := arg min
γ1,...,γK+Ry

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

j+1(y, X (m)
v(I)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

)

Ry
(cid:88)

−

γK+kv(i−1)

j+1 (yk, X (m)

j

k=1
where {yk}k=1,...,Ry = Ly, and deﬁne the continuation function c(i)

j by (4.6).

(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

,

)

Finally, deﬁne the approximation to the value function v(i)
j

for all i = {0, ..., I} by (4.7). After

ending the backwards induction use (v(I)

j

)j=0,...,J as an approximation to the true value function.

Remark 4.3. Note that the identity (4.8) also holds for the above algorithm. Moreover, since we
are only interested in v(I), we can discard the computation of c(i)
for all 0 ≤ j +i ≤ I −1,
since they do not contribute to the construction of v(I). The least squares problem then only
needs to be solved for (j, i) ∈ {0, ..., J −1}×{0, ..., I} with 0 ≤ j +i ≤ I −1 and 0 ≤ i ≤ (J −j)∧I.

j and v(i)

j

Remark 4.4. Choosing the number of iterations I = J we then have from the previous remark
that only the value functions on the diagonal j = i need to be constructed. In this case, denote
vj = v(j)
etc., and observe that the least squares problem which is solved in each step
j ∈ {J − 1, ..., 0} of the backwards induction is given by

, cj = c(j)
j

j

j,1, ..., γy
γy

j,K+Ry := arg min
γ1,...,γK+1

M
(cid:88)

m=1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

vj+1(y, X (m)

j+1) −

K
(cid:88)

k=1

γkψk(X (m)

j

) −

Ry
(cid:88)

k=1

γK+kvj+1(yk, X (m)

j

)

2

(cid:12)
(cid:12)
(cid:12)
(cid:12)

.

where {yk}k=1,...,Ry = Ly. Hence, for I = J the above algorithm represents a direct extension
of the reinforced regression algorithm in [BSSZ20] from optimal stopping to optimal control
problems.

5. Computational cost

We study the computational work of the modiﬁed reinforced regression algorithm of Sec-
tion 4.2. In what follows, the following operations are considered to be performed at constant
cost:

• Multiplications, additions and other primitive operations at cost c∗;
• Simulation from the distribution of the Markov process Xj at cost cX ;
• Evaluation of the standard basis functions ψi or of the payoﬀ Hj at cost cf ;

We furthermore introduce the following notations:

• We set R := maxy∈L Ry.

REINFORCED OPTIMAL CONTROL

11

• The cost of evaluating other non-trivial, but known functions ϕ (think of the value
function when all the required regression coeﬃcients are already known) will be denoted
by cost(ϕ).

If an expression involves several such operations, then only the most expensive constant is
reported. (E.g., evaluating a basis function and multiplying the value by a scalar constant is
considered to incur a cost cf .) We may also use constants c which do not depend on the speciﬁcs
of the algorithm. We now go through the individual stages of the algorithm.

(1) Simulating trajectories at cost cost1 = cX M (J + 1).
(2) Computing the terminal value function as in (4.1) for a given x ∈ Rd and all y ∈ L at

cost cost2 = cf |L| |K|.

(3) For ﬁxed 0 ≤ j ≤ J − 1 and y ∈ L set up the least squares problem (4.9) at cost

(cid:16)

M

cf K + cost

(cid:16)

(cid:17)(cid:17)

.

v(I)
j+1

(4) For ﬁxed 0 ≤ j ≤ J − 1 and y ∈ L, we solve the least squares problem (4.9) at cost

c∗M K 2. The total cost is cost4 = c∗JM K 2 |L|.

(5) For ﬁxed 0 ≤ j ≤ J − 1, y ∈ L, and 1 ≤ i ≤ I set up the least squares problem (4.10) at
v(i−1)
j+1
(6) For ﬁxed 0 ≤ j ≤ J − 1, y ∈ L, and 1 ≤ i ≤ I solve the least squares problem (4.10) at

cf K + cost

+ R cost

cost M

v(I)
j+1

(cid:17)(cid:17)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

.

cost c∗M (K + R)2, leading to a total cost of cost6 = c∗M (K + R)2J |L|.

List 5.1: Stages of the algorithm

For simplicity of the presentation, we shall only consider the following scenario:

Assumption 5.1. The total set of reinforced basis functions contains all available value func-
tions, i.e., (cid:83)

y∈L Ly = L.

For ﬁxed 0 ≤ i ≤ I and 0 ≤ j ≤ J let

(5.1)

v(i)
j

:=

(cid:16)

(cid:17)
v(i)
j (y, ·)

,

y∈L

c(i)
j

:=

(cid:16)

(cid:17)
c(i)
j (y, ·)

.

y∈L

rather than individual ones v(i)

The key step of the cost analysis is understanding the cost of evaluating the reinforced basis
functions, which are, in turn, given in terms of reinforced basis functions at later time steps. We
note that it is essential to analyze the cost of evaluating the full set of reinforced basis functions
v(i)
j (y, ·), as the latter method would show us an apparent
j
explosion of basis functions as we increase time.1 By (4.7), evaluating v(i)
requires evaluating
j
the payoﬀ functions for all combinations of controls y ∈ L and policies a ∈ K, then evaluating
c(i)
j , and taking the corresponding maxima. In total, this means
(cid:16)

(cid:16)

(cid:17)

(cid:17)

cost

v(i)
j

≤ |K| |L| (cf + c∗) + cost

c(i)
j

.

On the other hand, by (4.6) evaluating c(i)
j
K |L| elementary operations for summing them, one evaluation of v(i−1)

requires K evaluations of standard basis functions,
j+1 , and |L|2 elementary

1Suppose that each reinforced basis function v(i)

j+1 (y(cid:48), ·)
and v(i−1)
j+1 (y(cid:48)(cid:48), ·). If we follow this recursion for l ≤ i steps, we arrive at a total set of 2l basis functions. The catch
is that many, if not all, of these basis functions overlap with basis functions for other reinforced basis functions
v(i)
j (˜y, ·).

j (y, ·) depends on two reinforced basis functions v(i−1)

12

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

operations for their summation. In total, this means that

cost

(cid:17)

(cid:16)

c(i)
j

≤ Kcf + K |L| c∗ + 1i>0

(cid:16)

|L|2 c∗ + cost

(cid:16)

v(i−1)
j+1

(cid:17)(cid:17)

.

This implies the cost estimate
(cid:16)

(cid:17)

(5.2)

cost

v(i)
j

≤ |K| |L| (cf + c∗) + Kcf + K |L| c∗ + 1i>0

(cid:16)

|L|2 c∗ + cost

(cid:16)

v(i−1)
j+1

(cid:17)(cid:17)

.

Lemma 5.2. The cost of evaluating v(i)

j , i = 0, . . . , I, j = 0, . . . , J can be bounded by

(cid:40)

cost

(cid:17)

(cid:16)

v(i)
j

≤

(i + 1) (|K| |L| (cf + c∗) + Kcf + K |L| c∗) + i |L|2 c∗,
|L| |K| (cf + c∗) + (J − j) (|K| |L| (cf + c∗) + Kcf + (K + 1) |L| c∗) ,

j + i ≤ J,
j + i > J.

Proof. For a := |K| |L| (cf + c∗) + Kcf + K |L| c∗, consider the cost recursion

c(k + 1) ≤ a + |L| c∗c(k),

k ≥ 0.

Assuming that the recursion hits i = 0 before j = J, i.e., i+j ≤ J, the cost c(k) := cost
satisﬁes the recursion with c(0) ≤ a, and, hence, we obtain

(cid:16)

v(k)
j+i−k

(cid:17)

This gives the ﬁrst expression in the statement of the lemma with k = i.

c(k) ≤ (k + 1)a + k |L|2 c∗.

On the other hand,

(cid:16)

cost

v(i+j−J+k)
J−k

(cid:17)

if i + j > J, we hit j = J before i = 0.

In this case, c(k) :=
satisﬁes the same recursion, but with initial value c(0) ≤ |L| |K| (cf + c∗). (cid:3)

In order to shorten notation, we introduce

:= |L|2 c∗,

a := |K| |L| (cf + c∗) + Kcf + K |L| c∗,
b
d := |L| |K| (cf + c∗),
e

:= |K| |L| (cf + c∗) + Kcf + (K + 1) |L| c∗,

so that the estimate of Lemma 5.2 shortens to

(cid:40)

cost

(cid:17)

(cid:16)

v(i)
j

≤

(i + 1)a + ib,
d + (J − j)e,

j + i ≤ J,
j + i > J.

We next estimate the cost of setting up the regression problem (4.9), which is proved similarly.

Lemma 5.3. The cost of setting up the regression problem for c(0)
can be bounded by

j (y, ·), j = 0, . . . , J − 1, y ∈ L,

cost3 ≤ JM Kcf + M (J − I) ((I + 1)a + Ib) + M Id +

1
2

M I(I + 1)e.

The cost for setting up the least squares problem (4.10) is computed in a similar way.

Lemma 5.4. The cost of setting up the regression problem for c(i)
0, . . . , J − 1, y ∈ L, can be bounded by

j (y, ·), i = 1, . . . , I, j =

cost5 ≤ JM Kcf +

M
2

+

I [(I + 1)a + (I − 1)b] (J − I + 2)+

I (cid:2)11a + 2b − 9d + 5e + I(I + 6)a + 3I(I + b) + 3I 2d + I(I + 6)e(cid:3) .

M
6

REINFORCED OPTIMAL CONTROL

13

Proof. A closer look at (4.10) reveals that the total cost of setting up all these least squares
problems can be bounded by

(5.3)

cost5 ≤

(cid:32)

M

Kcf +

J−1
(cid:88)

j=0

I
(cid:88)

i=1

cost

(cid:16)
v(i−1)
j+1

(cid:17)

(cid:33)

,

taking into account that v(I)
problem (4.9) and, hence, does not need to be evaluated again. Using Lemma 5.2, we obtain


j+1 was already evaluated during the set-up of the least squares



J−1
(cid:88)

(J−j)∧(I−1)
(cid:88)

I−1
(cid:88)

cost5 ≤ JM Kcf + M



((i + 1)a + ib) +

(d + (J − j − 1)e)



j=0

i=0

i=1+(J−j)∧(I−1)

= JM Kcf + M

J−I+1
(cid:88)

(cid:34)I−1
(cid:88)

j=0

i=0

(cid:35)

((i + 1)a + ib)

+

J−1
(cid:88)

+ M





J−j
(cid:88)

((i + 1)a + ib) +

I−1
(cid:88)

(d + (J − j − 1)e)



 .

j=J−I

i=0

i=J−j+1

Evaluating the double sums gives the estimate from the statement of the lemma.

(cid:3)

Abandoning the diﬀerence between cf and c∗ using the trivial bounds cost3 ≤ const cost5,

cost4 ≤ const cost6, we obtain

Theorem 5.5. The computational cost of the algorithm presented in Section 4.2 can be bounded
by

cost ≤ const M J (cid:0)cX + I 2(K + |K| + |L|) |L| + (K + R)2 |L|(cid:1) ,

where const is a positive number independent of |K|, |L|, K, J, and I.

Remark 5.6. Recall that Remark 4.4 introduced a signiﬁcantly cheaper variant of algorithm B
for the case I = J. It is easy to see that the computational cost of this variant is bounded by
cost ≤ const M J (cid:0)cX + J(K + |K| + |L|) |L| + (K + R)2 |L|(cid:1) ,

i.e., the total cost is proportional to J 2 rather than J 3. Indeed, the main diﬀerence in the cost
analysis as compared to the full modiﬁed algorithm is that (5.3) can be replaced by

cost5 ≤

(cid:16)

M

J−1
(cid:88)

j=0

Kcf + cost

(cid:16)

v(J−j−1)
j+1

(cid:17)(cid:17)

.

Note that this essentially corresponds to the algorithm of [BSSZ20] directly generalized to optimal
control problems.

6. Convergence analysis

In this section we analyze the convergence properties of the standard and reinforced regression
algorithms introduced in the previous sections. For related convergence analysis in the case of
optimal stopping problems we refer the interested reader to [Zan13], [Zan18], and [BS20], see
also [BRS21]. Henceforth we assume that

(6.1)

max
j=0,...,J

sup
y∈L

sup
a∈K

sup
x∈X

|Hj(a, y, x)| ≤ CH ,

14

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

then all the value functions

v∗
j (y, x) :=

sup
(cid:96)=j ∈Aj (y,x)

A=(A(cid:96))J

E





J
(cid:88)

(cid:16)

H(cid:96)

A(cid:96), Y(cid:96)(A; j, y), X j,x

(cid:96)

(cid:17)





(cid:96)=j

are uniformly bounded by JCH . Fix a sequence of spaces Ψj, j = 0, . . . , J, of functions de-
ﬁned on X . We stress that these spaces are not necessarily linear at this point. Construct the
corresponding sequence of estimates (vj,M (y, x))J

j=0 via

(6.2)

vJ,M (y, x) = sup

HJ (a, y, x)

and

a∈Kj (y,x)

vj,M (y, x) = sup

(Hj(a, y, x) + TW Pj,M [vj+1,M ](ϕj+1(a, y), x)) ,

j < J,

a∈Kj (y,x)

where Pj,M [g](z, x) stands for the empirical projection of the conditional expectation E[g(z, X j,x
on Ψj, based on a sample

j+1)]

(6.3)

DM,j =

(cid:110)

(X (m)
j

, X (m)

j+1), m = 1, . . . , M

(cid:111)

from the joint distribution of (Xj, Xj+1), that is,
(cid:20)(cid:12)
(cid:12)g(z, X (m)
(cid:12)

Pj,M [g](z, ·) ∈ arg inf
ψ∈Ψj

M
(cid:88)

j+1) − ψ(X (m)

j

2(cid:21)

.

(cid:12)
(cid:12)
)
(cid:12)

m=1

In (6.2) TW is a truncation operator at level W = JCH deﬁned by

(cid:40)

TW f (x) =

f (x),
W sign(f (x)), otherwise.

|f (x)| ≤ W,

Due to Theorem 11.5 in [GKKW02], one has for all g with (cid:107)g(cid:107)∞ ≤ W, j = 0, . . . , J − 1, and all
z ∈ L, that
(cid:20)(cid:13)
(cid:13)
(cid:13)TW Pj,M [g](z, ·) − E

g(z, X j,·

(6.4) E

(cid:105)(cid:13)
2
(cid:13)
(cid:13)

j+1)

(cid:21)

(cid:104)

L2(µj )

≤ ε2

j,M + 2 inf
w ∈ Ψj

(cid:107)g(z, ·) − w(cid:107)2

L2(µj ) with ε2

j,M := cW 4 1 + log M

VC(Ψj),

M

where VC(Ψj) is the Vapnik-Chervonenkis dimension of Ψj (see Deﬁnition 9.6 in [GKKW02]), µj
is the distribution of Xj, and c is an absolute constant. In order to keep the analysis tractable,
we assume that the sets DM,j are independent for diﬀerent j, see Remark 6.5 below. More
speciﬁcally, we consider an algorithmic framework based on (6.2), where for every exercise date
the samples (6.3) are simulated independently, and consider the information sets
, m = 1, . . . , M (cid:1).
Gj,M := σ (cid:8)Xj;M , . . . , XJ;M (cid:9) with Xj;M := (cid:0)X (m)

j

Let us deﬁne for j < J, z ∈ L, x ∈ X ,

(6.5)

(cid:98)Cj(z, x) := TW Pj,M [vj+1,M ](z, x),

and for a generic (exact) dummy trajectory (Xl)l=0,...,J independent of Gj,M , let

(6.6)

(cid:101)Cj(z, x) := EGj+1,M

(cid:104)

vj+1,M (z, X j,x

(cid:105)
j+1)

.

Note that (cid:101)Cj (·, ·) is a Gj+1,M -measurable random function while the estimate (cid:98)Cj (·, ·) is a Gj-
measurable one. We further deﬁne

(6.7)

C ∗

j (z, x) = E

(cid:104)
j+1(z, X j,x
v∗

(cid:105)
j+1)

,

j < J.

The following lemma holds.

REINFORCED OPTIMAL CONTROL

15

Lemma 6.1. We have that,
(cid:20)(cid:13)
(cid:12)
(cid:12) (cid:101)Cj(z, ·) − C ∗
(cid:12)
(cid:13)
(cid:13)sup

(6.8)

E

z∈L

(cid:12)
(cid:12)
j (z, ·)
(cid:12)

(cid:13)
2
(cid:13)
(cid:13)

L2(µj )

(cid:21)

≤ E

(cid:20)(cid:13)
(cid:13)
(cid:13)sup

z∈L

(cid:12)
(cid:12) (cid:98)Cj+1(z, ·) − C ∗
(cid:12)

j+1(z, ·)

(cid:12)
(cid:12)
(cid:12)

(cid:13)
2
(cid:13)
(cid:13)

L2(µj+1)

(cid:21)

.

Proof. Let X be a generic (exact) dummy trajectory independent of Gj+1,M . Then from (6.6),
and (6.7) we see that for j < J,

(6.9)

(cid:12)
(cid:12) (cid:101)Cj(z, Xj) − C ∗
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ EGj+1,M
j (z, Xj)

(cid:2) (cid:12)
(cid:12)vj+1,M (z, Xj+1) − v∗

j+1(z, Xj+1)(cid:12)
(cid:12)

(cid:12)
(cid:12) Xj

(cid:3)

Next, by (2.7), (6.2), (6.5), and (6.7) we have that
j+1(z, x)(cid:12)

(cid:12)
(cid:12)vj+1,M (z, x) − v∗

(cid:12) ≤

(cid:12)
(cid:12) (cid:98)Cj+1(ϕj+2(a, z), x) − C ∗
(cid:12)

(cid:12)
(cid:12)
j+1(ϕj+2(a, z), x)
(cid:12)

sup
a∈Kj+1(z,x)
(cid:12)
(cid:12) (cid:98)Cj+1(z(cid:48), x) − C ∗
(cid:12)

≤ sup
z(cid:48)∈L

(cid:12)
j+1(z(cid:48), x)
(cid:12)
(cid:12) .

(6.10)

Hence, by (6.9) one has that

sup
z∈L

(cid:12)
(cid:12) (cid:101)Cj(z, Xj) − C ∗
(cid:12)

(cid:12)
(cid:12)
j+1(z, Xj+1)
(cid:12)
Finally, by taking the “all-in expectation” w.r.t. the law µj ⊗ PM , we observe that
(cid:26)

(cid:12)
(cid:12) (cid:98)Cj+1(z, Xj+1) − C ∗
(cid:12)

(cid:12)
(cid:12)
(cid:12) ≤ EGj+1,M
j (z, Xj)

sup
z∈L

(cid:20)

(cid:20)

(cid:20)

(cid:21)

Xj

.

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) (cid:101)Cj(z, Xj) − C ∗
(cid:12)

j (z, Xj)

2(cid:21)
(cid:12)
(cid:12)
(cid:12)

E

sup
z∈L

≤ E

EGj+1,M

sup
z∈L

(cid:12)
(cid:12) (cid:98)Cj+1(z, Xj+1) − C ∗
(cid:12)

j+1(z, Xj+1)

≤ E

(cid:20)

sup
z∈L

(cid:12)
(cid:12) (cid:98)Cj+1(z, Xj+1) − C ∗
(cid:12)

(cid:12)
(cid:12)
j+1(z, Xj+1)
(cid:12)

2(cid:21)

by Jensen’s inequality and the tower property.

In fact, Lemma 6.1 is the key to the next proposition.

Proposition 6.2. Set

Ej :=

(cid:13)
(cid:13)
(cid:13)sup

z∈L

(cid:12)
(cid:12) (cid:98)Cj(z, ·) − C ∗
(cid:12)

j (z, ·)

(cid:12)
(cid:13)
(cid:13)
(cid:12)
(cid:13)L2(µj ⊗PM )
(cid:12)

,

j = 0, . . . , J − 1,

with PM being the law of the sample X (m)

j

, m = 1, . . . , M, j = 1, . . . , J. Then it holds

(6.11)

Ej ≤ |L|

(cid:16)

εj,M +

√

2 sup
z∈L

inf
w ∈ Ψj

(cid:13) (cid:101)Cj(z, ·) − w(cid:13)
(cid:13)

(cid:13)L2(µj ⊗PM )

(cid:17)

+ |L| Ej+1

for all j = 0, . . . , J − 1, with EJ = 0 by deﬁnition.

(cid:21)(cid:27)2

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xj

(cid:3)

Proof. The case j = J − 1 follows from (6.4) and the fact that (cid:101)CJ−1 = C ∗
(cid:107) (cid:101)Cj(z, ·) − w(cid:107)L2(µj ⊗PM ). Due to (6.4) we have with probability 1,
inf
w ∈ Ψj

J−1. Set rj,M (z) =

(6.12)

Hence

(6.13)

EGj+1,M

(cid:20)(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13) (cid:98)Cj(z, ·) − (cid:101)Cj(z, ·)
(cid:13)

L2(µj )

(cid:21)

≤ ε2

j,M + 2r2

j,M (z).

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)Cj(z, ·) − (cid:101)Cj(z, ·)
(cid:13)L2(µj ⊗PM )

≤ εj,M +

√

2rj,M (z).

By applying (6.13) it follows that

(cid:13)
(cid:13) (cid:98)Cj(z, ·) − C ∗
(cid:13)

(cid:13)
(cid:13)
j (z, ·)
(cid:13)L2(µj ⊗PM )

≤ εj,M +

√

2rj,M (z) +

(cid:13)
(cid:13) (cid:101)Cj(z, ·) − C ∗
(cid:13)

(cid:13)
(cid:13)
j (z, ·)
(cid:13)L2(µj ⊗PM )

.

16

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

From this and Lemma 6.1 we imply
(cid:13)
(cid:13) (cid:98)Cj(z, ·) − C ∗
(cid:13)

j (z, ·)

(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

sup
z∈L

√

≤ εj,M +

2 sup
z∈L

rj,M (z)

+

(cid:13)
(cid:13)
(cid:13)sup

z∈L

(cid:12)
(cid:12) (cid:98)Cj+1(z, ·) − C ∗
(cid:12)

(cid:12)
(cid:12)
j+1(z, ·)
(cid:12)

(cid:13)
(cid:13)
(cid:13)L2(µj+1⊗PM )

and then (6.11) follows.

Corollary 6.3. Suppose that

sup
z∈L

inf
w ∈ Ψj

(cid:13) (cid:101)Cj(z, ·) − w(cid:13)
(cid:13)

(cid:13)L2(µj ⊗PM ) ≤ δ,

VC(Ψj) ≤ D,

0 ≤ j ≤ J − 1,

for some δ > 0 and D > 0. Proposition 6.2 then yields for j = 0, . . . , J − 1, by using (6.10),

(6.14)

(cid:13)
(cid:13)
(cid:13)sup

z∈L

(cid:12)
(cid:12)vj,M (z, ·) − v∗

(cid:13)
j (z, ·)(cid:12)
(cid:13)
(cid:12)
(cid:13)L2(µj ⊗PM )

≤

(cid:18)

cW 4 1 + log M

M

D +

√

2δ

(cid:19) |L|J−j+1 − |L|
|L| − 1

.

Corollary 6.4. By inserting the estimate
(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

(cid:13)
(cid:13) (cid:101)Cj(z, ·) − C ∗
(cid:13)
in Proposition 6.2, we get the alternative recursion

(cid:13)
(cid:13)
(cid:13) (cid:101)Cj(z, ·) − w

inf
w ∈ Ψj

≤

j (z, ·)

(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

+ inf

w ∈ Ψj

(cid:13)
(cid:13)C ∗

j (z, ·) − w(cid:13)

(cid:13)L2(µj )

(cid:18)

Ej ≤ |L|

εK,M +

√

2 sup
z∈L

inf
w ∈ Ψj

(cid:13)
(cid:13)C ∗

j (z, ·) − w(cid:13)

(cid:13)L2(µj )

(cid:19)

+ |L| (1 +

√

2)Ej+1,

and under the alternative assumption
j (z, ·) − w(cid:13)

(cid:13)
(cid:13)C ∗

sup
z∈L

inf
w ∈ Ψj

(cid:13)L2(µj ) ≤ δ,

VC(Ψj) ≤ D,

0 ≤ j ≤ J − 1,

for some δ > 0 and D > 0, we obtain for j = 0, . . . , J, the bounds

(cid:13)
(cid:13)
(cid:13)sup

z∈L

(cid:12)
(cid:12)vj,M (z, ·) − v∗

j (z, ·)(cid:12)
(cid:12)

(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

≤

(cid:18)

cW 4 1 + log M

M

√

(cid:19)

2δ

|L|

D +

√

(cid:0)(1 +

(1 +

2) |L|(cid:1)J−j
√
2) |L| − 1

− 1

(cid:3)

.

Remark 6.5. In [Zan13] the convergence of an “independent sample version” of the Longstaﬀ-
Schwartz algorithm was studied based on an assumption similar to the independence DM,j for
diﬀerent j here. However, in a later paper [Zan18] it was shown (by more involved analysis) that
the convergence rates based on one and the same sample are basically the same as in [Zan13]
up to certain constants. One therefore may naturally expect that similar conclusions apply in
our context. Therefore the numerical examples in Section 7 are based on a single sample of M
trajectories.

The proposed reinforced regression algorithm with I = J uses linear approximation spaces of

the form

(6.15)

Ψj = span{ψ1(x), . . . , ψK(x), vj+1,M (y1, x), . . . , vj+1,M (yR, x)},

j = 0, . . . , J − 1,

for L = {y1, . . . , yR}, where ψ1(x), . . . , ψK(x) are some ﬁxed basis functions (e.g. polynomials)
on X . In this case VC(Ψj) ≤ K + R,
0 ≤ j ≤ J − 1. In order to see the advantage of adding
additional basis functions more clearly, we prove the following proposition.

Proposition 6.6. Assume additionally that

(6.16)

max
j=1,...,J

sup
y∈L

sup
a∈K

sup
x∈X

|Hj(a, y, x1) − Hj(a, y, x2)| ≤ LH |x1 − x2|,

for all x1, x2 ∈ X and

(6.17)

max
j=1,...,J

max
(cid:96)=j,...,J

E[|X j,x1

(cid:96) − X j,x2

(cid:96)

|] ≤ LX |x1 − x2|,

∀x1, x2 ∈ X

REINFORCED OPTIMAL CONTROL

17

for some constants LH > 0, LX > 0. Then it holds for reinforced spaces (Ψj) from (6.15)

sup
z∈L

inf
w ∈ Ψj

(cid:13)
(cid:13)
(cid:13) (cid:101)Cj(z, ·) − w

(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

(cid:20)

≤ JLX LH

E

(cid:90)

Proof. We have

|X j,x

j+1 − x|2µj(dx)

(cid:21)1/2

.

sup
z∈L

inf
w ∈ Ψj

(cid:13)
(cid:13)
(cid:13) (cid:101)Cj(z, ·) − w

(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

(cid:13)
(cid:13)
(cid:13)EGj+1,M

≤ sup
z∈L

(cid:2)vj+1,M (z, X j,·

j+1) − vj+1,M (z, ·)(cid:3)(cid:13)
(cid:13)
(cid:13)L2(µj ⊗PM )

.

Under assumptions (6.16) and (6.17), we then have

max
j=1,...,J

sup
y∈L

|v∗

j (y, x1) − v∗

j (y, x2)| ≤ JLX LH |x1 − x2|,

∀x1, x2 ∈ X .

By using an additional truncation, one can achieve that the Lipschitz constants of the estimates
vj,M (z, ·), j = 0, . . . , J − 1, are all uniformly bounded by a constant JLX LH with probability
(cid:3)
1.

The above proposition implies that if JLH stays bounded for J → ∞, (for example if H scales
(cid:107) (cid:101)Cj(z, ·) − w(cid:107)L2(µj ⊗PM ) becomes small as J → ∞.

as 1/J), then the approximation error

inf
w ∈ Ψj

Note that the latter property can not be guaranteed when using ﬁxed (nonadaptive) linear
spaces Ψj. Of course, the exponential in J factor in (6.14) will lead to explosion of the overall
error as J → ∞, but the above observation still indicates that the inclusion of the functions
vj+1,M (y1, x), . . . , vj+1,M (yR, x) into Ψj can signiﬁcantly improve the quality of the estimates
vj,M (z, ·) especially in the case of large J. Concerning the dependence of the bound (6.14) on J,
we note that this estimate is likely to be too pessimistic, see also a discussion in [Zan13].

7. Numerical examples

We now present various numerical examples which demonstrate the accuracy of the reinforced
regression algorithm in practice. To allow for a direct comparison with the reinforced regression
algorithm of [BSSZ20], we ﬁrst consider a (single) optimal stopping problem, more particularly
a Bermudan max-call option. Our second example is a multiple stopping problem, for which the
hyperparameters already become crucial. Finally, our last example is an optimal control of a gas
storage.

We have tested both algorithms A and B for the HRR (hierarchical reinforced regression)
method, however, the latter version always gave slightly better results and therefore we have
only included the value obtained with the algorithm B. Intuitively this is to be expected, as the
algorithm B uses more accurate regression targets in the backwards induction. The algorithm
A may be of use in situations where one is interested in improving the approximation until a
certain accuracy threshold is reached, however, properly done this approach should also include
a calculation of upper bounds, which we do not discuss in our paper.

Before, let us also mention how a lower biased estimate to the value of a control problem
in a Markovian setting is calculated using the result of a regression procedure. Let c be an
approximation to the function c∗ given by

c∗
j (x, y) = E

(cid:104)
j+1(y, X j,x
v∗

j+1)

(cid:105)

,

x ∈ Rd, y ∈ L, j ∈ {0, ..., J − 1},

with cJ ≡ c∗
J ≡ 0 by convention. Using the hierarchical reinforced regression method, such
an approximation is given by c(I) deﬁned in (4.6). Further let (X (m))1≤m≤Mtest be sample
trajectories from the underlying Markov chain, generated independently from the samples used

18

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

in the regression procedure. Then we can iteratively deﬁne a sequence of polices (A(m))1≤m≤Mtest
with A(m) = (A(m)

, ..., A(m)

) by

0

J

A(m)
j

:=

arg max

A∈Kj (Y (m)

j

,X (m)
j

)

(cid:16)

Hj(A, Y (m)

j

, X (m)
j

) + cj(ϕ(A, Y (m)

j

), X (m)
j

)

(cid:17)

,

j , Y (m)
or all m = 1, ..., Mtest and j = 0, ..., J, where Y (m)
).
It then follows from the deﬁnition, that each A(m) is an admissible sequence of policies, i.e.
A(m) ∈ A0(y0, X (m)). Therefore, a lower estimate to the value E(v(y0, X0)) is given by

:= y0 ∈ L and Y (m)

j+1 := ϕj+1(Am

0

j

1
Mtest

Mtest(cid:88)

J
(cid:88)

m=1

j=0

Hj(A(m)

j

, Y (m)
j

, X (m)
j

).

Lower bounds allow a direct comparison of the performance of diﬀerent methods, in the sense
that the method yielding the highest lower bound (up to Monte Carlo errors) performed best,
since this value must be closest to the true value of the control problem. This direct comparison
is, however, not possible for the approximate value v0 which may lie above or below the true value.
Our main premise in the following is that the HRR algorithm is a more eﬃcient way to improve
the performance of the regression algorithm as compared with increasing the complexity of the
regression basis. Hence, for this relative comparison it is also suﬃcient to study lower bounds.

7.1. Bermudan max-call option. In this section we evaluate the performance of the hierarchi-
cal reinforced regression (HRR) method from Section 4 on the valuation of a Bermudan max-call
option. Let (Ω, F, (Ft)0≤t≤T , P ) be a ﬁltered probability space on which a d-dimensional Brow-
nian motion W = (W (t))0≤t≤T is deﬁned. Further let X = (X(t))0≤t≤T be the geometric
Brownian motion deﬁned by

dX k(t) = (r − δ) X k(t) dt + X k(t) σ dW k(t), X k(0) = x0,

0 ≤ t ≤ T,

k ∈ {1, ..., d},

where x0, r, δ, σ > 0. Option rights can be exercised on a predeﬁned set of possible exercise
dates {t0, t1, ..., tJ }, where at most one right can be exercised on any given date. Assume that
the exercise dates are equidistant tj := j · ∆t for all j ∈ {0, ..., J} with ∆t := T /J and deﬁne
the underlying Markov chain (Xj)j=0,...,J by Xj = X(j∆t). Recall from Example 2.3 that in
order to model a multiple stopping problem in the optimal control framework we deﬁne the set
of policies by K = {0, 1} and the set of controls by L = {0, ..., ymax}, where ymax is the number
of exercise rights. Further, we deﬁne

ϕj(a, y) := (y − a)+, Kj(x, y) := {0, 1 ∧ y}, Hj(a, y, x) := a · gj(x) e−tj r,

for all y ∈ L, a ∈ K and j ∈ {0, ..., J}, where g is the max-call pay-oﬀ function deﬁned by

g(x) := (max{x1, ...., xd} − C)+,

x ∈ Rd,

where C ∈ R+ is the option strike. Then the value function v∗
0(ymax, x0) deﬁned in (2.6) yields the
value of the Bermudan max-call option with underlying X and data (d, J, T, ymax, x0, C, r, δ, σ).

7.1.1. Single exercise right. We will ﬁrst consider the case of a single exercise right ymax = 1.
This is a standard example in the literature, see for example [BG04, Rog02, AB04] and more
recently [BCJ19]. The performance of the reinforced regression method for this example was
already analyzed in [BSSZ20]. We revisit this example in order to demonstrate that even in the
optimal stopping case our novel HRR method allows for improvements in computational costs
without sacriﬁcing the quality of the estimations.

REINFORCED OPTIMAL CONTROL

19

d Basis

2

3

5

10

Ψ1
Ψ1,g
Ψ2
Ψ3
Ψ1
Ψ1,g
Ψ2
Ψ3
Ψ1
Ψ1,g
Ψ2
Ψ3
Ψ1
Ψ1,g
Ψ2
Ψ3

Regression
I = 0
13.015 (0.022)
13.679 (0.019)
13.775 (0.016)
13.874 (0.016)
17.764 (0.029)
18.404 (0.022)
18.519 (0.020)
18.655 (0.021)
25.463 (0.024)
25.823 (0.026)
25.990 (0.023)
26.111 (0.022)
38.022 (0.025)
38.058 (0.024)
38.299 (0.023)
38.349 (0.021)

Lower bounds
HRR
I = 1
13.772 (0.015)
-
13.871 (0.015)
-
18.526 (0.017)
-
18.639 (0.017)
-
25.998 (0.021)
-
26.097 (0.019)
-
38.234 (0.022)
-
38.316 (0.020)
-

Reinf. Reg.
I = 9
13.794 (0.015)
-
13.882 (0.014)
-
18.540 (0.018)
-
18.653 (0.017)
-
25.990 (0.019)
-
26.109 (0.020)
-
38.225 (0.024)
-
38.331 (0.021)
-

CI from [BCJ19]

[13.880,13.910]

[18.673,18.699]

[26.138, 26.174]

[38.300,38.367]

Table 1. Lower bounds (± 99.7% Monte-Carlo error) for the value of the
Bermudan max-call option with data J = 9, T = 1, ymax = 1, x0 = C = 100,
r = 0.05, δ = 0.1, σ = 0.2 and diﬀerent numbers of underlying assets
d ∈ {2, 3, 5, 10}. For all methods we used M = 106 training sample paths and
Mtest = 107 paths for calculating the lower bound. The last column presents
the 95% conﬁdence intervals for the value of the Bermuda option from [BCJ19].

Deﬁne the functions fi

: Rd → R, x (cid:55)→ sort(x1, . . . , xd)i, the ith largest entry of x, for

i ∈ {1, . . . , d} and consider the following three sets of regression basis functions:

Ψ1 := {1, f1, ..., fd}, Ψ1,g := Ψ1 ∪ {g}, Ψ2 := Ψ1 ∪ {fi · fj | 1 ≤ i ≤ j ≤ d}
Ψ3 := Ψ1 ∪ Ψ2 ∪ {fi · fj · fk | 1 ≤ i ≤ j ≤ k ≤ d}.

2 d + 1, and |Ψ3| = 1

Note that the cardinalities of these sets are given by |Ψ1| = d + 1, |Ψ1,p| = d + 2, |Ψ2| =
2 d2 + 3
1
6 d + 1, respectively. Regarding the HRR method, we
use the algorithm of the second type described in Section 4.2. Further note that in the optimal
stopping case there is only one choice for the set of reinforced value function for the HRR method
since L = {1} and therefore we always set L1 = {1}.

6 d3 + d2 + 11

We considered two diﬀerent set-ups for the comparison of the diﬀerent methods:

• First we keep the number of exercises dates J ﬁxed and vary the number of underlying

assets d;

• Second we keep d ﬁxed and vary J (while also keeping T ﬁxed).

In Table 1 we present lower estimates to the value of a Bermudan max-call option with a
In the corresponding Figure 4 we have
single exercise right for J = 9 and d ∈ {2, 3, 5, 10}.
visualized the lower bounds for the comparison between the diﬀerent regression methods. For
each of the considered methods we used M = 106 simulated training samples paths to determine
the regression coeﬃcients and Mtest = 107 paths for calculating the lower bounds.

In order to give the reader an easy reference point, in Table 1 we have also included the
conﬁdence intervals for the value of the Bermudan max-call option from [BCJ19]. Note however

20

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

Figure 4. A visualization of the lower bounds from Table 1.

that we do not aim for an improvement of the latter values in terms of benchmarking. In fact
the method used in [BCJ19] is quite diﬀerent from ours, as it uses deep neural networks for
approximating the optimal stopping policies at each time step, and a direct comparison would
require the usage of higher order polynomials for our method.

We ﬁrst observe that across all numbers of assets d the HRR method with the set of basis
functions Ψ1 performs signiﬁcantly better then the standard regression method with the basis
Ψ1 and Ψ1,g. The same holds true when comparing the methods using the regression basis Ψ2.
More importantly however, we observe that for d ≤ 5 the HRR method with basis Ψ1 yields lower
bounds of the same quality as obtained with the standard method and the larger basis Ψ2. The
same holds true when comparing the HRR method with basis Ψ2 against the standard method
with the basis Ψ3. In the case d = 10 assets, the lower bounds obtained with the HRR method
and basis Ψ1 lie just slightly below the values of the lower bounds obtained with standard method
and the basis Ψ2, however one has to keep in mind that in this case |Ψ1| = 11 and |Ψ2| = 286.
Moreover, we see that the HRR method with a recursion depth I = 1 performs just as well as
the (full depth) reinforced regression method (I = J = 9).

Furthermore, in Figure 5 we have visualized the corresponding elapsed CPU times during the
backwards induction and the calculation of the lower bounds. As foreshadowed in Section 5,
we see that the computational costs of the HRR method are signiﬁcantly reduced by choosing a
small recursion depth I. In particular, we are able to state that for suﬃciently large d (d ≥ 5
respectively d ≥ 3) the HRR method with recursions depth I = 1 and the basis Ψ1 respectively
Ψ2 is more eﬃcient then the standard method with the basis Ψ2 respectively Ψ3.

The results of the second set-up are visualized in Figure 6. In this case, we have approximated
the value of Bermudan max-call options with a ﬁxed number of assets d = 4 and diﬀerent
numbers of exercise dates J ∈ {9, 18, 36, 72}, while also keeping T = 1 ﬁxed. When keeping all
other parameters ﬁxed, the value of the option clearly is non-decreasing in the number of exercise
dates J. Our ﬁrst observation is that for all considered methods there exists a threshold for J
at which the performance worsens. Indeed, Corollary 6.3 shows that the approximation error
depends exponentially on J.

However, in practice, some methods are less vulnerable to the error explosion in J than
others. In this example, we see that the standard regression method with the basis Ψ1,g and Ψ2,

11,g23112212.913.013.113.213.313.413.513.613.713.813.9d=211,g23112237.9037.9538.0038.0538.1038.1538.2038.2538.3038.35d=10Regression(I=0)HRR(I=1)RR(I=9)Regression(I=0)HRR(I=1)RR(I=9)REINFORCED OPTIMAL CONTROL

21

Figure 5. The elapsed CPU times during the backwards induction and calcu-
lation of the lower bounds from Table 1, plotted with respect to the number of
underlying assets d.

Figure 6. Visualization of the lower bounds (± 99.7% Monte-Carlo error) of
the values of Bermuda max-call options with J = 9, 18, 36, 72 exercise dates and
d = 4, T = 1, ymax = 1, x0 = C = 100, r = 0.05, δ = 0.1, σ = 0.2. The
values are obtained with the standard regression method I = 0 and the HRR
method I = 1, 2, 9. For all methods we used M = 106 training sample paths
and Mtest = 106 sample paths for the calculation for the lower bounds.

23510d050100150200250300350400CPU Time (s)1, I=01,g, I=02, I=03, I=01, I=12, I=11, I=92, I=9J = 9J = 18J = 36J = 720129012901290129I21.621.721.821.922.022.122.222.322.422.522.622.722.822.9I=0, 1,gI=0, 2I=0, 3I=1, 1I=2, 1I=9, 122

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

respectively, starts to perform worse for J ≥ 36. The lower bounds calculated with the HRR
method with the basis Ψ1 and I = 1 stay approximately on the same level as the lower bounds
calculated with the standard method and the basis Ψ2, for all numbers of exercise dates. The
lower bounds calculated with the standard method and the basis Ψ3 ﬁrst increase when moving
from 9 to 18 exercise dates and decrease at last when moving from 36 to 72 exercise dates.

The main observation is that when we increase J, the HRR methods with I = 2 and I = 9
come closer to the lower bounds calculated with the standard method and the basis Ψ3. This
underlines the theoretical discussion for J → ∞ from Section 6. Moreover, we see that the HRR
method performs at least as well with I = 2 as with I = 9. Regarding CPU time, the HRR
method with I = 2 is more eﬃcient than then the standard regression method with the basis
Ψ3 for J ≥ 9. Comparing the HRR methods with I = 2 and I = 9, we see that choosing the
parameter I small is necessary to in order to obtain desirable eﬃciency. Since on the other hand,
the HRR method with I = 1 performed signiﬁcantly worse then with I = 2, we also see that
in this case it was necessary to choose I > 1. These observations underline the relevance of the
HRR in its full complexity even in the case of optimal stopping problems.

7.1.2. Multiple exercise rights. Next we consider a Bermudan max-call option with ymax = 4
exercise rights. In this case the HRR method allows for diﬀerent possibilities of reinforced value
functions depending on the choice of the sets Ly (recall Remark 4.2). Since ymax is small, we
choose Ly ≡ {1, 2, 3, 4} for simplicity.

Basis

Ψ1
Ψ1,g
Ψ2
Ψ3

Regression
I = 0
90.863 (0.072)
91.837 (0.082)
92.140 (0.070)
92.571 (0.069)

Hierarchical Reinforced Regression

I = 1
92.038 (0.070)
-
92.418 (0.064)
-

I = 2
92.287 (0.070)
-
92.548 (0.060)
-

I = 3
92.311 (0.067)
-
92.631 (0.061)
-

I = 5
92.357 (0.061)
-
92.625 (0.061)
-

Table 2. Lower bounds (± 99.7% Monte-Carlo error) for the value of the
Bermudan max-call option with data J = 24, T = 2, ymax = 4, x0 = C = 100,
d = 5, r = 0.05, δ = 0.1, σ = 0.2. For all methods we used M = 106 training
sample paths and Mtest = 107 paths for calculating the lower bound. An upper
bound to the value, calculated with the dual approach from [Sch12] and [BSZ15],
is given by 92.971 (0.043), with the HRR method with I = 3 and basis Ψ2 using
105 outer and 103 inner sample paths.

In Table 3 we present lower bounds to the value of a Bermuda max-call option with ymax = 4
exercises rights, obtained with the standard regression method and HRR method for diﬀerent
choices of regression basis functions and the parameter I, with the implementation of the second
type described in Section 4.2. We ﬁrst observe that for a ﬁxed set of basis functions Ψ1 or Ψ2
increasing the parameter I yields increased, and thus improved, lower bounds. This improvement
is most signiﬁcant when moving from I = 0 (standard regression) to I = 1 and from I = 1 to
I = 2 and becomes less signiﬁcant when further increasing I. Moreover, we observe that the
HRR method with I = 1 and basis functions Ψ1 yields better lower bounds then the standard
regression method with the larger set of basis functions Ψ1,g and more importantly, for I ≥ 2 the
HRR with basis functions Ψ1 method yields better lower bounds than the standard regression
method with the even larger set of basis functions Ψ2. This observation prevails when comparing
the standard regression method with the basis Ψ3 against the HRR method with the basis Ψ2.
We can therefore conclude that the HRR method yields results of better quality than standard

REINFORCED OPTIMAL CONTROL

23

regression using fewer regression basis functions. Moreover, we realize that up to changes that
are insigniﬁcant with respect to the Monte Carlo error, the HRR reaches its best performance
already for I = 3, thus further increasing I is not necessary.

7.2. A gas storage problem. In this subsection we consider a gas-storage problem of the kind
introduced in Example 2.4. In contrast to the example in the previous subsection, this optimal
control problem is not of a multiple stopping type, which is a consequence of the anti-symmetry
in the policy set: injection of gas into the facility (a = 1), no action (a = 0) and production of
gas (a = −1).

For the gas price we use a similar but slightly more elaborate model to the one proposed in
[TDR09] (and also used in [GHW11]). More speciﬁcally, we use the following joint dynamics to
model the price of crude oil X 1 and the price of natural gas X 2

(7.1)

dX 1(t) = α1(β − X 1(t))dt + σ1X 1(t)dW 1(t) +

(cid:16)

(cid:17)
N (t−)+1 − X 1(t)
J 1

dX 2(t) = α2(X 1(t) − X 2(t))dt + σ2X 2(t)dW 2(t) +

(cid:16)

N (t−)+1 − X 2(t)
J 2

dN (t),

dN (t)
(cid:17)

for 0 ≤ t ≤ T , where β, αi, σi > 0 for i = 1, 2, W 1 and W 2 are Brownian motions with
correlation ρW ∈ [0, 1], N is a Poisson process with intensity λ > 0 and (Jk)k=1,... are i.i.d.
normal distributed random vectors with J i
1 ∼ N (µi, η2
1 ) ∈ [0, 1].
Moreover we assume that (W 1, W 2), N and (J 1, J 2) are independent. Note that both X 1 and
X 2 are mean reverting processes with jump contributions. The oil price process X 1 reverts
to the long-term constant mean β and the gas price process X 2 reverts towards the oil price
X 1, which is aiming to model the well known strong correlation between crude oil and natural
gas prices. Note also that we have assumed for simplicity that the jump signal, which has the
purpose of modeling price peaks, is the same Poisson process for both oil and gas prices, however
the magnitude of the jumps is given by diﬀerent (but correlated) normal distributed random
variables.

i ), µi, ηi > 0 and ρJ = Cor(J 1

1 , J 2

Denote by ( (cid:101)Xj)j=1,...,365 the 2-dimensional Markov chain that is obtained by discretizing the
above SDE (7.1) with an Euler-scheme on the time interval interval [0, 1]. We assume that the
manager of the gas storage facility has the possibility to buy and sell gas on a predeﬁned set
of dates in the year {tj}j=1,...,J ⊂ {1, ..., 365} with tj = j · δt and some δt, J ∈ N such that
δt · J ≤ 365. The 2-dimensional Markov chain underlying the optimal control problem is then
given by X = (Xj)j=0,...,J with Xj := (cid:101)Xtj .

Recall from Example 2.4 that we assume that the volume of gas in the storage can only be
increased or decreased by a fraction ∆ = 1/N for some N ∈ N over the time interval of δt days.
The state space of the control variable is then given by L = {0, ∆, 2∆, ..., 1}. Also recall the
deﬁnition of the space of policies K, the constraint sets Kj and the function ϕj from Example
2.4. We assume that there is no trading at j = 0 hence K0 ≡ {0}. The cash-ﬂow underlying
to the optimal control problem only depends on the second component of the Markov chain Xj
and is given by

Hj(a, y, x) = −a · ∆ · x2 · e−rj(δt/365),

a ∈ K, y ∈ L, j = 1, ..., J,

where r > 0 is the interest rate.

We do not pay attention to the physical units of the parameters quantifying the gas storage
capacity and the quotation of the gas price, since the linearity of the pay-oﬀ with respect to the
parameter ∆ and the gas price X 2
j allows to properly scale the resulting value of the optimal
control problem. The following speciﬁc choice of the price model parameters are oriented at the

24

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

values in [TDR09]

(7.2)

β = 45, α1 = 0.25, α2 = 0.5,

σ1 = σ2 = 0.2,

ρW = 0.6,

λ = 2, µ1 = µ2 = 100,

η1 = η2 = 30,

ρJ = 0.6.

Figure 7 shows a sample trajectory of the Markov chain X with the above parameters.

Figure 7. A sample path of the Markov chain (Xj)j=0,...,J = ( (cid:101)Xtj )j=0,...,J
where tj = j · 7 and J = 52. The approximation (cid:101)X = ( (cid:101)Xj)j=1,...,365 to the SDE
(7.1) is simulated with the parameters given in (7.2) and (cid:101)X0 = (100, 100). X 1
and X 2 serve as models for the prices of crude oil and natural gas.

Further we deﬁne the following sets of polynomial regression basis functions

(7.3)

Pi(X 2) := (cid:8)(x1, x2) (cid:55)→ (x2)p (cid:12)
Pi(X 1, X 2) := (cid:8)(x1, x2) (cid:55)→ (x1)p(x2)q (cid:12)

(cid:12) p = 0, ..., i(cid:9)

(cid:12) p, q = 0, ..., i, p + q ≤ i(cid:9) .

I

0

Basis
P1(X 2)
P1(X 1, X 2)
P2(X 2)
P2(X 1, X 2)
P3(X 1, X 2)
P4(X 1, X 2)
1 P1(X 1, X 2)

v0(Y0, X0) Lower bounds
70.489 (0.066)
70.635 (0.068)
71.253 (0.068)
71.402 (0.068)
71.333 (0.081)
71.498 (0.068)
71.579 (0.070)

78.381
78.575
73.072
73.207
72.929
72.595
71.991
Table 3. Approximate values and lower bounds for the gas storage problem
with parameters given in (7.4) and price model parameters given in (7.2). Note
that - although seemingly so - the estimate v0 not necessarily presents an upper
bound to the true value and is included in the table only for veriﬁcation purposes.

The quantities were obtained with the standard regression method (I = 0) and the HRR
method (I = 1), the diﬀerent sets of basis functions (7.3), M = 105 training sample paths and
Mtest = 106 paths for calculating the lower bounds.

We have approximated the value of the gas storage problem with the following parameters

(7.4)

δt = 7,

J = 52, ∆ = 1/8, X0 = (100, 100),

Y0 = 4/8,

r = 0.1.

0102030405080100120140X1X2REINFORCED OPTIMAL CONTROL

25

In this conﬁguration the gas storage facility is initially loaded with half its capacity and the gas
storage manager has the possibility to trade gas every seven days, and the amount by which the
manager can inject or produce gas is one height of the total capacity. In Table 3 we present the
numerical results that were obtained with the standard regression method and the HRR method.
We used M = 105 training sample paths and Mtest = 106 sample paths for the calculation of
the lower bounds. For the set of reinforced basis functions in the HRR method we have chosen
Ly ≡ {Y0}, i.e. in each step of the backwards induction, the regression basis was reinforced with
only one function.

We observe at ﬁrst, that the lower bounds obtained with the standard regression method
are improved when using polynomials in both variables (X 1, X 2) instead of just in the second
variable X 2 (gas price) and are also improved when using polynomials of increasing order (with
the only exception of the third degree polynomials). Moreover, we observe that the lower bound
obtained with the HRR method, using the set of basis functions P1(X 1, X 2) and I = 1, lies
above all lower bounds that were obtained with the standard regression, in particular the bound
obtained with the regression basis P4(X 1, X 2) (up to Monte Carlo errors). Hence the HRR
method based on polynomials of degree one performed at least as well as the standard method
with polynomials of degree four.

7.3. Conclusions. Let us summarize the ﬁndings of the numerical experiments. We observe
that the hierarchical reinforced regression algorithm (HRR) based on polynomial basis functions
of a certain degree deg tend to produce results comparable to standard regression (SR) based on
polynomial basis functions of degree deg +1 or even higher, see Figures 4, 6, Tables 2, and, most
impressively, 3.

The numerical results also indicate that, indeed, HRR with low depth of the hierarchy I already
performs very well, even if I (cid:28) J, see Figures 4, 6 and Table 2. Hence, HRR performs with
similar accuracy to the reinforced regression algorithm (RR) of [BSSZ20], but at much improved
cost. Additionally, when comparing HRR with SR at ﬁxed accuracy, the computational cost of
HRR is usually much smaller, especially for d large, see Figure 5.

Finally, we note that the accuracy of the HRR method increases substantially when the time
discretization is reﬁned, i.e., when J is increased for ﬁxed time horizon T . This theoretically
very plausible observation (see Section 6) is backed up by numerical experiments, see Figure 6.

References

[AB04]

[Åst12]
[BCJ19]

L. Andersen and M. Broadie. A Primal-Dual Simulation Algorithm for Pricing Multi-Dimensional
American Options. Management Science, 50(9):1222–1234, 2004.
Karl J Åström. Introduction to stochastic control theory. Courier Corporation, 2012.
Sebastian Becker, Patrick Cheridito, and Arnulf Jentzen. Deep optimal stopping. Journal of Machine
Learning Research, 20:74, 2019.

[BG04]

[BKS09]

[BCJW21] Sebastian Becker, Patrick Cheridito, Arnulf Jentzen, and Timo Welti. Solving high-dimensional opti-
mal stopping problems using deep learning. European Journal of Applied Mathematics, 32(3):470–514,
2021.
Mark Broadie and Paul Glasserman. A stochastic mesh method for pricing high-dimensional American
options. Journal of Computational Finance, 7:35–72, 2004.
D. Belomestny, A. Kolodko, and J. Schoenmakers. Regression methods for stochastic control problems
and their convergence analysis. SIAM Journal on Control and Optimization, 48:3562–3588, 01 2009.
Nicole Bäuerle and Ulrich Rieder. Markov decision processes with applications to ﬁnance. Springer
Science & Business Media, 2011.
Christian Bayer, Martin Redmann, and John Schoenmakers. Dynamic programming for optimal
stopping via pseudo-regression. Quantitative Finance, 21(1):29–44, 2021.
Denis Belomestny and John Schoenmakers. Advanced simulation-based methods for optimal stopping
and control. Palgrave Macmillan, London, 2018. With applications in ﬁnance.
Denis Belomestny and John Schoenmakers. Optimal stopping of McKean–Vlasov diﬀusions via re-
gression on particle systems. SIAM Journal on Control and Optimization, 58(1):529–550, 2020.

[BRS21]

[BR11]

[BS20]

[BS18]

26

C. BAYER, D. BELOMESTNY, P. HAGER, P. PIGATO, J. SCHOENMAKERS, AND V. SPOKOINY

[BSSZ20] Denis Belomestny, John Schoenmakers, Vladimir Spokoiny, and Bakhyt Zharkynbay. Optimal stop-

[BSZ15]

[GHW11]

ping via reinforced regression. Communications in Mathematical Sciences, 16(1):109–121, 2020.
Christian Bender, John Schoenmakers, and Jianing Zhang. Dual representations for general multiple
stopping problems. Math. Finance, 25(2):339–370, 2015.
Lajos Gyurko, B. Hambly, and Jan Witte. Monte Carlo methods via a dual approach for some discrete
time stochastic control problems. Mathematical Methods of Operations Research, 81, 12 2011.

[GKKW02] László Györﬁ, Michael Kohler, Adam Krzyżak, and Harro Walk. A distribution-free theory of non-

[HE16]
[Pha09]

parametric regression. Springer Series in Statistics. Springer-Verlag, New York, 2002.
Jiequn Han and Weinan E. Deep learning approximation for stochastic control problems, 2016.
Huyên Pham. Continuous-time stochastic control and optimization with ﬁnancial applications, vol-
ume 61. Springer Science & Business Media, 2009.
L. C. G. Rogers. Monte Carlo valuation of American options. Math. Finance, 12(3):271–286, 2002.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
J. Schoenmakers. A pure martingale dual for multiple stopping. Finance Stoch., 16:319–334, 2012.

[Rog02]
[SB18]
[Sch12]
[TDR09] Matt Thompson, Matt Davison, and Henning Rasmussen. Natural gas storage valuation and opti-

[TVR01]

[Zan13]

[Zan18]

mization: A real options application. Naval Research Logistics (NRL), 56(3):226–238, 2009.
John Tsitsiklis and Benjamin Van Roy. Regression methods for pricing complex American style op-
tions. IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council,
12:694–703, 02 2001.
Daniel Z. Zanger. Quantitative error estimates for a least-squares Monte Carlo algorithm for American
option pricing. Finance Stoch., 17(3):503–534, 2013.
Daniel Z. Zanger. Convergence of a least-squares Monte Carlo algorithm for American option pricing
with dependent sample data. Math. Finance, 28(1):447–479, 2018.

Christian Bayer, Weierstrass Institute, Berlin, Germany
Email address: christian.bayer@wias-berlin.de

Denis Belomestny, Duisburg-Essen University, Germany and National University Higher School

of Economics, Russia

Email address: denis.belomestny@uni-due.de

Paul Hager, Institut für Mathematik, Humboldt Universität zu Berlin, Germany
Email address: paul.hager@hu-berlin.de

Paolo Pigato, Department of Economics and Finance, University of Rome Tor Vergata, Italy
Email address: paolo.pigato@uniroma2.it

John Schoenmakers, Weierstrass Institute, Berlin, Germany
Email address: john.schoenmakers@wias-berlin.de

Vladimir Spokoiny, Weierstrass Institute, Berlin, Germany
Email address: vladimir.spokoiny@wias-berlin.de

