Adversarial Neural Trip Recommendation

Linlang Jiang1, 2*, Jingbo Zhou2, Tong Xu1, Yanyan Li2, Hao Chen3, Jizhou Huang3, Hui Xiong4
1University of Science and Technology of China, 2Business Intelligence Lab, Baidu Research, 3Baidu Inc.
4Artiï¬cial Intelligence Thrust, The Hong Kong University of Science and Technology
linlang@mail.ustc.edu.cn, {zhoujingbo, liyanyanliyanyan, chenhao13, huangjizhou01}@baidu.com
tongxu@ustc.edu.cn, xionghui@ust.hk

1
2
0
2

p
e
S
4
2

]

G
L
.
s
c
[

1
v
1
3
7
1
1
.
9
0
1
2
:
v
i
X
r
a

Abstract

Trip recommender system, which targets at recommending a
trip consisting of several ordered Points of Interest (POIs),
has long been treated as an important application for many
location-based services. Currently, most prior arts generate
trips following pre-deï¬ned objectives based on constraint
programming, which may fail to reï¬‚ect the complex latent
patterns hidden in the human mobility data. And most of
these methods are usually difï¬cult to respond in real time
when the number of POIs is large. To that end, we propose an
Adversarial Neural Trip Recommendation (ANT) framework
to tackle the above challenges. First of all, we devise a novel
attention-based encoder-decoder trip generator that can learn
the correlations among POIs and generate well-designed trips
under given constraints. Another novelty of ANT relies on an
adversarial learning strategy integrating with reinforcement
learning to guide the trip generator to produce high-quality
trips. For this purpose, we introduce a discriminator, which
distinguishes the generated trips from real-life trips taken by
users, to provide reward signals to optimize the generator.
Moreover, we devise a novel pre-train schema based on learn-
ing from demonstration, which speeds up the convergence
to achieve a sufï¬cient-and-efï¬cient training process. Exten-
sive experiments on four real-world datasets validate the ef-
fectiveness and efï¬ciency of our proposed ANT framework,
which demonstrates that ANT could remarkably outperform
the state-of-the-art baselines with short response time.

1

INTRODUCTION

Trip recommendation (or trip planning) aims to recommend
a trip consisting of several ordered Points of Interest (POIs)
for a user to maximize the user experience. This problem has
been extensively investigated over the past years (Lim et al.
2015; Chen, Ong, and Xie 2016; He, Qi, and Ramamoha-
narao 2019). Most existing studies tackle the problem by a
two-stage process. First, they majorly exploit POI popular-
ity, user preferences, or POI co-occurrence to score POIs and
design various objective functions respectively. Then, they
model the trip recommendation problem as a combinatorial
problem: Orienteering problem (Golden, Levy, and Vohra
1987), and generate trips by maximizing the pre-deï¬ned ob-
jective with the help of constraint programming (CP).

*This work was done when the ï¬rst author was an intern in

Baidu Research under the supervision of the second author.
This is a preprint.

Though using this CP-based paradigm to solve such a
combinatorial problem is very popular over the past years,
its drawbacks are still obvious. First, the recommended trips
by such methods are optimized by the pre-deï¬ned objec-
tive function, which may not follow the latent patterns hid-
den in the human mobility data generated by users. For in-
stance, according to the statistics from a real-life trip dataset
from Beijing (see Experiment section), after watching a ï¬lm,
26% users choose to go to a restaurant while only less than
1% users choose to go to a Karaoke bar. However, the pre-
deï¬ned objective may not be capable of capturing such mo-
bility sequential preferences and generate unusual trips like
(cinema â†’ Karaoke bar). Second, the time complexity of
such CP-based methods is usually too high to handle hun-
dreds of POIs in a city in real time. As shown in our exper-
iment section, the response time of such methods with 100
POIs can be more than 1 minute. Such a weakness is very
disruptive to the user experience.

To this end, we propose an Adversarial Neural Trip Rec-
ommendation (ANT) framework to solve the challenges
mentioned above. At ï¬rst, we propose an encoder-decoder
based trip generator that can generate the trip under given
constraints in an end-to-end fashion. Concretely, the en-
coder takes advantage of multi-head self-attention to cap-
ture correlations among POIs. Afterwards, the decoder sub-
sequently selects POI into a trip with mask mechanism to
meet the given constraints while maintaining a novel context
embedding to represent the contextual environment when
choosing POIs. Second, we devise an adversarial learning
strategy into the specially designed reinforcement learning
paradigm to train the generator. Speciï¬cally, we introduce
a discriminator to distinguish the real-life trips taken by
users from the trips generated by the trip generator for bet-
ter learning the latent human mobility patterns. During the
training process, once trips are produced by the generator,
they will be evaluated by the discriminator while the feed-
back from the discriminator can be regarded as reward sig-
nals to optimize the generator. Therefore, the generator will
push itself to generate high-quality trips to obtain high re-
wards from the discriminator. Finally, a signiï¬cant distinc-
tion of our framework from existing trip planning methods is
that we do not adopt the traditional constraint programming
methodology. Considering the excellent performance for in-
ference(prediction) of the deep-learning (DL) based models,

 
 
 
 
 
 
the efï¬ciency of our method is much better than such CP-
based methods.

To sum up, the contributions of this paper can be summa-

rized as follows:
â€¢ To the best of our knowledge, we are the ï¬rst to propose
an end-to-end DL-based framework to study the trip rec-
ommendation problem.

â€¢ We devise a novel encoder-decoder model to generate
trips under given constraints. Furthermore, we propose
an adversarial learning strategy integrating with rein-
forcement learning to guide the trip generator to produce
trips that follow the latent human mobility patterns.

â€¢ We conduct extensive experiments on four large-scale
real-world datasets. The results demonstrate that ANT
remarkably outperforms the state-of-the-art techniques
from both effectiveness and efï¬ciency perspectives.

2 RELATED WORK
Our study is related with POI recommendation and trip rec-
ommendation problems which are brieï¬‚y discussed in this
section respectively.

2.1 POI Recommendation
POI recommendation usually takes the userâ€™s historical
check-ins as input and aims to predict the POIs that the user
is interested in. This problem has been extensively investi-
gated in the past years. For example, Yang et al. (2017) pro-
posed to jointly learn user embeddings and POI embeddings
simultaneously to fully comprehend user-POI interactions
and predict user preference on POIs under various contexts.
Ma et al. (2018) investigated to utilize attention mechanism
to seek what factors of POIs users are concerned about, inte-
grating with geographical inï¬‚uence. Luo et al. (2020) stud-
ied to build a multi-level POI recommendation model with
considering the POIs in different spatial granularity levels.
However, such methods target on recommending an individ-
ual POI not a sequence of POI, and do not consider the de-
pendence and correlations among POIs. In addition, these
methods do not take time budget into consideration while it
is vital to recommend trips under the time budget constraint.

2.2 Trip Recommendation
Trip recommendation aims to recommend a sequence of
POIs (i.e. trip) to maximize user experience under given
constraints. Lim et al. (2015) focused on user interest based
on visit duration and personalize the POI visit duration for
different users. Chen, Ong, and Xie (2016) modeled the
POI transit probabilities, integrating with some manually
designed features to suggest trips. Another study modeled
POIs and users in a uniï¬ed latent space by integrating the
co-occurrences of POIs, user preferences and POI popu-
larity(He, Qi, and Ramamohanarao 2019). These methods
above share similar constraints: a start POI, an end POI and
a time budget or trip length constraint, and they all maximize
respective pre-deï¬ned objectives by adopting constraint pro-
gramming. However, such pre-deï¬ned objectives may fail
to generate trips that follow the latent human mobility pat-
terns among POIs. Different from these methods, Gu et al.

(2020) focused on the attractiveness of the routes between
POIs to recommend trips and generate trips by using greedy
algorithm. However, only modeling users and POIs in cate-
gory space may not be capable of learning the complex hu-
man mobility patterns. The prediction performance based on
greedy strategy is also not satisï¬ed enough.

3 PRELIMINARIES

In this section, we ï¬rst introduce the basic concepts and no-
tations, and then we give a formal deï¬nition of the trip rec-
ommendation problem.

3.1 Settings and Concepts

A POI l is a unique location with geographical coordinates
(Î±, Î²) and a category c, i.e. l =< (Î±, Î²), c >. A check-
in is a record that indicates a user u arrives in a POI l at
timestamp ta and leaves at timestamp td, which can be rep-
resented as r = (u, l, ta, td). We denote all check-ins as R
and the check-ins on a speciï¬c location l as Rl. Since we
have the check-ins generated by users, we can estimate the
user duration time on POIs. Given a POI l and correspond-
ing check-in data Rl, the expected duration time of a user
spends on the POI is denoted by Td(l), which is the average
duration time of all check-ins on location l:

(cid:80)

td âˆ’ ta

Td(l) =

(u,l,ta,td)âˆˆRl
|Rl|

(1)

We denote the transit time from a POI li to another POI lj
as Te(li, lj). The time cost along one trip can be calculated
by summing all the duration time of each POI and all the
time cost on the transit between POIs. In our experiment,
the transit time is estimated by the distance between POIs
and the walking speed of the user (e.g. 2m/s).

A trip is an ordered sequence of POIs S = l0 â†’ l1 â†’
Â· Â· Â· â†’ ln. Given a query user u, a time budget Tmax and
a start POI l0, we aim to plan a trip S = l0 â†’ l1 â†’
Â· Â· Â· â†’ ln for the user. We name the query user, the start
POI and the time budget a trip query, denoted as a triple
q = (u, l0, Tmax).

3.2 Trip Recommendation

Now we deï¬ne the trip recommendation problem formally.
Given a trip query q = (u, l0, Tmax), we aim to recommend
a well-designed trip that does not exceed the time budget
and maximize the likelihood that the user will follow the
planned trip. For convenience, we denote the sum of transit
time from current POI to the next POI and duration time on
the next POI as Ta(Si, Si+1) = Td(Si+1) + Te(Si, Si+1),
Si is the i-th POI in trip S. So the time cost on the planned
trip denoted as T (S) can be calculated by T (S) = Td(S0)+
|S|âˆ’1
(cid:80)
i=0
as follows:

Ta(Si, Si+1). Overall, the problem can be formulated

max
T (S)â‰¤Tmax

P (S | q)

(2)

Figure 1: An overview of the proposed framework.

4 APPROACH
The overall framework of ANT is shown in Figure 1. We
ï¬rst selectively retrieve hundreds of POIs to construct a can-
didate set. Next, we use a well-devised novel time-aware trip
generator G to generate the well-planned trip for users with
incorporating the time budget and the POI correlation.

The trip generation process can be considered as a se-
quential decision process, that is to say, at each step there
is a smart agent to select the best POI which can ï¬nally
form an optimal trip. Thus, we model the trip generation
procedure as a Markov Decision Process(MDP)(Bellman
1957), where we regard selecting POI as action, the prob-
ability distribution on POIs to be selected as a stochastic
policy, and contextual environment(e.g. available time, se-
lected POIs) when selecting POIs as state. Therefore, our
goal is to learn an optimal policy, which guarantees that the
agent can always take the best action, i.e. the POI with the
highest probability is the most promising option. To train the
policy, we construct a discriminator D (following the Gen-
erative Adversarial Networks(GAN) structure (Goodfellow
et al. 2014)) to provide feedback compared with the real-life
trips taken by users. Therefore, the generator can be trained
through policy gradient by reinforcement learning to draw
the generated trips and the real-life trips closer.

4.1 Candidate Construction
As for a trip query, it is usually not necessary to take all
the POIs into consideration to plan a reasonable trip. For in-
stance, those POIs that are too far away from the start POI
are impossible to be part of the trip. Here we propose a rule-
based retrieval procedure to pick up a small amount of POI
candidates from the large POI corpus, named candidate set,
which incorporates the impact of connection among trips
and geographic inï¬‚uence.

Drawing Lessons from Other Trips
If a user requests a
trip at the start location l0, former trips that are associated
with l0 are promising to provide a reference. Inspired by this,
we could assume that given the start POI l0 of a trip query,
those POIs that once co-occurred with l0 in the same trip
could be potential options for the trip query, which can be
named drawing lessons from other trips. Hypergraph pro-
vides a natural way to gather POIs belonging to different
trips and also to glimpse other trips via hyperedges.

Figure 2: An instance of hypergraph construction.

Deï¬nition 1 (Trip Hypergraph). Let G = (L, E) denote a
hypergraph, where L is the vertex set and E is the hyperedge
set. Each vertex represents a POI li and each hyperedge e âˆˆ
E connects two or more vertices, representing a trip.

Speciï¬cally, we use trips in the training set to build the
trip hypergraph. On one hand, all the POIs in the same trip
are linked by a hyperedge, which preserves the matching
information between POIs and trips. On the other hand, a
POI may exist in arbitrary hyperedges, connecting different
trips via hyperedges. Given a trip query (u, l0, Tmax), POIs
that are connected with l0 via hyperedges are promising to
be visited for the upcoming trip request, so we directly add
them into the candidate set. Figure 2 is a simple example of
trip hypergraph retrieval. If the start POI of the upcoming
trip query is l2, POIs {l1, l3, l4, l5, l6, l7, l8} will be added
into the candidate set for the corresponding trip query.

Spatial Retrieval Distance between users and POIs is a
crucial factor affecting userâ€™s decisions in location-relative
recommendation. It is typical that a userâ€™s check-ins are
mostly centralized on several areas(Hao et al. 2016, 2020),
which is the famous geographical clustering phenomenon
and is adopted by earlier work to enhance the performance of
location recommendation (Ma et al. 2018; Lian et al. 2014;
Li et al. 2015). Therefore, except for the candidates gen-
erated by hypergraph, we also add POIs into candidate set
from near to far. In our framework, we generate ï¬xed-length
candidate sets for every trip query, denoted as Aq for the cor-
responding trip query q. We ï¬rst use the hypergraph to gen-
erate candidates and then use the spatial retrieval. In other
words, if the numbers of candidates generated by the hyper-
graph retrieval for different trip queries are smaller than the
pre-deï¬ned number |Aq|, we pad the candidate set with the
sorted POIs in order of distance to a ï¬xed length.

4.2 Time-aware Trip Generator

As shown in Figure 3, the generator consists of two main
components: 1) a POI correlation encoding module (i.e., the
encoder), which outputs the representation of all POIs in
the candidate set; 2) a trip generation module (i.e., the de-
coder), which selects location sequentially by maintaining a
special context embedding, and keeps the time budget con-
straint satisï¬ed by masking mechanism.

Joint Embedding Given the trip query (u, l0, Tmax) and
the corresponding selected candidate set Aq, we use a simple

Time-awareTrip GeneratorMobilityDiscriminatorPlanningTripPolicyGradientMycurrentlocationisğ‘™!,pleaseplanatripformewithintimeğ‘‡"#$.CandidatePOIsReal Tripğ‘™!ğ‘™"ğ‘™#ğ‘™$ğ‘™%ğ‘™&ğ‘™â€™ğ‘™(ğ‘†!=ğ‘™",ğ‘™#,ğ‘™$ğ‘†#=ğ‘™%,ğ‘™#,ğ‘™&ğ‘†â€™=ğ‘™!,ğ‘™#,ğ‘™â€™,ğ‘™(Tripsğ‘’"ğ‘’!ğ‘’#where 1 â‰¤ i â‰¤ M, WQ, WK, WV âˆˆ RdÃ—dh , dh = d/M ,
M is the number of heads and dh is the dimension for each
head. The scaled dot-product attention computes as:

Attn(Q, K, V) = sof tmax(

QKT
âˆš
dh

)V

(5)

where the softmax is row-wise. M attention heads are able
to capture different aspects of attention information and the
results from each head are concatenated followed by a linear
projection to get the ï¬nal output of the MHA. We compute
the output of MHA sublayer as:

Ë†H(l) = [head(l)

1 ; Â· Â· Â· ; head(l)

M ]WO

(6)

where WO âˆˆ RdÃ—d. We endow the encoder with nonlin-
earity by adding interactions between dimensions by using
the FFN sublayer. The FFN we apply is a two-layer feed-
forward network, whose output is computed as:

H(l) = ReLu( Ë†H(l)Wf 1 + bf 1)Wf 2 + bf 2
(7)
where Wf 1 âˆˆ RdÃ—df , Wf âˆˆ Rdf Ã—d. Note that all the pa-
rameters for each attention layer is unique.

Besides, to stabilize and speed up converging, the multi-
head attention and feed-forward network are both followed
by skip connection and batch normalization (Vaswani et al.
2017). To sum up, by considering the interactions and inner
relationship among POIs, the encoder transforms the origi-
nal embeddings of POIs into informative representations.

Trip Generation It is of great importance to consider the
contextual environment when planning the trip so we design
a novel context embedding integrating candidate informa-
tion, time budget and selected POIs.

Self-Attention Context Embedding. By aggregating the
location embeddings, we apply a mean pooling of ï¬nal lo-
cation embedding Â¯h(L) = 1
N

as candidate embed-

h(L)
i

N
(cid:80)
i=1

ding. During the process of decoding, the decoder selects a
POI from the candidate set once at a time based on selected
POIs St(cid:48), t(cid:48) < t and the available time left . We keep track
of the remaining available time Tt at time step t. Initially
T1 = Tmax âˆ’ Td(S0), and Tt is updated as:

Tt+1 = Tt âˆ’ Ta(Stâˆ’1, St), t â‰¥ 1

(8)

where S0 = l0. Following existing methods to represent the
contextual environment in the procedure of decoding (Bello
et al. 2017; Kool, van Hoof, and Welling 2019), we employ
a novel context embedding hc conditioned on candidate set
and remaining time, which will change along the decoding
proceeds. The context embedding hc is deï¬ned as:
hc = [Â¯h(L); h(L)
Stâˆ’1

; Tt], t â‰¥ 1

(9)

where hc âˆˆ R1Ã—(2d+1).

Before deciding which POI to add into the trip at time step
t, it is important to look back the information about can-
didates and remind ourselves which POIs are optional and
which POIs should not be considered because they break the
given constraints. Therefore, we ï¬rst glimpse the candidates

Figure 3: Illustration of time-aware trip generator.

linear transform to combine the user u and the POI li in Aq
with its category c for embedding:
h(0)
i = [xli; xc; xu]WI + bI
(3)
where xli, xc, xu are POI embedding, category embedding
and user embedding (which are all trainable embedding),
[a; b; c] means concatenation of vectors a, b, c, and WI, bI
are trainable parameters. Thus, we get the matrix presenta-
tion of the candidates H(0) âˆˆ RN Ã—d, each row of H(0) is
the representation of a POI in the candidate set.

POI Correlation Encoding We apply a self-attention en-
coder to produce the representation of locations. The rea-
sons to use the self-attention encoder can be justiï¬ed from
two perspectives. The ï¬rst reason is due to the permutation
invariance for sets. For a candidate set, the order of POIs in
this set is invariant to the ï¬nal result, i.e. any permutation of
the inputs is supposed to produce the same output represen-
tation. Thus, we do not adopt the classical RNN-based en-
coder architecture because it focuses on the sequential infor-
mation of the inputs, which is not suitable for our problem.
Second, a reasonable generated trip is supposed to consider
the relationship between POIs. For instance, after staying at
a restaurant for a while a person is more interested in other
kinds of POIs but not another restaurant. So it is helpful to
produce a POI representation with attention to other POIs.

The encoder we apply is similar to the encoder used in
the Transformer architecture (Vaswani et al. 2017) while we
remove the position encoding, which is not suitable for our
problem. We stack multiple attention layers and each layer
has the same sublayers: a multi-head attention(MHA), and
a point-wise feed-forward network(FFN). The initial input
of the ï¬rst attention layer is H(0) and we apply the scaled
dot-product attention for each head in layer l as:

head(l)

i = Attn(H(lâˆ’1)WQ, H(lâˆ’1)WK, H(lâˆ’1)WV )

(4)

Multi-Head Self-AttentionCandidate POI listâ€¦Add & NormFeed Forwardâ€¦â€¦Time-aware Maskâ„!!PoolingSelf-Attention ContextEmbeddingSelf-AttentionPredictionâ€¦â€¦ğ‘†!Repeat until ğ‘‡!"#ğ’’$ğ’Œ%ğ’Œ&ğ’Œâ€™Generated trip ğ‘†!:#$!userğ’‰!(&)ğ’‰((&)ğ’‰)(&)ğ’‰*(&)ğ’‰+(&)(c) TripGeneration Module(b) POI CorrelationEncoding Module(a) JointEmbeddingğ‘†(ğ‘†#ğ‘†#$!ğ‘‡"Add & Normthat are optional, i.e. are never selected before and do not
exceed the time budget, and then integrate the information
with attention to the output from the encoder:
K vi = h(L)
Q ki = h(L)
i Wc
V
qckT
jâˆš
d

Î˜(Tt âˆ’ Ta(Stâˆ’1, lj))exp(

qc = hcWc

i Wc

(10)

)

Î±tj =

(cid:80)

lmâˆˆAq\S0:tâˆ’1

Î˜(Tt âˆ’ Ta(Stâˆ’1, lm))exp( qckT
mâˆš
d

)

Q âˆˆ R(2d+1)Ã—d, Wc

(11)
where Wc
is the
i-th row of the location embedding matrix H(L), and Î˜(Â·) is
a Heaviside step function, which plays a crucial role as the
time-aware mask operator. Thus, the reï¬ned context embed-
ding Â¯hc is computed as:

V âˆˆ RdÃ—d, h(L)

K, Wc

i

(cid:88)

Â¯hc =

Î±tj Â· vj

lj âˆˆAq

(12)

We omit the multi-head due to the page limit.

Self-Attention Prediction. After getting the reï¬ned con-
text embedding, we apply a ï¬nal attention layer with a single
attention head with mask mechanism.

(cid:40) Â¯hckT
jâˆš
d

ucj =

otherwise.

(13)

(14)

âˆ’âˆ if lj âˆˆ S0:tâˆ’1 or Tt < Ta(Stâˆ’1, lj).

Finally, the softmax is applied to get the probability distri-
bution:

p(St = lj|Â¯hc) =

eucj

(cid:80)

lmâˆˆAq

eucm

The decoding proceeds until there is no enough time left
and then we get the entire trip generated by the decoder S0:t.
To sum up, by maintaining a context embedding and using
the representation of location from the encoder, the decoder
constructs a trip with attention mechanism and meets the
constraints by mask mechanism.

4.3 Policy Optimization by Adversarial Learning
The next problem is how to train the encoder-decoder frame-
work for trip generation. We devise a mobility discriminator
to distinguish real-life trips taken by users between gener-
ated trips, which provides feedback to guide the optimiza-
tion of the trip generator. After the evaluation between gen-
erated trips and real-life trips, the output of the discriminator
can be regarded as reward signals to improve the generator.
By the adversarial procedure, the generator pushes itself to
generate high-quality trips to obtain high rewards from the
discriminator.

Mobility Discriminator The task for the discriminator es-
sentially is binary classiï¬cation. Here we apply a simple but
effective one-layer Gated Recurrent Unit (GRU) (Cho et al.
2014), followed by a two-layer feed-forward network to ac-
complish this task. We denote the mobility discriminator as
DÏ† and the trip generator as GÏ†, where Î¸ and Ï† represent the
parameters of the generator and discriminator respectively.
We denote all the real-life trips as Pdata. As a binary classi-
ï¬cation task, we train the discriminator DÏ† as follows:

max
Ï†

E Ë†Sâˆ¼Pdata

[log DÏ†( Ë†S)]+ESâˆ¼GÎ¸ [log(1âˆ’DÏ†(S))] (15)

Adversarial Learning with Policy Gradient We adopt
the reinforcement learning technique to train the generator.
The standard training algorithm for GAN does not apply
to our framework: the discrete output of the trip generator
blocks the gradient back-propagation, making it unable to
optimize the generator (Yu et al. 2017). As described pre-
viously, the trip generation process is a sequential decision
problem, leading us to tackle the problem by adopting rein-
forcement learning techniques. With modeling the trip gen-
eration procedure as an MDP, an important setting is to re-
gard the score from the discriminator as reward. Thus, we
deï¬ne the loss as: L(S) = EpÎ¸(S|q)[DÏ†(S)], which rep-
resents the expected score for the generated trip S given
trip query q. Following REINFORCE (Williams 1992) al-
gorithm, we optimize the loss by gradient ascent:
âˆ‡L(Î¸ | q) = EpÎ¸(S|q)[DÏ†(S)âˆ‡ log pÎ¸(S | q)]

(16)

Learning from Demonstration In order to accelerate the
training process and further improve the performance, we
propose a novel pre-train schema based on learning from
demonstration (Silver, Bagnell, and Stentz 2010), which not
only fully utilizes the data of real-life trips but also obtains
a decent trip generator before adversarial learning. Learn-
ing directly from rewards is sample-inefï¬cient and hard to
achieve the promising performance (Yu et al. 2017), which
is also our reason to introduce the pre-train schema. During
pre-training, we use real-life trips taken as ground-truth, re-
gard choosing POI at each time step as a multi-classiï¬cation
problem and optimize by softmax loss function. Neverthe-
less, during inference, the trip generator needs the preceding
POI to select the next POI while we have no access to the
true preceding POI in training, which may lead to cumula-
tive poor decisions (Bengio et al. 2015). To bridge such a
gap between training and inference, we select POI by sam-
pling with the probability distribution (deï¬ned in Equation
14) during training. Finally, the loss can be computed as:

Lc = âˆ’

(cid:88)

| Ë†S|
(cid:88)

Ë†SâˆˆPdata

t=1

log p( Ë†St|S0:tâˆ’1; Î¸)

(17)

where S is the actual generated trip during training and Ë†S is
the corresponding real-life trip.

Teacher Forcing The training process is usually unsta-
ble by optimizing the generator with Equation 16 (Li et al.
2017). The reason behind this is that once the generator de-
teriorates in some training batches and the discriminator will
recognize the unreasonable trips soon, then the generator
will be lost. The generator knows the generated trips are not
good based on the received rewards from the discriminator,
but it does not know how to improve the quality of gener-
ated trips. To alleviate this issue and give the generator more
access to real-life trips, after we update the generator with
adversarial loss, we also feed the generator real-life trips and
update it with supervised loss(Equation 17) again.

To sum up, we ï¬rst pre-train the trip generator by lever-
aging demonstration data. Afterwards, we alternately update
the discriminator and the generator with the respective ob-
jective. During updating the generator, we also feed real-life

Dataset

Foursquare

Map

NYC

Tokyo Beijing Chengdu

# users
# POIs
# trips

796
8619
16518

2019
14117
58893

22399
13008
212758

8869
8914
95166

Table 1: Dataset statistics.

trips to the generator, regulating the generator from devia-
tion from the demonstration data.

5 EXPERIMENTS

5.1 Experimental Setups
Dataset We use four real-world POI check-in datasets and
Table 1 summarizes the statistics of the four datasets.

Foursquare(Yang et al. 2015) This real-world check-in
dataset includes check-in data in New York City and Tokyo
collected from Foursquare. We sort the check-ins of a user
by timestamp and split them into non-overlapping trips. If
the time interval between two successive check-ins is more
than ï¬ve hours, we split them into two trips.

Map This dataset collects real-world check-ins in Beijing
and Chengdu from 1 July 2019 to 30 September 2019 from
an online map service provider in China. We consider the
check-ins in one day as a trip for a user.

We remove the trips of which length is less than 3 and we
remove the POIs visited by fewer than 5 users as they are
outliers in the dataset. We split the datasets in chronological
order, where the former 80 % for training, the medium 10 %
for validation, and the last 10 % for testing.

Baselines We compare the performance of our proposed
method with three state-of-the-art baselines that are de-
signed for trip recommendation: TRAR (Gu et al. 2020)
proposes the concept of attractive routes and enhances trip
recommendation with attractive routes. PERSTOUR (Lim
et al. 2015) personalizes the duration time for each user
based on their preferences and generates trips to maximize
user preference. C-ILP (He, Qi, and Ramamohanarao 2019)
learns a context-aware POI embedding by integrating POI
co-occurrences, user preferences and POI popularity, and
transforms the problem to an integer linear programming
problem. For C-ILP and PERSTOUR, we ï¬rst generate 100
candidates by using our proposed retrieval procedure be-
cause larger candidates can not be solved in a tolerable
time. C-ILP and PERSTOUR both utilize lpsolve (Berkelaar
et al. 2004), a linear programming package, to generate trips
among the candidates, which follows their implementation.
To fully validate the effectiveness of our proposed
method, we introduce some baselines designed for POI rec-
ommendation. These baselines share the same trip gener-
ation procedure: they repeatedly choose the POI with the
highest score among all the unvisited POIs until the time
budget exhausts. The scores of POIs are produced by the
corresponding model in SAE-NAD and GRU4Rec while the
scores are popularity(visit frequency) of POIs in POP. POP
is a naive method that measures the popularity of POIs by

counting the visit frequency of POIs. SAE-NAD (Ma et al.
2018) applies a self-attentive encoder for presenting POIs
and a neighbor-aware decoder for exploiting the geograph-
ical inï¬‚uence. GRU4Rec (Hidasi et al. 2016) models the
sequential information by GRU. The implementation and
hyper-parameters will be reported in the appendix.

Evaluation Metrics A trip is determined by the POIs that
compose the trip and the order of POIs in the trip. We eval-
uate these two aspects by Hit Ratio and Order-aware Pre-
cision (Huang et al. 2019) respectively. These two metrics
are popularly used for trip recommendation (and planning)
in previous studies.

Hit Ratio. Hit Ratio (HR) is a recall-based metric, which
measures how many POIs in the real trip are covered in the
planned trip except the start POI: HR = |Sâˆ© Ë†S|âˆ’1
| Ë†S|âˆ’1

.

Order-aware Sequence Precision (Huang et al. 2019).
Order-aware Sequence Precision (OSP) measures the order
precision of overlapped part between the real trip and the
generated trip except the start POI, which is deï¬ned as:
OSP = M/B where B is the number of all POI pairs in the
overlapped part and M is the number of the pairs that contain
the correct order. We give an example in the appendix.

5.2 Experimental Results

Effectiveness Table 2 shows the performance under HR
and OSP metrics on the four datasets with respect to differ-
ent methods. It can be observed that our proposed method
consistently outperforms all the baselines with a signiï¬cant
margin on all the four datasets, especially on OSP, which
demonstrates that our method can recommend high-quality
trips. PERSTOUR and CILP are both based on integer linear
programming, which restricts them to respond in real time
when the number of locations is large and affects their per-
formance. TRAR is ill-behaved because modeling users and
POIs only in category space are not enough to extract infor-
mative features to recommend reasonable trips. SAE-NAD
is a strong baseline with good performance on HR while it
performs poorly on OSP, which validates that conventional
POI recommendation methods are not capable of being ex-
tended to support trip recommendation directly.

Due to the page limit, we omit the results on Beijing and
Chengdu if without speciï¬cation in the following analysis
and the conclusions are similar on these two datasets, which
can be found in the appendix.

Efï¬ciency Besides the high prediction accuracy, another
advantage of our framework is its good efï¬ciency that is in-
vestigated in this section. We compare the running time of
ANT with trip recommendation baselines, i.e. TRAR, C-ILP
and PERSTOUR. Even though ANT can be parallelized, for
fair comparison we make ANT generate trips serially and we
run all four methods on the same CPU device (Intel 6258R).
The average running time of C-ILP and PERSTOUR on an
instance both exceed one minute, while the average running
time of ANT is less than 45 ms, which demonstrates the su-
periority of our model in efï¬ciency compared to traditional
CP-based models. Even though TRAR is faster than ANT

Method

NYC

Tokyo

Beijing

Chengdu

HR

OSP

HR

OSP

HR

OSP

HR

OSP

POP
SAE-NAD
GRU4Rec
TRAR

0.0397
0.0119
0.0181
0.0047
PERSTOUR 0.0075
0.0449
0.2103

C-ILP
ANT

0.0036
0.0001
0.0012
0.0020
0.0013
0.0031
0.1154

0.0482
0.0875
0.0276
0.0010
0.0197
0.0241
0.1922

0.0128
0.0003
0.0018
0.0001
0.0048
0.0012
0.1232

0.0461
0.1345
0.0307
0.0013
0.0134
0.0160
0.1610

0.0102
0.0005
0.0020
0.0001
0.0021
0.0001
0.0388

0.0483
0.1220
0.0206
0.0027
0.0145
0.0175
0.1348

0.0131
0.0005
0.0015
0.0001
0.0024
0.0003
0.0349

Table 2: Comparison with baselines.

Figure 4: Running time compared with baselines and run-
ning time on different numbers of candidates.

Figure 6: The impact of the number of candidates.

Figure 5: Ablation study of each component.

with the help of the greedy algorithm but TRARâ€™s perfor-
mance is much worse than ANT, even worse than PERS-
TOUR and C-ILP. To further validate ANTâ€™s availability to
scale up to various numbers of candidates, we run ANT con-
ditioned on varying numbers of candidates and show the re-
sult in Figure 4, which shows that the inference time of ANT
is relatively stable with different numbers of candidates.

Ablation Study To analyze the effect of each component
of the ANT framework, we conduct an experimental eval-
uation on four variants of ANT: ANT-E, ANT-D, ANT-A,
ANT-P. ANT-E means to remove the POI correlation mod-
ule, ANT-D means the trip generation module is replaced
with Pointer Networks (Vinyals, Fortunato, and Jaitly 2015),
ANT-A means that we train the whole model only using
learning from demonstration and ANT-P means we train the
model only using the adversarial learning. Due to the page
limit, we omit the performance under OSP, which can be
found in the appendix.

As can be observed in Figure 5, each component makes
contributions to the ï¬nal performance. Training our model
without pre-training leads to huge performance degradation,
indicating the effectiveness of pre-training on stabilizing and
speeding up the training process. Adversarial learning makes
the model further improved based on pre-training. Also, re-
moving the POI correlation module leads to a performance

drop, indicating the necessity of multi-head self-attention to
capture the POI correlation. And compared to Pointer Net-
works, the well-designed context embedding for trip recom-
mendation also shows its superiority.

Impact of Candidates Here we evaluate the impact of
candidates on the performance of ANT. Intuitively, when in-
creasing the number of candidates, the target POIs have a
high probability to be included in the candidate set for trip
recommendation, but it also raises the difï¬culty to plan the
correct trips. As we can see from Figure 6, when the number
of candidates is larger than 200, the prediction performance
of ANT (under HR and OSP) on the NYC dataset becomes
relatively stable with the number of candidates increasing.
The same phenomenon can be found on the Tokyo dataset
when the number of candidates is larger than 250. Therefore,
the performance is not sensitive to the number of candidates
if the number is relatively large enough. In this experimental
evaluation, we set the number of candidates to 200, which
can be also adjusted according to different characteristics of
different cities.

6 CONCLUSION

In this paper, we investigated the trip recommendation prob-
lem by an end-to-end deep learning model. Along this line,
we devised an encoder-decoder based trip generator to learn
a well-formed policy to select the optimal POI at each time
step by integrating POI correlation and contextual environ-
ment. Especially, we proposed a novel adversarial learning
strategy integrating with reinforcement learning to train the
trip generator. The extensive results on four large-scale real-
world datasets demonstrate our framework could remark-
ably outperform the state-of-the-art baselines both on effec-
tiveness and efï¬ciency.

NYCTokyoDataset10âˆ’410âˆ’2100102104106Running Time (s)TRARPERSTOURC-ILPANT100150200250300Number of Candidates0.0300.0350.0400.0450.050Running Time (s)NYCTokyoNYCTokyo0.100.150.200.25HRANT-PANT-EANT-DANT-AANTBeijingChengdu0.050.100.150.20HRANT-PANT-EANT-DANT-AANT100150200250300Nunmber of Candidates0.100.150.200.250.30HRNYCTokyo100150200250300Nunmber of Candidates0.050.100.150.20OSPNYCTokyoLian, D.; Zhao, C.; Xie, X.; Sun, G.; Chen, E.; and Rui,
Y. 2014. GeoMF: joint geographical modeling and ma-
trix factorization for point-of-interest recommendation. In
SIGKDD, 831â€“840.
Lim, K. H.; Chan, J.; Leckie, C.; and Karunasekera, S. 2015.
Personalized Tour Recommendation Based on User Interests
and Points of Interest Visit Durations. In AAAI, IJCAIâ€™15,
1778â€“1784. AAAI Press.
Luo, H.; Zhou, J.; Bao, Z.; Li, S.; Culpepper, J. S.; Ying, H.;
Liu, H.; and Xiong, H. 2020. Spatial object recommendation
with hints: When spatial granularity matters. In SIGIR, 781â€“
790.
Ma, C.; Zhang, Y.; Wang, Q.; and Liu, X. 2018. Point-of-
interest recommendation: Exploiting self-attentive autoen-
coders with neighbor-aware inï¬‚uence. In CIKM, 697â€“706.
Silver, D.; Bagnell, J. A.; and Stentz, A. 2010. Learning
from demonstration for autonomous navigation in complex
unstructured terrain. The International Journal of Robotics
Research, 29(12): 1565â€“1592.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.
Attention is All you Need. In NIPS, volume 30, 5998â€“6008.
Vinyals, O.; Fortunato, M.; and Jaitly, N. 2015. Pointer Net-
works. In NIPS, 2692â€“2700.
Williams, R. J. 1992. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chine learning, 8(3-4): 229â€“256.
Yang, C.; Bai, L.; Zhang, C.; Yuan, Q.; and Han, J.
2017. Bridging Collaborative Filtering and Semi-Supervised
Learning: A Neural Approach for POI Recommendation. In
SIGKDD, 1245â€“1254. Association for Computing Machin-
ery.
Yang, D.; Zhang, D.; Zheng, V. W.; and Yu, Z. 2015. Mod-
eling User Activity Preference by Leveraging User Spatial
Temporal Characteristics in LBSNs. IEEE Transactions on
Systems, Man, and Cybernetics: Systems, 45(1): 129â€“142.
Yu, L.; Zhang, W.; Wang, J.; and Yu, Y. 2017. SeqGAN:
Sequence Generative Adversarial Nets with Policy Gradient.
In AAAI, 2852â€“2858. AAAI Press.

References
Bellman, R. 1957. A Markovian decision process. Journal
of mathematics and mechanics, 6(5): 679â€“684.
Bello, I.; Pham, H.; Le, Q. V.; Norouzi, M.; and Bengio, S.
2017. Neural Combinatorial Optimization with Reinforce-
ment Learning. In ICLR.
Bengio, S.; Vinyals, O.; Jaitly, N.; and Shazeer, N. 2015.
Scheduled Sampling for Sequence Prediction with Recur-
rent Neural Networks. In NIPS, 1171â€“1179.
Berkelaar, M.; Eikland, K.; Notebaert, P.; et al. 2004.
lp-
solve: Open source (mixed-integer) linear programming sys-
tem. Eindhoven U. of Technology, 63.
Chen, D.; Ong, C. S.; and Xie, L. 2016. Learning points and
routes to recommend trajectories. In CIKM, 2227â€“2232.
Cho, K.; Van MerriÂ¨enboer, B.; Gulcehre, C.; Bahdanau, D.;
Bougares, F.; Schwenk, H.; and Bengio, Y. 2014. Learn-
ing Phrase Representations using RNN Encoder-Decoder
for Statistical Machine Translation. In EMNLP, 1724â€“1734.
ACL.
Golden, B. L.; Levy, L.; and Vohra, R. 1987. The orien-
teering problem. Naval Research Logistics (NRL), 34(3):
307â€“318.
Goodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A. C.; and Bengio,
Y. 2014. Generative Adversarial Nets. In NIPS, 2672â€“2680.
Gu, J.; Song, C.; Jiang, W.; Wang, X.; and Liu, M. 2020. En-
hancing Personalized Trip Recommendation with Attractive
Routes. In AAAI, 662â€“669.
Hao, T.; Zhou, J.; Cheng, Y.; Huang, L.; and Wu, H. 2016.
User identiï¬cation in cyber-physical space: a case study on
mobile query logs and trajectories. In SIGSPATIAL, 1â€“4.
Hao, T.; Zhou, J.; Cheng, Y.; Huang, L.; and Wu, H. 2020. A
uniï¬ed framework for user identiï¬cation across online and
IEEE Transactions on Knowledge and Data
ofï¬‚ine data.
Engineering.
He, J.; Qi, J.; and Ramamohanarao, K. 2019. A Joint
Context-Aware Embedding for Trip Recommendations. In
ICDE, 292â€“303. IEEE.
Hidasi, B.; Karatzoglou, A.; Baltrunas, L.; and Tikk, D.
2016.
Session-based Recommendations with Recurrent
Neural Networks. In Bengio, Y.; and LeCun, Y., eds., ICLR.
Huang, J.; Liu, Y.; Chen, Y.; and Jia, C. 2019. Dy-
namic Recommendation of POI Sequence Responding to
Historical Trajectory. ISPRS International Journal of Geo-
Information, 8(10).
Kool, W.; van Hoof, H.; and Welling, M. 2019. Attention,
Learn to Solve Routing Problems! In ICLR.
Li, J.; Monroe, W.; Shi, T.; Jean, S.; Ritter, A.; and Jurafsky,
D. 2017. Adversarial Learning for Neural Dialogue Genera-
tion. In EMNLP, 2157â€“2169. Association for Computational
Linguistics.
Li, X.; Cong, G.; Li, X.-L.; Pham, T.-A. N.; and Krish-
naswamy, S. 2015. Rank-geofm: A ranking based geograph-
ical factorization method for point of interest recommenda-
tion. In SIGIR, 433â€“442.

A APPENDIX
In this section, we ï¬rst introduce the training procedure for
ANT. Then we give an example of evaluation metrics and
introduce implementation, data pre-process and the param-
eter setting. Finally, we represent the rest of experimental
results, which are omitted in the experiment section.

A.1 Algorithm
Training Algorithm for ANT We represent the training
procedure for ANT in Algorithm 1.

Algorithm 1: Training Procedure for ANT
Input: Traing set D
Output: Trained model parameters Î¸ of the generator

1 Initialize GÎ¸, DÏ† with random parameters Î¸, Ï†;
2 Generate trips using GÎ¸ for pre-training DÏ†;
3 Pre-train DÏ† via softmax loss function;
4 Pre-train GÎ¸ via learning from demonstration;
5 for n epoches do
6

for m batches do

7

8

9

10

Generate trips by using GÎ¸;
Update DÏ† via softmax loss function;
Update GÎ¸ via adversarial loss;
Update GÎ¸ via supervised loss function;
// Teacher forcing

end

11
12 end

A.2 Experiment Details
An Example of Evaluation Metrics Here we give an ex-
ample about HR and OSP. If the real trip is l0 â†’ l1 â†’ l2 â†’
l3 â†’ l4 and the recommended trip is l0 â†’ l2 â†’ l5 â†’ l1 â†’
l4, it can be calculated that HR = 4âˆ’1
5âˆ’1 = 0.75. As for OSP,
the overlapped part is (l2, l1, l4) and all the ordered POI pairs
in the overlapped part is {l2 â‡’ l1, l2 â‡’ l4, l1 â‡’ l4}, i.e.,
B = 3. And {l2 â‡’ l1, l2 â‡’ l4} has the correct order as the
real trip, so M = 2 and OSP = 0.67.

Data Pre-process The raw Foursquare dataset does not in-
clude the departure timestamp so we estimate the departure
timestamp for check-ins. As for the successive check-ins in
a trip, we use the arrival timestamp on the next POI as the
departure timestamp for the current POI. Specially, for the
last check-in in the trip, we set the departure timestamp as
30 minutes after the arrival timestamp. The Map dataset al-
ready includes the full information that we need for trip rec-
ommendation so we donâ€™t pre-process it.

Implementation We implement ANT in PyTorch and the
model is trained on Tesla V100 GPU with running environ-
ment of Python 3.7, PyTorch 1.8.1 and CUDA 11.0.

Parameter Setting We set embedding dimensions of user,
POI, and category as 256, 256 and 32 respectively. For the
encoder, the dimension of multi-head self-attention is 256,
the number of attention heads is 8, the inner-layer dimension
of the feed-forward sublayer is 256, and we stack 6 attention

Figure 7: Running time compared with baselines and run-
ning time on different numbers of candidates.

Figure 8: Ablation study of each component.

layers in the encoder. For the decoder, we set the number of
attention heads as 8 and the dimension of attention is 256.
For the discriminator, we set the dimension of the hidden
state of GRU as 256, the dimensions of inner layers in the
feed-forward network are 32 and 2. As for training, we set
batch size as 512. We use Adam optimizer to train our whole
framework with a learning rate of 0.0001 in the pre-training
stage and 0.00001 in the adversarial learning stage.

A.3 Experimental Results
Efï¬ciency We compare the running time of our proposed
ANT with trip recommendation baselines, i.e. TRAR, C-ILP
and PERSTOUR, and the running time of ANT on different
numbers of candidates. We make ANT generate trips serially
the same as described in the experiment section. As shown
in Figure 7, the running time of C-ILP and PERSTOUR on
an instance both exceed one minute, while the running time
of ANT is less than 100ms, which demonstrates the excel-
lent efï¬ciency of ANT. The running time of TRAR is shorter
than ANT with the help of greedy algorithm, but its perfor-
mance is much worse than ANT, even worse than PERS-
TOUR and C-ILP. And we also represent the running time of
ANT on different numbers of candidates. The results show
that the inference time of ANT is relatively stable on differ-
ent numbers of candidates.

Ablation Study The performance of four variants of ANT
and ANT under OSP is showed in Figure 8. As can be ob-
served in Figure 8, each component makes contribution to
the ï¬nal performance. Training the whole framework with-
out pre-training results in a big performance drop, which
demonstrates the necessity of pre-training on stabilizing and
speeding up the training process. Based on the pre-training,
adversarial learning further improves the framework. Re-
moving the POI correlation encoding module also makes
the performance worse, indicating the effectiveness of multi-
head self-attention to capture the relationships among POIs.

BeijingChengduDataset10âˆ’410âˆ’2100102104106Running Time (s)TRARPERSTOURC-ILPANT100150200250300Number of Candidates0.0000.0250.0500.0750.1000.1250.150Running Time (s)BeijingChengduNYCTokyo0.0500.0750.1000.1250.1500.175OSPANT-PANT-EANT-DANT-AANTBeijingChengdu0.000.010.020.030.040.050.06OSPANT-PANT-EANT-DANT-AANTFigure 9: The impact of the number of candidates.

And our specially designed context embedding for trip rec-
ommendation also outperforms Pointer Networks.

Impact of Candidates The performance of ANT condi-
tioned on different numbers of candidates is illustrated in
Figure 9. As we can see from Figure 9, for Chengdu, the
performance is relatively stable on different numbers of can-
didates under both HR and OSP. As for Beijing, when the
number of candidates is less than 200, the performance im-
proves with increase of candidates under both HR and OSP,
and when the number of candidates is more than 200, the
performance deteriorates with increase of candidates. So the
proper number of candidates for Beijing is 200. Thus, we
can adjust the number of candidates according to the differ-
ent characteristics of different cities.

100150200250300Nunmber of Candidates0.000.050.100.150.200.25HRBeijingChengdu100150200250300Nunmber of Candidates0.000.020.040.060.08OSPBeijingChengdu