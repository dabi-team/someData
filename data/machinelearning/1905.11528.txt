Improved Training Speed, Accuracy, and Data
Utilization Through Loss Function Optimization

Santiago Gonzalez1,2 and Risto Miikkulainen1,2
1Cognizant Technology Solutions, San Francisco, California, USA
2Department of Computer Science, University of Texas at Austin, Austin, Texas, USA
Email: slgonzalez@utexas.edu, risto@cs.utexas.edu

0
2
0
2

r
p
A
7
2

]

G
L
.
s
c
[

3
v
8
2
5
1
1
.
5
0
9
1
:
v
i
X
r
a

Abstract—As the complexity of neural network models has
grown, it has become increasingly important to optimize their
design automatically through metalearning. Methods for discover-
ing hyperparameters, topologies, and learning rate schedules have
lead to signiﬁcant increases in performance. This paper shows
that loss functions can be optimized with metalearning as well,
and result in similar improvements. The method, Genetic Loss-
function Optimization (GLO), discovers loss functions de novo,
and optimizes them for a target task. Leveraging techniques from
genetic programming, GLO builds loss functions hierarchically
from a set of operators and leaf nodes. These functions are
repeatedly recombined and mutated to ﬁnd an optimal structure,
and then a covariance-matrix adaptation evolutionary strategy
(CMA-ES) is used to ﬁnd optimal coefﬁcients. Networks trained
with GLO loss functions are found to outperform the standard
cross-entropy loss on standard image classiﬁcation tasks. Training
with these new loss functions requires fewer steps, results in
lower test error, and allows for smaller datasets to be used.
Loss function optimization thus provides a new dimension of
metalearning, and constitutes an important step towards AutoML.

Index Terms—loss function, optimization, neural networks,

genetic algorithms, genetic programming, evolution

I. INTRODUCTION

Much of the power of modern neural networks originates
from their complexity, i.e., number of parameters, hyper-
parameters, and topology. This complexity is often beyond
human ability to optimize, and automated methods are needed.
An entire ﬁeld of metalearning has emerged recently to
address this issue, based on various methods such as gradient
descent, simulated annealing, reinforcement learning, Bayesian
optimization, and evolutionary computation (EC) [1].

While a wide repertoire of work now exists for optimizing
many aspects of neural networks, the dynamics of training are
still usually set manually without concrete, scientiﬁc methods.
Training schedules, loss functions, and learning rates all affect
the training and ﬁnal functionality of a neural network. Perhaps
they could also be optimized through metalearning?

The goal of this paper is to verify this hypothesis, focusing
on optimization of loss functions. A general framework for
loss function metalearning, covering both novel loss function
discovery and optimization, is developed and evaluated experi-
mentally. This framework, Genetic Loss-function Optimization
(GLO), leverages Genetic Programming to build loss functions
represented as trees, and subsequently a Covariance-Matrix

Adaptation Evolution Strategy (CMA-ES) to optimize their
coefﬁcients.

EC methods were chosen because EC is arguably the
most versatile of the metalearning approaches. EC, being a
type of population-based search method, allows for extensive
exploration, which often results in creative, novel solutions
[2]. EC has been successful in hyperparameter optimization
and architecture design in particular [3]–[6]. It has also been
used to discover mathematical formulas to explain experimental
data [7]. It is, therefore, likely to ﬁnd creative solutions in the
loss-function optimization domain as well.

Indeed, on the MNIST image classiﬁcation benchmark, GLO
discovered a surprising new loss function, named Baikal for
its shape. This function performs very well, presumably by es-
tablishing an implicit regularization effect. Baikal outperforms
the standard cross-entropy loss in terms of training speed,
ﬁnal accuracy, and data requirements. Furthermore, Baikal was
found to transfer to a more complicated classiﬁcation task,
CIFAR-10, while carrying over its beneﬁts.

At ﬁrst glance, Baikal behaves rather unintuitively; loss
does not decrease monotonically as a network’s predictions
become more correct. Upon further analysis, Baikal was found
to perform implicit regularization, which caused this effect.
Speciﬁcally, by preventing the network from being too conﬁdent
in its predictions, training was able to produce a more robust
model. This ﬁnding was surprising and encouraging, since it
means that GLO is able to discover loss functions that train
networks that are more generalizable and overﬁt less.

The next section reviews related work in metalearning and
EC, to help motivate the need for GLO. Following this review,
GLO is described in detail, along with the domains upon which
it has been evaluated. The subsequent sections present the
experimental results, including an analysis of the loss functions
that GLO discovers.

II. RELATED WORK

In addition to hyperparameter optimization and neural archi-
tecture search, new opportunities for metalearning have recently
emerged. In particular, learning rate scheduling and adaptation
can have a signiﬁcant impact on a model’s performance.
Learning rate schedules determine how the learning rate
changes as training progresses. This functionality tends to
be encapsulated away in practice by different gradient-descent
optimizers, such as AdaGrad [8] and Adam [9]. While the

 
 
 
 
 
 
general consensus has been that monotonically decreasing
learning rates yield good results, new ideas, such as cyclical
learning rates [10], have shown promise in learning better
models in fewer epochs.

Metalearning methods have also been recently developed for
data augmentation, such as AutoAugment [11], a reinforcement
learning based approach to ﬁnd new data augmentation policies.
In reinforcement learning tasks, EC has proven a successful
approach. For instance, in evolving policy gradients [12], the
policy loss is not represented symbolically, but rather as a
neural network that convolves over a temporal sequence of
context vectors. In reward function search [13], the task is
framed as a genetic programming problem, leveraging PushGP
[14].

In terms of loss functions, a generalization of the L2 loss
was proposed with an adaptive loss parameter [15]. This loss
function is shown to be effective in domains with multivariate
output spaces, where robustness might vary across between
dimensions. Speciﬁcally, the authors found improvements in
Variational Autoencoder (VAE) models, unsupervised monocu-
lar depth estimation, geometric registration, and clustering.

Additionally, work has found promise in moving beyond
the standard cross-entropy loss for classiﬁcation [16]. L1 and
L2 losses were found to have useful probabilistic properties.
The authors found certain loss functions to be more resilient
to noise than the cross-entropy loss.

For speciﬁc types of tasks, certain variations of the cross-
entropy loss have yielded performance improvements. For
example, for dense object detection, the inclusion of a new
hand-designed coefﬁcient in the cross-entropy loss aimed to
increase the importance of challenging objects in scenes with
many other easy objects [17]. These types of explorations are
somewhat limited in scope, both in terms of the tasks where
they apply, and the space of loss functions that are considered.
Notably, no existing work in the metalearning literature
automatically optimizes loss functions for neural networks. As
shown in this paper, evolutionary computation can be used in
this role to improve neural network performance, gain a better
understanding of the processes behind learning, and help reach
the ultimate goal of fully automated learning.

III. THE GLO APPROACH

The task of ﬁnding and optimizing loss functions can be
framed as a functional regression problem. GLO accomplishes
this through the following high-level steps (shown in Figure 1):
(1) loss function discovery: using approaches from genetic
programming, a genetic algorithm builds new candidate loss
functions, and (2) coefﬁcient optimization: to further optimize
a speciﬁc loss function, a covariance-matrix adaptation evolu-
tionary strategy (CMA-ES) is leveraged to optimize coefﬁcients.

A. Loss function discovery

GLO uses a population-based search approach, inspired
by genetic programming, to discover new optimized loss
function candidates. Under this framework, loss functions are
represented as trees within a genetic algorithm. Trees are a

logical choice to represent functions due to their hierarchical
nature. The loss function search space is deﬁned by the
following tree nodes:
Unary Operators: log(◦), ◦2,
Binary Operators: +, ∗, −, ÷
Leaf Nodes: x, y, 1, −1, where x represents a true label, and

√

◦

y represents a predicted label.

The search space is further reﬁned by automatically assigning
a ﬁtness of 0 to trees that do not contain both at least one x and
one y. Generally, a loss function’s ﬁtness within the genetic
algorithm is the validation performance of a network trained
with that loss function. To expedite the discovery process, and
encourage the invention of loss functions that make learning
faster, training does not proceed to convergence. Unstable
training sessions that result in NaN values are assigned a
ﬁtness of 0. Fitness values are cached to avoid needing to
retrain the same network twice. These cached values are each
associated with a canonicalized version of their corresponding
tree, resulting in fewer required evaluations.

The initial population is composed of randomly generated
trees with a maximum depth of two. Recursively starting
from the root, nodes are randomly chosen from the allowable
operator and leaf nodes using a weighting (where log(◦), x, y
◦ is two times as likely as
are three times as likely and
+, ∗, −, ÷, 1, −1). This weighting can impart a bias and prevent,
for example, the integer 1 from occurring too frequently. The
genetic algorithm has a population size of 80, incorporates
elitism with six elites per generation, and uses roulette
sampling.

√

Recombination is accomplished by randomly splicing two
trees together. For a given pair of parent trees, a random element
is chosen in each as a crossover point. The two subtrees, whose
roots are the two crossover points, are then swapped with
each other. Figure 1 presents an example of this method of
recombination. Both resultant trees become part of the next
generation. Recombination occurs with a probability of 80%.
the genetic
algorithm has the following mutations, applied in a bottom-up
fashion:

To introduce variation into the population,

• Integer scalar nodes are incremented or decremented with

a 5% probability.

• Nodes are replaced with a weighted-random node with
the same number of children with a 5% probability.
• Nodes (and their children) are deleted and replaced with
a weighted-random leaf node with a 5% ∗ 50% = 2.5%
probability.

• Leaf nodes are deleted and replaced with a weighted-
random element (and weighted-random leaf children if
necessary) with a 5% ∗ 50% = 2.5% probability.

Mutations, as well as recombination, allow for trees of arbitrary
depth to be evolved.

Combined, the iterative sampling, recombination, and muta-
tion of trees within the population leads to the discovery of
new loss functions which maximize ﬁtness.

Fig. 1. Genetic Loss Optimization (GLO) overview. A genetic algorithm constructs candidate loss functions as trees. The best loss functions from this set then
has its coefﬁcients optimized using CMA-ES. GLO loss functions are able to train models more quickly and more accurately.

B. Coefﬁcient optimization

Loss functions found by the above genetic algorithm can
all be thought of having unit coefﬁcients for each node in the
tree. This set of coefﬁcients can be represented as a vector
with dimensionality equal to the number of nodes in a loss
function’s tree. The number of coefﬁcients can be reduced
by pruning away coefﬁcients that can be absorbed by others
(e.g., 3 (5x + 2y) = 15x + 6y). The coefﬁcient vector is
optimized independently and iteratively using a covariance-
matrix adaptation evolutionary strategy (CMA-ES) [18]. The
speciﬁc variant of CMA-ES that GLO uses is (µ/µ, λ)-CMA-
ES [19], which incorporates weighted rank-µ updates [20] to
reduce the number of objective function evaluations that are
needed. The implementation of GLO presented in this paper
uses an initial step size σ = 1.5. As in the discovery phase, the
objective function is the network’s performance on a validation
dataset after a shortened training period.

C. Implementation details

Due to the large number of partial training sessions that are
needed for both the discovery and optimization phases, training
is distributed across the network to a cluster of dedicated
machines that use HTCondor [21] for scheduling. Each machine
in this cluster has one NVIDIA GeForce GTX Titan Black GPU
and two Intel Xeon E5-2603 (4 core) CPUs running at 1.80GHz
with 8GB of memory. Training itself is implemented with
TensorFlow [22] in Python. The primary components of GLO
(i.e., the genetic algorithm and CMA-ES) are implemented in
Swift. These components run centrally on one machine and
asynchronously dispatch work to the Condor cluster over SSH.
Code for the Swift CMA-ES implementation is open sourced
at: https://github.com/sgonzalez/SwiftCMA

IV. EXPERIMENTAL EVALUATION

This section provides an experimental evaluation of GLO, on
the MNIST and CIFAR-10 image classiﬁcation tasks. Baikal, a
GLO loss function found on MNIST, is presented and evaluated

in terms of its resulting testing accuracy, training speed, training
data requirements, and transferability to CIFAR-10.

A. Target tasks

Experiments on GLO are performed using two popular image
classiﬁcation datasets, MNIST Handwritten Digits [23] and
CIFAR-10 [24]. Both datasets, with MNIST in particular, are
well understood, and relatively quick to train. The choice of
these datasets allowed rapid iteration in the development of
GLO and allowed time for more thorough experimentation.
The selected model architectures are simple, since achieving
state-of-the-art accuracy on MNIST and CIFAR-10 is not the
focus of this paper, rather the improvements brought about by
using a GLO loss function are.

Both of these tasks, being classiﬁcation problems, are tradi-
tionally framed with the standard cross-entropy loss (sometimes
referred to as the log loss): LLog = − 1
i=0 xi log(yi), where
n
x is sampled from the true distribution, y is from the predicted
distribution, and n is the number of classes. The cross-entropy
loss is used as a baseline in this paper’s experiments.

(cid:80)n−1

1) MNIST: The ﬁrst target task used for evaluation was the
MNIST Handwritten Digits dataset [23], a widely used dataset
where the goal is to classify 28 × 28 pixel images as one of
ten digits. The MNIST dataset has 55,000 training samples,
5,000 validation samples, and 10,000 testing samples.

(1) 5 × 5 convolution with 32 ﬁlters,

(3) 5 × 5 convolution with 64 ﬁlters,

A simple CNN architecture with the following layers is
(2) 2 × 2 stride-2
used:
(4) 2 × 2
max-pooling,
stride-2 max-pooling, (5) 1024-unit fully-connected layer,
(6)
a dropout layer [25] with 40% dropout probability, and (7) a
softmax layer. ReLU [26] activations are used. Training uses
stochastic gradient descent (SGD) with a batch size of 100,
a learning rate of 0.01, and, unless otherwise speciﬁed, for
20,000 steps.

2) CIFAR-10: To further validate GLO, the more challenging
CIFAR-10 dataset [24] (a popular dataset of small, color
photographs in ten classes) was used as a medium to test the
transferability of loss functions found on a different domain.

××-1logxiyi}××-1xi1÷exlogyixiex(1) Loss function discovery genetic algorithm.(2) Coeﬃcient optimization via CMA-ES.××-1logxiyi111111111111[]ℒ=−1nn−1∑i=0xilog(yi)ℒ=−1nn−1∑i=0c1(c2xi*c3log(c4yi))1.10.81.41.211.2[]CIFAR-10 consists of 50,000 training samples, and 10,000
testing samples.

A simple CNN architecture, taken from [27] (and itself
inspired by AlexNet [28]), with the following layers is used:
(1) 5 × 5 convolution with 64 ﬁlters and ReLU activations,
(2) 3 × 3 max-pooling with a stride of 2,
(3) local response
normalization [28] with k = 1, α = 0.001/9, β = 0.75,
(4)
5×5 convolution with 64 ﬁlters and ReLU activations, (5) local
response normalization with k = 1, α = 0.001/9, β = 0.75,
(6) 3 × 3 max-pooling with a stride of 2,
(7) 384-unit fully-
(8) 192-unit fully-
connected layer with ReLU activations,
connected, linear layer, and (9) a softmax layer.

Inputs to the network are sized 24 × 24 × 3, rather than
32 × 32 × 3 as provided in the dataset; this enables more
sophisticated data augmentation. To force the network to
better learn spatial invariance, random 24 × 24 croppings
are selected from each full-size image, which are randomly
ﬂipped longitudinally, randomly lightened or darkened, and
their contrast is randomly perturbed. Furthermore, to attain
quicker convergence, an image’s mean pixel value and variance
are subtracted and divided, respectively, from the whole image
during training and evaluation. CIFAR-10 networks were
trained with SGD, L2 regularization with a weight decay of
0.004, a batch size of 1024, and an initial learning rate of 0.05
that decays by a factor of 0.1 every 350 epochs.

B. A new loss function: Baikal

The most notable loss function that GLO discovered against
the MNIST dataset (with 2,000-step training for candidate
evaluation) is the Baikal loss (named as such due to its
similarity to the bathymetry of Lake Baikal when its binary
variant is plotted in 3D, see Section V-A):

LBaikal = −

1
n

n
(cid:88)

i=0

log(yi) −

xi
yi

,

(1)

where x is a sample from the true distribution, y is a sample
from the predicted distribution, and n is the number of classes.
Baikal was discovered from a single run of GLO. Additionally,
after coefﬁcient optimization, GLO arrived at the following
version of the Baikal loss:

LBaikalCMA = −

1
n

i=0

n
(cid:88)

(cid:18)

c0

c1 ∗ log(c2 ∗ yi) − c3

(cid:19)

,

c4 ∗ xi
c5 ∗ yi

(2)
where c0 = 2.7279, c1 = 0.9863, c2 = 1.5352, c3 =
−1.1135, c4 = 1.3716, c5 = −0.8411.

This loss function, BaikalCMA, was selected for having
the highest validation accuracy out of the population. The
Baikal and BaikalCMA loss functions had validation accuracies
at 2,000 steps equal to 0.9838 and 0.9902, respectively. For
comparison, the cross-entropy loss had a validation accuracy
at 2,000 steps of 0.9700. Models trained with the Baikal loss
on MNIST and CIFAR-10 (to test transfer) are the primary
vehicle for validating GLO’s efﬁcacy, as detailed in subsequent
sections.

Fig. 2. Mean testing accuracy on MNIST, n = 10. Both Baikal and
BaikalCMA provide statistically signiﬁcant improvements to testing accuracy
over the cross-entropy loss.

Fig. 3. Training curves for different loss functions on MNIST. Baikal and
BaikalCMA result in faster and smoother training compared to the cross-entropy
loss.

C. Testing accuracy

Figure 2 shows the increase in testing accuracy that Baikal
and BaikalCMA provide on MNIST over models trained with
the cross-entropy loss. Over 10 trained models each, the
mean testing accuracies for cross-entropy loss, Baikal, and
BaikalCMA were 0.9899, 0.9933, and 0.9947, respectively.

This increase in accuracy from Baikal over cross-entropy
loss is found to be statistically signiﬁcant, with a p-value of
2.4 × 10−11, in a heteroscedastic, two-tailed T-test, with 10
samples from each distribution. With the same signiﬁcance
test, the increase in accuracy from BaikalCMA over Baikal
was found to be statistically signiﬁcant, with a p-value of
8.5045 × 10−6.

D. Training speed

Training curves for networks trained with the cross-entropy
loss, Baikal, and BaikalCMA are shown in Figure 3. Each
curve represents 80 testing dataset evaluations spread evenly
(i.e., every 250 steps) throughout 20,000 steps of training
on MNIST. Networks trained with Baikal and BaikalCMA
both learn signiﬁcantly faster than the cross-entropy loss.
These phenomena make Baikal a compelling loss function for
ﬁxed time-budget training, where the improvement in resultant
accuracy over the cross-entropy loss becomes most evident.

E. Training data requirements

Figure 4 provides an overview of the effects of dataset
size on networks trained with cross-entropy loss, Baikal,
and BaikalCMA. For each training dataset portion size, ﬁve

DataTest AccuracyLog LossBaikalBaikalCMA0.98980.99410.99450.98980.99370.99410.99020.99250.99490.98940.99320.9950.98950.99350.99520.99050.99240.99560.99020.99370.99440.98960.99340.99440.98980.9930.99440.98990.99350.9944Mean Test Accuracy0.98990.99330.9947Standard Deviation0.00030.00050.0005T-Test Baikal vs Log Loss2-Tailed1-TailedPaired0.0000001859170.000000092958Homoscedastic0.0000000000020.000000000001Heteroscedastic0.0000000000240.0000000000120.98000.98500.99000.99501.0000Log LossBaikalMean Test Accuracy***p = 0.000Mean Test Accuracy0.98000.98500.99000.99501.0000Log LossBaikalBaikalCMA******T-Test BaikalCMA vs Baikal2-Tailed1-TailedPairedHomoscedasticHeteroscedastic0.000008504450 2Testing Accuracy0.85000.88750.92500.96251.0000Training Step25030005750850011250140001675019500Log LossBaikal LossBaikalCMA LossFig. 4. Sensitivity to different dataset sizes for different loss functions
on MNIST. For each size, n = 5. Baikal and BaikalCMA increasingly
outperform the cross-entropy loss on small datasets, providing evidence of
reduced overﬁtting.

Fig. 6. Binary classiﬁcation loss functions at x0 = 1. Correct predictions lie
on the right side of the graph, and incorrect ones on the left. The log loss
decreases monotonically, while Baikal and BaikalCMA present counterintuitive,
sharp increases in loss as predictions, approach the true label. This phenomenon
provides regularization by preventing the model from being too conﬁdent in
its predictions.

transferred to a more complex dataset while still maintaining
performance improvements. This faster training provides a
particularly persuasive argument for using GLO loss functions
in ﬁxed time-budget scenarios.

V. WHAT MAKES BAIKAL WORK?

This section presents a symbolic analysis of the Baikal loss
function, followed by experiments that attempt to elucidate
why Baikal works better than the cross-entropy loss. A likely
explanation is that Baikal results in implicit regularization,
reducing overﬁtting.

A. Binary classiﬁcation

Loss functions used on the MNIST dataset, a 10-dimensional
classiﬁcation problem, are difﬁcult
to plot and visualize
graphically. To simplify, loss functions are analyzed in the
context of binary classiﬁcation, with n = 2, the Baikal loss
expands to

LBaikal2D = −

(cid:18)

log(y0) −

1
2

x0
y0

+ log(y1) −

(cid:19)

.

x1
y1

(3)

Since vectors x and y sum to 1, by consequence of being
passed through a softmax function, for binary classiﬁcation x =
(cid:104)x0, 1 − x0(cid:105) and y = (cid:104)y0, 1 − y0(cid:105). This constraint simpliﬁes
the binary Baikal loss to the following function of two variables
(x0 and y0):

LBaikal2D ∝ − log(y0) +

x0
y0

− log(1 − y0) +

1 − x0
1 − y0

.

(4)

This same methodology can be applied to the cross-entropy
loss and BaikalCMA.

In practice, true labels are assumed to be correct with
certainty, thus, x0 is equal to either 0 or 1. The speciﬁc
case where x0 = 1 is plotted in Figure 6 for the cross-
entropy loss, Baikal, and BaikalCMA. The cross-entropy loss
is shown to be monotonically decreasing, while Baikal and
BaikalCMA counterintuitively show an increase in the loss
value as the predicted label, y0, approaches the true label x0.
This unexpected increase allows the loss functions to prevent
the model from becoming too conﬁdent in its output predictions,

Fig. 5. Testing accuracy across varying training steps on CIFAR-10. The Baikal
loss, which has been transferred from MNIST, outperforms the cross-entropy
loss on all training durations.

individual networks were trained for each loss function. Due
to frequent numerical instability exhibited by the cross-entropy
loss on the 0.05 dataset portion, these speciﬁc experiments
had to be run many times to yield ﬁve fully-trained networks;
no other dataset portions exhibited instability. As in previous
experiments, MNIST networks were trained for 20,000 steps.
The degree by which Baikal and BaikalCMA outperform
cross-entropy loss increases as the training dataset becomes
smaller. This provides evidence of less overﬁtting when training
a network with Baikal or BaikalCMA. As expected, BaikalCMA
outperforms Baikal at all tested dataset sizes. The size of this
improvement in accuracy does not grow as signiﬁcantly as
the improvement over cross-entropy loss, leading to the belief
that the overﬁtting characteristics of Baikal and BaikalCMA
are very similar. Ostensibly, one could run the optimization
phase of GLO on a reduced dataset speciﬁcally to yield a loss
function with better performance than BaikalCMA on small
datasets.

F. Loss function transfer to CIFAR-10

Figure 5 presents a collection of 18 separate tests of the cross-
entropy loss and Baikal applied to CIFAR-10. Baikal is found
to outperform cross-entropy across all training durations, with
the difference becoming more prominent for shorter training
periods. These results present an interesting use case for GLO,
where a loss function that is found on a simpler dataset can be

1.00.50.20.10.020.0050.880.900.920.940.960.981.00TrainingDatasetPortionTestingAccuracyLogLossBaikalLossBaikalCMALossthus providing a form of regularization. Section V-B provides
reasoning for this unexplained result.

As also seen in Figure 6, the minimum for the Baikal loss
where x0 = 1 lies near 0.71, while the minimum for the
BaikalCMA loss where x0 = 1 lies near 0.77. This minimum,
along with the more pronounced slope around x0 = 0.5 is
likely a reason why BaikalCMA performs better than Baikal.

B. Implicit regularization

The Baikal and BaikalCMA loss functions are surprising in
that they incur a high loss when the output is very close to the
correct value (as illustrated in Figure 6). Although at ﬁrst glance
this behavior is counterintuitive, it may provide an important
advantage. The outputs of a trained network will not be exactly
correct, although they are close, and therefore the network is
less likely to overﬁt. Thus, these loss functions provide an
implicit form of regularization, enabling better generalization.
This effect is similar to that of the conﬁdence regularizer
[29], which penalizes low-entropy prediction distributions. The
bimodal distribution of outputs that results from conﬁdence
regularization is nearly identical to that of a network trained
with BaikalCMA. Note that while these outputs are typically
referred to as probabilities in the literature, this is often an
erroneous interpretation [30]. Histograms of these distributions
on the test dataset for cross-entropy and BaikalCMA networks,
after 15,000 steps of training on MNIST, are shown in Figure 7.
The abscissae in Figures 6 and 7 match, making clear how
the distribution for BaikalCMA has shifted away from the
extreme values. The improved behavior under small-dataset
conditions described in Section IV-E further supports implicit
regularization; less overﬁtting was observed when using Baikal
and BaikalCMA compared to the cross-entropy loss.

Notably, the implicit regularization provided by Baikal and
BaikalCMA complements the different types of regulariza-
tion already present in the trained networks. As detailed in
Section IV, MNIST networks are trained with dropout [25],
and CIFAR-10 networks are trained with L2 weight decay and
local response normalization [28], yet Baikal is able to improve
performance further.

VI. DISCUSSION AND FUTURE WORK

This paper proposes loss function discovery and optimization
as a new form of metalearning, and introduces an evolutionary
computation approach to it. GLO was evaluated experimentally
in the image classiﬁcation domain, and discovered a surprising
new loss function, Baikal. Experiments showed substantial
improvements in accuracy, convergence speed, and data re-
quirements. Further analysis suggested that these improvements
result from implicit regularization that reduces overﬁtting
to the data. This regularization complements the existing
regularization in trained networks.

In the future, GLO can be applied to other machine learning
datasets and tasks. The approach is general, and could result in
discovery of customized loss functions for different domains,
or even speciﬁc datasets. One particularly interesting domain
is generative adversarial networks (GANs) [31]. Signiﬁcant

manual tuning is necessary in GANs to ensure that the generator
and discriminator networks learn harmoniously. GLO could ﬁnd
co-optimal loss functions for the generator and discriminator
networks in tandem, thus making GANs more powerful, robust,
and easier to implement.

GAN optimization is an example of co-evolution, where
multiple interacting solutions are developed simultaneously.
GLO could leverage co-evolution more generally: for instance,
it could be combined with techniques like CoDeepNEAT [3]
to learn jointly-optimal network structures, hyperparameters,
learning rate schedules, data augmentation, and loss functions
simultaneously. Such an approach requires signiﬁcant com-
puting power, but may also discover and utilize interactions
between the design elements that result in higher complexity
and better performance than is currently possible.

VII. CONCLUSION

This paper proposes Genetic Loss-function Optimization
(GLO) as a general framework for discovering and optimizing
loss functions for a given task. A surprising new loss function,
Baikal, was discovered in the experiments, and shown to
outperform the cross-entropy loss on MNIST and CIFAR-10
in terms of accuracy, training speed, and data requirements.
Further analysis suggested that Baikal’s improvements result
from implicit regularization that reduces overﬁtting to the data.
GLO can be combined with other aspects of metalearning in
the future, paving the way to robust and powerful AutoML.

REFERENCES

[1] T. Elsken, J. H. Metzen, and F. Hutter, “Neural architecture search: A
survey,” Journal of Machine Learning Research, vol. 20, no. 55, pp.
1–21, 2019.

[2] J. Lehman, J. Clune, and D. Misevic, “The surprising creativity of digital
evolution: A collection of anecdotes from the evolutionary computation
and artiﬁcial life research communities,” in Proceedings of the Artiﬁcial
Life Conference. MIT Press, 2018, pp. 55–56.

[3] R. Miikkulainen, J. Liang, E. Meyerson, A. Rawal, D. Fink, O. Francon,
B. Raju, H. Shahrzad, A. Navruzyan, N. Duffy et al., “Evolving deep
neural networks,” in Artiﬁcial Intelligence in the Age of Neural Networks
and Brain Computing. Elsevier, 2019, pp. 293–312.

[4] K. O. Stanley,

J. Clune,

and R. Miikkulainen,
“Designing neural networks through neuroevolution,” Nature Machine
Intelligence, vol. 1, no. 1, pp. 24–35, 2019. [Online]. Available:
https://doi.org/10.1038/s42256-018-0006-z

J. Lehman,

[5] E. Real, A. Aggarwal, Y. Huang, and Q. V. Le, “Regularized evolution
for image classiﬁer architecture search,” in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 4780–4789.
[6] I. Loshchilov and F. Hutter, “CMA-ES for hyperparameter optimization
of deep neural networks,” arXiv preprint arXiv:1604.07269, 2016.
[7] M. Schmidt and H. Lipson, “Distilling free-form natural laws from
experimental data,” Science, vol. 324, no. 5923, pp. 81–85, 2009.
[Online]. Available: http://science.sciencemag.org/content/324/5923/81
[8] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for
online learning and stochastic optimization,” Journal of Machine Learning
Research, vol. 12, no. Jul, pp. 2121–2159, 2011.

[9] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”

arXiv preprint arXiv:1412.6980, 2014.

[10] L. N. Smith, “Cyclical learning rates for training neural networks,” in
2017 IEEE Winter Conference on Applications of Computer Vision
(WACV).

IEEE, 2017, pp. 464–472.

[11] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le, “Autoaug-
ment: Learning augmentation strategies from data,” in Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2019, pp. 113–123.

Fig. 7. Outputs of networks trained with cross-entropy loss and BaikalCMA. With BaikalCMA, the peaks are shifted away from extreme values and more
spread out, indicating implicit regularization. The BaikalCMA histogram matches that from a network trained with a conﬁdence regularizer [29].

Information Processing Systems 25, F. Pereira, C. J. C. Burges,
L. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc.,
2012, pp. 1097–1105. [Online]. Available: http://papers.nips.cc/paper/
4824-imagenet-classiﬁcation-with-deep-convolutional-neural-networks.
pdf

[29] G. Pereyra, G. Tucker, J. Chorowski, Ł. Kaiser, and G. Hinton, “Reg-
ularizing neural networks by penalizing conﬁdent output distributions,”
arXiv preprint arXiv:1701.06548, 2017.

[30] Y. Gal and Z. Ghahramani, “Dropout as a Bayesian approximation:
Representing model uncertainty in deep learning,” in Proceedings of the
33rd International Conference on Machine Learning (ICML), 2016, pp.
1050–1059.

[31] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,”
in Advances in Neural Information Processing Systems 27, 2014, pp.
2672–2680.

[12] R. Houthooft, Y. Chen, P. Isola, B. Stadie, F. Wolski, O. J. Ho, and
P. Abbeel, “Evolved policy gradients,” in Advances in Neural Information
Processing Systems 31. Curran Associates, Inc., 2018, pp. 5400–5409.
[13] S. Niekum, A. G. Barto, and L. Spector, “Genetic programming for
reward function search,” IEEE Transactions on Autonomous Mental
Development, vol. 2, no. 2, pp. 83–90, 2010.

[14] L. Spector, E. Goodman, A. Wu, W. B. Langdon, H. m. Voigt,
M. Gen, S. Sen, M. Dorigo, S. Pezeshk, M. Garzon, E. Burke, and
M. Kaufmann Publishers, “Autoconstructive evolution: Push, PushGP, and
Pushpop,” Proceedings of the 3rd Genetic and Evolutionary Computation
Conference (GECCO), 05 2001.

[15] J. T. Barron, “A general and adaptive robust loss function,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2019, pp. 4331–4339.

[16] K. Janocha and W. M. Czarnecki, “On loss functions for deep neural
networks in classiﬁcation,” arXiv preprint arXiv:1702.05659, 2017.
[17] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll´ar, “Focal loss
for dense object detection,” in Proceedings of the IEEE International
Conference on Computer Vision (ICCV), 2017, pp. 2980–2988.

[18] N. Hansen and A. Ostermeier, “Adapting arbitrary normal mutation
distributions in evolution strategies: The covariance matrix adaptation,”
in Proceedings of IEEE international conference on evolutionary
computation.

IEEE, 1996, pp. 312–317.

[19] ——, “Completely derandomized self-adaptation in evolution strategies,”

Evolutionary computation, vol. 9, no. 2, pp. 159–195, 2001.

[20] N. Hansen and S. Kern, “Evaluating the CMA evolution strategy on
multimodal test functions,” in International Conference on Parallel
Problem Solving from Nature. Springer, 2004, pp. 282–291.

[21] D. Thain, T. Tannenbaum, and M. Livny, “Distributed computing in
practice: the Condor experience,” Concurrency and computation: practice
and experience, vol. 17, no. 2-4, pp. 323–356, 2005.

[22] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga,
S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden,
M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: A system for large-scale
machine learning,” in 12th USENIX Symposium on Operating Systems
Design and Implementation (OSDI 16).
Savannah, GA: USENIX
Association, 2016, pp. 265–283. [Online]. Available: https://www.usenix.
org/conference/osdi16/technical-sessions/presentation/abadi

[23] Y. LeCun, C. Cortes, and C. Burges, “The MNIST dataset of handwritten

digits,” 1998.

[24] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from

tiny images,” 2009.

[25] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R.
Salakhutdinov, “Improving neural networks by preventing co-adaptation
of feature detectors,” arXiv preprint arXiv:1207.0580, 2012.

[26] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted Boltz-
mann machines,” in Proceedings of the 27th International Conference
on Machine Learning (ICML), 2010, pp. 807–814.

[27] S. Gonzalez, J. Landgraf, and R. Miikkulainen, “Faster training by
selecting samples using embeddings,” in 2019 International Joint
Conference on Neural Networks (IJCNN), 2019.

[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classiﬁcation
with deep convolutional neural networks,” in Advances in Neural

0.00.20.40.60.81.00200400600800LogLossNetworkOutputProbabilitiesQuantity0.00.20.40.60.81.00200400600800BaikalCMANetworkOutputProbabilitiesQuantity