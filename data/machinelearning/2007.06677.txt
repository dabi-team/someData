0
2
0
2

l
u
J

6
1

]
E
S
.
s
c
[

2
v
7
7
6
6
0
.
7
0
0
2
:
v
i
X
r
a

Gradient Descent over Metagrammars for Syntax-Guided Synthesis

NICOLAS CHAN, UC Berkeley
ELIZABETH POLGREEN, UC Berkeley
SANJIT A. SESHIA, UC Berkeley

Syntax-guided synthesis restricts the search space for program synthesis through the use of a grammar imposing a syntactic re-

striction. In practice, provision of such a grammar is often left to the user to do manually, though in the absence of such a grammar,

state-of-the-art solvers will provide their own default grammar, which is dependent on the signature of the target program to be sythe-

sized. In this work, we speculate this default grammar could be improved upon substantially. We build sets of rules, or metagrammars,

for constructing grammars, and perform a gradient descent over these metagrammars aiming to ﬁnd a metagrammar which solves

more benchmarks and on average faster. We show the resulting metagrammar enables CVC4 to solve 26% more benchmarks than the

default grammar within a 300s time-out, and that metagrammars learnt from tens of benchmarks generalize to performance on 100s

of benchmarks.

ACM Reference format:

Nicolas Chan, Elizabeth Polgreen, and Sanjit A. Seshia. 2020. Gradient Descent over Metagrammars for Syntax-Guided Synthesis.

Proc. ACM Program. Lang. 1, SYNT, Article 2 (July 2020), 6 pages.

DOI:

1 INTRODUCTION

A major theme in the advances in program synthesis over the past 15 years has been the use of syntactic restrictions

on the space of programs being searched. One prominent approach is that of Syntax-Guided Synthesis [1], which

augments the program synthesis problem with a syntactic template or grammar from which the program is to be

constructed. This template can be used to guide the synthesis algorithm and to reduce the search space of possible

solutions.

Provision of this grammar, however, typically requires a skilled domain expert. If the grammar is too small, it may

not be expressive enough to contain a solution to the synthesis task, or any solution might be intractably large. For
instance, constant literals may need to be constructed as the sum of smaller constant literals e.g., 1′s. If the grammar is
large, the space of potential solutions is large and so the run-time of the synthesis algorithm will be longer. Furthermore,

a grammar may contain some operators that are more expensive for a synthesis algorithm to reason about than others.

Consequently, choosing a good grammar is a key part in the success of SyGuS.

The best particular grammar for a synthesis problem will depend on the function to be synthesized, for example, it

will depend on the number and type of arguments of the function, the return type of the function, or any number of

other particularities about the synthesis problem. So to construct a good grammar in general, we need a set of rules

that output a good particular grammar for a particular synthesis problem.

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the owner/author(s).

© 2020 Copyright held by the owner/author(s). Manuscript submitted to ACM

Manuscript submitted to ACM

1

 
 
 
 
 
 
2

Nicolas Chan, Elizabeth Polgreen, and Sanjit A. Seshia

In this work, we call this set of rules a metagrammar. The metagrammar may include rules such as adding the
function arguments to the grammar, adding various functions (such as bvadd, bvor, etc. for bitvector problems), or
adding helper functions and constants present in the constraints or other parts of the problem statement.

We design a scoring system to reward metagrammars solving benchmarks quickly and penalize those failing to solve

benchmarks, and use a simple gradient descent over these metagrammars. We evaluate the resulting metagrammar on a

set of bitvector benchmarks from the SyGuS competition, using CVC4 [5] as the SyGuS solver, and ﬁnd metagrammars

learnt from a randomly selected training set of tens of benchmarks, are able to solve 26% more benchmarks than the

default grammar on the remaining benchmarks within a 300s time-out.

1.1 Related work

The balance between expressivity of a grammar and the speed of synthesis is discussed by Gulwani in the context

of data-wrangling [8], and further by Padhi et al, who hypothesise that the performance of SyGuS tools degrades

as the expressivity of the grammar increases, and propose a solution based on interleaving multiple grammars [12].

Morton et al. [11] use machine learning to ﬁlter the grammars used by SyGuS. This work assumes a grammar is given,

and optimizes that grammar, whereas we assume no grammar is provided and improve upon the default grammar.

A key diﬀerence is that Morton et al. use a neural network to perform the ﬁltering and so need a large amount of

training data. The authors test their techniques on programming-by-example benchmarks (benchmarks where the

speciﬁcation is a set of input-output examples which the desired program satisﬁes), where automatically generating

new benchmarks is straightforward. A key beneﬁt of our approach is that it requires very small amounts of training

data, which is diﬃcult to obtain for program synthesis for logical speciﬁcations (where the desired program must

satisfy a set of logical constraints). It is not straight forward to convert a logical speciﬁcation into a programming-by-

example speciﬁcation [13, 14]. Machine learning based program synthesis techniques [6, 7], which often do look at

identifying important pieces of a grammar [4, 9], are usually applied to PBE problems, in part due to this challenge. A

key advantage of our simple gradient descent based technique is that we do not need large amounts of training data

in order to achieve performance gains.

Research into techniques used for selecting grammars for SyGuS and research into techniques used for enumerating

through the grammars [3, 15] tackle similar problems, and good enumeration strategy should mitigate the eﬀects of a

grammar specifying a large search space. An interesting example is EUPHONY [10], which uses learns a probabilistic

model at run time that biases the enumeration. However, there are advantages to having an algorithm for selecting

grammars as well as an eﬀective enumeration algorithm. It provides ﬂexibility as the algorithm can be treated as a

bolt-on for any SyGuS solver. It is also likely that the best metagrammar will be be dependent on the set of benchmarks

you wish to solve. We can evision a scenario where a user has bodies of synthesis problems from diﬀerent speciﬁc

applications and uses our technique to generate a metagrammar for use with each speciﬁc benchmark set. Our frame-

work could also allow a user to specify rules as part of the metagrammar which extract features the user knows to be

important from the benchmark.

2 BACKGROUND

2.1 Syntax-Guided Synthesis

A Syntax-Guided Synthesis [1] (SyGuS) problem consists of two constraints, a semantic constraint given as a formula

ϕ, and a context-free grammar, often referred to as the syntactic template. The computational problem is then to ﬁnd

Manuscript submitted to ACM

Gradient Descent over Metagrammars for Syntax-Guided Synthesis

3

Set
Non-SI bitvector
Non-SI (orig LIA)
PBE bitvector
Total

total
421
429
705
1555

Default metagrammar
solved
276
165
244
685

avg. time
18.1s
14.9s
33.3s
22.7s

Reduced metagrammar

Enhanced metagrammar

solved
271
181
379
831

unique
12
19
146
177

avg. time
13.3s
13.6s
40.8s
25.9s

solved
282
165
556
1003

unique
21
0
331
352

avg. time
12.5s
15.8s
1.0s
6.6s

Table 1. The number of benchmarks solved and average solving time in seconds for the default, reduced and enhanced metagram-
mars. The unique columns show the number of benchmarks solved, that were not solved by the default grammar. We discount the
performance of the enhanced metagrammar on the PBE benchmarks because adding in constant literals to a grammar allows the
solver to produce large look-up tables as solution.

a function f , built from the context-free grammar, such that

∀x ϕ(f , x)

is valid, where x is the set of all possible inputs to f .

2.2 Metagrammars

A metagrammar is a set of rules which determine what to add to the grammar for a particular synthesis problem. A

rule is a mapping from a particular synthesis speciﬁcation ϕ to a set of terminal symbols S, where the speciﬁcation

includes the signature for the function f to be synthesized. For example, a rule specifying that the arguments of the

function are included in the grammar would inspect ϕ, extract the function arguments, and then output S consisting of

the arguments as terminals. Then each terminal in S can be included in the deﬁnition of the appropriate non-terminal

to construct the ﬁnal grammar for the particular synthesis problem (in union with the other rules).

3 METHODOLOGY

3.1 Building Metagrammars

We begin by identifyinga default grammar which includes a non-terminal for each data type in the function signature

plus booleans and the following rules for populating these non-terminals:

• Add argument variables of the same data type
• Add constants 1 and 0 for each data type
• Add functions (operators) for each data type
• Add predicates for each relevant data type to the Bool type (for use in the condition of if-then-else)

We identify a set of rules for producing the default grammar, where each rule adds a set of constants and/or operators

to the grammar. This set of rules forms the default metagrammar which we can use to generate a grammar for a

given synthesis problem. We use this basic set of rules to evaluate our framework but more complex rules could be

explored in future work. We could, for instance, introduce rules which extend the grammar, for example rules which

add constants and helper functions found in the synthesis speciﬁcation ﬁle.

3.2 Metagrammar Search

We score each metagrammar according to its performance on a set of training benchmarks. Where the benchmarks

are easily divisible into representative categories, for instance invariant generation, we stratify the training data across

Manuscript submitted to ACM

4

Nicolas Chan, Elizabeth Polgreen, and Sanjit A. Seshia

these sets in order to attempt to obtain representative training data. Based on these scores we perform a search by

expanding the neighbors of the best performing metagrammar.

As described, a metagrammar M is a set of rules M = {r1, r2, . . . , rn }. A smaller neighbor of M is a metagrammar Ni

which is missing one of the rules. That is, the set of smaller neighbors of M is {Ni = M \ {ri } : i = 1, 2, . . . , n}.

The search begins with the neighbors of the default metagrammar. Then the neighbors of the best neighbor are

expanded, and so on, until we have exhausted all of the rules.

3.2.1

Scoring. We designed a scoring function that aims to reward metagrammars solving a benchmark signiﬁ-

cantly faster than neighbor metagrammars, and heavily penalize metagrammars failing to solve a benchmark. We

base this on the normalized runtime against the neighbors. The score SB for a particular metagrammar M on a bench-

mark B, in the context of the results of the n neighboring metagrammars, for a single benchmark is determined by the

diﬀerence of M’s runtime against the average of its neighbors normalized by the standard deviation:

where r is the runtime of the metagrammar in seconds and σ is the standard deviation of the runtime of all the

10
rM − 1

n Í0≤j ≤n r Nj

σ

benchmark unsolved,

benchmark solved

SB = 



metagrammars on that benchmark. A lower score is better. The speciﬁc scoring system is a heuristic and may be

improved upon or tailored for speciﬁc use cases, for instance one could incorporate a metric that rewards ﬁnding

shorter solutions, or ﬁnding solutions that contain certain operators.

4 EVALUATION

We use CVC4 1.7 (84da9c0b) with the --cegqi-si=none option, and remove single invocation benchmarks from our

benchmark set since these do not beneﬁt from a grammar being provided. We use a 300 second timeout, with 20

benchmarks running in parallel on each Intel Xeon E5-2670 v2 CPU. We use a set of 1699 bitvector benchmarks taken

from the SyGuS competition [2], grouped into 3 sets, shown in Table 1: non single-invocation (non-SI) bitvector bench-

marks; programming-by-example (PBE) bitvector benchmarks; and non-SI Linear Integer Arithmetic benchmarks that

we translate into bitvectors (this includes invariant generation benchmarks).

We extract the default grammar used by CVC41 and identify a default metagrammar capable of producing this
grammar. We run the gradient descent process starting with the default metagrammar. We train on a stratiﬁed sample

of benchmarks, taking 48 from each of the 3 sets shown in Table 1. We stratify the training data across these 3 sets in

order to attempt to obtain representative training data. These categories are very broad though and better results may

be obtained by categorising the benchmarks into more precise categories, or by training a metagrammar for each set,

We obtain a metagrammar which we call the reduced metagrammar, and which produces the following grammar for a

function with bitvector arguments A1 and A2:

BV : : = A1 | A2 | 1 | ( i t e B ool BV BV ) | ( b v not BV BV ) |

( b v or BV BV ) | ( + BV BV ) | ( b v l s h r BV BV ) |

( b v s h l BV BV )

p r e d i c a t e : : = ( = BV BV ) | ( b v u l t BV BV )

B ool

: : = t r u e | ( not B ool ) | ( and B ool B ool ) |

( or B ool B ool ) | p r e d i c a t e

1 Obtained from CVC4 version 1.7 using the --trace=sygus-grammar-def option.

Manuscript submitted to ACM

Gradient Descent over Metagrammars for Syntax-Guided Synthesis

5

We also take the default metagrammar and manually add a rule adding constant literals found in the benchmark, and

we call the resulting metagrammar the enhanced metagrammar.

We evaluate all metagrammars over the remaining set of 1555 benchmarks. The reduced metagrammar solved 177

benchmarks that the default did not solve, while failing to solve 31 benchmarks the default metagrammar did solve. It

is 22% faster on the ﬁrst two categories of benchmarks, but slower on the PBE category. The enhanced metagrammar

solves 21 benchmarks that the default did not solve, taken from the bitvector category, but does is on average slower and

solves no new benchmarks in the category of benchmarks translated from LIA (which includes all benchmarks from

the invariant synthesis category). We hypothesize that this is because, whilst adding constant literals to a grammar

is helpful, it is not suﬃcient to do so without also reducing the size of the grammar when the benchmarks are more

complex. This suggests it would be beneﬁcial to explore using gradient descent over the space of metagrammars that

include rules that extend beyond the default metagrammar, with the aim of combining the success of the reduced and

enhanced metagrammars. We discount the results for the enhanced metagrammar on the PBE category since adding

in constants makes it possible for the solver to eﬀectively produce lookup table solutions to these benchmarks. The

diﬀerences in results across the categories suggest it would be worth further exploring stratiﬁed sampling for training

sets, learning metagrammars based on training sets taken solely from the category that the metagrammar is going to

be tested on, or metagrammars containing rules that attempt to classify benchmarks into such categories.

It is worth noting that in some cases the learnt metagrammar results in faster solving but potentially longer solutions.

This would not necessarily be a desirable outcome, but it this behaviour is expected given that our scoring function

only rewards speed of solving and number of benchmarks solved. A scoring function that places some weighting on

“quality” of solutions would be interesting to explore in future work. It is also worth noting that a diﬀerent training

set will result in a diﬀerent metagrammar; we report results from a randomly selected training set. A fuller evaluation

is in order to determine the optimum size of the training set in order to consistently produce these improvements, but

we note that we trained two further metagrammars which both improved over the default (solving 744 benchmarks in
an average of 22.0s, and solving 814 benchmarks in an average of 27.7s, with signiﬁcant improvements in the Non-SI
bitvector and Non-SI LIA translated categories respectively).

5 CONCLUSIONS

We have presented a framework that uses a gradient descent based search to ﬁnd a metagrammar that improves solver

performance over a set of SyGuS benchmarks. We have evaluated one speciﬁc instance of this framework, using bitvec-

tor benchmarks and the speciﬁc set of metagrammar rules and the scoring function described in this paper. Despite its

simplicity, and the minimal amount of training data benchmarks required, we found this instantiation of our method

solved 25% more benchmarks than the default metagrammar within the timeout. In future work we plan to explore

more advanced gradient descent algorithms, as well as incorporating more advanced rules in the metagrammars and

exploring the relationships between metagrammars and diﬀerent enumerative synthesis algorithms. We believe that

techniques like this could have signiﬁcant impact in application domains for synthesis such as synthesising invariants

for a speciﬁc type of software, where benchmarks are available but not in the abundance required for neural network

based techniques.

Acknowledgements. This work was supported in part by NSF grants 1739816 and 1837132, a gift from Intel under the

SCAP program, and the iCyPhy center.

Manuscript submitted to ACM

6

REFERENCES

Nicolas Chan, Elizabeth Polgreen, and Sanjit A. Seshia

[1] Rajeev Alur, Rastislav Bod´ık, Eric Dallal, Dana Fisman, Pranav Garg, Garvit Juniwal, Hadas Kress-Gazit, P. Madhusudan, Milo M. K. Martin,
Mukund Raghothaman, Shambwaditya Saha, Sanjit A. Seshia, Rishabh Singh, Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-
guided synthesis. In Dependable Software Systems Engineering, volume 40 of NATO Science for Peace and Security Series, D: Information and
Communication Security, pages 1–25. IOS Press, 2015.

[2] Rajeev Alur,

Dana

Fisman,

Rishabh

Singh,

and Abhishek

Udupa.

Syntax

guided

synthesis

competition.

http://sygus.seas.upenn.edu/SyGuS-COMP2017.html, 2017.

[3] Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. Scaling enumerative program synthesis via divide and conquer. In TACAS (1), volume

10205 of Lecture Notes in Computer Science, pages 319–336, 2017.

[4] Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In ICLR

(Poster). OpenReview.net, 2017.

[5] Clark W. Barrett, Haniel Barbosa, Martin Brain, Duligur Ibeling, Tim King, Paul Meng, Aina Niemetz, Andres N¨otzli, Mathias Preiner, Andrew

Reynolds, and Cesare Tinelli. CVC4 at the SMT competition 2018. CoRR, abs/1806.08775, 2018.

[6] Rudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for

neural program synthesis. 2018.

[7] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustﬁll: Neural program

learning under noisy I/O. In ICML, volume 70 of Proceedings of Machine Learning Research, pages 990–998. PMLR, 2017.

[8] Sumit Gulwani. Programming by examples - and its applications in data wrangling. In Dependable Software Systems Engineering , volume 45 of

NATO Science for Peace and Security Series - D: Information and Communication Security , pages 137–158. IOS Press, 2016.

[9] Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain, and Sumit Gulwani. Neural-guided deductive search for real-time

program synthesis from examples. In ICLR (Poster). OpenReview.net, 2018.

[10] Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik. Accelerating search-based program synthesis using learned probabilistic models. In PLDI,

pages 436–449. ACM, 2018.

[11] Kairo Morton, William T. Hallahan, Elven Shum, Ruzica Piskac, and Mark Santolucito. Grammar ﬁltering for syntax-guided synthesis. pages

1611–1618, 2020.

[12] Saswat Padhi, Todd D. Millstein, Aditya V. Nori, and Rahul Sharma. Overﬁtting in synthesis: Theory and practice. In CAV (1), volume 11561 of

Lecture Notes in Computer Science, pages 315–334. Springer, 2019.

[13] Hila Peleg, Sharon Shoham, and Eran Yahav. Programming not only by example. pages 1114–1124, 2018.
[14] Elizabeth Polgreen, Ralph Abboud, and Daniel Kroening. Counterexample guided neural synthesis. CoRR, abs/2001.09245, 2020.
[15] Andrew Reynolds, Haniel Barbosa, Andres N¨otzli, Clark W. Barrett, and Cesare Tinelli. cvc4sy: Smart and fast term enumeration for syntax-guided

synthesis. In CAV (2), volume 11562 of Lecture Notes in Computer Science, pages 74–83. Springer, 2019.

Manuscript submitted to ACM

