1
2
0
2

p
e
S
9

]

Y
C
.
s
c
[

1
v
4
0
9
7
0
.
9
0
1
2
:
v
i
X
r
a

A LITERATURE SURVEY ON STUDENT FEEDBACK ASSESSMENT
TOOLS AND THEIR USAGE IN SENTIMENT ANALYSIS

A PREPRINT

Himali Aryal
Department of Information Technology and Electrical Engineering
Norwegian University of Science and Technology
Gjøvik, Norway
himalia@stud.ntnu.no

September 17, 2021

ABSTRACT

Online learning is becoming increasingly popular, whether for convenience, to accommodate work
hours, or simply to have the freedom to study from anywhere. Especially, during the Covid-19
pandemic, it has become the only viable option for learning. The effectiveness of teaching vari-
ous hard-core programming courses with a mix of theoretical content is determined by the student
interaction and responses. In contrast to a digital lecture through Zoom or Teams, a lecturer may
rapidly acquire such response from students’ facial expressions, behavior, and attitude in a physical
session, even if the listener is largely idle and non-interactive. However, student assessment in vir-
tual learning is a challenging task. Despite the challenges, different technologies are progressively
being integrated into teaching environments to boost student engagement and motivation. In this pa-
per, we evaluate the effectiveness of various in-class feedback assessment methods such as Kahoot!,
Mentimeter, Padlet, and polling to assist a lecturer in obtaining real-time feedback from students
throughout a session and adapting the teaching style accordingly. Furthermore, some of the topics
covered by student suggestions includes tutor suggestions, enhancing teaching style, course content,
and other subjects. Any input gives the instructor valuable insight into how to improve the student’s
learning experience, however, manually going through all of the qualitative comments and extract-
ing the ideas is tedious. Thus, in this paper we propose a sentiment analysis model for extracting the
explicit suggestions from the students’ qualitative feedback comments.

Keywords Kahoot! · Mentimeter · Padlet · Sentiment-analysis · Feedback Assessment Tools · Feedback analysis

1

Introduction

Online education has gained in popularity over the years due to various aspects such as accessibility in form of massive
open online courses [1], affordability and ﬂexibility as result of distance and blended education [2–4], frameworks and
tools [5–8], interactivity [9], learning pedagogy and life-long learning [10].
It has proven to be a most effective
solutions for learning, especially during the COVID-19 pandemic. According to World Health Organization (WHO)
data, COVID-19 has been recorded in over 216 countries, with millions of conﬁrmed cases [11]. Many countries
have implemented precautionary measures, such as school and university lockdowns [12]. As a result, educational
institutions began to provide the majority of their services online, including lectures and various examinations, to
over 60% of students throughout the world via various platforms [9, 13]. Online learning environments represent a
distinct way of interaction. It entails the development of educational materials, delivery of instruction, and program
management using the internet and other essential technologies [14]. During online learning, it is critical to maintain
student interaction to curb dropouts [15–17], and react to their responses while also obtaining feedback to/from them.
Feedback and assessment are used to track student progress, manage learning pace, and assess instructional methods
[18]. There are two kinds of student evaluations: summative and formative assessment. Summative assessment is
a method of attempting to summarize student learning at a speciﬁc point in time, such as the end of a course. The

 
 
 
 
 
 
A PREPRINT - SEPTEMBER 17, 2021

majority of standardized examinations are in the form of a summative assessment. They aren’t designed to give
teachers and students quick, contextualized feedback during the learning process. Formative assessment, on the other
hand, occurs when teachers give students feedback in ways that help them learn more effectively, or when students
can engage in a comparable, self-reﬂective process [19]. If the primary purpose of assessment is to support high-
quality learning, then formative assessment ought to be understood as the most important assessment practice [20].
In a typical classroom, different characteristics such as students’ faces and gestures assist the teacher in evaluating
the behaviour [21] and sentiment of the students even though they are idle. However, formative assessment in virtual
learning is a challenging task. Despite the challenges, different technologies are progressively being integrated into
teaching environments to boost student engagement and motivation [22, 23]. Different student response systems have
been utilized to improve student participation and real-time responses, including Mentimeter, Plickers, GoSoapBox,
and Poll Everywhere [24]. In addition, in recent years, the usage of game-based applications such as Kahoot! in the
classroom has grown in popularity [25]. Although the concept is not new, its application in the classroom in the context
of play or competition has a direct impact on various aspects such as classroom participation and encouragement to
the development of the activity, perception of the learning, commitment to the subject, and interest in deepening
theoretical concepts [26].

Furthermore, student responses are also important for improving pedagogic and assessing students’ learning behavior,
and to provide personalized services and content via recommender systems [27]. It helps to improve the interaction
between teachers and students, and frequent input from students aids in the development of a better learning envi-
ronment [18]. However, in most cases, feedback is provided after the course or a lecture in e-learning, which might
be too late to reconsider. Also, feedback can come from a different source, and it has to be analyzed to understand
the opinion of students. Thus, in this literature based research paper we would focus on the effectiveness of different
feedback assessment tools and how the responses from these tools can be utilized in the sentiment analysis to identify
the student’s attitude towards the course or teaching approach.

Research Questions

• How many papers were published between 2015 to 2021 that have used digital tools to collect feedback

and/or to perform quizzes?

• What publications have these papers published in?

• Which tool has been explored more?

• When did the majority of the research take place?

• What are the most widely used evaluation metrics for feedback technologies that have been studied?

• How effective it is to adapt lectures and teaching style on the go during a lecture-based on students’ feedback?

• How sentiment analysis can be utilized in the responses collected from feedback assessment tools to identify

the student’s attitude towards the course or teaching approach?

2 Related work and Literature survey

There are currently a plethora of game-based learning tools available that use variations on game aspects to inspire,
provide feedback, and structure participation pathways [28]. In this study, only the state-of-the-art of tools like Ka-
hoot!, Mentimeter, and Padlet were explored. The bulk of research studies [26, 28–36] have looked at various aspects
of these technologies, such as their effects on engagement, learning, classroom dynamics, concentration, motivation,
and enjoyment, as well as how well they have been used in education. [37] Concluded that game-based learning has
a positive impact on student performance. The students enjoy the lectures that include a quiz or survey in the middle.
The studies [26, 28, 30, 34, 35, 37–39] also demonstrated that there are numerous chances of losing concentration in
e-learning, and these technologies aided to nurture concentration and increase engagement.

Additional focus of researches [37, 40, 41] were on how students and teachers perceived the use of these tools in
education. The students’ opinions of using these technologies in the classroom are also positive. According to the
research [40], students noticed that these technologies helped them pay attention in class, and that Kahoot! or Men-
timeter quizzes were particularly engaging. It has the ability to boost student attendance and has been regarded as
interactive and quick. They also stated that the anonymity aspect allowed all students, including those who lacked
conﬁdence, to participate.

Furthermore, some studies [29, 34] compared the performance of students who utilized game-based learning to those
who used followed traditional approaches. Their performance has greatly improved as a result of the ﬁndings. Students
were divided into groups and taught using traditional methods such as PowerPoint presentations and encouraging

2

A PREPRINT - SEPTEMBER 17, 2021

students to ask questions and interact verbally, while others were taught using game-based learning, in which students
played through a series of questions related to the topic while the teacher provided explanations in between. The
evaluation [32] found that students who had the opportunity to learn in a game-based learning environment learned
more than those who were taught using traditional methods.

Another point of emphasis was the course teachers’ real-time comments to the students. The [31,35,36,38–40] studies
looked at how the technologies studied could be used to interact with students and deliver immediate feedback based
on their performance on a quiz or another activity. Only a few research, however, delved deeper into the analysis of
feedback using machine learning techniques in order to use it to improve teaching style or other aspects of education.
In the research [42] text pre-processing techniques were utilized to analyze student feedback obtained through various
ways such as online polls and OMR sheets. According to the author, this experimental study of sentiment trees and
feedback rating results in accurate polarities such as positive, negative, or neutral. Furthermore, the author stated that
this technique would aid in determining better feedback ﬁndings, which faculty might use to improve their teaching
process.

The table 1 outlines the state-of-the-art of a few feedback assessment tools as well as summarizes the context in which
these tools are assessed and aspects of which they have been studied.

2.1 Literature Review on Sentiment Analysis

Sentiment analysis on student feedback has been an active research topic in the past years. The majority of the data
utilized for the analysis comes from online courses, with only a few from the student feedback system. In this section
we will highlight some of the research papers that uses student feedback in sentiment analysis. A survey table is also
included at the end of the section, demonstrating the effectiveness of various feedback assessment tools.

The research work led by Z. Kastrati et al. [45] highlighted a systematic mapping of sentiment analysis on student
feedback using NLP and Deep Learning. The research was conducted on 92 articles out of 612 that were found
in the subject matter in learning platform environments. The PRISMA framework was utilized to guide the search
procedure, which included only articles published between 2015 and 2020. The research identiﬁed that sentiment
analysis on student’s feedback using NLP and Deep Learning is a rapidly growing ﬁeld, however, in order to fully
mature the research and development in the ﬁeld, structured datasets, standardized solutions, and work concentrated
on emotional expression are essential.

Another study [46] presents a comparison of performances of 4 machine learning and natural language processing
techniques: Naive Bayes, Maximum Entropy, Long Short-Term Memory, Bi-Directional Long Short-Term Memory
for SA. According to the research, the Bi-Directional Long Short-Term Memory algorithm outperformed in terms of
the F1-score measurement with 92.0% on the sentiment classiﬁcation task and 89.6% on the topic classiﬁcation task.
In addition, a sentiment analysis application was developed that analyses student feedback and provides overview
reports for the administrator to identify and recognize student interests in the institution.

The research study conducted in [47] investigated on aspect-level sentiment analysis to automatically identiﬁes sen-
timent or opinion polarity of the student reviews. The weakly supervised tool was applied to effectively identify
the aspect categories discussed in the unlabeled students’ reviews. In addition, the results show that the framework
performs admirably in terms of both aspect category identiﬁcation and sentiment categorization. Furthermore, it is
intended that the proposed framework would replace labor-intensive sentiment analysis techniques that heavily rely on
manually labeled data. A similar research work is also conducted in [48] where various traditional machine learning
algorithms and deep neural networks are trained on real-world dataset composed of more than 21 thousands students’
reviews manually classiﬁed into 5 different aspect categories and 3 polarity classes. Further improvements were made
by applying deep learning models in [49]. These models, however, relies only on the basic NLP techniques to process
data for ﬁnding sentiments. To better capture domain speciﬁc sentiments for educational feedback, there is a need to
incorporate both semantic and contextual information form users input. Such a model could utilize domain-speciﬁc
ontology [50–53], and publicly available lexical resources such as SenticNet, SentiWordNet, etc., [54].

In the research [42] text pre-processing techniques were utilized to analyze student feedback obtained through various
ways such as online polls and OMR sheets. According to the author, this experimental study of sentiment trees and
feedback rating results in accurate polarities such as positive, negative, or neutral. Furthermore, the author stated that
this technique would aid in determining better feedback ﬁndings, which faculty might use to improve their teaching
process.

3

Table 1: Literature survey of effectiveness of feedback assessment tools

A PREPRINT - SEPTEMBER 17, 2021

Tool(s)

Context

Aspect

R.N.

[24]

[37]

[40]

Author(s)

B. Gökbulut

A. I. Wang et
al.
M. Mohin et
al.

Year

2020

2020

Kahoot!,
Mentimeter
Kahoot!

2020

Mentimeter

[41]

H. Bicen et al.

2018

Kahoot!

[29]

[43]

A. I. Wang et
al.

H. Uzunboylu
et al.

2019

2020

Kahoot!,
PowerPoint,
Sembly
Kahoot!

[30]

R. Raju et al.

2021

Kahoot!,
Mentimeter,
Quizizz
Kahoot!

To study effectiveness of using a gamiﬁca-
tion technique on e-learning
Study the advantages and disadvantages of
using Kahoot in the classroom
Working, its characteristics, applications
and educational beneﬁts, and students’ per-
ceptions of Mentimeter use
Student attitude towards opinions gamiﬁ-
cation methods

Evaluated students’ performance on three
different teaching styles

Evaluate the perception of teachers and
students on the use of Kahoot! as a teach-
ing tool

Students engagement during online lec-
tures

Effectiveness of Kahoot

2019

Mentimeter

Effectiveness of mentimeter in e-learning

2019

Catindig,
J.
M. S. Pru-
dente
F.
Canlas et al.

Quiroz

F. Soares et al.

2020

2019

2021

2018

A. R. M.
A.
Deni,
Ariﬁn
K. P. Nuci et
al.

Parra-
et

T.
Santos
al.
A. I. Wang, A.
Lieberoth

[38]

[39]

[44]

[31]

[32]

[26]

[28]

[33]

[34]

[35]

[36]

Cheat sheets,
Padlet
Padlet

Kahoot!,
Google Form
Quiz
Kahoot!

Effectiveness of tools

Evaluated Padlet as a repository which
stores resources related to classroom activ-
ities and students’ work-in-progress
In-lecture quizzes in online classes

Quizzes, and Questionnaire

2018

Kahoot!

Use of points and audio affect in the learn-
ing environment

H. Ucar, A. T.
Kumtepe

2017

Kahoot!

P . M. Tan, J.
J. Saucerman

2017

Kahoot!, Sur-
veyMonkey

2017

Kahoot!

Qualitative case study is to explore the stu-
dent’s perspectives on game-based learn-
ing
Evaluated the beneﬁts of gamiﬁcation on
students in the particular context of a stu-
dent response system (SRS)
Quizzes

C. M. Plump,
J. LaRosa
S. A. Licorish
et al.

2018

Kahoot!

Semi-structured interviews with students

4

Effectiveness of tools in the
e-learning
Performance,
and instructor attitudes
Formative assessment

and student

Student motivation, effec-
tiveness of using it class-
room
Interactiveness of gamiﬁed
methodologies for teaching
theoretical lectures
Evaluated student’s perfor-
mance and semi-structured
interview with teacher and
student
Student performance

Student performance, atti-
tude

Students’ participation, en-
gagement,
interaction with
instructor
Tool as a collaborative work

Improved accessibility to
teacher’s input and peers’
work-in-progress
Signiﬁcant increase in stu-
dents’ engagement, interac-
tion, and interest in lectures
Students’ attention, motiva-
tion and enjoyment are in-
creased
Student engagement, learn-
ing,
classroom dynamics,
concentration, motivation
and enjoyment
Engage and motivate the on-
line learners

Student motivation, enjoy-
ment, and encouragement to
collaborate
Student engagement and im-
mediate feedback
Classroom dynamics, moti-
vation and learning process,
real-time feedback

A PREPRINT - SEPTEMBER 17, 2021

3 Research Design

3.1 Search Strategy

We used systematic mapping as a research methodology to review the literature for this research. To conduct this
literature review, we followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
statement [55]. The PRISMA method of systematic review enables a comprehensive review of all relevant articles
and aids in the discovery of a clear response to the research question. The approach uses a variety of inclusion and
exclusion criteria to help with decision-making.

Figure 1: PRISMA Methodology

For this study, we followed a desk work methodology where scientiﬁc papers that contained information feedback
assessment tools were searched and gathered. The keywords search approach was used to locate these resources.
Keywords like ‘Kahoot!’, ‘Mentimeter’, ‘Padlet’, ‘Feedback Assessment Tools, ‘Game-Based approach’ were used to
search these resources.

Following the initial research, we limited our search only to a few feedback assessment tools (‘Kahoot!, ‘Mentimeter,
‘Padlet’) and focused our survey on which context of these has been studied and which aspect i.e. information about
the themes/topics for which students have commented on these tools are in the learning environment and whether they
are beneﬁcial to employ in the future. The table 2 includes the query string used for searching the articles.

5

A PREPRINT - SEPTEMBER 17, 2021

Table 2: Query String used for searching articles

Search String (Query)
Kahoot!, "Kahoot!" AND Feedback, "Kahoot!" AND "Feedback" AND "Sentiment"
Mentimeter, "Mentimeter" AND "Feedback", "Mentimeter" AND "Feedback" AND "Sentiment"
Padlet, "Padlet" AND "Feedback", "Padlet" AND "Feedback" AND "Sentiment"

3.2 Time Period and Digital Databases

The study incorporates the articles published between 2015 and 2021. Since the study was conducted in 2021, it
encompassed papers published up until July of that year. We used the following online research databases and engines
to conduct our search:

• ACM Digital Library

• IEEE Xplore

• ScienceDirect

• Scopus

• SpringerLink

3.3

Identiﬁcation of Primary Studies

The search began with a simple query, which we narrowed down sequentially. In the ﬁrst stage, using the queries
string ‘Kahoot!’, ‘Mentimeter’, and ‘Padlet’, a total of 1638 articles published in the English language were gathered.
The ﬁgure 2 depicts the statistics of all the articles acquired initially in the various digital libraries.

Figure 2: Number of articles found in different digital libraries

3.4 Study Selection/Screening

The papers were screened using a number of inclusive criteria. Articles published only in English, papers released
between 2015 and 2021, and prioritized peer-reviewed articles were among the inclusion criteria utilized for the

6

A PREPRINT - SEPTEMBER 17, 2021

ﬁltering. The table 3 displays annual data for articles published on various feedback tools on different database
sources.

Table 3: Articles published on different years

Tools

Kahoot

Mentimeter

Padlet

Year ACM DL IEEE Xplore
2015
2016
2017
2018
2019
2020
2021
2015
2016
2017
2018
2019
2020
2021
2015
2016
2017
2018
2019
2020
2021

0
3
2
20
20
31
8
0
0
0
3
2
6
5
0
2
5
5
6
20
11

0
1
3
9
2
5
5
0
0
1
0
0
0
0
1
1
0
0
1
0
0

Science Direct
4
7
13
23
24
42
51
0
2
1
3
7
12
23
3
2
9
7
9
8
21

Scopus
4
7
17
49
63
73
45
0
0
1
3
3
12
6
5
4
6
12
15
26
11

SpringerLink
6
11
14
60
71
99
156
1
1
4
4
16
19
43
12
9
18
31
42
51
57

3.5 Eligibility Criteria

The search is narrowed down to speciﬁc sorts of articles, such as journal articles and conference papers. The research
was also restricted to the subject of computer science. A total of 154 articles were identiﬁed to meet the eligibility
requirements using the following inclusion criteria.

• Type of publication

• Field of study

• Journal or conference papers

• Time-frame (2015-2021)

• Language (English)

The table 4 shows the ﬁnal 154 articles obtained with the above mentioned eligibility criteria. On the query string
"sentiment AND feedback AND (mentimeter OR Kahoot! OR Padlet)" a few articles were retrieved; however, upon
closer inspection, it was discovered that sentiment analysis was not done on the feedback received from the tools
(Kahoot!, Mentimeter, and Padlet).

4 Result and Discussion

The answers to the research questions are included in this section.

How many papers were published between 2015 to 2021 that have used digital tools to collect feedback and/or to
perform quizzes?
Since the development of Kahoot!, it has become a popular research topic in the ﬁeld of education, with a large
number of articles published since then. In the Mentimeter and Padlet, it’s the same. Without taking into account
varied inclusion and exclusion criteria, we found a total of 1638 articles in these three tools. Whereas, considering
different inclusion/exclusion criteria, 154 articles were found in journals and conferences in the ﬁeld of Computer
Science between 2015 to 2021. These 154 articles were published either on Kahoot! or Mentimeter or Padlet in the
context of feedback assessment or feedback analysis.

7

A PREPRINT - SEPTEMBER 17, 2021

Table 4: Number of articles collected from various databases

Search Queries

Digital Library No inclusion criteria with inclusion CS related

Kahoot! AND Feedback

Mentimeter AND feedback

Padlet AND feedback

Total

ACM
IEEE
ScienceDirect
Scopus
SpringerLink
ACM
IEEE
ScienceDirect
Scopus
SpringerLink
ACM
IEEE
ScienceDirect
Scopus
SpringerLink

68
7
116
40
382
12
0
30
4
72
0
1
40
15
212

67
7
116
40
346
12
0
30
4
70
0
1
39
15
177

2
6
23
26
106
1
0
3
3
21
0
1
3
9
38

Journal/Conference
2
6
22
17
56
1
0
2
2
15
0
1
3
6
19
154

What publications have these papers published in?
We focused our research on the ACM (Association for Computing Machinery), IEEE Xplore, ScienceDirect, Sco-
pus, and SpringerLink digital libraries. SpringerLink is the most popular publisher (90 articles that meet the in-
clusion/eligibility criterion), while IEEE Xplore has the most list articles (7 items that meet the inclusion/eligibility
criteria) as shown in the table 4. Scopus is in third place, with 25 conference papers, after ScienceDirect, which has
27 conference/journal papers.

When did the majority of the research take place?
The total number of publications published in the year 2021 was the highest, meeting the inclusion criteria and eligibil-
ity criteria as shown in the table 5. After 2018, the number of articles published each year is higher than before. This
increased number of articles in the recent years could be due to the COVID-19 outbreak. Because of the Covid-19 pan-
demic, educational institutions have been obliged to shift to an online teaching method all around the world [12]. As
a result, an increased study on the topic of e-learning and various online feedback assessment systems was observed.

Table 5: Articles published per year

Digital Library
ACM
IEEE
ScienceDirect
Scopus
SpringerLink
Total

2015
0
0
2
2
2
6

2016
0
0
2
1
4
7

2017
0
1
1
2
7
11

2018
1
1
3
9
19
34

2019
0
1
6
8
21
36

2020
1
0
5
1
17
25

2021
1
1
10
1
23
37

Which tool has been explored more?
We looked into Kahoot!, Mentimeter, and Padlet, which are three different feedback assessment tools. According
tools
to the data, Kahoot!
with considering the inclusion/eligibility criteria. Whereas 29 studies focused on Padlet feedback analysis and 20
publications on mentimeter were identiﬁed. According to our observations, Kahoot! is the most popular tool among
the technologies considered for feedback assessment in the learning environment.

is the most extensively used tool. We discovered 103 articles solely on the Kahoot!

What are the most widely used evaluation metrics for feedback technologies that have been studied?
The 154 articles for the metadata analysis were chosen once the ﬁnal inclusion and exclusion criteria were determined.
Upon closer inspection, we discovered that studies had been undertaken in the following areas:

• Student perception

• Collaborative knowledge building

• Improve student learning motivation and performance

• Effectiveness of tool(s)

8

A PREPRINT - SEPTEMBER 17, 2021

How effective it is to adapt lectures and teaching style on the go during a lecture-based on students’ feedback? The
implementation of a feedback system in the lecture is seen positively by both students and teachers [24, 40, 41, 43].
The perspectives and experiences of others who have used these tools in the classroom offer excellent insight all the
while diversifying the learning process [43].

5

Identiﬁed Challenges and Research Gap

According to the results of the survey, several in-class feedback assessment tools such as Kahoot!, Mentimeter, Padlet
are quite effective. However, we were unable to locate any research that focused on sentiment analysis on data collected
from feedback tools. We can’t deny that sentiment analysis research on student review isn’t new; nonetheless, the most
of studies have focused on data acquired from Coursera reviews [47], social media pages [56], or other online courses
platforms [57]. The research on the sentiment analysis based on the feedback assessment tools especially Kahoot!,
Mentimeter and Padlet is mostly unexplored.

5.1 Future Direction

One approach to determine the polarity of student comments received via feedback assessment tools (Kahoot!, Men-
timeter, and Padlet) is using sentiment analysis [49]. The methodology of Student Feedback Sentiment Analysis is
depicted in the ﬂowchart as shown in the ﬁgure 3. The depicted model is inspired from [58, 59]. We will focus on the
following aspects when conducting sentiment analysis:

Figure 3: Proposed Sentiment Analysis Model

9

A PREPRINT - SEPTEMBER 17, 2021

1. Data Collection: Data collecting is an important phase in the Sentiment Analysis. The more data there is,
the better the result. In a course during the Fall semester of 2021, testing will be conducted for a short-list of
tools such as Kahoot!, Mentimeter, and Padlet.

2. Data pre-processing: Data consist of the mixture of text, numbers, and special characters. Data pre-
processing is required before further data processing, including converting text to lowercase, reducing stop
words, removing unnecessary punctuation, locating exclamation marks or question marks, and removing
inconsistent letter casing.

3. Feature Extraction and Conceptualization of Sentiment: We can better interpret the text by employing
feature extraction, which will improve the accuracy of training the data using a new method. Term Pres-
ence and Frequency, Part of Speech Tagging, and Negation are some of the features that can be used. Also
incorporating the semantic context using publicly available lexical databases (i.e WordNet, SentiWordNet,
SenticNet, etc.) [54] or semantically rich representations using ontologies [60,61] and their thesaurus [62,63]
to identify opinion and attitude of users from text would be an import aspect to further investigate.

4. Training Model with different classiﬁers: Following feature extraction, we can use a number of existing
algorithms to model our data, including Naive Bayes, Support Vector Machines, CNN, and others. The
algorithm depends on the features of the data and there is no said algorithm that is perfect for any data. To
identify the optimal model for the analysis, we’ll run the data through a variety of features and algorithms.

6 Conclusion

Due to the increasing demand of online education and distant learning as a result of the COVID19 epidemic, students’
feedback analysis is the most vital task for professors as well as educational institutes. Hence implementing various
feedback assessment tool is essentials. Although a lot of study has been done on the effectiveness of feedback as-
sessment tools, there is still work to be done on sentiment analysis on data received from feedback assessment tools
(Kahoot!, Mentimeter, and Padlet). This article focused on the current state of research on feedback assessment tools,
as well as the contexts in which these tools have been investigated and the status of sentiment analysis. The ﬁndings
imply that feedback evaluation tools are now vital in the ﬁeld of education, and that sentiment analysis on data acquired
from these feedback assessment tools is rarely investigated.

References

[1] F. Dalipi, S. Y. Yayilgan, A. S. Imran, and Z. Kastrati, “Towards understanding the MOOC trend: pedagogical
challenges and business opportunities,” in International conference on learning and collaboration technologies.
Springer, 2016, pp. 281–291.

[2] A. S. Imran, “Interactive media learning object in distance and blended education,” in Proceedings of the 17th

ACM international conference on Multimedia, 2009, pp. 1139–1140.

[3] R. Garrison, “Implications of online and blended learning for the conceptual development and practice of distance
education,” International Journal of E-Learning & Distance Education/Revue internationale du e-learning et la
formation à distance, vol. 23, no. 2, pp. 93–104, 2009.

[4] Z. Kastrati, A. Kurti, and J. Hagelbäck, “The effect of a ﬂipped classroom in a spoc: Students’ perceptions and
attitudes,” in Proceedings of the 2019 11th International Conference on Education Technology and Computers,
ser. ICETC 2019. ACM, 2019, p. 246–249. [Online]. Available: https://doi.org/10.1145/3369255.3369304

[5] A. S. Imran and F. A. Cheikh, “Multimedia learning objects framework for e-learning,” in 2012 International

Conference on E-Learning and E-Technologies in Education (ICEEE).

IEEE, 2012, pp. 105–109.

[6] M. Aparicio, F. Bacao, and T. Oliveira, “An e-learning theoretical framework,” An e-learning theoretical frame-

work, no. 1, pp. 292–307, 2016.

[7] A. S. Imran, F. A. Cheikh, and S. J. Kowalski, “Automatic annotation of lecture videos for multimedia driven
pedagogical platforms,” Knowledge Management & E-Learning: An International Journal, vol. 8, no. 4, pp.
550–580, 2016.

[8] F. H. Glancy and S. K. Isenberg, “A conceptual elearning framework,” in European, Mediterranean & Middle

Eastern Conference on Information Systems.(May 30–31, 2011, Athens). Athens, 2011, pp. 636–650.

[9] A. S. Imran and S. J. Kowalski, “HIP–a technology-rich and interactive multimedia pedagogical platform,” in

International Conference on Learning and Collaboration Technologies. Springer, 2014, pp. 151–160.

10

A PREPRINT - SEPTEMBER 17, 2021

[10] D. Eckstein, B. Irby, and C.-S. Li, “An overview of online education: Attractiveness, beneﬁts, challenges,
concerns and recommendations,” in Proceedings of Society for Information Technology & Teacher Education
International Conference 2007, R. Carlsen, K. McFerrin, J. Price, R. Weber, and D. A. Willis, Eds.
San
Antonio, Texas, USA: Association for the Advancement of Computing in Education (AACE), March 2007, pp.
302–307. [Online]. Available: https://www.learntechlib.org/p/24550

[11] W. H. Organization. (2019-12) Who coronavirus disease (covid-19) dashboard. Accessed on 2021-08-09.

[Online]. Available: https://covid19.who.int/

[12] K. Mukhtar, K. Javed, M. Arooj, and A. Sethi, “Advantages, limitations and recommendations for online learning
during COVID-19 pandemic era,” Pakistan journal of medical sciences, vol. 36, no. COVID19-S4, p. S27, 2020.
[13] C. R. Graham, W. Woodﬁeld, and J. B. Harrison, “A framework for institutional adoption and implementation
of blended learning in higher education,” The Internet and Higher Education, vol. 18, pp. 4–14,
2013, blended Learning in Higher Education: Policy and Implementation Issues. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S1096751612000607

[14] P. Dillenbourg, D. Schneider, and P. Synteta, “Virtual learning environments,” Proceedings of the 3rd Hellenic

Conference Information & Communication Technologies in Education, vol. 2002, 01 2002.

[15] F. Dalipi, A. S. Imran, and Z. Kastrati, “MOOC dropout prediction using machine learning techniques: Review
IEEE, 2018,

and research challenges,” in 2018 IEEE Global Engineering Education Conference (EDUCON).
pp. 1007–1014.

[16] C. Márquez-Vera, A. Cano, C. Romero, A. Y. M. Noaman, H. Mousa Fardoun, and S. Ventura, “Early dropout
prediction using data mining: a case study with high school students,” Expert Systems, vol. 33, no. 1, pp. 107–
124, 2016.

[17] A. S. Imran, F. Dalipi, and Z. Kastrati, “Predicting student dropout in a MOOC: An evaluation of a deep neural
network model,” in Proceedings of the 2019 5th International Conference on Computing and Artiﬁcial Intelli-
gence, 2019, pp. 190–195.

[18] D. Nicol and D. Macfarlane, “Formative assessment and self-regulated learning: A model and seven principles

of good feedback practice,” Studies in Higher Education, vol. 31, pp. 199–218, 05 2006.

[19] M. Taras, “Assessment – summative and formative – some theoretical reﬂections,” British Journal of
Educational Studies, vol. 53, no. 4, pp. 466–478, 2005. [Online]. Available: https://doi.org/10.1111/j.
1467-8527.2005.00307.x

[20] S. Chappuis and J. Chappuis, “The best value in formative assessment,” Challenging the Whole Child: Reﬂections

on Best Practice in Learning, Teaching and Leadership, pp. 219–226, 2007.

[21] K. Pireva, A. S. Imran, and F. Dalipi, “User behaviour analysis on LMS and MOOC,” in 2015 IEEE Conference

on e-Learning, e-Management and e-Services (IC3e).

IEEE, 2015, pp. 21–26.

[22] K. Pireva, R. Tahir, A. S. Imran, and N. Chaudhary, “Evaluating learners’ emotional states by monitoring brain
waves for comparing game-based learning approach to pen-and-paper,” in 2019 IEEE Frontiers in Education
Conference (FIE).

IEEE, 2019, pp. 1–8.

[23] F. Dalipi, A. S. Imran, F. Idrizi, and H. Aliu, “An analysis of learner experience with MOOCs in mobile and
desktop learning environment,” in Advances in human factors, business management, training and education.
Springer, 2017, pp. 393–402.

[24] B. Gökbulut, “The effect of mentimeter and kahoot applications on university students’ e-learning,” World Jour-

nal on Educational Technology: Current Issues, vol. 12, pp. 107–116, 04 2020.

[25] T. Anastasiadis, G. Lampropoulos, and K. Siakas, “Digital game-based learning and serious games in education,”
International Journal of Advances in Scientiﬁc Research and Engineering (ijasre), vol. 4, no. 12, pp. 139–144,
2018.

[26] T. Parra-Santos,

formative assessment

J.-M. Molina-Jordá, G. Casanova-Pastor, and L.-P. Maiorano-Lauria, “Gamiﬁcation
the Sixth
in the framework of engineering learning,” in Proceedings of
for
International Conference on Technological Ecosystems for Enhancing Multiculturality, ser. TEEM’18.
New York, NY, USA: Association for Computing Machinery, 2018, p. 61–65.
[Online]. Available:
https://doi.org/10.1145/3284179.3284193

[27] A. S. Imran, K. Muhammad, N. Fayyaz, M. Sajjad et al., “A systematic mapping review on MOOC recommender

systems,” IEEE Access, 2021.

[28] A. I. Wang and A. Lieberoth, “The effect of points and audio on concentration, engagement, enjoyment, learning,
motivation, and classroom dynamics using kahoot,” in European conference on games based learning, vol. 20.
Academic Conferences International Limited, 2016.

11

A PREPRINT - SEPTEMBER 17, 2021

[29] A. I. Wang, R. Sætre, T. Rydland, and Y. Dahl, “Evaluation of interactive and gamiﬁed approaches for teaching
ict theory: A study of powerpoint, sembly, and kahoot,” in European Conference on Games Based Learning.
Academic Conferences International Limited, 2019, pp. 784–XXIV.

[30] R. Raju, S. Bhat, S. Bhat, R. D’Souza, and A. B. Singh, “Effective usage of gamiﬁcation techniques to boost

student engagement,” Journal of Engineering Education Transformations, vol. 34, pp. 713–717, 2021.

[31] A. R. M. Deni and A. Ariﬁn, “Using padlet for project-based learning in documentary ﬁlmmaking,”
in Proceedings of
the 2019 3rd International Conference on Education and E-Learning, ser. ICEEL
2019. New York, NY, USA: Association for Computing Machinery, 2019, p. 30–35. [Online]. Available:
https://doi.org/10.1145/3371647.3371648

[32] K. P. Nuci, R. Tahir, A. I. Wang, and A. S. Imran, “Game-based digital quiz as a tool for improving students’

engagement and learning in online lectures,” IEEE Access, vol. 9, pp. 91 220–91 234, 2021.

[33] H. Ucar and A. T. Kumtepe, “Using the game-based student response tool kahoot!

in an online class:
Perspectives of online learners,” in Proceedings of Society for Information Technology & Teacher Education
International Conference 2017, P. Resta and S. Smith, Eds. Austin, TX, United States: Association
for the Advancement of Computing in Education (AACE), March 2017, pp. 303–307. [Online]. Available:
https://www.learntechlib.org/p/177857

[34] P. M. Tan and J. J. Saucerman, “Enhancing learning and engagement through gamiﬁcation of student response
systems,” in 2017 ASEE Annual Conference & Exposition, no. 10.18260/1-2–28276. Columbus, Ohio: ASEE
Conferences, June 2017, https://peer.asee.org/28276.

[35] C. M. Plump and J. LaRosa, “Using kahoot! in the classroom to create engagement and active learning: A game-
based technology solution for elearning novices,” Management Teaching Review, vol. 2, no. 2, pp. 151–158,
2017.

[36] S. A. Licorish, H. E. Owen, B. Daniel, and J. L. George, “Students’ perception of kahoot!’s inﬂuence on teaching

and learning,” Research and Practice in Technology Enhanced Learning, vol. 13, no. 1, pp. 1–23, 2018.

[37] A. I. Wang and R. Tahir, “The effect of using kahoot!

for learning–a literature review,” Computers &
Education, vol. 149, p. 103818, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0360131520300208

[38] J. Catindig and M. S. Prudente, “Effectiveness of kahoot as a revision tool in studying waves,” in Proceedings
of the 10th International Conference on E-Education, E-Business, E-Management and E-Learning, ser. IC4E
’19. New York, NY, USA: Association for Computing Machinery, 2019, p. 119–123. [Online]. Available:
https://doi.org/10.1145/3306500.3306550

[39] F. Quiroz Canlas, S. Nair, and A. Nirmal Doss, “Mentimeter app in computer science courses: Integration model
and students’ reception,” in 2020 12th International Conference on Education Technology and Computers, ser.
ICETC’20. New York, NY, USA: Association for Computing Machinery, 2020, p. 1–5. [Online]. Available:
https://doi.org/10.1145/3436756.3436757

[40] M. Mohin, L. Kunzwa, and S. Patel, “Using mentimeter to enhance learning and teaching in a large class,” in

EdArXiv, 10 2020.

[41] H. Bicen and S. Kocakoyun, “Perceptions of students for gamiﬁcation approach: Kahoot as a case study.” Inter-

national Journal of Emerging Technologies in Learning, vol. 13, no. 2, 2018.

[42] T. Banan, S. Sekar, J. N. Mohan, P. Shanthakumar, and S. Kandasamy, “Analysis of student feedback by ranking
the polarities,” in Proceedings of the Second International Conference on Computer and Communication Tech-
nologies, S. C. Satapathy, K. S. Raju, J. K. Mandal, and V. Bhateja, Eds. New Delhi: Springer India, 2016, pp.
203–214.

[43] H. Uzunboylu, E. Galimova, R. Kurbanov, A. Belyalova, N. Deberdeeva, and M. Timofeeva, “The views
of the teacher candidates on the use of kahoot as a gaming tool,” International Journal of Emerging
Technologies in Learning (iJET), vol. 15, no. 23, pp. 158–168, December 2020. [Online]. Available:
https://www.learntechlib.org/p/218462

[44] F. Soares, C. P. Leão, and S. Araújo, “Cheat sheets and padlet: A metacognitive learning tool,” in
Eighth International Conference on Technological Ecosystems for Enhancing Multiculturality, ser. TEEM’20.
New York, NY, USA: Association for Computing Machinery, 2020, p. 393–398. [Online]. Available:
https://doi.org/10.1145/3434780.3436677

[45] Z. Kastrati, F. Dalipi, A. S. Imran, K. Pireva Nuci, and M. A. Wani, “Sentiment analysis of students’ feedback
with NLP and deep learning: A systematic mapping study,” Applied Sciences, vol. 11, no. 9, 2021. [Online].
Available: https://www.mdpi.com/2076-3417/11/9/3986

12

A PREPRINT - SEPTEMBER 17, 2021

[46] P. X. V. Nguyen, T. T. T. Hong, K. V. Nguyen, and N. L.-T. Nguyen, “Deep learning versus traditional classiﬁers
on vietnamese students’ feedback corpus,” in 2018 5th NAFOSTED Conference on Information and Computer
Science (NICS), 2018, pp. 75–80.

[47] Z. Kastrati, A. S. Imran, and A. Kurti, “Weakly supervised framework for aspect-based sentiment analysis on

students’ reviews of MOOCs,” IEEE Access, vol. 8, pp. 106 799–106 810, 2020.

[48] Z. Kastrati, B. Arifaj, A. Lubishtani, F. Gashi, and E. Nishliu, “Aspect-based opinion mining of students’
reviews on online courses,” in Proceedings of the 2020 6th International Conference on Computing and
Artiﬁcial Intelligence, 2020, pp. 510–514. [Online]. Available: https://doi.org/10.1145/3404555.3404633
[49] M. Edalati, A. S. Imran, Z. Kastrati, and S. M. Daudpota, “The potential of machine learning algorithms for
sentiment classiﬁcation of students’ feedback on MOOC,” in Proceedings of SAI Intelligent Systems Conference.
Springer, 2021, pp. 11–22.

[50] Z. Kastrati, A. S. Imran, and S. Y. Yayilgan, “SEMCON: semantic and contextual objective metric,” in Pro-
ceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015), 2015, pp.
65–68.

[51] Z. Kastrati, A. S. Imran, and S. Yildirim-Yayilgan, “SEMCON: a semantic and contextual objective metric for
enriching domain ontology concepts,” International Journal on Semantic Web and Information Systems (IJSWIS),
vol. 12, no. 2, pp. 1–24, 2016.

[52] Y. Choi, Y. Kim, and S.-H. Myaeng, “Domain-speciﬁc sentiment analysis using contextual

feature
generation,” in Proceedings of the 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass
Opinion. New York, NY, USA: Association for Computing Machinery, 2009, p. 37–44. [Online]. Available:
https://doi.org/10.1145/1651461.1651469

[53] Z. Kastrati, A. S. Imran, and S. Y. Yayilgan, “An improved concept vector space model for ontology based
classiﬁcation,” in 2015 11th International Conference on Signal-Image Technology & Internet-Based Systems
(SITIS).

IEEE, 2015, pp. 240–245.

[54] A. Esuli and F. Sebastiani, “SENTIWORDNET: A publicly available lexical resource for opinion mining,”
in Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06).
Genoa,
[Online]. Available:
http://www.lrec-conf.org/proceedings/lrec2006/pdf/384_pdf.pdf

European Language Resources Association (ELRA), May 2006.

Italy:

[55] D. Moher, A. Liberati, J. Tetzlaff, D. G. Altman, and P. Group, “Preferred reporting items for systematic reviews
and meta-analyses: the prisma statement,” PLoS medicine, vol. 6, no. 7, p. e1000097, 2009. [Online]. Available:
https://doi.org/10.1371/journal.pmed.1000097

[56] E. M. Aman Ullah, “Sentiment analysis of students feedback: A study towards optimal tools,” in 2016 Interna-

tional Workshop on Computational Intelligence (IWCI), 12 2016, pp. 175–180.

[57] Z. Kastrati, A. Kurti, and A. S. Imran, “WET: Word embedding-topic distribution vectors for MOOC video

lectures dataset,” Data in brief, vol. 28, p. 105090, 2020.

[58] A. S. Imran, S. M. Daudpota, Z. Kastrati, and R. Batra, “Cross-cultural polarity and emotion detection using
sentiment analysis and deep learning on COVID-19 related tweets,” IEEE Access, vol. 8, pp. 181 074–181 090,
2020.

[59] A. S. Imran, S. M. Doudpota, Z. Kastrati, and R. Bhatra, “Cross-cultural polarity and emotion detection using
sentiment analysis and deep learning–a case study on COVID-19,” arXiv preprint arXiv:2008.10031, 2020.
[60] Z. Kastrati, A. S. Imran, and S. Y. Yayilgan, “The impact of deep learning on document classiﬁcation using
semantically rich representations,” Information Processing & Management, vol. 56, no. 5, pp. 1618–1632, 2019.
[61] P. Thakor and S. Sasi, “Ontology-based sentiment analysis process for social media content,” Procedia Computer
Science, vol. 53, pp. 199–207, 2015, iNNS Conference on Big Data 2015 Program San Francisco, CA, USA
8-10 August 2015. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1877050915017986
[62] Z. Kastrati and A. S. Imran, “Performance analysis of machine learning classiﬁers on improved concept
vector space models,” Future Generation Computer Systems, vol. 96, pp. 552–562, 2019. [Online]. Available:
https://www.sciencedirect.com/science/article/pii/S0167739X1831745X

[63] K. D. K. Priya and P. Valarmathie, “Multilevel sentiment analysis using domain thesaurus,” Journal of Ambient

Intelligence and Humanized Computing, vol. 12, pp. 5017–5028, 2021.

13

