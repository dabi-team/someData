0
2
0
2

c
e
D
1
1

]

C
O
.
h
t
a
m

[

2
v
2
3
6
5
0
.
2
1
0
2
:
v
i
X
r
a

A generalised log-determinant regularizer for online
semi-deﬁnite programming and its applications⋆

Yaxiong Liu1,3, Ken-ichiro Moridomi4, Kohei Hatano2,3, and Eiji Takimoto1

yaxiong.liu,hatano,eiji
}

{

kenichiro moridomi@so-netmedia.jp4

@inf.kyushu-u.ac.jp123

1 Department of Informatics, Kyushu University, Japan
2 Faculty of Arts and Science, Kyushu University, Japan
3 RIKEN AIP, Japan
4 SMN Corporation

Abstract. We consider a variant of online semi-deﬁnite programming problem
(OSDP): The decision space consists of semi-deﬁnite matrices with bounded Γ -
trace norm, which is a generalization of trace norm deﬁned by a positive deﬁnite
matrix Γ . To solve this problem, we utilise the follow-the-regularized-leader al-
gorithm with a Γ -dependent log-determinant regularizer. Then we apply our gen-
eralised setting and our proposed algorithm to online matrix completion(OMC)
and online similarity prediction with side information. In particular, we reduce
the online matrix completion problem to the generalised OSDP problem, and the
side information is represented as the Γ matrix. Hence, due to our regret bound
for the generalised OSDP, we obtain an optimal mistake bound for the OMC by
removing the logarithmic factor.

1 Introduction

Online semi-deﬁnite programming(OSDP)[6] plays a central role in online learning
with matrix. Usually, OSDP is given as follows: on round t
[T ], algorithm predicts
, then adversary gives a loss matrix Lt, and the algorithm incurs the
a matrix Wt ∈ K
Lt. Our goal is to minimize the regret
Frobenius inner product of Wt and Lt as Wt •
deﬁned as

∈

T

T

•

W

Lt,

t=1
X

t=1
X

Wt •

K
∈ K

min
W
∈K

RegretT =

Lt −
is a set of positive deﬁnite matrices with bounded trace norm Tr(W )

where
≤
τ, W
. To solve OSDP problem, follow the regularizer leader(FTRL), a tradi-
tional algorithm in online learning [14], is always involved. For different online learn-
ing problem like expert advice, by choosing different regularizer, like entropy, we can
obtain satisfying regret bound [1]. To OSDP, [13] give FTRL with log-determinant reg-
ularizer and obtain regret bound as O(√τ T ). This model has been wildly utilised in
online collaborative ﬁltering [15] [2] [10], online max-cut problem [12], and min-max
problem [11].

(1)

⋆ This work was supported by JSPS KAKENHI Grant Numbers JP19H04174 and JP19H04067,

respectively.

 
 
 
 
 
 
2

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

In this paper we consider a generalization of online semi-deﬁnite programming
problem with bounded Γ -trace norm, where Γ is a strictly positive deﬁnite matrix.
Hence, the constraint of W is presented as Tr(Γ W Γ )
τ, a bounded Γ -trace norm.
We believe that this variance recovers the usual form, if Γ is identity matrix. In our set-
ting, algorithm from [13] can not be directly applied, since Tr(W ) can not be bounded
τ, while Γ is an arbi-
by Tr(Γ W Γ ), say Tr(Γ W Γ )
trary strictly positive deﬁnite matrix. Therefore, we generalise the log-determinant reg-
ularizer with respect to Γ , and achieve an upper bound to regret as O(√τ T ) for our
generalised OSDP problem.

τ implies not Tr(W )

≤

≤

≤

We believe that our generalised setting is not castle in the air. In this paper we
involve our result to online matrix completion(OMC) with side information [8] and on-
line similarity prediction [4] with side information. We ﬁrstly show that OMC with side
information can be reduced to our generalised OSDP problem. Instead of FTRL with
log-determinant regularizer in [13], the bound is only related to the size of comparator
matrix, if utilizing our proposed algorithm, we can obtain a even tighter mistake bound.
In OMC problem, side information, associated with row and column, implies the “pre-
dictiveness” of comparator matrix by some inherent characters, and we can represent
this side information in our generalised OSDP as matrix Γ . For an ideal case, if the
comparator matrix is latent block structured, the quasi-dimension, based on side in-
formation, can efﬁciently reduce the mistake bound. The reduction and algorithm for
online similarity prediction is same as OMC with side information.

So in this paper our main contribution is as follows:

– 1. We extend the FTRL algorithm for the generalised OSDP problem with bounded
Γ -trace norm, by introducing a new log-determinant regularizer depending on the
matrix Γ and give a regret bound. Note that our result recovers the previously
known bound [13] in the case that Γ is the identity matrix.

– 2. Applications of our ﬁrst technical results contain the OMC [7,8] and the online
similarity prediction [4,8] with side information. For the OMC with side informa-
tion we ﬁrstly reduce the problem to the generalised OSDP where the side informa-
tion is encoded as the matrix Γ . Then by running the proposed algorithm the FTRL
with our Γ dependent log-determinant regularizer, we achieve an optimal mistake
bound of OMC, which matches the lower bound when Γ is the identity matrix(no
side information case [7]), improving the previous result of Herbster et.al [8] by a
logarithmic factor. Furthermore, we show our reduction and an algorithm for the
online similarity prediction and obtain improved mistake bound without a logarith-
mic factor as well.

Our paper is composed as follows: In section 3, we give the formal setting of the
OSDP and the OMC with side information. The main algorithm and regret bound are
given in section 4. In section 5 and 6 we show application of our proposed algorithm
to the OMC and online similarity prediction with side information. In appendix we
describe technique lemmata, and some details of a case where side information matters.

A generalised log-determinant regularizer for online semi-deﬁnite programming

3

2 Related work

≤

OSDP problem has been explored by [6] [3] [13]. [13] give a regret bound by run-
O(√τ T ), where the
ning FTRL with log-determinant regularizer as RegretOSDP ≤
decision set is a set of positive deﬁnite matrix with bounded trace Tr(W )

τ.

recently work [8], authors give a mistake bound as O(

Online matrix completion has been studied by [7] and with side information [8]. In
b
Dγ2 ln(m + n)) for the realizable
case, where
is lower bounded by quasi-dimension of the comparator matrix. In ideal
case, if this comparator matrix obtains some latent structure, like (k, l)-biclustered,
then we can set that the side information matrix as PD-Laplacian corresponding to
comparator matrix, so
can achieve O(k + l), which leads a tighter bound than the
case that the side information is vacuous as O(m + n).

D

D

b

b

Side information is widely applied in online learning problem with graph-based
information[9] and online similarity prediction[4]. In these cases the side information
matrices are given as the PD-Laplacian of the matrix corresponding the the graph. In
this paper we left an additional section in Appendix B for this discussion. Furthermore,
our reduction method can applied in Online community membership prediction as well
in section 6.

3 Preliminaries

×

1, 2, . . . , N

×
N semi-positive symmetric matrices and the set of N

and SN
For a positive integer N , let [N ] denote the set
×
++
denote the set of N
N strictly
positive symmetric positive deﬁnite matrices, respectively. We deﬁne E as identity ma-
SN
trix. For a matrix X, let Xi denote the i-th row of X. For X
++ , we denote
×
∈
N
as trace norm of X, further Tr(Γ XΓ ) =
Tr(X) =
λi(Γ XΓ )
i=1 |
|
SN
++ , where λi(X) is the i-th largest eigenvalue of X, and
as Γ -trace norm for
m

and SN
+

N
i=1 |

P

1/p

×

{

}

N

N

N

N

×

vec(X)

λi(X)
|
Γ
P
∀
∈
(i,j)(Xi,j )p
kp =
[m] M +
(cid:17)
(cid:16)P
M = maxi
∈
d row-normalized matrices as
×

k
as
the class of m
[m]

R

.

. We denote that the squared radius of M
∈
ii , where M + is the pseudo inverse of matrix M . We deﬁne
¯Pik2 = 1, i

m,d =

Rm

d :

¯P

N

⊂

∈

k

{

×

×

Sm
+

}

3.1 Generalised online semi-deﬁnite programming with bounded Γ -trace norm

Our generalised online semi-deﬁnite problem(
norm is deﬁned as follows: Given a matrix Γ
Tr(Γ W Γ )
vec(L)
k
generalised OSDP problem is as follows: on round t

) with respect to bounded Γ -trace
,
L
K
SN
SN
N
N
++ , we deﬁne
++ :
=
×
×
SN
N
, as decision set, and
τ,
:
=
×
+
, as loss space, more precisely speaking a sparse loss space. Thus our

≤
g
k1 ≤

Wi,i| ≤

W
{
L
{

K
L

∈
∈

[N ],

i
∀

∈

∈

β

}

}

|

[T ],

∈

– 1. Algorithm chooses a matrix Wt ∈ K
,
– 2. Adversary gives a loss matrix Lt ∈ L
,
Lt.
– 3. Algorithm incurs the loss as Wt •

4

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

Our goal is to upper bound following regret

RegretT =

T

t=1
X

Wt •

Lt −

min
W
∈K

T

t=1
X

W

Lt.

•

(2)

Note that if Γ = E, then Γ W Γ = W , the original OSDP is as a special case of

our setting.

[13] introduce an algorithm follow the regularized leader(FTRL) with log-determinant

regularizer in matrix form. In OSDP (
and choose a matrix Wt ∈ K

on each round t according to

K

L

,

), we give a speciﬁc regularizer R :

R

K →

Wt = arg min

W

∈K  

R(W ) + η

1

t
−

s=1
X

W

Ls •

.

!

The log-determinant regularizer is deﬁned as

R(X) =

−

ln det(X + ǫE),

(3)

(4)

where ǫ is positive.

In [13], the regret bound of original OSDP is restricted in the case that Γ = E and

the bound is O(g√τ βT ).

3.2 Online matrix completion(OMC) with side information

×

m

{−

n, on each round t, adversary sends (it, jt)

1, +1
[n] to algorithm. Then algorithm predicts ˆyt ∈ {−

Consider a binary matrix
[m]
×
yt ∈ {−
I
·
the total loss (i.e. the number of mistakes) M =
We deﬁne a hinge loss function hγ : R

∈
. Next adversary reveals
=ˆyt, where
is true and 0, otherwise. The goal of the algorithm is to minimize

to the algorithm, at last algorithm suffers the loss lt = Iyt6

1, 1
= 1 if the event

1, +1

Iyt6

=ˆyt.

}

}

}

·

T
t=1
R, as
P

→

hγ(x) =

0
1

(

x/γ

−

x,

if γ
≤
otherwise,

for γ > 0. Assume that a sequence
[n]
deﬁne the hinge loss of the sequence

× {−

1, 1

}

)T , and let P and Q be matrices such that P QT

= ((i1, j1), y1),

· · ·

, ((iT , jT ), yT )
×

Rm

([m]
×
n, then, we

⊆

with respect to (P , Q) and γ as

∈

S

S

T

, (P , Q), γ) =

hγ

hloss(

S

ytPit QT
jt
Pit k2k

Qjtk2 !

k

t=1
X
n is deﬁned by

Rm

×

The max norm of a matrix U

∈
kmax = min

P QT =U{

U

k

max
i
1
≤
≤

m k

Pik

max
j
1
≤
≤

n k

Qjk}

,

.

(5)

(6)

 
A generalised log-determinant regularizer for online semi-deﬁnite programming

5

∈

(7)

where the minimum is over all matrices P
Rm
deﬁne quasi-dimension of matrix U
Sm
Sn
++ and N
++ at margin γ is deﬁned as
×

∈

m

×

n

∈

Rm

d and Q

Rn
∈
n with respect to side information M
×

d for all d. So we

∈

×

×

γ
M ,N (U ) = min

¯P ¯QT =γU R

D

M Tr

¯P T M ¯P

+

N Tr

R

¯QT N ¯Q

,

∈ N

∈ N

m,d and ¯Q

(cid:0)
where ¯P
n,d. Note that only when
1/γ, the inﬁmum
exists. Here we deﬁne that M , N as side information, especially if M and N are
γ
M,N = m + n when side information is vacuous. In following
identity matrix, and
D
, when it leads to no ambiguity. Moreover
part we simpliﬁes this quasi-dimension as
γ
M ,N (U ) for a ﬁxed comparator matrix U to OMC
in following part we denote
problem with side information M , N at margin γ.

(cid:0)
kmax ≤

D ≥ D

U

D

k

(cid:1)

(cid:1)

b

Let G = (V, E, W ) be an m-vertex connected, weighted and undirected graph with
E
m diagonal matrix such that Dii is the degree
A as Laplacian. Furthermore positive deﬁnite
m is a matrix,

positive weights. Let A be the m
and Aij = 0, otherwise. Let D be a m
of each vertex i. We deﬁne that L = D
Laplacian(PD-Laplacian) is given as ¯L = L +
whose entries are all 1.

m matrix such that Aij = Aji = Wij if (i, j)

1
m2 I, where I

×
−

Rm

R

×

∈

∈

×

L

4 Algorithm for OSDP with bounded Γ -trace norm and regret

bound

We utilise FTRL (3) with generalised log-determinant regularizer (8), deﬁned in follow,
to our generalised OSDP problem with respect to bounded Γ -trace norm (
), where

,

K

L

K

L

=

W

SN
N
++ :
×

Wii| ≤

|

∈

β, Tr(Γ W Γ )

τ

≤

=

(cid:8)




L

∈

S+

N

N :

×

vec(L)

k1 =

k

Li,j| ≤

|

i,j
X

,

(cid:9)
g






for a ﬁxed Γ

SN
N
++ .

×

∈

We deﬁne generalised log-determinant regularizer as follows:

R(X) =

−

ln det(Γ XΓ + ǫE).

(8)

Next we give our regret bound for FTRL with generalised log-determinant regular-

izer in following theorem.

Theorem 1. Given Γ

×

N

SN
++ , and ρ = maxi,j |
SN
N
++ :
×

vec(W )

. Let

(Γ −

1Γ −

1)i,j |
β, Tr(Γ W Γ )

τ

∈
W

=

{

∈

K

k
for some β > 0 and τ > 0, then let
∈
L ⊆ {
g > 0. Then, for any competitor matrix W ∗
∈ K
generalised log-determinant regularizer achieves

k∞ ≤
L

≤
}
SN
Li,j| ≤
for some
+
, the FTRL algorithm with respect to

i,j |

}

g

N

×

:

P

RegretOSDP(T,

,

K

, W ∗) = O

L

g2(β + ρǫ)2T η +

(cid:18)

τ
ǫη

.

(cid:19)

6

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

In particular, letting η =

τ

g2(β+ρǫ)2ǫT we have

q

RegretOSDP ≤

O

g2(β + ρǫ)2τ T /ǫ

.

(9)

(cid:16)p

(cid:17)

Note that in original OSDP, Γ = E, so the result O(g√τ βT ) in [13] is a special

case of our problem, by setting ǫ = β, and ρ = 1.

Before we prove this theorem, we need to involve some Lemmata and notations.
The negative entropy function over the set of probability distribution P over RN is
P [ln(P (x))]. The total variation distance between probability
dx. The characteristic
P [eiuT x]

deﬁned as H(P ) = Ex
∼
distribution P and Q over RN is deﬁned as 1
2
function of a probability distribution P over RN is deﬁned as φ(u) = Ex
where i is the imaginary unit.

Q(x)
|

x |
R

P (x)

−

∼

Deﬁnition 1. For a decision space
≥
R is said to be s-strongly convex with respect to a loss space
X, Y

and a real number s

and L

K

:

0, a regularizer R :

if for any α

∈

L

K →
[0, 1] any

∈ K

∈ L

R(αX + (1

α)Y )

−

≤

−

αR(X) + (1
s
2

α)
|

α(1

−

L

α)R(Y )

(X

2.

Y )
|

−

−

•

(10)

Lemma 1. Let G1 and G2 are two zero mean Gaussian distribution with covariance
matrix Γ ΣΓ and Γ ΘΓ . Furthermore Σ and Θ are positive deﬁnite matrices. If there
exists (i, j) such that

Θi,j| ≥
then the total variation distance between G1 and G2 is at least

δ(Σi,i + Θi,i + Σj,j + Θj,j),

Σi,j −

|

1
12e1/4 δ.

(11)

Proof. Given φ1(u) and φ2(u) as characteristic function of G1 and G2 respectively.
Due to Lemma 7 in [13], we have

G1(x)

G2(x)
|

dx

−

≥

max
u
∈

RN |

φ1(u)

φ2(u)
|

,

−

Zx |

(12)

So we only need to show the lower bound of maxu
∈

.
Then we set that characteristic function of G1 and G2 are φ1(u) = e

−1
2 uT Γ T ΣΓ u
2 uT Γ T ΘΓ u respectively. Setting that α1 = (Γ v)T Σ(Γ v), α2 =
RV ,

. Moreover we denote that ¯v = Γ v, for any ¯v

φ2(u)
|

φ1(u)

−

RN

−1

and φ2(u) = e
(Γ v)T Θ(Γ v) and Γ u = Γ v
there exists v

√α1+α2
RV . ¯u = Γ u in the same way.

∈

|

∈

We need only give the lower bound of maxu
∈

RN

|

φ1(u)

φ2(u)
|

.

−

A generalised log-determinant regularizer for online semi-deﬁnite programming

7

Next we have that

RN |

max
u
∈
= max
RN
u
∈
= max
RV
u
∈
max
RN
¯v
∈

≥

≥

max
RN
¯v
∈

e

(cid:12)
(cid:12)
e
(cid:12)
(cid:12)
(cid:12)
e
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

φ1(u)

−

φ2(u)
|
−1
2 uT Γ ΣΓ u

−
−1
2 (Γ u)T Σ(Γ u)

−1
2 uT Γ ΘΓ u

e

(cid:12)
−1
2 (Γ u)T Θ(Γ u)
(cid:12)
(cid:12)

e

−
−α2
2(α1 +α2)

e

(cid:12)
(cid:12)
(cid:12)

.

−α1
2(α1 +α2)

1
2e1/4

−
α1 −
α2
α1 + α2 (cid:12)
(cid:12)
(cid:12)
(cid:12)

(13)

(cid:12)
(cid:12)
(cid:12)

Then second inequality is due to Lemma 10 in [13].

Due to assumption in the Lemma we obtain for some (i, j) that

δ(Σi,i + Θi,i + Σj,j + Θj,j)

Σi,j −

Θi,j|

≤ |

1
2 |
eT
i (Σ

=

−

Θ)ei −

(ei + ej)T (Σ − Θ)(ei + ej)
Θ)ej|
Θ)(ei + ej), eT
i (Σ
3 (Σi,i + Θi,i + Σj,j + Θj,j).

eT
j (Σ

−

−

−

−

Θ)ei and eT

j (Σ

It implies that one of (ei + ej)T (Σ
has absolute value greater that 2δ

Since Σ, Θ are strictly positive deﬁnite matrices, we have that for all v

ej, ei, ej}

vT (Σ + Θ)v

≤

2(Σ + Θ)i,i + (Σ + Θ)j,j.

and therefore we have that

max
RN
¯v
∈

≥

¯v

1
2e1/4

α1 −
α2
α1 + α2 (cid:12)
(cid:12)
1
(cid:12)
(cid:12)
2e1/4

max

(cid:12)
(cid:12)
(cid:12)
(cid:12)
ei+ej ,ei,ej } (cid:12)
∈{
(cid:12)
(cid:12)
(cid:12)

vT (Σ
Θ)v
−
vT (Σ + Θ)v

δ
6e1/4

≥

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Lemma 2. Let X, Y

|

SN
+

∈
Xi,j −

N

be such that

×

Yi,j| ≥

δ(Xi,i + Yi,i + Xj,j + Yj,j),

(14)

Θ)ej

−

ei +

∈ {

(15)

(16)

⊓⊔

(17)

and Γ is symmetric strictly positive deﬁnite matrix. Then the following inequality holds
that

α)Γ Y Γ )

−

ln det(αΓ XΓ + (1
α ln det(Γ XΓ )

−
(1

≤ −

α(1

−

α)

δ2
72e1/2 .

−
2

α) ln det(Γ Y Γ )

−

(18)

−

8

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

Proof. Let G1 and G2 are zero mean Gaussian distribution with covariance matrix
Γ ΣΓ = Γ XΓ and Γ ΘΓ = Γ Y Γ . In matrix total variation distance between G1
12e1/4 . Consider the entropy of the follow-
and G2 is at least
ing probability distribution of v with probability α that v
G2 otherwise.
G1 and v
Its covariance matrix is αΓ ΣΓ + (1
α)Γ ΘΓ . Due to Lemma 8 and 9 in [13] we
obtain that

12e1/4 . We denote that ˜δ = δ

−

∼

∼

δ

α)Γ ΘΓ )

−
α)G2) + ln(2πe)V

ln det(αΓ ΣΓ + (1

≤

2H(αG1 + (1

−
2αH(G1) + 2(1

≤
= α ln det(Γ ΣΓ )

α)H(G2) + ln(2πe)V
α) ln det(Γ ΘΓ )

(1

−

α(1

α(1

−

−

−

−

α)˜δ2
α)˜δ2.

−

−

Proposition 1. The generalised log-determinant regularizer R(X) =
ǫE) is s-strongly convex with respect to
Here E is identity matrix.

for

K

L

⊓⊔
ln det(Γ XΓ +
with s = 1/(1152√e(β + ρǫ)2g2).

−

Proof. Firstly we know that Γ XΓ + ǫE = Γ (X + Γ −

Applying the Lemma 11 to X + Γ −
∈ K
1)i,j| ≤
= β + ǫρ,
. According to Lemma 2 and Deﬁnition 1 we have

1 and Y + Γ −
Xi,j|

1ǫEΓ −
maxi,j |

+ ǫρ, we have that β

(X + Γ −
(Γ −

1 for X, Y

′

where maxi,j |
where ρ = maxi,j |
this proposition.

1ǫEΓ −
1)i,j|
1Γ −

1ǫEΓ −

1)Γ .
1ǫEΓ −

Proof (Proof of Theorem 1). Due to Lemma 6 we obtain that

RegretOSDP(T,

,

K

, W ∗)

L

H0
η

+

η
s

T.

≤

Due to above Proposition we know that s = 1/(1152(β + ρǫ)2√eg2).

Thus we need only to show H0 ≤

maximizer of R respectively, then we obtain that

τ
ǫ . Given W0 and W1 is the minimizer and

(R(W )

′

R(W

)) = R(W1)

R(W0)

−

−

∈K
ln det(Γ W1Γ + ǫE) + ln det(Γ W0Γ + ǫE)

ln

λi(Γ W0Γ ) + ǫ
λi(Γ W1Γ ) + ǫ

=

ln

λi(Γ W0Γ )
λi(Γ W1Γ ) + ǫ

+

ǫ
λi(Γ W1Γ ) + ǫ

(cid:19)

(cid:18)

(cid:18)

ln

λi(Γ W0Γ )
ǫ

+ 1

λi(Γ W0Γ )
ǫ

=

(cid:19)
Tr(Γ W0Γ )
ǫ

τ
ǫ

.

≤

max
W ,W ′

−
N

=

=

i=1
X
N

i=1
X
N

i=1
X
N

i=1
X

≤

≤

So we have our conclusion.

⊓⊔

(19)

(20)

⊓⊔

A generalised log-determinant regularizer for online semi-deﬁnite programming

9

5 Application to OMC with side information

In this section, we show that the OMC with side information M , N can be reduced to
our generalised OSDP with bounded Γ -trace norm. The reduction is twofold: Firstly
reduce to an online matrix prediction(OMP) problem with side information M , N and
then further reduce to our generalised OSDP problem. Meanwhile we utilise mistake-
driven technique such that we can bound the number of mistakes without regret bound
with respect to T.

5.1 Reduction from OMC to OMP with side information

First we describe an OMP problem with side information M and N , to which our
n
problem is reduced. The problem is speciﬁed by a competitor class
deﬁned as follows. For any matrix A

l, we deﬁne

1, 1]m

X ⊆

−

×

[

Rk

×

∈

¯A = diag

1
A1k2

,

,

· · ·

1
Akk2 (cid:19)

A.

k
k,l, is a matrix obtained from A by normalising all row vectors. Then,

k

(cid:18)

is deﬁned as

Rm

×

n

∈

M Tr

∧ R

¯P T M ¯P

+

N Tr

R

¯QT N ¯Q

,

≤

D}

That is, ¯A
our competitor class

∈ N

X
¯P ¯QT : P QT

=

{

X

where

, and

is named as quasi dimension estimator.

D ≥ D

D

The OMP problem with side information for

(cid:0)

(cid:1)

(cid:0)

(cid:1)

b

is described as the following proto-

X

col.

b

b
For each round t

[T ],

∈
Rm
1. the algorithm chooses a matrix Xt ∈
2. observes a triple (it, jt, yt)
[n]
×
3. suffers a loss given by hγ(ytXt,(it,jt)).

[m]

∈

n,

×

× {−

, and then

1, 1

}

The goal of the algorithm is to minimize the regret:

RegretOMP(T,

X

T

, X ∗) =

hγ(ytXt,(it,jt))

t=1
X

T

−

t=1
X

hγ(ytX ∗(it,jt)),

for any competitor matrix X ∗
∈ X
prediction, we do not require Xt ∈ X
problem.

Below we give the reduction. Assume that we have an algorithm

for the OMP

A

. Note that unlike the standard setting of online
.

Run the algorithm
In each round t,

A

and receive the ﬁrst prediction matrix X1 from

.
A

1. observe an index pair (it, jt)
2. predict ˆyt = sgn(Xt,(it,jt)),
3. observe a true label yt ∈ {−

1, 1

,

}

[m]

∈

×

[n],

10

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

4. if ˆyt = yt then Xt+1 = Xt, and if ˆyt 6

proceed and receive Xt+1.

= yt, then feed (it, jt, yt) to

to let it

A

Note that we run the algorithm
T
for M =
t=1
algorithm above.

Iˆyt6

runs
=yt rounds, where M is the number of mistakes of the reduction

in the mistake-driven manner, and hence

A

A

The next lemma shows the performance of the reduction.

P

Lemma 3. Let RegretOMP(M,
reduction above for a competitor matrix X ∗
Then,

X

, X ∗) denote the regret of the algorithm

, where M =

∈ X

T
t=1

P

A
1(ˆyt 6

M

≤

inf P QT

(RegretOMP(M,

∈X
supX∗

∈X

RegretOMP(M,

X

X

, ¯P ¯QT ) + hloss(
S
, X ∗) + hloss(

, γ),

S

, (P , Q), γ))

≤
where we deﬁne that

hloss(

S

, γ) = min

¯P ¯QT

∈X

hloss(

S

, (P , Q), γ).

in the
= yt).

(21)
(22)

(23)

Remark 1. If M and N are identity matrices, then we have that
¯P ¯QT : P QT

= m + n. In this case

¯QT N ¯Q

=

X

{

M Tr
Rm

¯P T M ¯P
n
, and
×
(cid:0)

}

(cid:1)
D

+
=

R
∈

N Tr
R
m + n.
(cid:0)

(cid:1)

Proof. Let P and Q be arbitrary matrices such that P QT
R and y
y)

hγ(yx) for any x

, we have

1, 1

∈

∈ {−

}

≤

Rm

×

b
n. Since 1(sgn(x)

=

∈

M =

T

t=1
X

1(ˆyt 6

= yt)

≤

= RegretOMP(M,

=yt}
t:ˆyt6
X{
, ¯P ¯QT ) +

X

hγ(ytXt,(it,jt))

hγ(yt( ¯P ¯QT )it,jt)

=yt}

t:ˆyt6
X{
T

RegretOMP(M,

≤

= RegretOMP(M,

X

X

, ¯P ¯QT ) +

hγ(yt( ¯P ¯QT )it,jt )

t=1
X

, ¯P ¯QT ) + hloss(
S

, (P , Q), γ),

where the second equality follows from the deﬁnition of regret, and the third equality
follows from the fact that ( ¯P ¯QT )i,j = PiQT
Qjk2). Since P and Q are
chosen arbitrarily, we get (21).
Now, let P and Q be the matrices that attain (5). Then, the inequality above implies

Pik2k

j /(
k

that

M

≤

≤

which proves (22).

RegretOMP(M,
sup
X∗

RegretOMP(M,

X

X

, ¯P ¯QT ) + hloss(
S

, γ)
, X ∗) + hloss(

S

∈X

, γ),

⊓⊔

6
A generalised log-determinant regularizer for online semi-deﬁnite programming

11

5.2 Reduction from OMP with side information to generalised OSDP with

bounded Γ -trace norm

A similar technique is used in [7] and [6]. For side information matrix M , N we deﬁne
a matrix Γ for our generalised OSDP as follows:

Γ =

(cid:20)

√

R

M M 0
0

√

N N

R

.

(cid:21)

(24)

Next we deﬁne the decision class

. Let N = m + n, and for any matrices P and

Q such that P QT

∈

Rm

×

K
n, we deﬁne

WP ,Q =

¯P
¯Q
(cid:21)

(cid:20)

¯P T ¯QT

=

(cid:2)

(cid:3)

¯P ¯P T ¯P ¯QT
¯Q ¯P T ¯Q ¯QT

(cid:20)

.

(cid:21)

Note that WP ,Q is an N
right m
intuitively, WP ,Q can be viewed as a positive semi-deﬁnite embedding of ¯P ¯QT
Then, the decision class is any convex set

N symmetric and positive semi-deﬁnite matrix with its upper
n component matrix ¯P ¯QT is a competitor matrix for the OMP problem. So,
.

SN
++ that satisﬁes

∈ X

×

×

N

×

In this paper, we choose

K ⊇ {

K ∈
WP ,Q : P QT

Rm

n

×

.

}

∈

i ∈
be a matrix such that the (i, m + j)-th and (m + j, i)-th components are 1 and

×

L

∈

i
∀

[n], Wi,i ≤

Tr(Γ W Γ )

1

∧

. For any (i, j)

[m]

(25)

.

≤

D}
[n], let Z

b

i, j
h

=

W

SN
N
++ :
×

{

K

∈
Then, we deﬁne the loss matrix class
N

SN
+
the other components are 0. More formally,

∈

×

Z

i, j
h

i

=

1
2

eieT

m+j + em+jeT
i

,

where ek is the k-th basis vector of RN . Note that when we focus on its upper right
m

n component matrix, then only the (i, j)-th component is 1. Then,

is

(cid:0)

(cid:1)

×

L

=

cZ
{

i, j
h

i

L

: c

∈ {−

1/γ, 1/γ

, i

}

∈

[m], j

[n]

.

}

∈

Now we are ready to describe the reduction from the OMP problem for

OSDP problem (

,

). Let

L
Run the algorithm
In each round t,

K

A

A

be an algorithm for the OSDP problem.
and receive the ﬁrst prediction matrix W1 ∈ K

from

(26)

to the

X

.
A

1. let Xt be the upper right m

n component matrix of Wt.

×

Z

// Xt,(i,j) = Wt •
i, j
i
h
2. observe a triple (it, jt, yt)
[m]
3. suffer loss ℓt(Wt) where ℓt : W
yt
γ Z

∈

4. let Lt =

W ℓt(Wt) =

∇

−
0

(

[n]
× {−
hγ(yt(W

×
7→
it, jti
h

1, 1

,
}
Z
)),
it, jti
•
h
if ytXt,(i,j) ≤
γ
otherwise

,

12

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

5. feed Lt to the algorithm

A

to let it proceed and receive Wt+1.

−

ℓt(Wt)

ℓt(W ∗)

Since the loss function ℓt is convex, a standard linearlization argument ([14]) gives
Wt •

Lt −
. Moreover, since ℓt(Wt) = hγ(ytXt,(it,jt)) and ℓt(WP ,Q) =

for any W ∗
hγ(yt( ¯P ¯QT )it,jt ), the following lemma immediately follows.
Lt denote the
t=1(Wt −
•
in the reduction above for a competitor matrix WP ,Q and
hγ(yt( ¯P ¯QT )it,jt ) denote the

Lemma 4. Let RegretOSDP(T,
regret of the algorithm
A
, ¯P ¯QT ) =
RegretOMP(T,
regret of the reduction algorithm for ¯P ¯QT . Then,

P
T
t=1(hγ(ytXt,(it,jt))

, WP ,Q) =

WP ,Q)

∈ K

W ∗

Lt

−

≤

K

X

L

•

T

,

RegretOMP(T,

P
, ¯P ¯QT )

X

RegretOSDP(T,

,

K

, WP ,Q).

L

≤

Combining Lemma 3 and Lemma 4, we have the following corollary.

Corollary 1. Assume that we have an algorithm for the OSDP problem (
regret bound RegretOSDP(T,
rithm for the binary matrix completion problem with the following mistake bounds.

) with
. Then, there exists an algo-

, W ∗) for any W ∗

∈ K

K

K

L

L

,

,

(RegretOSDP(M,

,

K

, WP ,Q)

L

M

inf
P QT

≤
+ hloss(

∈X

sup

≤

W ∗

∈K

, (P , Q), γ))
S
RegretOSDP(M,

,

K

L

, W ∗) + hloss(

, γ).

S

5.3 Application to matrix completion

) with bounded Γ -trace norm deﬁned in (25)
For the generalised OSDP problem (
and (26), where Γ is respect to side information matrices M and N , we apply FTRL
algorithm with the generalised log-determinant regularizer. Speciﬁcally, the FTRL al-
gorithm makes predictions according to the following formula:

K

L

,

Wt+1 = arg min
∈K

W

−

ln det(Γ W Γ + ǫE) + η

where ǫ > 0 and η > 0 are parameters.

t

s=1
X

W

Ls,

•

(27)

Moreover the following lemma shows us the quasi-dimension with respect to side
SN
++ . Again, M and N are identity matrices, if the
information matrices M , N
side information is vacuous. In this case our generalised log-determinant regularizer
becomes the regular form as

ln det(W + ǫE).

∈

N

×

Lemma 5 (Lemma 8 [8]). Given side information matrices M, N
Γ as

N

SN
++ , we deﬁne

×

∈

−

Then we obtain that

Γ =

(cid:20)

√

R

M √M
0

0
N √N

√

R

.

(cid:21)

(28)

(29)

Tr(Γ WP,QΓ ) =

M Tr

R

¯P T M ¯P

+

N Tr

R

¯QT N ¯Q

.

(cid:0)

(cid:1)

(cid:0)

(cid:1)

A generalised log-determinant regularizer for online semi-deﬁnite programming

13

Remark 2. Since the deﬁnition of Γ in Equation (28), we have that ρ = 1.

Thus we set β = 1, g = 1/γ, ǫ = ρ = 1,τ =
next utilise Theorem 1, so we get the following result

D

, and Γ is given as in Equation (28)

b

RegretOSDP(T,

,

K

, W ∗) = O

L

T η
γ2 +

.

D
η !
b

(30)

Before stating our main result, we give in Algorithm 1 the algorithm for the OMC
problem with side information M , N which is obtained by putting together the two
reductions with the FTRL algorithm (27).

Algorithm 1 Online matrix completion with side information algorithm
1: Parameters: γ > 0, η > 0, side information matrices M ∈ Sm×m

++ . Quasi
D Γ is composed as in Equation (28), and decision set K is given
b

++ and N ∈ Sn×n

dimension estimator 1 ≤
as (25).

2: Initialize ∀W ∈ K, setting W1 = W .
3: for t = 1, 2, . . . , T do
4:
5:
6:
7:
8:

Receive (it, jt) ∈ [m] × [n].
Let Zt = 1
2 (eit
Predict ˆyt = sgn(Wt • Zt) and receive yt ∈ {−1, 1}.
if ˆyt 6= yt then
Let Lt = −yt
γ
Ls.

eT
m+jt + em+jt

eT
it ).

Zt and Wt+1 = arg minW ∈K − ln det(Γ W Γ + E) + η Pt

W •

s=1

Let Lt = 0 and Wt+1 = Wt.

else

9:
10:
end if
11:
12: end for

Usually we set η =

γ2

/T to minimize (30), we obtain O

T /γ2

regret

D

q

D

(cid:18)q

(cid:19)

bound. But in our case, the horizon T is set to be the number of mistakes M through
the reduction, which is unknown in advance. Nevertheless, the next theorem shows that
we can choose η independent of M to derive a good mistake bound.

b

b

Theorem 2. Algorithm 1 with η = cγ2 for some c > 0 achieves

M =

T

t=1
X

Iˆyt6

=yt = O

+ 2hloss(

S

, γ).

D
γ2
b

!

(31)

Proof. Combining Corollary 1 and the regret bound (30), we have

M = O

M η
γ2 +

D
η !
b

+ hloss(

S

, γ).

 
 
 
14

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

Choosing η = cγ2 for sufﬁciently small constant c, we get

M

≤

M
2

+ O

+ hloss(

S

, γ),

D
γ2
b

!

from which (31) follows.

⊓⊔
Again if the side information is vacuous, which means that M , N are identity ma-
= m + n and obtain the

trices, from Remark 1 and Theorem 2, we can set that
mistake bound as follows:

D

m + n
γ2 + 2hlossP QT

∈

Rm×n (

S

O

(cid:18)

b
, (P , Q), γ)

.

(cid:19)

In contrast, there is a case where side information matters non-trivially. Especially,
l)-biclustered structure(the details are in Appendix) then we obtain

O(k + l), which is strictly smaller than O(m + n).

D ∈
Note that in realizable case, our mistake bound becomes O
b

b
Dγ2

, which improves

b
Dγ2 ln(m + n)
(cid:17)

(cid:16)
in [8], removing the logarithmic factor ln(m +
the previous bound O
n). Furthermore, this bound matches the previously known lower bound of Herbster
l), γ can be set as γ = 1
et.al. [7]. When U contains (k
√l
and our regret bound becomes O(lm). On the other hand, the lower bound of Herbster
et.al. is Ω(lm). Thus, the mistake bound of Theorem 2 is optimal.

l)-biclustered structure (k

×

≥

(cid:17)

(cid:16)

if U contains (k
that

×

6 Application to online similarity prediction with side information

In this section, we show that our reduction method and generalised log-determinant
regularizer work in online similarity prediction with side information.

Let G = (V, E) be an undirected graph with n =
, yn}

edges.
E
. On each
Assign vertices to K classes such that
round t, for a given pair of vertices (it, jt) algorithm needs to predict whether they
are in the same class denoted as ˆyit,jt . If they are in the same class then yit,jt = 1,
1, otherwise. Our target is to give a bound of the prediction mistakes M =
yit,jt =
T
=yit,jt .
t=1

vertices and m =
, K
1,

V
|
where yi ∈ {

y1,

· · ·

· · ·

|
}

{

|

|

−
Iˆyit,jt 6

we abbreviate it to ΦG and the cut-size is given as

P
Deﬁnition 2. The set of cut-edges in (G, y) is denoted as ΦG(y) =
yj}
respect to class label k is denoted as ΦG
Note that
s (y)
|
(i, j)
degree of vertex i. We deﬁne the Laplacian as L = D

=
E : yi 6
∈
{
. The set of cut-edges with
.
= yj}
, yi 6
yi, yj}
∈
n such that Aij = Aji = 1 if
×
E(G) and Aij = 0 otherwise. D is denoted as diagonal matrix with Dii is the

s (y) =
. Given A

ΦG(y)
|
E : s

ΦG(y)
|

|
(i, j)
Rn

k
s=1 |

{
∈

(i, j)

= 2

∈ {

ΦG

A.

P

∈

|

−

Deﬁnition 3. If G is identiﬁed with a resistive network such that each edge is a unit
resistor, then the effective resistance RG
V 2 can be deﬁned as
ej), where ei is the i-th vector in the canonical basis of Rn.
ej)L+(ei −
RG
i,j = (ei −

i,j between pair (i, j)

∈

 
A generalised log-determinant regularizer for online semi-deﬁnite programming

15

[4] gave a mistake bound as M

O

≤

ΦG

|

|

max(i,j)∈V 2 RG

i,j

γ2

ln n

.

(cid:18)

(cid:19)

If we utilise our reduction method, same as in previous main part here we denote that
n,k and P QT = γU , where U is the potential online matrix for similarity

¯P , ¯Q
prediction. Therefore we give the decision set as

∈ B

WP ,Q =

¯P T ¯QT

=

.

(32)

¯P
¯Q
(cid:21)

(cid:20)

¯P ¯P T ¯P ¯QT
¯Q ¯P T ¯Q ¯QT

(cid:20)

(cid:21)

(cid:3)
Side information is given as PD-Laplacian ¯L from Laplacian L of graph G, thus we

(cid:2)

have

Γ =

¯L
R
0

"p

¯L

0

¯L#

¯L

R

Meanwhile given sparse matrix Zt in following equation

p

Zt =

1
2

.

(eieT

n+j + en+jeT

i ),

Hence we can give the reduced generalised OSDP problem (

(33)

(34)

,

) with bounded

L

K

and c

1/γ, 1/γ

∈ {−

}

Γ -trace norm as follows:

=

=

X

n
cZ
{

K

L

Sn
n
++ :
×

Xii| ≤

|

1, Tr(Γ XΓ )

:: c

i

∈ {−

1/γ, 1/γ

, i

}

∈

∈
i, j
h

≤
[n], j

D

b
∈

o
[n]

,

}

where Γ is deﬁned as above.

According to [8] if U obtains the (k, k)-biclustered structure, and U = RU ∗RT ,
O(k).

2Tr(RT LR)

¯L(RT U ∗R))

L + 2k

Tr(

≤

R

≤

we have that Tr(Γ XΓ )
Moreover we have that

≤

R

Tr(RT LR)

L

R

γ2

+ min
¯P ¯QT

T

hγ(yit,jt(P QT )it,jt )

M

O

≤

O

≤

(cid:18)

(cid:18)

k
γ2

+ min
¯P ¯QT

∈X

(cid:19)

(cid:19)
T

∈X

t=1
X
hγ(yit,jt(P QT )it,jt ),

t=1
X
n :

R

¯LTr( ¯P T ¯L ¯P ) +

¯LTr( ¯QT ¯L ¯Q)

R

for

≤

D}

where
some

=

¯P ¯QT : P QT
{
X
γ
¯L, ¯L(U ).
D ≥ D

Rn

×

∈

b

2 +
Remark 3. According to [8], we have that Tr(
2 counts only when there is a edge between different
2k, where
Rjk
Ri −
(i,j)
ΦG
2 =
ΦG
. On
, we have that
classes. Due to the deﬁnition of
|
L = maxii L+ so we obtain that
the other hand,
[k]. It implies
that 2

P
Rjk
Ri −
E k
∈
i L+ei,
eT
i
∈
∀

Ri −

¯L(RT U ∗R))

L
P
R

i,j k

R
max(i,j)

E k

V 2 RG
i,j.

P

(i,j)

R

≥

≤

2

L

∈

|

|

|

R

≥

∈

(35)

b
Rjk

16

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

7 Conclusion

In this paper, on the one hand we deﬁne a generalised OSDP problem with bounded Γ -
trace norm. To solve this problem, we involve FTRL with generalised log-determinant
regularizer and achieve regret bound as O((1 + ρ)g√βτ T ). On the other hand, we
utilise our result to OMC with side information particularly. We reduce OMC with side
information to our new OSDP with bounded Γ -trace norm, and obtain a tighter mistake
bound than previous work by removing logarithmic factor.

References

1. Cesa-Bianchi, N., Lugosi, G.: Prediction, learning, and games. Cambridge university press

(2006)

2. Cesa-Bianchi, N., Shamir, O.: Efﬁcient online learning via randomized rounding. In: Ad-

vances in Neural Information Processing Systems. pp. 343–351 (2011)

3. Christiano, P.: Online local learning via semideﬁnite programming. In: Proceedings of the
forty-sixth annual ACM symposium on Theory of computing. pp. 468–474. ACM (2014)
4. Gentile, C., Herbster, M., Pasteris, S.: Online similarity prediction of networked data from

known and unknown graphs. In: Conference on Learning Theory. pp. 662–695 (2013)

5. Hazan, E.: 10 the convex optimization approach to regret minimization. Optimization for

machine learning p. 287 (2012)

6. Hazan, E., Kale, S., Shalev-Shwartz, S.: Near-optimal algorithms for online matrix predic-

tion. In: Conference on Learning Theory. pp. 38–1 (2012)

7. Herbster, M., Pasteris, S., Pontil, M.: Mistake bounds for binary matrix completion. In: Ad-

vances in Neural Information Processing Systems. pp. 3954–3962 (2016)

8. Herbster, M., Pasteris, S., Tse, L.: Online matrix completion with side information. Advances

in Neural Information Processing Systems 33 (2020)

9. Herbster, M., Pontil, M., Wainer, L.: Online learning over graphs. In: Proceedings of the

22nd international conference on Machine learning. pp. 305–312 (2005)

10. Koltchinskii, V., Lounici, K., Tsybakov, A.B., et al.: Nuclear-norm penalization and opti-
mal rates for noisy low-rank matrix completion. The Annals of Statistics 39(5), 2302–2329
(2011)

11. Laraki, R., Lasserre, J.B.: Semideﬁnite programming for min–max problems and games.

Mathematical programming 131(1-2), 305–332 (2012)

12. Lasserre, J.B.: A max-cut formulation of 0/1 programs. Operations Research Letters 44(2),

158–164 (2016)

13. Moridomi, K.i., Hatano, K., Takimoto, E.: Online linear optimization with the log-
determinant regularizer. IEICE Transactions on Information and Systems 101(6), 1511–1520
(2018)

14. Shalev-Shwartz, S.: Online learning and online convex optimization. Foundations and

Trends® in Machine Learning 4(2), 107–194 (2012)

15. Shamir, O., Shalev-Shwartz, S.: Collaborative ﬁltering with the trace norm: Learning, bound-
ing, and transducing. In: Proceedings of the 24th Annual Conference on Learning Theory.
pp. 661–678 (2011)

T

t=1 Lt •

P

A generalised log-determinant regularizer for online semi-deﬁnite programming

17

8 Appendix

Lemma 6. [5] Let R :
FTRL with the regularizer R applied to (

K →

R be s-strongly convex with respect to

,

) achieves

L

K

for

L

K

. Then the

RegretOSDP(T,

,

K

, W ∗)

L

H0
η

+

η
s

T,

≤

(36)

where H0 = maxW ,W ′

(R(W )

∈K

−

′

R(W

)), W ∗ is the static optimal solution of

W . In particular if we choose η =

sH0/T then we have

RegretOSDP(T,

p
, W ∗)

L

,

K

H0T
s

.

2

≤

r

(37)

Lemma 7 (Lemma A.1 [13]). Let P and Q be probability distribution over RV and
φP (u) and φQ(u) be their characteristic functions, respectively. Then

max
u
∈

RN |

φP (u)

−

φQ(u)

| ≤

Zx |

P (x)

Q(x)
|

−

dx,

(38)

the right hand side is the total variation distance between any distribution Q and P.

Lemma 8 (Lemma A.2 [3]). Let P and Q be probability distributions over RN with
total variation distance δ. Then

H(αP + (1

α)Q)

−

≤

αH(P ) + (1

α)H(Q)

−

α(1

−

−

α)δ2,

(39)

where H(P ) = Ex

P [ln P (x)].

∼

Lemma 9 (Lemma A.3 [13]). For any probability distribution P over RV with zero
mean and covariance matrix Σ its entropy is bounded by the log-determinant of co-
variance matrix. That is

H(P )

−

1
2

≤

ln(det(Σ)(2πe)V ).

Lemma 10 (Lemma A.4 [13]).

−x
2

e

−

1−x
2

e−

1/4

e−
2

≥

2x),

(1

−

for 0

≤

x1/2.

(40)

(41)

Lemma 11 (Lemma 5.4 [13]). Let X, Y
Yi,i| ≤
Xi,i| ≤
and
|
there exists that

Then for any L

β

β

|

′

′

SN

×
++
=

∈
∈ L

N

{

be such that for all i
N
vec(L)
L
:

×

SN
+

∈

k

[N ]
g

}

∈
k1 ≤

Xi,j −

|

Yi,j | ≥

L

|

•

(X
−
4β′ g

Y )
|

(Xi,i + Yi,i + Xj,j + Yj,j).

(42)

18

Yaxiong Liu, Ken-ichiro Moridomi, Kohei Hatano, Eiji Takimoto

9 Appendix B. Biclustered structure and ideal case of

quasi-dimension

As in [8] we deﬁne (k, l)-biclustered structure as follows: For m

k and n

l,

≥

≥

Deﬁnition 4. the class of (k, l)-binary biclustered matrices is deﬁned as

Bm

n
k,l =
×

U

{

∈ {−

Ui,j = Vri,cj , i

1, +1

m

n : r

×

[k]m, c

}
[m], j

∈
[n]

.

}

∈

∈

[l]n, V

1,

1

}

−

∈ {

k

l,

×

∈

Denote

matrix U

∈

m,k and C
R
γ
M ,N ≤
D
Laplacian and

×

m

{

R

0, 1

⊂ {

m,d =
d :
B
k,l we can decompose U = RU ∗C T for some U ∗
Bm,n
n,l. In [8] if the comparator matrix U
M + 2Tr(C T N C)

[m], rank(R) = d
}
1, +1
∈
}
∈ {−
Bm
n
, we know that
×
k,l
N + 2k + 2l, if M, N are PD-

Rik2 = 1, i

, for any
l, R
k

2Tr(RT M R)

∈ B

∈

∈

}

k

×

γ
M ,N ≤

D

R

O(k + l).

R

In graph-based semi-supervised learning [8], for a given row corresponding to a
vertex in the row graph. The weight of edge (i, j) represents our prior belief that row
i and row j share the same underlying factor. Hence we may build a graph based on
vectorial data associated with the rows, for example, user demographics. Assume that
we know the partition of [m] vertices that maps rows to k factors. The rows that share
factors have an edge between them and there are no other edges. Therefore we have a
graph with k disjoint cliques. We assume that this graph has a structure that any pair
of vertices in this graph can be connected with a path within 4 length. Choose the side
information matrix M as PD-Laplacian ¯L of this row graph. For columns in a partition
of [n] with l cliques, we do the same work. At last we obtain that the
O(k+l).

γ
M ,N ∈

D

