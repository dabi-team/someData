9
1
0
2

y
a
M
3
2

]

G
L
.
s
c
[

2
v
6
8
8
6
0
.
5
0
9
1
:
v
i
X
r
a

AlgoNet: C ∞ Smooth Algorithmic Neural Networks

Felix Petersen
University of Konstanz
felix.petersen@uni.kn

Christian Borgelt
University of Salzburg
christian@borgelt.net

Oliver Deussen
University of Konstanz
oliver.deussen@uni.kn

Abstract

Artiﬁcial neural networks revolutionized many areas of computer science in recent
years since they provide solutions to a number of previously unsolved problems. On
the other hand, for many problems, classic algorithms exist, which typically exceed
the accuracy and stability of neural networks. To combine these two concepts, we
present a new kind of neural networks—algorithmic neural networks (AlgoNets).
These networks integrate smooth versions of classic algorithms into the topology
of neural networks. A forward AlgoNet includes algorithmic layers into existing
architectures while a backward AlgoNet can solve inverse problems without or with
only weak supervision. In addition, we present the algonet package, a PyTorch
based library that includes, inter alia, a smoothly evaluated programming language,
a smooth 3D mesh renderer, and smooth sorting algorithms.

1

Introduction

Artiﬁcial Neural Networks are employed to solve numerous problems, not only in computer science
but also in all other natural sciences. Yet, the reasoning for the topologies of neural networks seldom
reaches beyond empirically based decisions.

In this work, we present a novel approach to designing neural networks—algorithmic neural networks
(short: AlgoNet). Such networks integrate algorithms as algorithmic layers into the topology of
neural networks. However, propagating gradients through such algorithms is problematic, because
crisp decisions (conditions, maximum, etc.) introduce discontinuities into the loss function. If one
passes from one side of a crisp decision to the other, the loss function may change in a non-smooth
fashion—it may “jump”. That is, the loss function suddenly improves (or worsens, depending on
the direction) without these changes being locally noticeable anywhere but exactly at these “jumps”.
Hence, a gradient descent based training, regardless of the concrete optimizer, cannot approach
these “jumps” in a systematic fashion, since neither the loss function nor the gradient provides any
information about these “jumps” in any place other than exactly the location at which they occur.
Therefore, a smoothing is necessary, such that information about the direction of improvement
becomes exploitable by gradient descent also in the area surrounding the “jump”. I.e., by smoothing,
e.g., an if, one can smoothly, by gradient descent, undergo a transition between the two crisp cases
using only local gradient information.
Generally, for end-to-end trainable neural network systems, all components should at least be C 0
smooth, i.e., continuous, to avoid “jumps”. However, having C k smooth, i.e., k times differentiable
and then still continuous components with k ≥ 1 is favorable. This property of higher smoothness
allows for higher order derivatives and thus prevents unexpected behavior of the gradients. Hence, we
designed smooth approximations to basic algorithms where the functions representing the algorithms
are ideally C∞ smooth. For that, we designed pre-programmed neural networks (restricted to smooth
components) with the structure of given algorithms.

Algorithmic layers can solve sub-problems of the given problem, act as a custom algorithmic loss, or
assist in ﬁnding an appropriate solution for (ill-posed) inverse problems. Such algorithmic losses can

Preprint. Under review.

 
 
 
 
 
 
impose constraints on predicted solutions through optimization with respect to the algorithmic loss.
Ill-posed problems are a natural application for algorithmic losses and algorithmic layers.

In this work, we describe the basic concept of algorithmic layers and present some applications. In
Sec. 3.1.1, we start by showing that any algorithm, which can be emulated by a Turing machine, can
be approximated by a C∞ smooth function. In Sec. 4, we present some algorithmic layers that we
designed to solve underlying problems. All described algorithmic layers are provided in the algonet
package, a PyTorch [1] based library for AlgoNets.

2 Related Work

Related work [2]–[4] in neural networks focused on dealing with crisp decisions by passing through
gradients for the alternatives of the decisions. There is no smooth transition between the alternatives,
which introduces discontinuities in the loss function that hinder learning, which of the alternatives
should be chosen. TensorFlow contains a sorting layer (tf.sort) as well as a while loop construct
(tf.while_loop). Since the sorting layer only performs a crisp relocation of the gradients and the
while loop has a crisp exit condition, there is no gradient with respect to the conditions in these layers.
Concurrently, we present a smooth sorting layer in Sec. 4.1 and a smooth while loop in Sec. 3.1.1.

Theoretical work by DeMillo et al. [5] proved that any program could be modeled by a smooth
function. Consecutive works [6]–[8] provided approaches for smoothing programs using, inter alia,
Gaussian smoothing [7], [8].

3 AlgoNet

To introduce algorithmic layers, we show that smooth approximations for any Turing computable
algorithm exist and explain two ﬂavors of AlgoNets: forward and backward AlgoNets.

3.1 Smooth algorithms

To design a smooth algorithm, all discrete cases (e.g., conditions of if statements or loops) have to
be replaced by continuous or smooth functions. The essential property is that the implementation is
differentiable with respect to all internal choices and does not—as in previous work—only carry the
gradients through the algorithm. For example, an if statement can be replaced by a sigmoid-weighted
sum of both cases. By using a smooth sigmoid function, the statement is smoothly interpreted. Hence,
the gradient descent method can inﬂuence the condition to hold if the content of the then case
reduces the loss and inﬂuence the condition to fail if the loss is lower when the else case is executed.
Thus, the partial derivative with respect to a neuron is computed because the neuron is used in the if
statement. In contrast, when propagating back the gradient of the then or the else case depending
on the value of the condition, there is a discontinuity at the points where the value of the condition
changes and the partial derivative of the neuron in the condition equals zero.
(cid:26)0
1

1
1 + e−x·s

if x < 0
else

s1(x, s) =

with s = 1

s2(x) =

(1)

(2)

Here, the logistic sigmoid function (Eq. 1) is a C∞ smooth replacement for the Heaviside sigmoid
function (Eq. 2), which is equivalent to the if statement. Alternatively, one could use other sigmoid
functions, e.g., the C 1 smooth step function x2 − 2 · x3 for x ∈ [0, 1], and 0 and 1 for all values
before and after the given range, respectively.

(cid:16)

Another example is the max-operator, which, in neural networks, is commonly replaced by the
.
SoftMax operator
After designing an algorithmic layer, we can use it to build a neural network as shall be described in
Sec. 3.2 and 3.3.

SoftMax(x)i = exp(xi)

j exp(xj )

(cid:17)

(cid:80)

2

3.1.1 Smooth WHILE-Programs

In this section, we present smooth and differentiable approximations to an elementary programming
language based on the WHILE-language by Uwe Schöning [9]. The WHILE-language is
Turing-complete, and for variables (var) its grammar can be deﬁned as follows:

prog = WHILE var != 0 DO prog END

| prog prog
| var := var
| var := var + 1
| var := var - 1

left and right var unequal

//
// left and right var equal
// left and right var equal

Here, var ∈ {xn | n ∈ N0} and while x0 is the output, xn where n ∈ N0 are variables initialized
to 0 if not set to an input value. Thus, {xn | n ∈ N+} are inputs and/or local variables used in
the computations. Although this interpretation allows only for a single scalar output value (x0),
an arbitrary number of output values can be reached, either by interpreting additional variables as
output or by using multiple WHILE-programs (one for each output value). Provided Church’s thesis
holds, this language covers all effectively calculable functions. The WHILE-language is equivalent
to register machines which have no WHILE, but instead IF and GOTO statements.

k

k

1

0

2

p

x

-1

-2

(x) = p(old)

Figure 1: φ0 (magenta) and φ∞ (cyan).

We generate the approximation to this language by
executing all statements only to the extent of their
I.e., we keep track of a probability p,
probability.
indicating whether the current statement is still executed.
Initially, p = 1.
In the body of a while loop, the
probability for an execution is p(new)
· φk(x)
where k deﬁnes the smoothness of the probability
function for exiting the loop φk. To obtain C∞ smooth
WHILE-programs, we used φ∞ : R → [0, 1] : x (cid:55)→ (esx−1)2
e2sx+1 = 1 − sech(sx). For C 0 smoothness,
we used the shouldered fuzzy set φ0 : R → [0, 1] : x (cid:55)→ 1 − max(0, 1 − |x|) = min(1, |x|). For
x0...xn initialized as integers, using φ0, the result for the C 0 WHILE-program always equals the
result for the discrete WHILE-program since the probability is always either 1 or 0. For all k ∈ N0,
φk exists. Fig. 1 shows how these exit probability functions behave. Because of the symmetry
of φ, w.l.o.g., we can assume that ∀x ∈ R≥0. If the loop (in the discrete version) increases x
by 1, x will diverge. If x decreases by 1, p converges to 0. Since φ0(x) ≥ φ∞(x), it sufﬁces
to show that p converges to zero for φ0. Since p(new)(x) = p(old)(x) · φ0(x) ≤ φ0(x) ≤ |x|,
x := x − p(new)(x) ≥ x − |x| = 0. Thus, x and φ0(x) monotonically decrease and x ≥ 0. Eventually,
φ0(x) < 1. Thus, p(x) ≤ (φ0(x))n n→∞−−−−→ 0.
Here, x is the value of the current variable, s is the steepness, and p is the probability of the execution.
To apply the probabilities on the assignment, increment and decrement operators, we redeﬁne them
as:

x0 := x1
x0 := x0 + 1
x0 := x0 − 1

−→ x0 := p · x1 + (1 − p) · x0
−→ x0 := x0 + p
−→ x0 := x0 − p

(3)
(4)
(5)

Since the hyperbolic secant is C∞ smooth, our version of the WHILE-language is C∞ smooth.
While the probability converges to zero, it (in most cases) never reaches zero, and the loop would
never exit. Thus, we introduce (cid:15) > 0 and exit the loop if p ≤ (cid:15) or a maximum number of iterations is
reached. Although this introduces discontinuities, by choosing an (cid:15) of numerically negligible size,
the discontinuities also become numerically negligible.
As an experiment, we implemented the multiplication on positive integers, as shown on the left:
| WITH p1 := 1; p(cid:48)
WHILE x2 != 0 DO
|
x3 := x1
|
WHILE x3 != 0 DO
|
x0 := x0 + 1
|
x3 := x3 - 1
|
|
| WHILE p1 ≥ (cid:15)

1 := p1 · φ(x2) DO
x3 := p1 · x1 + (1 − p1) · x3
WITH p2 := p1 ; p(cid:48)

x0 := x0 + p2
x3 := x3 - p2

WHILE p2 ≥ (cid:15)
x2 := x2 - p1

END
x2 := x2 - 1

2 := p2 · φ(x3) DO

END

3

(a) x1 · x2

(b) C∞ WHILE(x1, x2)

(c) C 0 WHILE(x1, x2)

Figure 2: Multiplication of x1 and x2 implemented as a general multiplication, C∞ smooth
WHILE-program and C 0 smooth WHILE-program. The height indicates the result of the function.
Contrary to the common notion, the color indicates not the values but the analytic gradient of the
function.

Contrary to the discrete implementation, the smooth interpretation (as on the right) can interpolate
the result for arbitrary values x1, x2 ∈ R+.
Since the WHILE-language is Turing-complete, any high-level program could, in principle, using
an appropriate compiler, be translated into an equivalent program in WHILE-language. To this
WHILE-program, automatic smoothing could be applied using the rules that are illustrated here. Of
course, there are better ways of smoothing: manual smoothing using domain-speciﬁc knowledge
and smoothing using a higher-level language outperform the low-level automatic smoothing. For
example, in a higher-level language, the multiplication would be implemented since it is smooth itself.
Using domain-speciﬁc knowledge, algorithms could be reformulated in such a way that smoothing is
possible in a more canonical way. To translate a WHILE-program into a neural network, the WHILE
loops are considered as recurrent sub-networks.

3.2 Forward AlgoNet

The AlgoNet can be classiﬁed into two ﬂavors, the forward and the backward AlgoNet. To create
a forward AlgoNet, we use algorithmic layers and insert them into a neural network. By doing so,
the neural network may or may not ﬁnd a better local minimum by additionally employing the given
algorithm. We do so by using one of the following options for each algorithmic layer:

• Insert between two consecutive layers (Fig. 3a).
• Insert between two consecutive layers and also skip the algorithmic layer (Fig. 3b).
• Add a residual connection and apply the algorithmic layer on the residual part (Fig. 3c).

Generally, algorithmic layers do not have trainable weights. Regarding the accuracy, the output of
C∞ smooth WHILE-programs differs from the discrete WHILE-programs by a small factor and
offset. The output of C 0 smooth WHILE-programs equals the output of discrete WHILE-programs
for integer inputs (discrete WHILE-programs fail for non-integer inputs). One could counter this
factor and offset for C∞ smooth WHILE-programs by adding an additional weight and a bias to
each assignment in the WHILE-program. For that, one should regularize these weights and biases
to be close to one and zero, respectively. These parameters could be trained on a data set of integer
input/output pairs of the respective discrete WHILE-program to ﬁt the algorithmic layer. Moreover,
the algorithmic layer could also be trained to ﬁt the surrounding layers better.

AlgoNet

AlgoNet

...

AlgoNet

(a) fully applied AlgoNet layer

(b) shortcut AlgoNet layer

(c) residual AlgoNet layer

Figure 3: Different styles of the forward AlgoNet.

4

Figure 4: RAN System overview. The reconstructor receives an object from the input domain A
and predicts the corresponding reconstruction. The reconstruction, then, is validated through our
smooth inverse. The latter produces objects in a different domain, B, which are translated back to the
input domain A for training purposes (b2a). Unlike in traditional GAN systems, the purpose of our
discriminator D is mainly to indicate whether the two inputs match in content, not in style. Our novel
training scheme trains the whole network via ﬁve different data paths, including two which require
another domain translator, a2b.

3.3 Backward AlgoNet (RAN)

While forward AlgoNets can use arbitrary smooth algorithms—of course, an algorithm directly
related to the problem might perform better—backward AlgoNets use an algorithm that solves the
inverse of the given problem. E.g., a smooth renderer for 3D-reconstruction, a smooth iterated
function system (IFS) for solving the inverse-problem of IFS, and a smooth text-to-speech synthesizer
for speech recognition. While backward AlgoNets could be used in supervised settings, they are
designed for unsupervised or weakly supervised solving of inverse-problems. Their concept is the
following:

Input (∈ A) −→ Reconstructor −→ Goal −→ smooth inverse −→ Smooth version of input (∈ B)

This structure is similar to auto-encoders and the encoder-renderer architecture presented by Che et al.
[3]. Such an architecture, however, cannot directly be trained since there is a domain shift between
the input domain A and the smooth output domain B. Thus, we introduce domain translators (a2b
and b2a) to translate between these two domains. Since training with three consecutive components,
of which the middle one is highly restrictive, is extremely hard, we introduce a novel training schema
for these components: the Reconstructive Adversarial Network (RAN). For that, we also include
a discriminator to allow for adversarial training of the components a2b and b2a. Of our ﬁve
components four are trainable (the reconstructor, the domain translators a2b and b2a, and the
discriminator), and one is non-trainable (the smooth inverse).

Since, initially, neither the reconstructor nor the domain translators are trained, we are confronted
with a causality dilemma. A typical approach for solving such causality dilemmas is to solve the two
components coevolutionarily by iteratively applying various inﬂuences towards a common solution.
Fig. 4 depicts the structure of the RAN, which allows for such a coevolutionary training scheme.

The discriminator receives two inputs, one from space A and one from space B. One of these
inputs (either A or B) receives two values, a real and a fake value; the task of the discriminator is
to distinguish between these two, given the other input. For training, the discriminator is trained to
distinguish between the different path combinations for the generation of inputs. Consecutively, the
generator modules are trained to fool the discriminator. This adversarial game allows training the
RAN.

In the following, we will present this process, as well as its involved losses, in detail. Our optimization
of R, a2b, b2a, and D involves adversarial losses, cycle-consistency losses, and regularization losses.
Speciﬁcally, we solve the following optimization:

min
R

min
a2b

min
b2a

max
D

L

or in greater detail

min
R

min
a2b

min
b2a

max
D

5
(cid:88)

i=1

(αi · Li) + Lreg.

where αi is a weight in [0, 1] and L, and Li shall be deﬁned below. Lreg denotes the regularization
losses imposed on the reconstruction output.

5

We deﬁne b(cid:48), b(cid:48)(cid:48) ∈ B and a(cid:48), a(cid:48)(cid:48) ∈ A in dependency of a ∈ A according to Fig. 4 as

b(cid:48) = a2b(a)

b(cid:48)(cid:48) = Inv ◦ R(a)

a(cid:48) = b2a(b(cid:48))

a(cid:48)(cid:48) = b2a(b(cid:48)(cid:48)).

With that, our losses are (without hyper-parameter weights)
L1 = Ea∼A[log D(a, b(cid:48)(cid:48))] + Ea∼A[log(1 − D(a, b(cid:48)))] + Ea∼A[(cid:107)b(cid:48)(cid:48) − b(cid:48)(cid:107)1]
L2 = Ea∼A[log D(a, b(cid:48)(cid:48))] + Ea∼A[log(1 − D(a(cid:48)(cid:48), b(cid:48)(cid:48)))] + Ea∼A[(cid:107)a(cid:48)(cid:48) − a(cid:107)1]
L3 = Ea∼A[log D(a, b(cid:48))] + Ea∼A[log(1 − D(a(cid:48)(cid:48), b(cid:48)))] + Ea∼A[(cid:107)a(cid:48) − a(cid:107)1] + Ea∼A[(cid:107)b(cid:48)(cid:48) − b(cid:48)(cid:107)1]
L4 = Ea∼A[log D(a, b(cid:48)(cid:48))] + Ea∼A[log(1 − D(a(cid:48), b(cid:48)(cid:48)))] + Ea∼A[(cid:107)a(cid:48) − a(cid:107)1] + Ea∼A[(cid:107)b(cid:48)(cid:48) − b(cid:48)(cid:107)1]
L5 = Ea∼A[log D(a, b(cid:48))] + Ea∼A[log(1 − D(a(cid:48), b(cid:48)))] + Ea∼A[(cid:107)a(cid:48) − a(cid:107)1].

We alternately train the different sections of our network in the following order:

1. The discriminator D
2. The translation from B to A (b2a)
3. The components that perform a translation from A to B (R+Inv, a2b)

For each of these sections, we separately train the ﬁve losses L1, L2, L3, L4, and L5.
In our
experiments, we used one Adam optimizer [10] for each trainable component (R, a2b, b2a, and D).

4 Applications

In this section, we present speciﬁc AlgoNet-layers. Speciﬁcally, we present a smooth sorting
algorithm, a smooth median, a ﬁnite differences layer, a weighted SoftMax, smooth iterated function
systems, and a smooth 3D mesh renderer.

4.1 SoftSort

The SoftSort layer is a smooth sorting algorithm that is based on a parallelized version of bubble
sort [11] (see especially Section 5.3.4: Networks for Sorting), which sorts a tensor along an array of
scalars by repeatedly exchanging adjacent elements if necessary. Fig. 5 shows the structure of the
SoftSort algorithm. Contrasting our approach, the sorting layer in TensorFlow [2] is not smooth and
does not consider gradients with respect to the ordering induced by the sorting.

4.2 Finite differences

The ﬁnite differences method, which was introduced by Lewy et al. [12], is an essential tool for
ﬁnding numerical solutions of partial differential equations. In analogy, the ﬁnite differences layer
uses ﬁnite differences to compute the spatial derivative for one or multiple given dimensions of a
tensor. For that, we subtract the tensor from itself shifted by one in the given dimension. Optionally,
we normalize the result by shifting the mean to zero and/or add padding to output an equally sized
tensor. Thus, it is possible to integrate a spatially- or temporally-derivating layer into neural networks.

4.3 WeightedSoftMax

The weighted SoftMax (short: wSoftMax) allows a list that is fed to the SoftMax operator to be
smoothly sliced by weights indicating which elements are in the list. I.e., there are two inputs, the
actual values (x) and weights (w) from (0; 1] indicating which values of x should be considered
for the SoftMax. Thus, wSoftMax can be used when the maximum value of values, for which an
additional condition also holds, is searched by indicating whether the additional condition holds with
weights wi ∈ (0; 1]. We deﬁne the weighted SoftMax as:

wSoftMaxi(x, w) :=

exp(xi) · wi

(cid:80)(cid:107)w(cid:107)−1
i=0

exp(xi) · wi

=

exp(xi + log wi)

(cid:80)(cid:107)w(cid:107)−1
i=0

exp(xi + log wi)

(8)

= SoftMaxi(xi + log wi)

Accordingly, we deﬁne the weighted SoftMin (analogue to SoftMax/SoftMin) as wSoftMin(x, w) :=
wSoftMax(−x, w). By that, we enable a smooth selection to apply the SoftMax/SoftMin function
only to relevant values.

6

unsorted scalars

1st exchange

2nd exchange

sorted scalars

a0

a1

a2

...

an

a(1)
0

a(1)
1

a(1)
2

...

a(1)
n

(1 − e1
0)
e1
0

e1
0
(1 − e1
0)

(1 − e1
2)
e1
2

e1
n−1
(1 − e1

n−1)

M1

(1 − e2
1)
e2
1

e2
1
(1 − e2
1)

a(2)
0

a(2)
1

a(2)
2

...

a(2)
n

. . .

. . .

. . .

. . .

. . .

b0

b1

b2

...

bn

(cid:16)

ej
i : = σ

(a(j−1)

i+1 − a(j−1)

i

M : =

n−1
(cid:89)

i=1

Mi

M2

(cid:17)

) · s

M3

· · · Mn−1

where σ : x (cid:55)→

1
1 + e−x

b = a · M

t(cid:48) = t · M

(6)

(7)

Figure 5: The structure of SoftSort. Here, the exchanges of adjacent elements are represented by
matrices Mi. By multiplying these matrices with tensor a, we obtain b, the sorted version of a. By
instead multiplying with tensor t, we obtain t(cid:48): t sorted with respect to a. Using that, we can also sort
a tensor with respect to a learned metric. For sorting n values, we need n − 1 steps for an even n and
n steps for an odd n; to get a probabilistic coarse sorting, even fewer steps may sufﬁce. s denotes the
steepness of the sorting such that for s −→ ∞ we obtain a non-smooth sorting and for inﬁnitely many
sorting operations, all resulting values equal the mean of the input tensor. In the displayed graph, the
two recurrent layers are unrolled in time.

4.4 SoftMedian

The mean is a commonly used measure for reducing tensors, e.g., for normalizing a tensor. While
the median is robust against outliers, the mean is sensitive to all data points. This has two effects:
ﬁrstly, the mean is not the most representative value because it is inﬂuenced by outliers; secondly, the
derivative of a normalization substantially depends on the positions of outliers. I.e., outliers, which
might have accelerated gradients in the ﬁrst place, can inﬂuence all values during a normalization like
x(cid:48) := x − ¯x. While one would generally avoid these potentially malicious gradients by cutting the
gradients of ¯x, this is not adequate if changes in ¯x are expected. To reduce the inﬂuence of outliers in
a smooth way, we propose the SoftMedian, which comes in two styles, a precise and slower as well
as a signiﬁcantly faster version that only discards a ﬁxed number of outliers.

The precise version sorts the tensor with SoftSort and takes the middle value(s). For that, it is not
necessary to carry out the entire SoftSort; instead, only those computations that inﬂuence the middle
value need to be taken into account.

The faster variant to compute the SoftMedian (of degree j) is by its recursive deﬁnition in
which inﬂuence of the minimum and maximum values is reduced as follows: SoftMed(x)(j) :=
wSoftMin (cid:0)wSoftMin(x, SoftMed(x)(j−1)) + wSoftMax(x, SoftMed(x)(j−1)), SoftMed(x)(j−1)(cid:1)
where SoftMed(x)(0) := 1(cid:107)x(cid:107).

4.5 Smooth Iterated Function Systems

Iterated function systems (IFS) allow the construction of various fractals using only a set of parameters.
For example, pictures of plants like Barnsley‘s fern can be generated using only 4 × 6 = 24
parameters. Numerous different plants and objects can be represented using IFS. Since IFS are
parametric representations, they can be stored in very small space and be adjusted. This can be used,
e.g., in a computer game to avoid unnatural uniformity when rendering vegetation by changing the

7

parameters slightly, so that each plant looks slightly different. Finally, there are very fast algorithms
to generate images from IFS. While IFS provide many advantages, solving the inverse problem of
IFS, i.e., ﬁnding an IFS representation for any given image, is very hard and still unsolved. Towards
solving this inverse problem, we developed a C∞ smooth approximation to IFS.

Given a two-dimensional IFS with n bi-linear functions (fi)i∈{1..n} fi(x, y) := (x + a1 + a2x +
a3y, y + a4 + a5x + a6y), we repeatedly randomly select one function f ∈ (fi)i∈{1..n} and apply
it to an initial position or the proceeding result. We do that process arbitrarily often and plot every
intermediate step. Since it is not meaningful to interpolate multiple functions, because IFS rely on
randomized choices, and to provide consistency, we perform these random choices in advance.

The difﬁculty in this process is the rasterization since no crisp decision correlating pixels to points
can be made. Thus, we correlate each pixel to each point with a probability p ∈ [0; 1] where p = 1
is a full correlation and p = 0 means no correlation at all. By applying Gaussian smoothing on the
locations of the points, for each point, the probabilities p ∈ (0; 1) for each pixel deﬁne the correlation.
Concluding, for each pixel, the probabilities for all points to lie in the area of that pixel are known.
By aggregating these probabilities, we achieve a smooth rasterization.

We tested the smooth IFS by optimizing its parameters to ﬁt an image. For that, by setting the
standard deviation, different levels of details can be optimized.

4.6 Smooth Renderer

Lastly, we include a C∞ smooth 3D mesh renderer to the AlgoNet library, which projects a triangular
mesh onto an image while considering physical properties like perspective and shading. Compared to
previous differentiable renderers, this renderer is fully and not only locally differentiable. Moreover,
the continuity of the gradient allows for seamless integration into neural networks by avoiding
unexpected behavior altogether. By taking the decision which triangles cover a pixel, in analogy to
the smooth rasterization in Sec. 4.5, the silhouette of the mesh can be obtained. Consecutively, by
computing which of these triangles is the closest to the camera smoothly, our renderer’s depth buffer
is smooth. That allows for color handling and shading. The smooth depth buffer is the weighted
SoftMin of the distance between triangles and camera, weighted with the probability of a triangle to
exist at the coordinates of the pixel. In experiments, we have used the smooth renderer to perform
mesh optimization and mesh prediction using the backward AlgoNet.

5 Discussion and Conclusion

Concluding, in this work, we presented AlgoNets as a new kind of layers for neural networks and
developed a C∞ Turing complete interpreter. We have implemented the presented layers on top of
PyTorch and will publish our AlgoNet library upon publication of this work. Concurrent with their
beneﬁts, some AlgoNets can be computationally very expensive. For example, the rendering layer
requires a huge amount of computation, while the run time of the ﬁnite-differences layer is almost
negligible. On the other hand, the rendering layer is very powerful since it, e.g., allows training a 3D
reconstruction without 3D supervision using the backward AlgoNet.

The AlgoNet could also be used in the realm of explainable artiﬁcial intelligence [13] by adding
residual algorithmic layers into neural networks and then analyzing the neurons of the trained AlgoNet.
For that, network activation and/or network sensitivity can indicate the relevance of the residual
algorithmic layer. To compute the network sensitivity of an algorithmic layer, the gradient with
respect to additional weights (constant equal to one) in the algorithmic layer could be computed. By
that, similarities between classic algorithms and the behavior of neural networks could be inferred. An
alternative approach would be to gradually replace parts of trained neural networks with algorithmic
layers and analyzing the effect on the new model accuracy.

In future work, we could develop a high-level smooth programming language to improve the smooth
representations of higher level programming concepts. Adding trainable weights to the algorithmic
layers to improve the accuracy of smooth algorithms and/or allow the rest of the network to inﬂuence
the behavior of the algorithmic layer is subject to future research. The similarities of our smooth
WHILE-programs to analog as well as quantum computing shall be explored in future work. Another
future objective is the exploration of neural networks not with a ﬁxed but instead with a smooth
topology.

8

References

[1] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and

A. Lerer, “Automatic differentiation in PyTorch,” in NIPS-W, 2017.

[2] Mart’in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.
Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew
Harp, Geoffrey Irving, Michael Isard, Y. Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay
Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng, TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, 2015.
[Online]. Available: https://www.tensorflow.org/; https://www.tensorflow.org/api_
docs/python/tf/sort; https://www.tensorflow.org/api_docs/python/tf/while_loop;;

[3] C. Che, F. Luan, S. Zhao, K. Bala, and I. Gkioulekas, “Inverse Transport Networks,” Sep. 2018. [Online].

[4]

Available: http://arxiv.org/abs/1809.10820.
P. Henderson and V. Ferrari, “Learning to Generate and Reconstruct 3D Meshes with only 2D Supervision,”
Jul. 2018. [Online]. Available: https://arxiv.org/abs/1807.09259.

[5] R. A. DeMillo and R. J. Lipton, “Comments on “Deﬁning Software by Continuous Smooth Functions,”
IEEE Transactions on Software Engineering, vol. 19, no. 3, pp. 307–309, 1993, ISSN: 00985589. DOI:
10.1109/32.221140.

[6] Y. Nesterov, “Smooth minimization of non-smooth functions,” Mathematical Programming, vol. 103, no.

[7]

1, pp. 127–152, 2005, ISSN: 00255610. DOI: 10.1007/s10107-004-0552-5.
S. Chaudhuri and A. Solar-Lezama, “Smoothing a Program Soundly and Robustly,” in Computer Aided
Veriﬁcation, G. Gopalakrishnan and S. Qadeer, Eds., Berlin, Heidelberg: Springer Berlin Heidelberg,
2011, pp. 277–292, ISBN: 978-3-642-22110-1.

[8] Y. Yang and C. Barnes, “Approximate Program Smoothing Using Mean-Variance Statistics, with
Application to Procedural Shader Bandlimiting,” Jun. 2017. [Online]. Available: https: / /arxiv .
org/abs/1706.01208.

[9] U. Schöning, Theoretische Informatik - kurz gefasst. Spektrum Akademischer Verlag, 2008, ISBN:
9783827418241. [Online]. Available: https://books.google.de/books?id=eFqeJAAACAAJ.
[10] D. P. Kingma and J. Ba, “Adam: {A} Method for Stochastic Optimization,” CoRR, vol. abs/1412.6, 2014.

[Online]. Available: http://arxiv.org/abs/1412.6980.

[11] D. E. Knuth, The Art of Computer Programming: Volume 3: Sorting and Searching. Pearson Education,
1998, ISBN: 9780321635785. [Online]. Available: https : / / books . google . de / books ? id =
cYULBAAAQBAJ.

[12] F. K. C. R. Lewy H., “ÜBER DIE PARTIELLEN DIFFERENZENGLEICHUNGEN DER
MATHEMATISCHEN PHYSIK,” Mathematische Annalen, vol. 100, pp. 32–74, 1928. [Online].
Available: http://eudml.org/doc/159283.

[13] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal, “Explaining Explanations: An
Overview of Interpretability of Machine Learning,” May 2018. [Online]. Available: https://arxiv.
org/abs/1806.00069.

9

