1

Perfectly Parallel Fairness Certification
of Neural Networks

0
2
0
2

r
p
A
1
2

]
L
P
.
s
c
[

2
v
9
9
4
2
0
.
2
1
9
1
:
v
i
X
r
a

CATERINA URBAN, INRIA and DIENS, École Normale Supérieure, CNRS, PSL University, France
MARIA CHRISTAKIS, MPI-SWS, Germany
VALENTIN WÜSTHOLZ, ConsenSys Diligence, Germany
FUYUAN ZHANG, MPI-SWS, Germany

Recently, there is growing concern that machine-learning models, which currently assist or even automate
decision making, reproduce, and in the worst case reinforce, bias of the training data. The development of tools
and techniques for certifying fairness of these models or describing their biased behavior is, therefore, critical.
In this paper, we propose a perfectly parallel static analysis for certifying causal fairness of feed-forward neural
networks used for classification of tabular data. When certification succeeds, our approach provides definite
guarantees, otherwise, it describes and quantifies the biased behavior. We design the analysis to be sound,
in practice also exact, and configurable in terms of scalability and precision, thereby enabling pay-as-you-go
certification. We implement our approach in an open-source tool and demonstrate its effectiveness on models
trained with popular datasets.

1 INTRODUCTION
Due to the tremendous advances in machine learning and the vast amounts of available data,
software systems, and neural networks in particular, are of ever-increasing importance in our
everyday decisions, whether by assisting them or by autonomously making them. We are already
witnessing the wide adoption and societal impact of such software in criminal justice, health care,
and social welfare, to name a few examples. It is, therefore, not far-fetched to imagine a future
where most of the decision making is automated.

However, several studies have recently raised concerns about the fairness of such systems. For
instance, consider a commercial recidivism-risk assessment algorithm that was found racially
biased [Larson et al. 2016]. Similarly, a commercial algorithm that is widely used in the U.S. health
care system falsely determined that Black patients were healthier than other equally sick patients
by using health costs to represent health needs [Obermeyer et al. 2019]. There is also empirical
evidence of gender bias in image searches, for instance, there are fewer results depicting women
when searching for certain occupations, such as CEO [Kay et al. 2015]. Commercial facial recognition
algorithms, which are increasingly used in law enforcement, are less effective for women and
darker skin types [Buolamwini and Gebru 2018].

In other words, machine-learning software may reproduce, or even reinforce, bias that is directly
or indirectly present in the training data. This awareness will certainly lead to regulations and strict
audits in the future. It is, therefore, critical to develop tools and techniques for certifying fairness
of neural networks and understanding the circumstances of their potentially biased behavior.

Causal Fairness. We make a step forward in meeting these needs by designing a static analysis
framework for certifying causal fairness [Galhotra et al. 2017] of feed-forward neural networks
used for classification tasks. Specifically, given a choice (e.g., driven by a causal model) of input
features that are considered (directly or indirectly) sensitive to bias, a neural network is causally fair
if the output classification is not affected by different values of the chosen features. Note that, unlike

Authors’ addresses: Caterina Urban, INRIA , DIENS, École Normale Supérieure, CNRS, PSL University, Paris, France, caterina.
urban@inria.fr; Maria Christakis, MPI-SWS, Germany, maria@mpi-sws.org; Valentin Wüstholz, ConsenSys Diligence,
Germany, valentin.wustholz@consensys.net; Fuyuan Zhang, MPI-SWS, Germany, fuyuan@mpi-sws.org.

 
 
 
 
 
 
1:2

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

local robustness of neural networks, causal fairness is a global property, which is evaluated with
respect to all inputs, instead of only those within a particular distance metric.

Of course, the most obvious approach to avoid such bias is to remove any sensitive feature from
the training data, called fairness through unawareness [Grgić-Hlača et al. 2016]. However, this
does not work for three main reasons. First, neural networks learn from latent variables (e.g., [Lum
and Isaac 2016; Udeshi et al. 2018]). For instance, a credit-screening algorithm might not use race
(or gender) as an explicit input but still be biased with respect to it, say, by using the ZIP code of
applicants as proxy for race (or their first name as proxy for gender). Therefore, simply removing a
sensitive feature does not necessarily free the training data or the corresponding neural network
from bias. Second, the training data is only a relatively small sample of the entire input space, on
portions of which the neural network might end up being inaccurate. For example, if women are
underrepresented in the training data, a credit-screening algorithm is less likely to be accurate
for them. Third, the information provided by a sensitive feature might be necessary, for instance,
to introduce intended bias in a certain input region. Assume a credit-screening algorithm that
should not discriminate with respect to age unless it is above a particular threshold. Above this age
threshold, the higher the requested credit amount, the lower the chances of receiving it. In such
cases, removing the sensitive feature is not even possible.

Our Approach. Verification of global neural-network properties, such as causal fairness, is still
a long way from being practical (see Section 12). In this paper, we propose an approach that brings
us closer to this aspiration. Our approach certifies causal fairness of neural networks used for
classification of tabular data by employing a combination of a forward and a backward static analysis.
On a high level, the forward pass aims to reduce the overall analysis effort. At its core, it divides
the input space of the network into independent partitions. The backward analysis then attempts
to certify fairness of the classification within each partition (in a perfectly parallel fashion) with
respect to a chosen (set of) feature(s), which may be directly or indirectly sensitive, for instance,
race or ZIP code. In the end, our approach reports for which regions of the input space the neural
network is proved fair and for which there is bias. Note that we do not necessarily need to analyze
the entire input space; our technique is also able to answer specific bias queries about a fraction of
the input space, e.g., are Hispanics over 45 years old discriminated against with respect to gender?
The scalability-vs-precision tradeoff of our approach is configurable. Partitions that do not satisfy
the given configuration are excluded from the analysis and may be resumed later, with a more
flexible configuration. This enables usage scenarios in which our approach adapts to the available
resources, e.g., time or CPUs, and is run incrementally. In other words, we designed a pay-as-you-go
certification approach that the more resources it is given, the larger the region of the input space it
is able to analyze.

Related Work. In the literature, related work on determining fairness of machine-learning
models has focused on providing probabilistic guarantees [Bastani et al. 2019]. In contrast, our
approach gives definite guarantees for those input partitions that satisfy the analysis configuration.
Similarly to our approach, there is work that also aims to provide definite guarantees [Albarghouthi
et al. 2017b] (although for different fairness criteria). However, it has been shown to scale only up
to neural networks with two hidden neurons. Our approach is significantly more scalable since its
design enables perfectly parallel fairness certification of each input partition.

Contributions. We make the following contributions:
(1) We propose a perfectly parallel static analysis approach for certifying causal fairness of
feed-forward neural networks used for classification of tabular data. If certification fails, our
approach can describe and quantify the biased input space region(s).

Perfectly Parallel Fairness Certification of Neural Networks

1:3

Fig. 1. Small, constructed example of trained feed-forward neural network for credit approval.

(2) We show that our approach is sound and, in practice, exact for the analyzed regions of the

input space.

(3) We discuss the configurable scalability-vs-precision tradeoff of our approach that enables

pay-as-you-go certification.

(4) We implement our approach in an open-source tool called libra and evaluate it on neural
networks trained with popular datasets. We show the effectiveness of our approach in
detecting injected bias and answering bias queries. We also experiment with the precision
and scalability of the analysis and discuss the tradeoffs.

2 OVERVIEW
In this section, we give an overview of our approach using a small constructed example, which is
shown in Figure 1.

Example. The figure depicts a feed-forward neural network for credit approval. There are two
inputs x0,1 and x0,2 (shown in purple). Input x0,1 denotes the requested credit amount and x0,2
denotes age. Both inputs have continuous values in the range [0, 1]. Output x3,2 (shown in green)
denotes that the credit request is approved, whereas x3,1 (in red) denotes that it is denied. The
neural network also consists of two hidden layers with two nodes each (in gray).

Now, let us assume that this neural network is trained to deny requests for large credit amounts
from older people. Otherwise, the network does not discriminate with respect to age for small
credit amounts. There is also no bias for younger people with respect to the requested credit. When
choosing age as the sensitive input, our approach can certify fairness with respect to different age
groups for small credit amounts. Our approach is also able to find (as well as quantify) bias with
respect to age for large credit amounts. Note that this bias may be intended or accidental — our
analysis does not aim to address this question.

Our approach does not require age to be an explicit input of the neural network. For example,
x0,2 could denote the ZIP code of credit applicants, and the network could still use it as proxy for
age. That is, requests for large credit amounts are denied for a certain range of ZIP codes (where
older people tend to live), yet there is no discrimination between ZIP codes for small credit amounts.
When choosing the ZIP code as the sensitive input, our approach would again be able to detect
bias with respect to it for large credit amounts.

Below, we present on a high level how our approach achieves these results.

Naïve Approach. In theory, the simplest way to certify causal fairness is to first analyze the
neural network backwards starting from each output node, in our case x3,1 and x3,2. This allows
us to determine the regions of the input space (i.e., age and requested credit amount) for which
credit is approved and denied. For example, assume that we find that requests are denied for credit

x1,1x1,2x2,1x2,2x3,1x3,2x0,1x0,2-0.631.68-0.31-1.250.99-0.640.000.401.210.640.69-0.390.260.400.331.42-0.450.451:4

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

amounts larger than 10 000 (i.e., 10 000 < x0,1) and age greater than 60 (i.e., 60 < x0,2), while they
are approved for x0,1 ≤ 10 000 and 60 < x0,2 or for x0,2 ≤ 60.

The second step is to forget the value of the sensitive input (i.e., age) or, in other words, to project
these regions over the credit amount. In our example, after projection we have that credit requests
are denied for 10 000 < x0,1 and approved for any value of x0,1. A non-empty intersection between
the projected input regions indicates bias with respect to the sensitive input. In our example, the
intersection is non-empty for 10 000 < x0,1: there exist people that differ in age but request the
same credit amount (greater than 10 000), some of whom receive the credit while others do not.

This approach, however, is not practical. Specifically, neural networks with ReLU activation
functions (see Section 3 for more details, other activation functions are discussed in Section 9),
each hidden node effectively represents a disjunction between two activation statuses (active and
inactive). In our example, there are 24 possible activation patterns for the 4 hidden nodes. To retain
maximum precision, the analysis would have to explore all of them, which does not scale in practice.

Our Approach. Our analysis is based on the observation that there might exist many activation
patterns that do not correspond to a region of the input space [Hanin and Rolnick 2019]. Such patterns
can, therefore, be ignored during the analysis. We push this idea further by defining abstract
activation patterns, which fix the activation status of only certain nodes and thus represent sets of
(concrete) activation patterns. Typically, a relatively small number of abstract activation patterns
is sufficient for covering the entire input space, without necessarily representing and exploring all
possible concrete patterns.

Identifying those patterns that definitely correspond to a region of the input space is only possible
with a forward analysis. Hence, we combine a forward pre-analysis with a backward analysis.
The pre-analysis partitions the input space into independent partitions corresponding to abstract
activation patterns. Then, the backward analysis tries to prove fairness of the neural network for
each such partition.

More specifically, we set an upper bound U on the number of tolerated disjunctions (i.e., on the
number of nodes with an unknown activation status) per abstract activation pattern. Our forward
pre-analysis uses a cheap abstract domain (e.g., the boxes domain [Cousot and Cousot 1976]) to
iteratively partition the input space along the non-sensitive input dimensions to obtain fair input
partitions (i.e., boxes). Each partition satisfies one of the following conditions: (a) its classification is
already fair because only one network output is reachable for all inputs in the region, (b) it has an
abstract activation pattern with at most U unknown nodes, or (c) it needs to be partitioned further.
We call partitions that satisfy condition (b) feasible.

In our example, let U = 2. At first, the analysis considers the entire input space, that is, x0,1 : [0, 1]
(credit amount) and x0,2 : [0, 1] (age). (Note that we could also specify a part of the input space
for analysis.) The abstract activation pattern corresponding to this initial partition I is ϵ (i.e.,
no hidden nodes have fixed activation status) and, thus, the number of disjunctions would be 4,
which is greater than U. Therefore, I needs to be divided into I1 (x0,1 : [0, 0.5].x0,2 : [0, 1]) and I2
(x0,1 : [0.5, 1].x0,2 : [0, 1]). Observe that the input space is not split with respect to x0,2, which is the
sensitive input. Now, I1 is feasible since its abstract activation pattern is x1,2x2,1x2,2 (i.e., 3 nodes
are always active), while I2 must be divided further since its abstract activation pattern is ϵ.

To control the number of partitions, we impose a lower bound L on the size of each of their
dimensions. Partitions that require a dimension of a smaller size are excluded. In other words, they
are not considered until more analysis budget becomes available, that is, a larger U or a smaller L.
In our example, let L = 0.25. The forward pre-analysis further divides I2 into I2,1 (x0,1 :
[0.5, 0.75].x0,2 : [0, 1]) and I2,2 (x0,1 : [0.75, 1].x0,2 : [0, 1]). Now, I2,1 is feasible, with abstract
pattern x1,2x2,1, while I2,2 is not. However, I2,2 may not be split further because the size of the only

Perfectly Parallel Fairness Certification of Neural Networks

1:5

non-sensitive dimension x0,1 has already reached the lower bound L. As a result, I2,2 is excluded,
and only the remaining 75% of the input space is considered for analysis.

Next, feasible input partitions (within bounds L and U) are grouped by abstract activation patterns.
In our example, the pattern corresponding to I1, namely x1,2x2,1x2,2, is subsumed by the (more
abstract) pattern of I2,1, namely x1,2x2,1. Consequently, we group I1 and I2,1 under pattern x1,2x2,1.
The backward analysis is then run in parallel for each representative abstract activation pattern,
in our example x1,2x2,1. This analysis determines the region of the input space (within a given
partition group) for which each output of the neural network is returned, e.g., credit is approved for
c1 ≤ x0,1 ≤ c2 and a1 ≤ x0,2 ≤ a2. To achieve this, the analysis uses an expensive abstract domain,
for instance, disjunctive or powerset polyhedra [Cousot and Cousot 1979; Cousot and Halbwachs
1978], and leverages abstract activation patterns to avoid disjunctions. For instance, pattern x1,2x2,1
only requires reasoning about two disjunctions from the remaining hidden nodes x1,1 and x2,2.

Finally, fairness is checked for each partition in the same way that it is done by the naïve approach
for the entire input space. In our example, we prove that the classification within I1 is fair and
determine that within I2,1 the classification is biased. Concretely, our approach determines that
bias occurs for 0.54 ≤ x0,1 ≤ 0.75, which corresponds to 21% of the entire input space (assuming a
uniform probability distribution). In other words, the network returns different outputs for people
that request the same credit in the above range but differ in age. Recall that partition I2,2, where
0.75 ≤ x0,1 ≤ 1, was excluded from analysis, and therefore, we cannot draw any conclusions about
whether there is any bias for people requesting credit in this range.

Note that bias may also be quantified according to a probability distribution of the input space.
In particular, it might be that credit requests in the range 0.54 ≤ x0,1 ≤ 0.75 are more (resp.
less) common in practice. Given their probability distribution, our analysis computes a tailored
percentage of bias, which in this case would be greater (resp. less) than 21%.

3 FEED-FORWARD DEEP NEURAL NETWORKS
Formally, a feed-forward deep neural network consists of an input layer (l0), an output layer (ln),
and a number of hidden layers (l1, . . . , ln−1) in between. Each layer li contains |li | nodes and, with
the exception of the input layer, is associated to a |li | × |li−1|-matrix Wi of weight coefficients and
a vector Bi of |li | bias coefficients. In the following, we use X to denote the set of all nodes, Xi to
denote the set of nodes of the ith layer, and xi, j to denote the jth node of the ith layer of a neural
network. We focus here on neural networks used for classification tasks. Thus, |ln| is the number
of target classes (e.g., 2 classes in Figure 1).

The value of the input nodes is given by the input data: continuous data is represented by
one input node (e.g., x0,1 or x0,2 in Figure 1), while categorical data is represented by multiple
input nodes via one-hot encoding. In the following, we use K to denote the subset of input nodes
considered (directly or indirectly) sensitive to bias (e.g., x0,2 in Figure 1) and K def= X0 \ K to denote
the input nodes not deemed sensitive to bias.

The value of each hidden and output node xi, j is computed by an activation function f applied
to a linear combination of the values of all nodes in the preceding layer [Goodfellow et al. 2016],
and bi, j are weight and bias coefficients in Wi
i.e., xi, j = f
· xi−1,k + bi, j
and Bi , respectively. In a fully-connected neural network, all wi
are non-zero. Weights and biases
are adjusted during the training phase of the neural network. In what follows, we focus on already
trained neural networks, which we call neural-network models.

(cid:17), where wi
j,k

(cid:16)(cid:205)|li −1 |
k

wi

j,k

j,k

Nowadays, the most commonly used activation for hidden nodes is the Rectified Linear Unit
(ReLU) [Nair and Hinton 2010]: ReLU(x) = max(x, 0). In this case, the activation used for output

1:6

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

nodes is the identity function. The output values are then normalized into a probability distribution
on the target classes [Goodfellow et al. 2016]. We discuss other activation functions in Section 9.

4 TRACE SEMANTICS
Our approach expresses neural-network models as programs. These programs consist of assign-
ments for computing the activation value of each node (e.g., x1,1 = −0.31 ∗ x0,1 + 0.99 ∗ x0,2 − 0.63 in
Figure 1) and implementations of activation functions (e.g., if-statements for ReLUs). As is standard
practice in static program analysis, we define a semantics for these programs and use it to prove
soundness of our approach.

The semantics of a neural-network model is a mathematical characterization of its behavior
when executed for all possible input data. We model the operational semantics of a feed-forward
neural-network model M as a transition system ⟨Σ, τ ⟩, where Σ is a (potentially infinite) set of
states and the acyclic transition relation τ ⊆ Σ × Σ describes the possible transitions between states
[Cousot 2002; Cousot and Cousot 1977].

Y

Y

def= (cid:110)
s

(cid:12)
(cid:12) s ∈ Σ

More specifically, a state s ∈ Σ maps neural-network nodes to their values. Here, for simplicity, we
assume that nodes have real values, i.e., s : X → R. (We discuss floating-point values in Section 9.)
In the following, we often only care about the values of a subset of the neural-network nodes
(cid:111) be the restriction of Σ to a domain of interest Y .
in certain states. Thus, let Σ
Sets Σ X0
denote restrictions of Σ to the network nodes in the input and output layer,
and Σ Xn
respectively. With a slight abuse of notation, let Xi, j denote Σ {xi, j }
, i.e., the restriction of Σ to
the singleton set containing xi, j . Transitions happen between states with different values for
consecutive nodes in the same layer, i.e., τ ⊆ Xi, j × Xi, j+1, or between states with different values
for the last and first node of consecutive layers of the network, i.e., τ ⊆ Xi, |li | × Xi+1,0. The
def= {s ∈ Σ | ∀s ′ ∈ Σ : ⟨s, s ′⟩ (cid:60) τ } is the set of final states of the neural network. These are
set Ω
(cid:9) | 0 ≤ i ≤ |ln|(cid:9), depending on the
partitioned in a set of outcomes O def= (cid:8)(cid:8)s ∈ Ω | max Xn = xn,i
output node with the highest value (i.e., the target class with highest probability).

Let Σn def= {s0 · · · sn−1 | ∀i < n : si ∈ Σ} be the set of all sequences of exactly n states in Σ.
Let Σ+ def= (cid:208)
n ∈N+ Σn be the set of all non-empty finite sequences of states. A trace is a se-
quence of states that respects the transition relation τ , that is, ⟨s, s ′⟩ ∈ τ for each pair of con-
n def=
secutive states s, s ′ in the sequence. We write Σ
{s0 · · · sn−1 ∈ Σn | ∀i < n − 1 : ⟨si , si+1⟩ ∈ τ }. The trace semantics ϒ ∈ P (Σ+) generated by a transi-
tion system ⟨Σ, τ ⟩ is the set of all non-empty traces terminating in Ω [Cousot 2002]:

n for the set of all traces of n states: Σ

(cid:110)
s0 . . . sn−1 ∈ Σ

n | sn−1 ∈ Ω

(cid:111)

(1)

In the rest of the paper, we write

to denote the trace semantics of a neural-network model M.
The trace semantics fully describes the behavior of M. However, reasoning about a particular
property of M does not need all this information and, in fact, is facilitated by the design of a
semantics that abstracts away from irrelevant details about M’s behavior. In the following sections,
we formally define our property of interest, causal fairness, and systematically derive, using abstract
interpretation [Cousot and Cousot 1977], a semantics tailored to reasoning about this property.

(cid:74)

(cid:75)

ϒ

def= (cid:216)
n ∈N+
M

5 CAUSAL FAIRNESS
A property is specified by its extension, that is, by the set of elements having such a property
[Cousot and Cousot 1977, 1979]. Properties of neural-network models are properties of their
semantics. Thus, properties of network models with trace semantics in P (Σ+) are sets of sets of

Perfectly Parallel Fairness Certification of Neural Networks

1:7

traces in P (P (Σ+)). In particular, the set of neural-network properties forms a complete boolean
lattice ⟨P (P (Σ+)) , ⊆, ∪, ∩, ∅, P (Σ+)⟩ for subset inclusion, that is, logical implication. The strongest
property is the standard collecting semantics Λ ∈ P (P (Σ+)):

def= {ϒ}

Λ

(2)

denote the collecting semantics of a particular neural-network model M. Then, model M

Let
satisfies a given property H if and only if its collecting semantics is a subset of H :

M
(cid:77)

(cid:76)

M
(cid:76)
Here, we consider the property of causal fairness, which expresses that the classification deter-
mined by a network model does not depend on sensitive input data. In particular, the property
might interest the classification of all or just a fraction of the input space.

M |= H ⇔

⊆ H

(3)

(cid:77)

More formally, let V be the set of all possible value choices for all sensitive input nodes in K, e.g.,
for K = (cid:8)x0,i , x0, j
(cid:9)
(cid:9) one-hot encoding, say, gender information, V = {{1, 0} , {0, 1}}; for K = (cid:8)x0,k
encoding continuous data, say, in the range [0, 1], a possibility is V = {[0, 0.25], [0.25, 0.75], [0.75, 1]}.
In the following, given a trace σ ∈ P (Σ+), we write σ0 and σω to denote its initial and final state,
respectively. We also write σ0 =K σ ′
0 agree on all values of all
non-sensitive input nodes, and σω ≡ σ ′
to indicate that σ and σ ′ have the same outcome O ∈ O.
ω
We can now formally define when the sensitive input nodes in K are unused with respect to a set of
traces T ∈ P (Σ+) [Urban and Müller 2018]. For one-hot encoded sensitive inputs1 we have

0 to indicate that the states σ0 and σ ′

unusedK(T )

def= ∀σ ∈ T , V ∈ V : σ0(K) (cid:44) V ⇒ ∃σ ′ ∈ T : σ0 =K σ ′

0 ∧ σ ′

0(K) = V ∧ σω ≡ σ ′
ω ,

(4)

def= {σ0(x) | x ∈ K} is the image of K under σ0. Intuitively, the sensitive input nodes in
where σ0(K)
K are unused if any possible outcome in T (i.e., any outcome σω of any trace σ in T ) is possible
from all possible value choices for K (i.e., there exists a trace σ ′ in T for each value choice for K
with the same outcome as σ ). That is, each outcome is independent of the value choice for K.

Example 5.1. Let us consider again our example in Figure 1. We write ⟨c, a⟩ ⇝ o for a trace starting
in a state with x0,1 = c and x0,2 = a and ending in a state where o is the node with the highest value
(i.e., the output class). The sensitive input x0,2 (age) is unused in T = (cid:8)⟨0.5, a⟩ ⇝ x3,2 | 0 ≤ a ≤ 1(cid:9).
It is instead used in T ′ = (cid:8)⟨0.75, a⟩ ⇝ x3,2 | 0 ≤ a < 0.51(cid:9) ∪ (cid:8)⟨0.75, a⟩ ⇝ x3,1 | 0.51 ≤ a ≤ 1(cid:9).

The causal-fairness property FK can now be defined as FK

)}, that is, as
the set of all neural-network models (or rather, their semantics) that do not use the values of the
sensitive input nodes for classification. In practice, the property might interest just a fraction of the
input space, i.e., we define

| unusedK(

M
(cid:74)

M
(cid:74)

(cid:75)

(cid:75)

def= {

FK[Y ]

def= (cid:8)

Y | unusedK(
M
(cid:75)

M
(cid:74)

(cid:74)

Y )(cid:9) ,
(cid:75)

(5)

where Y ∈ P (Σ) is a set of initial states of interest and the restriction T Y def= {σ ∈ T | σ0 ∈ Y } only
contains traces of T ∈ P (Σ+) that start with a state in Y . Similarly, in the rest of the paper, we
write SY def= (cid:8)T Y | T ∈ S(cid:9) for the set of sets of traces restricted to initial states in Y . Thus, from
Equation 3, we have the following:

Theorem 5.2. M |= FK[Y ] ⇔

M
(cid:76)

Y ⊆ FK[Y ]
(cid:77)

Proof. The proof follows trivially from Equation 3 and the definition of FK[Y ] (cf. Equation 5)
□

and

1For continuous sensitive inputs, we can replace σ0(K) (cid:44) V (resp. σ0(K) = V) with σ0(K) ⊈ V (resp. σ0(K) ⊆ V).

M
(cid:76)

Y .
(cid:77)

1:8

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

6 DEPENDENCY SEMANTICS
We now use abstract interpretation to systematically derive, by successive abstractions of the
collecting semantics Λ, a sound and complete semantics Λ⇝
that contains only and exactly the
information needed to reason about FK[Y ].

6.1 Outcome Semantics
def= {σ ∈ T | σω ∈ Z } be the set of traces of T ∈ P (Σ+) that end with a state in Z ∈ P (Σ). As
Let TZ
def= {TZ | T ∈ S } for the set of sets of traces restricted to final states in Z . From
before, we write SZ
the definition of FK[Y ] (and in particular, from the definition of unusedK, cf. Equation 4), we have:

Lemma 6.1.

Proof. Let

M
(cid:76)

Y ⊆ FK[Y ] ⇔ ∀O ∈ O :
(cid:77)
M
(cid:76)

Y ⊆ FK[Y ]. From the definition of
(cid:77)

Y
O ⊆ FK[Y ]
(cid:77)

M
(cid:76)

(cid:76)

Y (cf. Equation 2), we have that
M
(cid:77)

M

Y ∈
(cid:74)
(cid:75)
Y ). Now, from
M
(cid:75)
O). Thus,
Y
(cid:75)
□

M

FK[Y ]. Thus, from the definition of FK[Y ] (cf. Equation 5), we have unusedK(
the definition of unusedK (cf. Equation 4), we equivalently have ∀O ∈ O : unusedK(
(cid:74)
O ⊆ FK[Y ].
we can conclude that ∀O ∈ O :
M
Y
(cid:77)

(cid:74)

(cid:76)

In particular, this means that in order to determine whether a neural-network model M satisfies
causal fairness, we can independently verify, for each of its possible target classes O ∈ O, that the
values of its sensitive input nodes are unused.

We use this insight to abstract the collecting semantics Λ by partitioning. More specifically, let
O | O ∈ O(cid:9) be a trace partition with respect to outcome. We have the following Galois
def= (cid:8)Σ+
•
connection

⟨P (cid:0)P (cid:0)Σ

⟨P (cid:0)P (cid:0)Σ
def= {TO | T ∈ S ∧ O ∈ O}. The order ⊆· is the pointwise ordering between sets of traces
def= (cid:211)O∈O (cid:219)AO ⊆ (cid:219)BO, where (cid:219)SZ denotes the only non-empty set of

where α•(S)
with the same outcome, i.e., A ⊆· B
traces in SZ . We can now define the outcome semantics Λ• ∈ P (P (Σ+)) by abstraction of Λ:

+(cid:1)(cid:1) , ⊆⟩ −−−−→←−−−−

+(cid:1)(cid:1) , ⊆·⟩,

(6)

α•

γ•

In the rest of the paper, we write
network model M.

def= α•(Λ) = {ϒO | O ∈ O}
• to denote the outcome semantics of a particular neural-
(cid:77)

(7)

Λ•
M
(cid:76)

6.2 Dependency Semantics
We observe that, to reason about causal fairness, we do not need to consider all intermediate
computations between the initial and final states of a trace. Thus, we can further abstract the
outcome semantics into a set of dependencies between initial states and outcomes of traces.

To this end, we define the following Galois connection2

⟨P (cid:0)P (cid:0)Σ

+(cid:1)(cid:1) , ⊆·⟩ −−−−−→←−−−−−

γ⇝

α⇝

⟨P (P (Σ × Σ)) , ⊆·⟩,

(8)

def= {{⟨σ0, σω ⟩ | σ ∈ T } | T ∈ S } [Urban and Müller 2018] abstracts away all interme-

(S)

where α⇝
diate states of any trace. We finally derive the dependency semantics Λ⇝
(Λ•) = {{⟨σ0, σω ⟩ | σ ∈ ϒO} | O ∈ O}

Λ⇝

def= α⇝

∈ P (P (Σ × Σ)):

(9)

In the following, let

M

denote the dependency semantics of a particular network model M.

2Note that here and in the following, for convenience, we abuse notation and reuse the order symbol ⊆· defined over sets of
sets of traces, instead of its abstraction, defined over sets of sets of pairs of states.

(cid:76)

(cid:77)⇝

Perfectly Parallel Fairness Certification of Neural Networks

1:9

Let RY def= {⟨s, _⟩ ∈ R | s ∈ Y } restrict a set of pairs of states to pairs whose first element is in Y
and, similarly, let SY def= (cid:8)RY | R ∈ S (cid:9) restrict a set of sets of pairs of states to first elements in Y .
The next result shows that Λ⇝
Theorem 6.2. M |= FK[Y ] ⇔

is sound and complete for proving causal fairness:

⊆· α⇝

Y
⇝
(cid:77)

M
(cid:76)
Proof. Let M |= FK[Y ]. From Theorem 5.2, we have that

connections in Equation 6 and 8, we have α⇝
M
Y
⇝
(cid:77)

M
Y )) ⊆· α⇝
(cid:76)
M
(α•(
⊆· α⇝
(cid:77)
(cid:76)
M
(cf. Equation 9), we can then conclude that
Y
⇝
(cid:76)
(cid:77)
(FK[Y ])

(cid:76)
Corollary 6.3. M |= FK[Y ] ⇔

(α•(FK[Y ]))

M
(cid:76)

Y
⇝
(cid:77)

⊆ α⇝

Y ⊆ FK[Y ]. Thus, from the Galois
(cid:77)
(α•(FK[Y ])). From the definition of
□
(α•(FK[Y ])).

Proof. The proofs follows trivially from the definition of ⊆· (cf. Equation 6 and 8) and Lemma 6.1.
□

Furthermore, we observe that partitioning with respect to outcome induces a partition of the
space of values of the input nodes used for classification. For instance, partitioning T ′ in Example 5.1
induces a partition on the values of (the indeed used node) x0,2. Thus, we can equivalently verify
whether
induces a partition
def= {s | ⟨_, s⟩ ∈ R}) be the selection of the first (resp. last)
of Y K
element from each pair in a set of pairs of states. We formalize this observation below.

(FK[Y ]) by checking if the dependency semantics

def= {s | ⟨s, _⟩ ∈ R} (resp. Rω

M
Y
⇝
(cid:76)
(cid:77)
. Let R0

⊆ α⇝

M
(cid:76)

Y
⇝
(cid:77)

Lemma 6.4. M |= FK[Y ] ⇔ ∀A, B ∈

: (Aω (cid:44) Bω ⇒ A0 K ∩ B0 K

= ∅)

M
(cid:76)

Y
⇝
(cid:77)

(cid:76)

M
Y
⇝
(cid:77)

(cf. Equation 9), we have ∀O ∈ O : α⇝

Proof. Let M |= FK[Y ]. From Corollary 6.3, we have that
M
Y
⇝
(cid:76)
(cid:77)
M
Y
O) ∈ α⇝
(cid:75)

(FK[Y ]). Thus, from the
(FK[Y ]). In particular, from
definition of
O) for each O ∈ O.
the definition of α⇝
and FK[Y ] (cf. Equation 5), we have that unusedK(
M
Y
(cid:75)
(cid:74)
From the definition of unusedK (cf. Equation 4), for each pair of non-empty
for
M
(cid:74)
different O1, O2 ∈ O (the case in which one or both are empty is trivial), it must necessarily be the
value of the non-sensitive input nodes in K that causes the different outcome O1 or O2. We can
□
: (Aω (cid:44) Bω ⇒ A0 K ∩ B0 K
thus conclude that ∀A, B ∈

⊆ α⇝

Y
O1
(cid:75)

Y
O2
(cid:75)

= ∅).

and

(
(cid:74)

M

(cid:74)

M
(cid:76)

Y
⇝
(cid:77)

7 NAÏVE CAUSAL-FAIRNESS ANALYSIS
In this section, we present a first static analysis for causal fairness that computes a sound over-
approximation Λ♮
. This analysis corresponds to
⇝
the naïve approach we discussed in Section 2. While it is too naïve to be practical, it is still useful
for building upon later in the paper.

of the dependency semantics Λ⇝

, i.e., Λ⇝

⊆· Λ♮

⇝

For simplicity, we consider ReLU activation functions. (We discuss extensions to other activation
functions in Section 9.) The naïve static analysis is described in Algorithm 1. It takes as input (cf.
Line 14) a neural-network model M, a set of sensitive input nodes K of M, a (representation of a)
set of initial states of interest Y , and an abstract domain A to be used for the analysis. The analysis
proceeds backwards for each outcome (i.e., each target class xn, j ) of M (cf. Line 17) in order to
determine an over-approximation of the initial states that satisfy Y and lead to xn, j (cf. Line 18).
(cf. Line 2) modifies a given abstract-domain
element to assume the given outcome x, that is, to assume that max Xn = x. The transfer functions
←−−−reluA
(cf. Line 5) respectively consider a ReLU operation and replace xi, j
with the corresponding linear combination of nodes in the preceding layer (see Section 3).

More specifically, the transfer function outcomeA

and ←−−−−−
assignA

x
(cid:75)

xi, j

xi, j

(cid:74)

(cid:75)

(cid:74)

(cid:74)

(cid:75)

Finally, the analysis checks whether the computed over-approximations satisfy causal fairness
as
with respect to K (cf. Line 19). In particular, it checks whether they induce a partition of Y K
observed for Lemma 6.4 (cf. Lines 7-13). If so, we have proved that M satisfies causal fairness. If

1:10

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Algorithm 1 : A Naïve Backward Analysis

xi, j

a)

(cid:75)

(cid:74)

▷ B: biased

1: function backward(M, A, x)
(newA)
2:
3:
4:

a ← outcomeA
for i ← n − 1 down to 0 do

x
(cid:74)
(cid:75)
for j ← |li | down to 0 do
a ← ←−−−−−
assignA

5:

xi, j
(cid:74)

(←−−−reluA
(cid:75)

return a
6:
7: function check(O)
8:
9:
10:
11:

B ← ∅
for all o1, a1 ∈ O do

for all o2 (cid:44) o1, a2 ∈ O do

if a1 ⊓A2 a2 (cid:44) ⊥A2 then
B ← B ∪ (cid:8)a1 ⊓A2 a2(cid:9)

return B

13:
14: function analyze(M, K, Y , A)
15:
16:

O ← (cid:219)∅
for j ← 0 up to |ln| do

a ← backward(M, A, xn, j )
(cid:110)xn, j (cid:55)→ (assumeA
O ← O ∪

B ← check(O)
return B = ∅, B

12:

17:

18:

19:
20:

(cid:111)

Y
(cid:74)

a) K
(cid:75)

▷ perfectly parallelizable

▷ fair: B = ∅, maybe biased: B (cid:44) ∅

not, the analysis returns a set B of abstract-domain elements over-approximating the input regions
in which bias might occur.

Theorem 7.1. If analyze(M, K, Y , A) of Algorithm 1 returns true, ∅ then M satisfies FK[Y ].

Proof (Sketch). analyze(M, K, Y , A) in Algorithm 1 computes an over-approximation a of the
regions of the input space that yield each target class xn, j (cf. Line 17). Thus, it actually computes
. Thus, if
an over-approximation
M
, i.e.,
(cid:76)
Y ♮
= ∅) (according to Lemma 6.4,
M
⇝
(cid:77)
□

(cid:76)
cf. Line 19), then by transitivity we can conclude that also

Y
⇝
(cid:77)
: (Aω (cid:44) Bω ⇒ A0 K ∩ B0 K

satisfies FK[Y ], i.e., ∀A, B ∈

of the dependency semantics

Y ♮
M
⇝
(cid:77)

Y ♮
M
⇝
(cid:77)

necessarily satisfies FK[Y ].

Y ♮
⇝
(cid:77)

M
(cid:76)

M
(cid:76)

Y
⇝
(cid:77)

⊆·

(cid:76)

(cid:76)

In the analysis implementation, there is a tradeoff between performance and precision, which
is reflected in the choice of abstract domain A and its transfer functions. Unfortunately, existing
numerical abstract domains that are less expressive than polyhedra [Cousot and Halbwachs 1978]
would make for a rather fast but too imprecise analysis. This is because they are not able to
precisely handle constraints like max Xn = x, which are introduced by outcomeA
to partition
with respect to outcome.

x
(cid:75)

Furthermore, even polyhedra would not be precise enough in general. Indeed, each ←−−−reluA

xi, j
(cid:75)
def= |l1| + · · · + |ln−1|
would over-approximate what effectively is a conditional branch. Let |M|
denote the number of hidden nodes (i.e., the number of ReLUs) in a model M. On the other side of
the spectrum, one could use a disjunctive completion [Cousot and Cousot 1979] of polyhedra, thus
keeping a separate polyhedron for each branch of a ReLU. This would yield a precise (in fact, exact)
but extremely slow analysis: even with parallelization (cf. Lines 16), each of the |ln| processes
would have to effectively explore 2 |M | paths!

(cid:74)

(cid:74)

Y ♮
M
⇝
(cid:77)

(cid:76)

Perfectly Parallel Fairness Certification of Neural Networks

1:11

Fig. 2. Hierarchy of semantics.

In the rest of the paper, we improve on this naïve analysis and show how far we can go all the

while remaining exact by using disjunctive polyhedra.

8 PARALLEL SEMANTICS
We first have to take a step back and return to reasoning at the concrete-semantics level. At the end
of Section 6, we observed that the dependency semantics of a neural-network model M satisfying
FK[Y ] effectively induces a partition of Y K

. We call this input partition fair.

More formally, given a set Y of initial states of interest, we say that an input partition I of Y is
fair if all value choices V for the sensitive input nodes K of M are possible in all elements of the
partitions: ∀I ∈ I, V ∈ V : ∃s ∈ I : s(K) = V. For instance, I = (cid:8)T0,T ′
(cid:9), with T and T ′ in Example 5.1
0
is a fair input partition of Y = (cid:8)s | s(x0,1) = 0.5 ∨ s(x0,1) = 0.75(cid:9).

Given a fair input partition I of Y , the following result shows that we can verify whether a model

M satisfies FK[Y ] for each element I of I, independently.
Lemma 8.1. M |= FK[Y ] ⇔ ∀I ∈ I : ∀A, B ∈

(cid:76)
Proof. The proof follows trivially from Lemma 6.4 and the fact that I is a fair partition.

□

I
⇝

M
(cid:77)

: (Aω (cid:44) Bω ⇒ A0 K ∩ B0 K

= ∅)

Galois connection

We use this new insight to further abstract the dependency semantics Λ⇝
⟨P (P (Σ × Σ)) , ⊆·⟩ −−−−→←−−−−

⟨P (P (Σ × Σ)) , ⊆·I⟩,

γI

αI

. We have the following

(10)

where αI(S)
of states restricted to first elements in the same I ∈ I, i.e., A ⊆·I B
the only non-empty set of pairs in S I. We can now derive the parallel semantics ΠI
⇝

def= (cid:8)RI | R ∈ S ∧ I ∈ I(cid:9). Here the order ⊆·I is the pointwise ordering between sets of pairs
def= (cid:211)I∈I (cid:219)AI ⊆ (cid:219)BI, where (cid:219)S I denotes
∈ P (P (Σ × Σ)):

Π

I
⇝

def= αI(Λ⇝

) = (cid:8)(cid:8)⟨σ0, σω ⟩ | σ ∈ ϒ

I
O

(cid:9) | I ∈ I ∧ O ∈ O(cid:9)

(11)

In fact, we derive a hierarchy of semantics, as depicted in Figure 2. We write {|M|}I
to denote
⇝
the parallel semantics of a particular neural-network model M. It remains to show soundness and
completeness for ΠI
⇝
Theorem 8.2. M |= FK[Y ] ⇔ {|M|}I
⇝

(α•(FK[Y ])))
Proof. Let M |= FK[Y ]. From Theorem 6.2, we have that

⊆·I αI(α⇝

.

⊆· α⇝
M
(α•(FK[Y ])). Thus, from
Y
⇝
) ⊆· αI(α⇝
(cid:77)
(cid:76)
(α•(FK[Y ]))). From the definition
⊆·I αI(α⇝
□

(α•(FK[Y ]))).

the Galois connections in Equation 10, we have αI(
of {|M|}I
(cid:76)
⇝
Corollary 8.3. M |= FK[Y ] ⇔ {|M|}I
⇝

(cf. Equation 11), we can then conclude that {|M|}I
⇝

⊆ αI(α⇝

M
Y
⇝
(cid:77)

(FK[Y ]))

⇧ <latexit sha1_base64="9Jh+QckvGjoZsd5n4m/stjnS83E=">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</latexit>⇧•<latexit sha1_base64="K2qYzUhgG/dkhkzeQfd1SaxUvG8=">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</latexit>⇧<latexit sha1_base64="zqfQEdmK4SI3FTRYfWz+DF8KlKw=">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</latexit>⇤<latexit sha1_base64="iT+3uuYiMggFzKCK7Pjvjaidrvw=">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</latexit>⇤•<latexit sha1_base64="kuinz/me8+58F3QR5/c44DI8SWQ=">AAAlAHiczVnNb+S2FZ8k/Ujd1tlt0FMvRMde2N3JdGZsx0aKBdIUCFJksevsej8Ay2tQEjVDmPqwSNmeCOqh/0ivvQW99t5re+5/00dSGkkcyt9BOoA9Evnej++Lj+9x3IRRLkaj/773/gc/+vFPfvrhz1Z+/otfrn704OGvXvM4Sz3yyotZnL51MSeMRuSVoIKRt0lKcOgy8sY9+ZOcf3NGUk7j6EDME3IU4mlEA+phAUPHD1f31nNHwRymU/coHw0nO7ujyacDeNjd3tnbhoet0XiyOynWHJdMaZRjRqcR8YuVdecpLORj9OgJcliQvHP4Kc9cTgQ5/YtzMCMCOw5QqaeNg030yPFJAIL6yIEvzJATYjFLCQMRsqRAG47AGQySFNaZooNNYCaRv1hxDda8s7THTpwJLw7JQuzccTPGTxk5LUqxa5pagcXYxsumJjANCuelRsd5XIAC1EcxcmiEnqNCUizUhIWUpiVPW13NeKAYX6Li+1HeJwmgksibS/1hwrDAca5kc17SaYgdQUPC9YAbi2KAHBImYg4DRW2rBmTTXPXw1RZT6yzbsDThVRb0aOqhF5rnxSXmuxcDqsUuM12nGrXFFEbTVnKgw0pK8zCHXV3IPXRljCwpOd7dHu9sgW6j3d3Rp/JhsrU9Gu+BkpyEOBLU4/m+Lb6uyanFvz3/OyfCIksxu1mIS8N4mOVfFsdfozqzaKs18ZWxnCQ+hzxYh3aIL2iIWekjJ4hBAIaoIv5aCQFMWZRx4h/TjRbgJrqDtRbaarHaWkSxcF7Q6UzgNI3PbWo06W9kr33gjX3CuLnmUxKI9P9ozUbyrU6TK0W4dqReAb0sWRkXL4/HA/g30QaxYlYxwwBhKBkOR0eOgnLgBBYS3MNJPT9Zmn9Sp9cbmRmywHK6uqMej3R+uhdlHhlpamX9h1CvPo9+IA27g/SeNJRp2K7bpYp9X3679u6f4jBsFwydrtu8l6RwF8nKIXViW+y/eT8SOpgls7rmvBVvLfUt2N89vtWaN68EdF0FPDdlucw4l5QNy4Xwbfit5rkG152Elux3FbvTRdfiTbBMQkRWascP+pWTUOUkVDkJjYcj9en3ys/+8cNff+f4sZeFJBIew5wfjkeJOMpxChuIkWLFgVorwd4JnpJDeIww9ARHuRKyQOuyEEOQBOEvEkiNNjlyHHI+D12glJuOm3Ny0DZ3mIlg7yinUZIJsKpeKMgYEjGSDTPyaUo8webwgL2UgqzIm4EZPAHl5EprGS5CnM5Tv6VKLujJt3pEPjHqpkCUqwTDBzgTMciEBz7xIMHLppwPExAyjNNkBm3hAF5gpYgPkphTOS8H+QwnhA81SHu5Cw5PYE34rKMznNI4A4Ujcg5RB8nKz51TQorcmQWUQT/FQadEcPotQdD0MADhpxlOIT7BFAhCDcUBIhc4TBhpw5BoyiifAZQgF0KIvBoo2nTK5DWVcoRBwj0Ktic1UTVg0Lkx1OM1lX41aBKILDBRTVUNGHSMRDWNfDHmpyn2SWOx8r2QZpU6QOaXib/NVHYhhbQOz1JJMwP1BJhVda+hG18AIo0gnEhQFPmTQtm5BoS+QIEoVxvgcWgAl72hnFAw8IChLACPYRRkkbeMUY0Wh1tHLaz+uKxf+1BaNI7A/pZCtqNNg8QukZxQfNOUYEG4QAG9SGIaCcMJXQCsAmAEd3Nnrp2bZy3uLElIitw4i/w2AHRiFgDXzZ9p9qpTi7LQlbu9HbREFIfjthVVUeVIY+qaSsMApRGjZS+6zF8VDPuFgtqokDYXQFCTNaFkNtfXD3l9SSIfEswIZA59J0dOsaLUKItbOvQYuRnkc2GAVjzF4eQoX4eAR04cqyowXwOJaoA1x0vV9Iz65Jz6YuakmEICpRcYrFA4MxnwwNOfOOEJpLHhTpitFcAFB5CsciHWa47+OFdgNLiAVXzKE4bnXMwh88AwTChKOGMmO+RCUUIJV81pJrlZGxwWnhbXgq9Mg03Oinc43htVrAZzTbLTJAlouZB+UF8FMlJZdWt0udtch3l+LDr91sD8XV7S5looaflPRkNQu5AZFnDlyHi4U+TgNiCEFG/m4XLR0u+Xu93qdfcqr8OKTZ+70ud2lze9tw3WrWxv9Xbb2Zq8yWB1tNXPTTdbvVw7uf6W/y0eljeVeW1DecstTWfaTQdnaTXtRG23CVRlleVkBAC/1fU6DejVFtF09bLjelHVOqo1x7WzGmvKeUsAmwqqkTupqBC68pJOdp16dq9+LU2rpbt0xUu6Yvtq29fVFHdqGusqrEtPVaFZzbxl03SypKkEqFSFHCVSHOnqEnHYfiTkAznmEfhe9LhGJSEvuY3T83lIpljXC+r3HSh84PBv86lKm9vP7T9r8RSJql2xy0z+GRTl6UImywnMoEmBFmNGOOWoP4bNmzZGynPUqlJ1JdwCfJc/BpkDMa8qNUWjrdMJpH/SsSkJnvxDiVT+7gPVmjZ1GyMGNVU/sGRlhqHQJkjfYw/0TyCpGlO4S740cDPR7YDnWjRN0+WBS40P3Y13QkRp+PLtMqOLDDoLdeDYlOyPB7oi1fo5VXfTgjjDLCMdGr0uFkxI9kOS0uA+K4t81VPmqSrf1cni5ULbQ7GBYTOjBQqCFqsLVDVv0OQNsDxPVtaTVPpVzI38EgVt6T/77EmHripULKrKovGgqAIAQtO+EudeB/PLMjWoI/4Tj8Wy37aDqC3YAfOVhgFGaJjCDoCMQ6faAfCs5TCdCnzokJFi6gJUv9PUti/fVbOGXS4tolrrweKt7JXkCOywOTejosFl5IPq9xuzeJJt5tQgruQp55qa6SF5GWK2kKaHSgw50QSAd2iuODHZq/shWQfK1gGSPZ2qZq4wMzgTS36sglfPNdfTQwYEA1TrvvuyxcuJBwTgzSBOQ5XVkGQtbU+XjogOE8iJdnTYTHBC4UBtBMP5cut/BuVdR/w9XRacijliWEDuIku5VN7lFapCgEPcvIqIze62lEhOtFaRFwQIRg3+TF2v2ADkjAUBhs2dFkeNbaH2nSFllmVF/srsBDjY5wtj8PwcnP3GGIwiWOCZyU6sh1+Y/7G9B9IUzzuCQGlkB/mmBaLzgh1EnmBEzDtcbZ4NinYZQB2BdkneLiEoYvR7cEgaw5EFgUPPyGKmW0xzgdJjasa+Bo1U5RbHzEBj8xTXPldv9rMkmYtZMzzK9wXx8YN+dY+Luh9eT4bjreHkm0n/8y/Ky94Pe7/p/ba30Rv3dnuf977q7fde9bzVv63+a/Xfq/9Z++va39e+W/uHJn3/vZLn417rs/bP/wF8rkD2</latexit>⇤ <latexit sha1_base64="/5Sh4i0wdoxGuuItsO4uLVyPqHg=">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</latexit>↵•<latexit sha1_base64="eKPyccKSMq17H9eNGT2oLLjVwRI=">AAAXAnicpVhLjyO3EZbtJLYnD69teC65ENEM4MSyLGl2vAsbBhwHCBwgWK9h7wOYHg/YLbZEDJvs4WNGcqNv+SO55hIEueaea3LMv0mR7Ja6KfZ6AQszErtY9ZH1YLGq05JRpWez/73y6ms/+enPXn/jzaOf/+KXv3rr3tvvPFXCyIw8yQQT8nmKFWGUkyeaakael5LgImXkWXr9Bzv/7JZIRQX/Vm9LclngFac5zbAG0tW9T6vEgVzIVXpZzabzB/fn52eT2XT24MHsYztYnN2fzR/WJwlm5RpfJcLoTBTkpD66ujcGPvdBh4N5MxiPms/jq7ff+3uyFJkpCNcZw0pdzGelvqyw1DRjpD5KjCIlzq7xilzAkOOCqMvKbbBGp0BZolxI+OcaOWpXosKFUtsiBc4C67UK5ywxNndhdP7wsqK8NJrwzC+UG4a0QNZkaEklyTTbwgBnksJeUbbGEmcaDHvUW0bpAsutXPZUqTS9/t5T7IjRVAITaC3FnZpgowXsCU+WJBPSuUVNS9hkIWS5pnw1gQdYiatJKRS185ao1rgkaupB+sttFIzAmvA5RbdYUmFAYU7uwG8F5ssquSGkrpJ1ThmrEgU6lVrR7wk6SVIGIOrGYAkeBlMgwpdI5IhscFEy0ochfAUhugYoTTZa66ol1H0+Z/I9l3NEwKIyCrYne6aWEPClgpsOlH8MeEqILDDRnqslBHyM8D2PfQjmVxIvSWex5rm2ZrU6EPiiWWDaJcnhKC5rax1lpOVZg3oazCoJWLtIxQYQKYdwInldV5/Vzs57QMyQA3GuDsBFEQDbb1lUdsLBwABTbj2GUW54dojRUuuLs8se1njuUgFH4wVKJF2ttQsuND5zyHG0VV7Gd2QnnNwK0pEmSqOcbkpBuQ6cMATAWgBG8LC0SePSyvSkTVkSiVJh+LIPwLGOAKRp9ciLw7yR4BFuitSe9n7QEl1fzPtWhKjKdWKN6UyYeBjgDGJU3AFcTN5+QwxUj2sH9X6L9Nsd0NFpDyqjMlM3jNzUjfYuzGBQYkYgcyTqRpmU3GDH6VEsBaDIDfoApYYBXwDaytQXi8vqFAIeJUJgRle8OoEd7QFOkky66TVdkju61OtEYgoJlG4wWKFO1jbgQWa8SIprSGPT88Kc1CBVH53Cnz0we4nxvHJgNN/AKkuqSoa3Sm8h8wAZJhwnXEqLc7JxnISpds4L2cPakYjI9KR2ck0a7Eq2stP5w1krGgjvWc67LDltFvID91OjIJUZ9jJuSxOWLYUe9FsH83dVw1v5TVnLfzibgtq1zbCAaynz6XldgduAEVJ8mIebRRu/v9jtUa+nP+R1WLHr89T6PO7yrvfug3Vb20e93Xe2Z+8KRB0d9XPXzVEv7528/7XfEQ9nxqa3nQ0TeLamC+3mg7Oxmneit9tiuthZzkYAyEdd79OAX20XTT+87Hy/qJX3a873zuqsaecjARwq6Cg/SkWHMJSXfLIb1HN49ZfStF16SFd8oCuOr3b/ZTXFg5oKX4UN6ekqtKiZz2KaLg40tQCtqpCjtMTcV5dIwfEjhZpYWkbgV8HVxA9LHahRMAtuz68KssK+XrCzCAofuPz7cq7SVvF7+09+e47F1a4YOplAfg1FudztKXIDsxJ041BPKarQeA6HV3YozT0aVanAG1qESn1XfQB7zvW2rdQcj7fOIBCRK1+DHigJnvy0QfJMtlrzpu5jCFDT9QMHVmYYCm2Ckm/oqsCTRGNjE5SlOdwDXwa4Rg874Cu/Nc8z5IEXGh+6m+ya6MbwzdOLjK4NdBbuwokpOZ5PfEXq9Uva7qYHcYuZIQMaPa13Qsj2Q5YzkL5tinzXU1bSle/uZskq7e3hxMCwJmiB8rwnmgLXXjbvyubY3idHp6W0ftXbIL/wvL/7Tz75bEBXFyoRVW3R+G3dBgCEZnwlpbIB4W+a1OCu+A8zJmy/HQdxR3AA5ksPA4LQMBUDAEZBpzoA8KjnMJ8KltAhIyc0BMjt64G97Ztn16zhVFmLuNZ6sntqeiVLgRO2VWFUdKSCfJA0XUFYPNk2cxUwt/tp5rqaeZJ9GRK2kKGHGgw70QWAZ2iuFAnFS2jaoXne2jrQtg6Q7OnKNXN1mMGZPvBjG7x+rrueJwUQDFCj5+6PPVlFMmAAb+ZCFi6rISva2J4eXBEDJrAT/eiImeCawoXaCYa7w9b/Fsq7gfj78+HGqd4ihjXkLnKQS+3bsNpVCHCJh68iRNjdNjuyE71V7AsCBNRA3rjXKzEAOxNBAHJ40gTvHAt37oJdGmPq6knYCSiwzxcB8e4OnP0sIHIOCzwKxUn08iuq3/fPgJR4OxAETqM4yNc9EJ8X4iD2BiN6O+Dq8G5wvIcA7gqM7+T5AYJjRh+BQ6SAKwsCh96S3czwNsMFGo+5mfgalLvKTQgWoLGtxHufu6f4XVJu9bobHs3zjvnq3ngevtA9HDxdTOdn08XXi/HnXzQve98Y/Xr0m9H7o/nowejz0Zejx6Mno2z019G/R/8Z/ff4L8d/O/7H8T8966uvNDLvjnqf43/9H/XmnKE=</latexit>↵•<latexit sha1_base64="eKPyccKSMq17H9eNGT2oLLjVwRI=">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</latexit>↵ <latexit sha1_base64="mU7xeOBAzmXzDyjlN9pIVkOwnhE=">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</latexit>↵ <latexit sha1_base64="mU7xeOBAzmXzDyjlN9pIVkOwnhE=">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</latexit>↵I<latexit sha1_base64="3NafXZMjSvWD/51unEp2oE1xC2I=">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</latexit>↵I<latexit sha1_base64="3NafXZMjSvWD/51unEp2oE1xC2I=">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</latexit>↵I<latexit sha1_base64="3NafXZMjSvWD/51unEp2oE1xC2I=">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</latexit>1:12

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Proof. The proofs follows trivially from the definition of ⊆·I (cf. Equation 6 and 8 and 10) and
□

Lemma 6.1 and 8.1.

Finally, from Lemma 8.1, we have that we can equivalently verify whether {|M|}I
⇝

⊆ αI(α⇝

(FK[Y ]))

by checking if the parallel semantics {|M|}I
⇝

induces a partition of each I

.

K

Lemma 8.4. M |= FK[Y ] ⇔ ∀I ∈ I : ∀A, B ∈ {|M|}I
⇝

: (AI
ω

Proof. The proof follows trivially from Lemma 8.1.

(cid:44) BI

ω ⇒ AI

0 K ∩ BI
0 K

= ∅)

□

.

⇝

⇝

of ΠI

, i.e., ΠI

9 PARALLEL CAUSAL-FAIRNESS ANALYSIS
In this section, we build on the parallel semantics to design our novel perfectly parallel static
analysis for causal fairness, which automatically finds a fair partition I and computes a sound
over-approximation ΠI♮
⇝

⇝
ReLU Activation Functions. We again only consider ReLU activation functions for now and
postpone the discussion of other activation functions to the end of the section. The analysis is
described in Algorithm 2. It combines a forward pre-analysis (Lines 15-24) with a backward analysis
(Lines 28-38). The forward pre-analysis uses an abstract domain A1 and builds partition I, while the
backward analysis uses an abstract domain A2 and performs the actual causal-fairness analysis of a
neural-network model M with respect to its sensitive input nodes K and a (representation of a) set
of initial states Y (cf. Line 13).

⊆·I ΠI♮

More specifically, the forward pre-analysis bounds the number of paths that the backward
analysis has to explore. Indeed, not all of the 2 |M| paths of a model M are necessarily viable starting
from its input space.

In the rest of this section, we represent each path by an activation pattern, which determines the
activation status of every ReLU operation in M. More precisely, an activation pattern is a sequence
of flags. Each flag pi, j represents the activation status of the ReLU operation used to compute the
value of hidden node xi, j . If pi, j is xi, j , the ReLU is always active, otherwise the ReLU is always
inactive and pi, j is xi, j .

An abstract activation pattern gives the activation status of only a subset of the ReLUs of M, and
thus, represents a set of activation patterns. ReLUs whose corresponding flag does not appear in
an abstract activation pattern have an unknown (i.e., not fixed) activation status. Typically, only a
relatively small number of abstract activation patterns is sufficient for covering the entire input space
of a neural-network model. The design of our analysis builds on this key observation.

We set an analysis budget by providing an upper bound U (cf. Line 13) on the number of tolerated
ReLUs with an unknown activation status for each element I of I, i.e., on the number of paths that
are to be explored by the backward analysis in each I. The forward pre-analysis starts with the
trivial partition I = {Y } (cf. Line 15). It proceeds forward for each element I in I (cf. Lines 17-18).
The transfer function −−−→relup
considers a ReLU operation and additionally builds an abstract
A
activation pattern p for I (cf. Line 5) starting from the empty pattern ϵ (cf. Line 2).

xi, j

If I leads to a unique outcome (cf. Line 19), then causal fairness is already proved for I, and there
is no need for a backward analysis; I is added to the set of completed partitions (cf. Line 20). Instead,
if abstract activation pattern p fixes the activation status of enough ReLUs (cf. Line 21), we say that
the backward analysis for I is feasible. In this case, the pair of p and I is inserted into a map F from
abstract activation patterns to feasible partitions (cf. Line 22). The insertion takes care of merging
abstract activation patterns that are subsumed by other (more) abstract patterns. In other words, it
groups partitions whose abstract activation patterns fix more ReLUs with partitions whose patterns
fix fewer ReLUs, and therefore, represent a superset of (concrete) patterns.

(cid:75)

(cid:74)

Perfectly Parallel Fairness Certification of Neural Networks

1:13

Algorithm 2 : Our Analysis Based on Activation Patterns

(−−−−−→assignA

xi, j

a)

(cid:75)

(cid:74)

1: function forward(M, A, I)
a, p ← assumeA
2:
for i ← 1 up to n do
3:
4:

I
(cid:74)
for j ← 0 up to |li | do
a, p ← −−−→relup

(newA), ϵ

5:

(cid:75)

xi, j

A

(cid:74)

(cid:75)

return a, p

6:
7: function backward(M, A, O, p)
8:

O
(cid:75)

a ← outcomeA
(newA)
for i ← n − 1 down to 0 do

(cid:74)
for j ← |li | down to 0 do
a ← ←−−−−−
assignA

9:
10:

11:

xi, j
(cid:74)

(←−−−relup
A
(cid:75)

(cid:74)

xi, j

return a

12:
13: function analyze(M, K, Y , A1, A2, L, U)
14:
15:

F, E, C ← (cid:219)∅, (cid:219)∅, ∅
I ← {Y }
while I (cid:44) ∅ do
I ← I.get()
a, p ← forward(M, A1, I)
if uniqely-classified(a) then

C ← C ∪ {I}

else if |M| − |p| ≤ U then
F ← F ⊎ {p (cid:55)→ I}
else if |I| ≤ L then
E ← E ⊎ {p (cid:55)→ I}

else

I ← I ∪ partitionK(I)

B ← ∅
for all p, I ∈ F do

O ← (cid:219)∅
for j ← 0 up to |ln| do

a ← backward(M, A2, xn, j , p)
O ← O ∪ (cid:8)xn, j (cid:55)→ a(cid:9)

16:
17:
18:
19:
20:
21:

22:
23:
24:
25:
26:

27:
28:
29:

30:
31:

32:

33:
34:
35:

36:

37:
38:

39:

a)

(cid:75)

▷ F: feasible, E: excluded, C: completed

▷ perfectly parallelizable

▷ I is already fair

▷ I is feasible

▷ I is excluded

▷ I must be partitioned further

▷ B: biased
▷ perfectly parallelizable

for all I ∈ I do
O’ ← (cid:219)∅
for all o, a ∈ O do
O’ ← O’ ∪

(cid:110)o (cid:55)→ (assumeA2

B ← B ∪ check(O’)
C ← C ∪ {I}
return C, B = ∅, B, E

(cid:111)

a) K

I

(cid:74)

(cid:75)

▷ fair: B = ∅, maybe biased: B (cid:44) ∅

Otherwise, I needs to be partitioned further, with respect to K (cf. Line 25). Partitioning may
continue until the size of I is smaller than the given lower bound L (cf. Lines 13 and 23). At this

1:14

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

point, I is set aside and excluded from the analysis until more resources (a larger upper bound U or
a smaller lower bound L) become available (cf. Line 24).

Note that the forward pre-analysis lends itself to choosing a relatively cheap abstract domain
A1 since it does not need to precisely handle polyhedral constraints (like max Xn = x, needed to
partition with respect to outcome, cf. Section 7).

The analysis then proceeds backwards, independently for each abstract activation path p and
associated group of partitions I (cf. Lines 28 and 31). The transfer function ←−−−relup
uses p to
A
choose which path(s) to explore at each ReLU operation, i.e., only the active (resp. inactive) path if
xi, j (resp. xi, j ) appears in p, or both if the activation status of the ReLU corresponding to hidden
node xi, j is unknown. The (as we have seen, necessarily) expensive backward analysis only needs
to run for each abstract activation pattern in the feasible map F . This is also why it is advantageous
to merge subsumed abstract activation paths as described above.

xi, j
(cid:74)

(cid:75)

Finally, the analysis checks causal fairness of each element I associated to p (cf. Line 37). The
analysis returns the set of input-space regions C that have been completed and a set B of abstract-
domain elements over-approximating the regions in which bias might occur (cf. Line 39). If B is
empty, then the given model M satisfies causal fairness with respect to K and Y over C.

Theorem 9.1. If function analyze(M, K, Y , A1, A2, L, U) in Algorithm 2 returns C, true, ∅, then
M satisfies FK[Y ] over the input-space fraction C.

Proof (Sketch). analyze(M, K, Y , A1, A2, L, U) in Algorithm 2 first computes the abstract activa-
tion patterns that cover a fraction C of the input space in which the analysis is feasible (Lines 15-24).
Then, it computes an over-approximation a of the regions of C that yield each target class xn, j (cf.
of the parallel semantics {|M|}I
Line 31). Thus, it actually computes an over-approximation {|M|}I♮
,
⇝
⇝
⊆· {|M|}I♮
i.e., {|M|}I
ω ⇒
⇝
AI
0 K ∩ BI
= ∅) (according to Lemma 8.4, cf. Lines 33-37), then by transitivity we can conclude
0 K
that also {|M|}I♮
□
⇝

satisfies FK[Y ], i.e., ∀I ∈ I : ∀A, B ∈ {|M|}I♮
⇝

necessarily satisfies FK[Y ].

. Thus, if {|M|}I♮
⇝

: (AI
ω

(cid:44) BI

⇝

Remark. Recall that we assumed neural-network nodes to have real values (cf. Section 4). Thus,
Theorem 9.1 is true for all choices of classical numerical abstract domains [Cousot and Cousot
1976; Cousot and Halbwachs 1978; Ghorbal et al. 2009; Miné 2006b, etc.] for A1 and A2. If we were
to consider floating-point values instead, the only sound choices would be floating-point abstract
domains [Chen et al. 2008; Miné 2004; Singh et al. 2019].

Other Activation Functions. Let us discuss how activation functions other than ReLUs would
(cf.
(cf. Line 11), which would have to be replaced with the transfer functions

be handled. The only difference in Algorithm 2 would be the transfer functions −−−→relup
A
Line 5) and ←−−−relup
A
corresponding to the considered activation function.

xi, j

xi, j

(cid:74)

(cid:75)

(cid:75)

(cid:74)

Piecewise-linear activation functions, like Leaky ReLU(x) = max(x, k · x) or Hard TanH(x) =
max(−1, min(x, 1)), can be treated analogously to ReLUs. The case of Leaky ReLUs is trivial. For
Hard TanHs, the patterns p used in Algorithm 2 will consist of flags pi, j with three possible values,
depending on whether the corresponding hidden node xi, j has value less than or equal to −1,
greater than or equal to 1, or between −1 and 1. For these activation functions, our approach
remains sound and, in practice, exact when using disjunctive polyhedra for the backward analysis.
1+e −x , can be soundly over-approximated [Singh
et al. 2019] and similarly treated in a piecewise manner. In this case, however, we necessarily lose
the exactness of the analysis, even when using disjunctive polyhedra.

Other activation functions, e.g., Sigmoid(x) = 1

Perfectly Parallel Fairness Certification of Neural Networks

1:15

10 IMPLEMENTATION
We implemented our causal-fairness analysis described in the previous section in a tool called
libra. The implementation is written in python and is open source3.

Tool Inputs. libra takes as input a neural-network model M expressed as a python program (cf.
Section 3), a specification of the input layer l0 of M, an abstract domain for the forward pre-analysis,
and budget constraints L and U. The specification for l0 determines which input nodes correspond
to continuous and (one-hot encoded) categorical data and, among them, which should be considered
bias sensitive. We assume that continuous data is in the range [0, 1]. A set Y of initial states of
interest is specified using an assumption at the beginning of the program representation of M.

Abstract Domains. For the forward pre-analysis, choices of the abstract domain are either
boxes [Cousot and Cousot 1976] (i.e., boxes in the following), or a combination of boxes and
symbolic constant propagation [Li et al. 2019; Miné 2006a] (i.e., symbolic in the following), or
the deeppoly domain [Singh et al. 2019], which is designed for proving local robustness of neural
networks. As previously mentioned, we use disjunctive polyhedra for the backward analysis. All
abstract domains are built on top of the apron abstract-domain library [Jeannet and Miné 2009].

Parallelization. Both the forward and backward analyses are parallelized to run on multiple
CPU cores. The pre-analysis uses a queue from which each process draws a fraction I of Y (cf.
Line 17). Fractions that need to be partitioned further are split in half along one of the non-sensitive
dimensions (in a round-robin fashion), and the resulting (sub)fractions are put back into the queue
(cf. Line 26). Feasible Is (with their corresponding abstract activation pattern p) are put into another
queue (cf. Line 22) for the backward analysis.

Tool Outputs. The analysis returns the fractions of Y that were analyzed and any (sub)regions of
these where bias was found. It also reports the percentage of the input space that was analyzed and
(an estimate of) the percentage that was found biased according to a given probability distribution
of the input space (uniform by default). To obtain the latter, we simply use the size of a box wrapped
around each biased region. More precise but also costlier solutions exist [Barvinok 1994].

11 EXPERIMENTAL EVALUATION
In this section, we evaluate our approach by focusing on the following research questions:

RQ1: Can our analysis detect seeded (i.e., injected) bias?
RQ2: Is our analysis able to answer specific bias queries?
RQ3: How does the model structure affect the scalability of the analysis?
RQ4: How does the analyzed input-space size affect the scalability of the analysis?
RQ5: How does the analysis budget affect the scalability-vs-precision tradeoff?
RQ6: Can our analysis effectively leverage multiple CPUs?

11.1 Data
For our evaluation, we used public datasets from the UCI Machine Learning Repository and
ProPublica (see below for more details) to train several neural-network models. We primarily
focused on datasets discussed in the literature [Mehrabi et al. 2019] or used by related techniques
(e.g., [Albarghouthi et al. 2017a,b; Albarghouthi and Vinitsky 2019; Bastani et al. 2019; Datta et al.
2017; Galhotra et al. 2017; Tramèr et al. 2017; Udeshi et al. 2018]).

We pre-processed these datasets both to make them fair with respect to a certain sensitive input
feature as well as to seed bias. We describe how we seeded bias in each particular dataset later on.

3https://github.com/caterinaurban/Libra

1:16

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 1. Analysis of Models Trained on Fair and {Age, Credit > 1000}-Biased Data (German Credit Data)

boxes

symbolic

deeppoly

credit

fair data

≤ 1000

> 1000

time
bias
47s
0.09%
5m 46s
0.19%
30m 59s
0.33%
1m 42s
2.21%
31m 42s
6.72%
14.96% 7h 7m 12s

biased data
time
bias
2m 17s
0.09%
13m 2s
0.45%
1h 56m 57s
0.95%
21m 11s
4.52%
1h 36m 51s
23.41%
33.19% 16h 50m 48s

fair data

time
13s

bias
0.09%
0.19%
0.33%
2.21%
6.72%
14.96% 4h 16m 52s

1m 5s
4m 8s
38s
8m 59s

biased data
time
1m 10s
2m 41s

bias
0.09%
0.45%
0.95%
13m 16s
4.52%
3m 7s
41m 44s
23.41%
33.19% 8h 5m 14s

fair data

bias
0.09%
0.19%
0.33%
2.21%
6.63%
14.96%

time

10s
1m 12s
5m 45s
39s

4m 58s
1h 9m 45s

bias
0.09%
0.45%
0.95%
4.52%
23.41%
31.17%

biased data
time

39s
1m 46s
18m 18s
4m 44s

15m 39s
6h 51m 50s

min
median
max
min
median
max

Our methodology for making the data fair was common across datasets. In particular, given
an original dataset and a sensitive feature (say, race), we selected the largest population with
a particular value for this feature (say, Caucasian) from the dataset (and discarded all others).
We removed any duplicate or inconsistent entries from this population. We then duplicated the
population for every other value of the sensitive feature (say, Asian and Hispanic). For example,
assuming the largest population was 500 Caucasians, we created 500 Asians and 500 Hispanics,
and any two of these populations differ only in the value of race. Consequently, the new dataset
is causally fair because there do not exist two inputs k and k ′ that differ only in the value of the
sensitive feature for which the classification outcomes are different.

We define the causal-unfairness score of a dataset as the percentage of inputs k in the dataset for
which there exists another input k ′ that differs from k only in the value of the sensitive feature and
the classification outcome. Our fair datasets have an unfairness score of 0%.
All datasets used in our experiments are open source as part of libra.

11.2 Setup
Since neural-network training is non-deterministic, we typically train eight neural networks on
each dataset, unless stated otherwise. The model sizes range from 2 hidden layers with 5 nodes
each to 32 hidden layers with 40 nodes each. All models used in our experiments are open source
as part of libra. For each model, we assume a uniform distribution of the input space.

We performed all experiments on a 12-core Intel ® Xeon ® X5650 CPU @ 2.67GHz machine

with 48GB of memory, running Debian GNU/Linux 9.6 (stretch).

11.3 Results
In the following, we present our experimental results for each of the above research questions.

RQ1: Detecting Seeded Bias. This research question focuses on detecting seeded bias by com-

paring the analysis results for models trained with fair versus biased data.

For this experiment, we used the German Credit dataset4. This dataset classifies creditworthiness
into two categories, “good” and “bad”. An input feature is age, which we consider sensitive to
bias. (Recall that this could also be an input feature that the user considers indirectly sensitive
to bias.) We seeded bias in the fair dataset by randomly assigning a bad credit score to people
of age 60 and above who request a credit amount of more than EUR 1 000 until we reached a
20% causal-unfairness score of the dataset. The median classification accuracy of the models (17
inputs and 4 hidden layers with 5 nodes each) trained on fair and biased data was 71% and 65%,
respectively. Note that accuracy does not improve by adding more layers or nodes per layer — we
tried up to 100 hidden layers with 100 nodes each.

To analyze these models, we set L = 0 to be sure to complete the analysis on 100% of the input
space. The drawback with this is that the pre-analysis might end up splitting input partitions

4https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)

Perfectly Parallel Fairness Certification of Neural Networks

1:17

Table 2. Queries on Models Trained on Fair and Race-Biased Data (ProPublica’s compas Data)

boxes

symbolic

deeppoly

qery

fair data

age < 25
race bias?

male
age bias?

caucasian
priors bias?

bias
0.22%
0.31%
2.46%
2.60%
6.08%
8.00%
2.18%
2.95%
5.36%

time
24m 32s
1h 54m 48s
2h 44m 11s
24m 14s
1h 49m 42s
5h 56m 6s
2h 54m 18s
6h 56m 44s

45h 2m 12s

biased data
time
14m 53s
57m 33s
5h 29m 19s
34m 23s
2h 3m 39s
8h 26m 55s
46m 53s
3h 50m 38s
70h 50m 10s

bias
0.12%
0.99%
8.33%
4.51%
6.95%
12.56%
2.92%
4.21%
6.98%

fair data

time
11m 34s

36m 0s
2h 17m 3s
25m 13s

bias
0.22%
0.32%
2.46%
2.64%
6.77%
1h 1m 51s
8.40%
2h 2m 22s
2.18% 1h 20m 41s
2.95% 4h 12m 28s
5.36% 60h 53m 6s

bias
0.12%
0.99%
8.50%
5.20%
7.02%
12.71%
2.92%
4.21%
6.98%

7m 14s
20m 43s
3h 34m 50s
29m 19s
1h 2m 26s
4h 55m 35s
30m 23s
3h 32m 52s
49h 51m 42s

biased data
time

fair data

bias
0.22%
0.32%
2.12%
2.70%
6.77%
8.84%
2.18%
2.95%
5.36%

time

5m 18s
47m 16s

1h 11m 43s
19m 47s
1h 13m 31s
2h 20m 23s

18m 26s
2h 36m 1s
52h 10m 2s

bias
0.12%
0.99%
6.48%
5.22%
7.00%
12.88%
2.92%
4.21%
6.95%

biased data
time
8m 46s

16m 38s
2h 5m 5s
20m 51s
47m 28s
3h 25m 21s
15m 29s
1h 34m 7s
17h 48m 22s

min
median
max
min
median
max
min
median
max

endlessly. To counteract, for each model, we chose the smallest upper bound U that did not cause
this issue. Table 1 shows the analysis results for the different choices of domain used for the forward
pre-analysis. In particular, it shows whether the models are biased with respect to age for credit
requests of 1 000 or less as well as for credit requests of over 1 000. Columns bias and time show
the detected bias (in percentage of the entire input space) and the analysis running time. We show
minimum, median, and maximum bias percentage and running time for each credit request group.
For each line in Table 1, we highlighted the choice of the abstract domain that entailed the shortest
analysis time. The analysis results for all models are shown in the appendix (cf. Tables 7-9).

For all models, the analysis finds little bias for small credit amounts, as intended. Instead, for
large credit amounts, the analysis finds significantly more bias (i.e., about three times as much
median bias) for the models trained on biased data in comparison to models trained on fair data.
This demonstrates that our approach is able to effectively detect seeded bias.

For the models trained on fair data, we observe a maybe unexpected difference in the bias found
for small credit amounts compared to larger amounts. This is in part due to the fact that bias is
given in percentage of the entire input space and not scaled with respect to the analyzed input
space. When considering the analyzed input space (small credit amounts correspond to a mere 4% of
the input space), the difference is less marked: the median bias is 0.19% / 4% = 4.75% for small credit
amounts and 6.72% / 96% = 7% (or 6.63% / 96% = 6.9% for the deeppoly domain) for large credit
amounts. The remaining difference indicates that the models contain bias that does not necessarily
depend on the credit amount. The bias is introduced by the training process itself (as explained in
the Introduction) and is not due to imprecision of our analysis. Recall that our approach is exact,
and imprecision is only introduced when estimating the bias percentage (cf. Section 10).

RQ2: Answering Bias Queries. To further evaluate the precision of our approach, we created
queries concerning bias within specific groups of people, each corresponding to a subset of the entire
input space. We used the compas dataset5 from ProPublica for this experiment. The data assigns a
three-valued recidivism-risk score (high, medium, and low) indicating how likely criminals are to
re-offend. The data includes both personal attributes (e.g., age and race) as well as criminal history
(e.g., number of priors and violent crimes). As for RQ1, we trained models both on fair and biased
data. Here, we considered race as the sensitive feature. We seeded bias in the fair data by randomly
assigning high recidivism risk to African Americans until we reached a 20% causal-unfairness score
of the dataset. The median classification accuracy of the 3-class models (19 inputs and 4 hidden
layers with 5 nodes each) trained on fair and biased data was 55% and 56%, respectively. Accuracy
does not improve with larger networks — we tried up to 100 hidden layers with 100 nodes each.

To analyze these models, we used a lower bound L of 0, and an upper bound U between 7 and 19.

Table 2 shows the results of our analysis (i.e., columns shown as in Table 1) for three queries:

5https://www.propublica.org/datastore/dataset/compas-recidivism-risk-score-data-and-analysis

1:18

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

QA: Is there bias with respect to race for people younger than 25?
QB : Is there bias with respect to age for males?
QC : Is there bias with respect to the number of priors for Caucasians?

For QA, the analysis detects only a small percentage of race bias in the fair models, but as intended,
the race bias is found to be significantly higher (about three times as much median bias) for the
biased models. In contrast, for QB , the analysis finds a comparable amount of age bias across
both sets of models. This becomes more evident when scaling the median bias with respect to the
analyzed input space (males correspond to 50% of the input space): the smallest median bias for the
models trained on fair data is 12.16% (for the boxes domain) and the largest median bias for the
models trained on biased data is 14.04% (for the symbolic domain). This bias is not intended and
was either present in the original data or introduced by the training process (or both). Finally, for
QC , the analysis detects significant bias across both sets of models with respect to the number of
priors. When considering the analyzed input space (Caucasians represent 1/6 of the entire input
space), this translates to 17.7% median bias for the models trained on fair data and 25.26% for the
models trained on biased data. This bias is intended and present in the original data: as one would
expect, recidivism risk differs for different numbers of priors. Overall, these results demonstrate the
effectiveness of our analysis in answering specific bias queries.

For each line in Table 2, we highlighted the choice of abstract domain that entailed the shortest
analysis time. We observe that deeppoly seems generally the better choice. The difference in
performance becomes more striking as the analyzed input space becomes smaller, i.e., for QC . This
is because deeppoly is specifically designed for proving local robustness of neural networks. Thus,
our input partitioning, in addition to allowing for parallelism, is also enabling analyses designed
for local properties to prove global properties, like causal fairness.

The analysis results for all models are shown in the appendix (see Tables 10, 11, and 12).

RQ3: Effect of Model Structure on Scalability. To evaluate the effect of the model structure
on the scalability of our analysis, we trained models on the Adult Census dataset6 by varying
the number of layers and nodes per layer. The dataset assigns a yearly income (> or ≤ USD 50K)
based on personal attributes such as gender, race, and occupation. We trained all models (with 23
inputs) on a fair dataset with respect to gender and ensured that each model reached a minimum
classification accuracy of 78%. Accuracy does not increase by adding more layers or nodes per layer,
in fact, it may significantly decrease — we tried up to 100 hidden layers with 100 nodes each.

Table 3 shows the results. The first column (|M|) shows the total number of hidden nodes and
introduces the marker symbols used in the scatter plot of Figure 3 (to identify the domain used for
the forward pre-analysis: left, center, and right symbols respectively refer to the boxes, symbolic,
and deeppoly domains). The models have the following number of hidden layers and nodes per
layer (from top to bottom): 2 and 5; 4 and 3; 4 and 5; 4 and 10; 9 and 5.

Column U shows the chosen upper bound for the analysis. For each model, we tried four different
choices of U. Column input shows the input-space coverage, i.e., the percentage of the input space
that was completed by the analysis. Column |C| shows the total number of analyzed (i.e., completed)
input space partitions. Column |F| shows the total number of abstract activation patterns (left) and
feasible input partitions (right) that the backward analysis had to explore. The difference between
|C| and the number of partitions shown in |F| are the input partitions that the pre-analysis found
to be already fair (i.e., uniquely classified). Finally, column time shows the analysis running time.
We used a lower bound L of 0.5 and a time limit of 13h. For each model in Table 3, we highlighted

6https://archive.ics.uci.edu/ml/datasets/adult

Perfectly Parallel Fairness Certification of Neural Networks

1:19

Table 3. Comparison of Different Model Structures (Adult Census Data)

|M|

10

12

20

40

45

U

4
6
8
10
4
6
8
10
4
6
8
10
4
6
8
10
4
6
8
10

boxes
|F|
1136
723
143

77
51
19

1
9
15
9
7
18
62
96
71
0
0
1
10
0
3
25
33

1
329
929
284
260
39
255
792
1339
0
0
2
13
0
22
234
488

|C|
1482
769
152

1
719
1197
342
313
1044
1123
1111
1390
10
10
12
23
50
72
282
522

input
88.26%
99.51%
100.00%
100.00%
49.83%
72.74%
98.68%
99.06%
38.92%
46.22%
64.24%
85.90%
0.35%
0.35%
0.42%
0.80%
1.74%
2.50%
9.83%
18.68%

time
33m 55s
1h 10m 25s
3h 47m 23s

55m 58s
13m 43s
2h 6m 49s
1h 46m 43s
1h 21m 47s
2m 6s
20m 51s
2h 24m 51s
>13h
1m 39s
1m 38s
14m 37s
1h 48m 43s
1m 38s
4m 35s
25m 30s
1h 51m 24s

input
95.14%
99.93%
100.00%
100.00%
72.29%
98.54%
98.78%
99.06%
51.01%
61.60%
74.27%
89.27%
34.62%
34.76%
35.56%
37.19%
41.98%
45.00%
47.78%
49.62%

|C|
1132
578
174
1
1177
333
323

307
933
916
1125
1435
768
817
840

880
891
822
651

714

symbolic
|F|

686
447
146
1
559
195
190

182
92
405
780
1157
1
5
28

75
49
143
229

65
47
18
1
11
7
9

5
31
67
78
60
1
4
21

50
14
32
46

51

time
19m 5s
39m 8s
1h 51m 2s
56m 8s
24m 9s
20m 46s
1h 27m 18s

1h 13m 55s
15m 28s
44m 40s
3h 26m 20s
>13h
6m 56s
43m 53s
2h 48m 15s

11h 32m 21s
10m 14s
45m 42s
1h 14m 5s

deeppoly

|F|

77
54
26
1
14
17
18
18
34
90
127

148
2
8
32
83
6
25
74
110

992
1042
824
1
423
594
724
1007
79
356
652

839
3
10
42
121
8
50
180
373

|C|
1894
1620
1170
1
1498
1653
1764
1639
1081
1335
1574

1711
648
592
686
699
805
847
975
1087

input
93.99%
99.83%
100.00%
100.00%
60.52%
66.46%
70.87%
80.76%
49.62%
59.20%
69.69%
76.25%
26.39%
26.74%
27.74%
30.56%
36.60%
38.06%
42.53%
48.68%

time
29m 55s
1h 24m 24s
8h 2m 27s
56m 43s
10m 32s
15m 44s
2h 19m 11s
3h 22m 11s
3m 2s
22m 13s
5h 6m 7s

4h 36m 23s
10m 11s
1h 23m 11s
2h 43m 2s
>13h
2m 47s
5m 7s
25m 1s
1h 58m 34s

294

3h 23m 20s

i

e
m
T
s
i
s
y
l
a
n
A

10,000s

1,000s

100s

i

e
m
T
s
i
s
y
l
a
n
A

10,000s

0

10 20 30 40 50 60 70 80 90 100

0

10 20 30 40 50 60 70 80 90 100

Analyzed Input Space

Analyzed Input Space

(a)

(b) Zoom on Best U-Configurations

Fig. 3. Comparison of Different Model Structures (Adult Census Data)

the configuration (i.e., domain used for the pre-analysis and chosen U) that achieved the highest
input-space coverage (the analysis running time being decisive in case of equality or timeout).

The scatter plot of Figure 3a visualizes the input coverage and analysis running time. We zoom

in on the best U-configurations for each pre-analysis domain (i.e., the chosen U) in Figure 3b.

Overall, we observe that coverage decreases for larger model structures, and the more precise
symbolic and deeppoly domains result in a significant coverage boost, especially for larger struc-
tures. We also note that, as in this case we are analyzing the entire input space, deeppoly generally
performs worse than the symbolic domain. In particular, for larger structures, the symbolic domain
often yields a higher input coverage in a shorter analysis running time. Finally, we observe that
increasing the upper bound U tends to increase coverage independently of the specific model structure.
However, interestingly, this does not always come at the expense of an increased running time. In
fact, such a change often results in decreasing the number of partitions that the expensive backward
analysis needs to analyze (cf. columns |F|) and, in turn, this reduces the overall running time.

1:20

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 4. Comparison of Different Input Space Sizes and Model Structures (Adult Census Data)

|M| qery

20

80

320

1280

F
0.009%
E
0.104%
D
1.042%
C
8.333%
B
50%
A
100%
F
0.009%
E
0.104%
D
1.042%
C
8.333%
B
50%
A
100%
F
0.009%
E
0.104%
D
1.042%
C
8.333%
B
50%
A
100%
F
0.009%
E
0.104%
D
1.042%
C
8.333%
B
50%
A
100%

input
100.000%
0.009%
99.996%
0.104%
99.978%
1.042%
99.696%
8.308%
97.318%
48.659%
94.032%
94.032%
99.931%
0.009%
99.583%
0.104%
97.917%
1.020%
83.503%
6.958%
25.634%
12.817%
0.052%
0.052%
99.931%
0.009%
99.583%
0.104%
97.917%
1.020%
83.333%
6.944%
25.000%
12.500%
0.000%
0.000%
99.931%
0.009%
99.583%
0.104%
97.917%
1.020%
83.333%
6.944%

−

−

boxes
|F|

2

9

3

39

|C|

9

83

time

3m 3s

3m 13s

457

13

176

5m

3173

20

1211

36m 12s

15415

61

5646

1h 39m 36s

18642

70

8700

2h 30m 46s

11

61

151

506

0

0

0

2

0

0

0

3

3m 5s

3m 6s

2m 56s

2h 1m

5516

7

11

1h 28m 6s

12

6

121

151

120

5744

0

11

31

151

481

−

−

0

0

0

0

0

0

0

0

0

0

0

−

−

0

0

0

0

0

0

0

0

0

0

0

−

−

25m 51s

3m 15s

3m 39s

6m 18s

30m 37s

2h 24m 36s

2h 54m 25s

7m 35s

15m 49s

1h 49s

7h 11m 39s

>13h

>13h

input
100.000%
0.009%
100.000%
0.104%
100.000%
1.042%
100.000%
8.333%
99.991%
49.996%
99.935%
99.935%
99.961%
0.009%
99.783%
0.104%
99.258%
1.034%
95.482%
7.956%
76.563%
38.281%
61.385%
61.385%
99.944%
0.009%
99.627%
0.104%
98.247%
1.024%
88.294%
7.358%
46.063%
23.032%
24.258%
24.258%
99.948%
0.009%
99.674%
0.104%
98.668%
1.028%

−

−

−

symbolic
|F|

1

3

9

|C|

5

26

292

time

3m 5s

3m 8s

2

9

63

4m 50s

2668

13

417

17m 40s

12617

34

2112

1h 1m 19s

15445

40

3481

1h 29m

17

89

297

0

0

0

0

0

0

3m 2s

3m 10s

3m 41s

885

25

34

>13h

4917

123

182

>13h

5156

73

102

10h 25m 2s

9

120

597

755

4676

2436

10

71

557

−

−

−

0

0

0

0

0

0

0

0

0

−

−

−

0

0

0

0

0

0

0

0

0

−

−

−

3m 35s

6m 34s

21m 9s

1h 36m 35s

7h 25m 57s

9h 41m 36s

24m 42s

51m 52s

3h 31m 45s

>13h

>13h

>13h

input
100.000%
0.009%
100.000%
0.104%
100.000%
1.042%
100.000%
8.333%
99.978%
49.989%
99.896%
99.896%
99.957%
0.009%
99.753%
0.104%
98.984%
1.031%
93.225%
7.768%
63.906%
31.953%
43.698%
43.698%
99.931%
0.009%
99.583%
0.104%
97.917%
1.020%
83.342%
6.945%
25.074%
12.537%
0.017%
0.017%
99.931%
0.009%
99.583%
0.104%
97.917%
1.020%
83.333%
6.944%

−

−

deeppoly

|F|

1

3

6

1

9

65

|C|

3

22

287

time

2m 33s

2m 38s

5m 14s

2887

10

519

29m 52s

13973

24

2405

1h 14m 19s

17784

39

4076

1h 47m 7s

10

74

477

0

0

0

0

0

0

2m 36s

2m 44s

2m 58s

1145

23

33

12h 57m 37s

7139

117

152

>13h

4757

68

88

>13h

6

31

301

483

5762

4

6

31

301

481

−

−

0

0

0

0

4

0

0

0

0

0

−

−

0

0

0

0

4

0

0

0

0

0

−

−

3m 30s

4m 22s

9m 35s

52m 29s

>13h

5h 3m 33s

7m 6s

15m 14s

1h 3m 33s

7h 12m 57s

>13h

>13h

RQ4: Effect of Analyzed Input Space on Scalability. As said above, the analysis of the models
considered in Table 3 is conducted on the entire input space. In practice, as already mentioned,
one might be interested in just a portion of the input space, e.g., depending on the probability
distribution. More generally, we argue that the size of the analyzed input space (rather than
the size of the analyzed neural network) is the most important factor that affects the
performance of the analysis. To support this claim, we trained even larger models and analyzed
them with respect to queries exercising different input space sizes. Table 4 shows the results. The
first column again shows the total number of hidden nodes for each trained model. In particular,
the models we analyzed have the following number of hidden layers and nodes per layer (from top
to bottom): 4 and 5; 8 and 10; 16 and 20; 32 and 40. Column qery shows the query used for the
analysis and the corresponding exercised input space size. Specifically, the queries identify people
with the following characteristics:

A: true

exercised input space: 100.0%

Perfectly Parallel Fairness Certification of Neural Networks

1:21

B: A ∧ age7 ≤ 53.5
C: B ∧ race = white
D: C ∧ work class = private
E: D ∧ marital status = single
F : E ∧ occupation = blue-collar

exercised input space: 50.00%
exercised input space: 8.333% (3 race choices)
exercised input space: 1.043% (4 work class choices)
exercised input space: 0.104% (5 marital status choices)
exercised input space: 0.009% (6 occupation choices)
For the analysis budget, we used L = 0.25, U = 0.1 ∗ |M|, and a time limit of 13h. Column input
shows, for each domain used for the forward pre-analysis, the coverage of the exercised input
space (i.e., the percentage of the input space that satisfies the query and was completed by the
analysis) and the corresponding input-space coverage (i.e., the same percentage but this time scaled
to the entire input space). Columns U, |C|, |F|, and time are as before. Where a timeout is indicated
(i.e., time > 13h) and the values for the input, |C|, and |F| columns are missing, it means that the
timeout occurred during the pre-analysis; otherwise, it happened during the backward analysis.
For each model and query, we highlighted the configuration (i.e., the abstract domain used for the
pre-analysis) that achieved the highest input-space coverage with the shortest analysis running
time. Note that, where the |F| column only contains zeros, it means that the backward analysis
had no activation patterns to explore; this implies that the entire covered input space (i.e., the
percentage shown in the input column) was already certified to be fair by the forward analysis.
Overall, we observe that whenever the analyzed input space is small enough (i.e., queries D − F ),
the size of the neural network has little influence on the input space coverage and slightly impacts
the analysis running time, independently of the domain used for the forward pre-analysis. Instead,
for larger analyzed input spaces (i.e., queries A − C) performance degrades quickly for larger
neural networks. These results thus support our claim. Again, as expected, we observe that the
symbolic domain generally is the better choice for the forward pre-analysis, in particular for
queries exercising a larger input space or larger neural networks.

RQ5: Scalability-vs-Precision Tradeoff. To evaluate the effect of the analysis budget (bounds
L and U), we analyzed a model using different budget configurations. For this experiment, we used
the Japanese Credit Screening8 dataset, which we made fair with respect to gender. Our 2-class
model (17 inputs and 4 hidden layers with 5 nodes each) had a classification accuracy of 86%. Note
that accuracy does not increase by adding more layers or nodes per layer, in fact, it may significantly
decrease — we tried up to 100 hidden layers with 100 nodes each.

Table 5 shows the results of the analysis for different budget configurations and choices for the
domain used for the forward pre-analysis. The best configuration in terms of input-space coverage
and analysis running time is highlighted. The symbol next to each domain name introduces the
marker used in the scatter plot of Figure 4a, which visualizes the coverage and running time.
Figure 4b zooms on 90.00% ≤ input and 1000s ≤ time ≤ 1000s.

Overall, we observe that the more precise symbolic and deeppoly domains boost input coverage,
most noticeably for configurations with a larger L. This additional precision does not always result
in longer running times. In fact, a more precise pre-analysis often reduces the overall running
time. This is because the pre-analysis is able to prove that more partitions are already fair without
requiring them to go through the backward analysis (cf. columns |F|).

Independently of the chosen domain for the forward pre-analysis, as expected, a larger U or a
smaller L increase precision. Increasing U or L typically reduces the number of completed partitions
(cf. columns |C|). Consequently, partitions tend to be more complex, requiring both forward and
backward analyses. Since the backward analysis tends to dominate the running time, more partitions

7This corresponds to aдe ≤ 0.5 with min-max scaling between 0 and 1.
8https://archive.ics.uci.edu/ml/datasets/Japanese+Credit+Screening

1:22

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 5. Comparison of Different Analysis Configurations (Japanese Credit Screening) — 12 CPUs

L

0.5

0.25

0.125

0

U

4
6
8
10
4
6
8
10
4
6
8
10
4
6
8
10

|C|
input
37
15.28%
39
17.01%
90
51.39%
89
79.86%
1115
59.09%
1404
83.77%
869
96.07%
409
99.54%
12449
97.13%
5919
99.83%
1926
99.98%
100.00%
428
100.00% 19299
4843
100.00%
1919
100.00%
486
100.00%

boxes
|F|

0
6
28
34
20
79
140
93
200
276
203
95
295
280
208
102

0
6
85
89
415
944
761
403
9519
4460
1568
427
15446
3679
1567
475

time
8s
51s
12m 2s
34m 15s
54m 32s
37m 19s
1h 7m 29s
1h 35m 20s
3h 33m 48s
3h 23m
2h 14m 25s
1h 39m 31s
6h 13m 24s
2h 24m 7s
2h 9m 59s
1h 41m 3s

|C|
input
79
58.33%
129
69.10%
88
82.64%
98
93.06%
884
95.94%
634
98.68%
310
99.72%
195
99.98%
1101
99.99%
988
100.00%
404
100.00%
100.00%
151
100.00% 1397
763
100.00%
404
100.00%
217
100.00%

symbolic
|F|

8
22
31
40
39
66
67
52
60
77
73
53
60
66
73
55

20
61
67
83
484
376
247
176
685
606
309
141
885
446
309
192

time
1m 26s
5m 41s
12m 35s
42m 32s
54m 31s
23m 31s
1h 3m 33s
1h 2m 13s
47m 46s
26m 47s
46m 31s
57m 32s
40m 5s
35m 24s
45m 48s
1h 2m 11s

input
69.79%
80.56%
91.32%
96.88%
98.26%
99.70%
99.98%
100.00%
99.99%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%

deeppoly
|F|

10
23
27
29
65
79
69
47
81
80
57
39

87
81
68
50

39
51
56
58
293
205
177
87
415
298
129
62

425
242
144
91

|C|
115
104
84
83
540
322
247
111
768
489
175
80

766
401
193
121

time
3m 18s
7m 53s
19m 33s
43m 39s
14m 29s
13m 25s
22m 52s
34m 56s
19m 1s
16m 54s
20m 11s
28m 33s

16m 41s
32m 29s
24m 16s
30m 53s

10,000s

i

e
m
T
s
i
s
y
l
a
n
A

1,000s

100s

10s

10,000s

i

e
m
T
s
i
s
y
l
a
n
A

1,000s

10

20

30

40

50

60

70

80

90 100

90

100

Analyzed Input Space

Analyzed Input Space

(a)

(b) Zoom on 90.00% ≤ input and 1000s ≤ time ≤ 1000s

Fig. 4. Comparison of Different Analysis Configurations (Japanese Credit Screening)

generally increase the running time (when comparing configurations with similar coverage). Based
on our experience, the optimal budget largely depends on the analyzed model.

RQ6: Leveraging Multiple CPUs. To evaluate the effect of parallelizing the analysis using
multiple cores, we re-ran the analyses of RQ5 on 4 CPU cores instead of 12. Table 6 shows these
results. We observe the most significant increase in running time for 4 cores for the boxes domain.
On average, the running time increases by a factor of 2.6. On the other hand, for the symbolic
and deeppoly domains, the running time with 4 cores increases less drastically, on average by a
factor of 1.6 and 2, respectively. This is again explained by the increased precision of the forward
analysis; fewer partitions require a backward pass, where parallelization is most effective.

The appendix includes the same experiment on 24 vCPUs (see Table 13).

12 RELATED WORK
Significant progress has been made on testing and verifying machine-learning models. We focus
on fairness, safety, and robustness properties in the following, especially of deep neural networks.

Perfectly Parallel Fairness Certification of Neural Networks

1:23

Table 6. Comparison of Different Analysis Configurations (Japanese Credit Screening) — 4 CPUs

L

0.5

0.25

0.125

0

U

4
6
8
10
4
6
8
10
4
6
8
10
4
6
8
10

|C|
input
44
15.28%
40
17.01%
96
51.39%
109
79.86%
1147
59.09%
1757
83.77%
1129
96.07%
510
99.54%
12398
97.13%
5919
99.83%
1331
99.98%
100.00%
428
100.00% 20631
6093
100.00%
1919
100.00%
402
100.00%

boxes
|F|

0
5
29
36
22
80
136
92
200
273
212
94
296
296
211
93

0
5
88
107
405
1149
950
497
9491
4460
1158
427
16611
4563
1567
401

time
22s
1m 3s
22m 47s
1h 1m 54s
54m 51s
2h 19m 50s
4h 13m 49s
5h 3m 34s
9h 46m
8h 40m 11s
4h 39m 58s
4h 45m 30s
>13h
9h 8m 47s
6h 15m 29s
4h 19m 3s

|C|
input
96
58.33%
97
69.10%
128
82.64%
104
93.06%
715
95.94%
693
98.68%
289
99.72%
158
99.98%
1864
99.99%
697
100.00%
293
100.00%
100.00%
211
100.00% 1424
632
100.00%
378
100.00%
180
100.00%

symbolic
|F|

8
18
34
36
43
73
62
57
58
71
71
55
58
72
79
56

26
45
87
92
407
400
232
150
1188
404
233
188
885
371
287
154

time
2m 31s
6m 52s
24m 5s
56m 8s
30m 12s
50m 57s
1h 5m 53s
1h 39m 14s
1h 46m 25s
50m 58s
1h 10m 5s
2h 4m 27s
1h 6m 30s
50m 37s
1h 18m 16s
1h 35m 56s

input
69.79%
80.56%
91.32%
96.88%
98.26%
99.70%
99.98%
100.00%
99.99%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%

|C|
85
131
117
69
488
322
153
109
1257
465
201
121

911
403
174
82

deeppoly
|F|

9
26
34
31
65
79
56
46
92
95
67
50

92
85
65
38

30
63
78
50
272
205
113
85
670
287
151
91

502
247
128
63

time
3m 57s
23m 6s
27m 28s
35m 2s
20m 35s
34m 42s
42m 25s
1h 8m 18s
51m 19s
47m 53s
56m 12s
1h 16m 29s

37m 58s
38m 26s
48m 20s
50m 51s

Fairness Criteria. There are countless fairness definitions in the literature. In this paper, we
focus on causal fairness (specifically the fairness notion considered by Galhotra et al. [Galhotra
et al. 2017]) and compare here with the most popular and related notions.

Demographic parity or group fairness [Feldman et al. 2015] is the most common non-causal notion
of fairness. It states that individuals with different values of sensitive features, hence belonging
to different groups, should have the same probability of being predicted to the positive class. For
example, a loan system satisfies group fairness with respect to gender if male and female applicants
have equal probability of getting loans. If unsatisfied, this notion is also referred to as disparate
impact. Our notion of fairness is stronger, as it imposes fairness on every pair of individuals that
differ only in sensitive features. A classifier that satisfies group fairness does not necessarily satisfy
causal fairness, because there may still exist pairs of individuals on which the classifier exhibits bias.
Another group-based notion of fairness is equality of opportunity [Hardt et al. 2016]. It states
that qualified individuals with different values of sensitive features should have equal probability of
being predicted to the positive class. For a loan system, this means that male and female applicants
who are qualified to receive loans should have an equal chance of being approved. By imposing
fairness on every qualified pair of individuals that differ only in sensitive features, we can generalize
causal fairness to also concern both prediction and actual results. We can then adapt our technique
to consider only the part of the input space that includes qualified individuals.

Other causal notions of fairness [Chiappa 2019; Kilbertus et al. 2017; Kusner et al. 2017; Nabi and
Shpitser 2018, etc.] require additional knowledge in the form of a causal model. A causal model can
drive the choice of the sensitive input(s) for our analysis.

Testing and Verifying Fairness. Galhotra et al. [Galhotra et al. 2017] proposed an approach,
Themis, that allows efficient fairness testing of software. Udeshi et al. [Udeshi et al. 2018] designed
an automated and directed testing technique to generate discriminatory inputs for machine-learning
models. Tramer et al. [Tramèr et al. 2017] introduced the unwarranted-associations framework and
instantiated it in FairTest. In contrast, our technique provides formal fairness guarantees.

Bastani et al. [Bastani et al. 2019] used adaptive concentration inequalities to design a scalable
sampling technique for providing probabilistic fairness guarantees for machine-learning models. As
mentioned in the Introduction, our approach differs in that it gives definite (instead of probabilistic)
guarantees. However, it might exclude partitions for which the analysis is not exact.

1:24

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Albarghouthi et al. [Albarghouthi et al. 2017b] encoded fairness problems as probabilistic pro-
gram properties and developed an SMT-based technique for verifying fairness of decision-making
programs. As discussed in the Introduction, this technique has been shown to scale only up to
neural networks with at most 3 inputs and a single hidden layer with at most 2 nodes. In contrast,
our approach is designed to be perfectly parallel, and thus, is significantly more scalable.

A recent technique [Ruoss et al. 2020] certifies individual fairness of neural networks, which is
a local property that coincides with robustness within a particular distance metric. In particular,
individual fairness dictates that similar individuals should be treated similarly. Our approach,
however, targets certification of neural networks for the global property of causal fairness.

For certain biased decision-making programs, the program repair technique proposed by Al-
barghouthi et al. [Albarghouthi et al. 2017a] can be used to repair their bias. Albarghouthi and
Vinitsky [Albarghouthi and Vinitsky 2019] further introduced fairness-aware programming, where
programmers can specify fairness properties in their code for runtime checking.

Robustness of Deep Neural Networks. Robustness is a desirable property for traditional soft-
ware [Chaudhuri et al. 2012; Goubault and Putot 2013; Majumdar and Saha 2009], especially control
systems. Deep neural networks are also expected to be robust. However, research has shown that
deep neural networks are not robust to small perturbations of their inputs [Szegedy et al. 2014]
and can even be easily fooled [Nguyen et al. 2015]. Subtle imperceptible perturbations of inputs,
known as adversarial examples, can change their prediction results. Various algorithms [Carlini
and Wagner 2017b; Goodfellow et al. 2015; Madry et al. 2018; Tabacof and Valle 2016; Zhang et al.
2019] have been proposed that can effectively find adversarial examples. Research on developing
defense mechanisms against adversarial examples [Athalye et al. 2018; Carlini and Wagner 2016,
2017a,b; Cornelius 2019; Engstrom et al. 2018; Goodfellow et al. 2015; Huang et al. 2015; Mirman
et al. 2018, 2019] is also active. Causal fairness is a special form of robustness in the sense that
neural networks are expected to be globally robust with respect to their sensitive features.

Testing Deep Learning Systems. Multiple frameworks have been proposed to test the robust-
ness of deep learning systems. Pei et al. [Pei et al. 2017] proposed the first whitebox framework
for testing such systems. They used neuron coverage to measure the adequacy of test inputs. Sun
et al. [Sun et al. 2018] presented the first concolic-testing [Godefroid et al. 2005; Sen et al. 2005]
approach for neural networks. Tian et al. [Tian et al. 2018] and Zhang et al. [Zhang et al. 2018]
proposed frameworks for testing autonomous driving systems. Gopinath et al. [Gopinath et al.
2018] used symbolic execution [Clarke 1976; King 1976]. Odena et al. [Odena et al. 2019] were
the first to develop coverage-guided fuzzing for neural networks. Zhang et al. [Zhang et al. 2019]
proposed a blackbox-fuzzing technique to test their robustness.

Formal Verification of Deep Neural Networks. Formal verification of deep neural networks
has mainly focused on safety properties. However, the scalability of such techniques for verifying
large real-world neural networks is limited. Early work [Pulina and Tacchella 2010] applied abstract
interpretation to verify a neural network with six neurons. Recent work [Gehr et al. 2018; Huang
et al. 2017; Katz et al. 2017; Singh et al. 2019; Wang et al. 2018] significantly improves scalability.
Huang et al. [Huang et al. 2017] proposed a framework that can verify local robustness of neural
networks based on SMT techniques [Barrett and Tinelli 2018]. Katz et al. [Katz et al. 2017] developed
an efficient SMT solver for neural networks with ReLU activation functions. Gehr et al. [Gehr et al.
2018] traded precision for scalability and proposed a sound abstract interpreter that can prove local
robustness of realistic deep neural networks. Singh et al. [Singh et al. 2019] proposed the deeppoly
domain for certifying robustness of neural networks. Wang et al. [Wang et al. 2018] are the first to
use symbolic interval arithmetic to prove security properties of neural networks.

Perfectly Parallel Fairness Certification of Neural Networks

1:25

13 CONCLUSION AND FUTURE WORK
We have presented an automated, perfectly parallel analysis for certifying fairness of neural
networks. The analysis is configurable to support a wide range of use cases throughout the de-
velopment lifecycle of neural networks: ranging from short sanity checks during development to
formal fairness audits before deployments.

In future work, we plan to extend our technique in various ways, for instance, by automatically
tuning parameters (such as the upper bound U) during the analysis or by feeding analysis results
to other tools. Such tools may be used to provide probabilistic fairness guarantees for partitions
that could not be certified or repair networks by eliminating bias.

REFERENCES
Aws Albarghouthi, Loris D’Antoni, and Samuel Drews. 2017a. Repairing Decision-Making Programs Under Uncertainty. In

CAV. 181–200. https://doi.org/10.1007/978-3-319-63387-9_9

Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and Aditya V. Nori. 2017b. FairSquare: Probabilistic Verification of

Program Fairness. PACMPL 1, OOPSLA (2017), 80:1–80:30. https://doi.org/10.1145/3133904

Aws Albarghouthi and Samuel Vinitsky. 2019. Fairness-Aware Programming. In FAT*. 211–219. https://doi.org/10.1145/

3287560.3287588

Anish Athalye, Nicholas Carlini, and David A. Wagner. 2018. Obfuscated Gradients Give a False Sense of Security:

Circumventing Defenses to Adversarial Examples. In ICML (PMLR), Vol. 80. PMLR, 274–283.

Clark W. Barrett and Cesare Tinelli. 2018. Satisfiability Modulo Theories. In Handbook of Model Checking. Springer, 305–343.
Alexander I. Barvinok. 1994. A Polynomial Time Algorithm for Counting Integral Points in Polyhedra When the Dimension

is Fixed. Mathematics of Operations Research 19, 4 (1994), 769–779. https://doi.org/10.1287/moor.19.4.769

Osbert Bastani, Xin Zhang, and Armando Solar-Lezama. 2019. Probabilistic verification of fairness properties via concentra-

tion. PACMPL 3, OOPSLA (2019), 118:1–118:27.

Joy Buolamwini and Timnit Gebru. 2018. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender

Classification. In FAT (PMLR), Vol. 81. PMLR, 77–91.

Nicholas Carlini and David A. Wagner. 2016. Defensive Distillation is Not Robust to Adversarial Examples. CoRR

abs/1607.04311 (2016).

Nicholas Carlini and David A. Wagner. 2017a. Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection

Methods. In AISec@CCS. ACM, 3–14.

Nicholas Carlini and David A. Wagner. 2017b. Towards Evaluating the Robustness of Neural Networks. In S&P. IEEE

Computer Society, 39–57.

Swarat Chaudhuri, Sumit Gulwani, and Roberto Lublinerman. 2012. Continuity and Robustness of Programs. Commun.

ACM 55, 8 (2012), 107–115. https://doi.org/10.1145/2240236.2240262

Liqian Chen, Antoine Miné, and Patrick Cousot. 2008. A Sound Floating-Point Polyhedra Abstract Domain. In APLAS. 3–18.

https://doi.org/10.1007/978-3-540-89330-1_2

Silvia Chiappa. 2019. Path-Specific Counterfactual Fairness. In AAAI. 7801–7808. https://doi.org/10.1609/aaai.v33i01.

33017801

Lori A. Clarke. 1976. A System to Generate Test Data and Symbolically Execute Programs. TSE 2 (1976), 215–222. Issue 3.
Cory Cornelius. 2019. The Efficacy of SHIELD under Different Threat Models. CoRR abs/1902.00541 (2019).
Patrick Cousot. 2002. Constructive Design of a Hierarchy of Semantics of a Transition System by Abstract Interpretation.

Theoretical Computer Science 277, 1-2 (2002), 47–103. https://doi.org/10.1016/S0304-3975(00)00313-3

Patrick Cousot and Radhia Cousot. 1976. Static Determination of Dynamic Properties of Programs. In Second International

Symposium on Programming. 106–130.

Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs

by Construction or Approximation of Fixpoints. In POPL. 238–252. https://doi.org/10.1145/512950.512973

Patrick Cousot and Radhia Cousot. 1979. Systematic Design of Program Analysis Frameworks. In POPL. 269–282. https:

//doi.org/10.1145/567752.567778

Patrick Cousot and Nicolas Halbwachs. 1978. Automatic Discovery of Linear Restraints Among Variables of a Program. In

POPL. 84–96. https://doi.org/10.1145/512760.512770

Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak Sen. 2017. Use Privacy in Data-Driven Systems:
Theory and Experiments with Machine Learnt Programs. In CCS. 1193–1210. https://doi.org/10.1145/3133956.3134097
Logan Engstrom, Andrew Ilyas, and Anish Athalye. 2018. Evaluating and Understanding the Robustness of Adversarial

Logit Pairing. CoRR abs/1807.10272 (2018).

1:26

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. 2015. Certifying

and Removing Disparate Impact. In KDD. ACM, 259–268.

Sainyam Galhotra, Yuriy Brun, and Alexandra Meliou. 2017. Fairness Testing: Testing Software for Discrimination. In FSE.

498–510. https://doi.org/10.1145/3106237.3106277

Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin T. Vechev. 2018.
AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation. In S & P. 3–18. https:
//doi.org/10.1109/SP.2018.00058

Khalil Ghorbal, Eric Goubault, and Sylvie Putot. 2009. The Zonotope Abstract Domain Taylor1+. In CAV. 627–633. https:

//doi.org/10.1007/978-3-642-02658-4_47

Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Automated Random Testing. In PLDI. ACM,

213–223.

Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. 2016. Deep Learning. MIT Press.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In ICLR.

http://arxiv.org/abs/1412.6572

Divya Gopinath, Kaiyuan Wang, Mengshi Zhang, Corina S. Pasareanu, and Sarfraz Khurshid. 2018. Symbolic Execution for

Deep Neural Networks. CoRR abs/1807.10439 (2018).

Eric Goubault and Sylvie Putot. 2013. Robustness Analysis of Finite Precision Implementations. In APLAS. 50–57. https:

//doi.org/10.1007/978-3-319-03542-0_4

Nina Grgić-Hlača, Muhammad Bilal Zafar, Krishna P. Gummadi, and Adrian Weller. 2016. The Case for Process Fairness in

Learning: Feature Selection for Fair Decision Making. In NIPS 2016 ML and the Law.

Boris Hanin and David Rolnick. 2019. Deep ReLU Networks Have Surprisingly Few Activation Patterns. In NIPS. Curran As-
sociates, Inc., 359–368. http://papers.nips.cc/paper/8328-deep-relu-networks-have-surprisingly-few-activation-patterns.
pdf

Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of Opportunity in Supervised Learning. In NIPS. 3315–3323.
Ruitong Huang, Bing Xu, Dale Schuurmans, and Csaba Szepesvári. 2015. Learning with a Strong Adversary. CoRR

abs/1511.03034 (2015). http://arxiv.org/abs/1511.03034

Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. 2017. Safety Verification of Deep Neural Networks. In CAV.

3–29. https://doi.org/10.1007/978-3-319-63387-9_1

Bertrand Jeannet and Antoine Miné. 2009. APRON: A Library of Numerical Abstract Domains for Static Analysis. In CAV.

661–667. https://doi.org/10.1007/978-3-642-02658-4_52

Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochenderfer. 2017. Reluplex: An Efficient SMT Solver

for Verifying Deep Neural Networks. In CAV. 97–117. https://doi.org/10.1007/978-3-319-63387-9_5

Matthew Kay, Cynthia Matuszek, and Sean A. Munson. 2015. Unequal Representation and Gender Stereotypes in Image

Search Results for Occupations. In CHI. ACM, 3819–3828.

Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing, and Bernhard Schölkopf.

2017. Avoiding Discrimination through Causal Reasoning. In NIPS. 656âĂŞ666.

James C. King. 1976. Symbolic Execution and Program Testing. CACM 19 (1976), 385–394. Issue 7.
Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfactual Fairness. In NIPS. 4069–4079.
Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. How We Analyzed the COMPAS Recidivism Algorithm.

https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm.

Jianlin Li, Jiangchao Liu, Pengfei Yang, Liqian Chen, Xiaowei Huang, and Lijun Zhang. 2019. Analyzing Deep Neural
Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification. In SAS. 296–319. https:
//doi.org/10.1007/978-3-030-32304-2_15

Kristian Lum and William Isaac. 2016. To Predict and Serve? Significance 13 (2016), 14–19. Issue 5.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2018. Towards Deep Learning

Models Resistant to Adversarial Attacks. In ICLR. OpenReview.net.

Rupak Majumdar and Indranil Saha. 2009. Symbolic Robustness Analysis. In RTSS. 355–363. https://doi.org/10.1109/RTSS.

2009.17

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019. A Survey on Bias and

Fairness in Machine Learning. CoRR abs/1908.09635 (2019).

Antoine Miné. 2004. Relational Abstract Domains for the Detection of Floating-Point Run-Time Errors. In ESOP. 3–17.

https://doi.org/10.1007/978-3-540-24725-8_2

Antoine Miné. 2006a. Symbolic Methods to Enhance the Precision of Numerical Abstract Domains. In VMCAI. 348–363.

https://doi.org/10.1007/11609773_23

Antoine Miné. 2006b. The Octagon Abstract Domain. Higher-Order and Symbolic Computation 19, 1 (2006), 31–100.

https://doi.org/10.1007/s10990-006-8609-1

Perfectly Parallel Fairness Certification of Neural Networks

1:27

Matthew Mirman, Timon Gehr, and Martin T. Vechev. 2018. Differentiable Abstract Interpretation for Provably Robust

Neural Networks. In ICML. 3575–3583.

Matthew Mirman, Gagandeep Singh, and Martin T. Vechev. 2019. A Provable Defense for Deep Residual Networks. CoRR

abs/1903.12519 (2019).

Razieh Nabi and Ilya Shpitser. 2018. Fair Inference on Outcomes. In AAAI. AAAI Press.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified Linear Units Improve Restricted Boltzmann Machines. In ICML. 807–814.
Anh Mai Nguyen, Jason Yosinski, and Jeff Clune. 2015. Deep Neural Networks are Easily Fooled: High Confidence Predictions

for Unrecognizable Images. In CVPR. 427–436. https://doi.org/10.1109/CVPR.2015.7298640

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting Racial Bias in an Algorithm

Used to Manage the Health of Populations. Science 366 (2019), 447–453. Issue 6464.

Augustus Odena, Catherine Olsson, David Andersen, and Ian J. Goodfellow. 2019. TensorFuzz: Debugging Neural Networks

with Coverage-Guided Fuzzing. In ICML (PMLR), Vol. 97. PMLR, 4901–4911.

Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. 2017. DeepXplore: Automated Whitebox Testing of Deep Learning

Systems. In SOSP. 1–18. https://doi.org/10.1145/3132747.3132785

Luca Pulina and Armando Tacchella. 2010. An Abstraction-Refinement Approach to Verification of Artificial Neural

Networks. In CAV. 243–257. https://doi.org/10.1007/978-3-642-14295-6_24

Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin T. Vechev. 2020. Learning Certified Individually Fair Representations.

CoRR abs/2002.10312 (2020).

Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit Testing Engine for C. In ESEC/FSE. ACM, 263–272.
Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. 2019. An Abstract Domain for Certifying Neural

Networks. PACMPL 3, POPL (2019), 41:1–41:30. https://doi.org/10.1145/3290354

Youcheng Sun, Min Wu, Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska, and Daniel Kroening. 2018. Concolic Testing

for Deep Neural Networks. In ASE. ACM, 109–119.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. 2014.

Intriguing Properties of Neural Networks. In ICLR. http://arxiv.org/abs/1312.6199

Pedro Tabacof and Eduardo Valle. 2016. Exploring the Space of Adversarial Images. In IJCNN. 426–433. https://doi.org/10.

1109/IJCNN.2016.7727230

Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray. 2018. DeepTest: Automated Testing of Deep-Neural-Network-Driven

Autonomous Cars. In ICSE. ACM, 303–314.

Florian Tramèr, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, Jean-Pierre Hubaux, Mathias Humbert, Ari Juels,
and Huang Lin. 2017. FairTest: Discovering Unwarranted Associations in Data-Driven Applications. In EuroS&P. IEEE,
401–416.

Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Automated directed fairness testing. In ASE. ACM,

98–108.

Caterina Urban and Peter Müller. 2018. An Abstract Interpretation Framework for Input Data Usage. In ESOP. 683–710.

https://doi.org/10.1007/978-3-319-89884-1_24

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal Security Analysis of Neural

Networks Using Symbolic Intervals. In Security. USENIX, 1599–1614.

Fuyuan Zhang, Sankalan Pal Chowdhury, and Maria Christakis. 2019. DeepSearch: Simple and Effective Blackbox Fuzzing

of Deep Neural Networks. CoRR abs/1910.06296 (2019).

Mengshi Zhang, Yuqun Zhang, Lingming Zhang, Cong Liu, and Sarfraz Khurshid. 2018. DeepRoad: GAN-Based Metamorphic

Testing and Input Validation Framework for Autonomous Driving Systems. In ASE. ACM, 132–142.

1:28

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 7. Analysis of Neural Networks Trained on Fair and {Age, Credit > 1000}-Biased Data (German Credit
Data) — Full Table (boxes Domain)

fair data
|C|
|F|
144
182
167
157
169
173
134
172

32
35
13
30
36
57
30
16

39
56
24
34
67
82
38
19

208
211
207
180
433
230
243
237

76
86
23
45
139
92
99
63

139
185
42
75
329
197
145
96

credit

≤ 1000

min
median
max

> 1000

min
median
max

U
8
9
2
13
6
13
9
6

13
15
3
13
7
16
10
10

bias
0.33%
0.17%
0.09%
0.15%
0.23%
0.30%
0.20%
0.16%
0.09%
0.19%
0.33%
12.20%
7.43%
2.21%
4.29%
9.73%
14.96%
6.00%
4.61%
2.21%
6.72%
14.96%

boxes

U
13
5
12
9
10
7
9
3

16
10
16
10
14
9
11
3

time
7m 7s
30m 59s
2m 2s
17m 30s
4m 24s
12m 36s
3m 13s
47s
47s
5m 46s
30m 59s
53m 27s
3h 45m 20s
1m 42s
36m 36s
16m 14s
7h 7m 12s
22m 1s
26m 48s
1m 42s
31m 42s
7h 7m 12s

bias
0.79%
0.31%
0.90%
0.42%
0.95%
0.41%
0.48%
0.09%
0.09%
0.45%
0.95%
27.59%
30.77%
33.19%
16.45%
30.27%
17.24%
19.23%
4.52%
4.52%
23.41%
33.19%

|F|

biased data
|C|
212
166
202
187
260
190
189
200

56
26
67
47
88
56
39
18

78
49
81
71
210
66
59
21

285
387
273
397
257
417
288
618

140
122
122
198
120
169
99
83

270
312
260
389
253
337
193
240

time
1h 56m 57s
2m 17s
24m 2s
17m 57s
1h 13m 14s
8m 7s
3m 2s
5m 23s
2m 17s
13m 2s
1h 56m 57s
16h 50m 48s
36m 39s
16h 49m 33s
2h 25m 20s
2h 13m 36s
1h 0m 6s
28m 34s
21m 11s
21m 11s
1h 36m 51s
16h 50m 48s

Table 8. Analysis of Neural Networks Trained on Fair and {Age, Credit > 1000}-Biased Data (German Credit
Data) — Full Table (symbolic Domain)

fair data
|C|
|F|
138
165
140
159
157

22
19
8
21
14

173
135
168

23
23
13

32
23
10
22
25

32
25
14

202
215

161
203

234
228
261
228

56
60

11
41

38
82
106
51

101
103

18
54

74
168
80
66

credit

≤ 1000

min
median
max

> 1000

min
median
max

U
7
6
2
9
3
8
6
5

12
15
2
9
3
16
8
9

bias
0.33%
0.17%
0.09%
0.15%
0.23%
0.30%
0.20%
0.16%
0.09%
0.19%
0.33%
12.20%
7.43%
2.21%
4.29%
9.73%
14.96%
6.00%
4.61%
2.21%
6.72%
14.96%

symbolic

U
10
4
12
5
8
2
12
2

13
6
15
8
14
6
6
2

time

52s
4m 8s
29s
2m 5s
1m 49s

1m 10s
1m 0s
13s
13s
1m 5s
4m 8s
32m 1s
2h 28m 9s

38s
6m 53s

2m 56s
4h 16m 52s
6m 6s
11m 4s
38s
8m 59s
4h 16m 52s

bias
0.79%
0.31%
0.90%
0.42%
0.95%
0.41%
0.48%
0.09%
0.09%
0.45%
0.95%
27.59%
30.77%
33.19%
16.45%
30.27%
17.24%
19.23%
4.52%
4.52%
23.41%
33.19%

biased data
|C|
|F|
196

47

56

141
198
194
173
182
181
196

412
371
309
324
241
389

340
325

17
52
28
52
24
23
10

189
75
126
136
98
76

66
45

26
59
38
77
33
39
10

334
179
257
229
219
162

134
90

time
7m 9s

1m 10s
13m 16s
3m 19s
10m 40s
2m 3s
1m 21s
1m 42s
1m 10s
2m 41s
13m 16s
4h 50m 24s
11m 52s
8h 5m 14s
1h 4m 52s
1h 39m 34s
18m 36s

4m 12s
11m 4s
3m 7s
41m 44s
8h 5m 14s

Perfectly Parallel Fairness Certification of Neural Networks

1:29

Table 9. Analysis of Neural Networks Trained on Fair and {Age, Credit > 1000}-Biased Data (German Credit
Data) — Full Table (deeppoly Domain)

fair data
|F|

|C|
170

211
176
212
217
213

193
193

321
329
217

239
268

403
313
264

21

10
4
9
8
17

11
9

25

10
5
9
15
23

11
10

85
75
15

24
29

116
92
50

150
125
16

33
87

231
115
74

credit

≤ 1000

min
median
max

> 1000

min
median
max

U
8
6
2
7
3
12
6
5

10
11
2
10
4
14
7
9

bias
0.33%
0.17%
0.09%
0.15%
0.23%
0.30%
0.20%
0.16%
0.09%
0.19%
0.33%
12.08%
7.43%
2.21%
4.29%
9.73%
14.96%
5.83%
4.61%
2.21%
6.63%
14.96%

deeppoly

U
8
4
12
4
10
2
3
1

11
7
7
6
13
5
8
2

time
3m 40s

4m 5s
14s
1m 31s
32s
5m 45s

52s
10s
10s
1m 12s
5m 45s

10m 30s
22m 33s
39s

4m 4s
4m 0s

1h 9m 45s
4m 17s
5m 38s
39s
4m 58s
1h 9m 45s

bias
0.79%
0.31%
0.82%
0.42%
0.95%
0.41%
0.48%
0.09%
0.09%
0.45%
0.95%
27.59%
30.77%
33.17%
16.45%
30.17%
17.24%
19.23%
4.52%
4.52%
23.41%
31.17%

|F|

biased data
|C|
260
218
271

42
9
53

53
20
61

242
260
226
228
206

498
394
435
448
418
460
363
331

21
42
20
19
5

234
70
185
162
141
91
79
45

28
67
26
34
5

333
228
327
260
332
217
189
95

time

5m 42s
1m 6s
18m 18s

1m 36s
3m 2s
1m 56s
39s
51s
39s
1m 46s
18m 18s

1h 16m 41s
6m 34s
6h 51m 50s
18m 25s
43m 12s
12m 53s
7m 24s
4m 44s
4m 44s
15m 39s
6h 51m 50s

A SUPPLEMENTARY MATERIAL FOR SECTION 11 (EXPERIMENTAL EVALUATION)

A.1 RQ1: Detecting Seeded Bias
Tables 7, 8 and 9 show the analysis results for all eight models trained on the German Credit
dataset. Column U shows the chosen upper bound for each model. As before, column bias shows
the detected bias, in percentage of the entire input space. We also again show minimum, median,
and maximum bias percentage for each credit request group. Column |C| shows the total number
of analyzed (i.e., completed) input space partitions. Column |F| shows the total number of abstract
activation patterns (left) and feasible input partitions (right) that the backward analysis had to
explore. Finally, column time shows the analysis running time. Again, we also show minimum,
median, and maximum running time for each credit request group. For all models, we highlighted
across all tables the choice of the abstract domain that entailed the shortest analysis time.

A.2 RQ2: Answering Bias Queries
Table 10, 11 and 12 show the analysis results for all eight models trained on the compas dataset
from ProPublica. All columns are shown as before and, again, we highlighted across all tables the
choice of the abstract domain that entailed the shortest analysis time.

A.3 RQ6: Leveraging Multiple CPUs.
Table 13 shows the results of the experiment with the Japanese Credit Screening dataset on 24 vCPU.

1:30

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 10. Queries on Neural Networks Trained on Fair and Race-Biased Data (ProPublica’s compas Data) —
Full Table (boxes Domain)

qery

fair data

boxes

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

12
14
14
18
19
12
13
15

age < 25
race bias?

min
median
max

male
age bias?

min
median
max

caucasian
priors bias?

min
median
max

282
358

|C|
93
98
51
191
221
107
70
32

bias
0.22%
0.64%
0.22%
0.23%
0.29%
0.33%
1.19%
2.46%
0.22%
0.31%
2.46%
3.68%
7.00%
7.92%
237
776
2.60%
4.29% 1175
397
5.16%
338
7.54%
8.00%
415
2.60%
6.08%
8.00%
2.18%
3.66%
2.73%
2.19%
3.17%
2.45%
3.94%
5.36%
2.18%
2.95%
5.36%

34
128
62
49

75
76
51
46

|F|

83
60
22
85
113
56
28
20

115
117

57
265
410
100
84
103

29
39
25
16

11
42
28
20

46
95
30
104
169
84
69
31

281
357

232
478
951
306
337
414

74
75
46
45

33
110
61
48

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

14
14
13
9
14
15
19
17

time
2h 0m 58s
1h 48m 37s
24m 32s
2h 44m 11s
2h 34m 6s
2h 30m 28s
41m 20s
36m 6s
24m 32s
1h 54m 48s
2h 44m 11s

1h 23m 52s
1h 55m 55s

24m 14s
3h 24m 34s
3h 32m 8s
5h 56m 6s
1h 1m 14s
1h 43m 28s
24m 14s
1h 49m 42s
5h 56m 6s
7h 3m 17s
6h 50m 10s
2h 54m 18s
37h 15m 28s

45h 2m 12s
8h 41m 43s
3h 7m 59s
6h 16m 33s
2h 54m 18s
6h 56m 44s
45h 2m 12s

bias
0.83%
8.33%
1.15%
0.42%
0.12%
1.54%
3.25%
0.18%
0.12%
0.99%
8.33%
4.51%
336
12.56% 478
179
7.00%
119
6.90%
303
4.96%
294
7.89%
6.31%
484
12.24% 377
4.51%
6.95%
12.56%
2.92%
6.98%
4.43%
3.40%
3.09%
5.79%
5.10%
3.99%
2.92%
4.21%
6.98%

35
53
40
67
54
57
47
46

|F|

biased data
|C|
65
66
28
21
70
60
206
28

27
37
12
12
34
33
155
13

64
65
20
20
69
59
205
27

111
135
57
35
75
50
228
143

16
23
11
23
21
32
30
22

335
477
172
118
264
293
483
376

34
52
39
66
53
56
46
45

time
5h 29m 19s
27m 14s
14m 53s
44m 55s
26m 0s
1h 17m 56s
1h 10m 10s
3h 8m 10s
14m 53s
57m 33s
5h 29m 19s
6h 36m 43s
1h 3m 18s
34m 23s
4h 1m 53s
1h 41m 39s
8h 26m 55s
1h 46m 51s
2h 20m 27s
34m 23s
2h 3m 39s
8h 26m 55s
5h 22m 42s
1h 38m 57s
1h 8m 37s
46m 53s
2h 29m 32s
5h 11m 44s
70h 50m 10s
15h 1m 10s
46m 53s
3h 50m 28s
70h 50m 10s

Perfectly Parallel Fairness Certification of Neural Networks

1:31

Table 11. Queries on Neural Networks Trained on Fair and Race-Biased Data (ProPublica’s compas Data) —
Full Table (symbolic Domain)

|F|

biased data
|C|
25
60
24

24
34
14

11
27
8

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

12
12
14
18
19
12
13
15

qery

age < 25
race bias?

min
median
max

male
age bias?

min
median
max

caucasian
priors bias?

min
median
max

fair data

|F|

17
20

13
33

54
15
14
13

49
60
39
170
269

70
61

92

14
34
22
18
10
21

28
18

24
24

15
36

63
17
24
16

136
127
135
322
497

159
249

317

39
57
43
46
39
43

53
46

symbolic

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

14
14
12
9
14
15
19
17

time
2h 17m 3s
19m 16s

11m 34s
54m 19s

50m 54s
53m 14s
12m 38s
21m 6s
11m 34s
36m 0s
2h 17m 3s
1h 46m 28s
30m 13s
38m 46s
2h 2m 22s
1h 35m 54s

1h 21m 58s
25m 13s

41m 44s
25m 13s
1h 1m 51s
2h 2m 22s
4h 30m 18s
2h 26m 43s
2h 17m 42s
35h 44m 27s
60h 54m 6s
3h 54m 37s

1h 20m 41s
7h 50m 23s
1h 20m 41s
4h 12m 28s
60h 53m 6s

|C|
57
44

bias
0.23%
0.75%
0.22%
41
0.26% 122
0.30% 148
0.33%
64
27
1.19%
2.46%
17
0.22%
0.32%
2.46%
4.27% 185
7.93% 166
8.36% 197
2.64% 734
4.54% 706
5.69% 227
7.84% 276
8.40% 318
2.64%
6.77%
8.40%
2.18%
3.66%
2.73%
2.19%
3.17%
2.45%
3.94%
5.36%
2.18%
2.95%
5.36%

46
68
51
47
40
57

61
47

17
32
34
162
17

bias
0.83%
8.50%
1.15%
0.42%
0.12%
1.59%
3.36%
0.18%
0.12%
0.99%
8.50%
5.20%
205
12.71% 404
7.09%
168
6.95%
70
200
5.47%
200
8.49%
6.64%
332
12.68% 286
5.20%
7.02%
12.71%
2.92%
6.98%
4.43%
3.40%
3.09%
5.79%
5.10%
3.99%
2.92%
4.21%
6.98%

45
46
51
54
49
46

44
50

time

1h 32m 10s
18m 48s
19m 50s

7m 14s
21m 35s
3h 34m 50s
41m 9s
15m 8s
7m 14s
20m 43s
3h 34m 50s
4h 55m 35s
50m 53s
29m 19s

1h 8m 47s
34m 15s
1h 21m 58s
56m 4s
1h 39m 0s
29m 19s
1h 2m 26s
4h 55m 35s
5h 29m 22s
1h 16m 36s

30m 23s
35m 28s
1h 40m 33s
5h 25m 11s
49h 51m 42s
13h 5m 34s
30m 23s
3h 32m 52s
49h 51m 42s

13
9
19
96
11

55
120
59

22
62
47
139
101

13
22

17
17
18
34
33
19

16
14
30
122
16

186
359
133

69
166
198
262
246

43
44

39
45
48
53
48
44

1:32

Caterina Urban, Maria Christakis, Valentin Wüstholz, and Fuyuan Zhang

Table 12. Queries on Neural Networks Trained on Fair and Race-Biased Data (ProPublica’s compas Data) —
Full Table (deeppoly Domain)

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

11
7
11
17
19
11
15
15

qery

age < 25
race bias?

min
median
max

male
age bias?

min
median
max

caucasian
priors bias?

min
median
max

fair data
|F|

18
14
17

28
49
18

17
17

96

45
47

168
280
68

62
90

21
38
32
28
30

26
29

35

20
16
22

29
54
21

23
31

180

77
143

232
415
154

226
266

53
55
57
57
53

52
52

89

deeppoly

biased data

U
10
10
10
10
10
10
10
10

10
10
10
10
10
10
10
10

11
11
14
7
13
14
17
14

time

1h 11m 43s
10m 33s
52m 29s

42m 2s
1h 0m 2s
53m 29s

9m 39s
5m 18s
5m 18s
47m 16s
1h 11m 43s
2h 30m 23s

19m 47s
28m 12s

1h 49m 9s
1h 33m 36s
1h 35m 25s

23m 10s
53m 26s
19m 47s
1h 13m 31s
2h 20m 23s

2h 32m 44s
18m 26s
39m 5s
16h 19m 14s
52h 10m 2s

2h 18m 42s
2h 39m 18s

3h 41m 16s
18m 26s
2h 36m 1s
52h 10m 2s

|C|
43

63
33
31

37
33
133
33

bias
0.83%
6.48%
1.15%
0.42%
0.12%
2.27%
3.41%
0.18%
0.12%
0.99%
6.48%
5.22%
204
12.38% 387
7.10%
181
96
6.90%
6.14%
157
8.10%
345
6.78%
251
12.88% 257
5.22%
7.00%
12.88%
2.92%
6.95%
4.43%
3.40%
3.09%
5.79%
5.10%
3.99%
2.92%
4.21%
6.95%

83
96
99
110
97

86
108
69

|F|

15

25
10
13

11
16
92
12

65
152
63
23

62
61
141
124

26
33
12

21
24
45
73
38

33

34
14
30

16
24
102
17

180
318
142
95

110
284
223
228

69
71
51

82
58
87
94
65

time
2h 5m 5s

8m 46s
11m 58s
10m 51s

18m 18s
1h 4m 35s
33m 43s
14m 58s
8m 46s
16m 38s
2h 5m 5s

3h 25m 21s
40m 49s
20m 51s
1h 21m 37s

27m 43s
47m 9s
50m 13s
47m 46s
20m 51s
47m 28s
3h 25m 21s

2h 26m 20s
15m 29s
1h 47m 5s

20m 1s
1h 8m 4s
1h 51m 2s
17h 48m 22s
1h 21m 8s
15m 29s
1h 34m 7s
17h 48m 22s

39
33

|C|
bias
0.23%
71
0.75%
33
34
0.22%
0.24% 118
0.31% 117
0.33%
59
1.19%
2.12%
0.22%
0.32%
2.12%
3.86% 242
8.84% 100
8.14% 204
2.70% 563
4.65% 545
5.77% 217
7.76% 252
8.70% 267
2.70%
6.77%
8.84%
2.18% 106
3.66% 105
2.73% 100
2.19% 101
3.17%
86
2.45%
3.94%
5.36%
2.18%
2.95%
5.36%

94
87

90

Table 13. Comparison of Different Analysis Configurations (Japanese Credit Screening) — 24 vCPUs

L

0.5

0.25

0.125

0

U

4
6
8
10
4
6
8
10
4
6
8
10
4
6
8
10

|C|
input
36
15.28%
39
17.01%
92
51.39%
89
79.86%
1320
59.09%
1600
83.77%
1148
96.07%
409
99.54%
12449
97.13%
4198
99.83%
1741
99.98%
100.00%
582
100.00% 16018
4675
100.00%
1609
100.00%
463
100.00%

boxes
|F|

0
6
30
34
21
80
141
93
203
266
217
97
288
279
217
99

0
7
86
89
433
1070
969
403
9519
3234
1488
564
12964
3503
1382
460

time
7s
49s
12m 27s
29m 41s
57m 33s
1h 6m 58s
2h 41m 1s
1h 38m 38s
3h 59m 27s
2h 31m 54s
2h 16m 27s
2h 16m 13s
5h 3m 18s
3h 2m 30s
2h 7m 9s
2h 12m 12s

|C|
input
120
58.33%
80
69.10%
96
82.64%
91
93.06%
656
95.94%
516
98.68%
260
99.72%
213
99.98%
1101
99.99%
759
100.00%
308
100.00%
100.00%
180
100.00% 1883
632
100.00%
326
100.00%
217
100.00%

symbolic

|F|

7
21
32
37
42
61
58
50
59
73
67
56
63
71
67
55

34
40
76
83
340
287
207
189
685
461
242
154
1196
371
252
192

time
3m 32s
4m 19s
14m 13s
47m 1s
32m 38s
18m 6s
28m 57s
1h 16m 11s
1h 2m 58s
51m 28s
33m 14s
1h 5m 59s
1h 52m 25s
38m 3s
1h 12s
1h 13m 55s

input
69.79%
80.56%
91.32%
96.88%
98.26%
99.70%
99.98%
100.00%
99.99%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%
100.00%

deeppoly
|F|

|C|
75
138
89
73
488
286
241
88
892
563
230
80

804
302
194
130

10
26
36
33
65
77
70
42
86
108
67
39

90
75
68
48

27
65
61
52
272
182
175
68
493
344
167
62

442
189
148
98

time
2m 43s
12m 27s
13m 33s
30m
14m 11s
13m 14s
29m 27s
20m 25s
18m 4s
40m 35s
22m 36s
30m 18s

19m 47s
19m 51s
26m 9s
50m 10s

