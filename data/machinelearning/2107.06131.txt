1
2
0
2

l
u
J

6

]

G
L
.
s
c
[

1
v
1
3
1
6
0
.
7
0
1
2
:
v
i
X
r
a

Identiﬁcation of Dynamical Systems using
Symbolic Regression

Gabriel Kronberger[0000−0002−3012−3189], Lukas Kammerer[0000−0001−8236−4294],
and Michael Kommenda

Josef Ressel Centre for Symbolic Regression
Heuristic and Evolutionary Algorithms Laboratory
University of Applied Sciences Upper Austria, Softwarepark 11, 4232 Hagenberg
gabriel.kronberger@fh-hagenberg.at

Abstract. We describe a method for the identiﬁcation of models for
dynamical systems from observational data. The method is based on the
concept of symbolic regression and uses genetic programming to evolve
a system of ordinary diﬀerential equations (ODE).
The novelty is that we add a step of gradient-based optimization of the
ODE parameters. For this we calculate the sensitivities of the solution
to the initial value problem (IVP) using automatic diﬀerentiation.
The proposed approach is tested on a set of 19 problem instances taken
from the literature which includes datasets from simulated systems as
well as datasets captured from mechanical systems. We ﬁnd that gradient-
based optimization of parameters improves predictive accuracy of the
models. The best results are obtained when we ﬁrst ﬁt the individual
equations to the numeric diﬀerences and then subsequently ﬁne-tune the
identiﬁed parameter values by ﬁtting the IVP solution to the observed
variable values.

Keywords: System dynamics, Genetic programming, Symbolic regression

1 Background and Motivation

Modelling, analysis and control of dynamical systems are core topics within the
ﬁeld of system theory focusing on the behavior of systems over time. System
dynamics can be modelled using ordinary diﬀerential equations (cf. [6]) which
deﬁne how the state of a system changes based on the current state for inﬁnites-
imal time steps.

Genetic programming (GP) is a speciﬁc type of evolutionary computation
in which computer programs are evolved to solve a given problem. Symbolic re-
gression (SR) is a speciﬁc task for which GP has proofed to work well. The goal
in SR is to ﬁnd an expression that describes the functional dependency between
a dependent variable and multiple independent variables given a dataset of ob-
served values for all variables. In contrast to other forms of regression analysis
it is not necessary to specify the model structure beforehand because the appro-
priate model structure is identiﬁed simultaneously with the numeric parameters

 
 
 
 
 
 
of the model. SR is therefore especially suited for regression tasks when a para-
metric model for the system or process is not known. Correspondingly, SR could
potentially be used for the identiﬁcation models for dynamical systems when an
accurate mathematical model of the system doesn’t exist.

We aim to use SR to ﬁnd the right-hand sides f (·) of a system of ordinary

diﬀerential equations such as:

˙u1 = f ˙u1(u1, u2, ~θ), ˙u2 = f ˙u2(u1, u2, ~θ)

We only consider systems without input variables (non-forced systems) and

leave an analysis for forced systems for future work.

1.1 Related Work

The vast literature on symbolic regression is mainly focused on static models in
which predicted values are independent given the input variable values. How-
ever, there are a several articles which explicitly describe GP-based methods
for modelling dynamical systems. A straight-forward approach which can be
implemented eﬃciently is to approximate the derivatives numerically [5,9,3,14].
Solving the IVP for each of the considered model candidates is more accurate
but also computationally more expensive; this is for instance used in [1].

In almost all methods discussed in prior work the individual equations of the
system are encoded as separate trees and the SR solutions hold multiple trees
[5,9,1]. A notable exception is the approach suggested in [3] in which GP is run
multiple times to produce multiple expressions for the numerically approximated
derivatives of each state variable. Well-ﬁtting expressions are added to a pool of
equations. In the end, the elements from the pool are combined and the best-
ﬁtting ODE system is returned. In this work the IVP is solved only in the model
combination phase.

Identiﬁcation of correct parameter values is critical especially for ODE sys-
tems. Therefore, several authors have included gradient-based numeric optimiza-
tion of parameter values. In [5], parameters are allowed only for scaling top-level
terms, to allow eﬃcient least-squares optimization of parameter values. In [14]
non-linear parameters are allowed and parameter values are iteratively opti-
mized using the Levenberg-Marquardt algorithm based on gradients determined
through automatic diﬀerentiation.

Partitioning is introduced in [1]. The idea is to optimize the individual parts
of the ODE system for each state variable separately, whereby it is assumed that
the values of all other state variables are known. Partitioning reduces compu-
tational eﬀort but does not guarantee that the combined system of equations
models system dynamics correctly.

Recently, neural networks have been used for modelling ODEs [2] whereby
the neural network parameters are optimized using gradients determined via the
adjoint sensitivity method and automatic diﬀerentiation. The same idea can be
used to ﬁt numeric parameters of SR models.

2 Methods

We extend the approach described in [5], whereby we allow numeric parameters
at any point in the symbolic expressions. We use an iterative gradient-based
method (i.e. Levenberg-Marquardt (LM)) for the optimization of parameters
and automatic diﬀerentiation for eﬃcient calculation of gradients similarly to
[14]. We analyse several diﬀerent algorithm variations, some of which solve the
IVP problem in each evaluation step as in [1,2]. For this we use the state-of-the-
art CVODES1 library for solving ODEs and calculating parameter sensitivities.
We compare algorithms based on the deviation of the IVP solution from the
observed data points for the ﬁnal models. The deviation measure is the sum of
normalized mean squared errors:

SNMSE(Y, ˆY ) =

D

X
i=1

1
var(yi,·)

1
N

N

X
j=1

(yi,j − ˆyi,j)2

(1)

Where Y = {yi,j}i=1..D,j=1..N is the matrix of N subsequent and equidistant
observations of the D the state variables. Each state variable is measured at
the same time points (ti)i=1..N . The matrix of predicted values variables ˆY =
{ˆyi,j}i=1..D,j=1..N is calculated by integrating the ODE system using the initial
values ˆy·,1 = y·,1

2.1 Algorithm description

The system of diﬀerential equations is represented as an array of expression
trees each one representing the diﬀerential equation for one state variable. The
evolutionary algorithm initializes each tree randomly whereby all of the state
variables are allowed to occur in the expression. In the crossover step, exchange of
sub-trees is only allowed between corresponding trees. This implicitly segregates
the gene pools for the state variables. For each of the trees within an individual
we perform sub-tree crossover with the probability given by the crossover rate
parameter. A low crossover rate is helpful to reduce the destructive eﬀect of
crossover.

Memetic optimization of numeric parameters has been shown to improve
GP performance for SR [13,7]. We found that GP performance is improved
signiﬁcantly even when we execute only a few iterations (3-10) of LM and update
the parameter values whenever an individual is evaluated. This ensures that
the improved parameter values are inherited and can subsequently be improved
even further when we start LM with these values [7]. We propose to use the same
approach for parameters of the ODEs with a small modiﬁcation based on the idea
of partitioning. In the ﬁrst step we use the approximated derivative values as the
target. In this step we use partitioning and assume all variable values are known.
The parameter values are updated using the optimized values. In the second
step we use CVODES to solve the IVP for all state variables simultaneously and

1 https://computation.llnl.gov/projects/sundials/cvodes

to calculate parameter sensitivities. The LM algorithm is used to optimize the
SNMSE (Equation 1) directly. Both steps can be turned on individually and the
number of LM iterations can be set independently.

2.2 Computational experiments

Algorithm conﬁguration Table 1 shows the GP parameter values that have
been used for the experiments.

Parameter

Population size
Initialization
Parent selection

Crossover
Mutation

Value

300
PTC2
proportional (ﬁrst parent)
random (second parent)
Subtree crossover
Replace subtree with random branch
Add x ∼ N (0, 1) to all numeric parameters
Add x ∼ N (0, 1) to a single numeric parameter.
Change a single function symbol.
30% (for each expression)
5% (for the whole individual)
oﬀspring must be better than both parents

Crossover rate
Mutation rate
Oﬀspring selection
Maximum selection pressure 100 < # evaluated oﬀspring / population size
Replacement
Terminal set
Function set

Generational with a single elite.
State variables and real-valued parameters
+, ∗, sin, cos,

Table 1. Parameter values for the GP algorithm that have been used for all experi-
ments. The number of generations and the maximum number of evaluated solutions is
varied for the experiments.

We compare two groups of diﬀerent conﬁgurations: in the ﬁrst group we rely
solely on the evolutionary algorithm for the identiﬁcation of parameter values.
In the second group we use parameter optimization optimization. Both groups
contain three conﬁgurations with diﬀerent ﬁtness functions. In the following we
use the identiﬁers D, I, D+I, Dopt, Iopt, and Dopt+Iopt for the six algorithm in-
stances. Conﬁguration D uses the SNMSE for the approximated derivatives for
ﬁtness assignment as in [5,3,14]; conﬁguration I uses the SNMSE for the solu-
tion to the IVP as in [1]; conﬁguration D+I uses the sum of both error measures
for ﬁtness evaluation. For the ﬁrst group we allow maximally 500.000 evaluated
solutions and 250 generations; for the second group we only allow 100.000 eval-
uated solutions and 25 generations. The total number of function evaluations
including evaluations required for parameter optimization is similar for all con-
ﬁgurations and approximately between 500.000 and 2 million (depending on the
dimensionality of the problem).

Problem instances We use 19 problem instances for testing our proposed ap-
proach as shown in Table 2 and Table 3. These have been taken from [5,9,10]
and include a variety of diﬀerent systems. The set of problem instances includes
simulated systems (Table 2) as well as datasets gathered with motion-tracking
from real mechanical systems (Table 3). The simulated datasets have been gen-
erated using fourth-order Runge-Kutta integration (RK45).The motion-tracked
datasets have been adapted from the original source to have equidistant obser-
vations using cubic spline interpolation.

3 Results

Table 4 shows the number of successful runs (from 10 independent runs) for
each problem instance and algorithm conﬁguration. A run is considered success-
ful if the IVP solution for the identiﬁed ODE system has an SNMSE < 0.01.
Some of the instances can be solved easily with all conﬁgurations. Overall the
conﬁguration Iopt+Dopt is the most successful. With this conﬁguration we are
able to produce solutions for all of the 19 instances with a high probability. The
beneﬁcial eﬀect of gradient-based optimization of parameters is evident from the
much larger number of successful runs.

Notably, when we ﬁt the expressions to the approximated derivatives using
partitioning (conﬁgurations D and Dopt) the success rate is low. The reason is
that IVP solutions might deviate strongly when we use partitioning to ﬁt of the
individual equations to the approximated derivatives. To achieve a good ﬁt the
causal dependencies must be represented correctly in the ODE system. This is
not enforced when we use partitioning.

4 Discussion

The results of our experiments are encouraging and indicate that it is indeed
possible to identify ODE models for dynamical systems solely from data using
GP and SR. However, there are several aspects that have not yet been fully
answered in our experiments and encourage further research.

A fair comparison of algorithm conﬁgurations would allow the same run-
time for all cases. We have use a similar amount of function evaluations but
have so far neglected the computational eﬀort that is required for numerically
solving the ODE in each evaluation step. Noise can have a large eﬀect in the nu-
meric approximation of derivatives. We have not yet studied the eﬀect of noisy
measurements. Another task for future research is the analysis of forecasting
accuracy of the models. So far we have only measured the performance on the
training set. Finally, for practical applications, it would be helpful to extend the
method to allow input variables (forced systems) as well as latent variables.

Acknowledgments

The authors gratefully acknowledge support by the Austrian Research Promo-
tion Agency (FFG) within project #867202, as well as the Christian Doppler

Instance

Expression

Initial value

N tmax

Chemical
reaction [5]

E-CELL
[5]

S-System
[5]

Lotka-Volterra
(3 species) [5]

Lotka-Volterra
(2 species) [3]

Glider
[9]

Bacterial
respiration [9]

Predator-Prey
[9]

Bar magnets
[9]

Shear ﬂow
[9]

˙y1 = −1.4y1
˙y2 = 1.4y1 − 4.2y2
˙y3 = 4.2y2
˙y1 = −10y1y3
˙y2 = 10y1y3 − 17y2
˙y3 = −10y1y3 + 17y2
−0.1
− 10y2
1
5

˙y1 = 15y3y
˙y2 = 10y2
1 − 10y2
2
−0.1
−0.1
− 10y
˙y3 = 10y
2
2
−0.1
˙y4 = 8y2
− 10y2
1y
4
5
4 − 10y2
˙y5 = 10y2
5

(0.1, 0, 0)

100

1

(1.2, 0.0, 1.2)

40

0.4

(0.1, 0.1, 0.1, 0.1, 0.1)

y2
3

(0.5, 0.5, 0.5, 0.5, 0.5)

3 ∗ 30 0.3

(1.5, 1.5, 1.5, 1.5, 1.5)

˙y1 = y1(1 − y1 − y2 − 10y3)
˙y2 = y2(0.992 − 1.5y1 − y2 − y3)
˙y3 = y3(−1.2 + 5y1 + 0.5y2)
˙y1 = y1(0.04 − 0.0005y2)
˙y2 = −y2(0.2 − 0.004y1)

˙v = −0.05v2 − sin(θ)
˙θ = v − cos(θ)/v;

˙x = (20 − x − xy)/(1 + 0.5x2)
˙y = (10 − xy)/(1 + 0.5x2)

˙x = x(4 − x − y/(1 + x))
˙y = y(x/(1 + x) − 0.075y)

˙θ1 = 0.5 sin(θ1 − θ2) − sin(θ1)
˙θ2 = 0.5 sin(θ2 − θ1) − sin(θ2)

˙θ = cot(θ) cos(φ)
˙φ = (cos(φ)2 + 0.1 sin(φ)2) sin(φ)

(0.2895, 0.2827, 0.126) 100

100

(20, 20)

300

300

(1.5, 1)

100

10

(1, 1)

100

10

(1.1, 7.36)

100

10

(0.7, −0.3)

100

10

(0.7, 0.4)

100

10

˙x = 10(y − (
˙y = −0.1x

1
3

x3 − x))

Van der Pol
Oscillator [9]
Table 2. The problem instances for which we have generated data using numeric
integration.

(2, 0.1)

100

10

Type

Name

File name

Variables N

512
Simulated
Linear oscillator
Motion-tracked Linear oscillator
879
Pendulum
Simulated
502
Motion-tracked Pendulum
568
x1, x2, v1, v2 200
Simulated
Motion-tracked Coupled oscillator real double linear h 1.txt x1, x2, v1, v2 150
Simulated
θ1, θ2, ω1, ω2 1355
Motion-tracked Double pendulum real double pend h 1.txt2
θ1, θ2, ω1, ω2 200

linear h 1.txt
real linear h 1.txt
pendulum h 1.txt
real pend h 1.txt

Double pendulum double pend h 1.txt2

Coupled oscillator double linear h 1.txt

x, v
x, v
θ, ω
θ, ω

Table 3. The problem instances for which we used the datasets from [11]. For each
system type we use two diﬀerent datasets, one generated via simulation, the other by
motion-tracking the real system.

Instance

D I I+D Dopt Iopt Iopt+Dopt

ChemicalReaction
E-CELL
S-System
Lotka-Volterra (three species)

Bacterial Respiration
Bar Magnets
Glider
Lotka-Volterra
Predator Prey
Shear Flow
Van der Pol Oscillator

7 4
5 0
0 0
0 0

5 3
3 4
0 0
0 0
0 0
0 0
1 0

0 5
Linear Oscillator (motion-tracked)
0 0
Linear Oscillator (simulation)
0 0
Pendulum (motion-tracked)
Pendulum (simulated)
4 9
Double Oscillator (motion-tracked) 0 0
Double Oscillator (simulated)
0 0
Double Pendulum (motion-tracked) 0 0
0 0
Double Pendulum (simulated)

2
4
0
0

3
5
0
0
0
0
1

9
5
0
9
0
0
0
0

9
9
10
0

10
10
9
1
3
7
6

1
0
0
0
0
0
0
10

10
10
10
0

10
10
10
3
10
10
10

9
10
10
10
0
0
0
0

10
10
10
8

10
10
10
10
10
10
10

10
10
10
10
6
10
7
10

Total

21 16 29

85 122

171

Table 4. Number of successful runs. A run is successful if the SNMSE for the inte-
grated system is < 0.01. Algorithm conﬁgurations: numeric diﬀerences (D), numeric
IVP solution (I), combination of numeric diﬀerences and IVP solution (I+D). The
conﬁgurations using the subscript opt include parameter optimization.

Research Association and the Federal Ministry of Digital and Economic Aﬀairs
within the Josef Ressel Centre for Symbolic Regression

References

1. Bongard, J., Lipson, H.: Automated reverse engineering of nonlinear dynamical
systems. Proc. of the National Academy of Sciences 104(24), 9943–9948 (2007)
2. Chen, T.Q., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.: Neural or-
dinary diﬀerential equations.
In: Bengio, S., Wallach, H., Larochelle, H.,
Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in Neural Infor-
mation Processing Systems 31, pp. 6571–6583. Curran Associates, Inc. (2018),
http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf

3. Gaucel, S., Keijzer, M., Lutton, E., Tonda, A.: Learning dynamical systems using
standard symbolic regression. In: Genetic Programming. pp. 25–36. Springer Berlin
Heidelberg, Berlin, Heidelberg (2014)

4. Gronwall, T.: Note on the derivatives with respect to a parameter of the solutions fo
a system of diﬀerential equations. The Annals of Mathematics 20, 292–296 (1919)
pro-
(2008).

5. Iba, H.:
gramming.
https://doi.org/https://doi.org/10.1016/j.ins.2008.07.029

equation models
178(23),

of
Information

diﬀerential
Sciences

Inference

by
–

genetic

4453

4468

6. Isermann, R., M¨unchhof, M.: Identiﬁcation of dynamic systems: an introduction

with applications. Springer, Berlin, Heidelberg (2011)

7. Kommenda, M., Kronberger, G., Winkler, S., Aﬀenzeller, M., Wagner, S.: Eﬀects
of constant optimization by nonlinear least squares minimization in symbolic re-
gression. In: Proceedings of the 15th annual conference companion on genetic and
evolutionary computation. pp. 1121–1128. ACM (2013)

8. Rackauckas, C., Ma, Y., Dixit, V., Guo, X., Innes, M., Revels, J., Nyberg,
J., Ivaturi, V.: A comparison of automatic diﬀerentiation and continuous sen-
sitivity analysis for derivatives of diﬀerential equation solutions. arXiv preprint
arXiv:1812.01892 (2018)

9. Schmidt, M., Lipson, H.: Data-mining dynamical systems: Automated symbolic
system identiﬁcation for exploratory analysis. In: 9th Biennial Conference on En-
gineering Systems Design and Analysis, Volume 2: Automotive Systems; Bioengi-
neering and Biomedical Technology; Computational Mechanics; Controls; Dynam-
ical Systems. ASME, Haifa, Israel (July 2008)

10. Schmidt, M., Lipson, H.: Distilling free-form natural laws from experimental data.

Science 324(5923), 81–85 (2009). https://doi.org/10.1126/science.1165893

11. Schmidt, M., Lipson, H.: Supporting online material for distilling free-form natrual

laws from experimental data. Online (April 2009)

12. Sengupta, B., Friston, K.J., Penny, W.D.: Eﬃcient gradient computation for dy-

namical models. Neuroimage 98, 521–527 (2014)

13. Topchy, A., Punch, W.F.: Faster genetic programming based on local gradient
search of numeric leaf values. In: Proceedings of the 3rd Annual Conference on Ge-
netic and Evolutionary Computation. pp. 155–162. Morgan Kaufmann Publishers
Inc. (2001)

14. Worm, T., Chiu, K.: Scaling up prioritized grammar enumeration for scientiﬁc
discovery in the cloud. In: 2014 IEEE International Conference on Big Data. pp.
621–626. IEEE (2014)

