DLTPy: DEEP LEARNING TYPE INFERENCE OF PYTHON FUNCTION
SIGNATURES USING NATURAL LANGUAGE CONTEXT

9
1
0
2
c
e
D
2

]
E
S
.
s
c
[

1
v
0
8
6
0
0
.
2
1
9
1
:
v
i
X
r
a

Casper Boone

Niels de Bruin

Delft University of Technology
c.c.boone@student.tudelft.nl

Delft University of Technology
d.j.m.debruin@student.tudelft.nl

Arjan Langerak

Fabian Stelmach

Delft University of Technology
a.c.langerak@student.tudelft.nl

Delft University of Technology
f.p.stelmach@student.tudelft.nl

November, 2019

Abstract

Due to the rise of machine learning, Python is an increasingly popular programming language.
Python, however, is dynamically typed. Dynamic typing has shown to have drawbacks when a project
grows, while at the same time it improves developer productivity. To have the beneﬁts of static typing,
combined with high developer productivity, types need to be inferred. In this paper, we present DLTPy:
a deep learning type inference solution for the prediction of types in function signatures based on
the natural language context (identiﬁer names, comments and return expressions) of a function. We
found that DLTPy is eﬀective and has a top-3 F1-score of 91.6%. This means that in most of the
cases the correct type is within the top-3 predictions. We conclude that natural language contained in
comments and return expressions are beneﬁcial to predicting types more accurately. DLTPy does not
signiﬁcantly outperform or underperform the previous work NL2Type for Javascript, but does show
that similar prediction is possible for Python.
Index terms— deep learning, natural language, type inference, python

1 Introduction

Programming languages with dynamic typing, such as
Python or JavaScript, are increasingly popular. In fact,
supported by the increasing use of machine learning,
Python is currently the top programming language in
the IEEE Spectrum rankings [1]. Dynamically typed
languages do not require manual type annotations and
only know the types of variables at run-time. They pro-
vide much ﬂexibility and are therefore very suitable for
beginners and for fast prototyping. There are, however,
drawbacks when a project grows large enough that no
single developer knows every single code element of the
project. At that point, statically typed languages can
check certain naming behaviors based on types auto-
matically whereas dynamic typing requires manual in-
tervention. While there is an ongoing debate on static
vs. dynamic typing in the developer community [2],
both excel in certain aspects [3]. There is scientiﬁc ev-
idence that static typing provides certain beneﬁts that
are useful when software needs to be optimized for ef-

ﬁciency, modularity or safety [4]. The beneﬁts include
better auto-completion in integrated development en-
vironments (IDEs) [5], more eﬃcient code generation
[4], improved maintainability [6], better readability of
undocumented source code [6], and preventing certain
run-time crashes [5]. It has also been shown that stat-
ically typed languages are "less error-prone than func-
tional dynamic languages" [2].

Weakly typed languages, such as JavaScript, PHP or
Python do not provide these beneﬁts. When static typ-
ing is needed, there are usually two solutions available.
Either using optional type syntax within the language
or to use a variant of the language, which is essentially
a diﬀerent language, that does have a type system in
place. For JavaScript, there are two often used solu-
tions: Flow [7], which uses type annotations within
JavaScript, and TypeScript [8], a JavaScript variant.
PHP oﬀers support for type declarations since PHP 7.0
1, and checks these types at run-time. A well-known

1https://www.php.net/manual/en/migration70.new-

features.php

1

 
 
 
 
 
 
PHP variant that has strong typing is HackLang [9], by
Facebook. Python has support for typing since Python
3.5 2. It does not do any run-time checking, and there-
fore these types do not provide any guarantees if no
type checker is run. The type checker mypy [10] is the
most used Python type checker.

Type inference for Python has been addressed from
multiple angles [11, 12, 13, 14, 15]. However, these so-
lutions require some manual annotations to provide ac-
curate results. Having to provide manual annotations
is one of the main arguments against static typing be-
cause this lowers developer productivity.

In an attempt to mitigate this need for manual an-
notations and support developers in typing their code-
bases, we present DLTPy: a deep learning type infer-
ence solution based on natural language for the pre-
diction of Python function types. Our work focuses on
answering the question of how eﬀective this approach
is.

DLTPy follows the ideas behind NL2Type [5], a sim-
ilar learning-based approach for JavaScript function
types. Our solution makes predictions based on com-
ments, on the semantic elements of the function name
and argument names, and on the semantic elements
of identiﬁers in the return expressions. The latter is
an extension of the ideas proposed in [5]. The idea to
use natural language contained in the parameter names
for type predictions in Python is not new, Zhaogui Xu
et al. already used this idea to develop a probabilis-
tic type inferencer [11]. Using the natural language of
these diﬀerent elements, we can train a classiﬁer that
predicts types. Similar to [5] we use a recurrent neu-
ral network (RNN) with a Long Short-Term Memory
(LSTM) architecture [16].

Using 5,996 open source projects mined from GitHub
and Libraries.io that are likely to have type annota-
tions, we train the model to predict types of func-
tions without annotations. This works because code
has been shown to be repetitive and predictable [17].
We make the assumption that comments and identiﬁers
convey the intent of a function [5].

We train and test multiple variants of DLTPy to
evaluate the usefulness of certain input elements and
the success of diﬀerent deep learning models. We ﬁnd
that return expressions are improving the accuracy of
the model, and that including comments has a positive
inﬂuence on the results.

DLTPy predicts types with a top-3 precision of
91.4%, a top-3 recall of 91.9%, and a top-3 F1-score
of 91.6%. DLTPy does not signiﬁcantly outperform or
underperform the previous work NL2Type [5].
This paper’s contributions are three-fold:

1. A deep learning network type inference system for

inferring types of Python functions

2https://www.python.org/dev/peps/pep-0484/

2

2. Evaluation of the usefulness of natural language
encoded in return expressions for type predictions

3. Evaluation of diﬀerent deep learning models that
indicates their usefulness for this classiﬁcation task

2 Method

DLTPy has two main phases: a training phase and a
prediction phase. In this section, we ﬁrst describe the
steps involved in the training process, and then dis-
cuss how prediction works, given the trained model.
The training process consists of multiple steps. First,
we extract relevant training data from Python projects
(section 2.1). Next, we preprocess the training data by
for instance lemmatizing the textual parts of the data
(section 2.2). The preprocessed training data is then
ﬁltered and only relevant functions are selected (section
2.3). Then, we generate input vectors using word em-
beddings and one-hot encoding (section 2.4). Finally,
we train an RNN (section 2.5). After the training pro-
cess has completed, the trained RNN can be used to
make predictions for function types (section 2.6).

2.1 Collecting Data from ASTs

For each Python project in our data set, we want to
export relevant parts of functions. Every Python ﬁle
is parsed to an abstract syntax tree (AST). From this
AST, we ﬁnd the functions within or outside a class
in the Python ﬁle. For each function, we extract the
following elements:

• n f : The name of the function

• d f : The docstring of the function

• c f : The comment of the function

• np : A list of the names of the function parameters

• tp : A list of the types of the function parameters

• cp : A list of the comments describing function pa-

rameters

• er : A list of the return expressions of the function

• tr : The return type of the function

• cr : The comment describing the return value

these

elements

Together,

form the

tuple
(n f , d f , c f , np , tp , cp , er , tr , cr ).
Figure 1a shows a
code sample, this sample is parsed and the informa-
tion for the tuple is extracted as described in Figure
1b. This tuple is similar to the input data used in
NL2Type [5], except for d f and er .

3

Figure 1: Overview of the process of training from annotated source code.

d f is the docstring of the Python function. This doc-
string often contains a few lines of text describing the
working of the function, and sometimes also contains
information about the parameters or the return value.
In some cases, a more structured format is used, such
as ReST, Google, or NumPy style. These formats de-
scribe parameters and the return value, separately from
the function description. In these cases, we can extract
this information for c f , cp , and cr . We extract these
comments only if the docstring is one of the structured
formats mentioned before.

er is a list of return expressions of the function. After
the preprocessing step (section 2.2), this contains a list
of all the identiﬁers and keywords used in the return
expressions. The intuition is that often variable names
are returned and that these names may convey useful
information.

2.2 Preprocessing

The information in the tuple is still raw natural lan-
guage text. To capture only the relevant parts of the
text, we ﬁrst preprocess the elements in the tuple.
The preprocessing pipeline consists of four steps and
is based on the preprocessing stage in [5]:

1. Remove punctuation, line breaks, and digits We
replace all non-alphabetical characters.
Line
breaks are also removed to create a single piece of
text. We replace a full stop that is not at the end
of a sentence with a space. We do this to make sure
that, for instance, an object ﬁeld or function access

is not treated as a sentence separator (for example
object.property becomes object property).

2. Tokenize We tokenize sentences using spaces as a
separator. Before tokenization, the underscores in
snake case and camel case identiﬁers are converted
to a space-separated sequence of words.

3. Lemmatize We convert all inﬂected words to their
lemma. For example, “removing” and “removed”
become “remove”.

4. Remove stop words We remove stopwords (such
as “was”, “be”, “and”, “while” and “the” 3) from
the sentences because these words are often less
relevant and thus more importance can be given
to non-stopwords. This step is not included in the
pipeline for identiﬁers (function names, parameter
names, and return expressions), considering that
in the short sentences these identiﬁers form, stop-
words are more relevant.

An example of a preprocessed tuple is shown in Fig-

ure 1c.

2.3 Function Selection

After collecting and preprocessing the function tuples,
we select relevant functions. We ﬁlter the set of func-
tions on a few criteria.

3See https://gist.github.com/sebleier/554280 for a full

list of stopwords

	def	exists(self,	first_name:	str,	last_name:	str)	->	bool:			"""			Check	if	a	customer	with	the	given	name	exists	in	the	database.				:param	first_name:	The	first	name	of	the	customer			:param	last_name:	The	last	name	of	the	customer			:return:	True	if	customer	is	present			"""			name	=	f'{first_name}	{last_name}'			customer_present	=	self.customers.find(name).any()			return	customer_present	(			exists,			Check if ... is present,			Check if ... the database,			[self,	first_name,	last_name],			[str,	str],			[The ﬁrst .. the customer, The last ... the customer],				[return	customer_present],			bool,			True	if	customer	is	present	)	(			exists,			check customer ... customer present,			check customer ... exist database,			[first	name,	last	name],			[str,	str],			[ﬁrst name customer, last name customer],				[customer	present],			bool,			true	customer	present				)	(			exists,			check customer ... customer present,			check customer ... exist database,			[first	name,	last	name],			[str,	str],			[ﬁrst name customer, last name customer],				[customer	present],			bool,			true	customer	present				)	(first	name,ﬁrst name customer,	str)		(last	name,last name customer,	str)		Text	(			exists,	check customer ... exist database,       true customer present,	customer	present,							first	name	last	name,	str	)			[		[1	0	0	...	0],		[1	1	1	...	1],		...		[-1.6	3	2.4	...	1.2]		]	[		[1	0	0	...	0],		[1	1	1	...	1],		...		[1.3	-2	-0.4		...	3]		]	[		[0	1	0	...	0],		[1	1	1	...	1],		...		[0.3	1	-.2		...	1.8]		]deep learningmodela) Original source codeb) Extracted tuplec) Preprocessed tupled) Preprocessed tuplee) Parameter and return datapointsf) Datapoints as vectore) Deep learning modelparse and extractpreprocessfunction selectiongenerateparameterdatapointsgeneratereturndatapointsconvert to vectorconvert to vectorxxy	[0	0	1	0	0	0	0	...	0]		[0	0	1	0	0	0	0	...	0]	y	[1	0	0	0	0	0	0	...	0]	Length

Features

Length

Features

4

1

1

6

1

15

1

6

1

12

1

10

Datapoint Type ( [ 1 0 0 ... 0 ]

)

Separator

Name (np,i )

Separator

Comment (cp,i )

Separator

Padding

Separator

Padding

Separator

Padding

Table 1: Vector representation of parameter datapoint.

First, a function must have at least one type in tp or
it must have tr , otherwise, it cannot serve as training
data. A function must also have at least one return
expression in re , since we do not want to predict the
type for a function that does not return anything.

Furthermore, for functions where np contains the pa-
rameter self, we remove this parameter from np , tp
and cp , since this parameter has a speciﬁc role for ac-
cessing the instance of the class in which the method is
deﬁned in. Therefore, the name of this parameter does
not reﬂect any information about its type and is thus
not relevant.

Finally, we do not predict the types None (can be de-
termined statically) and Any (is always correct). Thus,
we do not consider a function for predicting a parame-
ter type if the parameter Any, and a return type if the
return type is Any or None.

2.4 Vector Representation

From the selected function tuples, we create a param-
eter datapoint for each parameter and a return data-
point. We convert these datapoints to a vector. We ex-
plain the structure of these vectors in 2.4.1. All textual
elements are converted using word embeddings (see
2.4.2), and types using one-hot encoding (see 2.4.3).

2.4.1 Datapoints and Vector Structure

The format of the input vectors is shown in Table 1
for parameter datapoints, and in Table 2 for return
datapoints. All elements of the features have size 14.
This results in a 55 × 14 input vector.

The lengths of the features are based on an analysis
of the features in our dataset. The results are shown
in Table 3. A full analysis is available in our GitHub
repository (see section 3.1).

The datapoint type indicates whether the vector rep-
resents a parameter or a vector. A separator is a 1-

1

1

6

1

15

1

6

1

12

1

10

Datapoint Type ( [ 1 0 0 ... 0 ]

)

Separator

Function Name (n f )

Separator

Function Comment (c f ) if present,

otherwise Docstring (d f )

Separator

Return Comment (cr )

Separator

Return Expressions (er )

Separator

Parameter Names (np )

Table 2: Vector representation of return datapoint.

vector of size 14. For parameter datapoints, padding
(0-vectors) is used to ensure that the vectors for both
datapoints have the same dimensions.

2.4.2 Learning Embeddings

It is important that semantically similar words result in
vectors that are close to each other in the n-dimensional
vector space, hence we cannot assign random vectors
to words. Instead, we train an embeddings model that
builds upon Word2Vec [18]. Since the meaning of cer-
tain words within the context of a (speciﬁc) program-
ming language are diﬀerent than the meaning of those
words within the English language, we cannot use pre-
trained embeddings.

We train embeddings separately for comments and
identiﬁers. Comments are often long (sequences of)
sentences, while identiﬁers can be seen as short sen-
tences. Similarly to [5], we train two embeddings, be-
cause the identiﬁers “tend to contain more source code-
speciﬁc jargon and abbreviations than comments”.

Using the trained model, we convert all textual ele-

ments in the datapoints to sequences of vectors.

For the training itself, all words that occur 5 times
or less are not considered to prevent overﬁtting. Since
Word2Vec learns the context of a word by considering
a certain amount of neighbouring words in a sequence,
this amount of set to 5. The dimension of the word
embedding itself is found by counting all the unique
words found in the comments and identiﬁers and taking
the 4th root of the result as suggested in [19]. This
results in a recommended dimension of 14.

2.4.3 Representing Types

The parameter types and return type are not embed-
ded, however, we also encode these elements as vec-
tors. We use a one-hot encoding [20] that encodes to

5

Feature

Average

Median

Max

Chosen

Fully Covered

Length

Length

Length

Length

Data Points

Function Name*

2.28

Function Comment**

6.31

Return Comment

Return Expressions

Parameter names

Parameter Name*

Parameter Comment

8.40

6.01

2.84

1.45

6.38

2

5

5

3

2

1

4

11

482

533

1810

72

8

2491

6

15

6

12

10

6

15

99,77%

97,98%

65,98%

89,52%

97,67%

99,99%

93,46%

Table 3: The chosen vector lengths of the features.

Type

str

int

bool

float

Dict[str, Any]

Optional[str]

List[str]

dict

Type

1

2

3

4

5

6

7

8

9

10

torch.Tensor

Percentage

28,9%

11,7%

10,8%

2,9%

2,4%

2,2%

2,1%

1,5%

1,4%

1,3%

Table 4: The top 10 most frequent types in our dataset.

vectors of length |T f r equent |, where T f r equent is the set
of types that most frequently occur within the dataset.
We also add the type “other” to T f r equent to represent
all types not present in the set of most frequently oc-
curring types. We only select the most frequent types
because there is not enough training data for less fre-
quent types, resulting in a less eﬀective learning pro-
cess. The resulting vector for a type has all zeros except
at the location corresponding to the type, for example,
the type str may be encoded as [0, 1, 0, 0, ..., 0].

We limit the set T f r equent to the 1000 most frequent
types, as this has shown to be an eﬀective number in
earlier work [5]. We show the top 10 of the most fre-
quent types in Table 4.

natural language information.

We implement the RNN using LSTM units [21].
LSTM units have been successfully applied in NL2Type
[5], where the choice for LSTMs is made based on the
use for classiﬁcation tasks similar to our problem. We
describe the full details of the model in 3.4.1.

2.6 Prediction using the trained RNN
After training is done, the model can be used to predict
the type for new, unseen, functions. The input to the
model is similar to the input during the training phase.
This means that ﬁrst a function needs to be collected
from an AST (section 2.1), then the function elements
need to be preprocessed (section 2.2, and ﬁnally, the
function must be represented as multiple vectors for the
parameter types and for the return type as described
in 2.4.

The model can now be queried for the input vectors
to predict the corresponding types. The network out-
puts a set of likely types together with the individual
probability of the correctness of these types.

3 Evaluation

We evaluate the performance of DLTPy by creating an
implementation, collecting training data, and training
the model based on this data. We judge the results
using the metrics precision, recall, and F1-score. We
also perform experiments using variants of DLTPy for
comparison.

2.5 Training the RNN

Given the vector representations described in section
(2.4) we want to learn a function that would map
the input vectors x of dimensionality k to one of the
1000 types T, hence that would create the mapping
(cid:82)x∗k − > (cid:82)|T |. To learn this mapping, we train a recur-
rent neural network (RNN). An RNN has feedback con-
nections, giving it memory about previous input and
therefore the ability to process (ordered) sequences of
text. This makes it a good choice when working with

3.1 Implementation
We implement DLTPy in Python. We use GitPython
[22] for cloning libraries, astor [23] to parse Python
code to ASTs, docstring_parser [24] for extracting
comment elements, and NLTK [25] for lemmatization
and removing stopwords. We train two word embed-
ding models using the Word2Vec [18] library by gensim
[26]. The data is represented using Pandas dataframes
[27] and NumPy vectors [28]. Finally, the prediction

models are developed using PyTorch [29], a machine
learning framework.

We make the source for training and evaluating
DLTPy publicly available at https://github.com/
casperboone/dltpy/.

3.2 Experimental Setup

We collect training data from open-source GitHub
projects. We ﬁrst select projects by looking at mypy
[10] dependents in two ways. First, we look at the
dependents listed on GitHub’s dependency graph 4.
Then, we complete this list with the dependents listed
by Libraries.io 5. The intuition is that Python
projects using the type checker mypy are more likely
to have types than projects that do not have this de-
pendency. This results in a list of 5,996 projects, of
which we can download and process (section 2.1) 5,922
projects. 74 projects fail, for instance due to the un-
availability of a project on GitHub.

Together, these projects have 555,772 Python ﬁles.
5,054 ﬁles cannot be parsed. The ﬁles contain 4,977,420
functions, of which only have 585,413 functions have
types (at least one parameter type or a return type).

We complement this dataset with projects for which
type hints are provided in the Typeshed repository6.
These are curated type annotations for the Python
standard library and for certain Python packages. We
manually ﬁnd and link the source code of 35 packages,
including the standard library. Using retype we insert
the type annotations into the source code [30]. These
projects have 29,759 functions, of which 246 have types.
We randomly split this set of datapoints into a non-
overlapping training set (80%) and a test set (20%).
We repeat every experiment 3 times.

All experiments are conducted on a Windows 10 ma-
chine with an AMD Ryzen 9 3900X processor with
12 cores, 32 GB of memory, and an NVIDIA GeForce
GTX 1080 Ti GPU with 11 GB of memory.

3.3 Metrics

We evaluate the performance based on the accuracy of
predictions. We measure the accuracy in terms of pre-
cision (the fraction of found results that are relevant),
recall (the fraction of relevant results found) and F1-
score (harmonic mean of precision and recall). Because
in many contexts (for example IDE auto-completion)
it is not necessary to restrict to giving a single good
prediction, we look at the top-K predictions. Speciﬁ-
cally, we collect the three metrics for the top-1, top-2,
and top-3 predictions.

4https://github.com/python/mypy/network/dependents
5https://libraries.io/pypi/mypy/usage
6https://github.com/python/typeshed

6

We deﬁne p as the total number of predictions,
pv al i d as the number of predictions for which the pre-
diction is not other (and thus the model cannot make
a prediction). We deﬁne the three metrics as follows:

pv al i d_cor r ect
pv al i d

• t op-K pr eci si on =
where pv al i d_cor r ect
is the number of valid pre-
dictions for which the correct prediction is in the
top-K

,

• t op-K r ecal l =

pv al i d_cor r ect
p

,

• t op-K F 1 = 2 ×

t op-K pr eci si on × t op-K r ec al l
t op-K pr eci si on + t op-K r ec al l

.

3.4 Experiments and Models

While we mainly evaluate the performance of DLTPy
as described in the previous section, we also try, eval-
uate and compare the results when we use diﬀerent
models or diﬀerent input data. We train three diﬀer-
ent models and compare these to the model presented
in section 2.5. Also, we evaluate the results of selecting
diﬀerent input elements.

3.4.1 Models

We implement and train three models to evaluate and
compare their performance.

• Model A In the ﬁrst model, we made use of two
stacked LSTMs. Both LSTMs have a hidden size
of 14. The ﬁrst LSTM will feed its sequence fully
into the second, whereas for the second LSTM we
only use the output of the last unit and feed it into
a fully connected linear layer. Softmax is applied
to generate an approximation of the probability
for each type. The model has a total of 37,288
parameters.

• Model B In the second model, we take an approach
similar to Model A: we feed the input vector into
a Gated Recurrent Unit (GRU) [31] and feed the
output to a fully connected linear layer converting
into the output types. The model has a total of
11,780 parameters.

• Model C The third model is the model architec-
ture proposed by the authors of [5]. A single bi-
directional LSTM with a hidden layer of size 256
is fed into a fully connected layer with output size
1000. Due to the size of this model and the time
it took to train, the training consisted of only 25
epochs, compared to 100 epochs for the other three
models. The model has a total of 404,456 param-
eters.

7

Dataset

1 Complete

Size

Parameter

Return

Datapoints

Datapoints

Dimensions

84,661

64,690

19,971

55 × 14

2 Optional parameter and return comment

863,936

719,581

144,355

55 × 14

3 Optional docstring

4 Without return expressions

5 Without return expressions, lower dimension

1,018,787

719,581

299,206

55 × 14

84,661

84,661

64,690

64,690

19,971

55 × 14

19,971

42 × 14

Table 5: The sizes of the datasets used to evaluate DLTPy.

3.4.2 Datasets

4 Results

We try and evaluate variations in input data to mea-
sure the impact of certain elements in the datapoints.
To this purpose, we create ﬁve datasets. The size of
these datasets is listed in Table 5.

1. Complete This is the dataset as described in sec-
tion 2. All datapoints in this dataset are complete.
This means that the parameter datapoints have
cp , and the return datapoints have c f , cr , and er .

2. Optional parameter and return comment In this
dataset we make the parameter and return com-
ment optional. The presence of a docstring is still
required. This means that parameter datapoints
do not have to have cp (91,01%), and the return
datapoints do not have to have cr (84,79%), but
still have c f (which is either the parsed function
comment or the docstring) and er .

3. Optional docstring In this dataset we make the
docstring optional. This means that parameter
datapoints do not have to have cp (91,01%), and
the return datapoints do not have to have c f and
cr (92,66%), but still have er (51,75%). This can
be seen as a dataset without comments since only
20,52% of the datapoints in this set have com-
ments. This means that the prediction for param-
eters is purely based on the parameter name, and
for return datapoints, the prediction is based on
the function name, the return expressions, and the
parameter names.

4. Without return expressions To evaluate the use-
including return expressions in the
fulness of
model input, we perform the classiﬁcation task
also without return expressions.
In this dataset
all vectors representing parts of return expressions
are 0-vectors.

5. Without return expressions, lower dimension This
dataset is similar to the previous one.
In this
dataset, however, all vectors representing parts of
return expressions are removed, resulting in lower-
dimensional input vectors.

In this section we present the results of DLTPy. First,
we show and compare the results of our diﬀerent ex-
periments as described in section 3.4 using the metrics
described in section 3.3. Then, we compare the results
to previous work, speciﬁcally to NL2Type [5].

4.1 Models

We presented three diﬀerent architectures for our pre-
diction model (section 3.4. In Table 6 we present the
results. The underlined metric scores indicate the best
score for each model. The bold metric scores indicate
the best overall score. We use these scores to compare
the performance of the models.

Model C clearly outperforms models A and B in all
metrics. The top-1 F1-score is 82.4%, and for the top-3
it is 91.6%. The top-3 recall is 91.9%, this can be in-
terpreted as the model predicts the correct type within
its ﬁrst three suggestions in 91.9% of the cases it was
asked to make a prediction for.

Model B performs poorly compared to model A and
C. Using a GRU does not have a positive inﬂuence
on the results.
It is interesting to note that where
model C performs best on dataset 1, model B does
not beneﬁt from the return expressions and performs
better without them (dataset 4).

The performance of model A lies in between the per-
formance of model B and C. Interestingly, this model
performs best on the dataset with the lowest dimen-
sions. Dataset 4 and 5 contain the same data, the only
diﬀerence is that dataset 4 has an additional separator
vector and twelve 0-vectors. The same impact can also
be seen for model C when comparing dataset 4 and 5.
A possible explanation could be that the learning pro-
cess can converge faster because there is simply less to
learn, however, we have not found a provable cause for
this.

4.2 Input Elements Selection

We presented the elements of the datapoints of DLTPy
and four variations (section 3.4) that we compare with.

8

Model

Dataset

A

1 Complete

2 Optional parameter and return comment

3 Optional docstring

4 Without return expressions

5 Without return expressions, lower dimension

B

1 Complete

2 Optional parameter and return comment

3 Optional docstring

4 Without return expressions

5 Without return expressions, lower dimension

Top-1

Top-2

Top-3

Prec.

Rec.

F1

Prec.

Rec.

F1

Prec.

Rec.

F1

60.3

51.4

52.9

60.2

63.9

41.2

34.3

36.6

42.2

40.4

67.7

62.0

62.8

68.6

70.3

54.7

51.0

52.7

55.9

54.7

63.8

56.2

57.4

64.1

66.9

47.0

41.0

43.2

48.1

46.5

71.7

65.6

66.1

72.8

74.8

55.2

44.3

45.7

55.8

54.7

76.7

69.7

70.1

77.5

78.6

63.8

57.2

57.4

64.5

63.8

74.1

67.6

68.0

75.0

76.6

59.2

49.9

50.9

59.8

58.9

76.8

72.7

72.9

78.2

80.1

61.7

54.4

56.5

62.2

61.1

81.2

75.7

75.6

82.0

83.2

70.0

63.6

64.1

70.5

69.8

79.0

74.1

74.2

80.1

81.6

65.6

58.6

60.1

66.1

65.2

C

1 Complete

81.7

83.2

82.4

88.4

89.2

88.8

91.4

91.9

91.6

2 Optional parameter and return comment

3 Optional docstring

4 Without return expressions

5 Without return expressions, lower dimension

69.3

70.7

79.1

81.0

70.0

71.2

81.3

82.8

69.6

71.0

80.2

81.9

78.5

79.8

86.6

88.1

78.3

79.2

88.0

89.0

78.4

79.5

87.3

88.5

83.5

84.5

90.0

91.2

83.1

83.8

90.9

91.6

83.3

84.1

90.4

91.4

Table 6: The evaluation results of DLTPy.

Top-1

Top-3

Prec.

Rec.

F1

Prec.

Rec.

F1

DLTPy

81.7

83.2

82.4

DLTPy Dataset 4

79.1

NL2Type

84.1

81.3

78.9

80.2

81.4

91.4

90.0

95.5

91.9

90.9

89.6

91.6

90.4

92.5

Table 7: A comparison of DLTPy to NL2Type [5].

In Table 6 we present the results. For this comparison,
we look at the results of model C, the best performing
model.

The results of dataset 2 and 3 are signiﬁcantly worse
than of dataset 1. This shows that the natural language
contained in comments positively inﬂuences the type
classiﬁcation task.
Interestingly, the performance of
dataset 3 is slightly less good than of dataset 2, while
dataset 2 contains more comments.

Another observation from the results is that the
predictions positively inﬂuence from including return
expressions in the dataset. We see that the perfor-
mance is less good when return expressions are not in-
cluded (datasets 4 and 5) than when they are included
(dataset 1).

4.3 Comparison to previous work

section, we compare our

In this
results against
NL2Type [5]. DLTPy operates in a similar way on
similar data as NL2Type. However, NL2Type predicts
types for JavaScript ﬁles and DLTPy predicts types for
Python ﬁles. Also, DLTPy uses a diﬀerent input repre-
sentation (diﬀerent feature lengths and return expres-

sions as an addition) and a diﬀerent word embedding
size (14 instead of 100). This makes it interesting to
compare our results against this work.

From the results, we cannot observe that DLTPy sig-
niﬁcantly outperforms or underperforms NL2type (see
Table 7). It is, however, interesting to note that with-
out return expressions (dataset 4), DLTPy would un-
derperform. Another interesting observation is that
the results did not have a negative impact on the sig-
niﬁcantly smaller word embedding size.

5 Conclusion

Our work set out to study the applicability of using
natural language to infer types of Python function pa-
rameters and return values. We present DLTPy as a
deep learning type inference solution based on natural
language for the prediction of these types. It uses in-
formation from function names, comments, parameter
names, and return expressions.

The results show that DLTPy is eﬀective at predict-
ing types using natural language information. The top-
1 F1-score is 82.4%, and the top-3 F1-score is 91.6%.
This shows that in most of the cases the correct answer
is in the top 3 of predictions. The results show that
using natural language information from the context of
a function and using return expressions have a positive
impact on the results of the type prediction task.

We do not signiﬁcantly outperform or underperform
NL2Type. Without our additions to the ideas behind
NL2Type, however, DLTPy would underperform. This
shows that the main idea behind NL2Type, namely us-
ing natural language information for predicting types,

is generalizable from JavaScript to Python, but ad-
ditional
is
needed to get comparable results.

information, such as return expressions,

We identify two threats to the validity of our re-
sults. The ﬁrst threat is that there is no separation of
functions between the training and test set on project
level. Because functions within the same project are
more likely to be similar, this might inﬂuence the va-
lidity of our results. Also, since the best performing
model, model C, has 404,456 parameters and the best
performing dataset, dataset 1, has just 84,661 data-
points, which increases the risk of overﬁtting.

DLTPy has limitations that can be improved upon
in future work. Dataset 1, the complete dataset, is
relatively small. A better data retrieval strategy that
goes beyond looking at mypy dependents might result
in more data points and thus allows for better training
resulting in more accurate results. Furthermore, the
predictions of DLTPy are currently restricted to the
1000 most frequently used types in Python. Open type
predictions would improve the practical use of DLTPy,
given that there is enough training data available for
the types that are less frequent.

References

[1] “Interactive:

The

Top
2019.

Languages,”

Program-
[Online].
https://spectrum.ieee.org/static/

ming
Available:
interactive-the-top-programming-languages-2019
[2] B. Ray, D. Posnett, P. Devanbu, and V. Filkov,
“A large-scale
study of programming languages
and code quality in GitHub,” Communications of
the ACM, vol. 60, no. 10, pp. 91–100, 9 2017.
[Online]. Available: http://dl.acm.org/citation.cfm?
doid=3144574.3126905

[3] E. Meijer and P. Drayton, “Static Typing Where Pos-
sible , Dynamic Typing When Needed : The End of
the Cold War Between Programming Languages,” in
OOPSLA’04 Workshop on Revival of Dynamic Lan-
guages, 2004, New York, NY, USA, 2004.

languages

in Proceedings of

[4] M. M. Vitousek, A. M. Kent, J. G. Siek, J. Baker,
M. M. Vitousek, A. M. Kent, J. G. Siek, and
J. Baker, “Design and evaluation of gradual typing
the 10th ACM
for python,”
Symposium on Dynamic
- DLS ’14,
vol. 50, no. 2. New York (New York, USA): ACM
Press, 2014, pp. 45–56.
[Online]. Available: http:
//dl.acm.org/citation.cfm?doid=2661088.2661101
[5] R. S. Malik, J. Patra, and M. Pradel, “NL2Type:
Inferring JavaScript Function Types from Natural
Language Information,”
in 2019 IEEE/ACM 41st
International Conference on Software Engineering
(ICSE).
IEEE, 5 2019, pp. 304–315. [Online]. Avail-
able: https://ieeexplore.ieee.org/document/8811893/
[6] S. Hanenberg, S. Kleinschmager, R. Robbes, E. Tan-
“An empirical study on the
typing on software maintain-

ter, and A. Steﬁk,
impact of

static

9

ability,” Empirical Software Engineering, vol. 19,
no. 5, pp. 1335–1382, 10 2014.
[Online]. Available:
http://link.springer.com/10.1007/s10664-013-9289-1

[7] “Flow,” 2014. [Online]. Available: https://ﬂow.org/
[8] “TypeScript,”

[Online]. Available:

2012.

https:

//www.typescriptlang.org/

[9] “HackLang,”

2014.

[Online]. Available:

https://

hacklang.org/

[10] “mypy,” 2012. [Online]. Available: http://mypy-lang.

org/

support,”

[11] Z. Xu, X. Zhang, L. Chen, K. Pei, and B. Xu,
“Python probabilistic type inference with natural
language
the 2016
24th ACM SIGSOFT International Symposium on
Foundations of Software Engineering - FSE 2016.
New York, New York, USA: ACM Press, 2016,
pp. 607–618. [Online]. Available: http://dl.acm.org/
citation.cfm?doid=2950290.2950343

in Proceedings of

[12] M. Salib,

“Faster than C: Static type inference
with Starkiller,” PyCon Proceedings, Washington DC,
vol. 3, 2004.

[13] R. A. MacLachlan, “The python compiler for cmu com-
mon lisp,” ACM SIGPLAN Lisp Pointers, no. 1, pp.
235–246, 1992.

[14] M. Hassan, C. Urban, M. Eilers, and P. Müller,
“MaxSMT-Based Type Inference for Python 3,”
in CAV (2), volume 10982 of Lecture Notes in
Computer Science.
Springer, Cham, 7 2018, pp.
12–19. [Online]. Available: http://link.springer.com/
10.1007/978-3-319-96142-2_2

[15] E. Maia, N. Moreira, and R. Reis, “A static type in-

ference for python,” Proc. of DYLA, 2012.

[16] S. Hochreiter and J. Schmidhuber, “Long Short-Term
Memory,” Neural Computation, vol. 9, no. 8, pp. 1735–
1780, 11 1997.

“On the naturalness of software,”

[17] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and
in
P. Devanbu,
2012 34th International Conference on Software
Engineering (ICSE).
IEEE, 6 2012, pp. 837–
847.
[Online]. Available: http://ieeexplore.ieee.org/
document/6227135/

[18] T. Mikolov, K. Chen, G. Corrado, and J. Dean,
“Eﬃcient Estimation of Word Representations in
Vector Space,” 1 2013.
http:
//arxiv.org/abs/1301.3781

[Online]. Available:

Team,
Columns,”

[19] TensorFlow
Feature
able:
introducing-tensorﬂow-feature-columns.html

TensorFlow
Avail-
https://developers.googleblog.com/2017/11/

“Introducing
2017.

[Online].

[20] J. Neter, M. H. Kutner, C. J. Nachtsheim, and
W. Wasserman, Applied linear statistical models.
Ir-
win Chicago, 1996, vol. 4.

continual prediction with LSTM,”

[21] F. Gers, J. Schmidhuber, and F. Cummins, “Learning
to forget:
in
9th International Conference on Artiﬁcial Neural
Networks:
IEE, 1999, pp.
850–855. [Online]. Available: https://digital-library.
theiet.org/content/conferences/10.1049/cp_19991218
https://

ICANN ’99, vol. 1999.

[Online]. Available:

[22] “GitPython,”

2008.

github.com/gitpython-developers/GitPython

[23] “astor,” 2012.

[Online]. Available:

https://github.

10

com/berkerpeksag/astor

[24] “docstring_parser,” 2018. [Online]. Available: https:

//github.com/rr-/docstring_parser

[25] S. Bird, E. Klein, and E. Loper, Natural Language Pro-
cessing with Python, 1st ed. O’Reilly Media, Inc.,
2009.

[26] R. Řeh uřek and P. Sojka, “Software Framework for
Topic Modelling with Large Corpora,” in Proceedings
of the LREC 2010 Workshop on New Challenges for
NLP Frameworks. Valletta, Malta: ELRA, 5 2010,
pp. 45–50.

[27] W. McKinney, “Data Structures for Statistical Com-
puting in Python,” in Proceedings of the 9th Python in
Science Conference, S. van der Walt and J. Millman,
Eds., 2010, pp. 51 – 56.

[28] S. van der Walt, S. C. Colbert, and G. Varoquaux,
“The NumPy Array: A Structure for Eﬃcient
Numerical Computation,” Computing in Science
& Engineering, vol. 13, no. 2, pp. 22–30,
3
2011. [Online]. Available: http://ieeexplore.ieee.org/
document/5725236/

[29] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and
A. Lerer, “Automatic Diﬀerentiation in PyTorch,” in
NIPS Autodiﬀ Workshop, Long Beach, CA, USA, 2017.
[Online]. Available: https://github.

[30] “retype,” 2017.

com/ambv/retype

[31] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bah-
danau, F. Bougares, H. Schwenk, and Y. Ben-
gio,
“Learning phrase representations using RNN
encoder-decoder for statistical machine translation,” in
EMNLP 2014 - 2014 Conference on Empirical Meth-
ods in Natural Language Processing, Proceedings of the
Conference. Association for Computational Linguis-
tics (ACL), 2014, pp. 1724–1734.

