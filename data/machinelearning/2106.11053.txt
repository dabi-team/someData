LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsCatherineWong1KevinEllis2JoshuaB.Tenenbaum13JacobAndreas1AbstractInductiveprogramsynthesis,orinferringpro-gramsfromexamplesofdesiredbehavior,offersageneralparadigmforbuildinginterpretable,ro-bust,andgeneralizablemachinelearningsystems.Effectiveprogramsynthesisdependsontwokeyingredients:astronglibraryoffunctionsfromwhichtobuildprograms,andanefﬁcientsearchstrategyforﬁndingprogramsthatsolveagiventask.WeintroduceLAPS(LanguageforAbstrac-tionandProgramSearch),atechniqueforusingnaturallanguageannotationstoguidejointlearn-ingoflibrariesandneurally-guidedsearchmodelsforsynthesis.Whenintegratedintoastate-of-the-artlibrarylearningsystem(DreamCoder),LAPSproduceshigher-qualitylibrariesandimprovessearchefﬁciencyandgeneralizationonthreedo-mains–stringediting,imagecomposition,andabstractreasoningaboutscenes–evenwhennonaturallanguagehintsareavailableattesttime.1.IntroductionMachinelearningapproachesbasedonprogramsynthesis–theautomaticinferenceofsymbolicprograms–canofferrobustness,interpretability,veriﬁability,andstronggener-alizationinfew-shotlearningsettings(Appeletal.,2017;Lakeetal.,2017).Manymachinelearningtaskscanbeformulatedasprogramsynthesisproblems,includingdatamanipulation(Delawareetal.,2015;Gulwanietal.,2017),semanticparsing(Artzi&Zettlemoyer,2013;Liang,2016),structuredvisualunderstanding(Johnsonetal.,2017b;Yietal.,2018),imagegeneration(Ellisetal.,2017;Ganinetal.,2018),andpolicylearning(Fikes&Nilsson,1971;Cropper&Muggleton,2015;Silveretal.,2020).ThispaperintroducesLanguageforAbstractionandPro-gramSearch(LAPS),aframeworkforimprovingtheef-ﬁciencyandgeneralizabilityoflearnedprogramsynthesis1MIT2CornellUniversity3CenterforBrains,MindsandMachines(CBMM)-MIT.Correspondenceto:CatherineWong<catwong@mit.edu>.Proceedingsofthe38thInternationalConferenceonMachineLearning,PMLR139,2021.Copyright2021bytheauthor(s).modelsusingnaturallanguagesupervision.InLAPS,lan-guageguideslearningofbothlibrariesofreusableprogramabstractionsandheuristicsforsearchinginthespaceofprograms.High-qualityprogramlibrariesandsearchmeth-odsarethemainingredientsofeffectiveprogramsynthesisapproaches(Gulwanietal.,2017).Recentapproachestoprogramsynthesishaveattemptedtolearnsearchmodels(Gulwanietal.,2015;Polozov&Gulwani,2015;Balogetal.,2016;Devlinetal.,2017),programlibraries,orbothjointlyfromdata(Shinetal.,2019;Dumanci´c&Cropper;Ellisetal.,2021;2020;L´azaro-Gredillaetal.,2019),buteventhecurrentbestlearningapproachescanbecomputa-tionallyinefﬁcient(oftenrequiringupwardsofthousandsofCPUhourstobootstraplearning)anddonotalwaysdiscovergeneralizablelibrariesorsearchstrategies.LAPSbuildsontheintuitionthatnaturallanguageoffersapowerfulsourceofinformationfortacklingbothlearningproblems.Languagesimultaneouslyprovidesanefﬁcientchannelforcommunicatingthestructureofthesearchspace(aninstructionlikedrawalargehexagonnexttoasmallpentagondecomposesacomplexgraphicstaskintohigh-levelparts)andalexiconthatnamesimportantreusableconceptsinagivendomain(forinstance,suggestingthatafunctiontodrawvariable-sizedpolygonsmightbeuse-fulforfuturegraphicstasks).Inthisworkweshowhowinducingjointlycompositionalgenerativemodelsovernatu-rallanguageandprogramsprovidesastrongscaffoldforlibraryandsearchmodellearninginahierarchicalprograminductionmodel.Whenintegratedintoastate-of-the-artlearningalgorithm,DreamCoder(Ellisetal.,2021;2018),ourapproachdramaticallyimprovesperformanceonthreedifferentsynthesisdomains:stringediting,structuredimagegenerationandsceneunderstanding.Comparedtothebasesynthesisapproach,LAPSsolvesandlearnsmorequicklyfromsynthesistasks,andproduceshigher-qualitylibrariesthatimprovegeneralizationtodownstreamtaskswithoutnaturallanguagehints.LAPSbuildsonseveralrecentdevelopmentsin(non-language-based)programsynthesis,sowebeginwithareviewofrelatedwork(Sec.2),thenformalizethesearchandlibrarylearningproblems(Sec.3)andbasesynthesisalgorithm(Sec.4).WethendescribehowLAPSextendsthebasealgorithmtoincludelanguageinlearning(Sec.5)andconcludewithempiricalresults(Sec.6).arXiv:2106.11053v3  [cs.LG]  3 May 2022LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristics+ Iteratively learned jointly compositional generative models over program library and language(ii) Abstraction is structured over language to learn functions that compose like language (i) Neural search learns from generated language-annotated programs to condition on language as a high-level training signalLanguage-annotatedtraining tasks+ Iteratively learned library as a generative prior over programs(ii) Abstractionslearned from training programs may be overfit to training tasks(i) Conditional neuralsearch learned from program samples can struggle to generalize to hard training tasksTraining tasks with no ground truth programsA.Base learned synthesis algorithm (DreamCoder)a small nine gon next to a small squarefour nested squaresa five sidedsnowflake with a short line and a small seven gon as armsmove_pen(for ∞ (move_pen (* unit_line 3) (/ 2π 6)))learned_fn_0 = (for ∞ (move_pen (* unit_line 3) (/ 2πx )))gon_fn = (for ∞(move_penx (/ 2π y ))) largesix gonB. Language for abstraction and program search (LAPS)(for ∞ (move_pen (* unit_line 3) (/ 2π 6)))large_fn = (* unit_line 3) ....leverages compositional generativity of programs to learn....leverages compositional generativity oflanguageto learn programsLLibraryProgramExecuted exampleJoint library-languagemodel(Program,language)Executed example and annotationJlarge six gonforward sampleprogramslearned execution-conditioned inverselarge six gon(for ∞ (move (* unit_line 3) (/ 2π 6)))forward sampleprograms and language“large six gon”learned execution and languageconditioned inverse(for ∞ (move _pen(* unit_line 3) (/ 2π 6)))unit_linefor*add back to learned libraryadd back to learned librarylearned_fn_0gon_fn“large”“line”large_fnabstract overdiscoveredprogramsabstract jointlyover programs and language“gon”move_penunit_linefor*Figure1.Ourmodel,LanguageforAbstractionandProgramSearch(LAPS)integratesnaturallanguageintobaselearnedsynthesisalgorithmsformulatedashierarchicalBayesianinference(A,left)forjointlylearningalibraryofprogramabstractionsandaneuralsearchheuristicforsynthesis.Wegiveanextendedformulation(B,left)deﬁnedjointlyovertheprogramlibraryandnaturallanguagedescriptionsofsynthesistasks,thatcanbeusedtoincorporatenaturallanguageintobothabstractionandsearchheuristiclearning.Whenincorporatedintoaconcretelearningalgorithm,DreamCoder(A,right)weshowthatLAPSallowsthemodeltoleveragelanguagerichlyduringtrainingtoimprovethegeneralizationofboththelearnedneuralsearchmodelandthelearnedlibraryofprogramabstractions.2.RelatedWorkOurworkdrawsonrecentprogramsynthesisapproachesthatlearntosynthesizeprogramsfromexamplesusingneu-ralmodelstoguidesearch(Gulwanietal.,2015;Balogetal.,2016;Parisottoetal.,2016;Devlinetal.,2017;Polo-sukhin&Skidanov,2018;Abolaﬁaetal.,2018;Nyeetal.,2019;Ellisetal.,2019;Sietal.,2019;Yeetal.,2020a);andlearnlibrariesofsymbolicabstractionsfromacollectionofrelatedprogramsortasks(Dechteretal.,2013;Zhangetal.,2017;Shinetal.,2019;Dumanci´c&Cropper;Ellisetal.,2018;2021).Ourformulationbuildsonhierarchi-calBayesianformulationsofprogramlearningthatframebothsynthesisandlibrarylearningasprobabilisticinference(Liangetal.,2010;Lakeetal.,2015;Ellisetal.,2021).Naturallanguagehasalsobeenusedtoscaffoldlatentrepre-sentationlearning(Fromeetal.,2013;Jia&Liang,2016;Andreasetal.,2017;Yeetal.,2020b;Goyaletal.,2020;Liangetal.,2020;Muetal.,2019;Luketinaetal.,2019),andasahigh-levelspeciﬁcationforprogramsynthesistasks(Yeetal.,2020a;Nyeetal.,2019;Polosukhin&Skidanov,2018;Yeetal.,2020b;Desaietal.,2016;Srivastavaetal.,2017).Herewepresentanapproachthatintegrateslanguageannotationsintrainingforlearningamoregeneralizablelibraryandprogramsearchmodelthatcanbeusedaftertrainingwithnoadditionalannotationsfornewtasks.3.InductivesynthesisandlibrarylearningConsidertheproblemofwritingagraphicsprogramtodrawthelargehexagonimageintheleftcolumnofFig.1.Thisisaninductiveprogramsynthesisproblem:ataskt(likedrawalargehexagon)isspeciﬁedwithexamplesofwhataprogramshoulddo,whereeachexampleisgivenasaninputx(inthiscase,theblankimagecanvas)andthedesiredoutputy(thelargehexagonimage).Aprogramρsolvesthetaskifitproducesoutputsthatareconsistentwiththespeciﬁcationwhenexecuted–thatis,ifevaluatingρunderanexecutionmodelEyieldsJρKE(x)=y.ProgramsynthesisbeginswithalibraryL={l0,..ln}containingthesetofprimitivesthatcanbecombinedtopro-ducesolutionprograms,suchasthe(pseudo-code)primitivefunctionsinasimplegraphicslanguage:L=movepen|unitline|for|*|π|∞|0|1|2|...whichdrawlinesonacanvasparameterizedbytheirlengthandangle.Givenalibrary,thereisalsotheproblemofsearch:effectiveprogramsynthesisrequiresasearchstrat-egySthatcanbegivenataskspeciﬁcation(suchastheimageofahexagon)andautomaticallydiscoverasolutionprogramliketheoneshowninFig.1:(for∞(movepen(∗unitline3)(/2π6))bysearchingoverprogramsbuiltfromfunctionsinL.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsBothoftheseingredients–thelibraryL,andthesearchstrategyS–canbemademuchmoreefﬁcientifthesyn-thesisenginewillbeexpectedtosolvemultiplerelatedproblems.Inthegraphicsdomain,forexample,synthesisofthevariousimagesdepictedinFig.1ismuchmoreeasilyaccomplishedusingalibrarylikeL=polygon|largeline|smallline...inwhichtheoriginalhexagontaskcanbeexpressedaspolygon(6,largeline)Agoodlibraryalreadyprovidesafoundationforefﬁcientsearchbymakingsolutionseasiertoexpress.Evenwithsuchalibrary,searchcanbefurtherguidedbyinformationaboutthepriorstructureofprograms(forexample,thefactthatpolygonistypicallycalledwithalargelineorsmalllinefunctionasasecondargument)andbyinfor-mationaboutthetargettaskitself(forexample,thefactthatthetargetimagecontainssixlinesegments).Thus,onewaytodescribeaneffectivesearchstrategySisviaaprioroverprogramsP[ρ|L]inthelibraryandaconditionalinferencemodelforinferringP[ρ|t,L],thedistributionoverprogramslikelyintendedbytheobservedtaskexamplest.TheforegoingdiscussionlaysoutthebasicingredientsofahierarchicalBayesianformulationofprogramsynthesis(usedinlearningalgorithmslike(Ellisetal.,2021;Lakeetal.,2015;Dechteretal.,2013);seethegraphicalmodelinFig.1A,left)forjointlylearningalibraryandconditionalsearchmodelfromadatasetTofsynthesistasks.WedenoteaprioroverprogramsasP[ρ|L,θL],onalibraryLwithparametersθL.Giventheobservedtasks,wedeﬁnethelikelihoodofthelatentlibraryandparametersas:Φ(L,θL)=P[L,θL]Yt∈TXρP[t|ρ]P[ρ|L,θL](1)whereP[L,θL]isaprioroverallpossiblelibrariesandpa-rameterizations,andP[t|ρ]isthelikelihoodthateachinduc-tivetasktisconsistentwithaprogramρ(forourpurposes,P[t|ρ]=1iftheprogramproducesthedesiredoutputex-amplesand0otherwise.)LearninginthismodelmeansestimatingtheoptimallibraryanditsparametersL∗,θL∗=argmaxL,θLΦ(L,θL)(2)alongwithaconditionalmodelP[ρ|t,L∗]thatcaninferprogramsfornewtasks.Thisformulationalsoforeshadowsastraightforwardwayinwhichlinguisticdescriptionsoftasks(likethoseintheﬁrstcolumnofFig.1)couldbeintegratedintolearning:wecouldsimplyextendtheconditionalmodelasP[ρ|t,dt,L∗]toincludeatask’sdescriptiondt.Wecomebacktothis(anddescribeamorecompleteintegration)inourapproach,butﬁrstdescribeaconcreteimplementationofEq.2onwhichwecanrealizethelanguage-enrichedmodel.4.Baselearningalgorithm:DreamCoderTheLAPSframeworkwedescribeinthispaperisageneraloneforextendingBayesianmodelsofprogramlearningliketheoneinEq.2toincorporateinformationfromlan-guage.Forconcreteness,however,ourpresentationandexperimentsbuildonthespeciﬁcDreamCoderalgorithmofEllisetal.(2021),whichwebrieﬂyreviewhere.WechooseDreamCoderbecauseitexposesamodularimplementationofthelibraryandsearchlearningproblemsinEq.2andhaspreviouslydemonstratedstate-of-the-artperformanceacrossavarietyofsynthesisdomains(Ellisetal.,2021;2020).DreamCoderisinitializedwithabaselibraryL0ofstartingprimitivesandadatasetoftrainingtasksT.ItreturnsalearnedﬁnallibraryLfaugmentedwithprogramabstrac-tionsandalearnedneuralsearchmodelQ(ρ|t,L)thatpre-dictshighprobabilityprogramsconditionedonthetaskexamples.Learningisiterative:DreamCoderalternatelysearchesforsolutionprogramstothetrainingtasks(givenacurrentlibraryLiandsearchmodelQi)andupdatesthelibraryandsearchmodelbasedonnewsolvedtasks.Wegivedetailsoneachcomponentbelow.4.1.ProgrampriorDreamCoderdeﬁnestheprioroverprogramsasaproba-bilisticcontextfreegrammar(PFCG;Johnson1998)forprogramsgeneratedasproductionsfromalibraryLoffunc-tionsl∈L1.Formally,DreamCoderassignsareal-valuedweightθLitoeachlibraryfunction,whichwhennormal-izedyieldsaproductionprobabilityP[l|L,θL].ThepriorprobabilityofaprogramρisgivenbyP[ρ|L,θL]=Yl∈ρP[l|L,θL](3)theweightedproductofprobabilitiesofallofitsconstituentlibraryfunctions.AsallP[l|L,θL]<1,thisisequivalenttoadescriptionlengthprioroverprograms:longerpro-grams(withmoreconstitutentelements)willhavelowerpriorprobabilityunderEq.3sinceP[l|L,θL]monotonicallydecreasesas|ρ|=|{l∈ρ}|increases.4.2.AmortizedconditionalinferenceToidentifyprogramsthatsolvetaskstwhileobtaininghighprobabilityunderP[ρ|L,θL],DreamCodertrainsaneural1Inadditiontoinitialandlearnedfunctions,Ellisetal.(2021)deﬁneLtoalsoincludeanyinitialliteralsandaruleforgeneratingvariables,suchthatprogramscanbecompletelygeneratedasproductionsfromthePCFG.Weusethesameformulation.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicssearchheuristicQi(ρ|t,Li)ateachiterationitoapprox-imatetheinverseconditionalmodel.TheheuristicusesaneuralmodeltrainedtopredictprogramswritteninthecurrentlibraryLiaccordingtotheposterior:Qi(ρ|t,Li)≈P[ρ|t,(Li,θLi)]∝P[t|ρ]P[ρ|(Li,θLi)](4)conditionedonanencodingofthetrainingexamples(e.g.anembeddingoftheimageinthetaskspeciﬁcation).Thismodelistrainedinthedistantsupervisionsetting(whichbeginswithnosupervisedprogramdata)byleveragingtheforwardgenerativemodel:samplingprogramsfromtheprior,executingthemtoproduceobservedtasks,andthenminimizingQ(ρ|t,L)inEq.4onthesampledprograms,conditionedontheirexecutions.Thisgenerativetrainingprocedureisgenerallyapplicabletoanyneuralimplemen-tationofQ(ρ|t,L).(ButseeEllisetal.(2021)andoursupplementarymaterialforadditionaldetailsonthemodelarchitecture,whichwereimplementinourexperiments.)4.3.Abstractionlearningasprogramcompression(maximizingthelikelihoodofprograms)TheDreamCoderalgorithmalsoiterativelyupdatestheli-brary(Li,θLi)toapproximatelyoptimizeEq.2(ﬁndingL∗,θ∗Lwhichmaximizethelikelihoodoftheinferredlatentprograms).Ellisetal.(2021)leverageequivalencetoacom-pressionproblemdeﬁnedoverprogramsandthelibrary.Asdiscussedin4.1,thePCFGprogrampriorisequivalenttoadescriptionlengthprioroverprograms.Ellisetal.(2021)placeanadditionalDirichletprioroverthelibrarydescrip-tionlength:P[L]∝exp−λXρ∈Lsize(ρ)(5)Estimatingtheoptimallibrarythenbecomestheproblemofinferringnewlibraryabstractionswhichcanjointlycom-pressthelatenttrainingprograms(rewrittenunderthenewlibraryLi+1)andthedescriptionlength|Li+1|oftheup-datedlibrary(tooptimizeforsharedabstractionsacrossprograms).Thisobjectivewouldstillrequireinferenceoverallpossiblewaysofrefactoringthelatentprogramsundertheupdatedlibrary.Ellisetal.(2021)approximatethisbyonlyconsideringcandidateabstractionsandprogramrefac-toringsthatcanbefoundviaanefﬁcientlambda-abstractionalgorithm.Asanexample,thiscouldrefactorthelargehexagonprogram(for∞(movepen(∗unitline3)(/2π6))toexposeacandidateabstractionlikeλx.(for∞(movepen(∗unitline3)(/2πx))whilealsorewritingtheoriginalprogramusingthisabstrac-tion.Notably,thisfragment–whichdrawspolygonswithlinesoflength3forsides–isnotthemostintuitivelygener-alizableforthegraphicsdomain.Aprogrammerwithmoredomain-speciﬁcpriorknowledgewouldprobablypreferanabstractionlikeλxy.(for∞(movepen(∗unitliney)(/2πx))whichadditionallyparameterizesthepolygonbythelengthofitssides,andissemanticallyequivalenttothehigh-levelpolygonfndescribedintheproblemsetupinSec.3.However,learningabstractionsbycompressingthelibraryandcurrentsolvedtrainingtasksmayactuallydisfavorthismoreintuitivelygeneralizable(butlesscompressive)candi-date.Oursecondkeygoalinintroducinglanguagewillbetoleverageitasanadditionalsourceofpriorknowledgetoimproveabstractiongeneralization.5.OurApproach:LanguageforAbstractionandProgramSearchOurworkconsidershowthegenerallearningproblem–jointlylearningthelibraryLwhichdeﬁnestheprioroverprogramsandtheconditionalsearchstrategySwhichin-vertsfromtaskstoprograms–canbeenrichedinthelanguage-annotatedsetting.Here,atleastasubsetofthetrainingtasksareadditionallyannotatedwithanaturallan-guagedescriptiondt(suchasthenaturallanguagedescrip-tionlargesixgonforthelargehexagondrawingtaskinFig.1B).Languageoffersamoredirectsourceofinformationfordiscoveringalibraryliketheoneinoursetup,L=polygon|largeline|smallline...ifweleveragetheexpectationthatgeneralizableabstractions(likeacandidatepolygonfunction)shouldcorrespondsystematicallytonamedfragmentsinnaturallanguage(likethetokengon).Languagecanalsobeleveragedbytheconditionalsearchmodel:learningsystematiccorrespondencesbetweenlan-guageandprogramsfromdescriptionslikelargesixgonshouldinformsearchonnewtasks(liketheonedescribedasasmallninegonnexttoasmallsquareinFig.1B)onthebasisofsharedlanguage(likegon).Ourapproach,LAPS(LanguageforAbstractionandPro-gramSearch)formalizestheseintuitionsbyextendingthehierarchicalBayesianproblemformulationoverprogramsgiveninSec.3toadditionallygeneratenaturallanguagetaskdescriptions(seegraphicalmodelinFig1B,left).Inparticular,weassumetheexistenceofajointlygenerativemodelJ(ρ,dt)overlatentprogramsthatsolvetasks,andcorrespondingnaturallanguagedescriptions.WerewritetheoriginalprioroverprogramsP[ρ|L,θL]deﬁnedonalibraryLtoajointpriorP[ρ,dt|J,θJ],andextendthedistributioninEq.1overthelatentjointmodelJwithparametersθJ,LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicswrittenasΦ(J,θJ)=P[J,θJ]Yt∈TXρP[t|ρ]P[ρ,dt|J,θJ](6)Learninginthelanguage-augmentedsettingnowinvolvesestimatingtheoptimaljointmodelanditsparametersJ∗,θJ∗=argmaxJ,θJΦ(J,θJ)(7)alongwithalanguage-conditionedmodelP[ρ|t,d,J∗]thatcaninferprogramsfornewtasksbasedonbothspeciﬁcationexamplesandtaskdescriptions.Intheremainderofthissectionweﬁrstdescribeageneraljointmodelformulationthatcanbelearnedfromlanguage-annotatedtrainingtasks.Wethenshowhowthejointframe-workallowsnaturallanguagetoinformlearningatboththeabstractionandsearchlevelinaconcreteexample,usingDreamCoderasthebasehierarchicalalgorithm.5.1.JointprioroverprogramsandlanguageBasepriorWeformulateourjointprioroverlanguageandprogramsasP[ρ,dt]=P[ρ|L,θL]P[dt|ρ,L](8)decomposedastheproductoftheoriginalprogrampriordeﬁnedonaprogramlibraryP[ρ|L,θL],andalearnedprogram-to-natural-language“translation”modelT(dt|ρ,L)≈P[dt|ρ,L]whichdescribeshownaturallan-guagedescriptionsaregeneratedforlatentprograms(inourrunningexample,thismodelwoulddescribehowthelargesixgondescriptionwasgeneratedconditionedontheprogramsolutionforthattask.)ThisdecompositionbuildsmodularlyontheoriginalprogrampriordeﬁnedonlyonthelibraryL.LearningT(dt|ρ,L)formalizestheintuitionthatthereshouldbealearnablerelationshipbetweenlanguagethatdescribestasksandlatentprogramsthatsolvethem.T(dt|ρ,L)canbeimplementedinmanyways(e.g.(Wong&Mooney,2007;Joshi&Schabes,1997;Bahdanauetal.,2014;Chenetal.,2018)),compatiblewiththevastliteratureonstructuredtranslationbetweenlanguages,includingnatu-rallanguagesandprogramminglanguages.OurexperimentsusethetranslationmodelpopularlyknownasIBMModel4(Brownetal.,1993),oneofaclassofwell-studiedBayesianmachinetranslationmodels(Gal&Blunsom,2013)whichdecomposeT(dt|ρ,L)intoT(dt|ρ,L)∝Yw∈dt,l∈ρPT[w|l](9)aproductoflearnedtoken-leveltranslationprobabilitiesPT[w|l]betweenindividualfunctionslinatask’slatentprogramρandwordswinthetaskdescriptiondt.(Seesup-plementarymaterialsformodelimplementationandtrain-ingdetails.)Thistoken-leveldecompositionmoredirectlycapturestheintuitioninoursetup:thatabstractionsinaprogramminglibrarygenerallycorrespondsystematicallytoindividualnamesinnaturallanguagedescriptions,andthattheinverseconditionalsearchcanbeguidedbasedonagenerallycompositionalrelationshipbetweenprogramprimitivesandwords.Thisformulationalsoallowsthesecompositionalrelationshipstobeinferredfromfewerob-servedexamplesthanwouldbepossiblewithothertransla-tionmodelswithweakerinductivebiases.However,Eq.8shouldextendtoincludeanysimilartranslationmodelandneednotincludethisstrongerdecomposition.AddingricherpriorsInLAPS,thejointmodelcanalsoprovideacontrollableinterfaceforincorporatingadditionalpriorknowledgeaboutlanguageintolearning.Learnedtranslationmodelsareoftenﬁttoonlymaximizethelikeli-hoodoftheobservedlanguage(here,withrespecttoinferredlatenttrainingprograms).However,ourformulationalsosupportsT(dt|ρ,L)enrichedtoincludeadditionalpriorsoverlanguage(suchasspeaker-speciﬁclanguageusage,orpragmaticsmodelsthatcaptureaspeakers’othercommu-nicativegoals(Grice,1989;Goodman&Frank,2016).)Inourexperiments(Sec.6.1)weshowcasethiswithresultsfromanextendedmodelincorporatinganadditionalmutualexclusivityprior.Mutualexclusivitymodelstheexpectationthatnewlyencounteredwordsshouldcorrespondtodifferentmeaningsthanknownones.Thispriorhasbeenshowntoplayanimportantroleinlanguagelearningincognitivescience(Franketal.,2009;Markman&Wachtel,1988),andinmachinelearningmodels(Gandhi&Lake,2019).Inthesynthesissetting,mutualexclusivitycancapturetheexpectationthat“new”words(whichappearindescriptionsofcurrentlyunsolvedtasks)aremorelikelytocorrespondtodifferentprogramcomponentsthanthoseusedinsolvedtrainingtasks(andforwhichtherewouldotherwisebenosignaltolearnatranslationmodelinthedistantsetting).OurextendedmodelincorporatesthispriorbyupdatingEq.9todistinguishbetweenWknown(wordsthatappearinsolvedtrainingtaskswithlatentprograms)andWnew(newlyencounteredwords)asTME(dt|ρ,L)∝Yw∈d,l∈ρ(1[w∈Wknown]PT[w|l])(1[w∈Wnew]P[l|L,θL]−1])(10)wherenewwordsaremodeledasinverselyrelatedtoprimi-tivesundertheprogramprior(ﬁttopreviouslysolvedtasks)–modelingtheexpectationthatnewwordsmorelikelyrelatetoless-usedprogramcomponentsthanthoseusedsofar.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristics5.2.IntegratingthejointmodelintoamortizedconditionalsearchThejointmodelallowsLAPStoincorporatenaturallan-guageintothelearnedconditionalsearchmodeloverpro-grams.Inplaceoftheoriginalneuralamortizedmodelinthebasealgorithm(Sec.4.2),wetrainanextended,language-conditionedmodelQi(ρ|t,dt,Ji)ateachiterationtopredictprogramsaccordingto:Q(ρ|t,dt,Ji)≈P[ρ|t,dt,J,θJ]∝P[t|ρ]P[ρ,dt|J,θJ]∝P[t|ρ]P[dt|ρ]P[ρ|L,θL]≈P[t|ρ]T(dt|ρ,L)P[ρ|L,θL](11)whichamortizesprograminferenceunderourjointmodelformulation.Importantly,wecantrainthisneuralmodelusingsamplesfromthejointgenerativemodel,consistingofsampledprogramsandcorrespondinggeneratedlanguage.Aswiththeoriginallearningsetting,thissample-basedtrainingallowsLAPStolearnageneralizable,language-conditionedneuralsearchheuristic,capableofleveragingcompositionalpatternsinnaturallanguage,fromveryfewexamplesinthedistantsupervisionsetting.Wecanalsonowseethebeneﬁtsofricherlanguage-speciﬁcpriors(suchasmutualexclusivity):theneuralmodeltrainedtoamortizeinferencefromthejointgenerativemodelcanalsoapproxi-matethemutualexclusivitybias,enablingbetterexplorationandgeneralizationinthepresenceofnewwords.5.3.AbstractionlearningasjointmodelcompressionTheextendedjointmodelobjectiveinEq.2and7alsoal-lowsLAPStoincorporatenaturallanguageintoabstractionlearning.Extendingthecompression-basedabstractionob-jectiveinthebasealgorithm–whichoptimizedforlibrariesthatmaximallycompressthelatenttrainingprogramsandli-brary–requiresdeﬁningaprioroverthelanguage-programtranslationmodelTintermsoftheoptimalprogramlibrary.WeplaceaprioroverTdeﬁnedonaprogramlibraryLandanaturallanguagetokenvocabularyWasP[T|L]∝Xl∈L,w∈W−I(PT[w|l])(12)where−I(PT[w|l])=−log(PT[w|l]).Thismodelstheintuitionthatagoodlibrarycontainsprogramabstractionswhichcorrespondwelltoindividuallanguagetokens,andre-duceentropyinthecompositionaltranslationmodel.Deﬁn-ingthepriorcompositionallyalsoallowsthealgorithmtomaintainthedesirablypropertyfrom(Ellisetal.,2021),inwhichthejointlikelihoodcanbeefﬁcientlyre-approximatedwithrespecttoindividualcandidateprogramabstractionsbasedontheirconstituentsubcomponentslandcorrespond-ingtranslationdistributionsPT[w|l]underthecurrenttrans-lationmodel.Asinthebasesynthesisalgorithm,weAlgorithm1Input:InitiallibraryL0,annotatedtrainingtasks(T,D)InitializeJ←uniform;trainingtasksolutionsp←{}fori≤fdoQi(ρ|t,dt)←Trainon(p,T,dt)andsamples∼Jp←programsfromsearchamortizedwithQiLi←abstractionsoptimizedover(p,J)p←programsrewrittenusingabstractionsfromLiJi←FitθLandT(dt|ρ)to(p,dt)endforReturnQf,Lffullyre-estimateanewtranslationmodelateachiterationTi+1(dt|ρi+1,Li+1)toﬁttheupdatedlibraryandrefactoredprograms.Seethesupplementforextendeddetails.Takentogether,Alg.1summarizestheconcretealgorithmusingLAPStoincorporatelanguageinto(Ellisetal.,2021).6.ExperimentsWedemonstrateLAPSonthreedifferentdomains:stringediting,compositionalgraphicsdrawing,andscenerea-soning,whichwechoosetorepresentadiverserangeoftasksandaccompanyinglanguage(Fig.2).Inallthreedo-mains,weﬁndthatcomparedtothebasesynthesizer,LAPSlearnsandsolvesheldoutsynthesisproblemsfaster(Table1,Sec.1-2),andproduceshigher-qualitylibrariesthatim-provegeneralizationevenwhennaturallanguagehintsarenotavailableaftertraining(Table1,Sec.3).Belowwesummarizeeachdomain.WethendiscussresultsshowingthatLAPSiseffectivebecauseofhowthehier-archicalmodelincorporateslanguageduringlearning:weﬁndthat(1)LAPSsearchesmoreeffectivelyduringtraining,enablingittosolveandlearnfrommorediversetrainingtasksthanthebaselinemodel;(2)LAPSabstractsmoreeffectivelyduringtraining,addinginmoregeneralizablelibraryroutinesasitlearns;and(3)LAPScanuselanguageduringtestingifitisavailable,asanimportantadditionalsourceofhigh-levelinformationduringsynthesis.6.1.DomainsAllthreedomainsconsistofadatasetofinductivesynthe-sistaskstspeciﬁedasinput/outputexamples;procedurallygeneratedsyntheticlanguageannotations;andhumanlan-guageannotationssourcedfromMechanicalTurk.Weusesyntheticlanguageasourprimaryevaluationbenchmark:weareinterestedinacontrolledprobeoflearningwhenwordsaresystematicallyreusedandcomposed,butrefertomoreabstractconceptsthanintheinitialbaseprogramminglanguage.However,wealsousehumanlanguagetoevalu-atethepracticalityofourapproachinreal-worldsettings.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsA. String Editing (shown with sample I/O examples of n=30 and random human description of n=3)pavings → pavinbforgiveness → forgivenebenterprises → enterprisesif the word ends with consonant s replace that with bif the word ends with a consonant and s then change them both to bcools → gcoolscultivator → gcultivatorbloomed → bloomed(Synth) if the word starts with consonant vowel add g before that(Human) if word begins with consonant followed by vowel , add an g to the beginningtopazes -> topazsuburbs -> suburbsreckless -> recklsif there is e s remove that remove the e s from the wordshouldering -> shoululderinghath -> hathoutrun -> oututrununif there is u any letter double thatthe next letter with the letter u should be repeated as a pair for this transformationC. Compositional Graphics (shown with random human description of n=3)Simple shapesComplex objectsCompositional objects and relationsa small trianglesmall trianglea medium square one medium squarea medium eight gon octogona big circle just a circlea seven pointedstara seven sidedsnowflake with long triangles as armsa four stepped zigzagfour step ladder going from  top to bottoma greek spiral with eight turnsa long line that curls in on itself at right anglesa small five gon next to a small seven gona five sidedgon beside a seven sidedgona small nine gon separated by a big space from a small circlenine gon on left with small circle on right not connecteda small triangle connected by a big line to a medium trianglea small triangle with a long line and a medium trianglesix small five gons in a rowsix overlapped pentagons going left to rightseven sidedsnowflake with a short space and a short line and a short space and a small triangle as armsa seven sidedsnowflake with seven triangles and linefour nested squaresfour stacked squaresB. Scene Reasoning (shown with sample I/O examples of n=7 and random human description of n=2)Original CLEVR  (sample templates from full set)Extended scene manipulation and counterfactuals What number of gray rubber cubes are there?how many grey rubber cubes do you see212There is another thing that is the same color as the large rubber thing; what is it made of?what material is the other object that is the same color as the large rubber objectmetalrubbermetalWhat if the gray sphere became a small green metal sphere?what if the grey ball morphed into a small green ballIf you removed the red things, how many spheres would be left?count the spheres would be left after removing the red things330a small semicircle(f19 (f90 x))a medium semicircle(f3 (f90 x))a big semicircle(f9(* (/ ε 1) 5) x)f0=(λ (x y z) (for x (λ (u v) (move z y v)))) 1. . .forpen-upf4=(λ (x y z) (f0x (/ 2π y) 1z))f5=(λ (x y) (f4x x y))0.27 | gon0.22 | smallf9=(f0∞ ε)0.09 | small0.07 | semicirclef24=(λ (x y) (f23(λ (z u) (f21y 0 x u))))f17=(λ (x) (pen-up (λ (y) (f16x y))))0.67 | separated0.06 | space0.09 | snowflake0.09 | armsD. Example initialgraphics primitivesshown with learned high probability p(word | primitive) ......a small five gon(f55 x)a small nine gon(f59 x)a medium seven gon(f52 (f20 7 x))eight sidedsnowflake with a small seven gon as arms(f247 8 x)five sidedsnowflake with a short line and a medium five gon as arms(f245 (λ (x) (get/set (λ(y) (f2 1 (f41 5 y)))x))z)....and example program abstractions learned with languagerotates and draws a unit linelift pen betweenconsecutive shapesrotational symmetry by number of sidesmove pen in parameterized loopsmooth curverotate shapesaround axismove2+-Figure2.(A,B,C)Exampletasksfromallthreesynthesisdomainsshownwithsyntheticandsamplehumanlanguageannotations.Inductivesynthesisdomainsareshownwitharandomsubset(n=3)ofthepairedinput/outputexamples.Humanlanguageannotationsarealsorandomlysampled(alldomainswereannotatedbymultiplepeopleforabroaderrangeoflanguage.)(D)RepresentativeinitialprogramprimitivesandlibraryabstractionslearnedwithLAPSforthegraphicsdomain.Shownwithexampletaskssolvedwithsynthesizedprogramscontainingthelearnedabstractionsandhighprobabilitynaturallanguagelearnedfromthejointmodel.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsAdditionalinformationforalldomainsisinthesupplement.Stringediting:structuredstringtransformationproblemstakenfrom(Andreasetal.,2017)(n=1000train;n=500test).Tasksconsistofinputdictionarystringstransformedusingrandomlysampledregularexpressiontransducer(30I/Oexamplespertask).WechoosethisdomaintodemonstrateLAPSonanimportantclassicsynthesisdomain(Lau&Weld,1998).ThedatasetofAndreasetal.(2017)containshumanannotations;syntheticlanguageannotationsaregen-eratedovertheground-truthregexesusingtemplatesbasedontheoriginalhumanannotations.Weinitializesynthesiz-erswithfunctionalprogrammingprimitives(map,fold,cons,car,cdr,length,index)andcharacterconstants(followingthesimplertexteditingdomaininthebaselinepaper(Ellisetal.,2021)).TheneuralsearchmodelencodestheI/OtaskexamplesascharacterarrayswithabidirectionalGRU.Compositionalgraphics:inversegraphicsproblems(n=200train;n=111test)whereeachtaskisspeciﬁedbyanimageandsolvedbysynthesizingaprograminLOGOTurtlegraphics(Abelson&DiSessa,1986).Thisisinspiredbythegraphicsdomainin(Ellisetal.,2021)butre-designedtobemorechallenging(ground-truthprogramsaremuchlongeronaverageinthebaseprogramminglanguage)andexplicitlycompositional.Syntheticlanguageannotationsaregeneratedwithhigh-leveltemplatesovertheobjectsandrelationsineachtask;humanannotationsaresourcedasimagedescriptionsfromMTurk.Weinitializesynthesiz-erswiththegraphicsprimitivesin(Ellisetal.,2021).TheneuralmodelencodesimageexampleswithaCNN.Structuredscenereasoning:inductivescenereasoningtasks(n=212train;n=115test)whereeachsynthesisprob-lemisspeciﬁedbyastructuredinputscene,andoutputscanbeanumber(howmanyredrubberthingsarethere?),abooleanvalue(aretheremorebluethingsthangreen?),oranotherscene(whatifalloftheredthingsturnedblue?).ThisdomainismodeledonCLEVR(Johnsonetal.,2017a)butdesignedtosupportinductivesynthesistasksspeciﬁedoverthesymbolicscenerepresentations(anarrayofobjectsrepresentedasdictionariesofattributes)fromtheoriginalCLEVRtaskgeneratorinJohnsonetal.(2017a).Wealsoaddnewtasksthatrequiregeneratingorimagininglatentscenes(howmanymetalthingswouldbeleftifallthebluecylinderswereremoved?),whicharenotsolvableintheoriginalhigh-levelDSLhand-designedforJohnsonetal.(2017b)(andusedinsynthesis-basedapproacheslikeYietal.(2018)).Weincludethesetodemonstrateakeyfea-tureofourapproach:theabilitytolearngeneralizableli-brariesfromabasicbutexpressivesetofprimitives,ratherthanrestrictingtheprogramspacepre-emptivelywithahand-designedlanguage.Weusesyntheticlanguagean-notationsfromtheoriginaltemplatesin(Johnsonetal.,2017a)(andtemplateswritteninthesamestylefortheextendedtasks);humanannotationsaresourcedfroman-notatorsshownthesametasks.Weinitializesynthesizerswithfunctionalprogrammingprimitivessimilartothestring-editingdomain,withdomain-speciﬁcqueryfunctionsandconstants(getcolor(x);getshape(x);blue;cube).Theneu-ralmodelencodesthetaskexamplesasﬂattenedarraysofobjectattributesusingabidirectionalGRU.6.2.ResultsOnallthreedomains,wecompareourmodelagainstthebaselinesynthesizer(Table1,DreamCoder,nolanguage);amultimodalbaseline(Table1,multimodal,nogenera-tivemodel)thattrainsaneuralmodeldirectlyonsolvedtrainingtasks(similartoneuralsynthesismodelslikeDeep-Coder(Devlinetal.,2017)butaugmentedtoconditiononlanguage);andablatedLAPSvariants(Table1;LAPSrows)toevaluatetheadditivecontributionsoftheindividuallearn-ingcomponents.Wecompareallmodelsusingamatchedsearchbudgetpertaskandnumberoftrainingiterationsoverall,determinedusingahyperparametersearchwiththebaseline.Thesupplementcontainsfulldetails(andcode)toreplicateallexperiments;andadditionalqualitativeresults.Weﬁndthat:(1)LAPSsearchesmoreeffectivelyduringtraining,enablingittosolveandlearnfrommoretrainingtasksthanthebase-linesynthesizer.Underthehierarchicalmodelformulation,searchandabstractionarecloselyrelated:successfullysolv-ingtasksisthebasisforabstractionlearning.Comparingthemodellearningtrajectories(Fig.3)ontrain-ingtasksshowsthattheLAPSmodelsconsistentlysearchmoreeffectivelyduringtraining:ateachiterationtheysolvemoretaskswithinagiventimebudget.Fig.3alsohighlightsthatLAPSmodelsimprovetrainingrobustnessinthedistantlearningsetting:asinthebaselinepaper(Ellisetal.,2021),weﬁndthebaselinemodellearningtobehighlyvariablewithoutatrainingcurriculum(comparetrainingcurvesfromFig.3withdifferentrandomseedreplications;andthebestvs.meanperformance,Table1.)ComparingtheLAPSablationsalsosuggeststhatlinguisticpriors(likemutualexclusivity)canindeedbepracticallyusefulhereduringlearning(Table1,compareLAPSwithMEandwithout).Whatifwedouseacurriculum?Inthescenereasoningdomain(wherepreviousapproaches(e.g.Maoetal.2019)havearguedforacurriculum),wealsotestasimplecur-riculumbyorderingtasksaccordingtotheirnaturallan-guagetokenlength(whichcanbeevaluatedwithoutgroundtruthprograms).Table1showsthatourmodelisstillmoreeffective,andthatnon-curriculumperformanceisinfactcomparabletocurriculumperformance.(2)LAPSabstractsmoreeffectivelyduringtraining,addinginmoregeneralizablelibraryroutinesasitlearns.TheLeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsTable1.%held-outtest-taskssolved.Tocomparerobustness,werunrandomseedreplicationsinthegraphicsdomainforthesyntheticlanguagedataset.Bestreportsthebestmodelacrossreplications;Meanaveragesacrossreplications.LanguageModelStrings(ntest=500)Graphics(ntest=111)Scenes(ntest=115)%Solved%Solved(Best)%Solved(Mean)%Solved(Curric.)%Solved(Mean.)Synthtrain/testDreamCoder(nolanguage)33.449.5542.6467.8073.9Synthtrain/testMultimodal(nogenerativetranslationmodel)46.0026.1223.2076.5049.5Synthtrain/testLAPSinneuralsearch52.2092.7952.9395.688.1Synthtrain/testLAPS+mutualexclusivity57.0086.4980.1896.582.3Synthtrain/testLAPS+ME+language-programcompression54.6098.1981.9895.695.9Synthtrain/humantestLAPS+ME+language-programcompression54.6089.20–97.4–Humantrain/humantestLAPS+ME+language-programcompression48.6058.55–95.6–NolanguageattestNolanguageontrain/testOriginalDSL;Enumerative0.060.00–27.8–Nolanguageontrain/testDreamCoder(bestlibrary):Enumerative27.241.44–53.6–NolangattestLAPS(bestlibrary):Enumerative33.262.16–93.04–NolangattestLAPS(bestlibrary):example-onlyneuralsynthesis52.491.0–95.6–LAPS + ME  + lang. compression% Solved (0 –100%)# Learning Iterations (0 –27)DreamCoder  (no language)LAPS + mutual exclusivityLAPS  in neural searchMultimodal (no generative)Figure3.LearningcurvescomparingbaselinesandLAPSmodelsinTable1,showing%heldouttaskssolvedonthegraphicsdomainoverrandomtrainingtaskorderings.(MeanresultsinTable1showsaveragetest-timeperformancefromthetrainedmodelreplications.)variabilityacrosstrainingreplicationsinthebaselinesalsohighlightsachallengeforabstractionlearning:notallsharedsubroutinesencounteredintraininggeneralizewelltonewtasks.Addingpoorabstractionscanactuallybedetrimen-tal:theyincreasethecombinatorialsearchspace.Weﬁndthatourapproachproduceshigher-qualitylibrariesaftertraining:Table1(nolanguageattesttimesection)showsthatweconsistentlyimproveperformanceinahead-to-headcomparisonusingenumerativesearchfromthelibrarypri-orsalone–insomedomains,enumerativesearchwithourmodel’slibraryoutperformsneurallyguidedsearchfromthebaselinemodel.Wealsoﬁndthelearnedlibraryiseffectiveforneurally-guidedsynthesiswhennolanguagehintsareavailableaftertraining(Table1,nolanguageattest,example-guidedsynthesis),showingthatLAPSin-corporateslanguagetolearnamoreeffectivelibraryoverall,whichgeneralizestothenon-languagesetting.Seesupple-mentforexamplelearnedabstractionsfromLf.(3)LAPScanuselanguageduringtestingifitisavail-able,thoughitdoesn’tneedtoforcompetitiveperformance.Clearly,languagecanprovideausefulsourceofhigh-levelinformationifitisavailablefornewtasks.Ourapproachproducesaneuralsynthesizerpre-trainedtoconditiononlanguagewhereavailable.Resultsonallthreedomainsshowthatthemodelcanuseittoachieveadditionalperformancegains(Table1,seelanguageattestrows).Wealsoﬁndthatthemodelstrainedonsyntheticannotationsgeneralizeeffec-tivelytonaturalhumanlanguageattest(Table1,synthtrain,humantest),suggestingthatevenifhumanannotationistoocostly,inmanycaseshand-writingnaturallanguagetem-platestoaccompanyafewground-truthprogramsislikelysufﬁcient(andeasierthanhanddesigningafullDSL).7.ConclusionWepresentedLanguageforAbstractionandProgramSearch(LAPS).LAPSbuildsonhierarchicalBayesianmod-elsofprogramlearning:weofferageneralframeworkforintroducingjointlygenerativemodelsoverprogramsandlanguageintolearnedsynthesis.Goingforwards,animpor-tantavenueforfutureworkwillbeexploringdifferentcon-creteimplementationsofthebasealgorithmandtranslationmodelwhichrelatesprogramstolanguage.Apromisingfu-turedirectioncouldleveragerecentstructured,neuraljointmodelsthatcanlearnthecompositionalunitsoflanguage,andincorporatepre-trainedlanguagerepresentations(Joshi&Schabes,1997;Wisemanetal.,2018;Kimetal.,2019).ThehierarchicalBayesianframingalsodrawsconnectionstocomputationalcognitivemodelswhichmodelhumancon-ceptualrepresentationsandlearning(Goodmanetal.,2014;Fodor,1975;Rule,2020)asinferenceoverprogram-likerepresentations.FuturehumanexperimentscouldexploreLAPSasacognitivemodel,combiningparadigmsforstudy-inglanguagelearningwiththoseforstudyingnon-linguisticabstractionandsearch(e.g.Smithetal.2003;Hawkinsetal.2019;Lakeetal.2015;2019;Tianetal.2020).Acknowledgements:ManythankstoM.Nye,J.Mu,A.Mar-zoev,J.Fan,R.Hawkins,R.Levy,L.Schulzandouranonymousreviewersforinvaluablefeedback.SupportedbygrantsfromtheAirForceOfﬁceofScientiﬁcResearch,theNSFunderGrantNo.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsReferencesAbelson,H.andDiSessa,A.A.Turtlegeometry:Thecomputerasamediumforexploringmathematics.MITpress,1986.Abolaﬁa,D.A.,Norouzi,M.,Shen,J.,Zhao,R.,andLe,Q.V.Neuralprogramsynthesiswithpriorityqueuetrain-ing.arXivpreprintarXiv:1801.03526,2018.Andreas,J.,Klein,D.,andLevine,S.Learningwithlatentlanguage.arXivpreprintarXiv:1711.00482,2017.Appel,A.W.,Beringer,L.,Chlipala,A.,Pierce,B.C.,Shao,Z.,Weirich,S.,andZdancewic,S.Positionpaper:thescienceofdeepspeciﬁcation.PhilosophicalTransac-tionsoftheRoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,375(2104):20160331,2017.Artzi,Y.andZettlemoyer,L.Weaklysupervisedlearningofsemanticparsersformappinginstructionstoactions.TransactionsoftheAssociationforComputationalLin-guistics,1:49–62,2013.Bahdanau,D.,Cho,K.,andBengio,Y.Neuralmachinetranslationbyjointlylearningtoalignandtranslate.arXivpreprintarXiv:1409.0473,2014.Balog,M.,Gaunt,A.L.,Brockschmidt,M.,Nowozin,S.,andTarlow,D.Deepcoder:Learningtowriteprograms.arXivpreprintarXiv:1611.01989,2016.Brown,P.F.,DellaPietra,S.A.,DellaPietra,V.J.,andMercer,R.L.Themathematicsofstatisticalmachinetranslation:Parameterestimation.Computationallinguis-tics,19(2):263–311,1993.Chen,X.,Liu,C.,andSong,D.Tree-to-treeneuralnetworksforprogramtranslation.arXivpreprintarXiv:1802.03691,2018.Cropper,A.andMuggleton,S.H.Learningefﬁcientlogicalrobotstrategiesinvolvingcomposableobjects.AAAIPress/InternationalJointConferencesonArtiﬁcialIntelli-gence,2015.Dechter,E.,Malmaud,J.,Adams,R.P.,andTenenbaum,J.B.Bootstraplearningviamodularconceptdiscovery.InTwenty-ThirdInternationalJointConferenceonArtiﬁcialIntelligence,2013.Delaware,B.,Pit-Claudel,C.,Gross,J.,andChlipala,A.Fiat:Deductivesynthesisofabstractdatatypesinaproofassistant.AcmSigplanNotices,50(1):689–700,2015.1918839andNSF-fundedCenterforBrains,Minds,andMachines,theMIT-IBMWatsonAILab,Google,MicrosoftandAmazon.Desai,A.,Gulwani,S.,Hingorani,V.,Jain,N.,Karkare,A.,Marron,M.,andRoy,S.Programsynthesisusingnaturallanguage.InProceedingsofthe38thInternationalConferenceonSoftwareEngineering,pp.345–356,2016.Devlin,J.,Uesato,J.,Bhupatiraju,S.,Singh,R.,Mohamed,A.-r.,andKohli,P.Robustﬁll:Neuralprogramlearningundernoisyi/o.InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,pp.990–998.JMLR.org,2017.Dumanci´c,S.andCropper,A.Inventingabstractionsbyrefactoringknowledge.Ellis,K.,Ritchie,D.,Solar-Lezama,A.,andTenenbaum,J.B.Learningtoinfergraphicsprogramsfromhand-drawnimages.arXivpreprintarXiv:1707.09627,2017.Ellis,K.,Morales,L.,Sabl´e-Meyer,M.,Solar-Lezama,A.,andTenenbaum,J.Learninglibrariesofsubroutinesforneurally–guidedbayesianprograminduction.InAd-vancesinNeuralInformationProcessingSystems,pp.7805–7815,2018.Ellis,K.,Nye,M.,Pu,Y.,Sosa,F.,Tenenbaum,J.,andSolar-Lezama,A.Write,execute,assess:Programsynthesiswitharepl.arXivpreprintarXiv:1906.04604,2019.Ellis,K.,Wong,C.,Nye,M.,Sabl´e-Meyer,M.,Cary,L.,Morales,L.,Hewitt,L.,Solar-Lezama,A.,andTenen-baum,J.Dreamcoder:Growinggeneralizable,inter-pretableknowledgewithwake-sleepbayesianprogramlearning.ArXivpreprint,2020.Ellis,K.,Wong,C.,Nye,M.,Sabl´e-Meyer,M.,Cary,L.,Morales,L.,Hewitt,L.,Solar-Lezama,A.,andTenen-baum,J.Dreamcoder:Bootstrappinginductiveprogram-synthesiswithwake-sleeplibrarylearning.PLDI2021,2021.Fikes,R.E.andNilsson,N.J.Strips:Anewapproachtotheapplicationoftheoremprovingtoproblemsolving.Artiﬁcialintelligence,2(3-4):189–208,1971.Fodor,J.A.Thelanguageofthought,volume5.Harvarduniversitypress,1975.Frank,M.C.,Goodman,N.D.,andTenenbaum,J.B.Us-ingspeakers’referentialintentionstomodelearlycross-situationalwordlearning.Psychologicalscience,20(5):578–585,2009.Frome,A.,Corrado,G.S.,Shlens,J.,Bengio,S.,Dean,J.,Ranzato,M.,andMikolov,T.Devise:Adeepvisual-semanticembeddingmodel.InAdvancesinneuralinfor-mationprocessingsystems,pp.2121–2129,2013.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsGal,Y.andBlunsom,P.Asystematicbayesiantreatmentoftheibmalignmentmodels.InProceedingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssoci-ationforComputationalLinguistics:HumanLanguageTechnologies,pp.969–977,2013.Gandhi,K.andLake,B.M.Mutualexclusivityasachallengefordeepneuralnetworks.arXivpreprintarXiv:1906.10197,2019.Ganin,Y.,Kulkarni,T.,Babuschkin,I.,Eslami,S.A.,andVinyals,O.Synthesizingprogramsforimagesusingrein-forcedadversariallearning.InInternationalConferenceonMachineLearning,pp.1666–1675.PMLR,2018.Goodman,N.D.andFrank,M.C.Pragmaticlanguageinter-pretationasprobabilisticinference.Trendsincognitivesciences,20(11):818–829,2016.Goodman,N.D.,Tenenbaum,J.B.,andGerstenberg,T.Conceptsinaprobabilisticlanguageofthought.Tech-nicalreport,CenterforBrains,MindsandMachines(CBMM),2014.Goyal,P.,Niekum,S.,andMooney,R.J.Pixl2r:Guidingreinforcementlearningusingnaturallanguagebymap-pingpixelstorewards.arXivpreprintarXiv:2007.15543,2020.Grice,P.StudiesintheWayofWords.HarvardUniversityPress,1989.Gulwani,S.,Hern´andez-Orallo,J.,Kitzelmann,E.,Muggle-ton,S.H.,Schmid,U.,andZorn,B.Inductiveprogram-mingmeetstherealworld.CommunicationsoftheACM,58(11):90–99,2015.Gulwani,S.,Polozov,O.,Singh,R.,etal.Programsynthesis.FoundationsandTrends®inProgrammingLanguages,4(1-2):1–119,2017.Hawkins,R.X.,Goodman,N.D.,andGoldstone,R.L.Theemergenceofsocialnormsandconventions.Trendsincognitivesciences,23(2):158–169,2019.Jia,R.andLiang,P.Datarecombinationforneuralsemanticparsing.arXivpreprintarXiv:1606.03622,2016.Johnson,J.,Hariharan,B.,VanDerMaaten,L.,Fei-Fei,L.,LawrenceZitnick,C.,andGirshick,R.Clevr:Adiag-nosticdatasetforcompositionallanguageandelementaryvisualreasoning.InProceedingsoftheIEEEConfer-enceonComputerVisionandPatternRecognition,pp.2901–2910,2017a.Johnson,J.,Hariharan,B.,VanDerMaaten,L.,Hoffman,J.,Fei-Fei,L.,LawrenceZitnick,C.,andGirshick,R.Inferringandexecutingprogramsforvisualreasoning.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pp.2989–2998,2017b.Johnson,M.Pcfgmodelsoflinguistictreerepresentations.ComputationalLinguistics,24(4):613–632,1998.Joshi,A.K.andSchabes,Y.Tree-adjoininggrammars.InHandbookofformallanguages,pp.69–123.Springer,1997.Kim,Y.,Dyer,C.,andRush,A.M.Compoundprobabilisticcontext-freegrammarsforgrammarinduction.arXivpreprintarXiv:1906.10225,2019.Lake,B.M.,Salakhutdinov,R.,andTenenbaum,J.B.Human-levelconceptlearningthroughprobabilisticpro-graminduction.Science,350(6266):1332–1338,2015.Lake,B.M.,Ullman,T.D.,Tenenbaum,J.B.,andGersh-man,S.J.Buildingmachinesthatlearnandthinklikepeople.Behavioralandbrainsciences,40,2017.Lake,B.M.,Linzen,T.,andBaroni,M.Humanfew-shotlearningofcompositionalinstructions.arXivpreprintarXiv:1901.04587,2019.Lau,T.A.andWeld,D.S.Programmingbydemonstration:Aninductivelearningformulation.InProceedingsofthe4thinternationalconferenceonIntelligentuserinterfaces,pp.145–152,1998.L´azaro-Gredilla,M.,Lin,D.,Guntupalli,J.S.,andGeorge,D.Beyondimitation:Zero-shottasktransferonrobotsbylearningconceptsascognitiveprograms.ScienceRobotics,4(26),2019.Liang,P.Learningexecutablesemanticparsersfornaturallanguageunderstanding.CommunicationsoftheACM,59(9):68–76,2016.Liang,P.,Jordan,M.I.,andKlein,D.Learningprograms:Ahierarchicalbayesianapproach.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.639–646,2010.Liang,W.,Zou,J.,andYu,Z.Alice:Activelearningwithcontrastivenaturallanguageexplanations.arXivpreprintarXiv:2009.10259,2020.Luketina,J.,Nardelli,N.,Farquhar,G.,Foerster,J.,Andreas,J.,Grefenstette,E.,Whiteson,S.,andRockt¨aschel,T.Asurveyofreinforcementlearninginformedbynaturallanguage.arXivpreprintarXiv:1906.03926,2019.Mao,J.,Gan,C.,Kohli,P.,Tenenbaum,J.B.,andWu,J.Theneuro-symbolicconceptlearner:Interpretingscenes,words,andsentencesfromnaturalsupervision.arXivpreprintarXiv:1904.12584,2019.LeveragingLanguagetoLearnProgramAbstractionsandSearchHeuristicsMarkman,E.M.andWachtel,G.F.Children’suseofmutualexclusivitytoconstrainthemeaningsofwords.Cognitivepsychology,20(2):121–157,1988.Mu,J.,Liang,P.,andGoodman,N.Shapingvisualrepresen-tationswithlanguageforfew-shotclassiﬁcation.arXivpreprintarXiv:1911.02683,2019.Nye,M.,Hewitt,L.,Tenenbaum,J.,andSolar-Lezama,A.Learningtoinferprogramsketches.arXivpreprintarXiv:1902.06349,2019.Parisotto,E.,Mohamed,A.-r.,Singh,R.,Li,L.,Zhou,D.,andKohli,P.Neuro-symbolicprogramsynthesis.arXivpreprintarXiv:1611.01855,2016.Polosukhin,I.andSkidanov,A.Neuralprogramsearch:Solvingdataprocessingtasksfromdescriptionandexam-ples.2018.Polozov,O.andGulwani,S.Flashmeta:aframeworkforinductiveprogramsynthesis.InProceedingsofthe2015ACMSIGPLANInternationalConferenceonObject-OrientedProgramming,Systems,Languages,andAppli-cations,pp.107–126,2015.Rule,J.S.Thechildashacker:buildingmorehuman-likemodelsoflearning.PhDthesis,MassachusettsInstituteofTechnology,2020.Shin,E.C.,Allamanis,M.,Brockschmidt,M.,andPolo-zov,A.Programsynthesisandsemanticparsingwithlearnedcodeidioms.InAdvancesinNeuralInformationProcessingSystems,pp.10824–10834,2019.Si,X.,Yang,Y.,Dai,H.,Naik,M.,andSong,L.Learningameta-solverforsyntax-guidedprogramsynthesis.InInternationalConferenceonLearningRepresentations,2019.Silver,T.,Allen,K.R.,Lew,A.K.,Kaelbling,L.P.,andTenenbaum,J.Few-shotbayesianimitationlearningwithlogicalprogrampolicies.InProceedingsoftheAAAICon-ferenceonArtiﬁcialIntelligence,volume34,pp.10251–10258,2020.Smith,K.,Brighton,H.,andKirby,S.Complexsystemsinlanguageevolution:theculturalemergenceofcompo-sitionalstructure.AdvancesinComplexSystems,6(04):537–558,2003.Srivastava,S.,Labutov,I.,andMitchell,T.Jointconceptlearningandsemanticparsingfromnaturallanguageex-planations.InProceedingsofthe2017conferenceonempiricalmethodsinnaturallanguageprocessing,pp.1527–1536,2017.Tian,L.Y.,Ellis,K.,Kryven,M.,andTenenbaum,J.B.Learningabstractstructurefordrawingbyefﬁcientmotorprograminduction.arXivpreprintarXiv:2008.03519,2020.Wiseman,S.,Shieber,S.M.,andRush,A.M.Learn-ingneuraltemplatesfortextgeneration.arXivpreprintarXiv:1808.10122,2018.Wong,Y.W.andMooney,R.Learningsynchronousgram-marsforsemanticparsingwithlambdacalculus.InPro-ceedingsofthe45thAnnualMeetingoftheAssociationofComputationalLinguistics,pp.960–967,2007.Ye,X.,Chen,Q.,Dillig,I.,andDurrett,G.Benchmark-ingmultimodalregexsynthesiswithcomplexstructures.arXivpreprintarXiv:2005.00663,2020a.Ye,X.,Chen,Q.,Dillig,I.,andDurrett,G.Optimalneuralprogramsynthesisfrommultimodalspeciﬁcations.arXivpreprintarXiv:2010.01678,2020b.Yi,K.,Wu,J.,Gan,C.,Torralba,A.,Kohli,P.,andTenen-baum,J.Neural-symbolicvqa:Disentanglingreasoningfromvisionandlanguageunderstanding.InAdvancesinNeuralInformationProcessingSystems,pp.1031–1042,2018.Zhang,Y.,Pasupat,P.,andLiang,P.Macrogrammarsandholistictriggeringforefﬁcientsemanticparsing.arXivpreprintarXiv:1707.07806,2017.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsThiscontainsthesupplementalappendixtothe2021ICMLpaper.Itisorganizedsequentiallyinreferencetothemaintext;S{N}refersbacktosectionNinthemaintext.Acompletereleaseofcodeforourimplementation,includingcommandlinescriptstoreplicatetheexperimentsinthepaperandlinkstothedatasets,canbefoundat:https://bit.ly/3g9361W.S4.Baselearningalgorithm:DreamCoderTheLAPSframeworkdescribedinthemainpaper(Sec.5)isageneraloneforextendingBayesianmodelsofprogramlearningtoincorporateinformationfromnaturallanguage(see(Liangetal.,2010;Lakeetal.,2015;Dechteretal.,2013;Lakeetal.,2013)).OurconcreteimplementationandexperimentsusetheDreamCoderapproachof(Ellisetal.,2021;2018)asthebasesynthesisalgorithm,whichimple-mentsthehierarchicalBayesianformulationofprogramlearning.Itdeﬁnesamodularinterfacewithtwoprimarylearningcomponents:alearnedconditionalinferencemodelforsearch(asaneuralsearchheuristic);andalearnedab-stractionalgorithmforupdatingtheprogramprior(basedonprogramrefactoringandcompression)(Ellisetal.,2021).Eachoftheselearningcomponentshasbeenadditionallyimplementedinotherwork(suchas(Devlinetal.,2017;Polosukhin&Skidanov,2018;Nyeetal.,2019;Parisottoetal.,2016;Balogetal.,2016)forneurallyguidedsynthesis,and(Dechteretal.,2013;Zhangetal.,2017;Shinetal.,2019;Artzietal.,2014;Dumanci´c&Cropper)forprogramabstractionlearning).Thissupplementarysectionprovidestheoreticalandim-plementationdetailsontheDreamCoderalgorithmweuseinourexperiments(summarizedinSec.4).Wematchourimplementationascloselyaspossibletotheoriginalworkforcomparisonwithpublishedbaselines.Weprovidekeydetailsrelevanttothelanguage-guidedextension,butstronglyrecommendtheoriginalworkswhichintroducetheDreamCoderalgorithm(Ellisetal.,2021;2018)forfurtherreference.S4.1ProgrampriorandMDLequivalenceHierarchicalBayesianprogramlearningformulationsre-quireaprioroverexpressibleprograms.DreamCoderislearnediteratively:itisinitializedwithabaselibraryL0andreturnsalibraryLfcontainingprogramabstractionslearnedfromsolvingtrainingtasks.Therefore,Dream-CoderdeﬁnesitsprogrampriorwithrespecttothecurrentlibraryLimaintainedateachiteration.Thisisparame-terizedasasimplePCFGP[ρ|L,θL]whoseproductionsareoftheformli→lj∈L,eachwithareal-valuedweightθLl,wheretheprobabilityofaprogramρisgivenbyP[ρ|L,θL]=Ql∈ρP[l|L,θL](Sec.4.1).Minorcomplexityarisesinordertosupporttyping(Pierce,2002):following(Ellisetal.,2018),thelibraryLiisim-plementedasasetofpolymorphicallytypedλ-calculusexpressions.TheonlychangethisproducestotheoriginalpriordeﬁnitionistorestrictthesetofpossibleproductionsunderthePCFG:thatis,permissibleproductionsareoftheformli→lj∈{L|li→ljiswelltyped}.Thepriorproba-bilitiesofprogramsarethereforecalculatedwithrespecttothesetofwell-typedproductions.Asdiscussedinthemainpaper,thispriordeﬁnitionisequiv-alenttoaminimumdescription-lengthprioroverprogramsunder(L,θL)whenallθL<1.0,astheproductofaddi-tionalproductionsinanexpressionwillstrictlydecreaseasthenumberofproductionsinanexpressionincreases.S4.2AmortizedconditionalinferenceFigure1.ArchitectureoftheneuralmodelQi(ρ|t,Li).Themodeltakesasinputtaskexamplest.Theseareencodedusingadomain-speciﬁcencoderE(t).TaskencodingsfeedtoanMLPandactiva-tionlayerandoutputatensorQ.ThisparameterizesadistributionoverprogrambigramsintheﬁnalDSL,whichdeﬁnesaconditionaldistributionfromwhichtoenumerateprogramsduringsearch.ToidentifyprogramsthatsolvetaskstwhileobtaininghighprobabilityunderP[ρ|L,θL],DreamCodertrainsaneuralsearchheuristicQi(ρ|t,Li)ateachiterationitoapproxi-matetheinversemodel.Thetrainingprocedurein(Ellisetal.,2021)(summarizedinSec.4.2)isakeycontributionoftheoriginalworkforlearn-inginthedistantsupervisionsetting.Themodelistrainedonsamplesfromthegenerativeprior(providinganendlessarXiv:2106.11053v3  [cs.LG]  3 May 2022Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionstrainingstreamofrandomsynthesistasks);andthisproce-dureshouldgeneralizeimmediatelytoanyneuralmodelforpredictingprogramsconditionedonthetaskspeciﬁcation(e.g.(Devlinetal.,2017;Polosukhin&Skidanov,2018;Nyeetal.,2019;Parisottoetal.,2016;Balogetal.,2016)).Themodelisalsosupervisedonanyoriginaltrainingtaskexamplesandtheirprogramsolutionsdiscoveredduringlearning.Inourexperimentsweusethebaselineneuralmodelarchi-tecturein(Ellisetal.,2021).Thisisparameterizedbytwomodularcomponents:1.Adomain-speciﬁctaskencoderE(t).Thisencodesthetaskexamples(e.g.imagesinthegraphicsprogramdo-main,orinput-outputstringsinthetexteditingdomain)thatareinputtotheneuralmodel.Thistaskencoderar-chitectureisdeﬁneddomain-speciﬁcallybasedontheformofthetaskexamples(e.g.aCNNforthegraphicsdomain).Itoutputsaﬁxeddimensionalembeddingforanygiventaskasinputtothemodel.Inourexperi-mentsthisisa64-dimensionalembeddingacrossalldomains(SeeS6.1fordomain-speciﬁcarchitectures;andreleasedcode.)2.AconditionalmodeloverprogramsQ(ρ|E(t)).Thiscomponentreceivesthetaskencodingasinputandoutputsadistributionoverprograms.Following(Ellisetal.,2021),thisisa2-layerfully-connectedMLP(with64hiddenunitsandaﬁnaltanhactivationlayer)thatoutputsaﬁxed-dimensionalreal-valuedtensoren-codingadistributionoverprogramsinthelibraryLasoutput.Thereal-valuedtensorcorrespondstoweightsoverprogramprimitivesconditionedontheirlocalcon-textinthesyntaxtreeoftheprogram,consistingoftheparentnodeinthesyntaxtreeandwhichargumentisbeinggenerated.Thisfunctionsasa‘bigramtransitionmodel’overtreesthatencodesthelikelihoodoftransi-tionsfromoneprimitivetothenext.Qreturnsthisasa(|L|+1)×(|L|+2)×A-dimensionaltensor,whereAisthemaximumarityofanyprimitiveinthelibrary.Thisparameterizationsupportsfastsamplingofprogramsduringconditionalsynthesis:theneuralmodelrunsoncepertask(toencodethetaskexamplesandproducethebigramtransitionmodel)andtheresultingparameterizationcanthenbeusedtosampleprogramsduringsynthesis(e.g.byenumeratingprogramsbyexpandingtrees(as‘bigrams’overparentandchildrenprimitives)rankedinorderoftheirlikelihoodstartingfromtheprogramroot.)Following(Ellisetal.,2021),theneuralmodelistrainedtooptimizethefollowingMAPinferenceobjectiveonthetrainingtasksandthesampledtasksfromtheprior:LMAP=Et∼(L,θL)(cid:20)logQ(cid:18)argmaxρP[ρ|t,L,θL](cid:12)(cid:12)(cid:12)(cid:12)t(cid:19)(cid:21)(1)S4.3AbstractionlearningasprogramcompressionDreamCoderlearnsnewabstractionstoapproximatelyopti-mizeforEq.2(mainpaper),whichinfersanoptimallibraryandparameterswithrespecttotheobservedprogramsonthetrainingtasks.TheDreamCoderabstractionalgorithmisaprimarycon-tributionoftheoriginalworkin(Ellisetal.,2021),andisdiscussedextensivelyin(Ellisetal.,2021).WethereforeprovideadditionaltechnicaldetailsherethatarerelevanttoitsintegrationwithLAPSinourexperiments,butstronglyencouragereferencing(Ellisetal.,2021)forthefullimple-mentation.Asdiscussedin(Ellisetal.,2021)andourmainwork,DreamCoderapproachesabstractionusinganequivalencebetweenEq.3andtheminimumdescriptionlengthoftheprior(asthedescriptionlengthofthelibrary)andthepro-gramsproducedfromtheprior(underthePCFGdeﬁnitionoftheprior).Therefore,inpractice,inferringtheoptimalli-braryisequivalenttoinferringthelibrarywhichmaximallycompressesthedescriptionlengthofthelibraryandthedescriptionlengthofprogramswhichexplainthetrainingtasks.Inparticular,DreamCoderoptimizesthefollowingcompressionobjectivewithrespecttothetrainingtasksTandtheﬁnitebeamBtofprogramsolutionsdiscoveredforeachtrainingtaskduringlearning:logP[L]+argmaxθLXt∈TlogXρ∈BtP[t|ρ]maxρ0−→∗ρP[ρ0|L,θL]+logP[θL|L]−|θL|0(2)Thekeyaspectofthisalgorithmisthatitconsidersabstrac-tionswhichcompressnotonlytheprogramsastheyarecur-rentlywritten,butanysemanticallyequivalentrefactoringsoftheseprograms.Speciﬁcally,asprogramsarewritteninaλ-calculus,refactoringreferstoanyprogramwhichisequiv-alentuptoβ-reduction(i.e.,functionapplication/variablesubstitution(Pierce,2002)).Aprimarycontributionoftheoriginalworkin(Ellisetal.,2021)isanefﬁcientalgorithmforcomputingtheserefactoringsthatisunchangedwhenweintegratelanguage;werefertotheoriginaltextfordetails.Inourwork,theprimaryimportantaspectofthisaspectisthatrefactoringsaredeﬁnedcompositionallyovertheex-istingprogramprimitives.Speciﬁcally,refactoringscanbeefﬁcientlycalculatedaccordingtosemanticequivalencesinthetheλ-calculus(namely,thatfunctionapplicationandvariablesubstitutionguaranteethattheresultingrefactoredprogramsareequivalent.AbstractionscreatedbyvariableSupplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionssubstitutionwillalwaysbecomposedofsubcomponentsfromtheinitiallibrary.)Wetakeadvantageofthiscomposi-tionalitywhendeﬁningourjointabstractionalgorithmovernaturallanguage.Deﬁninganinitialcompositionaltransla-tionmodelbetweenlanguageandtheprogramcomponentsensuresthatwecanapproximatecompressioninthejointmodelaftertheprogramsarerefactored,withoutneedingtoinduceanentirelynewtranslationmodeloverlanguageandtherefactoredprograms.S5.OurApproach:LanguageforAbstractionandProgramSearchThissectionnowdescribestechnicaldetailsfortheconcreteLAPSimplementationinourreportedexperiments,whichisdeﬁnedovertheDreamCoderimplementation.Westruc-turethissectionaccordingtotheparallelimplementationsinthebasealgorithmforclarity.However,exceptforthespeciﬁcsofthejoint-abstractionalgorithm,thetechnicalimplementationofeachcomponentshouldextenddirectlytomostothersimilarlearnedsynthesisalgorithms(e.g.thejointmodelimplementationshouldbereusableinanysyn-thesisalgorithmthatusesanexplicitsymboliclibraryofprimitives.)S5.1JointprioroverprogramsandlanguageLAPSextendsthepriorP[ρ]overprogramsunderthelibrarytoajointpriorJ(ρ,dt)overprogramsforagiventaskandtheirnaturallanguagedescriptionsdt(Sec.5.1).WeformulatethispriorasJ(ρ,dt)=P[ρ|L,θL]P[dt|ρ,L]theproductoftheoriginalprioroverprogramsP[ρ|L,θL]deﬁnedontheprogramlibrary,andaprogramtodescrip-tions“translation”modelT(dt|ρ,L)≈P[dt|ρ,L]thatde-scribeshowdescriptionsaregeneratedforprogramswritteninthelibrary.Theconcreteimplementationdescribedinthemainpaperusesatranslationmodelthatadditionallydecomposescom-positionallyoverlanguageandprograms–inparticular,onthebasisoftoken-tokentranslationdistributionsPT[w|l]betweenwordsw∈dtandl∈L.Manyavailabletrans-lationandsemanticparsingmodels(suchassynchronousgrammarsovernaturallanguageandprograms)preservethisfurthercompositionalrequirement(e.g.(Artzietal.,2014;Wong&Mooney,2006)).SeeFigureS3(supplement)forexamplesamplesfromthegenerativemodelonthegraphicsdomainatearlierandlaterstagesoftraining.Ourimplementationusesaclassicalstatisticalmachinetranslationmodel(theModel4versionoftheIBMStatis-ticalMachineTranslationmodels(Gal&Blunsom,2013))whoseparameterscanbetractablyestimatedfromveryfewpairedprogramsanddescriptions(inthedistantsupervisionsettingusedintheoriginalwork,theremaybenomorethanacoupleofhundredtrainingtasksinthefulldataset,andfewerthan10solvedtasksonwhichtotrainthetrans-lationmodelatanygiventime.)Inadditiontoinferenceinsmalldatasettings,thistranslationmodelhasafullycompositionalgenerativedeﬁnition(Gal&Blunsom,2013)thatallowsittobeeasilyusedtotraintheneuralamortizedinferencemodelwhichconditionsonlanguage.Despitethis,however,thistranslationmodel(andthefurtherinductivebiasesusedtospeciﬁcallyrelateprogramtreestosentences)makestrongcompositonalityassumptionsabouttherelationshipbetweenprogramprimitivesandwordsasajointgenerativemodelofprogramsandlanguage;weﬁndthattheseinductivebiasesareusefulinthesmalldatasettingandproduceempiricallysuccessfulresults.However,thisislikelybecauseofhowthejointmodelisusedduringtraining,whichdoesnotrequireaperfectgenerativemodeloflanguage(orlanguagewithrespecttoprograms)foreitheramortizinginferenceorabstractioninordertouselanguageasaheuristicduringlearning.Afulldeﬁnitionofthestatisticaltranslationmodelweusecanbefoundin(Gal&Blunsom,2013).Were-summarizeimportantdetailshere.TheIBMfamilyoftranslationmodelsestimatestheconditionaltoken-tokenprobabilitiesPT[w|l]onthebasisofalignmentvariablesal,d,whichspec-ifyadirectcorrespondencebetweentokensinparalleltexts(e.g.awordinataskdescriptionandaprogramprimitive.)Thesealignmentsaremany:manybetweentokensinpro-gramsandnaturallanguagesentences–agivenwordcancorrespondtomultipleprimitives,andviceversa.Condi-tionedonasetofalignmentsfrompairedprogramsanddescriptions,theconditionalprobabilitiesinbothdirections(theprobabilityofgeneratingaprogramprimitiveinapro-grambasedonthepresenceofawordinasentence,andviceversa)aredeﬁnedbymarginalizingoverthealignmentvariables.Weprovideonedirection(PT[w|l]),astheotherissymmetrical:PT[w|l]∝Xa1...XamP[w,a1...am|l]∝mYi=1q(ai|i,l,m)whereaiarealignmentvariablesinferredoverapairedcor-pusandq(j|i,l,m)canbeinterpretedastheprobabilityofalignmentvariableai(forthetokenwithindexiinaprogram)takingvaluej(wherejisanindexintothecorre-spondingsentence)conditionedonthelengthslandmoftheprogramandnaturallanguagesentence(Gal&Blunsom,2013).Thesealignmentsareinferredbyapproximatelyinvertingthegenerativemodelin(Gal&Blunsom,2013)tomaxi-Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsmizethelikelihoodoftheobservedpairedsentencesandprograms.Oneimplementationdetail:thealignmentalgo-rithmoperatesoverpairsofstrings.Forconvenienceweinferalignmentsbetweensentencesandlinearizedtokensequencesintheprogramtree(whichcanbedonewithcom-pleterecoverabilityoftheoriginalprogramtree(Andreasetal.,2013)).Thisisanotherinductiveassumptionthatwechooseafterpreliminaryexperimentationandﬁndthatourimplementationyieldsstrongempiricalresultsregardless.TheIBMtranslationmodelisanoisy-channelgenerativemodelthatrequiresanadditionallanguagemodelp(d)togeneratelanguage(Gal&Blunsom,2013;Heaﬁeld,2011).Weuseanefﬁcientparallelizedimplementationforinferringthetranslationmodelparametersfrom(Koehnetal.,2007),whichalsocontainsabasiclanguagemodelinferencealgorithminferredoverthefullcorpusoftrainingtasksentences(asatrigrammodel,whichweagainﬁndsimplebuteffectiveforourverysmalldatasetting).Speciﬁcmodelhyperparametersforallexperimentsareavailableinthereleasedcoderepo(intheexperimentruntimecommands.)Mutualexclusivity:Section5.1ofthemainpaperalsodescribeshowthejointmodelcanbemodiﬁedtoincludelanguage-speciﬁcpriors,suchasasimpleimplementationofthewell-knownmutualexclusivitypriordocumentedinthecognitivelanguage-learningliterature(Markman&Wachtel,1988;Gandhi&Lake,2019)andgivenaBayesianformulationin(Franketal.,2009).Weprovideanimple-mentationtodemonstratethatthejointmodelcanbeeasilyextended:speciﬁcally,asimplemutualexclusivityassump-tioncanbeaddedintothejointmodelbysimplyupdatingthecompositionaltranslationmodeltoincludeadditionaldistributionstME(dnew|l)wherednewarewordsthatonlyappearinunsolvedtrainingtasksandtME(dnew|l)∝αP[l|L,θL]−1newwordsarenowassumedtocorrespondtoprimitivesin-verselyproportionaltotheircurrentusageunderthelearnedprogramprior.Asweshowinthenextsection,incorporat-ingthisprioratthelevelofthejointmodelcanbeusedtoapproximatemutualexclusivityassumptionsinthelearnedsearchheuristic,encouragingexplorationinthepresenceofnewwords.Practically,wecalculatethemutualexclusivitypriorinourconcreteimplementationbyleveragingthealignmentsuponwhichourtoken-tokentranslationprobabilitiesaredeﬁned.Speciﬁcally,weaddpseudoalignmentsbetweeneachdnewandeachl∝αP[l|L,θL]−1;whenthetoken-tokentransla-tionprobabilitiesmarginalizeoverthelatentalignmentsandthesepseudoalignments,theresultingtranslationprobabili-tiesencodethemutualexclusivityprior.S5.2IntegratingthejointmodelintoamortizedconditionalsearchFigure2.Architectureofthelanguage-conditionedneuralmodelQ(ρ|d,t).Themodeltakesasinputtaskexamplest.Theseareencodedusingadomain-speciﬁcencoderE(t).Themodelad-ditionallytakesintaskdescriptionsd,encodedusingalanguagencoderED(t)(implementedasaGRU).Taskencodingsarecon-catendatedandfeedtoanMLPandactivationlayerandoutputatensorQ.ThisparameterizesadistributionoverprogrambigramsintheﬁnalDSL,whichdeﬁnesaconditionaldistributionfromwhichtoenumerateprogramsduringsearch.TheamortizedconditionalinferencemodelQ(ρ|t)(Sec.4.2)extendsstraightforwardlyinLAPStoconditiononlan-guageQ(ρ|d,t)(Sec.5.2).Importantly,thetrainingproce-dureinSec.4.2(trainingtheneuralmodelonsamplesfromtheprior)alsoextendstothelanguage-enrichedcondition(trainingtheneuralmodelonsamplesfromthejointprior,whichincludegeneratedlanguageannotations.)InourexperimentsweimplementtheconcreteneuralmodelQ(ρ|d,t)inourexperimentsbyextendingmodularlyontheoriginalmodelin(Ellisetal.,2021)(andinthesupplementalS4.2)fordirectcomparison.Ourfullarchitecturethereforehasthreemodularcomponentstoadditionallyconditiononlanguage:1.AnaturallanguagetaskdescriptionsencoderED(d).Thisreceivesthetaskdescriptiondasinput.Weimple-mentthisasanRNNmodelusingabidirectionalGRU(Choetal.,2014)with64hiddenunits;weembednaturallanguagesymbolsas64-dimensionalvectors,andrandomlyinitializeandbackpropagatethroughtheembeddingduringtraining.Wetokenizethesentencesinuonwhitespaceandconcatenateeachsentence,de-limitedbyspecialstartandendofsentencetokens.Attesttime,wereplaceanyOOVtokenswithaspecialUNKtoken.2.Adomain-speciﬁctaskencoderE(t),followingS4.2.3.Abigramtransitionmodeloverprogramprimitives,followingS4.2.ToconditionjointlyonED(d)andE(t)wesimplyconcatenatethesetwoembeddingsandupdatetheﬁrstlayeroftheMLPtotakethe128-dimensionalconcatenatedembeddingsasinput.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractions5.3AbstractionlearningasjointmodelcompressionFinally,theabstractionlearningmodelin(Ellisetal.,2021)canalsobegeneralizedtoconditiononlanguage,byextend-ingtheoptimallibraryinferencealgorithmwithrespecttotheprogrampriortoanoptimallibraryinferencealgorithmwithrespecttothejointmodeloverlanguageandprograms(Eq.6and7,maintext.)InourconcreteimplementationwithrespecttotheDream-Coderalgorithm,thismeansextendingthedescription-lengthcompressionobjective–originallydeﬁnedovertheprogramlibraryandtrainingtaskprograms–toincludethetranslationmodeldeﬁnition.Themainpaperdeﬁnesadescription-lengthprioroverthecompositionaltranslationmodel(Eq.10).Optimizingthistractablyrequiresredeﬁn-ingtheabstractionalgorithmin(Ellisetal.,2021)–whichrefactorsλ-calculusprogramsvialambda-abstraction(seeS4.3forasummary)–toalsojointlyre-estimatethedescrip-tionlengthofthetranslationmodelT(dt|ρ,L0)usingtherefactoredprogramsunderthenewcandidatelibraryL0.Weimplementanefﬁcientapproximationthatcanbecal-culatedwithrespecttotheclassicalstatisticaltranslationmodeldescribedinS4.1(Gal&Blunsom,2013).Inparticu-lar,weleveragethealignment-baseddeﬁnition(whichuseslatentcorrespondencesinferredbetweenprogramtokensandsentencetokensinpairedprogramsanddescriptions)toapproximate−H(PT[w|l])=−log(PT[w|l]),theentropyofthetoken-tokentranslationprobabilities.Speciﬁcally,astheIBMmodeldeﬁnestheconditionaltoken-tokenprobabilitiesPT[w|l]∝Xa1...XamP[w,a1...am|l]marginalizedoveralignments,where(slightlyabusingnota-tion)inanygivenpairedprogramandsentencedescriptionwewillhaveestimatedasetofalignmentsawj,lk...lnbe-tweenthej-thtokeninthedescriptioncorrespondingtooneormoretokenslk...lninthepairedprogram.Wethereforedeﬁnethedescription-lengthofeachtoken-tokentransla-tionasthesumofthedescriptionlengthsofthealignmentswhichexpressitunderalibraryL:Xai...XamP[d,a1...am|l,L]∝Xa1...Xam|ai|LandthedescriptionlengthsundertherefactoredlibraryL0containingnewabstractionscompressesaccordingto|a0wj,l0k...l0n|L0<|a0wj,lk...ln|L⇐⇒{l0icontainsonlylk...lnassubcomponents|l0k...l0n}(3)andwesaythataprimitivel∈Lisasubcomponentofarefactoredabstractionl∈Liftheabstractioncanbeβ-reducedsuchthatlappearsinit.Thatis,arefactoredalignmenta0:wi→{l0...ln}iscompressedonlywhenanewabstractionl0encapsulatesoverastrictsubsetoftheconstituentprogramprimitivesalreadyalignedtothewordintheoriginalalignment.Thisallowsustore-approximatethedescriptionlengthofthenewtranslationmodelwithrespecttoasemantically-equivalentprogramrefactoringwithoutinducingPT[w|l]fromscratch(whichwouldrequireretrainingthefulltranslationmodeloverthesentencesandrefactoredprograms.)S6.ExperimentsThissectiondescribesadditionaldetailsoneachofthedo-mains–stringediting,compositionalgraphics,andsceneunderstanding–inSection6ofthemainpaper(seeFigure2,maintextforexamplesfromallthreedomains,shownalongwiththesyntheticandhumanlanguageannotations).Wealsoprovideadditionaldetailsonthemodelandbaselinehyperparametersavailableforeachdomain.Alldatasetsgeneratedfortheseexperiments(includinghumanlanguageannotations)arereleasedandlinkstostaticrepositoriesareprovidedinthecoderelease.Wealsoreleaseacompletesetofcommandstoexactlyreplicateallmodelexperiments.Allexperimentsforwereconductedonahigh-poweredcom-putingclusterusingaﬁxedtrainingbudgetofwall-clocksearchtimepertaskforallmodelsandbaselinesinagivendomain(determinedviahyperparametersearchusingthebaselinemodelperdomain,andreportedonaper-domainbasisbelow).Theexperimentsonthestringeditingandgraphicsdomainsusedmodelstrainedusing48CPUsforsearch(usingtheoriginalparallelenumerativesearchimple-mentedinthereleasedcodefortheDreamCodermodelin(Ellisetal.,2021));andtheexperimentstrainedonthescenereasoningtaskused24CPUs(aspreliminaryexperimentsrevealedthattheseexperimentsrequiredshortersearchtimeforourmainmodel,andwewishedtoreducethecarbonfootprintoftheremainingexperimentsafterourﬁrsttwodomains.)Forallexperimentswetraintheneuralmodelsfor1×104gradientsteps.Forexperimentswithlanguage-guidedcom-pression,weuseanupperboundof5newabstractionsin-troducedperiteration.Formutualexclusivityexperiments,wesetαME=0.1.Forallexperiments,duringprogram-onlycompression(see(Ellisetal.,2021)foradiscussionofprogram-onlycompressionhyperparameters)weusethehyperparametersfrom(Ellisetal.,2021)forparsimonywithearlierwork:astructurepenaltyof1.5andpseudocounts=30.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsS6.1Domains(SeeFigure2,maintextforexamplesfromallthreedo-mains,shownalongwiththesyntheticandhumanlanguageannotations.)Asdiscussedinthemainpaper,eachdomainconsistsofadatasetoftasks;asetofprocedurallygeneratedsyntheticlanguageannotations;andasetofhumanlan-guageannotationsprovidedbyMechanicalTurkworkers;wealsodescribedthebaseprimitivesL0withwhichallmodels(includingbaselinesandablations)wereinitializedforeachdomain.S6.1.1STRINGEDITINGTasks:structuredstringtransformationproblemstakenfromapubliclyreleaseddatasetin(Andreasetal.,2017)(n=1000train;n=500test).Tasksconsistofinputdictio-narystringstransformedusingrandomlysampledregularexpressiontransducer(n=30examplespertask).Transduc-ersweresampledaccordingtoabstracttemplatesdeﬁnedin(Andreasetal.,2017)andrequiredidentifyingmatchedsequencesofcharactersandaddinglettersbeforethem;re-movingsequences;replacingthemwithnewsequences,ordoublingthesequenceeachtimetheyappeared(SeeFigure2A,maintext).Languagedata:Thehumanlanguagedatasetforthisdo-mainwaspreviouslycollectedby(Andreasetal.,2017).Wedeﬁnedasyntheticgrammarofhigh-leveltemplatesoverthegroundtruthregularexpressiontransducers(correspondingtotheoriginaltemplatesusedtogeneratethetasks.)Thesynthetictemplatesweredeﬁnedbasedonlanguagefromtheoriginalhumanannotations,andinmostcasescloselymatchedthetruehumanprovidedannotations(whichweregenerallyquitestructured),thoughwithsigniﬁcantlylessvariation(theoriginallanguagecontainedmultiplehumandescriptionspertask.Wegenerateasinglesyntheticforeachone.Thesyntheticdatasethasavocabularysizeofn=44forbothtrainandtest.Weusethehumanannota-tionsintheoriginaldatasetwhenevaluatingonhumandata,whichhaveavocabularyofn=727(train)andn=622(test).)Wegenerateasyntheticdatasetonthisdomainpartlybe-causeofinaccuraciesnotedin(Andreasetal.,2017).Thereleasedcodecontainsthecompletegenerationprocedureforthesesyntheticannotations.SeeFigure2Aforrepresen-tativetaskswithexamples,syntheticlanguage,andhumandescriptions.Initialprogramprimitives:WeinitializeallmodelswithasetL0ofLISP-likeprimitivesthatoperateoversubstringsequencestobothconstructregularexpressionmatchse-quencesandmanipulatestrings,augmentedwiththreetextmanipulation-speciﬁcprimitivesintendedforexecutingcon-structedregularexpressionsequences;tisapolymorphictypevariableusingstandardHindley-Milnerpolymorphismtyping(Pierce,2002).Theexecutionenginedoesincludearegex-matchingmodel;however,thesynthesismodelisnaivetothisexecutionengineandsimplysearchesforma-nipulationsovertheinputstringsandtheregexesasdataarrays.L0contains14substringmanipulationprimitives,givenbelowwithtypeinformation.WealsogiveasemanticglossforprimitivesthatarenotstandardLISPprimitives.•if(bool→t→t→t)•cons(t→list(t)→list(t))•car(list(t)→t)•cdrlist(t)→list(t•map((t0→t1)→list(t0)→list(t1))•tail(list(t)→t)•append(t→list(t)→list(t))Appendselementtoendoflist.•revcdr(list(t)→list(t))Takesallexceptthelastelementofthelist.•match(substr→substr→bool)Returnstrueiftheﬁrstargument,whenexecutedasaregularexpression,matchesthesecondargument.•regexsplit(substr→fullstr→list(substr))Attemptstoexecutetheﬁrstargumentasaregularexpression,andsplitsthesecondargumentintoalistofsubstrings,usingtheregularexpressionmatchasadelimiter(andincludesthematchedsequencesinthereturnedlist.)•flatten(list(substr)→fullstr)Flattensalistofsubstringsbackintoastring.•rconcat(substr→substr→substr)Concatenatestwosubstrings.•rnot(substr→substr)Takesasubstringargumentsandreturnsthesubstringliteral[ˆs]•ror(substr→substr→substr)Takessubstringliteralsaandbandreturnsthesubstringliteral((a)—(b))Wealsoinclude26characterconstantsoftypesubstrandconstantsdot(regularexpressionwildcardcharacter)andempty(emptystring).DomainhyperparametersWelargelyfollowpriorwork(Ellisetal.,2021)tosetalgorithmtrainingparameters;theSupplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsearlier(Ellisetal.,2021)usesa720senumerativesearchbudgetforsolvingbothtexteditingandgenerallistmanip-ulationtasks.Weusethesame720senumerativebudgethere.TheencoderE(t)followsthedomain-speciﬁcencoderusedfortextandlisteditingproblemsin(Ellisetal.,2021),a2-layerGRUwith64hiddenunits.Themodelistrainedforaﬁxedgradientstepbudget(10,000gradientsteps)andwesampleequallyatrandombetweensupervisiononthesolvedtrainingtasks(andtheirsolutionprogramsinthecurrentDSL)andsamplesfromthejointgenerativemodel.Aswith(Ellisetal.,2021),whengeneratingtasksfromthegenerativemodel,weuserandomlysampleinputs(onwhichweexecutegeneratedprogramstoproduceanoutput.)S6.1.2COMPOSITIONALGRAPHICSTasks:inversegraphicsproblems(n=200train;n=111test)whereeachsynthesisproblemisspeciﬁedbyanimageandsolvedbysynthesizingaprograminLOGOTurtlegraph-ics(Abelson&DiSessa,1986).Thedomainisinspiredbythegraphicsdomainin(Ellisetal.,2021)butintentionallyre-designedtobemuchmorechallenging(ground-truthpro-gramsaremuchlongeronaverageinthebaseprogramminglanguage)andexplicitlycompositional:thetrainingandtestingtaskscontainsimpleshapetasksdeﬁnedbycomposi-tionalparametersforasetofbasicshapes(asmalltriangle,amediumsquare;asmallsemicircle);complexshapetasksthatrequireinferringmorechallenging(andlonger)param-eterizedshapes(agreekspiralwitheightturns);andcompo-sitionaltasksdeﬁnedbygeometricrulesandrelationsoverthesimpleshapes(asevensidedsnowﬂakewithashortlineandasmalltriangleasarms;asmalltriangleconnectedbyabigspacefromasmallcircle)(SeeFigure2C).Simpleparameterizedshapesareeitherpolygons(triangle,square,[n]gon),curves(semicircle,circle)orlines.Simpleshapesareparameterizedbyoneofthreesizes(smallorshort;medium;andbig).Whengeneratingsyntheticlan-guagedescriptions,pluralizedobjectsaretokenizedwithseparatetokensforthenounlemmaandatokenfortheplu-ralsufﬁx(e.g.squares).Complexparameterizedshapesrequireconstructingmorecompleximagesoutofbasiclines,andareintendedtoevalu-ateperformanceontasksthatposeagreatersearchchallengeintheinitialDSL,andwhosestructureisnotdirectlycuedbycompositionalrelationshipsovereasiercomponents.Fur-ther,thecomplexshapescanbesolvedusingabstractions(e.g.forrepeatedlyrotatingapenatrightangles)thatarenotdirectlycuedbysharedlexicalnames–weevaluatethealgorithm’sabilitytolearnanduseabstractionsthatcorre-spondtousefulsublexicalstructuressharedacrossmultiplelexemes.Wedeﬁnefourtemplatefamiliesforcomplexshapes:spirals,staircases,zigzags,andstars.Compositionalgraphicstasksinvokecompositionalrela-tionshipsoverthesimpleparameterizedshapes.Wedeﬁnetemplatesforgenerating6familiesofcompositionaltasks:nested,nextto,separatedby,connectedby,inarow,andsnowﬂakes.Languagedata:WegatherhumanlanguageannotationsbyaskingMechanicalTurkworkerstowriteanimagede-scriptionfortherenderedgraphicsimagesthatspecifyeachtask.Eachworkerlabeled20trainingand10testingimagesafterviewingadisjoint,randomlysampledsetof15exam-pleimagespairedwiththeirsyntheticlanguagecaptions.(Workerswereaskedtowriteashort,cleardescriptionthatapersonorrobotcouldusetorecreatethepicture,andtoldthattheexampleswerepairedwithautomaticallygen-eratedcaptionsasanexampleofthekindsofdescriptionsyoucouldwriteforthispicture.)Wecontrolfordescriptionqualitybyrequiringworkerstocompleteareferencetaskontheirowndescriptions:afterwritingtheirinitialannotations,workerswererequiredtocorrectlymatcheachannotationtothetargetimage(fromamidstasetof12distractorsdrawnheuristicallyfromsimilarimagesonthefulltaskdataset,andotherimagestheythemselveshaddescribed),andonlyannotationscorrectlymatchedtothetargetimagewerere-tained(workersweregivenachancetoredescribepicturestheyfailedtomatchtotheirowncaptions.)Wepreprocessthehumandatasetminimallytostandardizenumberterms(e.g.weusethesametokentypeforboth3andthree)andtosplitpluralsintoalemmaandsufﬁx,asinthesyntheticdataset.Theﬁnaldatasethasavocabularysizeofn=562forbothtrainandtest.Aswiththestringeditingdomain,wedeﬁneasyntheticdatasetusingparameterizedtemplatesbasedonsystematiclanguagereusedinthehumanannotations(seeFigure2Aforacomparisonbetweenhumanannotationsandsyntheticlan-guage);aswiththatdomain,wechooseasyntheticdatasettoensuresystematicre-useofhighleveltermsforrepeatedcompositionalobjects(suchasthe“n-gon”or“snowﬂake”terminology.)WethengenerategraphicstasksbydeﬁningparameterizedtemplatesovergroundtruthprogramsinL0,andacorre-spondinggeneratorforsynthesizingnaturallanguagede-scriptionsbasedoneachgroundtruthprogram.Itisimpor-tanttonotethatthetemplatesaredeﬁnedatanyextremelyhighlevelandwerewrittenwithrespecttolow-levelpro-gramsinasimplegraphicslanguage(manyofwhichwerederivedbygeneralizingcompositionallyovercomplexstruc-turesin(Ellisetal.,2021),suchasthe‘snowﬂake’images).Initialprogramprimitives:Forcomparisonwithpriorwork,ourinitiallibraryonthisdomain(andthebaselan-guageusedtogeneratethegroundtruthgraphicsprograms)isanimplementationoftheLOGOGraphicsDSLusedin(Ellisetal.,2021),whichconsistsoffourtyped,impera-Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionstiveprimitivesmodeledwithintheλ−calculuswithastatemonadS:move:distance→angle→S→Spen-up:(S→S)→S→Sfor:int→(S→S)→S→Sget/set:(S→S)→S→Saswellasfourarithmeticoperators(+,-,*./),integerconstants(1-9),unitdistancesandangles(1meterand2πradians),andspecialvalues∞and(cid:15).Figure3(maintext)showsexamplesofthegraphicstasks,syntheticdescriptions,humandescriptions,andsamplepro-gramsinthegroundtruthinitialDSL.DomainhyperparametersWelargelyfollowpriorwork(Ellisetal.,2021)tosetalgorithmtrainingparameters.Con-sistentwiththegraphicsprogramexperimentsin(Ellisetal.,2021),wetrainallmodels,includingbaselinesandabla-tions,usinganenumerativesearchbudgetof1800spertask(bothwhenusingpureenumerativesearchfromtheDSLprior,andneurally-guidedsearchconditionedonthetaskexamplesandlanguagedescriptions);theresultsinTable1comparetherelativeadvantageofourmodelgiventhisﬁxedsearchtime.Wetrainallmodelson48CPUsdur-ingparallelenumerativesearch,andrunthealgorithmforamaximumof27iterations(seelearningcurves.Aswerunmultiplerandomseedreplicationsofmodelsinthisdo-main,wetunedtheiterationlimitbasedonperformanceontheﬁrstreplication,allowingmodelsmodelstotrainwhileperformancecontinuedtoincrease.Toconservecomputa-tionalresources,welaterstoppedseveralofourownmodelreplicationsbefore27iterations,astheyhadreachednearceilingperformance.Aswereportthebestheld-outtestscoreacrossall27iterationsforanyonemodel,theearlystoppingwouldonlyservetogiveaconservativeestimateonperformanceforthesemodels.)Werandomlyreorderthetrainingsetoftasksoncebeforetheﬁrstloop,theniteratethroughbatchesofn=40tasksateachiteration;learningcurvesshowresultsfromevaluatingonheld-outtaskseveryn=3iterations.TheencoderE(t)followsthedomain-speciﬁcencoderusedfortheoriginalgraphicsdomainin(Ellisetal.,2021)foramoredirectcomparison:weusea6-layerCNN,whereeachlayerconsistsofa64x642Dconvolutionalsublayerwithkernelsize=3,aRELUactivationsublayer,andamax-poolingsublayerwithkernelsize=2.Themodelistrainedforaﬁxedgradientstepbudget(10,000gradientsteps)andwesampleequallyatrandombetweensupervisiononthesolvedtrainingtasks(andtheirsolutionprogramsinthecurrentDSL)andsamplesfromthejointgenerativemodel.S6.1.3SCENEREASONINGTasks:inductivescenereasoningtasks(n=212train;n=115test)whereeachsynthesisproblemisspeciﬁedbyastruc-turedinputscene,andoutputscanbeanumber(howmanyredrubberthingsarethere?),abooleanvalue(aretheremorebluethingsthangreenthings?),oranotherscene(whatifalloftheredthingsturnedblue?).ThisdomainismodeledonCLEVR(Johnsonetal.,2017)butdesignedtosupportnon-linguistic,inductivesynthesisintheprogramming-by-exampleparadigm:eachtaskisspeciﬁedwithn=7pairedinputoutputexamples.SeeFigure2B,maintextforexam-pletasksshowcasingtheoriginalandextendedtemplates,syntheticlanguageannotations,andhumanlanguageanno-tations.ThedatasetincludesquestionsrandomlygeneratedfromthefollowingsubsetoftheoriginalCLEVRquestiontemplates(see(Johnsonetal.,2017)foradditionaldetailsonthetaskgenerationprocessandquestiontemplates;wealsoreleaseourownaugmentedquestiongenerationcodeandthefulldataset):•zerohop:questionsthatrequirecountingoranswer-inganattributequeryaboutasubsetofobjectsinthescene.(e.g.Howmanysmallcylindersarethere?;Whatmaterialisthepurplething?).•onehop:questionssimilartothezerohoptasks,butthatrequirereasoningoveranadditionalrelationalquery(e.gWhatnumberofthingsarerightthesmallgraything?).•singleor:questionsthatadditionallyintroduceadis-junctionbetweensetsofobjects.(e.g.Howmanyobjectsareeitherlargemetalspheresorlargerubberthings?)).•(compareinteger:questionsthatadditionallyintro-ducea≥or≤operatorbetweencountsofsetsofob-jects.(e.g.Isthenumberoflargerubbercubeslessthanthenumberoflargegreenrubberthings?)•samerelate:questionsthatadditionallyrequirerea-soningaboutotherobjectswiththesameattributeasaspeciﬁedobject.(e.g.Howmanyotherthingsarethereofthesamesizeasthecyanthing?).WechoosethesetemplatesasarepresentativesubsetofthestyleofthefullCLEVRdataset,thatrequiresthefulllanguageofhigh-levelprimitivesin(Johnsonetal.,2017)tosolve.Weomitsomelongerquestionsinthesameformat(e.g.twohop)asourintentionistocomparesynthesisbaselines,ratherthantoachieveSOTAperformanceonCLEVR:thiswouldlikelyonlyincreasethecomputingresourcesneededtocomparethevariousmethodsandweSupplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsalreadyfoundasigniﬁcantdifferentialbetweenourmodelandthebaselinesontheshorterquestions.)WealsoaddnewquestiontemplatesgeneratedinthestyleoftheoriginalCLEVRtasks,butdesignedtomodelothercommonAItasks(suchasgeneratingnewscenesbasedonexistingones)andtorequirenewabstractions(thatwerenotexpressibleintheoriginalrestrictedsymboliclanguageusedtogeneratescenesin(Johnsonetal.,2017)):•localization:questionsforobjectlocalization.Thesereturnanoutputsceneconsistingofalocalizedsetofobjectsbasedonasetofqueryattributes(e.g.Findthegrayrubberthing.).•remove:questionsthateitherreturnanoutputscenewithasubsetoftheobjectsremoved,orthatqueryaboutlatentsceneswhereasubsetofobjectshasbeeremoved.(e.gWhatifyouremovedallofthegraymetalthings?;Ifyouremovedthegreencubes,howmanycubeswouldbeleft?).•transform:questionsthateitherreturnanoutputscenewhereasubsetoftheobjectshasbeentransformedtosetnewattributes,orthatqueryaboutlatentsceneswhereasubsetofobjectshasbeenmodiﬁedthisway.(e.gWhatifallthebluemetalthingsbecamerubberthings?;Ifallofthelargeyellowrubberthingsbecamegrayspheres,howmanygraysphereswouldtherebe?).Wetreattheseasprogramsynthesistasks:theinputscenesarespeciﬁedassymbolicscenegraphsconsistingofanar-rayofstructured,objectsdeﬁnedasadictionaryoftheirattributes,andprogramsaredesignedtomanipulatethesestructuredarrays(thisdatastructureistheoriginalformatinwhichscenesthemselvesaregeneratedin(Johnsonetal.,2017);theimagesdisplayedinFigure3,maintextareren-deredusingtheoriginalimagerenderingpipeline).Ourin-tentionisnottobuildavisualreasoningarchitecture:rather,weareinterestedinlearningstructuredmanipulationsofscenes.Weseeworkininversegraphics(suchas(Yietal.,2018))whichoutputsastructuredscenegraphbasedonpixelimagesastheﬁrststepinasymbolicprocessingandreasoningpipelineasanalogous;weareinterestedinthestructuredmanipulationofthesescenerepresentations.Languagedata:Syntheticlanguageannotationsaregener-atedbasedontheoriginalhigh-leveltemplatesin(Johnsonetal.,2017),aswellasadditionaltemplateswedeﬁnefortheextendedquestionsinthesamestyle.WegatherhumanlanguageannotationsbyaskingMechanicalTurkworkerstowriteaninstructionorquestiondescribingthesetofin-ductiveexamples.However,duetothedifﬁcultyofsolvingcertaintasksinalimitedtimeframebasedontheinductiveexamplesalone(suchasthequestionsaboutdisjunctionsoverscenes),weshowMechanicalTurkworkersthesyn-theticdescriptionsforthisdomainandaskthemtowriteasemanticallysimilardescriptionthatchangesmorethanonewordintheoriginalcaption,andthatwouldbe”morenatu-ralforahumantounderstand”.Thisparaphrasingparadigmissimilartothatusedin(Wangetal.,2015),thoughweﬁndthatincomparisontootherdomainsitgenerateslessdiverselanguagedata.)Weremoveallpunctuation,tokenizeonspaces,anduseanadditionaldomainheuristictostemallplurals(e.g.cubes).Initialprogramprimitives:WeinitializeallmodelswithasetL0ofLISP-likeprimitives.Thesearesimilartotheinitiallistmanipulationprimitivesusedinthestringeditingdomain:asbothdomainscanbetreatedasmanipulatingstructuredarrays,weareinterestedinlearningdifferenti-ated,domain-speciﬁcabstractionsbasedonaverysimilarbaselanguage.L0alsoincludesprimitivesforqueryingattributesofobjectsonthedomain(thesearetypedgettersthatsimplyquerytheobjectdictionaryofattributes)andsev-eraldomain-speciﬁcfunctionsnecessaryformanipulatingtheseattribute.Wedeliberatelyuseamuchmorebaselevelprogramminglanguagethanthehigh-level,domain-speciﬁclanguagehand-designedin(Johnsonetal.,2017);ourgoalistolearnthenecessaryabstractions.WegiveasemanticglossforprimitivesthatarenotstandardLISPprimitives.•if(bool→t→t→t)•cons(object→list(object)→list(object))•car(list(object)→object)•map((t0→t1)→list(t0)→list(t1))•fold((list(t)→list(t))→(t→list(t)→list(t))→list(t))•len(list(t)→int)•>(list(t)→bool)•<(list(t)→bool)•setunion(list(t)→list(t)→list(t))•setintersect(list(t)→list(t)→list(t))•setdifference(list(t)→list(t)→list(t))•relate(object→relation→list(t))Returnsanarrayofobjectsthatsat-isfyaspatialrelationwithrespecttoaninputobject.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsWealsoincludeequalitycomparatorsforeachoftheattributetypes(e.g.eqcolor?;gettersforeachat-tribute,andsettersforeachattribute.Wealsoincludeintegerconstants0-9forcountingandconstantsfortheattributes(blue,red,big,small,rubber,metal)basedontheoriginalobjectandspatialrelationconstants(Johnsonetal.,2017).Domainhyperparameters:Werunacoarsehyperparam-etersearchbasedonthebaselinemodeltosetthedomainhyperparameters.Wetrainallmodels,includingbaselinesandablations,usinganenumerativesearchbudgetof1000spertaskandrunthemodelsforamaximumof5iterations.werunmultiplerandomseedreplicationsreorderingthetrainingset,inthesamewayasthecompositionalgraphicsdomain.TheresultsinTable1alsocompareacurriculumorderingofthetrainingsetbasedonthenumberoftokensinthesyntheticlanguagecaptions(splitonspaces.)TheencoderE(t)isavariantoftheRNN-baseddomain-speciﬁcencoderusedfortextandlisteditingproblemsin(Ellisetal.,2021)(aswellasthestringeditingdomain).Themodelistrainedforaﬁxedgradientstepbudget(10,000gradientsteps)andwesampleequallyatrandombetweensupervisiononthesolvedtrainingtasks(andtheirsolutionprogramsinthecurrentDSL)andsamplesfromthejointgenerativemodel.Aswith(Ellisetal.,2021),whengen-eratingtasksfromthegenerativemodel,weuserandomlysampleinputs(onwhichweexecutegeneratedprogramstoproduceanoutput.)WeencodethesymbolicscenedatastructureswiththeRNNbyencodingaﬂattenedversionofthescenegraph.Thescenegraphisoriginallystoredasadictionaryofattributes;whenﬂattened,weindicatethedic-tionarystructureusingspecialtokenstodenotethekeysandthestartandendofanyarraydelimiters(theoriginalscenegraphisfullyreconstructablefromtheﬂattenedversion.)S6.2ResultsandAdditionalQualitativeResultsInthissection,wediscussadditionalqualitativeresultsfromanindepthexplorationofthegraphicsdomainthatwereomittedfromthemainpaperforspace,butprovideaddi-tionalinsightonthebehaviorofthelearnedmodelinthehardestlearningdomain(basedonthedifferentialbetweenbaselineandLAPS-augmentedperformance.)Learnedabstractionsandsynthesizedprograms.Fig-ureS4(supplement)showsampleabstractionsintheﬁnallibrariesLfforthebestperformingmodelsinthegraph-icsdomainasaconcreteexemplarofabstractionsthatarelearnedandhowtheyareused,alongwithsampletaskssolvedwiththeseabstractions.Theﬁguresareshownasdependencygraphstoindicatehowprogressivelymorecom-plexabstractionsbuildonabstractionsatprioriterationsoflearning;wealsoshowselectedprobabilitiesfromthetranslationmodel(depictedareexamplesfromthetop-3primitivetranslationsforagivenword;someprimitivesarenothighprobabilitytranslationsforanyword.)Jointgenerativemodelsamples.FigureS3(supplement)showssamplesfromthejointgenerativemodelonthegraph-icsdomain(programsfromthelibrarywhichareexecutedtoproducethetaskexampleimage,andtranslatedtoproducelanguageannotations)atearlyandlaterstagesoftraining,indicatingthatthejointmodelitselfimprovesaslearningimproves,whichitselfallowsbettertrainingforthecondi-tionalinferencemodelandbetterabstractionguidingbasedonlanguage.ReferencesAbelson,H.andDiSessa,A.A.Turtlegeometry:Thecomputerasamediumforexploringmathematics.MITpress,1986.Andreas,J.,Vlachos,A.,andClark,S.Semanticparsingasmachinetranslation.InProceedingsofthe51stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2:ShortPapers),pp.47–52,2013.Andreas,J.,Klein,D.,andLevine,S.Learningwithlatentlanguage.arXivpreprintarXiv:1711.00482,2017.Artzi,Y.,Das,D.,andPetrov,S.Learningcompactlexiconsforccgsemanticparsing.2014.Balog,M.,Gaunt,A.L.,Brockschmidt,M.,Nowozin,S.,andTarlow,D.Deepcoder:Learningtowriteprograms.arXivpreprintarXiv:1611.01989,2016.Cho,K.,VanMerri¨enboer,B.,Gulcehre,C.,Bahdanau,D.,Bougares,F.,Schwenk,H.,andBengio,Y.Learn-ingphraserepresentationsusingrnnencoder-decoderforstatisticalmachinetranslation.arXivpreprintarXiv:1406.1078,2014.Dechter,E.,Malmaud,J.,Adams,R.P.,andTenenbaum,J.B.Bootstraplearningviamodularconceptdiscovery.InTwenty-ThirdInternationalJointConferenceonArtiﬁcialIntelligence,2013.Devlin,J.,Uesato,J.,Bhupatiraju,S.,Singh,R.,Mohamed,A.-r.,andKohli,P.Robustﬁll:Neuralprogramlearningundernoisyi/o.InProceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,pp.990–998.JMLR.org,2017.Dumanci´c,S.andCropper,A.Inventingabstractionsbyrefactoringknowledge.Ellis,K.,Morales,L.,Sabl´e-Meyer,M.,Solar-Lezama,A.,andTenenbaum,J.Learninglibrariesofsubroutinesforneurally–guidedbayesianprograminduction.InAd-vancesinNeuralInformationProcessingSystems,pp.7805–7815,2018.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractions⍴"L,𝜃𝑑&𝑡̂TExecute to produce examplesSample programs from priorTranslate to produce languageJoint generative model fromlearned  translation model Ta small square connected by a short line and a smalltriangle as armsJoint model samples :  Iteration 3a small five gonsa small five gon gona small five gon anda small five gona small six gona medium five gonJoint model samples : Iteration 15small 5 gons in a rowa small 6 gonas armsa small squareshort line3 small 5 gonand a small triangle(f41 (λ (x) ((f48 x)) (fn21 1 5 5 x))(f34 9 6 x)(f17 (λ (x) (get/set (λ(z) (f12 1 (f24 (x z))))(f20 (λ (u) (f15 5 u)) 1 33))) v)((f10 3 x) f34 (f31 (λ (x) x) (λ (y z) z)) 4 9 u)) 𝑡̂ = 𝑑& = 𝜌"= (f6 4 4 1 x)(f0 5 0 1 x)(f1 1 (f2 5 x))Figure3.(left)JointgenerativemodelJoverprogramssampledfromtheDSLpriorandnaturallanguageproducedbythetranslationmodelT(D|L),inferredfromsolvedtrainingtasks.Samplesfromthemodelareusedtotrainaneuralsynthesizertoguidesearchonmorechallenging,unsolvedtasks.(right)SamplesfromtheJgenerativemodelinthegraphicsdomainshowshowprogramcomplexityincreasesandgeneratedlanguageimprovesacrossiterations,asthesystembothaddsricherabstractionstotheDSLandlearnsbetteralignmentsoverthesolutionset,enablingthetrainedneuralmodeltosolvemorecomplextasks.10.91 | three0.98 | triangle. . .formovepen-up0.94 | four0.89 | squareOriginal DSL primitivesLearned translationprobabilities p(π | u) 0.31  | line0.31  | short0.09 | a342New primitives added through abstraction learningf0=(λ (x y z) (for x (λ (u v) (move z y v)))) move pen in parameterized loopf9=(f0 ∞ ε)0.07 | semicirclea small semicircle(f19 (f90 x))a medium semicircle(f3 (f90 x))a big semicircle(f9(* (/ ε 1) 5) x)f14=(λ (x y) (for 7 (λ (z u) (f9x u)) y))a big circle(f14(logo_DIVL1 4) x)a small circle(f14(logo_DIVLε 1) x)two nested circles(f14ε (f14 ε (f16 x)))0.16 | circle0.08 | turns0.09 | nestedf4=(λ (x y z) (f0x (/ 2π y) 1z))0.09 | smallrotates and draws a unit linef5=(λ (x y) (f4x x y))0.27 | gon0.22 | smallrotational symmetry by number of sidesa small five gon(f55 x)a small nine gon(f59 x)a medium seven gon(f52 (f20 7 x))f6=(λ (x y z u) (for y (λ (v w)(f5z (f5x w))) u))four small squares in a row(f5 2 (f61 4 4 x))six small five gons in a row(f61 6 5 x)...f24=(λ (x y) (f23 (λ (z u) (f21 y 0 x u))))0.09 | snowflake0.09 | armseight sidedsnowflake with a small seven gon as arms(f247 8 x)five sidedsnowflake with a short line and a medium five gon as arms(f245 (λ (x) (get/set (λ(y) (f2 1 (f41 5 y))) x))z)f32=(λ (x) (for x (λ (y z)(move 1 (/ 2π 4) (move 1 (-2π (/ 2π 4)) z)))))1.0 | stepped0.64|staircase0.36|zigzaga seven stepped staircase(f327 (get/set (λ (x) x)y))a four stepped staircase (f324 (get/set (λ (x) x)y))a five stepped zigzag(f25 (λ (x) x) 3 8 (f325y)......f17=(λ (x) (pen-up (λ (y) (f16 x y))))0.67 | separated0.15|next0.06|spacea small circle next to a small six gon(f14 ε (f14 ε (f172 (f5 6x))))a small nine gon next to a medium square(f5 9 (f5 1 (f171 (f20 4x))))Figure4.Abstractionsandprogramslearnedforthegraphicsdomain.Sampleabstractions(right)learnedfromaminimalstartingDSL(left)forsolvingprogressivelymorecomplexgraphicsprogramsynthesistaskswithlanguageannotations.Alsoshownwithtranslationprobabilities.Ouriterativealgorithmlearnsalignment-basedtranslationprobabilitiesbetweennaturallanguagewordsandprogramprimitivestoguideprogramsearchandabstraction(depictedareexamplesfromthetop-3primitivetranslationsforagivenword;someprimitivesarenothighprobabilitytranslationsforanyword.Supplemental:LeveragingLanguagetoLearnProgramSearchHeuristicsandAbstractionsEllis,K.,Wong,C.,Nye,M.,Sabl´e-Meyer,M.,Cary,L.,Morales,L.,Hewitt,L.,Solar-Lezama,A.,andTenen-baum,J.Dreamcoder:Bootstrappinginductiveprogram-synthesiswithwake-sleeplibrarylearning.PLDI2021,2021.Frank,M.C.,Goodman,N.D.,andTenenbaum,J.B.Us-ingspeakers’referentialintentionstomodelearlycross-situationalwordlearning.Psychologicalscience,20(5):578–585,2009.Gal,Y.andBlunsom,P.Asystematicbayesiantreatmentoftheibmalignmentmodels.InProceedingsofthe2013ConferenceoftheNorthAmericanChapteroftheAssoci-ationforComputationalLinguistics:HumanLanguageTechnologies,pp.969–977,2013.Gandhi,K.andLake,B.M.Mutualexclusivityasachallengefordeepneuralnetworks.arXivpreprintarXiv:1906.10197,2019.Heaﬁeld,K.Kenlm:Fasterandsmallerlanguagemodelqueries.InProceedingsofthesixthworkshoponstatis-ticalmachinetranslation,pp.187–197.AssociationforComputationalLinguistics,2011.Johnson,J.,Hariharan,B.,VanDerMaaten,L.,Hoffman,J.,Fei-Fei,L.,LawrenceZitnick,C.,andGirshick,R.Inferringandexecutingprogramsforvisualreasoning.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pp.2989–2998,2017.Koehn,P.,Hoang,H.,Birch,A.,Callison-Burch,C.,Fed-erico,M.,Bertoldi,N.,Cowan,B.,Shen,W.,Moran,C.,Zens,R.,etal.Moses:Opensourcetoolkitforstatisticalmachinetranslation.InProceedingsofthe45thannualmeetingoftheassociationforcomputationallinguisticscompanionvolumeproceedingsofthedemoandpostersessions,pp.177–180,2007.Lake,B.M.,Salakhutdinov,R.R.,andTenenbaum,J.One-shotlearningbyinvertingacompositionalcausalprocess.InAdvancesinneuralinformationprocessingsystems,pp.2526–2534,2013.Lake,B.M.,Salakhutdinov,R.,andTenenbaum,J.B.Human-levelconceptlearningthroughprobabilisticpro-graminduction.Science,350(6266):1332–1338,2015.Liang,P.,Jordan,M.I.,andKlein,D.Learningprograms:Ahierarchicalbayesianapproach.InProceedingsofthe27thInternationalConferenceonMachineLearning(ICML-10),pp.639–646,2010.Markman,E.M.andWachtel,G.F.Children’suseofmutualexclusivitytoconstrainthemeaningsofwords.Cognitivepsychology,20(2):121–157,1988.Nye,M.,Hewitt,L.,Tenenbaum,J.,andSolar-Lezama,A.Learningtoinferprogramsketches.arXivpreprintarXiv:1902.06349,2019.Parisotto,E.,Mohamed,A.-r.,Singh,R.,Li,L.,Zhou,D.,andKohli,P.Neuro-symbolicprogramsynthesis.arXivpreprintarXiv:1611.01855,2016.Pierce,B.C.Typesandprogramminglanguages.MITPress,2002.ISBN978-0-262-16209-8.Polosukhin,I.andSkidanov,A.Neuralprogramsearch:Solvingdataprocessingtasksfromdescriptionandexam-ples.2018.Shin,E.C.,Allamanis,M.,Brockschmidt,M.,andPolo-zov,A.Programsynthesisandsemanticparsingwithlearnedcodeidioms.InAdvancesinNeuralInformationProcessingSystems,pp.10824–10834,2019.Wang,Y.,Berant,J.,andLiang,P.Buildingasemanticparserovernight.InProceedingsofthe53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers),pp.1332–1342,2015.Wong,Y.W.andMooney,R.J.Learningforsemanticpars-ingwithstatisticalmachinetranslation.InProceedingsofthemainconferenceonHumanLanguageTechnol-ogyConferenceoftheNorthAmericanChapteroftheAssociationofComputationalLinguistics,pp.439–446.AssociationforComputationalLinguistics,2006.Yi,K.,Wu,J.,Gan,C.,Torralba,A.,Kohli,P.,andTenen-baum,J.Neural-symbolicvqa:Disentanglingreasoningfromvisionandlanguageunderstanding.InAdvancesinNeuralInformationProcessingSystems,pp.1031–1042,2018.Zhang,Y.,Pasupat,P.,andLiang,P.Macrogrammarsandholistictriggeringforefﬁcientsemanticparsing.arXivpreprintarXiv:1707.07806,2017.