Shell Language Processing:
Unix command parsing for Machine Learning

Dmitrijs Trizna

dmitrijs.trizna@helsinki.fi
University of Helsinki
Avast Software

2
2
0
2

l
u
J

7

]

G
L
.
s
c
[

3
v
8
3
4
2
0
.
7
0
1
2
:
v
i
X
r
a

Abstract

In this article, we present a Shell Language Preprocessing (SLP) library, which imple-
ments tokenization and encoding directed on parsing of Unix and Linux shell commands. We
describe the rationale behind the need for a new approach with speciﬁc examples when con-
ventional Natural Language Processing (NLP) pipelines fail. Furthermore, we evaluate our
methodology on a security classiﬁcation task against widely accepted information and com-
munications technology (ICT) tokenization techniques and achieve signiﬁcant improvement
of an F1-score from 0.392 to 0.874.

1 Introduction

One of the most common interfaces that system engineers and administrators use to manage
computers is the command line. Numerous interpreters called shells allow applying the same
operational logic across various sets of operating systems. Bourne Shell (abbreviated sh), Debian
Almquist shell (dash), Z shell (zsh), and Bourne Again Shell (bash) are ubiquitous interfaces to
operate Unix systems, known for speed, eﬃciency, ability to automate and integrate the diversity
of tasks.

Contemporary enterprises rely on auditd telemetry from Unix servers, including execve
syscall data describing executed shell commands. Generally, this data is analyzed using static,
manually deﬁned signatures for needs like intrusion detection. There is sparse evidence of
the ability to use this information source eﬃciently by modern data analysis techniques, like
machine learning or statistical inference. Leading ICT companies underline the relevance of the
aforementioned problem and perform internal research to address this issue [5].

Complexity and malleability are key challenges in pre-processing shell commands. The struc-
ture has many intricate details, like aliases, diﬀerent preﬁxes, and order and values of the text,
which make commands condensed and fast to input, but time-consuming when reading and
interpreting them.

In this paper, we present a Shell Language Processing (SLP)1 library, showing how it performs
feature extraction from raw shell commands and successfully use it as input for diﬀerent machine
learning models. The shell command-speciﬁc syntax and challenges are discussed in Section 2,
inner working, and features of our pipeline are covered in Section 3. Sections 4 and 5 discuss a
speciﬁc application case and provides performance evaluation of diﬀerent encoding techniques.

1https://github.com/dtrizna/slp

Copyright © 2021 for this paper by its authors. Use permitted under Creative Commons
License Attribution 4.0 International (CC BY 4.0).
Proceedings of the Conference on Applied Machine Learning for Information Security, 2021

 
 
 
 
 
 
Shell Language Processing

Shell Language Processing

2 Shell speciﬁc challenges

The syntax of shell commands depends on diﬀerent Linux binary implementations and the
way they handle input parameters. Standard techniques like the tokenize method from the
nltk package would result in wrong tokens since shell language heavily deviates from natural
language. To start with, spaces do not always separate diﬀerent parts of the command. Some
commands like sed take a raw regular expression as a parameter, space (and any other special
character) will not necessarily mean the start of a next token:

sed ’s/^chr//;s/\..* / /’ filename

On the contrary, java takes ﬂags without spaces at all, knowing where its parameter name

ends:

java -Xms256m -Xmx2048m -jar remoting.jar

Such malleability in syntactic patterns possess a signiﬁcant challenge for the successful pars-
ing of shell command lines. We know two libraries that attempt to address such speciﬁcs for
bash - bashlex 2 and bashlint 3, however none of them does its job perfectly. We utilize bashlex
for our needs as a primary parsing source since it provides heuristics for tokenization of the most
general patterns of shell commands. Still, there are multiple problems in the bashlex library’s
original syntax analysis process, for example in the "command within a command" case (known
by shell syntax $(cmd) or `cmd` ) like in:

export IP=$(dig +short example.com)

Such syntax is not handled by bashlex, where an embedded command is treated as a single
element. Therefore we implement additional syntactic logic wrapped around bashlex’s object
classes to handle this and other problematic cases.

3 Encoding

Subsequently, we decided to look forward to diﬀerent ways of representing data numerically. To
produce arrays out of textual data we implement (a) label, (b) one-hot, and (c) term frequency-
inverse document frequency (TF-IDF) encodings. Label encoding is built on top of the scikit-
learn [1], whereas the one-hot and TF-IDF encodings are implemented natively.

Having the tokenization and encoding now as operational functionalities, we were ready to
provide a user-friendly interface to utilize the underlying code. Therefore, we created dedi-
cated Python classes that formed the core of our interface library. The tokenization interface
is available via ShellTokenizer() class. Besides bashlex, we utilized a Counter object from
the collections package to store unique tokens and their appearance count. The Counter allows
working conveniently with the data concerning various visualizations. Further encoding is avail-
able by ShellEncoder(), which class allows to generate diﬀerent encoding methods and returns
a scipy sparse array. Substantially, all preprocessing pipeline can be achieved within four lines
of code:

st = ShellTokenizer()
corpus, counter = st.tokenize(shell_commands)

se = ShellEncoder(corpus=corpus,

token_counter=counter,
top_tokens=100)

X_enc = se.tfidf()

2https://github.com/idank/bashlex
3https://github.com/skudriashev/bashlint

Proceedings of the Conference on Applied Machine Learning for Information Security, 2021

Shell Language Processing

Shell Language Processing

Tokenizer

AUC

F1

Precision Recall

SLP (ours)
WordPunct
WhiteSpace

0.994 0.874
0.392
0.988
0.164
0.942

0.980
1.0
1.0

0.789
0.244
0.089

Table 1: Comparison of tokenization tecnhiques on security classiﬁcation task: SLP, WordPunct-
Tokenizer, WhiteSpaceTokenizer.

At this stage X_enc can be supplied to fit() method of API-interface supported by machine
learning (ML) models in libraries like scikit-learn [1] or even AutoML realizations like TPOT [3].
As a result, every administrator, security analyst, or even non-technical manager working with
a Unix-like system may parse shell data for ML-based analytics using our library. Only basic
Python coding skills and no knowledge of conventional Natural Language Processing (NLP)
pipelines are needed.

4 Experimental setup

Assessment of tokenization and encoding quality is done on the security classiﬁcation problem,
where we train an ML model to distinguish malicious command samples from benign activ-
ity. Legitimate commands consist from nl2bash dataset [4], which represents a shell commands
collected from question-answering forums like stackoverflow.com or administratively focused
cheat-sheets.

We perform the collection of malicious samples by ourselves, accumulating harmful examples
across penetration testing and hacking resources that describe how to perform enumeration of
Linux targets and acquire reverse shell connections from Unix hosts4. All commands within the
dataset are normalized. Domain names are replaced by example.com and all IP addresses with
1.1.1.1, since in our evaluation we want to focus on the command interpretability. We advice
to perform a similar normalization even in production applications, otherwise ML model will
overﬁt training data.

For classiﬁcation, we train a gradient boosting ensemble of decision trees, with the speciﬁc
realization from XGBoost library [2]. During the analysis of tokenization we do not implement a
validation, ergo use full dataset for training and compute metrics on the same data. Conversely,
during the evaluation of encoding techniques, we implement a 10 fold cross-validation, so the
resulting metrics are the mean across all validation passes.

5 Evaluation

First experiments were conducted to assess the quality of our tokenization. We have tokenized
aforementioned dataset using our ShellTokenizer, with alternatives from NLTK WordPunctTokenizer,
and WhiteSpaceTokenizer which are known to be used in ICT industry for log parsing. Then
all the three corpora are encoded using the same term frequency-inverse document frequency
(TF-IDF) realization.

Security classiﬁcation results are available in Table 1. We see that conventional tokenization
techniques fail to distinguish crucial syntactic patterns of commands, therefore both cases are
biased towards the majority class of benign samples (with high precision and low recall). This
situation is unacceptable for tasks like security classiﬁcation - a model that fails to detect
malicious samples at all (false-negatives) cannot be deployed in a production environment. On

4For example https://blog.g0tmi1k.com/2011/08/basic-linux-privilege-escalation/

Proceedings of the Conference on Applied Machine Learning for Information Security, 2021

Shell Language Processing

Shell Language Processing

the contrary, our tokenizer achieves signiﬁcantly higher recall values, and, consequently, F1-score
(0.874 versus 0.392 from the closest alternative).

We consider on the evaluation of diﬀerent encoding techniques. Tests with our tokenization
implementation are done against three encoding schemes - TF-IDF, one hot, and label encoding.
Empirical evidence shows that none of the encoding techniques has a clear preference over the
others, therefore, all three encoding types are implemented within the library since even basic
logic like label encoding can yield the best results on speciﬁc data types and problem statements.
We encourage analysts to experiment with various shell preprocessing techniques to understand
which way beneﬁts their pipelines.

6 Conclusions

In this article, we presented custom tokenization and encoding techniques focused on Unix shell
commands. We describe the rationale of the dedicated tokenization approach, with speciﬁc
shell command examples where conventional NLP techniques fail, and brieﬂy cover the inner
working of our library. To distinguish this technique from known existing pipelines we evaluate
the security classiﬁcation task, with a custom dataset collected from real-world samples across
the web and an eﬃcient ensemble model. According to acquired metrics, our model achieves a
signiﬁcant improvement of the F1-score.

References

[1] L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Pret-
tenhofer, A. Gramfort, J. Grobler, R. Layton, J. VanderPlas, A. Joly, B. Holt, and G. Varo-
quaux. API design for machine learning software: experiences from the scikit-learn project.
In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108–
122, 2013.

[2] T. Chen and C. Guestrin. Xgboost. Proceedings of the 22nd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Aug 2016. doi: 10.1145/2939672.
2939785. URL http://dx.doi.org/10.1145/2939672.2939785.

[3] T. T. Le, W. Fu, and J. H. Moore. Scaling tree-based automated machine learning to

biomedical big data with a feature set selector. Bioinformatics, 36(1):250–256, 2020.

[4] X. V. Lin, C. Wang, L. Zettlemoyer, and M. D. Ernst. Nl2bash: A corpus and semantic

parser for natural language interface to the linux operating system, 2018.

[5] A. Patel, Z. Kunik, Z. Kocur, J. Bedziechowska, B. Nawrotek, P. Piasecki, and
M. Kowiel. Detecting adversarial behaviour by applying nlp techniques to command lines.
https://blog.f-secure.com/command-lines/, 2020.

Proceedings of the Conference on Applied Machine Learning for Information Security, 2021

