0
2
0
2

n
a
J

0
2

]

G
L
.
s
c
[

1
v
8
7
2
7
0
.
1
0
0
2
:
v
i
X
r
a

Mixed integer programming formulation of
unsupervised learning

Arturo Berrones-Santos
Universidad Aut´onoma de Nuevo Le´on

Facultad de Ingenier´ıa Mec´anica y El´ectrica

Posgrado en Ingenier´ıa de Sistemas

Facultad de Ciencias F´ısico Matem´aticas

Posgrado en Ciencias con Orientaci´on en Matem´aticas

AP 126, Cd. Universitaria, San Nicol´as de los Garza, NL 66450, M´exico

arturo.berronessn@uanl.edu.mx

January 22, 2020

Abstract

A novel formulation and training procedure for full Boltzmann ma-
chines in terms of a mixed binary quadratic feasibility problem is given.
As a proof of concept, the theory is analytically and numerically tested
on XOR patterns.

Keywords: Boltzamnn machines; Mixed integer programming; Unsu-

pervised learning.

1

Introduction

A central open question in machine learning is the eﬀective handling of unlabeled
data [1, 2]. The construction of balanced representative datasets for supervised
machine learning for the most part still requires a very close and time consuming
human direction, so the development of eﬃcient learning from data algorithms
in an unsupervised fashion is a very active area of research [1, 2]. A general
framework to deal with unlabeled data is the Boltzmann machine paradigm, in
which is attempted to learn a probability distribution for the patterns in the
data without any previous identiﬁcation of input and output variables. In its
most general setups however, the training of Blotzmann machines is compu-
tationally intractable [2, 3, 4].
In this contribution is established a relation,
which to the best of my knowledge was previously unknown, between Mixed
Integer Programing (MIP) and the full Boltzmann machine in binary variables.
Is hoped that this novel formulation opens the road to more eﬃcient learning
algorithms by taking advantage of the great variety of techniques available for
MIP.

1

 
 
 
 
 
 
2 Full Boltzmann machine with data as con-

straints

Consider a network of units with binary state space. Each unit depends on all
the others by a logistic-type response function,

xi = round





(cid:16)

1 + exp

1
− (cid:80)

j(cid:54)=i qj,ixj − bi



 ≡ fi,

(cid:17)

(1)

where the “round” indicates the nearest integer function, the q’s are pairwise
interactions between units and the b’s are shift parameters. As will later be
clear, the proposed model supports both supervised and unsupervised learning
and leads to a full Boltzmann machine in its classical sense.

Figure 1: Full Boltzmann machine with ﬁve units.

Suppose a data set of D visible binary vectors, {(cid:126)vd}, d = 1, 2, ..., D with I
components each and a collection of M hidden units um, m = 1, ..., M . The
total number of units in the system is N = I + M . If the connectivity and shift
parameters are given, each sample ﬁxes the binary vector (cid:126)xd = {(cid:126)vd, (cid:126)ud} and

2

therefore the data set imposes the following N D constraints:





I
(cid:88)



qi,jvd,j + bi

 ≤ 0,

(2)

(−1)vd,i

j(cid:54)=i

(−1)ud,m





M
(cid:88)

j(cid:54)=m



qm,jud,j + bm

 ≤ 0,

d = 1, 2, ..., D; i = 1, 2, ..., I; m = 1, 2, ..., M.

A posterior distribution for the parameters P ({qi,j, bi}|D), can be constructed
by the maximum entropy principle, which gives the less biased distribution that
is consistent with a set of constraints [5, 6]. This is done by the minimization
of the Lagrangian

(cid:90)

L =

P ln P d (cid:126)w +

N D
(cid:88)

r=1

λr (cid:104)constraint(r)(cid:105) ,

(3)

where the brackets represent average under the posterior, (cid:126)w is a vector that
contains the connectivity and shift parameters and the λ’s are positive Lagrange
multipliers. Due to the linearity of the system of inequalities (2), the average of
the constraints under P with ﬁxed unit values is simply given by the same set
of inequalities with the coeﬃcients qj,i’s and bi’s substituted by their averages
(cid:104)qj,i(cid:105)’s, (cid:104)bi(cid:105)’s. The maximum entropy distribution for the parameters is therefore
given by

P ({qi,j, bi}|D) =



exp

−

1
Z

(cid:88)

{d,i}

λ{d,i}


(−1)xd,i





N
(cid:88)







qi,jxd,j + bi





 ,

(4)

j(cid:54)=i

where Z is a normalization factor. So due to the linearity of the constraints, P
is a tractable (i. e. an easy to sample) product of independent two parameter
exponential distributions:

P ({qi,j, bi}|D) = P ( (cid:126)w|D) =

N
(cid:89)

n=1

αne−αn(wn−βn),

(5)

where (cid:104)wn(cid:105) = 1
+ βn [7]. Therefore a necessary and suﬃcient condition for the
αn
existence of the above distribution is the existence of the averages (cid:104)wn(cid:105), which
is determined by the satisfaction of the inequalities (2).

The representation of the posterior by its two parameter exponential form
Eq. (5) gives a codiﬁcation of the training data in terms of a tractable distribu-
tion for the parameters that in conjunction with Eq. (1) is in fact a distribution
for new unlabeled binary strings of data. For fully connected topologies, this is
what is usually understood by an equilibrium distribution of a full Boltzmann
machine [2].

3

Figure 2: Three diﬀerent architectures for the XOR problem.

2.1

Illustrative example 1: Supervised XOR

The theoretical soundness of the proposed approach is now shown through the
XOR logical table, D = {(0, 0, 0), (1, 0, 1), (0, 1, 1), (1, 1, 0)}. Let’s consider ﬁrst
a restricted architecture with only two directed arcs that connect two inputs
with an output unit, as represented in Figure 2A. The inequalities (2) in this
case read,

−q1,2 − b2 ≤ 0,

b2 ≤ 0, −q0,2 − b2 ≤ 0,
q0,2 + q1,2 + b2 ≤ 0.

(6)

There are no values for b2, q0,2 and q1,2 that satisfy all the inequalities. This is
reﬂected in the maximum entropy distribution,

P =

1
Z

e−λ1(b2)e−λ2(−q0,2−b2)e−λ3(−q1,2−b2)e−λ4(q0,2+q1,2+b2)

(7)

which to be a properly normalized product of two-parameter exponential dis-
tributions must satisfy the contradictory conditions b2 < 0, q0,2 > 0, q1,2 > 0,
|b2| < q0,2, |b2| < q1,2, |b2| > q0,2 + q1,2. A valid model is however attainable by
the addition of a single hidden unit. Consider the architecture represented in
Figure 2B. This leads to a two stage constraint satisfaction problem. The ﬁrst

4

stage is given by the data evaluated on the visible units,

q3,2f3(0, 0) + b2 ≤ 0,
−q0,2 − q3,2f3(1, 0) − b2 ≤ 0,
−q1,2 − q3,2f3(0, 1) − b2 ≤ 0,
q0,2 + q1,2 + q3,2f3(1, 1) + b2 ≤ 0,

for which solutions certainly exist. Take for instance,

|b2| < q0,2,

|b2| < q1,2,
f3(0, 0) = f3(1, 0) = f3(0, 1) = 0,

|b2 + q3,2| > q0,2 + q1,2,
f3(1, 1) = 1.

The second stage is consequently given by,

b3 ≤ 0,
q0,3 + b3 ≤ 0,
q1,3 + b3 ≤ 0,
−q0,3 − q1,3 − b3 ≤ 0,

(8)

(9)

(10)

for which solutions exist under the conditions b3 < −C, |b3| > |q0,3|, |b3| > |q1,3|,
|b3| < |q0,3 + q1,3|, where C is a positive constant. Therefore, the maximum
entropy distribution for the parameters of the model represented in the Figure
2B exists. Equivalently, this result shows that the classical XOR supervised
learning problem can be solved by the proposed MIP feasibility formulation.

2.2

Illustrative example 2: Unsupervised XOR

A model capable of unsupervised learning is sketched in Figure 2C. The system
of inequalities should be now extended to consider inputs to nodes x0 and x1,

(11)

q3,0f3(0, 0) + b0 ≤ 0,
−q2,0 − q3,0f3(1, 0) − b0 ≤ 0,
q1,0 + q2,0 + q3,0f3(0, 1) + b0 ≤ 0,
−q3,0f3(1, 1) − q1,0 − b0 ≤ 0,
q3,1f3(0, 0) + b1 ≤ 0,
q0,1 + q2,1 + q3,1f3(1, 0) + b1 ≤ 0,
−q2,1 − q3,1f3(0, 1) − b1 ≤ 0,
−q0,1 − q3,1f3(1, 1) − b1 ≤ 0,

which has indeed solutions, as discussed in the following section.

3 Sampling from the posterior distribution

The equilibrium posterior distribution of patterns can be sampled by taking
an arbitrary solution of the MIP feasibility problem and using it to deﬁne the

5

averages (cid:104)wn(cid:105) = 1
+ βn. The standard deviation of each two-parameter expo-
αn
nential distribution is given by σn = 1
, which can be at ﬁrst instance assigned
αn
to some positive value related to the constant C. If y is an uniform random
deviate in the interval [0, 1], then wn = − 1
ln(1 − y) + βn is a deviate from
αn
the two-parameter exponential distribution associated to wn. In this way, the
vector of visible units can be sampled in a computation time that is quadratic
in the number of total (visible and invisible) units.

Algorithm 1 (Pseudo-code for sampling from the maximum entropy posterior.)
1: Initialize: βn from a solution of the MIP feasibility problem and 1
αn

← (cid:15)C

((cid:15) arbitrary positive real number).

2: Asign value to size (desired number of samples).
3: Generate yτ , (τ = 1, ..., size×N ) uniform and independent random deviates

in the [0, 1] interval.
4: for s = 1 to size do
wn,s = − 1
5:
αn,s
6: Generate (cid:126)xs by inserting (cid:126)ws in Eq. (1)
7: end for

ln(1 − y) + βn,s, n = 1, ..., N

The step 6 of the algorithm above is made by starting with an inital random
binary vector (cid:126)x at each s. The self-consistent system Eq. (1) is then iterated.
No more than 10 iterations are needed to achieve convergence.

The sampling procedure Algorithm 1 is now shown through the XOR exam-

ple. Take an arbitrary solution of the MIP feasibility problem, say

f3(0, 0) = f3(1, 0) = f3(0, 1) = 0, f3(1, 1) = 1,
−b0 = −b1 = −b2 = −b3 = C,

(12)

q0,3 = q1,3 =

3
4

C,

q2,0 = q0,2 = q1,2 = q2,1 = −q0,1 = −q1,0 = 2C,
q3,0 = q3,1 = −q3,2 = 4C.

Due to the rounding operator in Eq. (1), any C > 0 can work. In the following
experiments the value C = 100 is used with sample sizes of 1500. Some of the
samples drawn for each C are shown.

(a) :

1
αn

= 0.1C ∀ n :

[011], [000], [110], [101], [000], [011], [000],

[000], [110], [110], [000], [011], [000], [000], [110]...

6

(b) :

1
αn

= 0.5C ∀ n :

[011], [000], [000], [111], [110], [000], [000],

[000], [000], [000], [111], [000], [101], [000], [011]...

(c) :

1
αn

= 2.0C ∀ n :

[100], [001], [111], [011], [000], [000], [101],

[000], [001], [000], [000], [001], [000], [100], [101]...

The resulting ratios of XOR patterns relative to non-XOR patterns over the

entire 1500 samples for each case are, (a) : 1, (b) : 0.9 and (c) : 0.6

4 Discussion

In the author’s view, this paper presents a formalism that has the potential not
only to give more eﬃcient learning algorithms but to improve the understanding
of the learning from data itself. Particularly, datasets explicitly constrain the
parameters of the learning model by a set of feasiblity mixed binary inequalities.
For ﬁxed binary values, the system is linear and continuous. For ﬁxed model
parameters, it’s a linear constraint satisfaction problem in binary variables. The
author together with collaborators is now working in diﬀerent ways to exploit
these structures in order to scale the framework to solve realistic large scale
unsupervised learning problems. In such problems, a measure proportional to
the number of satisﬁed constraints might be used to gide the learning procedure
and to assign sensible values to the 1
αn

hyperparameter.

acknowledgements

The author acknowledge partial ﬁnancial support from UANL and CONACyT.

Conﬂict of interest

The author declares that he have no conﬂict of interest.

References

[1] Oliver, A., Odena, A., Raﬀel, C. A., Cubuk, E. D., & Goodfellow, I. Realistic
evaluation of deep semi-supervised learning algorithms. Advances in Neural
Information Processing Systems, 3235-3246, (2018).

7

[2] Goodfellow, I., Bengio, Y., & Courville, A., Deep learning. MIT press,

(2016).

[3] Fischer, A., & Igel, C., Training restricted Boltzmann machines: An intro-

duction. Pattern Recognition, 47(1), 25-39, (2014).

[4] Li, R. Y., Albash, T., & Lidar, D. A., Improved Boltzmann machines with
error corrected quantum annealing. arXiv preprint arXiv:1910.01283, (2019).

[5] Jaynes, E. T., Information theory and statistical mechanics. Physical review,

106(4), 620, (1957).

[6] Shore, J., & Johnson, R., Axiomatic derivation of the principle of maximum
entropy and the principle of minimum cross-entropy. IEEE Transactions on
information theory, 26(1), 26-37, (1980).

[7] Kececioglu, D., Reliability engineering handbook (Vol. 1). DEStech Publi-

cations, Inc., (2002).

8

