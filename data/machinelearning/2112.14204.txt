1
2
0
2

c
e
D
8
2

]

C
O
.
h
t
a
m

[

1
v
4
0
2
4
1
.
2
1
1
2
:
v
i
X
r
a

Non-Convex Joint Community Detection and Group
Synchronization via Generalized Power Method

Sijin Chen∗

Xiwei Cheng†

Anthony Man-Cho So‡

Abstract

This paper proposes a Generalized Power Method (GPM) to tackle the problem of community
detection and group synchronization simultaneously in a direct non-convex manner. Under the
stochastic group block model (SGBM), theoretical analysis indicates that the algorithm is able
to exactly recover the ground truth in O(n log2 n) time, sharply outperforming the benchmark
method of semideﬁnite programming (SDP) in O(n3.5) time. Moreover, a lower bound of param-
eters is given as a necessary condition for exact recovery of GPM. The new bound breaches the
information-theoretic threshold for pure community detection under the stochastic block model
(SBM), thus demonstrating the superiority of our simultaneous optimization algorithm over the
trivial two-stage method which performs the two tasks in succession. We also conduct numerical
experiments on GPM and SDP to evidence and complement our theoretical analysis.

Keywords: Community detection, group synchronization, Generalized Power Method, iterative
algorithms, stochastic block model, orthogonal group, rotational group

1 Introduction

Community detection methods typically make use of the edge connectedness information of a given
observation network to generate an inference on the underlying clustering of the agents [1, 3, 12,
26]. However, if additional information of the agents besides the edge connectedness is available,
chances are that the recovery results of the clustering can outperform the pure community detection
methods, even its information-theoretic limit [2], by carefully exploiting these extra information
[7, 24, 27]. Based on this fact, one may proceed to wonder whether a simultaneous recovery of both
the underlying clustering and the additional agent structures can be achieved eﬃciently, or, more
eﬃciently than the trivial two-stage method that classiﬁes all the agents ﬁrst and then recovers the
structures according to the classiﬁcation results. In this paper, we study this question by considering
the scenarios where additional information comes from pairwise relative measurements in a group
G, which usually appears in group synchronization problems [4–6, 8, 17]. This assumption is also
motivated by a number of practical problems, for example the 2D class averaging in cryo-electron
microscopy single particle reconstruction [11, 19, 28], where 2D images rotated in diﬀerent directions
are required to be classiﬁed and aligned at the same time before further processes to denoise the
observation.

∗Department of Computer Science and Engineering, the Chinese University of Hong Kong (CUHK). E–mail:

sjchen0@cse.cuhk.edu.hk

†Department of Computer Science and Engineering, CUHK. E–mail: xwcheng0@cse.cuhk.edu.hk
‡Department of Systems Engineering and Engineering Management, CUHK. E–mail: manchoso@se.cuhk.edu.hk

1

 
 
 
 
 
 
Recently, in analogy to the celebrated stochastic block model (SBM) [2] for community detection,
[9] deﬁned a model and its joint optimization problem of community detection and synchronization,
which we inherit with necessary adjustments and generalizations. Assume that there are n agents
in a network partitioned into K communities, and each agent i corresponds to a group element
gi ∈ G. With probability p, we obtain the relative group measurement gig−1
j ∈ G for two agents
i, j belonging to the same cluster; with probability q, we obtain an observation noise g uniformly
sampled from G for two agents i, j belonging to diﬀerent clusters – a mechanism named the outlier
noise model [10, 18]. Given these observations, we aim to recover both the underlying clustering
and the corresponding group elements of all the agents. Figure 1 illustrates the problem settings
and the target of recovery.

Figure 1: Illustration of the joint optimization problem of community detection and group synchro-
nization. We observe a network of n = 30 agents falling in K = 3 equal-sized communities, with
relative measurement in G between any connected pair of nodes. The target is to recover both the
underlying cluster and the group element of each agent simultaneously.

This problem can be formulated as a maximum likelihood estimation (MLE) problem, yet it is
still non-convex and computationally challenging to solve. In [9], the authors proposed a semideﬁnite
programming (SDP) method to jointly perform the clustering and synchronization task. This joint
algorithm ﬁrst relaxes the original problem into a convex one that involves both the clustering
variables and the rotational group variables, and then use SDP to solve the convex problem. By
exploiting the mutual rotational measurements to recover the underlying community and rotation
of each agent simultaneously, it enjoys both theoretical and experimental advantages over the state-
of-the-art pure community detection methods. However, applying SDP to practice is yet time-
consuming when the problem scale gets large, since the standard interior point method for SDP
typically takes O(n3.5) time to ﬁnd an optimal solution. The theory developed in [9] also leaves
several open questions. For example, the theory only takes care of the G = SO(d) situation, and
rigorous analysis is only provided for two-cluster cases. These issues hinder the reliability of SDP
when applied to general situations.

1.1 Contributions

Diﬀerent from the methodology of convex relaxation adopted by the authors of [9], this paper pro-
poses an iterative Generalized Power Method (GPM) to directly tackle the non-convex optimization
problem of simultaneous community detection with group synchronization, and provides a theo-
retical guarantee for its linear convergence to the optimal solution under certain conditions. Our
contributions are threefold.

2

• Most signiﬁcantly, our algorithm sharply reduces the time complexity to O(n log2 n) from
O(n3.5) of SDP without any compensation on the lower bound for parameters. The iterative
algorithm is also structurally simple and practically convenient to implement.

• Moreover, in this paper, theoretical guarantees are provided for both rotational (G = SO(d))
and orthogonal (G = O(d)) synchronization, and the number of clusters can be ﬁnitely many,
i.e., Θ(1). These generalize the scenario of rotational synchronization with only 2 clusters, to
which the theoretical results of [9] apply.

• We also remark that the conditions for linear convergence in this paper are able to break the
K for pure community detection [2], thus
β >

information-theoretic lower bound
demonstrating the superiority of our joint method over the naive two-stage approaches.

α −

√

√

√

1.2 Organization

The rest of this paper is organized as follows. Section 2 brieﬂy introduces the preliminaries about
community detection and group synchronization, which are important to the presentation and
analysis of our joint problem. Then, Section 3 presents a formal deﬁnition of our probabilistic
model and formulate the non-convex optimization problem of simultaneous community detection
with group synchronization. Our non-convex methodology is elaborated in Section 4, followed by
a detailed statement of our main theoretical results in Section 5. Their proofs are developed in
Section 5–6. Section 7 presents the numerical results from computer simulations. Finally, we make
some discussions and conclude this paper in Section 8.

1.3 Notations

Throughout this paper, all matrices, vectors, and scalars are assumed to be real. We follow the
convention of using boldface letters to represent matrices and vectors, and reserve the normal letters
for scalars. We denote X (cid:62) the transpose of a matrix X. If the parameter d (to be deﬁned later)
divides the size of matrix X, we refer Xij to the (i, j)-th block of X of size d × d unless otherwise
speciﬁed, and refer Xi× to the i-th block row of X.

For any matrix X ∈ Rn×n and any positive integer k ≤ n, λk(X) is the k-th largest singular
value of X so that {σk(X)} forms a non-ascending sequence. Similarly, λk(X) denotes its k-th
largest (real) eigenvalue, given that X is symmetric. (cid:107)X(cid:107) := σ1(X), (cid:107)X(cid:107)F := tr (cid:0)X (cid:62)X(cid:1), and
(cid:107)X(cid:107)∗ := (cid:80) σk(X) respectively represents the operator norm, Frobenius norm, and nuclear norm
of X. We denote (cid:104)X, Y (cid:105) := tr (cid:0)X (cid:62)Y (cid:1) the (Frobenius) inner product of two matrices X and Y .

Consider the spectral decomposition QΛQ(cid:62) for any given symmetric X ∈ Rn×n, where the
diagonal elements in Λ are non-ascending. Based on Matlab notation, we deﬁne a mapping
U = eigs[k:l](X), such that U is the submatrix containing the k-th to l-th column of Q, where
1 ≤ k ≤ l ≤ n. When k = 1, we write for simplicity that U = eigsl(X).

For any integer n, m ≥ 1, the n × n identity matrix is denoted by In, the n × m all-one matrix
is denoted by 1n×m, and the n × m all-zero matrix is denoted by 0n×m. We use ⊗ and (cid:12) to
represent the operator of Kronecker product and Hadamard (elementwise) product of two matrices
with conforming shapes.

For two nonnegative functions f (n) and g(n), we say that f (n) = O(g(n)) if there exists some
C and N such that f (n) ≤ Cg(n) for all n ≥ N , and f (n) = Ω(g(n)) if there exists some C and
N such that f (n) ≥ Cg(n) for all n ≥ N . We say that f (n) = Θ(g(n)) if f (n) = O(g(n)) and
f (n)
f (n) = Ω(g(n)), and c is a constant if c = Θ(1). Also, f (n) = o(g(n)) if limn→∞
g(n) = 0. For any
integer n ≥ 1 we deﬁne [n] := {1, 2, ..., n}.

3

2 Preliminaries

This section introduces some important ingredients of community detection and group synchroniza-
tion problems that serve as preliminaries for the development of the remaining contents.

Community detection

The main problem to be studied in this paper has a strong correlation to community detection prob-
lems under the symmetric stochastic block model (SBM) [2] with parameters (n, p, q, K). Assume
that n agents in a network fall in K underlying communities of equal size m = n/K (balanced
clustering). Then, SBM(n, p, q, K) generates a random undirected graph G such that each two
distinct nodes are connected by an edge with probability p if they belong to the same cluster, and
with probability q otherwise. We assume without loss of generality that a node is always connected
to itself in SBM(n, p, q).

Clustering functions and clustering matrices [23] are deﬁned to formally represent the community
structure of the nodes. We denote C : [n] → [K] the clustering function that maps node i to cluster
C(i) where it belongs. Conversely, we deﬁne Ij := {i ∈ [n] | C(i) = j}, that is, the pre-image of C
at cluster j. A matrix H ∈ {0, 1}n×K is said to be a clustering matrix, if there exists a clustering
function C(i) such that Hij = 1 if and only if C(i) = j. This establishes a one-one correspondence
between C and H.

Further, one can show that H ∈ {0, 1}n×K is a clustering matrix if and only if H1K×1 = 1n×1,
and 11×nH = m11×K. It is then natural to deﬁne a set H of all n × K clustering matrices, such
that

H := (cid:8)H ∈ {0, 1}n×K (cid:12)

(cid:12) H1K×1 = 1n×1, 11×nH = m11×K

(cid:9) .

The fact that any permutation of the numbering of the clusters does not matter gives an equivalence
relation on H. For any H ∈ H, the equivalent class of H is deﬁned to be {HP | P ∈ SK}, where
SK is the K-dimensional permutation group. Therefore, given a ground truth H ∗, the estimation
error [23] of H ∈ H is deﬁned as

(cid:15)(H) = min
P ∈SK

(cid:107)H − HP (cid:107)F .

For arbitrary matrix M ∈ Rn×K, we say

ΠH(M ) := argmin
H∈H

(cid:107)M − H(cid:107)F = argmax

H∈H

(cid:104)H, M (cid:105)

(1)

is the projection of M onto H, where the second equality is due to the fact that (cid:107)H(cid:107)F =
n
for all H ∈ H. As is pointed out by [23], Problem 1 is equivalent to a minimum-cost assignment
problem (MCAP) that can be tackled eﬃciently by existing algorithms, such as [21]. The following
proposition will be useful in the later analysis on the time complexity of our algorithm.

√

Proposition 1 (Proposition 1, [23]). Problem 1 can be solved in O(K2n log n) time.

When the parameters p and q are located in the logarithmic sparsity region of SBM(n, p, q, K),
and β log n
n where α, β = O(1), [2] derived that one can recover the underlying
√
K under SBM. This is the information-theoretic limit for the

i.e., p = α log n
clustering if and only if
pure community detection problems.

β >

α −

√

√

n

4

Group synchronization

The main problem interacts with another non-convex optimization problem named group synchro-
nization, and our focus will be on the synchronization of orthogonal and rotational groups. We
denote O(d) the d-dimensional orthogonal group over R, such that

O(d) :=

Q ∈ Rd×d (cid:12)
(cid:110)
(cid:12) QQ(cid:62) = Q(cid:62)Q = Id
(cid:12)

(cid:111)

with the usual matrix multiplication as the group operation. The d-dimensional rotational group
SO(d), or special orthogonal group, is the kernel of the group homomorphism det(·) on O(d). In
other words,

SO(d) :=

Q ∈ Rd×d (cid:12)
(cid:111)
(cid:110)
(cid:12) QQ(cid:62) = Q(cid:62)Q = Id, det(Q) = 1
(cid:12)

.

In the typical formulation of group synchronization [8, 17], there are n agents in a measurement
network and the i-th agent corresponds to a group element gi ∈ G, where we specially consider
G ∈ {O(d), SO(d)} in this paper. For group G, we deﬁne a set of block matrices Gn ⊂ Rnd×d as
follows:

Gn :=

X =



X ∈ Rnd×d


(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)










g1
g2
...
gn






, gi ∈ G





.

Moreover, a block-diagonalization operator bdiag(·) is deﬁned, such that

bdiag(X) :=



g1






g2

. . .








gn

, ∀X =















g1
g2
...
gn

∈ Gn,

and we also denote bdiag(Gn) := {bdiag(X) | X ∈ Gn}.

The design and analysis of our algorithm heavily relies on the projection onto O(d), which is
also prevalent in various works on group synchronization problems [4, 16, 17]. For arbitrary matrix
X ∈ Rd×d, we deﬁne

ΠO(d)(X) := argmin
Q∈O(d)

(cid:107)X − Q(cid:107)F = argmax
Q∈O(d)

(cid:104)X, Q(cid:105)

(2)

d for
the projection of X onto O(d), where the second equality is due to the fact that (cid:107)Q(cid:107)F =
all Q ∈ O(d). Problem 2 can be categorized into the orthogonal Procrustes problem [14] that has a
closed-form solution as follows:

√

Proposition 2. For any given X ∈ Rd×d, suppose that U ΣV (cid:62) is the SVD of X. Then, ΠO(d)(X) =
U V (cid:62), and maxQ∈O(d) (cid:104)X, Q(cid:105) = tr(Σ) = (cid:107)X(cid:107)∗.

3 Problem formulation

In this section, our probabilistic model is deﬁned and the concerning non-convex optimization
problem is formulated as a maximum likelihood estimation (MLE). We then properly reformulate
this problem to ﬁt for our methodology and deﬁne a metric of estimation error.

5

3.1 The probabilistic model and the joint optimization problem

We now formally state our joint model named stochastic group block model (SGBM) with parameters
(n, p, q, K, d, G), and formulate the joint optimization problem as a maximum likelihood estimation
(MLE). Assume that n agents in a network fall in K underlying communities of equal size m = n/K,
and each agent i corresponds to a group element R∗
i ∈ G, where G ∈ {O(d), SO(d)}. All the group
elements constitute a block matrix R∗ ∈ Gn. Denote C∗ the clustering function, I ∗
k the pre-image
of C∗ at cluster k, and H ∗ ∈ H the clustering matrix. SGBM(n, p, q, K, d, G) ﬁrstly generates a
random undirected graph G = (V, E) under SBM(n, p, q, K). Then, it generates an observation
matrix A ∈ Rnd×nd, where its (i, j)-th block

Aij =






i R∗(cid:62)
j

R∗
,
Rij ∼ Unif(G),
R(cid:62)
ji,
0,

if {i, j} ∈ E and C∗(i) = C∗(j),
if {i, j} ∈ E, i < j, and C∗(i) (cid:54)= C∗(j),
if {i, j} ∈ E, i > j, and C∗(i) (cid:54)= C∗(j),
otherwise,

(3)

where Unif(G) is the uniform distribution over G with respect to the Haar measure. As an equivalent
statement, SGBM(n, p, q, K, d, G) generates a realization A of the random matrix Ar, where its
(i, j)-th block

Ar

ij =






i R∗(cid:62)
j

wijR∗
: wij ∼ Bern(p),
uijRij : uij ∼ Bern(q), Rij ∼ Unif(G),
A(cid:62)
ji,
Id,

if i < j and C∗(i) = C∗(j),
if i < j and C∗(i) (cid:54)= C∗(j),
if i > j,
otherwise.

(4)

Given the observation matrix A, [9] introduced the MLE problem for both the community

structure H ∈ H and the group elements R ∈ Gn, referred to as Joint-G in this paper:

max
R∈Gn
H∈H

(cid:88)

(cid:68)

Aij, RiR(cid:62)
j

(cid:69)

.

i,j∈Ik,∀k

(Joint-G)

3.2 Reformulation of the problem

Problem Joint-G is computationally intractable without proper reformulations. Deﬁning M ∈
Rnd×nd such that its (i, j)-block

(cid:40)

Mij =

RiR(cid:62)
j ,
0,

if C(i) = C(j),
otherwise,

we can convert Problem Joint-G to

Since

(cid:104)A, M (cid:105) .

max
R∈Gn
H∈H

Hi×H (cid:62)

j× =

(cid:40)

1,
0,

if C(i) = C(j),
otherwise,

(5)

(6)

(cid:16)

(cid:17)

Hi×H (cid:62)
we have Mij =
j×
which gives a low-rank decomposition of M .

RiR(cid:62)

j . Then, direct calculations can verify the following proposition

6

Proposition 3. Let V = (11×K ⊗ R) (cid:12) (H ⊗ 1d×d) ∈ Rnd×Kd. Then, for the matrix M ∈ Rnd×nd
deﬁned in (5), M = V V (cid:62). Moreover, V (cid:62)V = mIKd and hence

1√
m V is orthonormal.

Remark 1. Before heading for further results, let us take a closer look at the matrix V constructed
in the Proposition. It incorporates a similar structure with H, with exactly one nonzero block in
each block row and exactly m many nonzero blocks in each block column. It also encapsulates the
group information R, because Ri is precisely the unique nonzero block in the i-th block row of V .
1 0
1 0
0 1
0 1
may regard V as a generalization of H with group information R, and H a degenerated case of V
when G = SO(1) = {1}.

R1
0
0
R2
0 R3
0 R4

For example, given R =

. Therefore, one

, we have V =

R1
R2
R3
R4

and H =





































Proposition 3 then reformulates Problem 6 to a quadratic program subject to non-convex con-

straints, because

max
R∈Gn
H∈H

(cid:104)A, M (cid:105) = max
V ∈E

(cid:68)

A, V V (cid:62)(cid:69)

= max
V ∈E

tr(V (cid:62)AV ),

(JointQP-G)

where the feasible region E := {(11×K ⊗ R) (cid:12) (H ⊗ 1d×d) | R ∈ Gn, H ∈ H} is non-convex. Since
SO(d) is a subgroup of O(d), Problem JointQP-G can be relaxed to

tr(V (cid:62)AV ),

max
V ∈F

(JointQP-O(d))

where F := {(11×K ⊗ R) (cid:12) (H ⊗ 1d×d) | R ∈ O(d)n, H ∈ H} is again a non-convex feasible re-
gion. Carefully note that the outliers in SGBM(n, p, q, K, d, G) are still uniformly sampled from G,
although all the group elements are allowed to fall in the relaxed region O(d)n.

When E (cid:54)= F, it is necessary to introduce a rounding function R that maps a matrix V ∈ F

back to E. The rounding function is deﬁned blockwise:

R(V )ij = det(Vij)Vij, ∀i ∈ [n], j ∈ [K].

As we will prove at the end of Section 5.2, the relaxation from Gn to O(d)n is suﬃciently tight
(up to an observation-invariant transformation) thanks to this simple rounding function R, which
makes it possible to continue the study of both orthogonal and rotational scenarios in Problem
JointQP-O(d).

Finally, we say

ΠF (X) := argmin
W ∈F
is the projection of X onto F, for arbitrary X ∈ Rnd×Kd. An eﬃcient algorithm for computing the
projection will be presented in Section 4.

(cid:107)W − X(cid:107)F

3.3 The metric of estimation error

Consider an arbitrary V ∈ E. Similar to the permutation invariance mentioned in Section 2, any
permutation of the clusters makes no diﬀerence under the settings of SGBM, since the generation of
observation matrix does not depend on the speciﬁc numbering of clusters. Formally, V Q ∈ E would

7

yield the same probabilistic distribution of observation as V , where Q ∈ Q := {P ⊗ Id | P ∈ SK}
and SK is the permutation group on [K]. Moreover, right-multiplying any element of G commonly
on each cluster neither aﬀects the observation it induces. This is because for any pair of nodes i
and j belonging to the same cluster with group elements Ri, Rj, their relative measurement always
remains intact after a common right multiplication:

(RiU )(RjU )(cid:62) = RiU U (cid:62)R(cid:62)

j = RiR(cid:62)

j , ∀U ∈ G.

Therefore, V W ∈ E also yields the same probabilistic distribution of observation as V , where
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)










=: bdiag (cid:0)GK(cid:1) .

U1, ..., UK ∈ G










W ∈

. . .

UK

U2

U1



One can further deduce that all the combinations of these two types of operations in Q and
bdiag (cid:0)GK(cid:1) still preserves the observation, which constitute the following group:

Deﬁnition 1. Let PK(G) be a group equipped with the usual matrix multiplication as the group
operation, such that the matrix Q ∈ RKd×Kd belongs to PK(G) if and only if there exists R ∈ GK
and P ∈ SK such that Q = bdiag(R)(P ⊗ Id).

Remark 2. For example, given R1, R2, R3 ∈ O(d), the matrix Q =



R2



R3



R1

 ∈ P3(O(d)).

Note that PK(G) is a subgroup of O(Kd). To see this, we ﬁrst observe that it is a subset of O(Kd).
Then, for arbitrary Q1, Q2 ∈ PK(G), Q1Q−1
2 ∈ PK(G). Hence this is a subgroup of
O(Kd).

2 = Q1Q(cid:62)

With this deﬁnition at hand, an equivalence class of V is immediately given by the orbit of
V under the right group action of PK(G). To reiterate, this equivalence relation ∼ between two
points in E originates from the identical probabilistic distribution of their induced observations
under SGBM, which we cannot tell apart in any algorithm relying solely on these observations.
It naturally induces a quotient space E/ ∼. We now deﬁne a metric for this quotient space that
essentially measures the distance between the corresponding orbits of two arbitrary points.

Deﬁnition 2. For any V1, V2 ∈ E,

distG(V1, V2) := min

Q∈PK (G)

(cid:107)V1 − V2Q(cid:107)F

is said to be the distance between V1 and V2 in the quotient space E/ ∼.

Proposition 4. The quotient space E/ ∼ equipped with the distance function dist is a metric space.

Proof. To establish triangle inequality: for arbitrary V1, V2, V3 ∈ E, let

Q12 = argmin
Q∈PK (G)

(cid:107)V1 − V2Q(cid:107)F , Q23 = argmin
Q∈PK (G)

(cid:107)V2 − V3Q(cid:107)F .

Then, since PK(G) is a subgroup of O(Kd),

distG(V1, V2) + distG(V2, V3) = (cid:107)V1 − V2Q12(cid:107)F + (cid:107)V2 − V3Q23(cid:107)F
= (cid:107)V1 − V2Q12(cid:107)F + (cid:107)V2Q12 − V3Q23Q12(cid:107)F ≥ (cid:107)V1 − V3Q23Q12(cid:107)F
≥ min

(cid:107)V1 − V3Q(cid:107)F = distG(V1, V3).

Q∈PK (G)

The other two axioms for the metric space are easy to verify.

8

As a major motivation of Deﬁnition 2, we are specially interested in properly measuring the
estimation error of an arbitrary V ∈ E and the ground truth V ∗. Via the distance in the quotient
space, Deﬁnition 2 immediately gives a reasonable metric of estimation error of V :

Deﬁnition 3. Let V , V ∗ ∈ E be given and V ∗ is the ground truth. Then,

(cid:15)G(V ) := distG(V , V ∗) = min

Q∈PK (G)

(cid:107)V − V ∗Q(cid:107)F

is said to be the estimation error of V .

4 A non-convex methodology

We now propose a Generalized Power Method (GPM) to tackle Problem JointQP-O(d). The ap-
proach to initializing V 0 is given by Algorithm 3 to be covered in detail in Section 6.

Algorithm 1 GPM
1: Input: the observation matrix A, the initial point V 0.
2: for t = 0, 1, 2, . . . , T − 1 do
V t+1 ← ΠF (AV t)
3:
4: end for
5: if G = SO(d) then
6:
7: end if
8: Return: V T

V T ← R(V T )

It remains to design an algorithm that eﬃciently solves the projection onto F. To this end, we

ﬁrst deﬁne a mapping µ : Rnd×Kd → Rn×K that computes the blockwise nuclear norm

µ(X)ij = (cid:107)Xij(cid:107)∗ =

d
(cid:88)

k=1

σk(Xij), ∀i ∈ [n], j ∈ [K],

and present the algorithm as follows:

Algorithm 2 Projection onto F
1: Input: X ∈ Rnd×Kd
2: H ← ΠH(µ(X))
3: generate a sequence {ei}n
4: for i = 1, 2, · · · , n do
Ri ← ΠO(d)(Xiei)
5:
6: end for
7: V ← (11×K ⊗ R) (cid:12) (H ⊗ 1d×d)
8: Return: V

i=1 such that Hiei = 1

Proposition 5. Given that K, d = Θ(1), Algorithm 2 exactly solves ΠF (X) in O(n log n) time.

As its name suggests, the design of GPM is inspired by the classical power method as well
as its variant orthogonal iteration method [13] that computes the dominant eigenvector and the
invariant (orthogonal) subspaces of a matrix, respectively. In fact, one may have already observed

9

resemblances between Problem JointQP-O(d) and the classical eigenvalue problems above. For
example, both maximize quadratic objectives, and both the optimization variables are subject to
1√
norm and orthogonality constraints by noting that
m V is orthonormal. The algorithm in general is
concise: it reﬁnes the initial guess iteratively by simply taking matrix multiplication and projection
onto the relaxed feasible region F, involving no hyperparameters. Therefore, to implement our
GPM algorithm is practically convenient.

5 Main result

In this section, we ﬁrst formally state the main result, Theorem 1–3, on the guarantee of linear
convergence of GPM under the metric of the estimation error per iteration in the logarithmic
sparsity region of SGBM, i.e., p, q = O
, and then present the proof of Theorem 1. We
defer the proof of Theorem 2 to the Appendix, and Theorem 3 to section 6 where we provide a
randomized spectral clustering method for initialization of GPM.

(cid:16) log n
n

(cid:17)

5.1 Statement of the main result

Deﬁnition 4. Suppose V ∈ F with a clustering function C, and the observation matrix A ∈ Rnd×nd
is given. Let M = µ(AV ). Then the observation matrix A is said to preserve V by δ-separation
if MiC(i) − Mij ≥ δ > 0 for all i ∈ [n] and j (cid:54)= C(i), j ∈ [K].

Theorem 1 (Main). Suppose that the observation matrix A is generated by

(cid:18)

SGBM

n,

α log n
n

,

β log n
n

(cid:19)

, K, d, G

,

where α, β, K, d = Θ(1), and G ∈ {O(d), SO(d)}. Let V 0 ∈ F and A be the input of Algorithm 1.
Let V ∗ ∈ E ⊂ F be the ground truth matrix that incorporates the clustering function C∗. Then,
Algorithm 1 outputs V T ∈ E such that (cid:15)G(V T ) ≤ τ within O (cid:0)n log n log n
(cid:1) time, if the following
hold:
(i) there exists a constant χ > 0 such that A perserves V ∗ by χmp-separation;
2Kβ
(ii) for all i ∈ [n], µ(AV ∗)iC∗(i) ≥
α mp;
(iii) for χ > 0 satisfying (i), there exists

√

τ

√

(cid:115)
2

ρ > 2

d2
χ2 +

α2
2Kβ

such that (cid:15)O(d)(V 0) ≤

√
m
ρ .

Remark 3. Note that Theorem 1 is a deterministic statement given the three conditions above.
From a qualitative perspective, the ﬁrst and second conditions describe a desirable interaction between
the observation A and the ground truth V ∗ that suﬃciently separates the mutual measurement from
noise. As we would see through the proof of the theorem, they provide a decent bound for the Lipschitz
constant in our analysis of ΠF . The third condition, on the other hand, demands a reasonable initial
guess V 0 of O(
m) error before the reﬁnement steps in GPM. Since the largest possible estimation
error

√

sup
V ∈F

(cid:15)O(d)(V ) =

√

2Kdm = O(

m)

√

is of the same order, condition (iii) is rather tolerant.

10

For a suﬃciently large n, the following two companion theorems provide a probablistic lower
bound for the parameters (α, β, K, d) such that condition (i ) and (ii ) hold with probability at least
1 − n−Ω(1), and condition (iii ) holds with probability at least 1 − (log n)−Ω(1).

Theorem 2 (Condition (i ) and (ii )). Suppose that α, β, K, d = Θ(1), and the observation matrix
A is generated by SGBM for the given ground truth matrix V ∗ with the clustering mapping C∗. If

(cid:40) √

2Kβ < α,

√

α −

2Kβ log eα√

2Kβ

> K,

(7)

(8)

then condition (i) and (ii) in Theorem 1 happen simultaneously with probability at least 1 − n−Ω(1)
for a suﬃciently large n.

Theorem 3 (Condition (iii )). Suppose that α, β, K, d = Θ(1), and the observation matrix A is
generated by SGBM for the given ground truth matrix V ∗ with the clustering mapping C∗. Then,
there exists an algorithm that generates an initial V 0 satisfying condition (iii) in Theorem 1 with
proability at least 1 − (log n)−Ω(1) for a suﬃciently large n.

5.2 Proof of Theorem 1

To prove Theorem 1, we ﬁrst show the linear convergence of Algorithm 1 in the quotient space F/ ∼
under the relaxed metric of estimation error, (cid:15)O(d). Then, we present the tightness of the rounding
procedure R that brings forth Theorem 1 as a direct consequence.

Linear convergence under the metric (cid:15)O(d)

This section is devoted to proving the following theorem on the linear convergence of Algorithm 1
under the relaxed metric of estimation error (cid:15)O(d).

Theorem 4. Suppose that the observation matrix A is generated by

(cid:18)

SGBM

n,

α log n
n

,

β log n
n

(cid:19)

, K, d, G

,

where α, β, K, d = Θ(1), and G ∈ {O(d), SO(d)}. Let V 0 ∈ F and A be the input of Algorithm 1,
and {V 1, V 2, ...} a sequence generated by the iterations in Algorithm 1. Let V ∗ ∈ E ⊂ F be the
ground truth matrix that incorporates the clustering function C∗. Then,

(cid:15)O(d)(V t+1) ≤

1
2

(cid:15)O(d)(V t)

for arbitrary non-negative integer t, if condition (i)–(iii) in Theorem 1 hold.

Recall that GPM iteratively updates the variable by the map ΠF (A · ) : F → F. Therefore, in
order to prove Theorem 4, it should be crucial to identify the important properties of the projection
operator ΠF . We ﬁrst make two useful observations in the following Lemmas:
Lemma 1. For any Q ∈ PK(O(d)) and X ∈ Rnd×Kd, ΠF (XQ) = ΠF (X)Q.
Lemma 2. If condition (i) in Theorem 1 holds, then ΠF (AV ∗) = V ∗.

The core of the proof lies in controlling the behavior of the projection operator ΠF so that the
estimation error after each update can be bounded. One possible way, as Proposition 6 follows, is
to show ΠF possesses a Lipschitz-like property of linearly controlling the Frobenius distance of two
points after a projection.

11

Proposition 6. Let X = AV ∗. If condition (i) and (ii) in Theorem 1 hold, then

(cid:13)ΠF (X) − ΠF (X (cid:48))(cid:13)
(cid:13)

(cid:13)F ≤

(cid:115)

2
mp

d2
χ2 +

α2
2Kβ

(cid:13)X − X (cid:48)(cid:13)
(cid:13)

(cid:13)F .

for any X (cid:48) ∈ Rnd×Kd.

For the sequence {V 0, V 1, V 2, ...} generated by GPM, denote Qt = argminQ∈PK (G) (cid:107)V − V ∗Q(cid:107)F

for all t ≥ 0. Given that condtion (i ) and (ii ) in Theorem 1 hold, we have

(cid:13)V t+1 − V ∗Qt+1(cid:13)
(cid:13)

(cid:13)ΠF (AV t) − V ∗Qt(cid:13)

(cid:13)F ≤ (cid:13)
(cid:13)
(cid:13)
(cid:13)ΠF (AV tQt(cid:62)) − ΠF (AV ∗)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)ΠF (AV tQt(cid:62)) − V ∗(cid:13)
(cid:13)
(cid:13)F
α2
2Kβ

d2
χ2 +

2
mp

(cid:13)F =

(cid:115)

(cid:13)
(cid:13)

=

≤

(cid:13)
(cid:13)

(cid:13)AV tQt(cid:62) − AV ∗(cid:13)
(cid:13)
(cid:13)F

(cid:115)

=

2
mp

d2
χ2 +

α2
2Kβ

(cid:13)A(V t − V ∗Qt)(cid:13)
(cid:13)

(cid:13)F ,

where Lemma 1 yields the ﬁrst equality, Lemma 2 yields the second equality, and the second
inequality is due to Proposition 6. We can proceed to obtain

(cid:13)A(V t − V ∗Qt)(cid:13)
(cid:13)
(cid:13)F ≤
(cid:13)A − pV ∗V ∗(cid:62)(cid:13)
(cid:13)
(cid:13)V t − V ∗Qt(cid:13)
(cid:13)
(cid:13)
(cid:13)A − pV ∗V ∗(cid:62)(cid:13)

(cid:13)
(cid:13)(A − pV ∗V ∗(cid:62))(V t − V ∗Qt)
(cid:13)
(cid:13)
(cid:13)pV ∗V ∗(cid:62)(V t − V ∗Qt)
(cid:13)
1
m

(cid:13)V t − V ∗Qt(cid:13)
(cid:13)

(cid:13)F + m

(cid:13)F +

(cid:13)
(cid:13)
(cid:13)F

mp

(cid:13)
(cid:13)

(cid:13)
(cid:13)

(cid:13)
(cid:13)

√

+

≤

=

(cid:13)
(cid:13)pV ∗V ∗(cid:62)(V t − V ∗Qt)
(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)
(cid:13)
(cid:13)F

(9)

.

(V ∗Qt)(cid:62)V t − IKd

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)V t+1 − V ∗Qt+1(cid:13)
(cid:13)
(cid:13)
(cid:13)

(cid:13)
(cid:13)
(cid:13)
(cid:13)F
(cid:13)F and (cid:13)
(cid:0)V ∗Qt(cid:1)(cid:62) V t − IKd

(cid:13) and

1
m

(cid:13)V t − V ∗Qt(cid:13)
(cid:13)
(cid:13)
(cid:13)F

(cid:13)F , it re-
respectively,

Since we are expecting some relationship between (cid:13)
mains to bound the two components (cid:13)
as is presented in the following two Propositions.

(cid:13)A − pV ∗V ∗(cid:62)(cid:13)

Proposition 7. There exists c1, c2, c3 > 0 such that

(cid:13)
(cid:13)

(cid:13)A − pV ∗V ∗(cid:62)(cid:13)

(cid:13)
(cid:13) ≤ c1

√

qm + c2

√

pm + c3

(cid:112)log n

with probability at least 1 − n−Ω(1).

log n) bound. Diﬀerent
Remark 4. In the logarithmic sparsity region, Proposition 7 gives a O(
from the techniques in Theorem 6 of [17] for controlling a similar term, the result here does not rely
on the celebrated matrix Bernstein inequality [22], which loosely controls the term at O(log n).

√

Proposition 8. For ρ > 0,

m

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
m

(cid:0)V ∗Qt(cid:1)(cid:62) V t − IKd

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

1
2

(cid:114)

1 +

√

m
ρ

1
d

(cid:13)V t − V ∗Qt(cid:13)
(cid:13)
(cid:13)F

if (cid:13)

(cid:13)V t − V ∗Qt(cid:13)

(cid:13)F ≤

√
m
ρ .

12

Proof for Theorem 4. Invoking Proposition 7 and 8, (9) becomes

(cid:13)V t+1 − V ∗Qt+1(cid:13)
(cid:13)

(cid:13)F ≤

≤

(cid:115)

2
mp
√

α2
2Kβ

d2
χ2 +
(cid:115)

2 + o(1)
ρ

d2
χ2 +

α2
2Kβ

(cid:13)V t − V ∗Qt(cid:13)
(cid:13)
(cid:13)F

(cid:114)

1 +

(cid:32)

1
2

1
d

mp
ρ

(cid:33)

+ c0

(cid:112)log n

(cid:13)V t − V ∗Qt(cid:13)
(cid:13)
(cid:13)F

for a suﬃciently large n. When

√

(cid:115)
2

ρ > 2

d2
χ2 +

α2
2Kβ

,

we have (cid:15)O(d)(V t+1) ≤ 1
(cid:15)(V t) inductively applies to arbitrary nonnegative integer t, which establishes Theorem 4.

2 (cid:15)O(d)(V t). When condition (iii ) in Theorem 1 holds, the linear decay of

Tightness of rounding

It is an immediate implication of Theorem 4 that, prior to the rounding procedure, Algorithm 1
(cid:1) iterations. In each iteration of GPM,
obtains V T such that (cid:15)O(d)(V T ) ≤ τ within T = O (cid:0)log n
the projection takes O(n log n) time according to Proposition 5, and the time complexity of matrix
multiplication can also achieve O(n log n) due to their sparse structures. Therefore, the total time
complexity to obtain such a V T is O (cid:0)n log n log n
(cid:1). Compared with Theorem 1, the very diﬀerence
τ
lies in the metric of estimation error (cid:15)G after the rounding procedure R. We handle the tightness
of R, hence the tightness of relaxation from E to F unresolved in Section 3, by the following
Proposition.

τ

Proposition 9. Suppose that G = SO(d), and V ∗ ∈ E is the ground truth. Then, for any V ∈ F
such that (cid:15)O(d)(V ) <

2, (cid:15)G(R(V )) = (cid:15)O(d)(V ).

√

In theory and practice, the tolerance constant τ is usually far below

with Theorem 4, Proposition 9 completes the proof of Theorem 1.

√

2. Therefore, combined

6 Randomized spectral clustering

We now propose a randomized spectral clustering algorithm that generates an initial point V 0
satisfying condition (iii ) in Theorem 1 with high probability. Therefore, this algorithm is able to
serve as a qualiﬁed initializer of GPM.

13

Algorithm 3 Randomized spectral clustering

1: Input: the observation matrix A
2: Initialize: R0 ∈ Rnd×d
3: generate H 0 by Algorithm 2 in [12]
4: H 0 ← ΠH(H 0)
5: (cid:98)U ← eigsKd(A)
6: for i ∈ [K] do
pick τi ∈ I 0
7:
for v ∈ I 0
R0
end for

v ← argminR∈O(d)

10:
11: end for
12: V 0 ← (cid:0)11×K ⊗ R0(cid:1) (cid:12) (cid:0)H 0 ⊗ 1d×d
13: Return: V 0

9:

8:

(cid:1)

i uniformly randomly //randomized pivot selection
i do

(cid:13)
(cid:13)
(cid:13) (cid:98)Uv× − R (cid:98)Uτi×

(cid:13)
(cid:13)
(cid:13)F

(cid:16)

(cid:17)

= ΠO(d)

(cid:98)Uv× (cid:98)U (cid:62)
τi×

While the community structure and group information are jointly optimized in GPM for exact
recovery, this initialization algorithm adopts an intuitive two-stage design as condition (iii ) in
Theorem 1 tolerates a rough estimation. The ﬁrst stage generates a preliminary guess of the
community structure H. Similar to [23], we invoke Algorithm 2 in [12], a greedy spectral clustering
method, to obtain a (imbalanced) clustering matrix, which is then rounded to a balanced clustering
matrix H 0 ∈ H. Theory developed in [23] has shown that for any numerical constant C,

(cid:15)(H 0) ≤

(cid:115)

Cn
log n

(10)

with probability at least 1 − n−Ω(1). After obtaining an initialization H, the group elements are
estimated in the second stage. For each hypothesized cluster i obtained previously, a pivot node
denoted by τi is located by a randomized mechanism. Then, the relative group transformation
between the pivot and any other node is estimated utilizing the corresponding block rows of (cid:98)U =
eigsKd(A). We consolidate the two estimations into a matrix V 0 ∈ F as the return of the algorithm.
As the following Proposition points out, the algorithm has a decent theoretical performance

regarding the estimation precision of V 0 it generates.

Proposition 10. Suppose that α, β, K, d = Θ(1), and the observation matrix A is generated by
SGBM for the given ground truth matrix V ∗. Then, for any given constant ρ > 0, Algorithm 3
generates a V 0 such that

(cid:15)O(d)

(cid:0)V 0(cid:1) ≤

√

m
ρ

with probability at least 1 − (log n)−Ω(1) for a suﬃciently large n.

Proof of Theorem 3. In fact, equipped with Proposition 10, Theorem 3 is immediately established.
However, we remark that this is not necessarily the unique algorithm fulﬁlling the requirements
therein.

7 Numerical experiments

In order to corroborate the theoretical analysis completed previously, this section studies and evalu-
ates the performance of GPM through diﬀerent numerical experiments including its phase transition

14

behavior, convergence performance, and CPU time. When necessary, a comparison is also drawn
between GPM and the SDP method [9]. All the simulations are conducted via MATLAB R2021a on
a workstation hosting a 64-bit Windows 10 environment with 256GB RAM and Intel(R) Xeon(R)
CPU E5-2699 2.20GHz 2-processor CPU.

7.1 Phase transition

We ﬁrst report the phase transition behavior of GPM for both orthogonal and rotational scenarios.
For each selected pair of parameters (n, K), we increment α and β from 0 to n
log n , and generate the
observation under SGBM(n, α log n/n, β log n/n, K, d, G) mechanism within each iteration. Then
GPM is invoked N = 50 times to attempt to recover the ground truth. We regard an attempt
successful if and only if (cid:15)G(V (cid:62)) ≤ τ = 10−3, and the rate of success given α and β is thereby
deﬁned to be

r(α, β) =

number of successes
N

.

√

√

√

α −

β =

We plot r(α, β) versus the change of α and β in Figure 2, together with the theoretical threshold
for pure community detection
K (blue) and the lower bound claimed in Theorem
2 (red). In all the experiments, the results clearly exhibit a behavior of phase transition, and the
region of failure is suﬃciently controlled by the lower bound. The gap inbetween indicates that
even the improved lower bound is still not tight for GPM, which calls for further analysis of the
algorithm. One can also observe that the phase transition behave slightly diﬀerently for small and
large settings of α and β: empirically, the boundary delimiting failure and success concaves down
for smaller α and β, while it grows near-linear for larger parameters. This may suggest diﬀerent
properties of GPM in the logrithmic sparsity region and the linear region.

Then, the phase transition of our GPM is compared with SDP proposed in [9], which we imple-
ment with MATLAB CVX package and MOSEK solver. We only experiment on rotational scenarios
considering the focus of [9]. According to the results shown in Figure 3, the recovery performance
of GPM notably outperforms SDP, but the boundary of transition is less sharp.

7.2 Convergence performance and CPU time

The convergence performance of GPM is also studied. At G = O(d), n = 400 and three diﬀerent
settings of (K, α, β), we keep record of the estimation error stated in Deﬁnition 3 for each iteration
{V t}t≥0 executed by GPM. Experiments are repeated for 10 times for each group of parameters to
integrate the general patterns, with the resulting convergence curves plotted in Figure 4. Typically,
GPM is able to recover the ground truth within a considerably small number of iterations after
a linear decay of estimation error, which again aligns with our theoretical aﬃrmation. To further
evidence the superiority of GPM in respect of its time complexity, we also test the average CPU
time of both GPM and SDP on problems of diﬀerent scales. All the results reported in Table 1
obviously demonstrate a higher time eﬃciency of our algorithm than SDP.

8 Discussions and Conclusion

This paper proposes a Generalized Power Method (GPM) to directly tackle the non-convex problem
of joint community detection and group synchronization in the logarithmic sparsity region of SGBM.
From the theoretical side, we give a probabilistic bound for GPM to exactly recover the ground truth

15

(a) G = O(3), n = 100, K = 4

(b) G = O(3), n = 150, K = 3

(c) G = O(3), n = 200, K = 4

(d) G = SO(3), n = 100, K = 4

(e) G = SO(3), n = 150, K = 3

(f) G = SO(3), n = 200, K = 4

Figure 2: Phase transition results on GPM with three diﬀerent pairs of parameters (n, K) = (100, 4),
(150, 3), and (200, 4) in both orthogonal and rotational scenarios. The theoretical threshold for
pure community detection
K is plotted in blue; the improved lower bound claimed
in Theorem 2 is plotted in red.

β =

α −

√

√

√

(a) GPM

(b) SDP

Figure 3: Phase transition results on GPM and SDP with G = SO(3) and (n, K) = (50, 2). The
K is plotted in blue; the improved
theoretical threshold for pure community detection
lower bound claimed in Theorem 2 is plotted in red.

β =

α −

√

√

√

in O(n log2 n) time, as an improvement of the previous method relying on SDP [9] that typically
takes O(n3.5) time. We also propose a randomized spectral clustering method as an initializer
of GPM. The corresponding analysis indicates that the initializer itself is able to yield a good

16

051015200510152000.10.20.30.40.50.60.70.80.910510152025051015202500.10.20.30.40.50.60.70.80.9101020300510152025303500.10.20.30.40.50.60.70.80.91051015200510152000.10.20.30.40.50.60.70.80.910510152025051015202500.10.20.30.40.50.60.70.80.9101020300510152025303500.10.20.30.40.50.60.70.80.910246810024681000.10.20.30.40.50.60.70.80.910246810024681000.10.20.30.40.50.60.70.80.91(a) K = 5, α = 15, β = 10

(b) K = 8, α = 25, β = 15

(c) K = 10, α = 35, β = 25

Figure 4: Convergence results of GPM at G = O(d) and n = 400, with three diﬀerent groups of
parameters (K, α, β) = (5, 15, 10), (8, 25, 15), and (10, 35, 25).

parameters

CPU time (s)
GPM SDP
38.69
7.37
n = 50, α = 8, β = 5
n = 100, α = 15, β = 10
721.21
7.81
n = 200, α = 25, β = 15 11.40 3600+
n = 400, α = 35, β = 25 20.39 3600+

Table 1: Comparison of the average CPU time (in seconds) between GPM and SDP at G =
SO(3), K = 2.

approximation of the joint problem, which could also raise theoretical and practical interest.

As a generalization of the theory for SDP developed previously, our analysis of GPM applies to
both orthogonal and rotational synchronization, and makes up the unresolved cases of K ≥ 3. At
the same time, our probabilistic bound breaches the information-theoretic limit for pure community
detection under SBM, which implies that GPM ﬁnely exploits the additional information of group
structures embedded in the joint problem. Technically, we remark that the breach originates from
the concentration behavior of uniformly distributed SO(d) or O(d) when d ≥ 2, which is absent in
the Bernoulli random variables for the pure community detection problems.

Our work also opens up further questions of the joint problem and the theoretical understanding
of GPM. For example, the phase transition experiments indicate that the theoretical lower bound
of parameters remains sub-optimal and calls for continuing improvements. We mainly credit this to
the rather strong assumption of norm-separation on the blocks of V , i.e., condition (i) and (ii ) in
Theorem 1, while GPM still performs decently without this assumption in numerical experiments.
Therefore, it is hopeful to improve the lower bound by leveraging more properties of the problem
and the algorithm to circumvent this assumption. Moreover, this paper takes into consideration
the cases of K = O(1), yet K may increase with n in real problems. Generalization to broader
parameter regions then becomes another possible direction for future studies.

References

[1] E. Abbe, Community detection and stochastic block models: Recent developments, Journal of

Machine Learning Research, 18 (2018), pp. 1–86.

17

051015202530iteration10-810-610-410-2100102estimation error051015202530iteration10-810-610-410-2100102estimation error0510152025iteration10-810-610-410-2100102estimation error[2] E. Abbe and C. Sandon, Community detection in general stochastic block models: Funda-
mental limits and eﬃcient algorithms for recovery, 2015 IEEE 56th Annual Symposium on
Foundations of Computer Science, (2015), pp. 670–688.

[3] A. A. Amini and E. Levina, On semideﬁnite relaxations for the block model, 2016.

[4] M. Arie-Nachimson, S. Z. Kovalsky, I. Kemelmacher-Shlizerman, A. Singer, and
R. Basri, Global motion estimation from point matches, in 2012 Second International Confer-
ence on 3D Imaging, Modeling, Processing, Visualization & Transmission, IEEE, 2012, pp. 81–
88.

[5] A. S. Bandeira, Random laplacian matrices and convex relaxations, Foundations of Compu-

tational Mathematics, 18 (2018), pp. 345–379.

[6] A. S. Bandeira, N. Boumal, and V. Voroninski, On the low-rank approach for semideﬁ-
nite programs arising in synchronization and community detection, in Conference on Learning
Theory, 2016, pp. 361–382.

[7] N. Binkiewicz, J. T. Vogelstein, and K. Rohe, Covariate-assisted spectral clustering,

Biometrika, 104 (2017), pp. 361–377.

[8] N. Boumal, Nonconvex phase synchronization, SIAM Journal on Optimization, 26 (2016),

pp. 2355–2377.

[9] Y. Fan, Y. Khoo, and Z. Zhao, Joint community detection and rotational synchronization

via semideﬁnite programming, 2021.

[10] Y. Fan and Z. Zhao, Multi-frequency vector diﬀusion maps, in Proceedings of the 36th
International Conference on Machine Learning, K. Chaudhuri and R. Salakhutdinov, eds.,
vol. 97 of Proceedings of Machine Learning Research, PMLR, 09–15 Jun 2019, pp. 1843–1852.

[11] J. Frank,

Three-Dimensional Electron Microscopy

of Macromolecular Assem-
blies:Visualization of Biological Molecules in Their Native State: Visualization of Biological
Molecules in Their Native State, Oxford University Press, USA, 2006.

[12] C. Gao, Z. Ma, A. Y. Zhang, and H. H. Zhou, Achieving optimal misclassiﬁcation pro-
portion in stochastic block models, Journal of Machine Learning Research, 18 (2017), pp. 1–45.

[13] G. H. Golub and C. F. Van Loan, Matrix Computations, Johns Hopkins University Press,

4th ed., 2013.

[14] J. C. Gower and G. B. Dijksterhuis, Procrustes problems, vol. 30 of Oxford Statistical

Science Series, Oxford University Press, Oxford, UK, January 2004.

[15] B. Hajek, Y. Wu, and J. Xu, Achieving exact cluster recovery threshold via semideﬁnite

programming, 2016.

[16] S. Ling, Near-optimal performance bounds for orthogonal and permutation group synchroniza-

tion via spectral methods, 2020.

[17] H. Liu, M.-C. Yue, and A. M.-C. So, A uniﬁed approach to synchronization problems over

subgroups of the orthogonal group, 2020.

18

[18] A. Singer, Angular synchronization by eigenvectors and semideﬁnite programming., Applied

and computational harmonic analysis, 30 1 (2011), pp. 20–36.

[19] A. Singer, Z. Zhao, Y. Shkolnisky, and R. Hadani, Viewing angle classiﬁcation of cryo-
electron microscopy images using eigenvectors, SIAM Journal on Imaging Sciences, 4 (2011),
pp. 723–759.

[20] G. W. Stewart, Perturbation theory for the singular value decomposition, in In SVD and
Signal Processing, II: Algorithms, Analysis and Applications, Elsevier, 1990, pp. 99–109.

[21] T. Tokuyama and J. Nakano, Geometric algorithms for the minimum cost assignment

problem, Random Structures and Algorithms, 6 (1995), pp. 393–406.

[22] J. A. Tropp, User-friendly tail bounds for sums of random matrices, Foundations of Compu-

tational Mathematics, 12 (2011), p. 389–434.

[23] P. Wang, H. Liu, Z. Zhou, and A. M.-C. So, Optimal non-convex exact recovery in

stochastic block model via projected power method, 2021.

[24] H. Weng and Y. Feng, Community detection with nodal information, 2016.

[25] Y. Yu, T. Wang, and R. J. Samworth, A useful variant of the davis–kahan theorem for

statisticians, 2014.

[26] S.-Y. Yun and A. Proutiere, Accurate community detection in the stochastic block model

via spectral algorithms, 2014.

[27] Y. Zhang, E. Levina, and J. Zhu, Community detection in networks with node features,

Electronic Journal of Statistics, 10 (2016), pp. 3153 – 3178.

[28] Z. Zhao and A. Singer, Rotationally invariant image representation for viewing direction

classiﬁcation in cryo-em, Journal of structural biology, 186 (2014), pp. 153–166.

19

Appendix A Proofs

A.1 Proof of Proposition 5

The proof relies on the invariance of the inner product (cid:104)X, V (cid:105) for any X ∈ Rnd×Kd and V ∈ F,
when transitioning the mask H from V to X:

Lemma 3. For any X ∈ Rnd×Kd, R ∈ O(d)n, and H ∈ H,
X (cid:62)(cid:0) (11×K ⊗ R) (cid:12) (H ⊗ 1d×d)(cid:1)(cid:17)

= tr

tr

(cid:16)

(cid:16)(cid:0)X (cid:12) (H ⊗ 1d×d)(cid:1)(cid:62) (11×K ⊗ R)

(cid:17)

.

Proof of Proposition 5. We treat X as an n by K block matrix. By deﬁnition,

ΠF (X) = argmin

V ∈F

(cid:107)X − V (cid:107)F = argmin

V ∈F

where (cid:107)V (cid:107)2

F is constant because V ∈ F. Hence,

(cid:107)X(cid:107)2

F + (cid:107)V (cid:107)2

F − (cid:104)X, V (cid:105) ,

ΠF (X) = argmax

V ∈F

tr(X (cid:62)V ) = argmax

R∈O(d)n,H∈H

(cid:16)

tr

X (cid:62)(cid:0) (11×K ⊗ R) (cid:12) (H ⊗ 1d×d)(cid:1)(cid:17)

.

Applying Lemma 3,

ΠF (X) = argmax

R∈O(d)n,H∈H

= argmax

R∈O(d)n,H∈H

(cid:16)(cid:0)X (cid:12) (H ⊗ 1d×d)(cid:1)(cid:62) (11×K ⊗ R))
(cid:17)
tr
(cid:32) n
(cid:33)
(cid:88)

tr

(Xiei)(cid:62)Ri

,

i=1

(11)

where ei is such that Hiei = 1. For any ﬁxed H and every i ∈ [n], we denote U ΣV (cid:62) the SVD of
Xiei. By Proposition 2,

(cid:16)

(Xiei)(cid:62) Ri

= U V (cid:62)

(cid:17)

argmax
Ri∈O(d)

tr

and accordingly

max
Ri∈O(d)

tr

(cid:16)

(Xiei)(cid:62)Ri

(cid:17)

= tr(Σ) =: Miei.

Therefore, in order to maximize the expression in (11), it suﬃces to ﬁnd

(cid:88)

argmax
H∈H

Miei = argmax

H∈H

(cid:88)

MijHij = argmax

H∈H

(cid:104)H, M (cid:105) = ΠH(M ),

and then to perform O(d) projections for the selected blocks Xiei. This validates the algorithm.
Given that K, d = Θ(1), line 3 in the algorithm takes O(n log n) time according to Proposition 1,
and all others statements take O(n) time. This completes the proof.

A.2 Proof of Theorem 2

Firstly, we state two tail bounds for Bernoulli random variables and the random sum of uniformly
distributed orthogonal matrices, respectively.

Lemma 4 ([15], Lemma 2). Let X ∼ Binom(m, α log n/n) for m ∈ N, α = O(1), where m = n
some K > 0. Let τ ∈ (0, α]. Then for a suﬃciently large n,

K for

(cid:16)

Pr

X ≤

(cid:17)

log n

τ
K

= n− 1

K (α−τ log( eα

τ )+o(1)).

20

Lemma 5 ([9], Theorem A.3). Suppose that {ui}m
i=1 are two ﬁnite random sequences
independently and identically sampled from two independent distributions Bern(q) and Unif(O(d)),
respectively. Let S = (cid:80)m
i=1 uiRi. Then, with probability at least 1 − n−c,

i=1 and {Ri}m

(cid:107)S(cid:107) ≤ (cid:112)2qm(c log n + log 2d)

(cid:32)(cid:115)

1 +

c log n + log 2d
18qm

+

(cid:115)

c log n + log 2d
18qm

(cid:33)

.

Remark 5. Taking n = Km and q = β log n/n into Lemma 5, one can show that

(cid:107)S(cid:107) ≤

(cid:114)

2cβ
K

(cid:32)(cid:115)

1 +

cK
18β

+

(cid:115)

(cid:33)

cK
18β

log n

with probability at least 1 − n−c. Considering 18β (cid:29) cK, (12) is simpliﬁed to (cid:107)S(cid:107) ≤
This simpliﬁcation is always conducted throughout the following contents.

Now we present a straightforward result on the model parameters.

(12)

(cid:113) 2cβ

K log n.

Lemma 6. Suppose that the positive constants α, β, K, d are given. let f (τ ) = α − τ log eα
on (0, α]. If

τ deﬁned

(cid:40) √

2Kβ < α,

√

α −

2Kβ log eα√

2Kβ

> K,

then there exists ˜τ < α, ˜c > 1, and χ > 0, such that

(cid:40)

√

˜τ ≤
α − ˜τ log eα

2˜cKβ − χα
d ;
˜τ > K.

(13)

(14)

(15)

(16)

Proof. One can observe that f (τ ) = α − τ log (cid:0) eα
(cid:1) monotonically decreases in (0, α]. Therefore, the
τ
root ˜τ such that f (τ ∗) = K is uniquely determined in (0, α), and τ < τ ∗ if f (τ ) > K and τ ∈ (0, α].
2cKβ < τ ∗. By
By (13), there exists c1 > 1 such that
(14), there exists c2 > 1 such that

2cKβ < α for any 1 < c ≤ c1, and hence

√

√

α − (cid:112)2cKβ log

√

eα
2cKβ

> K

for 1 < c ≤ c2. Pick ˜c ∈ (1, min{c1, c2}], and ˜τ ∈ (
by taking χ = d(˜τ −

2˜cKβ)/α.

√

√

2˜cKβ, τ ∗). (15) and (16) immediately follow

Proof for Theorem 2. Denote M = µ(AV ∗). We ﬁrst consider the probability of two subevents
deﬁned as follows for ﬁxed i, j, j(cid:48) such that C(i) = j (cid:54)= j(cid:48), and then apply union bound.

(cid:40) there exists a constant χ > 0 such that Mij − Mij(cid:48) ≥ χmp;

Mij >

Observe that

√

2Kβ
α mp.

(17)

(18)

[AV ∗]ij =

(cid:88)

AikR∗

k =

(cid:88)

wikR∗

i +

(cid:88)

uikRikR∗
k,

k:C(k)=j

k:C(k)=j
C(k)=C(i)

k:C(k)=j
C(k)(cid:54)=C(i)

21

where wik ∼ Bern(p), uik ∼ Bern(q), Rik ∼ Unif(O(d)). In fact, the two parts in the summation
are complementary, i.e.

[AV ∗]ij =

(cid:40) (cid:16)(cid:80)

(cid:17)

R∗
k:C(k)=j wik
i ,
k:C(k)=j uikRikR∗
k, otherwise.

if C(i) = j,

(cid:80)

(19)

Therefore, due to edge independence, Mij = (cid:107)XR∗
i (cid:107)∗ = dX where X ∼ Binom(m, α log n/n).
Likewise, denoting S the random variable as stated in Lemma 5, σ1(Xij(cid:48)) = (cid:107)S(cid:107) since the distri-
bution of Unif(O(d)) is invariant under right (and left) orthogonal group actions, and consequently
Mij(cid:48) ≤ dσ1(Xij(cid:48)) = d (cid:107)S(cid:107). Then both (17) and (18) are guaranteed to happen when

(cid:40) X − (cid:107)S(cid:107) ≥ χ

d mp = χα

Kd log n;

(cid:107)S(cid:107) >

(cid:113) 2β
K .

(20)

(21)

With (7) and (8), we are able to invoke Lemma 6 to ﬁnd a group of parameters ˜τ , ˜c, χ such that






√

˜τ < α, ˜c > 1, χ > 0;
2˜cKβ − χα
d ;
˜τ > K.

˜τ ≤
α − ˜τ log eα

Then, Lemma 4 indicates that

(cid:18)

Pr

X ≥

(cid:19)

log n

˜τ
K

≥ 1 − n− 1

K (α−˜τ log eα

˜τ ),

while another probabilistic bound on (cid:107)S(cid:107) is derived from Lemma 5:

(cid:32)

Pr

(cid:107)S(cid:107) ≤

(cid:114)

2˜cβ
K

(cid:33)

log n

≥ 1 − n−˜c.

(22)

(23)

(24)

(25)

(26)

Combined with (23), the two events in 25 and 26 would immediately imply (20) and (21), and
consequently the subevents (17) and (18). They would further establish the ﬁnal proposition, given
that the probability of both events stated in (25) and (26) is suﬃciently high even after taking
union bound over all i ∈ [n] and j(cid:48) ∈ [K]. However, this is guaranteed by (22) and (24) because,
by union bound, both events hold for all i, j(cid:48) with probability at least

1 − nKn− 1

K (α−˜τ log eα

˜τ ) − n−˜c+1 = 1 − Kn− 1

K (α−˜τ log eα

˜τ −K) − n−˜c+1 = 1 − n−Ω(1).

This completes the proof.

A.3 Proof of Lemma 1

Proof. Since Q ∈ PK(O(d)), there exists a permutation π on [K] such that Qπ(i)i ∈ O(d) for all
i ∈ [K], and the remaining blocks of Q are zero. For any X = [Xij], we have

(XQ)ij =

K
(cid:88)

l=1

XilQlj = Xiπ(j)Qπ(j)j.

Therefore, µ(XQ)ij = (cid:13)
= µ(X)iπ(j), and µ(XQ) is in fact the column permutation of
µ(X) according to π. Denote H (cid:48), H the clustering matrices generated in the projection algorithm

(cid:13)Xiπ(j)

(cid:13)
(cid:13)∗

22

on the input X and XQ respectively. Then, H (cid:48)
such that H (cid:48)

ij = 1, we have

ij = 1 if and only if Hiπ(j) = 1. Now, for those i, j

ΠF (XQ)ij = ΠO(d) ((XQ)ij) = ΠO(d)

(cid:0)Xiπ(j)Qπ(j)j

(cid:1)

= ΠO(d)

(cid:0)Xiπ(j)

(cid:1) Qπ(j)j = ΠF (X)iπ(j)Qπ(j)j =

K
(cid:88)

l=1

ΠF (X)ilQlj.

Hence ΠF (XQ) = ΠF (X)Q.

A.4 Proof of Lemma 2

Proof. Denote H (cid:48) and H ∗ the clustering matrices determined in the projection algorithm on the in-
(cid:1) =
put AV ∗ and V ∗, respectively. Since condition (i ) holds, H (cid:48) = H ∗. By (19), ΠO(d)
R∗

(cid:0)(AV ∗)iC(i)

i . Hence ΠF (AV ∗) = V ∗.

A.5 Proof of Proposition 6

In order to establish the Lipschitz-like property of ΠF , we ﬁrst show that the maps µ and ΠO(d)
involved in the computation of ΠF have a similar behavior.

Lemma 7. For any X, X (cid:48) ∈ Rnd×Kd,

(cid:13)µ(X) − µ(X (cid:48))(cid:13)
(cid:13)

(cid:13)F ≤

√

(cid:13)X − X (cid:48)(cid:13)
d (cid:13)

(cid:13)F .

Proof. For simplicity we denote σk = σk(Xij) and σ(cid:48)

k = σk(X (cid:48)

ij). Then

(cid:12)
(cid:12)µ(X)ij − µ(X (cid:48))ij

1 + σ2 − σ(cid:48)

2 + ... + σd − σ(cid:48)
d

(cid:12)
(cid:12)

(cid:12) = (cid:12)
(cid:12)
(cid:12)σ1 − σ(cid:48)
√
(cid:113)

≤

√

≤

(σ1 − σ(cid:48)
d
d (cid:13)
(cid:13)Xij − X (cid:48)
ij

(cid:13)
(cid:13)F

,

1)2 + (σ2 − σ(cid:48)

2)2 + ... + (σd − σ(cid:48)

d)2

where Mirsky’s inequality [20] yields the ﬁnal step. Summing over the indices yields the desired
result.

Lemma 8 ([17], Lemma 2). If Xij = ηR where η > 0 and R ∈ O(d), then

(cid:13)
(cid:13)ΠO(d)(Xij) − ΠO(d)(X (cid:48)

ij)(cid:13)
(cid:13)F

≤

2
η

(cid:13)
(cid:13)Xij − X (cid:48)
ij

(cid:13)
(cid:13)F

for any X (cid:48)

ij ∈ Rd×d.

Proof of Proposition 6. Let ΠF (X) incoporate a community structure H and ΠF (X (cid:48)) incoporate
H (cid:48). Then one can observe

F = d (cid:13)
(cid:13)ΠF (X) − ΠF (X (cid:48))(cid:13)
(cid:13)
2
(cid:13)

(cid:13)H − H (cid:48)(cid:13)
2
F +
(cid:13)

(cid:88)

(cid:88)

j

i∈Ij ∩I(cid:48)
j

(cid:13)
(cid:13)ΠO(d)(Xij) − ΠO(d)(X (cid:48)

ij)(cid:13)
2
(cid:13)
F

.

23

By lemma 3 in [23], lemma 7, and lemma 8,

(cid:13)ΠF (X) − ΠF (X (cid:48))(cid:13)
(cid:13)
2
F ≤
(cid:13)

4d
δ2

(cid:13)M − M (cid:48)(cid:13)
(cid:13)
2
F +
(cid:13)

(cid:88)

(cid:88)

(cid:13)
(cid:13)ΠO(d)(Xij) − ΠO(d)(X (cid:48)

ij)(cid:13)
2
(cid:13)
F

j

i∈Ij ∩I(cid:48)
j

≤

≤

4d2
δ2

(cid:13)X − X (cid:48)(cid:13)
(cid:13)
2
F +
(cid:13)

4
η2

(cid:88)

(cid:88)

j

i∈Ij ∩I(cid:48)
j

(cid:13)
(cid:13)Xij − X (cid:48)
ij

(cid:13)
2
(cid:13)
F

(cid:18) 4d2

δ2 +

(cid:19)

4
η2

(cid:13)X − X (cid:48)(cid:13)
(cid:13)
2
F .
(cid:13)

Taking δ = χmp and η =

√

2Kβ
α mp yields the result.

A.6 Proof of Proposition 7

Proof. This is a direct generalization of Lemma 3.6 in [9] when K > 2 and the constraint is relaxed
from SO(d) to O(d). We apply similar notations. Observe that

Sout =










· · · S1K
S12 S13
0
S(cid:62)
· · · S2K
S23
0
12
13 S(cid:62)
S(cid:62)
· · · S3K
0
23
...
...
...
...
. . .
2K S(cid:62)
1K S(cid:62)
S(cid:62)
0
3K · · ·


















=

(cid:88)

j>i

S(cid:62)
ij

Sij









.

Therefore, (cid:107)Sout(cid:107) ≤ (cid:80)
the result can be established by union bound.

j>i (cid:107)Sij(cid:107), and likewise (cid:107)Sin(cid:107) ≤ (cid:80)

i (cid:107)Sii(cid:107). Following the argument therein,

A.7 Proof of Proposition 8

Proof. We denote for simplicity V e = V ∗Qt, and Z = 1
m V e(cid:62)V t. Recall that V e is the optimal
approximation of V t, so any per-cluster orthogonal transformation never yields a smaller diﬀerence.
Speciﬁcally,

(cid:13)
(cid:13)

(cid:13)V t − V eP (cid:62)(cid:13)

2
(cid:13)
(cid:13)
F

∈ PK(O(d)), Pi ∈ O(d).

subject to

(cid:13)V t − V e(cid:13)
(cid:13)
2
F = min
(cid:13)
P

P1








P =

P2

. . .








PK

24

However, note that

(cid:13)
(cid:13)

(cid:13)V t − V eP (cid:62)(cid:13)

2
(cid:13)
(cid:13)
F

min
P

=

=

=

=

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1





(cid:88)

j∈Ie

i ∩Ii

min
Pi∈O(d)



(cid:13)
(cid:13)V t
(cid:13)

ji − V e

jiP (cid:62)
i

(cid:13)
2
(cid:13)
(cid:13)
F

+

(cid:88)

(cid:13)
(cid:13)V t
(cid:13)

ji − V e

jiP (cid:62)
i

j∈Ie

i ∩Ii

i ∪Ii/Ie


2Kmd − 2 max
Pi∈O(d)



2Kmd − 2 max
Pi∈O(d)

(cid:88)

j∈Ie

i ∩Ii

(cid:42)

V t
ji,

(cid:68)

ji, V e
V t

jiP (cid:62)
i



(cid:69)

(cid:88)

jiP (cid:62)
V e
i

(cid:43)


(cid:18)

2Kmd − 2m max

Pi∈O(d)

j∈Ie

i ∩Ii
(cid:69)(cid:19)

(cid:68)
Pi, Z(cid:62)
ii

,

we obtain Pi = ΠO(d)

(cid:0)Z(cid:62)
ii

(cid:1) (and Pi = Id at the same time). By Proposition 2, we have

tr(Zii) =

(cid:68)

I, Z(cid:62)
ii

(cid:69)

(cid:68)

=

ΠO(d)

(cid:17)

(cid:16)

Z(cid:62)
ii

, Z(cid:62)
ii

(cid:69)

=

d
(cid:88)

k=1

σk (Zii) ;

(cid:107)Zii(cid:107)2

F =

(cid:16)

(cid:13)
(cid:13)
(cid:13)ΠO(d)

Z(cid:62)
ii

(cid:17)

Zii

(cid:13)
2
(cid:13)
(cid:13)
F

=

d
(cid:88)

k=1

σk (Zii)2 ;

σk(Zii) ∈ [0, 1].

We now claim that

To this end, observe that

(cid:107)Z − IKd(cid:107)2

F =

≤

≤

(cid:107)Z − IKd(cid:107)2

F ≤

(cid:18)

1 +

(cid:19)

1
d

[Kd − tr(Z)]2.


(cid:107)Zii(cid:107)2

F +

(cid:88)



(cid:107)Zij(cid:107)2

F − 2tr(Zii) + d



j(cid:54)=i

(cid:16)√


(cid:107)Zii(cid:107)2

F +

d − (cid:107)Zii(cid:107)F

(cid:17) (cid:88)

j(cid:54)=i



(cid:107)Zij(cid:107)F − 2tr(Zii) + d



(cid:18)

(cid:107)Zii(cid:107)2

F +

(cid:16)√

(cid:17)2

d − (cid:107)Zii(cid:107)F

(cid:19)

− 2tr(Zii) + d

.

K
(cid:88)

i=1

K
(cid:88)

i=1

K
(cid:88)

i=1

By (27), (28), and (29), we have

(cid:107)Zii(cid:107)2

F − 2tr(Zii) + d = σ2

1 + ... + σ2

d − 2(σ1 + ... + σd) + d

≤ ((1 − σ1) + ... + (1 − σd))2 = (d − tr(Zii))2 ,

and

(cid:16)√

d − (cid:107)Zii(cid:107)F

(cid:18)√

(cid:17)2

≤

d −

(cid:19)2

tr(Zii)
√
d

=

1
d

(d − tr(Zii))2 .

25





(cid:13)
2
(cid:13)
(cid:13)
F

(27)

(28)

(29)

(30)

(31)

(32)

Summing (31) and (32) over i, (30) yields

(cid:107)Z − IKd(cid:107)2

F ≤

(cid:18)

1 +

(cid:18)

=

1 +

(cid:19) K
(cid:88)

(d − tr(Zii))2 ≤

(cid:19)

i=1

(Kd − tr(Z))2 ,

1
d

1
d

(cid:18)

1 +

1
d

(cid:19) (cid:32) K
(cid:88)

i=1

(cid:33)2

d − tr(Zii)

which validates our claim. Finally, since (cid:13)

m (cid:107)Z − IKd(cid:107)F ≤

(cid:114)

1 +

1
2

for ρ > 0.

A.8 Proof of Proposition 9

(cid:13)V t − V e(cid:13)
2
F = 2mKd − 2mtr(Z), we have
(cid:13)
√

1
d

(cid:13)V t − V e(cid:13)
(cid:13)
2
F ≤
(cid:13)

(cid:114)

1
2

1 +

1
d

m
ρ

(cid:13)V t − V e(cid:13)
(cid:13)
(cid:13)F

Proof. We ﬁrst observe that the community structures of V and V ∗ are identical up to some
2. Otherwise, at least one node falls in a erroneous cluster and
permutation when (cid:15)O(d)(V ) <
(cid:15)O(d)(V ) ≥
2. Now, without loss of generality, we may identify the community structure
2d ≥
of V ∗ with that of V . Then no permutation is required to present the equivalence class of V ∗,
hence

√

√

√

(cid:15)O(d)(V ) =

min
W ∈bdiag(O(d)K )

(cid:107)V − V ∗W (cid:107)F ,

(cid:15)SO(d)(R(V )) =

min
W ∈bdiag(SO(d)K )

(cid:107)R(V ) − V ∗W (cid:107)F .

(33)

(34)

Our second observation is that no two group elements in the same cluster of V , say Ri and
Rj, belong to SO(d) and SO−(d) respectively. To see this, consider V e := V ∗W where W ∈
bdiag(O(d)K) is arbitrary. Observe that no two group elements in the same cluster of V e belong to
SO(d) and SO−(d) respectively, because W exerts a uniﬁed group action on each cluster of V ∗ ∈ E.
i ∈ SO−(d),
If the same does not hold for V , there must exist some i ∈ [n] such that Ri ∈ SO(d), Re
or Ri ∈ SO−(d), Re

i ∈ SO(d). In both cases, (cid:107)Ri − Re

2, ∀W ∈ bdiag(O(d)K), hence

√

i (cid:107)F ≥
√

(cid:15)O(d)(V ) = min (cid:107)(cid:107)F ≥

2.

Therefore, when (cid:15)O(d)(V ) <

√

2, the rounding procedure R(V ) = V T , where



D1Id

T =






D2Id

. . .








DKId

This together with (34) gives

∈ bdiag(O(d)K), Dk = det(Ri) ∀i ∈ Ik.

(cid:15)SO(d)(R(V )) =

min
W ∈bdiag(SO(d)K )

(cid:107)V T − V ∗W (cid:107)F .

Moreover, since T ∈ bdiag(O(d)n), (33) gives

(cid:15)O(d)(V ) =

min
W ∈bdiag(O(d)K )

(cid:107)V − V ∗W (cid:107)F =

min
W ∈bdiag(O(d)K )

(cid:107)V T − V ∗W (cid:107)F .

(35)

(36)

26

Denote W ∗ a minimizer of (36), i.e.,

W ∗ =

argmin
W ∈bdiag(O(d)K )

(cid:107)V T − V ∗W (cid:107)F .

√

Then (cid:107)V T − V ∗W ∗(cid:107)F <
observation that W ∗ ∈ bdiag(SO(d)K), lest the estimation error exceeds
minimizes (35). We then establish the equality (cid:15)SO(d)(V T ) = (cid:15)O(d)(V ).

2. Since V T ∈ E, it follows from an argument similar to the second
2. Therefore, W ∗ also

√

A.9 Proof of Proposition 10

We make use of the following variant of Davis-Kahan theorem on the distance between eigenspaces
of two real symmetric matrices.

Proposition 11 (Davis-Kahan, [25]). Let M , M ∗ ∈ RN ×N be symmetric matrices with eigenvalues
λ1 ≥ ... ≥ λN and λ∗
N , respectively. For any integers k, l such that 1 ≤ k ≤ l ≤ N ,
let U = eigs[k:l](M ), U ∗ = eigs[k:l](M ∗). Suppose that min{λ∗
l+1 > 0}, where
λ0 = +∞, λN +1 = −∞. Then, there exists Q∗ ∈ O(l − k + 1) such that

1 ≥ ... ≥ λ∗

k−1 − λ∗

l − λ∗

k, λ∗

(cid:107)U − U ∗Q∗(cid:107)F ≤

√

√
2
2
min{λ∗

l − k + 1 (cid:107)M − M ∗(cid:107)
k, λ∗
k−1 − λ∗
l+1}

l − λ∗

.

Proof of Theorem 3. We prove the existence of such an algorithm by showing that Algorithm 3 does
satisfy all the desired properties. Observe that V ∗/
m are the leading eigenvectors of pV ∗V ∗(cid:62)
with eigenvalues pm, while the other eigenvalues of pV ∗V ∗(cid:62) are all zero. By Lemma 11, there
exists Q∗ ∈ O(Kd) such that

√

(cid:13)
(cid:13)
(cid:98)U −
(cid:13)
(cid:13)

1
√
m

V ∗Q∗

(cid:13)
(cid:13)
(cid:13)
(cid:13)F

≤

√
2

2Kd
pm

(cid:13)
(cid:13)

(cid:13)A − pV ∗V ∗(cid:62)(cid:13)
(cid:13)
(cid:13) .

Denote Φ = 1√

m V ∗Q∗. By Proposition 7, for suﬃciently large n, there exists c4 > 0 such that

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13) (cid:98)U − Φ
(cid:13)F

2

≤

√

2Kd
pm

√

(cid:16)

c1

qm + c2

√

pm + c3

(cid:112)log n

(cid:17)

<

c4√

log m

.

Also, by direct calculation,

Φv× =

1
√
m

and it is a direct consequence that for all v ∈ [n],

R∗

vQ∗

C∗(v)×,

(cid:107)Φv×(cid:107)F =

d
m

.

Moreover, for v, u belonging to the same ground truth cluster, we have

Φv×Φ(cid:62)

u× =

1
m

R∗

vR∗(cid:62)
u .

Lemma 8 then implies

(cid:16)

(cid:13)
(cid:13)
(cid:13)ΠO(d)

(cid:17)

(cid:98)Uv× (cid:98)U (cid:62)
u×

− R∗

vR∗(cid:62)
u

(cid:13)
(cid:13)
(cid:13)F

≤ 2m

(cid:13)
(cid:13) (cid:98)Uv× (cid:98)U (cid:62)
(cid:13)

u× − Φv×Φ(cid:62)
u×

(cid:13)
(cid:13)
(cid:13)F

.

27

(37)

(38)

(39)

i ∩ I ∗
Now we consider u = τi and v ∈ I 0
whose validity with high probability will be proved at the end of this section:

π(i). Suppose the following conditions hold for all i ∈ [n],






τi ∈ I ∗

π(i);

there exists a constant c5 > 0 such that

(cid:13)
(cid:13)
(cid:13) (cid:98)Uτi× − Φτi×

(cid:13)
(cid:13)
(cid:13)F

(cid:113)

≤

1
(8Kd+c5)ρ2m .

(40)

(41)

Then (39) yields

(cid:13)
(cid:13)R0
(cid:13)

v − R∗

vR∗(cid:62)
τi

(cid:13)
(cid:13)
(cid:13)F

≤ 2m

(cid:13)
(cid:13) (cid:98)Uv× (cid:98)U (cid:62)
(cid:13)

τi× − Φv×Φ(cid:62)
τi×

(cid:13)
(cid:13)
(cid:13)F

.

Therefore, if we denote τC∗(v) = δ(v),

K
(cid:88)

i=1

≤8m2

(cid:88)

i ∩I∗

v∈I0
(cid:32) n
(cid:88)

π(i)

(cid:16)

(cid:13)
(cid:13)
(cid:13)

v=1
(cid:13)
(cid:13) (cid:98)U (cid:62)
(cid:13)

δ(v)×

≤8m2 max
v∈[n]

(cid:13)
(cid:13)R0
(cid:13)

v − R∗

vR∗(cid:62)
τi

(cid:13)
2
(cid:13)
(cid:13)
F

≤ 4m2

n
(cid:88)

v=1

(cid:13)
(cid:13) (cid:98)Uv× (cid:98)U (cid:62)
(cid:13)

δ(v)× − Φv×Φ(cid:62)

δ(v)×

(cid:13)
2
(cid:13)
(cid:13)
F

(cid:98)Uv× − Φv×

(cid:17)

(cid:98)U (cid:62)

δ(v)×

(cid:13)
2
(cid:13)
(cid:13)
F

+

(cid:13)
2
(cid:13)
(cid:13)
F

×

n
(cid:88)

v=1

(cid:13)
(cid:13)
(cid:13) (cid:98)Uv× − Φv×

n
(cid:88)

v=1
(cid:13)
2
(cid:13)
(cid:13)
F

(cid:16)

(cid:13)
(cid:13)
(cid:13)

(cid:98)Uδ(v)× − Φδ(v)×

(cid:33)

(cid:17)

Φ(cid:62)
v×

(cid:13)
2
(cid:13)
(cid:13)
F

+ 8m2 d
m

n
(cid:88)

v=1

(cid:13)
(cid:13)
(cid:13) (cid:98)Uδ(v)× − Φδ(v)×

(cid:13)
2
(cid:13)
(cid:13)
F

,

where the second inequality follows from triangle inequality, and the third from (38) and the in-

equality (cid:107)XY (cid:107)F ≤ (cid:107)X(cid:107)F (cid:107)Y (cid:107)F . Apply triangle inequality to (41), we have
some constant c6 > 0. (41) again implies

(cid:13)
(cid:13)
(cid:13) (cid:98)Uδ(v)×

(cid:13)
2
(cid:13)
(cid:13)
F

≤ c6

m for

K
(cid:88)

i=1

(cid:88)

v∈I0

i ∩I∗

π(i)

(cid:13)
(cid:13)R0
(cid:13)

v − R∗

vR∗(cid:62)
τi

(cid:13)
2
(cid:13)
(cid:13)
F

≤

8c2
4c6m
log m

+ 8m2Kd

1
(8Kd + c5)ρ2m

<

c7m
ρ2 ,

where 0 < c7 < 1 is a constant. This yields

(cid:15)(V 0)2 ≤

Cnd
log n

+

K
(cid:88)

i=1

v∈I0

π(i)

(cid:88)

i ∩I∗
m
ρ2 ,

≤

Cnd
log n

+

c7m
ρ2 <

(cid:13)
(cid:13)R0
(cid:13)

v − R∗

vR∗(cid:62)
τi

(cid:13)
2
(cid:13)
(cid:13)
F

which completes the proof.

We now show that (40) and (41) simultaneously hold with probability at least 1 − (log n)−Ω(1).

Firstly, according to (10), there exists a permutation π of the set [K] such that

(cid:12)
(cid:12)I 0
(cid:12)

i ∩ I ∗

π(i)

(cid:12)
(cid:12)
(cid:12) ≥ m −

CKm
2 log Km

for arbitrary i ∈ [K]. Therefore, for any ﬁxed i, τi picked in algorithm 3 satisfy (40) with probability
at least

1 −

CK
2 log Km

.

(42)

Secondly, for any size-m set T ⊂ [n], the size of the subset

(cid:26)

t ∈ T :

(cid:13)
(cid:13)
(cid:13) (cid:98)Ut× − Φt×

(cid:13)
2
(cid:13)
(cid:13)
F

≤

1
(8Kd + c5)ρ2m

(cid:27)

28

is at least m − m√

log m

. Otherwise (37) is contradicted since

(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13) (cid:98)U − Φ
(cid:13)
F

≥

(cid:88)

t∈T

(cid:13)
(cid:13)
(cid:13) (cid:98)Ut× − Φt×

(cid:13)
2
(cid:13)
(cid:13)
F

>

√

m
log m

1
(8Kd + c5)ρ2m

>

c2
4
log m

for a suﬃciently large m. Therefore, for any ﬁxed i, (41) holds with probability at least

By (42), (43) and union bound, (40) and (41) simultaneously hold for all i with probability at least

1 −

√

1
log m

.

(43)

1 − K

(cid:18) CK

2 log Km

+

√

(cid:19)

1
log m

= 1 − (log n)−Ω(1).

29

