9
1
0
2

t
c
O
4
1

]
L
M

.
t
a
t
s
[

2
v
1
2
6
9
0
.
9
0
9
1
:
v
i
X
r
a

On the Convergence of Approximate and Regularized Policy
Iteration Schemes

Elena Smirnova
e.smirnova@criteo.com
Criteo AI Lab

Abstract

Entropy regularized algorithms such as Soft
Q-learning and Soft Actor-Critic, recently
showed state-of-the-art performance on a
number of challenging reinforcement learn-
ing (RL) tasks. The regularized formulation
modiﬁes the standard RL objective and thus
generally converges to a policy diﬀerent from
the optimal greedy policy of the original RL
problem. Practically, it is important to con-
trol the sub-optimality of the regularized op-
timal policy. In this paper, we establish suf-
ﬁcient conditions for convergence of a large
class of regularized dynamic programming al-
gorithms, uniﬁed under regularized modiﬁed
policy iteration (MPI) and conservative value
iteration (VI) schemes. We provide explicit
convergence rates to the optimality depend-
ing on the decrease rate of the regularization
parameter. Our experiments show that the
empirical error closely follows the established
theoretical convergence rates. In addition to
optimality, we demonstrate two desirable be-
haviours of the regularized algorithms even
in the absence of approximations: robustness
to stochasticity of environment and safety of
trajectories induced by the policy iterates.

1 Introduction

The main principle of the entropy regularized approach
to RL [1, 2, 3, 4] is to modify the standard RL objec-
tive to additionally maximize the (relative) entropy of
a policy at each visited state. The regularization pa-
rameter, usually referred to as temperature, controls

Preliminary work. Under review by AISTATS 2020. Do
not distribute.

Elvis Dohmatob
e.dohmatob@criteo.com
Criteo AI Lab

the relative importance of the entropy term versus the
reward. The resulted regularized objective has shown
improved robustness to stochastic noise [3] and en-
vironment perturbations [1], as well as better explo-
ration targeted at high-value actions [2].

Recently, several theoretical frameworks have been
proposed to capture a multitude of entropy-regularized
methods [5, 6, 7]. In this paper, we focus on the regu-
larized MPI scheme [6] that generalizes both value and
policy-based regularized methods, such as popular soft
actor-critic [8] and soft VI [2]. We also analyse a re-
lated value-based scheme, the conservative VI [7], that
in addition to the soft VI includes the gap-increasing
methods, such as advantage learning [9, 10].

Despite the empirical success, the regularized algo-
rithms would not generally converge to the optimal
policy/value pair. A natural way of controlling the
optimality gap is through the regularization weight,
which when set at zero recovers the unregularized ob-
jective. Thus, a common idea is to gradually decrease
the regularization weight over the iterations to even-
tually converge into this regime.

Prior works considered decaying temperature during
[11]
learning in the context of speciﬁc algorithms.
proved asymptotic convergence of SARSA [12] with
Boltzmann policy and decaying temperature. Experi-
mentally, a linear schedule of the inverse temperature
over iterations was used with soft Q-learning [3] and
dual averaging algorithms [5]. The authors in [6, D.1]
suggested time-varying values of the regularizer weight
analogous to the learning rate in the gradient descent
approach.

A number of empirical and theoretical works suggest
that the regularization weight schedule changes the be-
haviour of the algorithms. In the approximate setting
with entropy regularizer, [7, 4.2] showed that larger
values of temperature parameter induce a higher de-
gree of error-tolerance of the value-based algorithms.
[8] interpreted temperature parameter as controlling
the stochasticity of the resulting policy with high val-

 
 
 
 
 
 
Manuscript under review by AISTATS 2020

ues of temperature resulting in policies close to uni-
form and thus, inducing more exploration.

2 Preliminaries

1.1 Summary of main contributions

In this work, we contribute by analysing convergence of
a large class of approximate and regularized dynamic
programming algorithms:

(1) We derive a convergence rate of the approximate
MPI [13] to optimality in terms of the decrease
rate of error sequence (Theorem 1),

(2) We derive a convergence rate of the regular-
ized MPI [6] to the optimal solution of the non-
regularized RL problem through the reduction to
the approximate MPI (Theorem 2).

(3) We derive a convergence rate of the conservative
VI [7] to optimality depending on the temperature
decay rate and the gap-increasing factor (Theo-
rem 3).

One consequence of the result (2) is that if the reg-
ularization weight decreases faster than the discount
factor (asymptotically), the regularized MPI scheme
converges as fast as the exact MPI. Otherwise, if tem-
perature decays at a slower rate, e.g., inverse poly-
nomially in the number of iterations, the algorithm
converges proportionally to the decay of the regular-
ization parameter. Thus, our result explicitly relates
the speed of convergence to a diﬀerent behaviour of
the algorithm, such as targeted exploration.

Our experiments demonstrate the convergence of the
soft VI [2] with temperature decay on a cliﬀ walking
domain [12]. We show that the empirical error follows
tightly the established theoretical bounds. In addition,
we show two interesting behaviour of the soft VI even
in the simplest exact dynamic programming setting:

2.1 Notations and terminology

∆X will denote the set of probability distributions
over ﬁnite set (or general measurable space) X and
Y X is a set of mappings from set X to set Y . We
consider a Markov decision process (MDP) is a tuple
M := (S, A, P, r, γ) where S is a state space, A is a
ﬁnite action space, P ∈ ∆S×A
is the transition ker-
S
nel so that the probability of the environment moving
to state s(cid:48) after the agent takes action a in state s is
P (s(cid:48)|s, a), accompanied by a reward r(s, a) (assumed
to be bounded). We deﬁne a stochastic stationary pol-
icy π ∈ ∆S
A. We consider the discounted setting with
discount factor γ ∈ [0, 1). We deﬁne the Bellman op-
erator T π for any function V ∈ RS , ∀s ∈ S as follows:
(cid:2)r(s, a) + γEs(cid:48)∼P (·|s,a)[V (s(cid:48))](cid:3)

[T πV ](s) := Ea∼π(·|s)

= rπ(s) + γP π(·|s)V,

(1)

where rπ ∈ RS and P π ∈ ∆S
S are deﬁned by rπ(s) :=
Ea∼π(·|s)[r(s, a)] and P π(s(cid:48)|s) := Ea∼π(·|s)[P (s(cid:48)|s, a)].
T π is a γ-contraction in (cid:96)∞ norm and its unique ﬁxed-
point is V π:= limk→∞(T π)kV = V π, where equality
holds component-wise. By denoting

QV (s, a) := r(s, a) + γEs(cid:48)∼p(s(cid:48)|s,a)[V (s(cid:48))],

(1) can be re-written as an inner-product

Eq.
[T πV ](s = (cid:104)π(·|s), QV (s, ·)(cid:105).

Finally, we deﬁne the Bellman max-operator as follows
(the max is point-wise)

T (cid:63)V := max
π∈∆S
A

T πV,

(2)

which again is a γ-contraction in (cid:96)∞ norm and its
unique ﬁxed-point is the optimal value function V (cid:63).
We denote by G(V ) the set of optimal policies that
achieve the maximum of Eq. (2) state-wise

(3) Faster convergence with an increased level of

stochasticity of the environment,

G(V ) := arg max

π∈∆S
A

T πV ⊆ ∆S
A.

(4) Safe trajectories induced by the policy iterates.

Equivalently, this set coincides with the set of optimal
policies: G(V ) = {π : T πV = T (cid:63)V }.

Finally, we show that the empirical convergence of the
conservative VI for varying temperature schedule and
gap-increasing factor is in line with the established the-
oretical rates.

Paper organisation.
In Section 2 we detail the no-
tations and introduce MPI-based algorithmic schemes.
In Section 3 we present our results (1-3), supported by
the experiments in Section 4. We discuss the related
works in Section 5.

2.2 Modiﬁed policy-iteration schemes

Modiﬁed Policy Iteration (MPI) [14]. MPI is a
classical dynamic programming algorithm that alter-
nates between policy improvement and (partial) policy
evaluation steps. For m ≥ 1, the m-step MPI algo-
rithm is deﬁned as follows

(cid:40)

πt+1 ∈ G(Vt)
Vt+1 = (T πt+1)mVt,

(3)

Manuscript under review by AISTATS 2020

where m = 1 corresponds to Value Iteration and m =
∞ corresponds to Policy Iteration. Here Vt denotes an
approximation of V πt.

Negative entropy regularizer. A practically im-
portant instance of the reg-MPI scheme corresponds
to the negative entropy regularizer

Policy

[13]. AMPI

Iteration
Approximate Modiﬁed
(AMPI)
is an approximate coun-
terpart of (3) that can be seen as a generalization of
MPI that allows errors in the policy improvement ((cid:15)(cid:48)
t)
and policy evaluation ((cid:15)t) steps

ΩEnt(π(·|s)) =

(cid:88)

a

π(a|s) log π(a|s).

(9)

Further, we will consider this regularization with a
time-varying regularization parameter λt > 0

(cid:40)

πt+1 ∈ G(cid:15)(cid:48)
(Vt)
Vt+1 = (T πt+1 Vt)m + (cid:15)t+1,

t+1

(4)

Ωt(π(·|s)) = λtΩEnt(π(·|s)).

Its convex conjugate is the smoothed maximum

t ∈ RS are respectively the evaluation
where (cid:15)t, (cid:15)(cid:48)
step and the policy improvement step error vectors
(one component per state) and π ∈ G(cid:15)(cid:48)(V ) ⇐⇒
∀π(cid:48) T π(cid:48)
V ≤ T πV + (cid:15)(cid:48). AMPI naturally arises from
MPI in practical settings with large state and / or ac-
tion spaces.

Convex conjugate functions. Following [6], we in-
troduce the regularized RL framework through convex
conjugate functions, see e.g. Section 3.3.1 in [15]. For
a strongly convex function Ω : ∆A → R its convex
conjugate Ω(cid:63) : RA → R is given by

Ω(cid:63)(q) = max
πs∈∆A

(cid:104)πs, q(cid:105) − Ω(πs), ∀q ∈ RA,

(5)

where (cid:104)πs, q(cid:105) := Ea∼πs [q(a)]. Ω(π) will be used as
a shorthand for the vector (Ω(πs))s∈S . Further, we
make use of the weighted regularizer Ωα(π) := αΩ(π)
that, by properties of the convex conjugate, results in
α(q) := αΩ∗(q/α). Another property of the convex
Ω∗
conjugate (Danskin’s Theorem) is that the maximizer
of (5) is given by the gradient of the dual function

∇Ω(cid:63)(q) = arg max
πs∈∆A

(cid:104)πs, q(cid:105) − Ω(πs).

(6)

Regularized Modiﬁed Policy Iteration (reg-
MPI) [6]. Similarly to standard Bellman opera-
tors (1), we deﬁne the regularized Bellman operator [6]

Ω V := T πV − Ω(π),
T π
and, by (5), the optimal regularized Bellman operator

(7)

T ∗
Ω V := max
π∈∆S
A

Ω V = (Ω∗(QV (s, ·)))s∈S .
T π

By virtue of (6) the corresponding optimal policy
GΩ(V ) ∈ ∆S

A is given by
Ω V = (∇Ω(cid:63)(QV (s, ·))s∈S .
T π

GΩ(V ) := arg max

π∈∆S
A

Reg-MPI is a MPI-type scheme that underlies several
state-of-the-art RL algorithms [2, 4, 8]

Ω(cid:63)

t (QV (s, ·)) = λt log

(cid:88)

a

exp(QV (s, a)/λt)

(10)

and the maximizing policy is given by the Boltzmann
policy πt+1(·|s) = ∇Ω(cid:63)
t (QVt(s, ·)) that takes the form
of

πt+1(a|s) =

exp(QVt(s, a)/λt)
a(cid:48) exp(QVt(s, a(cid:48))/λt)

(cid:80)

,

(11)

where λt > 0 is referred to as temperature parameter.

With entropic regularization and m = 1, the reg-MPI
scheme (8) describes the Soft Value Iteration [2] (soft
VI)

Vt+1 ← λt log

(cid:88)

a

exp(QVt(s, a)/λt).

(12)

The asynchronous counterpart of the soft VI is a core
principle of the soft Q-learning algorithm [3].

3 Error analysis and convergence
rates of AMPI algorithms

We now present the main contributions of this work,
namely a ﬁne-grained error analysis of the AMPI-type
algorithms, including suﬃcient conditions for conver-
gence with explicit rates.

3.1 General AMPI algorithms

The error propagation analysis links the error sequence
that occurred at previous iterations to the distance to
optimality of the current value iterate. In the following
Lemma, we restate the error propagation bounds of
AMPI established in [13, Theorem 7] for p = ∞.

Lemma 1 (AMPI error propagation [13]). For any
initial value function V0 and m ≥ 1, consider the
AMPI scheme (4). Then, one has

(cid:107)VN − V ∗(cid:107)∞ ≤

2
1 − γ

(cid:0)EN + γN (cid:107)V0 − V ∗(cid:107)∞

(cid:1) , (13)

(cid:40)

πt+1 ← GΩt(Vt)
Vt+1 ← (T πt+1

Ωt

)mVt.

(8)

where EN := (cid:80)N −1

t=1 γN −t((cid:107)(cid:15)t(cid:107)∞ + (cid:107)(cid:15)(cid:48)

t(cid:107)∞).

Manuscript under review by AISTATS 2020

Thus, convergence of the AMPI algorithm entirely de-
pends on controlling the cumulative error term EN .
This error term has a special structure in that the er-
rors at later iterations have more contribution to the
ﬁnal loss. In the next theorem, we show the general
convergence of AMPI if the sequence of sums of evalua-
tion step and improvement step errors (cid:107)(cid:15)N (cid:107)∞ +(cid:107)(cid:15)(cid:48)
N (cid:107)∞
converge to zero. By analysing the decrease rate of
the error sequence, we provide explicit rates of conver-
gence of the AMPI value iterates to the optimal value
function.

Theorem 1 (AMPI convergence). Suppose the error
sequences ((cid:107)(cid:15)N (cid:107)∞)N and ((cid:107)(cid:15)(cid:48)
N (cid:107)∞)N satisfy (cid:107)(cid:15)N (cid:107)∞ +
(cid:107)(cid:15)(cid:48)
N (cid:107)∞ ≤ CrN for some constant C > 0 and a
sequence rN −→ 0. Then, the AMPI scheme (4)
converges to the optimal greedy policy of the exact
MPI (3).

Furthermore, the limits ρ := lim rN /rN −1 and ρ :=
lim rN /rN −1. We have the following bounds

(A) Slow convergence. If ρ > γ, then

(cid:107)VN − V ∗(cid:107)∞ = O(rN ).

(B) (Almost) linear convergence. If ρ ≤ γ, then

(cid:107)VN − V ∗(cid:107)∞ =

(cid:40)

O(γN ),
O(N γN ),

if ρ < γ,
if ρ = γ.

√

We note that the conditions of the Theorem are not
restrictive. For example, the maximum error can de-
crease as slow as inverse logarithmically in the num-
ber of iterations and still eventually yield an optimal
policy at the rate given by Theorem 1(A). A similar
property holds for estimates of the form rN ∝ 1/N ;
rN ∝ 1/
N ; rN ∝ 1/ log N ; rN ∝ log N/N ; etc.
where ρ = ρ = 1 > γ. On the other hand, if the
error sequences decrease at at rate which is (asymptot-
ically) less than the discount factor γ, then the AMPI
converges at the same linear rate as the exact MPI!
Moreover, the conditions of Theorem 1 allow ﬁnitely
many deviations of ratios as soon as their inferior limit
is bounded away from γ. Fig. 4 in Appendix 6 illus-
trates these bounds.

3.2 Regularized MPI algorithms

We ﬁrst show that the reg-MPI (8) is an instance of
the AMPI (4). Then, using Theorem 1, we bound the
distance between the value iterates of the reg-MPI and
the optimal solution of the exact MPI (3). Without
loss of generality, we will consider the reg-MPI scheme
with weighted regularizer.

Theorem 2 (Reg-MPI convergence). Consider the
reg-MPI algorithm (8) with time-varying regulariza-
tion functions Ωt, and let the sequence (λt)t which
uniformly bounds Ωt, that is

sup
π

(cid:107)Ωt(π)(cid:107)∞ := sup
π, s

|Ωt(π(·|s))| ≤ λt.

(14)

Then it holds that

(cid:107)VN,Ω − V ∗(cid:107)∞ ≤

2
1 − γ

(cid:0)ΛN + γN (cid:107)V0,Ω − V ∗(cid:107)∞

(cid:1) ,

(15)
where ΛN := (1 + 1−γm
1−γ ) (cid:80)N −1
t=1 γN −tλt. Moreover, if
λt −→ 0, then the algorithm converges to the optimal
value function V ∗.

Furthermore, deﬁne the limits ρ := lim λN /λN −1 and
ρ := lim λN /λN −1. We have the following bounds

(A) Slow convergence. If ρ > γ, then the algorithm
converges to the optimal value function V ∗ with
the same rate as the step-sizes:

(cid:107)VN,Ω − V ∗(cid:107)∞ = O(λN ).

(B) (Almost) linear convergence. If ρ ≤ γ, then
the algorithm converges to the optimal value func-
tion V ∗ at same rate as the exact MPI (3) (i.e
linear rate of convergence). More precisely,

(cid:107)VN,Ω − V ∗(cid:107)∞ =

(cid:40)

O(γN ),
O(N γN ),

if ρ < γ,
if ρ = γ.

Remark. It should be noted that the reg-MPI (8) al-
gorithm above is an instance of the AMPI (4), with
:= Vt,Ω − (T πt)mVt,
policy evaluation step error (cid:15)t
and policy improvement step error given by (cid:15)t
:=
maxπ T πVt − maxπ T π
Vt.
Ωt

We note that prior works established the asymptotic
convergence of the soft VI and the soft Q-learning us-
ing contraction argument [2] and stochastic approxi-
mation tools [3]. We use a diﬀerent type of analysis
that provides ﬁnite-time error bounds for a large class
of regularized algorithms.

Special case: same base regularizer. The con-
dition (14) is satisﬁed by time-varying regularizers of
the form Ωt = τtΩ, for some uniformly bounded Ω.
These include the negative entropy regularizer as a
special case since supπ (cid:107)ΩEnt(π)(cid:107)∞ ≤ log |A|. Thus,
by controlling the temperature parameter of entropy-
regularized algorithm, we can control the error of the
value function returned by the algorithm w.r.t.
the
optimal value function.

Manuscript under review by AISTATS 2020

Relation to exploration. Prior works suggest that
the temperature parameter τt controls the amount of
exploration performed by the policy [2] and the error-
tolerance of the algorithm [7]. From (15) it is appar-
ent that this desired behaviour is achieved in exchange
for slower convergence if the decrease rate is greater
than the discount factor γ. In contrast, if the decrease
rate is fast enough (smaller than the discount factor
asymptotically), then the above-mentioned properties
are acquired with no impact on the convergence rate
that remains as fast as the exact MPI! To the best of
our knowledge, this is the ﬁrst result to relate non-
asymptotic performance of the MPI to exploration.

One limitation of Theorem 2 is that it does not pro-
vide a speciﬁc weight schedule to a problem at hand.
E.g., the amount of exploration needed depends on
the MDP structure. Too fast temperature decay im-
plies no regularization and leads to insuﬃcient explo-
ration. In contrast, too slow temperature decay results
in too strong regularization and unnecessary slow con-
vergence. This trade-oﬀ has also been shown empiri-
cally on a class of entropy regularized algorithms in [5].

3.3 Conservative Value Iteration (CVI)

We study the convergence rate of the conservative
VI [7] (CVI) scheme. CVI generalizes a number of
value-based algorithms such as dynamic policy pro-
gramming [16] and advantage learning [9, 10]. The
CVI algorithm is similar to the soft VI (12) in a sense
that it includes the entropic regularization. Diﬀer-
ently, it produces Q-value iterates with an increased
gap, i.e. ampliﬁed diﬀerence of Q-values between the
maximizing action and all other actions.

Similar to the optimal (regularized) Bellman operators
deﬁned in Section 2 for value function, we deﬁne the
optimal (regularized) Bellman operator for Q-values
as follows

[T ∗Q] (s, a) := r(s, a) + γP (·|s, a) max
a∈A

Q(s, a)

[T ∗

Ω Q] (s, a) := r(s, a) + γP (·|s, a)Ω∗(Q(s, ·)),

where Ω∗(Q(s, ·)) is given by the smoothed maxi-
mum (10).

Using this notation, the CVI scheme as deﬁned by [7,
Eq.(13)] can be presented in terms of entropic regular-
izer (9), where we additionally vary the regularization
weight Ωt := βtΩEnt

Qt+1(s, a) ← [T ∗
Ωt

Qt](s, a)+α (Qt(s, a) − Ω∗

t (Qt(s, ·))) ,
(16)
where (βt)t > 0 is sequence of weights and α ∈ [0, 1].
The second term in (16) penalizes Q-values of all sub-
optimal actions to produce a larger diﬀerence with the

optimal action by a factor of α. The resulting policy
πt is given by the Boltzmann policy

πt+1(·|s) ← GΩt(Qt(s, ·)).

Our analysis of the CVI is based on the analysis of the
approximate advantage learning (AL) algorithm

Qt+1(s, a) ← [T ∗Qt] (s, a)

(cid:18)

(cid:19)

+ α

Qt(s, a) − max
a∈A

Qt(s, a)

+ (cid:15)t(s, a),

(17)

where (cid:15)t ∈ RS×A is an error at iteration t. If α = 0, the
approximate AL algorithm coincides with the approx-
imate VI (Eq. (4) for m = 1). The following Lemma
restates the upper bound on the error of approximate
AL established in [7, Theorem 1,β = ∞].

Lemma 2 (AL error propagation [7]). Consider the
approximate AL scheme (17), and let ∆QN := QπN −
Q∗ be the Q-value regret after N iterations. Then, one
has

(cid:107)∆QN (cid:107)∞ ≤ 2γVmaxΓN
(cid:80)t
N
(cid:88)

γN −t

+

2γ
1 − γ

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t=1

k=0 αt−k(cid:15)k
AN

(18)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

,

where AN := (cid:80)N

k=0 αk and ΓN := 1
AN

(cid:80)N

t=0 γN −tαt.

If α = 0, the AL upper bound (18) is γ times the upper
bound of the approximate MPI (13), as expected since
the bound (13) is given in terms of value function (see
Lemma 4). If α > 0, we obtain a diﬀerent cumulative
error term that can be seen as a convolution of powers
of α with errors. The error-free term in (18) is of order
O(N max(α, γ)N ) if α < 1, and O(1/N ) if α = 1,
compared to O(γN ) in the approximate MPI (13).

Next, similar to Section 3.2, we proceed by showing the
reduction of the CVI (16) to the approximate AL (17).
Then, we establish the convergence of the CVI through
the appropriate error control in the approximate AL.

Theorem 3 (CVI convergence). Consider the CVI
algorithm (16) with time-varying regularization func-
tions Ωt, and let the sequence (λt)t which uniformly
bounds Ωt, that is

sup
π

(cid:107)Ωt(π)(cid:107)∞ := sup
π, s

|Ωt(π(·|s))| ≤ λt

Let ∆QN := QπN − Q∗ be the Q-value regret after N
iterations. Then it holds that

(cid:107)∆QN (cid:107)∞ ≤ 2γVmaxΓN +

2γ
1 − γ

ΛN

(19)

where ΛN := 1
AN
1
AN

t=1 γN −t (cid:16)(cid:80)t
(cid:80)N −1
t=0 γN −tαt, and AN := (cid:80)N

(cid:80)N

k=0 αt−kλt
, ΓN :=
k=0 αk. Moreover, if

(cid:17)

Manuscript under review by AISTATS 2020

λt −→ 0, then the algorithm converges to the optimal
solution Q∗.
Furthermore, deﬁne the quantities ¯λN := 1
N

t=0 λt,

(cid:80)N

(cid:40)

(ρ, ρ) :=

(lim λN /λN −1, lim λN /λN −1),
(lim ¯λN /¯λN −1, lim ¯λN /¯λN −1),

if α (cid:54)= 1,
if α = 1.

Denote α ∨ γ := max(α, γ). Then we have the bounds

(A) Slow convergence. If α = 1 or ρ > α ∨ γ, then

(cid:107)∆QN (cid:107)∞ =






O(λN ),
O(¯λN ∨ 1
O( 1
N ),

N ),

if α (cid:54)= 1, ρ > α ∨ γ,
if α = 1, ρ > γ,
if α = 1, ρ ≤ γ.

(B) (Almost) linear convergence. If 0 ≤ α < 1

and ρ ≤ γ, then

(cid:107)∆QN (cid:107)∞ = O(N (α ∨ γ)N ).

Remark. It should be noted that the CVI (16) algo-
rithm above is an instance of the approximate AL (17)
with error given by (cid:15)t := maxπ T πQt − maxπ T π
Ωt

Qt.

Theorem 3 shows that by controlling the sum of
weighted regularization terms, we control the distance
to optimality. As expected, if α = 0, we get the same
rates as for the reg-MPI (Theorem 2). If α ∈ [0, 1) and
the temperature decays quickly, the CVI convergence
at almost linear rate (bounds (B)). Diﬀerently from
the reg-MPI convergence, the fast geometric decay of
temperature can be slowed down by α = 1, where
the convergence becomes inverse linear in N (bounds
(A)). The value of α = 1 can be beneﬁcial with con-
stant temperature since the CVI would still converge
inverse linearly in N , whereas the reg-MPI bound (15)
turns to a constant.

4 Experimental results

We provide an empirical evidence on our theoretical re-
sults presented in Section 3. We experiment with the
soft VI, that is an instance of the reg-MPI scheme anal-
ysed in Section 3.2, and the conservative VI analysed
in Section 3.3. In experiments we use a cliﬀ walking
domain described below.

4.1 Cliﬀ walking domain

We use a cliﬀ walking domain [17] based on a 6x4
grid, also utilised for the analysis of the soft Q-learning
algorithm [3]. At each step the agent can move one
cell in 4 directions (up, down, left and right). The
target cell is located in the lower right corner and it is
a terminal state with zero reward. At all other cells,

agent receives a reward of -1 except the bottom row
that represents a cliﬀ where the agent is given a reward
of -100. Thus, the goal of the agent is to reach the
target cell as quickly as possible and avoid the cliﬀ.
For all experiments, we set the discount factor γ = 0.9.

4.2 Soft Value Iteration

Soft VI (12) is an instance of the reg-MPI scheme with
negative entropy regularizer and m = 1. We experi-
mentally analyse the convergence of the soft VI (12)
with varying temperature schedules. First, we demon-
strate the convergence rate and compare it with the
established bounds (Theorem 2). Next, we analyse
properties of the policy iterates. Finally, we experi-
ment with diﬀerent levels of stochasticity of the envi-
ronment. We emphasize that we carry on our study
in the simplest exact setting. As we shall see, the
behaviour of the regularized algorithms is already in-
teresting even in the absence of function approxima-
tion [7] or asynchronous updates [3].

Figure 1: Comparison of upper bounds on the error of
the reg-MPI for diﬀerent convergence regimes (The-
orem 2) and the empirical error of the soft VI over
iterations N on a cliﬀ walking domain. We see that
the proposed bounds are really tight.

Convergence. We experiment with the tempera-
ture schedules corresponding to diﬀerent regimes of
convergence given by the Theorem 2. We plug the
temperature schedules λN as a function of the number
of iterations N of the soft VI. We also compare to a
frequently used ﬁxed λN = λ = 0.01 and to the exact
VI that corresponds to λN = λ = 0.

First, we plot the maximum empirical error between
the current value iterate of the soft VI and the op-
timal value function with varying temperature sched-
ules, see Figure 2. As expected, the exact VI has the

Manuscript under review by AISTATS 2020

Figure 2: Empirical convergence of the exact VI (λN =
0) and the soft VI to optimality with varying temper-
ature schedules on a cliﬀ walking domain follows con-
vergence regimes given by Theorem 2.

fastest convergence. The soft VI with ﬁxed tempera-
ture value does not converge to the optimal value func-
tion resulting in irreducible error. The convergence of
the soft VI is evident for the fastest linear rates of the
temperature decay. The inverse linear schedule in N
is slower, however it surpasses the ﬁxed value in terms
of distance to optimality at around 102 iterations.

Next, we compare the theoretical bounds of the soft VI
(Theorem 2) with the empirical progression of errors.
As can be seen from Figure 1, our bounds provide good
description of the error in value function at ﬁnite time.

Safety of policy iterates. We demonstrated above
that the soft VI algorithm with temperature decay
converges to the optimal value function. In the follow-
ing, we show that (1) intermediate policies of the soft
VI with temperature annealing induce safe behaviour
and (2) the optimal policy is reached at convergence.
Figure 3 compares the progression of the soft VI and
the exact VI. As can be seen, the policy iterates of the
soft VI avoid the edge of the cliﬀ early in the learning,
but the algorithm eventually converges to the opti-
mal trajectory along the cliﬀ. The early iterates of
the exact VI directly act optimally following the edge
of the cliﬀ. In a diﬀerent asynchronous setting with
noise, similar observation has been made in [3, 6.2]
that in the cliﬀ walking domain the softmax policies
with positive temperature result in safe trajectories
far away from the cliﬀ. Our analysis shows that this
phenomenon is inherent to a fundamentally diﬀerent
behaviour of the soft VI algorithm.

Robustness to stochasticity. We analyse the
number of iterations necessary to achieve a certain

Figure 3: Evolution of value and policy iterates of the
exact VI and the soft VI with λN = γN on a cliﬀ
walking domain. Arrows and color scale indicate, re-
spectively, the best valued action of the policy and the
value of a state at iteration N .

level of accuracy at diﬀerent temperature schedules.
We study the cliﬀ walking domain from above with
added stochastic wind. It consists in replacing the tar-
get position by a horizontal or vertical slide of one cell
with probability p. Table 1 shows that on the stochas-
tic cliﬀ walking domain the number of iterations of
the soft VI is less than the number of iterations re-
quired for the deterministic environment. Moreover,
the number of iterations decreases with the amount of
added stochasticity. The inverse relation is observed
for the exact VI. Thus, we conclude that the soft VI in
stochastic environments is beneﬁcial in terms on con-
vergence, resulting, for fast rates of temperature decay,
in a comparable number of iterations.

Number of iterations
Stochastic

λN
0
(γ/2)N
γN
1/N
√
N
1/

Determ.
p = 0.0
18
29
166
15,691
247,394

p = 0.15
24
27
55
136
6379

p = 0.3
31
32
54
93
3440

Table 1: Number of iterations of the exact VI (λN = 0)
and the soft VI necessary to achieve accuracy (cid:15) = 10−8
on deterministic and stochastic cliﬀ walking domains.

4.3 Conservative Value Iteration

We analyse the empirical convergence of the CVI al-
gorithm with varying temperature decay rates ρ and
values of the gap-increasing factor α, presented in Ta-
ble 2. The color of the cells corresponds to diﬀer-
ent convergence regimes given by Theorem 3. As can

Manuscript under review by AISTATS 2020

be seen from Table 2, the temperature decay with
ρ ≤ γ has similar convergence rate across values of
α < γ. This matches the almost linear rate O(N γN )
predicted by Theorem 3 (B). We observe that fast tem-
perature decay ρ ≤ γ with large value of α = 0.95
slows down the convergence, as expected, to O(N αN ).
Slow inverse polynomial temperature schedule implies
ρ > max(α, γ) and hence, the convergence is of the
same order of magnitude for all values of α, predicted
as O(1/N 2) by Theorem 3 (A). We conclude that The-
orem 3 provides a good description of empirical con-
vergence regimes of the CVI algorithm. We observe
that, in line with our theoretical result, if α = 1. the
CVI does not converge in a reasonable amount of time.

λN
0.45N
0.8N
γN
1/N 2

ρ
0.45
0.8
γ
1

Number of iterations
α = 0.0 α = 0.6 α = 0.95
156
168
197
1058

167
179
208
1367

399
399
399
907

Table 2: Number of iterations of the conservative VI
necessary to achieve accuracy of (cid:15) = 10−8 with tem-
perature schedule λN , its rate ρ and the gap-increasing
factor α. Colors signify the convergence regimes pre-
dicted by the Theorem 3: (A) almost linear O(N γN )
(red) and O(N αN ) (blue), (B) slow O(1/N 2) (green).

5 Related works

We ﬁrst discuss closely related work [11, 18].
[11]
proves convergence to optimality of the SARSA algo-
rithm with GLIE policies (”greedy in the limit with
inﬁnite exploration”) that include a class of Boltz-
mann policies with decaying temperature. Another
close work [18] studies convergence to optimality of a
value iteration algorithm with a dynamic Boltzmann
operator that represents an instance of the reg-MPI
scheme with m = 1, negative entropy regularizer and
decreasing temperature schedule. Our work is diﬀer-
ent from the above-cited work since (1) we consider
MPI-based algorithms, (2) our result on convergence
rate holds over a class of approximate and regularized
MPI algorithms, and (3) we link the desirable prop-
erties of the regularized MPI such as targeted explo-
ration to its convergence rate through the schedule of
regularization parameter.

A theoretical justiﬁcation of empirical success of the
regularized value-based RL algorithms has been pro-
posed in [7]. The authors showed that (1) value-based
algorithms using softmax operator and ﬁxed tempera-
ture parameter are tolerant to any type of errors, e.g.
arising from function approximation or due to ﬁnite

sample of observations used to perform the updates;
(2) the temperature parameter controls the trade-oﬀ
between the asymptotic performance and the sensitiv-
ity to errors. Compared to this analysis, our work is
complementary in that we provide suﬃcient conditions
of convergence of a family of reg-MPI and conservative
VI algorithms.

The optimization perspective on the regularized MDP
framework proposed by [5, 6] allows the learning rate
interpretation of the regularization weight. In [6, D.1]
the regret of the weighted regularized MPI scheme is
analysed when it is subject to approximations. Our
work is diﬀerent in that we consider the regularization
itself as errors in the approximate MPI scheme.

The temperature schedules obtained in Section 3.1
have similarities with the decrease factors of the Boltz-
mann exploration in the multi-arm bandit setting,
e.g. O(1/N ) and O(log N/N ) are frequently used [19,
2.2]. Recently, it was shown that temperature sched-
N ) induce near-optimal perfor-
ules of the form O(1/
mance [20]. Despite these similarities, exploration in
the RL setup is not as well understood as in bandits
setting; our work contributes by providing a link to
the convergence rate.

√

6 Conclusion

Following the success of entropy-regularized methods
in RL, we study the convergence to optimality of
a class of dynamic programming algorithms uniﬁed
under the regularized MPI and the conservative VI
schemes. By the means of reduction to the approxi-
mate counterparts, we showed the general convergence
of these schemes to the optimal solution of the original
RL problem with decreasing schedule of the regulariza-
tion parameter over iterations. Moreover, our analysis
showed that the convergence of the regularized MPI is
as fast as the exact MPI, if the decrease rate of the
regularization weight is suﬃciently fast; otherwise the
algorithm’s convergence slows down to the same rate
as the decay rate of the regularization parameter.

We experimentally demonstrate that the empirical
convergence closely follows our theoretical results. We
showcase a diﬀerent behaviour of the regularized algo-
rithms even in the absence of approximations, namely,
robustness to stochasticity of the environment and
safety of trajectories induced by the policy iterates.

Manuscript under review by AISTATS 2020

[11] Satinder Singh, Tommi Jaakkola, Michael L
Littman, and Csaba Szepesv´ari. Convergence
results for single-step on-policy reinforcement-
learning algorithms. Machine learning, 38(3):287–
308, 2000.

[12] Richard S Sutton and Andrew G Barto. Rein-
forcement learning: An introduction. MIT press
Cambridge, 1998.

[13] Bruno Scherrer, Mohammad Ghavamzadeh, Vic-
tor Gabillon, Boris Lesner, and Matthieu Geist.
Approximate modiﬁed policy iteration and its ap-
plication to the game of tetris. Journal of Ma-
chine Learning Research, 16:1629–1676, 2015.

[14] Martin L Puterman. Markov decision processes.

Wiley, New York, 1994.

[15] Stephen Boyd and Lieven Vandenberghe. Convex

optimization. Cambridge university press, 2004.

[16] Mohammad Gheshlaghi Azar, Vicen¸c G´omez, and
Hilbert J Kappen. Dynamic policy program-
ming.
Journal of Machine Learning Research,
13(Nov):3207–3245, 2012.

[17] Richard S Sutton, Andrew G Barto, et al. Intro-
duction to reinforcement learning, volume 2. MIT
press Cambridge, 1998.

[18] Ling Pan, Qingpeng Cai, Qi Meng, Longbo
Huang, and Tie-Yan Liu. Reinforcement learning
with dynamic boltzmann softmax updates. arXiv
preprint arXiv:1903.05926, 2019.

[19] Joannes Vermorel and Mehryar Mohri. Multi-
armed bandit algorithms and empirical evalua-
tion. In European conference on machine learn-
ing, pages 437–448. Springer, 2005.

[20] Nicol`o Cesa-Bianchi, Claudio Gentile, G´abor Lu-
gosi, and Gergely Neu. Boltzmann exploration
done right.
In Advances in Neural Information
Processing Systems, pages 6284–6293, 2017.

References

[1] Brian D Ziebart, Andrew L Maas, J Andrew Bag-
nell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. In Aaai, volume 8, pages
1433–1438. Chicago, IL, USA, 2008.

[2] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel,
and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the
34th International Conference on Machine Learn-
ing, pages 1352–1361, 2017.

[3] Roy Fox, Ari Pakman, and Naftali Tishby. Tam-
ing the noise in reinforcement learning via soft up-
dates. In Proceedings of the Thirty-Second Con-
ference on Uncertainty in Artiﬁcial Intelligence,
UAI’16, pages 202–211, 2016.

[4] Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu,
and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In
Advances in Neural Information Processing Sys-
tems, pages 2775–2785, 2017.

[5] Gergely Neu, Anders Jonsson,

and Vicen¸c
G´omez. A uniﬁed view of entropy-regularized
markov decision processes.
arXiv preprint
arXiv:1705.07798, 2017.

[6] Matthieu Geist, Bruno Scherrer, and Olivier
Pietquin. A theory of regularized Markov deci-
sion processes. In Proceedings of the 36th Inter-
national Conference on Machine Learning, pages
2160–2169, 2019.

[7] Tadashi Kozuno, Eiji Uchibe, and Kenji Doya.
Theoretical analysis of eﬃciency and robustness
of softmax and gap-increasing operators in rein-
In The 22nd International
forcement learning.
Conference on Artiﬁcial Intelligence and Statis-
tics, pages 2995–3003, 2019.

[8] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel,
and Sergey Levine.
Soft actor-critic: Oﬀ-
policy maximum entropy deep reinforcement
learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.

[9] Leemon C Baird. Reinforcement learning through

gradient descent. 1999.

[10] Marc G Bellemare, Georg Ostrovski, Arthur
Guez, Philip S Thomas, and R´emi Munos.
In-
creasing the action gap: New operators for rein-
forcement learning.
In Thirtieth AAAI Confer-
ence on Artiﬁcial Intelligence, 2016.

Manuscript under review by AISTATS 2020

A Proofs

Refer to the manuscript for the various notations and terminology.

The following Lemma will be used repeated in the rest of the proofs.

Figure 4: Illustration of the bounds established in Lemma 3, for diﬀerent regimes of the per-iteration error
bounds rt. In these illustrations, we plugged θ = 0.9. We see that our proposed upper bounds are quite tight.

Lemma 3. Let r1, r2, . . . , rt, . . . be a sequence of positive real numbers and θ ∈ [0, 1). Deﬁne ρ := lim inf rN /rN −1
and ρ := lim sup rN /rN −1, and consider the sums SN := (cid:80)N −1
t=0 θN −trt, for N ≥ 1. We have the following bounds

(A) If ρ ≥ θ, then

(B) If ρ ≤ θ, then

SN = O(rN ).

SN =

(cid:40)

O(θN ),
O(N θN ),

if ρ < θ,
if ρ = θ.

(20)

(21)

Proof. (A) Suppose ρ := lim rN /rN −1 > θ. For suﬃciently large t ≤ N , we have rN ≥ ρrN −1 ≥ . . . ≥ ρN −trt
and so rt ≤ rN ρ−(N −t). So, for large N , one computes

SN :=

N −1
(cid:88)

t=1

θN −trt (cid:46)

N −1
(cid:88)

t=1

θN −trN ρ−(N −t) = rN

N −1
(cid:88)

(θ/ρ)N −t = rN (θ/ρ)

t=1

1 − (θ/ρ)N
1 − θ/ρ

(cid:46) θ

ρ − θ

rN = O(rN ).

(B) Suppose ρ := lim rN /rN −1 < θ. Then for suﬃciently large t ≤ N , it holds that rt ≤ ρrt−1 ≤ . . . ≤ ρt−1r1.
Thus for suﬃciently large N , one has

SN :=

N −1
(cid:88)

t=1

θN −trt (cid:46)

N −1
(cid:88)

t=1

θN −tr1ρt−1 = r1θN −1

(cid:19)t

N −2
(cid:88)

t=0

(cid:18) ρ
θ

= Cr1θN −1 1 − (ρ/θ)N −1

1 − ρ/θ

(cid:46) θ

θ − ρ

r1θN −1 = O(θN ).

Finally, for ρ = θ, a similar arguments yield SN (cid:46) r1θN −1 (cid:80)N −2

t=0 1 = O(N θN ).

A.1 Convergence rates for AMPI (Appproximate Modiﬁed Policy Iteration)

N (cid:107)∞)N satisfy (cid:107)(cid:15)N (cid:107)∞ +
N (cid:107)∞ ≤ CrN for some constant C > 0 and a sequence rN −→ 0. Then, the AMPI scheme (4) converges to the

Theorem 1 (AMPI convergence). Suppose the error sequences ((cid:107)(cid:15)N (cid:107)∞)N and ((cid:107)(cid:15)(cid:48)
(cid:107)(cid:15)(cid:48)
optimal greedy policy of the exact MPI (3).

Furthermore, the limits ρ := lim rN /rN −1 and ρ := lim rN /rN −1. We have the following bounds

(A) Slow convergence. If ρ > γ, then

(cid:107)VN − V ∗(cid:107)∞ = O(rN ).

Manuscript under review by AISTATS 2020

(B) (Almost) linear convergence. If ρ ≤ γ, then

(cid:107)VN − V ∗(cid:107)∞ =

(cid:40)

O(γN ),
O(N γN ),

if ρ < γ,
if ρ = γ.

Proof. The proof is based on basic properties of convergent sequences and series.
General convergence. Since rt −→ 0, it follows that for any δ > 0, rt (cid:46) δ (where the symbol ”at (cid:46) bt” means
that at ≤ bt for suﬃciently large t ). Thus for suﬃciently large N , one has

EN :=

N −1
(cid:88)

t=1

γN −t((cid:107)(cid:15)t(cid:107)∞ + (cid:107)(cid:15)(cid:48)

t(cid:107)∞) ≤ C

N −1
(cid:88)

t=1

γN −trt (cid:46) Cδ

N −1
(cid:88)

t=1

γN −t ≤ C

γ
1 − γ

δ.

Thus EN −→ 0 in the limit N → ∞, and by virtue of the bound (13) of Lemma 1 the algorithm converges to
the optimal value function V ∗ as claimed.

Convergence with explicit rates. We now establish the explicit rates of convergence claimed in the theorem
under corresponding additional assumptions.

(A) Suppose ρ := lim rN /rN −1 > γ. For suﬃciently large t ≤ N , we have rN ≥ ρrN −1 ≥ . . . ≥ ρN −trt and so
rt ≤ rN ρ−(N −t). So, for large N , one computes

EN :=

N −1
(cid:88)

t=1

γN −t((cid:107)(cid:15)t(cid:107)∞ + (cid:107)(cid:15)(cid:48)

t(cid:107)∞) ≤ C

N −1
(cid:88)

t=1

γN −trt = O(rN ),

where the last equality an application of Lemma 3 (more precisely, an application of the bound (20) with
θ = γ < ρ).

(B) Suppose ρ := lim rN /rN −1 < γ. Then for suﬃciently large t ≤ N , it holds that rt ≤ ρrt−1 ≤ . . . ≤ ρt−1r1.
Thus for suﬃciently large N , one has EN ≤ C (cid:80)N −1
t=1 γN −trt = O(γN ), by applying Lemma 3 (more precisely,
by applying the bound (21) with θ = γ > ρ).

Finally, for ρ = γ, a similar arguments yield EN = O(N γN ), by applying Lemma 3 (more precisely, by applying
the bound (21) with θ = γ = ρ).

A.2 Convergence rates for reg-MPI (regularized Modiﬁed Policy Iteration)

Theorem 2 (Reg-MPI convergence). Consider the reg-MPI algorithm (8) with time-varying regularization func-
tions Ωt, and let the sequence (λt)t which uniformly bounds Ωt, that is

Then it holds that

sup
π

(cid:107)Ωt(π)(cid:107)∞ := sup
π, s

|Ωt(π(·|s))| ≤ λt.

(cid:107)VN,Ω − V ∗(cid:107)∞ ≤

2
1 − γ

(cid:0)ΛN + γN (cid:107)V0,Ω − V ∗(cid:107)∞

(cid:1) ,

(14)

(15)

where ΛN := (1 + 1−γm
function V ∗.

1−γ ) (cid:80)N −1

t=1 γN −tλt. Moreover, if λt −→ 0, then the algorithm converges to the optimal value

Furthermore, deﬁne the limits ρ := lim λN /λN −1 and ρ := lim λN /λN −1. We have the following bounds

(A) Slow convergence. If ρ > γ, then the algorithm converges to the optimal value function V ∗ with the same

rate as the step-sizes:

(cid:107)VN,Ω − V ∗(cid:107)∞ = O(λN ).

(B) (Almost) linear convergence. If ρ ≤ γ, then the algorithm converges to the optimal value function V ∗

at same rate as the exact MPI (3) (i.e linear rate of convergence). More precisely,

(cid:107)VN,Ω − V ∗(cid:107)∞ =

(cid:40)

O(γN ),
O(N γN ),

if ρ < γ,
if ρ = γ.

Manuscript under review by AISTATS 2020

Proof. We proceed by bounding the policy evaluation and policy improvement step errors of the reg-MPI (8)
with respect to the exact MPI (3). We note that reg-MPI (8) is an instance of AMPI (4), with policy evaluation
step error (cid:15)t := Vt,Ω − (T πt)mVt, and policy improvement step errors given by (cid:15)t := maxπ T πVt − maxπ T π
Ωt

Vt.

Step 1: bound evaluation-step error (cid:107)(cid:15)t(cid:107)∞. To begin, it is easy to prove by induction on m (see Ap-
pendix A) that for every policy π ∈ ∆S

A and value function V ∈ RS and one has the formula

(T π

Ω )mV = (T π)mV −

m−1
(cid:88)

j=0

γj(P π)jΩ(π),

where (P π)j is the jth power of the matrix P π. Thus one has

(cid:107)(cid:15)t(cid:107)∞ = (cid:107)Vt,Ω − (T πt)mVt(cid:107)∞ = (cid:107)(T πt
Ωt

)mVt − (T πt)mVt(cid:107)∞ =

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

m−1
(cid:88)

j=0

≤

m−1
(cid:88)

j=0

γj(cid:107)(P πt)jΩt(πt)(cid:107)∞ ≤

m−1
(cid:88)

j=0

γj(cid:107)Ωt(πt)(cid:107)∞ =

1 − γm
1 − γ

where the last inequality follows from the Cauchy-Schwartz inequality

(cid:13)
(cid:13)
(cid:13)
γj(P πt)jΩt(πt)
(cid:13)
(cid:13)
(cid:13)∞
1 − γm
1 − γ

(cid:107)Ωt(πt)(cid:107)∞ ≤

αt,

(22)

(23)

(cid:107)(P π)jΩt(πt)(cid:107)∞ = max

s

|(P π)j(·|s)Ωt(πt)| ≤ max

s

(cid:107)(P πt)j(·|s)(cid:107)1(cid:107)Ωt(πt)(cid:107)∞ = (cid:107)Ωt(πt)(cid:107)∞,

since (cid:107)(P πt)j(·|s)(cid:107)1 = 1 because (P πt)j(·|s) is a probability distribution (over next states).

Step 2: bound policy improvement step error (cid:107)(cid:15)(cid:48)
and deﬁnition of the regularized operator T π

Ω , one has

t(cid:107)∞. Using elementary properties of the max operator

(cid:107)(cid:15)(cid:48)

t(cid:107)∞ = (cid:107) max

π

T πVt − max

π

T π
Ωt

Vt(cid:107)∞ ≤ max

π

(cid:107)T πVt − T π
Ωt

Vt(cid:107)∞ = max

π

(cid:107)Ωt(π)(cid:107)∞ ≤ αt.

(24)

By combining per-iteration error bounds (23) and (24) and using Lemma 1, one obtains (15). From this bound
and Theorem 1 invoked with rt := αt and C = 1 + 1−γm
1−γ , we get that the algorithm reg-MPI (8) converges to
the optimal value function V ∗, with the claimed rates of convergence.

Proof of formula (22). Let π be a policy and V be a value function. By (7), one has

Ω V = T πV − Ω(π) = T πV − γ0(P π)0Ω(π),
T π

and so the formula is valid for m = 1 step. Now suppose the formula (22) is valid for m steps. Then

(T π

Ω )m+1V = T π
Ω ((T π

(T π)mV −

Ω )mV ) = T π((T π
m−1
(cid:88)

= rπ + γP π

γj(P π)jΩ(π)

Ω )mV ) − Ω(π) = rπ + γP π(T π

Ω )mV − Ω(π)


 − Ω(π) = rπ + γP π(T π)mV − γP π

= T π((T π)mV ) −

m
(cid:88)

j=0

j=0

γj(P π)jΩ(π) = (T π)m+1V −

m
(cid:88)

j=0

γj(P π)jΩ(π),

which is the formula (22) for m + 1 steps.

A.3 Convergence rates for CVI (Conservative Value Iteration)

m−1
(cid:88)

j=0

γj(P π)jΩ(π) − Ω(π)

Theorem 3 (CVI convergence). Consider the CVI algorithm (16) with time-varying regularization functions
Ωt, and let the sequence (λt)t which uniformly bounds Ωt, that is

sup
π

(cid:107)Ωt(π)(cid:107)∞ := sup
π, s

|Ωt(π(·|s))| ≤ λt

Manuscript under review by AISTATS 2020

Let ∆QN := QπN − Q∗ be the Q-value regret after N iterations. Then it holds that

(cid:107)∆QN (cid:107)∞ ≤ 2γVmaxΓN +

2γ
1 − γ

ΛN

(19)

(cid:80)N −1

t=1 γN −t (cid:16)(cid:80)t

where ΛN := 1
AN
λt −→ 0, then the algorithm converges to the optimal solution Q∗.
Furthermore, deﬁne the quantities ¯λN := 1
N

, ΓN := 1
AN

k=0 αt−kλt

t=0 λt,

(cid:80)N

(cid:17)

(cid:80)N

t=0 γN −tαt, and AN := (cid:80)N

k=0 αk. Moreover, if

(cid:40)

(ρ, ρ) :=

(lim λN /λN −1, lim λN /λN −1),
(lim ¯λN /¯λN −1, lim ¯λN /¯λN −1),

if α (cid:54)= 1,
if α = 1.

Denote α ∨ γ := max(α, γ). Then we have the bounds

(A) Slow convergence. If α = 1 or ρ > α ∨ γ, then

(cid:107)∆QN (cid:107)∞ =






O(λN ),
O(¯λN ∨ 1
O( 1
N ),

N ),

if α (cid:54)= 1, ρ > α ∨ γ,
if α = 1, ρ > γ,
if α = 1, ρ ≤ γ.

(B) (Almost) linear convergence. If 0 ≤ α < 1 and ρ ≤ γ, then

(cid:107)∆QN (cid:107)∞ = O(N (α ∨ γ)N ).

Proof. The ﬁrst term ΓN in the upper-bound in Lemma 2 is itself upper-bounded as follows

ΓN =

(cid:40)

O(N (α ∨ γ)N ),
O( 1

N ),

if 0 ≤ α < 1
if α = 1.

So it remains to control the second term ΛN in the bound.
We ﬁrst bound the errors (cid:15)t := maxπ T πVt − maxπ T π
Ωt
the proof of Theorem 2, one has

Vt in the approximate AL (17). Similar to the Step 2 in

(cid:107)(cid:15)t(cid:107)∞ = (cid:107) max

π

T πVt − max

π

T π
Ωt

Vt(cid:107)∞ ≤ max

π

(cid:107)Ωt(π)(cid:107)∞ ≤ λt.

So, by the triangular inequality, we have the bound

N −1
(cid:88)

t=1

γN −t

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t
(cid:88)

αt−k(cid:15)k

k=0

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)∞

≤

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

(cid:33)

αt−k(cid:107)(cid:15)k(cid:107)∞

=

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

(cid:33)

αt−kλt

=: ΛN .

The inner and outer sums are of the same type that we analysed in Lemma 3.

Case 1: 0 ≤ α < 1.

In this case AN = Ω(1). We will consider diﬀerent subcases. Viz,

Case 1.1: ρ > α. Under this assumption, we have (cid:80)t
by applying the bound (20) with θ = α < ρ). Thus,

k=0 αt−kλk = O(λt) by applying Lemma 3 (more precisely,

ΛN :=

1
AN

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

αt−kλt

(cid:33)

(cid:46) 1
AN

N −1
(cid:88)

t=0

γN −tλt =

(cid:40)

O(λN ),
(N γN ),

if ρ > γ,
if ρ ≤ γ,

where the last equality is via another application of Lemma 3. Thus,

(cid:40)

(cid:107)∆QN (cid:107)∞ =

O(λN ∨ (N (α ∨ γ)N )),
O(N (α ∨ γ)N ),

if ρ > γ,
if ρ ≤ γ.

Manuscript under review by AISTATS 2020

Case 1.2: ρ < α. Under this assumption, we have (cid:80)t
precisely, by applying the bound (21) with θ = α > ρ). Thus,

k=0 αt−kλk = O(αt) by applying Lemma 3 (more

ΛN :=

1
AN

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

(cid:33)

αt−kλt

(cid:46)

N −1
(cid:88)

t=0

γN −tαt =




O(αN ),
O(γN ),
(N γN ),
= O(N (α ∨ γ)N ),



if α > γ,
if α < γ,
if α = γ.

where the last equality is via another application of Lemma 3. Thus (cid:107)∆QN (cid:107)∞ = O(N (α ∨ γ)N )).

Case 1.3: ρ = α. Under this assumption, we have (cid:80)t
precisely, by applying the bound (21) with θ = α = ρ). Thus,

k=0 αt−kλk = O(tαt) by applying Lemma 3 (more

ΛN :=

1
AN

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

(cid:33)

αt−kλt

(cid:46)

N −1
(cid:88)

t=0

γN −ttαt =




O(N αN ),
O(γN ),
(N γN ),
= O(N (α ∨ γ)N )



if α > γ,
if α < γ,
if α = γ

where the last equality is via another application of Lemma 3. Thus (cid:107)∆QN (cid:107)∞ = O(N (α ∨ γ)N )).

Case 2: α = 1. Under this assumption, we have (cid:80)t

k=0 αt−kλk = (cid:80)t

k=0 λk =: t¯λt and AN = N . Thus

ΛN :=

1
AN

N −1
(cid:88)

t=1

γN −t

(cid:32) t

(cid:88)

k=0

(cid:33)

αt−kλt

=

1
N

N −1
(cid:88)

t=0

γN −tt¯λt =

(cid:40)

O (cid:0)¯λN
(cid:1) ,
O(γN ),

if ρ > γ,
if ρ ≤ γ.

by applying Lemma 3 (more precisely, by applying the bound (21) with rt = t¯λt and θ = γ). Thus,

(cid:107)∆QN (cid:107)∞ =

(cid:40)

O((¯λN ∨ 1/N )),
O(1/N ),

if ρ > γ,
if ρ ≤ γ.

Applying Lemma 2 and grouping the various convergence rates of ΛN then yields the bounds on (cid:107)QπN − Q∗(cid:107)∞
claimed in the Theorem.

A.4 Relation between error in Q function and error in value function

Lemma 4. For Q functions Q1, Q2 ∈ RS×A with associated value functions V1, V2 ∈ RS , it holds that

Proof. For all s, a ∈ S × A

(cid:107)Q1 − Q2(cid:107)∞ ≤ γ(cid:107)V1 − V2(cid:107)∞.

|Q1(s, a) − Q2(s, a)| = |r(s, a) − γP (·|s, a)T V1 − r(s, a) − γP (·|s, a)T V2|

= γ|P (·|s, a)T (V1 − V2)| ≤ γ(cid:107)P (·|s, a)(cid:107)1(cid:107)V1 − V2(cid:107)∞ = γ(cid:107)V1 − V2(cid:107)∞,

where the inequality is due to application of Cauchy-Schwartz inequality and we also used the fact that
(cid:107)P (·|s, a)(cid:107)1 = 1 for all for all s, a ∈ S × A since P is a transition matrix.

