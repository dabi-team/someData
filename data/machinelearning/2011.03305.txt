Noname manuscript No.
(will be inserted by the editor)

A dynamic programming approach for generalized
nearly isotonic optimization

Zhensheng Yu · Xuyu Chen · Xudong Li

2
2
0
2

t
c
O
0
1

]

C
O
.
h
t
a
m

[

4
v
5
0
3
3
0
.
1
1
0
2
:
v
i
X
r
a

Received: date / Accepted: date

Abstract Shape restricted statistical estimation problems have been extensively
studied, with many important practical applications in signal processing, bioinfor-
matics, and machine learning. In this paper, we propose and study a generalized
nearly isotonic optimization (GNIO) model, which recovers, as special cases, many
classic problems in shape constrained statistical regression, such as isotonic regres-
sion, nearly isotonic regression and unimodal regression problems. We develop an
eﬃcient and easy-to-implement dynamic programming algorithm for solving the
proposed model whose recursion nature is carefully uncovered and exploited. For
special (cid:96)2-GNIO problems, implementation details and the optimal O(n) running
time analysis of our algorithm are discussed. Numerical experiments, including
the comparisons among our approach, the powerful commercial solver Gurobi,
and existing fast algorithms for solving (cid:96)1-GNIO and (cid:96)2-GNIO problems, on both
simulated and real data sets, are presented to demonstrate the high eﬃciency and
robustness of our proposed algorithm in solving large scale GNIO problems.

Keywords Dynamic programming · generalized nearly isotonic optimization ·
shape constrained statistical regression

Mathematics Subject Classiﬁcation (2020) 90C06 · 90C25 · 90C39

Zhensheng Yu
College of Science, University of Shanghai for Science and Technology, No. 516, Jungong Road,
Shanghai, China
E-mail: zhsh-yu@163.com

Xuyu Chen
School of Mathematics, Fudan University, No. 220, Handan Road, Shanghai, China
E-mail: chenxy18@fudan.edu.cn

Xudong Li
School of Data Science, Fudan University, No. 220, Handan Road, Shanghai, China
E-mail: lixudong@fudan.edu.cn

 
 
 
 
 
 
2

1 Introduction

Zhensheng Yu et al.

In this paper, we are interested in solving the following convex composite opti-
mization problem:

min
x∈(cid:60)n

n
(cid:88)

i=1

fi(xi) +

n−1
(cid:88)

i=1

λi(xi − xi+1)+ +

n−1
(cid:88)

i=1

µi(xi+1 − xi)+,

(1)

where fi : (cid:60) → (cid:60), i = 1, . . . , n, are convex loss functions, λi and µi are nonnegative
and possibly inﬁnite scalars, i.e.,

0 ≤ λi, µi ≤ +∞,

∀ i = 1, . . . , n,

and (x)+ = max(0, x) denotes the nonnegative part of x for any x ∈ (cid:60). Here, if
for some i ∈ {1, . . . , n − 1}, λi = +∞ (respectively, µi = +∞), the corresponding
regularization term λi(xi − xi+1)+ (respectively, µi(xi+1 − xi)+) in the objective
of (1) shall be understood as the indicator function δ(xi, xi+1 | xi − xi+1 ≤ 0)
(respectively, δ(xi, xi+1 | xi−xi+1 ≥ 0)), or equivalently the constraint xi−xi+1 ≤
0 (respectively, xi − xi+1 ≥ 0). To guarantee the existence of optimal solutions to
problem (1), throughout the paper, we make the following blanket assumption:

Assumption 1 Each fi : (cid:60) → (cid:60), i = 1, . . . , n, is a convex function and has
bounded level-sets, i.e., there exists α ∈ (cid:60) such that the α-level set {x ∈ (cid:60) | fi(x) ≤ α}
is non-empty and bounded.

Indeed, under this mild assumption, it is not diﬃcult to see that the objective
function in problem (1) is also a convex function and has bounded level-sets.
Then, by [27, Theorems 27.1 and 27.2], we know that problem (1) has a non-
empty optimal solution set.

Problem (1) is a generalization of the following nearly isotonic regression prob-

lem [34]:

min
x∈(cid:60)n

1
2

n
(cid:88)

i=1

(xi − yi)2 + λ

n−1
(cid:88)

i=1

(xi − xi+1)+,

(2)

where the nonnegative scalar λ ∈ (cid:60) is a given parameter, yi ∈ (cid:60), i = 1, . . . , n, are
n given data points. Clearly, (1) can be regarded as a generalization of model (2) in
the sense that more general convex loss functions fi and regularizers (constraints)
are considered; hence we refer (1) as the generalized nearly isotonic optimization
(GNIO) problem and the term (cid:80)n−1
i=1 µi(xi+1 − xi)+ in
the objective of (1) as the generalized nearly isotonic regularizer. We further note
that model (1) also subsumes the following fused lasso problem [15], or (cid:96)2 total
variation regularization problem:

i=1 λi(xi − xi+1)+ + (cid:80)n−1

min
x∈(cid:60)n

1
2

n
(cid:88)

i=1

(xi − yi)2 + λ

n−1
(cid:88)

i=1

|xi − xi+1|.

(3)

By allowing the parameters λi and/or µi in (1) to be positive inﬁnite, we see that
the generalized nearly isotonic optimization model (1) is closely related to the
shape constrained inference problems [30]. For instance, the important isotonic
regression [2, 4, 5, 6, 9, 1] and unimodal regression problems [31, 16] are special cases

DP for GNIO

3

of model (1). Indeed, given a positive weight vector w ∈ (cid:60)n and some 1 ≤ p < +∞,
in problem (1), if by taking

fi(xi) = wi|xi − yi|p,

i = 1, . . . , n,

and λi = +∞ and µi = 0 for i = 1, . . . , n − 1, we obtain the following (cid:96)p isotonic
regression problem:

n
(cid:88)

wi|xi − yi|p

min
x∈(cid:60)n

i=1
s.t. xi ≤ xi+1,

i = 1, · · · , n − 1.

(4)

In the above setting, if for some 1 ≤ m ≤ n − 2, by taking

λi = +∞, µi = 0,

i = 1, . . . , m,

and λi = 0, µi = +∞, i = m + 1, . . . , n − 1,

in model (1), we obtain the following unimodal regression problem:

n
(cid:88)

wi|xi − yi|p

min
x∈(cid:60)n

i=1
s.t. xi ≤ xi+1,
xi ≥ xi+1,

i = 1, · · · , m,

i = m + 1, · · · , n − 1.

(5)

These related problems indicate wide applicability of our model (1) in many ﬁelds
including operations research [1], signal processing [10, 26], medical prognosis [29],
and traﬃc and climate data analysis [24, 36]. Perhaps the closest model to our
generalized isotonic optimization problem is the Generalized Isotonic Median Re-
gression (GIMR) model studied in [18]. While we allow loss functions fi in problem
(1) to be general convex functions, the GIMR model in [18] assumes that each fi
is piecewise aﬃne. Besides, our model (1) deals with the positive inﬁnite param-
eters λi and/or µi more explicitly. We further note that in model (1) we do not
consider box constraints for each decision variable xi as in the GIMR, since many
important instances of model (1), e.g., problems (2), (3), (4) and (5), contain no
box constraints. Nevertheless, as one can observe later, our analysis can be gen-
eralized to the case where additional box constraints of xi are presented without
much diﬃculty.

Now we present a brief review on available algorithms for solving the aforemen-
tioned related models. For various shape constrained statistical regression prob-
lems including problems (4) and (5), a widely used and eﬃcient algorithm is the
pool adjacent violators algorithm (PAVA) [2, 6]. The PAVA was originally proposed
for solving the (cid:96)2 isotonic regression problem, i.e., problem (4) with p = 2. In [7],
Best and Chakravarti proved that the PAVA, when applied to the (cid:96)2 isotonic re-
gression problem, is in fact a dual feasible active set method. Later, the PAVA
was further extended to handle isotonic regression problems with general separa-
ble convex objective in [33, 8, 1]. Moreover, the PAVA was generalized to solve the
(cid:96)p unimodal regression [31] with emphasis on the (cid:96)1, (cid:96)2 and (cid:96)∞ cases. Recently,
Yu and Xing [37] proposed a generalized PAVA for solving separable convex min-
imization problems with rooted tree order constraints. Note that the PAVA can
also be generalized to handle the nearly isotonic regularizer. In [34], Tibshirani

4

Zhensheng Yu et al.

et al. developed an algorithm, which can be viewed as a modiﬁed version of the
PAVA, for computing the solution path of the nearly isotonic regression problem
(2). Closely related to the PAVA, a direct algorithm for solving (cid:96)2 total variation
regularization (3) was proposed in [12] by Condat, which appears to be one of
the fastest algorithms for solving (3). Despite the wide applicability of the PAVA,
it remains largely unknown whether the algorithm can be modiﬁed to solve the
general GNIO problem (1).

Viewed as a special case of the Markov Random Fields (MRF) problem [19],
the GIMR model [18] was eﬃciently solved by emphasizing the piecewise aﬃne
structures of the loss functions fi and carefully adopting the cut-derived thresh-
old theorem of Hochbaum’s MRF algorithm [19, Theorem 3.1]. However, it is not
clear whether the proposed algorithm in [18] can be extended to handle general
convex loss functions in the GNIO problem (1). Quite recently, as a generalization
of the GIMR model, Lu and Hochbaum [20] investigated the 1D generalized total
variation problem where in the presence of the box constraints over the decision
variables, general real-valued convex loss and regularization functions are consid-
ered. Moreover, based on the Karush-Kuhn-Tucker optimality conditions of the 1D
generalized total variation problem, an eﬃcient algorithm with a nice complexity
was proposed in [20]. However, special attentions are needed when the algorithm
is applied to handle problems consisting extended real-valued regularization func-
tions (e.g., indicator functions), as the subgradient of the regularizer at certain
intermediate points of the algorithm may be empty. Hence, it may not be able
to properly handle problem (1) when some parameters λi and/or µi are positive
inﬁnite.

On the other hand, based on the inherit recursion structures of the underlying
problems, eﬃcient dynamic programming (DP) approaches [28, 22] are designed
to solve the regularized problem (3) and the constrained problem (4) with p = 1.
Inspired by these successes, we ask the following question:

Can we design an eﬃcient DP based algorithm to handle more sophisticated
shape restricted statistical regression models than previously considered problems
(3) and (4) with p = 1?

In this paper, we provide an aﬃrmative answer to this question. In fact, our
careful examination of the algorithms in [28, 22] reveals great potential of the dy-
namic programming approach for handling general convex loss functions and var-
ious order restrictions as regularizers and/or constraints which are emphasized in
model (1). Particularly, we propose the ﬁrst eﬃcient and implementable dynamic
programming algorithm for solving the general model (1). By digging into the
recursion nature of (1), we start by reformulating problem (1) into an equivalent
form consisting a series of recursive optimization subproblems, which are suitable
for the design of a dynamic programming approach. Unfortunately, the involved
objective functions are also recursively deﬁned and their deﬁnitions require solv-
ing inﬁnitely many optimization problems1. Therefore, the naive extension of the
dynamic programming approaches in [28, 22] will not result a computationally
tractable algorithm for solving (1). Here, we overcome this diﬃculty by utilizing
the special properties of the generalized nearly isotonic regularizer in (1) to provide
explicit updating formulas for the objective functions, as well as optimal solutions,
of the involved subproblems. Moreover, the computations associated with each of

1 See (9) for more details.

DP for GNIO

5

the aforementioned formulas only involve solving at most two univariate convex
optimization problems. These formulas also lead to a semi-closed formula, in a
recursive fashion, of an optimal solution to (1). As an illustration, the implemen-
tation details and the optimal O(n) running time of our algorithm for solving
(cid:96)2-GNIO problems2 are discussed. We also conduct extensive numerical experi-
ments to demonstrate the robustness, as well as eﬀectiveness, of our algorithm for
handling diﬀerent generalized nearly isotonic optimization problems.

The remaining parts of this paper are organized as follows. In the next section,
we provide some discussions on the subdiﬀerential mappings of univariate convex
functions. The obtained results will be used later for designing and analyzing our
algorithm. In Section 3, a dynamic programming approach is proposed for solving
problem (1). The explicit updating rules for objectives and optimal solutions of the
involved optimization subproblems are also derived in this section. In Section 4, we
discuss some practical implementation issues and conduct running time analysis of
our algorithm for solving (cid:96)2-GNIO problems. Numerical experiments are presented
in Section 5. We conclude our paper in the ﬁnal section.

2 Preliminaries

In this section, we discuss some basic properties associated with the subdiﬀerential
mappings of univariate convex functions. These properties will be extensively used
in our algorithmic design and analysis.

Let f : (cid:60) → (cid:60) be any given function. We deﬁne the corresponding left deriva-

tive f (cid:48)

− and right derivative f (cid:48)

+ at a given point x in the following way:

f (cid:48)
−(x) = lim
z ↑ x

f (z) − f (x)
z − x

,

f (cid:48)
+(x) = lim
z ↓ x

f (z) − f (x)
z − x

,

if the limits exist. Of course, if f is actually diﬀerentiable at x, then f (cid:48)
−(x) =
+(x) = f (cid:48)(x). If f is assumed to be a convex function, one can say more about
f (cid:48)
f (cid:48)
+ and f (cid:48)
− and their relations with the subgradient mapping ∂f . We summarize
these properties in the following lemma which are mainly taken from [27, Theorems
23.1, 24.1]. Hence, the proofs are omitted here.

Lemma 1 Let f : (cid:60) → (cid:60) be a convex function. Then, f (cid:48)
ﬁnite-valued non-decreasing functions on (cid:60), such that

+ and f (cid:48)

− are well-deﬁned

+(z1) ≤ f (cid:48)
f (cid:48)

−(x) ≤ f (cid:48)

+(x) ≤ f (cid:48)

−(z2)

whenever

z1 < x < z2.

(6)

For every x, one has

f (cid:48)
−(x) = sup
z<x

f (z) − f (x)
z − x

,

f (cid:48)
+(x) = inf
z>x

f (cid:48)
−(x) = lim
z ↑ x

f (cid:48)
−(z) = lim
z ↑ x

f (cid:48)
+(z),

In addition, it holds that

,

f (z) − f (x)
z − x
f (cid:48)
+(z) = lim
z ↓ x

f (cid:48)
+(x) = lim
z ↓ x

(7a)

(7b)

f (cid:48)
−(z).

+(x)(cid:9) ,
2 Its deﬁnition can be found at the beginning of Section 4.

∂f (x) = (cid:8)d ∈ (cid:60) | f (cid:48)

−(x) ≤ d ≤ f (cid:48)

∀ x ∈ (cid:60).

(8)

6

Zhensheng Yu et al.

Given a convex function f : (cid:60) → (cid:60), the left or right continuity of f (cid:48)

− and
f (cid:48)
+ derived in Lemma 1, together with their non-decreasing properties, imply that
certain intermediate value theorem holds for the subgradient mapping ∂f .

−(x1), f (cid:48)

Lemma 2 Let f : (cid:60) → (cid:60) be a convex function. Given an interval [x1, x2] ⊆ (cid:60),
for any α ∈ [f (cid:48)
+(x2)], there exists c ∈ (cid:60) such that f (cid:48)
+(c), i.e.,
α ∈ ∂f (c). In fact, for the given α, a particular choice is
−(z) ≤ α(cid:9) .

c = sup (cid:8)z ∈ [x1, x2] | f (cid:48)

−(c) ≤ α ≤ f (cid:48)

Proof Let S = (cid:8)z ∈ [x1, x2] | f (cid:48)
−(z) ≤ α(cid:9). Since α ≥ f (cid:48)
−(x1), we know that x1 ∈ S,
i.e., S is nonempty. Meanwhile, since S is also upper bounded, by the completeness
of the real numbers, we know that c = sup S exists and x1 ≤ c ≤ x2.

Assume on the contrary that α (cid:54)∈ ∂f (c). Then, we can construct an operator

T through its graph

graph T = graph ∂f ∪ {(c, α)}.

It is not diﬃcult to verify that T is monotone. Since ∂f is maximal monotone [27],
it holds that T = ∂f . We arrive at a contradiction and complete the proof of the
lemma.

Corollary 1 Let f : (cid:60) → (cid:60) be a convex function and suppose that inf x f (cid:48)
supx f (cid:48)
+(x). For any given α ∈ (inf x f (cid:48)
that α ∈ ∂f (c) and a particular choice is

−(x) <
+(x)), there exists c ∈ (cid:60) such

−(x), supx f (cid:48)

c = sup (cid:8)z ∈ (cid:60) | f (cid:48)

−(z) ≤ α(cid:9) .

Proof Since inf x f (cid:48)
and −∞ < f (cid:48)
fact that

−(x) < α < supx f (cid:48)

+(x), there exist x1, x2 ∈ (cid:60) such that x1 < x2
+(x2) < +∞. The desired result then follows from the

−(x1) < α < f (cid:48)

sup (cid:8)z ∈ (cid:60) | f (cid:48)

−(z) ≤ α(cid:9) = sup (cid:8)z ∈ [x1, x2] | f (cid:48)

−(z) ≤ α(cid:9)

and Lemma 2.

Proposition 1 Let f : (cid:60) → (cid:60) be a convex function and suppose that inf x f (cid:48)
supx f (cid:48)
problem:

−(x) <
+(x). Given λ ∈ (cid:60), let Ω(λ) be the optimal solution set to the following

min
x∈(cid:60)

{f (x) − λx} .

Then, Ω(λ) is nonempty if and only if either one of the following two conditions
holds:
1. there exists ¯x ∈ (cid:60) such that f (cid:48)

−(x) = λ or f (cid:48)

−(¯x) = inf x f (cid:48)

+(¯x) = supx f (cid:48)

+(x) =

−(x) < λ < supx f (cid:48)

λ;
2. inf x f (cid:48)
+(x).
+(x)(cid:9).
Proof Observe that Ω(λ) = {x ∈ (cid:60) | λ ∈ ∂f (x)} = (cid:8)x ∈ (cid:60) | f (cid:48)
Thus, we see that if none of the above two conditions holds, then Ω(λ) is an empty
set, i.e., the “only if” part is proved.

−(x) ≤ λ ≤ f (cid:48)

Next, we focus on the “if” part. Suppose that condition 1 holds. Then, λ ∈
∂f (¯x), i.e., ¯x ∈ Ω(λ). Meanwhile, if condition 2 holds, by Corollary 1, we know
that there exists c ∈ (cid:60) such that λ ∈ ∂f (c), i.e., c ∈ Ω(λ). Therefore, in both
cases, we know that Ω(λ) (cid:54)= ∅. We thus complete the proof.

DP for GNIO

7

Let f : (cid:60) → (cid:60) be a convex function. Denote by f 0+ the recession function of
f . We summarize in the following lemma some useful properties associated with
f 0+. The proofs can be founded in [17, Propositions 3.2.1, 3.2.4, 3.2.8].

Lemma 3 Let f, g be two real-valued convex functions over (cid:60). It holds that

– for any d ∈ (cid:60), (f 0+)(d) = limt→+∞(f (x0 + td) − f (x0))/t, where x0 ∈ (cid:60) is

arbitrary;

– f has bounded level-sets if and only if (f 0+)(d) > 0 for all d (cid:54)= 0;
– (f + g)0+ = f 0+ + g0+.

3 A dynamic programming algorithm for solving (1)

In this section, we shall develop a dynamic programming algorithm for solving
the generalized nearly isotonic optimization problem (1). Inspired by similar ideas
explored in [22, 28], we uncover the recursion nature of problem (1) and reformulate
it into a form which is suitable for dynamic programming approaches.

Let h1(x1) = 0 for all x1 ∈ (cid:60) and for i = 2, . . . , n, deﬁne recursively functions

hi by

hi(xi) := min

xi−1∈(cid:60)

(cid:40)

fi−1(xi−1) + hi−1(xi−1)
+ λi−1(xi−1 − xi)+ + µi−1(xi − xi−1)+

(cid:41)

, ∀ xi ∈ (cid:60).

(9)

Then, it holds for any 2 ≤ i ≤ n that

min
xi∈(cid:60)

{fi(xi) + hi(xi)}

=

min
xi∈(cid:60),...,x1∈(cid:60)




i
(cid:88)



j=1

fj(xj) +

i−1
(cid:88)

j=1

λj(xj − xj+1)+ +

i−1
(cid:88)

j=1

µj(xj+1 − xj)+

(10)






.

In particular, the optimal value of problem (1) can be obtained in the following
way:

min
x1∈(cid:60),··· ,xn∈(cid:60)

(cid:40) n
(cid:88)

i=1

fi(xi) +

n−1
(cid:88)

i=1

= min
xn∈(cid:60)

{fn(xn) + hn(xn)} .

λi(xi − xi+1)+ +

(cid:41)

µi(xi+1 − xi)+

n−1
(cid:88)

i=1

These observations allow us to solve problem (1) via solving a series of subproblems
involving functions hi. Indeed, suppose that

x∗
n ∈ argmin
xn∈(cid:60)

{fn(xn) + hn(xn)} .

For i = n − 1, . . . , 1, let x∗
following problem:

i be recursively deﬁned as an optimal solution to the

x∗
i ∈ argmin

xi∈(cid:60)

(cid:8)fi(xi) + hi(xi) + λi(xi − x∗

i+1)+ + µi(x∗

i+1 − xi)+

(cid:9) .

(11)

8

Zhensheng Yu et al.

Based on (10), one can easily show that x∗ = (x∗
(1). For later use, we further deﬁne gi = fi + hi for all i = 1, . . . , n.

1, . . . , x∗

n) ∈ (cid:60)n solves problem

The above observations inspire us to apply the following dynamic programming
algorithm for solving problem (1). To express the high-level idea more clearly, we
only present the algorithm in the most abstract form here. More details will be
revealed in subsequent discussions.

Algorithm 1 A dynamic programming algorithm for problem (1)

i ) = update(gi−1, λi−1, µi−1)

i , b+

gi−1 = sum(hi−1, fi−1)
(hi, b−

1: Initialize: h1(x) = 0
2: for i = 2 : n do
3:
4:
5: end for
6: gn = sum(hn, fn)
7: xn = argminx∈(cid:60) gn(x)
8: x1, · · · , xn−1 = recover(xn,{(b−
9: Return x1, · · · , xn−1, xn

i , b+

i )}n

i=2)

i and b+

In the i-th iteration of the for-loop in the above algorithm, the “sum” function
computes the summation of fi−1 and hi−1 to obtain gi−1, i.e., gi−1 = hi−1 +
fi−1. Based on the deﬁnition of hi in (9), the “update” function computes hi
from gi−1. The extra outputs b−
from this step will be used to recover
i
the optimal solution in the “recover” function which is based on the backward
computations in (11). Hence, both the “update” and the “recover” steps involve
solving similar optimization problems in the form of (9). A ﬁrst glance of the
deﬁnition of hi in (9) may lead us to the conclusion that the “update” step is
intractable. The reason is that one has to compute hi over all xi ∈ (cid:60), i.e., inﬁnitely
many optimization problems have to be solved. To alleviate this diﬃculty, based
on a careful exploitation of the special structures of the generalized nearly isotonic
regularizer in (9), we present an explicit formula to compute hi. Speciﬁcally, we are
able to determine hi by calculating two special breakpoints b−
i ∈ (cid:60) via solving
at most two one-dimensional optimization problems. Moreover, these breakpoints
will also be used in the “recover” step. In fact, as one will observe later, an optimal
solution to problem (11) enjoys a closed-form representation involving b−
i and b+
i .
A concrete example on the detailed implementations of these steps will be discussed
in Section 4.

i , b+

3.1 An explicit updating formula for (9)

In this subsection, we study the “update” step in Algorithm 1. In particular, we
show how to obtain an explicit updating formula for hi deﬁned in (9).

We start with a more abstract reformulation of (9). Given a univariate convex
function g : (cid:60) → (cid:60) and nonnegative and possibly inﬁnite constants µ, λ, for any
given y ∈ (cid:60), let

py(x) = λ(x − y)+ + µ(y − x)+ and zy(x) = g(x) + py(x),

x ∈ (cid:60).

(12)

DP for GNIO

9

Here, if for some, λ = +∞ (respectively, µ = +∞), the corresponding regular-
ization term λ(x − y)+ (respectively, µ(y − x)+) in py shall be understood as the
indicator function δ(x | x ≤ y) (respectively, δ(x | x ≥ y)). We focus on the
optimal value function h deﬁned as follows:

h(y) := min
x∈(cid:60)

zy(x),

y ∈ (cid:60).

(13)

For the well-deﬁnedness of h in (13), we further assume that g has bounded level-
sets, i.e., there exists α ∈ (cid:60) such that the set {x ∈ (cid:60) | g(x) ≤ α} is non-empty and
bounded. Indeed, under this assumption, it is not diﬃcult to see that zy is also
a convex function and has bounded level-sets. Then, by [27, Theorems 27.1 and
27.2], we know that for any y, problem minx zy(x) has a non-empty and bounded
optimal solution set. Therefore, the optimal value function h is well-deﬁned on (cid:60)
with dom h = (cid:60).

Since dom g = (cid:60), we know from the deﬁnitions of zy and py in (12) and [27,

Theorem 23.8] that

∂zy(x) = ∂g(x) + ∂py(x),

∀ x ∈ (cid:60).

(14)

Since g has bounded level-sets, it holds from [27, Theorems 27.1 and 27.2] that
the optimal solution set to minx g(x), i.e., S = {u ∈ (cid:60) | 0 ∈ ∂g(u)}, is nonempty
and bounded. Let u∗ ∈ S be an optimal solution, then, by Lemma 1, inf x g(cid:48)
−(x) ≤
−(u∗) ≤ 0 ≤ g(cid:48)
g(cid:48)

+(x). We further argue that

+(u∗) ≤ supx g(cid:48)

inf
x

g(cid:48)
−(x) < 0 < sup
x

g(cid:48)
+(x).

(15)

Indeed, if inf x g(cid:48)

−(x) = 0, then

0 = inf
x

−(x) ≤ g(cid:48)
g(cid:48)

−(x) ≤ g(cid:48)

−(u∗) ≤ 0,

∀ x ∈ (−∞, u∗],

−(x) = 0 for all x ∈ (−∞, u∗]. Therefore, by Lemma 1, we know that
i.e., g(cid:48)
0 ∈ ∂g(x) for all x ∈ (−∞, u∗], i.e., (−∞, u∗] ⊆ S. This contradicts to the fact
that S is bounded. Hence, it holds that inf x g(cid:48)
−(x) < 0. Similarly, one can show
that supx g(cid:48)

+(x) > 0.

Next, based on observation (15), for any given nonnegative and possibly inﬁnite
parameters λ, µ, it holds that −λ ≤ 0 < supx g(cid:48)
−(x). Now,
we deﬁne two breakpoints b− and b+ associated with the function g and parameters
λ, µ. Particularly, we note from [27, Theorem 23.5] that

+(x) and µ ≥ 0 > inf x g(cid:48)

∂g∗(α) = {x ∈ (cid:60) | α ∈ ∂g(x)}

= the optimal solution set of problem min
z∈(cid:60)

{g(z) − αz}.

Deﬁne

and

(cid:40)

b−

= −∞,
∈ ∂g∗(−λ),

(cid:40)

b+

= +∞,
∈ ∂g∗(µ),

if λ = +∞ or ∂g∗(−λ) = ∅,

otherwise,

if µ = +∞ or ∂g∗(µ) = ∅,

otherwise

(16)

(17)

(18)

10

Zhensheng Yu et al.

with the following special case

b+ = b− ∈ ∂g∗(0),

if λ = µ = 0.

(19)

Here, the nonemptiness of ∂g∗(0) follows from (15) and Proposition 1. In fact,
Proposition 1 and (15) guarantee that there exist parameters λ and µ such that
b− and b+ are ﬁnite real numbers. Moreover, as one can observe from the above
deﬁnitions, to determine b− and b+, we only need to solve at most two one-
dimensional optimization problems.

Lemma 4 For b+ and b− deﬁned in (17), (18) and (19), it holds that b− ≤ b+.

Proof The desired result follows directly from the deﬁnitions of b+ and b− and the
monotonicity of ∂g∗.

With the above preparations, we have the following theorem which provides

an explicit formula for computing h.

Theorem 1 Suppose that the convex function g : (cid:60) → (cid:60) has bounded level-
sets. For any y ∈ (cid:60), x∗(y) = min(b+, max(b−, y)) is an optimal solution to
minx∈(cid:60) zy(x) in (13) and

h(y) =






g(b−) + λ(b− − y),

g(y),
g(b+) + µ(y − b+),

if y < b−,
if b− ≤ y ≤ b+,
if y > b+

(20)

with the convention that {y ∈ (cid:60) | y < −∞} = {y ∈ (cid:60) | y > +∞} = ∅. Moreover,
h : (cid:60) → (cid:60) is a convex function, and for any y ∈ (cid:60), it holds that

∂h(y) =






{−λ},
[−λ, g(cid:48)

+(b−)],

∂g(y),
[g(cid:48)

−(b+), µ],

{µ},

if y < b−,
if y = b−,
if b− < y < b+,
if y = b+,
if y > b+.

(21)

Proof Under the assumption that the convex function g has bounded level-sets, it
is not diﬃcult to see that zy also has bounded level-sets, and thus problem (13)
has nonempty bounded optimal solution set.

Recall the deﬁnitions of b− and b+ in (17), (18) and (19). From Lemma 4,
it holds that b− ≤ b+. We only consider the case where both b− and b+ are
ﬁnite. The case where b− and/or b+ takes extended real values (i.e., ±∞) can be
easily proved by slightly modifying the arguments presented here. Now from the
deﬁnitions of b− and b+, we know that λ, µ are ﬁnite nonnegative numbers and

− λ ∈ ∂g(b−)

and µ ∈ ∂g(b+).

(22)

According to the value of y, we discuss three situations:

(i) Suppose that y < b−, then ∂py(b−) = {λ} and thus by (14), ∂zy(b−) =
∂g(b−)+{λ}. From (22), we know that 0 ∈ ∂zy(b−), i.e., b− ∈ argminx∈(cid:60) zy(x).

DP for GNIO

11

(ii) Suppose that y ∈ [b−, b+]. Now, ∂py(y) = [−µ, λ] and ∂zy(y) = ∂g(y)+[−µ, λ].

From [27, Theorem 24.1], it holds that

−λ = g(cid:48)

−(b−) ≤ g(cid:48)

−(y) ≤ g(cid:48)

+(y) ≤ g(cid:48)

−(b+) = µ.

Then, we know from (8) that ∂g(y) ⊆ [−λ, µ] and thus 0 ∈ ∂zy(y), i.e., y ∈
argminx∈(cid:60) zy(x).

(iii) Suppose that y > b+, then ∂py(b+) = {−µ} and thus ∂zy(b+) = ∂g(b+) +
{−µ}. From (22), we know that 0 ∈ ∂zy(b+), i.e., b+ ∈ argminx∈(cid:60) zy(x).
We thus proved that x∗(y) = min(b+, max(b−, y)) ∈ argmin zy(x). The formula of
h in (20) follows directly by observing h(y) = zy(x∗(y)) for all y ∈ (cid:60).
Now we turn to the convexity of h. From (20), we have that

h(cid:48)
+(y) =






−λ,
g(cid:48)
+(y),
µ,

if y < b−,
if b− ≤ y < b+,
if y ≥ b+.

It is not diﬃcult to know from Lemma 1 and (22) that h(cid:48)
(cid:60). Then, the convexity of h follows easily from [17, Theorem 6.4].

+ is non-decreasing over

Lastly, we note that (21) can be obtained from (20) and the fact that −λ =
+(b+) = µ, i.e., ∂h(b−) =
−(b+), µ]. We thus complete the proof for the theo-

+(b−) = g(cid:48)
+(b−)] and ∂h(b+) = [g(cid:48)

+(b−) and g(cid:48)

−(b+) ≤ h(cid:48)

−(b+) = h(cid:48)

−(b−) ≤ h(cid:48)
h(cid:48)
[−λ, g(cid:48)
rem.

−(y) ≤ h(cid:48)

Theorem 1 indicates that the optimal value function h can be constructed di-
rectly from the input function g. Indeed, after identifying b− and b+, h is obtained
via replacing g over (−∞, b−) and (b+, +∞) by simple aﬃne functions. This pro-
cedure also results in a truncation of the subgradient of g. Indeed, from (21), we
know that −λ ≤ h(cid:48)
+(y) ≤ µ for all y ∈ (cid:60). That is, the subgradient
of h is restricted between the upper bound µ and lower bound −λ. To further
help the understanding of Theorem 1, we also provide a simple illustration here.
Speciﬁcally, let g be a quadratic function g(x) = (x − 0.5)2, x ∈ (cid:60) and set λ = 0.4,
µ = 0.2. In this case, simple computations assert that b− = 0.3 and b+ = 0.6.
In Figure 1, we plot functions g, h and their derivatives. Now, it should be more
clear that the updating formula in Theorem 1 can be regarded as a generalization
to the famous soft-thresholding [13, 14].

Next, we turn to problem (9). At each iteration of Algorithm 1, to preform the
“update” step using Theorem 1, we need to verify the assumption on gi which is
required in Theorem 1.

Proposition 2 Suppose that Assumption 1 holds. Then, it holds that for all i =
1, . . . , n, gi = fi + hi are real-valued convex functions with bounded level-sets.

Proof We prove the result by induction on i. The result clearly holds with i = 1
since h1 = 0 and f1 is assumed to be a convex function with bounded level-sets.
Now, suppose that for all i ≤ l, gi are convex and have bounded level-sets. Then, we
can invoke Theorem 1 with g = gl, λ = λl and µ = µl to know that hl+1 : (cid:60) → (cid:60)
is convex and takes the form as in (20). It is not diﬃcult to verify that

(hl+10+)(d) ≥ 0,

∀ d (cid:54)= 0.

12

Zhensheng Yu et al.

(a) Functions g and h

(b) Derivatives of g and h

Fig. 1 Illustration of Theorem 1, functions g, h in the left panel and their derivatives in the
right panel.

Since fl+1 is assumed to be a real-valued convex function with bounded level-sets,
we know from Lemma 3 that (fl+10+)(d) > 0 for all d (cid:54)= 0 and gl+1 is a real-valued
convex function satisfying

(gl+10+)(d) = (fl+10+)(d) + (hl+10+)(d) > 0,

∀ d (cid:54)= 0.

That is gl+1 has bounded level-set and the proof is completed.

If additional smoothness assumptions hold for the loss functions fi, i = 1, . . . , n,

a similar proposition on the diﬀerentiability of hi and gi can also be obtained.

Proposition 3 Suppose that Assumption 1 holds and each fi, i = 1, . . . , n, is
diﬀerentiable. Then, both gi and hi, i = 1, . . . , n, are diﬀerentiable functions on
(cid:60).

l+1, +∞). Hence, we should check the diﬀerentiability of h at b−

Proof We prove the proposition by induction on i. Clearly, the assertion holds
with i = 1 since h1 = 0, g1 = h1 + f1 = f1, and f1 is assumed to be diﬀerentiable.
Now, assume that for all i ≤ l, hl and gl are diﬀerentiable. Then, by Proposition 2
and Theorem 1, we know that hl+1 is diﬀerentiable over (−∞, b−
l+1),
and (b+
l+1 and
b+
l+1. Here, we only consider the case with −∞ < b−
l+1 < +∞. The case
where either b−
l+1 takes extended real values (i.e., ±∞) can be easily
proved by slightly modifying the arguments presented here. Recalling deﬁnitions
l+1 and b+
of b−
l+1 in (17) and (18), and using the diﬀerentiability of gl, we have
l(b+
l(b−
l+(b−
g(cid:48)
l+1) = g(cid:48)
l+1) = µl. Then, (21) in Theorem
1 implies that

l+1) = −λl and g(cid:48)

l+1 and/or b+

l+1 ≤ b+

l+1), (b−

l+1) = g(cid:48)

l+1, b+

l−(b+

∂hl+1(b−

l+1) = [−λl, g(cid:48)

l+(b−

l+1)] = {−λl} and ∂hl+1(b+

l+1) = [g(cid:48)

−(b+

l+1), µl] = {µl}.

Hence, hl+1 and gl+1 = hl+1 + fl+1 are diﬀerentiable over (cid:60). We thus complete
the proof.

With the above discussions, in particular, Theorem 1 and Proposition 2, we

can write the “update” step in Algorithm 1 in a more detailed fashion.

DP for GNIO

13

Algorithm 2 The “update” step in Algorithm 1: [h, b−, b+] = update(g, λ, µ)
1: Input function g and parameters λ, µ.
2: Compute (b−, b+) according to deﬁnitions (17), (18) and (22).
3: Compute h via (20): for all y ∈ (cid:60),
4:





g(b−) + λ(b− − y),

g(y),
g(b+) + µ(y − b+),

if y < b−,
if b− ≤ y ≤ b+,
if y > b+.

h(y) =

5: Return h, b−, b+

Meanwhile, we can further obtain the implementation details for the “recover”
step in Algorithm 1 based on the discussions in (11) and Theorem 1.

i , b+

i=2)
i , b+

Algorithm 3 The “recover” step in Algorithm 1:
recover(xn, {(b−
i )}n
1: Input xn and {(b−
2: for i = n : 2 do
3:
4: end for
5: Return x1, · · · , xn−1

xi−1 = min(b+

i , max(b−

i , xi))

i )}n

i=2)

[x1, . . . , xn−1] =

Thus, instead of using the deﬁnition (13) directly to compute hi, we leverage
on the special structure of the nearly isotonic regularizer and show that hi can
be explicitly constructed by solving at most two one-dimensional optimization
problems. Hence, we obtain an implementable dynamic programming algorithm
for solving the generalized nearly isotonic optimization problem (1).

Remark 1 One issue we do not touch seriously here is the computational details
of obtaining b− and b+ in Algorithm 2. Based on the deﬁnitions in (17), (18) and
(19), to obtain b− and b+, at most two one-dimensional optimization problems in
the form of (16) need to be solved. Given the available information of g (such as the
function value and/or subgradient of g at given points), various one-dimensional
optimization algorithms can be used. Moreover, in many real applications such as
the later discussed (cid:96)1-GNIO and (cid:96)2-GNIO problems, b− and b+ can be computed
via closed-form expressions.

4 Implementation details of Algorithm 1 for (cid:96)2-GNIO

In the previous section, an implementable DP based algorithmic framework is de-
veloped for solving the GNIO problem (1). We shall mention that for special classes
of loss functions, it is possible to obtain a low running complexity implementa-
tion of Algorithm 1. As a prominent example, in this section, we discuss some
implementation details of Algorithm 1 for solving (cid:96)2-GNIO problems, i.e., for all

14

Zhensheng Yu et al.

i = 1, . . . n, each loss function fi in (1) is a simple quadratic function. Speciﬁcally,
given data points {yi}n
i=1, we consider the convex
quadratic loss functions

i=1 and positive weights {wi}n

fi(xi) = wi(xi − yi)2,

xi ∈ (cid:60),

i = 1, . . . , n.

(23)

and the following problem:

((cid:96)2-GNIO)

min
x∈(cid:60)n

n
(cid:88)

i=1

wi(xi − yi)2 +

n−1
(cid:88)

i=1

λi(xi − xi+1)+ +

n−1
(cid:88)

i=1

µi(xi+1 − xi)+,

We note that quadratic loss functions have been extensively used in the context
of shape restricted statistical regression problems [6, 21, 29, 34].

In the following, special numerical representations of quadratic functions will be
introduced to achieve a highly eﬃcient implementation of Algorithm 1 for solving
(cid:96)2-GNIO problems. We start with a short introduction of univariate piecewise
quadratic functions. We say h is a univariate piecewise quadratic function if there is
a strictly increasing sequence {βl}K
l=1 ⊆ (cid:60) and h agrees with a quadratic function
on each of the intervals (−∞, β1), [βl, βl+1), l = 1, . . . , K − 1, and [βK , +∞).
Here, each βl is referred to as a “breakpoint” of h and univariate aﬃne functions
are regarded as degenerate quadratic functions. The following proposition states
that for (cid:96)2-GNIO problems, in each iteration of Algorithm 1, the corresponding
functions hi and gi, i = 1, . . . , n, are all convex diﬀerentiable piecewise quadratic
functions. The proof of the proposition is similar to that of Propositions 2 and 3
and is thus omitted.

Proposition 4 Let the loss functions fi, i = 1, . . . , n, in GNIO problem (1) be
convex quadratic functions as given in (23). Then, in Algorithm 1, all involved
functions hi and gi, i = 1, . . . , n, are univariate convex diﬀerentiable piecewise
quadratic functions. Moreover, it holds that

g(cid:48)
i(x) = −∞ and

inf
x

sup
x

g(cid:48)
i(x) = +∞,

∀ i = 1, . . . , n.

From Proposition 4, we see that an important issue in the implementation of
Algorithm 1 is the numerical representation of the univariate convex diﬀerentiable
piecewise quadratic functions. Here, inspired by the data structures exploited in
[28, 22], we adopt a strategy called “diﬀerence of coeﬃcients” to represent these
functions. Based on these representations, implementation details of the subrou-
tines “sum” and “update” in Algorithm 1 will be further discussed.

Let h be a univariate convex diﬀerentiable piecewise quadratic function and
l=1 be the associated breakpoints. These breakpoints deﬁne K + 1 intervals

{βl}K
in the following form:

(−∞, β1), [βl, βl+1), l = 1, . . . , K − 1, and [βK , +∞).

Assume that h agrees with ajx2 + bjx on the j-th (1 ≤ j ≤ K + 1) interval3. Here,
aj ≥ 0, bj ∈ (cid:60) are given data. To represent h, we ﬁrst store the breakpoints {βl}K
l=1
in a sorted list Bh with ascending order and store the number of breakpoints as

3 The intercepts are ignored for all the pieces as they are irrelevant in the optimization

process.

DP for GNIO

15

name
Bh
Dh
ch
L
ch
R
K h

data-type
list
list
tuple
tuple
integer

explaination

breaking points
diﬀerences of coeﬃcients
coeﬃcients of the leftmost piece
coeﬃcients of the rightmost piece
number of breaking points

Table 1 Representation of the univariate convex diﬀerentiable piecewise quadratic function
h.

K h = K. Associated with each breakpoint βl, we compute the diﬀerence of the
coeﬃcients between the consecutive piece and the current piece: dl = (al+1 −
l=1 in list Dh. Information of the leftmost
al, bl+1 − bl), l = 1, . . . , K and store {dl}K
and rightmost pieces are stored in two tuples ch
R = (aK+1, bK+1),
respectively. With these notation, we can write the representation symbolically as

L = (a1, b1) and ch

h = [Bh, Dh, ch

L, ch

R, K h].

We summarize the above representation of h in Table 1. In the same spirit, given
y ∈ (cid:60), for the quadratic function f (x) = w(x − y)2, x ∈ (cid:60), we have the fol-
lowing representation f = [∅, ∅, (w, −2wy), (w, −2wy), 0]. With the help of these
representations, for (cid:96)2-GNIO problems, one can easily perform the “sum” step in
Algorithm 1. More speciﬁcally, the representation of g = h + f can be written in
the following way

g = [Bg, Dg, cg

L, cg

R, K g] = [Bh, Dh, ch

L + (w, −2wy), ch

R + (w, −2wy), K h].

That is, to obtain the representation of g, one merely needs to modify the coeﬃ-
cients of the leftmost and rightmost pieces, i.e., ch
R, in the representation
of h. Hence, with this representation, the running time for the above “sum” step
is O(1). We summarize the above discussions into the following framework.

L and ch

Algorithm 4 “sum” in Algorithm 1 for (cid:96)2-GNIO: g = sum (h, f )
h
[∅, ∅, (w, −2wy), (w, −2wy), 0]

[B, D, cL, cR, K]

representations

1: Input

=

and

f

=

2: Compute the representation of g via

[B, D, cL, cR, K] = [B, D, cL + (w, −2wy), cR + (w, −2wy), K].

3: Return g = [B, D, cL, cR, K].

In the following, we show how to use the above representations to eﬃciently im-
plement the “update” step in Algorithm 1 for solving (cid:96)2-GNIO problem. By Propo-
sition 4, we know that in each iteration of Algorithm 1, the output b−
(respec-
i
tively, b+
i ) of the “update” step will always be ﬁnite real numbers except the case
with λi−1 (respectively, µi−1) being +∞. Hence, we focus on the computations
of ﬁnite b−
i . Consider an abstract instance where a univariate convex
diﬀerentiable piecewise quadratic function g = [Bg, Dg, cg
R, K g] and positive

i and/or b+

L, cg

16

Zhensheng Yu et al.

parameters λ and µ are given. We aim to compute [h, b−, b+] = update(g, λ, µ)
and the corresponding representation of h = [Bh, Dh, ch
R, K h]. Speciﬁcally,
from deﬁnition (17), we compute b− by solving the following equation:

L, ch

g(cid:48)(b−) + λ = 0.

Based on the representation of g, the derivative of g over all pieces can be easily
obtained. Hence, b− can be eﬃciently determined by searching from the leftmost
piece to the rightmost piece of g. Similarly, b+ can be computed in a reverse
searching order. The implementation details are presented in Algorithm 5. In fact,
as one can observe, the computations of b− and b+ can be done simultaneously. For
the simplicity, we do not exploit this parallelable feature in our implementation. We
also have the following observations in Algorithm 5: (1) in each round of Algorithm
5, the number of breakpoints, denoted ∆, satisﬁes ∆ = k− + k+ − 2 ≤ K g and
K h ≤ K g − ∆ + 2; (2) since b− ≤ b+, each breakpoint can be marked as “to
delete” at most once; (3) the total number of the executions of the while-loops is
the same as the number of the deleted breakpoints; (4) the output Bh is a sorted
list.

Now, we are ready to discuss the worst-case running time of Algorithm 1
for solving (cid:96)2-GNIO problems. We ﬁrst observe that the “sum” and “recover”
steps in Algorithm 1 takes O(n) time in total. For i = 1, . . . , n − 1, we denote
the number of breakpoints of gi by Ki and the number of deleted breakpoints
from gi by ∆i. From the observation (1), we know that ∆i ≤ Ki and Ki+1 ≤
Ki − ∆i + 2 for i = 1, . . . , n − 1. Simple calculations yield (cid:80)n−1
i=1 ∆i ≤ 2(n − 2),
i.e., the total number of the deleted breakpoints in Algorithm 1 is upper bounded
by 2n. This, together with the observation (3), implies that the total number
of executions of the while-loop in Algorithm 5 is upper bounded by 2n. Thus,
the overall running time of the “update” step is O(n). Therefore, based on the
“diﬀerence-of-coeﬃcients” representation scheme and the special implementations
of the “sum” and the “update” steps, we show that Algorithm 1 solves (cid:96)2-GNIO
with O(n) worst-case running time. That is our specially implemented Algorithm
1 is an optimal algorithm for solving (cid:96)2-GNIO problems.

Remark 2 In the above discussions, in addition to the numerical representations
of convex diﬀerentiable piecewise quadratic functions, we see that the data struc-
tures also play important roles in both the implementations and the running time
analysis. Hence, in order to obtain eﬃcient implementations of Algorithm 1 for
solving other GNIO problems, one has to explore appropriate problem dependent
data structures. For example, consider the (cid:96)1-GNIO problem, i.e., given y ∈ (cid:60)n
and a positive weight vector w ∈ (cid:60)n, let the loss functions in (1) take the following
form: fi(xi) = wi|xi − yi|, xi ∈ (cid:60), i = 1, . . . , n. Due to the nonsmoothness of fi, if
the data structure “list” were used to store the involved breakpoints, the running
time of each “sum” step in Algorithm 1 will be O(n) and the overall running time
will be O(n2) in the worst case. For the acceleration, we propose to use red-black
trees [11] to store these breakpoints. It can be shown that with this special data
structure, the running time of each “sum” step can be reduced to O(log n). Hence,
the overall running time of the “sum” step, as well as Algorithm 1, is O(n log n).
Since the running time analysis is not the main focus of the current paper, more
details on the O(n log n) complexity of Algorithm 1 for solving (cid:96)1-GNIO are pre-
sented in the Appendix. This result is also veriﬁed by various numerical evidences

DP for GNIO

17

b− = −∞

Algorithm 5 “update” in Algorithm 1 for (cid:96)2-GNIO: [h, b−, b+] = update (g, λ, µ)
1: Input the representation g = [B, D, cL, cR, K] and parameters λ, µ
2:
3: k− = k+ = 1
4: if λ = +∞ then
5:
6: else
7:
8:
9:
10:
11:
12:
13:
14:

while 2aB{k−} + b < −λ do (cid:46) B{k−} denotes the k−th element in B
(cid:46) D{k−} denotes the k−th element in D

(a, b) = (a, b) + D{k−}
Mark the k−th breakpoint as “to delete”, k− = k− + 1

(a, b) = cL
if K ≥ 1 then

(cid:46) Found the “correct” piece and

end if
b− = − b+λ

end while

2a , cL = (0, −λ)

g(cid:48)(b−) = 2ab− + b = −λ

b+ = b−, mark the k−, . . . , Kth breakpoints as “to delete”, k+ = K−k−+2

15: end if
16:
17: if µ = +∞ then
b+ = +∞
18:
19: else if λ = µ = 0 then
20:
21: else
22:
23:
24:
25:
26:
27:
28:
29:

(a(cid:48), b(cid:48)) = cR
if K ≥ 1 then

end if
b+ = µ−b(cid:48)

end while

2a(cid:48) , cR = (0, µ)

g(cid:48)(b+) = 2a(cid:48)b+ + b(cid:48) = µ

while 2a(cid:48)B{K − k+ + 1} + b(cid:48) > µ do
(a(cid:48), b(cid:48)) = (a(cid:48), b(cid:48)) − D{K − k+ + 1}
Mark the (K − k+ + 1)th breakpoint as “to delete”, k+ = k+ + 1

(cid:46) Found the “correct” piece and

30: end if
31:
32: Delete (k− + k+ − 2) breakpoints marked as “to delete” from B and the
corresponding coeﬃcients from D (cid:46) Since b− ≤ b+, each breakpoint can be
marked as “to delete” at most once

B = B.prepend(b−), D = D.prepend((a, b)−cL), K = K−(k−+k+−2)+1

33:
34: if λ < +∞ then
35:
36: end if
37: if µ < +∞ and λ + µ > 0 then
38:
39: end if
40: Return h = [B, D, cL, cR, K] and b−, b+

B = B.append(b+), D = D.append(cR − (a(cid:48), b(cid:48))), K = K + 1

18

Zhensheng Yu et al.

in Section 5. Finally, we shall mention that the O(n log n) running time matches
the best-known results [32] for some special (cid:96)1-GNIO problems, such as (cid:96)1-isotonic
regression and (cid:96)1-unimodal regression problems.

5 Numerical experiments

In this section, we shall evaluate the performance of our dynamic programming
Algorithm 1 for solving generalized nearly isotonic regression problems (1). Since
the (cid:96)1 losses and (cid:96)2 losses are widely used in practical applications, we implement
Algorithm 1 for solving both (cid:96)1-GNIO and (cid:96)2-GNIO problems:

min
x∈(cid:60)n

n
(cid:88)

i=1

wi|xi − yi| +

n−1
(cid:88)

i=1

λi(xi − xi+1)+ +

n−1
(cid:88)

i=1

µi(xi+1 − xi)+,

(24)

and

min
x∈(cid:60)n

n
(cid:88)

i=1

wi(xi − yi)2 +

n−1
(cid:88)

i=1

λi(xi − xi+1)+ +

n−1
(cid:88)

i=1

µi(xi+1 − xi)+,

(25)

where y ∈ (cid:60)n is a given vector, {wi}n
i=1 are positive weights, λi and µi, i =
1, . . . , n − 1 are nonnegative and possibly inﬁnite parameters. We test our al-
gorithms using both randomly generated data and the real data from various
sources. For the testing purposes, we vary the choices of parameters λi and µi,
i = 1, . . . , n − 1. Our algorithms are implemented in C/C++4 and all the compu-
tational results are obtained on a laptop (Intel(R) Core i7-10875H CPU at 2.30
GHz, 32G RAM, and 64-bit Windows 10 operating system).

Note that our algorithm, when applied to solve problems (24) and (25), is an
exact algorithm if the rounding errors are ignored. As far as we know, there is cur-
rently no other open-access exact algorithm which can simultaneously solve both
problems (24) and (25) under various settings of parameters. Fortunately, these
two problems can be equivalently rewritten as linear programming and convex
quadratic programming problems, respectively. Hence, in ﬁrst two subsections of
our experiments, we compare our algorithm with Gurobi (academic license, ver-
sion 9.0.3), which can robustly produce high accurate solutions and is among the
most powerful commercial solvers for linear and quadratic programming. For the
Gurobi solver, we use the default parameter settings, i.e., using the default stop-
ping tolerance and all computing cores. For all the experiments, our algorithm and
Gurobi output objective values whose relative gaps are of order 10−8, i.e., both of
them produce highly accurate solutions.

To further examine the eﬃciency of our dynamic programming algorithm, in
the last part of our numerical section, we conduct more experiments on solving
the fused lasso problem (3). Speciﬁcally, we compare our algorithm with the C
implementation5 of Condat’s direct algorithm [12], which, based on the exten-
sive evaluations in [3], appears to be one of the most eﬃcient algorithm specially
designed and implemented for solving the large-scale fused lasso problem (3).

4 The

available
(DOI:10.5281/zenodo.7172254).

code

is

at

https://github.com/chenxuyu-opt/DP_for_GNIO_CODE

5 https://lcondat.github.io/download/Condat_TV_1D_v2.c

DP for GNIO

19

5.1 DP algorithm versus Gurobi: Simulated data

We ﬁrst test our algorithm with simulated data sets. Speciﬁcally, the input vector
y ∈ (cid:60)n is set to be a random vector with i.i.d. uniform U(−100, 100) entries. The
problems sizes, i.e., n, vary from 104 to 107. The positive weights {wi}n
i=1 are
generated in three ways: (1) ﬁxed, i.e., wi = 1 in (cid:96)1-GNIO (24) and wi = 1/2
in (cid:96)2-GNIO (25), i = 1, . . . , n; (2) i.i.d. sampled from the uniform distribution
U(10−2, 102); (3) i.i.d. sampled from Gaussian distribution N (100, 100) with pos-
sible nonpositive outcomes replaced by 1. For all problems, we test seven settings
of parameters {λi}n−1
1. Isotonic: λi = +∞, µi = 0 for i = 1, . . . , n − 1;
2. Nearly-Isotonic: λi = log(n), µi = 0 for i = 1, . . . , n − 1;
3. Unimodal: λi = +∞, µi = 0 for i = 1, . . . , m and λi = 0, µi = +∞ for

i=1 and {µi}n−1
i=1 :

i = m + 1, . . . , n − 1 with m = [ n−1

2 ];

4. Fused: λi = µi = log(n) for i = 1, . . . , n − 1;
5. Uniform: All λi and µi, i = 1, . . . , n − 1 are i.i.d. sampled from the uniform

distribution U(0, 103);

6. Gaussian: All λi and µi, i = 1, . . . , n − 1 are i.i.d. sampled from Gaussian

distribution N (100, 100) with possible negative outcomes set to be 0;

7. Mixed: λi = +∞ for i = 1, . . . , [ n

5 ] + 1, . . . , n − 1 and µi, i = 1, . . . , n − 1 − [ n

i = [ n
uniform distribution U(0, 103).

5 ] and µi = +∞ for i = n − [ n

5 ], . . . , n − 1; λi,
5 ] are i.i.d. sampled from the

Table 2 reports the detailed numerical results of Algorithm 1 for solving (cid:96)1-GNIO
problem (24) under the above mentioned diﬀerent settings. As one can observe,
our algorithm is quite robust to various patterns of weights and parameters. More-
over, the computation time scales near linearly with problem dimension n, which
empirically veriﬁes our theoretical result on the worst-case O(n log n) running time
of Algorithm 1 for (cid:96)1-GNIO problems in Remark 2.

Note that when parameters λi and µi are ﬁnite, (cid:96)1-GNIO problem (24) can
be formulated as the following linear programming problem (see, e.g., [18, Section
7]):

min
x,z∈(cid:60)n,u,v∈(cid:60)n−1

s.t.

n
(cid:88)

wizi +

n−1
(cid:88)

λiui +

n−1
(cid:88)

i=1

µivi

i=1

i=1
zi ≥ yi − xi,
zi ≥ xi − yi,
xi − xi+1 ≤ ui,
xi+1 − xi ≤ vi,
u ≥ 0, v ≥ 0.

i = 1, · · · , n,

i = 1, · · · , n,

i = 1, · · · , n − 1,

i = 1, · · · , n − 1,

(26)

If some λi and/or µi is inﬁnite, certain modiﬁcations need to be considered. For
example, the isotonic regression (4) can be equivalently reformulated as:

min
x,z∈(cid:60)n

s.t.

n
(cid:88)

wizi

i=1
zi ≥ yi − xi,
zi ≥ xi − yi,
xi − xi+1 ≤ 0,

i = 1, · · · , n,

i = 1, · · · , n,

i = 1, · · · , n − 1.

(27)

20

Zhensheng Yu et al.

Runtime of Algorithm 1 for (24) with wi = 1

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.003
0.046
0.582
7.685

0.003
0.036
0.306
3.040

0.003
0.042
0.545
7.678

0.003
0.031
0.298
3.107

0.002
0.019
0.189
1.839

0.003
0.022
0.214
2.116

0.002
0.019
0.228
2.938

Runtime of Algorithm 1 for (24) with wi ∼ U (10−2, 102)

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.004
0.048
0.701
9.295

0.003
0.025
0.250
2.461

0.003
0.041
0.558
8.052

0.002
0.027
0.255
2.263

0.002
0.023
0.195
1.877

0.001
0.012
0.261
2.665

0.002
0.209
0.239
3.102

Runtime of Algorithm 1 for (24) with wi ∼ N (100, 100)

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.004
0.053
0.718
9.416

0.002
0.024
0.244
2.492

0.004
0.043
0.574
8.531

0.002
0.025
0.249
2.258

0.002
0.025
0.211
1.927

0.003
0.028
0.271
2.565

0.002
0.209
0.244
3.204

Table 2 Runtime (in seconds) of Algorithm 1 for solving (cid:96)1-GNIO problem (24) under diﬀer-
ent settings. Results are averaged over 10 simulations.

Now, we can compare our dynamic programming algorithm with Gurobi on ran-
domly generated data sets under various settings of parameters. For simplicity, we
only consider the ﬁxed weights here, i.e., wi ≡ 1, i = 1, . . . , n. As one can observe
in Table 3, for all the test instances, our dynamic programing algorithm outper-
forms Gurobi by a signiﬁcant margin. Speciﬁcally, for 17 out of 21 instances, our
algorithm can be at least 220 times faster than Gurobi. Moreover, for one instance
in the Gaussian setting, our algorithm can be up to 5,662 times faster than Gurobi.

Next, we test our dynamic programming algorithm for solving (cid:96)2-GNIO prob-
lem (25). Similarly, experiments are done under various settings of weights and
parameters. From Table 4, we see that our algorithm can robustly solve various
(cid:96)2-GNIO problems and the computation time scales linearly with the problem di-
mension n. This matches our result on the O(n) running time of Algorithm 1 for
solving (cid:96)2-GNIO problems.

Similar to the cases in (26) and (27), (cid:96)2-GNIO problem (25) can be equivalently
recast as quadratic programming. Again, we compare our dynamic programming
algorithm with Gurobi for solving problem (25) and present the detailed results in
Table 5. In the table, 0.000 indicates that the runtime is less than 0.001. Note that
for simplicity, in these tests, we ﬁx the weights by setting wi = 1/2, i = 1, . . . , n.
We can observe that for most of the test instances in this class, our dynamic
programming algorithm is able to outperform the highly powerful quadratic pro-
gramming solver in Gurobi by a factor of about 470–5122 in terms of computation
times.

DP for GNIO

21

n

parameters pattern

tDP

tGurobi

tGurobi/tDP

1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7

Isotonic
Isotonic
Isotonic
Isotonic
Nearly-isotonic
Nearly-isotonic
Nearly-isotonic
Nearly-isotonic
Unimodal
Unimodal
Unimodal
Unimodal
Fused
Fused
Fused
Fused
Uniform
Uniform
Uniform
Uniform
Gaussian
Gaussian
Gaussian
Gaussian
Mixed
Mixed
Mixed
Mixed

0.003
0.03
0.58
7.69
0.003
0.04
0.31
3.04
0.003
0.04
0.55
7.69
0.003
0.03
0.30
3.11
0.002
0.02
0.19
1.84
0.003
0.02
0.21
2.12
0.002
0.02
0.23
2.94

0.46
9.79
358.82
*
0.34
5.94
154.15
*
0.43
9.55
363.32
*
0.68
14.21
299.21
*
0.65
15.11
197.12
*
2.02
58.14
1180.75
*
0.79
14.37
535.19
*

153.33
326.33
620.38
*
113.33
148.50
497.26
*
143.33
238.75
660.58
*
226.67
473.67
997.36
*
325.00
755.50
1037.47
*
673.33
2907.00
5662.62
*
395.00
718.50
2326.91
*

Table 3 Runtime (in seconds) comparisons between our dynamic programming Algorithm
1 and Gurobi for solving problem (24) under diﬀerent settings. Results are averaged over 10
simulations. The entry “*” indicates that Gurobi reports “out of memory”.

5.2 DP algorithm versus Gurobi: Real data

In this subsection, we test our algorithm with real data. In particular, we collect
the input vector y ∈ (cid:60)n from various open sources. The following four data sets
are collected and tested:

1. gold: gold price index per minute in SHFE during (2010-01-01 – 2020-06-30)

[23];

2. sugar: sugar price index per minute in ZCE (2007-01-01 – 2020-06-30) [23];
3. aep: hourly estimated energy consumption at American Electric Power (2004

– 2008) [25];

4. ni: hourly estimated energy consumption at Northern Illinois Hub (2004 –

2008) [25].

In these tests, we ﬁx the positive weights with wi = 1 in (cid:96)1-GNIO problem (24) and
wi = 1/2 in (cid:96)2-GNIO problem (25), i = 1, . . . , n. Similar to the experiments con-
ducted in the previous subsection, under diﬀerent settings of parameters {λi}n−1
i=1
and {µi}n−1
i=1 , we test our algorithm against Gurobi. The detailed comparisons for
solving (cid:96)1-GNIO and (cid:96)2-GNIO problems are presented in Tables 6. As one can
observe, our algorithm is quite robust and is much more eﬃcient than Gurobi
for solving many of these instances. For large scale (cid:96)1-GNIO problems with data

22

Zhensheng Yu et al.

Runtime of Algorithm 1 for (25) with wi = 1/2

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.000
0.002
0.020
0.206

0.000
0.003
0.031
0.311

0.000
0.002
0.019
0.197

0.000
0.003
0.032
0.319

0.000
0.003
0.033
0.334

0.000
0.003
0.031
0.309

0.000
0.002
0.020
0.198

Runtime of Algorithm 1 for (25) with wi ∼ U (10−2, 102)

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.000
0.002
0.021
0.213

0.000
0.003
0.029
0.304

0.000
0.002
0.022
0.224

0.000
0.003
0.030
0.303

0.000
0.003
0.031
0.315

0.000
0.003
0.029
0.296

0.000
0.002
0.021
0.185

Runtime of Algorithm 1 for (25) with wi ∼ N (100, 100)

n

Isotonic Nearly-isotonic Unimodal

Fused Uniform Gaussian Mixed

1e4
1e5
1e6
1e7

0.000
0.002
0.020
0.205

0.000
0.003
0.029
0.298

0.000
0.002
0.020
0.211

0.000
0.003
0.029
0.292

0.000
0.003
0.030
0.312

0.000
0.003
0.029
0.299

0.000
0.002
0.018
0.172

Table 4 Runtime (in seconds) of Algorithm 1 for solving (cid:96)2-GNIO problem (25) under diﬀer-
ent settings. Results are averaged over 10 simulations.

sets sugar and gold, our algorithm can be over 600 times faster than Gurobi.
Speciﬁcally, when solving the (cid:96)1-GNIO problem with the largest data set gold
under the “Unimodal” setting, our algorithm is 80,000 times faster than Gurobi.
Meanwhile, for (cid:96)2-GNIO problems, our algorithm can be up to 18,000 times faster
than Gurobi. These experiments with real data sets again conﬁrm the robustness
and the high eﬃciency of our algorithm for solving various GNIO problems.

5.3 DP algorithm versus Condat’s direct algorithm

To further evaluate the performance of our algorithm, we test our DP algorithm
against Condat’s direct algorithm [12] for solving the following fused lasso problem:

min
x∈(cid:60)n

1
2

n
(cid:88)

i=1

(xi − yi)2 + λ

n−1
(cid:88)

i=1

|xi − xi+1|,

where {yi}n
i=1 are given data and λ is a given positive regularization parameter.
As is mentioned in the beginning of the numerical section, this direct algorithm
along with its highly optimized C implementation is regarded as one of the fastest
algorithms for solving the above fused lasso problem [3].

In this subsection, we test ﬁve choices of λ, i.e., λ = 1, 2, 5, 10, 100. The test
data sets are the aforementioned four real data sets and two simulated random
data sets of sizes 106 and 107. Table 7 reports the detailed numerical results of
the comparison between our algorithm and Condat’s direct algorithm. As one can
observe, in 14 out of 20 cases, our algorithm outperforms Condat’s direct algorithm
by a factor of about 1-2 in terms of computation times. These experiments indicate

DP for GNIO

23

n

parameters pattern

tDP

tGurobi

tGurobi/tDP

1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7
1e4
1e5
1e6
1e7

Isotonic
Isotonic
Isotonic
Isotonic
Nearly-isotonic
Nearly-isotonic
Nearly-isotonic
Nearly-isotonic
Unimodal
Unimodal
Unimodal
Unimodal
Fused
Fused
Fused
Fused
Uniform
Uniform
Uniform
Uniform
Gaussian
Gaussian
Gaussian
Gaussian
Mixed
Mixed
Mixed
Mixed

0.000
0.002
0.020
0.206
0.000
0.003
0.031
0.311
0.000
0.002
0.019
0.197
0.000
0.003
0.032
0.319
0.000
0.003
0.033
0.334
0.000
0.003
0.031
0.309
0.000
0.002
0.020
0.198

0.09
1.78
21.91
*
0.09
1.43
14.69
*
0.08
2.16
31.04
*
0.12
3.25
35.63
*
0.15
4.20
43.07
*
0.15
3.89
38.49
*
0.15
5.46
102.44
*

> 90
890.00
1095.50
*
> 90
476.67
473.87
*
> 80
1080.00
1633.68
*
> 120
1083.33
1113.44
*
> 150
1400.00
1305.16
*
> 150
1296.67
1241.61
*
> 150
2730.00
5122.00
*

Table 5 Runtime (in seconds) comparisons between our dynamic programming Algorithm
1 and Gurobi for solving problem (25) under diﬀerent settings. Results are averaged over 10
simulations. The entry “*” indicates that Gurobi reports “out of memory”.

that for the fused lasso problem, our algorithm is highly competitive even when
compared with the existing fastest algorithm in the literature.

6 Conclusions

In this paper, we studied the generalized nearly isotonic optimization problems.
The intrinsic recursion structure in these problems inspires us to solve them via
the dynamic programming approach. By leveraging on the special structures of the
generalized nearly isotonic regularizer in the objectives, we are able to show the
computational feasibility and eﬃciency of the recursive minimization steps in the
dynamic programming algorithm. Speciﬁcally, easy-to-implement and explicit up-
dating formulas are derived for these steps. Implementation details, together with
the optimal O(n) running time analysis, of our algorithm for solving (cid:96)2-GNIO
problems are provided. Building upon all the aforementioned desirable results,
our dynamic programming algorithm has demonstrated a clear computational ad-
vantage in solving large-scale (cid:96)1-GNIO and (cid:96)2-GNIO problems in the numerical
experiments when tested against the powerful commercial linear and quadratic
programming solver Gurobi and other existing fast algorithms.

24

Zhensheng Yu et al.

problem

n

parameters pattern

tDP

tGurobi

tGurobi/tDP

tDP

tGurobi

tGurobi/tDP

(cid:96)1-GNIO

(cid:96)2-GNIO

sugar

923025

gold

1097955

aep

121273

ni

58450

Isotonic
Nearly-isotonic
Unimodal
Fused
Uniform
Gaussian
Mixed
Isotonic
Nearly-isotonic
Unimodal
Fused
Uniform
Gaussian
Mixed
Isotonic
Nearly-isotonic
Unimodal
Fused
Uniform
Gaussian
Mixed
Isotonic
Nearly-isotonic
Unimodal
Fused
Uniform
Gaussian
Mixed

0.512
0.271
0.291
0.426
0.162
0.214
0.164
0.203
0.234
0.235
0.198
0.067
0.102
0.054
0.037
0.038
0.040
0.037
0.021
0.037
0.020
0.018
0.018
0.019
0.018
0.011
0.017
0.010

989.37
168.69
470.56
410.98
661.20
799.23
702.13
772.91
187.62
19593.87
384.32
586.71
584.66
782.46
44.77
6.61
31.57
19.15
18.47
29.02
14.74
14.55
2.76
10.47
5.14
8.21
10.36
6.64

1932.36
622.47
1617.04
964.74
4081.48
3734.72
4281.28
3807.44
801.79
83378.20
1941.01
8756.87
5731.96
14490.00
1210.01
173.95
789.25
517.56
879.52
784.32
737.00
808.33
153.33
551.05
285.56
746.36
609.41
6640.00

0.019
0.018
0.024
0.027
0.031
0.028
0.017
0.021
0.029
0.021
0.027
0.039
0.035
0.021
0.002
0.003
0.002
0.003
0.003
0.001
0.002
0.002
0.001
0.001
0.001
0.001
0.001
0.001

39.81
20.11
39.32
34.74
44.51
50.09
312.25
38.16
22.78
40.13
71.01
51.66
75.07
124.62
5.12
2.32
4.10
4.84
5.05
4.96
35.50
1.78
1.17
1.58
2.21
5.36
2.31
17.59

2095.26
1117.22
1638.33
1286.67
1435.81
1788.93
18367.64
1817.14
785.52
1910.95
2630.01
1324.62
2144.86
5934.28
2560
733.33
2050.00
1613.33
1683.33
4960
17750.00
895.00
1170.00
1580.00
2210.00
2680.00
2310.00
17590.00

Table 6 Runtime (in seconds) comparisons between our dynamic programming Algorithm 1
and Gurobi for solving (cid:96)1-GNIO problem (24) and (cid:96)2-GNIO problem (25) with real data.

Acknowledgements The authors would like to thank the Associate Editor and anonymous
referees for their helpful suggestions.

7 Appendix

7.1 O(n log n) complexity of Algorithm 1 for (cid:96)1-GNIO

As is mentioned in Remark 2, when Algorithm 1 is applied to solve (cid:96)1-GNIO,
special data structures are needed to obtain a low-complexity implementation.
Here, we propose to use a search tree to reduce the computation costs. The desired
search tree should have following methods:

– insert: adds a value into the tree while maintains the structure;
– popmax : deletes and returns maximal value in the tree;
– popmin: deletes and returns minimal value in the tree.

These methods of the search tree are assumed to take O(log n) time. We note
that these requirements are not restrictive at all. In fact, the well-known red-black
trees [11] satisfy all the mentioned properties. Now, we are able to sketch a proof
for the O(n log n) complexity of Algorithm 1 for solving the (cid:96)1-GNIO. Using the
desired search tree, in the “sum” step, the cost of insert a new breakpoint yi into
the tree is O(log n). Note that at most n breakpoints will be inserted. Hence, the
overall running time of the “sum” step is O(n log n). We also note that, in the
“update” step, breakpoints will be deleted one by one via the popmax or popmin

DP for GNIO

25

problem

n

sugar

923025

gold

1097955

aep

121273

ni

58450

random1

106

random2

107

λ

1
2
5
10
100
1
2
5
10
100
1
2
5
10
100
1
2
5
10
100
1
2
5
10
100
1
2
5
10
100

tDP

tcondat

tcondat/tDP

0.030
0.030
0.029
0.028
0.026
0.030
0.029
0.028
0.028
0.026
0.003
0.003
0.003
0.003
0.003
0.001
0.001
0.001
0.001
0.001
0.032
0.031
0.031
0.030
0.026
0.286
0.279
0.303
0.299
0.296

0.038
0.034
0.031
0.028
0.022
0.028
0.025
0.023
0.022
0.019
0.006
0.005
0.005
0.005
0.005
0.002
0.002
0.002
0.002
0.002
0.042
0.042
0.041
0.041
0.035
0.370
0.375
0.361
0.362
0.282

1.27
1.13
1.07
1.00
0.84
0.93
0.86
0.82
0.79
0.73
2.00
1.67
1.67
1.67
1.67
2.00
2.00
2.00
2.00
2.00
1.31
1.35
1.32
1.37
1.35
1.29
1.34
1.19
1.21
0.95

Table 7 Runtime (in seconds) comparisons between our dynamic programming Algorithm 1
and Condat’s algorithm [12] for solving (cid:96)2-total variation problem (3) with simulated and real
data sets.

operations. Recall that the total number of the deleted breakpoints in Algorithm
1 is upper bounded by 2n (see the discussions in Section 4). Therefore, the overall
running time of the “update” step is O(n log n). Hence, the total running time of
Algorithm 1 for solving (cid:96)1-GNIO is O(n log n).

References

1. R. K. Ahuja and J. B. Orlin, A fast scaling algorithm for minimizing separable convex

functions subject to chain constraints, Operations Research, 49, 784-789 (2001)

2. M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman, An empirical distribu-
tion function for sampling with incomplete information, Annals of Mathematical Statistics,
26, 641-647 (1955)

3. ´A. Barbero and S. Sra, Modular proximal optimization for multidimensional total-variation

regularization, Journal of Machine Learning Research, 19, 2232-2313 (2018)

4. D. J. Bartholomew, A test of homogeneity for ordered alternatives, Biometrika, 46, 36-48

(1959)

5. D. J. Bartholomew, A test of homogeneity for ordered alternatives II, Biometrika, 46, 328-

335 (1959)

26

Zhensheng Yu et al.

6. R. E. Barlow, D. J. Bartholomew, J. M. Bremner, and H. D. Brunk, Statistical Inference
under Order Restrictions: The Theory and Application of Isotonic Regression, Wiley, New
York (1972)

7. M. J. Best and N. Chakravarti, Active set algorithms for isotonic regression; A unifying

framework, Mathematical Programming, 47, 425-439 (1990)

8. M. J. Best, N. Chakravarti, and V. A. Ubhaya, Minimizing separable convex functions
subject to simple chain constraints, SIAM Journal on Optimization, 10, 658-672 (2000)
9. H. D. Brunk, Maximum likelihood estimates of monotone parameters, Annals of Mathe-

matical Statistics, 26, 607-616 (1955)

10. X. Chang, Y. Yu, Y. Yang, and E. P. Xing, Semantic pooling for complex event analysis
in untrimmed videos, IEEE Transactions on Pattern Analysis and Machine Intelligence, 39,
1617-1732 (2016)

11. T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms,

MIT Press (2009)

12. L. Condat, A direct algorithm for 1D total variation denoising, IEEE Signal Processing

Letters, 20, 1054-1057 (2013)

13. D. L. Donoho and J. M. Johnstone,

Ideal spatial adaptation by wavelet shrinkage,

Biometrika, 81, 425-455 (1994)

14. D. L. Donoho, De-noising by soft-thresholding, IEEE Transactions on Information Theory,

41, 613-627 (1995)

15. J. Friedman, T. Hastie, H. H¨oﬂing, and R. Tibshirani, Pathwise coordinate optimization,

The Annals of Applied Statistics, 1, 302-332 (2007)

16. M. Frisen, Unimodal regression, The Statistician, 35, 479-485 (1986)
17. J.-B. Hiriart-Urruty and C. Lemar´echal, Fundamentals of Convex Analysis, Springer Sci-

ence & Business Media (2004)

18. D. S. Hochbaum and C. Lu, A faster algorithm solving a generalization of isotonic median
regression and a class of fused lasso problems, SIAM Journal on Optimization, 27, 2563-2596
(2017)

19. D. S. Hochbaum, An eﬃcient algorithm for image segmentation, markov random ﬁelds

and related problems, Journal of the ACM, 48, 686-701 (2001)

20. C. Lu and D. S. Hochbaum, A uniﬁed approach for a 1D generalized total variation

problem, Mathematical Programming (2021)

21. H. H¨oeﬂing, A path algorithm for the fused lasso signal approximator, Journal of Compu-

tational and Graphical Statistics, 19, 984-1006 (2010)

22. N. A. Johnson, A dynamic programming algorithm for the fused lasso and l0-segmentation,

Journal of Computational and Graphical Statistics, 22, 246-260 (2013)

23. JoinQuant dataset, Continuous sugar price index ‘SR8888.XZCE’ and continuous gold

price index ‘AU8888.XSGE’, https://www.joinquant.com/data

24. I. Matyasovszky, Estimating red noise spectra of climatological time series, Quarterly

Journal of the Hungarian Meteorological Service, 117, 187-200 (2013)

25. R. Mulla, Over 10 years of hourly energy consumption data from PJM in Megawatts,
https://www.kaggle.com/robikscube/hourly-energy-consumption?select=AEP_hourly.
csv, Version 3

26. A. Restrepo and A. C. Bovik, Locally monotonic regression, IEEE Transactions on Signal

Processing, 41, 2796-2810 (1993)

27. R. T. Rockafellar, Convex Analysis, Princeton University Press, Princeton, NJ (1970)
28. G. Rote, Isotonic regression by dynamic programming, in 2nd Symposium on Simplicity

in Algorithms (2019)

29. Y. U. Ryu, R. Chandrasekaran, and V. Jacob, Prognosis using an isotonic prediction

technique, Management Science, 50, 777-785 (2004)

30. M. J. Silvapulle and P. K. Sen, Constrained Statistical Inference: Inequality, Order and

Shape Restrictions, John Wiley & Sons (2005)

31. Q. F. Stout, Unimodal regression via preﬁx isotonic regression, Computational Statistics

& Data Analysis, 53, 289-297 (2008)

32. Q. F. Stout, Fastest known isotonic regression algorithms, https://web.eecs.umich.edu/

~qstout/IsoRegAlg.pdf (2019)

33. U. Str¨omberg, An algorithm for isotonic regression with arbitrary convex distance function,

Computational Statistics & Data Analysis, 11, 205-219 (1991)

34. R. Tibshirani, H. H¨oeﬂing, and R. Tibshirani, Nearly-isotonic regression, Technometrics,

53, 54-61 (2011)

DP for GNIO

27

35. R. E. Tarjan, Amortized computational complexity, SIAM Journal on Algebraic Discrete

Methods, 6, 306-318 (1985)

36. C. Wu, J. Thai, S. Yadlowsky, A. Pozdnoukhov, and A. Bayen, Cellpath: Fusion of cellular
and traﬃc sensor data for route ﬂow estimation via convex optimization, Transportation
Research Part C: Emerging Technologies, 59, 111-128 (2015)

37. Y.-L. Yu and E. P. Xing, Exact algorithms for isotonic regression and related, Journal of

Physics: Conference Series 699 (2016)

