1
2
0
2

c
e
D
6
1

]

G
L
.
s
c
[

2
v
0
8
1
2
0
.
1
0
1
2
:
v
i
X
r
a

MAXIMUM A POSTERIORI INFERENCE OF RANDOM DOT
PRODUCT GRAPHS VIA CONIC PROGRAMMING∗

DAVID WU† , DAVID PALMER ‡ , AND DARYL DEFORD §

Abstract. We present a convex cone program to infer the latent probability matrix of a random
dot product graph (RDPG). The optimization problem maximizes the Bernoulli maximum likelihood
function with an added nuclear norm regularization term. The dual problem has a particularly nice
form, related to the well-known semideﬁnite program relaxation of the maxcut problem. Using the
primal-dual optimality conditions, we bound the entries and rank of the primal and dual solutions.
Furthermore, we bound the optimal objective value and prove asymptotic consistency of the proba-
bility estimates of a slightly modiﬁed model under mild technical assumptions. Our experiments on
synthetic RDPGs not only recover natural clusters, but also reveal the underlying low-dimensional
geometry of the original data. We also demonstrate that the method recovers latent structure in the
Karate Club Graph and synthetic U.S. Senate vote graphs and is scalable to graphs with up to a few
hundred nodes.

Key words. random dot product graph, maximum a posteriori, Bayesian inference, regulariza-
tion, maximum likelihood estimation, inference, conic programming, convex relaxation, clustering,
graph embedding, latent vectors, low rank, consistency

AMS subject classiﬁcations. 62F15, 90C35, 65F55, 68R10, 15B48

1. Introduction. Real-world networks appearing in neuroscience, sociology, and
machine learning encode an enormous amount of structure in a combinatorial form
that is challenging to analyze. A promising analytical approach is to make this struc-
ture manifest as geometry. Generative random graph models oﬀer a statistically
rigorous way to formulate these problems.

Among the simplest random graph models is the well-known Erd¨os-R´enyi graph
G(n, p), a random graph on n nodes with independent edge probabilities all equal to
p. Because all vertices look the same in this model, it is too weak to explain common
latent structures that appear in real graphs, such as clusters.

It is natural to hypothesize that such structures appear due to underlying aﬃnities
between nodes. For example, nodes in a social network represent people who might
cluster together based on their shared interests. This hypothesis suggests a natural
geometric random graph model in which nodes have latent vectors of characteristics,
and edge probabilities come from the inner products of these vectors. This model,
ﬁrst studied by Young and Scheinerman [44], is called the random dot product graph
(RDPG). In fact, many random graph models, including the Erd¨os-R´enyi graph and
the stochastic block model (SBM) [1] can be viewed as special cases of the RDPG
model.

In this paper, we study the problem of recovering the latent vectors from a graph
under the RDPG assumption. While the asymptotic behavior of the RDPG model
has been analyzed carefully, we show that with an appropriate prior, the maximum
a posteriori (MAP) problem can be formulated as a convex cone program, featuring
a single semideﬁnite cone constraint coupled to many exponential cone constraints.

∗Submitted to the editors January 6, 2021.
Funding:
†Departments of Math and Computer Science, Massachusetts

Institute of Technology

(dxwu@mit.edu).

‡Geometric Data Processing Group, Massachusetts Institute of Technology (drp@mit.edu).
§Department

Statistics, Washington

of Mathematics

State

and

University

(daryl.deford@wsu.edu).

1

 
 
 
 
 
 
2

D. WU, D. PALMER, AND D. DEFORD

This problem is easy to formulate, and it is solvable in polynomial time via commodity
conic solvers [4, 29]. Moreover, we show that under mild sparsity and nondegeneracy
assumptions the solution for a closely related model is asymptotically consistent, and
that in practice, our method can reveal a variety of latent geometry in RDPGs. In
doing so, we take ﬁrst steps towards connecting the subﬁelds of latent vector graph
models and conic programming.

1.1. Preliminaries. Let G = (V, E) be a simple undirected unweighted graph
with n = |V | nodes and m = |E| edges. For distinct i and j, we write i ∼ j if
(i, j) ∈ E, and i (cid:54)∼ j if (i, j) (cid:54)∈ E.
In the RDPG model, every node i ∈ V in
the graph comes with a latent vector Xi ∈ Rd, and edge probabilities are given by
the inner products of these latent vectors, P[i ∼ j] = X (cid:62)
i Xj. We assemble these
edge probabilities into the edge probability matrix P . It is important to note that
diagonal elements of P do not have any probabilistic interpretation since our graphs
are assumed to be simple. Conditioned on P , an RDPG with adjacency matrix A
has entries Aij that are independent Bernoulli random variables with corresponding
parameters Pij. That is, we have

P[A|P ] =

(cid:89)

i<j

P Aij
ij

(1 − Pij)1−Aij .

An implicit assumption of the RDPG model is that the embedding satisﬁes the
constraints 0 ≤ Pij ≤ 1 for all i (cid:54)= j. One can either view the node embeddings
as ﬁxed or themselves drawn from a latent vector distribution. Although most prior
[28]
works use the linear dot product structure, the recent work of O’Connor et al.
forms the edge probability matrix from the latent vectors using a more general link
function κ(Rd, Rd) → [0, 1] such as the logistic sigmoid function.

The primary problem of interest is the inference of either the probability matrix
P or the latent vectors X := (cid:0)X (cid:62)
∈ Rn×d from one sample of A. Note
that for a ﬁxed P , the latent embedding X is only unique up to a d-dimensional
orthogonal transformation. However, it is still possible to extract an embedding from
P via a Cholesky decomposition or eigendecomposition.

. . . X (cid:62)
n

(cid:1)(cid:62)

1

1.2. Maximum Likelihood. In this paper we solve an MAP problem with
conic programming to recover the probability matrix and latent vectors of an RDPG.
In this section, we ﬁrst formulate the maximum likelihood problem, and then we show
that regularization is necessary to recover a nontrivial solution.

Observe that P = XX (cid:62) is positive semideﬁnite (PSD) by construction. Con-
versely, if P is PSD then it can be decomposed as XX (cid:62) for some X ∈ Rn×k where
k ≤ n. Given an adjacency matrix A, this leads to the following maximum likelihood
problem (MLE):

(MLE)

max
P

(cid:88)

i∼j

log Pij +

(cid:88)

i(cid:54)∼j

subject to:

log(1 − Pij)

0 ≤ Pij ≤ 1
P (cid:23) 0,

for all i (cid:54)= j

where the notation P (cid:23) 0 means that P is PSD. It is important to note that the diag-
onal entries of P are unconstrained. This makes (MLE) highly underconstrained—as
we show below, the solution reproduces A with nonzero diagonal entries to ensure
semideﬁniteness.

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

3

Proposition 1.1. There exists an optimal solution P ∗ of (MLE) whose oﬀ-

diagonal entries coincide with those of A.

Proof. For feasible Pij, we have log Pij ≤ 0, with equality achieved at Pij = 1.
Similarly, log(1 − Pij) ≤ 0, with equality achieved at Pij = 0. Thus, the objective
function can be maximized by setting Pij = 1 if i ∼ j and Pij = 0 if i (cid:54)∼ j, so long
as the semideﬁniteness constraint is satisﬁed. As the diagonal of P is unconstrained,
we can ensure P ∗ (cid:23) 0 by shifting the diagonal. In particular, P ∗ = A + λI (cid:23) 0 for
suﬃciently large λ.

1.3. Regularization and MAP estimate. In view of Proposition 1.1, it is
prudent to regularize (MLE) to obtain a nontrivial solution. We do so with the hope
of encouraging low rank solutions, which are of particular theoretical and practical
interest. For example, low rank solutions are easier to visualize. In addition, low-rank
solutions to semideﬁnite optimization problems are often more eﬃcient to compute
via specialized algorithms that take advantage of the low rank, such as the B¨urer-
Monteiro method and extensions [10, 8, 14]. One might hope that such low-rank
methods would extend to the optimization problems featured in this paper.

With the goal of a low-rank estimate in mind, we choose a prior that encourages
the vector of eigenvalues of the probability matrix to be sparse. One such prior is the
Laplace distribution on tr P . This aligns with a common heuristic for encouraging
low rank solutions, which penalizes the nuclear norm (cid:107)P (cid:107)∗ := (cid:80)
i σi, where σi are
the singular values of P [33, 18]. Some justiﬁcation for the heuristic can be provided
by noting that the nuclear norm is the convex envelope of the rank function, i.e.
rank P ≥ (cid:107)P (cid:107)∗
(cid:107)P (cid:107) , where (cid:107)P (cid:107) denotes the operator norm of P . The nuclear norm of a
PSD matrix is just its trace. This leads to the following reﬁnement (REG) of (MLE),
which can be interpreted as an MAP estimation problem under the aforementioned
Laplace prior, parametrized by a real scalar C:

(REG)

max
P

(cid:88)

i∼j

log Pij +

(cid:88)

i(cid:54)∼j

log(1 − Pij) − C tr P

subject to:

0 ≤ Pij ≤ 1
P (cid:23) 0

for all i (cid:54)= j

As with any inference technique, the MAP estimate P ∗ varies with the observed
realization A; given a nondegenerate true edge probability matrix P , any simple
undirected graph is realizable, including the empty graph. As such, we cannot expect
to prove deterministic bounds on inference performance; probabilistic results are more
reasonable.

The constraints of (REG) are of semideﬁnite program (SDP) type, but the objec-
tive function is nonlinear. The problem can be rewritten as a convex cone program.
Introducing new variables αij ≤ log Pij for i (cid:54)= j and βij ≤ log(1 − Pij) for i (cid:54)= j,
and noting that at optimality these inequalities turn into equalities, we can express
the original objective function in a manifestly convex form. We can encode these
inequalities with exponential cones. Conveniently, including these inequalities for all
i (cid:54)= j also automatically enforces the inequality constraints 0 ≤ Pij ≤ 1. We state the
deﬁnitions of the primal and dual exponential cones below for reference [4].

Definition 1.2 (exponential cones). The primal exponential cone is the set

Kexp := {(x, y, z) ∈ R3 : x ≥ y exp(z/y), y > 0} ∪ {(x, 0, z) : x ≥ 0, z ≤ 0}.

4

D. WU, D. PALMER, AND D. DEFORD

The dual exponential cone is its conic dual

K ∗

exp := {(u, v, w) ∈ R3 : u ≥ −w exp(v/w − 1), u > 0, w < 0} ∪ {(u, v, 0) : u, v ≥ 0}.

Applying these transformations, we obtain the following convex cone program,

equivalent to (REG):

max

(cid:88)

i∼j

αij +

(cid:88)

i(cid:54)∼j

βij − C tr P

(CREG)

subject to:

P (cid:23) 0
(Pij, 1, αij) ∈ Kexp
(1 − Pij, 1, βij) ∈ Kexp

for all i (cid:54)= j

for all i (cid:54)= j

The cone program (CREG) is the main subject of study in this paper. Moreover, it
can be easily implemented with convex optimization interfaces such as CVXPY [20, 2]
that rely on solvers that support both semideﬁnite and exponential cone programming
such as SCS [29] and MOSEK [4]. As we will see later, the dual conic program
turns out to have an appealing interpretation. Finally, we remark that given the
binary matrix P ∗ of Proposition 1.1, one approach (quite diﬀerent from ours) that is
particularly suited for clustering tasks is to attempt to factor P ∗ into the product of
two low-rank binary matrices [45]. However, since binary edge probability matrices
are degenerate from the generative perspective, our cone program is more appropriate
for the RDPG model. Furthermore, our method recovers richer geometrical structure
than simple clusters.

1.4. Related work. Prior work on RDPG latent vector recovery has focused
on spectral methods such as the adjacency spectral embedding (ASE) and Laplacian
spectral embedding (LSE); see [6] for a comprehensive review. Assuming the latent
vectors reside in Rd, the ASE entails taking the top-d eigenvector approximation of the
adjacency matrix A. It is known that this provides a consistent estimate of the latent
vectors, up to an orthogonal transformation [38]. Furthermore, asymptotic normality
results exist for the ASE [7] and LSE [37]. In addition to these theoretical results,
these spectral techniques have been applied to graph machine learning, including
vertex nomination [43, 3], vertex embedding and classiﬁcation [36, 12], and joint
embedding [40] problems.

One limitation of the ASE is the requirement that the embedding dimension d is
known in advance. In order to address these limitations, Yang et al. [42] have recently
proposed a framework for simultaneously discovering the rank and number of clusters
for SBMs using an extended ASE and the Bayesian information criterion. We instead
propose a cone program with an adjustable regularization hyperparameter, so that
our method does not require a priori knowledge of d. Empirically, we show that our
method (which applies for general RDPGs) leads to a cross validation procedure for
rank discovery. Furthermore, our method outperforms the vanilla ASE on clustering
tasks and spectral norm distance, even when the true d is known.

Maximum likelihood (ML) approaches to recovering matrix structure or solving
graph problems are common in the literature. Arroyo et al.
[5] studied the graph
matching problem in corrupted graphs via ML estimators. For the speciﬁc problem
of latent vector recovery, Choi et al.
[13] studied ML estimation for the SBM and
showed that the when the number of blocks is O(n1/2), the fraction of misclassiﬁed
nodes converges in probability to 0. For latent vector graphs with a logistic sigmoid

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

5

link function, O’Connor et al.
[28] give a spectral approximation algorithm that is
asymptotically equivalent to ML latent vector recovery, but do not directly study
consistency results. Our cone program does not directly depend on any spectral
algorithms, but we are still able to study consistency and likelihood properties.

This work joins a body of literature that applies convex relaxation methods to
broader statistical inference problems, such as Logistic PCA [15, 23], angular syn-
chronization [35], and compressed sensing [17, 11]. In fact, semideﬁnite relaxations,
which are special cases of cone programs, have been used to study SBM inference
[1]. The convex relaxation method has also been fruitfully applied to low rank matrix
recovery, by relaxing the nonconvex rank function to a nuclear norm or max norm
[17, 11]. Standard statistical learning theory arguments are used to bound the gener-
alization error. Equipped with our cone program relaxation, we demonstrate a novel
application of tools from conic programming to the problem of RDPG inference.

Our proof technique for consistency bounds takes inspiration from the matrix
completion literature, but some care must be taken because of the failure of a Lipschitz-
like criterion. Our approach also adds a trace penalty term rather than restricting
the feasible set, which leads to looser requirements for the true P when formulating
consistency results.

1.5. Main contributions. We summarize our main contributions below. We
lay down theoretical foundations for the RDPG inference problem using a convex
cone programming approach. We believe that the inference and conic optimization
perspective oﬀers insights that have not been leveraged in the study of these latent
vector models. Our analysis, which centers around the study of our cone program’s
optimality conditions, yields interesting theoretical properties of the inference prob-
lem.

After taking the dual in section 2, we use the primal-dual optimality conditions
to derive an explicit relationship between the primal and dual optimal solutions P ∗
and Q∗ (see Propositions 2.4 and 3.1). Based on our bounds on the entries of P ∗
and Q∗, we attempt to control their ranks. Unfortunately, we demonstrate a coun-
terexample and a fundamental barrier to using our proof technique to prove that
rank P ∗ decreases arbitrarily as C → ∞ (see Proposition 3.4). On the other hand, we
see in subsections 3.2 and 3.3 that our optimality bounds lead to asymptotic likeli-
hood bounds and consistency bounds for a slightly modiﬁed model (see Theorems 3.8
and 3.11, respectively).

In section 4, we show empirical evidence that our inference approach indeed re-
covers ﬁne-grained latent vector geometry. Moreover, our algorithm’s single regular-
ization hyperparameter enables the use of simple cross-validation and a parameter
sweep to discover the rank of P ∗. Our cross-validation experiments provide strong
evidence for uniqueness of the optimal regularization parameter and rank. Discussion
and conclusions follow in section 5.

2. Dual cone program. In order to analyze the optimal solutions of (CREG),
it is fruitful to analyze its dual program. Proofs of the following propositions are
provided in Appendix A. As a ﬁrst step, we verify Slater’s condition, which is suﬃcient
for strong duality [9]:

Lemma 2.1. The feasible region of (CREG) has an interior point.
Proof. Set αij = βij = −1 for all i (cid:54)= j, Pij = 1

For suﬃciently large λ, P is positive deﬁnite and furthermore −1 < log 1
condition holds.

2 for all i (cid:54)= j and diag P = λ.
2 , so Slater’s

6

D. WU, D. PALMER, AND D. DEFORD

Proposition 2.2. The following problem is the strong dual to (CREG):

min

(cid:88)

i∼j

sij +

(cid:88)

i(cid:54)∼j

vij +

(cid:88)

i(cid:54)=j

uij

subject to:

(DREG)

sij ≥ −1 − log rij
vij ≥ −1 − log uij
uij − rij = Qij
uij, rij ≥ 0
diag Q = C

Q (cid:23) 0

for all i ∼ j

for all i (cid:54)∼ j

for all i (cid:54)= j

for all i (cid:54)= j

ij, β∗

At optimality, (CREG) and (DREG) satisfy the following KKT conditions [9]. Let
(P ∗, α∗
ij) be a pair of optimal primal and dual solutions to
(CREG) and (DREG), where λ∗
ij). Then for
all i and j we have

ij) and (Q∗, λ∗

ij) and ν∗

ij = (u∗

ij = (r∗

ij, w∗

ij, ν∗

ij, v∗

ij, s∗

ij, t∗

(2.1)

(2.2)

(2.3)

0 = P ∗Q∗
0 = λ∗
0 = ν∗

ij · (P ∗
ij · (1 − P ∗

ij, 1, α∗
ij)
ij, 1, β∗

= r∗
ij) = u∗

ijP ∗
ij + s∗
ij(1 − P ∗

ij + t∗
ij) + v∗

ijα∗
ij
ij + w∗

ijβ∗
ij.

We can further reduce the dual as follows:

Lemma 2.3. The following relations hold at optimality:
ij ≤ −1, then u∗
1. If i ∼ j and Q∗
ij > −1, then u∗
2. If i ∼ j and Q∗
ij < 1, then u∗
3. If i (cid:54)∼ j and Q∗
ij ≥ 1, then u∗
4. If i (cid:54)∼ j and Q∗

ij = 0 and r∗
ij = Q∗
ij = 1 and r∗
ij = Q∗

ij = 1.
ij = 1 − Q∗
ij.
ij = 0.

ij + 1 and r∗

ij = −Qij.

ij and r∗

Imposing these relations explicitly, we arrive at the following reduced formulation,
which agrees with (DREG) at optimality:
(DCREG)

min

Qij +

(−1 − log(−Qij)) +

(Qij − log Qij − 1)

(cid:88)

(cid:88)

(cid:88)

i∼j
Qij ≥−1

i∼j
Qij ≤−1

i(cid:54)∼j
Qij ≥1

subject to: diag Q = C

Q (cid:23) 0

Using the complementary slackness conditions for the exponential cones, we ob-

tain the following explicit relationships between P ∗ and Q∗.

Proposition 2.4. Let P ∗ and Q∗ be corresponding optimal solutions to (CREG)
and (DCREG), respectively. We have the following relationships between the oﬀ-
diagonal entries of P ∗ and Q∗:
1. If i ∼ j and Q∗
2. If i ∼ j and Q∗
3. If i (cid:54)∼ j and Q∗
4. If i (cid:54)∼ j and Q∗

ij ≤ −1, then Q∗
ij > −1, then P ∗
ij < 1, then P ∗
ij ≥ 1, then Q∗

ij = − 1
P ∗
ij
ij = 1.

ij = 0.
ij = 1

.

.

1−P ∗
ij

Proof. Suppose i ∼ j and Q∗

ij < −1. The KKT conditions (2.2) and (2.3) together

with Lemma 2.3 imply that

(2.4)

ijQ∗
P ∗

ij + 1 + log(−Q∗

ij) + α∗

ij = 0.

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

7

Recall from the deﬁnition of (CREG) that αij ≤ log Pij; at optimality this inequality
becomes an equality. Applying Lemma A.1, we see that (2.4) has the unique solution
ij = −1, so Q∗
ijQ∗
P ∗
ij ≥ −1, then the
KKT conditions yield P ∗
ij = 1.

ij + 1. Thus Lemma A.1 also yields P ∗

as desired. On the other hand, if Q∗

ij = log P ∗
The other two statements for i (cid:54)∼ j follow similarly.

ij = − 1
P ∗
ij

These explicit relationships between P ∗ and Q∗ lead to bounds on the entries of P ∗,
including the following corollary.

Corollary 2.5. Suppose C ≥ 1. If i ∼ j, then P ∗

ij ≥ 1

C , and if i (cid:54)∼ j, then

ij ≤ 1 − 1
P ∗
C .

Proof. Since Q (cid:23) 0 and diag Q = C, it follows that |Q∗

ij| ≤ C. The conclusions

then follow upon applying Proposition 2.4.

Note that Q can be decomposed as Y Y (cid:62), where Y ∈ Rn×k. Thus the rows of Y
C, and Qij is the dot product between the ith
lie on the (k − 1)-sphere with radius
and jth rows of Y . By Proposition 2.4, it follows that if i ∼ j, the indices (i, j) where
P ∗
ij < 1 correspond to the indices where Q∗
ij ≤ −1. Similarly, if i (cid:54)∼ j, the indices
(i, j) where P ∗

ij > 0 correspond to the indices where Q∗

ij ≥ 1.

√

2.1. Connection to MAXCUT. Inspecting (DCREG) and noting that |Q∗
ij| ≤
C, it follows for C ≤ 1 that we have the following problem with an objective that is
homogeneous in C:

(2.5)

min
Q

(cid:88)

i∼j

Qij

subject to:

diag Q = C ≤ 1

Q (cid:23) 0

In fact, this is an instance of the SDP relaxation of maxcut with weight matrix equal
to the adjacency matrix [22]. This suggests that (CREG) has deeper connections to
classical combinatorial graph structure recovery problems. This also implies that the
oﬀ-diagonal entries of the optimal P ∗ do not change in the region C ≤ 1. In particular,
we recover A as in Proposition 1.1.

3. Main results. We now derive our major results on the behavior of opti-
mal solutions to (CREG) and (DREG). Naturally, an optimal solution to the MAP
problem will still tend towards larger probabilities where there are edges and smaller
probabilities where there are nonedges. However, the penalty term also constrains
the norms of the individual latent vectors.

These bounds on the inferred probability matrix P ∗ allow us to further study
properties of the inference problem, such as consistency, in subsection 3.3. Perhaps
unsurprisingly, our bounds depend on node degrees and the regularization parameter
C. In fact, we show that P ∗
ij = Θ(1/C) for all i and j, so the probabilities and latent
vector lengths can be made arbitrarily small for suﬃciently large C. These bounds
follow solely from the complementary slackness conditions and basic facts about PSD
matrices.

Proposition 3.1. Suppose that G = (V, E), and write di := deg(i) for all i ∈ V .

8

D. WU, D. PALMER, AND D. DEFORD

Then the following bounds hold.

(3.1)

(3.2)

(3.3)

1
C minj dj

≤ P ∗

di
ii ≤
C
(cid:112)didj
C

P ∗

ij ≤

∀i

∀i (cid:54)= j

Q∗

ij ≤ min(di, dj) − 1 ∀i (cid:54)= j

Additionally, if C ≥ maxi di, we have:

(3.4)

(3.5)

Q∗

ij ≤

P ∗

ii ≥

C
C − (cid:112)didj
di
(cid:88)
C

−

i(cid:54)∼j

(cid:112)didj
C(C − (cid:112)didj)

∀i (cid:54)∼ j

∀i

Proof. The complementary slackness condition P ∗Q∗ = 0 (see Proposition 2.2)
furnishes us with n2 equations. Focusing on the diagonal entries and applying Propo-
sition 2.4 provides crude bounds on the entries of P ∗ and Q∗.
In particular, from diag(P ∗Q∗) = 0, we obtain for each i

(3.6)

CP ∗

ii +

(cid:88)

i∼j

max(−1, Q∗

ij) +

P ∗
ij
1 − P ∗
ij

(cid:88)

i(cid:54)∼j

= 0.

P ∗
ij
1−P ∗
ij

Since

≥ 0, and the sum over i ∼ j has di terms, we have P ∗

C , which proves
the second inequality of (3.1). Since the ﬁrst and third terms of (3.6) are nonnegative,
the sum over i ∼ j must be nonpositive, so Q∗
ij ≤ di − 1. (3.3) follows by symmetry.

ii ≤ di

In a PSD matrix, every 2 × 2 submatrix is PSD, so we have

(cid:113)

P ∗

ij ≤

iiP ∗
P ∗

jj ≤

(cid:112)didj
C

,

which is (3.2). On the other hand, we have

P ∗

ii ≥ max

j

(P ∗
ij)2
P ∗
jj

≥ max

j

C(P ∗
ij)2
dj

≥

1
C minj dj

,

where the last inequality follows from Corollary 2.5. Hence the ﬁrst inequality of (3.1)
holds.

Now suppose C ≥ max di. For (3.4), note that for i (cid:54)∼ j we have Q∗

by Proposition 2.4. Because C ≥ max di, P ∗

ij ≤ 1 by (3.2). Hence Q∗

ij ≤

ij ≤ 1
C
√

1−P ∗
ij
as

C−

didj

desired. Finally, substituting (3.2) into (3.6) and noting again that the denominator
in (3.6) is nonnegative, we have

0 ≤ CP ∗

ii − di +

(cid:112)didj
C − (cid:112)didj

,

(cid:88)

i(cid:54)∼j

and rearranging yields (3.5).

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

9

A couple points are worth noting.

If node i is isolated, i.e. di = 0, then the
learned latent vector is the zero vector. More generally, the maximum size of Pij is
controlled by the connectivity of nodes i and j. Finally, we immediately obtain the
following corollary characterizing the trace of P ∗.

Corollary 3.2. Let m = |E| be the number of edges in G. We have

(3.7)

Furthermore, if C > n then

(3.8)

0 ≤ tr P ∗ ≤

2m
C

.

tr P ∗ ≥

2m
C

−

n3
C(C − n)

.

Proof. Sum (3.1) over i, using the fact that (cid:80)

follows from (3.5), noting that (cid:112)didj ≤ n.

i di = 2m, to obtain (3.7). (3.8)

Remark 3.3. With C = 1, as discussed in subsection 2.1, the dual cone program
reduces to maxcut, and the corollary gives tr P ∗ ≤ 2m. In this case, we can construct
an explicit optimal solution to (MLE) that saturates the trace bound for C = 1.
Indeed, the signless Laplacian L := D + A, where D = diag(di), is PSD, its oﬀ-
diagonal entries are those of A, and tr L = 2m [16]. Also for C ≤ 1, the discussion in
subsection 2.1 implies P ∗

ij = Aij for i (cid:54)= j.

3.1. Rank of optimal solutions. Since our problem is aimed at recovering low
rank solutions, it behooves us to study the rank of optimal solutions. We propose
a strategy for analyzing the rank, partly motivated by our bounds on the trace of
optimal solutions. We construct an example where rank P ∗ = n
2 ; hence, no deter-
ministic bound can show that rank P ∗ < n
2 , although a probabilistic rank bound may
still hold. Furthermore, we establish that in the regime where C > n, the naive trace-
rank inequality furnished by the convex envelope relationship can only show that
rank P ∗ ≥ n
2 . Hence, the trace-rank inequality gives the best possible deterministic
bound in that regime. In doing so, we narrow the possible hypotheses for a positive
rank reduction theorem.

The relevant optimality condition for studying the rank comes from the PSD con-
straint that P ∗Q∗ = 0. It follows that rank P ∗ + rank Q∗ ≤ rank P ∗ + dim Nul P ∗ =
n. Therefore, we can upper-bound rank P ∗ by lower-bounding rank Q∗. It is natural
to formulate bounds using the trace, as the trace is the convex envelope of rank. More
precisely, for any M ∈ Rn×n, rank M ≥ tr M / (cid:107)M (cid:107). Conveniently, tr Q∗ = nC by
construction, so it remains to upper-bound the operator norm (cid:107)Q∗(cid:107). To that end, we
hope to apply the Gershgorin disks theorem.

However, we now show that for suﬃciently large C, this method cannot show that

rank Q∗ > n
2 .

Proposition 3.4. Let Q∗ be an optimal solution to (DREG). Then for any C >
2 . Furthermore, this

max di, the tightest bound oﬀered by Gershgorin is rank Q∗ ≥ n
bound is tight.

Proof. If C > max di, then Proposition 3.1 implies that P ∗

Therefore for i ∼ j, Proposition 2.4 implies that |Q∗

ij| ≥ C√

didj

ij < 1 for all i (cid:54)= j.
. Even if we assume

that Q∗

ij = 0 for all i (cid:54)∼ j and |Q∗

ij| = C√

didj

for all i ∼ j, Gershgorin gives

rank Q∗ ≥

nC
C + max di · C

min di

≥

n
2

.

10

D. WU, D. PALMER, AND D. DEFORD

C for all i. Similarly, P ∗

2-cliques. From Proposition 3.1, P ∗
(3.6) implies that P ∗

To see that this bound is tight, consider the graph consisting of n
2 disconnected
ij = 1
ii = 1
C for i ∼ j, and
ij = 0 for i (cid:54)∼ j. Then the rank of P ∗ is clearly n
2 .
Remark 3.5. The phenomenon of vanishing entries of P ∗ posing an obstruction
to rank results can be generalized as follows. For 0 ≤ p ≤ 1, let Λ(p) be the set
of pairs (i, j) with i (cid:54)= j such that P ∗
ij = p. Form the complement of the induced
subgraph of Λ(0). For any positive k, if this complement graph contains a k-clique,
then rank P ∗ ≥ k, as there are at least k mutually orthogonal latent vectors.
In
subsections 3.2 and 3.3 we will see how vanishing entries are also troublesome for
consistency results.

3.2. Likelihood bounds. Armed with the results of previous sections, we hope
to study consistency results. We argue that for (CREG), it is natural to formulate
consistency results in terms of the Kullback-Leibler (KL) divergence. However, we
ﬁnd that the KL divergence is not well behaved. Instead, we are able to show that the
(closely related) diﬀerence in Bernoulli log likelihoods of the true P and the inferred
P ∗ is O(m log C) with high probability.

To begin the discussion, we deﬁne the following notation and link the Bernoulli

likelihood function to the KL divergence away from the diagonal.

Definition 3.6. The Bernoulli likelihood given an adjacency matrix A is deﬁned

as

LA(M ) =

(cid:88)

i(cid:54)=j

Aij log(Mij) + (1 − Aij) log(1 − Mij).

This is precisely the objective function in (MLE). We denote the regularized objective
function by

LA,C(M ) := LA(M ) − C tr(M ).

Definition 3.7. We deﬁne the matrix KL-divergence between X and Y ∈ Rn×n

to be the mean oﬀ-diagonal entrywise divergence—that is,

D(X (cid:107) Y ) = −

1
n(n − 1)

(cid:88)

i(cid:54)=j

(cid:20)
Xij log

(cid:19)

(cid:18) Yij
Xij

+ (1 − Xij) log

(cid:18) 1 − Yij
1 − Xij

(cid:19)(cid:21)

.

We deﬁne the quantity only on the oﬀ-diagonal entries because the diagonal entries

of P do not carry probabilistic meaning. For a ﬁxed M , the deﬁnitions imply that

(3.9)

D(P (cid:107) M ) = −

1
n(n − 1)

E[LA(P ) − LA(M )],

from P . Denote by P ∗

where the expectation is taken with respect to the random adjacency matrix A dis-
tributed according to P . All subsequent expectations, unless noted otherwise, are
also taken with respect to A.
Let A0 be sampled i.i.d.

A0,C the optimal solution to
(CREG) with realization A0 and hyperparameter C. We hope to develop an upper
bound on D(P (cid:107) P ∗
A0,C). We ﬁrst claim that it is impossible to obtain a ﬁnite
deterministic bound on this quantity. Suppose that 0 < Pij < 1 for all i (cid:54)= j, so that
A0 is the empty graph with positive probability. Assuming A0 is indeed the empty
graph and A is nonempty, it follows that LA(P ∗
A0.C) = −∞. However, we can still
hope for a probabilistic bound. For example, if one could identify conditions on P
under which 1
n with high probability, then the results of subsection 3.3
apply.

n ≤ Pij ≤ 1 − 1

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

11

Nevertheless, it is clear that the likelihood function of (CREG) merits its own
study. We present our main result in this direction below; the details of the proof
are provided in Appendix B. The upshot is that with high probability, the likelihood
diﬀerence LA(P ) − LA(P ∗) is O(m log C).

Theorem 3.8. Let A ∼ P ∈ Rn×n and let C ≥ 1. Then we have

(3.10)

LA(P ) − LA(P ∗) ≤ C tr P − C tr P ∗.

Furthermore, for η > 0 and C > max di, we have

(3.11)

LA(P ) − LA(P ∗) ≥ −η − H[P ] + 2m log C −

(cid:88)

i

di log di,

and

(3.12)

LA(P ) − LA(P ∗) ≤ η − H[P ] + 2m log C + 2m,

with probability at least 1 − 4 exp(−η2/(8n(n − 1) + 4η)). Here, we have deﬁned
H[P ] := − (cid:80)
i(cid:54)=j Pij log Pij, the entropy of the edge probability matrix.
Proof sketch. Since P is feasible for the convex program (CREG),

(3.13)

(3.14)

0 ≤ LA,C(P ∗) − LA,C(P )

≤ LA(P ∗) − LA(P ) + C tr P − C tr P ∗.

Rearranging yields (3.10). To show (3.11) and (3.12), we ﬁrst use a recent Bernoulli
concentration result [46, Corollary 1] to bound LA(P ) with high probability. Then we
bound LA(P ∗) by constructing primal and dual feasible points and applying Corol-
lary 3.2.

From here, we can apply Hoeﬀding’s inequality to shift the bound’s dependence

on A to dependence on P with high probability, yielding the following corollary.

Corollary 3.9. Suppose M := (cid:80)

LA(P ) − LA(P ∗) = O(M log C).

i<j Pij ∈ ω(n). Then with high probability,

3.3. Consistency guarantees for a modiﬁed program. We now turn to
formulating a consistency bound after slightly modifying (CREG). In particular, we
impose the mild assumption that 1
n ≤ Pij ≤ 1 − 1
n for i (cid:54)= j. We think of these as
nontriviality conditions—they ensure that no node has expected degree less than 1
or greater than n − 1. Under this model, we obtain that with high probability, the
KL divergence D(P (cid:107) P ∗) is O(E[tr P ∗]n−1/2 log n + Cn−2 tr P ). We also identify
conditions under which this bound is eﬀective.

With these new constraints, the primal problem becomes

max
P

(cid:88)

i∼j

log Pij +

(cid:88)

i(cid:54)∼j

log(1 − Pij) − C tr P

(REG-MOD)

subject to:

1
n

≤ Pij ≤ 1 −

1
n
P (cid:23) 0

for all i (cid:54)= j

With the additional constraints in (REG-MOD), a duality analysis produces the
following bound (cf. Corollary 3.2); see Appendix B for the complete proof. To state
the result we recall the Λ notation introduced in Remark 3.5. We extend the notation

12

D. WU, D. PALMER, AND D. DEFORD

in a few ways. First, we let Λ((p, q)) denote the set of indices i (cid:54)= j where p < P ∗
ij < q.
Next, we deﬁne Λ0(p) := Λ(p) ∩ Ec and Λ1(p) := Λ(p) ∩ E, extending the deﬁnition to
Λ((p, q)) in the obvious way. Finally, we deﬁne Z(P ∗) := #Λ0( 1
n )).

n ) − #Λ0(( 1

n , 1 − 1

Proposition 3.10. Let P ∗ be an optimal solution to (REG-MOD). Then

(3.15) tr P ∗ ≤

2m
C

+

1
n

It follows that

(3.16)

#Λ( 1

n )−#Λ1( 1

n )−

n − 1
C

#Λ0(1− 1

n )−

1
n − 1

#Λ0(( 1

n , 1− 1

n )).

tr P ∗ ≤

2m
C

+

1
n

Z(P ∗).

We assume that the true P is feasible for (REG-MOD), so that LA,C(P ∗) ≥
LA,C(P ). For clarity, we denote by WA(M ) the centered random variable LA(M ) −
E[LA(M )]. Recalling (3.13), but this time adding and subtracting expectations, we
ﬁnd that

0 ≤ E[LA(P ) − LA(P ∗)] + WA(P ∗) − WA(P ) − C tr(P ∗) + C tr(P )
≤ −n(n − 1)D(P (cid:107) P ∗) + WA(P ∗) − WA(P ) − C tr(P ∗) + C tr(P ).

Theorem B.1 gives us concentration of WA(P ), so it suﬃces to obtain a probabilistic
upper bound on WA(P ∗). We bound this quantity over a compact neighborhood of
P ∗ rather than directly bounding it for P ∗. In particular, deﬁne the set FA to be
the intersection of the feasible set of (REG-MOD) with the trace ball centered at the
origin with radius tr P ∗. Clearly P ∗ ∈ FA, and since Proposition 3.10 controls tr P ∗,
FA is a compact subset of the original feasible set. Thus it suﬃces to probabilistically
upper-bound supM ∈FA WA(M ).

Indeed, this can be achieved through a similar approach to that in [17]; the details
of the proof are left to Appendix C. The upshot is that for sparse enough graphs, we
indeed obtain consistency as the number of nodes in the RDPG increases.

Theorem 3.11. Let P ∗ be an optimal solution to (REG-MOD), and suppose P
is feasible for (REG-MOD). Then there is an absolute constant K, such that with
probability at least 1 − δ, we have

D(P (cid:107) P ∗) ≤

KE[tr(P ∗)] log n
√

δ

n

+

C tr(P ) − C tr(P ∗)
n(n − 1)

−

LA(P ) + H[P ]
n(n − 1)

.

A quick application of Proposition 3.10 and Theorem B.1 yields the following

corollary.

Corollary 3.12. With high probability, we have

D(P (cid:107) P ∗) ≤

KM log n
√

δC

n

+

KE[Z(P ∗)] log n
√

δn

n

+

C tr(P )
n(n − 1)

+ O(1/n),

where M := E[m] = (cid:80)

i<j Pij is the expected number of observed edges.

Remark 3.13. There are two sources of the probabilistic nature of the bound: δ
and the concentration of LA(P ). We can for example take δ = O(log n) and η = ω(n)
in Theorem B.1.

We discuss the shape of the bound and its implications. Note that a priori

D(Pij (cid:107) P ∗

ij) ≤ log n,

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

13

since for every feasible M we have 1
n . Further inspection of Corol-
lary 3.12 reveals that under certain conditions on P ∗ and P , Theorem 3.11 nontrivially
improves on this log n bound and gives consistency as n → ∞.

n ≤ Mij ≤ 1 − 1

Corollary 3.14. Suppose E[Z(P ∗)] = O(n3/2/ log n). Then the optimal C with
respect to the bound in Corollary 3.12 satisﬁes C 2 = O(M n3/2 log n/ tr P ).
If we
further have tr P = O(M/n) and M = o(n7/4/(log n)1/2), then D(P (cid:107) P ∗) = o(1)
with high probability.

Remark 3.15. Note that owing to Lemma C.1, the assumption on tr P is the
tightest possible one. The exact assumption on E[Z(P ∗)] can potentially be relaxed
if the proof of Proposition 3.10 is reﬁned.

4. Experimental results. We investigate the performance of the inference tech-
nique for a variety of tasks. We ﬁnd that the learned embedding appears to be pushed
towards lower and lower ranks as the regularization hyperparameter C is increased.
Motivated by this, we examine regularization proﬁles that demonstrate that (CREG)
improves on the ASE in the spectral norm distance. This naturally leads to a cross
validation procedure to select an embedding dimension. Through visualization and
cluster analysis, we also ﬁnd that our method recovers latent geometry better than
the ASE in synthetic and more realistic networks.

The two primary quantities of interest for a solution P ∗ to (CREG) are (1) its ﬁt
to the true P and (2) its rank, which we denote by d∗. The former, which measures
consistency, is diﬃcult to measure directly with real data because we do not usually
know P . Similarly, we might not know the true dimension d of the underlying latent
vector distribution, so we must balance other considerations to select a value of the
regularization parameter C. As a ﬁrst step towards understanding how C aﬀects d∗,
we present the spectra in Figure 1, which illustrates an inverse relationship between
C and d∗. This makes sense since (CREG) is an MAP problem where C controls
the strength of the Laplace prior on the spectrum of P . In subsequent sections we
also observe this phenomenon of approximate low rank solutions as we increase C; a
similar phenomenon has been studied in related matrix completion problems [33].

To examine how well our inference procedure recovers the rank and geometry of
the true P , we turn to synthetic data, sampled from an RDPG with a known latent
vector distribution. In subsection 4.2, we consider three classes of synthetic data: the
SBM with a ﬁxed block probability matrix, latent vector distributions supported on
Sd−1, and latent vector distributions supported on the unit ball in Rd.

4.1. Implementation details. The convex cone program (CREG) was imple-
mented in CVXPY using the MOSEK [4] and SCS [29] solvers. Both solvers support
semideﬁnite and exponential cone constraints, but diﬀer in their algorithms: MOSEK
uses an interior point method, while SCS uses the alternating direction method of
multipliers (ADMM). In general, we found that MOSEK converges to more accurate
solutions than SCS. On the other hand, as SCS uses the ﬁrst-order ADMM, it can
handle larger problem instances.

Given P , we measured the closeness of ﬁt between P ∗ and P by directly com-
puting (cid:107)P ∗ − P (cid:107).
In order to compute d∗, we used the eigvalsh function in the
scipy.linalg package, with a tolerance of 1 · 10−6 for MOSEK and 1 · 10−3 for SCS.
These thresholds were selected based on empirical observations of eigenvalue spectra
for P ∗ such as the one shown in Figure 1, and reﬂect a disparity in numerical precision
between the solvers.

14

D. WU, D. PALMER, AND D. DEFORD

Fig. 1: Sample eigenvalue plots for P ∗
C for a ﬁxed realization A sampled from Figure 2b
as C is increased. The eigenvalues are sorted in ascending order and their base-ten
logarithms are plotted. Note the abrupt drop in eigenvalue size to around 10−6 for
each C.

(a) Latent vectors supported on S2.

(b) Latent vectors supported on two balls.

Fig. 2: Ground truth latent vector positions. We sampled 50 latent vectors in both
ﬁgures; their positions are marked by blue X’s. The orange surface traces out S2, and
the red surfaces trace out the support of the respective latent vector distributions.

4.2. Synthetic data experiments. In order to evaluate the consistency of
(CREG), we conducted the following three experiments, all using MOSEK for conic
optimization. In each experiment, we sampled 50 latent vectors from a ﬁxed distri-
bution and formed a probability matrix P . Keeping P ﬁxed, we then generated 100
RDPGs, and for each RDPG we ran the inference problem for 2 ≤ C ≤ 25 with a
step size for 1. We computed spectral distance (cid:107)P ∗
C for each RDPG and
each value of C.

C − P (cid:107) and d∗

In Figure 3a, we illustrate the results for the two patches on S2 shown in Fig-
ure 2a. Interestingly, although the spectral norm distance does not achieve its local
minimum where d∗
C = 3, we will later see that the 3D and 2D visualizations are still
salient. We also point out the smoothness of the cross validation curve and the clear

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

15

(a) Patches on S2

(b) Balls in B3

(c) SBM

Fig. 3: Regularization proﬁles for synthetic 50 node RDPGs. The rank (yellow)
decreases monotonically. Note also the clear local minimum in spectral norm distance
(blue) in all three plots. Furthermore, for a certain range of C, our method improves
on the top-3 eigenvector ASE spectral norm distance.

local minimum for the spectral norm at C ≈ 13, with d∗
signiﬁcant dimensionality reduction from the original rank 50 solution.

C ≈ 10, corresponding to a

In Figure 3b, we show the results from the two balls in the nonnegative unit ball
depicted in Figure 2b. We chose this example because the latent vectors do not all
have the same Euclidean norm, which could reasonably be interpreted as a disparity
in the intrinsic popularities of nodes in the RDPG. In Figure 3b, we see that the light
blue shaded band representing the two–standard deviation interval is wider. Indeed,
since the intra-cluster probabilities are not close to 1 (unlike the previous experiment),
there is higher variance in the realizations of the RDPG. Nevertheless, we observe a
local minimum at C ≈ 10 for the cross validation curve and a monotonic relationship
between d∗ and C.

In Figure 3c, we depict the results from an SBM with blocks of respective size 15,
(cid:3). Although the SBM had
10, and 25, using the block probability matrix (cid:2) 0.25 0.05 0.02
three clusters (as the block probability matrix is 3 × 3), the cross validation graph
is visually similar to that in Figure 3b, where the ground truth only contained two
clusters.

0.05 0.35 0.07
0.02 0.07 0.40

In Figures 3a to 3c, it is apparent that our method beats the ASE in terms of
spectral norm distance, even when the ASE is given oracle access to d. Even if P
is unknown, the same cross validation procedure can be used by simply replacing P
with A in the metric. However, we found experimentally that using (cid:107)A − P ∗(cid:107) does
not perform as well as using (cid:13)
(cid:13) for cross validation purposes. Hence we
computed the latter metric for the inferred solutions in the earlier experiments to
obtain the curves in Figures 4a to 4c. Interestingly, the shapes of the two curves are
very similar, especially in Figures 4b and 4c. More importantly, they have similar
local minima as a function of C. Thus, it is reasonable to use this cross validation
technique to select the best C and thus single out a preferred embedding or rank.

(cid:13)A2 − (P ∗)2(cid:13)

4.3. Visualization through latent vector extraction. Given the solution
P ∗
C for some C in (DREG), we can extract an embedding X by taking the top 2 or 3
eigenvectors and scaling them by the square roots of their eigenvalues. We have seen
that selecting C via cross validation can recover better solutions P ∗
C measured by the
spectral norm; the hope is that the same eﬀect can also be observed visually.

We conducted the following experiment to demonstrate the improvement in vi-
sualization capabilities in 3 dimensions over the ASE and the applications of the

16

D. WU, D. PALMER, AND D. DEFORD

(a) Patches on S2

(b) Balls in B3

(c) SBM

Fig. 4: Cross validation graphs comparing (cid:13)
(cid:13)A2 − (P ∗)2(cid:13)
(cid:13)
(empirical). The shape of the curves are generally similar. Crucially, the locations of
the local minimum of the empirical and true curves are similar as a function of C.

(cid:13) (true) and (cid:13)

(cid:13)P 2 − (P ∗)2(cid:13)

inference technique to larger graphs. Starting from the synthetic distribution illus-
trated in Figure 2a, we generated an RDPG with 300 nodes; note the two clusters on
S2 that support the distribution. Using SCS, we computed the regularization proﬁle
and cross validation plot for 10 ≤ C ≤ 150 with a step size of 10 (shown in Fig-
ures 5c and 5d) and selected the best C in spectral norm distance. We then visually
compared the top-3 eigenvector embedding P ∗
C with the top-3 eigenvector ASE. Fig-
ure 5a demonstrates that the ASE, while approximately recovering the geometry of
the latent vector distribution, is more diﬀuse than the corresponding visualization in
Figure 5b. In particular, notice in Figure 5b that fewer of the embeddings lie outside
of S2, and that the geometry more closely matches that displayed in Figure 2a.

We also demonstrate the clustering eﬀects of increasing regularization. Using the
same 300 node RDPG, we compare the top-3 eigenvector visualizations for various
values of C in Figures 6a to 6c. The shape of the embedding remains relatively
consistent as C is increased. As C is increased, the numerical rank of P ∗
C decreases,
and the two clusters become more compact.

In order to quantify this notion, we use the Dunn index [21]; larger Dunn indices
correspond to more compact clusters.
In Figure 6d we see that as C is increased,
the Dunn index also increases. Notice that the ASE has a low Dunn index, and that
as C increases, the learned embeddings approach a Dunn index close to that of the
true embedding. Furthermore, the best embedding (around C = 60) determined in
Figures 5c and 5d has a similar Dunn index to the true embedding.

4.4. Realistic network experiments. We ﬁrst study embeddings of the Karate
Club Graph. We solved the inference problem for 10 ≤ C ≤ 100 with a step size of
5. In Figure 7b, we select the smallest C with d∗
C = 2 and compare the latent vector
visualization to that of the top-2 eigenvector ASE. The color of each node represents
which community it belongs to. Comparing the two embeddings, we note that the
positions of the communities are swapped, which illustrates the inherent nonidentiﬁ-
ability of the embedding. The shrinkage eﬀects of increasing regularization are also
apparent. Although both the ASE and our method make the communities linearly
separable, our method tightens the clusters more than the ASE does.

Figure 8 depicts the results of an experiment using synthetic data generated from
the voting patterns of the 114th U.S. Senate. We choose this session as all n = 100
senators were active throughout the entire session. Similar latent space models [30, 31]
and network analyses of community detection from voting data have been studied in

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

17

(a) ASE visualization

(b) Embedding for C = 60

(c) Regularization proﬁle

(d) Cross validation

Fig. 5: Top-3 eigenvector visualizations with the ASE (left) and our method (right).
The unit sphere is overlayed, and the points are rotated to enhance visual understand-
ing for the embedding.

[19, 25, 26, 27, 32, 41]. The edge probability Pij is deﬁned to be the fraction of
votes in which i and j voted the same way. If this vote agreement data ﬁts the RDPG
assumption, our inference procedure should be able to recover two well deﬁned clusters
corresponding to the Democratic and Republican political parties. Indeed, Figure 8b
reveals an inferred embedding that displays the two clusters prominently. Moreover,
the nodes in the middle represent the more moderate senators, suggesting that the
model recovers accurate detailed geometry.

5. Conclusions. We have presented a conic programming approach to MAP
inference of latent vector embeddings of RDPGs. Our inference technique oﬀers a dif-
ferent perspective on recovering RDPG latent geometry compared to existing methods
which typically center around spectral decompositions of graph matrices. The conic
formulation led to a surprisingly explicit relationship between the primal and dual
problems. Importantly, the exponential cone constraints yield an exact algebraic re-
lationship between the optimal solutions to the primal and dual problems, whereas the
semideﬁnite cone constraint and trace regularization lead to bounds on the optimal
solutions.

Some of the limitations of our analysis stem from a lack of nontrivial lower bounds

18

D. WU, D. PALMER, AND D. DEFORD

(a) C = 20 and d∗ = 136

(b) C = 60 and d∗ = 60

(c) C = 100 and d∗ = 20

(d) Dunn indices

Fig. 6: Three dimensional spectral embeddings of the same graph with n = 300. Note
the tighter clusters as C increases, in accordance with the increasing Dunn indices.

(a) ASE

(b) C = 80 and d∗ = 2.

Fig. 7: Top-2 eigenvector visualization for the Karate Club Graph using the ASE (left)
and the rank 2 solution for C = 80 (right). The axis limits are ﬁxed to demonstrate
the shrinkage eﬀects of the regularization.

on P ∗
ij. For example, while we’ve shown the impossibility of a useful deterministic
bound on rank for large C, a better handle on elementwise lower bounds would likely
produce a nontrivial rank reduction result for small C. Similarly, lower bounds would
enable a probabilistic KL consistency result that does not require the model to be
altered. In particular, if one is able to prove that 1
n , then the results
of subsection 3.3 immediately follow.

ij ≤ 1 − 1

n ≤ P ∗

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

19

(a) ASE

(b) C = 40 and d∗ = 2

Fig. 8: Top-2 eigenvector visualization for a synthetic U.S. Senate Vote Graph us-
ing the ASE (left) and the rank 2 solution for C = 40 (right). Blue nodes denote
Democratic senators, red nodes denote Republican senators, and green nodes denote
independent senators. The axis limits are ﬁxed so the embeddings can be directly
compared.

The experimental results show that it is often possible to recover solutions of ar-
bitrary rank by varying the regularization parameter C. It is not possible to do so for
all realizations, as established by Proposition 3.4. Nevertheless, we conjecture that
with high probability, the rank of inferred embeddings can be reduced nontrivially,
say to order O(log n). Although we were not able to prove spectral norm consistency
of the extracted embedding, the results of our cross validation and visualization ex-
periments suggest nontrivial improvements on the ASE over a certain bounded range
of C values. Furthermore, we observe that as C increases arbitrarily, the spectral
norm distance worsens, which agrees with the theoretical guarantee that the entries
of P ∗ are Θ(1/C).

The proposed conic optimization problem is solved via second-order methods us-
ing MOSEK and via the ﬁrst-order ADMM method using SCS. For the algorithm to
be of interest for larger graphs, there would likely need to be a ﬁrst-order implemen-
tation that could exploit low-rank complexity reductions. In this vein, an extension of
the B¨urer-Monteiro method [14], along with more theoretical guarantees of optimality,
would be particularly interesting.

Appendix A. Derivation of dual program.

Proof of Proposition 2.2. Form the Lagrangian of (CREG), with notation deﬁned
as in section 2. Swapping the min and max operators and eliminating the primal
constraints by enforcing the dual constraints, we obtain

min
Q(cid:23)0
λij ,νij ∈K∗

exp

max
α,β,P

(cid:34)

(cid:88)

i∼j

αij +

(cid:88)

i(cid:54)∼j

βij − C tr(P )

(cid:88)

+

i(cid:54)=j

λij · (Pij, 1, αij) +

(cid:88)

i(cid:54)=j

νij · (1 − Pij, 1, βij) + (cid:104)Q, P (cid:105)

.

(cid:35)

20

D. WU, D. PALMER, AND D. DEFORD

Collecting terms involving each primal variable, we obtain

min
Q(cid:23)0
λij ,νij ∈K∗

exp

max
α,β,P

(cid:88)

i∼j

[(1 + tij)αij + wijβij] +

[(1 + wij)βij + tijαij]

(cid:88)

i(cid:54)∼j

(cid:88)

+

i

(Qii − C)Pii +

(cid:88)

[(Qij + rij − uij)Pij + sij + uij + vij],

i(cid:54)=j

where λij = (rij, sij, tij) and νij = (uij, vij, wij).

This is in turn equivalent to the following dual program:

min

(cid:88)

i(cid:54)=j

(sij + uij + vij)

subject to:

(A.1)

for all i ∼ j

for all i (cid:54)∼ j

for all i (cid:54)= j

for all i (cid:54)= j

tij = −1, wij = 0
tij = 0, wij = −1
Qij = uij − rij
λij, νij ∈ K ∗

exp
diag Q = C

Q (cid:23) 0

These explicit constraints follow from Lagrangian duality, or alternatively by not-

ing that the outer minimization necessarily avoids inner objective values of +∞.

Recall from Deﬁnition 1.2 that

K ∗

exp := {(u, v, w) ∈ R3 : u ≥ −w exp(v/w − 1), u > 0, w < 0} ∪ {(u, v, 0) : u, v ≥ 0}.

Applying the explicit constraints in (A.1), we simplify the dual objective function as
follows:

• For i ∼ j, λij ∈ K ∗

exp simpliﬁes to sij ≥ −1 − log rij. Also, wij = 0 implies
that vij ≥ 0. Since there are no other constraints on vij, and since we are
minimizing vij, it follows that at optimality v∗
ij = 0 for i ∼ j. Hence we can
rewrite the (cid:80)

i(cid:54)=j vij as (cid:80)

i(cid:54)∼j vij.

• For i (cid:54)∼ j, the constraint for νij simpliﬁes to vij ≥ −1 − log uij. Similarly,
ij = 0. Hence we can rewrite
i∼j sij. Putting these steps all together, the objective function

tij = 0 implies that sij ≥ 0, and at optimality s∗
(cid:80)

i(cid:54)=j sij as (cid:80)

becomes

min

(cid:88)

i∼j

sij +

(cid:88)

i(cid:54)∼j

vij +

(cid:88)

i(cid:54)=j

uij.

Thus we obtain (DREG), as desired.

We now prove the following simple lemma which is useful in the proof of Propo-

sition 2.4 and Lemma 2.3.

Lemma A.1. Let x be a positive real number. If x + log x = 1 or x − log x = 1,
If x + log(−x) = −1 or

then x = 1. Similarly, let x be a negative real number.
x − log(−x) = −1, then x = −1.

Proof. First suppose x > 0. By inspection, x = 1 satisﬁes x ± log x = 1. Since
x + log x is monotonic, x = 1 is the unique solution. Since x − log x is strictly convex
and has a critical point at x = 1, x = 1 is the unique solution. The case of x < 0
follows similarly.

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

21

Proof of Lemma 2.3. Note that sij is constrained by a single inequality sij ≥
−1 − log rij and is being minimized in the objective function. Thus, at optimality,
the inequality becomes tight, and we have sij = −1 − log rij. By a similar argument,
vij = −1 − log uij at optimality. These considerations reduce the dual to the following
form, equivalent at optimality:

min

(cid:88)

(uij − 1 − log rij) +

(cid:88)

(uij − 1 − log uij)

i∼j

i(cid:54)∼j

subject to:

(A.2)

for all i (cid:54)= j

for all i (cid:54)= j

uij − rij = Qij
uij, rij ≥ 0
diag Q = C

Q (cid:23) 0

We simplify the dual further by considering each pair i (cid:54)= j individually:

• Suppose i ∼ j. Substituting uij − Qij for rij, the corresponding objective
term is −1 − log(uij − Qij) + uij. If Qij ≥ −1, then by Lemma A.1 this has a
unique minimum at uij = Qij + 1 ≥ 0, which is in the feasible region. Thus
in this case rij = 1, and the objective term reduces to Qij.
On the other hand, if Qij < −1, then due to domain constraints and mono-
tonicity of the objective, the minimizer is uij = 0, so rij = −Qij. Thus, in
this case the objective term is −1 − log(−Qij).

• Next, consider the term uij − 1 − log uij where i (cid:54)∼ j. If Qij ≤ 1, then this has
a unique minimum at uij = 1 by Lemma A.1. Furthermore, the constraint
rij = uij − Qij ≥ 0 is satisﬁed. These terms vanish in the objective function.
However, if Qij ≥ 1, then the optimal choice for uij is uij = Qij. In this case
rij = 0, which simpliﬁes the objective term to Qij − log Qij − 1, thus proving
the claim.

Appendix B. Proofs of likelihood bounds. We begin by restating the

recent Bernoulli concentration result from [46].

Theorem B.1 ([46, Corollary 1]). For pij ∈ [0, 1], with 1 ≤ i (cid:54)= j ≤ n, and all

η > 0, we have

(cid:34)

P

(cid:12)
(cid:12)
(cid:12)LA(P ) − E[LA(P )]
(cid:12)
(cid:12)
(cid:12) ≥ η

(cid:35)

(cid:18)

≤ 4 exp

−

η2
4(2n(n − 1) + η)

(cid:19)

.

Proof of Theorem 3.8. We construct an upper bound by way of a dual-feasible
solution to (DCREG) in the restricted domain of symmetric diagonally dominant
matrices, which are manifestly PSD. We construct a dual-feasible, diagonally domi-
nant Q as follows.

(B.1)

Qij =






C
0
−C/di

i = j
i (cid:54)∼ j
i ∼ j

In fact, for C > max(di), (B.1) is optimal for symmetric, diagonally dominant Q.

To see this, note the following facts.

• The objective function of (DCREG) is monotonically nondecreasing in Qij.
• When we impose diagonal dominance, the program decouples for each i.

22

D. WU, D. PALMER, AND D. DEFORD

• The non-edge contribution to the objective function is nonnegative and van-

ishes for Qij ≤ 1.

Using these facts, the restricted program decouples to the following for each i:

(DCREG-RES)

(cid:88)

min

−1 − log(−Qij)

i∼j
subject to: − C ≤ Qij ≤ −1

(cid:88)

−

Qij ≤ C

j

i ∼ j

for all i

The assumption that C > max(di) ensures that the feasible set is nonempty. In fact,
(DCREG-RES) can be analytically minimized using AM-GM to see that indeed we
should take Qij = − C
di

Plugging (B.1) back into the original objective function in (DCREG), we obtain

.

−1 − log

(cid:88)

i∼j

(cid:19)

(cid:18) C
di

= −2m −

(cid:88)

i

di log

(cid:19)

.

(cid:18) C
di

Thus LA,C(P ∗) ≤ −2m − (cid:80)

i di log( C
di

), and combining this with Corollary 3.2,

we obtain

(B.2)

LA(P ∗) ≤ −

Now write

(cid:88)

i

di log

(cid:19)

(cid:18) C
di

= −2m log C +

(cid:88)

i

di log di.

LA(P ) − LA(P ∗) = (LA(P ) − E[LA(P )]) + (E[LA(P )] − LA(P ∗)).

We can combine Theorem B.1 with (B.2) to obtain (3.11), as desired.

For the upper bound on LA(P )−LA(P ∗), note that the scaled signless Laplacian
1
C (D +A) in Remark 3.3 achieves objective value −2m log C −2m for (CREG). Hence
applying Theorem B.1 again we obtain (3.12).

Appendix C. Proofs of consistency results.
Lemma C.1. For any RDPG generated by edge probability matrix P , we have

(C.1)

E[m] =

Pij ≤

n − 1
2

tr P .

(cid:88)

i<j

Proof. The ﬁrst equality immediately follows from the generative model. For the
≥ Pij. Summing over all i (cid:54)= j,

inequality, note that since P is PSD, we have Pii+Pjj
we obtain (n − 1) tr P ≥ (cid:80)

i(cid:54)=j Pij, and the result follows by symmetry of P .

2

Now we take the dual of (REG-MOD) in hopes of obtaining a similar trace bound
to Corollary 3.2. We introduce dual variables Q (cid:23) 0 and cij, dij ≥ 0 for i (cid:54)= j. Taking
the Lagrangian and swapping min and max as before, we obtain

min
Q(cid:23)0
cij ,dij ≥0

max
P

(cid:34)

(cid:88)

i∼j

log Pij +

(cid:88)

i(cid:54)∼j

log(1 − Pij) − C tr(P )

(cid:18)

cij

Pij −

(cid:19)

1
n

(cid:18)

+ dij

1 −

1
n

(cid:88)

+

i(cid:54)=j

(cid:19)

− Pij

+ (cid:104)Q, P (cid:105)

(cid:35)
.

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

23

The KKT conditions require the gradient of the Lagrangian with respect to P to

vanish at optimality. We do casework on i and j.

• If i ∼ j, then we obtain − 1
Pij
1
• If i (cid:54)∼ j, then we obtain
1−Pij
• If i = j, then we obtain Qii = C.

= cij − dij + Qij.
= cij − dij + Qij.

Hence in the dual problem, Q (cid:23) 0 and diag Q = C, from which it follows that
|Qij| ≤ C. The KKT conditions also require that c∗
ij) = 0,
and P ∗Q∗ = 0. Considering the diagonal entries of the latter constraint as before
(see Proposition 3.1) we obtain

n ) = 0, d∗

ij(1− 1

n −P ∗

ij(P ∗

ij − 1

(C.2)

CP ∗

ii +

(cid:88)

i∼j

ijQ∗
P ∗

ij +

(cid:88)

i(cid:54)∼j

ijQ∗
P ∗

ij = 0.

We now bound the edge terms by way of the KKT conditions.
ij ≥ −1.
ijQ∗

ij ≥ − 1
. Hence P ∗
P ∗
ij
ij ≥ −C, we have P ∗

ij ≤ 1 − 1
n , then recalling that Q∗

n < P ∗
ij = 1

• If 1
• If P ∗

n , then Q∗

ijQ∗

ij ≥ − C
n .

Before combining these two bounds, we deﬁne N (i) := {j : i ∼ j} and N (i)c :=

{j : i (cid:54)∼ j}. We have

(C.3)

(cid:88)

N (i)

ijQ∗
P ∗

ij ≥ −di +

(cid:88)

Λ( 1

n )∩N (i)

(cid:18)

1 −

(cid:19)

.

C
n

We now turn to bounding the nonedge terms.

• If 1
• If P ∗
• If P ∗

ij < 1 − 1

n , then Q∗

n < P ∗
ij = 1 − 1
ij = 1

ij = 1
ij ≥ n. Hence P ∗
n , then again recalling that Q∗

n , then Q∗

ij > 1

ijQ∗
ij ≥ n − 1.

. Hence P ∗
ijQ∗
ij ≥ −C, we have P ∗

1−P ∗
ij

n−1 .

ijQ∗

ij ≥ − C
n .

Hence

(C.4)

(cid:88)

N (i)c

ijQ∗
P ∗

ij ≥

(cid:88)

Λ(( 1

n ,1− 1

n ))∩N (i)c

1
n − 1

+

(cid:88)

(n − 1) −

(cid:88)

Λ(1− 1

n )∩N (i)c

Λ( 1

n )∩N (i)c

C
n

.

Having done all of the legwork, now we are ready to quickly prove Proposi-

tion 3.10.

Proof of Proposition 3.10. Plug (C.3) and (C.4) into (C.2). Summing over i and
dividing by C, we obtain (3.15). Equation (3.16) immediately follows since #Λ ≥ 0.

Proof of Theorem 3.11. Much of the following proof takes inspiration from [17],
although there are a few key modiﬁcations that must be made to adapt their technique.
The ﬁrst key observation is that log x and log(1 − x) are n-Lipschitz on the interval
[1/n, 1 − 1/n]. In order to apply the contraction principle [24, Theorem 4.12], under
typical hypotheses we need a Lipschitz function ϕ which vanishes at 0. However, upon
closer examination of its proof, it suﬃces for our purposes to show that | log x| ≤ ρx
for some ρ and all x ∈ [1/n, 1 − 1/n]. It is not hard to see that the minimum such
ρ is n log n. Applying symmetrization [39, Section 6.4] by introducing symmetric

24

D. WU, D. PALMER, AND D. DEFORD

Bernoulli variables (cid:15)ij, followed by contraction [24], we arrive at

E

(cid:20)

sup
M ∈FA

(cid:21)
WA(M )

≤ EA,(cid:15)



 sup
M ∈FA

(cid:88)

i(cid:54)=j


(cid:15)ij(1i∼j log(Mij) + 1i(cid:54)∼j log(1 − Mij))







≤ n log n · EA,(cid:15)

 sup
M ∈FA

(cid:88)

i(cid:54)=j

(cid:15)ij(1i∼jMij + 1i(cid:54)∼jMij)



≤ n log n · EA,(cid:15)

(cid:20)

sup
M ∈FA

(cid:21)

|(cid:104)E, M (cid:105)|

.

In the third line we have deﬁned the random matrix E by Eij := (cid:15)ij for i (cid:54)= j
and Eii := 0, and we have used the fact that 1i∼j + 1i(cid:54)∼j = 1i(cid:54)=j. Now we have
|(cid:104)E, M (cid:105)| ≤ (cid:107)E(cid:107) tr(M ), since M is PSD. By [34, Theorem 1.1] we have that there is
some absolute constant K so that (cid:107)E(cid:107) ≤ K

n. We thus have that

√

E

(cid:20)

(cid:21)

sup
M ∈FA

WA(M )

≤ n log n · E(cid:15)[(cid:107)E(cid:107)]EA

(cid:20)

(cid:21)

sup
M ∈FA

tr(M )

≤ Kn3/2 log nEA[tr(P ∗)],

where we have used the deﬁnition of FA in the last line. Dividing through by n(n−1),
and applying Markov’s inequality with parameter δ we arrive at the desired inequal-
ity.

Acknowledgments. We would like to thank Justin Solomon and Chris Scarvelis
for their thoughtful comments. We would also like to thank Nicolas Boumal and Aude
Genevay for helpful discussions.

REFERENCES

[1] E. Abbe, Community detection and stochastic block models: recent developments, The Journal

of Machine Learning Research, 18 (2017), pp. 6446–6531.

[2] A. Agrawal, R. Verschueren, S. Diamond, and S. Boyd, A rewriting system for convex

optimization problems, Journal of Control and Decision, 5 (2018), pp. 42–60.

[3] J. Agterberg, Y. Park, J. Larson, C. White, C. E. Priebe, V. Lyzinski, et al., Vertex
nomination, consistent estimation, and adversarial modiﬁcation, Electronic Journal of
Statistics, 14 (2020), pp. 3230–3267.

[4] M. ApS, The MOSEK Modeling Cookbook. Version 3.2.2B, 2020, https://docs.mosek.com/

modeling-cookbook/expo.html.

[5] J. Arroyo, D. L. Sussman, C. E. Priebe, and V. Lyzinski, Maximum likelihood estima-
tion and graph matching in errorfully observed networks, arXiv preprint arXiv:1812.10519,
(2018).

[6] A. Athreya, D. E. Fishkind, M. Tang, C. E. Priebe, Y. Park, J. T. Vogelstein, K. Levin,
V. Lyzinski, and Y. Qin, Statistical inference on random dot product graphs: a survey,
The Journal of Machine Learning Research, 18 (2017), pp. 8393–8484.

[7] A. Athreya, C. E. Priebe, M. Tang, V. Lyzinski, D. J. Marchette, and D. L. Sussman, A
limit theorem for scaled eigenvectors of random dot product graphs, Sankhya A, 78 (2016),
pp. 1–18.

[8] N. Boumal, V. Voroninski, and A. Bandeira, The non-convex burer-monteiro approach
works on smooth semideﬁnite programs, in Advances in Neural Information Processing
Systems, 2016, pp. 2757–2765.

[9] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004,

https://doi.org/10.1017/CBO9780511804441.

[10] S. Burer and R. D. Monteiro, A nonlinear programming algorithm for solving semideﬁnite
programs via low-rank factorization, Mathematical Programming, 95 (2003), pp. 329–357.

MAP INFERENCE ON RANDOM DOT PRODUCT GRAPHS

25

[11] T. Cai and W.-X. Zhou, A max-norm constrained minimization approach to 1-bit matrix
completion, The Journal of Machine Learning Research, 14 (2013), pp. 3619–3647.
[12] L. Chen, C. Shen, J. T. Vogelstein, and C. E. Priebe, Robust vertex classiﬁcation, IEEE

Transactions on Pattern Analysis and Machine Intelligence, 38 (2016), pp. 578–590.

[13] D. S. Choi, P. J. Wolfe, and E. M. Airoldi, Stochastic blockmodels with a growing number

of classes, Biometrika, 99 (2012), pp. 273–284.

[14] D. Cifuentes, On the burer-monteiro method for general semideﬁnite programs, arXiv preprint

arXiv:1904.07147, (2019).

[15] M. Collins, S. Dasgupta, and R. E. Schapire, A generalization of principal components
analysis to the exponential family, in Advances in neural information processing systems,
2002, pp. 617–624.

[16] D. Cvetkovi´c, P. Rowlinson, and S. K. Simi´c, Signless laplacians of ﬁnite graphs, Linear

Algebra and its applications, 423 (2007), pp. 155–171.

[17] M. A. Davenport, Y. Plan, E. Van Den Berg, and M. Wootters, 1-bit matrix completion,

Information and Inference: A Journal of the IMA, 3 (2014), pp. 189–223.

[18] M. A. Davenport and J. Romberg, An overview of low-rank matrix recovery from incomplete
observations, IEEE Journal of Selected Topics in Signal Processing, 10 (2016), pp. 608–622.
[19] D. DeFord and D. Rockmore, A random dot product model for weighted networks, arXiv

preprint arXiv:1611.02530, (2016).

[20] S. Diamond and S. Boyd, CVXPY: A Python-embedded modeling language for convex opti-

mization, Journal of Machine Learning Research, 17 (2016), pp. 1–5.

[21] J. C. Dunn, A fuzzy relative of the isodata process and its use in detecting compact well-

separated clusters, Journal of Cybernetics, (1973).

[22] M. X. Goemans and D. P. Williamson, Improved approximation algorithms for maximum cut
and satisﬁability problems using semideﬁnite programming, Journal of the ACM (JACM),
42 (1995), pp. 1115–1145.

[23] A. J. Landgraf and Y. Lee, Dimensionality reduction for binary data through the projection

of natural parameters, arXiv preprint arXiv:1510.06112, (2015).

[24] M. Ledoux and M. Talagrand, Probability in Banach Spaces: isoperimetry and processes,

Springer Science & Business Media, 2013.

[25] J. MOODY and P. J. MUCHA, Portrait of political party polarization, Network Science, 1

(2013), p. 119–121.

[26] P. J. Mucha and M. A. Porter, Communities in multislice voting networks, Chaos: An

Interdisciplinary Journal of Nonlinear Science, 20 (2010).

[27] P. J. Mucha, T. Richardson, K. Macon, M. A. Porter, and J.-P. Onnela, Commu-
nity structure in time-dependent, multiscale, and multiplex networks, Science, 328 (2010),
pp. 876–878.

[28] L. O’Connor, M. M´edard, and S. Feizi, Maximum likelihood latent space embedding of

logistic random dot product graphs, arXiv preprint arXiv:1510.00850, (2015).

[29] B. O’Donoghue, E. Chu, N. Parikh, and S. Boyd, SCS: Splitting conic solver, version 2.1.2.

https://github.com/cvxgrp/scs, Nov. 2019.

[30] S. D. Pauls, G. Leibon, and D. Rockmore, The social identity voting model: Ideology and

community structures, Research & Politics, (2015).

[31] K. T. Poole and H. Rosenthal, A spatial model for legislative roll call analysis, American
Journal of Political Science, 29 (1985), pp. 357–384, http://www.jstor.org/stable/2111172.
[32] M. A. Porter, P. J. Mucha, M. Newman, and A. Friend, Community structure in the united
states house of representatives, Physica A: Statistical Mechanics and its Applications, 386
(2007), pp. 414 – 438.

[33] B. Recht, M. Fazel, and P. A. Parrilo, Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization, SIAM review, 52 (2010), pp. 471–501.
[34] Y. Seginer, The expected norm of random matrices, Combinatorics, Probability and Comput-

ing, 9 (2000), pp. 149–166.

[35] A. Singer, Angular synchronization by eigenvectors and semideﬁnite programming, Applied

and computational harmonic analysis, 30 (2011), pp. 20–36.

[36] D. L. Sussman, M. Tang, and C. E. Priebe, Consistent latent position estimation and vertex
classiﬁcation for random dot product graphs, IEEE Transactions on Pattern Analysis and
Machine Intelligence, 36 (2014), pp. 48–57, https://doi.org/10.1109/TPAMI.2013.135.

[37] M. Tang, C. E. Priebe, et al., Limit theorems for eigenvectors of the normalized laplacian

for random graphs, The Annals of Statistics, 46 (2018), pp. 2360–2415.

[38] M. Tang, D. L. Sussman, C. E. Priebe, et al., Universally consistent vertex classiﬁcation
for latent positions graphs, The Annals of Statistics, 41 (2013), pp. 1406–1430.
[39] R. Vershynin, High-dimensional probability: An introduction with applications in data sci-

26

D. WU, D. PALMER, AND D. DEFORD

ence, vol. 47, Cambridge university press, 2018.

[40] S. Wang, J. Arroyo, J. T. Vogelstein, and C. E. Priebe, Joint embedding of graphs, IEEE

Transactions on Pattern Analysis and Machine Intelligence, (2019).

[41] A. S. Waugh, L. Pei, J. H. Fowler, P. J. Mucha, and M. A. Porter, Party polarization in
congress: A network science approach, 2011, https://arxiv.org/abs/0907.3509.
[42] C. Yang, C. E. Priebe, Y. Park, and D. J. Marchette, Simultaneous dimensionality and
complexity model selection for spectral graph clustering, Journal of Computational and
Graphical Statistics, (2020), pp. 1–20.

[43] J. Yoder, L. Chen, H. Pao, E. Bridgeford, K. Levin, D. E. Fishkind, C. Priebe, and
V. Lyzinski, Vertex nomination: The canonical sampling and the extended spectral nom-
ination schemes, Computational Statistics & Data Analysis, 145 (2020), p. 106916.

[44] S. J. Young and E. R. Scheinerman, Random dot product graph models for social networks,
in International Workshop on Algorithms and Models for the Web-Graph, Springer, 2007,
pp. 138–149.

[45] Z. Zhang, T. Li, C. Ding, and X. Zhang, Binary matrix factorization with applications, in
Seventh IEEE international conference on data mining (ICDM 2007), IEEE, 2007, pp. 391–
400.

[46] Y. Zhao, A note on new bernstein-type inequalities for the log-likelihood function of bernoulli

variables, Statistics & Probability Letters, (2020), p. 108779.

