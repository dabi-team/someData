26ms Inference Time for ResNet-50: Towards Real-Time Execution of all DNNs
on Smartphone

9
1
0
2

y
a
M
2

]

G
L
.
s
c
[

1
v
1
7
5
0
0
.
5
0
9
1
:
v
i
X
r
a

Wei Niu 1 Xiaolong Ma 2 Yanzhi Wang 2 Bin Ren 1

Abstract

modern mobile devices is highly challenging.

With the rapid emergence of a spectrum of high-
end mobile devices, many applications that re-
quired desktop-level computation capability for-
merly can now run on these devices without any
problem. However, without a careful optimiza-
tion, executing Deep Neural Networks (a key
building block of the real-time video stream pro-
cessing that is the foundation of many popular
applications) is still challenging, speciﬁcally, if
an extremely low latency or high accuracy in-
ference is needed. This work presents CADNN,
a programming framework to efﬁciently execute
DNN on mobile devices with the help of advanced
model compression (sparsity) and a set of thor-
ough architecture-aware optimization. The eval-
uation result demonstrates that CADNN outper-
forms all the state-of-the-art dense DNN execu-
tion frameworks like TensorFlow Lite and TVM.

1. Introduction

Nowadays, car autopilot and augmented reality (AR) tech-
niques1 become increasingly popular, which is mainly
boosted by the dramatic enhancement of real-time contin-
uous video stream processing in the past few years (Chen
et al., 2015). Deep Neural Networks (DNN) such as Con-
volution Neural Networks (CNN) and Recurrent Neural
Networks (RNN) serve as the state-of-the-art foundation of
high-quality real-time continuous video stream processing.
Due to its high demand for computation power and memory
storage, processing video streams with DNN in real time on

1Department of Computation, College of William & Mary,
Williamsburg, USA 2Department of Electrical and Computer
Engineering, Northeastern University, Boston, USA. Corre-
spondence to: Wei Niu <wniu@email.wm.edu>, Bin Ren
<bren@cs.wm.edu>, Xiaolong Ma <ma.xiaol@husky.neu.edu>,
Yanzhi Wang <yanz.wang@northeastern.edu>.

Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).

1https://developers.google.com/glass/,
https://www.microsoft.com/en-us/hololens/

There have been many efforts targeting this issue, such as
DeepMon (Huynh et al., 2017), DeepX (Lane et al., 2016),
DeepSense (Yao et al., 2017), MCDNN (Han et al., 2016),
TVM (Chen et al., 2018), TensorFlow Lite (Lite, 2017), etc.;
however, most of them do not explore the possible optimiza-
tion opportunities like computation and memory footprint
reductions offered by model compression, including weight
pruning (Han et al., 2015; Wen et al., 2016; Zhang et al.,
2018a) and weight quantization (Zhou et al., 2017; Wu et al.,
2016; Hubara et al., 2016; Rastegari et al., 2016). There-
fore, a signiﬁcant performance gap still exists between the
peak performance that can be potentially offered by state-
of-art mobile devices and what the existing systems actually
achieved.

There are two major obstacles when leveraging model com-
pression to improve the DNN inference execution perfor-
mance in the mobile environment: ﬁrst, model compression
(like weight pruning/sparsity and weight quantization) usu-
ally results in accuracy degradation in inference; and second,
the computation pattern of compressed models becomes
more irregular, causing more severe data locality and load
balancing issues and signiﬁcantly increasing the difﬁculty
in optimizations.

Within this context, this work proposes CADNN, a program-
ming framework to efﬁciently execute DNN inference on
mobile devices with a more advanced model compression
method that is designed to minimize the accuracy drop and
maximize the model compression rate, and a set of care-
ful architecture-aware optimizations that can effectively
address the extra irregularity brought by the model com-
pression. To our best knowledge, CADNN offers the most
thorough study of optimizing compressed DNN on mobile
devices and achieves the most signiﬁcant performance gains
compared to existing state-of-the-art dense DNN execution
frameworks.

The contribution of this work is as follows:

• CADNN adopts the advanced model compression
method that currently achieves the highest weight
pruning rates. It can result in 348×, 36×, and 8×
weight pruning rates with (almost) zero accuracy loss

 
 
 
 
 
 
CADNN

two subproblems that can be solved separately and efﬁ-
ciently (Boyd et al., 2011). Consider optimization problem
minx f (x) + g(x). In ADMM, it is decomposed into two
subproblems on x and z (auxiliary variable), which will be
solved iteratively until convergence. The ﬁrst subproblem
derives x given z: minx f (x) + q1(x|z). The second sub-
problem derives z given x: minz g(z) + q2(z|x). Both q1
and q2 are quadratic functions.

As a special property, ADMM can effectively deal with a
subset of combinatorial constraints and yield optimal (or
at least high quality) solutions (Hong et al., 2016). The
associated constraints in DNN weight pruning belong to
this subset of combinatorial constraints, making ADMM
applicable to this speciﬁc problem. In weight pruning prob-
lem, f (x) is the DNN loss function and the ﬁrst subproblem
is DNN training with dynamic regularization, which is to-
tally compatible with current gradient descent techniques
and solution tools for DNN training. g(x) corresponds
to the combinatorial constraints on the number of weights
for weight pruning. As a result of the compatibility with
ADMM, the second subproblem has an optimal, analytical
solution via Euclidean projection.

The ADMM-based method achieves state-of-art weight
pruning results, e.g., 21× overall weight reduction in
AlexNet. However, it lacks the algorithm-wise guarantee
of solution feasibility, i.e., all constraints should be satis-
ﬁed. We propose to extend over (Zhang et al., 2018a) in the
following three aspects. First, we propose an integration
of ADMM regularization and masked mapping and retrain-
ing. The former step resembles (Zhang et al., 2018a) while
the later step guarantees solution feasibility and further im-
proves solution quality (in terms of pruning rates). Second,
we propose a uniﬁed solution framework of weight pruning
and weight quantization using ADMM. For weight quantiza-
tion, the combinatorial constraints are also compatible with
ADMM. g(x) corresponds to the combinatorial constraints
on the values each weight could take, and the second sub-
problem again has an optimal analytical solution. Third, we
also develop effective techniques, including multi-ρ tech-
nique and progressive model compression, for enhancing
convergence speed and quality. We release codes and mod-
els at the anonymous link: http://bit.ly/2WMQSRi.

Our proposed framework achieves the best-in-class weight
pruning rate, as well as for weight pruning combined with
quantization. For non-structured weight pruning alone, we
achieve 348× overall weight reduction (only 0.28% re-
maining weights) in LeNet-5, 36× in AlexNet (ImageNet
dataset), 34× for VGGNet, and 9.2× on ResNet-50, with
(almost) no accuracy loss, 2× to 28× improvement over
competing methods. We achieve a reduction of up to 3,438×
in weight storage (using LeNet-5 model, not accounting for
indices), with almost no accuracy loss when weight pruning

Figure 1. Overview of CADNN

for LeNet-5, AlexNet, and ResNet-18 models, respec-
tively.

• CADNN carefully studies the performance challenges
brought by model compression and offers a set of
architecture-aware optimizations, such as computation
pattern transformation, redundant memory load elim-
ination, smart selection of memory and computation
optimization parameters, etc.

• CADNN is evaluated on a modern mobile platform
with its CPUs and GPUs respectively and demonstrates
superior execution performance.

Speciﬁcally, compared to the state-of-art dense DNN execu-
tion frameworks like TensorFlow Lite and TVM, CADNN
can achieve up to 8.8× and 6.4× speedup according to our
evaluation on 4 popular DNNs used in mobile applications,
including Inception-V3, MobileNet, and Resnet50.

2. Overview

Figure 1 illustrates the design overview of CADNN frame-
work that includes two stages, compression with weight
pruning and a set of architecture-aware optimization. In
particular, architecture-aware optimization consists of three
major components, model computation fusion and transfor-
mation, memory layout transformation and load optimiza-
tion, and optimization parameters selection. The basic idea
of each part is explained in the following sections.

3. Uniﬁed DNN Model Compression

Framework using ADMM

Recent work (Zhang et al., 2018a) (Zhang et al., 2018b) (Ye
et al., 2019) has proposed a systematic DNN weight pruning
technique using the advanced variable-splitting optimiza-
tion method ADMM (Alternating Direction Methods of
Multipliers) (Boyd et al., 2011). ADMM is a powerful op-
timization tool, by decomposing an original problem into

Architecture-aware OptimizationDNN ModelCompressionComputationFusion & TransformMemoryLayout Transform & LoadOptimization Parameters SelectionWeight Pruningand quantization are combined, outperforming the state-of-
art by two orders of magnitude.

state-of-the-art framework).

CADNN

4. Architecture-aware Optimization

CADNN supports three major optimizations targeting mod-
ern mobile architectures as follows:

Model computation fusion and transformation After
weight pruning, each layer’s computation is signiﬁcantly
reduced; however, their memory access becomes much
more irregular. This further magniﬁes the memory wall
issue of mobile devices, rendering the execution increas-
ingly memory-bound. CADNN therefore explores every
opportunity in DNN to fuse multiple layers into larger com-
putation kernels (e.g. Convolution layer/Depthwise Convo-
lution layer + BatchNorm layer + Activation layer in Mo-
bileNetV1), yielding beneﬁts in two aspects: ﬁrst, reducing
the intermediate irregular memory read and write therefore
improving memory performance; and second, packing more
computation workloads together therefore increasing the
SIMD (Single-Instruction Multiple Data) utilization. In par-
ticular, for convolution layers with 1 × 1 ﬁlters, CADNN is
able to further transform the convolution operation into ma-
trix multiplication operation to further improve its memory
and SIMD performance.

Memory layout transformation and load optimization
To further improve the data locality, CADNN also trans-
forms the memory layout of DNN’s ﬁlters to ﬁt CPU and
GPU, respectively. More speciﬁc techniques include tiling,
alignment, and padding. Speciﬁcally, based on a key obser-
vation that many elements in ﬁlters of convolution layers
are repeatedly loaded to registers, CADNN implements a
compiler code transformation to eliminate such redundant
memory loads.

Optimization parameters selection Above optimizations
work for varied DNNs on both CPU and GPU, however,
with very different parameters, e.g., for a speciﬁc DNN, the
best tile sizes for CPU and GPU are different from each
other, and the best tile sizes for varied layers are also dif-
ferent. A large number of optimization parameters (such as
tiling sizes on multiple dimensions, unrolling sizes, possible
computation reorders, etc.) imposes a big challenge for us to
select the best conﬁguration. CADNN adopts an optimized
tuning approach by pruning the redundant or sub-optimal
conﬁgurations with the knowledge from both DNNs and ar-
chitectures, and then uses a compiler source-to-source code
transformation to generate optimized computation kernels.

5. Evaluation

We compare CADNN’s performance with TensorFlow Lite
(a popular framework on the mobile device), and TVM (the

Figure 2. Inference performance comparison on CPU/GPU:
CADNN-DC (CADNN dense on CPU), CADNN-DG (CADNN
dense on GPU), CADNN-SC (CADNN compressed on CPU),
CADNN-SG (CADNN compressed on GPU), TFLITE-DC (Ten-
sorFlow Lite dense on CPU), TVM-DC (TVM dense on CPU),
and TVM-DG (TVM dense on GPU)

Platform CADNN is evaluated on a Xiaomi 6 cell phone
whose detailed conﬁguration is summarized in Table 1.

Table 1. Device information of Xiaomi 6 with Android 8.0.0

SOC
GPU
Memory

Snapdragon 835, up to 2.45GHz
Adreno 540, 710MHz
6GB shared by CPU and other devices

DNNs and data-set Table 2 characterizes the four DNNs
used in our evaluation, showing the model name, size, top-1
accuracy, top-5 accuracy, and the number of layers. All tests
are performed on the ImageNet data-set.

Performance Figure 2 shows the performance compari-
son between CADNN and TensorFlow Lite/TVM. CADNN
supports both dense and compressed models while Ten-
sorFlow Lite and TVM support only dense models. We
show CADNN and TVM’s performance on both CPU and
GPU, and TensorFlow Lite on CPU only. On both CPU and
GPU, CADNN outperforms the other two, achieving up to
6.4× and 6× speedup over TVM (that is better than Tensor-
Flow Lite) on CPU and GPU, respectively. This result also
demonstrates CADNN is super fast, and with relatively large
DNNs, Inception-V3 and Resnet50, it can ﬁnish inference
of a single image within 35 ms and 21 ms, respectively.

6. Conclusion and Work in Progress

This work presents CADNN, a programming framework
to efﬁciently execute DNN inference on mobile devices
with the help of a more advanced model compression and
a set of architecture-aware optimizations. Our evaluation
shows that CADNN is super fast, achieving up to 8.8× and

0100200300400500600700Mobilenet-V1Mobilenet-V2Inception-V3Resnet50Time(ms)CADNN-DCCADNN-DGCADNN-SCCADNN-SGTF-DCTVM-DCTVM-DGCADNN

Table 2. DNN Conﬁgurations

Model
MobileNet-V1
MobileNet-V2
Inception-V3
Resnet50

Size(M) Top1 (%) Top5 (%) Layer
17.1
14.1
95.4
102.4

89.9
91.0
93.9
92.2

70.9
71.9
78.0
75.2

31
66
126
94

6.4× speedup over TensorFlow Lite and TVM, two popular
and highly optimized dense DNN execution frameworks.
We plan to further optimize CADNN in two aspects: a
smarter optimization parameters selection scheme based on
other Machine Learning techniques, and a DNN proﬁler on
mobile devices to better detect the performance bottleneck
of DNN execution.

References

Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al.
Distributed optimization and statistical learning via the
alternating direction method of multipliers. Foundations
and Trends R(cid:13) in Machine learning, 3(1):1–122, 2011.

Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen,
H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. {TVM}:
An automated end-to-end optimizing compiler for deep
learning. In 13th {USENIX} Symposium on Operating
Systems Design and Implementation ({OSDI} 18), pp.
578–594, 2018.

Chen, T. Y.-H., Ravindranath, L., Deng, S., Bahl, P., and
Balakrishnan, H. Glimpse: Continuous, real-time object
recognition on mobile devices. In Proceedings of the
13th ACM Conference on Embedded Networked Sensor
Systems, pp. 155–168. ACM, 2015.

Han, S., Pool, J., Tran, J., and Dally, W. Learning both
weights and connections for efﬁcient neural network. In
NeurIPS, 2015.

Han, S., Shen, H., Philipose, M., Agarwal, S., Wolman, A.,
and Krishnamurthy, A. Mcdnn: An approximation-based
execution framework for deep stream processing under
resource constraints. In Proceedings of the 14th Annual
International Conference on Mobile Systems, Applica-
tions, and Services, pp. 123–136. ACM, 2016.

Hong, M., Luo, Z.-Q., and Razaviyayn, M. Convergence
analysis of alternating direction method of multipliers
for a family of nonconvex problems. SIAM Journal on
Optimization, 26(1):337–364, 2016.

Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y. Binarized neural networks. In NeurIPS, pp.
4107–4115, 2016.

Huynh, L. N., Lee, Y., and Balan, R. K. Deepmon: Mobile
gpu-based deep learning framework for continuous vision
applications. In Proceedings of the 15th Annual Interna-
tional Conference on Mobile Systems, Applications, and
Services, pp. 82–95. ACM, 2017.

Lane, N. D., Bhattacharya, S., Georgiev, P., Forlivesi, C.,
Jiao, L., Qendro, L., and Kawsar, F. Deepx: A soft-
ware accelerator for low-power deep learning inference
on mobile devices. In Proceedings of the 15th Interna-
tional Conference on Information Processing in Sensor
Networks, pp. 23. IEEE Press, 2016.

Lite, T. Android to launch tensorﬂow lite for mobile ma-

chine learning, 2017.

Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
Xnor-net: Imagenet classiﬁcation using binary convolu-
tional neural networks. In ECCV, 2016.

Wen, W., Wu, C., Wang, Y., Chen, Y., and Li, H. Learning
structured sparsity in deep neural networks. In NeurIPS,
pp. 2074–2082, 2016.

Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized
convolutional neural networks for mobile devices.
In
CVPR, 2016.

Yao, S., Hu, S., Zhao, Y., Zhang, A., and Abdelzaher, T.
Deepsense: A uniﬁed deep learning framework for time-
series mobile sensing data processing. In Proceedings of
the 26th International Conference on World Wide Web,
pp. 351–360. International World Wide Web Conferences
Steering Committee, 2017.

Ye, S., Feng, X., Zhang, T., Ma, X., Lin, S., Li, Z., Xu,
K., Wen, W., Liu, S., Tang, J., et al. Progressive dnn
compression: A key to achieve ultra-high weight prun-
ing and quantization rates using admm. arXiv preprint
arXiv:1903.09769, 2019.

Zhang, T., Ye, S., Zhang, K., Tang, J., Wen, W., Fardad, M.,
and Wang, Y. A systematic dnn weight pruning frame-
work using alternating direction method of multipliers.
In arXiv, 2018a.

Zhang, T., Zhang, K., Ye, S., Li, J., Tang, J., Wen, W., Lin,
X., Fardad, M., and Wang, Y. Adam-admm: A uniﬁed,
systematic framework of structured weight pruning for
dnns. arXiv preprint arXiv:1807.11091, 2018b.

Zhou, A., Yao, A., Guo, Y., Xu, L., and Chen, Y. Incre-
mental network quantization: Towards lossless cnns with
low-precision weights. arXiv preprint arXiv:1702.03044,
2017.

