Smart and Context-Aware System employing
Emotions Recognition

Stuti Sehgal
Department of Computer Science
and Engineering
SRM Institute of Science and Technology,
Kattankulathur, Tamil Nadu– 603203,
India
ss3537@srmist.edu.in

Harsh Sharma
Department of Computer Science
and Engineering
SRM Institute of Science and Technology,
Kattankulathur, Tamil Nadu– 603203,
India
hs7685@srmist.edu.in

Akshat Anand
Department of Computer Science
and Engineering
SRM Institute of Science and Technology,
Kattankulathur, Tamil Nadu– 603203,
India
aa4407@srmist.edu.in

1
2
0
2

n
u
J

9
2

]

C
H
.
s
c
[

1
v
1
0
1
5
1
.
6
0
1
2
:
v
i
X
r
a

Abstract—People have the ability to make sensible assumptions
about other people’s emotional states by being sympathetic, and
because of our common sense of knowledge and the ability to
think visually. Over the years, much research has been done on
providing machines with the ability to detect human emotions
and to develop automated emotional intelligence systems. The
computer’s ability to detect human emotions is gaining popularity
in creating sensitive systems such as learning environments,
health care systems and real-world. Improving people’s health
has been the subject of much research. This paper describes the
formation as conceptual evidence of emotional acquisition and
control in intelligent health settings. The authors of this paper
aim for an unconventional approach with a friendly look to get
emotional scenarios from the system to establish a functional,
non-intrusive and emotionally-sensitive environment where users
can do their normal activities naturally and see the program only
when pleasant mood activating services are received. The context-
sensitive system interacts with users to detect and differentiate
emotions through facial expressions or speech recognition, to
make music recommendations and mood color treatments with
the services installed on their IoT devices.

Keywords—human emotions, color therapy, music recommen-

dation, human computer interaction, context aware system

I. INTRODUCTION

Emotional monitoring is important as it contains informa-
tion that can help improve people’s well-being. Behavioral
automated modeling is essential for the development of mod-
ern applications in AI. Automatic emotional recognition has
many applications in areas where equipment or machines
need to communicate or monitor people. Gen-Y does not
have time to visit psychiatrists as they are busy with devices
connected on the internet, and during such testing times,
people’s mental health has taken a toll. In today’s world,
stigma and subjugation associated with mental health continue
to plague mankind. An efﬁcient system can greatly help people
to improve their emotional state by monitoring their behavior.
To ﬁx this subjugation; electrodes penetrating the skin are
attached to the head in an electroencephalogram (EEG) for
detecting human emotions by analyzing delta, theta, alpha,
beta, and gamma-band waves [1]; or hefty fees are paid to
psychiatrists; mood detection using facial expressions and
speech sensitivity becomes need of the hour.

An alternative approach not undertaken by the authors of
this paper, heterogeneous and wearable sensors send biometric
data to monitor the subject, generating large amounts of
sensitive data (e.g. images) [2]. Also, the location where such
systems are installed should be considered and agreed upon
by users, and it is considered that the sensors are located
in strategic locations that require the optimal location of the
target. In line with ethical standards, monitoring a person in
their private living space should be considered a sensitive
issue. [3]

Concretely, a person’s emotional state can be reﬂected in
a certain image or speech. This problem is widely studied in
computer view especially on two sides:

(1) facial analysis [4], and (2) posture and gesture analysis.
Face Recognition (FER) [5] can be widely used in a variety
of research areas, such as psychiatric diagnosis and social /
physical interactions. Speech emotion recognition (SER) [6] is
still a challenging task in the ﬁeld of high computation because
there are no deﬁned levels of sensitivity. The speech signal
contains large amounts of information related to the emotions
conveyed by a person. The speech recognition system fails
miserably if powerful strategies are not developed to deal with
speech sensitive emotion recognition.

The authors of this paper propose a context-aware system
consisting of user-controlled device cameras such as video
sensors and microphones as sound sensors, for interpreting fa-
cial expressions from static image annotations and companion
text or speech sensitive emotion recognition, aimed to create
a smart, advanced AI environment for sentiment analysis.
A non-intrusive approach has been demonstrated, using user
devices equipped with web cameras as image sensors to
perform facial recognition (FER) and microphones to perform
speech emotion recognition (SER). Due to the proliferation of
inexpensive sensors such as cameras and microphones, as well
as embedded processors, there are unprecedented opportunities
for realizing real-time applications such as real computer
immersion, and gesture control, and other human-centered
applications such as smart and assistive environments.

Smart environments are important for the well-being of
people with disabilities as it can greatly improve their daily

 
 
 
 
 
 
lives. People with mobility impairments tend to choose camera
and speech connectors, as this is customized, comfortable,
expensive, and does not require user-borne accessories that
can draw attention to their disability. Image and speech input
is processed in real-time so that users can work effectively
with assistive software to improve their lifestyle.

The remainder of this paper is structured as follows. In
Section II, a review of the previous research pertaining to
Emotion recognition and Context-Aware systems is presented
and in Section III, we discuss the Emotions Module of this
research which includes a detailed description of the proposed
system. Music recommendation and Color therapy actuation
services of the proposed system have been introduced in
sections IV and V, respectively followed by the analysis of the
results of the experiments in Section VI and the concluding
remarks in Section VII. Future work and visualization of a
truly efﬁcient, smart, natural and healthy environment system
has been discussed in Section VIII.

II. LITERATURE REVIEW

Recent research has focused on improving the quality of
life of a person by designing and building sensors that are
directly or indirectly connected to the human body [3]. This
method, previously researched in the ﬁeld of intelligent design,
however intrusive, is a fusion of a number of components
including physiological signals (blood volume and heart rate,
respiration, EEG, and skin conductance), sensory integration,
human-computer connectors, networking, and extensive com-
puting for the purpose of multimodal recognition of emotions
and actuation of response services [2]. One of the most
challenging domains in multi-sensor environments [7] is the
automatic analysis of multi-group conversations from sensory
data.

The importance of context in emotion perception is well
supported by various studies in psychology. Non-verbal com-
munication is important, as it is substantially documented in
social psychology and cognition, it opens up new opportunities
for new cameras and microphone-equipped spaces [8]. Human
interactions with the computer will be natural and effective
when interfaces are dealing with human emotions or stress.
Previous studies have focused on the acquisition of emotion
via multiple sensors but the recognition of facial expression
and speech sensitive emotion is gaining importance because of
its broader system. From a computer vision perspective, most
previous attempts have focused on analyzing facial expressions
and, in some cases, on body movements and gestures. Some
in certain settings. The two
of these methods work best
most popular methods used in the literature for automated
FER systems are based on geometry and appearance. Most
researchers use variations of Russel’s circumplex model (Fig.
1) which provides a distribution of basic emotions in a dual
space in relation to valence and arousal [9]: Valence (V),
which measures how pleasant emotions are, from bad to good;
Arousal (A), which measures a person’s level of disturbance,

from inactivity / calm to anger/willingness to act; and Domi-
nance (D) which measures the level of control a person feels
about the condition, from compliance / uncontrolled to in-
control / management.

Fig. 1. The Russel’s circumplex model of emotions

As emotion recognition presumes the modeling of the
dynamics of acoustic or visual features, some classiﬁcation
strategies in the ﬁeld of AER make use of dynamic clas-
siﬁers like Hidden Markov Models (HMM) and Dynamic
Bayesian Networks (DBN) [10]. Alternative strategies apply
static techniques such as Support Vector Machines (SVM)
that process statistical functions of low-level features which
are computed over longer data segments [11]. Quality of
the human-computer interface that mimics human speech
emotions relies heavily on the types of features used and also
on the classiﬁer employed for recognition [12]. A unimodal
framework for short-term context modeling in dyadic interac-
tions was proposed in [13].

The emotional state of a speaker can be identiﬁed from
the facial expression (Ekman, 1973; Davis and College, 1975;
Scherer and Ekman, 1984) [1] and spoken words (McGilloway
et al., 2000; Dellaert et al., 1996; Nicholson et al., 1999).
Ultimately, a combined analysis of these features leads to high
accuracy of recognition. In this paper, the focus is multimodal:
on facial expression and speech emotion recognition, allowing
for the deﬁnition of a desired emotion and evaluating its
intensity. [14]

III. EMOTIONS MODULE:

A. Dataset Construction

Over the past decade, much effort has been put into building
facial expressions and speech emotion recognizers. Because
most researchers use limited data sets, the generalizability of
these various methods remains unknown. Currently, techno-
logical approaches to image-related activities such as image
classiﬁcation and object acquisition are all based on Convo-
lution Neural Networks (CNNs). These operations require the

construction of CNN with millions of parameters. Commonly
used CNN extraction features include a set of layers that are
fully integrated at the end. Fully connected layers most of the
parameters in a CNN [15].

The basis of FER is the Facial Action Coding System,
which incorporates facial expressions using a set of speciﬁc
local facial movements, called Action Units. These facial-
based approaches often use facial geometry-based features to
describe the face. Afterward, the extracted features are used
to identify Action Units which include regions such as eyes,
cheeks, lips that are used to perceive basic emotions: anger,
joy, sadness and neutral speech.

Two datasets were used in this research work. The ﬁrst
dataset was Extended Yale Face database (Fig. 2) consisting
of:

Six basic emotion classes: anger, happy, neutral, sad and
disgust containing 16128 images of 28 human subjects under
9 poses and 64 illumination conditions. The dataset creator
had cropped the images to face-only, performed operations like
gray-scaling and resizing to 48x48. The second dataset was the
Kaggle Facial Emotion Recognition (FER2013) dataset [16]
[17] comprising of 35,887 images, each 48 x 48 pixels (8-bit
grayscale) with disgust being the only underrepresented one
within the Kaggle dataset (Fig. 2), at 1.

We found these datasets to be representative because of
their size, and unstructured nature of faces (in terms of facial
orientation, ethnicity, age, and gender of the subjects). Data
Preprocessing Steps:

• Manually cleaning the datasets to remove duplicates, mis-
classiﬁed expressions and bias due to underrepresented
class labels of disgust and fear emotions

• Splitting the data into train, validation and test (80:10:10
= 28275 train, 3530 test, 3532 validation were the number
of images taken)

• Applying image augmentation using image data generator
in Tensorﬂow, images were normalized to 0-255, +/- 15°
rotation, +15° zoom and horizontally ﬂipped

• The song ﬁle contains 1012 ﬁles: 44 trials per actor x 23

actors = 1012

• The ﬁles are in the WAV raw audio ﬁle format and all
have 16-bit bitrate and a sample rate of 48 kHz. All ﬁles
are uncompressed, lossless audio, which means that audio
ﬁles in the database have not lost any information/data or
modiﬁed from the original recording.

• We found this dataset to be representative because the
dataset is gender-balanced with 24 professional actors,
12 males and 12 females.

• Audio ﬁles are created in a controlled environment and
each contains the same statements spoken in the Ameri-
can manner. Speech emotion experiments are widely done
in Western side of the world so the data is usually biased
towards the American accent.

Data Processing and SER Exploration essentially uses the

following for feature extraction: -

1) Mel Scale - deals with the perception of frequency,
the scale of the pitches judged by the audience to be
equidistant from each other.

2) Pitch - determines how high or low the sound is.
Depending on the frequency, the higher the pitch the
higher the frequency.

3) Frequency - the vibration speed of sound, measuring

wave cycles per second.

4) Chroma - A sound representation where the spectrum is
displayed in 12 bins representing 12 different semitones
(or chroma). It is calculated by summarizing the size of
the log frequency spectrum across octaves.

5) Fourier Transforms - used to convert from time domain
to frequency domain. Time-domain shows how the sig-
nal changes over time. The frequency-domain indicates
how much signal is inside each frequency band, given a
range of frequencies. [19]

Mel-frequency cepstrum coefﬁcient (MFCC) is the most used
representation of the spectral property of voice signals. These
are the best for speech recognition as it takes human perception
sensitivity with respect to frequencies into consideration. For
each frame, the Fourier transform and the energy spectrum
were estimated and mapped into the Mel-frequency scale.

Fig. 2. Dataset used for training and testing [17]

• Haar Cascades to crop out only faces from the images

from live feed while getting real-time predictions

We chose RAVDESS dataset [18] for Speech Emotion

Recognitions :-

• It contains speech and song ﬁles by 247 untrained Amer-
icans in eight different emotions (Calmness, joy, sadness,
anger, fear, disgust, and surprise, and neutrality) at two
levels of intensity.

• The talk ﬁle contains 1440 ﬁles: 60 attempts per actor x

24 players = 1440

Fig. 3.

Image output of the audio by libROSA python library

SER Dataset preprocessing steps:
The raw audio ﬁle (.wav) is ﬁrst supposed to be prepro-
cessed in order to be classiﬁed as multi-class classiﬁcation. For

that, we have used Librosa library to convert the raw audio
.wav ﬁle to a spectrogram image and for feature extraction
like chroma, pitch, and frequency. The image thus generated
is the spectrogram image and is ready to be fed in our neural
network.

Fig. 4. Mel spectrogram for a characteristic emotion

B. Proposed Architecture

The FER and SER models (Fig. 5) have been designed
with the idea of attaining the best generalization accuracy. The
former layer of each block represent the layers of the proposed
FER model. The latter layer of each block represent the layers
of the proposed SER model architecture.

We propose a model for four-class facial emotion classiﬁ-
cation (anger, happy, sad and neutral) which we evaluate in
accordance to their test and generalization accuracy. These
primary emotions are often referred to as “archetypal” emo-
tions. Although these archetypal emotions cover a rather small
part of emotions in psychological research, they represent the
popularly known emotions and are recommended for testing
the capabilities of an automatic emotion recognizer. The FER
model(Fig. 5) was designed with the idea of attaining the best
generalization accuracy.

1) Input Layer: The input layer has ﬁxed and predetermined
dimensions. So, for pre-processing the detected face, we
used OpenCV library for face detection in the image
before feeding it into the layer by passing through pre-
trained ﬁlters from Haar Cascades.

2) Image data generator: 48x48 size gray-scale train and
validation images dataset was divided into a batch size
of 64.

3) Warming-up Model: VGG-16 was used as the trans-
fer learning model. After importing it, we set
lay-
ers.trainable as False, and select a favorable output layer,
in this case, ‘block5 conv1’. This freezes the transfer
learning model so that we can pre-train or ‘warm up’ the
layers of the sequential model on the given data before
starting the actual training. This helps the sequential
model to adjust weights by training on a lower learning
rate.

4) Fine tuning model: Deﬁning the Model, Using Sequen-

tial, the layers in the model are as follows:

• GlobalAverage- Pooling2D
• Flatten
• Dense (256, activation: ‘relu’)

• Dropout (0.4)
• Dense (128, activation: ‘relu’)
• Dropout (0.2)
• Dense (4, activation: ‘softmax’)

5) Training Speciﬁcations: The pre-training is done by
using RMSProp at learning rate: 1e-5 and for 30 epochs.
After pre-training, we set layers.trainable as True for
the whole model. Now the actual training will start. It is
done by taking Adam optimizer at learning rate: 1e-4 for
25 epochs. Setting the Hyper Parameters and constants
(Only the best parameters are displayed below):

• Batch size: 64
• Image Size: 48 x 48 x 3
• Optimizers: o RMS Prop (Pre-Train) o Adam
• Learning Rate: o Lr1 = 1e-5 (Pre-Train) o Lr2 =

1e-4

• Epochs 1 = 30 (Pre-Train) o Epochs 2 = 25
• Categorical Cross Entropy H(p, q) was chosen as
loss function and the equation is given as follows,
n
(cid:88)

H(p, q) = −

pi log qi

(1)

i=1
where pi is the truth label and qi is the softmax
probability for iith class and n is the number of
classes or 4.

We propose a model(Fig. 5) for sixteen-class Speech emo-
tion recognition classiﬁcation (neutral, calm, happy, sad, angry,
fearful, disgust, surprise, 8 for female and 8 for male speech)
which we evaluate in accordance with their test and gener-
alization accuracy. The model was designed to get the best
generalization accuracy.

1) Image Data Segregation: All the images are of RGB
Channels, separated with a ﬁxed batch size of 16, and
are segregated based on the actual 16 emotion classes.
2) A custom class of neural networks has been deﬁned to
achieve the goal. In this class, we ﬁrst deﬁned the core
sequential model with the image data generator as a pre-
training parameter. Convolution 1-D Layers along with
normal Dense layers are used for the core model. Since
this was a multi-class classiﬁcation, we implemented a
Conv1D Model along with a ’selu’ activation function
initially and a ‘softmax’ activation function in the end
with a relevant optimizer like ‘adam’ to implement the
core model for this purpose.

3) Fine Tuning: After Deﬁning the initial architecture,
the following were implemented to ensure the smooth
process of training and to expect a better-generalized
accuracy:

• Conv1D(256,input shape=(66,1))
• Activation(‘selu’)
• Batch Normalisation
• Dropout(0.4)
• Flatten
• Dense
• Activation(‘selu’)

• Batch Normalisation
• Dropout(0.4)
• Dense(16)

4) Training Speciﬁcations: The training has been ensured
to have a smooth learning process with help of ‘Adam’
Optimiser with learning rate=0.001 for 65 epochs. Early
callbacks have been imposed with a patience=10, to wait
initially for achieving better accuracy and minimizing
the loss function along with stopping the training if the
learning worsens. Also, we deﬁned a custom class to di-
rectly export the model with the best results achieved,i.e,
best accuracy in the entire training epochs in between,
and to keep track of whether the next epoch had a better
accuracy or the previous one.

C. Emotion Recognition Module

The system has to decide automatically the proper reaction
in response to the detected emotion, in order to simulate a
positive response in the user. Decision making can vary from
one user to another and also in accordance with his/her health
evolution. It is in “Decision making” where the intelligence
of the system lies [20]. Considering the emotion detected the
system reacts and decides the action to execute. For example,
considering that our system reacts to the user’s mood, once
this is detected, the system will vary the color and change
the music. For instance, if the action is to create a relaxing
environment, the system will launch commands addressed to
project warm colors, and play pleasant music (in broad terms)
[21].A variety of application services in ambient intelligence
environments can be realized at reduced cost by encapsulat-
ing attractive services like music recommendation [22] and
color/light-actuation in middleware applications that are shared
by the IoT connected devices. High level abstractions offered
by middle-ware infrastructures make it possible to hide the
complexities in ambient intelligence environments.

input 1 : Inputlayer

block1 conv1: Conv2D (FER)
conv1d: Conv1D (SER)

block1 conv2: Conv2D (FER)
activation: Activation (SER)

block1 pool: MaxPooling2D (FER)
conv1d 1: Conv1D (SER)

block2 conv1: Conv2D (FER)
batch normalisation: BatchNormalization (SER)

block2 conv2: Conv2D (FER)
activation 1: Activation (SER)

block2 pool: MaxPooling2D (FER)
dropout: Dropout (SER)

block3 conv1: Conv2D (FER)
conv1d 2: Conv1D (SER)

block3 conv2: Conv2D (FER)
activation 2: Activation (SER)

block3 conv3: Conv2D (FER)
conv1d 3: Conv1D (SER)

block3 pool: MaxPooling2D (FER)
batch normalisation 1: BatchNormalization (SER)

block4 conv1: Conv2D (FER)
activation 3: Activation (SER)

block4 conv2: Conv2D (FER)
dropout 1: Dropout (SER)

block4 conv3: Conv2D (FER)
ﬂatten: Flatten (SER)

block4 pool: MaxPooling2D (FER)
dense: Dense (SER)

block5 conv1: Conv2D (FER)
activation 4: Activation (SER)

block5 conv2: Conv2D (FER)
dense 1: Dense (SER)

block5 conv3: Conv2D (FER)
activation 5: Activation (SER)

block5 pool: MaxPooling2D (FER)
batch normalisation 2: BatchNormalization (SER)

Fig. 6. A glimpse of our application

global average pooling2d: GlobalAveragePooling2D (FER)
activation 6: Activation (SER)

dense: Dense (FER)
dropout 2: Dropout (SER)

dropout: Dropout (FER)
dense 2: Dense (SER)

Output: Dense (FER)
activation 7: Activation (SER)

Fig. 5. Architecture of FER and SER model

IV. MUSIC RECOMMENDATION MODULE

Balkwill and Thompson found that even listeners who are
unfamiliar with the tonal system are sensitive to the emotion
expressed by a piece of music. Hunter et al. explored how
tempo and musical harmony affect the emotional responses of
listeners [23]. They found that quick-tempo and high-speed

routes add to the excitement, while slow-moving, low-key
modes make listeners feel sad. (Interestingly, both levels of
happiness and sorrow were high when fast tempo was com-
bined with major modes or slower tempos with minor modes.)
These ﬁndings are consistent with Hevner’s conclusions, and
support
the use of tempo and tonality modulation within
the proposed application. Zentner, Grandjean, and Scherer’s
Lundqvist et al. [7] demonstrated that listeners’ responses
to happy stimuli generated “more zygomatic facial muscle
activity, more pleasure, and less sadness” than sad stimuli
which is often seen in greater magnitude than it may sound.
These results can be used as a basis for building a response to
the proposed plan. Of Saarikallio’s seven strategies for MMR,
AMAI [23] focuses on discharge (”venting anger or sadness
through music that expresses these emotions”), diversion (”for-
getting thoughts and feelings with the help of good music ”),
and progression from discharge to diversion. Given the results
of Lundqvist et al, their ﬁndings have conﬁrmed that the use
of subsequent diversion can have a consistent effect on all
listeners rather than the use of diversion technique alone.

In this paper we described the design, and implementation
of a system for generation and playback of music that adapts
to listeners’ affective states with the goal of increasing positive
affect- “diversion”. In particular, we explored the effectiveness
of employing the diversion MMR strategy. A case has been
made of dynamic music that has the ability to direct the
listener results more efﬁciently than standard music such as

V. COLOR/LIGHT ACTUATION

Color is another factor that is omnipresent in an environ-
ment. Moreover, evidence of the inﬂuence of color on human
emotions is validated by metonymic and metaphoric thinking,
formation of speciﬁc emotional reactions for color perception,
and sharing connotative structure in the language for color
and emotion terms. Color is a powerful communication tool
and can be used to express action, inﬂuence emotions, and
inﬂuence the body’s response. Certain colors are associated
with increased blood pressure, weight gain, and eyestrain.
Many authors emphasize that certain colors have a profound
effect on mood and control. For instance, in a trial, muscle
strength is decreased within 2: 7 s inmates in prisons who
resided for a limited time in bubble-gum pink cells (Baker-
Miller pink) cells [7]. It is important to note that it has been
shown that color features such as chroma, hue, or light also
have an effect on emotions. It is known that while perceiving
color, the brain associates it with a particular emotion. This
phenomenon is known as color emotion. One can appreciate
the colors that provide the desired change from negative to
positive according to previous works. Also, depending on the
initial sensitivity to the orthogonal scale, we have(ﬁg. 8):

Activated Unpleasant Effect

Color: Blue/Purple
Reason: Calm, serene, more produc-
tive

Unwanted Unpleasant Effect

Color: Yellow/Pink/Green
Reason: Uplifting effect on mood,
symbolizes health

Unactivated Unpleasant Effect

Color: Orange →Red
Reason: Strike Positivity

Activated Pleasant Effect

Color: Blue →Yellow
Reason: Person maintained ”pleas-
ant”

Fig. 7. Architecture of proposed system

Fig. 8. Schematic ﬂowchart of Color/Light Actuation

MP3, like in the case of AMAI, the conversion from discharge
to diversion was controlled by the user’s situation, with the
system having control of the precise timing of when to switch
sections and strategies. Using Selenium automation in Python,
whenever you make prediction from the proposed model, you
get a word as emotion – ‘ANGRY’, ‘HAPPY’, ‘NEUTRAL’,
‘SAD’ which is used in automation [16] for parsing the
YouTube web pages using ‘chromedriver 87.0.4280.88’ for
automatically playing music to simulate a positive effect
when users’ facial emotion is detected, and redirects to the
recommended YouTube MP4 video.

Fig. 9. Application of the color therapy for emotion sad

VI. RESULTS

In facial emotion recognition, we were able to achieve a
four-class generalization accuracy of 77.92%, test accuracy
75.39%, and train accuracy of 87.14% for classifying an image
of a face under labels “happy”, “sad”, “angry”, “neutral”. In
speech sensitive emotion recognition, we were able to achieve
a validation accuracy of 77.3%, test accuracy of 73.3% and
train accuracy of 93% for classifying 8 different emotions in
speech.

With the emerging advanced technologies in hardware and
sensors, FER and SER systems [24] have been developed to
support real-world application scenes, instead of laboratory
environments [25]. Although the laboratory-controlled systems
achieve very high accuracy, around 97%, the technical trans-
ferring from the laboratory to real-world applications faces a
great barrier of very low accuracy, approximately 50%.

Fig. 10. Training Loss and Accuracy of FER model

Fig. 11. Training Accuracy of SER model

learning algorithms [26] [27] has proven to be complex due
to the high variability of samples in each activity.

TABLE I
TRAIN, TEST, AND GENERALIZATION ACCURACY AFTER 30
EPOCHS(FER) AND 80 EPOCHS(SER

FER Model
SER Model

Train acc. (%)
87.14%
93%

Test acc. (%)
75.39%
73.3%

Generalization Acc. (%)
75.39%
77.3%

VII. CONCLUSION

The main purpose is to keep the user’s emotional state
healthy. The proposed constructs for the acquisition of emo-
tions and their regulation in the Smart Home Environment are
novel, open, and ﬂexible. As a result, the detection of facial
expressions and speech is done in a non-intrusive manner.
The architecture is not pre-designed to target a particular
health care problem, but should be able to deal with most
emotionally-related problems. Mixing multimodal data sig-
niﬁcantly increased recognition levels compared to unimodal
programs: multimodal method demonstrated more than 10%
in relation to the most successful unimodal
improvement
system. Moreover, decision making of the smart context-
aware system environment improved. This paper described
the complete structure (ﬁg. 7) of providing all the necessary
functions and interfaces for emotional recognition and control.
First,
the purpose is to determine the user’s feelings by
analyzing facial expressions or sensitive speech emotions. The
algorithm has three stages. In the image processing phase,
the facial region and facial action elements are extracted.
Haar Cascade is adopted to extract the facial region from an
image. Speech sensitive emotion recognition was performed in
two main steps: feature (mel scale, pitch, frequency, chroma,
fourier transform) extraction and classiﬁcation by employing
deep-learning algorithms. Then, the system makes decisions
to steer the emotions of the user to a positive state with music
and color/light actuation. On the output end, music and color
rendering is done by changing the color of their IoT devices
from a variety of colors to soften the acquired mood, for
instance, a person in an angry mood is toned down by colors
lying on the soft side of the color spectra whereas a person
feeling sad is uplifted by bright colors for driving the user to
a pleasant state of mood.

Research in the ﬁeld of color therapy has shown that
changes in the color of our environment cause a clear emo-
tional response. Color preferences have been shown to be
closely related to personality traits and emotions. Color tone
is managed in areas of the brain that deal with emotions and
feelings.

Undoubtedly, user feedback related to color depends on a
large set of external factors, such as gender, age, culture, pref-
erences, emotions, and content (e.g. time of day or location).
Identifying emotional state and interpreting well using deep-

VIII. FUTURE WORK

Ingenuine and forced expressions pose a challenge to actual
emotion detection and probing systems towards a healthier
and happier environment for the user. A system operating
in the real world would not rely completely on single or
separate information sources due to problems such as noise,
false positives or occlusions.

The difﬁculty in the introduction of a new intelligent system
is in the uncertainty of its effect in the society. The beneﬁt of
any system appears when it is implemented deeply into the

[8] A. Intelligence, “Ambient Intelligence : A Survey,” vol. 43, no. 4, 2011,

doi: 10.1145/1978802.1978815.

[9] M. D. Rabashette, “Emotion Based Music System,” Int. J. Emerg. Trends

Sci. Technol., 2016, doi: 10.18535/ijetst/v3i05.12.

[10] B. Schuller, G. Rigoll, and M. Lang, “Hidden Markov model-based
speech emotion recognition,” in 2003 IEEE International Conference on
Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP
’03)., 2003, vol. 2, pp. II–1, doi: 10.1109/ICASSP.2003.1202279.
[11] M. Jain et al., “Speech emotion recognition using support vector

machine,” arXiv, no. May 2014, 2020, doi: 10.5120/431-636.

[12] S. Ramakrishnan and I. M. M. El Emary, “Speech emotion recognition
approaches in human computer interaction,” Telecommun. Syst., vol. 52,
no. 3, pp. 1467–1478, 2013, doi: 10.1007/s11235-011-9624-z.

[13] W. Lim, D. Jang, and T. Lee, “Speech emotion recognition using con-
volutional and Recurrent Neural Networks,” in 2016 Asia-Paciﬁc Signal
and Information Processing Association Annual Summit and Conference
(APSIPA), 2016, pp. 1–4, doi: 10.1109/APSIPA.2016.7820699.
[14] M. W¨ollmer, A. Metallinou, F. Eyben, B. Schuller, and S. Narayanan,
“Context-sensitive multimodal emotion recognition from speech and
facial expression using bidirectional LSTM modeling,” Proc. 11th Annu.
Conf. Int. Speech Commun. Assoc. INTERSPEECH 2010, no. Septem-
ber, pp. 2362–2365, 2010.

[15] D. Amin and K. Sinha, “Touchy Feely : An Emotion Recognition

Challenge,” p. 92.

[16] A. Savoiu and J. Wong, Recognizing Facial Expressions Using Deep

Learning. 2017.

[17] I. J. Goodfellow et al., “Challenges in representation learning: A report
on three machine learning contests,” Neural Networks, vol. 64, pp.
59–63, 2015, doi: 10.1016/j.neunet.2014.09.005.

[18] S. R. Livingstone and F. A. Russo, The Ryerson Audio-Visual Database

of Emotional Speech and Song (RAVDESS). 2018.

[19] M. Bao and A. Huang, “Human vocal sentiment analysis,” arXiv, pp.

1–16, 2019.

[20] J. Bravo, D. Cook, and G. Riva, “Ambient

intelligence for health
environments,” J. Biomed. Inform., vol. 64, pp. 207–210, 2016, doi:
10.1016/j.jbi.2016.10.009.

[21] S. Gilda, H. Zafar, C. Soni, and K. Waghurdekar, “Smart music player in-
tegrating facial emotion recognition and music mood recommendation,”
Proc. 2017 Int. Conf. Wirel. Commun. Signal Process. Networking,
WiSPNET 2017, vol. 2018-Janua, no. 4, pp. 154–158, 2018, doi:
10.1109/WiSPNET.8299738.

[22] S. Bhutada and T. Iv, “EMOTION BASED MUSIC,” vol. 7, no. 4, pp.

2170–2175, 2020.

[23] D. Su, R. Picard, and Y. Liu, “AMAI: Adaptive music for affect
improvement,” ICMC 2018 - Proc. 2018 Int. Comput. Music Conf., pp.
87–92, 2018.

[24] P. Liu, S. Han, Z. Meng, and Y. Tong, “Facial expression recogni-
tion via a boosted deep belief network,” Proc. IEEE Comput. Soc.
Conf. Comput. Vis. Pattern Recognit., pp. 1805–1812, 2014, doi:
10.1109/CVPR.2014.233.

[25] A. Budrionis and J. G. Bellika, “The Learning Healthcare System:
Where are we now? A systematic review,” J. Biomed. Inform., vol. 64,
pp. 87–92, 2016, doi: 10.1016/j.jbi.2016.09.018.

[26] N. T. Vu, H. Adel, P. Gupta, and H. Sch¨utze, “Combining recurrent
and convolutional neural networks for relation classiﬁcation,” 2016
Conf. North Am. Chapter Assoc. Comput. Linguist. Hum. Lang.
Technol. NAACL HLT 2016 - Proc. Conf., pp. 534–539, 2016, doi:
10.18653/v1/n16-1065.

[27] K. Simonyan and A. Zisserman, “Very deep convolutional networks for
large-scale image recognition,” 3rd Int. Conf. Learn. Represent. ICLR
2015 - Conf. Track Proc., pp. 1–14, 2015.

society and impacts social activities in a large scope. Social
simulation, especially, through a method of inducing mood,
will provide an important tool to evaluate how the system
may change the behavior of the society. Researchers will be
in permanent contact with the users to solve problems that
may arise and to monitor the level of the users’ satisfaction,
so as to improve the system’s feedback in regulating emotions
by stimuli such as color and music.

Colors can be subjective - what can make one person feel
happy can make another person feel annoyed depending on the
user’s past experience or cultural differences. Therefore, the
smart context-aware environment system should be tailored
to each user according to his or her preference. Providing
immersion in AR, and giving listeners an active agency (rather
than a system that controls all aspects of music) can further
improve the system’s performance. Therefore, some user-
associated information is welcome.

While recent researches show much promise, they are ﬁrst
and foremost indicative of the fact that there’s a long way
true user-centered approach to
to go before we arrive at
ambient intelligence systems, visual lie-detectors and threat
detection security systems, which can combine automated
facial emotion, body language analysis and speech sensitive
emotion for spotting potentially risky situations. Relevant to
the development of AI systems that can make an environment
truly intelligent is the need for such an environment to be able
to:

1) learn habits, preferences and needs of the speciﬁc user,
2) enhance user capabilities and comfort by providing new

and automated service execution,

3) enhance user capabilities and comfort by providing new

and automated service execution,

4) integrate IoT connected elements like smart color blinds,

smart home and mobile robots, and

5) provide a structured way to analyze, decide and react

over that smart, friendly environment.

REFERENCES

[1] M. Schels, S. Scherer, M. Glodek, H. A. Kestler, G. Palm, and F.
Schwenker, “On the discovery of events in EEG data utilizing infor-
mation fusion,” Comput. Stat., vol. 28, no. 1, pp. 5–18, 2013, doi:
10.1007/s00180-011-0292-y.

[2] A. Dzedzickis, A. Kaklauskas, and V. Bucinskas, “Human emotion
recognition: Review of sensors and methods,” Sensors (Switzerland),
vol. 20, no. 3, pp. 1–41, 2020, doi: 10.3390/s20030592.

[3] M. T. Quazi, “Human Emotion Recognition Using Smart Sensors A
Thesis submitted in fulﬁlment of the Master of Engineering in,” p. 122,
2012.

[4] J. Kumari, R. Rajesh, and K. M. Pooja, “Facial Expression Recognition:
A Survey,” Procedia Comput. Sci., vol. 58, pp. 486–491, 2015, doi:
10.1016/j.procs.2015.08.011.

[5] S. S. Kulkarni, N. P. Reddy, and S. I. Hariharan, “Facial expression
(mood) recognition from facial images using committee neural net-
works,” Biomed. Eng. Online, vol. 8, pp. 1–12, 2009, doi: 10.1186/1475-
925X-8-16.

[6] B. Basharirad and M. Moradhaseli, “Speech emotion recognition meth-
ods: A literature review,” AIP Conf. Proc., vol. 1891, no. October, 2017,
doi: 10.1063/1.5005438.

[7] A. Fern´andez-Caballero et al., “Smart environment architecture for
emotion detection and regulation,” J. Biomed. Inform., vol. 64, pp.
55–73, 2016, doi: 10.1016/j.jbi.2016.09.015.

