Cumulant-free closed-form formulas for some common
(dis)similarities between densities of an exponential family

Frank Nielsen(cid:63) and Richard Nock†

(cid:63)Sony Computer Science Laboratories, Inc.
(cid:63)Tokyo, Japan
(cid:63)Frank.Nielsen@acm.org
†CSIRO Data61 & Australian National University
†Sydney, Australia
†Richard.Nock@data61.csiro.au

Abstract

It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler, α-divergences, and Jef-
freys’ divergences between densities belonging to a same exponential family have generic closed-
form formulas relying on the strictly convex and real-analytic cumulant function characterizing
the exponential family. In this work, we report (dis)similarity formulas which bypass the explicit
use of the cumulant function and highlight the role of quasi-arithmetic means and their mul-
tivariate mean operator extensions. In practice, these cumulant-free formulas are handy when
implementing these (dis)similarities using legacy Application Programming Interfaces (APIs)
since our method requires only to partially factorize the densities canonically of the considered
exponential family.

Keywords: Bhattacharyya coeﬃcient; Bhattacharyya distance; Hellinger distance; Jensen-
Shannon divergence; Kullback-Leibler divergence; α-divergences; Jeﬀreys divergence; Cauchy-
Schwarz divergence; quasi-arithmetic means; inverse function theorem; strictly monotone operator;
mean operator; information geometry; exponential family; mixture family.

1

Introduction

Let (X , F, µ) be a measure space [6] with sample space X , σ-algebra of events F, and positive
measure µ (i.e., Lebesgue or counting measures). The Kullback-Leibler divergence [25] (KLD),
Jeﬀreys’ divergence [20] (JD), Bhattacharyya coeﬃcient (BC), Bhattacharyya distance [5, 24] (BD)
and Hellinger distance [19] (HD) between two probability measures P and Q dominated by µ with
respective Radon-Nikodym densities p = dP
dµ are statistical (dis)similarities deﬁned

dµ and q = dQ

0
2
0
2

r
p
A
7

]
T
S
.
h
t
a
m

[

3
v
9
6
4
2
0
.
3
0
0
2
:
v
i
X
r
a

1

 
 
 
 
 
 
respectively by:

DKL[p : q] :=

(cid:90)

p(x) log

p(x)
q(x)

dµ(x),

DJ [p, q] := DKL[p : q] + DKL[q : p],

(cid:90)

ρ[p, q] :=

(cid:112)p(x) q(x)dµ(x),

X
DB[p, q] := − log(ρ[p, q]),
DH [p, q] := (cid:112)1 − ρ[p, q].

(Kullback-Leibler divergence)

(Jeﬀreys’ divergence)

(Bhattacharyya coeﬃcient)

(Bhattacharyya distance)

(Hellinger distance)

(1)

(2)

(3)

(4)

(5)

KLD is an oriented distance (i.e., DKL[p : q] (cid:54)= DKL[q : p]). JD is a common symmetrization of
the KLD which is not a metric distance because JD fails the triangle inequality. BC is a similarity
which ensures that ρ[p, q] ∈ (0, 1]. BD is a symmetric non-metric distance, and HD is a metric
distance. (KLD and JD are homogeneous divergences.)

All these divergences require to calculate deﬁnite integrals which can be calculated in theory
using Risch pseudo-algorithm [44] which depends on an oracle. In practice, computer algebra sys-
tems like Maple R(cid:13) or Maxima implement diﬀerent subsets of the theoretical Risch pseudo-algorithm
and thus face diﬀerent limitations.

When the densities p and q belong to a same parametric family E = {pθ : θ ∈ Θ} of densities,
i.e., p = pθ1 and q = pθ2, these (dis)similarities yield equivalent parameter (dis)similarities. For
example, we get the parameter divergence P (θ1 : θ2) = DE
KL(θ1 : θ2) := DKL(pθ1 : pθ2). We use
notationally the brackets to indicate that the (dis)similarities parameters are densities, and the
parenthesis to indicate parameter (dis)similarities.

In particular, when E is a natural exponential family [34, 3] (NEF)

E =

(cid:110)

pθ(x) = exp

(cid:16)

(cid:17)
θ(cid:62)t(x) − F (θ) + k(x)

(cid:111)

: θ ∈ Θ

,

with t(x) denoting the suﬃcient statistics, k(x) an auxiliary measure carrier term, and

F (θ) := log

exp(θ(cid:62)t(x))dµ(x)

(cid:19)

,

(cid:18)(cid:90)

X

(6)

(7)

the cumulant function1 also called log-normalizer, log-partition function, free energy, or log-Laplace
transform. Parameter θ is called the natural parameter. The cumulant function is a strictly smooth
convex function (real analytic function [3]) deﬁned on the open convex natural parameter space Θ.
Let D denote the dimension of the parameter space Θ (i.e., the order of the exponential family)
and d the dimension of the sample space X . We further assume full regular exponential families [3]
so that Θ is a non-empty open convex domainand t(x) is a minimal suﬃcient statistic [14].

Many common families of distributions {pλ(x) λ ∈ Λ} are exponential families in disguise after
reparameterization: pλ(x) = pθ(λ)(x). Those families are called exponential families (i.e., EFs and
not natural EFs to emphasize that θ(u) (cid:54)= u), and their densities are canonically factorized as
follows:

p(x; λ) = exp

(cid:16)

(cid:17)
θ(λ)(cid:62)t(x) − F (θ(λ)) + k(x)

.

(8)

1More precisely, κθ(u) = F (θ + u) − F (θ) is the cumulant generating function of the suﬃcient statistic t(x) from
which all moments can be recovered (the cumulant generating function being the logarithm of the moment generating
function).

2

We call parameter λ the source parameter (or the ordinary parameter, with λ ∈ Λ, the
source/ordinary parameter space) and parameter θ(λ) ∈ Θ is the corresponding natural parameter.
Notice that the canonical parameterization of Eq. 8 is not unique: For example, adding a constant
term c ∈ R to F (θ) can be compensated by subtracting this constant to k(x), or multiplying the
suﬃcient statistic t(x) by a symmetric invertible matrix A can be compensated by multiplying θ(λ)
by the inverse of A so that (Aθ(λ)))(cid:62)(A−1t(x)) = θ(λ))(cid:62)AA−1t(x) = θ(λ)(cid:62)t(x).

Another useful parameterization of exponential families is the moment parameter [34, 3]: η =

Epθ [t(x)] = ∇F (θ). The moment parameter space shall be denoted by H:

H = {Epθ [t(x)]

: θ ∈ Θ} .

(9)

Let C = CH{t(x) : x ∈ X } be the closed convex hull of the range of the suﬃcient statistic. When
In the remainder, we consider full regular and steep
H = int(C), the family is said steep [14].
exponential families.

To give one example, consider the family of univariate normal densities:

(cid:40)

N :=

p(x; λ) =

1
√
2π

σ

(cid:32)

exp

−

(cid:18) x − µ
σ

1
2

(cid:19)2(cid:33)

, λ = (µ, σ2) ∈ R × R++

.

(10)

(cid:41)

Family N is interpreted as an exponential family of order D = 2 with univariate parametric
densities (d = 1), indexed by the source parameter λ = (µ, σ2) ∈ Λ with Λ = R × R++. The
corresponding natural parameter is θ(λ) = (cid:0) µ
(cid:1), and the moment parameter is η(λ) =
(Epλ[x], Epλ[x2]) = (µ, µ2 + σ2) since t(x) = (x, x2). The cumulant function for the normal family
is F (θ) = − θ2
1
4θ2

σ2 , − 1
2σ2

2 log

− π
θ2

+ 1

(cid:17)

(cid:16)

.

When densities p = pθ1 and q = pθ2 both belong to the same exponential family, we have the
following well-known closed-form expressions [33, 34] for the (dis)similarities introduced formerly:

∗(θ1 : θ2),

DKL[pθ1 : pθ2] = BF
DJ [pθ1 : pθ2] = (θ2 − θ1)(cid:62)(η2 − η1),
ρ[pθ1 : pθ2] = exp(−JF (θ1 : θ2)),

DB[pθ1 : pθ2] = JF (θ1 : θ2)
DH [pθ1 : pθ2] = (cid:112)1 − exp(−JF (θ1 : θ2)),

Kullback-Leibler divergence

Jeﬀreys’ divergence

Bhattacharyya coeﬃcient

Bhattacharyya distance

Hellinger distance

(11)

(12)

(13)

(14)

(15)

where D∗ indicates the reverse divergence D∗(θ1 : θ2) := D(θ2 : θ1) (parameter swapping2), and BF
and JF are the Bregman divergence [8] and the Jensen divergence [33] induced by the functional
generator F , respectively:

BF (θ1 : θ2)

:= F (θ1) − F (θ2) − (θ1 − θ2)(cid:62)∇F (θ2)

JF (θ1 : θ2)

:=

F (θ1) + F (θ2)
2

− F

(cid:18) θ1 + θ2
2

(cid:19)

.

(16)

(17)

In information geometry, the Bregman divergences are the canonical divergences of dually ﬂat

spaces [28].

2Here, the star ‘*’ is not to be confused with the Legendre-Fenchel transform used in information geometry [1].
∗(θ1 : θ2) = BF (θ2 : θ1) =

For a Bregman divergence BF (θ1 : θ2), we have the reverse Bregman divergence BF
BF ∗ (∇F (θ1) : ∇F (θ2)), where F ∗ denotes convex conjugate of F obtained by the Legendre-Fenchel transform.

3

More generally, the Bhattacharrya distance/coeﬃcient can be skewed with a parameter α ∈

(0, 1) to yield the α-skewed Bhattacharyya distance/coeﬃcient [33]:

DB,α[p : q]

:= − log(ρα[p : q]),

ρα[p : q]

:=

p(x)αq(x)1−αdµ(x),

(cid:90)

X

= − log

(cid:90)

X

q(x)

(cid:19)α

(cid:18) p(x)
q(x)

dµ(x).

(18)

(19)

(20)

The ordinary Bhattacharyya distance and coeﬃcient are recovered for α = 1
2 . In statistics, the
maximum skewed Bhattacharrya distance is called Chernoﬀ information [10, 27] used in Bayesian
hypothesis testing:

DC[p : q] := max
α∈(0,1)

DB,α[p : q].

(21)

Notice that the Bhattacharyya skewed α-coeﬃcient of Eq. 20 also appears in the deﬁnition of

the α-divergences [1] (with α ∈ R):

Dα[p : q] :=






1

α(1−α) (1 − ρα[p : q]) , α ∈ R\{0, 1}
D1[p : q] = DKL[p : q], α = 1
D0[p : q] = DKL[q : p], α = 0

(22)

The α-divergences belong to the class of f -divergences [13] which are the invariant divergences3 in
information geometry [1].

When densities p = pθ1 and q = pθ2 both belong to the same exponential family E, we get the

following closed-form formula [33]:

where JF,α denotes the α-skewed Jensen divergence:

DB,α[pθ1 : pθ2] = JF,α(θ1 : θ2),

JF,α(θ1 : θ2) := αF (θ1) + (1 − α)F (θ2) − F (αθ1 + (1 − α)θ2).

(23)

(24)

All these closed-form formula can be obtained from the calculation of the following generic inte-
gral [37]:

Iα,β[p : q] :=

p(x)αq(x)βdµ(x),

(25)

(cid:90)

when p = pθ1 and q = pθ2. Indeed, provided that αθ1 + βθ2 ∈ Θ, we have

Iα,β[pθ1 : pθ2] = exp (−(αF (θ1) + βF (θ2) − F (αθ1 + βθ2))) Epαθ1+βθ2

[exp((α + β − 1)k(x))] . (26)

The calculation of Iα,β in Eq. 26 is easily achieved by bypassing the computation of the antideriva-
tive of the integrand in Eq. 25 (using the fact that (cid:82) pθ(x)dµ(x) = 1 for any θ ∈ Θ), see [37].

In particular, we get the following special cases:

• When α + β = 1, Iα,1−α[pθ1 : pθ2] = exp(−JF,α(θ1 : θ2)) since αθ1 + (1 − α)θ2 ∈ Θ (domain

Θ is convex).

3A statistical invariant divergence D is such that D[pθ : pθ+dθ] = (cid:80)

i,j gij(θ)dθidθj, where I(θ) = [gij(θ)] is the

Fisher information matrix [1].

4

• When k(x) = 0, and α + β > 0, Iα,β[pθ1 : pθ2] = exp (−(αF (θ1) + βF (θ2) − F (αθ1 + βθ2))).
This is always the case when Θ is a convex cone (e.g., Gaussian or Wishart family), see [26].

• When α + β = 1 with arbitrary α,

Iα,1−α[pθ1 : pθ2] = exp (−(αF (θ1) + (1 − α)F (θ2) − F (αθ1 + (1 − α)θ2))) .

(27)

This setting is useful for getting truncated series of f -divergences when the exponential family
has an aﬃne space Θ, see [38, 35].

When α → 1 or α → 0, we get the following limits of the α-skewed Bhattacharrya distances:

lim
α→1

lim
α→0

1
α(1 − α)
1
α(1 − α)

DB,α[p : q] = DKL[p : q]

DB,α[p : q] = DKL[q : p].

(28)

(29)

It follows that when the densities p = pθ1 and q = pθ2 both belong to the same exponential

family, we obtain

lim
α→0

lim
α→1

1
α(1 − α)
1
α(1 − α)

JF,α(θ1 : θ2) = BF (θ2 : θ1),

JF,α(θ1 : θ2) = BF (θ1 : θ2).

(30)

(31)

In practice, we would like to get closed-form formula for the (dis)similarities when the densities

belong to the same exponential families using the source reparameterization λ ∈ Λ:

DKL[pλ1 : pλ2] = BF (θ(λ2) : θ(λ1)),
DJ [pλ1 : pλ2] = (θ(λ1) : θ(λ2))(cid:62)(η(λ2) − η(λ1)),

ρ[pλ1 : pλ2] = exp(−JF (θ(λ1) : θ(λ2))),

DB[pλ1 : pλ2] = JF (θ(λ1) : θ(λ2))
DH [pλ1 : pλ2] = (cid:112)1 − ρ(θ(λ1) : θ(λ2)),

(32)

(33)

(34)

(35)

(36)

where θ(·) and η(·) are the D-variate functions for converting the source parameter λ to the natural
parameter θ and the moment parameter η, respectively. The Chernoﬀ information between two
densities pλ1 and pλ2 of the same exponential family amounts to a Jensen-Chernoﬀ divergence [27]:

JC(λ1 : λ2) := DC[pλ1 : pλ2] = max
α∈(0,1)
= max
α∈(0,1)

DB,α[pλ1 : pλ2],

JF,α(θ(λ1) : θ(λ2)),

(37)

(38)

that is the maximal value of a skew Jensen divergence for α ∈ (0, 1).

Thus to have closed-form formula, we need to explicit both the θ(·) conversion function and the
cumulant function F . This can be prone to human calculus mistakes (e.g., report manually these
formula without calculus errors for the multivariate Gaussian family). Furthermore, the cumulant
function F may not be available in closed-form [39].

5

In this work, we show how to easily bypass the explicit use of the cumulant function F . Our
method is based on a simple trick, and makes the programming of these (dis)similarities easy using
oﬀ-the-shelf functions of application programming interfaces (APIs) (e.g., the density function, the
entropy function or the moment function of a distribution family).

This paper is organized as follows: Section 2 explains the method for the Bhattacharyya coeﬃ-
cient and its related dissimilarities. Section 3 further carry on the principle of bypassing the explicit
use of the cumulant function and its gradient for the calculation of the Kullback-Leibler divergence
and its related Jeﬀreys’ divergence. Section 4 summarizes the results. Throughout the paper, we
present several examples to illustrate the methods. Appendix 4 displays some code written us-
ing the computer algebra system (CAS) Maxima to recover some formula for some exponential
families. Appendix A provides further examples.

2 Cumulant-free formula for the Bhattacharyya coeﬃcient and

distances derived thereof

2.1 A method based on a simple trick

The densities of an exponential family have all the same support [3] X . Consider any point ω ∈ X
in the support. Then observe that we can write the cumulant of a natural exponential family as:

F (θ) = − log pθ(ω) + t(ω)(cid:62)θ + k(ω).

(39)

Since the generator F of a Jensen or Bregman divergence is deﬁned modulo an aﬃne function
a(cid:62)θ + b (i.e., JF = JG and BF = BG for G(θ) = F (θ) + a(cid:62)θ + b for a ∈ RD and b ∈ R), we
consider the following equivalent generator (term +t(ω)(cid:62)θ + k(ω) is aﬃne) expressed using the
density parameterized by λ ∈ Λ:

Fλ(λ) = F (θ(λ)) = − log(pλ(ω)).

(40)

Then the Bhattacharyya coeﬃcient is expressed by this cumulant-free expression using the

source parameterization λ:

∀ω ∈ X ,

ρ[pλ1, pλ2] =

(cid:112)pλ1(ω)pλ2(ω)
p¯λ(ω)

=

(cid:115)

(cid:115)

pλ1(ω)
p¯λ(ω)

pλ2(ω)
p¯λ(ω)

,

with

¯λ = λ

(cid:18) θ(λ1) + θ(λ2)
2

(cid:19)

.

Similarly, the Bhattacharyya distance is written as

∀ω ∈ X , DB[pλ1, pλ2] = log

(cid:32)

p (cid:0)ω; ¯λ(cid:1)
(cid:112)p(ω, λ1)p(ω, λ2)

(cid:33)

.

Let l(x; λ) := log p(x; λ) be the log-density. Then we have

∀ω ∈ X , DB[pλ1, pλ2] = l (cid:0)ω; ¯λ(cid:1) −

l(ω, λ1) + l(ω, λ2)
2

.

6

(41)

(42)

(43)

(44)

This is a Jensen divergence for the strictly convex function −l(x; θ) (wrt. θ) since −l(x; θ) ≡ F (θ)
(modulo an aﬃne term).

Thus we do not need to explicitly use the expression of the cumulant function F in Eq. 41 and

Eq. 43 but we need the following parameter λ ⇔ θ conversion functions:

1. θ(λ) the ordinary-to-natural parameter conversion function, and

2. its reciprocal function λ(θ) (i.e., λ(·) = θ−1(·)), the natural-to-source parameter conversion

function

so that we can calculate the ordinary λ-parameter ¯λ corresponding to the natural mid-parameter
θ(λ1)+θ(λ2)
2

:

¯λ = λ

(cid:18) θ(λ1) + θ(λ2)
2

(cid:19)

.

(45)

Notice that in general, a linear interpolation in the natural parameter θ corresponds to a non-

linear interpolation in the source parameterization λ when θ(u) (cid:54)= u.

Since λ(·) = θ−1(·), we can interpret the non-linear interpolation ¯λ as a generalization of quasi-

arithmetic mean [33] (QAM):

where

¯λ = θ−1

(cid:18) θ(λ1) + θ(λ2)
2

(cid:19)

=: Mθ(λ1, λ2),

Mf (a, b) := f −1

(cid:18) 1
2

f (a) +

(cid:19)

f (b)

,

1
2

is the quasi-arithmetic mean induced by a strictly monotonic and smooth function f .4

We can extend the quasi-arithmetic mean to a weighted quasi-arithmetic mean as follows:

Mf,α(a, b) := f −1 (αf (a) + (1 − α)f (b)) , α ∈ [0, 1].

(46)

(47)

(48)

Weighted quasi-arithmetic means are strictly monotone [47] when the Range(f ) ⊂ R.

Let us remark that extensions of weighted quasi-arithmetic means have been studied recently

in information geometry to describe geodesic paths [16, 15] γab = {Mf,α(a, b) : α ∈ [0, 1]}.

In 1D, a bijective function on an interval (e.g., a parameter conversion function in our setting)

is a strictly monotonic function, and thus f deﬁnes a quasi-arithmetic mean.

To deﬁne quasi-arithmetic mean with multivariate generators using Eq. 47, we need to prop-
erly deﬁne f −1. When the function f is separable,
i=1 fi(xi) with the fi’s
strictly monotone and smooth functions, we can straightforwardly deﬁne the multivariate mean
as Mf (x, x(cid:48)) := (Mf1(x1, x(cid:48)

i.e., f (x) = (cid:80)D

1), . . . , MfD (x1, x(cid:48)

1)).

In general, the inverse function theorem in multivariable calculus [11] states that there exists
locally an inverse function f −1 for a continuously diﬀerentiable function f at point x if the deter-
minant of the Jacobian Jf (x) of f at x is non-zero. Moreover, f −1 is continuously diﬀerentiable
and we have Jf −1(y) = [Jf (x)]−1 (matrix inverse) at y = f (x).

4Notice that Mf (a, b) = Mg(a, b) when g(u) = c1f (u) + c2 with c1 (cid:54)= 0 ∈ R and c2 ∈ R. Thus we can assume that
f is a strictly increasing and smooth function. Function f is said to be in standard form if f (1) = 0, f (cid:48)(u) > 0 and
f (cid:48)(1) = 1).

7

In some cases, we get the existence of a global inverse function. For example, when f = ∇H is
the gradient of Legendre-type strictly convex and smooth function H, the reciprocal function f −1
is well-deﬁned f −1 = ∇H ∗ and global, where H ∗ denotes the convex conjugate of H (Legendre-
type). In that case, f = ∇H is a strictly monotone operator5 since it is the gradient of a strictly
convex function. We also refer to [2] for some work on operator means, and to [42] for multivariate
quasi-arithmetic means of covariance matrices.

To summarize, we can compute the Bhattacharyya coeﬃcient (and Bhattacharyya/Hellinger
distances) using the parametric density function pλ(x) and a quasi-arithmetic mean ¯λ = Mθ(λ1, λ2)
on the source parameters λ1 and λ2 as:

∀ω ∈ X ,

ρ[pλ1, pλ2] =

(cid:112)pλ1(ω)pλ2(ω)
p(ω; Mθ(λ1, λ2))

using the notation p(x; θ) := pθ(x), and

∀ω ∈ X , DB[pλ1, pλ2] = log

(cid:32)

p (ω; Mθ(λ1, λ2))
(cid:112)p(ω, λ1)p(ω, λ2)

(cid:33)

.

Similarly, we get the following cumulant-free expression for the Hellinger distance:

∀ω ∈ X , DH [pλ1, pλ2] =

1 −

(cid:115)

(cid:112)pλ1(ω)pλ2(ω)
p(ω; Mθ(λ1, λ2))

.

(49)

(50)

(51)

The Hellinger distance proves useful when using some generic algorithms which require to
handle metric distances. For example, the 2-approximation factor of Gonzalez [18] for k-center
metric clustering.

These cumulant-free formula are all the more convenient as in legacy software API, we usually
have access to the density function of the probability family. Thus if a parametric family of an API
is an exponential family E, we just need to implement the corresponding quasi-arithmetic mean
M E
θ .
More generally, the α-skewed Bhattacharyya distance [33] for α ∈ (0, 1) is expressed using the

following cumulant-free expression:

∀ω ∈ X , DB,α[pλ1 : pλ2] = log

(cid:18) p(ω, λ1)αp(ω, λ2)1−α
p (ω; Mθ,α(λ1, λ2))

(cid:19)

with

Mθ,α(λ1, λ2) = λ(αθ(λ1) + (1 − α)θ(λ2)) =: ¯λα.

(52)

(53)

Notice that the geometric α-barycenter of two densities pθ1 and pθ2 of an exponential family E

is a scale density of E:

p(x; αθ1 + (1 − α)θ2) =

(cid:18) p(ω; αθ1 + (1 − α)θ2)
pα(ω; θ1)p1−α(ω; θ2)

(cid:19)

pα(x; θ1)p1−α(x; θ2),

∀ω ∈ X .

5A mapping M : Rd → Rd is a strictly monotone operator iﬀ (p − q)(cid:62)(M (p) − M (q)) > 0 for all p, q ∈ Rd with

p (cid:54)= q. Monotone operators are a generalization of the concept of univariate monotonic functions.

8

Let ˜pθ(x) = exp(t(x)(cid:62)θ + k(x)) denote the unnormalized density of an exponential family (so

that pθ(x) = ˜pθ(x) exp(−F (θ))). We have the following invariant:

∀ω ∈ X ,

˜p(ω; Mθ, α(λ1, λ2))
˜p(ω, λ1)α ˜p(ω, λ2)1−α = 1.

It follows that we have:

ρα[pθ1 : pθ2] =

p(ω; αθ1 + (1 − α)θ2)
p(ω, λ1)αp(ω, λ2)1−α ,

=

exp(F (αθ1 + (1 − α)θ2))
exp(αF (θ1)) exp((1 − α)F (θ2))

,

= exp(−JF,α(θ1 : θ2)).

The Cauchy-Schwarz divergence [21, 41] is deﬁned by

DCS(p, q) := − log





(cid:113)(cid:0)(cid:82)

(cid:82)
X p(x)q(x)dµ(x)

X p(x)2dµ(x)(cid:1) (cid:0)(cid:82)

X q(x)2dµ(x)(cid:1)



 ≥ 0.

(54)

(55)

(56)

(57)

(58)

Thus the Cauchy-Schwarz divergence is a projective divergence: DCS(p, q) = DCS(λp, λq) for any
λ > 0. It can be shown that the Cauchy-Schwarz divergence between two densities pθ1 and pθ2 of
an exponential family with a natural parameter space a cone [31] (e.g., Gaussian) is:

DCS(pθ1, pθ2) = JF (2θ1 : 2θ2) + log





(cid:113)

Ep2θ1

[exp(k(x))] Ep2θ2
Epθ1+θ2

[exp(k(x))]

[exp(k(x))]



 .

(59)

See [31] for the formula extended to H¨older divergences which generalize the Cauchy-Schwarz
divergence.

2.2 Some illustrating examples

Let us start with an example of a continuous exponential family which relies on the arithmetic
mean A(a, b) = a+b
2 :

Example 1 Consider the family of exponential distributions with rate parameter λ > 0. The
densities of this continuous EF writes as pλ(x) = λ exp(−λx) with support X = [0, ∞). From the
partial canonical factorization of densities following Eq. 6, we get that θ(u) = u and θ−1(u) = u so
that Mθ(λ1, λ2) = A(λ1, λ2) = λ1+λ2
is the arithmetic mean. Choose ω = 0 so that pλ(ω) = λ. It
follows that

2

ρ[pλ1, pλ2] =

=

=

(cid:112)p(ω, λ1)p(ω, λ2)
p (cid:0)ω; ¯λ(cid:1)

,

√

,

λ1λ2
λ1+λ2
2
√
2
λ1λ2
λ1 + λ2

.

9

√

Let G(a, b) =

ab denote the geometric mean for a, b > 0. Notice that since G(λ1, λ2) ≤
A(λ1, λ2), we have ρ[pσ1, pσ2] ∈ (0, 1]. The Bhattacharyya distance between two exponential densi-
ties of rate λ1 and λ2 is

DBhat[pσ1, pσ2] = − log ρ[pλ1, pλ2] = log

λ1 + λ2
2

−

1
2

log λ1λ2 ≥ 0.

Since the logarithm function is monotonous, we have log A(λ1, λ2) ≥ log G(λ1, λ2) and therefore we
check that DBhat[pσ1, pσ2] ≥ 0.

Next, we consider a discrete exponential family which exhibits the geometric mean:
Example 2 The Poisson family of probability mass functions (PMFs) pλ(x) = λx exp(−λ)
where
λ > 0 denotes the intensity parameter and x ∈ X = {0, 1, . . . , } is a discrete exponential family with
t(x) = x (ω = 0, t(ω) = 0 and pλ(ω) = exp(−λ)), θ(u) = log u and λ(u) = θ−1(u) = exp(u). Thus
the quasi-arithmetic mean associated with the Poisson family is Mθ(λ1, λ2) = G(λ1, λ2) =
λ1λ2
the geometric mean. It follows that the Bhattacharrya coeﬃcient is

√

x!

ρ[pλ1, pλ2] =

=

p (cid:0)ω; ¯λ(cid:1)
(cid:112)p(ω, λ1)p(ω, λ2)
λ1λ2)

exp(−

√

,

(cid:112)exp(−λ1) exp(−λ2)

= exp

(cid:18) λ1 + λ2
2

(cid:19)

(cid:112)

λ1λ2

−

Hence, we recover the Bhattacharrya distance between two Poisson pmfs:

DB[pλ1, pλ2] =

λ1 + λ2
2

−

(cid:112)

λ1λ2 ≥ 0.

The negative binomial distribution with known number of failures yields also the same natural

parameter and geometric mean (but the density p(ω, λ) is diﬀerent).

To illustrate the use of the power means Pr(a, b) = (ar + br)

r of order r ∈ R (also called
H¨older means) for r (cid:54)= 0 (with limr→0 Pr(a, b) = G(a, b)), let us consider the family of Weibull
distributions.

1

Example 3 The Weibull distributions with a prescribed shape parameter k ∈ R++ (e.g., exponen-
tial distributions when k = 1, Rayleigh distributions when k = 2) form an exponential family. The
density of a Weibull distribution with scale λ and ﬁxed shape parameter k is

pλ(x) =

(cid:17)k−1

k
λ

(cid:16) x
λ

e−(x/λ)k

,

x ∈ X = R++

The ordinary↔natural parameter conversion functions are θ(u) = 1

(cid:16) 1
2 λ−k

1 + 1

2 λ−k

2

(cid:17)− 1

k is the power means of order −k.

uk and θ−1(u) = 1

. It

1
k

u

λk . It follows the closed-form formula for the Bhattacharrya

follows that ¯λ = Mθ(λ1, λ2) = P−k(λ1, λ2) =
We choose ω = 1 and get pλ(ω) = k
coeﬃcient for integer k ∈ {2, . . . , }:

λk e− 1

ρ[pλ1, pλ2] = (cid:112)p(ω, λ1)p(ω, λ2)
(cid:113)
2

=

1λk
λk
2
1 + λk
λk
2

.

p (cid:0)ω; ¯λ(cid:1)
,

(60)

(61)

10

Exponential family E
Exponential
Poisson
Laplace (ﬁxed µ)

Weibull (ﬁxed shape k > 0)

Bernoulli

θ(u)
u
log u
1
u
1
uk
log u
1−u

(logit function)

θ−1(u)
u
exp(u)
1
u
1
1
k
exp(u)
1+exp(u)
(logistic function) with a12 =

¯λ = Mθ(λ1, λ2)
A(λ1, λ2) = λ1+λ2
√
2
G(λ1, λ2) =
λ1λ2
H(λ1, λ2) = 2λ1λ2
λ1+λ2
1
(λ1, λ2) = ( 1
P 1
2 λ
k
k
Ber(λ1, λ2) = a12
1+a12
(cid:113) λ1λ2

u

1 + 1
2 λ

1
k

1 )− 1

k

(1−λ1)(1−λ2)

Table 1: Quasi-arithmetic means Mθ(λ1, λ2) = θ−1 (cid:16) θ(λ1)+θ(λ2)
2
discrete and continuous exponential families of order D = 1.

(cid:17)

associated with some common

For k = 2, the Weibull family yields the Rayleigh family with λ =
coeﬃcient is ρ[pλ1, pλ2] = G(λ1,λ2)
mean, respectively (with Q ≥ G).

2σ. The Bhattacharyya
Q(λ1,λ2) where G and Q denotes the geometric mean and the quadratic

√

Table 1 summarizes the quasi-arithmetic means associated with common univariate exponential
families. Notice that a same quasi-arithmetic mean can be associated to many exponential families:
For example, the Gaussian family with ﬁxed variance or the exponential distribution family have
both θ(u) = u yielding the arithmetic mean.

Let us now consider multi-order exponential families. We start with the bi-order exponential

family of Gamma distributions.

Example 4 The density of a Gamma distribution is

p(x; α, β) =

βα
Γ(α)

xα−1e−βx,

x ∈ X = (0, ∞),

for a shape parameter α > 0 and rate parameter β > 0 (i.e., a 2-order exponential family with
λ = (λ1, λ2) = (α, β)). The natural parameter is θ(λ) = (λ1 − 1, −λ2) and the inverse function
It follows that the generalized quasi-arithmetic mean is the bivariate
is λ(θ) = (θ1 + 1, −θ2).
) = ( ¯α, ¯β). We choose ω = 1 so that
arithmetic mean: Mθ(λ1, λ2) = A(λ1, λ2) = ( α1+α2
p(ω; α, β) = βα

, β1+β2
2

2

Γ(α) e−β.

We get the Bhattacharrya coeﬃcient:

ρ[pα1,β1 : pα2,β2] =

(cid:112)p(ω, λ1)p(ω, λ2)
p (cid:0)ω; ¯λ(cid:1)

,

(cid:115)

=

1 βα2
βα1
Γα1Γα2

2

¯β− ¯α,

and the Bhattacharrya distance:

Dα[pα1,β1 : pα2,β2] = ¯α log ¯β −

α1 log β1 + α2 log β2
2

+ log

(cid:112)Γ(α1)Γ(α2)
Γ(¯α)

.

11

The Dirichlet family which exhibits a separable (quasi-)arithmetic mean:

Example 5 Consider the family of Dirichlet distributions with densities deﬁned on the (d − 1)-
dimensional open standard simplex support

(cid:40)

X = ∆d :=

(x1, . . . , xd) : xi ∈ (0, 1),

(cid:41)

xi = 1

.

d
(cid:88)

i=1

(62)

The family of Dirichlet distributions including the family of Beta distributions when d = 2. The
density of a Dirichlet distribution is deﬁned by:
(cid:16)(cid:80)d

(cid:17)

pα(x) =

Γ

i=1 αi

d
(cid:89)

(cid:81)d

i=1 Γ (αi)

i=1

xαi−1
i

.

(63)

The Dirichlet distributions and are used in in Bayesian statistics as the conjugate priors of
the multinomial family. The Dirichlet distributions form an exponential family with d-dimensional
natural parameter θ = (α1 − 1, . . . , αd − 1) (D = d) and vector of suﬃcient statistics t(x) =
(log x1, . . . , log xd). The induced quasi-arithmetic mean is a multivariate separable arithmetic
means, i.e., the multivariate arithmetic mean ¯λ = Mθ(α1, α2) = A(α1, α2) = α1+α2
(cid:1) so that p(ω; α) =

Let us choose ω = (cid:0) 1
We get the following Bhattacharrya coeﬃcient between two Dirichlet densities pα1 and pα2:

i=1 αi)
i=1 Γ(αi)

d , . . . , 1

Γ((cid:80)d
(cid:81)d

d((cid:80)d

αi)−d

i=1

.

.

d

2

1

ρ[pα1 : pα2] =

(cid:1)

B (cid:0) α1+α2
(cid:112)B(α1)B(α2)

2

,

where

B(α) :=

(cid:81)d
i=1 Γ (αi)
(cid:17) .
(cid:16)(cid:80)d
i=1 αi

Γ

It follows that the Bhattacharrya distance between two Dirichlet densities pα1 and pα2 is

DB[pα1 : pα2] = log

(cid:32) (cid:112)B(α1)B(α2)
B (cid:0) α1+α2

(cid:1)

2

(cid:33)

.

The α-skewed Bhattacharrya coeﬃcient for a scalar α ∈ (0, 1) is:

ρα[pα1 : pα2] =

B (αα1 + (1 − α)α2)
Bα(α1)B1−α(α2)

,

and the α-skewed Bhattacharrya distance:

DB,α[pα1 : pα2] = log

(cid:18) Bα(α1)B1−α(α2)

B (αα1 + (1 − α)α2)

(cid:19)

,

= α log B(α1) + (1 − α) log B(α2) − log B (αα1 + (1 − α)α2) ,

with

log B(α) =

d
(cid:88)

i=1

log Γ(αi) − log Γ

(cid:32) d

(cid:88)

(cid:33)

αi

.

i=1

(This is in accordance with Eq. 15–17 of [43].)

12

(64)

(65)

(66)

(67)

(68)

(69)

(70)

Finally, we consider the case of the multivariate Gaussian family:

Example 6 Consider the d-dimensional Gaussian family [29] also called MultiVariate Normal
(MVN) family. The parameter λ = (λv, λM ) of a MVN consists of a vector part λv = µ and a d × d
matrix part λM = Σ. The Gaussian density is given by

pλ(x; λ) =

1
2 (cid:112)|λM |

d

(cid:18)

exp

−

1
2

(2π)

(x − λv)(cid:62)λ−1

M (x − λv)

(cid:19)

,

where | · | denotes the matrix determinant.

Partially factorizing the density into the canonical form of exponential family, we ﬁnd that
M ). It follows that the multivariate weighted mean

M θv, 1

2 θ−1

2 θ−1

θ(λ) = (cid:0)λ−1
is M α

(cid:1) and λ(θ) = ( 1
2 λ−1
θ (λ1, λ2) = ¯λα = (µα, Σα) with

M λv, 1

M

Σα = (cid:0)αΣ−1

1 + (1 − α)Σ−1
2

(cid:1)−1

,

(cid:1) .

(71)

(72)

(cid:1). Let ∆µ = µ2 − µ1. It follows

the matrix harmonic barycenter and

µα = Σα

We choose ω = 0 with pλ(0; λ) =

(cid:0)αΣ−1

1 µ1 + (1 − α)Σ−1
exp (cid:0)− 1

2 λ(cid:62)

v λ−1

M λv

2 µ2

1
√

d
2

|λM |
the following closed-form formula for the Bhattacharyya coeﬃcient between Gaussian densities:

(2π)

ρ[pµ1,Σ1, pµ2,Σ2] =

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Σ1 + Σ2
2

− 1
(cid:12)
2
(cid:12)
(cid:12)
(cid:12)

|Σ1|

1

4 |Σ2|

1
4 exp

(cid:32)

−

1
8

∆(cid:62)
µ

(cid:18) Σ1 + Σ2
2

(cid:19)−1

(cid:33)

∆µ

.

Thus the Bhattacharrya distance is

DB,α[pµ1,Σ1, pµ2,Σ2] =

(cid:18)

1
2

and the Hellinger distance:

αµ(cid:62)

1 Σ−1

1 µ1 + (1 − α)µ(cid:62)

2 Σ−1

2 µ2 − µ(cid:62)

α Σ−1

α µα + log

|Σ1|α|Σ2|1−α
|Σα|

(cid:19)

,

DH [pµ1,Σ1, pµ2,Σ2] =

(cid:118)
(cid:117)
(cid:117)
(cid:116)1 −

1

1
4

|Σ1|
(cid:12)
(cid:12) Σ1+Σ2
2

4 |Σ2|
1
(cid:12)
2 1
(cid:12)

(cid:32)

exp

−

1
8

∆µ(cid:62)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Σ1 + Σ2
2

(cid:12)
−1
(cid:12)
(cid:12)
(cid:12)

(cid:33)

∆µ

.

The Cauchy-Schwarz divergence between two Gaussians [31] is:

DCS(pµ1,Σ1, pµ2,Σ2) =

1
2

log

(cid:33)

(cid:32)

1
2d

|(Σ−1

2 )−1|

(cid:112)|Σ1| Σ2||
1 + Σ−1
1
2 Σ−1
µ(cid:62)
2
2 µ2)(cid:62)(Σ−1
1 µ1 + Σ−1

1 µ1 +

2 µ2

1 Σ−1
µ(cid:62)

(Σ−1

+

−

1
2
1
2

1 + Σ−1

2 )−1(Σ−1

1 µ1 + Σ−1

2 µ2).

(73)

13

3 Cumulant-free formula for the Kullback-Leibler divergence and

related divergences

The Kullback-Leibler divergence (KLD) DKL[p : q] := (cid:82) p(x) log p(x)
q(x) dµ(x) between two densities
p and q also called the relative entropy [12] amounts to a reverse Bregman divergence, DKL[pθ1 :
∗(θ1 : θ2) = BF (θ2 : θ1), when the densities belong to the same exponential family E,
pθ2] = BF
where the Bregman generator F is the cumulant function of E.

We present below two techniques to calculate the KLD by avoiding to compute the integral:

• The ﬁrst technique, described in §3.1, considers the KLD as a limit case of α skewed Bhat-

tacharrya distance.

• The second technique relies on the availability of oﬀ-the-shelf formula for the entropy and
moment of the suﬃcient statistic (§3.2), and is derived using the Legendre-Fenchel divergence.

3.1 Kullback-Leibler divergence as the limit case of a skewed Bhattacharrya

distance

We can obtain closed-form formula for the Kullback-Leibler divergence by considering the limit
case of α skewed Bhattacharrya distance:

DKL[pλ1 : pλ2] = lim
α→0

Bα[pλ2 : pλ1],

1
α
1
α
1
α
(cid:18) p(ω; λ1)
p(ω; λ2)

log

= lim
α→0

= lim
α→0

= log

log ρα[pλ2 : pλ1],

p (ω; Mθ,α(λ2, λ1))
p(ω, λ2)αp(ω, λ1)1−α ,

(cid:19)

+ lim
α→0

1
α

log

(cid:18) p (ω; Mθ,α(λ2, λ1))
p(ω, λ1)

(74)

(75)

(76)

(77)

(cid:19)

.

When we deal with uni-order exponential families (D = 1), we can use a ﬁrst-order Taylor

expansion of the quasi-arithmetic means when α (cid:39) 0 (see [30]):

Mθ(a, b) =α(cid:39)0 a + α

θ(b) − θ(a)
θ(cid:48)(a)

+ o (α(θ(b) − θ(a))) ,

(78)

where θ(cid:48)(·) denote the derivative of the ordinary-to-natural parameter conversion function.

It follows that we have:

DKL[pλ1 : pλ2] = log

(cid:19)

(cid:18) p(ω; λ1)
p(ω; λ2)

+ lim
α→0

1
α

(cid:16)



p

log



ω; λ1 + α θ(λ2)−θ(λ1)
p(ω; λ1)

θ(cid:48)(λ1)

(cid:17)



 , ∀ω ∈ X

(79)

Notice that we need to calculate case by case the limit as it depends on the density expression
p(x; λ) of the exponential family. This limit can be computed symbolically using a computer algebra
system (e.g., using Maxima6). The example below illustrates the technique for calculating the KLD
between two Weibull densities with prescribed shape parameter.

6http://maxima.sourceforge.net/

14

Example 7 Consider the Weibull family with prescribed shape parameter k that form an expo-
nential family (including the family of exponential distributions for k = 1 and the the Rayleigh
distributions for k = 2). The density of a Weibull distribution with scale λ > 0 and ﬁxed shape
parameter k is

(cid:16) x
λ
We have θ(u) = u−k and θ(cid:48)(u) = −ku−k−1. We set ω = 1 so that pλ(ω) = k
program the formula of Eq. 77 using the computer algebra system Maxima:

x ∈ X = R++

e−(x/λ)k

pλ(x) =

(cid:17)k−1

k
λ

,

λk exp(λ−k). Let us

/* KLD betwen Weibull distributions by calculating a limit */
declare( k , integer);
assume(lambda1>0);
assume(lambda2>0);
k:5;
omega:1;
t(u):=u**(-k);
tinv(u):=u**(-1/k);
tp(u):=k*u**(-k-1);
p(x,l):=(k/l)*((x/l)**(k-1))*exp(-(x/l)**k);
mean(a,b):= tinv(alpha*t(a)+(1-alpha)*t(b));
log(p(omega,l1)/p(omega,l2)) + (1.0/alpha)*log(p(omega,mean(l2,l1))/p(omega,l1));
limit (ratsimp(%), alpha, 0);
expand(%);

Figure 1 displays a snapshot of the result which can be easily simpliﬁed manually as

DKL[pλ1 : pλ2] = k log

(cid:19)k

λ2
λ1

+

(cid:18) λ1
λ2

− 1.

(80)

In general, the KLD between two Weibull densities with arbitrary shapes [4] is

DKL[pk1,λ1 : pk2,λ2] = log

k1
λk1
1

− log

k2
λk2
2

(cid:18)

+ (k1 − k2)

log λ1 −

(cid:19)k2

γ
k1

(cid:19)

+

(cid:18) λ1
λ2

Γ

(cid:18) k2
k1

(cid:19)

+ 1

− 1, (81)

where γ denotes the Euler-Mascheroni constant. Thus when k1 = k2 = k, we recover Eq. 80
since Γ(2) = 1. (However, the family of Weibull densities with varying parameter shape is not an
exponential family since the suﬃcient statistics depend on the shape parameters.)

In practice, we may program the formula of Eq. 77 by deﬁning:

DKL,α[pλ1 : pλ2] = log

(cid:19)

(cid:18) p(ω; λ1)
p(ω; λ2)

+

1
α

log

(cid:18) p (ω; Mθ,α(λ2, λ1))
p(ω; λ1)

(cid:19)

,

(82)

and approximate the KLD by DKL,α for a small value of α (say, α = 10−3). Thus we need only
θ(·) and θ−1(·) for deﬁning Mθ(·, ·), and the density p(x; θ). This approximation also works for
multivariate extensions of the quasi-arithmetic means.

Let us give some two examples using the ﬁrst-order approximation of the univariate quasi-

arithmetic mean:

15

Figure 1: Snapshot of the Maxima GUI displaying the result of symbolic calculations of the KLD
between two Weibull densities of prescribed parameter shape.

Example 8 Consider the family {p(x; λ) = λ exp(−λx)} of exponential distributions with support
X = [0, ∞). Set ω = 0, p(ω; λ) = λ, θ = λ, θ(u) = u and θ(cid:48)(u) = 1. We have

DKL[pλ1 : pλ2] = log

= log

= log

(cid:19)

(cid:19)

(cid:19)

(cid:18) λ1
λ2
(cid:18) λ2
λ1
(cid:18) λ1
λ2

+ lim
α→0

+ lim
α→0
λ2
λ1

+

− 1.

1
α
1
α

log

log

λ1 + α(λ2 − λ1)
λ1
(cid:18) λ2
λ1

1 + α

− 1

(cid:18)

,

(cid:19)(cid:19)

,

(83)

(84)

(85)

Example 9 Consider the Poisson family with ω = 0, pλ(ω) = exp(−λ), θ(u) = log u (Mθ is the
geometric mean) and θ(cid:48)(u) = 1

u . We get

DKL[pλ1 : pλ2] = log

p(ω; λ1)
p(ω; λ2)

(cid:16)

p

1
α

log

θ(cid:48)(λ1)

ω; λ1 + α θ(λ2)−θ(λ1)
p(ω; λ2)
λ1
λ2

),

+ lim
α→0
1
α

= λ2 − λ1 + lim
α→0

log exp(αλ1 log

= λ2 − λ1 + λ1 log

λ1
λ2

.

16

(cid:17)

,

(86)

(87)

(88)

3.2 Kullback-Leibler divergence formula relying on the diﬀerential entropy and

moments

Consider the Kullback-Leibler divergence [25] (relative entropy) DKL[p : q] = (cid:82) p(x) log p(x)
q(x) dµ(x)
between two probability densities p and q. When the densities belong to the same exponential
families, the KL divergences amounts to a Legendre-Fenchel divergence (the canonical expression
of divergences using the dual coordinate systems in dually ﬂat spaces of information geometry [1]):

DKL[pλ1 : pλ2] = AF (θ2 : θ1),

(89)

where the Legendre-Fenchel divergence is deﬁned for a pair of strictly convex and diﬀerentiable
conjugate generators F (θ) and F ∗(η) = supθ θ(cid:62)η − F (θ) by

AF (θ : η(cid:48)) := F (θ) + F ∗(η(cid:48)) − θ(cid:62)η(cid:48),

(90)

with η(cid:48) = ∇F (θ).

Since F is deﬁned modulo some aﬃne function, we can choose F (θ(λ)) = − log p(ω; θ(λ)).

Furthermore, for exponential families, we have

and the Shannon entropy [46]

η(λ) = Epλ[t(x)],

h(p) = −

(cid:90)

X

p(x) log p(x)dµ(x),

admits the following expression [36] when p = pθ belongs to an exponential family E:

h(pλ) = −F ∗(η(λ)) − Epλ[k(x)].

(91)

(92)

(93)

Thus if we already have at our disposal (1) the expectation of the suﬃcient statistics, and (2)

the entropy, we can easily recover the Kullback-Leibler divergence as follows:

DKL[pλ1 : pλ2] = − log p(ω; λ2) − h(pλ1) − Epθ1

[k(x)] − θ(λ2)(cid:62)Epθ1

[t(x)].

(94)

For densities pλ1 and pλ2 belonging to the same exponential family, the Jeﬀreys divergence is

DJ [pλ1 : pλ2] = DKL[p : q] + DKL[q : p] = (θ2 − θ1)(cid:62)(η2 − η1).

It follows that we can write Jeﬀreys’ divergence using the following cumulant-free expression:

DJ [pλ1 : pλ2] = (θ(λ2) − θ(λ1))(cid:62) (Epλ2

[t(x)] − Epλ1

[t(x)]).

Note that a strictly monotone operator O deﬁnes a symmetric dissimilarity:

DO(θ1, θ2) := (θ1 − θ2)(cid:62)(O(θ2) − O(θ1)) ≥ 0,

(95)

(96)

(97)

with equality if and only if θ1 = θ2. Since ∇F is a strictly monotone operator and Epλ[k(x)] =
∇F (θ), we may reinterpret the Jeﬀreys’ divergence as a symmetric dissimilarity induced by a strictly
monotone operator.

Let us report now some illustrating examples. We start with an example illustrating the use of

a separable multivariate quasi-arithmetic mean.

17

Example 10 (continue Example 5) Consider the Dirichlet exponential family. The diﬀerential
entropy of a Dirichlet density pα is

h(pα) = log B(α) +

(cid:32)

(cid:88)

i

(cid:33)

αi − d

ψ

(cid:32)

(cid:88)

(cid:33)

αi

−

i

d
(cid:88)

j=1

(αj − 1) ψ (αj) ,

where ψ(·) denotes the digamma function. We have E[t(x)] = E[(log x1, . . . , log xd)] = (ψ(α1) −
ψ(α(cid:80)), . . . , ψ(α1) − ψ(α(cid:80))) where α(cid:80) = (cid:80)d
i=1 αi. It follows that the Kullback-Leibler between pα
and pα(cid:48) is:

DKL[pα : p(cid:48)

α] = log Γ (cid:0)α(cid:80)(cid:1)−

d
(cid:88)

i=1

log Γ (αi)−log Γ

(cid:17)

(cid:16)

α(cid:48)

(cid:80)

d
(cid:88)

+

log Γ (cid:0)α(cid:48)
i

i=1

d
(cid:88)

(cid:1)+

i=1

(cid:0)αi − α(cid:48)

i

(cid:1) (cid:0)ψ (αi) − ψ (cid:0)α(cid:80)(cid:1)(cid:1) .

Next, we report an example illustrating a non-separable multivariate quasi-arithmetic mean.

Example 11 (continue Example 6) Consider the d-dimensional multivariate Gaussian family.
Since k(x) = 0 (so that E[k(x)] = 0), η(λ) = ∇θF (θ(λ)) = (µ, µµ(cid:62) + Σ) (since E[x] = µ,
E[xx(cid:62)] = µµ(cid:62) + Σ), and the usual diﬀerential entropy is known as h(pµ,Σ) = 1
2 log |2πeΣ| =
2 (1 + log 2π) + 1
d

2 log |Σ| since |aM | = |a|d|M |. we recover the Kullback-Leibler divergence as

DKL[pµ1,Σ1 : pµ2,Σ2] =

1
2

and the Jeﬀreys divergence as

(cid:18)

tr(Σ−1

2 Σ1) + ∆(cid:62)

µ Σ−1

2 ∆µ + log

(cid:19)

− d

,

|Σ2|
|Σ1|

(98)

DJ [pµ1,Σ1, pµ2,Σ2] = ∆(cid:62)
µ

(cid:18) Σ−1

1 + Σ−1
2
2

(cid:19)

∆µ +

1
2

tr (cid:0)Σ−1

2 Σ1 + Σ−1

1 Σ2

(cid:1) − d.

(99)

3.3 The Kullback-Leibler divergence expressed as a log density ratio

Let us express the Bregman divergence with the equivalent generator Fl(θ) := − log(pθ(ω)) for any
prescribed ω ∈ X instead of the cumulant function of Eq. 7. We get

DKL[pλ1 : pλ2] = BFl(θ(λ2) : θ(λ1)),

= log

(cid:19)

(cid:18) pλ1(ω)
pλ2(ω)

− (θ(λ2) − θ(λ1))(cid:62)∇θFl(θ(λ1)).

Let us remark that

We have

Fl(θ) = −θ(cid:62)t(ω) + F (θ) − k(ω).

∇Fl(θ(λ1)) = −

∇θpλ1(ω)
pλ1(ω)

,

= −

(t(ω) − ∇F (θ(λ1)))pλ1(ω)
pλ1(ω)

,

= −(t(ω) − ∇F (θ(λ1))),

18

(100)

(101)

(102)

(103)

(104)

(105)

where F (θ) is the cumulant function of Eq. 7. Alternatively, we can also directly ﬁnd that
∇Fl(θ(λ1)) = −(t(ω) − ∇F (θ(λ1))) from Eq. 102.

It follows that:

DKL[pλ1 : pλ2] = log

(cid:19)

(cid:18) pλ1(ω)
pλ2(ω)

+ (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))),

∀ω ∈ X .

(106)

Thus when t(ω)−∇F (θ(λ1)) is Euclidean orthogonal to θ(λ2)−θ(λ1) (i.e., (θ(λ2)−θ(λ1))(cid:62)(t(ω)−
∇F (θ(λ1))) = 0), the Bregman divergence (and the corresponding KLD on swapped parameter
order of the densities) is expressed as a log-density ratio quantity. Let

X E

⊥(λ1 : λ2) :=

(cid:110)

ω ∈ X : (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))) = 0

(cid:111)

.

(107)

Then DKL[pλ1 : pλ2] = log

(cid:17)

(cid:16) pλ1 (ω)
pλ2 (ω)

for all ω ∈ X E

⊥(λ1 : λ2).

Lemma 1 The Kullback-Leibler divergence between two densities pλ1 and pλ2 belonging to a same
exponential family E is expressed as a log density ratio, DKL[pλ1 : pλ2] = log
, whenever
ω ∈ X E

(cid:16) pλ1 (ω)
pλ2 (ω)

(cid:17)

⊥(λ1 : λ2).

Notice that a suﬃcient condition is to choose ω such that t(ω) = η1 = ∇F (θ1).
Thus if we carefully choose ω ∈ X E

⊥(λ1 : λ2) according to the source parameters, we may express
the Kullback-Leibler divergence as a simple log density ratio without requiring the formula for the
diﬀerential entropy nor the moment.

Example 12 Consider the exponential family E = {pλ(x) = λ exp(−λx), λ > 0} of exponential
distributions with ∇F (θ) = 1
θ for θ = λ. We have (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))) = 0 that
amounts to (λ2 − λ1)(ω − 1
) = 0, i.e., ω = 1
). In that case, we have
λ1
λ1

⊥(λ1 : λ2) =

(and X E

(cid:110) 1
λ1

(cid:111)



DKL[pλ1 : pλ2] = log



pλ1

pλ2



 ,

(cid:17)

(cid:17)

(cid:16) 1
λ1
(cid:16) 1
λ1
(cid:16)





= log

= log

(cid:17)



 ,

(cid:17)

1
λ1

1
λ1

λ1 exp

−λ1

(cid:16)

−λ2

λ2 exp
(cid:19)

(cid:18) λ1
λ2

+

λ2
λ1

− 1.

(108)

(109)

(110)

This formula matches the expression of Eq. 85.

Similarly, we may rewrite the Bregman divergence BF (θ1 : θ2) as

BF (θ1 : θ2) = BFa(θ1 : θ2) = Fa(θ1) − Fa(θ2),

(111)

for Fa(θ) = F (θ) − aθ (and ∇Fa(θ) = ∇F (θ) − a) for a = −∇Fa(θ2).

19

Example 13 Consider the exponential family of d-dimensional Gaussians with ﬁxed covariance
matrix Σ:

(cid:40)

E =

pλ(x) =

1
2 (cid:112)|Σ|
d

(cid:18)

exp

−

1
2

(2π)

(x − λ)(cid:62)Σ−1(x − λ)

(cid:19)

(cid:41)

, : λ ∈ Rd

.

(112)

It is an exponential family of order D = d with suﬃcient statistic t(x) = x. Let us choose ω such
that (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))) = 0. For example, we choose t(ω) = ∇F (θ1) = µ1. It
follows that we have



DKL[pλ1 : pλ2] = log



pλ1

pλ2

 ,



(cid:17)

(cid:17)

(cid:16) 1
λ1
(cid:16) 1
λ1
2 (cid:112)|Σ|
2 (cid:112)|Σ|

d

d

(cid:32)

= log

(2π)

(2π)

exp

(cid:18) 1
2

(µ1 − µ2)(cid:62)Σ−1(µ1 − µ2)

(cid:19)(cid:33)

,

(114)

=

1
2

(µ1 − µ2)(cid:62)Σ−1(µ1 − µ2).

(115)

This is half of the squared Mahalanobis distance obtained for the precision matrix Σ−1. We recover
the Kullback-Leibler divergence between multivariate Gaussians (Eq. 98) when Σ1 = Σ2 = Σ.

Example 14 Consider the exponential family of Rayleigh distributions:

(cid:26)

E =

pλ(x) =

x
λ2 exp

(cid:18)

−

x2
2λ2

(cid:19)(cid:27)

,

(116)

for X = [0, ∞). Let us choose ω such that (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))) = 0 with θ = − 1
t(x) = x2 and ∇F (θ(λ)) = 2λ2. We choose ω2 = 2λ2

2). It follows that we have

√

2λ2 ,

DKL[pλ1 : pλ2] = log



(cid:17)



1 (i.e., ω = λ1
(cid:16) 1
λ1
(cid:16) 1
λ1

 ,

(cid:17)



pλ1

pλ2
(cid:32) √

= log

= 2 log

λ2
2
λ1
(cid:19)

2
λ1
(cid:18) λ2
λ1

(cid:18)

exp

−

2λ2
1
2λ2
1

+

2λ2
1
2λ2
2

(cid:19)(cid:33)

,

+

λ2
1
λ2
2

− 1.]

This example shows the limitation of the method which we shall now overcome.

Example 15 Consider the exponential family of univariate Gaussian distributions:

(cid:26)

E =

pλ(x) =

√

1
2πλ2

(cid:18)

exp

−

(x − λ1)2
2λ2

(cid:19)(cid:27)

,

for X = (−∞, ∞). Let us choose ω such that (θ(λ2) − θ(λ1))(cid:62)(t(ω) − ∇F (θ(λ1))) = 0. Here,
∇F (θ(λ1))) = (µ1, µ2

1) and t(ω) = (x, x2). Thus we have t(ω) (cid:54)= ∇F (θ(λ1)) for any ω ∈ X .

1 + σ2

20

(113)

(117)

(118)

(119)

(120)

Let H = {∇F (θ) : θ ∈ Θ} denote the dual moment parameter space. When the exponential
family is regular, Θ is an open convex set, and so is H (F is of Legendre-type). The problem we
faced with the last example is that ω ∈ ∂H. However since Eq. 106 holds for any ω ∈ Ω, let us
choose s values for ω (i.e., ω1, . . . , ωs), and average Eq. 106 for these s values. We have

DKL[pλ1 : pλ2] =

1
s

s
(cid:88)

i=1

log

(cid:19)

(cid:18) pλ1(ωi)
pλ2(ωi)

+ (θ(λ2) − θ(λ1))(cid:62)

(cid:32)

1
s

s
(cid:88)

i=1

(cid:33)

t(ωi) − ∇F (θ(λ1))

.(121)

We can now choose the ωi’s such that 1
s
s t(ωi) = ∇F (θ) = E[t(x)] is solvable.

the system of equations 1

(cid:80)s

i=1 t(ωi) ∈ H for s > 1. We need to choose s so that

Example 16 Let us continue Example 15. Consider s = 2. We need to ﬁnd ω1 and ω2 such that
we can solve the following system of equations:

1 = ω2
The solution of this system of equations is ω1 = µ1 − σ1 and ω2 = µ1 + σ1. Thus it follows that

.

(122)

(cid:40)

µ1
1 + σ2
µ2

,

= ω1+ω2
2
1+ω2
2
2

we have:

DKL[pµ1,σ1 : pµ2,σ2] =

1
2

(cid:18)

log

(cid:18) pµ1,σ1(µ1 − σ1)
pµ2,σ2(µ1 − σ1)

(cid:19)

+ log

(cid:18) pµ1,σ1(µ1 + σ1)
pµ2,σ2(µ1 + σ1)

(cid:19)(cid:19)

.

(123)

We conclude with the following theorem extending Lemma 1:

Theorem 1 The Kullback-Leibler divergence between two densities pλ1 and pλ2 belonging to a full
regular exponential family E of order D can be expressed as the averaged sum of logarithms of
density ratios:

DKL[pλ1 : pλ2] =

1
s

s
(cid:88)

i=1

log

(cid:18) pλ1(ωi)
pλ2(ωi)

(cid:19)

,

where ω1, . . . , ωs are s ≤ D + 1 distinct samples of X chosen such that 1
s

(cid:80)s

i=1 t(ωi) = Epλ1

[t(x)].

The bound s ≤ D + 1 follows from Carath´eodory’s theorem [9].
Notice that the Monte Carlo stochastic approximation of the Kullback-Leibler divergence:

(cid:101)D(m)

KL [pλ1 : pλ2] =

1
m

m
(cid:88)

i=1

log

pλ1(xi)
pλ2(xi)

,

(124)

for m independent and identically distributed (iid) variates x1, . . . , xm from pλ1 is also an average
sum of log density ratios which yields a consistent estimator, i.e.,

lim
m→∞

(cid:101)D(m)

KL [pλ1 : pλ2] = DKL[pλ1 : pλ2].

(125)

In practice, to avoid potential negative values of (cid:101)D(m)

KL [pλ1 : pλ2], we estimate the extended

Kullback-Leibler divergence:

(cid:101)D(m)
KL+

[pλ1 : pλ2] =

1
m

m
(cid:88)

(cid:18)

log

i=1

pλ1(xi)
pλ2(xi)

+ pλ2(xi) − pλ1(xi)

≥ 0,

(126)

(cid:19)

with limm→∞ (cid:101)D(m)
KL+

[pλ1 : pλ2] = DKL[pλ1 : pλ2].

21

Example 17 We continue Example 6 of the d-dimensional multivariate Gaussian family. First,
let us consider the subfamily of zero-centered Gaussian densities. The suﬃcient statistic t(x) is a
d × d matrix: t(x) = xx(cid:62). We ﬁnd the d column vectors ωi’s from the singular value decomposition
of Σ1:

Σ1 =

d
(cid:88)

i=1

λieie(cid:62)
i ,

where the λi’s are the eigenvalues and the ei’s the corresponding eigenvectors. Let ωi =
i ∈ {1, . . . , d}. We have

1
d

d
(cid:88)

i=1

t(ωi) = Σ1 = EpΣ1

[xx(cid:62)].

(127)

√

dλiei for

(128)

It follows that we can express the KLD between two zero-centered Gaussians pΣ1 and pΣ2 as the

following weighted sum of log density ratios:

DKL[pΣ1 : pΣ2] =

1
d

d
(cid:88)

i=1

log

√
√

(cid:18) pΣ1(
pΣ2(

(cid:19)

,

dλiei)
dλiei)

(129)

where the λi’s are the eigenvalues of Σ1 with the ei’s the corresponding eigenvectors. Notice that
the order of the family is D = d(d+1)

but we used s = d ≤ D vectors ω1, . . . , ωd.

The closed-form formula for the Kullback-Leibler divergence between two zero-centered Gaus-

2

sians pΣ1 and pΣ2 is

DKL[pΣ1 : pΣ2] =

(cid:18)

1
2

tr (cid:0)Σ2Σ−1

1

(cid:1) + log

(cid:19)

(cid:18) |Σ2|
|Σ1|

(cid:19)

− d

.

(130)

Now consider the full family of multivariate normal densities. We shall use 2d vectors ωi as

follows:

(cid:26) ωi = µ1 −
ωi = µ1 +

√
√

dλiei,
dλiei,

i ∈ {1, . . . , d}
i ∈ {d + 1, . . . , 2d}

(131)

(cid:80)2d

i=1 ωi = Epλ1

where the λi’s are the eigenvalues of Σ1 with the ei’s the corresponding eigenvectors. It can be
(cid:80)2d
[x] = µ1 and 1
checked that 1
1 + Σ1. These points are called
2d
2d
the “sigma points” in the unscented transform [23, 17]. Moreover, we have
Σ1]·,i, the
λiei = [
i-th column of the square root of the covariance matrix of Σ1. Thus it follows that
(cid:0)µ1 +
(cid:0)µ1 +

DKL[pµ1,Σ1 : pµ2,Σ2] =

i = µ1µ(cid:62)

dλiei
dλiei

i=1 ωiω(cid:62)

pµ1,Σ1
pµ2,Σ2

1
2d

+ log

d
(cid:88)

√
√

√
√

(cid:33)(cid:33)

log

(cid:1)
(cid:1)

(cid:1)
(cid:1)

√

√

(cid:32)

(cid:33)

(cid:32)

(cid:32)

,

(cid:0)µ1 −
(cid:0)µ1 −
(cid:0)µ1 − [
(cid:0)µ1 − [

pµ1,Σ1
pµ2,Σ2
(cid:32)

dλiei
dλiei
√
√

√
√

(cid:33)

(cid:1)
(cid:1)

dΣ1]·,i
dΣ1]·,i

+ log

pµ1,Σ1
pµ2,Σ2

(cid:0)µ1 + [
(cid:0)µ1 + [

(cid:33)(cid:33)

,

(cid:1)
(cid:1)

dΣ1]·,i
dΣ1]·,i

=

1
2d

i=1
d
(cid:88)

i=1

(cid:32)

(cid:32)

log

pµ1,Σ1
pµ2,Σ2

λiei denotes the vector extracted from the i-th column of the square root matrix

√

where [
of dΣ1.

√

dΣ1]·,i =

This formula matches the ordinary formula for the Kullback-Leibler divergence between the two

Gaussian densities pµ1,Σ1 and pµ2,Σ2:

DKL[pµ1,Σ1 : pµ2,Σ2] =

1
2

(cid:18)

(µ2 − µ1)(cid:62)Σ−1

2 (µ2 − µ1) + tr (cid:0)Σ2Σ−1

1

(cid:1) + log

(cid:19)

(cid:18) |Σ2|
|Σ1|

(cid:19)

− d

.

(132)

22

Example 18 We continue the Example 4 of the exponential family of gamma distributions which
has order D = 2. The suﬃcient statistic vector is t(x) = (x, log x), and we have E[x] = α
β and
E[log x] = ψ(α)−log β, where ψ(·) is the digamma function. We want to express DKL[pα1,β1 : pα2,β2]
as an average sum of log ratio of densities. To ﬁnd the values of ω1 and ω2, we need to solve the
following system of equations:

(cid:26) ω1+ω2

2

log ω1+log ω2
2

= a,
= b

,

with a = α1
β1

and b = ψ(α1) − log β1. We ﬁnd the following two solutions:

ω1 = a −

(cid:112)

a2 − exp(2b), ω2 = a +

(cid:112)

a2 − exp(2b).

We have a2 − exp(2b) ≥ 0 since E[x]2 ≤ exp(2E[log x]).

It follows that

DKL[pα1,β1 : pα2,β2] =

(cid:18)

log

1
2

pα1,β1(ω1)
pα2,β2(ω1)

+ log

pα1,β1(ω2)
pα2,β2(ω2)

(cid:19)

,

This expression of the KLD is the same as the ordinary expression of the KLD:

DKL[pα1,β1 : pα2,β2] = (α1 − α2)ψ(α1) − log Γ(α1) + log Γ(α2) + α2 log

β1
β2

+ α1

β2 − β1
β1

.

(136)

Example 19 Consider the exponential family of Beta distributions:

(cid:26)

E =

pα,β(x) =

xα−1(1 − x)β−1
B(α, β)

: α > 0, β > 0, x ∈ X = (0, 1)

,

(137)

(cid:27)

where

B(α, β) =

Γ(α)Γ(β)
Γ(α + β)

(138)

The suﬃcient statistic vector is t(x) = (log x, log(1−x)). We have Epα1,β1
β1) =: A and Epα1,β1
of equations for s = 2:

[log(x)] = ψ(α1)−ψ(α1 +
[log(1 − x)] = ψ(β1) − ψ(α1 + β1) =: B. We need to solve the following system

This system is equivalent to

2

log(1−ω1)+log(1−ω2)
2

(cid:40) log ω1+log ω2

= A,
= B

We ﬁnd

where

(cid:26) ω1ω

= exp(2A) := a,
(1 − ω1)(1 − ω2) = exp(2B) := b

ω1 =

ω2 =

−b + a + 1 −

2

−b + a + 1 +

2

√

√

∆

∆

,

,

∆ = b2 − 2(a + 1)b + a2 − 2a + 1.

23

(133)

(134)

(135)

(139)

(140)

(141)

(142)

(143)

It follows that we have

DKL[pα1,β1 : pα2,β2] =

(cid:18)

log

1
2

pα1,β1(ω1)
pα2,β2(ω1)

+ log

pα1,β1(ω2)
pα2,β2(ω2)

(cid:19)

.

(144)

This formula is equivalent to the ordinary formula for the KLD between two beta densities pα1,β1

and pα2,β2:

DKL[pα1,β1 : pα2,β2] =
(cid:19)

(cid:18) B (α2, β2)
B(α1, β1)

log

+ (α1 − α2) ψ(α1) + (β1 − β2) ψ(β1) + (α2 − α1 + β2 − β1) ψ(α1 + β1).

(145)

Notice that the ωi’s are chosen according to λ1. Thus we may express the Voronoi bisector:

Bi(pλ1, pλ2) := {λ : DKL[pλ : pλ1] = DKL[pλ : pλ2]}

as follows:

(cid:40)

Bi(pλ1, pλ2) =

λ :

1
s

s
(cid:88)

i=1

log

(cid:19)

(cid:18) pλ(ωi)
pλ1(ωi)

=

1
s

s
(cid:88)

i=1

log

(cid:18) pλ(ωi)
pλ2(ωi)

(cid:19)(cid:41)

.

In particular, when s = 1, the Voronoi bisector is expressed as:

Bi(pλ1, pλ2) = {λ : pλ1(ω(λ)) = pλ2(ω(λ))} ,

(146)

(147)

(148)

where ω(λ) emphasizes on the fact that ω is a function of λ. The statistical Voronoi diagrams of a
ﬁnite set of densities belonging to an exponential family has been studied as equivalent Bregman
Voronoi diagrams in [7].

3.4 The Jensen-Shannon divergence

The Jensen-Shannon divergence [29] (JSD) is another symmetrization of the Kullback-Leibler di-
vergence which can be given many information-theoretic interpretations [32] and which is further
guaranteed to be always bounded by log 2 (KLD and JD are unbounded):

DJS[p, q]

:=

1
2

= h

(cid:18)

(cid:20)

DKL
(cid:18) p + q
2

(cid:19)

−

h(p) + h(q)
2

.

p :

(cid:21)

p + q
2

(cid:20)

+ DKL

q :

(cid:21)(cid:19)

,

p + q
2

(149)

(150)

pθ1 +pθ2
2

Usually, the JSD does not provably admit a closed-form formula [40]. However, in the particular
case when the mixture
belongs to the same parametric family of densities, we can calculate
the Jensen-Shannon divergence using the entropy function as shown in Eq. 150. For example, con-
sider a mixture family in information geometry [1]. That is, a statistical mixture with k prescribed
components p1(x), . . . , pk(x) which are linearly independent (so that all mixtures of the family are
identiﬁable by their corresponding parameters). Let mλ(x) = (cid:80)k
i=1 wipi(x). In that particular case
(e.g., mixture family with k prescribed Gaussians components), we get

mλ1 + mλ2
2

= m λ1+λ2

2

.

24

(151)

Thus the JSD for a mixture family can be expressed using the entropy as:

DJS[mλ1, mλ1] = h

(cid:16)

m λ1+λ2

2

(cid:17)

−

h(mλ1) + h(mλ2)
2

.

(152)

Although we do not have closed-form formula for the entropy of a mixture (except in few
cases, e.g., when the support of the distributions are pairwise disjoint [32]), but we can use any
approximation method for calculating the entropy of a mixture to approximate or bound [40] the
Jensen-Shannon divergence DJS.

4 Conclusion

We have described several methods to easily recover closed-form formula for some common
(dis)similarities between densities belonging to a same exponential family {p(x; λ)}λ∈Λ which ex-
press themselves using the cumulant function F of the exponential family (e.g., the Kullback-Leibler
divergence amounts to a reverse Bregman divergence and the Bhattacharyya distance amounts to a
Jensen divergence). Our trick consists in observing that the generators F of the Bregman or Jensen
divergences are deﬁned modulo an aﬃne term, so that we may choose F (θ(λ)) = − log p(ω, λ) for
any ω falling inside the support X . It follows that the Bhattacharyya coeﬃcient can be calculated
with the following cumulant-free expression:

ρ[pλ1, pλ2] =

p(ω; ¯λ)
(cid:112)p(ω, λ1)p(ω, λ2)
where ¯λ = Mθ(λ1, λ2) is a generalized quasi-arithmetic mean induced by the ordinary-to-natural
parameter conversion function θ(λ). Thus our method requires only partial canonical factorization
of the densities of an exponential family to get θ(λ). The formula for the Bhattacharyya distance,
Hellinger distance, and α-divergences follow straightforwardly:

∀ω ∈ X

(153)

,

DB[pλ1, pλ2] = log

(cid:32) (cid:112)p(ω, λ1)p(ω, λ2)
p(ω; ¯λ)

(cid:33)

,

(cid:115)

DH [pλ1, pλ2] =

1 −

p(ω; ¯λ)
(cid:112)p(ω, λ1)p(ω, λ2)

,

(154)

(155)

Dα[pλ1 : pλ2] =

1
α(1 − α)

(cid:32)

1 −

p(ω; ¯λ)
(cid:112)p(ω, λ1)p(ω, λ2)

(cid:33)

, α ∈ R\{0, 1}.

(156)

In practice, it is easy to program those formula using legacy software APIs which oﬀer many
parametric densities in their library: First, we check that the distribution is an exponential family.
Then we set ω to be any point of the support X , partially factorize the distribution in order to
retrieve θ(λ) and its reciprocal function λ(θ), and equipped with these functions, we implement the
corresponding generalized weighted quasi-arithmetic mean function Mθ,α(a, b) = θ−1(αθ(a) + (1 −
α)θ(b)) to calculate ¯λ = Mθ(λ1, λ2).

To calculate the Kullback-Leibler divergence (and Jeﬀreys’ divergence) without the explicit use
of the cumulant function, we reported two methods: The ﬁrst method consists in expressing the
KLD as a limit of α-skew Bhattacharyya distance which writes as:

DKL[pλ1 : pλ2] = log

(cid:19)

(cid:18) p(ω; λ1)
p(ω; λ2)

+ lim
α→0

1
α

log

(cid:18) p (ω; Mθ,α(λ2, λ1))
p(ω, λ1)

(cid:19)

.

(157)

25

This limit can be calculated symbolically using a computer algebra system, or approximated for a
small value of α by

DKL,α[pλ1 : pλ2] = log

(cid:19)

(cid:18) p(ω; λ1)
p(ω; λ2)

+

1
α

log

(cid:18) p (ω; Mθ,α(λ2, λ1))
p(ω; λ1)

(cid:19)

.

(158)

When dealing with uni-order exponential family, we can use a ﬁrst-order approximation of the

weighted quasi-arithmetic mean to express the KLD as the following limit:

DKL[pλ1 : pλ2] = log

(cid:19)

(cid:18) p(ω; λ1)
p(ω; λ2)

+ lim
α→0

1
α

(cid:16)



p

log



ω; λ1 + α θ(λ2)−θ(λ1)
p(ω; λ1)

θ(cid:48)(λ1)

(cid:17)



 .

(159)

Notice that we can also estimate DKL,α, ρα and related dissimilarities (e.g., when the cumulant

function is intractable) using density ratio estimation techniques [48].

The second approach consists in using the entropy and moment formula which are often available
when dealing with parametric distributions. When the parametric distributions form an exponential
family, the KLD is equivalent to a Legendre-Fenchel divergence, and we write this Legendre-Fenchel
divergence as:

DKL[pλ1 : pλ2] = − log p(ω; λ2) − h(pλ1) − Epλ1

[k(x)] − θ(λ2)(cid:62)Epλ1

[t(x)].

(160)

It follows that the Jeﬀreys’ divergence is expressed as

DJ [pλ1, pλ2] = (θ(λ2) − θ(λ1))(cid:62) (Epλ2

[t(x)] − Epλ1

[t(x)]).

(161)

Finally, we proved in §3.3 that the Kullback-Leibler divergence between two densities pλ1 and

pλ2 of an exponential family E of order D can be expressed as

DKL[pλ1 : pλ2] =

1
s

s
(cid:88)

i=1

log

(cid:18) pλ1(ωi)
pλ2(ωi)

(cid:19)

,

where ω1, . . . , ωs are s ≤ D+1 distinct samples of X chosen such that 1
[t(x)]. We
s
illustrated how to ﬁnd the ωi’s for the univariate Gaussian family and the multivariate zero-centered
Gaussian family.

i=1 t(ωi) = Epλ1

(cid:80)s

To conclude this work,

let us emphasize that we have revealed a new kind of invariance
when providing closed-form formula for common (dis)similarities between densities of an expo-
nential family without explicitly using the cumulant function of that exponential family: For the
Bhattacharrya/Hellinger/α-divergences, the ω can be chosen as any arbitrary point of the support
X . For the Kullback-Leibler divergence, by carefully choosing a set of ω’s, we may express the
Kullback-Leibler divergence as a weighted sum of log density ratios.

References

[1] Shun-ichi Amari. Information Geometry and Its Applications. Applied Mathematical Sciences.

Springer Japan, 2016.

[2] Tsuyoshi Ando and Fumio Hiai. Operator log-convex functions and operator means. Mathe-

matische Annalen, 350(3):611–630, 2011.

26

[3] Ole Barndorﬀ-Nielsen. Information and exponential families. John Wiley & Sons, 2014.

[4] Christian Bauckhage. Computing the Kullback–Leibler divergence between two Weibull dis-

tributions. arXiv preprint arXiv:1310.3713, 2013.

[5] Anil Bhattacharyya. On a measure of divergence between two multinomial populations.

Sankhy¯a: the indian journal of statistics, pages 401–406, 1946.

[6] Patrick Billingsley. Probability and Measure. Wiley, 3 edition, April 1995.

[7] Jean-Daniel Boissonnat, Frank Nielsen, and Richard Nock. Bregman Voronoi diagrams. Dis-

crete & Computational Geometry, 44(2):281–307, 2010.

[8] Lev M. Bregman. The relaxation method of ﬁnding the common point of convex sets and
its application to the solution of problems in convex programming. USSR computational
mathematics and mathematical physics, 7(3):200–217, 1967.

[9] Constantin Carath´eodory. ¨Uber den Variabilit¨atsbereich der Koeﬃzienten von Potenzreihen,

die gegebene Werte nicht annehmen. Mathematische Annalen, 64(1):95–115, 1907.

[10] Herman Chernoﬀ et al. A measure of asymptotic eﬃciency for tests of a hypothesis based on

the sum of observations. The Annals of Mathematical Statistics, 23(4):493–507, 1952.

[11] Francis Clarke. On the inverse function theorem. Paciﬁc Journal of Mathematics, 64(1):97–

102, 1976.

[12] Thomas M. Cover and Joy A. Thomas. Elements of information theory. John Wiley & Sons,

2012.

[13] Imre Csisz´ar. Information-type measures of diﬀerence of probability distributions and indirect

observation. studia scientiarum Mathematicarum Hungarica, 2:229–318, 1967.

[14] Joan Del Castillo. The singly truncated normal distribution: a non-steep exponential family.

Annals of the Institute of Statistical Mathematics, 46(1):57–66, 1994.

[15] Shinto Eguchi and Osamu Komori. Path connectedness on a space of probability density
functions. In International Conference on Geometric Science of Information, pages 615–624.
Springer, 2015.

[16] Jun Ichi Fujii. Path of quasi-means as a geodesic. Linear algebra and its applications,

434(2):542–558, 2011.

[17] Jacob Goldberger, Hayit K Greenspan, and Jeremie Dreyfuss. Simplifying mixture models
using the unscented transform. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 30(8):1496–1502, 2008.

[18] Teoﬁlo F Gonzalez. Clustering to minimize the maximum intercluster distance. Theoretical

Computer Science, 38:293–306, 1985.

[19] Ernst Hellinger. Neue begr¨undung der theorie quadratischer formen von unendlichvielen
ver¨anderlichen. Journal f¨ur die reine und angewandte Mathematik (Crelles Journal), 136:210–
271, 1909.

27

[20] Harold Jeﬀreys. An invariant form for the prior probability in estimation problems. Proceedings
of the Royal Society of London. Series A. Mathematical and Physical Sciences, 186(1007):453–
461, 1946.

[21] Robert Jenssen, Jose C Principe, Deniz Erdogmus, and Torbjørn Eltoft. The Cauchy–Schwarz
divergence and Parzen windowing: Connections to graph theory and Mercer kernels. Journal
of the Franklin Institute, 343(6):614–629, 2006.

[22] Shihao Ji, Balaji Krishnapuram, and Lawrence Carin. Variational Bayes for continuous hidden
Markov models and its application to active learning. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 28(4):522–532, 2006.

[23] Simon Julier, Jeﬀrey Uhlmann, and Hugh F Durrant-Whyte. A new method for the nonlinear
IEEE Transactions on

transformation of means and covariances in ﬁlters and estimators.
automatic control, 45(3):477–482, 2000.

[24] Thomas Kailath. The divergence and Bhattacharyya distance measures in signal selection.

IEEE transactions on communication technology, 15(1):52–60, 1967.

[25] Solomon Kullback and Richard A Leibler. On information and suﬃciency. The annals of

mathematical statistics, 22(1):79–86, 1951.

[26] Frank Nielsen. Closed-form information-theoretic divergences for statistical mixtures. In Pro-
ceedings of the 21st International Conference on Pattern Recognition (ICPR2012), pages 1723–
1726. IEEE, 2012.

[27] Frank Nielsen. An information-geometric characterization of Chernoﬀ information.

IEEE

Signal Processing Letters, 20(3):269–272, 2013.

[28] Frank Nielsen. An elementary introduction to information geometry.

arXiv preprint

arXiv:1808.08271, 2018.

[29] Frank Nielsen. On the Jensen–Shannon symmetrization of distances relying on abstract means.

Entropy, 21(5):485, 2019.

[30] Frank Nielsen. A generalization of the α-divergences based on comparable and distinct weighted

means. CoRR, abs/2001.09660, 2020.

[31] Frank Nielsen. A note on Onicescu’s informational energy and correlation coeﬃcient in expo-

nential families. arXiv preprint arXiv:2003.13199, 2020.

[32] Frank Nielsen. On a generalization of the Jensen–Shannon divergence and the Jensen–Shannon

centroid. Entropy, 22(2):221, 2020.

[33] Frank Nielsen and Sylvain Boltz. The Burbea-Rao and Bhattacharyya centroids. IEEE Trans-

actions on Information Theory, 57(8):5455–5466, 2011.

[34] Frank Nielsen and Vincent Garcia. Statistical exponential families: A digest with ﬂash cards.

Technical report, arXiv:0911.4863, 2009.

28

[35] Frank Nielsen and Ga¨etan Hadjeres. On power chi expansions of f -divergences. arXiv preprint

arXiv:1903.05818, 2019.

[36] Frank Nielsen and Richard Nock. Entropies and cross-entropies of exponential families. In

IEEE International Conference on Image Processing, pages 3621–3624. IEEE, 2010.

[37] Frank Nielsen and Richard Nock. A closed-form expression for the Sharma–Mittal entropy of
exponential families. Journal of Physics A: Mathematical and Theoretical, 45(3):032003, 2011.

[38] Frank Nielsen and Richard Nock. On the chi square and higher-order chi distances for approx-

imating f -divergences. IEEE Signal Processing Letters, 21(1):10–13, 2013.

[39] Frank Nielsen and Richard Nock. Patch matching with polynomial exponential families and
In International Conference on Similarity Search and Applications,

projective divergences.
pages 109–116. Springer, 2016.

[40] Frank Nielsen and Ke Sun. Guaranteed bounds on information-theoretic measures of univariate

mixtures using piecewise log-sum-exp inequalities. Entropy, 18(12):442, 2016.

[41] Frank Nielsen, Ke Sun, and St´ephane Marchand-Maillet. On H¨older projective divergences.

Entropy, 19(3):122, 2017.

[42] Emilio Porcu, Jorge Mateu, and George Christakos. Quasi-arithmetic means of covariance
functions with potential applications to space–time data. Journal of Multivariate Analysis,
100(8):1830–1844, 2009.

[43] Thomas W Rauber, Tim Braun, and Karsten Berns. Probabilistic distance measures of the

Dirichlet and Beta distributions. Pattern Recognition, 41(2):637–645, 2008.

[44] Robert H Risch. The solution of the problem of integration in ﬁnite terms. Bulletin of the

American Mathematical Society, 76(3):605–608, 1970.

[45] Christophe Saint-Jean and Frank Nielsen. Hartigans method for k-mle: Mixture modeling
In Geometric theory of

with Wishart distributions and its application to motion retrieval.
information, pages 301–330. Springer, 2014.

[46] Claude E Shannon. A mathematical theory of communication. Bell system technical journal,

27(3):379–423, 1948.

[47] Jana ˇSpirkov´a. Weighted operators based on dissimilarity function.

Information Sciences,

281:172–181, 2014.

[48] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. Density ratio estimation in machine

learning. Cambridge University Press, 2012.

[49] John Wishart. The generalised product moment distribution in samples from a normal multi-

variate population. Biometrika, pages 32–52, 1928.

29

Closed-form formula using the Maxima computer algebra system

Since the statistical (dis)similarities rely on integral calculations, we may use symbolic calculations
to check the results. For example, below is some code snippets written in Maxima.7 The code
snippet below calculates symbolically the Bhattacharyya coeﬃcient for several exponential families.

/* Quasi-arithmetic mean associated with the univariate Gaussian family */
ptheta(lambda):=[lambda[1]/lambda[2],-1/(2*lambda[2])];
plambda(theta):=[-theta[1]/(2*theta[2]),-1/(2*theta[2])];
ptheta(plambda([t0,t1]));
l1: [p1,p2];
l2: [q1,q2];
plambda(0.5*ptheta(l1)+0.5*ptheta(l2));
ratsimp(%);
/* end */

/* Quasi-arithmetic mean associated with the inverse Gaussian family */
ptheta(lambda):=[-lambda[2]/(2*lambda[1]**2),-lambda[2]/2];
plambda(theta):=[sqrt(theta[2]/theta[1]),-2*theta[2]];
ptheta(plambda([t0,t1]));
l1: [p1,p2];
l2: [q1,q2];
plambda(0.5*ptheta(l1)+0.5*ptheta(l2));
ratsimp(%);
/* end */

/* Exponential family of exponential distributions */
assume(lambda1>0);
assume(lambda2>0);
p(x,lambda) := lambda*exp(-lambda*x);

integrate(sqrt(p(x,lambda1)*p(x,lambda2)),x,0,inf);
ratsimp(%);
/* end */

/* Exponential family of zero-centered Gaussian densities */
assume(sigma>0);
p(x,sigma) := (1.0/(2*sigma))*exp(-abs(x)/sigma);
assume(sigma1>0);
assume(sigma2>0);

integrate(sqrt(p(x,sigma1)*p(x,sigma2)),x,-inf,inf);
ratsimp(%);
/* end */

7http://maxima.sourceforge.net/

30

/* Exponential family of centered-Laplacian distributions */
assume(lambda1>0);
assume(lambda2>0);
p(x,lambda) := (1/(2*lambda))*exp(-abs(x)/lambda);

integrate(sqrt(p(x,lambda1)*p(x,lambda2)),x,-inf,inf);
ratsimp(%);
/* end*/

/* Exponential family of Weibull densities with prescribed shape parameter k */
declare( k , integer);
assume(k>=1);
assume(lambda1>0);
assume(lambda2>0);
p(x,lambda) := (k/lambda)*(x/lambda)**(k-1)*exp(-(x/lambda)**k);

integrate(sqrt(p(x,lambda1)*p(x,lambda2)),x,0,inf);
expand(ratsimp(%));
/* end */

/* KLD betwen Weibull distributions by symbolic computing of the limit */
declare( k , integer);
assume(lambda1>0);
assume(lambda2>0);
k:3;
omega:1;
t(u):=u**(-k);
tp(u):=k*u**(-k-1);
p(x,l):=(k/l)*((x/l)**(k-1))*exp(-(x/l)**k);
mean(l1,l2):=l1+alpha*(t(l1)-t(l2))/tp(l1);
log(p(omega,l1)/p(omega,l2)) + (1.0/alpha)*log(p(omega,mean(l1,l2))/p(omega,l1));
limit (ratsimp(%), alpha, 0);
expand(%);
/* end */

A Further illustrating examples

The Laplacian exponential family illustrates the use of the harmonic mean H(a, b) = 2ab
a, b > 0:

a+b for

Example 20 Consider the family of zero-centered Laplacian densities [34] pλ(x) = 1
for x ∈ R. We have pλ(ω) = 1
H(λ1, λ2) = 2λ1λ2
λ1+λ2

2λ exp(− |x|
λ )
u = θ−1(u) so that ¯λ = Mθ(λ1, λ2) =
, where H denotes the harmonic mean. Applying the cumulant-free formula

2λ for ω = 0, θ(u) = − 1

31

Eq. 49, we get

ρ[pλ1, pλ2] =

=

=

,

(cid:112)p(ω, λ1)p(ω, λ2)
p (cid:0)ω; ¯λ(cid:1)
1
(cid:113) 1
2λ1

1
2 λ2

,

1
2 2λ1λ2
λ1+λ2
√
λ1λ2
2
λ1 + λ2

.

Note that the arithmetic mean A(λ1, λ2) = λ1+λ2
(i.e., A ≥ G) so that ρ[pλ1, pλ2] = G(λ1,λ2)

2

dominates the geometric mean G(λ1, λ2) =

√

λ1λ2

A(λ1,λ2) ∈ (ω, 1]. It follows that the Bhattacharyya distance is

DBhat[pλ1, pλ2] = log

λ1 + λ2
2

− log (cid:112)λ1 log λ2.

This yields the same formula as the exponential distribution.

Example 21 The univariate Gaussian distribution with source parameters λ = (µ, σ2) has density
pλ(x) = 1√
). We have

. The have θ(u) = ( u1
u2

) and θ−1(u) = (− u1
2u2

− (x−λ1)2
2λ2

, − 1
2u2

, − 1
2u2

exp

(cid:16)

(cid:17)

2πλ2

(cid:18) µ1σ2
2 + µ2σ2
1
1 + σ2
σ2
2
Our next example illustrates an unusual non-separable mean derived from the inverse Gaussian

1σ2
2σ2
2
1 + σ2
σ2
2

Mθ(λ1, λ2) =

(162)

(cid:19)

.

,

family:

Example 22 Consider the family of inverse Gaussian densities:

(cid:114)

p(x; µ, λ) =

λ
2πx3 exp
with X = (0, ∞), and source parameters (µ > 0, λ > 0). The inverse Gaussian densities form an
. The inverse conversion function
exponential family with natural parameters θ(λ) =
(cid:17)

λ(x − µ)2
2µ2x

− λ2
2λ2
1

, − λ2
2

. Thus the generalized quasi-arithmetic mean induced by the multivariate

−

(cid:16)

(cid:17)

,

, −2θ2

(cid:18)

(cid:19)

is θ−1(θ1, θ2) =
θ(·) is

(cid:16) θ2
θ1

Mθ(λ, λ(cid:48)) =

(cid:32)(cid:115)

(λ2 + λ(cid:48)
λ2(λ(cid:48)

1(λ(cid:48)
2)λ2
1)2
2λ2
1)2 + λ(cid:48)
1

,

λ2 + λ(cid:48)
2
2

(cid:33)

.

The calculation can be checked using a computer algebra system (detailed in Appendix 4). We
choose ω = 1.

Next, we consider the case of the exponential family of central Wishart densities [45].

Example 23 The density of a Wishart distribution [49] with positive-deﬁnite scale matrix S (cid:31) 0
and number of degrees of freedom n ≥ d (matrix generalization of the chi-squared distribution) is:

p(x; λ = (λs = n, λM = S)) =

32

n−d−1
2

|X|

nd
2 |S|

2

e− 1
2 Γd

2 tr(S−1X)
(cid:1)
(cid:0) n
2

n

,

(163)

where Γd denotes the multivariate Gamma function:

Γd(x) := πd(d−1)/4

d
(cid:89)

j=1

Γ(x + (1 − j)/2).

(164)

The sample space x ∈ X is the set of d × d positive-deﬁnite matrices. The natural parameter

consists of a scalar part θs and a matrix part θM :

θ(λ) = (θs, θM ) =

(cid:18) λs − d − 1
2

, −

(cid:19)

.

1
2

λ−1
M

The inverse conversion natural→ordinary function is

(cid:18)

λ(θ) =

2λs + d + 1, −

(cid:19)

.

1
2

λ−1
M

(165)

(166)

It follows that the quasi-arithmetic mean is the following separable scalar-vector quasi-arithmetic

mean (scalar arithmetic mean with a harmonic matrix mean):

Mθ((n1, S1), (n2, S2)) =

(cid:32)

n1 + n2
2

,

(cid:18) S−1

1 + S−1
2
2

(cid:19)−1(cid:33)

.

We choose ω = I (the identity matrix) so that

p(I; S, n) =

exp (cid:0)− 1

nd
2 |S|

2

2 tr(S−1)(cid:1)
(cid:1)
(cid:0) n
2 Γd
2

n

.

(167)

(168)

The suﬃcient statistics is t(x) = (log |X|, − 1

It follows that η = ∇F (θ) = E[t(x)] =
(− log | S−1
2 nS) (see [22]), where ψd is the multivariate digamma function
(the derivative of the logarithm of the multivariate gamma function). The diﬀerential entropy is:

i=1 ψ( n+1−i

2 | + (cid:80)d

2 X).

), − 1

2

h(pn,S) =

d + 1
2

ln |S| +

1
2

d(d + 1) ln 2 + ln Γd

(cid:17)

(cid:16) n
2

−

n − d − 1
2

ψd

(cid:17)

(cid:16) n
2

+

nd
2

.

It follows that the Kullback-Leibler divergence between two central Wishart densities is:

DKL (pn1,S1 : pn2,S2) = − log

(cid:32)

Γd
Γd

(cid:1)
(cid:1)

(cid:0) n1
2
(cid:0) n2
2

(cid:33)

(cid:18) n1 − n2
2

+

(cid:19)

ψd

(cid:16) n1
2

(cid:17)

+

n1
2

(cid:18)

− log

|S1|
|S2|

+ tr (cid:0)S−1

2 S1

(cid:19)

(cid:1) − d

.

(169)

This last example exhibits a non-usually mean when dealing with the Bernoulli family:

Example 24 Consider the Bernoulli family with density pλ(x) = λx(1 − λ)1−x where X = {0, 1}
and λ denotes the probability of success. The Bernoulli family is a discrete exponential family. We
have θ(u) = log u

1+eu . The associated quasi-arithmetic mean is

1−u and θ−1(u) = eu

Mθ(λ1, λ2) =

(cid:115)

λ1λ2
(1 − λ1)(1 − λ2)

.

(170)

33

