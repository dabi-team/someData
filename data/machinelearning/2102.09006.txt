Noname manuscript No.
(will be inserted by the editor)

Diﬀerence of convex algorithms for bilevel programs
with applications in hyperparameter selection

Jane J. Ye · Xiaoming Yuan · Shangzhi
Zeng · Jin Zhang

Received: date / Accepted: date

Abstract In this paper, we present diﬀerence of convex algorithms for solving
bilevel programs in which the upper level objective functions are diﬀerence of
convex functions, and the lower level programs are fully convex. This nontrivial
class of bilevel programs provides a powerful modelling framework for dealing
with applications arising from hyperparameter selection in machine learning.
Thanks to the full convexity of the lower level program, the value function of
the lower level program turns out to be convex and hence the bilevel program
can be reformulated as a diﬀerence of convex bilevel program. We propose two
algorithms for solving the reformulated diﬀerence of convex program and show
their convergence to stationary points under very mild assumptions. Finally we

This paper is dedicated to the memory of Olvi L. Mangasarian.

The research of the ﬁrst author was partially supported by NSERC. The second author
was supported by a General Research Fund from Hong Kong Research Grants Council. The
third author was supported by the Paciﬁc Institute for the Mathematical Sciences (PIMS).
The last author was supported by NSFC (No. 12222106), Shenzhen Science and Technol-
ogy Program (No. RCYX20200714114700072) and the Guangdong Basic and Applied Basic
Research Foundation (No. 2022B1515020082) .

Jane J. Ye
Department of Mathematics and Statistics, University of Victoria, Canada.
E-mail: janeye@uvic.ca

Xiaoming Yuan
Department of Mathematics, The University of Hong Kong, Hong Kong SAR, China.
E-mail: xmyuan@hku.hk

Shangzhi Zeng
Department of Mathematics and Statistics, University of Victoria, Canada.
E-mail: zengshangzhi@uvic.ca

Corresponding author. Jin Zhang
Department of Mathematics, SUSTech International Center for Mathematics, Southern Uni-
versity of Science and Technology, National Center for Applied Mathematics Shenzhen, Peng
Cheng Laboratory, Shenzhen, China.
E-mail: zhangj9@sustech.edu.cn

2
2
0
2

g
u
A
8
2

]

C
O
.
h
t
a
m

[

2
v
6
0
0
9
0
.
2
0
1
2
:
v
i
X
r
a

 
 
 
 
 
 
2

Jane J. Ye et al.

conduct numerical experiments to a bilevel model of support vector machine
classiﬁcation.

Keywords Bilevel program
selection, bilevel model of support vector machine classiﬁcation

diﬀerence of convex algorithm

·

·

hyperparameter

Mathematics Subject Classiﬁcation (2010) 90C26

90C30

·

1 Introduction

Bilevel programs are a class of hierarchical optimization problems which have
constraints containing a lower-level optimization problem parameterized by
upper-level variables. Bilevel programs capture a wide range of important
applications in various ﬁelds including Stackelberg games and moral hazard
problems in economics ([29,41]), hyperparameter selection and meta learning
in machine learning ([16,21,22,23,26,27,30,31,34]). More applications can be
found in the monographs [3,12,15,40], the survey on bilevel optimization [11,
14] and the references within.

In this paper, we develop some numerical algorithms for solving the fol-

lowing diﬀerence of convex (DC) bilevel program:

(DCBP)

min
x∈Rn,y∈Rm

F (x, y) := F1(x, y)

F2(x, y)

−

s.t.

x

X, y

∈

∈

S(x),

with S(x) being the set of optimal solutions of the lower level problem,

(Px) : min
y∈Y

f (x, y)

s.t. g(x, y)

0,

≤

⊆

Rn and Y
Rm

where X
functions gi : Rn
containing the set X
convex on an open convex set containing the set

Rm are nonempty closed sets, g := (g1, . . . , gl), all
R, i = 1, . . . , l are convex on an open convex set
R are

⊆
→
Y , and the functions F1, F2, f : Rn

Rm

→

×

×

×

C :=

(x, y)

X

Y : g(x, y)

0

.

{

∈
for all
To ensure the bilevel program is well-deﬁned, we assume that S(x)
∅
X, the
x
O ⊇
feasible region for the lower level program
is
0
Y : g(x, y)
nonempty and the lower level objective function f (x, y) is bounded below on

X. Moreover we assume that for all x in an open convex set

(x) :=

≤

≤

×

=

F

∈

∈

{

}

}

y

(x).

F

Although the objective function in the DC bilevel program we consider
must be a DC function, this setting is general enough to capture many cases
of practical interests. In particular any lower C2 function (i.e., a function
which can be locally written as a supremum of a family of C2 functions) and
C1+ function (i.e., a diﬀerentiable function whose gradient is locally Lipschitz
continuous) are DC functions and the class of DC functions is closed under

6
Diﬀerence of convex algorithms for bilevel programs

3

many operations encountered frequently in optimization; see, e.g., [19,44]. In
the lower level program, we assume all functions are fully convex, i.e., convex in
both variables x and y. However as pointed out by [24, Example 1 and Section
5], using some suitable reformulations one may turn a non-fully convex lower
level program into a fully convex one. Also as demonstrated in this paper, the
bilevel model for hyperparameter selection problem can be reformulated as a
bilevel program where the lower level is fully convex.

Solving bilevel programs numerically is extremely challenging. It is known
that even when all deﬁning functions are linear, the computational complexity
is already NP-hard [5]. If all deﬁning functions are smooth and the lower level
program is convex with respect to the lower level variable, the ﬁrst order ap-
proach was popularly used to replace the lower level problem by its ﬁrst order
optimality condition and to solve the resulting problem as the mathematical
program with equilibrium constraints (MPEC); see e.g. [1,3,14,28,36]. The
ﬁrst order approach may be problematic since it may not provide an equiv-
alent reformulation to the original bilevel program if only local (not global)
optimal solutions are considered; see [13]. Moreover even in the case of a fully
convex lower level program, [24, Example 1] shows that it is still possible that
a local optimal solution of the corresponding MPEC does not correspond to a
local optimal solution of the original bilevel program. Recently some numer-
ical algorithms have been introduced for solving bilevel programs where the
lower level problem is not necessarily convex in the lower level variable; see
e.g., [25,32,33]. However these approaches have limitations in the numbers of
variables in the bilevel program. In most of literature on numerical algorithms
for solving bilevel programs, smoothness of all deﬁning functions are assumed.
In some special cases, non-smoothness can be dealt with by introducing auxil-
iary variables and constraints to reformulate a nonsmooth lower level program
as a smooth constrained lower level program. But using such an approach the
numbers of variables or constraints would increase.

Our research on the DC bilevel program is motivated by a number of im-
portant applications in model selection and hyperparameter learning. Recently
in the statistical learning, the regularization parameters has been successfully
used, e.g., in the least absolute shrinkage and selection operator (lasso) method
for regression and support vector machines (SVMs) for classiﬁcation. However
the regularization parameters have to be set a priori and the choice of these
parameters dramatically aﬀects results on the model selection. The most com-
monly used method for selecting these parameters is the so-called T -fold cross
validation. By T -fold cross validation, a data set Ω is randomly partitioned
into T pairwise disjoint subsets called the validation sets Ωt
val, t = 1, . . . , T .
For each fold t = 1, . . . , T , a subset of Ω denoted by Ωt
val is used
for training and the validation set Ωt
val is used for testing the result. Take
ℓ
the SVM problem for example, suppose the data set Ω =
j=1 where
aj
1 indicate the class membership. For each
hyperparameters λ > 0, ¯w and each fold t = 1, . . . , T , the following SVM

Rn, and the labels bj =

(aj, bj)
}

trn := Ω

Ωt

±

∈

\

{

4

Jane J. Ye et al.

problem can be solved.

(P t

λ, ¯w)

min
− ¯w ≤ w ≤ ¯w
c ∈ R

λ
2 k




k

w

2 +

max(1

bj(aT

j w

−

−

c), 0)

.




Xj∈Ωt
The desirable hyperparameters λ∗ and ¯w∗ can be selected by minimizing some

measure of validation accuracy over all folds such as



trn

T

Θ(w1, . . . , wT , c) :=

1
T

1
Ωt
val| Xj∈Ωt
denotes the number of elements in set M and (wt

bj(aT

max(1

j wt

t=1
X

−

val

|

|

|

M

λ, ¯w) denotes
where
a solution to the SVM problem (P t
λ, ¯w). Here the cross validation error is based
on the hinge loss function. Other possible functions that can be used for cross
validation error can be found in [6,21,22]. In fact, the hyperparameter selection
for SVM has been proposed as the following bilevel program with T lower level
programs by [21,22]:

λ, ¯w, ct

λ, ¯w

ct
λ, ¯w), 0),

−

min
λ, ¯w,w1,...,wT ,c

Θ(w1, . . . , wT , c)

s.t. λlb

λ

¯wlb
and for t = 1, . . . , T :

λub,

≤

≤

¯w

≤

¯wub,

≤

(wt, ct)

∈

argmin
− ¯w ≤ w ≤ ¯w
c ∈ R

λ
2 k




k

w

2 +

max(1

bj(aT

j w

c), 0)

,

−

−




∈

Xj∈Ωt
RT is the vector with ct as the tth component. Here λlb, λub are
where c
given positive numbers and ¯wlb, ¯wub are given vectors in Rn. It is easy to see
that by changing the variable λ to µ := 1
λ we can reformulate the above SV
bilevel model selection equivalently as the following bilevel program with a
single lower level program





trn

(SVBP)

Θ(w1, . . . , wT , c)

min
µ, ¯w,w1,...,wT ,c
1
1
λub ≤
λlb
(w1, . . . , wT , c)

s.t.

≤

µ

,

¯w

¯wub,

≤

¯wlb

≤
S(µ, ¯w),

∈

where S(µ, ¯w) is the set of optimal solutions of the lower level problem

(Pµ, ¯w)

min
− ¯w ≤ wt ≤ ¯w
ct ∈ R
t = 1, . . . , T




t=1
X

T

2

wt
k
2µ

k



+

max(1

−

bj(aT

j wt

−

ct), 0)

.






Xj∈Ωt

trn


2/µ with
Moreover using the fact that a function in the form φ(x, µ) =
µ > 0 is convex as a perspective function [42, Example 3.18], the above bilevel

x
k







k

Diﬀerence of convex algorithms for bilevel programs

5

program has a fully convex lower level program and all required assumptions
hold; see the details in Section 4. The classical T -fold cross validation method
for selecting hyperparameters usually implements a grid search: training T
models at each point of a discretized parameter space in order to ﬁnd an
approximate optimal parameter. This method has many drawbacks and limi-
tations. In particular its computational complexity scales exponentially with
the number of hyperparameters and the number of grid points for each hyper-
parameter. Hence the grid search method is not practical for problem requiring
several hyperparameters, including our SV bilevel model selection where the
hyperparameters (µ, ¯w) are n + 1 dimentional. To deal with limitations of grid
search, introduced ﬁrst in [6] in 2006, the bilevel program has been used to
model hyperparameter selection problems in [6,21,22,23,30,31].

The above fully convex transformation using the perspective function can
be applied to some other model hyperparameter selection problems, for exam-
ple, the T -fold cross validation Lasso problem.

This paper is motivated by an interesting fact that, under our problem

setting, the value function of the lower level in (DCBP) deﬁned by

v(x) := inf

f (x, y) s.t. g(x, y)

y∈Y {

0

}

≤

is convex and locally Lipschitz continuous on X. We take full advantage of
this convexity and use the value function approach ﬁrst proposed in [35] for a
numerical purpose and further used to study optimality conditions in [48] to
reformulate (DCBP) as the following DC program:

(VP)

min
(x,y)∈C

F1(x, y)

F2(x, y)

−

s.t.

f (x, y)

v(x)

0.

≤

−

Unfortunately, due to the value function constraint, (VP) violates the usual
constraint qualiﬁcation such as the nonsmooth Mangasarian Fromovitz con-
straint qualiﬁcation (MFCQ) at each feasible point, see [48, Proposition 3.2]
for the smooth case and Proposition 7 for the nonsmooth case. It is well-
known that convergence of the diﬀerence of convex algorithm (DCA) is only
guaranteed under constraint qualiﬁcations such as the extended MFCQ, which
is MFCQ extended to infeasible points; see, e.g., [43]. To deal with this issue,
we consider the following approximate bilevel program

(VP)ǫ

min
(x,y)∈C

F1(x, y)

F2(x, y)

−

s.t.

f (x, y)

v(x)

ǫ,

≤

−

for some ǫ > 0. Such a relaxation strategy has been used for example in
[25] based with the reasoning that in numerical algorithms one usually obtain
an inexact optimal solution anyway and the solutions of (VP)ǫ approximate a
solution of the original bilevel program (VP) as ǫ approaches zero. In this paper
we will show that EMFCQ holds for problem (VP)ǫ when ǫ > 0 automatically.
0. When ǫ > 0, the
Hence we propose to solve problem (VP)ǫ with ǫ

≥

6

Jane J. Ye et al.

convergence of our algorithm to stationary points is guaranteed and when
ǫ = 0, the convergence is not guaranteed but it could still converge if the
penalty parameter sequence is bounded.

Using DCA approach, at each iterate point (xk, yk), one linearises the
concave part of the function, i.e., the functions F2(x, y), v(x) by using an
element of the subdiﬀerentials ∂F2(xk, yk), ∂v(xk) and solve a resulting convex
subproblem. The value function is an implicit function. How do we obtain
an element of the subdiﬀerential ∂v(xk)? At current iterate xk, assuming we
can solve the lower level problem (Pxk ) with a global minimizer ˜yk and a
corresponding Karush-Kuhn-Tucker (KKT) multiplier denoted by γk. Suppose
that the following partial derivative formula holds:

∂f (x, y) = ∂xf (x, y)

×

∂yf (x, y), ∂gi(x, y) = ∂xgi(x, y)

∂ygi(x, y)

(1)

×

at (x, y) = (xk, ˜yk). Then since by convex analysis

l

∂xf (xk, ˜yk) +

γk
i ∂xgi(xk, ˜yk)

i=1
X

∂v(xk),

⊆

we can select an element of ∂v(xk) from the set

l

∂xf (xk, ˜yk) +

i ∂xgi(xk, ˜yk)
γk

i=1
X
and use it to linearize the value function. We then solve the resulting convex
subproblem approximately to obtain a new iterate (xk+1, yk+1). Thanks to
recent developments in large-scale convex programming, using this approach
we can deal with a large scale DC bilevel program.
Now we summarize our contributions as follows.

– We propose two new algorithms for solving DC program. These algorithms
have modiﬁed the classical DCA in two ways. First, we add a proximal term
in each convex subproblem so the the objective function is strongly convex
and at each iterate point, only an approximate solution for the convex
subproblem is solved. Second, our penalty parameter update is simplier.
– We have laid down all theoretical foundations from convex analysis that are
required for our algorithms to work. In particular we have demonstrated
that under the minimal assumptions that we specify for problem (DCBP),
the value function is convex and locally Lipschitz on set X automatically.
– Using the two new algorithms for solving DC program, we propose two
corresponding algorithms to solve problem (DCBP). Our algorithms hold
under very mild and natural assumptions. In particular we allow all deﬁning
functions to be nonsmooth and we do not require any constraint qualiﬁca-
tion to hold for the lower level program. The main assumptions we need are
only the partial derivative formula (1) which holds under many practical
situations (see Proposition 1 for suﬃcient conditions) and the existence of
a KKT multiplier for the lower level program under each iteration.

Diﬀerence of convex algorithms for bilevel programs

7

– Taking advantage of large scale convex programming, our algorithm can
handle high dimensional hyperparameter selection problems. To test eﬀec-
tiveness of our algorithm, we have tested it in the SV bilevel model selection
(SVBP). Our results compare favourably with the MPEC approach [21,22,
23].

This paper is organized as follows. In Section 2 we propose two modiﬁed DCAs
and study their convergence to stationary points for a class of general DC pro-
grams. In Section 3, we derive explicit conditions for the bilevel program under
which the algorithms introduced in Section 3 can be applied. Numerical ex-
periments on the SV bilevel model selection is conducted on Section 4. Section
5 concludes the paper.

2 Modiﬁed inexact proximal DC algorithms

In order to solve the (relaxed) value function reformulation of problem DCBP,
in this section we propose numerical algorithms to solve the following diﬀerence
of convex program:

(DC)

f0(z) := g0(z)

min
z∈Σ
s.t. f1(z) := g1(z)

h0(z)

h1(z)

−

−

0,

≤

where Σ is a closed convex subset of Rd and g0(z), h0(z), g1(z), h1(z) : Σ
R
are convex functions. Although the results in this section can be generalized
to the case where there are more than one inequality in a straight-forwarded
manner, to simplify the notation and concentrate on the main idea we assume
there is only one inequality constraint in problem (DC). Our algorithms are
modiﬁcations of the classical DCA (see [43]). Recently, [37] studied problem
(DC) where h1 is a maximum of ﬁnitely many smooth convex functions and
proposed an algorithm for ﬁnding B-stationary points of it.

→

Before introducing our algorithms and conduct the convergence analysis,
we recall some notations from convex analysis and variational analysis. Let
ϕ(x) : Rn
] be a convex function, and let ¯x be a point where ϕ be
ﬁnite. The subdiﬀerential of ϕ at ¯x is a closed convex set deﬁned by

−∞

, +

→

∞

[

∂ϕ(¯x) :=

ξ

Rn

ϕ(x)

ϕ(¯x) +

ξ, x
h

¯x

,
i

,

x
}

[

|

{

∀

∈

−

→

∞

, +

−∞

≥
and a subgradient is an element of the subdiﬀerential. For a function ϕ : Rn
×
Rm
], we denote the partial subdiﬀerential of ϕ with respect to x
and y by ∂xϕ(x, y) and ∂yϕ(x, y) respectively. Let Σ be a convex subset in Rn
and ¯x
Σ(¯x). Let δΣ(x) denote
the indicator function of set Σ at x. The following partial subdiﬀerentiation
rule will be useful.
Proposition 1 (Partial subdiﬀerentiation) Let ϕ : Rn
[
be a convex function and let (¯x, ¯y) be a point where ϕ is ﬁnite. Then

Σ. The normal cone to Σ at ¯x is denoted by

]
∞

−∞

Rm

, +

→

N

×

∈

∂ϕ(¯x, ¯y)

⊆

∂xϕ(¯x, ¯y)

×

∂yϕ(¯x, ¯y).

(2)

8

Jane J. Ye et al.

The inclusion (2) becomes an equality under one of the following conditions.

∂xϕ(¯x, ¯y), it holds that ϕ(x, y)

ϕ(¯x, y)

ξ, x

≥ h

,
¯x
i

−

(x, y)

∀

∈

−

∈

(a) For every ξ
Rm.

Rn

×

(b) ϕ(x, y) = ϕ1(x) + ϕ2(y).
(c) For any ε > 0, there is δ > 0 such that

either ∂xϕ(¯x, ¯y)
or ∂yϕ(¯x, ¯y)

⊆
⊆

∂xϕ(¯x, y) + εBRn
∂yϕ(x, ¯y) + εBRm

y
∀
x
∀

∈
∈

B(¯y; δ)
B(¯x; δ),

(3)
(4)

where B(¯x; δ) denotes the open ball centered at ¯x with radius equal to δ and
BRn denotes the open unit ball centered at the origin in Rn.

(d) ϕ(x, y) is continuously diﬀerentiable respect to one of the variables x or y

at (¯x, ¯y).

Moreover (b) =

⇒

(a), (d) =

⇒

(c) =

⇒

(a).

Proof The inclusion (2) and its reverse under (a) follow directly from def-
initions of the convex subdiﬀerential and the partial subdiﬀerential. When
ϕ(x, y) = ϕ1(x) + ϕ2(y), we have that ∂ϕ(x, y) = ∂ϕ(x)
∂ϕ(y).
Hence obviously (b) implies (a). The implication of (d) to (c) is obvious. Now
∂xϕ(¯x, ¯y). Then according to (3), for any ε > 0,
suppose that (3) holds. Let ξ
there is δ > 0 such that ξ = η + εe, where e

BRn , and

× {

} ×

+

∈

0

0

{

}

∈
x

ξ, x
h

−

¯x

i ≤

ϕ(x, y)

−

ϕ(¯x, y) + ε

k

¯x

y
k ∀

−

∈

B(¯y; δ).

Thanks to the convexity of ϕ, using the proof technique of [10, Corollary 2.6
(c)], we can easily show that (a) holds. The proof for the case where (4) holds
is similar and thus omitted.

⊓⊔
Next, we ﬁrst brief some solution quality characterizations for problem

(DC).

Deﬁnition 1 Let ¯z be a feasible solution of problem (DC). We say that ¯z is a
0 such
stationary/KKT point of problem (DC) if there exists a multiplier λ
that

≥

∂g0(¯z)

∂h0(¯z) + λ(∂g1(¯z)

−

h1(¯z))λ = 0.

0
∈
(g1(¯z)

−

∂h1(¯z)) +

Σ(¯z),

N

−

Deﬁnition 2 Let ¯z be a feasible point of problem (DC). We say that the
nonzero abnormal multiplier constraint qualiﬁcation (NNAMCQ) holds at ¯z
for problem (DC) if either f1(¯z) < 0 or f1(¯z) = 0 but

∂g1(¯z)

0

6∈

−

∂h1(¯z) +

Σ(¯z).

N

(5)

∈

Let ¯z
Σ, we say that the extended no nonzero abnormal multiplier constraint
qualiﬁcation (ENNAMCQ) holds at ¯z for problem (DC) if either f1(¯z) < 0 or
f1(¯z)

0 but (5) holds.

≥

Diﬀerence of convex algorithms for bilevel programs

9

Note that NNAMCQ (ENNAMCQ) is equivalent to MFCQ (EMFCQ) respec-
tively; see e.g., [20].

Denote by ∂cϕ(x) the Clarke generalized gradient [9] of a locally Lipschitz
function ϕ at x. The following optimality condition follows from the nonsmooth
multiplier rule in terms of Clarke generalized gradients (see e.g. [9,20]) and
the fact that for two convex functions g, h which are Lipschitz around point
¯z, we have ∂c(g(¯z)

∂ch(¯z) = ∂g(¯z)

∂cg(¯z)

∂h(¯z).

h(¯z))

−

⊆

−

−

Proposition 2 Let ¯z be a local solution of problem (DC). If NNAMCQ holds
at ¯z and all functions g0, g1, h0, h1 are Lipschitz around point ¯z, then ¯z is a
KKT point of problem (DC).

2.1 Inexact proximal DCA with simpliﬁed penalty parameter update

In this subsection we propose an algorithm called inexact proximal DCA to
solve problem (DC) and show its convergence to stationary points.

By using the main idea of DCA which linearizes the concave part of the
DC structure, we propose a sequential convex programming scheme as follows.
Given a current iterate zk
Σ with k = 0, 1, . . ., we select a subdiﬀerential
∂hi(zk), for i = 0, 1. Then we solve the following subproblem approxi-
ξk
i ∈
mately and select zk+1 as an approximate minimizer:

∈

min
z∈Σ

˜ϕk(z) := g0(z)

h0(zk)

−

− h

zk

i

ξk
0 , z

−
h1(zk)

+ βk max
{

g1(z)

−

ξk
1 , z

− h

zk

+

, 0
i

}

−

(6)

ρ
2 k

z

zk

2,

k

−

where ρ is a given positive constant and βk represents the adaptive penalty
parameter. Our scheme is similar to that of DCA2 in [43] but diﬀerent in that
the subproblem (6) has a strongly convex objective function, the subprob-
lem is only solved approximately, and a simplier penalty parameter update is
used. We propose the following two inexact conditions for choosing zk+1 as an
approximate solution to (6):

dist(0, ∂ ˜ϕk(zk+1)+

Σ(zk+1))

N

≤

ζk,

for some ζk

0 satisfying

≥

and

dist(0, ∂ ˜ϕk(zk+1) +

Σ(zk+1))

ρ

zk

zk−1

,

k
where dist(x, M ) denotes the distance from a point x to set M .

N

−

≤

k

√2
2

ζ2
k <

∞

k=0
X

,
∞
(7)

(8)

Using above constructions, we are ready to propose the inexact proximal

DCA (iP-DCA) in Algorithm 1.

In DCA2 of [43], the subproblem (6) was solved as a constrained optimiza-
tion problem and a Lagrange multiplier is used to update the penalty parame-
ter. Since our penalty parameter update rule does not involve any multipliers,
it is easier to implement. In the rest of this section we show that the proposed

10

Jane J. Ye et al.

Algorithm 1 iP-DCA
1: Take an initial point z0 ∈ Σ; δβ > 0; an initial penalty parameter β0 > 0, tol > 0.
2: for k = 0, 1, . . . do
1. Compute ξk
2. Obtain an inexact solution zk+1 of (6) satisfying (7) or (8).
3. Stopping test. Compute tk+1 := max{g1(zk+1) − h1(zk) − hξk

i ∈ ∂hi(zk), i = 0, 1.

1 , zk+1 − zki, 0}. Stop

if max{kzk+1 − zkk, tk+1} < tol.

4. Penalty parameter update. Set

βk+1 =

βk + δβ ,
βk,

(

if max{βk, 1/tk+1} < kzk+1 − zkk
otherwise.

−1,

5. Set k := k + 1.

3: end for

algorithm converges. Let us start with the following lemma which provides a
suﬃcient decrease of the merit function of (DC) deﬁned by

ϕk(z) := g0(z)

h0(z) + βk max

g1(z)

{

−

h1(z), 0

.

}

−

Lemma 1 Let
be a sequence of iterates generated by iP-DCA as deﬁned
in Algorithm 1. If the inexact criterion (7) or (8) is applied, then zk satisﬁes

{

}

zk

ϕk(zk)

or

ϕk(zk)

ϕk(zk+1) +

ϕk(zk+1) +

≥

≥

ρ
2 k
ρ
2 k

zk+1

zk+1

zk

zk

2

k

−

2

k

−

−

−

1
2ρ
ρ
4 k

ζ2
k,

zk

zk−1

2,

k

−

where ζk

≥

0 satisfying

∞
k=0 ζ2

k <

∞

respectively.

Proof Since zk+1 is an approximation solution to problem (6) with inexact
∂ ˜ϕk(zk+1) +
criterion (7) or (8), there exists a vector ek such that ek

P

∈

Σ(zk+1)

∂( ˜ϕk + δΣ)(zk+1) and

N

⊆

ek

k

k ≤

ζk or

ek

k

k ≤

√2
2

zk

ρ

k

−

zk−1

,

k

(9)

respectively. As ˜ϕk is strongly convex with modulus ρ, Σ is a closed convex
set and zk

Σ, we have

∈

˜ϕk(zk)

≥

≥

˜ϕk(zk+1) +

˜ϕk(zk+1)

= ˜ϕk(zk+1)

ρ
2 k

+

i
zk+1

zk

ρ
2 k

zk+1

zk

2

−
2 +

k
ρ
2 k

zk

k

−

zk+1

zk

2

k

−

(10)

−

−

2

ek

ek, zk+1
h
1
2ρ k
1
2ρ k

ek

k

k

2.

−

−

Next, by the convexity of hi(z) and ξk

∂hi(zk), i = 0, 1, there holds that

hi(zk+1)

≥

hi(zk) +

zk

,
i

−

i = 0, 1,

i ∈
i , zk+1
ξk
h

Diﬀerence of convex algorithms for bilevel programs

11

and thus ˜ϕk(zk+1)

ϕk(zk) = ˜ϕk(zk)

≥

ϕk(zk+1) + ρ
2 k

≥
˜ϕk(zk+1)

1
2ρ k

ek

2

k

−

≥

−
k
ϕk(zk+1)

1
2ρ k

ek

k

−

2+

ρ
2 k

zk+1

zk

2.

k

−

zk+1

zk

2. Combined with (10), we have

The conclusion follows immediately from (9).

⊓⊔
The following theorem is the main result of this section. It proves that
any accumulation point of iP-DCA is a KKT point as long as the penalty
parameter sequence

is bounded.

βk

{

}

and
generated by iP-DCA are bounded. Moreover suppose functions g0, g1,
zk

Theorem 1 Suppose f0 is bounded below on Σ and the sequences
βk
{
h1, h0 are locally Lipschitz on set Σ. Then every accumulation point of
is a KKT point of problem (DC).

}

}

{

{

}

zk

Proof Since
βk = βk0,
below, ϕk(z) is bounded below for all k
(9) obtained in Lemma 1, we have

is bounded, there exists some iteration index k0 such that
k0. As f0 is bounded
k0, and thus ϕk(z) = ϕk0 (z) for all k
k0. Then, by the inequality (9) and

βk
{
k
∀

}
≥

≥

≥

∞

k=1
X

zk+1

k

zk

k

−

2 < +

,
∞

lim
k→∞ k

zk+1

zk

k

−

= 0,

zk+1

−1 always holds when k is large enough. According
and thus βk <
to the update strategy of βk in iP-DCA, there exists some iteration index k1
such that

zk

−

k

k

−

−

→

zk

−h

zk+1

h1(zk)

1 , zk+1
ξk

g1(zk+1)

tk+1 := max
{

, 0
i
0. Since zk+1 is an approximate solution to problem (6) and

and thus tk
inexact criterion (7) or (8) holds, there exists a vector ek such that ek
∈
∂ ˜ϕk(zk+1) +
Σ(zk+1) and (9) holds. According to the sum rule (see, e.g.,
[38, Theorem 23.8][9, Corollary 1 to Theorem 2.9.8]) and the subdiﬀerential
calculus rules for the pointwise maximum (see, e.g., [9, Proposition 2.3.12]),
there exist ˜λk+1

∂gi(zk+1)(i = 0, 1) such that

[0, 1] and ηk+1

} ≤ k

k1,

zk

N

−

≥

∀

k

k

i

∈

ηk+1
ek
0 −
∈
g1(zk+1)
−
˜λk+1(g1(zk+1)
tk+1(1

∈
0 + βk ˜λk+1(ηk+1
1 ) + ρ(zk+1
ξk
ξk
1 −
tk+1,
1 , zk+1
ξk
h1(zk)
zk
−
− h
zk
1 , zk+1
ξk
h1(zk)
0.

− h
tk+1

i ≤
−

−
˜λk+1) = 0,

i −

zk) +

−

tk+1) = 0,

N

Σ(zk+1), (11)
(12)
(13)

(14)

−
βk ˜λk+1
zk

{

{

}

{

Since
βk ˜λk+1
points of
without loss of generality we may assume that zk
Now passing onto the limit as k
in (7) or
locally Lipschitz continuous at ˜z, ∂gi(z), ∂hi(z), i = 0, 1 and
semicontinuous, we obtain that ˜z is a KKT solution of problem (DC).

is bounded, we may suppose that ˜z and ˜λ are accumulation
}
respectively. Taking subsequences if necessary,
and
˜λ.
˜z
0
→
0, since gi(z), hi(z), i = 0, 1 are
Σ(z) are outer

Σ and βk ˜λk+1
0 from ζk

in (11)-(13), as ek

0 in (8) and tk

→
→

→ ∞

zk+1

k →

zk

→

→

N

−

∈

}

k

≥

⊓⊔

12

Jane J. Ye et al.

Notice that the boundedness of the penalty parameters is needed for an
accumulation point to be a KKT point. The following proposition provides a
suﬃcient condition for the boundedness of the penalty parameters sequence
βk

.

{

}

Proposition 3 Suppose that the iterate sequence
generated by iP-DCA
is bounded. Moreover suppose functions g0, g1, h1, h0 are Lipschitz around at
zk
. Assume that ENNAMCQ holds at any accu-
any accumulation point of
}
zk
must be bounded.
mulation points of the sequence
{

. Then the sequence

βk

}

{

}

{

{

}

zk

Proof The proof is inspired by [43, Theorem 3.1]. To the contrary, suppose
. Then there exist inﬁnitely many indices j such that
that βk

as k

+

→

∞

→ ∞
zkj+1

βkj <

k

and thus

zkj

k

−

−1 and

tkj +1 >

zkj+1

k

zkj

,

k

−

lim
j→∞ k

zkj+1

zkj

k

−

= 0,

tkj +1 > 0,

j.

∀

+

∞

→

as j

From (14), since tkj +1 > 0 for all j, we have ˜λkj +1 = 1 for all j and thus
˜λkj +1
λkj +1 := βkj
. Taking a further subsequence, if neces-
→ ∞
sary, we can assume that zkj
˜z
h1(˜z) < 0, then as
Σ as j
−
∈
→ ∞
zkj
zkj+1
= 0,
is bounded, and limj→∞ k
g1, h1 are continuous at ˜z,
k
−
ξkj
when j is suﬃciently large, one has g1(zkj +1)
zkj
1 , zkj+1
< 0,
−
−h
−
i
ξkj
zkj
1 , zkj +1
g1(zkj +1)
which contradicts to tkj +1 := max
>
, 0
i
−
−h
−
{
0. From (11), we have
0 for all j. Thus, g1(˜z)

h1(zkj )
h1(zkj )

. If g1(˜z)

h1(˜z)

→
ξkj

}

{

}

−

≥

ekj ∈

∂g0(zkj +1)
+ ρ(zkj +1

−

−

∂h0(zkj ) + λkj +1∂g1(zkj +1)
zkj ) + NΣ(zkj +1),

−

λkj +1∂h1(zkj )

where λk+1 := βk ˜λk+1. Dividing both sides of this equality by λkj +1, and
passing onto the limit as j
Σ(˜z), which
, we have 0
contradicts ENNAMCQ.

∂h1(˜z) +

∂g1(˜z)

→ ∞

N

−

∈

⊓⊔

2.2 Inexact proximal linearized DCA with simpliﬁed penalty parameter
update

Recall that iP-DCA deﬁned in Algorithm 1 requires minimization of a strongly
convex subproblem (6). In this subsection, we assume that g1 is L-smooth
g1(z) is Lipschitz continuous with constant L on Σ. This
which means that
setting motivates a very simple linearization approach inspired by the idea
behind the proximal gradient method (see [4] and the references therein).
Speciﬁcally, we linearize both the concave part and the convex smooth part of
the DC structure. Such a linearization approach makes subproblems easier to
solve compared to iP-DCA. Given a current iterate zk
Σ with k = 0, 1, . . .,

∇

∈

Diﬀerence of convex algorithms for bilevel programs

13

we select a subgradient ξk
i ∈
subproblem approximately.

∂hi(zk), for i = 0, 1. Then we solve the following

min
z∈Σ

ˆϕk(z) := g0(z)

h0(zk)

−

− h

ξk
0 , z

zk

ρk
2 k

+

i

z

zk

2

k
−
h1(zk)

−
g1(zk), z

g1(zk) +

{

+ βk max

, 0
,
}
i
(15)
where ρk and βk are the adaptive proximal and penalty parameters respec-
tively. Choose zk+1 as an approximate minimizer of the convex subproblem
(15) satisfying one of the following two inexact criteria

i −

− h

h∇

−

−

ξk
1 , z

zk

zk

dist(0, ∂ ˆϕk(zk+1) +

Σ(zk+1))

N

ζk,

≤

for some ζk satisfying

∞

k=0
X

ζ2
k <

,
∞
(16)

and

dist(0, ∂ ˆϕk(zk+1) +

Σ(zk+1))

N

√2
2

≤

zk

σ

k

−

zk−1

.

k

(17)

This yields the inexact proximal linearized DCA (iPL-DCA), whose exact
description is given in Algorithm 2.

Algorithm 2 iPL-DCA
1: Take an initial point z0 ∈ Σ; δβ > 0, σ > 0, an initial penalty parameter β0 > 0, an

initial regularizer parameter ρ0 = 1

2 β0L + σ, tol > 0.

2: for k = 0, 1, . . . do
1. Compute ξk
2. Obtain an inexact solution zk+1 of (15) satisfying (16) or (17).
3. Stopping test. Compute tk+1 := max{g1(zk) + h∇g1(zk), zk+1 − zki − h1(zk) −

i ∈ ∂hi(zk), i = 0, 1.

hξk

1 , zk+1 − zki, 0}. Stop if max{kzk+1 − zkk, tk+1} < tol.

4. Penalty parameter update. Set

βk + δβ ,
βk,

if max{βk, 1/tk+1} < kzk+1 − zkk
otherwies.

−1,

βk+1 =

ρk+1 =

5. Set k := k + 1.

3: end for

(
1
2

βk+1L + σ.

Recall that the merit function of (DC) is deﬁned by ϕk(z) := g0(z)

−
. Similar to Lemma 1, we ﬁrst give following

h0(z) + βk max
g1(z)
{
suﬃciently decrease result of iPL-DCA.

h1(z), 0

−

}

zk

Lemma 2 Let
be the sequence of iterates generated by iPL-DCA as de-
ﬁned in Algorithm 2. If the inexact criterion (16) or (17) is applied, then zk
satisﬁes

{

}

ϕk(zk)

ϕk(zk)

≥

≥

ϕk(zk+1) +

ϕk(zk+1) +

σ
2 k
σ
2 k

zk+1

zk+1

zk

zk

2

2

k

k

−

−

−

−

1
2σ
σ
4 k

ζ2
k ,

zk

zk−1

2,

k

−

14

respectively.

Jane J. Ye et al.

Proof Since zk+1 is an approximation solution to problem (15) with inexact
∂ ˆϕk(zk+1) +
criterion (16) or (17), there exists a vector ek such that ek

∈

Σ(zk+1))

∂( ˜ϕk + δΣ)(zk+1) and

N

⊆

ek

k

k ≤

ζk or

ek

k

k ≤

√2
2

zk

σ

k

−

zk−1

,

k

(18)

respectively. As ˆϕk is strongly convex with modulus ρk and Σ is a closed
convex set, we have

ˆϕk(zk)

≥

ˆϕk(zk+1) +

ek, zk+1
h

−

zk

ˆϕk(zk+1)

≥
= ˆϕk(zk+1)

1
2σ k
1
2σ k

ek

ek

k

k

2

−
2 +

−

−

σ
2 k
ρk

−
2 +

zk+1

ρk
2 k

+

i
zk+1

σ

k

−
2

k

zk

−
zk+1

zk

−

k

zk

2

k
ρk
2 k
2.

zk+1

zk

2

k

−

(19)

Next, by the convexity of hi(z) and ξk

∂hi(zk), i = 0, 1, we have

hi(zk+1)

≥

hi(zk) +

And since g1 is L-smooth, we have

i ∈
ξk
i , zk+1
h

zk

,
i

−

i = 0, 1.

g1(zk+1)

g1(zk) +

g1(zk), z

h∇

≤

zk

i

−

+

L
2 k

zk+1

zk

2.

k

−

Thus, we have

ˆϕk(zk+1)

≥

ϕk(zk+1) +

ρk

βkL
−
2

k

zk+1

zk

2.

k

−

Combined with (19), we have

ϕk(zk) = ˆϕk(zk)

ˆϕk(zk+1)

≥

ϕk(zk+1)

≥
= ϕk(zk+1)

1
2σ k
1
2σ k
1
2σ k

ek

ek

ek

k

k

k

2 +

2 +

2 +

−

−

−

σ

ρk

−
2
2ρk

−

k
βkL
2
zk+1

σ
2 k

zk+1

−
σ

zk

2

k
zk+1

−

k
2.

zk

−

k

zk

2

k

−

Then the conclusion follows immediately from (18).

⊓⊔
Similar to Theorem 1 and Proposition 3, by Lemma 2, the following conver-
gence results of iPL-DCA can be derived easily. The proofs are purely technical
and thus omitted.

Theorem 2 Suppose f0 is bounded below and the sequences
}
generated by iPL-DCA are bounded, functions g0, h1, h0 are locally Lipschitz
on set Σ. Then every accumulation point of
is a KKT point for problem
(DC).

and

zk

βk

}

{

{

}

{

zk

Diﬀerence of convex algorithms for bilevel programs

15

Proposition 4 Suppose the sequence
functions g0, h1, h0 are Lipschitz around at any accumulation point of
and ENNAMCQ holds at any accumulation points of the sequence
the sequence

generated by iPL-DCA is bounded,
zk
,
}
. Then

is bounded.

zk

{

}

{

}

{

βk

zk

{

}

Remark 1 In fact, if g0 is further assumed to be diﬀerentiable and
g0 is
Lipschitz continuous, we can also linearize g0 in iPL-DCA. The proof of con-
vergence is similar.

∇

3 DC algorithms for solving DCBP

In this section we will show how to solve problem (DCBP) numerically. It is
obvious that problem (VP)ǫ is problem (DC) with

z := (x, y), f0(x, y) := F1(x, y)

F2(x, y), f1(x, y) := f (x, y)

v(x)

−

−

ǫ, Σ = C.

−

According to [38, Theorem 10.4], since F1(x, y), F2(x, y), f (x, y) are Lipschitz
continuous near every point on an open convex set containing C and hence Lip-
schitz continuous near every point on C. However our problem (VP)ǫ involves
]
the value function which is an extended-value function v(x) : X
∞
deﬁned by

,
−∞

→

[

v(x) := inf

y∈Y {

f (x, y) s.t. g(x, y)

,

0

}

≤

(x) is empty. To
with the convention of v(x) = +
apply the proposed DC algorithms, we need to answer the following questions.

if the feasible region

∞

F

(a) Is the value function convex and locally Lipschitz on the convex set X and

how to obtain one element from ∂v(xk) in terms of problem data?

(b) Will the constraint qualiﬁcation ENNAMCQ hold at any accumulation

point of the iterate sequence?

We now give answers to these questions in the next two subsections.

3.1 Lipschitz continuity and the subdiﬀerential of the value function

Thanks to the full convex structure of the lower level problem in (DCBP),
the value function turns out to be convex and Lipschitz continuous under our
problem setting as shown below.

Lemma 3 The value function v(x) : X
uous around any point in set X. Given ¯x

R is convex and Lipschitz contin-
X and ¯y

S(¯x), we have

∂v(¯x) =

ξ

where φ(x, y) := f (x, y) + δD(x, y), D :=

∈
(x, y)
being the open set deﬁned in the introduction.

∈

{

{

O

→
∈
Rn : (ξ, 0)

∈
∂φ(¯x, ¯y)
}
Y

,

∈ O ×

(20)

g(x, y)

, with

0

}

≤

|

16

Jane J. Ye et al.

Proof First we extend the deﬁnition of the value function from any element
x

X to the whole space Rn as follows:

∈

v(x) := inf

y∈Rm

φ(x, y),

Rn.

x

∀

∈

[

×

∞

→

Rm

for x

,
−∞

It follows that v(x) = +
. In our problem setting, f is fully
6∈ O
convex on an open convex set containing the convex set C and hence we can
assume without loss of generality that f is fully convex on the convex set
D. Therefore the extended-valued function φ(x, y) : Rn
]
∞
is convex. The convexity of the value function v(x) = inf y∈Rm φ(x, y) then
follows from [39, Theorem 1]. Hence the value function restricted on set X
is convex. Next, according to [39, Theorem 24], we have the equation (20).
By assumption stated in the introduction of the paper, the feasible region of
the lower level program
for all x in the open set
x :
Since domv :=
⊆
int(domv). The result on Lipschitz continuity of the value function follows
from [38, Theorem 10.4].

(x) :=
y
∈
. Hence v(x) : Rn
(x)

∅
−∞
] is proper convex.
X, we have X

⊓⊔
By using some sensitivity analysis techniques, a subgradient of the value
function v(x) can be expressed in terms of Lagrangian multipliers. In partic-
ular, given ¯y
S(¯x), we denote the set of KKT multipliers of the lower-level
problem (P¯x) by

} 6
≤
,
[
−∞

x : v(x) < +

Y : g(x, y)

∅} ⊇ O ⊇

and v(x)

F
O

→
=

∞}

∞

=

=

=

F

∈

{

0

{

{

KT (¯x, ¯y)

Y (¯y),

N

l

i=1
X

γigi(¯x, ¯y) = 0

.

)

l

∂yf (¯x, ¯y) +

γi∂ygi(¯x, ¯y) +

i=1
X

X and ¯y

∈

∈

l

S(¯x). Then

:=

γ

(

∈

Rl
+

0

∈

Theorem 3 Let ¯x

(cid:12)
(cid:12)
(cid:12)

∂v(¯x)

⊇

(ξ, 0)

∈

ξ
(

(cid:12)
(cid:12)
(cid:12)

∂f (¯x, ¯y) +

γi∂gi(¯x, ¯y) +

i=1
X

0

{

Y (¯y),

} × N

Rl, γ

γ

∈

0,

≥

and the equality holds in (21) provided that

l

E(¯x, ¯y) =

N

γi∂gi(¯x, ¯y) +

0

{

Y (¯y)

} × N

γ

|

≥

0,

i=1
n
X
(x, y)

{

where E :=

Moreover if the partial derivative formula holds

Rn

∈

×

Y : g(x, y)

0

.

}

≤

l

i=1
X

l

i=1
X

γigi(¯x, ¯y) = 0

, (21)

)

γigi(¯x, ¯y) = 0

, (22)

o

∂f (¯x, ¯y) = ∂xf (¯x, ¯y)

×

∂yf (¯x, ¯y), ∂gi(¯x, ¯y) = ∂xgi(¯x, ¯y)

∂ygi(¯x, ¯y) (23)

×

6
6
Diﬀerence of convex algorithms for bilevel programs

17

then

∂xf (¯x, ¯y) +

[γ∈KT (¯x,¯y)  

l

i=1
X

γi∂xgi(¯x, ¯y)

! ⊆

∂v(¯x),

(24)

and the equality in (24) holds provided that (22) holds.

Proof Let φE(x, y) := f (x, y) + δE(x, y) = f (x, y) + δY (y) +
0
with Ci :=
23.8][9, Corollary 1 to Theorem 2.9.8] ) and the fact that

l
i=1 δCi(x, y)
. Then by the sum rule (see, e.g., [38, Theorem
}
E = ∂δE, we have

(x, y)
|

gi(x, y)

P

≤

{

N

l

∂f (¯x, ¯y) +

0

{

Y (¯y) +

} × N

Ci(¯x, ¯y)

∂φE(¯x, ¯y).

(25)

N

⊆

i=1
X
intCi and hence γi = 0

Ci(¯x, ¯y).
When gi(¯x, ¯y) < 0, we have (¯x, ¯y)
Otherwise if gi(¯x, ¯y) = 0, by deﬁnition of subdiﬀerential and the normal cone
Ci(¯x, ¯y). Hence together with
we can show that for any γi
(25), we have

0, γi∂gi(¯x, ¯y)

∈ N

⊆ N

≥

∈

l

∂f (¯x, ¯y) +

γi∂gi(¯x, ¯y) +

n

i=1
X

∂φE(¯x, ¯y).

0

{

Y (¯y)

} × N

Rl, γ

γ

|

∈

0,

≥

l

i=1
X

γigi(¯x, ¯y) = 0

o

⊆
Since ∂φE(¯x, ¯y) = ∂φ(¯x, ¯y) where φ(x, y) = f (x, y) + δD(x, y) with D :=
(x, y)

, it follows from Lemma 3 that

g(x, y)

Y

0

{
∂v(¯x) =

∈ O ×
ξ
{

|

|
(ξ, 0)

∈

}
≤
∂φ(¯x, ¯y)
}
l

=

ξ
{

|

(ξ, 0)

∂φE(¯x, ¯y)
}

∈

ξ

|

⊇ (

(ξ, 0)

∈

∂f (¯x, ¯y) +

γi∂gi(¯x, ¯y) +

0

{

Y (¯y),

} × N

i=1
X

Rl, γ

γ

∈

0,

≥

l

i=1
X

γigi(¯x, ¯y) = 0

.
)

Hence (21) holds. Since f is Lipschitz continuous at (¯x, ¯y), by the sum rule (see,
e.g., [38, Theorem 23.8][9, Corollary 1 to Theorem 2.9.8] ), we have ∂φE(¯x, ¯y) =
∂f (¯x, ¯y) +

E(¯x, ¯y). Hence if (22) holds, then

N
ξ
{
|
(ξ, 0)

∂v(¯x) =

=

ξ
{

|

(ξ, 0)

∈
∂f (¯x, ¯y) +

∂φ(¯x, ¯y)
=
ξ
{
}
E(¯x, ¯y)
}
l

N

|

∈

(ξ, 0)

∂φE(¯x, ¯y)
}

∈

=

ξ
(

|

(ξ, 0)

∈

∂f (¯x, ¯y) +

γi∂gi(¯x, ¯y) +

i=1
X

0

{

Y (¯y),

} × N

0,

γ

≥

l

i=1
X

γigi(¯x, ¯y) = 0

.
)

This shows that the equality holds in (21).

18

Jane J. Ye et al.

Now suppose that (23) holds. Then for any γ

l
i=1 γigi(¯x, ¯y) =
0, by the sum rule (see, e.g., [38, Theorem 23.8][9, Corollary 1 to Theorem
2.9.8] ) we have

Rl, γ

P

0,

≥

∈

∂φE(¯x, ¯y)

l

∂f (¯x, ¯y) +

γi∂gi(¯x, ¯y) +

i=1
X

l

⊇

=

0

{

Y (¯y)

} × N

∂xf (¯x, ¯y) +

(

i=1
X

γi∂xgi(¯x, ¯y)

) × (

l

∂yf (¯x, ¯y) +

γi∂ygi(¯x, ¯y) +

i=1
X

Y (¯y)

.

)

N

Combining with (21), we obtain (24). Similarly when (22) holds, the equality
holds in (24).

⊓⊔
By the description in (24), ∂v(¯x) can be calculated as long as (22) is satis-
ﬁed. We claim that (22) is a mild condition. In fact, by convexity, there always
holds the inclusion

E(¯x, ¯y)

N

⊇

l

n

i=1
X

γi∂gi(¯x, ¯y) +

0

{

Y (¯y) : γ

} × N

0,

≥

l

i=1
X

γigi(¯x, ¯y) = 0

.

o

By virtue of [18, Theorem 4.1], the reverse inclusion also follows under standard
constraint qualiﬁcations. Some suﬀcient conditions for (22) are thus summa-
rized in the following proposition.

Proposition 5 Equation (22) holds provided that the set-valued map

Ψ (α) :=

(x, y)

{

∈

Rn

×

Y : g(x, y) + α

0

}

≤

is calm at (0, ¯x, ¯y), i.e., there exist κ, δ > 0 such that

distE(x, y)

κ

max
{

k

≤

g(x, y), 0

}k

(x, y)

∀

∈

Bδ(¯x, ¯y)

E;

∩

in particular if one of the following conditions:

(a) The linear constraint qualiﬁcation holds: g(x, y) is an aﬃne mapping of

(x, y) and Y is convex polyhedral.

(b) The Slater condition holds: there exists a point (x0, y0)

g(x0, y0) < 0.

Rn

×

∈

Y such that

Proof By virtue of [18, Theorem 4.1], the reverse inclusion

E(¯x, ¯y)

N

⊆

l

n

i=1
X

γi∂gi(¯x, ¯y) +

0

{

Y (¯y) : γ

} × N

0,

≥

l

i=1
X

γigi(¯x, ¯y) = 0

o

holds provided that the system y
well-known that (a) or (b) is a suﬃcient condition for calmness.

Y, g(x, y)

≤

∈

0 is calm at (0, ¯x, ¯y). It is

⊓⊔

Diﬀerence of convex algorithms for bilevel programs

19

3.2 Motivations for studying the approximate bilevel program

There are three motivations to consider the approximate program (VP)ǫ. First,
as shown in [25], the solutions of (VP)ǫ approximate a true solution of the
original bilevel program (DCBP) as ǫ approaches zero. Second, the proximity
from a local minimizer of (VP)ǫ to the solution set of (DCBP) can be controlled
by adjusting the value of ǫ. The third motivation is that the approximate
program (VP)ǫ when ǫ > 0 would satisfy the required constraint qualiﬁcation
automatically. We present the second motivation in the following proposition.

∗, the solution set of problem (VP), is compact and
Proposition 6 Suppose
the value function v(x) is continuous. Then for any δ > 0, there exists ¯ǫ > 0
such that for any ǫ
(0, ¯ǫ], there exists (xǫ, yǫ) which is a local minimum of
ǫ-approximation problem (VP)ǫ with dist((xǫ, yǫ),

∗) < δ.

∈

S

S

0 as k

with
Proof To the contrary, assume that there exist δ > 0 and sequence
ǫk
such that there does not exist (x, y) being a local minimum
of ǫk-approximation problem (VP)ǫk satisfying dist((xk, yk),
∗) < δ for all k.
Consider a point (ˆxk, ˆyk), which is a global minimum to the following problem

→ ∞

S

{

}

↓

ǫk

min
(x,y)∈C

F (x, y)

s.t.

f (x, y)

dist((x, y),

−

v(x)
∗)

S

≤

≤

ǫ,

δ.

Then by assumption, it holds that dist((ˆxk, ˆyk), Σ∗) = δ and F (ˆxk, ˆyk)
where F ∗ is the optimal value of problem (VP). As
(ˆxk, ˆyk)
{
}
(ˆxk, ˆyk)
→
taking k
→ ∞
≤
sibility of the limit point (¯x, ¯y) for problem (VP). Next, by taking k
dist((ˆxk, ˆyk),
F ∗, we obtain that dist((¯x, ¯y),
and F (¯x, ¯y)

F ∗,
∗ is compact, sequence
is bounded and we can assume without lost of generality that
. Since the value function v is continuous, by
(¯x, ¯y) as k
ǫk, we obtain the fea-
in
∗) = δ

∗) = δ and F (ˆxk, ˆyk)
F ∗, a contradiction.

C and f (ˆxk, ˆyk)

⊓⊔
Before we clarify the third motivation, we deﬁne the concepts of NNAMCQ

→ ∞
∈

in (ˆxk, ˆyk)

→ ∞
S

v(ˆxk)

S
≤

≤

≤

−

S

for problem (VP) and ENNAMCQ for problem (VP)ǫ where ǫ

0.

≥

Deﬁnition 3 Let (¯x, ¯y) be a feasible solution to problem (VP). We say that
NNAMCQ holds at (¯x, ¯y) for problem (VP) if

Let (¯x, ¯y)
either f (¯x, ¯y)

∈

0 /
∈

∂f (¯x, ¯y)

∂v(¯x)

0

+

C (¯x, ¯y).

}
C, we say that ENNAMCQ holds at (¯x, ¯y) for problem (V P )ǫ if

× {

N

−

(26)

v(¯x) < ǫ or f (¯x, ¯y)

v(¯x)

−

≥

ǫ but (26) holds.

−

Proposition 7 Let (¯x, ¯y) be a feasible solution to problem (V P ). Suppose that
v(x) is Lipschitz continuous around ¯x. Then NNAMCQ never holds at (¯x, ¯y).

20

Jane J. Ye et al.

v(¯x) < 0
Proof By deﬁnition of the value function, we can never have f (¯x, ¯y)
and we always have f (¯x, ¯y)
v(¯x) = 0. But since (¯x, ¯y) is a feasible solution to
problem (VP), it is easy to see that (¯x, ¯y) must be a solution to the following
problem

−

−

But by the optimality condition we must have

min
(x,y)∈C {

f (x, y)

v(x)
}

.

−

0

∈

∂f (¯x, ¯y)

∂v(¯x)

+

0

}

× {

−

C (¯x, ¯y).

N

This means that (26) would never hold.

⊓⊔
Although the NNAMCQ never hold for (VP), for ǫ > 0, ENNAMCQ is a
standard constraint qualiﬁcation for (VP)ǫ; see e.g. [25, Proposition 8]. More-
over thanks to the model structures, it holds automatically for (VP)ǫ if ǫ > 0.
Hence, according to the DC theories established in the preceding section, pow-
erful DCA can be employed to solve (VP)ǫ.

Proposition 8 For any (¯x, ¯y)
NAMCQ at (¯x, ¯y).

∈

C, problem (VP)ǫ with ǫ > 0 satisﬁes EN-

Proof If f (x, y)
−
(¯x, ¯y). Now suppose that f (¯x, ¯y)
i.e.,

v(x) < ǫ holds, then by deﬁnition, ENNAMCQ holds at
ǫ and ENNAMCQ does not hold,

v(¯x)

−

≥

It follows from the partial subdiﬀerentiation formula (2) that

0

∂f (¯x, ¯y)

∂v(¯x)

0

+

C (¯x, ¯y).

−

× {

}

N

∈

0

∈

(cid:20)

By (2) we have

∂xf (¯x, ¯y)

∂v(¯x)

−

∂yf (¯x, ¯y)

C (¯x, ¯y).

N

+

(cid:21)

(27)

C (¯x, ¯y) = ∂δC (¯x, ¯y)

∂xδC (¯x, ¯y)

∂yδC(¯x, ¯y)

N
where C(¯x) :=
that

y

{

∈

Y

|

⊆
gi(¯x, y)

×
0, i = 1, . . . , l

≤

Rn

× NC(¯x)(¯y),
. Thus, it follows from (27)

⊆

0

∂yf (¯x, ¯y) +

∈
which further implies that ¯y
the assumption that f (¯x, ¯y)
follows immediately.

−

≥

(¯x). However, an obvious contradiction to
ǫ occurs and thus the desired conclusion

∈ S
v(¯x)

⊓⊔
By using the deﬁnition of KKT points for (DC) in Deﬁnition 1, under the
assumption that the value function is locally Lipschitz continiuous, we deﬁne
KKT points for problem (VP)ǫ.

Deﬁnition 4 We say a point (¯x, ¯y) is a KKT point of problem (VP)ǫ with
ǫ

0 if there exists λ

0 such that

≥

(

0
∈
f (¯x, ¯y)

∂F1(¯x, ¯y)
v(¯x)

∂F2(¯x, ¯y) + λ∂f (¯x, ¯y)
0, λ (f (¯x, ¯y)
ǫ

−
v(¯x)

λ∂v(¯x)

× {
ǫ) = 0.

+

0

}

C (¯x, ¯y),

N

−

≤

−

−

≥

−
−

}
NC(¯x)(¯y),

Diﬀerence of convex algorithms for bilevel programs

21

By virtue of Proposition 8 and Theorem 1, we have the following necessary
optimality condition. Since the issue of constraint qualiﬁcations for problem
(VP) is complicated and it is not the main concern in this paper, we refer the
reader to discussions on this topic in [2,47].

Theorem 4 Let (¯x, ¯y) be a local optimal solution to problem (VP)ǫ with ǫ
0.
Suppose either ǫ > 0 or ǫ = 0 and a constraint qualiﬁcation holds. Then (¯x, ¯y)
is a KKT point of problem (VP)ǫ.

≥

3.3 Inexact proximal DCA for solving (VP)ǫ

In this subsection we implement the proposed DC algorithms in Section 2
to solve (VP)ǫ. To proceed, let us describe iP-DCA to solve (VP)ǫ. Given a
current iterate (xk, yk) for each k = 0, 1, . . ., solving the lower level problem
parameterized by xk

f (xk, y), s.t. g(xk, y)

min
y∈Y

0

≤

leads to a solution ˜yk
KT (xk, ˜yk). Select

∈

S(xk) and a corresponding KKT multiplier γk

∈

ξk
0 ∈

∂F2(xk, yk),

ξk
1 ∈

l

∂xf (xk, ˜yk) +

i ∂xgi(xk, ˜yk).
γk

(28)

i=1
X

Note that according to (24) in Theorem 3, if the partial derivative formula
holds, then ξk
i ∂xgi(xk, ˜yk) is an element of the subd-
iﬀerential ∂v(xk). Compute (xk+1, yk+1) as an approximate minimizer of the
strongly convex subproblem for (VP)ǫ given by

∂xf (xk, ˜yk) +

l
i=1 γk

1 ∈

P

min
(x,y)∈C

F1(x, y)

− h

ρ
ξk
0 , (x, y)
2 k
i
f (xk, ˜yk)

f (x, y)

+

(x, y)

−
ξk
1 , x

− h

+ βk max
{

−

(xk, yk)
k

2

(29)

xk

ǫ, 0

,

}

i −

−

satisfying one of the two inexact criteria. Under the assumption that KT (xk, y)
is nonempty for all y
0
now follows:

S(xk), the description of iP-DCA on (VP)ǫ with ǫ

≥

∈

22

Jane J. Ye et al.

Algorithm 3 iP-DCA for solving (VP)ǫ
1: Take an initial point (x0, y0) ∈ X × Y ; δβ > 0; an initial penalty parameter β0 > 0,

tol > 0.

2: for k = 0, 1, . . . do

γk ∈ KT (xk, ˜yk).

1. Solve the lower level problem Pxk deﬁned in (3.3) and obtain ˜yk ∈ S(xk) and

2. Compute ξk
2. Obtain an inexact solution (xk+1, yk+1) of (29).
3. Stopping test. Compute tk+1 = max{f (xk+1, yk+1) − f (xk, ˜yk) − hξk

i , i = 0, 1 according to (28).

ǫ, 0}. Stop if max{k(xk+1, yk+1) − (xk, yk)k, tk+1} < tol.

1 , xk+1 − xki −

4. Penalty parameter update. Set

βk+1 =

βk + δβ,
βk,

(

if max{βk, 1/tk+1} < k(xk+1, yk+1) − (xk, yk)k
otherwise.

−1,

5. Set k := k + 1.

3: end for

Thanks to Proposition 8, when ǫ > 0, problem (VP)ǫ satisﬁes ENNAMCQ
automatically. Moreover, since the partial subgradient formula (23) holds,
according to (24) in Theorem 3, the selection criteria in (28) implies that
∂v(xk). Hence the convergence of iP-DCA for solving (VP)ǫ (Algorithm
ξk
1 ∈
3) follows from Theorem 1 and Proposition 3.

Theorem 5 Assume that F is bounded below on C. Let
be an iter-
ate sequence generated by Algorithm 3. Moreover assume that the partial sub-
gradient formula (1) holds at every iterate point (xk, yk) and KT (xk, y)
for all y
βk
{
of problem (VP)ǫ.

∅
S(xk). Suppose that either ǫ > 0 or ǫ = 0 and the penalty sequence
is a KKT point

is bounded. Than any accumulation point of

(xk, yk)
}

(xk, yk)
}

=

∈

}

{

{

f
We now assume that the lower level objective f is diﬀerentiable and
∇
is Lipschitz continuous with modulus Lf on set C. Given a current iterate
(xk, yk), the next iterate (xk+1, yk+1) can be returned as an approximate min-
imizer of subproblem (29) with linearized f given by

min
(x,y)∈C

F1(x, y)

+ βk max
{

ρk
ξk
0 , (x, y)
2 k
−
i
f (xk, yk), (x, y)

(x, y)

− h
f (xk, yk) +

+

h∇

2

(xk, yk)
k
(xk, yk)
−
i
f (xk, ˜yk)

−

Under the assumption that KT (xk, y)
of iPL-DCA on (VP)ǫ with ǫ

=
0 thus reads as:

for all y

∅

≥

∈

xk

ξk
1 , x

− h

.
−
}
(30)
S(xk), the iterate scheme

i −

ǫ, 0

6
6
Diﬀerence of convex algorithms for bilevel programs

23

Algorithm 4 iPL-DCA for solving (VP)ǫ
1: Take an initial point (x0, y0) ∈ X × Y ; δβ, σ > 0; β0 > 0; ρ0 = 1
2: for k = 0, 1, . . . do

2 β0Lf + σ; tol > 0.

1. Solve the lower level problem Pxk deﬁned in (3.3) and obtain ˜yk ∈ S(xk) and

γk ∈ KT (xk, ˜yk).

i , i = 0, 1 according to (28).

2. Compute ξk
2. Obtain an inexact solution (xk+1, yk+1) of (30).
3. Stopping test. Compute tk+1 = max{f (xk, yk) + h∇f (xk, yk), (xk+1, yk+1) −
1 , xk+1 − xki − ǫ, 0}. Stop if max{k(xk+1, yk+1) −

(xk, yk)i − f (xk, ˜yk) − hξk
(xk, yk)k, tk+1} < tol.

4. Penalty parameter update. Set

βk + δβ,
βk,

if max{βk, 1/tk+1} < k(xk+1, yk+1) − (xk, yk)k
otherwies.

−1,

βk+1 =

ρk+1 =

(
1
2

βk+1Lf + σ.

5. Set k := k + 1.

3: end for

The convergence of iPL-DCA follows from Theorem 2 and Proposition 4

directly.

(xk, yk)
}

Theorem 6 Assume that F is bounded below on C, f is Lf smooth on C.
Let the sequence
be generated by Algorithm 4. Moreover assume
that the partial subgradient formula (1) holds at every iterate point (xk, yk)
S(xk). Suppose that either ǫ > 0 or ǫ = 0 and
and KT (xk, y)
=
∈
the penalty parameter sequence
is bounded. Then any accumulation point
of

is a KKT point of problem (VP)ǫ.

for all y

βk

{

∅

}

{

(xk, yk)
}

{

4 Numerical experiments on SV bilevel model selection

In this section, we will conduct numerical experiments on the SV bilevel model
selection problem (SVBP). Extensive numerical experiments of iP-DCA on
multiple hyperparameter selection models (e.g., elastic net, sparse group lasso,
low-rank matrix completion and etc) are presented in [17].

Rn+1, y := (w1, . . . , wT , c)

R(n+1)T , X = [ 1
λub

, 1
λlb

]

×

∈

Let x := (µ, ¯w)
[ ¯wlb, ¯wub], Y = R(n+1)T ,

∈

f (x, y) =

2

wt
k
2µ

k

+

T

t=1
X





Xj∈Ωt

trn

max(1

−

bj(aT

j wt

ct), 0)



,

−



and

g(x, y) = 

g1(x, y)
...
gT (x, y)




with

gt(x, y) =

¯w
−
wt

wt
¯w

−
−

(cid:19)

(cid:18)

, t = 1, . . . , T.






6
24

Jane J. Ye et al.

∅

F

≥

=

×

(x)

¯wlb > 0,

Obviously F , f and g are all convex functions deﬁned an open set containing
Y , and problem (SVBP) can be regarded as a special case of the DC
X
bilevel program (DCBP). Both F and f are bounded below on X
Y . When
¯wub
,
for an open set containing X. And since bj
}
f (x, y) is coercive and continuous with respect to lower-level variable y for any
given x in an open set containing X, thus S(x)
for all x in an open set
containing X. The function g is smooth and f is a sum of a smooth function
and a function which is independent of variable x. Hence by Proposition 1, the
partial diﬀerential formula (1) holds at each point (x, y). Since the lower level
constraints are aﬃne, KKT conditions holds at any y
X.
Therefore, all conditions required by the convergence results of iP-DCA in
Theorem 5 are satisﬁed.

S(x) for any x

∈ {−

1, 1

×

=

∈

∈

∅

We now describe how to calculate the main objects that are required in
iP-DCA on problem (SVBP). At the current iterate xk := (µk, ¯wk), solve
(Pµk, ¯wk ), the lower level problem parameterized by µk, ¯wk and obtain a solu-
tion ˜yk := ( ˜w1, . . . , ˜wT , ˜c)

S(xk) and a corresponding KKT multiplier

∈
1,1, . . . , γk
1,T , γk

(γk

2,1, . . . , γk

2,T )

KT (xk, ˜yk),

∈

where γk
1,t and γk
0, t = 1, . . . , T and wt
convex, we have ξk
x, ξk

1 can be calculated by

−

2,t are multipliers corresponding to constraints

≤
0, t = 1, . . . , T , respectively. Since F (x, y) is
0 = 0. Since f (x, y) and g(x, y) are both smooth in variable

¯wk

≤

−

−

¯wk

wt

2

PT

t=1 k ˜wtk
2(µk)2

ξk
1 =

−
T
t=1 γk

T
t=1 γk

2,t!

−
where γk
i
the rest of steps in Algorithm 3.

P
:= (γk

1,t −
1,t, γk

P

=

xf (xk, ˜yk)+

∇

T

t=1
X

∇

xgt(xk, ˜yk)T γk

∂v(xk),

t ⊆

2,t). With these objects calculated, we can then carry out

2

2

kwtk

T
t=1

Although the SV bilevel model selection is a nonsmooth bilevel program,
by using auxiliary variables, the problem can be reformulated as a smooth
bilevel program with a convex lower level program for which MPEC approach
and the iPL-DCA are both applicable. This approach has been taken in [22]
in which some nonlinear program solver has been used to solve the resulting
MPEC. Because the smooth lower level objective in the reformulated bilevel
2µ , and the Lipschitz constant Lf of the gradient
program consists of
kwtk
2µ with respect to variables (w1, . . . , wT ) and µ will be extremely
of
large when µ is optimized over the interval with small values. According to
the update rule for the regularizer parameter ρk in the iPL-DCA, ρk, as the
coeﬃcient of the regularizer terms in the subproblem during each iteration, is
linear w.r.t. Lf and will be extremely large. For this reason, the iPL-DCA is
not a good choice for this problem. In next subsection, we will compare our
proposed algorithms with the MPEC approach considered in [22]. In numer-
ical experiments, we will follow the suggestions given in [22] to replace the
complementarity constraints with the relaxed complementarity constraints.

T
t=1

P

P

6
6
 
Diﬀerence of convex algorithms for bilevel programs

25

As claimed by [22], such approach can facilitate an early termination of cross-
validation and ease the diﬃculty of dealing the complementarity constraints
for nonlinear program solver.

4.1 Numerical tests

point′, ′MaxIterations′ being 200 and ′MaxFunctionEvaluations′

|

|

Ω

−

All the numerical experiments are implemented on a laptop with Intel(R)
Core(TM) i7-9750H CPU@ 2.60GHz and 32.00 GB memory. All the codes are
implemented on MATLAB 2019b. The subproblems in iP-DCA are all con-
vex optimization problems and we apply the Matlab software package SDPT3
[45,46] with default setting to solve them. MPEC problem is solved by us-
ing fmincon in Matlab optimization toolbox with setting ′Algorithm′ being
′interior
being 106. MPEC approach is implemented with low and strict tolerance by
setting ′OptimalityTolerance′, ′ConstraintTolerance′, ′StepTolerance′
being tol = 10−2, and 10−6. As fmincon needs extremely long time to solve
large dimension MPEC problems, we ﬁrst use small size datasets to conduct
the numerical comparison between iP-DCA and MPEC approach. We test
here three real datasets “australian scale”, “breast-cancer scale” and “dia-
betes scale” downloaded from the SVMLib repository [8]1. Each dataset is
= ℓtrain data pairs, which is used
randomly split into a training set Ω with
in the cross-validation bilevel model and a hold-out test set
= ℓtest
data pairs. We give the descriptions of datasets in Table 1. For each dataset,
we use a three-fold cross-validation in the SV bilevel model selection problem
(SVBP), i.e. T = 3, and that each training fold consists of two-thirds of the
total training data and validation fold consists of one-third of the total train-
ing data. We repeat the experiments 20 times for each dataset. The values
of parameters in SV bilevel model selection (SVBP) are set as: λlb = 10−4,
λub = 104, ¯wlb = 10−6 and ¯wub = 1.5. For our approach, we test three diﬀer-
ent values of relaxation parameter ǫ
in (VP)ǫ. And the value
∈ {
of relaxation parameter of the relaxed complementarity constraints in MPEC
is set to be 10−6. The initial points for both iP-DCA and MPEC approach
are chosen as λ = 1, ¯w = 0.1e, where e denotes the vector whose elements
are all equal to 1, and the values of other variables are all equal to 0. These
settings are used for all experiments. Parameters in iP-DCA are set as β0 = 1,
ρ = 10−2 and δβ = 5. And we terminate iP-DCA when tk+1 < 10−4 and
(xk+1, yk+1)

(xk, yk)
k
For each experiment, after we obtain the hyperparameters ˆµ and ˆ¯w from
implementing our proposed algorithm and MPEC approach for the SV bilevel
model selection, we calculate their corresponding cross-validation error (CV
error) and test error for comparing the performances of these two methods. For
calculating the CV error, we put ˆµ and ˆ¯w back to the lower level problem in
problem (SVBP) to get the corresponding lower level solution ( ˆw1, . . . , ˆwT , ˆc)

(xk, yk)
k

0, 10−2, 10−4

) < tol.

/(1 +

with

|N |

N

−

k

k

}

1 http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/.

26

Jane J. Ye et al.

and calculate the corresponding cross-validation error Θ( ˆw1, . . . , ˆwT , ˆc). Next,
as in [22] we implement a post-processing procedure to calculate the general-
ization error on the hold-out data for each instance. In particular as suggested
in [22], since only two thirds of the data in Ω was used in each fold while
in testing we use all the training data from Ω, we should solve the following
support vector classiﬁcation problem with 3
2ˆµ and ˆ¯w as hyperparameter
2

ˆλ = 3

min
− ˆ¯w ≤ w ≤ ˆ¯w
c ∈ R




3
4ˆµ k

w

k

2 +

j∈Ω
X

max(1

−

bj(aT

j w

−

c), 0)




to obtain the ﬁnal classiﬁer ( ˆw, ˆc). Then the test (hold-out) error rate is cal-
culated as:





Test error =

1
ℓtest

1
2 |

sign(aT

i ˆw

ˆc)

bi

,

|

−

−

i∈N
X

|

,

|

bi

−

−

N

ˆc)

i ˆw

sign(aT

where sign(x) denote the sign function. Note that for each (ai, bi) in the test
is either equal to zero or 2 and hence the test error
set
is the average misclassiﬁcation by the ﬁnal classiﬁer. The achieved numerical
results averaged over 20 repetitions for each dataset are reported in Table 2.
We compare the computational performance of iP-DCA with diﬀerent val-
ues of ǫ and tol, i.e., ǫ = 0, 10−4, 10−2 and tol = 10−2, 10−3 and the MPEC
approach with diﬀerent values of tol, i.e., tol = 10−2, 10−6. Observe from Ta-
ble 2 that diﬀerent values of ǫ and tol do not inﬂuence the cross-validation
error and test error obtained by iP-DCA a lot. The case ǫ = 0 takes more time
to achieve desired tolerance on some data sets. This may be because that, as
we have shown in Propositions 7 and 8, when ǫ = 0, NNAMCQ never hold
for the problem (VP), the ENNAMCQ always holds for the problem (VP)ǫ
when ǫ > 0. As a consequence, the problem (VP) is more ill-conditioned com-
pared to the problem (VP)ǫ with ǫ > 0, then iP-DCA performs better on
the problem (VP)ǫ with ǫ > 0. Compared with MPEC approach, our pro-
posed iP-DCA achieves a smaller cross-validation error, which is exactly the
value of upper level objective of the bilevel problem (SVBP), on datasets
“breast-cancer scale” and “diabetes scale”. Furthermore, the time taken by
our proposed iP-DCA is shorter than MPEC approach. The test errors of our
proposed iP-DCA and MPEC approach are similar and iP-DCA achieves a
smaller test error than MPEC approach on dataset “diabetes scale”. More-
over both iP-DCA and MPEC approach can obtain a relatively good solution
without requiring a small tol.

Next, we are going to test our proposed iP-DCA on two large scale datasets
“mushrooms” and “phishing” downloaded from the SVMLib repository. The
descriptions of datasets are given in Table 1. We set tol = 10−2 for these
tests. The numerical results averaged over 20 repetitions for each data set are
reported in Table 3. It can be observed from Table 3 that diﬀerent values of ǫ
and tol do not inﬂuence the cross-validation error obtained by iP-DCA much
but the case ǫ = 0 takes more time to achieve desired tolerance on some data
sets. And iP-DCA can obtain a satisfactory solution within an acceptable time
on large scale problems.

Diﬀerence of convex algorithms for bilevel programs

27

Table 1 Description of datasets used

Dataset

australian scale
breast-cancer scale
diabetes scale
mushrooms
phishing

ℓtrain

345
339
384
4062
5526

ℓtest

345
344
384
4062
5529

n

14
10
8
112
68

T

3
3
3
3
3

Table 2 Numerical results comparing iP-DCA and MPEC approach

Dataset

australian scale

breast-cancer scale

diabetes scale

Method
iP-DCA(ǫ = 0, tol = 10−2)
iP-DCA(ǫ = 0, tol = 10−3)
iP-DCA(ǫ = 10−2, tol = 10−2)
iP-DCA(ǫ = 10−2, tol = 10−3)
iP-DCA(ǫ = 10−4, tol = 10−2)
iP-DCA(ǫ = 10−4, tol = 10−3)
MPEC approach(tol = 10−2)
MPEC approach(tol = 10−6)
iP-DCA(ǫ = 0, tol = 10−2)
iP-DCA(ǫ = 0, tol = 10−3)
iP-DCA(ǫ = 10−2, tol = 10−2)
iP-DCA(ǫ = 10−2, tol = 10−3)
iP-DCA(ǫ = 10−4, tol = 10−2)
iP-DCA(ǫ = 10−4, tol = 10−3)
MPEC approach(tol = 10−2)
MPEC approach(tol = 10−6)
iP-DCA(ǫ = 0, tol = 10−2)
iP-DCA(ǫ = 0, tol = 10−3)
iP-DCA(ǫ = 10−2, tol = 10−2)
iP-DCA(ǫ = 10−2, tol = 10−3)
iP-DCA(ǫ = 10−4, tol = 10−2)
iP-DCA(ǫ = 10−4, tol = 10−3)
MPEC approach(tol = 10−2)
MPEC approach(tol = 10−6)

CV error

Test error

Time(sec)

0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03
0.28 ± 0.03

0.07 ± 0.01
0.06 ± 0.01
0.07 ± 0.01
0.07 ± 0.01
0.07 ± 0.01
0.06 ± 0.01
0.09 ± 0.01
0.09 ± 0.01

0.55 ± 0.02
0.55 ± 0.02
0.56 ± 0.02
0.56 ± 0.02
0.55 ± 0.02
0.55 ± 0.02
0.59 ± 0.02
0.59 ± 0.02

0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01
0.15 ± 0.01

0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01
0.03 ± 0.01

0.24 ± 0.02
0.24 ± 0.02
0.24 ± 0.02
0.24 ± 0.02
0.24 ± 0.02
0.24 ± 0.02
0.26 ± 0.02
0.26 ± 0.02

70.0 ± 116.9
75.7 ± 123.8
10.1 ± 5.1
119.8 ± 55.8
58.5 ± 108.8
118.6 ± 123.6
130.0 ± 82.4
391.3 ± 226.9

26.2 ± 19.5
69.6 ± 52.3
15.6 ± 4.5
106.5 ± 44.8
17.5 ± 6.3
72.5 ± 55.0
54.3 ± 13.7
226.0 ± 108.4

14.5 ± 30.3
24.2 ± 44.8
3.1 ± 0.4
65.7 ± 33.6
18.2 ± 27.1
45.1 ± 53.8
52.1 ± 42.2
326.8 ± 312.0

Table 3 Numerical results of iP-DCA on datasets “mushrooms” and “phishing” with tol =
10−2

Dataset

Method

CV error

Test error

mushrooms

phishing

iP-DCA(ǫ = 0)
iP-DCA(ǫ = 10−2)
iP-DCA(ǫ = 10−4)

iP-DCA(ǫ = 0)
iP-DCA(ǫ = 10−2)
iP-DCA(ǫ = 10−4)

6.36e-04 ± 5.94e-04
1.53e-03 ± 3.85e-03
6.38e-04 ± 6.08e-04

0 ± 0
3.57e-04 ± 1.34e-03
0 ± 0

0.29 ± 0.00
0.29 ± 0.00
0.29 ± 0.00

0.09 ± 0.00
0.09 ± 0.00
0.09 ± 0.00

Time(sec)

334.3 ± 346.1
109.3 ± 35.2
162.9 ± 27.4

357.9 ± 95.2
222.1 ± 18.9
215.4 ± 46.5

4.2 Further numerical tests

In the follow-up paper [17], experiments on the SV bilevel model selection
problem (SVBP) and comparisions with the state-of-the-art approaches in
machine learning community, including the grid search, the random search
and the tree-structured Parzen estimator Bayesian approach (TPE) [7] have
been conducted. In this subsection we summarize these results.

All the numerical experiments are implemented on a computer with In-
tel(R) Core(TM) i9-9900K CPU @ 3.60GHz and 16.00 GB memory. All the
codes are implemented in Python and are available at https://github.com/SUSTech-Optimization/VF-iDCA.

28

Jane J. Ye et al.

Six real datasets “liver-disorders scale”, “diabetes scale”, “breast-cancer scale”,
“sonar”, “a1a”, “w1a” collected from the SVMLib repository are tested. For
each dataset, the SV bilevel model selection problem (SVBP) with three-fold
and six-fold cross-validations, i.e. T = 3, 6 are solved respectively. Each dataset
is randomly split in the same way as in Section 4.1. The experiments are re-
peated 30 times for each dataset. The values of parameters in SV bilevel model
selection (SVBP) are set as: λlb = 10−4, λub = 104, ¯wlb = 10−6 and ¯wub = 10.
For the implementation of iP-DCA, unlike in Section 4.1, in which iP-DCA
is applied to solve problem (SVBP) directly, for the numerical experiments in
this part, the hyperparameter decoupling technique is applied to reformulate
the problem (SVBP) into a DC bilevel program (DCBP) (see [17] for de-
tails) before applying iP-DCA. The strongly convex subproblem in iP-DCA
at each iteration is solved by using the CVXPY package. Parameters in iP-
DCA are set as ǫ = 0, β0 = 1 and δβ = 5. And iP-DCA is terminated when
max
< tol. The computa-
k
−
tional performance of iP-DCA is compared using two diﬀerent values of tol,
i.e., tol = 10−1, 10−2.

(xk, yk)
k

(xk, yk)
k

(xk+1, yk+1)

), tk+1

/(1 +

{k

}

For the implementation of the grid search and the random search, the

searches are run over two-dimension hyperparameter θ = (θ1, θ2) on

−

−

−

4,

6,

} × {−

3, . . . , 3, 4

5, . . . , 1, 2

and with setting µ = 10θ1 and ¯w =
}
{−
(10θ2, . . . , 10θ2)⊤ in the problem (SVBP). The subproblems in the grid search
and the random search are all solved by using the CVXPY package. And for the
implementation of TPE, the hyperparameter log10(µ) in [
4, 4], and the hy-
perparameter log10( ¯wi) in [
6, 2] are searched, respectively. Because TPE will
be extremely slow when the dimension of the hyperparameters is too high, the
maximum number of iteration of TBE is set to be 10. And the TPE method is
also tested on the simpliﬁed model with a two-dimension hyperparameter, that
has the same setting as the one solved by the search methods. This method is
denoted by “TPE2”. For this simpliﬁed “TPE2”, the maximum number of iter-
ations of the TPE method are set to be 100. The TPE method is implemented
using the code collected from https://github.com/hyperopt/hyperopt and
its subproblem is solved by using the CVXPY package. In the implementation
of all the methods, the CVXPY package is set with using the open source
solvers ECOS and SCS.

−

The achieved numerical results of the three-fold and the six-fold SV bilevel
model selection problem averaged over 30 repetitions for each dataset are re-
ported in Tables 4 and 5, respectively. The cross-validation error (CV error)
and the test error are calculated in the same way as in Section 4.1. Observe
from Table 4 and 5 that compared with the state of the art approaches, in-
cluding the grid search method, the random search method and the TPE, our
proposed iP-DCA shows superiority by achieving a smaller cross-validation er-
ror and also a smaller test error. Furthermore, the time spent by our proposed
iP-DCA is shorter than other approaches on most of the datasets. It can be
also observed that on all the datasets except “breast-cancer scale”, diﬀerent
values of tol do not inﬂuence the cross-validation error and test error obtained
by iP-DCA a lot. In view of the test error, iP-DCA can always obtain a rela-

Diﬀerence of convex algorithms for bilevel programs

29

tively good solution without requiring a tight tolerance. This suggests to set
a moderate algorithmic tolerance for iP-DCA when we apply it on practical
problems for obtaining a satisfactory solution with shorter computation time.

Table 4 Numerical results of three-fold SV bilevel model selection problem on datasets
“liver-disorders scale”, “diabetes scale”, “breast-cancer scale”, “sonar”, “a1a”, “w1a”

Dataset

liver-disorders scale
ℓtrain = 72
ℓtest = 73
n = 5

diabetes scale
ℓtrain = 384
ℓtest = 384
n = 8

breast-cancer scale
ℓtrain = 388
ℓtest = 345
n = 10

sonar
ℓtrain = 102
ℓtest = 106
n = 60

a1a
ℓtrain = 801
ℓtest = 804
n = 123

w1a
ℓtrain = 1236
ℓtest = 1241
n = 300

Method
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2

CV error

0.53 ± 0.07
0.53 ± 0.09
0.64 ± 0.08
0.58 ± 0.06
0.65 ± 0.07
0.61 ± 0.07

0.48 ± 0.02
0.48 ± 0.02
0.55 ± 0.03
0.56 ± 0.04
0.55 ± 0.03
0.54 ± 0.03

0.09 ± 0.01
0.05 ± 0.01
0.08 ± 0.01
0.09 ± 0.01
0.09 ± 0.01
0.07 ± 0.01

0.03 ± 0.02
0.00 ± 0.00
0.58 ± 0.08
0.54 ± 0.06
0.64 ± 0.10
0.57 ± 0.08

0.27 ± 0.02
0.27 ± 0.02
0.41 ± 0.02
0.41 ± 0.02
0.42 ± 0.03
0.41 ± 0.02

0.01 ± 0.00
0.01 ± 0.00
0.06 ± 0.01
0.06 ± 0.01
0.06 ± 0.01
0.06 ± 0.01

5 Concluding remarks

Test error

0.27 ± 0.03
0.28 ± 0.05
0.34 ± 0.06
0.32 ± 0.05
0.34 ± 0.05
0.33 ± 0.06

0.23 ± 0.01
0.23 ± 0.01
0.33 ± 0.05
0.30 ± 0.06
0.29 ± 0.05
0.32 ± 0.06

0.04 ± 0.01
0.03 ± 0.01
0.12 ± 0.06
0.08 ± 0.09
0.10 ± 0.11
0.09 ± 0.10

0.24 ± 0.04
0.24 ± 0.04
0.40 ± 0.12
0.34 ± 0.10
0.41 ± 0.12
0.37 ± 0.13

0.17 ± 0.01
0.18 ± 0.01
0.24 ± 0.02
0.22 ± 0.03
0.23 ± 0.03
0.24 ± 0.02

0.02 ± 0.00
0.02 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00

Time(sec)

0.09 ± 0.02
0.20 ± 0.06
0.53 ± 0.01
0.56 ± 0.03
0.37 ± 0.29
2.88 ± 1.16

0.18 ± 0.02
0.28 ± 0.03
1.70 ± 0.11
1.83 ± 0.09
6.64 ± 4.30
18.67 ± 7.84

0.14 ± 0.01
1.12 ± 0.59
1.63 ± 0.04
1.80 ± 0.03
9.14 ± 4.55
14.72 ± 6.02

0.48 ± 0.09
2.22 ± 1.50
3.19 ± 0.10
3.23 ± 0.06
40.77 ± 7.12
18.47 ± 6.84

1.10 ± 0.07
10.17 ± 5.47
8.04 ± 0.15
8.62 ± 0.30
176.59 ± 17.38
65.51 ± 16.24

4.87 ± 0.51
27.49 ± 7.31
20.21 ± 0.82
20.44 ± 1.10
299.62 ± 78.72
86.10 ± 28.19

Motivated by hyperparameter selection problems, in this paper we develop
two DCA type algorithms for solving the DC bilevel program. Our numeri-
cal experiments on the SV bilevel model selection show that our approach is
promising. Due to the space limit, we are not able to present more studies for
more complicated models in hyperparameter selection problems. We hope to
study these problems in our future work. Note that all of our results except
the result on the constraint qualiﬁcation ENNAMCQ in Proposition 8 can be
applied to the case where there are also some extra upper level constraints
G(x, y) := (G1(x, y), . . . , Gk(x, y))
0 as long as each function Gi(x, y) is
a diﬀerence of convex function. In this case, the corresponding approximate
bilevel program has an extra DC constraint G(x, y)
0. Although the con-
straint qualiﬁcation ENNAMCQ no longer holds automatically, it is reasonable

≤

≤

30

Jane J. Ye et al.

Table 5 Numerical results of six-fold SV bilevel model selection problem on datasets “liver-
disorders scale”, “diabetes scale”, “breast-cancer scale”, “sonar”, “a1a”, “w1a”

Dataset

liver-disorders scale
ℓtrain = 72
ℓtest = 73
n = 5

diabetes scale
ℓtrain = 384
ℓtest = 384
n = 8

breast-cancer scale
ℓtrain = 388
ℓtest = 345
n = 10

sonar
ℓtrain = 102
ℓtest = 106
n = 60

a1a
ℓtrain = 801
ℓtest = 804
n = 123

w1a
ℓtrain = 1236
ℓtest = 1241
n = 300

Method
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2
iP-DCA(tol = 10−1)
iP-DCA(tol = 10−2)
Grid Search
Random Search
TPE
TPE2

CV error

0.41 ± 0.08
0.41 ± 0.08
0.63 ± 0.08
0.62 ± 0.07
0.63 ± 0.08
0.62 ± 0.07

0.43 ± 0.02
0.43 ± 0.02
0.55 ± 0.03
0.56 ± 0.03
0.55 ± 0.03
0.55 ± 0.03

0.08 ± 0.01
0.03 ± 0.01
0.08 ± 0.02
0.09 ± 0.02
0.09 ± 0.01
0.07 ± 0.02

0.00 ± 0.00
0.00 ± 0.00
0.59 ± 0.08
0.54 ± 0.06
0.60 ± 0.07
0.57 ± 0.08

0.19 ± 0.01
0.18 ± 0.02
0.40 ± 0.02
0.40 ± 0.02
0.41 ± 0.03
0.40 ± 0.02

0.01 ± 0.00
0.01 ± 0.00
0.05 ± 0.00
0.05 ± 0.00
0.05 ± 0.01
0.05 ± 0.00

Test error

0.27 ± 0.04
0.27 ± 0.05
0.33 ± 0.07
0.31 ± 0.05
0.34 ± 0.05
0.32 ± 0.06

0.23 ± 0.01
0.23 ± 0.01
0.32 ± 0.05
0.31 ± 0.05
0.27 ± 0.06
0.33 ± 0.05

0.04 ± 0.01
0.03 ± 0.01
0.15 ± 0.06
0.07 ± 0.08
0.11 ± 0.13
0.08 ± 0.09

0.23 ± 0.04
0.23 ± 0.04
0.39 ± 0.11
0.32 ± 0.08
0.39 ± 0.12
0.36 ± 0.12

0.17 ± 0.01
0.17 ± 0.01
0.25 ± 0.02
0.21 ± 0.03
0.23 ± 0.03
0.24 ± 0.02

0.02 ± 0.00
0.02 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00
0.03 ± 0.00

Time(sec)

0.18 ± 0.03
0.33 ± 0.14
0.78 ± 0.02
0.79 ± 0.04
1.06 ± 1.04
6.88 ± 4.15

0.35 ± 0.01
0.56 ± 0.08
3.18 ± 0.14
3.63 ± 0.21
29.52 ± 13.13
51.85 ± 20.49

0.29 ± 0.08
2.01 ± 0.17
3.38 ± 0.25
3.92 ± 0.29
25.96 ± 12.95
38.69 ± 16.27

0.92 ± 0.02
0.92 ± 0.02
6.57 ± 0.32
6.44 ± 0.28
97.65 ± 31.37
58.19 ± 29.60

4.22 ± 0.37
63.01 ± 186.14
17.60 ± 0.36
18.59 ± 0.42
312.63 ± 60.60
161.68 ± 42.67

26.74 ± 3.67
97.50 ± 35.99
44.29 ± 1.39
61.80 ± 2.91
703.72 ± 82.75
190.04 ± 39.00

to impose ENNAMCQ for the corresponding approximate bilevel program to
hold.

Acknowledgements. We thank the guest editor and three reviewers for
their helpful and constructive comments that have helped improve this pa-
per substantially. The alphabetical order of the authors indicates their equal
contributions to the paper.

References

1. Allende, G., Still, G.: Solving bilevel programs with the KKT-approach. Mathematical

Programming. 138, 309-332 (2013)

2. Bai, K., Ye, J.J.: Directional necessary optimality conditions for bilevel programs. Math-

ematics of Operations Research, 47, 1169-1191 (2022)

3. Bard, J.: Practical Bilevel Optimization: Algorithms and Applications. Kluwer Academic

Publishers, Dordrecht (1998)

4. Beck, A.: First-order methods in optimization. Society for Industrial and Applied Math-

ematics (2017).

5. Ben-Tal, A., Blair, C.: Computational diﬃculties of bilevel linear programming. Opera-

tions Research. 38, 556-560 (1990)

6. Bennett, K.P., Hu, J., Ji., X., Kunapuli, G., Pang, J.-S.: Model selection via bilevel
optimization, in The 2006 IEEE International Joint Conference on Neural Network Pro-
ceedings. 1922-1929 (2006)

Diﬀerence of convex algorithms for bilevel programs

31

7. Bergstra, J., Yamins, D., Cox, D.: Making a science of model search: Hyperparameter op-
timization in hundreds of dimensions for vision architectures, In: International Conference
on Machine Learning. 28(1), 115-123 (2013)

8. Chang, C-C., Lin, C-J.: LIBSVM : a library for support vector machines. ACM Trans-

actions on Intelligent Systems and Technology. 2(3), 1-27 (2011)

9. Clarke, F.H.: Optimization and Nonsmooth Analysis. Society for Industrial and Applied

Mathematics, Philadelphia (1990)

10. Clarke, F.H., Ledyaev, Y.S., Stern, R.J., Wolenski, P.R.: Nonsmooth Analysis and Con-

trol Theorey. Springer Science & Business Media, New York (1998)

11. Colson, B., Marcotte, P., Savard, G.: An overview of bilevel optimization. Annals of

Operations Research. 153(1-2), 235-256 (2007)

12. Dempe, S.: Foundations of Bilevel Programming. Kluwer Academic Publishers, Dor-

drecht (2002)

13. Dempe, S., Dutta, J.: Is bilevel programming a special case of mathematical program-

ming with equilibrium constraints?. Mathematical Programming. 131, 37-48 (2012)

14. Dempe, S., Zemkoho, A.: Bilevel Optimization: Advances and Next Challenges. Springer

Optimization and its Applications, vol. 161 (2020)

15. Dempe, S., Kalashnikov, V., P´erez-Vald´es, G., Kalashnykova, N.: Bilevel Programming

Problems. Energy Systems, Springer Science & Business Media, Berlin (2015)

16. Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., Pontil, M.: Bilevel programming for
hyperparameter optimization and meta-learning. In: International Conference on Machine
Learning. 80, 1568-1577 (2018)

17. Gao, L., Ye, J.J., Yin, H., Zeng, S., Zhang, J.: Value function based diﬀerence-of-convex
algorithm for bilevel hyperparameter selection problems. In: International Conference on
Machine Learning. 162, 7164-7182 (2022)

18. Henrion, R., Jourani, A., Outrata, J.V.: On the calmness of a class of multifunctions.

SIAM Journal on Optimization. 13, 603–618 (2002)

19. Horst, R., Thoai, N.V.: DC programming: overview. Journal of Optimization Theory

and Applications. 103(1), 1–43 (1999)

20. Jourani, A.: Constraint qualiﬁcations and Lagrange multipliers in nondiﬀerentiable pro-
gramming problems. Journal of Optimization Theory and Applications. 81, 533-548 (1994)
21. Kunapuli, G.: A bilevel optimization approach to machine learning. Ph.D Thesis. (2008)
22. Kunapuli, G., Bennett, K.P., Hu, J., Pang, J-S.: Classiﬁcation model selection via bilevel

programming. Optimization Methods and Software. 23, 475-489 (2008)

23. Kunapuli, G., Bennett, K.P., Hu, J., Pang, J-S.: Bilevel model selection for support

vector machines. In: CRM proceedings and lecture notes. 45, 129-158 (2008)

24. Lampariello, L., Sagratella, S.: Numerically tractable optimistic bilevel problems. Com-

putational Optimization and Applications. 76, 277-303 (2020)

25. Lin, G., Xu, M., Ye, J.J.: On solving simple bilevel programs with a nonconvex lower

level program. Mathematical Programming. 144, 277-305 (2014)

26. Liu, R., Mu, P., Yuan, X., Zeng, S., Zhang, J.: A generic ﬁrst-order algorithmic frame-
work for bi-level programming beyond lower-level singleton. In: International Conference
on Machine Learning. 119, 6305-6315 (2020)

27. Liu, R., Mu, P., Yuan, X., Zeng, S., Zhang, J.: A generic descent aggregation frame-
work for gradient-based bi-level optimization. IEEE Transactions on Pattern Analysis and
Machine Intelligence. (2022)

28. Luo, Z-Q., Pang, J-S., Ralph, D.: Mathematical Programs with Equilibrium Constraints.

Cambridge University Press, Cambridge (1996)

29. Mirrlees, J.A.: The theory of moral hazard and unobservable behaviour: Part I. The

Review of Economic Studies. 66, 3-21 (1999)

30. Moore, G.: Bilevel programming algorithms for machine learning model selection. Ph.D

Thesis. (2010)

31. Moore, G., Bergeron, C., Bennett, K.P.: Model selection for primal SVM. Machine

Learning. 85, 175-208 (2011)

32. Nie, J., Wang, L., Ye, J.J.: Bilevel polynomial programs and semideﬁnite relaxation

methods. SIAM Journal on Optimization. 27, 1728-1757 (2017)

33. Nie, J., Wang, L., Ye, J.J., Zhong, S.: A Lagrange Multiplier Expression Method
for Bilevel Polynomial Optimization, SIAM Journal on Optimization. 31(3), 2368-2395
(2021).

32

Jane J. Ye et al.

34. Okuno, T., Kawana, A.: Bilevel optimization of regularization hyperparameters in ma-
chine learning. In: Bilevel Optimization: Advances and Next Challenges, Ch. 6. Springer
Optimization and its Applications, vol. 161 (2020).

35. Outrata, J. V.: On the numerical solution of a class of Stackelberg problems, ZOR -

Methods and Models of Operations Research. 34, 255–277 (1990)

36. Outrata, J., Kocvara, M., Zowe, J.: Nonsmooth Approach to Optimization Problems
with Equilibrium Constraints: Theory, Applications and Numerical Results. Kluwer Aca-
demic Publishers, Boston (1998)

37. Pang, J.S., Razaviyayn, M. and Alvarado, A.: Computing B-stationary points of nons-

mooth DC programs. Mathematics of Operations Research. 42(1), 95-118 (2017)
38. Rockafellar, R.T.: Convex Anlysis. Princeton University Press, Princeton (1970)
39. Rockafellar, R.T.: Conjugate duality and optimization. CBMS-NSF Regional Confer-

ence Series in Applied Mathematics. 16, 1-74 (1974)

40. Shimizu, K., Ishizuka, Y., Bard, J.: Nondiﬀerentiable and Two-level Mathematical Pro-

gramming. Kluwer Academic Publishers, Dordrecht (1997)

41. Stackelberg, H.: Market Structure and Equilibrium. Springer Science & Business Media,

Berlin (2010)

42. Stephen, B., Vandenberghe, L.: Convex Optimization. Cambridge University Press,

Cambridge (2004)

43. Thi, H.A.L., Dinh, D.T.: Advanced Computational Methods for Knowledge Engineer-
ing, DC programming and DCA for general DC programs. pp. 15-35. Springer, Cham,
Switzerland (2014)

44. Thi, H.A.L., Dinh, T.P.: DC programming and DCA: thirty years of developments.

Math. Program. 169, 5-68 (2018)

45. Toh, K.C., Todd, M.J., Tutuncu, R.H.: SDPT3 — a Matlab software package for

semideﬁnite programming, Optimization Methods and Software. 11, 545–581 (1999)

46. Tutuncu, R.H., Toh, K.C., Todd, M.J.: Solving semideﬁnite-quadratic-linear programs

using SDPT3. Mathematical Programming, Series B. 95, 189–217 (2003)

47. Ye, J.J.: Constraint qualiﬁcations and optimality conditions in bilevel optimization. In:
Bilevel Optimization: Advances and Next Challenges, Ch. 8. Springer Optimization and
its Applications, vol. 161 (2020).

48. Ye J.J., Zhu, D. L.: Optimality conditions for bilevel programming problems. Optimiza-

tion. 33, 9–27 (1995)

