0
2
0
2

l
u
J

6
1

]

G
L
.
s
c
[

2
v
8
2
7
1
1
.
1
1
9
1
:
v
i
X
r
a

On Scaling Data-Driven Loop Invariant Inference

SAHIL BHATIA, Microsoft Research, India
SASWAT PADHI, University of California Los Angeles, USA
NAGARAJAN NATARAJAN, Microsoft Research, India
RAHUL SHARMA, Microsoft Research, India
PRATEEK JAIN, Microsoft Research, India

Automated synthesis of inductive invariants is an important problem in software verification. Once all the
invariants have been specified, software verification reduces to checking of verification conditions. Although
static analyses to infer invariants have been studied for over forty years, recent years have seen a flurry
of data-driven invariant inference techniques which guess invariants from examples instead of analyzing
program text. However, these techniques have been demonstrated to scale only to programs with a small
number of variables. In this paper, we study these scalability issues and address them in our tool Oasis that
improves the scale of data-driven invariant inference and outperforms state-of-the-art systems on benchmarks
from the invariant inference track of the Syntax Guided Synthesis competition.

1 INTRODUCTION

Inferring inductive invariants is one of the core problems of software verification. Recently, there
has been a flurry of data-driven invariant inference techniques [Alur et al. 2017; Garg et al. 2014,
2016; Li et al. 2017; Nguyen et al. 2017, 2012; Padhi et al. 2016; Sharma and Aiken 2016; Sharma
et al. 2013b,a, 2012; Thakur et al. 2015; Zhu et al. 2018] that learn invariants from examples. These
data-driven techniques offer attractive features such as the ability to systematically generate
disjunctive invariants. Whereas the well-know static invariant inference techniques either fail to
infer disjunctive invariants [Colón et al. 2003; Cousot and Cousot 1977; Cousot and Halbwachs 1978;
Miné 2006] or require a user-provided bound on the number of disjunctions [Bagnara et al. 2006;
Gulwani and Jojic 2007; Gulwani et al. 2008; Gupta et al. 2013; Sankaranarayanan et al. 2006]. At
the heart of the data-driven techniques is an active learning [Hanneke 2009] loop: a learner guesses
a candidate invariant from data and provides the candidate to a teacher. The teacher either validates
that the candidate is a valid invariant or returns a counterexample. This example is added to the
data and the process is repeated until the learner guesses a correct invariant. In this architecture,
the more the number of program variables in the verification problem, the more the learner is
likely to choose an incorrect candidate or take a long time to generate good candidates [Padhi et al.
2019]. Hence, as we discuss in our evaluation in Section 6, existing data-driven invariant inference
techniques have been shown to be effective only for programs with a small number of variables.
For data-driven invariant inference to be applicable to verification of practical software, these
scalability challenges must be addressed. There are two main obstacles to scalability which are
related to the number of program variables: First, a program can have many variables and often only
a small subset of these variables are relevant to the invariants. Intuitively, writing correct programs
that require complicated invariants with many variables is hard for developers and prior works on
invariant inference are also biased towards simple invariants [Albarghouthi and McMillan 2013].
In the absence of a technique that separates the relevant variables from the irrelevant, the learner
can get bogged down by the irrelevant variables. In particular, invariant inference benchmarks
in the Syntax Guided Synthesis (SyGuS) competition are provided as logic formulas where static
slicing [Horwitz et al. 1988] fails to remove the semantically irrelevant variables. Second, data-driven

Authors’ addresses: Sahil Bhatia, Microsoft Research, India, t-sab@microsoft.com; Saswat Padhi, University of California
Los Angeles, USA, padhi@cs.ucla.edu; Nagarajan Natarajan, Microsoft Research, India, nagarajn@microsoft.com; Rahul
Sharma, Microsoft Research, India, rahsha@microsoft.com; Prateek Jain, Microsoft Research, India, prajain@microsoft.com.

 
 
 
 
 
 
2

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

techniques also rely on some form of enumeration to generate candidate predicates. Thus, a higher
number of variables causes the enumerator to take a long time to reach pertinent candidates. For
example, if the enumerator exhaustively generates expressions in increasing size [Alur et al. 2017;
Padhi et al. 2019] then before enumerating expressions of size s, it must enumerate all expression
of size s − 1 over all variables.

To exemplify these scalability issues, we consider LoopInvGen [Padhi et al. 2019, 2016], the
state-of-the-art data-driven invariant inference tool that won in the invariant inference track of
SyGuS competition held in 2017 and 2018. It uses exhaustive enumeration to synthesize Boolean
features (simple predicates). Then, a Boolean function learner generates a candidate invariant
which is a Boolean combination of the features. As the number of program variables increases,
the scalability degrades because the enumerator must explore exponentially many features and
the learner needs many examples to avoid generating candidates with irrelevant variables; with a
large number of variables, the learner can overfit on the irrelevant variables to generate incorrect
candidates that will be rejected by the teacher [Padhi et al. 2019].

We explore addressing both the scalability issues, caused by enumeration and irrelevant variables,
through machine learning (ML). In particular, we make the following two contributions. First, we
describe a learner that can infer the relevant variables, thus ensuring that data-driven invariant
inference is only applied to the simpler problem with a few or no irrelevant variables. Since the
number of relevant variables is typically small, data-driven invariant inference can scale to such
tasks better. Second, we show that exhaustive enumeration can be replaced by learners that are
much more scalable. Instead of a generate-and-check approach where an enumerator generates all
possible candidate features eagerly [Albarghouthi et al. 2013; Ernst et al. 2000; Padhi et al. 2016], we
employ a more scalable guess-and-check approach where the learner intelligently guesses features
from data.

We have implemented these techniques in a tool Oasis1 that takes as input logic formulas
which encode the verification of safety properties of programs over integer variables and outputs
inductive invariants that are sufficient to prove the properties. To this end, Oasis employs new ML
algorithms for the well-known binary classification problem: the learner’s goal is to find a classifier
that separates positive and negative examples. The classifier is a predicate that includes the positive
examples and excludes the negative examples. In the context of invariant inference, an example is a
program state that maps variables to integers. Oasis makes the following contributions.

First, Oasis uses binary classification to infer relevant and irrelevant variables (Section 4.3). It
uses symbolic execution to generate reachable states (positive examples) and bad states (negative
examples), which are backward reachable from states that violate the safety properties. Then it
finds a sparse classifier and we classify the variables occurring in the classifier as relevant. If a
variable is absent from the classifier and it is possible to separate samples of reachable states from
bad states without using the variable then it is likely to be irrelevant to the invariant. The sparsity
requirement ensures that we keep the number of relevant variables to minimal.

We remark that we need a custom learner for this task. Each state (reachable or bad) is a partial
map from variables to integers. In particular, there are some variables that are not in the domain of
the state. These variables are don’t care, i.e., they can be assigned any value without affecting the
label (positive or negative) of the state. The (partial) models generated by SMT solvers typically
have don’t cares. The well-known ML classifiers learn over total maps as opposed to partial maps.
Although one can extend a partial map to a total map by setting the don’t care variables to zero or
randomly assigned values, these alternatives are undesirable as a partial map corresponds to an
infinite number of possible total maps and supplanting it with any total map loses the information

1 The name Oasis stands for Optimization And Search for Invariant Synthesis.

On Scaling Data-Driven Loop Invariant Inference

3

encoded in the partial map. Hence, we have designed a custom learner that directly learns a classifier
using partial maps and is not limited to total maps. After we obtain the set of relevant variables
from the classifier, Oasis calls a modified version of LoopInvGen where the synthesis of features
is restricted to predicates over the relevant variables.

Second, Oasis uses a learner to synthesize Boolean features from data (Section 4.4). Internally,
LoopInvGen breaks down the problem of invariant inference into many small binary classification
tasks and uses Escher [Albarghouthi et al. 2013] to find features that solve them. Specifically, Escher
exhaustively enumerates all features in increasing size till it finds one that separates the positive
examples from the negative examples in the small task. Oasis replaces Escher with a learner to
find such features. Unlike traditional ML algorithms that have non-zero error, i.e., they fail to
separate some positive examples from some negative examples, LoopInvGen requires the feature
synthesizer to have zero error.

Oasis uses the same learner to solve both these problems, i.e., inferring relevant variables and
inferring features. In particular, the learner of Oasis solves a non-standard ML problem: finding
sparse classifiers with zero error in the presence of don’t cares. We describe a novel learner that
solves this problem (Section 5). To the best of our knowledge, all prior works on data-driven
invariant inference use learners that require total maps. We show how to encode the problem of
finding such a classifier as an instance of integer linear programming (ILP) which minimizes an
objective function subject to linear constraints. Although linear programming has previously been
used to assist invariant inference [Gupta et al. 2013], our encoding is novel. Specifically, we show
how to systematically encode domain-specific heuristics as objective functions or constraints for
effective learning in the context of invariant inference. Heavily optimized ILP solvers are available
as off-the-shelf tools and Oasis uses them to scale data-driven invariant inference.

To demonstrate the scalability of Oasis in practice, we evaluate Oasis on over 400 benchmarks
from the invariant (Inv) track of the SyGuS competion held in 2019 [syg 5 14] (Section 6). This
benchmark set includes the new community provided programs that have a large number of
irrelevant variables which test the scalability of invariant synthesis tools [Si et al. 2018]. Our
evaluation shows that Oasis significantly improves the scalability of data-driven invariant inference
on these benchmarks and solves 20% more benchmarks than LoopInvGen, the state-of-the-art data-
driven invariant inference tool. Oasis even outperforms state-of-the-art invariant inference tools
that are based on very different techniques. It solves more benchmarks than deductive synthesis
implemented in CVC4 [Barrett et al. 2011; Reynolds et al. 2015] and cooperative synthesis of
the recent work of DryadSynth [Huang et al. 2020] that combines enumerative and deductive
synthesis. Thus, our evaluation shows that Oasis significantly improves the state-of-the-art in
data-driven invariant inference and makes it as scalable as deductive and cooperative techniques.
Oasis solves more benchmarks than these tools and also solves benchmarks that are beyond the
reach of prior work.

The rest of the paper is organized as follows. We provide an example to show the end-to-end
working of Oasis (Section 2) and review the relevant background (Section 3). We describe Oasis in
detail (Section 4) followed by the ILP-based learner (Section 5). We evaluate Oasis in Section 6,
place it in the context of the landscape of invariant inference techniques in Section 7, and conclude
with directions for future work in Section 8.

4

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

1: assume (k ≥ 0 ∧ n ≥ 0)
2: i = j = 0
3: while (i ≤ n) do
(i, j, y) ← (i + 1, j + 1, i × j)
4:
5: assert (i + j + k ≥ 2n ∨ y ≥ n2)

Fig. 1. The C-program of working example.

2 WORKING EXAMPLE

We use a simple (contrived) benchmark to show the working of each component of Oasis. The
goal is to synthesize an inductive invariant I ((cid:174)x), where (cid:174)x = ⟨i, j, k, n, y⟩ that satisfies the following
verification conditions (VCs) expressed as Horn Clauses.

Pre((cid:174)x) ⇒ I ((cid:174)x) with Pre((cid:174)x) ≜ i = j = 0 ∧ k ≥ 0 ∧ n ≥ 0
I ((cid:174)x) ∧ Trans((cid:174)x, (cid:174)x ′) ⇒ I ((cid:174)x ′) with Trans((cid:174)x, (cid:174)x ′) ≜ i ≤ n ∧ i ′ = i + 1 ∧ j ′ = j + 1 ∧ y ′ = i × j
I ((cid:174)x) ⇒ Post((cid:174)x) with Post((cid:174)x) ≜ i ≤ n ∨ i + j + k ≥ 2n ∨ y ≥ n2

These VCs encode the verification of the C-program in Figure 1. If there exists a predicate I that
satisifes the VCs then for all possible inputs the assertion can never be violated. A state for this
example is a 5-tuple that maps i, j, k, n, y to integers or don’t cares (denoted by ⊤).

The first step is the identification of irrelevant variables. Oasis generates reachable states,
i.e., positive examples by computing satisfying assignments of Pre((cid:174)x) and Pre((cid:174)x) ∧ Trans((cid:174)x, (cid:174)x ′).
For bad states, i.e., negative examples, Oasis computes satisfying assignments of ¬Post((cid:174)x) and
¬Post((cid:174)x ′) ∧ Trans((cid:174)x, (cid:174)x ′). These satisfying assignments are obtained from off-the-shelf SMT solvers
and result in Table 1.

i

0
2
1
6

j

0
3
-1
4

k n

y

ℓ

0
0
0
0

0 ⊤ 1
1
2
1
0
-1
0
0
15
5

Table 1. Initial symbolic execution data.

i

0
2

j

0
3

k

-2
-3

n

-1
1

y

0
0

ℓ

0
0

Table 2. Additional data from robustness checking.

i

1
0
-2
-3

j

1
0
-2
-3

k

742
0
0
1

n

0
859
-2
-3

ℓ

1
1
0
0

i

0
1
5
5
6

j

0
1
15
0
1

k

21
115
0
1
0

n

0
38
5
4
4

ℓ

1
1
1
0
0

i

0
1
373

j

0
1
374

k

21
115
-3

n

0
38
372

ℓ

1
1
0

Table 3. Classification problems generated by LoopInvGen.

Here, the label ℓ = 1 corresponds to positive examples and ℓ = 0 corresponds to negative
examples. Our learner outputs i ≤ j as the classifier for the binary classification problem in Table 1.
We run LoopInvGen with (cid:174)r = {i, j} as relevant variables and the rest of the variables marked

On Scaling Data-Driven Loop Invariant Inference

5

irrelevant. Note that this set of relevant variables is incorrect and this instance of LoopInvGen will
fail. In parallel to LoopInvGen, we continue improving our set of relevant variables.

The predicate i ≤ j separates the positives from the negatives: it includes, i.e., is true for all
positive examples and excludes, i.e., is false for all negative examples in the data. Ideally, we want
the classifier to generalize well: it should not happen that if we generate few more examples then the
classifier can no longer separate the positives from the negatives. Next, we check for the robustness
of this separator by checking for existence of positive states that it excludes or negative states that
it includes. The former are generated via satisfying assignments of Pre((cid:174)x) ∧ Trans((cid:174)x, (cid:174)x ′) ∧ i ′ > j ′
and the latter from ¬Post((cid:174)x ′) ∧ Trans((cid:174)x, (cid:174)x ′) ∧ i ≤ j and are shown in Table 2. Note that no new
positive examples are added in this step as the former predicate is unsatisfiable.

Next, we use the learner to find a classifier using the data in Table 1 and Table 2. We repeat these
steps till an instance of LoopInvGen succeeds. Here, these iterations end at i ≤ j + k ∧ i ≤ n + 1
which labels i, j, k, n as relevant and y as irrelevant. Note that any syntactic slicing-based technique
would mark y as relevant but the semantic data guides our learner to determine the irrelevance of
y. Next, we show how LoopInvGen (with our improvements) successfully infers an invariant I
with this set of relevant variables.

LoopInvGen breaks down the process of finding I into two steps. First, it creates many small
binary classification problems. For each such problem, a feature synthesizer generates a feature
that separates the positives from the negatives. Second, the features are combined together using a
Boolean function learner to generate a candidate invariant. LoopInvGen repeats these steps till
a predicate that satisfies all the VCs is discovered. For our example, LoopInvGen generates the
classification problems in Table 3 (See [Padhi et al. 2016] for how LoopInvGen generates these
problems). Our contribution lies in using our learner to find features for each of these problems
rather than using LoopInvGen’s exhaustive enumeration based feature synthesizer. Here, our
learner generates the following features for these three problems: i ≥ 0, i ≤ j, and k ≥ 0. The
Boolean function learner combines these features to generate the following candidate invariant
i ≥ 0 ∧ i ≤ j ∧ k ≥ 0 that satisfies all the VCs. This inductive invariant shows that the assertion in
Figure 1 holds for all possible inputs.

3 BACKGROUND

In this section, we formally define the problem of verifying correctness of programs using loop
invariants, describe how invariant inference can be considered as a binary classification problem,
and then describe how LoopInvGen reduces this classification problem to many small binary
classification problems.

3.1 Program Verification and loop invariants
The first step in program verification is defining a specification for the desired property. Typi-
cally [web 5 23b,c] this is provided as a pair of logical formulas — (a) a precondition that constrains
the initial state of the program, and (b) a postcondition that validates the final state after execution
of the program. Many programming languages support the assume and assert keywords, where
assume(ϕ) silently halts executions that satisfy ¬ϕ and executing assert(ϕ) with a state that satis-
fies ¬ϕ raises an exception. For example, Figure 1 shows a program having a loop where the initial
values are specified by initializations/assume statements and the postcondition is specified using
the assert in the last line. Given such a specification, we define the verification problem as:

Definition 3.1 (Program Verification). Given a program P and a specification consisting of a pair
of formulas — a precondition ρ and a postcondition ϕ, the verification problem is to prove that for
all executions starting from states that satisfy ρ, the states obtained after executing P satisfy ϕ.

6

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

In Floyd-Hoare logic (FHL) [Floyd 1967; Hoare 1969], this problem is abbreviated to the formula
{ρ} P {ϕ}, called a Hoare triple. We say that a Hoare triple is valid if the correctness of P can be prov-
ably demonstrated. For example, while {x < 0} y ← - x {y > 0} is valid, {x < 0} y ← x + 1 {y < 0}
is not. FHL offers initial theoretical underpinnings for automatic verification by providing a set of
inference rules that can be used on the program structure. Today, state-of-the-art verification tools
have mechanized these rules and apply them automatically.

However, the FHL inference rules can automatically be applied only for validating Hoare triples
that are defined on loop-free programs. Applying these rules on a loop requires an additional
parameter called a loop invariant — a predicate over the program state that is preserved across each
iteration of the loop. To establish the validity of a Hoare triple, the FHL require a loop invariant
to satisfy three specific properties, and a predicate that satisfies all three is called a sufficient loop
invariant.

Definition 3.2 (Sufficient Loop Invariant). Consider a simple loop, while G do S, which executes
the statement S until the condition (loop guard) G holds and then it halts. Then, for the Hoare triple
{ρ} while G do S {ϕ} to be valid, there must exist a predicate I that satisfies:

VCpre :

VCind :

VCpost :

ρ =⇒ I, i.e., I must hold immediately before the loop
{G ∧ I} S {I}, i.e. I must be inductive (hold after each iteration)
I =⇒ G ∨ ϕ, i.e., I must certify the postcondition upon exiting the loop

These three properties are called the verification conditions (VCs) for the loop. Any predicate I
that satisfies the first two VCs is called a loop invariant. A loop invariant that also satisfies the
third VCs said to be sufficient (for proving the correctness of the Hoare triple). In this paper, we use
invariants to denote sufficient loop invariants for brevity.

Thanks to efficient theorem provers [Barrett et al. 2011; de Moura and Bjørner 2008], today it
is possible to automatically check if a given predicate is indeed an invariant. However, automati-
cally finding an invariant for arbitrary loops is undecidable in general, and even small loops are
challenging for state-of-the-art tools. The invariant inference track of the syntax guided synthesis
competition has hundreds of benchmarks where each benchmark provides a VCpre, a VCind, and a
VCpost as logical formulas. Different tools compete to solve these problems, i.e., to infer the invariants
every year.

3.2 Data-Drive Invariant Inference

An invariant can be viewed as a zero-error classifier — it should demonstrate that the set of possible
reachable states at the entry to a loop (called loop-head states) are disjoint from the bad states that
violate the postcondition; thus establishing that the postcondition is satisfied for all executions.

Consider verifying our motivating example from Figure 1. We visualize the classification problem
in Figure 2. VCpre and VCind from Definition 3.2 require I (dashed blue ellipse) to capture all
possible loop-head states (cyan dots). These include states satisfying the precondition ρ (green
circle), e.g., (i = j = 0, n = 2) appearing before the first iteration, and the subsequent states after
each iteration (indicated by the arrows), e.g., (i = j = 1, n = 2, y = 0), (i = j = 2, n = 2, y = 1) etc.
The ¬G ∧ ¬ϕ space (red rectangle) denotes the states violating the postcondition, e.g., (i = j =
2, k = 0, n = 1, y = −1). VCpost forces I to be disjoint with this space. An invariant I that satisfies
the VCs guarantees that no execution starting from ρ would terminate at a state that violates the
desired postcondition ϕ.

To infer invariants, we can label examples of loop-head states as positive and satisfying assign-
ments of ¬G ∧ ¬ϕ as negative and use a classification algorithm to separate these. The output
classifier is a candidate invariant. If the candidate satisfies all the VCs then we have succeeded

On Scaling Data-Driven Loop Invariant Inference

7

–

–

–

–

–

–

–

–

–

¬ G ∧ ¬ ϕ
–

ρ

–

–

–

I

–

Fig. 2. A sufficient loop invariant can be viewed as a classifier for states.

in inferring an invariant. If some VC is violated then SMT solvers can produce counterexamples
which can be added to positive or negative examples to generate another candidate. Since the
actual invariant can be complex, prior work has explored increasingly complex learning algorithms
including support vector machines [Li et al. 2017; Sharma et al. 2012], decision trees [Garg et al. 2016;
Zhu et al. 2018], algorithms for learning Boolean combinations of half-spaces [Sankaranarayanan
et al. 2006], Metropolis-Hastings sampling [Sharma and Aiken 2016], Gibbs sampling [Gulwani and
Jojic 2007], SMT-based constraint solving [Garg et al. 2014], and, finally, neural networks [Ryan
et al. 2020; Si et al. 2018]. An alternative approach was proposed by [Padhi et al. 2016] where this
classification problem is decomposed into smaller more tractable classification problems that can
be solved by simple learning algorithms. This approach is implemented in the tool LoopInvGen
that Oasis builds upon.

3.3 LoopInvGen

LoopInvGen [Padhi et al. 2016] is a state-of-the-art data-driven invariant inference tool. It consists
of a learner and a teacher that interact with each other. The teacher has access to an SMT solver and
can verify loop-free programs. In particular, given a candidate invariant I generated by the learner,
it can check the VCs and if some VC fails then it returns a program state as a counterexample.
LoopInvGen uses a multi-stage learning technique that composes the candidate invariant out of
several predicates, known as features, learned over smaller subproblems. Algorithm 1 outlines this
framework.

The main Infer procedure is invoked with a Hoare triple L ≡ {ρ} while G do S {ϕ}, and a set
P of reachable program states. Here, we assume the loop-body S to be loop-free2. The program
states are sampled at random by running the loop for a few iterations [Duran and Ntafos 1981]. All
the states that LoopInvGen deals with are total maps that map all variables to some integers. The
Check(B) procedure is a call to the teacher that invokes an SMT solver to check if B is valid. If B is
valid the call returns ⊥ otherwise it returns a (complete) satisfying assignment of ¬B.

Line 2 performs a sanity check: if ρ ∧ ¬G ∧ ¬ϕ is satisfiable then the input Hoare triple is invalid
and no invariant exists. LoopInvGen starts with a weak candidate invariant I ≡ (¬ G =⇒ ϕ),
and iteratively strengthens it (line 17) for inductiveness. These choices ensures that all candidate
invariants I satisfy VCpost. Lines 5 and 15 additionally check for VCpre and VCind respectively, and
add appropriate counterexamples. While a violation of VCpre adds a positive example, a violation
of VCind adds a negative example. Since the loop body S is loop free, VCind can be encoded as an

2

LoopInvGen does handle multiple and nested loops [Padhi et al. 2016].

8

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Algorithm 1 The LoopInvGen algorithm [Padhi et al. 2016]. The teacher is Check and the learner
is Learn.

function Infer({ρ} while G do S {ϕ}, P)

if Check(ρ =⇒ (G ∨ ϕ)) (cid:44) ⊥ then return False
I ← (¬ G =⇒ ϕ)
while True do

c ← Check(ρ =⇒ I)
if c (cid:44) ⊥ then return Infer({ρ} while G do S {ϕ} , P ∪ {c})
N ← {}
while True do
F ← {}
while True do

(P, N ) ← Conflict(P, N, F )
if P = N = {} then break
else F ← F ∪ Learn(P, N )

δ ← BoolCombine(F )
c ← Check({δ ∧ G ∧ I} S {I})
if c (cid:44) ⊥ then N ← N ∪ {c}

I ← (I ∧ δ )
if δ = True then return I

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

SMT formula (through a weakest precondition computation) whose validity ensures the validity of
VCind. The lines 10 – 14 indicates the key learning subcomponents.

In line 11, the Conflict procedure selects two sets P ⊆ P and N ⊆ N that are conflicting, i.e.,
these positive and negative examples are indistinguishable modulo F , the set of current features.
That is for all features f ∈ F .∀x, y ∈ P ∪ N .f (x) = f (y). For such P and N , line 13 learns a feature
that separates P and N by invoking the learner Learn. In LoopInvGen, the learner is implemented
using Escher [Albarghouthi et al. 2013] that exhaustively enumerates all predicates over all variables
in increasing size till it finds a feature f that separates P and N , and this f is added to F . The loop
in lines 10–13 has the following postcondition: ∀x ∈ P.∀y ∈ N .∃f ∈ F .f (x) (cid:44) f (y), i.e., for every
positive example x and every negative example y, there is a feature f that separates x and y. Once
F has enough features, line 14 uses a standard Boolean-function learner [Mitchell et al. 1997; Padhi
et al. 2016] BoolCombine to learn δ , a Boolean combination of these features, that separates P
and N . Then LoopInvGen logically strengthens the candidate invariant I by conjoining it with
δ . For more details on this framework, we refer to the LoopInvGen paper [Padhi et al. 2016]. In
particular, [Padhi et al. 2016] shows that breaking the binary classification problem of separating P
and N by a candidate invariant into the two step approach of first inferring features that separate
P ⊆ P and N ⊆ N and then combining the features is an effective approach to invariant inference.
The features are usually much simpler than the invariants, which makes inferring features much
more tractable than inferring candidate invariants.

Next, we discuss our contributions: the inference of relevant variables and the changes Oasis

makes to LoopInvGen followed by our ILP-based learning (Section 5).

4 OASIS FRAMEWORK
In this section, we overview our approach for accelerating invariant inference using a set of relevant
variables. First, we define the state space for programs and describe our encoding of the verification
conditions described in Definition 3.2. We then describe the notion of relevant variables for a

On Scaling Data-Driven Loop Invariant Inference

9

program verification problem, and present our approach for inferring sufficient loop invariants
using these relevant variables.

4.1 Notation
Given a program P we write (cid:174)x P, to denote the sequence ⟨x1, . . . , xn⟩ of variables appearing in it.
We omit the subscript P and simply write (cid:174)x when the program is clear from context. A program
state for P, denoted (cid:174)σ = ⟨v1, . . . , vn⟩, is a sequence of values assigned to the program variables —
any subset of these values may be irrelevant (denoted ⊤). A program state (cid:174)σ is said to be total if it
does not contain ⊤, and is said to be partial otherwise. Finally, we use the shorthand ((cid:174)x (cid:55)→ (cid:174)σ ) to
denote the value assignment predicate (x1 = v1 ∧ · · · ∧ xn = vn), where irrelevant values (⊤) are
simply dropped, e.g., (⟨x1, x2, x3⟩ (cid:55)→ ⟨v1, ⊤, v3⟩) ≡ (x1 = v1 ∧ x3 = v3).

Although, the techniques described in this work can be easily extended to programs containing
multiple and nested loop, for simplicity, we consider verifying our single-loop program from the
previous section: L ≡ {ρ} while G do S {ϕ}. We formally model the loop in our program P as a
transition relation trans P over program states. Two states (cid:174)σ1 and (cid:174)σ2 are related by trans P iff a
single iteration of the loop body (S) transitions the state (cid:174)σ1 to (cid:174)σ2. We need trans P to be a relation
as programs can have non-determinism. Formally,

trans P((cid:174)σ1, (cid:174)σ2) ⇐⇒ (cid:8)G ∧ ((cid:174)x (cid:55)→ (cid:174)σ1)(cid:9) S (cid:8)((cid:174)x (cid:55)→ (cid:174)σ2)(cid:9)

Similarly, we model the precondition and postcondition as unary predicates on program states:

pre P((cid:174)σ ) ⇐⇒ ((cid:174)x (cid:55)→ (cid:174)σ ) ∧ ρ

postP((cid:174)σ ) ⇐⇒ ((cid:174)x (cid:55)→ (cid:174)σ ) ∧ ¬G =⇒ ϕ

We omit the subscript P and simply write pre , trans and post when the program P is clear from
context. These relations together define a program verification (or equivalently, a sufficient loop
invariant inference) problem. Indeed, this encoding of program verification problems is a commonly
used language-agnostic intermediate representation. Moreover, existing program analysis tools can
automatically generate these relations from high-level programs and their formal specifications.
This representation facilitates the use of off-the-shelf SMT solvers.

A sufficient loop invariant that establishes the correctness of the Hoare triple L is also a predi-
cate defined over states of the program P. Such an invariant is required to satisfy the following
verification conditions from Definition 3.1 in terms of the pre , trans and post relations above:
∀(cid:174)σ . I((cid:174)σ ) =⇒ post((cid:174)σ )
∀(cid:174)σ , (cid:174)σ ′. I((cid:174)σ ) ∧ trans ((cid:174)σ , (cid:174)σ ′) =⇒ I((cid:174)σ ′)
(VCpost)
(VCind)

∀(cid:174)σ . pre ((cid:174)σ ) =⇒ I((cid:174)σ )
(VCpre)

Example 4.1. Consider again our motivating example from Figure 1 where (cid:174)x = ⟨i, j, k, n, y⟩. We
y ⟩ of values respectively.

use (cid:174)σ and (cid:174)σ ′ to denote the tuples ⟨vi , vj , vk , vn, vy ⟩ and ⟨v ′
The following pre , trans and post relations encode the verification problem:

k , v ′

n, v ′

i , v ′

j , v ′

pre ((cid:174)σ ) ≜ (vi = vj = 0) ∧ (vk ≥ 0) ∧ (vn ≥ 0)

trans ((cid:174)σ , (cid:174)σ ′) ≜ (v ′
k

= vi + 1 ∧ v ′
= vn) ∧
j
n)(cid:1)
post((cid:174)σ ) ≜ ¬(vi ≤ vn) =⇒ (cid:0)(vi + vj + vk ≥ 2 · vn) ∨ (vy ≥ v2

= vk ) ∧ (v ′
n

(vi ≤ vn) =⇒ (cid:0)v ′

i

(cid:16)

= vj + 1 ∧ v ′
y

= vi · vj

(cid:1)(cid:17)

4.2 Relevant Variables

Scalability is a major challenge for existing data-driven invariant inference techniques. As number
of variables increases the performance of these techniques degrades rapidly, although in many cases
a sufficient invariant for verifying these programs contains only a small number of variables. We
propose a novel technique that first identifies a small subset of variables over which a sufficient loop

10

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Algorithm 2 Oasis framework for scaling loop invariant inference

function Oasis(⟨pre , trans , post⟩ : Verification Problem, (cid:174)σ+ : States, (cid:174)σ− : States)

1

2

3

4

5

6

7

8

9

10

11

12

13

Classifier C ← Learn( (cid:174)σ+, (cid:174)σ−)
if C = ⊥ then return ⊥
Variables (cid:174)r ← FilterVariables(C)
do parallel

in thread 1 do

(cid:174)σ ← FindPosCounterExample(⟨pre , trans , post⟩, C)
if (cid:174)σ (cid:44) ⊥ then return Oasis(⟨pre , trans , post⟩, (cid:174)σ+ ∪ { (cid:174)σ }, (cid:174)σ−)

in thread 2 do

(cid:174)σ ← FindNegCounterExample(⟨pre , trans , post⟩, C)
if (cid:174)σ (cid:44) ⊥ then return Oasis(⟨pre , trans , post⟩, (cid:174)σ+, (cid:174)σ− ∪ { (cid:174)σ })

in thread 3 do

I ← RelInfer(⟨pre , trans , post⟩, (cid:174)σ+, (cid:174)σ−, (cid:174)r ) (cid:12)
if I (cid:44) ⊥ then return I

(cid:12) timeout = τ

invariant is likely to exist. Then, it simultaneously refines this subset and searches for a sufficient
invariant till one is found.

Our core framework, called Oasis, is outlined in Algorithm 2. Oasis accepts the standard set
of arguments for a data-driven verification technique (discussed in Section 3.3) — a verification
problem (encoded as a triple ⟨pre , trans , post⟩), and some sampled positive ( (cid:174)σ+) and negative
( (cid:174)σ−) program states typically sampled randomly. We first invoke the Learn function with these
sampled states to learn a predicate C that separates (cid:174)σ+ and (cid:174)σ−, i.e.,

(cid:0)∀(cid:174)σ ∈ (cid:174)σ+. C((cid:174)σ )(cid:1) ∧ (cid:0)∀(cid:174)σ ∈ (cid:174)σ−. ¬C((cid:174)σ )(cid:1)

We detail the Learn function in Section 5, which utilizes machine-learning techniques to efficiently
find a sparse separator for (cid:174)σ+ and (cid:174)σ−. In line 3 we drop irrelevant variables, those that do not affect
the prediction of the classifier over (cid:174)σ+ ∪ (cid:174)σ−, and consider the remaining variables (cid:174)r ⊆ (cid:174)x to be a
candidate set of relevant variables. In Section 2 we show some examples of classification problems,
the learned classifiers and relevant variables.

After a set (cid:174)r of relevant variables is identified, in lines 3 – 12, we try to refine the set of relevant
variables and find a sufficient invariant over them in parallel. In particular, we execute the following
three threads in parallel:

(1) one that attempts to find a positive state misclassified by the classifier
(2) one that attempts to find a negative state misclassified by the classifier
(3) one that runs invariant inference using the currently identified relevant variables

Schohn and Cohn [2000] showed that classifiers can be refined by sampling near the classification
boundary. This idea is used in Li et al. [2017], which showed that when compared to random
sampling, active learning improves the quality of sampled program states and accelerates the search
for sufficient invariants. Threads 1 and 2 are responsible for an active-learning-based refinement of
the relevant variables set, and thread 3 attempts to find a sufficient invariant over these variables, if
there exists one. Next, we detail our active learning strategy (Section 4.3) and our relevance-aware
invariant inference algorithm RelInfer (Section 4.4). Note that the RelInfer thread is run with a
timeout of τ so that long-running inference threads are automatically cleaned up as we spin up
more threads with refined sets of relevant variables.

On Scaling Data-Driven Loop Invariant Inference

11

Algorithm 3 Procedures for refinement of candidate relevant variables

function FindPosCounterExample(⟨pre , trans , post⟩ : Verification Problem, C : Predicate)

for k = 0 to ∞ do

Predicate Reachable(k) ≜ pre ((cid:174)σ0) ∧ trans ((cid:174)σ0, (cid:174)σ1) ∧ · · · ∧ trans ((cid:174)σk −1, (cid:174)σk )
Counterexample c ← Check(∀(cid:174)σ0, . . . , (cid:174)σk . Reachable(k) =⇒ C((cid:174)σk ))
if c (cid:44) ⊥ then return c[(cid:174)σk ]

function FindNegCounterExample(⟨pre , trans , post⟩ : Verification Problem, C : Predicate)

for k = 0 to ∞ do

Predicate Bad(k) ≜ ¬post((cid:174)σk ) ∧ trans ((cid:174)σk −1, (cid:174)σk ) ∧ · · · ∧ trans ((cid:174)σ0, (cid:174)σ1)
Counterexample c ← Check(∀(cid:174)σ0, . . . , (cid:174)σk . Bad(k) =⇒ ¬C((cid:174)σ0))
if c (cid:44) ⊥ then return c[(cid:174)σ0]

1

2

3

4

5

6

7

8

4.3 Refining Relevant Variables

We now detail our procedures for refining a set of relevant variables. The FindPosCounterExample
and FindNegCounterExample procedures, which run in threads 1 and 2 respectively, are outlined
in Algorithm 3. Each of these procedures returns a program state that is misclassified by the current
classifier C, which is then used to learn a new classifier, and thus a new set of relevant variables.
The FindPosCounterExample procedure identifies positive misclassifications — a reachable
program state (cid:174)σ that the classifier labels as a negative state, i.e., ¬C((cid:174)σ ). To identify such states, we
gradually expand the frontier of reachable states starting from the precondition pre and then re-
peatedly applying the transition relation trans . In line 2, we construct the predicate Reachable(k)
that captures all states that are reachable in exactly k applications of the transition relation, i.e., k
iterations of the loop. In line 3, we check if all such states are subsumed by the current classifier.
Upon finding a counterexample, in line 4, we return the misclassified state.

The FindNegCounterExample procedure works in a very similar manner and identifies negative
misclassifications — a bad program state (cid:174)σ (one that would lead to violation of the final assertion)
that the classifier labels as a positive state, i.e., C((cid:174)σ ). To identify such states, we gradually expand
the frontier of known bad states starting from those that violate the postcondition postand then
repeatedly reversing the transition relation trans . In line 6, we construct the predicate Bad(k)
that captures all states that lead to state to an assertion violation in exactly k applications of the
transition relation, i.e., k iterations of the loop. In line 7, we check if all such states are excluded by
the current classifier. Upon finding a counterexample, in line 8, we return the misclassified state.
Although these procedures can be computationally expensive, our implementation caches the
results of intermediate queries for reuse. In particular, unsatisfiable paths are generated at most
once.

4.4 Invariant Inference with Relevant Variables

Once we have a set of relevant variables from the learned classifier, we run our invariant inference
algorithm (in thread 3) with these variables together with all the positive states ( (cid:174)σ+) and negative
states ( (cid:174)σ−) sampled so far. In Algorithm 4 we outline this algorithm. They key difference with
respect to Algorithm 1 is the use of (cid:174)r — the set of relevant variables. While Algorithm 1 learns
features over all variables (cid:174)x in the program, Algorithm 4 only learns features over (cid:174)r , which is
provided to the Learn procedure in line 11. In the next section, we detail this relevance-aware
learning procedure.

12

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Algorithm 4 A loop invariant inference algorithm that utilizes relevant variable information

function RelInfer(⟨pre , trans , post⟩ : Verification Problem, (cid:174)σ+ : States, (cid:174)σ− : States, (cid:174)r : Variables)

if Check(∀(cid:174)σ . pre ((cid:174)σ ) =⇒ post((cid:174)σ )) (cid:44) ⊥ then throw “No Solution!”
Predicate I ← post
while True do

if Check(∀(cid:174)σ , (cid:174)σ ′. I((cid:174)σ ) ∧ trans ((cid:174)σ , (cid:174)σ ′) =⇒ I((cid:174)σ ′)) = ⊥ then return I
States P, N ← (cid:174)σ+, (cid:174)σ−
while True do

Features F ← {}
while True do

States (P, N ) ← Conflict(P, N, F )
if P = N = {} then break
else F ← F ∪ Learn(P, N , (cid:174)r )
Predicate δ ← BoolCombine(F )
Counterexample c ← Check(∀(cid:174)σ , (cid:174)σ ′. δ ((cid:174)σ ) ∧ I((cid:174)σ ) ∧ trans ((cid:174)σ , (cid:174)σ ′) =⇒ I((cid:174)σ ′))
if c = ⊥ then break
N ← N ∪ {c[(cid:174)σ ]}

I ← (I ∧ δ )
Counterexample (cid:174)σ ← ∀(cid:174)σ . pre ((cid:174)σ ) =⇒ I((cid:174)σ )
if (cid:174)σ (cid:44) ⊥ then
I ← post
(cid:174)σ+ ← (cid:174)σ+ ∪ { (cid:174)σ }

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

5 CLASSIFIER LEARNING

In this section, we formulate the problem of generating a classifier that separates positive program
states from negative program states. By default, the output classifier predicate can use any of the
program variables. If we restrict the classifier to use only a subset (cid:174)r of variables (e.g., the call to
Learn procedure in line 11 of Algorithm 4) then we first project the examples to (cid:174)r and then learn a
classifier over the projected states. Let x denote a vector of program variables that can occur in
the classifier. In this section, we use the standard notation that bold letters denote vectors (e.g.,
0 is a vector of all zeros). We model the problem of inferring a classifier h : Z|x| → {True, False}
as a search problem over the following class of CNF predicates with C denoting the number of
conjuncts and D the number of disjuncts in each conjunct:

HCNF =

(cid:26) (cid:219)

(cid:220)

c ∈[C]

d ∈[D]

⟨wcd , x⟩ + bcd > 0

(cid:27)

.

(1)

where b ∈ Z and ⟨w, x⟩ + b is an inner product between a vector w ∈ Z|x | and x. We use [n] to
denote the list {0, 1, . . . , n − 1}.

Given a set of program states with corresponding labels, our task is to find a classifier h ∈
HCNF such that (a) it separates the positive states from the negative states, and (b) it generalizes
to unseen program states The first part is a search question, whereas the second part suggests
learning to choose simple and natural predicates. Note that the class of invariants HCNF is very
powerful – one can trivially fit any given set of examples. We make the following observations. The
search problem becomes meaningful on a given set of program states, if we restrict the predicate
sizes (i.e., C and D) to be small. Furthermore, the coefficients are often bounded by the constants
occurring in the program. Finally, and most importantly, we are not dealing with arbitrary predicate
formulas, but ones that have a nice conjunction-of-disjunctions structure. These observations

On Scaling Data-Driven Loop Invariant Inference

13

enable reformulating the search problem as an integer-linear programming (ILP) problem that can
be efficiently solved in practice for our benchmarks by off-the-shelf ILP solvers.

Consider the search problem (1) above: formally, we want to find a predicate h ∈ HCNF that
accurately classifies a given set of labeled program states {σn, yn }N
n=1, where yn ∈ {0, 1}. It is
convenient to think of h as a tree of depth 3: the program variables form the input layer to the
linear inequalities, which are grouped by (cid:212) operators to yield disjunctive predicates. The root node
is the (cid:211) operator that represents conjunction of the predicates represented by the second layer.
The reduction of the search problem to ILP is given as follows.

(Input layer: linear inequalities) Write zncd = q {⟨wcd , σn⟩ + bcd > 0} ∈ {0, 1}, where the
indicator function q {p} of a predicate p maps True to 1 and False to 0. This is captured by the
following constraints, for a sufficiently large integer M:

∀n ∈ [N ], c ∈ [C], d ∈ [D], −M(1 − zncd ) < ⟨wcd , σn⟩ + bcd ≤ Mzncd ,
wcd ∈ Z |x | , bcd ∈ Z , zncd ∈ {0, 1}.

(2)
(Middle layer: Disjunctions) Note that the value of the c-th conjunct on a given input σn
(cid:9). This is captured

corresponds to summing zncd = q {pncd } over d, i.e., write y∨
nc
by the constraint:

d ∈[D] pncd

= q (cid:8)(cid:212)

∀n ∈ [N ], c ∈ [C], −M(1 − y∨

nc ) <

(cid:213)

d ∈[D]

zncd ≤ My∨
nc ,

(3)
(Final layer: Conjunction) The predicted label on a given input state is given by a conjunction
of the above disjunctions. Requiring that the predicted label match the observed label for each
example is equivalent to the following constraints:

y∨
nc ∈ {0, 1}.

for n ∈ [N ] s.t. yn = 1,

for n ∈ [N ] s.t. yn = 0,

(cid:213)

c ∈[C]
(cid:213)

c ∈[C]

y∨
nc ≥ C ,

y∨
nc ≤ C − 1 .

(4)

The search problem can now be stated as the ILP problem: find a feasible integral solution
{z, y∨, w, b} subject to the constraints Equations (2) to (4) combined. Note that the problem formulation
naturally handles partial states — if (σn)j is ⊤ then (wcd )j is set to zero. Equation (2) is applied
over only the variables that don’t map to ⊤. This ILP satisfies the following properties, the proofs
of which are straightforward and presented below for completeness. We abuse notation by using 0
(resp. 1) and False (resp. True) interchangeably.

= (cid:212)

Theorem 5.1. Any feasible solution to the ILP problem (5) is a member of HCNF.
Proof. Let {z, y∨, w, b} denote a feasible solution. First, note that for any fixed c, y∨
nc
d ∈[D] zncd = 0 because the conditions (3) hold. As y∨

(cid:205)
that y∨
nc
conditions (4) hold. So, we have yn = (cid:211)
nc . Finally, notice that for any n, c, d, zncd = 1 iff
y∨
c ∈[C]
⟨wcd , σn⟩ +bcd > 0 because the conditions (2) hold. In other words, zncd = q {⟨wcd , σn⟩ + bcd > 0}.
= (cid:211)
Putting these together, we have, yn = (cid:211)
□
c ∈[C]
c ∈[C]

= 0 iff
nc ∈ {0, 1} and zncd ∈ {0, 1}, it follows
= 1 because the

zncd , for each c. Next, it is immediate that yn = 1 iff every y∨
nc

⟨wcd , σn⟩ + bcd > 0 ∈ HCNF.

(cid:212)
d ∈[D]

y∨
nc

d ∈[D]

14

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Theorem 5.2. Given a set of labeled program states {σn, yn }, n ∈ [N ], if there is a h ∈ HCNF s.t.

h(σn) = yn for all n ∈ [N ], then the ILP problem (5) has at least one feasible solution.

Proof. This direction is easier to show. We can read off the integral coefficients w and b for all
the polynomials from h, and obtain zncd = q {⟨wcd , σn⟩ + bcd > 0} so that (2) hold. Then assign
y∨ variables as in the proof of Claim 5.1, so that (3) hold. Finally, because h(σn) = yn holds for all
□
n ∈ [N ], it follows that (4) also hold. We have a feasible solution.

Now, consider the problem of learning generalizable predicates (2). To this end, we follow the
Occam’s razor principle – seeking predicates that are “simple” and hence generalize better [Al-
barghouthi and McMillan 2013]. Simplicity in our case can be characterized by the size of the
predicate clauses and the magnitude of the coefficients. One way to achieve this is by constraining
the L1-norm of the coefficients w = [w1, . . . , wn], i.e., by minimizing (cid:205)
i ∈[n] |wi |. Note that L1-norm
can be expressed using linear constraints:3 ∥w∥1 = ⟨1, w+ + w−⟩, where w+ ≥ 0 and w− ≥ 0
(componentwise inequality) such that w = w+ − w−.

However, focusing only on the magnitude may lead to poor solutions. For example, consider
the Hoare triple: {n ≥ 0 ∧ x = n ∧ y = 0} while (x > 0) do {s ← y++; x--;} {y = n}. Here,
it is easy to verify that the loop invariant x + y = n is sufficient to assert VCpost. The equivalent
predicate in HCNF, x +y − n ≥ 0 ∧ n − x −y ≥ 0, however has a larger L1-norm though the invariant
is a simple equality. So, simply minimizing the L1-norm is not sufficient. Existing solvers [Padhi
et al. 2019, 2016] employ heuristics such as preferring equality to inequality. We handle this by
explicitly penalizing the inclusion of variables in the solution by using a penalty µ where µj = 0
iff ∀c ∈ [C].∀d ∈ [D]. (wcd )j
= 0. Intuitively, the more the number of variables with non-zero
coefficients in the classifier, the more the penalty. Our final objective function combines both µ and
L1-norm penalties:

min
w,w+,w−,b,z,y ∨, µ

(cid:213)

⟨1, w

+
cd

+ w−

cd ⟩ + λ⟨1, µ⟩

c ∈[C],d ∈[D]

subject to Equations (2) to (4), and
+ w−

1 − M(1 − µ) ≤

(cid:213)

w

+
cd

cd ≤ Mµ,

c ∈[C],d ∈[D]

∀c ∈ [C], d ∈ [D], wcd = w
cd ≥ 0 , w−
w

cd − w−
cd ,
cd ≥ 0 , µ ∈ {0, 1} |x|.

+

+

(5)

The key advantage of the above ILP formulation is that it can be solved optimally by off-the-shelf
solvers that leverage continuous and integer optimization techniques to solve such problems. This
enables efficient and scalable search compared to enumerative techniques.

We now formally give the implementation of Learn procedure in Figure 3. GenerateExpression
(in Step 3) executed on the solution {z, y∨, w, b} to the ILP problem Equation (5) outputs the
expression (cid:211)
c ∈[C]

⟨wcd , x⟩ + bcd > 0.

(cid:212)
d ∈[D]

3

Integrality constraints on w+, w− aren’t needed, so the problem as stated is technically a mixed ILP.

On Scaling Data-Driven Loop Invariant Inference

15

function Learn(data)

1

2

3

{z, y∨, w, b} ← Solve Equation (5) with labeled program states {σn, yn }N
expr ← GenerateExpression({z, y∨, w, b})
return expr

▷ returns a CNF expression over vars

n=1 from data

Fig. 3. Implementation of Learn procedure using the ILP formulation.

In practice, it suffices to restrict w and b to a small set of integers in Equation (2), and M to be
a very large integer. In the evaluation below, we use only two disjuncts4, i.e., C = 1 and D = 2
in Equation (5), constrain the coefficients to integers within [−1000, 1000] and use M = 100, 000
in Equation (5). In Figure 4, we show the constraints generated by our ILP formulation for the
Hoare triple {x = y = 0} while (x ≥ 0) do {x ← x + y} {False} from Gulavani et al. [2006].
We use the data in Table 4 to list our constraints.

x

y

ℓ

0
1
0
-1 ⊤ 0
Table 4. Execution data.

Constraints for Equation (2)

−M(1 − z111) < 0 ∗ w111 + 0 ∗ w112 ≤ z111
−M(1 − z121) < 0 ∗ w122 + 0 ∗ w122 ≤ z112
−M(1 − z111) < −1 ∗ w112 ≤ z211
−M(1 − z121) < −1 ∗ w111 ≤ z212
z111, z112, z211, z212 ∈ {0, 1}
Constraints for Equation (3)

11) < z111 + z112 ≤ My∨
−M(1 − y∨
21) < z211 + z212 ≤ My∨
−M(1 − y∨
11, y∨
y∨

21 ∈ {0, 1}

11

21

Constraints for Equation (4)
y∨
11 ≥ 1
y∨
21 ≤ 0

Constraints for Equation (5)
+
w111 = w
111
+
w112 = w
112
+
w121 = w
121
+
w122 = w
122
+
1 − M(1 − µ1) < w
111
+
1 − M(1 − µ1) < w
121
+
1 − M(1 − µ2) < w
112
+
1 − M(1 − µ2) < w
122

+ w−
111
+ w−
112
+ w−
121
+ w−
122
+ w−
111 ≤ Mµ1
+ w−
121 ≤ Mµ1
+ w−
112 ≤ Mµ2
+ w−
122 ≤ Mµ2

µ1, µ2 ∈ {0, 1}
Objective function to minimize
+ w−
111

+ w

+
121

+
111

w

+ w−
121

+ w

+
122

+ w−
122

+ λ(µ1 + µ2)

min
w,w+,w−,z,y ∨, µ
+ w−
+w
112

+
112

Fig. 4. Example to show the constraints generated by our ILP formulation.

6 EXPERIMENTAL EVALUATION
We have implemented Oasis using the LoopInvGen [Padhi et al. 2016] framework in OCaml,
and using Z3 [de Moura and Bjørner 2008] as the theorem prover for checking validity of the

4 An equality requires two disjuncts: we flip the labels (yn ) in Equation (5) and negate the optimal predicate.

16

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

verification conditions. We implement our technique for reducing the classification problem to ILPs
in a Python script, which discharges the ILP subproblems to the OR-Tools [web 5 23a] optimization
package. FindPosCounterExample and FindNegCounterExample procedures in Algorithm 3
are implemented in a python script and we use Z3 [de Moura and Bjørner 2008] to solve the
constraints. We evaluate Oasis on commodity hardware — a CPU-only machines with 2.5GHz Intel
Xeon processor, 32 GB RAM, and running Ubuntu Linux 18.04.

Solvers. We compare Oasis, against three tools: (a) LoopInvGen [Padhi et al. 2016] which uses
data-driven invariant inference, (b) CVC4 [Barrett et al. 2011; Reynolds et al. 2015] which uses
a refutation-based approach, and (c) DradSynth [Huang et al. 2020] which uses a combination
of enumerative and deductive synthesis (cooperative synthesis). CVC4 and LoopInvGen are re-
spectively the winners of the invariant-synthesis (Inv) track of SyGuS-Comp’19 [syg 5 14] and
SyGuS-Comp’18 [Alur et al. 2019]. Recently, Huang et al. [2020] showed that their cooperative
synthesis technique is able to perform better than LoopInvGen and CVC4 on invariant synthesis
tasks.

Benchmark

# Variables
Median Average Maximum

# Instances

Sygus 2018
Unconfounded
Confounded1
Confounded5

4
11
16
26
Table 5. Statistics of the 403 SyGuS instances used for evaluation.

127
92
92
92

3
10
15
25

9
22
32
44

Benchmarks. We evaluate our technique on 403 instances which were part of the SyGuS-
Comp’19 [syg 5 14] and also studied by Huang et al. [2020]. All these instances require reasoning
over linear arithmetic. Out of these 403 instances, 276 were published by Si et al. [2018] and
127 were part of the SyGuS-Comp’18. The 276 instances are divided into three groups of 92
instances each (a) Unconfounded, (b) Confounded1, and (c) Confounded5. Confounded1 and
Confounded5 instances [Si et al. 2018] were obtained by adding irrelevant variables to each of
the Unconfounded instances. The number of irrelevant variables ranges from 4-9 in Confounded1
and from 12-23 in Confounded5. In Table 5, we give key statistics of these benchmarks. These
instances are provided as a collection of logic formulas representing the VCs (Section 3.1) in the
SyGuS grammar [Raghothaman et al. 2019].

Tool

Solved
(out of 403)

CVC4
DradSynth
LoopInvGen
Oasis

287
346
272
353

Tool

Solved

CVC4
DradSynth
LoopInvGen
Oasis

0
11
1
13

Table 6. Comparison of Oasis with SyGuS tools on
the 403 instances which were part of the SyGuS-
Comp’19 [syg 5 14] invariant synthesis track.

Table 7. Number of uniquely solved in-
stances by each tool.

On Scaling Data-Driven Loop Invariant Inference

17

6.1 Results on SyGuS Benchmarks
Comparison with SyGuS Competitors. We report the number of instances each tool solves with
a timeout of 30 minutes5 in Table 6. Oasis synthesizes sufficient loop invariants on 353 instances,
7 more than the second best tool and 66 more than CVC4, the winner of invariant-synthesis (Inv)
track of SyGuS-Comp’19. Oasis is able to solve 13 instances which no other tool can solve. In
Table 7, we list the number of unique benchmarks each tool solves. Out of the 353 instances that
Oasis solves, 262 instances had disjunctive invariants.

Tool

Unconfounded
(out of 92)

Confounded1
(out of 92)

Confounded5
(out of 92)

Solved
(out of 276)

LoopInvGen
Oasis

62
84

59
84

44
67

165
235

Table 8. Comparison of Oasis and LoopInvGen, the state-of-the-art data driven tool, on the 276 instances
which were part of the SyGuS-Comp’19 [syg 5 14] and studied by Si et al. [2018]. These 276 instance are more
complex than the remaining 127 instances in terms of the number of variables present in them. These results
indicate Oasis scales than better LoopInvGen on program with large number of variables.

Comparison with Data-Driven Tools. Oasis solves 81 more benchmarks than LoopInvGen,
which is the state-of-the-art data-driven invariant inference tool, the winner of SyGuS-Comp’18 [Alur
et al. 2019], and runner up of SyGuS-Comp’19 [syg 5 14]. In Table 8, we give break down of the
number of instances LoopInvGen and Oasis solves in each category of the 276 instances from Si
et al. [2018] to show how the complexity of benchmarks affects the performance of data-driven
tools. Oasis solves 70 more benchmarks than LoopInvGen, indicating that Oasis scales better to
programs with large number of variables. Recently, Si et al. [2018] (code2inv) and Ryan et al. [2020]
(cln2inv) propose neural network based approaches for inferring invariants. We evaluate these
two tools on Unconfounded instances6, code2inv solves 64 and cln2inv solves 86 instances within
30 minutes. Garg et al. [2016] and Zhu et al. [2018] are two other data-driven invariant inference
tools. From Table 5, the complexity of the 127 instances from SyGuS-Comp’18 is lower than that
of the 276 instances from Si et al. [2018]. The 127 instances from SyGuS-Comp’18 subsumes the
benchmarks these two tools were evaluated on and Oasis can solve all the instances that they
succeeded on.

In Figure 5, we plot and compare the solving time for Oasis and LoopInvGen on the 403 instances.
Oasis is slower that LoopInvGen on most benchmarks because it runs many Z3 queries to refine
the set of relevant variables. Identifying relevant variables helps Oasis scale to programs with large
number of variables and Oasis solves fewer instances without it (Section 6.2).

[Huang et al. 2020] uses a timeout of 30 minutes and we keep the same timeout.

5
6 Confounded1 and Confounded5 instances are only available as logic formulas and code2inv/cln2inv require C files as

input. It is not straightforward to translate the constraints to C while maintaining a fair comparison.

18

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Fig. 5. Solving time comparison of Oasis and LoopInvGen on the 403 SyGuS instances.

Table 9. Details of Oasis invariant synthesis time, LoopInvGen invariant synthesis time, total variables in the
instance, number of relevant variable used by Oasis in the successful run of RelInfer, number of variables
in the invariant of the corresponding Unconfounded instance (Gold Solution) and size of the synthesized
invariant for Confounded1 and Confounded5 instances solved by Oasis. A ‘-’ indicates that LoopInvGen
times out on that instance.

Benchmark

Oasis Time

LoopInvGen
Time

# Variables

# Relevant
Variables

Gold Solution

Size

1_conf1
2_conf1
3_conf1
5_conf1
10_conf1
11_conf1
12_conf1
13_conf1
15_conf1
16_conf1
17_conf1
18_conf1
23_conf1
24_conf1
28_conf1
29_conf1
38_conf1
40_conf1
41_conf1
42_conf1
43_conf1
44_conf1
45_conf1
46_conf1
47_conf1
48_conf1

32.46
20.83
142.35
747.97
54.31
13.8
41.67
15.58
156.51
191.54
322.36
364.25
76.15
26.3
67.35
272.63
140.04
158.92
159.94
620.74
158.4
157.74
163.14
272.05
159.41
158

-
-
-
-
0.68
0.75
0.69
0.73
-
-
-
-
-
-
-
-
12.01
0.97
0.96
11.46
4.39
5.23
18.16
-
19.41
11.01

15
15
18
20
14
14
14
14
20
20
20
20
15
15
11
11
16
16
16
16
16
16
16
16
16
16

10
6
9
11
6
6
6
6
9
9
14
9
6
6
5
8
7
6
7
6
5
5
7
7
7
7

2
2
3
3
1
1
1
1
2
2
2
2
2
2
2
2
1
1
2
1
1
1
1
2
1
1

28
28
11
11
24
24
24
24
32
7
32
7
60
60
7
11
3
3
19
3
3
3
3
18
3
3

0.010.1110100100010000151101151201251301351401Time (in secs)BenchmarksOASISLoopInvGenOn Scaling Data-Driven Loop Invariant Inference

19

49_conf1
56_conf1
57_conf1
65_conf1
71_conf1
77_conf1
78_conf1
79_conf1
91_conf1
94_conf1
95_conf1
96_conf1
97_conf1
98_conf1
99_conf1
100_conf1
103_conf1
107_conf1
108_conf1
109_conf1
110_conf1
111_conf1
114_conf1
115_conf1
118_conf1
119_conf1
120_conf1
121_conf1
124_conf1
125_conf1
130_conf1
131_conf1
132_conf1
1_conf5
2_conf5
10_conf5
11_conf5
12_conf5
13_conf5
16_conf5
18_conf5
24_conf5
25_conf5
28_conf5
30_conf5
36_conf5
38_conf5
40_conf5
41_conf5
42_conf5
43_conf5
44_conf5
45_conf5
46_conf5
47_conf5
48_conf5
49_conf5
50_conf5
51_conf5
56_conf5
57_conf5
63_conf5
64_conf5
65_conf5
67_conf5
68_conf5
70_conf5

158.05
234.26
168.95
11.46
140.38
165.67
141.53
140.24
14.82
80.46
177.43
186.7
15.91
14.77
86.83
26.67
10.91
281.15
140.97
324.32
54.29
144.44
13.79
15.71
15.71
18.63
21.32
25.12
101.6
32.25
114.27
104.58
1087.6
62.36
25.22
23.32
27.18
73.78
27.07
1131.16
215.43
849.02
14.68
1001.83
13.48
189.78
152.22
166.24
174.61
165.92
163.34
180.09
171.74
178.62
167.27
169.15
165.85
164.16
199.07
177.1
700.96
90.45
84.12
122.15
88
289.74
122.22

15.35
-
0.82
0.05
110.69
0.75
37.24
32.57
-
-
1.36
0.89
0.69
0.68
1.86
-
0.05
-
1.08
-
8.79
9.14
1.06
0.86
8.75
8.86
-
-
-
-
-
-
1.59
-
-
1.97
1.82
2.52
2.78
-
-
-
-
-
-
1.03
28.63
15.81
2.74
40.38
13.57
58.54
62.23
-
52.93
63.3
53.48
1.54
1.23
26.27
1.49
-
-
-
-
-
-

16
16
16
14
21
16
16
16
12
19
20
20
20
20
17
17
9
21
23
23
17
17
16
16
17
17
15
15
19
19
32
32
26
24
24
23
23
23
22
29
29
24
17
19
17
24
26
26
26
26
26
24
26
26
26
26
24
24
24
26
24
23
23
23
25
25
25

7
6
6
6
4
8
6
6
5
10
10
10
5
5
8
8
3
9
5
11
8
8
6
6
8
8
6
6
10
10
12
12
14
9
6
6
6
6
6
9
9
8
3
10
3
6
7
6
7
6
5
5
7
7
7
7
7
5
7
6
6
6
8
9
8
8
8

1
2
2
2
1
2
1
1
2
4
3
2
1
1
3
3
1
3
2
3
2
2
1
1
2
2
2
2
4
4
3
3
1
2
2
1
1
1
1
2
2
2
1
2
1
1
1
1
2
1
1
1
1
2
1
1
1
1
1
2
2
2
2
2
2
3
3

3
7
7
22
3
7
3
3
7
14
53
53
3
3
39
30
19
11
7
11
63
63
19
19
63
63
56
56
47
47
11
11
3
14
14
7
7
7
7
7
7
41
3
7
3
3
3
3
7
3
3
3
3
27
3
3
3
3
3
7
7
7
7
7
7
14
11

20

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

71_conf5
77_conf5
78_conf5
79_conf5
83_conf5
84_conf5
85_conf5
91_conf5
94_conf5
95_conf5
96_conf5
97_conf5
98_conf5
99_conf5
100_conf5
101_conf5
102_conf5
103_conf5
107_conf5
108_conf5
114_conf5
115_conf5
120_conf5
124_conf5
128_conf5
132_conf5
133_conf5

146.19
171.97
151.23
183.15
236.27
32.48
162.42
24.29
185.83
259.72
215.47
35.41
26.88
42.8
180.4
147.21
106.3
18.53
162.78
149.05
23.81
22.33
66.52
1068.72
21.2
1681.38
17.97

61.43
1.23
102.33
-
-
-
-
1.02
-
24.03
47.57
0.8
0.7
2.01
-
-
-
0.95
-
3.31
1.05
0.93
-
-
18.11
5.35
27.28

29
24
24
24
23
23
23
20
27
29
29
29
29
26
26
19
19
17
30
32
25
25
24
28
19
38
19

4
8
6
6
6
6
6
5
10
10
10
5
5
8
8
5
5
6
9
5
6
6
6
10
5
14
5

1
2
1
1
2
2
2
2
4
3
3
1
1
3
3
2
2
1
3
2
1
1
2
4
1
1
2

3
7
3
3
7
7
7
7
17
53
53
3
3
39
27
11
11
3
11
7
31
31
37
47
3
3
7

Analysis. In Table 9, we give detailed statistics of 120 instances which are Confounded1 and
Confounded5 instances that Oasis solves7. Oasis takes 183.77 seconds on average for invariant
synthesis. LoopInvGen times out on 51 instances and averages 15.36 seconds on the 69 instances
it solves. We give the total number of variables in the instance, the number of relevant variables
Oasis uses to synthesize the invariant and the number of variables in the gold solution, i.e., the
number of variables appearing in the invariant of the corresponding Unconfounded instance. Oasis
reduces the number of variables it uses to solve the problem by 3× on average. Size of the invariant
is computed as the number of nodes in the SyGuS AST [Raghothaman et al. 2019] of the invariant.

6.2 Ablation Study

In Section 6.1, we saw that Oasis solves more instances than any other tool. Now, through this
study, we try to answer the following questions about Oasis:

(1) Does identifying relevant variables really help?
(2) Is our ILP formulation better than other techniques?
(3) Is our relevant variable identifying algorithm by itself sufficient to guess invariants?

Tool

Solved
(out of 403)

Oasis, No Vars Select
Oasis

326
353

Table 10. Comparison of Oasis and Oasis without our relevant variable identifying algorithm. Oasis, No
Vars Select uses all the variables appearing in the program as relevant variables.

To show that inference of relevant variables helps in solving more instances, we compare our

tool, Oasis, against the following configuration:

7 We don’t include the instances where the post-condition is a sufficient invariant.

On Scaling Data-Driven Loop Invariant Inference

21

function Oasis, Naive Vars Select(⟨pre , trans , post⟩ : Verification Problem, (cid:174)σ+ : States, (cid:174)σ− : States,
Vars: Variables in Problem)

for each subset s of Vars do

Variables (cid:174)r ← s
I ← RelInfer(⟨pre , trans , post⟩, (cid:174)σ+, (cid:174)σ−, (cid:174)r ) (cid:12)
if I (cid:44) ⊥ then return I

(cid:12) timeout = τ

1

2

3

4

Fig. 6. Implementation of Oasis with naive enumeration strategy. (cid:174)r is the set of relevant variables.

(1) Oasis, No Vars Select. We don’t use our relevant variables identifying algorithm, i.e.,
we don’t execute lines 1-3, thread 1 and thread 2 in Algorithm 2. We use all the variables
appearing in the program to infer the invariant. We still use our ILP formulation (Section 5)
for the Learn function in Algorithm 4.

We observe from Table 10 that inferring relevant variables helps solve 27 more instances. Moreover,
these results also show that out of the 81 benchmarks that Oasis solves more than LoopInvGen,
54 are because of replacing the exhaustive enumeration-based feature synthesizer in LoopInvGen
by ILP and 27 are because of the ILP-based relevant variable inference.

Tool

Solved
(out of 403)

Oasis, Naive Vars Select
Oasis+DT
Oasis, Complete Maps
Oasis, No Optimization
Oasis

258
338
336
297
353

Table 11. Comparison of Oasis and Oasis with our ILP formulation replaced with naive enumeration, decision
tree, ILP without objective function and ILP with complete maps instead of partial maps.

Next, to show that our ILP formulation is better suited for inferring invariants, we compare

Oasis to the following configurations:

(1) Oasis, Naive Vars Select. We use an exhaustive enumeration strategy to find the set of
relevant variables instead of using our ILP formulation (Section 5) to identify this set. We
replace our Algorithm 2 with the implementation in Figure 6.

(2) Oasis+DT. We use scikit-learn [web 5 14] implementation of decision tree in place of our
ILP formulation (Section 5) for the Learn function in Algorithm 2 to find the set of relevant
variables. There is no constraint on the height of the tree that the decision tree algorithm
can learn.

(3) Oasis, Complete Maps. Instead of using the partial maps returned by Z3 while sampling
states in Algorithm 3, we use complete maps. Partial maps are completed by replacing the
don’t care values with random integers.

(4) Oasis, No Optimization. We ignore the objective function used in Equation (5), which
biases our learner towards simple classifiers, and instead only search for solutions that satisfy
the constraints generated by our ILP formulation. This learner with no optimization is used
for the Learn function in both Algorithm 2 and Algorithm 4.

Again, in the configurations 1, 2 and 3, we still use our ILP formulation (Section 5) for the Learn
function in Algorithm 4.

22

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

We observe from Table 11 that using a simple strategy like enumerating over combinations of
variables is not adequate to find the set of relevant variables. Also, this strategy is not scalable for
problems with a large number of variables. Decision trees have been widely used for classification
tasks [Garg et al. 2016; Zhu et al. 2018]. Our ILP formulation performs slightly better than decision
trees because of the objective function which has specific penalties to learn simple and generalizable
expressions. We also observe that running our ILP formulation without the objective function results
in expressions without any constraints on the coefficients of the variables and the size of expression
and we solve far less number of instances than running with the objective function. This shows
that solving only the search problem in our ILP formulation is insufficient for generalization and
learning expressions consistent with the Occam’s razor principle helps in solving more instances.

Tool

Solved
(out of 403)

Oasis, No RelInfer
Oasis

82
353

Table 12. Comparison of Oasis and Oasis without our relevance-aware invariant inference algorithm (thread
3) in Algorithm 2.

Finally, one might think that the classifier that separates reachable and bad states during the
inference of relevant variables might be a good guess for a sufficient loop invariant. To show that
this classifier is usually not an invariant, we compare Oasis against the following configuration:
(1) Oasis, No RelInfer. We don’t run RelInfer, i.e., thread 3 in Algorithm 2. We use the

classifier C returned by the Learn function in Algorithm 2 as our invariant guess.

From Table 12, we observe that the classifier learnt between the good and bad states in Algorithm 2
is not usually a sufficient invariant. However, it is still an indicator of the relevant variables which
appear in the sufficient invariant.

7 RELATED WORK

Loop invariant inference is a challenging problem with a long history. Although, we focus on
numerical invariants in this paper, invariant inference over practical programs can be reduced
to numerical reasoning [Ball et al. 2001]. The existing techniques for numerical loop invariants
can be classified in two categories: those that are purely static and infer invariants from program
text and data-driven approaches that guess invariants from examples of program states. The
traditional static approaches for inferring loop invariants include abstract interpretation [Cousot
and Cousot 1977; Cousot et al. 2005], predicate abstraction [Ball et al. 2001; Godefroid et al. 2010],
interpolation [Henzinger et al. 2004; Jhala and McMillan 2006], constraint solving [Colón et al.
2003], and abductive inference [Dillig et al. 2012, 2013]. Although these approaches are mature
and can scale to large programs, the data-driven approaches are more recent and the scalability
is currently limited. However, data-driven invariant inference techniques (e.g., Garg et al. [2014];
Padhi et al. [2016]) have been shown to outperform static approaches for verification of small but
non-trivial loops.

Oasis reduces the problem of invariant inference to solving a constrained ILP problem, where a
solver minimizes a penalty while maintaining the feasibility of constraints. Similar to us, [Colón et al.
2003; Gulwani et al. 2008; Sankaranarayanan et al. 2006] also reduce invariant inference to constraint
solving. However, their constraints are non-linear and much harder to solve. Subsequently, Gupta
et al. [2013] use data to make these constraints linear. However, this line of work either doesn’t

On Scaling Data-Driven Loop Invariant Inference

23

support disjunctive invariants or requires the number of disjunctions to be fixed by a user-provided
template. Oasis has no such restrictions and can generate invariants that are arbitrary Boolean
combinations of linear inequalities. Moreover, these techniques only solve the feasibility problem
and do not have penalty terms. Although we can encode the search component of our ILP problem
as an SMT constraint [Garg et al. 2014], our penalty terms are effective at generalization (Table 11).
Thus, we use an optimization framework instead of a constraint solving framework like prior
works.

Unlike [Nguyen et al. 2017, 2012; Sharma et al. 2013b,a], which are data-driven techniques for
non-linear invariants, Oasis focuses on linear invariants. This design is primarily motivated by the
presence of hundreds of benchmarks from the SyGuS competition. There are only a few benchmarks
for non-linear invariants and these can already be solved well by existing techniques [Nguyen et al.
2017; Yao et al. 2020]. Additionally, since Oasis is implemented on top of LoopInvGen, it inherits
the capabilities to infer invariants for multiple loops and nested loops from Padhi et al. [2016].

Prior work on data-driven techniques to infer arbitrary Boolean combinations of linear inequal-
ities have all been evaluated on benchmarks at the scale of the SyGuS’18 benchmarks (Table 5)
that have less than ten variables. These include techniques that use SMT solvers directly [Garg
et al. 2014], PAC-learning [Sharma et al. 2013a], decision trees [Garg et al. 2016], SVMs [Li et al.
2017], and combinations of SVMs and decision trees [Zhu et al. 2018]. Techniques based on neural
networks [Ryan et al. 2020; Si et al. 2018] have been evaluated at the scale of Unconfounded
benchmarks (Table 5). Oasis uses ILP to scale data-driven inference to succeed on benchmarks
with even more variables.

8 CONCLUSIONS

Oasis makes the following contributions. Conceptually, Oasis reduces the problem of invariant
inference to learning relevant variables and learning features. Technically, Oasis provides a novel
ILP-based learner which generates sparse classifiers and solves both these problems effectively.
Practically, Oasis outperforms the state-of-the-art tools, including the most recent work of Huang
et al. [2020], on benchmarks from the invariant inference track of the Syntax Guided Synthesis
competition. Oasis both solves more benchmarks and can solve benchmarks that no other tool
could solve before. We are working towards integrating Oasis with a full-fledged verification
system for effective verification of complete applications.

Stepping back, the inference of loop invariants is an old problem with a rich history. Many
techniques have been applied to this problem and they all have their strengths and weaknesses.
Data-driven invariant inference techniques can handle challenging loops with confusing program
text by applying ML techniques to mine patterns directly from data. However, these techniques
have been evaluated only on loops with a small number of variables. This weakness is clear on
benchmarks with large number of irrelevant variables. Oasis uses ML to infer the relevant variables
which leads to simpler verification problems with fewer variables. We believe that this idea of
simplifying the verification problems using ML is generally applicable. Oasis demonstrates that
ML-based simplification is effective for data-driven invariant inference and we will explore it in
other contexts in the future.

REFERENCES

Accessed: 2019-05-23a. OR-Tools – Google Optimization Tools. https://github.com/google/or-tools.
Accessed: 2019-05-23b. The International Competition on Software Verification. https://sv-comp.sosy-lab.org.
Accessed: 2019-05-23c. The Syntax-Guided Synthesis Competition. https://sygus.org.
Accessed: 2020-05-14. Decision Tree - Scikit-Learn. https://scikit-learn.org/stable/modules/tree.html.
Accessed: 2020-05-14. The Syntax-Guided Synthesis Competition 2019. https://sygus.org/comp/2019/.

24

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Aws Albarghouthi, Sumit Gulwani, and Zachary Kincaid. 2013. Recursive Program Synthesis. In Computer Aided Verification
- 25th International Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013. Proceedings (Lecture Notes in Computer
Science, Vol. 8044), Natasha Sharygina and Helmut Veith (Eds.). Springer, 934–950.

Aws Albarghouthi and Kenneth L. McMillan. 2013. Beautiful Interpolants. In Computer Aided Verification - 25th International
Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013. Proceedings (Lecture Notes in Computer Science, Vol. 8044),
Natasha Sharygina and Helmut Veith (Eds.). Springer, 313–329.

Rajeev Alur, Dana Fisman, Saswat Padhi, Rishabh Singh, and Abhishek Udupa. 2019. SyGuS-Comp 2018: Results and

Analysis. CoRR abs/1904.07146 (2019). http://arxiv.org/abs/1904.07146

Rajeev Alur, Arjun Radhakrishna, and Abhishek Udupa. 2017. Scaling Enumerative Program Synthesis via Divide and
Conquer. In Tools and Algorithms for the Construction and Analysis of Systems - 23rd International Conference, TACAS
2017, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2017, Uppsala, Sweden,
April 22-29, 2017, Proceedings, Part I (Lecture Notes in Computer Science, Vol. 10205), Axel Legay and Tiziana Margaria
(Eds.). 319–336.

Roberto Bagnara, Patricia M. Hill, and Enea Zaffanella. 2006. Widening operators for powerset domains. Int. J. Softw. Tools

Technol. Transf. 8, 4-5 (2006), 449–466.

Thomas Ball, Rupak Majumdar, Todd D. Millstein, and Sriram K. Rajamani. 2001. Automatic Predicate Abstraction of C
Programs. In Proceedings of the 2001 ACM SIGPLAN Conference on Programming Language Design and Implementation
(PLDI). ACM. https://doi.org/10.1145/378795.378846

Clark Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan Jovanovic, Tim King, Andrew Reynolds,
and Cesare Tinelli. 2011. CVC4. In Computer Aided Verification - 23rd International Conference (CAV) (Lecture Notes in
Computer Science, Vol. 6806). Springer. https://doi.org/10.1007/978-3-642-22110-1_14

Michael Colón, Sriram Sankaranarayanan, and Henny Sipma. 2003. Linear Invariant Generation Using Non-linear Constraint
Solving. In Computer Aided Verification, 15th International Conference, CAV 2003, Boulder, CO, USA, July 8-12, 2003,
Proceedings (Lecture Notes in Computer Science, Vol. 2725), Warren A. Hunt Jr. and Fabio Somenzi (Eds.). Springer,
420–432.

Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs
by Construction or Approximation of Fixpoints. In Conference Record of the Fourth ACM Symposium on Principles of
Programming Languages (POPL). ACM. https://doi.org/10.1145/512950.512973

Patrick Cousot, Radhia Cousot, Jérôme Feret, Laurent Mauborgne, Antoine Miné, David Monniaux, and Xavier Rival. 2005.
The ASTREÉ Analyzer. In Programming Languages and Systems, 14th European Symposium on Programming (ESOP)
(Lecture Notes in Computer Science, Vol. 3444). Springer. https://doi.org/10.1007/978-3-540-31987-0_3

Patrick Cousot and Nicolas Halbwachs. 1978. Automatic Discovery of Linear Restraints Among Variables of a Program. In
Conference Record of the Fifth Annual ACM Symposium on Principles of Programming Languages, Tucson, Arizona, USA,
January 1978, Alfred V. Aho, Stephen N. Zilles, and Thomas G. Szymanski (Eds.). ACM Press, 84–96.

Leonardo Mendonça de Moura and Nikolaj Bjørner. 2008. Z3: An Efficient SMT Solver. In Tools and Algorithms for the
Construction and Analysis of Systems, 14th International Conference (TACAS) (Lecture Notes in Computer Science, Vol. 4963).
Springer. https://doi.org/10.1007/978-3-540-78800-3_24

Isil Dillig, Thomas Dillig, and Alex Aiken. 2012. Automated error diagnosis using abductive inference. In ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI ’12, Beijing, China - June 11 - 16, 2012, Jan Vitek,
Haibo Lin, and Frank Tip (Eds.). ACM, 181–192.

Isil Dillig, Thomas Dillig, Boyang Li, and Kenneth L. McMillan. 2013. Inductive Invariant Generation via Abductive Inference.
In Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &
Applications OOPSLA). ACM. https://doi.org/10.1145/2509136.2509511

Joe W. Duran and Simeon C. Ntafos. 1981. A Report on Random Testing. In Proceedings of the 5th International Conference
on Software Engineering, San Diego, California, USA, March 9-12, 1981, Seymour Jeffrey and Leon G. Stucki (Eds.). IEEE
Computer Society, 179–183.

Michael D. Ernst, Adam Czeisler, William G. Griswold, and David Notkin. 2000. Quickly detecting relevant program
invariants. In Proceedings of the 22nd International Conference on on Software Engineering, ICSE 2000, Limerick Ireland,
June 4-11, 2000, Carlo Ghezzi, Mehdi Jazayeri, and Alexander L. Wolf (Eds.). ACM, 449–458.

Robert W. Floyd. 1967. Assigning Meanings to Programms. In Proccedings of the AMS Symposium on Appllied Mathematics,
Vol. 19. American Mathematical Society. http://www.cs.virginia.edu/~weimer/2007-615/reading/FloydMeaning.pdf
Pranav Garg, Christof Löding, P. Madhusudan, and Daniel Neider. 2014. ICE: A Robust Framework for Learning Invariants.
In Computer Aided Verification - 26th International Conference, CAV 2014, Held as Part of the Vienna Summer of Logic,
VSL 2014, Vienna, Austria, July 18-22, 2014. Proceedings (Lecture Notes in Computer Science, Vol. 8559), Armin Biere and
Roderick Bloem (Eds.). Springer, 69–87.

Pranav Garg, Daniel Neider, P. Madhusudan, and Dan Roth. 2016. Learning invariants using Decision Trees and Implication
Counterexamples. In Proceedings of the 43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming

On Scaling Data-Driven Loop Invariant Inference

25

Language (POPL). ACM. https://doi.org/10.1145/2837614.2837664

Patrice Godefroid, Aditya V. Nori, Sriram K. Rajamani, and SaiDeep Tetali. 2010. Compositional may-must program analysis:
unleashing the power of alternation. In Proceedings of the 37th ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages, POPL 2010, Madrid, Spain, January 17-23, 2010, Manuel V. Hermenegildo and Jens Palsberg
(Eds.). ACM, 43–56.

Bhargav S. Gulavani, Thomas A. Henzinger, Yamini Kannan, Aditya V. Nori, and Sriram K. Rajamani. 2006. SYNERGY: a
new algorithm for property checking. In Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations
of Software Engineering, FSE 2006, Portland, Oregon, USA, November 5-11, 2006, Michal Young and Premkumar T. Devanbu
(Eds.). ACM, 117–127.

Sumit Gulwani and Nebojsa Jojic. 2007. Program verification as probabilistic inference. In Proceedings of the 34th ACM
SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2007, Nice, France, January 17-19, 2007,
Martin Hofmann and Matthias Felleisen (Eds.). ACM, 277–289.

Sumit Gulwani, Saurabh Srivastava, and Ramarathnam Venkatesan. 2008. Program analysis as constraint solving. In
Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation, Tucson, AZ,
USA, June 7-13, 2008, Rajiv Gupta and Saman P. Amarasinghe (Eds.). ACM, 281–292.

Ashutosh Gupta, Rupak Majumdar, and Andrey Rybalchenko. 2013. From tests to proofs. Int. J. Softw. Tools Technol. Transf.

15, 4 (2013), 291–303.

Steve Hanneke. 2009. Theoretical foundations of active learning. Technical Report. CARNEGIE-MELLON UNIV PITTSBURGH

PA MACHINE LEARNING DEPT.

Thomas A. Henzinger, Ranjit Jhala, Rupak Majumdar, and Kenneth L. McMillan. 2004. Abstractions from proofs. In
Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2004, Venice,
Italy, January 14-16, 2004, Neil D. Jones and Xavier Leroy (Eds.). ACM, 232–244.

C. A. R. Hoare. 1969. An Axiomatic Basis for Computer Programming. Commun. ACM 12, 10 (1969). https://doi.org/10.

1145/363235.363259

Susan Horwitz, Thomas W. Reps, and David W. Binkley. 1988.

Interprocedural Slicing Using Dependence Graphs. In
Proceedings of the ACM SIGPLAN’88 Conference on Programming Language Design and Implementation (PLDI), Atlanta,
Georgia, USA, June 22-24, 1988, Richard L. Wexelblat (Ed.). ACM, 35–46.

Kangjing Huang, Xiaokang Qiu, Peiyuan Shen, and Yanjun Wang. 2020. Reconciling Enumerative and Deductive Program
Synthesis. In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation
(London, UK) (PLDI 2020). Association for Computing Machinery, New York, NY, USA, 1159–1174. https://doi.org/10.
1145/3385412.3386027

Ranjit Jhala and Kenneth L. McMillan. 2006. A Practical and Complete Approach to Predicate Refinement. In Tools and
Algorithms for the Construction and Analysis of Systems, 12th International Conference (TACAS) (Lecture Notes in Computer
Science, Vol. 3920). Springer. https://doi.org/10.1007/11691372_33

Jiaying Li, Jun Sun, Li Li, Quang Loc Le, and Shang-Wei Lin. 2017. Automatic loop-invariant generation and refinement
through selective sampling. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software
Engineering, ASE 2017, Urbana, IL, USA, October 30 - November 03, 2017, Grigore Rosu, Massimiliano Di Penta, and Tien N.
Nguyen (Eds.). IEEE Computer Society, 782–792.

Antoine Miné. 2006. The octagon abstract domain. High. Order Symb. Comput. 19, 1 (2006), 31–100.
Tom M Mitchell et al. 1997. Machine learning. 1997. Burr Ridge, IL: McGraw Hill 45, 37 (1997), 870–877.
ThanhVu Nguyen, Timos Antonopoulos, Andrew Ruef, and Michael Hicks. 2017. Counterexample-Guided Approach to
Finding Numerical Invariants. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering
(ESEC/FSE). ACM. https://doi.org/10.1145/3106237.3106281

ThanhVu Nguyen, Deepak Kapur, Westley Weimer, and Stephanie Forrest. 2012. Using dynamic analysis to discover
polynomial and array invariants. In 34th International Conference on Software Engineering, ICSE 2012, June 2-9, 2012,
Zurich, Switzerland, Martin Glinz, Gail C. Murphy, and Mauro Pezzè (Eds.). IEEE Computer Society, 683–693.

Saswat Padhi, Todd Millstein, Aditya Nori, and Rahul Sharma. 2019. Overfitting in Synthesis: Theory and Practice. In
Computer Aided Verification - 30th International Conference (CAV) (Lecture Notes in Computer Science). Springer (To
Appear). https://arxiv.org/pdf/1905.07457

Saswat Padhi, Rahul Sharma, and Todd D. Millstein. 2016. Data-Driven Precondition Inference with Learned Features. In
Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI). ACM.
https://doi.org/10.1145/2908080.2908099

Mukund Raghothaman, Andrew Reynolds, and Abhishek Udupa. 2019. The SyGuS Language Standard Version 2.0. (2019).
Andrew Reynolds, Morgan Deters, Viktor Kuncak, Cesare Tinelli, and Clark W. Barrett. 2015. Counterexample-Guided
Quantifier Instantiation for Synthesis in SMT. In Computer Aided Verification - 27th International Conference (CAV)
(Lecture Notes in Computer Science, Vol. 9207). Springer. https://doi.org/10.1007/978-3-319-21668-3_12

26

Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, and Prateek Jain

Gabriel Ryan, Justin Wong, Jianan Yao, Ronghui Gu, and Suman Jana. 2020. CLN2INV: Learning Loop Invariants with
Continuous Logic Networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.

Sriram Sankaranarayanan, Franjo Ivancic, Ilya Shlyakhter, and Aarti Gupta. 2006. Static Analysis in Disjunctive Numerical
Domains. In Static Analysis, 13th International Symposium, SAS 2006, Seoul, Korea, August 29-31, 2006, Proceedings (Lecture
Notes in Computer Science, Vol. 4134), Kwangkeun Yi (Ed.). Springer, 3–17.

Greg Schohn and David Cohn. 2000. Less is More: Active Learning with Support Vector Machines. In Proceedings of the
Seventeenth International Conference on Machine Learning (ICML 2000), Stanford University, Stanford, CA, USA, June 29 -
July 2, 2000, Pat Langley (Ed.). Morgan Kaufmann, 839–846.

Rahul Sharma and Alex Aiken. 2016. From Invariant Checking to Invariant Inference Using Randomized Search. Formal

Methods in System Design 48, 3 (2016). https://doi.org/10.1007/s10703-016-0248-5

Rahul Sharma, Saurabh Gupta, Bharath Hariharan, Alex Aiken, Percy Liang, and Aditya V. Nori. 2013b. A Data Driven
Approach for Algebraic Loop Invariants. In Programming Languages and Systems - 22nd European Symposium on
Programming, ESOP 2013, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2013,
Rome, Italy, March 16-24, 2013. Proceedings (Lecture Notes in Computer Science, Vol. 7792), Matthias Felleisen and Philippa
Gardner (Eds.). Springer, 574–592.

Rahul Sharma, Saurabh Gupta, Bharath Hariharan, Alex Aiken, and Aditya V. Nori. 2013a. Verification as Learning Geometric
Concepts. In Static Analysis - 20th International Symposium (SAS) (Lecture Notes in Computer Science, Vol. 7935). Springer.
https://doi.org/10.1007/978-3-642-38856-9_21

Rahul Sharma, Aditya V. Nori, and Alex Aiken. 2012. Interpolants as Classifiers. In Computer Aided Verification - 24th
International Conference, CAV 2012, Berkeley, CA, USA, July 7-13, 2012 Proceedings (Lecture Notes in Computer Science,
Vol. 7358), P. Madhusudan and Sanjit A. Seshia (Eds.). Springer, 71–87.

Xujie Si, Hanjun Dai, Mukund Raghothaman, Mayur Naik, and Le Song. 2018. Learning Loop Invariants for Program
Verification. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing
Systems (NeurIPS). http://papers.nips.cc/paper/8001-learning-loop-invariants-for-program-verification

Aditya V. Thakur, Akash Lal, Junghee Lim, and Thomas W. Reps. 2015. PostHat and All That: Automating Abstract

Interpretation. Electron. Notes Theor. Comput. Sci. 311 (2015), 15–32.

Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, and Ronghui Gu. 2020. Learning nonlinear loop invariants with gated
continuous logic networks. In Proceedings of the 41st ACM SIGPLAN International Conference on Programming Language
Design and Implementation, PLDI 2020, London, UK, June 15-20, 2020, Alastair F. Donaldson and Emina Torlak (Eds.). ACM,
106–120.

He Zhu, Stephen Magill, and Suresh Jagannathan. 2018. A Data-Driven CHC Solver. In Proceedings of the 39th ACM SIGPLAN
Conference on Programming Language Design and Implementation (PLDI). ACM. https://doi.org/10.1145/3192366.3192416

