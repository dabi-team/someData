Task Programming: Learning Data Efﬁcient Behavior Representations

Jennifer J. Sun1

Ann Kennedy2

Eric Zhan1

David J. Anderson1

Yisong Yue1

Pietro Perona1

1Caltech

2Northwestern University

Code & Project Website: https://sites.google.com/view/task-programming

1
2
0
2

r
a

M
9
2

]

V
C
.
s
c
[

2
v
7
1
9
3
1
.
1
1
0
2
:
v
i
X
r
a

Abstract

Specialized domain knowledge is often necessary to ac-
curately annotate training sets for in-depth analysis, but
can be burdensome and time-consuming to acquire from do-
main experts. This issue arises prominently in automated
behavior analysis, in which agent movements or actions of
interest are detected from video tracking data. To reduce
annotation effort, we present TREBA: a method to learn
annotation-sample efﬁcient trajectory embedding for be-
havior analysis, based on multi-task self-supervised learn-
ing. The tasks in our method can be efﬁciently engineered
by domain experts through a process we call “task program-
ming”, which uses programs to explicitly encode structured
knowledge from domain experts. Total domain expert effort
can be reduced by exchanging data annotation time for the
construction of a small number of programmed tasks. We
evaluate this trade-off using data from behavioral neuro-
science, in which specialized domain knowledge is used to
identify behaviors. We present experimental results in three
datasets across two domains: mice and fruit ﬂies. Using
embeddings from TREBA, we reduce annotation burden by
up to a factor of 10 without compromising accuracy com-
pared to state-of-the-art features. Our results thus suggest
that task programming and self-supervision can be an ef-
fective way to reduce annotation effort for domain experts.

1. Introduction

Behavioral analysis of one or more agents is a core el-
ement in diverse ﬁelds of research, including biology [36,
26], autonomous driving [6, 39], sports analytics [42, 43],
and video games [20, 3]. In a typical experimental work-
the location and pose of agents is ﬁrst extracted
ﬂow,
from each frame of a behavior video, and then labels for
experimenter-deﬁned behaviors of interest are applied on a
frame-by-frame basis based on the pose and movements of
the agents. In addition to reducing human effort, automated
quantiﬁcation of behavior can lead to more objective, pre-

Correspondence to jjsun@caltech.edu.

Figure 1. Overview of our approach. Part 1: A typical behavior
study starts with extraction of tracking data from videos. We show
7 keypoints for each mouse, and draw the trajectory of the nose
keypoint. Part 2: Domain experts can either do data annotation
(Classiﬁer A) or task programming (Classiﬁer B) to reduce classi-
ﬁer error. The middle panel shows annotated frames at 30Hz. Col-
ors in the bottom plot represent interpolated performance based on
classiﬁer error at the circular markers (full results in Section 4.3).
The size of the marker represents the error variance.

cise, and scalable measurements compared to manual anno-
tation [1, 10]. However, training behavior detection models
can be data intensive and manual behavior annotation often
requires specialized domain knowledge and high-frequency
temporal labels. As a result, this process of generating train-
ing datasets is time-consuming and effort-intensive for ex-
perts. Therefore, methods to reduce annotation effort by
domain experts are needed to accelerate behavioral studies.
We study alternative ways for domain experts to improve

1

1.Record videos and extract tracking data. 2. Apply behavior classiﬁer for scalability.Human AnnotationBase Classiﬁer80k annotationsClassiﬁer B80k annotations + 10 programsClassiﬁer A781k annotations+701k annotations+10 programsBase Classiﬁer80k annotationsBase Classiﬁer80k annotationsClassiﬁer A781k annotationsClassiﬁer B80k annotations + 10 programsClassiﬁer AClassiﬁer BBase Classiﬁer 
 
 
 
 
 
classiﬁer accuracy beyond simply increasing the sheer vol-
ume of annotations. In particular, we propose a framework
that uniﬁes: (1) self-supervised representation learning, and
(2) encoding explicit structured knowledge on trajectory
data using expert-deﬁned programs. Domain experts can
construct these programs efﬁciently because keypoint tra-
jectories in each frame are typically low dimensional, and
experts can already hand-design effective features for tra-
jectory data [36, 28]. To best leverage this structured ex-
pert knowledge, we develop a framework to learn trajectory
representations based on multi-task self-supervised learn-
ing, which has not been well-explored for trajectory data.

Our Approach. Our framework, Trajectory Embedding
for Behavior Analysis (TREBA), learns trajectory represen-
tations through trajectory generation alongside a set of de-
coder tasks based on expert-engineered programs. These
programs are created by domain experts through a process
we call task programming, inspired by the data program-
ming paradigm [33]. Task programming is a process by
which domain experts identify trajectory attributes relevant
to the behaviors of interest under study, write programs,
and apply those programs to inform representation learn-
ing (Section 3.2). This ﬂexibility in decoder tasks allows
our framework to be applicable to a variety of agents and
behaviors studied across diverse ﬁelds of research.

Expert Effort Tradeoffs. Since task programming will
typically require a domain expert’s time, we study the trade-
off between doing task programming and data annotation.
We compare behavior classiﬁcation performance with dif-
ferent amounts of annotated training data and programmed
tasks. For example, for the domain illustrated in Figure 1,
domain experts can reduce error by 13% relative to the base
classiﬁer by annotating 701k additional frames, or they can
reduce error by 16% by learning a representation using 10
programmed tasks in our framework. Our approach allows
experts to trade a large number of annotations for a small
number of programmed tasks.

We study our approach across two domains in behavioral
neuroscience, namely mouse and ﬂy behavior. We chose
this setting because it requires specialized domain knowl-
edge for data annotation, and data efﬁciency is important for
domain experts. Furthermore, decoder tasks in our frame-
work can be efﬁciently programmed by experts based on
simple functions describing trajectory attributes for identi-
fying behaviors of interest. For example, for mouse social
behaviors such as attack [36], important behavior attributes
include the speed of each mouse and distance between mice.
The corresponding task could then be to decode these at-
tributes from the learned representations.

Our contributions are:

• We introduce task programming as an efﬁcient way for
domain experts to reduce annotation effort and encode
structural knowledge. We develop a novel method to

learn an annotation-sample efﬁcient trajectory repre-
sentation using self-supervision and programmatic su-
pervision.

• We study the effect of task programming, data annota-
tion, and different decoder losses on behavior classiﬁer
performance.

• We demonstrate these representations on three datasets
in two domains, showing that our method can lead to a
10× annotation reduction for mice, and 2× for ﬂies.

2. Related Work

Behavior Modeling. Behavior modeling using trajec-
tory data is studied across a variety of ﬁelds [26, 6, 39,
42, 20, 3].
In particular, there is an increasing effort to
automatically detect and classify behavior from trajectory
data [23, 1, 14, 27, 13, 36]. Our experiments are based
on behavior classiﬁcation datasets from behavioral neuro-
science [15, 4, 36], a ﬁeld where specialized domain knowl-
edge is important for identifying behaviors of interest.

The behavior analysis pipeline generally consists of the
following steps: (1) tracking the pose of agents, (2) com-
puting pose-based features, and (3) training behavior classi-
ﬁers [4, 21, 36, 28]. To address step 1, there are many exist-
ing pose estimation models [15, 27, 18, 36]. In our work, we
leverage two existing pose models, [36] for mice and [15]
for ﬂies, to produce trajectory data. In steps 2 and 3 of the
typical behavior analysis pipeline, hand-designed trajectory
features are computed from the animals’ pose, and classi-
ﬁers are trained to predict behaviors of interest in a fully
supervised fashion [4, 21, 15, 36]. Training fully super-
vised behavior classiﬁers requires time-consuming annota-
tions by domain experts [1]. Instead, our proposed approach
enables domain experts to trade time-consuming annotation
work for task programming with representation learning.

Another group of work uses unsupervised methods to
discover new motifs and behaviors [22, 41, 2, 26, 5]. Our
work focuses on the more common case where domain ex-
perts already know what types of actions they would like
to study in an experiment. We aim to improve the data-
efﬁciency of learning expert-deﬁned behaviors.

Representation Learning. Visual representation learn-
ing has made great progress in effective representations
for images and videos [17, 16, 7, 29, 25, 19, 38]. Self-
supervised signals are often used to train this visual rep-
resentation, such as learning relative positions of image
patches [11], predicting image rotations [16], predicting fu-
ture patches [29], and constrastive learning on augmented
images [7]. Compared to visual data, trajectory data is sig-
niﬁcantly lower dimensional in each frame, and techniques
from visual representation learning often cannot be applied
directly. For example, while we can create image patches
that represent the same visual class, it is difﬁcult to select

2

Figure 2. Task Programming and Data Annotation for Classiﬁer Training. Domain experts can choose between doing task program-
ming and/or data annotation. Task programming is the process for domain experts to engineer decoder tasks for representation learning.
The programs enable learning of annotation-sample efﬁcient trajectory features to improve performance instead of additional annotations.

a partial set of keypoints that represent the same behavior.
Our framework builds upon these approaches to learn effec-
tive representations for behavioral data.

We investigate different decoder tasks in order to learn
an effective behavior representation. One decoder task that
we investigate is self-decoding: the reconstruction of input
trajectories using generative modeling. Generative model-
ing has previously been applied to learn representations for
visual data [45, 38, 29] and language modeling [31]; for tra-
jectory data, we use imitation learning [40, 44, 43] to train
our trajectory representation. The other tasks in our multi-
task self-supervised learning framework are created by do-
main experts using task programming (Section 3.2). This
idea of using a human-provided function as part of train-
ing has been studied for training set creation [33, 32], and
controllable trajectory generation [43]. Our work explores
these additional decoder tasks to further improve the learned
representation over the generative loss alone.

Multi-Task Self-Supervised Learning. We jointly op-
timize a family of self-supervised tasks in an encoder-
decoder setup, making this work an example of multi-
task self-supervised learning. Multi-task self-supervised
learning has been applied to other domains such as visual
data [12, 25], accelerometer recordings [35], audio [34] and
multi-modal inputs [37, 30]. Generally in each of these do-
mains, tasks are deﬁned ahead of time, as is the case for
tasks such as frame reconstruction, colorization, ﬁnding rel-
ative position of image patches, and video-audio alignment.
Most of these tasks are designed for image or video data,
and cannot be applied directly to trajectory data. To con-
struct tasks for trajectory representation learning, we pro-
pose that domain experts can use task programming to en-
gineer decoder tasks and encode structural knowledge.

efﬁcient trajectory representation using self-supervision and
auxiliary decoder tasks engineered by domain experts. Fig-
ure 2 provides an overview of the expert’s role.
In our
framework, domain experts replace (a signiﬁcant amount
of) time-consuming manual annotation with the construc-
tion of a small number of programmed tasks, reducing total
expert effort. Each task places an additional constraint on
the learned trajectory embedding.

TREBA uses the expert-programmed tasks based on a
multi-task self-supervised learning approach, outlined in
Figure 3. To learn task-relevant low-dimensional represen-
tations of pose trajectories, we train a network jointly on
(1) reconstruction of the input trajectory (Section 3.1) and
(2) expert-programmed decoder tasks (Section 3.3). The
learned representation can then be used as input to behavior
modeling tasks, such as behavior classiﬁcation.

3.1. Trajectory Representations

Let D be a set of N unlabelled trajectories. Each tra-
jectory τ is a sequence of states τ = {(st)}T
t=1, where the
state si at timestep i corresponds to the location or pose of
the agents at that timestep. In this study, we divide trajecto-
ries from longer recordings into segments of length T , but
in general trajectory length can vary. For multiple agents,
the keypoints of each agent is stacked at each timestep.

Before we introduce our expert-programmed tasks,
we will use trajectory reconstruction as an initial self-
supervised task. Given a history of agent states, we would
like our model to predict the next state. This task is usually
studied with sequential generative models. We used trajec-
tory variational autoencoders (TVAEs) [9, 43] to embed the
input trajectory using an RNN encoder, qφ, and an RNN
decoder, pθ, to predict the next state. The TVAE loss is:

3. Methods

We introduce Trajectory Embedding for Behavior
Analysis (TREBA), a method to learn an annotation-sample

Ltvae = Eqφ

(cid:20) T

(cid:88)

t=1

− log(pθ(st+1|st, z))

(cid:21)

(1)

+DKL(qφ(z|τ )||pθ(z)).

3

Task ProgrammingExamine trajectory dataSelect behavior attributesWrite programsdist_nose(x1, y1, x2, y2):    x_diﬀ = x2 - x1    y_diﬀ = y2 - y1    dist = norm(x_diﬀ, y_diﬀ)Domain ExpertAdd decoder taskAnnotate frame-level behavior Classiﬁer TrainingFeature ExtractionModel TrainingData AnnotationMountOtherSniﬀDistanceSpeedFigure 3. TREBA Training and Inference Pipelines. During training, we use trajectory self-decoding and the programmed decoder tasks
to train the trajectory encoder. The learned representation is used for downstream tasks such as behavior classiﬁcation.

We use a prior distribution pθ(z) on z to regularize the
learned embeddings; in this study, our prior is the unit Gaus-
sian. By optimizing for the TVAE loss only, we learn an
unsupervised version of TREBA. When performing subse-
quent behavior modeling tasks such as classiﬁcation, we use
the embedding mean, zµ.

3.2. Task Programming

Task programming is the process by which domain
experts create decoder tasks for trajectory self-supervised
learning. This process consists of selecting attributes from
trajectory data, writing programs, and creating decoder
tasks based on the programs (Figure 2). Here, domain ex-
perts are people with specialized knowledge for studying
behavior, such as neuroscientists or sports analysts.

To start, domain experts identify attributes from trajec-
tory data relevant to the behaviors of interest under study.
Behavior attributes capture information that is likely rele-
vant to agent behavior, but is not explicitly included in the
trajectory states {(st)}T
t=1. These attributes represent struc-
tured knowledge that domain experts are implicitly or ex-
plicitly considering for behavior analysis, such as the dis-
tance between two agents, agent velocity, or the relative po-
sitioning of agent body parts.

Next, domain experts write a program to compute these
attributes on trajectory data, which can be done with exist-
ing tools such as MARS [36] or SimBA [28]. Algorithm 1
shows a sample program from the mouse social behavior
domain, for measuring the “facing angle” between a pair of
interacting mice. Each program can be used to construct
decoder tasks for self-supervised learning (Section 3.3).

Our framework is inspired by the data programming
paradigm [33], which applies programs to training set cre-
In comparison, our framework uses task program-
ation.
ming to unify expert-engineered programs, which encode
structured expert knowledge, with representation learning.

Algorithm 1: Sample Program for Facing Angle
Input: centroid of mouse 1 (x1, y1), centroid of
mouse 2 (x2, y2), heading of mouse 1 (φ1)
xdiff = x2 − x1
ydiff = y2 − y1
θ = arctan(ydiff, xdiff)
Return θ − φ1

Domain
Mouse

Fly

Behavior Attributes
Facing Angle Mouse 1 and 2, Speed Mouse 1 and 2
Nose-Nose Distance, Nose-Tail Distance,
Head-Body Angle Mouse 1 and 2
Nose Movement Mouse 1 and 2
Speed Fly 1 and 2, Fly-Fly Distance
Angular Speed Fly 1 and 2, Facing Angle Fly 1 and 2
Min and Max Wing Angles Fly 1 and 2
Major/Minor Axis Ratio Fly 1 and 2

Table 1. Behavior Attributes used in Task Programming. We
base our programmed tasks in our experiments on these behavior
attributes from domain experts in each domain.

Working with domain experts in behavioral neuro-
science, we created a set of programs to use in studying
our approach. The selected programs are a subset of be-
havior attributes in [36] (for mouse datasets) and a subset
of behavior attributes in [15] (for ﬂy datasets). We list the
programs used in Table 1, and provide more details about
the programs in the Supplementary Material.

3.3. Learning Algorithm

We develop a method to incorporate the programs from
domain experts as additional learning signals for TREBA.
We consider the following three approaches: (1) enforc-
ing attribute consistency in generated trajectories (Sec-
tion 3.3.1), (2) performing attribute decoding directly (Sec-
tion 3.3.2), (3) applying contrastive loss based on program
supervision (Section 3.3.3). Each of these methods applies

4

TrainTrajectory DataTrajectory EncoderEmbeddingTrajectory DecoderFor each timestamp tState at time tStatePredictionGenerated TrajectoryTrajectory Recon. LossFor each program Attribute DecoderRepresentation DecoderAttribute Decoding LossContrastive LossAttribute Consistency LossInference Trajectory DataTrajectory EncoderEmbeddingDownstream Model (ex: Behavior Classiﬁer)a different loss on the low-dimensional representation z of
trajectory τ . Any combinations of these decoding tasks can
be combined with self-decoding from Section 3.1 to inform
the trajectory embedding z.

3.3.1 Attribute Consistency

Let λ be a set of M domain-expert-designed functions mea-
suring agent behavior attributes, such as agent velocity or
facing angle. Recall that each λj, j = 1...M takes as input
a trajectory τ , and returns some expert-designed attribute
λj(τ ) computed from that trajectory. For λj designed for a
single frame, we apply the function to the center frame of
τ . Attribute consistency aims to maintain the same behav-
ior attribute labels for the generated trajectory as the orig-
inal. Let ˜τ be the trajectory generated by the TVAE given
the same initial condition as τ and encoding z.The attribute
consistency loss is:

sentation. We then apply the contrastive loss:

Lcntr. =

B
(cid:88)

M
(cid:88)

i=1

j=1

(cid:20) −1
Npos(i,j)

B
(cid:88)

k=1

1i(cid:54)=k · 1λj (τi)=λj (τk)

· log

(cid:80)N

l=1

exp(gi · gk/t)
1i(cid:54)=l · exp(gi · gl/t)

(cid:21)
,

(4)

where B is the batch size, Npos(i,j) is the number of posi-
tive matches for τi with λj, and t > 0 is a scalar temperature
parameter. Our form of contrastive loss supervised by task
programming is similar to the contrastive loss in [24] su-
pervised by human annotations. A beneﬁt of task program-
ming is that the supervision from programs can be quickly
and scalably applied to unlabelled datasets, as compared to
expert supervision which can be time-consuming. We note
that the unsupervised version of this contrastive loss is stud-
ied in [7], based on previous works such as [29].

Lattr = Eτ ∼D

1(λj(˜τ ) (cid:54)= λj(τ ))

(cid:21)
.

(cid:20) M
(cid:88)

j=1

(2)

3.3.4 Data Augmentation

Here, we show the loss for categorical λj, but in general,
λj can be continuous and any loss measuring differences
between λj(˜τ ) and λj(τ ) applies, such as mean squared er-
ror. We do not require λ to always be differentiable, and we
use the differentiable approximation introduced in [43] to
handle non-differentiable λ.

3.3.2 Attribute Decoding

Another option is to decode each attribute λj(τ ) directly
from the learned representation z. Here we apply a shallow
decoder f to the learned representation, with decoding loss:

Ldecode = Eτ ∼D

(cid:20) M
(cid:88)

j=1

1(f (qφ(zµ|τ )) (cid:54)= λj(τ ))

(cid:21)
.

(3)

We can perform data augmentation on trajectory data
based on our expert-provided programs. Given the set of
all possible augmentations, we deﬁne Λ to be the subset of
augmentations that are attribute-preserving: that is, for all
λj in the set of programs, λj(τ ) = λj(Λm(τ )) for some
augmentation Λm ∈ Λ. An example of a valid augmenta-
tion in the mouse domain is reﬂection of the trajectory data.
All losses presented above can be extended with data
augmentation, by replacing τ with Λm(τ ) in losses. For
contrastive loss, adding data augmentation corresponds to
extending the batch size to 2B, with B samples from the
original and augmented trajectories.

The augmentations we use in our experiments are reﬂec-
tions, rotations, translations, and a small Gaussian noise on
the keypoints (mouse data only). In practice, we add the
loss for each decoder with and without data augmentation.

Similar to Eq.
however any type of λ may be used.

(2), we show the loss for categorical λj,

4. Experiments

4.1. Datasets

3.3.3 Contrastive Loss

Lastly, the programmed tasks can be used to supervise con-
trastive learning of our representation. For a trajectory τi,
and for each λj, positive examples are those trajectories
with the same attribute class under λj. For λj with contin-
uous outputs, we create a discretized ˆλj in which we apply
ﬁxed thresholds to divide the output space into classes. For
our work, we apply two thresholds for each program such
that our classes are approximately equal in size.

We apply a shallow decoder g to the learned representa-
tion, and let g = g(qφ(zµ|τ )) represent the decoded repre-

We work with datasets from behavioral neuroscience,
where there are large-scale, expert-annotated datasets from
scientiﬁc experiments. We study behavior for the labora-
tory mouse and the fruit ﬂy, two of the most common model
organisms in behavioral neuroscience. For each organism,
we ﬁrst train TREBA using large unannotated datasets: for
the mouse domain we use an in-house dataset comprised of
approximately 100 hours of recorded diadic social interac-
tions (Mouse100), while for the ﬂy domain we use the Fly
vs. Fly dataset [15] without annotations.

After pre-training TREBA, we evaluate the suitability of
our trajectory representation for supervised behavior clas-

5

siﬁcation (classifying frame-level behaviors on continuous
trajectory data), on three additional datasets:

MARS. The MARS dataset [36] is a recently released
mouse social behavior dataset collected in the same condi-
tions as Mouse100. The dataset is annotated by neurobiol-
ogists on a frame-by-frame basis for three behaviors: sniff,
attack, and mount. We use the provided train, validation,
and test split (781k, 352k, and 184k frames respectively).
Trajectories are extracted by the MARS tracker [36].

CRIM13. CRIM13 [4] is a second mouse social behav-
ior dataset manually annotated on a frame-by-frame basis
by experts. To extract trajectories, we use a version of the
the MARS tracker [36] ﬁne-tuned on pose annotations on
CRIM13. We select a subset of videos from which trajecto-
ries can be reliably detected for a train, validation and test
split of 407k, 96k, and 142k frames respectively. We eval-
uated classiﬁer performance on the same three behaviors
studied in MARS (sniff, attack, mount).

CRIM13 is a useful test of the robustness of TREBA
trained on Mouse100, as the recording conditions in
CRIM13 (image resolution 640 × 480, frame rate 25Hz,
and non-centered cage location) are different from those of
Mouse100 (image resolution 1024 × 570, frame rate 30Hz,
and centered cage location).

Fly vs. Fly (Fly). We use the Aggression and Courtship
videos from the Fly dataset [15]. These videos record in-
teractions between a pair of ﬂies annotated on a frame-by-
frame basis for social behaviors by domain experts. Our
train, validation and test split has 1067k, 162k, 322k frames
respectively. We use the trajectories tracked by [15] and
evaluate on all behaviors with more than 1000 frames of an-
notations in the full training set (lunge, wing threat, tussle,
wing extension, circle, copulation).

4.2. Training and Evaluation Procedure

We use the attribute consistency loss (Section 3.3.1) and
contrastive loss (Section 3.3.3) to train TREBA using pro-
grams. With the same programs, we ﬁnd that different loss
combinations result in similar performance, and that the
combination of consistency and contrastive losses performs
the best overall. The results for all loss combinations are
provided in the Supplementary Material.

For the datasets in the mouse domain (MARS and
CRIM13) we train TREBA on Mouse100, with 10 programs
provided by mouse behavior domain experts. For the Fly
dataset, we train TREBA on the training split of Fly with-
out annotations, with 13 programs provided by ﬂy behavior
domain experts. The full list is in Table 1. We then use
the trained encoder, with pre-trained frozen weights, as a
trajectory feature extractor over T = 21 frames, where the
representation for each frame is computed using ten frames
before and after the current frame.

features, using Mean Average Precision (MAP). We com-
pute the mean over behaviors of interest with equal weight-
ing. Our classiﬁers are shallow fully-connected neural net-
works on the input features. To determine the relation-
ship between classiﬁer performance and training set size,
we sub-sample the training data by randomly sampling tra-
jectories (with lengths of 100 frames) to achieve a desired
fraction of the training set size. Sampling was performed to
achieve a similar class distribution as the full training set.
We train each classiﬁer nine times over three different ran-
dom selections of the training data for each training fraction
(1%, 2%, 5%, 10%, 25%, 50%, 75%, 100%). Additional
implementation details are in the Supplementary Material.

4.3. Main Results

We evaluate the data efﬁciency of our representation
for supervised behavior classiﬁcation, by training a clas-
siﬁer to predict behavior labels given both our learned
representation and one of either (1) raw keypoints or
(2) domain-speciﬁc features designed by experts. The
TREBA+keypoints evaluation allows us to test the effec-
tiveness of our representation without other hand-designed
features, while the TREBA+features evaluation is closer to
most potential use cases. The domain-speciﬁc features for
mice are the trajectory features from [36] and features for
ﬂies are the trajectory features from [4]. The input features
are a superset of the programs we use in Table 1.

Our representation is able to improve the data efﬁciency
for both keypoints and domain-speciﬁc features, over all
evaluated amounts of training data availability (Figure 4).
We discuss each dataset below:

MARS. Our representation signiﬁcantly improves clas-
siﬁcation performance over keypoints alone (Figure 4 A1).
We achieve the same performance as the full baseline train-
ing using only between 1% and 2% of the data. While
this result is partially because our representation contains
temporal information, we can also observe a signiﬁcant in-
crease in data efﬁciency in A2 compared to domain-speciﬁc
features, which also contains temporal features. Classi-
ﬁers using TREBA has the same performance as the full
baseline training set with around 5% ∼ 10% of data (i.e.,
10× ∼ 20× improved annotation efﬁciency).

CRIM13. We test the transfer learning ability of our
representation on CRIM13, a dataset with different image
properties than Mouse100, the training set of TREBA. Our
representation achieves the same performance as the base-
line training with keypoints using around 5% to 10% of the
training data (Figure 4 B1). With domain-speciﬁc features,
TREBA uses 50% of the data annotation to have the same
performance as the full training baseline (i.e., 2× improved
annotation efﬁciency). Our representation is able to gener-
alize to a different dataset of the same organism.

We evaluate our classiﬁers, with and without TREBA

Fly. When using keypoints only, our representation re-

6

Figure 4. Data Efﬁciency for Supervised Classiﬁcation. Training data fraction vs. classiﬁer error on MARS (left), CRIM13 (middle)
and ﬂy (right). The blue lines represent performance with baseline keypoints and features, and the orange lines are with TREBA. The
shaded regions correspond to the classiﬁer standard deviation over nine repeats. The gray dotted line marks the best observed classiﬁer
performance when trained on the baseline features (using the full training set). Note the log scale on both the x and y axes.

quires 10% of the data (Figure 4 C1) and for features, our
representation requires 50% of the data (Figure 4 C2) to
achieve the same performance as full baseline training. This
corresponds to 2× improved annotation efﬁciency.

4.4. Model Ablations

We perform the following model ablations to better char-
acterize our approach. In this section, percentage error re-
duction relative to baseline is averaged over all training frac-
tions. Additional results are in the Supplementary Material.
Varying Programmed Tasks. We test the performance
of TREBA trained with each single program provided by
the domain experts in Table 1, and the average, best, and
worst performance is visualized in Figure 5. On average,
representations learned from a single program is better than
using features alone, but using all provided programs fur-
ther improves performance.

For a single program, there could be a large variation in
performance depending on the selected program (Figure 5).
While the best performing single program is close in classi-
ﬁer MAP to using all programs, the worst performing pro-
gram may increase error, as in MARS and CRIM13. We
further tested the performance using more programs.

In the mouse domain, we found that with three ran-
domly selected programs, the variation between runs is
much smaller compared to single programs (Supplementary
Material). With three programs, we achieve comparable av-
erage error reduction from baseline features to using all pro-

grams (MARS: 14.6% error reduction for 3 programs vs.
15.3% for all, CRIM13: 9.2% for 3 programs vs. 9.5% for
all). For the ﬂy domain, we found that we needed seven
programs to achieve comparable performance (20.7% for 7
programs vs. 21.2% for all).

Varying Decoder Losses. When the programmed tasks
are ﬁxed, decoder losses with different combinations of
consistency (Section 3.3.1), decoding (Section 3.3.2), and
contrastive (Section 3.3.3) loss are similar in performance
(Supplementary Material). Additionally, we evaluate the
TREBA framework without programmed tasks, with de-
coder tasks using trajectory generation and unsupervised
contrastive loss. While self-supervised representations are
also effective at reducing baseline error, we achieve the
best classiﬁer performance using TREBA with programmed
tasks (Table 2). Furthermore, we found that training trajec-
tory representations without self-decoding, using the con-
trastive loss from [7, 8], resulted in less effective represen-
tations for classiﬁcation (Supplementary Material).

Data Augmentation. We removed the losses using the
data augmentation described in Section 3.3.4, and found
that performance was slightly lower for all datasets than
with augmentation. In particular, adding data augmentation
decreases error by 1.2% on MARS, 2.5% on CRIM13, and
5.3% on Fly compared to without data augmentation.

Pre-Training Variations The results shown for MARS
was obtained with pre-training TREBA on Mouse100, a
large in-house mouse dataset with the same image prop-

7

8×1021013×1015×1017×1019×101Error (Log Scale)A1. MARS Keypoints Data EfficiencyKeypointsKeypoints + TREBAB1. CRIM13 Keypoints Data EfficiencyC1. Fly Keypoints Data Efficiency102101100Training Data Fraction (Log Scale)8×1021013×1015×1017×1019×101Error (Log Scale)A2. MARS Features Data EfficiencyFeaturesFeatures + TREBA102101100Training Data Fraction (Log Scale)B2. CRIM13 Features Data Efficiency102101100Training Data Fraction (Log Scale)C2. Fly Features Data EfficiencyFigure 5. Varying Programmed Tasks. Effect of varying number of programmed tasks on classiﬁer data efﬁciency. The shaded region
corresponds to the best and worst classiﬁers trained using a single programmed task from Table 1. The grey dotted line corresponds to the
value where the baseline features achieve the best performance (using the full training set).

Decoder Loss
TVAE
TVAE+
Unsup. Contrast
TVAE+
Contrast+Consist

Decoder Loss
TVAE
TVAE+
Unsup. Contrast
TVAE+
Contrast+Consist

Keypoint Error Reduction (%)
CRIM13
34.7 ± 1.5

MARS
52.2 ± 4.0

Fly
15.4 ± 2.1

52.6 ± 3.9

37.4 ± 2.4

20.9 ± 1.7

55.1 ± 3.0

41.1 ± 2.1

33.7 ± 1.2

Features Error Reduction (%)
CRIM13
8.2 ± 4.6

MARS
13.7 ± 1.8

Fly
11.7 ± 4.7

14.3 ± 2.2

8.9 ± 4.1

16.1 ± 1.7

15.3 ± 2.1

9.5 ± 3.8

21.2 ± 4.5

Table 2. Decoder Error Reductions. Percentage error reduc-
tion relative to baseline keypoints and domain-speciﬁc features for
training with different decoder losses for TREBA. The average is
taken over all evaluated training fractions.

erties as MARS. Figure 6 demonstrates the effect of vary-
ing TREBA training data amount with TVAE only and with
programs. For both keypoints and features, we observe that
TVAE (MARS) has the largest error. We see that error can
be decreased by either adding more data (features + TVAE
(Mouse100) with 3.9% decrease) or adding task program-
ming (features + Programs (MARS) with 4.4% decrease).
Adding both more data and task programming results in an
average decrease of 5.7% error relative to TVAE (MARS)
and the lowest average error.

5. Conclusion

We introduce a method to learn an annotation-sample
efﬁcient Trajectory Embedding for Behavior Analysis
(TREBA). To train this representation, we study self-
supervised decoder tasks as well as decoder tasks with pro-
grammatic supervision, the latter created using task pro-
gramming. Our results show that TREBA can reduce anno-
tation requirements by a factor of 10 for mice and 2 for ﬂies.
Our experiments on three datasets (two in mice and one in
fruit ﬂies) suggest that our approach is effective across dif-
ferent domains. TREBA is not restricted to animal behavior

Figure 6. Pre-Training Data Variations. Effect of varying pre-
training data on classiﬁer data efﬁciency for the MARS dataset.
“TVAE” corresponds to training TREBA with TVAE losses only,
and “Programs” corresponds to training with all programs.

and may be applied to other domains where tracking data is
expensive to annotate, such as in sports analytics.

Our experiments highlight, and quantify, the tradeoff be-
tween task programming and data annotation. The choice
of which is more effective will depend on the cost of anno-
tation and the level of expert understanding in identifying
behavior attributes. Directions in creating tools to facilitate
program creation and data annotation will help further ac-
celerate behavioral studies.

6. Acknowledgements

We would like to thank Tomomi Karigo at Caltech
for providing the mouse dataset. The Simons Foundation
(Global Brain grant 543025 to PP) generously supported
this work, and this work is partially supported by NIH
Award #K99MH117264 (to AK), NSF Award #1918839 (to
YY), and NSERC Award #PGSD3-532647-2019 (to JJS).

8

102101100Training Data Fraction (Log Scale)1013×1015×1012×1014×101Error (Log Scale)MARS Features with Program VariationsFeaturesFeatures + TREBA (1 program)Features + TREBA (10 programs)102101100Training Data Fraction (Log Scale)CRIM13 Features with Program Variations102101100Training Data Fraction (Log Scale)Fly Features with Program Variations2×1013×1014×101Error (Log Scale)MARS Keypoints with Pre-Training VariationsKeypoints + TVAE (MARS)Keypoints + TVAE (Mouse100)Keypoints + Programs (MARS)Keypoints + Programs (Mouse100)102101100Training Data Fraction (Log Scale)1.2×1011.4×1011.6×1011.8×1012×1012.2×1012.4×101Error (Log Scale)MARS Features with Pre-Training VariationsFeatures + TVAE (MARS)Features + TVAE (Mouse100)Features + Programs (MARS)Features + Programs (Mouse100)References

[1] David J Anderson and Pietro Perona. Toward a science of
computational ethology. Neuron, 84(1):18–31, 2014. 1, 2
[2] Gordon J Berman, Daniel M Choi, William Bialek, and
Joshua W Shaevitz. Mapping the stereotyped behaviour of
freely moving fruit ﬂies. Journal of The Royal Society Inter-
face, 11(99):20140672, 2014. 2

[3] Brian Broll, Matthew Hausknecht, Dave Bignell, and Adith
Swaminathan. Customizing scripted bots: Sample efﬁcient
imitation learning for human-like behavior in minecraft. 1, 2
[4] Xavier P Burgos-Artizzu, Piotr Doll´ar, Dayu Lin, David J
Anderson, and Pietro Perona. Social behavior recognition in
continuous video. In 2012 IEEE Conference on Computer
Vision and Pattern Recognition, pages 1322–1329. IEEE,
2012. 2, 6

[5] Adam J Calhoun, Jonathan W Pillow, and Mala Murthy.
Unsupervised identiﬁcation of the internal states that shape
natural behavior. Nature neuroscience, 22(12):2040–2049,
2019. 2

[6] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-
jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Pe-
ter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse:
In Proceed-
3d tracking and forecasting with rich maps.
ings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 8748–8757, 2019. 1, 2

[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. ICML, 2020. 2, 5, 7, 15

[8] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey Hinton. Big self-supervised mod-
arXiv preprint
els are strong semi-supervised learners.
arXiv:2006.10029, 2020. 7, 15

[9] John D Co-Reyes, YuXuan Liu, Abhishek Gupta, Ben-
jamin Eysenbach, Pieter Abbeel, and Sergey Levine. Self-
consistent trajectory autoencoder: Hierarchical reinforce-
ment learning with trajectory embeddings. arXiv preprint
arXiv:1806.02813, 2018. 3

[10] Anthony I Dell, John A Bender, Kristin Branson, Iain D
Couzin, Gonzalo G de Polavieja, Lucas PJJ Noldus, Al-
fonso P´erez-Escudero, Pietro Perona, Andrew D Straw, Mar-
tin Wikelski, et al. Automated image-based tracking and
its application in ecology. Trends in ecology & evolution,
29(7):417–428, 2014. 1

[11] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-
vised visual representation learning by context prediction. In
Proceedings of the IEEE international conference on com-
puter vision, pages 1422–1430, 2015. 2

[12] Carl Doersch and Andrew Zisserman. Multi-task self-
supervised visual learning. In Proceedings of the IEEE Inter-
national Conference on Computer Vision, pages 2051–2060,
2017. 3

[13] SE Roian Egnor and Kristin Branson. Computational analy-
sis of behavior. Annual review of neuroscience, 39:217–236,
2016. 2

[14] Eyrun Eyjolfsdottir, Kristin Branson, Yisong Yue, and Pietro
Perona. Learning recurrent representations for hierarchical
behavior modeling. ICLR, 2017. 2

[15] Eyrun Eyjolfsdottir, Steve Branson, Xavier P Burgos-
Artizzu, Eric D Hoopfer, Jonathan Schor, David J Anderson,
and Pietro Perona. Detecting social actions of fruit ﬂies. In
European Conference on Computer Vision, pages 772–787.
Springer, 2014. 2, 4, 5, 6, 11, 14, 15

[16] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un-
supervised representation learning by predicting image rota-
tions. ICLR, 2018. 2

[17] Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan
Misra. Scaling and benchmarking self-supervised visual rep-
In Proceedings of the IEEE Interna-
resentation learning.
tional Conference on Computer Vision, pages 6391–6400,
2019. 2

[18] Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Ben-
jamin Koger, Blair R Costelloe, and Iain D Couzin. Deep-
posekit, a software toolkit for fast and robust animal pose
estimation using deep learning. Elife, 8:e47994, 2019. 2
[19] Tengda Han, Weidi Xie, and Andrew Zisserman. Video rep-
resentation learning by dense predictive coding. In Proceed-
ings of the IEEE International Conference on Computer Vi-
sion Workshops, pages 0–0, 2019. 2

[20] Katja Hofmann. Minecraft as ai playground and labora-
tory. In Proceedings of the Annual Symposium on Computer-
Human Interaction in Play, pages 1–1, 2019. 1, 2

[21] Weizhe Hong, Ann Kennedy, Xavier P Burgos-Artizzu,
Moriel Zelikowsky, Santiago G Navonne, Pietro Perona, and
David J Anderson. Automated measurement of mouse social
behaviors using depth sensing, video tracking, and machine
learning. Proceedings of the National Academy of Sciences,
112(38):E5351–E5360, 2015. 2

[22] Alexander I Hsu and Eric A Yttri. B-soid: An open source
unsupervised algorithm for discovery of spontaneous behav-
iors. bioRxiv, page 770271, 2020. 2

[23] Mayank Kabra, Alice A Robie, Marta Rivera-Alba, Steven
Branson, and Kristin Branson. Jaaba: interactive machine
learning for automatic annotation of animal behavior. Nature
methods, 10(1):64, 2013. 2

[24] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
arXiv
Dilip Krishnan. Supervised contrastive learning.
preprint arXiv:2004.11362, 2020. 5

[25] Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Re-
visiting self-supervised visual representation learning.
In
Proceedings of the IEEE conference on Computer Vision and
Pattern Recognition, pages 1920–1929, 2019. 2, 3

[26] Kevin Luxem, Falko Fuhrmann, Johannes K¨ursch, Ste-
Identifying behavioral struc-
fan Remy, and Pavol Bauer.
ture from deep variational embeddings of animal motion.
bioRxiv, 2020. 1, 2

[27] Alexander Mathis, Pranav Mamidanna, Kevin M Cury, Taiga
Abe, Venkatesh N Murthy, Mackenzie Weygandt Mathis,
and Matthias Bethge. Deeplabcut: markerless pose estima-
tion of user-deﬁned body parts with deep learning. Nature
neuroscience, 21(9):1281–1289, 2018. 2

[28] Simon RO Nilsson, Nastacia L Goodwin, Jia J Choong,
Sophia Hwang, Hayden R Wright, Zane Norville, Xiaoyu
Tong, Dayu Lin, Brandon S Bentzley, Neir Eshel, et al. Sim-
ple behavioral analysis (simba): an open source toolkit for

9

[41] Alexander B Wiltschko, Matthew J Johnson, Giuliano Iurilli,
Ralph E Peterson, Jesse M Katon, Stan L Pashkovski, Vic-
toria E Abraira, Ryan P Adams, and Sandeep Robert Datta.
Mapping sub-second structure in mouse behavior. Neuron,
88(6):1121–1135, 2015. 2

[42] Raymond A Yeh, Alexander G Schwing, Jonathan Huang,
and Kevin Murphy. Diverse generation for multi-agent sports
games. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 4610–4619, 2019. 1,
2

[43] Eric Zhan, Albert Tseng, Yisong Yue, Adith Swaminathan,
and Matthew Hausknecht. Learning calibratable policies us-
ing programmatic style-consistency. ICML, 2020. 1, 3, 5,
14

[44] Eric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and
Patrick Lucey. Generating multi-agent trajectories using pro-
grammatic weak supervision. ICLR, 2019. 3

[45] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
In Computer Vision
consistent adversarial networkss.
(ICCV), 2017 IEEE International Conference on, 2017. 3

computer classiﬁcation of complex social behaviors in ex-
perimental animals. BioRxiv, 2020. 2, 4

[29] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748, 2018. 2, 3, 5

[30] AJ Piergiovanni, Anelia Angelova, and Michael S Ryoo.
Evolving losses for unsupervised video representation learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 133–142, 2020.
3

[31] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative
pre-training, 2018. 3

[32] Alexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason
Fries, Sen Wu, and Christopher R´e. Snorkel: Rapid training
data creation with weak supervision. In Proceedings of the
VLDB Endowment. International Conference on Very Large
Data Bases, volume 11, page 269. NIH Public Access, 2017.
3

[33] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel
Selsam, and Christopher R´e. Data programming: Creating
large training sets, quickly. In Advances in neural informa-
tion processing systems, pages 3567–3575, 2016. 2, 3, 4
[34] Mirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel
Swietojanski, Joao Monteiro, Jan Trmal, and Yoshua Bengio.
Multi-task self-supervised learning for robust speech recog-
In ICASSP 2020-2020 IEEE International Confer-
nition.
ence on Acoustics, Speech and Signal Processing (ICASSP),
pages 6989–6993. IEEE, 2020. 3

[35] Aaqib Saeed, Tanir Ozcelebi, and Johan Lukkien. Multi-task
self-supervised learning for human activity detection. Pro-
ceedings of the ACM on Interactive, Mobile, Wearable and
Ubiquitous Technologies, 3(2):1–30, 2019. 3

[36] Cristina Segalin, Jalani Williams, Tomomi Karigo, May
Hui, Moriel Zelikowsky, Jennifer J. Sun, Pietro Perona,
David J. Anderson, and Ann Kennedy. The mouse ac-
tion recognition system (mars): a software pipeline for
automated analysis of social behaviors in mice. bioRxiv
https://doi.org/10.1101/2020.07.26.222299, 2020. 1, 2, 4,
6, 11, 14, 15

[37] Abhinav Shukla, Stavros Petridis, and Maja Pantic. Does
visual self-supervision improve learning of speech represen-
tations? arXiv preprint arXiv:2005.01400, 2020. 3

[38] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and
Cordelia Schmid. Videobert: A joint model for video and
language representation learning. In Proceedings of the IEEE
International Conference on Computer Vision, pages 7464–
7473, 2019. 2, 3

[39] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien
Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,
Yuning Chai, Benjamin Caine, et al. Scalability in perception
for autonomous driving: Waymo open dataset. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 2446–2454, 2020. 1, 2

[40] Ziyu Wang, Josh S Merel, Scott E Reed, Nando de Freitas,
Gregory Wayne, and Nicolas Heess. Robust imitation of di-
verse behaviors. In Advances in Neural Information Process-
ing Systems, pages 5320–5329, 2017. 3

10

Supplementary Material

We provide additional details and experimental results from task programming and TREBA.

• Section A describes the programs we use in the mouse and ﬂy domain (Section A.1) as well as experimental results with

varying number of programs (Section A.2).

• Section B provides implementation details on the representation learning architecture for TREBA and behavior classiﬁ-

cation models.

• Section C contains experimental results for decoder loss variations, time estimates, and classiﬁcation samples.

A. Program Details and Experiments

A.1. Program Details

Programs for the Mouse Domain. We provide additional details on the programs listed in Table 1 in Section 3 of the
paper. For the datasets in the mouse domain, programs are selected by domain experts based on the features used for mouse
behavior classiﬁcation in [36]. The experiments are recorded for a standard resident-intruder assay, where an intruder mouse
is introduced to the cage of the resident mouse. Mouse 1 corresponds to the resident mouse and mouse 2 corresponds to
the intruder mouse. These features are based on the anatomically deﬁned keypoints tracked by the MARS tracker for each
mouse: the nose, the ears, the base of the neck, the hips, and the base of the tail. A subset of the programs for the mouse
domain is visualized in Figure 7, and all the programs we use for the mouse domain are listed below.

• Facing angle: Relative angle between orientation of the body of the mouse to the line connecting centroids of the two
mice. The facing angle is computed for both mice. This describe how closely one mouse is facing the other mouse.

• Speed: Change in position of the centroid of the mouse across consecutive frames. The speed is computed for both

mice. This property is especially important for helping identify aggressive behavior.

• Distance between nose of mouse 1 and 2: Distance between the nose keypoints of each mouse. Distance between noses

can be used for identifying when the mice are interacting during social behavior such as sniff.

• Distance between nose of mouse 1 and tail of mouse 2: Distance between the nose keypoint of mouse 1 and base of tail
keypoint of mouse 2. Distance between nose of mouse 1 and tail of mouse 2 can be used to identify when the mice are
interacting during social behavior such as sniff.

• Head-Body Angle: For one mouse, the angle formed by the nose, neck, and base of tail keypoints. This angle is
computed for each mouse. This attribute helps describe the body shape of the mouse, since it varies with changes to the
relative orientation of the head and body of each mouse.

• Nose Movement: Nose movement of each mouse measured by speed relative to the movement of the centroid. This is
computed for each mouse. This attribute describes the nose and head speed with respect to the center of the mouse, and
can help identify aggressive behavior.

Programs for the Fly Domain. For the datasets in the ﬂy domain, these programs are selected by domain experts based
on the features used for ﬂy behavior classiﬁcation in [15]. For each ﬂy, the ﬂy tracker tracks the centroid of the body with a
ﬁtted ellipse for the body, the left wingtip keypoint and the right wingtip keypoint. A subset of the programs is visualized in
Figure 8 and all the programs we use for the ﬂy domain are listed below.

• Angular speed: Change in heading direction of the ﬂy across consecutive frames based on the ﬂy body ellipse. The
angular speed is computed for all ﬂies. This attribute describes how fast the ﬂy is turning and can help identify behaviors
such as tussle and circle.

• Minimum and maximum wing angles: The wing angle is the angle between the wing tip keypoint, the centroid, and
the point on the back of the body ellipse. Programs are used to compute both the minimum and maximum wing angles
for each ﬂy. The wing attributes are especially important behaviors deﬁned by wing position and motion, such as wing
extension and wing threat.

11

Figure 7. Visualizing behavior attributes for mouse dataset.

Figure 8. Visualizing behavior attributes for ﬂy dataset.

• Facing angle: Relative angle between orientation of the body of the ﬂy to the line connecting centroids of the two ﬂies.
The facing angle is computed for both ﬂies. The facing angle helps identify if a ﬂy is facing in the direction of the other
ﬂy.

• Speed: Change in position of the centroid of the ﬂy across consecutive frames. The speed is computed for all ﬂies. This

property helps identify behaviors such as lunge, which usually has high speed.

• Distance between centroid of ﬂy 1 and 2: The distance between ﬂies is often a good attribute to determine if the ﬂies are

interacting during social behavior.

• Ratio between the major and minor axis: The ratio between the major and minor axis length of the ﬂy body ellipse. This
is computed for each ﬂy. This attribute is a useful description of the body shape of the ﬂy. When the ﬂy is tilting up, the
ratio is usually smaller, and when the ﬂy is ﬂat against the surface, the ratio is usually larger.

A.2. Program Performance Results

We evaluate the representation learned using task programming for each individual program for the mouse and ﬂy domains
(Table 3 to Table 8). The evaluation procedure is the same as the main paper, where the MAP is averaged over nine runs
with three random selection for each training fraction. MAP@k% corresponds to the classiﬁer MAP supervised with k%
of the training data. We note that average error reduction discussed in this section is computed across all evaluated training
fractions from the main paper (1%, 2%, 5%, 10%, 25%, 50%, 100%).

Mouse Program Evaluations. We train TREBA with each individual program from the mouse domain on Mouse100
using the consistency and contrastive losses, and evaluate the performance of domain-speciﬁc features+TREBA on MARS
(Table 3) and CRIM13 (Table 5). In general, TREBA trained on a single program improves classiﬁer performance, except for
the bottom three programs and there is a high variation in performance (for example, Nose Movement Mouse 1 vs. Distance
Nose-Nose). The two distance attributes, Head-Body Angle of Mouse 1, and Facing Angle of Mouse 1 generally performs
the best across MARS and CRIM13 as a single program.

The best performing single program, Head-Body Angle Mouse 1 for MARS (Table 3) and Distance Nose-Nose for
CRIM13 (Table 5), is comparable in performance to using all programs. When comparing average error reduction from
baseline hand-designed features on MARS, TREBA using the top single program (Head-Body Angle Mouse 1, Table 3)
achieves an error reduction of 14.5% and using all programs achieves an error reduction of 15.3%. On CRIM13, the top sin-
gle program (Distance Nose-Nose, Table 5) achieves an error reduction of 9.3% and all programs achieves an error reduction

12

BFacing Angle (Mouse 1)Nose-Nose DistanceNose-Tail DistanceHead-Body Angle (Mouse 1)Head-Body Angle (Mouse 2)Facing Angle (Fly 1)Distance CentroidMax Wing Angle (Fly 1)Max Wing Angle (Fly 2)Axis Ratio (Fly 1)Axis Ratio (Fly 2)Domain-speciﬁc features

10

MAP@k%

100
50
0.824 0.838 0.847
+ Head-Body Angle Mouse 1 0.853 0.868 0.874
0.849 0.866 0.872
+ Facing Angle Mouse 1
0.849 0.866 0.868
+ Distance Nose-Nose
0.847 0.866 0.870
+ Distance Nose-Tail
0.851 0.866 0.872
+ Speed Mouse 2
+ Head-Body Angle Mouse 2 0.846 0.866 0.872
0.847 0.862 0.874
+ Speed Mouse 1
0.818 0.841 0.851
+ Nose Movement Mouse 1
0.814 0.834 0.843
+ Facing Angle Mouse 2
0.820 0.836 0.843
+ Nose Movement Mouse 2

+ All Programs

0.853 0.868 0.877

Table 3. Single Program Variations on MARS. Average MAP of
classiﬁers on MARS trained with features and with TREBA using
the speciﬁed single program. The order of the programs are based
on the average error reduction over all training fractions (highest
error reduction at the top).

10

MAP@k%

100
50
0.792 0.858 0.873
Domain-speciﬁc features
0.811 0.876 0.889
+ Distance Nose-Nose
0.807 0.881 0.891
+ Distance Nose-Tail
0.810 0.875 0.891
+ Speed Mouse 2
0.811 0.879 0.890
+ Facing Angle Mouse 1
+ Head-Body Angle Mouse 1 0.809 0.880 0.889
0.808 0.876 0.889
+ Speed Mouse 1
+ Head-Body Angle Mouse 2 0.802 0.870 0.881
0.767 0.851 0.868
+ Nose Movement Mouse 2
0.765 0.852 0.869
+ Nose Movement Mouse 1
0.764 0.848 0.865
+ Facing Angle Mouse 2

+ All Programs

0.808 0.876 0.888

Table 5. Single Program Variations on CRIM13. Average MAP
of classiﬁers on CRIM13 trained with features and with TREBA
using the speciﬁed single program. The order of the programs
are based on the average error reduction over all training fractions
(highest error reduction at the top).

10

50

MAP@k%

100
Domain-speciﬁc features 0.824 0.838 0.847
0.856 0.865 0.872
0.850 0.866 0.872
0.855 0.864 0.878

+ 3 programs (A)
+ 3 programs (B)
+ 3 programs (C)

Additional Program Variations on MARS. Aver-
Table 4.
age MAP of classiﬁers on MARS trained with features and with
TREBA using the three programs. The programs are: (A) Nose
Movement Mouse 1, Nose Movement Mouse 2, Facing Angle
Mouse 2; (B) Facing Angle Mouse 1, Head-Body Angle Mouse
1, Head-Body Angle Mouse 2; (C) Speed Mouse 1, Nose Move-
ment 1, Distance Nose-Nose.

10

50

MAP@k%

100
Domain-speciﬁc features 0.792 0.858 0.873
0.811 0.879 0.889
0.810 0.878 0.890
0.807 0.877 0.887

+ 3 programs (A)
+ 3 programs (B)
+ 3 programs (C)

Table 6. Additional Program Variations on CRIM13. Aver-
age MAP of classiﬁers on MARS trained with features and with
TREBA using the three programs. The programs are: (A) Nose
Movement Mouse 1, Nose Movement Mouse 2, Facing Angle
Mouse 2; (B) Facing Angle Mouse 1, Head-Body Angle Mouse
1, Head-Body Angle Mouse 2; (C) Speed Mouse 1, Nose Move-
ment 1, Distance Nose-Nose.

of 9.5%. In contrast, the worst performing single program may reduce performance in the mouse domain. We study whether
this performance variance can be reduced by adding more programs.

We experiment training TREBA using sets of three programs and evaluating behavior classiﬁcation performance (MARS
in Table 4, CRIM13 in Table 6). We note that program sets B and C are three randomly selected programs from the full mouse
program list, and program set A consists of the worst performing three single programs. Despite program set A consisting
of the lowest performing single programs, we see that for both MARS and CRIM13, the different sets of three programs are
similar in performance and is comparable to using all programs (Table 4, Table 6). By training TREBA with three programs
instead of one, the performance variation across program sets is much lower. We recommend that domain experts train with
multiple programs, unless the best performing single program is known.

Fly Program Evaluations. We train TREBA on the Fly dataset without annotations using individual programs from the ﬂy
domain and evaluate on the Fly dataset (Table 7). Training with TREBA with any single expert-engineered program improves
performance in the ﬂy domain. We see that the speed and wing angle features generally perform the best. The top performing
single program (Min. Wing Angle Fly 1, Table 7) achieves 19.4% average error reduction over baseline features, comparing
to 21.2% using all programs. Similar to the mouse domain, if the best performing program is known ahead of time, we can
achieve comparable performance to training TREBA using all programs. However, the best and worst single programs have
a large variance, and we experiment with adding more programs.

13

Domain-speciﬁc features

10

MAP@k%

100
50
0.774 0.829 0.868
0.820 0.864 0.885
+ Min. Wing Angle Fly 1
0.818 0.856 0.878
+ Speed Fly 1
0.804 0.861 0.880
+ Speed Fly 2
0.821 0.862 0.881
+ Angular Speed Fly 1
0.814 0.859 0.882
+ Max. Wing Angle Fly 2
0.814 0.859 0.886
+ Min. Wing Angle Fly 2
+ Distance Between Centroids 0.815 0.858 0.882
0.814 0.862 0.881
+ Facing Angle Fly 1
0.809 0.855 0.882
+ Axis Ratio Fly 1
0.811 0.853 0.879
+ Angular Speed Fly 2
0.811 0.853 0.876
+ Facing Angle Fly 2
0.811 0.855 0.883
+ Max. Wing Angle Fly 1
0.797 0.852 0.880
+ Axis Ratio Fly 2

+ All Programs

0.820 0.868 0.886

Table 7. Single Program Variations on Fly. Average MAP of
classiﬁers on Fly trained with features and with TREBA using the
speciﬁed single program. The order of the programs are based on
the average error reduction over all training fractions (highest error
reduction at the top).

10

50

MAP@k%

100
Domain-speciﬁc features 0.774 0.829 0.868
0.814 0.857 0.880
0.814 0.857 0.878
0.815 0.863 0.880
0.819 0.869 0.889
0.820 0.863 0.885
0.815 0.860 0.882

+ 3 programs (A)
+ 3 programs (B)
+ 3 programs (C)
+ 7 programs (A)
+ 7 programs (B)
+ 7 programs (C)

Table 8. Additional Program Variations on Fly. Average MAP
of classiﬁers on Fly trained with features and with TREBA using
three and seven programs. The sets of three programs are: (A)
Speed Fly 1, Min/Max Wing Angle Fly 1; (B) Speed Fly 2, Facing
Angle Fly 1, Axis Ratio Fly 2; (C) Min Wing Angle Fly 2, Speed
Fly 1, Axis Ratio Fly 1.
The sets of seven programs are: (A) Distance, Angular Speed Fly
1/2, Max Wing Angle Fly 1, Min/Max Wing Angle Fly 2, Facing
Angle Fly 1; (B) Distance, Speed/Angular Speed Fly 1, Max Wing
Angle Fly 1, Facing Angle Fly 2, Axis Ratio Fly 1/2; (C) Speed
Fly 2, Min/Max Wing Angle Fly 1/2, Facing Angle Fly 1/2.

Dataset
Mouse100
Fly

Batch size
128
128

z-dim Encoder Units Decoder Units

Temperature t
0.07
0.07
Table 9. Hyperparameters for Representation Learning.

256
256

256
256

32
32

Learning Rate
0.0002
0.0002

We start by training on sets of three programs, and found that there is a gap in performance to using all programs (Table 8).
We additionally experiment with sets of seven programs to close this performance gap. Training with randomly selected three
or seven programs (Table 8) has much smaller variations across program selections compared to single programs (Table 7).
For the ﬂy domain, we found that training with seven programs is able to achieve comparable performance to using all
programs.

B. Additional Implementation Details

We provide hyperparameters used in training TREBA (Table 9) and the classiﬁcation models (Table 10). Our code is

available at https://github.com/neuroethology/TREBA.

For training TREBA, the TVAE consists of a bi-directional GRU with 256 units for the encoder, followed by linear
layers, with a latent dimension of 32. We take the encoding mean from the encoder, zµ, as our learned representation of
the trajectory. The decoder for the self-decoding task is also a GRU with 256 units followed by linear layers to predict the
state in the next timestamp (by predicting the change from the current state to the next state). The decoder for the other tasks
(attribute decoding, contrastive loss) consists of a fully connected neural network with 32 units. For the attribute consistency
loss, we use the method proposed in [43] to train a 256 unit GRU to approximate non-differentiable programs. When the
corresponding decoder is used, we weigh the consistency loss by 1.0, contrastive loss by 10.0, and the decoding loss by 1.0.
We train TREBA using Adam optimizer with a learning rate of 0.0002. The input trajectories to the TREBA model are the
detected keypoints for mouse and ﬂy using the MARS tracker [36] and Fly tracker [15] respectively. At each frame, we stack
the keypoints of the agents, and a trajectory in our experiment consists of 21 frames. We normalize the coordinates of pose
keypoints by the image pixel dimensions.

For classiﬁcation, we use a shallow fully connected network with two hidden layers. We decrease the size of the network
as the input training fraction decreases (Table 10). The model size and other hyperparameters are chosen based on the
validation split. Our results are all reported on the test split. We train the classiﬁcation models using cross-entropy loss and
Adam optimizer with learning rate 0.001.

14

Dataset

MARS
CRIM13
Fly

Batch size

Classiﬁer Units
(100%, 75%, 50%)
256, 32
256, 32
256, 32

Classiﬁer Units
(5%, 2%, 1%)
64, 16
64, 16
64, 16
Table 10. Hyperparameters for Classiﬁcation Models.

Classiﬁer Units
(25%, 10%)
128, 16
128, 16
128, 16

512
512
512

Learning Rate

0.001
0.001
0.001

C. Additional Experimental Results

C.1. Decoder Loss Variations

We evaluate TREBA trained using different decoder losses on supervised behavior classiﬁcation. The procedure is the
same as described in the main paper. We evaluate performance given both our learned representation and one of either
(1) raw keypoints or (2) domain-speciﬁc features designed by experts. The input keypoints to the classiﬁcation model are
the detected poses from the MARS tracker [36] and the Fly tracker [15]. The input domain-speciﬁc features are the hand-
designed trajectory features for mouse [36] and ﬂy [15]. The input features are a superset of the programs we use to train
TREBA (listed in Table 1 from the main paper and described in Supplementary Material Section A.1).

We compare the MAP of TREBA representations trained with different decoder losses in Table 11. The rows for TVAE
and TVAE + Unsup. Contrast represents TREBA trained without programmed tasks and the remaining rows represents
different combinations of decoder losses with programmed tasks. The average error reduction of these runs from baseline
are shown in Table 2 in the main paper. Across all domains and training data amounts, we see that the learned representation
improves classiﬁer performance for both keypoints and domain-speciﬁc features. The improvements in performance are
generally larger when we use keypoints, most likely because domain-speciﬁc features already contain informative features
for classiﬁcation. Furthermore, we experiment training TREBA without self-decoding, using unsupervised contrastive loss
similar to [7, 8], and we see that the classiﬁer MAP is lower than training with self-decoding using the TVAE loss.

Table 11 demonstrates that when using task programming, different decoder loss combinations (attribute consistency,
decoding, and contrastive loss) are similar in performance in general, except when consistency loss is used alone. The lowest
performing loss is when we use attribute consistency loss alone, which is applied to the generated trajectory and not directly
to the representation. This result suggests that having at least one loss term directly applied on the representation (either
decoding or contrastive loss) is beneﬁcial.

Comparing task programming with TVAE loss alone, we see that TVAE loss is generally lower in performance (Table 11,
Table 2 in the main paper). We note that TVAE loss alone corresponds to self-supervised learning with self-decoding only.
Adding unsupervised contrastive loss to self-decoding improves performance relative to the TVAE, but we can improve the
performance further using losses based on task programming (for example, TVAE+Contrastive+Consistency).

Random Program Inputs. We further experiment with training TREBA (Contrastive + Consistency), without expert-
engineered programs, using a program that returns one of three classes randomly with equal probability for each trajectory.
We found that the error reduction when using a program with random outputs is between training using TVAE alone and
using unsupervised contrastive loss. On MARS relative to baseline features, TREBA with a random program achieves an
error reduction of 14.0%, compared to 13.7% for TVAE, 14.3% for TVAE + Unsup. Contrastive, 15.3% for all programs
(Table 2 in main paper). On Fly relative to baseline features, TREBA with a random program achieves an error reduction
of 13.6%, compared to 11.7% for TVAE, 16.1% for TVAE + Unsup. Contrastive, 21.2% for all programs (Table 2 in
main paper). We tried adding more random programs during training, but did not observe an increase in performance. The
lower performance of TREBA using random program inputs compared to all programs suggests that programs engineered
using structured expert knowledge based on behavior attributes is important for improving the effectiveness of the learned
representation.

C.2. Time Estimates

Based on domain expert estimates in neurobiology, behavior annotation takes 4 times the length of 30Hz videos, while task
programming takes 5 to 10 minutes per program. We note that this time estimate is from domain experts familiar with data

15

Dataset
MAP@k%

Keypoints
+ TVAE
+ TVAE+Unsup. Contrast
+ TVAE+Consist
+ TVAE+Contrast
+ TVAE+Decode
+ TVAE+Contrast+Consist
+ TVAE+Decode+Consist
+ TVAE+Contrast+Decode
+ TVAE+Contrast+Decode+Consist
+ Unsup. Contrast

Domain-speciﬁc features

+ TVAE
+ TVAE+Unsup. Contrast
+ TVAE+Consist
+ TVAE+Contrast
+ TVAE+Decode
+ TVAE+Contrast+Consist
+ TVAE+Decode+Consist
+ TVAE+Contrast+Decode
+ TVAE+Contrast+Decode+Consist
+ Unsup. Contrast

MARS
50
0.635
0.852
0.852
0.763
0.851
0.857
0.856
0.855
0.859
0.857
0.704

0.838
0.866
0.866
0.841
0.868
0.869
0.868
0.868
0.867
0.866
0.847

100
0.656
0.859
0.866
0.776
0.868
0.872
0.866
0.870
0.871
0.870
0.735

0.847
0.869
0.871
0.853
0.872
0.874
0.877
0.873
0.876
0.872
0.853

10
0.588
0.817
0.815
0.704
0.804
0.825
0.822
0.820
0.821
0.821
0.582

0.824
0.850
0.850
0.824
0.853
0.851
0.853
0.848
0.846
0.851
0.830

CRIM13
50
0.621
0.796
0.813
0.694
0.813
0.828
0.821
0.813
0.811
0.811
0.697

0.858
0.874
0.876
0.854
0.876
0.880
0.876
0.872
0.878
0.879
0.858

100
0.648
0.820
0.837
0.720
0.838
0.848
0.837
0.837
0.830
0.834
0.721

0.873
0.885
0.889
0.871
0.889
0.892
0.888
0.888
0.892
0.892
0.874

10
0.538
0.703
0.706
0.581
0.707
0.719
0.722
0.707
0.693
0.693
0.587

0.792
0.808
0.808
0.775
0.811
0.805
0.808
0.808
0.811
0.813
0.787

10
0.348
0.419
0.521
0.497
0.625
0.666
0.650
0.432
0.645
0.484
0.384

0.774
0.791
0.811
0.812
0.834
0.815
0.820
0.781
0.810
0.783
0.784

Fly
50
0.519
0.635
0.667
0.657
0.712
0.737
0.707
0.603
0.738
0.616
0.560

0.829
0.852
0.858
0.856
0.869
0.866
0.868
0.838
0.862
0.842
0.842

100
0.586
0.722
0.739
0.729
0.753
0.773
0.750
0.688
0.775
0.679
0.645

0.868
0.880
0.882
0.882
0.888
0.883
0.886
0.862
0.885
0.868
0.866

Table 11. Decoder Loss Variations. Comparing data efﬁciency of TREBA trained with different decoder losses with respect to classiﬁer
MAP. Keypoints and domain-speciﬁc features represent baseline input features. TVAE represents training TREBA with self-decoding only
and contrastive, decoding and consistency loss are described in Section 3.3 of the main paper.

annotation and trajectory feature design. Applying this estimate to Figure 4 A2 B2 C2 in the main paper (for data efﬁciency
with domain-speciﬁc features), in regions of low time investment (< 2 hours), it is generally better for domain experts to
annotate more data. For performance at > 2 hours, task programming provides a better return on investment of the expert’s
time. Task programming requires an initial effort to produce the programs, which then scales to any data amount with no
additional effort. Note that this time is variable depending on the domain expert and the domain, and our estimate is based
on neurobiologists familiar with data annotation and trajectory feature design.

C.3. Classiﬁcation Samples

We visualize classiﬁcation samples for each dataset using input domain-speciﬁc features and TREBA (MARS in Figure 9,
CRIM13 in Figure 10, Fly in Figure 11). We visualize the classiﬁer using TREBA at the training fraction such that the
classiﬁer MAP matches that of the fully-supervised baseline feature performance. Comparing the samples qualitatively, we
see that the classiﬁer output with the full training set is comparable to TREBA with 10× reduced annotations on MARS and
2× reduced annotations on CRIM13 and Fly. We note that the classiﬁer trained on reduced data alone (last row of Figures 9,
10, 11) is generally less accurate, compared to the classiﬁers trained with either full training data or with TREBA.

16

Figure 9. Annotations on MARS Test Set. Classiﬁer annotations on MARS dataset vs. ground truth at 30Hz. For each sample,
the second row corresponds to features + TREBA trained with 10 expert-engineered programs, with ∼ 10% of the supervised behavior
annotations. The third row corresponds to training using domain-speciﬁc features on the full dataset. The last row corresponds to training
using domain-speciﬁc features on ∼ 10% of data, without TREBA.

Figure 10. Annotations on CRIM13 Test Set. Classiﬁer annotations on CRIM13 dataset vs. ground truth at 25Hz. For each sample,
the second row corresponds to features + TREBA trained with 10 expert-engineered programs, with ∼ 50% of the supervised behavior
annotations. The third row corresponds to training using domain-speciﬁc features on the full dataset. The last row corresponds to training
using domain-speciﬁc features on ∼ 50% of data, without TREBA.

17

02004006008001000120014001600Frame NumberGround truth80k annotations  + 10 programs781k annotations80k annotationsMARS Test Sample 1sniffmountattack02004006008001000120014001600Frame NumberGround truth80k annotations  + 10 programs781k annotations80k annotationsMARS Test Sample 2sniffmountattack02004006008001000120014001600Frame NumberGround truth213k annotations  + 10 programs407k annotations213k annotationsCRIM13 Test Sample 1sniffmountattack02004006008001000120014001600Frame NumberGround truth213k annotations  + 10 programs407k annotations213k annotationsCRIM13 Test Sample 2sniffmountattackFigure 11. Annotations on Fly Test Set. Classiﬁer annotations on Fly dataset vs. ground truth at 30Hz. For each sample, the second row
corresponds to features + TREBA trained with 13 expert-engineered programs, with ∼ 50% of the supervised behavior annotations. The
third row corresponds to training using domain-speciﬁc features on the full dataset. The last row corresponds to training using domain-
speciﬁc features on ∼ 50% of data, without TREBA.

18

02004006008001000120014001600Frame NumberGround truth546k annotations  + 13 programs1067k annotations546k annotationsFly Test Sample 1 (Lunge)lunge02004006008001000120014001600Frame NumberGround truth546k annotations  + 13 programs1067k annotations546k annotationsFly Test Sample 2 (Tussle)tussle02004006008001000120014001600Frame NumberGround truth546k annotations  + 13 programs1067k annotations546k annotationsFly Test Sample 3 (Wing Extension)wing  extension