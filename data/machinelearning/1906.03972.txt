9
1
0
2

n
u
J

0
1

]

G
L
.
s
c
[

1
v
2
7
9
3
0
.
6
0
9
1
:
v
i
X
r
a

Evaluating the Robustness of Nearest Neighbor
Classiﬁers: A Primal-Dual Perspective

Lu Wang
Nanjing University
wangl@lamda.nju.edu.cn

Xuanqing Liu
University of California, Los Angeles
xqliu@cs.ucla.edu

Jinfeng Yi
JD AI Research
yijinfeng@jd.com

Zhi-Hua Zhou
Nanjing University
zhouzh@lamda.nju.edu.cn

Cho-Jui Hsieh
University of California, Los Angeles
chohsieh@cs.ucla.edu

Abstract

We study the problem of computing the minimum adversarial perturbation of the Nearest Neighbor
(NN) classiﬁers. Previous attempts either conduct attacks on continuous approximations of NN models
or search for the perturbation by some heuristic methods. In this paper, we propose the ﬁrst algorithm
that is able to compute the minimum adversarial perturbation. The main idea is to formulate the problem
as a list of convex quadratic programming (QP) problems that can be efﬁciently solved by the proposed
algorithms for 1-NN models. Furthermore, we show that dual solutions for these QP problems could give
us a valid lower bound of the adversarial perturbation that can be used for formal robustness veriﬁcation,
giving us a nice view of attack/veriﬁcation for NN models. For K-NN models with larger K, we show
that the same formulation can help us efﬁciently compute the upper and lower bounds of the minimum
adversarial perturbation, which can be used for attack and veriﬁcation.

1 Introduction

Adversarial robustness of neural networks has been extensively studied in the past few years. Given a data
point, adversarial attacks are developed to construct small imperceptible input perturbations to alter the
predicted label [26, 13, 5, 2, 4]. On the other hand, robustness veriﬁcation algorithms are also developed
to compute a “safe region” around the point such that the prediction is provably unchanged within such
region [31, 29, 34, 11]. An attack algorithm can be viewed as ﬁnding an upper bound of the “minimum
adversarial perturbation” while a veriﬁcation algorithm ﬁnds a lower bound of this value. In fact, robustness
veriﬁcation is often more important than attacks, since a veriﬁable behavior is required for mission-critical
systems. For neural network models, due to non-convexity, both attack and veriﬁcation cannot reach the
minimum adversarial perturbation, and there is still a huge gap between the (computable) upper and lower
bounds [24].

We study the problem of evaluating the robustness of the Nearest Neighbor (NN) classiﬁers. As a
non-continuous step function, NN classiﬁers are very different from neural networks, and the neural network
attack and veriﬁcation methods cannot be directly applied to them. Previous attempts on attacking nearest

 
 
 
 
 
 
neighbor models either use some simple heuristics [25] or apply gradient-based attacks to some continuous
substitute models of NN [25, 22, 10]. Unfortunately, these attacks are far from optimal and do not have any
theoretical guarantee. To the best of our knowledge, there is no existing approach on computing the minimum
adversarial perturbation that can change an NN classiﬁer’s output, and there is no existing veriﬁcation method
that can compute a meaningful lower bound of the safe region.

In this paper, we ﬁrst study the 1-NN classiﬁer and show that ﬁnding the minimum adversarial perturbation
can be formulated as a set of convex quadratic programming (QP) problems, and a solution can be computed
in polynomial time [16]. This is quite different from neural networks or tree-based models where ﬁnding
the minimum perturbation has shown to be NP-hard [15, 14]. Furthermore, our formulation provides a very
clean view of attack and veriﬁcation for nearest neighbor classiﬁers. An attacker could solve any QP problem
and any feasible solution will be a successful attack; a veriﬁer could solve the dual of these problems and
any feasible solution set will lead to a guaranteed lower bound of the minimum adversarial perturbation.
Moreover, the primal minimum and dual maximum will match at the value of the minimum adversarial
perturbation. We show that the QP problems can be solved efﬁciently by greedy coordinate ascent, and based
on this primal-dual perspective, we further provide several screening rules to speed up the quadratic solvers.
When extending to K-NN models with K > 1, our QP formulation will have the number of constraints
growing exponentially with K. However, we can still approximately solve the primal problems, and that
will give an attack algorithm outperforming previous works. Furthermore, we propose a way to set dual
feasible solutions to provide a tight lower bound of the minimum adversarial perturbation without solving any
problem. This leads to an efﬁcient K-NN veriﬁcation algorithm that works for any K.

We conduct experiments on real datasets and have the following interesting ﬁndings:
• For 1-NN models, our proposed algorithm can efﬁciently compute the minimum adversarial pertur-
bation. Our algorithm is provably optimal, achieves much smaller value, and is more efﬁcient than
previous attack methods. Also, this is the ﬁrst robustness veriﬁcation method for NN models.

• For K-NN models with larger K, computing the exact minimum adversarial perturbation is still
challenging, but our formulation provides an efﬁcient attack algorithm, which outperforms previous
attack methods. More importantly, our dual problems lead to an efﬁcient veriﬁcation algorithm to
compute the lower bound of adversarial perturbation and have time complexity independent to K.
Experiments show that the bounds are reasonably tight.

• Equipped with our algorithm, we accurately compute the robust error bound of the 1-NN model on
MNIST and Fashion-MNIST. We ﬁnd that a simple 1-NN model can achieve better robust error than
CNN on these data.

2 Related work

Adversarial robustness of neural networks. Adversarial robustness of neural networks has been studied
extensively in the past few years. To evaluate the robustness of neural networks, attack algorithms are
developed to ﬁnd adversarial examples that are close to the original example [5, 13, 18, 7, 8]. However, due to
the non-convexity of neural networks, these attacks cannot reach the minimum perturbation so they can only
provide some upper bound of robustness and cannot provide any robustness guarantee. For safety-critical
applications such as real-world control systems, it is essential to have robustness guarantees such that we
know the prediction is provably unchanged within a certain distance. This motivates recent studies on neural
network veriﬁcation which aims to compute a lower bound of the minimum adversarial perturbation [30, 31,
29, 12, 27, 34, 35]. Also, many of these robustness veriﬁcation bounds can be incorporated in the training
procedure to obtain “veriﬁable” networks [31, 32, 19].

2

Adversarial robustness of nearest neighbor classiﬁers.
Adversarial robustness of nearest neighbor
classiﬁers is less studied. Unfortunately, the algorithms mentioned above designed for neural networks cannot
be directly applied to NN models since NN models are discrete step functions. [28] discussed the robustness
of K-NN from the theoretical perspective and showed that the robustness of K-NN can approach the Bayesian
optimal classiﬁer. To compute an upper bound of the minimum adversarial perturbation (or equivalently,
attack), [22] proposed to employ a differentiable substitute for attacking 1-NN models; [25] proposed some
heuristic methods and another gradient-based model to attack another kind of continuous substitute of K-NN.
We will show in Section 3 that they cannot obtain the minimum adversarial perturbation and in experiments
that they lead to loose upper bounds. On the other hand, to the best of our knowledge, there is no existing
approach on computing the minimum adversarial perturbation or its lower bound, so our work is the ﬁrst to
verify the NN models. Finally, there are some recent work using NN models for defense, including [21, 10].
However, they usually combine NN with neural network models, which are out of the scope of this paper.

3 Background and motivation

First, we set up notations for the Nearest Neighbor (NN) classiﬁers. Assume there are C labels in total.
We use {(x1, y1), . . . , (xn, yn)} to denote the database where each xi is a d-dimensional vector and yi ∈
{1, . . . , C} is the corresponding label. A K-NN classiﬁer f : Rd → {1, . . . , C} maps a test instance to
a predicted label. Given a test instance z ∈ Rd, the classiﬁer will ﬁrst identify the K-nearest neighbors
{xπ(1), . . . , xπ(K)} based on the Euclidean distance (cid:107)xi − z(cid:107) and then predict the ﬁnal label by majority
voting among {yπ(1), . . . , yπ(K)}.

Next we deﬁne the notions of adversarial robustness, attack, and veriﬁcation. Given a test sample z and
without loss of generality, we assume it is correctly classiﬁed as class-1 by the NN model. An adversarial
perturbation is deﬁned as δ ∈ Rd such that f (z +δ) (cid:54)= 1. An attack algorithm aims to ﬁnd the minimum-norm
adversarial perturbation, and its norm is

(cid:15)∗ = (cid:8) min

δ

(cid:107)δ(cid:107) s.t. f (z + δ) (cid:54)= 1(cid:9).

(1)

A veriﬁcation algorithm aims to ﬁnd a lower bound r such that

f (z + δ) = 1,

∀(cid:107)δ(cid:107) ≤ r.

Clearly, by deﬁnition the maximum lower bound r∗ will match with the minimum perturbation norm (cid:15)∗ if
we have optimal attack and veriﬁcation. We will mainly focus on (cid:96)2 norm but will brieﬂy talk about how to
extend to (cid:96)∞ and (cid:96)1 norms later. Also, we will focus on 1-NN ﬁrst and then generalize to K > 1 later.

Failure cases of previous attack methods. At ﬁrst glance, the minimum adversarial perturbation seems
to be easy to compute for the 1-NN model. For instance, [25] mentioned that the minimum adversarial
perturbation has to be on the straight line connecting z and one of the training instances belonging to a
different class (yi (cid:54)= 1), so a simple linear time algorithm can solve this problem. Unfortunately, this claim is
not true. In Figure 1, we show that the optimal perturbation may not be on the lines connecting two points and
furthermore, only checking the line segments can ﬁnd an arbitrary bad solution. Other previous approaches try
to form a continuous approximation of NN classiﬁers [25, 22, 10], and clearly, they cannot ﬁnd the optimal
perturbation.

3

Figure 1: Illustration of the minimum adversarial perturbation for 1-NN model. The goal is to perturb z
to be classiﬁed as triangle. In (a), the red curve is the perturbation computed by [25] while the optimal
solution (blue perturbation, (cid:15)∗) could be much better, and the ratio can be arbitrary large by changing the
angle between red and blue. (b) shows that projection to the bisection hyperplanes may not be optimal; one
also needs to consider intersections of several bisections which can be exponentially many. (c) shows that
the optimal perturbation can be computed by evaluating the distance from z to each Voronoi cell of triangle
instances.

Connection to Voronoi diagrams and a solution for low-dimensional cases.
In fact, the decision bound-
ary of a 1-NN model can be captured by the Voronoi diagram (see Figure 1(c)). In the Voronoi diagram, each
training instance xi forms a cell, and the decision boundary of the cell is captured by the convex boundary
formed by bisections between xi and its neighbors. One can thus obtain the minimum adversarial perturbation
by computing the distances from z to all the cells with yi (cid:54)= 1. However, to compute the distance, we need
to check all the faces (captured by one bisection hyperplane) and angles (intersections of more than one
bisection hyperplanes) of the cell.

For 2-dimensional space (d = 2), it has been shown in [3] that each cell can only have ﬁnite faces
and angles and there exists a polynomial time algorithm for computing a Voronoi diagram. In general, for
d-dimensional problems with n points, Voronoi diagram computation requires O(n log n + n(cid:100) d
2 (cid:101)) time, which
works for low-dimensional problems. However, time complexity grows exponentially with dimension d, so in
general, it is hard to use this algorithm unless d is very small.

4 Primal-dual quadratic programming formulation

Usually, ﬁnding the minimum adversarial perturbation is hard. Computing minimum adversarial perturbations
for ReLU networks and tree ensembles are both NP-hard [15, 14]. Also, as discussed in the previous
section, we can connect it to Voronoi diagram computation, but the solver will require exponential time in
dimensionality. So is it NP-hard to compute the minimum adversarial perturbation for 1-NN? Surprisingly, it
is not as we will demonstrate below.

4

4.1 Quadratic (primal) problems for minimum adversarial perturbation

We consider the 1-NN model. For a given instance z, if we want to perturb it so that z + δ is closer to xj with
yj (cid:54)= 1 than to all class-1 instances, then the problem of ﬁnding the minimum perturbation can be formulated
as:

δT δ s.t. (cid:107)z + δ − xj(cid:107)2 ≤ (cid:107)z + δ − xi(cid:107)2, ∀i, yi = 1.

(2)

(cid:15)(j) = min

δ

1
2

Each constraint can be rewritten as δT (xj − xi) + (cid:107)z−xi(cid:107)2−(cid:107)z−xj (cid:107)2

2

≥ 0. Therefore (2) becomes

{

(cid:15)(j) = min

1
2
where A ∈ Rn×d and b ∈ Rn, for each row i with yi = 1, ai = (xj − xi) and bi = (cid:107)z−xi(cid:107)2−(cid:107)z−xj (cid:107)2
respectively (0 otherwise). By solving the quadratic programming (QP) problem (3) for each {j : yj (cid:54)= 1},
the ﬁnal minimum adversarial perturbation norm is (cid:15)∗ = minj:yj (cid:54)=1
2(cid:15)(j). It has been shown that convex
quadratic programming can be solved in polynomial time [16], so our formulation leads to a polynomial
time algorithm for ﬁnding (cid:15)∗.

δT δ} := P (j)(δ),

δ:Aδ+b≥0

(3)

√

2

4.2 Dual quadratic programming problems

We also introduce the dual form of each QP, which is more efﬁcient to solve in practice and will lead to a
veriﬁcation perspective of evaluating adversarial robustness. The dual problem of (3) can be written as

max
λ≥0

{−

1
2

λT AAT λ − λT b} := D(j)(λ),

(4)

where λ ∈ Rn are the corresponding dual variables. The derivation is easy, but for completeness, we include
it in Appendix A.1. The primal-dual relationship connects primal and dual variables:

δ = AT λ.

Based on the weak duality, we have D(j)(λ) ≤ P (j)(δ) for any dual feasible solution λ and primal
feasible solution δ. Furthermore, based on Slater’s condition we can easily show that strong duality holds
(D(j)(λ∗) = P (j)(δ∗)) if xj (cid:54)= xi, ∀i, yi = 1. 1 Based on strong duality, we have

1
2

((cid:15)∗)2 = min
j:yj (cid:54)=1

{P (j)(δ∗)} = min
j:yj (cid:54)=1

{max
λ≥0

D(j)(λ)} ≥ min
j:yj (cid:54)=1

{D(j)(λ(j))} with feasible λ(j),

(5)

so any feasible solution {λ(j)} leads to a lower bound of the minimum adversarial perturbation. In summary,
we conclude the primal-dual relationship between 1-NN attack and veriﬁcation:

• A primal feasible solution of P (j) for any yj (cid:54)= 1 is a successful attack and gives us an upper bound
of (cid:15)∗. Therefore, one can solve QPs with a subset of j; usually a xj closer to z will lead to a smaller
adversarial perturbation, so in practice we can sort xj by the distance to z, solve the subproblems
one by one, and stop at any time. It will give a valid adversarial perturbation. After solving all the
subproblems, the result will be (cid:15)∗.

• A set of dual feasible solutions {λ(j)}j:yj (cid:54)=1 will give a lower bound of (cid:15)∗ according to (5). So any
heuristic method for setting up a set of dual feasible solutions will give us a lower bound which can be
used for robustness veriﬁcation. After solving all the dual problems exactly, we will get the tightest
lower bound, which is also (cid:15)∗.

1One can observe that if the condition holds (yj (cid:54)= 1) then a small ball around δ = xj − z will be feasible solutions which satisﬁes

Slater’s condition, implying strong convexity.

5

1-NN veriﬁcation. Here we give an example of how to quickly set up dual variables to give a lower bound
of the minimum adversarial perturbation without solving any problem. For a dual problem D(j), consider
only having one variable λ(j)
to be nonzero while ﬁxing all the rest variables zero, the optimal closed-form
solution will be

i

λ(j)
i = max(0, −

bi
(cid:107)ai(cid:107)2 ), D(j)([0, . . . , 0, λ(j)

i

, 0, . . . , 0]) =

max(−bi, 0)2
2(cid:107)ai(cid:107)2

.

(6)

Note that bi = (cid:107)z−xi(cid:107)2−(cid:107)z−xj (cid:107)2
lower bound of (cid:15)∗ can be computed easily:

2

and (cid:107)ai(cid:107)2 = (cid:107)xj − xi(cid:107)2 both can be computed easily, so a guaranteed

(cid:15) = min
j:yj (cid:54)=1

(cid:0) max
i:yi=1

max((cid:107)z − xj(cid:107)2 − (cid:107)z − xi(cid:107)2, 0)
2(cid:107)xj − xi(cid:107)

(cid:1) ≤ (cid:15)∗.

(7)

This value has an interesting geometrical meaning. See Appendix A.2 for more details. In general, we can
also get improved lower bounds by solving more coordinates for each subproblem.

4.3 Solving the QP problems efﬁciently

Now we discuss how to efﬁciently solve a series of QP problems {D(j)}j:yj (cid:54)=1 in practice. Although we can
do this in polynomial time, in practice a naive algorithm is still too slow. Note that we have totally O(n)
quadratic problems and each QP has O(n) variables to solve, so roughly more than O(n3) time complexity is
required for doing this naively. Calling a commercial quadratic programming solver will take 20 seconds for
n = 6000 when solving only one QP problem. In the following, we show how to solve “all” QP problems in
3 seconds.

First, we ﬁnd that a greedy coordinate ascent algorithm can be efﬁciently applied to solve the dual QP
problem (4). This is mainly due to the sparsity of the solution—if λ∗ is dual optimal, then a nonzero λ∗
i
means the primal constraint (cid:107)z + δ − xj(cid:107)2 = (cid:107)z + δ − xi(cid:107)2, so the optimal z + δ will be on the bisection
of (xi, xj). Therefore, if (cid:107)λ∗(cid:107)0 = q then the optimal solution is the intersection of q bisection hyperplanes,
which means q is usually small. For instance, on MNIST dataset when we test on 100 subproblems, the
average number of (cid:107)λ∗(cid:107)0 is only 50.06, with 7612.59 dual variables per subproblem. The sparsity of the
solution motivates the use of the greedy coordinate ascent algorithm. Starting from λ = 0, we maintain the
gradient vector with g = −AAT λ − b and every time we pick the variable with the largest projected gradient

i∗ = arg max

i

|(max(λ + g, 0) − λ)i|

and then update a single variable λi∗ ← max(λi∗ + gi∗ /(cid:107)ai∗ (cid:107)2, 0). This is similar to the SMO method
proposed for training kernel SVM [23, 6], but since there is no equality constraint we only need to pick one
variable at a time. Since there are only a few nonzero λs, the algorithm usually converges much quicker than
standard quadratic optimization solvers.

Second, we propose a screening rule to remove variables in each dual QP problem (4). There are only a
few nonzero variables, and our screening rule will reduce the size of variables before solving the problem. We
introduce the following lemma:

Lemma 1. For a speciﬁc quadratic problem P (j)(δ), the optimal dual solution has λ∗

i = 0 if

−((cid:107)z − xi(cid:107)2 − (cid:107)z − xj(cid:107)2)/2 + (cid:107)xj − xi(cid:107)(cid:107)δ∗(cid:107) < 0,

(8)

where δ∗ is the optimal solution.

6

The proof is in Appendix A.3. Note that checking (8) does not need to solve the QP problem. To conduct
the screening rule in (8), we need to have an estimation of (cid:107)δ∗(cid:107) or its upper bound. A naive upper bound
(cid:107)δ∗(cid:107) ≤ (cid:107)xj − z(cid:107) can be used for running the screening rule.

With the methods mentioned above, each dual QP can be solved efﬁciently. However, there are O(n)
QPs in total, and solving all of them is still expensive. However, the ﬁnal solution (cid:15)∗ only depends on the
minimum among solutions of all QPs, so can we remove most of the irrelevant QPs?

We can use primal-dual relationship for removing most of the QP problems before solving them. Assume
we have a primal solution ¯δ then the minimum adversarial perturbation norm (cid:15)∗ ≤ (cid:107)¯δ(cid:107), so every dual problem
with D(j)(λ) > ¯δT ¯δ/2 for some λ can be removed. For a subproblem with respect to xj, based on (6) we
know the subproblem can be removed if

¯δT ¯δ < max(−bi, 0)2/(cid:107)ai(cid:107)2

(9)

for some i, thus we can use (9) to remove some unimportant subproblems. In practice, we sort the subproblems
in ascending order of (cid:107)z − xj(cid:107) and iteratively run the screening rule after solving one more subproblem. As
a result, most of the subproblems can be removed without solving them, and we achieve signiﬁcant speedup.
Our overall algorithm is illustrated in Algorithm 1.

Algorithm 1: Computing minimum adversarial perturbation

Input: Target instance z, database {(xj, yj)}n

j=1.

1 Initial (cid:15) = ∞ ;
2 Sort subproblems {D(j)}j:yj (cid:54)=1 by ascending distances of (cid:107)z − xj(cid:107);
3 for each j (according to the sorted order) do
4

if not screenable via (9) then

5

6

Solve the subproblem via greedy coordinate ascent with screening rule (8);
Update (cid:15) if we get a smaller value;

end

7
8 end

4.4 Extending to (cid:96)1 and (cid:96)∞ norms

Sometimes people are interested in ﬁnding the minimum (cid:96)∞ or (cid:96)1 norm adversarial perturbation (replacing
the (cid:96)2 norm in (1)). Those can be solved similarly using our framework but will require linear programming
instead of quadratic programming. For example, the minimum (cid:96)∞-norm adversarial perturbation can be
formulated as

(cid:15)(j) = min

δ

v s.t. Aδ + b ≥ 0, v ≥ δi ≥ −v ∀i = 1, . . . , d.

A similar formulation can be done for the (cid:96)1 case. This can also be solved efﬁciently by linear programming
solvers and the primal-dual relationship also holds.

4.5 Extending beyond 1-NN

We can extend our approach to K-NN with K > 1 by adding more constraints. Taking the 3-NN model and
binary classiﬁcation as an example, we can list all the possible combinations of {(j1, j2, j3) | yj1 = yj2 =

7

2, yj3 = 1} and then solve the following QP problem to force z + δ to be closer to xj1, xj2 than to all the
class-1 instances except xj3 :

(cid:15)(j1,j2,j3) = min

δ

1
2

δT δ s.t. (cid:107)z + δ − xj(cid:107)2 ≤ (cid:107)z + δ − xi(cid:107)2, ∀i, i (cid:54)= j3, yi = 1,

j ∈ {j1, j2}.

There will be O(2n) constraints so will be more expensive to solve. For general K > 1, we can write a
similar formulation with O(nK) constraints, but since the QP still has a sparse solution, greedy coordinate
ascent can still solve a subproblem efﬁciently. Using this we can still obtain an upper and lower bound,
corresponding to attack and veriﬁcation. For an upper bound (attack), we just need to heuristically choose
some (j1, j2, j3) tuples according to the distance to z and solve some QPs (more details in Appendix A.4).
For a lower bound, we can use the similar formulation to (7) as below:

Efﬁcient veriﬁcation for K > 1. We can apply (5) efﬁciently even for large K. Taking the K = 3 case as
an example, let Ci,j = max((cid:107)z−xj (cid:107)2−(cid:107)z−xi(cid:107)2,0)
then with some simple derivation we can get the veriﬁcation
bound for K = 3 case

2(cid:107)xj −xi(cid:107)

min
(j1,j2,j3):yj1 =2,yj2 =2,yj3 =1

(cid:0) max
i(cid:54)=j3

(max(Ci,j1, Ci,j2 ))(cid:1) ≥

min
(j1,j2):yj1 =2,yj2 =2

max(Dj1, Dj2)

where Dj = mint,yt=1(maxi(cid:54)=t,yi=1 Ci,j), which is the second largest value among Ci,j for all i. Therefore,
we just need to choose the second smallest of Dj among {j : yj = 2}. Note that this can be generalized to a
general K case, where the veriﬁcation lower bound becomes:

(cid:15) := {kthmin
j:yj =2

(kthmax
i:yi=1

Cij )} ≤ (cid:15)∗, k = (K + 1)/2,

(10)

which can be computed efﬁciently with time complexity independent to K.

5 Experiments

We show main results in Section 5.1, and analyze efﬁciency of our algorithm in Section 5.2. All experiments
are run on a cloud server with one Intel E5-2650V4 CPU and one NVIDIA V100 GPU.

5.1 Comparison of adversarial perturbations

We show that our formulation leads to better attack and veriﬁcation algorithms. Note that our QP framework
leads to the following proposed algorithms for exact computation, veriﬁcation, and attack:

• Exact: computes the exact minimum adversarial perturbation for 1-NN via Algorithm 1.
• Veriﬁer: computes a lower bound for 1-NN via (7) and for K-NN via (10).
• QP-1 and QP-10: compute upper bounds (attack) for 1-NN via Algorithm 1 but only iterate over the

top-1 and top-10 QP problems respectively.

• QP-greedy: computes an upper bound for K-NN by heuristically choosing QP subproblems (Ap-

pendix A.4).

Note that there is no existing algorithm for computing the exact minimum adversarial perturbation and no
existing veriﬁcation method for K-NN that can compute a lower bound. Therefore we are only able to
compare with the following attack methods:

8

Table 1: Mean perturbations and the total runtime of 100 correctly classiﬁed test instances for 1-NN. We
repeat 5 times to report the mean of total runtime.

MNIST
Perturbation ((cid:96)2) Runtime (s)

Fashion-MNIST
Perturbation ((cid:96)2) Runtime (s)

Exact

Lower bounds Veriﬁer

Upper bounds

QP-1
QP-10
Naive-1
Naive-10
Mean
Substitute

1.491

1.370

1.530
1.491
1.851
1.755
4.561
1.616

177.507

101.850

43.676
108.390
145.299
512.164
40.951
164.153

1.128

1.073

1.142
1.128
1.446
1.413
4.179
1.264

130.795

100.651

38.344
65.947
115.820
504.344
39.423
145.797

Table 2: Mean perturbations of 100 correctly classiﬁed test instances for K-NN.

Lower bounds Veriﬁer

Upper bounds

QP-greedy
Naive-1
Mean

K = 1 K = 3 K = 5 K = 7 K = 9

2.268

2.494
2.894
5.282

2.230

3.089
3.718
5.241

2.201

3.417
3.903
5.205

2.193

3.636
4.085
5.191

2.183

3.786
4.173
5.176

• Naive-1 and Naive-10 [1, 25]: compute upper bounds for 1-NN by moving towards a nearby other-class
instance (belonging to a class different from the one of the test instance). Naive-10 repeats the process
for 10 times and chooses the best perturbation. For K > 1, Naive-1 moves towards a nearby size
(K + 1)/2 other-class cluster.

• Mean [25]: computes an upper bound for K-NN by moving towards a class mean. The target class is

chosen by the class mean distance to the test instance.

• Substitute [22]: computes an upper bound for 1-NN by attacking a smoothed variant of NN.

All attack methods are tuned to have 100% attack success rates, such that the perturbation is strictly the upper
bound for the minimum adversarial perturbation.

Perturbations for 1-NN. Experiments are performed on MNIST [17] and Fashion-MNIST [33]. As Table 1
shows, Algorithm 1 (Exact) can efﬁciently compute the minimum adversarial perturbation. Veriﬁer can
compute a reasonable lower bound without solving any QP problems exactly. QP-1 and QP-10 are efﬁcient
and effective attack methods by iterating over only a few QP problems.

Perturbations with larger K. Experiments are performed on Binary-MNIST, where label 8 and label 0
are used. Veriﬁer and QP-greedy compute tight lower bounds and upper bounds respectively as shown in
Table 2. More results are in Appendix A.5.

9

Table 3: Comparison of robustness for 1-NN and neural networks with the same (cid:96)∞ perturbation.

Dataset

Model

MNIST

Fashion-MNIST

Neural Net
1-NN

Neural Net
1-NN

(cid:15)

0.1
0.1

0.1
0.1

Test error Attack error Veriﬁable robust error

1.07%
3.41%

9.36%
23.47%

81.68%
27.06%

81.85%
78.25%

100%
27.06%

100%
78.25%

Comparing robust error under (cid:96)∞-norm perturbation.
In this experiment, we compare the robustness
of 1-NN model with the CNN model on two datasets in Table 3. The CNN model has two convolutional
layers and two fully connected layers with ReLU activations. We observe that neural nets have better test
error, which is known in the literature. However, if we compare the error under the same amount of attack,
1-NN outperforms neural nets on these two datasets. Furthermore, since 1-NN is easy to verify using our
approach, and it is NP-hard to compute the minimum adversarial perturbation for a neural network, 1-NN
can have much better veriﬁable robust error than neural nets. Note that we use one of the state-of-the-art
veriﬁcation methods in [31] for computing the veriﬁable robust error of neural nets. We do not claim 1-NN is
a better model than neural nets. For more complex datasets such as CIFAR or ImageNet, the 1-NN method
will lead to bad clean error, so it is not comparable with neural nets. However, we think this experiment
suggests that for some simple data, NN models could be a better choice in terms of the robustness error.

5.2 Efﬁciency of our algorithm

We already show Algorithm 1 is efﬁcient in Table 1. It has three components: sorting, screening rules for
reducing the number of QPs, and the greedy coordinate ascent solver for each QP. We leave the experiments
about sorting in Appendix A.6 and talk about the other two in detail.

In the MNIST case, for every test instance, we have to solve about 54, 000 (all other-class
Screening.
instances) QP problems without screening. While with sorting and screening, only 2.53 QP problems on
average are left to solve. Therefore screening improves efﬁciency signiﬁcantly. The screening parameter nscr
(number of is chosen for each j in (9)) is also an important parameter for efﬁciency. There is a trade-off
between the number of screened subproblems and the screening overheads controlled by nscr. We plot the
tradeoff on MNIST in Figure 2. This shows a very small nscr is enough, and we choose 8 for our experiments.

Greedy coordinate ascent. Due to the sparsity of each QP problem, the greedy coordinate ascent solver is
much more efﬁcient than other standard QP solvers. To verify this, we compare greedy coordinate ascent
with SCS [20], CVXOPT and ECOS [9] for solving these QP problems. We ﬁx everything the same (with the
same screening rule and sorting technique) while only change the QP solver. Since it is difﬁcult for standard
QP solvers to deal with high dimensional problems, 1/10 training samples of MNIST are used. The results
are presented in Figure 3. Greedy coordinate ascent is faster than other solvers by more than 60 times for
computing K-NN robustness.

10

Figure 2: Screening trade-off.

Figure 3: Runtime of QP solvers.

6 Conclusion

In this paper, we show that computing the minimum adversarial perturbation of K-NN models can be
formulated as a series of quadratic programming problems. This framework is the ﬁrst algorithm that can
compute the minimum adversarial perturbation, and we propose an efﬁcient solver such that the computation
time is comparable with and often faster than previous attack algorithms. Furthermore, our framework also
motivates the ﬁrst algorithm for verifying K-NN robustness from the dual aspect.

References

[1] Laurent Amsaleg, James Bailey, Dominique Barbe, Sarah M. Erfani, Michael E. Houle, Vinh Nguyen,
and Milos Radovanovic. The vulnerability of learning to adversarial perturbation increases with intrinsic
dimensionality. In IEEE Workshop on Information Forensics and Security, pages 1–6, 2017.

[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International Conference on Machine Learning,
2018.

[3] Franz Aurenhammer and Rolf Klein. Voronoi diagrams. Handbook of computational geometry,

5(10):201–290, 1999.

[4] Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.

Pattern Recognition, 84:317–331, 2018.

[5] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE

Symposium on Security and Privacy, pages 39–57, 2017.

[6] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transac-

tions on Intelligent Systems and Technology, 2(3):27, 2011.

[7] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth order opti-
mization based black-box attacks to deep neural networks without training substitute models. In ACM
Workshop on Artiﬁcial Intelligence and Security, pages 15–26, 2017.

11

01020304050nscr2.252.502.753.003.253.50# subproblems5101520runtime (s)GCASCSCVXOPTECOSsolver0200400600runtime (s)  3.000200.989530.673546.177[8] Minhao Cheng, Thong Le, Pin-Yu Chen, Jinfeng Yi, Huan Zhang, and Cho-Jui Hsieh. Query-efﬁcient
hard-label black-box attack: An optimization-based approach. In International Conference on Learning
Representations, 2019.

[9] Alexander Domahidi, Eric Chu, and Stephen Boyd. ECOS: An SOCP solver for embedded systems. In

European Control Conference, pages 3071–3076, 2013.

[10] Abhimanyu Dubey, Laurens van der Maaten, Zeki Yalniz, Yixuan Li, and Dhruv Mahajan. Defense
against adversarial images using web-scale nearest-neighbor search. In IEEE Conference on Computer
Vision and Pattern Recognition, 2019.

[11] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual
approach to scalable veriﬁcation of deep networks. In Annual Conference on Uncertainty in Artiﬁcial
Intelligence, pages 162–171, 2018.

[12] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certiﬁcation of neural networks with abstract interpretation. In
IEEE Symposium on Security and Privacy, pages 3–18, 2018.

[13] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial

examples. In International Conference on Learning Representations, 2015.

[14] Alex Kantchelian, JD Tygar, and Anthony Joseph. Evasion and hardening of tree ensemble classiﬁers.

In International Conference on Machine Learning, pages 2387–2396, 2016.

[15] Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efﬁcient
SMT solver for verifying deep neural networks. In International Conference on Computer Aided
Veriﬁcation, pages 97–117, 2017.

[16] Mikhail K Kozlov, Sergei P Tarasov, and Leonid G Khachiyan. The polynomial solvability of convex
quadratic programming. USSR Computational Mathematics and Mathematical Physics, 20(5):223–228,
1980.

[17] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to

document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In International Conference on Learning
Representations, 2018.

[19] Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably

robust neural networks. In International Conference on Machine Learning, pages 3575–3583, 2018.

[20] Brendan O’Donoghue, Eric Chu, Neal Parikh, and Stephen Boyd. Conic optimization via operator
splitting and homogeneous self-dual embedding. Journal of Optimization Theory and Applications,
169(3):1042–1068, 2016.

[21] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: Towards conﬁdent, interpretable

and robust deep learning. CoRR, abs/1803.04765, 2018.

[22] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. CoRR, abs/1605.07277, 2016.

12

[23] John Platt. Sequential minimal optimization: A fast algorithm for training support vector machines.

Technical report, 1998.

[24] Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation

barrier to tight robust veriﬁcation of neural networks. CoRR, abs/1902.08722, 2019.

[25] Chawin Sitawarin and David Wagner. On the robustness of deep k-nearest neighbors. CoRR,

abs/1903.08333, 2019.

[26] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2013.

[27] Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efﬁcient formal safety
analysis of neural networks. In Advances in Neural Information Processing Systems, pages 6367–6377,
2018.

[28] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. Analyzing the robustness of nearest neighbors to
adversarial examples. In International Conference on Machine Learning, pages 5120–5129, 2018.

[29] Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning,
and Inderjit Dhillon. Towards fast computation of certiﬁed robustness for relu networks. In International
Conference on Machine Learning, pages 5273–5282, 2018.

[30] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. In
International Conference on Learning Representations, 2018.

[31] Eric Wong and J Zico Kolter. Provable defenses against adversarial examples via the convex outer

adversarial polytope. In International Conference on Machine Learning, 2018.

[32] Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial

defenses. In Advances in Neural Information Processing Systems, pages 8400–8409, 2018.

[33] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarking

machine learning algorithms. CoRR, abs/1708.07747, 2017.

[34] Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efﬁcient neural network
robustness certiﬁcation with general activation functions. In Advances in Neural Information Processing
Systems, pages 4939–4948, 2018.

[35] Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. RecurJac: An efﬁcient recursive algorithm for
bounding Jacobian matrix of neural networks and its applications. In AAAI Conference on Artiﬁcial
Intelligence, 2019.

13

A Appendixes

A.1 Derivation of the dual form

Consider the primal problem in (3). The Lagrangian can be written as

L(δ, λ) =

1
2

δT δ − λT (Aδ + b)

The dual problem is then

Taking derivative of Lagrangian we get

max
λ≥0

min
δ

L(δ, λ)

∂
∂δ

L = δ − AT λ = 0,

which gives us the primal-dual relationship δ = AT λ. Substitute this back to the dual problem we get

max
λ≥0

−

1
2

λT AAT λ − λT b.

A.2 Geometric meaning of our veriﬁcation bound

Here we discuss the geometric meaning of the following veriﬁcation bound for 1-NN (derived in (7)):

(cid:15) = min
j:yj (cid:54)=1

(cid:0) max
i:yi=1

max((cid:107)z − xj(cid:107)2 − (cid:107)z − xi(cid:107)2, 0)
2(cid:107)xj − xi(cid:107)

(cid:1) ≤ (cid:15)∗.

(11)

The inner value is the distance between z to the bisection between xi and xj, which means if we want to
perturb z to make it closer to xj than xi, the perturbation must be larger than the inner value. Then, if we
want to perturb z such that the nearest neighbor is xj, we need to by-pass all the bisections so we need to
take the max operation among all the distances to bisections. And a lower bound of (cid:15)∗ can be computed by
taking minimum over all the xj.

A.3 Proof of Lemma 1

Proof. By deﬁnition we have ∇D(j)(λ) = −AAT λ − b, so

∇D(j)(λ∗) = −AAT λ∗ − b

= −Aδ∗ − b.

∇D(j)
i

(λ∗) = −aT

i δ∗ − bi

= (xi − xj)T δ∗ −

≤ (cid:107)xi − xj(cid:107)(cid:107)δ∗(cid:107) −

(cid:107)z − xi(cid:107)2 − (cid:107)z − xj(cid:107)2
2
(cid:107)z − xi(cid:107)2 − (cid:107)z − xj(cid:107)2
2

.

Therefore, when (8) holds, by KKT conditions of the dual problem we know λ∗

i = 0.

14

Figure 4: Veriﬁcation (Lower bounds) via
(10) for Binary-MNIST.

Figure 5: Veriﬁcation (Lower bounds) via
(10) for MNIST.

A.4 Attack for the K > 1 case

Note that the problem is equivalent to forming A = [A1, A2] where the i-th row of A1 is (A1)i = (xj1 − xi)
and (A2)i = (xj2 − xi) for all class-1 instances {i : yi = 1}. We can ﬁrst choose j1 to be the class-2
instance closest to z, then try different j2 (sorting according to the distance to j1). After solving each pair of
j1, j2, we can try to remove one row of A1 and A2 which corresponds to j3. Note that only removing j3 with
nonzero λj3 can change the result and there are only few nonzero λs, so we could simply try all of them.

A greedy and more efﬁcient version is illustrated in Algorithm 2.

Algorithm 2: QP-greedy

Input: Target instance z, database {(xj, yj)}n

j=1, neighbor parameter K.

1 while True do
2

Select a size (cid:100)(K + 1)/2(cid:101) subset of other-class instances S− all with the same label;
Solve a QP problem to make S− nearest to z + δ;
if feasible then
break;

3

4

5

end

6
7 end
8 Select a size (cid:98)(K − 1)/2(cid:99) subset of same-class instances S+ (belonging to the class the same with the

one of the test instance), of which dual variables are not 0;

9 Solve a QP problem to make S− nearest to z + δ without constraints on S+;
10 return δ;

A.5 More experimental results for K-NN veriﬁcation

Since time complexity of our veriﬁcation method for K-NN is independent to K, we can efﬁciently compute
lower bounds of the minimum adversarial perturbation for a large K. Experimental results on Binary-MNIST
are illustrated in Figure 4.

The veriﬁcation method can be extended to the multi-class case. A simple way is just taking the true label
of the test instance as positive (label 1), and the others as negative (label 2). It could be easily veriﬁed that (cid:15)
in (10) is still a lower bound. Experimental results on MNIST are illustrated on Figure 5.

15

01020304050K2.02.12.22.3ℓ2 norm of perturbation01020304050K1.11.21.31.4ℓ2 norm of perturbationTable 4: The mean number of subproblems and the mean runtime of 100 correctly classiﬁed test instances. The
only difference of the two rows is whether sorting negative examples as a ﬁrst stage. No parallel mechanism
is employed across test instances.

# subproblems

runtime (s)

w/ sorting
w/o sorting

2.530
25.490

8.301
36.737

A.6 Effect of sorting in our Algorithm 1

We study whether sorting improves efﬁciency. All training data of MNIST are used as training instances. 100
correctly classiﬁed test instances are sampled randomly. All components of Algorithm 1 are employed, and
nscr = 8, i.e., 8 positive instances are used for screening. We report the mean number of subproblems and
the mean runtime of the 100 test instances. No extra parallel mechanism is employed for test instances. As
Table 4 shows, sorting reduces the number of subproblems and improves efﬁciency.

16

