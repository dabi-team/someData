1
2
0
2

l
u
J

3

]

G
L
.
s
c
[

1
v
5
2
3
1
0
.
7
0
1
2
:
v
i
X
r
a

Fair Decision Rules for Binary Classiﬁcation

Oktay G¨unl¨uk Connor Lawless

School of Operations Research and Information Engineering

Cornell University

July 6, 2021

Abstract

In recent years, machine learning has begun automating decision making in ﬁelds as varied as college

admissions, credit lending, and criminal sentencing. The socially sensitive nature of some of these

applications together with increasing regulatory constraints has necessitated the need for algorithms

that are both fair and interpretable. In this paper we consider the problem of building Boolean rule sets

in disjunctive normal form (DNF), an interpretable model for binary classiﬁcation, subject to fairness

constraints. We formulate the problem as an integer program that maximizes classiﬁcation accuracy

with explicit constraints on two diﬀerent measures of classiﬁcation parity: equality of opportunity and

equalized odds. Column generation framework, with a novel formulation, is used to eﬃciently search

over exponentially many possible rules. When combined with faster heuristics, our method can deal with

large data-sets. Compared to other fair and interpretable classiﬁers, our method is able to ﬁnd rule sets

that meet stricter notions of fairness with a modest trade-oﬀ in accuracy.

1 Introduction

With the explosion of artiﬁcial intelligence in recent years, machine learning (ML) has begun taking over

key decision making tasks in a variety of areas ranging from ﬁnance to driving. While in some applications

the objective of the ML model is purely predictive accuracy, in others, such as lending or hiring, additional

considerations come into play. In particular, when the decision making task at hand has a societal impact,

a natural question is whether or not the ML model is being fair to all those aﬀected. Recent results have

shown ML algorithms to be racially biased in a number of applications including facial identiﬁcation in

picture tagging and predicting criminal recidivism [33]. Further complicating the problem is the need for

model interpretability in certain classiﬁcation applications where ML models complement human decision

making, such as criminal justice and medicine. In these applications transparency is necessary for domain

experts to understand, critique, and consequently trust the ML models. With these dueling objectives in

mind, practitioners need to design classiﬁcation algorithms that are accurate, fair AND interpretable.

1

 
 
 
 
 
 
Many common classiﬁcation models can easily be expressed as mathematical optimization problems.

Discrete optimization is a natural tool for these problems as many interpretable machine learning models can

be represented by low-complexity discrete objects. So far, integer programming (IP), a promising technique

within discrete optimization that has been successful in many industrial applications such as production

planning, scheduling, and logistics, has been sparsely used in machine learning applications. An important

reason behind this is the presence of big datasets in ML applications which lead to large-scale IPs that are

considered to be computationally intractable. However with the help of algorithmic advances combined with

improvements in computational hardware IP methods are becoming an increasingly promising approach for

certain ML problems such as optimal decision trees [3, 8, 12, 34], risk scores [39], and rule sets [16] amongst

others.

In this paper, we focus on a well-studied interpretable class of ML models for binary classiﬁcation, namely

rule sets in disjunctive normal form (DNF, ’OR-of-ANDs’). For example, a DNF rule set with two rules for

predicting criminal recidivism could be

(cid:2)(Priors≥3) and (Age≤45) and (Score Factor = TRUE)(cid:3)

OR
(cid:2)(Priors≥20) and (Age≥45)(cid:3)

where Priors, Age, and Score Factor are features related to the defendant. This rule set has two clauses,

each of which check certain conditions on the features of the data. The fewer the clauses or conditions in

each clause, the more interpretable the rule set. In contrast to other interpretable classes of rule sets such

as decision trees [3, 8, 10, 24, 36], and decision lists [4, 29, 31, 37, 42, 45] the rules within a DNF rule

set are unordered and have been shown in a user study to require less eﬀort to understand [27]. In recent

years, discrete optimization approaches to learning DNF rule sets have seen impressive results in empirical

interpretable machine learning, including winning the FICO interpretable machine learning challenge [1, 16].

However there has been little work to extend these frameworks to include fairness constraints - even though

many applications requiring interpretability are socially sensitive in nature, including the credit approval

application in the FICO challenge.

1.1 Our contribution

In this paper we present an integer programming formulation to build Boolean DNF rule sets subject to ex-

plicit constraints on two notions of fairness: equality of opportunity and equalized odds [23]. Our framework

also allows for explicit bounds on the complexity (i.e. number of clauses and conditions) of the rule set, and

by extension it’s interpretability. Unlike other popular fair classiﬁcation algorithms, we directly incorporate

the original fairness constraints into our formulation instead of using relaxed versions of the criteria. The

addition of both the fairness and complexity constraints allows our approach to generate rule sets that trade

oﬀ accuracy, interpretability, and fairness.

2

Instead of directly optimizing for rule set accuracy our formulation optimizes for Hamming loss, a proxy

for 0-1 classiﬁcation error. This new objective function leads to a signiﬁcantly smaller IP formulation which

can be solved much faster while approximating the 0-1 objective closely. We use a column generation (CG)

framework to search over the candidate rules, only considering rules that can improve predictive accuracy

subject to the fairness constraints. To generate candidate clauses we use an integer programming formulation

that solves the pricing problem.

To help scale our method to larger data sets, we mine rules from heuristic tree-based methods as a

warm-start operation on our column generation procedure. In addition we incorporate other heuristics, such

as sub-sampling data points and features in the pricing problem, to further speed up solve time. These

computational ideas enable our framework to generate high-performing rule sets in a fraction of the time of

other IP-based approaches such as optimal decision trees.

We also present extensive computational studies on our method, and compare it to other state of the art

fair and interpretable machine learning algorithms. In addition, we provide empirical analysis of Hamming

loss as a proxy for 0-1 loss, and compare our column generation approach to other heuristic rule mining

strategies.

1.2 Related work

Our work builds upon two areas of related work: discrete optimization for interpretable ML, and fair ML.

Discrete optimization for machine learning: The last few years have seen a renewed interest in

using discrete optimization to solve machine learning problems. Ustun and Rudin have explored the use of

IP methods to create sparse linear models for classiﬁcation [40] and risk scores [39]. A number of researchers

have also approached the problem of constructing decision trees [3, 8, 12, 34]. Most of these approaches aim

to establish a certiﬁcate of optimality at the expense of computational eﬀort, requiring hours of computation

time.

Recent work in constructing rule sets has relied on rule miners to generate lists of candidate clauses

[28, 41, 43]. In contrast, our approach jointly generates candidate clauses and selects the optimal rule set.

Closely related to our approach, Dash et al.

[16] also formulate constructing DNF rule sets as an IP and

generate candidate clauses using column generation. Compared to Dash et al., our formulation includes

the fairness constraints together with some additional constraints that are not necessary without fairness

constraints. In addition, we use a more compact formulation for the pricing problem and reduce the number

of constraints by a factor of the dimensionality of the feature space. We also present a diﬀerent approach

to handle the upper bounds in the linear relaxation that prevents cycling which might happen with the

formulation used by Dash et al.. Computationally, we also diﬀer in the use of heuristic rule mining from

tree-based algorithms as a warm start to our column generation process.

Fair machine learning: Quantifying fairness is not a straight forward task and a number of metrics

have been proposed in the fair machine learning literature. These metrics broadly fall into three groups:

3

disparate treatment, classiﬁcation parity, and calibration [15]. The ﬁrst measure, disparate treatment, simply

excludes sensitive features (i.e. race, gender), or proxies of these sensitive attributes, from the data - however

removing sensitive attributes can lead to sub-optimal predictive performance [15]. Classiﬁcation parity, or

group fairness, ensures that some measure of prediction error (ex. Type I/II error, accuracy) is equal across

all groups. Recent results have built fair classiﬁers around various related metrics including demographic

parity [2, 11, 19, 20, 25], equalized odds [2, 23, 46], and equality of opportunity [23, 46]. We note that these

metrics are only meaningful if the algorithm is well calibrated (i.e. once a prediction has been made, it should

mean the same thing for each individual regardless of group). More importantly, recent impossibility results

[13, 26] have shown that simultaneously attaining perfect calibration and certain measures of classiﬁcation

parity is not possible. Thus, the chief goal of a fair classiﬁer is to maximize predictive accuracy subject to

some requirement on fairness.

Our work focuses on explicitly integrating fairness considerations directly into the training of a classiﬁ-

cation model. Previous work in this area has focused on adding either some form of fairness regularization

to the loss function [7, 48], or as a constraint in the underlying optimization problem [3, 47, 46]. However

many current approaches require the use of a relaxed version of the fairness constraints (i.e. convex, linear)

during optimization [17, 44, 46], which have been shown to have sub-par fairness on out-of-sample data [32].

Similar to our approach, Aghaei et al. [3] formulate optimal decision trees subject to explicit constraints on

fairness. However, unlike our approach which uses heuristics to speed up the solve time, their approach aims

to solve the MIP formulation to optimality, requiring multiple hours to construct a tree as opposed to the

ﬁve minute time limit we place on our approach.

1.3 Outline of the paper

Section 2 gives an overview of the two notions of fairness metrics we consider. Section 3 introduces our MIP

formulation for constructing boolean rule sets and the column generation procedure to generate candidate

clauses. We discuss computational tricks to improve the speed and scalability of our framework in section

4, and present empirical results in section 5. Comprehensive experimental results are left for the appendix.

2 Fairness metrics

As a starting point we consider the standard supervised binary classiﬁcation setting where given a training

set of n data points (Xi, yi) with labels yi ∈ {−1, 1} and features Xi ∈ {0, 1}p for i ∈ I = {1, . . . , n}, the
goal is to design a classiﬁer d : {0, 1}p → {−1, 1} that minimizes the expected error P(d(X) (cid:54)= Y ) between

the predicted label and the true label for unseen data. Note that assuming the data to be binary-valued is

not a restrictive assumption in practice and we discuss how to deal with numerical and categorical features

in Section 4.

Now consider the case when each data point also has an associated group (or protected feature) gi ∈ G

4

where G is a given discrete set and the classiﬁer is not only required to predict the labels well, but it is also

required to treat each group fairly. In particular, we will focus on two measures of group fairness related to

classiﬁcation parity: equality of opportunity, and equalized odds.

2.1 Equality of opportunity

Equality of opportunity requires the Type II error rate (i.e. false negative rate) to be equal across groups

by enforcing the following condition [23]:

P(d(X) = −1|Y = 1, G = g) = P(d(X) = −1|Y = 1)

(1)

for all g ∈ G. In other words, the false negative rate of the classiﬁer is required to be independent of the

group the data point belongs to. This criterion ensures fairness when there is a much larger societal cost to

false negatives than false positives, making it particularly well-suited for applications such as loan approval

or hiring decisions. For example, in the context of hiring, it ensures qualiﬁed candidates would be oﬀered a

job with equal probability, independent of their group membership (ex. male/female).

2.2 Equalized odds

A stricter condition on the classiﬁer is to require that both the Type I and Type II error rates are equal

across groups [23]. This requirement prevents possible trade-oﬀ between false negative and false positive

errors across groups and can be seen as a generalization of the equality of opportunity criterion to include

false positives. To achieve equalized odds, together with equation (1), the following condition is also enforced:

P(d(X) = 1|Y = −1, G = g) = P(d(X) = 1|Y = −1)

(2)

for all g ∈ G. This notion of fairness equalizes the error rate between the groups for both Type I and Type

II errors, but may be overly restrictive in applications where there is little diﬀerence in the societal cost

between the two types of errors.

In a practical setting, it is unrealistic to expect to ﬁnd classiﬁers that can satisfy the above criteria

exactly. In fact, in most non-trivial applications strong adherence to fairness criteria come at a large cost

to accuracy [26] and therefore one needs to consider how much these conditions are violated as a measure

of fairness. For example, in the context of equality of opportunity, the maximum violation can be used to

measure the unfairness of the classiﬁer by the following expression:

∆(d) = max
g,g(cid:48)∈G

(cid:12)
(cid:12)
(cid:12)

(cid:12)
P(d(X) = −1|Y = 1, G = g) − P(d(X) = −1|Y = 1, G = g(cid:48))
(cid:12)
(cid:12)

When training the classiﬁer d, one can then use ∆(d) in the objective function as a penalty term or can

explicitly require a constraint of the form ∆(d) ≤ (cid:15) to be satisﬁed by the classiﬁer d. We will focus on the

latter case as it allows for explicit control over tolerable unfairness.

5

3 Classiﬁcation framework: boolean decision rule sets

For the remainder of this paper, we focus on one speciﬁc type of classiﬁer: decision rule sets in DNF (OR-

of-ANDs). Given a dataset with a collection of features, a decision rule consists of a subset of the features

each with an associated condition (i.e. Age ≥ 25). A rule is satisﬁed by a data point if it meets all of the

conditions included in the rule. A decision rule set is composed of a collection of rules and a data point

satisﬁes the rule set if it meets any of its rules. We refer to a set of rules that could be included in a rule set

as a pool of candidate rules.

We now introduce our method to construct DNF rule sets for binary classiﬁcation subject to fairness

constraints, henceforth referred to as Fair CG. Note that when the input data is binary-valued, a DNF-rule

set simply corresponds to checking whether a subset of features satisﬁes a speciﬁc combination of 0s and 1s.

Moreover if each data point includes the complement of every feature, the rule set simpliﬁes even further

and only needs to check if a subset of features are all 1 for a given data point. Consequently, if there are p

binary features there can only be a ﬁnite number (2p − 1) of possible decision rules. Therefore, it is possible

to enumerate all possible rules and then formulate a large scale integer program (IP) to select a small subset

of rules that minimizes error on the training data. In this framework, it is also possible to explicitly require

the rule set to satisfy certain properties such as fairness or interpretability. However, for most practical

applications such an IP would be onerously large and computationally intractable. Instead, we solve the

continuous relaxation (LP) of the IP using column generation. Consequently, instead of enumerating all

possible rules, one can enumerate those that can potentially improve classiﬁcation error.

3.1 0-1 loss

When constructing a rule set, our ultimate aim is to minimize 0-1 classiﬁcation error. Let K denote the set

of all possible DNF rules and Ki ⊂ K be the set of rules met by data point i ∈ I. Assume that the data

points are partitioned into two sets based on their labels:

P = {i ∈ I : yi = 1}, and N = {i ∈ I : yi = −1}.

We denote P and N the positive and negative classes respectively. Furthermore, let K ⊆ K represent a DNF

rule set composed of candidate rules from K. For data points from the positive class (i.e. i ∈ P), the 0-1

loss is simply the indicator that the data point meets no rules in rule set (i.e. |Ki ∩ K| = ∅). For points in

the negative class, the 0-1 loss is the indicator of whether the data point meets at least one rule in the rule

set (i.e. |Ki ∩ K| > 0). Putting both terms together, we get that the 0-1 loss for a data point (xi, yi) and

rule set K is as follows:

Deﬁnition 1 (0-1 loss).

(cid:96)01(xi, yi, K) = I(i ∈ P)I(|Ki ∩ K| = 0) + I(i ∈ N )I(|Ki ∩ K| > 0)

6

Let wk ∈ {0, 1} be a variable indicating if rule k ∈ K is selected; ζi ∈ {0, 1} be a variable indicating if

data point i ∈ P ∪ N is misclassiﬁed. With this notation in mind, the problem of identifying the rule set

that minimizes 0-1 loss becomes:

min

(cid:88)

i∈P

ζi +

(cid:88)

i∈N

ζi

s.t.

ζi +

(cid:88)

k∈Ki

wk ≥ 1,

i ∈ P

wk ≤ ζi,

i ∈ N , k ∈ Ki

wk, ζi ∈ {0, 1},

k ∈ K, i ∈ P ∪ N

(3)

(4)

(5)

(6)

Any feasible solution ( ¯w, ¯ζ) to (4)-(6) corresponds to a rule set S = {k ∈ K : ¯wk = 1}. Constraint (4)

identiﬁes false negatives by forcing ζi to take value 1 if no rule that is satisﬁed by the point i ∈ P is selected.

Similarly, constraint (5) identiﬁes false positives by forcing ζi to take a value of 1 if any rule satisﬁed by

i ∈ N is selected. The objective forces ζi to be 0 when possible, so there is no need to add constraints

to track whether data point i is classiﬁed correctly in this formulation. Note that there are a potentially

exponential number of constraint (5) as there needs to be one constraint for each rule met by data point i.

In the presence of a cardinality constraint on the wk (i.e. a limit on the total number of rules), the set of

constraints (5) for each data point can be aggregated together to provide a form modern solvers will handle

more eﬃciently [14].

3.2 Hamming loss

While our aim is to optimize the 0-1 loss, the corresponding IP formulation is large and hard to solve in

practice. Instead, we follow the approach of [16] and optimize for Hamming loss, a proxy for classiﬁcation

error. For Hamming loss every data point of the positive class that is classiﬁed incorrectly incurs a cost of

1, and every data point in the negative class incurs a cost of 1 for every rule in the selected rule set that it

meets.

Deﬁnition 2 (Hamming Loss).

(cid:96)h(xi, yi, K) = I(i ∈ P)I(|Ki ∩ K| = 0) + I(i ∈ N )|Ki ∩ K|

Notice that Hamming loss is asymmetric with respect to errors for the positive and negative classes.

Speciﬁcally, while the loss for a false negative remain the same as 0-1 loss, a false positive may incur a loss

greater than 1 if it is satisﬁes multiple rules. Using the same notation as the previous IP formulation, the

problem of ﬁnding the rule set that minimizes Hamming loss is simply:

7

min

(cid:88)

i∈P

ζi +

(cid:88)

(cid:88)

wk

i∈N

k∈Ki

s.t.

ζi +

(cid:88)

k∈Ki

wk ≥ 1,

i ∈ P

wk, ζi ∈ {0, 1},

k ∈ K, i ∈ P ∪ N

(7)

(8)

(9)

The objective is now Hamming loss where for each i ∈ N the second term adds up the total number of

selected rules satisﬁed by i. Compared to the 0-1 loss formulation, this formulation does not have the large

number of constraints needed to track false positives. This leads to a much more compact formulation that

is more computationally eﬃcient to solve in practice.

However while it leads to a smaller IP formulation, we next observe that optimizing for Hamming loss

can lead to arbitrarily worse performance on the 0-1 loss than optimizing for 0-1 loss directly.

Theorem 3.1 (Hamming loss vs. 0-1 loss). Consider a dataset D = [(xi, yi)]n

0 . Let

K ∗

(cid:96) = argmin
K⊆K

n
(cid:88)

i=1

(cid:96)(xi, yi, K)

be the optimal DNF rule set K for loss function (cid:96) using a set of candidate rules K. There does not exist a

global constant M ∈ [1, ∞) such that:

M Exi,yi∼D[(cid:96)01(xi, yi, K ∗

(cid:96)01)] ≥ Exi,yi∼D[(cid:96)01(xi, yi, K ∗

(cid:96)h )]

for all rule sets K, data D. In other words, when evaluating the 0-1 loss on a dataset D, the rule set K ∗
(cid:96)h

selected to minimize Hamming loss can perform arbitrarily worse than the rule set K ∗
(cid:96)01

selected to minimize

0-1 loss.

Proof. Let Dn be a set of n points sampled uniformly from the set X = {(x, 1) : x2

1 + x2

2 = 1, x ∈ R2

+}}.

Consider the augmented set of points D which contains Dn−2 together with 2 additional points at the origin
with label -1, and the set of rules K = {Ix2=αx1 : α ∈ R+}. Graphically, the problem is represented in ﬁgure

1. Note that the rules are simply lines intersecting the origin.

Now consider the problem of constructing the optimal rule set. Note that to properly classify any data

point in Dn−2 correctly using rules from K we would need to include a rule that misclassiﬁes the two points

at the origin (as they meet every rule). Every data point in Dn−2 also needs its own rule, as each rule only

covers at most one point in X . When optimizing for 0-1 loss, it is clear that the optimal rule set for n > 4 is to

simply include the rules needed to classify all data points in Dn−2 correctly which results in an expected loss
of 2

n . However, the asymmetry in Hamming loss means that including any rule, and by extension classifying
any point in Dn−2 correctly incurs a cost of 2. Thus the optimal solution when optimizing for Hamming loss
is the empty rule set giving an expected 0-1 loss of n−2
n ≥ n−2

n . Clearly no constant M exists such that M 2

n

for all n, proving the desired claim.

8

Figure 1: (Left) Sample dataset D. Note there are two points at the origin, and n − 2 data points in Dn−2.

(Right) Sample rules in K. Note that they are all lines going through the origin.

While theoretically, Hamming loss can lead to arbitrarily worse performance on the 0-1 classiﬁcation

problem, we decide to use it in our formulation out of practicality. Its compact formulation can be solved more

eﬃciently than the 0-1 formulation, leading to a more computationally tractable framework. Empirically,

models trained with Hamming loss also perform comparably to those trained with 0-1 loss (as discussed in

section 5.1). In brief, we use Hamming loss because it is a computationally eﬃcient proxy for 0-1 loss that

performs well in practice, despite its theoretical limitations.

3.3 Base formulation without fairness considerations

In the preceding IP formulations, there is no need to ensure that ζi = 1 only when data point i is misclassiﬁed
as ζi = 0 in any optimal solution provided that (cid:80)

k∈Ki wk = 0. However, this is not the case when additional
fairness constraints are added to the formulation. To ready our formulation for the fairness setting, we

add additional constraints to correctly track true positives. In our base formulation below, we also add a

constraint on the total complexity of the selected rule set. Let ck denote the complexity of rule k ∈ K

which is deﬁned as a ﬁxed cost of 1 plus the number of conditions in the rule. The constraint helps control

over-ﬁtting and also ensures the selected rule set is not too complicated and thus interpretable. Building

upon the previous Hamming loss IP formulation, the problem for selecting the optimal rule set becomes:

9

zmip = min

(cid:88)

i∈P

s.t.

ζi +

(cid:88)

(cid:88)

wk

i∈N

ζi +

k∈Ki
(cid:88)

wk ≥ 1

i ∈ P

k∈Ki
(cid:88)

2wk ≤ C

Cζi +

i ∈ P

k∈Ki
(cid:88)

ckwk ≤ C

k∈K

w ∈ {0, 1}|K|, ζ ∈ {0, 1}|P|

(10)

(11)

(12)

(13)

(14)

Constraint (12) ensures that ζi can only take a value of 1 if no rules satisﬁed by i ∈ P are selected.

Here, we use the fact that ck ≥ 2 for all k ∈ K to get a tighter formulation. In principle, an even tighter

formulation would replace the co-eﬃcient of 2 with ck. However, this complicates our column generation

procedure, turning the pricing problem into a quadratic integer program - a much more computationally

demanding problem to solve. Any approach to solve the pricing problem with a co-eﬃcient of 2 (i.e. the

formulation presented), and then substitute the true complexity into the master problem (i.e. use ck instead

of 2 when solving the RMLP) runs the risk of cycling by repeatedly generating the same column. Constraint

(13) provides the bound on complexity of the ﬁnal rule set.

We denote this integer program the Master Integer Program (MIP), and its associated linear relaxation

the Master LP (MLP) (obtained by dropping the integrality constraint).

3.4 Fairness constraints

To extend the IP model to incorporate the fairness criteria discussed in Section 2 we add new constraints to

the master problem. For each group g ∈ G we denote the data points that have the protected feature g with

Gg = {i ∈ I : gi = g}

and let Pg = P ∩ Gg and Ng = N ∩ Gg. For simplicity, we describe the constraints assuming G = {1, 2} and

note that extending it to multiple groups is straightforward and simply adds constraints that scale linearly

with the number of groups.

Equality of opportunity: To incorporate the equality of opportunity criterion, we bound the diﬀerence

in the false negative rates between groups.

1
|P1|

1
|P2|

(cid:88)

i∈P1
(cid:88)

i∈P2

ζi −

ζi −

1
|P2|

1
|P1|

(cid:88)

i∈P2
(cid:88)

i∈P1

ζi ≤ (cid:15)1

ζi ≤ (cid:15)1

10

(15)

(16)

Constraints (15) and (16) bound the maximum allowed unfairness, denoted by ∆ in the previous section, by

a speciﬁed constant (cid:15)1 ≥ 0. If (cid:15)1 is chosen to be 0, then the fairness constraint is imposed strictly. Depending

on the application, (cid:15)1 can also be larger than 0, in which case a prescribed level of unfairness is tolerated.

Equalized odds: Similar to our use of Hamming loss as a proxy for optimizing 0-1 loss for the negative

class, we use it as a proxy for equalized odds. Speciﬁcally, instead of bounding the diﬀerence in false positive

rates between groups we bound the diﬀerence in the Hamming loss terms for the negative class. Thus in

conjunction with constraints (15) and (16), we also include the following constraints in the formulation:

1
|N1|

1
|N2|

(cid:88)

(cid:88)

wk −

i∈N1
(cid:88)

k∈Ki
(cid:88)

i∈N2

k∈Ki

wk −

1
|N2|

1
|N1|

(cid:88)

(cid:88)

i∈N2
(cid:88)

k∈Ki
(cid:88)

i∈N1

k∈Ki

wk ≤ (cid:15)2

wk ≤ (cid:15)2,

(17)

(18)

where (cid:15)2 ≥ 0 is a given constant. While constraints (15) and (16) bound the misclassiﬁcations gap between

group for the positive class P, constraints (17) and (18) do the same for negative class Z using Hamming

Loss as a proxy. The tolerance parameter (cid:15)2 in (17) and (18) can be set equal to (cid:15)1 in (15) and (16),

or, alternatively, (cid:15)1 and (cid:15)2 can be chosen separately. Note that we normalize the Hamming Loss terms to

account for the diﬀerence in group sizes and positive response rates between groups.

Similar to using Hamming loss in the objective, the Hamming loss proxy for false positives can lead

to arbitrarily unfair classiﬁers with respect to the true equalized odds criterion. However once again, the

Hamming loss proxy performs well empirically and generates classiﬁers that meet the true fairness constraint.

Other fairness metrics: While we restrict our focus to equality of opportunity and equalized odds, we note

that our framework can be adapted for any notion of classiﬁcation parity (i.e. balancing false positive rates

or overall accuracy). The only caveat is that for notions of fairness involving false positives our framework

would use the Hamming Loss term for false positives (similar to equalized odds). We also note that under

some notions of fairness (i.e. ensuring similar accuracy between the two groups) there is no guarantee that

the IP will be feasible for an arbitrary set of candidate rules. In such cases, a two stage approach could

be used to ﬁrst generate a set of candidate clauses that are feasible for a given fairness criteria prior to

optimizing for accuracy.

3.5 Column generation framework

Due to the exponential nature of our formulation, it is not computationally tractable to enumerate all

possible rules. Consequently, it is not practical to solve the MIP (10)-(14) using standard branch-and-bound

techniques [30] even without the fairness constraints. To overcome this problem, we solve the LP relaxation

of MIP using the column generation technique [14, 21] without explicitly enumerating all possible rules. Once

we solve the LP to optimality or near optimality, we then restrict our attention to the rules generated during

the process and pick the best subset of these rules by solving a restricted MIP. We note that it is possible to

integrate the column generation technique with branch-and-bound to solve the MIP to provable optimality

11

using the branch-and-price approach [6]. However, this would be quite time consuming in practice.

To solve the LP relaxation of the MIP, called the MLP, we start with a possibly empty subset ˆK ⊂ K of

all possible rules and solve an LP restricted to the variables associated with these rules only. Once this small

LP is solved, we use its optimal dual solution to identify a missing variable (rule) that has a negative reduced

cost. The search for such a rule is called the pricing problem and in our case this can be done by solving
a separate integer program. If a rule with a negative reduced cost is found, then ˆK is augmented with the

associated rule and the LP is solved again and this process is repeated until no such rule can be found. For

large problems, solving the MLP to optimality is not always computationally feasible. Out of practicality, we

put an overall time limit on the column generation process and terminate without a certiﬁcate of optimality

upon reaching it.

Equality of opportunity: Given a subset of rules ˆK ⊂ K, let the restricted MLP, deﬁned by (10)-(13),
(15)-(16) and denoted by RMLP, be the restriction of MLP to the rules in ˆK. In other words, RMLP is the
restriction of MLP where all variables wk associated with k ∈ K \ ˆK are ﬁxed to 0. Let (µ, α, λ, γ1, γ2) be

an optimal dual solution to RMLP, where variables µ, α, λ ≥ 0 are associated with constraints (11), (12),

and (13), respectively. Variables γ1 and γ2 are associated with fairness constraints (15) and (16). Using this
dual solution, the reduced cost of a variable wk associated with a rule k /∈ ˆK can be expressed as

ˆρk =

(cid:88)

i∈N

1{k∈Ki} −

(cid:88)

i∈P

µi1{k∈Ki} +

(cid:88)

i∈P

2αi1{k∈Ki} + λck

(19)

where the ﬁrst term simply counts the number of data points i ∈ Z that satisfy the rule k. Note that variable

wk does not appear in constraints (15) or (16) in RMLP and consequently (19) does not involve variables
γ1 or γ2. If there exists a k ∈ K \ ˆK with ˆρk < 0, then including variable wk in RMLP has the potential
of decreasing the objective function. Also note that ˆρk ≥ 0 for all k ∈ ˆK as the dual solution at hand is

optimal.

We can now formulate an integer program to ﬁnd a k ∈ K with the minimum reduced cost ˆρk. Remember

that a decision rule corresponds to a subset of the binary features J and classiﬁes a data point with a positive

response if the point has all the features selected by the rule. Let variable zj ∈ {0, 1} for j ∈ J denote if the

rule has feature j and let variable δi ∈ {0, 1} for i ∈ I denote if the rule misclassiﬁes data point i. Using
these variables, the complexity of a rule can be computed as (1 + (cid:80)

j∈J zj) and the reduced cost of the rule

becomes:

(cid:88)

i∈N

δi +

(cid:88)

i∈P

(2αi − µi)δi + λ(1 +

(cid:88)

j∈J

zj).

(20)

12

The full pricing problem thus simply minimizes (20) subject to the constraints:

Dδi +

δi +

(cid:88)

j∈Si
(cid:88)

j∈Si
(cid:88)

j∈J

zj ≤ D

i ∈ I −

zj ≥ 1

i ∈ I +

zj ≤ D

z ∈ {0, 1}|J|, δ ∈ {0, 1}|P|

(21)

(22)

(23)

(24)

where the set I − ⊆ I contains the indices of δi variables that have a negative coeﬃcient in the objective
(i.e. 2αi − µi < 0), and I + = I \ I −. Note that constraints (21) and (22) ensure that δi accurately reﬂects

whether the new rule classiﬁes data point i with a positive label, and constraint (23) puts an explicit bound

on the complexity of any rule using the parameter D. This individual rule complexity constraint can be set

independently of C in the master problem or can simply be set to C − 1.

We note that in [16] a similar pricing problem was formulated using the constraints δi + zj ≤ 1 for all

i ∈ P, j ∈ Si, in place of (21). The formulation we use here is computationally more eﬃcient as it has much

fewer constraints.

Equalized odds:

In this case the RMLP is deﬁned by (10)-(13), (15)-(18) and note that unlike

(15) and (16), constraints (17) and (18) do involve variables wk. Let (µ, α, λ, γ1, γ2, γ3, γ4) be an optimal
dual solution to RMLP, where variables γ3 and γ4 are associated with fairness constraints (17) and (18),
respectively. Using this dual solution, the reduced cost of a variable wk associated with k /∈ ˆK is similar to

the expression in (19), except it has the following 4 additional terms:

(cid:88)

i∈N1

γ3
|N1|

1{k∈Ki} −

(cid:88)

i∈N1

γ4
|N1|

1{k∈Ki} −

(cid:88)

i∈N2

γ3
|N2|

1{k∈Ki} +

(cid:88)

i∈N2

γ4
|N2|

1{k∈Ki}

Consequently, the pricing problem becomes

min (1 +

γ3 − γ4
|N1|

)

(cid:88)

i∈N1

δi + (1 +

γ4 − γ3
|N2|

)

(cid:88)

δi

i∈N2

+

(cid:88)

i∈P

(2αi − µi)δi + λ(1 +

(cid:88)

j∈J

zj)

s.t. (21) − (24).

Upper Bounds on the variables in the LP Relaxation: When solving the LP relaxation of the

IP model we relax not just the integrality of the decision variables, but also the upper bound of 1 on binary

variables. This is because modern optimization solvers implicitly add the upper bound on the binary decision

variables, causing an additional term in the formula for the reduced cost that is not accounted for in our

model. Take a simple example with one data point with a single binary feature (X = [1]) and a positive

response (Y = 1). Solving the master LP problem returns the following dual values:

The pricing problem predictably returns the optimal rule consisting of the single feature. However solving

the master LP with the optimal rule set gives identical dual values - prompting the column generation process

13

Constraint

(4)

(5)

(6)

Dual Value µ = 1 α = 0

λ = 0

Table 1: Dual Values

to cycle. However, if we explicitly add the upper bound on the binary variable (i.e. wk ≤ 1), we see that

it has an associated dual value of −1 giving the optimal rule a reduced cost of 0 in the pricing problem

(and thus a certiﬁcate that the LP relaxation has been solved). However, such a dual variable cannot be

integrated into the pricing problem formulation as each rule would have its own associated constraint and

thus the reduced cost term cannot be integrated into the objective of the pricing problem. This is not

just a feature of the toy example, this cycling phenomenon occurred during initial experiments with our

formulation. To get around this issue, we remove the constraint on wk being binary (i.e. we allow a rule to

be included multiple times in a rule set). During our experiments, we did not observe any instances where

wk > 0 for a solution.

4 Computational approach

We use the Python interface of Gurobi [22] to solve the linear and integer programs in our formulation. To

solve the MLP we use a barrier interior point method, which performs better in practice than simplex for

large sparse problems with many columns. For the pricing problem we use the default settings and return

all solutions generated during the algorithm’s run with negative reduced costs. In many problems, solving

either the integer pricing problem or the MLP to optimality can be computationally intensive. To place

a practical limit on the overall problem, we set time limits on the pricing problem and the overall column

generation (CG) process to solve the MLP.

During the solving process, the MIP solver retains a set of feasible solutions generated for the master

problem. We evaluate the 0-1 loss on the training set for all solutions generated, and return the rule set

with the lowest 0-1 loss rather than the optimal solution for the hamming loss problem. In other words,

while we optimize for hamming loss, we select the best solution from the candidate pool of feasible solutions

using 0-1 loss. This comes at a negligible computational cost (i.e. simply the cost of doing inference on the

training set for each solution), and can lead to a small bump in performance with respect to the 0-1 problem.

Table 2 summarizes the impact of selecting a ﬁnal rule set from the solution pool using hamming loss and

accuracy respectively. Selecting the ﬁnal rule set using accuracy leads to a modest increase in both train and

test set accuracy across all three datasets. Note that while the bump in performance is modest, it comes at

practically no additional computational cost.

For small problems, deﬁned roughly as having under 2000 data points and a couple hundred binary

features, we employ the CG framework described in Section 3.5, however for larger problems we use an

14

Table 2: Eﬀect of selecting a ﬁnal rule set from the solution pool using hamming loss and accuracy (standard

deviation in parenthesis)

Adult

Compas

Default

Hamming Accuracy Hamming Accuracy

Hamming

Accuracy

Hamming Loss

5305 (64)

5568 (441)

1624 (54)

1715 (306)

5950 (19)

5956 (15)

Train Accuracy

81.9 (0.1)

82.1 (0.2)

66.3 (0.3)

66.5 (0.3)

77.9 (0.003)

77.9 (0.03)

Test Accuracy

81.6 (0.7)

81.8 (0.8)

66.2 (2.0)

66.3 (2.7)

78.1 (0.02)

78.1 (0.5)

approximate version of the framework to limit the overall run time. For large datasets, the integer pro-

gramming formulation for the pricing problem may become computationally intractable and therefore we

sub-sample both our training data points and potential features to have on average 2000 rows and 100000

non-zeros, we then solve the pricing problem on the reduced problem.

To warm start our column generation process, we start by mining a candidate set of rules from quick tree-

based heuristics similar to [9] (see Section 5.2 for a discussion on how warm starting saves computationally

intensive CG iterations).

If a large number of candidate rules are generated (for both the heuristic and

integer programming formulation) we return the 100 rules with the lowest reduced costs. This is to trade oﬀ

the number of column generation iterations needed with the size of the restricted LP. Adding all columns

leads to larger candidate rule sets that slow down solving the RLP, while only adding the best new rule

tends to lead to more iterations of column generation needed.

5 Experimental results

To benchmark the performance of our algorithm, we evaluate it on three standard fair machine learning

datasets: default [18], adult [18], and compas [5]. For our experiments we include the sensitive attribute as

a feature, though our framework can easily work with the sensitive attribute exclude for inference. We refer

to [15] as to why including the sensitive attribute can lead to fairer results, absent regulatory constraints on

disparate treatment. Details on the sources of each dataset and how we pre-process data can be found in

the appendix.

To convert the data to be binary-valued, we use a standard methodology also used in [16, 38, 43]. For

categorical variables j we use one-hot encoding to binarize each variable into multiple indicator variables

that check Xj = x, and the negation Xj (cid:54)= x. For numerical variables we compare values against a sequence

of thresholds for that column and include both the comparison and it’s negation (i.e. Xj ≤ 1, Xj ≤ 2 and

Xj > 1, Xj > 2). For our experiments we use the sample deciles as the thresholds for each feature. We use

the binarized data for all the algorithms we test to control for the binarization method.

For all our results we used ten-fold cross validation and performed a two-stage process to generate a pool

of candidate rules and build a rule set. First, we run the CG algorithm on the training data for a subset of

15

potential hyperparameters (typically 3 diﬀerent complexity limits and epsilon values). Then, we use these

candidate rules and solve the master IP for a grid of potential epsilon values and complexity limits. To select

which complexity values to test we look at which complexity value leads to the best cross-validated accuracy

in the problem without fairness constraints and test values around and including it. We use a 45 second time

limit for each iteration of the pricing problem, and a ﬁve minute overall time limit for the column generation

process.

5.1 Hamming loss

To analyze the empirical performance of using Hamming Loss instead of 0-1 loss we ran a sequence of

experiments where we evaluated the 0-1 loss of rule sets trained under both objectives. For each experiment

we used the same pool of candidate rules generated by running our column generation procedure with diﬀerent

hyperparameters. We then ran the master model under both objectives with a ﬁxed fairness threshold ((cid:15))

and evaluated the rule set’s performance on both training and testing data. 10-fold cross-validation was used

to tune hyperparameters for each model separately.

Figure 2 shows the results for the compas dataset [5] with (cid:15) = 0.1. The left side of the ﬁgure is a violin

plot that shows the distribution of accuracy results over 1000 random splits of the dataset when optimizing

for each objective (each black dot represents an accuracy for a single split). We can see that Hamming Loss

and Accuracy have practically indistinguishable performance in terms of both train and test set accuracy.

However, the Hamming Loss model solves the IP problem in a fraction of the time of the model minimizing

accuracy as seen in the plot on the right. This shows that empirically, Hamming Loss is an eﬀective proxy

for accuracy - leading to comparable performance in a fraction of the computation time. Results for other

datasets and values for (cid:15) can be found in the appendix and show a similar trend.

Figure 2: Relative performance of models trained to maximize accuracy or minimize Hamming Loss respec-

tively under equality of opportunity constraint ((cid:15) = 0.025). The violin plots show the distribution of train

and test set accuracy for models under each objective for 100 random splits of the datasets.

16

TrainTestAdult0.790.800.810.820.830.84AccuracyTrainTestCompas0.580.600.620.640.660.680.70AccuracyTrainTestDefault0.740.750.760.770.780.79AccuracyHamming Loss0-1 LossTable 3: Mean (standard deviation) computation time in seconds for master model under diﬀerent objectives

with 600 s time limit

(cid:15)

0.025

Adult

0.1

Compas

Default

0.5

0.025

0.1

0.5

0.025

0.1

0.5

Hamming Loss

35.5 (48.6)

34 (24.1)

30.1 (27.1)

4.0 (3.5)

3.5 (3.2)

0.49 (0.3)

3.5 (0.8)

3.4 (0.7)

3.7 (0.7)

0-1 Loss

546 (101)

594 (33)

520 (78)

11.4 (6.5)

12.1 (6.5)

6.0 (3.6)

12.3 (6.1)

12.8 (7.2)

12.5 (4.7)

In addition to using Hamming Loss as a proxy for 0-1 error, we also use Hamming Loss terms to bound

the false negative error rate in the equalized odds formulation (constraints (17) and (18)) A natural question

is how well the Hamming Loss terms work as a proxy for false negatives in equalized odds (i.e. whether the

constraint does in fact bound allowable unfairness under equalized odds). Figure 3 plots the epsilon used to

constrain the Hamming Loss proxy for equalized odds on the x-axis, versus the true equalized odds of the

resulting rule set both on the training and testing data (averaged over 10 folds) for the three datasets. We

can see that in practice, the Hamming Loss version of the false negative constraint serves as an eﬀective proxy

for the true fairness constraint. Empirically, the rule sets trained with the Hamming Loss constraint never

have a true equalized odds larger than the prescribed (cid:15) for the training data. The out of sample equalized

odds does exceed the constraint in some scenarios, as expected due to it being new data not used to train

the model, but tracks relatively close to the true equalized odds on the training data. This is promising

evidence that the Hamming Loss terms are both an eﬀective proxy for the true fairness constraint and that

the fairness guarantees on the training set generalize well to unseen data.

Figure 3: Generalization of equalized odds constraint with Hamming loss proxy for false negative rate versus

true equalized odds gap measure on both training and testing data.

5.2 Rule mining

As we do not perform branch and price, our column generation procedure can be viewed as a heuristic for

generating a set of candidate rules to use in the master IP model. To that end, we evaluated how our method

compares to other rule mining heuristics. Following the procedure outlined in [9], we mined rules from a

17

0.020.040.060.080.10Training Equalized Odds Fairness Constraint ()0.020.040.060.080.10Observed Fairness (Max(TPR,TNR) Gap)Adult0.020.040.060.080.10Training Equalized Odds Fairness Constraint ()0.000.020.040.060.080.10Observed Fairness (Max(TPR,TNR) Gap)Compas0.0050.0100.0150.0200.0250.0300.0350.040Training Equalized Odds Fairness Constraint ()0.0050.0100.0150.0200.0250.0300.0350.040Observed Fairness (Max(TPR,TNR) Gap)DefaultTrainTestY=Xvariety of tree based models including CART, a random forest classiﬁer, and a fair decision tree learned

through the fair learn package [2]. We used tree-based models to mine rules because they have a natural

correspondence to rule sets, as every path from the root of the tree to any leaf node can be seen as a rule. We

extract all such rules where the label of the associated leaf node is for the positive class. For each tree-based

model, we trained the algorithm with a variety of hyperparameters and extract every rule generated by the

classiﬁer. We then run the master IP model with the extracted rules for a ﬁxed fairness criteria ((cid:15)) and do

cross-validation to select the best complexity bound with each rule set.

Figure 4 shows the relative performance of each rule set on the compas dataset for (cid:15) = 0.01 with the

equality of opportunity metric. FairCG produced the rule sets that lead to the lowest objective value when

compared to any other rule sources. However, there is a modest increase in performance by looking at the

union of all the rule sets. While they do not lead to superior performance, the rules mined from quicker

methods like random forests provide a powerful warm start to the column generation procedure to solve

the MIP. The second plot in ﬁgure 4 shows the number of column generation iterations needed to ﬁnd a

collection of rules with comparable accuracy to the mined rules from a random forest model. The results

show that using a pre-mined rule set can reduce the number of time intensive iterations of column generation

needed. Results for additional datasets and (cid:15) can be found in the appendix.

Figure 4:

(Left) Relative performance of rules mined from diﬀerent sources (’All’ signiﬁes union of all

the rule sets). (Right) Number of column generation iterations (blue line) needed to achieve comparable

performance to rules mined from random forest model (dotted line).

5.3 Fairness - accuracy trade-oﬀ

We compared our performance against three other popular interpretable fair binary classiﬁcation models:

Zafar 2017 [46], Hardt 2016 [23], and the exponential gradient method included in the Fair-Learn package

[2]. The ﬁrst two methods build fair logistic regression classiﬁers but take diﬀerent approaches. Zafar 2017

formulates the problem as a constrained optimization model and uses a convex relaxation of the fairness

constraint. Hardt 2016 also leverages a logistic regression classiﬁer but achieves fairness by selecting diﬀerent

18

025050075010001250150017502000Hamming Loss Objective0.00.10.20.30.40.50.6Train Set Accuracy0.00.10.20.30.40.50.6Test Set AccuracyDecision  Tree    Fair Decision    TreeRandom  ForestAll except  Fair CGFair CGAll02468101214Column Generation Iterations1600170018001900200021002200ObjectiveCold StartWarm Start: Starting Objectivediscrimination thresholds for the sensitive groups. The exponential gradient method from Fair-Learn works

by solving a sequence of cost-sensitive classiﬁcation problems to construct a randomized classiﬁer with low

error and the desired fairness. The framework works with any classiﬁer, however for our experiments we

chose to use a decision tree as the base learner.

We started by comparing the algorithm’s predictive accuracy in the absence of fairness criteria. Table

4 shows the 10-fold mean and standard deviation accuracy for each algorithm without fairness criteria (i.e.

(cid:15) = 1). While the algorithms are not trained to consider fairness, we report the average test set ’unfairness’

of each algorithm as a baseline for the amount of discrimination that happened in the absence of controls

on fairness. On two of the three datasets (compas and default), rule sets have the strongest predictive

performance. However, on the adult dataset the rule sets were outperformed by the two logistic regression

based classiﬁers (Zafar and Hardt).

Table 4: Mean accuracy and fairness results with no fairness constraints (standard deviation in parenthesis).

Equality of opportunity and equalized odds refer to the amount of unfairness between the two groups under

each fairness metric.

Fair CG Zafar 2017 Hardt 2016 Fair Learn

Accuracy

82.5 (0.5)

85.2 (0.5)

83.0 (0.4)

82.4 (0.4)

Adult

Equality of Opportunity

7.6 (0.5)

11.9 (3.7)

18.2 (4.8)

11.5 (4.6)

Equalized Odds

7.6 (0.5)

11.9 (3.7)

18.2 (4.8)

11.5 (4.6)

Accuracy

67.6 (1.1)

64.6 (1.9)

65.9 (2.7)

65.8 (2.9)

Compas

Equality of Opportunity 23.8 (5.3)

42.8 (5.4)

23.7 (6.4)

21.7 (7.1)

Equalized Odds

24.1 (5.1)

47.6 (5.8)

27.0 (5.2)

24.9 (4.5)

Accuracy

82.0 (0.7)

81.2 (0.8)

77.9 (1.7)

77.9 (1.7)

Default

Equality of Opportunity

1.3 (0.6)

2.7 (1.9)

Equalized Odds

1.9 (0.5)

4.2 (2.5)

0 (0)

0 (0)

0 (0)

0 (0)

We now consider adding constraints on the allowable unfairness. For each model we varied the hyperpa-

rameters in all the algorithms, performing 10-fold cross validation for each hyperparameter, to generate the

accuracy fairness trade-oﬀs. Figure 5 plots the fairness accuracy trade-oﬀs under both notions of fairness.

Fair CG generated classiﬁers that performed well, dominating all other fair classiﬁers on two of the three

datasets. Our algorithm performs especially well when generating classiﬁers under strict fairness. Specif-

ically, we dominate all other algorithms in regimes where unfairness is restricted to less than 2.5% with

either fairness criterion. Overall these results show that our framework is able to build interpretable models

that have competitive accuracy and substantially improved fairness. Moreover, our algorithm allows for

especially ﬁne control over unfairness. Figure 6 shows the eﬀect of relaxing the fairness constraint and its

19

Figure 5: Accuracy Fairness Frontier for Fair CG and other interpretable fair classiﬁers with respect to

equality of opportunity (left column) and equalized odds (right column).

associated aﬀect on the true positive rate of both groups for the compas dataset. We emphasise that the

allowed unfairness level during training translates directly to (practically) the same observed unfairness level

in testing, thus establishing the robustness of our approach. For the remainder of our results, and more

speciﬁcs on our experimental framework we refer you to the the appendix.

5.4 Interpretability

In our framework, the complexity of the rule set (deﬁned as the number of rules plus the number of conditions

in each rule) is used as a measure for the interpretability of the rule set. Table 5 summarizes the mean

complexity of the rule set selected for each dataset and some sample (cid:15) under the equality of opportunity

constraint.

20

0.000.010.020.030.040.050.06Unfairnesss (TPR Gap)0.760.780.800.820.84AccuracyAdultFairCGZafarHardtFair Learn (Exp. Grad)0.000.020.040.060.080.10Unfairnesss (Max(TPR, TNR) Gap)0.760.780.800.820.84AccuracyAdultFairCGZafarHardtFair Learn (Exp. Grad)0.000.010.020.030.040.05Unfairnesss (TPR Gap)0.780.790.800.810.82AccuracyDefaultFairCGZafarHardtFair Learn (Exp. Grad)0.000.010.020.030.040.05Unfairnesss (Max(TPR, TNR) Gap)0.780.790.800.810.82AccuracyDefaultFairCGZafarHardtFair Learn (Exp. Grad)0.000.020.040.060.080.10Unfairnesss (TPR Gap)0.540.560.580.600.620.640.66AccuracyCompasFairCGZafarHardtFair Learn0.000.020.040.060.080.10Unfairnesss (Max(TPR, TNR) Gap)0.540.560.580.600.620.640.66AccuracyCompasFairCGZafarHardtFair LearnFigure 6: Eﬀect of relaxing the equality of opportunity fairness constraint on both train and test set true

positive rate for each group.

Table 5: Average complexity of rule sets

(cid:15)

Adult Compas Default

0.01

59.9

11.2

11.4

0.1

60.1

10.9

0.5

50.5

9.2

9.3

9.3

To give a sense of the interpretability of FairCG classiﬁers, we generated sample rule sets to predict

criminal recidivism on the compas dataset with and without fairness constraints. We trained each rule set

on one train/test split of the data set, and report the rule set as well as its out of sample accuracy and

fairness. A sample rule set without any fairness constraints is:

Predict repeat oﬀence if:

(cid:2)(Score Factor=True) and (Misdemeanor=False) (cid:3)

OR
(cid:2)(Race(cid:54)=Black) and (Score Factor=True) and (Misdemeanor=True)
and (Age<45) and (Gender=’Male’)(cid:3)

OR
(cid:2)(Race=Black) and (Score Factor=True) and (Priors≥ 10)
and (Age<45) and (Gender=’Male’)(cid:3)

This rule set has a test set accuracy of 67.2%, but a 20% gap in the false negative rate, and 22% gap in

the false positive rate between the two groups respectively (i.e. 22% unfair with respect to equalized odds).

21

0.00.20.40.60.81.0TrainingEqualityofOpportunityFairnessConstraint((cid:15))0.400.450.500.550.60ObservedTruePositiveRate(TPR)CompasTrainG1TestG1TrainG2TestG2Adding an equalized odds constraint of 0.05, is:

Predict repeat oﬀence if:

(cid:2)(Race(cid:54)=Black) and (Score Factor=True) and (Age < 45)(cid:3)

OR
(cid:2)(Race=Black) and (Score Factor=True) and (Misdemeanor=True)
and (Age<45) and (Gender=’Male’)(cid:3)

The fair rule set is less accurate, it has a test set accuracy of 65.1%, but a 1% gap in the false negative

rate and 3.4% gap in the false positive rate between the two groups respectively. Both rule sets are arguably

quite interpretable - they have a small number of clauses, each with a small number of conditions. The fair

rule set is also interesting in that it highlights the the bias of compas’s recidivism tool (score factor indicates

whether or not the compas tool predict the oﬀender a high risk of recidivism). In our optimal rule set, the

compas tool seems to be a good predictor of recidivism for white oﬀenders, but for Black oﬀenders our rule

set adds a number of additional criteria including requiring the oﬀender to be young, male, and have a high

number of priors. One lackluster aspect of our rule set, is the fact that it never predicts a repeat oﬀence if

the oﬀender if a Black woman. This is a consequence of the relatively low rate of Black woman re-oﬀenders

in the data set (3 percent of the overall data). In fact, the error rate for Black women (30 percent) is below

the average error rate in the test data (37 percent). However, the false negative error rate is 100 percent and

thus could present fairness violations under equalized odds. To account for this, our formulation can easily

be extended to include both race and gender as sensitive attributes (i.e. one group for Black men, Black

women, white men, and white women respectively) by adding additional fairness constraints to bound the

discrepancy between each set of groups.

6 Conclusion

In this paper we introduced an IP formulation for building fair Boolean rule sets under both equality of

opportunity and equalized odds. Experimental results on classic fair machine learning datasets validated

that our algorithm is competitive with the state of the art - dominating popular fair classiﬁers on 2 of 3

datasets, and remaining unbeaten in regimes of strict fairness. Overall, our algorithm Fair CG provides

a powerful tool for practitioners that need simple, fair, and interpretable models for machine learning in

socially sensitive settings.

22

References

[1] FICO Explainable Machine Learning Challenge. https://community.fico.com/community/xml. Last

accessed 2018-05-16.

[2] Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reductions

approach to fair classiﬁcation. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th

International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,

pages 60–69. PMLR, 10–15 Jul 2018.

[3] Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees

for non-discriminative decision-making. Proceedings of the AAAI Conference on Artiﬁcial Intelligence,

33(01):1418–1426, Jul. 2019.

[4] Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin. Learning

certiﬁably optimal rule lists. In Proceedings of the 23rd ACM SIGKDD International Conference on

Knowledge Discovery and Data Mining (KDD), pages 35–44, 2017.

[5] Julia Angwin, Jeﬀ Larson, Surya Mattu, and Laura Kirchner. Machine bias: There’s software used

across the country to predict future criminals. and it’s biased against blacks. Available at https://www.

propublica.org/article/machine-bias-risk-assessments-in-\criminal-sentencing (2016).

[6] Cynthia Barnhart, Ellis L. Johnson, George L. Nemhauser, Martin W. P. Savelsbergh, and Pamela H.

Vance. Branch-and-price: Column generation for solving huge integer programs. Operations Research,

46(3):316–329, 1998.

[7] Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern,

Seth Neel, and Aaron Roth. A convex framework for fair regression. In Proceedings of the Fairness,

Accountability, and Transparency in Machine Learning conference, 2017.

[8] Dimitris Bertsimas and Jack Dunn. Optimal classiﬁcation trees. Mach. Learn., 106(7):1039–1082, July

2017.

[9] S Ilker Birbil, Mert Edali, and Birol Yuceoglu. Rule covering for interpretation and boosting. arXiv

preprint arXiv:2007.06379, 2020.

[10] Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. Classiﬁcation and Re-

gression Trees. Chapman & Hall/CRC, 1984.

[11] Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classiﬁcation.

Data Min. Knowl. Discov., 21:277–292, 09 2010.

23

[12] Emilio Carrizosa, Cristina Molero-R´ıo, and Dolores Romero Morales. Mathematical optimization in

classiﬁcation and regression trees. Top, 29(1):5–33, 2021.

[13] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction

instruments. Big Data, 5(2):153–163, Jun 2017.

[14] Michele Conforti, G´erard Cornu´ejols, Giacomo Zambelli, et al.

Integer programming, volume 271.

Springer, 2014.

[15] Sam Corbett-Davies and Sharad Goel. The measure and mismeasure of fairness: A critical review of

fair machine learning. arXiv preprint arXiv:1808.00023, 2018.

[16] Sanjeeb Dash, Oktay Gunluk, and Dennis Wei. Boolean decision rules via column generation.

In

S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances

in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.

[17] Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Empiri-

cal risk minimization under fairness constraints. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,

N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-

ume 31. Curran Associates, Inc., 2018.

[18] Dheeru Dua and Casey Graﬀ. UCI machine learning repository, 2017.

[19] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through

awareness. In ITCS ’12, pages 214–226. ACM.

[20] Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint

arXiv:1511.05897, 2015.

[21] P. C. Gilmore and R. E. Gomory. A linear programming approach to the cutting-stock problem.

Operations Research, 9(6):849–859, 1961.

[22] LLC Gurobi Optimization. Gurobi optimizer reference manual, 2020.

[23] Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In

D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information

Processing Systems, volume 29. Curran Associates, Inc., 2016.

[24] Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learning.

In Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM ’10, page 869–874,

USA, 2010. IEEE Computer Society.

[25] Faisal Kamiran, Indr˙e ˇZliobait˙e, and Toon Calders. Quantifying explainable discrimination and re-

moving illegal discrimination in automated decision making. Knowledge and Information Systems, 1:in

press, 06 2012.

24

[26] Jon M. Kleinberg, Sendhil Mullainathan, and Manish Raghavan. Inherent trade-oﬀs in the fair deter-

mination of risk scores. In Christos H. Papadimitriou, editor, 8th Innovations in Theoretical Computer

Science Conference, ITCS 2017, January 9-11, 2017, Berkeley, CA, USA, volume 67 of LIPIcs, pages

43:1–43:23. Schloss Dagstuhl - Leibniz-Zentrum f¨ur Informatik, 2017.

[27] Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. Interpretable decision sets: A joint frame-

work for description and prediction. In Proceedings of the 22nd ACM SIGKDD International Conference

on Knowledge Discovery and Data Mining, KDD ’16, page 1675–1684, New York, NY, USA, 2016. As-

sociation for Computing Machinery.

[28] Himabindu Lakkaraju, Stephen H. Bach, and Jure Leskovec. Interpretable decision sets: A joint frame-

work for description and prediction.

In Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Mining

(KDD), pages 1675–1684, 2016.

[29] Himabindu Lakkaraju and Cynthia Rudin. Learning cost-eﬀective and interpretable treatment regimes.

In Proceedings of the 20th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS),

volume 54, pages 166–175, Fort Lauderdale, FL, USA, 20–22 Apr 2017.

[30] A. H. Land and A. G. Doig. An automatic method for solving discrete programming problems. ECONO-

METRICA, 28(3):497–520, 1960.

[31] Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan.

Interpretable classi-

ﬁers using rules and Bayesian analysis: Building a better stroke prediction model. Ann. Appl. Stat.,

September(3):1350–1371, 09 2015.

[32] Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. Too relaxed to be fair. In Hal Daum´e III and

Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume

119 of Proceedings of Machine Learning Research, pages 6360–6369. PMLR, 13–18 Jul 2020.

[33] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey

on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.

[34] Matt Menickelly, Oktay G¨unl¨uk, Jayant Kalagnanam, and Katya Scheinberg. Optimal generalized

decision trees via integer programming. CoRR, abs/1612.03225, 2016.

[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,

R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-

esnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830,

2011.

[36] J. Ross Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers Inc., San

Francisco, CA, USA, 1993.

25

[37] Ronald L. Rivest. Learning decision lists. Machine Learning, 2(3):229–246, 1987.

[38] Guolong Su, Dennis Wei, Kush R. Varshney, and Dmitry M. Malioutov. Learning sparse two-level

Boolean rules. In Proc. IEEE Int. Workshop Mach. Learn. Signal Process. (MLSP), pages 1–6, Septem-

ber 2016.

[39] Berk Ustun and Cynthia Rudin. Learning optimized risk scores. Journal of Machine Learning Research,

20(150):1–75, 2019.

[40] Berk Ustun, Stefano Traca, and Cynthia Rudin. Supersparse linear integer models for interpretable

classiﬁcation. arXiv preprint arXiv:1306.6677, 2013.

[41] Fulton Wang and Cynthia Rudin. Falling rule lists. In Proc. Int. Conf. Artif. Intell. Stat. (AISTATS),

pages 1013–1022, 2015.

[42] Tong Wang and Cynthia Rudin. Learning Optimized Or’s of And’s, November 2015. arXiv:1511.02210.

[43] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampﬂ, and Perry MacNeille. A

Bayesian framework for learning rule sets for interpretable classiﬁcation. Journal of Machine Learning

Research, 18(70):1–37, 2017.

[44] Yongkai Wu, Lu Zhang, and Xintao Wu. Fairness-aware classiﬁcation: Criterion, convexity, and bounds.

arXiv preprint arXiv:1809.04737, 2018.

[45] Hongyu Yang, Cynthia Rudin, and Margo Seltzer. Scalable Bayesian rule lists.

In Proc. Int. Conf.

Mach. Learn. (ICML), pages 1013–1022, 2017.

[46] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P. Gummadi. Fairness

beyond disparate treatment & disparate impact. Proceedings of the 26th International Conference on

World Wide Web - WWW ’17, 2017.

[47] Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P. Gummadi. Fairness

Constraints: Mechanisms for Fair Classiﬁcation. In Aarti Singh and Jerry Zhu, editors, Proceedings of

the 20th International Conference on Artiﬁcial Intelligence and Statistics, volume 54 of Proceedings of

Machine Learning Research, pages 962–970, Fort Lauderdale, FL, USA, 20–22 Apr 2017. PMLR.

[48] Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.

In Sanjoy Dasgupta and David McAllester, editors, Proceedings of the 30th International Conference on

Machine Learning, pages 325–333, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR.

26

A Datasets and data processing

In our experiments, we use three common fair machine learning datasets: adult, compas and default. Both

adult1 and default2 can be found on the UCI machine learning dataset repository [18]. Both datasets were

unchanged from the data available at the referenced links. For the adult dataset we just use the training

data provided, and for default we use the the entire dataset provided. For the compas data from ProPublica

[5] we use the fair machine learning cleaned dataset3. Following the methodology of [47] we also restrict the

data to only look at African American and Caucasian respondents - ﬁltering all datapoints that belong to

other races and creating a new binary column which indicates whether or not the respondent was African

American. A summary of our data and the sensitive attributes we use for group identiﬁcation is included

below.

Table 6: Overview of datasets

Dataset Examples Features

Sensitive Variable

Adult

32,561

Compas

5,278

Default

30,000

14

7

23

Gender

Race

Gender (X2 column)

To convert the data to be binary-valued, we use a standard methodology also used in [16]. For categorical

variables j we use one-hot encoding to binarize each variable into multiple indicator variables that check

Xj = x, and the negation Xj (cid:54)= x. For numerical variables we compare values against a sequence of

thresholds for that column and include both the comparison and it’s negation (i.e. Xj ≤ 1, Xj ≤ 2 and

Xj > 1, Xj > 2). For our experiments we use the sample deciles as the thresholds for each column. We use

the binarized data for all the algorithms we test to control for the binarization method.

B Computing infrastructure

Our experiments were run on a windows desktop computer with an Intel core i7-9000 3 GHz processor and

32 GB of RAM. Our python environment was conﬁgured with anaconda and included the following package

dependencies:

1https://archive.ics.uci.edu/ml/datasets/adult
2https://archive.ics.uci.edu/ml/datasets/default
3https://www.kaggle.com/danofer/compass

27

Table 7: Overview of package dependencies

Package Version

Python

Gurobi

3.7.6

9.0.1

Numpy

1.18.1

Pandas

1.0.0

C Hamming loss

As part our experiments we tested the impact of optimizing for Hamming Loss instead of 0-1 loss directly. To

adapt our IP formulation to optimize for accuracy we have to both change the objective function to include

all errors, and add additional constraints to track the number of misclassiﬁed data points in the negative

class. The full IP formulation is:

min

s.t.

(cid:88)

i∈P

ζi +

(cid:88)

i∈N

ζi

ζi +

(cid:88)

k∈Ki
(cid:88)

k∈Ki

wk ≥ 1,

ζi ≥ 0, i ∈ P

wk ≤

C
2

ζi, i ∈ N ,

(cid:88)

ckwk ≤ C

k∈K
(cid:88)

i∈P1
(cid:88)

i∈P2

1
|P1|

1
|P2|

ζi−

ζi−

1
|P2|

1
|P1|

(cid:88)

i∈P2
(cid:88)

i∈P1

ζi ≤ (cid:15)1

ζi ≤ (cid:15)1

wk ∈ {0, 1},

k ∈ K

(25)

(26)

(27)

(28)

(29)

(30)

(31)

where constraint (27) is the new constraint to track misclassiﬁcations for the negative class. We upper

bound (cid:80)

k∈Ki wk with C/2, as the minimum complexity of a rule is 2 and thus the complexity constraint
(28) implies the upper bound on the number of rules is C/2. limit on the number of rules is C/2. Instead of

using disaggregated constraints (i.e. wk ≤ ζi), we use the aggregated form as modern solvers will handle it

more eﬃciently [14]. Constraints (29) and (30) bound the equality of opportunity gap. Note that we focus

on the equality of opportunity fairness metric, as the equalized odds constraints are speciﬁcally tailored for

the Hamming Loss model.

In our experiments comparing the Hamming loss and 0-1 loss formulations, we used a ﬁxed set of rules

generated from our column generation procedure (on an average of 6781, 1007, and 7584 rules per fold for the

28

adult, compas, and default datasets respectively). The ﬁxed rule sets were mined during our experiments on

FairCG (i.e. the ﬁrst stage of the procedure outlined in the computational approach section of the paper).

Using these ﬁxed rule sets, we ran both master IP models with the same fairness parameter ((cid:15)) and the

following ranges of complexity bounds:

• Adult: {10, 20, 30, 40}

• Compas: {10, 15, 20, 30}

• Default: {10, 15, 20, 30}

A time limit of 20 minutes was set to solve each IP model, though no instance reached the time limit and

all IPs were solved to optimality. We used 10-fold cross validation to select the best complexity bound for

each model on each dataset. Figure 7 summarizes the relative performance of each model on each dataset

with diﬀerent (cid:15) values. We can see that on all datasets the Hamming loss model is able to achieve practically

indistinguishable performance in terms of both train and test set accuracy at a fraction of the computation

time. Speciﬁcally, while the accuracy model appears to perform slightly better with respect to train accuracy,

the trend is reversed when it comes to test accuracy. This suggests Hamming loss may lead to rule sets with

better generalization. This is especially pronounced on larger datasets, such as Adult, where the accuracy

model takes nearly 100x the computation time. Table 13 summarizes the same results numerically. Jupyter

notebook with the implementation of this experiment can be found in our code base.

D Rule mining

We compare the rules generated from our column generation procedure to rules mined from other tree-based

models. For each tree-based model we train the classiﬁer using the split binarized data and use a range

of hyperparameters (outlined in Table 8) and extract every decision rule present. To extract the rules we

simply look at the decision path used for each leaf node which predicts a positive response and translate

that to which binarized features would be included in the rule. An implementation of the function used to

do the rule extraction is included in our code base.

We leverage three diﬀerent tree-based models: Decision Trees, Random Forests, and Fair Decision Trees

from fairlearn [2]. For the decision tree and random forest models we use the scikit-learn implementations

[35]. For decision trees we vary the depth of tree allowed, and for random forests we adjust both the tree

depth and the number of trees included in the forest. For the fair learn models we use the exponentiated

gradient approach and use decision trees as the base learner. We vary both the max depth of the trees used

and how strict the fairness criteria is (i.e. (cid:15)). The average size of the rule set generated for each dataset and

fold is outlined in Table 9.

To test the eﬃcacy of each rule set we run the master IP model with each rule set, and do 10-fold cross

validation to select the best rule set complexity for each model. We repeat the tests for three diﬀerent ﬁxed

29

Table 8: Overview of Hyperparameters Used for Rule Mining

Method

Hyperparameter

Values

Decision Tree

Max Depth

{1,3,5,..30}

Random Forest

Max Depth

{1,3,5,..30}

Number of Trees

{1,2,3,..10}

Fair Decision Tree

Max Depth

{1,3,5,..30}

Fairness (cid:15)

{0.01,0.03,...0.5}

Table 9: Average Rule set Size

Method

Adult Compas Default

Decision Tree

2276

317

2558

Random Forest

15083

3309

24680

Fair DT

4321

1034

4958

FairCG

6781

1071

7584

fairness criteria ((cid:15)). Tables 14, 15 and 16 summarize the results numerically. It is important to emphasize

that the performance reported for each rule set is not the performance of that model on the datasets (ex. the

accuracy of running a decision tree model), but rather the performance of the mined rule set from that model

when optimized within our IP framework (ex. the accuracy of the optimal rule set using rules mined from

a decision tree). Across both metrics and all three datasets we see that the rule set generated by column

generation performs better than those from other heuristic tree sources - outperforming them with respect

to Hamming loss, train set accuracy and test set accuracy.

E Fairness-accuracy trade-oﬀs

For our experiments we took a two-phase approach. During the ﬁrst rule generation phase, we ran our

column generation algorithm with a set of diﬀerent hyperparameters to generate a set of potential rules. To

warm start this procedure, we also start the column generation process with a set of rules mined from a

random forest classiﬁer (as discussed in section D). We then solve the master IP with the set of candidate

rules and a larger set of hyperparameters to generate the curves included in the body of the report. Tables

10 and 11 summarize the hyperparameters used for both Phase I and II. Note that for the equalized odds

formulation, we set (cid:15)1 = (cid:15)2 and use the values in Table 10.

We tested our algorithm against three other popular interpretable fair classiﬁers: Zafar 2017 [46], Hardt

30

Table 10: Overview of (cid:15) Hyperparameters Tested

Dataset

Phase 1

Phase 2

Adult

{0.01, 0.1, 1}

{0, 0.01, 0.05, 0.1, 0.15, 0.2, 1}

Compas

{0.01, 0.1, 1}

{0, 0.01, 0.05, 0.1, 0.15, 0.2, 1}

Default

{0.01, 0.1, 1}

{0, 0.01, 0.03, 0.05, 0.1, 0.2, 1}

Table 11: Overview of C Hyperparameters Tested

Dataset

Phase 1

Phase 2

Adult

{5, 20, 40}

{5, 15, 20, 30, 40}

Compas

{5, 15, 30}

{5, 10, 15, 20, 30}

Default

{5, 15, 30}

{5, 10, 15, 20, 30}

2016 [23], and Fair Decision trees trained using the exponentiated gradient algorithm in fairlearn [2]. For the

Zafar algorithm we used the optimizer parameters speciﬁed in Table 12, and tested a range of 30 diﬀerent (cid:15)

values (linearly spaced between 0 and 0.5) for the covariance threshold. For Hardt 2016 we used the logistic

regression implementation from scikit-learn [35] and tested 100 diﬀerent decision thresholds for each sub-

group (1% increments). Finally, for the exponentated gradient algorthm from fairlearn we used scikit-learn’s

decision tree as the base estimator and tested both a range of maximum depth hyperparameters (20 values

linearly spaced between 1 and 30), and 30 diﬀerent (cid:15) values (linearly spaced between 0 and 0.5) for the fairness

constraints. For all the algorithms we used 10-fold cross-validation to select the best hyperparameters for

every level of fairness and removed dominated points from the ﬁgures present in the results section of the

main paper.

Table 12: Overview of Zafar Optimizer Hyperparameters

Dataset

Adult

τ

5

Compas

20

µ

1.2

1.2

Default

0.5

1.2

31

Figure 7: Relative performance of models trained to maximize accuracy or minimize Hamming loss respec-

tively under equality of opportunity constraint ((cid:15) = 0.025). The violin plots show the distribution of train

and test set accuracy for models under each objective for 100 random splits of the datasets.

32

TrainTestAdult0.790.800.810.820.830.84AccuracyTrainTestCompas0.580.600.620.640.660.680.70Accuracy(cid:15)=0.025TrainTestDefault0.740.750.760.770.780.79AccuracyHammingLoss0-1LossTrainTestAdult0.790.800.810.820.830.84AccuracyTrainTestCompas0.580.600.620.640.660.680.70Accuracy(cid:15)=0.1TrainTestDefault0.740.750.760.770.780.79AccuracyHammingLoss0-1LossTrainTestAdult0.790.800.810.820.830.84AccuracyTrainTestCompas0.580.600.620.640.660.680.70Accuracy(cid:15)=0.5TrainTestDefault0.740.750.760.770.780.79AccuracyHammingLoss0-1LossTable 13: Summary of performance diﬀerence when optimizing for Hamming loss vs. accuracy (standard
deviation in parenthesis)

Dataset

(cid:15)

Objective Train Acc. Test Acc. Comp. Time (s)

0.025

Adult

0.1

0.5

0.025

Compas

0.1

0.5

0.025

Default

0.1

0.5

Accuracy
Hamming
Accuracy
Hamming
Accuracy
Hamming

Accuracy
Hamming
Accuracy
Hamming
Accuracy
Hamming

Accuracy
Hamming
Accuracy
Hamming
Accuracy
Hamming

81.24 (1.1)
81.21 (0.2)
81.8 (0.8)
81.7 (0.2)
81.8 (0.7)
81.8 (0.2)

65.4 (0.3)
65.2 (0.3)
66.3 (0.2)
66.1 (0.2)
67.1 (0.3)
66.9 (2.5)

77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)

80.8 (1.3)
81.4 (0.5)
81.2 (1.1)
81.6 (0.7)
81.3 (0.1)
81.7 (0.7)

63.6 (3.4)
64.7 (2.2)
65.1 (2.7)
65.5 (2.6)
65.9 (2.9)
66.0 (2.3)

77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)
77.9 (0)

573 (73)
31 (5)
565 (63)
30 (6)
568 (70)
30 (4)

24 (3)
6 (3)
21 (4)
4 (2)
6 (1)
2 (0.3)

511 (105)
3 (0.2)
475 (117)
4 (0.6)
519 (119)
3 (0.3)

Table 14: Summary of rule set performance on adult dataset (standard deviation in parenthesis)

(cid:15)

Rule Set

Train Acc. Test Acc. Hamming loss Train Acc. Test Acc. Hamming loss

Equality of Opportunity

Equalized Odds

0.025

0.1

0.5

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

77.1 (1.5)
76.5 (0.6)
76.5 (0.8)
77.1 (1.5)
80.7 (1.2)
80.7 (1.2)

77.6 (1.4)
77.9 (1.4)
76.8 (0.8)
77.9 (1.4)
81.8 (0.2)
81.9 (0.4)

77.6 (1.4)
79.8 (1.1)
77.0 (1.0)
79.8 (1.1)
81.8 (0.2)
81.9 (0.3)

77.0 (1.5)
76.4 (0.4)
76.5 (1.7)
77.0 (1.5)
80.5 (1.1)
80.5 (1.1)

77.3 (1.3)
77.8 (1.6)
76.5 (1.0)
77.8 (1.6)
81.6 (0.7)
81.3 (0.5)

77.3 (1.3)
79.7 (1.5)
76.9 (1.5)
79.7 (1.5)
81.6 (0.7)
81.3 (0.9)

6703 (434)
6883 (188)
6885 (243)
6703 (434)
5693 (358)
5693 (358)

6558 (402)
6458 (412)
6780 (385)
6458 (412)
5340 (67)
5338 (80)

6561 (399)
5926 (326)
6743 (305)
5926 (326)
5339 (67)
5335 (62)

33

76.9 (1.4)
76.5 (1.3)
76.1 (0.2)
77.0 (1.3)
81.0 (1.6)
81.1 (1.4)

77.4 (1.4)
78.7 (0.2)
76.7 (0.8)
78.7 (0.2)
81.3 (1.3)
81.6 (1.0)

77.4 (1.4)
78.7 (1.8)
76.9 (1.3)
78.7 (0.2)
81.4 (1.1)
81.6 (1.0)

76.7 (1.4)
76.5 (1.4)
76.1 (1.0)
76.8 (1.4)
81.0 (1.4)
81.0 (1.4)

77.2 (1.3)
78.7 (0.8)
76.5 (1.0)
78.7 (0.8)
81.4 (1.1)
81.3 (1.1)

77.2 (1.3)
78.6 (1.1)
76.7 (1.4)
78.7 (0.8)
81.4 (1.3)
81.3 (1.1)

6781 (418)
6900 (369)
7000 (64)
6781 (418)
5567 (438)
5566 (438)

6610 (414)
6244 (68)
6839 (232)
6244 (68)
5450 (291)
5417 (259)

6614 (409)
6253 (518)
6769 (372)
6244 (68)
5422 (260)
5417 (259)

Table 15: Summary of rule set performance on compas dataset (standard deviation in parenthesis)

(cid:15)

Rule Set

Train Acc. Test Acc. Hamming loss Train Acc. Test Acc. Hamming loss

Equality of Opportunity

Equalized Odds

0.025

0.1

0.5

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

57.5 (3.7)
60.2 (0.5)
62.8 (2.2)
62.8 (2.1)
64.5 (0.7)
64.6 (0.9)

59.8 (4.3)
64.2 (1.2)
64.7 (1.3)
64.9 (1.2)
65.5 (0.6)
65.6 (0.6)

62.6 (4.6)
65.9 (0.4)
66.0 (0.7)
66.1 (0.5)
66.4 (0.5)
66.6 (0.6)

57.2 (4.4)
59.4 (0.5)
62.4 (3.2)
62.6 (2.9)
64.4 (2.0)
64.3 (2.2)

59.9 (4.6)
63.5 (3.9)
64.5 (2.7)
64.6 (2.9)
65.2 (2.4)
652 (2.1)

62.5 (4.9)
65.8 (3.3)
65.9 (2.9)
66.0 (3.9)
66.0 (2.4)
66.3 (2.7)

2020 (175)
1891 (223)
1782 (116)
1750 (115)
1690 (34)
1679 (44)

1911 (202)
1701 (58)
1680 (63)
1649 (80)
1638 (27)
1630 (34)

1777 (220)
1620 (20)
1615 (31)
1601 (42)
1596 (21)
1586 (28)

60.3 (2.8)
62.1 (1.7)
64.0 (1.1)
64.0 (1.1)
65.0 (0.3)
65.3 (0.2)

62.5 (1.7)
62.6 (2.1)
65.5 (0.2)
65.5 (0.2)
65.9 (0.4)
66.1 (0.2)

66.6 (0.3)
64.9 (1.6)
66.5 (0.3)
66.5 (0.3)
66.8 (0.4)
66.9 (0.3)

60.2 (4.1)
62.5 (3.0)
63.7 (2.9)
63.7 (2.9)
64.6 (2.3)
64.7 (2.5)

62.3 (3.4)
62.6 (2.5)
65.5 (2.6)
65.5 (2.6)
65.2 (2.7)
65.3 (2.6)

66.0 (2.5)
64.1 (2.8)
65.8 (3.1)
65.8 (3.1)
66.0 (2.5)
66.1 (2.3)

1884 (131)
1802 (82)
1710 (51)
1710 (51)
1660 (14)
1650 (12)

1782 (83)
1778 (100)
1643 (15)
1643 (15)
1626 (20)
1612 (11)

1586 (15)
1666 (76)
1590 (15)
1590 (15)
1578 (22)
1571 (14)

Table 16: Summary of rule set performance on default dataset (standard deviation in parenthesis)

(cid:15)

Rule Set

Train Acc. Test Acc. Hamming loss Train Acc. Test Acc. Hamming loss

Equality of Opportunity

Equalized Odds

0.025

0.1

0.5

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

Decision Tree
Fair Decision Tree
Random Forest
All but CG
Fair CG
All

77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

5963 (17)
5963 (17)
5963 (17)
5963 (17)
5601 (30)
5501 (32)

5963 (17)
5963 (17)
5963 (17)
5963 (17)
5601 (30)
5501 (32)

5963 (17)
5963 (17)
5963 (17)
5963 (17)
5601 (30)
5501 (32)

34

77.9 (0.1)
77.9 (0.1)
77.9 (0.1)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

77.9 (0.1)
77.9 (0.1)
77.9 (0.1)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

77.9 (0.1)
77.9 (0.1)
77.9 (0.1)
77.9 (0.6)
81.4 (0.2)
82.6 (0.3)

78.2 (0.6)
78.2 (0.6)
78.2 (0.6)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

78.2 (0.6)
78.2 (0.6)
78.2 (0.6)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

78.2 (0.6)
78.2 (0.6)
78.2 (0.6)
78.1 (0.5)
80.9 (1.3)
82.0 (0.7)

5965 (18)
5965 (18)
5965 (18)
5963 (17)
5601 (30)
5501 (32)

5965 (18)
5965 (18)
5965 (18)
5963 (17)
5601 (30)
5501 (32)

5965 (18)
5965 (18)
5965 (18)
5963 (17)
5601 (30)
5501 (32)

