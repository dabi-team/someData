0
2
0
2

p
e
S
3

]

G
L
.
s
c
[

1
v
6
7
4
1
0
.
9
0
0
2
:
v
i
X
r
a

Optimality-based Analysis of XCSF Compaction
in Discrete Reinforcement Learning(cid:63)

Jordan T. Bishop and Marcus Gallagher

School of Information Technology and Electrical Engineering, The University of
Queensland, Brisbane, Queensland, 4072, Australia
{j.bishop,marcusg}@uq.edu.au

Abstract. Learning classiﬁer systems (LCSs) are population-based pre-
dictive systems that were originally envisioned as agents to act in rein-
forcement learning (RL) environments. These systems can suﬀer from
population bloat and so are amenable to compaction techniques that try
to strike a balance between population size and performance. A well-
studied LCS architecture is XCSF, which in the RL setting acts as a Q-
function approximator. We apply XCSF to a deterministic and stochastic
variant of the FrozenLake8x8 environment from OpenAI Gym, with its
performance compared in terms of function approximation error and pol-
icy accuracy to the optimal Q-functions and policies produced by solving
the environments via dynamic programming. We then introduce a novel
compaction algorithm (Greedy Niche Mass Compaction — GNMC) and
study its operation on XCSF’s trained populations. Results show that
given a suitable parametrisation, GNMC preserves or even slightly im-
proves function approximation error while yielding a signiﬁcant reduction
in population size. Reasonable preservation of policy accuracy also oc-
curs, and we link this metric to the commonly used steps-to-goal metric
in maze-like environments, illustrating how the metrics are complemen-
tary rather than competitive.

Keywords: Reinforcement learning · Learning classiﬁer system · XSCF
· Compaction

1

Introduction

Reinforcement learning (RL) is characterised by an agent learning a behavioural
policy in an environment by means of maximising a reward signal. Learning
Classiﬁer Systems (LCSs) are a paradigm of cognitive systems that originated
via representing agents in this framework, although due to ﬂexibility in imple-
mentation have also been widely adapted to other kinds of machine learning
(ML) tasks such as classiﬁcation and clustering [16]. The most widely-studied
LCS architecture to date, Wilson’s XCS [18], is at its heart a reinforcement
learner. More recently, an extension of XCS to allow for function approxima-
tion, dubbed XCSF [19], has been successfully used in the RL setting for value
function approximation [12,13].

(cid:63) The ﬁnal authenticated publication is available online at https://doi.org/10.1007/

978-3-030-58115-2 33

 
 
 
 
 
 
2

J.T. Bishop, M. Gallagher

LCSs utilise a combination of evolutionary computation and ML techniques
to create population-based solutions to prediction problems. The most common
style of LCSs are Michigan-style LCSs, where each individual (classiﬁer) in the
population represents a partial solution, and classiﬁers co-operate in a potentially
overlapping piecewise ensemble to deﬁne an overall solution [16]. A general issue
with Michigan-style LCSs is that of population bloat and/or redundancy. Since
these systems learn in an online fashion and regularly reﬁne their population via
a genetic algorithm (GA), after learning is complete there are often members
of the population that have not had time to properly adapt to the environment
and form accurate predictions.

A common way to deal with this issue is to perform a post-processing com-
paction procedure after the system is trained in order to remove low-quality
classiﬁers from the population [16]. Compaction seeks to shrink the population
size as much as possible while simultaneously minimising degradation of predic-
tive performance. This is often done as part of an analysis pipeline where the
system is being used to “mine” knowledge from the problem via interpretation of
the compacted population [17]. Wilson originally described a compaction algo-
rithm for a variant of XCS trained on a classiﬁcation problem in [20], and other
algorithms such as those detailed in [8,7,15] extended this line of work. These
algorithms all incorporate some kind of greedy heuristic to preferentially retain
some classiﬁers over others, and mainly use metrics related to classiﬁcation per-
formance. Since compaction is related to knowledge discovery, other works have
focused more on this latter task [4,9,17]. What all these works have in common
is that they study compaction in the context of supervised learning.

In this work we apply XCSF to discrete maze-like RL environments and per-
form compaction on the trained populations. We are interested in measuring the
performance of XCSF with respect to the optimal solutions to the environments,
and investigating how performance is impacted when performing compaction. As
part of our analysis we introduce a novel compaction algorithm called Greedy
Niche Mass Compaction (GNMC) as a generalisation of previous work. We also
attempt to connect our optimality metrics to the steps-to-goal metric used by
other work applying LCSs to maze-like environments.

2 Background

2.1 Reinforcement Learning

RL environments can be modelled as a Markov Decision Process (MDP), deﬁned
by components (S, A, T, R, γ) where S is the state space, A is the action space,
T is the transition function, R is the reward function and γ ∈ [0, 1] is the reward
discount factor [14]. We consider the case where the agent interacting with the
environment seeks to learn a deterministic behavioural policy π : S → A. From
the agent’s perspective, T and R are unknown and so learning becomes an act
of balancing exploration with exploitation to sample from T and R in order to
construct π. If the full deﬁnition of the MDP is known, dynamic programming

Optimality-based Analysis of XCSF Compaction in Discrete RL

3

methods such as value iteration can be used to exhaustively obtain an optimal
solution to the problem.

Value iteration yields an optimal Q-function Q∗ : S × A → R, which maps
each state-action pair (s, a) ∈ S × A to a real number representing the utility
of the pair: the expected amount of cumulative discounted reward that can be
obtained from performing action a in state s, and acting optimally thereafter. An
optimal policy π∗ can then be constructed by acting greedily with respect to Q∗.
One of the main approaches to RL is to have the agent build an approximation ˆQ
to Q∗ from its environmental experience using e.g. temporal diﬀerence learning
techniques such as Q-learning [14]. The agent’s approximation ˆπ to π∗ can then
be constructed by acting greedily with respect to ˆQ.

2.2 XCSF

XCSF is an LCS architecture designed to perform function approximation. It
diﬀers from XCS in that classiﬁers compute their predictions as a function of
their inputs, instead of predicting a scalar value. The system operates by adap-
tively partitioning the input space into subspaces with classiﬁers (evolutionary
component), in tandem with forming approximations to the target function in
the subspaces (ML component) [3].

Classiﬁers take the form of IF condition T HEN action rules. Partitioning
is accomplished by specifying the rule representation to be used by conditions.
Common choices include hyperrectangles [19,12] and hyperellipsoids [5]. In the
simplest case, linear functions can be used as a prediction scheme but extensions
to the non-linear case have been investigated [11]. Additionally, classiﬁers have a
number of parameters, denoted as cl.param, that store or calculate information
related to them; the following parameters being important in this work: ﬁtness
— the predictive accuracy of a classiﬁer relative to other classiﬁers in the action
set(s) (deﬁned below) that it participates in, numerosity — the number of copies
of a classiﬁer present in the population (necessary because the GA may produce
classiﬁers with duplicate rules), generality — a quantity in the range (0, 1] rep-
resenting the fraction of the input space covered by the classiﬁer’s condition.
Numerosity yields the concept of macroclassiﬁers and microclassiﬁers, deﬁned
as the classiﬁers in the population with unique rule structures (possibly having
numerosity > 1) and individual copies of these unique classiﬁers, respectively.

Applied as a RL method, XCSF uses a Q-learning style reinforcement com-
ponent to form classiﬁer predictions. The overall system output ˆQ is computed
for each (s, a) ∈ S × A according to:

ˆQ(s, a) =

(cid:80)

cl∈[A] cl.prediction(s) · cl.f itness
cl∈[A] cl.f itness

(cid:80)

(1)

[A] is termed an action set and contains classiﬁers in the population whose
conditions match s and who advocate action a, i.e. XCSF’s current knowledge
about a particular niche of the environment.

4

J.T. Bishop, M. Gallagher

3 Environments

We consider two variations of the FrozenLake environment with grid size 8
(FrozenLake8x8) from OpenAI Gym1. FrozenLake is an episodic, fully observ-
able grid navigation environment. In this environment, the agent must navigate
across frozen cells to reach a goal, without falling into any holes. If the agent
falls into a hole the episode terminates. The state representation used is an (x, y)
co-ordinate representing the location of the agent in the grid, as shown in Fig-
ure 1a. We use S to indicate the set of non-terminal states (frozen cells), and
ST to represent the set of terminal states (holes and the goal). The action space
A = {Lef t, Down, Right, U p}, constant over all s ∈ S.

A parameter pslip controls the level of stochasticity in the environmental tran-
sition dynamics. Figure 1b gives examples of transition dynamics with pslip =
0.1. Transition stochasticity is global over all s ∈ S. By default pslip = 2
3 ,
which is quite high. For our variants, we consider the cases where pslip = 0 and
pslip = 0.1, the latter because we wish to preserve the spirit of the default case
while making the problem substantially easier through lowering the amount of
noise incurred in transitions and therefore the reward signal. The reward func-
tion operates as follows: +1 if the agent transitions into G, 0 otherwise. We set
γ = 0.95 to ensure that there is time pressure to reach the goal. Note that a time
step is counted even if the agent does not move to a new state after performing
an action (as occurs with 90% probability in the leftmost example of Figure 1b).
Figure 2 shows the optimal policies for our two variants. In the deterministic
case reaching the goal is a shortest path problem, hence in some states there are
multiple optimal actions. In the stochastic case the optimal policy is more strict
as there is only a single optimal action in every state.

(a) Environment grid structure. Cell la-
bels: F = frozen, H = hole, G = goal.

(b) Transition dynamics with pslip = 0.1.
With probability 0.1 the agent slips to
one of the directions perpendicular to its
intended direction (mass shared equally
across both possible slip directions). As
illustrated by the leftmost example, the
agent is unable to maneuver oﬀ the grid.

Fig. 1: FrozenLake8x8 (a) structure and (b) example transition dynamics.

1 https://gym.openai.com/envs/FrozenLake8x8-v0/

0123FFFFFFFFFFFHFFFFFFFHFHHFFHFFFFFHFFFFFFFFFFFFFHFFFFFFFFHFHFHFFFFG4676540123570.90.050.050.050.050.9............Optimality-based Analysis of XCSF Compaction in Discrete RL

5

(a) pslip = 0

(b) pslip = 0.1

Fig. 2: Optimal policies for FrozenLake8x8, γ = 0.95.

4 XCSF Conﬁguration

We use our own implementation of XCSF written in Python2, faithful to the
base description of XCS given in [2]. We use the same linear prediction scheme
as in [19], where each classiﬁer has an associated weight vector and its prediction
is computed as a dot product between its weight vector and the input vector,
and classiﬁer weight vectors are updated via a normalised least mean squares
procedure with the prediction target calculated via the system’s reinforcement
component. Also incorporated is the extension to XCS from [10], termed XCSµ,
which is used to estimate uncertainty introduced by stochasticity in the environ-
ment. This involves adding a parameter µ to each classiﬁer which tracks min-
imum prediction error in the action sets the classiﬁer participates in, adjusted
by a separate learning rate β(cid:15).

The rule representation used is an interval-based representation, speciﬁcally
an integer-valued variant of min-percentage representation [6]. Interval minimum
alleles are retained but percentage-to-maximum alleles are replaced by “span-
to-maximum” alleles; interval maximums calculated as: max = min + span. The
covering and mutation operators from [6] are adopted and modiﬁed to work with
integer values, resembling those in [21]. Subsumption and calculation of condition
generality are the same as in [21]. GA selection is done via tournament selection
and uniform crossover is applied on allele sequences.

The chosen rule representation and prediction scheme yield a system that
learns linear predictions of value over rectangular regions of the input space.
This is suitable for both FrozenLake8x8 environments because it exploits the fact
that Q-values decay smoothly (due to discounting) when moving away from the
goal. In areas of the state space where there are no holes, accurate generalisation
over large areas is possible (refer to e.g. top two rows in Figure 1a) so only a
few classiﬁers are required to cover such an area. The opposite is true for areas

2 https://github.com/jtbish/piecewise, see also https://github.com/jtbish/ppsn2020

for experimental code that uses this.

01237654012357HHHHHHHHHHG460123HHHHHHHHHHG4676540123576

J.T. Bishop, M. Gallagher

near holes. Compared to other Q-function approximators used in RL (e.g. neural
networks), XCSF has the advantage of presenting its knowledge in a piecewise,
easily interpretable format that can reduced to a compact set of classiﬁers (as is
the theme of this work).

5 Training Experiments

5.1 Setup

We train our implementation of XCSF described in Section 4 on the two en-
vironments detailed in Section 3. For the ﬁrst environment (pslip = 0), the
training budget is 400,000 time steps (environmental transitions) and for the
second environment (pslip = 0.1) the budget is doubled to 800,000 time steps.
Hyperparameters for both cases are: N =5000, β=0.1, β(cid:15)=0.05, α=0.1, (cid:15)0=0.01,
ν=5, γ=0.95, θGA=50, τ =0.5, χ=1.0, υ=0.5, µ=0.05, θdel=50, δ=0.1, θsub=50,
(cid:15)I =10−3, fI =10−3, θmna=4, doGASubsumption=T rue,
doActionSetSubsumption=F alse, r0=4, m0=4, x0=10, η=0.1. Hyperparame-
ter meanings correspond to those given in [2,21,19,3,10], except for υ which is
our addition and controls the probability of an allele being crossed over dur-
ing uniform crossover. The two most critical hyperparameters are N (maximum
population size in number of microclassiﬁers) and (cid:15)0 (target absolute approxi-
mation error). We tuned their values manually, along with the training budget.
For other hyperparameters, we followed guidance from [16,2,12].

By default, the agent starts each episode in state (0, 0), which puts a heavy
emphasis on exploration to reach the goal, making learning relatively diﬃcult. To
make learning easier, we allow the agent to start a training episode in any s ∈ S,
selected uniformly at random. We can therefore safely adopt the alternating
explore-exploit action selection strategy used elsewhere in the literature, i.e. (cid:15)-
greedy with a ﬁxed value of (cid:15) = 0.5.

5.2 Metrics
Before training, we use value iteration to compute Q∗ and consequently π∗ for
each environment. XCSF’s ˆQ mean absolute error (MAE) can then be calculated
as:

|Q∗(s, a) − ˆQ(s, a)|

(2)

1
|S||A|

(cid:88)

(cid:88)

s∈S

a∈A

MAE is used because we wish to directly compare with (cid:15)0. To allow for compar-
ison between π∗ and ˆπ, policies are encoded as a series of binary action advocacy
vectors, one for each s ∈ S, whereby if policy π advocates action ai in state s, bit
i of π(s) is set to 1, 0 otherwise. For example, following Figure 2a the optimal
actions in state (0, 0) are {Down, Right}. Assuming the ordering of actions is
{Lef t, Down, Right, U p}, then the encoding π∗(cid:0)(0, 0)(cid:1) = [0, 1, 1, 0]. XCSF’s
ˆπ accuracy can then be calculated as:

1
|S|

(cid:88)

s∈S

C(cid:0)π∗(s), ˆπ(s)(cid:1)

(3)

Optimality-based Analysis of XCSF Compaction in Discrete RL

7

where C is a Boolean function that accepts two binary action advocacy vectors,
a∗ and ˆa, and determines if at least one of the actions advocated in a∗ is also
advocated in ˆa , i.e. determines if ˆa is “correct”:

C(cid:0)a∗, ˆa(cid:1) =

(cid:40)

1 count ones(cid:0)a∗ AND ˆa(cid:1) > 0
0

otherwise

(4)

Reusing the previous example, if s = (0, 0), π∗(s) = [0, 1, 1, 0] and also ˆπ(s) =
[0, 0, 1, 0] then C returns 1 because ˆπ advocates one of the optimal actions, Right.

5.3 Results

(a) pslip = 0

(b) pslip = 0.1

Fig. 3: XCSF training performance curves on FrozenLake8x8 environments. Solid
lines are the mean of 30 trials, shaded regions are one standard deviation.

Figure 3 shows XCSF training performance curves for both environments,
measured over time are ˆQ MAE and ˆπ accuracy. In the deterministic case, XCSF
converges to a small MAE that is slightly larger than the target error threshold
(cid:15)0, with ˆπ accuracy very close to the maximum value of 1. In the stochastic case,
MAE is still quite small but noticeably larger than in the deterministic case,
also with larger variance. ˆπ accuracy is signiﬁcantly lower and with much larger
variance. We now investigate this reduction of ˆπ accuracy in the stochastic case
in more detail. Figure 4 shows the frequency of optimal action predictions for
each s ∈ S over the 30 trained instances. From this we can see that XCSF is
quite often predicting the optimal action in a majority of states. However, there
are a few states that are degrading policy accuracy more than others. Table 1
shows the distributions of actions predicted in the four states with lowest optimal
action prediction frequencies, and indicates that for these states if the predicted
action is not optimal (U p or Down) it is at least sensible (Right). Thus the
situation is not as poor as it ﬁrst seems.

050100150200250300350400Training time step (thousands)0.000.050.100.150.200.250.300.350.400.45Q MAE0 = 0.010.550.600.650.700.750.800.850.900.951.00 accuracy0100200300400500600700800Training time step (thousands)0.000.050.100.150.200.250.300.350.400.45Q MAE0 = 0.010.550.600.650.700.750.800.850.900.951.00 accuracy8

J.T. Bishop, M. Gallagher

Fig. 4: XCSF optimal action pre-
diction frequency for FrozenLake8x8
pslip = 0.1, calculated over 30 in-
stances.

6 Compaction

Table 1: Distribution of action pre-
dictions for the four lowest fre-
quency states in Figure 4. Optimal
actions for each state are set in bold.

Action

L D R U

Optimal
Freq.
(0, 2) 0 0 24 6 6/30=0.2
(1, 2) 0 0 19 11 11/30=0.37
(6, 0) 0 5 25 0 5/30=0.17
(6, 1) 0 9 21 0 9/30=0.3

e
t
a
t
S

We now turn to the main consideration of this work: compaction of trained XCSF
populations. Algorithm 1 presents Greedy Niche Mass Compaction (GNMC), a
novel compaction algorithm designed for use on LCS populations applied to RL
environments with discrete state-action spaces. GNMC considers all environ-
mental action sets (niches) and greedily keeps some number of the best quality
classiﬁers in each. The notion of “best quality” is deﬁned by the parameter λ,
which is a function that assigns each classiﬁer a mass (quality weighting) in the
action set. ρ acts as a compression factor and controls the number of classiﬁers
kept in each action set; higher values result in more classiﬁers being discarded.

GNMC exhibits a number of desirable properties:

1. The exact number of classiﬁers discarded in each action set is dependent on

the distribution of classiﬁer mass; ρ is sensitive to this distribution.

2. ρ can be adjusted in a smooth manner without needing prior information

about the size of action sets.

3. It is guaranteed that no “gaps” in the predictive mapping are introduced,
due to all action sets being considered and at least a single classiﬁer being
kept in each action set (because ρ cannot equal 1).

4. Any classiﬁers that only match a state s ∈ ST (and so have zero experience
and do not contribute to overall predictions) are implicitly removed from the
population because they are never added into the toKeep set; the for loop
on line 2 operates only over S. This occurs even when ρ = 0.

Notably, simple compaction strategies such as removing all classiﬁers with expe-
rience less than some threshold do not uphold point 3 listed above. This property
is crucial for function approximation in RL where a complete mapping of the
state-action space is necessary.

GNMC can be viewed as a generalisation of previous work in the literature.
In particular, we consider the work of Tan et al. in [15], where the authors deﬁne

01234567State x co-ordinate01234567State y co-ordinate0.900.970.930.900.930.930.171.000.931.000.930.900.870.830.301.000.200.370.970.871.000.731.000.700.700.830.930.770.871.000.400.671.001.000.471.001.000.971.000.800.730.900.801.001.000.830.970.630.730.970.931.000.830.00.10.20.30.40.50.60.70.80.91.0Optimal action prediction freq. (normalised)Optimality-based Analysis of XCSF Compaction in Discrete RL

9

Algorithm 1: Greedy Niche Mass Compaction (GNMC)

Input: Classiﬁer mass function λ, mass removal factor ρ ∈ [0, 1);

1 toKeep = ∅;
2 for (s, a) ∈ S × A do
3

Create action set [A] for (s, a);
Create set [A](cid:48) by sorting [A] in descending order according to λ;
totalM ass = (cid:80)
targetM ass = (1 − ρ) · totalM ass;
currentM ass = 0;
while currentM ass < targetM ass do

cl∈[A](cid:48) λ(cl);

cl = next classiﬁer in [A](cid:48);
toKeep = toKeep ∪ {cl};
currentM ass += λ(cl);

4

5

6

7

8

9

10

11

end

12
13 end
14 Remove classiﬁers not in toKeep from the population [P ];

a compaction algorithm in the context of a classiﬁcation task, called Parameter
Driven Rule Compaction (PDRC). PDRC operates by forming a correct set [C]
for each environmental input (each training set data point) then keeping the
classiﬁer in [C] with the largest product of accuracy, numerosity, and generality.
All other classiﬁers in [C] are discarded. Translating between classiﬁcation and
RL, [C] is analogous to [A] and classiﬁer accuracy is analogous to ﬁtness because
Tan et al. employ a UCS (sUpervised Classiﬁer System [1]) variant where accu-
racy is equivalent to ﬁtness. GNMC is therefore equivalent to PDRC when the
mass function λ(cl) = cl.f itness × cl.numerosity × cl.generality and the mass
removal factor ρ is suﬃciently high so as to keep only a single classiﬁer from
each action set.

We apply GNMC to our trained XCSF populations, considering three dif-
ferent mass functions, named with subscripts. The ﬁrst is λf it = cl.f itness,
motivated by the manner in which XCSF calculates its overall predictions: see
Eqn. 1. Classiﬁers with higher ﬁtness have more weight in the overall prediction,
so using ﬁtness as a mass function makes sense. The second mass function is
λtan = cl.f itness × cl.numerosity × cl.generality. The ﬁnal mass function is
an antagonistic variant of the ﬁrst mass function that is designed to see what
1
happens when GNMC is operating with “bad information”: λinv f it =
cl.f itness .
Figure 5 shows results of applying GNMC with these three mass functions to
the XCSF instances trained on both environments, measured are the eﬀect on
performance ( ˆQ MAE and ˆπ accuracy) and population size (number of macro
and microclassiﬁers).

In the deterministic case, GNMC retains performance when either λf it or
λtan is used, for any value of ρ. ˆQ MAE improves very slightly as ρ increases,
and ˆπ accuracy is unchanged. Using λinv f it gives smooth degradation in both
metrics. Looking at the population sizes, both λf it and λtan exhibit a moderate

10

J.T. Bishop, M. Gallagher

(a) Performance, pslip = 0

(b) Performance, pslip = 0.1

(c) Pop. size, pslip = 0

(d) Pop. size, pslip = 0.1

Fig. 5: Results of applying GNMC with three diﬀerent mass functions to both
FrozenLake8x8 environments. All curves are the mean over 30 instances. Values
of ρ used are from 0 to 0.99 in increments of 0.01.

reduction in the number of microclassiﬁers and a signiﬁcant reduction in the
number of macroclassiﬁers as ρ increases. However λinv f it is diﬀerent, yielding
similar number of macroclassiﬁers in the extreme, but drastically smaller number
of microclassiﬁers. This conforms to our expectations of its operation; classiﬁers
that are kept by λinv f it have low ﬁtness values, and since low ﬁtness classiﬁers
tend to have low numerosity, the ratio of microclassiﬁers to macroclassiﬁers is
low. For λf it and λtan the ratio is much higher. In the stochastic case, the
situation is similar overall with a slight diﬀerence when considering the eﬀect on
performance: namely that ˆπ accuracy for both λf it and λtan is degraded slightly
instead of remaining constant as ρ increases.

7 Rollout Analysis

We now investigate the relationship between policy accuracy and the commonly
used black-box performance metric, steps-to-goal (STG), from other works ap-
plying LCSs to maze-like RL environments, e.g. [10,12]. Note that in such envi-

0.00.10.20.30.40.50.60.70.80.91.00.000.020.040.060.080.100.120.140.16Q MAEfittaninv_fit0 = 0.010.600.650.700.750.800.850.900.951.00 accuracy0.00.10.20.30.40.50.60.70.80.91.00.000.020.040.060.080.100.120.140.16Q MAEfittaninv_fit0 = 0.010.600.650.700.750.800.850.900.951.00 accuracy0.00.10.20.30.40.50.60.70.80.91.00100200300400500600700800900Num. macroclassifiers050010001500200025003000350040004500Num. microclassifiersfittaninv_fit0.00.10.20.30.40.50.60.70.80.91.00100200300400500600700800900Num. macroclassifiers050010001500200025003000350040004500Num. microclassifiersfittaninv_fitOptimality-based Analysis of XCSF Compaction in Discrete RL

11

*

*

GNMC λinv f it ρ = 0.99
GNMC λf it ρ = 0.99
No compaction
(Group C)
(Group B)
(Group A)
ˆπ
Num.
Max
Mean
ˆπ
Num.
Max
Mean
ˆπ
Num.
Max
Mean
acc.
roll
STG
STG
acc.
roll
STG
STG
acc.
roll
STG
STG
0.79
114
15.44 21
0.91
117
15.31 20
0.89
114
15.42 20
115
0.68
15.45 20
117
0.79
15.43 20
0.87
15.39 20
116
150 0.55
*
150 0.72
*
0.75
32.36 110 123
0.55
131
18.74 89
0.83
127
15.46 21
0.83
127
15.37 20
0.70
116
15.38 20
0.85
116
15.38 20
0.83
116
15.38 20
0.66
33.11 148 115
0.79
126
16.13 25
0.85
116
15.38 20
0.74
116
15.38 20
0.85
116
15.38 20
0.87
116
15.38 20
0.60
142
37.46 98
0.85
116
15.38 20
0.87
116
15.38 20
0.75
116
15.39 20
0.81
118
33.25 93
0.83
115
15.43 20
117
15.70 23
0.68
0.91
117
15.31 20
0.85
117
15.31 20
*
150 0.55
*
0.89
115
15.45 20
0.87
116
15.38 20
150 0.64
*
*
0.77
15.39 20
116
0.77
15.66 20
113
150 0.57
*
*
0.75
37.43 119 128
0.75
37.61 119 130
0.64
47.94 143 119
0.79
117
15.33 20
0.83
116
15.38 20
0.68
15.44 21
114
0.83
117
15.43 20
0.85
115
15.45 20
0.55
37.22 112 120
0.92
114
15.42 20
0.91
116
15.38 20
0.64
129
15.48 21
0.83
116
15.32 21
0.83
122
15.43 21
150 0.72
*
0.89
119
15.37 20
0.89
116
15.38 20
121
15.27 19
0.72
0.81
116
15.38 20
0.85
116
15.38 20
150 0.58
*
*
0.75
0.79
128
16.06 25
15.92 21
118
0.57
53.88 110 121
0.75
0.79 68.61 193 117
67.29 193 110
0.70
62.20 200 124
0.79
32.79 110 123
0.85
116
15.38 20
0.53
53.57 153 115
0.79
117
15.33 20
0.85
15.38 20
116
150 0.60
*
*
0.75
117
15.33 20
0.79
32.91 100 112
119
28.51 86
0.62
0.87
116
15.38 20
0.89
116
15.38 20
150 0.38
*
0.87
138
119
17.21 82
15.85 25
0.81
150 0.47
133
15.74 21
*
0.72
142 0.74 15.96 40
150 0.47
150 0.70 *
36.96 139 126
0.62
119
116
15.38 20
150 0.55
116
116
15.38 20

0.77 82.36 197 114
0.81

*
15.37 19
15.38 20

0.79
0.83
0.83

*
*
*

*

*

*

*

Instance
num.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30

Table 2: Results of STG testing procedure on FrozenLake8x8 pslip = 0.1 for three
diﬀerent groups of XCSF instances. Asterisks for mean and max STG indicate
incomplete data. Set in bold are the “worst” values for each column (ˆπ acc.
minimum, others maximum).

ronments, minimising STG is equivalent to maximising cumulative discounted re-
ward. For our analysis we consider only the stochastic variant of FrozenLake8x8,
as it showed the most interesting variations in ˆπ accuracy when GNMC was
applied to it in Section 6.

A testing procedure to measure STG is devised as follows: allow each XCSF
instance a budget of 150 rollouts (episodes) and in these 150 rollouts, attempt
to record STG (successfully reach the goal) 100 times. If 100 successes are not
achieved, STG data is incomplete. In each rollout, the agent’s initial state is
(0, 0) and the random seed of the environment is set to a unique number. Note
this is diﬀerent from training where the agent’s initial state was any s ∈ S, se-
lected uniformly at random. This testing procedure is applied to all 30 trained
XCSF instances in three groups, representing diﬀerent levels of GNMC com-
paction: no compaction, compaction with λf it ρ = 0.99, and compaction with
λinv f it ρ = 0.99. Table 2 shows the collected results: included are mean and
max STG, number of rollouts performed, and ˆπ accuracy (for reference). Note
that minimum STG in the environment is 14.

12

J.T. Bishop, M. Gallagher

Group A in general achieves admirable mean STG; only 5 out of 30 instances
could be considered as outliers. Number of rollouts for all instances is gener-
ally not much higher than 110. This indicates that in most instances XCSF is
quickly navigating towards the goal, with a low failure rate due to environmental
stochasticity. Comparing Group B to Group A, there are two instances in Group
B that have failed to collect complete STG data, also having the two lowest pol-
icy accuracies. In general, all four measures degrade only slightly between the
two groups, which indicates that the compaction applied to Group B is not hav-
ing a detrimental eﬀect on performance. Transitioning from Group A to Group
C however produces noticeable performance loss. 11 out of 30 instances fail to
record complete STG data, and generally those that do show degradation in all
four measures.

It is diﬃcult to determine the exact relationship between STG and policy
accuracy. Some instances (e.g. instance 1) exhibit minimal degradation in both
measures between groups, but some exhibit larger degradation (e.g. instance
22). In cases where degradation in policy accuracy is small but degradation
in STG is large, the cause is often a minority of states along the edge of the
grid that are advocating actions where the only possible way to advance further
towards the goal is to slip, e.g. advocating Right in any of the states in the
rightmost column (where x = 7). It is therefore clear that the two metrics are
complementary rather than competitive. Policy accuracy is measured globally
and is always deﬁned, and STG is measured on a speciﬁc task (starting state),
possibly being undeﬁned/incomplete. The starting state is an arbitrary choice
and if altered results in diﬀering STG but unchanged policy accuracy.

8 Conclusion

We trained XCSF on a deterministic and stochastic variant of FrozenLake8x8,
measuring its performance with respect to the optimal solutions produced via
dynamic programming. Results show that in both cases XCSF achieved low Q-
function approximation error, and in the deterministic case XCSF converged to
maximum policy accuracy. In the stochastic case policy accuracy was notice-
ably degraded both because of increased problem diﬃculty and increased strict-
ness of the optimal policy. Next we introduced Greedy Niche Mass Compaction
(GNMC), a compaction algorithm designed for LCSs applied to discrete RL en-
vironments. We showed GNMC is a generalisation of previous work and applied
it to our trained XCSF instances. Given a suitable mass function, GNMC can
yield a signiﬁcant reduction in population size without increasing function ap-
proximation error and only slightly decreasing policy accuracy. Finally we linked
our policy accuracy metric to the steps-to-goal metric used in previous work
across multiple groups of compacted XCSF instances. This highlighted how the
two metrics are complementary rather than competitive. Suggested future work
includes applying GNMC to environments where populations are larger/more
complex and the mass removal factor has more impact on performance. GNMC’s
concept could also be extended to continuous state and/or action spaces.

Optimality-based Analysis of XCSF Compaction in Discrete RL

13

References

1. Bernad-Mansilla, E., Garrell-Guiu, J.M.: Accuracy-Based Learning Classiﬁer Sys-
tems: Models, Analysis and Applications to Classiﬁcation Tasks. Evolutionary
Computation 11(3), 209–238 (Sep 2003)

2. Butz, M.V., Wilson, S.W.: An algorithmic description of XCS. Soft Computing -
A Fusion of Foundations, Methodologies and Applications 6(3-4), 144–153 (Jun
2002)

3. Butz, M.V.: Learning Classiﬁer Systems. In: Springer Handbook of Computational
Intelligence, pp. 961–981. Springer Handbooks, Springer, Berlin, Heidelberg (2015)
4. Butz, M.V., Lanzi, P.L., Llor, X., Goldberg, D.E.: Knowledge Extraction and Prob-
lem Structure Identiﬁcation in XCS. In: Parallel Problem Solving from Nature -
PPSN VIII. pp. 1051–1060. Lecture Notes in Computer Science, Springer, Berlin,
Heidelberg (2004)

5. Butz, M.V., Lanzi, P.L., Wilson, S.W.: Function Approximation With XCS: Hy-
perellipsoidal Conditions, Recursive Least Squares, and Compaction. IEEE Trans-
actions on Evolutionary Computation 12(3), 355–376 (Jun 2008)

6. Dam, H.H., Abbass, H.A., Lokan, C.: Be real! XCS with continuous-valued inputs.
In: Proceedings of the 2005 workshops on Genetic and evolutionary computation
- GECCO ’05. p. 85. ACM Press, Washington, D.C. (2005)

7. Dixon, P.W., Corne, D.W., Oates, M.J.: A Ruleset Reduction Algorithm for the
XCS Learning Classiﬁer System. In: Learning Classiﬁer Systems. pp. 20–29. Lec-
ture Notes in Computer Science, Springer, Berlin, Heidelberg (2003)

8. Fu, C., Davis, L.: A modiﬁed classiﬁer system compaction algorithm.

In:
GECCO02: Proceedings of the Genetic and Evolutionary Computation Confer-
ence. pp. 920–925. Morgan Kaufmann (2002)

9. Kharbat, F., Odeh, M., Bull, L.: New approach for extracting knowledge from the

XCS learning classiﬁer system. Int. J. Hybrid Intell. Syst. (2007)

10. Lanzi, P.L., Colombetti, M.: An extension to the XCS classiﬁer system for stochas-
tic environments. In: Proceedings of the 1st Annual Conference on Genetic and
Evolutionary Computation - Volume 1. pp. 353–360. GECCO’99, Morgan Kauf-
mann Publishers Inc., Orlando, Florida (Jul 1999)

11. Lanzi, P.L., Loiacono, D., Wilson, S.W., Goldberg, D.E.: Extending XCSF beyond
linear approximation. In: GECCO 2005: Genetic and Evolutionary Computation
Conference: Volume. pp. 1827–1834. ACM Press (2005)

12. Lanzi, P.L., Loiacono, D., Wilson, S.W., Goldberg, D.E.: XCS with computed
prediction in multistep environments. In: Proceedings of the 7th annual conference
on Genetic and evolutionary computation. pp. 1859–1866. GECCO ’05, Association
for Computing Machinery, Washington DC, USA (Jun 2005)

13. Lanzi, P., Loiacono, D., Wilson, S., Goldberg, D.: XCS with computed prediction
in continuous multistep environments. In: 2005 IEEE Congress on Evolutionary
Computation. vol. 3, pp. 2032–2039 Vol. 3 (Sep 2005)

14. Sutton, R.S., Barto, A.G.: Reinforcement learning: an introduction. Adaptive com-
putation and machine learning series, The MIT Press, Cambridge, MA, second
edition edn. (2018)

15. Tan, J., Moore, J., Urbanowicz, R.: Rapid Rule Compaction Strategies for Global
Knowledge Discovery in a Supervised Learning Classiﬁer System. In: Advances in
Artiﬁcial Life, ECAL 2013. pp. 110–117. MIT Press (Sep 2013)

16. Urbanowicz, R.J., Browne, W.N.: Introduction to learning classiﬁer systems.

Springer Berlin Heidelberg, New York, NY (2017)

14

J.T. Bishop, M. Gallagher

17. Urbanowicz, R.J., Granizo-Mackenzie, A., Moore, J.H.: An analysis pipeline with
statistical and visualization-guided knowledge discovery for Michigan-style learning
classiﬁer systems. IEEE Computational Intelligence Magazine 7(4), 35–45 (Nov
2012)

18. Wilson, S.W.: Classiﬁer Fitness Based on Accuracy. Evolutionary Computation

3(2), 149–175 (Jun 1995)

19. Wilson, S.W.: Classiﬁers that Approximate Functions. Natural Computing 1, 1–2

(2001)

20. Wilson, S.W.: Compact Rulesets from XCSI. In: Advances in Learning Classiﬁer
Systems. pp. 197–208. Lecture Notes in Computer Science, Springer, Berlin, Hei-
delberg (2002)

21. Wilson, S.W., Wilson, S.W.: Mining Oblique Data with XCS. In: Proceedings
of the Third International Workshop (IWLCS-2000), Lecture Notes in Artiﬁcial
Intelligence. pp. 158–174. Springer-Verlag (2000)

