EnsemblepruningviaanintegerprogrammingapproachwithdiversityconstraintsMarceloAntˆonioMendesBastos1HumbertoBrand˜aoC´esardeOliveira2CristianoArbexValle11DepartamentodeCiˆenciadaComputa¸c˜ao,UniversidadeFederaldeMinasGerais,BeloHorizonte,MG,Brasil{marcelo.bastos,arbex}@dcc.ufmg.br2DepartamentodeCiˆenciadaComputa¸c˜ao,UniversidadeFederaldeAlfenas,Alfenas,MG,Brazilhumberto@bcc.unifal-mg.edu.brAbstractEnsemblelearningcombinesmultipleclassiﬁersinthehopeofobtainingbetterpredictiveperformance.Em-piricalstudieshaveshownthatensemblepruning,thatis,choosinganappropriatesubsetoftheavailableclassiﬁers,canleadtocomparableorbetterpredictionsthanusingallclassiﬁers.Inthispaper,weconsiderabinaryclassiﬁcationproblemandproposeanintegerprogramming(IP)approachforselectingoptimalclassiﬁersubsets.Weproposeaﬂexibleobjectivefunctiontoadapttodesiredcriteriaofdiﬀerentdatasets.Wealsoproposeconstraintstoensureminimumdiversitylevelsintheensemble.DespitethegeneralcaseofIPbeingNP-Hard,state-of-the-artsolversareabletoquicklyobtaingoodsolutionsfordatasetswithupto60000datapoints.Ourapproachyieldscompetitiveresultswhencomparedtosomeofthebestandmostusedpruningmethodsinliterature.Keywords:ensemblelearning,ensemblepruning,integerprogramming,diversity1IntroductionEnsemblelearningisapopulartechniqueinthedomainofmachinelearning.Anensembleisdeﬁnedastheaggregationofmultipleclassiﬁcationsintoasingleﬁnaldecision.Itisgenerallyacceptedinliteraturethattheprecisionofanensembletendstoimprovewhencomparedtothebehaviourofindividualclassiﬁers[Zhangetal.,2006].WellknownaproachesforeﬃcientlygeneratingensemblesincludeBagging(bootstrapaggregating)[Breiman,1996]andBoosting[FreundandSchapire,1997].ThelattergaverisetopopularvariationssuchasRandomForests[Breiman,2001]andextremegradientboosting[ChenandGuestrin,2016].Ageneralfeatureoftheseapproachesisthatallclassiﬁersareconsideredintheaggregation.Thereare,however,theoreticalandempiricalstudieswhichhaveshownthatpruninganensemblebyselectingasubsetoftheclassiﬁerscanleadtocomparableorbetterpredictions[Luetal.,2010,Zhangetal.,2006].Inthisworkwetackletheensemblepruningproblembyintroducinganintegerprogramming(IP)approachforchoosinganoptimalsubsetofbinaryclassiﬁers.Ourformulationoptimisesaweightedfunctionofthepatternsinthebinaryconfusionmatrix.Thisprovidesenoughﬂexibilitytoensuresuitableoptimisationcriteriadependentonthepropertiesoftheunderlyingdataset.Asourobjectiveisperformance-basedwealsointroducelinearconstraintsthatensureminimumdiversitylevelsintheensemble.Despiteseveraltechniquesforensemblepruninghavingbeenpreviouslyproposed,webelievethatourapproachcontributestothecurrentknowledgeintheﬁeld.IPmodellingisaﬂexibletool,adaptabletoparticularitiesofdiﬀerentproblems.Oneofthemostimportantadvantagesinapplyingthistooltoensemblepruningisbeingabletocombineperformanceanddiversitycriteria.MoreoveranIPframeworkprovidesanexactmethod,asopposedtomostalgorithmsinliteraturewhicharegenerallysuboptimal.1arXiv:2205.01088v1  [cs.LG]  2 May 2022ThegeneralIPproblemisNP-Hard,howeverdecadesofresearchinalgorithmictechniqueshaveledtostate-of-the-artsolverswhichareabletoeﬀectivelysolveseveralindustrial-sizedproblems.Inthispaperweshowthatwithsuchsolverswecanﬁndgoodsolutionstorelativelylargeproblemsinreasonablecomputationaltimes.Wecompareourformulationtoafullensembleandsixotherwell-knownmethodsinliterature.Wereportcompetitiveresultsforpubliclyavailabledatasetsrangingfrom195to60000datapoints.Theremainderofthispaperisorganisedasfollows.InSection2wegiveabriefoverviewofexistingmethodsinensemblelearning.InSection3wepresentouroptimisationapproachandinSection4weamendittoenforceminimumdiversitylevels.OurcomputationalexperimentsareshowninSection5andinSection6wepresentourconcludingremarks.2LiteraturereviewTheﬁrststepinanensembleprocessistogenerateasetofdistinctclassiﬁersthatishopefullypreciseanddiverse.Highlycorrelatedclassiﬁersmayhinderthepotentialbeneﬁtofusinganensemble.Severaltechniquesforensuringdiversityinclassiﬁershavebeenproposed[Duin,2002,Cruzetal.,2018].Theseincluderandomisingclassiﬁers,tuningparameters(e.g.pruninglevelindecisiontrees)orexploringdiﬀerentar-chitectures(e.g.numberofhiddenlayersinmulti-layerperceptronneuralnetworks).Combiningdistinctclassiﬁermodelsintoheterogeneousensemblesisalsoapopularapproach.Otherdiversiﬁcationtechniquesincludetrainingclassiﬁerswithdiﬀerentdistributionsofthetrainingset(e.g.theaforementionedBaggingandBoostingtechniques)andwithdistinctsubsetsoffeatures.Thenextstepisselectinganappropriatesubsetoftheavailableclassiﬁers.Thisselectioncanbestaticordynamic.Indynamicselection,diﬀerentsubsetsarechosenfordiﬀerentdatapoints.Thereasoningisthatcertainsubsetsmaybemorespecialisedindiﬀerentpartsofthefeaturespace.Formoredetailswereferthereaderto[Brittoetal.,2014,Cruzetal.,2018].Mostworkshoweverapplystaticselection,whereasinglesubsetischosen.Staticselectionpoliciescanbebasedonranking,clustersandoptimisation.Rankingmethodssortclassiﬁersaccordingtoaﬁtnessfunction.Ingeneraltheygreedilyincreasethesubsetsize.InKappapruning[MargineantuandDietterich,1997],everypairofclassiﬁersissortedaccordingtoastatisticalmeasureofagreement.Pairswiththelowestagreementlevelsareselected.Reorderingtechniques[Mart´ınez-Mu˜nozandSu´arez,2006,2007]inbaggingclassiﬁershavebeenusedtobuildsubensemblesofincreasingsize.Tsoumakasetal.[2004]rankheterogeneousclassiﬁersaccordingtoasigniﬁcanceindex.DaiandHan[2016]proposedadynamicprogrammingapproachtoimprovethecomputationaleﬃciencyofthesemethods.Clustermethodsﬁrstapplyaclusteringalgorithmtoseparateclassiﬁersaccordingtosomesimilaritymeasureandthenpruneeachclusterseparatelytoincreasegeneraldiversity.Knownclusteringalgorithmsincludek-means[LazarevicandObradovic,2001,Qiangetal.,2005],wheresimilarityisbasedonEuclideandistance,andhierarchicalagglomerativeclustering[Giacintoetal.,2000],whichemploysprobabilities.Diﬀerentpoliciesforchoosingthenumberofclusters[Giacintoetal.,2000,LazarevicandObradovic,2001]havealsobeensuggested.Severaloptimisationmethodsforensemblepruninghavealsobeenproposed,withmostoﬀeringapproximatesolutions.Themostpopularmethodishillclimbing,whichhasbeenappliedwithseveraldiﬀerentﬁtnessfunc-tions.Somearebasedonperformance[Fanetal.,2002,Caruanaetal.,2004](e.g.accuracy),othersondiversity[MargineantuandDietterich,1997,Tangetal.,2006,Partalasetal.,2008].Twoexamplesofdiversity-basedﬁt-nessfunctionsareComplementariness[Martınez-Mu˜nozandSu´arez,2004]andConcurrency[Banﬁeldetal.,2005].Partalasetal.[2010]proposedtheUncertaintyWeightedAccuracy,whichasthenamesuggeststakesintoaccountuncertaintybyweightingdiﬀerentlytheimportanceofdatapointsconsidered“easy”and“hard”topredict.Parta-lasetal.[2006]employedareinforcementlearning-basedapproachforagreedymethodbasedondiversity.Zhangetal.[2006]andXuetal.[2012]proposedsemi-deﬁniteprogrammingapproacheswhichconsidertrade-oﬀsbetweenaccuracyanddiversity.Thelaststepintheprocedureiscombiningclassiﬁersintoasingleprediction,whichisusuallydonethroughmajorityvoting.Forfurtherdetailswereferthereaderto[Kittleretal.,1998].3FormulationConsiderabinaryclassiﬁcationproblemwheredatapointsbelongtoclasses1(positive)or0(negative).InthissectionwepresentanIPformulationforchoosinganoptimalsubsetofbinaryclassiﬁers.LetK={1,...,K}bethesetofclassiﬁers.LetN0={1,...,N0}andN1={1,...,N1}bethesetsofnegativeandpositivedatapointsrespectively,withN=N0+N1beingthetotalnumberofdatapoints.ConsideraN1×K2matrixBwhereβik=1ifclassiﬁerk∈Kcorrectlyclassiﬁeddatapointi∈N1aspositive,βik=0ifitmistakenlyclassiﬁediasnegative.AccordinglyconsideraN0×KmatrixAwhereαjk=0ifclassiﬁerk∈Kcorrectlyclassiﬁeddatapointj∈N0asnegative,αjk=1ifjwasmistakenlyclassiﬁedaspositive.SupposeS⊆KisasetofSclassiﬁersselectedtocomposeagivenprunedensemble.Foranydatapointi∈N1,Ps∈SβisisthenumberofcorrectpositiveclassiﬁcationswithinS.Accordingly,foranydatapointj∈N0,Ps∈Sαjsrepresentsthenumberof(wrong)positiveclassiﬁcationswithinS.Wedeﬁneathreshold0≤L≤Ssuchthatforagivendatapointi∈N1,Ps∈Sβis>Limpliesthattheensembleclassiﬁesiaspositive.IfPs∈Sβis≤L,theniisclassiﬁedbytheensembleasnegative.Similarlyforj∈N0,Ps∈Sαjs>LimpliesapositiveensembleclassiﬁcationandPs∈Sαjs≤Limpliesanegativeensembleclassiﬁcation.Forinstance,ifS=10andL=5,thentheensembleclassiﬁesadatapointaspositiveifatleast6individualclassiﬁcationsarepositive.If5orlessarepositive,thentheensembleclassiﬁesthatdatapointasnegative.InourformulationwelettheoptimisationdeﬁnebothSandL.HenceweincludeLasageneralintegervariablerepresentingtheclassiﬁcationthresholdandbinaryvariablesxk=1ifclassiﬁerk∈Kischosentocomposetheensemble(xk=0otherwise).Predicted10Actual1T+F−0F+T−Table1:BinaryclassiﬁcationconfusionmatrixConsiderthebinaryconfusionmatrixgiveninTable1,whereT+,F−,T−andF+arethetotalnumberofclassiﬁcationsofeachpossiblepattern.ForeachpatternsweassignweightsW+T,W−T,W+F,W−F∈R,andtheobjectivefunctionisdeﬁnedbytheweightedsumW+TT++W−FF−+W−TT−+W+FF+.Formodellingthisweightedsumwedeﬁnebinaryvariablest+i,f−iiftheensembleclassiﬁcationofi∈N1isrespectivelyatruepositiveorfalsenegative.Similarlywedeﬁnebinaryvariablest−j,f+jiftheensembleclassiﬁcationofj∈N0isatruenegativeorfalsepositive.TheIPformulationthatoptimisesaweightedsumofthepatternsinthebinaryconfusionmatrixisgivenbelow:maxN1Xi=1(W+Tt+i+W−Ff−i)+N0Xj=1(W−Tt−j+W+Ff+j)(1)subjectto(L+1)−KXk=1xkβik≤(K+1)(1−t+i),∀i∈N1(2)KXk=1xkβik−L≤(K+1)t+i,∀i∈N1(3)t+i+f−i=1,∀i∈N1(4)KXk=1xkαjk−L≤K(1−t−j),∀j∈N0(5)(L+1)−KXk=1xkαjk≤Kt−j,∀j∈N0(6)f+j+t−j=1,∀j∈N0(7)xk∈B∀k∈K(8)t+i,f−i∈B∀i∈N1(9)t−j,f+j∈B∀j∈N0(10)0≤L≤K,(11)L∈Z(12)3Constraints(2)ensurethatapositivedatapointi∈N1hast+i=1ifthenumberofindividualpositiveclassiﬁcationsexceedsL.Conversely,constraints(3)ensurethatt+i=0ifthenumberofindividualpositiveclassiﬁcationsisnomorethanL.Constraints(4)ensurethateithert+i=1orf−i=1.Constraints(5)guaranteethatanegativedatapointj∈N0hast−j=0ifthenumberofpositiveclassiﬁcationsexceedsL.Otherwise,constraints(6)makesurethatt−j=1.Constraints(7)ensurethateitherf+j=1ort−j=1.Constraints(8-12)deﬁnevariablesbounds.3.1ObjectivefunctionForsomeclassiﬁcationproblems,itmaybedesirabletooptimisesomepatternsinsteadofothers.Forinstance,inaninvestmentdecision,investinginthewrongprojectmaycausebankruptcywhilenotinvestinginapromisingprojectmaybeseenasaregretfulbutacceptablelostopportunity.InthiscaseprioritisingtheminimisationofF+isdesirable.TheweightsinEquation(1)provideﬂexibilityfordeﬁningoptimisationcriteriadependingonthecharacteristicsofthedatasetathand(suchasbeinghighlyimbalanced).Afewexamplesareoutlinedbelow.AccuracyisdeﬁnedasT++T−N.AsNisconstantwecanmaximiseaccuracybydeﬁningweightsW+T=W−T=1andW+F=W−F=0.Noticethatifwechoosethisobjectivethenconstraints(3)and(6)areredundantasmaximisingpositiveweightsW+TandW−Tensurethatt+i=1andt−i=1ifallowedbyconstraints(2)and(5).SimilarlyrecallisdeﬁnedasT+T++F−=T+N1andcanbemaximisedbysettingW+T=1andW−T=W+F=W−F=0(withconstraints(3)beingredundant).Nowconsiderθ=N1Nasthedatasetimbalancelevel.If,forinstance,θ≤1−(cid:15)forsmall(cid:15),ahighaccuracycanbeachievedbyclassifyingeverydatapointaspositive.Forimbalanceddatasetsapossiblyusefulconﬁgurationofobjectivefunction(1)isbysettingweightsW+T=(1−θ),W−T=θandW+F=W−F=0.4DiversityAsmentionedbeforemanyensemblepruningalgorithmsemploydiversitycriteria.Ourproposedformulationoptimisesaperformancemeasure,andinthissectionweintroduceawaytocontroldiversitywithlinearconstraints.WeconsideradiversitymeasurecalledPairwiseFailureCrediting(PFC),proposedoriginallybyChandraandYao[2006],chosenduetosatisfactoryperformanceinimbalanceddatasets[Bhowanetal.,2012,Fernandesetal.,2015].PFCmeasureshowdiverseanindividualclassiﬁerisfromtheremainingclassiﬁersintheensemble.PFCiscalculatedasfollows.Foreachclassiﬁerk,wecomputeafailurepattern(FP).AFPisastringof0’sand1’swithlengthN.A‘0’inthestringmeansthattheclassiﬁerfailedtocorrectlypredictthecorrespondingdatapointanda‘1’meansthatitpredictedthedatapointcorrectly(irrespectiveofitsrealvalue).OncewehaveallfailurepatternswetakeanytwoclassiﬁerskandlandcalculatetheirHammingdistance.TheHammingdistancebetweensame-lengthstringsisthenumberofdiﬀerentcharactersinthesamepositions.Forexample,ifFPk={0011011101}andFPl={0110001110},theHammingdistancebetweenkandlis5(characters2,4,6,9and10diﬀer).Next,wesumallfailuresbybothclassiﬁers-thatis,wesumthenumberofzerosinbothstringswhich,intheexample,is9.Thefailurecredit(FC)betweenkandlisobtainedbydividingtheHammingdistancebythesumoffailures.Intheexample,FCkl=5/9.Foreverypairk,l∈KwecomputeFCkl.ConsideragainSasasetofS≤Kclassiﬁersthatareselectedtocomposeanensemble.WeassumewithoutlossofgeneralitythatclassiﬁersinSareindexedbyk=1,...,S.TheirPFCvaluesaregivenby:PFCk=PSl=1,l6=kFCklS−1k∈SA(maximum)valueof1inPFCkmeansthatkclassiﬁesalldatapointsdiﬀerentlyfromeveryotherclassiﬁerintheensemble,anda(minimum)valueof0meansthatkisidenticaltoallotherclassiﬁers.Bothextremecasesimplythatallotherclassiﬁersareidenticalamongthemselves.Forensuringminimumdesireddiversitylevels,weproposetwoapproaches:(i)theminimumPFCvalueofanyindividualclassiﬁerisatleastacertainthreshold0≤τ≤1inordertopreventverysimilarpairsofclassiﬁersand(ii)theaveragePFCvalueoftheensemblemustbeatleastacertainthreshold0≤γ≤1toensureanoverallgoodlevelofdiversity.Clearlywemusthaveγ≥τ.Weaddthefollowingnewdecisionvariables.Letykl=1ifbothclassiﬁerskandlhavebeenselectedtobepartoftheensemble,andykl=0ifatmostoneofkandlischosentocomposetheensemble.Thisadds(cid:0)K2(cid:1)extravariables(foreverypossiblepairk,l).Forsimplicity,bothyklandylkdenotetheexactsamevariable.The4followingconstraintsensurethatykltakesthecorrectvalues:ykl≥xk+xl−1∀k,l∈K,k<l(13)ykl≤xk∀k,l∈K,k<l(14)ykl≤xl∀k,l∈K,k<l(15)ykl≥0∀k,l∈K,k<l(16)Noticethatthereisnoneedfortheyklvariablestobebinary.Asbothxkandxlarebinary,yklmusthaveintegervaluesinanyintegersolution.WecanthenrewritethePFCequationusingvariablesxkandykl:PFCk=PKl=1,l6=kFCklyklPKm=1xm−1∀k∈KThetermPKm=1xmisthecardinalityoftheensembleandanynon-selectedclassiﬁerk(withxk=0)hasaPFCvalueof0(asallykl=0,l6=k).ThefollowinglinearconstraintsensurethateveryclassiﬁerhasaminimumPFCvalueofτ:KXl=1l6=kFCklykl≥τ(cid:16)KXm=1xm−1(cid:17)−Kτ(1−xk)∀k∈K(17)ThetermKτ(1−xk)ensuresthattheconstraintsaboveareonlyenforcedifclassiﬁerkischosentocomposetheensemble.ThefollowingnonlinearconstraintensuresthattheaveragePFCoftheensembleisatleastγ:1PKm=1xmPKk=1PKl=1,l6=kFCklyklPKm=1xm−1≥γ(18)ObservethatinEquation(18)theFCvalueofeverypairisaddedtwice.Weusethisfacttolinearisethisexpression.ForagivensubsetS,theaveragePFCµPFCisgivenby:µPFC=1SSXk=1PSl=1,l6=kFCklS−1=1S(S−1)SXk=1SXl=1l6=kFCkl=2S(S−1)S−1Xk=1SXl=k+1FCkl=1(cid:0)S2(cid:1)S−1Xk=1SXl=k+1FCkl=µFCwhereµFCdenotestheaverageFCvalueofallpairsintheensemble.WeconcludethatµFC=µPFC,thatis,theaveragePFCamongallclassiﬁersintheensembleisequaltotheaverageFCamongallpairs.IfSclassiﬁersareselectedintheensemble,thenthenumberofyklvariablesthattakevalue1isexactly(cid:0)S2(cid:1).ThereforewecanensurethattheaveragePFCvalueisatleastγwiththefollowinglinearconstraint:K−1Xk=1KXl=k+1FCklykl≥γK−1Xk=1KXl=k+1ykl(19)Theexpandedformulationwithminimumdiversitylevelsisgivenbymaximising(1)subjectto(2)-(17)and(19).Itrequires(cid:0)K2(cid:1)extravariablesandasimilarnumberofextraconstraints,whichcouldleadittobemorecomputationallydemanding.HoweverweobservedempiricallyinSection5.3thattheinclusionofsuchconstraintscausesanegligibledecreaseinsolutionquality.55ComputationalexperimentsInthissectionweoutlinethecomputationalexperimentsusedtoevaluatetheproposedformulation.Weused9publiclyavailabledatasets,outlinedinTable21,andrangingfromN=195toN=60000.Wealsoincludeacolumnwiththevalueoftheimbalanceparameterθ.5.1DescriptionoftheexperimentsWeprepared10diﬀerentheterogeneousclassiﬁermodels.Eachmodelwasinstantiatedanumberoftimeswithdif-ferentrandomseedsandparameters.WesetKasmultiplesof10inordertohaveanequalnumberofinstantiationsofeachclassiﬁer.Forinstance,ifK=60,wehave6classiﬁersofeachmodel.Inourexperiments,reportedbelow,weusedK={40,60,80,100}.Eachclassiﬁerproduces,asoutput,aprobabilityofadatapointbeingpositive.ThisprobabilityisroundedtodeﬁnematricesAandB.Amorethoroughdescriptionoftheclassiﬁerscanbefoundinthesupplementarymaterialaccompanyingthispaper.Forevaluatingperformanceweusedastratiﬁed10-foldcross-validationprocedure.TheNdatapointsareinitiallyshuﬄedrandomlyandthedatasetissplitinto10folds.Ateachiteration,oneofthefoldsisleftoutasanindependentset(unseenbythealgorithms).Theresultspresentedbelowarebasedsolelyinthisset.Theother9folds,comprising90%oftheoriginaldataset,arejoinedandfurthersplitintotwosets:atrainingset,containing63%ofthedatapoints,isusedtooptimisetheindividualclassiﬁers.Avalidationset,comprisingtheremaining27%datapoints,isusedtooptimisetheensemblealgorithms.Theprocedureaboveisrepeated10times:ineachwevarytherandomseedsrequiredtobothshuﬄethedatasetandinitialisetheindividualclassiﬁers.ForeachvalueofKandforeachinstanceshowninTable2,werun100experiments:10randominitialisations×10folds.Forensuringreproducibilityofourresults,wehavemadeallnecessarydatapubliclyavailable.Alinkcanbefoundinthesupplementarymaterial.Table2:SelecteddatasetsfromtheUCIMachineLearningRepository[Lichman,2021]IdentiﬁerDatasetnameFeaturesNN0N1θPRKParkinsons23195481470.77MSKMusk(Version1)1684762692070.44BCWBreastCancerWisconsin325693572120.37QSRQSARbiodegradation4110553566990.66DRDDiabeticRetinopathyDebrecen2011515406110.53SPASpambase574601278818130.39DEFDefaultofcreditcardclients24300002336466360.22BMKBankMarketing21411883654846400.11APSAPSFailureatScaniaTrucks171600005900010000.025.2BenchmarksToevaluateourformulation,wecompareittosevenotherapproaches:Full(non-pruned)Ensemble(FE),Reduced-ErrorPrunningwithBackﬁtting[FriedmanandStuetzle,1981](herebyBackﬁttingorBFT),Kappapruning[MargineantuandDietterich,1997]andfourdiﬀerenthillclimbingbasedmethods.Duetospaceconstraintswereporthereonlyresultsforonlyfourapproaches.Thefullresultsareavailableinthesupplementarymaterialaccompanyingthispaper.Allbenchmarksclassifydatapointsbasedonmajorityvoting.Backﬁttingaimstomaximiseaccuracybyfollowingagreedyapproachwithrevision.Initially,theclassiﬁersubsetSisempty.Ateachiteration,thealgorithmaddstoSaclassiﬁerssuchthattheaccuracyofS∪sismaximised.ThisprocessisrepeateduntilapredeﬁnednumberMofclassiﬁersisaddedtoS.Tiesarebrokenarbitrarily.Wheneveraclassiﬁerisadded,thegreedychoiceisrevisedthroughalocalsearchprocedure.Eachclassiﬁerintheensembleisiterativelyreplacedbyanotherpreviouslyleftout.Iftheoverallaccuracyisimproved,themethodstartsagainwiththenewsubsetS.Thelocalsearchstopsafter100iterationsorwhenevernoclassiﬁerisabletoimprovethesolution,whicheverhappensﬁrst.Backﬁttingrequirestheensemblesubsetsizetobeﬁxed.Forafairercomparison,wevariedMwithin20%and80%ofK.Thebestin-sampleresultsareusedtoevaluatetheindependentset.1AlldatasetsusedinthispapercanbefoundattheUCIMachineLearningRepository[Lichman,2021]6Kappapruningfollowsasimilarprocedure,butwithtwonotablediﬀerences:thereisnorevisionofthegreedychoiceanditoptimisestheκ-statistic[Cohen,1960],adiversitymeasureofstatisticalagreementbetweenanytwoclassiﬁers.BothBackﬁttingandKappapruningrequiretheensemblesubsetsizetobeﬁxed.Forafairercomparison,wevariedMwithin20%and80%ofK.Thebestin-sampleresultsareusedtoevaluatetheindependentset.Theotherbenchmarksusetheforwardversionofthehillclimbingsearchalgorithm,andtheydiﬀeronlyintheselectedﬁtnessfunction.Inallfourmethods,theﬁrstiterationselectstheindividualclassiﬁerwithmaximumaccuracy,regardlessoftheﬁtnessfunctionchosen.Inthefollowingiterations,classiﬁersaregreedilyaddedsoastomaximisetheselectedﬁtness.ThisprocessisrepeateduntilallclassiﬁersareaddedtoS.Intheend,thechosenensembleistheonewiththebestaccuracyoveralltheensemblesiterativelycreated.Asopposedtotheotherbenchmarks,directhillclimbingdoesnotdeﬁnetheensemblesizeapriori.TheﬁtnessfunctionschosenarethesameastestedbyPartalasetal.[2010]:Accuracy,Complementariness[Martınez-Mu˜nozandSu´arez,2004],Concurrency[Banﬁeldetal.,2005]andUncertaintyWeightedAccuracy[Partalasetal.,2010].WerefertothemasHC-ACC,HC-CON,HC-COMandHC-UWA.IntheexperimentsreportedbelowwecompareourformulationtoBFT,twohillclimbingmethods,HC-CONandHC-UWA(whichhadabetterperformancewhencomparedtoHC-ACCandHC-COM)andFE(FullEnsemble).Thecompletesetofresultsisreportedinthesupplementarymaterial.Allalgorithmsareallowedtorunforamaximumof5minutes.5.3Solvingtheformulation0-1IPisnotoriouslycomputationallydiﬃcult.ItsfeasibilityversionisoneofKarp’s21NP-completeproblems[Karp,1972].ModerndaysolvershowevercaneﬀectivelysolvelargeinstancesofseveraldiﬀerentIPproblemsbyemployingacombinationoftechniques(suchasbranch-and-boundandcuttingplanes).Fromanoptimisationpointofview,importantconsiderationsare(i)uptowhatsizearemodernsolversabletooptimallysolveourformulationwithinareasonabletimelimit,and(ii)ifthelimitisreachedpriortoprovingoptimality,howlargearetheoptimalitygapsandhowquicklydoesthesolverﬁnd“goodenough”solutions?WhilesearchingfortheseanswersandstudyingtraditionalIPtechniquesinordertoimprovecomputationalperformanceiswithinthescopeofourcurrentandfuturework,wehaveopted,duetolimitedspace,torefrainfromfurtherdiscussingthem.Wehoweverobservedinpracticethat,withatimelimitofonly5minutes,wewereabletoeitheroptimallysolveorterminatethealgorithmwithsmalloptimalitygapsforallinstances.ThesegapsareshowninTable3.Table3:Averageoptimalitygapsandcorrespondingstandarddeviations(in%).Instance27%ofNNodiversityWithdiversityAvg.Std.Avg.Std.PRK260.00.00.00.0BCW1290.00.00.00.0MSK1540.00.00.00.0QSR2850.00.00.00.1DRD3114.32.06.11.8SPA12420.00.00.20.2DEF81006.70.46.90.4BMK111215.40.35.50.2APS162000.10.00.10.0ThistablesummarisestheaveragegapsandtheirrespectivestandarddeviationsforexperimentsreportedbelowinTable4withK=100.The“Nodiversity”columncorrespondstoF1inthattable,andonlyconstraints(2)-(12)areused.The“Withdiversity”columncorrespondstoF3,whichusesconstraints(2)-(17)and(19).Thelargestinstance,APS,hadaveragegapsofonly0.1%inbothcases.ThehardestinstancewasDEF(6.7%and6.9%).TheonlycasewhereadiﬀerencewasnotablewasfortheDRDinstance(4.3%and6.1%).Inourview,eventhehardestinstanceswerestillrelativelyclosetooptimalityconsideringtheshortcomputa-tionaltime.Forthisreason,inallcomputationalexperimentsreportedbelow,wesetatimelimitof5minutes.Ifthetimelimitisreached,wehaltthesolverandretrievethebestsolutionavailableatthatpoint.WeemployedCPLEX12.8[CPLEXOptimizer,2021]withdefaultparametersastheIPsolverandwerunallexperimentsinanIntelCore(TM)I7-7700@3.60GHzwith32GBofRAM,using8coresandhavingLinuxastheoperatingsystem.75.4AccuracyInourﬁrstsuiteofexperimentswesetweightsW+T=W−T=1andW+F=W−F=0,thatis,weseektomaximiseclassiﬁcationaccuracyregardlessofthedatasetimbalancelevel.ToevaluateboththeformulationintroducedinSection3andthediversityconstraintsintroducedinSection4,weproposethreediﬀerentconﬁgurations.Intheﬁrstweemployonlyconstraints(2)-(12),withoutenforcingdiversity-werefertothisconﬁgurationasF1.Theothertwoconﬁgurations,F2andF3,enforceminimumdiversitylevelsinthehopeofpreventingpossibleoverﬁttingofthetrainingandvalidationsets.InF2wesetτ=0andγ=PFCmin+PFCavg2,wherePFCminandPFCavgaretheminimumindividualPFCamongallclassiﬁersandtheaveragePFCvalueofthefullensemble.SoinF2weonlyconstraintheoverallaveragePFCvalue.InF3wealsosetτ=PFCmin,sorestrictingindividualPFCvaluesaswellastheaveragePFC.Table4:Out-of-sampleaverageaccurariesandstandarddeviations.DatasetKF1F2F3BFTHC-CONHC-UWAFEAvg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.PRK400.90660.01450.91340.01360.91140.01300.90410.01560.91180.01520.90980.01830.90140.0066600.90360.01240.90890.01030.90840.01130.89990.01460.90830.01760.90890.01950.89940.0088800.90510.01440.90520.01240.90560.01310.90150.00840.90480.01670.91290.02230.90040.00911000.90360.01540.90700.02020.90420.01830.90200.00800.90890.01740.91200.02130.89590.0092BCW400.96640.00510.96890.00520.96900.00540.96520.00580.96800.00320.96710.00500.96320.0058600.96760.00430.97010.00360.97030.00380.96710.00560.96960.00450.96800.00400.96290.0056800.96430.00570.96730.00320.96730.00320.96600.00490.97100.00480.97010.00490.96290.00511000.96180.00520.96760.00360.96760.00400.96730.00470.97100.00520.96920.00490.96270.0056MSK400.91090.01100.91650.00930.91270.00900.91170.01220.92380.00920.91920.01260.88960.0100600.91190.00860.91290.00920.91150.00910.90830.01360.92300.01030.91770.01140.88690.0097800.91920.00970.92080.00780.92080.00760.91630.01170.92910.00900.92070.00960.88940.00991000.91960.00660.91920.00750.91880.00850.90980.01350.92990.00810.92150.00770.88850.0096QSR400.87400.00410.87310.00380.87420.00470.87190.00460.87340.00490.87330.00400.87060.0033600.87550.00560.87560.00450.87400.00550.87100.00500.87360.00770.87250.00510.86920.0039800.87150.00470.87280.00470.87400.00330.87470.00450.87310.00620.87470.00550.87060.00251000.87220.00540.87570.00270.87320.00420.87290.00500.87240.00400.87380.00600.87050.0028DRD400.74010.00600.74050.00910.73890.00750.74150.00800.74180.00770.74670.00700.70860.0080600.74580.01140.74620.00960.74480.01040.74750.01000.74800.00860.74970.00680.71100.0077800.74490.00630.74120.00540.74370.00680.74640.00600.75010.00650.74920.00700.70940.00781000.74010.00830.74590.00540.74420.00650.74410.00650.74860.00790.75160.00560.71140.0076SPA400.95290.00140.95340.00130.95340.00130.95290.00080.95350.00120.95230.00110.94760.0010600.95330.00130.95420.00150.95370.00170.95350.00140.95470.00140.95340.00120.94690.0007800.95380.00180.95360.00200.95360.00140.95320.00140.95540.00160.95360.00180.94680.00081000.95350.00180.95380.00160.95370.00160.95300.00140.95570.00120.95370.00150.94670.0006DEF400.82060.00050.82050.00050.82040.00050.82060.00040.82050.00050.82110.00040.82070.0004600.82050.00050.82040.00050.82060.00030.82070.00030.82040.00040.82110.00040.82050.0006800.82030.00060.82060.00060.82060.00080.82030.00040.82060.00040.82120.00030.82030.00041000.82020.00040.82000.00030.82050.00060.82060.00040.82050.00040.82130.00030.82010.0004BMK400.91780.00040.91710.00060.91700.00060.91660.00060.91680.00070.91730.00050.91430.0007600.91750.00050.91730.00060.91730.00060.91670.00030.91700.00070.91730.00050.91340.0007800.91740.00070.91740.00070.91720.00050.91650.00060.91690.00090.91730.00060.91310.00051000.91760.00040.91740.00050.91730.00070.91670.00040.91680.00070.91730.00080.91240.0006APS400.99370.00020.99380.00020.99380.00020.99360.00020.99380.00010.99380.00020.99290.0001600.99380.00020.99380.00020.99380.00020.99360.00020.99370.00010.99380.00010.99250.0001800.99390.00010.99380.00010.99380.00010.99360.00020.99380.00010.99370.00020.99250.00011000.99380.00010.99390.00010.99390.00010.99370.00020.99380.00010.99380.00010.99250.0001Average:0.89850.00490.89970.00450.89930.00460.89790.00490.90120.00520.90090.00550.88940.0041Table4presentstheresultsforthe9datasetsandfourdiﬀerentvaluesofK.Foreachalgorithm,weincludetwocolumns:theaverage(Avg.)andstandarddeviation(Std.)oftheout-of-sampleaccuracy,calculatedwiththedatapointsintheindependentset.Weremindthereaderthateachentryinthetablerepresentstheaverageaccuracyof100diﬀerentruns.AboldvalueinanyoftheﬁrstthreeAvg.columnsmeansthatourformulationobtainedahigheraveragethanallfourbenchmarks.Thelastrowgivesanoverallaveragevalueofthecorrespondingcolumn.Wealsopresent,inTable5,theaveragerankpervalueofKacrossalldatasets.Therankingprocedure8[Demˇsar,2006]worksasfollows.Foreachdataset,thebestperformingalgorithmgetsrank1.0,thesecondbestgetsrank2.0,andsoon.Ifmultiplealgorithmstie,theyareassignedtheaverageoftheirranks.Table5:AverageranksofaccuraciesKF1F2F3BFTHC-CONHC-UWAFE403.823.743.794.043.713.665.25603.823.703.764.033.663.665.39803.823.803.754.093.543.585.421003.913.723.724.073.553.565.48Avg:3.843.743.754.063.613.615.38Allmethodsinthetableshave,asprimaryobjective,maximisingaccuracy.Theresultssuggestthatwhiletheformulationisoverallcompetitive,itwasslightlyoutperformedbyHC-CONandHC-UWA-bothintermsofaverageaccuracyandaveragerank.Still,withtheexceptionofFE,thediﬀerencebetweenBFT(worstperforming)andHC-CON(bestperforming)was0.33%intermsofaverageoverallaccuracyand0.45intermsofaveragerank.Addingdiversityconstraintstoourformulationalsohadasmallbeneﬁcialimpactinimprovingaverageaccuracyandreducingtheaverageranking.In11outofthe36cases,F2outperformedallbenchmarks.BothHCmethodshadahigherdispersionofaccuraciesthanBFTandourformulation.Moreoveraddingdiver-sityinF2andF3helpedreducethatdispersion.Whileaddingdiversityresultedinasmallimprovement,furtherstudiesoneitherbetterenforcingtheseconstraintsorproposingnewconstraintsbasedonalternativediversitymeasuresremainasfuturework.Sinceourproposedmethodisexactinnature(althoughlimitedto5minutes),inthesupplementarymaterialwediscussinmoredetailstheeﬀectsofoverﬁtting.5.5BalancedaccuracySeveralofthedatasetsusedinthisworkareimbalancedandsoaccuracymaynotbethebestcomparisonmetricamongthemethods.Inthissection,weuseanalternativemetriccalledBalancedAccuracy(BA),whichweighsequallytheaccuracyofpositivedatapointsandtheaccuracyofnegativedatapoints.BAisamoreappropriatemeasureforimbalanceddatasets[Brodersenetal.,2010]andisgivenby:BA=T+T++F−+T−T−+F+2=T+N1+T−N02(20)Withregardstoourformulationweanalysetheperformanceoftwodiﬀerentweightsassignmentsin(1).Herewedonotuseanydiversityconstraints-weemployF1asdeﬁnedearlierandamodiﬁedF1wherewemaximisetheθ-weightedconﬁgurationsuggestedinSection3.1.Infact,Theorem1showsthatmaximisingBAisequivalenttomaximisingtheθ-weightedfunction.Theorem1Maximisingtheθ-weightedconﬁgurationisequivalenttomaximisingbalancedaccuracy.Proof1Followingthedeﬁnitionoftheθ-weightedfunctioninSection3.1,objectivefunctionzcanbewrittenas:maxz=(cid:18)1−N1N(cid:19)T++N1NT−whereT+=PN1i=1t+i,T−=PN0j=1t−jandθ=N1N.AsN=N0+N1itfollowsthat:maxz=N0NT++N1NT−maxNz=N0T++N1T−maxcz=T+N1+T−N0whereconstantc=NN1N0>0.Ifc>0,theoptimalsolutiondoesnotdependonitsinceitisonlyascalingfactorinthesolutionvalue.Thatisvalidalsoforc=2asin(20),andthusmaximisingtheθ-weightedfunctionisequivalenttomaximisingbalancedaccuracy.9Table6:BalancedAccuracyaveragesandstandarddeviationsDatasetKF1F1(θ-weighted)BFTHC-CONHC-UWAFEAvg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.PRK400.84780.03280.85820.02890.85230.01770.86430.02860.85480.02900.83080.0110600.84670.02420.85450.01810.83640.02420.86110.03090.85520.03100.82370.0158800.84640.02900.85140.02040.84330.01690.85470.02440.86150.03310.82640.01461000.84870.03060.85570.02660.84160.01030.85990.02830.85980.03200.81540.0120BCW400.96450.00570.96680.00500.95810.00660.96240.00400.96080.00560.95660.0077600.96660.00520.96750.00370.96080.00640.96380.00470.96200.00410.95640.0073800.96460.00540.96510.00440.96010.00590.96550.00530.96470.00590.95620.00691000.96250.00500.96310.00560.96160.00590.96580.00520.96420.00550.95620.0073MSK400.91030.01020.91040.00830.90710.01330.92160.00900.91670.01290.88440.0108600.91080.00800.90940.00840.90470.01430.92070.01030.91530.01240.88150.0102800.91790.01000.91640.00750.91210.01240.92590.00950.91760.01010.88410.01051000.91800.00710.91620.00580.90560.01430.92680.00830.91840.00820.88330.0102QSR400.85740.00660.86450.00490.84960.00520.85330.00630.85050.00440.84320.0037600.86040.00720.86390.00520.84670.00510.85330.01010.85030.00570.84070.0040800.85590.00460.86490.00380.85150.00500.85320.00720.85300.00590.84210.00251000.85630.00500.86480.00520.84770.00490.85300.00480.85210.00630.84190.0030DRD400.74070.00560.74340.00620.74650.00780.74430.00800.74990.00730.71170.0079600.74630.01150.74370.00880.75200.00980.75080.00890.75310.00680.71450.0075800.74570.00640.74680.00460.75030.00570.75250.00650.75250.00720.71260.00771000.74040.00880.74660.00930.74790.00630.75060.00750.75510.00570.71480.0075SPA400.94930.00180.95130.00230.94780.00120.94900.00150.94780.00150.94150.0013600.94980.00160.95140.00200.94890.00180.95030.00150.94910.00150.94050.0008800.95040.00210.95210.00180.94880.00170.95120.00170.94930.00200.94030.00091000.95000.00220.95170.00250.94850.00200.95150.00150.94930.00180.94020.0008DEF400.66330.00220.69490.00230.65070.00110.65430.00240.65590.00090.65070.0011600.66430.00200.69730.00210.64910.00180.65410.00170.65560.00090.64900.0012800.66620.00140.69850.00160.64840.00200.65530.00090.65570.00110.64840.00111000.66610.00190.69920.00140.64820.00180.65420.00200.65600.00070.64730.0012BMK400.77310.00550.86070.00150.73310.00380.74530.00270.74620.00250.69930.0039600.77630.00490.86620.00190.73380.00280.74770.00290.74690.00190.68830.0038800.77650.00550.86840.00150.73220.00460.74770.00280.74690.00180.68220.00361000.77940.00530.86940.00120.73630.00400.74780.00290.74790.00180.67620.0031APS400.86900.00620.93820.00400.84040.00570.85320.00510.85130.00460.81550.0037600.87150.00620.93860.00370.83920.00470.85230.00520.85060.00400.80150.0033800.87310.00390.93950.00380.83980.00450.85350.00330.85000.00400.80210.00371000.87350.00530.94160.00410.84470.00640.85620.00400.85130.00320.80060.0032Average:0.84330.00800.86650.00630.83130.00690.83970.00750.83830.00760.81110.0057Tables6and7,whichhavethesamestructureasTables4and5,showtheresults.WedidnotreruntheexperimentsfortheaccuracyversionofF1norforthebenchmarks,ratherweusedthesameensemblesubsetstocalculatethecorrespondingbalancedaccuracies.AsopposedtotheresultsshowninSection5.4,hereourformulationwithbothobjectivefunctionsoutperformedthebenchmarks.Moreover,usingtheθ-weightedfunctionasobjectiveresultedinconsistentoutperformanceoverF1andallthebenchmarks,withbetteroverallaverageaccuracy,lowerdispersionandbetterranks,especiallyforthelarger(andmoreimbalanced)datasets.F1(θ-weighted)outperformedallbenchmarksin22outof36cases.WealsonotethatithadworseperformancethanthebenchmarksinMSKandDRD,whicharethemostbalanceddatasets.Theseresultssuggestthatbeingabletoconﬁguretheobjectivefunctionaccordingtothecharacteristicsofthedatasetathandcanbehighlybeneﬁcial.6ConclusionsandfuturedirectionsInthisworkweproposedanIPapproachfortheproblemofselectingasubsetofclassiﬁersinensemblelearning.Theobjectiveistomaximiseaweightedfunctionofthepatternsintheconfusionmatrix.Inordertocombineperformanceanddiversitycriteria,wealsoproposedlinearconstraintstoenforceminimumdiversitylevels.We10Table7:AverageranksofbalancedaccuraciesKF1F1BFTHC-CONHC-UWAFE(θ-weighted)402.982.373.943.333.434.95602.882.453.973.293.405.02802.922.393.973.253.385.091002.972.414.003.203.365.06Avg:2.942.403.973.273.395.03observedthatstate-of-the-artsolverscanﬁndgoodsolutionsinreasonablecomputationaltimesforthechosenpubliclyavailabledatasets.TheIPapproachis,inourview,abletoprovideaﬂexibleexactalgorithm(withregardstoboththechoiceofperformancemetricanddesireddiversitylevels)whichcanalsobeusedasaheuristicifshortcomputationaltimelimitsarerequired.Thisapproachhastheadditionaladvantageofprovidingboundsonoptimalvalues.Wecomparedourformulationtosevenwell-knownbenchmarks.Weusedastratiﬁed10-foldcrossvalidationprocedureandevaluatedtheeﬀectofenforcingminimumdiversitylevelsandvaryingtheweightsassignmentsoftheobjectivefunction.Inourviewtheresultssuggestthatourapproachiscompetitiveanditsﬂexibilitycanbebeneﬁcialwhenadaptingtodatasetswithdiﬀerentcharacteristics.Tohelpfutureresearchallthedatarequiredtoreproduceourresultsismadeavailableassupplementarymaterial.Asfutureworkweintendtoexperimentwithdiﬀerentcriteriaandlargerdatasets.WealsointendtoresearchIPtechniquesandformulation-basedheuristicsforbothﬁndinggoodsolutionsquicklyandsolvingtheformulationfaster.Otherlinesofresearchincludethestudyofalternativelineardiversityconstraintsbasedondiﬀerentcriteria.ReferencesRobertE.Banﬁeld,LawrenceO.Hall,KevinW.Bowyer,andW.PhilipKegelmeyer.Ensemblediversitymeasuresandtheirapplicationtothinning.InformationFusion,6(1):49–62,32005.ISSN15662535.doi:10.1016/j.inﬀus.2004.04.005.UrveshBhowan,MarkJohnston,MengjieZhang,andXinYao.Evolvingdiverseensemblesusinggeneticprogram-mingforclassiﬁcationwithunbalanceddata.IEEETransactionsonEvolutionaryComputation,17(3):368–386,2012.doi:{10.1109/TEVC.2012.2199119}.L.Breiman.Baggingpredictors.MachineLearning,24(2):123–140,1996.doi:{10.1007/BF00058655}.L.Breiman.Randomforests.MachineLearning,45(1):5–32,2001.doi:{10.1017/CBO9781107415324.004}.A.S.Britto,R.Sabourin,andL.E.S.Oliveira.Dynamicselectionofclassiﬁers-acomprehensivereview.PatternRecognition,47(11):3665–3680,2014.doi:{10.1016/j.patcog.2014.05.003}.KayHenningBrodersen,ChengSoonOng,KlaasEnnoStephan,andJoachimMBuhmann.Thebalancedaccuracyanditsposteriordistribution.In201020thinternationalconferenceonpatternrecognition,pages3121–3124.IEEE,2010.doi:{10.1109/ICPR.2010.764}.RichCaruana,AlexandruNiculescu-Mizil,GeoﬀCrew,andAlexKsikes.Ensembleselectionfromlibrariesofmodels.InProceedingsofthetwenty-ﬁrstinternationalconferenceonMachinelearning,page18.ACM,2004.doi:{10.1145/1015330.1015432}.A.ChandraandX.Yao.Ensemblelearningusingmulti-objectiveevolutionaryalgorithms.JournalofMathematicalModellingandAlgorithms,5(1):417–445,2006.doi:{10.1007/s10852-005-9020-3}.T.ChenandC.Guestrin.Xgboost:Ascalabletreeboostingsystem.InProceedingsofthe22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,pages785–794.ACM,2016.doi:{10.1145/2939672.2939785}.11J.Cohen.Acoeﬃcientofagreementfornominalscales.Educationalandpsychologicalmeasurement,20(1):37–46,1960.CPLEXOptimizer.IBM.Availablefromhttps://www.cplex.com,lastaccessedApril1st,2021.R.M.O.Cruz,R.Sabourin,andG.D.C.Cavalcanti.Dynamicclassiﬁerselection:Recentadvancesandperspec-tives.InformationFusion,41(May):195–216,2018.doi:{10.1016/j.inﬀus.2017.09.010}.Q.DaiandX.Han.Aneﬃcientordering-basedensemblepruningalgorithmviadynamicprogramming.AppliedIntelligence,44(4):816–830,2016.doi:{10.1007/s10489-015-0729-z}.JanezDemˇsar.Statisticalcomparisonsofclassiﬁersovermultipledatasets.TheJournalofMachineLearningResearch,7:1–30,2006.doi:{10.5555/1248547.1248548}.R.P.W.Duin.Thecombiningclassiﬁer:totrainornottotrain?Objectrecognitionsupportedbyuserinteractionforservicerobots,2(c):765–770,2002.doi:{10.1109/ICPR.2002.1048415}.WeiFan,FangChu,HaixunWang,andPhilipS.Yu.Pruninganddynamicschedulingofcost-sensitiveensembles.InAAAI/IAAI,pages146–151,2002.EverlandioRQFernandes,Andr´eCPLFdeCarvalho,andAndr´eLVCoelho.Anevolutionarysamplingapproachforclassiﬁcationwithimbalanceddata.InIJCNN’15.InternationalJointConferenceonNeuralNetworks.,pages1–7.IEEE,2015.doi:{10.1109/IJCNN.2015.7280760}.Y.FreundandR.E.Schapire.Adecision-theoreticgeneralizationofon-linelearningandanapplicationtoboosting.JournalofComputerandSystemSciences,55(1):119–139,1997.doi:{10.1006/jcss.1997.1504}.J.H.FriedmanandW.Stuetzle.Projectionpursuitregression.JournaloftheAmericanstatisticalAssociation,76(376):817–823,1981.doi:{10.1080/01621459.1981.10477729}.G.Giacinto,F.Roli,andG.Fumera.Designofeﬀectivemultipleclassiﬁersystemsbyclusteringofclassiﬁers.InProceedings15thInternationalConferenceonPatternRecognition.ICPR-2000,volume2,pages160–163.IEEE,2000.doi:{10.1109/ICPR.2000.906039}.R.M.Karp.ReducibilityamongCombinatorialProblems,pages85–103.SpringerUS,Boston,MA,1972.doi:{10.1007/978-1-4684-2001-2\9}.J.Kittler,M.Hatef,R.P.W.Duin,andJ.Matas.Oncombiningclassiﬁers.IEEEtransactionsonpatternanalysisandmachineintelligence,20(3):226–239,1998.doi:{10.1109/ICPR.1996.547205}.AleksandarLazarevicandZoranObradovic.Eﬀectivepruningofneuralnetworkclassiﬁerensembles.InIJCNN’01.InternationalJointConferenceonNeuralNetworks.,volume2,pages796–801.IEEE,2001.doi:{10.1109/IJCNN.2001.939461}.M.Lichman.UCIMachineLearningRepository.Availablefromhttp://archive.ics.uci.edu/ml,lastaccessedApril1st,2021.Z.Lu,X.Wu,X.Zhu,andJ.Bongard.Ensemblepruningviaindividualcontributionordering.InProceedingsofthe16thACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining,pages871–880.ACM,2010.doi:{10.1145/1835804.1835914}.D.D.MargineantuandT.G.Dietterich.Pruningadaptiveboosting.InProceedingsofthe14thInternationalConferenceofMachineLearning,volume97,pages211–218.Citeseer,1997.doi:{10.1007/11875581\39}.G.Mart´ınez-Mu˜nozandA.Su´arez.Pruninginorderedbaggingensembles.InProceedingsofthe23rdInternationalConferenceonMachineLearning,pages609–616.ACM,2006.doi:{10.1145/1143844.1143921}.G.Mart´ınez-Mu˜nozandA.Su´arez.Usingboostingtoprunebaggingensembles.PatternRecognitionLetters,28(1):156–165,2007.doi:{10.1016/j.patrec.2006.06.018}.GonzaloMartınez-Mu˜nozandAlbertoSu´arez.Aggregationorderinginbagging.InProc.oftheIASTEDInternationalConferenceonArtiﬁcialIntelligenceandApplications,pages258–263.Citeseer,2004.doi:{10.1.1.146.3650}.12IoannisPartalas,GrigoriosTsoumakas,IoannisKatakis,andIoannisVlahavas.Ensemblepruningusingrein-forcementlearning.InHellenicConferenceonArtiﬁcialIntelligence,pages301–310.Springer,2006.doi:{10.1007/11752912\31}.IoannisPartalas,GrigoriosTsoumakas,andIoannisVlahavas.Focusedensembleselection:Adiversity-basedmethodforgreedyensembleselection.FrontiersinArtiﬁcialIntelligenceandApplications,178:117–121,2008.doi:{10.3233/978-1-58603-891-5-117}.IoannisPartalas,GrigoriosTsoumakas,andIoannisVlahavas.Anensembleuncertaintyawaremeasurefordirectedhillclimbingensemblepruning.MachineLearning,81(3):257–282,2010.doi:{10.1007/s10994-010-5172-0}.FuQiang,HuShang-Xu,andZhaoSheng-Ying.Clustering-basedselectiveneuralnetworkensemble.JournalofZhejiangUniversity-ScienceA,6(5):387–392,2005.doi:{10.1631/jzus.2005.A0387}.EKeTang,PonnuthuraiNSuganthan,andXinYao.Ananalysisofdiversitymeasures.Machinelearning,65(1):247–271,2006.doi:{10.1007/s10994-006-9449-2}.G.Tsoumakas,I.Katakis,andI.Vlahavas.Eﬀectivevotingofheterogeneousclassiﬁers.InEuropeanConferenceonMachineLearning,pages465–476.Springer,2004.doi:{10.2147/JPR.S129139}.L.Xu,B.Li,andE.Chen.Ensemblepruningviaconstrainedeigen-optimization.In2012IEEE12thInternationalConferenceonDataMining,pages715–724.IEEE,2012.doi:{10.1109/ICDM.2012.97}.Y.Zhang,S.Burer,andW.N.Street.Ensemblepruningviasemi-deﬁniteprogramming.JournalofMachineLearningResearch,7(Jul):1315–1338,2006.doi:{10.1016/j.jasms.2006.06.007}.13EnsemblepruningviaanintegerprogrammingapproachwithdiversityconstraintsSupplementarymaterialInthissupplementarymaterialweincludethreesections.InSection1,weprovidethefullcomputationalresults,wherewecompareourformulationtosevenbenchmarksinsteadoffour,asreportedinthemainpaper.InSection2,wedescribealltheclassiﬁersusedtogeneratetheAandBmatricesdescribedinthemainpaperandusedasinputinbothourformulationandthebenchmarks.Section3providesalinkwherealltestinstancescanbedownloaded.Theirformatisalsoexplained.1FullcomputationalresultsInSection5.2ofthemainpaper,wementionthatourproposedformulationiscomparedtosevenotherapproaches.Inthepaper,however,weincludeonlyfourofthemduetospaceconstraints.Inthissectionweincludethefullresultswithallsevenbenchmarks.InsummarythebenchmarksareFullEnsemble(FE),Backﬁtting(BFT)[FriedmanandStuetzle,1981],Kappa-Pruning(KP)andfourhillclimbingmethodswithdiﬀerentﬁtnessfunctions:Accuracy(HC-ACC),Complementariness(HC-COM)[Martınez-Mu˜nozandSu´arez,2004],Concurrency(HC-CON)[Banﬁeldetal.,2005]andUncertaintyWeightedAccuracy(HC-UWA)[Partalasetal.,2010].1.1AccuracyTheﬁrstsetofresultsistheonemaximisingaccuracy,inwhichweemployedtheformulationwithoutandwithdiversityconstraints,asexplainedinthepaper.InsummaryweemployF1,withoutanydiversityconstraints,F2,withdiversityconstraintsontheminimumaveragePFCvalueandF3,whichenforcesbothminimumaverageandminimumindividualPFCvalues.ThefullsetofresultsinshowninTable1andtheaverageranksareshowninTable2.Theresultsareconsistentwiththediscussionpresentinthemainpaper,inwhichbothourformulationsandthebenchmarkshadverysimilarperformance,withhillclimbingbasedmethodshavingbetteroverallperformance,althoughbyarelativelysmallmargin.AmongthebenchmarksKPhadtheworstperformance,perhapsduetobeingtheonlymethodthatoptimisesadiversitymeasureinsteadofaperformance-basedmeasureoracombinationofboth.Theuseofintegerprogramming,whichattemptstoﬁndoptimalsolutions,mayoverﬁttheselectedensembletothedatasetusedintheoptimisation.Tohaveanoverallideaofoverﬁtting,wecomparetheaverageaccuracyinout-of-sampletestdatafromTable1withtheaccuracyofthesameselectedclassiﬁers,butinthein-samplevalidationdatausedintheoptimisationapproach.ThecomparisonisshowninTable3.IntheAvg.column,weperformthiscomparisonbycalculatingthe(averageout-of-sampleaccuracy-averagein-sampleaccuracy)/(averagein-sampleaccuracy).Astheresultsareaveragesof100executions,wealsoincludethestandarddeviationincolumnStd..ThelastrowincludestheaverageofalldatasetsandvaluesofKfromitsrespectivecolumn.Themorenegativethevaluesare,themoretheaccuracyoftheout-of-sampledecreasedrelatedtothein-sample,whichmaybeasignofoverﬁttingtothein-sampledata.Fromthetablewecanseethattheoverﬁttingproblemaﬀectedtheformulationsmorestronglythanthebench-marks.Weconsiderthisasexpectedbehavioursincetheformulationissolvedbyseekingoptimality,whiletheotheralgorithmsoﬀernooptimalityguarantees.Assuch,thein-sampleaccuraciesoftheformulationstendtobehigherthanthebenchmarks,butdeterioratemoreout-of-sample.Wenoticehoweverthatincludingdiversityconstraintshelpedreducethisdeterioration,andoveralltheformulationswerestillcompetitiveout-of-sample,albeitslightlyoutperformedbythehillclimbingmethods.Thismayindicatethatwithbetteroverﬁttingcontrolthroughfurtherstudiesonhowtoemploydiversitywemaybeabletoimprovetheout-of-sampleresults.1arXiv:2205.01088v1  [cs.LG]  2 May 2022Table1:Out-of-sampleaverageaccurariesandstandarddeviations,allbenchmarks.DatasetKF1F2F3KPBFTHC-ACCHC-CONHC-COMHC-UWAFEAvg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.PRK400.90660.01450.91340.01360.91140.01300.89070.01490.90410.01560.90730.01500.91180.01520.91130.01670.90980.01830.90140.0066600.90360.01240.90890.01030.90840.01130.88020.00860.89990.01460.90590.01830.90830.01760.90890.01750.90890.01950.89940.0088800.90510.01440.90520.01240.90560.01310.88370.01260.90150.00840.90680.01950.90480.01670.90480.01800.91290.02230.90040.00911000.90360.01540.90700.02020.90420.01830.88030.00910.90200.00800.90630.02020.90890.01740.90580.01780.91200.02130.89590.0092BCW400.96640.00510.96890.00520.96900.00540.96080.00670.96520.00580.96640.00220.96800.00320.96690.00550.96710.00500.96320.0058600.96760.00430.97010.00360.97030.00380.96180.00530.96710.00560.96830.00490.96960.00450.96890.00570.96800.00400.96290.0056800.96430.00570.96730.00320.96730.00320.96290.00290.96600.00490.96970.00600.97100.00480.96960.00470.97010.00490.96290.00511000.96180.00520.96760.00360.96760.00400.96360.00390.96730.00470.96870.00610.97100.00520.96850.00430.96920.00490.96270.0056MSK400.91090.01100.91650.00930.91270.00900.89230.00880.91170.01220.92010.00980.92380.00920.92220.00880.91920.01260.88960.0100600.91190.00860.91290.00920.91150.00910.88810.01280.90830.01360.91920.01180.92300.01030.92070.00720.91770.01140.88690.0097800.91920.00970.92080.00780.92080.00760.89460.01400.91630.01170.92040.01230.92910.00900.91940.00590.92070.00960.88940.00991000.91960.00660.91920.00750.91880.00850.89060.01220.90980.01350.92110.01270.92990.00810.92130.00950.92150.00770.88850.0096QSR400.87400.00410.87310.00380.87420.00470.87340.00440.87190.00460.87340.00380.87340.00490.87230.00400.87330.00400.87060.0033600.87550.00560.87560.00450.87400.00550.87080.00480.87100.00500.87250.00560.87360.00770.87200.00500.87250.00510.86920.0039800.87150.00470.87280.00470.87400.00330.87140.00380.87470.00450.87390.00500.87310.00620.87030.00680.87470.00550.87060.00251000.87220.00540.87570.00270.87320.00420.87390.00530.87290.00500.87370.00590.87240.00400.87040.00480.87380.00600.87050.0028DRD400.74010.00600.74050.00910.73890.00750.73130.00850.74150.00800.74480.00830.74180.00770.74560.00840.74670.00700.70860.0080600.74580.01140.74620.00960.74480.01040.73680.00930.74750.01000.74380.00900.74800.00860.74500.00780.74970.00680.71100.0077800.74490.00630.74120.00540.74370.00680.71340.01080.74640.00600.74670.00800.75010.00650.74700.00760.74920.00700.70940.00781000.74010.00830.74590.00540.74420.00650.71440.01390.74410.00650.74730.00530.74860.00790.74590.00570.75160.00560.71140.0076SPA400.95290.00140.95340.00130.95340.00130.94550.00110.95290.00080.95310.00120.95350.00120.95200.00210.95230.00110.94760.0010600.95330.00130.95420.00150.95370.00170.94470.00110.95350.00140.95330.00160.95470.00140.95290.00140.95340.00120.94690.0007800.95380.00180.95360.00200.95360.00140.94500.00100.95320.00140.95430.00180.95540.00160.95290.00180.95360.00180.94680.00081000.95350.00180.95380.00160.95370.00160.94460.00110.95300.00140.95380.00200.95570.00120.95260.00130.95370.00150.94670.0006DEF400.82060.00050.82050.00050.82040.00050.82060.00050.82060.00040.82080.00040.82050.00050.82050.00050.82110.00040.82070.0004600.82050.00050.82040.00050.82060.00030.82090.00030.82070.00030.82090.00030.82040.00040.82060.00040.82110.00040.82050.0006800.82030.00060.82060.00060.82060.00080.82010.00080.82030.00040.82060.00040.82060.00040.82060.00030.82120.00030.82030.00041000.82020.00040.82000.00030.82050.00060.81980.00100.82060.00040.82060.00040.82050.00040.82050.00030.82130.00030.82010.0004BMK400.91780.00040.91710.00060.91700.00060.91320.00050.91660.00060.91700.00050.91680.00070.91690.00080.91730.00050.91430.0007600.91750.00050.91730.00060.91730.00060.91170.00040.91670.00030.91720.00060.91700.00070.91700.00070.91730.00050.91340.0007800.91740.00070.91740.00070.91720.00050.91090.00050.91650.00060.91690.00060.91690.00090.91700.00090.91730.00060.91310.00051000.91760.00040.91740.00050.91730.00070.91010.00040.91670.00040.91700.00060.91680.00070.91690.00080.91730.00080.91240.0006APS400.99370.00020.99380.00020.99380.00020.99330.00020.99360.00020.99380.00010.99380.00010.99360.00020.99380.00020.99290.0001600.99380.00020.99380.00020.99380.00020.99290.00020.99360.00020.99370.00010.99370.00010.99360.00020.99380.00010.99250.0001800.99390.00010.99380.00010.99380.00010.99250.00010.99360.00020.99380.00020.99380.00010.99360.00020.99370.00020.99250.00011000.99380.00010.99390.00010.99390.00010.99240.00010.99370.00020.99380.00020.99380.00010.99360.00020.99380.00010.99250.0001Average:0.89850.00490.89970.00450.89930.00460.88930.00510.89790.00490.89990.00560.90120.00520.89980.00510.90090.00550.88940.00412Table2:Averageranksofaccuracies,allbenchmarksKF1F2F3KPBFTHC-ACCHC-CONHC-COMHC-UWAFE405.165.045.116.945.444.985.015.244.917.17605.114.935.027.155.415.054.885.224.877.35805.095.044.997.555.444.974.665.254.737.291005.204.934.947.565.414.974.705.244.687.37Avg:5.144.985.027.305.424.994.815.244.807.291.2BalancedaccuracyThesecondsetofresultsusesbalancedaccuracy,whichmaybemoreappropriateforimbalanceddatasets,asoptimisationmetric.Weemploytwodiﬀerentconﬁgurationsofourobjectivefunction,F1andF1withtheθ-weightedfunction.Asshowninthepapermaximisingtheθ-weightedfunctionisequivalenttomaximisingbalancedaccuracy.Nodiversityconstraintsareemployedintheseresults.ThefullsetofresultsisshowninTable4andtheaverageranksareshowninTable5.Thediscussiononthebalancedaccuracyresultspresentedinthemainpaperarealsovalidhere.F1,whichmaximisesaccuracy,outperformedallbenchmarks.TheresultswereevenfurtherimprovedwhenconsideringF1withtheθ-weightedfunction,whichwasconsistentlybetter,especiallyforBMKandAPSdatasets,whichareboththelargestandthemostimbalanced.2DescriptionoftheclassiﬁersAsmentionedinthemaintextofthepaper,weprepared10diﬀerentheterogeneousclassiﬁermodels,butuseduptoK=100diﬀerentclassiﬁersforcreatingensembles.Eachofthe10modelswasinitiated10timeswithdiﬀerentrandomseedsandparameters.Itisworthremindingthat,asmentionedinthepaper,weusedsubsetsofallclassiﬁers,varyingtheirsizesasinK={40,60,80,100}.WesetKasmultiplesof10inordertohaveanequalnumberofinstantiationsofeachclassiﬁermodel.Foreachmodel,theirinitialisationdiﬀersintwoways:parametersettingsandrandomseedsrequiredbythealgorithms.The10classiﬁermodelsareshownbelow.Weincludethefollowinginformation:ModelPackage,PackageClass(sklearnpackagemodels),GitHub,documentation,version,alistofparameterswevariedandifnormalisation(whenapplicable).1.XGBoost•Package:xgboost•GitHub:https://github.com/dmlc/xgboost•Documentation:https://xgboost.readthedocs.io/en/latest/•Usedversion:0.80•Usedparameters:seed,objective,booster,numboostround,eta,maxdepth,subsample,colsamplebytree,colsamplebylevel,colsamplebynode,gamma,minchildweight2.LightGBM•Package:lightgbm•GitHub:https://github.com/microsoft/LightGBM•Documentation:https://lightgbm.readthedocs.io/en/latest/•Usedversion:2.2.0•Usedparameters:datarandomseed,featurefractionseed,baggingseed,objective,boostingtype,numboostround,learningrate,maxdepth,numleaves,featurefraction,baggingfraction,baggingfreq,mindatainleaf,maxcattoonehot,lambdal1,lambdal23.Catboost3Table3:Relativediﬀerencebetweenaveragein-sampleandaverageout-of-sampleaccuraciesDatasetKF1F2F3KPBFTHC-ACCHC-CONHC-COMHC-UWAAvg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.PRK40-0.05830.0223-0.05120.0165-0.05340.0173-0.01860.0240-0.05290.0257-0.05220.0194-0.04910.0237-0.04750.0214-0.04740.023960-0.06580.0179-0.06040.0143-0.06090.0170-0.02680.0265-0.05980.0263-0.05860.0238-0.05580.0213-0.05370.0233-0.05200.027380-0.06670.0242-0.06660.0220-0.06610.0214-0.02480.0272-0.06110.0178-0.05920.0249-0.06240.0204-0.05980.0239-0.05110.0287100-0.06990.0237-0.06630.0283-0.06930.0270-0.02330.0278-0.05750.0166-0.06070.0249-0.05840.0225-0.05930.0252-0.05250.0281BCW40-0.02440.0057-0.02190.0062-0.02170.0065-0.00430.0067-0.01900.0083-0.01940.0050-0.01900.0071-0.01930.0085-0.01810.008960-0.02470.0050-0.02220.0051-0.02210.0053-0.00210.0045-0.01810.0094-0.01850.0087-0.01810.0081-0.01840.0097-0.01850.007680-0.02910.0068-0.02610.0046-0.02610.0046-0.00410.0051-0.01960.0074-0.01790.0090-0.01710.0082-0.01860.0083-0.01690.0086100-0.03230.0063-0.02640.0044-0.02640.0048-0.00410.0065-0.01880.0070-0.01950.0080-0.01800.0081-0.02070.0058-0.01860.0085MSK40-0.05030.0149-0.04450.0116-0.04840.01210.00450.0212-0.03440.0142-0.03380.0133-0.02740.0138-0.02700.0141-0.02680.018060-0.05180.0156-0.05080.0131-0.05230.0142-0.00030.0225-0.03840.0167-0.03650.0165-0.02900.0155-0.03020.0122-0.03010.017180-0.05220.0152-0.05050.0131-0.05050.0131-0.00270.0236-0.03520.0184-0.04110.0166-0.03050.0099-0.03810.0108-0.03380.0157100-0.05290.0108-0.05350.0108-0.05390.0138-0.00380.0217-0.04040.0187-0.04100.0173-0.02960.0100-0.03600.0151-0.03420.0135QSR40-0.03810.0066-0.03900.0074-0.03770.0089-0.00370.0103-0.03160.0108-0.02940.0067-0.02530.0086-0.02320.0102-0.02520.010260-0.04130.0088-0.04120.0076-0.04290.0079-0.00240.0090-0.03410.0107-0.03350.0095-0.02730.0097-0.02530.0086-0.02880.008180-0.04990.0100-0.04850.0108-0.04710.0091-0.00380.0103-0.03320.0078-0.03490.0067-0.03060.0115-0.02960.0117-0.02880.0099100-0.05190.0088-0.04810.0071-0.05080.0094-0.00010.0118-0.03560.0076-0.03820.0094-0.03340.0099-0.03060.0100-0.03240.0084DRD40-0.07300.0094-0.07190.0137-0.07420.0131-0.00830.0169-0.05160.0124-0.05060.0128-0.04580.0081-0.03470.0106-0.04070.007960-0.07860.0153-0.07660.0122-0.07860.0124-0.00580.0145-0.05560.0130-0.06000.0122-0.04310.0132-0.04020.0086-0.04270.009280-0.08580.0118-0.08660.0081-0.08070.0103-0.01030.0144-0.06380.0087-0.06180.0100-0.04690.0075-0.04100.0124-0.04880.0072100-0.09830.0114-0.08440.0103-0.08030.0116-0.01180.0190-0.07140.0094-0.06500.0069-0.05240.0104-0.04550.0108-0.04740.0105SPA40-0.01180.0030-0.01120.0023-0.01120.0024-0.00240.0019-0.00920.0024-0.00930.0028-0.00720.0021-0.00740.0023-0.00760.002760-0.01300.0024-0.01210.0025-0.01260.0032-0.00170.0022-0.01040.0026-0.01070.0018-0.00720.0024-0.00750.0022-0.00770.002280-0.01360.0024-0.01370.0032-0.01370.0024-0.00160.0023-0.01140.0015-0.01040.0029-0.00700.0026-0.00770.0027-0.00840.0023100-0.01460.0025-0.01420.0019-0.01430.0020-0.00170.0020-0.01200.0019-0.01140.0030-0.00700.0023-0.00800.0026-0.00880.0020DEF40-0.00460.0016-0.00450.0012-0.00450.0014-0.00000.0009-0.00350.0014-0.00340.0014-0.00200.0010-0.00190.0012-0.00230.001260-0.00520.0016-0.00490.0015-0.00450.00140.00040.0011-0.00390.0010-0.00370.0014-0.00230.0012-0.00200.0014-0.00260.001480-0.00540.0014-0.00470.0019-0.00460.00120.00020.0014-0.00470.0010-0.00440.0012-0.00210.0012-0.00200.0012-0.00270.0013100-0.00570.0015-0.00510.0016-0.00420.00170.00040.0013-0.00450.0013-0.00470.0011-0.00240.0012-0.00230.0011-0.00280.0011BMK40-0.00320.0009-0.00350.0010-0.00360.0012-0.00010.0011-0.00320.0011-0.00310.0010-0.00180.0013-0.00160.0013-0.00220.001160-0.00370.0009-0.00350.0010-0.00340.0012-0.00000.0011-0.00350.0010-0.00320.0011-0.00160.0012-0.00150.0013-0.00240.001080-0.00410.0013-0.00340.0009-0.00340.0011-0.00000.0010-0.00420.0012-0.00390.0012-0.00190.0013-0.00160.0012-0.00260.0009100-0.00400.0009-0.00320.0009-0.00320.00130.00010.0011-0.00430.0008-0.00400.0012-0.00210.0011-0.00190.0011-0.00290.0009APS40-0.00120.0003-0.00110.0003-0.00110.0002-0.00000.0002-0.00070.0002-0.00060.0002-0.00050.0002-0.00050.0002-0.00040.000260-0.00120.0002-0.00120.0002-0.00120.00030.00000.0002-0.00080.0002-0.00070.0002-0.00060.0001-0.00050.0002-0.00040.000280-0.00130.0002-0.00130.0002-0.00130.00010.00000.0002-0.00090.0002-0.00080.0002-0.00050.0002-0.00060.0002-0.00060.0001100-0.00140.0002-0.00130.0001-0.00130.0002-0.00000.0002-0.00090.0002-0.00090.0002-0.00060.0001-0.00060.0002-0.00060.0001Average:-0.03300.0076-0.03130.0069-0.03140.0073-0.00450.0095-0.02530.0079-0.02520.0078-0.02130.0074-0.02070.0078-0.02080.00824Table4:BalancedAccuracyaveragesandstandarddeviationsDatasetKF1F1(θ-weighted)KPBFTHC-ACCHC-CONHC-COMHC-UWAFEAvg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.Avg.Std.PRK400.84780.03280.85820.02890.80780.02070.85230.01770.85530.02730.86430.02860.86260.02820.85480.02900.83080.0110600.84670.02420.85450.01810.77600.01360.83640.02420.85440.02940.86110.03090.85950.03010.85520.03100.82370.0158800.84640.02900.85140.02040.78470.01960.84330.01690.85750.02760.85470.02440.85540.02730.86150.03310.82640.01461000.84870.03060.85570.02660.77580.01500.84160.01030.85640.02850.85990.02830.85210.02490.85980.03200.81540.0120BCW400.96450.00570.96680.00500.95430.00790.95810.00660.96020.00340.96240.00400.96080.00630.96080.00560.95660.0077600.96660.00520.96750.00370.95560.00650.96080.00640.96220.00550.96380.00470.96250.00590.96200.00410.95640.0073800.96460.00540.96510.00440.95640.00390.96010.00590.96430.00720.96550.00530.96360.00510.96470.00590.95620.00691000.96250.00500.96310.00560.95750.00470.96160.00590.96350.00700.96580.00520.96340.00480.96420.00550.95620.0073MSK400.91030.01020.91040.00830.88720.01030.90710.01330.91650.00940.92160.00900.91920.00950.91670.01290.88440.0108600.91080.00800.90940.00840.88340.01420.90470.01430.91540.01190.92070.01030.91800.00790.91530.01240.88150.0102800.91790.01000.91640.00750.88990.01500.91210.01240.91630.01220.92590.00950.91630.00630.91760.01010.88410.01051000.91800.00710.91620.00580.88590.01310.90560.01430.91720.01270.92680.00830.91830.00970.91840.00820.88330.0102QSR400.85740.00660.86450.00490.84890.00520.84960.00520.85410.00560.85330.00630.85060.00310.85050.00440.84320.0037600.86040.00720.86390.00520.84320.00560.84670.00510.85210.00650.85330.01010.85030.00640.85030.00570.84070.0040800.85590.00460.86490.00380.84500.00590.85150.00500.85420.00570.85320.00720.84840.00740.85300.00590.84210.00251000.85630.00500.86480.00520.84780.00670.84770.00490.85430.00450.85300.00480.84750.00530.85210.00630.84190.0030DRD400.74070.00560.74340.00620.73640.00870.74650.00780.74800.00850.74430.00800.74810.00830.74990.00730.71170.0079600.74630.01150.74370.00880.74160.00920.75200.00980.74700.00890.75080.00890.74730.00770.75310.00680.71450.0075800.74570.00640.74680.00460.71760.01050.75030.00570.74940.00770.75250.00650.74910.00780.75250.00720.71260.00771000.74040.00880.74660.00930.71830.01330.74790.00630.75010.00560.75060.00750.74780.00640.75510.00570.71480.0075SPA400.94930.00180.95130.00230.93840.00140.94780.00120.94900.00110.94900.00150.94740.00300.94780.00150.94150.0013600.94980.00160.95140.00200.93760.00130.94890.00180.94940.00200.95030.00150.94870.00190.94910.00150.94050.0008800.95040.00210.95210.00180.93780.00120.94880.00170.95070.00190.95120.00170.94860.00220.94930.00200.94030.00091000.95000.00220.95170.00250.93720.00120.94850.00200.95000.00200.95150.00150.94820.00200.94930.00180.94020.0008DEF400.66330.00220.69490.00230.65110.00160.65070.00110.65300.00070.65430.00240.65520.00170.65590.00090.65070.0011600.66430.00200.69730.00210.65100.00090.64910.00180.65250.00130.65410.00170.65530.00120.65560.00090.64900.0012800.66620.00140.69850.00160.64900.00290.64840.00200.65150.00140.65530.00090.65560.00100.65570.00110.64840.00111000.66610.00190.69920.00140.64680.00410.64820.00180.65120.00140.65420.00200.65520.00100.65600.00070.64730.0012BMK400.77310.00550.86070.00150.68620.00270.73310.00380.74230.00240.74530.00270.74760.00300.74620.00250.69930.0039600.77630.00490.86620.00190.66850.00330.73380.00280.74340.00320.74770.00290.74860.00290.74690.00190.68830.0038800.77650.00550.86840.00150.65940.00370.73220.00460.74060.00400.74770.00280.74790.00200.74690.00180.68220.00361000.77940.00530.86940.00120.65220.00330.73630.00400.74170.00350.74780.00290.74770.00300.74790.00180.67620.0031APS400.86900.00620.93820.00400.82960.00640.84040.00570.85060.00420.85320.00510.85150.00440.85130.00460.81550.0037600.87150.00620.93860.00370.81500.00700.83920.00470.84890.00270.85230.00520.85120.00450.85060.00400.80150.0033800.87310.00390.93950.00380.80090.00530.83980.00450.84980.00490.85350.00330.84930.00510.85000.00400.80210.00371000.87350.00530.94160.00410.79970.00550.84470.00640.85160.00610.85620.00400.85080.00310.85130.00320.80060.0032Average:0.84330.00800.86650.00630.80760.00730.83130.00690.83680.00770.83970.00750.83750.00720.83830.00760.81110.00575Table5:Averageranksofbalancedaccuracies,allbenchmarksKF1F1(θ-weighted)KPBFTHC-ACCHC-CONHC-COMHC-UWAFE403.943.186.775.514.774.534.634.647.03603.783.256.905.514.854.414.624.577.12803.803.167.235.454.824.274.704.507.081003.913.207.175.514.764.254.684.477.05Avg:3.863.207.025.494.804.364.654.547.07•Package:catboost•GitHub:https://github.com/catboost/catboost•Documentation:https://catboost.ai/docs/•Usedversion:0.11.1•Usedparameters:randomseed,iterations,catfeatures,learningrate,l2leafreg,onehotmaxsize,bootstraptype,Bayesian,subsample,depth,rsm4.Adaboost•Package:scikit-learn•Class:sklearn.ensemble.AdaBoostClassiﬁer•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.ensemble.AdaBoostClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,baseestimator,nestimators,learningrate5.GradientBoosting•Package:scikit-learn•Class:sklearn.ensemble.GradientBoostingClassiﬁer•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.ensemble.GradientBoostingClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,nestimators,learningrate,maxdepth,criterion,subsample,maxfeatures,minsamplessplit,minsamplesleaf6.Bagging•Package:scikit-learn•Class:sklearn.ensemble.BaggingClassiﬁer•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.ensemble.BaggingClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,baseestimator,nestimators,learningrate,maxfeatures,maxsamples7.RandomForest•Package:scikit-learn•Class:sklearn.ensemble.RandomForestClassiﬁer•GitHub:https://github.com/scikit-learn/scikit-learn6•Documentation:https://scikit-learn.org/.../sklearn.ensemble.RandomForestClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,nestimators,criterion,maxfeatures,maxdepth,minsamplessplit,minsamplesleaf8.ExtraTrees•Package:scikit-learn•Class:sklearn.ensemble.ExtraTreesClassiﬁer•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.ensemble.ExtraTreesClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,nestimators,criterion,maxfeatures,maxdepth,minsamplessplit,minsamplesleaf9.LogisticRegression•Package:scikit-learn•Class:sklearn.linearmodel.LogisticRegression•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.linearmodel.LogisticRegression.html•Usedversion:0.19.2•Usedparameters:randomstate,maxiter,solver,penalty,dual,ﬁtintercept,C,tol10.MultilayerPerceptron•Package:scikit-learn•Class:sklearn.neuralnetwork.MLPClassiﬁer.html•GitHub:https://github.com/scikit-learn/scikit-learn•Documentation:https://scikit-learn.org/.../sklearn.neuralnetwork.MLPClassiﬁer.html•Usedversion:0.19.2•Usedparameters:randomstate,maxiter,hiddenlayersizes,activation,solver,learningrate,learn-ingrateinit,momentum•Normalisation:sklearn.preprocessing.StandardScalerwithdefaultparametersAsmentionedinthepaper,weusedastratiﬁed10-Foldproceduretoevaluatetheproposedmethod.Togeneratediﬀerentfoldswevariedtherandomseedrequiredtoshuﬄethedatasets.Randomnumbergenerationisexecutedtwice(classiﬁerinitialisationanddatasetshuﬄing).Fortheshuﬄingweusedrandomseeds10,20,30,...,100.Theseedsusedintheclassiﬁersdependonthestratiﬁed10-foldseed.Forinstance,ifK=100andweusedseed10forthefolds,thentheseedsforeachofthe10classiﬁersofeachmodelwere10,11,...19.3AvailabilityofthedatasetWehavemadealltheﬁlesrequiredtoreproduceourexperimentsavailableatthefollowinglink:https://drive.google.com/open?id=13tqqyWc-7EtslfaVG_orR4STUEErN_SwAGoogledriveaccountwascreatedforuploadingthisdatasetinordertomaintainanonymity.Inthelink,thereisoneZIPﬁleforeachofthe9datasets.Thezipsforthelargerdatasetsarenaturallybigger.TheﬁlesforAPS,BMKandDEFarearound1GBinsize.EachZIPﬁlecontains100CSVﬁles.TheresultsinTables4and5inthepaperareallbasedintheseCSVﬁles.Foreachdataset,wehaveﬁlesforthe10seeds(10,20,...,100)andforthe10folds.7Eachﬁlecontains,ineachrow,thedatapointsinthevalidationandtest(independent)sets.(thetrainingsetisnotrequiredtoreproduceourexperiments).Therealclassiﬁcation(0or1)ofeachdatapointisgiven,aswellasalltheclassiﬁcationsfromeachindividualclassiﬁer.Theindividualclassiﬁcationsaregivenasnumbersbetween0and1,andneedtoberoundedtogeneratetheAandBmatrices.Weusedstandardrounding(0.5roundsto1).TheCSVﬁlesareorganisedasfollows:•Theﬁrstcolumn,datatype,indicateswhetherthatdatapointisinthevalidationset(27%ofthefulldataset)orthetest(independent)set(10%ofthefulldataset).•Thesecondcolumn,realclass,containstherealvalueofthatdatapoint(0or1).•Thethirdcolumn,index,isareferencetotheoriginaldataset,indicatingtheindexofthatdatapointintheoriginalﬁle.•Columns4to103containtheclassiﬁcationsforeachindividualclassiﬁer.Thereare100classiﬁersineachCSVﬁle.However,ifinaspeciﬁcexperimentK<100andamultipleof10,weusedonlytheﬁrstK/10columnsforeachclassiﬁermodel.Forinstance,ifK=40,weusedclassiﬁersxgboost1,...,xgboost4,lightgbm1,...,lightgbm4,...tocomposethefullensemble.TheoriginaldatasetsareavailableattheURIMachineLearningRepository,whichcanbefoundathttps://archive.ics.uci.edu/ml/index.php.ThespeciﬁcURLforeachdatasetisgivenbelow:Id.DatasetnameURLPRKParkinsonshttps://archive.ics.uci.edu/ml/datasets/parkinsonsMSKMusk(Version1)https://archive.ics.uci.edu/ml/datasets/Musk+(Version+1)BCWBreastCancerWisconsinhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)QSRQSARbiodegradationhttps://archive.ics.uci.edu/ml/datasets/QSAR+biodegradationDRDDiabeticRetinopathyDebrecenhttps://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+SetSPASpambasehttps://archive.ics.uci.edu/ml/datasets/SpambaseDEFDefaultofcreditcardclientshttps://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clientsBMKBankMarketinghttps://archive.ics.uci.edu/ml/datasets/Bank+MarketingAPSAPSFailureatScaniaTruckshttps://archive.ics.uci.edu/ml/datasets/APS+Failure+at+Scania+TrucksReferencesRobertE.Banﬁeld,LawrenceO.Hall,KevinW.Bowyer,andW.PhilipKegelmeyer.Ensemblediversitymeasuresandtheirapplicationtothinning.InformationFusion,6(1):49–62,32005.ISSN15662535.doi:10.1016/j.inﬀus.2004.04.005.J.H.FriedmanandW.Stuetzle.Projectionpursuitregression.JournaloftheAmericanstatisticalAssociation,76(376):817–823,1981.doi:{10.1080/01621459.1981.10477729}.GonzaloMartınez-Mu˜nozandAlbertoSu´arez.Aggregationorderinginbagging.InProc.oftheIASTEDInternationalConferenceonArtiﬁcialIntelligenceandApplications,pages258–263.Citeseer,2004.doi:{10.1.1.146.3650}.IoannisPartalas,GrigoriosTsoumakas,andIoannisVlahavas.Anensembleuncertaintyawaremeasurefordirectedhillclimbingensemblepruning.MachineLearning,81(3):257–282,2010.doi:{10.1007/s10994-010-5172-0}.8