0
2
0
2

g
u
A
5

]
L
M

.
t
a
t
s
[

2
v
2
9
5
8
0
.
7
0
9
1
:
v
i
X
r
a

Kernel Mode Decomposition
and programmable/interpretable regression networks

Houman Owhadi∗

Clint Scovel†

Gene Ryan Yoo‡

August 7, 2020

Abstract

Mode decomposition is a prototypical pattern recognition problem that can be
addressed from the (a priori distinct) perspectives of numerical approximation, sta-
tistical inference and deep learning. Could its analysis through these combined
perspectives be used as a Rosetta stone for deciphering mechanisms at play in deep
learning? Motivated by this question we introduce programmable and interpretable
regression networks for pattern recognition and address mode decomposition as a
prototypical problem. The programming of these networks is achieved by assem-
bling elementary modules decomposing and recomposing kernels and data. These
elementary steps are repeated across levels of abstraction and interpreted from the
equivalent perspectives of optimal recovery, game theory and Gaussian process re-
gression (GPR). The prototypical mode/kernel decomposition module produces an
approximation pw1, w2, ¨ ¨ ¨ , wmq of an element pv1, v2, . . . , vmq P V1 ˆ ¨ ¨ ¨ ˆ Vm of a
product of Hilbert subspaces pVi, } ¨ }Viq of a common Hilbert space from the ob-
servation of the sum v :“ v1 ` ¨ ¨ ¨ ` vm P V1 ` ¨ ¨ ¨ ` Vm. This approximation is
i“1 } ¨ }2
minmax optimal with respect to the relative error in the product norm
Vi
j Qjq´1v “ Erξi|
and obtained as wi “ Qip
j ξj “ vs where Qi and ξi „ N p0, Qiq
are the covariance operator and the Gaussian process deﬁned by the norm } ¨ }Vi.
The prototypical mode/kernel recomposition module performs partial sums of the
recovered modes wi and covariance operators Qi based on the alignment between
each recovered mode wi and the data v with respect to the inner product deﬁned
by S´1 with S :“
i Qi (which has a natural interpretation as model/data align-
@
D
D
@
2
S´1 “ Er
S´1s and variance decomposition in the GPR setting).
ξi, v
ment
wi, v
We illustrate the proposed framework by programming regression networks approxi-
`
i vi when the am-
mating the modes vi “ aiptqyi
plitudes ai, instantaneous phases θi and periodic waveforms yi may all be unknown
and show near machine precision recovery under regularity and separation assump-
tions on the instantaneous amplitudes ai and frequencies 9θi. The structure of some
of these networks share intriguing similarities with convolutional neural networks
while being interpretable, programmable and amenable to theoretical analysis.

of a (possibly noisy) signal

˘
θiptq

ř

ř

ř

ř

ř

m

∗Corresponding author. Caltech, MC 9-94, Pasadena, CA 91125, USA, owhadi@caltech.edu
†Caltech, MC 9-94, Pasadena, CA 91125, USA, clintscovel@gmail.com
‡Caltech, MC 253-37, Pasadena, CA 91125, USA, gyoo@caltech.edu

1

 
 
 
 
 
 
1

Introduction

The purpose of the Empirical Mode Decomposition (EMD) algorithm [51] can be loosely
expressed as solving a (usually noiseless) version of the following problem, illustrated in
Figure 1.

Problem 1. For m P N˚, let a1, . . . , am be piecewise smooth functions on r0, 1s and
let θ1, . . . , θm be strictly increasing functions on r0, 1s. Assume that m and the ai, θi
, t P
are unknown. Given the (possibly noisy) observation of vptq “
r0, 1s, recover the modes viptq :“ aiptq cos

m
i“1 aiptq cos

˘
θiptq

˘
θiptq

ř

`

`

.

Figure 1: A prototypical mode decomposition problem: given v “ v1 ` v2 ` v3 recover
v1, v2, v3.

In practical applications, generally the instantaneous frequencies ωi “ dθi

dt are as-
sumed to be smooth and well separated. Furthermore the ωi and the instantaneous
amplitudes are assumed to be varying at a slower rate than the instantaneous phases
θi so that near τ P r0, 1s the intrinsic mode function vi can be approximated by a
trigonometric function, i.e.

viptq « aipτ q cos

˘
`
ωipτ qpt ´ τ q ` θipτ q

for t « τ .

(1.1)

The diﬃculty of analyzing and generalizing the EMD approach and its popularity and
success in practical applications [50] have stimulated the design of alternative methods
aimed at solving Problem 1. Methods that are amenable to a greater degree of analysis
include synchrosqueezing [18, 60], variational mode decomposition [21] and non-linear
L1 minimization with sparse time-frequency representations [46, 47].

A Rosetta stone for deep learning? Since Problem 1 can be seen as prototypical
pattern recognition problem that can be addressed from the perspectives of numerical
approximation, statistical inference and machine learning, one may wonder if its analy-
sis, from the combined approaches of numerical approximation and statistical inference,
could be used as a Rosetta stone for deciphering deep learning. Indeed, although suc-
cessful industrial applications [58] have consolidated the recognition of artiﬁcial neural
networks (ANNs) as powerful pattern recognition tools, their utilization has recently

2

been compared to “operating on an alien technology” [53] due to the challenges brought
by a lag in theoretical understanding: (1) because ANNs are not easily interpretable the
resulting models may not be interpretable (and identifying causes of success or failure
may be challenging) (2) because ANNs rely on the resolution of non-convex (possibly
stochastic) optimization problems, they are not easily amenable to a complete uncer-
tainty quantiﬁcation analysis (3) because the architecture design of ANNs essentially
relies on trial and error, the design of architectures with good generalization properties
may involve a signiﬁcant amount of experimentation.

Since elementary operations performed by ANNs can be interpreted [77] as stack-
ing Gaussian process regression steps with nonlinear thresholding and pooling opera-
tions across levels of abstractions, it is natural to wonder whether interpretable Gaus-
sian process regression (GPR) based networks could be conceived for mode decomposi-
tion/pattern recognition. Could such networks (1) be programmable based on rational
and modular (object oriented) design? (2) be amenable to analysis and convergence
results? (3) help our understanding of fundamental mechanisms that might be at play
in pattern recognition and thereby help elaborate a rigorous theory for Deep Learning?
This paper is an attempt to address these questions, while using mode decomposition [51]
as a prototypical pattern recognition problem. As an application of the programmable
and interpretable regression networks introduced in this paper, we will also address the
following generalization of Problem 1, where the periodic waveforms may all be non-
trigonometric, distinct, and unknown and present an algorithm producing near machine
precision (10´7 to 10´4) recoveries of the modes.

Problem 2. For m P N˚, let a1, . . . , am be piecewise smooth functions on r´1, 1s, let
θ1, . . . , θm be piecewise smooth functions on r´1, 1s such that the instantaneous frequen-
cies 9θi are strictly positive and well separated, and let y1, . . . , ym be square-integrable 2π-
periodic functions. Assume that m and the ai, θi, yi are all unknown. Given the observa-
˘
.
θiptq
tion vptq “

(for t P r´1, 1s) recover the modes viptq :“ aiptqyi

˘
θiptq

ř

`

`

m
i“1 aiptqyi

One fundamental idea is that although Problems 1 and 2 are nonlinear, they can be,
to some degree, linearized by recovering the modes vi as aggregates of suﬃciently ﬁne
modes living in linear spaces (which, as suggested by the approximation (1.1), can be
chosen as linear spans of functions t Ñ cospωpt ´ τ q ` θq windowed around τ , i.e. Gabor
wavelets). The ﬁrst part of the resulting network recovers those ﬁner modes through a
linear optimal recovery operation. Its second part recovers the modes vi through a hier-
archy of (linear) aggregation steps sandwiched between (nonlinear) ancestor/descendant
identiﬁcation steps. These identiﬁcation steps are obtained by composing the align-
ments between v and the aggregates of the ﬁne modes with simple and interpretable
nonlinearities (such as thresholding, graph-cuts, etc...), as presented in Section 4.

3

Figure 2: Left: The mode decomposition problem. Right: The game theoretic approach.

2 Review of additive Gaussian process regression, empiri-

cal mode decomposition and synchrosqueezing

The kernel mode decomposition framework has relations to the ﬁelds of additive Gaussian
process regression, empirical mode decomposition and synchrosqueezing. Consequently,
here we review these subjects giving a context to our work. This section is not essential
to understanding the paper and so can be skipped on ﬁrst reading.

Although simple kriging and GPR are derived diﬀerently, they can be shown to be
equivalent and are often referred to as the same, see e.g. Yoo [112, Sec. 1.1] for a review
of kriging and its relationship with GPR. Regarding the origins of kriging, paraphrasing
Cressie [13], known for introducing kriging in spatial statistics, ”both Matheron [67]
(see also [66]) and Gandin [35] were the ﬁrst to publish a deﬁnitive development of
spatial kriging. D. G. Krige’s contributions in mining engineering were considerable but
he did not discover kriging, illustrating once again Stigler’s Law of Eponymy (Stigler
[96]), which states that ”no scientiﬁc discovery is named after its original discoverer.”
The eponymous title of Stigler’s work is playfully consistent with his law, since in it he
essentially names Merton [70, p. 356] as the discoverer of Stigler’s law.

2.1 Additive Gaussian processes

ř

ř

Following Hastie and Tibshirani [40, 39], the generalized additive model (GAM) replaces
j fjpxjq where the fj
j βjxj, where the βj are parameters, with
a linear predictor
are unspeciﬁed functions. For certain types of prediction problems such as binary target
`ř
variables, one may add a ﬁnal function h
. To incorporate fully dependent
responses we can consider models of the form f px1, . . . , xN q. Additive models have
been successfully used in regression, see Stone [97] and Fan et al. [29]. Vector valued
generalizations of GAMs have been developed in Yee and Wild [111] and Yee [110].
For vector valued additive models of large vector dimension with a large number of
dimensions in the observation data, Yee [110] develops methods for reducing the rank of
the systems used in their estimation.

j fjpxjqq

˘

4

ř

kipxi, x1

1, . . . , x1

When the underlying random variables are Gaussian and we apply to regression, we
naturally describe the model in terms of its covariance kernel kpx1, . . . , xN , x1
N q
iq, where the kernel is an additive sum of kernels
or as an additive model
depending on lower dimensional variables. It is natural to generalize this setting to a
covariance deﬁned by a weighted sum over all orders d of dependency of weighted sums
of kernels depending only on d D dimensional variables, where N “ Dd. Of course, such
general kernels are exponentially complex in the dimension N , so are not very useful.
Nearly simultaneously, Duvenaud et al. [27] and Durrande et al. [23, 24], introducing
Gaussian Additive Processes, addressed this problem. Duvenaud et al. [27] restricts the
sum at order d to be symmetric in the scalar components in the vector variables, thus
reduces this complexity in such a way that their complexity is mild and their estimation
is computationally tractable. Durrande et al. [23, 24] consider additive versions of vector
dependent kernels and product versions of them, and study their respective performance
properties along the the performance of their sum. Moreover, because of the additive
nature of these methodologies, they both achieve strong interpretability as described by
Plate [78].

2.2 Gaussian Process Regression

Williams and Rasmussen [106] provide an introduction to Gaussian Process Regression
(GPR). More generally, an excellent introduction to Gaussian processes in machine learn-
ing, along with a description of many of its applications and its history, can be found in
Rasmussen [84], and Rasmussen and Williams [85], see also Yoo [112]. Recent applica-
tion domain developments include source separation, which is related to subject of this
book, by Park and Choi [76] and Liutkus et al. [62] and the detection of periodicities by
Durrande et al. [25, 26] and Preot¸iuc-Pietro and Cohn [79].

When the number of dimensions of the observational data is large, computational
eﬃciency becomes extremely important. There has been much work in this area, the so-
called sparse methods, e.g. Tresp [103], Smola and Bartlett [92], Williams and Seeger
[105], Csat´o and Opper [15], Csat´o et al. [16], Csat´o [14], Qui˜nonero-Candela [80],
Lawrence et al. [56], Seeger [90], Seeger et al. [91], Schwaighofer and Tresp [89], Snelson
and Ghahramani [93]. Qui˜nonero-Candela and Rasmussen [81] provide a unifying frame-
work for the sparse methods based on expressing them in terms of their eﬀective prior.
The majority of these methods utilize the so-called inducing variable methods, which are
data points in the same domain as the unlabeled data. Some require these to be a subset
of the training data while others, such as Snelson and Ghahramani [93] allow them to
inferred along with the the primary hyperparameters using optimization. However, there
are notable exceptions such as Hensman et al. [42] who apply a Kullback-Liebler derived
variational formulation and utilize Bochner’s theorem on positive deﬁnite functions to
choose optimal features in Fourier space.

The majority of these methods use the Kullback-Liebler (KL) criterion to select the
induced points, See Rasmussen and Williams [85, Ch. 8] for a review.
In particular,
Seeger et al. [91], Seeger [90] among others, utilize the KL criterion to optimize both
the model hyperparameters and the inducing variables. However, they observe that the

5

approximation of the marginal likelihood is sensitive to the choice of inducing variables
and therefore convergence of the method is problematic. Snelson and Ghahramani [93]
attempt to resolve this problem by developing a KL formulation where the model hy-
perparameters and the inducing variables are jointly optimized. Nevertheless, since the
inducing variables determine an approximate marginal likelihood, these methods can
suﬀer from overﬁtting. Titsias’ [100] breakthrough, a development of Csat´o and Opper
[15] and Seeger [90], was the introduction of a KL variational framework where the model
hyperparameters and the inducing variables are selected in such a way as to maximize a
lower bound to the true marginal likelihood, and thus are selected to minimize the KL
distance between the sparse model and the true one. When the dimensions of the obser-
vational data are very large, Hensman et al. [44], utilizing recent advances in stochastic
variational inference of Hoﬀman et al. [45] and Hensman et al. [43], appear to develop
methods which scale well. Adam et al. [1] develop these results in the context Additive
GP applied to the source separation problem.

For vector Gaussian processes, one can proceed basically as in the scalar case, in-
cluding the development of sparse methods, however one needs to take care that the
vector covariance structure is positive deﬁnite (see the review by Alvarez et al. [4]) See
e.g. Yu et al. [114], Boyle and Frean [10, 9], Melkumyan and Ramos [69], Alvarez and
Lawrence [2, 3], Titsias and L´azaro-Gredilla [101]. Raissi et al. [82] develop methods to
learn linear diﬀerential equations using GPs.

2.3 Empirical Mode Decomposition (EMD)

The deﬁnition of an instantaneous frequency of a signal xptq is normally accomplished
through application of the Hilbert transform H deﬁned by the principle value of the
singular integral

ż

˘
`
Hpxq

ptq :“

1
π

P V

xpτ q
t ´ τ

dτ ,

R

which, when it is well deﬁned, determines the harmonic conjugate y :“ Hpxq of xptq of
a function

xptq ` iyptq “ aptqeiθptq

which has an analytic extension to the upper complex half plane in t, allowing the
derivative ω :“ 9θ the interpretation of an instantaneous frequency of

xptqq “ aptq cospθptqq .

However, this deﬁnition is controversial, see e.g. Boashash [7] for a review, and pos-
sesses many diﬃculties, and the Empirical Mode Decomposition (EMD) algorithm was
invented by Huang et al. [51] to circumvent them by decomposing a signal into a sum
of intrinsic mode functions (IMFs), essentially functions whose number of local extrema
and zero crossings are either equal or diﬀer by 1 and such that the mean of the envelope
of the local maxima and the local minima is 0, which are processed without diﬃculty
by the Hilbert transform. See Huang [49] for a more comprehensive discussion. This
combination of the EMD and the Hilbert transform, called the Hilbert-Huang transform,

6

is used decompose a signal into its fundamental AM-FM components. Following Rilling
et al. [87], the EMD appears as follows: Given a signal xptq

1. identify all local extrema of xptq

2. interpolate between the local minima (resp. maxima) to obtain the envelope eminptq

(resp. emaxptq)

3. compute the mean mptq :“ eminptq`emaxptq

2

4. extract the detail dptq :“ xptq ´ mptq

5. iterate on the residual mptq

The sifting process iterates steps (1) through (4) on the detail until it is close enough to
zero mean. Then the residual is computed and step (5) is applied.

Despite its remarkable success, see e.g. [12, 51, 94, 109, 11, 17, 20] and the review
on geophysical applications of Huang and Wu [52]. the original method is deﬁned by an
algorithm and therefore its performance is diﬃcult to analyze. In particular, sifting and
other iterative methods usually do not allow for backward error propagation. Despite
this, much is known about it, improvements have been made and eﬀorts are underway to
develop formulations which facilitate a performance analysis. To begin, it appears that
the EMD algorithm is sensitive to noise, so that Wu and Huang [108] introduce and study
an Ensemble EMD, further developed in Torres et al. [102], which appears to resolve
the noise problem while increasing the computational costs. On the other hand, when
applied to white noise Flandrin et al. [33, 31, 32] and Wu and Huang [107] demonstrate
that it acts as an adaptive wavelet-like ﬁlter bank, leading to Gilles’ [36] development of
empirical wavelets. Rilling and Flandrin [86] successfully analyze the performance of the
the algorithm on the sum of two cosines. Lin et al. [61] consider an alternative framework
for the empirical mode decomposition problem considering a moving average operator
instead of the mean function of the EMD. This leads to a mathematically analyzable
framework, and in some cases (such as the stationary case) to the analysis of Toeplitz
operators, a good theory with good results. This technique has been further developed
by Huang et al. [48], with some success. Approaches based on variational principles, such
as Feldman [30], utilizing an iterative variational approach using the Hilbert transform,
Hou and Shi [46], a compressed sensing approach, Daubechies et al. [18], the wavelet base
synchrosqueezing method to be discussed in a moment, and Dragomiretskiy and Zosso
[21], a generalization of the classic Wiener ﬁlter using the alternate direction method of
multipliers method, see Boyd et al.
[8], to solve the resulting bi-variate minimization
problem, appear to be good candidates for analysis. However, the variational objective
function in [46] uses higher order total variational terms so appears sensitive to noise,
[30] is an iterative variational approach, and the selection of the relevant modes in
[21] for problems with noise is currently under investigation, see e.g. Ma et al. [64]
and the references therein. On the other hand, Daubechies et al. [18] provide rigorous
performance guarantees under certain conditions. Nevertheless, there is still much eﬀort

7

in developing their work, see e.g. Auger et al. [5] for a review of synchrosqeezing and its
relationship with time-frequency reassignment.

2.4 Synchrosqueezing

Synchrosqueezing, introduced in Daubechies and Maes [19], was developed in Daubechies,
Lu and Wu [18] as an alternative to the EMD algorithm which would allow mathematical
performance analysis, and has generated much interest, see e.g. [72, 99, 98, 5, 59, 104].
Informally following [18], for a signal xptq we let

W pa, bq :“ a´ 1

2

ż

xptqψ

R

´

¯

dt

t ´ b
a

denote the wavelet transform of the signal xptq using the wavelet ψ. They demonstrate
that for a wavelet such that its Fourier transform satisﬁes ˆψpξq “ 0, ξ ă 0, when applied
to a pure tone

that

satisﬁes

xptq :“ A cospωtq

ωpa, bq :“ ´i

B ln W pa, bq
Bb

ωpa, bq “ ω ,

(2.1)

(2.2)

that is, it provides a perfect estimate of the frequency of the signal (2.1). This suggests
using (2.2) to deﬁne the map

to push the mass in the reconstruction formula

pa, bq ÞÑ pωpa, bq, bq

xpbq “ (cid:60)

”
C´1
ψ

ż

8

0

W pa, bqa´ 3

ı
2 da

,

where Cψ :“

ş

8
0

where

where

ˆψpξq
ξ dξ, to obtain the identity
ı
2 da

W pa, bqa´ 3

”
C´1
Re
ψ

ż

8

0

”
C´1
ψ

ż

R

ı

T pω, bqdω

,

(2.3)

“ (cid:60)

T pω, bq “

ż

Apbq

W pa, bqa´ 3
2 δ

`

˘

ωpa, bq ´ ω

da

(2.4)

Apbq :“ ta : W pa, bq ‰ 0u

and ωpa, bq is deﬁned as in (2.2) for pa, bq such that a P Apbq. We therefore obtain the
ż
reconstruction formula

ı

T pω, bqdω

R

(2.5)

xpbq “ (cid:60)

”
C´1
ψ

8

for the synchrosqueezed transform T .
In addition, [18, Thm. 3.3] demonstrates that
for a signal x comprised of a sum of AM-FM modes with suﬃciently separated fre-
quencies whose amplitudes are slowly varying with respect to their phases, that the
synchrosqueezed transform T pω, bq is concentrated in narrow bands ω « 9θipbq about the
instantaneous frequency of the i-th mode and restricting the integration in (2.5) to these
bands provides a good recovery of the modes.

3 The mode decomposition problem

To begin the general (abstract) formulation of the mode decomposition problem, let V
be a separable Hilbert space with inner product
and corresponding norm } ¨ }. Also
let I be a ﬁnite set of indices and let pViqiPI be linear subspaces Vi Ă V such that

@
¨, ¨

D

V “

ÿ

iPI

Vi .

(3.1)

The mode decomposition problem can be informally formulated as follows

Problem 3. Given v P V recover vi P Vi, i P I, such that v “

ř

iPI vi.

Our solution to Problem 3 will use the interface between numerical approximation,
inference and learning (as presented in [74, 75]), which although traditionally seen as
entirely separate subjects, are intimately connected through the common purpose of
making estimations with partial information [75]. Since the study of this interface has
been shown to help automate the process of discovery in numerical analysis and the
design of fast solvers [73, 74, 88], this paper is also motivated by the idea it might, in a
similar manner and to some degree, also help the process of discovery in machine learning.
Here, these interplays will be exploited to address the general formulation Problem 3
of the mode recovery problem from the three perspectives of optimal recovery, game
theory and Gaussian process regression. The corresponding minmax recovery framework
(illustrated in Figure 2 and presented below) will then be used as a building block for
the proposed programmable networks.

3.1 Optimal recovery setting

Problem 3 is ill-posed if the subspaces pViqiPI are not linearly independent, in the sense
that such a recovery will not be unique. Nevertheless, optimal solutions can be deﬁned
in the optimal recovery setting of Micchelli and Rivlin [71]. To this end, let } ¨ }B be a
quadratic norm on the product space

making B a Hilbert space, and let

ź

B “

Vi,

iPI

Φ : B Ñ V

9

(3.2)

be the information map deﬁned by

Φu :“

ÿ

iPI

ui,

u “ puiqiPI P B .

(3.3)

An optimal recovery solution mapping

Ψ : V Ñ B

for the mode decomposition problem is deﬁned as follows: for given v P V , we deﬁne
Ψpvq to be the minimizer w of

min
wPB|Φw“v

max
uPB|Φu“v

}u ´ w}B
}u}B

.

(3.4)

Lemma 3.1. Let Φ : B Ñ V be surjective. For v P V , the solution w of the convex
optimization problem

#

Minimize }w}B
Subject to w P B and Φw “ v .

(3.5)

determines the unique optimal minmax solution w “ Ψpvq to (3.4). Moreover,

Ψpvq “ Φ`v,

where the Moore-Penrose inverse Φ` : V Ñ B of Φ is deﬁned by

Φ` :“ ΦT

`

˘

´1 .

ΦΦT

Now let us be more speciﬁc about the structure of B that we will assume. Indeed,

let the subspaces pViqiPI be equipped with quadratic norms p} ¨ }ViqiPI making each

pVi, } ¨ }Viq
ś

a Hilbert space, and equip their product B “

iPI Vi with the product norm

}u}2

B :“

ÿ

iPI

}ui}2

Vi,

u “ puiqiPI P B .

(3.6)

We use the notation r¨, ¨s for the duality product between V ˚ on the left and V on the
right, and also for the duality product between V ˚
i and Vi for all i. The norm } ¨ }Vi
makes Vi into a Hilbert space if and only if

}vi}2

Vi “ rQ´1

i vi, vis,

vi P Vi,

(3.7)

for some positive symmetric linear bijection

Qi : V ˚

i Ñ Vi,

10

where by positive and symmetric we mean rφ, Qiφs ě 0 and rφ, Qiϕs “ rϕ, Qiφs for all
ϕ, φ P V ˚
to pVi, } ¨ }Viq is also a Hilbert space with
norm

i . For each i P I, the dual space V ˚
i

}φi}2

V ˚
i

:“ rφi, Qiφis,

φi P V ˚
i

,

(3.8)

and therefore the dual space B˚ of B can be identiﬁed with the product of the dual
spaces
ź

B˚ “

V ˚
i

(3.9)

with (product) duality product

ÿ

rφ, us “

rφi, uis,

iPI

iPI

φ “ pφiqiPI P B˚,

u “ puiqiPI P B .

(3.10)

Moreover the symmetric positive linear bijection

Q : B˚ Ñ B

(3.11)

deﬁning the quadratic norm } ¨ }B is the block-diagonal operator

Q :“ diagpQiqiPI

deﬁned by its action Qφ “ pQiφiqiPI, φ P B˚.

Let

ei : Vi Ñ V

be the subset inclusion and let its adjoint

e˚
i

: V ˚ Ñ V ˚
i

be deﬁned through re˚
transform the family of operators

i φ, vis “ rφ, eivis for φ P V ˚, vi P Vi. These operations naturally

into a family of operators

Qi : V ˚

i Ñ Vi,

i P I,

eiQie˚
i

: V ˚ Ñ V,

i P I,

all deﬁned on the same space, so that we can deﬁne their sum S : V ˚ Ñ V by

S “

ÿ

iPI

eiQie˚
i .

(3.12)

The following proposition demonstrates that S is invertible and that S´1 and S naturally
generate dual Hilbert space norms on V and V ˚ respectively.

11

Lemma 3.2. The operator S : V ˚ Ñ V , deﬁned in (3.12), is invertible. Moreover,

}v}2

S´1 :“ rS´1v, vs,

v P V,

deﬁnes a Hilbert space norm on V and

}φ}2

S :“ rφ, Sφs “

ÿ

iPI

}e˚

i φ}2
V ˚
i

, φ P V ˚ ,

deﬁnes a Hilbert space norm on V ˚ which is dual to that on V .

The following theorem determines the optimal recovery map Ψ.

(3.13)

(3.14)

Theorem 3.3. For v P V , the minimizer of (3.5) and therefore the minmax solution of
(3.4) is

Ψpvq “

`
Qie˚

i S´1v

˘

iPI .

(3.15)

Furthermore

where

and

`

˘

Φ

Ψpvq

“ v,

v P V,

Ψ : pV, } ¨ }S´1q Ñ pB, } ¨ }Bq

Φ˚ : pV ˚, } ¨ }Sq Ñ pB˚, } ¨ }B˚q

are isometries. In particular, writing Ψipvq :“ Qie˚

i S´1v, we have

}v}2

S´1 “ }Ψpvq}2

B “

ÿ

iPI

}Ψipvq}2
Vi

v P V .

(3.16)

Observe that the adjoint

Φ˚ : V ˚ Ñ B˚

of Φ : B Ñ V , deﬁned by rϕ, Φus “ rΦ˚pϕq, us for ϕ P V ˚ and u P B, is computed to be

Φ˚pϕq “ pe˚

i ϕqiPI,

ϕ P V ˚ .

(3.17)

The following theorem presents optimality results in terms of Φ˚.

Theorem 3.4. We have

}u ´ ΨpΦuq}2

B “ inf
φPV ˚

}u ´ QΦ˚pφq}2

B “ inf
φPV ˚

ÿ

iPI

12

}ui ´ Qie˚

i φ}2

Vi .

(3.18)

3.2 Game/decision theoretic setting

Optimal solutions to Problem 3 can also be deﬁned in the setting of the game/decision
theoretic approach to numerical approximation presented in [74].
In this setting the
minmax problem (3.4) is interpreted as an adversarial zero sum game (illustrated in
Figure 2) between two players and lifted to mixed strategies to identify a saddle point.
Let P2pBq be the set of Borel probability measures µ on B such that Eu„µ
ă 8, and
let LpV, Bq be the set of Borel measurable functions ψ : V Ñ B. Let E : P2pBqˆLpV, Bq Ñ
R be the loss function deﬁned by

}u}2
B

‰

“

Epµ, ψq “

“

Eu„µ

}u ´ ψpΦuq}2
B
Eu„µ
}u}2
B

‰

“

‰

,

µ P P2pBq, ψ P LpV, Bq .

(3.19)

Let us also recall the more general notion of a Gaussian ﬁeld as described in [74,
Chap. 17]. To that end, a Gaussian space H is a linear subspace H Ă L2pΩ, Σ, Pq
of the L2 space of a probability space consisting of centered Gaussian random vari-
ables. A centered Gaussian ﬁeld ξ on B with covariance operator Q : B˚ Ñ B, written
ξ „ N p0, Qq, is an isometry

from B˚ to a Gaussian space H, in that

ξ : B˚ Ñ H

rφ, ξs „ N

`

˘
0, rφ, Qφs

,

φ P B˚,

where we use the notation rφ, ξs to denote the action ξpφq of ξ on the element φ P B˚,
thus indicating that ξ is a weak B-valued Gaussian random variable. As discussed in
[74, Chap. 17], there is a one to one correspondence between Gaussian cylinder measures
and Gaussian ﬁelds1. Let ξ denote the Gaussian ﬁeld

ξ „ N p0, Qq

on B where Q : B˚ Ñ B is the block diagonal operator Q :“ diagpQiqiPI, and let µ: denote
the cylinder measure deﬁned by the Gaussian ﬁeld ξ ´ Erξ|Φξs, or the corresponding
Gaussian measure in ﬁnite dimensions.

We say that a tuple pµ1, ψ1q is a saddle point of the loss function E : P2pBqˆLpV, Bq Ñ

R if

Epµ, ψ1q ď Epµ1, ψ1q ď Epµ1, ψq, µ P P2pBq, ψ P LpV, Bq .

1 The cylinder sets of B consists of all sets of the form F ´1pBq where B P Rn is a Borel set and
F : B Ñ Rn is a continuous linear map, over all integers n. A cylinder measure µ, see also [74, Chap. 17],
on B, is a collection of measures µF indexed by F : B Ñ Rn over all n such that each µF is a Borel
measure on Rn and such that for F1 : B Ñ Rn1 and F2 : B Ñ Rn2 and G : Rn1 Ñ Rn2 linear and
continuous with F2 “ GF1, we have G˚µF1 “ µF2 , where G˚ is the pushforward operator on measures
corresponding to the map G, deﬁned by pG˚νqpBq :“ νpG´1Bq. When each measure µF is Gaussian,
the cylinder measure is said to be a Gaussian cylinder measure. A sequence µn of cylinder measures
such that the sequence pµnqF converges in the weak topology for each F , is said to converge in the weak
cylinder measure topology.

13

Theorem 3.5 shows that the optimal strategy of Player I is the Gaussian ﬁeld ξ ´Erξ|Φξs,
the optimal strategy of Player II is the conditional expectation

“
Ψpvq “ E

ξ

ˇ
ˇΦξ “ v

‰

,

(3.20)

and (3.20) is equal to (3.15).

Theorem 3.5. Let E be deﬁned as in (3.19). It holds true that

max
µPP2pBq

min
ψPLpV,Bq

Epµ, ψq “ min

ψPLpV,Bq

max
µPP2pBq

Epµ, ψq .

(3.21)

Furthermore,

• If dimpV q ă 8 then
(3.15) and (3.20).

`

˘
µ:, Ψ

is a saddle point for the loss (3.19), where Ψ is as in

• If dimpV q “ 8, then the loss (3.19) admits a sequence of saddle points pµn, Ψq P
P2pBq ˆ LpV, Bq where Ψ is as in (3.15) and (3.20), and the µn are Gaussian mea-
sures, with ﬁnite dimensional support, converging towards µ: in the weak cylinder
measure topology.

Proof. The proof is essentially that of [74, Thm. 18.2]

3.3 Gaussian process regression setting

Figure 3: The minmax solution of the mode decomposition problem.

Let us demonstrate that Theorem 3.5 implies that the minmax optimal solution to
Problem 3 with loss measured as the relative error in the norm (3.6) can be obtained
via Gaussian process regression. To that end, let ξi „ N p0, Qiq, i P I, be independent
Vi-valued Gaussian ﬁelds deﬁned by the norms } ¨ }Vi. Recall that Qi is deﬁned in (3.7)
and that ξi is an isometry from pV ˚
q onto a Gaussian space, mapping φ P V ˚
i
to rφ, ξis „ N p0, rφ, Qiφsq. Theorem 3.5 asserts that the minmax estimator is (3.20),

i , } ¨ }V ˚

i

14

which, written componentwise, determines the optimal reconstruction of each mode vj
of v “

ř

iPI vi to be

ÿ

‰

ξi “ v

“ Qjp

Qiq´1v .

(3.22)

ÿ

“
E

ξj

ˇ
ˇ

iPI

ř

ř

i eiQie˚

iPI
iPI Qi is a short-
where the right hand side of (3.22) is obtained from (3.15), and
hand notation for
i obtained by dropping the indications of the injections ei
and their adjoint projections e˚
i . From now on, we will use such simpliﬁed notations
whenever there is no risk of confusion. In summary, the minmax solution of the abstract
mode decomposition problem, illustrated in Figure 3, is obtained based on the speci-
ﬁcation of the operators Qi
i Ñ Vi and the injections ei : Vi Ñ V , of which the
former can be interpreted as quadratic norm deﬁning operators or as covariance opera-
tors. Table 1 illustrates the three equivalent interpretations -optimal recovery/operator
kernel/Gaussian process regression of our methodology.

: V ˚

Norm

}vi}2
Vi

:“ xQ´1

i vi, viy

#

arg min

minimize
ř
i wi “ v

ř

i }wi}2
Vi

Operator/Kernel

GP

Qi : V ˚

i Ñ Vi

`ř

˘

´1v

j Qj

Qi

ξi „ N p0, Qiq

ř

Erξi |

j ξj “ vs

Table 1: Three equivalent interpretations -optimal recovery/operator kernel/Gaussian
process regression of our methodology.

ş
t
0 ω1psq ds and θ2ptq “

Example 3.6. Consider the problem of recovering the modes v1, v2, v3, v4 from the ob-
servation of the signal v “ v1 ` v2 ` v3 ` v4 illustrated in Figure 4. In this example all
modes are deﬁned on the interval r0, 1s, v1ptq “ p1 ` 2t2q cospθ1ptqq ´ 0.5t sinpθ1ptqq,
v2ptq “ 2p1 ´ t3q cospθ2ptqq ` p´t ` 0.5t2q sinpθ2ptqq, v3ptq “ 2 ` t ´ 0.2t2, and v4
is white-noise (the instantiation of a centered GP with covariance function δps ´ tq).
ş
t
0 ω2psq ds are deﬁned by the instantaneous frequencies
θ1ptq “
ω1ptq “ 16πp1 ` tq and ω2ptq “ 30πp1 ` t2{2q.
In this recovery problem ω1ptq and
ω2ptq are known, v3 and the amplitudes of the oscillations of v1 and v2 are unknown
smooth functions of time, only the distribution of v4 is known. To deﬁne optimal re-
covery solutions one can either deﬁne the normed subspaces pVi, } ¨ }Viq or (equivalently
via (3.7)) the covariance functions/operators of the Gaussian processes ξi. In this ex-
ample it is simpler to use the latter. To deﬁne the covariance function of the GP ξ1
we assume that ξ1ptq “ ζ1,cptq cospθ1ptqq ` ζ1,sptq sinpθ1ptqq, where ζ1,c and ζ1,s are in-
dependent identically distributed centered Gaussian processes with covariance function
Erζ1,cpsqζ1,cptqs “ Erζ1,spsqζ1,sptqs “ e´ ps´tq2
γ2
(chosen with γ “ 0.2 as a prior regu-
larity assumption). Under this choice ξ1 is a centered GP with covariance function
K1ps, tq “ e´ ps´tq2

cospθ1psqq cospθ1ptqq ` sinpθ1psqq sinpθ1ptqq

. Note that the cosine

γ2

˘

`

15

Figure 4: (1) The signal v “ v1 ` v2 ` v3 ` v4 (2) The modes v1, v2, v3, v4 (3) v1 and its
approximation w1 (4) v2 and its approximation w2 (5) v3 and its approximation w3 (6)
v4 and its approximation w4.

`

˘

γ2

cospθ2psqq cospθ2ptqq ` sinpθ2psqq sinpθ2ptqq

and sine summation formulas imply that translating θ1 by an arbitrary phase b leaves
K1 invariant (knowing θ1 up to a phase shift is suﬃcient to construct that kernel).
Similarly we select the covariance function of the independent centered GP ξ2 to be
K2ps, tq “ e´ ps´tq2
ity of ξ3 we select its covariance function to be K3ps, tq “ 1 ` st ` e´ ps´tq2
. Finally
since v4 is white noise we represent it with a centered GP with covariance function
K4ps, tq “ δps ´ tq. Figure 4 shows the recovered modes using (3.22) (or equivalently
deﬁned as (3.15) and the minimizer of (3.5)). In this numerical implementation the
interval r0, 1s is discretized with 302 points (with uniform time steps between points),
ξ4 is a discretized centered Gaussian vector of dimension 302 and of identity covariance
matrix and ξ1, ξ2, ξ3 are discretized as centered Gaussian vectors with covariance matri-
˘
302
ces corresponding to the kernel matrices
i,j“1 corresponding to K1, K2 and K3
Kpti, tjq
determined by the sample points ti, i “ 1, . . . , 302.

. To enforce the regular-

`

4

Table 2 provides a summary of the approach of Example 3.6, illustrating the connec-
tion between the assumed mode structure and corresponding Gaussian process structure
and its corresponding reproducing kernel structure.

16

Mode
˘
`
θ1ptq

v1ptq “ a1ptq cos

θ1 known

a1 unknown smooth

v2ptq “ a2ptq cos

˘
`
θ2ptq

θ2 known

a2 unknown smooth

GP

`

˘

ξ1ptq “ ζ1ptq cos

θ1ptq

Erζ1psqζ1ptqs “ e´ |s´t|2

γ2

ξ2ptq “ ζ2ptq cos

θ2ptq

`

˘

Erζ2psqζ2ptqs “ e´ |s´t|2

γ2

Kernel

K1ps, tq “ e´ |s´t|2

γ2

`

˘

cos

θ1psq

cos

˘
`
θ1ptq

K2ps, tq “ e´ |s´t|2

γ2

`

˘

cos

θ2psq

cos

˘
`
θ2ptq

v3 unknown smooth

Erξ3psqξ3ptqs “ e´ |s´t|2

γ2

K3ps, tq “ e´ |s´t|2

γ2

v4 unknown white noise Erξ4psqξ4ptqs “ σ2δps´tq

K4ps, tq “ σ2δps ´ tq

v “ v1 ` v2 ` v3 ` v4

ξ “ ξ1 ` ξ2 ` ξ3 ` ξ4

K “ K1 ` K2 ` K3 ` K4

Table 2: A summary of the approach of Example 3.6, illustrating the connection be-
tween the assumed mode structure and corresponding Gaussian process structure and
its corresponding reproducing kernel structure. Note that, for clarity of presentation,
this summary does not exactly match that of Example 3.6.

ř

ř

i αiXi with an additive regression model

On additive models. The recovery approach of Example 3.6 is based on the de-
sign of an appropriate additive regression model. Additive regression models are not
new. They were introduced in [97] for approximating multivariate functions with sums
of univariate functions. Generalized additive models (GAMs) [40] replace a linear re-
i fipXiq where the fi are
gression model
unspeciﬁed (smooth) functions estimated from the data. Since their inception GAMs
have become increasingly popular because they are both easy to interpret and easy to
ﬁt [78]. This popularity has motivated the introduction of additive Gaussian processes
[27, 24] deﬁned as Gaussian processes whose high dimensional covariance kernels are
obtained from sums of low dimensional ones. Such kernels are expected to overcome
the curse of dimensionality by exploiting additive non-local eﬀects when such eﬀects are
present [27]. See Section 2.1. Of course, performing regression or mode decomposition
with Gaussian processes (GPs) obtained as sums of independent GPs (i.e. performing
kriging with kernels obtained as sums of simpler kernels) is much older since Tikhonov
regularization (for signal/noise separation) has a natural interpretation as a conditional
expectation Erξs|ξs ` ξσs where ξs is a GP with a smooth prior (for the signal) and
ξσ is a white noise GP independent from ξs. More recent applications include clas-

17

siﬁcation [65], source separation [76, 62], and the detection of the periodic part of a
function from partial point evaluations [26, 1]. For that latter application, the approach
of [26] is to (1) consider the RKHS HK deﬁned by a Mat´ern kernel K (2) interpolate
the data with the kernel K and (3) recover the periodic part by projecting the inter-
polator (using a projection that is orthogonal with respect to the RKHS scalar product
onto Hp :“ spantcosp2πkt{λq, sinp2πkt{λq | 1 ď k ď qu (the parameters of the Mat´ern
kernel and the period λ are obtained via maximum likelihood estimation). Deﬁning Kp
and Knp as the kernels induced on Hp and its orthogonal complement in HK, we have
K “ Kp ` Knp and the recovery (after MLE estimation of the parameters) can also be
identiﬁed as the conditional expectation of the GP induced by Kp conditioned on the
GP induced by Kp ` Knp.

4 Kernel mode decomposition networks (KMDNets)

Figure 5: Left: Problem 1 is hard as a mode decomposition problem because the modes
vj “ ajptq cospθjptqq live in non-linear functional spaces. Right: One fundamental idea
is to recover those modes as aggregates of ﬁner modes vi living in linear spaces.

The recovery approach described in Example 3.6 is based on the prior knowledge of
(1) the number of quasi-periodic modes (2) their phase functions θi and (3) their base
periodic waveform (which need not be a cosine function). In most applications (1) and
(2) are not available and the base waveform may not be trigonometric and may not be
known. Even when the base waveforms are known and trigonometric (as in Problem
1), when the modes’ phase functions are unknown, the recovery of the modes is still
signiﬁcantly harder than when they are known because, as illustrated in Figure 5, the
(under regularity assumptions
functional spaces deﬁned by the modes ajptq cos

˘
θjptq

`

18

on the aj and θj) are no longer linear spaces and the simple calculus of Section 3 requires
the spaces Vj to be linear.

To address the full Problem 1, one fundamental idea is to recover those modes vj
as aggregates of ﬁner modes vi living in linear spaces Vi (see Figure 5).
In partic-
ular, we will identify i with time-frequency-phase triples pτ, ω, θq and the spaces Vi
with one dimensional spaces spanned by functions that are maximally localized in the
time-frequency-phase domain (i.e. by Gabor wavelets as suggested by the approxima-
by aggregating the ﬁner recovered
tion (1.1)) and recover the modes ajptq cos
modes. The implementation of this idea will therefore transform the nonlinear mode

˘
θjptq

`

Figure 6: Mode decomposition/recomposition problem. Note that the nonlinearity of
this model is fully represented in the identiﬁcation of the relation i (cid:32) j; once this
identiﬁcation is determined all other operations are linear.

decomposition problem illustrated on the left hand side of Figure 5 into the mode de-
composition/recomposition problem illustrated in Figure 6 and transfer its nonlinearity
to the identiﬁcation of ancestor/descendant relationships i (cid:32) j.

To identify these ancestor/descendant relations we will compute the energy Epiq :“
}wi}2
for each recovered mode wi, which as illustrated in Figure 7 and discussed in
D
@
Vi
Section 4.1, can also be identiﬁed as the alignment
wi, v
S´1 between recovered mode
@
D
wi and the signal v or as the alignment ErVarr
ξi, v
S´1s between the model ξi and the
S´1 which
data v. Furthermore E satisfy an energy preservation identity
leads to its variance decomposition interpretation. Although alignment calculations are
linear, the calculations of the resulting child-ancestor relations may involve a nonlinearity
(such as thresholding, graph-cut, computation of a maximizer) and the resulting network
can be seen as a sequence of sandwiched linear operations and simple non-linear steps

i Epiq “ }v}2

ř

19

Figure 7: Derivation of ancestor/descendant relations from energy calculations.

having striking similarities with artiﬁcial neural networks.

Of course this strategy can be repeated across levels of abstractions and its complete
deployment will also require the generalization of the setting of Section 3 (illustrated in
Figure 3) to a hierarchical setting (illustrated in Figure 10 and described in Section 4.3).

4.1 Model/data alignment and energy/variance decomposition

Using the setting and notations of Section 3 and ﬁxing the observed data v P V , let
E : I Ñ R` be the function deﬁned by

Epiq :“ }Ψipvq}2

Vi,

i P I,

(4.1)

where Ψi are the components of the optimal recovery map Ψ evaluated in Theorem 3.3.
We will refer to Epiq as the energy of the mode i in reference to its numerical analysis
interpretation (motivated by the ”energy” representation of Epiq “ rQ´1
i Ψipvq, Ψipvqs
determined by (3.7), and the interpretation of Q´1
as an elliptic operator) and our
general approach will be based on using its local and/or global maximizers to decom-
pose/recompose kernels.
Writing Etot :“ }v}2

S´1, note that (3.16) implies that

i

Etot “

ÿ

iPI

Epiq .

Let

D
@
S´1 be the scalar product on V deﬁned by the norm } ¨ }S´1.
¨, ¨
Proposition 4.1. Let ξ „ N p0, Qq and φ :“ S´1v. It holds true that for i P I,

Epiq “

@
Ψipvq, v

D
S´1 “ Var

`

˘
rφ, ξis

`@

D

“ Var

ξi, v

S´1

˘

.

(4.2)

(4.3)

20

`@

D

˘

Observe that Epiq “ Var

implies that Epiq is a measure of the alignment
between the Gaussian process (GP) model ξi and the data v in V and (4.2) corresponds
to the variance decomposition

ξi, v

S´1

`@ ÿ

D

˘

Var

ξi, v

S´1

“

iPI

ÿ

iPI

`@

D

Var

ξi, v

˘

.

S´1

(4.4)

Therefore, the stronger this alignment Epiq is, the better the model ξi is at explain-
ing/representing the data. Consequently, we refer to the energy Epiq as the alignment
energy. Observe also that the identity Epiq “
S´1 with wi “ Ψipvq implies that
Epiq is also a measure of the alignment between the optimal approximation wi of vi
and the signal v. Table 3 illustrates the relations between the conservation of alignment
energies and the variance decomposition derived from Theorem 3.3 and Proposition 4.1.

@
wi, v

D

Epiq
ř

i Epiq

Norm
}Ψipvq}2
}v}2

S´1

Vi “

@

D

Ψipvq, v

S´1

Operator/Kernel
rS´1v, QiS´1vs
rS´1v, vs

Table 3: Identities for Epiq and

ř

i Epiq

GP
`
Var
`
Var

˘

xξi, vyS´1
ř
x

i ξi, vyS´1

˘

4.2 Programming modules and feedforward network

Figure 8: Elementary programming modules for Kernel Mode Decomposition.

We will now combine the alignment energies of Section 4.1 with the mode decompo-
sition approach of Section 3 to design elementary programming modules (illustrated in
Figure 8) for kernel mode decomposition networks (KMDNets). These will be introduced
in this section and developed in the following ones. Per Section 3 and Theorem 3.3, the
optimal recoveries of the modes pviqiPI given the covariance operators pQiqiPI and the
ř
i1 Qi1q´1v in Vi. This operation is illustrated
observation of
in module (1) of Figure 8. An important quantity derived from this recovery is the en-
ergy function E : I Ñ R`, deﬁned in (4.1) by Epiq :“ rQ´1
i wi, wis with wi :“ Ψipvq,

iPI vi are the elements Qip

ř

21

ř

iPI Epiq, where Etot :“ }v}2

S´1 is
and illustrated in module (2). Since, per (4.2), Etot “
the total energy (4.1), the function E can be interpreted as performing a decomposition
of the total energy over the set of labels I. When I can be identiﬁed with the set of
vertices of a graph, the values of the Epiq can be used to cut that graph into subgraphs
indexed by labels j P J and deﬁne a relation i (cid:32) j mapping i P I to its subgraph j. This
graph-cut operation is illustrated in module (3). Since, per Section 4.1, Epiq is also the
mean squared alignment between the model ξi and the data v, and (4.4) is a variance
decomposition, this clustering operation combines variance/model alignment informa-
tion (as done with PCA) with the geometric information (as done with mixture models
[68]) provided by the graph to assign a class j P J to each element i P I. However, the
relation i (cid:32) j may also be obtained through a projection step, possibly ignoring the
values of Epiq, as illustrated in module (4) (e.g. when i is an r-tuple pi1, i2, . . . , irq then
the truncation/projection map pi1, . . . , irq (cid:32) pi1, . . . , ir´1q naturally deﬁnes a relation
(cid:32)). As illustrated in module (5), combining the relation (cid:32) with a sum
i(cid:32)j produces
aggregated covariance operators Qj :“
i(cid:32)j wi and energies
i(cid:32)j Qi, modes wj :“
i(cid:32)j Vi, the modes pwiqi(cid:32)j are (which can be
i(cid:32)j Epiq such that for Vj :“
Epjq :“
proven directly or as an elementary application of Theorem 4.4 in the next section) to
i(cid:32)j Vi given the covariance operators pQiqi(cid:32)j and the
be optimal recovery modes in
j wj, wjs. Nat-
observation of wj “
urally, combining these elementary modules leads to more complex secondary modules
(illustrated in Figure 9) whose nesting produces a network aggregating the ﬁne modes
wi into increasingly coarse modes with the last node corresponding to v.

i(cid:32)j wi in Vj. Furthermore, we have Epjq “ rQ´1

ř
ř

ś

ř

ř

ř

ř

Figure 9: Programming modules derived from the elementary modules of Figure 8.

4.3 Hierarchical mode decomposition

We now describe how a hierarchy of mode decomposition/recomposition steps discussed
in Section 4.2 naturally produces a hierarchy of labels, covariance operators, subspaces
and recoveries (illustrated in Figure 10) along with important geometries and inter-
relationships. This description will lead to the meta-algorithm Algorithm 1, presented
in Section 4.4, aimed at the production of a KMDNet such as the one illustrated in

22

Figure 10: The generalization of abstract mode decomposition problem of Figure 3 to a
hierarchy as described in Section 4.3.

Figure 10. Section 4.5 will present a practical application to Problem 1.

Our ﬁrst step is to generalize the recovery approach of Section 3 to the case where
V is the sum of a hierarchy of linear nested subspaces labeled by a hierarchy of indices,
as deﬁned below.

Deﬁnition 4.2. For q P N˚, let I p1q, . . . , I pqq be ﬁnite sets of indices such that I pqq “ t1u
has only one element. Let Yq
l“1I plq be endowed with a relation (cid:32) that is (1) transitive,
i.e., i (cid:32) j and j (cid:32) k implies i (cid:32) k (2) directed, i.e., i P I psq and j P I prq with s ě r
implies i ­(cid:32) j (that is, i does not lead to j) and (3) locally surjective, i.e., any element
j P I prq with r ą 1 has at least one i P I pr´1q such that i (cid:32) j. For 1 ď k ă r ď q and
an element i P I prq, write ipkq :“ tj P I pkq | j (cid:32) iu for the level k ancestors of i.

Let V pkq

i

, i P I pkq, k P t1, . . . , qu, be a hierarchy of nested linear subspaces of a

separable Hilbert space V such that

V pqq
1 “ V

and, for each level in the hierarchy k P t1, . . . , q ´ 1u,

V pk`1q
i

“

ÿ

jPipkq

V pkq
j

,

i P I pk`1q .

Let Bpqq “ V and for k P t1, . . . , q ´ 1u, let Bpkq be the product space

Bpkq :“

V pkq
i

.

ź

iPIpkq
23

(4.5)

(4.6)

For k ă r and j P I prq, let

Bpkq
j

:“

ź

iPjpkq

V pkq
i

and let

be deﬁned by

Φpr,kq
j

: Bpkq

j Ñ V prq

j

Φpr,kq
j

puq :“

ÿ

iPjpkq

ui,

u P Bpkq

j

.

(4.7)

(4.8)

Putting these components together as Φpr,kq “ pΦpr,kq
map

j

qjPIprq, we obtain the multi-linear

Φpr,kq : Bpkq Ñ Bprq,

1 ď k ă r ď q,

deﬁned by

Φpr,kqpuq :“

` ÿ

iPjpkq

˘
jPIprq,

ui

u “ puiqiPIpkq P Bpkq .

(4.9)

To put hierarchical metric structure on these spaces, for k P t1, . . . , qu and i P I pkq,

let

Qpkq
i

: V pkq,˚

i Ñ V pkq

i

be positive symmetric linear bijections determining the quadratic norms

}v}2

V pkq
i

“ rQpkq,´1
i

v, vs,

v P V pkq
i

,

(4.10)

. Then for k P t1, . . . , qu, let Bpkq be endowed with the quadratic norm

on the V pkq
i
deﬁned by

ÿ

}u}2

Bpkq “

iPIpkq
and, for k ă r ď q and j P I prq, let Bpkq
j
norm deﬁned by
ÿ

}ui}2

V pkq
i

,

u P Bpkq ,

(4.11)

ś

iPjpkq V pkq

i

:“

be endowed with the quadratic

}u}2

Bpkq
j

“

iPjpkq

}ui}2

V pkq
i

,

u P Bpkq

j

.

For 1 ď k ă r ď q, the nesting relations (4.5) imply that

i Ă V prq
V pkq

j

,

i P jpkq, j P I prq,

so that the subset injection

epr,kq
j,i

: V pkq

i Ñ V prq

j

(4.12)

is well deﬁned for all i P jpkq, j P I prq, and since all spaces are complete, they have
well-deﬁned adjoints, which we write

epk,rq
i,j

: V prq,˚

j Ñ V pkq,˚

i

.

(4.13)

24

For 1 ď k ă r ď q, i P I pkq and j P I prq, let

be deﬁned by

Ψpk,rq
i,j

: V prq

j Ñ V pkq

i

i,j Qprq,´1
so that, when putting the components together as

pvjq “ Qpkq

i epk,rq

Ψpk,rq
i,j

j

vj,

Ψpk,rq
j

:“ pΨpk,rq

i,j

qiPjpkq,

(4.7) determines the multi-linear map

Ψpk,rq
j

: V prq

j Ñ Bpkq

j

.

vj P V prq

j

,

(4.14)

(4.15)

Further collecting components simultaneously over the range and domain as

we obtain from (4.6) the multi-linear map

Ψpk,rq “ pΨpk,rq

j

qjPIprq

Ψpk,rq : Bprq Ñ

ź

jPIprq

Bpkq
j

deﬁned by

`

˘
iPjpkq,
The following condition assumes that the relation (cid:32) determines a mapping (cid:32):

v “ pvjqjPIprq P Bprq .

i,j Qprq,´1

Ψpk,rqpvq “

i epk,rq

(4.16)

Qpkq

vj

j

I pkq Ñ I pk`1q for all k “ 1, . . . , q ´ 1.

Condition 4.3. For k P t1, . . . , q ´ 1u, every i P I pkq has a unique descendant in I pk`1q.
That is, there exists a j P I pk`1q with i (cid:32) j and there is no other j1 P I pk`1q such that
i (cid:32) j1.

Condition 4.3 simpliﬁes the previous results as follows: the subsets pti P jpkquqjPIpk`1q
form a partition of I pkq, so that, for k ă r, we obtain the simultaneous product structure

so that both

and

Bpkq “

Bprq “

ź

jPIprq
ź

jPIprq

Bpkq
j

V prq
i

Φpk,rq : Bpkq Ñ Bprq

Ψpk,rq : Bprq Ñ Bpkq

25

(4.17)

are diagonal multi-linear maps with components

and

Φpr,kq
j

Ψpk,rq
j

: Bpkq

j Ñ V prq

j

: V prq

j Ñ Bpkq

j

respectively. Moreover, both maps are linear under the isomorphism between products
and external direct sums of vector spaces. For r ą k, we have the following connections
between Bpkq, Bprq, V pkq

and V prq

.

i

j

Bpkq

Ψpk,rq

Φpr,kq

Bprq

ś

iPIpkq

ś

jPIprq

V pkq
i
ř

iPjpkq

V prq
j

(4.18)

The following theorem is a consequence of Theorem 3.3.

Theorem 4.4. Assume that Condition 4.3 holds and that the Qpkq
satisfy the nesting relations

i

: V pkq,˚
i

Ñ: V pkq
i

ÿ

Qpk`1q
j

“

epk`1,kq
j,i

Qpkq

i epk,k`1q

i,j

,

j P I pk`1q,

(4.19)

for k P t1, . . . , q ´ 1u. Then for 1 ď k ă r ď q,

iPjpkq

• Ψpk,rq ˝ Φpr,kqpuq is the minmax recovery of u P Bpkq given the observation of

Φpr,kqpuq P Bprq using the relative error in } ¨ }Bpkq norm as a loss.

• Φpr,kq ˝ Ψpk,rq is the identity map on Bprq

• Ψpk,rq : pBprq, } ¨ }Bprqq Ñ pBpkq, } ¨ }Bpkqq is an isometry.
• Φpk,rq,˚ : pBprq,˚, } ¨ }Bprq,˚q Ñ pBpkq,˚, } ¨ }Bpkq,˚q is an isometry.

Moreover we have the following semigroup properties for 1 ď k ă r ă s ď q:

• Φps,kq “ Φps,rq ˝ Φpr,kq

• Ψpk,sq “ Ψpk,rq ˝ Ψpr,sq

• Ψpr,sq “ Φpr,kq ˝ Ψpk,sq

Remark 4.5. The proof of Theorem 4.4 also demonstrates that, under its assumptions,
for 1 ď k ă r ď q and j P I prq, Ψpk,rq
given
the observation of Φpr,kq
Furthermore, Φpr,kq
pBpkq
j

˝ Φpr,kq
j
using the relative error in } ¨ }Bpkq

is the identity map on V prq
j
q Ñ pBpkq,˚

j
˝ Ψpk,rq
j
q and Φpk,rq,˚

puq is the minmax recovery of u P Bpkq

j
norm as a loss.

j
q are isometries.

j
puq P V prq

and Ψpk,rq

: pV prq,˚
j

, } ¨ }V prq

: pV prq
j

q Ñ

j

j

j

j

j

, } ¨ }V prq,˚

, } ¨ }Bpkq,˚

, } ¨ }Bpkq

j

j

j

j

26

(cid:15)
(cid:15)
(cid:111)
(cid:111)
(cid:15)
(cid:15)
(cid:79)
(cid:79)
(cid:111)
(cid:111)
Gaussian process regression interpretation As in the setting of Section 4.3, for
k P t1, . . . , qu, let

Qpkq : Bpkq,˚ Ñ Bpkq

be the block-diagonal operator

Qpkq :“ diagpQpkq

i

qiPIpkq

deﬁned by its action Qpkqφ :“ pQpkq
write

i φiqiPIpkq, φ P Bpkq,˚ , and, as discussed in Section 3.2,

ξpkq „ N p0, Qpkqq

for the centered Gaussian ﬁeld on Bpkq with covariance operator Qpkq.

Theorem 4.6. Under the assumptions of Theorem 4.4, for 1 ă k ď q, the distribution of
ξpkq is that of Φpk,1qpξp1qq. Furthermore ξp1q conditioned on Φpk,1qpξp1qq is a time reverse
martingale2 in k and, for 1 ď k ă r ď q, we have

“
Ψpk,rqpvq “ E

ξpkq | Φpr,kqpξpkqq “ v

‰
,

v P Bprq .

(4.20)

4.4 Mode decomposition through partitioning and integration

In the setting of Section 4.3, recall that I pqq “ t1u and V pqq
Ψpk,qq
i,j

deﬁned in (4.14) only has one value j “ 1 and 1pkq “ I pkq, and therefore

1 “ V so that the index j in

Ψpk,qq
i,1

pvq :“ Qpkq

i epk,qq

i,1 Qpqq,´1

1

v,

v P V, i P I pkq .

(4.21)

Fix a v P V and for k P t1, . . . , qu, let

Epkq : I pkq Ñ R,

deﬁned by

Epkqpiq :“

›
›Ψpk,qq
i,1

›
›2
pvq
V pkq
i

,

i P I pkq,

(4.22)

be the alignment energy of the mode i P I pkq. Under the nesting relations (4.19), the
deﬁnition (4.10) of the norms and the semigroup properties of the subspace embeddings
(4.12) imply that

Epk`1qpiq “

ÿ

i1Pipkq

Epkqpi1q,

i P I pk`1q, k P t1, . . . , q ´ 1u .

(4.23)

We will now consider applications where the space pV, }¨}V q is known, and the spaces
q, including their index set I p1q, are known, but the spaces pV pkq
, } ¨ }V p1q
, } ¨ }V pkq
q

j

pV p1q
i

j

i

2If Fn is a decreasing sequence of sub-σ ﬁelds of a σ-ﬁeld F and Y is a F measurable random variable,

then pXn, Fnq, where En :“ ErY |Fns is a reverse martingale, in that ErXn|Fn`1s “ Xn`1

27

Figure 11: Derivation of the hierarchy from alignments.

and their indices I pkq, are unknown for 1 ă k ă q, as is any relation (cid:32) connecting
them. Instead, they will be constructed by induction from model/data alignments as
illustrated in Figures 7 and 11 and explained below. In these applications

pV, } ¨ }V q “ pV pqq

1

, } ¨ }V pqq

1

q,

ř

V “
the sum

iPIp1q V p1q

i

and the operator Qpqq
1

: V ˚ Ñ V associated with the norm } ¨ }V pqq

is

1

Qpqq

1 “

ÿ

iPIp1q

1,i Qp1q
epq,1q

i ep1,qq

i,1

.

(4.24)

In this construction we assume that the set of indices I p1q are vertices of a graph
Gp1q, whose edges provide neighbor relations among the indices. The following meta-
algorithm, Algorithm 1, forms a general algorithmic framework for the adaptive deter-
mination of the intermediate spaces pV pkq
q, their indices I pkq, and a relation (cid:32),
in such a way that Theorem 4.4 applies. Observe that this meta-algorithm is obtained
by combining the elementary programming modules illustrated in Figures 8 and 9 and

, } ¨ }V pkq

j

j

28

discussed in Section 4.2. In the following Section 4.5, it is demonstrated on a problem
in time-frequency mode decomposition.

Algorithm 1 Mode decomposition through partitioning and integration.

1: for k “ 1 to q ´ 2 do
2:
3: Use the function Epkq to segment/partition the graph Gpkq

Compute the function Epkq : I pkq Ñ R` deﬁned by (4.21) and (4.22).

into subgraphs
qjPIpk`1q, thereby determining the indices I pk`1q. Deﬁne the ancestors jpkq

pGpk`1q
j
of j P I pk`1q as the vertices i P I pkq of the sub-graph Gpk`1q
Identify the subspaces V pk`1q

j
and the operators Qpk`1q

through (4.5) and (4.19).

.

j

4:
5: end for
6: Recover the modes pΨpq´1,qq

j

i

pvqqiPIpq´1q of v.

4.5 Application to time-frequency decomposition

We will now propose a solution to Problem 1 based on the hierarchical segmentation
approach described in Section 4.4. We will employ the GPR interpretation of Section
3.3 and assume that the noisy signal v “ u ` vσ, where vσ is the noise, is the realization
of a Gaussian process ξ obtained by integrating Gabor wavelets [34] against white noise.
To that end, for τ, θ P R and ω, α ą 0, let

χτ,ω,θptq :“

´

2
π3

c

¯ 1
4

ω
α

`

cos

ωpt ´ τ q ` θ

˘

e´ ω2pt´τ q2

α2

,

t P R ,

(4.25)

ş

R χ2

be the shifted/scaled Gabor wavelet, whose scaling is motivated by the normalization
ş
π
τ,ω,θptq dt dθ “ 1. See Figure 12 for an illustration of the Gabor wavelets. Recall
´π
[34] that each χ is minimally localized in the time-frequency domain (it minimizes the
product of standard deviations in the time and frequency domains) and the parameter α
is proportional to the ratio between localization in frequency and localization in space.

Figure 12: Gabor wavelets χτ,ω,θ (4.25) for various parameter values with α “ 16.

29

“
E

Let ζpτ, ω, θq be a white noise process on R3 (a centered GP with covariance function
‰
ζpτ, ω, θqζpτ 1, ω1, θ1q

“ δpτ ´ τ 1qδpω ´ ω1qδpθ ´ θ1q) and let
ż

ż

ż

ζpτ, ω, θqχτ,ω,θptqdτ dω dθ,

t P R .

(4.26)

π

ωmax

1

ξuptq :“

´π

ωmin

0

Letting, for each τ, ω and θ,

Kτ,ω,θps, tq :“ χτ,ω,θpsqχτ,ω,θptq,

s, t P R,

(4.27)

be the reproducing kernel associated with the wavelet χτ,ω,θ, it follows that ξu is a
centered GP with covariance function
ż
ωmax

ż

ż

π

1

Kups, tq “

Kτ,ω,θps, tqdτ dω dθ,

´π

ωmin

0

s, t P R .

(4.28)

Given σ ą 0, let ξσptq be a white noise process on R (independent from ζ) of variance
“ σ2δps ´ tq) and let ξ, the

“
σ2 (a centered GP with covariance function E
GP deﬁned by

‰
ξσpsqξσptq

be used to generate the observed signal v “ u ` vσ. ξ is a centered GP with covariance
function deﬁned by the kernel

ξ :“ ξu ` ξσ ,

(4.29)

with

K :“ Ku ` Kσ

Kσps, tq “ σ2δps ´ tq .

(4.30)

(4.31)

Hence, compared to the setting of Section 3, and apart from the mode corresponding
to the noise ξσ, the ﬁnite number of modes indexed by I has been turned into a continuum
of modes indexed by

(
(cid:32)
pτ, ω, θq P r0, 1s ˆ rωmin, ωmaxs ˆ p´π, πs

I :“

with corresponding one dimensional subspaces

V p1q
pτ,ω,θq “ spantχτ,ω,θu,

positive operators Qτ,ω,θ deﬁned by the kernels Kτ,ω,θps, tq and the integral
ż

ż

ż

π

ωmax

1

Kups, tq “

´π

ωmin

0

Kτ,ω,θps, tqdτ dω dθ,

s, t P R ,

of these kernels (4.28) to obtain a master kernel Ku instead of a sum

ÿ

S “

iPI

eiQie˚
i

as in (3.12). Table 4 illustrates the time-frequency version of Table 2 we have just devel-
oped and the following remark explains the connection between kernels and operators
in more detail.

30

Mode

GP

Kernel

vτ,ω,θptq “ aτ,ω,θptqχτ,ω,θptq

ξτ,ω,θptq “ ζpτ, ω, θqχτ,ω,θptq

aτ,ω,θ unknown in L2

Erζpτ, ω, θqζpτ 1, ω1, θ1qs
“ δpτ ´ τ 1qδpω ´ ω1qδpθ ´ θ1q

Kτ,ω,θps, tq “ χτ,ω,θpsqχτ,ω,θptq

vτ,ω “

ş
π
´π vτ,ω,θdθ

ξτ,ωptq “

ş

π
´π ξτ,ω,θptqdθ

Kτ,ωps, tq “

ş
π
´π Kτ,ω,θps, tqdθ

ş ş ş

vu “

vτ,ω,θdτ dωdθ

ξuptq “

ş ş ş

ξτ,ω,θptqdτ dωdθ

Kups, tq “

ş ş ş

Kτ,ω,θps, tqdτ dωdθ

vσ unknown white noise

Erξσpsqξσptqs “ σ2δps ´ tq

Kσps, tq “ σ2δps ´ tq

v “ vu ` vσ

ş

vi “

Apiq vτ,ωdτ dω

ξ “ ξu ` ξσ
ş

Apiq ξτ,ωdτ dω

ξi “

K “ Ku ` Kσ

ş

Ki “

Apiq Kτ,ωdτ dω

Table 4: The time-frequency version of Table 2

Remark 4.7 (Kernels, operators, and discretizations). This kernel mode decompo-
sition framework constructs reproducing kernels K through the integration of elementary
reproducing kernels, but the recovery formula of Theorem 3.3 requires the application
In general, there is
of operators, and their inverses, corresponding to these kernels.
no canonical connection between kernels and operators, but here we consider restrict-
ing to the unit interval r0, 1s Ă R in the time variable t. Then, each kernel K under
consideration other than Kσ corresponds to the symmetric positive integral operator

¯K : L2r0, 1s Ñ L2r0, 1s

deﬁned by

˘

`
¯Kf

ż

1

psq :“

Kps, tqf ptqdt,

0

s P r0, 1s, f P L2r0, 1s .

Moreover, these kernels all have suﬃcient regularity that ¯K is compact and therefore
not invertible, see e.g. Steinwart and Christmann [95, Thm. 4.27]. On the other hand,
the operator

¯Kσ : L2r0, 1s Ñ L2r0, 1s

corresponding to the white noise kernel Kσ (4.31) is

where

¯Kσ “ σ2I

I : L2r0, 1s Ñ L2r0, 1s

31

is the identity map. Since K “ Ku ` Kσ (4.30), the operator ¯K “ ¯Ku ` ¯Kσ is a
symmetric positive compact operator plus a positive multiple of the identity and therefore
it is Fredholm and invertible. Consequently, we can apply Theorem 3.3 for the optimal
recovery.

In addition, in numerical applications, τ and ω are discretized (using N ` 1 dis-
cretization steps) and the integrals in (4.35) are replaced by sums over τk :“ k{N and
ωk :“ ωmin ` k
N pωmax ´ ωminq (k P t0, 1, . . . , N u). Moreover, as in Example 3.6, the
time interval r0, 1s is discretized into M points and the corresponding operators on RM
˘
are σ2I, where I : RM Ñ RM is the identity, plus the kernel matrix pKupti, tjq
corresponding to the sample points ti, i “ 1, . . . , M .

M
i,j“1

For simplicity and conciseness, henceforth we will keep describing the proposed ap-
proach in the continuous setting. Moreover, except in Section 4.6, we will overload
notation and not use the ¯K notation, but instead use the same symbol K for a kernel
and its corresponding operator.

Figure 13: Mode decomposition through partitioning and integration. q “ 4, wp3q :“
Ψp3,4qv, wp2q :“ Ψp2,4qv, and σ corresponds to the noise component.

We now describe the hierarchical approach of Section 4.4 to this time-frequency

setting and illustrate it in Figure 13. To that end, we identify I with I p1q so that

I p1q “

(
(cid:32)
pτ, ω, θq P r0, 1s ˆ rωmin, ωmaxs ˆ p´π, πs

Y tσu,

where the noise mode has been illustrated in Figure 13 by adding an isolated point with
label σ to each set I pkq with k ă q “ 4.

Although Line 3 of Algorithm 1 uses the energy Ep1q at level k “ 1 to partition
the index set I p1q, the algorithm is ﬂexible with regards to if or how we use it.
In
this particular application we ﬁrst ignore the computation of Ep1q and straightforward

32

partition I p1q into a family of subsets

I p1q
τ,ω :“

(
(cid:32)
pτ, ω, θq : θ P p´π, πs

(cid:32)

Y

σ

(
,

pτ, ωq P r0, 1s ˆ rωmin, ωmaxs,

indexed by τ and ω, so that the corresponding index set at level k “ 2 is

I p2q “

(
(cid:32)
pτ, ωq P r0, 1s ˆ rωmin, ωmaxs

(cid:32)

Y

σ

(
,

and the ancestors of pτ, ω, σq are

pτ, ω, σqp2q “

(cid:32)

(
pτ, ω, θq : θ P p´π, πs

(cid:32)

Y

σ

(
.

The subspace corresponding to the label pτ, ωq is then

(
(cid:32)
V p2q
χτ,ω,θ | θ P p´π, πs
pτ,ωq “ span

and, as in (4.19), its associated positive operator is characterized by the kernel

ż

π

Kτ,ω :“

Kτ,ω,θdθ .

´π

(4.32)

We can evaluate Kτ,ω using (4.27) and (4.25) by deﬁning

χτ,ω,cptq

:“

χτ,ω,sptq

:“

´

´

2
π
2
π

c

c

¯ 1
4

¯ 1
4

ω
α
ω
α

cospωpt ´ τ qqe´ ω2pt´τ q2

α2

,

t P R,

sinpωpt ´ τ qqe´ ω2pt´τ q2

α2

,

t P R,

(4.33)

and using the cosine summation formula to obtain

Kτ,ωps, tq :“ χτ,ω,cpsqχτ,ω,cptq ` χτ,ω,spsqχτ,ω,sptq .

(4.34)

Therefore V p2q

pτ,ωq “ spantχτ,ω,c, χτ,ω,su and (4.28) reduces to

ż

ż

1

ωmax

Kups, tq “

Kτ,ωps, tqdτ dω .

(4.35)

ωmin

0

Using K :“ Ku`Kσ (4.30), let f be the solution of the linear system

vpsq, i.e.

Kf “ v ,

ş

1
0 Kps, tqf ptq dt “

(4.36)

and let Epτ, ωq be the energy of the recovered mode indexed by pτ, ωq, i.e.

ż

ż

1

1

Epτ, ωq “

f psqKτ,ωps, tqf ptq ds dt,

pτ, ωq P r0, 1s ˆ rωmin, ωmaxs .

(4.37)

0

0

Since Kf “ v implies that

vT K´1v “ f T Kf,

33

it follows that

vT K´1v “

ż

ż

1

ωmax

ωmin

0

Epτ, ωq dτ dω ` f T Kσf .

(4.38)

For the recovery of the m (which is unknown) modes using Algorithm 1, at the second
level k “ 2 we use Epτ, ωq to partition the time-frequency domain of pτ, ωq into n
disjoint subsets Ap1q, Ap2q, . . . Apnq. As illustrated in Figure 13, n “ 3 is determined
from Epτ, ωq, and I p3q is deﬁned as t1, 2, . . . , n, σu, the subspace corresponding to the
mode i ­“ σ as V p3q
i “ spantχτ,ω,c, χτ,ω,s | pτ, ωq P Apiqu and the kernel associated with
the mode i ­“ σ as

ż

Kips, tq “

Kτ,ωps, tqdτ dω,

s, t P R ,

(4.39)

pτ,ωqPApiq

as displayed in the bottom row in Table 4, so that

nÿ

Ku “

Ki .

i“1

We then apply the optimal recovery formula of Theorem 3.3 to approximate the modes
of v1, . . . , vn of u from the noisy observation of v “ u ` vσ (where vσ is a realization of
ξσ) with the elements w1, . . . , wn obtained via

wi “ KiK´1v “ Kif,

that is, the integration

wi “ Kif .

(4.40)

Figure 14 illustrates a three mode m “ 3 noisy signal, the correct determination of
n “ m “ 3, and the recovery of its modes. Figure 14.1 displays the total observed signal
v “ u ` vσ and the three modes v1, v2, v3 constituting u “ v1 ` v2 ` v3 are displayed in
3. Figure 14.8 also shows
Figures 14.5, 6 and 7, along with their recoveries w1, w2 and w3
approximations of the instantaneous frequencies obtained as

ωi,Eptq :“ argmaxω:pt,ωqPApiq Ept, ωq .

(4.41)

4.6 Convergence of the numerical methods

This section, which can be skipped on the ﬁrst reading, provides a rough overview of
how the empirical approach describe in Remark 4.7 generates convergence results. To
keep this discussion simple, we assume that the reproducing kernel K is continuous and
its corresponding integral operator ¯K is injective (the more general case is handled by
quotienting with respect to its nullspace). Then the RKHS HK can be described as the

3The recoveries wi in Figure 14.5,6 and 7, are indicated in red and the modes vi of the signal are in

blue. When the recovery is accurate, the red recovery blocks the blue and appears red.

34

Figure 14: (1) The signal v “ u ` vσ where u “ v1 ` v2 ` v3, vσ „ N p0, σ2δpt ´ sqq
and σ “ 0.01 (2) pτ, ωq Ñ Epτ, ωq deﬁned by (4.37) (one can identify three stripes) (3)
ω Ñ Ep0.6, ωq (4) Partitioning r0, 1s ˆ rωmin, ωmaxs “ Y3
i“1Apiq of the time frequency
domain into three disjoint subsets identiﬁed from E (5) v1 and its approximation w1 (6)
v2 and its approximation w2 (7) v3 and its approximation w3 (8) ω1, ω2, ω3 and their
approximations ω1,E, ω2,E, ω3,E.

1

1

2 : L2r0, 1s Ñ HK is an isometric isomorphism, see e.g.

2 pL2r0, 1sq Ă L2r0, 1s of the unique positive symmetric square root of ¯K and
image ¯K
the map ¯K
[55, Thm. 17.12].
Moreover, by the zero-one law of Luki´c and Beder [63, Thm. 7.2], the Gaussian stochastic
process with covariance K has its sample paths in HK with probability 1. Consequently,
the Gaussian stochastic process with covariance K will have some approximation error
when the observation v is not an element of HK. This is the classical situation justifying
the employment of Tikhonov regularization, motivating our introduction of the additive
white noise component to the stochastic model. However, before we discuss Tikhonov
regularization, let us begin with the case when v is an element of HK. Then, according
to Engl, Hanke and Neubauer’s [28, Ex. 3.25] analysis of the least-squares collocation
1
method in [28, Ex. 3.25] applied to solving the operator equation ¯K
2
is considered as ¯K
2 : L2r0, 1s Ñ HK, application of the dual least-squares method of
regularization, described in Engl, Hanke and Neubauer [28, Ch. 3.3], reveals that our
collocation discretization produces the least-squares collocation approximation fm of
the solution f of ¯K
2 fm “ Qmv,
where Qm : HK Ñ HK denotes the HK-orthogonal projection onto the span Ym of
the representers Φxj P HK of the point evaluations at the collocation points xj (i.e. we
have xw, Φxj yHK “ wpxjq, w P HK, j “ 1, . . . m). Moreover, [28, Thm. 3.24] asserts
that the resulting solution fm satisﬁes fm “ Pmf where Pm : L2r0, 1s Ñ L2r0, 1s is the
orthogonal projection onto ¯K
2 ,˚Ym. Quantitative analysis of the convergence of fm to

2 f “ v, i.e. the minimal norm solution fm of Qm ¯K

2 f “ v, where ¯K

1

1

1

1

1

35

f is then a function of the strong convergence of Pm to the identity operator and can be
assessed in terms of the expressivity of the set of representers Φxj . For v not an element
of HK, Tikhonov regularization is applied together with least-squares collocation as in
[28, Ch. 5.2].

5 Additional programming modules and squeezing

The KMDNets described in Section 4 not only introduce hierarchical structures to im-
plement nonlinear estimations using linear techniques, but can also be thought of as a
sparsiﬁcation technique whose goal is to reduce the computational complexity of solving
the corresponding GPR problem, much like the sparse methods have been invented for
GPR discussed in Section 2.2. The primary diﬀerence is that, whereas those methods
generally use a set of inducing points determining a low rank approximation and then
choose the location of those points to optimize its approximation, here we utilize the the
landscape of the energy function E : I Ñ R`, deﬁned in (4.1) and analyzed in Propo-
sition 4.1, interpreted as alignment energies near (4.4). In this section, this analogue of
sparse methods will be further developed for the KMDNets using the energy alignment
landscape to further develop programming modules which improve the eﬃcacy and ac-
curacy of the reconstruction. For another application of the alignment energies in model
construction, see Hamzi and Owhadi [38, Sec. 3.3.2] where it is used to estimate the
optimal time lag of a ARMA-like time series model.

In the approach described in Section 4.4, I pkq was partitioned into subsets pjpkqqjPIpk`1q
and the Qpkq
i were integrated (that is, summed over or average-pooled) using (4.5) and
(4.19) in Line 4 of Algorithm 1, over each subset to obtain the Qpk`1q
. This partitioning
approach can naturally be generalized to a domain decomposition approach by letting
the subsets be non-disjoint and such that, for some k, YjPIpk`1qjpkq forms a strict subset4
of I pkq (i.e. some i P I pkq may not have descendants). We will now generalize the relation
(cid:32) so as to (1) not satisfy Condition 4.3, that is, it does not deﬁne a map (a label i may
have multiple descendants) (2) be non directed, that is, not satisfy Deﬁnition 4.2 (some
j P I pk`1q may have descendants in I pkq) and (3) enable loops.

j

With this generalization the proposed framework is closer (in spirit) to an object
oriented programming language than to a meta-algorithm. This is consistent with what
Yann LeCun in his recent lecture at the SIAM Conference on Mathematics of Data
Science (MDS20) [57] has stated; paraphrasing him: ”The types of architectures people
use nowdays are not just chains of alternating linear and pointwise nonlinearities, they
are more like programs now.” We will therefore describe it as such via the introduction
of additional elementary programming modules and illustrate the proposed language by
programming increasingly eﬃcient networks for mode decomposition.

4Although the results of Theorem 4.4 do not hold true under this general domain-decomposition,
are

those of Theorem 3.3 remain true between levels k and q (in particular, at each level k the vpkq
optimal recovered modes given the Qpkq

and the observation v).

i

i

36

5.1 Elementary programming modules

Figure 15: Elementary programming modules.

We will now introduce new elementary programming modules in addition to the ﬁve
illustrated in Figure 8 and discussed in Section 4.2. These new modules are illustrated in
Figure 15, beginning with module (6). Here they will be discussed abstractly but forward
reference to speciﬁc examples.. The ﬁrst module (module (6)) of Figure 15 replaces the
average-pooling operation to the deﬁne the energy E by a max-pool operation. More
precisely module (6) combines a relation i (cid:32) j with an energy E to produce a max-pool
energy via

Epiq ,

Spjq “ max
i(cid:32)j
where i (cid:32) j here is over i from the previous level to that of j. In what follows we will
adhere to this semantic convention. As shown in module (7), this combination can also
be performed starting with a max-pool energy, i.e. module (7) combines a relation i (cid:32) j
with a max-pool energy S at one level to produce a max-pool energy at the next level
via

(5.1)

Spjq “ max
i(cid:32)j

Spiq .

(5.2)

Maximizers can naturally be derived from this max-pooling operation and modules (8)
and (9) deﬁne ipjq as the maximizer (or the set of maximizers if non-unique) of the
energy or the max-pool energy. More precisely module (8) combines a relation i (cid:32) j
with an energy function Epiq to produce

ipjq “ argmaxi(cid:32)j Epiq ,

(5.3)

and module (9)5combines a relation i (cid:32) j with a max-pool energy function Spiq to
produce

ipjq “ argmaxi(cid:32)j Spiq .

(5.4)

37

5.2 Programming the network

Programming of the network is achieved by assembling the modules of Figures 8 and 15
in a manner that (1) v is one of the inputs of the network and (if the network is used for
mode decomposition/pattern recognition) (2) the modes vm are one of the outputs of the
network. As with any interpretable programming language avoiding ineﬃcient coding
and bugs remains important. We will now use this language to program KMDNets.

5.3 Squeezing

We will now present an interpretation and a variant (illustrated in Figure 17) of the
synchrosqueezing transform due Daubechies et al. [19, 18] (see Section 2.4 for a descrip-
tion), in the setting of KMDNets, and thereby initiate its GP regression version. We
will demonstrate that this version generalizes to the case where the basic waveform is
non-periodic and/or unknown. We use the setting and notations of Section 4.5.

Let f be the solution of Kf “ v (4.36) and let

ż

ż

1

1

Epτ, ω, θq :“

f psqKτ,ω,θps, tqf ptq ds dt

(5.5)

0

0

be the energy of the mode indexed by pτ, ω, θq. For pτ, ωq P r0, 1s ˆ rωmin, ωmaxs, write
θepτ, ωq :“ argmaxθPp´π,πs Epτ, ω, θq .
Since the deﬁnitions (4.25) of χτ,ω,θ and (4.33) of χτ,ω,c and χτ,ω,s, together with the
cosine summation formula, imply that

(5.6)

χτ,ω,θptq “

1
?
π

`

˘
χτ,ω,cptq cospθq ´ χτ,ω,sptq sinpθq

,

t P R,

5The description of the remaining modules (10)-(17), which can be skipped on ﬁrst reading, is as
follows. Similarly to module (3) of Figure 8, module (10) of Figure 15 combines the max-pool energy S
with a graph operation to produce the ancestor-descendant relation i (cid:32) j. We will show that module
(10) leads to a more robust domain decomposition than module (3) due to its insensitivity to domain
discretization. Module (11) uses the functional dependence jpiq to deﬁne the relation i (cid:32) j. Module
it combines jpiq and kpjq to produce kpiq.
(12) expresses the transitivity of function dependence, i.e.
Similarly, module (13) expresses the transitivity of the relation (cid:32), i.e. i (cid:32) j and j (cid:32) k can be combined
to produce i (cid:32) k. Module (14) (analogously to module (4)) uses an injection step to deﬁne a functional
dependence ipjq (e.g. for the time-frequency application in Figure 19, if J is the set of pτ, ω1q and I is
that of pτ, ωq the injection ι : I X J Ñ I deﬁnes a functional dependence ipjq). Module (15) uses a
functional dependence ipjq to produce another functional dependence kpjq (e.g. for the time-frequency-
phase application in Figures 21 and 22, we can deﬁne the functional dependence pτ, ω1qpτ, ωq from
the functional dependence pτ, ω, θqpτ, ωq via ω1pτ, ωq “ Bτ θpτ, ωq). Module (16) utilizes the functional
dependence ipjq to produce a pullback covariance operator Qj :“ Qipjq (:“
iPipjq Qi if ipjq is a set-
valued rather than a single-valued mapping). Module (17) combines a functional dependence ipjq with
a relation j (cid:32) k to produce a covariance operator Qk (e.g. for the time-frequency-phase application of
Figures 21 and 22, for i “ pτ, ω, θq P I p1q and j “ pτ, kq P I p4q where the index k is the mode index, the
functional dependence ipjq deﬁnes through (5.20) estimated phases θk,ep¨q which can then be substituted
for θp¨q in the kernel Kps, tq “ e´|s´t|2{γ2 `
, producing for each
mode index k a kernel with corresponding operator Qk).

cospθpsqq cospθptq ` sinpθpsqq sinpθptq

ř

˘

38

it follows that, if we deﬁne

Wcpτ, ωq

:“

Wspτ, ωq

:“

ż

1

0
ż
1

0

χτ,ω,cptqf ptq dt

χτ,ω,sptqf ptq dt ,

(5.7)

we obtain

ż

1

0

χτ,ω,θptqf ptq dt “

`

1
?
π

cospθqWcpτ, ωq ´ sinpθqWspτ, ωq

.

(5.8)

˘

Consequently, we deduce from (5.5) and (4.27) that

1
π
It follows that, when either Wcpτ, ωq ‰ 0 or Wspτ, ωq ‰ 0, that
˘

cospθqWcpτ, ωq ´ sinpθqWspτ, ωq

Epτ, ω, θq “

`

`

θepτ, ωq “ phase

Wcpτ, ωq ´ iWspτ, ωq

,

where, for a complex number z,

phasepzq :“ θ P p´π, πs : z “ reiθ, r ą 0 .

˘
2 .

Moreover, it follows from (4.32), (4.37) and (5.5) that

ż

π

so that it follows from (5.9) that

Epτ, ωq “

Epτ, ω, θqdθ ,

´π

Epτ, ωq “ W 2

c pτ, ωq ` W 2

s pτ, ωq .

(5.9)

(5.10)

(5.11)

(5.12)

Now consider the mode decomposition problem with observation v “

vi under the
assumption that the phases vary much faster than the amplitudes. It follows that for
the determination of frequencies (not the determination of the phases) we can, without
loss of generality, assume each mode is of the form

ř

viptq “ aiptq cospθiptqq

(5.13)

where ai is slowly varying compared to θi. We will use the symbol « to denote an informal
approximation analysis. Theorem 6.1 asserts that K is approximately a multiple of the
identity operator, so we conclude that the solution f to Kf “ v in (4.36) is f « cv for
some constant c. Because we will be performing a phase calculation the constant c is
irrelevant and so can be set to 1, that is we have f « v and therefore we can write (5.7)
as

ż

1

Wcpτ, ωq «

χτ,ω,cptqvptq dt

0
ż
1

Wspτ, ωq «

χτ,ω,sptqvptq dt .

(5.14)

0

39

For ﬁxed τ , for t near τ ,

viptq « aipτ q cosppt ´ τ q 9θipτ q ` θipτ qq
so that, since the frequencies 9θi are relatively large and well separated, it follows from
the nulliﬁcation eﬀect of integrating cosines of high frequencies, that for ω « 9θipτ q,
(5.14) holds true with vi instead of v in the right-hand side. Because the amplitudes of
vi in (5.13) are slowly varying compared to their frequencies, it again follows from the
nulliﬁcation eﬀect of integrating cosines of high frequencies, the approximation formula
(5.15), the representation (4.33) of χτ,ω,c and χτ,ω,s and the sine and cosine summation
formulas, that

(5.15)

ż

1

Wcpτ, ωq « aipτ q cospθipτ qq

χτ,ω,cptq cosppt ´ τ qωq dt

0
ż

1

Wspτ, ωq « ´aipτ q sinpθipτ qq

χτ,ω,sptq sinppt ´ τ qωq dt .

0

Since the representation (4.33) of χτ,ω,c and χτ,ω,s, and the sine and cosine summation
ş
1
0 χτ,ω,sptq sinppt ´ τ qωq dt ą 0 ,
formulas, also imply that
it follows that

ş
1
0 χτ,ω,cptq cosppt ´ τ qωq dt «

Wcpτ, ωq ´ iWspτ, ωq « aipτ qeiθipτ q

χτ,ω,cptq cosppt ´ τ qωq dt ,

0

ż

1

so that θepτ, ωq, deﬁned in (5.10), is an approximation of θipτ q, and

ωepτ, ωq “

Bθe
Bτ

pτ, ωq

(5.16)

is an approximation of the instantaneous frequency 9θipτ q.
Remark 5.1. In the discrete case, on a set tτku of points, we proceed diﬀerently than in
(5.16). Ignoring for the moment the requirement (5.11) that the phase θepτ, ωq deﬁned
in (5.10) lies in p´π, πs, an accurate ﬁnite diﬀerence approximation ωepτk, ωq to the
frequency is determined by

θepτk, ωq ` ωepτk, ωqpτk`1 ´ τkq “ θepτk`1, ωq.

To incorporating the requirement, it is natural to instead deﬁne ωepτk, ωq as solving

ei ωepτk,ωqpτk`1´τkqeiθepτk,ωq “ eiθepτk`1,ωq,

which using (5.10) becomes

ei ωepτk,ωqpτk`1´τkqei phasepWcpτk,ωq´iWspτk,ωqq “ ei phasepWcpτk`1,ωq´iWspτk`1,ωqq ,

and has the solution
1
τk`1 ´ τk

ωepτk, ωq “

ˆ

atan2

Wcpτk`1, ωqWspτk, ωq ´ Wspτk`1, ωqWcpτk, ωq
Wcpτk`1, ωqWcpτk, ωq ` Wspτk`1, ωqWspτk, ωq

˙

,

(5.17)

where atan2 is Fortran’s four-quadrant inverse tangent.

40

Figure 16: (1) Wcpτ, ωq (2) Wspτ, ωq (3) τ Ñ pWcpτ, 300q, Wspτ, 300q, τ q (4) pτ, ωq Ñ
θepτ, ωq (5) pτ, ωq Ñ ωepτ, ωq (6) ω Ñ ωep0.6, ωq and ω Ñ Ep0.6, ωq (7) ω Ñ Sp0.6, ωq
(8)ω Ñ SEp0.6, ωq (9) pτ, ωq Ñ Spτ, ωq (10) t Ñ ωiptq and t Ñ ωi,eptq for i P t1, 2, 3u
(11) t Ñ cospθ1ptqq and t Ñ cospθ1,eptqq (12) t Ñ sinpθ1ptqq and t Ñ sinpθ1,eptqq.

In preparation for illustrating the application of the programming of KMDNets, as
a synchrosqueezing algorithm, to the decomposition problem when v and its modes are
as in Figure 14, Figure 16 illustrates the basic quantities we have just been developing.
In particular,

• The functions Wc and Ws are shown in Figures 16.1 and 16.2.

• The function τ Ñ pWcpτ, 300q, ´Wspτ, 300qq is shown in Figure 16.3 with τ the
vertical axis. The functions θepτ, 300q, Epτ, 300q and ωepτ, 300q are the phase,
square modulus and angular velocity of this function.

• The functions pτ, ωq Ñ θepτ, ωq, τ Ñ θepτ, ωi,Epτ qq (with ωi,E deﬁned in (4.41))
and t Ñ θiptq are shown in Figures 16.4, 11 and 12. Observe that τ Ñ θepτ, ωi,Epτ qq
is an approximation of τ Ñ θipτ q.

• The functions pτ, ωq Ñ ωepτ, ωq, ω Ñ ωep0.6, ωq and τ Ñ ωepτ, ωi,Epτ qq are shown
in Figures 16.5, 6 and 10. Observe that τ Ñ ωepτ, ωi,Epτ qq is an approximation of
the instantaneous frequency τ Ñ ωipτ q “ 9θipτ q of the mode vi.

41

To describe the remaining components of Figure 16 and simultaneously complete
the application of the programming of KMDNets as a synchrosqueezing algorithm and
introduce a max-pool version of synchrosqueezing, we now introduce the synchrosqueezed
energy SEpτ, ωq and the max-pool energy Spτ, ωq: Motivated by the synchrosqueezed
transform introduced in Daubechies et al. [18], the synchrosqueezed energy SEpτ, ωq
is obtained by transporting the energy Epτ, ωq via the map pτ, ωq Ñ pτ, ωepτ, ωqq (as
discussed in Section 2.4, especially near (2.3) ), and therefore satisﬁes

ż

ωmax

ż

ωmax

ϕpωqSEpτ, ωq dω “

ωmin

ωmin

ϕpωepτ, ω1qqEpτ, ω1q dω1

for all regular test function ϕ, i.e.

SEpτ, ωq “ lim
δÑ0

1
δ

ż

ω1:ωďωepτ,ω1qďω`δ

Epτ, ω1q dω1 ,

(5.18)

where numerically approximate (5.18) by taking δ small.

Returning to the application, the transport of the energy Epτ, ωq via the map
pτ, ωq Ñ pτ, ωepτ, ωqq is illustrated for τ “ 0.6 by comparing the plots of the func-
tions ω Ñ ωep0.6, ωq and ω Ñ Ep0.6, ωq in Figure 16.6 with the function ω Ñ SEp0.6, ωq
shown in Figure 16.8. As in [18], the value of SEpτ, ωq (and thereby the height of
the peaks in Figure 16.8) depends on the discretization and the measure dω1 used in
the integration (5.18). For example, using a logarithmic discretization or replacing the
Lebesgue measure dω1 by ω1dω1 in (5.18) will impact the height of those peaks. To avoid
this dependence on the choice of measure, we deﬁne the max-pool energy

Spτ, ωq “

max
ω1:ωepτ,ω1q“ω

Epτ, ω1q ,

(5.19)

illustrated in Figure 16.9. Comparing Figures 16.6, 7 and 8, observe that, although
both synchrosqueezing and max-pooling decrease the width of the peaks of the energy
plot ω Ñ Ep0.6, ωq, only max-squeezing preserves their heights (as noted in [18, Sec. 2]
a discretization dependent weighting of dω1 would have to be introduced to avoid this
dependence).

Figure 17 provides an interpretation of the synchrosqueezed and max-pool ener-
gies SEpτ, ωq and Spτ, ωq in the setting of KMDNet programming, where we note that
the left (synchrosqueezed) and right (max-pool) sub-ﬁgures are identical except for
In that interpretation I p1q and I p2q
the highlighted portions near their top center.
are, as in Section 4.5 and modulo the noise mode σ, respectively, the set of time-
frequency-phase labels pτ, ω, θq P r0, 1s ˆ rωmin, ωmaxs ˆ p´π, πs and the set of time-
frequency labels pτ, ωq P r0, 1s ˆ rωmin, ωmaxs. Modulo the noise label σ, I p3q is the
range of pτ, ωq Ñ pτ, ωepτ, ωqq and the ancestors of pτ, ω1q P I p3q are the pτ, ωq such that
ω1 “ ωepτ, ωq. Then, in that interpretation, the synchrosqueezed energy is simply the
level 3 energy Ep3q, whereas Spτ, ωq is the level 3 max-pool energy S p3q. Note that the
proposed approach naturally generalizes to the case where the periodic waveform y is
known and non-trigonometric by simply replacing the cosine function in (4.25) by y.

42

Figure 17: Synchrosqueezed (left) and max-pool (right) energies.

Figure 18: (1) The signal v “ v1 ` v2 ` v3 ` vσ where vσ „ N p0, σ2δps ´ tqq and σ “ 0.01
(2) instantaneous frequencies t Ñ ωiptq of the modes i “ 1, 2, 3 (3) pτ, ωq Ñ Spτ, ωq
(4) Sub-domains Ap1q, Ap2q and Ap3q of the time-frequency domain (5) approximated
instantaneous frequencies t Ñ ωi,eptq of the modes i “ 1, 2, 3 (6, 7, 8) v1, v2, v3 and their
approximations w1, w2, w3 obtained from the network shown in Figure 19 (9) phase θ1
and its approximation θ1,e (10, 11, 12) v1, v2, v3 and their approximations w1, w2, w3
obtained from the network shown in Figure 21.

43

5.4 Crossing instantaneous frequencies

Let us now demonstrate the eﬀectiveness of the max-pooling technique in its ability to
perform mode recovery when the instantaneous frequencies of the modes cross. Consider
the noisy signal v illustrated in Figure 18.1. This signal is composed of 4 modes, v “ v1 `
v2 ` v3 ` vσ, where vσ „ N p0, σ2δps ´ tqq is a white noise realization with σ “ 0.01. The
modes v1, v2, v3 are shown in Figures 18.6, 7 and 8, and their instantaneous frequencies
ω1, ω2, ω3 are shown in Figure 18.2 (see Footnote 3). Note that ω1 and ω2 cross each
other around t « 0.6 and v3 vanishes around t « 0.3. We now program two KMDNets
and describe their accuracy in recovering those modes.

Figure 19: Recovery from domain decomposition. The left-hand side of the ﬁgure is that
of the right-hand side (corresponding to max-pooling) of Figure 17. The remaining part
is obtained by identifying three subsets Ap1q, Ap2q, Ap3q of the time-frequency domain
pτ, ωq and integrating the kernel Kτ,ω (deﬁned as in (4.32)) over those subsets (as in
(4.39)).

The ﬁrst network, illustrated in Figures 19 and 20 recovers approximations to v1, v2, v3
by identifying three subsets Ap1q, Ap2q, Ap3q of the time-frequency domain pτ, ωq and in-
tegrating the kernel Kτ,ω (deﬁned as in (4.32)) over those subsets (as in (4.39)). For
this example, the subsets Ap1q, Ap2q, Ap3q are shown in Figure 18.4 and identiﬁed as
narrow sausages deﬁned by the peaks of the max-pool energy S p3qpτ, ω1q (computed
as in (5.19)) shown in 18.3). The corresponding approximations w1, w2, w3 (obtained
as in (4.40)) of the modes v1, v2, v3 are shown in Figures 18.6, 7 and 8. Note the
increased approximation error around t « 0.6 corresponding to the crossing point be-
tween ω1 and ω2 and Ap1q and Ap2q. The estimated instantaneous frequencies ωi,epτ q “
illustrated in Figure 18.5 also show an increased esti-
ωe

τ, argmaxω:pτ,ωqPApiq S p3qpτ, ωq

`

˘

44

mation error around that crossing point.

ş

ř

ř

dθ indicated as

i(cid:32)j Epiq and covariances Qj :“

Figure 20: The KMDNet program corresponding to Figure 19. Upper left provides
the symbolic connections between the indices i, j, k and the time-frequency parameters
along with the functional dependencies ipjq and kpjq. Beginning with the input v in
the lower left, the operators Qi corresponding to the baby kernels Kτ,ω,θ are used to
produce optimal recovery estimates wi and the corresponding alignment energies Epiq.
The projection function jpiq taking pτ, ω, θq to pτ, ωq is the relation i (cid:32) j which deter-
i(cid:32)j which then determines summed
mines the integration operation
ř
energies Epjq :“
i(cid:32)j Qi. Moreover, the projection
i (cid:32) j also determines a max operation arg maxθ which we denote by arg maxi(cid:32)j and the
resulting function θepτ, ωq :“ arg maxθ Eτ,ω,θ, which determines the functional depen-
dency ipjq “ pτ, ω, θepτ, ωqq. This function is then diﬀerentiated to obtain the functional
relation kpjq “ pτ, ωepτ, ωqq where ωepτ, ωq :“ B
θepτ, ωq. This determines the relation
Bτ
j (cid:32) k which determines the maximization operation maxj(cid:32)k that, when applied to the
alignment energies Epjq, produces the max-pooled energies Spkq. These energies are
then used to determine a graph cut establishing a relation k (cid:32) m where m is a mode
index. Combining this relation with the injection j (cid:32) k determines the relation j (cid:32) m,
j(cid:32)m over the preimages of the relation, thus de-
that then determines the summation
j(cid:32)m Qj. Optimal recovery
termining operators Qm indexed by the mode m by Qm :“
is then applied to obtain the estimates wm :“ Qmp

ř

ř

ř

m1 Qm1q´1.

The second network, illustrated in Figures 21 and 22, proposes a more robust ap-

proach based on the estimates θi,e of instantaneous phases θi obtained as

θi,epτ q “ θe

˘
`
τ, argmaxω:pτ,ωqPApiq S p3qpτ, ωq

,

(5.20)

45

Figure 21: Recovery from instantaneous phases approximations. The left-hand side of
the ﬁgure is that of the right-hand side (corresponding to max-pooling) of Figure 17 and
therefore also that of Figure 19, and proceeding to the right as in Figure 19, the three
subsets Ap1q, Ap2q, Ap3q of the time-frequency domain pτ, ωq and integrating the kernel
Kτ,ω (deﬁned as in (4.32)) over those subsets (as in (4.39)). However, to deﬁne the kernels
Km for the ﬁnal optimal recovery, we deﬁne ωm,epτ q :“ arg maxω1:pτ,ω1qPApiq S 3pτ, ω1q
to produce the θ function for each mode m through θm,epτ q “ θepτ, ωm,epτ qq. These
functions are inserted into (5.21) to produce Km and their associated operators Qm
which are then used in the ﬁnally recovery wm “ Qmp

m1 Qm1q´1v.

ř

where the Apiq are obtained as in the ﬁrst network, illustrated in Figure 19, and θepτ, ωq,
used in the deﬁnition (5.20) of θe,ipτ q, is identiﬁed as in (5.10). To recover the modes
vi, the proposed network proceeds as in Example 3.6 by introducing the kernels

Kips, tq “ e´ pt´sq2

γ2

`

˘
cospθi,eptqq cospθi,epsqq ` sinpθi,eptqq sinpθi,epsqq

,

(5.21)

with γ “ 0.2. Deﬁning Kσ as in (4.31), the approximations w1, w2, w3 of the modes
v1, v2, v3, shown in Figures 18.10, 11 and 12, are obtained as in (4.40) with f deﬁned as
the solution of pK1 ` K2 ` K3 ` Kσqf “ v. Note that the network illustrated in Figure
21 can be interpreted as the concatenation of 2 networks. One aimed at estimating
the instantaneous phases and the other aimed at recovering the modes based on those
phases. This principle of network concatenation is evidently generic.

46

Figure 22: The KMDNet program corresponding to Figure 21. Upper left provides
the symbolic connections between the indices i, j, k, l and the time-frequency parameters
along with the functional dependencies iplq and kplq and the deﬁnition of θm,e. Beginning
with the input v in the lower left, ignoring the bottom two rows for the moment, we begin
very much as in Figure 20 moving to the right until the determination of the energies
Spkq, the determination of a graph cut and its resulting k (cid:32) l, and the resulting arg max
relation kplq :“ arg maxk(cid:32)l Spkq which amounts to kplq “ pτ, ωm,epτ qq. Returning to the
second row from the bottom, we compose the functional relations of the injection jpkq
and the arg max function ipjq determined by the relation i (cid:32) j and the energy Epiq,
to obtain ipkq and then compose this with the argmax function kplq to produce the
functional dependence iplq deﬁned by iplq “ pτ, ωm,epτ q, θm,epτ qq. Using the projection
l (cid:32) m, this determines the function θm,ep¨q corresponding to the mode label m. These
functions are inserted into (5.21) to produce Km and their associated operators Qm
which are then used in the ﬁnally recovery wm “ Qmp

m1 Qm1q´1v.

ř

47

6 Alignments calculated in L2

The calculation of the energies for our prototypical application was done with respect
to the inner product deﬁned by the inverse of the operator associated with K deﬁned in
(4.30), i.e. the energy of the mode pτ, ω, θq was deﬁned as Epτ, ω, θq “ vT K´1Kτ,ω,θK´1v
with Kτ,ω,θ deﬁned in (4.27). The computational complexity of the method can be
accelerated by (1) using the L2 inner product instead of the one deﬁned by K´1 (i.e.
deﬁning the energy of the mode pτ, ω, θq by E2pτ, ω, θq “ vT Kτ,ω,θv (2) localizing this
calculation in a time-window centered around τ and of width proportional to 1{ω.

Our experiments show that this simpliﬁcation lowers the computational complexity
of the proposed approach without impacting its accuracy. Three points justify this
observation: (1) Replacing E by E2 is equivalent to calculating mean-squared alignments
with respect to the L2-scalar product instead of the one induced by the inverse of the
operator deﬁned by K (2) In the limit where σ Ñ 8 we have E « σ´4E2, therefore E and
E2 are proportional to each other in the high noise regime (3) If ωmin “ 0 and ωmax “ 8
then Ku deﬁned by (4.28) is the identity operator on L2. We will now rigorously show
that point (3) holds true when we extend the τ domain from r0, 1s to R and when the
base waveform is trigonometric, and then show in Section 7 that this results holds true
independently of the base waveform being used.

Let us recall the Schwartz class of test functions

S :“ tf P C8pRq : sup
xPR

|xm1Dm2f pxq| ă 8, m1, m2 P Nu

and the conﬂuent hypergeometric function 1F1, deﬁned by

1F1pα, γ; zq “ 1 `

α
γ

z
1!

`

αpα ` 1q
γpγ ` 1q

z2
2!

`

αpα ` 1qpα ` 2q
γpγ ` 1qpγ ` 2q

z3
3!

` . . . ,

see e.g. see Gradshteyn and Ryzhik [37, Sec. 9.21].

Theorem 6.1. Consider extending the deﬁnition (4.28) of the kernel Ku so that the
range of ω is extended from rωmin, ωmaxs to R` and that of τ is extended from r0, 1s to
R, so that

ż

π

ż

ż

Kτ,ω,θps, tqdτ dω dθ,

s, t P R ,

Kβps, tq “

´π

R`

R

where, as before,

Kτ,ω,θps, tq :“ χτ,ω,θpsqχτ,ω,θptq,

s, t P R,

but where we have introduced a perturbation parameter 0 ď β ď 1 deﬁning the Gabor
wavelets

χτ,ω,θptq :“

´

2
α2π3

¯ 1

4 ω

`

1´β
2 cos

ωpt ´ τ q ` θ

˘

e´ ω2pt´τ q2

α2

,

t P R ,

(6.1)

deﬁning the elementary kernels. Deﬁning the scaling constant

Hpβq :“ 2β´1?

πp

?

2αq1´βΓp

qqe´ α2

2 1F1

β
2

´

β
2

,

1
2

;

α2
2

¯

,

48

let Kβ denote the integral operator

`

˘

Kβf

psq :“

ż

R

1
Hpβq

Kβps, tqf ptqdt

associated to the kernel Kβ scaled by Hpβq. Then we have the semigroup property

Kβ1Kβ2f “ Kβ1`β2f,

f P S,

β1, β2 ą 0, β1 ` β2 ă 1 ,

and

˘

`
Kβf

lim
βÑ0

pxq “ f pxq,

x P R,

f P S

where the limit is taken from above.

7 Universality of the aggregated kernel

Let

yptq :“

Nÿ

cneint

´N
be the Fourier expansion of a general 2π periodic complex-valued waveform, which we
will refer to as the base waveform, and use it to deﬁne wavelets

χτ,ω,θptq :“ ω

1´β
2 y

`
ωpt ´ τ q ` θ

˘
e´ ω2

α2 |t´τ |2

as in the β-parameterized wavelet versions of (4.25) in Theorem 6.1, using the waveform
y instead of the cosine. The following lemma evaluates the aggregated kernel

ż

π

ż

ż

Kβps, tq :“ (cid:60)

´π

R`

R

Lemma 7.1. Deﬁne the norm

χτ,ω,θpsqχ˚

τ,ω,θptqdτ dωdθ .

(7.1)

}y}2 :“

Nÿ

n“´N

e´ |n|α2

2

|cn|2

(7.2)

of the base waveform y. We have

Kβps, tq “ 2π|s ´ t|β´1

Nÿ

n“´N

anps, tq|cn|2

where

anps, tq “

?
?

π
2

α
2

?

p

2αq1´βΓp

1 ´ β
2

qe´ |n|α2

2

1F1

´

β
2

;

1
2

;

|n|α2
2

¯

.

In particular, at β “ 0 we have

K0ps, tq “ α2π2|s ´ t|´1}y}2 .

49

7.1 Characterizing the norm

ř

2

N

n“´N e´ |n|α2
ř
N
´N cneint is expressed in terms of its Fourier

|cn|2

The norm (7.2) of the function yptq :“
coeﬃcients cn. The following lemma evaluates it directly in terms of the function y.

Lemma 7.2. The norm (7.2) of the function yptq :“

ř

N
´N cneint satisﬁes

}y}2 “

ż

π

ż

π

´π

´π

Gpt, t1qyptqy˚pt1qdtdt1

where

Gpt, t1q “ 2π

sinhp α2
2 q
2 q ´ cospt ´ t1q
Remark 7.3. The norm (7.2) is clearly insensitive to the size of the high frequency
(large n) components cneint of y. On the other hand, the alternative representation of
this norm in Lemma 7.2 combined with the fact that the kernel G satisﬁes

t, t1 P r´π, πs .

coshp α2

,

sinhp α2
2 q
2 q ` 1
which, for α ě 10, implies

coshp α2

ď Gpt, t1q ď 2π

sinhp α2
2 q
2 q ´ 1

coshp α2

,

t, t1 P r´π, πs ,

1 ´ 10´21 ď Gpt, t1q ď 1 ` 10´21,

t, t1 P r´π, πs ,

implies that

ˇ
ˇ
ˇ}y}2 ´

ż

ˇ
ˇ

π

ˇ
ˇ2

ˇ
ˇ
ˇ ď 10´21

ż
ˇ
ˇ

π

ˇ
ˇ2

|yptq|dt

yptqdt

´π

´π

that is, }y}2 is exponentially close to the square of its integral.

8 Non-trigonometric waveform and iterated KMD

We will now consider the mode recovery Problem 1 generalized to the case where the
base waveform of each mode is the same known, possibly non-trigonometric, square-
integrable 2π-periodic function t Ñ yptq. The objective of this problem can be loosely
expressed as solving the following generalization of Problem 1 towards the resolution of
the more general Problem 2. We now switch the time domain from r0, 1s to r´1, 1s.

Problem 4. For m P N˚, let a1, . . . , am be piecewise smooth functions on r´1, 1s, let
θ1, . . . , θm be strictly increasing functions on r´1, 1s, and let y be a square-integrable
2π-periodic function. Assume that m and the ai, θi are unknown and the base waveform
y is known. We further assume that, for some (cid:15) ą 0, aiptq ą (cid:15) and that 9θiptq{ 9θjptq R
(for t P r´1, 1s)
r1´(cid:15), 1`(cid:15)s for all i, j, t. Given the observation vptq “
recover the modes vi :“ aiptqy

m
i“1 aiptqy

˘
θiptq

θiptq

ř

`

˘

`

.

50

Figure 23: (1) Triangle base waveform (2) EKG base waveform.

˘
ş
2π
0 yEKGpsq ds

`
yEKGptq´p2πq´1

Example 8.1. Figure 23 shows two full periods of two 2π-periodic base waveforms (tri-
angle and EKG) which we will use in our numerical experiments/illustrations. The EKG
{}yEKG}L2pr0,2πqq with yEKGptq de-
(-like) waveform is
ﬁned on r0, 2πq as (1) 0.3 ´ |t ´ π| for |t ´ π| ă 0.3 (2) 0.03 cos2p π
0.6 pt ´ π ` 1qq for
|t ´ π ` 1| ă 0.3 (3) 0.03 cos2p π
0.6 pt ´ π ´ 1qq for |t ´ π ´ 1| ă 0.3 and (4) 0 otherwise.
Our approach, summarized in Algorithm 2 and explained in the following sections,
will be to (1) use the max-pool energy S (5.19) to obtain, using (5.20), an estimate
of the phase θlowptq associated with the lowest instantaneous frequency ωlow “ 9θlow (as
described in Section 8.2) (2) iterate a micro-local KMD (presented in Section 8.1) of
the signal v to obtain a highly accurate estimate of the phase/amplitude θi, ai of their
corresponding mode vi (this iteration can achieve near machine-precision accuracies when
the instantaneous frequencies are separated) (3) Peel oﬀ the mode vi from v (4) iterate
to obtain all the modes (5) perform a last micro-local KMD of the signal for higher
accuracy. To illustrate this approach, in the next two sections we will apply it to the
signals v displayed in Figures 24 and 25, where the modes of Figure 24 are triangular
and those of Figure 25 are EKG.

8.1 The Micro-local KMD module

We will now describe the micro-local KMD module, which will form the basis for the
iterated micro-local KMD algorithm described in Section 8.3.
It takes a time τ , an
estimated phase function of i-th mode θi,e, and a signal f , not necessarily equal to v.
Suppose the i-th mode is of form viptq “ aiptqypθiptqq and is indeed a mode within f .
The module outputs, (1) an estimate apτ, θi,e, f q of the amplitude aipτ q of the mode vi
and (2) a correction δθpτ, θi,e, f q determining an updated estimate θi,epτ q ` δθpτ, θi,e, f q
of the estimated mode phase function θi,e. We assume that ai is strictly positive, that
is, aiptq ě a0, t P r´1, 1s, for some a0 ą 0.

51

Figure 24: Triangle base waveform: (1) Signal v (2) Instantaneous frequencies ωi :“ 9θi
(3) Amplitudes ai (4, 5, 6) Modes v1, v2, v3.

Indeed, given α ą 0, τ P r´1, 1s, diﬀerentiable strictly increasing functions θ0 and θe
n,c and

on r´1, 1s, and n P t0, . . . , du (we set d “ 2 in applications in this section), let χτ,θe
χτ,θe
n,s be the wavelets deﬁned by

χτ,θe

n,c ptq

:“ cospθeptqqpt ´ τ qne´
`

χτ,θe

n,s ptq

:“ sinpθeptqqpt ´ τ qne´

and let ξτ,θe be the Gaussian process deﬁned by

`

˘

2

9θ0pτ qpt´τ q
α
˘

9θ0pτ qpt´τ q
α

ξτ,θeptq :“

dÿ

`

n“0

Xn,cχτ,θe

n,c ptq ` Xn,sχτ,θe

˘
n,s ptq

2

,

,

(8.1)

(8.2)

where Xn,c, Xn,s are independent N p0, 1q random variables. The function θ0 will be
ﬁxed throughout the iterations whereas the function θe will be updated. Let fτ be the
Gaussian windowed signal deﬁned by

`

9θ0pτ qpt´τ q
α

˘

2

fτ ptq “ e´

f ptq,

t P r´1, 1s ,

and, for pn, jq P t0, . . . , du ˆ tc, su, let

Zn,jpτ, θe, f q :“ lim
σÓ0

“
E
Xn,j

ˇ
ˇξτ,θe ` ξσ “ fτ

‰

,

(8.3)

(8.4)

where ξσ is white noise, independent of ξτ,θe, with variance σ2. To compute Zn,j, observe
that since both ξτ,θe and ξσ are Gaussian ﬁelds, it follows from (3.22) that
ˇ
ˇξτ,θe ` ξσ

“ Aσpξτ,θe ` ξσq

ξτ,θe

“
E

‰

52

Figure 25: EKG base waveform: (1) Signal v (2) Instantaneous frequencies ωi :“ 9θi (3)
Amplitudes ai (4, 5, 6) Modes v1, v2, v3.

for the linear mapping

Aσ “ Qτ,θe

`

Qτ,θe ` σ2I

˘

´1 ,

where Qτ,θe : L2 Ñ L2 is the covariance operator of the Gaussian ﬁeld ξτ,θe and σ2I
is the covariance operator of ξσ. Using the characterization of the limit of Tikhonov
regularization as the Moore-Penrose inverse, see e.g. Barata and Hussein [6, Thm. 4.3],
along with the orthogonal projections connected with the Moore-Penrose inverse, we
conclude that limσÑ0 Aσ “ Pχτ,θe , where Pχτ,θe is the L2-orthogonal projection onto the
span χτ,θe :“ spantχτ,θe

n,s , n “ 0, . . . , du, and therefore

ˇ
ˇξτ,θe ` ξσ

‰

ξτ,θe

“ Pχτ,θe pξτ,θe ` ξσq .

(8.5)

n,c , χτ,θe
“
E

lim
σÑ0

Since the deﬁnition (8.2) can be written ξτ,θe “

ř

n,j Xn,jχτ,θe

n,j , summing (8.4) and

using (8.5), we obtain

ÿ

n,j

Zn,jpτ, θe, f qχτ,θe

n,j “ Pχτ,θe fτ .

(8.6)

Consider the vector function Zpτ, θe, f q P R2d`2 with components Zn,jpτ, θe, f q, the 2d`2
dimensional Gaussian random vector X with components Xn,j, pn, jq P t0, . . . , duˆtc, su,
and the p2d ` 2q ˆ p2d ` 2q matrix Aτ,θe deﬁned by

Aτ,θe
pn,jq,pn1,j1q :“ xχτ,θe

n,j , χτ,θe

n1,j1yL2r´1,1s .

(8.7)

Straightforward linear algebra along with (8.6) establish that the vector Zpτ, θe, f q can
be computed as the solution of the linear system

Aτ,θeZpτ, θe, f q “ bτ,θef,

(8.8)

53

n,j pf q :“ xχτ,θe
where bτ,θepf q is the R2d`2 vector with components bτ,θe
n,j , fτ yL2. See sub-
ﬁgures (1) and (2) of both the top and bottom of Figure 28 for illustrations of the
ˇ
“
ˇξτ,θe ` ξσ “ fτ
windowed signal fτ ptq and of its projection limσÓ0 E
in (8.5) corre-
ξτ,θe
sponding to the signals f displayed in Figures 24 and 25.

‰

To apply these formulations to construct the module, suppose that f is a single mode

so that

f ptq “ aptq cospθptqq,

`

9θ0pτ qpt´τ q
α

˘

2

fτ ptq “ e´

aptq cospθptqq ,

(8.9)

and consider the modiﬁed function

`

9θ0pτ qpt´τ q
α

¯fτ ptq “ e´

˜

˘
2

dÿ

n“0

apnqpτ q
n!

¸

pt ´ τ qn

cospθptqq

(8.10)

obtained by replacing the function a with the ﬁrst d ` 1 terms of its Taylor series about
τ . In what follows, we will use the expression « to articulate an informal approximation
analysis. It is clear that ¯fτ P χτ,θe and, since
n,j , fτ ´ ¯fτ yL2 «
0, @pn, jq and therefore Pχτ,θe fτ « ¯fτ , and therefore (8.6) implies that

is small, that xχτ,θe

α
9θ0pτ q

ÿ

j1

Z0,j1pτ, θe, f qχτ,θe

0,j1 ptq « ¯fτ ptq,

t P r´1, 1s ,

(8.11)

which by (8.10) implies that

Z0,j1pτ, θe, f qχτ,θe

0,j1 ptq « e´

`

9θ0pτ qpt´τ q
α

˘

2

ÿ

j1

which implies that

apτ q cospθptqq,

t « τ ,

(8.12)

Z0,cpτ, θe, f q cospθeptqq ` Z0,spτ, θe, f q sinpθeptqq « apτ q cospθptqq,

t « τ .

(8.13)

Setting θδ :“ θ ´ θe as the approximation error, using the cosine summation formula, we
obtain

`
Z0,cpτ, θe, f q cospθeptqq`Z0,spτ, θe, f q sinpθeptqq « apτ q

˘
cospθδptqq cospθeptqq´sinpθδptqq sinpθeptq

.

However, t « τ implies that θδptq « θδpτ q, so that we obtain

`
Z0,cpτ, θe, f q cospθeptqq`Z0,spτ, θe, f q sinpθeptqq « apτ q

˘
cospθδpτ qq cospθeptqq´sinpθδpτ qq sinpθeptq

,

which, since 9θeptq positive and bounded away from 0, implies that

Z0,cpτ, θe, f q « apτ q cospθδpτ qq
Z0,spτ, θe, f q « ´apτ q sinpθδpτ qq .

54

Consequently, writing

b

apτ, θe, f q

:“

Z2

0,cpτ, θe, f q ` Z2

0,spτ, θe, f q

`

˘

δθpτ, θe, f q

:“ atan2

´ Z0,spτ, θe, f q, Z0,cpτ, θe, f q

,

(8.14)

we obtain that apτ, θe, f q « apτ q and δθpτ, θe, f q « θδpτ q. We will therefore use apτ, θe, f q
to estimate the amplitude apτ q of the mode f using the estimate θe and δθpτ, θ, f q to
estimate the mode phase θ through θpτ q “ θepτ q ` θδpτ q « θepτ q ` δθpτ, θe, f q. Unless
otherwise speciﬁed, Equation (8.14) will take d “ 2. Experimental evidence indicates
that d “ 2 is a sweet spot in the sense that d “ 0 or d “ 1 yields less ﬁtting power, while
larger d entails less stability. Iterating this reﬁnement process will allow us to achieve
near machine-precision accuracies in our phase/amplitude estimates. See sub-ﬁgures
(1) and (2) of the top and bottom of Figure 29 for illustrations of aptq, apτ, θe, vqptq,
θptq´θeptq and δθpτ, θe, vqptq corresponding to the ﬁrst mode v1 of the signals v displayed
in Figures 24.4 and 25.4.

8.2 The lowest instantaneous frequency

Figure 26: Max-squeezing with the EKG base waveform and derivation of the instan-
taneous phase estimates θi,e. (1,2) pτ, ωq Ñ Spτ, ω, vq and identiﬁcation of Alow (3, 4)
pτ, ωq Ñ Spτ, ω, v ´v1,eq and identiﬁcation of its Alow (5,6) pτ, ωq Ñ Spτ, ω, v ´v1,e ´v2,eq
and identiﬁcation of its Alow.

We will use the max-pool network illustrated in the right-hand side of Figure 17 and
the module of Section 8.1 to design a module taking a signal v as input and producing,
as output, an estimate of the instantaneous phase θlowpvq of the mode of v having the

55

lowest instantaneous frequency. We restrict our presentation to the situation where the
instantaneous frequencies 9θi do not cross each other. The main steps of the computation
performed by this module are as follows. Let Spτ, ω, vq be the max-pool energy deﬁned
as in (5.19), where now it is useful to indicate its dependence on v.

Let Alow be a subset of the time-frequency domain pτ, ωq identiﬁed (as in Figure
26.2) as a narrow sausage around the lowest instantaneous frequency deﬁned by the
local maxima of the Spτ, ω, vq. If no modes can be detected (above a given threshold)
in Spτ, ω, vq then we set θlowpvq “ H. Otherwise we let

`

ωlowpτ q :“ ωe

τ, argmaxω:pτ,ωqPAlow

˘
Spτ, ωq

(8.15)

be the estimated instantaneous frequency of the mode having the lowest instantaneous
frequency and, with θe deﬁned as in (5.6), let

θlowpτ q :“ θepτ, ωlowpτ qq

(8.16)

be the corresponding estimated instantaneous phase (obtained as in (5.20)).

8.3 The iterated micro-local KMD algorithm.

Figure 27: Modular representation of Algorithm 2, described in this section. The blue
module represents the estimation of the lowest frequency as illustrated in Figure 26. The
brown module represents the iterative estimation of the mode with lowest instantaneous
frequency of lines 10 through 14 of Algorithm 2. The yellow module represents the
iterative reﬁnement of all the modes in lines 21 through 27. The brown and yellow
modules used to reﬁne phase/amplitude estimates use the same code.

56

Algorithm 2 Iterated micro-local KMD.

1: i Ð 1
2: vp1q Ð v
3: while true do
4:

if θlowpvpiqq “ H then

break loop

else

θi,e Ð θlowpvpiqq

end if
ai,epτ q Ð 0
repeat

for j in t1, ..., iu do

ř

k‰j,kďi ak,eypθk,eq
˘

{c1
τ, θj,e, vj,res

vj,res Ð v ´ aj,e ¯ypθj,eq ´
˘
`
τ, θj,e, vj,res
aj,epτ q Ð a
`
θj,epτ q Ð θj,epτ q ` 1
2 δθ
ˇ
`
ˇδθ
ř

τ, θi,e, vi,res
jďi aj,eypθj,eq

˘ˇ
ˇ ă (cid:15)1

17:

15:

16:

end for
until supi,τ
vpi`1q Ð v ´
i Ð i ` 1
18:
19: end while
20: m Ð i ´ 1
21: repeat
22:

for i in t1, ..., mu6 do

vi,res Ð v ´ ai,e ¯ypθi,eq ´
˘
`
τ, θi,e, vi,res
ai,epτ q Ð a
`
θi,epτ q Ð θi,epτ q ` 1
2 δθ
`

ř

j‰i aj,eypθj,eq

˘

τ, θi,e, vi,res
˘ˇ
ˇ ă (cid:15)2

end for
26:
27: until supj,τ
28: Return the modes vi,eptq Ð ai,eptqypθi,eptqq for i “ 1, ..., m

τ, θj,e, vj,res

ˇ
ˇδθ

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

23:

24:

25:

The method of estimating the lowest instantaneous frequency, described in Section
8.2, provides a foundation for the iterated micro-local KMD algorithm, Algorithm 2. We
now describe Algorithm 2, presented in its modular representation in Figure 27, using
Figures 26, 28 and 29. To that end, let

8ÿ

yptq “ c1 cosptq `

cn cospnt ` dnq

(8.17)

n“2

be the Fourier representation of the base waveform y (which, without loss of generality,

6 This repeat loop, used to reﬁne the estimates, is optional. Also, all statements in Algorithms with

dummy variable τ imply a loop over all values of τ in the mesh T .

57

has been shifted so that the ﬁrst sine coeﬃcient is zero) and write

¯yptq :“ yptq ´ c1 cosptq

(8.18)

for its overtones.

ř

Let us describe how lines 1 to 19 provide reﬁned estimates for the amplitude and the
phase of each mode vi, i P t1, . . . , mu of the signal v. Although the overtones of y prevent
us from simultaneously approximating all the instantaneous frequencies 9θi from the max-
pool energy of the signal v, since the lowest mode vlow “ alowypθlowq can be decomposed
into the sum vlow “ alowc1 cospθlowq ` alow ¯ypθlowq of a signal alowc1 cospθlowq with a
cosine waveform plus the signal alow ¯ypθlowq containing its higher frequency overtones, the
method of Section 8.2 can be applied to obtain an estimate θlow,e of θlow and (8.14) can be
applied to obtain an estimate alow,ec1 of alowc1 producing an estimate alow,ec1 cospθlow,eq
of the primary component alowc1 cospθlowq of the ﬁrst mode. Since c1 is known, this
estimate produces the estimate alow,e ¯ypθlow,eq for the overtones of the lowest mode. Recall
that we calculate all quantities over the interval r´1, 1s in this setting. Estimates near
the borders, ´1 and 1, will be less precise, but will be reﬁned in the following loops.
To improve the accuracy of this estimate, in lines 13 and 14 the micro local KMD of
Section 8.1 is iteratively applied to the residual signal of every previously identiﬁed mode
k‰j,kďi ak,eypθk,eq, consisting of the signal v with the estimated
vj,res Ð v ´ aj,e ¯ypθj,eq ´
modes k ‰ j as well as the overtones of estimated mode j removed. This residual is
the sum of the estimation of the isolated base frequency component of vj and
jąi vj.
The rate parameter 1{2 in line 14 is to avoid overcorrecting the phase estimates, while
the parameters (cid:15)1 and (cid:15)2 in lines 16 and 27 are pre-speciﬁed accuracy thresholds. The
resulting estimated lower modes are then removed from the signal to determine the
residual vpi`1q :“ v ´

jďi aj,eypθj,eq in line 17.

Iterating this process, we peel oﬀ an estimate ai,eypθi,eq of the mode corresponding
to the lowest instantaneous frequency of the residual vpiq :“ v ´
jďi´1 aj,eypθj,eq of the
signal v obtained in line 17, removing the interference of the ﬁrst i ´ 1 modes, including
their overtones, in our estimate of the instantaneous frequency and phase of the i-th
mode. See Figure 26 for the evolution of the Alow sausage as these modes are peeled oﬀ.
See sub-ﬁgures (3) and (5) of the top and bottom of Figure 28 for the results of peeling
oﬀ the ﬁrst two estimated modes of the signal v corresponding to both Figures 24 and
25 and sub-ﬁgures (4) and (6) for the results of the corresponding projections in (8.5).
See sub-ﬁgures (3) and (4) of the top and bottom of Figure 29 for amplitude and its
estimate of the results of peeling oﬀ the ﬁrst estimated mode and sub-ﬁgures (5) and (6)
corresponding to peeling oﬀ the ﬁrst two estimated modes of the signal v corresponding
to both Figures 24 and 25.

ř

ř

ř

After the amplitude/phase estimates ai,e, θi,e, i P t1, . . . , mu, have been obtained
in lines 1 to 19, we have the option to further improve our estimates in a ﬁnal opti-
mization loop in lines 21 to 27. This option enables us to achieve even higher accu-
racies by iterating the micro local KMD of Section 8.1 on the residual signals vi,res Ð

58

Figure 28: Top: v is as in Figure 24 (the base waveform is triangular). Bottom: v is as in
Figure 25 (the base waveform is EKG). Both top and bottom: d “ 2, (1) The windowed
ˇ
‰
“
ˇξτ,θ2,e `ξσ “
signal vτ (2) limσÓ0 E
ξτ,θ1,e
‰
‰
.
pv ´ v1,eqτ

“
(3) pv´v1,eqτ (4) limσÓ0 E
“
(5) pv ´ v1,e ´ v2,eqτ (6) limσÓ0 E

ˇ
ˇξτ,θ3,e ` ξσ “ pv ´ v1,e ´ v2,eqτ

ˇ
ˇξτ,θ1,e `ξσ “ vτ

ξτ,θ2,e

ξτ,θ3,e

ř

v ´ ai,e ¯ypθi,eq ´
j ­“ i and estimated overtones of the mode i removed.

j‰i aj,eypθj,eq, consisting of the signal v with all the estimated modes

The proposed algorithm can be further improved by (1) applying a Savitsky-Golay
ﬁlter to locally smooth (de-noise) the curves corresponding to each estimate θi,e (which
corresponds to reﬁning our phase estimates through GPR ﬁltering) (2) starting with a
larger α (to decrease interference from other modes/overtones) and slowly reducing its
value in the optional ﬁnal reﬁnement loop (to further localize our estimates after other
components, and hence interference, have been mostly eliminated).

8.4 Numerical experiments

Here we present results for both the triangle and EKG base waveform examples. As
discussed in the previous section, these results are visually displayed in Figures 28 and

59

Figure 29: Top: v is as in Figure 24 (the base waveform is triangular). Bottom: v is as in
Figure 25 (the base waveform is EKG). Both top and bottom: τ “ 0. (1) the amplitude
of the ﬁrst mode a1ptq and its local Gaussian regression estimation apτ, θ1,e, vqptq (2) the
error in estimated phase of the ﬁrst mode θ1ptq ´ θ1,eptq and its local Gaussian regression
δθpτ, θ1,e, vqptq (3, 4) are as (1,2) with v and θ1,e replaced by v ´ v1,e and θ2,e (5,6) are
as (1,2) with v and θ1,e replaced by v ´ v1,e ´ v2,e and θ3,e.

29.

8.4.1 Triangle wave example

The base waveform is the triangle wave displayed in Figure 23. We observe the signal
1
v on a mesh spanning r´1, 1s spaced at intervals of
5000 and aim to recover each mode
vi over this time mesh. We take α “ 25 within the ﬁrst reﬁnement loop corresponding
to lines 1 to 19 and slowly decreased it to 6 in the ﬁnal loop corresponding to lines
21 to 27. The amplitudes and frequencies of each of the modes are shown in Figure
24. The recovery errors of each mode as well as their amplitude and phase functions
over the whole interval r´1, 1s and the interior third r´ 1
3 s are displayed in Table 5

3 , 1

60

and 6 respectively.
In the interior third of the interval, errors were found to be on
the order of 10´9 for the ﬁrst signal component and approximately 10´7 for the higher
two. However, over the full interval, the corresponding ﬁgures are in the 10´4 and 10´3
ranges due to recovery errors near the boundaries, ´1 and 1, of the interval. Still, a plot
superimposing vi and vi,e would visually appear to be one curve over r´1, 1s due to the
negligible recovery errors.

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
5.47 ˆ 10´4
6.42 ˆ 10´4
5.83 ˆ 10´4

}vi,e´vi}L8
}vi}L8
3.85 ˆ 10´3
2.58 ˆ 10´3
6.29 ˆ 10´3

}ai,e´ai}L2
}ai}L2
2.80 ˆ 10´4
3.80 ˆ 10´5
2.19 ˆ 10´4

}θi,e´θi}L2
4.14 ˆ 10´5
1.85 ˆ 10´4
6.30 ˆ 10´5

Table 5: Signal component recovery errors in the triangle base waveform example over
r´1, 1s.

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
1.00 ˆ 10´8
2.74 ˆ 10´7
2.37 ˆ 10´7

}vi,e´vi}L8
}vi}L8
2.40 ˆ 10´8
2.55 ˆ 10´7
3.67 ˆ 10´7

}ai,e´ai}L2
}ai}L2
7.08 ˆ 10´9
1.87 ˆ 10´8
1.48 ˆ 10´7

}θi,e´θi}L2
6.52 ˆ 10´9
2.43 ˆ 10´7
1.48 ˆ 10´7

Table 6: Signal component recovery errors in the triangle base waveform example over
r´ 1

3 , 1
3 s.

8.4.2 EKG wave example

The base waveform is the EKG wave displayed in Figure 23. We use the same discrete
mesh as in the triangle case. Here, we took α “ 25 in the loop corresponding to lines
1 to 19 and slowly decreased it to 15 in the ﬁnal loop corresponding to lines 21 to 27.
The amplitudes and frequencies of each of the modes are shown in Figure 25, while
the recovery error of each mode as well as their amplitude and phase functions are
shown both over the whole interval r´1, 1s and the interior third r´ 1
3 s in Tables 7 and
8 respectively. Within the interior third of the interval, amplitude and phase relative
errors are found to be on the order of 10´4 to 10´5 in this setting. However, over r´1, 1s,
the mean errors are more substantial, with amplitude and phase estimates in the 10´1
to 10´3 range. Note the high error rates in L8 stemming from errors in placement of the
tallest peak (the region around which is known as the R wave in the EKG community).
In the center third of the interval, vi,e and vi are visually indistinguishable due to the
small recovery errors.

3 , 1

61

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
5.66 ˆ 10´2
4.61 ˆ 10´2
1.34 ˆ 10´1

}vi,e´vi}L8
}vi}L8
1.45 ˆ 10´1
2.39 ˆ 10´1
9.39 ˆ 10´1

}ai,e´ai}L2
}ai}L2
4.96 ˆ 10´3
2.35 ˆ 10´2
9.31 ˆ 10´3

}θi,e´θi}L2
8.43 ˆ 10´3
1.15 ˆ 10´2
2.69 ˆ 10´2

Table 7: Signal component recovery errors on r´1, 1s in the EKG base waveform example.

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
1.80 ˆ 10´4
4.35 ˆ 10´4
3.63 ˆ 10´4

}vi,e´vi}L8
}vi}L8
3.32 ˆ 10´4
5.09 ˆ 10´4
1.08 ˆ 10´3

}ai,e´ai}L2
}ai}L2
3.52 ˆ 10´5
3.35 ˆ 10´5
7.23 ˆ 10´5

}θi,e´θi}L2
2.85 ˆ 10´5
7.18 ˆ 10´5
6.26 ˆ 10´5

Table 8: Signal component recovery errors on r´ 1
ple.

3 , 1

3 s in the EKG base waveform exam-

9 Unknown base waveforms

Here we consider the extension, Problem 2, of the mode recovery problem, Problem 1,
to the case where the periodic base waveform of each mode is unknown and may be
diﬀerent across modes. That is, given the observation

mÿ

`

vptq “

aiptqyi

θiptq

˘
,

t P r´1, 1s,

(9.1)

i“1
`

˘
θiptq

recover the modes vi :“ aiptqyi
. To avoid ambiguities caused by overtones when
the waveforms yi are not only non-trigonometric but also unknown, we will assume that
the corresponding functions pk 9θiqtPr´1,1s and pk1 9θi1qtPr´1,1s are distinct for i ­“ i1 and
k, k1 P N˚, that is, they may be equal for some t but not for all t. We represent the i-th
base waveform yi through its Fourier series

yiptq “ cosptq `

˘
`
ci,pk,cq cospktq ` ci,pk,sq sinpktq

,

kmaxÿ

k“2

(9.2)

that, without loss of generality has been scaled and translated. Moreover, since we
operate in a discrete setting, without loss of generality we can also truncate the series
at a ﬁnite level kmax, which is naturally bounded by the inverse of the resolution of
To illustrate our approach, we consider the signal v “
the discretization in time.
v1 `v1 `v3 and its corresponding modes vi :“ aiptqyi
displayed in Figure 30, where
the corresponding base waveforms y1, y2 and y3 are shown in Figure 31 and described in
Section 9.3.

˘
`
θiptq

62

Figure 30: (1) Signal v (the signal is deﬁned over r´1, 1s but displayed over r0, 0.4s for
visibility) (2) Instantaneous frequencies ωi :“ 9θi (3) Amplitudes ai (4, 5, 6) Modes v1,
v2, v3 over r0, 0.4s (mode plots have also been zoomed in for visibility).

Figure 31: (1) y1 (2) y2 (3) y3

Figure 32: High level structure of Algorithm 3 for the case when the waveforms are
unknown.

9.1 Micro-local waveform KMD

We now describe the micro-local waveform KMD, Algorithm 3, which takes as inputs
a time τ , estimated instantaneous amplitude and phase functions t Ñ aptq, θptq, and a
signal v, and outputs an estimate of the waveform yptq associated with the phase function
θ. The proposed approach is a direct extension of the one presented in Section 8.1 and

63

the shaded part of Figure 32 shows the new block which will be added to Algorithm 2,
the algorithm designed for the case when waveforms are non-trigonometric and known.
As described below this new block produces an estimator yi,e of the waveform yi from
an estimate θi,e of the phase θi.

Given α ą 0, τ P r´1, 1s, and diﬀerentiable function t Ñ θptq, deﬁne the Gaussian

process

`
ξy
τ,θptq “ e´

9θ0pτ qpt´τ q
α

˘
2´

X y

1,c cos

kmaxÿ

`

`

˘
θptq

`

X y

k,c cos

`

˘

kθptq

`X y

k,s sin

`

˘˘¯

kθptq

, (9.3)

k“2

where X y

1,c, X y

k,c, and X y

k,s are independent N p0, 1q random variables. Let

vτ ptq :“ e´
be the windowed signal, and deﬁne

Zy

k,jpτ, θ, vq :“ lim
σÓ0

and, for k P t2, . . . , kmaxu, j P tc, su, let

`

˘

9θ0pτ qpt´τ q
α

2

vptq,

τ P r´1, 1s,

“
E

ˇ
ˇξy
τ,θ ` ξσ “ vτ

‰

,

X y
k,j

ck,jpτ, θ, vq :“

Zy
Zy

k,jpτ, θ, vq
1,cpτ, θ, vq

.

(9.4)

(9.5)

(9.6)

When the assumed phase function θ :“ θi,e is close to the phase function θi of the i-th
mode of the signal v in the expansion (9.1), ck,jpτ, θi,e, vq yields an estimate of the Fourier
coeﬃcient ci,pk,jq (9.2) of the i-th base waveform yi at time t “ τ . This waveform recovery
is susceptible to error when there is interference in the overtone frequencies (that is for
the values of τ at which j1 9θi1 « j2 9θi2 for i1 ă i2). However, since the coeﬃcient ci,pk,jq
is independent of time, we can overcome this by computing ck,jpτ, θi,e, vq at each time
τ and take the most common approximate value over all τ as follows. Let T Ă r´1, 1s
be the ﬁnite set of values of τ used in the numerical discretization of the time axis with
N :“ |T | elements. For an interval I Ă R, let

TI :“ tτ P T |ck,jpτ, θi,e, vq P Iu ,

(9.7)

and let NI :“ |TI | denote the number of elements of TI . Let Imax be a maximizer of the
function I Ñ NI over intervals of ﬁxed width L, and deﬁne the estimate

ck,jpθi,e, vq :“

#

ř

1
NImax
0

τ PTImax

ck,jpτ, θi,e, vq

,

,

NImax

N ě 0.05
N ă 0.05

NImax

,

(9.8)

of the Fourier coeﬃcient ci,pk,jq to be the average of the values of ck,jpτ, θi,e, vq over τ P
TImax. The interpretation of the selection of the cutoﬀ 0.05 is as follows: if NImax
is small
then there is interference in the overtones at all time r´1, 1s and no information may be
obtained about the corresponding Fourier coeﬃcient. When the assumed phase function
is near that of the lowest frequency mode v1, which we write θ :“ θ1,e, Figures 33.2 and 4
shows zoomed-in histograms of the functions τ Ñ cp3,cqpτ, θ1,e, vq and τ Ñ cp3,sqpτ, θ1,e, vq
displayed in Figures 33.1 and 3.

N

64

Figure 33: (1) A plot of the function τ Ñ cp3,cqpτ, θ1,e, vq (2) A histogram (cropping
outliers) with bin width 0.002 of cp3,cqpτ, θ1,e, vq values. The true value c1,p3,cq is 1{9 since
y1 is a triangle wave. (3) A plot of the function τ Ñ cp3,sqpτ, θ1,e, vq (2) A histogram
(cropping outliers) with bin width 0.002 of cp3,sqpτ, θ1,e, vq values. The true value c1,p3,sq
of this overtone is 0.

On the interval width L.
In our numerical experiments, the recovered modes and
waveforms show little sensitivity to the choice of L. In particular, we set L to be 0.002,
whereas widths between 0.001 and 0.01 yield similar results. The rationale for the rough
selection of the value of L is as follows. Suppose v “ cospωtq and v1 “ v ` cosp1.5ωtq.
Deﬁne the quantity

`

˘
c2,cpτ, θ, v1q ´ c2,cpτ, θ, vq

max
τ

,

(9.9)

with the intuition of approximating the maximum corruption by the cosp1.5ωtq term in
the estimated ﬁrst overtone. This quantity provides a good choice for L and is mainly
dependent on the selection of α and marginally on ω. For our selection of α “ 10, we
numerically found its value to be approximately 0.002.

9.2 Iterated micro-local KMD with unknown waveforms algorithm

Except for the steps discussed in Section 9.1, Algorithm 3 is identical to Algorithm 2.
As illustrated in Figure 32, we ﬁrst identify the lowest frequency of the cosine component
of each mode (lines 6 and 7 in Algorithm 3). Next, from lines 10 to 18, we execute a
similar reﬁnement loop as in Algorithm 2 with the addition of an application of micro-
local waveform KMD on lines 15 and 16 to estimate base waveforms. Finally, once each
mode has been identiﬁed, we again apply waveform estimation in lines 28-29 (after nearly
eliminating other modes and reducing interference in overtones for higher accuracies).

9.3 Numerical experiments

To illustrate this learning of the base waveform of each mode, we take vptq “
where the lowest frequency mode a1ptqy1pθ1ptqq has the (unknown) triangle waveform y1
of Figure 23. We determine the waveforms yi, i “ 2, 3, randomly by setting ci,pk,jq to be

7 This repeat loop, used to reﬁne the estimates, is optional. Also, all statements in Algorithms with

dummy variable τ imply a loop over all values of τ in the mesh T .

65

ř
3
i“1 aiptqyipθiptqq,

Algorithm 3 Iterated micro-local KMD with unknown waveforms.
1: i Ð 1 and vp1q Ð v
2: while true do
3:

if θlowpvpiqq “ H then

break loop

else

θi,e Ð θlowpvpiqq
yi,e Ð cosptq

end if
ai,epτ q Ð 0
repeat

for l in t1, ..., iu do

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

19:

17:

18:

end for
until supl,τ
vpi`1q Ð v ´
i Ð i ` 1
20:
21: end while
22: m Ð i ´ 1
23: repeat
24:

ř

˘

`

k‰l,kďi ak,eyl,epθk,eq

vl,res Ð v ´ al,e ¯yl,epθl,eq ´
`
˘
τ, θl,e, vl,res
al,epτ q Ð a
{c1
`
θl,epτ q Ð θl,epτ q ` 1
2 δθ
τ, θl,e, vl,res
˘
cl,pk,jq,e Ð ck,j
θl,e, vl,res
`
ř
kmax
cl,pk,cq,e cospk¨q ` cl,pk,sq,e sinpk¨q
yl,ep¨q Ð cosp¨q `
k“2
˘ˇ
ˇ
`
ˇδθ
ˇ
ř

ă (cid:15)1

˘

τ, θl,e, vl,res
jďi aj,eyi,epθj,eq

for i in t1, . . . , mu7 do

ř

`

25:

26:

27:

28:

29:

j‰i aj,eyj,epθj,eq
˘

vi,res Ð v ´ ai,e ¯yi,epθi,eq ´
`
ai,epτ q Ð a
θi,epτ q Ð θi,epτ q ` 1
ci,pk,jq,e Ð ck,j
yi,ep¨q Ð cosp¨q `
ˇ
ˇδθ

˘
τ, θi,e, vi,res
`
2 δθ
τ, θi,e, vi,res
ř
θi,e, v ´
j‰i aj,eyj,epθj,eq
`
ř
kmax
ci,pk,cq,e cospk¨q ` ci,pk,sq,e sinpk¨q
k“2
˘ˇ
ˇ ă (cid:15)2

end for
30:
31: until supi,τ
32: Return the modes vi,eptq Ð ai,eptqypθi,eptqq for i “ 1, ..., m

`
τ, θi,e, vi,res

˘

˘

zero with probability 1{2 or to be a random sample from N p0, 1{k4q with probability 1{2,
for k P t2, . . . , 7u and j P tc, su. The waveforms y1, y2, y3 thus obtained are illustrated
in Figure 31. The modes v1, v2, v3, their amplitudes and instantaneous frequencies are
shown in Figure 30.

We use the same mesh and the same value of α values as in Section 8.4.1. The main
source of error for the recovery of the ﬁrst mode’s base waveform stems from the fact
that a triangle wave has an inﬁnite number of overtones, while in our implementation,

66

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
6.31 ˆ 10´3
3.83 ˆ 10´4
3.94 ˆ 10´4

}vi,e´vi}L8
}vi}L8
2.39 ˆ 10´2
1.08 ˆ 10´3
1.46 ˆ 10´3

}ai,e´ai}L2
}ai}L2
9.69 ˆ 10´5
5.75 ˆ 10´5
9.53 ˆ 10´5

}θi,e´θi}L2
1.41 ˆ 10´5
1.16 ˆ 10´4
6.77 ˆ 10´5

}yi,e´yi}L2
}yi}L2
6.32 ˆ 10´3
3.76 ˆ 10´4
3.80 ˆ 10´4

Table 9: Signal component recovery errors over r´1, 1s when the base waveforms are
unknown

we estimate only the ﬁrst 15 overtones. Indeed, the L2 recovery error of approximating
the ﬁrst 16 tones of the triangle wave is 3.57 ˆ 10´4, while the full recovery errors are
presented in Table 9. We omitted the plots of the yi,e as they are visually indistinguish-
able from those of the yi. Note that errors are only slightly improved away from the
borders as the majority of it is accounted for by the waveform recovery error.

10 Crossing frequencies, vanishing modes, and noise

The algorithm introduced in this section addresses the following generalization of the
mode recovery Problem 4, allowing for crossing frequencies, vanishing modes and noise.
The purpose of the δ, (cid:15)-condition in Problem 5 is to prevent a long overlap of the instan-
taneous frequencies of distinct modes.

Problem 5. For m P N˚, let a1, . . . , am be piecewise smooth functions on r´1, 1s, and let
θ1, . . . , θm be strictly increasing functions on r´1, 1s such that, for (cid:15) ą 0 and δ P r0, 1q,
the length of t with 9θiptq{ 9θjptq P r1 ´ (cid:15), 1 ` (cid:15)s is less than δ. Assume that m and the ai, θi
are unknown, and the square-integrable 2π-periodic base waveform y is known. Given the
` vσptq (for t P r´1, 1s), where vσ is a realization
θiptq
observation vptq “
of white noise with variance σ2, recover the modes viptq :“ aiptqy

m
i“1 aiptqy

˘
θiptq

ř

`

`

˘

.

We will use the following two examples to illustrate our algorithm, in particular
the identiﬁcation of the lowest frequency ωlowpτ q, at each time τ , and the process of
obtaining estimates of modes.

Example 10.1. Consider the problem of recovering the modes of the signal v “ v1 `v2 `
v3 ` vσ shown in Figure 34. Each mode has a triangular base waveform. In this example
v3 has the highest frequency and its amplitude vanishes over t ą ´0.25. The frequencies
of v1 and v2, cross around t “ 0.25. vσ „ N p0, σ2δpt ´ sqq is white noise with standard
deviation σ “ 0.5. While the signal-to-noise ratio is Varpv1 ` v2 ` v3q{ Varpvσq “ 13.1,
the SNR ratio against each of the modes Varpviq{ Varpvσq, i “ 1, 2, 3, is 2.7, 7.7, and
10.7 respectively.

Example 10.2. Consider the signal v “ v1 ` v2 ` v3 ` vσ shown in Figure 35. Each
mode has a triangular base waveform. In this example, the vanishing mode, v1, has the

67

Figure 34: (1) Signal v (2) Instantaneous frequencies ωi :“ 9θi (3) Amplitudes ai (4, 5,
6) Modes v1, v2, v3.

Figure 35: (1) Signal v (2) Instantaneous frequencies ωi :“ 9θi (3) Amplitudes ai.

lowest frequency over t À ´0.25 but then its amplitude vanishes over t Á ´0.25. The
frequencies of v2 and v3, cross around t “ 0.25. vσ „ N p0, σ2δpt ´ sqq is white noise
with standard deviation σ “ 0.5.

Examples 10.1 and 10.2 of Problem 5 cannot directly be solved with Algorithm 2
(where the mode with the lowest frequency is iteratively identiﬁed and peeled oﬀ) be-
cause the lowest observed instantaneous frequency may no longer be associated with
the same mode at diﬀerent times in r´1, 1s (due to vanishing amplitudes and crossing
frequencies). Indeed, as can be seen in Figure 34.2, the mode v1 will have lowest instan-
taneous frequency at times prior to the intersection, i.e. over t À 0.25, while the lowest
frequency is associated with v2 over t Á 0.25. Further, in Example 10.2 which has modes
with frequencies illustrated in Figure 35.2, Figure 35.3 shows that the amplitude of the
mode v1 vanishes for t Á ´0.5 and therefore will not contribute to a lowest frequency
estimation in that interval. Figure 35.2 implies that v1 will appear to have the lowest
instantaneous frequency for t À ´0.5, v2 will appear to for t Á 0.25, and v3 otherwise.

The algorithms introduced in this section will address these challenges by ﬁrst es-
timating the lowest frequency mode at each point of time in r´1, 1s and dividing the

68

domain into intervals with continuous instantaneous frequency and 9θlow « ωlow in Algo-
rithm 4. Divisions to r´1, 1s can be caused by either a mode vanishing or a frequency
intersection. The portions of modes corresponding to these resulting intervals with iden-
tiﬁed instantaneous frequencies are called mode fragments. Next, Algorithm 5 extends
the domain of these fragments to the maximal domain such that the instantaneous fre-
quency is continuous and 9θlow « ωlow, thus determining what are called mode segments.
The diﬀerence between fragments and segments is elaborated in the discussion of Figure
36. Furthermore, in Algorithm 6, the segments that are judged to be an artifact of noise
or a mode intersection are removed. After segments are grouped by the judgment of
the user of the algorithm into which belong to the same mode, they are then joined via
interpolation to create estimates of full modes. Finally, in Algorithm 7, mode estimates
are reﬁned as in the ﬁnal reﬁnement loop in Algorithm 2.

10.1

Identifying modes and segments

Algorithm 4, which follows, presents the main module mmodepv, V, Vsegq composing Al-
gorithm 7. The input of this module is the original signal v, a set of (estimated) modes
V :“ tvi,e : r´1, 1s Ñ Ru, and a set Vseg :“ tvi,e : Ji,e Ñ Ru of (estimated) seg-
ments vi,e, where each mode is deﬁned in terms of its amplitude ai,e and phase θi,e as
vi,eptq :“ ai,eptqypθi,eptqq, and each segment is deﬁned in terms of its amplitude ai,e and
phase θi,e as the function vi,eptq :“ ai,eptqypθi,eptqq on its closed interval domain Ji,e. In
Algorithm 4 we consider a uniform mesh T Ă r´1, 1s with mesh spacing δt and deﬁne a
mesh interval ra, bs :“ tt P T : a ď t ď bu, using the same notation for a mesh interval
as a regular closed interval. In particular, both the modes and segments vi,e, vi,e con-
tain, as data, their amplitudes ai,e, ai,e and phase functions θi,e, θi,e, while the segments
additionally contain as data their domain Ji,e. Moreover, their frequencies ωi,e, ωi,e can
also be directly extracted since they are a function of their phase functions. The output
of this module is an updated set of modes V out and segments V out
seg . The ﬁrst step of
this module (lines 2 to 5 of Algorithm 4) is to compute, for each time τ P r´1, 1s, the
residual

ÿ

ÿ

vτ :“ v ´

vi,e ´

vi,ePV

vi,ePVseg:τ PJi,e

vi,e
τ

(10.1)

of the original signal after peeling oﬀ the modes and localized segments, where the local-
ized segment

τ ptq :“ ai,epτ qe´
vi,e

`

ωi,epτ qpt´τ q
α

˘

2

`

˘
pt ´ τ qωi,epτ q ` θi,epτ q

,

y

t P r´1, 1s, τ P Ji,e, (10.2)

deﬁned from the amplitude, phase and frequency of segment vi,e, is well-deﬁned on the
whole domain r´1, 1s when τ P Ji,e. Extending vi,e
so that it is deﬁned as the zero
τ
function for τ R Ji,e, (10.1) appears more simply as
ÿ

ÿ

vi,e
τ

.

(10.3)

vτ :“ v ´

vi,e ´

V

Vseg

69

Note that unlike previous sections where the function θ0, common throughout many
iterations, would be determining the width parameter 9θ0pτ q in the exponential in (10.2),
here the latest frequency estimate ωi,e is used. The peeling (10.3) of the modes, as well as
the segments, oﬀ of the signal v is to identify other segments with higher instantaneous
frequencies.

Next, in line 6 of Algorithm 4, we compute the lowest instantaneous frequency
ωlowpτ, vτ q of vτ as in (8.15), where Alow is determined either by the user or a set of
rules, e.g. we identify ωlowpτ, vτ q as the lowest frequency local maxima of the energy
Spτ, ¨, vτ q that is greater than a set threshold (cid:15)0 (in our implementations, we set this
threshold as a ﬁxed fraction of maxτ,ω Spτ, ω, vq). If no energies are detected above this
given threshold in Spτ, ¨, vτ q we set ωlowpτ, vτ q “ H. We use the abbreviation ωlowpτ q
for ωlowpτ, vτ q. Figure 36.2 shows ωlowpτ q derived from S (Figure 36.1) in Example
10.2. Then, using the micro-local KMD approach of Section 8.1 with (the maximum
polynomial degree) d set to 0, lines 8 and 9 of Algorithm 4 compute an amplitude

and phase

alowpτ q :“ apτ, p¨ ´ τ qωlowpτ q, vq

θlowpτ q :“ δθpτ, p¨ ´ τ qωlowpτ q, vq

(10.4)

(10.5)

at t “ τ , using (8.14) applied to the locally estimated phase function p¨ ´ τ qωlowpτ q
determined by the estimated instantaneous frequency ωlowpτ q. The approximation (10.5)
is justiﬁed since this estimated phase function p¨ ´ τ qωlowpτ q vanishes at t “ τ , so that
the discussion below (8.14) demonstrates that the updated estimated phase 0 ` δθpτ, p¨ ´
τ qωlowpτ q, vq “ δθpτ, p¨ ´ τ qωlowpτ q, vq is an estimate of the instantaneous phase at t “ τ
and frequency ω “ ωlowpτ q. Then alowpτ qypθlowpτ qq is an estimate, at t “ τ , of the mode
having the lowest frequency. If ωlowpτ q “ H, we leave alow and θlow undeﬁned.

Figure 36: The identiﬁcation of the ﬁrst mode segments in Example 10.2 is shown. The
scale of the vertical axis is log10pωq in sub-ﬁgure (1) and ω in sub-ﬁgures (2) and (3)
Segments are labeled in (1). (1) Energy Sp¨, ¨, vq (2) the identiﬁed lowest frequency at
each time t with consistent segment numbering (3) identiﬁed mode segments including
an artifact of the intersection, labeled as segment 0.

70

Algorithm 4 Lowest frequency segment identiﬁcation
1: function mmodepv, V, Vsegq
for vi,e in Vseg do
2:

`

˘

ωi,epτ qpt´τ q
α

2

yppt ´ τ qωi,epτ q ` θi,epτ qq

vi,e
τ ptq Ð ai,epτ qe´
end for
ř
ř
vτ Ð v ´

vi,e
5:
τ
6: Get ωlowpτ, vτ q as in (8.15) and abbreviate it as ωlowpτ q
7:

V vi,e ´

if ωlowpτ q ­“ H

Vseg

alowpτ q Ð apτ, p¨ ´ τ qωlowpτ q, vτ q
θlowpτ q Ð δθpτ, p¨ ´ τ qωlowpτ q, vτ q

end if
Set T to be the regular time mesh with spacing δt
T Ð T X tτ |ωlowpτ q ­“ Hu
if T “ H then
Vseg Ð H
return V, Vseg and goto line 34

end if
Tcut Ð trminpT q, maxpT qsu (Initialize the set of mesh intervals Tcut)
for successive τ1, τ2 (τ2 ´ τ1 “ δt) in T do

if

ˆ

ˇ
ˇ
ˇ
ˇ
ˇ log
if rτ1, τ2s Ă rt1, t2s P Tcut then

˙ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)1 or

ˇ
ˇ
ˇ
ˇ
ˇ log

ωlowpτ2q
ωlowpτ1q

ˆ

pθlowpτ2q´θlowpτ1qqpτ2´τ1q´1
ωlowpτ1q

˙ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)2 then

Tcut Ð pTcut (cid:114) trt1, t2suq Y trt1, τ1s, rτ2, t2su

end if

end if
end for
vlow Ð alowypθlowq
for rt1, t2s in Tcut do
2s, t1

1, t1

1,t1

vseg,rt1
ş
t1
2
t1
1
Vseg Ð Vseg Y tvseg,rt1

ωlowpτ qdτ ą (cid:15)3 then
2su

1,t1

if

end if
end for
V out, V out
return V out, V out
seg

33:
34: end function

seg Ð MODE PROCESSpV, Vseg, Sp¨, ¨, vτ qq

2 Ð MODE EXTENDpv, vlow|rt1,t2s

, Sp¨, ¨, vτ qq

3:

4:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

25:

26:

27:

28:

29:

30:

31:

32:

Next, let us describe how we use the values of pτ, ωlowpτ qq to determine the interval
domains for segments. Writing Tcut for the set of interval domains of these segments,
Tcut is initially set, in line 17, to contain the single element T , that is, the entire time
mesh T . We split an element of Tcut whenever ωlow is not continuous or 9θlow and ωlow

71

Algorithm 5 Mode fragment extension
1: function MODE EXTENDpv, vseg, Sp¨, ¨, vτ qq
2:

smooth Ð True
τ1 Ð t1

3:
4: while smooth is True do
5:

θ1 Ð θsegpτ1q
ω1 Ð 9θsegpτ1q
τ2 Ð τ1 ´ dt
ω2 Ð argmaxωPrp1´εqω1,p1`εqω2s Spτ2, ω, vτ q
θ2 Ð δθpτ2, p¨ ´ τ2qω2, vτ q
¯ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)1 or
ˇ log

pθ2´θ1qpτ2´τ1q´1
ω1

ˇ
ˇ
ˇ
ˇ log

ω2
ω1

if

´

´

smooth Ð False

else

a2 Ð apτ2, p¨ ´ τ2qω2, vτ q
vsegpτ2q Ð a2ypθ2q
t1, τ1 Ð τ2

end if
end while
τ1 Ð t2

21:

18:
19: while smooth is True do
θ1 Ð θsegpτ1q
20:
ω1 Ð 9θsegpτ1q
τ2 Ð τ1 ` dt
ω2 Ð argmaxωPrp1´εqω1,p1`εqω2s Spτ2, ω, vτ q
θ2 Ð δθpτ2, p¨ ´ τ2qω2, vτ q
¯ˇ
ˇ
ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)1 or
ˇ log

pθ2´θ1qpτ2´τ1q´1
ω1

ˇ
ˇ
ˇ
ˇ log

ω2
ω1

24:

22:

23:

25:

if

´

´

¯

ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)2 then

¯

ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)2 then

smooth Ð False

else

a2 Ð apτ2, p¨ ´ τ2qω2, vτ q
vsegpτ2q Ð a2ypθ2q
t2, τ1 Ð τ2

end if
end while
return vseg, t1, t2

33:
34: end function

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

26:

27:

28:

29:

30:

31:

32:

are not approximately equal, as follows. If our identiﬁed instantaneous frequency around
t “ τ matches a single mode, we expect neither condition to be satisﬁed, i.e. we expect
both ωlow to be continuous and 9θlow « ωlow. In our discrete implementation (lines 18 to

72

24), we introduce a cut between two successive points, τ1 and τ2, of the time mesh T , if

ˇ
ˇ
ˇ
ˇ
ˇ log

ˆ

ωlowpτ2q
ωlowpτ1q

˙ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)1 or

ˇ
ˇ
ˇ
ˇ
ˇ log

ˆ

pθlowpτ2q ´ θlowpτ1qqpτ2 ´ τ1q´1
ωlowpτ1q

˙ˇ
ˇ
ˇ
ˇ
ˇ ą (cid:15)2 ,

(10.6)

where (cid:15)1 and (cid:15)2 are pre-set thresholds. Each potential mode segment is then identiﬁed
as vlow|rt1,t2s

for some t1 ă t2, t1, t2 P T .

Note that in Figure 36.2, the continuous stretch of ωlow labeled by 2 does not corre-
spond to the full mode segment labeled by 2 in Figure 36.1, but a fragment of it. This
is because the lowest frequency mode, v1, is identiﬁed by ωlowptq over t À ´0.5. We des-
ignate this partially identiﬁed mode segment as a mode fragment. Such fragments are
extended to fully identiﬁed segments (as in 2 on Figure 36.3) with the MODE EXTEND
module, with pseudo-code shown in Algorithm 5. This MODE EXTEND module iter-
atively extends the support, rt1, t2s, by applying, in lines 8 and 23, a max-squeezing to
identify instantaneous frequencies at neighboring mesh points to the left and right of
the interval rt1, t2s. The process is stopped if it is detected, in lines 10 and 25, that the
extension is discontinuous in phase according to (10.6). This sub-module returns (max-
imally continuous) full mode segments. Furthermore, to remove segments that may be
generated by noise or are mode intersections, in lines 26 to 31 of Algorithm 4, segments
such that

ż

t2

ωlowpτ qdτ ď (cid:15)3

t1

(10.7)

where (cid:15)3 is a threshold, are removed. In our implementation, we take (cid:15)3 :“ 20π, corre-
sponding to 10 full periods. Note that Figure 37.2 shows those segments deemed noise
at level (cid:15)3 :“ 20π but which are not deemed noise at level 3π, in the step after all three
modes have been estimated in Example 10.1. Consequently, it appears that the noise
level (cid:15)3 :“ 20π successfully removes most noise artifacts. Note that the mode segments
in Figure 37.2 are short and have quickly varying frequencies compared to those of full
modes.

Figure 37: (1) Energy Sp¨, ¨, v ´ v1,e ´ v2,e ´ v3,eq (2) identiﬁed mode segments (Vseg
obtained after the loop in Algorithm 4 on line 31).

73

Algorithm 6 Raw segment processing
1: function MODE PROCESSpV, Vseg, Sp¨, ¨, vτ qq
2:

Vgroup Ð H
for vi,e in Vseg do

if vi,e corresponds to a mode intersection or noise then

Vseg Ð Vseg (cid:114) tvi,eu

else

for Vgroup,j in pVgroup,j1qj1 do

if vi,e corresponds to the same mode as Vgroup,j then

Vgroup,j Ð Vgroup,j Y tvi,eu
break for loop

end if
end for
if vi,e not added to any mode block then
pVgroup,j1qj1 Ð pVgroup,j1qj1 Y ttvi,euu

end if

end if
end for
for Vgroup,j in pVgroup,j1qj1 do
if Vgroup,j is complete then

Transform the segments in Vgroup,j into a mode vj,e
V Ð V Y tvj,eu
Vseg Ð Vseg (cid:114) Vgroup,j

3:

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

end if
end for
return V, Vseg

25:
26: end function

˘

Vgroup,j

Next, line 32 of Algorithm 4 applies the function MODE PROCESS, Algorithm 6, to
V and Vseg, the sets of modes and segments, as well as the energy Sp¨, ¨, vτ q, to produce the
updated sets V out and V out
seg . This function utilizes a partition of a set Vgroup, initialized
`
to be empty, into a set of partition blocks
j, where Vgroup,j Ă Vgroup, @j. The
partition blocks consist of segments that have been identiﬁed as corresponding to the
same mode, indexed locally by j. Each segment in Vseg will either be discarded or
placed into a partition block. When a partition block is complete it will be turned into
a mode in V out by interpolating instantaneous frequencies and amplitudes in the (small)
missing sections of T and the elements of the partition block removed from Vgroup and
Vseg. All partition blocks that are not complete will be passed-on to the next iteration.
These selection steps depend on the prior information about the modes composing the
signal and may be based on (a) user input and/or (b) a set of pre-deﬁned rules. Further
details and rationale on the options to discard, place segments into partition blocks, and
determine the completeness of a block, will be discussed in the following paragraphs.
The ﬁrst loop in Algorithm 6, lines 3 to 17, takes each segment vi,e in Vseg, and either

74

discards it, adds it to a partition block in Vgroup, or creates a new partition block with
it. On line 4, we specify that a segment is to be discarded (i.e. removed from the set
of segments Vseg) whenever it corresponds to a mode intersection or noise, where we
identify a mode intersection whenever two modes’ instantaneous frequencies match at
any particular time. This can be seen in Figure 38.1 where the energies for the higher
two frequency modes on t Á ´.25 meet in frequency at time t « 0.25, as well as Figure
36.1, where the lower two frequency modes on t Á ´.25 also meet around t « 0.25.
Moreover, segment 0 in Figure 36.3 corresponds an artifact of this mode intersection. In
these two examples, it has been observed selecting (cid:15)3 large enough leads to no identiﬁed
noise artifacts. However, identiﬁed segments with these similar characteristics as those
in Figure 37.2, i.e. short with rapidly varying frequency, are discarded, especially if there
is a prior knowledge of noise in the signal.

Figure 38:
The scale of the vertical axis is log10pωq in the top row of sub-ﬁgures
(1,3,5) and ω in bottom row of sub-ﬁgures (2,4,6). Segments are labeled in (1). (1,
2) Energy Sp¨, ¨, vq and the identiﬁed lowest frequency segments (3, 4) First updated
energy Sp¨, ¨, v ´ v1,e ´ v2,eq and its identiﬁed lowest frequency segments (5, 6) Second
updated energy Sp¨, ¨, v ´ v1,e ´ v2,eq and its identiﬁed lowest frequency segments, where
v1,e results from joining mode segments 1 and 4, while v2,e is generated from joining
segments 3 and 2.

All segments vi,e that are not discarded are iteratively put into existing partition
blocks in lines 7-12 of Algorithm 6, or used to create a new partition block in line 14,
which we denote by ttvi,euu. For example, in Figure 38.2, we place segment 1 into its
own partition block on line 14 by default since when Vgroup is empty, the loop from
lines 7-12 is not executed. Then we do not place segment 2 in the partition block with
segment 1, but again place it in its own partition block on line 14 with the observation
they belong to diﬀerent modes (based on the max-squeezed energy S in Figure 38.1).
The end result of this iteration is segments 1 and 2 placed into separate partition blocks.
In the next iteration shown in 38.4, we construct two partition blocks, one consisting of

75

ttv1,e, v4,euu and the other ttv2,e, v3,euu. In the following iteration, illustrated in Figure
38.6, we again place segment 5 into its own partition block on line 14 by default. The
next iteration is the last since no segments which violate (10.7) are observed. Both
blocks are then designated as complete modes, that is correspond to a mode at all time
r´1, 1s, and are used to construct v1,e and v2,e. This determination can be based on (a)
user input and/or (b) a set of pre-deﬁned rules. Observing S at the third stage in Figure
38.5, we designate it as complete.

The ﬁnal loop of Algorithm 6 on lines 18-24 begins by checking whether the block is
complete. For a block deemed complete, in line 20, their segments are combined to create
an estimate of their corresponding mode by interpolating the amplitude and phase to
ﬁll the gaps and extrapolation by zero to the boundary. Then, in line 21, this estimated
mode is added to V and, in line 22, its generating segments removed from Vseg. Finally,
the segments of the incomplete blocks constitute the output Vseg of Algorithm 6.

In the implementation corresponding to Figure 38.2, each block consisting of seg-
ments 1 and 2 respectively are both determined to not be complete, and hence are
passed to the next iteration as members of Vseg to the next iteration. In Figure 38.4,
the block consisting of segments 1 and 4 and the block consisting of segments 2 and 3,
are deemed complete since each block appears to contain diﬀerent portions of the same
mode (with missing portions corresponding to the intersection between the correspond-
ing modes around t « 0.25), and consequently their segments are therefore designated to
be turned into modes v1,e from segments 1 and 4 and v2,e from 2 and 3. Finally in Figure
38.6, the block consisting of only segment 5 is determined to be complete and in line 20
is extrapolated by zero to produce its corresponding mode. In Example 10.2, shown in
Figure 36.3, we place segments 1, 2, and 3 in separate blocks (and disregard 0), but only
designate the block containing segment 1 as complete. The output of Algorithm 6, and
hence Algorithm 4, are the updated list of modes and segments.

10.2 The segmented micro-local KMD algorithm

The segmented iterated micro-local algorithm identiﬁes full modes in the setting of
Problem 5 and is presented in Algorithm 7. Except for the call of the function mmode,
Algorithm 4, Algorithm 7 is similar to Algorithm 2. It is initialized by V “ H and Vseg “
H, and the main iteration between lines 2 and 17 identiﬁes the modes or segments with
lowest instantaneous frequency and then provides reﬁned estimates for the amplitude
and the phase of each mode vi, i P t1, . . . , mu of the signal v. We ﬁrst apply mmode
to identify segments to be passed-on to the next iteration and mode-segments to be
combined into modes. This set of recognized modes V out will be reﬁned in the loop
between lines 8 to 14 by iteratively applying the micro-local KMD steps of Section 8.1
on the base frequency of each mode (these steps correspond to the ﬁnal optimization
loop, i.e. lines 21 to 27 in Algorithm 2). The loop is terminated when no additions are
made to V or Vseg.

76

Algorithm 7 Segmented iterated micro-local KMD.
1: tV, Vsegu Ð tH, Hu
2: while true do
3:

tV out, V out
if V out

seg u Ð mmodepv, V, Vsegq
seg “ H and |V out| “ |V| then

break loop

end if
if |V out| ą |V| then

repeat

for vi,e in V out do

ř

vi,res Ð v ´ ai,e ¯ypθi,eq ´
˘
`
{c1
τ, θi,e, vi,res
ai,epτ q Ð a
`
θi,epτ q Ð θi,epτ q ` 1
τ, θi,e, vi,res
2 δθ
`

˘

˘

j‰i aj,eypθj,eq

τ, θi,e, vi,res

| ă (cid:15)1

end for
until supi,τ |δθ

end if
tV, Vsegu Ð tV out, V out
seg u

16:
17: end while
18: Return the modes vi,eptq Ð ai,eptqypθi,eptqq for i “ 1, ..., m

4:

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

10.3 Numerical experiments

Figure 39: (1) v1,e and v1 (2) v2,e and v2 (3) v3,e and v3. See footnote 3

Figure 39 and Table 10 show the accuracy of Algorithm 7 in recovering the modes of
the signal described in Example 10.1, the results for Example 10.2 appearing essentially
the same, and thereby quantify its robustness to noise, vanishing amplitudes, and cross-
1
ing frequencies. We again take the mesh spanning r´1, 1s spaced at intervals of size
5000
and aim to recover each mode vi on the whole interval r´1, 1s. We kept α “ 25 constant
in our implementation. The amplitudes and frequencies of the modes composing v are
shown in Figure 34. The recovery errors of the modes are found to be consistently on
the order of 10´2. Note that in the noise-free setting with identical modes, the recovery
error is on the order of 10´3 implying the noise is mainly responsible for the errors shown

77

in Table 10.

Mode
i “ 1
i “ 2
i “ 3

}vi,e´vi}L2
}vi}L2
3.17 ˆ 10´2
2.49 ˆ 10´2
3.52 ˆ 10´2

}vi,e´vi}L8
}vi}L8
6.99 ˆ 10´2
7.09 ˆ 10´2
9.52 ˆ 10´2

}ai,e´ai}L2
}ai}L2
2.24 ˆ 10´2
1.64 ˆ 10´2
3.13 ˆ 10´2

}θi,e´θi}L2
1.99 ˆ 10´2
1.81 ˆ 10´2
2.02 ˆ 10´2

Table 10: Signal component recovery errors in Example 10.1. Note that the error in
phase for mode i “ 3 was calculated over r´1, ´ 1
3 s since the phase of a zero signal is
undeﬁned.

11 Proofs

11.1 Proof of Lemma 3.1

`
ΦΦT

We ﬁrst establish that Ψpvq “ Φ`v, where the Moore-Penrose inverse Φ` is deﬁned by
˘
´1, where ΦT is the Hilbert space adjoint of Φ. To that end, let w˚ be
Φ` :“ ΦT
the solution of (3.5). Since Φ : B Ñ V is surjective it follows that Φ : KerKpΦq Ñ V is
a bijection and therefore

tw : Φw “ vu “ w0 ` KerpΦq
for a unique w0 P KerKpΦq. Therefore, setting w1 :“ w´w0 we ﬁnd that pw1q˚ :“ w˚ ´w0
is a solution of

#

Minimize }w1 ` w0}B
Subject to w1 P B and Φw1 “ 0 ,

(11.1)

so that by the projection theorem we have pw1q˚ “ PKerpΦqp´w0q where PKerpΦq is the
orthogonal projection onto KerpΦq. Therefore w˚ “ w0 ` pw1q˚ “ w0 ´ PKerpΦqpw0q “
PKerKpΦqw0, so that we obtain

w˚ “ PKerKpΦqw0.

Since Φ is surjective and continuous it follows from the closed range theorem, see
e.g. Yosida [113, p. 208] that ImpΦT q “ KerKpΦq and KerpΦT q “ H, which implies that
ΦΦT : V Ñ V is invertible, so that the Moore-Penrose inverse Φ` : V Ñ B of Φ, is
well-deﬁned by
`

˘

Φ` :“ ΦT

ΦΦT

´1 .

It follows that PKerKpΦq “ Φ`Φ and ΦΦ` “ IV so that

w˚ “ PKerKpΦqw0 “ Φ`Φw0 “ Φ`v,

that is, we obtain the second assertion w˚ “ Φ`v.

For the ﬁrst assertion, suppose that Ker Φ “ H. Since it is surjective, it follows that
Φ is a bijection. Then, the unique solution to the minmax problem is the only feasible

78

one w˚ “ Φ´1v “ Φ`v. When Ker Φ ‰ H, observe that since all u which satisfy Φu “ v
have the representation u “ w0 ` u1 for ﬁxed w0 P KerKpΦq and some u1 P KerpΦq, it
follows that the inner maximum satisﬁes

max
uPB|Φu“v

}u ´ w}B
}u}B

“

“

max
u1PKerpΦq

}u1 ` w0 ´ w}B
}u1 ` w0}B

max
u1PKerpΦq

max
tPR

}tu1 ` w0 ´ w}B
}tu1 ` w0}B

ě 1

On the other hand, for w :“ Φ`v, we have

max
uPB|Φu“v

}u ´ w}B
}u}B

“

“

“

max
uPB|Φu“v

max
uPB|Φu“v

}u ´ Φ`v}B
}u}B
}u ´ Φ`Φu}B
}u}B

max
uPB|Φu“v

}u ´ PKerKpΦqu}B
}u}B

ď 1,

which implies that w :“ Φ`v is a minmax solution. To see that it is the unique optimal
solution, observe that we have just established that

max
uPB|Φu“v

}u ´ Ψpvq}B
}u}B

“ 1

(11.2)

for any optimal Ψ : V Ñ B. It then follows that

max
uPB

}u ´ ΨpΦuq}B
}u}B

“ 1

which implies that the map I ´ Ψ ˝ Φ : B Ñ B is a contraction. Moreover, by selecting
u P KerpΦq tending to 0, it follows from (11.2) that Ψp0q “ 0. Since, by deﬁnition,
Φ ˝ Ψ “ IV , we have

pI ´ Ψ ˝ Φq2puq “ pI ´ Ψ ˝ Φqpu ´ Ψ ˝ Φuq

“ u ´ Ψ ˝ Φu ´ Ψ ˝ Φpu ´ Ψ ˝ Φuq
˘
`
Φu ´ Φ ˝ Ψ ˝ Φu
“ u ´ Ψ ˝ Φu ´ Ψ
˘
`
Φu ´ Φu
“ u ´ Ψ ˝ Φu ´ Ψ
˘
`
0
“ u ´ Ψ ˝ Φu ´ Ψ
“ u ´ Ψ ˝ Φu

so that the map I ´ Ψ ˝ Φ is a projection. Since Φpu ´ Ψ ˝ Φuq “ Φu ´ Φ ˝ Ψ ˝ Φu “ 0 it
follows that ImpI ´ Ψ ˝ Φq Ă KerpΦq, but since for b P KerpΦq, we have pI ´ Ψ ˝ Φqpbq “
b ´ Ψ ˝ Φb “ b, we obtain the equality ImpI ´ Ψ ˝ Φq “ KerpΦq.

79

To show that a projection of this form is necessarily linear, let us demonstrate that
ImpΨ ˝ Φq “ KerKpΦq. To that end, use the decomposition B “ KerpΦq ‘ KerKpΦq to
write u “ u1 ` u2 with u1 P KerpΦq and u2 P KerKpΦq and write the contractive condition
}u ´ Ψ ˝ Φu}2 ď }u}2 as

}u1 ` u2 ´ Ψ ˝ Φpu1 ` u2q}2 ď }u1 ` u2}2,

which using the linearity of Φ and u1 P KerpΦq we obtain

}u1 ` u2 ´ Ψ ˝ Φu2}2 ď }u1 ` u2}2,

Suppose that Ψ ˝ Φu2 “ v1 ` v2 with v1 P KerpΦq nontrivial. Then, selecting u1 “ tv1,
with t P R, we obtain

}pt ´ 1qv1 ` u2 ´ v2|2 ď }tv1 ` u2}2

which amounts to

and therefore

pt ´ 1q2}v1}2 ` }u2 ´ v2|2 ď t2}v1}2 ` }u2}2

p1 ´ 2tq}v1}2 ` }u2 ´ v2|2 ď }u2}2,

which provides a contradiction for t large enough negative. Consequently, v1 “ 0 and
ImpΨ ˝ Φq Ă KerKpΦq. Since I “ Ψ ˝ Φ ` pI ´ Ψ ˝ Φq with ImpΨ ˝ Φq Ă KerKpΦq and
ImpI ´ Ψ ˝ Φq Ă KerpΦq it follows that ImpΨ ˝ Φq “ KerKpΦq. Since Ψ ˝ Φ is a projection
it follows that

Ψ ˝ Φu2 “ u2,

u2 P KerKpΦq .

Consequently, for two elements u1 “ u1

1 ` u2

1 and u2 “ u1

2 ` u2

2 with u1

i P KerpΦq and

i P KerKpΦq for i “ 1, 2 we have
u2

`
I ´ Ψ ˝ Φ

˘

pu1 ` u2q “ u1 ` u2 ´ Ψ ˝ Φpu1 ` u2q
1 ` u1
1 ` u1
2
1 ´ Ψ ˝ Φu2
1 ` u2
˘
pu1q `
I ´ Ψ ˝ Φ

“ u1
“ u1
“ u1
`
“

1 ` u1
`

1 ` u2

2 ` u2

2 ` u2
I ´ Ψ ˝ Φ

2 ´ Ψ ˝ Φu2
2
pu2q ,

˘

2 ´ Ψ ˝ Φpu2

1 ` u2
2q

and similarly, for t P R,

`

I ´ Ψ ˝ Φ

˘

`
ptu1q “ t

˘

I ´ Ψ ˝ Φ

pu1q,

so we conclude that I ´ Ψ ˝ Φ is linear.

Since according to Rao [83, Rem. 9, p. 51], a contractive linear projection on a Hilbert
space is an orthogonal projection, it follows that the map I ´ Ψ ˝ Φ is an orthogonal
projection, and therefore Ψ ˝ Φ “ PKerKpΦq. Since Φ` is the Moore-Penrose inverse, it
follows that PKerKpΦq “ Φ`Φ so that Ψ ˝ Φ “ Φ`Φ, and therefore the assertion Ψ “ Φ`
follows by right multiplication by Ψ using the identity Φ ˝ Ψ “ IV .

80

11.2 Proof of Lemma 3.2

Let us write Φ : B Ñ V as

Φu “

ÿ

iPI

eiui,

u “ pui P ViqiPI,

where we now include the subspace injections ei : Vi Ñ V in its description. Let ¯ei :
Vi Ñ B denote the component injection ¯eivi :“ p0, . . . , 0, vi, 0, . . . , 0q and let ¯eT
: B Ñ Vi
i
denote the component projection. Using this notation, the norm (3.6) on B becomes

}u}2

B :“

ÿ

iPI

}¯eT

i u}2
Vi,

u P B ,

(11.3)

with inner product

@
u1, u2

D
B :“

ÿ

iPI

@
i u1, ¯eT
¯eT

i u2

D

,

Vi

u1, u2 P B .

Clearly, ¯eT

j ¯ei “ 0, i ‰ j and ¯eT

i ¯ei “ IVi, so that

x¯eT

i u, viyVi “ x¯eT
ÿ

i u, ¯eT
x¯eT

i ¯eiviyVi
j u, ¯eT

j ¯eiviyVi

“

jPI

“ xu, ¯eiviyB,

implies that ¯eT
i

is indeed the adjoint of ¯ei. Consequently we obtain

Φ “

ÿ

iPI

ei¯eT
i

and therefore its Hilbert space adjoint ΦT : V Ñ B is

ΦT “

ÿ

iPI

¯eieT
i ,

where eT
i
isomorphism

: V Ñ Vi is the Hilbert space adjoint of ei. To compute it, use the Riesz

and the usual duality relationships to obtain

ι : V Ñ V ˚

eT
i “ Qie˚

i ι ,

where e˚

i : V ˚ Ñ V ˚
i

ΦΦT “

is the dual adjoint projection. Consequently we obtain
ÿ

ÿ

ÿ

ÿ

ÿ

ej ¯eT
j

¯eieT

i “

ej ¯eT

j ¯eieT

i “

eieT

i “

eiQie˚
i ι

jPI

iPI

i,jPI

iPI

iPI

81

and therefore deﬁning

it follows that

S :“

ÿ

iPI

eiQie˚
i

ΦΦT “ Sι.

Since ΦΦT and ι are invertible, S is invertible. The invertibility of S implies both
assertions regarding norms and their duality follows in a straightforward way from the
deﬁnition of the dual norm. For the Hilbert space version see, e.g., [74, Prop. 11.4].

11.3 Proof of Theorem 3.3

We use the notations and results in the proof of Lemma 3.2. The assumption V “
implies that the information map Φ : B Ñ V deﬁned by

ř

i Vi

Φu “

ÿ

iPI

ui,

u “ pui P ViqiPI,

is surjective. Consequently, Lemma 3.1 asserts that the minimizer of (3.5) is w˚ “
Ψpvq :“ Φ`v, where the Moore-Penrose inverse Φ` :“ ΦT pΦΦT q´1 of Φ is well deﬁned,
with ΦT : V Ñ B being the Hilbert space adjoint to Φ : B Ñ V . The proof of Lemma 3.2
obtained ΦΦT “ Sι where S :“
i and ι : V Ñ V ˚ is the Riesz isomorphism,
eT
i ι , where eT
i “ Qie˚
is
i
its dual space adjoint, and ΦT “
i , where ¯ei : Vi Ñ B denotes the component
injection ¯eivi :“ p0, . . . , 0, vi, 0, . . . , 0q.

: V Ñ Vi is the Hilbert space adjoint of ei and e˚

iPI eiQie˚
ř

i : V ˚ Ñ V ˚
i

iPI ¯eieT

ř

ř

Therefore, since pΦΦT q´1 “ ι´1S´1, we obtain Φ` “

iPI ¯eiQie˚

i ιι´1S´1 , which

amounts to

or in coordinates

Φ` “

ÿ

iPI

¯eiQie˚

i S´1 ,

pΦ`vqi “ Qie˚

i S´1v,

i P I,

(11.4)

establishing the ﬁrst assertion. The second follows from the general property ΦΦ` “
ΦΦT pΦΦT q´1 “ I of the Moore-Penrose inverse. The ﬁrst isometry assertion follows
from

}Φ`v}2

B “

ÿ

iPI

}pΦ`vqi}2

Vi “

ÿ

iPI

}Qie˚

i S´1v}2

Vi “

ÿ

iPI

rQ´1

i Qie˚

i S´1v, Qie˚

i S´1vs “

ÿ

iPI

re˚

i S´1v, Qie˚

i S´1vs

ÿ

“

rS´1v, eiQie˚

i S´1vs “ rS´1v,

iPI
for v P V .

For the second, write Φ “

deﬁned by

ÿ

iPI

eiQie˚

i S´1vs “ rS´1v, SS´1vs “ rS´1v, vs “ }v}2

S´1

ř

iPI ei¯eT

i and consider its dual space adjoint Φ : V ˚ Ñ B˚

Φ˚ “

ÿ

iPI

¯eT,˚
i

e˚
i .

82

A straightforward calculation shows that ¯eT,˚
iPI V ˚
the product B˚ “

i . Consequently, we obtain

ś

i

: V ˚

i Ñ B˚ is the component injection into

i Q¯eT,˚
¯eT

j “ δi,jQj,

i, j P I,

so that

ΦQΦ˚ “

ÿ

iPI

ei¯eT

i Q

ÿ

jPI

¯eT,˚
j e˚

j “

ÿ

i,jPI

and since, for φ P V ˚,

ei¯eT

i Q¯eT,˚

j e˚

j “

ÿ

iPI

eiQie˚

i “ S,

}Φ˚φ}2

B “ xΦ˚φ, Φ˚φyB˚ “ rΦ˚φ, QΦ˚φs “ rφ, ΦQΦ˚φs “ rφ, Sφs “ }φ}2
S,

it follows that Φ˚ is an isometry.

11.4 Proof of Theorem 3.4

Use the Riesz isomorphism between V and V ˚ to represent the dual space adjoint Φ˚ :
V ˚ Ñ B˚ of Φ : B Ñ V as Φ˚ : V Ñ B˚. It follows from the deﬁnition of the Hilbert
space adjoint ΦT : V Ñ B that

rΦ˚v, bs “ xv, Φby “ xΦT v, byB.

Since Q : B˚ Ñ B (3.11) deﬁnes the B inner product through

xb1, b2yB “ rQ´1b1, b2s,

b1, b2 P B,

it follows that rΦ˚v, bs “ xQΦ˚v, byB and therefore xQΦ˚v, byB “ xΦT v, byB, v P V, b P B,
so we conclude that

ΦT “ QΦ˚ .
Since Theorem 3.3 demonstrated that Ψ is the Moore-Penrose inverse Φ` which implies
that Ψ ˝ Φ is the orthogonal projection onto ImpΦT q it follows that Ψ ˝ Φu P ImpΦT q.
However, the identity ΦT “ QΦ˚ implies that ImpΦT q “ Q ImpΦ˚q so that we obtain
the ﬁrst part

}u ´ ΨpΦuq}B “ inf
φPV ˚

}u ´ QΦ˚pφq}B

of the assertion. The second half follows from the deﬁnition (3.6) of } ¨ }B.

11.5 Proof of Proposition 4.1

Restating the assertion using the injections ei : Vi Ñ V , our objective is to establish
that
˘
rφ, eiξis
eiξi, v
i φ, ξis, it follows that rφ, eiξis „ N p0, re˚

S´1
i φ, Qie˚

i φsq so that Var

Epiq “ Var

˘
rφ, eiξis

“ Var

`@

D

˘

`

`

.

“

Since rφ, eiξis “ re˚
re˚

i φ, Qie˚

i φs, which using φ “ S´1v becomes

`

˘
rφ, eiξis

Var

“ rS´1v, eiQie˚

i S´1vs .

83

On the other hand, the deﬁnitions (4.1) of Epiq, (3.7) of } ¨ }Vi, and Theorem 3.3 imply
that

“ rQ´1

i Qie˚

i S´1v, Qie˚
so that we conclude the ﬁrst part Epiq “ Var
rS´1v, eiξis “ xv, eiξiyS´1 we obtain the second.

Epiq :“ }Ψipvq}2
i S´1vs “ re˚

i Ψipvq, Ψipvqs

Vi “ rQ´1
i S´1v, Qie˚
˘
`
rφ, eiξis

i S´1vs “ rS´1v, eiQie˚

i S´1vs,

of the assertion. Since rφ, eiξis “

11.6 Proof of Theorem 4.4

j

: Bpkq

Fix 1 ď k ă r ď q. To apply Theorem 3.3, we select B :“ Bpkq and V :“ Bprq and
endow them with the external direct sum vector space structure of products of vector
spaces. Since the information operator Φpr,kq : Bpkq Ñ Bprq deﬁned in (4.9) is diagonal
with components Φpr,kq
, j P I prq and the norm on Bpkq “
is
the product norm }u}2ś
iPIprq }ui}2
, u “ puiqiPIprq , it follows from the
variational characterization of Lemma 3.1, the diagonal nature of the information map
Φpr,kq and the product metric structure on Bpkq that the optimal recovery solution Ψpk,rq
is the diagonal operator with components the optimal solution operators corresponding
to the component information maps Φpr,kq
: Bpkq
, j P I prq. Since each component
(4.8) of the observation operator is

j Ñ V prq
ř
j
“

j Ñ V prq

iPIprq Bpkq

iPIprq Bpkq

Bpkq
i

ś

j

j

i

i

Φpr,kq
j

puq :“

ÿ

iPjpkq

ui,

u P Bpkq

j

,

it follows that the appropriate subspaces of V prq

j

are

i Ă V prq
V pkq

j

,

i P jpkq .

Moreover, Condition 4.3 and the semigroup nature of the hierarchy of subspace embed-
dings implies that

ÿ

epk`2,kq
j,i

“

epk`2,k`1q
j,l

epk`1,kq
l,i

,

i P jpkq,

lPjpk`1q

where the sum, despite its appearance, is over one term, and by induction we can estab-
lish that assumption (4.19) implies that

Qprq

j “

ÿ

iPjpkq

j,i Qpkq
epr,kq

i epk,rq

i,j

,

j P I prq.

(11.5)

Utilizing the adjoint epk,rq

i,j

: V prq,˚

j Ñ V pkq,˚

i

(4.13) to the subspace embedding epr,kq

j,i

:

, it now follows from Theorem 3.3 and (11.5) that these component optimal

: V prq

j Ñ Bpkq

j
`
Qpkq

Ψpk,rq
j

pvjq :“

i epk,rq

i,j Qprq,´1

j

are those assumed in the theorem in (4.14) and (4.15)

˘

vj

iPjpkq,

vj P V prq

j

.

(11.6)

V pkq
i Ñ V prq
j
solution maps Ψpk,rq
as

j

84

The ﬁrst three assertions for each component j then follow from Theorem 3.3, thus
establishing the ﬁrst three assertions in full.

For the semigroup assertions, Condition 4.3 implies that, for k ă r ă s and l P I psq,
there is a one to one relationship between tj P lprq, i P jpkqu and ti P lpkqu. Consequently,
the deﬁnition (4.9) of Φpr,kq implies

Φps,rq ˝ Φpr,kqpuq “

´ ÿ

` ÿ

˘

¯
q

ui

jPlprq

iPjpkq

´ ÿ

¯

“

lPIpsq

ui

lPIpsq

iPlpkq

“ Φps,kqpuq ,

establishing the fourth assertion Φps,kq “ Φps,rq ˝ Φpr,kq.

For the ﬁfth, the deﬁnition (4.16) of Ψpk,rq implies that

Ψpk,rq ˝ Ψpr,sqpvq “

`

˘
pvq
iPjpkq
j,l Qpsq,´1
j epr,sq
˘
vl

l

iPjpkq

j

j

`

`

“

Qpkq

Qpkq

i,j Qprq,´1
i,j Qprq,´1
i,j epr,sq
i,l Qpsq,´1

Ψpr,sq
j
Qprq
j,l Qpsq,´1
˘
vl

i epk,rq
i epk,rq
i epk,rq
i epk,sq
Qpkq
“
“ Ψpk,sqpvq ,

Qpkq

“

`

l

l

iPlpkq

˘

vl

iPjpkq

establishing Ψpk,sq “ Ψpk,rq ˝ Ψpr,sq.

The last assertion follows directly from the second and the ﬁfth.

11.7 Proof of Theorem 4.6

Since ξpkq : Bpkq,˚ Ñ H is an isometry to a Gaussian space of real variables we can abuse
notation and write ξpkqpb˚q “ rb˚, ξpkqs which emphasizes the interpretation of ξpkq as a
weak Bpkq-valued random variable. Since, by Theorem 4.4,

Φpk,1q,˚ : pBpkq,˚, } ¨ }Bpkq,˚q Ñ pBp1q,˚, } ¨ }Bp1q,˚q is an isometry

(11.7)

and ξp1q : Bp1q,˚ Ñ H is an isometry, it follows that

Φpk,1qξp1q :“ ξp1q ˝ Φpk,1q,˚ : Bpkq,˚ Ñ H

is an isometry, and therefore a Gaussian ﬁeld on Bpkq. Since Gaussian ﬁelds transform
like Gaussian measures with respect to continuous linear transformations, we obtain that
ξp1q „ N p0, Q1q implies that

Φpk,1qξp1q „ N p0, Φpk,1qQ1Φpk,1q,˚q,

but the isometric nature (11.7) of Φpk,1q,˚ implies that

so we conclude that

Φpk,1qQp1qΦpk,1q,˚ “ Qpkq,

Φpk,1qξp1q „ N p0, Qkq

85

thus establishing the assertion that ξpkq is distributed as Φpk,1qξp1q.
“
The conditional expectation E

“
of conditional expectations E
linearity of conditional expectation of Gaussian random variables, appears as

ξpkq | Φpr,kqpξpkqq
rb˚, ξpkqs | Φpr,kqpξpkqq

is uniquely characterized by its ﬁeld
‰
, b˚ P Bpkq,˚, which, because of the

‰

“
E

‰
rb˚, ξpkqs | Φpr,kqpξpkqq

“ rAb˚, Φpr,kqpξpkqqs

for some Ab˚ P V ˚. Furthermore, the Gaussian conditioning also implies that the de-
pendence of Ab˚ on b˚ is linear so we write Ab˚ “ Ab˚ for some A : B˚ Ñ V ˚, thereby
obtaining

‰
“
rb˚, ξpkqs | Φpr,kqpξpkqq
E

“ rAb˚, Φpr,kqpξpkqqs,

b˚ P Bpkq,˚ .

(11.8)

Using the well-known fact, see e.g. Dudley [22, Thm. 10.2.9], that the conditional ex-
pectation of a square integrable random variable on a probability space pΩ, Σ1, P q with
respect to a sub-σ-algebra Σ1 Ă Σ is the orthogonal projection onto the closed subspace
L2pΩ, Σ1, P q Ă L2pΩ, Σ, P q, it follows that the conditional expectation satisﬁes

“`

E

rb˚, ξpkqs ´ rAb˚, Φpr,kqpξpkq

˘

‰
rv˚, Φpr,kqpξpkqs

“ 0,

b˚ P Bpkq,˚, v˚ P V pkq,˚ .

Rewriting this as

“`

‰
˘
rΦpr,kq,˚v˚, ξpkqs
rb˚, ξpkqs ´ rΦpr,kq,˚Ab˚, ξpkqs

E

“ 0,

b˚ P Bpkq,˚, v˚ P V pkq,˚ ,

we obtain

rb˚, QpkqΦpr,kq,˚v˚s “ rΦpr,kq,˚Ab˚, QpkqΦpr,kq,˚v˚s
“ rb˚, A˚Φpr,kqQpkqΦpr,kq,˚v˚s

for all b˚ P Bpkq,˚ and v˚ P V pkq,˚, and so conclude that

A˚Φpr,kqQpkqΦpr,kq,˚v˚ “ QpkqΦpr,kq,˚v˚,

b˚ P Bpkq,˚, v˚ P V pkq,˚,

which implies that

Since

A˚Φpr,kqb “ b,

b P ImpQpkqΦpr,kq,˚q .

(11.9)

xΦpr,kq,T bprq, bpkqyBpkq “ xbprq, Φpr,kqbpkqyBpkq

“ rQpkq,´1bprq, Φpr,kqbpkqs
“ rΦpr,kq,˚Qpkq,´1bprq, bpkqs
“ rQprq,´1QprqΦpr,kq,˚Qpkq,´1bprq, bpkqs
“ xQprqΦpr,kq,˚Qpkq,´1bprq, bpkqyBprq,

we conclude that

Φpr,kq,T “ QprqΦpr,kq,˚Qpkq,´1 ,

86

and therefore

ImpQprqΦpr,kq,˚q “ ImpΦpr,kq,T q .

Consequently, (11.9) now reads

A˚Φpr,kqb “ b,

b P ImpΦpr,kq,T q .

(11.10)

Since clearly

it follows that

A˚Φpr,kqb “ 0,

b P KerpΦpr,kqq

A˚Φpr,kq “ PImpΦpr,kq,T q

Since PImpΦpr,kq,T q “ pΦpr,kqq`Φpr,kq, the identity Φpr,kqpΦpr,kqq` “ I establishes that

A˚ “ pΦpr,kqq`

Since (11.8) implies that

“
E

rb˚, ξpkqs | Φpr,kqpξpkqq

‰

“ rb˚, A˚Φpr,kqpξpkqqs,

b˚ P Bpkq,˚ ,

which in turn implies that

“
E

ξpkq | Φpr,kqpξpkqq

‰

“ A˚Φpr,kqpξpkqq ,

we obtain

“
E

‰
ξpkq | Φpr,kqpξpkqq

“ pΦpr,kqq`Φpr,kqpξpkqq .

Since Theorem 3.3 established that the optimal solution operator Ψpk,rq corresponding to
the information map Φpr,kq was the Moore-Penrose inverse Ψpk,rq “ pΦpr,kqq` we obtain

“
E

‰
ξpkq | Φpr,kqpξpkqq

“ Ψpk,rq ˝ Φpr,kqpξpkqq ,

(11.11)

so that

“
ξpkq | Φpr,kqpξpkqq “ v
E

‰

“ Ψpk,rqpvq ,

thus establishing the ﬁnal assertion. To establish the martingale property, let us deﬁne
ˆξp1q :“ ξp1q and

‰
“
ˆξpkq :“ E
ξp1q | Φpk,1qpξp1qq

,

k “ 2, . . . .

as a sequence of Gaussian ﬁelds all on the same space Bp1q. (11.11) implies that

ˆξpkq “ Ψp1,kq ˝ Φpk,1qpξp1qq,

(11.12)

87

so that the identities Φpr,1q “ Φpr,kq ˝ Φpk,1q and Φpk,1q ˝ Ψp1,kq “ IBp1q from Theorem 4.4
imply that

Er ˆξpkq|Φpr,1qp ˆξpkqqs “ ErΨp1,kq ˝ Φpk,1qp ˆξp1qq|Φpr,1q ˝ Ψp1,kq ˝ Φpk,1qp ˆξp1qqs

“ ErΨp1,kq ˝ Φpk,1qp ˆξp1qq|Φpr,kq ˝ Φpk,1q ˝ Ψp1,kq ˝ Φpk,1qp ˆξp1qqs
“ ErΨp1,kq ˝ Φpk,1qp ˆξp1qq|Φpr,kq ˝ Φpk,1qp ˆξp1qqs
“ ErΨp1,kq ˝ Φpk,1qp ˆξp1qq|Φpr,1qp ˆξp1qqs
“ Ψp1,kq ˝ Φpk,1qEr ˆξp1q|Φpr,1qp ˆξp1qqs
“ Ψp1,kq ˝ Φpk,1q ˆξprq
“ Ψp1,kq ˝ Φpk,1qΨp1,rq ˝ Φpr,1q ˆξp1q
“ Ψp1,kq ˝ Φpk,1qΨp1,kqΨpk,rq ˝ Φpr,1q ˆξp1q
“ Ψp1,kq ˝ Ψpk,rq ˝ Φpr,1q ˆξp1q
“ Ψp1,rq ˝ Φpr,1q ˆξp1q
“ ˆξprq,

that is ˆξpkq is a reverse martingale.

11.8 Proof of Theorem 6.1

Let us simplify for the moment and deﬁne a scaled wavelet

¯χτ,ω,θptq :“ ω

`

1´β
2 cos

ωpt ´ τ q ` θ

˘

e´ ω2pt´τ q2

α2

,

t P R ,

(11.13)

so that at β “ 0 we have

χτ,ω,θ “

´

2
π3α2

¯ 1

4 ¯χτ,ω,θ .

(11.14)

Since

Kps, tq

:“

“

“

ż

π

ż

ż

´π
ż
π

R`

ż

R
ż

´π
ż
π

R`

ż

R
ż

´π

R`

R

¯χτ,ω,θpsq ¯χτ,ω,θptqdτ dω dθ

`
ωps ´ τ q ` θ

˘

cos

e´ ω2ps´τ q2

α2

`

cos

ωpt ´ τ q ` θ

˘

e´ ω2pt´τ q2

α2

dτ ω1´βdω dθ

`

˘

`

cos

ωps ´ τ q ` θ

cos

ωpt ´ τ q ` θ

˘

e´ ω2ps´τ q2

α2

e´ ω2pt´τ q2

α2

dτ ω1´βdω dθ ,

the trigonometric identity

`

cos
´

ωps ´ τ q ` θ
`

˘
ωps ´ τ q

˘

`
ωpt ´ τ q ` θ

˘

cos

`
cos θ ´ sin

˘
ωps ´ τ q

“

cos

¯´

sin θ

cos

˘
`
ωpt ´ τ q

`
cos θ ´ sin

˘

¯

ωpt ´ τ q

sin θ

88

and the integral identities
that

ż

ż

Kps, tq “ π

R`

R

ş

π

´π cos2 θdθ “

π

ş
´π sin2 θdθ “ π and

ş
π
´π cos θ sin θdθ “ 0 imply

´

`

˘
ωps´τ q

cos

`

cos

ωpt´τ q

`
˘
`sin

ωps´τ q

˘

¯

`
ωpt´τ q
sin

e´ ω2ps´τ q2

α2

e´ ω2pt´τ q2

α2

dτ ω1´βdω

so that the cosine subtraction formula implies

ż

ż

Kps, tq “ π

R`

R

˘
`
ωps ´ tq

cos

e´ ω2ps´τ q2

α2

e´ ω2pt´τ q2

α2

dτ ω1´βdω,

which amounts to

Kps, tq “ π(cid:60)

ż

ż

R`

R

Using the identity

eiωps´tqe´ ω2ps´τ q2

α2

e´ ω2pt´τ q2

α2

dτ ω1´βdω .

(11.15)

e´ ω2|s´τ |2

α2

e´ ω2|t´τ |2

α2 “ e´ ω2

α2

`

2τ 2´2ps`tqτ

˘

e´ ω2

α2

`

˘

s2`t2

and the integral identity
ż

e´aτ 2´2bτ dτ “

c

b2
a ,

e

π
a

a ą 0, b P C,

(11.16)

with the choice a :“ 2ω2
α2 and b :“ ´ ω2
the integral
ż
`

α2 ps ` tq, so that b2{a “ ω2
c

˘

2α2 ps ` tq2, we can evaluate

e´ ω2

α2

2τ 2´2ps`tqτ

dτ “

α
ω

π
2

ω2
2α2 ps`tq2

e

.

Consequently,

Kps, tq “ π(cid:60)

“ π(cid:60)

ż

ż

ż

eiωps´tqe´ ω2|s´τ |2

e´ ω2|t´τ |2
`
˘

α2

dτ ω1´βdω
˘

α2
`

eiωps´tqe´ ω2

α2

eiωps´tqe´ ω2

α2

s2`t2

`

s2`t2

α2

e´ ω2
˘´ż

2τ 2´2ps`tqτ

dτ ω1´βdω

`

2τ 2´2ps`tqτ

˘

¯
ω1´βdω

dτ

α2

e´ ω2
˘

`

eiωps´tqe´ ω2

α2

s2`t2

e

ω2
2α2 ps`tq2

ω´βdω

eiωps´tqe´ ω2

2α2 ps´tq2

ω´βdω

cospωps ´ tqqe´ ω2

2α2 ps´tq2

ω´βdω,

“ π(cid:60)
c

“ α

c

c

“ α

“ α

ż

ż

(cid:60)

(cid:60)
ż

π3
2
π3
2
π3
2

that is,

Kps, tq “ α

c

ż

π3
2

cospωps ´ tqqe´ ω2

2α2 ps´tq2

ω´βdω.

(11.17)

89

Utilizing the integral identity
ż

xµ´1e´p2x2

cospaxqdx “

p´µΓp

8

0

1
2

qe´ a2

4p2

µ
2

´

1F1

´

µ
2

`

1
2

,

1
2

;

a2
4p2

¯
,

a ą 0, µ ą 0,

from Gradshteyn and Ryzhik [37, 3.952:8], with a2
µ :“ 1 ´ β, we obtain

c

Kps, tq “ α

?
p

π3
2

1
2

2αq1´β|s ´ t|β´1Γp

1 ´ β
2

4p2 “ α2

2 , p2 “ |s´t|2

(11.18)
2α2 , a :“ |s ´ t| and

qe´ α2

´

¯
.

β
2

,

α2
2

1
;
2
¯ 1

2 1F1
´

2
π3α2

2 Kps, tq when

Consequently, reintroducing the scaling (11.14) obtains Kups, tq “
β “ 0. To indicate the dependence on β, we deﬁne

Kβps, tq “

?
p

1
2

2αq1´β|s ´ t|β´1Γp

1 ´ β
2

qe´ α2

2 1F1

β
2

,

1
2

;

α2
2

´

¯
,

so that Ku “ K0. For ﬁxed α, at the limit β “ 0, we have, recalling that Γp 1

(11.19)
?

π,

2 q “

`

and since 1F1

˘

0, 1

2 ; α2

2

K0ps, tq “

?

α|s ´ t|´1e´ α2

2 1F1

´

0,

¯

1
2

;

α2
2

2π
2

“ 1 we obtain

K0ps, tq “

?

2π
2

α|s ´ t|´1e´ α2
2 .

The scaling constant Hpβq deﬁned in the theorem satisﬁes

Hpβq :“

?
p

1
2

2αq1´βΓp1 ´

qe´ α2

2 1F1

β
2

β
2

,

1
2

;

α2
2

´

with

so that, by (11.19) we have

¯Hpβq :“ 2βπ

1
2

Γp β
2 q
Γp1 ´ β
2 q

,

1
Hpβq

Kβps, tq “

|s ´ t|β´1
¯Hpβq

.

Therefore, if we let Kβ denote the integral operator
`
Kβf

psq :“

ż

˘

1
Hpβq

R

Kβps, tqf ptqdt

¯
¯Hpβq

(11.20)

associated to the kernel Kβ scaled by Hpβq, it follows that

`

˘

Kβf

psq :

ż

R

1
¯Hpβq

|s ´ t|β´1f ptqdt,

ş
R |s ´ t|β´1f ptqdt cor-
namely that it is a scaled version of the integral operator f ÞÑ
responding to the Riesz potential |s ´ t|β´1. Consequently, according to Helgason [41,
Lem. 5.4 & Prop. 5.5], this scaling of the Riesz potential by ¯Hpβq implies the assertions
of the theorem.

90

11.9 Proof of Lemma 7.1

The outer most integral in the deﬁnition (7.1) of Kβ is
ż

ż

`

˘
y˚

ωpt ´ τ q ` θ

˘
dθ “

π

Nÿ

π

`

y

ωps ´ τ q ` θ

´π

Nÿ

´N

cneinpωps´τ q`θq

me´impωpt´τ q`θqdθ
c˚

´N

´π
Nÿ

Nÿ

“

n“´N

m“´N

einωps´τ qe´impωpt´τ qcnc˚
m

ż

π

´π

eipn´mqθdθ

so that

where

ż

ż

ż

Knps, tq “ (cid:60)

“ (cid:60)

“ (cid:60)

“ 2π

“ 2π

Nÿ

n“´N
Nÿ

n“´N

einωps´τ qe´inωpt´τ q|cn|2

einωps´tq|cn|2,

Kβps, tq “ 2π

Nÿ

n“´N

Knps, tq|cn|2,

einωps´tqe´ ω2|s´τ |2

e´ ω2|t´τ |2
`
˘

α2

dτ ω1´βdω
˘

α2
`

einωps´tqe´ ω2

α2

einωps´tqe´ ω2

α2

s2`t2

`

s2`t2

α2

e´ ω2
˘´ż

2τ 2´2ps`tqτ

dτ ω1´βdω

`

2τ 2´2ps`tqτ

˘

¯
ω1´βdω

dτ

α2

e´ ω2
˘

`
s2`t2

einωps´tqe´ ω2

α2

ω2
2α2 ps`tq2

e

ω´βdω

ż

ż

c

c

c

(cid:60)

(cid:60)
ż

π
2
π
2
π
2

“ α

“ α

“ α

einωps´tqe´ ω2

2α2 ps´tq2

ω´βdω

cospnωps ´ tqqe´ ω2

2α2 ps´tq2

ω´βdω .

Consequently, using the integral identity (11.18) with a “ |n||s ´ t|, µ “ 1 ´ β, p2 “

4p2 “ |n|α2
|s´t|2
2α2 , and therefore a2
?
π
?
2

Knps, tq “

?
p

α
2

2

and p “ |s´t|?
2α

we conclude that

2αq1´β|s ´ t|β´1Γp

1 ´ β
2

qe´ |n|α2

2

1F1

´

β
2

;

1
2

;

|n|α2
2

¯

,

´

which does not appear to have a nice dependency on n, except for β “ 0, where
1F1

“ 1 and Γp 1

π, so that

0; 1

?

¯

2 ; |n|α2

2

2 q “

Knps, tq “

α2πe´ |n|α2

2

|s ´ t|´1

1
2

91

and therefore

when written in terms of the norm }y}2 :“

K0ps, tq “ α2π2}y}2|s ´ t|´1,
ř
n“´N e´ |n|α2

N

2

|cn|2.

11.10 Proof of Lemma 7.2

For γ ą 0, let us evaluate the function

φpsq :“

8ÿ

n“´8

e´|n|γeins

(11.21)

with Fourier coeﬃcients ˆφpnq “ e´|n|γ. Since

φpsq “

8ÿ

e´|n|γeins

n“´8
8ÿ

“

“

n“1
8ÿ

n“1

e´nγeins ` 1 `

e´nγeins ` 1 `

´1ÿ

enγeins

n“´8
8ÿ

e´nγe´ins

n“1

“ 1 ` 2

e´nγ cospnsq,

8ÿ

n“1

the identity

1 ` 2

8ÿ

n“1

e´nγ cos ns “

sinhpγq
coshpγq ´ cospsq

(11.22)

of Gradshteyn and Ryzhik [37, 1.461:2] implies that

φpsq “

sinhpγq
coshpγq ´ cospsq

.

Consequently, with the choice γ :“ α2

4 in (11.21), that is, for

we ﬁnd that

φpsq :“

8ÿ

n“´8

e´|n| α2

4 eins,

φpsq “

sinhp α2
4 q
4 q ´ cospsq

coshp α2

.

(11.23)

We will need two basic facts about the Fourier transform of 2π-periodic functions, see

e.g. Katznelson [54, Sec. I]. If we denote the Fourier transform by ˆf pnq :“ 1
2π

π

ş
´π f psqe´ins, @n,

92

˘

the convolution theorem states that for periodic functions f, g P L1r´π, πs that the con-
ş
´π f ps ´ tqgptqdt is a well deﬁned periodic function in L1r´π, πs
volution
pnq “ ˆf pnqˆgpnq, @n . Moreover, for square integrable 2π-periodic func-

`
f ‹g
`
ˆ
f ‹ g

psq :“ 1
2π
˘

π

and that
tions in L2r´π, πs, the Parseval identity is

ř

8

n“´8 | ˆf pnq|2 “ 1

2π

ş
2π
0 |f psq|2 .

Consequently, observing that cn “ 0, n ă ´N, n ą N , the Parseval identity and the

convolution formula imply that

}y}2 “

Nÿ

e´ |n|α2

2

|cn|2

˘

8

n“´8}2
(cid:96)2

n“´N
`

`

“ }

“ }

e´ |n|α2
˘
ˆφˆy
ˆφ ‹ y
“ }
“ }φ ‹ y}2
ż

`

4 cn
8
n“´8}2
(cid:96)2
˘
8
n“´8}2
(cid:96)2

L2r´π,πs

“

“

“

“

“

|φ ‹ y|2
ż
ż ˇ
ˇ
ˇ
ż ´ż

φps ´ tqyptqdt

ˇ
ˇ
2
ˇ

ds
ż

¯

φps ´ tqyptqdt

φps ´ t1qy˚pt1qdt1

ds

φps ´ tqyptqφps ´ t1qy˚pt1qdtdt1ds

ż ż

ż

Gpt, t1qyptqy˚pt1qdtdt1 ,

ż

}y}2 “

Gpt, t1qyptqy˚pt1qdtdt1

ż

Gpt, t1q :“

φps ´ tqφps ´ t1qds

φpsq “

sinhp α2
4 q
4 q ´ cospsq

coshp α2

.

(11.24)

(11.25)

that is,

where

with

We can evaluate G using the identity (11.22) as follows: Since

ż

Gpt, t1q “

φps ´ tqφps ´ t1qds

ż ´

“

1 ` 2

8ÿ

n“1

e´n α2

4 cos nps ´ tq

1 ` 2

¯´

8ÿ

e´n1 α2

¯
4 cos n1ps ´ t1q

ds,

n1“1

93

and, for each product, we have

ż

ż

“

cos nps ´ tq cos n1ps ´ t1qds
`

˘`

cos ns cos nt ´ sin ns sin nt

ż

`

“ δn,n1

cos ns cos nt ´ sin ns sin nt

cos n1s cos n1t1 ´ sin n1s sin n1t1

˘

ds

˘`

cos ns cos nt1 ´ sin ns sin nt1

˘
ds .

ş

Using the L2-orthogonality of the cosines and the sines and the identities
and
ż

sin2 ns “ π, we conclude that

`

˘`

cos ns cos nt1 ´ sin ns sin nt1

cos nt cos nt1 ` sin nt sin nt1

˘
ds “ π

`

˘

ş

cos2 ns “ π

“ π cos npt ´ t1q

cos ns cos nt ´ sin ns sin nt

and therefore

ż

cos nps ´ tq cos n1ps ´ t1qds “ πδn,n1 cos npt ´ t1q .

(11.26)

Consequently, we obtain

ż ´

Gpt, t1q “

1 ` 2

ż ´

“

1 ` 4

8ÿ

n“1
8ÿ

e´n α2

e´n α2

¯´

4 cos nps ´ tq

1 ` 2

8ÿ

e´n1 α2

¯
4 cos n1ps ´ t1q

ds

n1“1

¯

2 cos nps ´ tq cos nps ´ t1q

ds

n“1
8ÿ

“ 2π ` 4π

n“1

e´n α2

2 cos npt ´ t1q

and therefore, using the identity (11.22) again, we conclude

Gpt, t1q “ 2π

sinhp α2
2 q
2 q ´ cospt ´ t1q

coshp α2

.

Acknowledgments The authors gratefully acknowledge support by the Air Force
Oﬃce of Scientiﬁc Research under award number FA9550-18-1-0271 (Games for Com-
putation and Learning).

References

[1] V. Adam, J. Hensman, and M. Sahani. Scalable transformed additive signal de-
composition by non-conjugate Gaussian process inference. In 2016 IEEE 26th In-
ternational Workshop on Machine Learning for Signal Processing (MLSP), pages
1–6. IEEE, 2016.

94

[2] M. Alvarez and N. D. Lawrence. Sparse convolved Gaussian processes for multi-
output regression. In Advances in Neural Information Processing Systems, pages
57–64, 2009.

[3] M. A. ´Alvarez and N. D. Lawrence. Computationally eﬃcient convolved multiple
output Gaussian processes. Journal of Machine Learning Research, 12(May):1459–
1500, 2011.

[4] M. A. Alvarez, L. Rosasco, and N. D. Lawrence. Kernels for vector-valued func-
tions: a review. Foundations and Trends R(cid:13) in Machine Learning, 4(3):195–266,
2012.

[5] F. Auger, P. Flandrin, Y.-T. Lin, S. McLaughlin, S. Meignen, T. Oberlin, and H.-
T. Wu. Time-frequency reassignment and synchrosqueezing: An overview. IEEE
Signal Processing Magazine, 30(6):32–41, 2013.

[6] J. C. A. Barata and M. S. Hussein. The Moore–Penrose pseudoinverse: A tutorial

review of the theory. Brazilian Journal of Physics, 42(1-2):146–165, 2012.

[7] B. Boashash. Estimating and interpreting the instantaneous frequency of a signal.

I. Fundamentals. Proceedings of the IEEE, 80(4):520–538, 1992.

[8] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimiza-
tion and statistical learning via the alternating direction method of multipliers.
Foundations and Trends R(cid:13) in Machine learning, 3(1):1–122, 2011.

[9] P Boyle and M. Frean. Dependent Gaussian processes.
Information Processing Systems, pages 217–224, 2005.

In Advances in Neural

[10] P. Boyle and M. Frean. Multiple output Gaussian process regression. 2005.

[11] M. Costa, A. A. Priplata, L. A. Lipsitz, Z. Wu, N. E. Huang, A. L. Goldberger,
and C.-K. Peng. Noise and poise: enhancement of postural complexity in the
elderly with a stochastic-resonance–based therapy. EPL (Europhysics Letters),
77(6):68008, 2007.

[12] K. T. Coughlin and K.-K. Tung.

11-year solar cycle in the stratosphere ex-
tracted by the empirical mode decomposition method. Advances in Space Research,
34(2):323–329, 2004.

[13] N. Cressie. The origins of Kriging. Mathematical Geology, 22(3):239–252, 1990.

[14] L. Csat´o. Gaussian Processes: Iterative Sparse Approximations. PhD thesis, Aston

University Birmingham, UK, 2002.

[15] L. Csat´o and M. Opper. Sparse on-line Gaussian processes. Neural Computation,

14(3):641–668, 2002.

95

[16] L. Csat´o, M. Opper, and O. Winther. TAP Gibbs free energy, belief propagation
and sparsity. In Advances in Neural Information Processing Systems, pages 657–
663, 2002.

[17] D. A. T. Cummings, R. A. Irizarry, N. E. Huang, T. P. Endy, A. Nisalak,
K. Ungchusak, and D. S. Burke. Travelling waves in the occurrence of dengue
haemorrhagic fever in Thailand. Nature, 427(6972):344–347, 2004.

[18] I. Daubechies, J. Lu, and H.-T. Wu. Synchrosqueezed wavelet transforms: An
empirical mode decomposition-like tool. Applied and Computational Harmonic
Analysis, 30(2):243–261, 2011.

[19] I. Daubechies and S. Maes. A nonlinear squeezing of the continuous wavelet trans-
In A. Aldroubi and M. Unser, editors,

form based on auditory nerve models.
Wavelets in Medicine and Biology, pages 527–546. World Scientiﬁc, 1996.

[20] R. Djemili, H. Bourouba, and M. C. Ammara Korba. Application of empirical
mode decomposition and artiﬁcial neural network for the classiﬁcation of normal
and epileptic EEG signals. Biocybernetics and Biomedical Engineering, 36(1):285–
291, 2016.

[21] K. Dragomiretskiy and D. Zosso. Variational mode decomposition. IEEE Trans-

actions on Signal Processing, 62(3):531–544, 2014.

[22] R. M. Dudley. Real Analysis and Probability, volume 74 of Cambridge Studies in
Advanced Mathematics. Cambridge University Press, Cambridge, 2002. Revised
reprint of the 1989 original.

[23] N. Durrande, D. Ginsbourger, and O. Roustant. Additive kernels for Gaussian
process modeling. Annales de la Facult´ee de Sciences de Toulouse, page 17, 2010.

[24] N. Durrande, D. Ginsbourger, and O. Roustant. Additive covariance kernels for
high-dimensional Gaussian process modeling. In Annales de la Facult´e des sciences
de Toulouse: Math´ematiques, volume 21, pages 481–499, 2012.

[25] N. Durrande, J. Hensman, M. Rattray, and N. D. Lawrence. Detecting periodicities

with Gaussian processes. PeerJ Computer Science, 2:e50, 2016.

[26] N. Durrande, J. Hensman, M. Rattray, and N. D. Lawrence. Gaussian process

models for periodicity detection. PeerJ Computer Science, 2016.

[27] D. K. Duvenaud, H. Nickisch, and C. E. Rasmussen. Additive Gaussian processes.
In Advances in Neural Information Processing Systems, pages 226–234, 2011.

[28] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of Inverse Problems,

volume 375. Springer Science & Business Media, 1996.

[29] Y. Fan, G. M. James, and P. Radchenko. Functional additive regression. The

Annals of Statistics, 43(5):2296–2325, 2015.

96

[30] M. Feldman. Time-varying vibration decomposition and analysis based on the
Hilbert transform. Journal of Sound and Vibration, 295(3-5):518–530, 2006.

[31] P. Flandrin and P. Goncalves. Empirical mode decompositions as data-driven
wavelet-like expansions. International Journal of Wavelets, Multiresolution and
Information Processing, 2(04):477–496, 2004.

[32] P. Flandrin, P. Gon¸calves, and G. Rilling. EMD equivalent ﬁlter banks, from
interpretation to applications. In Hilbert-Huang Transform and its Applications,
pages 57–74. World Scientiﬁc, 2005.

[33] P. Flandrin, G. Rilling, and P. Goncalves. Empirical mode decomposition as a

ﬁlter bank. IEEE Signal Processing Letters, 11(2):112–114, 2004.

[34] D. Gabor. Theory of communication. part 1: The analysis of information. Jour-
nal of the Institution of Electrical Engineers-Part III: Radio and Communication
Engineering, 93(26):429–441, 1946.

[35] L.

S. Gandin.

Gidrometeorotogicheskoizeda-
rael Program for Scientiﬁc Translations, Jerusalem, 1965, 238 pp.). 1963.

Objective

ﬁelds:
tel’stvo(GIMIZ), Leningrad (translated by Is-

of meteorological

analysis

[36] J. Gilles. Empirical wavelet transform. IEEE Transactions on Signal Processing,

61(16):3999–4010, 2013.

[37] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series, and Products. Aca-

demic, 6th edition, 2000.

[38] B. Hamzi and H. Owhadi. Learning dynamical systems from data: a simple cross-

validation perspective. arXiv:2007.05074, 2020.

[39] T. Hastie and R. Tibshirani. Generalized additive models. Statistical Science,

1(w3):297–310, 1986.

[40] T. J. Hastie and R. J. Tibshirani. Generalized Additive Models, volume 43. CRC

press, 1990.

[41] S. Helgason. The Radon Transform, volume 2. Springer, 1999.

[42] J. Hensman, N. Durrande, and A. Solin. Variational Fourier features for Gaussian
processes. The Journal of Machine Learning Research, 18(1):5537–5588, 2017.

[43] J. Hensman, N. Fusi, and N. D. Lawrence. Gaussian processes for big data. In

Uncertainty in Artiﬁcial Intelligence, page 282. Citeseer, 2013.

[44] J. Hensman, A. G. Matthews, and Z. Ghahramani. Scalable variational Gaus-
sian process classiﬁcation. Proceedings of Machine Learning Research, 38:351–360,
2015.

97

[45] M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley. Stochastic variational infer-

ence. The Journal of Machine Learning Research, 14(1):1303–1347, 2013.

[46] T. Y. Hou and Z. Shi. Adaptive data analysis via sparse time-frequency represen-

tation. Advances in Adaptive Data Analysis, 3(01n02):1–28, 2011.

[47] T. Y. Hou, Z. Shi, and P. Tavallali. Sparse time frequency representations and
dynamical systems. Communications in Mathematical Sciences, 13(3):673–694,
2015.

[48] C. Huang, L. Yang, and Y. Wang. Convergence of a convolution-ﬁltering-based
algorithm for empirical mode decomposition. Advances in Adaptive Data Analysis,
1(04):561–571, 2009.

[49] N. E. Huang. Introduction to the Hilbert-Huang transform and its related math-
In N. E. Huang and S. S. P. Shen, editors, Hilbert-Huang

ematical problems.
Transformation and it Applications, pages 1–26. World Scientiﬁc, 2014.

[50] N. E. Huang and S. S. P. Shen. Hilbert-Huang Transform and its Applications,

volume 16. World Scientiﬁc, 2014.

[51] N. E. Huang, Z. Shen, S. R. Long, M. C. Wu, H. H. Shih, Q. Zheng, N.-C. Yen,
C. C. Tung, and H. H. Liu. The empirical mode decomposition and the Hilbert
spectrum for nonlinear and non-stationary time series analysis. Proceedings of
the Royal Society of London. Series A: Mathematical, Physical and Engineering
Sciences, 454(1971):903–995, 1998.

[52] N. E. Huang and Z. Wu. A review on Hilbert-Huang transform: Method and its

applications to geophysical studies. Reviews of Geophysics, 46(2), 2008.

[53] M. Hutson. Has artiﬁcial intelligence become alchemy? Science, 360(6388):861,

2018.

[54] Y. Katznelson. An Introduction to Harmonic Analysis. Cambridge University

Press, 2004.

[55] R. Kress. Linear Integral Equations, volume 82. Springer, 3rd edition, 1989.

[56] N. D. Lawrence, M. Seeger, and R. Herbrich. Fast sparse Gaussian process meth-
ods: The informative vector machine. In Advances in Neural Information Process-
ing Systems, pages 625–632, 2003.

[57] Y. LeCun. The deep learning- applied math connection.

In SIAM Conference
on Mathematics of Data Science (MDS20), 2020. Streamd live on June 24, 2020,
https://www.youtube.com/watch?v=y9gutjsvc1c&feature=youtu.be&t=676.

[58] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436–444,

2015.

98

[59] C. Li and M. Liang. Time–frequency signal analysis for gearbox fault diagnosis
using a generalized synchrosqueezing transform. Mechanical Systems and Signal
Processing, 26:205–217, 2012.

[60] C.-Y. Lin, L. Su, and H.-T. Wu. Wave-shape function analysis. Journal of Fourier

Analysis and Applications, 24(2):451–505, 2018.

[61] L. Lin, Y. Wang, and H. Zhou. Iterative ﬁltering as an alternative algorithm for
empirical mode decomposition. Advances in Adaptive Data Analysis, 1(04):543–
560, 2009.

[62] A. Liutkus, R. Badeau, and G. Richard. Gaussian processes for underdetermined
IEEE Transactions on Signal Processing, 59(7):3155–3167,

source separation.
2011.

[63] M. Luki´c and J. Beder. Stochastic processes with sample paths in reproduc-
ing kernel Hilbert spaces. Transactions of the American Mathematical Society,
353(10):3945–3969, 2001.

[64] W. Ma, S. Yin, C. Jiang, and Y. Zhang. Variational mode decomposition de-
noising combined with the Hausdorﬀ distance. Review of Scientiﬁc Instruments,
88(3):035109, 2017.

[65] S. Maji, A. C. Berg, and J. Malik. Eﬃcient classiﬁcation for additive kernel SVMs.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:66–77, 2013.

[66] G. Matheron. Principles of geostatistics. Economic Geology, 58(8):1246–1266,

1963.

[67] G. Matheron. Trait´e de G´eostatistique Appliqu´ee. 2. Le Krigeage. Editions Tech-

nip, 1963.

[68] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake. Finite mixture models. Annual

Review of Statistics and its Application, 6:355–378, 2019.

[69] A. Melkumyan and F. Ramos. Multi-kernel Gaussian processes. In Twenty-second

International Joint Conference on Artiﬁcial Intelligence, 2011.

[70] R. K. Merton. The Sociology of Science: Theoretical and Empirical Investigations.

University of Chicago Press, 1973.

[71] C. A. Micchelli and T. J. Rivlin. A survey of optimal recovery. In Optimal Esti-

mation in Approximation Theory, pages 1–54. Springer, 1977.

[72] T. Oberlin, S. Meignen, and V. Perrier. The Fourier-based synchrosqueezing trans-
In 2014 IEEE International Conference on Acoustics, Speech and Signal

form.
Processing (ICASSP), pages 315–319. IEEE, 2014.

99

[73] H. Owhadi. Multigrid with rough coeﬃcients and multiresolution operator de-
composition from hierarchical information games. SIAM Review, 59(1):99–149,
2017.

[74] H. Owhadi and C. Scovel. Operator Adapted Wavelets, Fast Solvers, and Numer-
ical Homogenization, from a game theoretic approach to numerical approximation
and algorithm design. Cambridge Monographs on Applied and Computational
Mathematics. Cambridge University Press, 2019.

[75] H. Owhadi, C. Scovel, and F. Sch¨afer. Statistical numerical approximation. Notices

of the AMS, 66(10):1608–1617, 2019.

[76] S. Park and S. Choi. Gaussian processes for source separation.

In 2008 IEEE
International Conference on Acoustics, Speech and Signal Processing, pages 1909–
1912. IEEE, 2008.

[77] A. B. Patel, M. T. Nguyen, and R. Baraniuk. A probabilistic framework for deep
learning. In Advances in Neural Information Processing Systems, pages 2558–2566,
2016.

[78] T. A. Plate. Accuracy versus interpretability in ﬂexible modeling: Implementing
a tradeoﬀ using Gaussian process models. Behaviormetrika, 26(1):29–50, 1999.

[79] D. Preot¸iuc-Pietro and T. Cohn. A temporal model of text periodicities using
Gaussian processes. In Proceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 977–988, 2013.

[80] J. Qui˜nonero-Candela. Learning with Uncertainty: Gaussian Processes and Rel-
evance Vector Machines. PhD thesis, Technical University of Denmark Lyngby,
Denmark, 2004.

[81] J. Qui˜nonero-Candela and C. E. Rasmussen. A unifying view of sparse approximate
Gaussian process regression. Journal of Machine Learning Research, 6(Dec):1939–
1959, 2005.

[82] M. Raissi, P. Perdikaris, and G. E. Karniadakis. Machine learning of linear dif-
ferential equations using Gaussian processes. Journal of Computational Physics,
348:683–693, 2017.

[83] M. M. Rao. Foundations of Stochastic Analysis. Academic Press, 1981.

[84] C. E. Rasmussen. Gaussian processes in machine learning. In Summer School on

Machine Learning, pages 63–71. Springer, 2003.

[85] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning,

volume 2. MIT press Cambridge, MA, 2006.

[86] G. Rilling and P. Flandrin. One or two frequencies? The empirical mode decom-
position answers. IEEE Transactions on Signal Processing, 56(1):85–95, 2007.

100

[87] G. Rilling, P. Flandrin, and P. Goncalves. On empirical mode decomposition
and its algorithms. In IEEE-EURASIP Workshop on Nonlinear Signal and Image
Processing, volume 3, pages 8–11. NSIP-03, Grado (I), 2003.

[88] F. Sch¨afer, T. J. Sullivan, and H. Owhadi. Compression,

inversion, and ap-
proximate PCA of dense kernel matrices at near-linear computational complexity.
arXiv:1706.02205, 2017.

[89] A. Schwaighofer and V. Tresp. Transductive and inductive methods for approxi-
mate Gaussian process regression. In Advances in Neural Information Processing
Systems, pages 977–984, 2003.

[90] M. Seeger. Bayesian Gaussian process models: PAC-Bayesian generalisation error
bounds and sparse approximations. Technical report, University of Edinburgh,
2003.

[91] M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward selection to speed
up sparse Gaussian process regression. In Proceedings of the Ninth International
Workshop on Artiﬁcial Intelligence and Statistics, 2003.

[92] A. J. Smola and P. L. Bartlett. Sparse greedy Gaussian process regression. In

Advances in Neural Information Processing Systems, pages 619–625, 2001.

[93] E. Snelson and Z. Ghahramani. Sparse Gaussian processes using pseudo-inputs.

In Advances in Neural Information Processing Systems, pages 1257–1264, 2006.

[94] E. P. Souza Neto, M. A. Custaud, J. C. Cejka, P. Abry, J. Frutoso, C. Gharib,
and P. Flandrin. Assessment of cardiovascular autonomic control by the empirical
mode decomposition. Methods of Information in Medicine, 43(01):60–65, 2004.

[95] I. Steinwart and A. Christmann. Support Vector Machines. Springer Science &

Business Media, 2008.

[96] S. M. Stigler. Stigler’s law of eponymy. Transactions of the New York Academy of

Sciences, 39(1 Series II):147–157, 1980.

[97] C. J. Stone. Additive regression and other nonparametric models. The annals of

Statistics, pages 689–705, 1985.

[98] G. Thakur. The synchrosqueezing transform for instantaneous spectral analysis.

In Excursions in Harmonic Analysis, Volume 4, pages 397–406. Springer, 2015.

[99] G. Thakur, E. Brevdo, N. S. Fuˇckar, and H.-T. Wu. The synchrosqueezing algo-
rithm for time-varying spectral analysis: Robustness properties and new paleocli-
mate applications. Signal Processing, 93(5):1079–1094, 2013.

[100] M. Titsias. Variational learning of inducing variables in sparse Gaussian processes.

In Artiﬁcial Intelligence and Statistics, pages 567–574, 2009.

101

[101] M. K. Titsias and M. L´azaro-Gredilla. Spike and slab variational inference for
multi-task and multiple kernel learning. In Advances in Neural Information Pro-
cessing Systems, pages 2339–2347, 2011.

[102] M. E. Torres, M. A. Colominas, G. Schlotthauer, and P. Flandrin. A complete
ensemble empirical mode decomposition with adaptive noise. In 2011 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
4144–4147. IEEE, 2011.

[103] V. Tresp. A Bayesian committee machine. Neural computation, 12(11):2719–2741,

2000.

[104] S. Wang, X. Chen, G. Cai, B. Chen, X. Li, and Z. He. Matching demodulation
transform and synchrosqueezing in time-frequency analysis. IEEE Transactions
on Signal Processing, 62(1):69–84, 2013.

[105] C. K. I. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel
machines. In Advances in Neural Information Processing Systems, pages 682–688,
2001.

[106] K. I. Williams, C and C. E. Rasmussen. Gaussian processes for regression.

In

Advances in Neural Information Processing Systems, pages 514–520, 1996.

[107] Z. Wu and N. E. Huang. A study of the characteristics of white noise using the
empirical mode decomposition method. Proceedings of the Royal Society of London.
Series A: Mathematical, Physical and Engineering Sciences, 460(2046):1597–1611,
2004.

[108] Z. Wu and N. E. Huang. Ensemble empirical mode decomposition: a noise-assisted
data analysis method. Advances in Adaptive Data Analysis, 1(01):1–41, 2009.

[109] Z. Wu, E. K. Schnieder, Z.-Z. Hu, and L. Cao. The Impact of Global Warming
on ENSO Variability in Climate Records, volume 110. Center for Ocean-Land-
Atmosphere Studies Calverton, 2001.

[110] T. W. Yee. Vector Generalized Linear and Additive Models: with an Implementa-

tion in R. Springer, 2015.

[111] T. W. Yee and C. J. Wild. Vector generalized additive models. Journal of the

Royal Statistical Society: Series B (Methodological), 58(3):481–493, 1996.

[112] G. R. Yoo. Learning Patterns with Kernels and Learning Kernels from Patterns.

PhD thesis, California Institute of Technology, 2020.

[113] K. Yosida. Functional Analysis. Springer-Verlag, Berlin, 5th edition, 1978.

[114] K. Yu, V. Tresp, and A. Schwaighofer. Learning Gaussian processes from multiple
tasks. In Proceedings of the 22nd International Conference on Machine Learning,
pages 1012–1019, 2005.

102

