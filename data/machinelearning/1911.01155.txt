9
1
0
2

v
o
N
4

]

G
L
.
s
c
[

1
v
5
5
1
1
0
.
1
1
9
1
:
v
i
X
r
a

Learning based Methods for Code Runtime
Complexity Prediction

Jagriti Sikka1, Kushal Satya1, Yaman Kumar1, Shagun Uppal2, Rajiv Ratn
Shah2, and Roger Zimmermann3

1 Adobe, Noida
2 Midas Lab, IIIT Delhi
3 School of Computing, National University of Singapore
{jsikka,satya,ykumar}@adobe.com {shagun16088,rajivratn}@iiitd.ac.in
rogerz@comp.nus.edu.sg

Abstract. Predicting the runtime complexity of a programming code is
an arduous task. In fact, even for humans, it requires a subtle analysis
and comprehensive knowledge of algorithms to predict time complex-
ity with high ﬁdelity, given any code. As per Turing’s Halting problem
proof, estimating code complexity is mathematically impossible. Never-
theless, an approximate solution to such a task can help developers to
get real-time feedback for the eﬃciency of their code. In this work, we
model this problem as a machine learning task and check its feasibility
with thorough analysis. Due to the lack of any open source dataset for
this task, we propose our own annotated dataset CoRCoD: Code Run-
time Complexity Dataset 4, extracted from online judges. We establish
baselines using two diﬀerent approaches: feature engineering and code
embeddings, to achieve state of the art results and compare their per-
formances. Such solutions can be widely useful in potential applications
like automatically grading coding assignments, IDE-integrated tools for
static code analysis, and others.

Keywords: Time Complexity · Code Embeddings · Code Analysis.

1

Introduction

Time Complexity computation is a crucial aspect in the study and design of
well-structured and computationally eﬃcient algorithms. It is a measure of the
performance of a solution for a given problem. As a popular mistaken consid-
eration, it is not the execution time of a code. Execution time depends upon a
number of factors such as the operating system, hardware, processors etc. Since
execution time is machine dependent, it is not used as a standard measure to
analyze the eﬃciency of algorithms. Formally, Time Complexity quantiﬁes the
amount of time taken by an algorithm to process as a function of the input.
For a given algorithm, we consider its worst case complexity, which reﬂects the

4 The

complete dataset
research/corcod-dataset.

is available

for use at https://github.com/midas-

 
 
 
 
 
 
2

Authors Suppressed Due to Excessive Length

maximum time required to process it, given an input. Time complexity is rep-
resented in Big O notation. O(n) denotes the asymptotic upper bound of an
algorithm as a function of the input size n. Typically, the complexity classes in
Computer Science refer to P, NP classes of decision problems, however, for the
entire length of this paper, complexity class refers to a category of time com-
plexity. The commonly considered categories in computer science as well in our
work are O(1), O(logn), O(n) , O(nlogn) and O(n2).

In this work, we try to predict the time complexity of a solution, given the
code. This can have widespread applications, especially in the ﬁeld of education.
It can be used in automatic evaluation of code submissions on diﬀerent online
judges. It can also aid in static analyses, informing developers how optimized
their code is, enabling more eﬃcient development of industry level solutions.

A number of ways have been proposed to estimate code complexity. McCabe
et al. [13] is one of the earliest works done in this area, which proposed Cyclo-
matic Complexity. It is deﬁned as the quantitative measure of the number
of linearly independent paths through a program’s source code computed using
the control ﬂow graph of the program. Cyclomatic Complexity of a code can be
calculated through Equation 1.

C = E − N + M

(1)

where E is the number of the edges, N is the number of nodes and M is the
number of connected components.

Cyclomatic Complexity quantitatively measures a program’s logical strength
based on existing decision paths in the source code. However, the number of
independent paths does not represent how many times these paths were executed.
Hence, it is not a robust measure for time complexity.

Bentley et al. [8] proposed the Master Theorem. For the generic divide and
conquer problem which divides a problem of input size n into a subproblems each
of size n
b ,and combine the result in f (n) operations, Master theorem expresses
the recurrence relation as Equation 2.

T (n) = aT (

n
b

) + f (n)

(2)

However, the master theorem is limited to divide and conquer problems and

has several constraints on the permissible values of a, b and f (n).

Mathematically speaking, it is impossible to ﬁnd a universal function to com-
pute the time complexity of all programs. Rice’s theorem and other works in this
area [1,7] have established that the runtime complexity for problems in category
P is undecidable i.e. it is impossible to formulate a mathematical function to
calculate the complexity of any code with polynomial order complexity.

Therefore, we need a Machine Learning based solution which can learn the
internal structure of the code eﬀectively. Recent research in the areas of machine
learning and deep learning for programming codes provide several potential ap-
proaches which can be extended to solve this problem [6,14]. Also, several "Big
Code" datasets have been made available publicly. The Public Git Archive is a

Learning based Methods for Code Runtime Complexity Prediction

3

dataset of a large collection of Github repositories [12], [18] and [17] are datasets
of Question-code pairs mined from Stack Overﬂow. However, to the best of our
knowledge, at the time of writing this paper, there is no existing public dataset
that, given the source code, gives runtime complexity of the source code. In our
work, we have tried to address this problem by creating a Code Runtime Com-
plexity Dataset (CoRCoD) consisting of 932 code ﬁles belonging to 5 diﬀerent
classes of complexities, namely O(1), O(logn), O(n), O(nlogn) and O(n2) (see
Table 1).

We aim to substantially explore and solve the problem of code runtime com-

plexity prediction using machine learning with the following contributions:

• Releasing a novel annotated dataset of program codes with their runtime

complexities.

• Proposing baselines of ML models with hand-engineered feature and study

of how these features aﬀect the computational eﬃciency of the codes.

• Proposing another baseline, the generation of code embeddings from Ab-

stract Syntax Tree of source codes to perform classiﬁcation.

Furthermore, we ﬁnd that code embeddings have a comparable performance
to hand-engineered features for classiﬁcation using Support Vector Machines
(SVMs). To the best of our knowledge, CoRCoD is the ﬁrst public dataset for
code runtime complexity, and this is the ﬁrst work that uses Machine Learning
for runtime complexity prediction.

The rest of this paper is structured as follows. In Section 3, we talk about
dataset curation and its key characteristics. We experiment using two diﬀerent
baselines on the dataset: classiﬁcation using hand engineered features extracted
from code and using graph based methods to extract the code embeddings via
Abstract Syntax Tree of code. Section 4 explains the details and key ﬁndings of
these two approaches. In Section 5, we enumerate the results of our model and
data ablation experiments performed on these two baselines.

2 Related Work

In recent years, there has been extensive research in the deep learning community
on programming codes. Most of the research has been focused on two buckets,
either on predicting some structure/attribute in the program or generating code
snippets that are syntactically and/or semantically correct. In the latter case,
Sun et al. [16] made use of the Abstract Syntax Tree (AST) of a program, and
generated code by predicting the grammar rules using a CNN.

Variable/Method name prediction is a widely attempted problem, wherein
Allamanis et al. [4] used a convolutional neural network with attention technique
to predict method names, Alon et al. [5] suggested the use of AST paths to be
used as context for generating code embeddings and training classiﬁers on top

4

Authors Suppressed Due to Excessive Length

of them. Yonai et al. [19] used call graphs to compute method embeddings and
recommend names of existing methods with function similar to target function.
Another popular prediction problem is that of defect prediction, given a piece
of code. Li et al. [11] used Abstract Syntax Trees of programs in their CNN for
feature generation which are then used for defect prediction. A major goal in
all these approaches is to come up with a representation of the source program,
which eﬀectively captures the syntactic and semantic features of the program.
Chen and Monperrus [9] performed a survey on word embedding techniques used
on source codes. However, so far, there has been no such work for predicting time
complexity of programs using code embeddings. We have established the same
as one of our baselines using graph2vec [14].

Srikant and Aggarwal[15] extract hand-engineered features from Control Flow
and Data Dependency graphs of programs such as number of nested loops, num-
ber of instances of if statements in a loop etc. for automatic grading of programs.
They then used the grading criteria, that correct test programs would have sim-
ilar programming constructs/features as those in the correct hand-graded pro-
grams. We use the same idea of identifying key features as the other baseline,
which are constructs that a human evaluator would look at, to compute com-
plexity and use them to train the classiﬁcation models. Though, unlike [15],
our features are problem independent. Moreover, their dataset is not publicly
available.

3 Dataset

To construct our dataset, we collected source codes of diﬀerent problems from
Codeforces5. Codeforces is a platform that regularly hosts programming contests.
The large availability of contests having a wide variety of problems both in terms
of data structures and algorithms as well as runtime complexity, made Codeforces
a viable choice for our dataset.

Complexity class Number of samples

O(n)
O(n2)
O(nlogn)
O(1)
O(logn)

385
200
150
143
55

Table 1: Classwise data distribution

Features from Code Samples
Number of methods Number of breaks
Number of switches Number of loops

Priority queue present
Sort present
Hash set present
Hash map present
Nested loop depth Recursion present
Number of variables
Number of statements Number of jumps
Table 2: Extracted features

Number of ifs

For the purpose of construction of our dataset, we collected Java source codes
from Codeforces. We used the Codeforces API to retrieve problem and contest

5 https://codeforces.com

Learning based Methods for Code Runtime Complexity Prediction

5

information, and further used web scraping to download the solution source
codes.

In order to ensure correctness of evaluated runtime complexity, the source
codes selected should be devoid of issues such as compilation errors and segmen-
tation faults. To meet this criterion, we ﬁltered the source codes on the basis
of their verdict and only selected the codes having verdicts Accepted or Time
limit exceeded (TLE). For codes having TLE verdict, we ensured accuracy of so-
lutions by only selecting codes that successfully passed at least four Test Cases.
This criterion also allowed us to include multiple solutions for a single prob-
lem, diﬀerent solutions having diﬀerent runtime complexities. These codes were
then manually annotated by a group of ﬁve experts, hailing from programming
background each with a bachelor’s degree in Computer Science or related ﬁeld.
Each code was analyzed and annotated by two experts, in order to minimize the
potential for error. Only the order of complexity was recorded, for example, a
solution having two variable inputs, n and m, and having a runtime complexity
of O(n ∗ m) is labeled as n_square (O(n2)).

Certain agreed upon rules were followed for the annotation process. The ra-
tionale relies on the underlying implementations of these data structures in Java.
Following points list down the rules followed for annotation and the correspond-
ing rationale:

• Sorting algorithm’s implementation in Java collections has worst case com-

plexity O(nlogn).

• Insertion/retrieval in HashSet and HashMap is annotated to be O(1), given

n elements.

• TreeSet and TreeMap are implemented as Red-Black trees and thus have

O(logn) complexity for insertion/retrieval.

class noOfNestedLoops extends ASTVisitor {

int current = 0;
int max_depth = 0;
@Override
bool visit(WhileStatement node) {

current += 1;
max_depth = max(current, max_depth);
return true;

}
@Override
void endVisit(WhileStatement node){

current -= 1;

}

}

Listing 1: Extracting number Of nested loops using ASTVisitor

6

Authors Suppressed Due to Excessive Length

We removed few classes with insuﬃcient data points, and ended up with
932 source codes, 5 complexity classes, corresponding annotation and extracted
features. We selected nearly 400 problems from 170 contests, picking an average
of 3 problems per contest. For 120 of these problems, we collected 4-5 diﬀerent
solutions, with diﬀerent complexities.

4 Solution Approach

The classiﬁcation model is trained using two approaches: one, extracting hand-
engineered features from code using static analysis and two, learning a generic
representation of codes in the form of code embeddings.

4.1 Feature Engineering

Feature Extraction. We identiﬁed key coding constructs and extracted 14 fea-
tures (refer Table 2). We extracted these features from the Abstract Syntax
Tree (AST) of source codes. AST is a tree representation of syntax rules of a
programming language. ASTs are used by compilers to check codes for accuracy.
We used Eclipse JDT for feature extraction. A generic representation of AST as
parsed by ASTParser in JDT is shown in Figure 1.

Fig. 1: Extraction of features from code using AST Parser

Learning based Methods for Code Runtime Complexity Prediction

7

An ASTParser object creates the AST, and the ASTVisitor object "visits"
the nodes of the tree via visit and endVisit methods using Depth First Search.
One of the features chosen was the maximum depth of nested loops. Code snippet
in Listing 1 depicts how the value of depth of Nested Loops was calculated
using ASTVisitor provided by JDT. Other features were calculated in a similar
manner.

We observed that our code samples often have unused code like methods
or class implementations never invoked from the main function. Removing such
unused code manually from each code sample is tedious. Instead, we used JDT
plugins to identify the methods reachable from main function and used those
methods for extracting listed features. The same technique was also used while
creating the AST for the next baseline.

(a) Depth of nested loop

(b) Number of sort calls

Fig. 2: Density plot for the diﬀerent features

Figure 2 represents the density distribution of features across diﬀerent classes.
For nested loops, n_square has peak at depth 2 as expected; similarly n and
nlogn have peak at depth 1 loop depth (see Figure 2(a)). For number of sort
calls, n and n_square have higher peaks at 0, indicating peak in the absence of
sort whereas nlogn peaks both at 0 and 1 (see Figure 2(b)). Upon qualitative
analysis, we found that a large number of solutions were performing binary search
within a for loop, which explains the peak at 0, and others were simply using
sort, hence the peak at 1. This conﬁrms our intuition that sort calls and nested
loops are important parameters in complexity computation.

4.2 Code Embeddings

The Abstract Syntax Tree of a program captures comprehensive information
regarding a program’s structure, syntactic and semantic relationships between
variables and methods. An eﬀective method to incorporate this information is to
compute code embeddings from the program’s AST. We use graph2vec , a neural

8

Authors Suppressed Due to Excessive Length

embedding framework [14], to compute embeddings. Graph2vec automatically
generates task agnostic embeddings, and does not require a large corpus of data,
making it apt for our problem.We used the graph2vec implementation from [2]
to compute code embeddings.

Graph2vec is analogous to doc2vec[10] which predicts a document embedding
given the sequence of words in it. The goal of graph2vec is, given a set of graphs
G = {G1, G2, ...Gn}, learn a δ dimensional embedding vector for each graph.
Here, each graph G is represented as (N, E, λ) where N are the nodes of the
graph, E the edges and λ represents a function n → l which assigns a unique label
from alphabet l to every node n ∈ N . To achieve the same, graph2vec extracts
nonlinear substructures, more speciﬁcally, rooted subgraphs from each graph
which are analogical to words in doc2vec. It uses skipgram model for learning
graph embeddings which correspond to code embeddings in our scenario. The
model works by considering a subgraph sj ∈ c(gi) to be occurring in the context
of graph gi and tries to maximize the log likelihood in Equation 3:

D
(cid:88)

j=1

log P r(sj|gi)

(3)

where c(gi) gives all subgraphs of a graph gi and D is the total number of
subgraphs in the entire graph corpus.

We extracted AST from all codes using the JDT plugins. Each node in AST
has two attributes: a Node Type and an optional Node Value. For e.g., a Method-
Declaration Type node will have the declared function name as the node value.
Graph2vec expects each node to have a single label. To get a single label, we
followed two diﬀerent representations:

1. Concatenating Node Type and Node Value.
2. Choosing selectively for each type of node whether to include node type or
node value. For instance, every identiﬁer node has a SimpleName node as its
child. For all such nodes, only node value i.e. identiﬁer name was considered
as the label.

For both the AST representations, we used graph2vec to generate 1024-
dimensional code embeddings. These embeddings are further used to train SVM
based classiﬁcation model and several experiments are performed as discussed
in the next section.

5 Experiments and Results

5.1 Feature Engineering

Deep Learning(DL) algorithms tend to improve their performance with the
amount of data available unlike classical machine learning algorithms. With

Learning based Methods for Code Runtime Complexity Prediction

9

lesser amount of data and correctly hand engineered features, Machine Learn-
ing(ML) methods outperform many DL models. Moreover, the former are com-
putationally less expensive as compared to the latter. Therefore, we choose tra-
ditional ML classiﬁcation algorithms to verify the impact of various features
present in programming codes on their runtime complexities. We also perform a
similar analysis on a simple Multi level Perceptron(MLP) classiﬁer and compare
against others. Table 3 depicts the accuracy score, weighted precision and re-
call values for this classiﬁcation task using 8 diﬀerent algorithms, with the best
accuracy score achieved using the ensemble approach of random forests.

Algorithm

Accuracy Precision Recall

53.95
73.19
58.06
58.57
73.19

54.34
70.85
60.35
59.35
72.89

K-means
54.76
Random forest 74.26
57.75
Naive Bayes
59.89
k-Nearest
73.19
Logistic Regres-
sion
Decision Tree
73.79
71.12
MLP Classiﬁer 63.10
58.28
SVM
72.96
70.58
Table 3: Accuracy Score, Precision and
Recall values for diﬀerent classiﬁcation
algorithms

71.86
59.13
69.43

Feature

Mean Accuracy

44.35
No. of ifs
44.38
No. of switches
51.33
No. of loops
No. of breaks
43.85
Priority Queue present 45.45
52.40
No. of sorts
44.38
Hash Set present
43.85
Hash Map present
42.38
Recursion present
Nested loop depth
66.31
42.78
No. of Variables
42.19
No. of methods
43.65
No. of jumps
44.18
No. of statements

Table 4: Per feature accuracy score, av-
eraged over diﬀerent classiﬁcation algo-
rithms.

Further, as per Table 4 showing per-feature-analysis, we distinctly make out
that for the collected dataset, the most prominent feature which solely gives max-
imum accuracy is nested loop depth, followed by number of sorts and loops. For
all algorithms, accuracy peaks uptil the tenth feature, and decreases thereafter
[3]. This comprehensibly depicts that for the ﬁve complexity classes considered
here, the number of variables, number of methods, number of jumps and number
of statements do not play a notable role as compared to the other features.

Another interesting feature that comes up with the qualitative study of these
accuracy scores is that with these features, most of the true classiﬁcation predic-
tions come up with code samples of category O(1), O(n) and O(n2). This holds
a subtle resonance with the inherent obvious instincts of a human classiﬁer who
is more likely to correctly classify a program for these complexities instantly as
compared to O(logn) or O(nlogn). Tables 5 and 6 demarcate the diﬀerence be-
tween accuracy scores considering data samples from classes O(1), O(n), O(n2)
as compared to classes O(1), O(logn), O(nlogn). A clear increment in accuracy

10

Authors Suppressed Due to Excessive Length

scores is noticed amongst all the algorithms considered for the classiﬁcation task
for the former set of 3 classes considered as compared to the latter.

Algorithm

Accuracy Precision Recall

64.38
82.61

63.42
65.56
81.58

K-means
Random for-
est
Naive Bayes
k-Nearest
Logistic
Regression
Decision Tree 80.12
70.06
MLP Classi-
ﬁer
SVM

76.43

63.76
81.32

61.44
67.54
81.70

81.19
65.53

63.04
79.37

61.09
66.23
80.98

78.24
70.31

72.14

74.35

Algorithm

Accuracy Precision Recall

52.31
64.05

61.05
63.56
75.46

K-means
Random for-
est
Naive Bayes
k-Nearest
Logistic
Regression
Decision Tree 74.74
67.34
MLP Classi-
ﬁer
SVM

69.64

53.23
71.21

68.21
64.45
71.24

72.12
68.45

50.04
63.24

60.24
64.07
70.98

77.05
67.43

70.76

67.24

Table 5: Accuracy, Precision and Re-
call values for diﬀerent classiﬁcation al-
gorithms considering samples from com-
plexity classes O(1), O(n) and O(n2)

Table 6: Accuracy, Precision and Re-
call values for diﬀerent classiﬁcation
from
algorithms considering samples
complexity classes O(1), O(logn) and
O(nlogn)

5.2 Code Embeddings

We extracted ASTs from source codes, computed 1024-dimensional code em-
beddings from ASTs using graph2vec and trained an SVM classiﬁer on these
embeddings. Results are tabulated in Tables 7. We note that the maximum ac-
curacy obtained for SVM on code embeddings is comparable to that of SVM
on statistical features. Also, code embeddings baseline has better precision and
recall scores for both representations of AST.

5.3 Data Ablation Experiments

To get further insight into the learning framework, we perform following data
ablation tests:

Label Shuﬄing. Training models with shuﬄed class labels can indicate whether
the model is learning useful features pertaining to the task at hand. If the per-
formance does not signiﬁcantly decrease upon shuﬄing, it can imply that the
model is hanging on to statistical cues that do not contain meaningful informa-
tion w.r.t. the problem.

Method/Variable Name Alteration. Graph2vec uses node labels along with
edge information to generate graph embeddings. Out of randomly selected 50
codes having correct prediction, if the predicted class labels before and after

Learning based Methods for Code Runtime Complexity Prediction

11

data ablation are diﬀerent for a signiﬁcant number of test samples, it would
imply it would imply that the model relies on method/variable name tokens
whereas it should only rely on the relationships between variables/methods.

Replacing Input Variables with Constant Literals. Program complexity is a
function of input variables. Thus, to test the robustness of models, we replace
the input variables with constant values making resultant complexity O(1) for
50 randomly chosen codes, which earlier had non-constant complexity. A good
model should have a higher percentage of codes with predicted complexity as
O(1).

Removing Graph Substructures. We randomly remove program elements such
as for, if blocks with a probability of 0.1. The expectation is that the correctly
predicted class labels should not change heavily as the complexity most likely
does not change and hence should have a higher percentage of codes with same
correct label before and after removing graph substructures. This would imply
that the model is robust to changes in code that do not change the resultant
complexity.

AST Representation
Node Labels with concatenation
Node Labels without concatenation

Accuracy
73.86
70.45

Precision Recall
74
71

73
70

Table 7: Accuracy, Precision, Recall values for classiﬁcation of graph2vec embed-
dings, with and without node type & node value concatenation in node label.

Following are our observations regarding data ablation results in Table 8:
Label Shuﬄing. The drop in test performance is higher in graph2vec than that
in the basic model indicating that graph2vec learns better features compared to
simple statistical models.

Method/Variable Name Alteration. Table 8 shows that SVM correctly clas-
siﬁes most of the test samples’ embeddings upon altering method and variable
names, implying that the embeddings generated do not rely heavily on the actual
method/variable name tokens.

Replacing Input Variables with Constant Literals. We see a signiﬁcant and

unexpected dip in accuracy, highlighting one of the limitations of our model.

Removing Graph Substructures. Higher accuracy for code embeddings as com-
pared to feature engineering implies that the model must be learning the types
of nodes and their eﬀect on complexity to at least some extent, as removing
substructures does not change the predicted complexity class of a program sig-
niﬁcantly.

6 Limitations

The most pertinent limitation of the dataset is its size which is fairly small com-
pared to what is considered standard today. Another limitation of our work is low

12

Authors Suppressed Due to Excessive Length

Ablation Technique

Label Shuﬄing
Method/Variable Name Alter-
ation
Replacing Input Variables with
Constant Literals
Removing Graph Substructures

Accuracy

Feature En-
gineering
48.29
NA

Graph2vec: With
Concatenation
36.78
84.21

Graph2vec: With-
out Concatenation
31.03
89.18

NA

66.92

16.66

87.56

13.33

88.96

Table 8: Data Ablation Tests Accuracy of feature engineering and code embed-
dings (for two diﬀerent AST representations) baselines

accuracy of the models. An important point to note is that although we estab-
lished that using code embeddings is a better approach, still their accuracy does
not beat feature engineering signiﬁcantly. One possible solution is to increase
dataset size so that generated code embeddings can better model the charac-
teristics of programs that diﬀerentiate them into multiple complexity classes,
when trained on larger number of codes. However, generating a larger dataset is
a challenging task since annotation process is tedious and needs people with a
sound knowledge of algorithms. Lastly, we observe that replacing variables with
constant literals does not change the prediction to O(1) which highlights the
inability of graph2vec to identify the variable on which complexity depends.

7 Usefulness of the Dataset

Computational complexity is a quantiﬁcation of computational eﬃciency. Com-
putationally eﬃcient programs better utilize resources and improve software per-
formance. With rapid advancements, there is a growing demand for resources;
at the same time, there is greater need for optimizing existing solutions. Thus,
writing computationally eﬃcient programs is an asset for both students and
professionals. With this dataset, we aim to analyze attributes and capture rela-
tionships that best deﬁne the computational complexity of codes. We do so, not
just by heuristically picking up evident features, but by investigating their role
in the quality, structure and dynamics of the problem using ML paradigm. We
also capture relationships between various programming constructs by generat-
ing code embeddings from Abstract Syntax Trees. This dataset can not only help
automate the process of predicting complexities, but also, to help learners de-
cide apt features for well-structured and eﬃcient codes. It is essential for better
programming ethics apart from a good algorithm, since a good algorithm might
get restricted performance due to ineﬃcient coding, which can add an additional
ease and value to educational purposes. It can also be used to train models that
can be further integrated with IDEs and assist professional developers in writing
computationally eﬃcient programs for fast performance software development.

Learning based Methods for Code Runtime Complexity Prediction

13

8 Conclusion

The dataset presented and the baseline models established should serve as guide-
lines for the future work in this area. The dataset presented is balanced and well-
curated. Though both the baselines; Code Embeddings and Handcrafted features
have comparable accuracy, we have established through data ablation tests that
code embeddings learned from Abstract Syntax Tree of the code better capture
relationships between diﬀerent code constructs that are essential for predicting
runtime complexity. Work can be done in future to increase the size of the dataset
to verify our hypothesis that code embeddings will perform signiﬁcantly better
than hand crafted features. Moreover, we hope that the approaches discussed in
this work, their usage becomes explicit for programmers and learners to bring
into practice eﬃcient and optimized codes.

References

1. Are

runtime

decidable?
https://cstheory.stackexchange.com/questions/5004/
are-runtime-bounds-in-p-decidable-answer-no

bounds

in

p

(answer:

no)

2. Graph2vec implementation https://github.com/MLDroid/graph2vec_tf
at
3. More

additional

results

https://github.com/midas-research/

corcod-dataset

4. Allamanis, M., Peng, H., Sutton, C.: A convolutional attention network for extreme
summarization of source code. In: Balcan, M.F., Weinberger, K.Q. (eds.) Proceed-
ings of The 33rd International Conference on Machine Learning. Proceedings of
Machine Learning Research, vol. 48, pp. 2091–2100. PMLR, New York, New York,
USA (20–22 Jun 2016), http://proceedings.mlr.press/v48/allamanis16.html
5. Alon, U., Zilberstein, M., Levy, O., Yahav, E.: A general path-based representation
for predicting program properties. CoRR abs/1803.09544 (2018), http://arxiv.
org/abs/1803.09544

6. Alon, U., Zilberstein, M., Levy, O., Yahav, E.: Code2vec: Learning distributed
representations of code. Proc. ACM Program. Lang. 3(POPL), 40:1–40:29 (Jan
2019). https://doi.org/10.1145/3290353, http://doi.acm.org/10.1145/3290353
7. Asperti, A.: The intensional content of rice’s theorem. In: Proceedings of
the 35th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Pro-
gramming Languages. pp. 113–119. POPL ’08, ACM, New York, NY, USA
(2008). https://doi.org/10.1145/1328438.1328455, http://doi.acm.org/10.1145/
1328438.1328455

8. Bentley, J.L., Haken, D., Saxe, J.B.: A general method for

solving
recurrences. SIGACT News 12(3), 36–44 (Sep 1980).
http://doi.acm.org/10.1145/

divide-and-conquer
https://doi.org/10.1145/1008861.1008865,
1008861.1008865

9. Chen, Z., Monperrus, M.: A literature study of embeddings on source code. CoRR

abs/1904.03061 (2019), http://arxiv.org/abs/1904.03061

10. Le, Q.V., Mikolov, T.: Distributed representations of sentences and documents

(2014)

11. Li, J., He, P., Zhu, J., Lyu, M.R.: Software defect prediction via convolutional neu-
ral network. 2017 IEEE International Conference on Software Quality, Reliability
and Security (QRS) pp. 318–328 (2017)

14

Authors Suppressed Due to Excessive Length

12. Markovtsev, V., Long, W.: Public git archive: a big code dataset for all. CoRR

abs/1803.10144 (2018), http://arxiv.org/abs/1803.10144

13. McCabe, T.J.: A complexity measure. In: Proceedings of the 2Nd International
Conference on Software Engineering. pp. 407–. ICSE ’76, IEEE Computer Soci-
ety Press, Los Alamitos, CA, USA (1976), http://dl.acm.org/citation.cfm?id=
800253.807712

14. Narayanan, A., Chandramohan, M., Venkatesan, R., Chen, L., Liu, Y.,
Jaiswal, S.: graph2vec: Learning distributed representations of graphs. CoRR
abs/1707.05005 (2017), http://arxiv.org/abs/1707.05005

15. Srikant, S., Aggarwal, V.: A system to grade computer programming skills us-
ing machine learning. In: Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. pp. 1887–1896. KDD
’14, ACM, New York, NY, USA (2014). https://doi.org/10.1145/2623330.2623377,
http://doi.acm.org/10.1145/2623330.2623377

16. Sun, Z., Zhu, Q., Mou, L., Xiong, Y., Li, G., Zhang, L.: A grammar-
the
code generation. Proceedings of
for
Intelligence 33(01), 7055–7062 (Jul 2019).
https://www.aaai.org/ojs/

cnn decoder
based structural
AAAI Conference on Artiﬁcial
https://doi.org/10.1609/aaai.v33i01.33017055,
index.php/AAAI/article/view/4686

17. Yao, Z., Weld, D.S., Chen, W., Sun, H.: Staqc: A systematically mined question-
code dataset from stack overﬂow. CoRR abs/1803.09371 (2018), http://arxiv.
org/abs/1803.09371

18. Yin, P., Deng, B., Chen, E., Vasilescu, B., Neubig, G.: Learning to mine
aligned code and natural language pairs from stack overﬂow. In: International
Conference on Mining Software Repositories. pp. 476–486. MSR, ACM (2018).
https://doi.org/https://doi.org/10.1145/3196398.3196408

19. Yonai, H., Hayase, Y., Kitagawa, H.: Mercem: Method name recommendation
based on call graph embedding. CoRR abs/1907.05690 (2019), http://arxiv.
org/abs/1907.05690

