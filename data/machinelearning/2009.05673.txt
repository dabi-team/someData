2
2
0
2

y
a
M
7
1

]

G
L
.
s
c
[

5
v
3
7
6
5
0
.
9
0
0
2
:
v
i
X
r
a

Applications of Deep Neural Networks with Keras

Jeﬀ Heaton

Fall 2022.0

 
 
 
 
 
 
ii

Publisher: Heaton Research, Inc.
Applications of Deep Neural Networks
May, 2022
Author: [Jeﬀrey Heaton](https://orcid.org/0000-0003-1496-4049
ISBN: 9798416344269
Edition: 1

The text and illustrations of Applications of Deep Neural Networks by Jeﬀ Heaton are licensed under

CC BY-NC-SA 4.0. To view a copy of this license, visit CC BY-NC-SA 4.0.
All of the book’s source code is licensed under the GNU Lesser General Public License as published by the
Free Software Foundation; either version 2.1 of the license or (at your option) any later version. LGPL

Heaton Research, Encog, the Encog Logo, and the Heaton Research logo are all trademarks of Jeﬀ

Heaton in the United States and/or other countries.

TRADEMARKS: Heaton Research has attempted throughout this book to distinguish proprietary

trademarks from descriptive terms by following the capitalization style used by the manufacturer.

The author and publisher have done their best to prepare this book, so the content is based upon the
ﬁnal release of software whenever possible. Portions of the manuscript may be based upon pre-release
versions supplied by software manufacturer(s). The author and the publisher make no representation or
warranties of any kind about the completeness or accuracy of the contents herein and accept no liability
of any kind, including but not limited to performance, merchantability, ﬁtness for any particular purpose,
or any losses or damages of any kind caused or alleged to be caused directly or indirectly from this book.

DISCLAIMER
The author, Jeﬀrey Heaton, makes no warranty or representation, either expressed or implied, concern-
ing the Software or its contents, quality, performance, merchantability, or ﬁtness for a particular purpose.
In no event will Jeﬀrey Heaton, his distributors, or dealers be liable to you or any other party for direct,
indirect, special, incidental, consequential, or other damages arising out of the use of or inability to use the
Software or its contents even if advised of the possibility of such damage. In the event that the Software
includes an online update feature, Heaton Research, Inc. further disclaims any obligation to provide this
feature for any speciﬁc duration other than the initial posting.

The exclusion of implied warranties is not permitted by some states. Therefore, the above exclusion
may not apply to you. This warranty provides you with speciﬁc legal rights; there may be other rights
that you may have that vary from state to state. The pricing of the book with the Software by Heaton
Research, Inc. reﬂects the allocation of risk and limitations on liability contained in this agreement of
Terms and Conditions.

Contents

Introduction

1 Python Preliminaries

Sets

1.1 Part 1.1: Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.1 Origins of Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.2 What is Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.3 Regression, Classiﬁcation and Beyond . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.4 Why Deep Learning? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.5 Python for Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.6 Check your Python Installation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.1.7 Module 1 Assignment
1.2 Part 1.2: Introduction to Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3 Part 1.3: Python Lists, Dictionaries, Sets, and JSON . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.1 Lists and Tuples
1.3.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.3 Maps/Dictionaries/Hash Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.4 More Advanced Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3.5 An Introduction to JSON . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.1 Read a CSV File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.2 Read (stream) a Large CSV File . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.3 Read a Text File . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.4.4 Read an Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Part 1.5: Functions, Lambdas, and Map/Reduce . . . . . . . . . . . . . . . . . . . . . . . .
1.5.1 Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.2 Filter
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.3 Lambda . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5.4 Reduce

1.4 Part 1.4: File Handling

2 Python for Machine Learning

2.1 Part 2.1: Introduction to Pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Missing Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

iii

xiii

1
1
1
2
4
4
4
5
6
6
12
13
16
17
19
22
25
26
26
27
28
29
30
31
31
32

33
33
36

iv

CONTENTS

2.2 Part 2.2: Categorical and Continuous Values

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.2 Dealing with Outliers
2.1.3 Dropping Fields
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.4 Concatenating Rows and Columns . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.5 Training and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.6 Converting a Dataframe to a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . .
Saving a Dataframe to CSV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.7
Saving a Dataframe to Pickle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.8
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.1.9 Module 2 Assignment
. . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 Encoding Continuous Values
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.2 Encoding Categorical Values as Dummies . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 Removing the First Level
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.4 Target Encoding for Categoricals . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.5 Encoding Categorical Values as Ordinal
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.6 High Cardinality Categorical
2.3 Part 2.3: Grouping, Sorting, and Shuﬄing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Shuﬄing a Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1
2.3.2
Sorting a Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.3 Grouping a Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 Part 2.4: Apply and Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.1 Using Map with Dataframes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.2 Using Apply with Dataframes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4.3 Feature Engineering with Apply and Map . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Part 2.5: Feature Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.1 Calculated Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2 Google API Keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
2.5.3 Other Examples: Dealing with Addresses

3 Introduction to TensorFlow

Input and Output Neurons

3.1 Part 3.1: Deep Learning and Neural Network Introduction . . . . . . . . . . . . . . . . . . .
3.1.1 Classiﬁcation or Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.2 Neurons and Layers
3.1.3 Types of Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.5 Hidden Neurons
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.6 Bias Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.7 Other Neuron Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.8 Why are Bias Neurons Needed? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.9 Modern Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.10 Linear Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.11 Rectiﬁed Linear Units (ReLU)
3.1.12 Softmax Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.13 Step Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.14 Sigmoid Activation Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37
38
38
40
41
42
43
44
44
45
46
49
51
54
55
55
55
56
57
59
59
61
62
67
67
68
68

73
73
74
74
78
79
79
80
80
80
82
83
84
84
86
87

CONTENTS

v

87
3.1.15 Hyperbolic Tangent Activation Function . . . . . . . . . . . . . . . . . . . . . . . . .
88
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.16 Why ReLU?
89
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.17 Module 3 Assignment
89
3.2 Part 3.2: Introduction to Tensorﬂow and Keras . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.2.1 Why TensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2.2 Deep Learning Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
3.2.3 Using TensorFlow Directly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
. . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.4 TensorFlow Linear Algebra Examples
93
3.2.5 TensorFlow Mandelbrot Set Example
. . . . . . . . . . . . . . . . . . . . . . . . . .
96
Introduction to Keras
3.2.6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
Simple TensorFlow Regression: MPG . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.7
97
3.2.8
. . . . . . . . . . . . . . . . . . .
Introduction to Neural Network Hyperparameters
97
3.2.9 Controlling the Amount of Output . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.2.10 Regression Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.11 Simple TensorFlow Classiﬁcation: Iris . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.3 Part 3.3: Saving and Loading a Keras Neural Network . . . . . . . . . . . . . . . . . . . . . 105
3.4 Part 3.4: Early Stopping in Keras to Prevent Overﬁtting . . . . . . . . . . . . . . . . . . . . 107
3.4.1 Early Stopping with Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.4.2 Early Stopping with Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
3.5 Part 3.5: Extracting Weights and Manual Network Calculation . . . . . . . . . . . . . . . . 111
3.5.1 Weight Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
3.5.2 Manual Neural Network Calculation . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

4 Training for Tabular Data

119
4.1 Part 4.1: Encoding a Feature Vector for Keras Deep Learning . . . . . . . . . . . . . . . . . 119
4.1.1 Generate X and Y for a Classiﬁcation Neural Network . . . . . . . . . . . . . . . . . 124
4.1.2 Generate X and Y for a Regression Neural Network . . . . . . . . . . . . . . . . . . 125
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
4.1.3 Module 4 Assignment
4.2 Part 4.2: Multiclass Classiﬁcation with ROC and AUC . . . . . . . . . . . . . . . . . . . . . 125
4.2.1 Binary Classiﬁcation and ROC Charts . . . . . . . . . . . . . . . . . . . . . . . . . . 126
4.2.2 ROC Chart Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
4.2.3 Multiclass Classiﬁcation Error Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . 131
4.2.4 Calculate Classiﬁcation Accuracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
4.2.5 Calculate Classiﬁcation Log Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
4.2.6 Confusion Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
4.3 Part 4.3: Keras Regression for Deep Neural Networks with RMSE . . . . . . . . . . . . . . 138
4.3.1 Mean Square Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.3.2 Root Mean Square Error
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.3.3 Lift Chart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
4.4 Part 4.4: Training Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
4.4.1 Momentum Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.4.2 Batch and Online Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
4.4.3
Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
4.4.4 Other Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146

vi

CONTENTS

4.4.5 ADAM Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
4.4.6 Methods Compared . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
Specifying the Update Rule in Keras . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
4.4.7
4.5 Part 4.5: Error Calculation from Scratch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
4.5.1 Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

5 Regularization and Dropout

155
5.1 Part 5.1: Introduction to Regularization: Ridge and Lasso . . . . . . . . . . . . . . . . . . . 155
5.1.1 L1 and L2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
5.1.2 Linear Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.1.3 L1 (Lasso) Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158
5.1.4 L2 (Ridge) Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
5.1.5 ElasticNet Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.2 Part 5.2: Using K-Fold Cross-validation with Keras . . . . . . . . . . . . . . . . . . . . . . . 162
5.2.1 Regression vs Classiﬁcation K-Fold Cross-Validation . . . . . . . . . . . . . . . . . . 163
5.2.2 Out-of-Sample Regression Predictions with K-Fold Cross-Validation . . . . . . . . . 163
5.2.3 Classiﬁcation with Stratiﬁed K-Fold Cross-Validation . . . . . . . . . . . . . . . . . 166
. . . . . . . . . . . . . . . 169
5.2.4 Training with both a Cross-Validation and a Holdout Set
5.3 Part 5.3: L1 and L2 Regularization to Decrease Overﬁtting . . . . . . . . . . . . . . . . . . 172
5.4 Part 5.4: Drop Out for Keras to Decrease Overﬁtting . . . . . . . . . . . . . . . . . . . . . . 176
5.5 Part 5.5: Benchmarking Regularization Techniques . . . . . . . . . . . . . . . . . . . . . . . 180
5.5.1 Bootstrapping for Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
5.5.2 Bootstrapping for Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
5.5.3 Benchmarking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188

6 Convolutional Neural Networks (CNN) for Computer Vision

195
6.1 Part 6.1: Image Processing in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
6.1.1 Creating Images from Pixels in Python . . . . . . . . . . . . . . . . . . . . . . . . . 197
6.1.2 Transform Images in Python (at the pixel level)
. . . . . . . . . . . . . . . . . . . . 198
6.1.3
Standardize Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
6.1.4 Adding Noise to an Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
6.1.5 Preprocessing Many Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
6.1.6 Module 6 Assignment
6.2 Part 6.2: Keras Neural Networks for Digits and Fashion MNIST . . . . . . . . . . . . . . . 206
6.2.1 Common Computer Vision Data Sets
. . . . . . . . . . . . . . . . . . . . . . . . . . 206
6.2.2 Convolutional Neural Networks (CNNs) . . . . . . . . . . . . . . . . . . . . . . . . . 207
6.2.3 Convolution Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
6.2.4 Max Pooling Layers
6.2.5 Regression Convolutional Neural Networks
. . . . . . . . . . . . . . . . . . . . . . . 211
Score Regression Image Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
6.2.6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
6.2.7 Classiﬁcation Neural Networks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
6.2.8 Other Resources
6.3 Part 6.3: Transfer Learning for Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . 222
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227

6.3.1 Using the Structure of ResNet

CONTENTS

vii

6.4 Part 6.4: Inside Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
6.5 Part 6.5: Recognizing Multiple Images with YOLO5 . . . . . . . . . . . . . . . . . . . . . . 238
6.5.1 Using YOLO in Python . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
6.5.2
Installing YOLOv5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 240
6.5.3 Running YOLOv5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
6.5.4 Module 6 Assignment

7 Generative Adversarial Networks

251
7.1 Part 7.1: Introduction to GANS for Image and Data Generation . . . . . . . . . . . . . . . 251
7.1.1 Face Generation with StyleGAN and Python . . . . . . . . . . . . . . . . . . . . . . 251
7.1.2 Generating High Rez GAN Faces with Google CoLab . . . . . . . . . . . . . . . . . 253
7.1.3 Run StyleGan From Command Line . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
7.1.4 Run StyleGAN From Python Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
7.1.5 Examining the Latent Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
7.1.6 Module 7 Assignment
7.2 Part 7.2: Train StyleGAN3 with your Images . . . . . . . . . . . . . . . . . . . . . . . . . . 263
7.2.1 What Sort of GPU do you Have? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
7.2.2
Set Up New Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.2.3 Find Your Files . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.2.4 Convert Your Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
7.2.5 Clean Up your Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.2.6 Perform Initial Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.2.7 Resume Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
7.3 Part 7.3: Exploring the StyleGAN Latent Vector . . . . . . . . . . . . . . . . . . . . . . . . 266
7.3.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
7.3.2 Generate and View GANS from Seeds . . . . . . . . . . . . . . . . . . . . . . . . . . 269
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
7.3.3 Fine-tune an Image
7.4 Part 7.4: GANS to Enhance Old Photographs Deoldify . . . . . . . . . . . . . . . . . . . . . 273
Install Needed Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
Initialize Torch Device . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
7.5 Part 7.5: GANs for Tabular Synthetic Data Generation . . . . . . . . . . . . . . . . . . . . 277
Installing Tabgan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
7.5.1
7.5.2 Loading the Auto MPG Data and Training a Neural Network . . . . . . . . . . . . . 277
7.5.3 Training a GAN for Auto MPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
7.5.4 Evaluating the GAN Results

Installing Needed Software

7.4.1
7.4.2

8 Kaggle Data Sets

283
8.1 Part 8.1: Introduction to Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
8.1.1 Kaggle Ranks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
8.1.2 Typical Kaggle Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
8.1.3 How Kaggle Competition Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
8.1.4 Preparing a Kaggle Submission . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
Select Kaggle Competitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.1.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.1.6 Module 8 Assignment

viii

CONTENTS

8.3 Part 8.3: Architecting Network: Hyperparameters

8.2 Part 8.2: Building Ensembles with Scikit-Learn and Keras . . . . . . . . . . . . . . . . . . . 285
8.2.1 Evaluating Feature Importance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
8.2.2 Classiﬁcation and Input Perturbation Ranking . . . . . . . . . . . . . . . . . . . . . 287
8.2.3 Regression and Input Perturbation Ranking . . . . . . . . . . . . . . . . . . . . . . . 289
8.2.4 Biological Response with Neural Network . . . . . . . . . . . . . . . . . . . . . . . . 291
8.2.5 What Features/Columns are Important
. . . . . . . . . . . . . . . . . . . . . . . . . 293
8.2.6 Neural Network Ensemble . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
. . . . . . . . . . . . . . . . . . . . . . . 297
8.3.1 Number of Hidden Layers and Neuron Counts . . . . . . . . . . . . . . . . . . . . . . 298
8.3.2 Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.3.3 Advanced Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.3.4 Regularization: L1, L2, Dropout
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
8.3.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.3.6 Training Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
8.4 Part 8.4: Bayesian Hyperparameter Optimization for Keras . . . . . . . . . . . . . . . . . . 300
8.5 Part 8.5: Current Semester’s Kaggle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.5.1
Iris as a Kaggle Competition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
8.5.2 MPG as a Kaggle Competition (Regression) . . . . . . . . . . . . . . . . . . . . . . . 310

9 Transfer Learning

315
9.1 Part 9.1: Introduction to Keras Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . 315
9.1.1 Transfer Learning Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
9.1.2 Create a New Iris Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
9.1.3 Transfering to a Regression Network . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
9.1.4 Module 9 Assignment
9.2 Part 9.2: Keras Transfer Learning for Computer Vision . . . . . . . . . . . . . . . . . . . . 322
9.2.1 Transfering Computer Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
9.2.2 The Kaggle Cats vs. Dogs Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
9.2.3 Looking at the Data and Augmentations . . . . . . . . . . . . . . . . . . . . . . . . . 323
9.2.4 Create a Network and Transfer Weights . . . . . . . . . . . . . . . . . . . . . . . . . 326
9.2.5 Fine-Tune the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
. . . . . . . . . . . . . . . . . . . . . . . . 330
9.3.1 Beneﬁts of Early Stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
9.4 Part 9.4: Transfer Learning for Facial Points and GANs . . . . . . . . . . . . . . . . . . . . 338
9.4.1 Upload Starting and Ending Images . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
9.4.2
Install Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
9.4.3 Detecting Facial Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
9.4.4 Preprocess Images for Best StyleGAN Results . . . . . . . . . . . . . . . . . . . . . . 343
9.4.5 Convert Source to a GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
9.4.6 Convert Target to a GAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 346
9.4.7 Build the Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
9.4.8 Download your Video . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
9.5 Part 9.5: Transfer Learning for Keras Style Transfer . . . . . . . . . . . . . . . . . . . . . . 349
Image Preprocessing and Postprocessing . . . . . . . . . . . . . . . . . . . . . . . . . 353

9.3 Part 9.3: Transfer Learning for NLP with Keras

9.5.1

CONTENTS

ix

9.5.2 Calculating the Style, Content, and Variation Loss . . . . . . . . . . . . . . . . . . . 354
9.5.3 The VGG Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
9.5.4 Generating the Style Transferred Image . . . . . . . . . . . . . . . . . . . . . . . . . 358

10 Time Series in Keras

361
10.1 Part 10.1: Time Series Data Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
10.1.1 Module 10 Assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
10.2 Part 10.2: Programming LSTM with Keras and TensorFlow . . . . . . . . . . . . . . . . . . 366
10.2.1 Understanding LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
10.2.2 Simple Keras LSTM Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
10.2.3 Sun Spots Example
10.3 Part 10.3: Text Generation with LSTM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
10.3.1 Additional Information . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
10.3.2 Character-Level Text Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
10.4 Part 10.4: Introduction to Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
. . . . . . . . . . . . . . . . . . . . . . . . . . 384
10.4.1 High-Level Overview of Transformers
10.4.2 Transformer Hyperparameters
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
10.4.3 Inside a Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
. . . . . . . . . . . . . . . . . . . . . . . 386

10.5 Part 10.5: Programming Transformers with Keras

11 Natural Language Processing with Hugging Face

395
11.1 Part 11.1: Introduction to Hugging Face . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
11.1.1 Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
11.1.2 Entity Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
11.1.3 Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
11.1.4 Language Translation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398
11.1.5 Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
11.1.6 Text Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
11.2 Part 11.2: Hugging Face Tokenizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
11.3 Part 11.3: Hugging Face Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
11.4 Part 11.4: Training Hugging Face Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
11.5 Part 11.5: What are Embedding Layers in Keras . . . . . . . . . . . . . . . . . . . . . . . . 412
11.5.1 Simple Embedding Layer Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
11.5.2 Transferring An Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
11.5.3 Training an Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416

12 Reinforcement Learning

421
12.1 Part 12.1: Introduction to the OpenAI Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
12.1.1 OpenAI Gym Leaderboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
12.1.2 Looking at Gym Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
12.1.3 Render OpenAI Gym Environments from CoLab . . . . . . . . . . . . . . . . . . . . 425
12.2 Part 12.2: Introduction to Q-Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
12.2.1 Introducing the Mountain Car
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
12.2.2 Programmed Car . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431

x

CONTENTS

12.2.3 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433
12.2.4 Running and Observing the Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
12.2.5 Inspecting the Q-Table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
12.3 Part 12.3: Keras Q-Learning in the OpenAI Gym . . . . . . . . . . . . . . . . . . . . . . . . 440
12.3.1 DQN and the Cart-Pole Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441
12.3.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
12.3.3 Environment
12.3.4 Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
12.3.5 Policies
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
12.3.6 Metrics and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
12.3.7 Replay Buﬀer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
12.3.8 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
12.3.9 Training the agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
12.3.10 Visualization and Plots
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
12.3.11 Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 456
12.4 Part 12.4: Atari Games with Keras Neural Networks . . . . . . . . . . . . . . . . . . . . . . 457
12.4.1 Actual Atari 2600 Specs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
12.4.2 OpenAI Lab Atari Pong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
12.4.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
12.4.4 Atari Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 460
12.4.5 Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
12.4.6 Metrics and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
12.4.7 Replay Buﬀer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 465
12.4.8 Random Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
12.4.9 Training the Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
12.4.10 Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
12.5 Part 12.5: Application of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 469
12.5.1 Create an Environment of your Own . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
12.5.2 Testing the Environment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
12.5.3 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
12.5.4 Instantiate the Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482
12.5.5 Metrics and Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
12.5.6 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
12.5.7 Training the agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
12.5.8 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
12.5.9 Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489

13 Advanced/Other Topics

13.1 Part 13.1: Flask and Deep Learning Web Services

491
. . . . . . . . . . . . . . . . . . . . . . . 491
13.1.1 Flask Hello World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
13.1.2 MPG Flask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
13.1.3 Flask MPG Client . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
13.1.4 Images and Web Services
13.2 Part 13.2: Interrupting and Continuing Training . . . . . . . . . . . . . . . . . . . . . . . . 499

CONTENTS

xi

13.2.1 Continuing Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
13.3 Part 13.3: Using a Keras Deep Neural Network with a Web Application . . . . . . . . . . . 508
13.4 Part 13.4: When to Retrain Your Neural Network . . . . . . . . . . . . . . . . . . . . . . . 509
13.4.1 Preprocessing the Sberbank Russian Housing Market Data . . . . . . . . . . . . . . 512
13.4.2 KS-Statistic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
. . . . . . . . . 514
13.4.3 Detecting Drift between Training and Testing Datasets by Training
. . . . . . . . . . . . . . . . . . . . . . . . . . . 517
13.5.1 Preparing Data for TPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 518

13.5 Part 13.5: Tensor Processing Units (TPUs)

14 Other Neural Network Techniques

14.3 Part 14.3: Anomaly Detection in Keras

521
14.1 Part 14.1: What is AutoML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
. . . . . . . . . . . . . . . . . . . . . . . . . . . 521
14.1.1 AutoML from your Local Computer
14.1.2 AutoML from Google Cloud . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 521
14.1.3 Using AutoKeras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
14.2 Part 14.2: Using Denoising AutoEncoders in Keras . . . . . . . . . . . . . . . . . . . . . . . 526
14.2.1 Multi-Output Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
14.2.2 Simple Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
14.2.3 Autoencode (single image) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
14.2.4 Standardize Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 533
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
14.2.5 Image Autoencoder (multi-image)
14.2.6 Adding Noise to an Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
14.2.7 Denoising Autoencoder
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
14.3.1 Read in KDD99 Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544
14.3.2 Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
14.3.3 Training the Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
14.3.4 Detecting an Anomaly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549
14.4 Part 14.4: Training an Intrusion Detection System with KDD99 . . . . . . . . . . . . . . . . 550
14.4.1 Read in Raw KDD-99 Dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
14.4.2 Analyzing a Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 552
14.4.3 Encode the feature vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
14.4.4 Train the Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 555
14.5 Part 14.5: New Technologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
14.5.1 New Technology Radar
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
14.5.2 Programming Language Radar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
14.5.3 What About PyTorch? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558
14.5.4 Where to From Here? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 558

xii

CONTENTS

Introduction

Starting in the spring semester of 2016, I began teaching the T81-558 Applications of Deep Learning course
for Washington University in St. Louis. I never liked Microsoft Powerpoint for technical classes, so I placed
my course material, examples, and assignments on GitHub. This material started with code and grew to
include enough description that this information evolved into the book you see before you.

I license the book’s text under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-
NC-SA 4.0) license. Similarly, I oﬀer the book’s code under the LGPL license. Though I provide this book
both as a relatively inexpensive paperback and Amazon Kindle, you can obtain the book’s PDF here:

• https://arxiv.org/abs/2009.05673

The book’s code is available at the following GitHub repository:

• https://github.com/jeﬀheaton/t81_558_deep_learning

If you purchased this book from me, you have my sincere thanks for supporting my ongoing projects. I sell
the book as a relatively low-cost paperback and Kindle ebook for those who prefer that format or wish to
support my projects. I suggest that you look at the above GitHub site, as all of the code for this book is
presented there as Jupyter notebooks that are entirely Google CoLab compatible.

This book focuses on the application of deep neural networks. There is some theory; however, I do not
focus on recreating neural network fundamentals that tech companies already provide in popular frame-
works. The book begins with a quick review of the Python fundamentals needed to learn the subsequent
chapters. With Python preliminaries covered, we start with classiﬁcation and regression neural networks
in Keras.

In my opinion, PyTorch, Jax, and Keras are the top three deep learning frameworks. When I ﬁrst
created this course, neither PyTorch nor JAX existed.
I began the course based on TensorFlow and
migrated to Keras the following semester. I believe TensorFlow remains a good choice for a course focusing
on the application of deep learning. Some of the third-party libraries used for this course use PyTorch; as
a result, you will see a blend of both technologies. StyleGAN and TabGAN both make use of PyTorch.

The technologies that this course is based on change rapidly. I update the Kindle and paperback books
according to this schedule. Formal updates to this book typically occur just before each academic year’s
fall and spring semesters.

The source document for this book is Jupyter notebooks. I wrote a Python utility that transforms my
course Jupyter notebooks into this book. It is entirely custom, and I may release it as a project someday.
However, because this book is based on code and updated twice a year, you may ﬁnd the occasional typo. I

xiii

xiv

INTRODUCTION

try to minimize errors as much as possible, but please let me know if you see something. I use Grammarly
to ﬁnd textual issues, but due to the frequently updated nature of this book, I do not run it through a
formal editing cycle for each release. I also double-check the code with each release to ensure CoLab, Keras,
or another third-party library did not make a breaking change.

The book and course continue to be a work in progress. Many have contributed code, suggestions,
ﬁxes, and clariﬁcations to the GitHub repository. Please submit a GitHub issue or a push request with a
solution if you ﬁnd an error.

Chapter 1

Python Preliminaries

1.1 Part 1.1: Overview

Deep learning is a group of exciting new technologies for neural networks.[21]By using a combination of
advanced training techniques neural network architectural components, it is now possible to train neural
networks of much greater complexity. This book introduces the reader to deep neural networks, regu-
larization units (ReLU), convolution neural networks, and recurrent neural networks. High-performance
computing (HPC) aspects demonstrate how deep learning can be leveraged both on graphical processing
units (GPUs), as well as grids. Deep learning allows a model to learn hierarchies of information in a way
that is similar to the function of the human brain. The focus is primarily upon the application of deep
learning, with some introduction to the mathematical foundations of deep learning. Readers will make use
of the Python programming language to architect a deep learning model for several real-world data sets
and interpret the results of these networks.[9]

1.1.1 Origins of Deep Learning

Neural networks are one of the earliest examples of a machine learning model. Neural networks were initially
introduced in the 1940s and have risen and fallen several times in popularity. The current generation of
deep learning begain in 2006 with an improved training algorithm by Geoﬀrey Hinton.[12]This technique
ﬁnally allowed neural networks with many layers (deep neural networks) to be eﬃciently trained. Four
researchers have contributed signiﬁcantly to the development of neural networks. They have consistently
pushed neural network research, both through the ups and downs. These four luminaries are shown in
Figure 1.1.

The current luminaries of artiﬁcial neural network (ANN) research and ultimately deep learning, in

order as appearing in the ﬁgure:

• Yann LeCun, Facebook and New York University - Optical character recognition and computer vision

using convolutional neural networks (CNN). The founding father of convolutional nets.

• Geoﬀrey Hinton, Google and University of Toronto. Extensive work on neural networks. Creator of

deep learning and early adapter/creator of backpropagation for neural networks.

1

2

CHAPTER 1. PYTHON PRELIMINARIES

Figure 1.1: Neural Network Luminaries

• Yoshua Bengio, University of Montreal and Botler AI. Extensive research into deep learning, neural

networks, and machine learning.

• Andrew Ng, Badiu and Stanford University. Extensive research into deep learning, neural networks,

and application to robotics.

Geoﬀrey Hinton, Yann LeCun, and Yoshua Bengio won the Turing Award for their contributions to deep
learning.

1.1.2 What is Deep Learning

The focus of this book is deep learning, which is a prevalent type of machine learning that builds upon
the original neural networks popularized in the 1980s. There is very little diﬀerence between how a deep
neural network is calculated compared with the ﬁrst neural network. We’ve always been able to create and
calculate deep neural networks. A deep neural network is nothing more than a neural network with many
layers. While we’ve always been able to create/calculate deep neural networks, we’ve lacked an eﬀective
means of training them. Deep learning provides an eﬃcient means to train deep neural networks.

If deep learning is a type of machine learning, this begs the question, "What is machine learning?"

Figure 1.2 illustrates how machine learning diﬀers from traditional software development.

• Traditional Software Development - Programmers create programs that specify how to transform

input into the desired output.

• Machine Learning - Programmers create models that can learn to produce the desired output for

given input. This learning ﬁlls the traditional role of the computer program.

1.1. PART 1.1: OVERVIEW

3

Figure 1.2: ML vs Traditional Software Development

Researchers have applied machine learning to many diﬀerent areas. This class explores three speciﬁc
domains for the application of deep neural networks, as illustrated in Figure 1.3.

Figure 1.3: Application of Machine Learning

• Computer Vision - The use of machine learning to detect patterns in visual data. For example, is

an image a picture of a cat or a dog.

• Tabular Data - Several named input values allow the neural network to predict another named value
that becomes the output. For example, we are using four measurements of iris ﬂowers to predict the
species. This type of data is often called tabular data.

• Natural Language Processing (NLP) - Deep learning transformers have revolutionized NLP,

allowing text sequences to generate more text, images, or classiﬁcations.

• Reinforcement Learning - Reinforcement learning trains a neural network to choose ongoing

actions so that the algorithm rewards the neural network for optimally completing a task.

4

CHAPTER 1. PYTHON PRELIMINARIES

• Time Series - The use of machine learning to detect patterns in time. Typical time series applications

are ﬁnancial applications, speech recognition, and even natural language processing (NLP).

• Generative Models - Neural networks can learn to produce new original synthetic data from input.
We will examine StyleGAN, which learns to create new images similar to those it saw during training.

1.1.3 Regression, Classiﬁcation and Beyond

Machine learning research looks at problems in broad terms of supervised and unsupervised learning.
Supervised learning occurs when you know the correct outcome for each item in the training set. On the
other hand, unsupervised learning utilizes training sets where no correct outcome is known. Deep learning
supports both supervised and unsupervised learning; however, it also adds reinforcement and adversarial
learning. Reinforcement learning teaches the neural network to carry out actions based on an environment.
Adversarial learning pits two neural networks against each other to learn when the data provides no correct
outcomes. Researchers continue to add new deep learning training techniques.

Machine learning practitioners usually divide supervised learning into classiﬁcation and regression.
Classiﬁcation networks might accept ﬁnancial data and classify the investment risk as risk or safe. Similarly,
a regression neural network outputs a number and might take the same data and return a risk score.
Additionally, neural networks can output multiple regression and classiﬁcation scores simultaneously.

One of the most powerful aspects of neural networks is that the input and output of a neural network

can be of many diﬀerent types, such as:

• An image
• A series of numbers that could represent text, audio, or another time series
• A regression number
• A classiﬁcation class

1.1.4 Why Deep Learning?

For tabular data, neural networks often do not perform signiﬁcantly better that diﬀerent than other models,
such as:

• Support Vector Machines
• Random Forests
• Gradient Boosted Machines

Like these other models, neural networks can perform both classiﬁcation and regression. When applied
to relatively low-dimensional tabular data tasks, deep neural networks do not necessarily add signiﬁcant
accuracy over other model types. However, most state-of-the-art solutions depend on deep neural networks
for images, video, text, and audio data.

1.1.5 Python for Deep Learning

We will utilize the Python 3.x programming language for this book. Python has some of the widest
support for deep learning as a programming language. The two most popular frameworks for deep learning
in Python are:

1.1. PART 1.1: OVERVIEW

• TensorFlow/Keras (Google)
• PyTorch (Facebook)

5

Overall, this book focused on the application of deep neural networks. This book focuses primarily upon
Keras, with some applications in PyTorch. For many tasks, we will utilize Keras directly. We will utilize
third-party libraries for higher-level tasks, such as reinforcement learning, generative adversarial neural
networks, and others. These third-party libraries may internally make use of either PyTorch or Keras. I
chose these libraries based on popularity and application, not whether they used PyTorch or Keras.

To successfully use this book, you must be able to compile and execute Python code that makes use of

TensorFlow for deep learning. There are two options for you to accomplish this:

• Install Python, TensorFlow and some IDE (Jupyter, TensorFlow, and others).
• Use Google CoLab in the cloud, with free GPU access.

If you look at this notebook on Github, near the top of the document, there are links to videos that describe
how to use Google CoLab. There are also videos explaining how to install Python on your local computer.
The following sections take you through the process of installing Python on your local computer. This
process is essentially the same on Windows, Linux, or Mac. For speciﬁc OS instructions, refer to one of
the tutorial YouTube videos earlier in this document.

To install Python on your computer, complete the following instructions:

• Installing Python and TensorFlow - Windows/Linux
• Installing Python and TensorFlow - Mac Intel
• Installing Python and TensorFlow - Mac M1

1.1.6 Check your Python Installation

Once you’ve installed Python, you can utilize the following code to check your Python and library versions.
If you have a GPU, you can also check to see that Keras recognize it.

Code

# What v e r s i o n o f Python do you have ?
import s y s

import t e n s o r f l o w . k e r a s
import pandas a s pd
import s k l e a r n a s
sk
import t e n s o r f l o w a s

t f

check_gpu = len ( t f . c o n f i g . l i s t _ p h y s i c a l _ d e v i c e s ( 'GPU ' )) >0

print ( f " Tensor ␣Flow␣ V e r s i o n : ␣ { t f . __version__} " )
print ( f " Keras ␣ V e r s i o n : ␣{ t e n s o r f l o w . k e r a s . __version__} " )
print ( )
print ( f " Python ␣{ s y s . v e r s i o n } " )

6

CHAPTER 1. PYTHON PRELIMINARIES

print ( f " Pandas ␣{pd . __version__} " )
print ( f " S c i k i t −Learn ␣{ sk . __version__} " )
print ( "GPU␣ i s " ,

i f check_gpu \

" a v a i l a b l e "
e l s e "NOT␣AVAILABLE" )

Output

Tensor Flow V e r s i o n : 2 . 8 . 0
Keras V e r s i o n : 2 . 8 . 0
Python 3 . 7 . 1 3 ( d e f a u l t , Mar 16 2 0 2 2 , 1 7 : 3 7 : 1 7 )
[GCC 7 . 5 . 0 ]
Pandas 1 . 3 . 5
S c i k i t −Learn 1 . 0 . 2
GPU i s a v a i l a b l e

1.1.7 Module 1 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 1

1.2 Part 1.2: Introduction to Python

Python is an interpreted, high-level, general-purpose programming language. Created by Guido van
Rossum and ﬁrst released in 1991, Python’s design philosophy emphasizes code readability with its notable
use of signiﬁcant whitespace. Its language constructs and object-oriented approach aim to help program-
mers write clear, logical code for small and large-scale projects. Python has become a common language
for machine learning research and is the primary language for TensorFlow.

Python 3.0, released in 2008, was a signiﬁcant revision of the language that is not entirely backward-
compatible, and much Python 2 code does not run unmodiﬁed on Python 3. This course makes use of
Python 3. Furthermore, TensorFlow is not compatible with versions of Python earlier than 3. A non-
proﬁt organization, the Python Software Foundation (PSF), manages and directs resources for Python
development. On January 1, 2020, the PSF discontinued the Python 2 language and no longer provides
security patches and other improvements. Python interpreters are available for many operating systems.

The ﬁrst two modules of this course provide an introduction to some aspects of the Python programming
language. However, entire books focus on Python. Two modules will not cover every detail of this language.
The reader is encouraged to consult additional sources on the Python language.

Like most tutorials, we will begin by printing Hello World.

Code

print ( " H e l l o ␣World " )

1.2. PART 1.2: INTRODUCTION TO PYTHON

7

Output

H e l l o World

The above code passes a constant string, containing the text "hello world" to a function that is named

print.

You can also leave comments in your code to explain what you are doing. Comments can begin anywhere

in a line.
Code

# S i n g l e l i n e comment ( t h i s has no e f f e c t on your program )
print ( " H e l l o ␣World " ) # Say h e l l o

Output

H e l l o World

Strings are very versatile and allow your program to process textual information. Constant string,
enclosed in quotes, deﬁne literal string values inside your program. Sometimes you may wish to deﬁne a
larger amount of literal text inside of your program. This text might consist of multiple lines. The triple
quote allows for multiple lines of text.

Code

print ( " " " P r i n t
M u l t i p l e
L i n e s
" " " )

Output

P r i n t
M u l t i p l e
L i n e s

Like many languages Python uses single (’) and double (") quotes interchangeably to denote literal
string constants. The general convention is that double quotes should enclose actual text, such as words
or sentences. Single quotes should enclose symbolic text, such as error codes. An example of an error code
might be ’HTTP404’.

However, there is no diﬀerence between single and double quotes in Python, and you may use whichever

you like. The following code makes use of a single quote.

CHAPTER 1. PYTHON PRELIMINARIES

8

Code

print ( ' H e l l o ␣World ' )

Output

H e l l o World

In addition to strings, Python allows numbers as literal constants in programs. Python includes support
for ﬂoating-point, integer, complex, and other types of numbers. This course will not make use of complex
numbers. Unlike strings, quotes do not enclose numbers.

The presence of a decimal point diﬀerentiates ﬂoating-point and integer numbers. For example, the
value 42 is an integer. Similarly, 42.5 is a ﬂoating-point number.
If you wish to have a ﬂoating-point
number, without a fraction part, you should specify a zero fraction. The value 42.0 is a ﬂoating-point
number, although it has no fractional part. As an example, the following code prints two numbers.

Code

print ( 4 2 )
print ( 4 2 . 5 )

Output

42
4 2 . 5

So far, we have only seen how to deﬁne literal numeric and string values. These literal values are
constant and do not change as your program runs. Variables allow your program to hold values that can
change as the program runs. Variables have names that allow you to reference their values. The following
code assigns an integer value to a variable named "a" and a string value to a variable named "b."

Code

a = 10
b = " t e n "
print ( a )
print ( b )

Output

10
t e n

1.2. PART 1.2: INTRODUCTION TO PYTHON

9

The key feature of variables is that they can change. The following code demonstrates how to change

the values held by variables.

Code

a = 10
print ( a )
a = a + 1
print ( a )

Output

10
11

You can mix strings and variables for printing. This technique is called a formatted or interpolated
string. The variables must be inside of the curly braces. In Python, this type of string is generally called
an f-string. The f-string is denoted by placing an "f" just in front of the opening single or double quote
that begins the string. The following code demonstrates the use of an f-string to mix several variables with
a literal string.

Code

a = 10
print ( f ' The␣ v a l u e ␣ o f ␣a ␣ i s ␣{a} ' )

Output

The v a l u e o f a i s 10

You can also use f-strings with math (called an expression). Curly braces can enclose any valid Python
expression for printing. The following code demonstrates the use of an expression inside of the curly braces
of an f-string.

Code

a = 10
print ( f ' The␣ v a l u e ␣ o f ␣a ␣ p l u s ␣5␣ i s ␣ {a+5} ' )

Output

The v a l u e o f a p l u s 5 i s 15

10

CHAPTER 1. PYTHON PRELIMINARIES

Python has many ways to print numbers; these are all correct. However, for this course, we will use

f-strings. The following code demonstrates some of the varied methods of printing numbers in Python.

Code

a = 5

print ( f ' a␣ i s ␣{a} ' ) # P r e f e r r e d method f o r
print ( ' a␣ i s ␣ {} ' . format ( a ) )
print ( ' a␣ i s ␣ ' + s t r ( a ) )
print ( ' a␣ i s ␣%d ' % ( a ) )

t h i s c o u r s e .

Output

a i s 5
a i s 5
a i s 5
a i s 5

You can use if-statements to perform logic. Notice the indents? These if-statements are how Python
deﬁnes blocks of code to execute together. A block usually begins after a colon and includes any lines at
the same level of indent. Unlike many other programming languages, Python uses whitespace to deﬁne
blocks of code. The fact that whitespace is signiﬁcant to the meaning of program code is a frequent source
of annoyance for new programmers of Python. Tabs and spaces are both used to deﬁne the scope in a
Python program. Mixing both spaces and tabs in the same program is not recommended.

Code

a = 5
i f a >5:

print ( ' The␣ v a r i a b l e ␣a␣ i s ␣ g r e a t e r ␣ than ␣ 5 . ' )

e l s e :

print ( ' The␣ v a r i a b l e ␣a␣ i s ␣ not ␣ g r e a t e r ␣ than ␣5 ' )

Output

The v a r i a b l e a i s not g r e a t e r

than 5

The following if-statement has multiple levels. It can be easy to indent these levels improperly, so be
careful. This code contains a nested if-statement under the ﬁrst "a==5" if-statement. Only if a is equal to
5 will the nested "b==6" if-statement be executed. Also, note that the "elif" command means "else if."

1.2. PART 1.2: INTRODUCTION TO PYTHON

11

Code

a = 5
b = 6

i f a==5:

print ( ' The␣ v a r i a b l e ␣a␣ i s ␣5 ' )
i f b==6:

print ( ' The␣ v a r i a b l e ␣b␣ i s ␣ a l s o ␣6 ' )

e l i f a==6:

print ( ' The␣ v a r i a b l e ␣a␣ i s ␣ 6 ' )

Output

The v a r i a b l e a i s 5
The v a r i a b l e b i s a l s o 6

It is also important to note that the double equal ("==") operator is used to test the equality of two
expressions. The single equal ("=") operator is only used to assign values to variables in Python. The
greater than (">"), less than ("<"), greater than or equal (">="), less than or equal ("<=") all perform as
would generally be accepted. Testing for inequality is performed with the not equal ("!=") operator.

It is common in programming languages to loop over a range of numbers. Python accomplishes this
through the use of the range operation. Here you can see a for loop and a range operation that causes
the program to loop between 1 and 3.

Code

for x in range ( 1 , 3 ) : # I f you e v e r s e e xrange , you a r e i n Python 2

print ( x )

# I f you e v e r s e e p r i n t x ( no p a r e n t h e s i s ) , you a r e i n Python 2

Output

1
2

This code illustrates some incompatibilities between Python 2 and Python 3. Before Python 3, it was
acceptable to leave the parentheses oﬀ of a print function call. This method of invoking the print command
is no longer allowed in Python 3. Similarly, it used to be a performance improvement to use the xrange
command in place of range command at times. Python 3 incorporated all of the functionality of the xrange
Python 2 command into the normal range command. As a result, the programmer should not use the
xrange command in Python 3. If you see either of these constructs used in example code, then you are

12

CHAPTER 1. PYTHON PRELIMINARIES

looking at an older Python 2 era example.

The range command is used in conjunction with loops to pass over a speciﬁc range of numbers. Cases,
where you must loop over speciﬁc number ranges, are somewhat uncommon. Generally, programmers use
loops on collections of items, rather than hard-coding numeric values into your code. Collections, as well
as the operations that loops can perform on them, is covered later in this module.
The following is a further example of a looped printing of strings and numbers.

Code

a c c = 0
for x in range ( 1 , 3 ) :

a c c += x
print ( f " Adding ␣{x } , ␣sum␣ s o ␣ f a r ␣ i s ␣{ a c c } " )

print ( f " F i n a l ␣sum : ␣{ a c c } " )

Output

Adding 1 , sum s o f a r
Adding 2 , sum s o f a r
F i n a l sum : 3

i s 1
i s 3

1.3 Part 1.3: Python Lists, Dictionaries, Sets, and JSON

Like most modern programming languages, Python includes Lists, Sets, Dictionaries, and other data
structures as built-in types. The syntax appearance of both of these is similar to JSON. Python and
JSON compatibility is discussed later in this module. This course will focus primarily on Lists, Sets, and
Dictionaries. It is essential to understand the diﬀerences between these three fundamental collection types.

• Dictionary - A dictionary is a mutable unordered collection that Python indexes with name and

value pairs.

• List - A list is a mutable ordered collection that allows duplicate elements.
• Set - A set is a mutable unordered collection with no duplicate elements.
• Tuple - A tuple is an immutable ordered collection that allows duplicate elements.

Most Python collections are mutable, meaning the program can add and remove elements after deﬁnition.
An immutable collection cannot add or remove items after deﬁnition. It is also essential to understand
that an ordered collection means that items maintain their order as the program adds them to a collection.
This order might not be any speciﬁc ordering, such as alphabetic or numeric.

Lists and tuples are very similar in Python and are often confused. The signiﬁcant diﬀerence is that a
list is mutable, but a tuple isn’t. So, we include a list when we want to contain similar items and a tuple
when we know what information goes into it ahead of time.

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

13

Many programming languages contain a data collection called an array. The array type is noticeably
absent in Python. Generally, the programmer will use a list in place of an array in Python. Arrays in
most programming languages were ﬁxed-length, requiring the program to know the maximum number of
elements needed ahead of time. This restriction leads to the infamous array-overrun bugs and security
issues. The Python list is much more ﬂexible in that the program can dynamically change the size of a list.

The next sections will look at each collection type in more detail.

1.3.1 Lists and Tuples

For a Python program, lists and tuples are very similar. Both lists and tuples hold an ordered collection
of items. It is possible to get by as a programmer using only lists and ignoring tuples.

The primary diﬀerence that you will see syntactically is that a list is enclosed by square braces [], and

a tuple is enclosed by parenthesis (). The following code deﬁnes both list and tuple.

Code

l = [ ' a ' ,
t = ( ' a ' ,

' b ' ,
' b ' ,

' c ' ,
' c ' ,

' d ' ]
' d ' )

print ( l )
print ( t )

Output

[ ' a ' ,
( ' a ' ,

' b ' ,
' b ' ,

' c ' ,
' c ' ,

' d ' ]
' d ' )

The primary diﬀerence you will see programmatically is that a list is mutable, which means the program
can change it. A tuple is immutable, which means the program cannot change it. The following code
demonstrates that the program can change a list. This code also illustrates that Python indexes lists
starting at element 0. Accessing element one modiﬁes the second element in the collection. One advantage
of tuples over lists is that tuples are generally slightly faster to iterate over than lists.

Code

l [ 1 ] = ' changed '
#t [ 1 ] = ' changed ' # This would r e s u l t

i n an e r r o r

print ( l )

Output

[ ' a ' ,

' changed ' ,

' c ' ,

' d ' ]

14

CHAPTER 1. PYTHON PRELIMINARIES

Like many languages, Python has a for-each statement. This statement allows you to loop over every

element in a collection, such as a list or a tuple.

Code

# I t e r a t e o v e r a c o l l e c t i o n .
for s in l :

print ( s )

Output

a
changed
c
d

The enumerate function is useful for enumerating over a collection and having access to the index of

the element that we are currently on.

Code

# I t e r a t e o v e r a c o l l e c t i o n , and know where your i n d e x .
( Python i s
for i , l

in enumerate ( l ) :

zero −b a s e d ! )

print ( f " { i } : { l } " )

Output

0 : a
1 : changed
2 : c
3 : d

A list can have multiple objects added, such as strings. Duplicate values are allowed. Tuples do not

allow the program to add additional objects after deﬁnition.

Code

# Manually add items ,
c = [ ]
c . append ( ' a ' )
c . append ( ' b ' )
c . append ( ' c ' )

l i s t s a l l o w d u p l i c a t e s

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

15

c . append ( ' c ' )
print ( c )

Output

[ ' a ' ,

' b ' ,

' c ' ,

' c ' ]

Ordered collections, such as lists and tuples, allow you to access an element by its index number, as
done in the following code. Unordered collections, such as dictionaries and sets, do not allow the program
to access them in this way.

Code

print ( c [ 1 ] )

Output

b

A list can have multiple objects added, such as strings. Duplicate values are allowed. Tuples do not
allow the program to add additional objects after deﬁnition. The programmer must specify an index for
the insert function, an index. These operations are not allowed for tuples because they would result in a
change.

Code

' b ' ,

' c ' ]

' a0 ' )

# I n s e r t
c = [ ' a ' ,
c . i n s e r t ( 0 ,
print ( c )
# Remove
c . remove ( ' b ' )
print ( c )
# Remove a t
del c [ 0 ]
print ( c )

i n d e x

Output

[ ' a0 ' ,
[ ' a0 ' ,

' a ' ,
' a ' ,

' b ' ,
' c ' ]

' c ' ]

CHAPTER 1. PYTHON PRELIMINARIES

16

[ ' a ' ,

' c ' ]

1.3.2 Sets

A Python set holds an unordered collection of objects, but sets do not allow duplicates. If a program adds
a duplicate item to a set, only one copy of each item remains in the collection. Adding a duplicate item to
a set does not result in an error. Any of the following techniques will deﬁne a set.

Code

s = set ( )
s = { ' a ' ,
s = set ( [ ' a ' ,
print ( s )

' b ' ,

' c ' }

' b ' ,

' c ' ] )

Output

{ ' c ' ,

' a ' ,

' b ' }

A list is always enclosed in square braces [], a tuple in parenthesis (), and similarly a set is enclosed
in curly braces. Programs can add items to a set as they run. Programs can dynamically add items to a
set with the add function. It is important to note that the append function adds items to lists, whereas
the add function adds items to a set.

Code

s e t s do not a l l o w d u p l i c a t e s

l i s t s append .

I

f i n d t h i s annoying .

# Manually add items ,
# S e t s add ,
c = set ( )
c . add ( ' a ' )
c . add ( ' b ' )
c . add ( ' c ' )
c . add ( ' c ' )
print ( c )

Output

{ ' c ' ,

' a ' ,

' b ' }

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

17

1.3.3 Maps/Dictionaries/Hash Tables

Many programming languages include the concept of a map, dictionary, or hash table. These are all
very related concepts. Python provides a dictionary that is essentially a collection of name-value pairs.
Programs deﬁne dictionaries using curly braces, as seen here.

Code

d = { ' name ' :
print ( d )
print ( d [ ' name ' ] )

" J e f f " ,

' a d d r e s s ' : " 123 ␣Main " }

i f

i f

' name '
print ( "Name␣ i s ␣ d e f i n e d " )

in d :

in d :

' age '
print ( " age ␣ d e f i n e d " )

e l s e :

print ( " age ␣ u n d e f i n e d " )

Output

' J e f f ' ,

{ ' name ' :
J e f f
Name i s d e f i n e d
age u n d e f i n e d

' a d d r e s s ' :

' 1 2 3 Main ' }

Be careful that you do not attempt to access an undeﬁned key, as this will result in an error. You can
check to see if a key is deﬁned, as demonstrated above. You can also access the dictionary and provide a
default value, as the following code demonstrates.

Code

d . g e t ( ' unknown_key ' ,

' d e f a u l t ' )

Output

' d e f a u l t '

You can also access the individual keys and values of a dictionary.

18

Code

CHAPTER 1. PYTHON PRELIMINARIES

d = { ' name ' :
# A l l o f
t h e k e y s
print ( f " Key : ␣{d . k e y s ( ) } " )

" J e f f " ,

' a d d r e s s ' : " 123 ␣Main " }

# A l l o f
print ( f " Values : ␣{d . v a l u e s ( ) } " )

t h e v a l u e s

Output

Key : d i c t _ k e y s ( [ ' name ' ,
Values : d i c t _ v a l u e s ( [ ' J e f f ' ,

' a d d r e s s ' ] )

' 1 2 3 Main ' ] )

Dictionaries and lists can be combined. This syntax is closely related to JSON. Dictionaries and lists
together are a good way to build very complex data structures. While Python allows quotes (") and
apostrophe (’) for strings, JSON only allows double-quotes ("). We will cover JSON in much greater detail
later in this module.

The following code shows a hybrid usage of dictionaries and lists.

Code

# Python l i s t & map s t r u c t u r e s
c u s t o m e r s = [

{ " name " :

" J e f f ␣&␣ Tracy ␣ Heaton " ,

" p e t s " :

[ " Wynton " ,

" C r i c k e t " ,

" Hickory " ] } ,

{ " name " :
{ " name " :

" John ␣ Smith " ,
" Jane ␣Doe " }

" p e t s " :

[ " r o v e r " ] } ,

]

print ( c u s t o m e r s )

for customer in c u s t o m e r s :

print ( f " { customer [ ' name ' ] } : { customer . g e t ( ' p e t s ' , ␣ ' no␣ p e t s ' ) } " )

Output

' J e f f & Tracy Heaton ' ,

[ { ' name ' :
' Hickory ' ] } , { ' name ' :
Doe ' } ]
J e f f & Tracy Heaton : [ ' Wynton ' ,
John Smith : [ ' r o v e r ' ]

' p e t s ' :

[ ' Wynton ' ,

' C r i c k e t ' ,

' John Smith ' ,

' p e t s ' :

[ ' r o v e r ' ] } , { ' name ' :

' Jane

' C r i c k e t ' ,

' Hickory ' ]

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

19

Jane Doe : no p e t s

The variable customers is a list that holds three dictionaries that represent customers. You can
think of these dictionaries as records in a table. The ﬁelds in these individual records are the keys of the
dictionary. Here the keys name and pets are ﬁelds. However, the ﬁeld pets holds a list of pet names.
There is no limit to how deep you might choose to nest lists and maps. It is also possible to nest a map
inside of a map or a list inside of another list.

1.3.4 More Advanced Lists

Several advanced features are available for lists that this section introduces. One such function is zip. Two
lists can be combined into a single list by the zip command. The following code demonstrates the zip
command.

Code

a = [ 1 , 2 , 3 , 4 , 5 ]
b = [ 5 , 4 , 3 , 2 , 1 ]

print ( zip ( a , b ) )

Output

<z i p o b j e c t a t 0 x000001802A7A2E08>

To see the results of the zip function, we convert the returned zip object into a list. As you can see,
the zip function returns a list of tuples. Each tuple represents a pair of items that the function zipped
together. The order in the two lists was maintained.

Code

a = [ 1 , 2 , 3 , 4 , 5 ]
b = [ 5 , 4 , 3 , 2 , 1 ]

print ( l i s t ( zip ( a , b ) ) )

Output

[ ( 1 , 5 ) ,

( 2 , 4 ) ,

( 3 , 3 ) ,

( 4 , 2 ) ,

( 5 , 1 ) ]

The usual method for using the zip command is inside of a for-loop. The following code shows how a

for-loop can assign a variable to each collection that the program is iterating.

CHAPTER 1. PYTHON PRELIMINARIES

20

Code

a = [ 1 , 2 , 3 , 4 , 5 ]
b = [ 5 , 4 , 3 , 2 , 1 ]

for x , y in zip ( a , b ) :

print ( f ' {x}␣−␣{y} ' )

Output

1 − 5
2 − 4
3 − 3
4 − 2
5 − 1

Usually, both collections will be of the same length when passed to the zip command. It is not an
error to have collections of diﬀerent lengths. As the following code illustrates, the zip command will only
process elements up to the length of the smaller collection.

Code

a = [ 1 , 2 , 3 , 4 , 5 ]
b = [ 5 , 4 , 3 ]

print ( l i s t ( zip ( a , b ) ) )

Output

[ ( 1 , 5 ) ,

( 2 , 4 ) ,

( 3 , 3 ) ]

Sometimes you may wish to know the current numeric index when a for-loop is iterating through an
ordered collection. Use the enumerate command to track the index location for a collection element.
Because the enumerate command deals with numeric indexes of the collection, the zip command will
assign arbitrary indexes to elements from unordered collections.

Consider how you might construct a Python program to change every element greater than 5 to the
value of 5. The following program performs this transformation. The enumerate command allows the loop
to know which element index it is currently on, thus allowing the program to be able to change the value
of the current element of the collection.

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

21

Code

a = [ 2 , 1 0 , 3 , 1 1 , 1 0 , 3 , 2 , 1 ]
for i , x in enumerate ( a ) :

i f x >5:

a [ i ] = 5

print ( a )

Output

[ 2 , 5 , 3 , 5 , 5 , 3 , 2 , 1 ]

The comprehension command can dynamically build up a list. The comprehension below counts from

0 to 9 and adds each value (multiplied by 10) to a list.

Code

l s t = [ x ∗10 for x in range ( 1 0 ) ]
print ( l s t )

Output

[ 0 , 1 0 , 2 0 , 3 0 , 4 0 , 5 0 , 6 0 , 7 0 , 8 0 , 9 0 ]

A dictionary can also be a comprehension. The general format for this is:

d i c t _ v a r i a b l e = { key : v a l u e f o r

( key , v a l u e )

i n d i c t o n a r y . i t e m s ( ) }

A common use for this is to build up an index to symbolic column names.

Code

t e x t = [ ' c o l −z e r o ' , ' c o l −one ' ,
lookup = { key : v a l u e for ( value , key )
print ( lookup )

' c o l −two ' ,

' c o l −t h r e e ' ]

in enumerate ( t e x t ) }

Output

{ ' c o l −z e r o ' : 0 ,

' c o l −one ' : 1 ,

' c o l −two ' : 2 ,

' c o l −t h r e e ' : 3}

This can be used to easily ﬁnd the index of a column by name.

22

Code

CHAPTER 1. PYTHON PRELIMINARIES

print ( f ' The␣ i n d e x ␣ o f ␣ " c o l −two " ␣ i s ␣{ lookup [ " c o l −two " ] } ' )

Output

The i n d e x o f

" c o l −two "

i s 2

1.3.5 An Introduction to JSON

Data stored in a CSV ﬁle must be ﬂat; it must ﬁt into rows and columns. Most people refer to this type of
data as structured or tabular. This data is tabular because the number of columns is the same for every
row. Individual rows may be missing a value for a column; however, these rows still have the same columns.
This data is convenient for machine learning because most models, such as neural networks, also expect
incoming data to be of ﬁxed dimensions. Real-world information is not always so tabular. Consider if the
rows represent customers. These people might have multiple phone numbers and addresses. How would
you describe such data using a ﬁxed number of columns? It would be useful to have a list of these courses
in each row that can be variable length for each row or student.

JavaScript Object Notation (JSON) is a standard ﬁle format that stores data in a hierarchical format
similar to eXtensible Markup Language (XML). JSON is nothing more than a hierarchy of lists and
dictionaries. Programmers refer to this sort of data as semi-structured data or hierarchical data. The
following is a sample JSON ﬁle.

{

" John " ,
" Smith " ,

t r u e ,

" f i r s t N a m e " :
" lastName " :
" i s A l i v e " :
" age " : 2 7 ,
" a d d r e s s " : {

" 2 1 2nd S t r e e t " ,

" s t r e e t A d d r e s s " :
" c i t y " :
" s t a t e " :
" p o s t a l C o d e " :

"NY" ,

"New York " ,

"10021 −3100"

} ,
" phoneNumbers " :

[

{

} ,
{

" type " :
" number " :

" home " ,

" 2 1 2 555 −1234"

" type " :

" o f f i c e " ,

1.3. PART 1.3: PYTHON LISTS, DICTIONARIES, SETS, AND JSON

23

" number " :

" 6 4 6 555 −4567"

} ,
{

}

" type " :
" number " :

" m o b i l e " ,

" 1 2 3 456 −7890"

] ,
" c h i l d r e n " :
" s p o u s e " : n u l l

[ ] ,

}

The above ﬁle may look somewhat like Python code. You can see curly braces that deﬁne dictionaries
and square brackets that deﬁne lists. JSON does require there to be a single root element. A list or
dictionary can fulﬁll this role. JSON requires double-quotes to enclose strings and names. Single quotes
are not allowed in JSON.

JSON ﬁles are always legal JavaScript syntax. JSON is also generally valid as Python code, as demon-

strated by the following Python program.

Code

jsonHardCoded = {
" f i r s t N a m e " :
" lastName " :
" i s A l i v e " : True ,
" age " : 2 7 ,
" a d d r e s s " : {

" John " ,
" Smith " ,

" 21 ␣2nd␣ S t r e e t " ,

" s t r e e t A d d r e s s " :
" c i t y " :
" s t a t e " :
" p o s t a l C o d e " :

"NY" ,

"New␣York " ,

" 10021 −3100 "

} ,
" phoneNumbers " :

[

" type " :
" number " :

" home " ,

" 212 ␣ 555 −1234 "

" type " :
" number " :

" o f f i c e " ,

" 646 ␣ 555 −4567 "

{

} ,
{

} ,
{

" type " :
" number " :

" m o b i l e " ,

" 123 ␣ 456 −7890 "

24

}

}

] ,
[ ] ,
" c h i l d r e n " :
" s p o u s e " : None

CHAPTER 1. PYTHON PRELIMINARIES

Generally, it is better to read JSON from ﬁles, strings, or the Internet than hard coding, as demonstrated

here. However, for internal data structures, sometimes such hard-coding can be useful.

Python contains support for JSON. When a Python program loads a JSON the root list or dictionary

is returned, as demonstrated by the following code.

Code

import j s o n

j s o n _ s t r i n g = ' { " f i r s t " : " J e f f " , " l a s t " : " Heaton " } '
o b j = j s o n . l o a d s ( j s o n _ s t r i n g )
print ( f " F i r s t ␣name : ␣{ o b j [ ' f i r s t ' ] } " )
print ( f " Last ␣name : ␣{ o b j [ ' l a s t ' ] } " )

Output

F i r s t name : J e f f
Last name : Heaton

Python programs can also load JSON from a ﬁle or URL.

Code

import r e q u e s t s

r = r e q u e s t s . g e t ( " h t t p s : / / raw . g i t h u b u s e r c o n t e n t . com/ j e f f h e a t o n / "

+" t81_558_deep_learning / master / p e r s o n . j s o n " )

print ( r . j s o n ( ) )

Output

' lastName ' :

' Smith ' ,

' i s A l i v e ' : True ,

' age ' : 2 7 ,

' John ' ,

{ ' f i r s t N a m e ' :
' a d d r e s s ' : { ' s t r e e t A d d r e s s ' :
' s t a t e ' :
' postalCode ' :
' home ' ,
555 −4567 '} , { ' type ' :

' number ' :

'NY' ,

' 2 1 2nd S t r e e t ' ,
'10021 −3100 '} ,
' 2 1 2 555 −1234 '} , { ' type ' :

' c i t y ' :

'New York ' ,

' phoneNumbers ' :
' o f f i c e ' ,

' number ' :

[ { ' type ' :
' 6 4 6

' mobile ' ,

' number ' :

' 1 2 3 456 −78 90 '}] ,

' c h i l d r e n ' :

1.4. PART 1.4: FILE HANDLING

25

[ ] ,

' spouse ' : None}

Python programs can easily generate JSON strings from Python objects of dictionaries and lists.

Code

python_obj = { " f i r s t " : " J e f f " , " l a s t " : " Heaton " }
print ( j s o n . dumps ( python_obj ) )

Output

{ " f i r s t " :

" J e f f " ,

" l a s t " :

" Heaton " }

A data scientist will generally encounter JSON when they access web services to get their data. A data
scientist might use the techniques presented in this section to convert the semi-structured JSON data into
tabular data for the program to use with a model such as a neural network.

1.4 Part 1.4: File Handling

Files often contain the data that you use to train your AI programs. Once trained, your models may use
real-time data to form predictions. These predictions might be made on ﬁles too. Regardless of predicting
or training, ﬁle processing is a vital skill for the AI practitioner.

There are many diﬀerent types of ﬁles that you must process as an AI practitioner. Some of these ﬁle

types are listed here:

• CSV ﬁles (generally have the .csv extension) hold tabular data that resembles spreadsheet data.
• Image ﬁles (generally with the .png or .jpg extension) hold images for computer vision.
• Text ﬁles (often have the .txt extension) hold unstructured text and are essential for natural language

processing.

• JSON (often have the .json extension) contain semi-structured textual data in a human-readable

text-based format.

• H5 (can have a wide array of extensions) contain semi-structured textual data in a human-readable

text-based format. Keras and TensorFlow store neural networks as H5 ﬁles.

• Audio Files (often have an extension such as .au or .wav) contain recorded sound.

Data can come from a variety of sources. In this class, we obtain data from three primary locations:

• Your Hard Drive - This type of data is stored locally, and Python accesses it from a path that

looks something like: c:\data\myﬁle.csv or /Users/jheaton/data/myﬁle.csv.

• The Internet - This type of data resides in the cloud, and Python accesses it from a URL that

looks something like:

https://data.heatonresearch.com/data/t81-558/iris.csv.

26

CHAPTER 1. PYTHON PRELIMINARIES

• Google Drive (cloud) - If your code in Google CoLab, you use GoogleDrive to save and load
some data ﬁles. CoLab mounts your GoogleDrive into a path similar to the following: /content/-
drive/My Drive/myﬁle.csv.

1.4.1 Read a CSV File

Python programs can read CSV ﬁles with Pandas. We will see more about Pandas in the next section, but
for now, its general format is:

Code

import pandas a s pd

d f = pd . read_csv ( " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v " )

The above command loads Fisher’s Iris data set from the Internet. It might take a few seconds to load,
so it is good to keep the loading code in a separate Jupyter notebook cell so that you do not have to reload
it as you test your program. You can load Internet data, local hard drive, and Google Drive data this way.

Now that the data is loaded, you can display the ﬁrst ﬁve rows with this command.

Code

d i s p l a y ( d f [ 0 : 5 ] )

Output

sepal_l
5.1
4.9
4.7
4.6
5.0

sepal_w petal_l
3.5
3.0
3.2
3.1
3.6

1.4
1.4
1.3
1.5
1.4

petal_w species
0.2
0.2
0.2
0.2
0.2

Iris-setosa
Iris-setosa
Iris-setosa
Iris-setosa
Iris-setosa

0
1
2
3
4

1.4.2 Read (stream) a Large CSV File

Pandas will read the entire CSV ﬁle into memory. Usually, this is ﬁne. However, at times you may wish to
"stream" a huge ﬁle. Streaming allows you to process this ﬁle one record at a time. Because the program
does not load all of the data into memory, you can handle huge ﬁles. The following code loads the Iris
dataset and calculates averages, one row at a time. This technique would work for large ﬁles.

Code

import c s v
import u r l l i b . r e q u e s t

1.4. PART 1.4: FILE HANDLING

27

import c o d e c s
import numpy a s np

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v "
u r l s t r e a m = u r l l i b . r e q u e s t . u r l o p e n ( u r l )
c s v f i l e = c s v . r e a d e r ( c o d e c s . i t e r d e c o d e ( u r l s t r e a m ,
next ( c s v f i l e ) # S k i p h e a d e r row
sum = np . z e r o s ( 4 )
count = 0

' u t f −8 ' ) )

for l i n e in c s v f i l e :

# Convert each row t o Numpy a r r a y
l i n e 2 = np . a r r a y ( l i n e ) [ 0 : 4 ] . a s t y p e ( f l o a t )

# I f
i f

t h e l i n e

i s o f
len ( l i n e 2 ) == 4 :
sum += l i n e 2
count += 1

t h e r i g h t

l e n g t h ( s k i p empty l i n e s ) ,

t h e n add

# C a l c u l a t e t h e a v e r a g e , and p r i n t
# measurements ( f e a t u r e s )
print (sum/ count )

t h e a v e r a g e o f

t h e 4 i r i s

Output

[ 5 . 8 4 3 3 3 3 3 3 3 . 0 5 7 3 3 3 3 3 3 . 7 5 8

1 . 1 9 9 3 3 3 3 3 ]

1.4.3 Read a Text File

The following code reads the Sonnet 18 by William Shakespeare as a text ﬁle. This code streams the
document and reads it line-by-line. This code could handle a huge ﬁle.

Code

import u r l l i b . r e q u e s t

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ d a t a s e t s / sonnet_18 . t x t "
with u r l l i b . r e q u e s t . u r l o p e n ( u r l ) a s u r l s t r e a m :

for l i n e in c o d e c s . i t e r d e c o d e ( u r l s t r e a m ,

' u t f −8 ' ) :

print ( l i n e . r s t r i p ( ) )

28

Output

CHAPTER 1. PYTHON PRELIMINARIES

t e x t

l e a s e hath a l l

t h e eye o f heaven s h i n e s ,

I compare t h e e t o a summer ' s day ?

Sonnet 18 o r i g i n a l
William S h a k e s p e a r e
S h a l l
Thou a r t more l o v e l y and more t em p e r a te :
Rough winds do shake t h e d a r l i n g buds o f May ,
And summer ' s
t o o s h o r t a d a t e :
Sometime t o o hot
And o f t e n i s h i s g o l d complexion dimm ' d ;
from f a i r
And e v e r y f a i r
By chance o r nature ' s c h a n g i n g c o u r s e untrimm ' d ;
But thy e t e r n a l summer s h a l l not
Nor
f a i r
Nor
When i n e t e r n a l
t o time thou g r o w e s t :
So l o n g a s men can b r e a t h e o r e y e s can s e e ,
So l o n g l i v e s

l o s e p o s s e s s i o n o f
s h a l l Death brag thou wander ' s t

f a d e
thou owest ;
i n h i s

t h i s and t h i s g i v e s

sometime d e c l i n e s ,

t o t h e e .

l i n e s

t h a t

l i f e

shade ,

1.4.4 Read an Image

Computer vision is one of the areas that neural networks outshine other models. To support computer
vision, the Python programmer needs to understand how to process images. For this course, we will use
the Python PIL package for image processing. The following code demonstrates how to load an image
from a URL and display it.

Code

%m a t p l o t l i b i n l i n e
from PIL import Image
import r e q u e s t s
from i o import BytesIO

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "

r e s p o n s e = r e q u e s t s . g e t ( u r l )
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )

img

Output

1.5. PART 1.5: FUNCTIONS, LAMBDAS, AND MAP/REDUCE

29

1.5 Part 1.5: Functions, Lambdas, and Map/Reduce

Functions, lambdas, and map/reduce can allow you to process your data in advanced ways. We will
introduce these techniques here and expand on them in the next module, which will discuss Pandas.

Function parameters can be named or unnamed in Python. Default values can also be used. Consider

the following function.

Code

def s a y _ h e l l o ( s p e a k e r , person_to_greet , g r e e t i n g = " H e l l o " ) :

print ( f ' { g r e e t i n g }␣{ p erson _to_ greet } , ␣ t h i s ␣ i s ␣{ s p e a k e r } . ' )

s a y _ h e l l o ( ' J e f f ' ,
s a y _ h e l l o ( ' J e f f ' ,
s a y _ h e l l o ( s p e a k e r= ' J e f f ' , person_to _g re et=" John " , g r e e t i n g = " Goodbye " )

" John " )
" John " ,

" Goodbye " )

Output

H e l l o John ,
Goodbye John ,
Goodbye John ,

t h i s

i s J e f f .

t h i s
t h i s

i s J e f f .
i s J e f f .

30

CHAPTER 1. PYTHON PRELIMINARIES

A function is a way to capture code that is commonly executed. Consider the following function that

can be used to trim white space from a string capitalize the ﬁrst letter.

Code

def p r o c e s s _ s t r i n g ( s t r ) :
t = s t r . s t r i p ( )
return t [ 0 ] . upper ()+ t [ 1 : ]

This function can now be called quite easily.

Code

s t r = p r o c e s s _ s t r i n g ( " ␣␣ h e l l o ␣␣ " )
print ( f ' " { s t r } " ' )

Output

" H e l l o "

Python’s map is a very useful function that is provided in many diﬀerent programming languages. The
map function takes a list and applies a function to each member of the list and returns a second list
that is the same size as the ﬁrst.

Code

l = [ ' ␣␣␣ a p p l e ␣␣ ' ,
l i s t (map( p r o c e s s _ s t r i n g ,

' p e a r ␣ ' ,

l ) )

' o r a n g e ' ,

' p i n e ␣ a p p l e ␣␣ ' ]

Output

[ ' Apple ' ,

' Pear ' ,

' Orange ' ,

' Pine apple ' ]

1.5.1 Map

The map function is very similar to the Python comprehension that we previously explored. The
following comprehension accomplishes the same task as the previous call to map.

Code

l = [ ' ␣␣␣ a p p l e ␣␣ ' ,
l 2 = [ p r o c e s s _ s t r i n g ( x )
print ( l 2 )

' p e a r ␣ ' ,

' o r a n g e ' ,

' p i n e ␣ a p p l e ␣␣ ' ]

for x in l ]

1.5. PART 1.5: FUNCTIONS, LAMBDAS, AND MAP/REDUCE

31

Output

[ ' Apple ' ,

' Pear ' ,

' Orange ' ,

' Pine apple ' ]

The choice of using a map function or comprehension is up to the programmer. I tend to prefer

map since it is so common in other programming languages.

1.5.2 Filter

While a map function always creates a new list of the same size as the original, the ﬁlter function
creates a potentially smaller list.

Code

def g r e a t e r _ t h a n _ f i v e ( x ) :

return x>5

l = [ 1 , 1 0 , 2 0 , 3 , −2, 0 ]
l 2 = l i s t ( f i l t e r ( g r e a t e r _ t h a n _ f i v e ,
print ( l 2 )

l ) )

Output

[ 1 0 , 2 0 ]

1.5.3 Lambda

It might seem somewhat tedious to have to create an entire function just to check to see if a value is greater
than 5. A lambda saves you this eﬀort. A lambda is essentially an unnamed function.

Code

l = [ 1 , 1 0 , 2 0 , 3 , −2, 0 ]
l 2 = l i s t ( f i l t e r (lambda x : x>5,
print ( l 2 )

l ) )

Output

[ 1 0 , 2 0 ]

32

1.5.4 Reduce

CHAPTER 1. PYTHON PRELIMINARIES

Finally, we will make use of reduce. Like ﬁlter and map the reduce function also works on a list.
However, the result of the reduce is a single value. Consider if you wanted to sum the values of a list.
The sum is implemented by a lambda.

Code

from f u n c t o o l s import reduce

l = [ 1 , 1 0 , 2 0 , 3 , −2, 0 ]
r e s u l t = reduce (lambda x , y : x+y , l )
print ( r e s u l t )

Output

32

Chapter 2

Python for Machine Learning

2.1 Part 2.1: Introduction to Pandas

Pandas is an open-source library providing high-performance, easy-to-use data structures and data anal-
ysis tools for the Python programming language. It is based on the dataframe concept found in the R
programming language. For this class, Pandas will be the primary means by which we manipulate data to
be processed by neural networks.

The data frame is a crucial component of Pandas. We will use it to access the auto-mpg dataset. You
can ﬁnd this dataset on the UCI machine learning repository. For this class, we will use a version of the Auto
MPG dataset, where I added column headers. You can ﬁnd my version at https://data.heatonresearch.com/.
UCI took this dataset from the StatLib library, which Carnegie Mellon University maintains. The
dataset was used in the 1983 American Statistical Association Exposition. It contains data for 398 cars,
including mpg, cylinders, displacement, horsepower , weight, acceleration, model year, origin and the car’s
name.

The following code loads the MPG dataset into a data frame:

Code

# Simple d a t a f r a m e
import o s
import pandas a s pd

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " )

d i s p l a y ( d f [ 0 : 5 ] )

Output

33

34

CHAPTER 2. PYTHON FOR MACHINE LEARNING

mpg
18.0
15.0
18.0
16.0
17.0

cylinders
8
8
8
8
8

displacement
307.0
350.0
318.0
304.0
302.0

...
...
...
...
...
...

year
70
70
70
70
70

origin
1
1
1
1
1

name
chevrolet chevelle malibu
buick skylark 320
plymouth satellite
amc rebel sst
ford torino

0
1
2
3
4

The display function provides a cleaner display than merely printing the data frame. Specifying the

maximum rows and columns allows you to achieve greater control over the display.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

Output

mpg
18.0
15.0
...
28.0
31.0

cylinders
8
8
...
4
4

displacement
307.0
350.0
...
120.0
119.0

...
...
...
...
...
...

year
70
70
...
82
82

origin
1
1
...
1
1

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

0
1
...
396
397

It is possible to generate a second data frame to display statistical information about the ﬁrst data

frame.

Code

# S t r i p non−numerics
d f = d f . s e l e c t _ d t y p e s ( i n c l u d e =[ ' i n t ' ,

' f l o a t ' ] )

h e a d e r s = l i s t ( d f . columns . v a l u e s )
f i e l d s = [ ]

for f i e l d in h e a d e r s :

:

f i e l d s . append ( {
' name '
f i e l d ,
' mean ' : d f [ f i e l d ] . mean ( ) ,
' var ' : d f [ f i e l d ] . var ( ) ,
' sdev ' : d f [ f i e l d ] . s t d ( )

} )

2.1. PART 2.1: INTRODUCTION TO PANDAS

35

for f i e l d in f i e l d s :
print ( f i e l d )

Output

' mean ' : 2 3 . 5 1 4 5 7 2 8 6 4 3 2 1 6 0 7 ,

' var ' : 6 1 . 0 8 9 6 1 0 7 7 4 2 7 4 4 0 5 ,

'mpg ' ,

' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

{ ' name ' :
' sdev ' : 7 . 8 1 5 9 8 4 3 1 2 5 6 5 7 8 2 }
{ ' name ' :
2 . 8 9 3 4 1 5 4 3 9 9 2 0 0 0 3 ,
{ ' name ' :
1 0 8 7 2 . 1 9 9 1 5 2 2 4 7 3 8 4 ,
{ ' name ' :
' weight ' ,
7 1 7 1 4 0 . 9 9 0 5 2 5 6 7 6 3 ,
{ ' name ' :
7 . 6 0 4 8 4 8 2 3 3 6 1 1 3 8 3 ,
{ ' name ' :
' year ' ,
' sdev ' : 3 . 6 9 7 6 2 6 6 4 6 7 3 2 6 2 3 }
' o r i g i n ' ,
{ ' name ' :
0 . 6 4 3 2 9 2 0 2 6 8 8 5 0 5 4 9 ,

' a c c e l e r a t i o n ' ,

' mean ' : 5 . 4 5 4 7 7 3 8 6 9 3 4 6 7 3 4 ,

' var ' :

' sdev ' : 1 . 7 0 1 0 0 4 2 4 4 5 3 3 2 1 1 9 }

' mean ' : 1 9 3 . 4 2 5 8 7 9 3 9 6 9 8 4 9 3 ,

' sdev ' : 1 0 4 . 2 6 9 8 3 8 1 7 1 1 9 5 9 1 }

' mean ' : 2 9 7 0 . 4 2 4 6 2 3 1 1 5 5 7 8 ,
' sdev ' : 8 4 6 . 8 4 1 7 7 4 1 9 7 3 2 6 8 }

' var ' :

' mean ' : 1 5 . 5 6 8 0 9 0 4 5 2 2 6 1 3 0 7 ,

' sdev ' : 2 . 7 5 7 6 8 8 9 2 9 8 1 2 6 7 6 }

' var ' :

' var ' :

' mean ' : 1 . 5 7 2 8 6 4 3 2 1 6 0 8 0 4 0 2 ,

' var ' :

' sdev ' : 0 . 8 0 2 0 5 4 8 7 7 7 2 6 6 1 4 8 }

' mean ' : 7 6 . 0 1 0 0 5 0 2 5 1 2 5 6 2 9 ,

' var ' : 1 3 . 6 7 2 4 4 2 8 1 8 6 2 7 1 4 3 ,

This code outputs a list of dictionaries that hold this statistical information. This information looks
similar to the JSON code seen in Module 1. If proper JSON is needed, the program should add these
records to a list and call the Python JSON library’s dumps command.

The Python program can convert this JSON-like information to a data frame for better display.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 0 )
d f 2 = pd . DataFrame ( f i e l d s )
d i s p l a y ( d f 2 )

Output

36

CHAPTER 2. PYTHON FOR MACHINE LEARNING

cylinders
displacement

name
0 mpg
1
2
3 weight
4
5
6

acceleration
year
origin

mean
23.514573
5.454774
193.425879
2970.424623
15.568090
76.010050
1.572864

var
61.089611
2.893415
10872.199152
717140.990526
7.604848
13.672443
0.643292

sdev
7.815984
1.701004
104.269838
846.841774
2.757689
3.697627
0.802055

2.1.1 Missing Values

Missing values are a reality of machine learning. Ideally, every row of data will have values for all columns.
However, this is rarely the case. Most of the values are present in the MPG database. However, there are
missing values in the horsepower column. A common practice is to replace missing values with the median
value for that column. The program calculates the median. The following code replaces any NA values in
horsepower with the median:

Code

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

print ( f " horsepow er ␣ has ␣na ? ␣{pd . i s n u l l ( d f [ ' ho rsepower ' ] ) . v a l u e s . any ( ) } " )

print ( " F i l l i n g ␣ m i s s i n g ␣ v a l u e s . . . " )
med = d f [ ' h orse p ower ' ] . median ( )
d f [ ' ho rse p ow e r ' ] = d f [ ' horsepower ' ] . f i l l n a (med)
# d f = d f . dropna ( ) # you can a l s o s i m p l y drop NA v a l u e s

print ( f " h ors epo w er ␣ has ␣na ? ␣{pd . i s n u l l ( d f [ ' ho rsepower ' ] ) . v a l u e s . any ( ) } " )

Output

hor sepower has na ? True
F i l l i n g m i s s i n g v a l u e s . . .
hor sepower has na ? F a l s e

2.1. PART 2.1: INTRODUCTION TO PANDAS

37

2.1.2 Dealing with Outliers

Outliers are values that are unusually high or low. We typically consider outliers to be a value that is several
standard deviations from the mean. Sometimes outliers are simply errors; this is a result of observation
error. Outliers can also be truly large or small values that may be diﬃcult to address. The following
function can remove such values.

Code

# Remove a l l
def

rows where t h e s p e c i f i e d column i s +/− sd s t a n d a r d d e v i a t i o n s

r e m o v e _ o u t l i e r s ( df , name ,
sd ) :
drop_rows = d f . i n d e x [ ( np . abs ( d f [ name ] − d f [ name ] . mean ( ) )

d f . drop ( drop_rows , a x i s =0,

i n p l a c e=True )

>= ( sd ∗ d f [ name ] . s t d ( ) ) ) ]

The code below will drop every row from the Auto MPG dataset where the horsepower is two standard

deviations or more above or below the mean.

Code

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s c i p y . s t a t s import z s c o r e

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# c r e a t e f e a t u r e v e c t o r
med = d f [ ' horsepower ' ] . median ( )
d f [ ' h ors epower ' ] = d f [ ' horsepower ' ] . f i l l n a (med)

# Drop t h e name column
d f . drop ( ' name ' , 1 , i n p l a c e=True )

i n h o r s e p o w e r

# Drop o u t l i e r s
print ( " Length ␣ b e f o r e ␣MPG␣ o u t l i e r s ␣ dropped : ␣ {} " . format ( len ( d f ) ) )
r e m o v e _ o u t l i e r s ( df , 'mpg ' , 2 )
print ( " Length ␣ a f t e r ␣MPG␣ o u t l i e r s ␣ dropped : ␣ {} " . format ( len ( d f ) ) )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

38

Output

CHAPTER 2. PYTHON FOR MACHINE LEARNING

mpg
18.0
15.0
...
28.0
31.0

cylinders
8
8
...
4
4

displacement
307.0
350.0
...
120.0
119.0

horsepower weight
130.0
165.0
...
79.0
82.0

3504
3693
...
2625
2720

acceleration
12.0
11.5
...
18.6
19.4

year
70
70
...
82
82

origin
1
1
...
1
1

0
1
...
396
397

Length b e f o r e MPG o u t l i e r s dropped : 398
Length a f t e r MPG o u t l i e r s dropped : 388

2.1.3 Dropping Fields

You must drop ﬁelds that are of no value to the neural network. The following code removes the name
column from the MPG dataset.

Code

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

print ( f " B e f o r e ␣ drop : ␣{ l i s t ( d f . columns ) } " )
d f . drop ( ' name ' , 1 ,
print ( f " A f t e r ␣ drop : ␣{ l i s t ( d f . columns ) } " )

i n p l a c e=True )

Output

B e f o r e drop :
' weight ' ,
A f t e r drop :
' weight ' ,

[ ' mpg ' ,

' a c c e l e r a t i o n ' ,

' c y l i n d e r s ' ,
' year ' ,

' d i s p l a c e m e n t ' ,
' o r i g i n ' ,

' name ' ]

' horsepower ' ,

[ ' mpg ' ,

' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' horsepower ' ,

' a c c e l e r a t i o n ' ,

' year ' ,

' o r i g i n ' ]

2.1.4 Concatenating Rows and Columns

Python can concatenate rows and columns together to form new data frames. The code below creates a
new data frame from the name and horsepower columns from the Auto MPG dataset. The program

2.1. PART 2.1: INTRODUCTION TO PANDAS

39

does this by concatenating two columns together.

Code

# C rea t e a new d a t a f r a m e from name and h o r s e p o w e r

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

c o l _ h o r s e p o w e r = d f [ ' horsepower ' ]
col_name = d f [ ' name ' ]
r e s u l t = pd . c o n c a t ( [ col_name , c o l _ h o r s e p o w e r ] , a x i s =1)

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( r e s u l t )

Output

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

horsepower
130.0
165.0
...
79.0
82.0

0
1
...
396
397

The concat function can also concatenate rows together. This code concatenates the ﬁrst two rows

and the last two rows of the Auto MPG dataset.

Code

# Create a new d a t a f r a m e from f i r s t 2 rows and l a s t 2 rows

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

40

CHAPTER 2. PYTHON FOR MACHINE LEARNING

r e s u l t = pd . c o n c a t ( [ d f [ 0 : 2 ] , d f [ − 2 : ] ] , a x i s =0)

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 0 )
d i s p l a y ( r e s u l t )

Output

mpg
18.0
15.0
28.0
31.0

cylinders
8
8
4
4

displacement
307.0
350.0
120.0
119.0

...
...
...
...
...

year
70
70
82
82

origin
1
1
1
1

name
chevrolet chevelle malibu
buick skylark 320
ford ranger
chevy s-10

0
1
396
397

2.1.5 Training and Validation

We must evaluate a machine learning model based on its ability to predict values that it has never seen
before. Because of this, we often divide the training data into a validation and training set. The machine
learning model will learn from the training data but ultimately be evaluated based on the validation data.

• Training Data - In Sample Data - The data that the neural network used to train.
• Validation Data - Out of Sample Data - The data that the machine learning model is evaluated

upon after it is ﬁt to the training data.

There are two eﬀective means of dealing with training and validation data:

• Training/Validation Split - The program splits the data according to some ratio between a training

and validation (hold-out) set. Typical rates are 80% training and 20% validation.

• K-Fold Cross Validation - The program splits the data into several folds and models. Because
the program creates the same number of models as folds, the program can generate out-of-sample
predictions for the entire dataset.

The code below splits the MPG data into a training and validation set. The training set uses 80% of the
data, and the validation set uses 20%. Figure 2.1 shows how we train a model on 80% of the data and
then validated against the remaining 20%.

Code

import o s
import pandas a s pd
import numpy a s np

d f = pd . read_csv (

2.1. PART 2.1: INTRODUCTION TO PANDAS

41

Figure 2.1: Training and Validation

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# U s u a l l y a good i d e a t o s h u f f l e
d f = d f . r e i n d e x ( np . random . pe r mu t at i on ( d f . i n d e x ) )

mask = np . random . rand ( len ( d f ) ) < 0 . 8
trainDF = pd . DataFrame ( d f [ mask ] )
v a l i d a t i o n D F = pd . DataFrame ( d f [ ~ mask ] )

print ( f " T r a i n i n g ␣DF: ␣{ l e n ( trainDF ) } " )
print ( f " V a l i d a t i o n ␣DF: ␣ { l e n ( v a l i d a t i o n D F ) } " )

Output

T r a i n i n g DF: 333
V a l i d a t i o n DF: 65

2.1.6 Converting a Dataframe to a Matrix

Neural networks do not directly operate on Python data frames. A neural network requires a numeric
matrix. The program uses a data frame’s values property to convert the data to a matrix.

Code

d f . v a l u e s

Output

42

CHAPTER 2. PYTHON FOR MACHINE LEARNING

a r r a y ( [ [ 2 0 . 2 , 6 , 2 3 2 . 0 ,
[ 1 4 . 0 , 8 , 3 0 4 . 0 ,
[ 1 4 . 0 , 8 , 3 5 1 . 0 ,
. . . ,
[ 2 0 . 2 , 6 , 2 0 0 . 0 ,
[ 2 6 . 0 , 4 , 9 7 . 0 ,
[ 1 9 . 4 , 6 , 2 3 2 . 0 ,

. . . , 7 9 , 1 ,
. . . , 7 4 , 1 ,
. . . , 7 1 , 1 ,

' amc c o n c o r d d l 6 ' ] ,
' amc matador ( sw ) ' ] ,
' f o r d g a l a x i e 5 0 0 ' ] ,

. . . , 7 8 , 1 ,

' f o r d f a i r m o n t

( auto ) ' ] ,

. . . , 7 0 , 2 ,

. . . , 7 8 , 1 ,

' volkswagen 1131 d e l u x e sedan ' ] ,
' amc concord ' ] ] , dtype=o b j e c t )

You might wish only to convert some of the columns, to leave out the name column, use the following

code.

Code

d f [ [ 'mpg ' ,

' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

Output

a r r a y ( [ [ 2 0 . 2 ,
,
,

[ 1 4 .
[ 1 4 .
. . . ,
[ 2 0 . 2 ,
[ 2 6 .
,
[ 1 9 . 4 ,

6 .
8 .
8 .

6 .
4 .
6 .

, 2 3 2 .
, 3 0 4 .
, 3 5 1 .

, 2 0 0 .
,
9 7 .
, 2 3 2 .

,
,
,

,
,
,

. . . ,
. . . ,
. . . ,

. . . ,
. . . ,
. . . ,

1 8 . 2 ,
1 5 . 5 ,
1 3 . 5 ,

1 5 . 8 ,
2 0 . 5 ,
1 7 . 2 ,

7 9 .
7 4 .
7 1 .

7 8 .
7 0 .
7 8 .

,
,
,

,
,
,

1 .
1 .
1 .

1 .
2 .
1 .

] ,
] ,
] ,

] ,
] ,
] ] )

2.1.7 Saving a Dataframe to CSV

Many of the assignments in this course will require that you save a data frame to submit to the instructor.
The following code performs a shuﬄe and then saves a new copy.

Code

import o s
import pandas a s pd
import numpy a s np

path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

2.1. PART 2.1: INTRODUCTION TO PANDAS

43

f i l e n a m e _ w r i t e = o s . path . j o i n ( path ,
d f = d f . r e i n d e x ( np . random . pe r mu t a t i o n ( d f . i n d e x ) )
# S p e c i f y i n d e x = f a l s e t o not w r i t e row numbers
d f . to_csv ( f i l e n a m e _ w r i t e ,

i n d e x=F a l s e )

" auto−mpg−s h u f f l e . c s v " )

Output

Done

2.1.8 Saving a Dataframe to Pickle

A variety of software programs can use text ﬁles stored as CSV. However, they take longer to generate
and can sometimes lose small amounts of precision in the conversion. Generally, you will output to CSV
because it is very compatible, even outside of Python. Another format is Pickle. The code below stores the
Dataframe to Pickle. Pickle stores data in the exact binary representation used by Python. The beneﬁt
is that there is no loss of data going to CSV format. The disadvantage is that generally, only Python
programs can read Pickle ﬁles.

Code

import o s
import pandas a s pd
import numpy a s np
import p i c k l e

path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

f i l e n a m e _ w r i t e = o s . path . j o i n ( path ,
d f = d f . r e i n d e x ( np . random . pe r mu t at i on ( d f . i n d e x ) )

" auto−mpg−s h u f f l e . p k l " )

with open ( f i l e n a m e _ w r i t e , "wb" ) a s

f p :

p i c k l e . dump( df ,

f p )

Loading the pickle ﬁle back into memory is accomplished by the following lines of code. Notice that
the index numbers are still jumbled from the previous shuﬄe? Loading the CSV rebuilt (in the last step)
did not preserve these values.

CHAPTER 2. PYTHON FOR MACHINE LEARNING

44

Code

import o s
import pandas a s pd
import numpy a s np
import p i c k l e

path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

f i l e n a m e _ r e a d = o s . path . j o i n ( path ,

" auto−mpg−s h u f f l e . p k l " )

with open ( f i l e n a m e _ w r i t e , " rb " ) a s

f p :

d f = p i c k l e . l o a d ( f p )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

Output

mpg
38.0
25.4
...
31.6
30.5

cylinders
6
6
...
4
4

displacement
262.0
168.0
...
120.0
98.0

...
...
...
...
...
...

year
82
81
...
81
77

origin
1
3
...
3
1

name
oldsmobile cutlass ciera (diesel)
toyota cressida
...
mazda 626
chevrolet chevette

387
361
...
358
237

2.1.9 Module 2 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 2

2.2 Part 2.2: Categorical and Continuous Values

Neural networks require their input to be a ﬁxed number of columns. This input format is very similar
to spreadsheet data; it must be entirely numeric. It is essential to represent the data so that the neural
network can train from it. Before we look at speciﬁc ways to preprocess data, it is important to consider
four basic types of data, as deﬁned by[34]. Statisticians commonly refer to as the levels of measure:

2.2. PART 2.2: CATEGORICAL AND CONTINUOUS VALUES

45

• Character Data (strings)

– Nominal - Individual discrete items, no order. For example, color, zip code, and shape.
– Ordinal - Individual distinct items have an implied order. For example, grade level, job title,

Starbucks(tm) coﬀee size (tall, vente, grande)

–

Numeric Data

– Interval - Numeric values, no deﬁned start. For example, temperature. You would never say,

"yesterday was twice as hot as today."

– Ratio - Numeric values, clearly deﬁned start. For example, speed. You could say, "The ﬁrst

car is going twice as fast as the second."

2.2.1 Encoding Continuous Values

One common transformation is to normalize the inputs. It is sometimes valuable to normalize numeric
inputs in a standard form so that the program can easily compare these two values. Consider if a friend told
you that he received a 10-dollar discount. Is this a good deal? Maybe. But the cost is not normalized. If
your friend purchased a car, the discount is not that good. If your friend bought lunch, this is an excellent
discount!

Percentages are a prevalent form of normalization. If your friend tells you they got 10% oﬀ, we know that
this is a better discount than 5%. It does not matter how much the purchase price was. One widespread
machine learning normalization is the Z-Score:

z =

x − µ
σ

To calculate the Z-Score, you also need to calculate the mean(µ or ¯x) and the standard deviation (σ).

You can calculate the mean with this equation:

µ = ¯x =

x1 + x2 + · · · + xn
n

The standard deviation is calculated as follows:

v
u
u
t

1
N

σ =

N
X

(xi − µ)2

i=1

The following Python code replaces the mpg with a z-score. Cars with average MPG will be near zero,
above zero is above average, and below zero is below average. Z-Scores more that 3 above or below are
very rare; these are outliers.

CHAPTER 2. PYTHON FOR MACHINE LEARNING

46

Code

import o s
import pandas a s pd
from s c i p y . s t a t s import z s c o r e

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f [ 'mpg ' ] = z s c o r e ( d f [ 'mpg ' ] )
d i s p l a y ( d f )

Output

mpg
-0.706439
-1.090751
...
0.574601
0.958913

cylinders
8
8
...
4
4

displacement
307.0
350.0
...
120.0
119.0

...
...
...
...
...
...

year
70
70
...
82
82

origin
1
1
...
1
1

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

0
1
...
396
397

2.2.2 Encoding Categorical Values as Dummies

The traditional means of encoding categorical values is to make them dummy variables. This technique is
also called one-hot-encoding. Consider the following data set.

Code

import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d i s p l a y ( d f )

2.2. PART 2.2: CATEGORICAL AND CONTINUOUS VALUES

47

Output

id
1
2
...
1999
2000

job
vv
kd
...
qp
pe

area
c
c
...
c
c

...
...
...
...
...
...

retail_dense
0.492126
0.342520
...
0.598425
0.539370

crime
0.071100
0.400809
...
0.117803
0.451973

product
b
c
...
c
c

0
1
...
1998
1999

The area column is not numeric, so you must encode it with one-hot encoding. We display the number

of areas and individual values. There are just four values in the area categorical variable in this case.

Code

a r e a s = l i s t ( d f [ ' a r e a ' ] . unique ( ) )
print ( f ' Number␣ o f ␣ a r e a s : ␣{ l e n ( a r e a s ) } ' )
print ( f ' Areas : ␣{ a r e a s } ' )

Output

Number o f a r e a s : 4
[ ' c ' ,
Areas :

' d ' ,

' a ' ,

' b ' ]

There are four unique values in the area column. To encode these dummy variables, we would use four
columns, each representing one of the areas. For each row, one column would have a value of one, the rest
zeros. For this reason, this type of encoding is sometimes called one-hot encoding. The following code
shows how you might encode the values "a" through "d." The value A becomes [1,0,0,0] and the value B
becomes [0,1,0,0].

Code

dummies = pd . get_dummies ( [ ' a ' , ' b ' , ' c ' , ' d ' ] , p r e f i x= ' a r e a ' )
print ( dummies )

Output

area_a
0
1
2
3

area_b
1
0
0
0

area_c
0
1
0
0

area_d
0
0
1
0

0
0
0
1

48

CHAPTER 2. PYTHON FOR MACHINE LEARNING

We can now encode the actual column.

Code

dummies = pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x= ' a r e a ' )
print ( dummies [ 0 : 1 0 ] ) # J u s t show t h e

f i r s t 10

Output

area_c

area_b

area_a
0
1
. .
8
9
[ 1 0 rows x 4 columns ]

0
0
. . .
0
1

0
0
. . .
0
0

area_d

1
1
. . .
1
0

0
0
. . .
0
0

For the new dummy/one hot encoded values to be of any use, they must be merged back into the data

set.

Code

d f = pd . c o n c a t ( [ df , dummies ] , a x i s =1)

To encode the area column, we use the following code. Note that it is necessary to merge these dummies

back into the data frame.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f [ [ ' i d ' , ' j o b ' , ' a r e a ' , ' income ' , ' area_a ' ,

' area_b ' , ' area_c ' , ' area_d ' ] ] )

Output

2.2. PART 2.2: CATEGORICAL AND CONTINUOUS VALUES

49

id
1
2
3
4
5
...
1996
1997
1998
1999
2000

job
vv
kd
pe
11
kl
...
vv
kl
kl
qp
pe

area
c
c
c
c
d
...
c
d
d
c
c

income
50876.0
60369.0
55126.0
51690.0
28347.0
...
51017.0
26576.0
28595.0
67949.0
61467.0

area_a
0
0
0
0
0
...
0
0
0
0
0

area_b
0
0
0
0
0
...
0
0
0
0
0

area_c
1
1
1
1
0
...
1
0
0
1
1

area_d
0
0
0
0
1
...
0
1
1
0
0

0
1
2
3
4
...
1995
1996
1997
1998
1999

Usually, you will remove the original column area because the goal is to get the data frame to be entirely

numeric for the neural network.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f . drop ( ' a r e a ' , a x i s =1,
d i s p l a y ( d f [ [ ' i d ' , ' j o b ' , ' income ' , ' area_a ' ,

i n p l a c e=True )

' area_b ' , ' area_c ' , ' area_d ' ] ] )

Output

id
1
2
...
1999
2000

job
vv
kd
...
qp
pe

income
50876.0
60369.0
...
67949.0
61467.0

area_a
0
0
...
0
0

area_b
0
0
...
0
0

area_c
1
1
...
1
1

area_d
0
0
...
0
0

0
1
...
1998
1999

2.2.3 Removing the First Level

The pd.concat function also includes a parameter named drop_ﬁrst, which speciﬁes whether to get k-1
dummies out of k categorical levels by removing the ﬁrst level. Why would you want to remove the ﬁrst
level, in this case, area_a? This technique provides a more eﬃcient encoding by using the ordinarily unused
encoding of [0,0,0]. We encode the area to just three columns and map the categorical value of a to [0,0,0].
The following code demonstrates this technique.

50

Code

import pandas a s pd

CHAPTER 2. PYTHON FOR MACHINE LEARNING

dummies = pd . get_dummies ( [ ' a ' , ' b ' , ' c ' , ' d ' ] , p r e f i x= ' a r e a ' , d r o p _ f i r s t=True )
print ( dummies )

Output

area_b
0
1
2
3

area_c
0
1
0
0

area_d
0
0
1
0

0
0
0
1

As you can see from the above data, the area_a column is missing, as it get_dummies replaced it

by the encoding of [0,0,0]. The following code shows how to apply this technique to a dataframe.

Code

import pandas a s pd

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# encode t h e area column as dummy v a r i a b l e s
dummies = pd . get_dummies ( d f [ ' a r e a ' ] , d r o p _ f i r s t=True , p r e f i x= ' a r e a ' )
d f = pd . c o n c a t ( [ df , dummies ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# d i s p l a y t h e encoded d a t a f r a m e
pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f [ [ ' i d ' , ' j o b ' , ' income ' ,

' area_b ' , ' area_c ' , ' area_d ' ] ] )

Output

2.2. PART 2.2: CATEGORICAL AND CONTINUOUS VALUES

51

id
1
2
3
4
5
...
1996
1997
1998
1999
2000

job
vv
kd
pe
11
kl
...
vv
kl
kl
qp
pe

income
50876.0
60369.0
55126.0
51690.0
28347.0
...
51017.0
26576.0
28595.0
67949.0
61467.0

area_b
0
0
0
0
0
...
0
0
0
0
0

area_c
1
1
1
1
0
...
1
0
0
1
1

area_d
0
0
0
0
1
...
0
1
1
0
0

0
1
2
3
4
...
1995
1996
1997
1998
1999

2.2.4 Target Encoding for Categoricals

Target encoding is a popular technique for Kaggle competitions. Target encoding can sometimes increase
the predictive power of a machine learning model. However, it also dramatically increases the risk of
overﬁtting. Because of this risk, you must take care of using this method.

Generally, target encoding can only be used on a categorical feature when the output of the machine

learning model is numeric (regression).

The concept of target encoding is straightforward. For each category, we calculate the average target
value for that category. Then to encode, we substitute the percent corresponding to the category that the
categorical value has. Unlike dummy variables, where you have a column for each category with target
encoding, the program only needs a single column. In this way, target coding is more eﬃcient than dummy
variables.
Code

# Create a s m a l l sample d a t a s e t
import pandas a s pd
import numpy a s np

np . random . s e e d ( 4 3 )
d f = pd . DataFrame ( {

' cont_9 ' : np . random . rand ( 1 0 ) ∗ 1 0 0 ,
' cat_0 ' :
' cat_1 ' :
' y ' :

[ ' dog ' ] ∗ 5 + [ ' c a t ' ] ∗ 5 ,
[ ' w o l f ' ] ∗ 9 + [ ' t i g e r ' ] ∗ 1 ,

[ 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 ]

} )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 0 )
d i s p l a y ( d f )

CHAPTER 2. PYTHON FOR MACHINE LEARNING

52

Output

cont_9
11.505457
60.906654
13.339096
24.058962
32.713906
85.913749
66.609021
54.116221
2.901382
73.374830

cat_0
dog
dog
dog
dog
dog
cat
cat
cat
cat
cat

cat_1
wolf
wolf
wolf
wolf
wolf
wolf
wolf
wolf
wolf
tiger

y
1
0
1
1
1
1
0
0
0
0

0
1
2
3
4
5
6
7
8
9

We want to change them to a number rather than creating dummy variables for "dog" and "cat," we
would like to change them to a number. We could use 0 for a cat and 1 for a dog. However, we can encode
more information than just that. The simple 0 or 1 would also only work for one animal. Consider what
the mean target value is for cat and dog.

Code

means0 = d f . groupby ( ' cat_0 ' ) [ ' y ' ] . mean ( ) . t o _ d i c t ( )
means0

Output

{ ' cat ' : 0 . 2 ,

' dog ' : 0 . 8 }

The danger is that we are now using the target value (y) for training. This technique will potentially
lead to overﬁtting. The possibility of overﬁtting is even greater if a small number of a particular category.
To prevent this from happening, we use a weighting factor. The stronger the weight, the more categories
with fewer values will tend towards the overall average of y. You can perform this calculation as follows.

Code

d f [ ' y ' ] . mean ( )

Output

0 . 5

You can implement target encoding as follows. For more information on Target Encoding, refer to the

2.2. PART 2.2: CATEGORICAL AND CONTINUOUS VALUES

53

article "Target Encoding Done the Right Way", that I based this code upon.

Code

def calc_smooth_mean ( df1 , df2 , cat_name ,

t a r g e t , w e i g h t ) :

# Compute t h e g l o b a l mean
mean = d f [ t a r g e t ] . mean ( )

# Compute t h e number o f v a l u e s and t h e mean o f each group
' mean ' ] )
agg = d f . groupby ( cat_name ) [ t a r g e t ] . agg ( [ ' count ' ,
c o u n t s = agg [ ' count ' ]
means = agg [ ' mean ' ]

# Compute t h e " smoothed " means
smooth = ( c o u n t s ∗ means + w e i g h t ∗ mean ) / ( c o u n t s + w e i g h t )

# R e p l a c e each v a l u e by t h e a c c o r d i n g smoothed mean
i f d f 2 i s None :

return d f 1 [ cat_name ] . map( smooth )

e l s e :

return d f 1 [ cat_name ] . map( smooth ) , d f 2 [ cat_name ] . map( smooth . t o _ d i c t ( ) )

The following code encodes these two categories.

Code

WEIGHT = 5
d f [ ' cat_0_enc ' ] = calc_smooth_mean ( d f 1=df , d f 2=None ,

cat_name= ' cat_0 ' ,

t a r g e t= ' y ' , w e i g h t=WEIGHT)

d f [ ' cat_1_enc ' ] = calc_smooth_mean ( d f 1=df , d f 2=None ,

cat_name= ' cat_1 ' ,

t a r g e t= ' y ' , w e i g h t=WEIGHT)

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 0 )

d i s p l a y ( d f )

Output

54

CHAPTER 2. PYTHON FOR MACHINE LEARNING

cont_9
11.505457
60.906654
13.339096
24.058962
32.713906
85.913749
66.609021
54.116221
2.901382
73.374830

cat_0
dog
dog
dog
dog
dog
cat
cat
cat
cat
cat

cat_1
wolf
wolf
wolf
wolf
wolf
wolf
wolf
wolf
wolf
tiger

y
1
0
1
1
1
1
0
0
0
0

cat_0_enc
0.65
0.65
0.65
0.65
0.65
0.35
0.35
0.35
0.35
0.35

cat_1_enc
0.535714
0.535714
0.535714
0.535714
0.535714
0.535714
0.535714
0.535714
0.535714
0.416667

0
1
2
3
4
5
6
7
8
9

2.2.5 Encoding Categorical Values as Ordinal

Typically categoricals will be encoded as dummy variables. However, there might be other techniques to
convert categoricals to numeric. Any time there is an order to the categoricals, a number should be used.
Consider if you had a categorical that described the current education level of an individual.

• Kindergarten (0)
• First Grade (1)
• Second Grade (2)
• Third Grade (3)
• Fourth Grade (4)
• Fifth Grade (5)
• Sixth Grade (6)
• Seventh Grade (7)
• Eighth Grade (8)
• High School Freshman (9)
• High School Sophomore (10)
• High School Junior (11)
• High School Senior (12)
• College Freshman (13)
• College Sophomore (14)
• College Junior (15)
• College Senior (16)
• Graduate Student (17)
• PhD Candidate (18)
• Doctorate (19)
• Post Doctorate (20)

The above list has 21 levels and would take 21 dummy variables to encode. However, simply encoding this
to dummies would lose the order information. Perhaps the most straightforward approach would be to
simply number them and assign the category a single number equal to the value in the parenthesis above.

2.3. PART 2.3: GROUPING, SORTING, AND SHUFFLING

55

However, we might be able to do even better. A graduate student is likely more than a year so you might
increase one value.

2.2.6 High Cardinality Categorical

If there were many, perhaps thousands or tens of thousands, then one-hot encoding is no longer a good
choice. We call these cases high cardinality categorical. We generally encode such values with an embedding
layer, which we will discuss later when introducing natural language processing (NLP).

2.3 Part 2.3: Grouping, Sorting, and Shuﬄing

We will take a look at a few ways to aﬀect an entire Pandas data frame. These techniques will allow us
to group, sort, and shuﬄe data sets. These are all essential operations for both data preprocessing and
evaluation.

2.3.1 Shuﬄing a Dataset

There may be information lurking in the order of the rows of your dataset. Unless you are dealing with
time-series data, the order of the rows should not be signiﬁcant. Consider if your training set included
employees in a company. Perhaps this dataset is ordered by the number of years the employees were with
the company. It is okay to have an individual column that speciﬁes years of service. However, having the
data in this order might be problematic.

Consider if you were to split the data into training and validation. You could end up with your validation
set having only the newer employees and the training set longer-term employees. Separating the data into
a k-fold cross validation could have similar problems. Because of these issues, it is important to shuﬄe the
data set.

Often shuﬄing and reindexing are both performed together. Shuﬄing randomizes the order of the data
set. However, it does not change the Pandas row numbers. The following code demonstrates a reshuﬄe.
Notice that the program has not reset the row indexes’ ﬁrst column. Generally, this will not cause any
issues and allows tracing back to the original order of the data. However, I usually prefer to reset this
index. I reason that I typically do not care about the initial position, and there are a few instances where
this unordered index can cause issues.

Code

import o s
import pandas a s pd
import numpy a s np

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

#np . random . s e e d ( 4 2 ) # Uncomment

t h i s

l i n e t o g e t

t h e same s h u f f l e each time

56

CHAPTER 2. PYTHON FOR MACHINE LEARNING

d f = d f . r e i n d e x ( np . random . pe r mu t at i on ( d f . i n d e x ) )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

Output

mpg
29.0
36.1
...
14.0
10.0

cylinders
4
4
...
8
8

displacement
68.0
98.0
...
302.0
307.0

...
...
...
...
...
...

year
73
78
...
73
70

origin
2
1
...
1
1

name
ﬁat 128
ford ﬁesta
...
ford gran torino
chevy c20

117
245
...
88
26

The following code demonstrates a reindex. Notice how the reindex orders the row indexes.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f . r e s e t _ i n d e x ( i n p l a c e=True , drop=True )
d i s p l a y ( d f )

Output

mpg
29.0
36.1
...
14.0
10.0

cylinders
4
4
...
8
8

displacement
68.0
98.0
...
302.0
307.0

...
...
...
...
...
...

year
73
78
...
73
70

origin
2
1
...
1
1

name
ﬁat 128
ford ﬁesta
...
ford gran torino
chevy c20

0
1
...
396
397

2.3.2 Sorting a Data Set

While it is always good to shuﬄe a data set before training, during training and preprocessing, you may
also wish to sort the data set. Sorting the data set allows you to order the rows in either ascending or
descending order for one or more columns. The following code sorts the MPG dataset by name and displays
the ﬁrst car.

2.3. PART 2.3: GROUPING, SORTING, AND SHUFFLING

57

Code

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

d f = d f . s o r t _ v a l u e s ( by= ' name ' , a s c e n d i n g=True )
print ( f " The␣ f i r s t ␣ c a r ␣ i s : ␣{ d f [ ' name ' ] . i l o c [ 0 ] } " )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

Output

mpg
13.0
15.0
...
44.3
31.9

cylinders
8
8
...
4
4

displacement
360.0
390.0
...
90.0
89.0

...
...
...
...
...
...

year
73
70
...
80
79

origin
1
1
...
2
2

name
amc ambassador brougham
amc ambassador dpl
...
vw rabbit c (diesel)
vw rabbit custom

96
9
...
325
293

The f i r s t

c a r

i s : amc ambassador brougham

2.3.3 Grouping a Data Set

Grouping is a typical operation on data sets. Structured Query Language (SQL) calls this operation a
"GROUP BY." Programmers use grouping to summarize data. Because of this, the summarization row
count will usually shrink, and you cannot undo the grouping. Because of this loss of information, it is
essential to keep your original data before the grouping.

We use the Auto MPG dataset to demonstrate grouping.

Code

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,

58

CHAPTER 2. PYTHON FOR MACHINE LEARNING

na_values =[ 'NA ' ,

' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d i s p l a y ( d f )

Output

mpg
18.0
15.0
...
28.0
31.0

cylinders
8
8
...
4
4

displacement
307.0
350.0
...
120.0
119.0

...
...
...
...
...
...

year
70
70
...
82
82

origin
1
1
...
1
1

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

0
1
...
396
397

You can use the above data set with the group to perform summaries. For example, the following code
will group cylinders by the average (mean). This code will provide the grouping. In addition to mean,
you can use other aggregating functions, such as sum or count.

Code

g = d f . groupby ( ' c y l i n d e r s ' ) [ 'mpg ' ] . mean ( )
g

Output

c y l i n d e r s
3
4
5
6
8
Name : mpg , dtype :

2 0 . 5 5 0 0 0 0
2 9 . 2 8 6 7 6 5
2 7 . 3 6 6 6 6 7
1 9 . 9 8 5 7 1 4
1 4 . 9 6 3 1 0 7

f l o a t 6 4

It might be useful to have these mean values as a dictionary.

Code

d = g . t o _ d i c t ( )
d

Output

2.4. PART 2.4: APPLY AND MAP

59

{ 3 : 2 0 . 5 5 ,

4 : 2 9 . 2 8 6 7 6 4 7 0 5 8 8 2 3 6 ,
5 : 2 7 . 3 6 6 6 6 6 6 6 6 6 6 6 6 6 4 ,
6 : 1 9 . 9 8 5 7 1 4 2 8 5 7 1 4 2 8 4 ,
8 : 1 4 . 9 6 3 1 0 6 7 9 6 1 1 6 5 0 8 }

A dictionary allows you to access an individual element quickly. For example, you could quickly look
up the mean for six-cylinder cars. You will see that target encoding, introduced later in this module, uses
this technique.

Code

d [ 6 ]

Output

1 9 . 9 8 5 7 1 4 2 8 5 7 1 4 2 8 4

The code below shows how to count the number of rows that match each cylinder count.

Code

d f . groupby ( ' c y l i n d e r s ' ) [ 'mpg ' ] . count ( ) . t o _ d i c t ( )

Output

{ 3 : 4 , 4 : 2 0 4 , 5 : 3 , 6 : 8 4 , 8 : 103}

2.4 Part 2.4: Apply and Map

If you’ve ever worked with Big Data or functional programming languages before, you’ve likely heard of
map/reduce. Map and reduce are two functions that apply a task you create to a data frame. Pandas
supports functional programming techniques that allow you to use functions across en entire data frame.
In addition to functions that you write, Pandas also provides several standard functions for use with data
frames.

2.4.1 Using Map with Dataframes

The map function allows you to transform a column by mapping certain values in that column to other
values. Consider the Auto MPG data set that contains a ﬁeld origin_name that holds a value between

60

CHAPTER 2. PYTHON FOR MACHINE LEARNING

one and three that indicates the geographic origin of each car. We can see how to use the map function to
transform this numeric origin into the textual name of each origin.

We will begin by loading the Auto MPG data set.

Code

import o s
import pandas a s pd
import numpy a s np

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d i s p l a y ( d f )

Output

mpg
18.0
15.0
...
28.0
31.0

cylinders
8
8
...
4
4

displacement
307.0
350.0
...
120.0
119.0

...
...
...
...
...
...

year
70
70
...
82
82

origin
1
1
...
1
1

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

0
1
...
396
397

The map method in Pandas operates on a single column. You provide map with a dictionary of values
to transform the target column. The map keys specify what values in the target column should be turned
into values speciﬁed by those keys. The following code shows how the map function can transform the
numeric values of 1, 2, and 3 into the string values of North America, Europe, and Asia.

Code

# Apply t h e map
d f [ ' origin_name ' ] = d f [ ' o r i g i n ' ] . map(

{ 1 :

' North ␣ America ' , 2 :

' Europe ' , 3 :

' Asia ' } )

# S h u f f l e t h e data ,
# more r e g i o n s .
d f = d f . r e i n d e x ( np . random . pe r mu t at i on ( d f . i n d e x ) )

so t h a t we h o p e f u l l y s e e

# D i s p l a y

2.4. PART 2.4: APPLY AND MAP

61

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )
d i s p l a y ( d f )

Output

mpg
18.0
15.5
28.0
23.0
19.0
...
44.6
43.4
16.0
24.0
22.0

cylinders
6
8
4
4
6
...
4
4
6
4
6

displacement
258.0
351.0
151.0
120.0
232.0
...
91.0
90.0
225.0
116.0
198.0

...
...
...
...
...
...
...
...
...
...
...
...

origin
1
1
1
3
1
...
3
2
1
2
1

45
290
313
82
33
...
329
326
34
118
15

origin_name

name
amc hornet sportabout (sw) North America
North America
ford country squire (sw)
chevrolet citation
North America
toyouta corona mark ii (sw) Asia
amc gremlin
...
honda civic 1500 gl
vw dasher (diesel)
plymouth satellite custom
opel manta
plymouth duster

North America
...
Asia
Europe
North America
Europe
North America

2.4.2 Using Apply with Dataframes

The apply function of the data frame can run a function over the entire data frame. You can use either a
traditional named function or a lambda function. Python will execute the provided function against each
of the rows or columns in the data frame. The axis parameter speciﬁes that the function is run across
rows or columns. For axis = 1, rows are used. The following code calculates a series called eﬃciency that
is the displacement divided by horsepower.

Code

e f f i c i e n c y = d f . apply (lambda x : x [ ' d i s p l a c e m e n t ' ] / x [ ' ho rsepower ' ] , a x i s =1)
d i s p l a y ( e f f i c i e n c y [ 0 : 1 0 ] )

Output

45
290
313
82
33
249
27

2 . 3 4 5 4 5 5
2 . 4 7 1 8 3 1
1 . 6 7 7 7 7 8
1 . 2 3 7 1 1 3
2 . 3 2 0 0 0 0
2 . 3 6 3 6 3 6
1 . 5 1 4 2 8 6

62

CHAPTER 2. PYTHON FOR MACHINE LEARNING

7
302
179
dtype :

2 . 0 4 6 5 1 2
1 . 5 0 0 0 0 0
1 . 2 3 4 6 9 4
f l o a t 6 4

You can now insert this series into the data frame, either as a new column or to replace an existing

column. The following code inserts this new series into the data frame.

Code

d f [ ' e f f i c i e n c y ' ] = e f f i c i e n c y

2.4.3 Feature Engineering with Apply and Map

In this section, we will see how to calculate a complex feature using map, apply, and grouping. The data
set is the following CSV:

• https://www.irs.gov/pub/irs-soi/16zpallagi.csv

This URL contains US Government public data for "SOI Tax Stats - Individual Income Tax Statistics."
The entry point to the website is here:

• https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-2016-zip-code-data-soi

Documentation describing this data is at the above link.

For this feature, we will attempt to estimate the adjusted gross income (AGI) for each of the zip codes.

The data ﬁle contains many columns; however, you will only use the following:

• STATE - The state (e.g., MO)
• zipcode - The zipcode (e.g. 63017)
• agi_stub - Six diﬀerent brackets of annual income (1 through 6)
• N1 - The number of tax returns for each of the agi_stubs

Note, that the ﬁle will have six rows for each zip code for each of the agi_stub brackets. You can skip zip
codes with 0 or 99999.

We will create an output CSV with these columns; however, only one row per zip code. Calculate a

weighted average of the income brackets. For example, the following six rows are present for 63017:

zipcode
--
63017
63017
63017
63017
63017
63017

agi_stub N1
--
1
2
3
4
5
6

--
4710
2780
2130
2010
5240
3510

2.4. PART 2.4: APPLY AND MAP

63

We must combine these six rows into one. For privacy reasons, AGI’s are broken out into 6 buckets.
We need to combine the buckets and estimate the actual AGI of a zipcode. To do this, consider the values
for N1:

• 1 = 1 to 25,000
• 2 = 25,000 to 50,000
• 3 = 50,000 to 75,000
• 4 = 75,000 to 100,000
• 5 = 100,000 to 200,000
• 6 = 200,000 or more

The median of each of these ranges is approximately:

• 1 = 12,500
• 2 = 37,500
• 3 = 62,500
• 4 = 87,500
• 5 = 112,500
• 6 = 212,500

Using this, you can estimate 63017’s average AGI as:

>>> t o t a l C o u n t = 4710 + 2780 + 2130 + 2010 + 5240 + 3510
>>> totalAGI = 4710 ∗ 12500 + 2780 ∗ 37500 + 2130 ∗ 62500

+ 2010 ∗ 87500 + 5240 ∗ 112500 + 3510 ∗ 212500

>>> p r i n t ( totalAGI / t o t a l C o u n t )

8 8 6 8 9 . 8 9 2 0 5 1 0 3 0 4 2

We begin by reading the government data.

Code

import pandas a s pd

d f=pd . read_csv ( ' h t t p s : / /www. i r s . gov /pub/ i r s −s o i /16 z p a l l a g i . c s v ' )

First, we trim all zip codes that are either 0 or 99999. We also select the three ﬁelds that we need.

Code

d f=d f . l o c [ ( d f [ ' z i p c o d e ' ] ! = 0 ) & ( d f [ ' z i p c o d e ' ] ! = 9 9 9 9 9 ) ,

[ 'STATE ' , ' z i p c o d e ' , ' agi_stub ' , 'N1 ' ] ]

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )

64

CHAPTER 2. PYTHON FOR MACHINE LEARNING

pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f )

Output

6
7
8
9
10
...
179785 WY
179786 WY
179787 WY
179788 WY
179789 WY

STATE zipcode
AL
AL
AL
AL
AL
...

35004
35004
35004
35004
35004
...
83414
83414
83414
83414
83414

agi_stub N1
1
2
3
4
5
...
2
3
4
5
6

1510
1410
950
650
630
...
40
40
0
40
30

We replace all of the agi_stub values with the correct median values with the map function.

Code

medians = { 1 : 1 2 5 0 0 , 2 : 3 7 5 0 0 , 3 : 6 2 5 0 0 , 4 : 8 7 5 0 0 , 5 : 1 1 2 5 0 0 , 6 : 2 1 2 5 0 0 }
d f [ ' agi_stub ' ]= d f . agi_stub .map( medians )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )
d i s p l a y ( d f )

Output

2.4. PART 2.4: APPLY AND MAP

65

6
7
8
9
10
...
179785 WY
179786 WY
179787 WY
179788 WY
179789 WY

STATE zipcode
AL
AL
AL
AL
AL
...

35004
35004
35004
35004
35004
...
83414
83414
83414
83414
83414

agi_stub N1
12500
37500
62500
87500
112500
...
37500
62500
87500
112500
212500

1510
1410
950
650
630
...
40
40
0
40
30

Next, we group the data frame by zip code.

Code

gr ou ps = d f . groupby ( by= ' z i p c o d e ' )

The program applies a lambda across the groups and calculates the AGI estimate.

Code

d f = pd . DataFrame ( gro up s . apply (

lambda x :sum( x [ 'N1 ' ] ∗ x [ ' agi_stub ' ] ) /sum( x [ 'N1 ' ] ) ) ) \
. r e s e t _ i n d e x ( )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f )

Output

66

CHAPTER 2. PYTHON FOR MACHINE LEARNING

zipcode
1001
1002
1003
1005
1007
...
99921
99922
99925
99926
99929

0
52895.322940
64528.451001
15441.176471
54694.092827
63654.353562
...
48042.168675
32954.545455
45639.534884
41136.363636
45911.214953

0
1
2
3
4
...
29867
29868
29869
29870
29871

We can now rename the new agi_estimate column.

Code

d f . columns = [ ' z i p c o d e ' , ' a g i _ e s t i m a t e ' ]

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 0 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f )

Output

0
1
2
3
4
...
29867
29868
29869
29870
29871

zipcode
1001
1002
1003
1005
1007
...
99921
99922
99925
99926
99929

agi_estimate
52895.322940
64528.451001
15441.176471
54694.092827
63654.353562
...
48042.168675
32954.545455
45639.534884
41136.363636
45911.214953

Finally, we check to see that our zip code of 63017 got the correct value.

2.5. PART 2.5: FEATURE ENGINEERING

67

Code

d f [ d f [ ' z i p c o d e ' ]==63017 ]

Output

zipcode
63017

agi_estimate
88689.892051

19909

2.5 Part 2.5: Feature Engineering

Feature engineering is an essential part of machine learning. For now, we will manually engineer features.
However, later in this course, we will see some techniques for automatic feature engineering.

2.5.1 Calculated Fields

It is possible to add new ﬁelds to the data frame that your program calculates from the other ﬁelds. We
can create a new column that gives the weight in kilograms. The equation to calculate a metric weight,
given weight in pounds, is:

m(kg) = m(lb) × 0.45359237

The following Python code performs this transformation:

Code

import o s
import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

d f . i n s e r t ( 1 ,
' weight_kg ' ,
pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 6 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d f

( d f [ ' w e i g h t ' ] ∗ 0 . 4 5 3 5 9 2 3 7 ) . a s t y p e ( int ) )

Output

68

CHAPTER 2. PYTHON FOR MACHINE LEARNING

mpg weight_kg
1589
18.0
1675
15.0
...
...
1190
28.0
1233
31.0

cylinders
8
8
...
4
4

...
...
...
...
...
...

year
70
70
...
82
82

origin
1
1
...
1
1

name
chevrolet chevelle malibu
buick skylark 320
...
ford ranger
chevy s-10

0
1
...
396
397

2.5.2 Google API Keys

Sometimes you will use external APIs to obtain data. The following examples show how to use the Google
API keys to encode addresses for use with neural networks. To use these, you will need your own Google
API key. The key I have below is not a real key; you need to put your own there. Google will ask for a
credit card, but there will be no actual cost unless you use a massive number of lookups. YOU ARE NOT
required to get a Google API key for this class; this only shows you how. If you want to get a Google API
key, visit this site and obtain one for geocode.

You can obtain your key from this link: Google API Keys.

Code

i f

t h e API key i s d e f i n e d i n an e n v i r o n m e n t a l v a r i a b l e ,

in o s . e n v i r o n :

'GOOGLE_API_KEY '
# I f
# t h e use t h e env v a r i a b l e .
GOOGLE_KEY = o s . e n v i r o n [ 'GOOGLE_API_KEY ' ]

e l s e :

# I f you have a Google API key o f your own , you can a l s o j u s t
# p u t
GOOGLE_KEY = 'REPLACE␣WITH␣YOUR␣GOOGLE␣API␣KEY '

i t h e r e :

2.5.3 Other Examples: Dealing with Addresses

Addresses can be diﬃcult to encode into a neural network. There are many diﬀerent approaches, and you
must consider how you can transform the address into something more meaningful. Map coordinates can
be a good approach. latitude and longitude can be a useful encoding. Thanks to the power of the Internet,
it is relatively easy to transform an address into its latitude and longitude values. The following code
determines the coordinates of Washington University:

Code

import r e q u e s t s

a d d r e s s = " 1␣ B r o o k i n g s ␣Dr , ␣ St . ␣ Louis , ␣MO␣ 63130 "

2.5. PART 2.5: FEATURE ENGINEERING

69

r e s p o n s e = r e q u e s t s . g e t (

' h t t p s : / / maps . g o o g l e a p i s . com/maps/ a p i / g e o c o d e / j s o n ? key={}&a d d r e s s ={} ' \
. format (GOOGLE_KEY, a d d r e s s ) )

resp_json_payload = r e s p o n s e . j s o n ( )

i f

' e r r o r _ m e s s a g e '
print ( resp_json_payload [ ' e r r o r _ m e s s a g e ' ] )

in resp_json_payload :

e l s e :

print ( resp_json_payload [ ' r e s u l t s ' ] [ 0 ] [ ' geometry ' ] [ ' l o c a t i o n ' ] )

Output

{ ' l a t ' : 3 8 . 6 4 8 1 6 5 3 ,

' lng ' : −90.3049506}

They might not be overly helpful if you feed latitude and longitude into the neural network as two
features. These two values would allow your neural network to cluster locations on a map. Sometimes
cluster locations on a map can be useful. Figure 2.2 shows the percentage of the population that smokes
in the USA by state.

Figure 2.2: Smokers by State

The above map shows that certain behaviors, like smoking, can be clustered by the global region.
However, often you will want to transform the coordinates into distances.

It is reasonably easy to
estimate the distance between any two points on Earth by using the great circle distance between any two
points on a sphere:

The following code implements this formula:

70

CHAPTER 2. PYTHON FOR MACHINE LEARNING

∆σ = arccos(cid:0)sin φ1 · sin φ2 + cos φ1 · cos φ2 · cos(∆λ)(cid:1)

d = r, ∆σ

Code

from math import s i n , cos ,

s q r t , atan2 ,

r a d i a n s

URL= ' h t t p s : / / maps . g o o g l e a p i s . com ' + \

' /maps/ a p i / g e o c o d e / j s o n ? key={}&a d d r e s s ={} '

# D i s t a n c e f u n c t i o n
def d i s t a n c e _ l a t _ l n g ( l a t 1 , lng1 , l a t 2 , l n g 2 ) :
# a p p r o x i m a t e r a d i u s o f e a r t h i n km
R = 6 3 7 3 . 0

t o r a d i a n s ( l a t / l o n a r e i n d e g r e e s )

# d e g r e e s
l a t 1 = r a d i a n s ( l a t 1 )
l n g 1 = r a d i a n s ( l n g 1 )
l a t 2 = r a d i a n s ( l a t 2 )
l n g 2 = r a d i a n s ( l n g 2 )

d l n g = l n g 2 − l n g 1
d l a t = l a t 2 − l a t 1

a = s i n ( d l a t / 2 ) ∗ ∗ 2 + c o s ( l a t 1 ) ∗ c o s ( l a t 2 ) ∗ s i n ( d l n g / 2 ) ∗ ∗ 2
c = 2 ∗ atan2 ( s q r t ( a ) ,

s q r t ( 1 − a ) )

return R ∗ c

# Find l a t
def

l o n f o r a d d r e s s

lookup_lat_lng ( a d d r e s s ) :
r e s p o n s e = r e q u e s t s . g e t ( \

URL. format (GOOGLE_KEY, a d d r e s s ) )

j s o n = r e s p o n s e . j s o n ( )
i f

len ( j s o n [ ' r e s u l t s ' ] ) == 0 :

r a i s e V a l u e E r r o r ( " Google ␣API␣ e r r o r ␣on : ␣ {} " . format ( a d d r e s s ) )

map = j s o n [ ' r e s u l t s ' ] [ 0 ] [ ' geometry ' ] [ ' l o c a t i o n ' ]
return map[ ' l a t ' ] ,map[ ' l n g ' ]

2.5. PART 2.5: FEATURE ENGINEERING

71

# D i s t a n c e be twe en two l o c a t i o n s

import r e q u e s t s

a d d r e s s 1 = " 1 ␣ B r o o k i n g s ␣Dr , ␣ St . ␣ Louis , ␣MO␣ 63130 "
a d d r e s s 2 = " 3301 ␣ C o l l e g e ␣Ave , ␣ Fort ␣ Lauderdale , ␣FL␣ 33314 "

l a t 1 ,
l a t 2 ,

l n g 1 = lookup_lat_lng ( a d d r e s s 1 )
l n g 2 = lookup_lat_lng ( a d d r e s s 2 )

print ( " D i s t a n c e , ␣ St . ␣ Louis , ␣MO␣ t o ␣ Ft . ␣ Lauderdale , ␣FL : ␣ {} ␣km" . format (

d i s t a n c e _ l a t _ l n g ( l a t 1 , lng1 , l a t 2 , l n g 2 ) ) )

Output

D i s t a n c e , St . Louis , MO t o Ft . Lauderdale , FL : 1 6 8 5 . 3 0 1 9 8 0 8 6 0 7 4 2 6 km

Distances can be a useful means to encode addresses. It would help if you considered what distance

might be helpful for your dataset. Consider:

• Distance to a major metropolitan area
• Distance to a competitor
• Distance to a distribution center
• Distance to a retail outlet

The following code calculates the distance between 10 universities and Washington University in St. Louis:

Code

# Encoding o t h e r u n i v e r s i t i e s by t h e i r d i s t a n c e t o Washington U n i v e r s i t y

s c h o o l s = [

' P r i n c e t o n ' ] ,

[ " P r i n c e t o n ␣ U n i v e r s i t y , ␣ P r i n c e t o n , ␣NJ␣ 08544 " ,
[ " M a s s a c h u s e t t s ␣ Hall , ␣ Cambridge , ␣MA␣ 02138 " ,
[ " 5801 ␣S␣ E l l i s ␣Ave , ␣ Chicago , ␣ IL ␣ 60637 " ,
[ " Yale , ␣New␣Haven , ␣CT␣ 06520 " ,
[ " 116 th ␣ St ␣&␣Broadway , ␣New␣York , ␣NY␣ 10027 " ,
[ " 450 ␣ S e r r a ␣ Mall , ␣ S t a n f o r d , ␣CA␣ 94305 " ,
[ " 77 ␣ M a s s a c h u s e t t s ␣Ave , ␣ Cambridge , ␣MA␣ 02139 " ,
[ " Duke␣ U n i v e r s i t y , ␣Durham , ␣NC␣ 27708 " ,
[ " U n i v e r s i t y ␣ o f ␣ Pennsy lvania , ␣ P h i l a d e l p h i a , ␣PA␣ 19104 " ,

' S t a n f o r d ' ] ,

' Yale ' ] ,

'MIT ' ] ,

' Harvard ' ] ,

' Duke␣ U n i v e r s i t y ' ] ,

' U n i v e r s i t y ␣ o f ␣ Chicago ' ] ,

' Columbia ␣ U n i v e r s i t y ' ] ,

' U n i v e r s i t y ␣ o f ␣ P e n n s y l v a n i a ' ] ,

CHAPTER 2. PYTHON FOR MACHINE LEARNING

[ " Johns ␣ Hopkins ␣ U n i v e r s i t y , ␣ Baltimore , ␣MD␣ 21218 " ,

' Johns ␣ Hopkins ' ]

72

]

l a t 1 ,

l n g 1 = lookup_lat_lng ( " 1␣ B r o o k i n g s ␣Dr , ␣ St . ␣ Louis , ␣MO␣ 63130 " )

for a d d r e s s , name in s c h o o l s :

l a t 2 , l n g 2 = lookup_lat_lng ( a d d r e s s )
d i s t = d i s t a n c e _ l a t _ l n g ( l a t 1 , lng1 , l a t 2 , l n g 2 )
print ( " S c h o o l ␣ ' { } ' , ␣ d i s t a n c e ␣ t o ␣ w u s t l ␣ i s : ␣ {} " . format ( name , d i s t ) )

Output

S c h o o l
S c h o o l
S c h o o l
4 1 8 . 0 8 1 5 9 7 2 1 7 7 9 3 4
S c h o o l
S c h o o l
S c h o o l
S c h o o l
S c h o o l
S c h o o l
1 3 0 7 . 1 9 5 4 1 2 0 0 4 2 3
S c h o o l

' P r i n c e t o n ' , d i s t a n c e t o w u s t l
' Harvard ' , d i s t a n c e t o w u s t l
' U n i v e r s i t y o f Chicago ' , d i s t a n c e t o w u s t l

i s : 1 3 5 4 . 4 8 3 0 8 9 5 0 5 2 7 4 6

i s : 1 6 7 0 . 6 2 9 7 0 2 7 1 6 1 0 2 2

i s :

i s : 1 5 0 8 . 2 1 7 8 3 1 7 1 2 1 2 7

' Yale ' , d i s t a n c e t o w u s t l
' Columbia U n i v e r s i t y ' , d i s t a n c e t o w u s t l
' S t a n f o r d ' , d i s t a n c e t o w u s t l
'MIT' , d i s t a n c e t o w u s t l
' Duke U n i v e r s i t y ' , d i s t a n c e t o w u s t l
' U n i v e r s i t y o f Pe nn sylv an ia ' , d i s t a n c e t o w u s t l

i s : 1 6 7 2 . 4 4 4 4 4 8 9 6 6 5 6 9 6

i s : 2 7 8 0 . 6 8 2 9 3 9 8 1 1 4 1 1 4

i s :

i s : 1 0 4 6 . 7 9 7 0 9 8 4 4 2 3 7 1 9

i s : 1 4 1 8 . 2 2 6 4 0 8 3 2 9 5 6 9 5

' Johns Hopkins ' , d i s t a n c e t o w u s t l

i s : 1 1 8 4 . 3 8 3 1 0 7 6 5 5 5 4 2 5

Chapter 3

Introduction to TensorFlow

3.1 Part 3.1: Deep Learning and Neural Network Introduction

Neural networks were one of the ﬁrst machine learning models. Their popularity has fallen twice and is now
on its third rise. Deep learning implies the use of neural networks. The "deep" in deep learning refers to a
neural network with many hidden layers. Because neural networks have been around for so long, they have
quite a bit of baggage. Researchers have created many diﬀerent training algorithms, activation/transfer
functions, and structures. This course is only concerned with the latest, most current state-of-the-art
techniques for deep neural networks. I will not spend much time discussing the history of neural networks.
Neural networks accept input and produce output. The input to a neural network is called the feature
vector. The size of this vector is always a ﬁxed length. Changing the size of the feature vector usually
means recreating the entire neural network. Though the feature vector is called a "vector," this is not
always the case. A vector implies a 1D array. Later we will learn about convolutional neural networks
(CNNs), which can allow the input size to change without retraining the neural network. Historically the
input to a neural network was always 1D. However, with modern neural networks, you might see input
data, such as:

• 1D vector - Classic input to a neural network, similar to rows in a spreadsheet. Common in

predictive modeling.

• 2D Matrix - Grayscale image input to a CNN.
• 3D Matrix - Color image input to a CNN.
• nD Matrix - Higher-order input to a CNN.

Before CNNs, programs either encoded images to an intermediate form or sent the image input to a neural
network by merely squashing the image matrix into a long array by placing the image’s rows side-by-side.
CNNs are diﬀerent as the matrix passes through the neural network layers.

Initially, this book will focus on 1D input to neural networks. However, later modules will focus more

heavily on higher dimension input.

The term dimension can be confusing in neural networks. In the sense of a 1D input vector, dimension
refers to how many elements are in that 1D array. For example, a neural network with ten input neurons

73

74

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

has ten dimensions. However, now that we have CNNs, the input has dimensions. The input to the neural
network will usually have 1, 2, or 3 dimensions. Four or more dimensions are unusual. You might have a
2D input to a neural network with 64x64 pixels. This conﬁguration would result in 4,096 input neurons.
This network is either 2D or 4,096D, depending on which dimensions you reference.

3.1.1 Classiﬁcation or Regression

Like many models, neural networks can function in classiﬁcation or regression:

• Regression - You expect a number as your neural network’s prediction.
• Classiﬁcation - You expect a class/category as your neural network’s prediction.

A classiﬁcation and regression neural network is shown by Figure 3.1.

Figure 3.1: Neural Network Classiﬁcation and Regression

Notice that the output of the regression neural network is numeric, and the classiﬁcation output is a
class. Regression, or two-class classiﬁcation, networks always have a single output. Classiﬁcation neural
networks have an output neuron for each category.

3.1.2 Neurons and Layers

Most neural network structures use some type of neuron. Many diﬀerent neural networks exist, and pro-
grammers introduce experimental neural network structures. Consequently, it is not possible to cover every
neural network architecture. However, there are some commonalities among neural network implementa-
tions. A neural network algorithm would typically be composed of individual, interconnected units, even
though these units may or may not be called neurons. The name for a neural network processing unit
varies among the literature sources. It could be called a node, neuron, or unit.

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

75

Figure 3.2: An Artiﬁcial Neuron

A diagram shows the abstract structure of a single artiﬁcial neuron in Figure 3.2.
The artiﬁcial neuron receives input from one or more sources that may be other neurons or data fed
into the network from a computer program. This input is usually ﬂoating-point or binary. Often binary
input is encoded to ﬂoating-point by representing true or false as 1 or 0. Sometimes the program also
depicts the binary information using a bipolar system with true as one and false as -1.

An artiﬁcial neuron multiplies each of these inputs by a weight. Then it adds these multiplications and
passes this sum to an activation function. Some neural networks do not use an activation function. The
following equation summarizes the calculated output of a neuron:

f (x, w) = φ(

X

(θi · xi))

i

In the above equation, the variables x and θ represent the input and weights of the neuron. The variable
i corresponds to the number of weights and inputs. You must always have the same number of weights as
inputs. The neural network multiplies each weight by its respective input and feeds the products of these
multiplications into an activation function, denoted by the Greek letter φ (phi). This process results in a

76

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

single output from the neuron.

The above neuron has two inputs plus the bias as a third. This neuron might accept the following input

feature vector:

[1, 2]

Because a bias neuron is present, the program should append the value of one as follows:

[1, 2, 1]

The weights for a 3-input layer (2 real inputs + bias) will always have additional weight for the bias.

A weight vector might be:

[0.1, 0.2, 0.3]

To calculate the summation, perform the following:

0.11 + 0.22 + 0.3 ∗ 1 = 0.8

The program passes a value of 0.8 to the φ (phi) function, representing the activation function.
The above ﬁgure shows the structure with just one building block. You can chain together many
artiﬁcial neurons to build an artiﬁcial neural network (ANN). Think of the artiﬁcial neurons as building
blocks for which the input and output circles are the connectors. Figure 3.3 shows an artiﬁcial neural
network composed of three neurons:

The above diagram shows three interconnected neurons. This representation is essentially this ﬁgure,
minus a few inputs, repeated three times and then connected. It also has a total of four inputs and a single
output. The output of neurons N1 and N2 feed N3 to produce the output O. To calculate the output
for this network, we perform the previous equation three times. The ﬁrst two times calculate N1 and N2,
and the third calculation uses the output of N1 and N2 to calculate N3.

Neural network diagrams do not typically show the detail seen in the previous ﬁgure. We can omit the

activation functions and intermediate outputs to simplify the chart, resulting in Figure 3.4.

Looking at the previous ﬁgure, you can see two additional components of neural networks. First,
consider the graph represents the inputs and outputs as abstract dotted line circles. The input and output
could be parts of a more extensive neural network. However, the input and output are often a particular
type of neuron that accepts data from the computer program using the neural network. The output neurons
return a result to the program. This type of neuron is called an input neuron. We will discuss these neurons
in the next section. This ﬁgure shows the neurons arranged in layers. The input neurons are the ﬁrst layer,
the N1 and N2 neurons create the second layer, the third layer contains N3, and the fourth layer has O.
Most neural networks arrange neurons into layers.

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

77

Figure 3.3: Three Neuron Neural Network

The neurons that form a layer share several characteristics. First, every neuron in a layer has the same
activation function. However, the activation functions employed by each layer may be diﬀerent. Each of
the layers fully connects to the next layer. In other words, every neuron in one layer has a connection
to neurons in the previous layer. The former ﬁgure is not fully connected. Several layers are missing
connections. For example, I1 and N2 do not connect. The next neural network in Figure 3.5 is fully
connected and has an additional layer.

In this ﬁgure, you see a fully connected, multilayered neural network. Networks such as this one will
always have an input and output layer. The hidden layer structure determines the name of the network
architecture. The network in this ﬁgure is a two-hidden-layer network. Most networks will have between
zero and two hidden layers. Without implementing deep learning strategies, networks with more than two
hidden layers are rare.

You might also notice that the arrows always point downward or forward from the input to the output.

78

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.4: Three Neuron Neural Network

Later in this course, we will see recurrent neural networks that form inverted loops among the neurons.
This type of neural network is called a feedforward neural network.

3.1.3 Types of Neurons

In the last section, we brieﬂy introduced the idea that diﬀerent types of neurons exist. Not every neural
network will use every kind of neuron.
It is also possible for a single neuron to ﬁll the role of several
diﬀerent neuron types. Now we will explain all the neuron types described in the course.

There are usually four types of neurons in a neural network:

• Input Neurons - We map each input neuron to one element in the feature vector.
• Hidden Neurons - Hidden neurons allow the neural network to be abstract and process the input

into the output.

• Output Neurons - Each output neuron calculates one part of the output.
• Bias Neurons - Work similar to the y-intercept of a linear equation.

We place each neuron into a layer:

• Input Layer - The input layer accepts feature vectors from the dataset. Input layers usually have

a bias neuron.

• Output Layer - The output from the neural network. The output layer does not have a bias neuron.
• Hidden Layers - Layers between the input and output layers. Each hidden layer will usually have

a bias neuron.

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

79

Figure 3.5: Fully Connected Neural Network Diagram

3.1.4 Input and Output Neurons

Nearly every neural network has input and output neurons. The input neurons accept data from the
program for the network. The output neuron provides processed data from the network back to the
program. The program will group these input and output neurons into separate layers called the input
and output layers. The program normally represents the input to a neural network as an array or vector.
The number of elements contained in the vector must equal the number of input neurons. For example, a
neural network with three input neurons might accept the following input vector:

[0.5, 0.75, 0.2]

Neural networks typically accept ﬂoating-point vectors as their input. To be consistent, we will represent
the output of a single output neuron network as a single-element vector. Likewise, neural networks will
output a vector with a length equal to the number of output neurons. The output will often be a single
value from a single output neuron.

3.1.5 Hidden Neurons

Hidden neurons have two essential characteristics. First, hidden neurons only receive input from other
neurons, such as input or other hidden neurons. Second, hidden neurons only output to other neurons,
such as output or other hidden neurons. Hidden neurons help the neural network understand the input and

80

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

form the output. Programmers often group hidden neurons into fully connected hidden layers. However,
these hidden layers do not directly process the incoming data or the eventual output.

A common question for programmers concerns the number of hidden neurons in a network. Since the
answer to this question is complex, more than one section of the course will include a relevant discussion
of the number of hidden neurons. Before deep learning, researchers generally suggested that anything
more than a single hidden layer is excessive.[14]Researchers have proven that a single-hidden-layer neural
network can function as a universal approximator. In other words, this network should be able to learn to
produce (or approximate) any output from any input as long as it has enough hidden neurons in a single
layer.

Training refers to the process that determines good weight values. Before the advent of deep learning,
researchers feared additional layers would lengthen training time or encourage overﬁtting. Both concerns
are true; however, increased hardware speeds and clever techniques can mitigate these concerns. Before
researchers introduced deep learning techniques, we did not have an eﬃcient way to train a deep network,
which is a neural network with many hidden layers. Although a single-hidden-layer neural network can
theoretically learn anything, deep learning facilitates a more complex representation of patterns in the
data.

3.1.6 Bias Neurons

Programmers add bias neurons to neural networks to help them learn patterns. Bias neurons function like
an input neuron that always produces a value of 1. Because the bias neurons have a constant output of
1, they are not connected to the previous layer. The value of 1, called the bias activation, can be set to
values other than 1. However, 1 is the most common bias activation. Not all neural networks have bias
neurons. Figure 3.6 shows a single-hidden-layer neural network with bias neurons:

The above network contains three bias neurons. Except for the output layer, every level includes a
single bias neuron. Bias neurons allow the program to shift the output of an activation function. We will
see precisely how this shifting occurs later in the module when discussing activation functions.

3.1.7 Other Neuron Types

The individual units that comprise a neural network are not always called neurons. Researchers will
sometimes refer to these neurons as nodes, units, or summations. You will almost always construct neural
networks of weighted connections between these units.

3.1.8 Why are Bias Neurons Needed?

The activation functions from the previous section specify the output of a single neuron. Together, the
weight and bias of a neuron shape the output of the activation to produce the desired output. To see how
this process occurs, consider the following equation. It represents a single-input sigmoid activation neural
network.

f (x, w, b) =

1
1 + e−(wx+b)

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

81

Figure 3.6: Neural Network with Bias Neurons

The x variable represents the single input to the neural network. The w and b variables specify the
weight and bias of the neural network. The above equation combines the weighted sum of the inputs
and the sigmoid activation function. For this section, we will consider the sigmoid function because it
demonstrates a bias neuron’s eﬀect.

The weights of the neuron allow you to adjust the slope or shape of the activation function. Figure 3.7

shows the eﬀect on the output of the sigmoid activation function if the weight is varied:
The above diagram shows several sigmoid curves using the following parameters:

f (x, 0.5, 0.0)

f (x, 1.0, 0.0)

f (x, 1.5, 0.0)

f (x, 2.0, 0.0)

We did not use bias to produce the curves, which is evident in the third parameter of 0 in each case.
Using four weight values yields four diﬀerent sigmoid curves in the above ﬁgure. No matter the weight, we
always get the same value of 0.5 when x is 0 because all curves hit the same point when x is 0. We might
need the neural network to produce other values when the input is near 0.5.

82

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.7: Neuron Weight Shifting

Bias does shift the sigmoid curve, which allows values other than 0.5 when x is near 0. Figure 3.8 shows

the eﬀect of using a weight of 1.0 with several diﬀerent biases:

The above diagram shows several sigmoid curves with the following parameters:

f (x, 1.0, 1.0)

f (x, 1.0, 0.5)

f (x, 1.0, 1.5)

f (x, 1.0, 2.0)

We used a weight of 1.0 for these curves in all cases. When we utilized several diﬀerent biases, sigmoid
curves shifted to the left or right. Because all the curves merge at the top right or bottom left, it is not a
complete shift.

When we put bias and weights together, they produced a curve that created the necessary output. The
above curves are the output from only one neuron. In a complete network, the output from many diﬀerent
neurons will combine to produce intricate output patterns.

3.1.9 Modern Activation Functions

Activation functions, also known as transfer functions, are used to calculate the output of each layer of a
neural network. Historically neural networks have used a hyperbolic tangent, sigmoid/logistic, or linear

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

83

Figure 3.8: Neuron Bias Shifting

activation function. However, modern deep neural networks primarily make use of the following activation
functions:

• Rectiﬁed Linear Unit (ReLU) - Used for the output of hidden layers.[8]
• Softmax - Used for the output of classiﬁcation neural networks.
• Linear - Used for the output of regression neural networks (or 2-class classiﬁcation).

3.1.10 Linear Activation Function

The most basic activation function is the linear function because it does not change the neuron output.
The following equation 1.2 shows how the program typically implements a linear activation function:

φ(x) = x

As you can observe, this activation function simply returns the value that the neuron inputs passed to

it. Figure 3.9 shows the graph for a linear activation function:

Regression neural networks, which learn to provide numeric values, will usually use a linear activation
function on their output layer. Classiﬁcation neural networks, which determine an appropriate class for
their input, will often utilize a softmax activation function for their output layer.

84

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.9: Linear Activation Function

3.1.11 Rectiﬁed Linear Units (ReLU)

Since its introduction, researchers have rapidly adopted the rectiﬁed linear unit (ReLU).[25]Before the
ReLU activation function, the programmers generally regarded the hyperbolic tangent as the activation
function of choice. Most current research now recommends the ReLU due to superior training results. As
a result, most neural networks should utilize the ReLU on hidden layers and either softmax or linear on
the output layer. The following equation shows the straightforward ReLU function:

φ(x) = max(0, x)

Figure 3.10 shows the graph of the ReLU activation function:
Most current research states that the hidden layers of your neural network should use the ReLU acti-

vation.

3.1.12 Softmax Activation Function

The ﬁnal activation function that we will examine is the softmax activation function. Along with the linear
activation function, you can usually ﬁnd the softmax function in the output layer of a neural network.
Classiﬁcation neural networks typically employ the softmax function. The neuron with the highest value
claims the input as a member of its class. Because it is a preferable method, the softmax activation function
forces the neural network’s output to represent the probability that the input falls into each of the classes.

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

85

Figure 3.10: Rectiﬁed Linear Units (ReLU)

The neuron’s outputs are numeric values without the softmax, with the highest indicating the winning
class.

To see how the program uses the softmax activation function, we will look at a typical neural network
classiﬁcation problem. The iris data set contains four measurements for 150 diﬀerent iris ﬂowers. Each of
these ﬂowers belongs to one of three species of iris. When you provide the measurements of a ﬂower, the
softmax function allows the neural network to give you the probability that these measurements belong to
each of the three species. For example, the neural network might tell you that there is an 80% chance that
the iris is setosa, a 15% probability that it is virginica, and only a 5% probability of versicolor. Because
these are probabilities, they must add up to 100%. There could not be an 80% probability of setosa, a 75%
probability of virginica, and a 20% probability of versicolor---this type of result would be nonsensical.

To classify input data into one of three iris species, you will need one output neuron for each species.
The output neurons do not inherently specify the probability of each of the three species. Therefore, it is
desirable to provide probabilities that sum to 100%. The neural network will tell you the likelihood of a
ﬂower being each of the three species. To get the probability, use the softmax function in the following
equation:

φi(x) =

exp(xi)
j exp(xj)

P

In the above equation, i represents the index of the output neuron (φ) that the program is calculating,
and j represents the indexes of all neurons in the group/level. The variable x designates the array of output
neurons. It’s important to note that the program calculates the softmax activation diﬀerently than the
other activation functions in this module. When softmax is the activation function, the output of a single
neuron is dependent on the other output neurons.

To see the softmax function in operation, refer to this Softmax example website.

86

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Consider a trained neural network that classiﬁes data into three categories: the three iris species. In
this case, you would use one output neuron for each of the target classes. Consider if the neural network
were to output the following:

• Neuron 1: setosa: 0.9
• Neuron 2: versicolour: 0.2
• Neuron 3: virginica: 0.4

The above output shows that the neural network considers the data to represent a setosa iris. However, these
numbers are not probabilities. The 0.9 value does not represent a 90% likelihood of the data representing
a setosa. These values sum to 1.5. For the program to treat them as probabilities, they must sum to 1.0.
The output vector for this neural network is the following:

[0.9, 0.2, 0.4]

If you provide this vector to the softmax function it will return the following vector:

[0.47548495534876745, 0.2361188410001125, 0.28839620365112]

The above three values do sum to 1.0 and can be treated as probabilities. The likelihood of the data
representing a setosa iris is 48% because the ﬁrst value in the vector rounds to 0.48 (48%). You can
calculate this value in the following manner:

sum = exp(0.9) + exp(0.2) + exp(0.4) = 5.17283056695839

j0 = exp(0.9)/sum = 0.47548495534876745

j1 = exp(0.2)/sum = 0.2361188410001125

j2 = exp(0.4)/sum = 0.28839620365112

3.1.13 Step Activation Function

The step or threshold activation function is another simple activation function. Neural networks were
initially called perceptrons. McCulloch Pitts (1943) introduced the original perceptron and used a step
activation function like the following equation:[24]The step activation is 1 if x>=0.5, and 0 otherwise.

This equation outputs a value of 1.0 for incoming values of 0.5 or higher and 0 for all other values. Step
functions, also known as threshold functions, only return 1 (true) for values above the speciﬁed threshold,
as seen in Figure 3.11.

3.1. PART 3.1: DEEP LEARNING AND NEURAL NETWORK INTRODUCTION

87

Figure 3.11: Step Activation Function

3.1.14 Sigmoid Activation Function

The sigmoid or logistic activation function is a common choice for feedforward neural networks that need
to output only positive numbers. Despite its widespread use, the hyperbolic tangent or the rectiﬁed linear
unit (ReLU) activation function is usually a more suitable choice. We introduce the ReLU activation
function later in this module. The following equation shows the sigmoid activation function:

φ(x) =

1
1 + e−x

Use the sigmoid function to ensure that values stay within a relatively small range, as seen in Figure

3.12:

As you can see from the above graph, we can force values to a range. Here, the function compressed

values above or below 0 to the approximate range between 0 and 1.

3.1.15 Hyperbolic Tangent Activation Function

The hyperbolic tangent function is also a prevalent activation function for neural networks that must
output values between -1 and 1. This activation function is simply the hyperbolic tangent (tanh) function,
as shown in the following equation:

φ(x) = tanh(x)

88

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.12: Sigmoid Activation Function

The graph of the hyperbolic tangent function has a similar shape to the sigmoid activation function, as

seen in Figure 3.13.

The hyperbolic tangent function has several advantages over the sigmoid activation function.

3.1.16 Why ReLU?

Why is the ReLU activation function so popular? One of the critical improvements to neural networks
makes deep learning work.[25]Before deep learning, the sigmoid activation function was prevalent. We
covered the sigmoid activation function earlier in this module. Frameworks like Keras often train neural
networks with gradient descent. For the neural network to use gradient descent, it is necessary to take the
derivative of the activation function. The program must derive partial derivatives of each of the weights
for the error function. Figure 3.14 shows a derivative, the instantaneous rate of change.

The derivative of the sigmoid function is given here:

φ0(x) = φ(x)(1 − φ(x))

Textbooks often give this derivative in other forms. We use the above form for computational eﬃciency.

To see how we determined this derivative, refer to the following article.
We present the graph of the sigmoid derivative in Figure 3.15.

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

89

Figure 3.13: Hyperbolic Tangent Activation Function

The derivative quickly saturates to zero as x moves from zero. This is not a problem for the derivative

of the ReLU, which is given here:

φ0(x) =

(

1 x > 0
0 x ≤ 0

3.1.17 Module 3 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 3

3.2 Part 3.2: Introduction to Tensorﬂow and Keras

TensorFlow[1]is an open-source software library for machine learning in various kinds of perceptual and
language understanding tasks. It is currently used for research and production by diﬀerent teams in many
commercial Google products, such as speech recognition, Gmail, Google Photos, and search, many of which
had previously used its predecessor DistBelief. TensorFlow was originally developed by the Google Brain
team for Google’s research and production purposes and later released under the Apache 2.0 open source
license on November 9, 2015.

• TensorFlow Homepage

90

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.14: Derivative

• TensorFlow GitHib
• TensorFlow Google Groups Support
• TensorFlow Google Groups Developer Discussion
• TensorFlow FAQ

3.2.1 Why TensorFlow

• Supported by Google
• Works well on Windows, Linux, and Mac
• Excellent GPU support
• Python is an easy to learn programming language
• Python is extremely popular in the data science community

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

91

Figure 3.15: Sigmoid Derivative

3.2.2 Deep Learning Tools

TensorFlow is not the only game in town. The biggest competitor to TensorFlow/Keras is PyTorch. Listed
below are some of the deep learning toolkits actively being supported:

• TensorFlow - Google’s deep learning API. The focus of this class, along with Keras.
• Keras - Acts as a higher-level to Tensorﬂow.
• PyTorch - PyTorch is an open-source machine learning library based on the Torch library, used for
computer vision and natural language applications processing. Facebook’s AI Research lab primarily
develops PyTorch.

Other deep learning tools:

• Deeplearning4J - Java-based. Supports all major platforms. GPU support in Java!
• H2O - Java-based.

In my opinion, the two primary Python libraries for deep learning are PyTorch and Keras. Generally,
PyTorch requires more lines of code to perform the deep learning applications presented in this course.
This trait of PyTorch gives Keras an easier learning curve than PyTorch. However, if you are creating
entirely new neural network structures in a research setting, PyTorch can make for easier access to some
of the low-level internals of deep learning.

3.2.3 Using TensorFlow Directly

Most of the time in the course, we will communicate with TensorFlow using Keras[4], which allows you to
specify the number of hidden layers and create the neural network. TensorFlow is a low-level mathematics
API, similar to Numpy. However, unlike Numpy, TensorFlow is built for deep learning. TensorFlow
compiles these compute graphs into highly eﬃcient C++/CUDA code.

92

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

3.2.4 TensorFlow Linear Algebra Examples

TensorFlow is a library for linear algebra. Keras is a higher-level abstraction for neural networks that you
build upon TensorFlow. In this section, I will demonstrate some basic linear algebra that directly employs
TensorFlow and does not use Keras. First, we will see how to multiply a row and column matrix.

Code

import t e n s o r f l o w a s

t f

# Create a Constant op t h a t p r o d u c e s a 1 x2 m a t r i x . The op i s
# added as a node t o t h e d e f a u l t graph .
#
# The v a l u e r e t u r n e d by t h e c o n s t r u c t o r
# o f
t h e Constant op .
matrix1 = t f . c o n s t a n t ( [ [ 3 . ,

r e p r e s e n t s

t h e o u t p u t

3 . ] ] )

# Create a n o t h e r Constant
matrix2 = t f . c o n s t a n t ( [ [ 2 . ] , [ 2 . ] ] )

t h a t p r o d u c e s a 2 x1 m a t r i x .

# Create a Matmul op t h a t
# The r e t u r n e d v a l u e ,
# m u l t i p l i c a t i o n .
p r o d u c t = t f . matmul ( matrix1 , matrix2 )

' p r o d u c t ' ,

t a k e s

' m a t r i x 1 ' and ' m a t r i x 2 ' as

r e p r e s e n t s

t h e r e s u l t o f

i n p u t s .
t h e m a t r i x

print ( p r o d u c t )
print ( f l o a t ( p r o d u c t ) )

Output

t f . Tensor ( [ [ 1 2 . ] ] ,
1 2 . 0

shape =(1 , 1 ) , dtype=f l o a t 3 2 )

This example multiplied two TensorFlow constant tensors. Next, we will see how to subtract a constant

from a variable.

Code

import t e n s o r f l o w a s

t f

x = t f . V a r i a b l e ( [ 1 . 0 , 2 . 0 ] )
a = t f . c o n s t a n t ( [ 3 . 0 , 3 . 0 ] )

# Add an op t o s u b t r a c t

' a '

from ' x ' . Run i t and p r i n t

t h e r e s u l t

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

93

sub = t f . s u b t r a c t ( x , a )
print ( sub )
print ( sub . numpy ( ) )
# ==> [ −2. −1.]

Output

t f . Tensor ( [ − 2 . − 1 . ] ,
[ −2. −1.]

shape = ( 2 , ) , dtype=f l o a t 3 2 )

Of course, variables are only useful if their values can be changed. The program can accomplish this

change in value by calling the assign function.

Code

x . a s s i g n ( [ 4 . 0 , 6 . 0 ] )

Output

< t f . V a r i a b l e
numpy=a r r a y ( [ 4 . , 6 . ] , dtype=f l o a t 3 2 )>

' UnreadVariable '

shape =(2 ,) dtype=f l o a t 3 2 ,

The program can now perform the subtraction with this new value.

Code

sub = t f . s u b t r a c t ( x , a )
print ( sub )
print ( sub . numpy ( ) )

Output

t f . Tensor ( [ 1 . 3 . ] ,
[ 1 . 3 . ]

shape = ( 2 , ) , dtype=f l o a t 3 2 )

In the next section, we will see a TensorFlow example that has nothing to do with neural networks.

3.2.5 TensorFlow Mandelbrot Set Example

Next, we examine another example where we use TensorFlow directly. To demonstrate that TensorFlow
is mathematical and does not only provide neural networks, we will also ﬁrst use it for a non-machine

94

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

learning rendering task. The code presented here can render a Mandelbrot set. Note, I based this code
on a Mandelbrot example that I originally found with TensorFlow 1.0. I’ve updated the code slightly to
comply with current versions of TensorFlow.

Code

l i b r a r i e s

# Import
import t e n s o r f l o w a s
import numpy a s np

t f

f o r

s i m u l a t i o n

f o r v i s u a l i z a t i o n

# I m p o r t s
import PIL . Image
from i o import BytesIO
from IPython . d i s p l a y import Image , d i s p l a y

def D i s p l a y F r a c t a l ( a ,

fmt= ' j p e g ' ) :

" " " D i s p l a y an a r r a y o f

i t e r a t i o n c o u n t s as a

c o l o r f u l p i c t u r e o f a f r a c t a l . " " "

a _ c y c l i c = ( 6 . 2 8 ∗ a / 2 0 . 0 ) . r e s h a p e ( l i s t ( a . shape ) + [ 1 ] )
img = np . c o n c a t e n a t e ([10+20∗ np . c o s ( a _ c y c l i c ) ,
30+50∗np . s i n ( a _ c y c l i c ) ,
155−80∗np . c o s ( a _ c y c l i c ) ] , 2 )

img [ a==a .max ( ) ] = 0
a = img
a = np . u i n t 8 ( np . c l i p ( a , 0 , 2 5 5 ) )
f = BytesIO ( )
PIL . Image . f r o m a r r a y ( a ) . s a v e ( f ,
d i s p l a y ( Image ( data=f . g e t v a l u e ( ) ) )

fmt )

# Use NumPy t o c r e a t e a 2D a r r a y o f complex numbers

Y, X = np . mgrid [ − 1 . 3 : 1 . 3 : 0 . 0 0 5 , − 2 : 1 : 0 . 0 0 5 ]
Z = X+1 j ∗Y

xs = t f . c o n s t a n t (Z . a s t y p e ( np . complex64 ) )
z s = t f . V a r i a b l e ( xs )
ns = t f . V a r i a b l e ( t f . z e r o s _ l i k e ( xs ,

t f . f l o a t 3 2 ) )

# O p e r a t i o n t o u p d a t e t h e z s and t h e i t e r a t i o n count .
#
# Note : We k e e p computing z s a f t e r
#

i s v e r y w a s t e f u l ! There a r e b e t t e r ,

t h e y d i v e r g e ! This

i f a l i t t l e

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

95

#
#
for i

l e s s

s i m p l e , ways t o do t h i s .

in range ( 2 0 0 ) :

# Compute t h e new v a l u e s o f
zs_ = z s ∗ z s + xs

z :

z ^2 + x

# Have we d i v e r g e d w i t h t h i s new v a l u e ?
n ot _ di ve r ge d = t f . abs ( zs_ ) < 4

z s . a s s i g n ( zs_ ) ,
ns . assign_add ( t f . c a s t ( not_diverged ,

t f . f l o a t 3 2 ) )

D i s p l a y F r a c t a l ( ns . numpy ( ) )

Output

Mandlebrot rendering programs are both simple and inﬁnitely complex at the same time. This view
shows the entire Mandlebrot universe simultaneously, as a view completely zoomed out. However, if you
zoom in on any non-black portion of the plot, you will ﬁnd inﬁnite hidden complexity.

96

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

3.2.6 Introduction to Keras

Keras is a layer on top of Tensorﬂow that makes it much easier to create neural networks. Rather than
deﬁne the graphs, as you see above, you set the individual layers of the network with a much more high-level
API. Unless you are researching entirely new structures of deep neural networks, it is unlikely that you
need to program TensorFlow directly.

For this class, we will usually use TensorFlow through Keras, rather than direct Tensor-

Flow

3.2.7 Simple TensorFlow Regression: MPG

This example shows how to encode the MPG dataset for regression. This dataset is slightly more compli-
cated than Iris because:

• Input has both numeric and categorical
• Input has missing values

This example uses functions deﬁned above in this notepad, the "helpful functions". These functions allow
you to build the feature vector for a neural network. Consider the following:

To encode categorical values that are part of the feature vector, use the functions from above if the
categorical value is the target (as was the case with Iris, use the same technique as Iris). The iris technique
allows you to decode back to Iris text strings from the predictions.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

97

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =2, ep o c hs =100)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

Output

. . .
13/13 − 0 s − l o s s : 1 3 9 . 3 4 3 5
Epoch 100/100
13/13 − 0 s − l o s s : 1 3 5 . 2 2 1 7

3.2.8 Introduction to Neural Network Hyperparameters

If you look at the above code, you will see that the neural network contains four layers. The ﬁrst layer is
the input layer because it contains the input_dim parameter that the programmer sets to be the number
of inputs the dataset has. The network needs one input neuron for every column in the data set (including
dummy variables).

There are also several hidden layers, with 25 and 10 neurons each. You might be wondering how the
programmer chose these numbers. Selecting a hidden neuron structure is one of the most common questions
about neural networks. Unfortunately, there is no right answer. These are hyperparameters. They are
settings that can aﬀect neural network performance, yet there are no clearly deﬁned means of setting them.
In general, more hidden neurons mean more capability to ﬁt complex problems. However, too many
neurons can lead to overﬁtting and lengthy training times. Too few can lead to underﬁtting the problem
and will sacriﬁce accuracy. Also, how many layers you have is another hyperparameter. In general, more
layers allow the neural network to perform more of its feature engineering and data preprocessing. But
this also comes at the expense of training times and the risk of overﬁtting. In general, you will see that
neuron counts start larger near the input layer and tend to shrink towards the output layer in a triangular
fashion.

Some techniques use machine learning to optimize these values. These will be discussed in Module 8.3.

3.2.9 Controlling the Amount of Output

The program produces one line of output for each training epoch. You can eliminate this output by setting
the verbose setting of the ﬁt command:

• verbose=0 - No progress output (use with Jupyter if you do not want output).

98

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

• verbose=1 - Display progress bar, does not work well with Jupyter.
• verbose=2 - Summary progress output (use with Jupyter if you want to know the loss at each

epoch).

3.2.10 Regression Prediction

Next, we will perform actual predictions. The program assigns these predictions to the pred variable.
These are all MPG predictions from the neural network. Notice that this is a 2D array? You can always
see the dimensions of what Keras returns by printing out pred.shape. Neural networks can return multiple
values, so the result is always an array. Here the neural network only returns one value per prediction
(there are 398 cars, so 398 predictions). However, a 2D range is needed because the neural network has
the potential of returning more than one value.

Code

pred = model . p r e d i c t ( x )
print ( f " Shape : ␣{ pred . shape } " )
print ( pred [ 0 : 1 0 ] )

Output

( 3 9 8 , 1 )

Shape :
[ [ 2 2 . 5 3 9 4 2 5 ]
[ 2 7 . 9 9 5 2 0 3 ]
[ 2 5 . 8 5 1 4 3 3 ]
[ 2 5 . 7 1 1 1 1 7 ]
[ 2 3 . 7 0 1 8 4 7 ]
[ 3 1 . 8 9 3 7 5 5 ]
[ 3 5 . 5 5 6 5 0 3 ]
[ 3 4 . 4 5 2 4 3 ]
[ 3 6 . 2 7 0 1 4 ]
[ 3 1 . 3 5 8 7 7 6 ] ]

We would like to see how good these predictions are. We know the correct MPG for each car so we can

measure how close the neural network was.

Code

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

Output

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

99

F i n a l

s c o r e (RMSE) : 1 1 . 5 5 2 9 0 7 3 6 5 1 9 5 1 3 4

The number printed above is the average number of predictions above or below the expected output.

We can also print out the ﬁrst ten cars with predictions and actual MPG.

Code

# Sample p r e d i c t i o n s
for i

in range ( 1 0 ) :

print ( f " { i +1}. ␣Car␣name : ␣{ c a r s [ i ] } , ␣MPG: ␣{y [ i ] } , ␣ "

+ f " p r e d i c t e d ␣MPG: ␣{ pred [ i ] } " )

Output

c h e v e l l e malibu , MPG: 1 8 . 0 , p r e d i c t e d MPG:

c h e v r o l e t

1 . Car name :
[ 2 2 . 5 3 9 4 2 5 ]
2 . Car name : b u i c k s k y l a r k 3 2 0 , MPG: 1 5 . 0 , p r e d i c t e d MPG:
3 . Car name : plymouth s a t e l l i t e , MPG: 1 8 . 0 , p r e d i c t e d MPG:
4 . Car name : amc r e b e l
5 . Car name :
6 . Car name :
7 . Car name :
8 . Car name : plymouth f u r y i i i , MPG: 1 4 . 0 , p r e d i c t e d MPG:
9 . Car name : p o n t i a c c a t a l i n a , MPG: 1 4 . 0 , p r e d i c t e d MPG:
1 0 . Car name : amc ambassador dpl , MPG: 1 5 . 0 , p r e d i c t e d MPG:
[ 3 1 . 3 5 8 7 7 6 ]

f o r d t o r i n o , MPG: 1 7 . 0 , p r e d i c t e d MPG:
f o r d g a l a x i e 5 0 0 , MPG: 1 5 . 0 , p r e d i c t e d MPG:
impala , MPG: 1 4 . 0 , p r e d i c t e d MPG:
c h e v r o l e t

s s t , MPG: 1 6 . 0 , p r e d i c t e d MPG:

[ 2 7 . 9 9 5 2 0 3 ]

[ 2 5 . 8 5 1 4 3 3 ]

[ 2 5 . 7 1 1 1 1 7 ]

[ 2 3 . 7 0 1 8 4 7 ]

[ 3 1 . 8 9 3 7 5 5 ]
[ 3 5 . 5 5 6 5 0 3 ]
[ 3 4 . 4 5 2 4 3 ]

[ 3 6 . 2 7 0 1 4 ]

3.2.11 Simple TensorFlow Classiﬁcation: Iris

Classiﬁcation is how a neural network attempts to classify the input into one or more classes. The sim-
plest way of evaluating a classiﬁcation network is to track the percentage of training set items classiﬁed
incorrectly. We typically score human results in this manner. For example, you might have taken multiple-
choice exams in school in which you had to shade in a bubble for choices A, B, C, or D. If you chose the
wrong letter on a 10-question exam, you would earn a 90%. In the same way, we can grade computers;
however, most classiﬁcation algorithms do not merely choose A, B, C, or D. Computers typically report
a classiﬁcation as their percent conﬁdence in each class. Figure 3.16 shows how a computer and a human
might respond to question number 1 on an exam.

As you can see, the human test taker marked the ﬁrst question as "B." However, the computer test
taker had an 80% (0.8) conﬁdence in "B" and was also somewhat sure with 10% (0.1) on "A." The computer
then distributed the remaining points to the other two. In the simplest sense, the machine would get 80%
of the score for this question if the correct answer were "B." The computer would get only 5% (0.05) of the
points if the correct answer were "D."

100

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.16: Classiﬁcation Neural Network Output

We just saw a straightforward example of how to perform the Iris classiﬁcation using TensorFlow. The

iris.csv ﬁle is used rather than using the built-in data that many Google examples require.

Make sure that you always run previous code blocks. If you run the code block below,

without the code block above, you will get errors

Code

import pandas a s pd
import i o
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x = d f [ [ ' s e p a l _ l ' ,
dummies = pd . get_dummies ( d f [ ' s p e c i e s ' ] ) # C l a s s i f i c a t i o n
s p e c i e s = dummies . columns
y = dummies . v a l u e s

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

# B u i l d n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =2, ep o c hs =100)

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

101

Output

. . .
5/5 − 0 s − l o s s : 0 . 0 8 5 1
Epoch 100/100
5/5 − 0 s − l o s s : 0 . 0 8 8 0

Code

# P r i n t o u t number o f

s p e c i e s

found :

print ( s p e c i e s )

Output

Index ( [ ' I r i s −s e t o s a ' ,
dtype =' o b j e c t ' )

' I r i s −v e r s i c o l o r ' ,

' I r i s −v i r g i n i c a ' ] ,

Now that you have a neural network trained, we would like to be able to use it. The following code
makes use of our neural network. Exactly like before, we will generate predictions. Notice that three values
come back for each of the 150 iris ﬂowers. There were three types of iris (Iris-setosa, Iris-versicolor, and
Iris-virginica).

Code

pred = model . p r e d i c t ( x )
print ( f " Shape : ␣{ pred . shape } " )
print ( pred [ 0 : 1 0 ] )

Output

( 1 5 0 , 3 )

Shape :
[ [ 9 . 9 7 6 8 4 1 2 e −01 2 . 3 0 8 7 7 6 6 e −03 7 . 1 4 7 4 5 6 0 e −06]
[ 9 . 9 3 4 9 6 6 6 e −01 6 . 4 7 6 3 0 1 7 e −03 2 . 6 9 9 5 1 0 5 e −05]
[ 9 . 9 6 1 8 2 9 8 e −01 3 . 7 9 9 1 4 5 6 e −03 1 . 7 7 9 0 3 6 6 e −05]
[ 9 . 9 2 0 7 5 3 2 e −01 7 . 8 8 8 2 5 9 4 e −03 3 . 6 4 5 3 8 9 7 e −05]
[ 9 . 9 7 9 1 3 1 8 e −01 2 . 0 8 0 0 2 2 8 e −03 6 . 7 6 0 2 9 4 1 e −06]
[ 9 . 9 6 8 4 9 9 5 e −01 3 . 1 4 4 2 6 1 4 e −03 5 . 8 1 1 2 0 0 0 e −06]
[ 9 . 9 5 4 7 1 3 6 e −01 4 . 5 0 8 6 8 8 1 e −03 1 . 9 9 4 6 1 0 3 e −05]
[ 9 . 9 6 2 5 9 2 1 e −01 3 . 7 2 8 8 4 9 3 e −03 1 . 2 0 4 0 5 0 6 e −05]
[ 9 . 9 0 1 1 1 8 9 e −01 9 . 8 2 9 6 8 5 1 e −03 5 . 8 4 3 4 5 3 6 e −05]

102

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

[ 9 . 9 4 4 7 2 0 3 e −01 5 . 5 0 6 7 8 8 4 e −03 2 . 1 2 7 2 4 2 1 e − 0 5 ] ]

If you would like to turn of scientiﬁc notation, the following line can be used:

Code

np . s e t _ p r i n t o p t i o n s ( s u p p r e s s=True )

Now we see these values rounded up.

Code

print ( y [ 0 : 1 0 ] )

Output

[ [ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ]
[ 1 0 0 ] ]

Usually, the program considers the column with the highest prediction to be the prediction of the neural
network. It is easy to convert the predictions to the expected iris species. The argmax function ﬁnds the
index of the maximum prediction for each row.

Code

p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = np . argmax ( y , a x i s =1)
print ( f " P r e d i c t i o n s : ␣{ p r e d i c t _ c l a s s e s } " )
print ( f " Expected : ␣{ e x p e c t e d _ c l a s s e s } " )

Output

P r e d i c t i o n s :
0 0 0 0 0 0 0 0 0

[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1

3.2. PART 3.2: INTRODUCTION TO TENSORFLOW AND KERAS

103

2 1

1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2

2 2

2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2

2 2

2 2 ]

Expected :
0 0 0 0 0 0 0

[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

1 1

1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2

2 2

2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2

2 2

2 2 ]

Of course, it is straightforward to turn these indexes back into iris species. We use the species list that

we created earlier.

Code

print ( s p e c i e s [ p r e d i c t _ c l a s s e s [ 1 : 1 0 ] ] )

Output

Index ( [ ' I r i s −s e t o s a ' ,
' I r i s −s e t o s a ' ,
' I r i s −s e t o s a ' ] ,

dtype =' o b j e c t ' )

' I r i s −s e t o s a ' ,
' I r i s −s e t o s a ' ,

' I r i s −s e t o s a ' ,
' I r i s −s e t o s a ' ,

' I r i s −s e t o s a ' ,
' I r i s −s e t o s a ' ,

Accuracy might be a more easily understood error metric. It is essentially a test score. For all of the
iris predictions, what percent were correct? The downside is it does not consider how conﬁdent the neural
network was in each prediction.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e

c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " Accuracy : ␣{ c o r r e c t } " )

Output

104

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Accuracy : 0 . 9 7 3 3 3 3 3 3 3 3 3 3 3 3 3 4

The code below performs two ad hoc predictions. The ﬁrst prediction is a single iris ﬂower, and the
second predicts two iris ﬂowers. Notice that the argmax in the second prediction requires axis=1? Since
we have a 2D array now, we must specify which axis to take the argmax over. The value axis=1 speciﬁes
we want the max column index for each row.

Code

sam ple_fl ower = np . a r r a y (
pred = model . p r e d i c t ( sa mple_f lower )
print ( pred )
pred = np . argmax ( pred )
print ( f " P r e d i c t ␣ t h a t ␣{ s ample _flow er }␣ i s : ␣{ s p e c i e s [ pred ] } " )

[ [ 5 . 0 , 3 . 0 , 4 . 0 , 2 . 0 ] ] , dtype=f l o a t )

Output

[ [ 0 . 0 0 0 6 5 0 0 1 0 . 1 7 2 2 2 1 8 1 0 . 8 2 7 1 2 8 2 ] ]
P r e d i c t

[ [ 5 . 3 . 4 . 2 . ] ]

t h a t

i s :

I r i s −v i r g i n i c a

You can also predict two sample ﬂowers.

Code

sam ple_fl ower = np . a r r a y (

[ [ 5 . 0 , 3 . 0 , 4 . 0 , 2 . 0 ] , [ 5 . 2 , 3 . 5 , 1 . 5 , 0 . 8 ] ] , \

dtype=f l o a t )

pred = model . p r e d i c t ( sa mple_f lower )
print ( pred )
pred = np . argmax ( pred , a x i s =1)
print ( f " P r e d i c t ␣ t h a t ␣ t h e s e ␣two␣ f l o w e r s ␣ { sa mpl e_ flower }␣ " )
print ( f " a r e : ␣{ s p e c i e s [ pred ] } " )

Output

[ [ 0 . 0 0 0 6 5 0 0 1 0 . 1 7 2 2 2 1 5 7 0 . 8 2 7 1 2 8 4 ]

[ 0 . 9 8 8 7 9 3 7

P r e d i c t

t h a t
[ 5 . 2 3 . 5 1 . 5 0 . 8 ] ]

0 . 0 1 1 1 7 7 5 1 0 . 0 0 0 0 2 8 8 6 ] ]
t h e s e two f l o w e r s

[ [ 5 .

3 .

4 .

2 .

]

a r e :

Index ( [ ' I r i s −v i r g i n i c a ' ,

' I r i s −s e t o s a ' ] , dtype =' o b j e c t ' )

3.3. PART 3.3: SAVING AND LOADING A KERAS NEURAL NETWORK

105

3.3 Part 3.3: Saving and Loading a Keras Neural Network

Complex neural networks will take a long time to ﬁt/train. It is helpful to be able to save these neural
networks so that you can reload them later. A reloaded neural network will not require retraining. Keras
provides three formats for neural network saving.

• JSON - Stores the neural network structure (no weights) in the JSON ﬁle format.
• HDF5 - Stores the complete neural network (with weights) in the HDF5 ﬁle format. Do not confuse

HDF5 with HDFS. They are diﬀerent. We do not use HDFS in this class.

Usually, you will want to save in HDF5.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

save_path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' h ors epower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

106

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

model . f i t ( x , y , v e r b o s e =2, ep o c hs =100)

# P r e d i c t
pred = model . p r e d i c t ( x )

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y ) )
print ( f " B e f o r e ␣ s a v e ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

# s a v e n e u r a l network s t r u c t u r e t o JSON ( no w e i g h t s )
model_json = model . t o_ j so n ( )
with open ( o s . path . j o i n ( save_path , " network . j s o n " ) ,

"w" ) a s

j s o n _ f i l e :

j s o n _ f i l e . w r i t e ( model_json )

# s a v e e n t i r e network t o HDF5 ( s a v e e v e r y t h i n g ,
model . s a v e ( o s . path . j o i n ( save_path , " network . h5 " ) )

s u g g e s t e d )

Output

. . .
13/13 − 0 s − l o s s : 5 0 . 2 1 1 8 − 25ms/ epoch − 2ms/ s t e p
Epoch 100/100
13/13 − 0 s − l o s s : 4 9 . 8 8 2 8 − 25ms/ epoch − 2ms/ s t e p
B e f o r e s a v e s c o r e (RMSE) : 7 . 0 4 4 4 3 1 6 9 0 3 0 0 9 0 3

The code below sets up a neural network and reads the data (for predictions), but it does not clear
the model directory or ﬁt the neural network. The code loads the weights from the previous ﬁt. Now we
reload the network and perform another prediction. The RMSE should match the previous one exactly if
we saved and reloaded the neural network correctly.

Code

from t e n s o r f l o w . k e r a s . models import load_model
model2 = load_model ( o s . path . j o i n ( save_path , " network . h5 " ) )
pred = model2 . p r e d i c t ( x )
# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y ) )
print ( f " A f t e r ␣ l o a d ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

Output

A f t e r

l o a d s c o r e (RMSE) : 7 . 0 4 4 4 3 1 6 9 0 3 0 0 9 0 3

3.4. PART 3.4: EARLY STOPPING IN KERAS TO PREVENT OVERFITTING

107

3.4 Part 3.4: Early Stopping in Keras to Prevent Overﬁtting

It can be diﬃcult to determine how many epochs to cycle through to train a neural network. Overﬁtting
will occur if you train the neural network for too many epochs, and the neural network will not perform
well on new data, despite attaining a good accuracy on the training set. Overﬁtting occurs when a neural
network is trained to the point that it begins to memorize rather than generalize, as demonstrated in Figure
3.17.

Figure 3.17: Training vs. Validation Error for Overﬁtting

It is important to segment the original dataset into several datasets:

• Training Set
• Validation Set
• Holdout Set

You can construct these sets in several diﬀerent ways. The following programs demonstrate some of these.
The ﬁrst method is a training and validation set. We use the training data to train the neural network
until the validation set no longer improves. This attempts to stop at a near-optimal training point. This
method will only give accurate "out of sample" predictions for the validation set; this is usually 20% of the
data. The predictions for the training data will be overly optimistic, as these were the data that we used
to train the neural network. Figure 3.18 demonstrates how we divide the dataset.

3.4.1 Early Stopping with Classiﬁcation

We will now see an example of classiﬁcation training with early stopping. We will train the neural network
until the error no longer improves on the validation set.

108

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Figure 3.18: Training with a Validation Set

Code

import pandas a s pd
import i o
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x = d f [ [ ' s e p a l _ l ' ,
dummies = pd . get_dummies ( d f [ ' s p e c i e s ' ] ) # C l a s s i f i c a t i o n
s p e c i e s = dummies . columns
y = dummies . v a l u e s

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

i n t o v a l i d a t i o n and t r a i n i n g s e t s

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

3.4. PART 3.4: EARLY STOPPING IN KERAS TO PREVENT OVERFITTING

109

model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5,

v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )
model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,
c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p o c h s =1000)

Output

Train on 112 samples , v a l i d a t e on 38 samples
. . .
112/112 − 0 s − l o s s : 0 . 1 0 1 7 − v a l _ l o s s : 0 . 0 9 2 6
Epoch 107/1000
R e s t o r i n g model w e i g h t s from t h e end o f
112/112 − 0 s − l o s s : 0 . 1 0 0 1 − v a l _ l o s s : 0 . 0 8 6 9
Epoch 0 0 1 0 7 : e a r l y s t o p p i n g

t h e b e s t epoch .

There are a number of parameters that are speciﬁed to the EarlyStopping object.

• min_delta This value should be kept small. It simply means the minimum change in error to be
registered as an improvement. Setting it even smaller will not likely have a great deal of impact.

• patience How long should the training wait for the validation error to improve?
• verbose How much progress information do you want?
• mode In general, always set this to "auto". This allows you to specify if the error should be minimized
or maximized. Consider accuracy, where higher numbers are desired vs log-loss/RMSE where lower
numbers are desired.

• restore_best_weights This should always be set to true. This restores the weights to the values
they were at when the validation set is the highest. Unless you are manually tracking the weights
yourself (we do not use this technique in this course), you should have Keras perform this step for
you.

As you can see from above, the entire number of requested epochs were not used. The neural network
training stopped once the validation set no longer improved.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e

pred = model . p r e d i c t ( x _t e s t )
p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = np . argmax ( y_test , a x i s =1)
c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " Accuracy : ␣ { c o r r e c t } " )

110

Output

Accuracy : 1 . 0

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

3.4.2 Early Stopping with Regression

The following code demonstrates how we can apply early stopping to a regression problem. The technique
is similar to the early stopping for classiﬁcation code that we just saw.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' ho rse p ow e r ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

i n t o v a l i d a t i o n and t r a i n i n g s e t s

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

3.5. PART 3.5: EXTRACTING WEIGHTS AND MANUAL NETWORK CALCULATION

111

model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,
c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p o c h s =1000)

Output

Train on 298 samples , v a l i d a t e on 100 samples
. . .
298/298 − 0 s − l o s s : 3 4 . 0 5 9 1 − v a l _ l o s s : 2 9 . 3 0 4 4
Epoch 317/1000
R e s t o r i n g model w e i g h t s from t h e end o f
298/298 − 0 s − l o s s : 3 2 . 9 7 6 4 − v a l _ l o s s : 2 9 . 1 0 7 1
Epoch 0 0 3 1 7 : e a r l y s t o p p i n g

t h e b e s t epoch .

Finally, we evaluate the error.

Code

# Measure RMSE e r r o r . RMSE i s common f o r
pred = model . p r e d i c t ( x _t e s t )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣ { s c o r e } " )

r e g r e s s i o n .

Output

F i n a l

s c o r e (RMSE) : 5 . 2 9 1 2 1 9 3 0 0 7 9 9 3 9 8

3.5 Part 3.5: Extracting Weights and Manual Network Calcula-

tion

3.5.1 Weight Initialization

The weights of a neural network determine the output for the neural network. The training process
can adjust these weights, so the neural network produces useful output. Most neural network training

112

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

algorithms begin by initializing the weights to a random state. Training then progresses through iterations
that continuously improve the weights to produce better output.

The random weights of a neural network impact how well that neural network can be trained. If a
neural network fails to train, you can remedy the problem by simply restarting with a new set of random
weights. However, this solution can be frustrating when you are experimenting with the architecture of a
neural network and trying diﬀerent combinations of hidden layers and neurons. If you add a new layer,
and the network’s performance improves, you must ask yourself if this improvement resulted from the new
layer or from a new set of weights. Because of this uncertainty, we look for two key attributes in a weight
initialization algorithm:

• How consistently does this algorithm provide good weights?
• How much of an advantage do the weights of the algorithm provide?

One of the most common yet least practical approaches to weight initialization is to set the weights to
random values within a speciﬁc range. Numbers between -1 and +1 or -5 and +5 are often the choice. If
you want to ensure that you get the same set of random weights each time, you should use a seed. The seed
speciﬁes a set of predeﬁned random weights to use. For example, a seed of 1000 might produce random
weights of 0.5, 0.75, and 0.2. These values are still random; you cannot predict them, yet you will always
get these values when you choose a seed of 1000.
Not all seeds are created equal. One problem with random weight initialization is that the random weights
created by some seeds are much more diﬃcult to train than others. The weights can be so bad that training
is impossible. If you cannot train a neural network with a particular weight set, you should generate a new
set of weights using a diﬀerent seed.

Because weight initialization is a problem, considerable research has been around it. By default, Keras
uses the Xavier weight initialization algorithm, introduced in 2006 by Glorot Bengio[7], produces good
weights with reasonable consistency. This relatively simple algorithm uses normally distributed random
numbers.

To use the Xavier weight initialization, it is necessary to understand that normally distributed random
numbers are not the typical random numbers between 0 and 1 that most programming languages generate.
Normally distributed random numbers are centered on a mean (µ, mu) that is typically 0. If 0 is the center
(mean), then you will get an equal number of random numbers above and below 0. The next question
is how far these random numbers will venture from 0. In theory, you could end up with both positive
and negative numbers close to the maximum positive and negative ranges supported by your computer.
However, the reality is that you will more likely see random numbers that are between 0 and three standard
deviations from the center.

The standard deviation (σ, sigma) parameter speciﬁes the size of this standard deviation. For example,
if you speciﬁed a standard deviation of 10, you would mainly see random numbers between -30 and +30,
and the numbers nearer to 0 have a much higher probability of being selected.

The above ﬁgure illustrates that the center, which in this case is 0, will be generated with a 0.4 (40%)
probability. Additionally, the probability decreases very quickly beyond -2 or +2 standard deviations. By
deﬁning the center and how large the standard deviations are, you can control the range of random numbers
that you will receive.

The Xavier weight initialization sets all weights to normally distributed random numbers. These weights
are always centered at 0; however, their standard deviation varies depending on how many connections are
present for the current layer of weights. Speciﬁcally, Equation 4.2 can determine the standard deviation:

3.5. PART 3.5: EXTRACTING WEIGHTS AND MANUAL NETWORK CALCULATION

113

V ar(W ) =

2
nin + nout

The above equation shows how to obtain the variance for all weights. The square root of the variance
is the standard deviation. Most random number generators accept a standard deviation rather than a
variance. As a result, you usually need to take the square root of the above equation. Figure 3.19 shows
how this algorithm might initialize one layer.

Figure 3.19: Xavier Weight Initialization

We complete this process for each layer in the neural network.

3.5.2 Manual Neural Network Calculation

This section will build a neural network and analyze it down the individual weights. We will train a simple
neural network that learns the XOR function. It is not hard to hand-code the neurons to provide an XOR
function; however, we will allow Keras for simplicity to train this network for us. The neural network is
small, with two inputs, two hidden neurons, and a single output. We will use 100K epochs on the ADAM
optimizer. This approach is overkill, but it gets the result, and our focus here is not on tuning.

114

Code

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
import numpy a s np

f o r

t h e XOR f u n c t i o n

# Cr e a t e a d a t a s e t
x = np . a r r a y ( [
[ 0 , 0 ] ,
[ 1 , 0 ] ,
[ 0 , 1 ] ,
[ 1 , 1 ]

] )

y = np . a r r a y ( [

0 ,
1 ,
1 ,
0

] )

# B u i l d t h e network
# s g d = o p t i m i z e r s .SGD( l r =0.01 , decay=1e −6, momentum=0.9 , n e s t e r o v=True )

done = F a l s e
c y c l e = 1

while not done :

print ( " Cycle ␣#{} " . format ( c y c l e ) )
c y c l e+=1
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 ,
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =0, ep o c hs =10000)

input_dim =2, a c t i v a t i o n= ' r e l u ' ) )

# P r e d i c t
pred = model . p r e d i c t ( x )

s u c c e s s f u l .

# Check i f
# s m a l l o f a network
done = pred [ 0 ] < 0 . 0 1 and pred [ 3 ] < 0 . 0 1 and pred [ 1 ] > 0 . 9 \

runs w i t h t h i s

s e v e r a l

t a k e s

I t

and pred [ 2 ] > 0 . 9

3.5. PART 3.5: EXTRACTING WEIGHTS AND MANUAL NETWORK CALCULATION

115

print ( pred )

Output

Cycle #1
[ [ 0 . 4 9 9 9 9 9 9 7 ]
[ 0 . 4 9 9 9 9 9 9 7 ]
[ 0 . 4 9 9 9 9 9 9 7 ]
[ 0 . 4 9 9 9 9 9 9 7 ] ]

Cycle #2
[ [ 0 . 3 3 3 3 3 3 3 4 ]
[ 1 .
]
[ 0 . 3 3 3 3 3 3 3 4 ]
[ 0 . 3 3 3 3 3 3 3 4 ] ]

Cycle #3
[ [ 0 . 3 3 3 3 3 3 3 4 ]
[ 1 .
]
[ 0 . 3 3 3 3 3 3 3 4 ]
[ 0 . 3 3 3 3 3 3 3 4 ] ]

Cycle #4
[ [ 0 . ]
[ 1 . ]
[ 1 . ]
[ 0 . ] ]

Code

pred [ 3 ]

Output

a r r a y ( [ 0 . ] , dtype=f l o a t 3 2 )

The output above should have two numbers near 0.0 for the ﬁrst and fourth spots (input [0,0] and
[1,1]). The middle two numbers should be near 1.0 (input [1,0] and [0,1]). These numbers are in scientiﬁc
notation. Due to random starting weights, it is sometimes necessary to run the above through several
cycles to get a good result.

Now that we’ve trained the neural network, we can dump the weights.

116

Code

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

# Dump w e i g h t s
for layerNum ,

l a y e r

in enumerate ( model . l a y e r s ) :

w e i g h t s = l a y e r . g e t _ w e i g h t s ( ) [ 0 ]
b i a s e s = l a y e r . g e t _ w e i g h t s ( ) [ 1 ]

for toNeuronNum , b i a s

in enumerate ( b i a s e s ) :

print ( f ' { layerNum }B␣−>␣L{ layerNum+1}N{toNeuronNum } : ␣{ b i a s } ' )

for fromNeuronNum , wgt in enumerate ( w e i g h t s ) :

for toNeuronNum , wgt2 in enumerate ( wgt ) :

print ( f 'L{ layerNum }N{fromNeuronNum} ␣ \

␣␣ ␣ ␣␣␣␣␣␣␣␣␣␣␣␣␣␣ ␣−>␣L{ layerNum+1}N{toNeuronNum}␣=␣ { wgt2 } ' )

Output

0B −> L1N0 : 1 . 3 0 2 5 7 6 0 9 1 4 3 3 1 3 8 6 e −08
0B −> L1N1 : −1.4192625741316078 e −08
L0N0
L0N0
L0N1
L0N1
1B −> L2N0 : −1.9757269598130733 e −08
L1N0
L1N1

−> L1N0 = 0 . 6 5 9 2 8 9 5 3 8 8 6 0 3 2 1
−> L1N1 = −0.9533336758613586
−> L1N0 = −0.659289538860321
−> L1N1 = 0 . 9 5 3 3 3 3 6 7 5 8 6 1 3 5 8 6

−> L2N0 = 1 . 5 1 6 7 8 4 3 1 0 3 4 0 8 8 1 3
−> L2N0 = 1 . 0 4 8 9 5 0 6 7 2 1 4 9 6 5 8 2

If you rerun this, you probably get diﬀerent weights. There are many ways to solve the XOR function.
In the next section, we copy/paste the weights from above and recreate the calculations done by the
neural network. Because weights can change with each training, the weights used for the below code came
from this:

0B −> L1N0 : −1.2913415431976318
0B −> L1N1 : −3.021530048386012 e −08
L0N0 −> L1N0 = 1 . 2 9 1 3 4 1 6 6 2 4 0 6 9 2 1 4
L0N0 −> L1N1 = 1 . 1 9 1 2 6 9 9 9 3 7 8 2 0 4 3 5
L0N1 −> L1N0 = 1 . 2 9 1 3 4 1 1 8 5 5 6 9 7 6 3 2
L0N1 −> L1N1 = 1 . 1 9 1 2 6 9 7 5 5 3 6 3 4 6 4 4
1B −> L2N0 : 7 . 6 2 6 2 4 1 2 9 7 5 8 7 0 3 4 e −36
L1N0 −> L2N0 = −1.548777461051941
L1N1 −> L2N0 = 0 . 8 3 9 4 4 0 4 6 4 9 7 3 4 4 9 7

3.5. PART 3.5: EXTRACTING WEIGHTS AND MANUAL NETWORK CALCULATION

117

Code

i n p u t 0 = 0
i n p u t 1 = 1

hidden0Sum = ( i n p u t 0 ∗ 1 . 3 ) + ( i n p u t 1 ∗ 1 . 3 ) + ( − 1 . 3 )
hidden1Sum = ( i n p u t 0 ∗ 1 . 2 ) + ( i n p u t 1 ∗ 1 . 2 ) + ( 0 )

print ( hidden0Sum ) # 0
print ( hidden1Sum ) # 1 . 2

hidden0 = max( 0 , hidden0Sum )
hidden1 = max( 0 , hidden1Sum )

print ( hidden0 ) # 0
print ( hidden1 ) # 1 . 2

outputSum = ( hidden0 ∗ −1.6)+( hidden1 ∗ 0 . 8 ) + ( 0 )
print ( outputSum ) # 0 . 9 6

output = max( 0 , outputSum )

print ( output ) # 0 . 9 6

Output

0 . 0
1 . 2
0
1 . 2
0 . 9 6
0 . 9 6

118

CHAPTER 3.

INTRODUCTION TO TENSORFLOW

Chapter 4

Training for Tabular Data

4.1 Part 4.1: Encoding a Feature Vector for Keras Deep Learning

Neural networks can accept many types of data. We will begin with tabular data, where there are well-
deﬁned rows and columns. This data is what you would typically see in Microsoft Excel. Neural networks
require numeric input. This numeric form is called a feature vector. Each input neurons receive one feature
(or column) from this vector. Each row of training data typically becomes one vector. This section will
see how to encode the following tabular data into a feature vector. You can see an example of tabular data
below.

Code

import pandas a s pd

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 9 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d i s p l a y ( d f )

Output

119

120

CHAPTER 4. TRAINING FOR TABULAR DATA

id
1
2
...
1999
2000

job
vv
kd
...
qp
pe

area
c
c
...
c
c

income
50876.0
60369.0
...
67949.0
61467.0

...
...
...
...
...
...

pop_dense
0.885827
0.874016
...
0.909449
0.925197

retail_dense
0.492126
0.342520
...
0.598425
0.539370

crime
0.071100
0.400809
...
0.117803
0.451973

product
b
c
...
c
c

0
1
...
1998
1999

You can make the following observations from the above data:

• The target column is the column that you seek to predict. There are several candidates here. However,

we will initially use the column "product". This ﬁeld speciﬁes what product someone bought.

• There is an ID column. You should exclude his column because it contains no information useful for

prediction.

• Many of these ﬁelds are numeric and might not require further processing.
• The income column does have some missing values.
• There are categorical values: job, area, and product.

To begin with, we will convert the job code into dummy variables.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

dummies = pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " )
print ( dummies . shape )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 9 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( dummies )

Output

4.1. PART 4.1: ENCODING A FEATURE VECTOR FOR KERAS DEEP LEARNING

121

job_11
0
0
0
1
0
...
0
0
0
0
0

job_al
0
0
0
0
0
...
0
0
0
0
0

job_am job_ax
0
0
0
0
0
...
0
0
0
0
0

0
0
0
0
0
...
0
0
0
0
0

...
...
...
...
...
...
...
...
...
...
...
...

job_rn
0
0
0
0
0
...
0
0
0
0
0

job_sa
0
0
0
0
0
...
0
0
0
0
0

job_vv
1
0
0
0
0
...
1
0
0
0
0

job_zz
0
0
0
0
0
...
0
0
0
0
0

0
1
2
3
4
...
1995
1996
1997
1998
1999

( 2 0 0 0 , 3 3 )

Because there are 33 diﬀerent job codes, there are 33 dummy variables. We also speciﬁed a preﬁx
because the job codes (such as "ax") are not that meaningful by themselves. Something such as "job_ax"
also tells us the origin of this ﬁeld.

Next, we must merge these dummies back into the main data frame. We also drop the original "job"

ﬁeld, as the dummies now represent it.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f = pd . c o n c a t ( [ df , dummies ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 9 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )

d i s p l a y ( d f )

Output

122

CHAPTER 4. TRAINING FOR TABULAR DATA

id
1
2
3
4
5
...
1996
1997
1998
1999
2000

area
c
c
c
c
d
...
c
d
d
c
c

income
50876.0
60369.0
55126.0
51690.0
28347.0
...
51017.0
26576.0
28595.0
67949.0
61467.0

aspect
13.100000
18.625000
34.766667
15.808333
40.941667
...
38.233333
33.358333
39.425000
5.733333
16.891667

...
...
...
...
...
...
...
...
...
...
...
...

job_rn
0
0
0
0
0
...
0
0
0
0
0

job_sa
0
0
0
0
0
...
0
0
0
0
0

job_vv
1
0
0
0
0
...
1
0
0
0
0

job_zz
0
0
0
0
0
...
0
0
0
0
0

0
1
2
3
4
...
1995
1996
1997
1998
1999

We also introduce dummy variables for the area column.

Code

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 9 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 0 )
d i s p l a y ( d f )

Output

id
1
2
3
4
5
...
1996
1997
1998
1999
2000

income
50876.0
60369.0
55126.0
51690.0
28347.0
...
51017.0
26576.0
28595.0
67949.0
61467.0

aspect
13.100000
18.625000
34.766667
15.808333
40.941667
...
38.233333
33.358333
39.425000
5.733333
16.891667

subscriptions
1
2
1
1
3
...
1
2
3
0
0

...
...
...
...
...
...
...
...
...
...
...
...

area_a
0
0
0
0
0
...
0
0
0
0
0

area_b
0
0
0
0
0
...
0
0
0
0
0

area_c
1
1
1
1
0
...
1
0
0
1
1

area_d
0
0
0
0
1
...
0
1
1
0
0

0
1
2
3
4
...
1995
1996
1997
1998
1999

The last remaining transformation is to ﬁll in missing income values.

4.1. PART 4.1: ENCODING A FEATURE VECTOR FOR KERAS DEEP LEARNING

123

Code

med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

There are more advanced ways of ﬁlling in missing values, but they require more analysis. The idea
would be to see if another ﬁeld might hint at what the income was. For example, it might be beneﬁcial to
calculate a median income for each area or job category. This technique is something to keep in mind for
the class Kaggle competition.

At this point, the Pandas dataframe is ready to be converted to Numpy for neural network training.
We need to know a list of the columns that will make up x (the predictors or inputs) and y (the target).

The complete list of columns is:

Code

print ( l i s t ( d f . columns ) )

Output

' a s p e c t ' ,
' d i s t _ u n h e a l t h y ' ,

' income ' ,

[ ' id ' ,
' save_rate ' ,
' crime ' ,
' job_by ' ,
' job_gv ' ,
'job_mm ' ,
' job_pz ' ,
' area_a ' ,

' product ' ,
' job_cv ' ,
' job_kd ' ,
' job_nb ' ,
' job_qp ' ,
' area_b ' ,

' s u b s c r i p t i o n s ' ,

' d i s t _ h e a l t h y ' ,

' job_11 ' ,
' job_de ' ,
' job_ke ' ,
' job_nn ' ,
' job_qw ' ,
' area_c ' ,

' age ' ,
' job_al ' ,
' job_dz ' ,
' job_kl ' ,
' job_ob ' ,
' job_rn ' ,
' area_d ' ]

' pop_dense ' ,
' job_am ' ,
' job_e2 ' ,
' job_kp ' ,
' job_pe ' ,
' job_sa ' ,

' r e t a i l _ d e n s e ' ,

' job_ax ' ,
' job_f8 ' ,
' job_ks ' ,
' job_po ' ,
' job_vv ' ,

' job_bf ' ,
' job_gj ' ,
' job_kw ' ,
' job_pq ' ,
' job_zz ' ,

This data includes both the target and predictors. We need a list with the target removed. We also

remove id because it is not useful for prediction.

Code

x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
print ( l i s t ( x_columns ) )

Output

' a s p e c t ' ,

[ ' income ' ,
' d i s t _ u n h e a l t h y ' ,
' job_11 ' ,
' job_de ' ,
' job_ke ' ,

' job_al ' ,
' job_dz ' ,
' job_kl ' ,

' s u b s c r i p t i o n s ' ,
' pop_dense ' ,

' age ' ,

' d i s t _ h e a l t h y ' ,
' r e t a i l _ d e n s e ' ,

' save_rate ' ,
' crime ' ,

' job_am ' ,
' job_e2 ' ,
' job_kp ' ,

' job_ax ' ,
' job_f8 ' ,
' job_ks ' ,

' job_bf ' ,
' job_gj ' ,
' job_kw ' ,

' job_by ' ,
' job_gv ' ,
'job_mm ' ,

' job_cv ' ,
' job_kd ' ,
' job_nb ' ,

124

CHAPTER 4. TRAINING FOR TABULAR DATA

' job_nn ' ,
' job_qw ' ,
' area_c ' ,

' job_ob ' ,
' job_rn ' ,
' area_d ' ]

' job_pe ' ,
' job_sa ' ,

' job_po ' ,
' job_vv ' ,

' job_pq ' ,
' job_zz ' ,

' job_pz ' ,
' area_a ' ,

' job_qp ' ,
' area_b ' ,

4.1.1 Generate X and Y for a Classiﬁcation Neural Network

We can now generate x and y. Note that this is how we generate y for a classiﬁcation problem. Regression
would not use dummies and would encode the numeric value of the target.

Code

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

We can display the x and y matrices.

Code

print ( x )
print ( y )

Output

[ [ 5 . 0 8 7 6 0 0 0 0 e+04 1 . 3 1 0 0 0 0 0 0 e+01 1 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

1 . 0 0 0 0 0 0 0 0 e+00 0 . 0 0 0 0 0 0 0 0 e +00]

[ 6 . 0 3 6 9 0 0 0 0 e+04 1 . 8 6 2 5 0 0 0 0 e+01 2 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

1 . 0 0 0 0 0 0 0 0 e+00 0 . 0 0 0 0 0 0 0 0 e +00]

[ 5 . 5 1 2 6 0 0 0 0 e+04 3 . 4 7 6 6 6 6 6 7 e+01 1 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

1 . 0 0 0 0 0 0 0 0 e+00 0 . 0 0 0 0 0 0 0 0 e +00]
. . .
[ 2 . 8 5 9 5 0 0 0 0 e+04 3 . 9 4 2 5 0 0 0 0 e+01 3 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

0 . 0 0 0 0 0 0 0 0 e+00 1 . 0 0 0 0 0 0 0 0 e +00]

[ 6 . 7 9 4 9 0 0 0 0 e+04 5 . 7 3 3 3 3 3 3 3 e+00 0 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

1 . 0 0 0 0 0 0 0 0 e+00 0 . 0 0 0 0 0 0 0 0 e +00]

[ 6 . 1 4 6 7 0 0 0 0 e+04 1 . 6 8 9 1 6 6 6 7 e+01 0 . 0 0 0 0 0 0 0 0 e+00 . . . 0 . 0 0 0 0 0 0 0 0 e+00

1 . 0 0 0 0 0 0 0 0 e+00 0 . 0 0 0 0 0 0 0 0 e + 0 0 ] ]

[ [ 0 1 0 . . . 0 0 0 ]
[ 0 0 1 . . . 0 0 0 ]

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

125

[ 0 1 0 . . . 0 0 0 ]
. . .
[ 0 0 0 . . . 0 1 0 ]
[ 0 0 1 . . . 0 0 0 ]
[ 0 0 1 . . . 0 0 0 ] ]

The x and y values are now ready for a neural network. Make sure that you construct the neural

network for a classiﬁcation problem. Speciﬁcally,

• Classiﬁcation neural networks have an output neuron count equal to the number of classes.
• Classiﬁcation neural networks should use categorical_crossentropy and a softmax activation

function on the output layer.

4.1.2 Generate X and Y for a Regression Neural Network

The program generates the x values the say way for a regression neural network. However, y does not use
dummies. Make sure to replace income with your actual target.

Code

y = d f [ ' income ' ] . v a l u e s

4.1.3 Module 4 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 4

4.2 Part 4.2: Multiclass Classiﬁcation with ROC and AUC

The output of modern neural networks can be of many diﬀerent forms. However, classically, neural network
output has typically been one of the following:

• Binary Classiﬁcation - Classiﬁcation between two possibilities (positive and negative). Common

in medical testing, does the person has the disease (positive) or not (negative).

• Classiﬁcation - Classiﬁcation between more than 2. The iris dataset (3-way classiﬁcation).
• Regression - Numeric prediction. How many MPG does a car get? (covered in next video)

We will look at some visualizations for all three in this section.

It is important to evaluate the false positives and negatives in the results produced by a neural network.

We will now look at assessing error for both classiﬁcation and regression neural networks.

126

CHAPTER 4. TRAINING FOR TABULAR DATA

4.2.1 Binary Classiﬁcation and ROC Charts

Binary classiﬁcation occurs when a neural network must choose between two options: true/false, yes/no,
correct/incorrect, or buy/sell. To see how to use binary classiﬁcation, we will consider a classiﬁcation
system for a credit card company. This system will either "issue a credit card" or "decline a credit card."
This classiﬁcation system must decide how to respond to a new potential customer.

When you have only two classes that you can consider, the objective function’s score is the number of
false-positive predictions versus the number of false negatives. False negatives and false positives are both
types of errors, and it is essential to understand the diﬀerence. For the previous example, issuing a credit
card would be positive. A false positive occurs when a model decides to issue a credit card to someone
who will not make payments as agreed. A false negative happens when a model denies a credit card to
someone who would have made payments as agreed.

Because only two options exist, we can choose the mistake that is the more serious type of error, a
false positive or a false negative. For most banks issuing credit cards, a false positive is worse than a false
negative. Declining a potentially good credit card holder is better than accepting a credit card holder who
would cause the bank to undertake expensive collection activities.

Consider the following program that uses the wcbreast_wdbc dataset to classify if a breast tumor is

cancerous (malignant) or not (benign).

Code

import pandas a s pd

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ wcbreast_wdbc . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 5 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

d i s p l a y ( d f )

Output

diagnosis

id
842302 M
842517 M
...
...
927241 M
B
92751

0
1
...
567
568

... worst_symmetry worst_fractal_dimension
...
...
...
...
...

0.11890
0.08902
...
0.12400
0.07039

0.4601
0.2750
...
0.4087
0.2871

ROC curves can be a bit confusing. However, they are prevalent in analytics. It is essential to know
how to read them. Even their name is confusing. Do not worry about their name; the receiver operating
characteristic curve (ROC) comes from electrical engineering (EE).

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

127

Binary classiﬁcation is common in medical testing. Often you want to diagnose if someone has a disease.

This diagnosis can lead to two types of errors, known as false positives and false negatives:

• False Positive - Your test (neural network) indicated that the patient had the disease; however, the

patient did not.

• False Negative - Your test (neural network) indicated that the patient did not have the disease;

however, the patient did have the disease.

• True Positive - Your test (neural network) correctly identiﬁed that the patient had the disease.
• True Negative - Your test (neural network) correctly identiﬁed that the patient did not have the

disease.

Figure 4.1 shows you these types of errors.

Figure 4.1: Type of Error

Neural networks classify in terms of the probability of it being positive. However, at what possibility
do you give a positive result? Is the cutoﬀ 50%? 90%? Where you set, this cutoﬀ is called the threshold.
Anything above the cutoﬀ is positive; anything below is negative. Setting this cutoﬀ allows the model to
be more sensitive or speciﬁc:

More info on Sensitivity vs. Speciﬁcity: Khan Academy

Code

%m a t p l o t l i b i n l i n e
import m a t p l o t l i b . p y p l o t a s p l t
import numpy a s np
import s c i p y . s t a t s a s
import math

s t a t s

mu1 = −2
mu2 = 2
v a r i a n c e = 1
sigma = math . s q r t ( v a r i a n c e )
x1 = np . l i n s p a c e (mu1 − 5∗ sigma , mu1 + 4∗ sigma , 1 0 0 )
x2 = np . l i n s p a c e (mu2 − 5∗ sigma , mu2 + 4∗ sigma , 1 0 0 )
p l t . p l o t ( x1 ,

s t a t s . norm . pdf ( x1 , mu1 ,

sigma ) / 1 , c o l o r=" g r e e n " ,

l i n e s t y l e= ' dashed ' )

p l t . p l o t ( x2 ,
p l t . a x v l i n e ( x=−2, c o l o r=" b l a c k " )

s t a t s . norm . pdf ( x2 , mu2 ,

sigma ) / 1 , c o l o r=" r e d " )

128

CHAPTER 4. TRAINING FOR TABULAR DATA

p l t . a x v l i n e ( x=0, c o l o r=" b l a c k " )
p l t . a x v l i n e ( x=+2, c o l o r=" b l a c k " )
p l t . t e x t ( − 2 . 7 , 0 . 5 5 , " S e n s i t i v e " )
p l t . t e x t ( − 0 . 7 , 0 . 5 5 , " Balanced " )
p l t . t e x t ( 1 . 7 , 0 . 5 5 , " S p e c i f i c " )
p l t . ylim ( [ 0 , 0 . 5 3 ] )
p l t . xlim ( [ − 5 , 5 ] )
p l t . l e g e n d ( [ ' N e g a t i v e ' , ' P o s i t i v e ' ] )
p l t . y t i c k s ( [ ] )
p l t . show ( )

Output

We will now train a neural network for the Wisconsin breast cancer dataset. We begin by preprocessing

the data. Because we have all numeric data, we compute a z-score for each column.

Code

from s c i p y . s t a t s import z s c o r e

x_columns = d f . columns . drop ( ' d i a g n o s i s ' ) . drop ( ' i d ' )
for c o l

in x_columns :

d f [ c o l ] = z s c o r e ( d f [ c o l ] )

t o numpy − R e g r e s s i o n

# Convert
x = d f [ x_columns ] . v a l u e s
y = d f [ ' d i a g n o s i s ' ] . map( { 'M' : 1 , "B" : 0 } ) . v a l u e s # Binary c l a s s i f i c a t i o n ,

# M i s 1 and B i s 0

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

129

We can now deﬁne two functions. The ﬁrst function plots a confusion matrix. The second function

plots a ROC chart.

Code

%m a t p l o t l i b i n l i n e
import m a t p l o t l i b . p y p l o t a s p l t
from s k l e a r n . m e t r i c s import roc_curve , auc

# P l o t a c o n f u s i o n m a t r i x .
# cm i s
def p l o t _ c o n f u s i o n _ m a t r i x (cm , names ,

t h e c o n f u s i o n matrix , names a r e t h e names o f

t h e c l a s s e s .

cmap=p l t . cm . B l u e s ) :
i n t e r p o l a t i o n= ' n e a r e s t ' , cmap=cmap )

t i t l e = ' C o n f u s i o n ␣ matrix ' ,

p l t . imshow (cm ,
p l t . t i t l e ( t i t l e )
p l t . c o l o r b a r ( )
tick_marks = np . a r a n g e ( len ( names ) )
p l t . x t i c k s ( tick_marks , names ,
p l t . y t i c k s ( tick_marks , names )
p l t . t i g h t _ l a y o u t ( )
p l t . y l a b e l ( ' True ␣ l a b e l ' )
p l t . x l a b e l ( ' P r e d i c t e d ␣ l a b e l ' )

r o t a t i o n =45)

# P l o t an ROC. pred − t h e p r e d i c t i o n s , y − t h e e x p e c t e d o u t p u t .
def p l o t _ r o c ( pred , y ) :

tpr , _ = roc_curve ( y , pred )

f p r ,
roc_auc = auc ( f p r ,

t p r )

l a b e l= 'ROC␣ c u r v e ␣ ( a r e a ␣=␣ %0.2 f ) ' % roc_auc )

tpr ,

' k−− ' )

[ 0 , 1 ] ,

p l t . f i g u r e ( )
p l t . p l o t ( f p r ,
p l t . p l o t ( [ 0 , 1 ] ,
p l t . xlim ( [ 0 . 0 , 1 . 0 ] )
p l t . ylim ( [ 0 . 0 , 1 . 0 5 ] )
p l t . x l a b e l ( ' F a l s e ␣ P o s i t i v e ␣ Rate ' )
p l t . y l a b e l ( ' True ␣ P o s i t i v e ␣ Rate ' )
p l t . t i t l e ( ' R e c e i v e r ␣ Op e r a ti n g ␣ C h a r a c t e r i s t i c ␣ (ROC) ' )
p l t . l e g e n d ( l o c=" l o w e r ␣ r i g h t " )
p l t . show ( )

4.2.2 ROC Chart Example

The following code demonstrates how to implement a ROC chart in Python.

130

Code

CHAPTER 4. TRAINING FOR TABULAR DATA

# C l a s s i f i c a t i o n n e u r a l network
import numpy a s np
import t e n s o r f l o w . k e r a s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ,

k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )

model . add ( Dense ( 5 0 , a c t i v a t i o n= ' r e l u ' , k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' , k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )
model . add ( Dense ( 1 , a c t i v a t i o n= ' s i g m o i d ' , k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )
model . compile ( l o s s= ' b i n a r y _ c r o s s e n t r o p y ' ,

o p t i m i z e r=t e n s o r f l o w . k e r a s . o p t i m i z e r s . Adam ( ) ,
m e t r i c s =[ ' a c c u r a c y ' ] )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

Output

. . .
14/14 − 0 s − l o s s : 0 . 0 4 5 8 − a c c u r a c y : 0 . 9 8 3 6 − v a l _ l o s s : 0 . 0 4 8 6 −
v a l _ a c c u r a c y : 0 . 9 8 6 0 − 119ms/ epoch − 8ms/ s t e p
Epoch 13/1000
R e s t o r i n g model w e i g h t s from t h e end o f
14/14 − 0 s − l o s s : 0 . 0 4 1 7 − a c c u r a c y : 0 . 9 8 8 3 − v a l _ l o s s : 0 . 0 4 7 7 −
v a l _ a c c u r a c y : 0 . 9 8 6 0 − 124ms/ epoch − 9ms/ s t e p
Epoch 1 3 : e a r l y s t o p p i n g

t h e b e s t epoch : 8 .

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

131

Code

pred = model . p r e d i c t ( x _t e s t )
p l o t _ r o c ( pred , y _ te s t )

Output

4.2.3 Multiclass Classiﬁcation Error Metrics

If you want to predict more than one outcome, you will need more than one output neuron. Because a
single neuron can predict two results, a neural network with two output neurons is somewhat rare.
If
there are three or more outcomes, there will be three or more output neurons. The following sections will
examine several metrics for evaluating classiﬁcation error. We will assess the following classiﬁcation neural
network.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

132

CHAPTER 4. TRAINING FOR TABULAR DATA

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

Code

# C l a s s i f i c a t i o n n e u r a l network
import numpy a s np
import t e n s o r f l o w . k e r a s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ,

k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )

model . add ( Dense ( 5 0 , a c t i v a t i o n= ' r e l u ' , k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' , k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ,

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

133

model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' ,

k e r n e l _ i n i t i a l i z e r= ' random_normal ' ) )

o p t i m i z e r=t e n s o r f l o w . k e r a s . o p t i m i z e r s . Adam ( ) ,
m e t r i c s =[ ' a c c u r a c y ' ] )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5,
v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

Output

. . .
47/47 − 0 s − l o s s : 0 . 6 6 2 4 − a c c u r a c y : 0 . 7 1 4 7 − v a l _ l o s s : 0 . 7 5 2 7 −
v a l _ a c c u r a c y : 0 . 6 8 0 0 − 328ms/ epoch − 7ms/ s t e p
Epoch 21/1000
R e s t o r i n g model w e i g h t s from t h e end o f
47/47 − 1 s − l o s s : 0 . 6 5 5 8 − a c c u r a c y : 0 . 7 1 6 0 − v a l _ l o s s : 0 . 7 6 5 3 −
v a l _ a c c u r a c y : 0 . 6 7 2 0 − 527ms/ epoch − 11ms/ s t e p
Epoch 2 1 : e a r l y s t o p p i n g

t h e b e s t epoch : 1 6 .

4.2.4 Calculate Classiﬁcation Accuracy

Accuracy is the number of rows where the neural network correctly predicted the target class. Accuracy is
only used for classiﬁcation, not regression.

accuracy =

c
N

Where c is the number correct and N is the size of the evaluated set (training or validation). Higher

accuracy numbers are desired.

As we just saw, by default, Keras will return the percent probability for each class. We can change

these prediction probabilities into the actual iris predicted with argmax.

Code

pred = model . p r e d i c t ( x _t e s t )
pred = np . argmax ( pred , a x i s =1)
# raw p r o b a b i l i t i e s

t o chosen c l a s s

( h i g h e s t p r o b a b i l i t y )

Now that we have the actual iris ﬂower predicted, we can calculate the percent accuracy (how many

were correctly classiﬁed).

134

Code

CHAPTER 4. TRAINING FOR TABULAR DATA

from s k l e a r n import m e t r i c s

y_compare = np . argmax ( y_test , a x i s =1)
s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_compare , pred )
print ( " Accuracy ␣ s c o r e : ␣ {} " . format ( s c o r e ) )

Output

Accuracy s c o r e : 0 . 7

4.2.5 Calculate Classiﬁcation Log Loss

Accuracy is like a ﬁnal exam with no partial credit. However, neural networks can predict a probability of
each of the target classes. Neural networks will give high probabilities to predictions that are more likely.
Log loss is an error metric that penalizes conﬁdence in wrong answers. Lower log loss values are desired.

The following code shows the output of predict_proba:

Code

from IPython . d i s p l a y import d i s p l a y

# Don ' t d i s p l a y numpy i n s c i e n t i f i c n o t a t i o n
np . s e t _ p r i n t o p t i o n s ( p r e c i s i o n =4)
np . s e t _ p r i n t o p t i o n s ( s u p p r e s s=True )

# Generate p r e d i c t i o n s
pred = model . p r e d i c t ( x _t e s t )

print ( "Numpy␣ a r r a y ␣ o f ␣ p r e d i c t i o n s " )
d i s p l a y ( pred [ 0 : 5 ] )

print ( " As␣ p e r c e n t ␣ p r o b a b i l i t y " )
print ( pred [ 0 ] ∗ 1 0 0 )

s c o r e = m e t r i c s . l o g _ l o s s ( y_test , pred )
print ( " Log␣ l o s s ␣ s c o r e : ␣ {} " . format ( s c o r e ) )

# raw p r o b a b i l i t i e s
pred = np . argmax ( pred , a x i s =1)

t o chosen c l a s s

( h i g h e s t p r o b a b i l i t y )

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

135

Output

Numpy a r r a y o f p r e d i c t i o n s
a r r a y ( [ [ 0 .
[ 0 .
[ 0 .
[ 0 .
[ 0 .

, 0 .
, 0 . 1 2 0 1 , 0 . 7 2 8 6 , 0 . 1 4 9 4 , 0 . 0 0 1 8 , 0 .
, 0 . 6 9 6 2 , 0 . 3 0 1 6 , 0 . 0 0 0 1 , 0 . 0 0 2 2 , 0 .
, 0 .
, 0 . 7 2 3 4 , 0 . 2 7 0 8 , 0 . 0 0 0 3 , 0 . 0 0 5 3 , 0 . 0 0 0 1 , 0 .
, 0 .
, 0 . 3 8 3 6 , 0 . 6 0 3 9 , 0 . 0 0 8 6 , 0 . 0 0 3 9 , 0 .
, 0 .
, 0 . 0 6 0 9 , 0 . 6 3 0 3 , 0 . 3 0 7 9 , 0 . 0 0 1 , 0 .

dtype=f l o a t 3 2 ) As p e r c e n t p r o b a b i l i t y

[ 0 . 0 0 0 1 1 2 . 0 1 4 3 7 2 . 8 5 7 8 1 4 . 9 4 4 6
Log l o s s

s c o r e : 0 . 7 4 2 3 4 0 1 4 2 9 2 8 0 6 3 8

0 . 1 8 2 3

0 . 0 0 0 9

0 . 0 0 0 1 ]

] ,
] ,
] ,
] ,
] ] ,

Log loss is calculated as follows:

log loss = −

1
N

N
X

i=1

(yi log(ˆyi) + (1 − yi) log(1 − ˆyi))

You should use this equation only as an objective function for classiﬁcations that have two outcomes.
The variable y-hat is the neural network’s prediction, and the variable y is the known correct answer. In
this case, y will always be 0 or 1. The training data have no probabilities. The neural network classiﬁes it
either into one class (1) or the other (0).

The variable N represents the number of elements in the training set the number of questions in the
test. We divide by N because this process is customary for an average. We also begin the equation with a
negative because the log function is always negative over the domain 0 to 1. This negation allows a positive
score for the training to minimize.

You will notice two terms are separated by the addition (+). Each contains a log function. Because y
will be either 0 or 1, then one of these two terms will cancel out to 0. If y is 0, then the ﬁrst term will
reduce to 0. If y is 1, then the second term will be 0.

If your prediction for the ﬁrst class of a two-class prediction is y-hat, then your prediction for the second
class is 1 minus y-hat. Essentially, if your prediction for class A is 70% (0.7), then your prediction for class
B is 30% (0.3). Your score will increase by the log of your prediction for the correct class. If the neural
network had predicted 1.0 for class A, and the correct answer was A, your score would increase by log (1),
which is 0. For log loss, we seek a low score, so a correct answer results in 0. Some of these log values for
a neural network’s probability estimate for the correct class:

• -log(1.0) = 0
• -log(0.95) = 0.02
• -log(0.9) = 0.05
• -log(0.8) = 0.1
• -log(0.5) = 0.3
• -log(0.1) = 1
• -log(0.01) = 2

136

CHAPTER 4. TRAINING FOR TABULAR DATA

• -log(1.0e-12) = 12
• -log(0.0) = negative inﬁnity

As you can see, giving a low conﬁdence to the correct answer aﬀects the score the most. Because log (0)
is negative inﬁnity, we typically impose a minimum value. Of course, the above log values are for a single
training set element. We will average the log values for the entire training set.

The log function is useful to penalizing wrong answers. The following code demonstrates the utility of

the log function:

Code

%m a t p l o t l i b i n l i n e
from m a t p l o t l i b . p y p l o t import f i g u r e ,
from numpy import arange ,

s i n , p i

show

#t = arange (1 e −5, 5 . 0 , 0 . 0 0 0 0 1 )
#t = arange ( 1 . 0 , 5 . 0 , 0 . 0 0 0 0 1 ) # computer
t = a r a n g e ( 0 . 0 , 1 . 0 , 0 . 0 0 0 0 1 ) # d a t a

s c i e n t i s t s
s c i e n t i s t s

f i g = f i g u r e ( 1 , f i g s i z e =(12 , 1 0 ) )

ax1 = f i g . add_subplot ( 2 1 1 )
ax1 . p l o t ( t , np . l o g ( t ) )
ax1 . g r i d ( True )
ax1 . s et _y l im (( −8 , 1 . 5 ) )
ax1 . s et _x l im ( ( − 0 . 1 , 2 ) )
ax1 . s e t _ x l a b e l ( ' x ' )
ax1 . s e t _ y l a b e l ( ' y ' )
ax1 . s e t _ t i t l e ( ' l o g ( x ) ' )

show ( )

Output

4.2. PART 4.2: MULTICLASS CLASSIFICATION WITH ROC AND AUC

137

4.2.6 Confusion Matrix

A confusion matrix shows which predicted classes are often confused for the other classes. The vertical
axis (y) represents the true labels and the horizontal axis (x) represents the predicted labels. When the
true label and predicted label are the same, the highest values occur down the diagonal extending from
the upper left to the lower right. The other values, outside the diagonal, represent incorrect predictions.
For example, in the confusion matrix below, the value in row 2, column 1 shows how often the predicted
value A occurred when it should have been B.

Code

import numpy a s np
from s k l e a r n import svm , d a t a s e t s
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from s k l e a r n . m e t r i c s import c o n f u s i o n _ m a t r i x

# Compute c o n f u s i o n m a t r i x
cm = c o n f u s i o n _ m a t r i x ( y_compare , pred )
np . s e t _ p r i n t o p t i o n s ( p r e c i s i o n =2)

# Normalize t h e c o n f u s i o n m a t r i x by row ( i . e by t h e number o f
# i n each c l a s s )
cm_normalized = cm . a s t y p e ( ' f l o a t ' ) / cm .sum( a x i s = 1 ) [ : , np . newaxis ]
print ( ' Normalized ␣ c o n f u s i o n ␣ matrix ' )
print ( cm_normalized )
p l t . f i g u r e ( )
p l o t _ c o n f u s i o n _ m a t r i x ( cm_normalized , p r o d u c t s ,

s a m p l e s

t i t l e = ' Normalized ␣ c o n f u s i o n ␣ matrix ' )

p l t . show ( )

Output

138

CHAPTER 4. TRAINING FOR TABULAR DATA

Normalized c o n f u s i o n matrix
[ [ 0 . 9 5 0 . 0 5 0 .

0 .
0 .
0 .
0 .
[ 0 . 0 2 0 . 7 8 0 . 2
0 . 0 1 0 .
0 . 2 9 0 . 7
[ 0 .
0 . 7 1 0 . 2 9 0 .
0 .
[ 0 .
0 .
0 .
[ 0 .
0 .
1 .
0 .
0 .
[ 0 . 5 9 0 . 4 1 0 .
0 .
0 .
0 .
0 .
[ 1 .

0 .
0 .
0 .
0 .
0 .
0 .
0 .

0 .
0 .
0 .
0 .
0 .
0 .
0 .

]
]
]
]
]
]
] ]

4.3 Part 4.3: Keras Regression for Deep Neural Networks with

RMSE

We evaluate regression results diﬀerently than classiﬁcation. Consider the following code that trains a
neural network for regression on the data set jh-simple-dataset.csv. We begin by preparing the data
set.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import m a t p l o t l i b . p y p l o t a s p l t

# Read t h e d a t a s e t

4.3. PART 4.3: KERAS REGRESSION FOR DEEP NEURAL NETWORKS WITH RMSE

139

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# Generate dummies f o r p r o d u c t
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' p r o d u c t ' ] , p r e f i x=" p r o d u c t " ) ] , a x i s =1)
d f . drop ( ' p r o d u c t ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

# Convert
t o numpy − C l a s s i f i c a t i o n
x_columns = d f . columns . drop ( ' age ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
y = d f [ ' age ' ] . v a l u e s

# Create t r a i n / t e s t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

Next, we create a neural network to ﬁt the data we just loaded.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

140

CHAPTER 4. TRAINING FOR TABULAR DATA

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,
p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

Output

Train on 1500 samples , v a l i d a t e on 500 samples
. . .
1500/1500 − 0 s − l o s s : 0 . 4 0 8 1 − v a l _ l o s s : 0 . 5 5 4 0
Epoch 124/1000
R e s t o r i n g model w e i g h t s from t h e end o f
1500/1500 − 0 s − l o s s : 0 . 4 3 5 3 − v a l _ l o s s : 0 . 5 5 3 8
Epoch 0 0 1 2 4 : e a r l y s t o p p i n g

t h e b e s t epoch .

4.3.1 Mean Square Error

The mean square error (MSE) is the sum of the squared diﬀerences between the prediction (ˆy) and the
expected (y). MSE values are not of a particular unit. If an MSE value has decreased for a model, that
is good. However, beyond this, there is not much more you can determine. We seek to achieve low MSE
values. The following equation demonstrates how to calculate MSE.

MSE =

1
n

n
X

i=1

(ˆyi − yi)2

The following code calculates the MSE on the predictions from the neural network.

Code

from s k l e a r n import m e t r i c s

# P r e d i c t
pred = model . p r e d i c t ( x _t e s t )

4.3. PART 4.3: KERAS REGRESSION FOR DEEP NEURAL NETWORKS WITH RMSE

141

# Measure MSE e r r o r .
s c o r e = m e t r i c s . mean_squared_error ( pred , y _t e s t )
print ( " F i n a l ␣ s c o r e ␣ (MSE) : ␣ {} " . format ( s c o r e ) )

Output

F i n a l

s c o r e (MSE) : 0 . 5 4 6 3 4 4 7 8 2 9 6 7 7 6 0 7

4.3.2 Root Mean Square Error

The root mean square (RMSE) is essentially the square root of the MSE. Because of this, the RMSE error
is in the same units as the training data outcome. We desire Low RMSE values. The following equation
calculates RMSE.

RMSE =

v
u
u
t

1
n

n
X

i=1

(ˆyi − yi)2

Code

import numpy a s np

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " F i n a l ␣ s c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

r e g r e s s i o n .

Output

F i n a l

s c o r e (RMSE) : 0 . 7 3 9 1 5 1 3 9 3 8 0 7 6 2 9 1

4.3.3 Lift Chart

We often visualize the results of regression with a lift chart. To generate a lift chart, perform the following
activities:

• Sort the data by expected output and plot these values.
• For every point on the x-axis, plot that same data point’s predicted value in another color.
• The x-axis is just 0 to 100% of the dataset. The expected always starts low and ends high.
• The y-axis is ranged according to the values predicted.

142

CHAPTER 4. TRAINING FOR TABULAR DATA

You can interpret the lift chart as follows:

• The expected and predict lines should be close. Notice where one is above the other.
• The below chart is the most accurate for lower ages.

Code

# R e g r e s s i o n c h a r t .
def c h a r t _ r e g r e s s i o n ( pred , y ,

s o r t=True ) :

t = pd . DataFrame ( { ' pred ' : pred ,
i f

s o r t :

' y ' : y . f l a t t e n ( ) } )

t . s o r t _ v a l u e s ( by=[ ' y ' ] ,

p l t . p l o t ( t [ ' y ' ] . t o l i s t ( ) ,
p l t . p l o t ( t [ ' pred ' ] . t o l i s t ( ) ,
p l t . y l a b e l ( ' output ' )
p l t . l e g e n d ( )
p l t . show ( )

i n p l a c e=True )
l a b e l= ' e x p e c t e d ' )

l a b e l= ' p r e d i c t i o n ' )

# P l o t
c h a r t _ r e g r e s s i o n ( pred . f l a t t e n ( ) , y _t e s t )

t h e c h a r t

Output

4.4 Part 4.4: Training Neural Networks

Backpropagation[29]is one of the most common methods for training a neural network. Rumelhart, Hinton,
Williams introduced backpropagation, and it remains popular today. Programmers frequently train deep
neural networks with backpropagation because it scales really well when run on graphical processing units
(GPUs). To understand this algorithm for neural networks, we must examine how to train it as well as
how it processes a pattern.

4.4. PART 4.4: TRAINING NEURAL NETWORKS

143

Researchers have extended classic backpropagation and modiﬁed to give rise to many diﬀerent training
algorithms. This section will discuss the most commonly used training algorithms for neural networks. We
begin with classic backpropagation and end the chapter with stochastic gradient descent (SGD).

Backpropagation is the primary means of determining a neural network’s weights during training.
Backpropagation works by calculating a weight change amount (vt) for every weight(θ, theta) in the neural
network. This value is subtracted from every weight by the following equation:

θt = θt−1 − vt

We repeat this process for every iteration(t). The training algorithm determines how we calculate the
weight change. Classic backpropagation calculates a gradient (∇, nabla) for every weight in the neural
network for the neural network’s error function (J). We scale the gradient by a learning rate (η, eta).

vt = η∇θt−1J(θt−1)

The learning rate is an important concept for backpropagation training. Setting the learning rate can

be complex:

• Too low a learning rate will usually converge to a reasonable solution; however, the process will be

prolonged.

• Too high of a learning rate will either fail outright or converge to a higher error than a better learning

rate.

Common values for learning rate are: 0.1, 0.01, 0.001, etc.

Backpropagation is a gradient descent type, and many texts will use these two terms interchangeably.
Gradient descent refers to calculating a gradient on each weight in the neural network for each training
element. Because the neural network will not output the expected value for a training element, the gradient
of each weight will indicate how to modify each weight to achieve the expected output. If the neural network
did output exactly what was expected, the gradient for each weight would be 0, indicating that no change
to the weight is necessary.

The gradient is the derivative of the error function at the weight’s current value. The error function
measures the distance of the neural network’s output from the expected output. We can use gradient
descent, a process in which each weight’s gradient value can reach even lower values of the error function.
The gradient is the partial derivative of each weight in the neural network concerning the error function.
Each weight has a gradient that is the slope of the error function. Weight is a connection between two
neurons. Calculating the gradient of the error function allows the training method to determine whether
it should increase or decrease the weight. In turn, this determination will decrease the error of the neural
network. The error is the diﬀerence between the expected output and actual output of the neural network.
Many diﬀerent training methods called propagation-training algorithms utilize gradients. In all of them,
the sign of the gradient tells the neural network the following information:

• Zero gradient - The weight does not contribute to the neural network’s error.

144

CHAPTER 4. TRAINING FOR TABULAR DATA

• Negative gradient - The algorithm should increase the weight to lower error.
• Positive gradient - The algorithm should decrease the weight to lower error.

Because many algorithms depend on gradient calculation, we will begin with an analysis of this process.
First of all, let’s examine the gradient. Essentially, training is a search for the set of weights that will cause
the neural network to have the lowest error for a training set. If we had inﬁnite computation resources,
we would try every possible combination of weights to determine the one that provided the lowest error
during the training.

Because we do not have unlimited computing resources, we have to use some shortcuts to prevent
the need to examine every possible weight combination. These training methods utilize clever techniques
to avoid performing a brute-force search of all weight values. This type of exhaustive search would be
impossible because even small networks have an inﬁnite number of weight combinations.

Consider a chart that shows the error of a neural network for each possible weight. Figure 4.2 is a

graph that demonstrates the error for a single weight:

Looking at this chart, you can easily see that the optimal weight is where the line has the lowest y-value.
The problem is that we see only the error for the current value of the weight; we do not see the entire

Figure 4.2: Derivative

4.4. PART 4.4: TRAINING NEURAL NETWORKS

145

graph because that process would require an exhaustive search. However, we can determine the slope of
the error curve at a particular weight. In the above chart, we see the slope of the error curve at 1.5. The
straight line barely touches the error curve at 1.5 gives the slope. In this case, the slope, or gradient, is
-0.5622. The negative slope indicates that an increase in the weight will lower the error.
The gradient is the instantaneous slope of the error function at the speciﬁed weight. The derivative of the
error curve at that point gives the gradient. This line tells us the steepness of the error function at the given
weight.
Derivatives are one of the most fundamental concepts in calculus. For this book, you need to under-
stand that a derivative provides the slope of a function at a speciﬁc point. A training technique and this
slope can give you the information to adjust the weight for a lower error. Using our working deﬁnition of
the gradient, we will show how to calculate it.

4.4.1 Momentum Backpropagation

Momentum adds another term to the calculation of vt:

vt = η∇θt−1 J(θt−1) + λvt−1

Like the learning rate, momentum adds another training parameter that scales the eﬀect of momen-
tum. Momentum backpropagation has two training parameters: learning rate (η, eta) and momentum (λ,
lambda). Momentum adds the scaled value of the previous weight change amount (vt−1) to the current
weight change amount(vt).

This technique has the eﬀect of adding additional force behind the direction a weight is moving. Figure

4.3 shows how this might allow the weight to escape local minima.

A typical value for momentum is 0.9.

4.4.2 Batch and Online Backpropagation

How often should the weights of a neural network be updated? We can calculate gradients for a training
set element. These gradients can also be summed together into batches, and the weights updated once per
batch.

• Online Training - Update the weights based on gradients calculated from a single training set

element.

• Batch Training - Update the weights based on the sum of the gradients over all training set elements.
• Batch Size - Update the weights based on the sum of some batch size of training set elements.
• Mini-Batch Training - The same as batch size, but with minimal batch size. Mini-batches are

very popular, often in the 32-64 element range.

Because the batch size is smaller than the full training set size, it may take several batches to make it
completely through the training set.

• Step/Iteration - The number of processed batches.
• Epoch - The number of times the algorithm processed the complete training set.

146

CHAPTER 4. TRAINING FOR TABULAR DATA

Figure 4.3: Momentum

4.4.3 Stochastic Gradient Descent

Stochastic gradient descent (SGD) is currently one of the most popular neural network training algorithms.
It works very similarly to Batch/Mini-Batch training, except that the batches are made up of a random
set of training elements.

This technique leads to a very irregular convergence in error during training, as shown in Figure 4.4.
Image from Wikipedia
Because the neural network is trained on a random sample of the complete training set each time, the

error does not make a smooth transition downward. However, the error usually does go down.

Advantages to SGD include:

• Computationally eﬃcient. Each training step can be relatively fast, even with a huge training set.
• Decreases overﬁtting by focusing on only a portion of the training set each step.

4.4.4 Other Techniques

One problem with simple backpropagation training algorithms is that they are susceptible to learning rate
and momentum. This technique is diﬃcult because:

• Learning rate must be adjusted to a small enough level to train an accurate neural network.
• Momentum must be large enough to overcome local minima yet small enough not to destabilize the

training.

• A single learning rate/momentum is often not good enough for the entire training process. It is often

helpful to automatically decrease the learning rate as the training progresses.

• All weights share a single learning rate/momentum.

4.4. PART 4.4: TRAINING NEURAL NETWORKS

147

Figure 4.4: SGD Error

Other training techniques:

• Resilient Propagation - Use only the magnitude of the gradient and allow each neuron to learn at
its rate. There is no need for learning rate/momentum; however, it only works in full batch mode.

• Nesterov accelerated gradient - Helps mitigate the risk of choosing a bad mini-batch.
• Adagrad - Allows an automatically decaying per-weight learning rate and momentum concept.
• Adadelta - Extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing

learning rate.

• Non-Gradient Methods - Non-gradient methods can sometimes be useful, though rarely outper-
form gradient-based backpropagation methods. These include: simulated annealing, genetic algo-
rithms, particle swarm optimization, Nelder Mead, and many more.

4.4.5 ADAM Update

ADAM is the ﬁrst training algorithm you should try. It is very eﬀective. Kingma and Ba (2014) introduced
the Adam update rule that derives its name from the adaptive moment estimates.[19]Adam estimates the
ﬁrst (mean) and second (variance) moments to determine the weight corrections. Adam begins with an
exponentially decaying average of past gradients (m):

mt = β1mt−1 + (1 − β1)gt

148

CHAPTER 4. TRAINING FOR TABULAR DATA

This average accomplishes a similar goal as classic momentum update; however, its value is calculated
automatically based on the current gradient (gt). The update rule then calculates the second moment (vt):

vt = β2vt−1 + (1 − β2)g2
t

The values mt and vt are estimates of the gradients’ ﬁrst moment (the mean) and the second moment
(the uncentered variance). However, they will be strongly biased towards zero in the initial training cycles.
The ﬁrst moment’s bias is corrected as follows.

ˆmt =

mt
1 − βt
1

Similarly, the second moment is also corrected:

ˆvt =

vt
1 − βt
2

These bias-corrected ﬁrst and second moment estimates are applied to the ultimate Adam update rule,

as follows:

θt = θt−1 −

α · ˆmt
√
ˆvt + η

ˆmt

Adam is very tolerant to initial learning rate (\alpha) and other training parameters. Kingma and Ba

(2014) propose default values of 0.9 for β1, 0.999 for β2, and 10-8 for η.

4.4.6 Methods Compared

The following image shows how each of these algorithms train. It is animated, so it is not displayed in the
printed book, but can be accessed from here: https://bit.ly/3kykkbn.

Image credits: Alec Radford

4.4.7 Specifying the Update Rule in Keras

TensorFlow allows the update rule to be set to one of:

• Adagrad
• Adam
• Ftrl
• Momentum
• RMSProp
• SGD

4.4. PART 4.4: TRAINING NEURAL NETWORKS

149

Code

%m a t p l o t l i b i n l i n e

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import pandas a s pd
import m a t p l o t l i b . p y p l o t a s p l t

# R e g r e s s i o n c h a r t .
def c h a r t _ r e g r e s s i o n ( pred , y ,

s o r t=True ) :

t = pd . DataFrame ( { ' pred ' : pred ,
i f

s o r t :

' y ' : y . f l a t t e n ( ) } )

t . s o r t _ v a l u e s ( by=[ ' y ' ] ,

p l t . p l o t ( t [ ' y ' ] . t o l i s t ( ) ,
p l t . p l o t ( t [ ' pred ' ] . t o l i s t ( ) ,
p l t . y l a b e l ( ' output ' )
p l t . l e g e n d ( )
p l t . show ( )

i n p l a c e=True )
l a b e l= ' e x p e c t e d ' )

l a b e l= ' p r e d i c t i o n ' )

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# Generate dummies f o r p r o d u c t
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' p r o d u c t ' ] , p r e f i x=" p r o d u c t " ) ] , a x i s =1)
d f . drop ( ' p r o d u c t ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )

150

CHAPTER 4. TRAINING FOR TABULAR DATA

d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

# Convert
t o numpy − C l a s s i f i c a t i o n
x_columns = d f . columns . drop ( ' age ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
y = d f [ ' age ' ] . v a l u e s

# Create t r a i n / t e s t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' ) # Modify h e r e
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5,
v e r b o s e =1, mode= ' auto ' ,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s =1000)

t h e c h a r t

# P l o t
pred = model . p r e d i c t ( x _t e s t )
c h a r t _ r e g r e s s i o n ( pred . f l a t t e n ( ) , y _t e s t )

Output

4.5. PART 4.5: ERROR CALCULATION FROM SCRATCH

151

R e s t o r i n g model w e i g h t s from t h e end o f
Epoch 0 0 1 0 5 : e a r l y s t o p p i n g

t h e b e s t epoch .

4.5 Part 4.5: Error Calculation from Scratch

We will now look at how to calculate RMSE and logloss by hand. RMSE is typically used for regression.
We begin by calculating RMSE with libraries.

Code

from s k l e a r n import m e t r i c s
import numpy a s np

p r e d i c t e d = [ 1 . 1 , 1 . 9 , 3 . 4 , 4 . 2 , 4 . 3 ]
e x p e c t e d = [ 1 , 2 , 3 , 4 , 5 ]

score_mse = m e t r i c s . mean_squared_error ( p r e d i c t e d , e x p e c t e d )
score_rmse = np . s q r t ( score_mse )
print ( " S c o r e ␣ (MSE) : ␣ {} " . format ( score_mse ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( score_rmse ) )

Output

S c o r e (MSE) : 0 . 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 7
S c o r e (RMSE) : 0 . 3 7 6 8 2 8 8 7 3 6 2 8 3 3 5 5 6

We can also calculate without libraries.

152

Code

CHAPTER 4. TRAINING FOR TABULAR DATA

score_mse = ( ( p r e d i c t e d [0] − e x p e c t e d [ 0 ] ) ∗ ∗ 2 + ( p r e d i c t e d [1] − e x p e c t e d [ 1 ] ) ∗ ∗ 2
+ ( p r e d i c t e d [2] − e x p e c t e d [ 2 ] ) ∗ ∗ 2 + ( p r e d i c t e d [3] − e x p e c t e d [ 3 ] ) ∗ ∗ 2
+ ( p r e d i c t e d [4] − e x p e c t e d [ 4 ] ) ∗ ∗ 2 ) / len ( p r e d i c t e d )
score_rmse = np . s q r t ( score_mse )

print ( " S c o r e ␣ (MSE) : ␣ {} " . format ( score_mse ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( score_rmse ) )

Output

S c o r e (MSE) : 0 . 1 4 2 0 0 0 0 0 0 0 0 0 0 0 0 0 7
S c o r e (RMSE) : 0 . 3 7 6 8 2 8 8 7 3 6 2 8 3 3 5 5 6

4.5.1 Classiﬁcation

We will now look at how to calculate a logloss by hand. For this, we look at a binary prediction. The
predicted is some number between 0-1 that indicates the probability true (1). The expected is always 0
or 1. Therefore, a prediction of 1.0 is completely correct if the expected is 1 and completely wrong if the
expected is 0.

Code

from s k l e a r n import m e t r i c s

e x p e c t e d = [ 1 , 1 , 0 , 0 , 0 ]
p r e d i c t e d = [ 0 . 9 , 0 . 9 9 , 0 . 1 , 0 . 0 5 , 0 . 0 6 ]

print ( m e t r i c s . l o g _ l o s s ( expected , p r e d i c t e d ) )

Output

0 . 0 6 6 7 8 8 0 1 3 0 5 4 9 5 8 4 3

Now we attempt to calculate the same logloss manually.

Code

import numpy a s np

s c o r e _ l o g l o s s = ( np . l o g (1.0 − np . abs ( e x p e c t e d [0] − p r e d i c t e d [ 0 ] ) ) + \

4.5. PART 4.5: ERROR CALCULATION FROM SCRATCH

153

np . l o g (1.0 − np . abs ( e x p e c t e d [1] − p r e d i c t e d [ 1 ] ) ) + \
np . l o g (1.0 − np . abs ( e x p e c t e d [2] − p r e d i c t e d [ 2 ] ) ) + \
np . l o g (1.0 − np . abs ( e x p e c t e d [3] − p r e d i c t e d [ 3 ] ) ) + \
np . l o g (1.0 − np . abs ( e x p e c t e d [4] − p r e d i c t e d [ 4 ] ) ) ) \
∗(−1/ len ( p r e d i c t e d ) )

print ( f ' S c o r e ␣ L o g l o s s ␣{ s c o r e _ l o g l o s s } ' )

Output

S c o r e L o g l o s s 0 . 0 6 6 7 8 8 0 1 3 0 5 4 9 5 8 4 3

154

CHAPTER 4. TRAINING FOR TABULAR DATA

Chapter 5

Regularization and Dropout

5.1 Part 5.1: Introduction to Regularization: Ridge and Lasso

Regularization is a technique that reduces overﬁtting, which occurs when neural networks attempt to
memorize training data rather than learn from it. Humans are capable of overﬁtting as well. Before
examining how a machine accidentally overﬁts, we will ﬁrst explore how humans can suﬀer from it.

Human programmers often take certiﬁcation exams to show their competence in a given programming
language. To help prepare for these exams, the test makers often make practice exams available. Consider
a programmer who enters a loop of taking the practice exam, studying more, and then retaking the practice
exam. The programmer has memorized much of the practice exam at some point rather than learning the
techniques necessary to ﬁgure out the individual questions. The programmer has now overﬁtted for the
practice exam. When this programmer takes the real exam, his actual score will likely be lower than what
he earned on the practice exam.

Although a neural network received a high score on its training data, this result does not mean that the
same neural network will score high on data that was not inside the training set. A computer can overﬁt as
well. Regularization is one of the techniques that can prevent overﬁtting. Several diﬀerent regularization
techniques exist. Most work by analyzing and potentially modifying the weights of a neural network as it
trains.

5.1.1 L1 and L2 Regularization

L1 and L2 regularization are two standard regularization techniques that can reduce the eﬀects of over-
ﬁtting. These algorithms can either work with an objective function or as part of the backpropagation
algorithm. The regularization algorithm is attached to the training algorithm by adding an objective in
both cases.

These algorithms work by adding a weight penalty to the neural network training. This penalty en-
courages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty
diﬀerently. You can add this penalty calculation to the calculated gradients for gradient-descent-based
algorithms, such as backpropagation. The penalty is negatively combined with the objective score for
objective-function-based training, such as simulated annealing.

155

156

CHAPTER 5. REGULARIZATION AND DROPOUT

We will look at linear regression to see how L1 and L2 regularization work. The following code sets up

the auto-mpg data for this purpose.

Code

import LassoCV

from s k l e a r n . l i n e a r _ m o d e l
import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
names = [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ]

x = d f [ names ] . v a l u e s
y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =45)

x , y ,

We will use the data just loaded for several examples. The ﬁrst examples in this part use several forms
of linear regression. For linear regression, it is helpful to examine the model’s coeﬃcients. The following
function is utilized to display these coeﬃcients.

Code

# Simple f u n c t i o n t o e v a l u a t e t h e c o e f f i c i e n t s o f a r e g r e s s i o n
%m a t p l o t l i b i n l i n e
from IPython . d i s p l a y import d i s p l a y , HTML

def

r e p o r t _ c o e f ( names , c o e f , i n t e r c e p t ) :
r = pd . DataFrame ( { ' c o e f ' : c o e f ,

' p o s i t i v e ' : c o e f >=0

} ,

i n d e x = names )

r = r . s o r t _ v a l u e s ( by=[ ' c o e f ' ] )

5.1. PART 5.1: INTRODUCTION TO REGULARIZATION: RIDGE AND LASSO

157

d i s p l a y ( r )
print ( f " I n t e r c e p t : ␣{ i n t e r c e p t } " )
r [ ' c o e f ' ] . p l o t ( kind= ' barh ' , c o l o r=r [ ' p o s i t i v e ' ] . map(
' r ' } ) )

' b ' , F a l s e :

{ True :

5.1.2 Linear Regression

Before jumping into L1/L2 regularization, we begin with linear regression. Researchers ﬁrst introduced
the L1/L2 form of regularization for linear regression. We can also make use of L1/L2 for neural networks.
To fully understand L1/L2 we will begin with how we can use them with linear regression.

The following code uses linear regression to ﬁt the auto-mpg data set. The RMSE reported will not be

as good as a neural network.

Code

import s k l e a r n

# Create l i n e a r
r e g r e s s o r = s k l e a r n . l i n e a r _ m o d e l . L i n e a r R e g r e s s i o n ( )

r e g r e s s i o n

# F i t / t r a i n l i n e a r
r e g r e s s o r . f i t ( x_train , y _ t r a i n )
# P r e d i c t
pred = r e g r e s s o r . p r e d i c t ( x _t e s t )

r e g r e s s i o n

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

r e p o r t _ c o e f (

names ,
r e g r e s s o r . coef_ ,
r e g r e s s o r . i n t e r c e p t _ )

Output

158

CHAPTER 5. REGULARIZATION AND DROPOUT

cylinders
weight
horsepower
displacement
acceleration
year
origin

positive

coef
-0.427721 False
-0.007255 False
-0.005491 False
True
0.020166
True
0.138575
True
0.783047
True
1.003762

F i n a l
I n t e r c e p t : −19.101231042200112

s c o r e (RMSE) : 3 . 0 0 1 9 3 4 5 9 8 5 8 6 0 7 8 4

5.1.3 L1 (Lasso) Regularization

L1 regularization, also called LASSO (Least Absolute Shrinkage and Selection Operator) should be used to
create sparsity in the neural network. In other words, the L1 algorithm will push many weight connections
to near 0. When the weight is near 0, the program drops it from the network. Dropping weighted
connections will create a sparse neural network.

Feature selection is a useful byproduct of sparse neural networks. Features are the values that the
training set provides to the input neurons. Once all the weights of an input neuron reach 0, the neural
network training determines that the feature is unnecessary. If your data set has many unnecessary input
features, L1 regularization can help the neural network detect and ignore unnecessary features.

L1 is implemented by adding the following error to the objective to minimize:

E1 = α

X

|w|

w

You should use L1 regularization to create sparsity in the neural network.

In other words, the L1
algorithm will push many weight connections to near 0. When the weight is near 0, the program drops it
from the network. Dropping weighted connections will create a sparse neural network.

The following code demonstrates lasso regression. Notice the eﬀect of the coeﬃcients compared to the

previous section that used linear regression.

5.1. PART 5.1: INTRODUCTION TO REGULARIZATION: RIDGE AND LASSO

159

Code

import s k l e a r n
from s k l e a r n . l i n e a r _ m o d e l

import Lasso

# C re a te l i n e a r
r e g r e s s i o n
r e g r e s s o r = Lasso ( random_state =0, a l p h a =0.1)

# F i t / t r a i n LASSO
r e g r e s s o r . f i t ( x_train , y _ t r a i n )
# P r e d i c t
pred = r e g r e s s o r . p r e d i c t ( x _t e s t )

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

r e p o r t _ c o e f (

names ,
r e g r e s s o r . coef_ ,
r e g r e s s o r . i n t e r c e p t _ )

Output

cylinders
weight
horsepower
displacement
acceleration
origin
year

positive

coef
-0.012995 False
-0.007328 False
-0.002715 False
True
0.011601
True
0.114391
True
0.708222
True
0.777480

160

CHAPTER 5. REGULARIZATION AND DROPOUT

F i n a l
I n t e r c e p t : −18.506677982383252

s c o r e (RMSE) : 3 . 0 6 0 4 0 2 1 9 0 4 0 3 3 3 0 3

5.1.4 L2 (Ridge) Regularization

You should use Tikhonov/Ridge/L2 regularization when you are less concerned about creating a space
network and are more concerned about low weight values. The lower weight values will typically lead to
less overﬁtting.

E2 = α

X

w2

w

Like the L1 algorithm, the α value determines how important the L2 objective is compared to the
neural network’s error. Typical L2 values are below 0.1 (10%). The main calculation performed by L2 is
the summing of the squares of all of the weights. The algorithm will not sum bias values.

You should use L2 regularization when you are less concerned about creating a space network and are
more concerned about low weight values. The lower weight values will typically lead to less overﬁtting.
Generally, L2 regularization will produce better overall performance than L1. However, L1 might be useful
in situations with many inputs, and you can prune some of the weaker inputs.

The following code uses L2 with linear regression (Ridge regression):

Code

import s k l e a r n
from s k l e a r n . l i n e a r _ m o d e l

import Ridge

r e g r e s s i o n
# Create l i n e a r
r e g r e s s o r = Ridge ( a l p h a =1)

# F i t / t r a i n Ridge
r e g r e s s o r . f i t ( x_train , y _ t r a i n )
# P r e d i c t
pred = r e g r e s s o r . p r e d i c t ( x _t e s t )

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

r e p o r t _ c o e f (

names ,
r e g r e s s o r . coef_ ,
r e g r e s s o r . i n t e r c e p t _ )

5.1. PART 5.1: INTRODUCTION TO REGULARIZATION: RIDGE AND LASSO

161

Output

cylinders
weight
horsepower
displacement
acceleration
year
origin

positive

coef
-0.421393 False
-0.007257 False
-0.005385 False
True
0.020006
True
0.138470
True
0.782889
True
0.994621

F i n a l
I n t e r c e p t : −19.07980074425469

s c o r e (RMSE) : { s c o r e }

5.1.5 ElasticNet Regularization

The ElasticNet regression combines both L1 and L2. Both penalties are applied. The amount of L1 and
L2 are governed by the parameters alpha and beta.

a ∗ L1 + b ∗ L2

Code

import s k l e a r n
from s k l e a r n . l i n e a r _ m o d e l

import E l a s t i c N e t

# C reate l i n e a r
r e g r e s s o r = E l a s t i c N e t ( a l p h a =0.1 ,

r e g r e s s i o n

l 1 _ r a t i o =0.1)

# F i t / t r a i n LASSO

162

CHAPTER 5. REGULARIZATION AND DROPOUT

r e g r e s s o r . f i t ( x_train , y _ t r a i n )
# P r e d i c t
pred = r e g r e s s o r . p r e d i c t ( x _t e s t )

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

r e p o r t _ c o e f (

names ,
r e g r e s s o r . coef_ ,
r e g r e s s o r . i n t e r c e p t _ )

Output

cylinders
weight
horsepower
displacement
acceleration
year
origin

positive

coef
-0.274010 False
-0.007303 False
-0.003231 False
True
0.016194
True
0.132348
True
0.777482
True
0.782781

F i n a l
I n t e r c e p t : −18.389355690429767

s c o r e (RMSE) : 3 . 0 4 5 0 8 9 9 9 6 0 7 7 5 0 1 3

5.2 Part 5.2: Using K-Fold Cross-validation with Keras

You can use cross-validation for a variety of purposes in predictive modeling:

• Generating out-of-sample predictions from a neural network

5.2. PART 5.2: USING K-FOLD CROSS-VALIDATION WITH KERAS

163

• Estimate a good number of epochs to train a neural network for (early stopping)
• Evaluate the eﬀectiveness of certain hyperparameters, such as activation functions, neuron counts,

and layer counts

Cross-validation uses several folds and multiple models to provide each data segment a chance to serve as
both the validation and training set. Figure 5.1 shows cross-validation.

It is important to note that each fold will have one model (neural network). To generate predictions
for new data (not present in the training set), predictions from the fold models can be handled in several
ways:

• Choose the model with the highest validation score as the ﬁnal model.
• Preset new data to the ﬁve models (one for each fold) and average the result (this is an ensemble).
• Retrain a new model (using the same settings as the cross-validation) on the entire dataset. Train

for as many epochs and with the same hidden layer structure.

Generally, I prefer the last approach and will retrain a model on the entire data set once I have selected
hyper-parameters. Of course, I will always set aside a ﬁnal holdout set for model validation that I do not
use in any aspect of the training process.

5.2.1 Regression vs Classiﬁcation K-Fold Cross-Validation

Regression and classiﬁcation are handled somewhat diﬀerently concerning cross-validation. Regression is
the simpler case where you can break up the data set into K folds with little regard for where each item
lands. For regression, the data items should fall into the folds as randomly as possible. It is also important
to remember that not every fold will necessarily have the same number of data items. It is not always
possible for the data set to be evenly divided into K folds. For regression cross-validation, we will use the
Scikit-Learn class KFold.

Cross-validation for classiﬁcation could also use the KFold object; however, this technique would not
ensure that the class balance remains the same in each fold as in the original. The balance of classes that
a model was trained on must remain the same (or similar) to the training set. Drift in this distribution
is one of the most important things to monitor after a trained model has been placed into actual use.
Because of this, we want to make sure that the cross-validation itself does not introduce an unintended
shift. This technique is called stratiﬁed sampling and is accomplished by using the Scikit-Learn object
StratiﬁedKFold in place of KFold whenever you use classiﬁcation. In summary, you should use the
following two objects in Scikit-Learn:

• KFold When dealing with a regression problem.
• StratiﬁedKFold When dealing with a classiﬁcation problem.

The following two sections demonstrate cross-validation with classiﬁcation and regression.

5.2.2 Out-of-Sample Regression Predictions with K-Fold Cross-Validation

The following code trains the simple dataset using a 5-fold cross-validation. The expected performance of
a neural network of the type trained here would be the score for the generated out-of-sample predictions.

164

CHAPTER 5. REGULARIZATION AND DROPOUT

We begin by preparing a feature vector using the jh-simple-dataset to predict age. This model is set up
as a regression problem.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# Generate dummies f o r p r o d u c t
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' p r o d u c t ' ] , p r e f i x=" p r o d u c t " ) ] , a x i s =1)
d f . drop ( ' p r o d u c t ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n
# Convert
x_columns = d f . columns . drop ( ' age ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
y = d f [ ' age ' ] . v a l u e s

Now that the feature vector is created a 5-fold cross-validation can be performed to generate out-of-
sample predictions. We will assume 500 epochs and not use early stopping. Later we will see how we can
estimate a more optimal epoch count.

5.2. PART 5.2: USING K-FOLD CROSS-VALIDATION WITH KERAS

165

Code

EPOCHS=500

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import KFold
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n

# Cross−V a l i d a t e
k f = KFold ( 5 ,
oos_y = [ ]
oos_pred = [ ]

s h u f f l e=True ,

random_state =42) # Use f o r KFold c l a s s i f i c a t i o n

f o l d = 0
for t r a i n ,

t e s t

in k f . s p l i t ( x ) :

f o l d+=1
print ( f " Fold ␣#{ f o l d } " )

x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

model = S e q u e n t i a l ( )
model . add ( Dense ( 2 0 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) , v e r b o s e =0,

e p oc h s=EPOCHS)

pred = model . p r e d i c t ( x_ t e s t )

oos_y . append ( y _ t es t )
oos_pred . append ( pred )

# Measure t h i s

f o l d ' s RMSE

166

CHAPTER 5. REGULARIZATION AND DROPOUT

s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ t es t ) )
print ( f " Fold ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

# B u i l d t h e oos p r e d i c t i o n l i s t and c a l c u l a t e t h e e r r o r .
oos_y = np . c o n c a t e n a t e ( oos_y )
oos_pred = np . c o n c a t e n a t e ( oos_pred )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( oos_pred , oos_y ) )
print ( f " F i n a l , ␣ out ␣ o f ␣ sample ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

# Write t h e c r o s s −v a l i d a t e d p r e d i c t i o n
oos_y = pd . DataFrame ( oos_y )
oos_pred = pd . DataFrame ( oos_pred )
oosDF = pd . c o n c a t (
#oosDF . to_csv ( f i l e n a m e _ w r i t e , i n d e x=F a l s e )

[ df , oos_y , oos_pred ] , a x i s =1 )

Output

Fold #1
Fold s c o r e (RMSE) : 0 . 6 8 1 4 2 9 9 4 2 6 5 1 1 2 0 8
Fold #2
Fold s c o r e (RMSE) : 0 . 4 5 4 8 6 5 1 3 7 1 9 4 8 7 1 6 5
Fold #3
Fold s c o r e (RMSE) : 0 . 5 7 1 6 1 5 0 4 1 8 7 6 3 9 2
Fold #4
Fold s c o r e (RMSE) : 0 . 4 6 4 1 6 3 5 6 0 8 1 1 1 6 9 1 6
Fold #5
Fold s c o r e (RMSE) : 1 . 0 4 2 6 5 1 8 4 9 1 6 8 5 4 7 5
F i n a l , out o f

sample s c o r e (RMSE) : 0 . 6 7 8 3 1 6 0 7 7 5 9 7 4 0 8

As you can see, the above code also reports the average number of epochs needed. A common technique

is to then train on the entire dataset for the average number of epochs required.

5.2.3 Classiﬁcation with Stratiﬁed K-Fold Cross-Validation

The following code trains and ﬁts the jh-simple-dataset dataset with cross-validation to generate out-of-
sample. It also writes the out-of-sample (predictions on the test set) results.

It is good to perform stratiﬁed k-fold cross-validation with classiﬁcation data. This technique ensures
that the percentages of each class remain the same across all folds. Use the StratiﬁedKFold object
instead of the KFold object used in the regression.

5.2. PART 5.2: USING K-FOLD CROSS-VALIDATION WITH KERAS

167

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

We will assume 500 epochs and not use early stopping. Later we will see how we can estimate a more

optimal epoch count.

Code

import pandas a s pd
import o s
import numpy a s np

168

CHAPTER 5. REGULARIZATION AND DROPOUT

from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n

# np . argmax ( pred , a x i s =1)
# Cross−v a l i d a t e
# Use f o r S t r a t i f i e d K F o l d c l a s s i f i c a t i o n
k f = S t r a t i f i e d K F o l d ( 5 ,

s h u f f l e=True ,

random_state =42)

oos_y = [ ]
oos_pred = [ ]
f o l d = 0

# Must s p e c i f y y S t r a t i f i e d K F o l d f o r
for t r a i n ,

in k f . s p l i t ( x , d f [ ' p r o d u c t ' ] ) :

t e s t

f o l d+=1
print ( f " Fold ␣#{ f o l d } " )

x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

model = S e q u e n t i a l ( )
# Hidden 1
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

v e r b o s e =0, e po c h s=EPOCHS)

pred = model . p r e d i c t ( x_ t e s t )

oos_y . append ( y _ t es t )
# raw p r o b a b i l i t i e s
pred = np . argmax ( pred , a x i s =1)
oos_pred . append ( pred )

t o chosen c l a s s

( h i g h e s t p r o b a b i l i t y )

# Measure t h i s
y_compare = np . argmax ( y_test , a x i s =1) # For a c c u r a c y c a l c u l a t i o n

f o l d ' s a c c u r a c y

5.2. PART 5.2: USING K-FOLD CROSS-VALIDATION WITH KERAS

169

s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_compare , pred )
print ( f " Fold ␣ s c o r e ␣ ( a c c u r a c y ) : ␣ { s c o r e } " )

# B u i l d t h e oos p r e d i c t i o n l i s t and c a l c u l a t e t h e e r r o r .
oos_y = np . c o n c a t e n a t e ( oos_y )
oos_pred = np . c o n c a t e n a t e ( oos_pred )
oos_y_compare = np . argmax ( oos_y , a x i s =1) # For a c c u r a c y c a l c u l a t i o n

s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( oos_y_compare , oos_pred )
print ( f " F i n a l ␣ s c o r e ␣ ( a c c u r a c y ) : ␣{ s c o r e } " )

# Write t h e c r o s s −v a l i d a t e d p r e d i c t i o n
oos_y = pd . DataFrame ( oos_y )
oos_pred = pd . DataFrame ( oos_pred )
oosDF = pd . c o n c a t (
#oosDF . to_csv ( f i l e n a m e _ w r i t e , i n d e x=F a l s e )

[ df , oos_y , oos_pred ] , a x i s =1 )

Output

Fold #1
Fold s c o r e ( a c c u r a c y ) : 0 . 6 3 2 5
Fold #2
Fold s c o r e ( a c c u r a c y ) : 0 . 6 7 2 5
Fold #3
Fold s c o r e ( a c c u r a c y ) : 0 . 6 9 7 5
Fold #4
Fold s c o r e ( a c c u r a c y ) : 0 . 6 5 7 5
Fold #5
Fold s c o r e ( a c c u r a c y ) : 0 . 6 7 5
F i n a l

s c o r e ( a c c u r a c y ) : 0 . 6 6 7

5.2.4 Training with both a Cross-Validation and a Holdout Set

If you have a considerable amount of data, it is always valuable to set aside a holdout set before you
cross-validate. This holdout set will be the ﬁnal evaluation before using your model for its real-world use.
Figure ?? HOLDOUT shows this division.

The following program uses a holdout set and then still cross-validates.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

170

CHAPTER 5. REGULARIZATION AND DROPOUT

from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# Generate dummies f o r p r o d u c t
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' p r o d u c t ' ] , p r e f i x=" p r o d u c t " ) ] , a x i s =1)
d f . drop ( ' p r o d u c t ' , a x i s =1,

i n p l a c e=True )

f o r income
# M i s s i n g v a l u e s
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

# Convert
t o numpy − C l a s s i f i c a t i o n
x_columns = d f . columns . drop ( ' age ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
y = d f [ ' age ' ] . v a l u e s

Now that the data has been preprocessed, we are ready to build the neural network.

Code

from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s

5.2. PART 5.2: USING K-FOLD CROSS-VALIDATION WITH KERAS

171

from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import KFold

# Keep a 10% h o l d o u t
x_main , x_holdout , y_main , y_holdout = t r a i n _ t e s t _ s p l i t (

x , y ,

t e s t _ s i z e =0.10)

# Cross−v a l i d a t e
k f = KFold ( 5 )

oos_y = [ ]
oos_pred = [ ]
f o l d = 0
for t r a i n ,

t e s t

in k f . s p l i t ( x_main ) :

f o l d+=1
print ( f " Fold ␣#{ f o l d } " )

x _ t r a i n = x_main [ t r a i n ]
y _ t r a i n = y_main [ t r a i n ]
x _ te s t = x_main [ t e s t ]
y _ te s t = y_main [ t e s t ]

model = S e q u e n t i a l ( )
model . add ( Dense ( 2 0 ,
model . add ( Dense ( 5 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

v e r b o s e =0, e po c h s=EPOCHS)

pred = model . p r e d i c t ( x_ t e s t )

oos_y . append ( y _ t es t )
oos_pred . append ( pred )

# Measure a c c u r a c y
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ t es t ) )
print ( f " Fold ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

# B u i l d t h e oos p r e d i c t i o n l i s t and c a l c u l a t e t h e e r r o r .

172

CHAPTER 5. REGULARIZATION AND DROPOUT

oos_y = np . c o n c a t e n a t e ( oos_y )
oos_pred = np . c o n c a t e n a t e ( oos_pred )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( oos_pred , oos_y ) )
print ( )
print ( f " Cross−v a l i d a t e d ␣ s c o r e ␣ (RMSE) : ␣ { s c o r e } " )

# Write t h e c r o s s −v a l i d a t e d p r e d i c t i o n ( from t h e l a s t n e u r a l network )
holdout_pred = model . p r e d i c t ( x_holdout )

s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( holdout_pred , y_holdout ) )
print ( f " Holdout ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

Output

Fold #1
Fold s c o r e (RMSE) : 0 . 5 4 4 1 9 5 2 9 9 2 1 6 6 9 6
Fold #2
Fold s c o r e (RMSE) : 0 . 4 8 0 7 0 5 9 9 3 4 2 9 1 0 3 5 3
Fold #3
Fold s c o r e (RMSE) : 0 . 7 0 3 4 5 8 4 7 6 5 9 2 8 9 9 8
Fold #4
Fold s c o r e (RMSE) : 0 . 5 3 9 7 1 4 1 7 8 5 1 9 0 4 7 3
Fold #5
Fold s c o r e (RMSE) : 2 4 . 1 2 6 2 0 5 2 1 3 0 8 0 0 7 7
Cross−v a l i d a t e d s c o r e (RMSE) : 1 0 . 8 0 1 7 3 2 7 3 1 2 0 7 9 4 7
s c o r e (RMSE) : 2 4 . 0 9 7 6 5 7 9 4 7 2 9 7 6 7 7
Holdout

5.3 Part 5.3: L1 and L2 Regularization to Decrease Overﬁtting

L1 and L2 regularization are two common regularization techniques that can reduce the eﬀects of overﬁt-
ting[26]. These algorithms can either work with an objective function or as a part of the backpropagation
algorithm. In both cases, the regularization algorithm is attached to the training algorithm by adding an
objective.

These algorithms work by adding a weight penalty to the neural network training. This penalty en-
courages the neural network to keep the weights to small values. Both L1 and L2 calculate this penalty
diﬀerently. You can add this penalty calculation to the calculated gradients for gradient-descent-based
algorithms, such as backpropagation. The penalty is negatively combined with the objective score for
objective-function-based training, such as simulated annealing.

Both L1 and L2 work diﬀerently in that they penalize the size of the weight. L2 will force the weights
into a pattern similar to a Gaussian distribution; the L1 will force the weights into a pattern similar to a
Laplace distribution, as demonstrated in Figure 5.3.

5.3. PART 5.3: L1 AND L2 REGULARIZATION TO DECREASE OVERFITTING

173

As you can see, L1 algorithm is more tolerant of weights further from 0, whereas the L2 algorithm is
less tolerant. We will highlight other important diﬀerences between L1 and L2 in the following sections.
You also need to note that both L1 and L2 count their penalties based only on weights; they do not count
penalties on bias values. Keras allows l1/l2 to be directly added to your network.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

f o r income
# M i s s i n g v a l u e s
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

We now create a Keras network with L1 regression.

174

Code

CHAPTER 5. REGULARIZATION AND DROPOUT

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import KFold
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s

# Cross−v a l i d a t e
k f = KFold ( 5 ,

s h u f f l e=True ,

random_state =42)

oos_y = [ ]
oos_pred = [ ]
f o l d = 0

for t r a i n ,

t e s t

in k f . s p l i t ( x ) :

f o l d+=1
print ( f " Fold ␣#{ f o l d } " )

x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

#k e r n e l _ r e g u l a r i z e r=r e g u l a r i z e r s . l 2 ( 0 . 0 1 ) ,

model = S e q u e n t i a l ( )
# Hidden 1
model . add ( Dense ( 5 0 ,

input_dim=x . shape [ 1 ] ,

a c t i v a t i o n= ' r e l u ' ,

a c t i v i t y _ r e g u l a r i z e r=r e g u l a r i z e r s . l 1 ( 1 e −4)))

# Hidden 2
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ,

a c t i v i t y _ r e g u l a r i z e r=r e g u l a r i z e r s . l 1 ( 1 e −4)))

# Output
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) )
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

v e r b o s e =0, e po c h s =500)

5.3. PART 5.3: L1 AND L2 REGULARIZATION TO DECREASE OVERFITTING

175

pred = model . p r e d i c t ( x_ t e s t )

oos_y . append ( y _ t es t )
# raw p r o b a b i l i t i e s
pred = np . argmax ( pred , a x i s =1)
oos_pred . append ( pred )

t o chosen c l a s s

( h i g h e s t p r o b a b i l i t y )

f o l d ' s a c c u r a c y

# Measure t h i s
y_compare = np . argmax ( y_test , a x i s =1) # For a c c u r a c y c a l c u l a t i o n
s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_compare , pred )
print ( f " Fold ␣ s c o r e ␣ ( a c c u r a c y ) : ␣ { s c o r e } " )

# B u i l d t h e oos p r e d i c t i o n l i s t and c a l c u l a t e t h e e r r o r .
oos_y = np . c o n c a t e n a t e ( oos_y )
oos_pred = np . c o n c a t e n a t e ( oos_pred )
oos_y_compare = np . argmax ( oos_y , a x i s =1) # For a c c u r a c y c a l c u l a t i o n

s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( oos_y_compare , oos_pred )
print ( f " F i n a l ␣ s c o r e ␣ ( a c c u r a c y ) : ␣{ s c o r e } " )

# Write t h e c r o s s −v a l i d a t e d p r e d i c t i o n
oos_y = pd . DataFrame ( oos_y )
oos_pred = pd . DataFrame ( oos_pred )
oosDF = pd . c o n c a t (
#oosDF . to_csv ( f i l e n a m e _ w r i t e , i n d e x=F a l s e )

[ df , oos_y , oos_pred ] , a x i s =1 )

Output

Fold #1
Fold s c o r e ( a c c u r a c y ) : 0 . 6 4
Fold #2
Fold s c o r e ( a c c u r a c y ) : 0 . 6 7 7 5
Fold #3
Fold s c o r e ( a c c u r a c y ) : 0 . 6 8 2 5
Fold #4
Fold s c o r e ( a c c u r a c y ) : 0 . 6 6 7 5
Fold #5
Fold s c o r e ( a c c u r a c y ) : 0 . 6 4 5
F i n a l

s c o r e ( a c c u r a c y ) : 0 . 6 6 2 5

176

CHAPTER 5. REGULARIZATION AND DROPOUT

5.4 Part 5.4: Drop Out for Keras to Decrease Overﬁtting

Hinton, Srivastava, Krizhevsky, Sutskever, Salakhutdinov (2012) introduced the dropout regularization
algorithm.[33]Although dropout works diﬀerently than L1 and L2, it accomplishes the same goal---the pre-
vention of overﬁtting. However, the algorithm does the task by actually removing neurons and connections-
--at least temporarily. Unlike L1 and L2, no weight penalty is added. Dropout does not directly seek to
train small weights.
Dropout works by causing hidden neurons of the neural network to be unavailable during part of the train-
ing. Dropping part of the neural network causes the remaining portion to be trained to still achieve a good
score even without the dropped neurons. This technique decreases co-adaptation between neurons, which
results in less overﬁtting.

Most neural network frameworks implement dropout as a separate layer. Dropout layers function like
a regular, densely connected neural network layer. The only diﬀerence is that the dropout layers will
periodically drop some of their neurons during training. You can use dropout layers on regular feedforward
neural networks.

The program implements a dropout layer as a dense layer that can eliminate some of its neurons.
Contrary to popular belief about the dropout layer, the program does not permanently remove these
discarded neurons. A dropout layer does not lose any of its neurons during the training process, and it will
still have the same number of neurons after training. In this way, the program only temporarily masks the
neurons rather than dropping them.
Figure 5.4 shows how a dropout layer might be situated with other layers.

The discarded neurons and their connections are shown as dashed lines. The input layer has two input
neurons as well as a bias neuron. The second layer is a dense layer with three neurons and a bias neuron.
The third layer is a dropout layer with six regular neurons even though the program has dropped 50% of
them. While the program drops these neurons, it neither calculates nor trains them. However, the ﬁnal
neural network will use all of these neurons for the output. As previously mentioned, the program only
temporarily discards the neurons.

The program chooses diﬀerent sets of neurons from the dropout layer during subsequent training iter-
ations. Although we chose a probability of 50% for dropout, the computer will not necessarily drop three
neurons. It is as if we ﬂipped a coin for each of the dropout candidate neurons to choose if that neuron
was dropped out. You must know that the program should never drop the bias neuron. Only the regular
neurons on a dropout layer are candidates.
The implementation of the training algorithm inﬂuences the process of discarding neurons. The dropout
set frequently changes once per training iteration or batch. The program can also provide intervals where
all neurons are present. Some neural network frameworks give additional hyper-parameters to allow you
to specify exactly the rate of this interval.

Why dropout is capable of decreasing overﬁtting is a common question. The answer is that dropout
can reduce the chance of codependency developing between two neurons. Two neurons that develop code-
pendency will not be able to operate eﬀectively when one is dropped out. As a result, the neural network
can no longer rely on the presence of every neuron, and it trains accordingly. This characteristic decreases
its ability to memorize the information presented, thereby forcing generalization.

Dropout also decreases overﬁtting by forcing a bootstrapping process upon the neural network. Boot-
strapping is a prevalent ensemble technique. Ensembling is a technique of machine learning that combines
multiple models to produce a better result than those achieved by individual models. The ensemble is a

5.4. PART 5.4: DROP OUT FOR KERAS TO DECREASE OVERFITTING

177

term that originates from the musical ensembles in which the ﬁnal music product that the audience hears
is the combination of many instruments.

Bootstrapping is one of the most simple ensemble techniques. The bootstrapping programmer simply
trains several neural networks to perform precisely the same task. However, each neural network will
perform diﬀerently because of some training techniques and the random numbers used in the neural network
weight initialization. The diﬀerence in weights causes the performance variance. The output from this
ensemble of neural networks becomes the average output of the members taken together. This process
decreases overﬁtting through the consensus of diﬀerently trained neural networks.

Dropout works somewhat like bootstrapping. You might think of each neural network that results
from a diﬀerent set of neurons being dropped out as an individual member in an ensemble. As training
progresses, the program creates more neural networks in this way. However, dropout does not require the
same amount of processing as bootstrapping. The new neural networks created are temporary; they exist
only for a training iteration. The ﬁnal result is also a single neural network rather than an ensemble of
neural networks to be averaged together.

The following animation shows how dropout works: animation link

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

178

CHAPTER 5. REGULARIZATION AND DROPOUT

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

Now we will see how to apply dropout to classiﬁcation.

Code

########################################
# Keras w i t h d r o p o u t
########################################

f o r C l a s s i f i c a t i o n

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import KFold
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n , Dropout
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s

# Cross−v a l i d a t e
k f = KFold ( 5 ,

s h u f f l e=True ,

random_state =42)

oos_y = [ ]
oos_pred = [ ]
f o l d = 0

for t r a i n ,

t e s t

in k f . s p l i t ( x ) :

f o l d+=1
print ( f " Fold ␣#{ f o l d } " )

x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

#k e r n e l _ r e g u l a r i z e r=r e g u l a r i z e r s . l 2 ( 0 . 0 1 ) ,

5.4. PART 5.4: DROP OUT FOR KERAS TO DECREASE OVERFITTING

179

model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dropout ( 0 . 5 ) )
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' , \

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

a c t i v i t y _ r e g u l a r i z e r=r e g u l a r i z e r s . l 1 ( 1 e −4))) # Hidden 2

# U s u a l l y do not add d r o p o u t a f t e r
#model . add ( Dropout ( 0 . 5 ) )
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

f i n a l h i d d e n l a y e r

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) , \

v e r b o s e =0, e po c h s =500)

pred = model . p r e d i c t ( x_ t e s t )

oos_y . append ( y _ t es t )
# raw p r o b a b i l i t i e s
pred = np . argmax ( pred , a x i s =1)
oos_pred . append ( pred )

t o chosen c l a s s

( h i g h e s t p r o b a b i l i t y )

f o l d ' s a c c u r a c y

# Measure t h i s
y_compare = np . argmax ( y_test , a x i s =1) # For a c c u r a c y c a l c u l a t i o n
s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_compare , pred )
print ( f " Fold ␣ s c o r e ␣ ( a c c u r a c y ) : ␣ { s c o r e } " )

# B u i l d t h e oos p r e d i c t i o n l i s t and c a l c u l a t e t h e e r r o r .
oos_y = np . c o n c a t e n a t e ( oos_y )
oos_pred = np . c o n c a t e n a t e ( oos_pred )
oos_y_compare = np . argmax ( oos_y , a x i s =1) # For a c c u r a c y c a l c u l a t i o n

s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( oos_y_compare , oos_pred )
print ( f " F i n a l ␣ s c o r e ␣ ( a c c u r a c y ) : ␣{ s c o r e } " )

# Write t h e c r o s s −v a l i d a t e d p r e d i c t i o n
oos_y = pd . DataFrame ( oos_y )
oos_pred = pd . DataFrame ( oos_pred )
oosDF = pd . c o n c a t (
#oosDF . to_csv ( f i l e n a m e _ w r i t e , i n d e x=F a l s e )

[ df , oos_y , oos_pred ] , a x i s =1 )

Output

180

CHAPTER 5. REGULARIZATION AND DROPOUT

Fold #1
Fold s c o r e ( a c c u r a c y ) : 0 . 6 8
Fold #2
Fold s c o r e ( a c c u r a c y ) : 0 . 6 9 5
Fold #3
Fold s c o r e ( a c c u r a c y ) : 0 . 7 4 2 5
Fold #4
Fold s c o r e ( a c c u r a c y ) : 0 . 7 1
Fold #5
Fold s c o r e ( a c c u r a c y ) : 0 . 6 6 2 5
s c o r e ( a c c u r a c y ) : 0 . 6 9 8
F i n a l

5.5 Part 5.5: Benchmarking Regularization Techniques

Quite a few hyperparameters have been introduced so far. Tweaking each of these values can have an eﬀect
on the score obtained by your neural networks. Some of the hyperparameters seen so far include:

• Number of layers in the neural network
• How many neurons in each layer
• What activation functions to use on each layer
• Dropout percent on each layer
• L1 and L2 values on each layer

To try out each of these hyperparameters you will need to run train neural networks with multiple settings
for each hyperparameter. However, you may have noticed that neural networks often produce somewhat
diﬀerent results when trained multiple times. This is because the neural networks start with random
weights. Because of this it is necessary to ﬁt and evaluate a neural network times to ensure that one
set of hyperparameters are actually better than another. Bootstrapping can be an eﬀective means of
benchmarking (comparing) two sets of hyperparameters.

Bootstrapping is similar to cross-validation. Both go through a number of cycles/folds providing vali-
dation and training sets. However, bootstrapping can have an unlimited number of cycles. Bootstrapping
chooses a new train and validation split each cycle, with replacement. The fact that each cycle is chosen
with replacement means that, unlike cross validation, there will often be repeated rows selected between
cycles. If you run the bootstrap for enough cycles, there will be duplicate cycles.

In this part we will use bootstrapping for hyperparameter benchmarking. We will train a neural network
for a speciﬁed number of splits (denoted by the SPLITS constant). For these examples we use 100. We
will compare the average score at the end of the 100. By the end of the cycles the mean score will have
converged somewhat. This ending score will be a much better basis of comparison than a single cross-
validation. Additionally, the average number of epochs will be tracked to give an idea of a possible optimal
value. Because the early stopping validation set is also used to evaluate the the neural network as well,
it might be slightly inﬂated. This is because we are both stopping and evaluating on the same sample.

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

181

However, we are using the scores only as relative measures to determine the superiority of one set of
hyperparameters to another, so this slight inﬂation should not present too much of a problem.

Because we are benchmarking, we will display the amount of time taken for each cycle. The following

function can be used to nicely format a time span.

Code

# N i c e l y f o r m a t t e d time s t r i n g
def hms_string ( s e c _ e l a p s e d ) :

h = int ( s e c _ e l a p s e d / ( 6 0 ∗ 6 0 ) )
m = int ( ( s e c _ e l a p s e d % ( 6 0 ∗ 6 0 ) ) / 6 0 )
s = s e c _ e l a p s e d % 60
return " { } : { : > 0 2 } : { : > 0 5 . 2 f } " . format ( h , m,

s )

5.5.1 Bootstrapping for Regression

Regression bootstrapping uses the ShuﬄeSplit object to perform the splits. This technique is similar
to KFold for cross-validation; no balancing occurs. We will attempt to predict the age column for the
jh-simple-dataset; the following code loads this data.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# Generate dummies f o r p r o d u c t
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' p r o d u c t ' ] , p r e f i x=" p r o d u c t " ) ] , a x i s =1)
d f . drop ( ' p r o d u c t ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s

f o r income

182

CHAPTER 5. REGULARIZATION AND DROPOUT

med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

# Convert
t o numpy − C l a s s i f i c a t i o n
x_columns = d f . columns . drop ( ' age ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
y = d f [ ' age ' ] . v a l u e s

The following code performs the bootstrap. The architecture of the neural network can be adjusted to

compare many diﬀerent conﬁgurations.

Code

import pandas a s pd
import o s
import numpy a s np
import time
import s t a t i s t i c s
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import S h u f f l e S p l i t

SPLITS = 50

# B o o t s t r a p
boot = S h u f f l e S p l i t ( n _ s p l i t s=SPLITS ,

t e s t _ s i z e =0.1 , random_state =42)

# Track p r o g r e s s
mean_benchmark = [ ]
epochs_needed = [ ]
num = 0

# Loop t h r o u g h s a m p l e s

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

183

for t r a i n ,

t e s t

in boot . s p l i t ( x ) :

s t a r t _ t i m e = time . time ( )
num+=1

t r a i n and t e s t

# S p l i t
x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

# C o n s t r u c t n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 0 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

input_dim=x _ t r a i n . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =0, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

# Train on t h e b o o t s t r a p sample
model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s =1000)

e po c h s = monitor . stopped_epoch
epochs_needed . append ( e po c h s )

# P r e d i c t on t h e o u t o f b o o t ( v a l i d a t i o n )
pred = model . p r e d i c t ( x_ t e s t )

l o g l o s s

# Measure t h i s b o o t s t r a p ' s
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
mean_benchmark . append ( s c o r e )
m1 = s t a t i s t i c s . mean ( mean_benchmark )
m2 = s t a t i s t i c s . mean ( epochs_needed )
mdev = s t a t i s t i c s . p s t d e v ( mean_benchmark )

# Record t h i s
i t e r a t i o n
time_took = time . time ( ) − s t a r t _ t i m e
print ( f "#{num } : ␣ s c o r e ={ s c o r e : . 6 f } , ␣mean␣ s c o r e ={m1 : . 6 f } , "

f " ␣ s t d e v={mdev : . 6 f } " ,
f " ␣ e p o ch s={e po c h s } , ␣mean␣ e p o c h s={ i n t (m2) } " ,
f " ␣ time={hms_string ( time_took ) } " )

184

Output

CHAPTER 5. REGULARIZATION AND DROPOUT

time = 0 : 0 0 : 2 0 . 8 5

time = 0 : 0 0 : 0 8 . 7 0

time = 0 : 0 0 : 1 2 . 5 6

#1: s c o r e =0.630750 , mean s c o r e =0.630750 ,
mean e p oc h s =147
#2: s c o r e =1.020895 , mean s c o r e =0.825823 ,
mean e p oc h s =124
#3: s c o r e =0.803801 , mean s c o r e =0.818482 ,
mean e p oc h s =134
#4: s c o r e =0.540871 , mean s c o r e =0.749079 ,
mean e p oc h s =131
#5: s c o r e =0.802589 , mean s c o r e =0.759781 ,
mean e p oc h s =128
#6: s c o r e =0.862807 , mean s c o r e =0.776952 ,
mean e p oc h s =124
#7: s c o r e =0.550373 , mean s c o r e =0.744584 ,
mean e p oc h s =125
#8: s c o r e =0.659148 , mean s c o r e =0.733904 ,

time = 0 : 0 0 : 1 0 . 6 4

time = 0 : 0 0 : 1 0 . 6 5

time = 0 : 0 0 : 1 0 . 8 4

time = 0 : 0 0 : 1 0 . 8 5

. . .

s c o r e =0.911419 , mean s c o r e =0.747607 ,

mean e p oc h s =116
#49:
mean e p oc h s =116
#50:
mean e p oc h s =116

time = 0 : 0 0 : 0 9 . 3 3

time = 0 : 0 0 : 1 0 . 6 6

time = 0 : 0 0 : 2 0 . 9 1

s c o r e =0.599252 , mean s c o r e =0.744639 ,

s t d e v =0.000000

e p o ch s =147 ,

s t d e v =0.195072

e p o ch s =101 ,

s t d e v =0.159614

e p o ch s =155 ,

s t d e v =0.183188

e p o ch s =122 ,

s t d e v =0.165240

e p o ch s =116 ,

s t d e v =0.155653

e p o ch s =108 ,

s t d e v =0.164478

e p o ch s =131 ,

s t d e v =0.156428

e p o ch s =118 ,

s t d e v =0.185098

e po c h s =124 ,

s t d e v =0.184411

e po c h s =132 ,

The bootstrapping process for classiﬁcation is similar, and I present it in the next section.

5.5.2 Bootstrapping for Classiﬁcation

Regression bootstrapping uses the StratiﬁedShuﬄeSplit class to perform the splits. This class is similar
to StratiﬁedKFold for cross-validation, as the classes are balanced so that the sampling does not aﬀect
proportions. To demonstrate this technique, we will attempt to predict the product column for the jh-
simple-dataset; the following code loads this data.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

185

na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

f o r income
# M i s s i n g v a l u e s
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

We now run this data through a number of splits speciﬁed by the SPLITS variable. We track the

average error through each of these splits.

Code

import pandas a s pd
import o s
import numpy a s np
import time
import s t a t i s t i c s
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s

186

CHAPTER 5. REGULARIZATION AND DROPOUT

from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d S h u f f l e S p l i t

SPLITS = 50

# B o o t s t r a p
boot = S t r a t i f i e d S h u f f l e S p l i t ( n _ s p l i t s=SPLITS ,

t e s t _ s i z e =0.1 ,

random_state =42)

# Track p r o g r e s s
mean_benchmark = [ ]
epochs_needed = [ ]
num = 0

# Loop t h r o u g h s a m p l e s
for t r a i n ,

t e s t

in boot . s p l i t ( x , d f [ ' p r o d u c t ' ] ) :

s t a r t _ t i m e = time . time ( )
num+=1

t r a i n and t e s t

# S p l i t
x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

# C o n s t r u c t n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

p a t i e n c e =25 , v e r b o s e =0, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

# Train on t h e b o o t s t r a p sample
model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s =1000)

e po c h s = monitor . stopped_epoch
epochs_needed . append ( e po c h s )

# P r e d i c t on t h e o u t o f b o o t ( v a l i d a t i o n )
pred = model . p r e d i c t ( x_ t e s t )

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

187

l o g l o s s

# Measure t h i s b o o t s t r a p ' s
y_compare = np . argmax ( y_test , a x i s =1) # For l o g l o s s
s c o r e = m e t r i c s . l o g _ l o s s ( y_compare , pred )
mean_benchmark . append ( s c o r e )
m1 = s t a t i s t i c s . mean ( mean_benchmark )
m2 = s t a t i s t i c s . mean ( epochs_needed )
mdev = s t a t i s t i c s . p s t d e v ( mean_benchmark )

c a l c u l a t i o n

# Record t h i s
i t e r a t i o n
time_took = time . time ( ) − s t a r t _ t i m e
print ( f "#{num } : ␣ s c o r e ={ s c o r e : . 6 f } , ␣mean␣ s c o r e ={m1 : . 6 f } , " +\

f " s t d e v={mdev : . 6 f } , ␣ e p oc h s={ep o c hs } , ␣mean␣ e po c h s={ i n t (m2) } , " +\
f " ␣ time={hms_string ( time_took ) } " )

Output

#1: s c o r e =0.666342 , mean s c o r e =0.666342 , s t d e v =0.000000 , ep o c hs =66 ,
mean e p oc h s =66 , time = 0 : 0 0 : 0 6 . 3 1
#2: s c o r e =0.645598 , mean s c o r e =0.655970 , s t d e v =0.010372 , ep o c hs =59 ,
mean e p oc h s =62 , time = 0 : 0 0 : 1 0 . 6 3
#3: s c o r e =0.676924 , mean s c o r e =0.662955 , s t d e v =0.013011 , ep o c hs =66 ,
mean e p oc h s =63 , time = 0 : 0 0 : 1 0 . 6 4
#4: s c o r e =0.672602 , mean s c o r e =0.665366 , s t d e v =0.012017 , ep o c hs =84 ,
mean e p oc h s =68 , time = 0 : 0 0 : 0 8 . 2 0
#5: s c o r e =0.667274 , mean s c o r e =0.665748 , s t d e v =0.010776 , ep o c hs =73 ,
mean e p oc h s =69 , time = 0 : 0 0 : 1 0 . 6 5
#6: s c o r e =0.706372 , mean s c o r e =0.672518 , s t d e v =0.018055 , ep o c hs =50 ,
mean e p oc h s =66 , time = 0 : 0 0 : 0 4 . 8 1
#7: s c o r e =0.687937 , mean s c o r e =0.674721 , s t d e v =0.017565 , ep o c hs =71 ,
mean e p oc h s =67 , time = 0 : 0 0 : 0 6 . 8 9
#8: s c o r e =0.734794 , mean s c o r e =0.682230 , s t d e v =0.025781 , ep o c hs =43 ,

. . .

mean e p oc h s =66 , time = 0 : 0 0 : 0 4 . 1 4
#49:
mean e p oc h s =66 , time = 0 : 0 0 : 1 0 . 6 5
#50:
mean e p oc h s =65 , time = 0 : 0 0 : 0 5 . 2 2

s c o r e =0.665493 , mean s c o r e =0.673305 , s t d e v =0.049060 , e p o c h s =60 ,

s c o r e =0.692625 , mean s c o r e =0.673691 , s t d e v =0.048642 , e p o c h s =55 ,

188

CHAPTER 5. REGULARIZATION AND DROPOUT

5.5.3 Benchmarking

Now that we’ve seen how to bootstrap with both classiﬁcation and regression, we can start to try to optimize
the hyperparameters for the jh-simple-dataset data. For this example, we will encode for classiﬁcation
of the product column. Evaluation will be in log loss.

Code

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] ,

d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

a x i s =1)

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

I performed some optimization, and the code has the best settings that I could determine. Later in this

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

189

book, we will see how we can use an automatic process to optimize the hyperparameters.

Code

import pandas a s pd
import o s
import numpy a s np
import time
import t e n s o r f l o w . k e r a s . i n i t i a l i z e r s
import s t a t i s t i c s
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n , Dropout
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d S h u f f l e S p l i t
from t e n s o r f l o w . k e r a s . l a y e r s import LeakyReLU , PReLU

SPLITS = 100

# B o o t s t r a p
boot = S t r a t i f i e d S h u f f l e S p l i t ( n _ s p l i t s=SPLITS ,

t e s t _ s i z e =0.1)

# Track p r o g r e s s
mean_benchmark = [ ]
epochs_needed = [ ]
num = 0

# Loop t h r o u g h s a m p l e s
fo r t r a i n ,

t e s t

in boot . s p l i t ( x , d f [ ' p r o d u c t ' ] ) :

s t a r t _ t i m e = time . time ( )
num+=1

t r a i n and t e s t

# S p l i t
x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ t e s t = x [ t e s t ]
y _ t e s t = y [ t e s t ]

# C o n s t r u c t n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,

input_dim=x . shape [ 1 ] , a c t i v a t i o n=PReLU ( ) , \

k e r n e l _ r e g u l a r i z e r=r e g u l a r i z e r s . l 2 ( 1 e −4))) # Hidden 1

190

CHAPTER 5. REGULARIZATION AND DROPOUT

model . add ( Dropout ( 0 . 5 ) )
model . add ( Dense ( 1 0 0 , a c t i v a t i o n=PReLU ( ) , \

a c t i v i t y _ r e g u l a r i z e r=r e g u l a r i z e r s . l 2 ( 1 e −4))) # Hidden 2

model . add ( Dropout ( 0 . 5 ) )
model . add ( Dense ( 1 0 0 , a c t i v a t i o n=PReLU ( ) , \

a c t i v i t y _ r e g u l a r i z e r=r e g u l a r i z e r s . l 2 ( 1 e −4)

) ) # Hidden 3
model . add ( Dropout ( 0 . 5 ) ) − U s u a l l y b e t t e r performance

#
# w i t h o u t d r o p o u t on f i n a l

l a y e r

model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =100 , v e r b o s e =0, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

# Train on t h e b o o t s t r a p sample
model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) , \

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p o c h s =1000)

e po c h s = monitor . stopped_epoch
epochs_needed . append ( e p o c h s )

# P r e d i c t on t h e o u t o f b o o t ( v a l i d a t i o n )
pred = model . p r e d i c t ( x_ t e s t )

l o g l o s s

# Measure t h i s b o o t s t r a p ' s
y_compare = np . argmax ( y_test , a x i s =1) # For l o g l o s s
s c o r e = m e t r i c s . l o g _ l o s s ( y_compare , pred )
mean_benchmark . append ( s c o r e )
m1 = s t a t i s t i c s . mean ( mean_benchmark )
m2 = s t a t i s t i c s . mean ( epochs_needed )
mdev = s t a t i s t i c s . p s t d e v ( mean_benchmark )

c a l c u l a t i o n

i t e r a t i o n
# Record t h i s
time_took = time . time ( ) − s t a r t _ t i m e
print ( f "#{num } : ␣ s c o r e ={ s c o r e : . 6 f } , ␣mean␣ s c o r e ={m1 : . 6 f } , "

f " s t d e v={mdev : . 6 f } , ␣ e p oc h s={ep o c hs } , "
f " mean␣ e po c h s={ i n t (m2) } , ␣ time={hms_string ( time_took ) } " )

Output

#1: s c o r e =0.642887 , mean s c o r e =0.642887 , s t d e v =0.000000 ,
e p o c h s =325 ,mean e p o c hs =325 , time = 0 : 0 0 : 4 2 . 1 0
#2: s c o r e =0.555518 , mean s c o r e =0.599202 , s t d e v =0.043684 ,

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

191

e p oc h s =208 ,mean e p o c hs =266 , time = 0 : 0 0 : 4 1 . 7 4
#3: s c o r e =0.605537 , mean s c o r e =0.601314 , s t d e v =0.035793 ,
e p oc h s =187 ,mean e p o c hs =240 , time = 0 : 0 0 : 2 4 . 2 2
#4: s c o r e =0.609415 , mean s c o r e =0.603339 , s t d e v =0.031195 ,
e p oc h s =250 ,mean e p o c hs =242 , time = 0 : 0 0 : 4 1 . 7 2
#5: s c o r e =0.619657 , mean s c o r e =0.606603 , s t d e v =0.028655 ,
e p oc h s =201 ,mean e p o c hs =234 , time = 0 : 0 0 : 2 6 . 1 0
#6: s c o r e =0.638641 , mean s c o r e =0.611943 , s t d e v =0.028755 ,
e p oc h s =172 ,mean e p o c hs =223 , time = 0 : 0 0 : 4 1 . 7 3
#7: s c o r e =0.671137 , mean s c o r e =0.620399 , s t d e v =0.033731 ,
e p oc h s =203 ,mean e p o c hs =220 , time = 0 : 0 0 : 2 6 . 5 8
#8: s c o r e =0.635294 , mean s c o r e =0.622261 , s t d e v =0.031935 ,

. . .

s c o r e =0.697473 , mean s c o r e =0.649279 , s t d e v =0.042577 ,

e p oc h s =173 ,mean e p o c hs =196 , time = 0 : 0 0 : 2 2 . 7 0
#99:
e p oc h s =172 ,mean e p o c hs =196 , time = 0 : 0 0 : 4 1 . 7 9
#100:
e p oc h s =169 ,mean e p o c hs =196 , time = 0 : 0 0 : 2 1 . 9 0

s c o r e =0.678298 , mean s c o r e =0.649569 , s t d e v =0.042462 ,

192

CHAPTER 5. REGULARIZATION AND DROPOUT

Figure 5.1: K-Fold Crossvalidation

5.5. PART 5.5: BENCHMARKING REGULARIZATION TECHNIQUES

193

Figure 5.2: Cross-Validation and a Holdout Set

Figure 5.3: L1 vs L2

194

CHAPTER 5. REGULARIZATION AND DROPOUT

Figure 5.4: Dropout Regularization

Chapter 6

Convolutional Neural Networks
(CNN) for Computer Vision

6.1 Part 6.1: Image Processing in Python

Computer vision requires processing images. These images might come from a video stream, a camera, or
ﬁles on a storage drive. We begin this chapter by looking at how to process images with Python. To use
images in Python, we will make use of the Pillow package. The following program uses Pillow to load and
display an image.

Code

I m a g e F i l e

from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO
import numpy a s np

%m a t p l o t l i b i n l i n e

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "

r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )

' M o z i l l a / 5 . 0 ' } )

print ( np . a s a r r a y ( img ) )

img

195

196

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Output

[ [ [ 1 9 9 213 2 4 0 ]
[ 2 0 0 214 2 4 0 ]
[ 2 0 0 214 2 4 0 ]
. . .
[ 86
[ 48
[ 57

34
4
21

9 6 ]
5 7 ]
6 5 ] ]

[ [ 1 9 9 213 2 3 9 ]
[ 2 0 0 214 2 4 0 ]
[ 2 0 0 214 2 4 0 ]
. . .
[ 2 1 5 215 2 5 1 ]
[ 2 5 2 242 2 5 5 ]
[ 2 3 7 218 2 5 0 ] ]

[ [ 2 0 0 214 2 4 0 ]

. . .

[ 1 3 1
. . .
[ 86
[ 89

98

9 1 ]

82
85

5 7 ]
6 0 ]

6.1. PART 6.1: IMAGE PROCESSING IN PYTHON

197

[ 89

85

6 0 ] ] ]

6.1.1 Creating Images from Pixels in Python

You can use Pillow to create an image from a 3D NumPy cube-shaped array. The rows and columns
specify the pixels. The third dimension (size 3) deﬁnes red, green, and blue color values. The following
code demonstrates creating a simple image from a NumPy array.

Code

from PIL import Image
import numpy a s np

w, h = 6 4 , 64
data = np . z e r o s ( ( h , w, 3 ) , dtype=np . u i n t 8 )

# Y e l l o w
for row in range ( 3 2 ) :

for c o l

in range ( 3 2 ) :

data [ row , c o l ] = [ 2 5 5 , 2 5 5 , 0 ]

# Red
for row in range ( 3 2 ) :

for c o l

in range ( 3 2 ) :

data [ row +32 , c o l ] = [ 2 5 5 , 0 , 0 ]

# Green
fo r row in range ( 3 2 ) :

fo r c o l

in range ( 3 2 ) :

data [ row +32 , c o l +32] = [ 0 , 2 5 5 , 0 ]

# Blue
fo r row in range ( 3 2 ) :

fo r c o l

in range ( 3 2 ) :

data [ row , c o l +32] = [ 0 , 0 , 2 5 5 ]

img = Image . f r o m a r r a y ( data ,
img

'RGB ' )

Output

198

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

6.1.2 Transform Images in Python (at the pixel level)

We can combine the last two programs and modify images. Here we take the mean color of each pixel and
form a grayscale image.

Code

from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO

I m a g e F i l e

%m a t p l o t l i b i n l i n e

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "
r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :

' M o z i l l a / 5 . 0 ' } )

img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )

img_array = np . a s a r r a y ( img )
rows = img_array . shape [ 0 ]
c o l s = img_array . shape [ 1 ]

print ( " Rows : ␣ { } , ␣ C o l s : ␣ {} " . format ( rows , c o l s ) )

# Create new image
img2_array = np . z e r o s ( ( rows ,
for row in range ( rows ) :

fo r c o l

in range ( c o l s ) :

c o l s , 3 ) , dtype=np . u i n t 8 )

t = np . mean ( img_array [ row , c o l ] )
img2_array [ row , c o l ] = [ t , t , t ]

img2 = Image . f r o m a r r a y ( img2_array ,
img2

'RGB ' )

6.1. PART 6.1: IMAGE PROCESSING IN PYTHON

199

Output

Rows : 7 6 8 , C o l s : 1024

6.1.3 Standardize Images

When processing several images together, it is sometimes essential to standardize them. The following
code reads a sequence of images and causes them to all be of the same size and perfectly square. If the
input images are not square, cropping will occur.

Code

I m a g e F i l e

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
import numpy a s np
from i o import BytesIO
from IPython . d i s p l a y import d i s p l a y , HTML

images = [

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / S e i g l e H a l l . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g "

200

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

]

def crop_square ( image ) :

width , h e i g h t = image . s i z e

# Crop t h e image , c e n t e r e d
new_width = min( width , h e i g h t )
new_height = new_width
l e f t = ( width − new_width ) / 2
top = ( h e i g h t − new_height ) / 2
r i g h t = ( width + new_width ) / 2
bottom = ( h e i g h t + new_height ) / 2
return image . c r o p ( ( l e f t ,

top ,

r i g h t , bottom ) )

x = [ ]

for u r l

in images :

I m a g e F i l e .LOAD_TRUNCATED_IMAGES = F a l s e
r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )
img = crop_square ( img )
img = img . r e s i z e ( ( 1 2 8 , 1 2 8 ) ,
print ( u r l )
d i s p l a y ( img )
img_array = np . a s a r r a y ( img )
img_array = img_array . f l a t t e n ( )
img_array = img_array . a s t y p e ( np . f l o a t 3 2 )
img_array = ( img_array −128)/128
x . append ( img_array )

Image . ANTIALIAS)

' M o z i l l a / 5 . 0 ' } )

x = np . a r r a y ( x )

print ( x . shape )

Output

6.1. PART 6.1: IMAGE PROCESSING IN PYTHON

201

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / S e i g l e H a l l . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g

( 3 , 4 9 1 5 2 )

6.1.4 Adding Noise to an Image

Sometimes it is beneﬁcial to add noise to images. We might use noise to augment images to generate more
training data or modify images to test the recognition capabilities of neural networks. It is essential to see
how to add noise to an image. There are many ways to add such noise. The following code adds random
black squares to the image to produce noise.

Code

from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO

I m a g e F i l e

%m a t p l o t l i b i n l i n e

202

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

def add_noise ( a ) :
a2 = a . copy ( )
rows = a2 . shape [ 0 ]
c o l s = a2 . shape [ 1 ]
s = int (min( rows , c o l s ) / 2 0 ) # s i z e o f

for i

in range ( 1 0 0 ) :

x = np . random . r a n d i n t ( c o l s −s )
y = np . random . r a n d i n t ( rows−s )
a2 [ y : ( y+s ) , x : ( x+s ) ] = 0

return a2

s p o t

i s 1/20 o f

s m a l l e s t dimension

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "

r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )

' M o z i l l a / 5 . 0 ' } )

img_array = np . a s a r r a y ( img )
rows = img_array . shape [ 0 ]
c o l s = img_array . shape [ 1 ]

print ( " Rows : ␣ { } , ␣ C o l s : ␣ {} " . format ( rows , c o l s ) )

# Create new image
img2_array = img_array . a s t y p e ( np . u i n t 8 )
print ( img2_array . shape )
img2_array = add_noise ( img2_array )
img2 = Image . f r o m a r r a y ( img2_array ,
img2

'RGB ' )

Output

6.1. PART 6.1: IMAGE PROCESSING IN PYTHON

203

Rows : 7 6 8 , C o l s : 1024
( 7 6 8 , 1 0 2 4 , 3 )

6.1.5 Preprocessing Many Images

To download images, we deﬁne several paths. We will download sample images of paperclips from the URL
speciﬁed by DOWNLOAD_SOURCE. Once downloaded, we will unzip and perform the preprocessing
on these paper clips. I mean for this code as a starting point for other image preprocessing.

Code

import o s

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / r e l e a s e s / "
#DOWNLOAD_SOURCE = URL+"download / v1 / i r i s −image . z i p "
DOWNLOAD_SOURCE = URL+" download / v1 / p a p e r c l i p s . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "
EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (PATH,
TARGET = o s . path . j o i n (PATH, " / c o n t e n t / c l i p s −p r o c e s s e d " )

" / c o n t e n t / c l i p s / p a p e r c l i p s " )

e l s e :

204

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

l o c a l l y on my machine , you may need d i f f e r e n t

# I used t h i s
PATH = " / U s e r s / j e f f /temp "
EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (PATH,
TARGET = o s . path . j o i n (PATH, " c l i p s −p r o c e s s e d " )

" c l i p s / p a p e r c l i p s " )

Next, we download the images. This part depends on the origin of your images. The following code

downloads images from a URL, where a ZIP ﬁle contains the images. The code unzips the ZIP ﬁle.

Code

! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}
! mkdir −p {EXTRACT_TARGET}
! u n z i p −o −j −d {SOURCE} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

The following code contains functions that we use to preprocess the images. The crop_square function
converts images to a square by cropping extra data. The scale function increases or decreases the size of
an image. The standardize function ensures an image is full color; a mix of color and grayscale images
can be problematic.

Code

import i m a g e i o
import g l o b
from tqdm import tqdm
from PIL import Image
import o s

def

scale_w idth ,

s c a l e ( img ,
# S c a l e t h e image
img = img . r e s i z e ( (

s c a l e _ h e i g h t ) :

scale_width ,
s c a l e _ h e i g h t ) ,
Image . ANTIALIAS)

return img

def

s t a n d a r d i z e ( image ) :
rgbimg = Image . new ( "RGB" ,
rgbimg . p a s t e ( image )
return rgbimg

image . s i z e )

6.1. PART 6.1: IMAGE PROCESSING IN PYTHON

205

def

f a i l _ b e l o w ( image , check_width , c h e c k_ h e i g ht ) :
width , h e i g h t = image . s i z e
a s s e r t width == check_width
a s s e r t h e i g h t == c h e c k _ h ei g h t

Next, we loop through each image. The images are loaded, and you can apply any desired transforma-

tions. Ultimately, the script saves the images as JPG.

Code

f i l e s = g l o b . g l o b ( o s . path . j o i n (SOURCE, " ∗ . j p g " ) )

for f i l e in tqdm ( f i l e s ) :

try :

t a r g e t = " "
name = o s . path . basename ( f i l e )
f i l e n a m e , _ = o s . path . s p l i t e x t ( name )
img = Image . open ( f i l e )
img = s t a n d a r d i z e ( img )
img = crop_square ( img )
img = s c a l e ( img , 1 2 8 , 1 2 8 )
#f a i l _ b e l o w ( img , 128 , 128)

t a r g e t = o s . path . j o i n (TARGET, f i l e n a m e+" . j p g " )
img . s a v e ( t a r g e t , q u a l i t y =25)

except K e y b o a r d I n t e r r u p t :

print ( " Keyboard ␣ i n t e r r u p t " )
break

except A s s e r t i o n E r r o r :
print ( " A s s e r t i o n " )
break

except :

print ( " Unexpected ␣ e x c e p t i o n ␣ w h i l e ␣ p r o c e s s i n g ␣ image ␣ s o u r c e : ␣ " \

f " { f i l e } , ␣ t a r g e t : ␣ { t a r g e t } "

, e x c _ i n f o=True )

Now we can zip the preprocessed ﬁles and store them somewhere.

6.1.6 Module 6 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 6

206

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

6.2 Part 6.2: Keras Neural Networks for Digits and Fashion

MNIST

This module will focus on computer vision. There are some important diﬀerences and similarities with
previous neural networks.

• We will usually use classiﬁcation, though regression is still an option.
• The input to the neural network is now 3D (height, width, color)
• Data are not transformed; no z-scores or dummy variables.
• Processing time is much longer.
• We now have diﬀerent layer times: dense layers (just like before), convolution layers, and max-pooling

layers.

• Data will no longer arrive as CSV ﬁles. TensorFlow provides some utilities for going directly from

the image to the input for a neural network.

6.2.1 Common Computer Vision Data Sets

There are many data sets for computer vision. Two of the most popular classic datasets are the MNIST
digits data set and the CIFAR image data sets. We will not use either of these datasets in this course, but
it is important to be familiar with them since neural network texts often refer to them.

The MNIST Digits Data Set is very popular in the neural network research community. You can see a

sample of it in Figure 6.1.

Figure 6.1: MNIST Data Set

Fashion-MNIST is a dataset of Zalando ’s article images---consisting of a training set of 60,000 examples
and a test set of 10,000 examples. Each example is a 28x28 grayscale image associated with a label from 10

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

207

classes. Fashion-MNIST is a direct drop-in replacement for the original MNIST dataset for benchmarking
machine learning algorithms. It shares the same image size and structure of training and testing splits.
You can see this data in Figure 6.2.

Figure 6.2: MNIST Fashon Data Set

The CIFAR-10 and CIFAR-100 datasets are also frequently used by the neural network research com-

munity.

The CIFAR-10 data set contains low-rez images that are divided into 10 classes. The CIFAR-100 data

set contains 100 classes in a hierarchy.

6.2.2 Convolutional Neural Networks (CNNs)

The convolutional neural network (CNN) is a neural network technology that has profoundly impacted the
area of computer vision (CV). Fukushima (1980)[5]introduced the original concept of a convolutional neural
network, and LeCun, Bottou, Bengio Haﬀner (1998)[20]greatly improved this work. From this research, Yan
LeCun introduced the famous LeNet-5 neural network architecture. This chapter follows the LeNet-5 style
network.
of

convolutional

neural

208

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Figure 6.3: CIFAR Data Set

Although computer vision primarily uses CNNs, this technology has some applications outside of the
ﬁeld. You need to realize that if you want to utilize CNNs on non-visual data, you must ﬁnd a way to
encode your data to mimic the properties of visual data.

The order of the input array elements is crucial to the training.

In contrast, most neural networks
that are not CNNs treat their input data as a long vector of values, and the order in which you arrange
the incoming features in this vector is irrelevant. You cannot change the order for these types of neural
networks after you have trained the network.

The CNN network arranges the inputs into a grid. This arrangement worked well with images because
the pixels in closer proximity to each other are important to each other. The order of pixels in an image
is signiﬁcant. The human body is a relevant example of this type of order. For the design of the face, we
are accustomed to eyes being near to each other.

This advance in CNNs is due to years of research on biological eyes.

In other words, CNNs utilize
overlapping ﬁelds of input to simulate features of biological eyes. Until this breakthrough, AI had been
unable to reproduce the capabilities of biological vision.
Scale, rotation, and noise have presented challenges for AI computer vision research. You can observe the
complexity of biological eyes in the example that follows. A friend raises a sheet of paper with a large
number written on it. As your friend moves nearer to you, the number is still identiﬁable. In the same way,
you can still identify the number when your friend rotates the paper. Lastly, your friend creates noise by
drawing lines on the page, but you can still identify the number. As you can see, these examples demonstrate
the high function of the biological eye and allow you to understand better the research breakthrough of
CNNs. That is, this neural network can process scale, rotation, and noise in the ﬁeld of computer vision.

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

209

You can see this network structure in Figure 6.4.

Figure 6.4: A LeNET-5 Network (LeCun, 1998)

So far, we have only seen one layer type (dense layers). By the end of this book we will have seen:

• Dense Layers - Fully connected layers.
• Convolution Layers - Used to scan across images.
• Max Pooling Layers - Used to downsample images.
• Dropout Layers - Used to add regularization.
• LSTM and Transformer Layers - Used for time series data.

6.2.3 Convolution Layers

The ﬁrst layer that we will examine is the convolutional layer. We will begin by looking at the hyper-
parameters that you must specify for a convolutional layer in most neural network frameworks that support
the CNN:

• Number of ﬁlters
• Filter Size
• Stride
• Padding
• Activation Function/Non-Linearity

The primary purpose of a convolutional layer is to detect features such as edges, lines, blobs of color, and
other visual elements. The ﬁlters can detect these features. The more ﬁlters we give to a convolutional
layer, the more features it can see.

A ﬁlter is a square-shaped object that scans over the image. A grid can represent the individual pixels
of a grid. You can think of the convolutional layer as a smaller grid that sweeps left to right over each
image row. There is also a hyperparameter that speciﬁes both the width and height of the square-shaped
ﬁlter. The following ﬁgure shows this conﬁguration in which you see the six convolutional ﬁlters sweeping
over the image grid:

A convolutional layer has weights between it and the previous layer or image grid. Each pixel on each
convolutional layer is a weight. Therefore, the number of weights between a convolutional layer and its
predecessor layer or image ﬁeld is the following:

210

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

[ F i l t e r S i z e ] ∗ [ F i l t e r S i z e ] ∗ [# o f F i l t e r s ]

For example, if the ﬁlter size were 5 (5x5) for 10 ﬁlters, there would be 250 weights.
You need to understand how the convolutional ﬁlters sweep across the previous layer’s output or image

grid. Figure 6.5 illustrates the sweep:

Figure 6.5: Convolutional Neural Network

The above ﬁgure shows a convolutional ﬁlter with 4 and a padding size of 1. The padding size is
responsible for the border of zeros in the area that the ﬁlter sweeps. Even though the image is 8x7,
the extra padding provides a virtual image size of 9x8 for the ﬁlter to sweep across. The stride speciﬁes
the number of positions the convolutional ﬁlters will stop. The convolutional ﬁlters move to the right,
advancing by the number of cells speciﬁed in the stride. Once you reach the far right, the convolutional
ﬁlter moves back to the far left; then, it moves down by the stride amount and
continues to the right again.

Some constraints exist concerning the size of the stride. The stride cannot be 0. The convolutional
ﬁlter would never move if you set the stride. Furthermore, neither the stride nor the convolutional ﬁlter
size can be larger than the previous grid. There are additional constraints on the stride (s), padding (p),
and the ﬁlter width (f ) for an image of width (w). Speciﬁcally, the convolutional ﬁlter must be able to
start at the far left or top border, move a certain number of strides, and land on the far right or bottom
border. The following equation shows the number of steps a convolutional operator
must take to cross the image:

steps =

w − f + 2p
s

+ 1

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

211

The number of steps must be an integer. In other words, it cannot have decimal places. The purpose

of the padding (p) is to be adjusted to make this equation become an integer value.

6.2.4 Max Pooling Layers

Max-pool layers downsample a 3D box to a new one with smaller dimensions. Typically, you can always
place a max-pool layer immediately following the convolutional layer. The LENET shows the max-pool
layer immediately after layers C1 and C3. These max-pool layers progressively decrease the size of the
dimensions of the 3D boxes passing through them. This technique can avoid overﬁtting (Krizhevsky,
Sutskever Hinton, 2012).

A pooling layer has the following hyper-parameters:

• Spatial Extent (f )
• Stride (s)

Unlike convolutional layers, max-pool layers do not use padding. Additionally, max-pool layers have no
weights, so training does not aﬀect them. These layers downsample their 3D box input. The 3D box output
by a max-pool layer will have a width equal to this equation:

w2 =

w1 − f
s

+ 1

The height of the 3D box produced by the max-pool layer is calculated similarly with this equation:

h2 =

h1 − f
s

+ 1

The depth of the 3D box produced by the max-pool layer is equal to the depth the 3D box received
as input. The most common setting for the hyper-parameters of a max-pool layer is f=2 and s=2. The
spatial extent (f) speciﬁes that boxes of 2x2 will be scaled down to single pixels. Of these four pixels, the
pixel with the maximum value will represent the 2x2 pixel in the new grid. Because squares of size 4 are
replaced with size 1, 75% of the pixel information is lost. The following ﬁgure shows this transformation
as a 6x6 grid becomes a 3x3:

Of course, the above diagram shows each pixel as a single number. A grayscale image would have this
characteristic. We usually take the average of the three numbers for an RGB image to determine which
pixel has the maximum value.

6.2.5 Regression Convolutional Neural Networks

We will now look at two examples, one for regression and another for classiﬁcation. For supervised computer
vision, your dataset will need some labels. For classiﬁcation, this label usually speciﬁes what the image

212

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Figure 6.6: Max Pooling Layer

is a picture of. For regression, this "label" is some numeric quantity the image should produce, such as a
count. We will look at two diﬀerent means of providing this label.

The ﬁrst example will show how to handle regression with convolution neural networks. We will provide
an image and expect the neural network to count items in that image. We will use a dataset that I created
that contains a random number of paperclips. The following code will download this dataset for you.

Code

import o s

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / r e l e a s e s / "
DOWNLOAD_SOURCE = URL+" download / v1 / p a p e r c l i p s . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "

e l s e :

# I used t h i s
PATH = " / U s e r s / j e f f /temp "

l o c a l l y on my machine , you may need d i f f e r e n t

EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (EXTRACT_TARGET,

" p a p e r c l i p s " )

Next, we download the images. This part depends on the origin of your images. The following code

downloads images from a URL, where a ZIP ﬁle contains the images. The code unzips the ZIP ﬁle.

Code

! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

213

! mkdir −p {EXTRACT_TARGET}
! u n z i p −o −j −d {SOURCE} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

The labels are contained in a CSV ﬁle named train.csvfor regression. This ﬁle has just two labels, id
and clip_count. The ID speciﬁes the ﬁlename; for example, row id 1 corresponds to the ﬁle clips-1.jpg.
The following code loads the labels for the training set and creates a new column, named ﬁlename, that
contains the ﬁlename of each image, based on the id column.

Code

import pandas a s pd

d f = pd . read_csv (

o s . path . j o i n (SOURCE, " t r a i n . c s v " ) ,
na_values =[ 'NA ' ,

' ? ' ] )

d f [ ' f i l e n a m e ' ]= " c l i p s −"+d f [ " i d " ] . a s t y p e ( s t r )+ " . j p g "

This results in the following dataframe.

Code

d f

Output

0
1
2
3
4
...
19995
19996
19997
19998
19999

id
30001
30002
30003
30004
30005
...
49996
49997
49998
49999
50000

clip_count
11
2
26
41
49
...
35
54
72
24
35

ﬁlename
clips-30001.jpg
clips-30002.jpg
clips-30003.jpg
clips-30004.jpg
clips-30005.jpg
...
clips-49996.jpg
clips-49997.jpg
clips-49998.jpg
clips-49999.jpg
clips-50000.jpg

Separate into a training and validation (for early stopping)

214

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Code

TRAIN_PCT = 0 . 9
TRAIN_CUT = int ( len ( d f ) ∗ TRAIN_PCT)

d f _ t r a i n = d f [ 0 :TRAIN_CUT]
d f _ v a l i d a t e = d f [TRAIN_CUT : ]

print ( f " T r a i n i n g ␣ s i z e : ␣ { l e n ( d f _ t r a i n ) } " )
print ( f " V a l i d a t e ␣ s i z e : ␣{ l e n ( d f _ v a l i d a t e ) } " )

Output

T r a i n i n g s i z e : 18000
V a l i d a t e

s i z e : 2000

We are now ready to create two ImageDataGenerator objects. We currently use a generator, which
creates additional training data by manipulating the source material. This technique can produce consid-
erably stronger neural networks. The generator below ﬂips the images both vertically and horizontally.
Keras will train the neuron network both on the original images and the ﬂipped images. This augmentation
increases the size of the training data considerably. Module 6.4 goes deeper into the transformations you
can perform. You can also specify a target size to resize the images automatically.

The function ﬂow_from_dataframe loads the labels from a Pandas dataframe connected to our
train.csv ﬁle. When we demonstrate classiﬁcation, we will use the ﬂow_from_directory; which loads
the labels from the directory structure rather than a CSV.

Code

import t e n s o r f l o w a s
import k e r a s _ p r e p r o c e s s i n g
from k e r a s _ p r e p r o c e s s i n g import image
from k e r a s _ p r e p r o c e s s i n g . image import ImageDataGenerator

t f

t r a i n i n g _ d a t a g e n = ImageDataGenerator (

r e s c a l e = 1 . / 2 5 5 ,
h o r i z o n t a l _ f l i p=True ,
v e r t i c a l _ f l i p=True ,
f i l l _ m o d e= ' n e a r e s t ' )

t r a i n _ g e n e r a t o r = t r a i n i n g _ d a t a g e n . flow_from_dataframe (

dataframe=d f _ t r a i n ,
d i r e c t o r y=SOURCE,
x_col=" f i l e n a m e " ,

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

215

y_col=" c l i p _ c o u n t " ,
t a r g e t _ s i z e =(256 , 2 5 6 ) ,
b a t c h _ s i z e =32 ,
class_mode= ' o t h e r ' )

v a l i d a t i o n _ d a t a g e n = ImageDataGenerator ( r e s c a l e = 1 . / 2 5 5 )

v a l _ g e n e r a t o r = v a l i d a t i o n _ d a t a g e n . flow_from_dataframe (

dataframe=d f _ v a l i d a t e ,
d i r e c t o r y=SOURCE,
x_col=" f i l e n a m e " ,
y_col=" c l i p _ c o u n t " ,
t a r g e t _ s i z e =(256 , 2 5 6 ) ,
class_mode= ' o t h e r ' )

Output

Found 18000 v a l i d a t e d image f i l e n a m e s .
Found 2000 v a l i d a t e d image f i l e n a m e s .

We can now train the neural network. The code to build and train the neural network is not that
diﬀerent than in the previous modules. We will use the Keras Sequential class to provide layers to the
neural network. We now have several new layer types that we did not previously see.

• Conv2D - The convolution layers.
• MaxPooling2D - The max-pooling layers.
• Flatten - Flatten the 2D (and higher) tensors to allow a Dense layer to process.
• Dense - Dense layers, the same as demonstrated previously. Dense layers often form the ﬁnal output

layers of the neural network.

The training code is very similar to previously. This code is for regression, so a ﬁnal linear activation
is used, along with mean_squared_error for the loss function. The generator provides both the x and y
matrixes we previously supplied.

Code

from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import time

model = t f . k e r a s . models . S e q u e n t i a l ( [

t h e d e s i r e d s i z e o f

t h e image 150 x150

sha pe i s

# Note t h e i n p u t
# w i t h 3 b y t e s c o l o r .
# This
t f . k e r a s . l a y e r s . Conv2D ( 6 4 ,

f i r s t

t h e

i s

c o n v o l u t i o n

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ,

216

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

input_shape =(256 , 2 5 6 , 3 ) ) ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# The second c o n v o l u t i o n
t f . k e r a s . l a y e r s . Conv2D ( 6 4 ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
t f . k e r a s . l a y e r s . F l a t t e n ( ) ,
# 512 neuron h i d d e n l a y e r
t f . k e r a s . l a y e r s . Dense ( 5 1 2 , a c t i v a t i o n= ' r e l u ' ) ,
t f . k e r a s . l a y e r s . Dense ( 1 , a c t i v a t i o n= ' l i n e a r ' )

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) ,

] )

model . summary ( )
epoch_steps = 250 # needed f o r 2 . 2
v a l i d a t i o n _ s t e p s = len ( d f _ v a l i d a t e )
model . compile ( l o s s = ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

s t a r t _ t i m e = time . time ( )
h i s t o r y = model . f i t ( t r a i n _ g e n e r a t o r ,

v e r b o s e = 1 ,
v a l i d a t i o n _ d a t a=v a l _ g e n e r a t o r ,

c a l l b a c k s =[ monitor ] , e po c h s =25)

e l a p s e d _ t i m e = time . time ( ) − s t a r t _ t i m e
print ( " Elapsed ␣ time : ␣ {} " . format ( hms_string ( e l a p s e d _ t i m e ) ) )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

( None , 2 5 4 , 2 5 4 , 6 4 )

conv2d ( Conv2D )
max_pooling2d ( MaxPooling2D ( None , 1 2 7 , 1 2 7 , 6 4 )
)
conv2d_1 ( Conv2D )
max_pooling2d_1 ( MaxPooling
2D)
f l a t t e n ( F l a t t e n )
d e n s e ( Dense )

( None , 1 2 5 , 1 2 5 , 6 4 )
( None , 6 2 , 6 2 , 6 4 )

( None , 2 4 6 0 1 6 )
( None , 5 1 2 )

1792
0

36928
0

0
125960704

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

217

dense_1 ( Dense )

( None , 1 )

513

=================================================================
T o t a l params : 1 2 5 , 9 9 9 , 9 3 7

. . .

3 . 2 3 9 9 − v a l _ l o s s : 4 . 0 4 4 9
Epoch 25/25
563/563 [==============================] − 53 s 94ms/ s t e p − l o s s :
3 . 2 8 2 3 − v a l _ l o s s : 4 . 4 8 9 9
Elapsed time : 0 : 2 2 : 2 2 . 7 8

This code will run very slowly if you do not use a GPU. The above code takes approximately 13 minutes

with a GPU.

6.2.6 Score Regression Image Data

Scoring/predicting from a generator is a bit diﬀerent than training. We do not want augmented images,
and we do not wish to have the dataset shuﬄed. For scoring, we want a prediction for each input. We
construct the generator as follows:

• shuﬄe=False
• batch_size=1
• class_mode=None

We use a batch_size of 1 to guarantee that we do not run out of GPU memory if our prediction set is
large. You can increase this value for better performance. The class_mode is None because there is no
y, or label. After all, we are predicting.

Code

d f _ t e s t = pd . read_csv (

o s . path . j o i n (SOURCE, " t e s t . c s v " ) ,
na_values =[ 'NA ' ,

' ? ' ] )

d f _ t e s t [ ' f i l e n a m e ' ]= " c l i p s −"+d f _ t e s t [ " i d " ] . a s t y p e ( s t r )+ " . j p g "

t e s t _ d a t a g e n = ImageDataGenerator ( r e s c a l e = 1 . / 2 5 5 )

t e s t _ g e n e r a t o r = v a l i d a t i o n _ d a t a g e n . flow_from_dataframe (

dataframe=d f _ t e s t ,
d i r e c t o r y=SOURCE,
x_col=" f i l e n a m e " ,
b a t c h _ s i z e =1,
s h u f f l e=F a l s e ,

218

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

t a r g e t _ s i z e =(256 , 2 5 6 ) ,
class_mode=None )

Output

Found 5000 v a l i d a t e d image f i l e n a m e s .

We need to reset the generator to ensure we are always at the beginning.

Code

t e s t _ g e n e r a t o r . r e s e t ( )
pred = model . p r e d i c t ( t e s t _ g e n e r a t o r , s t e p s=len ( d f _ t e s t ) )

We can now generate a CSV ﬁle to hold the predictions.

Code

df_submit = pd . DataFrame ( { ' i d ' : d f _ t e s t [ ' i d ' ] , ' c l i p _ c o u n t ' : pred . f l a t t e n ( ) } )
df_submit . to_csv ( o s . path . j o i n (PATH, " submit . c s v " ) , i n d e x=F a l s e )

6.2.7 Classiﬁcation Neural Networks

Just like earlier in this module, we will load data. However, this time we will use a dataset of images of
three diﬀerent types of the iris ﬂower. This zip ﬁle contains three diﬀerent directories that specify each
image’s label. The directories are named the same as the labels:

• iris-setosa
• iris-versicolour
• iris-virginica

Code

import o s

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / r e l e a s e s "
DOWNLOAD_SOURCE = URL+" / download / v1 / i r i s −image . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "
EXTRACT_TARGET = o s . path . j o i n (PATH, " i r i s " )

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

219

SOURCE = EXTRACT_TARGET # In t h i s c a s e i t s

t h e same , no s u b f o l d e r

e l s e :

l o c a l l y on my machine , you may need d i f f e r e n t

# I used t h i s
PATH = " / U s e r s / j e f f /temp "
EXTRACT_TARGET = o s . path . j o i n (PATH, " i r i s " )
SOURCE = EXTRACT_TARGET # In t h i s c a s e i t s

t h e same , no s u b f o l d e r

Just as before, we unzip the images.

Code

! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}
! mkdir −p {EXTRACT_TARGET}
! u n z i p −o −d {EXTRACT_TARGET} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

You can see these folders with the following command.

Code

! l s / c o n t e n t / i r i s

Output

i r i s −s e t o s a

i r i s −v e r s i c o l o u r

i r i s −v i r g i n i c a

We set up the generator, similar to before. This time we use ﬂow_from_directory to get the labels

from the directory structure.

Code

import t e n s o r f l o w a s
import k e r a s _ p r e p r o c e s s i n g
from k e r a s _ p r e p r o c e s s i n g import image
from k e r a s _ p r e p r o c e s s i n g . image import ImageDataGenerator

t f

t r a i n i n g _ d a t a g e n = ImageDataGenerator (

r e s c a l e = 1 . / 2 5 5 ,
h o r i z o n t a l _ f l i p=True ,
v e r t i c a l _ f l i p=True ,
w i d t h _ s h i f t _ r a n g e =[ −200 ,200] ,
r o t a t i o n _ r a n g e =360 ,

220

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

f i l l _ m o d e= ' n e a r e s t ' )

t r a i n _ g e n e r a t o r = t r a i n i n g _ d a t a g e n . f l o w _ f r o m _ d i r e c t o r y (

d i r e c t o r y=SOURCE,
class_mode= ' c a t e g o r i c a l ' , b a t c h _ s i z e =32 ,

t a r g e t _ s i z e =(256 , 2 5 6 ) ,

s h u f f l e=True )

v a l i d a t i o n _ d a t a g e n = ImageDataGenerator ( r e s c a l e = 1 . / 2 5 5 )

v a l i d a t i o n _ g e n e r a t o r = v a l i d a t i o n _ d a t a g e n . f l o w _ f r o m _ d i r e c t o r y (

d i r e c t o r y=SOURCE,
class_mode= ' c a t e g o r i c a l ' , b a t c h _ s i z e =32 ,

t a r g e t _ s i z e =(256 , 2 5 6 ) ,

s h u f f l e=True )

Output

Found 421 images b e l o n g i n g t o 3 c l a s s e s .
Found 421 images b e l o n g i n g t o 3 c l a s s e s .

Training the neural network with classiﬁcation is similar to regression.

Code

from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

c l a s s _ c o u n t = len ( t r a i n _ g e n e r a t o r . c l a s s _ i n d i c e s )

model = t f . k e r a s . models . S e q u e n t i a l ( [

t h e d e s i r e d s i z e o f

t h e image

sha pe i s

# Note t h e i n p u t
# 300 x300 w i t h 3 b y t e s c o l o r
# This
t f . k e r a s . l a y e r s . Conv2D ( 1 6 ,

f i r s t

t h e

i s

c o n v o l u t i o n

input_shape =(256 , 2 5 6 , 3 ) ) ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# The second c o n v o l u t i o n
t f . k e r a s . l a y e r s . Conv2D ( 3 2 ,
t f . k e r a s . l a y e r s . Dropout ( 0 . 5 ) ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# The t h i r d c o n v o l u t i o n
t f . k e r a s . l a y e r s . Conv2D ( 6 4 ,
t f . k e r a s . l a y e r s . Dropout ( 0 . 5 ) ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# The f o u r t h c o n v o l u t i o n

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ,

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) ,

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) ,

6.2. PART 6.2: KERAS NEURAL NETWORKS FOR DIGITS AND FASHION MNIST

221

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) ,

t f . k e r a s . l a y e r s . Conv2D ( 6 4 ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# The f i f t h c o n v o l u t i o n
t f . k e r a s . l a y e r s . Conv2D ( 6 4 ,
t f . k e r a s . l a y e r s . MaxPooling2D ( 2 , 2 ) ,
# F l a t t e n t h e r e s u l t s

t o f e e d i n t o a DNN

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) ,

t f . k e r a s . l a y e r s . F l a t t e n ( ) ,
t f . k e r a s . l a y e r s . Dropout ( 0 . 5 ) ,
# 512 neuron h i d d e n l a y e r
t f . k e r a s . l a y e r s . Dense ( 5 1 2 , a c t i v a t i o n= ' r e l u ' ) ,
# Only 1 o u t p u t neuron .
I t w i l l c o n t a i n a v a l u e from 0−1
t f . k e r a s . l a y e r s . Dense ( c l a s s _ c o u n t , a c t i v a t i o n= ' softmax ' )

] )

model . summary ( )

model . compile ( l o s s = ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

model . f i t ( t r a i n _ g e n e r a t o r , e p o ch s =50 , steps_per_epoch =10 ,

v e r b o s e = 1 )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l _ 1 "

Layer

( type )

Output Shape

Param #

=================================================================

conv2d_2 ( Conv2D )
max_pooling2d_2 ( MaxPooling
2D)
conv2d_3 ( Conv2D )
dropout
( Dropout )
max_pooling2d_3 ( MaxPooling
2D)
conv2d_4 ( Conv2D )
dropout_1 ( Dropout )
max_pooling2d_4 ( MaxPooling
2D)

( None , 2 5 4 , 2 5 4 , 1 6 )

( None , 1 2 7 , 1 2 7 , 1 6 )

( None , 1 2 5 , 1 2 5 , 3 2 )
( None , 1 2 5 , 1 2 5 , 3 2 )
( None , 6 2 , 6 2 , 3 2 )

( None , 6 0 , 6 0 , 6 4 )
( None , 6 0 , 6 0 , 6 4 )

( None , 3 0 , 3 0 , 6 4 )

448
0

4640
0
0

18496
0
0

. . .

222

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

_________________________________________________________________
. . .
10/10 [==============================] − 5 s 458ms/ s t e p − l o s s : 0 . 7 9 5 7
Epoch 50/50
10/10 [==============================] − 5 s 501ms/ s t e p − l o s s : 0 . 8 6 7 0

The iris image dataset is not easy to predict; it turns out that a tabular dataset of measurements is

more manageable. However, we can achieve a 63%.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e
import numpy a s np

v a l i d a t i o n _ g e n e r a t o r . r e s e t ( )
pred = model . p r e d i c t ( v a l i d a t i o n _ g e n e r a t o r )

p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = v a l i d a t i o n _ g e n e r a t o r . c l a s s e s

c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " Accuracy : ␣{ c o r r e c t } " )

Output

Accuracy : 0 . 6 3 8 9 5 4 8 6 9 3 5 8 6 6 9 9

6.2.8 Other Resources

• Imagenet:Large Scale Visual Recognition Challenge 2014
• Andrej Karpathy - PhD student/instructor at Stanford.
• CS231n Convolutional Neural Networks for Visual Recognition - Stanford course on computer vi-

sion/CNN’s.
• CS231n - GitHub
• ConvNetJS - JavaScript library for deep learning.

6.3 Part 6.3: Transfer Learning for Computer Vision

Many advanced prebuilt neural networks are available for computer vision, and Keras provides direct access
to many networks. Transfer learning is the technique where you use these prebuilt neural networks. Module
9 takes a deeper look at transfer learning.

6.3. PART 6.3: TRANSFER LEARNING FOR COMPUTER VISION

223

There are several diﬀerent levels of transfer learning.

• Use a prebuilt neural network in its entirety
• Use a prebuilt neural network’s structure
• Use a prebuilt neural network’s weights

We will begin by using the MobileNet prebuilt neural network in its entirety. MobileNet will be loaded and
allowed to classify simple images. We can already classify 1,000 images through this technique without
ever having trained the network.

Code

import pandas a s pd
import numpy a s np
import o s
import t e n s o r f l o w . k e r a s
import m a t p l o t l i b . p y p l o t a s p l t
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , GlobalAveragePooling2D
from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s import MobileNet
from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g import image
from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s . m o b i l e n e t import p r e p r o c e s s _ i n p u t
from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g . image import ImageDataGenerator
from t e n s o r f l o w . k e r a s . models import Model
from t e n s o r f l o w . k e r a s . o p t i m i z e r s import Adam

We begin by downloading weights for a MobileNet trained for the imagenet dataset, which will take

some time to download the ﬁrst time you train the network.

Code

model = MobileNet ( w e i g h t s= ' imagenet ' , i n c l u d e _ t o p=True )

The loaded network is a Keras neural network. However, this is a neural network that a third party
engineered on advanced hardware. Merely looking at the structure of an advanced state-of-the-art neural
network can be educational.

Code

model . summary ( )

Output

Model :
_________________________________________________________________

" mobilenet_1 . 0 0 _224 "

Layer

( type )

Output Shape

Param #

224

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

=================================================================

input_1 ( InputLayer )
conv1 ( Conv2D )
conv1_bn ( B a t c h N o r m a l i z a t i o
n )
conv1_relu (ReLU)
conv_dw_1 ( DepthwiseConv2D )
conv_dw_1_bn ( BatchNormaliz
a t i o n )
conv_dw_1_relu (ReLU)
conv_pw_1 ( Conv2D )
conv_pw_1_bn ( BatchNormaliz

[ ( None , 2 2 4 , 2 2 4 , 3 ) ]
( None , 1 1 2 , 1 1 2 , 3 2 )

( None , 1 1 2 , 1 1 2 , 3 2 )

( None , 1 1 2 , 1 1 2 , 3 2 )

( None , 1 1 2 , 1 1 2 , 3 2 )
( None , 1 1 2 , 1 1 2 , 3 2 )

( None , 1 1 2 , 1 1 2 , 3 2 )
( None , 1 1 2 , 1 1 2 , 6 4 )

( None , 1 1 2 , 1 1 2 , 6 4 )

0
864
128

0
288
128

0
2048
256

. . .

=================================================================
T o t a l params : 4 , 2 5 3 , 8 6 4
T r a i n a b l e params : 4 , 2 3 1 , 9 7 6
Non−t r a i n a b l e params : 2 1 , 8 8 8
_________________________________________________________________

Several clues to neural network architecture become evident when examining the above structure.
We will now use the MobileNet to classify several image URLs below. You can add additional URLs of

your own to see how well the MobileNet can classify.

Code

I m a g e F i l e

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
import numpy a s np
from i o import BytesIO
from IPython . d i s p l a y import d i s p l a y , HTML
from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s . m o b i l e n e t import d e c o d e _ p r e d i c t i o n s

IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
IMAGE_CHANNELS = 3

ROOT = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ images / "

def make_square ( img ) :

6.3. PART 6.3: TRANSFER LEARNING FOR COMPUTER VISION

225

c o l s , rows = img . s i z e

i f

rows>c o l s :

pad = ( rows−c o l s ) / 2
img = img . c r o p ( ( pad , 0 , c o l s , c o l s ) )

e l s e :

pad = ( c o l s −rows ) / 2
img = img . c r o p ( ( 0 , pad , rows , rows ) )

return img

def c l a s s i f y _ i m a g e ( u r l ) :

x = [ ]
I m a g e F i l e .LOAD_TRUNCATED_IMAGES = F a l s e
r e s p o n s e = r e q u e s t s . g e t ( u r l )
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )
img = img . r e s i z e ( (IMAGE_WIDTH,IMAGE_HEIGHT) , Image . ANTIALIAS)

x = image . img_to_array ( img )
x = np . expand_dims ( x , a x i s =0)
x = p r e p r o c e s s _ i n p u t ( x )
x = x [ : , : , : , : 3 ] # maybe an a l p h a c h a n n e l
pred = model . p r e d i c t ( x )

d i s p l a y ( img )
print ( np . argmax ( pred , a x i s =1))

l s t = d e c o d e _ p r e d i c t i o n s ( pred ,
fo r itm in l s t [ 0 ] :
print ( itm )

top =5)

We can now classify an example image. You can specify the URL of any image you wish to classify.

Code

c l a s s i f y _ i m a g e (ROOT+" s o c c e r _ b a l l . j p g " )

Output

226

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

[ 8 0 5 ]
Downloading data from h t t p s : / / s t o r a g e . g o o g l e a p i s . com/ download . t e n s o r f l
ow . o r g / data / i m a g e n e t _ c l a s s _ i n d e x . j s o n
40960/35363 [==================================] − 0 s 0 us / s t e p
49152/35363 [=========================================] − 0 s 0 us / s t e p
( ' n04254680 ' ,
( ' n03530642 ' ,
( ' n03255030 ' ,
( ' n02782093 ' ,
( ' n04548280 ' ,

' s o c c e r _ b a l l ' , 0 . 9 9 9 9 9 3 8 )
' honeycomb ' , 3 . 8 6 2 4 1 2 e −06)
' dumbbell ' , 4 . 4 4 2 4 5 8 e −07)
' b a l l o o n ' , 3 . 7 0 3 8 9 8 7 e −07)
' w a l l _ c l o c k ' , 3 . 1 4 3 9 1 1 e −07)

Code

c l a s s i f y _ i m a g e (ROOT+" r a c e _ t r u c k . j p g " )

Output

6.3. PART 6.3: TRANSFER LEARNING FOR COMPUTER VISION

227

Overall, the neural network is doing quite well.
For many applications, MobileNet might be entirely acceptable as an image classiﬁer. However, if you
need to classify very specialized images, not in the 1,000 image types supported by imagenet, it is necessary
to use transfer learning.

6.3.1 Using the Structure of ResNet

We will train a neural network to count the number of paper clips in images. We will make use of the
structure of the ResNet neural network. There are several signiﬁcant changes that we will make to ResNet
to apply to this task. First, ResNet is a classiﬁer; we wish to perform a regression to count. Secondly, we
want to change the image resolution that ResNet uses. We will not use the weights from ResNet; changing
this resolution invalidates the current weights. Thus, it will be necessary to retrain the network.

Code

import o s
URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / "
DOWNLOAD_SOURCE = URL+" r e l e a s e s / download / v1 / p a p e r c l i p s . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "

e l s e :

# I used t h i s
PATH = " / U s e r s / j e f f /temp "

l o c a l l y on my machine , you may need d i f f e r e n t

228

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (EXTRACT_TARGET,

" p a p e r c l i p s " )

Output

[ 7 5 1 ]
( ' n04037443 ' ,
( ' n03100240 ' ,
( ' n04285008 ' ,
( ' n03930630 ' ,
( ' n02704792 ' ,

' r a c e r ' , 0 . 7 1 3 1 9 5 1 )
' c o n v e r t i b l e ' , 0 . 1 0 0 8 9 6 7 7 6 )
' s p o r t s _ c a r ' , 0 . 0 7 7 0 7 6 8 )
' pickup ' , 0 . 0 2 6 3 5 3 0 5 )
' amphibian ' , 0 . 0 1 1 6 3 6 1 6 9 )

Next, we download the images. This part depends on the origin of your images. The following code

downloads images from a URL, where a ZIP ﬁle contains the images. The code unzips the ZIP ﬁle.

Code

! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}
! mkdir −p {EXTRACT_TARGET}
! u n z i p −o −j −d {SOURCE} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

The labels are contained in a CSV ﬁle named train.csv for the regression. This ﬁle has just two
labels, id and clip_count. The ID speciﬁes the ﬁlename; for example, row id 1 corresponds to the ﬁle
clips-1.jpg. The following code loads the labels for the training set and creates a new column, named
ﬁlename, that contains the ﬁlename of each image, based on the id column.

Code

d f _ t r a i n = pd . read_csv ( o s . path . j o i n (SOURCE,
d f _ t r a i n [ ' f i l e n a m e ' ] = " c l i p s −" + d f _ t r a i n . id . a s t y p e ( s t r ) + " . j p g "

" t r a i n . c s v " ) )

We want to use early stopping. To do this, we need a validation set. We will break the data into 80
percent test data and 20 validation. Do not confuse this validation data with the test set provided by
Kaggle. This validation set is unique to your program and is for early stopping.

Code

TRAIN_PCT = 0 . 9
TRAIN_CUT = int ( len ( d f _ t r a i n ) ∗ TRAIN_PCT)

6.3. PART 6.3: TRANSFER LEARNING FOR COMPUTER VISION

229

d f _ t r a i n _ c u t = d f _ t r a i n [ 0 :TRAIN_CUT]
d f _ v a l i d a t e _ c u t = d f _ t r a i n [TRAIN_CUT : ]

print ( f " T r a i n i n g ␣ s i z e : ␣{ l e n ( d f _ t r a i n _ c u t ) } " )
print ( f " V a l i d a t e ␣ s i z e : ␣{ l e n ( d f _ v a l i d a t e _ c u t ) } " )

Output

T r a i n i n g s i z e : 18000
V a l i d a t e

s i z e : 2000

Next, we create the generators that will provide the images to the neural network during training. We
normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1. We also use
the ﬂow_from_dataframe generator to connect the Pandas dataframe to the actual image ﬁles. We
see here a straightforward implementation; you might also wish to use some of the image transformations
provided by the data generator.

The HEIGHT and WIDTH constants specify the dimensions to which the image will be scaled (or

expanded). It is probably not a good idea to expand the images.

Code

import t e n s o r f l o w a s
import k e r a s _ p r e p r o c e s s i n g
from k e r a s _ p r e p r o c e s s i n g import image
from k e r a s _ p r e p r o c e s s i n g . image import ImageDataGenerator

t f

WIDTH = 256
HEIGHT = 256

t r a i n i n g _ d a t a g e n = ImageDataGenerator (

r e s c a l e = 1 . / 2 5 5 ,
h o r i z o n t a l _ f l i p=True ,
#v e r t i c a l _ f l i p=True ,
f i l l _ m o d e= ' n e a r e s t ' )

t r a i n _ g e n e r a t o r = t r a i n i n g _ d a t a g e n . flow_from_dataframe (

dataframe=df_train_cut ,
d i r e c t o r y=SOURCE,
x_col=" f i l e n a m e " ,
y_col=" c l i p _ c o u n t " ,
t a r g e t _ s i z e =(HEIGHT, WIDTH) ,
# Keeping t h e t r a i n i n g b a t c h s i z e s m a l l

230

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

# USUALLY i n c r e a s e s performance
b a t c h _ s i z e =32 ,
class_mode= ' raw ' )

v a l i d a t i o n _ d a t a g e n = ImageDataGenerator ( r e s c a l e = 1 . / 2 5 5 )

v a l _ g e n e r a t o r = v a l i d a t i o n _ d a t a g e n . flow_from_dataframe (

dataframe=d f _ v a l i d a t e _ c u t ,
d i r e c t o r y=SOURCE,
x_col=" f i l e n a m e " ,
y_col=" c l i p _ c o u n t " ,
t a r g e t _ s i z e =(HEIGHT, WIDTH) ,
# Make t h e v a l i d a t i o n b a t c h s i z e as
# have memory f o r
b a t c h _ s i z e =256 ,
class_mode= ' raw ' )

l a r g e as you

Output

Found 18000 v a l i d a t e d image f i l e n a m e s .
Found 2000 v a l i d a t e d image f i l e n a m e s .

We will now use a ResNet neural network as a basis for our neural network. We will redeﬁne both the
input shape and output of the ResNet model, so we will not transfer the weights. Since we redeﬁne the
input, the weights are of minimal value. We begin by loading, from Keras, the ResNet50 network. We
specify include_top as False because we will change the input resolution. We also specify weights as
false because we must retrain the network after changing the top input layers.

Code

from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s . r e s n e t 5 0 import ResNet50
from t e n s o r f l o w . k e r a s . l a y e r s import Input

i n p u t _ t e n s o r = Input ( shape =(HEIGHT, WIDTH, 3 ) )

base_model = ResNet50 (

i n c l u d e _ t o p=F a l s e , w e i g h t s=None ,
input_shape=None )

i n p u t _ t e n s o r=i n p u t _ t e n s o r ,

Now we must add a few layers to the end of the neural network so that it becomes a regression model.

6.3. PART 6.3: TRANSFER LEARNING FOR COMPUTER VISION

231

Code

from t e n s o r f l o w . k e r a s . l a y e r s import Dense , GlobalAveragePooling2D
from t e n s o r f l o w . k e r a s . models import Model

x=base_model . output
x=GlobalAveragePooling2D ( ) ( x )
x=Dense ( 1 0 2 4 , a c t i v a t i o n= ' r e l u ' ) ( x )
x=Dense ( 1 0 2 4 , a c t i v a t i o n= ' r e l u ' ) ( x )
model=Model ( i n p u t s=base_model . input , o u t p u t s=Dense ( 1 ) ( x ) )

We train like before; the only diﬀerence is that we do not deﬁne the entire neural network here.

Code

from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from t e n s o r f l o w . k e r a s . m e t r i c s import RootMeanSquaredError

# Important , c a l c u l a t e a v a l i d s t e p s i z e f o r
STEP_SIZE_VALID=v a l _ g e n e r a t o r . n// v a l _ g e n e r a t o r . b a t c h _ s i z e

t h e v a l i d a t i o n d a t a s e t

model . compile ( l o s s = ' mean_squared_error ' , o p t i m i z e r= ' adam ' ,

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

m e t r i c s =[ RootMeanSquaredError ( name=" rmse " ) ] )

p a t i e n c e =50 , v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

h i s t o r y = model . f i t ( t r a i n _ g e n e r a t o r , e p o c hs =100 , steps_per_epoch =250 ,

v a l i d a t i o n _ d a t a = v a l _ g e n e r a t o r ,
v e r b o s e = 1 , v a l i d a t i o n _ s t e p s=STEP_SIZE_VALID)

c a l l b a c k s =[ monitor ] ,

Output

. . .
250/250 [==============================] − 61 s 243ms/ s t e p − l o s s :
1 . 9 2 1 1 − rmse : 1 . 3 8 6 0 − v a l _ l o s s : 1 7 . 0 4 8 9 − val_rmse : 4 . 1 2 9 0
Epoch 72/100
250/250 [==============================] − 61 s 243ms/ s t e p − l o s s :
2 . 3 7 2 6 − rmse : 1 . 5 4 0 3 − v a l _ l o s s : 1 6 7 . 8 5 3 6 − val_rmse : 1 2 . 9 5 5 8

232

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

6.4 Part 6.4: Inside Augmentation

The ImageDataGenerator class provides many options for image augmentation. Deciding which augmenta-
tions to use can impact the eﬀectiveness of your model. This part will visualize some of these augmentations
that you might use to train your neural network. We begin by loading a sample image to augment.

Code

import u r l l i b . r e q u e s t
import s h u t i l
from IPython . d i s p l a y import Image

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / t81_558_deep_learning / " +\

" b l o b / master / p h ot o s / l a n d s c a p e . j p g ? raw=t r u e "

LOCAL_IMG_FILE = " / c o n t e n t / l a n d s c a p e . j p g "

with u r l l i b . r e q u e s t . u r l o p e n (URL) a s

r e s p o n s e , \

open (LOCAL_IMG_FILE,

'wb ' ) a s o u t _ f i l e :

s h u t i l . c o p y f i l e o b j ( r e s p o n s e , o u t _ f i l e )

Image ( f i l e n a m e=LOCAL_IMG_FILE)

Output

Next, we introduce a simple utility function to visualize four images sampled from any generator.

6.4. PART 6.4: INSIDE AUGMENTATION

233

Code

from numpy import expand_dims
from k e r a s . p r e p r o c e s s i n g . image import load_img
from k e r a s . p r e p r o c e s s i n g . image import img_to_array
from k e r a s . p r e p r o c e s s i n g . image import ImageDataGenerator
from m a t p l o t l i b import p y p l o t
import m a t p l o t l i b . p y p l o t a s p l t
import numpy a s np
import m a t p l o t l i b

def v i s u a l i z e _ g e n e r a t o r ( i m g _ f i l e , gen ) :

# Load t h e r e q u e s t e d image

img = load_img ( i m g _ f i l e )
data = img_to_array ( img )
samples = expand_dims ( data , 0 )

# Generat a u g u m e n t a t i o n s from t h e g e n e r a t o r

i t = gen . f l o w ( samples , b a t c h _ s i z e =1)
images = [ ]
for i

in range ( 4 ) :

batch = i t . next ( )
image = batch [ 0 ] . a s t y p e ( ' u i n t 8 ' )
images . append ( image )

images = np . a r r a y ( images )

# Create a g r i d o f 4 images from t h e g e n e r a t o r

index , h e i g h t , width , c h a n n e l s = images . shape
nrows = i n d e x //2

g r i d = ( images . r e s h a p e ( nrows , 2 , h e i g h t , width , c h a n n e l s )

. swapaxes ( 1 , 2 )
. r e s h a p e ( h e i g h t ∗ nrows , width ∗ 2 , 3 ) )

f i g = p l t . f i g u r e ( f i g s i z e = ( 1 5 . , 1 5 . ) )
p l t . a x i s ( ' o f f ' )
p l t . imshow ( g r i d )

We begin by ﬂipping the image. Some images may not make sense to ﬂip, such as this landscape.
However, if you expect "noise" in your data where some images may be ﬂipped, then this augmentation
may be useful, even if it violates physical reality.

234

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Code

v i s u a l i z e _ g e n e r a t o r (
LOCAL_IMG_FILE,
ImageDataGenerator ( h o r i z o n t a l _ f l i p=True , v e r t i c a l _ f l i p=True ) )

Output

Next, we will try moving the image. Notice how part of the image is missing? There are various ways
to ﬁll in the missing data, as controlled by ﬁll_mode. In this case, we simply use the nearest pixel to ﬁll.
It is also possible to rotate images.

Code

v i s u a l i z e _ g e n e r a t o r (

LOCAL_IMG_FILE,
ImageDataGenerator ( w i d t h _ s h i f t _ r a n g e =[ −200 ,200] ,

f i l l _ m o d e= ' n e a r e s t ' ) )

Output

6.4. PART 6.4: INSIDE AUGMENTATION

235

We can also adjust brightness.

Code

v i s u a l i z e _ g e n e r a t o r (
LOCAL_IMG_FILE,
ImageDataGenerator ( b r i g h t n e s s _ r a n g e = [ 0 , 1 ] ) )

# b r i g h t n e s s _ r a n g e=None ,

shear_range =0.0

Output

236

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Shearing may not be appropriate for all image types, it stretches the image.

Code

v i s u a l i z e _ g e n e r a t o r (
LOCAL_IMG_FILE,
ImageDataGenerator ( shear_range =30))

Output

6.4. PART 6.4: INSIDE AUGMENTATION

237

It is also possible to rotate images.

Code

v i s u a l i z e _ g e n e r a t o r (
LOCAL_IMG_FILE,
ImageDataGenerator ( r o t a t i o n _ r a n g e =30))

Output

238

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

6.5 Part 6.5: Recognizing Multiple Images with YOLO5

Programmers typically design convolutional neural networks to classify a single item centered in an image.
However, as humans, we can recognize many items in our ﬁeld of view in real-time. It is advantageous to
recognize multiple items in a single image. One of the most advanced means of doing this is YOLOv5.
You Only Look Once (YOLO) was introduced by Joseph Redmon, who supported YOLO up through
V3.[28]The fact that YOLO must only look once speaks to the eﬃciency of the algorithm. In this context,
to "look" means to perform one scan over the image. It is also possible to run YOLO on live video streams.
Joseph Redmon left computer vision to pursue other interests. The current version, YOLOv5 is sup-
ported by the startup company Ultralytics, who released the open-source library that we use in this
class.[36]

Researchers have trained YOLO on a variety of diﬀerent computer image datasets. The version of
YOLO weights used in this course is from the dataset Common Objects in Context (COCO).[23]This
dataset contains images labeled into 80 diﬀerent classes. COCO is the source of the ﬁle coco.txt used in
this module.

6.5.1 Using YOLO in Python

To use YOLO in Python, we will use the open-source library provided by Ultralytics.

• YOLOv5 GitHub

The code provided in this notebook works equally well when run either locally or from Google CoLab. It
is easier to run YOLOv5 from CoLab, which is recommended for this course.

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

239

We begin by obtaining an image to classify.

Code

import u r l l i b . r e q u e s t
import s h u t i l
from IPython . d i s p l a y import Image
! mkdir / c o n t e n t / images /

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / t81_558_deep_learning "
URL += " /raw/ master / p h o t os / j e f f _ c o o k . j p g "
LOCAL_IMG_FILE = " / c o n t e n t / images / j e f f _ c o o k . j p g "

with u r l l i b . r e q u e s t . u r l o p e n (URL) a s

r e s p o n s e , \

open (LOCAL_IMG_FILE,

'wb ' ) a s o u t _ f i l e :

s h u t i l . c o p y f i l e o b j ( r e s p o n s e , o u t _ f i l e )

Image ( f i l e n a m e=LOCAL_IMG_FILE)

Output

240

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

6.5.2 Installing YOLOv5

YOLO is not available directly through either PIP or CONDA. Additionally, YOLO is not installed in
Google CoLab by default. Therefore, whether you wish to use YOLO through CoLab or run it locally, you
need to go through several steps to install it. This section describes the process of installing YOLO. The
same steps apply to either CoLab or a local install. For CoLab, you must repeat these steps each time the
system restarts your virtual environment. You must perform these steps only once for your virtual Python
environment for a local install. If you are installing locally, install to the same virtual environment you
created for this course. The following commands install YOLO directly from its GitHub repository.

Code

! g i t c l o n e h t t p s : / / g i t h u b . com/ u l t r a l y t i c s / y o l o v 5 −−t a g 6 . 1
! mv / c o n t e n t / 6 . 1 / c o n t e n t / y o l o v 5
%cd / c o n t e n t / y o l o v 5
%p i p i n s t a l l −qr

r e q u i r e m e n t s . t x t

from y o l o v 5 import u t i l s

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

241

d i s p l a y = u t i l s . n o t e b o o k _ i n i t ( )

Output

Setup c o m p l e t e

( 1 2 CPUs , 8 3 . 5 GB RAM, 3 9 . 9 / 1 6 6 . 8 GB d i s k )

Next, we will run YOLO from the command line and classify the previously downloaded kitchen picture.

You can run this classiﬁcation on any image you choose.

Code

! python d e t e c t . py −−w e i g h t s y o l o v 5 s . pt −−img 640 \

−−c o n f 0 . 2 5 −−s o u r c e / c o n t e n t / images /

URL = ' / c o n t e n t / y o l o v 5 / r un s / d e t e c t / exp / j e f f _ c o o k . j p g '
d i s p l a y . Image ( f i l e n a m e=URL, width =300)

Output

242

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

t o

s o u r c e=/c o n t e n t / images / ,

imgsz =[640 , 6 4 0 ] , c o n f _ t h r e s =0.25 ,

Downloading h t t p s : / / u l t r a l y t i c s . com/ a s s e t s / A r i a l . t t f
/ r o o t / . c o n f i g / U l t r a l y t i c s / A r i a l . t t f . . .
d e t e c t : w e i g h t s =[ ' y o l o v 5 s . pt ' ] ,
data=data / c o c o 1 2 8 . yaml ,
i o u _ t h r e s =0.45 , max_det=1000 , d e v i c e =, view_img=F a l s e ,
save_conf=F a l s e ,
save_crop=F a l s e , nosave=F a l s e ,
agnostic_nms=F a l s e , augment=F a l s e , v i s u a l i z e=F a l s e , update=F a l s e ,
p r o j e c t=ru n s / d e t e c t , name=exp , e x i s t _ o k=F a l s e ,
l i n e _ t h i c k n e s s =3,
h i d e _ l a b e l s=F a l s e , h i d e _ c o n f=F a l s e , h a l f=F a l s e , dnn=F a l s e
YOLOv5
40536MiB)
Downloading h t t p s : / / g i t h u b . com/ u l t r a l y t i c s / y o l o v 5 / r e l e a s e s / download / v6
. 1 / y o l o v 5 s . pt
100% 1 4 . 1M/ 1 4 . 1M [ 0 0 : 0 0 < 0 0 : 0 0 , 135MB/ s ]
Fusing l a y e r s . . .

v6.1−85− g 6 f 4 e b 9 5 t o r c h 1 . 1 0 . 0 + cu111 CUDA: 0 ( A100−SXM4−40GB,

t o y o l o v 5 s . pt . . .

save_txt=F a l s e ,

c l a s s e s=None ,

. . .

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

243

image 1/1 / c o n t e n t / images / j e f f _ c o o k . j p g : 640 x480 1 person , 1 dog , 3
b o t t l e s , 1 microwave , 2 ovens , 1 s i n k , Done .
Speed : 0 . 6 ms pre−p r o c e s s , 1 5 . 9 ms i n f e r e n c e , 2 9 . 3 ms NMS p e r image a t
shape ( 1 , 3 , 6 4 0 , 6 4 0 )
R e s u l t s

saved t o r u n s / d e t e c t / exp

( 0 . 0 1 6 s )

6.5.3 Running YOLOv5

In addition to the command line execution, we just saw. The following code adds the downloaded YOLOv5
to Python’s environment, allowing yolov5 to be imported like a regular Python library.

Code

import s y s
s y s . path . append ( s t r ( " / c o n t e n t / y o l o v 5 " ) )

from y o l o v 5 import u t i l s
d i s p l a y = u t i l s . n o t e b o o k _ i n i t ( )

Output

Setup c o m p l e t e

( 1 2 CPUs , 8 3 . 5 GB RAM, 3 9 . 9 / 1 6 6 . 8 GB d i s k )

Next, we obtain an image to classify. For this example, the program loads the image from a URL.
YOLOv5 expects that the image is in the format of a Numpy array. We use PIL to obtain this image. We
will convert it to the proper format for PyTorch and YOLOv5 later.

Code

from PIL import Image
import r e q u e s t s
from i o import BytesIO
import t o r c h v i s i o n . t r a n s f o r m s . f u n c t i o n a l a s TF

u r l = " h t t p s : / / raw . g i t h u b u s e r c o n t e n t . com/ j e f f h e a t o n / " \
" t81_558_deep_learning / master / images / cook . j p g "
r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )

' M o z i l l a / 5 . 0 ' } )

The following libraries are needed to classify this image.

244

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Code

import a r g p a r s e
import o s
import s y s
from p a t h l i b import Path

import cv2
import t o r c h
import t o r c h . backends . cudnn a s cudnn

from models . common import DetectMultiBackend
from u t i l s . d a t a s e t s import IMG_FORMATS, VID_FORMATS, LoadImages , LoadStreams
import (LOGGER, c h e c k _ f i l e , check_img_size , check_imshow ,
from u t i l s . g e n e r a l

check_requirements ,
increment_path , non_max_suppression ,
p r i n t _ a r g s ,
s c a l e _ c o o r d s ,
xyxy2xywh )

c o l o r s t r ,

s t r i p _ o p t i m i z e r ,

from u t i l s . p l o t s import Annotator ,
from u t i l s . t o r c h _ u t i l s import s e l e c t _ d e v i c e ,

c o l o r s ,

save_one_box
time_sync

We are now ready to load YOLO with pretrained weights provided by the creators of YOLO. It is also

possible to train YOLO to recognize images of your own.

Code

d e v i c e = s e l e c t _ d e v i c e ( ' ' )
w e i g h t s = ' / c o n t e n t / y o l o v 5 / y o l o v 5 s . pt '
imgsz = [ img . h e i g h t ,
o r i g i n a l _ s i z e = imgsz
model = DetectMultiBackend ( w e i g h t s , d e v i c e=d e v i c e , dnn=F a l s e )
s t r i d e , names , pt ,

img . width ]

j i t , onnx , e n g i n e = model . s t r i d e , model . names , \

model . pt , model . j i t , model . onnx , model . e n g i n e

imgsz = check_img_size ( imgsz ,
print ( f " O r i g i n a l ␣ s i z e : ␣{ o r i g i n a l _ s i z e } " )
print ( f "YOLO␣ i n p u t ␣ s i z e : ␣{ imgsz } " )

s=s t r i d e ) # c h e c k image s i z e

Output

O r i g i n a l
YOLO i n p u t

s i z e :

[ 3 2 0 , 2 4 0 ]

s i z e :

[ 3 2 0 , 2 5 6 ]

The creators of YOLOv5 built upon PyTorch, which has a particular format for images. PyTorch

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

245

images are generally a 4D matrix of the following dimensions:

• batch_size, channels, height, width

This code converts the previously loaded PIL image into this format.

Code

import numpy a s np
s o u r c e = ' / c o n t e n t / images / '

c o n f _ t h r e s =0.25 # c o n f i d e n c e t h r e s h o l d
i o u _ t h r e s =0.45 # NMS IOU t h r e s h o l d
c l a s s e s = None
agnostic_nms=F a l s e , # c l a s s −a g n o s t i c NMS
max_det=1000

model . warmup ( imgsz =(1 , 3 , ∗ imgsz ) ) # warmup
dt ,

s e e n = [ 0 . 0 , 0 . 0 , 0 . 0 ] , 0

# h t t p s : / / s t a c k o v e r f l o w . com/ q u e s t i o n s /50657449/
# c o n v e r t −image−to−proper −dimension−p y t o r c h
img2 = img . r e s i z e ( [ imgsz [ 1 ] , imgsz [ 0 ] ] ,

Image . ANTIALIAS)

img_raw = t o r c h . from_numpy ( np . a s a r r a y ( img2 ) ) . t o ( d e v i c e )
img_raw = img_raw . f l o a t ( ) # u i n t 8 t o f p 1 6 /32
img_raw /= 255 # 0 − 255 t o 0 . 0 − 1 . 0
img_raw = img_raw . unsqueeze_ ( 0 )
img_raw = img_raw . permute ( 0 , 3 , 1 , 2 )
print ( img_raw . shape )

Output

t o r c h . S i z e ( [ 1 , 3 , 3 2 0 , 2 5 6 ] )

With the image converted, we are now ready to present the image to YOLO and obtain predictions.

Code

pred = model ( img_raw , augment=F a l s e , v i s u a l i z e=F a l s e )
pred = non_max_suppression ( pred , c o n f _ t h r e s ,

i o u _ t h r e s ,

c l a s s e s ,

agnostic_nms , max_det=max_det )

We now convert these raw predictions into the bounding boxes, labels, and conﬁdences for each of the

images that YOLO recognized.

246

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Code

r e s u l t s = [ ]
for i , d e t

in enumerate ( pred ) : # p e r image

gn = t o r c h . t e n s o r ( img_raw . shape ) [ [ 1 , 0 , 1 , 0 ] ]

i f

len ( d e t ) :
# R e s c a l e b o x e s from img_size t o im0 s i z e
d e t [ : ,

: 4 ] = s c a l e _ c o o r d s ( o r i g i n a l _ s i z e , d e t [ : ,

: 4 ] ,

imgsz ) . round ( )

# Write r e s u l t s
for ∗xyxy , conf ,

c l s

in reversed ( d e t ) :

xywh = ( xyxy2xywh ( t o r c h . t e n s o r ( xyxy ) . view ( 1 , 4 ) ) / \

gn ) . view ( −1). t o l i s t ( )

# Choose betw ee n xyxy and xywh as your d e s i r e d format .
r e s u l t s . append ( [ names [ int ( c l s ) ] ,

f l o a t ( c o n f ) ,

[ ∗ xyxy ] ] )

We can now see the results from the classiﬁcation. We will display the ﬁrst 3.

Code

for itm in r e s u l t s [ 0 : 3 ] :

print ( itm )

Output

[ ' bowl ' , 0 . 2 8 4 8 4 1 9 5 4 7 0 8 0 9 9 3 7 ,
t e n s o r ( 1 2 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 3 4 . , d e v i c e =' cuda : 0 ' ) ] ]
[ ' oven ' , 0 . 3 1 5 3 1 6 1 7 0 4 5 4 0 2 5 2 7 ,
t e n s o r ( 1 2 8 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 3 1 . , d e v i c e =' cuda : 0 ' ) ] ]
[ ' b o t t l e ' , 0 . 3 5 6 7 5 0 7 5 6 5 0 2 1 5 1 5 ,
t e n s o r ( 8 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 0 1 . , d e v i c e =' cuda : 0 ' ) ] ]

[ t e n s o r ( 5 5 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 9 3 . , d e v i c e =' cuda : 0 ' ) ,

[ t e n s o r ( 2 4 5 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 5 6 . , d e v i c e =' cuda : 0 ' ) ,

[ t e n s o r ( 2 1 5 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 2 2 3 . , d e v i c e =' cuda : 0 ' ) ,

It is important to note that the yolo class instantiated here is a callable object, which can ﬁll the role
of both an object and a function. Acting as a function, yolo returns three arrays named boxes, scores,
and classes that are of the same length. The function returns all sub-images found with a score above
the minimum threshold. Additionally, the yolo function returns an array named called nums. The ﬁrst
element of the nums array speciﬁes how many sub-images YOLO found to be above the score threshold.

• boxes - The bounding boxes for each sub-image detected in the image sent to YOLO.

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

247

• scores - The conﬁdence for each of the sub-images detected.
• classes - The string class names for each item. These are COCO names such as "person" or "dog."
• nums - The number of images above the threshold.

Your program should use these values to perform whatever actions you wish due to the input image. The
following code displays the images detected above the threshold.

To demonstrate the correctness of the results obtained, we draw bounding boxes over the original image.

Code

from PIL import Image ,

ImageDraw

img3 = img . copy ( )
draw = ImageDraw . Draw ( img3 )

fo r itm in r e s u l t s :

b = itm [ 2 ]
print ( b )
draw . r e c t a n g l e ( b )

img3

Output

248

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

[ t e n s o r ( 5 5 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 9 3 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 2 4 5 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 5 6 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 2 1 5 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 2 3 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 1 8 2 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 5 6 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 2 0 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 1 0 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 1 7 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 7 9 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 9 1 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 8 5 . , d e v i c e =' cuda : 0 ' ) ,
[ t e n s o r ( 1 4 2 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 5 3 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 1 2 0 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 1 3 4 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 1 2 8 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 2 3 1 . , d e v i c e =' cuda : 0 ' ) ]
t e n s o r ( 8 0 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 0 1 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 1 0 5 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 1 2 8 . , d e v i c e =' cuda : 0 ' ) ]
t e n s o r ( 7 1 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 1 0 1 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 9 6 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 2 6 9 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 1 7 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 8 3 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 2 9 . , d e v i c e =' cuda : 0 ' ) ,
t e n s o r ( 2 3 3 . , d e v i c e =' cuda : 0 ' ) ]

t e n s o r ( 1 8 3 . , d e v i c e =' cuda : 0 ' ) ,

t e n s o r ( 2 6 7 . , d e v i c e =' cuda : 0 ' ) ]

6.5. PART 6.5: RECOGNIZING MULTIPLE IMAGES WITH YOLO5

249

6.5.4 Module 6 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 6

250

CHAPTER 6. CONVOLUTIONAL NEURAL NETWORKS (CNN) FOR COMPUTER VISION

Chapter 7

Generative Adversarial Networks

7.1 Part 7.1: Introduction to GANS for Image and Data Gener-

ation

A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow
in 2014.[10]Two neural networks compete with each other in a game. The GAN training algorithm starts
with a training set and learns to generate new data with the same distributions as the training set. For
example, a GAN trained on photographs can generate new photographs that look at least superﬁcially
authentic to human observers, having many realistic characteristics.

This chapter makes use of the PyTorch framework rather than Keras/TensorFlow. While there are
versions of StyleGAN2-ADA that work with TensorFlow 1.0, NVIDIA has switched to PyTorch for Style-
GAN. Running this notebook in this notebook in Google CoLab is the most straightforward means of
completing this chapter. Because of this, I designed this notebook to run in Google CoLab. It will take
some modiﬁcations if you wish to run it locally.

This original StyleGAN paper used neural networks to automatically generate images for several pre-
viously seen datasets: MINST and CIFAR. However, it also included the Toronto Face Dataset (a private
dataset used by some researchers). You can see some of these images in Figure 7.1.

Only sub-ﬁgure D made use of convolutional neural networks. Figures A-C make use of fully con-
nected neural networks. As we will see in this module, the researchers signiﬁcantly increased the role of
convolutional neural networks for GANs.

We call a GAN a generative model because it generates new data. You can see the overall process in

Figure 7.2.

7.1.1 Face Generation with StyleGAN and Python

GANs have appeared frequently in the media, showcasing their ability to generate highly photorealistic
faces. One signiﬁcant step forward for realistic face generation was the NVIDIA StyleGAN series. NVIDIA
introduced the origional StyleGAN in 2018.[17]StyleGAN was followed by StyleGAN2 in 2019, which im-
proved the quality of StyleGAN by removing certian artifacts.[18]Most recently, in 2020, NVIDIA released

251

252

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

Figure 7.1: GAN Generated Images

StyleGAN2 adaptive discriminator augmentation (ADA), which will be the focus of this module.[16]We
will see both how to train StyleGAN2 ADA on any arbitray set of images; as well as use pretrained weights
provided by NVIDIA. The NVIDIA weights allow us to generate high resolution photorealistic looking
faces, such seen in Figure 7.3.

The above images were generated with StyleGAN2, using Google CoLab. Following the instructions in
this section, you will be able to create faces like this of your own. StyleGAN2 images are usually 1,024 x
1,024 in resolution. An example of a full-resolution StyleGAN image can be found here.

The primary advancement introduced by the adaptive discriminator augmentation is that the algorithm
augments the training images in real-time. Image augmentation is a common technique in many convolution
neural network applications. Augmentation has the eﬀect of increasing the size of the training set. Where
StyleGAN2 previously required over 30K images for an eﬀective to develop an eﬀective neural network;
now much fewer are needed. I used 2K images to train the ﬁsh generating GAN for this section. Figure
7.4 demonstrates the ADA process.

The ﬁgure shows the increasing probability of augmentation as p increases. For small image sets, the
discriminator will generally memorize the image set unless the training algorithm makes use of augmen-
tation. Once this memorization occurs, the discriminator is no longer providing useful information to the
training of the generator.

While the above images look much more realistic than images generated earlier in this course, they
are not perfect. Look at Figure 7.5. There are usually several tell-tail signs that you are looking at a
computer-generated image. One of the most obvious is usually the surreal, dream-like backgrounds. The
background does not look obviously fake at ﬁrst glance; however, upon closer inspection, you usually can’t
quite discern what a GAN-generated background is. Also, look at the image character’s left eye. It is

7.1. PART 7.1: INTRODUCTION TO GANS FOR IMAGE AND DATA GENERATION

253

Figure 7.2: GAN Structure

Figure 7.3: StyleGAN2 Generated Faces

slightly unrealistic looking, especially near the eyelashes.

Look at the following GAN face. Can you spot any imperfections?

• Image A demonstrates the abstract backgrounds usually associated with a GAN-generated image.
• Image B exhibits issues that earrings often present for GANs. GANs sometimes have problems with

symmetry, particularly earrings.

• Image C contains an abstract background and a highly distorted secondary image.
• Image D also contains a highly distorted secondary image that might be a hand.

Several websites allow you to generate GANs of your own without any software.

• This Person Does not Exist
• Which Face is Real

The ﬁrst site generates high-resolution images of human faces. The second site presents a quiz to see if
you can detect the diﬀerence between a real and fake human face image.

In this chapter, you will learn to create your own StyleGAN pictures using Python.

7.1.2 Generating High Rez GAN Faces with Google CoLab

This notebook demonstrates how to run NVidia StyleGAN2 ADA inside a Google CoLab notebook.
I
suggest you use this to generate GAN faces from a pretrained model. If you try to train your own, you

254

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

Figure 7.4: StyleGAN2 ADA Training

will run into compute limitations of Google CoLab. Make sure to run this code on a GPU instance. GPU
is assumed.

First, we clone StyleGAN3 from GitHub.

Code

! g i t c l o n e h t t p s : / / g i t h u b . com/ NVlabs / s t y l e g a n 3 . g i t
! p i p i n s t a l l n i n j a

Verify that StyleGAN has been cloned.

Code

! l s / c o n t e n t / s t y l e g a n 3

Output

a vg _spec t ra . py
c a l c _ m e t r i c s . py
v i s u a l i z e r . py
d a t a s e t _ t o o l . py
d n n l i b

D o c k e r f i l e
d o c s

gen_video . py m e t r i c s
g u i _ u t i l s

README.md

t r a i n . py

environment . yml
gen_images . py

l e g a c y . py
LICENSE . t x t

t o r c h _ u t i l s
t r a i n i n g

v i z

7.1.3 Run StyleGan From Command Line

Add the StyleGAN folder to Python so that you can import it. I based this code below on code from
NVidia for the original StyleGAN paper. When you use StyleGAN you will generally create a GAN from a
seed number. This seed is an integer, such as 6600, that will generate a unique image. The seed generates
a latent vector containing 512 ﬂoating-point values. The GAN code uses the seed to generate these 512
values. The seed value is easier to represent in code than a 512 value vector; however, while a small change
to the latent vector results in a slight change to the image, even a small change to the integer seed value
will produce a radically diﬀerent image.

7.1. PART 7.1: INTRODUCTION TO GANS FOR IMAGE AND DATA GENERATION

255

Figure 7.5: StyleGAN2 Face

Code

URL = " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s e a r c h / " \

" s t y l e g a n 3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l "

! python / c o n t e n t / s t y l e g a n 3 / gen_images . py \

−−network={URL} \

−−o u t d i r =/c o n t e n t / r e s u l t s −−s e e d s =6600−6625

We can now display the images created.

Code

! l s / c o n t e n t / r e s u l t s

256

Output

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

s e e d 6 6 0 0 . png
s e e d 6 6 0 1 . png
s e e d 6 6 0 2 . png
s e e d 6 6 0 3 . png
s e e d 6 6 0 4 . png
s e e d 6 6 0 5 . png

s e e d 6 6 0 6 . png
s e e d 6 6 0 7 . png
s e e d 6 6 0 8 . png
s e e d 6 6 0 9 . png
s e e d 6 6 1 0 . png
s e e d 6 6 1 1 . png

s e e d 6 6 1 2 . png
s e e d 6 6 1 3 . png
s e e d 6 6 1 4 . png
s e e d 6 6 1 5 . png
s e e d 6 6 1 6 . png
s e e d 6 6 1 7 . png

s e e d 6 6 1 8 . png
s e e d 6 6 1 9 . png
s e e d 6 6 2 0 . png
s e e d 6 6 2 1 . png
s e e d 6 6 2 2 . png
s e e d 6 6 2 3 . png

s e e d 6 6 2 4 . png
s e e d 6 6 2 5 . png

Next, copy the images to a folder of your choice on GDrive.

Code

! cp / c o n t e n t / r e s u l t s /∗ \

/ c o n t e n t / d r i v e /My\ Drive / p r o j e c t s / s t y l e g a n 3

7.1.4 Run StyleGAN From Python Code

Add the StyleGAN folder to Python so that you can import it.

Code

" / c o n t e n t / s t y l e g a n 3 " )

import s y s
s y s . path . i n s e r t ( 0 ,
import p i c k l e
import o s
import numpy a s np
import PIL . Image
from IPython . d i s p l a y import Image
import m a t p l o t l i b . p y p l o t a s p l t
import IPython . d i s p l a y
import t o r c h
import d n n l i b
import l e g a c y

def s e e d 2 v e c (G,

s e e d ) :

return np . random . RandomState ( s e e d ) . randn ( 1 , G. z_dim )

def di spl ay _im ag e ( image ) :

p l t . a x i s ( ' o f f ' )
p l t . imshow ( image )
p l t . show ( )

7.1. PART 7.1: INTRODUCTION TO GANS FOR IMAGE AND DATA GENERATION

257

def generate_image (G, z ,
# Render images
Gs_kwargs = {

t r u n c a t i o n _ p s i ) :

f o r d l a t e n t s

i n i t i a l i z e d from random s e e d s .

' output_transform ' : dict ( f u n c= t f l i b . convert_images_to_uint8 ,
nchw_to_nhwc=True ) ,
' randomize_noise ' : F a l s e

}
i f

t r u n c a t i o n _ p s i

i s not None :

Gs_kwargs [ ' t r u n c a t i o n _ p s i ' ] = t r u n c a t i o n _ p s i

l a b e l = np . z e r o s ( [ 1 ] + G. input_shapes [ 1 ] [ 1 : ] )
# [ m i n i b a t c h , h e i g h t , width , c h a n n e l ]
images = G. run ( z ,
return images [ 0 ]

l a b e l , ∗∗G_kwargs )

def g e t _ l a b e l (G, d e v i c e , c l a s s _ i d x ) :

l a b e l = t o r c h . z e r o s ( [ 1 , G. c_dim ] , d e v i c e=d e v i c e )
i f G. c_dim != 0 :

i f

c l a s s _ i d x i s None :

c t x . f a i l ( " Must␣ s p e c i f y ␣ c l a s s ␣ l a b e l ␣ with ␣−−c l a s s ␣when␣ u s i n g ␣ " \

" a␣ c o n d i t i o n a l ␣ network " )

l a b e l [ : ,

c l a s s _ i d x ] = 1

e l s e :

i f

c l a s s _ i d x i s not None :

print ( " warn : ␣−−c l a s s=l b l ␣ i g n o r e d ␣when␣ r u n n i n g ␣on␣ " \

" an␣ u n c o n d i t i o n a l ␣ network " )

return l a b e l

def generate_image ( d e v i c e , G, z ,

t r u n c a t i o n _ p s i =1.0 , noise_mode= ' c o n s t ' ,

c l a s s _ i d x=None ) :

z = t o r c h . from_numpy ( z ) . t o ( d e v i c e )
l a b e l = g e t _ l a b e l (G, d e v i c e ,
img = G( z ,
l a b e l ,
img = ( img . permute ( 0 , 2 , 3 , 1 ) ∗ 1 2 7 . 5 + 1 2 8 ) . clamp ( 0 , 2 5 5 ) . t o ( \

t r u n c a t i o n _ p s i=t r u n c a t i o n _ p s i , noise_mode=noise_mode )

c l a s s _ i d x )

t o r c h . u i n t 8 )

return PIL . Image . f r o m a r r a y ( img [ 0 ] . cpu ( ) . numpy ( ) ,

'RGB ' )

Code

#URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / p r e t r a i n e d −gan−f i s h / r e l e a s e s /"\
# " download / 1 . 0 . 0 / f i s h −gan −2020−12−09. p k l "
#URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / p r e t r a i n e d −merry−gan−mas/ r e l e a s e s /"\

258

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

# " download / v1 / c h r i s t m a s −gan −2020−12−03. p k l "
URL = " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s e a r c h / s t y l e g a n 3 / " \

" v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l "

print ( f ' Loading ␣ networks ␣ from ␣ " {URL } " . . . ' )
d e v i c e = t o r c h . d e v i c e ( ' cuda ' )
with d n n l i b . u t i l . open_url (URL) a s

f :

G = l e g a c y . load_network_pkl ( f ) [ 'G_ema ' ] . t o ( d e v i c e ) # t y p e :

i g n o r e

Output

Loading networks from " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s
e a r c h / s t y l e g a n 3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l " . . .

We can now generate images from integer seed codes in Python.

Code

# Choose your own s t a r t i n g and e n d i n g s e e d .
SEED_FROM = 1000
SEED_TO = 1003

# Generate t h e images
for i

in range (SEED_FROM, SEED_TO) :

f o r

t h e s e e d s .

print ( f " Seed ␣{ i } " )
z = s e e d 2 v e c (G,
i )
img = generate_image ( d e v i c e , G, z )
di spl ay _im ag e ( img )

Output

7.1. PART 7.1: INTRODUCTION TO GANS FOR IMAGE AND DATA GENERATION

259

Seed 1000
S e t t i n g up PyTorch p l u g i n " b i a s _ a c t _ p l u g i n " . . . Done .
S e t t i n g up PyTorch p l u g i n " f i l t e r e d _ l r e l u _ p l u g i n " . . . Done .

Seed 1001

260

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

Seed 1002

7.1.5 Examining the Latent Vector

Figure 7.6 shows the eﬀects of transforming the latent vector between two images. We accomplish this
transformation by slowly moving one 512-value latent vector to another 512 vector. A high-dimension
point between two latent vectors will appear similar to both of the two endpoint latent vectors. Images
that have similar latent vectors will appear similar to each other.

Figure 7.6: Transforming the Latent Vector

Code

def expand_seed ( s e e d s , v e c t o r _ s i z e ) :

r e s u l t = [ ]

for s e e d in s e e d s :

7.1. PART 7.1: INTRODUCTION TO GANS FOR IMAGE AND DATA GENERATION

261

rnd = np . random . RandomState ( s e e d )
r e s u l t . append ( rnd . randn ( 1 , v e c t o r _ s i z e )

)

return r e s u l t

#URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / p r e t r a i n e d −gan−f i s h / r e l e a s e s /"\
# " download / 1 . 0 . 0 / f i s h −gan −2020−12−09. p k l "
#URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / p r e t r a i n e d −merry−gan−mas/ r e l e a s e s /"\
# " download / v1 / c h r i s t m a s −gan −2020−12−03. p k l "
#URL = " h t t p s : / / n v l a b s −f i −cdn . n v i d i a . com/ s t y l e g a n 2 −ada / p r e t r a i n e d / f f h q . p k l "
URL = " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s e a r c h / s t y l e g a n 3 / " \

" v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l "

print ( f ' Loading ␣ networks ␣ from ␣ " {URL } " . . . ' )
d e v i c e = t o r c h . d e v i c e ( ' cuda ' )
with d n n l i b . u t i l . open_url (URL) a s

f :

G = l e g a c y . load_network_pkl ( f ) [ 'G_ema ' ] . t o ( d e v i c e ) # t y p e :

i g n o r e

v e c t o r _ s i z e = G. z_dim
# range ( 8 1 9 2 , 8 3 0 0 )
s e e d s = expand_seed (
#g e n e r a t e _ i m a g e s (Gs ,
print ( s e e d s [ 0 ] . shape )

Output

[ 8 1 9 2 + 1 , 8 1 9 2 + 9 ] , v e c t o r _ s i z e )
s e e d s , t r u n c a t i o n _ p s i =0.5)

Loading networks from " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s
e a r c h / s t y l e g a n 3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l " . . .
( 1 , 5 1 2 )

The following code will move between the provided seeds. The constant STEPS specify how many

frames there should be between each seed.

Code

# Choose your s e e d s
# t a k e t o g e t

t o each .

t o morph t h r o u g h and t h e number o f

s t e p s

t o

SEEDS = [ 6 6 2 4 , 6 6 1 8 , 6 6 1 6 ] # B e t t e r
#SEEDS = [ 1 0 0 0 , 1 0 0 3 , 1 0 0 1 ] # B e t t e r
STEPS = 100

f o r

f o r

f a c e s
f i s h

# Remove any p r i o r
! rm / c o n t e n t / r e s u l t s /∗

r e s u l t s

262

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

from tqdm . notebook import tqdm

o s . makedirs ( " . / r e s u l t s / " , e x i s t _ o k=True )

# Generate t h e images
i d x = 0
for i

in range ( len (SEEDS) −1):

f o r

t h e v i d e o .

v1 = s e e d 2 v e c (G, SEEDS [ i ] )
v2 = s e e d 2 v e c (G, SEEDS [ i +1])

d i f f = v2 − v1
s t e p = d i f f / STEPS
c u r r e n t = v1 . copy ( )

for j

in tqdm ( range (STEPS ) , d e s c=f " Seed ␣ {SEEDS [ i ] } " ) :

c u r r e n t = c u r r e n t + s t e p
img = generate_image ( d e v i c e , G,
img . s a v e ( f ' . / r e s u l t s / frame −{i d x } . png ' )
i d x+=1

c u r r e n t )

# Link t h e images
! ffmpeg −r 30 − i / c o n t e n t / r e s u l t s / frame−%d . png −vcodec mpeg4 −y movie . mp4

i n t o a v i d e o .

You can now download the generated video.

Code

from g o o g l e . c o l a b import f i l e s
f i l e s . download ( ' movie . mp4 ' )

Output

<IPython . c o r e . d i s p l a y . J a v a s c r i p t
o b j e c t ><IPython . c o r e . d i s p l a y . J a v a s c r i p t o b j e c t >

7.1.6 Module 7 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 7

7.2. PART 7.2: TRAIN STYLEGAN3 WITH YOUR IMAGES

263

7.2 Part 7.2: Train StyleGAN3 with your Images

Training GANs with StyleGAN is resource-intensive. The NVIDA StyleGAN researchers used computers
with eight high-end GPUs for the high-resolution face GANs trained by NVIDIA. The GPU used by
NVIDIA is an A100, which has more memory and cores than the P100 or V100 oﬀered by even Colab
Pro+.
In this part, we will use StyleGAN2 to train rather than StyleGAN3. You can use networks
trained with StyleGAN2 from StyleGAN3; however, StyleGAN3 usually is more eﬀective at training than
StyleGAN2.

Unfortunately, StyleGAN3 is compute-intensive and will perform slowly on any GPU that is not the
latest Ampere technology. Because Colab does not provide such technology, I am keeping the training
guide at the StyleGAN2 level. Switching to StyleGAN3 is relatively easy, as will be pointed out later.

Make sure that you are running this notebook with a GPU runtime. You can train GANs with either
Google Colab Free or Pro.
I recommend at least the Pro version due to better GPU instances, longer
runtimes, and timeouts. Additionally, the capability of Google Colab Pro to run in the background is
valuable when training GANs, as you can close your browser or reboot your laptop while training continues.
You will store your training data and trained neural networks to GDRIVE. For GANs, I lay out my

GDRIVE like this:

• ./data/gan/images - RAW images I wish to train on.
• ./data/gan/datasets - Actual training datasets that I convert from the raw images.
• ./data/gan/experiments - The output from StyleGAN2, my image previews, and saved network

snapshots.

You will mount the drive at the following location.

/ c o n t e n t / d r i v e /MyDrive/ data

7.2.1 What Sort of GPU do you Have?

The type of GPU assigned to you by Colab will signiﬁcantly aﬀect your training time. Some sample times
that I achieved with Colab are given here. I’ve found that Colab Pro generally starts you with a V100,
however, if you run scripts non-stop for 24hrs straight for a few days in a row, you will generally be throttled
back to a P100.

• 1024x1024 - V100 - 566 sec/tick (CoLab Pro)
• 1024x1024 - P100 - 1819 sec/tick (CoLab Pro)
• 1024x1024 - T4 - 2188 sec/tick (CoLab Free)

By comparison, a 1024x1024 GAN trained with StyleGAN3 on a V100 is 3087 sec/tick.

If you use Google CoLab Pro, generally, it will not disconnect before 24 hours, even if you (but not your
script) are inactive. Free CoLab WILL disconnect a perfectly good running script if you do not interact
for a few hours. The following describes how to circumvent this issue.

• How to prevent Google Colab from disconnecting?

264

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

7.2.2 Set Up New Environment

You will likely need to train for >24 hours. Colab will disconnect you. You must be prepared to restart
training when this eventually happens. Training is divided into ticks, every so many ticks (50 by default),
your neural network is evaluated, and a snapshot is saved. When CoLab shuts down, all training after
the last snapshot is lost. It might seem desirable to snapshot after each tick; however, this snapshotting
process itself takes nearly an hour. Learning an optimal snapshot size for your resolution and training data
is important.

We will mount GDRIVE so that you will save your snapshots there. You must also place your training

images in GDRIVE.

You must also install NVIDIA StyleGAN2 ADA PyTorch. We also need to downgrade PyTorch to a

version that supports StyleGAN.

Code

! p i p i n s t a l l
! g i t c l o n e h t t p s : / / g i t h u b . com/ NVlabs / s t y l e g a n 2 −ada−p y t o r c h . g i t
! p i p i n s t a l l n i n j a

t o r c h ==1.8.1 t o r c h v i s i o n ==0.9.1

7.2.3 Find Your Files

The drive is mounted to the following location.

/ c o n t e n t / d r i v e /MyDrive/ data

It might be helpful to use an ls command to establish the exact path for your images.

Code

! l s / c o n t e n t / d r i v e /MyDrive/ data / gan / images

7.2.4 Convert Your Images

You must convert your images into a data set form that PyTorch can directly utilize. The following
command converts your images and writes the resulting data set to another directory.

Code

CMD = " python ␣ / c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h / d a t a s e t _ t o o l . py␣ " \
"−−s o u r c e ␣/ c o n t e n t / d r i v e /MyDrive/ data / gan / images / c i r c u i t ␣ " \
"−−d e s t ␣/ c o n t e n t / d r i v e /MyDrive/ data / gan / d a t a s e t / c i r c u i t "

! {CMD}

7.2. PART 7.2: TRAIN STYLEGAN3 WITH YOUR IMAGES

265

You can use the following command to clear out the newly created dataset. If something goes wrong
and you need to clean up your images and rerun the above command, you should delete your partially
completed dataset directory.

Code

#! rm −R / c o n t e n t / d r i v e /MyDrive/ d a t a / gan / d a t a s e t / c i r c u i t /∗

7.2.5 Clean Up your Images

All images must have the same dimensions and color depth. This code can identify images that have issues.

Code

from o s import l i s t d i r
from o s . path import i s f i l e ,
import o s
from PIL import Image
from tqdm . notebook import tqdm

j o i n

IMAGE_PATH = ' / c o n t e n t / d r i v e /MyDrive/ data / gan / images / f i s h '
f i l e s = [ f

in l i s t d i r (IMAGE_PATH)

i s f i l e ( j o i n (IMAGE_PATH,

for f

i f

f ) ) ]

b a s e _ s i z e = None
for f i l e in tqdm ( f i l e s ) :

f i l e 2 = o s . path . j o i n (IMAGE_PATH, f i l e )
img = Image . open ( f i l e 2 )
s z = img . s i z e
i f b a s e _ s i z e and s z != b a s e _ s i z e :

print ( f " I n c o n s i s t a n t ␣ s i z e : ␣{ f i l e 2 } " )

e l i f

img . mode!= 'RGB ' :

print ( f " I n c o n s i s t a n t ␣ c o l o r ␣ format : ␣{ f i l e 2 } " )

e l s e :

b a s e _ s i z e = s z

7.2.6 Perform Initial Training

This code performs the initial training. Set SNAP low enough to get a snapshot before Colab forces you
to quit.

Code

import o s

266

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

# Modify t h e s e t o s u i t your needs
EXPERIMENTS = " / c o n t e n t / d r i v e /MyDrive/ data / gan / e x p e r i m e n t s "
DATA = " / c o n t e n t / d r i v e /MyDrive/ data / gan / d a t a s e t / c i r c u i t "
SNAP = 10

# B u i l d t h e command and run i t
cmd = f " / u s r / b i n / python3 ␣/ c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h / t r a i n . py␣ " \

f "−−snap ␣{SNAP}␣−−o u t d i r ␣{EXPERIMENTS} ␣−−data ␣{DATA} "

! { cmd}

7.2.7 Resume Training

You can now resume training after you are interrupted by something in the pervious step.

Code

import o s

# Modify t h e s e t o s u i t your nee ds
EXPERIMENTS = " / c o n t e n t / d r i v e /MyDrive/ data / gan / e x p e r i m e n t s "
NETWORK = " network−snapshot −000100. p k l "
RESUME = o s . path . j o i n (EXPERIMENTS, \

" 00008− c i r c u i t −auto1−resumecustom " , NETWORK)

DATA = " / c o n t e n t / d r i v e /MyDrive/ data / gan / d a t a s e t / c i r c u i t "
SNAP = 10

# B u i l d t h e command and run i t
cmd = f " / u s r / b i n / python3 ␣/ c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h / t r a i n . py␣ " \

f "−−snap ␣{SNAP}␣−−resume ␣{RESUME}␣−−o u t d i r ␣ {EXPERIMENTS} ␣−−data ␣{DATA} "

! { cmd}

7.3 Part 7.3: Exploring the StyleGAN Latent Vector

StyleGAN seeds, such as 3000, are only random number seeds used to generate much longer 512-length
latent vectors, which create the GAN image. If you make a small change to the seed, for example, change
3000 to 3001, StyleGAN will create an entirely diﬀerent picture. However, if you make a small change to a
few latent vector values, the image will only change slightly. In this part, we will see how we can ﬁne-tune
the latent vector to control, to some degree, the resulting GAN image appearance.

7.3. PART 7.3: EXPLORING THE STYLEGAN LATENT VECTOR

267

7.3.1

Installing Needed Software

We begin by installing StyleGAN.

Code

! g i t c l o n e h t t p s : / / g i t h u b . com/ NVlabs / s t y l e g a n 3 . g i t
! p i p i n s t a l l n i n j a

We will use the same functions introduced in the previous part to generate GAN seeds and images.

Code

" / c o n t e n t / s t y l e g a n 3 " )

import s y s
s y s . path . i n s e r t ( 0 ,
import p i c k l e
import o s
import numpy a s np
import PIL . Image
from IPython . d i s p l a y import Image
import m a t p l o t l i b . p y p l o t a s p l t
import IPython . d i s p l a y
import t o r c h
import d n n l i b
import l e g a c y

def s e e d 2 v e c (G,

s e e d ) :

return np . random . RandomState ( s e e d ) . randn ( 1 , G. z_dim )

def di spl ay _im ag e ( image ) :

p l t . a x i s ( ' o f f ' )
p l t . imshow ( image )
p l t . show ( )

def generate_image (G, z ,
# Render images
Gs_kwargs = {

t r u n c a t i o n _ p s i ) :

f o r d l a t e n t s

i n i t i a l i z e d from random s e e d s .

' output_transform ' : dict ( f u n c= t f l i b . convert_images_to_uint8 ,
nchw_to_nhwc=True ) ,
' randomize_noise ' : F a l s e

}
i f

t r u n c a t i o n _ p s i

i s not None :

Gs_kwargs [ ' t r u n c a t i o n _ p s i ' ] = t r u n c a t i o n _ p s i

l a b e l = np . z e r o s ( [ 1 ] + G. input_shapes [ 1 ] [ 1 : ] )

268

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

# [ m i n i b a t c h , h e i g h t , width , c h a n n e l ]
images = G. run ( z ,
return images [ 0 ]

l a b e l , ∗∗G_kwargs )

def g e t _ l a b e l (G, d e v i c e , c l a s s _ i d x ) :

l a b e l = t o r c h . z e r o s ( [ 1 , G. c_dim ] , d e v i c e=d e v i c e )
i f G. c_dim != 0 :

i f

c l a s s _ i d x i s None :

c t x . f a i l ( ' Must␣ s p e c i f y ␣ c l a s s ␣ l a b e l ␣ with ␣−−c l a s s ' \

' when␣ u s i n g ␣a␣ c o n d i t i o n a l ␣ network ' )

l a b e l [ : ,

c l a s s _ i d x ] = 1

e l s e :

i f

c l a s s _ i d x i s not None :

print ( ' warn : ␣−−c l a s s=l b l ␣ i g n o r e d ␣when␣ r u n n i n g ␣ ' \

' on␣an␣ u n c o n d i t i o n a l ␣ network ' )

return l a b e l

def generate_image ( d e v i c e , G, z ,

t r u n c a t i o n _ p s i =1.0 ,

noise_mode= ' c o n s t ' , c l a s s _ i d x=None ) :

z = t o r c h . from_numpy ( z ) . t o ( d e v i c e )
l a b e l = g e t _ l a b e l (G, d e v i c e , c l a s s _ i d x )
img = G( z ,

l a b e l ,

t r u n c a t i o n _ p s i=t r u n c a t i o n _ p s i ,

noise_mode=noise_mode )

img = ( img . permute ( 0 , 2 , 3 , 1 ) ∗ 1 2 7 . 5 + 1 2 8 ) \

. clamp ( 0 , 2 5 5 ) . t o ( t o r c h . u i n t 8 )

return PIL . Image . f r o m a r r a y ( img [ 0 ] . cpu ( ) . numpy ( ) ,

'RGB ' )

Next, we load the NVIDIA FFHQ (faces) GAN. We could use any StyleGAN pretrained GAN network

here.

Code

# HIDE CODE

URL = " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s e a r c h / " \
" s t y l e g a n 3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l "

print ( ' Loading ␣ networks ␣ from ␣"%s " . . . ' % URL)
d e v i c e = t o r c h . d e v i c e ( ' cuda ' )
with d n n l i b . u t i l . open_url (URL) a s

f p :

G = l e g a c y . load_network_pkl ( f p ) [ 'G_ema ' ] \

. r e q u i r e s _ g r a d _ ( F a l s e ) . t o ( d e v i c e )

7.3. PART 7.3: EXPLORING THE STYLEGAN LATENT VECTOR

269

Output

Loading networks from " h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s
e a r c h / s t y l e g a n 3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l " . . .
Downloading h t t p s : / / a p i . ngc . n v i d i a . com/ v2 / models / n v i d i a / r e s e a r c h / s t y l e
gan3 / v e r s i o n s /1/ f i l e s / s t y l e g a n 3 −r−f f h q −1024 x1024 . p k l

. . . done

7.3.2 Generate and View GANS from Seeds

We will begin by generating a few seeds to evaluate potential starting points for our ﬁne-tuning. Try out
diﬀerent seeds ranges until you have a seed that looks close to what you wish to ﬁne-tune.

Code

# Choose your own s t a r t i n g and e n d i n g s e e d .
SEED_FROM = 4020
SEED_TO = 4023

# Generate t h e images
fo r i

in range (SEED_FROM, SEED_TO) :

f o r

t h e s e e d s .

print ( f " Seed ␣{ i } " )
z = s e e d 2 v e c (G,
i )
img = generate_image ( d e v i c e , G, z )
di spl ay _im ag e ( img )

Output

270

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

Seed 4020
S e t t i n g up PyTorch p l u g i n " b i a s _ a c t _ p l u g i n " . . . Done .
S e t t i n g up PyTorch p l u g i n " f i l t e r e d _ l r e l u _ p l u g i n " . . . Done .

. . .

7.3.3 Fine-tune an Image

If you ﬁnd a seed you like, you can ﬁne-tune it by directly adjusting the latent vector. First, choose the
seed to ﬁne-tune.

Code

START_SEED = 4022

c u r r e n t = s e e d 2 v e c (G, START_SEED)

Next, generate and display the current vector. You will return to this point for each iteration of the

ﬁnetuning.

Code

img = generate_image ( d e v i c e , G,

c u r r e n t )

7.3. PART 7.3: EXPLORING THE STYLEGAN LATENT VECTOR

271

SCALE = 0 . 5
di spl ay _im ag e ( img )

Output

Choose an explore size; this is the number of diﬀerent potential images chosen by moving in 10 diﬀerent
directions. Run this code once and then again anytime you wish to change the ten directions you are
exploring. You might change the ten directions if you are no longer seeing improvements.

Code

EXPLORE_SIZE = 25

e x p l o r e = [ ]
fo r i

in range (EXPLORE_SIZE ) :

e x p l o r e . append ( np . random . rand ( 1 , 5 1 2 ) − 0 . 5 )

Each image displayed from running this code shows a potential direction that we can move in the latent
vector. Choose one image that you like and change MOVE_DIRECTION to indicate this decision. Once
you rerun the code, the code will give you a new set of potential directions. Continue this process until
you have a latent vector that you like.

272

Code

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

# Choose t h e d i r e c t i o n t o move . Choose −1 f o r
MOVE_DIRECTION = −1
SCALE = 0 . 5

t h e

i n i t i a l

i t e r a t i o n .

i f MOVE_DIRECTION >=0:

c u r r e n t = c u r r e n t + e x p l o r e [MOVE_DIRECTION]

for i , mv in enumerate ( e x p l o r e ) :

print ( f " D i r e c t i o n ␣{ i } " )
z = c u r r e n t + mv
img = generate_image ( d e v i c e , G, z )
di spl ay _im ag e ( img )

Output

D i r e c t i o n 0

. . .

7.4. PART 7.4: GANS TO ENHANCE OLD PHOTOGRAPHS DEOLDIFY

273

7.4 Part 7.4: GANS to Enhance Old Photographs Deoldify

For the last two parts of this module, we will examine two applications of GANs. The ﬁrst application
is named deoldify, which uses a PyTorche-based GAN to transform old photographs into more modern-
looking images. The complete source code to Deoldify is provided, along with several examples notebooks
upon which I based this part.

7.4.1

Install Needed Software

We begin by cloning the deoldify repository.

Code

! g i t c l o n e h t t p s : / / g i t h u b . com/ j a n t i c / DeOldify . g i t DeOldify
%cd DeOldify

Install any additional Python packages needed.

Code

! p i p i n s t a l l −r c o l a b _ r e q u i r e m e n t s . t x t

Install the pretrained weights for deoldify.

Code

! mkdir
CMD = " wget ␣ h t t p s : / / data . d e e p a i . o r g / d e o l d i f y / C o l o r i z e A r t i s t i c _ g e n . pth " \

' . / models / '

" ␣−O␣ . / models / C o l o r i z e A r t i s t i c _ g e n . pth "

! {CMD}

The authors of deoldify suggest that you might wish to include a watermark to let others know that
AI-enhanced this picture. The following code downloads this standard watermark. The authors describe
the watermark as follows:

"This places a watermark icon of a palette at the bottom left corner of the image. The authors intend
this practice to be a standard way to convey to others viewing the image that AI colorizes it. We want
to help promote this as a standard, especially as the technology continues to improve and the distinction
between real and fake becomes harder to discern. This palette watermark practice was initiated and led
by the MyHeritage in the MyHeritage In Color feature (which uses a newer version of DeOldify than what
you’re using here)."

Code

CMD = " wget ␣ h t t p s : / / media . g i t h u b u s e r c o n t e n t . com/ media / j a n t i c / " \

" DeOldify / master / r e s o u r c e _ i m a g e s / watermark . png␣ " \
"−O␣/ c o n t e n t / DeOldify / r e s o u r c e _ i m a g e s / watermark . png "

274

! {CMD}

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

7.4.2 Initialize Torch Device

First, we must initialize a Torch device. If we have a GPU available, we will detect it here. I assume that
you will run this code from Google CoLab, with a GPU. It is possible to run this code from a local GPU;
however, some modiﬁcation will be necessary.

Code

import s y s

This must be t h e

#NOTE:
from d e o l d i f y import d e v i c e
from d e o l d i f y . d e v i c e _ i d import D e v i c e I d
#c h o i c e s : CPU, GPU0 . . . GPU7
d e v i c e . set ( d e v i c e=D e v i c e I d .GPU0)

f i r s t

c a l l

i n o r d e r t o work p r o p e r l y !

import t o r c h

i f not t o r c h . cuda . i s _ a v a i l a b l e ( ) :

print ( 'GPU␣ not ␣ a v a i l a b l e . ' )

e l s e :

print ( ' Using ␣GPU. ' )

Output

Using GPU.

We can now call the model. I will enhance an image from my childhood, probably taken in the late
1970s. The picture shows three miniature schnauzers. My childhood dog (Scooby) is on the left, followed
by his mom and sister. Overall, a stunning improvement. However, the red in the ﬁre engine riding toy is
lost, and the red color of the picnic table where the three dogs were sitting.

Code

import f a s t a i
from d e o l d i f y . v i s u a l i z e import ∗
import wa rnin gs
from u r l l i b . p a r s e import u r l p a r s e
import o s

7.4. PART 7.4: GANS TO ENHANCE OLD PHOTOGRAPHS DEOLDIFY

275

wa rni ngs . f i l t e r w a r n i n g s ( " i g n o r e " , c a t e g o r y=UserWarning ,

message=" . ∗ ? Your␣ . ∗ ? ␣ s e t ␣ i s ␣empty . ∗ ? " )

URL = ' h t t p s : / / raw . g i t h u b u s e r c o n t e n t . com/ j e f f h e a t o n / ' \

' t81_558_deep_learning / master / p ho t o s / sc oo by _ fam il y . j p g '

! wget {URL}

a = u r l p a r s e (URL)
b e f o r e _ f i l e = o s . path . basename ( a . path )

RENDER_FACTOR = 35
WATERMARK = F a l s e

c o l o r i z e r = g e t _ i m a g e _ c o l o r i z e r ( a r t i s t i c =True )

a f t e r _ i m a g e = c o l o r i z e r . get_transformed_image (

b e f o r e _ f i l e ,
watermarked=WATERMARK)

r e n d e r _ f a c t o r=RENDER_FACTOR,

#p r i n t ( " S t a r t i n g image : " )

You can see the starting image here.

Code

from IPython import d i s p l a y
d i s p l a y . Image (URL)

Output

276

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

You can see the deoldify version here. Please note that these two images will look similar in a black

and white book. To see it in color, visit this link.

Code

a f t e r _ i m a g e

Output

7.5. PART 7.5: GANS FOR TABULAR SYNTHETIC DATA GENERATION

277

7.5 Part 7.5: GANs for Tabular Synthetic Data Generation

Typically GANs are used to generate images. However, we can also generate tabular data from a GAN. In
this part, we will use the Python tabgan utility to create fake data from tabular data. Speciﬁcally, we will
use the Auto MPG dataset to train a GAN to generate fake cars. Cite:ashrapov2020tabular

7.5.1

Installing Tabgan

Pytorch is the foundation of the tabgan neural network utility. The following code installs the needed
software to run tabgan in Google Colab.

Code

CMD = " wget ␣ h t t p s : / / raw . g i t h u b u s e r c o n t e n t . com/ Diyago / " \

"GAN−f o r −t a b u l a r −data / master / r e q u i r e m e n t s . t x t "

! {CMD}
! p i p i n s t a l l −r
! p i p i n s t a l l

tabgan

r e q u i r e m e n t s . t x t

Note, after installing; you may see this message:

• You must restart the runtime in order to use newly installed versions.

If so, click the "restart runtime" button just under the message. Then rerun this notebook, and you should
not receive further issues.

7.5.2 Loading the Auto MPG Data and Training a Neural Network

We will begin by generating fake data for the Auto MPG dataset we have previously seen. The tabgan
library can generate categorical (textual) and continuous (numeric) data. However, it cannot generate
unstructured data, such as the name of the automobile. Car names, such as "AMC Rebel SST" cannot
be replicated by the GAN, because every row has a diﬀerent car name; it is a textual but non-categorical
value.

The following code is similar to what we have seen before. We load the AutoMPG dataset. The tabgan
library requires Pandas dataframe to train. Because of this, we keep both the Pandas and Numpy values.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import pandas a s pd
import i o
import o s

278

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

COLS_USED = [ ' c y l i n d e r s ' ,
' a c c e l e r a t i o n ' ,

' d i s p l a c e m e n t ' ,
' y e a r ' ,

' o r i g i n ' , 'mpg ' ]

' ho rsepower ' ,

' w e i g h t ' ,

COLS_TRAIN = [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ]

d f = d f [COLS_USED]

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# S p l i t
df_x_train , df_x_test , df_y_train , df_y_test = t r a i n _ t e s t _ s p l i t (

i n t o t r a i n i n g and t e s t

s e t s

d f . drop ( "mpg" , a x i s =1) ,
d f [ "mpg" ] ,
t e s t _ s i z e =0.20 ,
#s h u f f l e=F a l s e ,
random_state =42 ,

)

# Create d a t a f r a m e v e r s i o n s
f o r
df_x_test , df_y_test = df_x_test . r e s e t _ i n d e x ( drop=True ) , \

t a b u l a r GAN

df_y_test . r e s e t _ i n d e x ( drop=True )
df_y_train = pd . DataFrame ( df_y_train )
df_y_test = pd . DataFrame ( df_y_test )

# Pandas t o Numpy
x _ t r a i n = df_x_train . v a l u e s
x _ te s t = df_x_test . v a l u e s
y _ t r a i n = df_y_train . v a l u e s
y _ te s t = df_y_test . v a l u e s

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
# Hidden 1

7.5. PART 7.5: GANS FOR TABULAR SYNTHETIC DATA GENERATION

279

input_dim=x _ t r a i n . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 2 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,
c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p o c h s =1000)

We now evaluate the trained neural network to see the RMSE. We will use this trained neural network
to compare the accuracy between the original data and the GAN-generated data. We will later see that
you can use such comparisons for anomaly detection. We can use this technique can be used for security
systems. If a neural network trained on original data does not perform well on new data, then the new
data may be suspect or fake.

Code

pred = model . p r e d i c t ( x _t e s t )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " F i n a l ␣ s c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

Output

F i n a l

s c o r e (RMSE) : 4 . 3 3 6 3 3 9 3 6 4 5 2 5 4 5

7.5.3 Training a GAN for Auto MPG

Next, we will train the GAN to generate fake data from the original MPG data. There are quite a few
options that you can ﬁne-tune for the GAN. The example presented here uses most of the default values.
These are the usual hyperparameters that must be tuned for any model and require some experimentation
for optimal results. To learn more about tabgab refer to its paper or this Medium article, written by the
creator of tabgan.

Code

from tabgan . sam ple r import GANGenerator
import pandas a s pd
import numpy a s np
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

280

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

gen_x , gen_y = GANGenerator ( gen_x_times =1.1 , c a t _ c o l s=None ,

b o t _ f i l t e r _ q u a n t i l e =0.001 ,
i s _ p o s t _ p r o c e s s=True ,
adversarial_model_params={

t o p _ f i l t e r _ q u a n t i l e =0.999 , \

" m e t r i c s " :
" l e a r n i n g _ r a t e " : 0 . 0 2 ,

" rmse " ,

" max_depth " : 2 ,

" max_bin " : 1 0 0 ,

" random_state " : \

4 2 ,

" n _ e s t i m a t o r s " : 5 0 0 ,

} , p r e g e n e r a t i o n _ f r a c =2, only_generated_data=F a l s e , \
gan_params = { " b a t c h _ s i z e " : 5 0 0 ,

" p a t i e n c e " : 2 5 , \

: 5 0 0 , } ) . generate_data_pipe ( df_x_train , df_y_train , \

" e po c h s "
df_x_test , deep_copy=True , o n l y _ a d v e r s a r i a l=F a l s e , \
u s e _ a d v e r s a r i a l=True )

Output

F i t t i n g CTGAN t r a n s f o r m e r s
[ 0 0 : 0 0 < ? , ? i t / s ] T r a i n i n g CTGAN, e p oc h s : :
[ 0 0 : 0 0 < ? , ? i t / s ]

f o r each column :

0%|

0%|

| 0/8

| 0/500

Note: if you receive an error running the above code, you likely need to restart the runtime. You should
have a "restart runtime" button in the output from the second cell. Once you restart the runtime, rerun
all of the cells. This step is necessary as tabgan requires speciﬁc versions of some packages.

7.5.4 Evaluating the GAN Results

If we display the results, we can see that the GAN-generated data looks similar to the original. Some
values, typically whole numbers in the original data, have fractional values in the synthetic data.

Code

gen_x

Output

7.5. PART 7.5: GANS FOR TABULAR SYNTHETIC DATA GENERATION

281

cylinders
5
5
4
5
4
...
8
8
4
4
5

displacement
296.949632
247.744505
259.648421
319.208637
386.237667
...
304.000000
304.000000
98.000000
97.500000
138.526374

horsepower weight
106.872450
97.532052
108.111921
93.764364
129.837418
...
150.000000
150.000000
80.000000
80.000000
68.958515

2133
2233
2424
2054
1951
...
3672
3433
2164
2126
2497

acceleration
18.323035
19.490136
19.898952
19.420225
20.989091
...
11.500000
12.000000
15.000000
17.000000
13.495784

year
73
75
79
78
82
...
72
70
72
72
71

origin
2
2
3
3
2
...
1
1
1
1
1

0
1
2
3
4
...
542
543
544
545
546

Finally, we present the synthetic data to the previously trained neural network to see how accurately
we can predict the synthetic targets. As we can see, you lose some RMSE accuracy by going to synthetic
data.

Code

# P r e d i c t
pred = model . p r e d i c t ( gen_x . v a l u e s )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , gen_y . v a l u e s ) )
print ( " F i n a l ␣ s c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

Output

F i n a l

s c o r e (RMSE) : 9 . 0 8 3 7 4 5 2 2 5 6 3 3 0 9 8

282

CHAPTER 7. GENERATIVE ADVERSARIAL NETWORKS

Chapter 8

Kaggle Data Sets

8.1 Part 8.1: Introduction to Kaggle

Kaggle runs competitions where data scientists compete to provide the best model to ﬁt the data. A simple
project to get started with Kaggle is the Titanic data set. Most Kaggle competitions end on a speciﬁc date.
Website organizers have scheduled the Titanic competition to end on December 31, 20xx (with the year
usually rolling forward). However, they have already extended the deadline several times, and an extension
beyond 2014 is also possible. Second, the Titanic data set is considered a tutorial data set. There is no
prize, and your score in the competition does not count towards becoming a Kaggle Master.

8.1.1 Kaggle Ranks

You achieve Kaggle ranks by earning gold, silver, and bronze medals.

• Kaggle Top Users
• Current Top Kaggle User’s Proﬁle Page
• Jeﬀ Heaton’s (your instructor) Kaggle Proﬁle
• Current Kaggle Ranking System

8.1.2 Typical Kaggle Competition

A typical Kaggle competition will have several components. Consider the Titanic tutorial:

• Competition Summary Page
• Data Page
• Evaluation Description Page
• Leaderboard

283

284

CHAPTER 8. KAGGLE DATA SETS

8.1.3 How Kaggle Competition Scoring

Kaggle is provided with a data set by the competition sponsor, as seen in Figure 8.1. Kaggle divides this
data set as follows:

• Complete Data Set - This is the complete data set.

– Training Data Set - This dataset provides both the inputs and the outcomes for the training

portion of the data set.

– Test Data Set - This dataset provides the complete test data; however, it does not give the

outcomes. Your submission ﬁle should contain the predicted results for this data set.

∗ Public Leaderboard - Kaggle does not tell you what part of the test data set contributes
to the public leaderboard. Your public score is calculated based on this part of the data set.
∗ Private Leaderboard - Likewise, Kaggle does not tell you what part of the test data
set contributes to the public leaderboard. Your ﬁnal score/rank is calculated based on this
part. You do not see your private leaderboard score until the end.

Figure 8.1: How Kaggle Competition Scoring

8.1.4 Preparing a Kaggle Submission

You do not submit the code to your solution to Kaggle. For competitions, you are scored entirely on the
accuracy of your submission ﬁle. A Kaggle submission ﬁle is always a CSV ﬁle that contains the Id of the
row you are predicting and the answer. For the titanic competition, a submission ﬁle looks something like
this:

P a s s e n g e r I d , S u r v i v e d
8 9 2 , 0
8 9 3 , 1

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

285

8 9 4 , 1
8 9 5 , 0
8 9 6 , 0
8 9 7 , 1
. . .

The above ﬁle states the prediction for each of the various passengers. You should only predict on
ID’s that are in the test ﬁle. Likewise, you should render a prediction for every row in the test ﬁle. Some
competitions will have diﬀerent formats for their answers. For example, a multi-classiﬁcation will usually
have a column for each class and your predictions for each class.

8.1.5 Select Kaggle Competitions

There have been many exciting competitions on Kaggle; these are some of my favorites. Some select
predictive modeling competitions which use tabular data include:

• Otto Group Product Classiﬁcation Challenge
• Galaxy Zoo - The Galaxy Challenge
• Practice Fusion Diabetes Classiﬁcation
• Predicting a Biological Response

Many Kaggle competitions include computer vision datasets, such as:

• Diabetic Retinopathy Detection
• Cats vs Dogs
• State Farm Distracted Driver Detection

8.1.6 Module 8 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 8

8.2 Part 8.2: Building Ensembles with Scikit-Learn and Keras

8.2.1 Evaluating Feature Importance

Feature importance tells us how important each feature (from the feature/import vector) is to predicting
a neural network or another model. There are many diﬀerent ways to evaluate the feature importance of
neural networks. The following paper presents an excellent (and readable) overview of the various means
of assessing the signiﬁcance of neural network inputs/features.

• An accurate comparison of methods for quantifying variable importance in artiﬁcial neural networks

using simulated data[27]. Ecological Modelling, 178(3), 389-397.

In summary, the following methods are available to neural networks:

286

CHAPTER 8. KAGGLE DATA SETS

• Connection Weights Algorithm
• Partial Derivatives
• Input Perturbation
• Sensitivity Analysis
• Forward Stepwise Addition
• Improved Stepwise Selection 1
• Backward Stepwise Elimination
• Improved Stepwise Selection

For this chapter, we will use the input Perturbation feature ranking algorithm. This algorithm will work
with any regression or classiﬁcation network. In the next section, I provide an implementation of the input
perturbation algorithm for scikit-learn. This code implements a function below that will work with any
scikit-learn model.

Leo Breiman provided this algorithm in his seminal paper on random forests. [Citebreiman2001random:]
Although he presented this algorithm in conjunction with random forests, it is model-independent and
appropriate for any supervised learning model. This algorithm, known as the input perturbation algorithm,
works by evaluating a trained model’s accuracy with each input individually shuﬄed from a data set.
Shuﬄing an input causes it to become useless---eﬀectively removing it from the model. More important
inputs will produce a less accurate score when they are removed by shuﬄing them. This process makes
sense because important features will contribute to the model’s accuracy. I ﬁrst presented the TensorFlow
implementation of this algorithm in the following paper.

• Early stabilizing feature importance for TensorFlow deep neural networks[11]

This algorithm will use log loss to evaluate a classiﬁcation problem and RMSE for regression.

Code

from s k l e a r n import m e t r i c s
import s c i p y a s sp
import numpy a s np
import math
from s k l e a r n import m e t r i c s

def p e r t u r b a t i o n _ r a n k ( model , x , y , names ,

r e g r e s s i o n ) :

e r r o r s = [ ]

for i

in range ( x . shape [ 1 ] ) :

h o l d = np . a r r a y ( x [ : ,
np . random . s h u f f l e ( x [ : ,

i ] )

i ] )

i f

r e g r e s s i o n :

pred = model . p r e d i c t ( x )
e r r o r = m e t r i c s . mean_squared_error ( y , pred )

e l s e :

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

287

pred = model . p r e d i c t ( x )
e r r o r = m e t r i c s . l o g _ l o s s ( y , pred )

e r r o r s . append ( e r r o r )
i ] = h o l d
x [ : ,

max_error = np .max( e r r o r s )
i m po r t an c e = [ e / max_error for e in e r r o r s ]

data = { ' name ' : names , ' e r r o r ' : e r r o r s , ' i m po r t an c e ' : i mp o r ta n c e }
r e s u l t = pd . DataFrame ( data , columns = [ ' name ' , ' e r r o r ' , ' i m po r ta n c e ' ] )
r e s u l t . s o r t _ v a l u e s ( by=[ ' i m po r ta n c e ' ] , a s c e n d i n g = [ 0 ] ,
r e s u l t . r e s e t _ i n d e x ( i n p l a c e=True , drop=True )
return r e s u l t

i n p l a c e=True )

8.2.2 Classiﬁcation and Input Perturbation Ranking

We now look at the code to perform perturbation ranking for a classiﬁcation neural network. The imple-
mentation technique is slightly diﬀerent for classiﬁcation vs. regression, so I must provide two diﬀerent
implementations. The primary diﬀerence between classiﬁcation and regression is how we evaluate the accu-
racy of the neural network in each of these two network types. We will use the Root Mean Square (RMSE)
error calculation, whereas we will use log loss for classiﬁcation.

The code presented below creates a classiﬁcation neural network that will predict the classic iris dataset.

Code

import pandas a s pd
import i o
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

# Convert
x = d f [ [ ' s e p a l _ l ' ,

t o numpy − C l a s s i f i c a t i o n

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

288

CHAPTER 8. KAGGLE DATA SETS

dummies = pd . get_dummies ( d f [ ' s p e c i e s ' ] ) # C l a s s i f i c a t i o n
s p e c i e s = dummies . columns
y = dummies . v a l u e s

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
model . f i t ( x_train , y_train , v e r b o s e =2, ep o c hs =100)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

Next, we evaluate the accuracy of the trained model. Here we see that the neural network performs
great, with an accuracy of 1.0. We might fear overﬁtting with such high accuracy for a more complex
dataset. However, for this example, we are more interested in determining the importance of each column.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e

pred = model . p r e d i c t ( x _t e s t )
p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = np . argmax ( y_test , a x i s =1)
c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " Accuracy : ␣{ c o r r e c t } " )

Output

Accuracy : 1 . 0

We are now ready to call the input perturbation algorithm. First, we extract the column names and
remove the target column. The target column is not important, as it is the objective, not one of the inputs.
In supervised learning, the target is of the utmost importance.

We can see the importance displayed in the following table. The most important column is always 1.0,
and lessor columns will continue in a downward trend. The least important column will have the lowest
rank.

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

289

Code

# Rank t h e f e a t u r e s
from IPython . d i s p l a y import d i s p l a y , HTML

names = l i s t ( d f . columns ) # x+y column names
names . remove ( " s p e c i e s " ) # remove t h e t a r g e t ( y )
rank = p e r t u r b a t i o n _ r a n k ( model , x_test , y_test , names , F a l s e )
d i s p l a y ( rank )

Output

error
name
petal_l
2.609378
petal_w 0.480387
sepal_l
0.223239
sepal_w 0.128518

importance
1.000000
0.184100
0.085553
0.049252

0
1
2
3

8.2.3 Regression and Input Perturbation Ranking

We now see how to use input perturbation ranking for a regression neural network. We will use the MPG
dataset as a demonstration. The code below loads the MPG dataset and creates a regression neural network
for this dataset. The code trains the neural network and calculates an RMSE evaluation.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

save_path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

290

CHAPTER 8. KAGGLE DATA SETS

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x_train , y_train , v e r b o s e =2, ep o c hs =100)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

# P r e d i c t
pred = model . p r e d i c t ( x )

Just as before, we extract the column names and discard the target. We can now create a ranking of

the importance of each of the input features. The feature with a ranking of 1.0 is the most important.

Code

# Rank t h e f e a t u r e s
from IPython . d i s p l a y import d i s p l a y , HTML

names = l i s t ( d f . columns ) # x+y column names
names . remove ( " name " )
names . remove ( "mpg" ) # remove t h e t a r g e t ( y )
rank = p e r t u r b a t i o n _ r a n k ( model , x_test , y_test , names , True )
d i s p l a y ( rank )

Output

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

291

name
displacement
acceleration
origin
year
cylinders
horsepower

0
1
2
3
4
5
6 weight

error
139.657598
139.261508
134.637690
134.177126
132.747246
121.501102
75.244610

importance
1.000000
0.997164
0.964056
0.960758
0.950519
0.869993
0.538779

8.2.4 Biological Response with Neural Network

The following sections will demonstrate how to use feature importance ranking and ensembling with a
more complex dataset. Ensembling is the process where you combine multiple models for greater accuracy.
Kaggle competition winners frequently make use of ensembling for high-ranking solutions.

We will use the biological response dataset, a Kaggle dataset, where there is an unusually high number
of columns. Because of the large number of columns, it is essential to use feature ranking to determine the
importance of these columns. We begin by loading the dataset and preprocessing. This Kaggle dataset is
a binary classiﬁcation problem. You must predict if certain conditions will cause a biological response.

• Predicting a Biological Response

Code

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s c i p y . s t a t s import z s c o r e
from s k l e a r n . m o d e l _ s e l e c t i o n import KFold
from IPython . d i s p l a y import HTML, d i s p l a y

URL = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ k a g g l e / "

d f _ t r a i n = pd . read_csv (

URL+" b i o _ t r a i n . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

d f _ t e s t = pd . read_csv (

URL+" b i o _ t e s t . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

a c t i v i t y _ c l a s s e s = d f _ t r a i n [ ' A c t i v i t y ' ]

292

CHAPTER 8. KAGGLE DATA SETS

A large number of columns is evident when we display the shape of the dataset.

Code

print ( d f _ t r a i n . shape )

Output

( 3 7 5 1 , 1 7 7 7 )

The following code constructs a classiﬁcation neural network and trains it for the biological response

dataset. Once trained, the accuracy is measured.

Code

t f

import o s
import pandas a s pd
import t e n s o r f l o w a s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import numpy a s np
import s k l e a r n

# Encode f e a t u r e v e c t o r
# Convert
t o numpy − C l a s s i f i c a t i o n
x_columns = d f _ t r a i n . columns . drop ( ' A c t i v i t y ' )
x = d f _ t r a i n [ x_columns ] . v a l u e s
y = d f _ t r a i n [ ' A c t i v i t y ' ] . v a l u e s # C l a s s i f i c a t i o n
x_submit = d f _ t e s t [ x_columns ] . v a l u e s . a s t y p e ( np . f l o a t 3 2 )

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

print ( " F i t t i n g / T r a i n i n g . . . " )
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 ) )
model . add ( Dense ( 1 , a c t i v a t i o n= ' s i g m o i d ' ) )
model . compile ( l o s s= ' b i n a r y _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

293

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,
p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s =1000)

print ( " F i t t i n g ␣ done . . . " )

# P r e d i c t
pred = model . p r e d i c t ( x _t e s t ) . f l a t t e n ( )

# C l i p so t h a t min i s n e v e r e x a c t l y 0 , max n e v e r 1
pred = np . c l i p ( pred , a_min=1e −6,a_max=(1−1e −6))
print ( " V a l i d a t i o n ␣ l o g l o s s : ␣ {} " . format (

s k l e a r n . m e t r i c s . l o g _ l o s s ( y_test , pred ) ) )

# E v a l u a t e s u c c e s s u s i n g a c c u r a c y
pred = pred >0.5 # I f g r e a t e r than 0 . 5 p r o b a b i l i t y ,
s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_test , pred )
print ( " V a l i d a t i o n ␣ a c c u r a c y ␣ s c o r e : ␣ {} " . format ( s c o r e ) )

t h e n t r u e

# B u i l d r e a l
pred_submit = model . p r e d i c t ( x_submit )

s u b m i t

f i l e

# C l i p so t h a t min i s n e v e r e x a c t l y 0 , max n e v e r 1 ( would be a NaN s c o r e )
pred = np . c l i p ( pred , a_min=1e −6,a_max=(1−1e −6))
submit_df = pd . DataFrame ( { ' M o l e c u l e I d ' : [ x+1 for x \

in range ( len ( pred_submit ) ) ] , ' P r e d i c t e d P r o b a b i l i t y ' : \

submit_df . to_csv ( " submit . c s v " ,

pred_submit . f l a t t e n ( ) } )
i n d e x=F a l s e )

Output

F i t t i n g / T r a i n i n g . . .
Epoch 7 : e a r l y s t o p p i n g
F i t t i n g done . . .
V a l i d a t i o n l o g l o s s : 0 . 5 5 6 4 7 0 8 7 8 1 7 5 2 7 9 2
V a l i d a t i o n a c c u r a c y s c o r e : 0 . 7 5 1 5 9 9 1 4 7 1 2 1 5 3 5 2

8.2.5 What Features/Columns are Important

The following uses perturbation ranking to evaluate the neural network.

294

Code

CHAPTER 8. KAGGLE DATA SETS

# Rank t h e f e a t u r e s
from IPython . d i s p l a y import d i s p l a y , HTML

names = l i s t ( d f _ t r a i n . columns ) # x+y column names
names . remove ( " A c t i v i t y " ) # remove t h e t a r g e t ( y )
rank = p e r t u r b a t i o n _ r a n k ( model , x_test , y_test , names , F a l s e )
d i s p l a y ( rank [ 0 : 1 0 ] )

Output

name

0 D27
1 D1049
2 D51
3 D998
4 D1059
5 D961
6 D1407
7 D1309
8 D1100
9 D1275

error
0.603974
0.565997
0.565883
0.563872
0.563745
0.563723
0.563532
0.562244
0.561902
0.561659

importance
1.000000
0.937122
0.936934
0.933604
0.933394
0.933357
0.933041
0.930908
0.930341
0.929940

8.2.6 Neural Network Ensemble

A neural network ensemble combines neural network predictions with other models. The program deter-
mines the exact blend of these models by logistic regression. The following code performs this blend for a
classiﬁcation. If you present the ﬁnal predictions from the ensemble to Kaggle, you will see that the result
is very accurate.

Code

import numpy a s np
import o s
import pandas a s pd
import math
from t e n s o r f l o w . k e r a s . wrappers . s c i k i t _ l e a r n import K e r a s C l a s s i f i e r
from s k l e a r n . n e i g h b o r s import K N e i g h b o r s C l a s s i f i e r
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from s k l e a r n . ensemble import R a n d o m F o r e s t C l a s s i f i e r
from s k l e a r n . ensemble import E x t r a T r e e s C l a s s i f i e r

8.2. PART 8.2: BUILDING ENSEMBLES WITH SCIKIT-LEARN AND KERAS

295

from s k l e a r n . ensemble import G r a d i e n t B o o s t i n g C l a s s i f i e r
from s k l e a r n . l i n e a r _ m o d e l

import L o g i s t i c R e g r e s s i o n

SHUFFLE = F a l s e
FOLDS = 10

def build_ann ( i n p u t _ s i z e , c l a s s e s , neu ro n s ) :

model = S e q u e n t i a l ( )
model . add ( Dense ( neurons ,
model . add ( Dense ( 1 ) )
model . add ( Dense ( c l a s s e s , a c t i v a t i o n= ' softmax ' ) )
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
return model

input_dim=i n p u t _ s i z e , a c t i v a t i o n= ' r e l u ' ) )

def m l o g l o s s ( y_test , p r e d s ) :

e p s i l o n = 1 e −15
sum = 0
for row in zip ( preds , y _t e s t ) :

x = row [ 0 ] [ row [ 1 ] ]
x = max( e p s i l o n , x )
x = min(1− e p s i l o n , x )
sum+=math . l o g ( x )

return ( (−1/ len ( p r e d s ) ) ∗sum)

def

s t r e t c h ( y ) :
return ( y − y . min ( ) ) / ( y .max( ) − y . min ( ) )

def blend_ensemble ( x , y , x_submit ) :
k f = S t r a t i f i e d K F o l d (FOLDS)
f o l d s = l i s t ( k f . s p l i t ( x , y ) )

models = [

K e r a s C l a s s i f i e r ( b u i l d _ f n=build_ann , n eur ons =20 ,

i n p u t _ s i z e=x . shape [ 1 ] , c l a s s e s =2) ,

K N e i g h b o r s C l a s s i f i e r ( n_neighbors =3) ,
R a n d o m F o r e s t C l a s s i f i e r ( n _ e s t i m a t o r s =100 , n_jobs=−1,

c r i t e r i o n= ' g i n i ' ) ,

R a n d o m F o r e s t C l a s s i f i e r ( n _ e s t i m a t o r s =100 , n_jobs=−1,

E x t r a T r e e s C l a s s i f i e r ( n _ e s t i m a t o r s =100 , n_jobs=−1,

c r i t e r i o n= ' e n t r o p y ' ) ,

E x t r a T r e e s C l a s s i f i e r ( n _ e s t i m a t o r s =100 , n_jobs=−1,

c r i t e r i o n= ' g i n i ' ) ,

296

CHAPTER 8. KAGGLE DATA SETS

G r a d i e n t B o o s t i n g C l a s s i f i e r ( l e a r n i n g _ r a t e =0.05 ,

subsample =0.5 , max_depth=6, n _ e s t i m a t o r s =50)]

c r i t e r i o n= ' e n t r o p y ' ) ,

d a t a s e t _ b l e n d _ t r a i n = np . z e r o s ( ( x . shape [ 0 ] ,
d a t a s e t _ b l e n d _ t e s t = np . z e r o s ( ( x_submit . shape [ 0 ] ,

len ( models ) ) )

len ( models ) ) )

for j , model

in enumerate ( models ) :
print ( " Model : ␣ {} ␣ : ␣ {} " . format ( j , model )
fold_sums = np . z e r o s ( ( x_submit . shape [ 0 ] ,
t o t a l _ l o s s = 0
( t r a i n ,
for i ,

in enumerate ( f o l d s ) :

t e s t )

)

len ( f o l d s ) ) )

x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]
model . f i t ( x_train , y _ t r a i n )
pred = np . a r r a y ( model . p r e d i c t _ p r o b a ( x _t e s t ) )
d a t a s e t _ b l e n d _ t r a i n [ t e s t ,
pred2 = np . a r r a y ( model . p r e d i c t _ p r o b a ( x_submit ) )
fold_sums [ : ,
i ] = pred2 [ : , 1 ]
l o s s = m l o g l o s s ( y_test , pred )
t o t a l _ l o s s+=l o s s
print ( " Fold ␣ #{}: ␣ l o s s ={} " . format ( i , l o s s ) )

j ] = pred [ : , 1 ]

print ( " { } : ␣Mean␣ l o s s ={} " . format ( model . __class__ . __name__,

t o t a l _ l o s s / len ( f o l d s ) ) )

d a t a s e t _ b l e n d _ t e s t [ : ,

j ] = fold_sums . mean ( 1 )

print ( )
print ( " B l e n d i n g ␣ models . " )
b l e n d = L o g i s t i c R e g r e s s i o n ( s o l v e r= ' l b f g s ' )
b l e n d . f i t ( d a t a s e t _ b l e n d _ t r a i n , y )
return b l e n d . p r e d i c t _ p r o b a ( d a t a s e t _ b l e n d _ t e s t )

i f __name__ == '__main__ ' :

np . random . s e e d ( 4 2 ) # s e e d t o s h u f f l e t h e t r a i n s e t

print ( " Loading ␣ data . . . " )
URL = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ k a g g l e / "

d f _ t r a i n = pd . read_csv (

URL+" b i o _ t r a i n . c s v " ,

8.3. PART 8.3: ARCHITECTING NETWORK: HYPERPARAMETERS

297

na_values =[ 'NA ' ,

' ? ' ] )

df_submit = pd . read_csv (

URL+" b i o _ t e s t . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

p r e d i c t o r s = l i s t ( d f _ t r a i n . columns . v a l u e s )
p r e d i c t o r s . remove ( ' A c t i v i t y ' )
x = d f _ t r a i n [ p r e d i c t o r s ] . v a l u e s
y = d f _ t r a i n [ ' A c t i v i t y ' ]
x_submit = df_submit . v a l u e s

i f SHUFFLE :

i d x = np . random . p er m u t a t i o n ( y . s i z e )
x = x [ i d x ]
y = y [ i d x ]

submit_data = blend_ensemble ( x , y , x_submit )
submit_data = s t r e t c h ( submit_data )

####################
# B u i l d s u b m i t
####################
i d s = [ id+1 for id in range ( submit_data . shape [ 0 ] ) ]
submit_df = pd . DataFrame ( { ' M o l e c u l e I d ' :

i d s ,

f i l e

' P r e d i c t e d P r o b a b i l i t y ' :
submit_data [ : , 1 ] } ,
columns =[ ' M o l e c u l e I d ' ,
' P r e d i c t e d P r o b a b i l i t y ' ] )

submit_df . to_csv ( " submit . c s v " ,

i n d e x=F a l s e )

8.3 Part 8.3: Architecting Network: Hyperparameters

You have probably noticed several hyperparameters introduced previously in this course that you need to
choose for your neural network. The number of layers, neuron counts per layer, layer types, and activation
functions are all choices you must make to optimize your neural network. Some of the categories of
hyperparameters for you to choose from coming from the following list:

• Number of Hidden Layers and Neuron Counts
• Activation Functions
• Advanced Activation Functions
• Regularization: L1, L2, Dropout

298

CHAPTER 8. KAGGLE DATA SETS

• Batch Normalization
• Training Parameters

The following sections will introduce each of these categories for Keras. While I will provide some general
guidelines for hyperparameter selection, no two tasks are the same. You will beneﬁt from experimentation
with these values to determine what works best for your neural network. In the next part, we will see how
machine learning can select some of these values independently.

8.3.1 Number of Hidden Layers and Neuron Counts

The structure of Keras layers is perhaps the hyperparameters that most become aware of ﬁrst. How many
layers should you have? How many neurons are on each layer? What activation function and layer type
should you use? These are all questions that come up when designing a neural network. There are many
diﬀerent types of layer in Keras, listed here:

• Activation - You can also add activation functions as layers. Using the activation layer is the same

as specifying the activation function as part of a Dense (or other) layer type.

• ActivityRegularization Used to add L1/L2 regularization outside of a layer. You can specify L1

and L2 as part of a Dense (or other) layer type.

• Dense - The original neural network layer type. In this layer type, every neuron connects to the
next layer. The input vector is one-dimensional, and placing speciﬁc inputs next does not aﬀect each
other.

• Dropout - Dropout consists of randomly setting a fraction rate of input units to 0 at each update

during training time, which helps prevent overﬁtting. Dropout only occurs during training.

• Flatten - Flattens the input to 1D and does not aﬀect the batch size.
• Input - A Keras tensor is a tensor object from the underlying back end (Theano, TensorFlow, or
CNTK), which we augment with speciﬁc attributes to build a Keras by knowing the inputs and
outputs of the model.

• Lambda - Wraps arbitrary expression as a Layer object.
• Masking - Masks a sequence using a mask value to skip timesteps.
• Permute - Permutes the input dimensions according to a given pattern. Useful for tasks such as

connecting RNNs and convolutional networks.

• RepeatVector - Repeats the input n times.
• Reshape - Similar to Numpy reshapes.
• SpatialDropout1D - This version performs the same function as Dropout; however, it drops entire

1D feature maps instead of individual elements.

• SpatialDropout2D - This version performs the same function as Dropout; however, it drops entire

2D feature maps instead of individual elements

• SpatialDropout3D - This version performs the same function as Dropout; however, it drops entire

3D feature maps instead of individual elements.

There is always trial and error for choosing a good number of neurons and hidden layers. Generally, the
number of neurons on each layer will be larger closer to the hidden layer and smaller towards the output
layer. This conﬁguration gives the neural network a somewhat triangular or trapezoid appearance.

8.3. PART 8.3: ARCHITECTING NETWORK: HYPERPARAMETERS

299

8.3.2 Activation Functions

Activation functions are a choice that you must make for each layer. Generally, you can follow this guideline:

• Hidden Layers - RELU
• Output Layer - Softmax for classiﬁcation, linear for regression.

Some of the common activation functions in Keras are listed here:

• softmax - Used for multi-class classiﬁcation. Ensures all output neurons behave as probabilities and

sum to 1.0.

• elu - Exponential linear unit. Exponential Linear Unit or its widely known name ELU is a function
that tends to converge cost to zero faster and produce more accurate results. Can produce negative
outputs.

• selu - Scaled Exponential Linear Unit (SELU), essentially elu multiplied by a scaling constant.
• softplus - Softplus activation function. log(exp(x) + 1) Introduced in 2001.
• softsign Softsign activation function. x/(abs(x) + 1) Similar to tanh, but not widely used.
• relu - Very popular neural network activation function. Used for hidden layers, cannot output

negative values. No trainable parameters.

• tanh Classic neural network activation function, though often replaced by relu family on modern

networks.

• sigmoid - Classic neural network activation. Often used on output layer of a binary classiﬁer.
• hard_sigmoid - Less computationally expensive variant of sigmoid.
• exponential - Exponential (base e) activation function.
• linear - Pass-through activation function. Usually used on the output layer of a regression neural

network.

For more information about Keras activation functions refer to the following:

• Keras Activation Functions
• Activation Function Cheat Sheets

8.3.3 Advanced Activation Functions

Hyperparameters are not changed when the neural network trains. You, the network designer, must deﬁne
the hyperparameters. The neural network learns regular parameters during neural network training. Neural
network weights are the most common type of regular parameter. The "advanced activation functions,"
as Keras call them, also contain parameters that the network will learn during training. These activation
functions may give you better performance than RELU.

• LeakyReLU - Leaky version of a Rectiﬁed Linear Unit. It allows a small gradient when the unit is

not active, controlled by alpha hyperparameter.

• PReLU - Parametric Rectiﬁed Linear Unit, learns the alpha hyperparameter.

8.3.4 Regularization: L1, L2, Dropout

• Keras Regularization
• Keras Dropout

300

CHAPTER 8. KAGGLE DATA SETS

8.3.5 Batch Normalization

• Keras Batch Normalization
• Ioﬀe, S., Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing

internal covariate shift. arXiv preprint arXiv:1502.03167.

Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains
the mean activation close to 0 and the activation standard deviation close to 1. Can allow learning rate to
be larger.

8.3.6 Training Parameters

• Keras Optimizers
• Batch Size - Usually small, such as 32 or so.
• Learning Rate - Usually small, 1e-3 or so.

8.4 Part 8.4: Bayesian Hyperparameter Optimization for Keras

Bayesian Hyperparameter Optimization is a method of ﬁnding hyperparameters more eﬃciently than a
grid search. Because each candidate set of hyperparameters requires a retraining of the neural network,
it is best to keep the number of candidate sets to a minimum. Bayesian Hyperparameter Optimization
achieves this by training a model to predict good candidate sets of hyperparameters.[32]

• bayesian-optimization
• hyperopt
• spearmint

Code

# I g n o r e u s e l e s s W0819 war nin gs g e n e r a t e d by TensorFlow 2 . 0 .
# H o p e f u l l y can remove t h i s
# See h t t p s : / / g i t h u b . com/ t e n s o r f l o w / t e n s o r f l o w / i s s u e s /31308
import l o g g i n g , o s
l o g g i n g . d i s a b l e ( l o g g i n g .WARNING)
o s . e n v i r o n [ "TF_CPP_MIN_LOG_LEVEL" ] = " 3 "

i g n o r e i n t h e f u t u r e .

import pandas a s pd
from s c i p y . s t a t s import z s c o r e

# Read t h e d a t a s e t
d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ jh−s i m p l e −d a t a s e t . c s v " ,
na_values =[ 'NA ' , ' ? ' ] )

8.4. PART 8.4: BAYESIAN HYPERPARAMETER OPTIMIZATION FOR KERAS

301

# Generate dummies f o r
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' j o b ' ] , p r e f i x=" j o b " ) ] , a x i s =1)
d f . drop ( ' j o b ' , a x i s =1,

i n p l a c e=True )

j o b

# Generate dummies f o r area
d f = pd . c o n c a t ( [ df , pd . get_dummies ( d f [ ' a r e a ' ] , p r e f i x=" a r e a " ) ] , a x i s =1)
d f . drop ( ' a r e a ' , a x i s =1,

i n p l a c e=True )

# M i s s i n g v a l u e s
f o r income
med = d f [ ' income ' ] . median ( )
d f [ ' income ' ] = d f [ ' income ' ] . f i l l n a (med)

# S t a n d a r d i z e r a n g e s
d f [ ' income ' ] = z s c o r e ( d f [ ' income ' ] )
d f [ ' a s p e c t ' ] = z s c o r e ( d f [ ' a s p e c t ' ] )
d f [ ' s a v e _ r a t e ' ] = z s c o r e ( d f [ ' s a v e _ r a t e ' ] )
d f [ ' age ' ] = z s c o r e ( d f [ ' age ' ] )
d f [ ' s u b s c r i p t i o n s ' ] = z s c o r e ( d f [ ' s u b s c r i p t i o n s ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' p r o d u c t ' ) . drop ( ' i d ' )
x = d f [ x_columns ] . v a l u e s
dummies = pd . get_dummies ( d f [ ' p r o d u c t ' ] ) # C l a s s i f i c a t i o n
p r o d u c t s = dummies . columns
y = dummies . v a l u e s

Now that we’ve preprocessed the data, we can begin the hyperparameter optimization. We start by creating
a function that generates the model based on just three parameters. Bayesian optimization works on a
vector of numbers, not on a problematic notion like how many layers and neurons are on each layer. To
represent this complex neuron structure as a vector, we use several numbers to describe this structure.

• dropout - The dropout percent for each layer.
• neuronPct - What percent of our ﬁxed 5,000 maximum number of neurons do we wish to use? This

parameter speciﬁes the total count of neurons in the entire network.

• neuronShrink - Neural networks usually start with more neurons on the ﬁrst hidden layer and then
decrease this count for additional layers. This percent speciﬁes how much to shrink subsequent layers
based on the previous layer. We stop adding more layers once we run out of neurons (the count
speciﬁed by neuronPct).

These three numbers deﬁne the structure of the neural network. The commends in the below code show
exactly how the program constructs the network.

302

Code

CHAPTER 8. KAGGLE DATA SETS

import pandas a s pd
import o s
import numpy a s np
import time
import t e n s o r f l o w . k e r a s . i n i t i a l i z e r s
import s t a t i s t i c s
import t e n s o r f l o w . k e r a s
from s k l e a r n import m e t r i c s
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d K F o l d
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n , Dropout ,
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from s k l e a r n . m o d e l _ s e l e c t i o n import S t r a t i f i e d S h u f f l e S p l i t
from s k l e a r n . m o d e l _ s e l e c t i o n import S h u f f l e S p l i t
from t e n s o r f l o w . k e r a s . l a y e r s import LeakyReLU , PReLU
from t e n s o r f l o w . k e r a s . o p t i m i z e r s import Adam

def generate_model ( dropout , neuronPct , n e u r o n S h r i n k ) :

# We s t a r t w i t h some p e r c e n t o f 5000 s t a r t i n g neurons on
# t h e
neuronCount = int ( neuronPct ∗ 5 0 0 0 )

f i r s t h i d d e n l a y e r .

InputLa ye r

# C o n s t r u c t n e u r a l network
model = S e q u e n t i a l ( )

t h e r e would have been a t

# So l o n g as
# f e w e r than 10
# l a y e r s , c r e a t e a new l a y e r .
l a y e r = 0
while neuronCount >25 and l a y e r <10:

l e a s t 25 neurons and

# The f i r s t
i f

l a y e r ==0:

(0 t h )

l a y e r nee ds an i n p u t

input_dim ( neuronCount )

model . add ( Dense ( neuronCount ,
input_dim=x . shape [ 1 ] ,
a c t i v a t i o n=PReLU ( ) ) )

e l s e :

model . add ( Dense ( neuronCount , a c t i v a t i o n=PReLU ( ) ) )

l a y e r += 1

# Add d r o p o u t a f t e r each h i d d e n l a y e r

8.4. PART 8.4: BAYESIAN HYPERPARAMETER OPTIMIZATION FOR KERAS

303

model . add ( Dropout ( dropout ) )

# S h r i n k neuron count
neuronCount = neuronCount ∗ n e u r o n S h r i n k

f o r each l a y e r

model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output
return model

We can test this code to see how it creates a neural network based on three such parameters.

Code

# Generate a model and s e e what
model = generate_model ( dropout =0.2 , neuronPct =0.1 , n e u r o n S h r i n k =0.25)
model . summary ( )

t h e r e s u l t i n g s t r u c t u r e l o o k s

l i k e .

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

( Dropout )

d e n s e ( Dense )
dropout
dense_1 ( Dense )
dropout_1 ( Dropout )
dense_2 ( Dense )
dropout_2 ( Dropout )
dense_3 ( Dense )

( None , 5 0 0 )
( None , 5 0 0 )
( None , 1 2 5 )
( None , 1 2 5 )
( None , 3 1 )
( None , 3 1 )
( None , 7 )

24500
0
62750
0
3937
0
224

=================================================================
T o t a l params : 9 1 , 4 1 1
T r a i n a b l e params : 9 1 , 4 1 1
Non−t r a i n a b l e params : 0
_________________________________________________________________

We will now create a function to evaluate the neural network using three such parameters. We use
bootstrapping because one training run might have "bad luck" with the assigned random weights. We use
this function to train and then evaluate the neural network.

Code

SPLITS = 2
EPOCHS = 500

304

PATIENCE = 10

CHAPTER 8. KAGGLE DATA SETS

def evaluate_network ( dropout , l e a r n i n g _ r a t e , neuronPct , n e u r o n S h r i n k ) :

# B o o t s t r a p

# f o r C l a s s i f i c a t i o n
boot = S t r a t i f i e d S h u f f l e S p l i t ( n _ s p l i t s=SPLITS ,
# f o r R e g r e s s i o n
# b o o t = S h u f f l e S p l i t ( n _ s p l i t s=SPLITS ,

t e s t _ s i z e =0.1)

t e s t _ s i z e =0.1)

# Track p r o g r e s s
mean_benchmark = [ ]
epochs_needed = [ ]
num = 0

# Loop t h r o u g h s a m p l e s
for t r a i n ,

t e s t

in boot . s p l i t ( x , d f [ ' p r o d u c t ' ] ) :

s t a r t _ t i m e = time . time ( )
num+=1

t r a i n and t e s t

# S p l i t
x _ t r a i n = x [ t r a i n ]
y _ t r a i n = y [ t r a i n ]
x _ te s t = x [ t e s t ]
y _ te s t = y [ t e s t ]

model = generate_model ( dropout , neuronPct , n e u r o n S h r i n k )
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' ,

o p t i m i z e r=Adam( l e a r n i n g _ r a t e=l e a r n i n g _ r a t e ) )

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,
p a t i e n c e=PATIENCE, v e r b o s e =0, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

# Train on t h e b o o t s t r a p sample
model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s=EPOCHS)

e p oc h s = monitor . stopped_epoch
epochs_needed . append ( e p o c h s )

# P r e d i c t on t h e o u t o f b o o t ( v a l i d a t i o n )
pred = model . p r e d i c t ( x _t e s t )

# Measure t h i s b o o t s t r a p ' s

l o g l o s s

8.4. PART 8.4: BAYESIAN HYPERPARAMETER OPTIMIZATION FOR KERAS

305

y_compare = np . argmax ( y_test , a x i s =1) # For l o g l o s s
s c o r e = m e t r i c s . l o g _ l o s s ( y_compare , pred )
mean_benchmark . append ( s c o r e )
m1 = s t a t i s t i c s . mean ( mean_benchmark )
m2 = s t a t i s t i c s . mean ( epochs_needed )
mdev = s t a t i s t i c s . p s t d e v ( mean_benchmark )

c a l c u l a t i o n

# Record t h i s
i t e r a t i o n
time_took = time . time ( ) − s t a r t _ t i m e

t e n s o r f l o w . k e r a s . backend . c l e a r _ s e s s i o n ( )
return (−m1)

You can try any combination of our three hyperparameters, plus the learning rate, to see how eﬀective
these four numbers are. Of course, our goal is not to manually choose diﬀerent combinations of these four
hyperparameters; we seek to automate.

Code

print ( evaluate_network (

dropout =0.2 ,
l e a r n i n g _ r a t e =1e −3,
neuronPct =0.2 ,
n e u r o n S h r i n k = 0 . 2 ) )

Output

−0.6668764846259546

First, we must install the Bayesian optimization package if we are in Colab.

Code

! p i p i n s t a l l b a y e s i a n −o p t i m i z a t i o n

We will now automate this process. We deﬁne the bounds for each of these four hyperparameters and
begin the Bayesian optimization. Once the program ﬁnishes, the best combination of hyperparameters
found is displayed. The optimize function accepts two parameters that will signiﬁcantly impact how long
the process takes to complete:

• n_iter - How many steps of Bayesian optimization that you want to perform. The more steps, the

more likely you will ﬁnd a reasonable maximum.

• init_points: How many steps of random exploration that you want to perform. Random exploration

can help by diversifying the exploration space.

306

Code

CHAPTER 8. KAGGLE DATA SETS

from bayes_opt import B a y e s i a n O p t i m i z a t i o n
import time

# S u p r e s s NaN w a rni ngs
import wa rnin gs
wa rn i ngs . f i l t e r w a r n i n g s ( " i g n o r e " , c a t e g o r y =RuntimeWarning )

# Bounded r e g i o n o f parameter s p a c e
( 0 . 0 , 0 . 4 9 9 ) ,
pbounds = { ' dropout ' :

' l e a r n i n g _ r a t e ' :
' neuronPct ' :
' n e u r o n S h r i n k ' :

( 0 . 0 1 , 1 ) ,

( 0 . 0 1 , 1 )

( 0 . 0 , 0 . 1 ) ,

}

o p t i m i z e r = B a y e s i a n O p t i m i z a t i o n (

f=evaluate_network ,
pbounds=pbounds ,
v e r b o s e =2, # v e r b o s e = 1 p r i n t s o n l y when a maximum
# i s o b s e r v e d , v e r b o s e = 0 i s
random_state =1,

s i l e n t

)

s t a r t _ t i m e = time . time ( )
o p t i m i z e r . maximize ( i n i t _ p o i n t s =10 , n _ i t e r =20 ,)
time_took = time . time ( ) − s t a r t _ t i m e

print ( f " T o t a l ␣ runtime : ␣ { hms_string ( time_took ) } " )
print ( o p t i m i z e r .max)

Output

|

|

|

|

i t e r

t a r g e t

dropout

l e a r n i . . .

| neuronPct

|
neuron . . .
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
−−−
|
1
|
|
|
|
|

0 . 0 0 9 2 3 4 |

| −0.7167

| −0.8092

| −17.87

0 . 0 5 3 8 8

0 . 0 7 3 2 3

0 . 0 1 0 1 1

0 . 0 7 2 0 3

0 . 6 8 8 4

0 . 1 9 4 4

0 . 2 0 8 1

0 . 3 5 2 1

0 . 3 0 9 3

0 . 4 2 5

0 . 1 9 8

3

2

|

|

|

|

|

|

|

|

|

|

|

|

8.5. PART 8.5: CURRENT SEMESTER’S KAGGLE

307

| −0.8022

| −0.9209

| −17.96

|

|

|

0 . 1 0 2

0 . 2 0 8 2

0 . 3 9 9 6

|

|

|

0 . 0 8 7 8 1

0 . 0 5 5 8 7

0 . 0 9 6 8 3

|

|

|

0 . 0 3 7 1 1

0 . 1 4 9

0 . 3 2 0 3

|

|

|

0 . 6 7 3 8

0 . 2 0 6 1

0 . 6 9 5 4

|
|
|
|
|

4

5

6

. . .

T o t a l
runtime : 1 : 3 6 : 1 1 . 5 6
{ ' t a r g e t ' : −0.6955536706512794 ,
0 . 2 5 0 4 5 6 1 7 7 3 4 1 2 2 0 3 ,
' neuronPct ' : 0 . 0 1 2 6 4 8 7 9 1 5 2 1 8 1 1 8 2 6 ,
0 . 5 2 2 9 7 4 8 8 3 1 5 5 2 0 3 2 } }

' l e a r n i n g _ r a t e ' : 0 . 0 0 7 6 2 3 2 3 4 6 7 0 9 1 4 2 9 2 4 ,

' params ' : { ' dropout ' :

' neuronShrink ' :

As you can see, the algorithm performed 30 total iterations. This total iteration count includes ten random
and 20 optimization iterations.

8.5 Part 8.5: Current Semester’s Kaggle

Kaggke competition site for current semester:

• Fall 2022 coming soon.

Previous Kaggle competition sites for this class (NOT this semester’s assignment, feel free to use code):

• Spring 2022 Kaggle Assignment
• Fall 2021 Kaggle Assignment
• Spring 2021 Kaggle Assignment
• Fall 2020 Kaggle Assignment
• Spring 2020 Kaggle Assignment
• Fall 2019 Kaggle Assignment
• Spring 2019 Kaggle Assignment
• Fall 2018 Kaggle Assignment
• Spring 2018 Kaggle Assignment
• Fall 2017 Kaggle Assignment
• Spring 2017 Kaggle Assignment
• Fall 2016 Kaggle Assignment

8.5.1

Iris as a Kaggle Competition

If I used the Iris data as a Kaggle, I would give you the following three ﬁles:

• kaggle_iris_test.csv - The data that Kaggle will evaluate you on. It contains only input; you must

provide answers. (contains x)

308

CHAPTER 8. KAGGLE DATA SETS

• kaggle_iris_train.csv - The data that you will use to train. (contains x and y)
• kaggle_iris_sample.csv - A sample submission for Kaggle. (contains x and y)

Important features of the Kaggle iris ﬁles (that diﬀer from how we’ve previously seen ﬁles):

• The iris species is already index encoded.
• Your training data is in a separate ﬁle.
• You will load the test data to generate a submission ﬁle.

The following program generates a submission ﬁle for "Iris Kaggle". You can use it as a starting point for
assignment 3.

Code

import o s
import pandas a s pd
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
import t e n s o r f l o w a s
import numpy a s np
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

t f

d f _ t r a i n = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ d a t a s e t s / "+\
" k a g g l e _ i r i s _ t r a i n . c s v " , na_values =[ 'NA ' , ' ? ' ] )

# Encode f e a t u r e v e c t o r
d f _ t r a i n . drop ( ' i d ' , a x i s =1,

i n p l a c e=True )

num_classes = len ( d f _ t r a i n . groupby ( ' s p e c i e s ' ) . s p e c i e s . nunique ( ) )

print ( " Number␣ o f ␣ c l a s s e s : ␣ {} " . format ( num_classes ) )

t o numpy − C l a s s i f i c a t i o n
# Convert
x = d f _ t r a i n [ [ ' s e p a l _ l ' ,
' sepal_w ' ,
dummies = pd . get_dummies ( d f _ t r a i n [ ' s p e c i e s ' ] ) # C l a s s i f i c a t i o n
s p e c i e s = dummies . columns
y = dummies . v a l u e s

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =45)

x , y ,

# Train , w i t h e a r l y s t o p p i n g

8.5. PART 8.5: CURRENT SEMESTER’S KAGGLE

309

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 ) )
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) )
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,
p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =0, e p oc h s =1000)

Output

c l a s s e s : 3

Number o f
R e s t o r i n g model w e i g h t s from t h e end o f
Epoch 1 0 8 : e a r l y s t o p p i n g

t h e b e s t epoch : 1 0 3 .

Now that we’ve trained the neural network, we can check its log loss.

Code

from s k l e a r n import m e t r i c s

l o g l o s s e r r o r

# C a l c u l a t e m u l t i
pred = model . p r e d i c t ( x _t e s t )
s c o r e = m e t r i c s . l o g _ l o s s ( y_test , pred )
print ( " Log␣ l o s s ␣ s c o r e : ␣ {} " . format ( s c o r e ) )

Output

Log l o s s

s c o r e : 0 . 1 0 9 8 8 0 1 0 5 0 8 9 3 9 6 2 3

Now we are ready to generate the Kaggle submission ﬁle. We will use the iris test data that does not

contain a y target value. It is our job to predict this value and submit it to Kaggle.

Code

# Generate Kaggle s u b m i t

f i l e

# Encode f e a t u r e v e c t o r
d f _ t e s t = pd . read_csv (

310

CHAPTER 8. KAGGLE DATA SETS

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ d a t a s e t s / "+\
" k a g g l e _ i r i s _ t e s t . c s v " , na_values =[ 'NA ' , ' ? ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
i d s = d f _ t e s t [ ' i d ' ]
d f _ t e s t . drop ( ' i d ' , a x i s =1,
x = d f _ t e s t [ [ ' s e p a l _ l ' ,
y = dummies . v a l u e s

i n p l a c e=True )

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

# Generate p r e d i c t i o n s
pred = model . p r e d i c t ( x )
#pred

# Create s u b m i s s i o n d a t a s e t

df_submit = pd . DataFrame ( pred )
df_submit . i n s e r t ( 0 , ' i d ' , i d s )
df_submit . columns = [ ' i d ' , ' s p e c i e s −0 ' , ' s p e c i e s −1 ' , ' s p e c i e s −2 ' ]

f i l e
# Write s u b m i t
df_submit . to_csv ( " i r i s _ s u b m i t . c s v " ,

l o c a l l y

i n d e x=F a l s e )

print ( df_submit [ : 5 ] )

Output

i d
0
1
2
3
4

s p e c i e s −0

s p e c i e s −1

s p e c i e s −2

100
101
102
103
104

0 . 0 2 2 3 0 0
0 . 0 0 1 3 0 9
0 . 0 0 1 1 5 3
0 . 9 5 8 0 0 6
0 . 9 7 6 9 3 2

0 . 7 7 7 8 5 9
0 . 2 7 3 8 4 9
0 . 3 1 9 3 4 9
0 . 0 4 1 9 8 9
0 . 0 2 3 0 6 6

0 . 1 9 9 8 4 1
0 . 7 2 4 8 4 2
0 . 6 7 9 4 9 8
0 . 0 0 0 0 0 5
0 . 0 0 0 0 0 2

8.5.2 MPG as a Kaggle Competition (Regression)

If the Auto MPG data were used as a Kaggle, you would be given the following three ﬁles:

• kaggle_mpg_test.csv - The data that Kaggle will evaluate you on. Contains only input, you must

provide answers. (contains x)

• kaggle_mpg_train.csv - The data that you will use to train. (contains x and y)
• kaggle_mpg_sample.csv - A sample submission for Kaggle. (contains x and y)

8.5. PART 8.5: CURRENT SEMESTER’S KAGGLE

311

Important features of the Kaggle iris ﬁles (that diﬀer from how we’ve previously seen ﬁles):

The following program generates a submission ﬁle for "MPG Kaggle".

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

save_path = " . "

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ d a t a s e t s / "+\
" k a g g l e _ a u t o _ t r a i n . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

312

CHAPTER 8. KAGGLE DATA SETS

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

v e r b o s e =2, c a l l b a c k s =[ monitor ] , e p o c h s =1000)

v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

# P r e d i c t
pred = model . p r e d i c t ( x _t e s t )

Now that we’ve trained the neural network, we can check its RMSE error.

Code

import numpy a s np

# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " F i n a l ␣ s c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

r e g r e s s i o n .

Output

F i n a l

s c o r e (RMSE) : 6 . 0 2 3 7 7 6 4 0 5 9 4 7 5 0 1

Now we are ready to generate the Kaggle submission ﬁle. We will use the MPG test data that does not

contain a y target value. It is our job to predict this value and submit it to Kaggle.

Code

import pandas a s pd

# Generate Kaggle s u b m i t

f i l e

# Encode f e a t u r e v e c t o r
d f _ t e s t = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ d a t a s e t s / "+\
" k a g g l e _ a u t o _ t e s t . c s v " , na_values =[ 'NA ' , ' ? ' ] )

t o numpy − r e g r e s s i o n

# Convert
i d s = d f _ t e s t [ ' i d ' ]
d f _ t e s t . drop ( ' i d ' , a x i s =1,

i n p l a c e=True )

# Handle m i s s i n g v a l u e
d f _ t e s t [ ' horsepower ' ] = d f _ t e s t [ ' horsepower ' ] . \

f i l l n a ( d f [ ' horsepower ' ] . median ( ) )

8.5. PART 8.5: CURRENT SEMESTER’S KAGGLE

313

x = d f _ t e s t [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

# Generate p r e d i c t i o n s
pred = model . p r e d i c t ( x )
#p red

# Create s u b m i s s i o n d a t a s e t

df_submit = pd . DataFrame ( pred )
df_submit . i n s e r t ( 0 , ' i d ' , i d s )
df_submit . columns = [ ' i d ' , 'mpg ' ]

# Write s u b m i t
f i l e
df_submit . to_csv ( " auto_submit . c s v " ,

l o c a l l y

i n d e x=F a l s e )

print ( df_submit [ : 5 ] )

Output

i d
0
1
2
3
4

350
351
352
353
354

mpg

2 7 . 1 5 8 8 1 9
2 4 . 4 5 0 6 2 1
2 4 . 9 1 3 3 5 5
2 6 . 9 9 4 8 6 7
2 6 . 6 6 9 2 6 8

314

CHAPTER 8. KAGGLE DATA SETS

Chapter 9

Transfer Learning

9.1 Part 9.1: Introduction to Keras Transfer Learning

Human beings learn new skills throughout their entire lives. However, this learning is rarely from scratch.
No matter what task a human learns, they are most likely drawing on experiences to learn this new skill
early in life. In this way, humans learn much diﬀerently than most deep learning projects.

A human being learns to tell the diﬀerence between a cat and a dog at some point. To teach a neural
network, you would obtain many cat pictures and dog pictures. The neural network would iterate over all
of these pictures and train on the diﬀerences. The human child that learned to distinguish between the
two animals would probably need to see a few examples when parents told them the name of each type of
animal. The human child would use previous knowledge of looking at diﬀerent living and non-living objects
to help make this classiﬁcation. The child would already know the physical appearance of sub-objects, such
as fur, eyes, ears, noses, tails, and teeth.

Transfer learning attempts to teach a neural network by similar means. Rather than training your
neural network from scratch, you begin training with a preloaded set of weights. Usually, you will remove
the topmost layers of the pretrained neural network and retrain it with new uppermost layers. The layers
from the previous neural network will be locked so that training does not change these weights. Only the
newly added layers will be trained.

It can take much computing power to train a neural network for a large image dataset. Google,
Facebook, Microsoft, and other tech companies have utilized GPU arrays for training high-quality neural
networks for various applications. Transferring these weights into your neural network can save considerable
eﬀort and compute time. It is unlikely that a pretrained model will exactly ﬁt the application that you
seek to implement. Finding the closest pretrained model and using transfer learning is essential for a deep
learning engineer.

9.1.1 Transfer Learning Example

Let’s look at a simple example of using transfer learning to build upon an imagenet neural network. We
will begin by training a neural network for Fisher’s Iris Dataset. This network takes four measurements
and classiﬁes each observation into three iris species. However, what if later we received a data set that

315

316

CHAPTER 9. TRANSFER LEARNING

included the four measurements, plus a cost as the target? This dataset does not contain the species; as a
result, it uses the same four inputs as the base model we just trained.

We can take our previously trained iris network and transfer the weights to a new neural network that
will learn to predict the cost through transfer learning. Also of note, the original neural network was a
classiﬁcation network, yet we now use it to build a regression neural network. Such a transformation is
common for transfer learning. As a reference point, I randomly created this iris cost dataset.

The ﬁrst step is to train our neural network for the regular Iris Dataset. The code presented here is

the same as we saw in Module 3.

Code

import pandas a s pd
import i o
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

t o numpy − C l a s s i f i c a t i o n

# Convert
x = d f [ [ ' s e p a l _ l ' ,
dummies = pd . get_dummies ( d f [ ' s p e c i e s ' ] ) # C l a s s i f i c a t i o n
s p e c i e s = dummies . columns
y = dummies . v a l u e s

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

# B u i l d n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) ) # Output

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =2, ep o c hs =100)

Output

. . .

9.1. PART 9.1: INTRODUCTION TO KERAS TRANSFER LEARNING

317

5/5 − 0 s − l o s s : 0 . 0 8 6 8 − 15ms/ epoch − 3ms/ s t e p
Epoch 100/100
5/5 − 0 s − l o s s : 0 . 0 8 9 2 − 8ms/ epoch − 2ms/ s t e p

To keep this example simple, we are not setting aside a validation set. The goal of this example is to
show how to create a multi-layer neural network, where we transfer the weights to another network. We
begin by evaluating the accuracy of the network on the training set.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e
pred = model . p r e d i c t ( x )
p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = np . argmax ( y , a x i s =1)
c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " T r a i n i n g ␣ Accuracy : ␣{ c o r r e c t } " )

Output

T r a i n i n g Accuracy : 0 . 9 8 6 6 6 6 6 6 6 6 6 6 6 6 6 7

Viewing the model summary is as expected; we can see the three layers previously deﬁned.

Code

model . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

d e n s e ( Dense )
dense_1 ( Dense )
dense_2 ( Dense )

( None , 5 0 )
( None , 2 5 )
( None , 3 )

250
1275
78

=================================================================
T o t a l params : 1 , 6 0 3
T r a i n a b l e params : 1 , 6 0 3
Non−t r a i n a b l e params : 0
_________________________________________________________________

318

CHAPTER 9. TRANSFER LEARNING

9.1.2 Create a New Iris Network

Now that we’ve trained a neural network on the iris dataset, we can transfer the knowledge of this neural
network to other neural networks. It is possible to create a new neural network from some or all of the
layers of this neural network. We will create a new neural network that is essentially a clone of the ﬁrst
neural network to demonstrate the technique. We now transfer all of the layers from the original neural
network into the new one.

Code

model2 = S e q u e n t i a l ( )
for l a y e r

in model . l a y e r s :

model2 . add ( l a y e r )

model2 . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l _ 1 "

Layer

( type )

Output Shape

Param #

=================================================================

d e n s e ( Dense )
dense_1 ( Dense )
dense_2 ( Dense )

( None , 5 0 )
( None , 2 5 )
( None , 3 )

250
1275
78

=================================================================
T o t a l params : 1 , 6 0 3
T r a i n a b l e params : 1 , 6 0 3
Non−t r a i n a b l e params : 0
_________________________________________________________________

As a sanity check, we would like to calculate the accuracy of the newly created model. The in-sample

accuracy should be the same as the previous model that the new model transferred.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e
pred = model2 . p r e d i c t ( x )
p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
e x p e c t e d _ c l a s s e s = np . argmax ( y , a x i s =1)
c o r r e c t = a c c u r a c y _ s c o r e ( e x p e c t e d _ c l a s s e s , p r e d i c t _ c l a s s e s )
print ( f " T r a i n i n g ␣ Accuracy : ␣{ c o r r e c t } " )

Output

9.1. PART 9.1: INTRODUCTION TO KERAS TRANSFER LEARNING

319

T r a i n i n g Accuracy : 0 . 9 8 6 6 6 6 6 6 6 6 6 6 6 6 6 7

The in-sample accuracy of the newly created neural network is the same as the ﬁrst neural network.

We’ve successfully transferred all of the layers from the original neural network.

9.1.3 Transfering to a Regression Network

The Iris Cost Dataset has measurements for samples of these ﬂowers that conform to the predictors con-
tained in the original iris dataset: sepal width, sepal length, petal width, and petal length. We present the
cost dataset here.

Code

d f _ c o s t = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ i r i s _ c o s t . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

d f _ c o s t

Output

sepal_l
7.8
5.0
6.9
5.9
5.1
...
4.7
7.2
6.6
5.7
7.6

sepal_w petal_l
3.0
2.2
2.6
2.2
3.9
...
2.1
3.0
3.4
3.7
4.0

6.2
1.7
3.7
3.7
6.8
...
4.0
4.3
4.6
3.1
5.1

petal_w cost
2.0
1.5
1.4
2.4
0.7
...
2.3
1.1
1.4
0.4
1.4

10.740
2.710
4.624
6.558
7.395
...
5.721
5.266
5.776
2.233
7.508

0
1
2
3
4
...
245
246
247
248
249

For transfer learning to be eﬀective, the input for the newly trained neural network most closely conforms

to the ﬁrst neural network we transfer.

We will strip away the last output layer that contains the softmax activation function that performs
this ﬁnal classiﬁcation. We will create a new output layer that will output the cost prediction. We will
only train the weights in this new layer. We will mark the ﬁrst two layers as non-trainable. The hope is
that the ﬁrst few layers have learned to abstract the raw input data in a way that is also helpful to the
new neural network.
This process is accomplished by looping over the ﬁrst few layers and copying them to the new neural
network. We output a summary of the new neural network to verify that Keras stripped the previous

CHAPTER 9. TRANSFER LEARNING

320

output layer.

Code

model3 = S e q u e n t i a l ( )
for i

in range ( 2 ) :

l a y e r = model . l a y e r s [ i ]
l a y e r . t r a i n a b l e = F a l s e
model3 . add ( l a y e r )

model3 . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l _ 2 "

Layer

( type )

Output Shape

Param #

=================================================================

d e n s e ( Dense )
dense_1 ( Dense )

( None , 5 0 )
( None , 2 5 )

250
1275

=================================================================
T o t a l params : 1 , 5 2 5
T r a i n a b l e params : 0
Non−t r a i n a b l e params : 1 , 5 2 5
_________________________________________________________________

We add a ﬁnal regression output layer to complete the new neural network.

Code

model3 . add ( Dense ( 1 ) ) # Output

model3 . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model3 . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l _ 2 "

Layer

( type )

Output Shape

Param #

=================================================================

d e n s e ( Dense )
dense_1 ( Dense )
dense_3 ( Dense )

( None , 5 0 )
( None , 2 5 )
( None , 1 )

250
1275
26

9.1. PART 9.1: INTRODUCTION TO KERAS TRANSFER LEARNING

321

=================================================================
T o t a l params : 1 , 5 5 1
T r a i n a b l e params : 26
Non−t r a i n a b l e params : 1 , 5 2 5
_________________________________________________________________

Now we train just the output layer to predict the cost. The cost in the made-up dataset is dependent

on the species, so the previous learning should be helpful.

Code

t o numpy − C l a s s i f i c a t i o n

# Convert
x = d f _ c o s t [ [ ' s e p a l _ l ' ,
y = d f _ c o s t . c o s t . v a l u e s

' sepal_w ' ,

' p e t a l _ l ' ,

' petal_w ' ] ] . v a l u e s

# Train t h e l a s t
model3 . f i t ( x , y , v e r b o s e =2, ep o c hs =100)

t h e network

l a y e r o f

Output

. . .
8/8 − 0 s − l o s s : 1 . 8 8 5 1 − 17ms/ epoch − 2ms/ s t e p
Epoch 100/100
8/8 − 0 s − l o s s : 1 . 8 8 3 8 − 9ms/ epoch − 1ms/ s t e p

We can evaluate the in-sample RMSE for the new model containing transferred layers from the previous

model.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e
pred = model3 . p r e d i c t ( x )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y ) )
print ( f " F i n a l ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

Output

F i n a l

s c o r e (RMSE) : 1 . 3 7 1 6 5 8 9 6 2 5 8 2 3 0 7 2

322

CHAPTER 9. TRANSFER LEARNING

9.1.4 Module 9 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 9

9.2 Part 9.2: Keras Transfer Learning for Computer Vision

We will take a look at several popular pretrained neural networks for Keras. The following two sites, among
others, can be great starting points to ﬁnd pretrained models for use in your projects:

• TensorFlow Model Zoo
• Papers with Code

Keras contains built-in support for several pretrained models. In the Keras documentation, you can ﬁnd
the complete list.

9.2.1 Transfering Computer Vision

There are many pretrained models for computer vision. This section will show you how to obtain a
pretrained model for computer vision and train just the output layer. Additionally, once we train the
output layer, we will ﬁne-tune the entire network by training all weights using by applying a low learning
rate.

9.2.2 The Kaggle Cats vs. Dogs Dataset

We will train a neural network to recognize cats and dogs for this example. The [cats and dogs dataset]
comes from a classic Kaggle competition. We can achieve a very high score on this data set through modern
training techniques and ensemble learning. I based this module on a tutorial provided by Francois Chollet,
one of the creators of Keras. I made some changes to his example to ﬁt with this course.

We begin by downloading this dataset from Keras. We do not need the entire dataset to achieve high
accuracy. Using a portion also speeds up training. We will use 40% of the original training data (25,000
images) for training and 10% for validation.

The dogs and cats dataset is relatively large and will not easily ﬁt into a less than 12GB system, such
as Colab. Because of this memory size, you must take additional steps to handle the data. Rather than
loading the dataset as a Numpy array, as done previously in this book, we will load it as a prefetched
dataset so that only the portions of the dataset currently needed are in RAM. If you wish to load the
dataset, in its entirety as a Numpy array, add the batch_size=-1 option to the load command below.

Code

import t e n s o r f l o w _ d a t a s e t s a s
import t e n s o r f l o w a s

t f

t f d s

t f d s . d i s a b l e _ p r o g r e s s _ b a r ( )

train_ds , v a l i d a t i o n _ d s = t f d s . l o a d (

9.2. PART 9.2: KERAS TRANSFER LEARNING FOR COMPUTER VISION

323

" cats_vs_dogs " ,
s p l i t =[ " t r a i n [ : 4 0 % ] " ,
a s _ s u p e r v i s e d=True , # I n c l u d e l a b e l s

" t r a i n [40%:50%] " ] ,

)

num_train = t f . data . e x p e r i m e n t a l . c a r d i n a l i t y ( t r a i n _ d s )
num_test = t f . data . e x p e r i m e n t a l . c a r d i n a l i t y ( v a l i d a t i o n _ d s )

print ( f " Number␣ o f ␣ t r a i n i n g ␣ samp les : ␣{ num_train } " )
print ( f " Number␣ o f ␣ v a l i d a t i o n ␣ sa mpl es : ␣ { num_test } " )

Output

Number o f
Number o f v a l i d a t i o n sa mpl es : 2326

t r a i n i n g sa m pl es : 9305

9.2.3 Looking at the Data and Augmentations

We begin by displaying several of the images from this dataset. The labels are above each image. As can
be seen from the images below, 1 indicates a dog, and 0 indicates a cat.

Code

import m a t p l o t l i b . p y p l o t a s p l t

p l t . f i g u r e ( f i g s i z e =(10 , 1 0 ) )
fo r i ,

( image ,

l a b e l )

in enumerate ( t r a i n _ d s . t a k e ( 9 ) ) :

ax = p l t . s u b p l o t ( 3 , 3 ,
p l t . imshow ( image )
p l t . t i t l e ( int ( l a b e l ) )
p l t . a x i s ( " o f f " )

i + 1 )

Output

324

CHAPTER 9. TRANSFER LEARNING

Upon examining the above images, another problem becomes evident. The images are of various sizes.

We will standardize all images to 190x190 with the following code.

Code

s i z e = ( 1 5 0 , 1 5 0 )

t r a i n _ d s = t r a i n _ d s .map(lambda x , y :
v a l i d a t i o n _ d s = v a l i d a t i o n _ d s .map(lambda x , y : \

( t f . image . r e s i z e ( x ,

s i z e ) , y ) )

( t f . image . r e s i z e ( x ,

s i z e ) , y ) )

We will batch the data and use caching and prefetching to optimize loading speed.

Code

b a t c h _ s i z e = 32

t r a i n _ d s = t r a i n _ d s . c a c h e ( ) . batch ( b a t c h _ s i z e ) . p r e f e t c h ( b u f f e r _ s i z e =10)
v a l i d a t i o n _ d s = v a l i d a t i o n _ d s . c a c h e ( ) \

9.2. PART 9.2: KERAS TRANSFER LEARNING FOR COMPUTER VISION

325

. batch ( b a t c h _ s i z e ) . p r e f e t c h ( b u f f e r _ s i z e =10)

Augmentation is a powerful computer vision technique that increases the amount of training data
available to your model by altering the images in the training data. To use augmentation, we will allow
horizontal ﬂips of the images. A horizontal ﬂip makes much more sense for cats and dogs in the real world
than a vertical ﬂip. How often do you see upside-down dogs or cats? We also include a limited degree of
rotation.

Code

from t e n s o r f l o w import k e r a s
from t e n s o r f l o w . k e r a s import l a y e r s

data_augmentation = k e r a s . S e q u e n t i a l (
[ l a y e r s . RandomFlip ( " h o r i z o n t a l " ) ,

l a y e r s . RandomRotation ( 0 . 1 ) , ]

)

The following code allows us to visualize the augmentation.

Code

import numpy a s np

fo r images ,

l a b e l s

in t r a i n _ d s . t a k e ( 1 ) :

p l t . f i g u r e ( f i g s i z e =(10 , 1 0 ) )
f i r s t _ i m a g e = images [ 0 ]
in range ( 9 ) :
fo r i

ax = p l t . s u b p l o t ( 3 , 3 ,
augmented_image = data_augmentation (

i + 1 )

t f . expand_dims ( f i r s t _ i m a g e , 0 ) ,

t r a i n i n g=True

)
p l t . imshow ( augmented_image [ 0 ] . numpy ( ) . a s t y p e ( " i n t 3 2 " ) )
p l t . t i t l e ( int ( l a b e l s [ 0 ] ) )
p l t . a x i s ( " o f f " )

Output

326

CHAPTER 9. TRANSFER LEARNING

9.2.4 Create a Network and Transfer Weights

We are now ready to create our new neural network with transferred weights. We will transfer the weights
from an Xception neural network that contains weights trained for imagenet. We load the existing Xcep-
tion neural network with keras.applications. There is quite a bit going on with the loading of the
base_model, so we will examine this call piece by piece.

The base Xception neural network accepts an image of 299x299. However, we would like to use 150x150.
It turns out that it is relatively easy to overcome this diﬀerence. Convolutional neural networks move a
kernel across an image tensor as they scan. Keras deﬁnes the number of weights by the size of the layer’s
kernel, not the image that the kernel scans. As a result, we can discard the old input layer and recreate
an input layer consistent with our desired image size. We specify include_top as false and specify our
input shape.

We freeze the base model so that the model will not update existing weights as training occurs. We
create the new input layer that consists of 150x150 by 3 RGB color components. These RGB components
are integer numbers between 0 and 255. Neural networks deal better with ﬂoating-point numbers when
you distribute them around zero. To accomplish this neural network advantage, we normalize each RGB
component to between -1 and 1.

9.2. PART 9.2: KERAS TRANSFER LEARNING FOR COMPUTER VISION

327

The batch normalization layers do require special consideration. We need to keep these layers in
inference mode when we unfreeze the base model for ﬁne-tuning. To do this, we make sure that the base
model is running in inference mode here.

Code

base_model = k e r a s . a p p l i c a t i o n s . Xception (

w e i g h t s=" imagenet " , # Load w e i g h t s pre−t r a i n e d on ImageNet .
input_shape =(150 , 1 5 0 , 3 ) ,
i n c l u d e _ t o p=F a l s e ,

) # Do not

i n c l u d e t h e ImageNet c l a s s i f i e r a t

t h e t o p .

# F r e e z e t h e base_model
base_model . t r a i n a b l e = F a l s e

# Create new model on t o p
i n p u t s = k e r a s . Input ( shape =(150 , 1 5 0 , 3 ) )
x = data_augmentation ( i n p u t s ) # Apply random d a t a a ugme nta tion

# Pre−t r a i n e d X c e p t i o n w e i g h t s
# from ( 0 , 255) t o a range o f ( −1. , +1.) ,
# o u t p u t s : `( i n p u t s ∗ s c a l e ) + o f f s e t `
s c a l e _ l a y e r = k e r a s . l a y e r s . R e s c a l i n g ( s c a l e =1 / 1 2 7 . 5 , o f f s e t =−1)
x = s c a l e _ l a y e r ( x )

t h e r e s c a l i n g l a y e r

i n p u t be s c a l e d

r e q u i r e s

t h a t

t o k e e p them i n i n f e r e n c e mode

# The b a s e model c o n t a i n s batchnorm l a y e r s .
# We want
# when we u n f r e e z e t h e b a s e model
# so we make s u r e t h a t
# base_model
x = base_model ( x ,
x = k e r a s . l a y e r s . GlobalAveragePooling2D ( ) ( x )
x = k e r a s . l a y e r s . Dropout ( 0 . 2 ) ( x ) # R e g u l a r i z e w i t h d r o p o u t
o u t p u t s = k e r a s . l a y e r s . Dense ( 1 ) ( x )
model = k e r a s . Model ( i n p u t s , o u t p u t s )

running i n i n f e r e n c e mode h e r e .

t r a i n i n g=F a l s e )

f i n e −t u n i n g ,

t h e

f o r

i s

model . summary ( )

Output

Downloading data from h t t p s : / / s t o r a g e . g o o g l e a p i s . com/ t e n s o r f l o w / k e r a s −
a p p l i c a t i o n s / x c e p t i o n / x cep tio n_ wei g ht s_ tf_ dim _o rde ri ng_ tf_ ke rne ls_ no to
p . h5

328

CHAPTER 9. TRANSFER LEARNING

83689472/83683744 [==============================] − 1 s 0 us / s t e p
83697664/83683744 [==============================] − 1 s 0 us / s t e p
Model :
_________________________________________________________________

" model "

Layer

( type )

Output Shape

Param #

=================================================================

( S e q u e n t i a l )

input_2 ( I npu tLayer )
s e q u e n t i a l
r e s c a l i n g ( R e s c a l i n g )
x c e p t i o n ( F u n c t i o n a l )
g l o b a l _ a v e r a g e _ p o o l i n g 2 d (G ( None , 2 0 4 8 )
l o b a l A v e r a g e P o o l i n g 2 D )

[ ( None , 1 5 0 , 1 5 0 , 3 ) ]
( None , 1 5 0 , 1 5 0 , 3 )
( None , 1 5 0 , 1 5 0 , 3 )
( None , 5 , 5 , 2 0 4 8 )

0
0
0
20861480
0

. . .

=================================================================
T o t a l params : 2 0 , 8 6 3 , 5 2 9
T r a i n a b l e params : 2 , 0 4 9
Non−t r a i n a b l e params : 2 0 , 8 6 1 , 4 8 0
_________________________________________________________________

Next, we compile and ﬁt the model. The ﬁtting will use the Adam optimizer; because we are performing

binary classiﬁcation, we use the binary cross-entropy loss function, as we have done before.

Code

model . compile (

o p t i m i z e r=k e r a s . o p t i m i z e r s . Adam ( ) ,
l o s s=k e r a s . l o s s e s . B i n a r y C r o s s e n t r o p y ( f r o m _ l o g i t s=True ) ,
m e t r i c s =[ k e r a s . m e t r i c s . BinaryAccuracy ( ) ] ,

)

e p oc h s = 20
model . f i t ( train_ds , e p oc h s=epochs , v a l i d a t i o n _ d a t a=v a l i d a t i o n _ d s )

Output

. . .
291/291 [==============================] − 11 s 37ms/ s t e p − l o s s :
0 . 0 9 0 7 − b i n a r y _ a c c u r a c y : 0 . 9 6 2 7 − v a l _ l o s s : 0 . 0 7 1 8 −
val_binary_accuracy : 0 . 9 7 2 9
Epoch 20/20
291/291 [==============================] − 11 s 37ms/ s t e p − l o s s :

9.2. PART 9.2: KERAS TRANSFER LEARNING FOR COMPUTER VISION

329

0 . 0 8 9 9 − b i n a r y _ a c c u r a c y : 0 . 9 6 5 2 − v a l _ l o s s : 0 . 0 6 9 4 −
val_binary_accuracy : 0 . 9 7 4 6

The training above shows that the validation accuracy reaches the mid 90% range. This accuracy is

good; however, we can do better.

9.2.5 Fine-Tune the Model

Finally, we will ﬁne-tune the model. First, we set all weights to trainable and then train the neural
network with a low learning rate (1e-5). This ﬁne-tuning results in an accuracy in the upper 90% range.
The ﬁne-tuning allows all weights in the neural network to adjust slightly to optimize for the dogs/cats
data.

Code

# U n f r e e z e t h e base_model . Note t h a t
# s i n c e we p a s s e d ` t r a i n i n g=F a l s e ` when c a l l i n g
# t h e batchnorm l a y e r s w i l l not u p d a t e t h e i r b a t c h s t a t i s t i c s .
# This p r e v e n t s
# we ' ve done so f a r .
base_model . t r a i n a b l e = True
model . summary ( )

t h e batchnorm l a y e r s

from undoing a l l

i t k e e p s running i n i n f e r e n c e mode

i t . This means t h a t

t h e t r a i n i n g

model . compile (

o p t i m i z e r=k e r a s . o p t i m i z e r s . Adam( 1 e −5) , # Low l e a r n i n g r a t e
l o s s=k e r a s . l o s s e s . B i n a r y C r o s s e n t r o p y ( f r o m _ l o g i t s=True ) ,
m e t r i c s =[ k e r a s . m e t r i c s . BinaryAccuracy ( ) ] ,

)

e p oc h s = 10
model . f i t ( train_ds , e p oc h s=epochs , v a l i d a t i o n _ d a t a=v a l i d a t i o n _ d s )

Output

Model :
_________________________________________________________________

" model "

Layer

( type )

Output Shape

Param #

=================================================================

( S e q u e n t i a l )

input_2 ( InputLayer )
s e q u e n t i a l
r e s c a l i n g ( R e s c a l i n g )
x c e p t i o n ( F u n c t i o n a l )
g l o b a l _ a v e r a g e _ p o o l i n g 2 d (G ( None , 2 0 4 8 )

[ ( None , 1 5 0 , 1 5 0 , 3 ) ]
( None , 1 5 0 , 1 5 0 , 3 )
( None , 1 5 0 , 1 5 0 , 3 )
( None , 5 , 5 , 2 0 4 8 )

0
0
0
20861480
0

330

CHAPTER 9. TRANSFER LEARNING

l o b a l A v e r a g e P o o l i n g 2 D )
dropout
d e n s e ( Dense )

( Dropout )

( None , 2 0 4 8 )
( None , 1 )

0
2049

=================================================================
T o t a l params : 2 0 , 8 6 3 , 5 2 9
T r a i n a b l e params : 2 0 , 8 0 9 , 0 0 1

. . .

val_binary_accuracy : 0 . 9 8 3 7
Epoch 10/10
291/291 [==============================] − 41 s 140ms/ s t e p − l o s s :
0 . 0 1 6 2 − b i n a r y _ a c c u r a c y : 0 . 9 9 4 4 − v a l _ l o s s : 0 . 0 5 4 8 −
val_binary_accuracy : 0 . 9 8 1 9

9.3 Part 9.3: Transfer Learning for NLP with Keras

You will commonly use transfer learning with Natural Language Processing (NLP). Word embeddings are
a common means of transfer learning in NLP where network layers map words to vectors. Third parties
trained neural networks on a large corpus of text to learn these embeddings. We will use these vectors as
the input to the neural network rather than the actual characters of words.

This course has an entire module covering NLP; however, we use word embeddings to perform sentiment
analysis in this module. We will speciﬁcally attempt to classify if a text sample is speaking in a positive
or negative tone.

The following three sources were helpful for the creation of this section.

• Universal sentence encoder[3]. arXiv preprint arXiv:1803.11175)
• Deep Transfer Learning for Natural Language Processing: Text Classiﬁcation with Universal Embed-

dings[15]

• Keras Tutorial: How to Use Google’s Universal Sentence Encoder for Spam Classiﬁcation

These examples use TensorFlow Hub, which allows pretrained models to be loaded into TensorFlow easily.
To install TensorHub use the following commands.

Code

! p i p i n s t a l l

tens o rflo w _hu b

It is also necessary to install TensorFlow Datasets, which you can install with the following command.

9.3. PART 9.3: TRANSFER LEARNING FOR NLP WITH KERAS

331

Code

! p i p i n s t a l l

t e n s o r f l o w _ d a t a s e t s

Movie reviews are a good source of training data for sentiment analysis. These reviews are textual,
and users give them a star rating which indicates if the viewer had a positive or negative experience with
the movie. Load the Internet Movie DataBase (IMDB) reviews data set. This example is based on a
TensorFlow example that you can ﬁnd here.

Code

import t e n s o r f l o w a s
import tens orflow _h ub a s hub
import t e n s o r f l o w _ d a t a s e t s a s

t f

t f d s

train_data ,

t e s t _ d a t a = t f d s . l o a d ( name=" imdb_reviews " ,

s p l i t =[ " t r a i n " ,
b a t c h _ s i z e =−1, a s _ s u p e r v i s e d=True )

" t e s t " ] ,

train_examples ,
test_examples ,

t r a i n _ l a b e l s = t f d s . as_numpy ( t r a i n _ d a t a )

t e s t _ l a b e l s = t f d s . as_numpy ( t e s t _ d a t a )

# / Users / j h e a t o n / t e n s o r f l o w _ d a t a s e t s / imdb_reviews / p l a i n _ t e x t / 0 . 1 . 0

Load a pretrained embedding model called gnews-swivel-20dim. Google trained this network on GNEWS

data and can convert raw text into vectors.

Code

model = " h t t p s : / / t f h u b . dev / g o o g l e / t f 2 −p r e v i e w / gnews−s w i v e l −20dim/1 "
hub_layer = hub . KerasLayer ( model , output_shape = [ 2 0 ] ,

input_shape = [ ] ,

dtype=t f . s t r i n g ,

t r a i n a b l e=True )

The following code displays three movie reviews. This display allows you to see the actual data.

Code

t r a i n _ e x a m p l e s [ : 3 ]

Output

a r r a y ( [ b " This was an a b s o l u t e l y t e r r i b l e movie . Don ' t be l u r e d i n by
C h r i s t o p h e r Walken o r Michael
t h i s must s i m p l y be t h e i r w or st

I r o n s i d e . Both a r e g r e a t a c t o r s , but
r o l e i n h i s t o r y . Even t h e i r g r e a t

332

CHAPTER 9. TRANSFER LEARNING

r i d i c u l o u s

a c t i n g c o u l d not redeem t h i s movie ' s
i s an e a r l y n i n e t i e s US propaganda p i e c e . The most p a t h e t i c s c e n e s
were t h o s e when t h e Columbian r e b e l s were making t h e i r
r e v o l u t i o n s . Maria Conchita Alonso appeared phony , and h e r pseudo−l o v e
a f f a i r with Walken was n o t h i n g but a p a t h e t i c e m o t i o n a l p l u g i n a
movie t h a t was d e v o i d o f any r e a l meaning .
I am d i s a p p o i n t e d t h a t
t h e r e a r e movies
good name .

r u i n i n g a c t o r ' s
through i t . " ,

l i k e C h r i s t o p h e r Walken ' s

s t o r y l i n e . This movie

I c o u l d b a r e l y s i t

c a s e s

t h i s ,

l i k e

f o r

b ' I have been known t o f a l l a s l e e p d u r i n g f i l m s , but

t h i s

i s

u s u a l l y due t o a c o m b i n a t i o n o f
t h i n g s
warm and c o m f o r t a b l e on t h e s e t t e and having j u s t e a t e n a l o t . However
on t h i s o c c a s i o n I

f e l l a s l e e p b e c a u s e t h e f i l m was r u b b i s h . The p l o t

r e a l l y t i r e d , b e i n g

i n c l u d i n g ,

. . .

r us h . Mr . Mann and company appear
Deadwood ,
/>Canadian v i e w e r s be p r e p a r e d f o r a R e e f e r Madness type o f e n j o y a b l e
howl with t h i s

t o shake your head i n d i s g u s t . ' ] ,

t h e American Wild West.< br /><br

t o have mistaken Dawson City f o r

t h e Canadian North f o r

l u d i c r o u s p l o t , or ,

dtype=o b j e c t )

The embedding layer can convert each to 20-number vectors, which the neural network receives as input

in place of the actual words.

Code

hub_layer ( t r a i n _ e x a m p l e s [ : 3 ] )

Output

< t f . Tensor :
a r r a y ( [ [ 1 . 7 6 5 7 8 5 9 , −3.882232
,

shape =(3 , 2 0 ) , dtype=f l o a t 3 2 , numpy=

,

3 . 9 1 3 4 2 4

, −1.5557289 , −3.3362343

−1.7357956 , −1.9954445 ,

1 . 2 9 8 9 5 5

,

5 . 0 8 1 5 9 7

, −1.1041285

−2.0503852 , −0.7267516 , −0.6567596 ,

0 . 2 4 4 3 6 1 4 5 , −3.7208388

2 . 0 9 5 4 8 3 5 ,

2 . 2 9 6 9 3 3 2 , −2.0689783 , −2.9489715 , −1.1315986

[ 1 . 8 8 0 4 4 8 5 , −2.5852385 ,

3 . 4 0 6 6 9 9 4 ,

1 . 0 9 8 2 6 7 6 , −4.056685

−4.891284

, −2.7855542 ,

1 . 3 8 7 4 2 2 7 ,

3 . 8 4 7 6 4 5 8 , −0.9256539

,

,

] ,

,

,

9.3. PART 9.3: TRANSFER LEARNING FOR NLP WITH KERAS

333

−1.896706

,

1 . 2 1 1 3 2 8 1 ,

0 . 1 1 4 7 4 7 1 6 ,

0 . 7 6 2 0 9 4 5 6 , −4.8791065

,

. . .

,

−2.2268343 ,

0 . 0 7 4 4 6 6 1 6 , −1.4075902 , −0.706454

, −1.907037

1 . 4 4 1 9 7 8 8 ,

1 . 9 5 5 1 8 6 4 , −0.42660046 , −2.8022065 ,

0 . 4 3 7 2 7 0 6 7 ] ] ,

dtype=f l o a t 3 2 )>

We add additional layers to classify the movie reviews as either positive or negative.

Code

model = t f . k e r a s . S e q u e n t i a l ( )
model . add ( hub_layer )
model . add ( t f . k e r a s . l a y e r s . Dense ( 1 6 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( t f . k e r a s . l a y e r s . Dense ( 1 , a c t i v a t i o n= ' s i g m o i d ' ) )

model . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

k e r a s _ l a y e r
d e n s e ( Dense )
dense_1 ( Dense )

( KerasLayer )

( None , 2 0 )
( None , 1 6 )
( None , 1 )

400020
336
17

=================================================================
T o t a l params : 4 0 0 , 3 7 3
T r a i n a b l e params : 4 0 0 , 3 7 3
Non−t r a i n a b l e params : 0
_________________________________________________________________

We are now ready to compile the neural network. For this application, we use the adam training method

for binary classiﬁcation. We also save the initial random weights for later to start over easily.

334

Code

CHAPTER 9. TRANSFER LEARNING

model . compile ( o p t i m i z e r= ' adam ' ,

l o s s= ' b i n a r y _ c r o s s e n t r o p y ' ,
m e t r i c s =[ ' a c c u r a c y ' ] )

i n i t _ w e i g h t s = model . g e t _ w e i g h t s ( )

Before ﬁtting, we split the training data into the train and validation sets.

Code

x_val = t r a i n _ e x a m p l e s [ : 1 0 0 0 0 ]
p a r t i a l _ x _ t r a i n = t r a i n _ e x a m p l e s [ 1 0 0 0 0 : ]

y_val = t r a i n _ l a b e l s [ : 1 0 0 0 0 ]
p a r t i a l _ y _ t r a i n = t r a i n _ l a b e l s [ 1 0 0 0 0 : ]

We can now ﬁt the neural network. This ﬁtting will run for 40 epochs and allow us to evaluate the

eﬀectiveness of the neural network, as measured by the training set.

Code

h i s t o r y = model . f i t ( p a r t i a l _ x _ t r a i n ,
p a r t i a l _ y _ t r a i n ,
e p oc h s =40 ,
b a t c h _ s i z e =512 ,
v a l i d a t i o n _ d a t a =(x_val , y_val ) ,
v e r b o s e =1)

Output

. . .
30/30 [==============================] − 1 s 37ms/ s t e p − l o s s : 0 . 0 7 1 1 −
a c c u r a c y : 0 . 9 8 2 0 − v a l _ l o s s : 0 . 3 5 6 2 − v a l _ a c c u r a c y : 0 . 8 7 3 8
Epoch 40/40
30/30 [==============================] − 1 s 37ms/ s t e p − l o s s : 0 . 0 6 6 1 −
a c c u r a c y : 0 . 9 8 4 7 − v a l _ l o s s : 0 . 3 6 2 6 − v a l _ a c c u r a c y : 0 . 8 7 2 8

9.3.1 Beneﬁts of Early Stopping

While we used a validation set, we ﬁt the neural network without early stopping. This dataset is complex
enough to allow us to see the beneﬁt of early stopping. We will examine how accuracy and loss progressed

9.3. PART 9.3: TRANSFER LEARNING FOR NLP WITH KERAS

335

for training and validation sets. Loss measures the degree to which the neural network was conﬁdent in
incorrect answers. Accuracy is the percentage of correct classiﬁcations, regardless of the neural network’s
conﬁdence.

We begin by looking at the loss as we ﬁt the neural network.

Code

%m a t p l o t l i b i n l i n e
import m a t p l o t l i b . p y p l o t a s p l t

h i s t o r y _ d i c t = h i s t o r y . h i s t o r y
a c c = h i s t o r y _ d i c t [ ' a c c u r a c y ' ]
val_acc = h i s t o r y _ d i c t [ ' v a l _ a c c u r a c y ' ]
l o s s = h i s t o r y _ d i c t [ ' l o s s ' ]
v a l _ l o s s = h i s t o r y _ d i c t [ ' v a l _ l o s s ' ]

e p oc h s = range ( 1 ,

len ( a c c ) + 1 )

' bo ' ,

l o s s ,

p l t . p l o t ( epochs ,
p l t . p l o t ( epochs , v a l _ l o s s ,
p l t . t i t l e ( ' T r a i n i n g ␣and␣ v a l i d a t i o n ␣ l o s s ' )
p l t . x l a b e l ( ' Epochs ' )
p l t . y l a b e l ( ' Loss ' )
p l t . l e g e n d ( )

' b ' ,

l a b e l= ' T r a i n i n g ␣ l o s s ' )

l a b e l= ' V a l i d a t i o n ␣ l o s s ' )

p l t . show ( )

Output

We can see that training and validation loss are similar early in the ﬁtting. However, as ﬁtting continues

336

CHAPTER 9. TRANSFER LEARNING

and overﬁtting sets in, training and validation loss diverge from each other. Training loss continues to fall
consistently. However, once overﬁtting happens, the validation loss no longer falls and eventually begins
to increase a bit. Early stopping, which we saw earlier in this course, can prevent some overﬁtting.

Code

p l t . c l f ( )

# c l e a r

f i g u r e

l a b e l= ' T r a i n i n g ␣ a c c ' )

' bo ' ,

p l t . p l o t ( epochs , acc ,
p l t . p l o t ( epochs , val_acc ,
p l t . t i t l e ( ' T r a i n i n g ␣and␣ v a l i d a t i o n ␣ a c c u r a c y ' )
p l t . x l a b e l ( ' Epochs ' )
p l t . y l a b e l ( ' Accuracy ' )
p l t . l e g e n d ( )

' b ' ,

l a b e l= ' V a l i d a t i o n ␣ a c c ' )

p l t . show ( )

Output

The accuracy graph tells a similar story. Now let’s repeat the ﬁtting with early stopping. We begin by
creating an early stopping monitor and restoring the network’s weights to random. Once this is complete,
we can ﬁt the neural network with the early stopping monitor enabled.

Code

from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,

p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,
r e s t o r e _ b e s t _ w e i g h t s=True )

9.3. PART 9.3: TRANSFER LEARNING FOR NLP WITH KERAS

337

model . s e t _ w e i g h t s ( i n i t _ w e i g h t s )

h i s t o r y = model . f i t ( p a r t i a l _ x _ t r a i n ,
p a r t i a l _ y _ t r a i n ,
e p oc h s =40 ,
b a t c h _ s i z e =512 ,
c a l l b a c k s =[ monitor ] ,
v a l i d a t i o n _ d a t a =(x_val , y_val ) ,
v e r b o s e =1)

Output

. . .
30/30 [==============================] − 1 s 39ms/ s t e p − l o s s : 0 . 1 4 7 5 −
a c c u r a c y : 0 . 9 5 0 8 − v a l _ l o s s : 0 . 3 2 2 0 − v a l _ a c c u r a c y : 0 . 8 7 0 0
Epoch 34/40
29/30 [============================>.] − ETA: 0 s − l o s s : 0 . 1 4 1 9 −
a c c u r a c y : 0 . 9 5 2 8 R e s t o r i n g model w e i g h t s from t h e end o f
epoch : 2 9 .
30/30 [==============================] − 1 s 38ms/ s t e p − l o s s : 0 . 1 4 1 4 −
a c c u r a c y : 0 . 9 5 3 1 − v a l _ l o s s : 0 . 3 2 3 1 − v a l _ a c c u r a c y : 0 . 8 7 0 4
Epoch 0 0 0 3 4 : e a r l y s t o p p i n g

t h e b e s t

The training history chart is now shorter because we stopped earlier.

Code

h i s t o r y _ d i c t = h i s t o r y . h i s t o r y
a c c = h i s t o r y _ d i c t [ ' a c c u r a c y ' ]
val_acc = h i s t o r y _ d i c t [ ' v a l _ a c c u r a c y ' ]
l o s s = h i s t o r y _ d i c t [ ' l o s s ' ]
v a l _ l o s s = h i s t o r y _ d i c t [ ' v a l _ l o s s ' ]

e p oc h s = range ( 1 ,

len ( a c c ) + 1 )

' bo ' ,

l o s s ,

p l t . p l o t ( epochs ,
p l t . p l o t ( epochs , v a l _ l o s s ,
p l t . t i t l e ( ' T r a i n i n g ␣and␣ v a l i d a t i o n ␣ l o s s ' )
p l t . x l a b e l ( ' Epochs ' )
p l t . y l a b e l ( ' Loss ' )
p l t . l e g e n d ( )

' b ' ,

l a b e l= ' T r a i n i n g ␣ l o s s ' )

l a b e l= ' V a l i d a t i o n ␣ l o s s ' )

338

CHAPTER 9. TRANSFER LEARNING

p l t . show ( )

Output

Finally, we evaluate the accuracy for the best neural network before early stopping occured.

Code

from s k l e a r n . m e t r i c s import a c c u r a c y _ s c o r e
import numpy a s np

pred = model . p r e d i c t ( x_val )
# Use 0 . 5 as t h e t h r e s h o l d
p r e d i c t _ c l a s s e s = pred . f l a t t e n () >0.5

c o r r e c t = a c c u r a c y _ s c o r e ( y_val , p r e d i c t _ c l a s s e s )
print ( f " Accuracy : ␣{ c o r r e c t } " )

Output

Accuracy : 0 . 8 6 8 5

9.4 Part 9.4: Transfer Learning for Facial Points and GANs

I designed this notebook to work with Google Colab. You can run it locally; however, you might need to
adjust some of the installation scripts contained in this notebook.

9.4. PART 9.4: TRANSFER LEARNING FOR FACIAL POINTS AND GANS

339

This part will see how we can use a 3rd party neural network to detect facial features, particularly the
location of an individual’s eyes. By locating eyes, we can crop portraits consistently. Previously, we saw
that GANs could convert a random vector into a realistic-looking portrait. We can also perform the reverse
and convert an actual photograph into a numeric vector. If we convert two images into these vectors, we
can produce a video that transforms between the two images.

NVIDIA trained StyleGAN on portraits consistently cropped with the eyes always in the same location.
To successfully convert an image to a vector, we must crop the image similarly to how NVIDIA used
cropping.

The code presented here allows you to choose a starting and ending image and use StyleGAN2 to
produce a "morph" video between the two pictures. The preprocessing code will lock in on the exact
positioning of each image, so your crop does not have to be perfect. The main point of your crop is for you
to remove anything else that might be confused for a face. If multiple faces are detected, you will receive
an error.

Also, make sure you have selected a GPU Runtime from CoLab. Choose "Runtime," then "Change

Runtime Type," and choose GPU for "Hardware Accelerator."

These settings allow you to change the high-level conﬁguration. The number of steps determines how
long your resulting video is. The video plays at 30 frames a second, so 150 is 5 seconds. You can also
specify freeze steps to leave the video unchanged at the beginning and end. You will not likely need to
change the network.

Code

NETWORK = " h t t p s : / / nvlabs −f i −cdn . n v i d i a . com/ " \
" s t y l e g a n 2 −ada−p y t o r c h / p r e t r a i n e d / f f h q . p k l "

STEPS = 150
FPS = 30
FREEZE_STEPS = 30

9.4.1 Upload Starting and Ending Images

We will begin by uploading a starting and ending image. The Colab service uploads these images. If you
are running this code outside of Colab, these images are likely somewhere on your computer, and you
provide the path to these ﬁles using the SOURCE and TARGET variables.

Choose your starting image.

Code

import o s
from g o o g l e . c o l a b import f i l e s

uploaded = f i l e s . upload ( )

i f

len ( uploaded ) != 1 :

print ( " Upload ␣ e x a c t l y ␣1␣ f i l e ␣ f o r ␣ s o u r c e . " )

CHAPTER 9. TRANSFER LEARNING

340

e l s e :

for k , v in uploaded . i t e m s ( ) :

_, e x t = o s . path . s p l i t e x t ( k )
o s . remove ( k )
SOURCE_NAME = f " s o u r c e { e x t } "
open (SOURCE_NAME,

'wb ' ) . w r i t e ( v )

Also, choose your ending image.

Code

uploaded = f i l e s . upload ( )

i f

len ( uploaded ) != 1 :

print ( " Upload ␣ e x a c t l y ␣1␣ f i l e ␣ f o r ␣ t a r g e t . " )

e l s e :

for k , v in uploaded . i t e m s ( ) :

_, e x t = o s . path . s p l i t e x t ( k )
o s . remove ( k )
TARGET_NAME = f " t a r g e t { e x t } "
open (TARGET_NAME,

'wb ' ) . w r i t e ( v )

9.4.2 Install Software

Some software must be installed into Colab, for this notebook to work. We are speciﬁcally using these
technologies:

• Training Generative Adversarial Networks with Limited Data

Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, Timo Aila

• One-millisecond face alignment with an ensemble of regression trees Vahid Kazemi, Josephine Sullivan

Code

! wget h t t p : / / d l i b . n e t / f i l e s / shape_predictor_5_face_landmarks . dat . bz2
! b z i p 2 −d shape_predictor_5_face_landmarks . dat . bz2

Code

import s y s
! g i t c l o n e h t t p s : / / g i t h u b . com/ NVlabs / s t y l e g a n 2 −ada−p y t o r c h . g i t
! p i p i n s t a l l n i n j a

9.4. PART 9.4: TRANSFER LEARNING FOR FACIAL POINTS AND GANS

341

s y s . path . i n s e r t ( 0 ,

" / c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h " )

9.4.3 Detecting Facial Features

First, I will demonstrate how to detect the facial features we will use for consistent cropping and centering
of the images. To accomplish this, we will use the dlib package, a neural network library that gives us
access to several pretrained models. The DLIB Face Recognition ResNET Model V1 is the model we will
use; This is a 5-point landmarking model which identiﬁes the corners of the eyes and bottom of the nose.
The creators of this network trained it on the dlib 5-point face landmark dataset, which consists of 7198
faces.

We begin by initializing dlib and loading the facial features neural network.

Code

import cv2
import numpy a s np
from PIL import Image
import d l i b
from m a t p l o t l i b import p y p l o t a s p l t

d e t e c t o r = d l i b . g e t _ f r o n t a l _ f a c e _ d e t e c t o r ( )
p r e d i c t o r = d l i b . s h a p e _ p r e d i c t o r ( ' shape_predictor_5_face_landmarks . dat ' )

Let’s start by looking at the facial features of the source image. The following code detects the ﬁve

facial features and displays their coordinates.

Code

img = cv2 . imread (SOURCE_NAME)
i f

img i s None :

r a i s e V a l u e E r r o r ( " S o u r c e ␣ image ␣ not ␣ found " )

gray = cv2 . c v t C o l o r ( img , cv2 .COLOR_BGR2GRAY)
r e c t s = d e t e c t o r ( gray , 0 )

i f

len ( r e c t s ) == 0 :

r a i s e V a l u e E r r o r ( "No␣ f a c e s ␣ d e t e c t e d " )

e l i f

len ( r e c t s ) > 1 :

r a i s e V a l u e E r r o r ( " M u l t i p l e ␣ f a c e s ␣ d e t e c t e d " )

shape = p r e d i c t o r ( gray ,

r e c t s [ 0 ] )

w = img . shape [ 0 ] / / 5 0

342

CHAPTER 9. TRANSFER LEARNING

for i

in range ( 0 , 5 ) :
pt1 = ( shape . p a r t ( i ) . x ,
pt2 = ( shape . p a r t ( i ) . x+w,
cv2 . r e c t a n g l e ( img , pt1 , pt2 , ( 0 , 2 5 5 , 2 5 5 ) , 4 )
print ( pt1 , pt2 )

shape . p a r t ( i ) . y )

shape . p a r t ( i ) . y+w)

Output

( 1 0 9 8 , 5 4 6 )
( 9 9 4 , 5 5 4 )
( 7 3 1 , 5 5 6 )
( 8 3 3 , 5 5 6 )
( 9 2 5 , 7 2 9 )

( 1 1 2 8 , 5 7 6 )

( 1 0 2 4 , 5 8 4 )
( 7 6 1 , 5 8 6 )
( 8 6 3 , 5 8 6 )
( 9 5 5 , 7 5 9 )

We can easily plot these features onto the source image. You can see the corners of the eyes and the

base of the nose.

Code

img = cv2 . c v t C o l o r ( img , cv2 .COLOR_BGR2RGB)
p l t . imshow ( img )
p l t . t i t l e ( ' s o u r c e ' )
p l t . show ( )

Output

9.4. PART 9.4: TRANSFER LEARNING FOR FACIAL POINTS AND GANS

343

9.4.4 Preprocess Images for Best StyleGAN Results

Using dlib, we will center and crop the source and target image, using the eye positions as reference. I
created two functions to accomplish this task. The ﬁrst calls dlib and ﬁnd the locations of the person’s
eyes. The second uses the eye locations to center the image around the eyes. We do not exactly center;
we are oﬀsetting slightly to center, similar to the original StyleGAN training set. I determined this oﬀset
by detecting the eyes of a generated StyleGAN face. The distance between the eyes gives us a means of
telling how big the face is, which we use to scale the images consistently.

Code

def

f i n d _ e y e s ( img ) :

gray = cv2 . c v t C o l o r ( img , cv2 .COLOR_BGR2GRAY)
r e c t s = d e t e c t o r ( gray , 0 )

i f

len ( r e c t s ) == 0 :

r a i s e V a l u e E r r o r ( "No␣ f a c e s ␣ d e t e c t e d " )

e l i f

len ( r e c t s ) > 1 :

r a i s e V a l u e E r r o r ( " M u l t i p l e ␣ f a c e s ␣ d e t e c t e d " )

shape = p r e d i c t o r ( gray ,
f e a t u r e s = [ ]

r e c t s [ 0 ] )

for i

in range ( 0 , 5 ) :

344

CHAPTER 9. TRANSFER LEARNING

f e a t u r e s . append ( ( i ,

( shape . p a r t ( i ) . x ,

shape . p a r t ( i ) . y ) ) )

return ( int ( f e a t u r e s [ 3 ] [ 1 ] [ 0 ] + f e a t u r e s [ 2 ] [ 1 ] [ 0 ] ) // 2 , \

int ( f e a t u r e s [ 3 ] [ 1 ] [ 1 ] + f e a t u r e s [ 2 ] [ 1 ] [ 1 ] ) // 2 ) , \
( int ( f e a t u r e s [ 1 ] [ 1 ] [ 0 ] + f e a t u r e s [ 0 ] [ 1 ] [ 0 ] ) // 2 , \
int ( f e a t u r e s [ 1 ] [ 1 ] [ 1 ] + f e a t u r e s [ 0 ] [ 1 ] [ 1 ] ) // 2 )

def c r o p _ s t y l e g a n ( img ) :

t h e f a c e

r i g h t _ e y e = f i n d _ e y e s ( img )

l e f t _ e y e ,
# C a l c u l a t e t h e s i z e o f
d = abs ( r i g h t _ e y e [ 0 ] − l e f t _ e y e [ 0 ] )
z = 255/ d
# C ons ide r t h e a s p e c t
a r = img . shape [ 0 ] / img . shape [ 1 ]
w = img . shape [ 1 ] ∗ z
img2 = cv2 . r e s i z e ( img ,
b o r d e r s i z e = 1024
img3 = cv2 . copyMakeBorder (

( int (w) ,

r a t i o

int (w∗ a r ) ) )

img2 ,
top=b o r d e r s i z e ,
bottom=b o r d e r s i z e ,
l e f t =b o r d e r s i z e ,
r i g h t=b o r d e r s i z e ,
borderType=cv2 .BORDER_REPLICATE)

l e f t _ e y e 2 ,

r i g h t _ e y e 2 = f i n d _ e y e s ( img3 )

t o t h e o f f s e t used by StyleGAN2

# A d j u s t
c r o p 1 = l e f t _ e y e 2 [ 0 ] − 385
c r o p 0 = l e f t _ e y e 2 [ 1 ] − 490
return img3 [ c r o p 0 : c r o p 0 +1024 , c r o p 1 : c r o p 1 +1024]

The following code will preprocess and crop your images. If you receive an error indicating multiple
faces were found, try to crop your image better or obscure the background. If the program does not see a
face, then attempt to obtain a clearer and more high-resolution image.

Code

image_source = cv2 . imread (SOURCE_NAME)
i f

image_source i s None :

r a i s e V a l u e E r r o r ( " S o u r c e ␣ image ␣ not ␣ found " )

image_target = cv2 . imread (TARGET_NAME)

9.4. PART 9.4: TRANSFER LEARNING FOR FACIAL POINTS AND GANS

345

i f

image_target

i s None :

r a i s e V a l u e E r r o r ( " S o u r c e ␣ image ␣ not ␣ found " )

cropped_source = c r o p _ s t y l e g a n ( image_source )
c r o p p e d _ t a r g e t = c r o p _ s t y l e g a n ( image_target )

img = cv2 . c v t C o l o r ( cropped_source , cv2 .COLOR_BGR2RGB)
p l t . imshow ( img )
p l t . t i t l e ( ' s o u r c e ' )
p l t . show ( )

img = cv2 . c v t C o l o r ( cropped_target , cv2 .COLOR_BGR2RGB)
p l t . imshow ( img )
p l t . t i t l e ( ' t a r g e t ' )
p l t . show ( )

cv2 . i m w r i t e ( " cropped_source . png " , cropped_source )
cv2 . i m w r i t e ( " c r o p p e d _ t a r g e t . png " , c r o p p e d _ t a r g e t )

#p r i n t ( f i n d _ e y e s ( cropped_source ) )
#p r i n t ( f i n d _ e y e s ( c r o p p e d _ t a r g e t ) )

Output

346

True

CHAPTER 9. TRANSFER LEARNING

The two images are now 1024x1024 and cropped similarly to the ﬀhq dataset that NVIDIA used to

train StyleGAN.

9.4.5 Convert Source to a GAN

We will use StyleGAN2, rather than the latest StyleGAN3, because StyleGAN2 contains a projector.py
utility that converts images to latent vectors. StyleGAN3 does not have as good support for this projection.
First, we convert the source to a GAN latent vector. This process will take several minutes.

Code

cmd = f " python ␣/ c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h / p r o j e c t o r . py␣ " \
f "−−save−v i d e o ␣0␣−−num−s t e p s ␣ 1000 ␣−−o u t d i r=out_source ␣ " \
f "−−t a r g e t=cropped_source . png␣−−network={NETWORK} "

! { cmd}

9.4.6 Convert Target to a GAN

Next, we convert the target to a GAN latent vector. This process will also take several minutes.

Code

cmd = f " python ␣/ c o n t e n t / s t y l e g a n 2 −ada−p y t o r c h / p r o j e c t o r . py␣ " \
f "−−save−v i d e o ␣0␣−−num−s t e p s ␣ 1000 ␣−−o u t d i r=o u t _ t a r g e t ␣ " \
f "−−t a r g e t=c r o p p e d _ t a r g e t . png␣−−network={NETWORK} "

! { cmd}

With the conversion complete, lets have a look at the two GANs.

Code

img_gan_source = cv2 . imread ( ' / c o n t e n t / out_source / p r o j . png ' )
img = cv2 . c v t C o l o r ( img_gan_source , cv2 .COLOR_BGR2RGB)
p l t . imshow ( img )
p l t . t i t l e ( ' s o u r c e −gan ' )
p l t . show ( )

Output

9.4. PART 9.4: TRANSFER LEARNING FOR FACIAL POINTS AND GANS

347

Code

img_gan_target = cv2 . imread ( ' / c o n t e n t / o u t _ t a r g e t / p r o j . png ' )
img = cv2 . c v t C o l o r ( img_gan_target , cv2 .COLOR_BGR2RGB)
p l t . imshow ( img )
p l t . t i t l e ( ' t a r g e t −gan ' )
p l t . show ( )

Output

348

CHAPTER 9. TRANSFER LEARNING

As you can see, the two GAN-generated images look similar to their real-world counterparts. However,

they are by no means exact replicas.

9.4.7 Build the Video

The following code builds a transition video between the two latent vectors previously obtained.

Code

import t o r c h
import d n n l i b
import l e g a c y
import PIL . Image
import numpy a s np
import i m a g e i o
from tqdm . notebook import tqdm

l v e c 1 = np . l o a d ( ' / c o n t e n t / out_source / projected_w . npz ' ) [ 'w ' ]
l v e c 2 = np . l o a d ( ' / c o n t e n t / o u t _ t a r g e t / projected_w . npz ' ) [ 'w ' ]

network_pkl = " h t t p s : / / nvlabs −f i −cdn . n v i d i a . com/ s t y l e g a n 2 " \

"−ada−p y t o r c h / p r e t r a i n e d / f f h q . p k l "

d e v i c e = t o r c h . d e v i c e ( ' cuda ' )
with d n n l i b . u t i l . open_url ( network_pkl ) a s

f p :
G = l e g a c y . load_network_pkl ( f p ) [ 'G_ema ' ] \

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

349

. r e q u i r e s _ g r a d _ ( F a l s e ) . t o ( d e v i c e )

d i f f = l v e c 2 − l v e c 1
s t e p = d i f f / STEPS
c u r r e n t = l v e c 1 . copy ( )
t a r g e t _ u i n t 8 = np . a r r a y ( [ 1 0 2 4 , 1 0 2 4 , 3 ] , dtype=np . u i n t 8 )

v i d e o = i m a g e i o . g e t _ w r i t e r ( ' / c o n t e n t / movie . mp4 ' , mode= ' I ' ,
c o d e c= ' l i b x 2 6 4 ' , b i t r a t e= ' 16M' )

f p s=FPS ,

for j

in tqdm ( range (STEPS ) ) :

z = t o r c h . from_numpy ( c u r r e n t ) . t o ( d e v i c e )
synth_image = G. s y n t h e s i s ( z , noise_mode= ' c o n s t ' )
synth_image = ( synth_image + 1 ) ∗ ( 2 5 5 / 2 )
synth_image = synth_image . permute ( 0 , 2 , 3 , 1 ) . clamp ( 0 , 2 5 5 ) \

. t o ( t o r c h . u i n t 8 ) [ 0 ] . cpu ( ) . numpy ( )

r e p e a t = FREEZE_STEPS i f

j==0 or j ==(STEPS−1) e l s e 1

for i

in range ( r e p e a t ) :

v i d e o . append_data ( synth_image )

c u r r e n t = c u r r e n t + s t e p

v i d e o . c l o s e ( )

9.4.8 Download your Video

If you made it through all of these steps, you are now ready to download your video.

Code

from g o o g l e . c o l a b import f i l e s
f i l e s . download ( " movie . mp4" )

9.5 Part 9.5: Transfer Learning for Keras Style Transfer

In this part, we will implement style transfer. This technique takes two images as input and produces a
third. The ﬁrst image is the base image that we wish to transform. The second image represents the style
we want to apply to the source image. Finally, the algorithm renders a third image that emulates the style
characterized by the style image. This technique is called style transfer.[6]

350

CHAPTER 9. TRANSFER LEARNING

Figure 9.1: Style Transfer

I based the code presented in this part on a style transfer example in the Keras documentation created

by François Chollet.

We begin by uploading two images to Colab. If running this code locally, point these two ﬁlenames at

the local copies of the images you wish to use.

• base_image_path - The image to apply the style to.
• style_reference_image_path - The image whose style we wish to copy.

First, we upload the base image.

Code

import o s
from g o o g l e . c o l a b import f i l e s

uploaded = f i l e s . upload ( )

i f

len ( uploaded ) != 1 :

print ( " Upload ␣ e x a c t l y ␣1␣ f i l e ␣ f o r ␣ s o u r c e . " )

e l s e :

for k , v in uploaded . i t e m s ( ) :

_, e x t = o s . path . s p l i t e x t ( k )
o s . remove ( k )
base_image_path = f " s o u r c e { e x t } "
open ( base_image_path ,

'wb ' ) . w r i t e ( v )

We also, upload the style image.

Code

uploaded = f i l e s . upload ( )

i f

len ( uploaded ) != 1 :

print ( " Upload ␣ e x a c t l y ␣1␣ f i l e ␣ f o r ␣ t a r g e t . " )

e l s e :

for k , v in uploaded . i t e m s ( ) :

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

351

_, e x t = o s . path . s p l i t e x t ( k )
o s . remove ( k )
s t y l e _ r e f e r e n c e _ i m a g e _ p a t h = f " s t y l e { e x t } "
open ( s t y l e _ r e f e r e n c e _ i m a g e _ p a t h ,

'wb ' ) . w r i t e ( v )

The loss function balances three diﬀerent goals deﬁned by the following three weights. Changing these

weights allows you to ﬁne-tune the image generation.

• total_variation_weight - How much emphasis to place on the visual coherence of nearby pixels.
• style_weight - How much emphasis to place on emulating the style of the reference image.
• content_weight - How much emphasis to place on remaining close in appearance to the base image.

Code

import numpy a s np
import t e n s o r f l o w a s
from t e n s o r f l o w import k e r a s
from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s import vgg19

t f

r e s u l t _ p r e f i x = " g e n e r a t e d "

t h e d i f f e r e n t

# Weights o f
t o t a l _ v a r i a t i o n _ w e i g h t = 1 e−6
s t y l e _ w e i g h t = 1 e−6
content_weight = 2 . 5 e−8

l o s s components

t h e g e n e r a t e d p i c t u r e .

# Dimensions o f
width , h e i g h t = k e r a s . p r e p r o c e s s i n g . image . load_img ( base_image_path ) . s i z e
img_nrows = 400
img_ncols = int ( width ∗ img_nrows / h e i g h t )

We now display the two images we will use, ﬁrst the base image followed by the style image.

Code

from IPython . d i s p l a y import Image , d i s p l a y

print ( " S o u r c e ␣ Image " )
d i s p l a y ( Image ( base_image_path ) )

Output

352

CHAPTER 9. TRANSFER LEARNING

S o u r c e Image

Code

print ( " S t y l e ␣ Image " )
d i s p l a y ( Image ( s t y l e _ r e f e r e n c e _ i m a g e _ p a t h ) )

Output

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

353

S t y l e Image

9.5.1 Image Preprocessing and Postprocessing

The preprocess_image function begins by loading the image using Keras. We scale the image to the size
speciﬁed by img_nrows and img_ncols. The img_to_array converts the image to a Numpy array, to which
we add dimension to account for batching. The dimensions expected by VGG are colors depth, height,
width, and batch. Finally, we convert the Numpy array to a tensor.

The deprocess_image performs the reverse, transforming the output of the style transfer process back
to a regular image. First, we reshape the image to remove the batch dimension. Next, The outputs are
moved back into the 0-255 range by adding the mean value of the RGB colors. We must also convert the
BGR (blue, green, red) colorspace of VGG to the more standard RGB encoding.

Code

def p r e p r o c e s s _ i m a g e ( image_path ) :

f u n c t i o n t o open ,

# U t i l
# p i c t u r e s
img = k e r a s . p r e p r o c e s s i n g . image . load_img (

i n t o a p p r o p r i a t e t e n s o r s

r e s i z e and format

image_path ,

t a r g e t _ s i z e =(img_nrows ,

img_ncols )

)
img = k e r a s . p r e p r o c e s s i n g . image . img_to_array ( img )
img = np . expand_dims ( img , a x i s =0)

354

CHAPTER 9. TRANSFER LEARNING

img = vgg19 . p r e p r o c e s s _ i n p u t ( img )
return t f . c o n v e r t _ t o _ t e n s o r ( img )

def deprocess_image ( x ) :

i n t o a v a l i d image

img_ncols , 3 ) )

f u n c t i o n t o c o n v e r t a t e n s o r

# U t i l
x = x . r e s h a p e ( ( img_nrows ,
# Remove zero −c e n t e r by mean p i x e l
x [ : ,
x [ : ,
x [ : ,
# 'BGR'−>'RGB'
x = x [ : ,
x = np . c l i p ( x , 0 , 2 5 5 ) . a s t y p e ( " u i n t 8 " )
return x

: , 0 ] += 1 0 3 . 9 3 9
: , 1 ] += 1 1 6 . 7 7 9
: , 2 ] += 1 2 3 . 6 8

: : − 1 ]

: ,

9.5.2 Calculating the Style, Content, and Variation Loss

Before we see how to calculate the 3-part loss function, I must introduce the Gram matrix’s mathematical
concept. Figure 9.2 demonstrates this concept.

Figure 9.2: The Gram Matrix

We calculate the Gram matrix by multiplying a matrix by its transpose. To calculate two parts of the
loss function, we will take the Gram matrix of the outputs from several convolution layers in the VGG
network. To determine both style, and similarity to the original image, we will compare the convolution
layer output of VGG rather than directly comparing the image pixels. In the third part of the loss function,
we will directly compare pixels near each other.

Because we are taking convolution output from several diﬀerent levels of the VGG network, the Gram
matrix provides a means of combining these layers. The Gram matrix of the VGG convolution layers
represents the style of the image. We will calculate this style for the original image, the style-reference
image, and the ﬁnal output image as the algorithm generates it.

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

355

Code

# The gram m a t r i x o f an image t e n s o r ( f e a t u r e −w i s e o u t e r p r o d u c t )
def gram_matrix ( x ) :

x = t f . t r a n s p o s e ( x ,
f e a t u r e s = t f . r e s h a p e ( x ,
gram = t f . matmul ( f e a t u r e s ,
return gram

( 2 , 0 , 1 ) )

( t f . shape ( x ) [ 0 ] , −1))

t f . t r a n s p o s e ( f e a t u r e s ) )

t h e r e f e r e n c e image i n t h e g e n e r a t e d image .

i s b a s e d on t h e gram m a t r i c e s ( which c a p t u r e s t y l e ) o f

l o s s "

i s d e s i g n e d t o maintain

# The " s t y l e
# t h e s t y l e o f
# I t
# f e a t u r e maps from t h e s t y l e r e f e r e n c e image
# and from t h e g e n e r a t e d image
def

s t y l e _ l o s s ( s t y l e , co m b i n a t i o n ) :
S = gram_matrix ( s t y l e )
C = gram_matrix ( c o m b i n a t i o n )
c h a n n e l s = 3
s i z e = img_nrows ∗ img_ncols
return t f . reduce_sum ( t f . s q u a r e ( S − C) ) /\
( 4 . 0 ∗ ( c h a n n e l s ∗∗ 2 ) ∗ ( s i z e ∗∗ 2 ) )

# An a u x i l i a r y l o s s
# d e s i g n e d t o maintain t h e " c o n t e n t " o f
# b a s e image i n t h e g e n e r a t e d image
def c o n t e n t _ l o s s ( base , c o m b i n a t i o n ) :

f u n c t i o n

t h e

return t f . reduce_sum ( t f . s q u a r e ( c o m b i n a t i o n − b a s e ) )

# The 3 rd l o s s
t o t a l
# d e s i g n e d t o k e e p t h e g e n e r a t e d image l o c a l l y c o h e r e n t
def

v a r i a t i o n l o s s ,

f u n c t i o n ,

t o t a l _ v a r i a t i o n _ l o s s ( x ) :
a = t f . s q u a r e (

x [ : ,

:

img_nrows − 1 ,
:

:
img_ncols − 1 ,

img_ncols − 1 ,
: ]

− x [ : , 1 : ,

)
b = t f . s q u a r e (

x [ : ,

:
− x [ : ,

img_nrows − 1 ,

:

:

img_nrows − 1 , 1 : ,

img_ncols − 1 ,
: ]

)
return t f . reduce_sum ( t f . pow( a + b , 1 . 2 5 ) )

: ] \

: ] \

356

CHAPTER 9. TRANSFER LEARNING

The style_loss function compares how closely the current generated image (combination) matches
the style of the reference style image. The Gram matrixes of the style and current generated image are
subtracted and normalized to calculate this diﬀerence in style. Precisely, it consists in a sum of L2 distances
between the Gram matrices of the representations of the base image and the style reference image, extracted
from diﬀerent layers of VGG. The general idea is to capture color/texture information at diﬀerent spatial
scales (fairly large scales, as deﬁned by the depth of the layer considered).

The content_loss function compares how closely the current generated image matches the original
image. You must subtract Gram matrixes of the original and generated images to calculate this diﬀerence.
Here we calculate the L2 distance between the base image’s VGG features and the generated image’s
features, keeping the generated image close enough to the original one.

Finally, the total_variation_loss function imposes local spatial continuity between the pixels of the

generated image, giving it visual coherence.

9.5.3 The VGG Neural Network

VGG19 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman.[31]The
model achieves 92.7% top-5 test accuracy in ImageNet, a dataset of over 14 million images belonging to
1000 classes. We will transfer the VGG16 weights into our style transfer model. Keras provides functions
to load the VGG neural network.

Code

# B u i l d a VGG19 model
model = vgg19 . VGG19( w e i g h t s=" imagenet " ,

l o a d e d w i t h pre−t r a i n e d ImageNet w e i g h t s

i n c l u d e _ t o p=F a l s e )

t h e s y m b o l i c o u t p u t s o f each " key "

# Get
o u t p u t s _ d i c t = dict ( [ ( l a y e r . name ,

l a y e r ( we g a v e them u n i q u e names ) .
in model . l a y e r s ] )

for l a y e r

l a y e r . output )

# S e t up a model
# VGG19 ( as a d i c t ) .
f e a t u r e _ e x t r a c t o r = k e r a s . Model ( i n p u t s=model . i n p u t s , o u t p u t s=o u t p u t s _ d i c t )

t h e a c t i v a t i o n v a l u e s

f o r e v e r y l a y e r

r e t u r n s

t h a t

i n

We can now generate the complete loss function. The following images are input to the compute_loss

function:

• combination_image - The current iteration of the generated image.
• base_image - The starting image.
• style_reference_image - The image that holds the style to reproduce.

The layers speciﬁed by style_layer_names indicate which layers should be extracted as features from VGG
for each of the three images.

Code

# L i s t o f
style _layer_names = [

l a y e r s

t o use f o r

t h e s t y l e

l o s s .

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

357

" block1_conv1 " ,
" block2_conv1 " ,
" block3_conv1 " ,
" block4_conv1 " ,
" block5_conv1 " ,

]
t o use f o r
# The l a y e r
content_layer_name = " block5_conv2 "

t h e c o n t e n t

l o s s .

def compute_loss ( combination_image , base_image ,

s t y l e _ r e f e r e n c e _ i m a g e ) :

i n p u t _ t e n s o r = t f . c o n c a t (

[ base_image ,

s t y l e _ r e f e r e n c e _ i m a g e , combination_image ] , a x i s =0

)
f e a t u r e s = f e a t u r e _ e x t r a c t o r ( i n p u t _ t e n s o r )

# I n i t i a l i z e
l o s s = t f . z e r o s ( shape =())

t h e l o s s

l o s s

# Add c o n t e n t
l a y e r _ f e a t u r e s = f e a t u r e s [ content_layer_name ]
b a s e _ i m a g e _ f e a t u r e s = l a y e r _ f e a t u r e s [ 0 ,
: ,
c o m b i n a t i o n _ f e a t u r e s = l a y e r _ f e a t u r e s [ 2 ,
l o s s = l o s s + content_weight ∗ c o n t e n t _ l o s s (

: ,

: ,

: ,

: ]

: ]

base_image_features , c o m b i n a t i o n _ f e a t u r e s

)
# Add s t y l e
for layer_name in style_layer_na mes :

l o s s

l a y e r _ f e a t u r e s = f e a t u r e s [ layer_name ]
s t y l e _ r e f e r e n c e _ f e a t u r e s = l a y e r _ f e a t u r e s [ 1 ,
c o m b i n a t i o n _ f e a t u r e s = l a y e r _ f e a t u r e s [ 2 ,
s l = s t y l e _ l o s s ( s t y l e _ r e f e r e n c e _ f e a t u r e s , c o m b i n a t i o n _ f e a t u r e s )
l o s s += ( s t y l e _ w e i g h t / len ( styl e_la yer_nam es ) ) ∗ s l

: ,

: ,

: ,

: ,

: ]

: ]

# Add t o t a l
l o s s += t o t a l _ v a r i a t i o n _ w e i g h t ∗ \

v a r i a t i o n l o s s

t o t a l _ v a r i a t i o n _ l o s s ( combination_image )

return l o s s

358

CHAPTER 9. TRANSFER LEARNING

9.5.4 Generating the Style Transferred Image

The compute_loss_and_grads function calls the loss function and computes the gradients. The parameters
of this model are the actual RGB values of the current iteration of the generated images. These parameters
start with the base image, and the algorithm optimizes them to the ﬁnal rendered image. We are not
training a model to perform the transformation; we are training/modifying the image to minimize the loss
functions. We utilize gradient tape to allow Keras to modify the image in the same way the neural network
training modiﬁes weights.

Code

@tf . f u n c t i o n
def compute_loss_and_grads ( combination_image , \
base_image ,
with t f . GradientTape ( ) a s

s t y l e _ r e f e r e n c e _ i m a g e ) :
t a p e :

l o s s = compute_loss ( combination_image , \

s t y l e _ r e f e r e n c e _ i m a g e )
g r a d s = t a p e . g r a d i e n t ( l o s s , combination_image )
return l o s s , g r a d s

base_image ,

We can now optimize the image according to the loss function.

Code

o p t i m i z e r = k e r a s . o p t i m i z e r s .SGD(

k e r a s . o p t i m i z e r s . s c h e d u l e s . ExponentialDecay (

i n i t i a l _ l e a r n i n g _ r a t e =100.0 , deca y_steps =100 , decay_rate =0.96

)

)

base_image = p r e p r o c e s s _ i m a g e ( base_image_path )
s t y l e _ r e f e r e n c e _ i m a g e = p r e p r o c e s s _ i m a g e ( s t y l e _ r e f e r e n c e _ i m a g e _ p a t h )
combination_image = t f . V a r i a b l e ( p r e p r o c e s s _ i m a g e ( base_image_path ) )

i t e r a t i o n s = 4000
in range ( 1 ,
for i

i t e r a t i o n s + 1 ) :

l o s s , g r a d s = compute_loss_and_grads (
combination_image , base_image ,

s t y l e _ r e f e r e n c e _ i m a g e

)
o p t i m i z e r . a p p l y _ g r a d i e n t s ( [ ( grads , combination_image ) ] )
i f

i % 100 == 0 :

print ( " I t e r a t i o n ␣%d : ␣ l o s s =%.2 f " % ( i ,
img = deprocess_image ( combination_image . numpy ( ) )
fname = r e s u l t _ p r e f i x + " _ a t _ i t e r a t i o n _%d . png " % i
k e r a s . p r e p r o c e s s i n g . image . save_img ( fname ,

l o s s ) )

img )

9.5. PART 9.5: TRANSFER LEARNING FOR KERAS STYLE TRANSFER

359

Output

I t e r a t i o n 1 0 0 :
I t e r a t i o n 2 0 0 :
I t e r a t i o n 3 0 0 :
I t e r a t i o n 4 0 0 :
I t e r a t i o n 5 0 0 :
I t e r a t i o n 6 0 0 :
I t e r a t i o n 7 0 0 :
I t e r a t i o n 8 0 0 :
I t e r a t i o n 9 0 0 :
I t e r a t i o n 1 0 0 0 :
I t e r a t i o n 1 1 0 0 :
I t e r a t i o n 1 2 0 0 :
I t e r a t i o n 1 3 0 0 :
I t e r a t i o n 1 4 0 0 :
I t e r a t i o n 1 5 0 0 :

l o s s =4890.20
l o s s =3527.19
l o s s =3022.59
l o s s =2751.59
l o s s =2578.63
l o s s =2457.19
l o s s =2366.39
l o s s =2295.66
l o s s =2238.67

l o s s =2191.59
l o s s =2151.88
l o s s =2117.95
l o s s =2088.56
l o s s =2062.86
l o s s =2040.14

. . .

I t e r a t i o n 3 6 0 0 :
I t e r a t i o n 3 7 0 0 :
I t e r a t i o n 3 8 0 0 :
I t e r a t i o n 3 9 0 0 :
I t e r a t i o n 4 0 0 0 :

l o s s =1840.82
l o s s =1836.87
l o s s =1833.16
l o s s =1829.65
l o s s =1826.34

We can display the image.

Code

d i s p l a y ( Image ( r e s u l t _ p r e f i x + " _ a t _ i t e r a t i o n _ 4 0 0 0 . png " ) )

Output

360

CHAPTER 9. TRANSFER LEARNING

We can download this image.

Code

from g o o g l e . c o l a b import f i l e s
f i l e s . download ( r e s u l t _ p r e f i x + " _ a t _ i t e r a t i o n _ 4 0 0 0 . png " )

Chapter 10

Time Series in Keras

10.1 Part 10.1: Time Series Data Encoding

There are many diﬀerent methods to encode data over time to a neural network.
In this chapter, we
will examine time series encoding and recurrent networks, two topics that are logical to put together
because they are both methods for dealing with data that spans over time. Time series encoding deals
with representing events that occur over time to a neural network. This encoding is necessary because a
feedforward neural network will always produce the same output vector for a given input vector. Recurrent
neural networks do not require encoding time series data because they can automatically handle data that
occur over time.

The variation in temperature during the week is an example of time-series data. For instance, if we know
that today’s temperature is 25 degrees Fahrenheit and tomorrow’s temperature is 27 degrees, the recurrent
neural networks and time series encoding provide another option to predict the correct temperature for
the week. Conversely, a traditional feedforward neural network will always respond with the same output
for a given input. If we train a feedforward neural network to predict tomorrow’s temperature, it should
return a value of 27 for 25. It will always output 27 when given 25 might hinder its predictions. Surely
the temperature of 27 will not always follow 25. It would be better for the neural network to consider the
temperatures for days before the prediction. Perhaps the temperature over the last week might allow us to
predict tomorrow’s temperature. Therefore, recurrent neural networks and time series encoding represent
two diﬀerent approaches to representing data over time to a neural network.

Previously we trained neural networks with input (x) and expected output (y). X was a matrix, the
rows were training examples, and the columns were values to be predicted. The x value will now contain
sequences of data. The deﬁnition of the y value will stay the same.

Dimensions of the training set (x):

• Axis 1: Training set elements (sequences) (must be of the same size as y size)
• Axis 2: Members of sequence
• Axis 3: Features in data (like input neurons)

Previously, we might take as input a single stock price to predict if we should buy (1), sell (-1), or hold
(0). The following code illustrates this encoding.

361

362

Code

x = [

[ 3 2 ] ,
[ 4 1 ] ,
[ 3 9 ] ,
[ 2 0 ] ,
[ 1 5 ]

]

y = [

1 ,
−1,
0 ,
−1,
1

]

print ( x )
print ( y )

Output

[ [ 3 2 ] ,
[ 1 , −1, 0 , −1, 1 ]

[ 4 1 ] ,

[ 3 9 ] ,

CHAPTER 10. TIME SERIES IN KERAS

[ 2 0 ] ,

[ 1 5 ] ]

The following code builds a CSV ﬁle from scratch. To see it as a data frame, use the following:

Code

from IPython . d i s p l a y import d i s p l a y , HTML
import pandas a s pd
import numpy a s np

x = np . a r r a y ( x )
print ( x [ : , 0 ] )

d f = pd . DataFrame ( { ' x ' : x [ : , 0 ] ,
d i s p l a y ( d f )

' y ' : y } )

Output

10.1. PART 10.1: TIME SERIES DATA ENCODING

363

x
32
41
39
20
15

y
1
-1
0
-1
1

0
1
2
3
4

[ 3 2 41 39 20 1 5 ]

You might want to put volume in with the stock price. The following code shows how to add a dimension

to handle the volume.

Code

x = [

[ 3 2 , 1 3 8 3 ] ,
[ 4 1 , 2 9 2 8 ] ,
[ 3 9 , 8 8 2 3 ] ,
[ 2 0 , 1 2 5 2 ] ,
[ 1 5 , 1 5 3 2 ]

]

y = [

1 ,
−1,
0 ,
−1,
1

]

print ( x )
print ( y )

Output

[ [ 3 2 , 1 3 8 3 ] ,
[ 1 , −1, 0 , −1, 1 ]

[ 4 1 , 2 9 2 8 ] ,

[ 3 9 , 8 8 2 3 ] ,

[ 2 0 , 1 2 5 2 ] ,

[ 1 5 , 1 5 3 2 ] ]

Again, very similar to what we did before. The following shows this as a data frame.

CHAPTER 10. TIME SERIES IN KERAS

364

Code

from IPython . d i s p l a y import d i s p l a y , HTML
import pandas a s pd
import numpy a s np

x = np . a r r a y ( x )
print ( x [ : , 0 ] )

d f = pd . DataFrame ( { ' p r i c e ' : x [ : , 0 ] ,
d i s p l a y ( d f )

' volume ' : x [ : , 1 ] ,

' y ' : y } )

Output

price
32
41
39
20
15

volume
1383
2928
8823
1252
1532

y
1
-1
0
-1
1

0
1
2
3
4

[ 3 2 41 39 20 1 5 ]

Now we get to sequence format. We want to predict something over a sequence, so the data format
needs to add a dimension. You must specify a maximum sequence length. The individual sequences can
be of any size.

Code

x = [

[ [ 3 2 , 1 3 8 3 ] , [ 4 1 , 2 9 2 8 ] , [ 3 9 , 8 8 2 3 ] , [ 2 0 , 1 2 5 2 ] , [ 1 5 , 1 5 3 2 ] ] ,
[ [ 3 5 , 8 2 7 2 ] , [ 3 2 , 1 3 8 3 ] , [ 4 1 , 2 9 2 8 ] , [ 3 9 , 8 8 2 3 ] , [ 2 0 , 1 2 5 2 ] ] ,
[ [ 3 7 , 2 7 3 8 ] , [ 3 5 , 8 2 7 2 ] , [ 3 2 , 1 3 8 3 ] , [ 4 1 , 2 9 2 8 ] , [ 3 9 , 8 8 2 3 ] ] ,
[ [ 3 4 , 2 8 4 5 ] , [ 3 7 , 2 7 3 8 ] , [ 3 5 , 8 2 7 2 ] , [ 3 2 , 1 3 8 3 ] , [ 4 1 , 2 9 2 8 ] ] ,
[ [ 3 2 , 2 3 4 5 ] , [ 3 4 , 2 8 4 5 ] , [ 3 7 , 2 7 3 8 ] , [ 3 5 , 8 2 7 2 ] , [ 3 2 , 1 3 8 3 ] ] ,

]

y = [

1 ,
−1,
0 ,
−1,

10.1. PART 10.1: TIME SERIES DATA ENCODING

365

1

]

print ( x )
print ( y )

Output

[ 3 2 , 1 3 8 3 ] ,

[ [ [ 3 2 , 1 3 8 3 ] ,
8 2 7 2 ] ,
[ 3 5 , 8 2 7 2 ] ,
2 7 3 8 ] ,
[ 3 7 , 2 7 3 8 ] ,
[ 1 , −1, 0 , −1, 1 ]

[ 3 5 , 8 2 7 2 ] ,

[ 4 1 , 2 9 2 8 ] ,

[ 3 9 , 8 8 2 3 ] ,

[ 2 0 , 1 2 5 2 ] ,

[ 1 5 , 1 5 3 2 ] ] ,

[ 4 1 , 2 9 2 8 ] ,

[ 3 9 , 8 8 2 3 ] ,

[ 2 0 , 1 2 5 2 ] ] ,

[ [ 3 5 ,
[ [ 3 7 , 2 7 3 8 ] ,

[ 3 2 , 1 3 8 3 ] ,

[ 4 1 , 2 9 2 8 ] ,

[ 3 9 , 8 8 2 3 ] ] ,

[ [ 3 4 , 2 8 4 5 ] ,

[ 3 7 ,

[ 3 2 , 1 3 8 3 ] ,

[ 4 1 , 2 9 2 8 ] ] ,

[ [ 3 2 , 2 3 4 5 ] ,

[ 3 4 , 2 8 4 5 ] ,

[ 3 5 , 8 2 7 2 ] ,

[ 3 2 , 1 3 8 3 ] ] ]

Even if there is only one feature (price), you must use 3 dimensions.

Code

x = [

[ [ 3 2 ] , [ 4 1 ] , [ 3 9 ] , [ 2 0 ] , [ 1 5 ] ] ,
[ [ 3 5 ] , [ 3 2 ] , [ 4 1 ] , [ 3 9 ] , [ 2 0 ] ] ,
[ [ 3 7 ] , [ 3 5 ] , [ 3 2 ] , [ 4 1 ] , [ 3 9 ] ] ,
[ [ 3 4 ] , [ 3 7 ] , [ 3 5 ] , [ 3 2 ] , [ 4 1 ] ] ,
[ [ 3 2 ] , [ 3 4 ] , [ 3 7 ] , [ 3 5 ] , [ 3 2 ] ] ,

]

y = [

1 ,
−1,
0 ,
−1,
1

]

print ( x )
print ( y )

Output

[ [ [ 3 2 ] ,
[ [ 3 7 ] ,

[ 4 1 ] ,

[ 3 9 ] ,

[ 2 0 ] ,

[ 1 5 ] ] ,

[ [ 3 5 ] ,

[ 3 2 ] ,

[ 4 1 ] ,

[ 3 9 ] ,

[ 2 0 ] ] ,

[ 3 5 ] ,

[ 3 2 ] ,

[ 4 1 ] ,

[ 3 9 ] ] ,

[ [ 3 4 ] ,

[ 3 7 ] ,

[ 3 5 ] ,

[ 3 2 ] ,

[ 4 1 ] ] ,

[ [ 3 2 ] ,

366

CHAPTER 10. TIME SERIES IN KERAS

[ 3 4 ] ,
[ 3 5 ] ,
[ 3 7 ] ,
[ 1 , −1, 0 , −1, 1 ]

[ 3 2 ] ] ]

10.1.1 Module 10 Assignment

You can ﬁnd the ﬁrst assignment here: assignment 10

10.2 Part 10.2: Programming LSTM with Keras and TensorFlow

So far, the neural networks that we’ve examined have always had forward connections. Neural networks of
this type always begin with an input layer connected to the ﬁrst hidden layer. Each hidden layer always
connects to the next hidden layer. The ﬁnal hidden layer always connects to the output layer. This manner
of connection is why these networks are called "feedforward." Recurrent neural networks are not as rigid,
as backward linkages are also allowed. A recurrent connection links a neuron in a layer to either a previous
layer or the neuron itself. Most recurrent neural network architectures maintain the state in the recurrent
connections. Feedforward neural networks don’t keep any state.

10.2.1 Understanding LSTM

Long Short Term Memory (LSTM) layers are a type of recurrent unit that you often use with deep neural
networks.[13]For TensorFlow, you can think of LSTM as a layer type that you can combine with other
layer types, such as dense. LSTM makes use of two transfer function types internally.

The ﬁrst type of transfer function is the sigmoid. This transfer function type is used form gates inside

of the unit. The sigmoid transfer function is given by the following equation:

S(t) =

1
1 + e−t

The second type of transfer function is the hyperbolic tangent (tanh) function, which allows you to
scale the output of the LSTM. This functionality is similar to how we have used other transfer functions
in this course.

We provide the graphs for these functions here:

Code

%m a t p l o t l i b i n l i n e

import m a t p l o t l i b
import numpy a s np
import m a t p l o t l i b . p y p l o t a s p l t
import math

10.2. PART 10.2: PROGRAMMING LSTM WITH KERAS AND TENSORFLOW

367

def s i g m o i d ( x ) :
a = [ ]
for item in x :

a . append (1/(1+ math . exp(−item ) ) )

return a

def

f 2 ( x ) :
a = [ ]
for item in x :

a . append ( math . tanh ( item ) )

return a

x = np . a r a n g e ( −10. , 1 0 . , 0 . 2 )
y1 = s i g m o i d ( x )
y2 = f 2 ( x )

print ( " Sigmoid " )
p l t . p l o t ( x , y1 )
p l t . show ( )

print ( " H y p e r b o l i c ␣ Tangent ( tanh ) " )
p l t . p l o t ( x , y2 )
p l t . show ( )

Output

Sigmoid

368

CHAPTER 10. TIME SERIES IN KERAS

H y p e r b o l i c Tangent ( tanh )

Both of these two functions compress their output to a speciﬁc range. For the sigmoid function, this

range is 0 to 1. For the hyperbolic tangent function, this range is -1 to 1.

LSTM maintains an internal state and produces an output. The following diagram shows an LSTM
unit over three timeslices: the current time slice (t), as well as the previous (t-1) and next (t+1) slice, as
demonstrated by Figure 10.1.

Figure 10.1: LSTM Layers

10.2. PART 10.2: PROGRAMMING LSTM WITH KERAS AND TENSORFLOW

369

The values ˆy are the output from the unit; the values (x) are the input to the unit, and the values c
are the context values. The output and context values always feed their output to the next time slice. The
context values allow the network to maintain the state between calls. Figure 10.2 shows the internals of a
LSTM layer.

Figure 10.2: Inside a LSTM Layer

A LSTM unit consists of three gates:

• Forget Gate (ft) - Controls if/when the context is forgotten. (MC)
• Input Gate (it) - Controls if/when the context should remember a value. (M+/MS)
• Output Gate (ot) - Controls if/when the remembered value is allowed to pass from the unit. (RM)

370

CHAPTER 10. TIME SERIES IN KERAS

10.2.2 Simple Keras LSTM Example

The following code creates the LSTM network, an example of an RNN for classiﬁcation. The following
code trains on a data set (x) with a max sequence size of 6 (columns) and six training elements (rows)

Code

from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g import s e q u e n c e
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , Embedding
from t e n s o r f l o w . k e r a s . l a y e r s import LSTM
import numpy a s np

max_features = 4 # 0 , 1 , 2 , 3 ( t o t a l o f 4)
x = [

[ [ 0 ] , [ 1 ] , [ 1 ] , [ 0 ] , [ 0 ] , [ 0 ] ] ,
[ [ 0 ] , [ 0 ] , [ 0 ] , [ 2 ] , [ 2 ] , [ 0 ] ] ,
[ [ 0 ] , [ 0 ] , [ 0 ] , [ 0 ] , [ 3 ] , [ 3 ] ] ,
[ [ 0 ] , [ 2 ] , [ 2 ] , [ 0 ] , [ 0 ] , [ 0 ] ] ,
[ [ 0 ] , [ 0 ] , [ 3 ] , [ 3 ] , [ 0 ] , [ 0 ] ] ,
[ [ 0 ] , [ 0 ] , [ 0 ] , [ 0 ] , [ 1 ] , [ 1 ] ]

]
x = np . a r r a y ( x , dtype=np . f l o a t 3 2 )
y = np . a r r a y ( [ 1 , 2 , 3 , 2 , 3 , 1 ] , dtype=np . i n t 3 2 )

# Convert y2 t o dummy v a r i a b l e s
y2 = np . z e r o s ( ( y . shape [ 0 ] , max_features ) , dtype=np . f l o a t 3 2 )
y2 [ np . a r a n g e ( y . shape [ 0 ] ) , y ] = 1 . 0
print ( y2 )

print ( ' B u i l d ␣ model . . . ' )
model = S e q u e n t i a l ( )
model . add (LSTM( 1 2 8 , dropout =0.2 ,

r e c u r r e n t _ d r o p o u t =0.2 , \

model . add ( Dense ( 4 , a c t i v a t i o n= ' s i g m o i d ' ) )

input_shape=(None , 1 ) ) )

# t r y u s i n g d i f f e r e n t o p t i m i z e r s and d i f f e r e n t o p t i m i z e r c o n f i g s
model . compile ( l o s s= ' b i n a r y _ c r o s s e n t r o p y ' ,

o p t i m i z e r= ' adam ' ,
m e t r i c s =[ ' a c c u r a c y ' ] )

print ( ' Train . . . ' )
model . f i t ( x , y2 , ep o c hs =200)
pred = model . p r e d i c t ( x )

10.2. PART 10.2: PROGRAMMING LSTM WITH KERAS AND TENSORFLOW

371

p r e d i c t _ c l a s s e s = np . argmax ( pred , a x i s =1)
print ( " P r e d i c t e d ␣ c l a s s e s : ␣ {} " , p r e d i c t _ c l a s s e s )
print ( " Expected ␣ c l a s s e s : ␣ {} " , p r e d i c t _ c l a s s e s )

Output

[ [ 0 . 1 . 0 . 0 . ]
[ 0 . 0 . 1 . 0 . ]
[ 0 . 0 . 0 . 1 . ]
[ 0 . 0 . 1 . 0 . ]
[ 0 . 0 . 0 . 1 . ]
[ 0 . 1 . 0 . 0 . ] ]

B u i l d model . . .
Train . . .
. . .
1/1 [==============================] − 0 s 66ms/ s t e p − l o s s : 0 . 2 6 2 2 −
a c c u r a c y : 0 . 6 6 6 7
Epoch 200/200
1/1 [==============================] − 0 s 39ms/ s t e p − l o s s : 0 . 2 3 2 9 −
a c c u r a c y : 0 . 6 6 6 7
P r e d i c t e d c l a s s e s : {} [ 1 2 3 2 3 1 ]
Expected c l a s s e s : {} [ 1 2 3 2 3 1 ]

We can now present a sequence directly to the model for classiﬁcation.

Code

def

i n p ) :

r u n i t ( model ,
i n p = np . a r r a y ( inp , dtype=np . f l o a t 3 2 )
pred = model . p r e d i c t ( i n p )
return np . argmax ( pred [ 0 ] )

print (

r u n i t ( model ,

[ [ [ 0 ] , [ 0 ] , [ 0 ] , [ 0 ] , [ 0 ] , [ 1 ] ] ]

) )

Output

1

372

CHAPTER 10. TIME SERIES IN KERAS

10.2.3 Sun Spots Example

This section shows an example of RNN regression to predict sunspots. You can ﬁnd the data ﬁles needed
for this example at the following location.

• Sunspot Data Files
• Download Daily Sunspots - 1/1/1818 to now.

The following code loads the sunspot ﬁle:

Code

import pandas a s pd
import o s

names = [ ' y e a r ' ,

' month ' ,

' day ' ,

' dec_year ' ,

' sn_value '

,

' s n _ e r r o r ' ,

' obs_num ' ,

' unused1 ' ]

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/SN_d_tot_V2 . 0 . c s v " ,
s e p= ' ; ' , h e a d e r=None , names=names ,
na_values =[ '−1 ' ] ,

i n d e x _ c o l=F a l s e )

print ( " S t a r t i n g ␣ f i l e : " )
print ( d f [ 0 : 1 0 ] )

print ( " Ending ␣ f i l e : " )
print ( d f [ − 1 0 : ] )

Output

S t a r t i n g f i l e :
y e a r month
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818

0
1
2
3
4
5
6
7
8
9
Ending f i l e :

day
1
2
3
4
5
6
7
8
9
10

dec_year
1 8 1 8 . 0 0 1
1 8 1 8 . 0 0 4
1 8 1 8 . 0 0 7
1 8 1 8 . 0 1 0
1 8 1 8 . 0 1 2
1 8 1 8 . 0 1 5
1 8 1 8 . 0 1 8
1 8 1 8 . 0 2 1
1 8 1 8 . 0 2 3
1 8 1 8 . 0 2 6

sn_value
−1
−1
−1
−1
−1
−1
−1
65
−1
−1

s n _ e r r o r
NaN
NaN
NaN
NaN
NaN
NaN
NaN
1 0 . 2
NaN
NaN

obs_num unused1
1
1
1
1
1
1
1
1
1
1

0
0
0
0
0
0
0
1
0
0

y e a r month

day

dec_year

sn_value

s n _ e r r o r

obs_num

unused1

10.2. PART 10.2: PROGRAMMING LSTM WITH KERAS AND TENSORFLOW

373

. . .

0
72863
0
72864
0

2017

2017

6

6

29

2 0 1 7 . 4 9 2

30

2 0 1 7 . 4 9 5

12

11

0 . 5

0 . 5

25

30

As you can see, there is quite a bit of missing data near the end of the ﬁle. We want to ﬁnd the starting
index where the missing data no longer occurs. This technique is somewhat sloppy; it would be better to
ﬁnd a use for the data between missing values. However, the point of this example is to show how to use
LSTM with a somewhat simple time-series.

Code

s t a r t _ i d = max( d f [ d f [ ' obs_num ' ] == 0 ] . i n d e x . t o l i s t ())+1
# Find t h e l a s t
z e r o and move one beyond
print ( s t a r t _ i d )
d f = d f [ s t a r t _ i d : ] # Trim t h e rows

t h a t have m i s s i n g o b s e r v a t i o n s

Output

11314

Code

d f [ ' sn_value ' ] = d f [ ' sn_value ' ] . a s t y p e ( f l o a t )
d f _ t r a i n = d f [ d f [ ' y e a r ' ] <2000]
d f _ t e s t = d f [ d f [ ' y e a r ' ] >=2000]

s p o t s _ t r a i n = d f _ t r a i n [ ' sn_value ' ] . t o l i s t ( )
s p o t s _ t e s t = d f _ t e s t [ ' sn_value ' ] . t o l i s t ( )

print ( " T r a i n i n g ␣ s e t ␣ has ␣ {} ␣ o b s e r v a t i o n s . " . format ( len ( s p o t s _ t r a i n ) ) )
print ( " Test ␣ s e t ␣ has ␣ {} ␣ o b s e r v a t i o n s . " . format ( len ( s p o t s _ t e s t ) ) )

Output

T r a i n i n g s e t has 55160 o b s e r v a t i o n s .
s e t has 6391 o b s e r v a t i o n s .
Test

374

CHAPTER 10. TIME SERIES IN KERAS

To create an algorithm that will predict future values, we need to consider how to encode this data to
be presented to the algorithm. The data must be submitted as sequences, using a sliding window algorithm
to encode the data. We must deﬁne how large the window will be. Consider an n-sized window. Each
sequence’s x values will be a n data points sequence. The y’s will be the next value, after the sequence, that
we are trying to predict. You can use the following function to take a series of values, such as sunspots,
and generate sequences (x) and predicted values (y).

Code

import numpy a s np

def t o _ s e q u e n c e s ( s e q _ s i z e , obs ) :

x = [ ]
y = [ ]

for i

in range ( len ( obs)−SEQUENCE_SIZE ) :

#p r i n t ( i )
window = obs [ i : ( i+SEQUENCE_SIZE ) ]
after_window = obs [ i+SEQUENCE_SIZE ]
for x in window ]
window = [ [ x ]
#p r i n t ( " { } − { } " . format ( window , after_window ) )
x . append ( window )
y . append ( after_window )

return np . a r r a y ( x ) , np . a r r a y ( y )

SEQUENCE_SIZE = 10
x_train , y _ t r a i n = t o _ s e q u e n c e s (SEQUENCE_SIZE, s p o t s _ t r a i n )
x_test , y _ t e s t = t o _ s e q u e n c e s (SEQUENCE_SIZE, s p o t s _ t e s t )

print ( " Shape ␣ o f ␣ t r a i n i n g ␣ s e t : ␣ {} " . format ( x _ t r a i n . shape ) )
print ( " Shape ␣ o f ␣ t e s t ␣ s e t : ␣ {} " . format ( x _ t es t . shape ) )

Output

Shape o f
Shape o f

t r a i n i n g s e t :
s e t :
t e s t

( 6 3 8 1 , 1 0 , 1 )

( 5 5 1 5 0 , 1 0 , 1 )

We can see the internal structure of the training data. The ﬁrst dimension is the number of training
elements, the second indicates a sequence size of 10, and ﬁnally, we have one data point per timeslice in
the window.

10.2. PART 10.2: PROGRAMMING LSTM WITH KERAS AND TENSORFLOW

375

Code

x _ t r a i n . shape

Output

( 5 5 1 5 0 , 1 0 , 1 )

We are now ready to build and train the model.

Code

from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g import s e q u e n c e
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , Embedding
from t e n s o r f l o w . k e r a s . l a y e r s import LSTM
from t e n s o r f l o w . k e r a s . d a t a s e t s import imdb
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import numpy a s np

print ( ' B u i l d ␣ model . . . ' )
model = S e q u e n t i a l ( )
model . add (LSTM( 6 4 , dropout =0.0 ,

r e c u r r e n t _ d r o p o u t =0.0 ,\

input_shape=(None , 1 ) ) )

model . add ( Dense ( 3 2 ) )
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5,
v e r b o s e =1, mode= ' auto ' ,

r e s t o r e _ b e s t _ w e i g h t s=True )

print ( ' Train . . . ' )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

Output

B u i l d model . . .
Train . . .
. . .
1724/1724 − 10 s − l o s s : 4 9 7 . 0 3 9 3 − v a l _ l o s s : 2 1 5 . 1 7 2 1 − 10 s / epoch −
6ms/ s t e p
Epoch 11/1000

376

CHAPTER 10. TIME SERIES IN KERAS

R e s t o r i n g model w e i g h t s from t h e end o f
1724/1724 − 10 s − l o s s : 4 9 5 . 1 9 2 0 − v a l _ l o s s : 2 2 0 . 1 8 2 6 − 10 s / epoch −
6ms/ s t e p
Epoch 1 1 : e a r l y s t o p p i n g

t h e b e s t epoch : 6 .

Finally, we evaluate the model with RMSE.

Code

from s k l e a r n import m e t r i c s

pred = model . p r e d i c t ( x _t e s t )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

10.3 Part 10.3: Text Generation with LSTM

Recurrent neural networks are also known for their ability to generate text. As a result, the neural network’s
output can be free-form text. This section will demonstrate how to train an LSTM on a textual document,
such as classic literature, and learn to output new text that appears to be of the same form as the training
material. If you train your LSTM on Shakespeare, it will learn to crank out new prose similar to what
Shakespeare had written.

Don’t get your hopes up. You will not teach your deep neural network to write the next Pulitzer Prize
for Fiction. The prose generated by your neural network will be nonsensical. However, the output text
will usually be nearly grammatically correct and similar to the source training documents.

A neural network generating nonsensical text based on literature may not seem helpful. However, this
technology gets so much interest because it forms the foundation for many more advanced technologies. The
LSTM will typically learn human grammar from the source document opens a wide range of possibilities.
You can use similar technology to complete sentences when entering text. The ability to output free-form
text has become the foundation of many other technologies. In the next part, we will use this technique
to create a neural network that can write captions for images to describe what is going on in the picture.

10.3.1 Additional Information

The following are some articles that I found helpful in putting this section together.

• The Unreasonable Eﬀectiveness of Recurrent Neural Networks
• Keras LSTM Generation Example

10.3.2 Character-Level Text Generation

There are several diﬀerent approaches to teaching a neural network to output free-form text. The most
basic question is if you wish the neural network to learn at the word or character level. Learning at the

10.3. PART 10.3: TEXT GENERATION WITH LSTM

377

character level is the more interesting of the two. The LSTM is learning to construct its own words without
even being shown what a word is. We will begin with character-level text generation. In the next module,
we will see how we can use nearly the same technique to operate at the word level. We will implement
word-level automatic captioning in the next module.

We import the needed Python packages and deﬁne the sequence length, named maxlen. Time-series
neural networks always accept their input as a ﬁxed-length array. Because you might not use all of the
sequence elements, ﬁlling extra pieces with zeros is common. You will divide the text into sequences of
this length, and the neural network will train to predict what comes after this sequence.

Code

from t e n s o r f l o w . k e r a s . c a l l b a c k s import LambdaCallback
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense
from t e n s o r f l o w . k e r a s . l a y e r s import LSTM
from t e n s o r f l o w . k e r a s . o p t i m i z e r s import RMSprop
from t e n s o r f l o w . k e r a s . u t i l s
import numpy a s np
import random
import s y s
import i o
import r e q u e s t s
import r e

import g e t _ f i l e

We will train the neural network on the classic children’s book Treasure Island. We begin by loading

this text into a Python string and displaying the ﬁrst 1,000 characters.

Code

r = r e q u e s t s . g e t ( " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ t e x t / " \

" t r e a s u r e _ i s l a n d . t x t " )

raw_text = r . t e x t
print ( raw_text [ 0 : 1 0 0 0 ] )

Output

f o r

The P r o j e c t Gutenberg EBook o f T r e a s u r e I s l a n d , by Robert L o u i s
S t e v e n s o n
This eBook i s
a l mo s t no r e s t r i c t i o n s w h a t s o e v e r . You may copy i t , g i v e
re−u s e i t under
with t h i s eBook o r o n l i n e a t www. g u t e n b e r g . n e t
T i t l e : T r e a s u r e I s l a n d
Author : Robert L o u i s S t e v e n s o n

t h e u s e o f anyone anywhere a t no c o s t and with

t h e terms o f

t h e P r o j e c t Gutenberg L i c e n s e i n c l u d e d

i t away o r

378

CHAPTER 10. TIME SERIES IN KERAS

I l l u s t r a t o r : Milo Winter
R e l e a s e Date : January 1 2 , 2009 [ EBook #27780]
Language : E n g l i s h
∗∗∗ START OF THIS PROJECT GUTENBERG EBOOK TREASURE ISLAND ∗∗∗
Produced by J u l i e t S u t h e r l a n d , Stephen B l u n d e l l and t h e
O n l i n e D i s t r i b u t e d P r o o f r e a d i n g Team a t h t t p : / /www. pgdp . n e t
THE ILLUSTRATED CHILDREN' S LIBRARY

. . .

Milo Winter
[ I l l u s t r a t i o n ]
GRAMERCY BOOKS
NEW YORK

Foreword c o p y r i g h t

1986 by Random House V

We will extract all unique characters from the text and sort them. This technique allows us to assign a
unique ID to each character. Because we sorted the characters, these IDs should remain the same. The IDs
will change if we add new characters to the original text. We build two dictionaries. The ﬁrst char2idx
is used to convert a character into its ID. The second idx2char converts an ID back into its character.

Code

p r o c e s s e d _ t e x t = raw_text . l o w e r ( )
p r o c e s s e d _ t e x t = r e . sub ( r ' [ ^ \ x00−\x 7 f ] ' , r ' ' , p r o c e s s e d _ t e x t )

print ( ' c o r p u s ␣ l e n g t h : ' ,

len ( p r o c e s s e d _ t e x t ) )

c h a r s = sorted ( l i s t ( set ( p r o c e s s e d _ t e x t ) ) )
print ( ' t o t a l ␣ c h a r s : ' ,
i )
c h a r _ i n d i c e s = dict ( ( c ,
i n d i c e s _ c h a r = dict ( ( i , c )

len ( c h a r s ) )

for i , c in enumerate ( c h a r s ) )
for i , c in enumerate ( c h a r s ) )

Output

c o r p u s
t o t a l c h a r s : 60

l e n g t h : 397400

We are now ready to build the actual sequences. Like previous neural networks, there will be an x and
y. However, for the LSTM, x and y will be sequences. The x input will specify the sequences where y is
the expected output. The following code generates all possible sequences.

10.3. PART 10.3: TEXT GENERATION WITH LSTM

379

Code

i n semi−redundant

s e q u e n c e s o f maxlen c h a r a c t e r s

t h e t e x t

# c u t
maxlen = 40
s t e p = 3
s e n t e n c e s = [ ]
next_chars = [ ]
for i

in range ( 0 ,

len ( p r o c e s s e d _ t e x t ) − maxlen ,

s t e p ) :

s e n t e n c e s . append ( p r o c e s s e d _ t e x t [ i :
next_chars . append ( p r o c e s s e d _ t e x t [ i + maxlen ] )

i + maxlen ] )

print ( ' nb␣ s e q u e n c e s : ' ,

len ( s e n t e n c e s ) )

Output

nb s e q u e n c e s : 132454

Code

s e n t e n c e s

Output

t r e a s u r e

t r e a s u r e

[ ' t h e p r o j e c t g u t e n b e r g ebook o f

' ,
i s l ' ,
t r e a s u r e i s l a n d ' ,
t r e a s u r e i s l a n d , b ' ,
t r e a s u r e i s l a n d , by r ' ,
t r e a s u r e i s l a n d , by robe ' ,
' ,
lou ' ,
' ,
s t e ' ,
s t e v e n ' ,
s t e v e n s o n ' ,
s t e v e n s o n \ r \n\ r ' ,

' p r o j e c t g u t e n b e r g ebook o f
' o j e c t g u t e n b e r g ebook o f
' c t g u t e n b e r g ebook o f
' g u t e n b e r g ebook o f
' e n b e r g ebook o f
' e r g ebook o f
' ebook o f
' ook o f
' o f
'
' e a s u r e i s l a n d , by r o b e r t
' u r e i s l a n d , by r o b e r t
'
i s l a n d , by r o b e r t
' land , by r o b e r t

s t e v e n s o n \ r \n\ r \ nth ' ,
' ,

t r e a s u r e i s l a n d , by r o b e r t

t r e a s u r e i s l a n d , by r o b e r t

t r e a s u r e i s l a n d , by r o b e r t

t r e a s u r e i s l a n d , by r o b e r t

t r e a s u r e i s l a n d , by r o b e r t

s t e v e n s o n \ r \n\ r \ n t h i s

l o u i s

l o u i s

l o u i s

l o u i s

l o u i s

l o u i s

l o u i s

. . .

' s t o f c o l o r p l a t e s _

' ,

380

CHAPTER 10. TIME SERIES IN KERAS

' o f c o l o r p l a t e s _
' c o l o r p l a t e s _
' o r p l a t e s _
. . . ]

' ,
' ,
' ,

We can now convert the text into vectors.

Code

print ( ' V e c t o r i z a t i o n . . . ' )
x = np . z e r o s ( ( len ( s e n t e n c e s ) , maxlen ,
y = np . z e r o s ( ( len ( s e n t e n c e s ) ,
for i ,

s e n t e n c e in enumerate ( s e n t e n c e s ) :

len ( c h a r s ) ) , dtype=np . bool )

len ( c h a r s ) ) , dtype=np . bool )

for t , c h a r in enumerate ( s e n t e n c e ) :
t , c h a r _ i n d i c e s [ c h a r ] ] = 1

x [ i ,

y [ i , c h a r _ i n d i c e s [ next_chars [ i ] ] ] = 1

Output

V e c t o r i z a t i o n . . .

Next, we create the neural network. This neural network’s primary feature is the LSTM layer, which

allows the sequences to be processed.

Code

# b u i l d t h e model : a s i n g l e LSTM
print ( ' B u i l d ␣ model . . . ' )
model = S e q u e n t i a l ( )
model . add (LSTM( 1 2 8 ,
model . add ( Dense ( len ( c h a r s ) , a c t i v a t i o n= ' softmax ' ) )

input_shape=(maxlen ,

len ( c h a r s ) ) ) )

o p t i m i z e r = RMSprop ( l r =0.01)
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r=o p t i m i z e r )

Output

B u i l d model . . .

10.3. PART 10.3: TEXT GENERATION WITH LSTM

381

Code

model . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

l s t m (LSTM)
d e n s e ( Dense )

( None , 1 2 8 )
( None , 6 0 )

96768
7740

=================================================================
T o t a l params : 1 0 4 , 5 0 8
T r a i n a b l e params : 1 0 4 , 5 0 8
Non−t r a i n a b l e params : 0
_________________________________________________________________

The LSTM will produce new text character by character. We will need to sample the correct letter

from the LSTM predictions each time. The sample function accepts the following two parameters:

• preds - The output neurons.
• temperature - 1.0 is the most conservative, 0.0 is the most conﬁdent (willing to make spelling and

other errors).

The sample function below essentially performs a softmax on the neural network predictions. This process
causes each output neuron to become a probability of its particular letter.

Code

f u n c t i o n t o sample an i n d e x from a p r o b a b i l i t y a r r a y

t e m p e r a t u r e = 1 . 0 ) :

def sample ( preds ,
# h e l p e r
p r e d s = np . a s a r r a y ( p r e d s ) . a s t y p e ( ' f l o a t 6 4 ' )
p r e d s = np . l o g ( p r e d s ) / t e m p e r a t u r e
exp_preds = np . exp ( p r e d s )
p r e d s = exp_preds / np .sum( exp_preds )
p r ob a s = np . random . m u l t i n o m i a l ( 1 , preds , 1 )
return np . argmax ( p rob as )

Keras calls the following function at the end of each training Epoch. The code generates sample text
generations that visually demonstrate the neural network better at text generation. As the neural network
trains, the generations should look more realistic.

382

Code

CHAPTER 10. TIME SERIES IN KERAS

def on_epoch_end ( epoch , _ ) :

# Function i n v o k e d a t end o f each epoch . P r i n t s g e n e r a t e d t e x t .
print ( " ∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗ " )
print ( '−−−−−␣ G e n e r a t i n g ␣ t e x t ␣ a f t e r ␣Epoch : ␣%d ' % epoch )

s t a r t _ i n d e x = random . r a n d i n t ( 0 ,
for t e m p e r a t u r e in [ 0 . 2 , 0 . 5 , 1 . 0 , 1 . 2 ] :

len ( p r o c e s s e d _ t e x t ) − maxlen − 1 )

print ( '−−−−−␣ t e m p e r a t u r e : ' ,

t e m p e r a t u r e )

g e n e r a t e d = ' '
s e n t e n c e = p r o c e s s e d _ t e x t [ s t a r t _ i n d e x :
g e n e r a t e d += s e n t e n c e
print ( '−−−−−␣ G e n e r a t i n g ␣ with ␣ s e e d : ␣ " ' + s e n t e n c e + ' " ' )
s y s . s t d o u t . w r i t e ( g e n e r a t e d )

s t a r t _ i n d e x + maxlen ]

for i

in range ( 4 0 0 ) :

x_pred = np . z e r o s ( ( 1 , maxlen ,
for t , c h a r in enumerate ( s e n t e n c e ) :

len ( c h a r s ) ) )

x_pred [ 0 ,

t , c h a r _ i n d i c e s [ c h a r ] ] = 1 .

p r e d s = model . p r e d i c t ( x_pred , v e r b o s e = 0 ) [ 0 ]
next_index = sample ( preds ,
next_char = i n d i c e s _ c h a r [ next_index ]

t e m p e r a t u r e )

g e n e r a t e d += next_char
s e n t e n c e = s e n t e n c e [ 1 : ] + next_char

s y s . s t d o u t . w r i t e ( next_char )
s y s . s t d o u t . f l u s h ( )

print ( )

We are now ready to train. Depending on how fast your computer is, it can take up to an hour to train

this network. If you have a GPU available, please make sure to use it.

Code

# I g n o r e u s e l e s s W0819 war nin gs g e n e r a t e d by TensorFlow 2 . 0 .
H o p e f u l l y can remove t h i s
# See h t t p s : / / g i t h u b . com/ t e n s o r f l o w / t e n s o r f l o w / i s s u e s /31308
import l o g g i n g , o s
l o g g i n g . d i s a b l e ( l o g g i n g .WARNING)
o s . e n v i r o n [ "TF_CPP_MIN_LOG_LEVEL" ] = " 3 "

i g n o r e i n t h e f u t u r e .

10.4. PART 10.4: INTRODUCTION TO TRANSFORMERS

383

t h e model

# F i t
p r i n t _ c a l l b a c k = LambdaCallback ( on_epoch_end=on_epoch_end )

model . f i t ( x , y ,

b a t c h _ s i z e =128 ,
e po c h s =60 ,
c a l l b a c k s =[ p r i n t _ c a l l b a c k ] )

Output

. . .
1035/1035 [==============================] − 74 s 71ms/ s t e p − l o s s :
1 . 1 3 6 1
Epoch 60/60
1029/1035 [============================>.] − ETA: 0 s − l o s s :
1 . 1 3 3 9 ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗ ∗
−−−−− G e n e r a t i n g t e x t a f t e r Epoch : 59
−−−−− t e m p e r a t u r e : 0 . 2
−−−−− G e n e r a t i n g with s e e d :
t a i l o f
" i t ' s a don ' t be men b e l i e f
blows and t h e s t o c k a d e , and a man o f
c a p t a i n . he was a p a i r s and seemed t o t h e barrows and t h e part , and i
saw he was a s t a t e b e f o r e t h e p i r a t e s o f h i s
−−−−− t e m p e r a t u r e : 0 . 5

i n my t i l l be c a p t a i n s i l v e r had been t h e

t h e b o a t s was p l a c e and t h e

i t on h i s u n r u l y f o l l o w e r s .

th "

s

"

. . .

i t on h i s u n r u l y f o l l o w e r s .

−−−−− G e n e r a t i n g with s e e d :
t a i l o f
l e v o r , a s
i c o u l d now s h e e ehe me s o come k i n t and s o s o
1035/1035 [==============================] − 74 s 72ms/ s t e p − l o s s :
1 . 1 3 3 9

th "

"

10.4 Part 10.4: Introduction to Transformers

Transformers are neural networks that provide state-of-the-art solutions for many of the problems previously
assigned to recurrent neural networks.[35]Sequences can form both the input and the output of a neural
network, examples of such conﬁgurations include::

384

CHAPTER 10. TIME SERIES IN KERAS

• Vector to Sequence - Image captioning
• Sequence to Vector - Sentiment analysis
• Sequence to Sequence - Language translation

Sequence-to-sequence allows an input sequence to produce an output sequence based on an input sequence.
Transformers focus primarily on this sequence-to-sequence conﬁguration.

10.4.1 High-Level Overview of Transformers

This course focuses primarily on the application of deep neural networks. The focus will be on presenting
data to a transformer and a transformer’s major components. As a result, we will not focus on implementing
a transformer at the lowest level. The following section provides an overview of critical internal parts of
a transformer, such as residual connections and attention. In the next chapter, we will use transformers
from Hugging Face to perform natural language processing with transformers.
If you are interested in
implementing a transformer from scratch, Keras provides a comprehensive example.
Figure 10.3 presents a high-level view of a transformer for language translation.

Figure 10.3: High Level View of a Translation Transformer

We use a transformer that translates between English and Spanish for this example. We present the

English sentence "the cat likes milk" and receive a Spanish translation of "al gato le gusta la leche."

We begin by placing the English source sentence between the beginning and ending tokens. This input
can be of any length, and we presented it to the neural network as a ragged Tensor. Because the Tensor is
ragged, no padding is necessary. Such input is acceptable for the attention layer that will receive the source

10.4. PART 10.4: INTRODUCTION TO TRANSFORMERS

385

sentence. The encoder transforms this ragged input into a hidden state containing a series of key-value
pairs representing the knowledge in the source sentence. The encoder understands to read English and
convert to a hidden state. The decoder understands how to output Spanish from this hidden state.

We initially present the decoder with the hidden state and the starting token. The decoder will predict
the probabilities of all words in its vocabulary. The word with the highest probability is the ﬁrst word of
the sentence.

The highest probability word is attached concatenated to the translated sentence, initially containing
only the beginning token. This process continues, growing the translated sentence in each iteration until
the decoder predicts the ending token.

10.4.2 Transformer Hyperparameters

Before we describe how these layers ﬁt together, we must consider the following transformer hyperparam-
eters, along with default settings from the Keras transformer example:

• num_layers = 4
• d_model = 128
• dﬀ = 512
• num_heads = 8
• dropout_rate = 0.1

Multiple encoder and decoder layers can be present. The num_layers hyperparameter speciﬁes how
many encoder and decoder layers there are. The expected tensor shape for the input to the encoder layer
is the same as the output produced; as a result, you can easily stack these layers.

We will see embedding layers in the next chapter. However, you can think of an embedding layer as a
dictionary for now. Each entry in the embedding corresponds to each word in a ﬁxed-size vocabulary. Sim-
ilar words should have similar vectors. The d_model hyperparameter speciﬁes the size of the embedding
vector. Though you will sometimes preload embeddings from a project such as Word2vec or GloVe, the
optimizer can train these embeddings with the rest of the transformer. Training your embeddings allows
the d_model hyperparameter to set to any desired value. If you transfer the embeddings, you must set
the d_model hyperparameter to the same value as the transferred embeddings.

The dﬀ hyperparameter speciﬁes the size of the dense feedforward layers. The num_heads hyperpa-
rameter sets the number of attention layers heads. Finally, the dropout_rate speciﬁes a dropout percentage
to combat overﬁtting. We discussed dropout previously in this book.

10.4.3 Inside a Transformer

In this section, we will examine the internals of a transformer so that you become familiar with essential
concepts such as:

• Embeddings
• Positional Encoding
• Attention and Self-Attention
• Residual Connection

386

CHAPTER 10. TIME SERIES IN KERAS

You can see a lower-level diagram of a transformer in Figure 10.4.

While the original transformer paper is titled "Attention is All you Need," attention isn’t the only layer
type you need. The transformer also contains dense layers. However, the title "Attention and Dense Layers
are All You Need" isn’t as catchy.

The transformer begins by tokenizing the input English sentence. Tokens may or may not be words.
Generally, familiar parts of words are tokenized and become building blocks of longer words. This tok-
enization allows common suﬃxes and preﬁxes to be understood independently of their stem word. Each
token becomes a numeric index that the transformer uses to look up the vector. There are several special
tokens:

• Index 0 = Pad
• Index 1 = Unknow
• Index 2 = Start token
• Index 3 = End token

The transformer uses index 0 when we must pad unused space at the end of a tensor.
unknown words. The starting and ending tokens are provided by indexes 2 and 3.

Index 1 is for

The token vectors are simply the inputs to the attention layers; there is no implied order or position.

The transformer adds the slopes of a sine and cosine wave to the token vectors to encode position.

Attention layers have three inputs: key (k), value(v), and query (q). This layer is self-attention if the
query, key, and value are the same. The key and value pairs specify the information that the query operates
upon. The attention layer learns what positions of data to focus upon.

The transformer presents the position encoded embedding vectors to the ﬁrst self-attention segment in
the encoder layer. The output from the attention is normalized and ultimately becomes the hidden state
after all encoder layers are processed.

The hidden state is only calculated once per query. Once the input Spanish sentence becomes a hidden
state, this value is presented repeatedly to the decoder until the decoder forms the ﬁnal Spanish sentence.
This section presented a high-level introduction to transformers. In the next part, we will implement
the encoder and apply it to time series. In the following chapter, we will use Hugging Face transformers
to perform natural language processing.

10.5 Part 10.5: Programming Transformers with Keras

This section shows an example of a transformer encoder to predict sunspots. You can ﬁnd the data ﬁles
needed for this example at the following location.

• Sunspot Data Files
• Download Daily Sunspots - 1/1/1818 to now.

The following code loads the sunspot ﬁle:

Code

import pandas a s pd
import o s

10.5. PART 10.5: PROGRAMMING TRANSFORMERS WITH KERAS

387

names = [ ' y e a r ' ,

' month ' ,

' day ' ,

' dec_year ' ,

' sn_value '

,

' s n _ e r r o r ' ,

' obs_num ' ,

' e x t r a ' ]

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/SN_d_tot_V2 . 0 . c s v " ,
s e p= ' ; ' , h e a d e r=None , names=names ,
na_values =[ '−1 ' ] ,

i n d e x _ c o l=F a l s e )

print ( " S t a r t i n g ␣ f i l e : " )
print ( d f [ 0 : 1 0 ] )

print ( " Ending ␣ f i l e : " )
print ( d f [ − 1 0 : ] )

Output

S t a r t i n g f i l e :
y e a r month
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818
1
1818

0
1
2
3
4
5
6
7
8
9
Ending f i l e :

day
1
2
3
4
5
6
7
8
9
10

dec_year
1 8 1 8 . 0 0 1
1 8 1 8 . 0 0 4
1 8 1 8 . 0 0 7
1 8 1 8 . 0 1 0
1 8 1 8 . 0 1 2
1 8 1 8 . 0 1 5
1 8 1 8 . 0 1 8
1 8 1 8 . 0 2 1
1 8 1 8 . 0 2 3
1 8 1 8 . 0 2 6

sn_value
−1
−1
−1
−1
−1
−1
−1
65
−1
−1

s n _ e r r o r
NaN
NaN
NaN
NaN
NaN
NaN
NaN
1 0 . 2
NaN
NaN

obs_num e x t r a
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
1
1
0
1
0

72855

. . .

72860
72861
72862
72863
72864

y e a r month
6
2017

day
21

dec_year
2 0 1 7 . 4 7 0

sn_value
35

s n _ e r r o r
1 . 0

obs_num e x t r a
0

41

2017
2017
2017
2017
2017

6
6
6
6
6

26
27
28
29
30

2 0 1 7 . 4 8 4
2 0 1 7 . 4 8 6
2 0 1 7 . 4 8 9
2 0 1 7 . 4 9 2
2 0 1 7 . 4 9 5

21
19
17
12
11

1 . 1
1 . 2
1 . 1
0 . 5
0 . 5

25
36
22
25
30

0
0
0
0
0

As you can see, there is quite a bit of missing data near the end of the ﬁle. We want to ﬁnd the starting
index where the missing data no longer occurs. This technique is somewhat sloppy; it would be better to

388

CHAPTER 10. TIME SERIES IN KERAS

ﬁnd a use for the data between missing values. However, the point of this example is to show how to use
a transformer encoder with a somewhat simple time series.

Code

# Find t h e l a s t
z e r o and move one beyond
s t a r t _ i d = max( d f [ d f [ ' obs_num ' ] == 0 ] . i n d e x . t o l i s t ())+1
print ( s t a r t _ i d )
d f = d f [ s t a r t _ i d : ] # Trim t h e rows

t h a t have m i s s i n g o b s e r v a t i o n s

Output

11314

Divide into training and test/validation sets.

Code

d f [ ' sn_value ' ] = d f [ ' sn_value ' ] . a s t y p e ( f l o a t )
d f _ t r a i n = d f [ d f [ ' y e a r ' ] <2000]
d f _ t e s t = d f [ d f [ ' y e a r ' ] >=2000]

s p o t s _ t r a i n = d f _ t r a i n [ ' sn_value ' ] . t o l i s t ( )
s p o t s _ t e s t = d f _ t e s t [ ' sn_value ' ] . t o l i s t ( )

print ( " T r a i n i n g ␣ s e t ␣ has ␣ {} ␣ o b s e r v a t i o n s . " . format ( len ( s p o t s _ t r a i n ) ) )
print ( " Test ␣ s e t ␣ has ␣ {} ␣ o b s e r v a t i o n s . " . format ( len ( s p o t s _ t e s t ) ) )

Output

T r a i n i n g s e t has 55160 o b s e r v a t i o n s .
s e t has 6391 o b s e r v a t i o n s .
Test

The to_sequences function takes linear time series data into an x and y where x is all possible
sequences of seq_size. After each x sequence, this function places the next value into the y variable.
These x and y data can train a time-series neural network.

Code

import numpy a s np

def t o _ s e q u e n c e s ( s e q _ s i z e , obs ) :

x = [ ]

10.5. PART 10.5: PROGRAMMING TRANSFORMERS WITH KERAS

389

y = [ ]

for i

in range ( len ( obs)−SEQUENCE_SIZE ) :

#p r i n t ( i )
window = obs [ i : ( i+SEQUENCE_SIZE ) ]
after_window = obs [ i+SEQUENCE_SIZE ]
for x in window ]
window = [ [ x ]
#p r i n t ( " { } − { } " . format ( window , after_window ) )
x . append ( window )
y . append ( after_window )

return np . a r r a y ( x ) , np . a r r a y ( y )

SEQUENCE_SIZE = 10
x_train , y _ t r a i n = t o _ s e q u e n c e s (SEQUENCE_SIZE, s p o t s _ t r a i n )
x_test , y _ t es t = t o _ s e q u e n c e s (SEQUENCE_SIZE, s p o t s _ t e s t )

print ( " Shape ␣ o f ␣ t r a i n i n g ␣ s e t : ␣ {} " . format ( x _ t r a i n . shape ) )
print ( " Shape ␣ o f ␣ t e s t ␣ s e t : ␣ {} " . format ( x _ t es t . shape ) )

Output

Shape o f
Shape o f

t r a i n i n g s e t :
s e t :
t e s t

( 6 3 8 1 , 1 0 , 1 )

( 5 5 1 5 0 , 1 0 , 1 )

We can view the results of the to_sequences encoding of the sunspot data.

Code

print ( x _ t r a i n . shape )

Output

( 5 5 1 5 0 , 1 0 , 1 )

Next, we create the transformer_encoder; I obtained this function from a Keras example. This layer
includes residual connections, layer normalization, and dropout. This resulting layer can be stacked multiple
times. We implement the projection layers with the Keras Conv1D.

390

Code

from t e n s o r f l o w import k e r a s
from t e n s o r f l o w . k e r a s import l a y e r s

CHAPTER 10. TIME SERIES IN KERAS

def

t r a n s f o r m e r _ e n c o d e r ( i n p u t s , head_size , num_heads ,
# N o r m a l i z a t i o n and A t t e n t i o n
x = l a y e r s . L a y e r N o r m a l i z a t i o n ( e p s i l o n =1e −6)( i n p u t s )
x = l a y e r s . MultiHeadAttention (

ff_dim , dropout =0):

key_dim=head_size , num_heads=num_heads , dropout=dropout

) ( x , x )
x = l a y e r s . Dropout ( dropout ) ( x )
r e s = x + i n p u t s

# Feed Forward Part
x = l a y e r s . L a y e r N o r m a l i z a t i o n ( e p s i l o n =1e −6)( r e s )
x = l a y e r s . Conv1D ( f i l t e r s =ff_dim , k e r n e l _ s i z e =1, a c t i v a t i o n=" r e l u " ) ( x )
x = l a y e r s . Dropout ( dropout ) ( x )
x = l a y e r s . Conv1D ( f i l t e r s =i n p u t s . shape [ − 1 ] , k e r n e l _ s i z e =1)( x )
return x + r e s

The following function is provided to build the model, including the attention layer.

Code

def build_model (
input_shape ,
head_size ,
num_heads ,
ff_dim ,
num_transformer_blocks ,
mlp_units ,
dropout =0,
mlp_dropout =0,

) :

i n p u t s = k e r a s . Input ( shape=input_shape )
x = i n p u t s
for _ in range ( num_transformer_blocks ) :

x = t r a n s f o r m e r _ e n c o d e r ( x , head_size , num_heads ,

ff_dim , dropout )

x = l a y e r s . GlobalAveragePooling1D ( data_format=" c h a n n e l s _ f i r s t " ) ( x )
for dim in mlp_units :

x = l a y e r s . Dense ( dim , a c t i v a t i o n=" r e l u " ) ( x )
x = l a y e r s . Dropout ( mlp_dropout ) ( x )

10.5. PART 10.5: PROGRAMMING TRANSFORMERS WITH KERAS

391

o u t p u t s = l a y e r s . Dense ( 1 ) ( x )
return k e r a s . Model ( i n p u t s , o u t p u t s )

We are now ready to build and train the model.

Code

input_shape = x _ t r a i n . shape [ 1 : ]

model = build_model (
input_shape ,
h e a d _ s i z e =256 ,
num_heads=4,
ff_dim =4,
num_transformer_blocks =4,
mlp_units = [ 1 2 8 ] ,
mlp_dropout =0.4 ,
dropout =0.25 ,

)

model . compile (

l o s s=" mean_squared_error " ,
o p t i m i z e r=k e r a s . o p t i m i z e r s . Adam( l e a r n i n g _ r a t e =1e −4)

)
#model . summary ( )

c a l l b a c k s = [ k e r a s . c a l l b a c k s . E a r l y S t o p p i n g ( p a t i e n c e =10 , \

r e s t o r e _ b e s t _ w e i g h t s=True ) ]

model . f i t (

x_train ,
y_train ,
v a l i d a t i o n _ s p l i t =0.2 ,
e po c h s =200 ,
b a t c h _ s i z e =64 ,
c a l l b a c k s=c a l l b a c k s ,

)

model . e v a l u a t e ( x_test , y_test , v e r b o s e =1)

Output

. . .

392

CHAPTER 10. TIME SERIES IN KERAS

690/690 [==============================] − 11 s 15ms/ s t e p − l o s s :
6 7 9 . 1 3 2 0 − v a l _ l o s s : 2 8 9 . 7 0 4 6
Epoch 37/200
690/690 [==============================] − 11 s 16ms/ s t e p − l o s s :
6 7 3 . 3 4 0 0 − v a l _ l o s s : 2 9 7 . 0 6 8 7
200/200 [==============================] − 1 s 5ms/ s t e p − l o s s :
2 1 4 . 5 6 0 3
2 1 4 . 5 6 0 3 1 7 9 9 3 1 6 4 0 6

Finally, we evaluate the model with RMSE.

Code

from s k l e a r n import m e t r i c s

pred = model . p r e d i c t ( x _t e s t )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

Output

S c o r e (RMSE) : 1 4 . 6 4 7 8 7 5 9 4 6 2 8 3 0 0 7

10.5. PART 10.5: PROGRAMMING TRANSFORMERS WITH KERAS

393

Figure 10.4: Architectural Diagram from the Paper

394

CHAPTER 10. TIME SERIES IN KERAS

Chapter 11

Natural Language Processing with
Hugging Face

11.1 Part 11.1: Introduction to Hugging Face

Transformers have become a mainstay of natural language processing. This module will examine the
Hugging Face Python library for natural language processing, bringing together pretrained transformers,
data sets, tokenizers, and other elements. Through the Hugging Face API, you can quickly begin using
sentiment analysis, entity recognition, language translation, summarization, and text generation.

Colab does not install Hugging face by default. Whether installing Hugging Face directly into a local

computer or utilizing it through Colab, the following commands will install the library.

Code

! p i p i n s t a l l
! p i p i n s t a l l

t r a n s f o r m e r s
t r a n s f o r m e r s [ s e n t e n c e p i e c e ]

Now that we have Hugging Face installed, the following sections will demonstrate how to apply Hugging
Face to a variety of everyday tasks. After this introduction, the remainder of this module will take a deeper
look at several speciﬁc NLP tasks applied to Hugging Face.

11.1.1 Sentiment Analysis

Sentiment analysis uses natural language processing, text analysis, computational linguistics, and biomet-
rics to identify the tone of written text. Passages of written text can be into simple binary states of positive
or negative tone. More advanced sentiment analysis might classify text into additional categories: sadness,
joy, love, anger, fear, or surprise.

To demonstrate sentiment analysis, we begin by loading sample text, Shakespeare’s 18th sonnet, a

famous poem.

395

396

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Code

from u r l l i b . r e q u e s t import u r l o p e n

# Read sample t e x t , a poem
URL = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ " \

" d a t a s e t s / sonnet_18 . t x t "

f = u r l o p e n (URL)
t e x t = f . r e a d ( ) . decode ( " u t f −8" )

Usually, you have to preprocess text into embeddings or other vector forms before presentation to a
neural network. Hugging Face provides a pipeline that simpliﬁes this process greatly. The pipeline allows
you to pass regular Python strings to the transformers and return standard Python values.

We begin by loading a text-classiﬁcation model. We do not specify the exact model type wanted, so

Hugging Face automatically chooses a network from the Hugging Face hub named:

• distilbert-base-uncased-ﬁnetuned-sst-2-english

To specify the model to use, pass the model parameter, such as:

p i p e = p i p e l i n e ( model=" r o b e r t a −l a r g e −mnli " )

The following code loads a model pipeline and a model for sentiment analysis.

Code

import pandas a s pd
from t r a n s f o r m e r s import p i p e l i n e

c l a s s i f i e r = p i p e l i n e ( " t e x t − c l a s s i f i c a t i o n " )

We can now display the sentiment analysis results with a Pandas dataframe.

Code

o u t p u t s = c l a s s i f i e r ( t e x t )
pd . DataFrame ( o u t p u t s )

Output

label

score

0 POSITIVE 0.984666

As you can see, the poem was considered 0.98 positive.

11.1. PART 11.1: INTRODUCTION TO HUGGING FACE

397

11.1.2 Entity Tagging

Entity tagging is the process that takes source text and ﬁnds parts of that text that represent entities,
such as one of the following:

• Location (LOC)
• Organizations (ORG)
• Person (PER)
• Miscellaneous (MISC)

The following code requests a "named entity recognizer" (ner) and processes the speciﬁed text.

Code

t e x t 2 = " Abraham␣ L i n c o l n ␣was␣a␣ p r e s i d e n t ␣who␣ l i v e d ␣ i n ␣ t h e ␣ United ␣ S t a t e s . "

t a g g e r = p i p e l i n e ( " n e r " , a g g r e g a t i o n _ s t r a t e g y=" s i m p l e " )

We similarly view the results as a Pandas data frame. As you can see, the person (PER) of Abraham

Lincoln and location (LOC) of the United States is recognized.

Code

o u t p u t s = t a g g e r ( t e x t 2 )
pd . DataFrame ( o u t p u t s )

Output

entity_group

0 PER
1 LOC

word
score
0.998893 Abraham Lincoln
0.999651 United States

start
0
49

end
15
62

11.1.3 Question Answering

Another common task for NLP is question answering from a reference text. We load such a model with
the following code.

Code

r e a d e r = p i p e l i n e ( " q u e s t i o n −a n s w e r i n g " )
q u e s t i o n = "What␣now␣ s h a l l ␣ f a d e ? "

For this example, we will pose the question "what shall fade" to Hugging Face for Sonnet 18. We see

the correct answer of "eternal summer."

398

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Code

o u t p u t s = r e a d e r ( q u e s t i o n=q u e s t i o n , c o n t e x t=t e x t )
pd . DataFrame ( [ o u t p u t s ] )

Output

score
0.471141

start
414

end
428

answer
eternal summer

0

11.1.4 Language Translation

Language translation is yet another common task for NLP and Hugging Face.

Code

t r a n s l a t o r = p i p e l i n e ( " t r a n s l a t i o n _ e n _ t o _ d e " ,

model=" H e l s i n k i −NLP/ opus−mt−en−de " )

The following code translates Sonnet 18 from English into German.

Code

o u t p u t s = t r a n s l a t o r ( t e x t , c l e a n _ u p _ t o k e n i z a t i o n _ s p a c e s=True ,

min_length =100)
print ( o u t p u t s [ 0 ] [ ' t r a n s l a t i o n _ t e x t ' ] )

Output

i s t

i c h d i c h mit einem

s c h n e r und g e m i g t e r : Raue Winde

s e i n Gold T e i n t dimm ' d ; Und j e d e

Irgendwann zu h e i das Auge d e s Himmels l e u c h t e t , Und

l i e b l i c h e n Knospen d e s Mai , Und d e r Sommervertrag hat zu

Sonnet 18 O r i g i n a l t e x t William S h a k e s p e a r e S o l l
Sommertag v e r g l e i c h e n ? Du b i s t
s c h t t e l n d i e
kurz e i n Datum :
o f t
s i n k t , Durch Z u f a l l o d e r d i e Natur we ch se lnd en Kurs untrimm ' d ; Aber
d e i n e w i g e r Sommer wird n i c h t v e r b l a s s e n noch v e r l i e r e n B e s i t z von dem
Schnen du s c h u l d ; noch wird d e r Tod p r a h l e n du wandert
Schatten , Wenn i n ewigen L i n i e n z u r Z e i t wachsen : So l a n g e d i e
Menschen atmen o d e r Augen se h e n knnen , So l a n g e l e b t d i e s und d i e s
g i b t d i r Leben .

f a i r e von F a i r

irgendwann

i n seinem

11.1. PART 11.1: INTRODUCTION TO HUGGING FACE

399

11.1.5 Summarization

Summarization is an NLP task that summarizes a more lengthy text into just a few sentences.

Code

f r u i t produced by an a p p l e t r e e ( Malus d o m e s t i c a ) .

t e x t 2 = " " "
An a p p l e i s an e d i b l e
Apple t r e e s a r e c u l t i v a t e d w o r l d w i d e and a r e t h e most w i d e l y grown s p e c i e s
i n t h e genus Malus . The t r e e o r i g i n a t e d i n C e n t r a l Asia , where i t s w i l d
a n c e s t o r , Malus
found t o d a y . A p p l e s have been grown
f o r
by European c o l o n i s t s . A p p l e s have r e l i g i o u s and m y t h o l o g i c a l
i n many c u l t u r e s ,
" " "

i n c l u d i n g Norse , Greek , and European C h r i s t i a n t r a d i t i o n .

i n Asia and Europe and were b r o u g h t

t h o u s a n d s o f y e a r s

t o North America

s i g n i f i c a n c e

s i e v e r s i i ,

s t i l l

i s

summarizer = p i p e l i n e ( " summarization " )

The following code summarizes the Wikipedia entry for an "apple."

Code

o u t p u t s = summarizer ( t e x t 2 , max_length =45 ,

c l e a n _ u p _ t o k e n i z a t i o n _ s p a c e s=True )

print ( o u t p u t s [ 0 ] [ ' summary_text ' ] )

Output

An a p p l e i s an e d i b l e
d o m e s t i c a ) Apple t r e e s a r e c u l t i v a t e d worldwide and a r e t h e most
w i d e l y grown s p e c i e s
m y t h o l o g i c a l

f r u i t produced by an a p p l e t r e e ( Malus

i n t h e genus Malus . Apples have r e l i g i o u s and

11.1.6 Text Generation

Finally, text generation allows us to take an input text and request the pretrained neural network to
continue that text.

Code

from u r l l i b . r e q u e s t import u r l o p e n

g e n e r a t o r = p i p e l i n e ( " t e x t −g e n e r a t i o n " )

400

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Here an example is provided that generates additional text after Sonnet 18.

Code

o u t p u t s = g e n e r a t o r ( t e x t , max_length =400)
print ( o u t p u t s [ 0 ] [ ' g e n e r a t e d _ t e x t ' ] )

Output

t e x t

l e a s e hath a l l

I compare t h e e t o a summer ' s day ?

Sonnet 18 o r i g i n a l
William S h a k e s p e a r e
S h a l l
Thou a r t more l o v e l y and more t em p e r a te :
Rough winds do shake t h e d a r l i n g buds o f May ,
t o o s h o r t a d a t e :
And summer ' s
Sometime t o o hot
And o f t e n i s h i s g o l d complexion dimm ' d ;
And e v e r y f a i r
from f a i r
By chance o r nature ' s c h a n g i n g c o u r s e untrimm ' d ;
But thy e t e r n a l summer s h a l l not
Nor
f a i r
Nor
When i n e t e r n a l
t o time thou g r o w e s t :
So l o n g a s men can b r e a t h e o r e y e s can s e e ,

l o s e p o s s e s s i o n o f
s h a l l Death brag thou wander ' s t

f a d e
thou owest ;
i n h i s

t h e eye o f heaven s h i n e s ,

sometime d e c l i n e s ,

l i n e s

t h a t

shade ,

. . .

[ I t a l i a n : The Tale o f
Cat ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

t h e

' S i r !

s i r

l a verde '~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

~~~~~~~~~~~~~~~~~~~~~~~~
[ I r i s h : The Tale o f

11.2 Part 11.2: Hugging Face Tokenizers

Tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing
away certain characters, such as punctuation. Consider how the program might break up the following
sentences into words.

• This is a test.
• Ok, but what about this?
• Is U.S.A. the same as USA.?

11.2. PART 11.2: HUGGING FACE TOKENIZERS

401

• What is the best data-set to use?
• I think I will do this-no wait; I will do that.

The hugging face includes tokenizers that can break these sentences into words and subwords. Because
English, and some other languages, are made up of common word parts, we tokenize subwords. For example,
a gerund word, such as "sleeping," will be tokenized into "sleep" and "##ing".

We begin by installing Hugging Face if needed.

Code

! p i p i n s t a l l
! p i p i n s t a l l

t r a n s f o r m e r s
t r a n s f o r m e r s [ s e n t e n c e p i e c e ]

First, we create a Hugging Face tokenizer. There are several diﬀerent tokenizers available from the

Hugging Face hub. For this example, we will make use of the following tokenizer:

• distilbert-base-uncased

This tokenizer is based on BERT and assumes case-insensitive English text.

Code

from t r a n s f o r m e r s import AutoTokenizer
model = " d i s t i l b e r t −base−uncased "
t o k e n i z e r = AutoTokenizer . f r o m _ p r e t r a i n e d ( model )

We can now tokenize a sample sentence.

Code

encoded = t o k e n i z e r ( ' T o k e n i z i n g ␣ t e x t ␣ i s ␣ e a s y . ' )
print ( encoded )

Output

{ ' input_ids ' :
' attention_mask ' :

[ 1 0 1 , 1 9 2 0 4 , 6 0 2 6 , 3 7 9 3 , 2 0 0 3 , 3 7 3 3 , 1 0 1 2 , 1 0 2 ] ,

[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ] }

The result of this tokenization contains two elements:

• input_ids - The individual subword indexes, each index uniquely identiﬁes a subword.
• attention_mask - Which values in input_ids are meaningful and not padding.

This sentence had no padding, so all elements have an attention mask of "1". Later, we will request the
output to be of a ﬁxed length, introducing padding, which always has an attention mask of "0". Though

402

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

each tokenizer can be implemented diﬀerently, the attention mask of a tokenizer is generally either "0" or
"1".

Due to subwords and special tokens, the number of tokens may not match the number of words in the

source string. We can see the meanings of the individual tokens by converting these IDs back to strings.

Code

t o k e n i z e r . convert_ids_to_tokens ( encoded . i n p u t _ i d s )

Output

[ ' [ CLS ] ' ,

' token ' ,

'## i z i n g ' ,

' t e x t ' ,

' i s ' ,

' easy ' ,

' . ' ,

' [ SEP ] ' ]

As you can see, there are two special tokens placed at the beginning and end of each sequence. We will
soon see how we can include or exclude these special tokens. These special tokens can vary per tokenizer;
however, [CLS] begins a sequence for this tokenizer, and [SEP] ends a sequence. You will also see that the
gerund "tokening" is broken into "token" and "*ing".

For this tokenizer, the special tokens occur between 100 and 103. Most Hugging Face tokenizers use
this approximate range for special tokens. The value zero (0) typically represents padding. We can display
all special tokens with this command.

Code

t o k e n i z e r . convert_ids_to_tokens ( [ 0 , 1 0 0 , 1 0 1 , 1 0 2 , 1 0 3 ] )

Output

[ ' [PAD] ' ,

' [UNK] ' ,

' [ CLS ] ' ,

' [ SEP ] ' ,

' [MASK] ' ]

This tokenizer supports these common tokens:

• [CLS] - Sequence beginning.
• [SEP] - Sequence end.
• [PAD] - Padding.
• [UNK] - Unknown token.
• [MASK] - Mask out tokens for a neural network to predict. Not used in this book, see MLM paper.

It is also possible to tokenize lists of sequences. We can pad and truncate sequences to achieve a standard
length by tokenizing many sequences at once.

Code

t e x t = [

" This ␣ movie ␣was␣ g r e a t ! " ,

11.3. PART 11.3: HUGGING FACE DATASETS

403

" I ␣ hated ␣ t h i s ␣move , ␣ waste ␣ o f ␣ time ! " ,
" Epic ? "

]

encoded = t o k e n i z e r ( t e x t , padding=True , a d d _ s p e c i a l _ t o k e n s=True )

print ( " ∗∗ Input ␣ IDs ∗∗ " )
for a in encoded . i n p u t _ i d s :

print ( a )

print ( " ∗∗ A t t e n t i o n ␣Mask∗∗ " )
for a in encoded . attention_mask :

print ( a )

Output

∗∗ Input IDs ∗∗
[ 1 0 1 , 2 0 2 3 , 3 1 8 5 , 2 0 0 1 , 2 3 0 7 , 9 9 9 , 1 0 2 , 0 , 0 , 0 , 0 ]
[ 1 0 1 , 1 0 4 5 , 6 2 8 3 , 2 0 2 3 , 2 6 9 3 , 1 0 1 0 , 5 9 4 9 , 1 9 9 7 , 2 0 5 1 , 9 9 9 , 1 0 2 ]
[ 1 0 1 , 8 6 8 0 , 1 0 2 9 , 1 0 2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]
∗∗ A t t e n t i o n Mask∗∗
[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 ]
[ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]
[ 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]

Notice the input_id’s for the three movie review text sequences. Each of these sequences begins with
101 and we pad with zeros. Just before the padding, each group of IDs ends with 102. The attention masks
also have zeros for each of the padding entries.

We used two parameters to the tokenizer to control the tokenization process. Some other useful pa-

rameters include:

• add_special_tokens (defaults to True) Whether or not to encode the sequences with the special

tokens relative to their model.

• padding (defaults to False) Activates and controls truncation.
• max_length (optional) Controls the maximum length to use by one of the truncation/padding pa-

rameters.

11.3 Part 11.3: Hugging Face Datasets

The Hugging Face hub includes data sets useful for natural language processing (NLP). The Hugging Face
library provides functions that allow you to navigate and obtain these data sets. When we access Hugging

404

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Face data sets, the data is in a format speciﬁc to Hugging Face. In this part, we will explore this format
and see how to convert it to Pandas or TensorFlow data.

We begin by installing Hugging Face if needed. It is also essential to install Hugging Face datasets.

Code

! p i p i n s t a l l
! p i p i n s t a l l
! p i p i n s t a l l d a t a s e t s

t r a n s f o r m e r s
t r a n s f o r m e r s [ s e n t e n c e p i e c e ]

We begin by querying Hugging Face to obtain the total count and names of the data sets. This code

obtains the total count and the names of the ﬁrst ﬁve datasets.

Code

from d a t a s e t s import l i s t _ d a t a s e t s

a l l _ d a t a s e t s = l i s t _ d a t a s e t s ( )

print ( f " Hugging ␣ Face ␣hub␣ c u r r e n t l y ␣ c o n t a i n s ␣{ l e n ( a l l _ d a t a s e t s ) } " )
print ( f " d a t a s e t s . ␣The␣ f i r s t ␣5 ␣ a r e : " )
print ( " \n " . j o i n ( a l l _ d a t a s e t s [ : 1 0 ] ) )

Output

Hugging Face hub c u r r e n t l y c o n t a i n s 3832
d a t a s e t s . The f i r s t 5 a r e :
a c r o n y m _ i d e n t i f i c a t i o n
ade_corpus_v2
a d v e r s a r i a l _ q a
a e s l c
a f r i k a a n s _ n e r _ c o r p u s
ag_news
a i 2 _ a r c
a i r _ d i a l o g u e
a j g t _ t w i t t e r _ a r
a l l e g r o _ r e v i e w s

We begin by loading the emotion data set from the Hugging Face hub. Emotion is a dataset of English
Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise.[30]The following
code loads the emotion data set from the Hugging Face hub.

11.3. PART 11.3: HUGGING FACE DATASETS

405

Code

from d a t a s e t s import l o a d _ d a t a s e t

e mo t io ns = l o a d _ d a t a s e t ( " emotion " )

Output

0%|

0%|

s c r i p t :

( download : 1 . 9 7

| 0 . 0 0 / 1 . 6 1 k [ 0 0 : 0 0 < ? ,

| 0 . 0 0 / 1 . 6 6 k [ 0 0 : 0 0 < ? ,

t o / r o o t / . c a c h e / h u g g i n g f a c e / d a t a s e t s / emotion / d e f a u l t / 0 . 0 . 0 / 3 4 8 f 6 3

Downloading b u i l d e r
?B/ s ] Downloading metadata :
?B/ s ] Downloading and p r e p a r i n g d a t a s e t emotion / d e f a u l t
MiB , g e n e r a t e d : 2 . 0 7 MiB , post−p r o c e s s e d : Unknown s i z e ,
MiB)
c a 8 e 2 7 b 3 7 1 3 b 6 c 0 4 d 7 2 3 e f e 6 d 8 2 4 a 5 6 f b 3 d 1 4 4 9 7 9 4 7 1 6 c 0 f 0 2 9 6 0 7 2 7 0 5 . . .
Downloading data :
0%|
?B/ s ] Downloading data :
?B/ s ] Downloading data :
0%|
?B/ s ] G e n e r a t i n g t r a i n s p l i t :
examples / s ] G e n e r a t i n g v a l i d a t i o n s p l i t :
[ 0 0 : 0 0 < ? , ? examples / s ] G e n e r a t i n g t e s t
[ 0 0 : 0 0 < ? , ? examples / s ] D a t a s e t emotion downloaded and p r e p a r e d t o / r o o
t / . c a c h e / h u g g i n g f a c e / d a t a s e t s / emotion / d e f a u l t / 0 . 0 . 0 / 3 4 8 f 6 3 c a 8 e 2 7 b 3 7 1 3 b
6 c 0 4 d 7 2 3 e f e 6 d 8 2 4 a 5 6 f b 3 d 1 4 4 9 7 9 4 7 1 6 c 0 f 0 2 9 6 0 7 2 7 0 5 . Subsequent c a l l s w i l l
r e u s e t h i s data .

| 0 . 0 0 / 2 0 4 k [ 0 0 : 0 0 < ? ,
| 0 . 0 0 / 2 0 7 k [ 0 0 : 0 0 < ? ,

| 0 . 0 0 / 1 . 6 6M [ 0 0 : 0 0 < ? ,

| 0/16000 [ 0 0 : 0 0 < ? , ?

0%|
s p l i t :

t o t a l : 4 . 0 5

| 0/2000

| 0/2000

0%|

0%|

0%|

0%|

| 0/3 [ 0 0 : 0 0 < ? , ? i t / s ]

A quick scan of the downloaded data set reveals its structure.

In this case, Hugging Face already
separated the data into training, validation, and test data sets. The training set consists of 16,000 obser-
vations, while the test and validation sets contain 2,000 observations. The dataset is a Python dictionary
that includes a Dataset object for each of these three divisions. The datasets only contain two columns,
the text and the emotion label for each text sample.

Code

e mo t io n s

Output

D a t a s e t D i c t ( {

t r a i n : D a t a s e t ( {
f e a t u r e s :
num_rows : 16000

[ ' t e x t ' ,

' l a b e l ' ] ,

406

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

} )
v a l i d a t i o n : D a t a s e t ( {

f e a t u r e s :
num_rows : 2000

[ ' t e x t ' ,

' l a b e l ' ] ,

} )
t e s t : D a t a s e t ( {

f e a t u r e s :
num_rows : 2000

[ ' t e x t ' ,

' l a b e l ' ] ,

} )

} )

You can see a single observation from the training data set here. This observation includes both the text

sample and the assigned emotion label. The label is a numeric index representing the assigned emotion.

Code

e mo t i o ns [ ' t r a i n ' ] [ 2 ]

Output

{ ' l a b e l ' : 3 ,
wrong ' }

' t e x t ' :

' im g r a b b i n g a minute t o p o s t

i

f e e l g r e e d y

We can display the labels in order of their index labels.

Code

e mo t i o ns [ ' t r a i n ' ] . f e a t u r e s

Output

{ ' l a b e l ' : C l a s s L a b e l ( num_classes =6, names =[ ' s a d n e s s ' ,
' anger ' ,

' s u r p r i s e ' ] ,

i d=None ) ,

' f e a r ' ,

' joy ' ,

' l o v e ' ,

' t e x t ' : Value ( dtype =' s t r i n g ' ,

i d=None ) }

Hugging face can provide these data sets in a variety of formats. The following code receives the emotion

data set as a Pandas data frame.

Code

import pandas a s pd

e mo t i o ns . s et_ f orm a t ( type= ' pandas ' )

11.3. PART 11.3: HUGGING FACE DATASETS

407

d f = em o t i on s [ " t r a i n " ] [ : ]
d f [ : 5 ]

Output

text
i didnt feel humiliated
i can go from feeling so hopeless to so damned...
im grabbing a minute to post i feel greedy wrong
i am ever feeling nostalgic about the ﬁreplac...
i am feeling grouchy

label
0
0
3
2
3

0
1
2
3
4

We can use the Pandas "apply" function to add the textual label for each observation.

Code

def

l a b e l _ i t ( row ) :

return e mo ti o ns [ " t r a i n " ] . f e a t u r e s [ " l a b e l " ] . i n t 2 s t r ( row )

d f [ ' label_name ' ] = d f [ " l a b e l " ] . apply ( l a b e l _ i t )
d f [ : 5 ]

Output

text
i didnt feel humiliated
i can go from feeling so hopeless to so damned...
im grabbing a minute to post i feel greedy wrong
i am ever feeling nostalgic about the ﬁreplac...
i am feeling grouchy

label
0
0
3
2
3

label_name
sadness
sadness
anger
love
anger

0
1
2
3
4

With the data in Pandas format and textually labeled, we can display a bar chart of the frequency of

each of the emotions.

Code

import m a t p l o t l i b . p y p l o t a s p l t

d f [ " label_name " ] . v al ue_ co un t s ( a s c e n d i n g=True ) . p l o t . barh ( )
p l t . show ( )

408

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Output

Finally, we utilize Hugging Face tokenizers and data sets together. The following code tokenizes the
entire emotion data set. You can see below that the code has transformed the training set into subword
tokens that are now ready to be used in conjunction with a transformer for either inference or training.

Code

from t r a n s f o r m e r s import AutoTokenizer

def

t o k e n i z e ( rows ) :
return t o k e n i z e r ( rows [ ' t e x t ' ] , padding=True ,

t r u n c a t i o n=True )

model_ckpt = " d i s t i l b e r t −base−uncased "
t o k e n i z e r = AutoTokenizer . f r o m _ p r e t r a i n e d ( model_ckpt )

e mo t i o ns . s e t _ f o r ma t ( type=None )

encoded = t o k e n i z e ( em ot i on s [ " t r a i n " ] [ : 2 ] )

print ( " ∗∗ Input ␣ IDs ∗∗ " )
for a in encoded . i n p u t _ i d s :

print ( a )

Output

Downloading :
0%|
0 . 0 0 / 2 2 6 k [ 0 0 : 0 0 < ? , ?B/ s ] Downloading :

0%|

| 0 . 0 0 / 4 8 3 [ 0 0 : 0 0 < ? , ?B/ s ] Downloading :

0%|

| 0 . 0 0 / 2 8 . 0 [ 0 0 : 0 0 < ? , ?B/ s ] Downloading :

0%|
| 0 . 0 0 / 4 5 5 k

|

11.4. PART 11.4: TRAINING HUGGING FACE MODELS

409

[ 0 0 : 0 0 < ? , ?B/ s ] ∗ ∗ Input IDs ∗∗
[ 1 0 1 , 1 0 4 5 , 2 1 3 4 , 2 1 0 2 , 2 5 1 4 , 2 6 6 0 8 , 1 0 2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ,
0 , 0 , 0 , 0 , 0 , 0 , 0 ]
[ 1 0 1 , 1 0 4 5 , 2 0 6 4 , 2 1 7 5 , 2 0 1 3 , 3 1 1 0 , 2 0 6 1 , 2 0 6 2 5 , 2 0 0 0 , 2 0 6 1 , 9 6 3 6 ,
1 7 7 7 2 , 2 0 7 4 , 2 0 1 3 , 2 1 0 8 , 2 1 0 5 , 2 6 1 9 , 2 0 4 0 , 1 4 9 7 7 , 1 9 9 8 , 2 0 0 3 , 8 3 0 0 ,
1 0 2 ]

11.4 Part 11.4: Training Hugging Face Models

Up to this point, we’ve used data and models from the Hugging Face hub unmodiﬁed. In this section,
we will transfer and train a Hugging Face model. We will use Hugging Face data sets, tokenizers, and
pretrained models to achieve this training.

We begin by installing Hugging Face if needed. It is also essential to install Hugging Face datasets.

Code

! p i p i n s t a l l
! p i p i n s t a l l
! p i p i n s t a l l d a t a s e t s

t r a n s f o r m e r s
t r a n s f o r m e r s [ s e n t e n c e p i e c e ]

We begin by loading the emotion data set from the Hugging Face hub. Emotion is a dataset of English
Twitter messages with six basic emotions: anger, fear, joy, love, sadness, and surprise. The following code
loads the emotion data set from the Hugging Face hub.

Code

from d a t a s e t s import l o a d _ d a t a s e t

e mo t io n s = l o a d _ d a t a s e t ( " emotion " )

You can see a single observation from the training data set here. This observation includes both the text

sample and the assigned emotion label. The label is a numeric index representing the assigned emotion.

Code

e mo t io n s [ ' t r a i n ' ] [ 2 ]

Output

{ ' l a b e l ' : 3 ,
wrong ' }

' t e x t ' :

' im g r a b b i n g a minute t o p o s t

i

f e e l g r e e d y

410

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

We can display the labels in order of their index labels.

Code

e mo t io ns [ ' t r a i n ' ] . f e a t u r e s

Output

{ ' l a b e l ' : C l a s s L a b e l ( num_classes =6, names =[ ' s a d n e s s ' ,
' anger ' ,

' s u r p r i s e ' ] ,

i d=None ) ,

' f e a r ' ,

' joy ' ,

' l o v e ' ,

' t e x t ' : Value ( dtype =' s t r i n g ' ,

i d=None ) }

Next, we utilize Hugging Face tokenizers and data sets together. The following code tokenizes the entire
emotion data set. You can see below that the code has transformed the training set into subword tokens
that are now ready to be used in conjunction with a transformer for either inference or training.

Code

from t r a n s f o r m e r s import AutoTokenizer

def

t o k e n i z e ( rows ) :
return t o k e n i z e r ( rows [ ' t e x t ' ] , padding=" max_length " ,

t r u n c a t i o n=True )

model_ckpt = " d i s t i l b e r t −base−uncased "
t o k e n i z e r = AutoTokenizer . f r o m _ p r e t r a i n e d ( model_ckpt )

e mo t i o ns . s e t _ f o r ma t ( type=None )

t o k e n i z e d _ d a t a s e t s = em ot i on s .map( t o k e n i z e , batched=True )

We will utilize the Hugging Face DefaultDataCollator to transform the emotion data set into Ten-

sorFlow type data that we can use to ﬁnetune a neural network.

Code

from t r a n s f o r m e r s import D e f a u l t D a t a C o l l a t o r

d a t a _ c o l l a t o r = D e f a u l t D a t a C o l l a t o r ( r e t u r n _ t e n s o r s=" t f " )

Now we generate a shuﬄed training and evaluation data set.

11.4. PART 11.4: TRAINING HUGGING FACE MODELS

411

Code

s m a l l _ t r a i n _ d a t a s e t = t o k e n i z e d _ d a t a s e t s [ " t r a i n " ] . s h u f f l e ( s e e d =42)
s m a l l _ e v a l _ d a t a s e t = t o k e n i z e d _ d a t a s e t s [ " t e s t " ] . s h u f f l e ( s e e d =42)

We can now generate the TensorFlow data sets. We specify which columns should map to the input

features and labels. We do not need to shuﬄe because we previously shuﬄed the data.

Code

t f _ t r a i n _ d a t a s e t = s m a l l _ t r a i n _ d a t a s e t . t o _ t f _ d a t a s e t (

columns =[ " attention_mask " ,
l a b e l _ c o l s =[ " l a b e l s " ] ,
s h u f f l e=True ,
c o l l a t e _ f n=d a t a _ c o l l a t o r ,
b a t c h _ s i z e =8,

)

" i n p u t _ i d s " ,

" token_type_ids " ] ,

t f _ v a l i d a t i o n _ d a t a s e t = s m a l l _ e v a l _ d a t a s e t . t o _ t f _ d a t a s e t (

columns =[ " attention_mask " ,
l a b e l _ c o l s =[ " l a b e l s " ] ,
s h u f f l e=F a l s e ,
c o l l a t e _ f n=d a t a _ c o l l a t o r ,
b a t c h _ s i z e =8,

)

" i n p u t _ i d s " ,

" token_type_ids " ] ,

We will now load the distilbert model for classiﬁcation. We will adjust the pretrained weights to predict

the emotions of text lines.

Code

import t e n s o r f l o w a s
from t r a n s f o r m e r s import T F A u t o M o d e l F o r S e q u e n c e C l a s s i f i c a t i o n

t f

model = T F A u t o M o d e l F o r S e q u e n c e C l a s s i f i c a t i o n . f r o m _ p r e t r a i n e d ( \

" d i s t i l b e r t −base−uncased " , num_labels =6)

We now train the neural network. Because the network is already pretrained, we use a small learning

rate.

Code

model . compile (

o p t i m i z e r=t f . k e r a s . o p t i m i z e r s . Adam( l e a r n i n g _ r a t e =5e −5) ,
l o s s=t f . k e r a s . l o s s e s . S p a r s e C a t e g o r i c a l C r o s s e n t r o p y ( f r o m _ l o g i t s=True ) ,

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

m e t r i c s=t f . m e t r i c s . S p a r s e C a t e g o r i c a l A c c u r a c y ( ) ,

412

)

model . f i t ( t f _ t r a i n _ d a t a s e t , v a l i d a t i o n _ d a t a=t f _ v a l i d a t i o n _ d a t a s e t ,

e po c h s =5)

Output

. . .
2000/2000 [==============================] − 346 s 173ms/ s t e p − l o s s :
0 . 1 0 9 2 − s p a r s e _ c a t e g o r i c a l _ a c c u r a c y : 0 . 9 4 8 6 − v a l _ l o s s : 0 . 1 6 5 4 −
v a l _ s p a r s e _ c a t e g o r i c a l _ a c c u r a c y : 0 . 9 2 9 5
Epoch 5/5
2000/2000 [==============================] − 347 s 173ms/ s t e p − l o s s :
0 . 0 9 6 0 − s p a r s e _ c a t e g o r i c a l _ a c c u r a c y : 0 . 9 5 8 5 − v a l _ l o s s : 0 . 1 8 3 0 −
v a l _ s p a r s e _ c a t e g o r i c a l _ a c c u r a c y : 0 . 9 2 2 0

11.5 Part 11.5: What are Embedding Layers in Keras

Embedding Layers are a handy feature of Keras that allows the program to automatically insert additional
information into the data ﬂow of your neural network. In the previous section, you saw that Word2Vec
could expand words to a 300 dimension vector. An embedding layer would automatically allow you to
insert these 300-dimension vectors in the place of word indexes.

Programmers often use embedding layers with Natural Language Processing (NLP); however, you can
use these layers when you wish to insert a lengthier vector in an index value place. In some ways, you can
think of an embedding layer as dimension expansion. However, the hope is that these additional dimensions
provide more information to the model and provide a better score.

11.5.1 Simple Embedding Layer Example

• input_dim = How large is the vocabulary? How many categories are you encoding? This parameter

is the number of items in your "lookup table."

• output_dim = How many numbers in the vector you wish to return.
• input_length = How many items are in the input feature vector that you need to transform?

Now we create a neural network with a vocabulary size of 10, which will reduce those values between 0-9 to
4 number vectors. This neural network does nothing more than passing the embedding on to the output.
But it does let us see what the embedding is doing. Each feature vector coming in will have two such
features.

11.5. PART 11.5: WHAT ARE EMBEDDING LAYERS IN KERAS

413

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Embedding
import numpy a s np

model = S e q u e n t i a l ( )
embedding_layer = Embedding ( input_dim =10 , output_dim =4,
model . add ( embedding_layer )
model . compile ( ' adam ' ,

' mse ' )

i n p u t _ l e n g t h =2)

Let’s take a look at the structure of this neural network to see what is happening inside it.

Code

model . summary ( )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l "

Layer

( type )

Output Shape

Param #

=================================================================

embedding ( Embedding )

( None , 2 , 4 )

40

=================================================================
T o t a l params : 40
T r a i n a b l e params : 40
Non−t r a i n a b l e params : 0
_________________________________________________________________

For this neural network, which is just an embedding layer, the input is a vector of size 2. These two
inputs are integer numbers from 0 to 9 (corresponding to the requested input_dim quantity of 10 values).
Looking at the summary above, we see that the embedding layer has 40 parameters. This value comes
from the embedded lookup table that contains four amounts (output_dim) for each of the 10 (input_dim)
possible integer values for the two inputs. The output is 2 (input_length) length 4 (output_dim) vectors,
resulting in a total output size of 8, which corresponds to the Output Shape given in the summary above.
Now, let us query the neural network with two rows. The input is two integer values, as was speciﬁed

when we created the neural network.

Code

input_data = np . a r r a y ( [

[ 1 , 2 ]

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

414

] )

pred = model . p r e d i c t ( input_data )

print ( input_data . shape )
print ( pred )

Output

( 1 , 2 )
[ [ [ − 0 . 0 4 4 9 4 9 1 7
[ −0.04002655

0 . 0 1 9 3 7 4 6 8 −0.00152863
0 . 0 3 4 4 1 8 9 5

0 . 0 4 4 6 2 5 8 8 − 0 . 0 1 4 7 2 5 9 7 ] ] ]

0 . 0 4 8 0 8 6 5 9 ]

Here we see two length-4 vectors that Keras looked up for each input integer. Recall that Python arrays
are zero-based. Keras replaced the value of 1 with the second row of the 10 x 4 lookup matrix. Similarly,
Keras returned the value of 2 by the third row of the lookup matrix. The following code displays the lookup
matrix in its entirety. The embedding layer performs no mathematical operations other than inserting the
correct row from the lookup table.

Code

embedding_layer . g e t _ w e i g h t s ( )

Output

0 . 0 1 0 6 6 5 1 1 ] ,
0 . 0 2 8 9 8 7 7 4 , −0.0273805 ,
[ a r r a y ( [ [ − 0 . 0 3 1 6 4 1 9 6 ,
0 . 0 4 8 0 8 6 5 9 ] ,
0 . 0 1 9 3 7 4 6 8 , −0.00152863 ,
[ − 0 . 0 4 4 9 4 9 1 7 ,
0 . 0 4 4 6 2 5 8 8 , − 0 . 0 1 4 7 2 5 9 7 ] ,
[ − 0 . 0 4 0 0 2 6 5 5 ,
0 . 0 3 4 4 1 8 9 5 ,
0 . 0 2 5 8 9 8 3 1 ] ,
0 . 0 0 9 9 8 2 3 ,
[ 0 . 0 2 4 8 0 4 6 4 , −0.02585896 ,
0 . 0 2 5 1 7 6 1 7 , −0.03199299 ,
0 . 0 0 1 2 7 8 4 2 ] ,
[ − 0 . 0 2 5 0 2 6 5 5 ,
0 . 0 2 7 0 9 3 4 4 , −0.04335414 , − 0 . 0 1 7 9 3 2 0 1 ] ,
[ − 0 . 0 0 2 0 5 7 9 7 ,
0 . 0 4 4 5 2 9 5 , − 0 . 0 2 1 6 0 5 5 5 ] ,
[ 0 . 0 3 9 2 6 5 3 7 ,
0 . 0 2 9 3 8 5 5 ,
0 . 0 4 9 0 6 5 8 6 , − 0 . 0 2 3 8 4 9 7 5 ] ,
[ −0.0075082 , −0.03241253 ,
0 . 0 0 1 5 1 9 9 1 ] ,
[ 0 . 0 0 2 6 4 5 2 9 , −0.01921672 , −0.0031809 ,
[ − 0 . 0 2 4 0 7 7 0 5 , −0.04659952 , −0.02667597 , − 0 . 0 4 1 0 8 5 0 4 ] ] ,

dtype=f l o a t 3 2 ) ]

The values above are random parameters that Keras generated as starting points. Generally, we will
transfer an embedding or train these random values into something useful. The following section demon-
strates how to embed a hand-coded embedding.

11.5. PART 11.5: WHAT ARE EMBEDDING LAYERS IN KERAS

415

11.5.2 Transferring An Embedding

Now, we see how to hard-code an embedding lookup that performs a simple one-hot encoding. One-hot
encoding would transform the input integer values of 0, 1, and 2 to the vectors [1, 0, 0], [0, 1, 0], and [0, 0, 1]
respectively. The following code replaced the random lookup values in the embedding layer with this
one-hot coding-inspired lookup table.

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Embedding
import numpy a s np

embedding_lookup = np . a r r a y ( [

[ 1 , 0 , 0 ] ,
[ 0 , 1 , 0 ] ,
[ 0 , 0 , 1 ]

] )

model = S e q u e n t i a l ( )
embedding_layer = Embedding ( input_dim =3, output_dim =3,
model . add ( embedding_layer )
model . compile ( ' adam ' ,

' mse ' )

i n p u t _ l e n g t h =2)

embedding_layer . s e t _ w e i g h t s ( [ embedding_lookup ] )

We have the following parameters for the Embedding layer:

• input_dim=3 - There are three diﬀerent integer categorical values allowed.
• output_dim=3 - Three columns represent a categorical value with three possible values per one-hot

encoding.

• input_length=2 - The input vector has two of these categorical values.

We query the neural network with two categorical values to see the lookup performed.

Code

input_data = np . a r r a y ( [

[ 0 , 1 ]

] )

pred = model . p r e d i c t ( input_data )

print ( input_data . shape )
print ( pred )

416

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

Output

( 1 , 2 )
[ [ [ 1 . 0 . 0 . ]

[ 0 . 1 . 0 . ] ] ]

The given output shows that we provided the program with two rows from the one-hot encoding table.
This encoding is a correct one-hot encoding for the values 0 and 1, where there are up to 3 unique values
possible.

The following section demonstrates how to train this embedding lookup table.

11.5.3 Training an Embedding

First, we make use of the following imports.

Code

from numpy import a r r a y
from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g . t e x t import one_hot
from t e n s o r f l o w . k e r a s . p r e p r o c e s s i n g . s e q u e n c e import pad_sequences
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import F l a t t e n , Embedding , Dense

We create a neural network that classiﬁes restaurant reviews according to positive or negative. This
neural network can accept strings as input, such as given here. This code also includes positive or negative
labels for each review.

Code

# D e f i n e 10 r e s t u r a n t
r e v i e w s = [

r e v i e w s .

' Never ␣ coming ␣ back ! ' ,
' H o r r i b l e ␣ s e r v i c e ' ,
' Rude␣ w a i t r e s s ' ,
' Cold ␣ f o o d . ' ,
' H o r r i b l e ␣ f o o d ! ' ,
' Awesome ' ,
' Awesome␣ s e r v i c e ! ' ,
' Rocks ! ' ,
' poor ␣ work ' ,
' Couldn \ ' t ␣ have ␣ done ␣ b e t t e r ' ]

# D e f i n e l a b e l s (1= n e g a t i v e , 0= p o s i t i v e )
l a b e l s = a r r a y ( [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] )

11.5. PART 11.5: WHAT ARE EMBEDDING LAYERS IN KERAS

417

Notice that the second to the last label is incorrect. Errors such as this are not too out of the ordinary,

as most training data could have some noise.

We deﬁne a vocabulary size of 50 words. Though we do not have 50 words, it is okay to use a value
larger than needed. If there are more than 50 words, the least frequently used words in the training set are
automatically dropped by the embedding layer during training. For input, we one-hot encode the strings.
We use the TensorFlow one-hot encoding method here rather than Scikit-Learn. Scikit-learn would expand
these strings to the 0’s and 1’s as we would typically see for dummy variables. TensorFlow translates all
words to index values and replaces each word with that index.

Code

VOCAB_SIZE = 50
encoded_reviews = [ one_hot ( d , VOCAB_SIZE)
print ( f " Encoded ␣ r e v i e w s : ␣{ encoded_reviews } " )

for d in r e v i e w s ]

Output

Encoded r e v i e w s :
[ 2 0 ] ,

[ 2 0 , 3 1 ] ,

[ [ 4 0 , 4 3 , 7 ] ,

[ 2 7 , 3 1 ] ,

[ 4 9 , 4 6 ] ,

[ 2 , 2 8 ] ,

[ 2 7 , 2 8 ] ,

[ 3 9 ] ,

[ 1 8 , 3 9 ] ,

[ 1 1 , 3 , 1 8 , 1 1 ] ]

The program one-hot encodes these reviews to word indexes; however, their lengths are diﬀerent. We

pad these reviews to 4 words and truncate any words beyond the fourth word.

Code

MAX_LENGTH = 4

padded_reviews = pad_sequences ( encoded_reviews , maxlen=MAX_LENGTH,

padding= ' p o s t ' )

print ( padded_reviews )

Output

[ [ 4 0 43
[ 2 7 31
[ 4 9 46
[ 2 28
[ 2 7 28
[ 2 0
0
[ 2 0 31
0
[ 3 9
[ 1 8 39
[ 1 1

7
0
0
0
0
0
0
0
0

0 ]
0 ]
0 ]
0 ]
0 ]
0 ]
0 ]
0 ]
0 ]

3 18 1 1 ] ]

418

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

As speciﬁed by the padding=post setting, each review is padded by appending zeros at the end, as

speciﬁed by the padding=post setting.

Next, we create a neural network to learn to classify these reviews.

Code

model = S e q u e n t i a l ( )
embedding_layer = Embedding (VOCAB_SIZE, 8 ,
model . add ( embedding_layer )
model . add ( F l a t t e n ( ) )
model . add ( Dense ( 1 , a c t i v a t i o n= ' s i g m o i d ' ) )
model . compile ( o p t i m i z e r= ' adam ' ,

i n p u t _ l e n g t h=MAX_LENGTH)

l o s s= ' b i n a r y _ c r o s s e n t r o p y ' ,

m e t r i c s =[ ' a c c ' ] )

print ( model . summary ( ) )

Output

Model :
_________________________________________________________________

" s e q u e n t i a l _ 2 "

Layer

( type )

Output Shape

Param #

=================================================================

embedding_2 ( Embedding )
f l a t t e n ( F l a t t e n )
d e n s e ( Dense )

( None , 4 , 8 )
( None , 3 2 )
( None , 1 )

400
0
33

=================================================================
T o t a l params : 433
T r a i n a b l e params : 433
Non−t r a i n a b l e params : 0
_________________________________________________________________
None

This network accepts four integer inputs that specify the indexes of a padded movie review. The ﬁrst
embedding layer converts these four indexes into four length vectors 8. These vectors come from the
lookup table that contains 50 (VOCAB_SIZE) rows of vectors of length 8. This encoding is evident by the
400 (8 times 50) parameters in the embedding layer. The output size from the embedding layer is 32 (4
words expressed as 8-number embedded vectors). A single output neuron is connected to the embedding
layer by 33 weights (32 from the embedding layer and a single bias neuron). Because this is a single-class
classiﬁcation network, we use the sigmoid activation function and binary_crossentropy.

The program now trains the neural network. The embedding lookup and dense 33 weights are updated

to produce a better score.

11.5. PART 11.5: WHAT ARE EMBEDDING LAYERS IN KERAS

419

Code

t h e model

# f i t
model . f i t ( padded_reviews ,

l a b e l s , e p o ch s =100 , v e r b o s e =0)

Output

We can see the learned embeddings. Think of each word’s vector as a location in the 8 dimension
space where words associated with positive reviews are close to other words. Similarly, training places
negative reviews close to each other. In addition to the training setting these embeddings, the 33 weights
between the embedding layer and output neuron similarly learn to transform these embeddings into an
actual prediction. You can see these embeddings here.

Code

print ( embedding_layer . g e t _ w e i g h t s ( ) [ 0 ] . shape )
print ( embedding_layer . g e t _ w e i g h t s ( ) )

Output

( 5 0 , 8 )
[ a r r a y ( [ [ − 0 . 1 1 3 8 9 5 5 9 , −0.04778124 ,
0 . 0 5 6 7 0 2 5 9 ,

0 . 1 0 0 3 4 3 8 7 ,

0 . 1 2 8 8 7 0 3 7 ,

−0.09982903 , −0.15423775 , − 0 . 0 6 7 7 4 8 0 5 ] ,
0 . 0 0 5 2 7 7 4 5 ,

[ − 0 . 0 4 8 3 9 2 4 6 ,

0 . 0 0 8 4 3 0 6 , −0.03498586 ,

0 . 0 1 0 7 7 2

,

,

0 . 0 4 0 1 5 7 1 1 ,

0 . 0 3 5 6 4 4 5 2 , − 0 . 0 0 8 4 9 3 3 6 ] ,

[ − 0 . 1 1 0 0 3 1 5 7 , −0.05829103 ,

0 . 1 2 3 7 0 5 3 5 , −0.07124459 , −0.0667479

−0.14339209 , −0.13791779 , − 0 . 1 3 9 4 7 7 2 1 ] ,

[ − 0 . 1 5 3 9 5 7 6 5 , −0.08560142 , −0.15915371 , −0.0882007 ,

0 . 1 5 7 5 6 0 0 4 ,

−0.10337664 , −0.12412377 , − 0 . 1 0 2 8 2 9 6 1 ] ,

[ 0 . 0 4 9 1 9 6 3 7 , −0.00870635 , −0.02393281 ,

0 . 0 4 4 4 5 9 5 3 ,

0 . 0 1 2 4 3 5 1

,

. . .

0 . 0 4 1 5 3 9 6 4 ,

−0.04445877 , −0.00612149 , − 0 . 0 3 4 3 0 6 6 3 ] ,

[ − 0 . 0 8 4 9 3 9 2 8 , −0.10910758 ,

0 . 0 6 0 5 1 7 8 , −0.10072854 ,

420

CHAPTER 11. NATURAL LANGUAGE PROCESSING WITH HUGGING FACE

−0.11677803 ,

−0.05648913 , −0.13342443 , − 0 . 0 8 5 1 6 3 1 8 ] ] , dtype=f l o a t 3 2 ) ]

We can now evaluate this neural network’s accuracy, including the embeddings and the learned dense

layer.

Code

l o s s , a c c u r a c y = model . e v a l u a t e ( padded_reviews ,
print ( f ' Accuracy : ␣{ a c c u r a c y } ' )

l a b e l s , v e r b o s e =0)

Output

Accuracy : 1 . 0

The accuracy is a perfect 1.0, indicating there is likely overﬁtting.

It would be good to use early

stopping to not overﬁt for a more complex data set.

Code

print ( f ' Log−l o s s : ␣{ l o s s } ' )

Output

Log−l o s s : 0 . 4 8 4 4 6 8 6 3 8 8 9 6 9 4 2 1 4

However, the loss is not perfect. Even though the predicted probabilities indicated a correct prediction in
every case, the program did not achieve absolute conﬁdence in each correct answer. The lack of conﬁdence
was likely due to the small amount of noise (previously discussed) in the data set. Some words that
appeared in both positive and negative reviews contributed to this lack of absolute certainty.

Chapter 12

Reinforcement Learning

12.1 Part 12.1: Introduction to the OpenAI Gym

OpenAI Gym aims to provide an easy-to-setup general-intelligence benchmark with various environments.
The goal is to standardize how environments are deﬁned in AI research publications to make published
research more easily reproducible. The project claims to provide the user with a simple interface. As of
June 2017, developers can only use Gym with Python.

OpenAI gym is pip-installed onto your local machine. There are a few signiﬁcant limitations to be

aware of:

• OpenAI Gym Atari only directly supports Linux and Macintosh
• OpenAI Gym Atari can be used with Windows; however, it requires a particular installation procedure
• OpenAI Gym can not directly render animated games in Google CoLab.

Because OpenAI Gym requires a graphics display, an embedded video is the only way to display Gym in
Google CoLab. The presentation of OpenAI Gym game animations in Google CoLab is discussed later in
this module.

12.1.1 OpenAI Gym Leaderboard

The OpenAI Gym does have a leaderboard, similar to Kaggle; however, the OpenAI Gym’s leaderboard is
much more informal compared to Kaggle. The user’s local machine performs all scoring. As a result, the
OpenAI gym’s leaderboard is strictly an "honor system." The leaderboard is maintained in the following
GitHub repository:

• OpenAI Gym Leaderboard

You must provide a write-up with suﬃcient instructions to reproduce your result if you submit a score. A
video of your results is suggested but not required.

421

422

CHAPTER 12. REINFORCEMENT LEARNING

12.1.2 Looking at Gym Environments

The centerpiece of Gym is the environment, which deﬁnes the "game" in which your reinforcement algorithm
will compete. An environment does not need to be a game; however, it describes the following game-like
features:

• action space: What actions can we take on the environment at each step/episode to alter the

environment.

• observation space: What is the current state of the portion of the environment that we can observe.

Usually, we can see the entire environment.

Before we begin to look at Gym, it is essential to understand some of the terminology used by this library.

• Agent - The machine learning program or model that controls the actions.

Step - One round of issuing actions that aﬀect the observation space.

• Episode - A collection of steps that terminates when the agent fails to meet the environment’s

objective or the episode reaches the maximum number of allowed steps.

• Render - Gym can render one frame for display after each episode.
• Reward - A positive reinforcement that can occur at the end of each episode, after the agent acts.
• Non-deterministic - For some environments, randomness is a factor in deciding what eﬀects actions

have on reward and changes to the observation space.

It is important to note that many gym environments specify that they are not non-deterministic even
though they use random numbers to process actions. Based on the gym GitHub issue tracker, a non-
deterministic property means a deterministic environment behaves randomly. Even when you give the
environment a consistent seed value, this behavior is conﬁrmed. The program can use the seed method of
an environment to seed the random number generator for the environment.

The Gym library allows us to query some of these attributes from environments. I created the following

function to query gym environments.

Code

import gym

def query_environment ( name ) :
env = gym . make ( name )
s p e c = gym . s p e c ( name )
print ( f " Action ␣ Space : ␣{ env . a c t i o n _ s p a c e } " )
print ( f " O b s e r v a t i o n ␣ Space : ␣{ env . o b s e r v a t i o n _ s p a c e } " )
print ( f "Max␣ Episode ␣ S t e p s : ␣{ s p e c . max_episode_steps } " )
print ( f " N o n d e t e r m i n i s t i c : ␣{ s p e c . n o n d e t e r m i n i s t i c } " )
print ( f " Reward␣Range : ␣{ env . reward_range } " )
print ( f " Reward␣ T hres h ol d : ␣{ s p e c . r e w a r d _ t h r e s h o l d } " )

12.1. PART 12.1: INTRODUCTION TO THE OPENAI GYM

423

We will look at the MountainCar-v0 environment, which challenges an underpowered car to escape

the valley between two mountains. The following code describes the Mountian Car environment.

Code

query_environment ( " MountainCar−v0 " )

Output

Action Space : D i s c r e t e ( 3 )
O b s e r v a t i o n Space : Box ( −1.2000000476837158 , 0 . 6 0 0 0 0 0 0 2 3 8 4 1 8 5 7 9 ,
f l o a t 3 2 )
Max Episode S t e p s : 200
N o n d e t e r m i n i s t i c : F a l s e
Reward Range : (− i n f ,
Reward Thre shol d : −110.0

i n f )

( 2 , ) ,

This environment allows three distinct actions: accelerate forward, decelerate, or backward. The obser-
vation space contains two continuous (ﬂoating point) values, as evident by the box object. The observation
space is simply the position and velocity of the car. The car has 200 steps to escape for each episode. You
would have to look at the code, but the mountain car receives no incremental reward. The only reward for
the vehicle occurs when it escapes the valley.

Code

query_environment ( " CartPole−v1 " )

Output

Action Space : D i s c r e t e ( 2 )
O b s e r v a t i o n Space : Box ( −3.4028234663852886 e +38 ,
3 . 4 0 2 8 2 3 4 6 6 3 8 5 2 8 8 6 e +38 ,
Max Episode S t e p s : 500
N o n d e t e r m i n i s t i c : F a l s e
Reward Range : (− i n f ,
Reward T h re shol d : 4 7 5 . 0

f l o a t 3 2 )

( 4 , ) ,

i n f )

The CartPole-v1 environment challenges the agent to balance a pole while the agent. The environment

has an observation space of 4 continuous numbers:

• Cart Position
• Cart Velocity
• Pole Angle

424

CHAPTER 12. REINFORCEMENT LEARNING

• Pole Velocity At Tip

To achieve this goal, the agent can take the following actions:

• Push cart to the left
• Push cart to the right

There is also a continuous variant of the mountain car. This version does not simply have the motor on
or oﬀ. The action space is a single ﬂoating-point number for the continuous cart that speciﬁes how much
forward or backward force the cart currently utilizes.

Code

query_environment ( " MountainCarContinuous−v0 " )

Output

f l o a t 3 2 )

Action Space : Box ( −1.0 , 1 . 0 ,
( 1 , ) ,
O b s e r v a t i o n Space : Box ( −1.2000000476837158 , 0 . 6 0 0 0 0 0 0 2 3 8 4 1 8 5 7 9 ,
f l o a t 3 2 )
Max Episode S t e p s : 999
N o n d e t e r m i n i s t i c : F a l s e
Reward Range : (− i n f ,
Reward Thre shol d : 9 0 . 0

i n f )

( 2 , ) ,

Note: If you see a warning above, you can safely ignore it; it is a relatively minor bug in OpenAI Gym.
Atari games, like breakout, can use an observation space that is either equal to the size of the Atari
screen (210x160) or even use the RAM of the Atari (128 bytes) to determine the state of the game. Yes,
that’s bytes, not kilobytes!

Code

! wget h t t p : / /www. a t a r i m a n i a . com/ roms /Roms . r a r
! u n r a r x −o+ / c o n t e n t /Roms . r a r >/dev / n u l
! python −m a t a r i _ p y . import_roms / c o n t e n t /ROMS >/dev / n u l

Code

query_environment ( " Breakout−v0 " )

Output

Action Space : D i s c r e t e ( 4 )
O b s e r v a t i o n Space : Box ( 0 , 2 5 5 ,

( 2 1 0 , 1 6 0 , 3 ) , u i n t 8 )

12.1. PART 12.1: INTRODUCTION TO THE OPENAI GYM

425

Max Episode S t e p s : 10000
N o n d e t e r m i n i s t i c : F a l s e
Reward Range : (− i n f ,
Reward Thre shol d : None

i n f )

Code

query_environment ( " Breakout−ram−v0 " )

Output

Action Space : D i s c r e t e ( 4 )
O b s e r v a t i o n Space : Box ( 0 , 2 5 5 ,
Max Episode S t e p s : 10000
N o n d e t e r m i n i s t i c : F a l s e
Reward Range : (− i n f ,
Reward Thre shol d : None

i n f )

( 1 2 8 , ) , u i n t 8 )

12.1.3 Render OpenAI Gym Environments from CoLab

It is possible to visualize the game your agent is playing, even on CoLab. This section provides information
on generating a video in CoLab that shows you an episode of the game your agent is playing. I based this
video process on suggestions found here.

Begin by installing pyvirtualdisplay and python-opengl.

Code

! p i p i n s t a l l gym p y v i r t u a l d i s p l a y > / dev / n u l l 2>&1
! apt−g e t

i n s t a l l −y xvfb python−o p e n g l

ffmpeg > / dev / n u l l 2>&1

Next, we install the needed requirements to display an Atari game.

Code

i n s t a l l cmake > / dev / n u l l 2>&1

! apt−g e t update > / dev / n u l l 2>&1
! apt−g e t
! p i p i n s t a l l −−upgrade s e t u p t o o l s 2>&1
! p i p i n s t a l l ez_setup > / dev / n u l l 2>&1
! p i p i n s t a l l gym [ a t a r i ] > / dev / n u l l 2>&1

Next, we deﬁne the functions used to show the video by adding it to the CoLab notebook.

426

Code

CHAPTER 12. REINFORCEMENT LEARNING

import gym
from gym . wrappers import Monitor
import g l o b
import i o
import b a se 6 4
from IPython . d i s p l a y import HTML
from p y v i r t u a l d i s p l a y import D i s p l a y
from IPython import d i s p l a y a s

i p y t h o n d i s p l a y

d i s p l a y = D i s p l a y ( v i s i b l e =0,
d i s p l a y . s t a r t ( )

s i z e =(1400 , 9 0 0 ) )

" " "
U t i l i t y f u n c t i o n s
and d i s p l a y i n g
To e n a b l e v i d e o ,
" " "

i t .

t o e n a b l e v i d e o r e c o r d i n g o f gym environment

j u s t do " env = wrap_env ( env ) " "

def show_video ( ) :

m p 4 l i s t = g l o b . g l o b ( ' v i d e o / ∗ . mp4 ' )
i f

len ( m p 4 l i s t ) > 0 :
mp4 = m p 4 l i s t [ 0 ]
v i d e o = i o . open (mp4 ,
encoded = b a s e6 4 . b64encode ( v i d e o )
i p y t h o n d i s p l a y . d i s p l a y (HTML( data= ' ' '<v i d e o a l t =" t e s t " a u t o p l a y

' r+b ' ) . r e a d ( )

l o o p c o n t r o l s
<s o u r c e s r c =" d a t a : v i d e o /mp4 ; base64 , { 0 } "
</v i d e o > ' ' ' . format ( encoded . decode ( ' a s c i i ' ) ) ) )

s t y l e =" h e i g h t : 400 px ;" >

t y p e =" v i d e o /mp4" />

e l s e :

print ( " Could ␣ not ␣ f i n d ␣ v i d e o " )

def wrap_env ( env ) :

env = Monitor ( env ,
return env

' . / v i d e o ' ,

f o r c e=True )

Now we are ready to play the game. We use a simple random agent.

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

427

Code

#env = wrap_env ( gym . make ( " MountainCar−v0 " ) )
env = wrap_env (gym . make ( " A t l a n t i s −v0 " ) )

o b s e r v a t i o n = env . r e s e t ( )

while True :

env . r e n d e r ( )

# your a g e n t g o e s h e r e
a c t i o n = env . a c t i o n _ s p a c e . sample ( )

o b s e r v a t i o n ,

reward , done ,

i n f o = env . s t e p ( a c t i o n )

i f done :

break

env . c l o s e ( )
show_video ( )

12.2 Part 12.2: Introduction to Q-Learning

Q-Learning is a foundational technology upon which deep reinforcement learning is based. Before we
explore deep reinforcement learning, it is essential to understand Q-Learning. Several components make
up any Q-Learning system.

• Agent - The agent is an entity that exists in an environment that takes actions to aﬀect the state

of the environment, to receive rewards.

• Environment - The environment is the universe that the agent exists in. The environment is always

in a speciﬁc state that is changed by the agent’s actions.

• Actions - Steps that the agent can perform to alter the environment
• Step - A step occurs when the agent performs an action and potentially changes the environment

state.

• Episode - A chain of steps that ultimately culminates in the environment entering a terminal state.
• Epoch - A training iteration of the agent that contains some number of episodes.
• Terminal State - A state in which further actions do not make sense. A terminal state occurs
when the agent has one, lost, or the environment exceeds the maximum number of steps in many
environments.

Q-Learning works by building a table that suggests an action for every possible state. This approach runs
into several problems. First, the environment is usually composed of several continuous numbers, resulting

428

CHAPTER 12. REINFORCEMENT LEARNING

in an inﬁnite number of states. Q-Learning handles continuous states by binning these numeric values into
ranges.

Out of the box, Q-Learning does not deal with continuous inputs, such as a car’s accelerator that
can range from released to fully engaged. Additionally, Q-Learning primarily deals with discrete actions,
such as pressing a joystick up or down. Researchers have developed clever tricks to allow Q-Learning to
accommodate continuous actions.

Deep neural networks can help solve the problems of continuous environments and action spaces. In
the next section, we will learn more about deep reinforcement learning. For now, we will apply regular
Q-Learning to the Mountain Car problem from OpenAI Gym.

12.2.1 Introducing the Mountain Car

This section will demonstrate how Q-Learning can create a solution to the mountain car gym environment.
The Mountain car is an environment where a car must climb a mountain. Because gravity is stronger
than the car’s engine, it cannot merely accelerate up the steep slope even with full throttle. The vehicle
is situated in a valley and must learn to utilize potential energy by driving up the opposite hill before the
car can make it to the goal at the top of the rightmost hill.

First, it might be helpful to visualize the mountain car environment. The following code shows this
environment. This code makes use of TF-Agents to perform this render. Usually, we use TF-Agents for
the type of deep reinforcement learning that we will see in the next module. However, TF-Agents is just
used to render the mountain care environment for now.

Code

import t f _ a g e n t s
from t f _ a g e n t s . e n v i r o n m e n t s import suite_gym
import PIL . Image
import p y v i r t u a l d i s p l a y

d i s p l a y = p y v i r t u a l d i s p l a y . D i s p l a y ( v i s i b l e =0,

s i z e =(1400 , 9 0 0 ) ) . s t a r t ( )

env_name = ' MountainCar−v0 '
env = suite_gym . l o a d ( env_name )
env . r e s e t ( )
PIL . Image . f r o m a r r a y ( env . r e n d e r ( ) )

Output

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

429

The mountain car environment provides the following discrete actions:

• 0 - Apply left force
• 1 - Apply no force
• 2 - Apply right force

The mountain car environment is made up of the following continuous values:

• state[0] - Position
• state[1] - Velocity

The cart is not strong enough. It will need to use potential energy from the mountain behind it. The
following code shows an agent that applies full throttle to climb the hill.

Code

import gym
from gym . wrappers import Monitor
import g l o b
import i o
import b a se 6 4
from IPython . d i s p l a y import HTML
from p y v i r t u a l d i s p l a y import D i s p l a y
from IPython import d i s p l a y a s

i p y t h o n d i s p l a y

d i s p l a y = D i s p l a y ( v i s i b l e =0,
d i s p l a y . s t a r t ( )

s i z e =(1400 , 9 0 0 ) )

430

CHAPTER 12. REINFORCEMENT LEARNING

def show_video ( ) :

m p 4 l i s t = g l o b . g l o b ( ' v i d e o / ∗ . mp4 ' )
i f

len ( m p 4 l i s t ) > 0 :
mp4 = m p 4 l i s t [ 0 ]
v i d e o = i o . open (mp4 ,
encoded = b a s e6 4 . b64encode ( v i d e o )
i p y t h o n d i s p l a y . d i s p l a y (HTML( data= ' ' '<v i d e o a l t =" t e s t " a u t o p l a y

' r+b ' ) . r e a d ( )

l o o p c o n t r o l s
<s o u r c e s r c =" d a t a : v i d e o /mp4 ; base64 , { 0 } "
t y p e =" v i d e o /mp4" />

s t y l e =" h e i g h t : 400 px ;" >

</v i d e o > ' ' ' . format ( encoded . decode ( ' a s c i i ' ) ) ) )

e l s e :

print ( " Could ␣ not ␣ f i n d ␣ v i d e o " )

def wrap_env ( env ) :

env = Monitor ( env ,
return env

' . / v i d e o ' ,

f o r c e=True )

We are now ready to train the agent.

Code

import gym

i f COLAB:

env = wrap_env (gym . make ( " MountainCar−v0 " ) )

e l s e :

env = gym . make ( " MountainCar−v0 " )

env . r e s e t ( )
done = F a l s e

i = 0
while not done :
i += 1
s t a t e ,
env . r e n d e r ( )
print ( f " Step ␣{ i } : ␣ S t a t e={ s t a t e } , ␣Reward={reward } " )

reward , done , _ = env . s t e p ( 2 )

env . c l o s e ( )

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

431

Output

Step 1 : S t a t e =[ −0.50905189
Step 2 : S t a t e =[ −0.50726329
Step 3 : S t a t e =[ −0.50459717
Step 4 : S t a t e =[ −0.50107348
Step 5 : S t a t e =[ −0.4967186
Step 6 : S t a t e =[ −0.4915651
Step 7 : S t a t e =[ −0.48565149
Step 8 : S t a t e =[ −0.47902187
Step 9 : S t a t e =[ −0.47172557
Step 1 0 : S t a t e =[ −0.46381676
Step 1 1 : S t a t e =[ −0.45535392
Step 1 2 : S t a t e =[ −0.44639934
Step 1 3 : S t a t e =[ −0.4370186
Step 1 4 : S t a t e =[ −0.42727993
Step 1 5 : S t a t e =[ −0.41725364

0 . 0 0 0 8 9 7 6 6 ] , Reward=−1.0
0 . 0 0 1 7 8 8 5 9 ] , Reward=−1.0
0 . 0 0 2 6 6 6 1 3 ] , Reward=−1.0
0 . 0 0 3 5 2 3 6 9 ] , Reward=−1.0
0 . 0 0 4 3 5 4 8 8 ] , Reward=−1.0

0 . 0 0 5 1 5 3 5 ] , Reward=−1.0

0 . 0 0 5 9 1 3 6 1 ] , Reward=−1.0
0 . 0 0 6 6 2 9 6 2 ] , Reward=−1.0
0 . 0 0 7 2 9 6 2 9 ] , Reward=−1.0

0 . 0 0 7 9 0 8 8 1 ] , Reward=−1.0
0 . 0 0 8 4 6 2 8 5 ] , Reward=−1.0
0 . 0 0 8 9 5 4 5 8 ] , Reward=−1.0
0 . 0 0 9 3 8 0 7 4 ] , Reward=−1.0
0 . 0 0 9 7 3 8 6 7 ] , Reward=−1.0
0 . 0 1 0 0 2 6 2 9 ] , Reward=−1.0

. . .

Step 1 9 6 : S t a t e =[ −0.26463414 − 0 . 0 0 3 3 6 8 1 8 ] , Reward=−1.0
Step 1 9 7 : S t a t e =[ −0.26875498 − 0 . 0 0 4 1 2 0 8 5 ] , Reward=−1.0
Step 1 9 8 : S t a t e =[ −0.27360632 − 0 . 0 0 4 8 5 1 3 4 ] , Reward=−1.0
Step 1 9 9 : S t a t e =[ −0.27916172 −0.0055554 ] , Reward=−1.0
Step 2 0 0 : S t a t e =[ −0.28539045 − 0 . 0 0 6 2 2 8 7 3 ] , Reward=−1.0

It helps to visualize the car. The following code shows a video of the car when run from a notebook.

Code

show_video ( )

12.2.2 Programmed Car

Now we will look at a car that I hand-programmed. This car is straightforward; however, it solves the
problem. The programmed car always applies force in one direction or another. It does not break. Whatever
direction the vehicle is currently rolling, the agent uses power in that direction. Therefore, the car begins
to climb a hill, is overpowered, and turns backward. However, once it starts to roll backward, force is
immediately applied in this new direction.

The following code implements this preprogrammed car.

432

Code

import gym

i f COLAB:

CHAPTER 12. REINFORCEMENT LEARNING

env = wrap_env (gym . make ( " MountainCar−v0 " ) )

e l s e :

env = gym . make ( " MountainCar−v0 " )

s t a t e = env . r e s e t ( )
done = F a l s e

i = 0
while not done :
i += 1

i f

s t a t e [ 1 ] > 0 :
a c t i o n = 2

e l s e :

a c t i o n = 0

reward , done , _ = env . s t e p ( a c t i o n )

s t a t e ,
env . r e n d e r ( )
print ( f " Step ␣{ i } : ␣ S t a t e={ s t a t e } , ␣Reward={reward } " )

env . c l o s e ( )

Output

Step 1 : S t a t e =[ −5.84581471 e −01 −5.49227966 e −04] , Reward=−1.0
Step 2 : S t a t e =[ −0.58567588 −0.0010944 ] , Reward=−1.0
Step 3 : S t a t e =[ −0.58730739 − 0 . 0 0 1 6 3 1 5 1 ] , Reward=−1.0
Step 4 : S t a t e =[ −0.58946399 −0.0021566 ] , Reward=−1.0
Step 5 : S t a t e =[ −0.59212981 − 0 . 0 0 2 6 6 5 8 2 ] , Reward=−1.0
Step 6 : S t a t e =[ −0.59528526 − 0 . 0 0 3 1 5 5 4 5 ] , Reward=−1.0
Step 7 : S t a t e =[ −0.5989072 − 0 . 0 0 3 6 2 1 9 4 ] , Reward=−1.0
Step 8 : S t a t e =[ −0.60296912 − 0 . 0 0 4 0 6 1 9 2 ] , Reward=−1.0
Step 9 : S t a t e =[ −0.60744137 − 0 . 0 0 4 4 7 2 2 5 ] , Reward=−1.0
Step 1 0 : S t a t e =[ −0.61229141 − 0 . 0 0 4 8 5 0 0 4 ] , Reward=−1.0
Step 1 1 : S t a t e =[ −0.61748407 − 0 . 0 0 5 1 9 2 6 7 ] , Reward=−1.0
Step 1 2 : S t a t e =[ −0.62298187 −0.0054978 ] , Reward=−1.0
Step 1 3 : S t a t e =[ −0.62874529 − 0 . 0 0 5 7 6 3 4 2 ] , Reward=−1.0
Step 1 4 : S t a t e =[ −0.63473313 − 0 . 0 0 5 9 8 7 8 3 ] , Reward=−1.0

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

433

Step 1 5 : S t a t e =[ −0.64090281 − 0 . 0 0 6 1 6 9 6 8 ] , Reward=−1.0

. . .

Step 1 4 9 : S t a t e = [ 0 . 3 0 9 7 5 4 8 7 0 . 0 4 9 4 7 6 6 5 ] , Reward=−1.0
Step 1 5 0 : S t a t e = [ 0 . 3 5 8 7 3 5 4 7 0 . 0 4 8 9 8 0 6 ] , Reward=−1.0
Step 1 5 1 : S t a t e = [ 0 . 4 0 7 5 2 9 3 9 0 . 0 4 8 7 9 3 9 2 ] , Reward=−1.0
Step 1 5 2 : S t a t e = [ 0 . 4 5 6 4 7 0 2 7 0 . 0 4 8 9 4 0 8 8 ] , Reward=−1.0
Step 1 5 3 : S t a t e = [ 0 . 5 0 5 9 1 1 0 9 0 . 0 4 9 4 4 0 8 2 ] , Reward=−1.0

We now visualize the preprogrammed car solving the problem.

Code

show_video ( )

12.2.3 Reinforcement Learning

Q-Learning is a system of rewards that the algorithm gives an agent for successfully moving the environment
into a state considered successful. These rewards are the Q-values from which this algorithm takes its name.
The ﬁnal output from the Q-Learning algorithm is a table of Q-values that indicate the reward value of
every action that the agent can take, given every possible environment state. The agent must bin continuous
state values into a ﬁxed ﬁnite number of columns.

Learning occurs when the algorithm runs the agent and environment through episodes and updates the
Q-values based on the rewards received from actions taken; Figure 12.1 provides a high-level overview of
this reinforcement or Q-Learning loop.

Figure 12.1: Reinforcement/Q Learning

The Q-values can dictate action by selecting the action column with the highest Q-value for the current
environment state. The choice between choosing a random action and a Q-value-driven action is governed
by the epsilon ((cid:15)) parameter, the probability of random action.

Each time through the training loop, the training algorithm updates the Q-values according to the

following equation.

434

CHAPTER 12. REINFORCEMENT LEARNING

Qnew(st, at) ← Q(st, at)
|
}
{z
old value

+

α
|{z}
learning rate

·

z
(cid:18)

+

rt
|{z}
reward

γ
|{z}
discount factor

·

temporal diﬀerence
}|
max
a

Q(st+1, a)
}

{
(cid:19)

− Q(st, at)
}
{z
|
old value

|

|

{z
estimate of optimal future value
}

{z
new value (temporal diﬀerence target)

There are several parameters in this equation:
* alpha (α) - The learning rate, how much should the current step cause the Q-values to be updated.
* lambda (λ) - The discount factor is the percentage of future reward that the algorithm should consider
in this update.

This equation modiﬁes several values:

* Q(st, at) - The Q-table. For each combination of states, what reward would the agent likely receive
for performing each action?
* st - The current state.
* rt - The last reward received.
* at - The action that the agent will perform.

The equation works by calculating a delta (temporal diﬀerence) that the equation should apply to the
old state. This learning rate (α) scales this delta. A learning rate of 1.0 would fully implement the tem-
poral diﬀerence in the Q-values each iteration and would likely be very chaotic.

There are two parts to the temporal diﬀerence: the new and old values. The new value is subtracted
from the old value to provide a delta; the full amount we would change the Q-value by if the learning rate
did not scale this value. The new value is a summation of the reward received from the last action and
the maximum Q-values from the resulting state when the client takes this action. Adding the maximum of
action Q-values for the new state is essential because it estimates the optimal future values from proceeding
with this action.

## Q-Learning Car

We will now use Q-Learning to produce a car that learns to drive itself. Look out, Tesla! We begin
by deﬁning two essential functions.

Code

import gym
import numpy a s np

f u n c t i o n c o n v e r t s

# This
# d i s c r e t e v a l u e s . This
# t h e range t h a t
# each r e g i o n t o a b u c k e t .

t h e f l o a t i n g p o i n t

s t a t e v a l u e s

i n t o

i s o f t e n c a l l e d b i n n i n g . We d i v i d e

t h e s t a t e v a l u e s might occupy and a s s i g n

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

435

def c a l c _ d i s c r e t e _ s t a t e ( s t a t e ) :

d i s c r e t e _ s t a t e = ( s t a t e − env . o b s e r v a t i o n _ s p a c e . low ) / b u c k e t s
return tuple ( d i s c r e t e _ s t a t e . a s t y p e ( int ) )

i f

# Run one game . The q _ t a b l e t o use i s p r o v i d e d . We a l s o
# p r o v i d e a f l a g t o i n d i c a t e
# r e n d e r e d / animated .
# a f l a g t o i n d i c a t e
def run_game ( q_table ,
done = F a l s e
d i s c r e t e _ s t a t e = c a l c _ d i s c r e t e _ s t a t e ( env . r e s e t ( ) )
s u c c e s s = F a l s e

t h e q _ t a b l e s h o u l d be u p d a t e d .

F i n a l l y , we a l s o p r o v i d e

t h e game s h o u l d be

should_update ) :

r e n d e r ,

i f

while not done :

# E x p l o i t or e x p l o r e
i f np . random . random ( ) > e p s i l o n :

# E x p l o i t − use q−t a b l e t o t a k e c u r r e n t b e s t a c t i o n
# ( and p r o b a b l y r e f i n e )
a c t i o n = np . argmax ( q_table [ d i s c r e t e _ s t a t e ] )

e l s e :

# E x p l o r e − t
a c t i o n = np . random . r a n d i n t ( 0 , env . a c t i o n _ s p a c e . n )

# Run s i m u l a t i o n s t e p
new_state ,

reward , done , _ = env . s t e p ( a c t i o n )

# Convert c o n t i n u o u s
new_state_disc = c a l c _ d i s c r e t e _ s t a t e ( new_state )

s t a t e t o d i s c r e t e

# Have we r e a c h e d t h e g o a l p o s i t i o n ( have we won ?)?
i f new_state [ 0 ] >= env . unwrapped . g o a l _ p o s i t i o n :

s u c c e s s = True

# Update q−t a b l e
i f

should_update :

max_future_q = np .max( q_table [ new_state_disc ] )
current_q = q_table [ d i s c r e t e _ s t a t e + ( a c t i o n , ) ]
new_q = ( 1 − LEARNING_RATE) ∗ current_q + LEARNING_RATE ∗ \

( reward + DISCOUNT ∗ max_future_q )
q_table [ d i s c r e t e _ s t a t e + ( a c t i o n , ) ] = new_q

d i s c r e t e _ s t a t e = new_state_disc

436

CHAPTER 12. REINFORCEMENT LEARNING

i f

r e n d e r :

env . r e n d e r ( )

return s u c c e s s

Several hyperparameters are very important for Q-Learning. These parameters will likely need adjust-
ment as you apply Q-Learning to other problems. Because of this, it is crucial to understand the role of
each parameter.

• LEARNING_RATE The rate at which previous Q-values are updated based on new episodes run

during training.

• DISCOUNT The amount of signiﬁcance to give estimates of future rewards when added to the
reward for the current action taken. A value of 0.95 would indicate a discount of 5% on the future
reward estimates.

• EPISODES The number of episodes to train over. Increase this for more complex problems; how-

ever, training time also increases.

• SHOW_EVERY How many episodes to allow to elapse before showing an update.
• DISCRETE_GRID_SIZE How many buckets to use when converting each continuous state
variable. For example, [10, 10] indicates that the algorithm should use ten buckets for the ﬁrst and
second state variables.

• START_EPSILON_DECAYING Epsilon is the probability that the agent will select a random
action over what the Q-Table suggests. This value determines the starting probability of randomness.
• END_EPSILON_DECAYING How many episodes should elapse before epsilon goes to zero
and no random actions are permitted. For example, EPISODES//10 means only the ﬁrst 1/10th of
the episodes might have random actions.

Code

LEARNING_RATE = 0 . 1
DISCOUNT = 0 . 9 5
EPISODES = 50000
SHOW_EVERY = 1000

DISCRETE_GRID_SIZE = [ 1 0 , 1 0 ]
START_EPSILON_DECAYING = 0 . 5
END_EPSILON_DECAYING = EPISODES//10

We can now make the environment. If we are running in Google COLAB, we wrap the environment to be
displayed inside the web browser. Next, create the discrete buckets for state and build Q-table.

Code

i f COLAB:

env = wrap_env (gym . make ( " MountainCar−v0 " ) )

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

437

e l s e :

env = gym . make ( " MountainCar−v0 " )

e p s i l o n = 1
e p s i l o n _ c h a n g e = e p s i l o n / (END_EPSILON_DECAYING − START_EPSILON_DECAYING)
b u c k e t s = ( env . o b s e r v a t i o n _ s p a c e . h i g h − env . o b s e r v a t i o n _ s p a c e . low ) \

/ DISCRETE_GRID_SIZE

q_table = np . random . uniform ( low=−3, h i g h =0,

s i z e =(DISCRETE_GRID_SIZE

+ [ env . a c t i o n _ s p a c e . n ] ) )

s u c c e s s = F a l s e

We can now make the environment. If we are running in Google COLAB, we wrap the environment to

be displayed inside the web browser. Next, create the discrete buckets for state and build Q-table.

Code

e p i s o d e = 0
s u c c e s s _ c o u n t = 0

# Loop t h r o u g h t h e r e q u i r e d number o f e p i s o d e s
while e p i s o d e < EPISODES :

e p i s o d e += 1
done = F a l s e

# Run t h e game .
# a t SHOW_EVERY i n t e r v a l s .
i f

e p i s o d e % SHOW_EVERY == 0 :

I f we a r e l o c a l , d i s p l a y r e n d e r animation

print ( f " Current ␣ e p i s o d e : ␣ { e p i s o d e } , ␣ s u c c e s s : ␣ { s u c c e s s _ c o u n t } " +

f " ␣ { ( f l o a t ( s u c c e s s _ c o u n t ) /SHOW_EVERY) } " )

s u c c e s s = run_game ( q_table , True , F a l s e )
s u c c e s s _ c o u n t = 0

e l s e :

s u c c e s s = run_game ( q_table , F a l s e , True )

# Count s u c c e s s e s
i f

s u c c e s s :

s u c c e s s _ c o u n t += 1

# Move e p s i l o n t o w a r d s
i f END_EPSILON_DECAYING >= e p i s o d e >= START_EPSILON_DECAYING:

i t s e n d i n g v a l u e ,

s t i l l n ee ds

i t

i f

t o move

e p s i l o n = max( 0 , e p s i l o n − e p s i l o n _ c h a n g e )

print ( s u c c e s s )

438

Output

CHAPTER 12. REINFORCEMENT LEARNING

Current e p i s o d e : 1 0 0 0 ,
Current e p i s o d e : 2 0 0 0 ,
Current e p i s o d e : 3 0 0 0 ,
Current e p i s o d e : 4 0 0 0 ,
Current e p i s o d e : 5 0 0 0 ,
Current e p i s o d e : 6 0 0 0 ,
Current e p i s o d e : 7 0 0 0 ,
Current e p i s o d e : 8 0 0 0 ,
Current e p i s o d e : 9 0 0 0 ,
Current e p i s o d e : 1 0 0 0 0 ,
Current e p i s o d e : 1 1 0 0 0 ,
Current e p i s o d e : 1 2 0 0 0 ,
Current e p i s o d e : 1 3 0 0 0 ,
Current e p i s o d e : 1 4 0 0 0 ,
Current e p i s o d e : 1 5 0 0 0 ,

s u c c e s s : 0 0 . 0
s u c c e s s : 0 0 . 0
s u c c e s s : 0 0 . 0
s u c c e s s : 31 0 . 0 3 1
s u c c e s s : 321 0 . 3 2 1
s u c c e s s : 602 0 . 6 0 2
s u c c e s s : 620 0 . 6 2
s u c c e s s : 821 0 . 8 2 1
s u c c e s s : 707 0 . 7 0 7

s u c c e s s : 714 0 . 7 1 4
s u c c e s s : 574 0 . 5 7 4
s u c c e s s : 443 0 . 4 4 3
s u c c e s s : 480 0 . 4 8
s u c c e s s : 458 0 . 4 5 8
s u c c e s s : 327 0 . 3 2 7

. . .

Current e p i s o d e : 4 7 0 0 0 ,
Current e p i s o d e : 4 8 0 0 0 ,
Current e p i s o d e : 4 9 0 0 0 ,
Current e p i s o d e : 5 0 0 0 0 ,
True

s u c c e s s : 1000 1 . 0
s u c c e s s : 1000 1 . 0
s u c c e s s : 1000 1 . 0
s u c c e s s : 1000 1 . 0

As you can see, the number of successful episodes generally increases as training progresses. It is not
advisable to stop the ﬁrst time we observe 100% success over 1,000 episodes. There is a randomness to
most games, so it is not likely that an agent would retain its 100% success rate with a new run. It might
be safe to stop training once you observe that the agent has gotten 100% for several update intervals.

12.2.4 Running and Observing the Agent

Now that the algorithm has trained the agent, we can observe the agent in action. You can use the following
code to see the agent in action.

Code

run_game ( q_table , True , F a l s e )
show_video ( )

12.2. PART 12.2: INTRODUCTION TO Q-LEARNING

439

12.2.5 Inspecting the Q-Table

We can also display the Q-table. The following code shows the agent’s action for each environment state.
As the weights of a neural network, this table is not straightforward to interpret. Some patterns do emerge
in that direction, as seen by calculating the means of rows and columns. The actions seem consistent at
both velocity and position’s upper and lower halves.

Code

import pandas a s pd

d f = pd . DataFrame ( q_table . argmax ( a x i s =2))

d f . columns = [ f ' v−{x} '
d f . i n d e x = [ f ' p−{x} '
d f

for x in range (DISCRETE_GRID_SIZE [ 0 ] ) ]

for x in range (DISCRETE_GRID_SIZE [ 1 ] ) ]

Output

v-0
2
0
1
2
2
1
2
0
2
2

v-1
2
1
0
0
0
1
2
2
0
2

v-2
2
0
0
0
0
2
0
1
1
2

v-3
2
1
2
0
0
1
0
0
2
1

v-4
2
2
2
2
0
1
0
0
0
1

v-5
2
2
2
2
2
0
0
1
0
0

v-6
2
2
2
2
0
1
2
2
2
2

v-7
0
2
1
1
2
1
2
2
2
2

v-8
2
2
1
2
2
2
2
2
1
2

v-9
0
1
0
2
2
2
2
2
2
1

p-0
p-1
p-2
p-3
p-4
p-5
p-6
p-7
p-8
p-9

Code

d f . mean ( a x i s =0)

Output

v−0
v−1
v−2
v−3
v−4
v−5

1 . 4
1 . 0
0 . 8
0 . 9
1 . 0
1 . 1

440

CHAPTER 12. REINFORCEMENT LEARNING

v−6
v−7
v−8
v−9
dtype :

1 . 7
1 . 5
1 . 8
1 . 4
f l o a t 6 4

Code

d f . mean ( a x i s =1)

Output

p−0
p−1
p−2
p−3
p−4
p−5
p−6
p−7
p−8
p−9
dtype :

1 . 6
1 . 3
1 . 1
1 . 3
1 . 0
1 . 2
1 . 2
1 . 2
1 . 2
1 . 5
f l o a t 6 4

12.3 Part 12.3: Keras Q-Learning in the OpenAI Gym

As we covered in the previous part, Q-Learning is a robust machine learning algorithm. Unfortunately,
Q-Learning requires that the Q-table contain an entry for every possible state that the environment can
take. Traditional Q-learning might be a good learning algorithm if the environment only includes a handful
of discrete state elements. However, the Q-table can become prohibitively large if the state space is large.
Creating policies for large state spaces is a task that Deep Q-Learning Networks (DQN) can usually
handle. Neural networks can generalize these states and learn commonalities. Unlike a table, a neural
network does not require the program to represent every combination of state and action. A DQN maps
the state to its input neurons and the action Q-values to the output neurons. The DQN eﬀectively becomes
a function that accepts the state and suggests action by returning the expected reward for each possible
action. Figure 12.2 demonstrates the DQN structure and mapping between state and action.

As this diagram illustrates, the environment state contains several elements. For the basic DQN, the
state can be a mix of continuous and categorical/discrete values. For the DQN, the discrete state elements
the program typically encoded as dummy variables. The actions should be discrete when your program
implements a DQN. Other algorithms support continuous outputs, which we will discuss later in this

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

441

Figure 12.2: Deep Q-Learning (DQL)

chapter.

This chapter will use TF-Agents to implement a DQN to solve the cart-pole environment. TF-Agents
makes designing, implementing, and testing new RL algorithms easier by providing well-tested modu-
lar components that can be modiﬁed and extended.
It enables fast code iteration with functional test
integration and benchmarking.

12.3.1 DQN and the Cart-Pole Problem

Barto (1983) ﬁrst described the cart-pole problem.[2]A cart is connected to a rigid hinged pole. The cart is
free to move only in the vertical plane of the cart/track. The agent can apply an impulsive "left" or "right"
force F of a ﬁxed magnitude to the cart at discrete time intervals. The cart-pole environment simulates
the physics behind keeping the pole reasonably upright position on the cart. The environment has four
state variables:

• x The position of the cart on the track.
• θ The angle of the pole with the vertical
• ˙x The cart velocity.

442

CHAPTER 12. REINFORCEMENT LEARNING

• ˙θ The rate of change of the angle.

The action space consists of discrete actions:

• Apply force left
• Apply force right

To apply DQN to this problem, you need to create the following components for TF-Agents.

• Environment
• Agent
• Policies
• Metrics and Evaluation
• Replay Buﬀer
• Data Collection
• Training

These components are standard in most DQN implementations. Later, we will apply these same compo-
nents to an Atari game, and after that, a problem with our design. This example is based on the cart-pole
tutorial provided for TF-Agents.

First, we must install TF-Agents.

Code

i f COLAB:

i n s t a l l −y xvfb ffmpeg x11−u t i l s

! sudo apt−g e t
! p i p i n s t a l l −q 'gym==0.10.11 '
! p i p i n s t a l l −q ' i m a g e i o ==2.4.0 '
! p i p i n s t a l l −q PILLOW
! p i p i n s t a l l −q ' p y g l e t ==1.3.2 '
! p i p i n s t a l l −q p y v i r t u a l d i s p l a y
! p i p i n s t a l l −q t f −a g e n t s
! p i p i n s t a l l −q pygame

We begin by importing needed Python libraries.

Code

import b a se 6 4
import i m a g e i o
import IPython
import m a t p l o t l i b
import m a t p l o t l i b . p y p l o t a s p l t
import numpy a s np
import PIL . Image
import p y v i r t u a l d i s p l a y

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

443

import t e n s o r f l o w a s

t f

from t f _ a g e n t s . a g e n t s . dqn import dqn_agent
from t f _ a g e n t s . d r i v e r s import dynamic_step_driver
from t f _ a g e n t s . e n v i r o n m e n t s import suite_gym
from t f _ a g e n t s . e n v i r o n m e n t s import tf_py_environment
from t f _ a g e n t s . eval
from t f _ a g e n t s . m e t r i c s import t f _ m e t r i c s
from t f _ a g e n t s . networks import q_network
from t f _ a g e n t s . p o l i c i e s
from t f _ a g e n t s . r e p l a y _ b u f f e r s import t f _ u n i f o r m _ r e p l a y _ b u f f e r
from t f _ a g e n t s . t r a j e c t o r i e s
from t f _ a g e n t s . u t i l s

import random_tf_policy

import m e t r i c _ u t i l s

import t r a j e c t o r y

import common

To allow this example to run in a notebook, we use a virtual display that will output an embedded
video. If running this code outside a notebook, you could omit the virtual display and animate it directly
to a window.

Code

# S e t up a v i r t u a l d i s p l a y f o r
d i s p l a y = p y v i r t u a l d i s p l a y . D i s p l a y ( v i s i b l e =0,

r e n d e r i n g OpenAI gym e n v i r o n m e n t s .

s i z e =(1400 , 9 0 0 ) ) . s t a r t ( )

12.3.2 Hyperparameters

We must deﬁne Several hyperparameters for the algorithm to train the agent. The TF-Agent example
provided reasonably well-tuned hyperparameters for cart-pole. Later we will adapt these to an Atari game.

Code

i n i t i a l d a t a .

# How l o n g s h o u l d t r a i n i n g run ?
n u m _ i t e r a t i o n s = 20000
# How many i n i t i a l random s t e p s , b e f o r e t r a i n i n g s t a r t ,
# c o l l e c t
i n i t i a l _ c o l l e c t _ s t e p s = 1000
# How many s t e p s
# d a t a from .
c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n = 1
# How much d a t a s h o u l d we s t o r e f o r
replay_buffer_max_length = 100000

s h o u l d we run each i t e r a t i o n t o c o l l e c t

t r a i n i n g e x a m p l e s .

t o

b a t c h _ s i z e = 64

444

CHAPTER 12. REINFORCEMENT LEARNING

l e a r n i n g _ r a t e = 1 e−3
# How o f t e n s h o u l d t h e program p r o v i d e an u p d a t e .
l o g _ i n t e r v a l = 200

# How many e p i s o d e s
num_eval_episodes = 10
# How o f t e n s h o u l d an e v a l u a t i o n o c c u r .
e v a l _ i n t e r v a l = 1000

s h o u l d t h e program use f o r each e v a l u a t i o n .

12.3.3 Environment

TF-Agents use OpenAI gym environments to represent the task or problem to be solved. Standard en-
vironments can be created in TF-Agents using tf_agents.environments suites. TF-Agents has suites
for loading environments from sources such as the OpenAI Gym, Atari, and DM Control. We begin by
loading the CartPole environment from the OpenAI Gym suite.

Code

env_name = ' CartPole−v0 '
env = suite_gym . l o a d ( env_name )

We will quickly render this environment to see the visual representation.

Code

env . r e s e t ( )
PIL . Image . f r o m a r r a y ( env . r e n d e r ( ) )

Output

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

445

The environment.step method takes an action in the environment and returns a TimeStep tuple

containing the following observation of the environment and the reward for the action.

The time_step_spec() method returns the speciﬁcation for the TimeStep tuple. Its observation
attribute shows the shape of observations, the data types, and the ranges of allowed values. The reward
attribute shows the same details for the reward.

Code

print ( ' O b s e r v a t i o n ␣ Spec : ' )
print ( env . time_step_spec ( ) . o b s e r v a t i o n )

Output

O b s e r v a t i o n Spec :
BoundedArraySpec ( shape = ( 4 , ) , dtype=dtype ( ' f l o a t 3 2 ' ) ,
name=' o b s e r v a t i o n ' , minimum=[ −4.8000002 e+00 −3.4028235 e+38
−4.1887903 e −01 −3.4028235 e +38] , maximum= [ 4 . 8 0 0 0 0 0 2 e+00 3 . 4 0 2 8 2 3 5 e+38
4 . 1 8 8 7 9 0 3 e −01 3 . 4 0 2 8 2 3 5 e +38])

Code

print ( ' Reward␣ Spec : ' )
print ( env . time_step_spec ( ) . reward )

Output

446

CHAPTER 12. REINFORCEMENT LEARNING

Reward Spec :
ArraySpec ( shape =() , dtype=dtype ( ' f l o a t 3 2 ' ) , name=' reward ' )

The action_spec() method returns the shape, data types, and allowed values of valid actions.

Code

print ( ' Action ␣ Spec : ' )
print ( env . a c t i o n _ s p e c ( ) )

Output

Action Spec :
BoundedArraySpec ( shape =() , dtype=dtype ( ' i n t 6 4 ' ) , name=' a c t i o n ' ,
minimum=0, maximum=1)

In the Cartpole environment:

• observation is an array of 4 ﬂoats:

– the position and velocity of the cart
– the angular position and velocity of the pole
–
–

reward is a scalar ﬂoat valueaction is a scalar integer with only two possible values:

– 0 --- "move left"
– 1 --- "move right"

Code

time_step = env . r e s e t ( )
print ( ' Time␣ s t e p : ' )
print ( time_step )

a c t i o n = np . a r r a y ( 1 , dtype=np . i n t 3 2 )

next_time_step = env . s t e p ( a c t i o n )
print ( ' Next ␣ time ␣ s t e p : ' )
print ( next_time_step )

Output

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

447

Time s t e p :
TimeStep (
{ ' d i s c o u n t ' : a r r a y ( 1 . , dtype=f l o a t 3 2 ) ,

' o b s e r v a t i o n ' : a r r a y ( [ − 0 . 0 3 2 7 9 8 5 9 ,

0 . 0 3 5 6 2 8 9 2 , −0.04014493 ,

− 0 . 0 4 9 1 1 8 0 2 ] , dtype=f l o a t 3 2 ) ,

' reward ' : a r r a y ( 0 . , dtype=f l o a t 3 2 ) ,
' step_type ' : a r r a y ( 0 , dtype=i n t 3 2 ) } )

Next time s t e p :
TimeStep (
{ ' d i s c o u n t ' : a r r a y ( 1 . , dtype=f l o a t 3 2 ) ,

' o b s e r v a t i o n ' : a r r a y ( [ − 0 . 0 3 2 0 8 6 0 1 ,

0 . 2 3 1 3 0 2 8 3 , −0.04112729 ,

− 0 . 3 5 4 1 9 1 8 4 ] , dtype=f l o a t 3 2 ) ,

' reward ' : a r r a y ( 1 . , dtype=f l o a t 3 2 ) ,
' step_type ' : a r r a y ( 1 , dtype=i n t 3 2 ) } )

Usually, the program instantiates two environments: one for training and one for evaluation.

Code

train_py_env = suite_gym . l o a d ( env_name )
eval_py_env = suite_gym . l o a d ( env_name )

The Cartpole environment, like most environments, is written in pure Python and is converted to
TF-Agents and TensorFlow using the TFPyEnvironment wrapper. The original environment’s API
uses Numpy arrays. The TFPyEnvironment turns these to Tensors to make them compatible with
Tensorﬂow agents and policies.

Code

t r ai n _ e nv = tf_py_environment . TFPyEnvironment ( train_py_env )
eval_env = tf_py_environment . TFPyEnvironment ( eval_py_env )

12.3.4 Agent

An Agent represents the algorithm used to solve an RL problem. TF-Agents provides standard implemen-
tations of a variety of Agents:

• DQN (used in this example)
• REINFORCE
• DDPG
• TD3
• PPO
• SAC.

448

CHAPTER 12. REINFORCEMENT LEARNING

You can only use the DQN agent in environments with a discrete action space. The DQN uses a QNetwork,
a neural network model that learns to predict Q-Values (expected returns) for all actions given a state from
the environment.

The following code uses tf_agents.networks.q_network to create a QNetwork, passing in the ob-
servation_spec, action_spec, and a tuple describing the number and size of the model’s hidden layers.

Code

fc_layer_params = ( 1 0 0 , )

q_net = q_network . QNetwork (

t r ai n _ e n v . o b s e r v a t i o n _ s p e c ( ) ,
t r ai n _ e n v . a c t i o n _ s p e c ( ) ,
fc_layer_params=fc_layer_params )

Now we use tf_agents.agents.dqn.dqn_agent to instantiate a DqnAgent.

In addition to the
time_step_spec, action_spec and the QNetwork, the agent constructor also requires an optimizer (in
this case, AdamOptimizer), a loss function, and an integer step counter.

Code

o p t i m i z e r = t f . compat . v1 . t r a i n . AdamOptimizer ( l e a r n i n g _ r a t e=l e a r n i n g _ r a t e )

t r a i n _ s t e p _ c o u n t e r = t f . V a r i a b l e ( 0 )

a g e n t = dqn_agent . DqnAgent (

t r ai n _ e n v . time_step_spec ( ) ,
t r ai n _ e n v . a c t i o n _ s p e c ( ) ,
q_network=q_net ,
o p t i m i z e r=o p t i m i z e r ,
t d _ e r r o r s _ l o s s _ f n=common . element_wise_squared_loss ,
t r a i n _ s t e p _ c o u n t e r=t r a i n _ s t e p _ c o u n t e r )

a g e n t . i n i t i a l i z e ( )

12.3.5 Policies

A policy deﬁnes the way an agent acts in an environment. Typically, reinforcement learning aims to train
the underlying model until the policy produces the desired outcome.

In this example:

• The desired outcome is keeping the pole balanced upright over the cart.
• The policy returns an action (left or right) for each time_step observation.

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

449

Agents contain two policies:

• agent.policy - The algorithm uses this main policy for evaluation and deployment.
• agent.collect_policy - The algorithm this secondary policy for data collection.

Code

e v a l _ p o l i c y = a g e n t . p o l i c y
c o l l e c t _ p o l i c y = a g e n t . c o l l e c t _ p o l i c y

You can create policies independently of agents. For example, use random_tf_policy to create a policy
that will randomly select an action for each time_step. We will use this random policy to create initial
collection data to begin training.

Code

random_policy = random_tf_policy . RandomTFPolicy ( t r ai n _ e nv . time_step_spec ( ) ,

t r ai n _ e nv . a c t i o n _ s p e c ( ) )

To get an action from a policy, call the policy.action method. The time_step contains the obser-
vation from the environment. This method returns a PolicyStep, which is a named tuple with three
components:

• action - The action to be taken (in this case, 0 or 1).
• state - Used for stateful (that is, RNN-based) policies.
• info - Auxiliary data, such as log probabilities of actions.

Next, we create an environment and set up the random policy.

Code

example_environment = tf_py_environment . TFPyEnvironment (

suite_gym . l o a d ( ' CartPole−v0 ' ) )
time_step = example_environment . r e s e t ( )
random_policy . a c t i o n ( time_step )

Output

P o l i c y S t e p ( a c t i o n=<t f . Tensor :
numpy=a r r a y ( [ 0 ] ) > ,

s t a t e =() ,

shape = ( 1 , ) , dtype=i n t 6 4 ,

i n f o =())

450

CHAPTER 12. REINFORCEMENT LEARNING

12.3.6 Metrics and Evaluation

The most common metric used to evaluate a policy is the average return. The return is the sum of rewards
obtained while running a policy in an environment for an episode. Several episodes are run, creating an
average return. The following function computes the average return, given the policy, environment, and
number of episodes. We will use this same evaluation for Atari.

Code

def compute_avg_return ( environment , p o l i c y , num_episodes =10):

t o t a l _ r e t u r n = 0 . 0
for _ in range ( num_episodes ) :

time_step = environment . r e s e t ( )
e p i s o d e _ r e t u r n = 0 . 0

while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = environment . s t e p ( a c t i o n _ s t e p . a c t i o n )
e p i s o d e _ r e t u r n += time_step . reward

t o t a l _ r e t u r n += e p i s o d e _ r e t u r n

avg_return = t o t a l _ r e t u r n / num_episodes
return avg_return . numpy ( ) [ 0 ]

# See a l s o t h e m e t r i c s module f o r
# o f d i f f e r e n t m e t r i c s .
# h t t p s : / / g i t h u b . com/ t e n s o r f l o w / a g e n t s / t r e e / master / t f _ a g e n t s / m e t r i c s

s t a n d a r d i m p l e m e n t a t i o n s

Running this computation on the random_policy shows a baseline performance in the environment.

Code

compute_avg_return ( eval_env ,

random_policy , num_eval_episodes )

Output

1 5 . 2

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

451

12.3.7 Replay Buﬀer

The replay buﬀer keeps track of data collected from the environment. This tutorial uses TFUniformRe-
playBuﬀer. The constructor requires the specs for the data it will be collecting. This value is available
from the agent using the collect_data_spec method. The batch size and maximum buﬀer length are
also required.

Code

r e p l a y _ b u f f e r = t f _ u n i f o r m _ r e p l a y _ b u f f e r . TFUniformReplayBuffer (

data_spec=a g e n t . c o l l e c t _ d a t a _ s p e c ,
b a t c h _ s i z e=t r a in _ e n v . b a t c h _ s i z e ,
max_length=replay_buffer_max_length )

For most agents, collect_data_spec is a named tuple called Trajectory, containing the specs for

observations, actions, rewards, and other items.

Code

a g e n t . c o l l e c t _ d a t a _ s p e c

Output

T r a j e c t o r y (
{ ' a c t i o n ' : BoundedTensorSpec ( shape =() , dtype=t f . i n t 6 4 , name=' a c t i o n ' ,
minimum=a r r a y ( 0 ) , maximum=a r r a y ( 1 ) ) ,

' d i s c o u n t ' : BoundedTensorSpec ( shape =() , dtype=t f . f l o a t 3 2 ,

name=' d i s c o u n t ' , minimum=a r r a y ( 0 . , dtype=f l o a t 3 2 ) , maximum=a r r a y ( 1 . ,
dtype=f l o a t 3 2 ) ) ,

' next_step_type ' : TensorSpec ( shape =() , dtype=t f . i n t 3 2 ,

name=' step_type ' ) ,

' o b s e r v a t i o n ' : BoundedTensorSpec ( shape = ( 4 , ) , dtype=t f . f l o a t 3 2 ,

name=' o b s e r v a t i o n ' , minimum=a r r a y ( [ − 4 . 8 0 0 0 0 0 2 e +00 , −3.4028235 e +38 ,
−4.1887903 e −01 , −3.4028235 e +38] ,

dtype=f l o a t 3 2 ) , maximum=a r r a y ( [ 4 . 8 0 0 0 0 0 2 e +00 , 3 . 4 0 2 8 2 3 5 e +38 ,

4 . 1 8 8 7 9 0 3 e −01 , 3 . 4 0 2 8 2 3 5 e +38] ,

dtype=f l o a t 3 2 ) ) ,

' p o l i c y _ i n f o ' :
' reward ' : TensorSpec ( shape =() , dtype=t f . f l o a t 3 2 , name=' reward ' ) ,
' step_type ' : TensorSpec ( shape =() , dtype=t f . i n t 3 2 , name=' step_type ' ) } )

( ) ,

452

CHAPTER 12. REINFORCEMENT LEARNING

12.3.8 Data Collection

Now execute the random policy in the environment for a few steps, recording the data in the replay buﬀer.

Code

def c o l l e c t _ s t e p ( environment , p o l i c y , buffer ) :

time_step = environment . current_time_ste p ( )
a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
next_time_step = environment . s t e p ( a c t i o n _ s t e p . a c t i o n )
t r a j = t r a j e c t o r y . f r o m _ t r a n s i t i o n ( time_step , a c t i o n _ s t e p , \

next_time_step )

# Add t r a j e c t o r y t o t h e r e p l a y b u f f e r
buffer . add_batch ( t r a j )

def c o l l e c t _ d a t a ( env , p o l i c y , buffer ,

s t e p s ) :

for _ in range ( s t e p s ) :

c o l l e c t _ s t e p ( env , p o l i c y , buffer )

c o l l e c t _ d a t a ( train_env ,

random_policy ,

r e p l a y _ b u f f e r ,

s t e p s =100)

# This l o o p i s
# For more d e t a i l s
# h t t p s : / /www. t e n s o r f l o w . o r g / a g e n t s / api_docs / python / t f _ a g e n t s / d r i v e r s

t h a t we p r o v i d e s t a n d a r d i m p l e m e n t a t i o n s .

s e e t h e d r i v e r s module .

so common i n RL,

The replay buﬀer is now a collection of Trajectories. The agent needs access to the replay buﬀer. TF-
Agents provides this access by creating an iterable tf.data.Dataset pipeline, which will feed data to the
agent.

Each row of the replay buﬀer only stores a single observation step. But since the DQN Agent needs both
the current and following observation to compute the loss, the dataset pipeline will sample two adjacent
rows for each item in the batch (num_steps=2).

The program also optimizes this dataset by running parallel calls and prefetching data.

Code

# D a t a s e t g e n e r a t e s
d a t a s e t = r e p l a y _ b u f f e r . a s _ d a t a s e t (

t r a j e c t o r i e s w i t h shape [ Bx2x . . . ]

n u m _ p a r a l l e l _ c a l l s =3,
sample_batch_size=b a t c h _ s i z e ,
num_steps =2). p r e f e t c h ( 3 )

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

453

d a t a s e t

Output

WARNING: t e n s o r f l o w : From / u s r / l o c a l / l i b / python3 . 7 / d i s t −
p a c k a g e s / t e n s o r f l o w / python / autograph / impl / a p i . py : 3 7 7 :
R e p l a y B u f f e r . get_next
d e p r e c a t e d and w i l l be removed i n a f u t u r e v e r s i o n .
I n s t r u c t i o n s
Use ` a s _ d a t a s e t ( . . . ,
<P r e f e t c h D a t a s e t element_spec =( T r a j e c t o r y (
{ ' a c t i o n ' : TensorSpec ( shape =(64 , 2 ) , dtype=t f . i n t 6 4 , name=None ) ,

s i n g l e _ d e t e r m i n i s t i c _ p a s s=F a l s e )

( from t f _ a g e n t s . r e p l a y _ b u f f e r s . r e p l a y _ b u f f e r )

f o r u p d a t i n g :

i n s t e a d .

i s

' d i s c o u n t ' : TensorSpec ( shape =(64 , 2 ) , dtype=t f . f l o a t 3 2 , name=None ) ,
' next_step_type ' : TensorSpec ( shape =(64 , 2 ) , dtype=t f . i n t 3 2 ,

name=None ) ,

' o b s e r v a t i o n ' : TensorSpec ( shape =(64 , 2 , 4 ) , dtype=t f . f l o a t 3 2 ,

name=None ) ,

' p o l i c y _ i n f o ' :
' reward ' : TensorSpec ( shape =(64 , 2 ) , dtype=t f . f l o a t 3 2 , name=None ) ,
' step_type ' : TensorSpec ( shape =(64 , 2 ) , dtype=t f . i n t 3 2 , name=None ) } ) ,

( ) ,

B u f f e r I n f o ( i d s=TensorSpec ( shape =(64 , 2 ) , dtype=t f . i n t 6 4 , name=None ) ,
p r o b a b i l i t i e s=TensorSpec ( shape = ( 6 4 , ) , dtype=t f . f l o a t 3 2 , name=None ))) >

Code

i t e r a t o r = i t e r ( d a t a s e t )

print ( i t e r a t o r )

Output

<t e n s o r f l o w . python . data . ops . i t e r a t o r _ o p s . OwnedIterator o b j e c t a t
0 x 7 f 0 5 c 0 0 0 6 c 1 0 >

12.3.9 Training the agent

Two things must happen during the training loop:

• Collect data from the environment
• Use that data to train the agent’s neural network(s)

454

CHAPTER 12. REINFORCEMENT LEARNING

This example also periodically evaluates the policy and prints the current score.

The following will take ~5 minutes to run.

Code

# ( O p t i o n a l ) Optimize by wrapping some o f
# u s i n g TF f u n c t i o n .
a g e n t . t r a i n = common . f u n c t i o n ( a g e n t . t r a i n )

t h e code i n a graph

# R e s e t
a g e n t . t r a i n _ s t e p _ c o u n t e r . a s s i g n ( 0 )

t h e t r a i n s t e p

# E v a l u a t e t h e a g e n t ' s p o l i c y once b e f o r e t r a i n i n g .
avg_return = compute_avg_return ( eval_env , a g e n t . p o l i c y ,

num_eval_episodes )

r e t u r n s = [ avg_return ]

for _ in range ( n u m _ i t e r a t i o n s ) :

# C o l l e c t a few s t e p s u s i n g c o l l e c t _ p o l i c y and
# s a v e t o t h e r e p l a y b u f f e r .
for _ in range ( c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n ) :

c o l l e c t _ s t e p ( train_env , a g e n t . c o l l e c t _ p o l i c y ,

r e p l a y _ b u f f e r )

# Sample a b a t c h o f d a t a from t h e b u f f e r and u p d a t e
# t h e a g e n t ' s network .
e x p e r i e n c e , unused_info = next ( i t e r a t o r )
t r a i n _ l o s s = a g e n t . t r a i n ( e x p e r i e n c e ) . l o s s

s t e p = a g e n t . t r a i n _ s t e p _ c o u n t e r . numpy ( )

i f

s t e p % l o g _ i n t e r v a l == 0 :

print ( ' s t e p ␣=␣ { 0 } : ␣ l o s s ␣=␣ {1} ' . format ( s t e p ,

t r a i n _ l o s s ) )

i f

s t e p % e v a l _ i n t e r v a l == 0 :

avg_return = compute_avg_return ( eval_env , a g e n t . p o l i c y ,

num_eval_episodes )

print ( ' s t e p ␣=␣ { 0 } : ␣ Average ␣ Return ␣=␣ {1} ' . format ( s t e p , avg_return ) )
r e t u r n s . append ( avg_return )

Output

WARNING: t e n s o r f l o w : From / u s r / l o c a l / l i b / python3 . 7 / d i s t −

12.3. PART 12.3: KERAS Q-LEARNING IN THE OPENAI GYM

455

i s

c a l l i n g f o l d r _ v 2

f o r u p d a t i n g :

i s d e p r e c a t e d . C o n s i d e r u s i n g t f . s t o p _ g r a d i e n t

p a c k a g e s / t e n s o r f l o w / python / u t i l / d i s p a t c h . py : 1 0 8 2 :
( from t e n s o r f l o w . python . ops . f u n c t i o n a l _ o p s ) with back_prop=F a l s e
d e p r e c a t e d and w i l l be removed i n a f u t u r e v e r s i o n .
I n s t r u c t i o n s
back_prop=F a l s e
i n s t e a d .
I n s t e a d o f :
r e s u l t s = t f . f o l d r ( fn , elems , back_prop=F a l s e )
Use :
r e s u l t s = t f . n e s t . map_structure ( t f . s t o p _ g r a d i e n t ,
l o s s = 2 3 . 1 5 8 3 7 4 7 8 6 3 7 6 9 5 3
s t e p = 2 0 0 :
l o s s = 7 . 1 5 8 8 1 7 7 6 8 0 9 6 9 2 4
s t e p = 4 0 0 :
l o s s = 3 0 . 9 7 6 9 9 7 3 7 5 4 8 8 2 8
s t e p = 6 0 0 :
l o s s = 9 . 8 3 1 3 3 7 9 2 8 7 7 1 9 7 3
s t e p = 8 0 0 :

t f . f o l d r ( fn , e le m s ) )

. . .

s t e p = 1 9 4 0 0 :
s t e p = 1 9 6 0 0 :
s t e p = 1 9 8 0 0 :
s t e p = 2 0 0 0 0 :
s t e p = 2 0 0 0 0 : Average Return = 1 9 8 . 3 0 0 0 0 3 0 5 1 7 5 7 8

l o s s = 1 6 . 5 9 9 0 0 6 6 5 2 8 3 2 0 3
l o s s = 1 6 . 2 5 3 8 4 9 0 2 9 5 4 1 0 1 6
l o s s = 1 2 4 . 6 3 1 8 0 5 4 1 9 9 2 1 8 8
l o s s = 2 2 . 4 5 9 1 7 3 2 0 2 5 1 4 6 5

12.3.10 Visualization and Plots

Use matplotlib.pyplot to chart how the policy improved during training.

One iteration of Cartpole-v0 consists of 200 time steps. The environment rewards +1 for each step
the pole stays up, so the maximum return for one episode is 200. The charts show the return increasing
towards that maximum each time the algorithm evaluates it during training. (It may be a little unstable
and not increase each time monotonically.)

Code

i t e r a t i o n s = range ( 0 , n u m _ i t e r a t i o n s + 1 , e v a l _ i n t e r v a l )
p l t . p l o t ( i t e r a t i o n s ,
p l t . y l a b e l ( ' Average ␣ Return ' )
p l t . x l a b e l ( ' I t e r a t i o n s ' )
p l t . ylim ( top =250)

r e t u r n s )

Output

456

CHAPTER 12. REINFORCEMENT LEARNING

( 3 . 8 5 9 9 9 9 7 9 9 7 2 8 3 9 4 , 2 5 0 . 0 )

12.3.11 Videos

The charts are nice. But more exciting is seeing an agent performing a task in an environment.

First, create a function to embed videos in the notebook.

Code

def embed_mp4( f i l e n a m e ) :

" " " Embeds an mp4 f i l e
v i d e o = open ( f i l e n a m e ,
b64 = b a s e6 4 . b64encode ( v i d e o )
t a g = ' ' '

i n t h e n o t e b o o k . " " "

' rb ' ) . r e a d ( )

<v i d e o w i d t h ="640" h e i g h t ="480" c o n t r o l s >
<s o u r c e s r c =" d a t a : v i d e o /mp4 ; base64 , { 0 } "

t y p e =" v i d e o /mp4">

Your b r o w s e r d o e s not
</v i d e o > ' ' ' . format ( b64 . decode ( ) )

s u p p o r t

t h e v i d e o t a g .

return IPython . d i s p l a y .HTML( t a g )

Now iterate through a few episodes of the Cartpole game with the agent. The underlying Python
environment (the one "inside" the TensorFlow environment wrapper) provides a render() method, which
outputs an image of the environment state. We can collect these frames into a video.

Code

def c r e a t e _ p o l i c y _ e v a l _ v i d e o ( p o l i c y ,
f i l e n a m e = f i l e n a m e + " . mp4"
with i m a g e i o . g e t _ w r i t e r ( f i l e n a m e ,

f i l e n a m e , num_episodes =5,

f p s =30):

f p s=f p s ) a s v i d e o :

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

457

for _ in range ( num_episodes ) :

time_step = eval_env . r e s e t ( )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )
while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = eval_env . s t e p ( a c t i o n _ s t e p . a c t i o n )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )

return embed_mp4( f i l e n a m e )

c r e a t e _ p o l i c y _ e v a l _ v i d e o ( a g e n t . p o l i c y ,

" t r a i n e d −a g e n t " )

For fun, compare the trained agent (above) to an agent moving randomly. (It does not do as well.)

Code

c r e a t e _ p o l i c y _ e v a l _ v i d e o ( random_policy ,

" random−a g e n t " )

12.4 Part 12.4: Atari Games with Keras Neural Networks

The Atari 2600 is a home video game console from Atari, Inc., Released on September 11, 1977. Most
credit the Atari with popularizing microprocessor-based hardware and games stored on ROM cartridges
instead of dedicated hardware with games built into the unit. Atari bundled their console with two joystick
controllers, a conjoined pair of paddle controllers, and a game cartridge: initially Combat, and later Pac-
Man.

Atari emulators are popular and allow gamers to play many old Atari video games on modern computers.

These emulators are even available as JavaScript.

• Virtual Atari

Atari games have become popular benchmarks for AI systems, particularly reinforcement learning. OpenAI
Gym internally uses the Stella Atari Emulator. You can see the Atari 2600 in Figure 12.3.

12.4.1 Actual Atari 2600 Specs

• CPU: 1.19 MHz MOS Technology 6507
• Audio + Video processor: Television Interface Adapter (TIA)
• Playﬁeld resolution: 40 x 192 pixels (NTSC). It uses a 20-pixel register that is mirrored or copied,

left side to right side, to achieve the width of 40 pixels.

• Player sprites: 8 x 192 pixels (NTSC). Player, ball, and missile sprites use pixels 1/4 the width of

playﬁeld pixels (unless stretched).

• Ball and missile sprites: 1 x 192 pixels (NTSC).

458

CHAPTER 12. REINFORCEMENT LEARNING

Figure 12.3: The Atari 2600

• Maximum resolution: 160 x 192 pixels (NTSC). Max resolution is achievable only with programming

tricks that combine sprite pixels with playﬁeld pixels.

• 128 colors (NTSC). 128 possible on screen. Max of 4 per line: background, playﬁeld, player0 sprite,
and player1 sprite. Palette switching between lines is common. Palette switching mid-line is possible
but not common due to resource limitations.

• 2 channels of 1-bit monaural sound with 4-bit volume control.

12.4.2 OpenAI Lab Atari Pong

You can use OpenAI Gym with Windows; however, it requires a special installation procedure.

This chapter demonstrates playing Atari Pong. Pong is a two-dimensional sports game that simulates
table tennis. The player controls an in-game paddle by moving it vertically across the left or right side of
the screen. They can compete against another player controlling a second paddle on the opposing side.
Players use the paddles to hit a ball back and forth. The goal is for each player to reach eleven points
before the opponent; you earn points when one fails to return it to the other. For the Atari 2600 version
of Pong, a computer player (controlled by the Atari 2600) is the opposing player.

This section shows how to adapt TF-Agents to an Atari game. You can quickly adapt this example to
any Atari game by simply changing the environment name. However, I tuned the code presented here for
Pong, and it may not perform as well for other games. Some tuning will likely be necessary to produce
a good agent for other games. Compared to the pole-cart game presented earlier in this chapter, some
changes are required.

We begin by importing the needed Python packages.

Code

import b a se 6 4

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

459

import i m a g e i o
import IPython
import m a t p l o t l i b
import m a t p l o t l i b . p y p l o t a s p l t
import numpy a s np
import PIL . Image
import p y v i r t u a l d i s p l a y

import t e n s o r f l o w a s

t f

s u i t e _ a t a r i

from t f _ a g e n t s . a g e n t s . dqn import dqn_agent
from t f _ a g e n t s . d r i v e r s import dynamic_step_driver
from t f _ a g e n t s . e n v i r o n m e n t s import suite_gym ,
from t f _ a g e n t s . e n v i r o n m e n t s import tf_py_environment
from t f _ a g e n t s . e n v i r o n m e n t s import batched_py_environment
from t f _ a g e n t s . eval
from t f _ a g e n t s . m e t r i c s import t f _ m e t r i c s
from t f _ a g e n t s . networks import q_network , network
from t f _ a g e n t s . p o l i c i e s
from t f _ a g e n t s . r e p l a y _ b u f f e r s import t f _ u n i f o r m _ r e p l a y _ b u f f e r
from t f _ a g e n t s . t r a j e c t o r i e s
from t f _ a g e n t s . u t i l s
from t f _ a g e n t s . a g e n t s . c a t e g o r i c a l _ d q n import c a t e g o r i c a l _ d q n _ a g e n t
from t f _ a g e n t s . networks import c a t e g o r i c a l _ q _ n e t w o r k

import random_tf_policy

import m e t r i c _ u t i l s

import t r a j e c t o r y

import common

from t f _ a g e n t s . s p e c s import t e n s o r _ s p e c
from t f _ a g e n t s . t r a j e c t o r i e s

import time_step a s

t s

# S e t up a v i r t u a l d i s p l a y f o r
d i s p l a y = p y v i r t u a l d i s p l a y . D i s p l a y ( v i s i b l e =0,

r e n d e r i n g OpenAI gym e n v i r o n m e n t s .

s i z e =(1400 , 9 0 0 ) ) . s t a r t ( )

12.4.3 Hyperparameters

The hyperparameter names are the same as the previous DQN example; however, I tuned the numeric
values for the more complex Atari game.

Code

# 10K a l r e a d y t a k e s a w h i l e t o complete , w i t h minimal
# To g e t an e f f e c t i v e a g e n t
n u m _ i t e r a t i o n s = 10000

r e q u i r e s much more .

r e s u l t s .

460

CHAPTER 12. REINFORCEMENT LEARNING

i n i t i a l _ c o l l e c t _ s t e p s = 200
c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n = 10
replay_buffer_max_length = 100000

b a t c h _ s i z e = 32
l e a r n i n g _ r a t e = 2 . 5 e−3
l o g _ i n t e r v a l = 1000

num_eval_episodes = 5
e v a l _ i n t e r v a l = 25000

The algorithm needs more iterations for an Atari game. I also found that increasing the number of

collection steps helped the algorithm train.

12.4.4 Atari Environment

You must handle Atari environments diﬀerently than games like cart-poll. Atari games typically use their
2D displays as the environment state. AI Gym represents Atari games as either a 3D (height by width by
color) state spaced based on their screens or a vector representing the game’s computer RAM state. To
preprocess Atari games for greater computational eﬃciency, we skip several frames, decrease the resolution,
and discard color information. The following code shows how we can set up an Atari environment.

Code

! wget h t t p : / /www. a t a r i m a n i a . com/ roms /Roms . r a r
! mkdir / c o n t e n t /ROM/
! u n r a r e −o+ / c o n t e n t /Roms . r a r / c o n t e n t /ROM/
! python −m a t a r i _ p y . import_roms / c o n t e n t /ROM/

Code

#env_name = ' Breakout−v4 '
env_name = ' Pong−v0 '
#env_name = ' B r e a k o u t D e t e r m i n i s t i c −v4 '
#env = suite_gym . l o a d ( env_name )

# A t a r i P r e p r o c e s s i n g runs 4 frames a t a time , max−p o o l i n g o v e r t h e l a s t 2
# frames . We need t o a c c o u n t
# i n t e r v a l s .
ATARI_FRAME_SKIP = 4

t h i s when computing t h i n g s

l i k e u p d a t e

f o r

max_episode_frames =108000 # ALE frames

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

461

env = s u i t e _ a t a r i . l o a d (

env_name ,
max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,
gym_env_wrappers=s u i t e _ a t a r i .DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)

#env = batched_py_environment . BatchedPyEnvironment ( [ env ] )

We can now reset the environment and display one step. The following image shows how the Pong

game environment appears to a user.

Code

env . r e s e t ( )
PIL . Image . f r o m a r r a y ( env . r e n d e r ( ) )

Output

We are now ready to load and wrap the two environments for TF-Agents. The algorithm uses the ﬁrst

environment for evaluation and the second to train.

Code

train_py_env = s u i t e _ a t a r i . l o a d (

env_name ,
max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,
gym_env_wrappers=s u i t e _ a t a r i .DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)

eval_py_env = s u i t e _ a t a r i . l o a d (

env_name ,
max_episode_steps=max_episode_frames / ATARI_FRAME_SKIP,
gym_env_wrappers=s u i t e _ a t a r i .DEFAULT_ATARI_GYM_WRAPPERS_WITH_STACKING)

t r ai n _ e nv = tf_py_environment . TFPyEnvironment ( train_py_env )
eval_env = tf_py_environment . TFPyEnvironment ( eval_py_env )

462

12.4.5 Agent

CHAPTER 12. REINFORCEMENT LEARNING

I used the following code from the TF-Agents examples to wrap up the regular Q-network class. The
AtariQNetwork class ensures that the pixel values from the Atari screen are divided by 255. This division
assists the neural network by normalizing the pixel values between 0 and 1.

Code

# A t a r i P r e p r o c e s s i n g runs 4 frames a t a time , max−p o o l i n g o v e r t h e l a s t 2
# frames . We need t o a c c o u n t
# i n t e r v a l s .
ATARI_FRAME_SKIP = 4

t h i s when computing t h i n g s

l i k e u p d a t e

f o r

c l a s s A t a r i C a t e g o r i c a l Q N e t w o r k ( network . Network ) :

" " " C a t e g o r i c a l Q N e t w o r k s u b c l a s s

t h a t d i v i d e s o b s e r v a t i o n s by 2 5 5 . " " "

def __init__ ( s e l f ,

input_tensor_spec , a c t i o n _ s p e c , ∗∗ kwargs ) :

super ( A t a r i C a t e g o r i c a l Q N e t w o r k ,

s e l f ) . __init__ (

input_tensor_spec ,

s t a t e _ s p e c =())

i n p u t _ t e n s o r _ s p e c = t f . TensorSpec (

dtype=t f . f l o a t 3 2 ,

shape=i n p u t _ t e n s o r _ s p e c . shape )

s e l f . _categorical_q_network = \

c a t e g o r i c a l _ q _ n e t w o r k . CategoricalQNetwork (

input_tensor_spec , a c t i o n _ s p e c , ∗∗ kwargs )

@property
def num_atoms ( s e l f ) :

return s e l f . _categorical_q_network . num_atoms

def c a l l ( s e l f , o b s e r v a t i o n ,

step_type=None , net w or k_ sta te = ( ) ) :

t f . f l o a t 3 2 )

s t a t e = t f . c a s t ( o b s e r v a t i o n ,
# We d i v i d e t h e g r a y s c a l e p i x e l v a l u e s by 255 h e r e r a t h e r than
# s t o r i n g n o r m a l i z e d v a l u e s b e a u s e u i n t 8 s a r e 4 x c h e a p e r t o
# s t o r e than f l o a t 3 2 s .
# TODO( b / 1 2 9 8 0 5 8 2 1 ) : h a n d l e t h e d i v i s i o n by 255 f o r
# t r a i n _ e v a l _ a t a r i . py i n
# a p r e p r o c e s s i n g l a y e r
s t a t e = s t a t e / 255
return s e l f . _categorical_q_network (

i n s t e a d .

s t a t e ,

step_type=step_type , n etwo r k_sta t e=ne two rk _state )

Next, we introduce two hyperparameters speciﬁc to the neural network we are about to deﬁne.

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

463

Code

fc_layer_params = ( 5 1 2 , )
conv_layer_params = ( ( 3 2 ,

( 8 , 8 ) , 4 ) ,

( 6 4 ,

( 4 , 4 ) , 2 ) ,

( 6 4 ,

( 3 , 3 ) , 1 ) )

q_net = A t a r i C a t e g o r i c a l Q N e t w o r k (
t r ai n _ e n v . o b s e r v a t i o n _ s p e c ( ) ,
t r ai n _ e n v . a c t i o n _ s p e c ( ) ,
conv_layer_params=conv_layer_params ,
fc_layer_params=fc_layer_params )

Convolutional neural networks usually comprise several alternating pairs of convolution and max-
pooling layers, ultimately culminating in one or more dense layers. These layers are the same types
as previously seen in this course. The QNetwork accepts two parameters that deﬁne the convolutional
neural network structure.

The more simple of the two parameters is fc_layer_params. This parameter speciﬁes the size of

each of the dense layers. A tuple speciﬁes the size of each of the layers in a list.

The second parameter, named conv_layer_params, is a list of convolution layers parameters, where
each item is a length-three tuple indicating (ﬁlters, kernel_size, stride). This implementation of QNetwork
supports only convolution layers. If you desire a more complex convolutional neural network, you must
deﬁne your variant of the QNetwork.

The QNetwork deﬁned here is not the agent. Instead, the QNetwork is used by the DQN agent to
implement the actual neural network. This technique allows ﬂexibility as you can set your class if needed.
Next, we deﬁne the optimizer. For this example, I used RMSPropOptimizer. However, AdamOptimizer

is another popular choice. We also created the DQN agent and referenced the Q-network.

Code

o p t i m i z e r = t f . compat . v1 . t r a i n . RMSPropOptimizer (

l e a r n i n g _ r a t e=l e a r n i n g _ r a t e ,
decay =0.95 ,
momentum=0.0 ,
e p s i l o n =0.00001 ,
c e n t e r e d=True )

t r a i n _ s t e p _ c o u n t e r = t f . V a r i a b l e ( 0 )

o b s e r v a t i o n _ s p e c = t e n s o r _ s p e c . from_spec ( t r a in _ e n v . o b s e r v a t i o n _ s p e c ( ) )
time_step_spec = t s . time_step_spec ( o b s e r v a t i o n _ s p e c )

a c t i o n _ s p e c = t e n s o r _ s p e c . from_spec ( t ra i n _ en v . a c t i o n _ s p e c ( ) )
t a r g e t _ u p d a t e _ p e r i o d = 32000 # ALE frames
update_period = 16 # ALE frames
_update_period = update_period / ATARI_FRAME_SKIP

464

CHAPTER 12. REINFORCEMENT LEARNING

a g e n t = c a t e g o r i c a l _ d q n _ a g e n t . CategoricalDqnAgent (

time_step_spec ,
a c t i o n _ s p e c ,
c a t e g o r i c a l _ q _ n e t w o r k=q_net ,
o p t i m i z e r=o p t i m i z e r ,
# e p s i l o n _ g r e e d y=e p s i l o n ,
n_step_update =1.0 ,
target_update_tau =1.0 ,
t a r g e t _ u p d a t e _ p e r i o d =(

t a r g e t _ u p d a t e _ p e r i o d / ATARI_FRAME_SKIP / _update_period ) ,

gamma=0.99 ,
r e w a r d _ s c a l e _ f a c t o r =1.0 ,
g r a d i e n t _ c l i p p i n g=None ,
debug_summaries=F a l s e ,
summarize_grads_and_vars=F a l s e )

a g e n t . i n i t i a l i z e ( )

12.4.6 Metrics and Evaluation

There are many diﬀerent ways to measure the eﬀectiveness of a model trained with reinforcement learning.
The loss function of the internal Q-network is not a good measure of the entire DQN algorithm’s overall
ﬁtness. The network loss function measures how close the Q-network ﬁts the collected data and does not
indicate how eﬀectively the DQN maximizes rewards. The method used for this example tracks the average
reward received over several episodes.

Code

def compute_avg_return ( environment , p o l i c y , num_episodes =10):

t o t a l _ r e t u r n = 0 . 0
for _ in range ( num_episodes ) :

time_step = environment . r e s e t ( )
e p i s o d e _ r e t u r n = 0 . 0

while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = environment . s t e p ( a c t i o n _ s t e p . a c t i o n )
e p i s o d e _ r e t u r n += time_step . reward

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

465

t o t a l _ r e t u r n += e p i s o d e _ r e t u r n

avg_return = t o t a l _ r e t u r n / num_episodes
return avg_return . numpy ( ) [ 0 ]

# See a l s o t h e m e t r i c s module f o r
# d i f f e r e n t m e t r i c s .
# h t t p s : / / g i t h u b . com/ t e n s o r f l o w / a g e n t s / t r e e / master / t f _ a g e n t s / m e t r i c s

s t a n d a r d i m p l e m e n t a t i o n s o f

12.4.7 Replay Buﬀer

DQN works by training a neural network to predict the Q-values for every possible environment state. A
neural network needs training data, so the algorithm accumulates this training data as it runs episodes.
The replay buﬀer is where this data is stored. Only the most recent episodes are stored; older episode data
rolls oﬀ the queue as the queue accumulates new data.

Code

r e p l a y _ b u f f e r = t f _ u n i f o r m _ r e p l a y _ b u f f e r . TFUniformReplayBuffer (

data_spec=a g e n t . c o l l e c t _ d a t a _ s p e c ,
b a t c h _ s i z e=t r a in _ e n v . b a t c h _ s i z e ,
max_length=replay_buffer_max_length )

# D a t a s e t g e n e r a t e s
d a t a s e t = r e p l a y _ b u f f e r . a s _ d a t a s e t (

t r a j e c t o r i e s w i t h sh ape [ Bx2x . . . ]

n u m _ p a r a l l e l _ c a l l s =3,
sample_batch_size=b a t c h _ s i z e ,
num_steps =2). p r e f e t c h ( 3 )

Output

WARNING: t e n s o r f l o w : From / u s r / l o c a l / l i b / python3 . 7 / d i s t −
p a c k a g e s / t e n s o r f l o w / python / autograph / impl / a p i . py : 3 7 7 :
R e p l a y B u f f e r . get_next
d e p r e c a t e d and w i l l be removed i n a f u t u r e v e r s i o n .
I n s t r u c t i o n s
Use ` a s _ d a t a s e t ( . . . ,

s i n g l e _ d e t e r m i n i s t i c _ p a s s=F a l s e )

f o r u p d a t i n g :

i n s t e a d .

( from t f _ a g e n t s . r e p l a y _ b u f f e r s . r e p l a y _ b u f f e r )

i s

466

CHAPTER 12. REINFORCEMENT LEARNING

12.4.8 Random Collection

The algorithm must prime the pump. Training cannot begin on an empty replay buﬀer. The following
code performs a predeﬁned number of steps to generate initial training data.

Code

random_policy = random_tf_policy . RandomTFPolicy ( t r ai n _ e nv . time_step_spec ( ) ,

t r ai n _ e nv . a c t i o n _ s p e c ( ) )

def c o l l e c t _ s t e p ( environment , p o l i c y , buffer ) :

time_step = environment . current_time_ste p ( )
a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
next_time_step = environment . s t e p ( a c t i o n _ s t e p . a c t i o n )
t r a j = t r a j e c t o r y . f r o m _ t r a n s i t i o n ( time_step , a c t i o n _ s t e p , \

next_time_step )

# Add t r a j e c t o r y t o t h e r e p l a y b u f f e r
buffer . add_batch ( t r a j )

def c o l l e c t _ d a t a ( env , p o l i c y , buffer ,

s t e p s ) :

for _ in range ( s t e p s ) :

c o l l e c t _ s t e p ( env , p o l i c y , buffer )

c o l l e c t _ d a t a ( train_env ,

random_policy ,

r e p l a y _ b u f f e r ,

s t e p s=i n i t i a l _ c o l l e c t _ s t e p s )

12.4.9 Training the Agent

We are now ready to train the DQN. Depending on how many episodes you wish to run through, this
process can take many hours. This code will update both the loss and average return as training occurs.
As training becomes more successful, the average return should increase. The losses reported reﬂecting the
average loss for individual training batches.

Code

i t e r a t o r = i t e r ( d a t a s e t )

# ( O p t i o n a l ) Optimize by wrapping some o f
# u s i n g TF f u n c t i o n .
a g e n t . t r a i n = common . f u n c t i o n ( a g e n t . t r a i n )

t h e code i n a graph

12.4. PART 12.4: ATARI GAMES WITH KERAS NEURAL NETWORKS

467

# R e s e t
a g e n t . t r a i n _ s t e p _ c o u n t e r . a s s i g n ( 0 )

t h e t r a i n s t e p

# E v a l u a t e t h e a g e n t ' s p o l i c y once b e f o r e t r a i n i n g .
avg_return = compute_avg_return ( eval_env , a g e n t . p o l i c y ,

num_eval_episodes )

r e t u r n s = [ avg_return ]

for _ in range ( n u m _ i t e r a t i o n s ) :

# C o l l e c t a few s t e p s u s i n g c o l l e c t _ p o l i c y and
# s a v e t o t h e r e p l a y b u f f e r .
for _ in range ( c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n ) :

c o l l e c t _ s t e p ( train_env , a g e n t . c o l l e c t _ p o l i c y ,

r e p l a y _ b u f f e r )

# Sample a b a t c h o f d a t a from t h e b u f f e r and
# u p d a t e t h e a g e n t ' s network .
e x p e r i e n c e , unused_info = next ( i t e r a t o r )
t r a i n _ l o s s = a g e n t . t r a i n ( e x p e r i e n c e ) . l o s s

s t e p = a g e n t . t r a i n _ s t e p _ c o u n t e r . numpy ( )

i f

s t e p % l o g _ i n t e r v a l == 0 :

print ( ' s t e p ␣=␣ { 0 } : ␣ l o s s ␣=␣ {1} ' . format ( s t e p ,

t r a i n _ l o s s ) )

i f

s t e p % e v a l _ i n t e r v a l == 0 :

avg_return = compute_avg_return ( eval_env , a g e n t . p o l i c y ,

num_eval_episodes )

print ( ' s t e p ␣=␣ { 0 } : ␣ Average ␣ Return ␣=␣ {1} ' . format ( s t e p , avg_return ) )
r e t u r n s . append ( avg_return )

Output

s t e p = 1 0 0 0 :
s t e p = 2 0 0 0 :
s t e p = 3 0 0 0 :
s t e p = 4 0 0 0 :
s t e p = 5 0 0 0 :
s t e p = 6 0 0 0 :
s t e p = 7 0 0 0 :
s t e p = 8 0 0 0 :

l o s s = 3 . 9 2 7 9 0 1 7 4 4 8 4 2 5 2 9 3
l o s s = 3 . 9 2 8 0 2 1 4 3 0 9 6 9 2 3 8 3
l o s s = 3 . 9 2 4 9 3 1 5 2 6 1 8 4 0 8 2
l o s s = 3 . 9 2 0 9 0 6 5 4 3 7 3 1 6 8 9 5
l o s s = 3 . 9 1 9 5 5 1 1 3 4 1 0 9 4 9 7
l o s s = 3 . 9 1 9 5 8 8 3 2 7 4 0 7 8 3 7
l o s s = 3 . 9 0 7 4 0 0 8 4 6 4 8 1 3 2 3 2
l o s s = 3 . 8 9 5 4 0 1 4 7 7 8 1 3 7 2 0 7

468

CHAPTER 12. REINFORCEMENT LEARNING

s t e p = 9 0 0 0 :
s t e p = 1 0 0 0 0 :

l o s s = 3 . 8 8 6 5 5 7 8 1 7 4 5 9 1 0 6 4
l o s s = 3 . 8 9 5 8 4 5 8 9 0 0 4 5 1 6 6

12.4.10 Videos

Perhaps the most compelling way to view an Atari game’s results is a video that allows us to see the agent
play the game. We now have a trained model and observed its training progress on a graph. The following
functions are deﬁned to watch the agent play the game in the notebook.

Code

def embed_mp4( f i l e n a m e ) :

" " " Embeds an mp4 f i l e
v i d e o = open ( f i l e n a m e ,
b64 = b a s e6 4 . b64encode ( v i d e o )
t a g = ' ' '

i n t h e n o t e b o o k . " " "

' rb ' ) . r e a d ( )

<v i d e o w i d t h ="640" h e i g h t ="480" c o n t r o l s >
<s o u r c e s r c =" d a t a : v i d e o /mp4 ; base64 , { 0 } "

t y p e =" v i d e o /mp4">

Your b r o w s e r d o e s not
</v i d e o > ' ' ' . format ( b64 . decode ( ) )

s u p p o r t

t h e v i d e o t a g .

return IPython . d i s p l a y .HTML( t a g )

def c r e a t e _ p o l i c y _ e v a l _ v i d e o ( p o l i c y ,
f i l e n a m e = f i l e n a m e + " . mp4"
with i m a g e i o . g e t _ w r i t e r ( f i l e n a m e ,
fo r _ in range ( num_episodes ) :

f i l e n a m e , num_episodes =5,

f p s =30):

f p s=f p s ) a s v i d e o :

time_step = eval_env . r e s e t ( )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )
while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = eval_env . s t e p ( a c t i o n _ s t e p . a c t i o n )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )

return embed_mp4( f i l e n a m e )

First, we will observe the trained agent play the game.

Code

c r e a t e _ p o l i c y _ e v a l _ v i d e o ( a g e n t . p o l i c y ,

" t r a i n e d −a g e n t " )

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

469

For comparison, we observe a random agent play. While the trained agent is far from perfect, with

enough training, it does outperform the random agent considerably.

Code

c r e a t e _ p o l i c y _ e v a l _ v i d e o ( random_policy ,

" random−a g e n t " )

12.5 Part 12.5: Application of Reinforcement Learning

Creating an environment is the ﬁrst step to applying TF-Agent-based reinforcement learning to a problem
with your design. This part will see how to create your environment and apply it to an agent that allows
actions to be ﬂoating-point values rather than the discrete actions employed by the Deep Q-Networks
(DQN) that we used earlier in this chapter. This new type of agent is called a Deep Deterministic Policy
Gradients (DDPG) network. From an application standpoint, the primary diﬀerence between DDPG and
DQN is that DQN only supports discrete actions, whereas DDPG supports continuous actions; however,
there are other essential diﬀerences that we will cover later in this chapter.

The environment that I will demonstrate in this chapter simulates paying oﬀ a mortgage and saving for
retirement. This simulation allows the agent to allocate their income between several types of accounts,
buying luxury items, and paying oﬀ their mortgage. The goal is to maximize net worth. Because we
wish to provide the agent with the ability to distribute their income among several accounts, we provide
continuous (ﬂoating point) actions that determine this distribution of the agent’s salary.

Similar to previous TF-Agent examples in this chapter, we begin by importing needed packages.

Code

import b a se 6 4
import i m a g e i o
import IPython
import m a t p l o t l i b
import m a t p l o t l i b . p y p l o t a s p l t
import numpy a s np
import PIL . Image
import p y v i r t u a l d i s p l a y
import math
import numpy a s np

import t e n s o r f l o w a s

t f

from t f _ a g e n t s . a g e n t s . ddpg import actor_network
from t f _ a g e n t s . a g e n t s . ddpg import c r i t i c _ n e t w o r k
from t f _ a g e n t s . a g e n t s . ddpg import ddpg_agent

from t f _ a g e n t s . a g e n t s . dqn import dqn_agent

470

CHAPTER 12. REINFORCEMENT LEARNING

import m e t r i c _ u t i l s

from t f _ a g e n t s . d r i v e r s import dynamic_step_driver
from t f _ a g e n t s . e n v i r o n m e n t s import suite_gym
from t f _ a g e n t s . e n v i r o n m e n t s import tf_py_environment
from t f _ a g e n t s . eval
from t f _ a g e n t s . m e t r i c s import t f _ m e t r i c s
from t f _ a g e n t s . networks import q_network
from t f _ a g e n t s . p o l i c i e s
from t f _ a g e n t s . r e p l a y _ b u f f e r s import t f _ u n i f o r m _ r e p l a y _ b u f f e r
from t f _ a g e n t s . t r a j e c t o r i e s
from t f _ a g e n t s . t r a j e c t o r i e s
from t f _ a g e n t s . u t i l s

import t r a j e c t o r y
import p o l i c y _ s t e p

import random_tf_policy

import common

import gym
from gym import s p a c e s
from gym . u t i l s
from gym . envs . r e g i s t r a t i o n import r e g i s t e r
import PIL . ImageDraw
import PIL . Image
from PIL import ImageFont

import s e e d i n g

If you get the following error, restart and rerun the Google CoLab environment. Sometimes a restart

is needed after installing TF-Agents.

A t t r i b u t e E r r o r : module ' g o o g l e . p r o t o b u f . d e s c r i p t o r ' has no

a t t r i b u t e

' _ i n t e r n a l _ c r e a t e _ k e y '

We create a virtual display to view the simulation in a Jupyter notebook.

Code

# S e t up a v i r t u a l d i s p l a y f o r
v d i s p l a y = p y v i r t u a l d i s p l a y . D i s p l a y ( v i s i b l e =0,

r e n d e r i n g OpenAI gym e n v i r o n m e n t s .

s i z e =(1400 , 9 0 0 ) ) . s t a r t ( )

12.5.1 Create an Environment of your Own

An environment is a simulator that your agent runs in. An environment must have a current state. Some
of this state is visible to the agent. However, the environment also hides some aspects of the state from
the agent. Likewise, the agent takes actions that will aﬀect the state of the environment. There may also
be internal actions outside the agent’s control. For example, in the ﬁnance simulator demonstrated in this
section, the agent does not control the investment returns or rate of inﬂation. Instead, the agent must
react to these external actions and state components.

The environment class that you create must contain these elements:

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

471

• Be a child class of gym.Env
• Implement a seed function that sets a seed that governs the simulation’s random aspects. For this

environment, the seed oversees the random ﬂuctuations in inﬂation and rates of return.

• Implement a reset function that resets the state for a new episode.
• Implement a render function that renders one frame of the simulation. The rendering is only for

display and does not aﬀect reinforcement learning.

• Implement a step function that performs one step of your simulation.

The class presented below implements a ﬁnancial planning simulation. The agent must save for retirement
and should attempt to amass the greatest possible net worth. The simulation includes the following key
elements:

• Random starting salary between 40K (USD) and 60K (USD).
• Home loan for a house with a random purchase price between 1.5 and 4 times the starting salary.
• Home loan is a standard amortized 30-year loan with a ﬁxed monthly payment.
• Paying higher than the home’s monthly payment pays the loan down quicker. Paying below the

monthly payment results in late fees and eventually foreclosure.

• Ability to allocate income between luxury purchases and home payments (above or below payment

amount) and a taxable and tax-advantaged savings account.

The state is composed of the following ﬂoating-point values:

• age - The agent’s current age in months (steps)
• salary - The agent’s starting salary, increases relative to inﬂation.
• home_value - The value of the agent’s home, increases relative to inﬂation.
• home_loan - How much the agent still owes on their home.
• req_home_pmt - The minimum required home payment.
• acct_tax_adv - The balance of the tax advantaged retirement account.
• acct_tax - The balance of the taxable retuirement account.

The action space is composed of the following ﬂoating-point values (between 0 and 1):

• home_loan - The amount to apply to a home loan.
• savings_tax_adv - The amount to deposit in a tax-advantaged savings account.
• savings taxable - The amount to deposit in a taxable savings account.
• luxury - The amount to spend on luxury items/services.

The actions are weights that the program converts to a percentage of the total. For example, the home
loan percentage is the home loan action value divided by all actions (including a home loan). The following
code implements the environment and provides implementation details in the comments.

Code

c l a s s SimpleGameOfLifeEnv (gym . Env ) :

metadata = {

' r e n d e r . modes ' :

[ ' human ' ,

' rgb_array ' ] ,

472

}

' v i d e o . frames_per_second ' : 1

CHAPTER 12. REINFORCEMENT LEARNING

STATE_ELEMENTS = 7
STATES = [ ' age ' ,

' s a l a r y ' ,

' home_value ' ,

' home_loan ' ,

' req_home_pmt ' ,

' acct_tax_adv ' ,
" t a x _ d e p o s i t " ,
" tax_adv_deposit " ,

" net_worth " ]

' acct_tax ' ,

" e x p e n s e s " ,

" actual_home_pmt " ,

STATE_AGE = 0
STATE_SALARY = 1
STATE_HOME_VALUE = 2
STATE_HOME_LOAN = 3
STATE_HOME_REQ_PAYMENT = 4
STATE_SAVE_TAX_ADV = 5
STATE_SAVE_TAXABLE = 6

MEG = 1 . 0 e6

ACTION_ELEMENTS = 4
ACTION_HOME_LOAN = 0
ACTION_SAVE_TAX_ADV = 1
ACTION_SAVE_TAXABLE = 2
ACTION_LUXURY = 3

INFLATION = ( 0 . 0 1 5 ) / 1 2 . 0
INTEREST = ( 0 . 0 5 ) / 1 2 . 0
TAX_RATE = ( . 1 4 2 ) / 1 2 . 0
EXPENSES = 0 . 6
INVEST_RETURN = 0 . 0 6 5 / 1 2 . 0
SALARY_LOW = 4 0 0 0 0 . 0
SALARY_HIGH = 6 0 0 0 0 . 0
START_AGE = 18
RETIRE_AGE = 80

def __init__ ( s e l f , g o a l _ v e l o c i t y =0):

s e l f . v e r b o s e = F a l s e
s e l f . v i e w e r = None

s e l f . a c t i o n _ s p a c e = s p a c e s . Box (

low =0.0 ,
h i g h =1.0 ,
shape =(SimpleGameOfLifeEnv .ACTION_ELEMENTS, ) ,
dtype=np . f l o a t 3 2

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

473

)
s e l f . o b s e r v a t i o n _ s p a c e = s p a c e s . Box (

low =0,
h i g h =2,
shape =(SimpleGameOfLifeEnv .STATE_ELEMENTS, ) ,
dtype=np . f l o a t 3 2

)

s e l f . s e e d ( )
s e l f . r e s e t ( )

s e l f . s t a t e _ l o g = [ ]

def s e e d ( s e l f ,

s e e d=None ) :

s e l f . np_random ,
return [ s e e d ]

s e e d = s e e d i n g . np_random ( s e e d )

def _calc_net_worth ( s e l f ) :

home_value = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_HOME_VALUE]

p r i n c i p a l = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_HOME_LOAN]

worth = home_value − p r i n c i p a l
worth += s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_SAVE_TAX_ADV]

worth += s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE]

return worth

def _eval_action ( s e l f , a c t i o n , payment ) :

# C a l c u l a t e a c t i o n s
act_home_payment = a c t i o n [

SimpleGameOfLifeEnv .ACTION_HOME_LOAN]

act_tax_adv_pay = a c t i o n [

SimpleGameOfLifeEnv .ACTION_SAVE_TAX_ADV]

a c t _ t a x a b l e = a c t i o n [

SimpleGameOfLifeEnv .ACTION_SAVE_TAXABLE]

act_luxury = a c t i o n [

SimpleGameOfLifeEnv .ACTION_LUXURY]

i f payment <= 0 :

act_home_payment = 0

t o t a l _ a c t = act_home_payment + act_tax_adv_pay \

+ a c t _ t a x a b l e + \

474

CHAPTER 12. REINFORCEMENT LEARNING

act_luxury + s e l f . e x p e n s e s

i f

t o t a l _ a c t < 1 e −2:

pct_home_payment = 0
pct_tax_adv_pay = 0
p c t _ t a x a b l e = 0
pct_luxury = 0

e l s e :

pct_home_payment = act_home_payment / t o t a l _ a c t
pct_tax_adv_pay = act_tax_adv_pay / t o t a l _ a c t
p c t _ t a x a b l e = a c t _ t a x a b l e / t o t a l _ a c t
pct_luxury = act_luxury / t o t a l _ a c t

return pct_home_payment , pct_tax_adv_pay , pct_taxable , pct_luxury

def

s t e p ( s e l f , a c t i o n ) :
s e l f . l a s t _ a c t i o n = a c t i o n
age = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_AGE]
s a l a r y = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SALARY]
home_value = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_VALUE]
p r i n c i p a l = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_LOAN]
payment = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT]
n e t 1 = s e l f . _calc_net_worth ( )
r e m a i n i n g _ s a l a r y = s a l a r y

# C a l c u l a t e a c t i o n s
pct_home_payment , pct_tax_adv_pay , pct_taxable , pct_luxury = \

s e l f . _eval_action ( a c t i o n , payment )

# Expenses
c u r r e n t _ e x p e n s e s = s a l a r y ∗ s e l f . e x p e n s e s
r e m a i n i n g _ s a l a r y −= c u r r e n t _ e x p e n s e s
i f

s e l f . v e r b o s e :
print ( f " Expenses : ␣{ c u r r e n t _ e x p e n s e s } " )
print ( f " Remaining ␣ S a l a r y : ␣ { r e m a i n i n g _ s a l a r y } " )

# Tax a d v a n t a g e d d e p o s i t a c t i o n
my_tax_adv_deposit = min( s a l a r y ∗ pct_tax_adv_pay ,

# Govt CAP
my_tax_adv_deposit = min( my_tax_adv_deposit ,

r e m a i n i n g _ s a l a r y )

s e l f . y ear_tax_ad v_deposit_le ft −= my_tax_adv_deposit

s e l f . y ear_ta x_a d v _deposi t_l eft )

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

475

r e m a i n i n g _ s a l a r y −= my_tax_adv_deposit
# Company match
tax_adv_deposit = my_tax_adv_deposit ∗ 1 . 0 5
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SAVE_TAX_ADV] += \

int ( tax_adv_deposit )

i f

s e l f . v e r b o s e :
print ( f " IRA␣ D e p o s i t : ␣ { tax_adv_deposit } " )
print ( f " Remaining ␣ S a l a r y : ␣ { r e m a i n i n g _ s a l a r y } " )

# Tax
r e m a i n i n g _ s a l a r y −= r e m a i n i n g _ s a l a r y ∗ \

i f

SimpleGameOfLifeEnv .TAX_RATE
s e l f . v e r b o s e :
print ( f " Tax␣ S a l a r y : ␣{ r e m a i n i n g _ s a l a r y } " )

# Home payment
actual_payment = min( s a l a r y ∗ pct_home_payment ,

r e m a i n i n g _ s a l a r y )

i f p r i n c i p a l > 0 :

i p a r t = p r i n c i p a l ∗ SimpleGameOfLifeEnv . INTEREST
p p a r t = actual_payment − i p a r t
p r i n c i p a l = int ( p r i n c i p a l −p p a r t )
i f p r i n c i p a l <= 0 :

p r i n c i p a l = 0
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT] = 0

e l i f actual_payment < payment :

s e l f . l a t e _ c o u n t += 1
i f

s e l f . l a t e _ c o u n t > 1 5 :

s e l l = ( home_value−p r i n c i p a l ) / 2
s e l l −= 20000
s e l l = max( s e l l , 0 )
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE] \

+= s e l l
p r i n c i p a l = 0
home_value = 0
s e l f . e x p e n s e s += . 3
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT] \

i f

= 0
s e l f . v e r b o s e :
print ( f " F o r e c l o s u r e ! ! " )

e l s e :

476

CHAPTER 12. REINFORCEMENT LEARNING

l a t e _ f e e = payment ∗ 0 . 1
p r i n c i p a l += l a t e _ f e e
i f

s e l f . v e r b o s e :
print ( f " Late ␣ Fee : ␣{ l a t e _ f e e } " )

s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_LOAN] = p r i n c i p a l
r e m a i n i n g _ s a l a r y −= actual_payment

i f

s e l f . v e r b o s e :
print ( f "Home␣Payment : ␣{ actual_payment } " )
print ( f " Remaining ␣ S a l a r y : ␣ { r e m a i n i n g _ s a l a r y } " )

# T a x a b l e s a v i n g s
a c t u a l _ s a v i n g s = r e m a i n i n g _ s a l a r y ∗ p c t _ t a x a b l e
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE] \

+= a c t u a l _ s a v i n g s

r e m a i n i n g _ s a l a r y −= a c t u a l _ s a v i n g s

i f

s e l f . v e r b o s e :
print ( f " Tax␣ Save : ␣{ a c t u a l _ s a v i n g s } " )
print ( f " Remaining ␣ S a l a r y ␣ ( g o e s ␣ t o ␣ Luxury ) : ␣ { r e m a i n i n g _ s a l a r y } " )

# I n v e s t m e n t
r e t u r n _ t a x a b l e = s e l f . s t a t e [

income

SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE] \
∗ s e l f . i n v e s t _ r e t u r n
return_tax_adv = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_SAVE_TAX_ADV] \
∗ s e l f . i n v e s t _ r e t u r n

r e t u r n _ t a x a b l e ∗= 1−SimpleGameOfLifeEnv .TAX_RATE
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE] \

+= r e t u r n _ t a x a b l e

s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SAVE_TAX_ADV] \

+= return_tax_adv

# Y e a r l y e v e n t s
i f age > 0 and age % 12 == 0 :
s e l f . p e r f o r m _ y e a r l y ( )

# Monthly e v e n t s
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_AGE] += 1

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

477

# Time t o r e t i r e ( by age ?)
done = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_AGE] > \

( SimpleGameOfLifeEnv .RETIRE_AGE∗ 1 2 )

# C a l c u l a t e reward
n e t 2 = s e l f . _calc_net_worth ( )
reward = n e t 2 − n e t 1

# Track p r o g r e s s
s e l f . v e r b o s e :
i f
print ( f " Networth : ␣{nw} " )
print ( f " ∗∗∗ ␣End␣ Step ␣{ s e l f . step_num } : ␣ S t a t e={ s e l f . s t a t e } , ␣ \

␣␣␣␣␣␣␣␣␣␣Reward={reward } " )

s e l f . s t a t e _ l o g . append ( s e l f . s t a t e + [ c u r r e n t _ e x p e n s e s ,

actual_payment ,
a c t u a l _ s a v i n g s ,
my_tax_adv_deposit ,
n e t 2 ] )

s e l f . step_num += 1

# Normalize s t a t e and f i n i s h up
norm_state = [ x/ SimpleGameOfLifeEnv .MEG for x in s e l f . s t a t e ]
return norm_state ,

reward / SimpleGameOfLifeEnv .MEG, done , {}

def p e r f o r m _ y e a r l y ( s e l f ) :

s a l a r y = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SALARY]
home_value = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_VALUE]

s e l f . i n f l a t i o n = SimpleGameOfLifeEnv . INTEREST + \

s e l f . np_random . normal ( l o c =0,

s c a l e =1e −2)

s e l f . i n v e s t _ r e t u r n = SimpleGameOfLifeEnv .INVEST_RETURN + \

s e l f . np_random . normal ( l o c =0,

s c a l e =1e −2)

s e l f . y ear_tax_ad v_deposit_le ft = 19000
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SALARY] = \

int ( s a l a r y ∗ (1+ s e l f . i n f l a t i o n ) )

s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_VALUE] \

= int ( home_value ∗ (1+ s e l f . i n f l a t i o n ) )

def

r e s e t ( s e l f ) :
s e l f . e x p e n s e s = SimpleGameOfLifeEnv . EXPENSES
s e l f . l a t e _ c o u n t = 0

478

CHAPTER 12. REINFORCEMENT LEARNING

s e l f . step_num = 0
s e l f . l a s t _ a c t i o n = [ 0 ] ∗ SimpleGameOfLifeEnv .ACTION_ELEMENTS
s e l f . s t a t e = [ 0 ] ∗ SimpleGameOfLifeEnv .STATE_ELEMENTS
s e l f . s t a t e _ l o g = [ ]
s a l a r y = f l o a t ( s e l f . np_random . r a n d i n t (

low=SimpleGameOfLifeEnv .SALARY_LOW,
h i g h=SimpleGameOfLifeEnv .SALARY_HIGH) )

house_mult = s e l f . np_random . uniform ( low =1.5 , h i g h =4)
v a l u e = round ( s a l a r y ∗ house_mult )
p = ( v a l u e ∗ 0 . 9 )
i = SimpleGameOfLifeEnv . INTEREST
n = 30 ∗ 12
m = f l o a t ( int ( p ∗ ( i ∗ ( 1 + i ) ∗ ∗ n ) / ( ( 1 + i ) ∗ ∗ n − 1 ) ) )
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_AGE] = \

SimpleGameOfLifeEnv .START_AGE ∗ 12

s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SALARY] = s a l a r y / 1 2 . 0
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_VALUE] = v a l u e
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_LOAN] = p
s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT] = m
s e l f . y ear_tax_ad v_deposit_le ft = 19000
s e l f . p e r f o r m _ y e a r l y ( )
return np . a r r a y ( s e l f . s t a t e )

def

r e n d e r ( s e l f , mode= ' human ' ) :
screen_width = 600
s c r e e n _ h e i g h t = 400

img = PIL . Image . new ( 'RGB ' ,
d = PIL . ImageDraw . Draw ( img )
f o n t = ImageFont . l o a d _ d e f a u l t ( )
y = 0
_, h e i g h t = d . t e x t s i z e ( "W" ,

f o n t )

( 6 0 0 , 4 0 0 ) )

age = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_AGE]
s a l a r y = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_SALARY] ∗ 1 2
home_value = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_HOME_VALUE]

home_loan = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_HOME_LOAN]

home_payment = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT]

balance_tax_adv = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_SAVE_TAX_ADV]

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

479

b a l a n c e _ t a x a b l e = s e l f . s t a t e [

SimpleGameOfLifeEnv .STATE_SAVE_TAXABLE]

net_worth = s e l f . _calc_net_worth ( )

d . t e x t ( ( 0 , y ) ,
y += h e i g h t
d . t e x t ( ( 0 , y ) ,
y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f " Age : ␣{ age /12} " ,

f i l l =(0 , 2 5 5 , 0 ) )

f " S a l a r y : ␣{ s a l a r y : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

f "Home␣ Value : ␣{ home_value : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f "Home␣Loan : ␣{home_loan : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f "Home␣Payment : ␣{home_payment : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f " Balance ␣Tax␣Adv : ␣{ balance_tax_adv : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f " Balance ␣ Taxable : ␣ { b a l a n c e _ t a x a b l e : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,
y += h e i g h t ∗2

f " Net ␣Worth : ␣{ net_worth : , } " ,

f i l l =(0 , 2 5 5 , 0 ) )

payment = s e l f . s t a t e [ SimpleGameOfLifeEnv .STATE_HOME_REQ_PAYMENT]
pct_home_payment , pct_tax_adv_pay , pct_taxable , pct_luxury = \

s e l f . _eval_action ( s e l f . l a s t _ a c t i o n , payment )

d . t e x t ( ( 0 , y ) ,

f " P e r c e n t ␣Home␣Payment : ␣{pct_home_payment} " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f " P e r c e n t ␣Tax␣Adv : ␣{pct_tax_adv_pay} " ,

f i l l =(0 , 2 5 5 , 0 ) )

y += h e i g h t
d . t e x t ( ( 0 , y ) ,
y += h e i g h t
d . t e x t ( ( 0 , y ) ,

f " P e r c e n t ␣ Taxable : ␣{ p c t _ t a x a b l e } " ,

f i l l =(0 , 2 5 5 , 0 ) )

f " P e r c e n t ␣ Luxury : ␣ { pct_luxury } " ,

f i l l =(0 , 2 5 5 , 0 ) )

return np . a r r a y ( img )

def c l o s e ( s e l f ) :

pass

480

CHAPTER 12. REINFORCEMENT LEARNING

You must register the environment class with TF-Agents before your program can use it.

Code

r e g i s t e r (

id= ' s i m p l e −game−of −l i f e −v0 ' ,
e n t r y _ p o i n t=f ' {__name__} : SimpleGameOfLifeEnv ' ,

)

12.5.2 Testing the Environment

This ﬁnancial planning environment is complex. It took me some degree of testing to perfect it. Even at
the current state of this simulator, it is far from a complete ﬁnancial simulator. The primary objective of
this simulator is to demonstrate creating your environment for a non-video game project.

I used the following code to help test this simulator. I ran the simulator with ﬁxed actions and then

loaded the state into a Pandas data frame for easy viewing.

Code

env_name = ' s i m p l e −game−of −l i f e −v0 '
env = gym . make ( env_name )

env . r e s e t ( )
done = F a l s e

i = 0
env . v e r b o s e = F a l s e
while not done :
i += 1
s t a t e ,
env . r e n d e r ( )

reward , done , _ = env . s t e p ( [ 1 , 1 , 0 , 0 ] )

env . c l o s e ( )

Code

import pandas a s pd

d f = pd . DataFrame ( env . s t a t e _ l o g , columns=SimpleGameOfLifeEnv . STATES)
d f = d f . round ( 0 )
d f [ ' age ' ] = d f [ ' age ' ] / 1 2
d f [ ' age ' ] = d f [ ' age ' ] . round ( 2 )
in d f . columns :
for c o l

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

481

d f [ c o l ] = d f [ c o l ] . apply (lambda x :

" { : , } " . format ( x ) )

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 7 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 1 2 )
d i s p l a y ( d f )

Output

age
18.08
18.17
18.25
18.33
18.42
...
79.75
79.83
79.92
80.0
80.08

salary
4,876
4,876
4,876
4,876
4,876
...
6,830
6,830
6,830
6,830
6,816

home_value
214,749
214,749
214,749
214,749
214,749
...
302,304
302,304
302,304
302,304
301,724

...
...
...
...
...
...
...
...
...
...
...
...

tax_deposit
0.0
0.0
0.0
0.0
0.0
...
0.0
0.0
0.0
0.0
0.0

tax_adv_deposit
1,880.0
1,875.0
1,875.0
1,875.0
1,875.0
...
683.0
683.0
683.0
683.0
683.0

net_worth
24,578.0
25,791.0
27,039.0
28,321.0
29,640.0
...
3,990,102.0
3,989,629.0
3,989,157.0
3,988,684.0
3,987,632.0

0
1
2
3
4
...
740
741
742
743
744

1810888.5833333335

12.5.3 Hyperparameters

I tuned the following hyperparameters to get a reasonable result from training the agent. Further opti-
mization would be beneﬁcial.

Code

# How l o n g s h o u l d t r a i n i n g run ?
n u m _ i t e r a t i o n s = 3000
# How o f t e n s h o u l d t h e program p r o v i d e an u p d a t e .
l o g _ i n t e r v a l = 500

i n i t i a l d a t a .

# How many i n i t i a l random s t e p s , b e f o r e t r a i n i n g s t a r t ,
# c o l l e c t
i n i t i a l _ c o l l e c t _ s t e p s = 1000
# How many s t e p s
# d a t a from .
c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n = 50
# How much d a t a s h o u l d we s t o r e f o r
replay_buffer_max_length = 100000

s h o u l d we run each i t e r a t i o n t o c o l l e c t

t r a i n i n g e x a m p l e s .

t o

482

CHAPTER 12. REINFORCEMENT LEARNING

b a t c h _ s i z e = 64

# How many e p i s o d e s
num_eval_episodes = 100
# How o f t e n s h o u l d an e v a l u a t i o n o c c u r .
e v a l _ i n t e r v a l = 5000

s h o u l d t h e program use f o r each e v a l u a t i o n .

12.5.4 Instantiate the Environment

We are now ready to make use of our environment. Because we registered the environment with TF-Agents
the program can load the environment by its name "simple-game-of-life-v".

Code

env_name = ' s i m p l e −game−of −l i f e −v0 '
#env_name = ' MountainCarContinuous−v0 '
env = suite_gym . l o a d ( env_name )

We can now have a quick look at the ﬁrst state rendered. Here we can see the random salary and home
values are chosen for an agent. The learned policy must be able to consider diﬀerent starting salaries and
home values and ﬁnd an appropriate strategy.

Code

env . r e s e t ( )
PIL . Image . f r o m a r r a y ( env . r e n d e r ( ) )

Output

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

483

Just as before, the program instantiates two environments: one for training and one for evaluation.

Code

train_py_env = suite_gym . l o a d ( env_name )
eval_py_env = suite_gym . l o a d ( env_name )

t r ai n _ e nv = tf_py_environment . TFPyEnvironment ( train_py_env )
eval_env = tf_py_environment . TFPyEnvironment ( eval_py_env )

You might be wondering why a DQN does not support continuous actions. This limitation is that
the DQN algorithm maps each action as an output neuron. Each of these neurons predicts the likely
future reward for taking each action. The algorithm knows the future rewards for each particular action.
Generally, the DQN agent will perform the action that has the highest reward. However, because a
continuous number represented in a computer has an eﬀectively inﬁnite number of possible values, it is not
possible to calculate a future reward estimate for all of them.

We will use the Deep Deterministic Policy Gradients (DDPG) algorithm to provide a continuous action
space.[22]This technique uses two neural networks. The ﬁrst neural network, called an actor, acts as the
agent and predicts the expected reward for a given value of the action. The second neural network, called a
critic, is trained to predict the accuracy of the actor-network. Training two neural networks in parallel that
operate adversarially is a popular technique. Earlier in this course, we saw that Generative Adversarial
Networks (GAN) used a similar method. Figure 12.4 shows the structure of the DDPG network that we
will use.

The environment provides the same input (x(t)) for each time step to both the actor and critic networks.
The temporal diﬀerence error (r(t)) reports the diﬀerence between the estimated reward and the actual
reward at any given state or time step.

The following code creates the actor and critic neural networks.

484

CHAPTER 12. REINFORCEMENT LEARNING

Figure 12.4: Actor Critic Model

Code

a c t o r _ f c _ l a y e r s = ( 4 0 0 , 3 0 0 )
c r i t i c _ o b s _ f c _ l a y e r s = ( 4 0 0 , )
c r i t i c _ a c t i o n _ f c _ l a y e r s = None
c r i t i c _ j o i n t _ f c _ l a y e r s = ( 3 0 0 , )
ou_stddev = 0 . 2
ou_damping = 0 . 1 5
target_update_tau = 0 . 0 5
t a r g e t _ u p d a t e _ p e r i o d = 5
d q d a _ c l i p p i n g = None
t d _ e r r o r s _ l o s s _ f n = t f . compat . v1 . l o s s e s . h u b e r _ l o s s
gamma = 0 . 9 9 5
r e w a r d _ s c a l e _ f a c t o r = 1 . 0
g r a d i e n t _ c l i p p i n g = None

a c t o r _ l e a r n i n g _ r a t e = 1 e−4
c r i t i c _ l e a r n i n g _ r a t e = 1 e−3
debug_summaries = F a l s e
summarize_grads_and_vars = F a l s e

g l o b a l _ s t e p = t f . compat . v1 . t r a i n . g e t _ o r _ c r e a t e _ g l o b a l _ s t e p ( )

a c t o r _ n e t = actor_network . ActorNetwork (

t r ai n _ e n v . time_step_spec ( ) . o b s e r v a t i o n ,
t r ai n _ e n v . a c t i o n _ s p e c ( ) ,
fc_layer_params=a c t o r _ f c _ l a y e r s ,

)

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

485

c r i t i c _ n e t _ i n p u t _ s p e c s = ( tr a i n _ en v . time_step_spec ( ) . o b s e r v a t i o n ,

t r a in _ e n v . a c t i o n _ s p e c ( ) )

c r i t i c _ n e t = c r i t i c _ n e t w o r k . C r i t i c N e t w o r k (

c r i t i c _ n e t _ i n p u t _ s p e c s ,
o b s e r v a t i o n _ f c _ l a y e r _ p a r a m s=c r i t i c _ o b s _ f c _ l a y e r s ,
action_fc_layer_params=c r i t i c _ a c t i o n _ f c _ l a y e r s ,
j o i n t _ f c _ l a y e r _ p a r a m s=c r i t i c _ j o i n t _ f c _ l a y e r s ,

)

t f _ a g e n t = ddpg_agent . DdpgAgent (

t r ai n _ e n v . time_step_spec ( ) ,
t r ai n _ e n v . a c t i o n _ s p e c ( ) ,
actor_network=actor_net ,
c r i t i c _ n e t w o r k=c r i t i c _ n e t ,
a c t o r _ o p t i m i z e r=t f . compat . v1 . t r a i n . AdamOptimizer (

l e a r n i n g _ r a t e=a c t o r _ l e a r n i n g _ r a t e ) ,

c r i t i c _ o p t i m i z e r=t f . compat . v1 . t r a i n . AdamOptimizer (

l e a r n i n g _ r a t e=c r i t i c _ l e a r n i n g _ r a t e ) ,

ou_stddev=ou_stddev ,
ou_damping=ou_damping ,
target_update_tau=target_update_tau ,
t a r g e t _ u p d a t e _ p e r i o d=target_update_period ,
d q d a _ c l i p p i n g=dqda_clipping ,
t d _ e r r o r s _ l o s s _ f n=t d _ e r r o r s _ l o s s _ f n ,
gamma=gamma ,
r e w a r d _ s c a l e _ f a c t o r=r e w a r d _ s c a l e _ f a c t o r ,
g r a d i e n t _ c l i p p i n g=g r a d i e n t _ c l i p p i n g ,
debug_summaries=debug_summaries ,
summarize_grads_and_vars=summarize_grads_and_vars ,
t r a i n _ s t e p _ c o u n t e r=g l o b a l _ s t e p )

t f _ a g e n t . i n i t i a l i z e ( )

12.5.5 Metrics and Evaluation

Just as in previous examples, we will compute the average return over several episodes to evaluate perfor-
mance.
Code

def compute_avg_return ( environment , p o l i c y , num_episodes =10):

t o t a l _ r e t u r n = 0 . 0

486

CHAPTER 12. REINFORCEMENT LEARNING

for _ in range ( num_episodes ) :

time_step = environment . r e s e t ( )
e p i s o d e _ r e t u r n = 0 . 0

while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = environment . s t e p ( a c t i o n _ s t e p . a c t i o n )
e p i s o d e _ r e t u r n += time_step . reward

t o t a l _ r e t u r n += e p i s o d e _ r e t u r n

avg_return = t o t a l _ r e t u r n / num_episodes
return avg_return . numpy ( ) [ 0 ]

# See a l s o t h e m e t r i c s module f o r
# d i f f e r e n t m e t r i c s .
# h t t p s : / / g i t h u b . com/ t e n s o r f l o w / a g e n t s / t r e e / master / t f _ a g e n t s / m e t r i c s

s t a n d a r d i m p l e m e n t a t i o n s o f

12.5.6 Data Collection

Now execute the random policy in the environment for a few steps, recording the data in the replay buﬀer.

Code

def c o l l e c t _ s t e p ( environment , p o l i c y , buffer ) :

time_step = environment . current_time_ste p ( )
a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
next_time_step = \

environment . s t e p ( a c t i o n _ s t e p . a c t i o n )

t r a j = t r a j e c t o r y . f r o m _ t r a n s i t i o n ( \

time_step , a c t i o n _ s t e p , \
next_time_step )

# Add t r a j e c t o r y t o t h e r e p l a y b u f f e r
buffer . add_batch ( t r a j )

def c o l l e c t _ d a t a ( env , p o l i c y , buffer ,

s t e p s ) :

for _ in range ( s t e p s ) :

c o l l e c t _ s t e p ( env , p o l i c y , buffer )

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

487

random_policy = random_tf_policy . RandomTFPolicy ( \

t r ai n _ e n v . time_step_spec ( ) , \
t r ai n _ e n v . a c t i o n _ s p e c ( ) )

r e p l a y _ b u f f e r = t f _ u n i f o r m _ r e p l a y _ b u f f e r . TFUniformReplayBuffer (

data_spec=t f _ a g e n t . c o l l e c t _ d a t a _ s p e c ,
b a t c h _ s i z e=t r a in _ e n v . b a t c h _ s i z e ,
max_length=replay_buffer_max_length )

c o l l e c t _ d a t a ( train_env ,

random_policy ,

r e p l a y _ b u f f e r ,

s t e p s =100)

# D a t a s e t g e n e r a t e s
d a t a s e t = r e p l a y _ b u f f e r . a s _ d a t a s e t (

t r a j e c t o r i e s w i t h sh ape [ Bx2x . . . ]

n u m _ p a r a l l e l _ c a l l s =3,
sample_batch_size=b a t c h _ s i z e ,
num_steps =2). p r e f e t c h ( 3 )

Output

WARNING: t e n s o r f l o w : From / u s r / l o c a l / l i b / python3 . 7 / d i s t −
p a c k a g e s / t e n s o r f l o w / python / autograph / impl / a p i . py : 3 7 7 :
R e p l a y B u f f e r . get_next
d e p r e c a t e d and w i l l be removed i n a f u t u r e v e r s i o n .
I n s t r u c t i o n s
Use ` a s _ d a t a s e t ( . . . ,

s i n g l e _ d e t e r m i n i s t i c _ p a s s=F a l s e )

f o r u p d a t i n g :

i n s t e a d .

( from t f _ a g e n t s . r e p l a y _ b u f f e r s . r e p l a y _ b u f f e r )

i s

12.5.7 Training the agent

We are now ready to train the agent. Depending on how many episodes you wish to run through, this
process can take many hours. This code will update on both the loss and average return as training occurs.
As training becomes more successful, the average return should increase. The losses reported reﬂect the
average loss for individual training batches.

Code

i t e r a t o r = i t e r ( d a t a s e t )

# ( O p t i o n a l ) Optimize by wrapping some o f
# TF f u n c t i o n .
t f _ a g e n t . t r a i n = common . f u n c t i o n ( t f _ a g e n t . t r a i n )

t h e code i n a graph u s i n g

488

CHAPTER 12. REINFORCEMENT LEARNING

# R e s e t
t f _ a g e n t . t r a i n _ s t e p _ c o u n t e r . a s s i g n ( 0 )

t h e t r a i n s t e p

# E v a l u a t e t h e a g e n t ' s p o l i c y once b e f o r e t r a i n i n g .
avg_return = compute_avg_return ( eval_env ,

t f _ a g e n t . p o l i c y ,

num_eval_episodes )

r e t u r n s = [ avg_return ]

for _ in range ( n u m _ i t e r a t i o n s ) :

# C o l l e c t a few s t e p s u s i n g c o l l e c t _ p o l i c y and
# s a v e t o t h e r e p l a y b u f f e r .
for _ in range ( c o l l e c t _ s t e p s _ p e r _ i t e r a t i o n ) :

c o l l e c t _ s t e p ( train_env ,

t f _ a g e n t . c o l l e c t _ p o l i c y ,

r e p l a y _ b u f f e r )

# Sample a b a t c h o f d a t a from t h e b u f f e r and u p d a t e t h e
# a g e n t ' s network .
e x p e r i e n c e , unused_info = next ( i t e r a t o r )
t r a i n _ l o s s = t f _ a g e n t . t r a i n ( e x p e r i e n c e ) . l o s s

s t e p = t f _ a g e n t . t r a i n _ s t e p _ c o u n t e r . numpy ( )

i f

s t e p % l o g _ i n t e r v a l == 0 :

print ( ' s t e p ␣=␣ { 0 } : ␣ l o s s ␣=␣ {1} ' . format ( s t e p ,

t r a i n _ l o s s ) )

i f

s t e p % e v a l _ i n t e r v a l == 0 :

avg_return = compute_avg_return ( eval_env ,

t f _ a g e n t . p o l i c y ,

num_eval_episodes )

print ( ' s t e p ␣=␣ { 0 } : ␣ Average ␣ Return ␣=␣ {1} ' . format ( s t e p , avg_return ) )
r e t u r n s . append ( avg_return )

Output

s t e p = 5 0 0 :
s t e p = 1 0 0 0 :
s t e p = 1 5 0 0 :
s t e p = 2 0 0 0 :
s t e p = 2 5 0 0 :
s t e p = 3 0 0 0 :

l o s s = 0 . 0 0 0 1 6 3 5 1 1 9 9 8 8 3 5 0 1 9 7 7
l o s s = 6 . 3 4 3 8 1 0 6 7 3 5 7 0 2 6 e −05
l o s s = 0 . 0 0 1 2 6 6 6 2 4 3 1 2 8 4 7 8 5 2 7
l o s s = 0 . 0 0 0 4 1 3 2 1 0 3 0 5 8 5 0 9 5 2 8 6
l o s s = 0 . 0 0 0 6 3 2 1 9 4 1 6 1 8 8 0 7 6 1 4
l o s s = 0 . 0 0 0 6 6 1 1 0 0 5 5 1 9 5 1 6 7 6 6

12.5. PART 12.5: APPLICATION OF REINFORCEMENT LEARNING

489

12.5.8 Visualization

The notebook can plot the average return over training iterations. The average return should increase as
the program performs more training iterations.

12.5.9 Videos

We use the following functions to produce video in Jupyter notebook. As the person moves through their
career, they focus on paying oﬀ the house and tax advantage investing.

Code

def embed_mp4( f i l e n a m e ) :

" " " Embeds an mp4 f i l e
v i d e o = open ( f i l e n a m e ,
b64 = b a s e6 4 . b64encode ( v i d e o )
t a g = ' ' '

i n t h e n o t e b o o k . " " "

' rb ' ) . r e a d ( )

<v i d e o w i d t h ="640" h e i g h t ="480" c o n t r o l s >
<s o u r c e s r c =" d a t a : v i d e o /mp4 ; base64 , { 0 } "

t y p e =" v i d e o /mp4">

Your b r o w s e r d o e s not
</v i d e o > ' ' ' . format ( b64 . decode ( ) )

s u p p o r t

t h e v i d e o t a g .

return IPython . d i s p l a y .HTML( t a g )

def c r e a t e _ p o l i c y _ e v a l _ v i d e o ( p o l i c y ,
f i l e n a m e = f i l e n a m e + " . mp4"
with i m a g e i o . g e t _ w r i t e r ( f i l e n a m e ,
for _ in range ( num_episodes ) :

f i l e n a m e , num_episodes =5,

f p s =30):

f p s=f p s ) a s v i d e o :

time_step = eval_env . r e s e t ( )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )
while not time_step . i s _ l a s t ( ) :

a c t i o n _ s t e p = p o l i c y . a c t i o n ( time_step )
time_step = eval_env . s t e p ( a c t i o n _ s t e p . a c t i o n )
v i d e o . append_data ( eval_py_env . r e n d e r ( ) )

return embed_mp4( f i l e n a m e )

c r e a t e _ p o l i c y _ e v a l _ v i d e o ( t f _ a g e n t . p o l i c y ,

" t r a i n e d −a g e n t " )

490

CHAPTER 12. REINFORCEMENT LEARNING

Chapter 13

Advanced/Other Topics

13.1 Part 13.1: Flask and Deep Learning Web Services

Suppose you would like to create websites based on neural networks. In that case, we must expose the
neural network so that Python and other programming languages can eﬃciently execute. The usual means
for such integration is a web service. One of the most popular libraries for doing this in Python is Flask.
This library allows you to quickly deploy your Python applications, including TensorFlow, as web services.
Neural network deployment is a complex process, usually carried out by a company’s Information
Technology (IT) group. When large numbers of clients must access your model, scalability becomes es-
sential. The cloud usually handles this. The designers of Flask did not design for high-volume systems.
When deployed to production, you will wrap models in Gunicorn or TensorFlow Serving. We will discuss
high-volume cloud deployment in the next section. Everything presented in this part ith Flask is directly
compatible with the higher volume Gunicorn system. When early in the development process, it is common
to use Flask directly.

13.1.1 Flask Hello World

Flask is the server, and Jupyter usually ﬁlls the role of the client. It is uncommon to run Flask from a
Jupyter notebook. However, we can run a simple web service from Jupyter. We will quickly move beyond
this and deploy using a Python script (.py). Because we must use .py ﬁles, it won’t be easy to use Google
CoLab, as you will be running from the command line. For now, let’s execute a Flask web container in
Jupyter.

Code

from werkzeug . wrappers import Request , Response
from f l a s k import F l a s k

app = F l a s k (__name__)

491

492

CHAPTER 13. ADVANCED/OTHER TOPICS

@app . r o u t e ( " / " )
def h e l l o ( ) :

return " H e l l o ␣World ! "

i f __name__ == '__main__ ' :

from werkzeug . s e r v i n g import run_simple
run_simple ( ' l o c a l h o s t ' , 9 0 0 0 , app )

This program starts a web service on port 9000 of your computer. This cell will remain running
(appearing locked up). However, it is merely waiting for browsers to connect. If you point your browser
at the following URL, you will interact with the Flask web service.

• http://localhost:9000/

You should see Hello World displayed.

13.1.2 MPG Flask

Usually, you will interact with a web service through JSON. A program will send a JSON message to your
Flask application, and your Flask application will return a JSON. Later, in module 13.3, we will see how to
attach this web service to a web application that you can interact with through a browser. We will create
a Flask wrapper for a neural network that predicts the miles per gallon. The sample JSON will look like
this.

{

}

" c y l i n d e r s " : 8 ,
" d i s p l a c e m e n t " : 3 0 0 ,
" ho r sep ow er " : 7 8 ,
" w e i g h t " : 3 5 0 0 ,
" a c c e l e r a t i o n " : 2 0 ,
" y e a r " : 7 6 ,
" o r i g i n " : 1

We will see two diﬀerent means of POSTing this JSON data to our web server. First, we will use a
utility called POSTman. Secondly, we will use Python code to construct the JSON message and interact
with Flask.

First, it is necessary to train a neural network with the MPG dataset. This technique is very similar
to what we’ve done many times before. However, we will save the neural network so that we can load it
later. We do not want to have Flask train the neural network. We wish to have the neural network already
trained and deploy the already prepared .H5 ﬁle to save the neural network. The following code trains an
MPG neural network.

13.1. PART 13.1: FLASK AND DEEP LEARNING WEB SERVICES

493

Code

from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import pandas a s pd
import i o
import o s
import r e q u e s t s
import numpy a s np
from s k l e a r n import m e t r i c s

d f = pd . read_csv (

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ data / t81 −558/ auto−mpg . c s v " ,
na_values =[ 'NA ' ,

' ? ' ] )

c a r s = d f [ ' name ' ]

# Handle m i s s i n g v a l u e
d f [ ' horsepower ' ] = d f [ ' horsepower ' ] . f i l l n a ( d f [ ' ho rsepower ' ] . median ( ) )

# Pandas t o Numpy
x = d f [ [ ' c y l i n d e r s ' ,

' d i s p l a c e m e n t ' ,

' ho rsepower ' ,

' w e i g h t ' ,

' a c c e l e r a t i o n ' ,

' y e a r ' ,

' o r i g i n ' ] ] . v a l u e s

y = d f [ 'mpg ' ] . v a l u e s # r e g r e s s i o n

i n t o v a l i d a t i o n and t r a i n i n g s e t s

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# B u i l d t h e n e u r a l network
model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 1 0 , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 2
model . add ( Dense ( 1 ) ) # Output
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) ) # Hidden 1

monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3, p a t i e n c e =5, \

v e r b o s e =1, mode= ' auto ' , \
r e s t o r e _ b e s t _ w e i g h t s=True )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) , \

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

494

Output

CHAPTER 13. ADVANCED/OTHER TOPICS

Train on 298 samples , v a l i d a t e on 100 sa mpl es
. . .
298/298 − 0 s − l o s s : 3 9 . 0 5 5 5 − v a l _ l o s s : 3 1 . 4 9 8 1
Epoch 52/1000
R e s t o r i n g model w e i g h t s from t h e end o f
298/298 − 0 s − l o s s : 3 7 . 9 4 7 2 − v a l _ l o s s : 3 2 . 6 1 3 9
Epoch 0 0 0 5 2 : e a r l y s t o p p i n g

t h e b e s t epoch .

Next, we evaluate the score. This evaluation is more of a sanity check to ensure the code above worked

as expected.

Code

pred = model . p r e d i c t ( x _t e s t )
# Measure RMSE e r r o r . RMSE i s common f o r
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y _ te s t ) )
print ( f " A f t e r ␣ l o a d ␣ s c o r e ␣ (RMSE) : ␣{ s c o r e } " )

r e g r e s s i o n .

Output

A f t e r

l o a d s c o r e (RMSE) : 5 . 4 6 5 1 9 3 6 8 8 1 3 0 7 3 2

Next, we save the neural network to a .H5 ﬁle.

Code

model . s a v e ( o s . path . j o i n ( " . / dnn/ " , " mpg_model . h5 " ) )

We want the Flask web service to check that the input JSON is valid. To do this, we need to know
what values we expect and their logical ranges. The following code outputs the expected ﬁelds and their
ranges, and packages all of this information into a JSON object that you should copy to the Flask web
application. This code allows us to validate the incoming JSON requests.

Code

c o l s = [ x fo r x in d f . columns

i f x not in ( 'mpg ' , ' name ' ) ]

print ( " { " )
for i , name in enumerate ( c o l s ) :

print ( f ' " { name } " : { { " min " : { d f [ name ] . min ( ) } , \

␣␣ ␣ ␣␣␣␣␣␣␣ "max " : { d f [ name ] . max ( ) } } } { " , " ␣ i f ␣ i <( l e n ( c o l s ) −1) ␣ e l s e ␣ " " } ' )

13.1. PART 13.1: FLASK AND DEEP LEARNING WEB SERVICES

495

print ( " } " )

Output

{
" c y l i n d e r s " : { " min " : 3 , " max " : 8 } ,
" d i s p l a c e m e n t " : { " min " : 6 8 . 0 , " max " : 4 5 5 . 0 } ,
" horsepower " : { " min " : 4 6 . 0 , " max " : 2 3 0 . 0 } ,
" w e i g h t " : { " min " : 1 6 1 3 , " max " : 5 1 4 0 } ,
" a c c e l e r a t i o n " : { " min " : 8 . 0 , " max " : 2 4 . 8 } ,
" y e a r " : { " min " : 7 0 , " max " : 8 2 } ,
" o r i g i n " : { " min " : 1 , " max " : 3 }
}

Finally, we set up the Python code to call the model for a single car and get a prediction. You should

also copy this code to the Flask web application.

Code

import o s
from t e n s o r f l o w . k e r a s . models import load_model
import numpy a s np

model = load_model ( o s . path . j o i n ( " . / dnn/ " , " mpg_model . h5 " ) )
x = np . z e r o s (

( 1 , 7 )

)

x [ 0 , 0 ] = 8 # ' c y l i n d e r s ' ,
x [ 0 , 1 ] = 400 # ' d i s p l a c e m e n t ' ,
x [ 0 , 2 ] = 80 # ' h o r s e p o w e r ' ,
x [ 0 , 3 ] = 2000 # ' w e i g h t ' ,
x [ 0 , 4 ] = 19 # ' a c c e l e r a t i o n ' ,
x [ 0 , 5 ] = 72 # ' y e a r ' ,
x [ 0 , 6 ] = 1 # ' o r i g i n '

pred = model . p r e d i c t ( x )
f l o a t ( pred [ 0 ] )

Output

6 . 2 1 2 1 0 0 5 0 5 8 2 8 8 5 7

496

CHAPTER 13. ADVANCED/OTHER TOPICS

The completed web application can be found here:

• mpg_server_1.py

You can run this server from the command line with the following command:

python mpg_server_1 . py

If you are using a virtual environment (described in Module 1.1), use the activate tensorflow com-

mand for Windows or source activate tensorflow for Mac before executing the above command.

13.1.3 Flask MPG Client

Now that we have a web service running, we would like to access it. This server is a bit more complicated
than the "Hello World" web server we ﬁrst saw in this part. The request to display was an HTTP GET.
We must now do an HTTP POST. To accomplish access to a web service, you must use a client. We will
see how to use PostMan and directly through a Python program in Jupyter.

We will begin with PostMan. If you have not already done so, install PostMan.
To successfully use PostMan to query your web service, you must enter the following settings:

• POST Request to http://localhost:5000/api/mpg
• RAW JSON and paste in JSON from above
• Click Send and you should get a correct result

Figure 13.1 shows a successful result.

This same process can be done programmatically in Python.

Figure 13.1: PostMan JSON

13.1. PART 13.1: FLASK AND DEEP LEARNING WEB SERVICES

497

Code

import r e q u e s t s

j s o n = {

" c y l i n d e r s " : 8 ,
" d i s p l a c e m e n t " : 3 0 0 ,
" horsepower " : 7 8 ,
" w e i g h t " : 3 5 0 0 ,
" a c c e l e r a t i o n " : 2 0 ,
" y e a r " : 7 6 ,
" o r i g i n " : 1

}

r = r e q u e s t s . p o s t ( " h t t p : / / l o c a l h o s t : 5 0 0 0 / a p i /mpg" , j s o n=j s o n )
i f

r . s t a t u s _ c o d e == 2 0 0 :

print ( " S u c c e s s : ␣ {} " . format ( r . t e x t ) )

e l s e : print ( " F a i l u r e : ␣ {} " . format ( r . t e x t ) )

Output

S u c c e s s : {
" e r r o r s " :
" i d " :
"mpg " : 2 3 . 8 8 5 4 3 8 9 1 9 0 6 7 3 8 3

[ ] ,

" 6 4 3 d027e −554 f −4401−ba5f −78592 a e 7 e 0 7 0 " ,

}

13.1.4 Images and Web Services

We can also accept images from web services. We will create a web service that accepts images and
classiﬁes them using MobileNet. You will follow the same process; load your network as we did for the
MPG example. You can ﬁnd the completed web service can here:

image_server_1.py
You can run this server from the command line with:

python mpg_server_1 . py

If you are using a virtual environment (described in Module 1.1), use the activate tensorflow com-

mand for Windows or source activate tensorflow for Mac before executing the above command.
To successfully use PostMan to query your web service, you must enter the following settings:

• POST Request to http://localhost:5000/api/image

498

CHAPTER 13. ADVANCED/OTHER TOPICS

• Use "Form Data" and create one entry named "image" that is a ﬁle. Choose an image ﬁle to classify.
• Click Send, and you should get a correct result

Figure 13.2 shows a successful result.

Figure 13.2: PostMan Images

This same process can be done programmatically in Python.

Code

import r e q u e s t s
r e s p o n s e = r e q u e s t s . p o s t ( ' h t t p : / / l o c a l h o s t : 5 0 0 0 / a p i / image ' ,

f i l e s =\

dict ( image=( ' h i c k o r y . j p e g ' , open ( ' . / p h o to s / h i c k o r y . j p e g ' , ' rb ' ) ) ) )

i f

r e s p o n s e . s t a t u s _ c o d e == 2 0 0 :

print ( " S u c c e s s : ␣ {} " . format ( r e s p o n s e . t e x t ) )

e l s e : print ( " F a i l u r e : ␣ {} " . format ( r e s p o n s e . t e x t ) )

Output

S u c c e s s : {
" pred " :

[

" name " :
" prob " : 0 . 9 1 7 8 2 8 1 4 2 6 4 2 9 7 4 9

" boxer " ,

{

} ,
{

" name " :

" A m e r i c a n _ S t a f f o r d s h i r e _ t e r r i e r " ,

13.2. PART 13.2: INTERRUPTING AND CONTINUING TRAINING

499

" prob " : 0 . 0 4 4 5 8 1 9 4 9 7 1 0 8 4 5 9 5

" name " :
" prob " : 0 . 0 1 8 7 3 6 2 3 2 0 7 2 1 1 4 9 4 4

" French_bulldog " ,

" name " :
" prob " : 0 . 0 0 0 9 8 6 2 5 1 9 8 0 0 6 6 2 9 9 4

" pug " ,

} ,
{

} ,
{

. . .

}

]

}

13.2 Part 13.2: Interrupting and Continuing Training

We would train our Keras models in one pass in an ideal world, utilizing as much GPU and CPU power
as we need. The world in which we train our models is anything but ideal. In this part, we will see that
we can stop and continue and even adjust training at later times. We accomplish this continuation with
checkpoints. We begin by creating several utility functions. The ﬁrst utility generates an output directory
that has a unique name. This technique allows us to organize multiple runs of our experiment. We provide
the Logger class to route output to a log ﬁle contained in the output directory.

Code

import o s
import r e
import s y s
import time
import numpy a s np
from t y p i n g import Any , L i s t , Tuple , Union
from t e n s o r f l o w . k e r a s . d a t a s e t s import mnist
from t e n s o r f l o w . k e r a s import backend a s K
import t e n s o r f l o w a s
t f
import t e n s o r f l o w . k e r a s
import t e n s o r f l o w a s
t f
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g , \

L e a r n i n g R a t e S c h e d u l e r , ModelCheckpoint
from t e n s o r f l o w . k e r a s import r e g u l a r i z e r s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l

500

CHAPTER 13. ADVANCED/OTHER TOPICS

from t e n s o r f l o w . k e r a s . l a y e r s import Dense , Dropout , F l a t t e n
from t e n s o r f l o w . k e r a s . l a y e r s import Conv2D , MaxPooling2D
from t e n s o r f l o w . k e r a s . models import load_model
import p i c k l e

def g e n e r a t e _ o u t p u t _ d i r ( o u t d i r ,

run_desc ) :

prev_run_dirs = [ ]
i f o s . path . i s d i r ( o u t d i r ) :

prev_run_dirs = [ x for x in o s . l i s t d i r ( o u t d i r )

i f o s . path . i s d i r ( \

o s . path . j o i n ( o u t d i r , x ) ) ]

prev_run_ids = [ r e . match ( r ' ^\d+ ' , x )
prev_run_ids = [ int ( x . group ( ) )
cur_run_id = max( prev_run_ids , d e f a u l t =−1) + 1
run_dir = o s . path . j o i n ( o u t d i r ,
a s s e r t not o s . path . e x i s t s ( run_dir )
o s . makedirs ( run_dir )
return run_dir

for x in prev_run_dirs ]

for x in prev_run_ids

i f x i s not None ]

f ' { cur_run_id : 0 5 d}−{run_desc } ' )

# From StyleGAN2
c l a s s Logger ( object ) :

" " " R e d i r e c t
o p t i o n a l l y f o r c e f l u s h i n g on b o t h s t d o u t and t h e

t o s t d o u t , o p t i o n a l l y p r i n t

s t d e r r

f i l e . " " "

s t d o u t

t o a f i l e , and

def __init__ ( s e l f ,

f i l e _ n a m e : s t r = None ,

f i l e _ m o d e : s t r = "w" , \

s h o u l d _ f l u s h : bool = True ) :

s e l f . f i l e = None

i f

f i l e _ n a m e i s not None :

s e l f . f i l e = open ( file_name ,

f i l e _ m o d e )

s e l f . s h o u l d _ f l u s h = s h o u l d _ f l u s h
s e l f . s t d o u t = s y s . s t d o u t
s e l f . s t d e r r = s y s . s t d e r r

s y s . s t d o u t = s e l f
s y s . s t d e r r = s e l f

def __enter__ ( s e l f ) −> " Logger " :

return s e l f

def __exit__ ( s e l f , exc_type : Any , exc_value : Any , \
t r a c e b a c k : Any) −> None :

s e l f . c l o s e ( )

13.2. PART 13.2: INTERRUPTING AND CONTINUING TRAINING

501

def w r i t e ( s e l f ,

t e x t : s t r ) −> None :

" " " Write t e x t
i f

len ( t e x t ) == 0 :

t o s t d o u t

( and a f i l e ) and o p t i o n a l l y f l u s h . " " "

return

i f

s e l f . f i l e i s not None :
s e l f . f i l e . w r i t e ( t e x t )

s e l f . s t d o u t . w r i t e ( t e x t )

i f

s e l f . s h o u l d _ f l u s h :

s e l f . f l u s h ( )

def

f l u s h ( s e l f ) −> None :
" " " F l u s h w r i t t e n t e x t
i f

s e l f . f i l e i s not None :

t o b o t h s t d o u t and a f i l e ,

i f open . " " "

s e l f . f i l e . f l u s h ( )

s e l f . s t d o u t . f l u s h ( )

def c l o s e ( s e l f ) −> None :

" " " Flush , c l o s e p o s s i b l e

f i l e s , and remove

s t d o u t / s t d e r r m i r r o r i n g . " " "

s e l f . f l u s h ( )

# i f u s i n g m u l t i p l e l o g g e r s , p r e v e n t c l o s i n g i n wrong o r d e r
i f

s y s . s t d o u t

s e l f :

i s

s y s . s t d o u t = s e l f . s t d o u t

i f

s y s . s t d e r r

i s

s e l f :

s y s . s t d e r r = s e l f . s t d e r r

i f

s e l f . f i l e i s not None :

s e l f . f i l e . c l o s e ( )

def obtain_data ( ) :

( x_test , y _ t e s t ) = mnist . load_data ( )

( x_train , y _ t r a i n ) ,
print ( " Shape ␣ o f ␣ x _ t r a i n : ␣ {} " . format ( x _ t r a i n . shape ) )
print ( " Shape ␣ o f ␣ y _ t r a i n : ␣ {} " . format ( y _ t r a i n . shape ) )
print ( )
print ( " Shape ␣ o f ␣ x _t e s t : ␣ {} " . format ( x _ t es t . shape ) )
print ( " Shape ␣ o f ␣ y _ t e s t : ␣ {} " . format ( y _ t es t . shape ) )

502

CHAPTER 13. ADVANCED/OTHER TOPICS

image d i m e n s i o n s

# i n p u t
img_rows ,
i f K. image_data_format ( ) == ' c h a n n e l s _ f i r s t ' :

img_cols = 2 8 , 28

x _ t r a i n = x _ t r a i n . r e s h a p e ( x _ t r a i n . shape [ 0 ] , 1 ,
x _ te s t = x_ t e s t . r e s h a p e ( x _ t es t . shape [ 0 ] , 1 ,
input_shape = ( 1 ,

img_cols )

img_rows ,

img_rows ,

img_cols )

img_rows ,

img_cols )

e l s e :

x _ t r a i n = x _ t r a i n . r e s h a p e ( x _ t r a i n . shape [ 0 ] ,
x _ te s t = x_ t e s t . r e s h a p e ( x _ t es t . shape [ 0 ] ,
input_shape = ( img_rows ,

img_cols , 1 )

img_rows ,

img_cols , 1 )

img_rows ,

img_cols , 1 )

x _ t r a i n = x _ t r a i n . a s t y p e ( ' f l o a t 3 2 ' )
x _ te s t = x_ t e s t . a s t y p e ( ' f l o a t 3 2 ' )
x _ t r a i n /= 255
x _ te s t /= 255
print ( ' x _ t r a i n ␣ shape : ' , x _ t r a i n . shape )
print ( " T r a i n i n g ␣ sa mples : ␣ {} " . format ( x _ t r a i n . shape [ 0 ] ) )
print ( " Test ␣ samp les : ␣ {} " . format ( x _ t es t . shape [ 0 ] ) )
# c o n v e r t c l a s s v e c t o r s
y _ t r a i n = t f . k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y_train , num_classes )
y _ te s t = t f . k e r a s . u t i l s . t o _ c a t e g o r i c a l ( y_test , num_classes )

t o b i n a r y c l a s s m a t r i c e s

return input_shape , x_train , y_train , x_test , y _t e s t

We deﬁne the basic training parameters and where we wish to write the output.

Code

o u t d i r = " . / data / "
run_desc = " t e s t −t r a i n "
b a t c h _ s i z e = 128
num_classes = 10

run_dir = g e n e r a t e _ o u t p u t _ d i r ( o u t d i r ,
print ( f " R e s u l t s ␣ saved ␣ t o : ␣{ run_dir } " )

run_desc )

Output

R e s u l t s

saved t o :

. / data /00000− t e s t −t r a i n

Keras provides a prebuilt checkpoint class named ModelCheckpoint that contains most of our desired
functionality. This built-in class can save the model’s state repeatedly as training progresses. Stopping
neural network training is not always a controlled event. Sometimes this stoppage can be abrupt, such
as a power failure or a network resource shutting down. If Microsoft Windows is your operating system

13.2. PART 13.2: INTERRUPTING AND CONTINUING TRAINING

503

of choice, your training can also be interrupted by a high-priority system update. Because of all of this
uncertainty, it is best to save your model at regular intervals. This process is similar to saving a game at
critical checkpoints, so you do not have to start over if something terrible happens to your avatar in the
game.

We will create our checkpoint class, named MyModelCheckpoint. In addition to saving the model,
we also save the state of the training infrastructure. Why save the training infrastructure, in addition to
the weights? This technique eases the transition back into training for the neural network and will be more
eﬃcient than a cold start.

Consider if you interrupted your college studies after the ﬁrst year. Sure, your brain (the neural network)
will retain all the knowledge. But how much rework will you have to do? Your transcript at the university
is like the training parameters. It ensures you do not have to start over when you come back.

Code

c l a s s MyModelCheckpoint ( ModelCheckpoint ) :
def __init__ ( s e l f , ∗ a r g s , ∗∗ kwargs ) :
super ( ) . __init__ ( ∗ a r g s , ∗∗ kwargs )

def on_epoch_end ( s e l f , epoch ,

l o g s=None ) :

super ( ) . on_epoch_end ( epoch , l o g s ) \

# Also s a v e t h e o p t i m i z e r
f i l e p a t h = s e l f . _ g et _ fi l e _p a th ( epoch=epoch ,

s t a t e

l o g s=l o g s , batch=None )

f i l e p a t h = f i l e p a t h . r s p l i t (
f i l e p a t h += " . p k l "

" . " , 1 ) [ 0 ]

with open ( f i l e p a t h ,

'wb ' ) a s

f p :

p i c k l e . dump(

{

' opt ' : model . o p t i m i z e r . g e t _ c o n f i g ( ) ,
' epoch ' : epoch+1

# Add a d d i t i o n a l k e y s
} ,

fp , p r o t o c o l=p i c k l e .HIGHEST_PROTOCOL)

i f you need t o s t o r e more v a l u e s

print ( ' \nEpoch␣%05d : ␣ s a v i n g ␣ o p t i m i z a e r ␣ t o ␣%s ' % ( epoch + 1 ,

f i l e p a t h ) )

The optimizer applies a step decay schedule during training to decrease the learning rate as training
progresses. It is essential to preserve the current epoch that we are on to perform correctly after a training
resume.
Code

def step_decay_schedule ( i n i t i a l _ l r =1e −3, d e c a y _ f a c t o r =0.75 ,

s t e p _ s i z e =10):

def

s c h e d u l e ( epoch ) :
return i n i t i a l _ l r ∗ ( d e c a y _ f a c t o r ∗∗ np . f l o o r ( epoch / s t e p _ s i z e ) )

504

CHAPTER 13. ADVANCED/OTHER TOPICS

return L e a r n i n g R a t e S c h e d u l e r ( s c h e d u l e )

We build the model just as we have in previous sessions. However, the training function requires a few
extra considerations. We specify the maximum number of epochs; however, we also allow the user to select
the starting epoch number for training continuation.

Code

def build_model ( input_shape , num_classes ) :

model = S e q u e n t i a l ( )
model . add ( Conv2D ( 3 2 , k e r n e l _ s i z e =(3 , 3 ) ,

a c t i v a t i o n= ' r e l u ' ,
input_shape=input_shape ) )

( 3 , 3 ) , a c t i v a t i o n= ' r e l u ' ) )

model . add ( Conv2D ( 6 4 ,
model . add ( MaxPooling2D ( p o o l _ s i z e =(2 , 2 ) ) )
model . add ( Dropout ( 0 . 2 5 ) )
model . add ( F l a t t e n ( ) )
model . add ( Dense ( 1 2 8 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dropout ( 0 . 5 ) )
model . add ( Dense ( num_classes , a c t i v a t i o n= ' softmax ' ) )
model . compile (

l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' ,
o p t i m i z e r=t f . k e r a s . o p t i m i z e r s . Adam ( ) ,
m e t r i c s =[ ' a c c u r a c y ' ] )

return model

def train_model ( model ,

i n i t i a l _ e p o c h =0, max_epochs =10):

s t a r t _ t i m e = time . time ( )

checkpoint_cb = MyModelCheckpoint (

o s . path . j o i n ( run_dir ,
monitor= ' v a l _ l o s s ' , v e r b o s e =1)

' model−{epoch : 0 2 d}−{ v a l _ l o s s : . 2 f } . hdf5 ' ) ,

lr_sched_cb = step_decay_schedule ( i n i t i a l _ l r =1e −4, d e c a y _ f a c t o r =0.75 , \

s t e p _ s i z e =2)

cb = [ checkpoint_cb ,

lr_sched_cb ]

model . f i t ( x_train , y_train ,

b a t c h _ s i z e=b a t c h _ s i z e ,
e p oc h s=max_epochs ,
i n i t i a l _ e p o c h = i n i t i a l _ e p o c h ,
v e r b o s e =2, c a l l b a c k s=cb ,
v a l i d a t i o n _ d a t a =(x_test , y_ t e st ) )

13.2. PART 13.2: INTERRUPTING AND CONTINUING TRAINING

505

s c o r e = model . e v a l u a t e ( x_test , y_test , v e r b o s e =0, c a l l b a c k s=cb )
print ( ' Test ␣ l o s s : ␣ {} ' . format ( s c o r e [ 0 ] ) )
print ( ' Test ␣ a c c u r a c y : ␣ {} ' . format ( s c o r e [ 1 ] ) )

e l a p s e d _ t i m e = time . time ( ) − s t a r t _ t i m e
print ( " Elapsed ␣ time : ␣ {} " . format ( hms_string ( e l a p s e d _ t i m e ) ) )

We now begin training, using the Logger class to write the output to a log ﬁle in the output directory.

Code

with Logger ( o s . path . j o i n ( run_dir ,

' l o g . t x t ' ) ) :

input_shape , x_train , y_train , x_test , y _ te s t = obtain_data ( )
model = build_model ( input_shape , num_classes )
train_model ( model , max_epochs=3)

Output

( 6 0 0 0 0 , 2 8 , 2 8 )
( 6 0 0 0 0 , )

Downloading data from h t t p s : / / s t o r a g e . g o o g l e a p i s . com/ t e n s o r f l o w / t f −
k e r a s −d a t a s e t s / mnist . npz
11493376/11490434 [==============================] − 0 s 0 us / s t e p
11501568/11490434 [==============================] − 0 s 0 us / s t e p
Shape o f x _ t r a i n :
Shape o f y _ t r a i n :
Shape o f x _ t es t :
Shape o f y _ t es t :
x _ t r a i n shape :
T r a i n i n g s amples : 60000
Test
s amples : 10000
. . .
469/469 − 2 s − l o s s : 0 . 2 2 8 4 − a c c u r a c y : 0 . 9 3 3 2 − v a l _ l o s s : 0 . 1 0 8 7 −
v a l _ a c c u r a c y : 0 . 9 6 7 7 − l r : 1 . 0 0 0 0 e −04 − 2 s / epoch − 5ms/ s t e p
Epoch 3/3

( 1 0 0 0 0 , 2 8 , 2 8 )
( 1 0 0 0 0 , )

( 6 0 0 0 0 , 2 8 , 2 8 , 1 )

. . .

469/469 − 2 s − l o s s : 0 . 1 5 7 5 − a c c u r a c y : 0 . 9 5 4 1 − v a l _ l o s s : 0 . 0 8 3 7 −
v a l _ a c c u r a c y : 0 . 9 7 4 6 − l r : 7 . 5 0 0 0 e −05 − 2 s / epoch − 5ms/ s t e p
Test
Test a c c u r a c y : 0 . 9 7 4 6 0 0 0 1 7 0 7 0 7 7 0 3
Elapsed time : 0 : 0 0 : 2 2 . 0 9

l o s s : 0 . 0 8 3 6 5 7 0 1 1 3 8 9 7 3 2 3 6

You should notice that the above output displays the name of the hdf5 and pickle (pkl) ﬁles produced

506

CHAPTER 13. ADVANCED/OTHER TOPICS

at each checkpoint. These ﬁles serve the following functions:

• Pickle ﬁles contain the state of the optimizer.
• HDF5 ﬁles contain the saved model.

For this training run, which went for 3 epochs, these two ﬁles were named:

• ./data/00013-test-train/model-03-0.08.hdf5
• ./data/00013-test-train/model-03-0.08.pkl

We can inspect the output from the training run. Notice we can see a folder named "00000-test-train".
This new folder was the ﬁrst training run. The program will call the next training run "00001-test-train",
and so on. Inside this directory, you will ﬁnd the pickle and hdf5 ﬁles for each checkpoint.

Code

! l s

. / data

Output

00000− t e s t −t r a i n

Code

! l s

. / data /00000− t e s t −t r a i n

Output

l o g . t x t
model −03 −0.08. p k l
model −01 −0.20. hdf5 model −02 −0.11. hdf5 model −03 −0.08. hdf5

model −02 −0.11. p k l

model −01 −0.20. p k l

Keras stores the model itself in an HDF5, which includes the optimizer. Because of this feature, it is
not generally necessary to restore the internal state of the optimizer (such as ADAM). However, we include
the code to do so. We can obtain the internal state of an optimizer by calling get_conﬁg, which will
return a dictionary similar to the following:

{ ' name ' :
' beta_1 ' : 0 . 9 ,

'Adam' ,

' l e a r n i n g _ r a t e ' : 7 . 5 e −05 ,

' decay ' : 0 . 0 ,

' beta_2 ' : 0 . 9 9 9 ,

' e p s i l o n ' : 1 e −07 ,

' amsgrad ' : F a l s e }

In practice, I’ve found that diﬀerent optimizers implement get_conﬁg diﬀerently. This function will
always return the training hyperparameters. However, it may not always capture the complete internal
state of an optimizer beyond the hyperparameters. The exact implementation of get_conﬁg can vary per
optimizer implementation.

13.2. PART 13.2: INTERRUPTING AND CONTINUING TRAINING

507

13.2.1 Continuing Training

We are now ready to continue training. You will need the paths to both your HDF5 and PKL ﬁles. You
can ﬁnd these paths in the output above. Your values may diﬀer from mine, so perform a copy/paste.

Code

MODEL_PATH = ' . / data /00000− t e s t −t r a i n / model −03 −0.08. hdf5 '
OPT_PATH = ' . / data /00000− t e s t −t r a i n / model −03 −0.08. p k l '

The following code loads the HDF5 and PKL ﬁles and then recompiles the model based on the PKL

ﬁle. Depending on the optimizer in use, you might have to recompile the model.

Code

import t e n s o r f l o w a s
from t e n s o r f l o w . k e r a s . models import load_model
import p i c k l e

t f

def

load_model_data ( model_path , opt_path ) :
model = load_model ( model_path )
with open ( opt_path ,

' rb ' ) a s

f p :

d = p i c k l e . l o a d ( f p )
epoch = d [ ' epoch ' ]
opt = d [ ' opt ' ]
return epoch , model , opt

epoch , model , opt = load_model_data (MODEL_PATH, OPT_PATH)

# n o t e : o f t e n i t
model . compile (

i s not n e c e s s a r y t o r e c o m p i l e t h e model

l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' ,
o p t i m i z e r=t f . k e r a s . o p t i m i z e r s . Adam . f r o m _ c o n f i g ( opt ) ,
m e t r i c s =[ ' a c c u r a c y ' ] )

Finally, we train the model for additional epochs. You can see from the output that the new training
starts at a higher accuracy than the ﬁrst training run. Further, the accuracy increases with additional
training. Also, you will notice that the epoch number begins at four and not one.

Code

o u t d i r = " . / data / "
run_desc = " cont−t r a i n "
num_classes = 10

508

CHAPTER 13. ADVANCED/OTHER TOPICS

run_dir = g e n e r a t e _ o u t p u t _ d i r ( o u t d i r ,
print ( f " R e s u l t s ␣ saved ␣ t o : ␣{ run_dir } " )

run_desc )

with Logger ( o s . path . j o i n ( run_dir ,

' l o g . t x t ' ) ) :

input_shape , x_train , y_train , x_test , y _ te s t = obtain_data ( )
train_model ( model ,

i n i t i a l _ e p o c h=epoch , max_epochs=6)

Output

( 6 0 0 0 0 , 2 8 , 2 8 , 1 )

( 1 0 0 0 0 , 2 8 , 2 8 )
( 1 0 0 0 0 , )

. / data /00001− cont−t r a i n
( 6 0 0 0 0 , 2 8 , 2 8 )
( 6 0 0 0 0 , )

saved t o :
R e s u l t s
Shape o f x _ t r a i n :
Shape o f y _ t r a i n :
Shape o f x _ t es t :
Shape o f y _ t es t :
x _ t r a i n shape :
T r a i n i n g sam ples : 60000
Test
samples : 10000
. . .
469/469 − 2 s − l o s s : 0 . 1 0 9 9 − a c c u r a c y : 0 . 9 6 7 7 − v a l _ l o s s : 0 . 0 6 1 2 −
v a l _ a c c u r a c y : 0 . 9 8 1 8 − l r : 5 . 6 2 5 0 e −05 − 2 s / epoch − 5ms/ s t e p
Epoch 6/6
Epoch 6 :
Epoch 0 0 0 0 6 :
t r a i n / model −06 −0.06. p k l
469/469 − 2 s − l o s s : 0 . 0 9 9 0 − a c c u r a c y : 0 . 9 7 1 1 − v a l _ l o s s : 0 . 0 5 6 1 −
v a l _ a c c u r a c y : 0 . 9 8 2 7 − l r : 5 . 6 2 5 0 e −05 − 2 s / epoch − 5ms/ s t e p
Test
Test a c c u r a c y : 0 . 9 8 2 6 9 9 9 9 0 2 7 2 5 2 2
Elapsed time : 0 : 0 0 : 1 1 . 7 2

s a v i n g model t o . / data /00001− cont−t r a i n / model −06 −0.06. hdf5

l o s s : 0 . 0 5 6 1 0 6 4 7 0 5 2 5 2 6 4 7 4

t o . / data /00001− cont−

s a v i n g o p t i m i z a e r

13.3 Part 13.3: Using a Keras Deep Neural Network with a Web

Application

In this part, we will extend the image API developed in Part 13.1 to work with a web application. This
technique allows you to use a simple website to upload/predict images, such as in Figure 13.3.

I added neural network functionality to a simple ReactJS image upload and preview example. To do
this, we will use the same API developed in Module 13.1. However, we will now add a ReactJS website
around it. This single-page web application allows you to upload images for classiﬁcation by the neural
network. If you would like to read more about ReactJS and image uploading, you can refer to the blog
post that provided some inspiration for this example.

13.4. PART 13.4: WHEN TO RETRAIN YOUR NEURAL NETWORK

509

Figure 13.3: AI Web Application

I built this example from the following components:

• GitHub Location for Web App
• image_web_server_1.py - The code both to start Flask and serve the HTML/JavaScript/CSS needed

to provide the web interface.

• Directory WWW - Contains web assets.

– index.html - The main page for the web application.
– style.css - The stylesheet for the web application.
– script.js - The JavaScript code for the web application.

13.4 Part 13.4: When to Retrain Your Neural Network

Dataset drift is a problem frequently seen in real-world applications of machine learning. Academic prob-
lems that courses typically present in school assignments usually do not experience this problem. For a
class assignment, your instructor provides a single data set representing all of the data you will ever see
for a task. In the real world, you obtain initial data to train your model; then, you will acquire new data
over time that you use your model to predict.

Consider this example. You create a startup company that develops a mobile application that helps
people ﬁnd jobs. To train your machine learning model, you collect attributes about people and their
careers. Once you have your data, you can prepare your neural network to suggest the best jobs for
individuals.

510

CHAPTER 13. ADVANCED/OTHER TOPICS

Once your application is released, you will hopefully obtain new data. This data will come from job
seekers using your app. These people are your customers. You have x values (their attributes), but you
do not have y-values (their jobs). Your customers have come to you to ﬁnd out what their be jobs will
be. You will provide the customer’s attributes to the neural network, and then it will predict their jobs.
Usually, companies develop neural networks on initial data and then use the neural network to perform
predictions on new data obtained over time from their customers.

Your job prediction model will become less relevant as the industry introduces new job types and the
demographics of your customers change. However, companies must look if their model is still relevant as
time passes. This change in your underlying data is called dataset drift. In this section, we will see ways
that you can measure dataset drift.

You can present your model with new data and see how its accuracy changes over time. However, to
calculate eﬃciency, you must know the expected outputs from the model (y-values). You may not know the
correct outcomes for new data that you are obtaining in real-time. Therefore, we will look at algorithms
that examine the x-inputs and determine how much they have changed in distribution from the original
x-inputs that we trained on. These changes are called dataset drift.

Let’s begin by creating generated data that illustrates drift. We present the following code to create a

chart that shows such drift.

Code

import numpy a s np

import m a t p l o t l i b . p y p l o t a s p l o t
from s k l e a r n . l i n e a r _ m o d e l

import L i n e a r R e g r e s s i o n

def

t r u e _ f u n c t i o n ( x ) :
x2 = ( x ∗ 8 ) − 1
return ( ( np . s i n ( x2 ) / x2 ) ∗ 0 . 6 ) + 0 . 3

#
x _ t r a i n = np . a r a n g e ( 0 , 0 . 6 , 0 . 0 1 )
x _ te s t = np . a r a n g e ( 0 . 6 , 1 . 1 , 0 . 0 1 )
x_true = np . c o n c a t e n a t e (

( x_train , x _ te s t )

)

#
y_true_train = t r u e _ f u n c t i o n ( x _ t r a i n )
y_true_test = t r u e _ f u n c t i o n ( x _t e s t )
= np . c o n c a t e n a t e (
y_true

( y_true_train , y_true_test )

)

#
y _ t r a i n = y_true_train + ( np . random . rand ( ∗ x _ t r a i n . shape ) − 0 . 5 ) ∗ 0 . 4
y _ te s t = y_true_test + ( np . random . rand ( ∗ x _ te s t . shape ) − 0 . 5 ) ∗ 0 . 4

#

13.4. PART 13.4: WHEN TO RETRAIN YOUR NEURAL NETWORK

511

l r _ x _ t r a i n = x _ t r a i n . r e s h a p e ( ( x _ t r a i n . shape [ 0 ] , 1 ) )
r e g = L i n e a r R e g r e s s i o n ( ) . f i t ( lr_x_train , y _ t r a i n )
reg_pred = r e g . p r e d i c t ( l r _ x _ t r a i n )
print ( r e g . coef_ [ 0 ] )
print ( r e g . i n t e r c e p t _ )

#
p l o t . xlim ( [ 0 , 1 . 5 ] )
p l o t . ylim ( [ 0 , 1 ] )
l 1 = p l o t . s c a t t e r ( x_train , y_train , c=" g " ,
l 2 = p l o t . s c a t t e r ( x_test , y_test , c=" r " ,
l 3 , = p l o t . p l o t ( lr_x_trai n ,

reg_pred ,

l a b e l=" T r a i n i n g ␣Data " )

l a b e l=" T e s t i n g ␣Data " )

c o l o r= ' b l a c k ' ,

l i n e w i d t h =3,

l a b e l=" Trained ␣ Model " )

l 4 , = p l o t . p l o t ( x_true , y_true ,
p l o t . l e g e n d ( h a n d l e s =[ l 1 ,

l 2 ,

l 3 ,

l a b e l = " True ␣ Function " )

l 4 ] )

#
p l o t . t i t l e ( ' D r i f t ' )
p l o t . x l a b e l ( ' Time ' )
p l o t . y l a b e l ( ' S a l e s ' )
p l o t . g r i d ( True , which= ' both ' )
p l o t . show ( )

Output

−1.1979470956001936
0 . 9 8 8 8 3 4 0 1 5 3 2 1 1 4 4 5

The "True function" represents what the data does over time. Unfortunately, you only have the training

512

CHAPTER 13. ADVANCED/OTHER TOPICS

portion of the data. Your model will do quite well on the data that you trained it with; however, it will
be very inaccurate on the new test data presented. The prediction line for the model ﬁts the training data
well but does not ﬁt the est data well.

13.4.1 Preprocessing the Sberbank Russian Housing Market Data

The examples provided in this section use a Kaggle dataset named The Sberbank Russian Housing Market,
which you can access from the following link.

• Sberbank Russian Housing Market

Because Kaggle provides datasets as training and test, we must load both of these ﬁles.

Code

import o s
import numpy a s np
import pandas a s pd
from s k l e a r n . p r e p r o c e s s i n g import LabelEncoder

PATH = " / U s e r s / j h e a t o n / Downloads / sberbank−r u s s i a n −housing −market "

t r a i n _ d f = pd . read_csv ( o s . path . j o i n (PATH, " t r a i n . c s v " ) )
t e s t _ d f = pd . read_csv ( o s . path . j o i n (PATH, " t e s t . c s v " ) )

I provide a simple preprocess function that converts all numerics to z-scores and all categoricals to

dummies.

Code

def p r e p r o c e s s ( d f ) :

for i

in d f . columns :

i f d f [ i ] . dtype == ' o b j e c t ' :

d f [ i ] = d f [ i ] . f i l l n a ( d f [ i ] . mode ( ) . i l o c [ 0 ] )

e l i f

( d f [ i ] . dtype == ' i n t ' or d f [ i ] . dtype == ' f l o a t ' ) :

d f [ i ] = d f [ i ] . f i l l n a ( np . nanmedian ( d f [ i ] ) )

enc = LabelEncoder ( )
in d f . columns :
fo r i

i f

( d f [ i ] . dtype == ' o b j e c t ' ) :

d f [ i ] = enc . f i t _ t r a n s f o r m ( d f [ i ] . a s t y p e ( ' s t r ' ) )
d f [ i ] = d f [ i ] . a s t y p e ( ' o b j e c t ' )

Next, we run the training and test datasets through the preprocessing function.

13.4. PART 13.4: WHEN TO RETRAIN YOUR NEURAL NETWORK

513

Code

p r e p r o c e s s ( t r a i n _ d f )
p r e p r o c e s s ( t e s t _ d f )

Finally, we remove thr target variable. We are only looking for drift on the x (input data).

Code

t r a i n _ d f . drop ( ' p r i c e _ d o c ' , a x i s =1, i n p l a c e=True )

13.4.2 KS-Statistic

We will use the KS-Statistic to determine the diﬀerence in distribution between columns in the training
and test sets. As a baseline, consider if we compare the same ﬁeld to itself. In this case, we are comparing
the kitch_sq in the training set. Because there is no diﬀerence in distribution between a ﬁeld in itself, the
p-value is 1.0, and the KS-Statistic statistic is 0. The P-Value is the probability of no diﬀerence between
the two distributions. Typically some lower threshold is used for how low a P-Value is needed to reject the
null hypothesis and assume there is a diﬀerence. The value of 0.05 is a standard threshold for p-values.
Because the p-value is NOT below 0.05, we expect the two distributions to be the same. If the p-value
were below the threshold, the statistic value becomes interesting. This value tells you how diﬀerent the
two distributions are. A value of 0.0, in this case, means no diﬀerences.

Code

from s c i p y import s t a t s

s t a t s . ks_2samp ( t r a i n _ d f [ ' k i t c h _ s q ' ] ,

t r a i n _ d f [ ' k i t c h _ s q ' ] )

Output

Ks_2sampResult ( s t a t i s t i c = −0.0 , p v a l u e =1.0)

Now let’s do something more interesting. We will compare the same ﬁeld kitch_sq between the test
and training sets. In this case, the p-value is below 0.05, so the statistic value now contains the amount
of diﬀerence detected.

Code

s t a t s . ks_2samp ( t r a i n _ d f [ ' k i t c h _ s q ' ] ,

t e s t _ d f [ ' k i t c h _ s q ' ] )

Output

514

CHAPTER 13. ADVANCED/OTHER TOPICS

Ks_2sampResult ( s t a t i s t i c =0.25829078867676714 , p v a l u e =0.0)

Next, we pull the KS-Stat for every ﬁeld. We also establish a boundary for the maximum p-value and

how much of a diﬀerence is needed before we display the column.

Code

for c o l

in t r a i n _ d f . columns :

ks = s t a t s . ks_2samp ( t r a i n _ d f [ c o l ] ,
i f ks . p v a l u e < 0 . 0 5 and ks . s t a t i s t i c > 0 . 1 :

t e s t _ d f [ c o l ] )

print ( f ' { c o l } : ␣{ ks } ' )

Output

i d : Ks_2sampResult ( s t a t i s t i c =1.0 , p v a l u e =0.0)
timestamp : Ks_2sampResult ( s t a t i s t i c =0.8982081426022823 , p v a l u e =0.0)
l i f e _ s q : Ks_2sampResult ( s t a t i s t i c =0.2255084471628891 ,
p v a l u e =7.29401465948424 e −271)
max_floor : Ks_2sampResult ( s t a t i s t i c =0.17313454154786817 ,
p v a l u e =7.82000315371674 e −160)
b u i l d _ y e a r : Ks_2sampResult ( s t a t i s t i c =0.3176883950430345 , p v a l u e =0.0)
num_room : Ks_2sampResult ( s t a t i s t i c =0.1226755470309048 ,
p v a l u e =1.8622542043144584 e −80)
k i t c h _ s q : Ks_2sampResult ( s t a t i s t i c =0.25829078867676714 , p v a l u e =0.0)
s t a t e : Ks_2sampResult ( s t a t i s t i c =0.13641341252952505 ,
p v a l u e =2.1968159319271184 e −99)
p r e s c h o o l _ q u o t a : Ks_2sampResult ( s t a t i s t i c =0.2364160801236304 ,
p v a l u e =1.1710777340471466 e −297)
sc h o o l_ qu o t a : Ks_2sampResult ( s t a t i s t i c =0.25657342859882415 ,

. . .

cafe_sum_2000_max_price_avg :
Ks_2sampResult ( s t a t i s t i c =0.10732529051140638 ,
p v a l u e =1.1100804327460878 e −61)
cafe_avg_price_2000 : Ks_2sampResult ( s t a t i s t i c =0.1081218037860151 ,
p v a l u e =1.3575759911857293 e −62)

13.4.3 Detecting Drift between Training and Testing Datasets by Training

Sample the training and test into smaller sets to train. We want 10K elements from each; however, the
test set only has 7,662, so we only sample that amount from each side.

13.4. PART 13.4: WHEN TO RETRAIN YOUR NEURAL NETWORK

515

Code

SAMPLE_SIZE = min( len ( t r a i n _ d f ) , len ( t e s t _ d f ) )
SAMPLE_SIZE = min(SAMPLE_SIZE, 1 0 0 0 0 )
print (SAMPLE_SIZE)

Output

7662

We take the random samples from the training and test sets and add a ﬂag called source_training

to tell the two apart.

Code

t r a i n i n g _ s a m p l e = t r a i n _ d f . sample (SAMPLE_SIZE,
t e s t i n g _ s a m p l e = t e s t _ d f . sample (SAMPLE_SIZE,

random_state =49)

random_state =48)

t h e d a t a from t h e t r a i n i n g s e t ?

# I s
t r a i n i n g _ s a m p l e [ ' s o u r c e _ t r a i n i n g ' ] = 1
t e s t i n g _ s a m p l e [ ' s o u r c e _ t r a i n i n g ' ] = 0

Next, we combine the data that we sampled from the training and test data sets and shuﬄe them.

Code

# B u i l d combined t r a i n i n g s e t
combined = t e s t i n g _ s a m p l e . append ( t r a i n i n g _ s a m p l e )
combined . r e s e t _ i n d e x ( i n p l a c e=True , drop=True )

# Now randomize
combined = combined . r e i n d e x ( np . random . p er m ut a ti o n ( combined . i n d e x ) )
combined . r e s e t _ i n d e x ( i n p l a c e=True , drop=True )

We will now generate x and y to train. We attempt to predict the source_training value as y, which
indicates if the data came from the training or test set. If the model successfully uses the data to predict
if it came from training or testing, then there is likely drift. Ideally, the train and test data should be
indistinguishable.

Code

# Get r e a d y t o t r a i n
y = combined [ ' s o u r c e _ t r a i n i n g ' ] . v a l u e s
combined . drop ( ' s o u r c e _ t r a i n i n g ' , a x i s =1, i n p l a c e=True )

516

CHAPTER 13. ADVANCED/OTHER TOPICS

x = combined . v a l u e s

y

Output

a r r a y ( [ 1 , 1 , 1 ,

. . . , 1 , 0 , 0 ] )

We will consider anything above a 0.75 AUC as having a good chance of drift.

Code

from s k l e a r n . ensemble import R a n d o m F o r e s t C l a s s i f i e r
from s k l e a r n . m o d e l _ s e l e c t i o n import c r o s s _ v a l _ s c o r e

model = R a n d o m F o r e s t C l a s s i f i e r ( n _ e s t i m a t o r s = 6 0 , max_depth = 7 ,

min_samples_leaf = 5 )

l s t = [ ]

for i

in combined . columns :

s c o r e = c r o s s _ v a l _ s c o r e ( model , pd . DataFrame ( combined [ i ] ) , y , cv =2,

s c o r i n g= ' roc_auc ' )

i f

( np . mean ( s c o r e ) > 0 . 7 5 ) :

l s t . append ( i )
print ( i , np . mean ( s c o r e ) )

Output

i d 1 . 0
timestamp 0 . 9 6 0 1 8 6 2 1 1 1 9 7 5 6 8 8
f u l l _ s q 0 . 7 9 6 6 7 8 5 6 1 1 4 2 4 9 1 1
l i f e _ s q 0 . 8 7 2 4 2 1 8 3 3 0 1 6 6 0 3 8
b u i l d _ y e a r 0 . 8 0 0 4 8 2 5 1 7 6 6 8 8 1 9 1
k i t c h _ s q 0 . 9 0 7 0 0 9 3 8 0 4 6 7 2 6 3 4
cafe_sum_500_min_price_avg 0 . 8 4 3 5 9 2 0 0 3 6 0 3 5 6 8 9
cafe_avg_price_500 0 . 8 4 5 3 5 3 3 8 3 5 3 4 4 6 7 1

13.5. PART 13.5: TENSOR PROCESSING UNITS (TPUS)

517

13.5 Part 13.5: Tensor Processing Units (TPUs)

This book focuses primarily on NVIDIA Graphics Processing Units (GPUs) for deep learning acceleration.
NVIDIA GPUs are not the only option for deep learning acceleration. TensorFlow continues to gain
additional support for AMD and Intel GPUs. TPUs are also available from Google cloud platforms to
accelerate deep learning. The focus of this book and course is on NVIDIA GPUs because of their wide
availability on both local and cloud systems.

Though this book focuses on NVIDIA GPUs, we will brieﬂy examine Google Tensor Processing Units
(TPUs). These devices are an AI accelerator Application-Speciﬁc Integrated Circuit (ASIC) developed
by Google. They were designed speciﬁcally for neural network machine learning, mainly using Google’s
TensorFlow software. Google began using TPUs internally in 2015 and in 2018 made them available for
third-party use, both as part of its cloud infrastructure and by oﬀering a smaller version of the chip for
sale.

The full use of a TPU is a complex topic that I only introduced in this part. Supporting TPUs is
slightly more complicated than GPUs because specialized coding is needed. Changes are rarely required
to adapt CPU code to GPU for most relatively simple mainstream GPU tasks in TensorFlow. I will cover
the mild code changes needed to utilize in this part.

We will create a regression neural network to count paper clips in this part. I demonstrated this dataset
and task several times previously in this book. This part focuses on the utilization of TPUs and not the
creation of neural networks. I covered the design of computer vision previously in this book.

Code

import o s
import pandas a s pd

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / "
DOWNLOAD_SOURCE = URL+" r e l e a s e s / download / v1 / p a p e r c l i p s . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "

e l s e :

# I used t h i s
PATH = " / U s e r s / j e f f /temp "

l o c a l l y on my machine , you may need d i f f e r e n t

EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (EXTRACT_TARGET,

" p a p e r c l i p s " )

# Download p a p e r c l i p d a t a
! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}
! mkdir −p {EXTRACT_TARGET}

518

CHAPTER 13. ADVANCED/OTHER TOPICS

! u n z i p −o −j −d {SOURCE} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

# Add f i l e n a m e s
d f _ t r a i n = pd . read_csv ( o s . path . j o i n (SOURCE,
d f _ t r a i n [ ' f i l e n a m e ' ] = " c l i p s −" + d f _ t r a i n . id . a s t y p e ( s t r ) + " . j p g "

" t r a i n . c s v " ) )

13.5.1 Preparing Data for TPUs

To present the paperclips dataset to the TPU, we will convert the images to a Keras Dataset. Because
we will load the entire dataset to RAM, we will only utilize the ﬁrst 1,000 images. We previously loaded
the labels from the train.csv ﬁle. The following code loads these images and converts them to a Keras
dataset.
Code

t f

import t e n s o r f l o w a s
import k e r a s _ p r e p r o c e s s i n g
import glob , o s
import tqdm
import numpy a s np
from PIL import Image

IMG_SHAPE = ( 1 2 8 , 1 2 8 )
BATCH_SIZE = 32

# R e s i z e each image and c o n v e r t
def

load_images ( f i l e s ,

img_shape ) :

t h e 0−255 ranged RGB v a l u e s

t o 0−1 range .

c n t = len ( f i l e s )
x = np . z e r o s ( ( cnt ,)+ img_shape + ( 3 , ) , dtype=np . f l o a t 3 2 )
i = 0
for f i l e in tqdm . tqdm ( f i l e s ) :

img = Image . open ( f i l e )
img = img . r e s i z e ( img_shape )
img = np . a r r a y ( img )
img = img /255
x [ i , : , : , : ] = img
i+=1
return x

t r a i n i n g d a t a

# P r o c e s s
d f _ t r a i n = pd . read_csv ( o s . path . j o i n (SOURCE,
d f _ t r a i n [ ' f i l e n a m e ' ] = " c l i p s −" + d f _ t r a i n . id . a s t y p e ( s t r ) + " . j p g "

" t r a i n . c s v " ) )

13.5. PART 13.5: TENSOR PROCESSING UNITS (TPUS)

519

# Use o n l y t h e
d f _ t r a i n = d f _ t r a i n [ 0 : 1 0 0 0 ]

f i r s t 1000 images

# Load images
images = [ o s . path . j o i n (SOURCE, x )
x = load_images ( images , IMG_SHAPE)
y = d f _ t r a i n . c l i p _ c o u n t . v a l u e s

for x in d f _ t r a i n . f i l e n a m e ]

t o d a t a s e t

# Convert
d a t a s e t = t f . data . D a t a s e t . f r o m _ t e n s o r _ s l i c e s ( ( x , y ) )
d a t a s e t = d a t a s e t . batch (BATCH_SIZE)

TPUs are typically Cloud TPU workers, diﬀerent from the local process running the user’s Python
program. Thus, it would be best to do some initialization work to connect to the remote cluster and
initialize the TPUs. The TPU argument to tf.distribute.cluster_resolver. TPUClusterResolver is
a unique address just for Colab. If you are running your code on Google Compute Engine (GCE), you
should instead pass in the name of your Cloud TPU. The following code performs this initialization.

Code

try :

tpu = t f . d i s t r i b u t e . c l u s t e r _ r e s o l v e r . TPUClusterResolver . c o n n e c t ( )
print ( " Device : " ,
s t r a t e g y = t f . d i s t r i b u t e . TPUStrategy ( tpu )

tpu . master ( ) )

except :

s t r a t e g y = t f . d i s t r i b u t e . g e t _ s t r a t e g y ( )

print ( " Number␣ o f ␣ r e p l i c a s : " ,

s t r a t e g y . num_replicas_in_sync )

We will now use a ResNet neural network as a basis for our neural network. We begin by loading, from
Keras, the ResNet50 network. We will redeﬁne both the input shape and output of the ResNet model,
so we will not transfer the weights. Since we redeﬁne the input, the weights are of minimal value. We
specify include_top as False because we will change the input resolution. We also specify weights as
false because we must retrain the network after changing the top input layers.

Code

from t e n s o r f l o w . k e r a s . a p p l i c a t i o n s . r e s n e t 5 0 import ResNet50
from t e n s o r f l o w . k e r a s . l a y e r s import Input
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , GlobalAveragePooling2D
from t e n s o r f l o w . k e r a s . models import Model
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
from t e n s o r f l o w . k e r a s . m e t r i c s import RootMeanSquaredError

def create_model ( ) :

520

CHAPTER 13. ADVANCED/OTHER TOPICS

i n p u t _ t e n s o r = Input ( shape=IMG_SHAPE+ ( 3 , ) )

base_model = ResNet50 (

i n c l u d e _ t o p=F a l s e , w e i g h t s=None ,
input_shape=None )

i n p u t _ t e n s o r=i n p u t _ t e n s o r ,

x=base_model . output
x=GlobalAveragePooling2D ( ) ( x )
x=Dense ( 1 0 2 4 , a c t i v a t i o n= ' r e l u ' ) ( x )
x=Dense ( 1 0 2 4 , a c t i v a t i o n= ' r e l u ' ) ( x )
model=Model ( i n p u t s=base_model . input , o u t p u t s=Dense ( 1 ) ( x ) )
return model

with s t r a t e g y . s c o p e ( ) :

model = create_model ( )

model . compile ( l o s s = ' mean_squared_error ' , o p t i m i z e r= ' adam ' ,

m e t r i c s =[ RootMeanSquaredError ( name=" rmse " ) ] )

h i s t o r y = model . f i t ( d a t a s e t , e p oc h s =100 , steps_per_epoch =32 ,

v e r b o s e = 1 )

Output

. . .
32/32 [==============================] − 1 s 44ms/ s t e p − l o s s : 1 8 . 3 9 6 0
− rmse : 4 . 2 8 9 1
Epoch 100/100
32/32 [==============================] − 1 s 44ms/ s t e p − l o s s : 1 0 . 4 7 4 9
− rmse : 3 . 2 3 6 5

You might receive the following error while ﬁtting the neural network.

I n v a l i d A r g u m e n t E r r o r : Unable t o p a r s e t e n s o r p r o t o

If you do receive this error, it is likely because you are missing proper authentication to access Google

Drive to store your datasets.

Chapter 14

Other Neural Network Techniques

14.1 Part 14.1: What is AutoML

Automatic Machine Learning (AutoML) attempts to use machine learning to automate itself. Data is
passed to the AutoML application in raw form, and models are automatically generated.

14.1.1 AutoML from your Local Computer

The following AutoML applications are free:

• AutoKeras
• Auto-SKLearn
• Auto PyTorch
• TPOT

The following AutoML applications are commercial:

• Rapid Miner - Free student version available.
• Dataiku - Free community version available.
• DataRobot - Commercial
• H2O Driverless - Commercial

14.1.2 AutoML from Google Cloud

There are also cloud-hosted AutoML platforms:

• Google Cloud AutoML Tutorial
• Azure AutoML

This module will show how to use AutoKeras. First, we download the paperclips counting dataset that
you saw previously in this book.

521

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

522

Code

import o s
import pandas a s pd

URL = " h t t p s : / / g i t h u b . com/ j e f f h e a t o n / data−m i r r o r / "
DOWNLOAD_SOURCE = URL+" r e l e a s e s / download / v1 / p a p e r c l i p s . z i p "
DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE. r f i n d ( ' / ' ) + 1 : ]

i f COLAB:

PATH = " / c o n t e n t "

e l s e :

# I used t h i s
PATH = " / U s e r s / j e f f /temp "

l o c a l l y on my machine , you may need d i f f e r e n t

EXTRACT_TARGET = o s . path . j o i n (PATH, " c l i p s " )
SOURCE = o s . path . j o i n (EXTRACT_TARGET,

" p a p e r c l i p s " )

# Download p a p e r c l i p d a t a
! wget −O { o s . path . j o i n (PATH,DOWNLOAD_NAME) } {DOWNLOAD_SOURCE}
! mkdir −p {SOURCE}
! mkdir −p {TARGET}
! mkdir −p {EXTRACT_TARGET}
! u n z i p −o −j −d {SOURCE} { o s . path . j o i n (PATH, DOWNLOAD_NAME) } >/dev / n u l l

t r a i n i n g d a t a

# P r o c e s s
d f _ t r a i n = pd . read_csv ( o s . path . j o i n (SOURCE,
d f _ t r a i n [ ' f i l e n a m e ' ] = " c l i p s −" + d f _ t r a i n . id . a s t y p e ( s t r ) + " . j p g "

" t r a i n . c s v " ) )

# Use o n l y t h e
d f _ t r a i n = d f _ t r a i n [ 0 : 1 0 0 0 ]

f i r s t 1000 images

One limitation of AutoKeras is that it cannot directly utilize generators. Without resorting to complex
techniques, all training data must reside in RAM. We will use the following code to load the image data
to RAM.
Code

t f

import t e n s o r f l o w a s
import k e r a s _ p r e p r o c e s s i n g
import glob , o s
import tqdm
import numpy a s np
from PIL import Image

14.1. PART 14.1: WHAT IS AUTOML

523

IMG_SHAPE = ( 1 2 8 , 1 2 8 )

def

load_images ( f i l e s ,

img_shape ) :

c n t = len ( f i l e s )
x = np . z e r o s ( ( cnt ,)+ img_shape + ( 3 , ) )
i = 0
for f i l e in tqdm . tqdm ( f i l e s ) :

img = Image . open ( f i l e )
img = img . r e s i z e ( img_shape )
img = np . a r r a y ( img )
img = img /255
x [ i , : , : , : ] = img
i+=1
return x

images = [ o s . path . j o i n (SOURCE, x )
x = load_images ( images , IMG_SHAPE)
y = d f _ t r a i n . c l i p _ c o u n t . v a l u e s

for x in d f _ t r a i n . f i l e n a m e ]

14.1.3 Using AutoKeras

AutoKeras is an AutoML system based on Keras. The goal of AutoKeras is to make machine learning
accessible to everyone. DATA Lab develops it at Texas A&M University. We will see how to provide the
paperclips dataset to AutoKeras and create an automatically tuned Keras deep learning model from this
dataset. This automatic process frees you from choosing layer types and neuron counts.

We begin by installing AutoKeras.

Code

! p i p i n s t a l l a u t o k e r a s

AutoKeras contains several examples demonstrating image, tabular, and time-series data. We will make
use of the ImageRegressor. Refer to the AutoKeras documentation for other classiﬁers and regressors
to ﬁt speciﬁc uses.

We deﬁne several variables to determine the AutoKeras operation:

• MAX_TRIALS - Determines how many diﬀerent models to see.
• SEED - You can try diﬀerent random seeds to obtain diﬀerent results.
• VAL_SPLIT - What percent of the dataset should we use for validation.
• EPOCHS - How many epochs to try each model for training.
• BATCH_SIZE - Training batch size.

524

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

Setting MAX_TRIALS and EPOCHS will have a great impact on your total runtime. You must balance
how many models to try (MAX_TRIALS) and how deeply to try to train each (EPOCHS). AutoKeras
utilize early stopping, so setting EPOCHS too high will mean early stopping will prevent you from reaching
the EPOCHS number of epochs.

One strategy is to do a broad, shallow search. Set TRIALS high and EPOCHS low. The resulting

model likely has the best hyperparameters. Finally, train this resulting model fully.

Code

import numpy a s np
import a u t o k e r a s a s ak

MAX_TRIALS = 2
SEED = 42
VAL_SPLIT = 0 . 1
EPOCHS = 1000
BATCH_SIZE = 32

auto_reg = ak . I m a g e R e g r e s s o r ( o v e r w r i t e=True ,

m a x _ t r i a l s=MAX_TRIALS,
s e e d =42)

auto_reg . f i t ( x , y , v a l i d a t i o n _ s p l i t=VAL_SPLIT, b a t c h _ s i z e=BATCH_SIZE,

e p oc h s=EPOCHS)
print ( auto_reg . e v a l u a t e ( x , y ) )

Output

T r i a l 2 Complete [ 0 0 h 04m 17 s ]
v a l _ l o s s : 3 6 . 5 1 2 6 9 5 3 1 2 5
Best v a l _ l o s s So Far : 3 6 . 1 2 3 9 9 2 9 1 9 9 2 1 8 7 5
T o t a l e l a p s e d time : 01h 05m 46 s
INFO : t e n s o r f l o w : O r a c l e t r i g g e r e d e x i t
. . .
32/32 [==============================] − 3 s 85ms/ s t e p − l o s s : 2 4 . 9 2 1 8
− mean_squared_error : 2 4 . 9 2 1 8
Epoch 1000/1000
32/32 [==============================] − 2 s 78ms/ s t e p − l o s s : 2 4 . 9 1 4 1
− mean_squared_error : 2 4 . 9 1 4 1
INFO : t e n s o r f l o w : A s s e t s w r i t t e n t o :
32/32 [==============================] − 2 s 30ms/ s t e p − l o s s : 2 4 . 9 0 7 7
− mean_squared_error : 2 4 . 9 0 7 7
[ 2 4 . 9 0 7 7 4 9 1 7 6 0 2 5 3 9 , 2 4 . 9 0 7 7 4 9 1 7 6 0 2 5 3 9 ]

. / i m a g e _ r e g r e s s o r / best_model / a s s e t s

14.1. PART 14.1: WHAT IS AUTOML

525

We can now display the best model.

Code

print ( type ( auto_reg ) )
model = auto_reg . export_model ( )
model . summary ( )

Output

< c l a s s
Model :
_________________________________________________________________

' a u t o k e r a s . t a s k s . image . ImageRegressor '>
" model "

Layer

( type )

Output Shape

Param #

=================================================================

input_1 ( InputLayer )
c a s t _ t o _ f l o a t 3 2 ( CastToFloa
t 3 2 )
r e s n e t 5 0 ( F u n c t i o n a l )
f l a t t e n ( F l a t t e n )
r e g r e s s i o n _ h e a d _ 1 ( Dense )

[ ( None , 1 2 8 , 1 2 8 , 3 ) ]
( None , 1 2 8 , 1 2 8 , 3 )

0
0

( None , None , None , 2 0 4 8 )
( None , 3 2 7 6 8 )
( None , 1 )

23587712
0
32769

=================================================================
T o t a l params : 2 3 , 6 2 0 , 4 8 1
T r a i n a b l e params : 3 2 , 7 6 9
Non−t r a i n a b l e params : 2 3 , 5 8 7 , 7 1 2
_________________________________________________________________

This top model can be saved and either utilized or trained further.

Code

from k e r a s . models import load_model

print ( type ( model ) )

try :

model . s a v e ( " model_autokeras " ,

save_format=" t f " )

except E x c e p t i o n :

model . s a v e ( " model_autokeras . h5 " )

loaded_model = load_model ( " model_autokeras " , \

custom_objects=ak .CUSTOM_OBJECTS)

526

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

print ( loaded_model . e v a l u a t e ( x , y ) )

Output

' k e r a s . e n g i n e . f u n c t i o n a l . F u n c t i o n a l '>

< c l a s s
INFO : t e n s o r f l o w : A s s e t s w r i t t e n t o : model_autokeras / a s s e t s
32/32 [==============================] − 2 s 21ms/ s t e p − l o s s : 2 4 . 9 0 7 7
− mean_squared_error : 2 4 . 9 0 7 7
[ 2 4 . 9 0 7 7 4 9 1 7 6 0 2 5 3 9 , 2 4 . 9 0 7 7 4 9 1 7 6 0 2 5 3 9 ]

14.2 Part 14.2: Using Denoising AutoEncoders in Keras

Function approximation is perhaps the original task of machine learning. Long before computers and
even the notion of machine learning, scientists came up with equations to ﬁt their observations of nature.
Scientists ﬁnd equations to demonstrate correlations between observations. For example, various equations
relate mass, acceleration, and force.

Looking at complex data and deriving an equation does take some technical expertise. The goal of
function approximation is to remove intuition from the process and instead depend on an algorithmic
method to automatically generate an equation that describes data. A regression neural network performs
this task.

We begin by creating a function that we will use to chart a regression function.

Code

# R e g r e s s i o n c h a r t .
def c h a r t _ r e g r e s s i o n ( pred , y ,

s o r t=True ) :

t = pd . DataFrame ( { ' pred ' : pred ,
i f

s o r t :

' y ' : y . f l a t t e n ( ) } )

t . s o r t _ v a l u e s ( by=[ ' y ' ] ,

p l t . p l o t ( t [ ' y ' ] . t o l i s t ( ) ,
p l t . p l o t ( t [ ' pred ' ] . t o l i s t ( ) ,
p l t . y l a b e l ( ' output ' )
p l t . l e g e n d ( )
p l t . show ( )

i n p l a c e=True )
l a b e l= ' e x p e c t e d ' )

l a b e l= ' p r e d i c t i o n ' )

Next, we will attempt to approximate a slightly random variant of the trigonometric sine function.

Code

import t e n s o r f l o w a s
import numpy a s np

t f

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

527

import pandas a s pd
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g
import m a t p l o t l i b . p y p l o t a s p l t

rng = np . random . RandomState ( 1 )
x = np . s o r t ( ( 3 6 0 ∗ rng . rand ( 1 0 0 , 1 ) ) , a x i s =0)
y = np . a r r a y ( [ np . s i n ( x ∗ ( np . p i / 1 8 0 . 0 ) ) . r a v e l ( ) ] ) . T

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,
model . add ( Dense ( 5 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 1 ) )
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =0, b a t c h _ s i z e=len ( x ) , e po c h s =25000)

pred = model . p r e d i c t ( x )

print ( " Actual " )
print ( y [ 0 : 5 ] )

print ( " Pred " )
print ( pred [ 0 : 5 ] )

c h a r t _ r e g r e s s i o n ( pred . f l a t t e n ( ) , y , s o r t=F a l s e )

Output

Actual

528

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

[ [ 0 . 0 0 0 7 1 8 6 4 ]
[ 0 . 0 1 8 0 3 3 8 2 ]
[ 0 . 1 1 4 6 5 5 9 3 ]
[ 0 . 1 2 1 3 8 6 1 ]
[ 0 . 1 7 1 2 3 3 3 ] ]

Pred
[ [ 0 . 0 0 0 7 8 3 3 4 ]
[ 0 . 0 1 8 0 2 4 3 ]
[ 0 . 1 1 7 0 5 8 7 2 ]
[ 0 . 1 1 8 3 8 5 5 2 ]
[ 0 . 1 7 2 0 0 7 3 8 ] ]

As you can see, the neural network creates a reasonably close approximation of the random sine function.

14.2.1 Multi-Output Regression

Unlike most models, neural networks can provide multiple regression outputs. This feature allows a neural
network to generate various outputs for the same input. For example, you might train the MPG data set to
predict MPG and horsepower. One area in that multiple regression outputs can be helpful is autoencoders.
The following diagram shows a multi-regression neural network. As you can see, there are multiple output
neurons. Usually, you will use multiple output neurons for classiﬁcation. Each output neuron will represent
the probability of one of the classes. However, in this case, it is a regression neural network. Figure 13.MRG
shows multi-output regression.

The following program uses a multi-output regression to predict both sin and cos from the same input

data.

Code

from s k l e a r n import m e t r i c s

rng = np . random . RandomState ( 1 )
x = np . s o r t ( ( 3 6 0 ∗ rng . rand ( 1 0 0 , 1 ) ) , a x i s =0)
y = np . a r r a y ( [ np . p i ∗ np . s i n ( x ∗ ( np . p i / 1 8 0 . 0 ) ) . r a v e l ( ) , np . p i \

∗ np . c o s ( x ∗ ( np . p i / 1 8 0 . 0 ) ) . r a v e l ( ) ] ) . T

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,
model . add ( Dense ( 5 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 2 ) ) # Two o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =0, b a t c h _ s i z e=len ( x ) , e po c h s =25000)

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

529

Figure 14.1: Multi-Output Regression

r e g r e s s i o n DNN model .

# F i t
pred = model . p r e d i c t ( x )

s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , y ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )

np . s e t _ p r i n t o p t i o n s ( s u p p r e s s=True )

print ( " P r e d i c t e d : " )
print ( np . a r r a y ( pred [ 2 0 : 2 5 ] ) )

print ( " Expected : " )
print ( np . a r r a y ( y [ 2 0 : 2 5 ] ) )

Output

S c o r e (RMSE) : 0 . 0 6 1 3 6 9 5 2 2 2 0 4 6 6 9 5 6
P r e d i c t e d :

530

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

[ [ 2 . 7 2 0 4 0 4

[ 2 . 7 6 1 1 2 5 6
[ 2 . 9 1 0 6 0 3 8
[ 3 . 0 0 5 5 3 2
[ 3 . 0 4 1 5 2 5 6

1 . 5 9 0 4 2 6
]
1 . 5 1 6 5 5 1 5 ]
1 . 2 4 5 4 0 2 6 ]
1 . 0 3 5 9 6 6 2 ]
0 . 9 0 7 3 1 0 6 6 ] ]

Expected :
[ [ 2 . 7 0 7 6 5 3 1 3 1 . 5 9 3 1 7 8 8 8 ]
[ 2 . 7 5 1 3 8 4 4 5 1 . 5 1 6 4 0 6 2 8 ]
[ 2 . 8 9 2 9 9 9 9 9 1 . 2 2 4 8 0 8 3 5 ]
[ 2 . 9 7 6 0 3 9 4 2 1 . 0 0 6 3 7 6 5 5 ]
[ 3 . 0 1 3 8 1 7 2 3 0 . 8 8 6 8 5 4 0 4 ] ]

14.2.2 Simple Autoencoder

An autoencoder is a neural network with the same number of input neurons as it does outputs. The hidden
layers of the neural network will have fewer neurons than the input/output neurons. Because there are
fewer neurons, the auto-encoder must learn to encode the input to the fewer hidden neurons. The predictors
(x) and output (y) are precisely the same in an autoencoder. Because of this, we consider autoencoders to
be unsupervised. Figure 14.2 shows an autoencoder.

Figure 14.2: Simple Auto Encoder

The following program demonstrates a very simple autoencoder that learns to encode a sequence of

numbers. Fewer hidden neurons will make it more diﬃcult for the autoencoder to understand.

Code

from s k l e a r n import m e t r i c s
import numpy a s np

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

531

import pandas a s pd
from IPython . d i s p l a y import d i s p l a y , HTML
import t e n s o r f l o w a s

t f

x = np . a r r a y ( [ range ( 1 0 ) ] ) . a s t y p e ( np . f l o a t 3 2 )
print ( x )

model = S e q u e n t i a l ( )
model . add ( Dense ( 3 ,
model . add ( Dense ( x . shape [ 1 ] ) ) # M u l t i p l e o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , x , v e r b o s e =0, ep o c hs =1000)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

pred = model . p r e d i c t ( x )
s c o r e = np . s q r t ( m e t r i c s . mean_squared_error ( pred , x ) )
print ( " S c o r e ␣ (RMSE) : ␣ {} " . format ( s c o r e ) )
np . s e t _ p r i n t o p t i o n s ( s u p p r e s s=True )
print ( pred )

Output

[ [ 0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . ] ]
S c o r e (RMSE) : 0 . 0 2 4 2 4 5 1 8 7 6 4 0 1 9 0 1 2 5
[ [ 0 . 0 0 0 0 0 4 7 1 1 . 0 0 0 9 7 0 1
6 . 9 3 0 8 0 9 5

2 . 0 0 3 2 2 8 7
8 . 0 1 4 7 3 9

6 . 0 2 5 2 1 2

3 . 0 0 0 9 1 1
9 . 0 1 4 7 6 2

4 . 0 0 1 2 2 1 7
] ]

5 . 0 0 2 5 4 7 3

14.2.3 Autoencode (single image)

We are now ready to build a simple image autoencoder. The program below learns a capable encoding for
the image. You can see the distortions that occur.

Code

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
from t e n s o r f l o w . k e r a s . o p t i m i z e r s import SGD
import r e q u e s t s
from i o import BytesIO

I m a g e F i l e

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "

532

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :

' M o z i l l a / 5 . 0 ' } )

img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )
img = img . r e s i z e ( ( 1 2 8 , 1 2 8 ) ,
img_array = np . a s a r r a y ( img )
img_array = img_array . f l a t t e n ( )
img_array = np . a r r a y ( [
img_array = img_array . a s t y p e ( np . f l o a t 3 2 )
print ( img_array . shape [ 1 ] )
print ( img_array )

img_array ] )

Image . ANTIALIAS)

model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 ,
model . add ( Dense ( img_array . shape [ 1 ] ) ) # M u l t i p l e o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( img_array , img_array , v e r b o s e =0, ep o c hs =20)

input_dim=img_array . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

print ( " Neural ␣ network ␣ output " )
pred = model . p r e d i c t ( img_array )
print ( pred )
print ( img_array )
c o l s , rows = img . s i z e
img_array2 = pred [ 0 ] . r e s h a p e ( rows , c o l s , 3 )
img_array2 = img_array2 . a s t y p e ( np . u i n t 8 )
img2 = Image . f r o m a r r a y ( img_array2 ,
img2

'RGB ' )

Output

49152
[ [ 2 0 3 . 2 1 7 . 2 4 0 .
Neural network output
[ [ 2 3 8 . 3 1 0 8 8 2 3 9 . 5 5 9 1 3 1 9 4 . 4 7 5 3 6 . . .
[ [ 2 0 3 . 2 1 7 . 2 4 0 .

9 2 .

9 4 .

9 2 .

9 4 .

. . .

. . .

6 8 . ] ]

6 8 . ] ]

6 7 . 1 2 2 9 5

6 6 . 1 5 0 8 3

7 4 . 9 4 3 3 2 ] ]

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

533

14.2.4 Standardize Images

When processing several images together, it is sometimes essential to standardize them. The following
code reads a sequence of images and causes them to all be of the same size and perfectly square. If the
input images are not square, cropping will occur.

Code

I m a g e F i l e

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
import numpy a s np
from i o import BytesIO
from IPython . d i s p l a y import d i s p l a y , HTML

images = [

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / Brown_Hall . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g "

]

def make_square ( img ) :

c o l s , rows = img . s i z e

i f

rows>c o l s :

pad = ( rows−c o l s ) / 2
img = img . c r o p ( ( pad , 0 , c o l s , c o l s ) )

e l s e :

pad = ( c o l s −rows ) / 2
img = img . c r o p ( ( 0 , pad , rows , rows ) )

return img

x = [ ]

fo r u r l

in images :

I m a g e F i l e .LOAD_TRUNCATED_IMAGES = F a l s e
r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )
img = make_square ( img )

' M o z i l l a / 5 . 0 ' } )

534

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

Image . ANTIALIAS)

img = img . r e s i z e ( ( 1 2 8 , 1 2 8 ) ,
print ( u r l )
d i s p l a y ( img )
img_array = np . a s a r r a y ( img )
img_array = img_array . f l a t t e n ( )
img_array = img_array . a s t y p e ( np . f l o a t 3 2 )
img_array = ( img_array −128)/128
x . append ( img_array )

x = np . a r r a y ( x )

print ( x . shape )

Output

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / Brown_Hall . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g

. . .

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

535

14.2.5 Image Autoencoder (multi-image)

Autoencoders can learn the same encoding for multiple images. The following code learns a single encoding
for numerous images.

Code

I m a g e F i l e

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO
from s k l e a r n import m e t r i c s
import numpy a s np
import pandas a s pd
import t e n s o r f l o w a s
from IPython . d i s p l a y import d i s p l a y , HTML

t f

r e g r e s s i o n DNN model .

# F i t
print ( " C r e a t i n g / T r a i n i n g ␣ n e u r a l ␣ network " )
model = S e q u e n t i a l ( )
model . add ( Dense ( 5 0 ,
model . add ( Dense ( x . shape [ 1 ] ) ) # M u l t i p l e o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , x , v e r b o s e =0, ep o c hs =1000)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

print ( " S c o r e ␣ n e u r a l ␣ network " )
pred = model . p r e d i c t ( x )

c o l s , rows = img . s i z e
fo r i

in range ( len ( pred ) ) :

print ( pred [ i ] )
img_array2 = pred [ i ] . r e s h a p e ( rows , c o l s , 3 )
img_array2 = ( img_array2 ∗128)+128
img_array2 = img_array2 . a s t y p e ( np . u i n t 8 )
img2 = Image . f r o m a r r a y ( img_array2 ,
d i s p l a y ( img2 )

'RGB ' )

Output

536

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

t o <f u n c t i o n

t h e l a s t 11 c a l l s

C r e a t i n g / T r a i n i n g n e u r a l network
S c o r e n e u r a l network
WARNING: t e n s o r f l o w : 5 out o f
Model . make_predict_function .< l o c a l s >. p r e d i c t _ f u n c t i o n a t
0 x 7 f e 6 0 5 6 5 4 3 2 0 > t r i g g e r e d t f . f u n c t i o n r e t r a c i n g . T r a c i n g i s
and t h e e x c e s s i v e number o f
@tf . f u n c t i o n r e p e a t e d l y i n a loop ,
shapes ,
( 3 ) p a s s i n g Python o b j e c t s
d e f i n e your @tf . f u n c t i o n o u t s i d e o f
has e x p e r i m e n t a l _ r e l a x _ s h a p e s=True o p t i o n t h a t
t h a t can a v o i d u n n e c e s s a r y r e t r a c i n g . For
h t t p s : / /www. t e n s o r f l o w . o r g / g u i d e / f u n c t i o n#c o n t r o l l i n g _ r e t r a c i n g and
h t t p s : / /www. t e n s o r f l o w . o r g / api_docs / python / t f / f u n c t i o n f o r more
d e t a i l s .
[ 0 . 9 8 4 4 6 8 4 6

( 2 ) p a s s i n g t e n s o r s with d i f f e r e n t
t e n s o r s . For
i n s t e a d o f

0 . 9 8 4 5 6 8 3 6 . . . −0.17971231 −0.20315537

t r a c i n g s c o u l d be due t o ( 1 ) c r e a t i n g

( 2 ) , @tf . f u n c t i o n
r e l a x e s argument s h a p e s

t h e l o o p . For

( 3 ) , p l e a s e

( 1 ) , p l e a s e

0 . 9 8 4 4 9 4 3

e x p e n s i v e

r e f e r

t o

−0.20320868]

[ 0 . 5 1 4 0 9 4 3

0 . 5 9 2 7 1 0 5 5

0 . 6 6 3 3 0 8 9

. . . −0.40498623 −0.40472946

−0.54082954]

[ −0.40605062

0 . 0 8 6 3 3 2 3 8

0 . 6 5 7 1 7 1 6

. . . −0.12500083 −0.22656606

−0.3437891 ]

. . .

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

537

14.2.6 Adding Noise to an Image

Autoencoders can handle noise. First, it is essential to see how to add noise to an image. There are many
ways to add such noise. The following code adds random black squares to the image to produce noise.

Code

from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO

I m a g e F i l e

%m a t p l o t l i b i n l i n e

def add_noise ( a ) :
a2 = a . copy ( )
rows = a2 . shape [ 0 ]
c o l s = a2 . shape [ 1 ]
s = int (min( rows , c o l s ) / 2 0 ) # s i z e o f

for i

in range ( 1 0 0 ) :

x = np . random . r a n d i n t ( c o l s −s )
y = np . random . r a n d i n t ( rows−s )
a2 [ y : ( y+s ) , x : ( x+s ) ] = 0

return a2

s p o t

i s 1/20 o f

s m a l l e s t dimension

u r l = " h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g "
#u r l = " h t t p : / /www. h e a t o n r e s e a r c h . com/ images / about − j e f f . j p g "

r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )

' M o z i l l a / 5 . 0 ' } )

img_array = np . a s a r r a y ( img )
rows = img_array . shape [ 0 ]
c o l s = img_array . shape [ 1 ]

print ( " Rows : ␣ { } , ␣ C o l s : ␣ {} " . format ( rows , c o l s ) )

# C reate new image
img2_array = img_array . a s t y p e ( np . u i n t 8 )
print ( img2_array . shape )

538

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

img2_array = add_noise ( img2_array )
img2 = Image . f r o m a r r a y ( img2_array ,
img2

'RGB ' )

Output

Rows : 7 6 8 , C o l s : 1024
( 7 6 8 , 1 0 2 4 , 3 )

14.2.7 Denoising Autoencoder

You design a denoising autoencoder to remove noise from input signals. You train the network to convert
noisy data (x) to the original input (y). The y becomes each image/signal (just like a normal autoencoder);
however, the x becomes a version of y with noise added. Noise is artiﬁcially added to the images to produce
x. The following code creates ten noisy versions of each of the images.

Code

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
import numpy a s np

I m a g e F i l e

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

539

from i o import BytesIO
from IPython . d i s p l a y import d i s p l a y , HTML

#u r l = " h t t p : / /www. h e a t o n r e s e a r c h . com/ images / about − j e f f . j p g "

images = [

" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / Brown_Hall . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g " ,
" h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g "

]

def make_square ( img ) :

c o l s , rows = img . s i z e

i f

rows>c o l s :

pad = ( rows−c o l s ) / 2
img = img . c r o p ( ( pad , 0 , c o l s , c o l s ) )

e l s e :

pad = ( c o l s −rows ) / 2
img = img . c r o p ( ( 0 , pad , rows , rows ) )

return img

x = [ ]
y = [ ]
loaded_images = [ ]

for u r l

in images :

I m a g e F i l e .LOAD_TRUNCATED_IMAGES = F a l s e
r e s p o n s e = r e q u e s t s . g e t ( u r l , h e a d e r s={ ' User−Agent ' :
img = Image . open ( BytesIO ( r e s p o n s e . c o n t e n t ) )
img . l o a d ( )
img = make_square ( img )
img = img . r e s i z e ( ( 1 2 8 , 1 2 8 ) ,

Image . ANTIALIAS)

' M o z i l l a / 5 . 0 ' } )

loaded_images . append ( img )
print ( u r l )
d i s p l a y ( img )
for i

in range ( 1 0 ) :

img_array = np . a s a r r a y ( img )
img_array_noise = add_noise ( img_array )

img_array = img_array . f l a t t e n ( )

540

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

img_array = img_array . a s t y p e ( np . f l o a t 3 2 )
img_array = ( img_array −128)/128

img_array_noise = img_array_noise . f l a t t e n ( )
img_array_noise = img_array_noise . a s t y p e ( np . f l o a t 3 2 )
img_array_noise = ( img_array_noise −128)/128

x . append ( img_array_noise )
y . append ( img_array )

x = np . a r r a y ( x )
y = np . a r r a y ( y )

print ( x . shape )
print ( y . shape )

Output

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / Brown_Hall . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r / b r o o k i n g s . j p e g

h t t p s : / / data . h e a t o n r e s e a r c h . com/ images / j u p y t e r /WUSTLKnight . j p e g

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

541

. . .

We now train the autoencoder neural network to transform the noisy images into clean images.

Code

I m a g e F i l e

%m a t p l o t l i b i n l i n e
from PIL import Image ,
from m a t p l o t l i b . p y p l o t import imshow
import r e q u e s t s
from i o import BytesIO
from s k l e a r n import m e t r i c s
import numpy a s np
import pandas a s pd
import t e n s o r f l o w a s
from IPython . d i s p l a y import d i s p l a y , HTML

t f

r e g r e s s i o n DNN model .

# F i t
print ( " C r e a t i n g / T r a i n i n g ␣ n e u r a l ␣ network " )
model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 0 ,
model . add ( Dense ( 5 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( 1 0 0 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( x . shape [ 1 ] ) ) # M u l t i p l e o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x , y , v e r b o s e =1, ep o c hs =20)

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

print ( " Neural ␣ network ␣ t r a i n e d " )

Output

C r e a t i n g / T r a i n i n g n e u r a l network
. . .
1/1 [==============================] − 0 s 105ms/ s t e p − l o s s : 0 . 0 0 6 8
Epoch 20/20
1/1 [==============================] − 0 s 110ms/ s t e p − l o s s : 0 . 0 0 5 6
Neural network t r a i n e d

We are now ready to evaluate the results.

542

Code

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

for z in range ( 3 ) :

print ( " ∗∗∗ ␣ T r i a l ␣ {} " . format ( z +1))

# Choose random image
i = np . random . r a n d i n t ( len ( loaded_images ) )
img = loaded_images [ i ]
img_array = np . a s a r r a y ( img )
rows = img . s i z e
c o l s ,

# Add n o i s e
img_array_noise = add_noise ( img_array )

#D i s p l a y n o i s y image
img2 = img_array_noise . a s t y p e ( np . u i n t 8 )
img2 = Image . f r o m a r r a y ( img2 ,
print ( " With␣ n o i s e : " )
d i s p l a y ( img2 )

'RGB ' )

# P r e s e n t n o i s y image t o a u t o e n c o d e r
img_array_noise = img_array_noise . f l a t t e n ( )
img_array_noise = img_array_noise . a s t y p e ( np . f l o a t 3 2 )
img_array_noise = ( img_array_noise −128)/128
img_array_noise = np . a r r a y ( [ img_array_noise ] )
pred = model . p r e d i c t ( img_array_noise ) [ 0 ]

r e s u l t

# D i s p l a y n e u r a l
img_array2 = pred . r e s h a p e ( rows , c o l s , 3 )
img_array2 = ( img_array2 ∗128)+128
img_array2 = img_array2 . a s t y p e ( np . u i n t 8 )
img2 = Image . f r o m a r r a y ( img_array2 ,
print ( " A f t e r ␣ auto ␣ encode ␣ n o i s e ␣ removal " )
d i s p l a y ( img2 )

'RGB ' )

Output

14.2. PART 14.2: USING DENOISING AUTOENCODERS IN KERAS

543

∗∗∗ T r i a l 1
With n o i s e :

A f t e r auto encode n o i s e removal

∗∗∗ T r i a l 2
With n o i s e :

A f t e r auto encode n o i s e removal

∗∗∗ T r i a l 3
With n o i s e :

544

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

A f t e r auto encode n o i s e removal

14.3 Part 14.3: Anomaly Detection in Keras

Anomaly detection is an unsupervised training technique that analyzes the degree to which incoming data
diﬀers from the data you used to train the neural network. Traditionally, cybersecurity experts have used
anomaly detection to ensure network security. However, you can use anomalies in data science to detect
input for which you have not trained your neural network.

There are several data sets that many commonly use to demonstrate anomaly detection. In this part,

we will look at the KDD-99 dataset.

• Stratosphere IPS Dataset
• The ADFA Intrusion Detection Datasets (2013) - for HIDS
• ITOC CDX (2009)
• KDD-99 Dataset

14.3.1 Read in KDD99 Data Set

Although the KDD99 dataset is over 20 years old, it is still widely used to demonstrate Intrusion Detection
Systems (IDS) and Anomaly detection. KDD99 is the data set used for The Third International Knowledge
Discovery and Data Mining Tools Competition, held in conjunction with KDD-99, The Fifth International
Conference on Knowledge Discovery and Data Mining. The competition task was to build a network
intrusion detector, a predictive model capable of distinguishing between "bad" connections, called intrusions
or attacks, and "good" normal connections. This database contains a standard set of data to be audited,
including various intrusions simulated in a military network environment.

The following code reads the KDD99 CSV dataset into a Pandas data frame. The standard format of

KDD99 does not include column names. Because of that, the program adds them.

Code

import pandas a s pd
from t e n s o r f l o w . k e r a s . u t i l s

import g e t _ f i l e

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 6 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

try :

path = g e t _ f i l e ( ' kdd−with−columns . c s v ' , o r i g i n =\
' h t t p s : / / g i t h u b . com/ j e f f h e a t o n / j h e a t o n −ds2 /raw/main/ ' \
' kdd−with−columns . c s v ' , a r c h i v e _ f o r m a t=None )

except :

print ( ' E r r o r ␣ downloading ' )
r a i s e

14.3. PART 14.3: ANOMALY DETECTION IN KERAS

545

print ( path )

# O r i g i o n a l
d f = pd . read_csv ( path )

f i l e : h t t p : / / kdd . i c s . u c i . edu / d a t a b a s e s / kddcup99 / kddcup99 . html

print ( " Read␣ {} ␣ rows . " . format ( len ( d f ) ) )
# d f = d f . sample ( f r a c =0.1 , r e p l a c e=F a l s e ) # Uncomment
# sample o n l y 10% o f
t h e d a t a s e t
d f . dropna ( i n p l a c e=True , a x i s =1)
# For now ,

j u s t drop NA ' s ( rows w i t h m i s s i n g v a l u e s )

t h i s

l i n e t o

# d i s p l a y 5 rows
pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 5 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d f

Output

0
1
...
494019
494020

duration
0
0
...
0
0

protocol_type
tcp
tcp
...
tcp
tcp

...
...
...
...
...
...

dst_host_srv_rerror_rate
0.0
0.0
...
0.0
0.0

outcome
normal.
normal.
...
normal.
normal.

Downloading data from h t t p s : / / g i t h u b . com/ j e f f h e a t o n / j h e a t o n −
ds2 /raw/main/kdd−with−columns . c s v
68132864/68132668 [==============================] − 1 s 0 us / s t e p
68141056/68132668 [==============================] − 1 s 0 us / s t e p
/ r o o t / . k e r a s / d a t a s e t s /kdd−with−columns . c s v
Read 494021 rows .

The KDD99 dataset contains many columns that deﬁne the network state over time intervals during
which a cyber attack might have taken place. The " outcome " column speciﬁes either "normal," indicating
no attack, or the type of attack performed. The following code displays the counts for each type of attack
and "normal".

Code

d f . groupby ( ' outcome ' ) [ ' outcome ' ] . count ( )

546

Output

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

outcome
back .
b u f f e r _ o v e r f l o w .

2203
30

w a r e z c l i e n t .
warezmaster .
Name : outcome , Length : 2 3 , dtype :

. . .
1020
20

i n t 6 4

14.3.2 Preprocessing

We must perform some preprocessing before we can feed the KDD99 data into the neural network. We
provide the following two functions to assist with preprocessing. The ﬁrst function converts numeric
columns into Z-Scores. The second function replaces categorical values with dummy variables.

Code

# Encode a numeric column as
def encode_numeric_zscore ( df , name , mean=None ,

z s c o r e s

sd=None ) :

i f mean i s None :

mean = d f [ name ] . mean ( )

i f

sd i s None :

sd = d f [ name ] . s t d ( )

d f [ name ] = ( d f [ name ] − mean ) / sd

# Encode t e x t v a l u e s
# f o r red , green , b l u e )
def encode_text_dummy ( df , name ) :

t o dummy v a r i a b l e s ( i . e .

dummies = pd . get_dummies ( d f [ name ] )
fo r x in dummies . columns :

dummy_name = f " {name}−{x} "
d f [ dummy_name ] = dummies [ x ]

d f . drop ( name , a x i s =1,

i n p l a c e=True )

[ 1 , 0 , 0 ] , [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ]

This code converts all numeric columns to Z-Scores and all textual columns to dummy variables. We
now use these functions to preprocess each of the columns. Once the program preprocesses the data, we
display the results.

14.3. PART 14.3: ANOMALY DETECTION IN KERAS

547

Code

# Now encode t h e f e a t u r e v e c t o r

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 6 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

for name in d f . columns :
i f name == ' outcome ' :

pass

e l i f name in [ ' p r o t o c o l _ t y p e ' , ' s e r v i c e ' , ' f l a g ' , ' l a n d ' , ' l o g g e d _ i n ' ,

' i s _ h o s t _ l o g i n ' , ' i s _ g u e s t _ l o g i n ' ] :

encode_text_dummy ( df , name )

e l s e :

encode_numeric_zscore ( df , name )

# d i s p l a y 5 rows

d f . dropna ( i n p l a c e=True , a x i s =1)
d f [ 0 : 5 ]

Output

duration
-0.067792
-0.067792
-0.067792
-0.067792
-0.067792

src_bytes
-0.002879
-0.002820
-0.002824
-0.002840
-0.002842

dst_bytes
0.138664
-0.011578
0.014179
0.014179
0.035214

...
...
...
...
...
...

is_host_login-0
1
1
1
1
1

is_guest_login-0
1
1
1
1
1

is_guest_login-1
0
0
0
0
0

0
1
2
3
4

We divide the data into two groups, "normal" and the various attacks to perform anomaly detection.

The following code divides the data into two data frames and displays each of these two groups’ sizes.

Code

normal_mask = d f [ ' outcome ']== ' normal . '
attack_mask = d f [ ' outcome ' ] ! = ' normal . '

d f . drop ( ' outcome ' , a x i s =1, i n p l a c e=True )

df_normal = d f [ normal_mask ]
d f _ a t t a c k = d f [ attack_mask ]

548

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

print ( f " Normal␣ count : ␣{ l e n ( df_normal ) } " )
print ( f " Attack ␣ count : ␣{ l e n ( d f _ a t t a c k ) } " )

Output

Normal count : 97278
Attack count : 396743

Next, we convert these two data frames into Numpy arrays. Keras requires this format for data.

Code

t h e numeric f e a t u r e v e c t o r , as

i s

# This
x_normal = df_normal . v a l u e s
x_attack = d f _ a t t a c k . v a l u e s

i t g o e s

t o t h e n e u r a l n e t

14.3.3 Training the Autoencoder

It is important to note that we are not using the outcome column as a label to predict. We will train
an autoencoder on the normal data and see how well it can detect that the data not ﬂagged as "normal"
represents an anomaly. This anomaly detection is unsupervised; there is no target (y) value to predict.

Next, we split the normal data into a 25% test set and a 75% train set. The program will use the test

data to facilitate early stopping.

Code

from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t

x_normal_train , x_normal_test = t r a i n _ t e s t _ s p l i t (

x_normal ,

t e s t _ s i z e =0.25 , random_state =42)

We display the size of the train and test sets.

Code

print ( f " Normal␣ t r a i n ␣ count : ␣{ l e n ( x_normal_train ) } " )
print ( f " Normal␣ t e s t ␣ count : ␣{ l e n ( x_normal_test ) } " )

Output

Normal
Normal

t r a i n count : 72958
t e s t count : 24320

14.3. PART 14.3: ANOMALY DETECTION IN KERAS

549

We are now ready to train the autoencoder on the normal data. The autoencoder will learn to compress
the data to a vector of just three numbers. The autoencoder should be able to also decompress with
reasonable accuracy. As is typical for autoencoders, we are merely training the neural network to produce
the same output values as were fed to the input layer.

Code

from s k l e a r n import m e t r i c s
import numpy a s np
import pandas a s pd
from IPython . d i s p l a y import d i s p l a y , HTML
import t e n s o r f l o w a s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n

t f

input_dim=x_normal . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model = S e q u e n t i a l ( )
model . add ( Dense ( 2 5 ,
model . add ( Dense ( 3 , a c t i v a t i o n= ' r e l u ' ) ) # s i z e t o compress t o
model . add ( Dense ( 2 5 , a c t i v a t i o n= ' r e l u ' ) )
model . add ( Dense ( x_normal . shape [ 1 ] ) ) # M u l t i p l e o u t p u t neurons
model . compile ( l o s s= ' mean_squared_error ' , o p t i m i z e r= ' adam ' )
model . f i t ( x_normal_train , x_normal_train , v e r b o s e =1, e po c h s =100)

Output

. . .
2280/2280 [==============================] − 6 s 3ms/ s t e p − l o s s :
0 . 0 5 1 2
Epoch 100/100
2280/2280 [==============================] − 5 s 2ms/ s t e p − l o s s :
0 . 0 5 6 2

14.3.4 Detecting an Anomaly

We are now ready to see if the abnormal data is an anomaly. The ﬁrst two scores show the in-sample and
out of sample RMSE errors. These two scores are relatively low at around 0.33 because they resulted from
normal data. The much higher 0.76 error occurred from the abnormal data. The autoencoder is not as
capable of encoding data that represents an attack. This higher error indicates an anomaly.

Code

pred = model . p r e d i c t ( x_normal_test )
s c o r e 1 = np . s q r t ( m e t r i c s . mean_squared_error ( pred , x_normal_test ) )

550

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

pred = model . p r e d i c t ( x_normal )
s c o r e 2 = np . s q r t ( m e t r i c s . mean_squared_error ( pred , x_normal ) )
pred = model . p r e d i c t ( x_attack )
s c o r e 3 = np . s q r t ( m e t r i c s . mean_squared_error ( pred , x_attack ) )
print ( f " Out␣ o f ␣ Sample ␣Normal␣ S c o r e ␣ (RMSE) : ␣{ s c o r e 1 } " )
print ( f " Insample ␣Normal␣ S c o r e ␣ (RMSE) : ␣ { s c o r e 2 } " )
print ( f " Attack ␣Underway␣ S c o r e ␣ (RMSE) : ␣{ s c o r e 3 } " )

Output

Out o f Sample Normal S c o r e (RMSE) : 0 . 2 7 4 8 5 2 6 7 6 4 1 0 4 4 2 7 5
Insample Normal S c o r e (RMSE) : 0 . 2 4 6 1 3 7 6 2 5 0 9 0 9 3 5 8 7
Attack Underway S c o r e (RMSE) : 0 . 6 3 9 8 4 9 2 4 7 1 9 7 4 8 5 8

14.4 Part 14.4: Training an Intrusion Detection System with

KDD99

The KDD-99 dataset is very famous in the security ﬁeld and almost a "hello world" of Intrusion Detection
Systems (IDS) in machine learning. An intrusion detection system (IDS) is a program that monitors
computers and network systems for malicious activity or policy violations. Any intrusion activity or
violation is typically reported to an administrator or collected centrally. IDS types range in scope from
single computers to large networks. Although the KDD99 dataset is over 20 years old, it is still widely used
to demonstrate Intrusion Detection Systems (IDS). KDD99 is the data set used for The Third International
Knowledge Discovery and Data Mining Tools Competition, which was held in conjunction with KDD-99,
The Fifth International Conference on Knowledge Discovery and Data Mining. The competition task was to
build a network intrusion detector, a predictive model capable of distinguishing between "bad" connections,
called intrusions or attacks, and "good" normal connections. This database contains a standard set of data
to be audited, including various intrusions simulated in a military network environment.

14.4.1 Read in Raw KDD-99 Dataset

The following code reads the KDD99 CSV dataset into a Pandas data frame. The standard format of
KDD99 does not include column names. Because of that, the program adds them.

Code

import pandas a s pd
from t e n s o r f l o w . k e r a s . u t i l s

import g e t _ f i l e

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 6 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

14.4. PART 14.4: TRAINING AN INTRUSION DETECTION SYSTEM WITH KDD99

551

try :

path = g e t _ f i l e ( ' kdd−with−columns . c s v ' , o r i g i n =\
' h t t p s : / / g i t h u b . com/ j e f f h e a t o n / j h e a t o n −ds2 /raw/main/ ' \
' kdd−with−columns . c s v ' , a r c h i v e _ f o r m a t=None )

except :

print ( ' E r r o r ␣ downloading ' )
r a i s e

print ( path )

# O r i g i o n a l
d f = pd . read_csv ( path )

f i l e : h t t p : / / kdd . i c s . u c i . edu / d a t a b a s e s / kddcup99 / kddcup99 . html

print ( " Read␣ {} ␣ rows . " . format ( len ( d f ) ) )
# d f = d f . sample ( f r a c =0.1 , r e p l a c e=F a l s e ) # Uncomment
# sample o n l y 10% o f
t h e d a t a s e t
d f . dropna ( i n p l a c e=True , a x i s =1)
# For now ,

j u s t drop NA ' s ( rows w i t h m i s s i n g v a l u e s )

t h i s

l i n e t o

# d i s p l a y 5 rows
pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 5 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )
d f

Output

0
1
...
494019
494020

duration
0
0
...
0
0

protocol_type
tcp
tcp
...
tcp
tcp

...
...
...
...
...
...

dst_host_srv_rerror_rate
0.0
0.0
...
0.0
0.0

outcome
normal.
normal.
...
normal.
normal.

Downloading data from h t t p s : / / g i t h u b . com/ j e f f h e a t o n / j h e a t o n −
ds2 /raw/main/kdd−with−columns . c s v
68132864/68132668 [==============================] − 1 s 0 us / s t e p
68141056/68132668 [==============================] − 1 s 0 us / s t e p
/ r o o t / . k e r a s / d a t a s e t s /kdd−with−columns . c s v
Read 494021 rows .

552

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

14.4.2 Analyzing a Dataset

Before we preprocess the KDD99 dataset, let’s look at the individual columns and distributions. You can
use the following script to give a high-level overview of how a dataset appears.

Code

import pandas a s pd
import o s
import numpy a s np
from s k l e a r n import m e t r i c s
from s c i p y . s t a t s import z s c o r e

def e x p a n d _ c a t e g o r i e s ( v a l u e s ) :

r e s u l t = [ ]
s = v a l u e s . v al ue _co un ts ( )
t = f l o a t ( len ( v a l u e s ) )
for v in s . i n d e x :

r e s u l t . append ( " {}:{}% " . format ( v , round ( 1 0 0 ∗ ( s [ v ] / t ) , 2 ) ) )

return " [ { } ] " . format ( " , " . j o i n ( r e s u l t ) )

def a n a l y z e ( d f ) :
print ( )
c o l s = d f . columns . v a l u e s
t o t a l = f l o a t ( len ( d f ) )

print ( " {} ␣ rows " . format ( int ( t o t a l ) ) )
fo r c o l

in c o l s :

u n i q u e s = d f [ c o l ] . unique ( )
unique_count = len ( u n i q u e s )
i f unique_count >100:

print ( " ∗∗ ␣ { } : { } ␣ ({}%) " . format ( c o l , unique_count , \

int ( ( ( unique_count ) / t o t a l ) ∗ 1 0 0 ) ) )

e l s e :

print ( " ∗∗ ␣ { } : { } " . format ( c o l , e x p a n d _ c a t e g o r i e s ( d f [ c o l ] ) ) )
e x p a n d _ c a t e g o r i e s ( d f [ c o l ] )

The analysis looks at how many unique values are present. For example, duration, a numeric value, has
2495 unique values, and there is a 0% overlap. A text/categorical value such as protocol_type only has a
few unique values, and the program shows the percentages of each. Columns with many unique values do
not have their item counts shown to save display space.

14.4. PART 14.4: TRAINING AN INTRUSION DETECTION SYSTEM WITH KDD99

553

Code

# Analyze KDD−99
a n a l y z e ( d f )

Output

494021 rows
∗∗ d u r a t i o n : 2 4 9 5 (0%)
∗∗ p r o t o c o l _ t y p e : [ icmp : 5 7 . 4 1 % , t c p : 3 8 . 4 7 % , udp : 4 . 1 2 % ]
∗∗ s e r v i c e : [ e c r _ i : 5 6 . 9 6 % , p r i v a t e : 2 2 . 4 5 % , h t t p : 1 3 . 0 1 % , smtp : 1 . 9 7 % , o t h e r : 1
.46% , domain_u : 1 . 1 9 % , ftp_data : 0 . 9 6 % , eco_i : 0 . 3 3 % , f t p : 0 . 1 6 % , f i n g e r : 0 . 1 4 % ,
urp_i : 0 . 1 1 % , t e l n e t : 0 . 1 % , ntp_u : 0 . 0 8 % , auth : 0 . 0 7 % , pop_3 : 0 . 0 4 % , time : 0 . 0 3 % ,
csnet_ns : 0 . 0 3 % , remote_job : 0 . 0 2 % , gopher : 0 . 0 2 % , imap4 : 0 . 0 2 % , d i s c a r d : 0 . 0 2 %
, domain : 0 . 0 2 % , i s o _ t s a p : 0 . 0 2 % , s y s t a t : 0 . 0 2 % , s h e l l : 0 . 0 2 % , echo : 0 . 0 2 % , r j e : 0
.02% , whois : 0 . 0 2 % , s q l _ n e t : 0 . 0 2 % , p r i n t e r : 0 . 0 2 % , nntp : 0 . 0 2 % , c o u r i e r : 0 . 0 2 % ,
su nrp c : 0 . 0 2 % , n e t b i o s _ s s n : 0 . 0 2 % , mtp : 0 . 0 2 % , vmnet : 0 . 0 2 % , uucp_path : 0 . 0 2 % , u
ucp : 0 . 0 2 % , k l o g i n : 0 . 0 2 % , bgp : 0 . 0 2 % , s s h : 0 . 0 2 % , supdup : 0 . 0 2 % , nnsp : 0 . 0 2 % , l o g
i n : 0 . 0 2 % , hostnames : 0 . 0 2 % , e f s : 0 . 0 2 % , daytime : 0 . 0 2 % , l i n k : 0 . 0 2 % , n e t b i o s _ n s
: 0 . 0 2 % , pop_2 : 0 . 0 2 % , l d a p : 0 . 0 2 % , netbios_dgm : 0 . 0 2 % , e x e c : 0 . 0 2 % , http_443 : 0 .
02%, k s h e l l : 0 . 0 2 % , name : 0 . 0 2 % , c t f : 0 . 0 2 % , n e t s t a t : 0 . 0 2 % , Z39_50 : 0 . 0 2 % , IRC : 0
.01% , urh_i : 0 . 0 % , X11 : 0 . 0 % , tim_i : 0 . 0 % ,pm_dump: 0 . 0 % , tftp_u : 0 . 0 % , red_i : 0 . 0

. . .

∗∗ outcome : [ smurf . : 5 6 . 8 4 % , neptune . : 2 1 . 7 % , normal . : 1 9 . 6 9 % , back . : 0 . 4 5 % , s a
tan . : 0 . 3 2 % , i ps w ee p . : 0 . 2 5 % , p orts weep . : 0 . 2 1 % , w a r e z c l i e n t . : 0 . 2 1 % , t e a r d r o p
. : 0 . 2 % , pod . : 0 . 0 5 % , nmap . : 0 . 0 5 % , guess_passwd . : 0 . 0 1 % , b u f f e r _ o v e r f l o w . : 0 . 0
1%, l a n d . : 0 . 0 % , warezmaster . : 0 . 0 % , imap . : 0 . 0 % , r o o t k i t . : 0 . 0 % , loadmodule . : 0
.0% , f t p _ w r i t e . : 0 . 0 % , multihop . : 0 . 0 % , phf . : 0 . 0 % , p e r l . : 0 . 0 % , spy . : 0 . 0 % ]

14.4.3 Encode the feature vector

We use the same two functions provided earlier to preprocess the data. The ﬁrst encodes Z-Scores, and
the second creates dummy variables from categorical columns.

Code

# Encode a numeric column as
def encode_numeric_zscore ( df , name , mean=None ,

z s c o r e s

sd=None ) :

i f mean i s None :

mean = d f [ name ] . mean ( )

554

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

i f

sd i s None :

sd = d f [ name ] . s t d ( )

d f [ name ] = ( d f [ name ] − mean ) / sd

# Encode t e x t v a l u e s
# [ 0 , 1 , 0 ] , [ 0 , 0 , 1 ]
def encode_text_dummy ( df , name ) :

f o r red , green , b l u e )

t o dummy v a r i a b l e s ( i . e .

[ 1 , 0 , 0 ] ,

dummies = pd . get_dummies ( d f [ name ] )
for x in dummies . columns :

dummy_name = f " {name}−{x} "
d f [ dummy_name ] = dummies [ x ]

d f . drop ( name , a x i s =1,

i n p l a c e=True )

Again, just as we did for anomaly detection, we preprocess the data set. We convert all numeric values

to Z-Score and translate all categorical to dummy variables.

Code

# Now encode t h e f e a t u r e v e c t o r

pd . s e t _ o p t i o n ( ' d i s p l a y . max_columns ' , 6 )
pd . s e t _ o p t i o n ( ' d i s p l a y . max_rows ' , 5 )

for name in d f . columns :
i f name == ' outcome ' :

pass

e l i f name in [ ' p r o t o c o l _ t y p e ' , ' s e r v i c e ' , ' f l a g ' , ' l a n d ' , ' l o g g e d _ i n ' ,

' i s _ h o s t _ l o g i n ' , ' i s _ g u e s t _ l o g i n ' ] :

encode_text_dummy ( df , name )

e l s e :

encode_numeric_zscore ( df , name )

# d i s p l a y 5 rows

d f . dropna ( i n p l a c e=True , a x i s =1)
d f [ 0 : 5 ]

t o numpy − C l a s s i f i c a t i o n

# Convert
x_columns = d f . columns . drop ( ' outcome ' )
x = d f [ x_columns ] . v a l u e s

14.4. PART 14.4: TRAINING AN INTRUSION DETECTION SYSTEM WITH KDD99

555

dummies = pd . get_dummies ( d f [ ' outcome ' ] ) # C l a s s i f i c a t i o n
outcomes = dummies . columns
num_classes = len ( outcomes )
y = dummies . v a l u e s

We will attempt to predict what type of attack is underway. The outcome column speciﬁes the attack
type. A value of normal indicates that there is no attack underway. We display the outcomes; some attack
types are much rarer than others.

Code

d f . groupby ( ' outcome ' ) [ ' outcome ' ] . count ( )

Output

outcome
back .
b u f f e r _ o v e r f l o w .

2203
30

w a r e z c l i e n t .
warezmaster .
Name : outcome , Length : 2 3 , dtype :

. . .
1020
20

i n t 6 4

14.4.4 Train the Neural Network

We now train the neural network to classify the diﬀerent KDD99 outcomes. The code provided here
implements a relatively simple neural with two hidden layers. We train it with the provided KDD99 data.

Code

import pandas a s pd
import i o
import r e q u e s t s
import numpy a s np
import o s
from s k l e a r n . m o d e l _ s e l e c t i o n import t r a i n _ t e s t _ s p l i t
from s k l e a r n import m e t r i c s
from t e n s o r f l o w . k e r a s . models import S e q u e n t i a l
from t e n s o r f l o w . k e r a s . l a y e r s import Dense , A c t i v a t i o n
from t e n s o r f l o w . k e r a s . c a l l b a c k s import E a r l y S t o p p i n g

# C reate a t e s t / t r a i n s p l i t .

25% t e s t

556

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

i n t o t r a i n / t e s t

# S p l i t
x_train , x_test , y_train , y_ t e st = t r a i n _ t e s t _ s p l i t (
t e s t _ s i z e =0.25 , random_state =42)

x , y ,

# C r e a te n e u r a l n e t
model = S e q u e n t i a l ( )
model . add ( Dense ( 1 0 ,
model . add ( Dense ( 5 0 ,
model . add ( Dense ( 1 0 ,
model . add ( Dense ( 1 , k e r n e l _ i n i t i a l i z e r= ' normal ' ) )
model . add ( Dense ( y . shape [ 1 ] , a c t i v a t i o n= ' softmax ' ) )
model . compile ( l o s s= ' c a t e g o r i c a l _ c r o s s e n t r o p y ' , o p t i m i z e r= ' adam ' )
monitor = E a r l y S t o p p i n g ( monitor= ' v a l _ l o s s ' , min_delta=1e −3,
p a t i e n c e =5, v e r b o s e =1, mode= ' auto ' ,

input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )
input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )
input_dim=x . shape [ 1 ] , a c t i v a t i o n= ' r e l u ' ) )

model . f i t ( x_train , y_train , v a l i d a t i o n _ d a t a =(x_test , y _ te s t ) ,

c a l l b a c k s =[ monitor ] , v e r b o s e =2, e p oc h s =1000)

r e s t o r e _ b e s t _ w e i g h t s=True )

Output

. . .
11579/11579 − 22 s − l o s s : 0 . 0 1 3 9 − v a l _ l o s s : 0 . 0 1 5 3 − 22 s / epoch −
2ms/ s t e p
Epoch 19/1000
R e s t o r i n g model w e i g h t s from t h e end o f
11579/11579 − 23 s − l o s s : 0 . 0 1 4 1 − v a l _ l o s s : 0 . 0 1 5 2 − 23 s / epoch −
2ms/ s t e p
Epoch 1 9 : e a r l y s t o p p i n g

t h e b e s t epoch : 1 4 .

We can now evaluate the neural network. As you can see, the neural network achieves a 99% accuracy

rate.

Code

# Measure a c c u r a c y
pred = model . p r e d i c t ( x _t e s t )
pred = np . argmax ( pred , a x i s =1)
y_eval = np . argmax ( y_test , a x i s =1)
s c o r e = m e t r i c s . a c c u r a c y _ s c o r e ( y_eval , pred )
print ( " V a l i d a t i o n ␣ s c o r e : ␣ {} " . format ( s c o r e ) )

Output

14.5. PART 14.5: NEW TECHNOLOGIES

557

V a l i d a t i o n s c o r e : 0 . 9 9 7 7 0 0 5 1 6 5 7 4 0 9 3 5

14.5 Part 14.5: New Technologies

This course changes often to keep up with the rapidly evolving deep learning landscape. If you would like
to continue to monitor this class, I suggest following me on the following:

• GitHub - I post all changes to GitHub.
• Jeﬀ Heaton’s YouTube Channel - I add new videos for this class on my channel.

14.5.1 New Technology Radar

Currently, these new technologies are on my radar for possible future inclusion in this course:

• More advanced uses of transformers
• More Advanced Transfer Learning
• Augmentation
• Reinforcement Learning beyond TF-Agents

This section seeks only to provide a high-level overview of these emerging technologies. I provide links to
supplemental material and code in each subsection. I describe these technologies in the following sections.
Transformers are a relatively new technology that I will soon add to this course. They have resulted
in many NLP applications. Projects such as the Bidirectional Encoder Representations from Transformers
(BERT) and Generative Pre-trained Transformer (GPT-1,2,3) received much attention from practitioners.
Transformers allow the sequence to sequence machine learning, allowing the model to utilize variable
length, potentially textual, input. The output from the transformer is also a variable-length sequence. This
feature enables the transformer to learn to perform such tasks as translation between human languages
or even complicated NLP-based classiﬁcation. Considerable compute power is needed to take advantage
of transformers; thus, you should be taking advantage of transfer learning to train and ﬁne-tune your
transformers.

Complex models can require considerable training time. It is not unusual to see GPU clusters trained
for days to achieve state-of-the-art results. This complexity requires a substantial monetary cost to train
a state-of-the-art model. Because of this cost, you must consider transfer learning. Services, such as
Hugging Face and NVIDIA GPU Cloud (NGC), contain many advanced pretrained neural networks for
you to implement.

Augmentation is a technique where algorithms generate additional training data augmenting the training
data with new items that are modiﬁed versions of the original training data. This technique has seen many
applications in computer vision.
In this most basic example, the algorithm can ﬂip images vertically
and horizontally to quadruple the training set’s size. Projects such as NVIDIA StyleGAN3 ADA have
implemented augmentation to substantially decrease the amount of training data that the algorithm needs.
Currently, this course makes use of TF-Agents to implement reinforcement learning. TF-Agents is
convenient because it is based on TensorFlow. However, TF-Agents has been slow to update compared

558

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

to other frameworks. Additionally, when TF-Agents is updated, internal errors are often introduced that
can take months for the TF-Agents team to ﬁx. When I compare simple "Hello World" type examples
for Atari games on platforms like Stable Baselines to their TF-Agents equivalents, I am left wanting more
from TF-Agents.

14.5.2 Programming Language Radar

Python has an absolute lock on the industry as a machine learning programming language. Python is not
going anywhere any time soon. My main issue with Python is end-to-end deployment. Python will be your
go-to language unless you are dealing with Jupyter notebooks or training/pipeline scripts. However, you
will certainly need to utilize other languages to create edge applications, such as web pages and mobile
apps. I do not suggest replacing Python with any of the following languages; however, these are some
alternative languages and domains that you might choose to use them.

• IOS Application Development - Swift
• Android Development - Kotlin and Java
• Web Development - NodeJS and JavaScript
• Mac Application Development - Swift or JavaScript with Electron or React Native
• Windows Application Development - C# or JavaScript with Electron or React Native
• Linux Application Development - C/C++ w with Tcl/Tk or JavaScript with Electron or React

Native

14.5.3 What About PyTorch?

Technical folks love debates that can reach levels of fervor generally reserved for religion or politics. Python
and TensorFlow are approaching this level of spirited competition. There is no clear winner, at least at this
point. Why did I base this class on Keras/TensorFlow, as opposed to PyTorch? There are two primary
reasons. The ﬁrst reason is a fact; the second is my opinion.

PyTorch was not available in early 2016 when I introduced/developed this course.

PyTorch exposes lower-level details that would be distracting for applications of deep learning course.
I recommend being familiar with core deep learning techniques and being adaptable to switch between
these two frameworks.

14.5.4 Where to From Here?

So what’s next? Here are some ideas.

• Google CoLab Pro - If you need more GPU power; but are not yet ready to buy a GPU of your own.
• TensorFlow Certiﬁcation
• Coursera

I hope that you have enjoyed this course.
If you have any suggestions for improvement or technology
suggestions, please get in touch with me. This course is always evolving, and I invite you to subscribe to
my YouTube channel for my latest updates. I also frequently post videos beyond the scope of this course,
so the channel itself is a good next step. Thank you very much for your interest and focus on this course.
Other social media links for me include:

14.5. PART 14.5: NEW TECHNOLOGIES

559

• Jeﬀ Heaton GitHub
• Jeﬀ Heaton Twitter
• Jeﬀ Heaton Medium

560

CHAPTER 14. OTHER NEURAL NETWORK TECHNIQUES

Bibliography

[1] Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat,
S., Irving, G., Isard, M., et al. Tensorﬂow: A system for large-scale machine learning.
In
12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) (2016),
pp. 265–283.

[2] Barto, A. G., Sutton, R. S., and Anderson, C. W. Neuronlike adaptive elements that can solve
diﬃcult learning control problems. IEEE transactions on systems, man, and cybernetics, 5 (1983),
834–846.

[3] Cer, D., Yang, Y., Kong, S.-y., Hua, N., Limtiaco, N., John, R. S., Constant, N.,
Guajardo-Cespedes, M., Yuan, S., Tar, C., et al. Universal sentence encoder. arXiv preprint
arXiv:1803.11175 (2018).

[4] François, C. Deep learning with python, 2017.

[5] Fukushima, K. Neocognitron: A self-organizing neural network model for a mechanism of pattern

recognition unaﬀected by shift in position. Biological cybernetics 36, 4 (1980), 193–202.

[6] Gatys, L. A., Ecker, A. S., and Bethge, M. Image style transfer using convolutional neural
networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (2016),
pp. 2414–2423.

[7] Glorot, X., and Bengio, Y. Understanding the diﬃculty of training deep feedforward neural net-
works. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and statistics
(2010), pp. 249–256.

[8] Glorot, X., Bordes, A., and Bengio, Y. Deep sparse rectiﬁer neural networks. In Proceedings
of the fourteenth international conference on artiﬁcial intelligence and statistics (2011), pp. 315–323.

[9] Goodfellow, I., Bengio, Y., and Courville, A. Deep learning. MIT press, 2016.

[10] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,
Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information
processing systems (2014), pp. 2672–2680.

561

562

BIBLIOGRAPHY

[11] Heaton, J., McElwee, S., Fraley, J., and Cannady, J. Early stabilizing feature importance for
tensorﬂow deep neural networks. In 2017 International Joint Conference on Neural Networks (IJCNN)
(2017), IEEE, pp. 4618–4624.

[12] Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets.

Neural computation 18, 7 (2006), 1527–1554.

[13] Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation 9, 8 (1997),

1735–1780.

[14] Hornik, K., Stinchcombe, M., and White, H. Multilayer feedforward networks are universal

approximators. Neural networks 2, 5 (1989), 359–366.

[15] Howard, J., and Ruder, S. Universal language model ﬁne-tuning for text classiﬁcation. arXiv

preprint arXiv:1801.06146 (2018).

[16] Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training
generative adversarial networks with limited data. Advances in Neural Information Processing Systems
33 (2020), 12104–12114.

[17] Karras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2019),
pp. 4401–4410.

[18] Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., and Aila, T. Analyzing and

improving the image quality of stylegan. arXiv preprint arXiv:1912.04958 (2019).

[19] Kingma, D. P., and Ba, J. Adam: A method for stochastic optimization.

arXiv preprint

arXiv:1412.6980 (2014).

[20] LeCun, Y., Bengio, Y., et al. Convolutional networks for images, speech, and time series. The

handbook of brain theory and neural networks 3361, 10 (1995), 1995.

[21] LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. nature 521, 7553 (2015), 436–444.

[22] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and
Wierstra, D. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971
(2015).

[23] Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., and
Zitnick, C. L. Microsoft coco: Common objects in context. In European conference on computer
vision (2014), Springer, pp. 740–755.

[24] McCulloch, W. S., and Pitts, W. A logical calculus of the ideas immanent in nervous activity.

The bulletin of mathematical biophysics 5, 4 (1943), 115–133.

[25] Nair, V., and Hinton, G. E. Rectiﬁed linear units improve restricted boltzmann machines. In
Proceedings of the 27th international conference on machine learning (ICML-10) (2010), pp. 807–814.

BIBLIOGRAPHY

563

[26] Ng, A. Y. Feature selection, l 1 vs. l 2 regularization, and rotational invariance. In Proceedings of

the twenty-ﬁrst international conference on Machine learning (2004), p. 78.

[27] Olden, J. D., Joy, M. K., and Death, R. G. An accurate comparison of methods for quantifying
variable importance in artiﬁcial neural networks using simulated data. Ecological modelling 178, 3-4
(2004), 389–397.

[28] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. You only look once: Uniﬁed, real-time
object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition
(2016), pp. 779–788.

[29] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. Learning representations by back-

propagating errors. nature 323, 6088 (1986), 533–536.

[30] Saravia, E., Liu, H.-C. T., Huang, Y.-H., Wu, J., and Chen, Y.-S. Carer: Contextualized
In Proceedings of the 2018 conference on empirical

aﬀect representations for emotion recognition.
methods in natural language processing (2018), pp. 3687–3697.

[31] Simonyan, K., and Zisserman, A. Very deep convolutional networks for large-scale image recogni-

tion. arXiv preprint arXiv:1409.1556 (2014).

[32] Snoek, J., Larochelle, H., and Adams, R. P. Practical bayesian optimization of machine learning

algorithms. In Advances in neural information processing systems (2012), pp. 2951–2959.

[33] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.
Dropout: a simple way to prevent neural networks from overﬁtting. The journal of machine learning
research 15, 1 (2014), 1929–1958.

[34] Stevens, S. S. On the theory of scales of measurement. Science 103, 2684 (1946), 677–680.

[35] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł.,
and Polosukhin, I. Attention is all you need. Advances in neural information processing systems
30 (2017).

[36] Zhu, X., Lyu, S., Wang, X., and Zhao, Q. Tph-yolov5: Improved yolov5 based on transformer
prediction head for object detection on drone-captured scenarios. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (2021), pp. 2778–2788.

