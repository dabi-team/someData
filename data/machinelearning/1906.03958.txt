2
2
0
2

g
u
A
8

]

C
N
.
o
i
b
-
q
[

3
v
8
5
9
3
0
.
6
0
9
1
:
v
i
X
r
a

O R I G I N A L A R T I C L E

E d u c a t i o n

Parallel scalable simulations of biological neural
networks using TensorFlow: A beginner’s guide

Rishika Mohanta1

| Collins Assisi1

1Indian Institute of Science Education and
Research, Pune, Maharashtra, India

Correspondence
Rishika Mohanta and Collins Assisi
Indian Institute of Science Education and
Research, Pune - 411008, Maharashtra,
India
Email: neurorishika@gmail.com,
collins@iiserpune.ac.in

Funding information
Kishore Vaigyanik Protsahan Yojana (KVPY)
Fellowship, SB-1712051; DBT–Wellcome
India Alliance Intermediate Fellowship,
IA/I/11/2500290; IISER Pune

Biological neural networks are often modeled as systems

of coupled, nonlinear, ordinary or partial diﬀerential equa-

tions. The number of diﬀerential equations used to model

a network increases with the size of the network and the

level of detail used to model individual neurons and synapses.

As one scales up the size of the simulation, it becomes es-

sential to utilize powerful computing platforms. While many

tools exist that solve these equations numerically, they are

often platform-speciﬁc. Further, there is a high barrier of

entry to developing ﬂexible platform-independent general-

purpose code that supports hardware acceleration on mod-

ern computing architectures such as GPUs/TPUs and Dis-

tributed Platforms. TensorFlow is a Python-based open-

source package designed for machine learning algorithms.

However, it is also a scalable environment for a variety of

computations, including solving diﬀerential equations us-

ing iterative algorithms such as Runge-Kutta methods. In

this article and the accompanying tutorials, we present a

simple exposition of numerical methods to solve ordinary

diﬀerential equations using Python and TensorFlow. The

tutorials consist of a series of Python notebooks that, over

Abbreviations: CPU, central processing unit; GPU, graphical processing unit; ODE, ordinary diﬀerential equation.

1

NBDT} 
 
 
 
 
 
2

Mohanta & Assisi, 2022

the course of ﬁve sessions, will lead novice programmers

from writing programs to integrate simple one-dimensional

ordinary diﬀerential equations using Python to solving a

large system (1000’s of diﬀerential equations) of coupled

conductance-based neurons using a highly parallelized and

scalable framework. Embedded with the tutorial is a physio-

logically realistic implementation of a network in the insect

olfactory system. This system, consisting of multiple neu-

ron and synapse types, can serve as a template to simulate

other networks.

K E Y W O R D S

TensorFlow, Python, ODE, Jupyter Notebook, Neuronal network,

GPU

1 | MOTIVATION

Information processing in the nervous system spans a number of spatial and temporal scales [1]. Millisecond ﬂuc-

tuations in ionic concentration at a synapse can cascade into long-term (hours to days) changes in the behavior of

an organism. Capturing the temporal scales and the details of the dynamics of the brain is a colossal computational

endeavor. The dynamics of single neurons (modeled using one or a few compartments), and small networks of such

neurons, can be simulated on a desktop computer with high-level, readable programming languages like Python. How-

ever, large networks of conductance-based neurons are often simulated on clusters of CPUs. More recently, graphical

processing units (GPUs) have become increasingly available on individual workstations and from cloud services like

Google Colab/Cloud and Amazon Web Services (AWS), among others. GPUs typically have hundreds of cores, while

CPUs have relatively few (up to 80 at this writing) to execute any computation. Compute-intensive tasks that can

be parallelized may be oﬄoaded to the GPU. GPU cores, though many in number, are slower than their CPU coun-

terparts (see Fig. 1 for a comparison of CPUs and GPUs). Therefore, one has to judge whether the simulations can

be eﬃciently completed on a CPU or whether they would beneﬁt from GPU acceleration. Writing code for diﬀerent

platforms is nontrivial and requires a considerable investment of time to master diﬀerent software tools. For example,

implementing parallelism in multi-core shared-memory architectures is often achieved using Open Multi-Processing

(OpenMP) with C or C++ [2]. Message Passing Interface (MPI) libraries are used to implement code that computes

over high-performance computing clusters [3]. Compute Uniﬁed Device Architecture (CUDA) allows users to run pro-

grams on NVIDIA’s GPUs [4]. However, code written for one platform cannot be used on other platforms. This poses a

high barrier of entry for Neuroscientists conversant with a high-level programming language, attempting to test simu-

lations on diﬀerent platforms and scaling up a simulation. To lower this barrier, we created a well-documented tutorial

with clear example code to simulate neuronal networks in a platform-independent manner. This tutorial is available

in the form of Jupyter notebooks that can be downloaded or run online (https://github.com/neurorishika/PSST)

[5]

Our tutorial leverages TensorFlow, an open-source platform for machine learning [6] that is highly scalable. Code

written using TensorFlow functions can work seamlessly on single cores, multi-core shared memory processors, high-

Mohanta & Assisi, 2022

3

F I G U R E 1 Comparison between CPU and GPU architecture. ALUs (Arithmetic/Logical Units, i.e. CPU
Cores/CUDA Cores) are the main circuits that perform all the computations. CPUs (left panel) have fewer ALUs than
GPUs (right panel). But, at the same time, the CPU cores can run at a higher clock speed and can do more complex
calculations. On the other hand, GPU cores are specialized for linear algebra especially vector operations.
Furthermore, a single CPU processor has common memory and control units that must be refreshed every time a
new thread of operations is executed. In contrast, GPUs have a hierarchical structure with multiple memory and
control units, making them faster at executing multiple threads of operations. This makes GPUs better for simple but
highly repetitive tasks executed in parallel, while the CPU is suitable for large complex computations executed
serially. However, GPUs have one drawback, external memory (RAM) is typically larger, faster and more robust than
internal GPU memory, which is why computation on the GPU is constrained by the amount of data used/generated
more often than on CPUs.[5]

performance computer clusters, and GPUs. We found (as others have [7, 8]), TensorFlow functions can be used to

implement numerical methods to solve ODEs. Doing so gave us a signiﬁcant speed-up even on a single desktop with

a multi-core processor compared to similar Python code that did not use TensorFlow functions and operated on a

single core (Fig 2). The code itself was highly readable and could be debugged with ease. Familiarity with the Python

programming language and a brief introduction to some TensorFlow functions proved suﬃcient to write the code.

Python is a popular programming language that is used across a number of disciplines and has found a broad user

base among Biologists [9, 10, 11]. We found that introducing a few TensorFlow functions in Python, an easy addition

to a familiar language can bring readers to a point where they can simulate large networks of neurons in a platform-

independent manner. Further, by piggybacking on TensorFlow, we could also take advantage of an active TensorFlow

developer community and a wide range of Python libraries.

The tutorials that accompany this paper [5] were written to address the needs of a group of undergraduate

students in our institute. These students came from diverse backgrounds and had a basic introduction to Python

during their ﬁrst semester. Some were interested in working on problems in Computational Neuroscience. Our goal

was to introduce them to some of the numerical tools and mathematical models in Neuroscience while also allowing

them to tinker with advanced projects. Therefore, we were careful to keep the innards of the code visible —this

included the form of the integrator and the speciﬁcation of the diﬀerential equations. One could argue that our

implementation rested upon an extensive software library, namely, TensorFlow, whose workings remained mysterious

to a beginning programmer. In coming up with this tutorial, we decided that it was important to get students to engage

4

Mohanta & Assisi, 2022

F I G U R E 2 Comparison of the Matrix Multiplication and Numerical Integration performance of NumPy (CPU
only), TensorFlow CPU and TensorFlow GPU. TensorFlow GPU is much faster in (a) Matrix multiplication of N × N
matrices and (b) RK4-based Numerical Integration of parallel ODEs for large systems and TensorFlow CPU is
marginally better than NumPy possibly due to better optimization for parallel computing. Note that here all the
equations are independent, but in real systems, they will have interactions for which the computations will be faster
on the GPU than the CPU if properly vectorized. Thus, the estimates for the improvement in RK4 integration on
TensorFlow GPU are likely underestimated for many practical tasks. Median runtimes were estimated across 20
replicates running on a Google Colab Instance with 1 core, 2 thread 2.30 GHz Intel(R) Xeon(R) CPU and Nvidia a
single Tesla K80 GPU with 12 GB GDDR5 Memory.

with actually integrating ODEs and further put them in a position to examine a whole class of problems that require

iterative computation in a platform-independent manner. This is not a goal that current simulation environments

seek to achieve. We anticipate that the code described here will serve as a starting point to simulate ODEs and

could potentially include more sophisticated and faster integration algorithms and methods to manage limitations

imposed by memory. We hope that the utility of this tutorial will extend beyond the test cases and the domains we

consider here. Towards the end of the tutorial, that many students managed to complete within a day or two, they

were in a position to write codes simulating networks of neurons in the antennal lobe [12] (the insect equivalent of

the olfactory bulb in mammals), ﬁring rate models of grid cells [13], detailed networks of stellate cells and inhibitory

interneurons [14] and networks with plastic synapses [15].

2 | HOW TO USE THE TUTORIALS

This paper and the accompanying tutorials may be used in a classroom setting to supplement the material taught

in a Computational Neuroscience or Mathematical Biology course, where integrating ordinary diﬀerential equations

in diﬀerent contexts would be a core learning objective. We believe that the tutorial may also be appropriate for

self-study by students who are familiar with the Python programming language and popular libraries such as NumPy

ab10-11001011021031041051060.010.1110Time taken for RK4 Integration(Lower is Better)Number of Equations (N)MedianRuntime(insecs)NumPy 1.19.5TensorFlow 2.6.0 (CPU)TensorFlow 2.6.0 (GPU)20222426282102122140.000010.00010.0010.010.1110100Time taken for NxN Matrix Multiplication(Lower is Better)Matrix Size(N)MedianRuntime(insecs)NumPy 1.19.5TensorFlow 2.6.0 (CPU)TensorFlow 2.6.0 (GPU)Mohanta & Assisi, 2022

5

F I G U R E 3 Flowchart of the tutorial [5] accompanying this paper

and Matplotlib that are used to represent some of the data structures and plot the results. These libraries are well

documented with excellent introductory guides. Fig. 3 provides an overview of the tutorials. Readers who are inter-

ested in solving diﬀerential equations in other domains will ﬁnd the tutorial on Days 1, 2, and 5 self-contained. On

days 3 and 4, we introduce conductance based neurons using the Hodgkin-Huxley formalism. Our introduction to the

Hodgkin-Huxley model is terse and will beneﬁt from supplementary classroom instruction or from self-study using

one of several excellent guides on the topic ([16, 17, 18]). The tutorials are linked in the Supporting information and

are available as Jupyter notebooks (.ipynb ﬁles). The notebooks can be viewed or run online using Binder, Google

Colab or Kaggle Notebook. The respective links are available in each notebook. An online version of the tutorials is

also available as a JupyterBook linked in the supporting information. Please enable GPU usage on Google Colab and

Kaggle Notebooks for the best performance. We also provide .html ﬁles that can be read using any browser. To run

the notebooks locally, we recommend that readers install Python 3.6 or above, Jupyter Notebook, NumPy 1.20.or

above [19], Matplotlib 3.4 or above [20], and TensorFlow 2.8 or above using the Anaconda distribution of Python 3.9.

We suggest that users utilize TensorFlow 2.x with eager execution disabled for optimal stability and performance.

DAY 1Numerical Integration in Python using NumPy- Euler’s Method- Runge Kutta (Order 4) Method (RK4) DAY 2Numerical Integration usingTensorFlow- Introduction to Computational Graphs- Using tf.scan for efficient recursion- Euler and RK4 in TensorFlow DAY 5Dealing with Limited Memory- Minimizing memory usage during a single program run- Splitting the simulation into batches DAY 3Simulating a Single RealisticNeuron- Introduction to Hodgkin HuxleyNeuron Model (HH Model)- Simulating HH Neurons in TensorFlow- Efficiently simulating multiple independentneurons parallely DAY 4Simulating Synapses and Networks of Neurons- Modeling different types of Synapses- Using and storing synaptic information efficiently in the memory- Simulating a network of interacting neuronsExample:A Model of the Locust Brain- Implementing a model of the AntennalLobe (AL) in the Locust Olfactory System described in Bazhenov (2001) 6

Mohanta & Assisi, 2022

3 | COMPUTATIONAL GRAPHS AND TENSORFLOW

TensorFlow is an open-source library developed by researchers and engineers in the Google Brain team. TensorFlow

has a number of functions that make it particularly suitable for machine learning applications. However, it is primarily

an interface for numerical computation [6]. All computations in TensorFlow are speciﬁed as computational graphs.

Computational graphs, also known as data ﬂow graphs, are directed graphs (nodes connected by arrows) where the

nodes represent operations (for example, addition or multiplication), while incoming edges to each node represent

the data stored as tensors (scalars, vectors, matrices, and higher-dimensional arrays) - these are the actual values that

are operated upon. The output of the computation is also a tensor. For example, consider the following computation

where two vectors a and b serve as inputs to the node, a matrix multiplication operation, that produces a matrix c as

output (Fig 4a).

The following program implements the computation described in Fig 4a.

# Creating nodes in the computation graph

a = tf.constant([[1.],[2.],[3.]], dtype=tf.float64) # a 3x1 column matrix

b = tf.constant([[1.,2.,3.]], dtype=tf.float64) # a 1x3 row matrix

c = tf.matmul(a, b)

# To run the graph, we need to create a session.

# Creating the session initializes the computational device.

with tf.Session() as sess:

output = sess.run(c)

print(output)

1

2

3

4

5

6

7

8

9

Any computation may be thus deﬁned as a sequence of TensorFlow operations acting on tensors. The ﬁnal

output is computed by passing the data through the directed edges of the computational graph. By specifying the

computational graph, we also specify the dependencies between operations. One can thus split the graph into smaller

chunks or sub-graphs that can be independently computed by diﬀerent devices that coordinate with each other.

Consider, for example, the computational graph shown in Fig 4b. Here, each node represents an operation. Assume

that the time taken to complete each operation is the same, and that we have two processors available to complete

the computation. As the computation progresses, some operations may be scheduled in parallel. The computational

graph states the dependency between operations, (operations E and J can only be completed once C and D, and H

and I are complete). This allows us to eﬃciently schedule operations such that the entire program can be completed
in the least number of time units (in Fig 4b, the number of operations is 6 1). Further, common sub-graphs, if any,

may be eliminated to speed up the computation. Computational graphs make it possible to develop programs that

are fast, device-independent, and scalable across CPUs, GPUs, and clusters of servers. These graphs are not unique

to TensorFlow. Other machine learning frameworks such as PyTorch and Theano also utilize a similar system in the

background. Therefore, the concepts developed in this tutorial can also be translated to other frameworks.

4 | BRAIN SIMULATIONS AS COMPUTATIONAL GRAPHS

Days 1 and 2 of the tutorial develop iterative methods to integrate ordinary diﬀerential equations (ODEs). In particular,

we present simple implementations of Runge-Kutta methods of order one and four and elaborate on some of the

1Example
https://www.cs.princeton.edu/ dpw/courses/cos326-12/notes/parallel-schedules.php

Functional

lecture

course

based

notes

on

on

of

a

Programing

by

David Walker,

Princeton

University,

Mohanta & Assisi, 2022

7

F I G U R E 4 Computational graphs for TensorFlow. a. Example of a simple computational graph to multiply two
vectors a and b and store it in c. b. A simple computational graph is shown on the left. An eﬃcient schedule (right)
to compute the operations assuming each operation takes the same time and two processors are available. The table
on the right shows the time steps at which speciﬁc operations are completed. c. A for loop implemented as a
computational graph. d. A schematic showing the diﬀerent steps involved in implementing the Euler integrator as a
computational graph to solve an initial value problem of the form d y
= f (y , t ) given an initial condition y0 (left top),
d t
a time vector t (left bottom) and the function f (y , t ) (bottom). The graph for f (y , t ) is fed to the euler_step
function which is in turn fed to the tf.scan function along with the time-step vector ∆t and the initial condition y0.
tf.scan iterated recursively over the time diﬀerence and calculates the integrated time-evolved output y .

dcabABCJIHGFEDtimeoperation216543FE, JH, IC, DB, GAΔtn-1+yn-1tn-1fxΔt2+y2t2fxΔt1yn+y1t1fx8

Mohanta & Assisi, 2022

peculiarities associated with implementing these recursive methods using TensorFlow. An example of ODEs at the

core of our tutorial are the Hodgkin-Huxley equations that describe the dynamics of action potential generation and

propagation in the giant axon of the squid [21]. The Hodgkin-Huxley equations have kindled a revolution in our

understanding of brain function, touching multiple scales of organization, ranging from the dynamics of ion channels

to the collective behavior of networks of neurons (see [22] for a review). Alan Hodgkin and Andrew Huxley arrived at

these equations through a series of clever experiments that tested the limits of the technology available at the time.

It also tested the limits of computational tools available. In order to compute action potentials, Huxley numerically

integrated the equations using a hand-operated Bunsviga mechanical calculator. The calculation took nearly three

weeks to complete [23]. On day 3 of the tutorial, we use a fourth-order Runge-Kutta integrator (implemented on day

2 of the tutorial) to integrate the Hodgkin-Huxley equations. Further, since much of our implementation is geared

towards simulating networks of neurons, the tutorial dwells on conductance-based neuronal models and provides

speciﬁc pointers on how to integrate the diﬀerential equations describing neuronal networks eﬃciently (days 3 and

4). Consider an initial value problem of the form,

d y
d t

= f (y , t )

with

y (t 0) = y0

(1)

where, y is an N −dimensional vector and t typically stands for time. The function f (y , t ) may be a nonlinear function
of y that explicitly depends on t . Euler’s method iteratively solves eqn (1). Here we start from y (t = t 0) = y0 and
compute the solution at subsequent time points (t 0 +∆t , t 0 +2∆t , t 0 +3∆t . . . ). The solution at each step can be derived
by truncating a Taylor series after the ﬁrst term. That is, the solution at time t 0 + ∆t is given by,

y (t 0 + ∆t ) = y0 + ∆t

d y
d t

+ O (∆t

2)

(2)

where d y
d t

= f (y , t ). The higher order terms O (∆t 2) are ignored in this approximation.

Euler integration is essentially a recursive process over time-series [t 0, t 1, t 2, . . . t n ]. The eqn (2) thus can be

written as a recursive function F such that Xi +1 = F (Xi , ti , (cid:15)i ). The ﬁnal solution of the ODE becomes
[X0, F (X0, t 0, (cid:15)0), F (F (X0, t 0, (cid:15)0), t 1, (cid:15)1)...]. In Python, this recursive function can be implemented using a "for" loop.

However, implementing a similar loop in TensorFlow would result in a computational graph consisting of a long chain

of sub-graphs, each sub-graph representing a single iteration of the loop (Fig 4c ). Note, each box (demarcated by

the dashed lines) represents a single step of a ﬁrst order Euler integrator. The integrator itself is detailed in Fig 4d.

The size of this computational graph increases with the number of iterations and results in a memory-intensive way

to compute a recursive function. TensorFlow provides a more eﬃcient solution in the form of the function tf.scan

[7] to iterate over the time series. tf.scan takes in 3 inputs: (i) a recursive function (ii) the list to iterate over and (iii)

the initial value. If the initial value is not speciﬁed, it uses the ﬁrst element of the list as an initial value. For example,

consider the following program that calculates the cumulative sum over a list. Every step involves adding an element

from the list onto the last addition.

1

2

3

4

# define the recursive function that takes the accumulated

# value and the additional input from a list.

def recursive_addition(sum_till_now,next_value):

return sum_till_now + next_value

Mohanta & Assisi, 2022

9

5

6

7

8

9

10

11

12

13

# define the list over which we iterate

elems = np.array([1, 2, 3, 4, 5, 6])

# accumulate with the starting number 5

cum_sum = tf.scan(recursive_addition, elems, tf.constant(5))

with tf.Session() as sess:

output = sess.run(cum_sum)

print(output)

# This prints :

#[ 6 8 11 15 20 26]

We can use the same principle to create a recursive function that calculates each step in Euler’s method (see

accompanying tutorial [5] for the implementation). The computational graph in Fig 4c and d is a schematic showing

the data ﬂow in Euler’s method. Here the function is called recursively to compute and accumulate the value of the
yi N ]T at time ti . TensorFlow includes a visualization tool, TensorBoard, that may be
state variable [yi 1
used to create a data ﬂow graph. The graph visualized in Fig 4d was not generated using this tool but is a schematic

yi 2

. . .

that outlines the steps involved in computing the solution of (1) using Euler’s method. In addition to Euler’s method,

the tutorial ([5]) implements the Runge-Kutta method of order 4 (abbreviated as RK4). While the error per iteration
is O (∆t 5) in the RK4 method, it requires additional computations to calculate the value of the solution at each time
step.

Each iteration operates upon multiple tensors and computes a solution. TensorFlow has several built-in functions

that speed up Tensor computations using available multi-core CPUs and GPU hardware. Two components of the code

where a signiﬁcant speed-up can be achieved are,

1.

Iterations of Numerical Integration: TensorFlow can be set to implement numerical integration algorithms (Eu-

ler/RK4) as computational graphs that are compiled and run eﬃciently (see Day 1 of the accompanying tutorial

[5]).

2. Vectorized Functional Evaluations: Converting loops into array operations is often termed ‘vectorization’. Array

operations are computed by highly optimized functions and are, as a result, nearly an order of magnitude faster

to evaluate. The form of the equations that describe the neural dynamics are similar across neurons even though

the speciﬁc parameters may vary. A large number of such equations may thus be vectorized, eliminating lengthy

for loops (see Day 3 of the accompanying tutorial [5]).
Say (cid:174)X = [V , m, n, h ] is the state vector of a single neuron and its dynamics are deﬁned using parameters Cm, gK , ...EL .
The equations governing the dynamics of the system is of the form:

d (cid:174)X
d t

= [f1 ( (cid:174)X , Cm, gK , ...EL ), f2 ( (cid:174)X , Cm, gK , ...EL )...fm ( (cid:174)X , Cm, gK , ...EL ) ]

(3)

We can convert these equations to a form in which some evaluations are done as array operations and not as

for loops. Despite the parameters being diﬀerent, the functional forms of the equations are similar for the same

state variable of diﬀerent neurons. Thus, the trick is to reorganize X as
X(cid:48) = [ (V1,V2, ...VN ), (m1, m2, ...mN ), (h1, h2, ...hN ), (n1, n2, ...nN ) ] = [ (cid:174)V , (cid:174)m, (cid:174)h, (cid:174)n ]. And the parameters as [ (cid:174)Cm, (cid:174)gK ] =
[Cm1 . . . CmN , gK1 . . . gKN ] and so on.

10

Mohanta & Assisi, 2022

The advantage of this re-ordering is that the system of diﬀerential equations of the form,

dVi
d t

= f (Vi , mi , hi , ni , Cmi , gKi ...)

where i = 1, ..., N , can be rewritten in vector form as,

The ‘vectorized’ equations now read,

d (cid:174)V
d t

= f ( (cid:174)V , (cid:174)m, (cid:174)h, (cid:174)n, (cid:174)Cm, (cid:174)gK ...)

d X(cid:48)
d t

= (cid:104) d (cid:174)V
d t

,

d (cid:174)m
d t

,

d (cid:174)h
d t

,

(cid:105)

d (cid:174)n
d t

(4)

(5)

(6)

These equations may then be solved eﬃciently using highly optimized, in-built array operations.

5 | CIRCUMVENTING MEMORY CONSTRAINTS

Modern architectures such as GPUs/TPUs allow several parallel computations. However, GPU/TPUs are also subject

to certain constraints that need to be circumvented to use them eﬀectively. One such constraint is that of memory.

In order to allow extensive parallelization, GPUs/TPUs are equipped with high bandwidth but smaller memory than

traditional CPUs, a trade-oﬀ between memory size and speed due to cost constraints (Fig. 2). Further, as the GPU is

managed by a driver and not directly by the operating system, it also faces memory scheduling constraints that reduce

the eﬀectiveness of dynamic memory allocation.

Using Python and TensorFlow allows us to write code that is readable, parallelizable, and scalable across a variety

of computational devices. However, given the potential scale of our problem (integrating large networks of neurons),

our implementation is memory intensive. The iterators in TensorFlow do not follow the normal process of memory

allocation and garbage collection. Since TensorFlow is designed to work on diverse hardware like GPUs and distributed

platforms, memory allocation is done adaptively during the TensorFlow session and not cleared until the Python kernel

has stopped execution. To deal with these issues, we need to carefully optimize how we build our computational

graphs in diﬀerent ways:

1. Optimize graphs to minimize memory required: In a network with n neurons, there are at most n 2 synapses
of each type. The actual number of synapses may be much smaller. Arranging synaptic variables as an n × n

matrix is computationally eﬃcient as it reduces the computation at each step to a matrix multiplication that is

optimized in TensorFlow. However, storing an n × n matrix is memory intensive. In many neural networks, where
synaptic connections are few in number, several of these n 2 elements will be set to zero. Therefore, we store
only the non-zero elements of the n × n matrix, also termed a sparse representation. This is eﬃcient when

updating synaptic variables do not require array operations. However, when array operations, such as matrix

multiplications, become necessary in order to eliminate time-consuming for loops, we switch from a sparse to

a dense matrix representation (Fig. 5). This makes the implementation of the RK4 integrator memory eﬃcient

while also minimizing computation time. (see Day 4 of the tutorial for details [5])

Mohanta & Assisi, 2022

11

F I G U R E 5 Dense and sparse representations of Synaptic variables. Often, when looking at realistic network of
neurons, not all neurons are connected to each other, i.e. every neuron forms a synapse with a small fraction of
other neurons. Thus, the values of any synaptic variables can be represented as either a dense or sparse variable
both of which oﬀer diﬀerent beneﬁts. Consider the example of a network with n neurons. A dense representation
will have n 2 values many of which will be 0, while it will take a larger amount of memory to store it, it can be used to
write synaptic updates as matrix equations which can be sped up using TensorFlow. However, since memory is a
limited resource for computations on a GPU, when the dense representation is not needed, it becomes more
eﬃcient to store the synaptic variables as a sparse representation which will consume only as much memory as the
number of synapses.

2. Split execution into batches: The maximum memory used by the computational graph is approximately two times

the size of the solution matrix when the computation ﬁnishes and copies the ﬁnal data into the memory. Larger

network sizes and longer simulation times result in larger solution matrices. For any given network, the maximum

simulation length is limited by the memory available. One way to increase the maximum length is to divide the

simulation into smaller batches. The queuing time between batches adds some overhead that slows down our

code but allows longer simulation times. Further, memory is cleared only when the Python kernel is closed. By

calling a Python script iteratively to execute successive batches of the simulation, we can achieve indeﬁnitely

long simulations while optimizing the usage of the available memory.

3. Disabling eager execution: Earlier versions of TensorFlow used a static computational graph that had to be com-

piled before the simulation was run. This allowed TensorFlow to determine redundancies in the graph and opti-

mize scheduling various computations assigned to diﬀerent cores. TensorFlow v2.0 introduced eager execution

which allows dynamic memory allocation and access without explicitly having to create and compile computa-

tional graphs in a session. Eager execution is enabled by default in TensorFlow v 2.0. While this is more intuitive

and user-friendly than deﬁning computational graphs, we found that it predictably slowed down our simulations.

We were able to circumvent this issue by disabling eager execution in TensorFlow v2.0.

6 | CONCLUSION

In this paper and accompanying tutorial, our attempt was to delve into the practical question, how does one simu-

late a network of neurons in a platform-independent manner? There are several simulation packages that simulate

brain dynamics at various scales ranging from the sub-cellular [24, 25] to cellular [24, 25, 26, 27, 28, 29] and inter-

areal networks [30]. These toolboxes perform with remarkable eﬃciency and employ sophisticated user interfaces.

However, switching platforms, when possible, requires re-writing some components of the code and incorporating

diﬀerent libraries [31, 32]. The degree of diﬃculty in achieving this can vary across toolboxes. At this point, it is

12

Mohanta & Assisi, 2022

clear that horizontal scalability remains a problem that will require signiﬁcant changes to the code written for single

and multi-core CPUs to be deployed on multi-node HPCs and GPUs. Simulation packages provide sophisticated user

interfaces that hide the actual computations (numerical integrators and support functions) to allow the end-user to

focus on the scientiﬁc question at hand. As the suite of features implemented in diﬀerent simulation environments

increases, it becomes diﬃcult to grasp the details of the implementation comprehensively. Our approach, in contrast,

was to minimize the implementation to only those elements that were necessary to simulate a network of neurons

eﬃciently. In doing so, we found that the entirety of the code became accessible to novice programmers who could

progress from understanding the basics of numerical integration to actually implementing large scale simulations in a

platform-independent manner using TensorFlow.

In recent years, there have been several advances in the hardware available for large-scale computing. In 2016,

Google announced a new hardware architecture known as TPUs (Tensor Processing Units) that were speciﬁcally de-

signed for use with the TensorFlow library. TPUs utilize a specialized Matrix Multiplication Unit that maximizes parallel

computation and are capable of much faster matrix multiplication than CPUs or GPUs. As a result, TPUs can further

speed up the the integration of ODEs and other numerical simulations. While, the code we presented in this tutorial

can be run on a system with available TPUs, the allocation of tasks to diﬀerent TPU cores is not currently automated

and must be done manually. And so, using TPUs eﬃciently may require a few additional steps which are outside the

scope of this tutorial. However, as TPUs are becoming more popular, it is likely that future versions of the TensorFlow

library will be able to automatically allocate tasks to diﬀerent TPU cores.

supporting information

The code in this tutorial is implemented as a series of Jupyter notebooks that can be found in the following GitHub

repository: https://github.com/neurorishika/PSST or OSF archive http://doi.org/10.17605/OSF.IO/YBZKQ

. It is also available as an online book at the following link: https://neurorishika.github.io/PSST.

acknowledgments

RM received a KVPY fellowship SB-1712051 and support from IISER Pune. CA was funded by DBT–Wellcome India

Alliance through an Intermediate fellowship IA/I/11/2500290 and IISER Pune. We thank members of the Assisi

and Nadkarni labs at IISER Pune and several students who tested the code. We thank Prof. Maxim Bazhenov for

discussions and code related to the insect antennal lobe model. We also acknowledge the National Supercomputing

Mission (NSM) for providing computing resources of ‘PARAM Brahma’ at IISER Pune, which is implemented by C-

DAC and supported by the Ministry of Electronics and Information Technology (MeitY) and Department of Science

and Technology (DST), Government of India.

conﬂict of interest

Authors declare no competing interests.

references

[1] Churchland PS, Sejnowski TJ.
Nov;242(4879):741–745.

Perspectives on cognitive neuroscience.

Science (New York, NY) 1988

Mohanta & Assisi, 2022

13

[2] Chapman B, Jost G, Van der Pas R. Using OpenMP. Portable shared memory parallel programming. The MIT Press; 2007.

[3] Gropp W, Lusk E, Skjellum A. Using MPI. Portable programming with the message passing interface. 3 ed. The MIT

Press; 2014.

[4] Storti D, Yurtoglu M. CUDA for Engineers: An introduction to High-Performance Parallel Computing. 1 ed. Pearson

Education India; 2016.

[5] Mohanta R, Assisi C. A Tutorial for Parallelised Scalable Simulations in TensorFlow; 2019, http://doi.org/10.17605/

OSF.IO/YBZKQ.

[6] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, et al., TensorFlow: Large-Scale Machine Learning on Hetero-

geneous Systems; 2015. https://www.tensorflow.org/, software available from tensorﬂow.org.

[7] TensorFlow API docs;. www.tensorflow.org/api_docs/python.

[8] McClure N. 11. In: TensorFlow Machine Learning Cookbook Packt Publishing; 2018. .

[9] Ekmekci B, McAnany CE, Mura C. An Introduction to Programming for Bioscientists: A Python-Based Primer. PLoS

Computational Biology 2016;12(6):1–43.

[10] Bassi S. A Primer on Python for Life Science Researchers. PLoS Computational Biology 2007;.

[11] Langtangen Hans P. A Primer on Scientiﬁc Programming with Python, vol. 6 of Texts in Computational Science and

Engineering. Springer; 2009.

[12] Bazhenov M, Stopfer M, Rabinovich M, Huerta R, Abarbanel HD, Sejnowski TJ, et al. Model of transient oscillatory

synchronization in the locust antennal lobe. Neuron 2001;30(2):553–567.

[13] Burak Y, Fiete IR. Accurate path integration in continuous attractor network models of grid cells. PLoS Computational

Biology 2009;5(2).

[14] Neru A, Assisi C. Theta oscillations gate the transmission of reliable sequences in the medial entorhinal cortex. eNeuro

2021;8(3):1–18.

[15] Bazhenov M, Stopfer M, Sejnowski TJ, Laurent G. Fast odor learning improves reliability of odor responses in the locust

antennal lobe. Neuron 2005;46(3):483–492.

[16] Neuronal dynamics;. https://www.edx.org/course/neuronal-dynamics.

[17] Johnston D, Wu SMS. Foundations of cellular neurophysiology. MIT Press; 1995.

[18] Dayan P, Abbott LF. Theoretical Neuroscience - Computational and Mathematical Modeling of Neural Systems. MIT

Press; 2005.

[19] NumPy package for scientiﬁc computing;. https://www.numpy.org/.

[20] Matplotlib Python plotting library;. https://matplotlib.org/.

[21] Huxley AL, Hodgkin AF. Quantitative description of nerve current. Journal of Physiology 1952;.

[22] Catterall WA, Raman IM, Robinson HPC, Sejnowski TJ, Paulsen O. The Hodgkin-Huxley Heritage: From Channels to

Circuits. Journal of Neuroscience 2012;32(41):14064–14073.

[23] Hodgkin AL. Chance and design in electrophysiology: An informal account of certain experiments on nerve carried out

between 1934 and 1952. Journal of Physiology 1976;263:1–21.

14

Mohanta & Assisi, 2022

[24] Ray S, Bhalla U. PyMOOSE: interoperable scripting in Python for MOOSE. Frontiers in Neuroinformatics 2008;2:6.

https://www.frontiersin.org/article/10.3389/neuro.11.006.2008.

[25] Carnevale NT, Hines ML. The NEURON Book. Cambridge University Press; 2006.

[26] Bower JM, Beeman D. The Book of GENESIS: Exploring realistic neural models with the GEneral NEural SImulation

System. 2 ed. New York, NY: Springer New York; 1998.

[27] Gewaltig MO, Diesmann M. NEST (NEural Simulation Tool). Scholarpedia 2007;2(4):1430.

[28] Stimberg M, Brette R, Goodman DFM. Brian 2, an intuitive and eﬃcient neural simulator. eLife 2019;8:1–41.

[29] Goodman D, Brette R. The Brian simulator. Frontiers in Neuroscience 2009;3:26.

[30] Sanz Leon P, Knock S, Woodman M, Domide L, Mersmann J, McIntosh A, et al. The Virtual Brain: a simulator of primate

brain network dynamics. Frontiers in Neuroinformatics 2013;7:10.

[31] Kumbhar P, Hines M, Fouriaux J, Ovcharenko A, King J, Delalondre F, et al. CoreNEURON : An Optimized Compute

Engine for the NEURON Simulator. Frontiers in Neuroinformatics 2019;13(September).

[32] Stimberg M, Goodman DFM, Nowotny T. Brian2GeNN: accelerating spiking neural network simulations with graphics

hardware. Scientiﬁc Reports 2020;10(1):1–12.

