AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

Shi Yu 1 Yuxin Chen 1 Hussain Zaidi 1

0
2
0
2

b
e
F
7
1

]
L
C
.
s
c
[

1
v
7
8
9
4
0
.
3
0
0
2
:
v
i
X
r
a

Abstract

We develop a chatbot using Deep Bidirectional
Transformer models (BERT) (Devlin et al., 2019)
to handle client questions in ﬁnancial investment
customer service. The bot can recognize 381 in-
tents, and decides when to say I don’t know and
escalates irrelevant/uncertain questions to human
operators. Our main novel contribution is the
discussion about uncertainty measure for BERT,
where three different approaches are systemati-
cally compared on real problems. We investigated
two uncertainty metrics, information entropy and
variance of dropout sampling in BERT, followed
by mixed-integer programming to optimize de-
cision thresholds. Another novel contribution
is the usage of BERT as a language model in
automatic spelling correction. Inputs with acci-
dental spelling errors can signiﬁcantly decrease
intent classiﬁcation performance. The proposed
approach combines probabilities from masked lan-
guage model and word edit distances to ﬁnd the
best corrections for misspelled words. The chat-
bot and the entire conversational AI system are
developed using open-source tools, and deployed
within our company’s intranet. The proposed ap-
proach can be useful for industries seeking similar
in-house solutions in their speciﬁc business do-
mains. We share all our code and a sample chatbot
built on a public dataset on Github.

1. Introduction

Since their ﬁrst appearances decades ago (Weizenbaum,
1966) (Colby et al., 1971), Chatbots have always been
marking the apex of Artiﬁcial Intelligence as forefront of
all major AI revolutions, such as human-computer interac-
tion, knowledge engineering, expert system, Natural Lan-
guage Processing, Natural Language Understanding, Deep
Learning, and many others. Open-domain chatbots, also
known as Chitchat bots, can mimic human conversations

*Equal contribution 1The Vanguard Group, Malvern, PA, USA.

Correspondence to: Shi Yu <shi.yu@hotmail.com>.

Under Review, Copyright 2020 by the authors.

to the greatest extent in topics of almost any kind, thus
are widely engaged for socialization, entertainment, emo-
tional companionship, and marketing. Earlier generations
of open-domain bots, such as Mitsuku(Worswick, 2019)
and ELIZA(Weizenbaum, 1966), relied heavily on hand-
crafted rules and recursive symbolic evaluations to capture
the key elements of human-like conversation. New advances
in this ﬁeld are mostly data-driven and end-to-end systems
based on statistical models and neural conversational models
(Gao et al., 2018) aim to achieve human-like conversations
through more scalable and adaptable learning process on
free-form and large data sets (Gao et al., 2018), such as
MILABOT(Serban et al., 2017), XiaoIce(Zhou et al., 2018),
Replika(Fedorenko et al., 2017), Zo(Microsoft, 2019), and
Meena(Adiwardana et al., 2020).

Unlike open-domain bots, closed-domain chatbots are de-
signed to transform existing processes that rely on human
agents. Their goals are to help users accomplish speciﬁc
tasks, where typical examples range from order placement
to customer support, therefore they are also known as task-
oriented bots (Gao et al., 2018). Many businesses are ex-
cited about the prospect of using closed-domain chatbots to
interact directly with their customer base, which comes with
many beneﬁts such as cost reduction, zero downtime, or no
prejudices. However, there will always be instances where a
bot will need a humans input for new scenarios. This could
be a customer presenting a problem it has never expected
for (Larson et al., 2019), attempting to respond to a naughty
input, or even something as simple as incorrect spelling. Un-
der these scenarios, expected responses from open-domain
and closed-domain chatbots can be very different: a success-
ful open-domain bot should be ”knowledgeable, humourous
and addictive”, whereas a closed-domain chatbot ought to
be ”accurate, reliable and efﬁcient”. One main difference
is the way of handling unknown questions. A chitchat bot
would respond with an adversarial question such as Why do
you ask this?, and keep the conversation going and deviate
back to the topics under its coverage (Sethi, 2019). A user
may ﬁnd the chatbot is out-smarting, but not very helpful in
solving problems. In contrast, a task-oriented bot is scoped
to a speciﬁc domain of intents, and should terminate out-of-
scope conversations promptly and escalate them to human
agents.

This paper presents AVA (A Vanguard Assistant), a task-

 
 
 
 
 
 
AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

Figure 1. End-to-end conceptual diagram of AVA

oriented chatbot supporting phone call agents when they
interact with clients on live calls. Traditionally, when phone
agents need help, they put client calls on hold and consult
experts in a support group. With a chatbot, our goal is to
transform the consultation processes between phone agents
and experts to an end-to-end conversational AI system. Our
focus is to signiﬁcantly reduce operating costs by reduc-
ing the call holding time and the need of experts, while
transforming our client experience in a way that eventually
promotes client self-provisioning in a controlled environ-
ment. Understanding intents correctly and escalating irrel-
evant intents promptly are keys to its success. Recently,
NLP community has made many breakthroughs in context-
dependent embeddings and bidirectional language models
like ELMo, OpenAI, GPT, BERT, RoBERTa, DistilBERT,
XLM, XLNet (Dai & Le, 2015; Peters et al., 2017; Devlin
et al., 2019; Peters et al., 2018a; Lample & Conneau, 2019;
Peters et al., 2018b; Howard & Ruder, 2018; Yang et al.,
2019; Liu et al., 2019; Tang et al., 2019). In particular,
the BERT model (Devlin et al., 2019) has become a new
NLP baseline including sentence classiﬁcation, question an-
swering, named-entity recognition and many others. To our
knowledge there are few measures that address prediction
uncertainties in these sophisticated deep learning structures,
or explain how to achieve optimal decisions on observed
uncertainty measures. The off-the-shelf softmax outputs of
these models are predictive probabilities, and they are not a
valid measure for the conﬁdence in a networks predictions
(Gal & Ghahramani, 2016; Maddox et al., 2019; Pearce
et al., 2018; Shridhar et al., 2019), which are important
concerns in real-world applications (Larson et al., 2019).

Our main contribution in this paper is applying advances in
Bayesian Deep Learning to quantify uncertainties in BERT
intent predictions. Formal methods like Stochastic Gradient
(SG)-MCMC (Li et al., 2016; Rao & Frtunikj, 2018; Welling
& Teh, 2011; Park et al., 2018; Maddox et al., 2019; Seedat
& Kanan, 2019), variational inference (Blundell et al., 2015;
Gal & Ghahramani, 2016; Graves, 2011; Hern´andez-Lobato
& Adams, 2015) extensively discussed in literature may

require modifying the network. Re-implementation of the
entire BERT model for Bayesian inference is a non-trivial
task, so here we took the Monte Carlo Dropout (MCD)
approach (Gal & Ghahramani, 2016) to approximate varia-
tional inference, whereby dropout is performed at training
and test time, using multiple dropout masks. Our dropout
experiments are compared with two other approaches (En-
tropy and Dummy-class), and the ﬁnal implementation is
determined among the trade-off between accuracy and efﬁ-
ciency.

We also investigate the usage of BERT as a language
model to decipher spelling errors. Most vendor-based chat-
bot solutions embed an additional layer of service, where
device-dependent error models and N-gram language mod-
els(Lin et al., 2012) are utilized for spell checking and
language interpretation. At representation layer, Word-
piece model(Schuster & Nakajima, 2012) and Byte-Pair-
Encoding(BPE) model(Gage, 1994; Sennrich et al., 2016)
are common techniques to segment words into smaller units,
thus similarities at sub-word level can be captured by NLP
models and generalized on out-of-vocabulary(OOV) words.
Our approach combines efforts of both sides: words cor-
rected by the proposed language model are further tokenized
by Wordpiece model to match pre-trained embeddings in
BERT learning.

Despite all advances of chatbots, industries like ﬁnance and
healthcare are concerned about cyber-security because of the
large amount of sensitive information entered during chatbot
sessions. Task-oriented bots often require access to criti-
cal internal systems and conﬁdential data to ﬁnish speciﬁc
tasks. Therefore, 100% on-premise solutions that enable
full customization, monitoring, and smooth integration are
preferable than cloud solutions. In this paper, the proposed
chatbot is designed using RASA open-source version and
deployed within our enterprise intranet. Using RASA’s con-
versational design, we hybridize RASA’s chitchat module
with the proposed task-oriented conversational systems de-
veloped on Python, Tensorﬂow and Pytorch. We believe our

ClientPhone AgentExpertSentence Completion ModelIntent ClassificationModelInformation Retrieval and Question AnsweringHelp Documents RepoChatbotrelevantIrrelevant (human escalation)Human expert supportConversational AI supportFuture self-provisioning interactionAVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

approach can provide some useful guidance for industries
contemplate adopting chatbot solutions in their business
domains.

2. Background

Recent breakthroughs in NLP research are driven by two
intertwined directions: Advances in distributed representa-
tions, sparked by the success of word embeddings (Mikolov
et al., 2010; 2013), character embeddings (Kim et al., 2015;
dos Santos & Gatti, 2014; Dos Santos & Zadrozny, 2014),
contextualized word embeddings (Peters et al., 2018a; Rad-
ford & Sutskever, 2018; Devlin et al., 2019), have success-
fully tackled the curse of dimensionality in modeling com-
plex language models. Advances of neural network archi-
tecture, represented by CNN (Collobert & Weston, 2008;
Collobert et al., 2011), RNN(Elman, 1990), Attention Mech-
anism (Bahdanau et al., 2015), and Transformer as seq2seq
model with parallelized attentions (Vaswani et al., 2017),
have deﬁned the new state of the art deep learning models
for NLP.

Principled uncertainty estimation in regression (V. Kuleshov
& Ermon, 2018), reinforcement learning (et al., 2016) and
classiﬁcation (et al., 2017) are active areas of research with
a large volume of work. The theory of Bayesian neural
networks (Neal, 1995; MacKay, 1992) provides the tools
and techniques to understand model uncertainty, but these
techniques come with signiﬁcant computational costs as
they double the number of parameters to be trained. Gal and
Ghahramani (Gal & Ghahramani, 2016) showed that a neu-
ral network with dropout turned on at test time is equivalent
to a deep Gaussian process and we can obtain model uncer-
tainty estimates from such a network by multiple-sampling
the predictions of the network at test time. Non-Bayesian ap-
proaches to estimating the uncertainty are also shown to pro-
duce reliable uncertainty estimates (B. Lakshminarayanan,
2017); our focus in this paper is on Bayesian approaches. In
classiﬁcation tasks, the uncertainty obtained from multiple-
sampling at test time is an estimate of the conﬁdence in the
predictions similar to the entropy of the predictions. In this
paper, we compare the threshold for escalating a query to
a human operator using model uncertainty obtained from
dropout-based chatbot against setting the threshold using
the entropy of the predictions. We choose dropout-based
Bayesian approximation because it does not require changes
to the model architecture, does not add parameters to train,
and does not change the training process as compared to
other Bayesian approaches. We minimize noise in the data
by employing spelling correction models before classifying
the input. Further, the labels for the user queries are human
curated with minimal error. Hence, our focus is on quan-
tifying epistemic uncertainty in AVA rather than aleatoric
uncertainty (Kendall & Gal, 2017). We use mixed-integer

optimization to ﬁnd a threshold for human escalation of a
user query based on the mean prediction and the uncertainty
of the prediction. This optimization step, once again, does
not require modiﬁcations to the network architecture and
can be implemented separately from model training. In
other contexts, it might be fruitful to have an integrated
escalation option in the neural network (Geifman, 2019),
and we leave the trade-offs of integrated reject option and
non-Bayesian approaches for future work.

Similar approaches in spelling correction, besides those
mentioned in Section 1, are reported in Deep Text Corrector
(Atpaino, 2017) that applies a seq2seq model to automat-
ically correct small grammatical errors in conversational
written English. Optimal decision threshold learning under
uncertainty is studied in (Lepora, 2016) as Reinforcement
learning and iterative Bayesian optimization formulations.

3. System Overview and Data Sets

3.1. Overview of the System

Figure 1 illustrates system overview of AVA. The proposed
conversational AI will gradually replace the traditional
human-human interactions between phone agents and inter-
nal experts, and eventually allows clients self-provisioning
interaction directly to the AI system. Now, phone agents in-
teract with AVA chatbot deployed on Microsoft Teams in our
company intranet, and their questions are preprocessed by
a Sentence Completion Model (introduced in Section 6) to
correct misspellings. Then, inputs are classiﬁed by an intent
classiﬁcation model (Section 4 & 5), where relevant ques-
tions are assigned predicted intent labels, and downstream
information retrieval and questioning answering modules
are triggered to extract answers from a document reposi-
tory. Irrelevant questions are escalated to human experts
following the decision thresholds optimized using methods
introduced in section 5. This paper only discusses the Intent
Classiﬁcation model and the Sentence Completion model.

3.2. Data for Intent Classiﬁcation Model

Training data for AVA’s intent classiﬁcation model is col-
lected, curated, and generated by a dedicated business team
from interaction logs between phone agents and the expert
team. The whole process takes about one year to ﬁnish.
In total 22,630 questions are selected and classiﬁed to 381
intents, which compose the relevant questions set for the
intent classiﬁcation model. Additionally, 17,395 questions
are manually synthesized as irrelevant questions, and none
of them belongs to any of the aforementioned 381 intents.
Each relevant question is hierarchically assigned with three
labels from Tier 1 to Tier 3. In this hierarchy, there are 5
unique Tier-1 labels, 107 Tier-2 labels, and 381 Tier-3 la-
bels. Our intent classiﬁcation model is designed to classify

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

T1 label

Account Maintenance

Account Permission

TAX FAQ

Transfer of Asset

Banking

Irrelevant

T2 label
Call Authentication
Call Authentication
Web Reset
Call Authentication
Agent Incapactiated
Miscellaneous
Unlike registrations
Brokerage Transfer
Add Owner
Add/Change/Delete
-
-
-

T3 label
Type 2
Type 5
Type 1
Type 2
Type 3
What is
Type 2
Type 3
Type 4
Type 3
-
-
-

Questions
Am I allowed to give the client their Social security number?
Do the web security questions need to be reset by the client if their web access is blocked?
How many security questions are required to be asked to reset a clients web security questions?
How are the web security questions used to authenticate a client?
Is it possible to set up Agent Certiﬁcation for an Incapacitated Person on an Individual Roth 401k?
Do I need my social security number on the 1099MISC form?
Does the client need to provide special documentation if they want to transfer from one account to another account?
Is there a list of items that need to be included on a statement to transfer an account?
Once a bank has been declined how can we authorize it?
Does a limited agent have authorization to adjust bank info?
How can we get into an account with only one security question?
Am I able to use my Roth IRA to set up a margin account?
What is the best place to learn about Vanguard’s investment philosophy?

Table 1. Example questions used in AVA intent classiﬁcation model training

relevant input questions into 381 Tier 3 intents and then trig-
gers downstream models to extract appropriate responses.
The ﬁve Tier-1 labels and the numbers of intents include in
each label are: Account Maintenance (9074), Account Per-
missions (2961), Transfer of Assets (2838), Banking (4788),
Tax FAQ (2969). At Tier-1, general business issues across
intents are very different, but at Tier-3 level, questions are
quite similar to each other, where differences are merely at
the speciﬁc responses. Irrelevant questions, compared to
relevant questions, have two main characteristics:

• Some questions are relevant to business intents but
unsuitable to be processed by conversational AI. For
example, in Table 1, question ”How can we get into
an account with only one security question?” is re-
lated to Call Authentication in Account Permission, but
its response needs further human diagnosis to collect
more information. These types of questions should be
escalated to human experts.

• Out of scope questions. For example, questions like
”What is the best place to learn about Vanguard’s in-
vestment philosophy?” or ”What is a hippopotamus?”
are totally outside the scope of our training data, but
they may still occur in real world interactions.

3.3. Textual Data for Pretrained Embeddings and

Sentence Completion Model

Inspired by the progress in computer vision, transfer learn-
ing has been very successful in NLP community and has
become a common practice. Initializing deep neural net-
work with pre-trained embeddings, and ﬁne-tune the models
towards task-speciﬁc data is a proven method in multi-task
NLP learning. In our approach, besides applying off-the-
shelf embeddings from Google BERT and XLNet, we also
pre-train BERT embeddings using our company’s propri-
etary text to capture special semantic meanings of words
in the ﬁnancial domain. Three types of textual datasets are
used for embeddings training:

cluding web pages, word documents, ppt slides, pdf
documents, and notes from internal CRM systems.

• Emails: About 8G bytes of customer service emails

are extracted.

• Phone call transcriptions: We apply AWS to transcribe
500K client service phone calls, and the transcription
text is used for training.

All embeddings are trained in case-insensitive settings. At-
tention and hidden layer dropout probabilities are set to 0.1,
hidden size is 768, attention heads and hidden layers are
set to 12, and vocabulary size is 32000 using SentencePiece
tokenizer. On AWS P3.2xlarge instance each embeddings
is trained for 1 million iterations, and takes about one week
CPU time to ﬁnish. More details about parameter selection
for pre-training are avaialble in the github code. The same
pre-trained embeddings are used to initialize BERT model
training in intent classiﬁcation, and also used as language
models in sentence completion.

Model
BERT small + Sharepoint Embeddings
BERT small + Google Embeddings
BERT large + Google Embeddings
XLNet Large + Google Embeddings
LSTM with Attention + Word2Vec
LSTM + Word2Vec
Logistic Regression + TFIDF
Xgboost + TFIDF
Naive Bayes + TFIDF

Performance
0.944
0.949
0.954
0.927
0.913
0.892
0.820
0.760
0.661

Table 2. Comparison of intent classiﬁcation performance. BERT
and XLNet models were all trained for 30 epochs using batch size
16.

4. Intent Classiﬁcation Performance on

Relevant Questions

• Sharepoint text: About 3.2G bytes of corpora scraped
from our company’s internal Sharepoint websites, in-

Using only relevant questions, we compare various popular
model architectures to ﬁnd one with the best performance

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

capability to handle 381 intents simultaneously at 94.5%
accuracy makes it an ideal intent classiﬁer candidate in a
chatbot. This section describes how we quantify uncer-
tainties on BERT predictions and enable the bot to detect
irrelevant questions. Three approaches are compared:

• Predictive-entropy:

We measure

uncer-
tainty of predictions using Shannon entropy
H = − (cid:80)K
k=1 pik log pik where pik is the prediction
probability of i-th sample to k-th class. Here, pik
is softmax output of the BERT network (B. Laksh-
minarayanan, 2017). A higher predictive entropy
corresponds to a greater degree of uncertainty. Then,
an optimally chosen cut-off threshold applied on
entropies should be able to separate the majority of
in-sample questions and irrelevant questions.

• Drop-out: We apply Monte Carlo (MC) dropout by
doing 100 Monte Carlo samples. At each inference
iteration, a certain percent of the set of units to drop
out. This generates random predictions, which are in-
terpreted as samples from a probabilistic distribution
(Gal & Ghahramani, 2016). Since we do not employ
regularization in our network, τ −1 in Eq. 7 in Gal and
Ghahramani (Gal & Ghahramani, 2016) is effectively
zero and the predictive variance is equal to the sample
variance from stochastic passes. We could then investi-
gate the distributions and interpret model uncertainty
as mean probabilities and variances.

• Dummy-class: We simply treat escalation questions
as a dummy class to distinguish them from original
questions. Unlike entropy and dropout, this approach
requires retraining of BERT models on the expanded
data set including dummy class questions.

5.1. Experimental Setup

All results mentioned in this section are obtained using
BERT small + sharepoint embeddings (batch size 16). In
Entropy and Dropout approaches, both relevant questions
and irrelevant questions are split into ﬁve folds, where four
folds (80%) of relevant questions are used to train the BERT
model. Then, among that 20% held-out relevant questions
we further split them into ﬁve folds, where 80% of them
(equals to 16% of the entire relevant question set) are com-
bined with four folds of irrelevant questions to learn the
optimal decision variables. The learned decision variables
are applied on BERT predictions of the remaining 20%
(906) of held-out relevant questions and held-out irrelevant
questions (4000), to obtain the test performance. In dummy
class approach, BERT model is trained using four folds of
relevant questions plus four folds of irrelevant questions,
and tested on the same amount of test questions as Entropy
and Dropout approaches.

Figure 2. Comparison of test set accuracy using different Embed-
dings and Batch Sizes

on 5-fold validation. Not surprisingly, BERT models gen-
erally produce much better performance than other models.
Large BERT (24-layer, 1024-hidden, 16-heads) has a slight
improvement over small BERT (12-layer, 768-hidden, 12-
heads), but less preferred because of expensive computa-
tions. To our surprise, XLNet, a model reported outperform-
ing BERT in mutli-task NLP, performs 2 percent lower on
our data.

BERT models initialized by proprietary embeddings con-
verge faster than those initialized by off-the-shelf embed-
dings (Figure 2.a). And embeddings trained on company’s
sharepoint text perform better than those built on Emails and
phone-call transcriptions (Figure 2.b). Using larger batch
size (32) enables models to converge faster, and leads to
better performance.

5. Intent Classiﬁcation Performance including

Irrelevant Questions

We have shown how BERT model outperforming other mod-
els on real datasets that only contain relevant questions. The

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

(a) Predictive Entropy distributions on
relevant (orange) and escalation (blue)
questions

(b) Test Accuracy when adding Escala-
tion Questions in optimization procedure

(c) Optimal Entropy cut-off value when
adding Escalation Questions in optimiza-
tion procedure

Figure 3. Optimizing entropy threshold to detect irrelevant questions. As shown in (a), in-sample test questions and irrelevant questions
have very different distributions of predictive entropies. Subﬁgure (b) shows how test accuracies, evaluated using decision variables b
solved by (1) on BERT predictions on test data, change when different numbers of irrelevant questions involved in training. Subﬁgure (c)
shows the impact of δ on the optimized thesholds when number of irrelevant questions increase optimization.

5.2. Optimizing Entropy Decision Threshold

To ﬁnd the optimal threshold cutoff b, we consider the fol-
lowing Quadratic Mixed-Integer programming problem

varied on all layers. Our MC dropout experiments are con-
ducted as follows:

1 Change

dropout

ratios

in

encod-

ing/decoding/attention/output layer of BERT

min
x,b
s.t.

(cid:80)

i,k(xik − lik)2

xik = 0
xik = 1
xik ∈ {0, 1}
(cid:80)K+1

k=1 xik = 1

b ≥ 0

if Ei ≥ b, for k in 1, ..., K
if Ei ≥ b, for k = K + 1

2 Train BERT model on 80% of relevant questions for

10 or 30 epochs

∀i in 1, ..., N

3 Export and serve the trained model by Tensorﬂow serv-

ing

(1)

to minimize the quadratic loss between the predictive as-
signments xik and true labels lik. In (1), i is sample index,
and k is class (intent) indices. xik is N × (K + 1) binary
matrix, and lik is also N × (K + 1), where the ﬁrst K
columns are binary values and the last column is a uniform
vector δ, which represents the cost of escalating questions.
Normally δ is a constant value smaller than 1, which en-
courages the bot to escalate questions rather than making
mistaken predictions. The ﬁrst and second constraints of (1)
force an escalation label when entropy Ei ≥ b. The third
and fourth constraints restrict xik as binary variables and
ensure the sum for each sample is 1. Experimental results
(Figure 3) indicate that (1) needs more than 5000 escalation
questions to learn a stabilized b. The value of escalation
cost δ has a signiﬁcant impact on the optimal b value, and
in our implementation is set to 0.5.

5.3. Monte Carlo Drop-out

In BERT model, dropout ratios can be customized at encod-
ing, decoding, attention, and output layer. A combinatorial
search for optimal dropout ratios is computationally chal-
lenging. Results reported in the paper are obtained through
simpliﬁcations with the same dropout ratio assigned and

4 Repeat inference 100 times on questions, then average
the results per each question to obtain mean probabil-
ities and standard deviations, then average the devia-
tions for a set of questions.

According to the experimental results illustrated in Figure
4, we make three conclusions: (1) Epistemic uncertainty
estimated by MCD reﬂects question relevance: when inputs
are similar to the training data there will be low uncertainty,
whilst data is different from the original training data should
have higher epistemic uncertainty. (2) Converged models
(more training epochs) should have similar uncertainty and
accuracy no matter what drop ratio is used. (3) The number
of epochs and dropout ratios are important hyper-parameters
that have substantial impacts on uncertainty measure and
predictive accuracy and should be cross-validated in real
applications.

We use mean probabilities and standard deviations obtained
from models where dropout ratios are set to 10% after 30
epochs of training to learn optimal decision thresholds. Our
goal is to optimize lowerbound c and upperbound d, and des-
ignate a question as relevant only when the mean predictive
probability Pik is larger than c and standard deviation Vik is
lower than d. Optimizing c and d, on a 381-class problem,

0123456Entropy0.00.10.20.30.40.5Escalation questionsTest questions100025005000750010000number of escalation questions0.800.820.840.860.880.900.920.94test accuracy = 0.5 = 0.6 = 0.7 = 0.8 = 0.9100025005000750010000number of escalation questions0.250.500.751.001.251.501.752.00optimal entropy cutoff = 0.5 = 0.6 = 0.7 = 0.8 = 0.9AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

(a) Intent accuracy at different drop
out ratios

(b) Uncertainties on questions after
training 10 epochs

(c) Uncertainties on questions after
training 30 epochs

Figure 4. Classiﬁcation accuracy and uncertainties obtained from Monte Carlo Dropout

Entropy

Dropout

Dummy Class

number of irrelevant
questions in training
optimal entropy cutoff b
optimal mean prob cutoff c
optimal std. cutoff d
mean accuracy in 381 classes
accuracy of the dummy class
precision (binary classiﬁcation)
recall (binary classiﬁcation)
F1 score (binary classiﬁcation)

8000
5000
1000
0.85
1.13
2.36
-
-
-
-
-
-
91.9%
85.6%
88.3%
79.25% 91.2% 93.25%
70.2%
91.3%
0.794

51.4%
96.7%
0.671

74.7%
88.1%
0.808

10000
0.55
-
-
81.7%
95.2%
79.8%
83.5%
0.816

100
-
0.8172
0.1533

1000
-
0.6654
0.0250

88.41% 80.13%
86.69% 91.83%

90.7%
93.9%
0.738

68.8%
82.7%
0.751

2000
-
0.7921
0.0261
80.24%
91.95%
68.9%
83.2%
0.754

3000
-
0.0459
0.0132
74.72%
92.57%
63.7%
84.7%
0.727

1000
-
-
-

5000
-
-
-

94.2% 93.7%
73.6% 94.5%
95.3%
98.7%
0.967

81%
99.7%
0.894

8000
-
-
-
87.7%
99.4%
99.5%
92.6%
0.959

10000
-
-
-
82%
99.6%
99.6%
86%
0.923

Table 3. Performance cross comparison of three approaches evaluated on test data of same size (906 relevant questions plus 4000 irrelevant
questions). Precision/Recall/F1 scores were calculated assuming relevant questions are true positives. In entropy and dropout optimization
processes, δ is set to 0.5. Other delta values for dropout approach are listed in appendix.

is much more computationally challenging than learning
entropy threshold because the number of constraints is pro-
portional to class number. As shown in (2), we introduce
two variables α and β to indicate the status of mean prob-
ability and deviation conditions, and the ﬁnal assignment
variables x is the logical AND of α and β. Solving (2) with
more than 10k samples is very slow (shown in Appendix),
so we use 1500 original relevant questions, and increase
the number of irrelevant questions from 100 to 3000. For
performance testing, the optimized c and d are applied as
decision variables on samples of BERT predictions on test
data. Performance from dropout are presented in Table 3
and Appendix. Our results showed decision threshold opti-
mized from (2) involving 2000 irrelevant questions gave the
best F1 score (0.754), and we validated it using grid search

and conﬁrmed its optimality (shown in appendix).

min
x,c,d

(cid:80)

i,k(xik − lik)2

s.t.

αik =

βik =

(cid:40)
0
1

(cid:40)
0
1

if Pik ≤ c, for k in 1, ..., K
if otherwise
if Vik ≥ d, for k in 1, ..., K
if otherwise

xik = 0 if αik = 0 OR βik = 0
xik = 1 if αik = 1 AND βik = 1
(cid:80)K+1
k

xik = 1 ∀i in 1, ..., N

1 ≥ c ≥ 0
1 ≥ d ≥ 0

(2)

5.4. Dummy-class Classiﬁcation

Our third approach is to train a binary classiﬁer using both
relevant questions and irrelevant questions in BERT. We use
a dummy class to represent those 17,395 irrelevant ques-
tions, and split the entire data sets, including relevant and
irrelevant, into ﬁve folds for training and test.

Performance of dummy class approach is compared with
Entropy and Dropout approaches (Table 3). Deciding an
optimal number of irrelevant questions involved in threshold
learning is non-trivial, especially for Entropy and Dummy
class approaches. Dropout doesn’t need as many irrelevant
questions as entropy does to learn optimal threshold, mainly

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

because the number of constraints in (2) is proportional to
the class number (381), so the number of constraints are
large enough to learn a suitable threshold on small samples
(To support this conclusion, we present extensive studies
in Appendix on a 5-class classiﬁer using Tier 1 intents).
Dummy class approach obtains the best performance, but
its success assumes the learned decision boundary can be
generalized well to any new irrelevant questions, which is
often not valid in real applications. In contrast, Entropy and
Dropout approaches only need to treat a binary problem in
the optimization and leave the intent classiﬁcation model in-
tact. The optimization problem for entropy approach can be
solved much more efﬁciently, and is selected as the solution
for our ﬁnal implementation.

It is certainly possible to combine Dropout and Entropy
approach, for example, to optimize thresholds on entropy
calculated from the average mean of MCD dropout predic-
tions. Furthermore, it is possible that the problem deﬁned
in (2) can be simpliﬁed by proper reformulation, and can be
solved more efﬁciently, which will be explored in our future
works.

6. Sentence Completion using Language

Model

6.1. Algorithm

We assume misspelled words are all OOV words, and we
can transform them as [MASK] tokens and use bidirec-
tional language models to predict them. Predicting masked
word within sentences is an inherent objective of a pre-
trained bidirectional model, and we utilize the Masked Lan-
guage Model API in the Transformer package (Hugging-
Face, 2017) to generate the ranked list of candidate words
for each [MASK] position. The sentence completion algo-
rithm is illustrated in Algorithm 1.

6.2. Experimental Setup

For each question, we randomly permutate two characters
in the longest word, the next longest word, and so on. In
this way, we generate one to three synthetic misspellings in
each question. We investigate intent classiﬁcation accuracy
changes on these questions, and how our sentence comple-
tion model can prevent performance changes. All models
are trained using relevant data (80%) without misspellings
and validated on synthetic misspelled test data. Five set-
tings are compared: (1) No correction: classiﬁcation per-
formance without applying any auto-correction; (2) No LM:
Auto-corrections made only by word edit distance with-
out using Masked Language model; (3) BERT Sharepoint:
Auto-corrections made by Masked LM using pre-trained

sharepoint embeddings together with word edit distance;
(4) BERT Email: Auto-corrections using pretrained email
embeddings together with word edit distance; (5) BERT
Google: Auto-corrections using pretrained Google Small
uncased embedding data together with word edit distance.

We also need to decide what is an OOV, or, what should be
included in our vocabulary. After experiments, we set our
vocabulary as words from four categories: (1) All words in
the pre-trained embeddings; (2) All words that appear in
training questions; (3) Words that are all capitalized because
they are likely to be proper nouns, fund tickers or service
products; (4) All words start with numbers because they
can be tax forms or speciﬁc products (e.g., 1099b, 401k,
etc.). The purposes of including (3) and (4) is to avoid
auto-correction on those keywords that may represent sig-
niﬁcant intents. Any word falls outside these four groups
is considered as an OOV. During our implementation, we
keep monitoring OOV rate, deﬁned as the ratio of OOV
occurrences to total word counts in recent 24 hours. When
it is higher than 1%, we apply manual intervention to check
chatbot log data.

We also need to determine two additional parameters M , the
number of candidate tokens prioritized by masked language
model and B, the beam size in our sentence completion
model. In our approach, we set M and B to the same value,
and it is benchmarked from 1 to 10k by test sample accuracy.
Notice that when M and B are large, and when there are
more than two OOVs, Beam Search becomes very inefﬁ-
cient in Algorithm 1. To simplify this, instead of ﬁnding the
optimal combinations of candidate tokens that maximize

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

(a) Accuracy - Single OOV

(b) Accuracy - Two OOVs

(c) Accuracy - Three OOVs

(d) Accuracy per beam size

Figure 5. As expected, misspelled words can signiﬁcantly decrease intent classiﬁcation performance. The same BERT model that achieved
94% on clean data, dropped to 83.5% when a single OOV occured in each question. It further dropped to 68% and 52%, respectively,
when two and three OOVs occured. In all experiments, LM models proved being useful to help correcting words and reduce performance
drop, while domain speciﬁc embeddings trained on Vanguard Sharepoint and Email text outperform off-the-shelf Google embeddings.
The beam size B (M ) was benchmarked as results shown in subﬁgure (d), and was set to 4000 to generate results in subﬁgure (a) to (c).

the joint probability arg max (cid:81)d
i=1 pi, we assume they are
independent and apply a simpliﬁed Algorithm (shown in
Appendix) on single OOV separately. An improved ver-
sion of sentence completion algorithm to maximize joint
probability will be our future research. We haven’t consider
situations when misspellings are not OOV in our paper. To
detect improper words in a sentence may need evaluation of
metrics such as Perplexity or Sensibleness and Speciﬁcity
Average (SSA)(Adiwardana et al., 2020), and will be our
future goals.

During our implementation, we further explore how the
intent classiﬁcation model API can be served in real appli-
cations under budget. We gradually reduce the numbers
of attention layer and hidden layer in the original BERT
Small model (12 hidden layers, 12 attention heads) and
create several smaller models. By reducing the number of
hidden layers and attention layers in half, we see a remark-
able 100% increase in performance (double the throughput,
half the latency) with the cost of only 1.6% drop in intent
classiﬁcation performance.

6.3. Results

According to the experimental results illustrated in Figure 5,
pre-trained embeddings are useful to increase the robustness
of intent prediction on noisy inputs. Domain-speciﬁc em-
beddings contain much richer context-dependent semantics
that helps OOVs get properly corrected, and leads to better
task-oriented intent classiﬁcation performance. Benchmark
shows B≥4000 leads to the best performance for our prob-
lem. Based on this, we apply sharepoint embeddings as the
language model in our sentence completion module.

7. Implementation

The chatbot has been implemented fully inside our company
network using open source tools including RASA(Bocklisch
et al., 2017), Tensorﬂow, Pytorch in Python enviornment.
All backend models (Sentence Completion model, Intent
Classiﬁcation model and others) are deployed as REST-
FUL APIs in AWS Sagemaker. The front-end of chatbot is
launched on Microsoft Teams, powered by Microsoft Bot-
framework and Microsoft Azure directory, and connected to
backend APIs in AWS environment. All our BERT model
trainings, including embeddings pretraining, are based on
BERT Tensorﬂow running on AWS P3.2xlarge instance.
The optimization procedure uses Gurobi 8.1 running on
AWS C5.18xlarge instance. BERT language model API in
sentence completion model is developed using Transformer
2.1.1 package on PyTorch 1.2 and Tensorﬂow 2.0.

Model
12A-12H
6A-12H
12A-9H
3A-9H
3A-12H
6A-6H

Performance
0.944
0.941
0.934
0.933
0.930
0.928

Throughput Avg. Latency

8.9/s
9.0/s
11.8/s
12.0/s
9.1/s
18.1/s

1117 ms
1108 ms
843 ms
831 ms
1097 ms
552 ms

Table 4. Benchmark of intent classiﬁcation API performance
across different models in real application. Each model is tested
using 10 threads, simulating 10 concurrent users, for a duration of
10 minutes. In this test, models are not served as Monte Carlo sam-
pling, so the inference is done only once. All models are hosted on
identical AWS m5.4xlarge CPU instances. As seen, the simplest
model (6A-6H, 6 attention layers and 6 hidden layers) can have
double throughput rate and half latency than the original BERT
small model, and the accuracy performance only drops 1.6%. The
performance is evaluated using JMeter at client side, and APIs are
served using Domino Lab 3.6.17 Model API. Throughput indicates
how many API responses being made per second. Latency is mea-
sured as time elapse between request sent till response received at
client side.

8. Conclusions

Our results demonstrate that optimized uncertainty thresh-
olds applied on BERT model predictions are promising to
escalate irrelevant questions in task-oriented chatbot im-
plementation, meanwhile the state-of-the-art deep learning
architecture provides high accuracy on classifying into a
large number of intents. Another feature we contribute is

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

the application of BERT embeddings as language model to
automatically correct small spelling errors in noisy inputs,
and we show its effectiveness in reducing intent classiﬁca-
tion errors. The entire end-to-end conversational AI system,
including two machine learning models presented in this
paper, is developed using open source tools and deployed
as in-house solution. We believe those discussions provide
useful guidance to companies who are motivated to reduce
dependency on vendors by leveraging state-of-the-art open
source AI solutions in their business.

We will continue our explorations in this direction, with
particular focuses on the following issues: (1) Current ﬁne-
tuning and decision threshold learning are two separate parts,
and we will explore the possibility to combine them as a
new cost function in BERT model optimization. (2) Dropout
methodology applied in our paper belongs to approximated
inference methods, which is a crude approximation to the ex-
act posterior learning in parameter space. We are interested
in a Bayesian version of BERT, which requires a new archi-
tecture based on variational inference using tools like TFP
Tensorﬂow Probability. (3) Maintaining chatbot produc-
tion system would need a complex pipeline to continuously
transfer and integrate features from deployed model to new
versions for new business needs, which is an uncharted
territory for all of us. (4) Hybridizing ”chitchat” bots, us-
ing state-of-the-art progresses in deep neural models, with
task-oriented machine learning models is important for our
preparation of client self-provisioning service.

9. Acknowledgement

We thank our colleagues in Vanguard CAI (ML-DS team
and IT team) for their seamless collaboration and support.
We thank colleagues in Vanguard Retail Group (IT/Digital,
Customer Care) for their pioneering effort collecting and
curating all the data used in our approach. We thank Robert
Fieldhouse, Sean Carpenter, Ken Reeser and Brain Heck-
man for the fruitful discussions and experiments.

References

Adiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N.,
Thoppilan, R., Yang, Z., Kulshreshtha, A., Nemade, G.,
Lu, Y., and Le, Q. V. Towards a human-like open-domain
chatbot, 2020.

International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, 2015.

Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
D. Weight uncertainty in neural network. In Proceedings
of the 32nd ICML, pp. 1613–1622, 2015.

Bocklisch, T., Faulkner, J., Pawlowski, N., and Nichol, A.
Rasa: Open source language understanding and dialogue
management. CoRR, abs/1712.05181, 2017. URL http:
//arxiv.org/abs/1712.05181.

Colby, K. M., Weber, S., and Hilf, F. D. Artiﬁcial paranoia.
ISSN

Artiﬁcial Intelligence, 2(1):125, January 1971.
0004-3702.

Collobert, R. and Weston, J. A uniﬁed architecture for
natural language processing: Deep neural networks with
multitask learning. In Proceedings of the 25th ICML, pp.
160167, 2008.

Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. P. Natural language
processing (almost) from scratch. JMLR, 12, 2011. ISSN
1532-4435.

Dai, A. M. and Le, Q. V. Semi-supervised sequence learning.
In In Proceedings of Advances in Neural Information
Processing Systems 28, pp. 3079–3087. 2015.

Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:
Pre-training of deep bidirectional transformers for lan-
guage understanding. In Proceedings of the 2019 Confer-
ence of the NACL, Vol 1, pp. 4171–4186, June 2019.

dos Santos, C. and Gatti, M. Deep convolutional neural
networks for sentiment analysis of short texts. In Pro-
ceedings of the 25th International Conference on Compu-
tational Linguistics, pp. 69–78, Dublin, Ireland, August
2014.

Dos Santos, C. N. and Zadrozny, B. Learning character-level
representations for part-of-speech tagging. In Proceed-
ings of the 31st International Conference on Machine
Learning - Volume 32, ICML14, pp. II1818II1826, 2014.

Elman, J. L. Finding structure in time. COGNITIVE SCI-

ENCE, 14(2):179–211, 1990.

et al., C. G. On calibration of modern neural networks.

Atpaino. Deep-text-corrector. https://github.com/

volume 70, 2017.

atpaino/deep-text-corrector, 2017.

B. Lakshminarayanan, A. Prtizel, C. B. Simple and scalable
predictive uncertainty estimation using deep ensembles.
pp. 6405, 2017.

Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate. In 3rd

et al., M. G. Bayesian reinforcement learning. Foundations

and Trends in Machine Learning, 8(5-6), 2016.

Fedorenko, D. G., Smetanin, N., and Rodichev, A. Avoiding
echo-responses in a retrieval-based conversation system.
Conference on Artiﬁcial Intelligence and Natural Lan-
guage, pp. 91–97, 2017.

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

Gage, P. A new algorithm for data compression. C Users J.,

12(2):2338, February 1994. ISSN 0898-9788.

Gal, Y. and Ghahramani, Z. Dropout as bayesian approxi-
mation: representing model uncertainty in deep learning.
volume 48, pp. 1050, 2016.

Gao, J., Galley, M., and Li, L. Neural approaches to conver-

sational ai. In SIGIR ’18, 2018.

Lin, Y., Michel, J.-B., Aiden, E. L., Orwant, J., Brockman,
W., and Petrov, S. Syntactic annotations for the google
books ngram corpus. In Proceedings of the ACL 2012
System Demonstrations, pp. 169174, USA, 2012.

Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy,
O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Roberta:
A robustly optimized BERT pretraining approach. CoRR,
abs/1907.11692, 2019.

Geifman, Y. Selectivenet: A deep neural network with an

integrated reject option. 2019.

MacKay, D. A practical bayesian framework for backpropa-

gation networks. volume 4, 1992.

Graves, A. Practical variational inference for neural net-
works. In Advances in Neural Information Processing
Systems 24, pp. 2348–2356. 2011.

Hern´andez-Lobato, J. M. and Adams, R. P. Probabilistic
backpropagation for scalable learning of bayesian neural
In Proceedings of the 32nd ICML, Vol 37,
networks.
ICML15, pp. 18611869, 2015.

Howard, J. and Ruder, S. Universal language model ﬁne-
tuning for text classiﬁcation. In Proceedings of the 56th
Annual Meeting of the ACL, pp. 328–339, Melbourne,
Australia, July 2018.

HuggingFace. Transformers. https://github.com/

huggingface/transformers, 2017.

Kendall, A. and Gal, Y. What uncertainties do we need in
bayesian deep learning for computer vision? volume 30,
2017.

Kim, Y., Jernite, Y., Sontag, D. A., and Rush, A. M.
Character-aware neural language models. In Proceed-
ings of the Thirtieth AAAI Conference on Artiﬁcial Intel-
ligence, pp. 27412749, 2015.

Lample, G. and Conneau, A. Cross-lingual language model

pretraining. CoRR, abs/1901.07291, 2019.

Larson, S., Mahendran, A., Peper, J. J., Clarke, C., Lee, A.,
Hill, P., Kummerfeld, J. K., Leach, K., Laurenzano, M. A.,
Tang, L., and Mars, J. An evaluation dataset for intent
classiﬁcation and out-of-scope prediction. In Proceedings
of the 9th EMNLP-IJCNLP, November 2019.

Lepora, N. F. Threshold learning for optimal decision mak-
ing. In Advances in Neural Information Processing Sys-
tems 29, pp. 3763–3771. 2016.

Li, C., Stevens, A., Chen, C., Pu, Y., Gan, Z., and Carin,
L. Learning weight uncertainty with stochastic gradient
mcmc for shape classiﬁcation. In 2016 IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp.
5666–5675, June 2016.

Maddox, W., Garipov, T., Izmailov, P., Vetrov, D. P., and
Wilson, A. G. A simple baseline for bayesian uncertainty
in deep learning. Neural Information Processing Systems
(NeurIPS), 2019.

Microsoft. Zo. https://www.zo.ai, 2019.

Mikolov, T., Karaﬁt, M., Burget, L., Cernock, J., and Khu-
danpur, S. Recurrent neural network based language
model. volume 2, pp. 1045–1048, 01 2010.

Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean,
J. Distributed representations of words and phrases and
their compositionality. CoRR, abs/1310.4546, 2013.

Neal, R. Bayesian learning for neural networks. PhD thesis,

1995.

Park, C., Kim, J., Ha, S. H., and Lee, J. Sampling-based
bayesian inference with gradient uncertainty. CoRR,
abs/1812.03285, 2018.

Pearce, T., Zaki, M., Brintrup, A., and Neely, A. Uncer-
tainty in neural networks: Bayesian ensembling. ArXiv,
abs/1810.05546, 2018.

Peters, M., Ammar, W., Bhagavatula, C., and Power, R.
Semi-supervised sequence tagging with bidirectional lan-
guage models. In Proceedings of the 55th ACL, pp. 1756–
1765, Vancouver, Canada, July 2017.

Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C.,
Lee, K., and Zettlemoyer, L. Deep contextualized word
representations. In Proceedings of the 2018 Conference
of the NAACL, pp. 2227–2237, New Orleans, Louisiana,
June 2018a.

Peters, M. E., Neumann, M., Zettlemoyer, L., and Yih, W.
Dissecting contextual word embeddings: Architecture
and representation. CoRR, abs/1808.08949, 2018b.

Radford, A. and Sutskever, I. Improving language under-

standing by generative pre-training. In arxiv, 2018.

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhut-
dinov, R., and Le, Q. V. Xlnet: Generalized autore-
gressive pretraining for language understanding. CoRR,
abs/1906.08237, 2019.

Zhou, L., Gao, J., Li, D., and Shum, H. The design and
implementation of xiaoice, an empathetic social chatbot.
CoRR, abs/1812.08989, 2018.

Rao, Q. and Frtunikj, J. Deep learning for self-driving cars:
Chances and challenges. In 2018 IEEE/ACM 1st Inter-
national Workshop on Software Engineering for AI in
Autonomous Systems (SEFAIAS), pp. 35–38, Los Alami-
tos, CA, USA, may 2018. IEEE Computer Society.

Schuster, M. and Nakajima, K. Japanese and korean voice
search. In 2012 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP), pp. 5149–
5152, March 2012.

Seedat, N. and Kanan, C. Towards calibrated and scalable
uncertainty representations for neural networks. ArXiv,
abs/1911.00104, 2019.

Sennrich, R., Haddow, B., and Birch, A. Neural machine
translation of rare words with subword units. In Proceed-
ings of the 54th ACL, pp. 1715–1725, Berlin, Germany,
August 2016.

Serban, I. V., Sankar, C., Germain, M., Zhang, S., Lin,
Z., Subramanian, S., Kim, T., Pieper, M., Chandar, S.,
Ke, N. R., Mudumba, S., de Br´ebisson, A., Sotelo, J.,
Suhubdy, D., Michalski, V., Nguyen, A., Pineau, J.,
and Bengio, Y. A deep reinforcement learning chat-
bot. CoRR, 2017. URL http://arxiv.org/abs/
1709.02349.

Sethi,

S.

The

2019.
URL https://hackernoon.com/

2019.
the-state-of-chatbots-in-2019-d97f85f2294b.

chatbots

state

of

in

Shridhar, K., Laumann, F., and Liwicki, M. A comprehen-
sive guide to bayesian convolutional neural network with
variational inference. CoRR, abs/1901.02731, 2019.

Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and Lin,
J. Distilling task-speciﬁc knowledge from BERT into
simple neural networks. CoRR, abs/1903.12136, 2019.

V. Kuleshov, N. F. and Ermon, S. Accurate uncertainties for

deep learning using calibrated regression. 2018.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention
is all you need. In NIPS, 2017.

Weizenbaum, J. Elizaa computer program for the study
of natural language communication between man and
machine. 9(1):3645, January 1966. ISSN 0001-0782.

Welling, M. and Teh, Y. W. Bayesian learning via stochastic
gradient langevin dynamics. In Proceedings of the 28th
International Conference on International Conference on
Machine Learning, ICML11, pp. 681688, Madison, WI,
USA, 2011. Omnipress. ISBN 9781450306195.

Worswick, S. Mitsuku. http://www.mitsuku.com,

2019.

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

A. Appendix

All extended materials and source code related to this paper
are avaliable on https://github.com/cyberyu/
ava Our repo is composed of two parts: (1) Extended mate-
rials related to the main paper, and (2) Source code scripts.
To protect proprietary intellectual property, we cannot share
the question dataset and proprietary embeddings. We use
an alternative data set from Larson et al., “An Evaluation
Dataset for Intent Classiﬁcation and Out-of-Scope Predic-
tion”, EMNLP-IJCNLP 2019, to demonstrate the usage of
code.

A.1. Additional Results for the Main Paper

Some extended experimental results about MC dropout and
optimization are presented on github.

A.1.1. HISTOGRAM OF UNCERTAINTIES BY DROPOUT

RATIOS

We compare histograms of standard deviations observed
from random samples of predictions. The left side contains
histograms generated by 381-class intent models trained for
10 epochs, with dropout ratio varied from 10 percent to 90
percent. The right side shows histograms generated by 381
class models trained for 30 epochs.

A.1.2. UNCERTAINTY COMPARISON BETWEEN

381-CLASS VS 5-CLASS

the number
To understand how uncertainties change vs.
of classes in BERT, we train another intent classiﬁer using
only Tier 1 labels. We compare uncertainty and accuracy
changes at different dropout rates between the original 381-
class problem and the new 5-class problem.

A.1.3. GRID SEARCH FOR OPTIMAL THRESHOLD ON

DROPOUT

Instead of using optimization, we use a grid search to ﬁnd
optimal combinations of average probability threshold and
standard deviation threshold. The search space is set as a
100 x 100 grid on space [0,0] to [1,1], where thresholds vary
by step of 0.01 from 0 to 1. Applying thresholds to outputs
of BERT predictions give us classiﬁcations of relevance vs.
irrelevance questions, and using the same combination of
test and irrelevant questions we visualize the F1 score in
contour map shown on github repo.

A.1.4. OPTIMAL THRESHOLD LEARNING ON DROPOUT

381 CLASSES VS 5 CLASSES

Using the same optimization process mentioned in equation
(2) of the main paper, we compare the optimal results (also
CPU timing) learned from 381 classes vs. 5 classes.

A.1.5. SIMPLE ALGORITHM FOR SENTENCE

COMPLETION MODEL

When multiple OOVs occur in a sentence, in order to avoid
the computational burden using large beamsize to ﬁnd the
optimal joint probabilities, we assume all candidate words
for OOVs are independent, and apply Algorithm 2 one by
one to correct the OOVs.

A.2. Intent Classiﬁcation Source Code

A.2.1. BERT EMBEDDINGS MODEL PRETRAINING

The jupyter notebook for pretraining embeddings is
https://github.com/cyberyu/ava/blob/
at
master/scripts/notebooks/BERT_PRETRAIN_
Ava.ipynb. Our script is adapted from Denis An-
tyukhov’s blog “Pre-training BERT from scratch with cloud
TPU”. We set the VOC SIZE to 32000, and use Sentence-
Piece tokenizer as approximation of Google’s WordPiece.
The learning rate is set to 2e-5, training batch size is 16,
training setps set to 1 million, MAX SEQ LENGTH set to
128, and MASKED LM PROB is set to 0.15.

To ensure the embeddings is training at the right architecture,
please make sure the bert conﬁg.json ﬁle referred in the
script has the right numbers of hidden and attention layers.

A.2.2. BERT MODEL TRAINING AND EXPORTING

at

The jupyter notebook for BERT intent classiﬁcation
training, validation, prediciton and exporting
model
https://github.com/cyberyu/ava/
is
blob/master/scripts/notebooks/BERT_
run_classifier_Ava.ipynb.
The main script
run classiﬁer inmem.py is tweaked from the default
BERT script run classiﬁer.py, where a new function serv-
ing input fn(): is added. To export that model in the same
command once training is ﬁnished, the ’–do export=true’

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

need be set True, and the trained model will be exported to
directory speciﬁed in ’–export dir’ FLAG.

A.2.3. MODEL SERVING API SCRIPT

a

to

for

intent

create

jupyter

notebook

classiﬁcation,

demonstrate
We
how exported model can be served as in-memory
classiﬁer
at
https://github.com/cyberyu/ava/scripts/
notebooks/inmemory_intent.ipynb. The script
will load the entire BERT graph in memory from exported
directory, keep them in memory and provide inference
results on new questions. Please notice that in “getSess()”
function, users need to specify the correct exported
directory, and the correct embeddings vocabulary path.

located

A.2.4. MODEL INFERENCE WITH DROPOUT SAMPLING

We provide a script that performs Monte Carlo dropout
inference using in-memory classiﬁer. The script assumes
three groups of questions are saved in three separate ﬁles:
training.csv, test.csv, irrelevant.csv. Users need to specify
the number of random samples, and prediction probabilities
results are saved as corresponding pickle ﬁles. The script is
available at https://github.com/cyberyu/ava/
scripts/dropout_script.py

A.2.5. VISUALIZATION OF MODEL ACCURACY AND

UNCERTAINTY

The visualization notebook https://github.com/
cyberyu/ava/scripts/notebooks/BERT_
dropout_visualization.ipynb uses output pickle
ﬁles from the previous script
to generate histogram
distribution ﬁgures and ﬁgures 4(b) and (c).

A.3. Threshold Optimization Source Code

A.3.1. THRESHOLD FOR ENTROPY

Optimization script ﬁnding best threshold for entropy
is available at https://github.com/cyberyu/
ava/blob/master/scripts/optimization/
optimize_entropy_threshold.py.
requires Python 3.6 and Gurobi 8.1.

The script

A.3.2. THRESHOLD FOR MEAN PROBABILITY AND

STANDARD DEVIATION

Optimization script ﬁnding best mean probability thresh-
old and standard deviation threshold is available at
https://github.com/cyberyu/ava/blob/
master/scripts/optimization/optimize_
dropout_thresholds.py

A.4. Sentence Completion Source Code

depends

The complete Sentence Completion RESTFUL API
code is in https://github.com/cyberyu/ava/
scripts/sentence_completion/serve.py.
on BertForMaskedLM func-
The model
tion from Transformer package (ver 2.1.1)
to gen-
erate token probabilities. We use transformers-cli
(https://huggingface.co/transformers/
converting_tensorflow_models.html) to con-
vert our early pretrained embeddings to PyTorch formats.
The input parameters for API are:

• Input sentence. The usage can be three cases:

– The input sentence can be noisy (containing mis-
spelled words) that require auto-correction. As
shown in the example, the input sentence has
some misspelled words.

– Alternatively, it can also be a masked sentence, in
the form of Does it require [MASK] signature for
IRA signup. [MASK] indicates the word needs
to be predicted. In this case, the predicted words
will not be matched back to input words. Every
MASKED word will have a separate output of
top M predict words. But the main output of the
completed sentence is still one (because it can
be combined with misspelled words and cause a
large search) .

– Alternatively, the sentence can be a complete sen-
tence, which only needs to be evaluated only for
Perplexity score. Notice the score is for the entire
sentence. The lower the score, the more usual the
sentence is.

• Beamsize: This determines how many alternative
choices the model needs to explore to complete
the sentence. We have three versions of functions,
predict oov v1, predict oov v2 and predict oov v3.
When there are multiple [MASK] signs in a sentence,
and beamsize is larger than 100, v3 function is used
as independent correction of multiple OOVs. If beam-
size is smaller than 100, v2 is used as joint-probability
based correction. If a sentence has only one [MASK]
sign, v1 (Algorithm 2 in Appendix) is used.

• Customized Vocabulary: The default vocabulary is the
encoding vocabulary when the bidirectional language
model was trained. Any words in the sentence that do
not occur in vocabulary will be treated as OOV, and
will be predicted and matched. If you want to avoid
predicting unwanted words, you can include them in
the customized vocabulary. For multiple words, com-
bine them with — and the algorithm will split them
into list. It is possible to turn off this customized vo-

AVA: A Financial Service Chatbot based on Deep Bidirectional Transformers

cabulary during runtime, which simply just put None
in the parameters.

• Ignore rule: Sometimes we expect the model to ignore
a range of words belonging to speciﬁc patterns, for
example, all words that are capitalized, all words that
start with numbers. They can be speciﬁed as ignore
rules using regular expressions to skip processing them
as OOV words. For example, expression ”[A-Z]+” tells
the model to ignore all uppercase words, so it will not
treat ‘IRA’ as an OOV even it is not in the embeddings
vocabulary (because the embeddings are lowercased).
To turn this function off, use None as the parameter.

The model returns two values: the completed sentence, and
its perplexity score.

A.5. RASA Server Source Code

The proposed chatbot utilizes RASA’s open framework to
integrate RASA’s “chitchat” capability with our proposed
customized task-oriented models. To achieve this, we set
up an additional action endpoint server to handle dialogues
that trigger customized actions (sentence completion+intent
classiﬁcation), which is speciﬁed in actions.py ﬁle. Dia-
logue management is handled by RASA’s Core dialogue
management models, where training data is speciﬁed in sto-
ries.md ﬁle. So, in RASA dialogue model.py ﬁle run core
function, the agent loads two components: nlu interpreter
and action endpoint.

The entire RASA project for chatbot is shared under
https://github.com/cyberyu/ava/bot. Please
follow the github guidance in README ﬁle to setup the
backend process.

A.6. Microsoft Teams Setup

Our chatbot uses Microsoft Teams as front-end to connect to
RASA backend. We realize setting up MS Teams smoothly
is a non-trivial task, especially in enterprise controlled env-
iornment. So we shared detailed steps on Github repo.

A.7. Connect MS Teams to RASA

At RASA side, the main tweak to allow MS Team connec-
tion is at dialogue model.py ﬁle. The BotFrameworkInput
library needs to be imported, and the correct app id and
app password speciﬁed in MS Teams setup should be as-
signed to initialize RASA InputChannel.

