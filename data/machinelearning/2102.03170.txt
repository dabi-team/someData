1
2
0
2

b
e
F
5

]

D
S
.
s
c
[

1
v
0
7
1
3
0
.
2
0
1
2
:
v
i
X
r
a

White-box Audio VST Effect Programming

Christopher Mitcheltree
Tokyo Institute of Technology
Qosmo Inc.
christhetree@gmail.com

Hideki Koike
Tokyo Institute of Technology
koike@c.titech.ac.jp

Abstract

Learning to program an audio production VST plugin is a time consuming process,
usually obtained through inefﬁcient trial and error and only mastered after extensive
user experience. We propose a white-box, iterative system that provides step-by-
step instructions for applying audio effects to change a user’s audio signal towards
a desired sound. We apply our system to Xfer Records Serum: currently one of
the most popular and complex VST synthesizers used by the audio production
community. Our results indicate that our system is consistently able to provide
useful feedback for a variety of different audio effects and synthesizer presets.

Figure 1: Mel spectrogram progression of our system applying three effects to a user’s input audio.

1

Introduction and Background

Sound design is the process of using a synthesizer and audio effects to craft a desired output sound,
typically by leveraging virtual studio technology (VST) on a computer. Often, the audio effects
applied to the synthesizer play the biggest role in producing a desired sound. Sound design for the
music industry is a very difﬁcult task done by professionals with years of experience. Educational
tools are limited and beginners are usually forced to learn via trial and error or from online resources
created by others more experienced who typically also learned in a similar way.

Prior work leveraging AI to program audio VSTs uses genetic algorithms [1; 2; 3], genetic pro-
gramming [3], k-means clustering + tree-search [4], and deep convolutional neural networks [2; 5]
to achieve this objective. There has also been research on using deep learning to model or apply
audio effects directly [6; 7; 8; 9]. These systems typically suffer from one or more of the following
problems: they are applied to toy VSTs with little practical use, they are incompatible with existing
VSTs, their inference time is prohibitively long, or they are black-boxes with uninterpretable results.

When using an AI assisted system, the user’s sense of ownership over their work should be preserved.
Our system is inspired by white-box automatic image post-processing systems [10] and collaborative
production tools [11; 12] that can educate and augment a user rather than aiming to replace them.

4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020, Vancouver, Canada.

 
 
 
 
 
 
2 System Overview

Our system iteratively nudges an input audio towards the same timbre of a desired target audio and
provides interpretable intermediate steps. It uses, to the best of our knowledge, a novel approach
consisting of an ensemble of models working together: a recurrent neural network (RNN) to select
which effect to apply next and then a collection of convolutional neural networks (CNN), one per
supported effect, to apply the correct effect parameters. An example sequence of spectrograms and
steps output by our system is shown in Figure 1.

We collect training data using ﬁve timbre-changing audio effects from Serum’s effects rack: multi-
band compression, distortion, equalizer (EQ), phaser, and hall reverb. We also use 12 different
synthesizer presets split into three groups: basic shapes (sine, triangle, saw, and square wave),
advanced shapes (four presets), and advanced modulating shapes (four presets).

Since our system focuses on reproducing a desired timbre, we represent input audio as power dB Mel
spectrograms. Using a modiﬁed automated VST rendering tool [13], ~120k one second long audio
clips are generated for each synthesizer preset and are sampled from all possible combinations of the
ﬁve supported effects.

The CNN effect models take as input two spectrograms stacked together (two channels total): the
target spectrogram and the input spectrogram. Their outputs vary depending on the effect they
are modeling, but consist of some combination of binary, categorical, and continuous outputs. A
Cartesian product is used for selecting current and target spectrograms to train on, thus resulting in
~1.2M available training data points for each effect.

The RNN model takes as input an arbitrary length sequence consisting of the CNN effect model
spectrogram input and a sequence of one hot vectors of the same length representing the used effects.
Its output is a 5-dimensional softmax layer indicating the probability of the next effect to be applied.

More details about data collection, model architectures, and training can be found in supplemental
Figures 2 and 3 and supplemental Table 2.

3 Evaluation and Discussion

Table 1: Mean errors and ∆s for input and output audio from the basic shapes preset group.

Mean Error against Target Audio

Mean Error ∆ per Step

Metric

Init. Audio

Final Audio ∆

1

2

3

4

5

MSE
MAE
MFCC
LSD

0.055
0.172
157.15
16.16

0.012
0.074
70.06
7.62

-0.043
-0.098
-87.09
-8.54

-0.024
-0.052
-45.97
-4.62

-0.013
-0.034
-28.64
-3.04

-0.008
-0.020
-20.13
-1.64

-0.004
-0.008
-7.30
-0.60

-0.002
-0.004
-4.28
-0.32

The CNN effect models are evaluated individually against their parameter reconstruction ability and
how closely their output matches the target audio. Audio similarity is measured via four different
metrics: MAE and MSE between the two power dB spectrograms, and the mean Euclidean distance
between the ﬁrst 20 Mel frequency cepstral coefﬁcients (MFCC) and the log-spectral distance (LSD)
between the two power spectrograms. The RNN model is evaluated against its prediction accuracy
for the next effect and the entire ensemble of models is evaluated against changes in audio similarity
as steps are taken by the system. Evaluation results for our entire system are shown in Table 1 and
additional results can be found in supplemental Tables 3, 4, 5, and 6.

The results indicate that our system is consistently able to produce intermediate steps that bring
the input audio signiﬁcantly closer to the target audio.1 Our system also provides near real-time,
quantitative feedback about which effects are the most important. The user can pick and choose
which intermediate steps they would like to use and can feed tweaked versions back into the system
for additional ﬁne-tuning or to learn more. We also noticed fun creative applications when our system
produced unexpected results or was given signiﬁcantly out of domain target audio.

1Audio examples can be listened to at https://bit.ly/serum_rnn

2

4 Ethical Implications

Combining artiﬁcial intelligence with creativity carries with it various different ethical considerations,
one of which is a potential future decrease in demand for professional audio producers due to an
increasing ability to replace them with technology. We believe the best approach to this is to build
systems that are collaborative and can augment people rather than replacing them entirely. While
we believe our research is just the tip of the iceberg for building AI powered sound design tools, we
can imagine a future where tools like ours might be able to ﬁnd more efﬁcient and simpler methods
of creating sounds, thus educating students more effectively and democratizing sound design. We
compare this to a similar situation that occurred when beat-matching technology was invented and
added to DJ systems (to the disgruntlement of some DJing "purists"). However, this sometimes
controversial technology democratized DJing and enabled a new generation of artists to focus on new
creative applications, thus progressing the community as a whole.

5 Acknowledgements

We would like to thank Erwin Wu for providing additional computing resources.

6 References

[1] Tatar, K., Matthieu Macret and P. Pasquier. “Automatic Synthesizer Preset Generation with PresetGen.”
Journal of New Music Research 45 (2016).

[2] Yee-King, Matthew, Leon Fedden and Mark d’Inverno. “Automatic Programming of VST Sound Synthesizers
Using Deep Networks and Other Techniques.” IEEE Transactions on Emerging Topics in Computational
Intelligence 2 (2018).

[3] Macret, Matthieu and P. Pasquier. “Automatic design of sound synthesizers as pure data patches using
coevolutionary mixed-typed cartesian genetic programming.” GECCO ’14 (2014).

[4] Cáceres, Juan-Pablo. “Sound Design Learning for Frequency Modulation Synthesis Parameters.” (2007).

[5] Barkan, Oren, David Tsiris, O. Katz and Noam Koenigstein. “InverSynth: Deep Estimation of Synthesizer
Parameter Conﬁgurations From Audio Signals.” IEEE/ACM Transactions on Audio, Speech, and Language
Processing 27 (2019).

[6] Ramírez, M. A. M. and J. Reiss. “End-to-end equalization with convolutional neural networks.” International
Conference on Digital Audio Effects (2018).

[7] Damskägg, Eero-Pekka, Lauri Juvela, Etienne Thuillier and V. Välimäki. “Deep Learning for Tube Ampliﬁer
Emulation.” ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) (2019).

[8] Sheng, Di and Gyorgy Fazekas. “A Feature Learning Siamese Model for Intelligent Control of the Dynamic
Range Compressor.” 2019 International Joint Conference on Neural Networks (IJCNN) (2019).

[9] Engel, J., Lamtharn Hantrakul, Chenjie Gu and Adam Roberts. “DDSP: Differentiable Digital Signal
Processing.” 2020 International Conference on Learning Representations (ICLR) (2020).

[10] Hu, Yuanming & He, Hao & Xu, Chenxi & Wang, Baoyuan & Lin, Stephen. "Exposure: A White-Box
Photo Post-Processing Framework." ACM Transactions on Graphics. (2017).

[11] Sommer, Nathan and A. Ralescu. “Developing a Machine Learning Approach to Controlling Musical
Synthesizer Parameters in Real-Time Live Performance.” MAICS (2014).

[12] Thio, Vibert, and Chris Donahue. "Neural Loops." 2019 NeurIPS Workshop on Machine Learning for
Creativity and Design (2019).

[13] Fedden, Leon. "RenderMan". GitHub. https://github.com/fedden/RenderMan (accessed 2020).

3

7 Supplemental Material

7.1 Data Collection

Table 2: Parameters sampled from the Serum VST synthesizer.

Effect

Parameter Name

Type

Sampled Values

Compressor Low-band Compression
Compressor Mid-band Compression
Compressor High-band Compression
Distortion
Distortion
Equalizer
Equalizer
Equalizer
Phaser
Phaser
Phaser
Hall Reverb Mix
Hall Reverb Low Frequency Cutoff
Hall Reverb High Frequency Cutoff

Continuous
Continuous
Continuous
Categorical
Mode
Continuous
Drive
High Frequency Cutoff
Continuous
High Frequency Resonance Continuous
Continuous
High Frequency Gain
Continuous
LFO Depth
Continuous
Frequency
Continuous
Feedback
Continuous
Continuous
Continuous

[0.0, 1.0]
[0.0, 1.0]
[0.0, 1.0]
12 classes
[0.3, 1.0]
[0.50, 0.95]
[0.0, 1.0]
[0.0, 0.4] and [0.6, 1.0]
[0.0, 1.0]
[0.0, 1.0]
[0.0, 1.0]
[0.3, 0.7]
[0.0, 1.0]
[0.0, 1.0]

Data collection and processing systems represent a signiﬁcant portion of the software engineering
effort required for this project. Table 2 summarizes which Serum synthesizer parameters are sampled
for each supported effect. Parameter sampling value ranges are occasionally limited to lie within
practical, everyday use regions.

The basic shapes preset group consists of the single oscillator sine, triangle, saw, and square wave
default Serum presets.

The advanced shapes preset group consists of the dry (no effects) dual oscillator "LD Power 5ths",
"SY Mtron Saw", "SY Shot Dirt Stab", and "SY Vintage Bells" default Serum presets.

The advanced modulating shapes preset group consists of
the dry dual oscillator "LD
Iheardulike5ths", "LD Postmodern Talking", "SQ Busy Lines", and "SY Runtheharm"
default Serum presets. All of these presets also use intense time varying modulations.

Audio samples are played and rendered for one second using a MIDI pitch of C4, maximum velocity,
and a sampling rate of 44100 Hz. In the future we would like to include audio pitch, attack, decay,
sustain, and release features directly into our system by modifying these values.

Mel spectrograms are calculated using a hop length of 512 samples, a FFT window length of 4096,
and 128 Mel ﬁlters.

7.2 Modeling

Figure 2: CNN effect model architecture (not to scale).

4

All ﬁve CNN effect models use ELU activations and a 50% dropout rate for each of their fully
connected (FC) layers. Their architecture is shown in Figure 2. They are trained using a batch size
of 128, mean squared error loss for continuous parameters, binary cross-entropy loss for binary
parameters, and categorical cross-entropy loss for categorical parameters.

Figure 3: CNN model architecture used in the RNN next effect prediction model (not to scale).

The RNN model consists of a bi-directional, 128-dimensional LSTM layer followed by a 128-
dimensional FC layer and lastly the 5-dimensional softmax output layer. The FC layer uses ELU
activation units and a 50% dropout rate. Features are extracted from the Mel spectrogram sequence
input using a smaller, time-distributed CNN with an architecture displayed in Figure 3. This sequence
of extracted, 128-dimensional Mel spectrogram features is concatenated with the sequence of one hot
vectors representing which effects have been used and is then fed as input to the LSTM layer. The
RNN model is trained with a batch size of 32.

All models are trained for 100 epochs with early stopping and a validation and test split of 0.10 and
0.05 respectively. The Adam optimizer is used with a learning rate of 0.001.

7.3 Evaluation

Table 3: CNN effect models mean error reduction for all three preset groups.

Effect

Metric

Basic Shapes Adv. Shapes Adv. Mod. Shapes

Mean Error ∆ against Target Audio

-0.012
Compressor MSE
Compressor MAE
-0.050
Compressor MFCC -53.19
Compressor LSD

-4.20

Distortion
Distortion
Distortion
Distortion

Equalizer
Equalizer
Equalizer
Equalizer

Phaser*
Phaser*
Phaser*
Phaser*

-0.036
MSE
MAE
-0.062
MFCC -60.50
LSD

-5.30

-0.004
MSE
MAE
-0.018
MFCC -24.63
LSD

-1.31

0.002
0.005

MSE
MAE
MFCC 1.23
0.64
LSD

-0.016
Hall Reverb MSE
Hall Reverb MAE
-0.064
Hall Reverb MFCC -47.16
Hall Reverb LSD

-6.27

-0.013
-0.049
-54.66
-4.15

-0.019
-0.056
-60.70
-5.11

-0.009
-0.038
-45.20
-3.27

0.000
-0.002
-5.98
-0.11

-0.005
-0.029
-26.61
-2.46

5

-0.007
-0.030
-37.42
-2.42

-0.037
-0.082
-88.40
-7.18

-0.005
-0.019
-29.93
-1.54

0.002
0.008
1.96
0.99

-0.007
-0.033
-31.59
-3.10

* It’s important to note that Mel spectrograms do not represent phase information well. As a result,
the error metrics used are less representative of the phaser effect’s error. A positive error ∆ may
occur even when the predicted audio sample clearly sounds much closer to the target audio sample
when compared to the initial audio sample. We plan to include phase information in future iterations
of our system.

Table 4: RNN model next effect prediction accuracy for all three preset groups.

Mean Next Effect Prediction Accuracy

Step Basic Shapes Adv. Shapes Adv. Mod. Shapes

1
2
3
4
5

All

0.997
0.983
0.981
0.973
0.999

0.983

0.993
0.985
0.979
0.988
1.000

0.985

0.997
0.989
0.983
0.977
0.997

0.986

Table 5: Mean errors and ∆s for input and output audio from the advanced shapes preset group.

Mean Error against Target Audio

Mean Error ∆ per Step

Metric

Init. Audio

Final Audio ∆

1

2

3

4

5

MSE
MAE
MFCC
LSD

0.039
0.150
146.79
14.13

0.009
0.067
62.25
6.71

-0.030
-0.083
-84.54
-7.42

-0.017
-0.042
-43.45
-3.91

-0.010
-0.030
-30.36
-2.66

-0.007
-0.022
-21.87
-1.83

-0.003
-0.009
-9.00
-0.70

0.000
-0.001
-0.40
-0.05

Table 6: Mean errors and ∆s for input and output audio from the advanced mod. shapes preset group.

Mean Error against Target Audio

Mean Error ∆ per Step

Metric

Init. Audio

Final Audio ∆

1

2

3

4

5

MSE
MAE
MFCC
LSD

0.049
0.181
176.37
16.90

0.013
0.077
72.52
7.74

-0.036
-0.104
-103.85
-9.16

-0.022
-0.070
-68.97
-6.17

-0.010
-0.026
-24.92
-2.22

-0.008
-0.018
-18.50
-1.49

-0.000
-0.004
-4.26
-0.33

-0.002
-0.004
-3.49
-0.32

6

