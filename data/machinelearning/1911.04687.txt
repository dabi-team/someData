MCPA: Program Analysis as Machine Learning

Marcel Böhme
Monash University, marcel.boehme@acm.org

9
1
0
2

v
o
N
2
1

]
E
S
.
s
c
[

1
v
7
8
6
4
0
.
1
1
9
1
:
v
i
X
r
a

ABSTRACT
Static program analysis today takes an analytical approach which
is quite suitable for a well-scoped system. Data- and control-flow
is taken into account. Special cases such as pointers, procedures,
and undefined behavior must be handled. A program is analyzed
precisely on the statement level. However, the analytical approach
is ill-equiped to handle implementations of complex, large-scale,
heterogeneous software systems we see in the real world. Existing
static analysis techniques that scale, trade correctness (i.e., sound-
ness or completeness) for scalability and build on strong assump-
tions (e.g., language-specificity). Scalable static analysis are well-
known to report errors that do not exist (false positives) or fail to
report errors that do exist (false negatives). Then, how do we know
the degree to which the analysis outcome is correct?

In this paper, we propose an approach to scale-oblivious grey-
box program analysis with bounded error which applies efficient
approximation schemes (FPRAS) from the foundations of machine
learning: PAC learnability. Given two parameters δ and ϵ, with prob-
ability at least (1 − δ ), our Monte Carlo Program Analysis (MCPA)
approach produces an outcome that has an average error at most ϵ.
The parameters δ > 0 and ϵ > 0 can be choosen arbitrarily close
to zero (0) such that the program analysis outcome is said to be
probably-approximately correct (PAC). We demonstrate the perti-
nent concepts of MCPA using three applications: (ϵ, δ )-approximate
quantitative analysis, (ϵ, δ )-approximate software verification, and
(ϵ, δ )-approximate patch verification.

1 INTRODUCTION
In 2018, Harman and O’Hearn launched an exciting new research
agenda: the innovation of frictionless1 program analysis techniques
that thrive on industrial-scale software systems [21]. Much progress
has been made. Tools like Sapienz, Infer, and Error Prone are rou-
tinely used at Facebook and Google [1, 9, 37]. Yet, many challenges
remain. For instance, the developers of Infer set the clear expectation
that the tool may report many false alarms, does not handle certain
language features, and can only report certain types of bugs.2 Such
tools often trade soundness or completeness for scalability. Then,
just how sound or complete is an analysis which ignores, e.g., expen-
sive pointer analysis, reflection in Java, undefined behaviors in C,
or third-party libraries for which code is unavailable?

1Friction is a technique-specific resistance to adoption, such as developers’ reluctance
to adopt tools with high false positive rates.
2https://fbinfer.com/docs/limitations.html

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.nnnnnnn

1

User-Generated Executions Per Day

Accuracy ϵ

HFD

Ro3

[46] 3.6 · 10−6 2.3 · 10−11
OSS-Fuzz 200.0 billion test inputs*
[50] 5.5 · 10−6 5.3 · 10−11
86.4 billion API requests
Netflix
6.2 billion videos watched [47] 2.1 · 10−5 7.4 · 10−10
Youtube
[47] 2.1 · 10−5 8.2 · 10−10
5.6 billion searches
Google
[51] 3.6 · 10−5 2.3 · 10−9
2.0 billion swipes
Tinder
[48] 4.2 · 10−5 3.1 · 10−9
1.5 billion user logins
Facebook
[47] 6.2 · 10−5 6.8 · 10−9
681.7 million tweets posted
Twitter
[47] 1.0 · 10−4 1.8 · 10−8
253.8 million calls
Skype
[45] 1.3 · 10−4 3.1 · 10−8
150.0 million transactions
Visa
Instagram 71.1 million photos uploaded [47] 1.9 · 10−4 6.5 · 10−8
(*) per widely-used, security-critical library, on average. 10 trillion total.

Figure 1: Daily number of executions of large software sys-
tems in 2018, and the lower (Ro3) and upper bounds (HFD)
on the accuracy ϵ of an estimate ˆµ of the probability µ that a
binary program property φ holds (e.g., bug is found). Specifi-
cally, µ ∈ [ ˆµ − ϵ, ˆµ + ϵ] with probability δ = 0.01 (i.e., 99%-CIs).

We believe that static program analysis today resembles the an-
alytical approach in the natural sciences. However, in the natural
sciences—when analytical solutions are no longer tractable—other
approaches are used. Monte Carlo methods are often the only means
to solve very complex systems of equations ab initio [19]. For in-
stance, in quantum mechanics the following multi-dimensional
integral gives the evolution of an atom that is driven by laser light,
undergoes “quantum jumps” in a time interval (0, t), and emits
exactly n photons at times tn ≥ . . . ≥ t1:
∫ tn

∫ t2

∫ t

dtn

0

0

dtn−1 . . .

dt1p[0, t )(t1, . . . , tn )

0

where p[0,t ) is the elementary probability density function [14].

Solving this equation is tractable only using Monte Carlo (MC)
integration [33]. MC integration solves an integral F = ∫ ti
0 dti f (x)
by “executing” f (x) on random inputs x ∈ Rn . Let X j ∈ [0, ti ]
be the j-th of N samples from the uniform distribution, then F is
estimated as ⟨F N ⟩ = ti
j=1 f (X j ). MC integration guarantees
N
N .
that this estimate converges to the true value F at a rate of 1/
This is true, no matter how many variables ti the integral has or
how the function f : Rn → R is “implemented”.

(cid:205)N

√

We argue that frictionless program analysis at the large, indus-
trial scale requires a fundamental change of perspective. Rather
than starting with a precise program analysis, and attempting to
carefully trade some of this precision for scale, we advocate an
inherently scale-oblivious approach. In this paper, we cast scale-
oblivious program analysis as a probably-approximately correct
(PAC)-learning problem [25], provide the probabilistic framework,
and develop several fully polynomial-time randomized approxima-
tion schemes (FPRAS). Valiant [43] introduced the PAC framework
to study the computational complexity of machine learning tech-
niques. The objective of the learner is to receive a set of samples
and generate a hypothesis that with high probability has a low gen-
eralization error. We say the hypothesis is probably, approximately
correct, where both adverbs are formalized and quantified.

 
 
 
 
 
 
Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

We call our technique Monte Carlo program analysis (MCPA).
The learner is the program analysis. The samples (more formally
Monte Carlo trials) are distinct program executions that are poten-
tially generated by different users of the software system under
normal workload. The hypothesis is the analysis outcome. Given
two parameters δ and ϵ, an MCPA produces an outcome that with
probability at least (1 −δ ) has an average error at most ϵ. It is impor-
tant to note that the analysis outcome is probably-approximately
correct w.r.t. the distribution of Monte Carlo trials. For instance, if
the executions are generated by real users, the analysis outcome is
probably-approximately correct w.r.t. the software system as it is
used by real users.

In contrast to existing program analysis techniques, MCPA

• allows to reason about the entire software systems under
normal workload, i.e., an MCPA while the system is used
normally produces an outcome w.r.t. normal system use,
• requires only greybox access to the software system, i.e., no
source code is analyzed, instead lightweight program instru-
mentation allows to monitor software properties of interest,
• provides probabilistic guarantees on the correctness and ac-
curacy of the analysis outcome; i.e., to exactly quantify the
scalability-correctness tradeoff,

• can be implemented in a few lines of Python code, and
• is massively parallelizable, scale-oblivious, and can be in-

terupted at any time.

MCPA is a general approach that can produce outcomes with
bounded error for arbitrary program analyses. This is because the
PAC-framework covers all of machine learning which can learn
arbitrary concepts, as well. Shalev-Shwartz and Ben-David [41]
provide an excellent overview of the theory of machine learning.
However, in this paper we focus here on (ϵ, δ )-approximations
of the probability that a binary property holds for an arbitrary exe-
cution of a terminating program. This has important applications,
e.g., in assessing reliability [5, 6, 16], quantifying information leaks
[34], or exposing side-channels [3]. Given parameters δ > 0 and
ϵ > 0, our MCPA guarantees that the reported estimate ˆµ is no more
than ϵ away from the true value µ with probability at least 1 − δ ,
i.e., P(| ˆµ − µ | ≥ ϵ) ≤ δ . In statistical terms, MCPA guarantees the
(1 − δ )-confidence interval ˆµ ± ϵ. Note that confidence and accuracy
parameters δ and ϵ can be chosen arbitrarily close to zero (0) to
minimize the probability of false negatives.

To illustrate the power of MCPA, we show how often well-known,
industrial-scale software systems are executed per day (Fig. 1). For
instance, Netflix handles 86.4 billion API requests (8.64 · 1010) per
day. MCPA guarantees that the following statements are true with
probability greater than 99% w.r.t. the sampled distribution:

• No matter which binary property we check or how many
properties we check simultaneously, we can guarantee a
priori that the error ϵ of our estimate(s) ˆµ of µ is bounded
between 5.3 · 10−11 ≤ ϵ ≤ 5.3 · 10−6 such that µ ∈ ˆµ ± ϵ.
• If 0 out of 86.4 billion API requests expose a security flaw,
then the true probability that there exists a security flaw that
has not been exposed is less than 5.3 · 10−11 (Sec. 4).

• If 1000 of 86.4 billion API requests expose a vulnerability,
then the true probability µ is probabilistically guaranteed to
be within µ ∈ 1.16 × 10−8 ± 1.45 × 10−9 (Sec. 5).

2

• If a patch was submitted, it would require at most 400 million
API requests (equiv. 7 minutes) to state with confidence (p-
value< 10−3) that the failure probability has indeed reduced
if none of those 400M executions exposes the bug after the
patch is applied. MCPA provides an a-priori conditional prob-
abilistic guarantee that the one-sided null-hypothesis can be
rejected at significance level α = 0.01 (Sec. 6).

• For an arbitrary program analysis, let H be a finite hypothe-
sis class containing all possible program analysis outcomes.
If MCPA chooses the outcome h ∈ H which explains the
analysis target (e.g., points-to-analysis) for all 5.6 billion daily
Google searches, then h explains the analysis targets with a
log(|H |/δ )
for “unseen” searches [41].
maximum error of ϵ ≤
5.6·109
On a high level, MCPA combines the advantages of static and
dynamic program analysis while mitigating individual challenges.
Like a static program analysis, MCPA allows to make statements
over all executions of a program. However, MCPA does not take
an analytical approach. A static analysis evaluates statements in
the program source code which requires strong assumptions about
the programming language and its features. This poses substantial
challenges for static program analysis. For instance, alias analy-
sis is undecidable [35]. Like a dynamic program analysis, MCPA
overcomes these challenges by analyzing the execution of the soft-
ware system. However, MCPA does not analyze only one execution.
Instead, MCPA allows to make statements over all executions gen-
erated from a given (operational) distribution.
The contributions of this article are as follows:
• We discuss the opportunities of employing Monte Carlo methods
as an approximate, greybox approach to large-scale quantitative
program analysis with bounded error.

• We develop several fully-polynomial randomized approximation
schemes (FPRAS) that guarantee that the produced estimate ˆµ of
the probability µ that a program property holds is no more than ϵ
away from µ with probability at least (1 − δ ). We tackle an open
problem in automated program repair: When is it appropriate to
claim with some certainty that the bug is indeed repaired?
• We evaluate the efficiency of our MCPA algorithms probabilis-
tically by providing upper and lower bounds for all 0 ≤ µ ≤ 1,
and empirically in more than 1018 simulation experiments.

2 MONTE CARLO PROGRAM ANALYSIS
Monte Carlo program analysis (MCPA) allows to derive statements
about properties of a software system that are probably approxi-
mately correct [42]. Given a confidence parameter 0 < δ < 1 and an
accuracy parameter ϵ > 0, with probability at least (1 − δ ), MCPA
produces an analysis outcome that has an average error at most ϵ.
Problem statement. In the remainder of this paper, we focus on
a special case of MCPA, i.e., to assess the probability µ that a binary
property φ holds (for an arbitrary number of such properties). The
probability that φ does not hold (i.e., that ¬φ holds) is simply (1 − µ).
Suppose, during the analysis phase, it was observed that φ holds
for the proportion ˆµ of sample executions. We call those sample
executions in the analysis phase as Monte Carlo trials. The analysis
phase can be conducted during testing or during normal usage of
the software system. The analysis outcome of an MCPA is expected
to hold w.r.t. the concerned executions, e.g., further executions of

MCPA: Program Analysis as Machine Learning

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

the system during normal usage. Our objective is to guarantee for
the concerned executions that the reported estimate ˆµ is no more
than ϵ away from the true value µ with probability at least 1 − δ ,
i.e., P(| ˆµ − µ | ≥ ϵ) ≤ δ .

Assumptions. We adopt the classical assumptions from soft-
ware testing: We assume that (i) all executions terminate, (ii) prop-
erty violation is observable (e.g., an uncaught exception), and (iii) the
Monte Carlo trials are representative of the concerned executions.
We elaborate each assumption and their consequences in the fol-
lowing sections. We make no other assumptions about the program
P or the property φ. For instance, P could be deterministic, proba-
bilistic, distributed, or concurrent, written in C, Java, or Haskell, or
a trained neural network, or a plain old program. We could define φ
to hold if an execution yields correct behavior, if a security property
is satisfied, if a soft deadline is met, if an energy threshold is not
exceeded, if the number of cache-misses is sufficiently low, if no
buffer overwrite occurs, if an assertion is not violated, et cetera.

2.1 Assumption 1: All Executions Terminate
We assume that all executions of the analyzed program P terminate.
With terminating executions we mean independent units of behav-
ior that have a beginning and an end. For instance, a user session
runs from login to logout, a transaction runs from the initial hand-
shake until the transaction is concluded, and a program method
runs from the initial method call until the call is returned. Other
examples are shown in Figure 1. This is a realistic assumption also
in software testing.

If assumption does not hold. As we require all (sample and
concerned) executions to terminate, MCPA cannot verify tempo-
ral properties, such as those expressed in linear temporal logic
(LTL). If the reader wishes to verify temporal properties over non-
terminating executions, we refer to probabilistic model checking
[22, 24, 28] or statistical model checking [29, 39]. If the reader
wishes to verify binary properties over non-terminating executions,
we suggest to use probabilistic symbolic execution [18] on partial
path constraints of bounded length k.

2.2 Assumption 2: Property Outcomes Are

Observable

We assume that the outcome of property φ ∈ {0, 1} can be auto-
matically observed for each execution. Simply speaking, we cannot
estimate the proportion ˆµ of concerned executions for which φ
holds, if we cannot observe whether φ holds for any Monte Carlo
trial during the analysis phase. This is a realistic assumption that
also exists in software testing.

Greybox access. Some properties φ can be observed externally
without additional code instrumentation. For instance, we can mea-
sure whether latency, performance, or energy thresholds are ex-
ceeded by measuring the time it takes to respond or to compute
the final result, or how much energy is consumed. Other properties
can be observed by injecting lightweight instrumention directly
into the program binary, e.g., using DynamoRIO or Intel Pin. Other
properties can be made observable at compile-time causing very
low runtime overhead. An example is AddressSanitizer [40] which
is routinely compiled into security-critical program binaries to re-
port (exploitable) memory-related errors, such as buffer overflows
and use-after-frees.

2.3 Assumption 3: Property Outcomes Are

Independent and Identically Distributed

The sequence of n MC trials is a stochastic process F {Xm }n
m=1
where Xm ∈ F is a binomial random variable that is true if φ = 1
in the m-th trial. We assume that the property outcomes F are in-
dependent and identically distributed (IID). This is a classic assump-
tion in testing, e.g., all test inputs are sampled independently from
the same (operational) distribution [16]. More generally, through-
out the analysis phase, the probability µ that φ holds is invariant;
the binomial distribution over Xm is the same for all Xm ∈ F .

Testing IID. In order to test whether F is IID, there are several
statistical tools available. For instance, the turning point test is a
statistical test of the independence of a series of random variables.
If assumption does not hold. The assumption that executions
are identically distributed does not hold for stateful programs where
the outcome of φ in one execution may depend on previous execu-
tions. In such cases, we suggest to understand each fine-grained
execution as a transition from one state to another within a single
non-terminating execution. The state transitions can then be mod-
elled as Markov chain and checked using tools such as probabilistic
model checking [22, 24].

2.4 Assumption 4: Monte Carlo Trials
Represent Concerned Executions

We assume that the Monte Carlo trials are representative of the con-
cerned executions. In other words, the executions of the software
system that were generated during the analysis phase are from
the same distribution as the executions w.r.t. which the analysis
outcome is expected to hold. This is a realistic assumption shared
with software testing, and any empirical analysis in general.

Realistic Behavior. A particular strength of MCPA compared
to existing techniques is that the software system can be analyzed
under normal workload to derive an probably-approximately cor-
rect analysis outcome that holds w.r.t. the software system as it is
normally executed.

3 MOTIVATION: CHALLENGES OF THE

ANALYTICAL APPROACH

The state-of-the-art enabling technology for quantitative program
analysis is probabilistic symbolic execution (PSE) which combines
symbolic execution and model counting. In a 2017 LPAR keynote
[44], Visser called PSE the new hammer. Indeed, we find that PSE is
an exciting, new tool in the developer’s reportoire for quantitative
program analysis problems particularly if an exact probability is re-
quired. However, the analytical approach of PSE introduces several
challenges for the analysis of large-scale, heterogeneous software
systems which MCPA is able to overcome.

3.1 Probabilistic Symbolic Execution
Suppose property φ is satisfied for all program paths I . Conceptually,
for each path i ∈ I , PSE computes the probability pi that an input
exercises i and then reports µ = (cid:205)
i ∈I pi . To compute the probability
pi that an input exercises path i, PSE first uses symbolic execution to
translate the source code that is executed along i into a Satisfiability
Modulo Theory (SMT) formula, called path condition. The path
condition π (i) is satisfied by all inputs that exercise i. A model

3

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

(cid:75)

counting tool can then determine the number of inputs
exercise i. Given the size
D
(cid:74)
probability pi to exercise path i can be computed as pi =

that
of the entire input domain D, the
.
D
(cid:75)
(cid:74)
There are two approaches to establish the model count. Exact
model counting [3, 7, 16, 18] determines the model count precisely.
For instance, LaTTE [31] computes
as the volume of a con-
vex polytope which represents the path constraint. However, an
exact count cannot be established efficiently and turns out to be
intractable in practice [2, 11].3 Hence, recent research has focussed
on PSE involving approximate model counting.

π (i)
(cid:74)
π (i)
(cid:74)

π (i)
(cid:74)

(cid:75)
/
(cid:75)

(cid:75)

π (i)
(cid:75)
(cid:74)

Approximate model counting trades accuracy for efficiency and
determines
approximately. Inputs are sampled from non-
overlapping, axis-aligned bounding boxes that together contain all
(but not only) solutions of π (i) [5, 6, 17, 32]. To determine whether
the sampled input exercises i, it is plugged into π (i) and checked for
satisfiability. The proportion of “hits” multiplied by the size of the
bounding box, summed over all boxes gives the approximate model
count. In contrast to arbitrary polytopes, the size of an axis-aligned
bounding box can be precisely determined efficiently.

3.2 Challenges
Probabilistic symbolic execution uses an incremental encoding of
the program’s source code as a set of path conditions. Whether
static or dynamic symbolic execution, the semantic meaning of
each individual program statement is translated into an equivalent
statement in the supported Satisfiability Modulo Theory (SMT).
The quantitative program analysis, whether exact or approximate,
is then conducted upon those path conditions. This indirection
introduces several limitations.

(1) Only programs that can be represented in the available SMT
theory can be analyzed precisely (e.g., bounded linear integer
arithmetic [18] or strings [7]). For approximate PSE, only
bounded numerical input domains are allowed. Otherwise,
bounding boxes and their relative size cannot be derived. Pro-
grams that are built on 3rd-party libraries, are implemented
in several languages, contain unbounded loops, involve com-
plex data structures, execute several threads simultaneously
pose challenges for probabilistic symbolic execution.
(2) Only functional properties φ can be checked; those can be
encoded within a path condition, e.g., as assertions over vari-
able values. In contrast, non-functional properties, such as
whether the response-time or energy consumption exceeds
some threshold value, are difficult to check.

(3) Only a lower bound on the probability µ that φ holds can
be provided. PSE cannot efficiently enumerate all paths for
which property φ holds. Reachability is a hard problem. Oth-
erwise, symbolic execution would be routinely used to prove
programs correct. Hence, PSE computes µ as µ ′ = (cid:205)
i ∈I ′ pi
only for a reasonable subset of paths I ′ ⊆ I that satisfy φ
[16], which is why the reported probability µ ′ ≤ µ.

3Gulwani et al. [38] observe that the “exact volume determination for real polyhedra
is quite expensive, and often runs out of time/memory on the constraints that are
obtained from our benchmarks”. Time is exponential in the number of input variables.
While a simple constraint involving four variables is quantified in less then 3 minutes in
some experiments, a similarly simple constraint involving eight variables is quantified
in just under 24 hours [31].

4

(4) No confidence intervals are available for approximate PSE.
While approximate PSE trades accuracy (of the analysis out-
come) for efficiency (of the analysis), it does not provide
any means to assess this trade-off.4 Approximate PSE pro-
vides only a (negatively biased) point estimate ˆµ of µ. Due
to the modular nature of the composition of the estimate ˆµ,
it is difficult to derive a reliable (1 − δ )-confidence interval
[ ˆµ − ϵ, ˆµ + ϵ].

(5) To provide an analysis outcome for programs under normal
workload, a usage profile must be developed that can be
integrated with the path condition. PSE requires to formally
encode user behavior as a usage profile [16]. However, de-
riving the usage profile from a sample of typical execution
can introduce uncertainty in the stochastic model that must
be accounted for [30].

We believe that many of these limitations can be addressed. Yet,
they do demonstrate a shortcoming of the analytical approach. A
static program analysis must parse the source code and interpret
each program statement on its own. It cannot rely on the compiler
to inject meaning into each statement. The necessary assumptions
limit the applicability of the analysis. The required machinery is
a substantial performance bottleneck. For PSE, we failed to find
reports of experiments on programs larger than a few hundred lines
of code. Notwithstanding, we are excited about recent advances in
scalable program analysis where correctness is carefully traded for
scalability. However, how do we quantify the error of the analysis
outcome? What is the probability of a false positive/negative?

3.3 Opportunities
Monte Carlo program analysis resolves many of these limitations.

(1) Every executable program can be analyzed. Apart from termi-
nation we make no assumptions about the software system.
(2) Every binary property φ can be checked, including a non-
functional property, as long as it can be automatically ob-
served. Moreover, the number of required trials is indepen-
dent of the number of simultaneously checked properties.
(3) The maximum likelihood estimate ˆµ for the binomial pro-
portion µ is unbiased. Unlike PSE, MCPA does not require
to identify or enumerate program paths that satisfy φ.
(4) The error is bounded. The analysis results are probably ap-
proximately correct, i.e., given the parameters δ and ϵ, with
probability (1 − δ ), we have that µ ∈ [ ˆµ − ϵ, ˆµ + ϵ].

(5) The best model of a system is the running system itself. The
system can be analyzed directly during normal execution
with analysis results modulo concerned executions.

It is interesting to note that approximate probabilistic symbolic
execution employs Monte Carlo techniques upon each path con-
dition, effectively simulating the execution of the program. Our
key observation is that the compiler already imbues each statement
with meaning and program binaries can be executed concretely
many orders of magnitutes faster than they can be simulated.

4We note that the sample variance ˆσ is not an indicator of estimator accuracy. Suppose,
µ = 0.5 such that the (true) variance σ 2 = 0.25/n is maximal. Due to randomness, we
may take n samples where φ does not hold. Our estimate ˆµ = 0 is clearly inaccurate,
yet the sample variance ˆσ 2 = 0 gives no indication of this inaccuracy.

MCPA: Program Analysis as Machine Learning

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Algorithm 1 (ϵ, δ )-Approximate Software Verification
Input: Program P, Binary properties Φ
Input: Accuracy ϵ, Confidence δ
1: Let n = log(δ )/log(1 − ϵ)
2: for Each of n Monte Carlo trials do
3:

Execute program P and observe property outcomes
for φi ∈ Φ do

4:

5:

6:

if φi does not hold in current execution
then return “Property φi violated for current execution P”

end for

7:
8: end for

return “For all φi ∈ Φ, we estimate the probability that φi holds in
P is ˆµL = (n+1)/(n+2) and guarantee that the true probability
µi ∈ [1 − ϵ, 1] with probability at least (1 − δ ).”

4 (ϵ, δ )-APPROXIMATE SOFTWARE

VERIFICATION

During the analysis of large software systems, the probablity µ
that a binary program property φ holds for an execution is often
almost or exactly one.5 For a very large number of successive Monte
Carlo trials, we might not ever observe that φ is violated. Still,
there is always some residual risk that ¬φ holds for an infinitesimal
proportion of concerned executions, i.e., (1−µ) > 0 [4]. Hence, (ϵ, δ )-
approximate software verification allows to specify an allowable
residual risk ϵ. It guarantees 0 ≤ (1 − µ) ≤ ϵ with probability (1 − δ ).
If the binary program property φ holds for all of n Monte-Carlo
trials, our empirical estimate ˆµ of the probability µ that φ holds
is ˆµ = n
= 1. In this section, we provide a better point estimate
n
ˆµL for µ in the absence of empirical evidence for the violation of
φ. Now, Höffding’s inequality already provides probabilistic error
bounds (Sec. 5): Given the confidence parameter δ , we can compute
the accuracy ϵ of the estimate ˆµ such that µ is within ˆµ ± ϵ with
probability (1 − δ ). However, in this special case we can leverage a
generalization of the rule-of-three (Ro3) to reduce the radius ϵ of
the guaranteed confidence interval by many orders of magnitude.
Point estimate. Given that the sun has risen ever since we
were born n days ago, what is the probability that the sun will rise
tomorrow? This riddle is known as the sunrise problem and the
solution is due to Laplace. Obviously, our plugin estimator ˆµ = 1 is
positively biased, as there is always some residual probability that
the sun will fail to rise tomorrow (i.e., that ¬φ holds). If we have
observed that φ holds in all of n trials, the Laplace estimate ˆµL of µ
is computed as

ˆµL = n + 1
n + 2
Error bounds. The rule-of-three [20] is widely used in the evalu-
ation of medical trials. It yields the 95%-confidence interval [0, 3/n]
for the probability of adverse effects given a trial where none of the
n participants experienced adverse effects. Given an accuracy pa-
rameter ϵ > 0, Hanley and Lippmann-Hand [20] provide an upper
bound on the probability that the estimate ˆµ deviates more than ϵ

(1)

5Note that we can reduce the alternative case—where the probablity µ that φ holds is
almost or exactly zero—to the case that we consider here. In that case, the probablity
(1 − µ) that φ does not hold (i.e., ¬φ holds) is almost or exactly one.

5

Wolfram Alpha LLC. 2019.
https://www.wolframalpha.com/input/?i=plot+log(delta)/log(1-epsilon)+for+delta=0..1,+epsilon=0..1 (access June 23,
2019).

Figure 2: The number n of Monte Carlo trials required, as
confidence δ and accuracy ϵ varies. The surface plot (left) is
a 3D plot of the function. The contour plot (right) visualizes
how n changes as δ and ϵ varies.

from the true value µ, and since ˆµ = 1 also on the probability that
µ ≥ 1 − ϵ.

P(| ˆµ − µ | ≥ ϵ) ≤ (1 − ϵ)n

(2)

Given a confidence parameter δ : 0 < δ ≤ 1, we can compute the
number n of Monte Carlo trials required to guarantee the (1 − δ )-
confidence interval [ ˆµ − ϵ, ˆµ + ϵ] as

n ≥

log(δ )
log(1 − ϵ)

(3)

Analogously, given n Monte Carlo trials, with probability at least
1 − δ , the absolute difference between the estimate ˆµ and the true
value µ is at most ϵ where

ϵ ≤ 1 − δ

1
n

(4)

Note that for a 95%-CI (i.e., δ = 0.05), we have that ϵ ≤ 3/n giving
rise to the name rule-of-three.

Efficiency. Our Algorithm 1 runs in time that is polynomial in
log(1/(1 − ϵ)))−1 < 1/ϵ and in log(1/δ ). It is thus a fully polyno-
mial randomized approximation scheme (FPRAS). The efficiency
of Algorithm 1 is visualized in Figure 2. To reduce ϵ by an order of
magnitude increases the required Monte Carlo trials also only by an
order of magnitude. This is further demonstrated for the real-world
examples shown in Figure 1.

5 (ϵ, δ )-APPROXIMATE QUANTITATIVE

ANALYSIS

Quantitative program analysis is concerned with the proportion µ
of executions that satisfy some property φ of interest. Quantitative
analysis has important applications, e.g., in assessing software reli-
ability [5, 6, 16], computing performance distributions [10], quanti-
fying information leaks [34], quantifying the semantic difference
across two program versions [15], or exposing side-channels [3].
We propose (ϵ, δ )-approximate quantitative analysis which yields
an estimate ˆµ that with probability at least (1 − δ ) has an average
error at most ϵ. While the generalized rule of three (Ro3) provides
a lower bound on the number trials needed to compute an (ϵ, δ )-
approximation of µ, Höffding’s inequality provides an upper bound.

ComputedbyWolfram|Alpha𝛿ε0.00.20.40.60.81.00.00.20.40.60.81.0(cid:1)(cid:2)ComputedbyWolfram|Alpha𝛿εTechnical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

Algorithm 2 (ϵ, δ )-Approximate Quantitative Program Analysis
(Fully-Polynomial Randomized Approximation Scheme)
Input: Program P, Binary property φ
Input: Accuracy ϵ, Confidence δ

2)

1: Let n = log(2/δ )/(2ϵ
2: Let m = 1
3: repeat
4:

Execute P, observe outcome of φ, and increment m
Let ϵ ′ be the radius of the current CP interval (cf. Eq. (9))

5:
6: until (m == n) or (ϵ ′ ≤ ϵ)
7: Let ˆµ = X /m where X is the total frequency φ has held

return “We estimate the prob. that φ holds in P is ˆµ and guarantee

that the true prob. µ ∈ ˆµ ± ϵ with prob. at least (1 − δ ).”

is found as solution to the constraint
L (1 − pL )n−k = δ
pk
2

n
(cid:213)

and

(cid:19)

(cid:18)n
k

X
(cid:213)

(cid:19)

(cid:18)n
k

U (1 − pU )n−k = δ
pk
2

k =X
where (cid:0)n
k
while pU = 1 when X = n. The radius ϵ of the CP interval is

is the binomial coefficient; pL = 0 when X = 0

n!
k !(n−k )!

(cid:1) =

k =0

2

(9)

ϵ = pU − pL
Given the same number of Monte Carlo trials, the radius ϵ of the
CP interval is lower-bounded by the generalized rule-of-three and
upper-bounded by Höffding’s inequality. The CP interval is conser-
vative, i.e., the probability that µ ∈ CI may actually be higher than
(1 − δ ), particularly for probabilities µ that are close to zero or one.
Wald interval. The most widely-used confidence intervals for
binomial proportions are approximate and cannot be used for (ϵ, δ )-
approximate quantiative analysis. The CP interval is computation-
ally expensive and conservative. Hence, the most widely-used CIs
are based on the approximation of the binomial with the normal
distribution. For instance, the well-known Wald interval [8] is
µ(1 − µ)
n

µ(1 − µ)
n

, ˆµ + zδ /2

ˆµ − zδ /2

CIWald =

(10)

(cid:114)

(cid:114)

(cid:34)

(cid:35)

where zδ /2 is the (1 − δ
2 ) quantile of a standard normal distribution.
However, as we observe in our experiments, the Wald interval
has poor coverage propertoes, i.e., for small µ, the nominal 95%-
confidence interval actually contains µ only with a 10% probability.
Wilson score interval. The radius of the Wald interval is un-
reasonably small when the estimand µ gets closer to zero or one.
The Wilson score CI mitigates this issue and has become the rec-
ommended interval for binomial proportions [8]. The Wilson score
interval [52] is

CIWilson = [ ˆµ ′ − ϵ ′, ˆµ ′ + ϵ ′]

where ˆµ ′ is the relocated center estimate

(cid:32)

ˆµ′ =

ˆµ +

(cid:33) (cid:44) (cid:32)

1 +

z2
δ /2
2n

(cid:33)

z2
δ /2
n

and ϵ ′
zδ /2

is the corrected standard deviation

ϵ ′ = zδ /2

(cid:115)

ˆµ(1 − ˆµ)
n

+

z2
δ /2
4n2

(cid:44) (cid:32)

1 +

(cid:33)

z2
δ /2
n

(11)

(12)

(13)

Wolfram Alpha LLC. 2019. https://www.wolframalpha.com/input/?i=plot+log(2/delta)/(2epsilon^2)+for+delta=0..1+epsilon=0..1 (access June 23, 2019).

Figure 3: The number n of Monte Carlo trials required, as
confidence δ and accuracy ϵ vary. The surface plot (left) is a
3D plot of the function. The contour plot (right) visualizes
how n changes as δ and ϵ vary. The bright area at the bottom
demonstrates substantial growth in n as ϵ approaches 0.

5.1 Höffding’s Inequality
Höffding’s inequality provides probabilistic bounds on the accuracy
of an estimate ˆµ of the probability µ = P(φ) that a property φ holds
for an execution of a program. The number of times φ holds is
a binomial random variable X = Bin(µ, n). The probability µ that
φ holds is a binomial proportion. An unbiased estimator of the
binomial proportion is ˆµ = X /n, i.e., the proportion of trials in
which φ was observed to hold. Given an accuracy parameter ϵ > 0,
Höffding [23] provides an upper bound on the probability that the
estimate ˆµ deviates more than ϵ from the true value µ.

P(| ˆµ − µ | ≥ ϵ) ≤ 2 exp(−2nϵ
(5)
Given a confidence parameter δ : 0 < δ ≤ 1, we can compute the
number n of sample executions required to guarantee the (1 − δ )-
confidence interval [ ˆµ − ϵ, ˆµ + ϵ] as

2)

Analogously, given n Monte Carlo trials, with probability at least
1 − δ , the absolute difference between the estimate ˆµ and the true
value µ is at most ϵ where

n >=

log(2/δ )
2ϵ 2

ϵ ≤

(cid:114) log(2/δ )
2n

(6)

(7)

Note that the probability that the absolute difference between es-
timate and true value exceeds our accuracy threshold ϵ decays
exponentially in the number of sample executions.

5.2 Binomial Proportion Confidence Intervals
6
CP interval. Clopper and Pearson [12] discuss a statistical method
to compute a (1−δ )-confidence interval CI = [pL, pU ] for a binomial
proportion µ, such that µ ∈ CI with probability at least (1 − δ ) for
all µ : 0 < µ < 1. The Clopper-Pearson (CP) interval is constructed
by inverting the equal-tailed test based on the binomial distribution.
Given the number of times X that property φ has been observed to
hold in n MC trials (i.e., X : 0 ≤ X ≤ n), the confidence interval
CICP = [pL, pU ]

(8)

6A probabilistic method starts with the underlying process and makes predictions
about the observations the process generates. In contrast, a statistical analysis starts
with the observations and attempts to derive the parameters of the underlying random
process that explain the observations. In other words, a statistical method requires
observations to compute the estimates while a probabilistic method provides a-priori
bounds on such estimates (e.g., based on Ro3 or HfD).

6

ComputedbyWolfram|Alpha𝛿ε0.00.20.40.60.81.00.00.20.40.60.81.0(cid:1)(cid:2)ComputedbyWolfram|Alphaε𝛿MCPA: Program Analysis as Machine Learning

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Algorithm 3 (ϵ, δ )-Approximate Quantitative Program Analysis
(Reduced number of Clopper-Pearson CI calculations)
Input: Program P, Binary property φ
Input: Accuracy ϵ, Confidence δ

Execute P and observe the outcome of φ

1: Let n = nnext = log(δ )/log(1 − ϵ)
2: for each of nnext Monte Carlo trials do
3:
4: end for
5: Let ˆµ = X /n where X is the total frequency φ has held
6: if 0 < ˆµ < 1 then
7:

repeat

8:

Let nnext = (cid:108)

(cid:113)

2

p0q0 + z

2(cid:1)(cid:109)
(z
where p0 = ˆµ and q0 = 1 − p0 and z = zδ /2

0 + 2ϵp0q0 + ϵ)/(cid:0)2ϵ

z2p2

0q2

for each of nnext Monte Carlo trials do

Execute P and observe the outcome of φ

9:
10:
11:
12:
13:
14:
15:
16: end if
return “We estimate the prob. that φ holds in P is ˆµ and guarantee

end for
Let n = n + nnext
Let ϵ ′ be the radius of the current CP interval (cf. Eq. (9))
Let ˆµ = X /n where X is the total frequency φ has held

until ϵ ′ ≤ ϵ

that the true prob. µ ∈ ˆµ ± ϵ with prob. at least (1 − δ ).”

Evaluation. We experimentally investigate properties of the
three confidence intervals for binomial proportions in Section 7.
We find that only the Clopper-Pearson confidence interval guaran-
tees that µ ∈ CI with probability at least (1 −δ ) for all µ : 0 ≤ µ ≤ 1.
The other two intervals cannot be used for (ϵ, δ )-approximate quan-
tiative analysis.

5.3 Quantitative Monte Carlo Program Analysis
Algorithm 2 shows the procedure of the MCPA. Höffding’s inequal-
ity provides the upper bound on the number of Monte Carlo trials re-
quired for an (ϵ, δ )-approximation of the probability µ that property
φ holds in program P (Eq. (6)). The conservative Clopper-Pearson
confidence interval provides an early exit condition (Line 6).

Efficiency. Algorithm 2 runs in time that is polynomial in 1/ϵ
and in log(1/δ ) and is thus a fully-polynomial randomized approxi-
mation scheme (FPRAS). The worst-case running time is visualized
in Figure 3. Reducing δ (i.e., increasing confidence) by an order of
magnitude less than doubles execution time while reducing ϵ (i.e.,
increasing accuracy) by an order of magnitude increases execution
time by two orders of magnitude. However, computing the Clopper-
Pearson (CP) CI after each Monte Carlo trial is expensive. In our
experiments, computing the CP interval 105 times takes 58 seconds,
on average. Computing the CP interval for each of n = 2 · 1011 test
inputs that OSS-Fuzz generates on a good day (cf. Figure 1) would
take about 3.8 years.

Optimization. Hence, Algorithm 3 predicts the number of Monte
Carlo trials needed for a Clopper-Pearson interval with radius ϵ.
The estimator in Line 9 was developed by Krishnamoorthy and
Peng [27] and requires an initial guess p0 of µ. The algorithm com-
putes the first guess p0 = ˆµL after running the minimal number of
trials required for the given confidence and accuracy parameters
(Lines 1–6)—as provided by the generalized rule-of-three (Line 1).
Subsequent guesses are computed from the improved maximum
likelihood estimate (Line 14).

Algorithm 4 Approximate Patch Verification
Input: Program Pfix, Binary property φ, Confidence δ
Input: Total nbug and unsuccessful trials Xbug in Pbug
1: Let nfix = log(δ )/log (1 − pL) where pL is the lower limit

of the Clopper-Pearson interval (cf. Eq. (8)).

2: for Each of nfix Monte Carlo trials do
3:

Execute program Pfix and observe outcome of φ
if φ does not hold in the current execution
then return “Property φ violated for current execution Pfix”

4:

5:
6: end for

return “The null hypothesis can be rejected at significance-level at
least δ , to accept the alternative that failure rate has decreased.”

6 APPROXIMATE PATCH VERIFICATION

“It turns out that detecting whether a crash is fixed or not is an
interesting challenge, and one that would benefit from further
scientific investigation by the research community. [..] How long
should we wait, while continually observing no re-occurrence of
a failure (in testing or production) before we claim that the root
cause(s) have been fixed?” [1]
Let µbug and µfix be the probability to expose an error before and
after the bug was fixed, respectively. Suppose, we have nbug exe-
cutions of the buggy program out of which Xbug = Bin(µbug, nbug)
exposed the bug. Hence, an unbiased estimator of µbug is ˆµbug =
Xbug/nbug. We call an execution as successful if no bug (of interest)
was observed. Given a confidence parameter δ : 0 < δ < 1, we
ask how many successful executions nfix of the fixed program (i.e.,
0 = Bin(µfix, nfix) = Xfix) are needed to reject the null hypothesis
with statistical significance α = δ ?

To reject the null hypothesis at significance-level δ , we require that
there is no overlap between both (1 − δ )-confidence intervals. For
the buggy program version, we can compute the Clopper-Pearson
interval CICP = [pL, pU ] (cf. Sec. 5.2). Recall that the probability
that a property φ holds is simply the complement of probability that
̸ φ holds. For the fixed program version, we leverage the generalized
1/nfix ] (cf. Eq. (4)). In order to reject
rule-of-three CIRo3 = [0, 1 − δ
the null, we seek nfix such that
1 − δ

(14)

which is true for

nfix >

(15)

1
nfix < pL
log(δ )
log (1 − pL)

where pL is the lower limit of the Clopper-Pearson CI (cf. Eq. (8)).
Efficiency. The CP limit pL is upper- and lower-bounded by the
generalized rule-of-three and Höffding’s inequality, respectively,

ˆµbug −

(cid:115) log(2/δ )
2nbug

≤ pL ≤ ˆµ −

(cid:18)

1 − δ

(cid:19)

1
nbug

(16)

for all 0 ≤ µbug ≤ 1. Hence, Algorithm 4 is an FPRAS that runs in
time that is polynomial in log(1/(1 − ˆµbug))−1 and in log(1/δ ). The
worst-case efficiency of Algorithm 4 is visualized in Figure 4. For
instance, if 800k of 200 billion executions expose a bug in Pbug, then
it requires less than 1.8k executions to reject the Null at significance
level p < 0.001 in favor of the alternative that the probability of
exposing a bug has indeed reduced for Pfix.

7

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

Wolfram Alpha LLC. 2019. https://www.wolframalpha.com/input/?i=plot+log(delta)%2Flog(1-mu%2Bsqrt(log(2%2Fdelta)%2F(2*4e6)))+where+mu=0.001..1,delta=0.001..1 (access June 23, 2019).

Figure 4: The number nfix of Monte Carlo trials required for
the patched program to reject the Null and conclude that the
failure rate has indeed decreased as significance-level δ and
failure rate ˆµbug for the buggy program vary (nbug
= 4e6).
The contour plot shows how nfix changes as δ and µbug vary.

7 EXPERIMENTS
We implemented Algorithms 1–4 into 300 lines of R code. The binom
package [13] was used to compute various kinds of confidence
intervals. R is a statistical programming language.

7.1 Approximate Quantitative Analysis

Figure 5: The observed probability that µ ∈ CI given the nom-
inal probability (1 − δ ) = 95% as µ varies. We generated 100
thousand repetitions of 100 million trials for one thousand
values of µ, i.e., ≈ 1018 Monte Carlo trials in total.

RQ1. Can the Clopper-Pearson interval CICP be used for
(ϵ, δ )-approximate quantitative analysis? An (ϵ, δ )-approxima-
tion ˆµ of a binomial proportion µ guarantees that µ ∈ [ ˆµ − ϵ, ˆµ + ϵ]
with probability at least (1 − δ ). Figure 5 shows the results for
1018 simulation experiments. For various values of µ, we generated
n = 108 trials by sampling from µ and computed the 95% confidence
interval according to the methods of Wald, Wilson, and Clopper-
Peason. We repeated each experiment 105 times and measured the
proportion of intervals that contain µ. This proportion gives the
observed while 95% is the nominal confidence-level.

Yes. The Clopper-Pearson 95%-confidence interval CICP contains µ
with probability at least 95% for all values of µ. Algorithms 2 and
3 provide valid (ϵ, δ )-approximations. However, both the Wald
and the Wilson procedures provide 95%-CIs that contain µ with
probability less than 95%, especially for µ → 1 (or µ → 0, resp.).
For instance, if µ = 1 − 10−8, the Wald 95%-confidence interval
CIWald, which is the most widely-used interval for binomial
proportions, contains µ with probability P(µ ∈ CIWald) < 80%.

Figure 6: Required MC trials n for an (ϵ, δ )-approximation
of µ. The rule-of-three (Ro3) provides the lower while Höff-
ding’s inequality (HfD) provides the upper bound for all 0 ≤
µ ≤ 1. In the first row, δ = 0.01. In the second row, ϵ = 0.01.

RQ2 (Efficiency). How efficient are Algorithms 2 and 3 w.r.t.
the probabilistic upper and lower bounds? Here, efficiency is
measured as the number of MC trials required to compute an (ϵ, δ )-
approximation of the probability µ that φ holds. The upper and
lower bounds for all µ : 0 ≤ µ ≤ 1 are given by Höffding’s in-
equality (HfD) and the generalized rule-of-three (Ro3), respectively.
Figure 6 shows the efficiency of both algorithms as µ varies, as well
as the probabilistic bounds, for various values of ϵ and δ . Each ex-
periment was repeated 1000 times and average values are reported.

Both algorithms approach the probabilistic lower bound (Ro3) as
µ → 1 (or µ → 0, resp.). For instance, for δ = 0.01, ϵ = 0.001, at
least 4603 and most 2.6 million Monte Carlo trials are required;
for µ = 10−4, Algorithm 2 requires 4809 trials, which is just
4% above the lower bound. Moreover, Algorithm 3 requires only
slightly more Monte Carlo trials than Algorithm 2 but reduces the
number of computed intervals substantially from n to at most 2.

BinaryTree

v ∈ [0, 9]
ˆµ
µ
0.9375
0.9374
0.4592
0.4589
0.5745
0.5740
0.4592
0.4591
0.5745
0.5742
0.0203
0.0202
0.0189
0.0189
0.0346
0.0347
0.0361
0.0361
0.0361
0.0361
0.1077
0.1077
0.5745
0.5742
0.5745
0.5739

Loc
0
1
2
3
4
8
9
10
11
12
13
14
15

Loc
1
4
6
7
8
9
10
13
14
15
16
17
18
19
20

BinomialHeap

v ∈ [0, 9]
ˆµ
µ
0.64451
0.64464
0.22831
0.22842
0.60350
0.60359
0.26932
0.26947
0.22831
0.22842
0.38161
0.38148
0.30389
0.30421
0.00765
0.00767
0.11863
0.11879
0.00765
0.00767
0.05552
0.05551
0.06621
0.06640
0.04787
0.04784
0.00456
0.00454
0.00765
0.00767

v ∈ [0, 499]
ˆµ
µ
0.74799
0.74833
0.31062
0.31051
0.68599
0.68618
0.37263
0.37265
0.31062
0.31051
0.40583
0.40648
0.40416
0.40398
0.00025
0.00025
0.00274
0.00275
0.00025
0.00025
0.00149
0.00150
0.00137
0.00138
0.00124
0.00125
0.00012
0.00012
0.00024
0.00025

Figure 7: Branch probabilities computed by Algorithm 3 ( ˆµ)
and JPF-ProbSym (µ). We generated sufficient MC trials to
guarantee (ϵ, δ )-approximations for all 0 ≤ µ ≤ 1 s.t. δ =
ϵ = 10−3. That is, 3.8 · 106 executions of random values v on
BinaryTree (4.6 seconds) and BinomialHeap (0.5 seconds).

8

ComputedbyWolfram|Alphaµbug𝛿0.20.40.60.81.00.20.40.60.81.0(cid:1)(cid:2)ComputedbyWolfram|Alpha𝛿µbugNominal conf. level: 95%WaldWilsonClopper−Pearson10−1010−810−610−410−210010−1010−810−610−410−210010−1010−810−610−410−210075.0%80.0%85.0%90.0%95.0%100.0%Probability (1−m)Observed conf. level P(m˛CI)Delta = 0.1Delta = 0.01Delta = 0.001Epsilon = 0.1Epsilon = 0.01Epsilon = 0.00110010−210−410−610010−210−410−610010−210−410−6102104106102104106Probability (1−m)Monte Carlo trials nAlgorithm 2Algorithm 3Upper Bound (HfD)Lower Bound (Ro3)MCPA: Program Analysis as Machine Learning

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

RQ3 (Experience with PSE). Geldenhuys et al. [18] developed
what is now the state-of-the-art of exact quantitative program analy-
sis, probabilistic symbolic execution (PSE). The authors implemented
PSE into the JPF-ProbSymb tool and evaluated it using the two pro-
grams BinaryTree and BinomialHeap. For both programs, Figure 7
shows the exact branch probabilities µ as computed by PSE and the
corresponding (ϵ, δ )-approximations as computed by Algorithm 3.
To generate a Monte Carlo trial, we randomly sampled from the
same domain that PSE is given.

As we can see, our Monte Carlo program analysis (MCPA) ap-
proximates the branch probabilities µ within the specified minimum
accuracy. For δ = ϵ = 0.001, our estimates ˆµ are usually within
±0.0005 of the true values µ. We would like to highlight that ap-
proximate PSE [5, 6, 17, 32] does not provide any such guarantees
on the accuracy of the produced estimates. We also observed that
PSE is tractable only for very small integer domains (e.g,. v ∈ [0, 9]
[18] or v ∈ [0, 100] [10]) while our MCPA works with arbitrarily
large domains just as efficiently. In fact, unlike PSE, MCPA works
for executions generated while the software is deployed and used.

The results of Alg. 3 are comparable to those produced by JPF-
ProbSym with an error ϵ that can be set arbitrarily close to zero.

However, without the additional machinery of PSE, our MCPA
algorithm produces the analysis result much faster. For Binomial-
Heap, our algorithm took half a second (0.5s) while JPF-ProbSymb
took 57 seconds. For BinaryTree, our algorithm took about 5 sec-
onds while JPF-ProbSymb took about seven minutes. In general, we
expect that a sufficiently large number of Monte Carlo trials can
be generated quickly for arbitrarily large programs (cf. Figure 1 on
page 1), which makes MCPA effectively scale-oblivious.

Versus JPF-ProbSym, our MCPA is orders of magnitude faster.

7.2 Approximate Software Verification

Figure 8: Efficiency versus residual risk. Both plots show the
number of required Monte Carlo trials n, where φ is observed
to hold, to guarantee—with an error that exceeds ϵ with prob-
ability at most δ = 0.05—that φ always holds.

RQ4 (Residual Risk). For a given allowable residual risk ϵ,
how efficient is Alg. 1 in providing the probabilistic guaran-
tee that no bug exists when none has been observed? Figure 8
demonstrates the efficiency of approximate software verification.
Alg. 1 exhibits a performance that is nearly inversely proportional
to the given allowable residual risk. For instance, it requires about
3 · 105 trials where no error is observed to guarantee that no error

exists with an error that exceeds a residual risk of ϵ = 10−5 with
probability at most 5%. Decreasing ϵ by an order of magnitude to
ϵ = 10−6 also increases the number of required trials only by an
order of magnitude to 3 · 10−6.

Algorithm 1 is highly efficient. It exhibits a performance that is
nearly inversely proportional to the given allowable residual risk.

Moreover, the Clopper-Pearson interval CICP provides an upper
limit that is very close to the probabilistic lower bound as given by
the rule-of-three—confirming our observation in RQ2.

7.3 Approximate Patch Verification

Figure 9: Efficiency versus significance. The number of re-
quired successful executions nfix of Pfix to reject the Null at
significance α = δ . The blue, dashed line shows α = 0.05. We
conducted 1015 simulation experiments, in total; i.e., 103 rep-
= 106 trials on Pbug for each of 102 values of
etitions of nbug
nfix ≤ 104 on Pfix, and 3 values of µbug.

RQ5 (Efficiency). How efficient is Algorithm 4 in rejecting
the null hypothesis at a desired p-value compared to exist-
ing hypothesis testing methodologies? In Figure 9, we compare
our MCPA to the Fisher’s exact test and the Mann-Whitney U test.
The Fisher’s exact test is the standard two-sample hypothesis test
for binomial proportions when the one of the estimates (here ˆµfix)
is zero or one.7 The Mann-Whitney U test is the standard test used
in the AB testing platform at Netflix [49, 50] to assess, e.g., perfor-
mance degradation between versions. The U test is a non-parametric
test of the null hypothesis that it is equally likely that a randomly
selected value from one sample will be less than or greater than a
randomly selected value from a second sample.

Algorithm 4 is more efficient than existing techniques for high
levels of confidence (e.g, p-value = α ≤ 0.05) on this task of ap-
proximate patch verification. The Mann-Whitney test that Netflix
uses for A/B testing performs worst on this task; the difference to
our MCPA increases as α → 0. Fisher’s exact test approaches the
performance of our MCPA as α → 0.

7We note that the popular Chi-squared test, while easier to compute, is an approxima-
tion method that cannot be used when the one of the estimates ˆµbug, ˆµfix is 0 or 1.

9

1001021041061081e+001e−021e−041e−061e−08Accuracy eSuccessful Monte Carlo trials n.Rule−of−threeClopper−PearsonHoeffding's Ineq.0·10+02.5·10+85·10+87.5·10+81·10+91e+001e−021e−041e−061e−08Accuracy eµbug=0.9µbug=0.99µbug=0.99910−310−210−110010−310−210−110010−310−210−1100100101102103104Significance level α  (=δ)Monte Carlo Trials nfixApprox. Patch Verif.Mann−Whitney U TestFisher's Exact Test 0.1  0.01  0.001 Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

7.4 Threats to Validity
As in every empirical study, there are threats to the validity of
the results. In terms of external validity, the assumptions of Monte
Carlo program analysis (MCPA) may not be realistic. A particular
advantage of simulation experiments is their generality: Our results
apply to arbitrary software systems of arbitrary size, as long as
the stochastic process of generating the Monte Carlo trials satisfies
the assumptions of MCPA. Thus, whether or not the assumptions
are realistic is the only threat to external validity. In Section 2, we
discuss these assumptions, how realistic they are, and mitigation
strategies in case they do not hold. Particularly, most assumptions
are shared with those of automatic software testing.

In terms of internal validity, we cannot guarantee that our imple-
mentation is correct, but make our script available for the reader to
scrutinize the implementation and reproduce our results.8 To mini-
mize the probability of bugs, we make use of standard R packages,
e.g., binom for the computation of the confidence intervals.

8 RELATED WORK
We are interested in the probability that a binary property
φ holds
for an execution of a terminating program. This problem is also
tackled by probabilistic symbolic execution [18]. After a discussion
of the relationship to probabilistic symbolic execution, we discuss
differences to A/B testing and runtime verification techniques.

9

Probabilistic Symbolic Execution (PSE) computes for each
path i that satisfies the property φ the probability pi using sym-
bolic execution and exact [16, 18] or approximate model counting
[5, 6, 17, 32], and then computes µ = (cid:205)
i pi to derive the proba-
bility that φ holds. In contrast to MCPA, PSE allows to analyse
non-terminating programs by fixing the maximum length of the
analyzed path. However, in contrast to PSE, MCPA does not require
a formal encoding of the operational distribution as usage profiles.
Instead, the analysis results are directly derived from actual usage
of the software system. While PSE is a static quantitative program
analysis that never actually executes the program, MCPA is a dy-
namic quantitative analysis technique that requires no access to
the source code or the heavy machinery of constraint encoding
and solving. In contrast to approximate PSE [5, 6, 17, 32], MCPA
provides probabilistic guarantees on the accuracy of the produced
point estimate ˆµ. In contrast to statistical PSE [17], MCPA does not
require an exact model count for the branch probabilities of each
if-conditional. Other differences to PSE are discussed in Section 3.
A/B Testing [26, 49] provides a statistical framework to evalu-
ate the performance of two software systems (often the new ver-
sion versus the old version) within quantifiable error. In contrast
to A/B testing, (ϵ, δ )-approximate quantitative analysis as well as
(ϵ, δ )-approximate software verification pertains to a single pro-
gram. Moreover, in simulation experiments, our (ϵ, δ )-approximate
patch verification outperforms the Fisher’s exact test and the Mann-
Whitney U test, which is the two-sample hypothesis test of choice
for A/B testing at Netflix [49, 50]. We also note that A/B testing is
subject to the same assumptions.

8https://www.dropbox.com/sh/fxiz3ycvvbi51xg/AACk0UyO_nG0UlPMEH645jbXa?
dl=0
9Quantitative program properties, such as energy, are considered by checking whether
they exceed a threshold (for an arbitrary number of thresholds).

Statistical/Probabilistic Model Checking. Informally speak-
ing, (δ, ϵ)-approximate software verification is to probabilistic and
statistical model checking as software verification is to classical
model checking. Our MCPA techniques are neither focused on tem-
poral properties nor concerned with a program’s state space. More
specifically, instead of verifying the expanding prefix of an infinite
path through the program’s state space, MCPA requires several
distinct, terminating executions (Sec. 8). In this setting, the work
on probabilistic symbolic execution [6, 16, 18] is most related.

In model checking, there is the problem of deciding whether a
model S of a potentially non-terminating stochastic system satisfies
a temporal logic property ϕ with a probability greater than or
equal to some threshold θ : S |= P ≥θ (ϕ). The temporal property ϕ
is specified in a probabilistic variant of linear time logic (LTL) or
computation tree logic (CTL). For instance, we could check: “When a
shutdown occurs, the probability of a system recovery being completed
between 1 and 2 hours without further failure is greater than 0.75”:
S |= down → P>0.75[¬fail U [1,2] up]
Probabilistic model checking (PMC) [22, 24, 28] checks such prop-
erties using a (harder-than-NP) analytical approach. Hence, PMC
becomes quickly untractable as the number of states increases. PMC
also requires a formal model of the stochastic system S as discrete-
or continuous-time Markov chain. Statistical model checking (SMC)
[29, 39] does not take an analytical but a statistical approach. SMC
checks such properties by performing hypothesis testing on a num-
ber of (fixed-length) simulations of the stochastic system S. Similar
to MCPA, Sen et al. [39] propose to execute the stochastic system
directly. However, Sen et al. assume that a “trace” is available, i.e.,
S can identify and report the sequence of states which S visits
and how long each state transition takes. The properties that are
checked concern particular state sequences and time intervals. In
contrast, the focus of our current work is on binary (rather than
temporal) properties for executions of terminating (rather than
non-terminating) systems. In our setting, the concept of time is
rather secondary. Moreover, (δ, ϵ)-approximate software and patch
verification are just two particular instantiations of MCPA.

9 CONCLUSION
We are excited about the tremendous advances that have been made
in scalable program analysis, particularly in the area of separation
logic. For instance, the ErrorProne static analysis tool is routinely
used at the scale of Google’s two-billion-line codebase [37]. The In-
fer tool has substantial success at finding bugs at Facebook scale [9].
Yet, there still remain several open challenges; e.g., the developers
of Infer set the clear expectation that the tool may report many false
alarms, does not handle certain language features, and can only re-
port certain types of bugs.10 We strongly believe that many of these
challenges are going to be addressed in the future. However, we also
feel that it is worthwhile to think about alternative approaches to
scalable program analysis. Perhaps to think about approaches that
do not attempt to maintain formal guarantees at an impractical cost.
If we are ready to give up on soundness or completeness in favor
of scalability, we should at least be able to quantify the trade-off.
The probabilistic and statistical methodologies that are presented
in this paper represent a significant progress in this direction.

10https://fbinfer.com/docs/limitations.html

10

MCPA: Program Analysis as Machine Learning

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

ACKNOWLEDGMENTS
We thank Prof. David Rosenblum for his inspiring ASE’16 keynote
speech on probabilistic thinking [36]. This research was partially
funded by the Australian Government through an Australian Re-
search Council Discovery Early Career Researcher Award (DE190100046).

REFERENCES
[1] Nadia Alshahwan, Xinbo Gao, Mark Harman, Yue Jia, Ke Mao, Alexander Mols,
Taijin Tei, and Ilya Zorin. 2018. Deploying Search Based Software Engineering
with Sapienz at Facebook. In Search-Based Software Engineering. 3–45.

[2] Sanjeev Arora and Boaz Barak. 2009. Computational Complexity: A Modern

Approach (1st ed.). Cambridge University Press, New York, NY, USA.

[3] Lucas Bang, Abdulbaki Aydin, Quoc-Sang Phan, Corina S. Păsăreanu, and Tevfik
Bultan. 2016. String Analysis for Side Channels with Segmented Oracles. In Pro-
ceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations
of Software Engineering (FSE 2016). 193–204.

[4] Marcel Böhme. 2019. Assurance in Software Testing: A Roadmap. In Proceedings of
the 41st International Conference on Software Engineering: New Ideas and Emerging
Results (ICSE-NIER ’19). IEEE Press, Piscataway, NJ, USA, 5–8.

[5] Mateus Borges, Antonio Filieri, Marcelo D’Amorim, and Corina S. Păsăreanu. 2015.
Iterative Distribution-aware Sampling for Probabilistic Symbolic Execution. In
Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering
(ESEC/FSE 2015). 866–877.

[6] Mateus Borges, Antonio Filieri, Marcelo D’Amorim, Corina S. Păsăreanu, and
Willem Visser. 2014. Compositional Solution Space Quantification for Probabilis-
tic Software Analysis. In Proceedings of the 35th ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ’14). 123–132.

[7] Tegan Brennan, Nestan Tsiskaridze, Nicolás Rosner, Abdulbaki Aydin, and Tevfik
Bultan. 2017. Constraint Normalization and Parameterized Caching for Quantita-
tive Program Analysis. In Proceedings of the 2017 11th Joint Meeting on Foundations
of Software Engineering (ESEC/FSE 2017). 535–546.

[8] Lawrence D. Brown, T. Tony Cai, and Anirban DasGupta. 2001. Interval Estima-

tion for a Binomial Proportion. Statist. Sci. 16, 2 (05 2001), 101–133.

[9] Cristiano Calcagno, Dino Distefano, Peter W. O’Hearn, and Hongseok Yang. 2011.
Compositional Shape Analysis by Means of Bi-Abduction. J. ACM 58, 6, Article
26 (Dec. 2011), 66 pages.

[10] Bihuan Chen, Yang Liu, and Wei Le. 2016. Generating Performance Distributions
via Probabilistic Symbolic Execution. In Proceedings of the 38th International
Conference on Software Engineering (ICSE ’16). 49–60.

[11] Dmitry Chistikov, Rayna Dimitrova, and Rupak Majumdar. 2017. Approximate
counting in SMT and value estimation for probabilistic programs. Acta Informat-
ica 54, 8 (01 Dec 2017), 729–764.

[12] C. J. Clopper and E. S. Pearson. 1934. The Use of Confidence or Fiducial Limits
Illstrated in the Case of the Binomial. Biometrika 26, 4 (12 1934), 404–413.
[13] Sundar Dorai-Raj. 2015. R – Binom package. https://cran.r-project.org/web/

packages/binom/binom.pdf. Accessed: 2019-06-17.

[14] R. Dum, P. Zoller, and H. Ritsch. 1992. Monte Carlo simulation of the atomic
master equation for spontaneous emission. Physical Review A 45 (Apr 1992),
4879–4887. Issue 7.

[15] A. Filieri, C. S. Pasareanu, and G. Yang. 2015. Quantification of Software Changes
through Probabilistic Symbolic Execution (N). In Proceedings of the 30th IEEE/ACM
International Conference on Automated Software Engineering (ASE’15). 703–708.
[16] Antonio Filieri, Corina S. Păsăreanu, and Willem Visser. 2013. Reliability Analysis
in Symbolic Pathfinder. In Proceedings of the 2013 International Conference on
Software Engineering (ICSE ’13). 622–631.

[17] Antonio Filieri, Corina S. Păsăreanu, Willem Visser, and Jaco Geldenhuys. 2014.
Statistical Symbolic Execution with Informed Sampling. In Proceedings of the 22Nd
ACM SIGSOFT International Symposium on Foundations of Software Engineering
(FSE 2014). 437–448.

[18] Jaco Geldenhuys, Matthew B. Dwyer, and Willem Visser. 2012. Probabilistic Sym-
bolic Execution. In Proceedings of the 2012 International Symposium on Software
Testing and Analysis (ISSTA 2012). 166–176.

[19] Brian L Hammond, William A Lester, and Peter James Reynolds. 1994. Monte

Carlo methods in ab initio quantum chemistry. Vol. 1. World Scientific.

[20] JA Hanley and A Lippman-Hand. 1983. If nothing goes wrong, is everything all
right? Interpreting zero numerators. Journal of the American Medical Association
249, 13 (1983), 1743–1745.

[21] Mark Harman and Peter O’Hearn. 2018. From Start-ups to Scale-ups: Opportuni-
ties and Open Problems for Static and Dynamic Program Analysis. Keynote at
the 18th IEEE International Working Conference on Source Code Analysis.
[22] T. Hérault, R. Lassaigne, F. Magniette, and S. Peyronnet. 2004. Approximate
Probabilistic Model Checking. In Proceedings of the 5th International Conference
on Verification, Model Checking and Abstract Interpretation (VMCAI’04). 307–329.
[23] Wassily Hoeffding. 1963. Probability Inequalities for Sums of Bounded Random

Variables. J. Amer. Statist. Assoc. 58, 301 (1963), 13–30.

11

[24] Joost-Pieter Katoen. 2016. The Probabilistic Model Checking Landscape. In
Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science
(LICS ’16). 31–45.

[25] Michael J. Kearns and Umesh V. Vazirani. 1994. An Introduction to Computational

Learning Theory. MIT Press, Cambridge, MA, USA.

[26] Ron Kohavi and Roger Longbotham. 2017. Online Controlled Experiments and

A/B Testing. Springer US, 922–929.

[27] K. Krishnamoorthy and Jie Peng. 2007. Some Properties of the Exact and Score
Methods for Binomial Proportion and Sample Size Calculation. Communications
in Statistics - Simulation and Computation 36, 6 (2007), 1171–1186.

[28] Marta Kwiatkowska, Gethin Norman, and David Parker. 2011. PRISM 4.0: Verifi-
cation of Probabilistic Real-time Systems. In Proceedings of the 23rd International
Conference on Computer Aided Verification (CAV’11). 585–591.

[29] Axel Legay, Benoît Delahaye, and Saddek Bensalem. 2010. Statistical Model
Checking: An Overview. In Proceedings of the First International Conference on
Runtime Verification (RV’10). 122–135.

[30] Yamilet R. Serrano Llerena, Marcel Böhme, Marc Brünink, Guoxin Su, and David S.
Rosenblum. 2018. Verifying the Long-Run Behavior of Probabilistic System
Models in the Presence of Uncertainty. In Proceedings of the 12th Joint meeting of
the European Software Engineering Conference and the ACM SIGSOFT Symposium
on the Foundations of Software Engineering (ESEC/FSE). 1–11.

[31] Jesús A. De Loera, Raymond Hemmecke, Jeremiah Tauzer, and Ruriko Yoshida.
2004. Effective lattice point counting in rational convex polytopes. Journal of
Symbolic Computation 38, 4 (2004), 1273 – 1302.

[32] Kasper Luckow, Corina S. Păsăreanu, Matthew B. Dwyer, Antonio Filieri, and
Willem Visser. 2014. Exact and Approximate Probabilistic Symbolic Execution
for Nondeterministic Programs. In Proceedings of the 29th ACM/IEEE International
Conference on Automated Software Engineering (ASE ’14). 575–586.

[33] Klaus Mølmer, Yvan Castin, and Jean Dalibard. 1993. Monte Carlo wave-function
method in quantum optics. Journal of the Optical Society of America B 10, 3 (Mar
1993), 524–538.

[34] Quoc-Sang Phan, Pasquale Malacaria, Corina S. Păsăreanu, and Marcelo
D’Amorim. 2014. Quantifying Information Leaks Using Reliability Analysis.
In Proceedings of the 2014 International SPIN Symposium on Model Checking of
Software (SPIN 2014). 105–108.

[35] G. Ramalingam. 1994. The Undecidability of Aliasing. ACM Trans. Program. Lang.

Syst. 16, 5 (Sept. 1994), 1467–1471.

[36] D. S. Rosenblum. 2016. The power of probabilistic thinking. In 2016 31st IEEE/ACM
International Conference on Automated Software Engineering (ASE). 3–3.
[37] Caitlin Sadowski, Edward Aftandilian, Alex Eagle, Liam Miller-Cushon, and Ciera
Jaspan. 2018. Lessons from Building Static Analysis Tools at Google. Commun.
ACM 61, 4 (March 2018), 58–66.

[38] Sriram Sankaranarayanan, Aleksandar Chakarov, and Sumit Gulwani. 2013. Static
Analysis for Probabilistic Programs: Inferring Whole Program Properties from
Finitely Many Paths. In Proceedings of the 34th ACM SIGPLAN Conference on
Programming Language Design and Implementation (PLDI ’13). 447–458.
[39] Koushik Sen, Mahesh Viswanathan, and Gul Agha. 2004. Statistical Model Check-
ing of Black-Box Probabilistic Systems. In Computer Aided Verification. 202–215.
[40] Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry
Vyukov. 2012. AddressSanitizer: A Fast Address Sanity Checker. In Proceed-
ings of the 2012 USENIX Conference on Annual Technical Conference (USENIX
ATC’12). USENIX Association, Berkeley, CA, USA, 28–28.

[41] Shai Shalev-Shwartz and Shai Ben-David. 2014. Understanding Machine Learning:

From Theory to Algorithms. Cambridge University Press, New York, NY, USA.

[42] Leslie Valiant. 2013. Probably Approximately Correct: Nature’s Algorithms for

Learning and Prospering in a Complex World. Basic Books, Inc.

[43] L. G. Valiant. 1984. A Theory of the Learnable. Commun. ACM 27, 11 (Nov. 1984),

1134–1142.

[44] Willem Visser. 2017. Probabilistic Symbolic Execution: A New Hammer. https:
//easychair.org/smart-program/LPAR-21/LPAR-21-s3-Visser.pdf Keynote at the
21st International Conference on Logic for Programming, Artificial Intelligence
and Reasoning (LPAR-21).

[45] Website. 2010.

Visa: Transactions per Day.
run-your-business/small-business-tools/retail.html. Accessed: 2019-06-17.
[46] Website. 2017. OSS-Fuzz: Five Months Later. https://testing.googleblog.com/

https://usa.visa.com/

2017/05/oss-fuzz-five-months-later-and.html. Accessed: 2017-11-13.

[47] Website. 2018. Domo: Data Never Sleeps Report 6.0. https://www.domo.com/

blog/data-never-sleeps-6/. Accessed: 2018-11-16.

[48] Website. 2018. Facebook: Company Info and Statistics. https://newsroom.fb.com/

company-info/. Accessed: 2018-11-16.

[49] Website. 2018.

at Netflix with Kayenta.
automated-canary-analysis-at-netflix-with-kayenta-3260bc7acc69.
cessed: 2018-10-13.

Netflix Tech Blog: Automated Canary Analysis
https://medium.com/netflix-techblog/
Ac-

[50] Website. 2018. Netflix TechBlog: Edge Load Balancing. https://medium.com/
netflix-techblog/netflix-edge-load-balancing-695308b5548c. Accessed: 2018-11-
16.

Technical Report @ Arxiv, Submitted: 12. Nov. 2019, Melbourne Australia

Marcel Böhme

[51] Website. 2019. Visa: Swipes per Day. https://www.gotinder.com/press?locale=en.

Accessed: 2019-06-17.

[52] Edwin B. Wilson. 1927. Probable Inference, the Law of Succession, and Statistical

Inference. J. Amer. Statist. Assoc. 22, 158 (1927), 209–212.

12

