2
2
0
2

r
p
A
6

]
E
S
.
s
c
[

2
v
1
9
4
5
1
.
3
0
2
2
:
v
i
X
r
a

Improving the Learnability of Machine Learning APIs by
Semi-Automated API Wrapping

Lars Reimann, Günter Kniesel-Wünsche
{reimann,gk}@cs.uni-bonn.de
Smart Data Analytics, Institute for Computer Science III, University of Bonn
Bonn, Germany

ABSTRACT
A major hurdle for students and professional software developers
who want to enter the world of machine learning (ML), is mastering
not just the scientific background but also the available ML APIs.
Therefore, we address the challenge of creating APIs that are easy
to learn and use, especially by novices. However, it is not clear
how this can be achieved without compromising expressiveness.
We investigate this problem for scikit-learn, a widely used ML
API. In this paper, we analyze its use by the Kaggle community,
identifying unused and apparently useless parts of the API that
can be eliminated without affecting client programs. In addition,
we discuss usability issues in the remaining parts, propose related
design improvements and show how they can be implemented by
semi-automated wrapping of the existing third-party API.

CCS CONCEPTS
• Software and its engineering → Software libraries and repos-
itories; • Computing methodologies → Machine learning.

KEYWORDS
APIs, libraries, usability, learnability, machine learning

ACM Reference Format:
Lars Reimann, Günter Kniesel-Wünsche. 2022. Improving the Learnability
of Machine Learning APIs by Semi-Automated API Wrapping. In New Ideas
and Emerging Results (ICSE-NIER’22), May 21–29, 2022, Pittsburgh, PA, USA.
ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3510455.3512789

1 INTRODUCTION
The success of machine learning (ML) is leading to an ever increas-
ing demand for data scientists. However, software developers who
try to fill this gap and have already delved into the necessary sci-
entific ML background still face the challenge of learning how to
correctly use the related ML APIs. The importance of API learnabil-
ity and usability has been emphasized in many influential studies
[3, 21, 22, 26, 27]. Therefore, we aimed to find out whether and how
the learning curve can be flattened and correct use can be fostered.
Using scikit-learn [18], a widely-adopted ML library as a case

study, we address the following research questions:

RQ1 Can the complexity of the API be reduced without affect-

ing its existing users?

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA
© 2022 Association for Computing Machinery.
This is the author’s version of the work. It is posted here for your personal use.
Not for redistribution. The definitive Version of Record was published in New Ideas
and Emerging Results (ICSE-NIER’22), May 21–29, 2022, Pittsburgh, PA, USA, https:
//doi.org/10.1145/3510455.3512789.

RQ2 Which deviations from established principles of good
API design make learning and correct use of ML APIs hard,
even for developers who have the required mathematical
and software engineering skills?

RQ3 Can learnability and correct use be improved by a differ-

ent design of the API?

RQ4 Given an improved API design, can an implementation be
derived and maintained automatically, based on the official
library? What precisely can be automated and how much
human effort is still needed?

2 API COMPLEXITY REDUCTION
Existing ML APIs, such as scikit-learn, follow a “one size fits all”
approach, irrespective of user expertise. We questioned this ap-
proach, assuming that novices need just a subset of the API and are
hence distracted and confused by additional functionalities [27]. We
consider novices to be persons who aim to develop an ML applica-
tion and have the required mathematical and software engineering
background, but no experience using the particular ML API.

2.1 Empirical Results
To verify our assumption, we aimed to quantify the amount of
functionalities that novice users do not need. However, lacking a
representative pool of programs written only by novices we started
with a broader analysis of all usages that we could find. We did this
in two steps:

API statistics We counted the number of API elements in
scikit-learn version 0.24.2. In Fig. 1, the dark blue bars indi-
cate the total total numbers of classes, functions, and param-
eters, whereas the red bars show the respective numbers of
public elements.

Usage statistics By analyzing 92,402 programs contributed to
Kaggle competitions1 by users of any skill level we found
41,867 that used scikit-learn. Within this pool, we counted
the number of uses of each class, function and function pa-
rameter. Based on this raw data, we computed the additional
two bars shown in Fig. 1. The green bars indicate the number
of classes, functions and parameters actually used in Kaggle
competitions. The light blue bar indicates useful parameters,
which are set to at least two different values. We call pa-
rameters that are always set to the same value useless, since
they can be replaced by a local variable in the containing
function and eliminated from the API. An example of a use-
less parameter is copy_X (optional parameter with default
True) in the Lasso model, since it is set to True in all 748

1https://www.kaggle.com/competitions

 
 
 
 
 
 
ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

Lars Reimann, Günter Kniesel-Wünsche

(a) Classes

(b) Functions

(c) Parameters

Figure 1: Numbers of classes, functions, and parameters in
scikit-learn: Dark blue for the entire API, red for public
elements, green for elements used in Kaggle competitions,
light blue for parameters that are set to at least two differ-
ent values. The white area shows absolute and relative re-
ductions compared to the public API.

cases the constructor is invoked. However, the parameter
is not unused since users passed the value True three times
explicitly.

From Fig. 1 we can infer the reduction of the API’s complexity
if users were presented just with the used classes and functions
and useful parameters. This benefits novices and experts alike: It
shows that even among the public elements, 56 classes (21%) and
597 functions (47%) are unused, while 2467 parameters (57%) are
useless. These API elements can be eliminated from the user-visible
API without affecting the functionality of the analysed applications.
The improvements are huge and can be a significant time-saver
when learning a new API.

2.2 Plausibility Check
We wanted to be sure that the above results are not biased by the
choice of Kaggle competitions as sample data, but can be generalized
to other ML applications. To this end, we inspected unused classes
and functions, and useless parameters, in search of reasons why
elements are generally not needed for application development.

Unused classes We found that most unused classes are in-
deed clearly irrelevant for application programmers, being
designed just for extending the API by additional models and
internal code reuse. Examples include mixins, base classes,
and a few unused, exotic ML models, like MultiTaskLasso.
Unused functions Similarly, several groups of unused func-
tions are intended for API extensions only, such as validation
functions for inputs. Some are related to sample datasets
(fetch_california_housing), which are clearly not used in
applications that include their own data. However, there
are also unused core ML functions, often composite ones

(fit_transform, fit_predict etc.) that exist for API consistency,
and which we do not want to remove for this reason. We
guess they are not called because users might call the com-
ponent functions separately (e.g. fit or transform). Finally,
some introspection functions (decision_functions) are not
used either and also deserve further investigation.

Unused and useless parameters Given the sheer number of
parameters, we have not yet completed categorizing them.
However, our findings to date confirm the generality of the
empirical results, also for parameters.

Since our plausibility check suggests that the results obtained
for Kaggle can be generalized, we conclude that for scikit-learn,
the complexity of the API can indeed be reduced without affecting
existing users (RQ1) by roughly the numbers indicated in Fig. 1, by
eliminating unused and useless elements from the public API.

3 API REDESIGN
The manual inspection done for our plausibility check revealed
aspects that were not evident from the statistical data. We found
that even the used and useful part of the API is subject to design and
usability problems that deserve specific attention. This motivated
our research questions RQ2 and RQ3. In the following, we address
these questions in turn, enumerating found usability issues (RQ2)
and proposing related design improvements (RQ3).

3.1 Proliferation of Primitives
One of the design principle of scikit-learn is non-proliferation of
classes, which means classes are only added for learning algorithms.
All other concepts are expressed by primitive types, or types from
other scientific libraries, like numpy or scipy [2]. This, however,
results in long lists of parameters of primitive types, contradict-
ing established design principles. McConnell’s guideline, based on
psychological research, is to use at most seven parameters [14, p.
178]. Martin ([13, p. 288]) suggest to keep parameter lists as short
as possible and to avoid more than three parameters. While the
removal of unused parameters (Sec. 2.1) alleviates the problem, it
does not eliminate it. This section discusses related issues and ways
to address them without sacrificing the generality of the API:

Tangled concerns The long parameter lists of the construc-
tors of ML models hide the important hyperparameters be-
tween parameters that are unrelated to the core ML concern,
such as debugging or performance options. For example, the
constructor of the SVC model has 15 parameters (all of which
are used) for a multitude of concerns: kernel is a hyperparam-
eter, while verbose is for debugging, and cache_size concerns
performance.
Such parameters can be removed from the constructors of
ML models leaving only the hyperparameters, drastically
increasing their visibility. If needed, attributes for non-ML
concerns in the created object can anyway be set to non-
default values before calling its fit method to start training.
Implicit dependencies Flat lists of inter-dependent param-
eters can allow combinations of parameter values that are
semantically wrong. For instance, it is non-obvious that the
degree parameter of the SVC constructor should only be set
if the kernel parameter is set to ''poly''. The invocation SVC(

Improving the Learnability of Machine Learning APIs by Semi-Automated API Wrapping

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

kernel=''linear'', degree=3) is possible but most surely an error,
since the degree value will be ignored. However, SVC(kernel=
''poly'', degree=3) is correct.
Grouping dependent parameters in one object, called a pa-
rameter object in [4], improves understandabilty [11], makes
dependencies clear, and prevents any accidental misuse of
the API. In our example, we create different variants of Kernel
objects, with a degree parameter only in the ''poly'' variant.
Via SVC(Kernel.linear) and SVC(Kernel.poly(degree=3)) we
can then create legal SVC configurations, but the erroneous
example above will be impossible to create.

Magic strings Scikit-learn often uses string parameters that
only have a limited set of legal values2. This has the downside
that statically ensuring use of a valid value is difficult. This
problem is similar to the anti-pattern of using integers to
encode such limited value sets [1, 13].
The problem is worsened by the fact that scikit-learn some-
times does not validate parameters. For example, the crite-
rion parameter of the DecisionTreeClassifier should either be
''gini'' or ''entropy''. However, the creation of an instance with
the call DecisionTreeClassifier(criterion=''giny'') is silently ac-
cepted but, when we later call the fit method on the model,
the program crashes. The error message KeyError: ''giny''
gives us only a vague hint about the source of the error.
Using instead enums in conjunction with type hints3 lets
static analysis tools ensure that a correct value is passed.

The proposed introduction of parameter objects and enums
would increase the number of classes, contrary to the non-prolife-
ration of classes design of scikit-learn. It remains to be investigated
which design is easier to learn and use correctly. We are confident
that, with a careful redesign, (1) the improved error-resilience of
the redesigned API is worth the effort and (2) the increase in the
size of the API will be outweighed by the usage-based reduction
discussed in Sec. 2.

3.2 Module Structure
Module structure is another target for API redesign: Models for
supervised learning are grouped by model category, such as Support
Vector Machine (SVM) models. The task they solve is only indicated
by a suffix on the class name. For example, SVC is an SVM model
for classification, and SVR is one for regression. Similarly, metrics
for model evaluation are grouped in sklearn.metrics, regardless of
task, even though the sets of metrics applicable to different tasks
are disjoint.

However, developers are interested in fulfilling a task, rather
than exploring a particular family of algorithms. Therefore, we
suggest grouping of models and metrics by task, like in Apache
Spark MlLib [15]. This speeds up the search for models applicable to
a task, or metrics suited to evaluate their performance, and makes
it obvious, which models and metrics can be used interchangeably.
The importance of aligning software components with tasks has
already been discussed in [9, 24, 25].

2In 2007, when scikit-learn was initially released, Python offered no alternative. Later,
the string types had to be kept for backwards compatibility.
3https://docs.python.org/3/library/typing.html (added in Python 3.5)

Annotation Meaning

@remove
@attribute

@group
@enum
@move

Remove unused & unnecessary API element.
Remove parameter from the constructor of a
model and keep it only as a model attribute.
Group dependent parameters as an object.
Replace string type with enum type.
Move class / global function to another module.

Table 1: Annotations that can be set in the annotation editor
and their implied effect on wrapper creation.

4 API WRAPPING
The design improvements discussed so far should not be misun-
derstood as a proposal for redesigning scikit-learn, a high-quality,
widely used ML library. Neither do we want to reinvent the wheel,
nor break the huge number of programs using scikit-learn.

Our vision is to wrap the existing scikit-learn library into an
API that implements the suggested improvements so that it is more
suited for novices. However, the size of the scikit-learn library
prohibits manual wrapper creation and maintenance. Thus, our ap-
proach raises the two challenges, summarized in RQ4: (1) automated
initial wrapper creation and (2) automated update of wrappers,
whenever a version of the scikit-learn library is released.

4.1 Initial Wrapper Creation
As a basis for automation, the manually derived design improve-
ments outlined previously need to be expressed in a machine-
readable form that specifies precisely which changes should be
performed on which API elements.

We do this by annotating elements of the scikit-learn API. Each
annotation type (Table 1) corresponds to one of the improvements
outlined in Sec. 2 and 3. A user can attach these annotations to
classes, functions and parameters in a web-based annotation ed-
itor4. To reduce the workload of annotators and guide them to
relevant elements, an initial set of @remove annotations is created
automatically from the usage data described in Sec. 2.1. Based on
the annotations and the original scikit-learn API, the new API is
inferred and corresponding wrappers [5] are generated.

4.2 Wrapper Update
The authors of scikit-learn follow a strict release policy aimed not
to break existing clients. API elements scheduled for removal are
deprecated two versions in advance. Renamings and moves are
implemented by deprecating the existing element and adding a new
one. Similarly, changes of legal parameter values are implemented
by deprecating the current value and adding a new one. This ad-
dition of new elements and forwarding from old to new ones is
basically a wrapping step. Thus, a lightweight evolution policy can
reflect the deprecations, additions and deletions from scikit-learn in
our adapted API, delegating the task of updating client programs to
users of our API. Additions and deletions can be identified by our
code analysis (Sec. 2.1). Deprecation information can be extracted
automatically from the scikit-learn documentation using a mix of
parsing, and rudimentary natural language processing (NLP). This
avoids having to repeat the initial manual annotation work in any

4https://github.com/lars-reimann/api-editor

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

Lars Reimann, Günter Kniesel-Wünsche

future releases. Only a fraction of the added elements might need
new annotations.

5 RELATED WORK

API usability. Among the rich body of literature about API us-
ability, other contributions that were not cited so far but have
influenced our work include the empirical study of API usability
problems by Piccioni et al. [19], the evaluation of API usability
using Human-Computer Interaction methods by Grill et al. [7],
general recommendations for API designers by Myers and Stylos
[17], and the JavaDoc alternative by Stylos et al. [23], which takes
usage data of an API as input to create better documentation. How-
ever, we only found one other assessment of APIs in the context
of ML, namely by Király et al. [10], who compiled the interfaces
and design patterns underlying scikit-learn and other ML APIs and
derived high-level guidelines for ML API design but didn’t investi-
gate their practical implications on a concrete framework, such as
scikit-learn. In addition their scitype concept also makes the notion
of tasks explicit, which provides a formal basis for the task-oriented
restructuring that we propose in Sec. 3.2.

ML for novices. Our simplified API has been developed as part
of the Simple-ML project [6, 20], as a back-end for a user-friendly
Domain Specific Language (DSL) for ML. The Simple-ML API and
DSL are embedded into an Integrated Development Environment
(IDE) for ML workflows. The entire system is tailored to the needs
of data science novices, to make ML more accessible.

Wrapper creation. Our API wrapping approach described in Sec.
4 is similar to the one of Hossny et al. [8], who use semantic anno-
tations to hide proprietary APIs of different cloud providers behind
a generic one, in order to combat vendor lock-in. Wrappers are
automatically created so they can easily be kept up-to-date.

API evolution. The vast literature about techniques to tackle the
evolution of an API and automatically keep client code up-to-date
is compiled in a recent survey [12]. However, our focus is not on
updating client code but on updating our generated wrappers. For
this, we need to be able to detect changes in scikit-learn and identify
conflicts with respect to the changes implied by our annotations. An
extensive review of related change detection and change merging
approaches can be found in [16].

6 THREATS TO VALIDITY

Usage data. For now, we only pulled usage data from Kaggle
programs that were linked to competitions. This can skew results,
since we got groups of programs that solve the same problem
and, therefore, might employ similar methods. Moreover, some
competitions were very popular, so we could get 1000 entries5,
whereas others only had a handful.

scikit-learn v1.0. In the meantime, a new version of scikit-learn
has been released. The few contained API changes6 slightly change
the absolute numbers but not the percentages reported in Sec. 2.

5The maximum that can be retrieved via Kaggle’s REST API
6https://scikit-learn.org/stable/whats_new/v1.0.html

7 FUTURE PLANS
To eliminate possible bias (Sec. 6), we want to extend the gathered
Kaggle data with additional scikit-learn usage data from GitHub
repositories. Afterwards, our plausibility check (Sec. 2.2) for pa-
rameters needs to be completed. The suggestions for API redesign
(Sec. 3) initially had to be entered manually in the annotation editor.
We have started automating this by extracting information from
the documentation of scikit-learn, using a mix of parsing (for the
structured parts of the documentation) and natural language pro-
cessing (for the rest). Task-oriented restructuring of modules (Sec.
3.2) will be partially automated by checking the suffix of model
names (e.g Classifier vs. Regressor). The annotation editor (Sec. 4)
is fully implemented. Annotation of scikit-learn is in progress and
the added annotations will be used as input for wrapper generation,
and as a test oracle for automated extraction of necessary design
changes from the documentation. Finally, the resulting API needs
to be carefully evaluated through usability studies to determine
whether it really yields the conjectured improvement.

8 CONCLUSION
In this paper, we have presented a novel approach for easing the
learning and correct use of a complex API. Unlike approaches to
learnability that focused on improving the documentation of an API
[21, 22], we investigated whether API complexity can be reduced, to
avoid overwhelming application developers. Based on analysis of
the API’s usage in 41,867 programs, we showed that 21% of classes,
47% of functions, and 57% of parameters can be eliminated, without
affecting any of the analysed programs (Fig. 1).

Analysing code and documentation of the remaining API ele-
ments, we found a proliferation of elements of primitive types,
which impedes learnability and eases API misuse. Tangling of dif-
ferent concerns in long parameter lists hides the essential hyperpa-
rameters. Hard to debug errors arise from implicit dependencies of
parameters and hidden narrowing of parameter domains in code
that behaves correctly just on a subset of the values that can be
passed (Sec. 3).

We showed that design improvements inspired by refactorings
can solve these issues. However, since we cannot refactor a third-
party API, we proposed an alternative approach: semi-automated
API wrapping. We showed how a layer of wrappers that implement
an improved API can be built and kept up-to-date with minimal
manual effort, based on analysis of the original library’s source
code and documentation, usage statistics, and code generation.

The only ML-specific argument in our discussion is the distinc-
tion between hyperparameters and parameters for other concerns.
This notion can be generalized to a distinction between domain-
specific parameters and others. Hence, semi-automated API wrap-
ping should be applicable to other domains and APIs, as a general
way to reduce API complexity and improve API design.

ACKNOWLEDGMENTS
We want to thank the reviewers for their very constructive com-
ments and inspiring suggestions for future work. This work was
partially funded by the German Federal Ministry of Education and
Research (BMBF) under project Simple-ML (01IS18054).

Improving the Learnability of Machine Learning APIs by Semi-Automated API Wrapping

ICSE-NIER’22, May 21–29, 2022, Pittsburgh, PA, USA

[23] Jeffrey Stylos, Andrew Faulring, Zizhuang Yang, and Brad A. Myers. 2009. Im-
proving API documentation using API usage information. In 2009 IEEE Sympo-
sium on Visual Languages and Human-Centric Computing (VL/HCC). 119–126.
https://doi.org/10.1109/VLHCC.2009.5295283

[24] Ferdian Thung, Shaowei Wang, David Lo, and Julia Lawall. 2013. Automatic
Recommendation of API Methods from Feature Requests (ASE’13). IEEE Press,
290–300. https://doi.org/10.1109/ASE.2013.6693088

[25] Jinshui Wang, Xin Peng, Zhenchang Xing, and Wenyun Zhao. 2013.
How developers perform feature location tasks: a human-centric and
Journal of Software: Evolution
process-oriented exploratory study.
and Process 25, 11 (2013), 1193–1224.
https://doi.org/10.1002/smr.1593
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/smr.1593

[26] Wei Wang and Michael W. Godfrey. 2013. Detecting API usage obstacles: A study
of iOS and Android developer questions. In 2013 10th Working Conference on
Mining Software Repositories (MSR). 61–64. https://doi.org/10.1109/MSR.2013.
6624006

[27] Minhaz Zibran, Farjana Eishita, and Chanchal Roy. 2011. Useful, But Usable?
Factors Affecting the Usability of APIs. Proceedings - Working Conference on
Reverse Engineering, WCRE, 151–155. https://doi.org/10.1109/WCRE.2011.26

REFERENCES
[1] Joshua Bloch. 2017. Effective Java (3rd ed.). Addison-Wesley Professional.
[2] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas
Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël
Varoquaux. 2013. API design for machine learning software: experiences from
the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining
and Machine Learning. 108–122.

[3] Cleidson R. B. de Souza and David L. M. Bentolila. 2009. Automatic evaluation
of API usability using complexity metrics and visualizations. In 2009 31st In-
ternational Conference on Software Engineering - Companion Volume. 299–302.
https://doi.org/10.1109/ICSE-COMPANION.2009.5071006

[4] Martin Fowler. 2018. Refactoring: Improving the Design of Existing Code (2nd ed.).

Addison-Wesley Professional.

[5] Erich Gamma, Richard Helm, Ralph Johnson, and John M. Vlissides. 1994. Design
Patterns: Elements of Reusable Object-Oriented Software (1st ed.). Addison-Wesley
Professional.

[6] Simon Gottschalk, Nicolas Tempelmeier, Günter Kniesel, Vasileios Iosifidis,
Besnik Fetahu, and Elena Demidova. 2019. Simple-ML: Towards a Framework for
Semantic Data Analytics Workflows. In Semantic Systems. The Power of AI and
Knowledge Graphs, Maribel Acosta, Philippe Cudré-Mauroux, Maria Maleshkova,
Tassilo Pellegrini, Harald Sack, and York Sure-Vetter (Eds.). Springer International
Publishing, Cham, 359–366.

[7] Thomas Grill, Ondrej Polacek, and Manfred Tscheligi. 2012. Methods towards
API Usability: A Structural Analysis of Usability Problem Categories. In Human-
Centered Software Engineering, Marco Winckler, Peter Forbrig, and Regina Bern-
haupt (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 164–180.

[8] Eman Hossny, Sherif Khattab, Fatma A. Omara, and Hesham Hassan. 2016.
Semantic-Based Generation of Generic-API Adapters for Portable Cloud Ap-
plications. In Proceedings of the 3rd Workshop on CrossCloud Infrastructures &
Platforms (London, United Kingdom) (CrossCloud ’16). Association for Comput-
ing Machinery, New York, NY, USA, Article 1, 5 pages. https://doi.org/10.1145/
2904111.2904117

[9] Mik Kersten and Gail C. Murphy. 2006. Using Task Context to Improve Program-
mer Productivity (SIGSOFT ’06/FSE-14). Association for Computing Machinery,
New York, NY, USA, 1–11. https://doi.org/10.1145/1181775.1181777

[10] Franz J. Király, Markus Löning, Anthony Blaom, Ahmed Guecioueur, and Raphael
Sonabend. 2021. Designing Machine Learning Toolboxes: Concepts, Principles
and Patterns. arXiv:2101.04938

[11] Guilherme Lacerda, Fabio Petrillo, Marcelo Pimenta, and Yann Gaël Guéhéneuc.
2020. Code smells and refactoring: A tertiary systematic review of challenges
and observations. Journal of Systems and Software 167 (2020), 110610. https:
//doi.org/10.1016/j.jss.2020.110610

[12] Maxime Lamothe, Yann-Gaël Guéhéneuc, and Weiyi Shang. 2021. A Systematic
Review of API Evolution Literature. ACM Comput. Surv. 54, 8, Article 171 (Oct.
2021), 36 pages. https://doi.org/10.1145/3470133

[13] Robert C. Martin. 2009. Clean Code: A Handbook of Agile Software Craftsmanship

(1st ed.). Prentice Hall.

[14] Steve McConnell. 2004. Code Complete (2nd ed.). Microsoft Press.
[15] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkatara-
man, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris
Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, and Ameet
Talwalkar. 2016. MLlib: Machine Learning in Apache Spark. J. Mach. Learn. Res.
17, 1 (Jan. 2016), 1235–1241.

[16] T. Mens. 2002. A state-of-the-art survey on software merging. IEEE Transactions
on Software Engineering 28, 5 (2002), 449–462. https://doi.org/10.1109/TSE.2002.
1000449

[17] Brad A. Myers and Jeffrey Stylos. 2016. Improving API Usability. Commun. ACM

59, 6 (May 2016), 62–69. https://doi.org/10.1145/2896587

[18] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[19] Marco Piccioni, Carlo A. Furia, and Bertrand Meyer. 2013. An Empirical Study of
API Usability. In 2013 ACM / IEEE International Symposium on Empirical Software
Engineering and Measurement. 5–14. https://doi.org/10.1109/ESEM.2013.14
[20] Lars Reimann and Günter Kniesel-Wünsche. 2020. Achieving Guidance in Applied
Machine Learning through Software Engineering Techniques. In Conference
Companion of the 4th International Conference on Art, Science, and Engineering of
Programming (Porto, Portugal) (<programming> ’20). Association for Computing
Machinery, New York, NY, USA, 7–12. https://doi.org/10.1145/3397537.3397552
[21] Martin P. Robillard. 2009. What Makes APIs Hard to Learn? Answers from
Developers. IEEE Software 26, 6 (2009), 27–34. https://doi.org/10.1109/MS.2009.
193

[22] Martin P. Robillard and Robert DeLine. 2011. A field study of API learning
obstacles. Empirical Software Engineering 16 (2011), 703–732. https://doi.org/10.
1007/s10664-010-9150-8

