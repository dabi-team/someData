Contrast Trees and Distribution Boosting

Jerome H. Friedman∗

9
1
0
2

c
e
D
8

]
L
M

.
t
a
t
s
[

1
v
5
8
7
3
0
.
2
1
9
1
:
v
i
X
r
a

Abstract

A new method for decision tree induction is presented. Given
a set of predictor variables x = (x1, x2, · · ·, xp) and two out-
come variables y and z associated with each x, the goal is to
identify those values of x for which the respective distribu-
tions of y | x and z | x, or selected properties of those distri-
butions such as means or quantiles, are most diﬀerent. Con-
trast trees provide a lack-of-ﬁt measure for statistical models
of such statistics, or for the complete conditional distribu-
tion py(y | x), as a function of x. They are easily interpreted
and can be used as diagnostic tools to reveal and then un-
derstand the inaccuracies of models produced by any learn-
ing method. A corresponding contrast boosting strategy is
described for remedying any uncovered errors thereby pro-
ducing potentially more accurate predictions. This leads to a
distribution boosting strategy for directly estimating the full
conditional distribution of y at each x under no assumptions
concerning its shape, form or parametric representation.

Keywords: prediction diagnostics, classiﬁcation, regression,
boosting, quantile regression, conditional distribution estima-
tion

1

Introduction

In statistical (machine) learning one has a system under study
with associated attributes or variables. The goal is to estimate
the unknown value of one of the variables y, given the known
joint values of other (predictor) variables x = (x1 · ··, xp) asso-
ciated with the system. It is seldom the case that a particular
set of x–values gives rise to a unique value for y. There are
quantities other than those in x that inﬂuence y whose values
are neither controlled nor observed. Specifying a particular
set of joint values for x results in a probability distribution
of possible y-values, py(y | x), induced by the varying values
of the uncontrolled quantities. Given a sample {yi, xi}N
i=1 of
previous solved cases, the goal is to estimate the distribution
py(y | x), or some of its properties, as a function of the pre-
dictor variables x. These can then be used to predict likely
values of y realized at each x.

∗Department of Statistics, Stanford University, Stanford, 94305,

USA.(jhf@stanford.edu)

1

Usually only a single property of py(y | x) is used for pre-
diction, namely a measure of its central tendency such as the
mean or median. This provides no information concerning in-
dividual accuracy of each prediction. Only average accuracy
over a set of predictions (cross-validation) can be determined.
In order to estimate individual prediction accuracy at each x
one needs additional properties of py(y | x) such as various
quantiles, or the distribution itself. These can be estimated
as functions of x using maximum likelihood or minimum risk
techniques. Such methods however do not provide a measure
of accuracy (goodness-of-ﬁt) for their respective estimates as
functions of x. There is no way to know how well the results
actually characterize the distribution of y at each x.

Contrast trees can be used to assess lack-of-ﬁt of any es-
timate of py(y | x), or its properties (mean, quantiles), as a
function of x. In cases where the ﬁt is found to be lacking,
contrast boosting applied to the output can often improve
accuracy. A special case of contrast boosting, distribution
boosting, can be used to estimate the full conditional distri-
bution py(y | x) under no assumptions. Contrast trees can
also be used to uncover concept drift and reveal discrepancies
in the predictions of diﬀerent learning algorithms.

2 Contrast trees

A classiﬁcation or regression tree partitions the space of x
- values into easily interpretable regions deﬁned by simple
conjunctive rules. The goal is to produce regions in x - space
such that the variation of y values within each is made small.
A contrast tree also partitions the x - space into similarly
deﬁned regions, but with a diﬀerent purpose. There are two
outcome variables y and z associated with each x. The goal
is to ﬁnd regions in x - space where the values of the two
variables are most diﬀerent.

In some applications of contrast trees the outcomes y and
z can be diﬀerent functions of x, y = f (x) and z = g(x), such
as predictions produced by two diﬀerent learning algorithms.
The goal of the contrast tree is then to identify regions in x -
space where the two predictions most diﬀer. In other cases the
outcome y may be observations of a random variable assumed
to be drawn from some distribution at x, y ∼ py(y | x). The
quantity z might be an estimate for some property of that dis-
tribution such as its estimated mean ˆE(y | x) or p-th quantile
ˆQp(y | x) as a function of x. One would like to identify x -

 
 
 
 
 
 
values where the estimates z appear to be the least accurate.
Alternatively z itself could be a random variable independent
of y (given x) with distribution pz(z | x) and interest is in
identifying regions of x - space where the two distributions
py(y | x) and pz(z | x) most diﬀer.

In these applications contrast trees can be used as diagnos-
tics to ascertain the lack-of-ﬁt of statistical models to data or
to other models. As with other tree based methods the uncov-
ered regions are deﬁned by conjunctive rules based on simple
logical statements concerning the variable values. Thus it
is straightforward to understand the joint predictor variable
values at which discrepancies have been identiﬁed. Such infor-
mation may temper conﬁdence in some predictions or suggest
ways to improve accuracy.

In prediction problems z is taken to be an estimate of some
property of the distribution py(y | x), or of the distribution
itself. One way to improve accuracy is to modify the pre-
dicted values z in a way that reduces their discrepancy with
the actual values as represented by the data. Contrast trees
attempt to identify regions of x - space with the largest dis-
crepancies. The z - values within in each such region can then
be modiﬁed to reduce discrepancy with y. This produces new
values of z which can then be contrasted with y using an-
other contrast tree. This process can then be applied to the
regions of the new tree thereby producing further modiﬁed
z - values. This “boosting” strategy of successively building
contrast trees on the output of previously induced trees can
be continued until the average discrepancy stops improving.

3 Building contrast trees

The data consists of N observations {xi, yi, zi}N
i=1 each with a
joint set of predictor variable values xi and two outcome vari-
ables yi and zi. Contrast trees are constructed from this data
in an iterative manner. At the M th iteration the tree parti-
tions the space of x - values into M disjoint regions {Rm}M
m=1
each containing a subset of the data {xi, yi, zi}xi∈Rm. At the
ﬁrst iteration there is a single region containing the entire
data set. Associated with any data subset is a discrepancy
measure between the y and z values of the subset

dm = D({yi}xi∈Rm, {zi}xi∈Rm).

(1)

Choice of a particular discrepancy measure depends on the
speciﬁc application as discussed in Section 4.

At the next (M + 1)st iteration each of the regions Rm
deﬁned at the M th iteration (1 ≤ m ≤ M ) are provision-
ally partitioned (split) into two regions R(l)
m ∪
R(r)
m = Rm). Each of these “daughter” regions contains its
own data subset with associated discrepancy measure d(l)
m and
d(r)
m (1). The quality of the split is deﬁned as

m and R(r)

m (R((l)

m and f (r)

where f (l)
m are the fraction of observations in the
“parent” region Rm associated with each of the two daugh-
ters.

The types of splits considered here are the same as in ordi-
nary classiﬁcation and regression trees (Breiman et al 1984).
Each involves one of the predictor variables xj . For numeric
variables splits are speciﬁed by a particular value of that vari-
able (split point) s. The corresponding daughter regions are
deﬁned by

x ∈ Rm & xj ≤ s =⇒ x ∈ R(l)
m
x ∈ Rm & xj > s =⇒ x ∈ R(r)
m .

(3)

For categorical variables (factors) the respective levels are or-
dered by discrepancy (1). The discrepancy at each respective
level of the factor for the observations in the mth region is
computed. Splits are then considered in this order.

Split quality (2) is evaluated jointly for all current regions
Rm, all predictor variables, and all possible splits of each
variable. The region with largest estimated split improvement

Im = max(d(l)

m , d(r)

m ) − dm

(4)

m , d(r)

is then chosen to create two new regions (3). Here dm (1) is
the discrepancy associated with the data in the parent region
and d(l)
m are corresponding discrepancies of the data sub-
sets in the two daughters. These two new regions replace the
corresponding parent producing M + 1 total regions. Split-
ting stops when no estimated improvement (4) is greater than
zero, the tree reaches a speciﬁed size or the observation count
within all regions is below a speciﬁed threshold.

4 Discrepancy measures

By deﬁning diﬀerent discrepancy measures contrast trees can
be applied to a variety of diﬀerent problems. Even within a
particular type of problem there may be a number of diﬀerent
appropriate discrepancy measures that can be used.

When the two outcomes are simply functions of x, y =
f (x) and z = g(x), any quantity that reﬂects their diﬀerence
in values at the same x can be used to form a discrepancy
measure such as

dm =

1
Nm X
xi∈Rm

| yi − zi |.

If y is a random variable and z is an estimate for a prop-
erty S of its conditional distribution at x, zi = ˆS(py(y | xi)),
such as its mean E(y | xi) or pth quantile Qp(y | xi), a natural
discrepancy measure is

dm = |S({yi}xi∈Rm) − M ({zi}xi∈Rm)| .

(5)

Qm(l, r) = (f (l)

m · f (r)

m ) max(d(l)

m , d(r)

m )2

Here S({yi}xi∈Rm) is the value of the corresponding statis-
in the region Rm and
tic computed for observations

(2)

2

M ({zi}xi∈Rm) is an appropriate measure of central tendency
for the corresponding estimates.

If both y ∼ py(y | x) and z ∼ pz(z | x) are both indepen-
dent random variables associated with each x, a discrepancy
measure reﬂects the distance between their respective distri-
butions. There are many proposed empirical measures of dis-
tribution distance. Every two–sample test has one. For the
examples below a variant of the Anderson–Darling (Anderson
and Darling 1954) statistic is used. Let {ti} = {yi} ∪ {zi}
represent the pooled (y, z) sample in a region Rm. Then dis-
crepancy between the distributions of y and z is taken to be

dm =

1
2Nm − 1

2Nm−1

X
i=1

(cid:12)
(cid:12)
(cid:12)

ˆFy(t(i)) − ˆFz(t(i))(cid:12)
(cid:12)
(cid:12)
i · (2Nm − i)

p

(6)

where t(i) is the ith value of t is sorted order, and ˆFy and ˆFz
are the respective empirical cumulative distributions of y and
z. Note that this discrepancy measure (6) can be employed
in the presence of arbitrarily censored or truncated data sim-
ply by employing a nonparametric method to estimate the
respective CDF’s such as Turnbul (1976) .

Discrepancy measures are often customized to particular
individual applications. In this sense they are similar to loss
criteria in prediction problems. However, in the context of
contrast trees (and boosting) there is no requirement that
they be convex or even diﬀerentiable. Several such examples
are provided in the Appendix.

5 Boosting contrast trees

As indicated above, and illustrated in the examples presented
below and in the Appendix, contrast trees can be employed
as diagnostics to examine the lack of accuracy of predic-
tive models. To the extent that inaccuracies are uncovered,
boosted contrast trees can be used to attempt to mitigate
them, thereby producing more accurate predictions. Contrast
boosting derives successive modiﬁcations to an initially spec-
iﬁed z, each reducing its discrepancy with y over the data.
Prediction then involves starting with the initial value of z
and then applying the modiﬁcations to produce the resulting
estimate.

5.1 Estimation contrast boosting

In this case z is taken to be an estimate of some parameter of
py(y | x). The z - values within each region R(1)
m of a contrast
tree can be modiﬁed z → z(1) = z + δ(1)
m ) so that
the discrepancy (1) with y is zero in that region

m (x ∈ R(1)

D({yi}xi∈R(1)

m

, {z(1)

i }xi∈R(1)

m

) = 0.

(7)

partitions of the x - space deﬁning diﬀerent regions {R(2)
m }M
1
for which this discrepancy is not small. These may be uncov-
ered by building a second tree to contrast y with z(1) produc-
ing updates

z(2) = z(1) + δ(2)

m (x ∈ R(2)

m ).

(8)

m }M

m }M

1 and corresponding updates {δ(3)

These in turn can be contrasted with y to produce new regions
{R(3)
1 . Such iterations
can be continued K times until the updates become small.
As with gradient boosting (Friedman 2001) performance ac-
curacy is often improved by imposing a learning rate. At
each step k the computed update δ(k)
m is
reduced by a factor 0 < α ≤ 1. That is, δ(k)
m in (8).
Each tree k in the boosted sequence 1 ≤ k ≤ K partitions
the x - space into a set of regions {R(k)
m }. Any point x lies
within one region mk(x) of each tree with corresponding up-
date δ(k)
mk(x). Starting with a speciﬁed initial value z0(x) the
estimate ˆz(x) at x is then

m in each region R(k)
m ← α δ(k)

ˆz(x) = z0(x) +

K

X
k=1

δ(k)
mk(x).

(9)

5.2 Distribution contrast boosting

Here y and z are both considered to be random variables in-
dependently generated from respective distributions py(y | x)
and pz(z | x). The purpose of a contrast tree is to identify
regions of x - space where the two distributions most diﬀer.
The goal of distribution boosting is to estimate a (diﬀerent)
transformation of z at each x, gx(z ), such that the distribu-
tion of the transformed variable is the same as that of y at x.
That is,

pgx (gx(z ) | x) = py(y | x).

(10)

Thus, starting with z values sampled from a known distri-
bution pz(z | x) at each x, one can use the estimated trans-
formation ˆgx(z ) to obtain an estimate ˆpy(y | x) of the y -
distribution at that x. Note that the transformation gx(z ) is
usually a diﬀerent function of z at each diﬀerent x.

The z - values within each region R(1)
m of a contrast tree can
m (z) (x ∈ R(1)
be transformed z(1) = g(1)
m ) so that the discrep-
ancy (6) with y is zero in that region. The transformation is
given by

m (z) = ˆF −1
g(1)

y (cid:16)

ˆFz(z)(cid:17)

(11)

where ˆFy(y) is the empirical cumulative distribution of y for
x ∈ R(1)
m and ˆFz(z) is the corresponding distribution of z for
x ∈ R(1)
m . This transformation function is represented by the
quantile-quantile (QQ) plot of y versus z in the region.

This in turn yields zero average discrepancy between y and
z(1) over the regions deﬁned by the terminal nodes of the cor-
responding contrast tree. However, there may well be other

As with estimation boosting, the distribution of the mod-
iﬁed (transformed) variable z(1) can then be contrasted with
that of y using another contrast tree. This produces another

3

m }M

m (z(1)) (x ∈ R(2)

region set {R(2)
1 where the distributions of y and z(1) dif-
fer. This discrepancy (6) can be removed by transforming
z(1) to match the distribution of y in each new region z(2)
= g(2)
m ). These in turn can be contrasted with
y producing new regions each with a corresponding transfor-
mation function. Such distribution boosting iterations can
be continued K times until the discrepancy between the dis-
tributions of z(K) and y becomes small in each new region.
As with estimation, moderating the learning rate by shrink-
ing each estimated transformation function towards identity
g(k)
m (z) ↼ (1 − α) z + α g(k)
m (z) usually increases accuracy at
the expense of computation (more transformations).
Predicting py(y | x) starts with a sample {zi}n

1 drawn from
the speciﬁed distribution of z, pz(z | x), at x. This x lies
within one of the regions mk(x) of each contrast tree k with
corresponding transformation function g(k)
mk(x)(·). A given
value of z can be transformed to a estimated value for y,
ˆy = ˆgx(z ), where

ˆgx(z ) = g(K)

mK (x)(g(K−1)

mK−1(x)(g(K−2)

mK−2(x) · · · g(1)

m1(x)(z))).

(12)

That is, the transformed output of each successive tree is fur-
ther transformed by the next tree in the boosted sequence.
A diﬀerent transformation is chosen at each step depending
on the region of the corresponding tree containing the partic-
ular joint values of the predictor variables x. With K trees
each containing M regions (terminal nodes) there are M K
potentially diﬀerent transformations ˆgx(z ) each correspond-
ing to diﬀerent values of x. To the extent the overall trans-
formation estimate ˆgx(z ) is accurate, the distribution of the
transformed sample {ˆyi = ˆgx(zi )}n
1 can be regarded as being
similar to that of y at x, py(y | x). Statistics computed from
the values of ˆy estimating selected properties of its distribu-
tion, or the distribution itself, can be regarded as estimates
of the corresponding quantities for py(y | x).

6 Diagnostics

In this section we illustrate use of contrast trees as diagnos-
tics for uncovering and understanding the lack-of-ﬁt of pre-
diction models for classiﬁcation and conditional distribution
estimation. Quantile regression models are examined in the
Appendix.

6.1 Classiﬁcation

Contrast tree classiﬁcation diagnostics are illustrated on the
census income data obtained from the Irvine Machine Learn-
ing repository (Kohvai 1996). This data sample, taken from
1994 US census data, consists of observations from 48842 peo-
ple divided into a training set of 32561 and an independent
test set of 16281. The outcome variable y is binary and indi-
cates whether or not a person’s income is greater than $50000

4

per year. There are 14 predictor variables x consisting of vari-
ous demographic and ﬁnancial properties associated with each
person. Here we use contrast trees to diagnose the predictions
of gradient boosted regression trees (Friedman 2001).

The predictive model produced by the gradient boosting
procedure applied to the training data set produced an error
rate of 13% on the test data. This quantity is the expected
error as averaged over all test set predictions.
It may be
of interest to discover certain x - values for which expected
error is much higher or lower. This can be ascertained by
contrasting the binary outcome variable y with the model
prediction z.

A natural discrepancy measure for this application is error

rate in each region Rm

dm =

1
Nm X
i∈Rm

I(yi 6= zi).

(13)

The goal in applying contrast trees is to uncover regions in x
- space with exceptionally high values of (13). For this pur-
pose the test data set was randomly divided into two parts
of 10000 and 6281 observations respectively. The contrast
tree procedure applied to the 10000 test data set produced 10
regions. Figure 1 summarizes these regions using the separate
6281 observation data set. The upper barplot shows the error
rate of the gradient boosting classiﬁer in each region ordered
from largest to smallest. The lower barplot indicates the ob-
servation count in each respective region. The number below
each bar is simply the contrast tree node identiﬁer for that
region. The horizontal (red) line indicates the 13% average
error rate.

As Fig. 1 indicates the contrast tree has uncovered many
regions with substantially higher error rates than the over-
all average and several others with substantially lower error
rates. The lowest error region covers 43% of the test set ob-
servations with an average error rate of 2.7%. The highest
error region covering 5.6% of the data has an average error
rate of 41% .

Each of the regions represented in Fig. 1 are easily de-
scribed. For example, the rule deﬁning the lowest error region
is

Node 4

relationship ∈ {Own-child, Husband, Not-in-family,
Other-relative}
&
education ≤ 12

Predictions satisfying that rule suﬀer only a 2.7% average
error rate. Predictions satisfying the rule deﬁning the highest
error region

30

19

21

20

29

31

11

28

10

4

Node

k
s
R

i

s
s
a
C

l

s
t
n
u
o
C

3
.
0

2
.
0

1
.
0

0
.
0

0
0
5
2

0
0
0
2

0
0
5
1

0
0
0
1

0
0
5

0

30

19

21

20

29

31

11

28

10

4

node

Figure 1: Error rate (upper) and observation count (lower) of
classiﬁcation contrast tree regions on census income data.

3

5

9

16

19

23

18

25

28

29

Node

)

1
=
y

(

b
o
r
P

s
t
n
u
o
C

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
0
0
6

0
0
0
5

0
0
0
4

0
0
0
3

0
0
0
2

0
0
0
1

0

3

5

9

16

19

23

18

25

28

29

node

Figure 2: Census income data. Upper frame: fraction of posi-
tive observations (blue) and mean probability prediction (red)
for probability contrast tree regions. Lower frame: observa-
tion count in each region.

Node 30

relationship /∈ {Own-child, Husband, Not-in-family,
Other-relative}
&
occupation ∈ { Exec-managerial, Transport-moving,
Armed-Forces }
&
education ≤ 12

have a 41% average error rate. Thus conﬁdence in salary
predictions for people in node 4 might be higher than for those
in node 30.

6.2 Probability estimation

The discrepancy measure (13) is appropriate for procedures
that predict a class identity and the corresponding contrast
tree attempts to identify x - values associated with high lev-
els of misclassiﬁcation. Some procedures such as gradient
boosting return estimated class probabilities at each x which
are then thresholded to predict class identities. In this case
the probability estimate contains information concerning ex-
pected classiﬁcation accuracy. The closer the respective class
probabilities are to each other the higher is the likelihood
of misclassiﬁcation. This shifts the issue from classiﬁcation
accuracy to probability estimation accuracy which can be as-
sessed with a contrast tree.

For binary classiﬁcation a natural discrepancy for proba-

bility estimation is

dm =

1
Nm

(cid:12)
(cid:12)
X
(cid:12)
i∈Rm
(cid:12)
(cid:12)

(yi − zi)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(14)

where y ∈ {0, 1} is the binary outcome variable and 0 < z < 1
is its predicted probability
Pr(y = 1). This (14) measures the
diﬀerence between the empirical probability of y = 1 in region
c
Rm and the corresponding average probability prediction z in
that region. The contrast tree was built on the test data set
with the gradient boosting probability estimates based on the
training data.

The top frame of Fig. 2 shows the empirical probabil-
ity y = 1 (blue) and the average gradient boosting predic-
tion z (red) within each region of the resulting contrast tree.
The bottom frame shows the number of counts in each corre-
sponding region. One sees a general trend of over-smoothing.
The large probabilities are being under-estimated whereas the
smaller ones are substantially over-estimated by the gradient
boosting procedure. In node 3 containing 40% of the observa-
tions the empirical Pr(y = 1) is 0.037 whereas the mean of the
predictions is ¯z = 0.18. In node 16 the empirical probability
is 0.05 while the gradient boosting mean prediction in that re-
gion is 0.17. In node 9 gradient boosting is under-estimating
the actual probability Pr(y = 1) = 0.84 with ¯z = 0.73. As

5

 
 
 
 
 
 
 
y
c
n
a
p
e
r
c
s
D

i

e
g
a
r
e
v
A

2
1
.
0

0
1
.
0

8
0
.
0

6
0
.
0

4
0
.
0

2
0
.
0

0
0

.

0

0.2

0.4

0.6

0.8

1.0

Fraction of Observations

Figure 3: Census income data. Lack-of-ﬁt contrast curves
comparing probability y = 1 estimates by logistic gradient
boosting (black), random forests (blue), squared-error gradi-
ent boosting (red) and contrast boosting (green).

above these regions are deﬁned by simple rules based on the
values of a few predictor variables.

A convenient way to summarize the overall results of a
contrast tree is through its corresponding lack-of-ﬁt contrast
curve. A discrepancy value is assigned to each observation as
being that of the corresponding region containing it. Obser-
vations are then ranked on their assigned discrepancies. For
each one the mean discrepancy of those observations with
greater than or equal rank is computed. This is plotted on
the vertical axis versus normalized rank (fraction) on the hor-
izontal axis. The left most point on each curve thus represents
the discrepancy value of the largest discrepancy region of its
corresponding tree. The right most point gives the discrep-
ancy averaged over all observations. Intermediate points give
average discrepancy over the highest discrepancy observations
containing the corresponding fraction.

The black curve in Fig. 3 shows the lack-of-ﬁt contrast
curve for the gradient boosting estimates based on a 50 node
contrast tree. Its error in estimated probability averaged over
all test set predictions is seen to be 0.081 (extreme right).
The error corresponding to the largest discrepancy region (ex-
treme left) is 0.128. The blue curve is the corresponding lack-
of-ﬁt curve for random forest probability prediction (Breiman
2001). Its average error is half of that for gradient boosting
and its worst error is 30% less.

The contrast tree represented in Fig. 2 suggests that the

6

problem with the gradient boosting procedure here is over-
smoothng.
It is failing to accurately estimate the extreme
probability values. Gradient boosting for binary probability
estimation generally uses a Bernoulli log–likelihood loss func-
tion based on a logistic distribution. The logistic transform
has the eﬀect of suppressing the inﬂuence of extreme prob-
abilities. Random forests are based on squared–error loss.
This suggests that using a diﬀerent loss function with gra-
dient boosting for this problem may improve performance,
especially at the extreme values.

The red curve in Fig. 3 shows the corresponding lack-of-
ﬁt contrast curve for gradient boosting probability estimates
using squared–error loss. This change in loss criterion has dra-
matically improved accuracy of gradient boosting estimates.
Both its average and maximum discrepancies are seen to be
at least three times smaller than those using the logistic re-
gression based loss criterion.

The green contrast curve in Fig. 3 represents results of
contrast boosting applied to the output of squared-error loss
gradient boosting as discussed in Section 5.1.
It is seen to
provide only little improvement here. The quantile regression
example provided in the Appendix however shows substantial
improvement when contrast boosting is applied to the gradi-
ent boosting output.

Standard errors on the results shown in Fig. 3 can be com-
puted using bootstrap resampling (Efron 1979). Standard
errors on the left most points, representing the highest dis-
crepancy regions of the respective curves, are 0.0089, 0.016,
0.012, and 0.012 (top to bottom). For the right most points
on the curves, representing average tree discrepancy, the cor-
responding errors are 0.0037, 0.0026, 0.0026 and 0.0026. Er-
rors corresponding to intermediate points on each curve are
between those of its two extremes

Table 1

Classiﬁcation error rates corresponding to the several
probability estimation methods.

Error rate
Method
Gradient Boosting – logistic loss
13.0%
Gradient Boosting – squared-error loss 12.9%
13.6%
Random Forest
12.8%
Contrast Boosting

Table 1 shows classiﬁcation error rate for each of the meth-
ods considered. They are all seen to be very similar. This il-
lustrates that classiﬁcation prediction accuracy can be a very
poor proxy for probability estimation accuracy. In this case
the over–smoothing of probability estimates caused by using
the logistic log–likelihood does not change many class assign-
ments. However, in applications that require high precision
accurate estimation of extreme probabilities is more impor-
tant.

 
 
6.3 Conditional distributions

Here we consider the case in which both y and z are considered
to be random variables independently drawn from respective
distributions py(y | x) and pz(z | x).
Interest is in contrast-
ing these two distributions as functions of x. Speciﬁcally we
wish to uncover regions of x - space where the distributions
most diﬀer. For this we use contrast trees (Section 3) with
discrepancy measure (6).

A well known way to approximate py(y | x) under the as-
sumption of homoskedasticity is through the residual boot-
strap (Efron and Tibshirani 1994). One obtains a loca-
tion estimate such as the conditional median ˆm(y | x) and
forms the data residuals ri = yi − ˆm(y | xi) for each obser-
vation 1 ≤ i ≤ N . Under the assumption that the condi-
tional distribution of r, pr(r | x), is independent of x (ho-
moskedasticity) one can draw random samples from py(y | xi)
as yi = ˆm(y | xi) + rπ(i) where π(i) is random permutation of
the integers i ∈ [1, N ]. These samples can then be used to
derive various regression statistics of interest.

A fundamental ingredient for the validity of residual boot-
strap approach is the homoskedasticity assumption. Here we
test this on the online news popularity data set (Fernandes,
Vinagre and Cortez, 2015) also available from the Irvine Ma-
chine Learning Data Repository. It summarizes a heteroge-
neous set of features about articles published by Mashable
web site over a period of two years. The goal is to pre-
dict the number of shares y in social networks (popularity).
There are N = 39797 observations (articles). Associated with
each are p = 59 attributes to be used as predictor variables
x. These are described at the download web site. Gradient
boosting was used to estimate the median function ˆm(y | x),
and {zi}N
i=1 was taken as a corresponding residual bootstrap
sample to be contrasted with y.

Figure 4 shows quantle-quantile (QQ)-plots of y versus z
for the nine highest discrepancy regions of a 50 node contrast
tree. The red line represents equality. One sees that there are
x - values (regions) where the distribution of y is very diﬀerent
from its residual bootstrap approximation z; homoskedastic-
ity is rather strongly violated. The average discrepancy (6)
over all 50 regions is 0.19.

The outcome variable y (number of shares) is strictly pos-
itive and its marginal distribution is highly skewed toward
larger values. In such situations it is common to model its log-
arithm. Figure 5 shows the corresponding results for contrast-
ing the distribution of log10(y) with its residual bootstrap
counterpart. Homoskedasticity appears to more closely hold
on the logarithm scale but there are still regions of x - space
where the approximation is not good. Here the average dis-
crepancy (6) over all 50 regions is 0.13. A null distribution for
average discrepancy under the hypothesis of homoskedasticity
can be obtained by repeatedly contrasting pairs of randomly
generated log10(y) residual bootstrap distributions. Based on
50 replications, this distribution had a mean of 0.078 with a

7

y

y

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

Node 19 : 501

Node 47 : 501

Node 15 : 1730

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

0

5000

10000

15000

0

5000

10000

15000

0

5000

10000

15000

z

z

z

Node 58 : 502

Node 23 : 510

Node 33 : 500

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

0

5000

10000

15000

0

5000

10000

15000

0

5000

10000

15000

z

z

z

Node 16 : 558

Node 18 : 695

Node 48 : 511

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

y

0
0
0
0
4

0
0
0
0
3

0
0
0
0
2

0
0
0
0
1

0

0

5000

10000

15000

0

5000

10000

15000

0

5000

10000

15000

z

z

z

Figure 4: QQ–plots of y versus parametric bootstrap z distri-
butions for the nine highest discrepancy regions of a 50 node
contrast tree using online news popularity data. The red line
represents equality.

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

y

y

Node 73 : 566

Node 11 : 716

Node 12 : 664

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

z

z

z

Node 67 : 500

Node 23 : 645

Node 45 : 500

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

z

z

z

Node 33 : 501

Node 31 : 763

Node 48 : 519

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

3.0

3.5

4.0

4.5

5.0

z

z

z

Figure 5: QQ–plots of log10(y) versus corresponding para-
metric bootstrap z distributions for the nine highest discrep-
ancy regions of a 50 node contrast tree using online news
popularity data. The red line represents equality.

standard deviation of 0.003.

Simulated  Test  Data

0
3
.
0

5
2
.
0

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

7 Distribution boosting – simulated

data

The notion of distribution boosting (Section 5.2) is suﬃciently
unusual that we ﬁrst illustrate it on simulated data where the
estimates ˆpy(y | x) can be compared to the true data gener-
ating distributions py(y | x). Distribution boosting applied to
the online news popularity data described in Section 6.3 is
presented in the Appendix.

y
c
n
a
p
e
r
c
s
D

i

e
g
a
r
e
v
A

7.0.1 Data

There are N = 25000 training observations each with a set
of p = 10 predictor variables xi randomly generated from a
standard normal distribution. The outcome variable y | x is
generated from a transformed asymmetric logistic distribu-
tion (Friedman 2018)

y = h (f (x) + η(x))

(15)

with

η(x) = −sl(x) · | ε |, prob = sl(x)/(sl(x) + su(x))
η(x) = +su(x) · | ε |, prob = su(x)/(sl(x) + su(x)).

Here ε is a standard logistic random variable, and the trans-
formation h(z) is

h(z) = sign(z) (0.5 | z | + 1.5 z2).

(16)

The untransformed mode f (x) and lower/upper scales
sl(x) / su(x) are each diﬀerent functions of the ten predictor
variables x. The simulated mode function is taken to be

f (x) =

10

X
j=1

cj Bj(xj ) / stdxj (Bj (xj) )

(17)

with the value of each coeﬃcient cj being randomly drawn
from a standard normal distribution. Each basis function
takes the form

Bj(xj ) = sign(xj) | xj |rj

(18)

with each exponent rj being separately drawn from a uniform
distribution rj ∼ U (0, 2). The denominator in each term
of (17) prevents the suppression of the inﬂuence of highly
nonlinear terms in deﬁning f (x).

The scale functions are taken to be sl(x) = 0.2 + exp (tl(x))
and su(x) = 0.2 + exp(tu(x)) where the log–scale functions
tl(x) and tu(x) are constructed in the same manner as (17)
(18) but with diﬀerent randomly drawn values for the 20 pa-
1 producing diﬀerent functions of x. The
rameters {cj, rj }10

8

0

100

200

Trees

300

400

Figure 6: Test data discrepancy averaged over the terminal
nodes (regions) of successive contrast trees for the ﬁrst and
then every tenth iteration for 400 iterations of distribution
boosting on simulated training data. The solid red curve is a
running median smooth.

average pair-wise absolute correlation between the three func-
tions is 0.18. The overall resulting distribution p(y | x) (15–
18) has location, scale, asymmetry, and shape being highly
dependent on the joint values of the predictors x in a com-
plex and unrelated way.

7.0.2 Conditional distribution estimation

Distribution boosting is applied to this simulated data to es-
timate its distribution py(y | x) as a function of x. For each
observation the contrasting random variable z is taken to be
independently generated from a standard normal distribution,
z | x ∼ N (0, 1) , independent of x. The goal is to produce an
estimated transformation of z, ˆy = ˆgx(z), at each x such that
pˆy(ˆy | x) = py(y | x). To the extent the estimate ˆgx(z) accu-
rately reﬂects the true transformation function gx(z) at each
x one can apply it to a sample of standard normal random
numbers to produce a sample drawn from the distribution
py(y | x). This sample can then be used to plot that distribu-
tion or compute the value of any of its properties.

Figure 6 plots the average terminal node discrepancy (6) for
400 iterations of distribution boosting applied to the training
data, as evaluated on a 25000 observation independent “test”
data set generated from the same joint (x, y) - distribution
(15 – 18). Results are shown for the ﬁrst and then every
tenth successive tree. The test set discrepancy is seen to gen-
erally decrease with increasing number of trees. There is a
diminishing return after about 200 iterations (trees).

 
 
y

y

5
1

0
1

5

0

0
2

5
1

0
1

5

0

5
−

0
2

0
1

Node 29 : 503

Node 56 : 599

Node 10 : 900

Node 106 : 576

Node 6 : 500

Node 95 : 531

0
2

0
1

0
1

5

0
1

5

0
1

5

y

0

y

0

y

0

y

0

y

0

5
−

0
1
−

5
1
−

0
2
−

0
1
−

0
2
−

−10

−5

0

z

5

10

−10

−5

0

z

5

10

−10

−5

0

5

10

z

Node 36 : 559

Node 90 : 578

Node 14 : 840

0
2

0
1

0
2

0
1

5
−

0
1
−

0
1

5

5
−

0
1
−

5
−

0
1
−

−10

−5

0

z

5

10

−10

−5

0

z

5

10

−10

−5

5

10

0

z

Node 49 : 500

Node 129 : 500

Node 87 : 503

0
1

5

0
1

5

y

0

y

0

y

0

y

0

y

0

−10

−5

0

z

5

Node 28 : 700

y

0

y

0
1
−

0
2
−

−10

−5

5

0

z

Node 38 : 672

0
1
−

0
2
−

0
2

5
1

0
1

5

0

5
−

0
1
−

0
2
−

0
2

5
1

0
1

5

0

5
−

y

−10

−5

0

5

10

z

Node 46 : 505

5
−

0
1
−

0
1

5

5
−

0
1
−

5
−

0
1
−

−10

−5

0

z

5

10

−10

−5

0

z

5

10

−10

−5

5

10

0

z

Node 39 : 501

Node 16 : 706

Node 13 : 500

0
1

5

0
1

5

y

0

y

0

y

0

5
−

0
1
−

5
−

0
1
−

5
−

0
1
−

−10

−5

0

z

5

10

−10

−5

0

z

5

10

−15

−10

−5

0

5

10

z

−10

−5

0

z

5

10

−10

−5

0

z

5

10

−10

−5

5

10

0

z

Figure 7: QQ–plots of y versus z (standard normal) for the
nine highest discrepancy regions of a 10 node contrast tree on
the simulated test data set. The red lines represent equality.

Figure 8: QQ–plots of y versus ˆy = ˆgx(z) for the nine highest
discrepancy regions of a 10 node contrast tree on the simu-
lated test data set. The red lines represent equality.

Note that with contrast boosting average tree discrepancy
on test or training data does not necessarily decrease mono-
tonically with successive iterations (trees). Each contrast tree
represents a greedy solution to a non convex optimization
with multiple local optima. As a consequence the inclusion
of an additional tree can, and often does, increase average
discrepancy of the current ensemble. Boosting is continued
as long as there is a general downward trend in average tree
discrepancy.

Lack-of-ﬁt to the data of any model for the distribution
py(y | x) can be can be assessed by contrasting y with a sam-
ple drawn from that distribution. Figure 7 shows QQ–plots of
y versus initial z (everywhere standard normal) for the nine
highest discrepancy regions of a 10 node tree contrasting the
two quantities on the test data set. The red lines represent
equality. One sees that py(y | x) is here far from being every-
where standard normal.

For the distribution boosted model ˆy = ˆgx(z) lack-of-ﬁt can
be assessed by contrasting the distributions of y and ˆy with a
contrast tree using the test data set. Figure 8 shows QQ–plots
of y versus ˆy for the nine highest discrepancy regions of a 10
node tree contrasting the two quantities on the test data set.
The red lines represent equality. The transformation ˆgx(z)
at each separate x - value was evaluated using the 400 tree
model built on the training data. The nine highest discrep-
ancy regions shown in Fig. 8 together cover 27% of the data.
They show that while the transformation model ﬁts most of
the test data quite well, it is not everywhere perfect. There
are minor departures between the two distributions in some

small regions. However these discrepancies appear in sparse
tails where QQ–plots themselves tend to be unstable.

A measure of the diﬀerence between the estimated and true

CDFs can be deﬁned as

AAE =

1
100

100

X
j=1

| CDF (uj) − \CDF (uj) |

(19)

where CDF is the true cumulative distribution of y | x com-
puted form (15–18) and \CDF is the corresponding estimate
from the distribution boosting model. The 100 evaluation
points {uj}100
are a uniform grid between the 0.001 and 0.999
quantiles of the true distribution CDF .

1

Figure 9 summarizes the overall accuracy of the distribu-
tion boosting model. The upper left frame shows a histogram
of the distribution of (19) for observations in the test data set.
The 50, 75 and 90 percentiles of this distribution are respec-
tively 0.0352, 0.0489 and 0.0773 indicated by the red marks.
The remaining plots show estimated (black) and true (red)
distributions for three observations with (19) closest to these
respective percentiles. Thus 50% of the estimated distribu-
tions are closer to the truth than that shown in the upper
right frame. Seventy ﬁve percent are closer than that shown
in the lower left frame, and 90% are closer than that seen in
the lower right frame.

Distribution boosting produces an estimate for the full dis-
tribution of y | x by providing a function ˆgx(z) that transforms
a random variable z with a known distribution pz(z | x) to the
estimated distribution ˆpy(y | x). One can then easily compute

9

Obs 2003 :  Rms Diff =  0.0352

GB (25%)  AAE = 0.41

GB (50%)  AAE = 0.31

GB (75%)  AAE = 0.34

y
c
n
e
u
q
e
r
F

)

y

(
F
D
C

0
0
4
1

0
0
2
1

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

)

y

(
F
D
C

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0.00

0.05

0.10

0.15

−10

−5

RMS  Difference

0

y

5

10

Obs 9201 :  Rms Diff =  0.0489

Obs 2263 :  Rms Diff =  0.0773

)

y

(
F
D
C

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

−10

−5

0

y

5

10

−10

−5

0

y

5

10

Figure 9: Upper left: CDF estimation error distribution for
simulated data. Upper right: estimated (black) and true (red)
CDFs for observation with median error. Lower: correspond-
ing plots for 75% and 90% decile errors.

any statistic ˆS(x) = S[ ˆpy(y | x)], which can be used as an es-
timate for the value of the corresponding quantity S(x) = S[
py(y | x)] on the actual distribution. For some quantities S(x)
an alternative is to directly estimate them by minimizing em-
pirical prediction risk based on an appropriate loss function

ˆS(x) = arg min
f ∈ℑ

1
N

N

X
i=1

L(yi, f (xi))

(20)

were ℑ is the function class associated with the learning
method. Here we compare distribution boosting (DB) esti-
mates of the quartiles Qp(x), p ∈ [0.25, 0.5, 0.75], with those
of gradient boosting quantile regression (GB), which uses loss

Lp(y, z) = (1 − p) (z − y)+ + p (y − z)+,

(21)

on the simulated data set where the truth is known.

Figure 10 shows true versus predicted values for each of the
two methods (rows) on the three quartiles (columns). The red
lines represent a running median smooth and the blue lines
show equality. The average absolute error AAE associated
with each of these plots is

AAE(h, v) = mean(| h − v |)/mean(| v − median(v) |) (22)

where h is the quantity plotted on the horizontal and v the
vertical axes. The quantile values derived from the estimates

4

2

0

2
−

4
−

h
t
u
r
T

6

4

h
t
u
r
T

2

0

2
−

−10 −8

−6

−4

−2

0

2

−4

−2

0

2

4

−2

0

2

4

6

Estimate

Estimate

Estimate

DB (25%)  AAE = 0.35

DB (50%)  AAE = 0.26

DB (75%)  AAE = 0.30

4

2

0

2
−

4
−

h
t
u
r
T

6

4

h
t
u
r
T

2

0

2
−

h
t
u
r
T

h
t
u
r
T

2

0

2
−

4
−

6
−

8
−

0
1
−

2

0

2
−

4
−

6
−

8
−

0
1
−

−10 −8

−6

−4

−2

0

2

−4

−2

0

2

4

−2

0

2

4

6

Estimate

Estimate

Estimate

Figure 10: Predicted versus true values for the three quartiles
as functions of x (columns) for gradient boosting quantile
regression (upper row) and distribution boosting (lower row)
on the simulated data. The red lines represent a running
median smooth and the blue lines show equality.

of the full distribution (bottom row) are here seen to be some-
what more accurate than those obtained from gradient boost-
ing quantile regression (top row).

With quantile regression each quantile is estimated sepa-
rately without regard to estimates of other quantiles. Dis-
tribution boosting quantile estimates are all derived from a
common probability distribution and thus have constraints
imposed among them. For example, two quantile estimates
have the property ˆQp(x) < ˆQp′(x) for all p < p′ at any x.
These constraints can improve accuracy especially when the
quantile estimates are being used to compute quantities de-
rived from them.

There is an additional advantage of computing quantities
such as means or quantiles from the estimated conditional
distributions ˆS(x) = S[ ˆpy(y | x)]. As noted in Section 4, dis-
tribution contrast trees can be constructed in the presence of
arbitrary censoring or truncation. This extends to contrast
boosted distribution estimates ˆpy(y | x) and any quantities de-
rived from them. This in turn allows application to ordinal
regression which can be considered a special case of interval
censoring (Friedman 2018).

8 Related work

Regression trees have a long history in Statistics and Ma-
chine Learning. Since their ﬁrst introduction (Morgan and
Songquist 1963) many proposed modiﬁcations have been in-
troduced to increase accuracy and extend applicability. See
Loh (2014) for a nice survey. More recent extensions include

10

 
 
 
 
 
 
 
 
 
Mediboost (Valdes et al 2016) and the Additive Tree (Luna
et al 2019). All of these proposals are focused towards es-
timating the properties of a single outcome variable. There
seems to have been little to no work related to applications
involving contrasting two variables.

Friedman and Fisher (1999) proposed using tree style recur-
sive partitioning strategies to identify interpretable regions in
x - space within which the mean of a single outcome y was
relatively large (“hot spots”). With a similar goal Buja and
Lee (2001) proposed using ordinary regression trees with a
splitting criterion based on the maximum of the two daugh-
ter node means.

Classiﬁcation tree boosting was proposed by Freund and
Schapire (1997). Extension to regression trees was developed
by Friedman (2001). Since then there has been considerable
research attempting to improve accuracy and extending its
scope. See Mayr et al (2014) for a good summary.

Although boosted contrast trees have not been previously
proposed they are generally appropriate for the same types
of applications as gradient boosted regression trees, such as
classiﬁcation, regression, and quantile regression. They can
be beneﬁcial in applications where a contrast tree indicates
lack-of-ﬁt of a model produced by some estimation method.
In such situations applying contrast boosting to the model
predictions often provides improvement in accuracy.

Tree ensembles have also been applied to nonparametric
conditional distribution estimation. Meinshausen (2006) used
classical random forests to deﬁne local neighborhoods in x -
space. The empirical conditional distribution in each such de-
ﬁned local region around a prediction point x is taken as the
corresponding conditional distribution estimate at x. Athey,
Tibshirani and Wagner (2019) noted that since the regres-
sion trees used by random forests are designed to detect only
mean diﬀerences the resulting neighborhoods will fail to ad-
equately capture distributions for which higher moments are
not generally functions of the mean. They proposed modiﬁed
tree building strategies based on gradient boosting ideas to
customize random forest tree construction for speciﬁc appli-
cations including quantile regression.

Boosted regression trees have been used as components in
procedures for parametric ﬁtting of conditional distributions
and transformations. A parametric form for the conditional
distribution or transformation is hypothesized and the pa-
rameters as functions of x are estimated by regression tree
gradient boosting using negative log–likelihood as the predic-
tion risk. See for example Mayr et al (2012), Friedman (2018),
Pratola et al (2019), Hothorn (2019) and Mukhopadlhyay &
Wang (2019). Some diﬀerences between these previous meth-
ods and the corresponding approaches proposed here include
use of contrast rather than regression trees, and no parametric
assumptions.

The principal beneﬁt of the contrast tree based procedures
is a lack-of-ﬁt measure. As seen in Table 1 of Section 6.2,

and in the Appendix, values of negative log–likelihoods or
prediction risk need not reﬂect actual lack-of-ﬁt to the data.
The values of their minima can depend upon other unmea-
sured quantities. The goal of contrast trees as illustrated in
this paper is to provide such a measure. Contrast trees can
be applied to assess lack-of-ﬁt of estimates produced by any
method, including those mentioned above.
If discrepancies
are detected, contrast boosting can be employed to remedy
them and thereby improve accuracy.

9 Summary

Contrast trees as described in Sections 3 and 4 are designed
to provide interpretable goodness-of-ﬁt diagnostics for esti-
mates of the parameters of py(y | x), or the full distribution.
Examples involving classiﬁcation, probability estimation and
conditional distribution estimation were presented in Section
6. A quantile regression example is presented in the Ap-
pendix. Two–sample contrast trees for detecting discrepan-
cies between separate data sets are also described in the Ap-
pendix.

Boosting of contrast trees is a natural extension. Given
an initial estimate ˆz(x) from any learning method a contrast
tree can assess its goodness or lack-of-ﬁt to the data. If found
lacking, the boosting strategy attempts to improve the ﬁt by
successively modifying ˆz(x) to bring it closer to the data. The
Appendix provides an example involving quantile regression
where this strategy substantially improved prediction accu-
racy.

Contrast boosting the full conditional distribution is illus-
trated on simulated data in Section 7.0.2 and on actual data in
the Appendix. Note that the conditional distribution proce-
dure of Section 5.2 can be applied in the presence of arbitrar-
ily censored or truncated data by employing Turnbul’s (1976)
algorithm to compute CDFs and corresponding quantiles.

Contrast trees and boosting inherit all of the data ana-
lytic advantages of classiﬁcation and regression trees. These
include categorical variables, missing values,
invariance to
monotone transformations of the predictor variables, resis-
tance to irrelevant predictors, variable importance measures,
and few tuning parameters.

Important discussions with Trevor Hastie and Rob Tibshi-
rani on the subject of this work are gratefully acknowledged.
An R procedure implementing the methods described herein
is available.

References

[1] Anderson, T. and Darling, D. (1952). Asymptotic theory
of certain ”goodness-of-ﬁt” criteria based on stochastic
processes. Ann. Stat. 23, 193–212.

11

[2] Athey, S., Tibshirani, J., Wagner, S. (2019). Generalized

random forests. Ann. Stat. 47(2), 1148-1178.

[3] Breiman, L., Friedman, J., Olshen, R., and Stone, C.
(1984). Classiﬁcation and Regression Trees. Chapman
and Hall.

[4] Breiman, L. (2001). Random forests. Machine Learning

45, 5-32.

[5] Buja, A and Lee, Y. (2001). Data mining criteria for tree-
based regression and classiﬁcation. Proceedings of KDD
2001, 27–36.

[6] Efron, B. (1979). Bootstrap Methods: Another look at

the jackknife. Ann. Stat. 7, 1-26.

[7] Efron, B. and Tibshirani, R. (1994). An Introduction to

the Bootstrap. Springer.

[8] Fernandes, K., Vinagre, P. and Cortez, P. (2015). A
proactive intelligent decision support system for predict-
ing the popularity of online news. Proceedings of the 17th
EPIA 2015 - Portuguese Conference on Artiﬁcial Intel-
ligence. September, Coimbra, Portugal.

[17] Mayr, A., Fenske, N., Hofner, B., Kneib, T., Schmid,
M. (2012). GAMLSS for high dimensional data–a ﬂexi-
ble approach based on boosting. J. R. Stat. Soc. Ser. C
(Appl. Stat) 61(3), 403–427.

[18] Mayr, A., Binder, H., Gefeller, O., and Schmid, M.
(2014). The evolution of boosting algorithms from ma-
chine learning to statistical modelling. Methods Inf. Med.
53, 419–427.

[19] Meinshausen, M. (2006). Quantile random forests. J.

Machine Learning Research 7, 983–999.

[20] Morgan, J. and Sonquist, J. (1963). Problems in the anal-
ysis of survey data, and a proposal. J. Amer. Statist.
Assoc. 58, 415–434.

[21] Mukhopadhyay, S. and Wang, K. (2019). On the prob-
lem of relevance in statistical inference. J. Amer. Statist.
Assoc.(submitted).

[22] Pratola, M. T., Chipman, H. A, George, E. I., and Mc-
Culloch, R. E. (2019). Heteroscedastic BART via mul-
tiplicative regression trees, J. of Comput. and Graphical
Statist., DOI: 10.1080/10618600.2019.1677243

[9] Freund, Y. and Schapire, R. (1997). A decision-theoretic
generalization of on-line learning and an application to
boosting. J. Computer and System Sciences 55, 119-139.

[23] Turnbull, B. W. (1976). The empirical distribution func-
tion with arbitrarily grouped, censored and truncated
data. J. Royal Statist. Soc. B 38, 290-295.

[10] Friedman, J. and Fisher, N. (1999) Bump hunting in
high-dimensional Data. Statistics and Computing, 9,
123-143.

[11] Friedman, J. (2001). Greedy function approximation: a
gradient boosting machine. Ann. Stat. 29 1189-1232.

[12] Friedman, J. (2018). Predicting regression probability
distributions with imperfect data through optimal trans-
formations. Stanford University Statistics Technical Re-
port.

[13] Hothorn,
ing
https://doi.org/10.1007/s11222-019-09870-4.

Transformation
and

machines.

Statistics

(2019).

T.

boost-
Computing.

[14] Kohavi, R. (1996). Scaling up the accuracy of naive-bayes
classiﬁers: a decision-tree hybrid. Proceedings of the Sec-
ond International Conference on Knowledge Discovery
and Data Mining, 1996

[15] Loh, W. (2014). Fifty years of classiﬁcation and regres-

sion Trees. Inter. Statist. Rev. 82, 3, 329–348.

[16] Luna, J., Gennatas, E., Eaton, E., Diﬀenderfer, E., Un-
gar, L., Jensen, S., Simone, C., Friedman, J., Valdes, G.
(2019). The additive tree. Proc. Nat. Acad. Sci.116 (40),
19887-19893.

[24] Valdes, G., Luna, J., Eaton, E., Simone, C , Ungar, L.
and Solberg, T. (2016). MediBoost: a patient stratiﬁca-
tion tool for interpretable decision making in the era of
precision medicine. Scientiﬁc Reports 6, Article number:
37854.

Appendix

A Lack-of-ﬁt estimation

Here contrast tree lack-of-ﬁt estimates are compared with
known truth on simulated data. There are N = 25000 ob-
servations each with p = 10 predictor variables x randomly
generated from a standard normal distribution. The outcome
y is generated from a simple model

y = f (x) + s(x) · ε

with ε a standard normal random variable. The location f (x)
and scale s(x) functions are given by (17) (18) with diﬀerent
randomly generated parameters. The correlation between the
two functions over the data is cor(f (x), s(x)) = 0.06. The
signal/noise is IQR(f (x))/(2 · med(s(x))) = 3. The goal is to
estimate the location function f (x).

12

0
.
2

5
.
1

y
$
u

0
.
1

5
.
0

0
.
0

root-mean-squared discrepancy over the same test observa-
tions calculated from the respective contrast trees for each
method. The discrepancy associated with an observation is
that of the contrast tree region that contains it.

Contrast tree discrepancy as computed on the data and es-
timation error based on the (usually unknown) truth are seen
here to track each other fairly well. Contrast tree discrepancy
is generally smaller that RMS error but relative ratios of the
two between the methods are similar.

It is important to note that contrast trees are not perfect.
As with any learning method they can sometimes fail to cap-
ture suﬃciently complex dependencies on the predictor vari-
ables x. In such situations lack-of-ﬁt may be under estimated.
Thus contrast trees can reject ﬁt quality but never absolutely
insure it.

0.0

0.2

0.4

0.6

0.8

1.0

u$x

B Quantile regression example

Lack-of-ﬁt contrast curves on simulated data.
Figure 11:
Black: constant ﬁt , purple: single CART tree, blue:
linear
model, violet: random forest, orange: squared-error and red:
absolute loss gradient boosting, green: truth.

Lack-of-ﬁt contrast curves for six methods are shown in
Fig. 11. The methods are (top to bottom): black constant
ﬁt (global mean), purple single CART tree, blue linear least-
squares ﬁt, violet random forest, orange squared-error and
red absolute loss gradient boosting. The bottom green curve
represents the lack-of-ﬁt contrast curve for the true function
f (x) on these data. All curves were evaluated on a separate
25000 observation test data set not used to train the respec-
tive models.

Table A1
RMS estimation error and contrast tree RMS discrepancy
for several methods

Method
constant
CART tree
linear model
random forest
sqr-error boost
abs-error boost
truth

estimation error
0.99
0.57
0.33
0.21
0.15
0.11
0

discrepancy
0.86
0.34
0.23
0.13
0.090
0.063
0.046

Since the data are simulated and truth f (x) is here known
one can directly compute root-mean-squared estimation error

RM SE = qmean((f (x) − ˆf (x))2)

for each method. This is shown in Table A1 (second column)
for each method (ﬁrst column). The third column shows the

Use of contrast trees in quantile regression is illustrated on the
online news popularity data set described in Section 6.3. Here
we apply contrast trees to diagnose the accuracy of gradient
boosting estimates of the median and 25th percentile function
of y | x.

The usual quantile regression loss used by gradient boosting
for estimating the pth quantile z is given by (21) where here
p ∈ {0.5, 0.25} and z is the corresponding quantile estimate.
With contrast trees we use

p −

1
Nm X
i∈Rm

dm = (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

I(yi < zi)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(23)

as a discrepancy measure. This quantity can be interpreted
as prediction error on the probability scale. It is the absolute
diﬀerence between the target probability p and the empirical
probability Pr(y < z) as averaged over the region.

The data were randomly divided into two parts: a learning
data set of Nl = 20000 and and test data set of Nt = 19644.
A ten region tree to contrast the median of py(y | x) from
its gradient boosting predictions was built using (23) on the
learning data set and evaluated on the test data set. The
results are shown in Fig. 12.

The upper frame shows the empirical (blue) and predicted
(red) median in each of the regions in order of absolute dis-
crepancy (23). The lower frame gives the number of counts in
each corresponding region. One sees that for 85% of the data
(node 20) gradient boosted model predictions of the median
appear to be quite close. In other regions of x -space there
are small to moderate diﬀerences.

Figure 13 shows lack-of-ﬁt contrast curves for estimating
the median of y given x by four methods. The green curve
represents a constant prediction of the global median at each
x - value. The red curve is for linear quantile regression. The
linear model seems only a little better than the constant one.

13

34

22

8

15

2

27

26

35

14

20

Node

y
c
n
a
p
e
r
c
s
D

i

e
g
a
r
e
v
A

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

e

l
i
t
n
e
c
r
e
P

h
t
5
7

s
t
n
u
o
C

0
0
0
3

0
0
5
2

0
0
0
2

0
0
5
1

0
0
0
1

0
0
5

0

0
0
0
4
1

0
0
0
0
1

0
0
0
6

0
0
0
2

0

34

22

8

15

2

27

26

35

14

20

Node

Figure 12: Online news data. Upper frame: Empirical value
of the median for observations in each region (blue), along
with the corresponding median of the model predictions (red)
in that region, for a quantile contrast tree. Lower frame:
counts in each region.

y
c
n
a
p
e
r
c
s
D

i

e
g
a
r
e
v
A

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

Fraction of Observations

Figure 13: Online news data. Lack-of-ﬁt contrast curves com-
paring conditional median estimates by constant (green), lin-
ear (red) quantile regression, gradient boosting (black), and
contrast boosting (blue).

0.0

0.2

0.4

0.6

0.8

1.0

Fraction of Observations

Figure 14: Online news data. Lack-of-ﬁt contrast curves
comparing conditional 25–percentile estimates by constant
(green),
linear (red) quantile regression, gradient boosting
(black), and contrast boosting (blue).

The black curve represents the gradient boosting predictions
based on (21) which are somewhat better. The blue curve
is the result of applying contrast boosting (Section 5.1) to
the gradient boosting output. Here this strategy appears to
substantially improve prediction. For the left most points
on each curve the bootstraped errors are respectively 0.015,
0.016, 0.018, and 0.016 (top to bottom). For the right most
points the corresponding errors are 0.0023, 0.0026, 0.0031 and
0.0033. Thus, the larger diﬀerences between the curves seen
in Fig. 13 are highly signiﬁcant.

Figure 14 shows lack-of-ﬁt contrast curves for estimating
the conditional ﬁrst quartile (p = 0.25) as a function of x for
the same four methods. Here one sees that the global con-
stant ﬁt appears slightly better that the linear model, while
the gradient boosting quantile regression estimate is about
twice as accurate. Contrast boosting seems to provide no
improvement in this case. Standard errors on the left most
points of the respective curves are 0.014, 0.0092, 0.020, and
0.015 (top to bottom). For the right most curves the corre-
sponding errors are 0.0027, 0.0039, 0.0029 and 0.0030.

Table B1
Prediction risk corresponding to the several quantile
regression methods for online news data

Method
Constant
Linear
Gradient Boosting
Contrast Boosting

Median
2489.5
2488.5
2481.7
2479.9

1st Quartile
678.3
678.4
674.1
674.1

Table B1 shows quantile regression prediction risk based on
L1 loss (21) for median (p = 0.5) and ﬁrst quartile (p = 0.25)

14

 
 
 
 
 
 
Mashable  Website  log10 ( shares )

PAD (50%)  =  0.028

y
c
n
e
u
q
e
r
F

0
0
0
2

0
0
5
1

0
0
0
1

0
0
5

0

2.0

2.5

3.0

3.5

4.0

4.5

5.0

ynews

Figure 15: Distribution of log10(shares) for the online news
data.

using the four methods shown in Figs. 13 and 14. Although
here prediction risk measures lack-of-accuracy of the methods
in the same order as their respective contrast trees, it gives
no indication of their actual relative or absolute lack-of-ﬁt to
the data as seen from their respective contrast curves in Figs
13 and 14.

C Distribution boosting example

Distribution boosting is illustrated using the online news pop-
ularity data described in Section 6.3. The goal is to esti-
mate the distribution py(y | x) of (log10) popularity of news
articles y for given sets of predictor variable values x. Here
we investigate the variation of the ﬁnal distribution estimate
ˆpy(y | x) to diﬀerent initial z - distributions pz(z | x). For the
same py(y | x), changing the initial pz(z | x) distribution can
substantially change the nature of the target transformation
functions gx(z) to be estimated. This can aﬀect ultimate ac-
curacy of the estimates ˆpy(y | x).

Distribution boosting was applied to the 20000 observa-
tion training data set using three diﬀerent initial pz(z | x).
The ﬁrst was the same normal distribution z ∼ N (¯y, σ2
y)
at every x, where ¯y and σ2
y are the mean and variance of
y = log10(popularity). The second initial distribution, also
independent of x, is the empirical marginal distribution of y
as shown in Fig. 15. The third initial z - distribution is that
of the residual bootstrap at each x as described in Section 6.3.
This assumes homoscedasticity on the log-scale with varying
location.

The upper left frame of Fig. 16 shows the distribution
of the average pair-wise diﬀerence between the three CDF

F
D
C

0
1

.

8
0

.

6

.

0

4
0

.

2
0

.

0
0

.

0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14

2.5

3.0

3.5

4.0

4.5

Pairwise  Average  Difference  (PAD)

Y

PAD (75%)  =  0.040

PAD (90%)  =  0.053

F
D
C

0
1

.

8
0

.

6

.

0

4
0

.

2
0

.

0
0

.

y
c
n
e
u
q
e
r
F

F
D
C

0
5
1

0
0
1

0
5

0

0
1

.

8
0

.

6

.

0

4
0

.

2
0

.

0
0

.

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

Y

Y

Figure 16: Upper left: distribution of average pair-wise diﬀer-
ence between CDF estimates resulting from the three diﬀer-
ent initial z - distributions for online news data. Upper right:
CDF estimates for parametric bootstrap (black), Gaussian
(green) and empirical marginal (red) starting distributions
for observation with median pairwise diﬀerence. Lower: cor-
responding plots for 75% and 90% decile diﬀerence.

estimates on each (test set) observation x, resulting from the
three diﬀerent beginning z - distributions. Diﬀerence between
two CDF estimates is given by (19) with the 100 evaluation
points {uj}100
being a uniform grid between the minimum of
0.001 quantiles and the maximum of the 0.999 quantiles of
the three distributions.

1

The 50, 75, and 90 percentiles of the distribution shown in
the upper left frame are respectively 0.028, 0.040, and 0.053.
As in Fig. 9 the remaining plots in Fig. 16 show the three
corresponding CDF s for those observations with pair-wise
average diﬀerence closest to these respective percentiles. The
green curves display the estimate corresponding to the Gaus-
sian starting z - distribution, red for the empirical marginal
distribution of Fig. 15, and black for the residual bootstrap
start. The upper right frame shows that for at least half of the
observations the three estimates are fairy similar. The other
half exhibit moderate diﬀerences. The residual bootstrap es-
timates tend to be diﬀerent from the other two, which are
usually similar to each other.

Figure 16 shows that diﬀerent starting z - distributions give
rise to at least slightly diﬀerent conditional distribution esti-
In general, diﬀerent methods produce diﬀerent an-
mates.
swers and one would like to ascertain their respective accu-
racies. Contrast trees provide a lack-of-ﬁt measure. With
conditional distribution estimates one can contrast y with

15

Node 39 : 500

Node 44 : 528

Node 56 : 560

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

y

5
.
4

0
.
4

y

5
.
3

0
.
3

5
.
2

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

z

z

z

Node 16 : 538

Node 6 : 593

Node 53 : 502

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

0
.
2

y

y

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

0
.
2

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

z

z

z

Node 26 : 646

Node 11 : 500

Node 31 : 503

y

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

0
.
2

5
.
1

y

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

y

y

y

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

0
.
2

0
.
5

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

0
.
2

5
.
4

0
.
4

5
.
3

0
.
3

5
.
2

y
c
n
a
p
e
r
c
s
D

i

t
s
e
T

e
g
a
r
e
v
A

0
2
.
0

8
1
.
0

6
1
.
0

4
1
.
0

2
1
.
0

0
1
.
0

8
0
.
0

0.0

0.2

0.4

0.6

0.8

1.0

2.5

3.0

3.5

4.0

4.5

2.0

2.5

3.0

3.5

4.0

4.5

2.5

3.0

3.5

4.0

4.5

z

z

z

Fraction  of  Observations

Figure 17: QQ–plots of y versus ˆy = ˆgx(z) calculated from
parametric bootstrap start for the nine highest discrepancy
regions of a 50 node contrast tree on the online news test data
set. The red lines represent equality.

Figure 18:
Lack-of-ﬁt contrast curves for three trees con-
trasting y with ˆy = ˆgx(z) based on the diﬀerent starting z -
distributions: Gaussian (green), empirical marginal (red) and
parametric bootstrap (black).

ˆy = ˆgx(z) on an independent test data set not involved in
the estimation as was illustrated in Fig. 5. Here we employ
this strategy to evaluate the respective accuracies of the three
conditional distribution estimates obtained by the three dif-
ferent starting z - distributions.

Figure 17 shows QQ – plots of y versus the estimates
ˆy = ˆgx(z) based on the residual bootstrap starting z - dis-
tribution. Shown are the nine largest discrepancy regions of
a 50 terminal node contrast tree. These nine regions account
for 25% of the data. This can be compared to Fig. 5 which
shows the corresponding QQ –plots for y versus the original
residual bootstrap z before distribution boosting.

Figure 18 shows the lack-of-ﬁt contrast curves correspond-
ing to the three distribution boosting estimates based on the
three diﬀerent starting z - distributions. Each line summa-
rizes a diﬀerent tree contrasting y with one of the correspond-
ing three estimates ˆy = ˆgx(z). The green and red curves in
Fig. 18 summarize the results for contrasting y with ˆgx(z)
based on the respective Gaussian and empirical marginal dis-
tribution (Fig. 15) starting z - distributions. Their accu-
racies are seen to be similar. The black curve summarizes
the tree depicted in Fig. 17 contrasting y with the estimates
ˆgx(z) based on the residual bootstrap starting z - distribution.
These ˆgx(z) estimates appear to be somewhat more accurate.
Bootstrap standard errors on the left most points of all three
curves are 0.022. For the right most points the corresponding
errors are 0.0055, 0.0052 and 0.0049.

The average discrepancy of the tree contrasting y and the

residual bootstrap estimated ˆgx(z) (black) is 0.081. The cor-
responding averages for the respective Gaussian and empiri-
cal marginal distribution (Fig. 15) starting z - distributions
are 0.10 and 0.092 respectively. These results can be com-
pared with the discrepancies of their initial untransformed z
- distributions. Average discrepancy for contrasting y with
the untransformed residual bootstrap distribution (Fig. 5) is
0.13. The corresponding average discrepancies with y for the
untransformed Gaussian z distribution is 0.26, and that for
the empirical marginal distribution is 0.24. Thus the residual
bootstrap provided a much closer starting point for estimat-
ing py(y | x) ultimately resulting in somewhat more accurate
results.

One can obtain a null distribution for average transformed
discrepancy by repeatedly applying the contrast boosting pro-
cedure with y and z randomly sampled from the same distri-
bution. In this case py(y | x) and pz(z | x) are the same and
any diﬀerences detected by the distribution boosting proce-
dure, as revealed by a ﬁnal contrast tree, are caused by the
random nature of the data and not actual diﬀerences between
the respective distributions. Fifty replications of contrast-
ing boosting based on pairs of random samples, each drawn
from from the (same) residual bootstrap distribution, pro-
duced and average tree discrepancy of 0.085 with a standard
deviation of 0.003. Thus there is no evidence here for a sys-
tematic diﬀerence between the distribution of the original y
and its estimate ˆy = ˆgx(z) based on the residual bootstrap
initial z - distribution.

16

 
 
 
 
D Two-sample contrast trees

F (blue)  &  M (red)  Big  Salary  Probability

Contrast trees as so far described are applied to a single data
set where each observation has two outcomes y and z, and
a single set of predictor variables x. A similar methodology
can be applied to two–sample problems where there are sepa-
rate predictor variable measurements for y and z. Speciﬁcally
the data consists of two samples {x(1)
, zi}N2
1 .
The predictor values x(1)
correspond to outcomes yi, and the
values x(2)
correspond to zi. The goal to to identify regions in
x - space where the two conditional distributions py(y | x) and
pz(z | x), or selected properties of those distributions, most
diﬀer.

1 and {x(2)

, yi}N1

i

i

i

i

Discrepancy measures for each region Rm of x - space can

be deﬁned in analogy with (1)

dm = D({yi}x(1)

i

∈Rm

, {zi}x(2)

i

∈Rm

).

(24)

Regions and splits are based on the pooled predictor variable
i=1 ∪ {x(2)
sample {xi}N
i=1 with N = N1 + N2.
Tree construction strategy is the same as that described in
Section 3 using (24).

i=1 = { x(1)

i }N1

i }N 2

We illustrate two–sample contrast trees using the census
income data set described in Section 6.1. One of the samples
is taken to be the data from the 32650 males, and the other
sample data from the 16192 females. The goal is to investi-
gate gender diﬀerences in probability of high salary (greater
than $50K/year, $100K 2020 equivalent) in terms of the other
demographic and ﬁnancial variables as reﬂected in this data
set.

The high salary probability averaged over all males in the
data set is 0.30 whereas that for females is 0.11. Thus the rel-
ative odds of high salary for men is almost three times that
for women over the entire data set. Here we use two–sample
contrast trees to investigate whether there are special demo-
graphic and ﬁnancial characteristics for which these relative
odds are diﬀerent. Trees were built on a random half sample
of 24421 observations and the corresponding node statistics
computed on the other left out half.

We ﬁrst use two–sample contrast trees to seek regions in
predictor variable x - space for which male/female relative
high salary probability is larger than 3/1. For this we use a
ratio discrepancy measure

12

22

13

7

5

21

25

23

24

4

Node

n
a
e
M

y
c
n
e
u
q
e
r
F

0
3
.
0

0
2
.
0

0
1
.
0

0
0
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

12

22

13

7

5

21

25

23

24

4

Node

Figure 19: Upper frame: probability of income greater that
$50K for women (blue) and men (red) in regions designed for
relatively large values of the latter. Lower frame: Fraction of
women (blue) and men (red) in each region.

women/men in the region.
In the bottom frame the blue
bars represent the fraction of the 16192 women in the region
whereas red signiﬁes the corresponding fraction of the 32650
men. The blue/red horizontal lines represent the female/male
global average high salary probabilities.

This contrast tree has found several small regions for which
the male/female odds ratio (25) is much greater than its
global average 3/1. For example region 12 containing 551
observations has a 10.3/1 ratio. Region 22 with 501 observa-
tions has a 4.6/1 ratio. However, in all of the highest ratio
regions the actual male/female probabilities of high salary
are well below their respective global averages. In the higher
probability regions the ratios roughly correspond to the cor-
responding global averages.

We next attempt to uncover regions in x - space where the
female/male high salary odds ratio is much greater than its
global average of 1/3 by using the inverse discrepancy measure

dm = mean(yi | x(1)

i ∈ Rm)/mean(zi | x(2)

i ∈ Rm)

(25)

dm = mean(zi | x(2)

i ∈ Rm)/mean(yi | x(1)

i ∈ Rm).

i }Nm

where {yi, x(1)
{zi, x(2)
i }
indicators of high (male and female) salary and x(1)
the corresponding predictor variables.

i=1 represents the Nm = 32650 males and
Nf
i=1 the Nf = 16192 females. Here yi and zi are
are

, x(2)
i

i

Figure 19 summarizes results for a ten region contrast
tree using (25). In the top frame the height of blue/red bars
represent the probability of income greater that $50K for the

Figure 20 summarizes the regions of the corresponding ten
region contrast tree in the same format as Fig. 19. The
tree has uncovered three regions in which the high salary
probability for women is higher than that for men and much
higher than its global average (blue line). In region 12 the
In regions 13
female/male high salary odds ratio is 2.6/1.
and 11 the probabilities are about equal.
In region 11 the
overall probability of high salary for both is relatively very

17

F (blue)  &  M (red)  Big  Salary  Probability

12

5

13

11

21

34

35

20

28

9

Node

n
a
e
M

y
c
n
e
u
q
e
r
F

4
0

.

3
0

.

2
0

.

1
0

.

0
0

.

5

.

0

4
0

.

3
0

.

2
0

.

1
0

.

0
0

.

12

5

13

11

21

34

35

20

28

9

Node

Figure 20: Upper frame: probability of income greater that
$50K for women (blue) and men (red) in regions designed for
relatively large values of the former. Lower frame: Fraction
of women (blue) and men (red) in each region.

high (0.47). This region contains 57% of the males and only
11% of the females in the data set. The rules deﬁning these
three regions are

Node 12

22 ≤ age < 50
&
martial status = never married
&
hours/week ≤ 34

Node 13

age > 50
&
martial status = never married
&
hours/week ≤ 34

Node 11
age > 22
&
martial status = never married
&
hours/week > 34

This data set was originally constructed for the purpose
of comparing performance of various machine learning algo-
rithms for predicting high salary. There is no information
as to its representativeness, even for 1994. The analysis pre-
sented here is meant to illustrate the variety of the types of
problems to which contrast trees can be applied.

Contrast trees can be used to compare any two samples
based on the same measured quantities. In particular, the two
samples may be taken from the same system under diﬀerent
conditions or at diﬀerent times. In these situations contrast
trees can detect the presence of any associated “concept drift”
between the samples, and if detected explain its nature.

18

