9
1
0
2

g
u
A
6
2

]
L
M

.
t
a
t
s
[

1
v
5
1
9
9
0
.
8
0
9
1
:
v
i
X
r
a

Convex Programming for Estimation
in Nonlinear Recurrent Models

Sohail Bahmani
School of Electrical & Computer Engineering
Georgia Institute of Technology
email: sohail.bahmani@ece.gatech.edu

Justin Romberg
School of Electrical & Computer Engineering
Georgia Institute of Technology
email: jrom@ece.gatech.edu

August 28, 2019

Abstract

We propose a formulation for nonlinear recurrent models that includes simple parametric
models of recurrent neural networks as a special case. The proposed formulation leads to a
natural estimator in the form of a convex program. We provide a sample complexity for this
estimator in the case of stable dynamics, where the nonlinear recursion has a certain contraction
property, and under certain regularity conditions on the input distribution. We evaluate the
performance of the estimator by simulation on synthetic data. These numerical experiments also
suggest the extent at which the imposed theoretical assumptions may be relaxed.

Keywords: recurrent neural networks, convex programming

1 Introduction

Given a diﬀerentiable and convex function f : (cid:82)n → (cid:82) with ∇f (0) = 0, we consider the dynamics
described by the recursion

xt = ∇f (A?xt−1 + B?ut−1) ,

(1)

where u0, u1, . . . are i.i.d. copies of a random vector u ∈ (cid:82)p, the initial state x0 is zero, and the
matrices A? ∈ (cid:82)n×n and B? ∈ (cid:82)n×p are the parameters of the model. In the setup described above,
we want to address the following problem.

Problem. Given a time horizon T , estimate the model parameters A? and B?, from a single
observed trajectory (u0, x0 = 0), (u1, x1), . . . , (uT , xT ).

The speciﬁc form of the nonlinearity in (1) might seem strange at ﬁrst, but many common
choices of nonlinearities used in practice are special cases of this formulation. For instance, increas-
ing nonlinearities that act coordinate-wise can be modeled by choosing the appropriate separable
convex function f in (1). Particularly, the (parameterized) ReLU function x 7→ x+ + c(−x)+ for

1

 
 
 
 
 
 
some constant c ≥ 0, which is popular in neural network models, corresponds to the choice of
f (x) = Pn

+/2 in our proposed model.

+/2 + c(−xi)2

i=1(xi)2

With β > 0 denoting a suﬃciently large normalizing constant, we collect the ground truth

parameters in C? =

h
A? β−1B?

i

and for t = 0, 1, . . . , we set

zt =

"

#

.

xt
β ut

Therefore, the dynamics can be equivalently expressed as

z0 =

#

"

0
β u0

, and

zt =

#
"
∇f (C?zt−1)
β ut

, for t ≥ 1 .

Our goal is eﬀectively to estimate C?, from the observations z0, z1, . . . , zT .

Not surprisingly, further model assumptions are needed to exclude inherently intractable instances

of the problem. Below in Section 1.3 we state the assumptions we make to analyze the problem.

1.1 Related work

Recurrent Neural Networks (RNN) and similar models of random dynamical systems have become
the main tool in machine learning applications dealing with sequential data. In this section we
brieﬂy review some recent results that provide theoretical analysis for these models.

Parameter estimation in discrete-time linear dynamical systems whose state variable are generally

governed by the recursion

xt = A?xt−1 + B?ut−1 + ξt ,

(2)

with ξt denoting an additive observation noise, are studied in (Hardt et al., 2018; Faradonbeh et al.,
2018; Simchowitz et al., 2018; Du et al., 2018; Sarkar and Rakhlin, 2019). The diﬀerence in the
mentioned results stem from variations to the model such as

• observing the state variable indirectly, through the sequence

yt = A0

?xt + B0

?ut + ξ0
t ,

(3)

with ξ0

t denoting an additive output noise,

• observing single versus multiple trajectories,

• restricting A? (e.g., maxi |λi(A?)| < 1 in the stable model versus mini |λi(A?)| > 1 in the

explosive model), and

• choosing to have an input (i.e., B? = 0 versus B? 6= 0) with a certain distribution.

Hardt et al. (2018) consider a prediction problem in controllable linear dynamical systems with
indirect observations as in (3). Speciﬁcally, formulating the prediction problem naturally as a
(non-convex) least squares, the prediction error achieved by stochastic gradient descent (SGD)
is analyzed under some technical assumptions. In a stable single input single output setting, it
is shown that with N trajectories of length T observed, the prediction error can be bounded as
O(p(n5 + σ2n3)/(T N )) where n is the number of controllable parameters, and σ2 is the variance
of the zero-mean noise terms ξ0

t in (3).

2

Under technical assumptions, Faradonbeh et al. (2018) establish a sample complexity for
estimation of A? in the explosive regime (i.e., mini |λi(A?)| > 1) with heavy-tailed noise and
deactivated input (i.e., B? = 0).

Simchowitz et al. (2018) analyze the ordinary least squares (OLS) in estimation of A? from a
single trajectory of observations x0, x1, . . . where there is no input (i.e., B? = 0) and the process
noise is i.i.d. samples of a zero-mean isotropic Gaussian random variable. It is shown in Simchowitz
et al. (2018) that for “marginally stable” systems (i.e., maxi |λi(A?)| ≤ 1), the estimate bA produced
(cid:46) pn/T up to
by the OLS, with high probability, achieves the natural error rate of
some constants and log factors depending implicitly on A?. Remarkably, this result applies to
systems where the spectral radius of A? equals one (i.e., maxi |λi(A?)| = 1) where the more standard
arguments based on mixing time which require stability of the system do not apply.

(cid:13)
(cid:13)
(cid:13) bA − A?

(cid:13)
(cid:13)
(cid:13)

Oymak and Ozay (2018) considers estimation from a single trajectory of input/ouput observation
pairs (u0, y0), (u1, y1), . . . where the output sequence y0, y1, . . . is generated by the recursion (3).
Assuming the input, the state noise, and the output noise each to have i.i.d. samples from zero-mean
isotropic Gaussian distributions, (Oymak and Ozay, 2018) studies accuracy of a least squares
i
? B?
approach in estimation of the parameter matrix G? =
that characterizes the dynamics.

?B? A0

?AT −2

· · · A0

?A?B?

? A0

h
B0

Similar to (Simchowitz et al., 2018), Sarkar and Rakhlin (2019) establish the estimation error
rate for OLS in the single observation trajectory regime under the model (2) with deactivated
input (i.e., B? = 0) and sub-Gaussian noise ξt. Particularly, in the three regimes of stable
or marginally stable systems (i.e. maxi |λi(A?)| < 1 + O(1/T )), marginally stable systems (i.e.
maxi |λi(A?)| < 1 − O(1/T )), and explosive systems (in the sense that mini |λi(A?)| > 1 + O(1/T ))
the operator norm of the error roughly decays as 1/

T , 1/T , and e−T , respectively.

√

Du et al. (2018) study the minimax rate of estimation from multiple trajectories in simple linear
recurrent neural networks (and convolutional neural networks). Considering the state variable to be
linearly collapsed to a scalar in the output, under a subGaussian model for the input sequence as
well as the output noise, the mentioned paper provides upper and lower bounds for the minimax risk
of the mean squared error. In particular, it is shown that the minimax rate of estimating from T
trajectories of length L, is orderwise between pmin{n, L}p/T and p(p + L) min{n, p} log(Lp)/T .
From a technical point of view, linearity in recurrent models typically provides the convenience
of “unfolding” the state recursion into explicit equations in terms of the past input. This convenient
feature disappears immediately as nonlinearities are introduced in the recursion as in (1). Miller and
Hardt (2019) showed that in the stable regime nonlinear RNNs can be approximated by “truncated”
RNNs. Furthermore, they showed that, for unstable RNNs, gradient descent does not necessarily
converge. Oymak (2019) studies the parameter estimation under (1) when the nonlinearity ∇f is
replaced by an activation function that is strictly increasing and applies coordinatewise. Formulating
the problem as nonconvex least squares, (Oymak, 2019) establishes a sample complexity for the
convergence of in Frobenius norm. Basically, (Oymak, 2019) shows that if T (cid:38) ρ(n + p) with ρ
being a certain notion of condition number of B?, then with high probability, SGD converges at a
linear, albeit dimension dependent, rate.

In this paper, we generalize the results of (Oymak, 2019) in two directions. First, our formulation
of the recurrence (1) admits a broader class of nonlinearities, and, as will be seen in the sequel, it
enables us to formulate a convex program as the estimator. Second, the analysis of (Oymak, 2019)
relies critically on the assumption that the input distribution is Gaussian. This is partly due to the
use of the Gaussian concentration inequality for Lipschitz functions. At the cost of having a stricter
form of nonlinearity, we relax the requirement on the input distribution by allowing the random
input to have heavier tail.

3

1.2 Proposed Estimator

Our proposed estimator is formulated as a convex program as follows

bC ∈ argmin

T
X

C∈(cid:82)n×(n+p)

t=1

f (Czt−1) − hxt, Czt−1i .

(4)

Readers familiar with convex analysis may observe that if f∗, the convex conjugate of f , is smooth,
then ∇f∗ ≡ (∇f )−1 and (1) is equivalent to

∇f∗(xt+1) = A?xt−1 + B?ut−1 .

Should ∇f∗ be easy to compute, it is evident that the resulting system of linear equations can be
solved by the common least squares approach to estimate A? and B?. However, we prefer (4) as
the estimator, since it can be implemented regardless of f∗ and its properties.

In view of (1) and convexity of f , it is straightforward to verify that C? is a minimizer for (4).
Under the assumptions speciﬁed below in Section 1.3, we will show that the minimizer of (4) is
unique and therefore bC = C?. In particular, with f assumed to be λ-strongly convex, we have

f (Czt−1) − hxt, Czt−1i ≥ f (C?zt−1) − hxt, C?zt−1i +

λ
2

k(C − C?)zt−1k2
2 .

Therefore, to guarantee uniqueness of the minimizer in (4), it suﬃces to show that, with high
probability, the smallest eigenvalue of

Σ def=

T −1
X

t=0

ztzT
t ,

(5)

is strictly positive with high probability.

1.3 Assumptions

With no restricting conditions imposed on the observation model in (1), the posed estimation
problem is not meaningful. For instance, any aﬃne function f is permitted in the core model above,
but clearly its corresponding trajectory conveys no information about C?.

Assumption 1 (regularity of f ). The function f has the following properties:

1. The function f is λ-strongly convex and Λ-smooth in the usual sense, i.e.,

λ
2

ky − xk2

2 ≤ f (y) − f (x) − h∇f (x), y − xi ≤

Λ
2

ky − xk2
2 ,

(6)

holds for all x, y ∈ (cid:82)n.

2. There exist a matrix-valued function F : (cid:82)n → (cid:82)n×n and a relatively small constant ε > 0

such that, for all x, y ∈ (cid:82)n, we have

(cid:13)
(cid:13)
(cid:13)
(cid:13)

1
2

(∇f (x + y) − ∇f (x − y)) − F (x)y

(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≤ εkyk2 .

(7)

4

Perhaps the simplest example of the functions that meet the conditions of Assumption 1 is the
convex quadratic functions. Let Q be a positive semideﬁnite matrix that satisﬁes λI (cid:22) Q (cid:22) ΛI.
Then f (x) = 1
2 xTQx clearly satisﬁes (6), and also satisﬁes (7) for F (x) ≡ Q and ε = 0.
Another example of the function f that meets the above conditions is the piecewise quadratic

function

f (x) =

1
2

n
X

i=1

max{λ(−xi)2

+, Λ(xi)2

+} .

The gradient of this function can be written as

∇f (x) =









Λ+λ

2 |x1|

2 x1 + Λ−λ
...
2 xn + Λ−λ




2 |xn|

Λ+λ

,

whose coordinates happen to be the (parameterized) ReLU functions. For this speciﬁc f , the
mapping F can be chosen as

Λ+λ

2 + Λ−λ

2

sgn(x1)









F (x) =

Λ+λ

2 + Λ−λ

2

sgn(x2)

. . .









sgn(xn)

,

Λ+λ

2 + Λ−λ

2

for which (7) holds with ε = (Λ − λ)/2.

An immediate consequence of Assumption 1 is the following.

Lemma 1. Under Assumption 1, the mapping F obeys

(λ − ε)kyk2 ≤ kF (x)yk2 ≤ (Λ + ε)kyk2 ,

for all x, y ∈ (cid:82)n.

Proof. Using the standard equivalent deﬁnitions of strong convexity and smoothness (Nesterov,
2013, Theorem 2.1.5), we have

λky − xk2 ≤ k∇f (y) − ∇f (x)k2 ≤ Λky − xk2 .

Rewriting these inequalities, in terms of the pair (−x + y, x + y) in place of (x, y), we can obtain

2λkyk ≤ k∇f (x + y) − ∇f (x − y)k2 ≤ 2Λkyk2 .

Furthermore, by (7) and the triangle inequality we have

kF (x)yk2 − εkyk2 ≤

1
2

k∇f (x + y) − ∇f (x − y)k2 ≤ kF (x)yk2 + εkyk2 .

The lemma easily follows from the latter two lines of inequalities.

We make the following assumption on the input u.

Assumption 2 (regularity of the input distribution). The input u has the following properties:

1. The input u ∈ (cid:82)p is a zero-mean isotropic random variable, i.e.,

(cid:69)(u) = 0, and (cid:69)(uuT) = I .

5

2. The coordinates of u have independent symmetric distributions, i.e., for all measurable subsets

A = A1 × . . . × Ap of (cid:82)p, we have

(cid:80)(u ∈ A) =

p
Y

i=1

(cid:80)(ui ∈ Ai) =

p
Y

i=1

(cid:80)(−ui ∈ Ai) .

3. For a certain α ≥ 1, the input u has a bounded directional Orlicz ψα norm, i.e., there exists a

ﬁnite absolute constant K > 0 such that

(cid:16)

e|hh,ui|α/Kα(cid:17)

(cid:69)

≤ 2 .

sup
h∈(cid:83)p−1

(8)

The following lemma is an immediate consequence of (8).

Lemma 2. Let u be the random variable under the Assumption 2 and u0 be an independent copy
u. The vector u has a bounded directional fourth moment, i.e., there exist η ∈ [1, 2(4/α)4/αK4]
such that

(cid:16)
(hh, ui)4(cid:17)

(cid:69)

≤ η ,

(9)

holds for all h ∈ (cid:83)p−1. Furthermore, for all h, h0 ∈ (cid:83)p−1 we have
(cid:16)(cid:0)hh, ui + hh0, ui(cid:1)4(cid:17)

≤ max{η, 3}

khk2

(cid:16)

(cid:69)

2 + (cid:13)

(cid:13)h0(cid:13)
2
(cid:13)
2

(cid:17)2

.

Proof. Clearly, existence of the exponential moments guarantees that (cid:69) (cid:0)|hh, ui|4(cid:1) < ∞ for all
h ∈ (cid:83)p−1. To prove the ﬁrst part, we show that (9) holds for η = 2(4/α)4/αK4. For all h ∈ (cid:83)p−1
we have

(cid:16)

|hh, ui|4(cid:17)

(cid:69)

=

≤

η
2

η
2

(cid:69)

(cid:69)

 (cid:18) |hh, ui|α
(η/2)α/4
(cid:18) 4
α

exp

(cid:18)

|hh, ui|α
(η/2)α/4

(cid:19)(cid:19)

.

(cid:19)4/α!

For the prescribed η we have (η/2)α/4α/4 = Kα. Thus, in view of (8), we obtain
(cid:16)

|hh, ui|4(cid:17)

≤

(cid:69)

2 = η ,

η
2

(cid:69)

(cid:16)(cid:0)hh, ui + hh0, ui(cid:1)4(cid:17)

as desired. Since u and u0 are zero-mean, isotropic, i.i.d, and further obey (9), we have
(hh, ui)4 + 6 (hh, ui)2 (cid:0)hh0, ui(cid:1)2 + (cid:0)hh0, ui(cid:1)4(cid:17)
(cid:13)
2 + η(cid:13)
(cid:13)h0(cid:13)
(cid:13)h0(cid:13)
2
4
(cid:13)
(cid:13)
2
(cid:17)2
2 + (cid:13)
(cid:13)h0(cid:13)
2
(cid:13)
2

2 + 6khk2
2
(cid:16)
khk2

≤ max{η, 3}

≤ ηkhk4

= (cid:69)

(cid:16)

,

which proves the second part.

In addition to the assumptions made above, our analysis crucially depends on a form of contraction
that can be ensured by the following assumption. Note that Λ can be taken as Λ = Lip(∇f ), i.e.,
the Lipschitz constant of ∇f with respect to the usual Euclidean metric.

Assumption 3 (conctractive dynamics). The nonlinearity ∇f (·) and the matrix A? induce a
contraction in the sense that

Λ kA?k < 1 .

6

2 Main result

Our main theorem below, eﬀectively guarantees that T = eO(n + p) is suﬃcient for the matrix Σ to
be (strictly) positive deﬁnite.

Theorem 1. Suppose that the energy of B? is well-spread among its columns in the sense that
kB?k1→2/kB?kF = O(p−1/2). Furthermore, suppose that the constant

θ = θα,β,ε,λ,K,B?

def= −εK log

1

α (10 max{η, 3}) kB?k1→2
(cid:16)

n
β, (λ − ε)λ1/2
min

min

B?\iB?

+ 0.6 min

i=1,...,p

is strictly positive. Furthermore, suppose that L satisﬁes

L ≥ 1 +

(cid:18)

log

c2
θ2 log

(cid:16) 2(T −1)(p+1)
δ

(cid:17) (cid:16) ΛkB?kF
1−ΛkA?k

(cid:17)2(cid:19)

log

1
ΛkA?k

,

for a suﬃciently large constant c > 0. Then, for

T (cid:38) max{η2, 9} (n + p)L log

(cid:19)

(cid:18) eT /L
n + p

+ log

(cid:19)

(cid:18) 8L
δ

,

we have

λmin(Σ) (cid:38)

θ2
max{η, 3}

T ,

(cid:17)o

,

T
\i

(10)

(11)

(12)

with probability ≥ 1 − δ. Consequently, on the same event, (4) recovers C?, exactly.

A critical condition of Theorem 1 is that θ is strictly positive. This condition implicitly requires
ε in (7) to be suﬃciently small, which in turn implies the condition number of f is suﬃciently close
to 1 (i.e. ∇f is nearly linear). Furthermore, it is needed that the energy of B? to be well-spread
not only among its columns, but also in a “spectral” sense. More precisely, we need the quantity

max
i=1,...,p

kB?k1→2
λ1/2
min(B?\iB?

,

T

\i)

to be suﬃciently small. The equation (10) also suggests that a reasonable choice of the normalizing
constant β should satisfy β ≈ (λ − ε)λ1/2
? ).

min (B?BT

3 Simulation

We evaluated the proposed estimator numerically on synthetic data in a setup similar to the
experiments of (Oymak, 2019). In all of the experiments, we consider the dimensions to be n = 50,
p = 100, and the time horizon to be T = 500. For α ∈ {0.2, 0.8} we choose A? = αR with R being a
uniformly distributed n × n orthogonal matrix. Furthermore, B? ∈ (cid:82)n×p is generated randomly with
i.i.d. standard normal entries. The normalizing factor is chosen as β = following what prescribed in
(Oymak, 2019). We consider two diﬀerent models for the input u. Let g ∼ Normal(0, 1) denote a
standard Normal random scalar. The ﬁrst model is similar to the model of (Oymak, 2019) where the
entries of u are i.i.d. copies of g, whereas in the second model takes i.i.d. copies of g3 as the entries

7

of u. We refer to these models as the Gaussian model and the heavy-tailed model, respectively. The
nonlinearity in (1) is described by one of the functions

f (x) =

1 − ρ
2

n
X

(xi)2

+ +

i=1

ρ
2

n
X

i=1

x2
i ,

at ρ = 1 (i.e., linear activation), ρ = 0.5 (i.e., leaky ReLU activation with slope 0.5 over (cid:82)≤0),
ρ = 0.3 (i.e., leaky ReLU activation with slope 0.3 over (cid:82)≤0), and ρ = 0 (i.e., ReLU activation).

For each choice of α and ρ, we solved (4) using Nesterov’s Accelerated Gradient Method (AGM)
(Nesterov, 1983; Nesterov, 2013, Section 2.2), for 100 randomly generated instances of the problem.
For the Gaussian model the step-size is set to 10−3, whereas for the heavy-tailed model the step-size
is set to 10−4. In each trial, the AGM is run for a maximum of 500 iterations and terminated only
if the relative error dropped below 10−8 (i.e.,
F ≤ 10−8). The optimization task
can be solved by the SGD as well. However, slower convergence of the SGD is only tolerable for
large-scale problems where lower memory load is crucial. Nevertheless, because the estimator (4) is
formulated as a convex program, we can apply the SGD methods with variance reduction (see e.g.,
Johnson and Zhang, 2013; Schmidt et al., 2017; Defazio et al., 2014) and rely on their theoretical
guarantees.

(cid:13)
(cid:13)
(cid:13) bC − C?

/kC?k2

(cid:13)
2
(cid:13)
(cid:13)

F

Figures 1 and 2 depict the achieved relative error under the Gaussian model and the heavy-tailed
model for the chosen values of α and ρ, respectively. The solid lines show the median of the achieved
relative error, whereas the dashed lines show the 0.1 and 0.9 quantiles of the relative error. Perhaps,
the result that might strike as counter intuitive at ﬁrst, is that the estimation performance is not
monotonic with respect to the strength of stability. For instance, the plots in the ﬁrst two rows
of Figure 1 suggest that convergence is faster for the less stable system (i.e., α = 0.8). A similar
conclusion can be made regarding the plots in the ﬁrst row of Figure 2 corresponding to linear
activation functions. However, it appears that this behavior is sensitive to the level of nonlinearity,
particularly in the case of the heavy-tailed input distributions.

4 Proof of the main result

Proof of Theorem 1. Recall the deﬁnition of Σ in (5). We would like to ﬁnd a lower bound for
the smallest eigenvalue of Σ that holds with high probability. Consider a suﬃciently large integer L
as a stride parameter and for ‘ = 0, . . . , L − 1 let

T‘ = {t : L ≤ t < T and t = ‘ mod L} ,

which partition {L, . . . , T − 1} to sets of subsampled time indices with stride L. For each ‘ =
0, . . . , L − 1, we deﬁne the “restarted” state variables x(‘)
t

through the recursion

x(‘)

t+1 =




0

∇f



(cid:16)

A?x(‘)

t + B?ut

(cid:17)

, t = ‘ mod L

, t 6= ‘ mod L ,

and the corresponding restarted version of zt as

z(‘)
t =

#

.

"

x(‘)
t
β ut

8

(13)

(a) α = 0.2, ρ = 1

(b) α = 0.8, ρ = 1

(c) α = 0.2, ρ = 0.5

(d) α = 0.8, ρ = 0.5

(e) α = 0.2, ρ = 0.3

(f) α = 0.8, ρ = 0.3

(g) α = 0.2, ρ = 0

(h) α = 0.8, ρ = 0

Figure 1: Gaussian model

9

010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-510-410-310-210-1100iteration0.1 quantilemedian0.9 quantile010020030040050010-2.010-1.510-1.010-0.5100.0iteration0.1 quantilemedian0.9 quantile(a) α = 0.2, ρ = 1

(b) α = 0.8, ρ = 1

(c) α = 0.2, ρ = 0.5

(d) α = 0.8, ρ = 0.5

(e) α = 0.2, ρ = 0.3

(f) α = 0.8, ρ = 0.3

(g) α = 0.2, ρ = 0

(h) α = 0.8, ρ = 0

Figure 2: Heavy-tailed model

10

010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-810-610-410-2100iteration0.1 quantilemedian0.9 quantile010020030040050010-610-510-410-310-210-1100iteration0.1 quantilemedian0.9 quantile010020030040050010-1.510-1.010-0.5100.0iteration0.1 quantilemedian0.9 quantileFor any w ∈ (cid:83)n+p−1 we have

wTΣw ≥

T −1
X

t=L

(wTzt)2 =

L−1
X

X

‘=0

t∈T‘

(wTzt)2 .

To ﬁnd a lower bound for P
(wTzt)2, the strategy is to approximate this summation by its
corresponding restarted version. Aggregating the obtained bounds for all ‘ = 0, . . . , L − 1 then
yields the desired lower bound for wTΣw.

t∈T‘

By the Cauchy-Schwarz inequality we have

1
2
Summing over t ∈ T‘ and rearranging the terms then yields
(cid:16)

(wTzt)2 +

zt − z(‘)
t

wT (cid:16)

(cid:17)(cid:17)2

(cid:17)2

≥

(cid:16)

(cid:16)

X

(cid:16)

wTz(‘)
t

(cid:17)2

.

(wTzt)2 ≥

1
2

X

t∈T‘

wTz(‘)
t

t∈T‘
|

{z
def
= S‘(w)

}

− X
t∈T‘
|

wT (cid:16)

zt − z(‘)
t

(cid:17)(cid:17)2

.

{z
def
= eS‘(w)

}

Observe that the term S‘ (w) is a sum of independent random quadratic functions. Therefore,
deriving a uniform lower bound for S‘ (w) is amenable to standard techniques. We also need
to establish a uniform upper bound for the term eS‘ (w) for which we leverage the contraction
assumption.

Denote the matrix M with its ith column replaced by the zero vector as M\i. The following
‘=0 S‘(w).

lemma, whose proof is relegated to the appendix, provides a uniform lower bound on PL−1
The proof for this lemma is also provided in the appendix.

Lemma 3 (uniform lower bound for S‘ (w)). With probability ≥ 1 − δ, for all w ∈ (cid:83)n+p−1 we have

L−1
X

‘=0

S‘(w) ≥ θ2L|T‘|






0.1
max{η, 3}

−

v
u
u
t

where θ is deﬁned as in (10).

2(n + p) log eT /L

(n+p) + log 4L
|T‘|

δ




 ,

Furthermore, we have the following lemma that establishes a uniform upperbound for PL−1

‘=0 eS‘(w).
Lemma 4 (uniform upper bound for eS‘ (w)). Suppose that µ def= p1/2kB?k1→2/kB?kF = O(1) and
let (cid:15) > 0 be a parameter. If for a certain absolute constant c > 0, we have
(cid:17)2(cid:19)
(cid:18)

log

c2T
(cid:15)

log

(cid:16) 2(T −1)(p+1)
δ

(cid:17) (cid:16) ΛkB?kF
1−ΛkA?k

L ≥ 1 +

,

(14)

log

1
ΛkA?k

then with probability ≥ 1 − δ, we can guarantee

L−1
X

‘=0

eS‘(w) ≤ (cid:15) .

Consquently, under (11) and (12), it follows from Lemmas 3 and 4 that

holds uniformly for all w ∈ (cid:83)n+p−1 with probability ≥ 1 − δ.

wTΣw (cid:38)

θ2
max{η, 3}

T .

11

Acknowledgements

This work was supported in part by the Semiconductor Research Corporation (SRC) and DARPA.

References

V. H. de la Peña and E. Giné. Decoupling: From dependence to independence. Probability and its Applications.

Springer-Verlag, New York, 1999.

A. Defazio, F. Bach, and S. Lacoste-Julien. SAGA: A fast incremental gradient method with support for
non-strongly convex composite objectives. In Advances in Neural Information Processing Systems 27, pages
1646–1654. 2014.

L. Devroye, L. Györﬁ, and G. Lugosi. A probabilistic theory of pattern recognition, volume 31. Springer

Science & Business Media, 2013.

S. S. Du, Y. Wang, X. Zhai, S. Balakrishnan, R. Salakhutdinov, and A. Singh. How many samples are needed

to estimate a convolutional neural network? preprint arXiv:1805.07883 [stat.ML], 2018.

M. K. S. Faradonbeh, A. Tewari, and G. Michailidis. Finite time identiﬁcation in unstable linear systems.

Automatica, 96:342–353, 2018.

M. Hardt, T. Ma, and B. Recht. Gradient descent learns linear dynamical systems. Journal of Machine

Learning Research, 19(29):1–44, 2018.

R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In

Advances in Neural Information Processing Systems 26, pages 315–323. 2013.

V. Koltchinskii. Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Lecture
Notes in Mathematics: École d’Été de Probabilités de Saint-Flour XXXVIII-2008. Springer-Verlag Berlin
Heidelberg, 2011.

J. Miller and M. Hardt. Stable recurrent models. In International Conference on Learning Representations,

2019. URL https://openreview.net/forum?id=Hygxb2CqKm.

Y. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Springer, 2013.

Y. E. Nesterov. A method for solving the convex programming problem with convergence rate O(1/k2). In

Dokl. akad. nauk Sssr, volume 269, pages 543–547, 1983.

S. Oymak. Stochastic gradient descent learns state equations with nonlinear activations. In Proceedings of the
Thirty-Second Conference on Learning Theory, volume 99 of Proceedings of Machine Learning Research,
pages 2551–2579, Phoenix, USA, 25–28 Jun 2019. PMLR.

S. Oymak and N. Ozay. Non-asymptotic identiﬁcation of LTI systems from a single trajectory. preprint

arXiv:1806.05722 [cs.LG], 2018.

R. E. A. C. Paley and A. Zygmund. A note on analytic functions in the unit circle. Mathematical Proceedings

of the Cambridge Philosophical Society, 28(3):266–272, 1932.

T. Sarkar and A. Rakhlin. Near optimal ﬁnite time identiﬁcation of arbitrary linear dynamical systems.
In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of
Machine Learning Research, pages 5610–5618, Long Beach, California, USA, 09–15 Jun 2019. PMLR.

M. Schmidt, N. Le Roux, and F. Bach. Minimizing ﬁnite sums with the stochastic average gradient.

Mathematical Programming, 162(1):83–112, Mar 2017.

12

M. Simchowitz, H. Mania, S. Tu, M. I. Jordan, and B. Recht. Learning without mixing: Towards a sharp
In Proceedings of the 31st Conference On Learning Theory,

analysis of linear system identiﬁcation.
volume 75 of Proceedings of Machine Learning Research, pages 439–473. PMLR, 06–09 Jul 2018.

V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their

probabilities. Theory of Probability & Its Applications, 16(2):264–280, 1971.

A Proofs for technical lemmas

Proof of Lemma 3. For each ‘ = 0, . . . , L − 1, the vectors z(‘)
t with t ∈ T‘ are independent and
identically distributed. Let θ > 0 be a parameter to be speciﬁed later. Using a simple truncation
we can write

X

(cid:16)

(cid:17)2

wTz(‘)
t

t∈T‘

≥ θ2 X
t∈T‘

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:49)

.

To bound the right-hands side of the inequality above uniformly with respect to the set of binary
functions

F‘

def=

z 7→ (cid:49) (|wTz| ≥ θ) : w ∈ (cid:83)n+p−1o
n

,

we can resort to classic VC bounds (Vapnik and Chervonenkis, 1971; see also Devroye et al., 2013,
chapters 13 & 14). Particularly, because the VC dimension of F‘ is no more than 2 (n + p), with
probability ≥ 1 − δ/L we have

1
|T‘|

X

t∈T‘

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:49)

≥ (cid:80)

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

−

v
u
u
t

2(n + p) log e|T‘|

n+p + log 4L
|T‘|

δ

,

for all w ∈ (cid:83)n+p−1. It only remains to ﬁnd appropriate lower bounds for the probability in the
summation. Lemma 6 below provides the needed lower bound.

Taking the union bound over ‘ then shows that with probability ≥ 1 − δ we obtain




L−1
X

‘=0

1
|T‘|

X

t∈T‘

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:49)

≥ L

0.1
max{η, 3}




−

v
u
u
t

2(n + p) log e|T‘|

n+p + log 4L
|T‘|

δ


 ,

which yields the desired bound.

Proof of Lemma 4. Recall the deﬁnition of z(‘)
have

t

in (13). For every t ∈ T‘ and w ∈ (cid:83)n+p−1 we

(cid:16)

wT (cid:16)

zt − z(‘)
t

(cid:17)(cid:17)2

≤

=

t

(cid:13)
(cid:13)
2
(cid:13)zt − z(‘)
(cid:13)
(cid:13)
(cid:13)
2
(cid:13)
(cid:13)
(cid:13)∇f (C?zt−1) − ∇f

(cid:16)

C?z(‘)
t−1

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

.

Furthermore, we can write

(cid:13)
(cid:13)
(cid:13)∇f (C?zt−1) − ∇f

(cid:16)

C?z(‘)
t−1

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

zt−1 − z(‘)
t−1
(cid:16)

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

∇f (C?zt−2) − ∇f

(cid:16)

≤ Λ2(cid:13)
(cid:13)
(cid:13)C?
= Λ2(cid:13)
(cid:13)
(cid:13)A?
≤ (ΛkA?k)2 (cid:13)
(cid:16)
(cid:13)
(cid:13)

∇f (C?zt−2) − ∇f

(cid:16)

C?z(‘)
t−2
(cid:16)

(cid:17)(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2
(cid:17)(cid:17)(cid:13)
2
C?z(‘)
(cid:13)
(cid:13)
t−2
2

.

13

Using the above inequality recursively yields

(cid:13)
(cid:13)
(cid:13)∇f (C?zt−1) − ∇f

(cid:16)

C?z(‘)
t−1

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

≤ (ΛkA?k)2(L−2) (cid:13)
(cid:13)
(cid:13)∇f (C?zt−L+1) − ∇f
≤ (ΛkA?k)2(L−1) kxt−Lk2
2 .

(cid:16)

C?z(‘)

t−L+1

(cid:17)(cid:13)
2
(cid:13)
(cid:13)
2

Therefore, we deduce that

(cid:16)

wT (cid:16)

zt − z(‘)
t

(cid:17)(cid:17)2

≤ (ΛkA?k)2(L−1) kxt−Lk2
2 .

(15)

Furthermore, for any time index s ≥ 1 we have

kxsk2 ≤ ΛkA?xs−1 + B?us−1k2

≤ ΛkA?kkxs−1k2 + ΛkB?us−1k2 .

Therefore, we can write

max
1≤s≤T −1

kxsk2 ≤ ΛkA?k max

1≤s≤T −1

kxs−1k2 + Λ max

1≤s≤T −1

kB?us−1k2 ,

which implies

max
1≤s≤T −1

kxsk2 ≤

Λ
1 − ΛkA?k

max
1≤s≤T −1

kB?us−1k2 .

Since µ = p1/2kB?k1→2/kB?kF = O(1) by assumption, using the matrix Bernstein inequality,

stated in Lemma 5 below, for each s = 1, . . . , T − 1, with probability ≥ 1 − δ/(T − 1) we have

kB?us−1k2 ≤ ckB?kF log

1
2

(cid:18) 2(T − 1)(p + 1)
δ

(cid:19)

,

for some absolute constant c > 0. It then follows from a simple union bound that

max
1≤s≤T −1

kB?us−1k2 ≤ ckB?kF log

1
2

(cid:18) 2(T − 1)(p + 1)
δ

(cid:19)

,

holds with probability ≥ 1 − δ. Consequently,

max
1≤s≤T −1

kxsk2 ≤ c log

1
2

(cid:18) 2(T − 1)(p + 1)
δ

(cid:19) ΛkB?kF
1 − ΛkA?k

,

holds with probability ≥ 1 − δ. Under the same event and in view of (15) we have

(cid:16)

wT (cid:16)

zt − z(‘)
t

(cid:17)(cid:17)2

≤ (ΛkA?k)2(L−1) c2 log

(cid:18) 2(T − 1)(p + 1)
δ

(cid:19) (cid:18) ΛkB?kF
1 − ΛkA?k

(cid:19)2

,

for all w ∈ (cid:83)n+p−1, 0 ≤ ‘ ≤ L − 1, and t ∈ T‘. Summation over t ∈ T‘ then yields

eS‘ (w) = X
t∈T‘
T
L

≤

(cid:16)

wT (cid:16)

zt − z(‘)
t

(cid:17)(cid:17)2

(ΛkA?k)2(L−1) c2 log

(cid:18) 2(T − 1)(p + 1)
δ

(cid:19) (cid:18) ΛkB?kF
1 − ΛkA?k

(cid:19)2

.

14

Therefore, for (cid:15) > 0 if

L ≥ 1 +

(cid:18)

log

c2T
(cid:15)

log

(cid:16) 2(T −1)(p+1)
δ

(cid:17) (cid:16) ΛkB?kF
1−ΛkA?k

(cid:17)2(cid:19)

log

1
ΛkA?k

,

then with probability ≥ 1 − δ for all w ∈ (cid:83)n+p−1 we have

L−1
X

‘=0

eS‘ (w) ≤ (cid:15) .

B Auxiliary lemmas

We use a special case of a matrix Bernstein inequality (Koltchinskii, 2011, Corollary 2.1). For
reference, the following lemma states the special inequality we need; we omit the proof and refer
the reader to (Koltchinskii, 2011) for the general Bernstein inequality.

Lemma 5. Suppose that u obeys the Assumption 2. Furthermore, deﬁne a coherence parameter for
B? as µ def= p1/2kB?k1→2/kB?kF. Then, for some absolute constant c > 0, and any γ ∈ (0, 1], the
bound

kB?uk2

≤ max

(
c

(cid:16)

1
2 log

1
2

2γ−1(p + 1)

(cid:17)

, c max{K, 2}µ log

1
α (max{K, 2}µ)

log (cid:0)2γ−1(p + 1)(cid:1)
p1/2

)

kB?kF ,

holds with probability ≥ 1−γ. In particular, if µ = O(1), meaning that the weight of B? is distributed
almost evenly across its columns, and p is suﬃciently large, the bound stated above eﬀectively reduces
to

kB?uk2 ≤ ckB?kF log

(cid:16)

1
2

2γ−1(p + 1)

(cid:17)

,

for some absolute constant c > 0.

In general, the coherence parameter µ deﬁned in Lemma 5 obeys 1 ≤ µ ≤ p1/2. However, we
assume we operate in the scenario that µ = O(1) so that we apply the simpler bound stated in the
lemma. Therefore, choosing γ = 1/p and for a suﬃciently large p we have

(cid:80)

wT

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

"

F (A?x(‘)

t−1)B?ut−1
β ut
"
F (A?x(‘)

t−1)B?ut−1
β ut

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ (cid:80)

wT

− εkB?ut−1k2 ≥ θ

!

≥ θ + cε log

1

2 (2p(p + 1)) kB?kF

!

−

1
p

.

for some absolute constant c > 0.

Lemma 6 (lower bound for the probabilities). With θ deﬁned as in (10), for each ‘ ∈ {0, 1, . . . , L−1},
and every t ∈ T‘ we have

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:80)

≥

0.1
max{η, 3}

15

Proof. For t = 0, 1, . . . , let it be i.i.d. integers uniformly distributed over {1, . . . , p}, independent of
everything else. For any vector v, we use the notation v−i to denote the vector obtained by ﬂipping
the sign of the ith coordinate of v. Furthermore, for t ∈ T‘ let

z(‘)
t =


∇f (A?x(‘)


t−1 + B?u−it−1
−β ut

t−1


 .

Recall that, by assumption, ut−1 and ut have coordinates with independent symmetric distributions.
Therefore, it is straightforward to show that z(‘)
are identically distributed, and for any
θ > 0 we can write

and z(‘)
t

t

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:80)

=

≥

1
2
1
2

(cid:80)

(cid:80)

t

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)
(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:17)

+

1
(cid:12)
(cid:12)
(cid:12) ≥ θ
2
(cid:12)
(cid:12)
(cid:12)wTz(‘)
(cid:12)
(cid:12)
(cid:12) +

t

(cid:80)

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)
(cid:17)

t

(cid:12)
(cid:12)
(cid:12) ≥ 2θ

.

(cid:17)

(cid:12)
(cid:12)
(cid:12) ≥ θ

Then, it follows from the triangle inequality, and the assumption (7), that

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θ

(cid:17)

(cid:80)

≥

≥

≥

1
2
1
2

1
2

(cid:80)

(cid:80)

(cid:80)

(cid:16)(cid:12)
(cid:12)

(cid:12)wT (cid:16)

"

"

 (cid:12)
(cid:12)
(cid:12)
wT
(cid:12)
(cid:12)
 (cid:12)
(cid:12)
(cid:12)
wT
(cid:12)
(cid:12)

(cid:17)(cid:12)
(cid:12)
(cid:12) ≥ 2θ

(cid:17)

t

z(‘)
t − z(‘)
∇f (A?x(‘)

F (A?x(‘)

t−1)B?

2β ut

(cid:16) 1
2 ut−1 + 1
β ut

2 u−it−1

t−1

t−1 + B?ut−1) − ∇f (A?x(‘)

t−1 + B?u−it−1

t−1

!

≥ 2θ

#(cid:12)
(cid:12)
)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

− ε

(cid:13)
(cid:13)
B?
(cid:13)
(cid:13)

(cid:18) 1
2

ut−1 −

1
2

u−it−1
t−1

!

.

≥ θ

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)2
(16)

Furthermore, for any γ ∈ (0, 1] we can write

(cid:80)

+ (cid:80)

≥ (cid:80)

wT

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18)(cid:13)
(cid:13)
B?
(cid:13)
(cid:13)
 (cid:12)
(cid:12)
(cid:12)
wT
(cid:12)
(cid:12)

"
F (A?x(‘)

t−1)B?

(cid:16) 1
2 ut−1 + 1
β ut

2 u−it−1

t−1

(cid:17)

− ε

(cid:13)
(cid:13)
B?
(cid:13)
(cid:13)

(cid:18) 1
2

ut−1 −

1
2

u−it−1
t−1

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

!

≥ θ

ut−1 −

(cid:18) 1
2
F (A?x(‘)

"

1
2

(cid:19)(cid:13)
(cid:13)
u−it−1
(cid:13)
t−1
(cid:13)2
(cid:16) 1
2 ut−1 + 1
β ut

t−1)B?

≥ K log

1
α

2 u−it−1

t−1

(cid:19)

(cid:19)

kB?k1→2

≥ θ + εK log

(cid:19)

1
α

(cid:18) 2
γ

!

kB?k1→2

.

(17)

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:18) 2
γ
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:17)

Observe that (v − v−i)/2 = v|i and (v + v−i)/2 = v|\i are respectively the selectors of the ith
coordinate and its complement. With this convention, on one hand we can write

(cid:80)

(cid:18)(cid:13)
(cid:13)
B?
(cid:13)
(cid:13)

(cid:18) 1
2

ut−1 −

1
2

u−it−1
t−1

(cid:19)(cid:13)
(cid:13)
(cid:13)
(cid:13)2

≥ K log

ut−1 |it−1

(cid:17)(cid:13)
(cid:13)
(cid:13)2

≥ K log

1
α

(cid:18) 2
γ

= (cid:80)

(cid:18)(cid:13)
(cid:13)
(cid:13)B?

(cid:16)

≤ γ ,

(cid:19)

(cid:18) 2
γ

1
α

(cid:19)

kB?k1→2
(cid:19)

kB?k1→2

(cid:19)

(18)

where the third line follows from the fact that kB?k1→2 is equal to the greatest ‘2 norm of the
columns of B?, and that under the assumption (8) we have

(cid:18)(cid:12)
(cid:12)
(cid:12)(ut−1)it−1

(cid:80)

(cid:12)
(cid:12)
(cid:12) ≥ K log

1
α

(cid:19)(cid:19)

(cid:18) 2
γ

≤ γ .

16

On the other hand, we can write

B?

(cid:18) 1
2

ut−1 +

(cid:19)

u−it−1
t−1

1
2

= B?\it−1ut−1

and invoke Lemma 7 below to obtain

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

wT

"
F (A?x(‘)

t−1)B?\it−1ut−1

β ut

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ 0.36 min

i=1,...,p

min

n
β, (λ − ε)λ1/2
min

(cid:16)

B?\iB?

T
\i

!

(cid:17)o

≥

0.4
max{η, 3}

Therefore, recalling the assumed condition (7), by choosing

(19)

and

θ = θα,β,ε,λ,K,B? ,

γ =

0.2
max{η, 3}

,

and in view of (16), (17), (18), and (19) we obtain the desired bound

(cid:16)(cid:12)
(cid:12)wTz(‘)
(cid:12)

t

(cid:12)
(cid:12)
(cid:12) ≥ θα,β,ε,λ,K,B?

(cid:17)

(cid:80)

≥

0.1
max{η, 3}

.

Lemma 7. With the notation and conditions as in Lemma 6 we have

 (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:80)

wT

"
F (A?x(‘)

t−1)B?\it−1ut−1

β ut

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ 0.36 min

i=1,...,p

min

n
β, (λ − ε)λ1/2
min

(cid:16)

B?\iB?

T
\i

!

(cid:17)o

≥

0.4
max{η, 3}

Proof. By conditioning on x(‘)
1932; de la Peña and Giné, 1999, Corollary 3.3.2) we have

t−1 and applying the Paley-Zygmund inequality (Paley and Zygmund,


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:80)

wT

"
F (A?x(‘)

t−1)B?\it−1ut−1

β ut

2

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ 0.36 (cid:69)


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

wT

"
F (A?x(‘)

t−1)B?\it−1ut−1

β ut

(cid:12)
(cid:12) x(‘)
(cid:12)

t−1









(cid:12)
(cid:12) x(‘)
(cid:12)

t−1

#(cid:12)
2
(cid:12)
(cid:12)
(cid:12)
(cid:12)


(cid:69)

≥ 0.4


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:69)

wT

"

F (A?x(‘)

t−1)B?\it−1ut−1

β ut

"

wT

F (A?x(‘)

t−1)B?\it−1ut−1

β ut

2

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)




2





(cid:12)
(cid:12) x(‘)
(cid:12)

t−1

4





(cid:12)
(cid:12) x(‘)
(cid:12)

t−1

(20)

Using the assumption that ut−1 and ut are independent, zero-mean, and isotropic we obtain



(cid:69)



"

(cid:12)
(cid:12)
(cid:12)
wT
(cid:12)
(cid:12)

F (A?x(‘)

t−1)B?\it−1ut−1

β ut

2

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)


 =

(cid:12)
(cid:12) x(‘)
(cid:12)

t−1

"(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

F (x(‘)

t−1)B?\it−1

(cid:17)T

0

0
β I

#

w

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

17

Furthermore, in view of Lemma 2, the denominator in (20) can be bounded from above as

"

wT


(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)

(cid:69)

F (A?x(‘)

t−1)B?\it−1ut−1

β ut

4

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12) x(‘)
(cid:12)

t−1


(cid:13)
(cid:13)
(cid:13)
 ≤ max{η, 3}
(cid:13)
(cid:13)

"(cid:16)

F (x(‘)

t−1)B?\it−1

(cid:17)T

0

0
β I

#

w

4
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

Therefore, (20) reduces to

"

wT



(cid:80)



(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

F (A?x(‘)

t−1)B?\it−1ut−1

β ut

2

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥ 0.36 (cid:69)

"

wT





(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

F (A?x(‘)

t−1)B?\it−1ut−1

β ut





(cid:12)
(cid:12) x(‘)
(cid:12)

t−1





(cid:12)
(cid:12) x(‘)
(cid:12)

t−1

2

#(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

≥

0.4
max{η, 3}

.

It follows from Lemma 1 that

(21)

λmin

 "

F (y)B?\i
0

#"(cid:16)

0
βI

F (y)B?\i
0

(cid:17)T

#!

0
βI

≥ min{β2, (λ − ε)2λmin

(cid:16)

B?\iB?

T
\i

(cid:17)

}

for all y. In particular,

min{β2, (λ − ε)2 λ2

min

(cid:16)

B?\iB?

T
\i

(cid:17)

} ≤

"(cid:16)

(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)

t−1)B?\i

F (x(‘)
0

(cid:17)T

0
βI

#

w

2
(cid:13)
(cid:13)
(cid:13)
(cid:13)
(cid:13)
2

.

Therefore, the conditional expectation in (21) can be replaced by

min
i=1,...,p

min{β2, (λ − ε)2 λmin

(cid:16)

B?\iB?

T
\i

(cid:17)

} .

Finally, taking the expectation with respect to x(‘)

t−1 completes the proof.

18

