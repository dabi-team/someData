2
2
0
2

r
a

M
9

]

Y
S
.
s
s
e
e
[

2
v
6
3
6
0
0
.
3
0
2
2
:
v
i
X
r
a

Distributional Reinforcement Learning for Scheduling of Chemical
Production Processes

Max Mowbraya, Dongda Zhanga,∗, Ehecatl Antonio Del Rio Chanonab,∗

aCentre for Process Integration, School of Chemical Engineering and Analytical Science, The University of
Manchester, Manchester, M13 9PL, United Kingdom
bSargent Centre for Process Systems Engineering, Department of Chemical Engineering, Imperial College London,
London, SW7 2AZ, United Kingdom

Abstract

Reinforcement Learning (RL) has recently received signiﬁcant attention from the process systems en-
gineering and control communities. Recent works have investigated the application of RL to identify
optimal scheduling decision in the presence of uncertainty. In this work, we present a RL methodol-
ogy tailored to eﬃciently address production scheduling problems in the presence of uncertainty. We
consider commonly imposed restrictions on these problems such as precedence and disjunctive con-
straints which are not naturally considered by RL in other contexts. Additionally, this work naturally
enables the optimization of risk-sensitive formulations such as the conditional value-at-risk (CVaR),
which are essential in realistic scheduling processes. The proposed strategy is investigated thoroughly
in a parallel batch production environment, and benchmarked against mixed integer linear program-
ming (MILP) strategies. We show that the policy identiﬁed by our approach is able to account for
plant uncertainties in online decision-making, with expected performance comparable to existing MILP
methods. Additionally, the framework gains the beneﬁts of optimizing for risk-sensitive measures, and
identiﬁes online decisions orders of magnitude faster than the most eﬃcient optimization approaches.
This promises to mitigate practical issues and ease in handling realizations of process uncertainty in
the paradigm of online production scheduling.

Keywords: Distributional Reinforcement Learning, Optimal Control, Chemical Production
Scheduling, Multi-product Parallel Batch Operations, Machine Learning

1. Introduction

1.1. Online production scheduling: optimization and simulation

The development of methods for eﬃcient production scheduling of batch processes is an area of
signiﬁcant interest within the domain of process systems engineering and operations research [1, 2].
There are three main drivers for research within online production scheduling: a) identifying modelling

∗Corresponding authors
Email addresses: dongda.zhang@manchester.ac.uk (Dongda Zhang), a.del-rio-chanona@imperial.ac.uk

(Ehecatl Antonio Del Rio Chanona)

Preprint submitted to Elsevier

March 11, 2022

 
 
 
 
 
 
approaches that integrate with the practicalities of online scheduling, b) considering plant uncertain-
ties, and c) handling nonlinearities [3]. There is a diverse array of modelling and solution methods
for production scheduling problems [4]. On one hand, modelling approaches can be broadly classiﬁed
as discrete or continuous time [5]; on the other hand, optimization approaches can be characterized
as simulation-based or optimization. The former is generally underpinned by stochastic search algo-
rithms [6], whereas the latter is dominated via the use of mixed integer programming (MIP). However,
industrial practice is generally dominated by the use of heuristics given the challenges outlined in a-c.
Recent works have argued that formulation of the underlying scheduling problem in terms of a
discrete-time, state space model alleviates many of the problems associated with the former challenge
(i.e. a)) [7]. The beneﬁts of this approach primarily relate to the ease of incorporating state-feedback
into the scheduling MIP model and mitigate the requirement for various heuristics to update the
model when uncertainty is observed (as is the case in other approaches e.g. continuous-time models)
[8]. However, the intuitive beneﬁts inherited from state-space modelling produces scheduling models
of a much larger size than continuous-time formulations. This has implications for the solution time
and suboptimality can be introduced if the discretization of the time domain is too coarse. Further,
considering uncertainty within the model formulation is essentially intractable via MIP solution meth-
ods, at least for problems of industrial size. For example, in the case of stochastic MIP, considerable
approximations to the scenario tree are required to solve the models (even oﬄine) [9]. This means
the most practical way to consider uncertainty is often via robust formulations. This leads to smaller
model sizes but requires approximation of plant uncertainties via a few ﬁnite dimensional, deterministic
expressions [10]. It is well known that such formulations are generally conservative [11].

Considering uncertainty is widely acknowledged as an important facet of solution approaches [12,
13, 14]. In the contenxt of scheduling, the simulation community has demonstrated strong beneﬁts in
handling uncertainty [15, 16]. The high level idea here is generally to compute approximately optimal
schedules by evaluating their performance via a Monte Carlo method and a stochastic search algorithm.
These approaches have also proven useful in integrating decision-making functions where nonlinearities
often arise in the underlying model [17]. This is primarily because one avoids the formulation of
mixed integer nonlinear programs (MINLP). However, it should be noted that optimization approaches
fair well with regard to integrated problems when the resultant formulation is mixed integer linear
programming (MILP) [18, 19]. In addition, simulation based approaches are generally expensive to
conduct online because of the sample ineﬃciency associated with Monte Carlo methods and stochastic
search algorithms.

The expense of conducting simulations online has led to the development of Reinforcement Learning
(RL) based approaches to online production scheduling [20]. The primary idea here is to exploit the
Markov decision process (MDP) framework to model production scheduling systems as a stochastic
process. One can then use RL to learn a functionlization of an optimal decision policy for the underlying
environment through oﬄine simulations of the model. The policy function can then be deployed online
to provide scheduling decisions, instead of online optimization. This allows the scheduling element to
a) consider nonlinearities and uncertainties within the process dynamics, b) use arbitrary distributions
of the uncertainty, and c) identify scheduling decisions in a very short time frame (i.e. on the order
of micro to milliseconds). Despite the additional work required in oﬄine, simulation-based policy
learning, these facets are appealing in the context of modern production environments.

1.2. Online production scheduling and Reinforcement Learning

Although RL has been demonstrated for sequential decision making in a number of case studies
[21, 22, 23, 24], its application to physical production systems has been relatively limited. For example,

2

[25] applied deep Q networks to optimize a ﬂexible jobshop (i.e. a multipurpose multi-stage production
facility), however, the authors provide little information as to how the method proposed accounts for
constraints. Further, the approach is benchmarked to common heuristics rather than optimization
formulations. In [26], an RL rescheduling approach is presented to repair and ensure the feasibility of
the schedule when subject to realizations of plant uncertainty. However, the method does not consider
the optimality of the original schedule and is not benchmarked to an existing method. More recently,
[20] provided an extensive analysis of a Reinforcement Learning approach compared to deterministic
and stochastic MIP for a single-stage, continuous production scheduling problem. The RL method is
demonstrated to be an appealing solution approach. Despite their achievements, as this is a ﬁrst proof-
of-concept, the case study considered is relatively simple and does not involve precedence constraints
or requirements for setup time between the end and start of subsequent operations within a given unit.
In addition, considering constraints is an important step in the development of RL algorithms,
given the MDP framework does not provide an explicit mechanism to handle them. There has been
much research in other decision-making problems regarding this [27, 28, 29, 30]. The presence of
precedence and disjunctive constraints in production scheduling provides a challenge of a diﬀerent
nature. Detailed examples of these constraints are explored in Section 3.2. Further, the use of RL
poses challenges such as robustness and reliability [31].

Recently, there has been a lot of interest in the development of distributional RL algorithms [32].
Instead of formalizing the objective via expected performance, distributional RL algorithms try to ﬁnd
the optimal policy for other measures of the performance distribution [32]. This enables identiﬁcation
of more risk-sensitive policies and consideration of the tails of the policy performance [33]. The
proposition of risk-sensitive formulations has been common to mathematical programming for some
time [34], however, its use in scheduling problems has been limited [35, 36]. In this work, we utilize
distributional RL to consider risk-sensitive formulations and low probability, worst-case events. This
is particularly important in engineering and business applications, where decision-making is generally
risk-averse and catastrophic events are highly disfavoured.

1.3. Contribution

In this work, we develop the methodology of Reinforcement Learning to production scheduling by
proposing a novel and eﬃcient policy optimization method that a) handles precedence and disjunctive
constraints, b) is practical and observes stability in learning, and c) provides means to identify risk-
sensitive policies. In doing so, we improve the reliability and robustness of RL algorithms, but also
inherit the advantages of identifying online scheduling decisions in real time from a function, and ease
in handling nonlinearity and uncertainty.

To handle a), we present a logic based framework that implements transformations of the RL
scheduling decisions to ensure they satisfy those constraints derived from propositional logic (i.e.
precedence and disjunctive constraints). This is discussed in detail in Section 3.2. The optimization of
the policy in view of this framework is handled by a stochastic search optimization algorithm (PSO-
SA) which combines particle swarm optimization (PSO) and simulated annealing (SA) to balance
exploitation and exploration of the policy parameters. The use of stochastic search optimization
approaches lend themselves naturally to distributional RL and facilitate b) and c). Speciﬁcally, one can
estimate risk-sensitive measures such as the conditional value-at-risk via a Monte Carlo method. This
has particular advantage over other policy gradient and action-value based approaches to distributional
RL, as one is not required to make assumption on the form of the performance distribution induced
by the policy. Further, the use of stochastic search methods removes the dependence on ﬁrst-order
policy learning methods, which can lack robustness. This is partly due to the use of noisy directions

3

for policy improvement, whose evaluation is known to be expensive (i.e. policy gradients and deep Q
learning) and sometimes unreliable [37, 38].

The proposed method is benchmarked on a classical production scheduling problem against an MIP
approach. The problem is a multiproduct batch plant with parallel production lines. The case study
is a modiﬁed version of the original study provided in [39] to include processing time and due date
uncertainty. Extensive analysis of the results is provided.

The rest of this paper is organized as follows:

in Section 2, we present the problem statement;
in Section 3 we outline a novel and eﬃcient RL approach to scheduling; in Section 4, we outline the
details of a case study, the results and discussion of which are presented subsequently in Section 5;
and, in Section 6 we ﬁnish with concluding thoughts and plans for future work.

2. Problem Statement

The scheduling problem is generally subject to both endogenous and exogenous sources of uncer-
tainty. In this work, we focus on the online scheduling of parallel, sequential batch operations in a chem-
ical production plant [4]. We assume that the state of the plant at a given time index, t ∈ {0, . . . , T },
within a discrete ﬁnite time horizon (of length T ), is represented by a state, xt ∈ X ⊆ Rnx , where xt
can be thought as (but not limited to) current product and raw material inventory, unit availability,
and tasks currently being processed. At discrete time steps within the scheduling process of the plant,
the scheduler (agent or algorithm who decides the scheduling actions) is able to observe the state of the
plant, and select a control action, ut ∈ U ⊆ Znu , which represents an appropriate scheduling decision
on the available equipment. The state of the plant then evolves according to the following diﬀerence
equation:

xt+1 = f (xt, ut, st)
(1)
where st ∈ S ⊆ Rns represents a realization of some uncertain plant parameters or disturbance. Eq.
1 describes the stochastic evolution of the plant and could be equivalently expressed as a conditional
probability density function (CPDF), p(xt+1|xt, ut). Here, we identify that the system has the Markov
property (i.e. the future state only depends on the current state and control actions) and hence the
system may be described as a Markov decision process (MDP).

The aim of the scheduler is to minimize objectives such as makespan (which deﬁnes the time to
complete all of the required tasks on the available equipment) and the tardiness of product completion.
Given these objectives, one can deﬁne a reward function, R : X × U × X → R, which describes the
performance of the decisions taken (e.g.
the higher the reward, the more proﬁt achieved by the
scheduler). Solution methods for MDPs aim to identify a control policy, π : X → U, whose aim is to
maximize the reward:

T −1
(cid:88)

Z =

Rt+1

π∗ = arg max

π

t=0
Eπ

(cid:2)Z|X0 ∼ p(x0)(cid:3)

(2a)

(2b)

where X0 ∈ X is the initial state of the plant, which is treated as a random variable and described
by an initial state distribution, p(x0). The return, Z ∼ pπ(z), is a random variable that is described
according to a probability density function, pπ(z), under the current policy, π, because the plant
dynamics are subject uncertainty. Generally, exact expressions of pπ(z) in closed form are unavailable.
Therefore, the solution policy, π∗, (i.e. Eq. 2b) is evaluated via the sample average approximation.

4

t × ˆU(2)

Operationally, there is a constraint set, ˆUt = ˆU(1)

t ⊂ Z, that deﬁnes
the available tasks or jobs that may be scheduled in the nu units at any given time index. This may be
deﬁned by the viable sequencing of operations in units; requirements for unit cleaning and maintenance
periods; requirements for orders to be processed in campaigns; and that processing of batches must be
ﬁnished before another task is assigned to a given unit, to name a few. General intuition behind these
constraints is provided in Section 3.2. These constraints may or may not be functions of uncertain
process variables (e.g.
if processing times are subject to uncertainties). In essence, we are trying to
solve a discrete time ﬁnite horizon stochastic optimal control problem (SOCP) of the form:

, where ˆU(l)

. . . × ˆU(nu)

t

t

P(π) :=






(cid:2)Z(cid:3)

Eπ

max
π
s.t.
X0 ∼ p(x0)
st ∈ S ⊆ Rns
xt+1 = f (xt, ut, st)
ut = π(xt)
ut ∈ ˆUt ⊂ Znu
∀t ∈ {0, ..., T }

(3)

where π : X → U is a control policy, which takes the state as input and outputs a control action,
ut. In practice, mixed integer approaches to the scheduling problem either make large approximations
to the SOCP formed (i.e. via stochastic programming) or assume description of a nominal model,
neglecting the presence of uncertain variables entirely. The latter approach is especially common when
considering problem sizes of industrial relevance. In the following section, we present an eﬃcient and
novel methodology to identify a policy π, which provides an approximately optimal solution to the
ﬁnite horizon SOCP detailed by Eq. 3, via RL. Speciﬁcally, π(θ, ·) is deﬁned by a neural network
with parameters, θ ∈ Rnθ . The problem (i.e. Eq. 3) is then reduced to identifying the optimal policy
parameters, θ∗.

3. Methodology

The approach that follows is proposed given the following characteristics of the RL production
scheduling problem: a) Eq. 3 formulates control inputs (decisions) as discrete integer values, U ⊂ Znu ,
that identify the allocation of a task (or job) in a unit at a given time index, and b) one must handle
the hard constraints imposed by ut ∈ ˆUt.

3.1. Identifying discrete control decisions

In this work, we are concerned with identifying a policy, which is suitable for optimization via
stochastic search methods and is able to handle relatively large1 control spaces. The reasoning behind
the use of stochastic search is discussed extensively in Section 3.3. However, as stochastic search
methods are known not to perform well when the eﬀective dimension of a problem is high2, there is a
requirement for careful construction of the policy function.

1The term “large” is used to indicate a control space with high cardinality, |U|.
2Stochastic search is known to be less eﬀective when the number of decision variables is in the order of 1000s, as is

common in the neural network function approximations often used in RL.

5

We propose to make predictions via the policy function in a continuous latent space, wt ∈ W ⊆ Rnu ,
and then transform that prediction to a corresponding discrete control decision, ut. As a result, the
policy function requires an output dimensionality equivalent to that of the control space (i.e. nu the
number of units or equipment items in the context of production scheduling). The transformation could
be deﬁned by a stochastic or deterministic rounding policy. For example, the nearest integer function
(i.e. round up or down to the nearest integer), denoted fr : W → U, is a deterministic rounding policy
demonstrated implicitly by many of the studies provided in [40]. Both of these transformations are
non-smooth and generally assume that the latent space, w ∈ W, is a relaxed, continuous equivalent of
the original control space. In this work, we implement a nearest integer function approach, fr(w) =
nint(w).

In the context of this work, this approach is appealing because it minimizes the number of output
nodes required in the policy function, and as a result, the number of function parameters. This lowers
the eﬀective dimensionality of the problem compared to other means of control selection (e.g. directly
predicting the probability mass of a control in the output of the function, as is common in policy
gradients with discrete control spaces3). This is key for the use of stochastic search optimization
methods.

In the subsequent section, we explore the development of an approach to handling constraints

imposed on the scheduling problem.

3.2. Constraint handling

Handling the constraints imposed on RL control selection (input constraints) is generally done
explicitly [30, 41]. In the class of problem of concern to this work, the structure of the constraints on
the control space arises from standard operating procedures (SOPs). SOPs can generally be condensed
into propositional logic. For example, consider the special case of a production environment with one
processing unit available and two tasks i and m that require processing. We may deﬁne a sequencing
In the
constraint through the following statement: “task i can only be processed after task m”.
language of propositional logic, this statement is associated with a True or False boolean.
If task
m has just been completed and the statement is True (i.e. task i can succeed task m), then task i
belongs to the constraint set at the current time index t, i.e. i ∈ ˆUt. This is known as a precedence
constraint. More complex expressions can be derived by relating two or more propositional statements.
For example, consider the following expression that relates requirements for starting, T s, and end times,
T f , of tasks i and m in a production environment with one unit:

{T s

i ≥ T f

m} ∨ {T s

m ≥ T f
i }

where ∨ is a disjunction operator and can be interpreted as an OR relation. Essentially, this statement
says that no two tasks can be processed in a given unit at the same time, and is otherwise known as a
disjunctive constraint [4]. Such scheduling rules are conventionally transcribed into either generalized
disjunctive programming or mixed integer programming formulations. Both solution methods enable
constraint satisfaction.

In this work, we hypothesise that a set of controls, ut ∈ ¯Ut ⊆ Znu , may be identiﬁed that adhere
to the logic provided by the SOPs at each discrete control interaction, based on the current state of
the plant, xt ∈ Xt. This functional transformation is denoted, fSOP : U × X → ¯U, where ¯U ⊆ Znu ,
and is assumed non-smooth.

3This approach requires |U| nodes in the output layer of the policy function, where |U| >> nu.

6

If we are able to deﬁne and satisfy all constraints via propositional logic (i.e. fSOP ), we proceed
to use the rounding policy, fr, deﬁned in Section 3.1. Speciﬁcally, by redeﬁning fr : W → ¯U, the
implementation provides means to identify a mapping from a continuous latent space to (possibly
discrete) controls that are feasible. This mechanism is widely known via action shaping, action masking
or domain restriction, and has been shown to signiﬁcantly improve the eﬃciency of RL learning process
[42, 40].

In the case one is unable to identify (and satisfy) all constraints ˆU via fSOP , we penalise the
violation of those constraints that cannot be handled, and incorporate a penalty function for the
constraint violation, φ : X × U × X → ϕt+1 ∈ R. For example, using discrete-time state space models,
one cannot impose the constraint “no two units can start processing the same task, at the same time”
via this logical transformation, without imposing considerable bias on the control selection of the
policy. This is discussed further in Appendix B.5. Given g(x, u) = [g1(x, u), . . . , gng (x, u)], where
gi : X × U → R, ∀i ∈ {1, . . . , ng}, represent the ng constraints that cannot be handled innately via
propositional logic, we deﬁne the penalised return from an episode as:

φ = R − κg

(cid:13)[g(x, u)]+(cid:13)
(cid:13)
(cid:13)p

Z φ =

T −1
(cid:88)

t=0

ϕt+1

(4)

where κg ∈ R is the penalty weight; [y]+ = max(0, y) deﬁnes an element wise operation over y ∈ Rny ;
and, (cid:107)·(cid:107)p deﬁnes the general lp-norm. From here we can identify a solution policy to Eq. 3, π∗ as
follows:

π∗ = arg max

Eπ

(cid:2)Z φ(cid:3)

π

(5)

A ﬁgurative description of the approach proposed is provided by Fig. 1, and formalized by Algorithm
1.

Figure 1: Figurative description of the feedback control framework used to formulate the scheduling problem. A)
A feedback control framework that utilizes logic to identify feasible scheduling decisions. B) Control selection via a
deterministic rounding policy.

3.3. Stochastic search policy optimization

Due to the nature of the problem at hand, this work proposes the use of stochastic search policy
optimization algorithms. In general, any stochastic search algorithm can be used. A high level de-
scription of the algorithm used in this work is provided by Algorithm 2. Summarizing this approach,

7

Algorithm 1: Control selection for production scheduling of an uncertain plant

Input: Policy function, π(θ0, ·); functional transformation derived from plant standard operat-
ing procedure, fSOP : X × U → ¯U; description of uncertain plant dynamics, f : X × U × S → X
(see Eq. 1); rounding policy, fr : W → ¯U; initial state distribution, p(x0); the number of
units (equipment) available, nu; the number of tasks to be processed, nT ; the control set,
U = {(u(1), . . . , u(nu)) | u(l) ∈ U(l), ∀l ∈ {1, . . . nu}}, where U(l) ⊂ Z+; penalty function,
φ : X × U × X → R; and, empty memory buﬀer, Binf o;

1. Draw initial state, x0 ∼ p(x0)
2. for t = 0, . . . , T − 1 do

a. Identify control set, ¯Ut = {fSOP (xt, u) ∈ Znu , ∀u ∈ U} ⊂ Znu ;
b. Predict latent coordinate conditional to current state, wt = π(xt; θ);
c. Implement rounding policy, ut = fr(wt);
d. Implement scheduling decision and simulate, xt+1 = f (xt, ut, st), where st ∈ S;
e. Observe feedback from penalty function, ϕt+1 = φ(xt, ut, xt+1);

end

3. Assess Z φ (see Eq.4) and store, together with information required for policy optimization,
in Binf o

Output: Binf o

we use a hybrid algorithm, which combines particle swarm optimization (PSO) [43] with simulated
annealing (SA) [44] and a search space reduction strategy [45]. For clarity, we refer to this algorithm
simply as PSO-SA. The hybridization of the two algorithms helps to eﬃciently balance exploration
and exploitation of the policy function’s parameter space. For more details on this algorithm, please
refer to Appendix A.

The use of stochastic search policy optimization inherits three major beneﬁts. The ﬁrst being that
stochastic search algorithms are easily parallelizable, which means oﬄine computational time in policy
learning can be reduced. The second beneﬁt is that one is freed from reliance on accurate estimation
of ﬁrst order gradients indicative of directions for policy improvement, as in policy gradients and deep
Q learning. Estimation of these directions is known to be computationally expensive [37], and there
is potential for policies to become stuck in local optima, as well as instability in policy learning. This
is particularly likely because the loss landscape with respect to the policy parameters is thought to be
non-smooth and rough in scheduling problems4 [46, 47]. The ﬁnal bonus is that one can easily optimize
for a variety of measures of the distribution of penalised return, pπ(zφ), (i.e. go beyond optimization
in expectation as is declared in Eq. 2b) provided one is able to obtain a suﬃcient number of samples.
This is discussed further in the next section.

4This is due to the structure of the control space and the nature of the scheduling task, i.e. controls that are close in

the control space (have small euclidean distance between them) do not necessarily induce similar process dynamics.

8

3.4. Optimizing for the distribution of returns

Recent developments within RL have enabled the ﬁeld to move beyond optimization in expecta-
tion. This subﬁeld is known as distributional RL. One of the advantages provided by distributional
RL is that one can incentivize optimization of the tails of the distribution of returns, providing a
more risk-sensitive formulation. The concept of distributional RL was ﬁrst introduced by [48] in 2017.
However, optimization of risk-sensitive criteria has been well established in the portfolio optimization
and management community for some time, with the advent of e.g. value-at-risk (VaR), conditional
value-at-risk (CVaR) [49], and Sharpe’s ratio. Conventionally, (stochastic gradient-based) distribu-
tional RL algorithms are dependent upon making approximations of the probability density function
of returns pπ(zφ), so as to gain tractable algorithms [48, 33]. The major beneﬁt of stochastic search
approaches is that one is freed from such approximations by simply estimating the interested measure
(e.g. mean, varaince, (C)VaR) directly from samples (i.e. a Monte Carlo method). In the following,
we outline the CVaR in the context of stochastic search policy optimization, and reasons for its use as
metric to optimize pπ(zφ).

3.4.1. The conditional value-at-risk (CVaR)

The CVaR is closely related to the value-at-risk (VaR). The VaR deﬁnes the value, zφ

β , of the
random variable, Z φ, that occurs with probability less than or equal to a certain pre-deﬁned level,
β = [0, 1], under the cumulative distribution function (CDF), Fπ(zφ) = P(Z φ ≤ zφ|π). Informally,
VaR (for some β) gives us a value, which is the best possible value we can get with a probability of at
most β. For example, conceptualize that Z φ represents the mass produced in a process plant per day
(kg/day). If the plant has zφ
β = 1000 kg/day, and β = 0.05, this means that there is a 0.05 probability
that the production will be of 1000 kg/day or less.

In the context of sequential decision-making problems, it is important to note that the CDF, and
hence the VaR, is dependent on the policy, π. The following deﬁnitions are provided in terms of reward
maximization, rather than loss (given the context of RL and MDPs). Speciﬁcally, the VaR is deﬁned:

β = max{zφ : Fπ(zφ) ≤ β} ⇐⇒ zφ
zφ

β = F −1

π (β)

(6)

It follows then that the CVaR is the expected value, µφ
probability less than or equal to β under Fπ(zφ):

β ∈ R, of the random variable, Z φ, with a

µφ
β = zφ

β +

1
β

(cid:90)

zφ<zφ
β

pπ(zφ)(zφ − zφ

β )dzφ

(7)

Eq. 7 expresses the CVaR (for a given probability β). This can be interpreted as the VaR minus the
expected diﬀerence between the VaR and the returns, zφ, which are realized with probability ≤ β, i.e.
such that Fπ(zφ) ≤ β. This is further reinforced by Fig. 2, which provides a visualization of the VaR
and CVaR.

The optimization of the CVaR has advantage over the VaR in engineering applications because it
provides more information about the performance of the policy within the tails of the distribution,
pπ(zφ). This is particularly beneﬁcial if the distribution is characterised by heavy tails, which is often
the case, given the possibility for rare events within the plant. Further, Z φ
β possesses many undesirable
functional properties that makes its optimization more diﬃcult than µφ
β [34, 50]. Furthermore, the
accuracy of the estimation of the Z φ
β [51]. Therefore, in
this work we favor the CVaR and present means to estimate from samples in the proposed framework.

β via samples, converges at a slower rate than µφ

9

(a) A probability density function view

(b) A cumulative distribution function view

Figure 2: Description of the conditional value-at-risk, CV aRβ , and the value-at-risk, V aRβ , for a given probability level
β, as well as the expected value, µ under a) the probability density function, pπ(zφ), and b) the cumulative distribution
function, Fπ(zφ).

3.4.2. Optimization of CVaR via sampling

We seek means by which to gain approximations of Eqs. 6 and 7 via sampling. Speciﬁcally, assuming
N ], then Eq. 6 may be approximated via the (cid:98)βN (cid:99)th order

M C = [zφ

1 , . . . , zφ

one has N realizations, Z φ
statistic, which we denote ¯zφ

β . The CVaR, µφ

β, may then be approximated via ¯µφ

β as follows:

β = ¯zφ
¯µφ

β +

1
βN

(cid:34) N
(cid:88)

i=1

(cid:35)

min(0, zφ

i − ¯zφ
β )

(8)

Here, we simply replace the integral term in Eq. 7 via its sample average approximation. The minimum
operator enforces the bounds of integration detailed, such that only realizations less than the VaR are
considered. The statistical properties associated with estimation of ¯µφ
β have been well researched.
Under some assumptions, the Monte Carlo estimate of the CVaR converges with increasing samples
at the same rate as the sample average approximation [51, 52]. We direct the interested reader to the
cited works for more information.

How one incorporates this distributional perspective into the RL problem is highly ﬂexible when
applying stochastic search policy optimization. The CVaR may be either optimized as a sole objective
or it may be enforced as a constraint subject to appropriate deﬁnition of β. In other words, one may
formulate either of the two following optimization problems:

π∗
β = arg max

π

¯µφ
β

π∗
β = arg max

π

Eπ

(cid:2)Z φ(cid:3)

s.t.

¯µφ
β ≥ b

(9a)

(9b)

where b ∈ R is some minimum desired value. For the use of RL, we are dependent upon obtaining a
closed form expression as an optimization objective. Optimization of Eq. 9a is trivial. Whereas Eq. 9b
may be handled via a penalty method, or simply by discarding infeasible policies in the case of stochastic
search algorithms. Please see [50] for more information on CVaR optimization formulations. A general
approach to optimization of either of the formulations (Eqs. 9a and 9b) is provided by Algorithm
2. The description is kept general to ensure clarity of the methodology. For further information on
stochastic search optimization and the algorithm used in this work, PSO-SA, please see Appendix A.

10

Algorithm 2: A general approach to distributional stochastic search policy optimization

Input: Policy function (a neural network in this study), π(ˆθ, ·), parameterized by ˆθ; a number
of samples to evaluate each candidate policy, nI ; sample approximation of objective function to
optimize, fSA(·); a general stochastic search optimization algorithm, fSSO(·); population size,
P ; upper, θU B, and lower, θLB, bounds on the search space; population initialization method,
finit(ˆθ, P, θU B, θLB); a number of optimization iterations, K; a memory buﬀer, BSSO; memory
for metrics of optimal policy, Bπ∗ = {Jπ∗ , π(θ∗, ·)}
1. Generate initial (neural network) population parameters, Θ1 = finit(ˆθ, P, θU B, θLB), where
Θ1 = {θ1, . . . , θP };
2. for k = 1, . . . , K do

a. Construct policy population, Πk = {π(θi, ·), ∀θi ∈ Θk};
b. for each (neural network) policy πc,i ∈ Πk do

i. Evaluate penalized return of the policy, Z φ, of πc,i via Algorithm 1 for nI samples;
ii. Return distribution information of the policy, Bπc,i = {B1
Algorithm 1;
iii. Assess sample approximate objective, Ji = fSA(Bπc,i);
iv. Collect information for policy πc,i and append [Ji, Bπc,i] to BSSO;
v. if Ji > Jπ∗ then update best known policy Bπ∗ = {Ji, πc,i};

inf o, . . . , BnI

inf o} from

end

c. Generate new parameters Θk+1 = fSSO(BSSO), where Θk+1 = {θc,1, . . . , θc,P };
end

Output: π(θ∗, ·) ∈ Bπ∗

To provide demonstration of the ideas discussed, we now investigate the application of the method-
ology on a case study, which has been adopted from early work provided in [39]. We add plant
uncertainties to the case study and construct a discrete-time simulation of the underlying plant. The
case study is discussed further in the next Section.

4. Case Studies

4.1. Problem deﬁnition

We consider a multi-product plant where the conversion of raw material to product only requires
one processing stage. We assume there is an unlimited amount of raw material, resources, storage and
wait time (of units) available to the scheduling element. Further, the plant is completely reactive to
the scheduling decisions of the policy, π, although this assumption can be relaxed (with appropriate
modiﬁcation to the method stated here) if decision-making is formulated within an appropriate frame-
work as shown in [20]. The scheduling element must decide the sequencing of tasks (which correspond
uniquely to client orders) on the equipment (units) available to the plant. The production scheduling
environment is characterized by the following properties and requirements:

11

1. A given unit l has a maximum batch size for a given task i. Each task must be organized
in campaigns (i.e. processed via multiple batches sequentially) and completed once during the
scheduling horizon. All batch sizes are predetermined, but there is uncertainty as to the process-
ing time (this is speciﬁc to task and unit).

2. The task should be processed before the delivery due date of the client, which is assumed to
be an uncertain variable (the due date is approximately known at the start of the scheduling
horizon, but is conﬁrmed with the plant a number of periods before the order is required by the
client).

3. There are constraints on the viable sequencing of tasks within units (i.e. some tasks may not be

processed before or after others in certain units).

4. There is a sequence and unit dependent cleaning period required between operations, during

which no operations should be scheduled in the given unit.
5. Each task may be scheduled in a subset of the available units.
6. Some units are not available from the beginning of the horizon and some tasks may not be
processed for a ﬁxed period from the start of the horizon (i.e. they have a ﬁxed release time).
7. Processing of a task in a unit must terminate before another task can be scheduled in that unit.

The objective is to minimize the makespan and the tardiness of task (order) completion. Once
all the tasks have been successfully processed according to the operational rules deﬁned, then the
decision making problem can be terminated. The problem is modelled as an MDP with a discrete-
time formulation. The original work [39] utilized a continuous-time formulation. Further discussion is
provided later in the text on this topic. Full details of the MDP construction, uncertain state space
model and model data used in this work is provided by Appendix B.

4.2. Benchmark

The benchmark for the following experiments as presented in Section 4.3 is provided by both oﬄine
and online implementations of the continuous time, mixed integer linear programming (MILP) model
ﬁrst detailed in [39].

To ensure that the solutions from the two time transcriptions (i.e. the continuous time formulation
of the benchmark and the discrete-time formulation of the methodology proposed) are comparable,
the data which deﬁnes the sequence dependent cleaning times, task-unit dependent processing times
and release times are redeﬁned from the original study to ensure that their greatest common factor is
equal to the length of a time interval in the discrete-time transcription.

Online implementation was dependent upon updating the model data by receiving feedback from
the state of the plant at a given discrete time index. Given that there are uncertainties in the plant
dynamics, the MILP model utilizes the expected values of the uncertain data. Similarly, in RL ex-
pected values of uncertain variables are maintained in the state representation, until the uncertainty
is realized at which point the state is updated appropriately [20]. Please see [39] and Appendix C for
more information on the model and data used for the following experiments, respectively. All MILP
results reported were generated via the Gurobi v9.1.2 solver together with the Pyomo v6.0.1 modelling
framework. The proposed method utilized the PyTorch v1.9.0 python package and Anaconda v4.10.3.

4.3. Experiments
4.3.1. Problem instances and sizes

In the following, we present the formulation of a number of diﬀerent experiments across two diﬀerent
problem instances. The ﬁrst problem instance is deﬁned by a small problem size. Speciﬁcally, we are

12

concerned with the sequencing of 8 client orders (tasks) on four diﬀerent units. This corresponds to 304
binary decision variables and 25 continuous decision variables within the benchmark continuous-time
MILP formulation. The second problem instance investigates the ability of the framework to handle
larger problems with 15 orders and 4 units. In this case, the MILP formulation consists of 990 binary
decision variables and 46 continuous decision variables.

4.3.2. Study designs and assessment of performance

Both of the problem instances are investigated thoroughly. We demonstrate the ability of the
method to handle: a) uncertainty in processing times; b) uncertainty in the due date; and, c) the
presence of ﬁnite release times. These three considerations, a)-c), are used as factors to construct a
full factorial design of experiments. Each factor has two levels, either it is present in the underlying
problem, or it is not. As a result, the majority of analysis focuses on the investigation of the method’s
ability to optimize in expectation. This is captured by experiments E1-E8 in Table 1.

A number of the experimental conditions were used to demonstrate the ability of the method to
optimize for the CVaR of the distribution also. This was investigated exclusively within problem
instance 1, as detailed by experiments D3-D8 in Table 1.

In all experiments, the processing time uncertainty is deﬁned via a uniform probability density
function (see Eq. B.10a) and the due date uncertainty is described by a Poisson distribution (see Eq.
B.10b).

Table 1: Table of experimental conditions investigated. Details of the exact descriptions of uncertain variables are
provided by Appendix B.

Optimizing in Expectation (Eq. 5)

Problem Instance 1 & 2

Reference Processing time uncertainty Due date uncertainty Finite release times

E1
E2
E3
E4
E5
E6
E7
E8

(cid:55)
(cid:55)
(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)

(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)

Optimizing for Conditional Value at Risk (CVaR) (Eq. 9a)

Problem Instance 1

Reference Processing time uncertainty Due date uncertainty Finite release times

D3
D4
D5
D6
D7
D8

(cid:55)
(cid:55)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
(cid:88)
(cid:55)
(cid:55)
(cid:88)
(cid:88)

(cid:55)
(cid:88)
(cid:55)
(cid:88)
(cid:55)
(cid:88)

Further to the experiments proposed in Table 1, the proposed and benchmark methods are com-

13

pared with respect to online and oﬄine computational burden. The robustness of the proposed RL
method to misspeciﬁcation of the plant uncertainties is also investigated. The results are detailed in
Section 5.4 and 5.5, respectively.

(cid:2)Z(cid:3), the standard deviation of the performance, σZ = Σπ

The performance indicators used to evaluate the respective methods include the expected perfor-
(cid:2)Z(cid:3)
mance of the scheduling policy, µz = Eπ
and the conditional-value-at-risk. Speciﬁcally, we use a version of the CVaR, ¯µβ, which considers the
non-penalised returns5 with β = 0.2 (i.e. ¯µβ represents the expected value of the worst case policy
performance observed with probability less than or equal to 0.2). Finally, we utilize a statistically ro-
bust approximation to the probability of constraint satisfaction, FLB. Formal details of evaluation of
FLB are provided by Appendix E. In the following section, the results for the diﬀerent experiments are
presented for the method proposed and benchmarked relative to a continuous-time MILP formulation.

5. Results and Discussion

In this section, we turn our attention to analysing the policy training process and ultimate perfor-
mance of the framework proposed. In all cases, results were generated by the hybrid PSO-SA stochastic
search algorithm with space reduction. In learning (or stochastic search), all candidate policies were
evaluated over nI = 50 samples (when the plant investigated was subject to uncertainty, if determinis-
tic nI = 1), with a population size of P = 60 and maximum optimization iterations of K = 150. The
structure of the network utilized for policy paramaterization is detailed by Appendix A.

5.1. Policy training

Demonstration of the training proﬁles for experiment E8, problem instance 1 are provided by Fig.
3. Fig. 3a details the formulation provided by Eq. 5 and Fig. 3b details Eq. 9a (i.e. the expected
and CVaR objective, respectively). The plots detail how the methodology steadily makes progress
with respect to the measure of the objective posed by the respective problem formulation. The VaR
represents the maximum objective performance observed with probability less than or equal to 0.2.
Hence, the similarity between the VaR and the expected performance, as expressed by Fig. 3, indicates
that the returns are not well described by a Gaussian distribution [53]. This is an approximation
required by many distributional RL algorithms, but not stochastic search optimization approaches
as used in this work. Optimization iterations proceed after population initialization. Therefore, the
performance of the initial best known policy is dependent upon the initialization. Thereafter, each
iteration consists of evaluating each candidate policy in the population over nI = 50 samples, such that
150 iterations is equivalent to obtaining 450,000 simulations of the uncertain model. The computational
time cost of this is discussed in Section 5.4. All current best known policies identiﬁed throughout
training satisfy the constraints imposed on the problem, indicating the eﬃcacy of framework proposed.

5.2. Problem instance 1

In this section, the results of investigations for experiments corresponding to problem instance 1 are
presented. In this problem instance there are 8 customer orders and 4 units available to the production
plant. Firstly, we present results conﬁrming the ability of the method to identify an optimal scheduling
solution for a deterministic plant, which corresponds to the generation of an oﬄine production schedule.
We then turn our attention to scheduling of the plant subject to uncertainties.

5This is assessed via appropriate modiﬁcation to Eq. 8, such that we consider the sum of rewards, rather than the

return under the penalty function.

14

(a) Expected training proﬁle

(b) Distributional training proﬁle

Figure 3: The training proﬁle of the RL agent on experiment E8, problem instance 1. Metrics of the best known policy
are tracked as the population is iterated. Plot a) shows the mean, standard deviation (shaded region around the expected
proﬁle), value-at-risk (β = 0.2), the corresponding conditional-value-at-risk and probability of constraint satisfaction,
FLB, for formulation Eq. 5. Plot b) displays the same information for formulation Eq. 9a.

5.2.1. Optimization of the deterministic plant (oﬄine production scheduling)

There are two experiments that investigate the optimality of the method proposed in generation
of an oﬄine production schedule: E1 and E2 (see Table 1). Investigation E1 deﬁnes a deterministic
plant without ﬁnite release times, whereas E2 includes ﬁnite release times.

Fig. 4a and 4b provides a comparative plot of the schedule generated via the policy identiﬁed via the
method proposed and the MILP formulation for experiment E1, respectively. As detailed from Table
2, the policy obtained from the method proposed achieves the same objective value as the benchmark
MILP method, despite the slight diﬀerence in the structure of the solutions identiﬁed. The diﬀerence
in the RL solution arises in the sequencing of tasks in unit 3 and 4, but does not aﬀect the overall
quality of schedule identiﬁed. Importantly, this result highlights the ability of the method proposed
to account for e.g. sequence dependent cleaning times, sequencing constraints, and requirements to
complete production in campaigns in the appropriate units as imposed in this case study

Next, we turn our attention to the handling of ﬁnite release time within the framework provided.
This is probed by experiment E2. Again, a comparative plot of the relative production schedules
identiﬁed by the MILP formulation and proposed method is provided by Fig. 4c and 4d. Despite
a slight diﬀerence in the structure of the solution, arising from the sequencing of tasks in unit 3; as
detailed from Table 2, the policy obtained from the method proposed achieves the same objective
value as the benchmark MILP method on experiment E2. This further supports the capacity of the
framework to handle the constraints imposed on the problem. In the following section, we explore the
ability of the framework to handle plant uncertainties.

5.2.2. Optimization of the uncertain plant (reactive production scheduling)

In this section, we turn our attention to demonstrating the ability of the framework to handle
plant uncertainty. Speciﬁcally, we incrementally add elements of plant uncertainty to the problem via
investigation of experiments E3-E8. Again, the MILP formulation is used to benchmark the work.
For both approaches, policies were validated by obtaining 500 Monte Carlo (MC) simulations of the

15

(a) Experiment E1: RL solution

(b) Experiment E1: MILP solution

(c) Experiment E2: RL solution

(d) Experiment E2: MILP solution

Figure 4: Investigating the oﬄine schedule generated for the deterministic plant. The results for experiment E1, problem
instance 1 generated by the a) RL and b) MILP methods. The results for experiment E2, problem instance 1 generated
by the c) RL and d) MILP methods. The label Ti details the scheduling of task i in a given unit.

policy under the plant dynamics. This enables us to accurately estimate measures of the distribution
of return, pπ(z).

Table 2 presents the results for the method proposed and the MILP benchmark for experiments
E3-8. Here, the proposed method aims to optimize µZ (the expected performance), whereas the
MILP formulation assumes the plant is deterministic, and only accounts for plant uncertainty via
In 4 out of 6 experiments (E3, E4, E5 and E7), the RL method outperforms the
state-feedback.
MILP approach. The most signiﬁcant of these is experiment E7, where the proposed approach (-
67.4) outperforms the MILP (-71.6) by 5.8% in the objective. The MILP approach only marginally
outperforms the proposed method (by ≤ 0.2%) in 2 of 6 of the experiments. This is observed in the
results of experiment E6 and E8.

Further, in all but one of the experiments, constraints are respected absolutely across all 500
MC simulations by both the methods.
In experiment E8, however, the method proposed violated
the constraints in what equates to 1 out of the 500 MC simulations. This is primarily because in
the optimization procedure associated with the method candidate policies are evaluated over 50 MC
simulations. This means that a realization of uncertainty, which caused the policy to violate the
constraint expressed by the penalty function in validation, was not observed in the learning phase
(see Appendix B.5 for more information regarding the speciﬁc constraint). This can be mitigated
practically by increasing the number of samples, nI , that a policy is evaluated for, at the cost of
increased computation.

Most of the analysis so far has concentrated on the expected performance, µZ, of the policy and the

16

Table 2: Table of results for the proposed method from investigation of experimental conditions detailed by Table 1 for
Problem Instance 1. The policies synthesised were optimized under the objective provided by Eq. 5.

Reference Method

µZ

E1
E2
E3
E4
E5
E6
E7
E8

Proposed

-62.0
-65.0
-61.9
-66.0
-66.8
-73.8
-67.4
-75.3

σZ

0.0
0.0
4.4
4.9
8.7
10.7
10.9
11.5

¯µβ

FLB Method

µZ

-62.0
-65.0
-72.5
-75.6
-86.0
-97.5
-86.8
-101.13

1.0
1.0
1.0
1.0
1.0
1.0
1.0
0.99

MILP

-62.0
-65.0
-63.3
-66.3
-70.1
-73.6
-71.6
-75.1

σZ

0.0
0.0
4.4
4.9
9.6
10.3
11.3
11.7

¯µβ

FLB

-62.0
-65.0
-72.2
-76.5
-90.2
-94.0
-93.5
-97.7

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

probability of constraint satisfaction, FLB. Generally, all standard deviation, σZ, of pπ(z) across the
experiments are similar for both the RL approach and the MILP approach. There is some discrepancy
between the CVaR, ¯µβ, of the RL and MILP which considers the worst 20% of returns associated
with pπ(z), however, this should be interpreted with care given that there is no justiﬁcation for either
method’s formulation to be better than the other. However, this can be incentivized in the RL approach
and this is discussed in the subsequent section.

5.2.3. Optimizing risk-sensitive measures

In this section, we present the results from investigation of experiments D3-8. The diﬀerence
between these experiments and E3-8, is that now we are interested in optimizing for a diﬀerent measure
of the distribution, pπ(z). In E3-8, the objective was optimization in expectation, µZ. In D3-8, the
objective is optimization for the expected cost of the worst 20% of the penalised returns, i.e. Eq.
9a, with β = 0.2. Again, the optimization procedure associated with the proposed method utilizes a
sample size of nI = 50. All policies were then evaluated for 500 MCs.

Table 3: Results for distributional RL corresponding to investigation of experimental conditions detailed by Table 1 for
Problem Instance 1. Results that are emboldened detail those policies that show improved CVaR, ¯µβ , for β = 0.2, over
the MILP approach (as detailed in Table 2).

Method

Reference

µZ

Proposed

D3
D4
D5
D6
D7
D8

-62.1
-65.5
-67.2
-74.9
-67.9
-73.2

σZ

4.1
4.9
8.2
11.2
9.7
11.6

¯µβ

-69.8
-75.5
-83.9
-99.0
-87.1
-96.7

FLB

0.99
0.99
1.0
1.0
0.99
1.0

Table 3 details the results of the investigations D3-8. As before, the results corresponding to ¯µβ
quantify the CVaR of the sum of rewards, Z, i.e. not including penalty for violation of constraints.
The results that are emboldened show improvements in the CVaR over the MILP results as detailed in
Table 2. To reiterate, the only diﬀerence here is that the MILP uses the expected values of data in its
formulation, whereas the distributional RL approach (the proposed method) accounts for uncertainty
in the data and optimizes for the expected value of the worst 20% of penalized returns. This leads to

17

improvements in the CVaR in 5 out of 6 of all experiments (D3-8), with an average improvement of
2.29% in the expected value of the worst 20% of the returns. This ﬁgure should be interpreted with
care, given that there is likely to be some statistical inaccuracy from the sampling process. However,
the weight of the result supports that there is an improvement in performance gained from the RL
formulation.

The expected performance, µZ, of the policy identiﬁed for experiments D3-8 is also competitive
with the MILP formulation. However, this is not promised to hold generally, given improvements
in the expected performance are not incentivized within the formulation (i.e. Eq. 9a, although this
result is also noted in [54]). To balance both objectives the formulation provided by Eq. 9b could be
investigated, if it is of interest to a given production setting.

(a) Histogram of objective performance

(b) Empirical cumulative distribution function

Figure 5: The distributions of returns observed in validation of the RL policy obtained from optimizing expectation (i.e.
E8) and conditional value-at-risk (i.e. D8) within the same production environment. Plot a) shows a histogram of the
returns, and b) shows a plot of the empirical cumulative distribution function associated with each policy.

Similar observations can be made with respect to the probability of constraint satisfaction as in
Section 5.2.2. In 3 of the 6 experiments, the constraint imposed in the penalty function is violated in
approximately 1 of the 500 MC simulations. Again, this could be mitigated by increasing the number
of samples, nI , in policy evaluation during the optimization procedure. However, even at the current
setting of nI = 50, the constraints are respected with high probability, and this was deemed suﬃcient
for this work.

Fig. 5 expresses the distribution of returns under the RL policies optimizing expectation and
CVaR, as obtained in experiment E8 and D8, respectively. Both policies are learned in production
environments deﬁned by the same dynamics and uncertainties. Their quantitative metrics can be
found in Table 2 and 3, respectively. Particularly, Fig. 5b highlights how the CVaR policy observes
improved performance in the tail of the distribution over the policy optimizing for expectation (i.e.
the CVaR plot is shifted along the x-axis to the right). Although, the policy does not observe as good
a best-case performance, this eloquently highlights the utility of distributional formulations to identify
risk-sensitive policies.

18

5.3. Problem instance 2

5.3.1. Optimization of the deterministic plant (oﬄine production scheduling)

Here, the results of investigation of experiments E1 and E2 are presented for problem instance
2. The purpose of the investigation is to determine whether the method is able to scale with larger
problem instances eﬃciently and retain a zero optimality-gap as in Section 5.2. Both experiments
consider the plant to be deterministic, with E2 additionally considering the presence of ﬁnite release
times.

Fig. 6a and 6b presents the comparative results of the schedule identiﬁed for experiment E1 of
the RL and MILP approach. There are clear diﬀerences between the schedules. These arise in the
sequencing of task 6 and 9 in unit 1, and in the sequencing of unit 3. However, for this case the
RL approach is able to retain a zero optimality-gap. This provides promise for the method proposed
in scaling to larger problem sizes. This is reinforced by the value of the objective as presented in
Table 4, where both methods achieve an objective score of -107. Fig. 6c and 6d presents comparative

(a) RL solution

(b) MILP solution

(c) RL solution

(d) MILP solution

Figure 6: Investigating the oﬄine schedule generated for the deterministic plant. The results for experiment E1, problem
instance 2 generated by the a) RL and b) MILP methods. The results for experiment E2, problem instance 2 generated
by the c) RL and d) MILP methods. The label Ti details the scheduling of task i in a given unit.

results of the schedule generated for experiment E2. Here, we see that again there is slight diﬀerence
between the schedule generated for the RL relative to the MILP. However in this case, there is an
optimality gap of 4.4% in the RL schedule generated. This is conﬁrmed by Table 4, which details the
RL method achieved an objective score of -143.0, whereas the MILP method achieved a score of -137.0.
The diﬀerence between the schedules, which causes the optimality-gap, is observed in the scheduling
of task 10 in unit 2 (in the case of RL). This leads to a slight increase in the overall makespan of

19

production. This could be attributed to the complex and non-smooth mapping between the state and
optimal control, which makes model structure selection a diﬃcult task when using parametric models,
such as neural networks. An alternate hypothesis exists in the parameter space, rather than the model
space. The mapping between policy parameters and objective is again likely to be highly non-smooth,
which could pose diﬃculties for stochastic search optimization. It should be noted, however, that the
latter proposition should hold true for the experiment without ﬁnite release times (E1), but in this
case an optimality gap is not observed. One could therefore consider devising algorithms to: a) enable
use of function approximators more suited to expressing non-smooth relationships between the state
and optimal control e.g. decision trees; b) automate the identiﬁcation of the appropriate parametric
model structure; or c) improve on the existing hybrid stochastic search algorithm used in this work.

Despite the presence of the optimality gap in experiment E2, in both experiments, the framework
is able to eﬃciently handle the constraints imposed on the plant. In the following section, we consider
the addition of process uncertainties via investigation of experiments E3-8.

5.3.2. Optimization of the uncertain plant (reactive production scheduling)

From Table 4, it is clear that generally the MILP outperforms the RL formulation proposed. On
average, the performance gap is 1.75%. This can be decomposed, such that if we consider those
experiments with ﬁnite release time present, the average gap is 2.73%; for those experiments that did
not include ﬁnite release times, the average gap is just 0.77%. This further supports the argument
that the optimality gap is dependent upon the underlying complexity of the optimal control mapping
(i.e. the optimal scheduling policy becomes more diﬃcult to identify when ﬁnite release times are
included).

Table 4: Table of results for the proposed method from investigation of experimental conditions detailed by Table 1 for
Problem Instance 2. The policies synthesised were optimized under the objective provided by Eq. 5.

Reference Method

µZ

σZ

FLB Method

µZ

σZ

FLB

E1
E2
E3
E4
E5
E6
E7
E8

Proposed

-107.0
-143.0
-113.3
-150.5
-119.4
-167.1
-127.2
-171.6

0.0
0.0
8.5
10.2
16.4
21.9
19.6
21.7

1.0
1.0
0.98
0.97
1.0
0.98
0.95
0.99

MILP

-107.0
-137.0
-108.4
-142.7
-123.4
-166.6
-125.9
-167.5

0.0
0.0
7.15
11.43
17.1
20.6
19.5
25.1

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

It is also of note that the probability of constraint satisfaction decreases in this case study relative to
problem instance 1, across experiments E3-8 (see Table 2). However, these changes are generally small,
with the largest change equivalent to a 4% decrease. This is likely due to realizations of uncertainty
unseen in the training having greater impact in validation, given the increased complexity of the
decision making problem. However, as previously stated, this could be overcome by increasing nI in
training, at the cost of added computational complexity.

5.4. Computational time cost in policy identiﬁcation and decision-making

In this section, we are interested in the comparative time intensity of the RL approach proposed
relative to the MILP approach. Having identiﬁed a model of the plant and the associated uncertainties,

20

we are interested in how long it takes to identify: a) a control policy, π, (this is conducted oﬄine) and
b) a control decision, ut, conditional to observation of the plant state, xt, at discrete time index, t
(this is conducted online). The code was parallelized to minimize the time complexity of computation.
In practice, the computational metrics reported in the following could be bettered considerably, with
respect to policy identiﬁcation, if the appropriate hardware were available.

In the case of a), in RL we consider the amount of time to conduct stochastic search optimization
associated with the proposed method when the sample size nI = 50; whereas, the MILP incurs no time
cost in identiﬁcation of the policy, given the optimization formulation is essentially the policy, π. In b),
we consider the time taken: for RL to conduct a forward pass of the neural network parameterization
of the control policy; and, to solve an MILP problem. Table 5 details the respective results for problem
instance 1 and 2.

Table 5: Normalized times for a) oﬄine identiﬁcation of a control policy, π, and b) identiﬁcation of online scheduling
decisions for problem instances 1 and 2.

Object of identiﬁcation

Problem instance MILP

RL )

Time for identiﬁcation

Control decision (online)

Control policy (oﬄine)

1
2

1
2

1
1

0
0

0.0067
0.002

1
1

From Table 5, it is clear that the RL method demands a much greater investment in the compu-
tational synthesis of a scheduling policy, π, oﬄine. Generally, policy synthesis required between 1-2
hours for problem instance 1 and 2 under the sample size, nI , detailed by this work. The diﬀerences
in time for oﬄine policy identiﬁcation between problems 1 and 2 arises due to an increased number
of orders (leading to a longer makespan in problem instance 2). However, the work required online
to update the control schedule, given a realization of plant uncertainty, is far lower in the case of
RL than in the MILP formulation. For problem instance 1, the MILP is characterized by 304 binary
decision variables and 25 continuous decision variables. In this case, inference of a control decision is
150 times cheaper via the RL method proposed. In problem instance 2, the MILP formulation consists
of 990 binary decision variables and 46 continuous decision variables. For a problem of this size, the
RL approach is 500 times cheaper than the MILP. The online computational beneﬁts of RL are likely
to be considerable for larger case studies, which is of signiﬁcant practical beneﬁt to the scheduling
operation.

5.5. The eﬀects of inaccurate estimation of plant uncertainties

To further consider the practicalities of RL, in this section, we consider the eﬀects of inaccurately
estimating the real plant uncertainties for the policy synthesised via experiment E8, problem instance
1. Speciﬁcally, we assume that in the oﬄine model of the plant dynamics and plant uncertainties
are as previous. While in the following experiments M1-8, we assume that actual plant uncertainties
are diﬀerent from that of the oﬄine model. The processing time uncertainty has the same form (a
discrete uniform probability distribution), but with the upper and lower bounds misspeciﬁed by an
additive constant, kpt ∈ Z+. Similarly, the rate of the Poisson distribution descriptive of the due
date uncertainty is misspeciﬁed. Here, however, the misspeciﬁcation is treated probabilistically, such

21

that the rate is either unperturbed or misspeciﬁed by a constant, kdd ∈ Z+ (this is either added or
subtracted), with uniform probability. See Appendix D for further details.

In the respective experiments M1-8, kpt and kdd are deﬁned to investigate the eﬀects of diﬀerent
degrees of plant-model mismatch. In all cases, the policy identiﬁed from experiment E8 in problem
instance 1 was rolled out in a plant where the true uncertainties are misspeciﬁed for 500 MC simulations.
The experiments and their associated results are detailed by Table 6.

Table 6: Table of experimental conditions investigated. In each experiment, we take the trained policy from experimental
condition E8, problem instance 1 and evaluate its performance in a plant deﬁned by diﬀerent uncertainties. The degree
of misspeciﬁcation increases with experiment number.

Reference

Processing time, kpt Due date, kdd

µZ

Model (E8)
M1
M2
M3
M4
M5
M6
M7
M8

0
0
0
1
1
1
2
2
2

0
1
2
0
1
2
0
1
2

-75.3
-74.8
-73.8
-76.0
-77.3
-78.0
-83.9
-81.5
-83.4

σZ

11.5
12.3
11.8
13.3
13.2
14.8
17.6
17.5
18.7

¯µφ
β

-101.1
-101.3
-99.9
-106.8
-104.4
-105.6
-121.7
-116.2
-118.5

FLB

0.99
0.98
0.99
0.95
0.95
0.95
0.78
0.82
0.82

From Table 6, it is clear that misspeciﬁcation of plant uncertainties generally impacts the per-
formance of the policy identiﬁed. The misspeciﬁcations identiﬁed are relatively mild in the case of
due date uncertainty, however, they are more pronounced in the case of processing times. This is
reﬂected in the results of the experiments, where little variation is observed in the policy performance
when kpt < 2. For example, the maximum deviation in terms of expected performance, µz, when this
condition is not imposed, is observed in experiment M5 where the corresponding decrease in µz from
the performance observed in the oﬄine model (experiment E8) is just 3.6%. Similarly, the probability
of constraint satisfaction, FLB, remains high (slightly decreases to 0.95). Notably in experiments M1
and M2, the policy performance increases. As the estimated values for due dates are maintained in
the state (fed to the policy), the misspeciﬁcation is unlikely to induce a signiﬁcantly diﬀerent state
distribution than the model and hence the result should be interpreted with caution (i.e. it likely arises
from the statistical inaccuracies of sampling).

The eﬀects on policy performance become notable when the processing time misspeciﬁcation, kpt =
2. In all experiments M6-8, where this condition is imposed, the decrease in expected performance,
µZ, is in the region of 10%. Additionally, the magnitude of the CVaR for β = 0.2 (i.e. the expected
value of the worst 20% of policy performances) decreases by 15-20%, and this is mirrored further by a
considerable decrease in the probability of constraint satisfaction, such that FLB ≈ 0.8.

From the analysis above, it is clear that accurate estimation of the true plant uncertainties is
important if RL is to be applied to a real plant. However, it does appear that RL is likely to be robust
to small amounts of error, which may be unavoidable given the limitations of ﬁnite plant data.

6. Conclusions

Overall, in this work, we have presented an eﬃcient RL-based methodology to address production
scheduling problems, including typical constraints imposed derived from standard operating proce-

22

dures and propositional logic. The methodology is demonstrated on a classical single stage, parallel
batch scheduling production problem, which is modiﬁed to enable comparison between discrete-time
and continuous-time formulations as well as to consider the eﬀects of uncertainty. The RL methodol-
ogy is benchmarked against a continuous-time MILP formulation. The results demonstrate that the
RL methodology is able to handle constraints and plant uncertainties eﬀectively. Speciﬁcally, on a
small problem instance RL shows particular promise with performance improvements over the MILP
approach by up to 4.7%. In the larger problem instances, the RL approach had practically the same
performance as MILP. Further, the RL methodology is able to optimize for various risk-aware for-
mulations and we demonstrate the approach is able to improve the 20% worst case performances by
2.3% on average. The RL is orders of magnitude (150-500 times) cheaper to evaluate online than the
MILP approach for the problem sizes considered. This will become even greater when larger problem
sizes are considered or if compared to discrete-time scheduling formulations. Finally, the RL approach
demonstrated reasonable robustness to misspeciﬁcation of plant uncertainties, showing promise for ap-
plication to real plants. In future work, we will consider the translation of this methodology to larger
problem instances, multi-agent problems, and integration with other PSE decision-making functions,
such as maintenance and process control.

Acknowledgements

Tha authors would like to thank Panagiotis Petsagkourakis, Ilya Orson Sandoval and Zhengang

Zhong for insightful discussions and feedback throughout the project.

References

[1] M. Sarkis, A. Bernardi, N. Shah, and M. M. Papathanasiou, “Decision support tools for next-
generation vaccines and advanced therapy medicinal products: present and future,” Current Opin-
ion in Chemical Engineering, vol. 32, p. 100689, 2021.

[2] Z. Kis, C. Kontoravdi, R. Shattock, and N. Shah, “Resources, production scales and time required
for producing rna vaccines for the global pandemic demand,” Vaccines, vol. 9, no. 1, p. 3, 2020.
[3] I. Harjunkoski, C. T. Maravelias, P. Bongers, P. M. Castro, S. Engell, I. E. Grossmann, J. Hooker,
C. M´endez, G. Sand, and J. Wassick, “Scope for industrial applications of production scheduling
models and solution methods,” Computers & Chemical Engineering, vol. 62, pp. 161–193, 2014.
[4] C. T. Maravelias, “General framework and modeling approach classiﬁcation for chemical produc-

tion scheduling,” AIChE Journal, vol. 58, no. 6, pp. 1812–1828, 2012.

[5] C. A. Floudas and X. Lin, “Continuous-time versus discrete-time approaches for scheduling of
chemical processes: a review,” Computers & Chemical Engineering, vol. 28, no. 11, pp. 2109–
2129, 2004.

[6] J. C. Spall, Introduction to stochastic search and optimization: estimation, simulation, and control,

vol. 65. John Wiley & Sons, 2005.

[7] D. Gupta and C. T. Maravelias, “A general state-space formulation for online scheduling,” Pro-

cesses, vol. 5, no. 4, p. 69, 2017.

[8] K. Subramanian, C. T. Maravelias, and J. B. Rawlings, “A state-space model for chemical pro-

duction scheduling,” Computers & chemical engineering, vol. 47, pp. 97–110, 2012.

[9] C. Li and I. E. Grossmann, “A review of stochastic programming methods for optimization of

process systems under uncertainty,” Frontiers in Chemical Engineering, p. 34, 2021.

23

[10] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski, Robust optimization. Princeton university press,

2009.

[11] Z. Li and M. G. Ierapetritou, “Robust optimization for process scheduling under uncertainty,”

Industrial & Engineering Chemistry Research, vol. 47, no. 12, pp. 4148–4157, 2008.

[12] Z. Li and M. Ierapetritou, “Process scheduling under uncertainty: Review and challenges,” Com-

puters & Chemical Engineering, vol. 32, no. 4-5, pp. 715–727, 2008.

[13] B. Beykal, S. Avraamidou, and E. N. Pistikopoulos, “Data-driven optimization of mixed-integer
bi-level multi-follower integrated planning and scheduling problems under demand uncertainty,”
Computers & Chemical Engineering, vol. 156, p. 107551, 2022.

[14] Y. Tian and E. N. Pistikopoulos, “Synthesis of operable process intensiﬁcation systems—steady-
state design with safety and operability considerations,” Industrial & Engineering Chemistry
Research, vol. 58, no. 15, pp. 6049–6068, 2018.

[15] F. B. Oyebolu, R. Allmendinger, S. S. Farid, and J. Branke, “Dynamic scheduling of multi-product
continuous biopharmaceutical facilities: A hyper-heuristic framework,” Computers & Chemical
Engineering, vol. 125, pp. 71–88, 2019.

[16] Y. Fu, H. Wang, G. Tian, Z. Li, and H. Hu, “Two-agent stochastic ﬂow shop deteriorating schedul-
ing via a hybrid multi-objective evolutionary algorithm,” Journal of Intelligent Manufacturing,
vol. 30, no. 5, pp. 2257–2272, 2019.

[17] L. S. Dias, R. C. Pattison, C. Tsay, M. Baldea, and M. G. Ierapetritou, “A simulation-based op-
timization framework for integrating scheduling and model predictive control, and its application
to air separation units,” Computers & Chemical Engineering, vol. 113, pp. 139–151, 2018.
[18] F. Santos, R. Fukasawa, and L. Ricardez-Sandoval, “An integrated machine scheduling and per-
sonnel allocation problem for large-scale industrial facilities using a rolling horizon framework,”
Optimization and Engineering, vol. 22, no. 4, pp. 2603–2626, 2021.

[19] V. M. Charitopoulos, L. G. Papageorgiou, and V. Dua, “Closed-loop integration of planning,
scheduling and multi-parametric nonlinear control,” Computers & Chemical Engineering, vol. 122,
pp. 172–192, 2019.

[20] C. D. Hubbs, C. Li, N. V. Sahinidis, I. E. Grossmann, and J. M. Wassick, “A deep reinforce-
ment learning approach for chemical production scheduling,” Computers & Chemical Engineering,
vol. 141, p. 106982, 2020.

[21] C. Caputo and M.-A. Cardin, “Analyzing real options and ﬂexibility in engineering systems design
using decision rules and deep reinforcement learning,” Journal of Mechanical Design, vol. 144,
no. 2, 2022.

[22] N. P. Lawrence, M. G. Forbes, P. D. Loewen, D. G. McClement, J. U. Backstr¨om, and R. B.
Gopaluni, “Deep reinforcement learning with shallow controllers: An experimental application to
pid tuning,” Control Engineering Practice, vol. 121, p. 105046, 2022.

[23] J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner,
A. Abdolmaleki, D. de las Casas, C. Donner, L. Fritz, C. Galperti, A. Huber, J. Keeling, M. Tsim-
poukelli, J. Kay, A. Merle, J.-M. Moret, S. Noury, F. Pesamosca, D. Pfau, O. Sauter, C. Som-
mariva, S. Coda, B. Duval, A. Fasoli, P. Kohli, K. Kavukcuoglu, D. Hassabis, and M. Riedmiller,
“Magnetic control of tokamak plasmas through deep reinforcement learning,” Nature, vol. 602,
pp. 414–419, Feb. 2022.

[24] H. Yoo, H. E. Byun, D. Han, and J. H. Lee, “Reinforcement learning for batch process control:

Review and perspectives,” Annual Reviews in Control, vol. 52, pp. 108–119, 2021.

[25] B. Waschneck, A. Reichstaller, L. Belzner, T. Altenm¨uller, T. Bauernhansl, A. Knapp, and
A. Kyek, “Optimization of global production scheduling with deep reinforcement learning,” Pro-

24

cedia Cirp, vol. 72, pp. 1264–1269, 2018.

[26] J. A. Palombarini, J. C. Barsce, and E. C. Mart´ınez, “Generating rescheduling knowledge using

reinforcement learning in a cognitive architecture,” 2018.

[27] P. Petsagkourakis, I. O. Sandoval, E. Bradford, F. Galvanin, D. Zhang, and E. A. del Rio-Chanona,
“Chance constrained policy optimization for process control and optimization,” Journal of Process
Control, vol. 111, pp. 35–45, 2022.

[28] J. Achiam, D. Held, A. Tamar, and P. Abbeel, “Constrained policy optimization,” in International

conference on machine learning, pp. 22–31, PMLR, 2017.

[29] Y. Zhang, Q. Vuong, and K. Ross, “First order constrained optimization in policy space,” Advances

in Neural Information Processing Systems, vol. 33, pp. 15338–15349, 2020.

[30] M. Mowbray, P. Petsagkourakis, E. del Rio-Chanona, and D. Zhang, “Safe chance constrained
reinforcement learning for batch process control,” Computers & chemical engineering, vol. 157,
pp. 107630–, 2022.

[31] C. Waubert de Puiseau, R. Meyes, and T. Meisen, “On reliability of reinforcement learning based
production scheduling systems: a comparative survey,” Journal of Intelligent Manufacturing,
pp. 1–17, 2022.

[32] M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Reinforcement Learning. MIT

Press, 2022. http://www.distributional-rl.org.

[33] Y. C. Tang, J. Zhang, and R. Salakhutdinov, “Worst cases policy gradients,” 2019.
[34] R. Rockafellar and S. Uryasev, “Conditional value-at-risk for general deviation measure,” J. Bank-

ing Financ, vol. 26, pp. 1443–1471, 2002.

[35] A. Najjarbashi and G. J. Lim, “A variability reduction method for the operating room scheduling
problem under uncertainty using cvar,” Operations Research for Health Care, vol. 20, pp. 25–32,
2019.

[36] Z. Chang, S. Song, Y. Zhang, J.-Y. Ding, R. Zhang, and R. Chiong, “Distributionally robust single
machine scheduling with risk aversion,” European Journal of Operational Research, vol. 256, no. 1,
pp. 261–274, 2017.

[37] M. Riedmiller, J. Peters, and S. Schaal, “Evaluation of policy gradient methods and variants
on the cart-pole benchmark,” in 2007 IEEE International Symposium on Approximate Dynamic
Programming and Reinforcement Learning, pp. 254–261, IEEE, 2007.
[38] C. Nota and P. S. Thomas, “Is the policy gradient a gradient?,” 2020.
[39] J. Cerd´a, G. P. Henning, and I. E. Grossmann, “A mixed-integer linear programming model for
short-term scheduling of single-stage multiproduct batch plants with parallel lines,” Industrial &
Engineering Chemistry Research, vol. 36, no. 5, pp. 1695–1707, 1997.

[40] C. D. Hubbs, H. D. Perez, O. Sarwar, N. V. Sahinidis, I. E. Grossmann, and J. M. Wassick,

“Or-gym: A reinforcement learning library for operations research problems,” 2020.

[41] E. Pan, P. Petsagkourakis, M. Mowbray, D. Zhang, and E. A. del Rio-Chanona, “Constrained
model-free reinforcement learning for process optimization,” Computers & Chemical Engineering,
vol. 154, p. 107462, 2021.

[42] A. Kanervisto, C. Scheller, and V. Hautam¨aki, “Action space shaping in deep reinforcement

learning,” in 2020 IEEE Conference on Games (CoG), pp. 479–486, IEEE, 2020.

[43] J. Kennedy and R. Eberhart, “Particle swarm optimization,” in Proceedings of ICNN’95-

international conference on neural networks, vol. 4, pp. 1942–1948, IEEE, 1995.

[44] S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, “Optimization by simulated annealing,” science,

vol. 220, no. 4598, pp. 671–680, 1983.

[45] J.-B. Park, K.-S. Lee, J.-R. Shin, and K. Y. Lee, “A particle swarm optimization for economic

25

dispatch with nonsmooth cost functions,” IEEE Transactions on Power systems, vol. 20, no. 1,
pp. 34–42, 2005.

[46] A. Merchant, L. Metz, S. S. Schoenholz, and E. D. Cubuk, “Learn2hop: Learned optimization
on rough landscapes,” in Proceedings of the 38th International Conference on Machine Learning
(M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 7643–
7653, PMLR, 18–24 Jul 2021.

[47] B. Amos, “Tutorial on amortized optimization for learning to optimize over continuous domains,”

2022.

[48] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective on reinforcement

learning,” in International Conference on Machine Learning, pp. 449–458, PMLR, 2017.

[49] R. T. Rockafellar, S. Uryasev, et al., “Optimization of conditional value-at-risk,” Journal of risk,

vol. 2, pp. 21–42, 2000.

[50] S. Uryasev, “Conditional value-at-risk: Optimization algorithms and applications,” in Proceed-
ings of the IEEE/IAFE/INFORMS 2000 Conference on Computational Intelligence for Financial
Engineering (CIFEr)(Cat. No. 00TH8520), pp. 49–57, IEEE, 2000.

[51] L. J. Hong and G. Liu, “Simulating sensitivities of conditional value at risk,” Management Science,

vol. 55, no. 2, pp. 281–293, 2009.

[52] K. P. Murphy, Probabilistic Machine Learning: Advanced Topics. MIT Press, 2023.
[53] R. R. Wilcox, Applying contemporary statistical techniques. Elsevier, 2003.
[54] S. C. Sarin, H. D. Sherali, and L. Liao, “Minimizing conditional-value-at-risk for stochastic

scheduling problems,” Journal of Scheduling, vol. 17, no. 1, pp. 5–15, 2014.

[55] M. Imaizumi and K. Fukumizu, “Deep neural networks learn non-smooth functions eﬀectively,”

2018.

[56] N. Heess, J. J. Hunt, T. P. Lillicrap, and D. Silver, “Memory-based control with recurrent neural

networks,” arXiv preprint arXiv:1512.04455, 2015.

[57] C. J. Clopper and E. S. Pearson, “The use of conﬁdence or ﬁducial limits illustrated in the case

of the binomial,” Biometrika, vol. 26, no. 4, pp. 404–413, 1934.

Appendix A. Particle swarm and simulated annealing (PSO-SA) hybrid algorithm

The general intuition behind stochastic search is provided by Algorithm 2. Most stochastic search
algorithms adhere to the description provided, which is summarised as follows. In step 1 a population
of parameters for a policy (network) is instantiated by a space ﬁlling procedure of choice. Then, in
step 2, the following steps are performed iteratively: a) a population of neural policies are constructed
from the population of parameters; bi-iv ) the population is evaluated via Monte Carlo according to
the estimator deﬁned; bv ) the current best policy is tracked and stored in memory (if a criterion
is satisﬁed); and, c) the population parameters are then perturbed according to a stochastic search
optimization algorithm. After a number of optimization iterations, the best policy identiﬁed is output.
The number of optimization iterations is deﬁned according to the available computational budget or
according to a threshold on the rate of improvement.

In this work, we use a hybrid particle swarm optimization - simulated annealing stochastic search
optimization algorithm with a search space reduction strategy [45] in order to identify the optimal
scheduling policy parameters, θ∗ ∈ Rnθ . Hybridisation of particle swarm optimization and simulated
annealing enables us to balance exploitation and exploration of the parameter space. The search space
reduction strategy was found to improve performance, especially in the harder experiments conducted
i.e. for experiment E3-8.

26

Appendix A.1. Particle Swarm Optimization

Particle swarm optimization is a stochastic search algorithm initially proposed in [43], which takes
inspiration from the behaviour of animals such as ﬁsh and birds in a shoal or murmuration, respectively.
The population is deﬁned by a number of individuals, P . Each individual (candidate will be as a
i ∈ Rnθ , and a position,
synonymous term) within the population is ﬁrst initialised with a velocity, v0
θ0
i , ∀i ∈ {1, . . . , P }. All individual’s positions are subject to upper and lower bounds, such that
θi = [θLB, θU B]nθ . Similarly, all velocities are subject to upper bounds, such that vi = [−∞, vmax]nθ ,
where vmax ∈ R is typically deﬁned as:

vmax = c3(θU B − θLB)
where c3 = [0, 1]. Each candidate is then rated with respect to an objective function, FSA : Rnθ → R
i ∈ Rnθ ; the locally
that one would like to minimize. A note of: the candidate’s current best position, b∗
i ∈ Rnθ , within a neighbourhood of nh individuals; and, of the global best, θ∗, is then
best position, g∗
made. The optimization procedure may then proceed by updating the position of each individual in
the population iteratively via the following equation:

vk+1
i = ωvk
θk+1
i = θk

i + c1r1
i + vk+1
i

(cid:0)b∗

i − θk

i ) + c2r2(g∗

i − θk
i )

(A.1)

where ω ∈ R+, c1 ∈ R+ and c2 ∈ R+ may be interpreted to represent a weighting for inertia, individual
acceleration and global acceleration, respectively. The individual and local best components of the total
acceleration are adjusted stochastically by the deﬁnition of r1 ∼ U (0, 1) and r2 ∼ U (0, 1). From Eq.
A.1, the update of the individual’s position makes use of information both from the trajectory of the
individual but the trajectories collected across the swarm.

Appendix A.2. Simulated Annealing

Simulated annealing is also a stochastic search optimization algorithm, which is superﬁcially related
to structural transitions of physical systems under temperature change. The algorithm updates the
position of an individual, θk
i , probabilistically based on the improvement provided in the proposed
update with respect to the objective, FSA, and the current temperature, T ∈ R+, of the algorithm.
Speciﬁcally, an updated candidate, ¯θk

i , is proposed and accepted or rejected as follows:

i + wk
(A.2a)
i
where wk
i = [−1, 1]nθ is described according to a distribution of choice on the space [−1, 1]nθ . Eq.
A.2a details the perturbation of a candidate’s position. The evaluation and acceptance or rejection of
the proposed position follows:

¯θk
i = θk

i ) − FSA(¯θk
i )
if ∆E > 0 or ¯z ≥ exp( ∆E
T )

∆E = FSA(θk
(cid:40)¯θk
i ,
θk
i , otherwise

θk+1
i =

(A.2b)

(A.2c)

where ¯z ∼ U (0, 1). From Eq. A.2c, it is seen that if the proposed candidate does not improve
with respect to the objective function, then it is accepted probabilistically to balance exploration and
exploitation.

Generally, schemes are implemented to update the temperature, T , given that it controls the
acceptance rate. The lower the value of T , the higher the probability of acceptance. The larger the
value of T the lower the probability of acceptance. Due to the nature of the hybridisation used in this
work, large constant values of T were used.

27

Appendix A.3. Search Space Reduction

Search space reduction strategies are known to improve the performance of general stochastic search
In this work, preliminary experiments demonstrated that the use of a space reduction
algorithms.
strategy improved performance of θ∗ as identiﬁed by the algorithm. The strategy follows the reasoning
of the work provided in [45]:

LB + α(θk
U B − α(θk
where α = [0, 1] represents a learning rate. Algorithm 3 details the hybridization of the constituent
components discussed in this section.

θk+1
LB = θk
θk+1
U B = θk

LB − θ∗)
U B − θ∗)

(A.3)

28

Algorithm 3: A hybrid particle swarm optimization - simulated annealing algorithm with a
search space reduction strategy

U B, and lower bounds, θ0

Input: Initial upper, θ0
LB, on the parameter search space; Population
size, P ; Maximum velocity, vmax = [vmax,1, . . . , vmax,nθ ]; Search space reduction step size,
α; Particle swarm optimization algorithm, gP SO(·); Simulated annealing algorithm, gSA(·);
Temperature, T ; Cooling schedule, gT (·); Search space reduction rule, gSSR; Objective function,
FSA(·); Memory buﬀer, Binf o; Maximum number of search iterations, K; Logic condition to
trigger simulated annealing search optimization algorithm, logicSA; Logic condition to trigger
search space reduction, logicSSR;

i,nθ

i,1, . . . , θ1

i = [θ1
Initialize a velocity for each individual

1. Generate initial population of individual parameters, Θ1 = {θ1
P }. Each parameter set-
ting θ1
] ∈ Θ1 is generated such that, θi,j ∼ U (θLB,j, θU B,j), ∀j ∈ {1, . . . , nθ}.;
2.
in the population via the following strategy:
i = (2a − 1)(θU B − θLB), where a = [a1, . . . , anθ ], and ai ∼ U (0, 1). Deﬁne V1 = {vi, ∀i ∈
v0
{1, . . . , P }};
3. for k = 1, . . . , K do

1, . . . , θ1

a. Evaluate J k

i = FSA(θi), ∀θi ∈ Θk. Append Ji to Binf o, ∀i;

b. Determine the current local best, g∗
for θi ∈ Θk. Deﬁne Gk = {g∗

i , ∀i ∈ {1, . . . , P }}.;

i , for a neighbourhood composed of nh individuals,

c. According to J j
is the best parameter
setting observed by the individual. Store this position as the current individual best, b∗
i
and store in Bk = {b∗

i ∈ Binf o, ∀j ∈ {1, . . . , k}, determine whether θk
i

i , ∀i ∈ {1, . . . , P }};

i. Reduce search space: θk

U B, θk

LB = gSSR(Gk, θk)

d. if logicSSR then

end

e. if logicSA then

i. Conduct simulated annealing and update individual and neighbourhood best:
Θk, Gk, Bk, Binf o = gSA(Θk, Gk, Bk, Binf o, FSA, T k), via e.g. Eq. A.2;

ii. Update temperature: T k+1 = gT (T k)
end

f. Update population via Particle Swarm: Θk+1, Vk+1 = gP SO(Θk, Gk, Bk) via e.g. Eq.
∈ Vk+1 satisfy constraints provided by θLB, θU B and
A.1. Ensure θk+1
vmax;
end

∈ Θk+1 and vk+1

i

i

4. Determine global best parameters, θ∗, from BK and Binf o;

Output: θ∗

29

Appendix A.4. Policy network structure selection

Generally, it was observed that the performance of π learned via the method proposed was depen-
dent upon proper selection of the neural network structure. It was found that a neural network 3 layer
network with: an input layer of nx = 2N + 2u + 1 nodes, where N is the number of tasks and nu is
the number of units; hidden layer 1 composed of 10 feedforward nodes; hidden layer 2 composed of 4
Elman recurrent (tanh) nodes; hidden layer 3 composed of feedforward 2 nodes; and, an output layer
composed of nu nodes. Hyperbolic tangent activation functions were applied across hidden layer 2, a
sigmoid over hidden layer 3, and a ReLU6 function over the output layer. The investigation of deeper
networks was generally led by research supporting their use for approximation of non-smooth func-
tions, as provided in [55]. The use of recurrency within the network was used to handle the potnetial
partial observability of the problem when uncertainty was present in case study [56].

Appendix B. Deﬁnition of the production scheduling problem

The following text deﬁnes the problem case studies presented in this work. Full code implementation
of the system deﬁned here is available at https://github.com/mawbray/ps-gym and is compatible
with stable RL baseline algorithms (as it is coded via the recipe provided by OpenAI gym custom
classes).

Appendix B.1. Problem deﬁnition

We consider a multi-product plant where the conversion of raw material to product only requires one
processing stage. We assume there is an unlimited amount of raw material, resources, storage and wait
time (of units) available to the scheduling element. Further, we assume that the plant is completely
reactive to the scheduling decisions of the policy, π, although this assumption can be relaxed if decision-
making is formulated within an appropriate framework as shown in [20]. The scheduling element must
decide the sequencing of tasks (which correspond uniquely to client orders) on the equipment (units)
available to the plant. The following operational rules are imposed on the scheduling element:

1. A given unit l has a maximum batch size for a given task i. Each task must be organized
in campaigns (i.e. processed via multiple batches sequentially) and completed once during the
scheduling horizon. All batch sizes are predetermined, but there is uncertainty as to the process-
ing time (this is speciﬁc to task and unit).

2. Further, the task should be processed before the delivery due date of the client, which is assumed
to be an uncertain variable (the due date is approximately known at the start of the scheduling
horizon, but is conﬁrmed with the plant a number of periods before the order is required by the
client).

3. There are constraints on the viable sequencing of tasks within units (i.e. some tasks may not be

processed before or after others in certain units).

4. There is a sequence and unit dependent cleaning period required between operations, during

which no operations should be scheduled in the given unit.
5. Each task may be scheduled in a subset of the available units.
6. Some units are not available from the beginning of the horizon and some tasks may not be
processed for a ﬁxed period from the start of the horizon (i.e. they have a ﬁxed release time).
7. Processing of a task in a unit must terminate before another task can be scheduled in that unit.

30

The objective of operation is to minimize the makespan and the tardiness of task (order) completion.
This means that once all the tasks have been successfully processed according to the operational rules
deﬁned, then the decision making problem can be terminated. As in the original work, we formalize the
notation of a number of problem sets and parameters in Table B.7. We try to keep notation consistent
with that work. It should be noted that in this work, we transcribe the problem as a discrete-time
formulation. The original work [39] utilized a continuous-time formulation. Further discussion is
provided later in the text on this topic. We formalize the notation of a number of problem sets and
parameters in Table B.7.

Table B.7: Table of problem parameters and sets. *D.T.I. is shorthand for discrete time indices.

Sets

Tasks (orders) to be processed
Available units
Available units for task i
The task most recently processed in unit l
Tasks which have been completely processed
Feasible successors of task i in unit l
Feasible successors of task i
The task currently being processed in unit l

Parameters

Number of tasks
Due date of client order (task)
Number of units
Batch size of unit l for task i
Number of batches required to process task i in unit l
Sequence dependent set up time
Release time of tasks in D.T.I.
Release time of units in D.T.I.
Cleaning time required for unit l between processing tasks m and i successively

Miscellaneous Variables

Variable indicating campaign production of task i starts at time t in unit l
Variable indicating unit l is processing task i at time t
Integer variable denoting task i is being processed in unit l at time t
Estimated D.T.I. for unit l to process a batch of task i
Actual D.T.I. for unit l to process batch n of task i
Actual D.T.I. to ﬁnish processing current campaign in unit l at time t
Current inventory of task (client order) i at time t
D.T.I. until due date of task i at time t
The length of a discrete time index

Notation
I = {1, . . . , N } ⊂ Z+
L = {1, . . . , nu} ⊂ Z+
Li ⊆ L
Ml ⊂ Z; |Ml| ≤ 1
Tf ⊂ Z
SUil ⊂ Z+
SUi = ∪l∈LSUil ⊂ Z+
Ol
Notation
N ∈ R+
τi ∈ Z+
nu ∈ Z+
Bil ∈ R+
N Bil ∈ Z+
ˆυilt ∈ Z+
(RTT)i ∈ Z+
(RTU)l ∈ Z+
T CLmil ∈ Z+
Notation
Wilt ∈ Z2
¯Wilt ∈ Z2
wlt ∈ U
¯P T il ∈ Z
P Linl ∈ Z
δlt ∈ Z
Iit ∈ R+
ρit ∈ Z
dt ∈ R+

Appendix B.2. Formulating discrete-time scheduling problems as Markov decision processes

The methodology presented in Section 3 utilizes formulation of the scheduling problem as an MDP,
which is an approximation to solving the ﬁnite-horizon stochastic optimal control problem (as detailed
in Eq. 3). Construction of the problem follows.

31

Firstly, the time horizon is discretized into T = 200 ﬁnite periods of length dt = 0.5 days, where

t ∈ {0, . . . , T } describes the time at a given index in the discrete time horizon.

We hypothesise that the system is made completely observable by the following state representation:

(cid:20)
I1t, . . . , IN t, w1t, . . . , wnut, δ1t, . . . , δnut, ρ1t, . . . , ρN t, t

(cid:21)T

xt =

∈ R2N +2nu+1

(B.1)

where Iit ∀i ∈ I quantiﬁes the current inventory of client orders in the plant; the tasks processed within
units in the plant over the previous time interval, wlt ∀l ∈ L; the discrete time indices remaining until
completion of the task campaigns being processed in all units, δlt ∀l ∈ L; the discrete time indices
remaining until orders (tasks) are due, ρit ∀i ∈ I; and, the current discrete time index, t.

Similarly, we deﬁne the control space, u = [u1, . . . , unu ]T ∈ U as the set of integer decisions (tasks)
that could be scheduled in the available units. Hence, one can deﬁne U = (cid:83)nu
+ , where
l=1
Ul = I ∪ {N + 1}, and N + 1 is an integer value, which represents the decision to idle the unit for one
time period.

Ul ⊂ Znu

A sparse reward function is deﬁned as follows:

(cid:40)

R =

dT xt+1,
0,

if t = T − 1 or Tf = I
otherwise

(B.2)

where d ∈ Znx denotes a vector speciﬁed to penalise order lateness and makespan; I ⊆ Z denotes the
set of tasks (or client orders); and, Tf ⊆ Z denotes the set of tasks, which have been completed. If
Tf = I is satisﬁed, the decision-making problem is terminated. Deﬁnition of d follows:

d = (cid:2)01,N , 01,nu , 01,nu , 11,N , −1(cid:3)T

∈ Z2N +2nu+1

(B.3)

where 01,j represents a row vector of zeros of dimension j; and, 11,j represents a row vector of ones
of dimension j. How the deﬁnition provided by Eq. B.2 enforces operational objectives is clariﬁed by
full deﬁnition of the dynamics, as presented subsequently.

The inventory balance is deﬁned to highlight that the underlying state space model is both non-

smooth and subject to uncertain variables:

Iit+1 = Iit +

N Bil(cid:88)

(cid:88)

n=1

l∈Li

BilWilt−ˆυilt−(cid:80)n

ˆn=1 P Li ˆnl ∀i ∈ I

(B.4)

where Bil is the maximum batch size (kg) of unit l for task i ; N Bil is the integer number of batches
required to satisfy the client’s order size in unit l ; P Linl is a random variable that indicates the number
of discrete time indices required to produce batch n of task i in unit l and represents a realization of
an uncertain variable; Wilt ∈ Z2 is a binary variable, indicating a production campaign for task i was
ﬁrst scheduled in unit l at time t; ˆυilt deﬁnes the current sequence dependent unit set up time required
to process order i in unit l at time t. The sequence dependent unit set up time is deﬁned as follows:

ˆυilt =




(cid:80)

m∈Ml

max(max([TCLmil + tml − t]+, [(RTT)i − t]+), [(RTU)l − t]+(cid:1),

if |Ml| = 1

max([(RTT)i − t]+, [(RTU)l − t]+(cid:1),



otherwise

(B.5)

(B.6)

32

where Ml ⊂ Z+, deﬁnes a set denoting the task most recently processed in unit l, which ﬁnished at
time, tml ≤ t, such that the cardinality of the set |Ml| ∈ Z2; TCLmil ∈ R+ deﬁnes the cleaning times
between successive tasks m and i ; (RTT)i ∈ Z+ deﬁnes the release time (in number of discrete time
indices) a task i cannot be processed for at the start of the horizon; (RTU)l ∈ Z+ deﬁnes the release
time of unit l.

It is apparent that in order to handle the requirements for cleaning time between processing of
successive tasks m and i in unit l, we directly incorporate the cleaning time required and all other
mandated setup time (collectively denoted ˆυilt) into the total processing time of task i in unit l when
ﬁrst scheduled at time t. The total processing time for a task i in unit l at time ˆtl is then equivalent
to (cid:80)N Bil

.

n=1 P Linl + ˆυilˆtl

Further, if a diﬀerent task ˆi is scheduled by the policy at time, ¯t, in unit l, before the end of the
((cid:80)N Bil
) + ˆtl > ¯t > ˆtl), then the task

n=1 P Linl + ˆυilˆtl

production campaign for task i (i.e. where Wilˆtl
= 0 is redeﬁned.
indicator Wilˆtl

Similarly, if a production campaign of task i successfully terminates at time t in unit l, then:
= 0; Ml = {i}; tml = t; and, the task is added to a list of completed orders, Tf = Tf ∪ {i}. In

Wilˆtl
both cases redeﬁnition of the binary variable is retroactive to the initial scheduling decision.

Next, we update a representation of ’lifting variables’ [7] that indicate current unit operations:

wlt+1 = ult ∀l ∈ L

(B.7)

where ult ∈ Z+ is the scheduled decision at the previous time index. This is deemed a necessary
part of the state representation, given that it provides information regarding the operational status of
the available equipment at the current time index. For example: if ult = N + 1, then the unit is idle;
if 0 ≤ ult < N + 1, then it is not. A control must be predicted at each discrete time index. Typically,
the length of a production campaign will be greater than the duration of discrete time interval, dt. To
handle this if ult = ult−1 it is deemed that the scheduling element is indicating to continue with the
current operation in unit l.

To determine the amount of processing time left for a given campaign in a unit l, we deﬁne:

Alt =

(cid:88)

Wilt

(cid:18) N Bil(cid:88)

P Linl + ˆυilt − δlt

+ δlt

(cid:19)

i∈IL

n=1
δlt+1 = Alt − 1 ∀l ∈ L

(B.8)

The formulation assumes that we know the processing time required for batch n of task i in unit l,
P Li,n,l, ahead of its realization. In reality it is treated as a random variable, which is only observed
when a given batch in a production campaign terminates. This is discussed further in Section Ap-
pendix B.3. The ﬁnal equation comprising the system updates the number of discrete time indices
until the order i is to be completed for delivery to the client, ρit:

ρit+1 =

(cid:40)

if i ∈ Tf
[ρit]−,
ρit − 1, otherwise

∀i ∈ I

(B.9)

where [y]− = min(0, y). The use of the logic condition enforced in Eq. B.9 is essentially redundant
from the point of view of providing information about completed tasks, given that this is equivalently
expressed by the inventory representation in the state vector. However, it is particularly useful in the
allocation of rewards as it provides means to easily allocate penalty for tardy completion of tasks as
expressed in Eq. B.2.

33

Appendix B.3. A forecasting framework for handling future plant uncertainty

In this study, the processing time of a batch, P Linl, and the due date of the client orders, τi, are
described by uniform and poisson distributions, respectively. This is detailed bu Eq. B.10a and B.10b:

P Linl ∼ U (max(1, ¯P T il − c), ¯P T il + c) ∀n ∈ {1, . . . , N Bil},

(B.10a)
where c ∈ Z+ deﬁnes the variance of the distribution (c = 1 in this work); and, ¯P T il is the expected
processing time for a batch of task i in unit l. The due date uncertainty is described as follows:

∀l ∈ Li,

∀i ∈ I

τi ∼ P (¯τi) ∀i ∈ I

(B.10b)

where ¯τi is the expected due date. In practice, we do not observe the realizations of these variables in
advance, and hence maintain estimates according to their expected values within the state represen-
tation (i.e. Eq. B.1). For example, in the case of the due date, the client is required to conﬁrm the
delivery date a number of days before the order - at which point τi is observed and updated within
the state.

Appendix B.4. Deﬁning the initial system state

Eq. B.4 - B.9 represent the discrete-time plant dynamics (i.e. Eq. 1) that we are concerned with.
To initiate the system, it is necessary to deﬁne an initial state, x0, that represents the plant at the
start of the scheduling horizon, t = 0. This is described by the following set of expressions:

Ii0 = 0 ∀i ∈ I

(B.11a)

which assumes that at the start of the horizon, the plant holds no inventory of the products one desires
to produce;

wl0 = N + 1 ∀l ∈ L

(B.11b)

denotes that at the start of the horizon all units are idled. Even if this is not the case (i.e. the unit
is unavailable for other reasons, such as production of another task which it began before the start
of the scheduling horizon), the allocation appears satisfactory according to experiments that follow in
Section 5;

δl0 = (RTU)l ∀l ∈ L

(B.11c)

Eq. B.11c assumes that the unit is required to be idled for at least a period equivalent to its release
time. At the start of the horizon, the number of discrete time indices until a task is due to be delivered
to a client is equivalent to the task due date:

ρi0 = ¯τi ∀i ∈ I

(B.11d)

It should also be noted that at t = 0, Ml = {} ∀l and Tf = {}.

Having provided detail of the state, controls space and plant dynamics, we now turn our attention

to identiﬁcation of the feasible controls.

34

Appendix B.5. Deﬁning the set of feasible controls

We seek to identify the set of controls ˆUt ⊂ U ⊂ Znu , which innately satisfy the constraints imposed

on the scheduling element.

Here, we deﬁne a number of sets that enable us to identify ¯Ut:
1. A given unit l can process a subset, Il ⊆ I of the tasks that require processing.
2. If unit l has just processed task i then there exists a set of tasks which may be processed

subsequently, SUil.

3. There exists a set, Ol = {i}, which describes the task currently being processed in unit l. At

4. As previously mentioned there exists a set, Tf , descriptive of those tasks, which have already

t = 0, Ol = ∅. If the unit is idled, then Ol = ∅.

been processed successfully. At t = 0, Tf = ∅.

5. Lastly, again, there exists a set, Ml descriptive of the task most recently processed in unit l.

Likewise, at t = 0, Ml = ∅.

It follows then that any given time in processing, one can deﬁne the set of controls that partially satisfy
the imposed operational rules as:

¯Ut =

nu(cid:91)

l=1






(Il ∪ {N + 1})\Tf
((Il ∪ {N + 1}) ∩ SUml)\Tf where, m ∈ Ml
Ol

if Ol = ∅ & Ml = ∅
if Ol = ∅ & |Ml| = 1
if Ol (cid:54)= ∅

(B.12)

Note, however, in this instance we cannot innately satisfy the control set ˆUt, as ˆUt ⊂ ¯Ut. Speciﬁcally,
¯Ut permits the scheduling of the same task i in two diﬀerent units l and l’ at the same time index t.
We propose to handle this through use of a penalty function, which we deﬁne as:

φ = R − κg

(cid:13)[g(x, u)]+(cid:13)
(cid:13)
(cid:13)2

g(x, u) = [g1, . . . , gN ]
(cid:88)
¯Wilt − 1

gi =

l∈Li

(B.13)

where κg = 250 and ¯Wilt ∈ Z2 is a lifting variable that indicates unit l is processing task i at time t,
such that if ult = i, then ¯Wilt = 1 and, otherwise ¯Wilt = 0. Note that technically, under the discrete-
time state space model formulation used in this work (i.e. Eq. B.7), this could also be considered as a
state inequality constraint. However, due to the uncertainty associated with the evolution of the state,
this formalism will be explored in future work. By deploying the methodology proposed in Section 3,
but modifying the rounding policy such that it is deﬁned fr : W → ¯U, we can ensure that the decisions
of the policy function, π satisfy the constraints imposed on the scheduling problem (see Appendix B.1)
as originally proposed in [39] by maximising the penalised return deﬁned under φ.

Appendix C. Deﬁnition of experimental data used in computational experiments

This section details the data used to deﬁne the case studies investigated in Section 5. Please see
Tables C.8, C.9, C.10, C.11, C.12 for information regarding: the feasible processing of tasks in units
and respective maximum batch sizes; nominal processing times; the viable successors of a given task;
the cleaning times required between successive tasks; and information regarding order sizes, due dates
and release times, respectively.

35

Table C.8: Maximum task batch size (kg/batch) for every unit. RTU* denotes the ﬁnite release time of the unit in days.
The length of a discrete time index corresponds to 0.5 days.

Unit, l

Task (order)

1

2

3

4

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15
RTU*

100

140

280

200
250

120

0.0

210
170

390

190
140
155

2.0

120
90
210

270

115
130
3.0

130

290
120

150

145
3.0

Table C.9: Order processing times (days/batch), P Til. The length of a discrete time index corresponds to 0.5 days.

Task (order), Ti

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15

Unit, l

2

3

4

1.0
1.0

1.0

1.5
2.0
1.0

1.5
1.5
2.0

2.0

2.5
1.0

1.0

1.5
2.0

1.5

2.0

1

2.0

1.0

2.5

1.5
2.5

3.0

36

Table C.10: Set of feasible successors.

Task (order)

Feasible Successors

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15

T6, T9, T10, T13
T3, T11, T13
T1, T2, T7, T9, T10, T11, T12
T5, T10, T14, T15
T4, T6, T7, T8, T12, T14
T1, T3, T4, T9, T13, T14, T15
T2, T5, T8, T12
T7, T11, T12, T15
T1, T3, T6, T10, T13
T1, T3, T4, T9, T13, T14, T15
T2, T12, T13
T3, T5, T7, T8, T11, T15
T1, T2, T3, T6, T7, T9, T10, T11, T12
T4, T5, T6, T10, T15
T4, T6, T7, T8, T10, T12, T14

Table C.11: Cleaning times required between pairs of orders (days) in all units. The length of a discrete time index
corresponds to 0.5 days.

Succeeding Task

Task (order) T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T14 T15

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15

1.0

0.5

1.5

2.0
1.0

2.0

1.0

1.5

0.5

1.0

0.5

1.0
0.5

1.0
2.0

0.5

1.0

0.5

0.5

0.5

1.0

0.5

0.5

1.5

1.0

2.0
2.5

0.5

0.5

0.5

2.0
0.5
0.5

1.5

1.0

2.5

0.5

0.5
0.5

1.5

0.5
0.5
1.5

1.0
1.0

0.5

0.5
1.0

2.0

0.5

1.0
0.5

0.5

1.5

0.5

0.5
0.5

0.5

1.0

0.5

1.0

1.5

0.5

0.5
0.5

1.5
1.5

0.5

3.0
0.5
2.5

2.0
0.5
1.0

1.0

1.5

1.5

2.0

1.0

1.0

0.5

0.5

37

Table C.12: Order sizes, due dates (days) and release times. The length of a discrete time index corresponds to 0.5 days.

Task (order) Ti Order size, Mi (kg) Due date, ¯τi (days) Release time of order, (RT O)i (days)

T1
T2
T3
T4
T5
T6
T7
T8
T9
T10
T11
T12
T13
T14
T15

700
850
900
900
500
1350
950
850
450
650
300
450
200
700
300

0
5
0
6
0
2
3
0
2
6
0
1.5
0
0
5.5

10
22
25
20
28
30
17
23
30
21
30
28
15
29
40

38

Appendix D. Misspeciﬁcation of plant uncertainty

The processing time uncertainty has the same form as Eq. B.10a, but with misspeciﬁcation of
the parameters of the distribution by an additive constant, kpt ∈ Z+. In this case, the actual plant
processing time uncertainty, Eq. B.10a, is redeﬁned:

P Linl ∼ U (cid:0) max(1, ¯P T il − ˆc), ¯P T il + ˆc(cid:1),

∀n ∈ {1, . . . , N Bi,l},

∀l ∈ Li,

∀i ∈ I

where ˆc = c + kpt (in this work c = 1). Similarly, the rate of the Poisson distribution descriptive of
the due date uncertainty (Eq. B.10b) is misspeciﬁed. Here, however, the misspeciﬁcation is treated
probabilistically, such that the rate, ¯τi, ∀i ∈ I, is perturbed by a relatively small amount. Speciﬁcally,
we redeﬁne the due date uncertainty of the real plant as follows:

where ˆτi = ¯τi + kddzi, zi ∼ U (−1, 1) ∀i ∈ I and kdd ∈ Z+. Details of kpt and kdd investigated in this
work are provided by Table 6 in Section 5.5.

τi ∼ P (ˆτi),

∀i ∈ I

Appendix E. The probability of constraint satisfaction

In this work, we are interested in satisfying hard constraints on the scheduling decisions (control
In the case that the underlying plant is subject to uncertainties, we evaluate
inputs) to a plant.
this constraint as we would a soft constraint, i.e. probabilistically. This means we are interested in
quantifying the probability, FU , that the constraints on the control inputs, ut ∈ ˆUt, ∀t are satisﬁed:

(cid:18) T −1
(cid:92)

FU = P

{ui ∈ ˆUi}

(cid:19)

(E.1)

In order to estimate FU , we gain empirical approximation, FSA, known as the empirical cumulative
distribution function, by sampling. In this work, nI = 500 Monte Carlo samples were used to estimate
FLB in the results reported as follows:

i=0

FSA =

1
nI

nI(cid:88)

i=1

Y i

(E.2)

where Y i is a random variable, which takes a value of Y i = 1, if ut ∈ ˆUt, ∀t, and Y i = 0 if ∃ ut /∈ ˆUt, ∀t.
Due to the nature of estimation from ﬁnite samples, the FSA is prone to estimation error.Hence, we
employ a concept from the binomial proportion conﬁdence interval
literature, known as the Clop-
per–Pearson interval (CPI) [57]. The use of the CPI helps to ensure the probability of joint satisfaction
with a given conﬁdence level, 1 − υ. This is expressed by Lemma 1, which is recycled from [30].

Lemma 1. Joint chance constraint satisfaction via the Clopper-Pearson conﬁdence inter-
val [57]: Consider the realisation of FSA based on nI independently and identically distributed samples.
The lower bound of the true value FLB may be deﬁned with a given conﬁdence 1 − υ, such that:

P(FU ≥ FLB) ≥ 1 − υ
FLB = 1 − betainv(υ, nI + 1 − nI FSA, nI FSA)
where betainv(·) is the inverse of the Beta cumulative distribution function with parameters {nI + 1 −
nI FSA} and {nI FSA}.

(E.3)

39

