0
2
0
2

n
u
J

3
1

]

G
L
.
s
c
[

1
v
3
9
6
7
0
.
6
0
0
2
:
v
i
X
r
a

A New Algorithm for Tessellated Kernel Learning

Brendon K. Colbert
Arizona State University
Tempe, AZ 85287
brendon.colbert@asu.edu

Matthew M. Peet
Arizona State University
Tempe, AZ 85287
mpeet@asu.edu

Abstract

The accuracy and complexity of machine learning algorithms based on kernel
optimization are limited by the set of kernels over which they are able to optimize.
An ideal set of kernels should: admit a linear parameterization (for tractability);
be dense in the set of all kernels (for robustness); be universal (for accuracy). The
recently proposed Tesselated Kernels (TKs) is currently the only known class which
meets all three criteria. However, previous algorithms for optimizing TKs were
limited to classiﬁcation and relied on Semideﬁnite Programming (SDP) - limiting
them to relatively small datasets. By contrast, the 2-step algorithm proposed here
scales to 10,000 data points and extends to the regression problem. Furthermore,
when applied to benchmark data, the algorithm demonstrates signiﬁcant improve-
ment in performance over Neural Nets and SimpleMKL with similar computation
time.

Introduction

1
Kernel methods for classiﬁcation and regression (and Support Vector Machines (SVMs) in particular)
require selection of a kernel. Kernel Learning (KL) algorithms such as those found in [23, 21, 24]
automate this task by ﬁnding the kernel, k ∈ K which optimizes an achievable metric such as the
soft margin (for classiﬁcation). The set of kernels, k ∈ K, over which the algorithm can optimize,
however, strongly inﬂuences the performance and robustness of the resulting classiﬁer or predictor.

To understand how the choice of K inﬂuences performance and robustness, three properties were
proposed in [4] to characterize the set K - tractability, density, and universality. Speciﬁcally, K is
tractable if K is convex (or, preferably, a linear variety) - implying the KL problem is solvable using,
e.g. [17, 10, 12, 16, 9]. The set K has the density property if, for any (cid:15) > 0 and any positive kernel,
k∗ there exists a k ∈ K where (cid:107)k − k∗(cid:107) ≤ (cid:15). The density property implies the kernel will perform
well on untrained data (robustness or generalizability). The set K has the universal property if any
k ∈ K is universal - ensuring the classiﬁer/predictor will perform arbitrarily well on large sets of
training data.

In [4], the Tessellated Kernels (TKs) were shown to have all 3 properties, the ﬁrst known such class
of kernels. This work was based on a general framework for using positive matrices to parameterize
positive kernels (as opposed to positive kernel matrices as in [12, 16, 15]). Unfortunately, however,
the algorithms proposed in [4] were either based on SemiDeﬁnite Programming (SDP) (thereby
limiting the amount of training data) or used a randomized linear basis for the kernels (implying
loss of density). Thus, while the algorithms in [4] outperformed all other methods (including deep
learning) as measured by Test Set Accuracy (TSA), the computation times were not competitive.
Furthermore, the results in [4] did not encompass the problem of regression.

In this paper, we extend the TK framework proposed in [4] to the problem of regression. The KL
problem in regression has been studied using SDP in [16, 15] and Quadratic Programming (QP) in
e.g. [17, 10]. However, neither of these previous works considered a set of kernels with both the
tractability and the density property. By generalizing the Tessellated KL framework proposed in [4]

33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
to the regression problem, we demonstrate signiﬁcant increases in performance, as measured by
Mean Square Error (MSE), and when compared to the results in [17, 10, 16].

In addition, we show that the SDP-based algorithm [4] for classiﬁcation, and extended here to
regression, can be decomposed into primal and dual sub-problems, OP T _A and OP T _P - similar
to the approach taken in [17, 10]. Furthermore, we show that OP T _P (an SDP) admits an analytic
solution using the Singular Value Decomposition (SVD) - an approach which allows us to consider
higher dimensional feature spaces and more complex TKs. In addition, OP T _A is a convex QP and
may be solved efﬁciently with achieved complexity which scales as O(m2.16) where m is the number
of data points. We use a two-step algorithm on OP T _A and OP T _P and show that termination
at OP T _A = OP T _P is equivalent to global optimality. The resulting algorithm, then, does not
require the use of SDP and, when applied to several standard test cases, is shown to retain the
favorable TSA of [4] for classiﬁcation, while offering improved MSE for regression, and competitive
computation times as compared to other KL and deep learning algorithms.

2 An Ideal Set of Kernels for KL in Classiﬁcation and Regression
Consider a generalized representation of the KL problem, which encompasses both classiﬁcation
and regression where (using the representor theorem [19]) the learned function is of the form
fα,k(z) = (cid:80)m

i=1 αik(xi, z).

min
k∈K

min
α∈Rm,b

(cid:107)fα,k(cid:107)2 + C

(cid:88)m

i=1

l(fα,k, b)yi,xi

(1)

Here (cid:107)fα,k(cid:107)X = (cid:80)m
j=1 αiαjk(xi, xj) and l(fα,k, b)yi,xi is the loss function and is deﬁned for
SVM binary classiﬁcation and SVM regression as lc(fα,k, b)yi,xi and lr(fα,k, b)yi,xi, respectively,
where

(cid:80)m

i=1

lc(fα,k, b)yi,xi= max{0, 1−yi(fα,k(xi)−b)} and l(fα,k, b)r

yi,xi

= max{0, |yi−(fα,k(xi)−b)|−(cid:15)}.

The properties of the classiﬁer/predictor, fα,k, resulting from Optimization Problem 1 will depend on
the properties of the set K, which is presumed to be a subset of the convex cone of all positive kernels.
To understand how K inﬂuences the tractability of the optimization problem and the resulting ﬁt, we
consider three properties of the set, K.

2.1 Tractability
We say a set of kernel functions, K, is tractable if it can be represented using a countable basis.

Deﬁnition 1. The set of kernels K is tractable if there exist a countable set {Gi(x, y)}i such that,
for any k ∈ K, there exists NG ∈ N where k(x, y) = (cid:80)NG

i=1 viGi(x, y) for some v ∈ RNG .

Note the Gi(x, y) need not be positive kernel functions. The tractable property is required for the KL
problem to be tractable using algorithms for convex optimization.

2.2 Universality
Universal kernel functions always have positive deﬁnite (full rank) kernel matrices, implying that
i=1, there exists a function f (z) = (cid:80)m
for arbitrary data {yi, xi}m
i=1 αik(xi, z), such that f (xj) = yj
for all j = 1, .., m. Conversely, if a kernel is not universal, then exists a data set {xi, yi}m
i=1 such
that for any α ∈ Rm, there exists some j ∈ {1, · · · , m} such that f (yj) (cid:54)= (cid:80)m
i=1 αik(xi, xj). This
ensures that SVMs using universal kernels can always beneﬁt from additional training data, whereas
non-universal kernels may saturate.
Deﬁnition 2. A kernel k : X × X → R is said to be universal on the compact metric space X if
it is continuous and there exists an inner-product space W and feature map, Φ : X → W such
that k(x, y) = (cid:104)Φ(x), Φ(y)(cid:105)W and where the unique Reproducing Kernel Hilbert Space (RKHS),
H := {f : f (x) = (cid:104)v, Φ(x)(cid:105), v ∈ W} with associated norm (cid:107)f (cid:107)H := inf v{(cid:107)v(cid:107)W : f (x) =
(cid:104)v, Φ(x)(cid:105)} is dense in C(X) := {f : X → R : f is continuous} where (cid:107)f (cid:107)C := supx∈X |f (x)|.

The following deﬁnition extends the universal property to a set of kernels.

Deﬁnition 3. A set of kernel functions K has the universal property if every kernel function k ∈ K is
universal.

2

2.3 Density
The third property is density which distinguishes the TK class from other sets of kernel functions
with the universal property. For instance consider a set containing a single Gaussian kernel function -
which is clearly not ideal for kernel learning. The set containing a single Gaussian is tractable (it has
only one element) and every member of the set is universal. However, it is not dense.

Considering SVM for classiﬁcation, the KL problem determines the kernel k ∈ K for which we may
obtain the maximum separation in the kernel-associated feature space. Increasing this separation
distance makes the resulting classiﬁer more robust (generalizable) [2]. The density property, then,
ensures that the resulting KL algorithm will be maximally robust (generalizable) in the sense of
separation distance.

Likewise, considering SVMs for regression, the KL problem ﬁnds the kernel k ∈ K which permits
the “ﬂattest” [20] function in feature space. In this case, the density property ensures that the resulting
KL algorithm will be maximally robust (generalizable) in the sense of ﬂatness.

These arguments motivate the following deﬁnition of the pointwise density property.
Deﬁnition 4. The set of kernels K is said to be pointwise dense if for any positive kernel, k∗, any set
of data {xi}m

i=1, and any (cid:15) > 0, there exists k ∈ K such that (cid:107)k(xi, xj) − k∗(xi, xj)(cid:107) ≤ (cid:15).

3 A General Framework for Representation of Tractable Kernel Sets
Here we deﬁne a framework for constructing classes of tractable positive kernel functions and
illustrate this approach on the class of General Polynomial Kernels.
Proposition 5. Let N be any bounded measurable function N : X × Y → Rq and P ∈ Rq×q be a
positive semideﬁnite matrix P ≥ 0. Then

is a positive kernel function.

k(x, y) =

(cid:90)

X

N (z, x)T P N (z, y)dz

(2)

The proof for Proposition (5) may be found in [4].
Lemma 6. Let N be any bounded measurable function N : X × Y → Rq on compact X and Y .
Then the set of kernel functions

(cid:26)

K :=

k | k(x, y) =

(cid:90)

X

N (z, x)T P N (z, y)dz, P ≥ 0

(cid:27)

is tractable.

(3)

For a given N , the map P (cid:55)→ k is linear. Speciﬁcally,
(cid:88)q

(cid:88)q

k(x, y) =

Pi,jGi,j(x, y) where Gi,j(x, y) =

i=1

j=1

(cid:90)

X

Ni(z, x)Nj(z, y)dz.

and thus by Deﬁnition 1 K is tractable.

In Subsection 3.1 we apply this framework to obtain Generalized Polynomial Kernels. In Subsec-
tion 4.1, we use the framework to obtain the TK class.

3.1 The Class of General Polynomial Kernels is Tractable
The class of General Polynomial Kernels (GPKs) is deﬁned as the set of all polynomials, each of
which is a positive kernel.

: k is a positive kernel}

KP := {k ∈ R[x, y]
The GPK class is not universal, but is tractable, as per the following lemma.
Lemma 7. KP is tractable.
Proof. Let Zd : Rn → Rq be the vector of monomials of degree d or less. From [4], we have that a
polynomial k of degree 2d is a positive polynomial kernel if and only if there exists some P ≥ 0 such
that k(x, y) = Zd(x)T P Zd(y). Now for any ﬁnite-dimensional subset of KP , let d be the maximum
degree over this subset and deﬁne N (z, y) = Zd(y). Then Lemma 6 implies that KP is tractable.

(4)

This lemma implies that a representation of the form of Equation (2) is necessary and sufﬁcient for
a GPK to be positive. For convenience, we denote the set of GPK kernels of degree d or less as
follows [18].

Kd

P := {k : k(x, y) = Zd(x)T P Zd(y) : P ≥ 0}

(5)

3

4 Tessellated Kernels: Tractable, Dense and Universal
In this section, we deﬁne the class of TK kernels and show it is tractable, dense, and universal.

4.1 Tessellated Kernels
Again, let Zd : Rn × Rn → Rq be the vector of monomials of degree d. Deﬁne I, the indicator
function for the positive orthant, and the following choice of N : Rn × Rn → R2q as
(cid:21)
(cid:20)Zd(z, x)I(z − x)
Zd(z, x)I(x − z)

z ≥ 0
otherwise,

and N d

T (z, x) =

(cid:26)1
0

I(z) =

(6)

where z ≥ 0 means zi ≥ 0 for all i. We now deﬁne the set of TK kernels for a < b ∈ Rn as

(cid:40)

(cid:41)

Kd

T :=

k : k(x, y) =

N d

T (z, x)T P N d

T (z, y)dz, P ≥ 0

, KT := {k : k ∈ Kd

T , d ∈ N}.

(cid:90) b

a

Kernels in the TK class are “Tessellated” in the sense that each datapoint deﬁnes a vertex which
bisects each dimension of the domain of the resulting classiﬁer/predictor - resulting in a tessellated
partition of the feature space.

4.2 The Set of TK Kernels is Tractable
The class of TK kernels is prima facie in the form of Eqn. (3) in Lemma 6 and hence is tractable.

However, we will expand on this result by specifying the basis for the set of TK kernels, which will
then be used in Section 5.
Corollary 8. Suppose that for a < b ∈ Rn, and d ∈ N. We deﬁne the ﬁnite set Dd := {(δ, λ) ∈ N2n :
(cid:107)(δ, λ)(cid:107)1 ≤ d}. Let {[δi, γi]}q
i=1 ⊆ Dd be some ordering of Dd and deﬁne Zd(x, z)j = xδj zγj
where zδj xγj := (cid:81)n
i xγj ,i
. Now let k be as deﬁned in Eqn. (2) for some P > 0 and where N

i=1 zδj ,i
is as deﬁned in Eqn. (6). If we partition P =

i

(cid:21)

(cid:20) Q R
RT S

then we have,

k(x, y) =

(cid:88)q

i,j=1

Qi,jgi,j(x, y) + Ri,jti,j(x, y) + RT

i,jti,j(y, x) + Si,jhi,j(x, y)

where gi,j, ti,j, hi,j : R2n → R are deﬁned as

gi,j(x, y) := xδiyδj T (p∗(x, y), b, γi,j + 1), ti,j(x, y) := xδiyδj T (x, b, γi,j + 1) − gi,j(x, y), and
hi,j(x, y) := xδiyδj T (a, b, γi + γj + 1) − gi,j(x, y) − ti,j(x, y) − ti,j(y, x),

where 1 ∈ Nn is the vector of ones, p∗ : R2n → Rn is deﬁned elementwise as p∗(x, y)i =
max{xi, yi}, and T : Rn × Rn × Nn → R is deﬁned as
(cid:32) yζj
j
ζj

T (x, y, ζ) =

xζj
j
ζj

(cid:89)n

j=1

(cid:33)

−

.

The proof of Corollary 8 can be found in [4].

4.3 The TK Class is Dense
The density property differentiates the set of TK kernels from other sets of kernel functions (e.g. a
linear combination of Gaussian kernels of ﬁxed bandwidths).

From [4] we have that the set of TK kernels satisﬁes the pointwise density property.
Theorem 9. For any kernel matrix K ∗ and any ﬁnite set {xi}m
such that if Ki,j = k(xi, xj), then K = K ∗.

i=1, there exists a d ∈ N and k ∈ Kd
T

In [4] an analytical solution, K ∗, was found for the optimal trace-constrained kernel matrix that
maximized the separation distance between two classes of points in the feature space. It was shown
in this work that when {yi} has an equal number of positive and negative labels, K ∗ contains an
equal number of positive and negative elements - illustrating the importance of using kernels which
are not pointwise positive (Gaussians are pointwise positive).

4

To illustrate the density property, then, we show how optimal
GPK and TK kernels yield kernel matrices which approximate the
analytic solution, K ∗, of the optimal kernel matrix problem for a
given set of data {xi} and labels {yi}, while Gaussian kernels do
not. Speciﬁcally, we consider the following optimization problem.

(cid:107)K − K ∗(cid:107)∞ s.t. Ki,j = k(xi, xj)

min
k∈K

(7)

In these problems, the sets K will be: Kγ
G - the sum of N Gaussians
with bandwidths γi; Kd
T - the TK
kernels of degree d. More precisely, for bandwidths γ ∈ RN , we
deﬁne Kγ

P - the GPKs of degree d; and Kd

k : k(x, y) = (cid:80)N

||x−y||2
2
γi

: µi > 0

(cid:27)

(cid:26)

.

i=1 µie

G :=

Figure 1: The achieved objective
((cid:107)K − K ∗(cid:107)∞) of Optimization
Problem 7 for TK and GPK of
degree d; and m Gaussians with
bandwidths in [.01, 10]. The
number of bandwidths is se-
lected so that the number of deci-
sion variables match in the Gaus-
sian and TK cases.

Consider a spiral data set with 20 samples, using equal numbers of
positive and negative labels. Fig. 1 shows the achieved objective
value of Problem (7) for Kγ
P , and Kd
G, Kd
T as a function of the
number of bandwidths (top x axis - N in Kγ
P , and Kd
G), polynomial degree (bottom x axis - d in Kd
T ).
The x-axes of the plots are scaled to show equal numbers of decision variables. As expected, the
case K = KG
γ saturates with an objective value signiﬁcantly larger than the lower bound. The cases
K = Kd

T , meanwhile have almost no error at degree d = 7.

P and K = Kd

4.4 TK Kernels are Universal
Finally we discuss the universality property of the class of TK kernels which ensures that every TK
function can ﬁt the training data well.

The following theorem from [4] shows that any TK kernel with P > 0 is necessarily universal.
Theorem 10. Suppose k is as deﬁned in Eqn. (2) for some P > 0, d ∈ N and N as deﬁned in
Eqn. (6). Then k is universal for a < b ∈ Rn.

This theorem implies that even if we use the subset of TK kernels deﬁned by d = 0, this subset is still
universal.

5 A New Algorithm for KL in Classiﬁcation and Regression using TKs
In this section, we express the KL optimization problem for both classiﬁcation and regression and
break this optimization problem into two sub-problems which allow us to express the problem in
primal and dual form. For convenience, we deﬁne the feasible sets for the sub-problems as

Yc := {α ∈ Rm :

m
(cid:88)

i=1

X := {P ∈ Rq×q : trace(P ) = q, P > 0}

αiyi = 0, 0 ≤ αi ≤ C}, Yr := {α ∈ Rm :

αi = 0, αi ∈ [−C, C]}.

m
(cid:88)

The common part of the objective is

O(α, P ) := −

1
2

(cid:88)m

(cid:88)m

i=1

j=1

αiαj

(cid:90) b

a

while the unique parts of the objective are

i=1

N d

T (z, xi)T P N d

T (z, yj)dz,

(8)

κc(α) :=

(cid:88)m

i=1

αi

and

κr(α) := −(cid:15)

(cid:88)m

i=1

|αi| +

(cid:88)m

i=1

yiαi.

Then the KL optimization problem (OP T ) for TK kernels ((cid:12) being elementwise multiplication) is
as follows for classiﬁcation and regression, respectively.

OP T := min
P ∈X

max
α∈Yc

O(α (cid:12) y, P ) + κc(α),

and OP T := min
P ∈X

max
α∈Yr

O(α, P ) + κr(α).

Primal Formulation: We can formulate the primal problem (OP TP ) as

OP TP = min
P ∈X

max
α∈Y

O(α, P ) + κr(α) or O(α (cid:12) y, P ) + κc(α) = min
P ∈X

OP T _A(P )

(9)

where for classiﬁcation and regression, respectively,

OP T _A(P ) := max
α∈Yc

O(α (cid:12) y, P ) + κc(α),

and OP T _A(P ) := max
α∈Yr

O(α, P ) + κr(α).

5

Polynomial Degree (d)1234567∞00.20.40.60.811.2Tessellated KernelPolynomial KernelGaussian Kernel62155120231406666||K−K*||image

(a) An
from
Google Maps of a section
of
the Grand Canyon
corresponding to (36.04,
-112.05)
and
(36.25, -112.3) longitude.

latitude

(b) Elevation data (m =
750) from [1] for a sec-
tion of the Grand Canyon
between (36.04, -112.05)
latitude and (36.25,
-
112.3) longitude.

(c) Predictor using a hand-
tuned Gaussian kernel
trained on the elevation
data in (b). The Gaus-
sian predictor poorly rep-
resents the sharp edge at
the north and south rim.

(d) Predictor from Algo-
rithm 1 trained on the ele-
vation data in (b). The TK
predictor accurately repre-
sent the north and south
rims of the canyon.

Figure 2: Subﬁgure (a) shows an 3D representation of the section of the Grand Canyon to be ﬁtted.
In (b) we plot elevation data of this section of the Grand Canyon. In (c) we plot the predictor for a
hand-tuned Gaussian kernel. In (d) we plot the predictor from Algorithm 1 where d = 2.

Dual Formulation: Alternatively, we have the dual formulation (OP TD).

OP TD = max
α∈Y

OP T _P (α)

(10)

where Y = Yc for classiﬁcation and Y = Yr regression. Likewise, for classiﬁcation and regression,
respectively,

OP T _P (α) := min
P ∈X

O(α (cid:12) y, P ) + κc(α)

and OP T _P (α) := min
P ∈X

O(α, P ) + κr(α).

Lemma 11. For α ∈ Y, P ∈ X , OP T _A(P ) = OP T _P (α) if and only if: {α, P } solve OP T ; P
solves OP TP ; and α solves OP TD.
Proof. For any minmax optimization problem with objective function φ, we have

d∗ = max
α∈Y

min
P ∈X

φ(P, α) ≤ min
P ∈X

max
α∈Y

φ(P, α) = p∗,

and strong duality holds (p∗ − d∗ = 0) if X and Y are both convex and one is compact, φ(·, α) is
convex for every α ∈ Y and φ(P, ·) is concave for every P ∈ X , and the function φ is continuous
[7]. In our case, these conditions hold for both classiﬁcation and regression where φ(P, α) =
O(α, P ) + κr(α) or O(α (cid:12) y, P ) + κc(α). Hence if α∗ solves OP T _P and P ∗ solves OP T _A,
then {α∗, P ∗} solves OP T and

OP T _A(P ) = OP T _A(P ∗).

OP T _P (α∗) = max
α∈Y

OP T _P (α) = min
P ∈X

Conversely, suppose α ∈ Y, P ∈ X , then

OP T _P (α) ≤ max
α∈Y

OP T _P (α) = OP T _P (α∗)

= OP T _A(P ∗) = min
P ∈X

OP T _A(P ) ≤ OP T _A(P ).

Hence if OP T _A(P ) = OP T _P (α), then OP T _A(P ) = OP T _A(P ∗) = OP T _P (α∗) =
OP T _P (α) and hence P and α solve OP T _A and OP T _P , respectively.
We propose Algorithm 1 as a two-step iterative algorithm for solving Optimization Problem (10).

Algorithm 1 Two Step TKL
Initialize P = I;
while OP T _P (αk) − OP T _A(Pk) > (cid:15) do

5.1 Solving OP T _A(P )
For a given P > 0, OP T _A(P ) is a Quadratic
Program (QP). General purpose QP solvers as
applied to this problem have a worst-case com-
plexity which scales as O(m3) [25] where m is
the number of data points. This computational
complexity may be improved, however, by not-
ing that the problem formulation is compatible
with the representation deﬁned in [3] for QPs
derived from SVM. In this case, the algorithm in LibSVM [3] can reduce the computational burden
somewhat. This improved performance is illustrated in Figure 3 where we observe the achieved
complexity scales as O(m2.1). Note that for the 2-step algorithm proposed in this manuscript, solving
the QP in OP T _A(P ) is signiﬁcantly slower that solving the Singular Value Decomposition (SVD)
required for OP T _P (α), which is deﬁned in the following subsection. However, the achieved com-
plexity of O(m2.1) is also signiﬁcantly faster than solving the large SDP, as described in [12], [16],
and [4]. This complexity comparison will be further discussed in Section 6.

αk+1 = OP T _A(Pk)
Pk+1 = PK +tOP T _P (αk+1)
line search)
k = k + 1

(select t using

end while

1+t

6

(a) Numerical complexity
analysis of TKL for clas-
siﬁcation versus m.

(b) Numerical complexity
analysis of TKL for re-
gression versus m.

(c) Numerical complexity
analysis of TKL for clas-
siﬁcation versus q.

(d) Numerical complexity
analysis of TKL for re-
gression versus q.

Figure 3: In (a) and (b) we plot log scale plots of the time taken to optimize TKL as the number of
inputs change for P ∈ Rq×q. The line of best linear ﬁt is plotted as a dotted line. In (c) and (d) we
plot log scale plots of the time taken to optimize TKL as the value of q changes for four different
values of m.
5.2 Solving OP T _P (α)
For a given α, OP T _P (α) is an SDP. Fortunately, however, this SDP is structured so as to admit an
analytic solution using the SVD. To solve OP T _P (α) we minimize O(α, P ) from Eq. (8) which, as
per Corollary 8, is linear in P and can be formulated as

OP T _P (α) := min

P ∈Rq×q
trace(P )=q
P >0

O(α, P ) := min

P ∈Rq×q
trace(P )=q
P >0

trace(C(α)T P )

(11)

where

Ci,j(α) =

m
(cid:88)

(αkyk)Gi,j(xk, xl)(αlyl),

Gi,j(x, y) :=

k,l=1

and g, t and h can be found in Corollary 8.






gi,j(x, y)
ti,j(x, y)
ti,j(y, x)
hi,j(x, y)

2

if i ≤ q
if i ≤ q
if i > q
if i > q

2 , j ≤ q
2 , j > q
2 , j ≤ q
2 , j > q

2

2

2

The following theorem gives an analytic solution for OP T _P using the SVD.
Theorem 12. Let C = V ΣV T be the SVD of symmetric C ∈ Rq×q and v be the right singular
vector corresponding to the minimum singular value of C. Then P ∗ = qvvT solves OP T _P .
Proof. Recall OP T _P has the form min

trace(C T P ) s.t. P ≥ 0, trace(P ) = q.

P ∈Rq×q

Denote the minimum singular value of C as σmin(C). Then for any feasible P ∈ X , by [8] we have
trace(C T P ) ≥ σmin(C)trace(P ) = σmin(C)q.

Now consider P = qvvT ∈ Rq×q. P is feasible since P ≥ 0, and trace(P ) = q. Furthermore,

trace(CP ) = q trace(V ΣV T vvT ) = q trace(vT V ΣV T v) = q σmin(C)

as desired.

Note that the size of the SVD problem in OP T _P (α) is q2, which increases with the number
of features, which is typically relatively small. As a result, we observe that the OP T _P step of
Algorithm 1 is typically less computationally intense than the OP T _A step.

6 Complexity and Scalability of the New TK Kernel Learning Algorithm
We consider the computational complexity of Algorithm 1. If we deﬁne the number of data points
used to learn the TK kernel function as m and the size of P as q × q, then we ﬁnd experimentally
that the complexity of Algorithm 1 scales as approximately O(m2.16q2.23) for classiﬁcation and
O(m2.24q3.59) for regression as can be seen in Fig. 3. These results are lower with respect to m than
the value of O(m2.6q1.9) reported in [4] for binary classiﬁcation. The values for classiﬁcation and
regression are both estimated using the data set: Combined Cycle Power Plant (CCPP) in [22, 11],
containing 4 features and m = 9568 samples. In the case of classiﬁcation, labels with value greater
than or equal to the median of the were relabeled as 1, and those less than the median were relabeled
as −1. Note that to study scalability in q, we varied the number of features in the dataset - thereby
incrementing the size of the matrix P ∈ Rq×q.

Aside from improved scalability, the overall time required for Algorithm 1 is signiﬁcantly reduced
when compared with the algorithm in [4], improving by two orders of magnitude in some cases. This
is illustrated for classiﬁcation using four data sets in Table 1. This improved complexity is likely due
to the lower overhead associated with QP and the SVD.

7

103104Number of Inputs10-1100101102Time to Optimize (seconds)q = 4q = 6q = 8q = 10103104Number of Inputs10-1100101102103104Time to Optimize (seconds)q = 4q = 6q = 8q = 1045678910Value of q101102Time to Optimize (seconds)m=5367m=6244m=7122m=800045678910Value of q101102103Time to Optimize (seconds)m=5367m=6244m=7122m=8000Table 1: We report the mean computation time (in seconds), along with standard deviation, for 30
trials comparing the SDP algorithm in [4] and the new TKL algorithm on several data sets. All tests
are run on a computer with an Intel i7-5960X CPU at 3.00 GHz with 128 Gb of RAM.

Method
SDP
TKL

Liver [6]
95.75 ± 2.68
1.10 ± 0.24

Cancer [13]
636.17 ± 25.43
8.20 ± 0.36

Heart [6]
221.67 ± 29.63
3.35 ± 0.26

Pima [6]
1211.66 ± 27.01
12.66 ± 0.44

Table 2: Mean Square Error comparison for algorithms [TKL], [SimpleMKL] and [Neural Net]. In
the data set column m is the number of points in the training data set and n is the number of features.
All tests are run on a computer with an Intel i7-5960X CPU at 3.00 GHz with 128 Gb of RAM.

Data Set
CCPP [22, 11]
n = 4, m = 8000
mt = 1568
Airfoil [6]
n = 5, m = 1300
mt = 203

Method
TKL
SimpleMKL
Neural Net
TKL
SimpleMKL
Neural Net

Error
9.70
13.77
15.00
1.46
3.63
4.28

Time
1463.8
26097.1
850.4
92.1
1025.0
61.3

Data Set
Abalone [6]
n = 8, m = 4000
mt = 177
Forest [5]
n = 10, m = 457
mt = 50

Method
TKL
SimpleMKL
Neural Net
TKL
SimpleMKL
Neural Net

Error
3.43
4.28
8.72
2.05
2.07
6.40

Time
522.5
1185.3
483.4
7.6
0.8
117.7

7 Accuracy of the New TK Kernel Learning Algorithm for Regression
As expected, for classiﬁcation, the accuracy of the new TK kernel learning algorithm (TKL) is
identical to the analysis in [4].

For regression, we evaluate the accuracy of TKL when compared to other state of the art machine
learning algorithms. Because the set of TK kernels is dense, for classiﬁcation (as shown in [4]), TKL
outperforms all existing algorithm with respect to TSA. For regression, the appropriate metric is
Mean Square Error (MSE). The algorithms used in our comparison are as follows.
[TKL] Algorithm 1 with d = 1, (cid:15) = .1 and we scale the data so that xi ∈ [0, 1]n, and then select
[a, b] = [0 − δ, 1 + δ]n, where δ > 0 and C are chosen by 5-fold cross-validation;

[SimpleMKL] We use SimpleMKL [17] with a standard selection of Gaussian and polynomial
kernels with bandwidths arbitrarily chosen between .5 and 10 and polynomial degrees one through
three - yielding approximately 13(n + 1) kernels. We set (cid:15) = .1 as in TKL and C is chosen by 5-fold
cross-validation;

[Neural Net] We use a 3 layer neural network with 50 hidden layers using MATLABs
(feedforwardnet) implementation and stopped learning after the error in a validation set decreased
sequentially 50 times.

In Table 2, we see the average MSE on the test set for these three approaches as applied to randomly
selected regression benchmark data sets where n is the dimension of the data, m is the number of
training data and mt is the number of testing data points. In all cases except Forest, [TKL] had both
a lower (or comparable) computation time and MSE than both SimpleMKL and Neural Net. In all
cases, the MSE for TKL was signiﬁcantly lower - illustrating the importance of the density property.

To further illustrate the importance of density property and the TKL framework for practical regression
problems, we used elevation data from [1] to learn a TK kernel and associated SVM predictor
representing the surface of the Grand Canyon in Arizona. This data set is particularly challenging due
to the variety of geographical features. The result of the TKL algorithm can be seen in Figure 2(d).

8 Conclusion
We have extended the TK kernel learning framework to regression problems and proposed a faster
algorithm for TK kernel learning which can be used for both classiﬁcation and regression. The set
of TK kernels is tractable, dense, and universal - implying that KL algorithms based on TK kernels
are more robust - resulting in higher TSA for classiﬁcation and lower MSE for regression. These
three properties, combined with the improved computational complexity of the new algorithm, has
resulted in a kernel learning framework which achieves both lower MSE and computation time when
compared to both SimpleMKL and neural networks.

8

Broader Impacts

While machine learning algorithm have become very accurate in recent years, they perform poorly
when faced with changes in the underlying process. As evidenced by Covid19, predictive models
based on ML algorithms can be brittle [14]. The density property of the TK class ensures that
the models generated using the algorithms described in this manuscript will be more robust to
such changes in environment. Naturally, however, over-reliance on predictive models, without
understanding of the process, can lead to negative outcomes, even if the models are robust.

References

[1] J.J. Becker, D.T. Sandwell, W.H.F. Smith, J. Braud, B. Binder, J.L. Depner, D. Fabre, J. Factor,
S. Ingalls, S.H. Kim, et al. Global bathymetry and elevation data at 30 arc seconds resolution:
Srtm30_plus. Marine Geodesy, 32(4):355–371, 2009.

[2] B. Boehmke and B.M. Greenwell. Hands-On Machine Learning with R. CRC Press, 2019.
[3] C-C. Chang and C-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http:
//www.csie.ntu.edu.tw/~cjlin/libsvm.

[4] B.K. Colbert and M.M. Peet. A convex parametrization of a new class of universal kernel

functions. Journal of Machine Learning Research, 21(45):1–29, 2020.

[5] P. Cortez and A. Morais. A data mining approach to predict forest ﬁres using meteorological

data. 2007.

[6] D. Dua and C. Graff. UCI machine learning repository, 2017.
[7] K. Fan. Minimax theorems. Proceedings of the National Academy of Sciences of the United

States of America, 39(1):42, 1953.
[8] Y. Fang, K.A. Loparo, and X. Feng.

Inequalities for the trace of matrix product.

IEEE

Transactions on Automatic Control, 39(12):2489–2490, 1994.

[9] M. Gönen and E. Alpaydın. Multiple kernel learning algorithms. Journal of Machine Learning

Research, 2011.

[10] A. Jain, S. Vishwanathan, and M. Varma. SPF-GMKL: generalized multiple kernel learning
with a million kernels. In Proceedings of the ACM International Conference on Knowledge
Discovery and Data Mining, 2012.

[11] H. Kaya, P. Tüfekci, and F.S. Gürgen. Local and global learning methods for predicting power
of a combined gas & steam turbine. In Proceedings of the international conference on emerging
trends in computer and electronics engineering icetcee, pages 13–18, 2012.

[12] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the kernel matrix

with semideﬁnite programming. Journal of Machine Learning Research, 2004.

[13] O.L. Mangasarian, R. Setiono, and W.H. Wolberg. Pattern recognition via linear programming:

Theory and application to medical diagnosis. 1990.

[14] C. Mims. AI isn’t magical and won’t help you reopen your business. Wall Street Journal, May

2020.

[15] K. Ni, S. Kumar, and T. Nguyen. Learning the kernel matrix for superresolution. In 2006 IEEE

Workshop on Multimedia Signal Processing, pages 441–446, 2006.

[16] S. Qiu and T. Lane. Multiple kernel learning for support vector regression. Computer Science
Department, The University of New Mexico, Albuquerque, NM, USA, Tech. Rep, page 1, 2005.
[17] A. Rakotomamonjy, F. R. Bach, S. Canu, and Y. Grandvalet. SimpleMKL. Journal of Machine

Learning Research, 2008.

[18] B. Recht. Convex Modeling with Priors. PhD thesis, Massachusetts Institute of Technology,

2006.

[19] B. Schölkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. In International

conference on computational learning theory, pages 416–426, 2001.

[20] A.J. Smola and B. Schölkopf. A tutorial on support vector regression. Statistics and computing,

14(3):199–222, 2004.

9

[21] S. Sonnenburg, G. Rätsch, S. Henschel, C. Widmer, J. Behr, A. Zien, F. De Bona, A. Binder,
C. Gehl, and V. Franc. The shogun machine learning toolbox. Journal of Machine Learning
Research, 11(60):1799–1802, 2010.

[22] P. Tüfekci. Prediction of full load electrical power output of a base load operated combined
cycle power plant using machine learning methods. International Journal of Electrical Power
& Energy Systems, 60:126–140, 2014.

[23] Z. Xu, R. Jin, H. Yang, I. King, and M.R. Lyu. Simple and efﬁcient multiple kernel learning by
group lasso. In Proceedings of the 27th international conference on machine learning, pages
1175–1182, 2010.

[24] H. Yang, Z. Xu, J. Ye, I. King, and M.R. Lyu. Efﬁcient sparse generalized multiple kernel

learning. IEEE Transactions on neural networks, 22(3):433–446, 2011.

[25] Y. Ye and E. Tse. An extension of karmarkar’s projective algorithm for convex quadratic

programming. Mathematical programming, 44(1-3):157–179, 1989.

10

