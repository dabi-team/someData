October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

To appear in Quantitative Finance, Vol. 08, No. 15, August 2020, 1–22

0
2
0
2

t
c
O
3
1

]
T
S
.
n
i
f
-
q
[

3
v
5
2
2
6
0
.
8
0
0
2
:
v
i
X
r
a

Neural Network-based
Automatic Factor Construction

Jie Fangσ†, Jianwu Lin∗†, Shutao Xiaβ, Zhikang Xiaβ, Shenglei Hu,
Xiang Liuβ and Yong Jiang†

†Tsinghua-Berkeley Shenzhen Institute, Tsinghua University, Shenzhen, China
βTsinghua Shenzhen International Graduate School, Shenzhen, China
†Sino-UK Blockchain Industry Institute, Guangxi University, Guangxi, China

(Received 01 June 2020; in ﬁnal form 05 August 2020)

Instead of conducting manual factor construction based on traditional and behavioural ﬁnance analy-
sis, academic researchers and quantitative investment managers have leveraged Genetic Programming
(GP) as an automatic feature construction tool in recent years, which builds reverse polish mathematical
expressions from trading data into new factors. However, with the development of deep learning, more
powerful feature extraction tools are available. This paper proposes Neural Network-based Automatic
Factor Construction (NNAFC), a tailored neural network framework that can automatically construct
diversiﬁed ﬁnancial factors based on ﬁnancial domain knowledge and a variety of neural network struc-
tures. The experiment results show that NNAFC can construct more informative and diversiﬁed factors
than GP, to eﬀectively enrich the current factor pool. For the current market, both fully connected and
recurrent neural network structures are better at extracting information from ﬁnancial time series than
convolution neural network structures. Moreover, new factors constructed by NNAFC can always improve
the return, Sharpe ratio, and the max draw-down of a multi-factor quantitative investment strategy due
to their introducing more information and diversiﬁcation to the existing factor pool.

Keywords: Factor Construction, Deep Learning, Neural Network, Quantitative Investment

JEL Classiﬁcation: C23, C33, C45, C53, C63

1.

Introduction

1.1. Background

In quantitative trading, predicting future returns of stocks is one of the most important and chal-
lenging tasks. Various factors can be used to predict future returns based on ﬁnancial time series,
such as price, volume, and the company’s accounting data. Usually, researchers deﬁne the factors
that are constructed from price and volume data as technical factors, and the other factors, which
are constructed from the company’s accounting data, are deﬁned as fundamental factors. Typically,
researchers conduct manual factor selection based on traditional and behavioural ﬁnance analysis.
After Sharpe (1964) proposed the single-factor model for stock returns, Fama and French (1993)
proposed the Fama-French Three-factor Model, which selected three important factors that could
provide the most important major information to explain the stock return. Fama (2015) proposed
the Fama-French Five-factor Model, which added two more factors into their Three-factor Model.

σFirst author. Email: fangx18@mails.tsinghua.edu.cn
∗Corresponding author. Email: lin.jianwu@sz.tsinghua.edu.cn

1

 
 
 
 
 
 
October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Currently, some commercial multi-factor risk models, such as the Barra and Axioma model, man-
ually select factors that are updated periodically by human researchers. However, there are two
limitations in manual factor selection. First, it is very time-consuming to update factors man-
ually. Second, it is not easy to construct certain nonlinear factors from high-dimensional data.
Thus, to address data from a rapidly developing ﬁnancial market, both academic researchers and
quantitative investment managers have paid more and more attention to automatic ﬁnancial fac-
tor construction tools. In the computer science ﬁeld, this task is deﬁned as Automatic Feature
Construction, Krawiec (2002).

Feature construction in computer science is a process that discovers missing relationships between
features and outcomes and augments the space of the features by inferring or creating new features.
In this sense, a ﬁnancial factor is a special case among general features in computer science.
During this process, new features can be generated from a combination of existing features. A more
straightforward description is that the algorithms use operators, hyper-parameters and existing
features to construct a new feature. Sometimes, both feature construction and feature selection
occur in the same procedure. Dash (1997) summarized these methods, which consist of wrapping,
ﬁltering, and embedding. Filtering utilizes only some criteria to choose a feature, and sometimes
it can help us to monitor the feature construction process. It is easy to conduct but achieves
poor performance. Wrapping performs well by directly applying the model’s results as an objective
function. Thus, it can treat an individually trained model as a newly constructed feature. However,
a considerable amount of computational resources and time is required. Embedding is a method
that uses generalized features and a pruning technique to select or combine features, which serves
as a middle choice between ﬁltering and wrapping.

Statistical learning is a common factor construction tool that is used for ﬁnancial factor con-
struction, which is similar to embedding in feature construction. Harvey et al. (2016) proposed a
new method for estimating latent asset pricing factors that ﬁt the time-series and cross-section
returns by using Principal Component Analysis (PCA), as proposed by Wold (1987). Feng et al.
(2020) proposed model selection procedures to select among many asset pricing factors by using
cross-sectional LASSO regression, which was introduced by Tibshirani (1996). However, this statis-
tical learning mainly focuses on factor selection from existing factor pools but does not construct
brand-new factors.

1.2. Literature Review

The most well-known and frequently employed method for automatic brand-new factor construction
is Genetic Programming (GP), which is a type of wrapping method in feature construction that uses
reverse polish expressions to construct new factors by an evolutionary process. However, diﬀerent
domains require diﬀerent objective functions, and the input data structure can diﬀer. Thus, it
is very important to design this task within a speciﬁc domain. This method has been proved to
work well in many industries, such as object detection, according to Lillywhite (2013), and in the
educational industry, according to Romero (2004). However, its drawback is that the constructed
formulas are very similar and have highly correlated outputs. The ﬁnancial factor construction task
uses GP to conduct the evolutionary process of formulaic factors, according to Thomas (1999) and
Ravisankar (2011). WorldQuant published 101 formulaic alpha factors, which are also constructed
by using this method, Kakushadze (2016). However, it only constructs a large number of similar
factors, which do not contain much new information.

With the development of deep learning, more and more researchers have begun to use neural
networks to extract information from raw data, and then, they add a fully connected layer to
reshape the output. Some researchers have designed some speciﬁc loss functions according to speciﬁc
domain knowledge. Examples are Zhang (2019) and Zhang (2020), who designed a vector-error
function and derived an implicit-dynamic equation with a time-varying parameter in a neural
network structure to solve the disturbed time-varying inversion problem. Yang Zhong deployed
the Convolution Neural Network (CNN) to construct facial descriptors, and this method achieves

2

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

better accuracy than its benchmark. Many researchers directly treat a trained neural network as a
newly constructed feature. There are many representative studies in this ﬁeld. For example, Shan
(2017) conducted experiments on this task and deployed a deeper and wider CNN. Hidasi (2016)
used a Recurrent Neural Network (RNN) to pre-locate the factor-rich region and construct more
puriﬁed features. In a text classiﬁcation task, Botsis (2011) leveraged the RNN to build a hierarchy
classiﬁer for text data, in which each classiﬁer represents a part of the text. Lai (2015) proposed a
network structure that uses both an RNN and a CNN to extract text information. The produced
features contain more information than the previous work. With the help of a neural network’s
strong ﬁtting ability, highly informative factors can be produced by tailoring the network structure
for diﬀerent industries. There is no conclusion on whether using a neural network to represent a new
feature can achieve better prediction performance. However, using an individual neural network
to represent a new feature can add more interpretability, which is highly valued in the ﬁnance,
medical, and educational sectors.

Some researchers have begun to use neural networks to give an embedding representation of a
ﬁnancial time series. More speciﬁcally, Feng (2019) leveraged LSTM to embed various stock time
series and then used adversarial training to make a binary classiﬁcation on the stock’s return.
Sidra (2019) adopted a well-designed LSTM to extract factors from unstructured news data and,
then, formed a continuous embedding. The experiment result shows that these unstructured data
can provide much information and they are very helpful for event-driven trading. Most of these
research studies focus on predicting single-stock returns by neural networks using their own time
series. However, on ﬁnancial factor construction tasks, we do not ﬁnd literature that provides a full
solution for automatic factor construction by using neutral networks.

1.3. Contributions

In this paper, a novel network structure called Neural Network-based Automatic Factor Con-
struction (NNAFC) is proposed, which can use deep neural networks to automatically construct
ﬁnancial factors. Diﬀerent from previous research, we make three contributions in this paper for
ﬁnancial factor construction. (1) We create a novel loss function that diﬀers from the accuracy of
the stock’s return by using the Rank Information Coeﬃcient (Rank IC) between stock factor values
and stock returns. (2) We deﬁne a new derivable correlation formula for the Rank IC calculation to
make back preparation training of neural networks possible. (3) We adopt pre-training and model
pruning to add up enough diversity into the constructed factors, which helps to produce more
diversiﬁed factors. NNAFC has outperformed many benchmarks in all aspects. Furthermore, dif-
ferent pre-training networks for prior knowledge are equipped to further improve their performance
in real-world situations.

2. Deﬁnition of the Factor Construction Task

2.1. Deﬁnition of an alpha factor

The alpha factor is a raw forecasting vector that has a certain correlation with the ﬁnancial assets’
future returns. Richard (1999) pointed out that the informed expected return E(r|g) can be deﬁned
as the expected return conditional on an alpha factor g at time T . In formula (1) and (2), r
represents the asset return in the future, E(r) represents the consensus return, E(g) represents
the expected forecast, Cov(r, g) means the covariance between r and g, and V ar(g) represents the
variance of g.

E(r|g) = E(r) +

Cov(r, g)
V ar(g)

× (g − E(g))

(1)

3

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

E(r|g) − E(r) =

Cov(r, g)
Std(r)Std(g)

× Std(r) ×

g − E(g)
Std(g)

(2)

The excess return of the informed expected return over the consensus return is the so-called
alpha of the active investment. The goal of the active investment is to maximize the alpha, which
can be rewritten in formula (3).

Alpha = IC × σ × Zscore(g)

(3)

where IC = correlation(r, g) = Cov(r,g)

Std(r)Std(g) , Zscore(g) = g−E(g)
Std(g) , and σ = Std(r). The Information
Coeﬃcient (IC) is deﬁned as the correlation between the future asset returns and the current value
of the alpha factor. Since the standard deviation of the future asset returns σ cannot be impacted
by our selection of the alpha factors and the Zscore(g) is more of a trade-oﬀ between the excess
return and the investment’s capacity, maximizing the alpha is type of equivalent to maximizing
the IC.

2.2. Deﬁnition of the factor construction process

In cases where the alpha factor consists of many sub factors, Qian (2007) pointed out that maxi-
mizing IC is aimed at optimizing the following objective function in formula (4).

max
v

vT (cid:102)IC − 0.5λvT Σ

(cid:102)ICv

(4)

where v is the weight vector of the sub-factors in the alpha factor, (cid:102)IC is a value vector of the
(cid:102)IC is the IC-value co-variance matrix of the sub-factors, and λ is a penalty coeﬃcient

sub-factors, Σ
for the volatility of sub-factor IC. The optimal weight can be written as

The optimal IC can be written as

v∗ =

(cid:102)IC
λΣ

(cid:102)IC

IC∗ = (

T
)

(cid:102)IC

(cid:102)IC
λΣ

(cid:102)IC

(5)

(6)

The optimal IC shows that the goal of the factor construction problem is to construct new sub
factors that have a lower correlation with existing factors (smaller elements in Σ
(cid:102)IC vectors) but a
higher correlation with the ﬁnancial assets’ future returns (larger elements in (cid:102)IC matrix). Thus,
the factor construction problem can be deﬁned as the following optimization problem in formula
(7) below. Note that λ in formula (4) can be ignored because it is a ﬁxed number.

max
F C

(

(cid:94)ICcombine
Σ (cid:94)ICcombine

T

)

(cid:94)ICcombine

(7)

4

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

where (cid:94)ICcombine is the IC vector for a set of sub-factors called F actorcombine, which consists of
an existing factor set F actorold and a newly constructed factor set F actorcombine. Here, Σ (cid:94)ICcombine
is the IC co-variance matrix of this set of sub-factors.

Factor construction (FC) is deﬁned as a process for constructing new factors from all ﬁnancial
data sets F D and an existing factor set F actornew in a set of mathematical presentations MP to
maximize the objective function in formula (7). The new factor set can be written as

F actorcombine = F C(F D, F actorold, M P )

(8)

Based on this deﬁnition, manual factor construction stands for a factor construction process to
search potential factors in closed-form mathematical presentations by leveraging human experts’
knowledge in economics and behavioural ﬁnance research. The whole process is not completely
automatic and quantitative. GP stands for a factor construction process to search for potential
factors in closed-form mathematical presentations created from a set of mathematical operations
based on certain genetic programming algorithms. The whole process becomes completely auto-
matic and quantitative, and it imitates how a human constructs a mathematical formula from a set
of operations. However, the set of mathematical presentations is still limited by all mathematical
presentations that can be constructed by the set of mathematical operations. If the new factor must
be represented by a higher order of non-linearity or fractal factions, it is diﬃcult to accomplish by
GP. NNAFC stands for a factor construction process to search for potential factors in all parame-
ters of a neural network structure based on a network optimization algorithm to maximize IC and
reduce the correlation with existing factors. The whole process becomes completely automatic and
quantitative and can cover all mathematical presentations. According to Leshno (1993), the deep
neural network is proved to be able to represent all types of mathematical formulas if the deep
neural network has suﬃcient depth and width. Thus, the MP of NNAFC is a super set of manual
factor construction and GP. In the following sections, we discuss the details of GP and NNAFC.

3. Framework Introduction

3.1. Baseline method

Before we introduce our NNAFC, we must introduce its baseline method of Genetic Programming
(GP) as a benchmark. GP leverages reverse polish expression to represent factors. Each tree is an
explicit formula. We use GP to conduct its evolutionary process, such as adding, deleting, merging,
evolving and selecting.

In each round, the samples are selected by a pre-deﬁned objective function. This objective func-
tion is the Spearman Coeﬃcient between the factor value and factor return, which is exactly the
same as the objective function in NNAFC. GP adds diversity into the constructed factors by
changing certain parts of a reverse polish expression. In an explicit formula, a small change in the
operators can make the meaning of the factor totally diﬀerent. Thus, in GP, the parent and child
samples can have nothing in common. As a result, the child sample might not successfully inherent
some good characteristics from its parent samples, such as a good ability to predict a future stock
return. Thus, genetic programming is not a good method for constructing new factors, due to its
ineﬃcient evolutionary process on this task.

GP adds diversity into the constructed factors by changing certain parts of a reverse polish
expression. For example, we use a binary tree to present initial factor 1 shown in formula (9) and
then conduct an evolution process via GP. In the evolutionary process, GP can make a small change
in factor 1. Afterward, we can obtain a newly constructed factor 2, shown in formula (10).

5

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

F actor1 =

high price − low price
volume

F actor2 =

high price − volume
volume

(9)

(10)

As can be seen from factor 1, it tells the relative strength of the price compared with the volume,
which can be explained from the perspective of economic principles. However, factor 2 is totally
diﬀerent from factor 1, and it is very diﬃcult to explain. As a result, the parent factor and child
factor have little in common. The parent factor has a high IC, but the child factor might not
successfully inherent the good characteristics from its parent factor. As a result, we think that GP
is not a good method for constructing new factors, due to its ineﬃcient evolutionary process on
this task. The direction of its evolution and good characteristics, such as high IC, are not seen to
be converging together quickly. Thus, GP on this task does not conduct an evolutionary process. It
is more like a searching process, which aims at ﬁnding good solutions for an optimization problem,
as shown in formula (7). GP’s searching process is shown in Figure 1.

Figure 1. This is the GP’s evolution process. It uses reverse polish expression to represent ﬁnancial factors. We can
merge two formulas together to get a new formula or change a single operator in the formula to make it diﬀerent.

3.2. Neural network-based automatic factor construction

To overcome these drawbacks of GP, as mentioned in section 3.1, this paper proposes the Neural
Network-based Automatic Factor Construction (NNAFC), a process based on a tailored neural net-
work structure that can automatically construct ﬁnancial factors toward good characteristics, such
as high IC. It also diversiﬁes the constructed factors based on prior knowledge, as represented by
the variety of neural network structures. Thus, it can ﬁnd good solutions eﬃciently for optimization
problems, as shown in formula (7). The major characters of this novel network structure include
the following: 1. NNAFC uses Spearman Correlation to serve as a loss function, which mimics
common human practices of quantitative investment. 2. A meaningful derivable kernel function is
proposed to replace the un-derivable operator rank(). 3. The network is pre-trained with many

6

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

ﬁnancial descriptors called Prior Knowledge (PK). They serve as initial seeds, which can improve
the diversity of the newly constructed factors.

In NNAFC, we pre-train the neural network with an existing ﬁnancial factor each time, which
performs like a seed that can provide diversity to the system. According to the neural network’s uni-
versal approximation theorem, in Leshno (1993), technically, the neural network can approximate
with arbitrary precision any measurable function from one ﬁnite dimensional space to another. By
pre-training, the neural network can inherit good characteristics from its parent factor. With the
guidance of the objective function, the new factor should have better IC than the old factor. Thus,
we believe that the neural network can provide us with a more eﬃcient and intuitive evolution
process than GP on this task. The NNAFC framework is shown in Figure 2.

Figure 2. Neural Network-based Automatic Factor Construction’s framework

We design the batch sampling rules in which all of the stocks in the same trading day are
grouped into one batch, because NNAFC focuses on predicting the relative return of the stocks
on the same trading day, rather than its absolute return. More speciﬁcally, for a daily multi-factor
trading strategy, we care only about the relative cross-sectional performance. Additionally, we long
the relatively strong stocks and short the relatively weak stocks in each trading day to make a
proﬁt. The input tensor’s shape is (n, 5, m), because there are n stocks in each trading day, and 5
types of time series, which are the open price, high price, low price, close price and volume. The
input length of each time series is m. In Figure 2, the output tensor of this network is called a
feature list and consists of factor values with the shape (n, 1). Here, we assume that all of the
selected networks in Figure 2 are Multi-layer Perceptrons (MLPs), for which it is easy to give a
general mathematical description. In a later section, the experimental results are based on more
complicated and diversiﬁed networks. Wi is the kernel matrix in the i th layer, bi is the bias matrix
in the i th layer, and ai is the activation function in the i th layer. The initial layer starts from
1, and there are p layers in total. The factor return tensor, with the shape (n, 1), consists of the
returns with which we can earn from the assets if we hold the asset for a period of time. The length
of the holding time is ∆t, and the current state is t. According to this principle, the factor value
x and factor return y can be written as in formula (11) and formula (12).

x = lp = ap(Wp

T lp−1 + bp),

l1 = a1(W1

T Input + b1).

(11)

7

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

y = F actor Return =

close pricet+∆t
close pricet

− 1

(12)

The Spearman Correlation uses the operator rank() to remove some of the anomalies in ﬁnancial
time series, and rank() is not derivable, which is not acceptable for back-propagation in the training
of neural networks. Thus, we need a derivable kernel function g(x) to replace rank().

g(x) =

1

1 + exp(−p ∗

x−¯x
2∗std(x) )

(13)

Formula (13) standardizes x into a normal distribution that is zero-centralized. Next, when the
hyper-parameter p equals 1.83, it ensures that 2.5%-97.5% of the data will lie in the range of
[mean − 2std, mean + 2std]. For example, one outlier xi = ¯x + 2std(x), and g(xi)−g(¯x)
¯x , and
thus, the result is std ≤ 0.362¯x. This ﬁnding means that if one distribution’s standard deviation
is large, and it is larger than 0.362¯x, then g(x) can shorten the distance between the outliers and
the central point. If the distribution’s standard deviation is very small, then g(x) will make it
worse. However, even in this case, we can ensure that 95% of the points are in the range [mean −
2std, mean+2std], which is acceptable. To show how this kernel function works, we also performed a
sensitivity analysis of the hyper-parameters. Diﬀerent hyper-parameter sets can represent diﬀerent
situations. In some stock indices, stocks’ attributes are very diﬀerent. However, in others, the
diﬀerence is very low. The formula focuses on cutting out the extreme part.

g(¯x) ≤ xi−¯x

Figure 3. We simulate 4 Gaussian distributions that have diﬀerent mean and deviation.

8

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Shown in Figure 3, if the distribution’s deviation is very small, there is no need to use g(x), but
using g(x) will not make things worse. If the distribution’s deviation is very large, then g(x) can
perform almost the same function as the operator rank(), and they can bring positive impacts.

With the kernel function g(x), we deﬁne the ﬁnal objective function in formula (14), where E(x)
represents the expected value of x, and ¯x represents the average value of x. In each batch, we
calculate the average correlation in q trading days, which can make the optimization process more
robust.

IC(x, y) =

E(g(x) − ¯g(x), g(y) − ¯g(y))
E(g(x) − ¯g(x))E(g(y) − ¯g(y)))

,

Loss = −

1
q

q
(cid:88)

i=1

IC(xi, yi).

(14)

3.3. Putting prior knowledge into pre-training networks

We can use pre-training to initialize a neural network. Combining pre-training and pruning on input
data can improve the signal’s diversity because they introduce prior knowledge into the network.
Usually, the prior knowledge is represented by certain ﬁnancial indicators in mathematical function
formats. Model stealing is a process of ﬁtting a mathematical function based on a given input x
and output y. However, its technique is not always helpful for learning a distribution without
tailoring the network structure. If we have a ﬁxed network structure but have no idea about the
target distribution, techniques such as removing the outliers (for continuous prior knowledge) and
using high-temperature in knowledge distillation (for discrete prior knowledge) can improve the
performance of this process. In this paper, we use some classical technical indicators, such as
MA, EMA, MACD, RSI, and BOLL, and some typical ﬁnancial descriptors are selected as prior
knowledge for the pre-training, as shown in Table 1.

Table 1. Here are the formula of some classical technical indicators and ﬁnancial descriptors. They serve as prior
knowledge for ADNN. Close refers to stock close price, volume refers to stock volume, and AdjClose refers to
adjusted close price.

Technical Indicator Mathematical Expression
k=0 xn−k
(cid:80)∞

(cid:80)N

MA
EMA
MACD

PVT

TOP10

DC

BOLL

M A10top10%

k=0( N −1

N +1 )kxn−k

M AN (xn) = 1
N
EM AN (xn) = 2
N +1
M ACD = EM Am(i) − EM An(i)
P V T (i) = P V T (i − 1) + volume(i)∗
(close(i) − close(i − 1))/close(i − 1)
M A10 = M A (Close, 10)
T OP 10 = M A10
− 1
H = M A (High × AdjClose/Close, n)
L = M A (Low × AdjClose/Close, n)
M = 1
2 (H + L)
DC = AdjClose/M
StdV = M Stdv (Close, n)
M ean = M A (Close, n)
LB = M ean − Stdv
BBL = LB
Close
M Stdvn,t = Stdv (Closet−n:t)

We use f (x) = a(wT x + b) to embed the input data. The data is embedded by MLP, composed
of several fully connected layers with tanh and relu activation functions. The number of neurons in

9

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

each layer should be decided by the length of the input time series, where w is the kernel matrix,
b is the bias matrix, and a is the activation function. Then, we use this embedded layer to mimic
the prior knowledge. In this part, we use the min(mean squared error) as the objective function,
which is shown in formula (15).

min
a,b,w

1
N

N
(cid:88)

(yi − f (xi))2

i=1

(15)

In this paper, we have proposed many network structures. Diﬀerent network structures require
diﬀerent input data structures. Some of them require time series data, but others require cross-
sectional data on many stocks in the same trading day. Diﬀerent input structures must be used
to represent the data. However, the general rule for all of the network structures is that in each
back-propagation, the input data should have all the stocks in the same trading day. To better
learn the prior knowledge from technical indicators, we propose two types of network structure to
learn the prior knowledge. Some technical indicators leverage only the operations on time series
i=1 closet−i − 1
data, such as SM A = 1
i=1 closet−i. For this type of technical indicator, MLP
n
m
can eﬃciently learn its prior knowledge. Its network structure is shown in Figure 4.

(cid:80)m

(cid:80)n

Figure 4. Pre-training Structure for Time Series Factors

For other technical indicators, which require an operation on the cross-sectional data, the common
MLP structure is not helpful. For example, some technical indicators need the data from both stock
1 and stock 2. Stock 1 and stock 2 are independent samples, which means that they cannot share
the data with each other. Before the calculation, we do not know what parts of the data in stock
1 are needed by stock 2. To automatically and freely learn this relationship, we propose a new
network structure.

Figure 5. Pre-training Structure factors with cross-sectional data

10

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Shown in Figure 5, we leverage convolution kernels to calculate the data that belongs to diﬀerent
stocks. Since one kernel’s size is truly small, it cannot calculate the samples at a far distance. How-
ever, if we repeat the convolution layer several times, all of the stock’s data can have the opportunity
to obtain the relevant data. Actually, this structure can also be used to learn time series technical
indicators. In addition, this structure is not tailored for only time series. The previous structure
can better mimic the performance and costs less training time. In the following experiment, these
two types of network structures are both called Fully Connected Networks (FCNs).

yi

(cid:80)N

i=1 abs( yi−f (xi)

For a deep neural network, almost all of the technical factors can be easily learned. Here, MSE
or MAE cannot represent the real pre-training performance, because all of the factor values are
truly small, which makes all MSE values very small. To have a better measure of the performance,
1
) is used to measure its error rates. The error rates of SMA, ROC, RSI, BOLL,
N
FORCE, and other typical technical indicators are selected as prior knowledge for the pre-training.
Some indicators with diﬀerent parameters, such as RSI(6,12) and RSI(7,21), will be regarded as
diﬀerent prior knowledge because they can provide more diversity in the factor construction. We
have performed experiments for a long period of time, to test whether our networks have successfully
learned the prior knowledge. The training period is from Jane 2009 to March 2018, and the testing
period is from May 2018 to March 2020. As mentioned above, the common MLP structure cannot
address cross-sectional factors. Thus, in the following experiments, we run the pre-training process
on the two structures and choose the better case. As mentioned above, although low MSE is not
suﬃcient to prove that we have successfully mimicked the prior knowledge, it can show that the
in-sample training loss and out-of-sample testing loss are small and close to each other, which is
helpful in illustrating the consistent nature of the algorithm’s out-of-sample performance.

Table 2. Pre-training performance, measured with the MSE of the neural network output and technical indicator
Train
Factor
MA
0.00240
EMA 0.00371
MACD 0.00685
0.16262
0.00291
0.04314
0.05336
0.00357

Test
0.00324
0.00635
0.01392
0.16525
0.00286
0.05257
0.05582
0.00352

RSI
Top10
DC
BOLL
PVT

To better prove that we have successfully learned the prior knowledge, we use these factors
to make binary buy/sell decisions. If the learned trading decisions also match the real trading
decisions given by prior knowledge, then we can strongly prove that it learned the prior knowledge.
The accuracy of the binary classiﬁcation is shown in Table 3.

Test

Table 3. Pre-training performance, measured with accuracy for binary classiﬁcation.
Train
Factor
MA
93.92% 93.48%
EMA 91.66% 87.81%
MACD 92.82% 93.77%
95.28% 89.11%
95.09% 91.25%
98.05% 91.47%
94.53% 86.75%
90.99% 84.75%

RSI
Top10
DC
BOLL
PVT

As shown in Table 2 and Table 3, the MLP can learn prior knowledge with approximately 90%
out-of-sample accuracy. We do not require it to learn 100% of the knowledge because 90% of

11

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

the knowledge is suﬃcient to serve as a source of diversity. However, why is pre-training with
prior knowledge needed? Zhang (2016) pointed out that according to the concept of multi-task
learning, pre-training can permanently keep some parts of the domain knowledge in the network.
The diﬀerent domain knowledge can increase the diversity. Pruning can ﬁlter out noise from the
neural network and retain the prior knowledge that we need. In addition, the pruning rate should
be controlled. A larger pruning rate will bring too much diﬃculty for pre-training to converge to
the ﬁnal optimization direction, but a smaller pruning rate could lose the diversity. Frankle and
Carbin (2018) pointed out that the ideal pruning rate should be approximately 0.2-0.5, and the
mask matrix is composed of only 0 and 1. All of the settings are the same as those in the paper
of Ding (2015). After embedding the data as f (x) in formula (16), we obtain its parameter matrix
W . Then, we create a mask matrix to prune the parameters. For example, xij in the parameter
matrix is relatively small, which means that some of the input data is useless. Then, Mij=0 is set to
permanently mask this value. If the xij is not useless, then we set Mij=1. This method can retain
the diversity in the network. Furthermore, it can focus on improving the current situation, without
heading into an unknown local minimum. The pruning process by using matrix M is shown in
formula (16):

f (x) = (W · M )Tx + b

(16)

After pre-training and pruning the network, we use the objective function shown in formula (14)
for the NNAFC’s training. We simply reshape the input data into a graph, and then, we use the
Saliency Map proposed by Simonyan (2014) to look at how the raw data contributes to the ﬁnal
constructed factor. The Saliency Map’s calculation method is shown in formula (17). If we assume
that the input pixel is I, then the Saliency Value can be approximated by the First Order Taylor
Expansion, S(Ip) ≈ wT Ip + b. After calculating the derivative on the variable I, we can obtain the
contribution rate of pixel Ip.

WIp =

∂S(Ip)
∂Ip

(17)

Figure 6. Factor construction process of one stock series for illustrating the evolution process of NNAFC. However,
in our factor construction process, we do use cross-sectional data as inputs.

The training process is illustrated in Figure 6. The y-axis is [open price, high price, low price,
close price, volume]. The x-axis is the time step of an input time series. The red colour means a

12

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

positive contribution, and the blue means a negative contribution. The colour’s depth has a positive
correlation with the contribution rate. From Figure 6, we can obtain further understanding of the
construction process. First, there is a reverse technical factor, according to the raw data’s negative
contributions at the latest time. Second, we can know that this factor is mainly constructed by the
price information, but not volume information. Third, the components of this factor are discrete.
The factors constructed by human experts are usually continuous. This diﬀerence makes the factors
constructed by NNAFC uncorrelated to manually constructed factors by human experts. Because
the trading opportunities are limited, if we all use similar trading signals, the trading will be
more and more crowded. Prior knowledge can be viewed as seeds for new factors, which can bring
diversiﬁed and robust initialization. Starting from this prior knowledge would be a very safe choice.
Furthermore, as mentioned in section 3.1, one of GP’s shortcomings is that the child factor might
not inherit the good characteristics of its parent factor. However, for NNAFC, all of the updates in
the parameters are dominated by back-propagation. The mechanism of back-propagation is gradient
descent, which ensures that every optimization step is continuous. These continuous characteristics
can ensure that the parent factor and child factor have a lot in common. Thus, GP is more similar
to an advanced searching algorithm, but NNAFC is more like an advanced evolutionary algorithm.
We also conduct experiments on diﬀerent feature extractors (in the computer science ﬁeld, diﬀer-
ent feature extractors mean diﬀerent neural network structures) to have a better understanding of
the pre-training on diﬀerent ﬁnancial time series. There are two motivations for conducting experi-
ments on diﬀerent feature extractors. First, diﬀerent feature extractors require diﬀerent input data
structures. We explored many diﬀerent methods for organizing the input data. We conducted exper-
iments on many basic networks and SOTA (state-of-the-art) networks, including Fully connected
network (FCN), Le-net, Resnet-50, Long Short Term Memory Network (LSTM), Transformer (be-
cause Transformer is too large, we used only the self-attention encoder) and Temporal Convolution
Network (TCN). FCN has 3-5 hidden layers, with 64-128 neutrals in each layer. We used the tanh
and relu function to serve as the activation function, and the output layer should not be activated.
For Le-net, Resnet-50, Transformer, and TCN, we did not change their structures, but we employed
only a smaller stride because the input data has a low dimension compared with the real picture.
The second motivation is that diﬀerent feature extractors have their own advantages and dis-
advantages. Some of them aim at extracting temporal information, but the others aim at spatial
information. Some of them are designed for a long term series, but others are designed for quick
training. We believe that they can make our factor pool even more diversiﬁed.

3.4. Summarizing the pros and cons

Until now, we have introduced all of the network settings of NNAFC, and we have shown the
characteristics of NNAFC compared with GP. Here, we summarize their pros and cons. First,
GP conducts a very low eﬃciency evolution process, and its performance is more similar to a
searching process. As a result, the factors constructed from GP are very similar. However, NNAFC
can conduct an eﬃcient evolutionary process, and it fully inherits the diversity from the prior
knowledge. Thus, the factors constructed by NNAFC are more diversiﬁed.

Second, the factors constructed by GP are formulaic factors, which is easier to explain. However,
for NNAFC, although we attempt to explain it in section 3.4, its explainable ability is still limited.
Third, for some simple factors, GP is truly easier to explain. However, the simple factors cannot
provide much information. In other words, there is no room for us to improve both the quality
and quantity together when the factors are very simple. Only when the formula is very complex,
we have the possibility to reproduce many diﬀerent and useful factors based on it. However, if the
formula is very complex, we cannot understand and explain the factors constructed by GP, either.
In this situation, both methods cannot be easily explained. The non-formulaic factors constructed
by NNAFC could be better. At least, this approach has the ability to retain more information. As
mentioned in section 3.1, a deep neural network has the ability to represent any formulas.

13

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

4. Numerical Experiments

4.1. Experiment setting

In the experiment part, we performed two types of experiment. First, we conducted experiments
to compare the performance of NNAFC and GP from the perspective of IC and diversity. Second,
we conducted experiments to measure their contribution in real trading strategies. We compared
a strategy that uses only the prior knowledge, a strategy that uses both prior knowledge and
the factors constructed by GP, and a strategy that uses both prior knowledge and the factors
constructed by NNAFC. This comparison can show how much contribution our method can bring
to the current multi-factor strategy.

We used daily trading data in the Chinese A-share stock market (in the following, we call it the
A-share market data), including the daily open price, high price, low price, close price and trading
volume over the past 30 trading days. The raw data is standardized by using its time-series mean
and standard deviation. Both the mean and standard deviation are calculated from the training
set. We attempted to use these inputs to predict the stock return in the next 5 trading days, which
stands for the weekly frequency and is a common period used in short-term technical indicators.
In the A-share market, we cannot short single stocks easily because of the current limitations in
stock lending, but we can short some important index futures.

We performed some research on selecting some reasonable hyper-parameters. During the model
training process, we calculated the average value of IC over 20 randomly selected trading days. For
each experiment, 250 trading days served as the training set (no technical factor can work well for
a long period of time), and the following 30 trading days served as the validation set, while the
following 90 trading days served as the testing set. These hyper-parameters are common settings in
industrial practices. To make a fair comparison, the same setting is deployed for the GP algorithm.
In this paper, we analyse the construed factors’ performance from diﬀerent perspectives. Based
on the deﬁnition of alpha factors, we use the information coeﬃcient (IC), shown in formula (6),
to measure how much information is carried by a factor. For diversity, each factor value should be
normalized at ﬁrst. The softmax function in formula (18) can eliminate the eﬀect from the scale
without losing the factors’ rank information.

Sof tmax(xi) =

exp(xi)
(cid:80) exp(xi)

(18)

Then, the cross-entropy is used to measure the distance between two diﬀerent factors’ distribu-

tions on the same trading day.

Distance(f1, f2) =

(cid:88)

sof tmax(f1)log

1
sof tmax(f2)

(19)

In formula (19), f1 and f2 refer to diﬀerent factors’ distributions in the same trading day. K-means
is used to cluster the distance matrix of the relative distance between two factors. The average
distance between each cluster centre refers to the diversity of factors on this trading day. We have
shown in formula (6) that a higher optimal IC for an investment strategy is related to a higher sub
IC and a larger factor diversity. In addition to measuring the IC and diversity, the performance
of a trading strategy based on the constructed factors is also measured, such as absolute return,
max-drawdown, and sharp-ratio.

To put all in a nutshell, we summarize the construction procedure mentioned in section 3. And
then, we illustrate the entire process of constructing a new factor via NNAFC. First, we pre-train
a neural network with a technical indicator MA as prior knowledge. The pre-training performance
has been well illustrated in Tables 2 and 3, and we can pre-train this factor with 93.92% accuracy.

14

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Second, we train the neural network by maximizing IC deﬁned in formula (14). During the back-
propagation, the neural network has been changed, and the output factor value changes also. This
process has been shown in Figure 6. Because we use the IC to serve as an objective function and
the mechanism of back-propagation is gradient descent, the newly constructed factors will have
higher IC than the initialized factor. We also use the following experiment results to prove this
assumption. At last, the trained neural network is a newly constructed factor. As mentioned in
the section 3.4, it has pros and cons compared with the benchmark. We deﬁnitely cannot know its
closed-form formula; however, we can know what raw data contributes to it and how diﬀerent it is
compared with the initialized factor, shown in Figure 6.

4.2. Beating the state-of-the-art technique

NNAFC can be equipped with diﬀerent neural networks. In this test case, NNAFC is equipped
with 4 layers in a fully connected neural network (FCN). The experiment shows that NNAFC can
beat the GP. We propose three schemes to help illustrate NNAFC’s contribution and to show how
it beat the GP. The only GP means only using GP, Only NNAFC means only using NNAFC to
construct factors, and GP and NNAFC means using the GP’s value to initialize NNAFC and then
construct factors. The out-of-sample results of the experiments are summarized in Table 4.

Table 4. The performance of diﬀerent schemes

Object
Only GP
GP and NNAFC
Only NNAFC

Information Coeﬃcient Diversity

0.094
0.122
0.107

17.21
25.44
21.65

Only NNAFC is better than Only GP, which means that NNAFC outperforms GP on this task.
We also ﬁnd that GP and NNAFC is the best, which means that our method can even improve
on the performance of GP. However, in real practice, we should leverage the constructed factors to
form a multi-factor strategy and compare its performance with GP. The speciﬁc strategy setting
is the same as in section 3.4, and we have repeated this experiment for diﬀerent periods of time.
The long-term back-testing result is shown in Table 5.

Table 5. Strategy’s absolute return for each scheme.

Time
Train:2015.01-2015.12
Test: 2016.02-2016.03
Train:2016.01-2016.12
Test: 2017.02-2017.03
Train:2017.01-2017.12
Test: 2018.02-2018.03
Train:2018.01-2018.12
Test: 2019.02-2019.03

Only GP GP and NNAFC Only NNAFC

ZZ500

+2.59%

+5.74%

+4.52%

+1.67%

+5.40%

+10.26%

+8.33%

+2.53%

-5.27%

-4.95%

-4.16%

-6.98%

+13.00%

+15.62%

+15.41%

+13.75%

As shown in Table 5, the Only NNAFC always has better performance than the Only GP during
the long-term backtest. The results show that our method has also beaten the SOTA in real practice.
However, will there be more powerful feature extractors to discover knowledge from ﬁnancial time
series? And what shall be the suitable input data structure for diﬀerent ﬁnancial time series? We
attempt to answer these questions in the next few sections.

15

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

4.3. Comparing diﬀerent feature extractors

For the hardware equipment, we used 20 g GPU (NVIDIA 1080Ti) and 786 g CPU (Intel Xeon
E5-2680 v2, 10 cores). Based on this setting, we show the amount of time that we need to construct
50 factors. Moreover, the time to restore 50 trained networks and obtain their factor values will be
substantially faster than traditional factors. Because some of the traditional factors are constructed
with complicated explicit formulas, these formulas are not suitable for matrix computing. Using
neural networks to represent factors in matrix computing, which have faster testing speeds. For
the overhead of this framework, the time complexity is acceptable. With 20G GPU, it costs only
several hours to construct 50 factors. However, the GPU memory is a large problem. In each back-
propagation, we need to store more than 3000 stocks’ time series, each stock has at least 5 time
series, and each time series contains more than 100 points. Thus, this framework requires at least
20G GPU in resources for the Chinese stock market’s backtest, and more GPU resources will be
better.

Type
Baseline
Vanilla

Table 6. The higher the information coeﬃcient (IC) and diversity are, the better is their performance. Normally, a
good factor’s long-term IC should be higher than 0.05.
IC
0.072
0.124
0.123
0.108
0.170
0.105
0.111

Network
GP
FCN
Le-net
Resnet-50
LSTM
TCN
Transformer

Time
0.215 hours
0.785 hours
1.365 hours
3.450 hours
1.300 hours
2.725 hours
4.151 hours

Diversity
17.532
22.151
20.194
21.403
24.469
21.139
25.257

Temporal

Spatial

Shown in Table 6, T ype means the category of neural networks. For example, both Le-net and
Resnet are designed for extracting spatial information. Allen (1999) pointed out that all neural
networks can produce more diversiﬁed factors than using GP. For Le-net and Resnet, they do not
provide us with more informative factors, but for more diversiﬁed factors, there is LeCun (1999)
and He (2016). Temporal extractors are especially better at producing diversiﬁed factors, such as
LSTM and Transformer, Hochreiter (1997) and Vaswani (2017). For TCN, Dea (2018) proves its
ability to capture the temporal rules buried in data. However, they have enormous diﬀerences. TCN
relies on the CNN, but LSTM and Transformer still contain an RNN. Normally, the transformer
uses an RNN to embed the input data. The existence of a recurrent neural network structure
can contribute to more diversity. All of the neural networks mentioned above can produce more
informative and diversiﬁed factors than GP. For the same types of networks, the results suggest
that the simple network structure performs relatively better than the sophisticated networks.

4.4. Real-world use case test

In the real-world use case test, we use the factor constructed via NNAFC. At the same time, we
also use the factors constructed by human experts. This approach should give a fair setting for a
comparison, and we want to see whether the NNAFC can bring marginal beneﬁts for the traditional
and mature investment system. Table 7 shows the back-testing results of our strategy.

In the training set, the stocks whose returns rank in the top 30% in each trading day are labelled
as 1, and the stocks whose return ranked in the last 30% of each trading day are labelled as 0.
We abandon the remaining stocks in the training set, according to Fama (1993). After training
these factors with Chen (2015)’s XGBoost using binary logistics mode, we can obtain a trained
model that can make binary predictions. The prediction result reﬂects the odds as to whether the
stock’s return will be larger than 0 in the following several trading days. It deﬁnes the 50 factors
constructed by human experts as PK 50, and the 50 factors constructed by NNAFC as New 50. In

16

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

another case, in the training set, we use XGBoost to train 100 factors, which are composed of PK
50 and New 50. Then, we select 50 factors whose factor weights are relatively large among these
100 factors. The weights can be calculated by the frequency of using the factor. For example, in a
tree-based algorithm, the important features can obtain more opportunity to serve as a splitting
feature. By using this approach, we can use XGBoost to select 50 important factors, which consist
of both traditional factors and neural network-based factors. We want to stress the point again
that the feature selection process occurred in the training set. We do not conduct feature selection
in the validation set or testing set. Furthermore, during each backtesting, this factor selection
process should only be conducted once. Last, we deﬁne the selected 50 factors as Combine 50,
which represents the practical use of our factors.

Type

Baseline

Target
ZZ500
HS300
PK

Table 7. The investment target is all Chinese A-share stocks, except for the stocks that cannot be traded during
this period of time. The strategy’s commission fee is 0.3%.
Revenue MD
Group
SR
19.60% 13,50% 1.982
Stock Index
18.60% 20.30% 1.606
Stock Index
24.70% 18.90% 2.314
PK 50
17.60% 25.30% 1.435
GP 50
25.40% 14.80% 2.672
Combine 50
29.60% 15.70% 3.167
Combine 50
27.50% 16.40% 2.921
Combine 50
29.30% 17.20% 2.787
Combine 50
29.90% 15.00% 3.289
Combine 50
26.90% 16.80% 2.729
Combine 50
27.20% 15.10% 2.806
Transformer Combine 50

FCN
Le-net
Resnet-50
LSTM
TCN

Temporal

Vanilla

Spatial

GP

As shown in Table 7, HS300 and ZZ500 are important stock indices in the A-share stock market.
The strategy for the excess return is the annualized excess return of the long portfolio vs. the index.
The max drawdown is the worst loss of the excess return from its peak. The Sharpe ratio is the
annually adjusted excess return divided by a certain level of risk. These indicators can show the
strategy’s performance from the perspective of both return and risk.

In a multi-factor strategy, a higher correlation among the factors will reduce the strategy’s
performance, and it is shown in the deﬁnition of the factor construction process in formula (7).
The goal of factor construction is not to ﬁnd factors with higher performance, but to ﬁnd factors
that can improve the overall performance of the combined factors selected from PK 50 and New
50. Thus, combining the factors from both the new and existing human experts’ factors is more
reasonable and suitable for practical use cases. In all cases, our combined 50 is better than PK 50
and GP’s Combine 50, which means that the NNAFC can construct more useful factors than GP
with a reasonable factor selection process.

4.5. Comprehending the results

In section 4, the numerical experiment results show that the LSTM can extract more information
than FCN. We suspect that only the RNN and FCN are helpful for extracting information from
the ﬁnancial time series. To verify this idea, we constructed 50 factors by using an FCN, 50 factors
by using a spatial neural network and 50 factors by using a temporal neural network. Then, we
clustered all of these factors into three groups by using k-means. The goal of this process is that
there are mainly three types of neural networks, and we want to ﬁnd whether the constructed
factors have a similarity relationship. The deﬁnition of the distance has been mentioned in formula
(19) in section 4.1. To visualize this distance matrix, this matrix should be transformed into a 2D
graph. We initialize one of the cluster centres as (0, 0) and then determine the other two cluster
centres according to their relative distances and a given direction. This direction will inﬂuence

17

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

only the outlook of this graph and will not inﬂuence the shared space between the two diﬀerent
clusters. For samples that belong to the same cluster, their location is determined according to the
relative distance between a cluster centre and a randomly generated direction. As a result, we can
obtain the distribution of the constructed factors. The larger the factor’s sparsity is, the larger the
contribution of a certain type of network. Sometimes, the factor’s sparsity is low, but if it has never
been overlapped by the other network’s factors, its distinct contributions are also highly valued.
The experiment results are shown in Figure 7.

Figure 7. Cluster diﬀerent networks (spatial against temporal)

As shown in Figure 7(left), the factors constructed by the LSTM have the sparsest distribution,
which means that the network structure that focuses on temporal information is excellent at ex-
tracting diversiﬁed information from the ﬁnancial time series. However, a large space is shared by
FCN and Le-net. We can regard Le-net’s information as a subset of the FCN. Combined with the
CNN’s poor performance in sections 4.2 and 4.3, it looks such as that the CNN structure does not
make a substantial contribution in extracting information from the ﬁnancial time series. Figure
7(right) is an extra experiment, whose results support this conclusion as well.

Except for the type of neural network, actually, the complexity of the neural networks also
inﬂuences the result. We provide extra experiments in Figure 8. Normally, the model’s complexity
should meet the dataset’s complexity. Most of the ﬁnancial decisions are still made by humans, and
thus, the signals are mostly linear and simple because this way is how the human brain processes
information. A very complicated network structure will bring in extra risk of over-ﬁtting.

Figure 8. More complicated neural networks compared with the networks used in ﬁgure 8.

18

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

In Figure 8, the shared space between Vanilla and Spatial is still larger than the space shared
between Vanilla and Temporal. Thus, it appears that the reason why the LSTM’s information
coeﬃcient can outperform the other networks is that the recurrent structure and fully connected
structure are truly helpful in extracting information from ﬁnancial time series. At the same time,
these two structures focus on a diﬀerent part of the information, which makes their joint eﬀort
more valuable. Using an RNN to obtain an embedding representation of a ﬁnancial time series is
the best choice, as shown in our experiments. However, a more complex time-decay structure could
be used, such the TCN, but the Transformer does not perform better than the LSTM. We believe
that the trading signals in the Chinese A-share market are mostly linear, and thus, a very complex
non-linear feature extractor (a neural network structure) is not suitable at present.

However, while the stock market is developing, more and more investors crowd into this game.
We think that the factor crowding phenomenon will become more and more clear. In addition, as
more and more tradings are made by algorithms, the non-linear part in the trading signals will be
larger. Thus, for quantitative trading, we believe that the complicated and tailored neural network
structure will have its supreme moment in the near future.

5. Conclusions and Future Research

In this paper, we propose Neural Network-based Automatic Factor Construction (NNAFC). This
framework can automatically construct diversiﬁed and highly informative technical indicators with
the help of prior knowledge and diﬀerent feature extractors. In both numerical experiment and real-
world use case tests, it can perform better than the state-of-the-art in this task, which is genetic
programming. Although diﬀerent network structures perform diﬀerently in this factor construction
task, they can contribute to the diversity of the factor pool. Thus, they are also highly valuable
for the multi-factor quantitative investment strategy. Furthermore, we also conduct experiments
to comprehend their contributions and diﬀerences. For further research, this framework can also
be tested on a company’s fundamental and market news data.

References

Allen F, Karjalainen R. Using genetic algorithms to ﬁnd technical trading rules[J]. Journal of Financial
Economics, 1999, 51(2): 245-271.

Botsis T, Nguyen M D, Woo E J, et al. Text mining for the Vaccine Adverse Event Reporting
System: medical text classiﬁcation using informative factor selection[J]. Journal of the American Medical
Informatics Association, 2011, 18(5): 631-638.

Chen T, He T, Benesty M, et al. Xgboost: extreme gradient boosting[J]. R package version 0.4-2,
2015: 1-4.

Dash M, Liu H. Feature selection for classiﬁcation[J]. Intelligent data analysis, 1997, 1(3): 131-
156.

Ding X, Zhang Y, Liu T, et al. Deep learning for event-driven stock prediction[C]. Twenty-fourth
international joint conference on artiﬁcial intelligence. 2015.

Edward E. Qian, Ronald H. Hua, Eric H. Sorensen, Quantitative Equity Portfolio Management,
Chapman and Hall/CRC, 2007.

Fama E F, French K R. Common risk factors in the returns on stocks and bonds[J]. Journal of
Financial Economics, 1993.

19

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Fama E F, French K R. A ﬁve-factor asset pricing model[J]. Journal of ﬁnancial economics, 2015,
116(1): 1-22.

Feng G, Giglio S, Xiu D. Taming the factor zoo: A test of new factors[J]. The Journal of Fi-
nance, June 2020, 75(3): 1327-1370.

Fuli Feng, Huimin Chen, Xiangnan He,
Prediction
Stock Movement
Enhancing
https://EconPapers.repec.org/RePEc:arx:papers:1810.09936, 2019.

Ji Ding, Maosong Sun,
Adversarial
with

and Tat-Seng Chua.
Papers.arXiv.org.

Training.

Frankle, Jonathan, and Michael Carbin. “The lottery ticket hypothesis: Finding sparse, trainable
neural networks.” arXiv preprint arXiv:1803.03635 (2018).

Fischer T, Krauss C. Deep learning with long short-term memory networks for ﬁnancial market
predictions[J]. European Journal of Operational Research, 2018, 270(2): 654-669.

Grinold, Richard C. and Kanh, Ronald N.,Active portfolio management: a quantitative approach
for providing superior returns and controlling risk[M], McGraw-Hill, 1999.

Harvey, C.R., Liu, Y. and Zhu, H. The Cross-Section of Expected Returns. The Review of Finan-
cial Studies, 2016, 29, 5-68.

Hidasi B, Quadrana M, Karatzoglou A, et al. Parallel recurrent neural network architectures for
factor-rich session-based recommendations[C]. Proceedings of the 10th ACM conference on recommender
systems. ACM, 2016: 241-248.

He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. Proceedings of the
IEEE conference on computer vision and pattern recognition. 2016: 770-778.

Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-
1780.

Juuti M, Szyller S, Marchal S, et al. PRADA: protecting against DNN model stealing attacks[C].
2019 IEEE European Symposium on Security and Privacy. IEEE, 2019: 512-527.

Fang, J., Xia, Z., Liu, X., Xia, S., Jiang, Y., & Lin, J. (2019). Alpha Discovery Neural Network
based on Prior Knowledge. arXiv preprint arXiv:1912.11761.

J. Wang, Y. Yang, J. Mao, Z. Huang, C. Huang and W. Xu, ”CNN-RNN: A Uniﬁed Framework
for Multi-label Image Classiﬁcation,” 2016 IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), 2016, 2285-2294.

Fang, J., & Lin, J. (2020). Prior knowledge distillation based on ﬁnancial time series. arXiv preprint
arXiv:2006.09247.

Krawiec K. Genetic programming-based construction of
edge discovery tasks[J]. Genetic Programming and Evolvable Machines, 2002, 3(4): 329-343.

factors for machine learning and knowl-

Kakushadze Z. 101 formulaic alphas[J]. Wilmott, 2016, 84: 72-81.

Lillywhite K, Lee D J, Tippetts B, et al. A factor construction method for general object recog-
nition[J]. Pattern Recognition, 2013, 46(12): 3300-3314.

Lai S, Xu L, Liu K, et al. Recurrent convolutional neural networks for text classiﬁcation[C]. Twenty-ninth
AAAI conference on artiﬁcial intelligence. 2015.

20

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

Leshno M, Lin V Y, Pinkus A, et al. Multilayer feedforward networks with a nonpolynomial acti-
vation function can approximate any function[J]. Neural networks, 1993, 6(6): 861-867.

LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J].
Proceedings of the IEEE, 1998, 86(11): 2278-2324.

Lea C, Flynn M D, Vidal R, et al. Temporal convolutional networks for action segmentation and
detection[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017:
156-165.

Mittal S. A survey of techniques for approximate computing[J]. ACM Computing Surveys (CSUR), 2016,
48(4): 1-33.

Romero, et al. Knowledge discovery with genetic programming for providing feedback to course-
ware authors[J]. User Modeling and User-Adapted Interaction, 2004, 14(5):425–464.

Ravisankar P, Ravi V, Rao G R, et al. Detection of ﬁnancial statement fraud and factor selection
using data mining techniques[J]. Decision Support Systems, 2011, 50(2): 491-500.

Sharpe W F. Capital asset prices: A theory of market equilibrium under conditions of risk[J].
Journal of ﬁnance, 1964, 19(3): 425-442.

Shan K, Guo J, You W, et al. Automatic facial expression recognition based on a deep convolutional-
neural-network structure[C]. 2017 IEEE 15th International Conference on Software Engineering Research,
Management and Applications (SERA). IEEE, 2017: 123-128.

Sidra Mehtab
diction
Learning
Using
https://EconPapers.repec.org/RePEc:arx:papers:1912.07700, 2019.

Sen. A Robust
Natural

and
Deep

Jaydip

and

Predictive Model
Language

for
Processing.

Stock
PricePre-
Papers.arXiv.org.

Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Deep Inside Convolutional Networks: Visual-
ising Image Classiﬁcation Models and Saliency Maps. CoRR, abs/1312.6034.

Tibshirani R. Regression shrinkage and selection via the lasso[J]. Journal of the Royal Statistical
Society: Series B (Methodological), 1996, 58(1): 267-288.

Tran B, Xue B, Zhang M. Genetic programming for factor construction and selection in classiﬁca-
tion on high-dimensional data[J]. Memetic Computing, 2016, 8(1): 3-15.

Thomas J D, Sycara K. The importance of simplicity and validation in genetic programming for
data mining in ﬁnancial data[C]. Proceedings of the joint AAAI-1999 and GECCO-1999 Workshop on
Data Mining with Evolutionary Algorithms. 1999.

Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]. Advances in neural
mation processing systems. 2017: 5998-6008.

infor-

Wold S, Esbensen K, Geladi P. Principal component analysis[J]. Chemometrics and intelligent
laboratory systems, 1987, 2(1-3): 37-52.

Yu Zhang, Ying Wei, et.al . Learning to Multitask[C]. NeurlPS 2018.

Z. Zhang, T. Chen, M. Wang and L. Zheng, An Exponential-Type Anti-Noise Varying-Gain Net-
work for Solving Disturbed Time-Varying Inversion System, IEEE Transactions on Neural Networks and
Learning Systems, 2019.

Z. Zhang, L. Zheng, T. Qiu and F. Deng, ”Varying-Parameter Convergent-Diﬀerential Neural So-

21

October 14, 2020

Quantitative Finance

Neural˙Network-based˙Automatic˙Factor˙Construction

lution to Time-Varying Overdetermined System of Linear Equations,” in IEEE Transactions on
Automatic Control, 2020, 65(2), 874-881.

Zhong Y, Sullivan J, Li H. Face attribute prediction using oﬀ-the-shelf cnn factors[C]. 2016 Inter-
national Conference on Biometrics (ICB). IEEE, 2016: 1-7.

22

