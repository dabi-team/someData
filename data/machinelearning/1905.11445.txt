9
1
0
2

y
a
M
7
2

]

G
L
.
s
c
[

1
v
5
4
4
1
1
.
5
0
9
1
:
v
i
X
r
a

COSET: A Benchmark for Evaluating
Neural Program Embeddings

Ke Wang
Visa Research
Palo Alto, CA 94306
kewang@visa.com

Mihai Christodorescu
Visa Research
Palo Alto, CA 94306
mihai.christodorescu@visa.com

Abstract

Neural program embedding can be helpful in analyzing large software, a task that is
challenging for traditional logic-based program analyses due to their limited scala-
bility. A key focus of recent machine-learning advances in this area is on modeling
program semantics instead of just syntax. Unfortunately evaluating such advances
is not obvious, as program semantics does not lend itself to straightforward metrics.
In this paper, we introduce a benchmarking framework called COSET for standard-
izing the evaluation of neural program embeddings. COSET consists of a diverse
dataset of programs in source-code format, labeled by human experts according to
a number of program properties of interest. A point of novelty is a suite of program
transformations included in COSET. These transformations when applied to the
base dataset can simulate natural changes to program code due to optimization and
refactoring and can serve as a “debugging” tool for classiﬁcation mistakes. We con-
ducted a pilot study on four prominent models—TreeLSTM [1], gated graph neural
network (GGNN) [2], AST–Path neural network (APNN) [3], and DYPRO [4].
We found that COSET is useful in identifying the strengths and limitations of
each model and in pinpointing speciﬁc syntactic and semantic characteristics of
programs that pose challenges.

1

Introduction

Analyzing large software artifacts (so-called “Big Code”) has proven extremely challenging for
logic-base approaches, which quickly force a trade-off between accuracy and scalability. Applying
deep learning architectures to this task is a promising direction that ﬂourished in the last few years,
with a number of techniques being proposed around a variety of features extracted from software
programs, ranging from syntactic to statically derived from source code to dynamically inferred from
execution traces. Since these models operate in separate application domains, it is hard to draw a
comparison. Our goal in this paper is to measure how precisely models can capture the program
semantics and to this end we propose a framework, COSET, to standardize the evaluation of neural
program embeddings.

The idea is to reuse the knowledge distilled from existing code repositories in an attempt to simplify
the future development of software, and in turn improve the product quality, is quite powerful. Code
completion for example is a signiﬁcant boost to programmer productivity and as such is a worthwhile
task to pursue. Early methods in the ﬁeld applied NLP techniques to discover the textual patterns
existed in the source code [5; 6; 7]; following approaches proposed to learn the syntactic program
embedding from the Abstract Syntax Trees (AST) [8; 9; 10]. Such approaches addressed code
completion, but fell short of more complex tasks such as program synthesis or repair, where thorough
understanding and precise representations of program semantics are required. A number of new
deep-learning architectures developed to speciﬁcally address this issue [2; 3; 4; 11; 12] tried a variety

Preprint. Under review.

 
 
 
 
 
 
of improvements, in terms of features (static features based on program source, dynamic featured
from the execution of the program [4], abstract features from symbolic program traces [11], features
from the graph representation of a program [2]). This diversity also makes it hard to evaluate these
approaches as they do not uniformly or universally improve on existing work, but rather exhibit
trade-offs across the natural characteristics of programs.

We propose a new benchmarking framework, called COSET, that aims to provide a consistent
baseline for evaluating the performance of any neural program embedding. COSET captures both the
human-induced variations in software (due to, e.g., coding style, algorithmic choices, code layout
and structure) and algorithmic transformations to the software (due to, e.g., software refactoring,
code optimization). COSET consists of a dataset of almost eighty-ﬁve thousand programs, developed
by a large number of users, solving one of ten coding problems, and labeled with a number of
characteristics of interest (e.g. running time, pre- and post-condition, loop invariant). This dataset is
paired with a number of program transformations, which when applied to any program in the dataset
can produce variations of that program. These two elements (the dataset and the transformations) gives
COSET the power to distinguish between various neural program embeddings with high precision.

One use of COSET is to provide a classiﬁcation task for the neural program embeddings under
evaluation. The benchmark will measure the accuracy and the stability of the embedding, giving the
user a precise answer on how various embeddings compare. A second use of COSET is as a debugging
tool: once a user determines that a neural program embedding fails short of some goal, the COSET
program transformations can be used (in delta debugging fashion) to identify the characteristic of the
programming language, the program code, or the program execution that causes the accuracy drop.

We have conducted a pilot study with this benchmarking framework on four of the latest deep learning
models. We selected DYPRO [4], TreeLSTM [1], the gated graph neural network (GGNN) [2], and the
AST-Path neural network (APNN) [3] as evaluation subjects for COSET. Through our comprehensive
evaluation, we ﬁnd that DYPRO achieves the best result in the semantics prediction task and is
signiﬁcantly more stable with its prediction results than static models, which have difﬁculties in
capturing the program semantics. As a result, static models are much less stable against the syntax
variations even when the semantics are preserved. On the other hand, the generalization from static
program features as done in GGNN and APNN, while insufﬁciently accurate, is scalable to large or
long-running programs, which overwhelm the dynamic model of DYPRO. Through careful use of
COSET’s debugging features, we then identify a number of speciﬁc shortcomings in the tested models,
from lack of support for variable types, to confusion about logging and other ancillary program
aspects, and to limitations in representing APIs.

We make the following contributions:

• We design COSET, a novel benchmark framework that expresses both human and algorithmic

variations in software artifacts.

• We propose COSET for evaluating how precise the models can learn to interpret the program se-
mantics and for identifying speciﬁc program characteristics that are the source of misclassiﬁcation.
• We present our evaluation results including the strength and weakness of each model, followed by

an in-depth discussion of our ﬁndings.

2 Motivation and Running Example

For an intuition on why programs are difﬁcult to learn, consider the program of Figure 1. The
method Program.Difference() takes as input an array, sorts it, and returns the difference between
the smallest value and the largest value.

To learn the semantics of this program sufﬁciently well as to classify it correctly as a sorting routine
using the Bubblesort strategy requires bridging the gap between the syntactic representation and its
runtime execution. This gap is typically expressed in abstract interpretation by considering a program
to be the sum-total of all its possible execution traces and considering a program’s source code to be
an abstraction of the program-as-traces model. Intuitively this distinction implies that learning to
classify programs may be limited in accuracy when using only the source code, and correspondingly
benchmarks should focus on samples whose source code and execution traces are not trivially related.

2

The gap between syntax and (runtime) semantics is characterized by several properties, which become
the requirements for our COSET benchmark. In other words, we wish for COSET to have enough
training and test samples to cover the whole range of distinctions between syntax and semantics.

First, the gap between program syntax and seman-
tics is manifested in that similar programs of similar
syntax may have to vastly different semantics. For
example, consider the two sorting implementations,
both sorting an array via two nested loops, both
comparing the current element to its successor, and
both swapping them if the order is incorrect. Yet
the two functions could implement different sort-
ing strategies, namely Bubblesort and Insertionsort.
Therefore minor syntactic discrepancies can lead to
signiﬁcant semantic differences. Our ﬁrst require-
ment for the benchmark is to include sufﬁcient sam-
ples that vary both syntactically and semantically,
though solving the same software task.

public static int Difference ( int [] a )
{

int swappbit =1;
while ( swappbit != 0)
{

swappbit =0;
for ( int i =0; i < a . Length - 1; i ++)
{

int temp1 , temp2 ;
temp1 = a [ i ];
temp2 = a [ i + 1];
if ( temp1 < temp2 )
{

a [ i ]= temp2 ;
a [ i + 1]= temp1 ;
swappbit =1;

}

}

}
int b = a [0] - a [ a . Length - 1];
return b ;

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22

}

Second, program statements are almost never inter-
preted in the order in which the corresponding token
sequence appears in the source code (the only excep-
tion being straight-line programs, i.e., ones without
any control-ﬂow statements). For example, a condi-
tional statement only executes one branch each time,
but its token sequence is expressed sequentially as
multiple branch bodies. Similarly, when iterating
over a looping structure at runtime, it is unclear in
which order any two tokens are executed when considering different loop iterations. In Figure 1
the statement on line 23 may or may not execute after the statement on line 25 from a previous
loop iteration. A related characteristic is that the order of independent statements (e.g., lines 19
and 20) is arbitrary and the learning model needs to be evaluated for syntactic-ordering biases. The
second requirement for our benchmark is to include samples that vary only in the syntactic order of
statements, without changing semantics.

Figure 1: Sample code from COSET dataset,
sorting an array using a Bubblesort-style strat-
egy and then computing the difference between
the smallest and the largest elements.

Third, many tokens in the source code are not relevant to program semantics. For example, variable
names do not affect the results computed by a program, but rather the dependencies between variables
(both data and control dependencies) play an essential role in deﬁning program semantics. In Figure 1
the variables swappbit, i, temp1, temp2, b, and a could have any other names, distinct from each
other, and the program would behave the same way. The third requirement for our benchmark is to
include samples with arbitrarily values for syntactic tokens that do not affect semantics.

3 Benchmark Design

The requirements from the previous section determine the need for a wide range and wide diversity
of programs, both in terms of source code and in terms of execution behavior. Before describing
COSET’s design, we add an additional requirement, that of non-adversarial, natural style for our
dataset samples. The goal of COSET is to provide the capability of testing learning models on their
accuracy in capturing program semantics, and we wish to explicitly exclude adversarial examples from
this benchmark. The rationale is that adversarial examples, which underwent program obfuscation
in order to make their analysis and understanding exceedingly hard, are an orthogonal problem
that requires separate solutions and accordingly a distinct benchmark. For example, a common
obfuscation used by malicious programs is that of embedding an interpreter. The original (malicious)
program is replaced by a new program that consists of an interpreter and an reimplementation of the
original program in a randomly generated language, to be handled by the interpreter. Understanding
any adversarial examples obfuscated this way requires addressing three tasks: (1) identifying the
presence of an interpreter, (2) learning the semantics of that interpreter, and (3) learning the semantics
of the source code embedded alongside of the interpreter. COSET as a benchmark focuses only on
task (3) and thus explicitly excludes adversarially generated samples.

3

With this additional requirement in mind, we design the benchmark to include samples that capture
normal variation in terms of syntax, style, algorithms, etc. arising in day-to-day programming work.
This means using program samples from a diverse set of programmers, all solving the same coding
problem, as well as including tools to transform these samples according to two common steps in the
software-development lifecycle, refactoring and optimization.

3.1 Program Dataset

The dataset consists of 84,165 programs in total. They are obtained from a popular online coding
platform. Programs were written in several different languages: Java, C# and Python. All programs
solve a particular coding problem. We hand picked the problems to ensure the diversity of the
programs in the dataset. Speciﬁcally it contains introductory programming exercises for beginners,
coding puzzles that exhibit considerable algorithmic complexity and challenging problems frequently
appearing on coding interviews. We split the whole dataset into a training set containing 64,165
programs, a validation set of 10,000 programs, and a test set of the remaining 10,000 programs.

3.2 Program Labels

The dataset have been manually analyzed and labeled on the basis of operational semantics (e.g.
Bubblesort, Insertionsort, Mergesort, etc., for a sorting routine). We allow certain kind of variations
to keep the total number of labels manageable. For example, we ignore local variables allocated for
temporary storage, the iterative style of looping or recursion, sort order: descending or ascending, etc.

The work is done by fourteen PhD students and exchange scholars at University of California, Davis.
They come from different research backgrounds such as programming language, database, security,
graphics, machine learning, etc. All of them have been interviewed and tested for their knowledge on
program semantics. To reduce the labeling error, we distributed programs solving the same coding
problem mostly to a single person and have them cross check the results for validation. The whole
process took more than three months to complete. In the end, we deﬁned 38 labels in total, with more
than 2,000 programs on average for each label.1

3.3 Program Transformations

In this section, we introduce the transformations we apply to COSET for generating program variants.
They serve two purposes in our experiments: debugging the model and gauging the model stability.
The former refers to identifying the reason of misclassiﬁcation; the latter measures how stable are
the models against software variations, both of which will be explained in details in Section 4.
COSET considers three types of transformations, semantics-preserving, semantics-approximating,
and semantics-changing.

Semantics-Preserving Transformations: This category can be further split into compiler opti-
mizations (i.e. for improving the performance of a program in code compilation) and software
refactoring (i.e. for improving the code readability and maintainability in software development).
Even though they are designed for different settings and purposes, all of them preserve the semantics
of the original programs. In this paper we choose Constant and Variable Propagation (CVP), Dead
Code Elimination (DCE), Loop Unrolling (LU) and Hoisting for compiler optimization and Variable
Renaming (VR), Nested Condition Simpliﬁcation (NCS), Control Flag Removal (CFR) and Control
Statement Uniﬁcation (CSU) for software refactoring. Interested readers are invited to consult Aho et
al.’s compiler textbook [13] and the Eclipse IDE documentation [14; 15] for the examples of each
transformation.

Semantics-Approximating Transformations: Semantics-approximating transformations change
a program in minor ways producing a new program with semantics close to that of the original. In par-
ticular we adopt two kinds of transformations. First we change type names to others within the same
group. For example, we replace int with long in the group of integer types, or double with float in
the group of ﬂoating-point types. Second we also change an API in the program to another of the sim-
ilar semantics such as changing Array.Sort(a:array) to Array.Sort(a:array,c:comparer)
from Microsoft’s .NET Framework for C#.

1Readers are encouraged to consult the supplemental material.

4

(a) Prediction (accuracy).

(b) Prediction (F1 score).

(c) Scalability (accuracy).

(d) Scalability (F1 score).

Figure 2: Evaluation Results.

Semantics-Changing Transformations: We also include transformations that change the seman-
tics of a program. Those transformations are necessary solely for the purpose of debugging a model.
Speciﬁcally COSET can remove error handling code from a program and can ﬁx a buggy program with
SARFGEN [16], a productized repair tool often used in Massive Open Online Courses (MOOCs).

4 Evaluation

In this section, we describe in details how we use COSET to evaluate the strengths and weaknesses of
each deep neural architecture by measuring two characteristics—accuracy and stability.

4.1 Evaluating Classiﬁcation Accuracy Using COSET

We train GGNN, APNN, TreeLSTM and DYPRO on COSET to predict the semantic label of a test
program. In this experiment we use prediction accuracy and F1 score to measure classiﬁcation results.

Results. Figure 2a depicts the prediction accuracy of all models on COSET. Overall, DYPRO
comes on top as the most accurate model at 84.7%, with GGNN achieves 75.2% prediction accuracy.
In terms of F1 score (Figure 2b), DYPRO and GGNN keep their top positions while APNN and
TreeLSTM swap their places at the bottom of the ranking. The other metric of interest is scalability in
terms of program size. In order to compare GGNN, APNN, and TreeLSTM (which operate on source
code, typically measured as number of lines) on one hand, and DYPRO (which operates on execution
trace, measured as number of state changes) on the other, we unify the program-size dimension as
number of bytes, with a line of code being on average 15 bytes in length and a trace state changes
20 bytes. As shown in Figures 2c and 2d, accuracy quickly drops as program size increases, with
static models (GGNN, APNN, and TreeLSTM) losing all accuracy once a program exceeds 100 lines.

Discussion of Misclassiﬁcation Types. COSET is able to trigger many types of misclassiﬁcations.
To gain a better understanding of the errors each model made, we have conducted a large scale manual
inspection and summarize our ﬁndings below.

Types and Variable Names: Type name can cause issues. For example, Figure 3a shows a program
in COSET that causes APNN to misclassify. Replacing float to double ﬁxes this error. As a more
subtle example, program in Figure 3b is misclassiﬁed by all of the static models. The error is due to
variable col and row. Since vast majority of programs in COSET use row (resp. col) as the induction
variable in the outer (resp. inner) loop , static models do not recognize the same semantics a program

5

static int Average ( int [] a )
{

static void PrintChessBoard ()
{

static int CountParentheses ( string s )
{

var b = 0;
foreach ( int num in a )

b += num ;

float c = (float)

( b / a . Length );

for ( int col=0;col <8;col++) {
for ( int row=0;row <8;row++)
if ((col+row)%2 == 0)

Console . Write ( " X " );

else

Console . Write ( " O " );

return c ;

}

}

Console . WriteLine (); }

int open = 0 , close = 0 , depth = 0;

foreach ( char c in s )
{

switch (c) {

case '( ':

open ++;
if ( open - depth > close )

depth ++;

break ;

case ') ':

close ++;
break ;

default:

(a)

(b)

static int Ksmallest (

int [] a , int k )

static string Reversal ( string s )
{

}}

{

}

Array. Sort (a , (x, y)

=> y.CompareTo(x));

string temp = s.ElementAt(0);
for ( int i = s . Length -2; i >=1; i - -)
temp = temp + s .ElementAt(i);

return

temp +

if ( open != close )
depth = 0;

return a [ k - 1];

s .ElementAt(s.Length - 1);

return depth ;

}

}

(c)

(d)

(g)

bool IsUglyNumber ( int num )
{

if (num <= 0)
{11

return false;

}11

for ( int i =2; i <6; i ++)
{

while ( num % i ==0)

num /= i ;

}

bool isUgly = num ==1;

char FindExtraChar ( string s ,

string t )

{

int [] arr = new int [26];
int ground =

char.GetNumericValue(’a’);

for ( int i =0; i < s . Length ; i ++) {

int val =
char.GetNumericValue(s[i]);
arr [ val - ground ]++;}

for ( int j =0; j < t . Length ; j ++) {

int val =
char.GetNumericValue(t[i]);
if ( - - arr [ val - ground ] < 0)

return s [ j ];}

static int Difference ( int [] a )
{

int swappbit = 1;
while (swappbit != 0) {
swappbit = 0;
for ( int i =0; i < a . Length -1; i ++){

int temp1 , temp2 ;
temp1 = a [ i ];
temp2 = a [ i + 1];
if ( temp1 > temp2 ) {
a [ i ] = temp2 ;
a [ i + 1] = temp1 ;
swappbit = 1;

}}}111

return isUgly ;

return '0 ';

return a [ a . Length - 1] - a [0];

}

}

}

(e)

(f)

(h)

Figure 3: Misclassiﬁcations triggered by COSET. Underlined code is the root cause of the misclassiﬁ-
cation, as identiﬁed by COSET’s debugging capability. See Section 4 for a detailed analysis.

denotes after some of its variables (i.e. row and col) are interchanged. It sufﬁces to conclude that
static models weigh variable names in their prediction, where in fact they should not be considered
for any semantics-based classiﬁcation task.

Syntactic Structure: To capture human programming habits in terms of code style and code structure,
COSET exposes a uneven distribution of code samples in terms of program syntax (i.e. some are
more frequently used than others), which helps to trigger misclassiﬁcations. For example, the switch
statement in program 3g is the reason of the error. In other words, models would have correctly
classiﬁed the program if the switch is converted to a if-else, a far more popular selection statement.

Scalability: COSET also reveals scalability being a major issue among all models including both
static and dynamic. As the size of programs/execution traces increases, all models suffer notable
drops in accuracy (resp. F1 score) depicted in Figure 2c (resp. Figure 2d). In particular APNN is
shown to be the most vulnerable.

API Usage: COSET discloses issues among static models in generalizing API calls. Figures 3c and 3d
show two examples. Figure 3c (resp. Figure 3d) can be ﬁxed by replacing the underlined API with
Array.Sort(a) (resp. indexing operator []). Granted, COSET bears some blame as it contains very
few programs of the same label as the two examples that has the exact identical API signatures, but
to expect a sufﬁcient coverage of all API prototypes (e.g. 17 in total for Array.Sort according to
Microsoft .NET framework 4.7) is a challenging task for any dataset. Therefore generalizing to
largely similar despite partially unseen API is necessary and immediate. In addition, in both cases
such API variations account for a small portion of the whole program. In particular we ﬁnd 86.2%

6

Models

GGNN
APNN

Compiler Optimization

Software Refactoring

CVP

DCE

LU

Hoisting

VR

NCS

CFR

CSU

4.2%
5.6% 19.1%
7.4% 12.7% 27.7%
5.5% 14.6%
5.3%
4.5%

TreeLSTM 7.2%
2.6%

DYPRO

7.8% 8.4% 13.5% 11.0% 17.2%
11.7% 7.1% 16.3% 14.9% 22.8%
8.5% 19.7%
0.0%
0.0%

6.4% 5.4% 12.8%
0.0%
1.7% 0.0%

Table 1: Results of measuring the stability using COSET, as percentage of changed predictions when
applying semantics-preserving transformations.

(resp. 68.9%) of the programs in the training set have a very similar body (i.e. differ by less than eight
tokens) to Figure 3c (resp. Figure 3d) according to DECKARD [17], arguably the state-of-the-art
clone detection algorithm. As a result, it is reasonable to expect static models to be immune from the
minor API changes.

Error Handling: COSET shows error handling code can be another cause of misclassiﬁcation.
Figure 3e depicts an example that is misclassiﬁed by APNN and TreeLSTM. By removing the if
clause at the beginning of the function, both models produce the correct result. This is another
example COSET demonstrates that static models are unstable against syntax change that barely affects
the program semantics.

Buggy Code: COSET reveals the DYPRO model as the most susceptible to buggy programs. The
reason being the dynamic nature of program representation DYPRO adopts. That is if there is a bug
in the program, its runtime dynamic traces will likely lead to greater discrepancies than the syntactic
representations (e.g. tokens, ASTs, etc.). Take the program in Figure 3f for example, the bug lies
in the API call in int val = char.GetNumericValue(‘a’) which converts a character of digit to an
integer. In contrast what the programmer intended is to directly get the ascii code/unicode of the
character using int val = ‘a’. Unfortunately this bug leads to a chaotic dynamic representation (i.e.
char.GetNumericValue() will be evaluated to -1 for any non-digit character), DYPRO is not able to
overcome. In comparison, all static models display a stronger fault tolerance by correctly classifying
the buggy program.

Semantic Property: COSET gives insights into the limitations of static models in learning semantic
program properties. The program in Figure 3h triggers a misclassiﬁcation of this kind on all static
models. Worth mentioning the program does not implement a standard Bubblesort strategy, in which
the inner loop systematically shifts its bound towards the beginning of an array after bigger numbers
are being bubbled up to the end of the array. Instead the inner loop in the program keeps sorting the
entire array until reaching an iteration where no items are swapped, at which point the array is sorted.

Our methodology for analyzing this misclassiﬁcation starts with formally deﬁning the semantic label
L of Bubblesort to apply to programs that have a function satisfying the following properties:
1. time complexity of O(n2);
2. input type signature of a : array and post-condition that the array a is sorted, ∀i ∈ [0, a.Length−

1). a[i] ≤ a[i + 1];

3. having a nested loop structure and an invariant of the outer loop assuming i is the number of its

iterations, ∀j ∈ [a.Length − i, a.Length − 1]. a[j − 1] ≤ a[j].

Similarly we give deﬁnitions of all other labels in COSET. To identify the source of the misclassiﬁ-
cation (i.e. a program not being classiﬁed as L = Bubblesort), we repeat the classiﬁcation task for
a broader set of classes, using one property at a time as a new label deﬁnition. For example, using
property (1) alone we collect all programs with time complexity O(n2) in COSET as the training
data for a label L(cid:48). Similarly for property (2) we use all programs that implement a sorting routine
and label them L(cid:48)(cid:48); and for (3) we collect all programs that display the bubbling behavior under
label L(cid:48)(cid:48)(cid:48). Results show static models incorrectly predict the program in Figure 3h only when using
property (3) as label deﬁnition. In other words, the static models GGNN, APNN, and TreeLSTM fail
to learn a sufﬁciently expressive discriminator to understand the non-standard bubbling sort strategy.
This methodology demonstrates COSET’s utility in discovering the limitations of models in learning
semantic properties.

7

4.2 Evaluating Model Stability Using COSET

In this experiment we evaluate how stable the models are with their predictions. The reason stability
is a relevant and important metric is because changes in source code are inevitable, and a model
that is resilient against changes, especially the semantics-preserving kind, will yield many beneﬁts.
As mentioned in the benchmark design (Section 3), COSET applies normal, standard, and widely
adopted program transformations to simulate how software evolves during its normal lifecycle (e.g.
code optimization or software refactoring).

Given the models that are trained for the experiment in Section 4.1 and COSET’s testing program, we
apply code transformations to generate a new test set, preserving semantics for all samples. Programs
with no applicable transformations are excluded from the new test set. We then examine if models
hold on to their prior predictions. Table 1 depicts the results. The number in each cell denotes the
percentage of programs in the new test set on which a model changed its prediction. Overall, all
models display decent stability against software transformations. No transformation was able to
sway 30% or more predictions for any model. For the static models, TreeLSTM is shown to be the
most stable deep neural architecture while APNN is the most sensitive model to the transformations.
Unsurprisingly, DYPRO handles almost all semantics-preserving transformations as its features are
extracted from execution traces and thus largely unaffected by these source-code transformations.

5 Related Work

MNIST [18] (Modiﬁed National Institute of Standards and Technology database) is a database of
handwritten digits that is widely used for training and testing machine learning algorithms. The
MNIST database contains 60,000 training images and 10,000 testing images. There have been
many attempts to achieve the lowest error rate, among which some indeed have accomplished "near-
human performance". ImageNet [19] is a visual database designed for visual object recognition
research. More than 14 million images have been hand-annotated to indicate what objects are pictured.
ImageNet contains more than 20,000 categories, each of which consists of several hundred images.
Since 2010, ImageNet has been adopted in an annual contest, the ImageNet Large Scale Visual
Recognition Challenge (ILSVRC), where machine learning models compete to correctly classify and
detect objects and scenes. CIFAR-10 [20] (Canadian Institute For Advanced Research) is another
database of images that are commonly used to train machine learning and computer vision algorithms.
It is one of the most widely used datasets for machine learning research. The CIFAR-10 dataset
contains 60,000 32x32 color images in 10 different classes e.g. cars, birds, cats, etc. Labeled Faces in
the Wild [21] is a database of face photographs designed for studying the problem of unconstrained
face recognition. The dataset contains more than 13,000 images of faces collected from the web.
Each face has been labeled with the name of the person pictured. 1680 of the people pictured have
two or more distinct photos in the data set. The only constraint on these faces is that they were
detected by the Viola-Jones face detector.

Unlike the existing databases designed almost exclusively for training and testing machine learning
models in computer vision and image processing domain, we present COSET to evaluate how precise
deep neural architecture can learn the semantics of a program. In addition we also introduce a
suite of program transformations for assessing the stability of model prediction and debugging the
classiﬁcation mistake.

6 Conclusion

We introduce a benchmark framework called COSET for evaluating the accuracy and stability of neural
program embeddings and related neural network architectures proposed for software classiﬁcation
task. COSET consists of a set of natural, non-adversarial source-code samples from a variety of
programmers tasked with well-deﬁned coding challenges, and supplements this set with source-code
transformations representative of common software optimization and refactoring. In our evaluation
we observed that COSET was effective in measuring differences among various models proposed in
literature and provided debugging capabilities to identify the root causes of misclassiﬁcations.

8

References

[1] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representations
from tree-structured long short-term memory networks. CoRR, abs/1503.00075, 2015. URL
http://arxiv.org/abs/1503.00075.

[2] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent

programs with graphs. arXiv preprint arXiv:1711.00740, 2017.

[3] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: Learning distributed

representations of code. arXiv preprint arXiv:1803.09473, 2018.

[4] Ke Wang. Learning scalable and precise representation of program semantics. arXiv preprint

arXiv:1905.05251, 2019.

[5] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu. On the
naturalness of software. In Software Engineering (ICSE), 2012 34th International Conference
on, pages 837–847. IEEE, 2012.

[6] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepﬁx: Fixing common c

language errors by deep learning. 2017.

[7] Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay.

sk_p: a
neural program corrector for moocs. In Companion Proceedings of the 2016 ACM SIGPLAN
International Conference on Systems, Programming, Languages and Applications: Software for
Humanity, pages 39–40. ACM, 2016.

[8] Chris Maddison and Daniel Tarlow. Structured generative models of natural source code. In

International Conference on Machine Learning, pages 649–657, 2014.

[9] Pavol Bielik, Veselin Raychev, and Martin Vechev. Phog: probabilistic model for code. In

International Conference on Machine Learning, pages 2933–2942, 2016.

[10] Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree

structures for programming language processing. 2016.

[11] Jordan Henkel, Shuvendu Lahiri, Ben Liblit, and Thomas Reps. Code vectors: Understanding
programs through embedded abstracted symbolic traces. arXiv preprint arXiv:1803.06686,
2018.

[12] Daniel DeFreez, Aditya V. Thakur, and Cindy Rubio-González. Path-based function embeddings.
In Proceedings of the 40th International Conference on Software Engineering: Companion
Proceeedings, ICSE ’18, pages 430–431, New York, NY, USA, 2018. ACM. ISBN 978-1-4503-
5663-3. doi: 10.1145/3183440.3195042. URL http://doi.acm.org/10.1145/3183440.
3195042.

[13] Alfred V. Aho, Monica S. Lam, Ravi Sethi, and Jeffrey D. Ullman. Compilers: Principles,
Techniques, and Tools (2Nd Edition). Addison-Wesley Longman Publishing Co., Inc., Boston,
MA, USA, 2006. ISBN 0321486811.

[14] A brief history of eclipse. https://www.ibm.com/developerworks/rational/library/

nov05/cernosek/index.html. Accessed: 2019-05-22.

[15] Greg Goth. Beware the march of this ide: Eclipse is overshadowing other tool technologies.

IEEE software, 22(4):108–111, 2005.

[16] Ke Wang, Rishabh Singh, and Zhendong Su. Search, align, and repair: Data-driven feedback
generation for introductory programming exercises. In Proceedings of the 39th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI 2018, pages 481–495,
New York, NY, USA, 2018. ACM. ISBN 978-1-4503-5698-5. doi: 10.1145/3192366.3192384.
URL http://doi.acm.org/10.1145/3192366.3192384.

9

[17] Lingxiao Jiang, Ghassan Misherghi, Zhendong Su, and Stephane Glondu. Deckard: Scalable
and accurate tree-based detection of code clones. In Proceedings of the 29th International
Conference on Software Engineering, ICSE ’07, pages 96–105, Washington, DC, USA, 2007.
IEEE Computer Society. ISBN 0-7695-2828-7. doi: 10.1109/ICSE.2007.30. URL https:
//doi.org/10.1109/ICSE.2007.30.

[18] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning

applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.

ImageNet: A Large-Scale

Hierarchical Image Database. In CVPR09, 2009.

[20] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,

Citeseer, 2009.

[21] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the
wild: A database forstudying face recognition in unconstrained environments. In Workshop on
faces in’Real-Life’Images: detection, alignment, and recognition, 2008.

10

