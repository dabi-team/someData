A Semideﬁnite Relaxation for Sums of Heterogeneous Quadratics
on the Stiefel Manifold

Kyle Gilman∗

Sam Burer†

Laura Balzano∗

May 30, 2022

Abstract

We study the maximization of sums of heterogeneous quadratic functions over the Stiefel manifold,
a nonconvex problem that arises in several modern signal processing and machine learning applications
such as heteroscedastic probabilistic principal component analysis (HPPCA). In this work, we derive a
novel semideﬁnite program (SDP) relaxation and study a few of its theoretical properties. We prove a
global optimality certiﬁcate for the original nonconvex problem via a dual certiﬁcate, which leads us to
propose a simple feasibility problem to certify global optimality of a candidate local solution on the Stiefel
manifold. In addition, our relaxation reduces to an assignment linear program for jointly diagonalizable
problems and is therefore known to be tight in that case. We generalize this result to show that it is
also tight for close-to jointly diagonalizable problems, and we show that the HPPCA problem has this
characteristic. Numerical results validate our global optimality certiﬁcate and suﬃcient conditions for
when the SDP is tight in various problem settings.

Contents

1 Introduction

2 Semideﬁnite program relaxation

2.1 Strong duality and equivalence of optimizers . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Related work

4 Theoretical Results

4.1 Dual certiﬁcate of the SDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2 SDP tightness in the close-to jointly diagonalizable (CJD) case . . . . . . . . . . . . . . . . .
4.2.1 Continuity and tightness in the CJD case . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2 HPPCA is CJD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Numerical experiments

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1 Assessing the rank-one property (ROP)
5.2 Assessing global optimality of local solutions
. . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Computation time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

3
4

4

5
5
6
6
7

8
8
9
10

2
2
0
2

y
a
M
6
2

]

C
O
.
h
t
a
m

[

1
v
3
5
6
3
1
.
5
0
2
2
:
v
i
X
r
a

6 Future Work & Conclusion

10
∗Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, MI, 48105 USA (email:
kgilman@umich.edu). K. Gilman and L. Balzano were supported in part by ARO YIP award W911NF1910027, AFOSR
YIP award FA9550-19-1-0026, and NSF BIGDATA award IIS-1838179.

†Department of Business Analytics, University of Iowa , Iowa City, IA 52242 USA.

1

 
 
 
 
 
 
A Related Work

B Proofs of Section 2

B.1 Derivation of (SDP-D) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C Counterexample for Convex-Hull Result

D Proof of Theorem 4.1

D.1 Arithmetic Complexity - more details

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Proof of Theorem 4.5

F Example of SDP with rank-one solutions, but Mi that are not almost commuting

G Extended Experiments

G.1 Assessing the ROP: random PSD Mi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
G.2 Assessing the ROP: HPPCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
G.3 Assessing global optimality of local solutions

H Extension to the sum of Brocketts with linear terms

15

16
16

18

20
22

22

28

29
29
29
29

29

1 Introduction

This paper studies the problem known in the literature as the maximization of sums of heterogeneous
quadratic functions over the Stiefel manifold 1 [15; 7; 35; 6]. Speciﬁcally, given d × d-size symmetric positive
semideﬁnite (PSD) matrices M1, . . . , Mk (cid:23) 0 for k ≤ d, we wish to maximize the convex objective function
(cid:80)k
iMiui over the nonconvex constraint that U = [u1 · · · uk] ∈ Rd×k has orthonormal columns:

i=1 u(cid:48)

max
U ∈St(k,d)

k
(cid:88)

i=1

u(cid:48)

iMiui,

(1)

where St(k, d) = {U ∈ Rd×k : U (cid:48)U = Ik} denotes the Stiefel manifold. This problem arises in modern
signal processing and machine learning applications like heteroscedastic probabilistic principal component
analysis (HPPCA) [24], heterogeneous clutter in radar sensing [38], and robust sparse PCA [14]. Each of
these applications involves learning a signal subspace for data possessing heterogeneous statistics.

In particular, HPPCA [24] models data collected from sources of varying quality with diﬀerent addi-
tive noise variances, and estimates the best approximating low-dimensional subspace by maximizing the
likelihood, providing superior estimation compared to standard PCA. Speciﬁcally, given L data groups
[Y1, . . . , YL] with Y(cid:96) ∈ Rd×n(cid:96), second-order statistics A(cid:96) := 1
(cid:96) (cid:23) 0 for (cid:96) ∈ [L], and known positive
v(cid:96)
weights w(cid:96),i for ((cid:96), i) ∈ [L] × [k], a subproblem of HPPCA involves optimizing the sum of Brockett cost
functions [2, Section 4.8] with respect to a k-dimensional orthonormal basis U , and can be equivalently
recast in the form (1) as follows:

Y(cid:96)Y (cid:48)

max
U :U (cid:48)U =I

L
(cid:88)

(cid:96)=1

tr(U (cid:48)A(cid:96)U W(cid:96)) = max

U :U (cid:48)U =I

L
(cid:88)

k
(cid:88)

(cid:96)=1

i=1

u(cid:48)

iw(cid:96),iA(cid:96)ui = max

U :U (cid:48)U =I

k
(cid:88)

i=1

u(cid:48)

iMiui,

(2)

where W(cid:96) := diag({w(cid:96),i}k
(cid:96)=1 w(cid:96),iA(cid:96) for all i. Other sensing problems such as
independent component analysis (ICA) [40] and approximate joint diagonalization (AJD) [33] also model

i=1) for all (cid:96) and Mi := (cid:80)L

1We note here that “heterogeneous” refers to the fact the Mi are distinct and the problem is not separable in each ui.

Indeed, the objective in (1) is a homogeneous polynomial in the entries of U since all terms are degree 2.

2

data with heterogeneous statistics and optimize objective functions of a similar form, as we discuss more in
Section 3.

For (2), the case of a single Brockett cost function (L = 1) has known analytical solutions obtained by the
SVD or eigendecomposition [2, Section 4.8], whereas analytical solutions are not known for L ≥ 2. Indeed,
for L ≥ 2 and general A(cid:96), few, if any, guarantees for optimal recovery exist except in special cases, such as
when the constructed Mi commute [7]. Generally speaking, existing theory only gives restrictive suﬃcient
conditions for global optimality that are typically diﬃcult to check in practice; see [35], for example. Given
that (1) is nontrivial and challenging in several ways—nonconvex due to the Stiefel manifold constraint, non-
separable because of the weighted sum of objectives, and not readily solved by singular value or eigenvalue
decomposition—many works apply iterative local solvers to the nonconvex problem (1).

However, given the nonconvexity of (1), it can be unclear whether these local approaches ﬁnd a global
maximum. An alternative approach is to relax problems such as (1) to a semideﬁnite program (SDP), allowing
the use of standard convex solvers. While the SDP has stronger optimality guarantees, the challenge is then
to derive conditions under which the SDP is tight, i.e., returns the optimal solution to the original nonconvex
problem. SDP relaxations such as the “Fantope” [20; 31] exist for solving PCA-like problems, but to the
best of our knowledge, no previous convex methods exist to solve (1).

The main contribution of this paper is a novel convex SDP relaxation of (1), whose constraint set is
related to the Fantope but distinctly unique. By studying this SDP and its optimality criteria, we derive
suﬃcient conditions to certify the global optimality of a local stationary point, e.g., a candidate solution
obtained from any iterative solver for the nonconvex problem. We then propose a straightforward method
to certify global optimality by solving a much smaller SDP feasibility problem that scales favorably with
the problem dimensions. Our work also generalizes existing results for (1) with commuting matrices to
the case with “almost commuting” matrices, showing that as long as the data matrices are within an open
neighborhood of a commuting tuple of data matrices (to be deﬁned precisely in Section 4.2), the SDP is
tight and provably recovers an optimal solution of (1).

Notation We use italic, boldface, upper case letters A to denote matrices, v to denote vectors, and c for
scalars. We denote the cone of d × d-size symmetric positive semideﬁnite matrices as Sd
+, and use A (cid:23) 0 to
denote an element A ∈ Sd
+. We denote the Hermitian transpose of a matrix as A(cid:48), the trace of a matrix
as tr(A), and the inner product of matrices (with compatible inner dimensions) (cid:104)A, B(cid:105) := tr(A(cid:48)B). The
spectral norm of a matrix is denoted by (cid:107)A(cid:107) and the Frobenius norm by (cid:107)A(cid:107)F . The identity matrix of size
d is denoted as Id. Notation i ∈ [k] means i = 1, . . . , k.

2 Semideﬁnite program relaxation

By relaxing the considered nonconvex problem (1) to a convex one, the well-established principles of convex
optimization permit us to study when an optimal solution of the SDP relaxation recovers a global maximum
of (1) and importantly, when a given local stationary point is a global maximum. After re-expressing the
original problem using equivalent constraints, we lift the variables into the cone of PSD matrices, relax the
nonconvex constraints to convex surrogates, and obtain an SDP.

First, we begin by slightly rewriting (1) and the Stiefel manifold constraints as

max
u1,...,uk

tr

(cid:32) k

(cid:88)

i=1

(cid:33)

Miuiu(cid:48)
i

s.t. tr (uiu(cid:48)

i) = 1 ∀i ∈ [k],

tr (uju(cid:48)

i) = 0 ∀i (cid:54)= j.

(3)

Letting Xi = uiu(cid:48)

i ∈ Rd×d, this is equivalent to the lifted problem:

max
X1,...,Xk

tr

(cid:32) k

(cid:88)

i=1

(cid:33)

MiXi

s.t. λj

(cid:32) k

(cid:88)

(cid:33)

Xi

∈ {0, 1} ∀j ∈ [d]

i=1
tr(Xi) = 1,

rank(Xi) = 1, Xi (cid:60) 0 ∀i ∈ [k],

(4)

3

where λj(·) indicates the j-th eigenvalue of its argument. Note that this problem is nonconvex due to the
rank constraint and the constraint that the eigenvalues are binary. Similar to the relaxations in [41; 30], we
relax the eigenvalue constraint in (4) to 0 (cid:52) (cid:80)k
i=1 Xi (cid:52) I and remove the rank constraint, which yields the
SDP relaxation we consider throughout the remainder of this work.

p∗ = min

X1,...,Xk

− tr

(cid:33)

MiXi

s.t.

(cid:32) k

(cid:88)

i=1

k
(cid:88)

i=1

Xi (cid:52) I,

tr(Xi) = 1, Xi (cid:60) 0

i = 1, . . . , k.

(SDP-P)

Note that 0 (cid:52) (cid:80)k
i=1 Xi can be omitted since it is already satisﬁed when Xi (cid:60) 0 for all i. The feasible set
of (SDP-P) is closely related to the convex set found in [41; 30; 21] called the Fantope. The Fantope is the
convex hull of all matrices U U (cid:48) ∈ Rd×d such that U ∈ Rd×k and U (cid:48)U = I [20; 31]. Indeed, our relaxation
can be viewed as providing a decomposition of the Fantope variable X into the sum X1 + · · · + Xk such
that each Xi satisﬁes tr(Xi) = 1 and 0 (cid:22) Xi (cid:22) I. This decomposition allows (SDP-P) to capture the exact
form of the objective function, which sums the individual terms tr(MiXi). Precisely, the feasible set of
(SDP-P) is a convex relaxation of the set {(u1u(cid:48)
k) : U (cid:48)U = I}. Naturally, one wonders whether
1, . . . , uku(cid:48)
our relaxation always solves the original nonconvex problem. We show in Appendix C that it does not, using
a counter-example. Our work therefore studies this SDP in two ways: First, we provide a global optimality
certiﬁcate. Second, we study a class of “close-to jointly diagonalizable” problem instances, which includes
the heteroscedastic PCA problem, and show that the SDP is tight for this class.

For dual variables Zi ∈ Sd

+, Y ∈ Sd

+, ν ∈ Rk, the dual of (SDP-P), which will play a central role in

the theory of this paper, is

d∗ = min
Y ,Zi,ν

tr(Y ) +

k
(cid:88)

i=1

νi

s.t. Y (cid:60) 0, Y = Mi + Zi − νiI, Zi (cid:60) 0 ∀i ∈ [k].

(SDP-D)

Deﬁnition 2.1 (Rank-one property (ROP)). A solution to (SDP-P) is said to have the rank-one property
if X1, . . . , Xk are all rank-one.

We note that if a solution has the rank-one property, the ﬁrst singular vectors of the Xi are necessarily
i Xi is a projection matrix, due to the constraint (cid:80)

mutually orthogonal, and (cid:80)

i Xi (cid:52) I.

2.1 Strong duality and equivalence of optimizers

Our theoretical results require the following lemmas, whose proofs can be found in Appendix B.

Lemma 2.2. Strong duality holds for the SDP relaxation with primal (SDP-P) and dual (SDP-D).

Lemma 2.3. The solution to the SDP relaxation in (SDP-P) is the optimal solution to the original nonconvex
problem in (1) (equivalently (4)) if and only if the optimal Xi have the rank-one property.

Lemma 2.4. Assume Mi are PSD. Then the optimal νi ≥ 0.

Lemma 2.5. If the optimal dual variables Zi for i = 1, . . . , k are each rank d − 1, the optimal solution
variables Xi have the rank-one property.

3 Related work

There are a few important related works on the objective in (1), as well as many more related works than
can be reviewed here, including ones on eigenvalue/eigenvector problems and the many variations thereof,
low-rank SDPs, nonconvex quadratics where Mi are not PSD, etc. Appendix A in the supplement provides
an extensive related work section. Here, we focus on the works most directly related to (1).

Bolla et al. [7], Rapcs´ak [35], and Berezovskyi [6] previously investigated the sum of heterogeneous
quadratics in (1). The work in [7] only studied the structure of this problem when the matrices Mi were

4

commuting. Rapcs´ak [35] derived suﬃcient second-order global optimality conditions, but these conditions
are diﬃcult to check in general and do not seem to hold for the heteroscedastic PCA problem. Works such as
Huang and Palomar [25] and Pataki [32] consider a very similar problem to (2), but without the eigenvalue
constraint in (4), making their SDP a rank-constrained separable SDP; see also Luo et al. [30, Section 4.3].
Pataki studied upper bounds on the rank of optimal solutions of general SDPs, but in the case of (SDP-P),
since our problem introduces the additional constraint summing the Xi, Pataki’s bounds do not guarantee
rank-one, or even low-rank, optimal solutions.

Recent works have also studied convex relaxations of PCA and other low-rank subspace problems that
seek to bound the eigenvalues of a single matrix [41; 39; 44], rather than the sum of multiple matrices as in
our setting. The works in [10; 34] prove global convergence of nonconvex Burer–Monteiro factorization [16]
approaches to solve low-rank semideﬁnite programs without orthogonality constraints. Other works have
studied optimizers of the nonconvex problem, like those in [14; 13; 38; 12; 24], using minorize-maximize
or Riemannian gradient ascent algorithms, which do not come with optimality guarantees. Our problem
also has interesting connections to approximate joint diagonalization (AJD), which is well-studied and often
applied to blind source separation or independent component analysis (ICA) problems [40; 8; 26; 3; 37]. See
Appendix A of the supplement for further details.

4 Theoretical Results
4.1 Dual certiﬁcate of the SDP
In practical settings for high-dimensional data, a variety of iterative local methods are often applied to
solve nonconvex problems over the Stiefel manifold, from gradient ascent by geodesics [2; 18; 1] and more
recently majorization-minimization (MM) algorithms, where Breloy et al. [14] applied MM methods to solve
(1) with guarantees of convergence to a stationary point. While the computational complexity and memory
requirements of these solvers scale well, their obtained solutions lack any global optimality guarantees. Our
contribution seeks to ﬁll this gap by proposing a check for global optimality of a local solution.

By Lemma 2.3, an optimal solution of (SDP-P) with rank-one matrices Xi globally solves the original
nonconvex problem (1). In this section, given a candidate stationary point ¯U = [ ¯u1 · · · ¯uk] ∈ St(k, d) to (1),
we investigate conditions guaranteeing that the rank-one matrices ¯Xi = ¯ui ¯u(cid:48)
i, which are feasible for (SDP-
P), in fact comprise an optimal solution of (SDP-P), implying that ¯U optimizes (1). Similar to [43; 19] for
Fantope problems, our results yield a dual SDP certiﬁcate to verify the primal optimality of the candidates
¯X1, . . . , ¯Xk constructed from a local solution ¯U . We show our certiﬁcate scales favorably in computation
over the full SDP, with the most complicated computations of our algorithm requiring us to solve a feasibility
problem in k variables with d × d-size linear matrix inequalities (LMI).

Theorem 4.1. Let ¯U ∈ St(k, d) be a local maximizer to (1), and let ¯Λ = (cid:80)k
i=1
and ei is the ith standard basis vector in Rk. If there exist ¯ν = [¯ν1 · · · ¯νk] ∈ Rk
+ such that

¯U (cid:48)Mi ¯U Ei, where Ei (cid:44) eie(cid:48)
i

¯U ( ¯Λ − D ¯ν ) ¯U (cid:48) + ¯νiI − Mi (cid:23) 0 ∀i = 1, . . . , k

¯Λ − D ¯ν (cid:23) 0,

(5)

where D ¯ν := diag(¯ν1, . . . , ¯νk), then ¯U is an optimal solution to (SDP-P) and a globally optimal solution to
the original nonconvex problem (1).

In light of Theorem 4.1, to test whether a candidate stationary point ¯U is globally optimal, we simply
assess whether system (5) is feasible using an LMI solver. If it is indeed feasible, then ¯U is globally optimal.
On the other hand, if (5) is infeasible, it indicates one of two things: 1) The SDP does not return tight,
rank-one solutions ¯Xi, and the SDP strictly upper bounds the original problem on the Stiefel manifold. The
candidate stationary point ¯U may or may not be globally optimal to the original nonconvex problem. 2)
The SDP is tight, but the candidate stationary point ¯U is a suboptimal local maximum. The proof, found
in Appendix D, constructs dual variables and veriﬁes the KKT conditions. Appendix H also describes an
extension of the certiﬁcate to the sum of Brocketts with additive linear terms.

5

Arithmetic complexity While SDP relaxations of nonconvex optimization problems can provide strong
provable guarantees, their practicality can be limited by the time and space required to solve them, particu-
larly when using oﬀ-the-shelf interior-point solvers, which in our case require O(d3) [5] storage and ﬂoating
point operations (ﬂops) per iteration. In contrast, the SDP relaxation admits improved practical tools to
transfer theoretical guarantees to the nonconvex setting. The proposed global certiﬁcate signiﬁcantly reduces
the number of variables from O(d2) in (SDP-D) (upon eliminating the variables Zi) to merely k variables in
(5). Using [4, Section 6.6.3] it can be shown that computing the certiﬁcate only, with a given ¯U , results in a
substantial reduction in ﬂops by a factor of O(d3/k) over solving (SDP-D). Subsequently, a ﬁrst-order MM
solver [14], whose cost is O(dk2 + k3) per iteration, combined with our global optimality certiﬁcate, is an
obvious preference to solving the full SDP in (SDP-P) for large problems. See Appendix D.1 for more details.

4.2 SDP tightness in the close-to jointly diagonalizable (CJD) case

While Section 4.1 provides a technique to certify the global optimality of a solution to the nonconvex
problem, the check will fail if the point is not globally optimal or if the SDP is not tight. General conditions
on Mi that guarantee tightness of (SDP-P) are still not known. However, when the matrices Mi are jointly
diagonalizable, our problem reduces to a linear programming (LP) assignment problem, and by standard
LP theory, a solution with rank-one Xi exists and the SDP (or equivalent LP) is a tight relaxation [7]. Our
next major contribution is to show that a solution with rank-one Xi exists also for cases that are close-to
jointly diagonalizable (CJD). We ﬁrst give a continuity result showing there is a neighborhood around the
diagonal case for which (SDP-P) is still tight. Then we show that for the HPPCA problem, the matrices
Mi are close-to diagonalizable and can be made arbitrarily close as the number of data points n grows or
the noise levels diminish. This gives strong support for tightness of the SDP for the HPPCA problem when
n is large or the noise levels are small.

Deﬁnition 4.2 (Close-to jointly diagonalizable (CJD)). We say that unit spectral-norm matrices A and
B are CJD if they are almost commuting, that is, when the commuting distance A and B, (cid:107)[A, B](cid:107) :=
(cid:107)AB − BA(cid:107) ≤ δ for 0 < δ (cid:28) 1.

4.2.1 Continuity and tightness in the CJD case

In this section, we employ a technical continuity result for the dual feasible set to conclude that there
is a neighborhood of problem instances around every diagonal instance for which (SDP-P) gives rank-one
solutions Xi. All proofs for results in this subsection are found in Appendix E.

Given a k-tuple of symmetric matrices (M1, . . . , Mk), our primal-dual pair is given by (SDP-P) and
(SDP-D). Note that, without loss of generality, we may assume each Mi is positive semideﬁnite since the
primal constraint tr(Xi) = 1 ensures that replacing Mi by Mi + λiI (cid:23) 0, where λi is a positive constant,
simply shifts the objective value by λi. In addition, we have shown in Lemma 2.4 that Mi (cid:23) 0 implies νi is
nonnegative at dual optimality. So we assume Mi (cid:23) 0 and enforce νi ≥ 0 for all i = 1, . . . , k.

For a ﬁxed, user-speciﬁed upper bound µ > 0, we deﬁne the closed convex set

C := {c = (M1, . . . , Mk) : 0 (cid:22) Mi (cid:22) µI ∀ i = 1, . . . , k},

to be our set of admissible coeﬃcient k-tuples. We know that both (SDP-P) and (SDP-D) have interior
points for all c ∈ C, so that strong duality holds.

Lemma 4.3. Let c = (M1, . . . , Mk) ∈ C. If Mi are jointly diagonalizable for i = 1, . . . , k and (SDP-P) has
a unique optimal solution, then there exists an optimal solution of (SDP-D) with rank(Zi) ≥ d − 1 for all
i = 1, . . . , k.

Deﬁnition 4.4. For c = (M1, . . . , Mk) ∈ C and ¯c = ( ¯M1, . . . , ¯Mk) ∈ C, deﬁne dist(c, ¯c) (cid:44) maxi∈[k] (cid:107)Mi −
¯Mi(cid:107).

We are now ready to state our main result in this subsection.

6

Theorem 4.5. Let ¯c := ( ¯M1, . . . , ¯Mk) ∈ C be given such that ¯Mi, i = 1, . . . , k, are jointly diagonalizable
and the primal problem (SDP-P) with objective coeﬃcients ¯c has a unique optimal solution. Then there
exists a full-dimensional neighborhood ¯C (cid:51) ¯c in C such that (SDP-P) has the rank-one property for all
c = (M1, . . . , Mk) ∈ ¯C.
Proof Sketch. Using Lemma 4.3, let y0 := ( ¯Y , ¯Zi, ¯νi) be the optimal solution of the dual problem (SDP-D)
for ¯c = ( ¯M1, . . . , ¯Mk), which has rank( ¯Zi) ≥ d − 1 for all i. Proposition E.3 in Appendix E considers a
function y(c; y0) that returns the optimal solution of (SDP-D) for c = (M1, . . . , Mk) closest to y0, and
shows that function is continuous. It follows that its preimage

y−1 ({(Y , Zi, νi) : rank(Zi) ≥ d − 1 ∀ i})

contains ¯c and is an open set because the set of all (Y , Zi, νi) with rank(Zi) ≥ d − 1 is an open set. After
intersecting with C, we have shown existence of this full-dimensional set ¯C. From complementarity of the
KKT conditions of the assignment LP, rank(Zi) = d−1 for i = 1, . . . , k. Applying Lemma 2.5 then completes
the theorem.

The next corollary shows that for a general tuple of almost commuting matrices c := (M1, . . . , Mk) that
are CJD, (SDP-P) is tight and has the rank-one property. In the following results, we will then prove the
HPPCA generative model for Mi results in a problem that is CJD. While these are suﬃcient conditions,
they are by no means necessary, and Appendix F gives an example of Mi that are not CJD but where the
convex relaxation has the rank-one property.

Corollary 4.5.1. Assume (cid:107)Mi(cid:107) ≤ 1 for all i ∈ [k], and let c := (M1, . . . , Mk). Suppose (cid:107)[Mi, Mj](cid:107) :=
(cid:107)MiMj − MjMi(cid:107) ≤ δ for all i, j ∈ [k]. Then there exists ¯c := ( ¯M1, . . . , ¯Mk) of commuting, jointly-
diagonalizable matrices such that (cid:107)[ ¯Mi, ¯Mj](cid:107) = 0 for all i, j ∈ [k] where dist(c, ¯c) ≤ O((cid:15)(δ)) and (cid:15)(δ) is a
function satisfying limδ→0 (cid:15)(δ) = 0. If δ is small enough, there exists (cid:15) > (cid:15)(δ) > 0 such that dist(c, ¯c) ≤ (cid:15)
implies c ∈ ¯C and (SDP-P) has the rank-one property.

4.2.2 HPPCA is CJD

Consider the heteroscedastic probabilistic PCA problem in [24] where L data groups of n1, . . . , nL samples
(n = (cid:80)L

(cid:96)=1 n(cid:96)) with known noise variances v1, . . . , vL respectively are generated by the model

y(cid:96),i = U Θz(cid:96),i + η(cid:96),i ∈ Rd ∀(cid:96) ∈ [L], i ∈ [n(cid:96)].
√

√

(6)

λk) represent the known signal amplitudes,
Here, U ∈ St(k, d) is a planted subspace, Θ = diag(
iid∼ N (0, Ik) are latent variables, and η(cid:96),i
iid∼ N (0, v(cid:96)Id) are additive Gaussian heteroscedastic noises.
z(cid:96),i
Assume that λi (cid:54)= λj for i (cid:54)= j ∈ [k] and v(cid:96) (cid:54)= vm for (cid:96) (cid:54)= m ∈ [L]. The maximum likelihood problem in [24,
Equation 3] with respect to U is then equivalently (2) for A(cid:96) = (cid:80)n(cid:96)
∈ (0, 1].
Our next result says that, as the number of samples n grows or the signal-to-noise ratio λi/v(cid:96) grows, the
matrices in the HPPCA problem are almost commuting. The proof is found in Appendix E.

(cid:96),i and w(cid:96),i = λi
λi+v(cid:96)

λ1, . . . ,

y(cid:96),iy(cid:48)

1
v(cid:96)

i=1

Proposition 4.6. Let c = ( 1
n Mk) be the (normalized) data matrices of the HPPCA problem.
Then there exists commuting ¯c = ( ¯M1, . . . , ¯Mk) (constructed in the proof ) such that, for a universal constant
C > 0 and with probability exceeding 1 − e−t for t > 0,

n M1, . . . , 1

(cid:107)Mi − ¯Mi(cid:107)
(cid:107) ¯M1(cid:107)

≤ min




L
(cid:88)



(cid:96)=1

1
+ 1

, C

λi
v(cid:96)




(cid:115) ¯ξi
¯σi



max

log d + t

n

¯ξi
¯σi

,

log d + t

n

log(n)











, where

(7)

¯σi
¯σ1

(cid:19)

¯σi = (cid:107) ¯Mi(cid:107) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(cid:18) λ1
v(cid:96)

+ 1

and

¯ξi = tr( ¯Mi) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(cid:32)

1
v(cid:96)

k
(cid:88)

i=1

(cid:33)

λi + d

.

7

5 Numerical experiments

All of the following numerical experiments were computed using MATLAB R2018a on a MacBook Pro with
a 2.6 GHz 6-Core Intel Core i7 processor. When solving SDPs, we use the SDPT3 solver of the CVX package
in MATLAB [23]. All code necessary to reproduce our experiments is available at https://github.com/
kgilman/Sums-of-Heterogeneous-Quadratics. When executing each algorithm in practice, we remark
that the results may vary with the choice of user speciﬁed numerical tolerances and other settings. We point
the reader to Appendix G for detailed settings.

5.1 Assessing the rank-one property (ROP)
In this subsection, we conduct experiments showing that, for many random instances of the HPPCA ap-
plication, the SDP relaxation (SDP-P) is tight with rank-one orthonormal primal solutions Xi, yielding a
globally optimal solution of (1). Similar experiments for other forms of (1), including where Mi are random
low-rank PSD matrices, are found in Appendix G and give similar insights. Here, the Mi are generated
according to our HPPCA model in (6) where A(cid:96) = 1
(cid:96),i and weight matrices W(cid:96) are calculated
v(cid:96)
as W(cid:96) = diag(w(cid:96),1, . . . , w(cid:96),k) for w(cid:96),i = λi/(λi + v(cid:96)). We make λ a k-length vector of entries uniformly
spaced in the interval [1, 4], and vary ambient dimension d, rank k, samples n, and variances v for L = 2
noise groups. Each random instance is generated from a new random draw of U on the Stiefel manifold,
latent variables z(cid:96),i, and noise vectors η(cid:96),i.

i=1 y(cid:96),iy(cid:48)

(cid:80)n(cid:96)

Tables 1 and 2 show the results of these experiments for various choices of dimension d and rank k. We
solve the SDP for 100 random data instances in Matlab CVX. The table shows the fraction of trials that result
in rank-one Xi with ﬁrst eigenvectors orthogonal across i = 1, . . . , k. We compute the average error of the
sorted eigenvalues of each ¯Xi to e1, i.e. 1
i=1 (cid:107) diag(Σi) − e1(cid:107)2
i , and count any trial
k
with error greater than 10−5 as not tight. The SDP solutions possess the ROP in the vast majority of trials.
As the Mi concentrate to be almost commuting with increased sample sizes, the convex relaxation becomes
tight in 100% of the trials, as shown in Table 1. Similarly, as we decrease the spread of the variances, Table 2
shows the fraction of tight instances increases, reaching 100% in the homoscedastic setting, as expected.

2 where ¯Xi = ViΣiV (cid:48)

(cid:80)k

Fraction of 100 trials with ROP
k = 7
k = 3
1
1
0.98
1
0.98
0.99
0.99
0.98
0.96
0.97
1
1
1
1
1
1
0.97
1
0.98
1
1
1
1
1
1
1
1
1
1
1

k = 10
1
0.99
0.97
0.98
0.98
1
1
0.98
0.95
0.97
1
1
1
1
1

k = 5
0.99
0.98
0.93
0.91
0.95
1
1
1
1
0.98
1
1
1
1
1

=
n

] d = 10
0
d = 20
2
,
5
d = 30
[
d = 40
d = 50
] d = 10
0
8
d = 20
,
0
d = 30
2
[
d = 40
d = 50
d = 10
d = 20
d = 30
d = 40
d = 50

]
0
0
4
,
0
0
1
[

=
n

=
n

Fraction of 100 trials with ROP
k = 7
k = 3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0.99
1
1
1
1
1
1
1
0.97
0.99
0.97
1
0.96
1

k = 10
1
1
1
1
1
1
1
1
1
0.99
1
1
0.99
0.99
0.98

k = 5
1
1
1
1
1
1
1
0.98
1
1
1
1
0.99
0.98
0.97

=
v

2
,
1
[

1
,
1
[

] d = 10
d = 20
d = 30
d = 40
d = 50
] d = 10
d = 20
d = 30
d = 40
d = 50
] d = 10
d = 20
d = 30
d = 40
d = 50

3
,
1
[

=
v

=
v

Table 1:
(HPPCA) Numerical experiments
showing the fraction of trials where the SDP was
tight for instances of the HPPCA problem as we
vary d, k, and n using L = 2 groups with noise
variances v = [1, 4].

Table 2:
(HPPCA) Numerical experiments
showing the fraction of trials where the SDP was
tight for instances of the HPPCA problem as we
vary d, k, and v using L = 2 groups with samples
n = [10, 40].

8

5.2 Assessing global optimality of local solutions
In this section, we use the Stiefel majorization-minimization (StMM) solver with a linear majorizer from
Breloy et al. [14] to obtain a local solution ¯UMM to (1) for various inputs Mi and use Theorem 4.1 to certify
the local solution either as globally optimal or as a non-optimal stationary point. For comparison, we obtain
candidate solutions ¯Xi from the SDP and perform a rank-one SVD of each to form ¯USDP, i.e.

¯USDP = PSt([ ¯u1 · · · ¯uk]),

¯ui = argmax
u:(cid:107)u(cid:107)2=1

u(cid:48) ¯Xiu,

while measuring how close the solutions are to being rank-one. In the case the SDP is not tight, the rank-one
directions of the Xi will not be orthonormal, so as a heuristic, we project ¯USDP onto the Stiefel manifold
by the orthogonal Procrustes solution, denoted by the operator PSt(·) [14].

Synthetic CJD matrices To empirically verify our theory from Section 4, we generate each Mi ∈ Rd×d
to be a diagonally dominant matrix resembling an approximately rank-r sample covariance matrix, such
that, in a similar manner to HPPCA, M1 (cid:23) M2 (cid:23) · · · (cid:23) Mk (cid:23) 0. Speciﬁcally, we ﬁrst construct Mk =
Dk + Nk, where Dk is a diagonal matrix with r nonzero entries drawn uniformly at random from [0, 1], and
10d SS(cid:48) for S ∈ Rd×10d whose entries are drawn i.i.d. as N (0, σI). We then generate the remaining
Nk = 1
Mi for i = k − 1, . . . , 1 as Mi = Mi−1 + Di + Ni with new random draws of Di and Ni and normalize
all by 1/ maxi∈[k] (cid:107)Mi(cid:107) so that (cid:107)Mi(cid:107) ≤ 1. With this setup, when we sweep σ, we sweep through a range
of commuting distances, i.e. maxi,j∈[k] (cid:107)MiMj − MjMi(cid:107). For all experiments, we generate problems with
parameters d = 10, k = 3, r = 3, and run StMM for 2000 maximum iterations or until the norm of the
gradient on the manifold is less than 10−10.

Fig. 1a shows the gap of the objective values between the SDP relaxation (before projection onto the
Stiefel) and the nonconvex problem (pSDP − pStMM) versus the commuting distance. Fig. 1b shows the
¯USDP| − I(cid:107)F (where | · | denotes taking
distance between the two obtained solutions computed as 1√
k
the elementwise absolute value) versus commuting distance. Fig. 1c shows the percentage of trials where
¯UStMM could not be certiﬁed globally optimal. Like before, we declare an SDP’s solution “tight” if the mean
error of its solutions to rank-one approximations is less than 10−5. Trials with the marker “◦” indicate trials
where global optimality was certiﬁed. The marker “×” represents trials where ¯U was not certiﬁed as globally
optimal and the SDP relaxation was not tight; “(cid:52)” markers indicate trials where the SDP was tight, but
(5) was not satisﬁed, implying a suboptimal local maximum.

(cid:107)| ¯U (cid:48)

StMM

Towards the left of Fig. 1a, with small σ and the (M1, . . . , Mk) all being very close to commuting, 100%
of experiments return tight rank-one SDP solutions. Interestingly, there appears to be a sharp cut-oﬀ point
where this behavior ends, and the SDP relaxation is not tight in a small percentage of cases. While the large
majority of trials still admit a tight convex relaxation, these results empirically corroborate the suﬃcient
conditions derived in Theorem 4.5 and Corollary 4.5.1.

Where the SDP is tight, Fig. 1 shows the StMM solver returns the globally optimal solution in more
than 95% of the problem instances. Indicated by the “(cid:52)” markers, the remaining cases can only be certiﬁed
as stationary points, implying a local maximum was found. Indeed, we observe a correspondence between
trials with both large objective value gap and distance of the candidate solution to the globally optimal
solution returned by the SDP.

HPPCA We repeat the experiments just described for Mi generated by the model in (6) for d = 50,
λ = [4, 3.25, 2.5, 1.75, 1], and L = 2 noise groups with variances v = [1, 4]. We draw 100 random models
with a diﬀerent generative U for sample sizes n = [n1, 4n1], where we sweep through increasing values of n1
on the horizontal axis in Fig. 2c, drawing 100 diﬀerent random data initializations. For each experiment, we
normalize the Mi by the maximum of their spectral norms, and then record the results obtained from the
SDP and StMM solvers with respect to the computed maximum commuting distance of the Mi in Fig. 2.
We run StMM for a maximum of 10,000 iterations, and record whether the SDP was tight and the global
optimality certiﬁcation of each StMM run.

Proposition 4.6 suggests that, even with poor SNR like in this example, as the number of data samples
increases, the Mi should concentrate to be nearly commuting. This is indeed what we observe: as the number

9

(a)

(b)

(c)

Figure 1: Numerical simulations for synthetic CJD matrices for d = 10, k = 3 with increasing σ.

(a)

(b)

(c)

Figure 2: Numerical simulations for Mi generated by the HPPCA model in (6) for d = 50, k = 5, noise
variances [1, 4], and λ = [4, 3.25, 2.5, 1.75, 1] with increasing samples n.

of samples increases, the maximum commuting distance on the horizontal axis of Figs. 2a and 2b decreases.
In this nearly-commuting regime, the SDP obtains tight rank-one Xi in 100% of the trials, and interestingly,
all of the StMM runs attain the global maximum, suggesting a seemingly benign nonconvex landscape. In
contrast, we observed several trials in the low-sample setting where the SDP failed to be tight and the dual
certiﬁcate was null. Also within this regime, several trials of the StMM solver ﬁnd suboptimal local maxima.

5.3 Computation time
Fig. 3 compares the scalability of our SDP relaxation in (SDP-P) to the StMM solver with the global
certiﬁcate check in (5) for synthetically generated HPPCA problems of varying data dimension. We measure
the median computation time across 10 independent trials of both algorithms. The experiment strongly
demonstrates the computational superiority of the ﬁrst-order method with our certiﬁcate compared to the
full SDP. StMM+Certiﬁcate scales nearly 60 times better in computation time for the largest dimension
with k = 3 and 15 times for k = 10, while oﬀering a crucial theoretical guarantee to a nonconvex problem
that may contain spurious local maxima. Thus, we can solve the nonconvex problem posed in (1) using any
choice of solver on the Stiefel manifold and perform a fast check of its terminal output for global optimality.

6 Future Work & Conclusion
In this work, we proposed a novel SDP relaxation for the sums of heterogeneous quadratics problem, from
which we derived a global optimality certiﬁcate to check a local solution of a nonconvex program. Our other
major contribution proved a continuity result showing suﬃcient conditions when the relaxation has the ROP

10

Figure 3: Computation time of (SDP-P) versus StMM for 2000 iterations with global certiﬁcate check (5)
for HPPCA problems as the data dimension varies. We use v = [1, 4], and n = [100, 400] and make λ a
k-length vector with entries equally spaced in the interval [1, 4]. Markers indicate the median computation
time taken over 10 trials, and error bars show the standard deviation.

and provided both theoretical and empirical support that one motivating signal processing application–the
HPPCA problem–possesses a tight relaxation in many instances.

While the global certiﬁcate check we propose scales well compared to solving the full SDP, the LMI
feasiblity program still requires forming and factoring d×d size matrices, requiring storage of O(d2) elements.
One exciting possibility is to apply recent works like [45] to our problem, which use randomized algorithms to
reduce the storage and arithmetic costs for scalable semideﬁnite programming. Further, it remains a strong
interest to prove a suﬃcient analytical certiﬁcate, in addition to proving more general suﬃcient conditions
on the Mi to guarantee the ROP.

While we hope the work herein has a positive impact in HPPCA applications like air quality monitoring
[24] or medical imaging, we acknowledge the potential for dimensionality reduction algorithms to yield
disparate reconstruction errors on populations within a dataset, such as PCA on labeled faces in the wild
data set (LFW), which returns higher reconstruction error for women than men even with equal population
ratios in the dataset [36]; also see [39].

Acknowledgments

The authors would like to thank Nicolas Boumal for his helpful discussions, references, and notes relating to
dual certiﬁcates of low-rank SDP’s and manifold optimization. They would also like to thank David Hong
and Jeﬀrey Fessler for their feedback on this paper and their discussions relating to heteroscedastic PPCA.

References

[1] Traian E. Abrudan, Jan Eriksson, and Visa Koivunen. Steepest descent algorithms for optimization
under unitary matrix constraint. IEEE Transactions on Signal Processing, 56(3):1134–1147, 2008. doi:
10.1109/TSP.2007.908999.

[2] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization Algorithms on Matrix Manifolds. Princeton

University Press, Princeton, NJ, 2008. ISBN 978-0-691-13298-3.

11

[3] Bijan Afsari. Sensitivity analysis for the problem of matrix joint diagonalization. SIAM Journal on
Matrix Analysis and Applications, 30(3):1148–1171, 2008. doi: 10.1137/060655997. URL https://doi.
org/10.1137/060655997.

[4] Aharon Ben-Tal and Arkadi Nemirovski. Lectures on Modern Convex Optimization. Society for Indus-
trial and Applied Mathematics, 2001. doi: 10.1137/1.9780898718829. URL https://epubs.siam.org/
doi/abs/10.1137/1.9780898718829.

[5] Aharon Ben-Tal and Arkadi Nemirovski. Lectures on modern convex optimization. MPS/SIAM Series
on Optimization. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA; Mathe-
matical Programming Society (MPS), Philadelphia, PA, 2001. ISBN 0-89871-491-5. doi: 10.1137/1.
9780898718829. URL https://doi-org.proxy.lib.uiowa.edu/10.1137/1.9780898718829. Analy-
sis, algorithms, and engineering applications.

[6] O. A. Berezovskyi. On the lower bound for a quadratic problem on the Stiefel manifold. Cybernetics
and Sys. Anal., 44(5):709–715, September 2008. ISSN 1060-0396. doi: 10.1007/s10559-008-9038-4. URL
https://doi.org/10.1007/s10559-008-9038-4.

[7] Marianna Bolla, Gy¨orgy Michaletzky, G´abor Tusn´ady, and Margit Ziermann. Extrema of sums of
heterogeneous quadratic forms. Linear Algebra and its Applications, 269(1):331 – 365, 1998.
ISSN
0024-3795. doi: https://doi.org/10.1016/S0024-3795(97)00230-9. URL http://www.sciencedirect.
com/science/article/pii/S0024379597002309.

[8] F. Bouchard, J. Malick, and M. Congedo. Riemannian optimization and approximate joint diagonaliza-
tion for blind source separation. IEEE Transactions on Signal Processing, 66(8):2041–2054, 2018. doi:
10.1109/TSP.2018.2795539.

[9] Florent Bouchard, Bijan Afsari, J´erˆome Malick, and Marco Congedo. Approximate joint diagonalization
with Riemannian optimization on the general linear group. SIAM Journal on Matrix Analysis and
Applications, 41, 01 2019. doi: 10.1137/18M1232838.

[10] Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira. The non-convex Burer-Monteiro ap-
In D. Lee, M. Sugiyama, U. Luxburg,
proach works on smooth semideﬁnite programs.
I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
ume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/
3de2334a314a7a72721f1f74a6cb4cee-Paper.pdf.

[11] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. doi:

10.1017/CBO9780511804441.

[12] Arnaud Breloy, Guillaume Ginolhac, Fr´ed´eric Pascal, and Philippe Forster. Clutter subspace estimation
in low rank heterogeneous noise context. IEEE Transactions on Signal Processing, 63(9):2173–2182,
2015. doi: 10.1109/TSP.2015.2403284.

[13] Arnaud Breloy, Guillaume Ginolhac, Fr´ed´eric Pascal, and Philippe Forster. Robust covariance matrix
estimation in heterogeneous low rank context. IEEE Transactions on Signal Processing, 64(22):5794–
5806, 2016. doi: 10.1109/TSP.2016.2599494.

[14] Arnaud Breloy, Sandeep Kumar, Ying Sun, and Daniel P. Palomar. Majorization-minimization on the
Stiefel manifold with application to robust sparse PCA. IEEE Transactions on Signal Processing, 69:
1507–1520, 2021. doi: 10.1109/TSP.2021.3058442.

[15] Roger W Brockett. Least squares matching problems. Linear algebra and its applications, 122:761–777,

1989.

[16] Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semideﬁnite

programs via low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.

12

[17] Samuel Burer, Kurt M. Anstreicher, and Mirjam D¨ur. The diﬀerence between 5 × 5 doubly nonnegative
and completely positive matrices. Linear Algebra Appl., 431(9):1539–1552, 2009. ISSN 0024-3795. doi:
10.1016/j.laa.2009.05.021.

[18] Alan Edelman, Tom´as A Arias, and Steven T Smith. The geometry of algorithms with orthogonality

constraints. SIAM journal on Matrix Analysis and Applications, 20(2):303–353, 1998.

[19] Ky Fan. On a theorem of Weyl concerning eigenvalues of linear transformations. Proceedings of the
National Academy of Sciences, 35(11):652–655, 1949. doi: 10.1073/pnas.35.11.652. URL https://www.
pnas.org/doi/abs/10.1073/pnas.35.11.652.

[20] P. A. Fillmore and J. P. Williams. Some convexity theorems for matrices. Glasgow Math. J., 12:110–117,

1971. ISSN 0017-0895. doi: 10.1017/S0017089500001221.

[21] Dan Garber and Ron Fisher. Eﬃcient algorithms for high-dimensional convex subspace optimization

via strict complementarity. arXiv preprint arXiv:2202.04020, 2022.

[22] Klaus Glashoﬀ and Michael M Bronstein. Almost-commuting matrices are almost jointly diagonalizable.

arXiv preprint arXiv:1305.2135, 2013.

[23] Michael Grant and Stephen Boyd. CVX: Matlab software for disciplined convex programming, version

2.1. http://cvxr.com/cvx, March 2014.

[24] David Ke Hong, Kyle Gilman, Laura Balzano, and Jeﬀrey A. Fessler. HePPCAT: Probabilistic PCA
for data with heteroscedastic noise. IEEE Transactions on Signal Processing, 69:4819–4834, 2021.

[25] Yongwei Huang and Daniel P Palomar. Rank-constrained separable semideﬁnite programming with

applications to optimal beamforming. IEEE Transactions on Signal Processing, 58(2):664–678, 2009.

[26] M. Kleinsteuber and H. Shen. Uniqueness analysis of non-unitary matrix joint diagonalization. IEEE

Transactions on Signal Processing, 61(7):1786–1796, 2013. doi: 10.1109/TSP.2013.2242065.

[27] Vladimir Koltchinskii and Karim Lounici. Concentration inequalities and moment bounds for sample
covariance operators. Bernoulli, 23(1):110–133, 2017. ISSN 13507265. doi: 10.3150/15-BEJ730.

[28] Terry A Loring and Adam PW Sørensen. Almost commuting self-adjoint matrices: the real and self-dual

cases. Reviews in Mathematical Physics, 28(07):1650017, 2016.

[29] Karim Lounici. High-dimensional covariance matrix estimation with missing observations. Bernoulli,
20(3):1029 – 1058, 2014. doi: 10.3150/12-BEJ487. URL https://doi.org/10.3150/12-BEJ487.

[30] Zhi-Quan Luo, Tsung-Hui Chang, DP Palomar, and YC Eldar. SDP relaxation of homogeneous
quadratic optimization: approximation. Convex Optimization in Signal Processing and Communica-
tions, page 117, 2010.

[31] Michael L. Overton and Robert S. Womersley. On the sum of the largest eigenvalues of a symmetric
matrix. SIAM J. Matrix Anal. Appl., 13(1):41–45, 1992. ISSN 0895-4798. doi: 10.1137/0613006.

[32] G. Pataki. On the rank of extreme matrices in semideﬁnite programs and the multiplicity of optimal

eigenvalues. Math. Oper. Res., 23:339–358, 1998.

[33] Dinh-Tuan Pham and Marco Congedo. Least square joint diagonalization of matrices under an in-
In T¨ulay Adali, Christian Jutten, Jo˜ao Marcos Travassos Romano, and Al-
trinsic scale constraint.
lan Kardec Barros, editors, ICA 2009 - 8th International Conference on Independent Component
Analysis and Signal Separation, volume 5441 of Lecture Notes in Computer Science, pages 298–
305, Paraty, Brazil, February 2009. Springer. doi: 10.1007/978-3-642-00599-2\ 38. URL https:
//hal.archives-ouvertes.fr/hal-00371941.

13

[34] Thomas Pumir, Samy Jelassi, and Nicolas Boumal.

Smoothed analysis of the low-rank ap-
proach for smooth semideﬁnite programs.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
ume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
a1d50185e7426cbb0acad1e6ca74b9aa-Paper.pdf.

[35] Tam´as Rapcs´ak. On minimization on Stiefel manifolds. European Journal of Operational Research, 143

(2):365–376, 2002.

[36] Samira Samadi, Uthaipon Tantipongpipat, Jamie H Morgenstern, Mohit Singh, and Santosh Vempala.
The price of fair pca: One extra dimension. Advances in neural information processing systems, 31,
2018.

[37] Xizhi Shi. Joint Approximate Diagonalization Method, pages 175–204. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2011. ISBN 978-3-642-11347-5. doi: 10.1007/978-3-642-11347-5 8. URL https:
//doi.org/10.1007/978-3-642-11347-5_8.

[38] Ying Sun, Arnaud Breloy, Prabhu Babu, Daniel P. Palomar, Fr´ed´eric Pascal, and Guillaume Ginol-
hac. Low-complexity algorithms for low rank clutter parameters estimation in radar systems. IEEE
Transactions on Signal Processing, 64(8):1986–1998, 2016. doi: 10.1109/TSP.2015.2512535.

[39] Uthaipon Tantipongpipat, Samira Samadi, Mohit Singh, Jamie H Morgenstern, and Santosh Vempala.
Multi-criteria dimensionality reduction with applications to fairness. Advances in neural information
processing systems, 32, 2019.

[40] Fabian J. Theis, Thomas P. Cason, and P. A. Absil. Soft dimension reduction for ICA by joint diago-
nalization on the Stiefel manifold. In T¨ulay Adali, Christian Jutten, Jo˜ao Marcos Travassos Romano,
and Allan Kardec Barros, editors, Independent Component Analysis and Signal Separation, Berlin, Hei-
delberg, 2009. Springer Berlin Heidelberg.

[41] Vincent Q Vu, Juhee Cho, Jing Lei, and Karl Rohe. Fantope projection and selection: A near-optimal
convex relaxation of sparse PCA. In Advances in neural information processing systems, pages 2670–
2678, 2013.

[42] Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller diﬀerentialgle-
ichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen, 71
(4):441–479, 1912. doi: 10.1007/BF01456804. URL https://doi.org/10.1007/BF01456804.

[43] Joong-Ho Won, Teng Zhang, and Hua Zhou. Orthogonal trace-sum maximization: Tightness of the
semideﬁnite relaxation and guarantee of locally optimal solutions. arXiv preprint arXiv:2110.05701,
2021.

[44] Joong-Ho Won, Hua Zhou, and Kenneth Lange. Orthogonal trace-sum maximization: Applications,
local algorithms, and global optimality. SIAM Journal on Matrix Analysis and Applications, 42(2):
859–882, 2021.

[45] Alp Yurtsever, Joel A Tropp, Olivier Fercoq, Madeleine Udell, and Volkan Cevher. Scalable semideﬁnite

programming. SIAM Journal on Mathematics of Data Science, 3(1):171–200, 2021.

14

A Related Work

In this extended related work discussion, we ﬁrst describe works very closely related to our problem in (1), and
then describe works more generally related to SDP relaxations of rank or orthogonality constrained problems.
Bolla et al. [7], Rapcs´ak [35], and Berezovskyi [6] also previously investigated the sum of heterogeneous
quadratics in (1). The work in [7] only studied the structure of this problem for some special cases where all
of the matrices Mi were either equal, diagonal, or commuting. Rapcs´ak [35] derived suﬃcient second-order
global optimality conditions for the Hessian of the Lagrangian. However, these conditions are generally
diﬃcult to check in practice since the Hessian scales quickly with sizes kd × kd and, in general, is not PSD
over the entire space—hence requiring veriﬁcation of its positive semideﬁniteness restricted to vectors on
the manifold tangent space. Berezovskyi [6] proved that the dual Lagrangian bound is exact for the case
of Boolean problem variables.

Works such as Huang and Palomar [25] and Pataki [32] consider a very similar problem to (2), but
without the constraint in (4), making their SDP a rank-constrained separable SDP; see also Luo et al. [30,
Section 4.3]. Pataki studied upper bounds on the rank of optimal solutions of general SDPs, but in the case
of (SDP-P), since our problem introduces the additional constraint summing the Xi, Pataki’s bounds do
not guarantee rank-1, or even low-rank, optimal solutions.

Our problem also has interesting connections to the well-studied problem in the literature of approximate
joint diagonalization (AJD), which is often applied to blind source separation or independent component
analysis (ICA) problems [40; 8; 26; 3; 37]. Given a set of symmetric PSD matrices that represent second
order data statistics, one seeks the matrix, usually constrained to lie in the set of orthogonal or invertible
matrices, that jointly diagonalizes the set of matrices optimally, albeit approximately. When all matrices in
the set commute, the diagonalizer is simply the shared eigenspace, but often in practice, due to noise, ﬁnite
samples, or numerical errors, the set does not commute and can only be approximately diagonalized.
Expanding our matrix variable U ∈ Rd×k to a full basis U ∈ Rd×d, problem (2) is equivalent to

min
U ∈Rd×d:U (cid:48)U =U U (cid:48)=I

L
(cid:88)

(cid:96)=1

1
2

(cid:107)U (cid:48)A(cid:96)U − W(cid:96)(cid:107)2

F + C,

(8)

where W(cid:96) = diag(w(cid:96),1, . . . , w(cid:96),k, 0, . . . , 0) (cid:23) 0, and C is a constant. The objective functions in Pham and
Congedo [33, Equation 4] and Bouchard et al. [9, Equation 8], given a ﬁxed diagonal matrix, bear great
similarity to ours, with the diﬀerence being that the diagonal matrix W(cid:96) above is not a function of U ,
making it a distinct problem from AJD. However, if the diagonal matrix is ﬁxed, then AJD simpliﬁes to (8).
Accordingly, problems (2) and (8) can be loosely interpreted as ﬁnding the U that best approximately jointly
diagonalizes the data second-order statistics A(cid:96) to each W(cid:96). The AJD literature often employs Riemannian
manifold optimization to solve the chosen objective function iteratively. To the best of our knowledge, no
work has yet shown an analytical solution beyond the case when all the matrices commute nor proven global
optimality criteria for these nonconvex programs.

The works in [10; 34] prove global convergence of nonconvex Burer–Monteiro factorization approaches
to solve low-rank semideﬁnite programs, but these are distinct from our problem in which the columns
of the orthonormal basis are constrained together in (4). Other works have studied optimizers to the
nonconvex problem, like those in [14; 13; 38; 12], using minorize-maximize or Riemannian gradient ascent
algorithms. While eﬃcient and scalable, these methods do not have global optimality guarantees beyond
proof of convergence to a critical point. Recent works have also studied convex relaxations of PCA and
other low-rank subspace problems that bound the eigenvalues of a single matrix [41; 39; 44], rather than the
sum of multiple matrices as in our setting. Won et al. [44, 43] study the SDP relaxation of maximizing the
sum of traces of matrix quadratic forms on a product of Stiefel manifolds using the Fantope and propose a
global optimality certiﬁcate. We emphasize their problem pertains to optimizing a trace sum over multiple
orthonormal bases, each on a diﬀerent Stiefel manifold, whereas our problem separates over the columns of a
single basis on the Stiefel and is completely distinct from theirs. Extending the theory of the dual certiﬁcate
from Fan [19] to the orthogonal trace maximization problem, they propose a simple way to test the global
optimality of a given stationary point from an iterative solver of the nonconvex problem. Then in [43], the

15

same authors prove that for an additive noise model with small noise, their SDP relaxation is tight, and the
solution of the nonconvex problem is globally optimal with high probability.

Many works study SDP relaxations of low-rank problems without Fantope constraints, a few of which
we highlight here. The works in [A2; A3; A4] study SDP relaxations of Burer-Monteiro factorizations for
optimization problems with multiple linear constraints. From the local properties of candidate solutions,
they devise dual certiﬁcates to check for global optimality. [A5; A6] show for low-rank SDPs with rank-r and
m linear constraints, no spurious local minima exist if (r + 1)(r + 2)/2 > m + 1; [A6] also proves convergence
of the nonconvex Burer-Monteiro factorization to the optimal SDP solution, with [A7] strengthening this
result, showing such algorithms converge provably in polynomial time, given that r (cid:38) (cid:112)2(1 + η)m for
any ﬁxed constant η > 0.

Similar to our work, the authors in [A1] seek to recover multiple rank-one matrices, in their case for the
overcomplete ICA problem. They solve separate SDP relaxations for each atom of the dictionary, using a
deﬂation method to ﬁnd the atoms in succession. In contrast, our work estimates all of the rank-one matrices
simultaneously, and requires that their ﬁrst principal components form an orthonormal basis, whereas the
dictionary atoms in ICA are only constrained to be unit-norm.

B Proofs of Section 2

B.1 Derivation of (SDP-D)

The Lagrangian function of (SDP-P), with dual variables ν ∈ Rk, Y ∈ Sd

+, Zi ∈ Sd

+ for i = 1, . . . , k, is

L(Xi, ν, Y , Zi) =
(cid:33)

(cid:32) k

(cid:88)

− tr

MiXi

−

i=1

for which the dual function is

k
(cid:88)

i=1

νi(1 − tr(Xi)) − tr

Y

I −

(cid:32)

(cid:32)

(cid:33)(cid:33)

Xi

−

k
(cid:88)

i=1

k
(cid:88)

i=1

tr(ZiXi),

(9)

g(Y , Zi, ν) = inf
Xi

L(Xi, ν, Y , Zi) =

This yields the dual problem

(cid:40)

− tr(Y ) − (cid:80)k
−∞

i=1 νi

s.t. Y = Mi + Zi − νiI ∀i ∈ [k]
otherwise.

(10)

max
Y ,Zi,ν

g(Y , Zi, ν)

s.t. Y (cid:23) 0, Zi (cid:23) 0, Y = Mi + Zi − νiI,

∀i ∈ [k].

(11)

Lemma B.1 (Restatement of Lemma 2.2). Strong duality holds for the SDP relaxation with primal (SDP-P)
and dual (SDP-D).

Proof of Lemma B.1/Lemma 2.2. The problem is convex and satisﬁes Slater’s condition, see Lemma B.2.
Speciﬁcally, at optimality we have (cid:104)I − ((cid:80)k

¯Xi), ¯Y (cid:105) = 0 and therefore tr( ¯Y ) = (cid:104) ¯Y , (cid:80)k

¯Xi(cid:105). Then

i=1

i=1

d∗ = −

(cid:42) k

(cid:88)

i=1

Mi + ¯Zi + ¯νiI, ¯Xi

+

(cid:43)

k
(cid:88)

i=1

¯νi = − tr

(cid:33)

Mi ¯Xi

,

(cid:32) k

(cid:88)

i=1

since (cid:104) ¯Zi, ¯Xi(cid:105) = 0 and (cid:80)k

i=1 ¯νi(1 − tr( ¯Xi)) = 0. Thus, p∗ = d∗.

Lemma B.2. The primal problem in (SDP-P) is strictly feasible.

Proof. To be strictly feasible we must have Xi, i = 1, . . . , k such that

0 ≺

k
(cid:88)

i=1

Xi ≺ I,

tr(Xi) = 1, Xi (cid:31) 0,

i = 1, . . . , k

16

Suppose Xi = 1
(cid:80)k
i=1 Xi ≺ I.

d I for all i. Then tr(Xi) = 1 and Xi (cid:31) 0 for all i, and (cid:80)k

i=1 Xi = k

d I, satisfying 0 ≺

Lemma B.3 (Restatement of Lemma 2.3). The solution to the SDP relaxation in (SDP-P) is the optimal
solution to the original nonconvex problem in (1) (equivalently (4)) if and only if the optimal Xi have the
rank-one property.

Proof of Lemma B.3/Lemma 2.3. Since the problem in (SDP-P) has a larger constraint set than (1), any
solution to (SDP-P) that satisﬁes the constraints of (1) is also a solution to this original nonconvex problem.
For the “if” direction assume that the optimal Xi for (SDP-P) have the rank-one property. Since
tr(Xi) = 1 by deﬁnition of (SDP-P), when we decompose Xi = uiu(cid:48)
i we have ui that are norm-1. In order
for (cid:80)k
i=1 Xi (cid:52) I, the ui must be orthogonal. For the “only if” direction, assume that the solution to the
SDP relaxation in (SDP-P) is the optimal solution to the original nonconvex problem in (1). The constraint
in (1) that the columns of U are orthonormal implies that Xi must have the rank-one property.

Lemma B.4 (Restatement of Lemma 2.4). Assume Mi are PSD. Then the optimal νi ≥ 0.

Proof of Lemma B.4/Lemma 2.4. Another equivalent formulation of the dual problem eliminates Y :

d∗ = min
νi,Zi

1
k

k
(cid:88)

{tr(Zi + Mi) − (d − k)νi}

i=1

s.t. Mi + Zi (cid:60) νiI ∀i = 1, . . . , k

Mi + Zi − νiI = Mj + Zj − νjI ∀i, j = 1, . . . , k
Zi (cid:60) 0

(12)

(13)

Suppose otherwise that νi < 0. Then d∗ > 0 since tr(Zi + Mi) ≥ 0. But if instead we let νi = 0,

Zi = Mi = 0, we have a feasible solution where d∗ = 0.

Lemma B.5. Suppose Xi for i = 1, . . . , k are each trace 1 and each has λ1(Xi) = 1, and therefore each Xi
is rank 1. We decompose Xi = uiu(cid:48)

i and note that ui are norm-1. Then (cid:80)k

i=1 Xi satisﬁes

if and only if

0 (cid:52)

k
(cid:88)

i=1

Xi (cid:52) I

u(cid:48)

iuj = 0 ∀i (cid:54)= j .

Proof. Forward direction: Suppose X = (cid:80)k
i=1 Xi has eigenvalues in [0, 1] and tr(X) = k. Since rank(X) ≤ k
by subadditivity of rank, this implies both that X is rank-k and its eigenvalues are either zero or one. Note
then that

tr(XX (cid:48)) = k = tr

(cid:32)

(cid:88)
(

uiu(cid:48)

i)(

(cid:88)

i

i

(cid:33)

uiu(cid:48)
i)

=





(cid:88)

(u(cid:48)

iui)2 + tr

(cid:88)

(u(cid:48)

iuj)2

 .

2

i

i(cid:54)=j

Since ui are norm-1 then the sum (cid:80)

i(u(cid:48)

iui)2 = k. This means



tr

2

(cid:88)

i(cid:54)=j



(u(cid:48)

iuj)2

 = 0 ,

which is true if and only if u(cid:48)

iuj = 0.

The backward direction is direct because when u(cid:48)

decomposition of X with k eigenvalues equal to one.

iuj = 0 for i (cid:54)= j, (cid:80)k

i=1 uiu(cid:48)

i is the singular value

17

Lemma B.6 (Restatement of Lemma 2.5). If the optimal dual variables Zi for i = 1, . . . , k are each rank
d − 1, the optimal solution variables Xi have the rank-one property.

Proof of Lemma B.6/Lemma 2.5. Suppose Zi is rank d − 1. By complementarity at optimality, we have
ZiXi = 0 ∀i, which means Xi lies in the nullspace of Zi, which has dimension 1, so each Xi is rank-1.
By primal feasibility, tr(Xi) = 1, so λ1(Xi) = 1 ∀i = 1, . . . , k. By Lemma B.5, the optimal solution is an
orthogonal projection matrix, and the optimal Xi are orthogonal.

C Counterexample for Convex-Hull Result

By construction, the feasible set of (SDP-P) is a convex relaxation of the set

{(u1u(cid:48)

1, . . . , uku(cid:48)

k) : U (cid:48)U = I} .

(14)

Given its relationship with the Fantope, a natural question is whether our relaxation captures the convex
hull of (14), which would guarantee that our SDP relaxation is always exact. We prove here that this is not
the case. Even so, there might exist suﬃcient conditions on (M1, . . . , Mk) guaranteeing that the relaxation
is exact. We do not explore such suﬃcient conditions in this subsection.

So let us prove formally that the feasible set of (SDP-P) in general does not capture the convex hull of

(14). Speciﬁcally, we claim that, for d = 4 and k = 2, the matrix X = X1 + X2 given by

and

X1 :=

X2 :=


1
0


0

0

1
2

1
12


3
1


3

1

0
1
0
0

1
3
1
3

0
0
0
0

3
1
3
1







0
0
0
0







1
3
1
3

i

cannot be a strict convex combination of feasible points X (j) = X (j)
for some j = 1, . . . , J such that
every X (j)
is rank-1. Said diﬀerently, (X1, X2) cannot be a strict convex combination of elements of (14).
Note that rank(X1) = rank(X2) = 2, so that (X1, X2) itself is not an element of (14). In addition, it is easy
to verify that rank(X) = 4 and λmax[X] = 1. Our argument is based on the following proposition, whose
contrapositive states that (X1, X2) cannot be a strict convex combination because rank(X) = 4.

1 + X (j)

2

Proposition C.1. Let d ≥ k = 2 be given. Suppose X = X1 + X2 is feasible for (SDP-P) such that:

• (X1, X2) is a strict convex combination of points in (14), i.e., for some integer J ≥ 2, there exist

positive scalars λ1, . . . , λJ and Stiefel matrices

U (j) :=

(cid:16)

u(j)
1

(cid:17)

u(j)
2

∈ Rd×2 ∀ j = 1, . . . , J

such that

(X1, X2) =

J
(cid:88)

j=1

λj

• rank(X1) = rank(X2) = 2;

• λmax[X] = 1.

Then rank(X) ≤ 3.

(cid:16)
1 (u(j)
u(j)

1 )(cid:48), u(j)

2 (u(j)

2 )(cid:48)(cid:17)

,

J
(cid:88)

j=1

λj = 1;

18

Proof. For each i = 1, 2, the equation

Xi =

J
(cid:88)

j=1

λju(j)

i (u(j)

i )(cid:48) =

(cid:16)√

λ1u(1)
i

· · ·

√

λJ u(J)
i

(cid:17) (cid:16)√

λ1u(1)
i

· · ·

√

λJ u(J)
i

(cid:17)(cid:48)

(15)

i

, . . . , u(J)

ensures Range(Xi) = Span{u(1)
i }; see Lemma 1 of [17] for example. Keeping in mind that
rank(X1) = rank(X2) = 2 by assumption, we claim that, without loss of generality, we can reorder
j = 1, . . . , J such that Range(Xi) = Span({u(1)
, u(2)
In other words, we
claim that each Xi “gets its rank” from the vectors u(1)

i
If J = 2, the claim is obvious. If J > 2, ﬁrst reorder the indices {1, . . . , J} such that Range(X1) =
1 }). If the claim holds for this new ordering, we are done. Otherwise, we can further reorder

i }) for both i simultaneously.

1 , u(2)

and u(2)

.

i

i

Span({u(1)
{3, . . . , J} such that

Range(X1) = Span({u(1)
Range(X2) = Span({u(1)

1 , u(2)
2 , u(2)

1 , u(3)
2 , u(3)

1 }) with u(1)
2 }) with u(1)

∦ u(2)
1
2 (cid:107) u(2)

1

2

and u(1)
2

∦ u(3)
2 .

We now consider two exhaustive subcases. First, if u(1)
1
u(3)
1
second subcase u(2)
1

and X2 gets its rank from u(1)
is similar.

2 , u(3)

∦ u(3)
1

With the claim proven, deﬁne Wi := Span{u(1)

i
adding the equations (15) for i = 1, 2, we also have

1 , then we see that X1 gets its rank from u(1)
1 ,
2 . So by another reordering of {1, 2, 3}, the claim is proved. The

∦ u(3)

, u(2)

i } = Span{u(1)

i

, . . . , u(J)

i } for both i = 1, 2. By

rank(X) = dim(W1 + W2) = dim(Span{u(1)

1 , u(2)

1 , u(1)

2 , u(2)

2 }).

Next, let v be a maximum eigenvector of X with (cid:107)v(cid:107) = 1 by deﬁnition. Also, for each j, deﬁne

Vj := Span{u(j)

1 , u(j)

2 } = Range(U (j)), and let

be the squared norm of the projection of v onto Vj. We have

αj := (vT u(j)

1 )2 + (vT u(j)

2 )2 ≤ 1

1 = vT Xv =

(cid:16)

λj

J
(cid:88)

j=1

(vT u(j)

1 )2 + (vT u(j)

2 )2(cid:17)

=

J
(cid:88)

j=1

λjαj.

Since each αj ≤ 1 and since λ is a convex combination, it follows that αj = 1 for all j, which then implies
v ∈ Vj for all j, i.e., v ∈ V1 ∩ V2.

Finally, we have W1 + W2 = V1 + V2 because both Minkowski sums span the four vectors u(j)

for i = 1, 2

i

and j = 1, 2. Hence,

rank(X) = dim(W1 + W2)

= dim(V1 + V2) = dim(V1) + dim(V2) − dim(V1 ∩ V2)
≤ 2 + 2 − 1 = 3.

where the inequality follows because v ∈ V1 ∩ V2.

19

D Proof of Theorem 4.1

Lemma D.1. Let F(U ) denote the objective function with respect to U in (1) over St(k, d). A point
U ∈ St(k, d) is a local maximum of F if

Λ = U (cid:48)

k
(cid:88)

i=1

MiU Ei

is symmetric, and

k
(cid:88)

(cid:104) ˙U , Mi ˙U Ei(cid:105) + (cid:104) ˙U Λ, ˙U (cid:105) ≥ 0 ∀ ˙U ∈ Rd×k such that

−

˙U (cid:48)U + U (cid:48) ˙U = 0.

i=1

i=1 MiU Ei = 2 (cid:2)M1u1

Proof. Taking ¯F to be the quadratic function in (1) over Euclidean space, the Euclidean gradient ∇ ¯F(U ) =
2 (cid:80)k
i ∈ Rk×k and ei is the ith standard basis vector
in Rk. The Euclidean Hessian can also easily be derived as ∇2 ¯F(U )[ ˙U ] = 2 (cid:80)k
i=1 Mi ˙U Ei. Restricting ¯F
to the Stiefel manifold, let F := ¯F|St(k,d). If U ∈ St(k, d) is a local maximizer of (1), then

(cid:3), where Ei := eie(cid:48)

· · · Mkuk

∇F(U ) = 0

and ∇2F(U ) (cid:22) 0,

where ∇F and ∇2F denote the Riemannian gradient and Hessian of F, respectively.

From [2], the gradient on the manifold for local maximizer U satisﬁes

∇F = ∇ ¯F − U sym(U (cid:48)∇ ¯F(U )) = (I − U U (cid:48))∇ ¯F + U skew(U (cid:48)∇ ¯F),

= 2(I − U U (cid:48))

k
(cid:88)

i=1

MiU Ei + U

k
(cid:88)

[U (cid:48)MiU , Ei]

i=1

= 0

(16)

(17)

(18)

(19)

2 (A + A(cid:48)), skew(A) = 1

where sym(A) = 1
2 (A − A(cid:48)), and [A, B] = AB − BA. We note the left and
right expressions of the Riemannian gradient in (18) lie in the orthogonal complement of Span(U ) and
the Span(U ), respectively, so ∇F vanishes if and only if (I − U U (cid:48))∇ ¯F = 0, and (cid:80)k
i=1[U (cid:48)MiU , Ei] = 0,
implying U (cid:48)∇ ¯F = ∇ ¯F (cid:48)U . Letting Λ := sym(U (cid:48)∇ ¯F), this also implies

U Λ = ∇ ¯F =

k
(cid:88)

i=1

MiU Ei,

(20)

and multiplying both sides by U (cid:48) yields the expression for Λ, which is symmetric as shown above so we can
drop the sym(·) operator.

It can be shown the Riemannian Hessian is negative semideﬁnite if and only if

(cid:104) ˙U , ∇2 ¯F(U )[ ˙U ] − ˙U Λ(cid:105) ≤ 0

(21)

˙U ∈ TU St(d, k), where TU St(d, k) is the tangent space of the Stiefel manifold, i.e. the set TU St(d, k) =
for all
{ ˙U ∈ Rd×k : U (cid:48) ˙U + ˙U (cid:48)U = 0}. Plugging in the expressions for Λ and the Hessian of ¯F yield the main
result.

The following lemma is adapted from [7, Corollary 4.2]

Lemma D.2. Let ¯U ∈ Rd×k be a local maximum of (1). Then ¯Λ = ¯U (cid:48) (cid:80)k

i=1 Mi ¯U Ei is positive semideﬁnite.
Proof. Since k < d, there exists a unit vector z in the span of ¯U⊥ ∈ Rd×d−k where ¯U (cid:48) ¯U⊥ = 0. Let
a = [a1, . . . , ak](cid:48) ∈ Rk be an arbitrary nonzero vector. Let ˙U := za(cid:48), and let ¯Λ = (cid:80)k
¯U (cid:48)Mi ¯U Ei. Then

i=1

20

clearly ¯U (cid:48) ˙U = 0, and ¯U (cid:48) ˙U + ˙U (cid:48) ¯U = 0, so the second-order stationary necessary condition in Lemma D.1
applies:

a(cid:48) ¯Λa = (cid:104) ˙U ¯Λ, ˙U (cid:105) ≥

k
(cid:88)

i=1

(cid:104) ˙U , Mi ˙U Ei(cid:105) =

k
(cid:88)

(ai)2z(cid:48)Miz ≥ 0.

i=1

(22)

Therefore, since a(cid:48) ¯Λa ≥ 0 for arbitrary a, ¯Λ is positive semideﬁnite.

Theorem D.3 (Restatement of Theorem 4.1). Let ¯U ∈ St(k, d) be a local maximizer to (1), and let ¯Λ =
(cid:80)k
i and ei is the ith standard basis vector in Rk. If there exist ¯ν = [¯ν1 · · · ¯νk] ∈
Rk

¯U (cid:48)Mi ¯U Ei, where Ei (cid:44) eie(cid:48)

i=1
+ such that

¯U ( ¯Λ − D ¯ν ) ¯U (cid:48) + ¯νiI − Mi (cid:23) 0 ∀i = 1, . . . , k

¯Λ − D ¯ν (cid:23) 0,

(23)

where D ¯ν := diag(¯ν1, . . . , ¯νk), then ¯U is an optimal solution to (SDP-P) and a globally optimal solution to
the original nonconvex problem (1).

Proof of Theorem D.3/ Theorem 4.1. By Lemma 2.2, primal and dual feasible solutions of (SDP-P) and
(SDP-D), ¯Xi, ¯Zi, ¯Y , ¯ν, are simultaneously optimal if and only if they satisfy the following Karush-Kuhn
Tucker (KKT) conditions [11], where the variables and constraints are indexed by i ∈ [k]:

¯Xi (cid:23) 0,

k
(cid:88)

i=1

¯Xi (cid:22) I,

tr( ¯Xi) = 1

¯Y = Mi + ¯Zi − ¯νiI,

¯Y (cid:23) 0

k
(cid:88)

(cid:104)I −

¯Xi, ¯Y (cid:105) = 0

i=1
(cid:104) ¯Zi, ¯Xi(cid:105) = 0
¯Zi (cid:23) 0.

(KKT-a)

(KKT-b)

(KKT-c)

(KKT-d)

(KKT-e)

Similar to the work in [44], our strategy is then to construct ¯Xi and ¯Y , ¯Zi, ¯ν satisfying these conditions.
Given ¯U and ¯ν in the statement of the theorem, we deﬁne ¯Xi = ¯ui ¯u(cid:48)
i, ¯Y = ¯U ( ¯Λ − D ¯ν ) ¯U (cid:48), and ¯Zi =
¯Y + ¯νiI − Mi. By construction, ¯Xi satisfy (KKT-a), and it is clear that ¯Y = Mi + ¯Zi − ¯νiI satisﬁes
(KKT-b). One can also verify that (cid:104)I − ¯X, ¯Y (cid:105) = 0 by construction, thus satisfying (KKT-c). So it remains
to show ¯Y (cid:23) 0, (cid:104) ¯Zi, ¯Xi(cid:105) = 0, and ¯Zi (cid:23) 0.

The assumption that ¯Λ (cid:23) D ¯ν ensures ¯Y (cid:23) 0 (KKT-b). We note that we have shown in Lemma D.1 and
Lemma D.2 that ¯Λ is symmetric PSD, which is a necessary condition for this assumption to hold, given the
fact that the Lagrange multipliers ¯νi corresponding to the trace constraints are nonnegative by Lemma 2.4.
Moreover, ¯Zi (cid:23) 0 by the assumption in (5), satisfying (KKT-e). We ﬁnally verify (KKT-d),
i.e.

(cid:104) ¯Zi, ¯Xi(cid:105) = 0, with ¯U = [ ¯u1 · · · ¯uk]:

(cid:104) ¯Zi, ¯Xi(cid:105) = (cid:104) ¯Y + ¯νiI − Mi, ¯Xi(cid:105) = (cid:104) ¯U ( ¯Λ − D ¯ν ) ¯U (cid:48) + ¯νiI − Mi, ¯ui ¯u(cid:48)
i(cid:105)

= ¯u(cid:48)
i

¯U ¯U (cid:48)

k
(cid:88)

j=1

Mj ¯U Ej ¯U (cid:48) ¯ui − ¯u(cid:48)
i

¯U D ¯ν ¯U (cid:48) ¯ui + ¯νi − ¯u(cid:48)

iMi ¯ui

= e(cid:48)
i

¯U (cid:48)

k
(cid:88)

j=1

Mj ¯uje(cid:48)

jei − e(cid:48)

iD ¯ν ei + ¯νi − ¯u(cid:48)

iMi ¯ui

= ¯u(cid:48)

iMi ¯ui − ¯νi + ¯νi − ¯u(cid:48)

iMi ¯ui = 0.

21

One may ask if there is an analytical way to verify the dual variables ¯Y and ¯Zi are PSD without
computing the LMI feasibility problem in (5). While it is possible to derive suﬃcient upper bounds on
the feasible ¯νi to guarantee ¯Λ (cid:23) D ¯ν so that ¯Y (cid:23) 0, this is insuﬃcient to certify ¯Zi (cid:23) 0 based on these
bounds alone. This is in contrast to [44]; their particular dual certiﬁcate matrix is monotone in the Lagrange
multipliers (analogous to our ¯νi), so it is suﬃcient to test the positive semideﬁniteness of the certiﬁcate
matrix using the analytical upper bounds. Let ¯U⊥i denote an orthonormal basis for Span(I − ¯ui ¯u(cid:48)
i). Here,
since ¯Zi = ¯U ¯Λ ¯U (cid:48) − (cid:80)k
− Mi, each ¯Zi is monotone in ¯νi but not in ¯νj for j (cid:54)= i.
Therefore, there is tension between inﬂating ¯νi and guaranteeing all the ¯Zi are PSD. As such, an analytical
solution to check that ¯Λ (cid:23) D ¯ν and the ¯Zi are PSD remains unknown, requiring computation of the LMI
feasibility problem in (5).

j + ¯νi ¯U⊥i

j(cid:54)=i ¯νj ¯uj ¯u(cid:48)

¯U (cid:48)
⊥i

D.1 Arithmetic Complexity - more details

While SDP relaxations of nonconvex optimization problems can provide strong provable guarantees, their
practicality can be limited by the time and space required to solve them, particularly when using oﬀ-the-
shelf interior-point solvers. Interior-point methods are provably polynomial-time, but in our case the number
of ﬂoating point operations and the storage per iteration to solve (SDP-P) both grow as O(d3) [5], which
practically limits d to be in the few hundreds.

On the other hand, the study of the SDP relaxation admits improved practical tools to transfer theoretical
guarantees to the nonconvex setting; that is, to investigate when the convex relaxation is tight, and if it is,
when a candidate solution of the nonconvex problem is globally optimal. In comparison to the dual problem
of the SDP (SDP-D) (upon eliminating the variables Zi), the proposed global certiﬁcate signiﬁcantly reduces
the number of variables from O(d2) to merely k variables. Precisely, the total computational savings can be
shown using [4, Section 6.6.3], for which (SDP-D) scales in arithmetic complexity as O((kd)1/2kd6) ﬂoating
point operations (ﬂops) and the certiﬁcate scales by O((kd)1/2k2d3) ﬂops, showing a substantial reduction by
a factor of O(d3/k) ﬂops. Subsequently, a ﬁrst order MM solver in [14], whose cost is O(dk2+k3) per iteration,
combined with our global optimality certiﬁcate is an obvious preference to solving the full SDP in (SDP-P)
for large problems. Given the global certiﬁcate tool in Theorem 4.1, if (1) has a tight convex relaxation,
we can reliably and cheaply certify the terminal output of a ﬁrst order solver with possibly fewer restarts
and without resorting to heuristics in nonconvex optimization, which commonly entails computing many
multiple algorithm runs from diﬀerent initializations and taking the solution with the best objective value.

E Proof of Theorem 4.5

We start this section by giving general convex analysis results that allow us to prove Theorem 4.5.

Let C ⊆ Rn be a closed, convex set. For all c ∈ C, consider a primal-dual pair of linear conic pro-

grams parameterized by c:

p(c) := min

{cT x : Ax = b, x ∈ K}

d(c) := max

{bT y : c − AT y ∈ K∗}

x

y

(P ; c)

(D; c)

Here, the data A ∈ Rm×n and b ∈ Rm are ﬁxed; K ⊆ Rn is a closed, convex cone; and K∗ := {s ∈ Rn :
sT x ≥ 0 ∀ x ∈ K} is its polar dual. We imagine, in particular, that K is a direct product of a nonnegative
orthant, second-order cones, and positive semideﬁnite cones, corresponding to linear, second-order-cone,
and semideﬁnite programming.

Deﬁne Feas(P ) := {x ∈ K : Ax = b} and Feas(D; c) := {y : c − AT y ∈ K∗} to be the feasible sets

of (P ; c) and (D; c), respectively. We assume:

Assumption E.0.1. Feas(P ) is interior feasible, and Feas(D; c) is interior feasible for all c ∈ C.

22

Then, for all c, strong duality holds between (P ; c) and (D; c) in the sense that p(c) = d(c) and both p(c)
and d(c) are attained in their respective problems. Accordingly, we also deﬁne

Opt(D; c) := {y ∈ Feas(D; c) : bT y = d(c)}

to be the nonempty, dual optimal solution set for each c ∈ C.

In addition, we assume the existence of linear constraints f − ET y ≥ 0, independent of c, such that

satisﬁes:

Extra(D) := {y : f − ET y ≥ 0}

Assumption E.0.2. For all c ∈ C, Feas(D; c) ∩ Extra(D) is interior feasible and bounded, and Opt(D; c) ⊆
Extra(D).

In words, irrespective of c, the extra constraints f − ET y ≥ 0 bound the dual feasible set without cutting oﬀ
any optimal solutions and while still maintaining interior, including interiority with respect to f − ET y ≥ 0.
Note also that Assumption E.0.2 implies the recession cone of Feas(D; c) ∩ Extra(D) is trivial for (and
independent of) all c, i.e., {∆y : −AT ∆y ∈ K∗, −ET ∆y ≥ 0} = {0}.

We ﬁrst prove a continuity result related to the dual feasible set, in which we use the following deﬁnition
of a convergent sequence of bounded sets in Euclidean space: a sequence of bounded sets {Lk} converges to
a bounded set ¯L, written {Lk} → ¯L, if and only if: (i) given any sequence {yk ∈ Lk}, every limit point ¯y of
the sequence satisﬁes ¯y ∈ ¯L; and (ii) every member ¯y ∈ ¯L is the limit point of some sequence {yk ∈ Lk}.

Lemma E.1. Under Assumptions E.0.1 and E.0.2, let {ck ∈ C} → ¯c be any convergent sequence. Then

(cid:8)Feas(D; ck) ∩ Extra(D)(cid:9) → Feas(D; ¯c) ∩ Extra(D).

Proof. For notational convenience, deﬁne Lk := Feas(D; ck) ∩ Extra(D) and ¯L := Feas(D; ¯c) ∩ Extra(D).
Note that Lk and ¯L are bounded with interior by Assumption E.0.2. We wish to show {Lk} → ¯L.

We ﬁrst note that any sequence {yk ∈ Lk} must be bounded.

If not, then {∆yk := yk/(cid:107)yk(cid:107)} is a

bounded sequence satisfying

(cid:107)∆yk(cid:107) = 1,

ck
(cid:107)yk(cid:107)

− AT ∆yk ∈ K∗,

f
(cid:107)yk(cid:107)

− ET ∆yk ≥ 0

and hence has a limit point ∆y satisfying

∆y (cid:54)= 0, −AT ∆y ∈ K∗, −ET ∆y ≥ 0,

but this is a contradiction by the discussion after the statement of Assumption E.0.2. We thus conclude that
any sequence {yk ∈ Lk} has a limit point.

Appealing to the deﬁnition of the convergence of sets stated before the lemma, we ﬁrst let ¯y be a limit

point of any {yk ∈ Lk} and prove that ¯y ∈ ¯L. Since

ck − AT yk ∈ K∗,

f − ET yk ≥ 0

for all k, by taking the limit of {ck} and {yk}, we have ¯c − AT ¯y ∈ K∗ and f − ET ¯y ≥ 0 so that indeed
¯y ∈ ¯L.

Next, we must show that every ¯y ∈ ¯L is the limit point of some sequence {yk ∈ Lk}. For this proof,

deﬁne

κ( ¯y) := min{k : ¯y ∈ L(cid:96) ∀ (cid:96) ≥ k},

i.e., κ( ¯y) is the smallest k such that ¯y is a member of every set in the tail Lk, Lk+1, Lk+2, . . .. By convention,
if there exists no such k, we set κ( ¯y) = ∞.

23

Let us ﬁrst consider the case ¯y ∈ int( ¯L). We claim κ( ¯y) < ∞, so that setting yk = ¯y for all k ≥ κ( ¯y)
yields the desired sequence converging to ¯y. Indeed, as ¯y satisﬁes ¯c − AT ¯y ∈ int(K∗) and f − ET ¯y > 0, the
equation

ck − AT ¯y = (cid:0)¯c − AT ¯y(cid:1) + (cid:0)ck − ¯c(cid:1)
shows that {ck − AT ¯y} equals ¯c − AT ¯y ∈ int(K∗) plus the vanishing sequence {ck − ¯c}. Hence its tail is
contained in int(K∗), thus proving κ( ¯y) < ∞, as desired.

Now we consider the case ¯y ∈ bd( ¯L). Let y0 ∈ int( ¯L) be arbitrary, so that κ(y0) < ∞ by the previous
paragraph. For a second index (cid:96) = 1, 2, . . ., deﬁne z(cid:96) := (1/(cid:96))y0 + (1 − 1/(cid:96)) ¯y ∈ int( ¯L). Clearly, κ(z(cid:96)) < ∞
for all (cid:96) and {z(cid:96)} → ¯y. We then construct the desired sequence {yk ∈ Lk} converging to ¯y as follows. First,
set

k1 := κ(z1) = κ(y0)
k(cid:96) := max{k(cid:96)−1 + 1, κ(z(cid:96))} ∀ (cid:96) = 2, 3, . . .

and then, for all (cid:96) and for all k ∈ [k(cid:96), k(cid:96)+1 − 1], deﬁne yk := z(cid:96). Essentially, {yk} is the sequence {z(cid:96)},
except with entries repeated to ensure yk is in fact a member of Lk for all k. Hence, {yk} converges to ¯y as
desired.

We now specialize Lemma E.1 to the dual optimality set.

Lemma E.2. Under Assumptions E.0.1 and E.0.2, let {ck ∈ C} → ¯c be any convergent sequence. Then

(cid:8)Opt(D; ck)(cid:9) → Opt(D; ¯c).

Proof. Lemma E.1 establishes {Feas(D; ck)∩Extra(D)} → Feas(D; ¯c)∩Extra(D). Because problems (D; ck)
and (D; ¯c) share the same objective bT y, Assumption E.0.2 and Lemma E.1 imply {d(ck)} → d(¯c). Hence,
the sequence of hyperplanes {{y : bT y = d(ck)}} converges to {y : bT y = d(¯c)}, and so {Opt(D; ck)} →
Opt(D; ¯c) by intersecting the two sequences of convergent sets.

Finally, for given c ∈ C and ﬁxed y0 ∈ Rm, we deﬁne the function

y(c) := y(c; y0) = argmin{(cid:107)y − y0(cid:107) : y ∈ Opt(D; c)},

i.e., y(c) equals the point in Opt(D; c), which is closest to y0. Since Opt(D; c) is closed and convex, y(c) is
well deﬁned. We next use Lemma E.2 to show that that y(c) is continuous in c.

Proposition E.3. Under the Assumptions E.0.1 and E.0.2, given y0 ∈ Rm, the function y(c) := y(c; y0)
is continuous in c.

Proof. We must show that, for any convergent {ck} → ¯c, we also have convergence {y(ck)} → y(¯c). This
follows because {Opt(D; ck)} → Opt(D; ¯c) by Lemma E.2.

Theorem 4.5 uses Proposition E.3 in its proof. Here we discuss how the primal-dual pair SDP-P-(SDP-D)
satisfy the assumptions for the proposition. We would like to establish conditions under which (SDP-P) has
the rank-1 property. For this, we apply the general theory developed above, speciﬁcally Proposition E.3.
To show that the general theory applies, we must deﬁne the closed, convex set C, which contains the set of
admissible objective matrices/coeﬃcients (M1, . . . , Mk) and which satisﬁes Assumptions E.0.1 and E.0.2.
In particular, for a ﬁxed, user-speciﬁed upper bound µ > 0, we deﬁne

C := {c = (M1, . . . , Mk) : 0 (cid:22) Mi (cid:22) µI ∀ i = 1, . . . , k},

to be our set of admissible coeﬃcient k-tuples.

We know that both (SDP-P) and (SDP-D) have interior points for all c ∈ C, so that strong duality holds.

For the dual in particular, the equation µI = Mi + ((µ + (cid:15))I − Mi) − (cid:15)I shows that, for all (cid:15) > 0,

Y ((cid:15)) := µI,

Z((cid:15))i := (µ + (cid:15))I − Mi,

ν((cid:15))i := (cid:15)

24

is interior feasible with objective value dµ + k(cid:15). In particular, the redundant constraint ν ≥ 0 is satisﬁed
strictly. This veriﬁes Assumption E.0.1.

We next verify Assumption E.0.2. Since the objective value just mentioned is independent of c =
i=1 νi ≤ dµ + k without
In particular, the solution
2 )i, ν( 1
2 satisﬁes the new, extra constraint strictly. Finally, note that
i νi ≤ dµ + k bounds Y and ν in the presence of the constraints Y (cid:23) 0 and ν ≥ 0, and

(M1, . . . , Mk), we can take (cid:15) = 1 and enforce the extra constraint tr(Y ) + (cid:80)k
cutting oﬀ any dual optimal solutions and while still maintaining interior.
2 ), Z( 1
(Y ( 1
tr(Y ) + (cid:80)
consequently the constraint Zi = Y − Mi + νiI bounds Zi for each i.

2 )i) corresponding to (cid:15) = 1

We now repeat the discussion leading up to Theorem 4.5 for completeness. The ﬁrst lemma says that
the diagonal problem has dual variables Zi such that rank(Zi) ≥ d − 1, implying that the primal vari-
ables Xi are rank-1.

Lemma E.4 (Restatement of Lemma 4.3). Let c = (M1, . . . , Mk) ∈ C. If Mi is jointly diagonalizable
for each i = 1, . . . , k and (SDP-P) has a unique optimal solution, then there exists an optimal solution of
(SDP-D) with rank(Zi) ≥ d − 1 for all i = 1, . . . , k.

Proof of Lemma E.4/Lemma 4.3. Because of the jointly diagonalizable property, we may assume without
loss of generality that each Mi is diagonal. So (SDP-P) is equivalent to the assignment LP

max

(cid:40) k

(cid:88)

i=1

diag(Mi)(cid:48) diag(Xi) :

e(cid:48) diag(Xi) = 1, diag(Xi) ≥ 0 ∀ i = 1, . . . , k
(cid:80)k

i=1 diag(Xi) ≤ e

(cid:41)

,

where e is the vector of all ones, and (SDP-D) is equivalent to the LP

(cid:40)

min

e(cid:48) diag(Y ) +

k
(cid:88)

i=1

νi :

diag(Y ) = diag(Mi) + diag(Zi) − νie ∀ i = 1, . . . , k
diag(Zi) ≥ 0 ∀ i = 1, . . . , k,

diag(Y ) ≥ 0

(cid:41)

.

Since the primal is an assignment problem, its unique optimal solution has the property that each diag(Xi)
is a standard basis vector (i.e., each has a single entry equal to 1 and all other entries equal to 0). By the
Goldman-Tucker strict complementarity theorem for LP, there exists an optimal primal-dual pair such that
diag(Xi) + diag(Zi) > 0 for each i. Hence, there exists a dual optimal solution with rank(Zi) ≥ d − 1 for
each i, as desired.

Deﬁnition E.5 (Restatement of Deﬁnition 4.4). For c = (M1, . . . , Mk) ∈ C and ¯c = ( ¯M1, . . . , ¯Mk) ∈ C,
deﬁne

dist(c, ¯c) (cid:44) max
i∈[k]

(cid:107)Mi − ¯Mi(cid:107).

We now repeat the continuity result and give a more precise proof.

Theorem E.6 (Restatement of Theorem 4.5). Let ¯c := ( ¯M1, . . . , ¯Mk) ∈ C be jointly diagonalizable such
that the primal problem (SDP-P) with objective coeﬃcients ¯c has a unique optimal solution. Then there
exists an full-dimensional neighborhood of ¯C (cid:51) ¯c in C such that (SDP-P) has the rank-1 property for all
c = (M1, . . . , Mk) ∈ ¯C.
Proof of Theorem E.6/Theorem 4.5. Using Lemma 4.3, let y0 := ( ¯Y , ¯Zi, ¯νi) be the optimal solution of the
dual problem (SDP-D) for ¯c = ( ¯M1, . . . , ¯Mk), which has rank( ¯Zi) ≥ d − 1 for all i. Then by Proposition
E.3, the function y(c) := y(c; y0), which returns the optimal solution of (SDP-D) for c = (M1, . . . , Mk)
closest to y0, is continuous in c. It follows that the preimage

y−1 ({(Y , Zi, νi) : rank(Zi) ≥ d − 1 ∀ i})

contains ¯c and is an open set because the set of all (Y , Zi, νi) with rank(Zi) ≥ d − 1 is an open set. After
intersecting with C, this full-dimensional set ¯C proves the theorem via the complementarity of the KKT
conditions of the assignment LP, rank(Zi) = d − 1 for i = 1, . . . , k, and Lemma 2.5.

25

The next corollary is a slightly more general version of Corollary 4.5.1, which shows that for a general
tuple of almost commuting matrices c := (M1, . . . , Mk) that are CJD, (SDP-P) is tight and has the rank-1
property. We ﬁrst state Lin’s theorem, which we use in the proof.

Lemma E.7. Lin’s Theorem [28; 22]:
For all (cid:15) > 0 there exists a δ > 0 such that if (cid:107)[A, B](cid:107)2 :=
(cid:107)AB − BA(cid:107)2 ≤ δ for Hermitian symmetric matrices A and B where (cid:107)A(cid:107) ≤ 1 and (cid:107)B(cid:107) ≤ 1, then there exist
Hermitian symmetric, commuting matrices (cid:101)A and (cid:101)B in Rd×d such that (cid:107)[ ˜A, ˜B](cid:107) = 0 and (cid:107)A − ˜A(cid:107)2 ≤ (cid:15) and
(cid:107)B − ˜B(cid:107)2 ≤ (cid:15).

Corollary E.7.1 (Restatement of Corollary 4.5.1 with an additional conclusion). Assume (cid:107)Mi(cid:107) ≤ 1 for all
i ∈ [k], and let c := (M1, . . . , Mk). Suppose (cid:107)[Mi, Mj](cid:107) := (cid:107)MiMj − MjMi(cid:107) ≤ δ for all i, j ∈ [k]. Then
there exists ¯c := ( ¯M1, . . . , ¯Mk) of commuting, jointly-diagonalizable matrices such that (cid:107)[ ¯Mi, ¯Mj](cid:107) = 0 for
all i, j ∈ [k] where dist(c, ¯c) ≤ O((cid:15)(δ)) and (cid:15)(δ) is a function satisfying limδ→0 (cid:15)(δ) = 0. If δ is small enough,
there exists (cid:15) > (cid:15)(δ) > 0 such that dist(c, ¯c) ≤ (cid:15) implies c ∈ ¯C and (SDP-P) has the rank-one property.

Further, for the problem in (2), assume (cid:107)A(cid:96)(cid:107) ≤ 1 for all (cid:96) ∈ [L]. If (cid:107)[A(cid:96), Am](cid:107) ≤ δ for all (cid:96), m ∈ [L] for

some δ > 0, then there exists a ¯c ∈ C such that dist(c, ¯c) ≤ O((cid:80)L

(cid:96)=1 w(cid:96),i(cid:15)(δ)).

Proof of Corollary E.7.1 / Corollary 4.5.1. The general result follows from directly applying Lemma E.7 to
each Mi, and for the instance of problem (2), we apply Lemma E.7 to each A(cid:96). Then there exist Hermitian
symmetric matrices ¯A(cid:96) such that (cid:107)[ ¯A(cid:96), ¯Am](cid:107) = 0 for all (cid:96), m ∈ [L] such that (cid:107)A(cid:96) − ¯A(cid:96)(cid:107) ≤ (cid:15)(δ) for all (cid:96) ∈ [L].
Let ¯Mi := (cid:80)L

(cid:96)=1 w(cid:96),i ¯A(cid:96). Then the matrices ¯Mi commute and are jointly diagonalizable:

[ ¯Mi, ¯Mj] = ¯Mi ¯Mj − ¯Mj ¯Mi = 2

L
(cid:88)

(cid:96)(cid:54)=m

w(cid:96),iwm,j( ¯A(cid:96) ¯Am − ¯Am ¯A(cid:96)) = 0.

(24)

Now we measure the distance between each Mi and ¯Mi:

(cid:107)Mi − ¯Mi(cid:107) = (cid:107)

L
(cid:88)

(cid:96)=1

w(cid:96),i(A(cid:96) − ¯A(cid:96))(cid:107) ≤

L
(cid:88)

(cid:96)=1

w(cid:96),i(cid:107)A(cid:96) − ¯A(cid:96)(cid:107) ≤

L
(cid:88)

(cid:96)=1

w(cid:96),i(cid:15)(δ).

(25)

The next lemma is from Koltchinskii and Lounici [27] and also [29].

Lemma E.8. Let y1, . . . , yn ⊆ Rd be i.i.d. centered Gaussian random variables in a separable Banach space
with covariance operator Σ and sample covariance ˆΣ = 1
i. Then with some constant C > 0 and
n
with probability at least 1 − e−t for t > 0,

i=1 yiy(cid:48)

(cid:80)n

(cid:107) ˆΣ − Σ(cid:107) ≤ C(cid:107)Σ(cid:107) max

(cid:40)(cid:114)

˜r(Σ) log d + t
n

,

(˜r(Σ) log d + t) log n
n

(cid:41)

,

where ˜r(Σ) := tr(Σ)/(cid:107)Σ(cid:107).

Lemma E.9. Let ¯Mi := E[Mi] ∈ Rd×d, where the expectation is taken with respect to the data observations,
and let C > 0 be a universal constant. Normalize each Mi by 1/n. Then (cid:107)[ ¯Mi, ¯Mj](cid:107) = 0, and with probability
at least 1 − e−t for t > 0

(cid:107)Mi − ¯Mi(cid:107)
(cid:107) ¯M1(cid:107)

≤ C

¯σi
¯σ1

max




(cid:115) ¯ξi
¯σi



log d + t

n

¯ξi
¯σi

,

log d + t

n




log(n)



, where

(26)

¯σi = (cid:107) ¯Mi(cid:107) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(

λ1
v(cid:96)

+ 1)

and

¯ξi = tr( ¯Mi) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(

1
v(cid:96)

k
(cid:88)

i=1

λi + d).

26

Proof. Let ˜y(cid:96),i :=
rescaling, Mi = 1
n

(cid:113) w(cid:96),i
v(cid:96)
(cid:80)L

(cid:96)=1

y(cid:96),i be a rescaling of the data vectors. Then ˜y(cid:96),i
(cid:80)n(cid:96)

iid∼ N (0, w(cid:96),i( 1
v(cid:96)
(cid:96),j. Taking the expectation over the data, we have

j=1 ˜y(cid:96),j ˜y(cid:48)

U Θ2U (cid:48) + I)). After

E[Mi] =

1
n

L
(cid:88)

n(cid:96)(cid:88)

(cid:96)=1

j=1

E[ ˜ym,j ˜y(cid:48)

m,j|m = (cid:96)] =

L
(cid:88)

(cid:96)=1

w(cid:96),i

n(cid:96)
n

(cid:18) 1
v(cid:96)

(cid:19)

U Θ2U (cid:48) + I

.

(27)

Let U⊥ ∈ Rd×d−k be an orthonormal basis spanning the orthogonal complement of Span(U ). Noting that
I = U U (cid:48) + U⊥U (cid:48)

⊥, rewrite E[Mi] in terms of its eigendecomposition by

E[Mi] = U

(cid:32) L
(cid:88)

(cid:96)=1

= (cid:2)U U⊥

(cid:3)

w(cid:96),i

n(cid:96)
n
(cid:20)Σ
0

(cid:17)

(cid:19)(cid:33)

U (cid:48) +

Θ2 + Ik

(cid:18) 1
v(cid:96)

(cid:33)

(cid:32) L
(cid:88)

(cid:96)=1

w(cid:96),i

n(cid:96)
n

U⊥U (cid:48)
⊥

0
γId−k

(cid:21) (cid:20) U (cid:48)
(cid:21)
U (cid:48)
⊥,

(28)

(29)

where Σ := (cid:80)L
n(cid:96)
n , from which we obtain the expressions for
¯σi = (cid:107)E[Mi](cid:107) and ¯ξi = tr(E[Mi]). Then invoking Lemma E.8 to bound the concentration of a sample
covariance matrix to its expectation with high probability yields the ﬁnal result.

and γ := (cid:80)L

Θ2 + Ik

(cid:96)=1 w(cid:96),i

(cid:96)=1 w(cid:96),i

n(cid:96)
n

(cid:16) 1
v(cid:96)

Proposition E.10 (Restatement of Proposition 4.6). Let c = ( 1
n Mk) be the (normalized) data
matrices of the HPPCA problem. Then there exists commuting ¯c = ( ¯M1, . . . , ¯Mk) (constructed in the proof )
such that, for a universal constant C > 0 and with probability exceeding 1 − e−t for t > 0,

n M1, . . . , 1

(cid:107)Mi − ¯Mi(cid:107)
(cid:107) ¯M1(cid:107)

≤ min




L
(cid:88)



(cid:96)=1

1
+ 1

, C

λi
v(cid:96)




(cid:115) ¯ξi
¯σi



max

log d + t

n

¯ξi
¯σi

,

log d + t

n

log(n)











, where

(30)

¯σi = (cid:107) ¯Mi(cid:107) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(cid:18) λ1
v(cid:96)

+ 1

and

¯ξi = tr( ¯Mi) =

L
(cid:88)

(cid:96)=1

λi
v(cid:96)

λi
v(cid:96)
+ 1

n(cid:96)
n

(cid:32)

1
v(cid:96)

k
(cid:88)

i=1

(cid:33)

λi + d

.

¯σi
¯σ1

(cid:19)

Proof of Proposition E.10/Proposition 4.6. We argue there are two possible sets of commuting ( ¯M1, . . . , ¯Mk)
that (M1, . . . , Mk) can converge to, depending on the signal to noise ratios λi
and the number of samples
v(cid:96)
n.

Consider that we can scale all the Mi in (SDP-P) by a positive scalar constant without changing the
optimal solution. Since all the Mi can be arbitrarily scaled in this manner, and thereby changing any
distance measure, we will choose to normalize the matrices Mi and ¯Mi by the number of samples and the
largest spectral norm of the ¯Mi, which is equivalent to also normalizing the distance. For the HPPCA
application, since ¯M1 (cid:23) ¯M2 . . . (cid:23) ¯Mk, we normalize by 1/(cid:107)n ¯M1(cid:107).

First, if the variances are zero or all the same, i.e. noiseless or homoscedastic noisy data, then all the Mi
commute. Otherwise, in the case where each SNR λi/v(cid:96) of the ith components is large or close to the same
value for all (cid:96) ∈ [L], the weights w(cid:96),i = λi/v(cid:96)
λi/v(cid:96)+1 are very close to 1 or some constant less than 1, respectively.
Therefore, let ¯M := 1
n

(cid:96)=1 A(cid:96) for all i ∈ [k]. Then

(cid:80)L

(cid:107)Mi − ¯M(cid:107)
(cid:107) ¯M(cid:107)

=

(cid:80)L

(cid:13)
(cid:13)
(cid:13)

(cid:96)=1(w(cid:96),i − 1)A(cid:96)
(cid:107) (cid:80)L

(cid:96)=1 A(cid:96)(cid:107)

(cid:13)
(cid:13)
(cid:13)

≤

(cid:80)L

1
+1

(cid:107)A(cid:96)(cid:107)

(cid:96)=1
λi
v(cid:96)
(cid:107) (cid:80)L
(cid:96)=1 A(cid:96)(cid:107)

≤

L
(cid:88)

(cid:96)=1

1
+ 1

,

λi
v(cid:96)

(31)

the last inequality above results from the fact

symmetric PSD matrices [42].

(cid:107)A(cid:96)(cid:107)
(cid:96)=1 A(cid:96)(cid:107)

(cid:107) (cid:80)L

≤ 1 for all (cid:96) ∈ [L] using Weyl’s inequality for

While the bound above depends on the SNR, it fails to capture the eﬀects of the sample sizes, which
also play an important role in how close the Mi are to commuting. Even in the case where the variances

27

are larger and more heterogeneous, since the Mi form a weighted sum of sample covariance matrices, given
enough samples, they should concentrate to their respective sample covariance matrices, which commute
between i, j ∈ [k]. We show exactly this using the concentration of sample covariances to their expectation
in [29], and choose ¯c = ( ¯M1, . . . , ¯Mk) for ¯Mi := E[Mi], where the expectation here is with respect to the
data generated by the model in (6).

Let ¯Mi := E[Mi] ∈ Rd×d, where the expectation is taken with respect to the data observations. Then

by Lemma E.9 and taking the minimum with (31), we obtain the ﬁnal result.

F Example of SDP with rank-one solutions, but Mi that are not

almost commuting

In our paper, we give suﬃcient conditions for when the SDP returns rank-one orthogonal primal solutions
in the case the Mi matrices almost commute. However, this is not a necessary condition, and we give
a counter-example here.

Proposition F.1. Construct Mi for i = 1, . . . , k as follows for given length-d vectors vi, i = 1, . . . , k:

M1 = v1v(cid:48)
M2 =

2 + · · · +vkv(cid:48)
k
2 + · · · +vkv(cid:48)
k

1+ v2v(cid:48)
v2v(cid:48)
...

Mk =

vkv(cid:48)
k

such that M1 (cid:23) M2 (cid:23) · · · (cid:23) M1 (cid:23) 0. Let {u1, . . . , uk} be an orthonormal basis for Span{v1, . . . , vk}
such that, for all i = 1, . . . , k, Span{u1, . . . , ui} = Span{v1, . . . , vi}. Then Mi for i = 1, . . . , k need not be
almost commuting, and ( ¯Xi, . . . , ¯Xk) = (u1u(cid:48)
k) is the optimal SDP solution with optimal value
p = tr(M1).
Proof. ¯Xi are clearly feasible with objective value

1, . . . , uku(cid:48)

p = (cid:104)M1, u1u(cid:48)
k
(cid:88)

1(cid:105) + (cid:104)M2, u2u(cid:48)

2(cid:105) + · · · + (cid:104)Mk, uku(cid:48)
k(cid:105)
k
(cid:88)

=

(v(cid:48)

iu1)2 +

(v(cid:48)

iu2)2 + · · · +

(v(cid:48)

iuk−1)2 + (v(cid:48)

kuk)2

k
(cid:88)

i=2

i=k−1

i=1

k
(cid:88)

i=1

=

(cid:107)vi(cid:107)2

2 = tr(M1).

(32)

(33)

(34)

For any feasible solution, we have

k
(cid:88)

i=1

(cid:104)Mi, Xi(cid:105) ≤

k
(cid:88)

(cid:104)M1, Xi(cid:105) = (cid:104)M1,

i=1

k
(cid:88)

i=1

Xi(cid:105) ≤ (cid:104)M1, I(cid:105) = tr(M1),

since M1 (cid:60) Mi for all i and (cid:80)k

i=1 Xi (cid:52) I. So ¯Xi are optimal.

We next consider a rank-2 case to show the Mi need not be almost commuting. From the construction
above, represent M1 = v1v(cid:48)
2 for some v1 = γu1 and v2 = αu1 + βu2 and coef-
2v1(cid:107)v1v(cid:48)
ﬁcients γ, α, β. It is easy to show (cid:107)M1M2 − M2M1(cid:107)2 = v(cid:48)
1(cid:107)2 ≤
γ2αβ((cid:107)u1u(cid:48)
1(cid:107)2) = 2γ2αβ. Since there could exist u1 and u2 such that the commutator norm is
as large as 2γ2αβ, and unless one or more of the coeﬃcients is small, then M1 and M2 need not be almost
commuting.

2 and M2 = v2v(cid:48)

1(cid:107)2 = γ2αβ(cid:107)u1u(cid:48)

2(cid:107)2 + (cid:107)u2u(cid:48)

2 − u2u(cid:48)

1 + v2v(cid:48)

2 − v2v(cid:48)

28

G Extended Experiments

G.1 Assessing the ROP: random PSD Mi
For Mi that are random PSD matrices of rank k, we generate the matrix A ∈ Rd×k with i.i.d. Gaussian
samples and compute Mi = AA(cid:48).

Fraction of 100 trials with ROP
k = 7
k = 3
0.3
0.97
0.13
0.92
0.14
0.93
0.04
0.92
0.05
0.95

k = 10
0.14
0
0
0
0

k = 5
0.61
0.48
0.53
0.45
0.53

D d = 10
S
d = 20
P
d = 30
d
n
d = 40
a
R
d = 50

Table 3: Numerical experiments showing the percentage of trials where the SDP was tight for random
synthetic PSD Mi.

G.2 Assessing the ROP: HPPCA

Table 4 and Table 5 display the full experiment results of their abbreviated versions–Table 1 and Table 2–in
Section 5 of the main paper.

G.3 Assessing global optimality of local solutions

Further experiment details For 100 random experiments of each choice of σ, we obtain candidate
solutions ¯Xi from the SDP and perform a rank-one SVD of each to form ¯USDP, i.e.
¯USDP = [ ¯u1 · · · ¯uk],

u(cid:48) ¯Xiu,

¯ui = argmax
u:(cid:107)u(cid:107)2=1

while measuring how close the solutions are to being rank-1. In the case the SDP is not tight, the rank-1
directions of the Xi will not be orthonormal, so as a heuristic, we project ¯USDP onto the Stiefel manifold by
its QR decomposition. For comparison, we use the Stiefel majorization-minimization (StMM) solver with
a linear majorizer [14] to obtain a candidate solution ¯UMM and use Theorem 4.1 to certify it either as a
globally optimal or as a stationary point.

When executing each algorithm in practice, we remark that the results may vary with the choice of user
speciﬁed numerical tolerances and other settings. For the StMM algorithm, we choose a random initialization
of U each trial and run the algorithm either for speciﬁed maximum number of iterations or until the gradient
on the Stiefel manifold is less than some tolerance threshold; here we set tol = 10−10. Using MATLAB’s
CVX implementation to solve (SDP-P) and (5), we found setting cvx precision to high guarantees the
best results for returning tight solutions and verifying global optimality. However, iterates of the StMM
algorithm that converge close to a tight SDP solution may still not be suﬃcient for the feasibility LMI to
return a positive certiﬁcate if the solution is not numerically optimal to a high level of precision.

H Extension to the sum of Brocketts with linear terms

Given coeﬃcient matrices and vectors {(Mi, ci)}k
terms giving the following optimization problem that appears in [14]:

i=1, suppose the problem in (1) is augmented with linear

max
U :U (cid:48)U =I

k
(cid:88)

i=1

u(cid:48)

iMiui + c(cid:48)

iui.

29

(35)

Fraction of 100 trials with ROP
k = 7
k = 3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
0.99
1
1
1
1
1
1
1
0.97
0.99
0.97
1
0.96
1
0.99
1
0.98
1
0.99
1
0.92
0.98
0.98
0.99

k = 10
1
1
1
1
1
1
1
1
1
0.99
1
1
0.99
0.99
0.98
1
0.99
0.96
0.96
0.88

k = 5
1
1
1
1
1
1
1
0.98
1
1
1
1
0.99
0.98
0.97
1
1
0.99
0.97
0.96

=
v

=
v

1
,
1
[

2
,
1
[

] d = 10
d = 20
d = 30
d = 40
d = 50
] d = 10
d = 20
d = 30
d = 40
d = 50
] d = 10
d = 20
d = 30
d = 40
d = 50
] d = 10
d = 20
d = 30
d = 40
d = 50

4
,
1
[

3
,
1
[

=
v

=
v

(HPPCA) Numerical experiments
Table 5:
showing the percentage of trials where the SDP
was tight for instances of the HPPCA problem
as we vary d, k, and v using L = 2 groups with
samples n = [10, 40].

Fraction of 100 trials with ROP
k = 7
k = 3
1
1
0.98
1
0.98
0.99
0.99
0.98
0.96
0.97
0.99
1
0.98
1
0.99
1
0.92
0.98
0.98
0.99
1
1
1
1
1
1
0.97
1
0.98
1
1
1
1
1
1
1
0.99
1
0.98
1
1
1
1
1
1
1
1
1
1
1

k = 10
1
0.99
0.97
0.98
0.98
1
0.99
0.96
0.96
0.88
1
1
0.98
0.95
0.97
1
1
1
1
1
1
1
1
1
1

k = 5
0.99
0.98
0.93
0.91
0.95
1
1
0.99
0.97
0.96
1
1
1
1
0.98
1
1
1
1
1
1
1
1
1
1

=
n

=
n

] d = 10
0
d = 20
2
,
5
d = 30
[
d = 40
d = 50
] d = 10
0
4
d = 20
,
0
d = 30
1
[
d = 40
d = 50
] d = 10
0
8
d = 20
,
0
d = 30
2
[
d = 40
d = 50
d = 10
d = 20
d = 30
d = 40
d = 50
d = 10
d = 20
d = 30
d = 40
d = 50

]
0
0
4
,
0
0
1
[

]
0
0
2
,
0
5
[

=
n

=
n

=
n

Table 4:
(HPPCA) Numerical experiments
showing the percentage of trials where the SDP
was tight for instances of the HPPCA problem
as we vary d, k, and n using L = 2 groups with
noise variances v = [1, 4].

30

It is then easy to see that for the matrices

˜Mi :=

(cid:20)Mi
c(cid:48)
i

(cid:21)

,

ci
0

˜Xi :=

(cid:21)

(cid:20)Xi ui
u(cid:48)
1
i

,

Xi := uiu(cid:48)
i

(36)

that (cid:80)k
d](cid:48) ∈ R(d+1)×d and ed+1 to be the d+1-standard basis
vector in Rd+1. Extending (SDP-P), we obtain a generalized relaxation for the problem with linear terms:

iui = (cid:104) ˜Mi, ˜Xi(cid:105). Deﬁne A := [Id 0(cid:48)

iMiui+c(cid:48)

i=1 u(cid:48)

k
(cid:88)

(cid:104) ˜Mi, ˜Xi(cid:105)

i=1

max
˜Xi

s.t. A(cid:48)

k
(cid:88)

i=1

˜XiA (cid:52) I

(cid:104)AA(cid:48), ˜Xi(cid:105) = 1,

e(cid:48)
d+1

˜Xied+1 = 1

˜Xi (cid:23) 0.

(37)

(38)

(39)

By the Schur complement, the constraint ˜Xi (cid:23) 0 guarantees that Xi − uiu(cid:48)

i (cid:23) 0 and therefore also
Xi (cid:23) 0. The linear operator A acts to impose the relevant Fantope-like constraints onto the top-left d × d
positions of the primal variables, and the added constraint on the (d + 1, d + 1)th element of each ˜Xi forces
it to be 1. For dual variables ˜Zi ∈ Sd+1

+, ν ∈ Rk, and ξ ∈ R, the KKT conditions are

+ , Y ∈ Sd

˜Xi (cid:23) 0, A(cid:48)

k
(cid:88)

i=1

¯XiA (cid:22) I,

(cid:104)AA(cid:48), ˜Xi(cid:105) = 1,

e(cid:48)
d+1

˜Xied+1 = 1

AY A(cid:48) = ˜Mi + ˜Zi − ¯νiAA(cid:48) − ξed+1e(cid:48)

d+1,

Y (cid:23) 0

(cid:104)I − A(cid:48)

k
(cid:88)

i=1

¯XiA, Y (cid:105) = 0

(cid:104) ˜Zi, ˜Xi(cid:105) = 0
˜Zi (cid:23) 0,

(40)

(41)

(42)

(43)

(44)

which, in fact, are the same KKT conditions as before. If we denote Zi := A(cid:48) ˜ZiA to be the top d + 1 × d + 1
positions of ˜Zi, multiplying (41) by A(cid:48) on the left and A on the right gives back exactly (KKT-b) for
the relaxation in (SDP-P).

References

[A1] Podosinnikova, A., Perry, A., Wein, A., Bach, F., D’Aspremont, A. & Sontag, D. Overcomplete in-
dependent component analysis via SDP. The 22nd International Conference On Artiﬁcial Intelligence
And Statistics. pp. 2583-2592 (2019)

[A2] Journ´ee, M., Bach, F., Absil, P. & Sepulchre, R. Low-rank optimization on the cone of positive

semideﬁnite matrices. SIAM Journal On Optimization. 20, 2327-2351 (2010)

[A3] Boumal, N., Voroninski, V. & Bandeira, A. Deterministic Guarantees for Burer-Monteiro Factorizations
of Smooth Semideﬁnite Programs. Communications On Pure And Applied Mathematics. 73, 581-608
(2020)

[A4] Bandeira, A., Boumal, N. & Singer, A. Tightness of the maximum likelihood semideﬁnite relaxation

for angular synchronization. Mathematical Programming. 163, 145-167 (2017)

[A5] Zhou, F. & Low, S. Conditions for Exact Convex Relaxation and No Spurious Local Optima. IEEE

Transactions On Control Of Network Systems. (2021)

31

[A6] Burer, S. & Monteiro, R. Local minima and convergence in low-rank semideﬁnite programming. Math-

ematical Programming. 103, 427-444 (2005)

[A7] Cifuentes, D. & Moitra, A. Polynomial time guarantees for the Burer-Monteiro method. ArXiv Preprint

ArXiv:1912.01745. (2019)

32

