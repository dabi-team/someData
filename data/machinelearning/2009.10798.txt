ORACLE: Collaboration of Data and Control
Planes to Detect DDoS Attacks

Sebasti´an G´omez Mac´ıas
Universidad de Antioquia
Medell´ın, Colombia
sebastian.gomez3@udea.edu.co

Luciano Paschoal Gaspary
Universidade F. do Rio Grande do Sul
Porto Alegre, Brasil
paschoal@inf.ufrgs.br

Juan Felipe Botero
Universidad de Antioquia
Medell´ın, Colombia
juanf.botero@udea.edu.co

0
2
0
2

p
e
S
2
2

]
I

N
.
s
c
[

1
v
8
9
7
0
1
.
9
0
0
2
:
v
i
X
r
a

Abstract—The possibility of programming the control and
data planes, enabled by the Software-Deﬁned Networking (SDN)
paradigm, represents a fertile ground on top of which novel
operation and management mechanisms can be fully explored,
being Distributed Denial of Service (DDoS) attack detection based
on machine learning techniques the focus of this work. To carry
out the detection, this paper proposes ORACLE: cOllaboRation
of dAta and Control pLanEs to detect DDoS attacks, an architec-
ture that promotes the coordination of control and data planes to
detect network attacks. As its ﬁrst contribution, this architecture
delegates to the data plane the extraction and processing of
trafﬁc information collected per ﬂow. This is done in order to
ease the calculation and classiﬁcation of the feature set used in
the attack detection, as the needed ﬂow information is already
processed when it arrives at the control plane. Besides, as the
second contribution, this architecture breaks the limitations to
calculate some features that are not possible to implement in
a traditional OpenFlow-based environment. In the evaluation of
ORACLE, we obtained up to 96% of accuracy in the testing
phase, using a K-Nearest Neighbor model.

Index Terms—DDoS, Security, SDN, P4, Machine-Learning

I. INTRODUCTION

A DDoS attack is considered a powerful weapon against
Internet security due to its ability to bring down even the
largest websites by overloading servers’ resources [1]. In
turn, Software-Deﬁned Networking (SDN) is an emerging
network paradigm that decouples the control and data planes
where the network control is logically centralized and pro-
grammable. Considering the different proposed DDoS detec-
tion approaches, several leverage the SDN advantages: global
network view and control plane programmability. Among
these mechanisms, those based on Machine Learning (ML)
algorithms are considered the best in terms of key metrics
such as detection speed, accuracy, and reliability [2], [3].

A traditional ML-based detection system is mainly based
on: i) a trafﬁc information collecting mechanism, ii) a feature
calculation module, and iii) a classiﬁcation model; all of them
implemented at the control plane. The collecting mechanism
obtains trafﬁc information from the packets that arrive at
the SDN controller (Packet in) and also from the statistics
provided by OpenFlow (SDN southbound interface). The
feature calculation module processes the collected information
to build the set of descriptors, and, ﬁnally, the classiﬁcation
module classiﬁes this set of features in search of possible
ongoing DDoS attacks. However, in the implementation of

these detection systems, we observe two design challenges,
which we enumerate and describe next.

The ﬁrst challenge is to prevent overloading the control
plane due to the execution of tasks related to polling infor-
mation from the network trafﬁc, as well as the calculation
and classiﬁcation of the feature set. To carry out these tasks,
the detection system has to store, update, and ﬁlter different
information samples, which are sorted and processed per ﬂow,
connection, or source-destination pair. The execution of these
tasks in the presence of large amounts of packets and ﬂows
can easily overload the control plane.

The second challenge lies in extracting, in real-time, the
information needed to calculate the feature set selected as
the best trafﬁc descriptors. In an OpenFlow-based SDN en-
vironment, limited processing can be performed on a per-
packet basis at the data plane. The extraction of sufﬁcient
trafﬁc information with ﬁne granularity needed to calculate
a wide variety of features in real-time becomes impractical.
Therefore, from a huge amount of trafﬁc features proposed
in the literature, just a speciﬁc group of them (e.g., ﬂow’s
byte/packet count and duration) are possible to implement.

In order to overcome these challenges, we propose ORA-
CLE, an approach based on the cOllaboRation of dAta and
Control pLanEs to detect DDoS attacks using the paradigm
of programmable data planes. The proposed approach fully
explores the programming of packet processing at a switch
level. To program the data plane, we use the P4 programming
language [4] together with the P4Runtime interface (to provide
communication with the control plane). At the data plane, we
propose a hash-based data structure to store customized per-
ﬂow information. We also propose a mechanism to periodi-
cally report such information to the control plane, where the
feature set is subsequently calculated and classiﬁed per each
reported ﬂow. The main contributions of ORACLE are:

• A novel strategy that is expected to be deployed over the
proposed SDN-P4 architecture to periodically calculate
speciﬁc ﬂow features to be used by the detection system.
• An architecture that delegates to the data plane the
tasks of collecting, processing, and storing the required
information per trafﬁc ﬂow. This facilitates the calculation
and classiﬁcation of the feature set by the control plane.
The architecture is not limited only to DDoS attacks. On

 
 
 
 
 
 
the contrary, it provides the framework for the calculation
of any feature needed to detect other attacks.

II. BACKGROUND AND RELATED WORK

According to [5], for trafﬁc classiﬁcation, the list of po-
tential features is divided in ﬁve categories: packet level (e.g.,
packet length mean), ﬂow level (e.g., average packet length and
packets per ﬂow), connection level (e.g., advertised window
sizes), intra-ﬂow level (e.g., features based on inter-arrival
times between packets of the same ﬂow), and multiﬂow.

In order to extract information at the ﬂow or intra-ﬂow
level, three ﬂow deﬁnitions apply [5]: unidirectional ﬂow,
made up of packets sharing the same ﬁelds (source/destination
IP, source/destination TCP/UDP ports and transport protocol
number); bidirectional ﬂow, made up of a pair of unidirectional
ﬂows that are going in opposite directions between the same
source and destination IP addresses and ports; and full ﬂow, a
bidirectional ﬂow captured during its entire lifetime.

Recent research efforts indicate that SDN is suitable for
the implementation of sophisticated software solutions that
allow detecting DDoS attacks. The following proposals, im-
plemented at the control plane, make use of different feature
sets and ML-based detection techniques.

In [6], the authors perform the DDoS detection by polling,
each second, OpenFlow statistics per each unidirectional ﬂow.
To calculate the feature set, they store several samples of the
same bidirectional ﬂow (computed in the control plane from
the unidirectional ﬂows) to calculate features based on average
and standard deviation (std) at ﬂow level. Results show 96%
of accuracy with the Random Forest (RF) classiﬁer.

The authors of [7] propose an integrated framework that
comprises two operational phases at the control plane: 1) a
lightweight processing phase that consists of monitoring the
ﬂows looking for an entropy anomaly, and 2) a heavyweight
processing phase that is only invoked when an anomalous ﬂow
is detected. The heavyweight process uses OpenFlow statistics
(packet count, byte count, duration) at ﬂow level to detect
DDoS attacks. A Support Vector Machine (SVM) is used as
the ML-classiﬁer providing a detection accuracy score of 88%.
This means that the reduction of heavy processes at the control
plane is achieved at big costs in terms of accuracy.

In the previous detection systems, and also in others such
as [8], we noticed that the implemented features are limited
to a speciﬁc group coming from header ﬁeld values extracted
from the OpenFlow packet in event. In [9], we can ﬁnd a wide
variety of features that one could implement to detect different
attacks (e.g., DDoS). Among the huge list of features, those
at the intra-ﬂow level and several at the connection level are
not possible to calculate in an SDN/OpenFlow environment.
Consequently, we propose ORACLE, where the calculation of
such features is possible, in real-time, via the programming
and coordination of the data and control planes.

Fig. 1. ORACLE system architecture

III. ORACLE: COLLABORATION OF DATA AND CONTROL
PLANES TO DETECT DDOS ATTACKS

A. Architecture

ORACLE is organized in two modules, namely Data Plane
and Control Plane, which we show in Fig. 1 and describe next.
Data Plane: ORACLE relies on a time window-based
implementation. During each window, the data plane performs
two tasks simultaneously: it i) collects new ﬂow information,
and ii) reports ﬂow information collected and processed in
the previous time window to the control plane. The data
plane’s module is composed of the following four components,
developed using P4 primitives such as registers, clone packet,
hash functions, and basic binary operations (+,-,*).

• Data Structure: It is used for the statistics organization,
management, and storage per unidirectional ﬂow. To
access them,
the ﬂow identiﬁer (ﬂow-id) is mapped,
through a hash function, to the respective index (position
where the statistics are stored) in the data structure.
• Statistics Collector: In charge of extracting and pro-
cessing the needed information from the bit string of
every packet traversing the device. With this information,
the statistics of the ﬂow associated with the packet are
updated.

• Pipeline Handler: In charge of controlling the time win-
dow duration and validating if the data structure contains
new ﬂow information to report. If so, the incoming packet
is cloned and forwarded to the Information Transmitter
component along with the per-ﬂow information that will
be subsequently sent to the controller.

• Information Transmitter: This component transforms the
cloned packet in a report packet that is forwarded to the
control plane using the P4Runtime interface. The cloned
packet is modiﬁed by eliminating its payload and adding
the ﬂow information as a new custom header.

Control Plane: It is composed of an SDN controller and a
trafﬁc classiﬁcation component connected via REST-API. To
classify the reported ﬂows, three components are involved:

PortExtract FlowInformationClassiﬁerControllerP4RuntimeRESTAPIMachineLearning ModelAlertData Plane  /  PipelineFeaturesCalculationPipelineHandlerStatisticsCollectorInformationTransmitterP4RuntimeDataStructurePortPacketClonedStore  /UpdateConsultEraseReport PacketControl PlaneFig. 2. Mechanism to store statistics per ﬂow.

• Extract Flow Information: This component extracts the
custom header from the report packet and parses it to get
the information of every ﬂow sent by the data plane.
• Feature Calculation: It builds a tuple with the features

set calculated with the ﬂow information.

• Classiﬁer: An already loaded ML model classiﬁes every
tuple received in DDoS or Benign class. When a possible
attack is detected, an alarm is issued.

B. Data Plane Storage Mechanism

ORACLE collects and stores statistics of packets belonging
to each ﬂow on the programmable switches. It guarantees that
ﬂow statistics can be consulted and updated each time a new
packet belonging to the ﬂow arrives to the P4 device.

We use P4 registers to deﬁne a data structure to store ﬂow
statistics. A register is a ﬁxed-size array that uses an index
to point to the stored elements. To consolidate the structure,
we use a set of such registers to build a matrix-like structure,
where each row stores values of the same statistics belonging
to different ﬂows, and each column stores different statistics
of the same unidirectional ﬂow (see Fig. 2). To designate a
unique index (column) to each unidirectional ﬂow, we use
the hash function with an input value (Flow-Id) that consists
of source/destination IP, source/destination port, and transport
protocol. The hash’s output is the index mapped to the ﬂow.
Figure 2 shows how per-ﬂow statistics are stored and
updated by the Statistics Collector component. Once a packet
enters the P4 device, its bit string is parsed to extract and map
each header ﬁeld value to P4 variables. From these variables,
the collector builds the 5-tuple (Flow-Id) to calculate the hash
function. Once having the ﬂow index, the ﬂow statistics are
updated in the data structure with the needed information that
is obtained by processing the P4 variables.

As we are working with bidirectional ﬂows and the method
stores unidirectional ﬂows, it is important to identify the pair
of ﬂows (indexes) deﬁning the bidirectional ﬂow within the
data structure. To know the index of the opposite direction
of the ﬂow, we calculate again the hash function exchanging
the order of the ﬂow identiﬁer ﬁelds (see Fig. 2). Hence, we

Fig. 3. Lane-based mechanism to handle time windows and buffers.

deﬁne forward (FWD) as the direction of the ﬁrst packet of
the ﬂow and backward (BWD), the opposite direction.

C. Data Plane Flow Reporting Mechanism

The Pipeline Handler component is in charge of reporting
ﬂow information to the control plane at the beginning of each
time window. In order to forward the information of all bi-
directional ﬂows collected at the previous time window to the
control plane, it is necessary to know the indexes that point
to the speciﬁc ﬂows within the data structure. To ease this
process, each time a new ﬂow is instantiated at the statistics
data structure, both indexes (FWD and BWD) are stored in a
new data structure composed of two registers and a counter
(see Fig. 2). One register stores the indexes of the FWD
unidirectional ﬂows, and the other stores the indexes of the
BWD ﬂows. Hence, each column will contain the indexes
of a different bidirectional ﬂow and the counter will be just
a reference to know the position of the last pair of stored
indexes. This data structure works like a LIFO buffer: indexes
are stored sequentially as the ﬂows are instantiated. When the
information is to be sent to the control plane, the indexes are
consumed from the buffer in the reverse arrival order.

The implementation of the control data structure with a
single buffer implies that the data plane interrupts the process
of collecting information from new ﬂows until the buffer is
emptied. As the system must be able to perform both tasks
(collecting and reporting ﬂows to the control plane) simulta-
neously, we propose to abstract the idea of two independent
monitoring “lanes”, each one with its buffer. Therefore, is
possible to delegate, over a same time window, one task to
each lane, avoiding the process to be interrupted (see Fig. 3).
Using this strategy based on independent buffers to store the
indexes of bidirectional ﬂows, the Pipeline Handler component
can validate, in each time window, if there are ﬂow statistics
available to forward to the control plane. If so, this component
looks for per-ﬂow statistics from the data structure, clones the
number of incoming packets needed to transmit this informa-
tion, and delegates the transmitting process to the respective
component. The Information Transmitter component embeds
the information of each ﬂow into a custom header. The number
of ﬁelds (ﬂows) within this header should be as big as the
network MTU value to avoid injecting too many report packets
into the network.

Index 1(FWD)Index 2(BWD)Indexes BWDIndexes FWDHASH ( Src_IP,  Dst_IP,  Src_port,  Dst_port,  Protocol )HASH (Dst_IP,  Src_IP,  Dst_port,  Src_Port,  Protocol )ParserStatistic 1Statistic 2Statistic n. . . STATISTICSDATASTRUCTURE5901234567891011...M01234567891011M......012345MSameuni-dirFlowCONTROL DATASTRUCTURE(BUFFER)SourceDestinationForward directionBackward direction......SameStatistic3Counter Indexesbi-dirFlowt  SecTime-Window 1Time-Window 2Time-Window 3CollectingIndexesBWDIndexesFWD......CounterLeftSendingIndexesBWDIndexesFWD......CounterRightCollectingCollectingSendingIndexesBWDIndexesFWDCounterRightRIGHT LANELEFT LANEIndexesBWDIndexesFWDCounterRightt 2t 3tIndexesBWDIndexesFWD......CounterLeftIndexesBWDIndexesFWD......CounterLeft = 0..................TABLE I
SELECTED FEATURE SET

TABLE II
INFORMATION PER-FLOW REPORTED BY THE DATA PLANE

#

F1

F2

F3

F4

Feature name

Description

FlowDuration

Flow duration in microseconds

FlowIATStd

avgPacketSize

BwdPktLenStd

Standard deviation of packet inter-arrival
times of the same ﬂow.
Average of packet payload sizes of the
same ﬂow.
Standard deviation of packet payload
sizes of the same ﬂow in the backward
direction

Level / Type

Flow /
(Bidirectional)
Intra-Flow /
(Bidirectional)
Flow /
(Bidirectional)
Intra-Flow /
(Unidirectional)

IV. DETECTION SYSTEM IMPLEMENTED ON ORACLE

As proof of concept, we implemented a DDoS attack
detection system that uses the feature set listed in Table I.
These features are recommended in [9], where the authors
used the RandomForestRegressor technique to select, among
80 features, a small set considered the most adequate for
DDoS attack detection. This set is made up of two features
at ﬂow level (F1 and F3) and two at intra-ﬂow level (F2 and
F4). On the other hand, there are features that are calculated
considering the statistics in both ﬂow directions (bidirectional),
and one of them (F4) considering only the BWD direction.

In Table I, F2, F3, and F4 are based on average (avg) and
standard deviations (std) operations, calculated using Eq. 1 and
2, respectively. xi corresponds to an information unit extracted
from packet i, which belongs to a group of n packets (of a
same ﬂow). The meaning of xi changes depending on the
feature to calculate; for F2, it corresponds to the packet inter-
arrival time (IAT), and for F3-F4, it corresponds to the size in
bytes of the packet’s payload.

avg = x =

(cid:80)n

i:1 xi
n − 1

std =

(cid:115) (cid:80)n

i:1 (xi − x)2
n − 1

=

(cid:115) (cid:80)n

i:1 x2

i − 2x (cid:80)n
n − 1

i:1 xi + nx2

(1)

(2)

In an SDN/OpenFlow environment, the features at ﬂow level
(F1 and F3) can be calculated from OpenFlow statistics. The
real challenge is to calculate the intra-ﬂow features (F2 and
F4) because: i) OpenFlow does not permit to know the packet
IAT, and ii) to calculate the std of an intra-ﬂow measure, every
xi value is required at the control plane to subtract the mean
from it when the sum is being computed (see Eq. 2, left side).
ORACLE allows overcoming these limitations as it stores
individual information for each packet, even IAT values, and
reports it periodically to the control plane. However,
this
may result in high latency and storage/processing resource
consumption. A more ambitious solution would be to calculate
these features directly on the data plane and avoid sending
every xi. This solution is not possible because programmable
data planes are usually limited in the available mathematical
operations (e.g., division and square root are not allowed).

Therefore, we propose the following alternative. The tradi-
tional std (Eq. 2) is transformed using arithmetic properties to
have the right side representation. Now, this new representation

#

F1
S2
S3
S4

S5

S6

S7

S8

Feature / Statistic

Description

FlowDuration
TotFwdPkt
TotBwdPkt
TotLenFwdPkt

TotLenBwdPkt

TotLenBwdPktSqrt

IATTotal

IATTotalSqrt

Flow duration in microseconds
Total number of packets in the FWD direction
Total number of packets in the BWD direction
Aggregation of packets’ payload size in FWD
direction ((cid:80) xi)
Aggregation of packets’ payload size in BWD
direction ((cid:80) xi)
Aggregation of packets’ payload size
square in BWD direction (cid:0)(cid:80) x2
i
Aggregation of the packet inter-arrival times
((cid:80) xi)
Aggregation of the packet inter-arrival times
square (cid:0)(cid:80) x2
i

(cid:1)

(cid:1)

i

depends on two sums: the aggregation of values ((cid:80) xi), and
(cid:1). Using the data plane
the aggregation of square values (cid:0)(cid:80) x2
mechanisms proposed in ORACLE, we calculate, directly at
the data plane, the result of both sums per each ﬂow, as the on-
going packets traverse the data plane device. Therefore, when
these aggregation results are reported together with the packet
counters to the control plane (see Table II), the controller has
only to replace them in Eq. 1 and 2 to calculate F2, F3, and
F4 per each ﬂow in an easy and fast way, as speciﬁed below.
Regarding F1, it is calculated directly in the data plane and
reported to the control plane together with the statistics.

• FlowIATStd (F2):

n = S2 + S3 (Bidir.)

;

x =

S7
n − 1

(cid:115)

std =

• avgPacketSize (F3):

S8 − 2 ∗ x ∗ S7 + nx2
n − 1

n = S2 + S3 (Bidir.)

;

x =

• BwdPktLenStd (F4):

n = S3 (U nidir.)

;

x =

S4 + S5
n − 1

S5
n − 1

(cid:115)

std =

S6 − 2 ∗ x ∗ S5 + nx2
n − 1

V. EVALUATION

(3)

(4)

(5)

(6)

(7)

A. Experimental Setup and Methodology

We replicated the packet

trace used in [9] over a real
scenario. This experiment consisted of a 20 minute-long DDoS
attack generated from different devices located in an external
network, trying to deny the service provided by a web service
located in the DMZ of a local network. This trace, which has
trafﬁc captured for 90 minutes, is composed of 762,973 DDoS
packets and 1,382,900 benign packets, stored in a PCAP ﬁle.
Fig. 4 shows the implemented scenario. The data plane,
emulated using Mininet, consists of: i) a BMV2 virtual switch
with P4 support (in charge of executing the data plane strat-
egy); ii) a host responsible for reproducing the trafﬁc trace
using the TCPReplay tool (representing the external network
where the DDoS attack is originated); and iii) a victim host

sisted of training the ML models in an ofﬂine manner. We used
and compared two ML classiﬁers per each time window: RF
and K-Nearest Neighbor (KNN) (8 models in total). To build
the models, we implemented cross-validation of 10 iterations
using the datasets shown in Table III. In each iteration, a grid
search was performed to ﬁnd the best set of hyperparameter
values that lead to the most accurate models. After ten itera-
tions, the best model was chosen (among the ten evaluated).
In Table IV, we show the best set of hyperparameter values
with which it was possible to train the best model found for
each time window duration.

TABLE IV
BEST HYPERPARAMETERS / TRAINING TIME

Random Forest (RF)

-

M1: 5 Sec M2: 20 Sec M3: 40 Sec M4: 60 Sec

Max depth
N estimators
Training time [S]

6
300
332.8 ± 6

6
500
304.4 ± 8.1

6
500
264.4 ± 20

6
300
272.6 ± 20

K-Nearest Neighbor (KNN)

-

M5: 5 Sec M6: 20 Sec M7: 40 Sec M8: 60 Sec

# Neighbors
Training time [S]

3
59.8 ± 2

3
37.7 ± 1

3
36.3 ± 3

3
26.7 ± 1

In the testing phase, each model was pre-loaded into the
classiﬁer component. Then, the WL2 was replicated eight
times per each model for the online evaluation. In these
experiments, all packets of WL2 were also tagged to compare
the tag of each ﬂow with the classiﬁer’s classiﬁcation result
(in real-time). In this phase, both the feature set and the ﬂow
tag were not stored. Instead, they were sent to the classiﬁer
component.

The metrics used in the testing phase to evaluate the
detection performance are based on the number of true positive
(TP), true negative (TN), false positive (FP) and false negative
(FN) classes (DDoS and Benign) obtained during the experi-
ment. The metrics are introduced as follows:

• Accuracy is the number of correctly detected cases in the

total ﬂow sample, considering both classes.

Accuracy =

T N +T P
T N +T P +F P +F N

(8)

• Recall represents the proportion of DDoS samples cor-
rectly detected in relation to the total DDoS ﬂow samples.
It is best known as the True Positive Rate (TPR).

Recall = T P R = T P

T P +F N

(9)

• Selectivity represents the proportion of Benign samples
correctly detected in relation to the total Benign ﬂow
samples; also known as the True Negative Rate (TNR).

Selectivity = T N R = T N

T N +F P

(10)

Fig. 4. Experimental setup.

(Web Server), where all replicated trafﬁc is forwarded to. In
turn, the control plane is composed by the ONOS controller,
which is connected to the BMV2 switch using the P4Runtime
interface. This plane has a classiﬁer component, which loads a
previously trained ML-model. This scenario was implemented
on a Core i5 5200 computer, with 12 GB of RAM and the
Ubuntu 18.04 LTE operating system.

The implementation of our proposed system was exposed
to two different workloads, one for the training phase of the
ML model (WL1) and another for the testing phase of the
same model (WL2). To create both workloads, the pcap ﬁle
was divided into 18 sets of 5 minutes each, labeled from 1
to 18 considering their order in the trace. The sets with odd
numbering were put together to build WL1. Then, the sets
with even numbering were also joined to build WL2.

Before training the classiﬁcation model, we had to build
a database containing ﬂow information. To create it, an
ONOS application was developed to store, inside a .csv ﬁle,
the feature set, and the respective tag (class: “Benign”, or
“DDoS”) for every ﬂow reported by the data plane (see Fig
4). These tags, needed to train the supervised model, were
obtained directly from the packets belonging to WL1. They
were previously marked (00: Benign, 11: DDoS) by modifying
the last two bits of the IPv4 ToS header ﬁeld.

Besides focusing on training the best ML model to detect
DDoS attacks, we also decided to investigate how the detection
system could be affected by different time window values at
the data plane. For this assessment, we built a different training
dataset per each time window value. Table III shows the char-
acterization of the training datasets constructed considering
different time window values (5, 20, 40, and 60 seconds).

TABLE III
TRAINING DATASET CHARACTERIZATION

Dataset per Time Window Duration

Tag / Time Window

5 Sec

20 Sec

40 Sec

60 Sec

Benign Flows
DDoS Flows

61,736
49,012

40,462
45,688

31,894
44,511

29,670
43,255

Total Flows per Dataset

110,748

86,150

76,405

72,925

• F1 Score can be seen as the weighted average of the

precision and recall metrics values.

After creating the mentioned databases, the next step con-

F 1 score =

2T P
2T P +F P +F N

(11)

ONOSgRPCConnectionAttacker(TCP_Replay)Switch(BMV2)DataSet forTrainingCSVReportPacketVictimWeb ServerClassiﬁerComponentFeatures+ TagFeatures+ TagMachine Learning ModelB. Experimental Results

Table IV shows the average time to calculate the best set
of hyperparameters and train the model with them. We notice
that KNN is the best option when it comes to the performance
of the training phase, as it is faster than RF.

Fig. 5 shows the detection accuracy results as a function of
the time window duration (in the testing phase). It is possible
to observe that the eight models trained lead to an accuracy
higher than 93%, which is within the upper range seen in
the literature [6], [7]. Besides, the KNN accuracy results are
higher than the RF ones for all time windows. Nevertheless,
the difference between models does not exceed 1.2%.

Increasingly better results are obtained as the time window
duration increases, reaching the best values for time windows
of 60 seconds. We highlight here a vital trade-off between
accuracy and detection delay. A signiﬁcant
time window
duration might lead to an intolerably high latency to detect
and mitigate a DDoS attack. We advocate that as the detection
rates vary just slightly (1.1% for KNN and 1.7% for RF), one
should favor lower time window durations.

Table V shows the results obtained for the remaining
metrics. One can observe how accurate every model is in
classifying each class separately (Recall: “DDoS”, Selectivity:
“Benign”). Also, the F1 score shows the second indicator of
accuracy. Note that the main interest of the detection system is
to have a classiﬁcation model very precisely classifying DDoS
(low FN) and, at the same time, presenting the least possible
number of false alarms (low FP). According to the metrics,
a higher Recall value represents a model more accurately
detecting DDoS, and a higher Selectivity value represents a
model with a lower number of false alarms (see Equations
9 and 10). According to that, the M1 model is the most
accurate in detecting DDoS attacks, with a Recall value of
98%. However, M1 has the lowest selectivity value, which
makes this model one of the least reliable, since the false alarm
rate would be higher than the other models. This analysis is
reﬂected in the low value of the F1 Score.

Aiming at a better trade-off among accuracy, false alarms,
and time window duration, we consider that M3 and M7,
both in the 40-second window, are the best models to use by
our detection system. These models have the closest Recall
and Selectivity values, which means a more balanced choice
concerning the mentioned optimization aspects.

VI. CONCLUSION AND FUTURE WORK

In this paper, we presented ORACLE, an architecture im-
plemented over an SDN/P4 environment that promotes the
cooperation of both the control and data planes to detect
network attacks using Machine Learning models. ORACLE
leverages data plane programmability to extract ﬂow informa-
tion at a higher level of granularity. This results in a more
straightforward calculation and classiﬁcation of the feature
set in the attack detection process. As proof of concept, we
implemented a DDoS detection system based on the computa-
tion of intra-ﬂow level features, which cannot be implemented
in a traditional SDN/OpenFlow environment. Obtained results

Fig. 5. Accuracy of the evaluated models as a function of window duration

TABLE V
COMPARISON OF METRIC RESULTS OBTAINED IN THE TESTING PHASE

Random Forest (RF)

T.W. / Metric

Recall

Selectivity

F1 Score

M1: 5 seg.
M2: 20 seg.
M3: 40 seg.
M4: 60 seg.

0.982 ± 0.001
0.933 ± 0.005
0.957 ± 0.002
0.963 ± 0.003

0.920 ± 0.003
0.939 ± 0.003
0.957 ± 0.003
0.941 ± 0.002

0.934 ± 0.002
0.933 ± 0.003
0.952 ± 0.003
0.959 ± 0.002

K-Nearest Neighbor (KNN)

T.W. / Metric

Recall

Selectivity

F1 Score

M5: 5 seg.
M6: 20 seg.
M7: 40 seg.
M8: 60 seg.

0.967 ± 0.002
0.951 ± 0.003
0.970 ± 0.003
0.970 ± 0.003

0.935 ± 0.003
0.943 ± 0.004
0.967 ± 0.003
0.946 ± 0.003

0.937 ± 0.003
0.945 ± 0.003
0.962 ± 0.002
0.964 ± 0.002

show that the detection system has an accuracy rate of up to
96% while reducing the processing complexity performed by
the controller. As future work, we envisage making ORACLE a
comprehensive detection system capable of detecting different
types of attacks.

REFERENCES

[1] S. Weisman.

(2019) What
and

(ddos)

attack
norton.

service
them?
internetsecurity-emerging-threats-what-is-a-ddos-attack-30sectech-by-norton.
html

Available:

[Online].

distributed
you
can

denial
of
about
do
https://us.norton.com/

a

is
what

[2] J. C. C. Chica et al., “Security in sdn: A comprehensive survey,” Journal

of Network and Computer Applications, p. 102595, 2020.

[3] N. Sultana et al., “Survey on sdn based network intrusion detection
system using machine learning approaches,” Peer-to-Peer Networking and
Applications, vol. 12, no. 2, pp. 493–501, Mar 2019.

[4] P. Bosshart et al., “P4: Programming protocol-independent packet pro-

cessors,” SIGCOMM CCR, vol. 44, no. 3, pp. 87–95, Jul. 2014.

[5] T. T. Nguyen and G. Armitage, “A survey of techniques for internet trafﬁc
classiﬁcation using machine learning,” IEEE communications surveys &
tutorials, vol. 10, no. 4, pp. 56–76, 2008.

IEEE ISCC 2017.

[6] G. A. Ajaeiya et al., “Flow-based intrusion detection system for sdn,” in
IEEE, 2017, pp. 787–793.
[7] A. S. da Silva et al., “Atlantic: A framework for anomaly trafﬁc detection,
IEEE, 2016,

classiﬁcation, and mitigation in sdn,” in IEEE NOMS 2016.
pp. 27–35.

[8] C. Yoon et al., “Enabling security functions with sdn: A feasibility study,”

Computer Networks, vol. 85, pp. 19–35, 2015.

[9] I. Sharafaldin, A. H. Lashkari, and A. A. Ghorbani, “Toward generating
a new intrusion detection dataset and intrusion trafﬁc characterization.”
in ICISSP, 2018, pp. 108–116.

5204060Time-Window Duration [Sec]0.700.750.800.850.900.951.00Accuracy0.9480.9480.9590.9590.9450.9360.9490.953Accuracy Results in Testing PhaseKNNRF