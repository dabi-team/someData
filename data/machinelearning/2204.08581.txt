On Parametric Optimal Execution and Machine Learning
Surrogates ∗

Tao Chen † Mike Ludkovski ‡ Moritz Voß §

April 29, 2022

Abstract

2
2
0
2

r
p
A
8
2

]

R
T
.
n
i
f
-
q
[

2
v
1
8
5
8
0
.
4
0
2
2
:
v
i
X
r
a

We investigate optimal order execution problems in discrete time with instantaneous price
impact and stochastic resilience. First,
in the setting of linear transient price impact we
derive a closed-form recursion for the optimal strategy, extending the deterministic results
from Obizhaeva and Wang [47]. Second, we develop a numerical algorithm based on dynamic
programming and deep learning for the case of nonlinear transient price impact as proposed
by Bouchaud et al. [22]. Speciﬁcally, we utilize an actor-critic framework that constructs two
neural-network (NN) surrogates for the value function and the feedback control. The ﬂexible
scalability of NN functional approximators enables parametric learning, i.e., incorporating sev-
eral model or market parameters as part of the input space. Precise calibration of price impact,
resilience, etc., is known to be extremely challenging and hence it is critical to understand sen-
sitivity of the execution policy to these parameters. Our NN learner organically scales across
multiple input dimensions and is shown to accurately approximate optimal strategies across a
wide range of parameter conﬁgurations. We provide a fully reproducible Jupyter Notebook with
our NN implementation, which is of independent pedagogical interest, demonstrating the ease
of use of NN surrogates in (parametric) stochastic control problems.

Keywords: optimal execution, parametric control, neural network surrogates, stochastic resilience

1

Introduction

In the past decade an extensive literature has analyzed optimal execution of trades within a micro-
structural framework that accounts for the interaction between trades, prices, and limit order book
liquidity. For example, one notable strand follows the approach of Obizhaeva and Wang [47] which
is able to generate closed-form formulas for optimal trading strategy with linear transient price
impact. However, any practical use of these elegant mathematical derivations must immediately
confront the strong dependence of the solution on the given model parameters. Concepts such as
order book resilience or book depth are mathematical abstractions and are not directly available
in the real world. Similarly, parameters such as inventory penalty, are model-speciﬁc and have to
be entered by the user. Consequently, model calibration becomes highly nontrivial and in turn
requires understanding the interaction between model parameters and the resulting strategy. In
parallel, model risk, i.e. mis-speciﬁcation of the dynamics, is also a major concern. Model risk can
be partially mitigated by considering more realistic nonlinear models, but this comes at the cost of
losing closed-form solutions.

Motivated by these issues, in this article we approach optimal execution using the lens of
parametric stochastic control. To this end, we investigate numerical algorithms that determine

∗The accompanying Jupyter Notebook is available at https://github.com/moritz-voss/Parametric_Optimal_

Execution_ML.

†University of Michigan, Department of Mathematics, 530 Church Street Ann Arbor, MI 48109-1043, USA, email

chenta@umich.edu.

‡University of California Santa Barbara, Department of Statistics & Applied Probability, Santa Barbara, CA

93106-3110, USA, email ludkovski@pstat.ucsb.edu.

§University of California Los Angeles, Department of Mathematics, Los Angeles, CA 90095, USA, email

voss@math.ucla.edu.

1

 
 
 
 
 
 
the optimal execution strategy jointly in terms of the state variables (inventory and limit order
book spread/mark-up), as well as model parameters (instantaneous price impact, resilience factor,
inventory penalty, etc). We consider augmenting 1-4 model parameters to the training space,
yielding multi-dimensional control problems.

Our contribution is two-fold. In terms of the numerical methods, we propose a direct approach
to parametric control that focuses on generating functional approximators to the value function.
A concrete choice of such statistical surrogates that we present below are Neural Networks (NNs).
Our approach employs NNs to approximate the Bellman equation and can be contrasted with other
ways of using NNs, such as Deep Galerkin Methods [12, 13, 32] for PDEs, or parameterization of
the feedback control [14, 39] in tandem with stochastic gradient descent. One advantage of our
method is its conceptual simplicity; as such it as an excellent pedagogical testbed for machine
Indeed, given that optimal execution is now a core problem that is familiar
learning methods.
to anyone working in mathematical ﬁnance, we believe that this example oﬀers a great entryway
to students or researchers who wish to understand and “play” with modern numerical tools for
stochastic control. To this end, we provide a detailed Python Jupyter notebook that allows a fully
reproducible checking of our results. Our notebook is made as simple as possible, in order to distill
where the statistics tools come in, and aiming to remove much of the “ML mystique” that can
be sometimes present. Our approach also emphasizes the questions of how to train the statistical
surrogate, and how to approximate the optimal control, both important implementation aspects
whose discussion is often skipped.

In terms of the ﬁnancial application, we contribute to the optimal execution literature in several
ways that would be of independent interest to experts in that domain. Speciﬁcally, our optimal
execution modeling framework builds on the discrete-time, linear transient price impact model with
exponential decay from Obizhaeva and Wang [47] and additionally allows for (i) nonlinear, power-
type price impact `a la Bouchaud et al. [21, 22], Gatheral [30], Alfonsi et al. [6], Dang [26], Curato
et al. [25]; (ii) a stochastic transient price impact driven by its own noise, akin to the continuous-
time model in Becherer et al. [18]; and (iii) a risk-aversion-type running quadratic penalty on the
inventory as arising, e.g., in Schied et al. [51]. In other words, our model nests various existing
proposals in the literature, oﬀering a uniﬁed discrete-time framework that we investigate in detail.
Other related work on optimal order execution with (exponentially) decaying (linear) transient
price impact in discrete and continuous time include, e.g., Alfonsi et al. [5], Alfonsi and Schied [8],
Predoiu et al. [49], Alfonsi et al. [9], Gatheral et al. [31], Lorenz and Schied [45], Alfonsi and Schied
[7], Alfonsi and Acevedo [3], Bank and Fruth [17], Alfonsi and Blanc [4], Fruth et al. [28, 29], Graewe
and Horst [33], Lehalle and Neuman [43], Horst and Xia [37], Chen et al. [24], Ackermann et al. [1,
2], Forde et al. [27], Neuman and Voß [46]. Moreover, in the linear case, we also establish a new
explicit formula, see Proposition 1, for the optimal execution strategy with stochastic transient price
impact and inventory penalty, which extends the explicit deterministic solution from Obizhaeva and
Wang [47] and allows us to also accurately benchmark our machine learning approach. Therefore,
our numerical experiments provide new reliable insights on the interaction between diﬀerent model
parameters and the optimal strategy. Obtaining these insights, in other words building a better
intuition on how the model behaves in diﬀerent regimes, was the original motivation for our work,
and is valuable for practitioners who must develop gut feelings on how the model reacts as the real
world (i.e., calibrated parameters) changes. In particular, our numerical analysis complements the
studies on deterministic optimal execution strategies with non-linear transient price impact carried
out in Dang [26] and Curato et al. [25].

In the broader context of machine learning methods for stochastic optimal control, our work is
related to other applications of neural networks to ﬁnancial problems, see Bachouch et al. [14], Hur´e
et al. [39], and Ismail and Pham [40]. Perhaps the closest is Leal et al. [42] who also study execution

2

problems, but in a model with only temporary and permanent price impact `a la Bertsimas and
Lo [19], Almgren and Chriss [10], Cartea and Jaimungal [23]. For other approaches in optimal
execution in the presence of temporary price impact more in the ﬂavor of reinforcement learning
see, e.g., the recent survey articles by Hambly et al. [35] and Jaimungal [41] and the references
therein.

This article is organized as follows. Section 2 formulates our generalized optimal execution set-
ting. Section 3 presents an explicit reference solution for unconstrained trading strategies. Section
4 describes our methodology for parametric stochastic control via statistical surrogates. Section 5
presents the numerical experiments and resulting insights. Following a brief conclusion in Section
6, Section 7 contains the proofs.

2 Problem formulation

Let (Ω, F , F = (Fn)n=0,...,N , P) be a discrete-time ﬁltered probability space with trivial σ-ﬁeld F0
and a terminal time horizon N ∈ N. We consider a ﬁnancial market with one risky asset whose
F-adapted real-valued unaﬀected fundamental price process is denoted by P = (Pn)n=0,...,N . We
set P0 := p0 ∈ R+.

Suppose a large trader dynamically trades in the risky security and incurs price impact in an
adverse manner. Speciﬁcally, for n ∈ {1, . . . , N }, by choosing her number of shares in the risky
asset at time n − 1, she trades un ∈ Fn−1 shares in the n-th trading period and confronts the n-th
fundamental random shock ∆Pn := Pn − Pn−1 ∈ Fn. Her action permanently aﬀects the future
evolution of the mid-price process M = (Mn)n=0,...,N which becomes

Mn := Pn + γ

n
(cid:88)

j=1

uj, n = 1, . . . , N,

(1)

after the n-th order un is executed (we set M0 := P0). The parameter γ ≥ 0 represents linear
permanent price impact.

In addition, the trader’s market orders are ﬁlled at a deviation Dn from the mid-price Mn in (1).
The post-execution dynamics of these deviations D = (Dn)n=1,...,N from the mid-price after trading
un shares at time n ∈ {1, . . . , N } are modeled as

D0 := d0,
Dn := (1 − κ)Dn−1 + η|un|α sgn(un) + (cid:15)n, n = 1, . . . , N,

(2)

with d0 ∈ R denoting the given initial deviation. Thus, in the absence of the large trader’s actions,
the deviation tends to revert exponentially to zero at rate κ ∈ (0, 1], with the latter parameter
known as the book resilience. The resilience captures the transience of the instantaneous price
impact, with κ representing the fraction by which the deviation from the mid-price M incurred by
past trades diminishes over a trading period. Empirically, the deviation process D can be calibrated
by considering the limit order book (LOB) spread and depth.

Due to ﬁnite market depth, which is measured by 1/η > 0, the trader’s turnover of un shares
pushes the deviation in the trade’s direction by a constant factor η times the instantaneous price
impact, which is assumed to be |un|α sgn(un), α > 0. Following, e.g., Bouchaud et al. [21, 22],
the latter power-type term generalizes the common linear situation α = 1 from Obizhaeva and
Wang [47] where the instantaneous price impact is ηun. However, empirically the instantaneous
price impact is observed to be concave (at least for relatively small un), so that α < 1 seems more
realistic; see Lillo et al. [44], Bouchaud et al. [21], Bacry et al. [15].

3

n
(cid:88)

The R-valued, F-adapted sequence of zero-mean random variables ((cid:15)n)n=1,...,N represents addi-
tional small perturbations in the deviation stemming, e.g., from market and limit orders, which are
placed at time n by other small market participants, making the transient price impact captured
by the deviation process D stochastic; cf., e.g., Becherer et al. [18].

For the rest of the paper, we assume that (∆Pn)n=1,...,N are independent and square integrable
random variables with mean zero. The perturbations ((cid:15)n)n=1,...,N in (2) are i.i.d. normally dis-
tributed random variables with mean zero and variance σ2 > 0, and independent of (∆Pn)n=1,...,N
as well. We also refer to the case σ = 0 where (cid:15)n ≡ 0 for every n ∈ {1, . . . , N }. Finally, we let the
ﬁltration F be given by Fn = σ({∆P1, (cid:15)1, . . . , ∆Pn, (cid:15)n}) for all n ∈ {1, . . . , N }.

2.1 Optimal Trade Execution

From now on, we suppose that the large trader wants to carry out a buying program to buy X0 > 0
shares by executing N market buy orders un ≥ 0, n = 1, . . . , N . She starts with zero inventory and
we use Xn to denote the remaining number of shares she needs to buy after step n to reach her
target. That is, we set

Xn := X0 −

uj,

n = 1, . . . , N,

(3)

j=1

where Xn represents her remaining order to be ﬁlled after she executed her n-th trade un.

As common in the literature, in order to describe the evolution of the large trader’s cash balance,
we assume that the n-th transaction un aﬀects the mid-price M and deviation D gradually. More
precisely, half of the n-th order is ﬁlled at the pre-transaction’s mid-price Mn−1, as well as the
refreshed pre-transaction’s deviation (1 − κ)Dn−1, whereas the other half is executed at the less
favorable post-transaction quantities Mn − ∆Pn (before the n-th fundamental random shock hits
the stock price) and Dn. Hence, assuming zero interest rates, the self-ﬁnancing condition dictates
that changes in the trader’s cash balance (Cn)n=1,...,N with initial value c0 ∈ R are only due to her
buying activity of the risky asset which is executed at the previously described average execution
prices:

C0 := c0,

Cn := Cn−1 −

(cid:18) Mn−1 + (1 − κ)Dn−1 + Mn − ∆Pn + Dn
2

(cid:19)

un

(cid:16)

Pn−1 −

γ
2

= Cn−1 −
(cid:18)

−

(1 − κ)Dn−1 +

(Xn + Xn−1) + γX0

(cid:17)

un

η
2

uα
n +

1
2

(cid:19)

(cid:15)n

un,

n = 1, . . . , N.

(4)

Lemma 1. The terminal cash position CN at time N of a buying schedule (un)n=1,...,N with un ≥ 0
for all n ∈ {1, . . . , N } and terminal state constraint XN = 0 is given by

CN = c0 −

(cid:16)

p0 +

(cid:17)

X0

γ
2

X0 −

N −1
(cid:88)

n=1

Xn∆Pn −

N
(cid:88)

n=1

(cid:18)

(1 − κ)Dn−1 +

η
2

uα
n +

1
2

(cid:19)

(cid:15)n

un.

(5)

4

n=1
(cid:18)

N
(cid:88)

−

n=1

N −1
(cid:88)

= c0 −

n=1
(cid:18)

−

N
(cid:88)

n=1

Proof. Using un = Xn−1 − Xn we obtain from (4) together with (2) and (1)

CN = C0 −

N
(cid:88)

Pn−1un +

γ
2

N
(cid:88)

(X 2

n−1 − X 2

n) − γX0

n=1

N
(cid:88)

n=1

un

(1 − κ)Dn−1 +

η
2

uα
n +

(cid:19)

(cid:15)n

un

1
2

Xn∆Pn − p0X0 − γX0(X0 − XN ) +

γ
2

(X 2

0 − X 2
N )

(1 − κ)Dn−1 +

η
2

uα
n +

1
2

(cid:19)

(cid:15)n

un.

(6)

Under the terminal state constraint XN = 0, the representation in (6) simpliﬁes to (5)

Next, in order to introduce the trader’s optimization problem let us denote for all time steps

n = 1, . . . , N the collection of admissible buying strategies by

An :=






(uj)j=n,...,N : uj ∈ L2(Fj−1, P), uj ≥ 0 a.s. for all j = n, . . . , N,

N
(cid:88)

j=n

uj = Xn−1






.

(7)

The trader aims to maximize her expected terminal cash position given in (5) while also controlling
for inventory risk. The latter is modeled through a quadratic urgency penalty νX 2
n for an urgency
parameter ν ≥ 0 on her outstanding order. Combining the two terms, the objective is to minimize

inf
(un)n=1,...,N ∈A1

(cid:34) N
(cid:88)

E

n=1

(cid:110)(cid:16)

(1 − κ)Dn−1 +

(cid:17)

uα
n

η
2

(cid:35)
un + ν(Xn−1 − un)2(cid:111)

.

(8)

Note that Xn−1−un = Xn is the remaining inventory; our notation emphasizes the role of Xn−1 and
Dn−1 as the state variables, and un as the control. As it is well-known in the literature, we remark
that the permanent impact γ disappears in (8) and from our further discussion since it only adds a
ﬁxed oﬀset −0.5γX 2
0 to the terminal cash position, irrespective of the trading strategy. Similarly,
the martingale terms (cid:80)
n (cid:15)nun in (5) disappear as well after taking expectations
thanks to the independence of price increments and perturbations in the deviation process.

n Xn∆Pn and (cid:80)

We introduce for all n ∈ {1, . . . , N } the value function as

Vn(x, d) :=

inf
(uj )j=n,...,N ∈An

(cid:34) N
(cid:88)

E

(cid:26) (cid:16)

j=n

(1 − κ)Dj−1 +

(cid:17)

uα
j

uj

η
2

+ ν(Xj−1 − uj)2

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(9)

(cid:35)

Xn−1 = x, Dn−1 = d

.

To characterize Vn, we use the corresponding dynamic programming (DP) equation. Since at
the last period N , the admissible set AN is a singleton uN ≡ XN −1, we have the terminal condition

VN (x, d) = (1 − κ) · d · x +

η
2

xα+1,

(10)

5

and then for n = N − 1, . . . , 1

(cid:34)

Vn(x, d) = min
u∈[0,x]

E

(1 − κ) · d · u +

η
2

uα+1 + ν(x − u)2

+ Vn+1(x − u, Dn)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Xn−1 = x, Dn−1 = d

(11)

(cid:35)

with Dn = (1 − κ)d + ηuα + (cid:15)n as postulated in (2) and the expectation being with respect to the
Gaussian noise (cid:15)n ∼ N (0, σ2).

The DP equation (11) has two primary state variables (Xn−1, Dn−1). Below, we will also
consider its dependence on the static parameters κ, η, α, ν, σ. Recall that κ ∈ (0, 1] is the book
resilience (smaller κ increases the transient price impact); η > 0 is the instantaneous price impact
(larger η makes trades aﬀect Dn more); ν ≥ 0 is the urgency parameter (larger ν encourages larger
buys to mitigate inventory risk); α ≈ 1 is the exponent of the instantaneous price impact function
(α ≷ 1 leads to convex (resp. concave) price impact) and σ > 0 is the standard deviation of the
one-step-ahead deviation Dn. Note that while κ and α are dimensionless, the value of ν should be
thought of relative to the initial inventory X0, and the values of η, σ should be picked relative to
ﬂuctuations in Dn. For example, if X0 = 105 (buying program of a hundred thousand shares) then
ν should be on the order of 10−4, η should be on the order of 10−3 (so that Dn is on the order of
10-100), and σ should be on the order of 1.

Remark 1 (Unconstrained Problem). In the above formulated optimal trade execution problem
it is tempting to a priori allow for trading in both directions (buy orders un > 0 and sell orders
un < 0) as in the discrete-time linear transient price impact model in Ackermann et al. [2]; i.e., to
compute

V ◦
n (x, d) :=

inf
(uj )j=n,...,N ∈A ◦
n

(cid:34) N
(cid:88)

E

(cid:26) (cid:16)

j=n

(1 − κ)Dj−1 +

|uj|α sgn(uj)

(cid:17)

uj

η
2

over the set of unconstrained order schedules

+ ν(Xj−1 − uj)2

(cid:27) (cid:12)
(cid:12)
(cid:12)
(cid:12)

(12)

(cid:35)

Xn−1 = x, Dn−1 = d

A ◦

n :=






(uj)j=n,...,N : uj ∈ L2(Fj−1, P), R-valued for all j = n, . . . , N,

N
(cid:88)

j=n

uj = Xn−1






.

(13)

However, as it will become apparent in Section 3, the exogenous noise ((cid:15)n)n=1,...,N in the deviation
process D in (2) would then trigger price manipulation in the sense of Huberman and Stanzl [38].
That is, there would exist proﬁtable round-trip trades, i.e., nonzero strategies (un)n=1,...,N ∈ A ◦
1
that generate strictly negative expected costs with X0 = 0 = XN by exploiting a nonzero deviation
Dn for some n ∈ {0, 1, . . . , N }. This can be ruled out by either introducing a bid-ask spread or
conﬁning trading in one direction only; see also the discussion on price manipulation in [2, 28, 29]
for the linear case α = 1, as well as [8, 25, 30] for the nonlinear case α (cid:54)= 1.

3 Explicit Solution for the Unconstrained Linear Case

The unconstrained optimal trade execution problem in (12) and (13) can be solved explicitly in the
case of linear transient price impact (α = 1) following a similar computation as done by Obizhaeva

6

and Wang [47]. They derived a solution in the deterministic case (σ = 0) without inventory penalty
(ν = 0) and with initial deviation d0 = 0.

Proposition 1. Let α = 1 and let σ ≥ 0. Deﬁne

aN :=

η
2

, bN := 1 − κ, cN := 0

(14)

and, recursively, for all n = N − 1, . . . , 1, set






an := ν + an+1 −

(2ν + 2an+1 − ηbn+1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

,

bn := (1 − κ)bn+1 +

2(1 − κ) (2ν + 2an+1 − ηbn+1) (1 − bn+1 + 2ηcn+1)
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

,

(15)

cn := (1 − κ)2cn+1 −

(1 − κ)2(1 − bn+1 + 2ηcn+1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

.

Then the value function in (12) is quadratic in its arguments and given by

n (x, d) = anx2 + bnxd + cnd2 + σ2
V ◦

N
(cid:88)

j=n+1

cj

(n = 1, . . . , N ).

(16)

Moreover, the optimal order execution strategy (u◦

n)n=1,...,N ∈ A ◦

1 is given by

u◦
n =

(2ν + 2an+1 − ηbn+1)X ◦

n−1 − (1 − bn+1 + 2ηcn+1)(1 − κ)D◦

n−1

η + 2ν + 2an+1 − 2ηbn+1 + 2η2cn+1

(n = 1, . . . , N − 1),

(17)

N = X ◦
u◦

N −1,
n−1 = X0 − (cid:80)n−1

j=1 u◦

j and

where X ◦

D◦

n−1 = (1 − κ)n−1d0 + η

for all n = 1, . . . , N + 1.

n−1
(cid:88)

(1 − κ)(n−1)−ju◦

j +

n−1
(cid:88)

(1 − κ)(n−1)−j(cid:15)j

(18)

j=1

j=1

Remark 2. In the deterministic case σ = 0 and setting ν = 0 as well as d0 = 0, the solution
in Proposition 1 coincides with the deterministic solution presented in Obizhaeva and Wang [47,
Proposition 1].

Observe that for every n ∈ {1, . . . , N } the optimal execution policy u◦
n in (17) is given as a de-
terministic linear feedback function of the controlled random state variables X ◦
n−1. Also,
the coeﬃcients of this linear function do not depend on σ. Therefore, in the deterministic version
of the problem where σ = 0, the optimal policy is given by the exact same feedback law in (17).
Moreover, since the state variables X ◦
j )j=1,...,n−1 and the
i.i.d. zero-mean Gaussian noise ((cid:15)j)j=1,...,n−1 , it follows that the optimal trades (u◦
n)n=1,...,N of the
stochastic version of the problem with σ > 0 are in fact just a Gaussian-distributed perturbation
from the corresponding deterministic solution with mean given by the latter.

n−1 are themselves linear in (u◦

n−1 and D◦

n−1 and D◦

Corollary 1. On average, the optimal order executions (u◦
the deterministic problem with σ = 0. Moreover, viewed as a random variable, u◦
distribution with the above mean.

n)n=1,...,N in (17) coincide with u◦

n for
n has a Gaussian

7

Remark 3 (Proﬁtable Round-Trip Strategies). Let N ≥ 2. By virtue of Lemma 2 below, we obtain
that the coeﬃcients (cn)n=1,...,N −1 in (15) are all strictly negative for κ < 1. As a consequence, if
σ > 0 and taking zero initial buying volume outstanding (X0 = 0), we have V ◦
n (0, 0) < 0
in (16) for n < N . In other words, the resulting optimal strategy in (17) is a proﬁtable round-trip
strategy in the sense of Huberman and Stanzl [38]. This happens because the feedback policy readily
exploits a nonzero deviation and its resilience as it arises; see also the detailed discussion in [2,
28, 29]. Similarly, in line with the latter references, observe that there are no proﬁtable round-trip
strategies (nor transaction triggered price manipulation strategies in the sense of Alfonsi et al. [9])
in the deterministic resilience case σ = 0, as long as the initial deviation satisﬁes d0 = 0 (because
0 (0, 0) = 0 and u◦
V ◦

n ≡ 0 for all n = 1, . . . , N ).

n (0, d) < V ◦

3.1 Deterministic Resilience Case σ = 0

In the deterministic case σ = 0, the solution presented in Proposition 1 can be rewritten in an
explicit closed-form without backward recursion in (15) and forward feedback policy in (17). In view
of Corollary 1, this is useful for revealing the dependence of the average optimal order executions
1, . . . , u◦
u◦
Proposition 2. Let σ = 0. Moreover, let a, b, c, ax, bx, cx, ad, bd, cd ∈ RN denote the vectors deﬁned
in (43), (45), (47) below. Set

N on the model parameters κ ∈ (0, 1], η > 0 and ν ≥ 0.

ˆb := −

ˆc := −

a(cid:62)(ηb + (1 − κ)bd) + (1 − κ)(ad)(cid:62)b + 2ν(ax)(cid:62)bx
ηa(cid:62)a + 2(1 − κ)(ad)(cid:62)a + 2ν(ax)(cid:62)ax
a(cid:62)(ηc + (1 − κ)cd) + (1 − κ)(ad)(cid:62)c + 2ν(ax)(cid:62)cx
ηa(cid:62)a + 2(1 − κ)(ad)(cid:62)a + 2ν(ax)(cid:62)ax

,

.

Then the optimal order execution strategy from Proposition 1 in (17) is given by

1 = ˆb · d0 + ˆc · X0
u◦

and

1 + bn · d0 + cn · X0
Moreover, the optimally controlled deviation process and remaining inventory are given by

(n = 2, . . . , N ).

n = an · u◦
u◦

(19)

(20)

(21)

X ◦
D◦

n = ax
n = ad

n+1 · u◦
n+1 · u◦

1 + bx
1 + bd

n+1 · d0 + cx
n+1 · d0 + cd

n+1 · X0,
n+1 · X0

(n = 0, . . . , N − 1).

(22)

Observe that in Proposition 2 the deterministic optimal strategy u◦

N in (21) is fully
characterized by the ﬁrst trade u◦
1 in (20) and the size of the orders vary over time n ∈ {1, . . . , N }.
A further simpliﬁcation is obtained when there is no urgency/inventory penalty, i.e., ν = 0. Specif-
ically, all intermediate trades u◦
N −1 are ﬂat and determined as a κ-fraction of the ﬁrst order
u◦
1, shifted by a proportion of d0.

2, . . . , u◦

1, . . . , u◦

Corollary 2. Set ν = 0 in Proposition 2. Then in (19) it holds that

ˆb = −

ηb(N − 2)(cid:0)1 + κ(N − 1)(cid:1) + κ(1 − κ)
κη(N − 1)(2 + (N − 2)κ)

and

ˆc =

1
2 + (N − 2)κ

,

(23)

and the optimal intermediate trades in (21) with initial trade u◦
to

1 in (20) are constant and simplify

n = κ · u◦
u◦

1 + ˜b · d0

(n = 2, . . . , N − 1)

(24)

8

with ˜b deﬁned in (41) below. The ﬁnal trade is given by u◦
The optimally controlled deviation process and remaining inventory in (22) simplify to

N = X0 − (1 + (N − 2)κ) · u◦

1 − (N − 2)˜b · d0.

X ◦
D◦

n = X0 − (1 + (n − 1)κ) · u◦
n = η · u◦

1 + (1 − κ) · d0

1 − (n − 1)˜b · d0,

(n = 1, . . . , N − 1).

(25)

In particular, if d0 = 0 it holds that

1 = u◦
u◦

N =

X0
2 + (N − 2)κ

and u◦

2 = . . . = u◦

N −1 = κu◦
1.

(26)

Corollary 2 shows that in the deterministic case without urgency penalty ν = 0 the optimal
execution strategy in (24) is constant for the intermediate trades from 2 to N − 1 and keeps the
deviation process in (25) ﬂat until N − 1. If, in addition, d0 = 0 (i.e., the setup in Obizhaeva and
Wang [47]) the optimal strategy simpliﬁes to the symmetric U-shaped (26) and becomes indepen-
dent of the instantaneous price impact parameter η (as observed in [47]). Initial and last trades
are the same and all remaining intermediate trades are simply prescribed as a constant κ-fraction
of the initial trade. In particular, for full resilience κ = 1 all trades are just equal to X0/N .

Remark 4. The simple formula in (26) has also been derived in Alfonsi et al. [6, Corollary 6.1].

3.2 Optimal Execution Proﬁles

Figure 1 illustrates the behaviour of an optimal buying program from Proposition 2 with linear
transient price impact (α = 1) for diﬀerent values of the model parameters κ, η. We consider buying
X0 = 100, 000 shares in N = 10 trades, with initial deviation d0 = 0. As discussed, when ν = 0
we obtain the well-known U-shape for the buying schedule, driven only by κ. With positive ν > 0
several novel qualitative eﬀects appear: (i) the optimal strategy now depends on both resilience rate
κ and temporary price impact η; (ii) intermediate trades are in general not ﬂat anymore; and (iii)
the overall pattern may shift from a U-shaped to a monotonically decreasing sequence of trades.
The presence of an inventory penalty ν > 0 creates an incentive to sell faster in the beginning,
which becomes more pronounced for small η. Consequently, the case of ν > 0 and small η might
lead to a decreasing execution schedule, cf. the purple curve in Figure 1.

Figure 1: Benchmark unconstrained buying programs (u◦
n) based on Proposition 2 for the case
σ = 0. We consider executing X0 = 105 shares in N = 10 steps for diﬀerent values of the model
parameters κ, η and ν = 0.00005, d0 = 0.

9

12345678910Period n01000020000300004000050000Order Size un=0.4,=1/3000=0.8,=1/3000=0.4,=1/500=0.8,=1/500n. For example, the initial trade u◦

Observe that relatively small changes in model parameters generate signiﬁcant impact on the
optimal strategy u◦
1 can be over 55% of total X0 when κ is small
and η is small (which causes the emphasis to be on inventory risk), and less than 25% of X0 for κ
large and η large (where strong resilience and strong temporary impact encourage to trade nearly
equal amounts at each step). Also note the non-monotone behavior of the strategies, where the
curves n (cid:55)→ u◦

n cross each other at diﬀerent steps.

In the right panel of Figure 2 we illustrate how the pathwise strategies u◦

n for σ > 0 from
Proposition 1 are normally distributed and coincide on average with the deterministic solution,
cf. Corollary 1. In the plot the blue bars correspond to the optimal deterministic solution from
Proposition 2 and Corollary 2; and the boxplots show the distribution of (u◦
n)n=1,...,10 from Propo-
sition 1. We take σ = 2 and average over 10,000 paths, considering both the classical case with
ν = 0 (left panel with U-shaped strategy) and our extension to positive inventory penalty ν > 0
(right panel). In line with Corollary 1, the empirical means match. Since this feature is true for
any conﬁguration of (κ, η, ν) the above qualitative eﬀects can be deduced for the stochastic optimal
feedback controls in (17).

κ = 0.8, η = 1/500, ν = 0

κ = 0.4, η = 1/1000, ν = 0.00005

Figure 2: Distribution of (u◦
n)n=1,...,10 from Proposition 1 over 10,000 paths for σ = 2 (boxplots)
together with the empirical mean (in red) compared to the formula from Proposition 2 for σ = 0
(barplot).

4 Numerical Implementation

In the case of nonlinear transient price impact α (cid:54)= 1, no closed-form solution is possible and
numerical methods are needed to compute an optimal solution (u∗
n)n=1,...,N for the minimization
problem in (8). A numeric algorithm is also needed for α = 1 to handle the constrained buy-only
setting un ≥ 0. Finally, the algorithm will be useful to study the dependence of Vn, u∗
n on model
parameters. Our algorithm to compute the value function and the optimal strategy relies on the
Bellman equation (11) which provides a recursive characterization for Vn and therefore for u∗
n.
Given the parametric setup, we consider a generic state y that fuses stochastic states (x, d) and
some (or none) of the aforementioned model parameters, and for the rest of this section treat Vn
and u∗

n as a function of y.

In order to solve (11) we need to be able to evaluate its right-hand-side. This entails (i)
evaluating Vn+1(·); (ii) evaluating the conditional expectation; (iii) taking the inf over u. None of
those steps are possible to do analytically and numerical techniques are necessary.

Below, we propose and implement a direct approach based on constructing a functional ap-
proximator, also known as a surrogate, ˆV , to the value function that is trained via an empirical

10

12345678910Time step n02000400060008000100001200014000Order size unEmpirical mean w/ =2Deterministic solution12345678910Time step n05000100001500020000250003000035000Order size unEmpirical mean w/ =2Deterministic solutionregression. Speciﬁcally, we consider the use of (feed-forward) Neural Networks (NN) for the latter.
Note that our method is based on the DP equation; we do not make any reference to Hamilton-
Jacobi-Bellman equations that are used in continuous-time setups, and which admit their own suites
of NN approaches. Neither do we consider reinforcement learning (RL) that dispenses with the
divide-and-conquer paradigm underlying the Bellman equation and the value function, and aims
to maximize total trading revenue on the entire horizon. Namely, RL solves for all Vn’s in parallel
over n = 1, 2, . . . , N while we maintain the sequential backward learning of VN −1, VN −2, . . . , V1.

Instead, our approach is conceptually faithful to the classical DP philosophy and oﬀers the

following advantages:

• It is straightforward to understand and implement, in some sense oﬀering the most immediate
approach to employing surrogate and other machine learning techniques for DP. As such, we
bypass the more subtle ideas that have been advanced, oﬀering a pedagogic-ﬂavored setup;

• It provides modularization, emphasizing that NN solvers are just one type of many potential
surrogates. Thus, it de-mystiﬁes deep learning, simply treating it as a choice among many.
Indeed, our implementation requires just a few lines of code to substitute a diﬀerent surrogate
type.

4.1 Building a Surrogate

Denote by X ⊆ Rd the state space of the surrogate, which includes the bona ﬁde inputs X, D,
as well as all the relevant model parameters among κ, η, α, ν which we wish to capture.
In the
examples below we consider d ∈ {2, 3, 4, 5}, with the main illustrative example being d = 4 where
we take y = (X, D, κ, η), i.e. we simultaneously learn the value function and the strategy as a
function of (X, D), the resilience κ and the instantaneous price impact η for some ﬁxed α and ν.

We denote by G(y, u, (cid:15)) the one-step transition function of y given external noise (cid:15) and action u.

For example,

G((X, D, κ, η), u, (cid:15)) = (X − u, (1 − κ)D + ηuα + (cid:15), κ, η).

Our resolution of the DP equation (11) operates as-is with each of the underlying sub-steps.
This means that considering a generic intermediate step n of the backward recursion and given a
surrogate ˆVn+1 we ﬁrst substitute it in place of Vn+1. Next, the expectation in (11) is over the
stochastic shocks (cid:15)n which are one-dimensional Gaussian random variables. We employ Gaussian
quadrature to replace the respective integral with a ﬁnite sum. Speciﬁcally, we rely on the optimal
quantization of Bally et al. [16] and Pag`es et al. [48] to select j = 1, . . . , N (cid:48) weights wj and respective
knots ej to approximate

E

(cid:104) ˆVn+1(G(y, u, (cid:15)n)) (cid:12)

(cid:12) Yn = y

(cid:105)

(cid:39)

N (cid:48)
(cid:88)

j=1

wj ˆVn+1

(cid:0)G(y, u, ej)(cid:1) .

(27)

Remark 5. One can straightforwardly consider non-Gaussian (e.g. heavy tailed) noise distributions
for (cid:15)n thereby providing a diﬀerent nonlinear generalization. Non-Gaussian (cid:15)n just reduces to taking
a diﬀerent set of ej, wj’s.

The optimization over the number of shares to buy un is done via a numerical optimizer, namely
the standard gradient-free optimization routine (such as L-BFGS) that is available in any software
package. Note that un is scalar, allowing the use of fast one-dimensional root ﬁnding algorithms.
In our implementation, we do not evaluate any gradients of ˆVn+1 although that is feasible. In order

11

to restrict to u ∈ [0, Xn] and rule out any selling, we optimize on the above bounded interval,
straightforwardly supported by such solvers.

Finally, it remains to construct ˆVn. As a machine learning task, the goal is to learn the true
input-output map y (cid:55)→ Vn(y). Such functional approximation, aka surrogate construction [34],
is carried out by selecting a collection of training inputs y1:M ∈ X , evaluating (a noisy version
and then ﬁtting a statistical representation ˆVn(·) that can interpolate (or
of) Vn(y1:M ) =: v1:M
extrapolate) to new, out-of-sample y’s. The evaluation of vm
n is achieved by direct computation via
the nonlinear optimizer and the quantized integral on the right-hand-side of (11).
The overall algorithm is recursive backward in time, and summarized by:
• Set ˆVN (y) ≡ VN (y) = (cid:0)(1 − κ)d + η

2 (X)α(cid:1) X (no approximation needed at terminal time)

n

• For n = N − 1, N − 2, . . . , 1

– Select the experimental design y1:M
n−1
n := inf u{(cid:80)N (cid:48)

– Evaluate vm

j=1 wj ˆVn+1(G(ym

u)2} for m = 1, . . . , M
– Use the dataset (y1:M

n−1, v1:M

n

) to ﬁt the surrogate ˆVn(·) : X → R

n−1, u, ej)) + (1 − κm)dmu + ηm

2 uα+1 + ν(X m

n−1 −

• EndFor

4.2 Policy Approximation

n(y).

The primary output of the numerical solver is the execution strategy, given in feedback form as
y (cid:55)→ u∗
Indeed, the strategy is what the controller is ultimately after, and yields a clear
interpretation of how many shares the solver recommends to buy next. In contrast, the approximate
value function ˆV is harder to interpret (since it is not in pure monetary dollars but potentially also
involves the abstract inventory costs) and moreover due to the intermediate approximations does
not have any concrete probabilistic representation. Indeed, while the true V is the expected cost
of the strategy, ˆV is not an expectation on [0, T ], since it is obtained from one-step recursions.

Conventionally, u∗ is characterized as the arg min of the Bellman recursion (11), so that to
n(y) one must re-do the optimization over ˆVn. This is time-consuming, ineﬃcient and
obtain u∗
non-transparent to the user. Instead we seek a direct representation for u∗
n and in the spirit of
the machine learning mindset (speciﬁcally actor-critic frameworks) propose to construct a second
surrogate, auxiliary to the one describing ˆV . Accordingly, we construct a separate surrogate ˆun(·)
that is trained based on the recorded um
n−1. Note that
the ﬁtting of ˆun is independent of the main loop above, so can be done in parallel with ˆVn or
after-the-fact.

n , the optimal execution amounts for each ym

Even when the training inputs um

this would be true for the ﬁtted prediction ˆu(ym
based on the fractions um
all predictions (again interpreted as fractions of current inventory to be sold) into (0, 1).

n−1], it is not generally guaranteed that
n−1). In order to enforce this constraint, we train ˆu
n−1 ∈ [0, 1] and use a sigmoid transformation to intrinsically restrict

n are in the range [0, X m

n /X m

Remark 6. In our setup, we ﬁrst pointwise approximate un(ym
n−1) and then ﬁt a statistical surrogate
to those samples; in Bachouch et al. [14] and Hur´e et al. [39] the strategy is the opposite: ﬁrst
parametrize potential u(·; θn) through a neural network with weights θn; then use back-propagation
to optimize the respective hyper-parameters θn. In that sense, their resulting y (cid:55)→ u(y; θn) is not a
solution of any optimization problem, and there is no underlying dataset (y1:M
) like for our
approach.

n−1, u1:M

n

12

4.3 Neural Network Solvers

A popular class of surrogates consists of feed-forward neural networks (NN). NN’s allow eﬃcient
ﬁtting of high-dimensional parametric surrogates using back-propagation and a variety of stochastic
optimization techniques, such as stochastic gradient descent.

Training a NN requires specifying the training inputs y1:M

A NN represents ˆVn as a composition, using linear hidden units at each layer, a user-chosen
activation function across layers, and a user-selected number of layers. In our context, the pre-
cise architecture of the feed-forward NN is not conceptually important. The NN parameters are
optimized via batch stochastic gradient descent. Relative to other statistical models, NN is over-
parametrized with thousands of parameters, known as the weights. Nevertheless NN is known to
enjoy excellent empirical convergence (i.e. the algorithms ﬁnd near-optimal weights), especially for
large scale datasets, including in other stochastic control applications [14, 36, 39, 40].
, κ1:M
n

) ∈ X
and initializing the NN weights. For the former, we propose a space-ﬁlling experimental design,
matching the standard approach in statistics. As default, we select a hyper-rectangular training
domain ¯X and sample each coordinate of y uniformly and independently on the respective training
interval, for example we sample the inventory Xn ∈ [0, X0]. One may also implement joint sampling
in ¯X , such as Latin Hypercube Sampling (LHS). A further option is to use low-discrepancy Quasi
Monte Carlo (QMC) sequences to achieve a space ﬁlling training set of arbitrary size M . Compared
to i.i.d. Uniform sampling, LHS and QMC oﬀer lower variance and better coverage, avoiding any
clusters or gaps in the training locations, which is relevant when M is relatively small compared to
the dimension of y. Note that training sets are indexed by n; we sample fresh y1:M
’s at each step,
so that the training inputs vary across n’s, although they have the same size and shape.

n ≡ (x1:M

, η1:M
n

, d1:M
n

n

n

To initialize the weights, we found it very beneﬁcial to rescale the inputs to the unit hypercube,
which permits the use of standard NN weight priors (namely Truncated Normal with mean zero).
Similarly, for ˆV we re-scale the training outputs vm
n to be in the range [0, 1] too. For training
the policy surrogate ˆu we apply a sigmoid activation function on the output layer of the NN that
directly ensures that ˆu(y) ∈ (0, 1). The latter fraction is multiplied by the current inventory X to
get the number of shares to trade.

Our implementation (see the supplementary Jupyter Notebook) employs the TensorFlow library
in Python, which provides one of the most popular engines for NNs. We employ “factory defaults”
to construct our neural networks using the tensorflow.keras.Sequential architecture. This is
a linear stack of layers; we use the same number of neurons per layer and the same activation
function across layers. For the experiments below we utilize 3 layers and 16 neurons, with the ELU
activation function, ELU (x) = x1{x>0} + (ex − 1)1{x≤0}. Training uses the Adam algorithm with
default learning rate, batch size of 64 and E epochs. The latter represents a single pass through
all training data, and many epochs are needed given the high noise in the underlying stochastic
gradient descent optimizer of the NN weights. With the above choices, training a NN takes just a
few lines in TensorFlow. Indeed, it takes more code to scale/re-scale the inputs and outputs than
to actually build and ﬁt the Neural Net. Other neural network libraries, such as scikit-learn or
PyTorch could be straightforwardly substituted.
We end this section with a few ﬁnal remarks:

• It is completely straightforward to modify the NN architecture. For instance, to replace a
3-layer “deep” architecture with a single layer, it takes just commenting out 2 lines in our
code. Similarly, one can add more layers with a single line change.

• Because we maintain the underlying Dynamic Programming paradigm, the NNs are ﬁtted
one-by-one. Therefore, one has complete ﬂexibility in modifying any aspect of the ﬁtting

13

procedure to make it step-dependent. This includes the size M and shape of the training
set y1:M
n−1; the parameters for the NN optimization, including the initial NN weights; the NN
architecture, such as the number of neurons; and even the surrogate type itself. For example,
¯Xn to reﬂect the
one could mimic RL techniques to select time-dependent training regions
natural time-dependency of the solution, such as the remaining order to ﬁll Xn decreasing
over time.

• The convergence of the stochastic gradient descent to ﬁnd a good ˆVn is quite slow. We ﬁnd
that E (cid:29) 1000 epochs are necessary to achieve good results. The number of epochs is the
primary determinant of ﬁt quality, cf. Section 5.3.

• Any surrogate can be re-trained/updated at any point of the overall backward loop. For
example, we suggest training ˆVn’s for all n’s and then training more (i.e. run the backward
loop again with same training samples or newly generated ones) as one way to improve
empirical convergence. This process is completely transparent and just requires loading the
existing NN objects corresponding to ˆVn’s rather than initializing new ones. Similarly, one
can straightforwardly implement warm starting, using the ﬁtted NN weights at step n + 1 as
an initial guess for the weights of ˆVn.

• A typical failure point for surrogates is unstable prediction when extrapolating beyond the
range of the training region. Thus, care must be taken to select the training domain ¯X in
order to minimize extrapolation. Usually the modeler knows a priori the test cases of interest
and so can ensure that the training range is at least as large. For example, below we wish to
test for κ ∈ {0.4, 0.6, 0.8} and therefore we train on κ1:M
n ∈ [0.38, 0.82] to mitigate any issues
with prediction at or beyond the edge of the training domain.

Remark 7. Other surrogate types could be employed in place of NNs. For example, Gaussian Pro-
cesses (GP) [50] is a kernel regression method where the kernel hyperparameters are ﬁt using maxi-
mum likelihood. They are implemented in, e.g. scikit-learn in Python. GPs oﬀer variable selec-
tion through automatic relevance determination, which allows to “turn oﬀ ” covariates/parameters
that make little impact on the response. GP surrogates are known for excellent performance on
limited datasets and are very popular for emulating expensive computer and stochastic experiments.
Another surrogate type are LASSO linear models that explicitly project ˆVn onto the span of the
basis functions {Br(y)}, ˆVn(y) = β0 + (cid:80)
r βrBr(y). The respective coeﬃcients are determined from
the (L1-penalized) least squares equations. Finally, we note that not all regression methods are
appropriate; non-smooth frameworks like Random Forests would yield unstable or discontinuous
estimates of ˆu(·) and therefore should not be applied.

4.4 Workﬂow

To summarize, the algorithmic workﬂow to solve the parametric optimal execution problem is as
follows:

1. Build the training designs y1:M

n

.

2. Set up the neural net surrogates ˆVn and ˆun.
3. Implement the backward dynamic programming, saving the regression objects that deﬁne ˆVn;

4. In parallel with above, implement a separate functional approximator ˆun(·) using training
n is the pointwise recorded optimal control corresponding to input

), where um

n−1, u1:M

n

data (y1:M
ym
n−1;

14

5. Evaluate the performance of the strategy by generating M (cid:48) fresh out-of-sample forward paths
n−1 ) as the control at step n. Record the resulting costs ˇvm and the empirical

that utilize ˆun(y1:M (cid:48)
average execution cost

ˇV (0, Y0) =

1
M (cid:48)

M (cid:48)
(cid:88)

m=1

ˇvm.

(28)

We provide a TensorFlow implementation of the above in the supplementary fully-reproducible

Jupyter notebook.

5 Numerical Experiments

5.1 Comparison to Reference Model

With linear price impact α = 1, the unconstrained strategy (u◦
n) is available by parsing recursively
the formulas in Proposition 1. This yields a concrete benchmark to evaluate our numerical algo-
rithms. In this Section we compare a NN surrogate to the above ground truth. Even though u◦ is
not feasible (since it can and does turn negative in some states), this comparison is still useful. For
the chosen parameter conﬁgurations, u◦
n (cid:29) 0 (cf. Fig 2) so that the non-negativity constraint is not
binding on the vast proportion of the paths and hence u◦ is very nearly optimal for the constrained
problem as well; that is, u◦ ≈ u∗ with u∗ denoting the minimizer in (8). This ”near”-optimality is
conﬁrmed by the closeness between the constrained NN- and unconstrained LF-strategies, oﬀering
an additional consistency check on the NN solver.

n=0 , we ﬁx the set of “noise” (cid:15)1:M (cid:48)

To enable an apples-to-apples assessment of (ˆun)N −1

that feed
into the realized Dn’s, and employ this ﬁxed database of (cid:15)’s to generate the test forward trajectories
both for the NN approximator and for the exact solution. Since Dn depends on the past execution
amounts, the trajectories of the approximating strategy will diﬀer at each and every step n. Thus,
we are not making a pairwise comparison between ˆun(·) and u◦
n(·) from (17), but compare in terms
of the ﬁnal execution cost in (28). Indeed, due to stochastic ﬂuctuations, on any given path the
realized costs from the latter may be higher or lower than the costs of the benchmark strategy,
however the Law of Large Numbers guarantees that for M (cid:48) large, the average execution cost must
be at least as much as the benchmark.

n

For illustration purposes, we train a NN that takes in the four inputs (Xn, Dn, κ, η) and com-
putes the corresponding un. The respective 4-dimensional training domain is taken to be a hyper-
rectangle speciﬁed as Xn ∈ [0, 105], Dn ∈ [0, 100], κ ∈ [0.38, 0.82], η ∈ [1/900, 1/5000]. The re-
maining parameters are ﬁxed as ν = 0.00005, σ = 1. We then select M = 4000 training points
i.i.d. uniformly in the above domain, independently for each step n. Note that while we kept the
same training domain across steps, one can vary this as n changes. The ranges of the training
domain are ultimately driven by the desired test conﬁgurations. For example, below we show the
results for the test set with κ = 0.4, η = 1/1000 and initial condition X0 = 105, D0 = 0. The latter
aﬀect the range of Xn. Since Xn ≤ X0 and XN = 0, we train on the range [0, X0]. The range for
Dn depends on D0, X0 and η. With X0 = 105, we expect to buy up to 50-60K shares in the ﬁrst
step, which with η (cid:39) 1/1000 would lead to Dn ∈ [40, 80]. Note that Dn is highly sensitive to η,
hence the respective training range should reﬂect the bounds on η values. The range for the re-
silience κ covers the test case of κ = 0.4 and is simultaneously quite wide to cover a range of market
conditions (calibration of κ is known to be diﬃcult). The range for the instantaneous impact η is
similarly chosen to cover a range of market conditions, with η = 1/1000 leading to impact that is

15

5 times stronger than that of η = 1/5000. The quantization of the conditional expectation in (27)
uses N (cid:48) = 50 knots.

Figure 3: Left: histogram of the relative error in ﬁnal costs between the NN-strategy and the
benchmark strategy based on M (cid:48) = 104 forward trajectories. Right: optimal policy vs the NN-
strategy along the same path (ﬁxed set of (cid:15)n ’s). Parameters ﬁxed at κ = 0.4, η = 1/1000, ν =
0.00005 and σ = 1.

The left panel of Figure 3 shows the histogram of the diﬀerence between the realized execution
costs ˇvm coming from an NN approximator and the benchmark, across 104 test trajectories. We
observe that on average the former are 0.041% higher. Note that for about 11.6% of the paths, the
realized costs from our approximate strategy were less than from the benchmark. Conversely, on
more than 95.5% of the paths, the NN-based costs were less than 0.1% higher than those from the
reference strategy, which is practically a very good level of accuracy.

The right panel of Figure 3 compares the reference u◦(ym(cid:48)

1:N )
on one sample forward trajectory. We observe that the two strategies are very close in a pathwise
sense as well, conﬁrming the high approximation quality.

1:N ) and the NN-based control ˆu(ym(cid:48)

5.2 Nonlinear Price Impact

We now proceed to consider strategies with nonlinear price impact α (cid:54)= 1. Two comparators are
the linear feedback strategy deﬁned by (17) and constrained to buys-only uLF
n ∨ 0 ∧ Xn,
n
and the volume-weighted average policy, known as VWAP. The ﬁrst uLF comparator utilizes the
benchmark formula as a function of current Xn, Dn, in other way it postulates the counter-factual
α = 1 even when α is not unity. This means it will under-estimate price impact when α > 1,
and will overestimate price impact when α < 1. The above over- or under-estimation of price
impact can lead to severe instability in the execution strategy. The second VWAP comparator
ﬁxes uV W
n = X0/N . That strategy is completely model independent, and therefore its performance
is little aﬀected by α.
In that sense the VWAP is the opposite of the Linear Feedback policy
which makes strong assumptions on the market environment; VWAP implements the same trades
no matter the model parameters.

:= u◦

While for a ﬁxed set of model parameters there are multiple ways to construct a nonlinear solver,
this would be computationally intractable to do for a large collection of (κ, η, α). Consequently,
our parametric solver is indispensable to provide a comprehensive solution across many parameter
conﬁgurations. Moreover, since we train jointly across a range of α, the linear case α = 1 is covered
and can be used to indirectly assess the ﬁt quality. In other words, we expect the performance of
the NN surrogate for α (cid:54)= 1 to be similar to its performance when α = 1.

16

0.100.050.000.050.100.15Relative Percentage Error in Total Execution Cost (in %)0100200300400500600700246810Period n05000100001500020000250003000035000Control u*nDNNLFα = 0.9

α = 1.1

Figure 4: Execution strategy (mean values across M (cid:48) = 104 simulations) for X0 = 100, 000, D0 = 0
and N = 10 with σ = 1, urgency parameter ν = 0.0001, η = 1/500 and diﬀerent price impact
parameters κ and α, utilizing a 5D solver in (X, D, κ, η, α).

Figure 4 compares the resulting NN-based strategies to the linear feedback program restricted
to buy-only. We observe that the linear feedback strategy uLF is very sensitive to α. For α > 1,
LF generates oscillatory strategies, cf. the right panel of Figure 4. This occurs because the linear
feedback underestimates the convex impact of trading on (Dn), so that it ﬁrst over-shoots relative
to the target Dn+1, then under-shoots, etc. The unconstrained reference strategy actually tries to
sell u◦
n ≥ 0, we still obtain
strong oscillations, no buying at n = 2 and almost no buying at n = 3. In contrast, the (estimated)
optimal strategy smoothly maintains un > 0 throughout. Conversely for α < 1 (left panel), the LF
strategy underpurchases in the ﬁrst trade since it does not correctly judge the reduced impact of
large trades due to the concave price impact function.

2 < 0 in the second step n = 2, as well as in the 4th step. Forcing uLF

For α = 0.9, price impact is much weaker and as a result the inventory urgency penalty domi-
nates and leads to an L-shaped strategy, with a lot of buying in the ﬁrst step or two, and a trickle for
the rest of the steps. For α = 1.1, price impact is the dominant feature and yields U-shaped strate-
gies, with largest trades at n = 1 and n = N . We also note that the dependence of the strategies
on κ, η remains complex for α (cid:54)= 1. Since in all cases, total trades must add up to X0 = 100, 000,
as parameters change, the resulting eﬀect on un is non-monotone, i.e. more is traded at some steps
and less in others. As a result, the various (average) strategy curves cross each other, often more
than once.

Table 1 reports the performance of a NN solver versus the LF one. Speciﬁcally, we use a 5D
NN solver that is parametric in (X, D, κ, η, α). The NN strategy is found to lead to total execution
costs that are 10-15% cheaper than LF, indicating that the diﬀerences observed in the respective
strategies in Figure 4 are material. Gains are larger for larger κ and larger η, i.e. for conﬁgurations
where price impact is more short-lived and more severe. While we do not have a “gold standard” to
compare against, we may use the α = 1.0 performance (where LF is exact up to the non-negativity
constraint that is almost never binding for this parameter conﬁguration) as a yardstick for NN
solution quality, since the NN solver is trained across diﬀerent choices of α.

5.3 NN Implementations

NN solvers necessarily contain multiple tuning parameters that must be chosen during implemen-
tation. It is a folk theorem that some ﬁnetuning is always necessary, one of the reasons that using

17

12345678910Period n0100002000030000400005000060000Control unDNN =0.4DNN =0.8LF =0.4LF =0.812345678910Period n05000100001500020000250003000035000Control unDNN =0.4DNN =0.8LF =0.4LF =0.8κ = 0.4

κ = 0.8

α = 0.9
α = 1.0
α = 1.1

η = 1/3000
10.33
−0.20
10.34

η = 1/500
7.33
−0.07
8.70

η = 1/3000
15.05
−0.52
13.01

η = 1/500
10.58
−0.04
5.85

Table 1: Performance of NN 5D solver in (X, D, κ, η, α) vs LF; based on 104 simulations with σ = 1
and inventory penalty of ν = 0.0001. We report relative percent diﬀerence of total execution cost
using Linear Feedback as baseline. Thus, positive values mean that the NN yields lower costs than
LF, and negative means that LF outperforms. We expect positive values for α (cid:54)= 1 and small
negative values for α = 1.

NN is a bit of a “black art”.

In our setting, the feed-forward neural networks for ˆVn and ˆmyxn are very straightforward and
do not require any bells and whistles. As mentioned, this simplicity of implementation is one of the
reasons that we advocate this problem as a good pedagogical case study for building NN solvers for
stochastic control problems. In particular, the NN architecture plays little signiﬁcance: as long as
one has suﬃcient ﬂexibility, the choice of the number of neurons, the number of layers, etc., is very
much secondary. Consequently, we default to the “canonical” set up of 16 neurons and 3 layers,
that has been used in multiple prior works.

Nevertheless, there are certainly some tuning parameters that aﬀect performance, ﬁrst and
foremost the eﬀort spent on training. The latter is driven by the size of the training set, and
the number of training epochs. The next set of experiments investigates in more detail how these
parameters impact solution quality. In Table 2 we compare the average P&L of the NN strategy
relative to LF for α = 1.0 as we vary the number of epochs E and the number of training points
M . As expected, accuracy increases as either M or E increase. We observe that the running time
is roughly linear in M (since the latter is a straightforward loop) and sub-linear in E.

In addition, we also compare solvers that live in diﬀerent dimensions, taking advantage of the
fact that our implementation is fully dimension-agnostic and a single line change is needed to
change d. For NN training purposes, that dimension d does not matter, so the running time is
constant as d changes. However, as expected, smaller d implies more dense training sets (since the
volume of the training domain shrinks) and hence better accuracy. This reﬂects the fundamental
property that learning a functional approximator is more laborious on a larger domain, and the
respective “volume” grows in d. Thus, for the same M, E, a 3D solver will be more accurate than
a 4D one, and less accurate than a 2D one. This is the (computational) price to pay for learning
simultaneously across multiple parameters.

Figure 5 further compares solvers in diﬀerent dimensions against each other in the nonlinear
price impact case α = 1.1. We consider NN solvers in 2D that only take (X, D) coordinates, as
well as in 3D with (X, D, κ), (X, D, η) coordinates, in 4D with (X, D, κ, η) and ﬁnally in 5D with
(X, D, κ, η, α). The fact that all solvers yield very similar strategies is an empirical indication of
the convergence of the NNs.

Finally, the right panel of Figure 5 shows the ﬁtted dependence of the control un on κ, η at n = 3.
Such dependence plots are the raison d’ˆetre of parametric solvers, providing the modeler with a
direct view of the sensitivity of the strategy to model parameters. Without a parametric solver,
it would be prohibitively expensive to generate such surfaces through re-solving each conﬁguration
one-by-one. We observe that at this intermediate step un shrinks in η and in κ.

18

NN Conﬁguration

4D w/(X, D, κ, η) M = 1000, E = 1000
M = 2000, E = 1000
M = 2000, E = 2000
M = 4000, E = 2000
M = 8000, E = 3000
M = 2000, E = 2000
M = 2000, E = 2000

3D w/(X, D, κ)
2D w/(X, D)

κ = 0.4 κ = 0.8 Time (min)
2.73
0.10%
5.39
0.20%
8.66
0.05%
16.97
0.03%
42.36
0.02%
8.38
0.04%
8.14
0.03%

1.43%
0.88%
0.46%
0.28%
0.27%
0.24%
0.15%

Table 2: Average P&L error compared to the linear feedback reference strategy for α = 1 across
diﬀerent NN implementations.
In all cases, the NN has 3 layers with 16 neurons in each. M :
number of training inputs; E: number of training epochs. The other test parameters are ﬁxed at
η = 0.0001, ν = 0.0001, σ = 1 initial condition X0 = 100, 000, D0 = 0, N = 10, utilizing a 4D solver
in (X, D, κ, η), except for the last two rows. All running times are based on a Intel i7-11370H
3.0GHz laptop with 32GB RAM.

Figure 5: Left: Comparing diﬀerent NN solvers with α = 1.1 and κ = 0.6, η = 0.001, ν = 0.00005.
Right: Dependence of NN-based execution un(X, D; κ, η) strategy on κ and η, keeping other pa-
rameters ﬁxed at n = 3, Xn = 60, 000, Dn = 20. We use the 5D solver shown on the left.

5.4 Square-Root Price Impact

We end our numerical experiments by investigating the special case where α = 0.5. This “square-
root law” of price impact is advocated by some practitioners (c.f., [11, 15, 44]) and has also been
addressed numerically in Curato et al. [25] via a brute force optimization of the cost function.
In contrast, our case study highlights once more the usefulness of our parametric solver in that
it readily computes an optimal execution schedule jointly across a range of diﬀerent price impact
parameters κ and η, as well as urgency rates ν, and thus unveils with ease after a single round
of training the solution’s dependence on the latter under this square-root price impact regime.
Speciﬁcally, for α = 0.5 ﬁxed, we train a 5D NN solver that is parametric in (X, D, κ, η, ν) over the
hyper-rectangle Xn ∈ [0, 105], Dn ∈ [0, 1], κ ∈ [0.35, 0.85], η ∈ [1/100, 1/1000], ν ∈ [0, 10−5] with
M = 8000 i.i.d. uniformly selected training points, and set σ = 0.1.

Figure 6 compares the resulting NN-based strategies to the linear feedback benchmark case
and Table 3 reports their performances. We ﬁrst note that the obtained strategy is very sensitive
to whether the urgency parameter ν is zero or not. This is sensible because with α = 0.5, the
magnitude of the incurred price impact is on the small scale and even further reduced by large
values for 1/η. As a consequence, as soon as ν becomes nonzero, the focus on rapidly reducing

19

12345678910Period n5000750010000125001500017500200002250025000Control un2D3D with 3D with 4D with ,5D with ,,0.40      0.50      0.60      0.70      0.80      1/40001/25001/16001/12001/1000NN Control un at n=314000      16000      18000      20000      22000      15000160001700018000190002000021000ν = 10−6

ν = 0

Figure 6: Execution strategy (mean values across M (cid:48) = 104 simulations) for X0 = 100, 000, D0 = 0
and N = 10 in the square root case α = 0.5 with σ = 0.1, diﬀerent price impact parameters κ and
η, and diﬀerent urgency parameters ν, utilizing a 5D solver in (X, D, κ, η, ν).

outstanding inventory dominates the overall order schedule; see the left panel in Figure 6.
In
contrast, when inventory control is turned oﬀ (i.e., ν = 0) the NN strategy exhibits an oscillatory
behavior: peaks of large buy orders are interrupted by near to zero-volume orders; see the right
panel in Figure 6. This observation is somewhat consistent with the numerical results presented
in [25, section 4.4] where the computed strategy consists of a few bursts of buying interspersed with
long periods of no trading. In terms of total execution costs, the NN solver always signiﬁcantly
outperforms the LF benchmark strategy in all considered parameter conﬁgurations for κ, η, ν; see
Table 3. Apparently, the LF feedback policy overestimates the price impact which leads to an
overall sub-optimal behavior.

ν = 10−6 κ = 0.8
κ = 0.4
ν = 0

η = 1/900
54.85
30.40

η = 1/200
17.56
28.11

Table 3: Performance of the 5D NN solver with y = (X, D, κ, η, ν) vs. LF; based on 104 simulations
with σ = 0.1. We report relative percent diﬀerence of total execution cost using Linear Feedback
as baseline. Positive values mean that the NN yields lower costs than LF.

6 Conclusion

In this article we have investigated neural network surrogates for solving optimal execution problems
across a range of model parameters. Our approach jointly learns an optimal strategy as a function
of the stochastic system state and of the market conﬁguration.

The developed algorithm, and the accompanying Jupyter Notebook can be used as a starting
point for many other related analyses. For example, it would be straightforward to modify the code
to handle parametric stochastic control problems of similar ﬂavor (e.g. discrete-time hedging).

A further use case is to use the trained neural network as a building block in a more sophisticated
setup. In particular, one may consider frameworks that explicitly account for model risk, in the
sense of imprecisely known parameters. In the adaptive approach (including the Predictive Model

20

12345678910Period n010000200003000040000500006000070000Control unDNN =0.8, 1/=900DNN =0.8, 1/=200LF =0.8, 1/=900LF =0.8, 1/=20012345678910Period n010000200003000040000Control unDNN =0.4, 1/=900DNN =0.4, 1/=200LF =0.4, 1/=900LF =0.4, 1/=200Control popular in engineering), the modeler ﬁrst develops learning dynamics, which convert static
parameters, such as κ, into a stochastic process (ˆκn), where ˆκn is the best estimate of the book
resilience at step n. Updating equations for ˆκn+1 in terms of the previous ˆκn and new information
from step n+1 (such as using the Bayesian paradigm) yield dynamics that can be merged with those
of (Xn, Dn). One then plugs-in the resulting ˆκn (and other similarly learned/updated parameters)
into the NN-learned ˆu(Xn, Dn, ˆκn) to obtain the adaptive strategy—which takes into account the
latest parameter estimates, but does not solve the full Bellman equation. Conversely, one could
also consider robust approaches, that minimize ˆVn(·) over feasible parameter settings in order
to protect against a worst-case situation. The latter again requires access to the computed ˆVn
as a building block. Finally, we may mention the adaptive robust approach [20] that combines
dynamic learning with a worst-case min-max optimization to protect against incorrect estimates
or mis-speciﬁed dynamics. The resulting numerical algorithms will be investigated in a separate,
forthcoming sequel.

7 Proofs

We start with Lemma 2 which provides an intermediate computation relevant for showing V ◦
0 in (16) (existence of round-trips) and characterizing the optimal u◦ in in (17).

n (0, 0) <

Lemma 2. For all n = N, N − 1, . . . , 2 the constants an, bn, cn recursively deﬁned in (14) and (15)
satisfy 2η + 4ν + 4an − 4ηbn + 4η2cn > 0.

Proof. For n = N we directly get from (14) that

2η + 4ν + 4aN − 4ηbN + 4η2cN = 4ηκ + 4ν > 0.

For n = N − 1, using the deﬁnition in (15), one computes

2η + 4ν + 4aN −1 − 4ηbN −1 + 4η2cN −1 =

32ηκν + 16ν2 + 4η2κ2(4 − κ2)
4ηκ + 4ν

> 0.

The general claim can then be checked similarly with a tedious backward induction relying on the
following recursive relation obtained from (15)

2η + 4ν + 4an − 4ηbn + 4η2cn = 2η + 4ν + 4(an+1 + ν) − 4η(1 − κ)bn+1 + 4η2(1 − κ)2cn+1

−

(cid:0)2(2ν + 2an+1 − ηbn+1) + 2η(1 − κ)(1 − bn+1 + 2ηcn+1)(cid:1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

= 4ηκ + 4ν + 4η2κ2 2cn+1(2an+1 + 2ν − η) − (1 − bn+1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

.

(29)

Proof of Proposition 1: In the linear case α = 1 the unconstrained version of the optimal
execution problem formulated in (12) is a linear quadratic stochastic control problem. Therefore,
it is well known that for all n ∈ {1, . . . , N } the value functions V ◦
n (x, d) are linear quadratic in
x and d. This motivates the ansatz V ◦
n (x, d) = anx2 + bnxd + cnd2 + en, where the coeﬃcients
an, bn, cn, en ∈ R are determined via backward induction by using the corresponding dynamic
programming equations in (10) and (11).

21

First, the terminal condition in (10) yields aN = η

2 , bN = 1 − κ, cN = 0 as claimed in (14), as
well as eN = 0. Next, for the inductive step, let n ∈ {N − 1, . . . , 1}. The dynamic programming
equation in (11) (for the considered unconstrained version of the problem) yields

V ◦
n (x, d) = min
u∈R

(cid:26)

(cid:26)

(1 − κ)du +

= min
u∈R

(1 − κ)du +

η
2
η
2

(cid:104)
V ◦
u2 + ν(x − u)2 + E
n+1(Xn, Dn)

(cid:12)
(cid:12)
(cid:12) Xn−1 = x, Dn−1 = d

(cid:105)(cid:27)

u2 + ν(x − u)2

(cid:104)
V ◦
+ E
n+1

(cid:0)x − u, (1 − κ)d + ηu + (cid:15)n

(cid:1) (cid:12)
(cid:12)
(cid:12) Xn−1 = x, Dn−1 = d

(cid:105)(cid:27)

.

(30)

Plugging in V ◦

n+1(x, d) = an+1x2 + bn+1xd + cn+1d2 + en+1 in (30) and solving the squares we obtain

V ◦
n (x, d)
(cid:40)
u2 (cid:16) η
2
(cid:16)

= min
u∈R

+ u

+ ν + an+1 − ηbn+1 + η2cn+1

(cid:17)

(ηbn+1 − 2ν − 2an+1)x + (1 − bn+1 + 2ηcn+1)(1 − κ)d

(cid:17)

+ (ν + an+1)x2 + (1 − κ)bn+1xd + (1 − κ)2cn+1d2 + en+1
(cid:19)

u (1 − 2bn+1 + 4ηcn+1) + (bn+1x + 2(1 − κ)cn+1d)

+

(cid:18) 1
2

E (cid:2)(cid:15)n

(cid:12)
(cid:12) Xn−1 = x, Dn−1 = d(cid:3)

+ cn+1E (cid:2)(cid:15)2

n

(cid:12)
(cid:12) Xn−1 = x, Dn−1 = d(cid:3)

(cid:41)

= min
u∈R

(cid:40)
u2 (cid:16) η
2
(cid:16)

+ u

+ ν + an+1 − ηbn+1 + η2cn+1

(cid:17)

(ηbn+1 − 2ν − 2an+1)x + (1 − bn+1 + 2ηcn+1)(1 − κ)d

(cid:17)

(31)

+ (ν + an+1)x2 + (1 − κ)bn+1xd + (1 − κ)2cn+1d2 + en+1 + cn+1σ2

(cid:41)
,

where we used the fact that E[(cid:15)n | Xn−1, Dn−1] = E[(cid:15)n] = 0 as well as E[(cid:15)2
σ2. Minimizing (31) with respect to u gives

n | Xn−1, Dn−1] = E[(cid:15)2

n] =

u = −

(ηbn+1 − 2ν − 2an+1)x + (1 − bn+1 + 2ηcn+1)(1 − κ)d
η + 2ν + 2an+1 − 2ηbn+1 + 2η2cn+1

(32)

and hence the feedback policy u◦
In particular, note that it follows from
Lemma 2 that η + 2ν + 2an+1 − 2ηbn+1 + 2η2cn+1 > 0 and that u in (32) is indeed the unique

n as claimed in (17).

22

minimum in (31). Moreover, inserting (32) back into (31) yields

(cid:16)

V ◦
n (x, d) = −

(cid:17)2

(ηbn+1 − 2ν − 2an+1)x + (1 − bn+1 + 2ηcn+1)(1 − κ)d
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1
+ (ν + an+1)x2 + (1 − κ)bn+1xd + (1 − κ)2cn+1d2
+ cn+1σ2 + en+1
(cid:18)

(cid:19)

(ηbn+1 − 2ν − 2an+1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

x2

=

ν + an+1 −

(cid:18)

+ (1 − κ)

bn+1 −

(cid:18)

+ (1 − κ)2

cn+1 −

(ηbn+1 − 2ν − 2an+1)(1 − bn+1 + 2ηcn+1)
η + 2ν + 2an+1 − 2ηbn+1 + 2η2cn+1
(cid:19)
(1 − bn+1 + 2ηcn+1)2
2η + 4ν + 4an+1 − 4ηbn+1 + 4η2cn+1

(cid:19)

xd

d2 + cn+1σ2 + en+1,

which implies the desired recursive formulas provided in (15) and the representation of the value
function in (16). Also note in (17) that u◦
n is
normally distributed for all n = 2, . . . , N . Indeed, since u◦
n−1, and the
state variables X ◦
j )j=1,...,n−1 and the i.i.d. zero-mean Gaussian noise
((cid:15)j)j=1,...,n−1, one checks that u◦
n is ultimately just a linear transformation of ((cid:15)j)j=1,...,n−1. As a
direct consequence, we can conclude that (u◦
1 as
deﬁned in (13). Finally, the representation of D◦ in (18) follows directly from its state dynamics
in (2).

1 is just a deterministic constant in R and that u◦

n)n=1,...,N is an admissible strategy in the set A ◦

n−1 are linear in (u◦

n is linear in X ◦

n−1 and D◦

n−1 and D◦

Proof of Proposition 2: In the case σ = 0 the optimal control problem in (12) (with α = 1)

is deterministic. We can introduce the corresponding cost functional C : RN → R given by

C(u1, . . . , uN ) (cid:44)

N
(cid:88)

n=1

(cid:110)(cid:16)

(1 − κ)Dn−1 +

η
2

(cid:17)

un + ν(Xn−1 − un)2(cid:111)

un

=

η
2

N
(cid:88)

n=1

u2
n + (1 − κ)

N
(cid:88)

n=1

Dn−1un + ν

N
(cid:88)

(Xn−1 − un)2.

(33)

n=1

For all n = 1, . . . , N , considering the state variables Xn(u1, . . . , uN ) (cid:44) Xn and Dn(u1, . . . , uN ) (cid:44) Dn
in (3) and (2) as functions in u1, . . . , uN ∈ R, we note that

∂Xn
∂ui

= −1,

∂Dn
∂ui

= η(1 − κ)n−i

(1 ≤ i ≤ n).

In particular, we have the relation

∂Dn
∂ui

= (1 − κ)

∂Dn
∂ui+1

(1 ≤ i ≤ n − 1).

(34)

Therefore, for all i ∈ {1, . . . , N − 1} we can compute

∂C
∂ui

= ηui + (1 − κ)Di−1 + (1 − κ)

= ηui + (1 − κ)Di−1 + (1 − κ)

N
(cid:88)

n=i+1
(cid:32)

∂Di
∂ui

∂Dn−1
∂ui

un − 2ν

N
(cid:88)

n=i

Xn

ui+1 +

N
(cid:88)

n=i+2

∂Dn−1
∂ui

(cid:33)

un

− 2ν

N
(cid:88)

n=i

Xn

= ηui + (1 − κ)Di−1 + (1 − κ)ηui+1 + (1 − κ)2

N
(cid:88)

n=i+2

∂Dn−1
∂ui+1

un − 2ν

N
(cid:88)

n=i

Xn,

(35)

23

where we used (34) in the last step. Similarly,

∂C
∂ui+1

= ηui+1 + (1 − κ)Di + (1 − κ)

N
(cid:88)

n=i+2

∂Dn−1
∂ui+1

un − 2ν

N
(cid:88)

n=i+1

Xn.

(36)

Hence, using (36) in (35) we obtain the recursive equation

∂C
∂ui

= (ηui + (1 − κ)Di−1)(2κ + κ2) − 2νκ

N
(cid:88)

n=i+1

Xn − 2νXi + (1 − κ)

∂C
∂ui+1

(1 ≤ i ≤ N − 1). (37)

Next, minimizing (33) under the constraint X0 − (cid:80)N
multiplier, we obtain the ﬁrst order conditions

n=1 un = 0 and denoting λ ∈ R the Lagrangian

∂C
∂ui

= λ

(i = 1, . . . , N ),

(38)

which, together with (37), can be rewritten as

λκ = κ(2 − κ)(ηui + (1 − κ)Di−1) − 2νκ

N
(cid:88)

n=i+1

Xn − 2νXi

(i = 1, . . . , N − 1).

(39)

Computing the diﬀerences of the equations in (39) for successive i and i − 1 (for i ∈ {2, . . . , N − 1})
and rearranging the terms yields the recursive formula

ui =˜aui−1 + ˜bDi−2 + ˜cXi−2

=˜aui−1 + ˜b(1 − κ)i−2d0 +

i−2
(cid:88)

j=1

where

(˜bη(1 − κ)i−2−j − ˜c)uj + ˜cX0

(i = 2, . . . , N − 1),

(40)

˜a :=

κ(2κη − κ2η + 2ν)
2κη − κ2η − 2κν + 2ν

,

˜b :=

κ2(2 − 3κ + κ2)
2κη − κ2η − 2κν + 2ν

,

˜c :=

−2κν
2κη − κ2η − 2κν + 2ν

.

(41)

Due to its linear structure, the recursion in (40) can be solved explicitly. We obtain the represen-
tation

ui = aiu1 + bid0 + ciX0

(i = 1, . . . , N )

(42)

where a, b, c ∈ RN are given by a1 := 1, b1 := c1 := 0, a2 := ˜a, b2 := ˜b, c2 := ˜c,

ai := ˜a · ai−1 +

i−2
(cid:88)

j=1

(cid:17)
(cid:16)˜bη(1 − κ)i−2−j − ˜c

aj,

bi := ˜a · bi−1 + ˜b(1 − κ)i−2 +

i−2
(cid:88)

j=1

(cid:17)
(cid:16)˜bη(1 − κ)i−2−j − ˜c

bj,

(43)

ci := ˜a · ci−1 + ˜c +

(cid:17)
(cid:16)˜bη(1 − κ)i−2−j − ˜c

cj,

i−2
(cid:88)

j=1

for i = 3, . . . , N − 1, as well as aN := − (cid:80)N −1
comes from the terminal condition uN = XN −1. Moreover, (42) implies the representation

j=1 bj, cN := 1 − (cid:80)N −1

j=1 aj, bN := − (cid:80)N −1

j=1 cj, which

Xi = ax

i+1u1 + bx

i+1d0 + cx

i+1X0

(i = 0, . . . , N − 1),

(44)

24

where ax, bx, cx ∈ RN are given by ax

1 := 0, cx

1 := 1 and

ax
i+1 := −

i
(cid:88)

j=1

aj,

as well as

1 := bx
i
(cid:88)

j=1

bx
i+1 := −

bj,

cx
i+1 := 1 −

i
(cid:88)

j=1

cj

(i = 1, . . . , N − 1);

(45)

(i = 0, . . . , N − 1),

(46)

i+1u1 + bd
where ad, bd, cd ∈ RN are deﬁned as ad

Di = ad

i+1d0 + cd
1 := 0, bd

i+1X0
1 := 1, cd
i
(cid:88)

1 := 0 and

η(1 − κ)i−jaj, bd

i+1 := (1 − κ)i +

η(1 − κ)i−jbj

η(1 − κ)i−jcj

j=1

(i = 1, . . . , N − 1).

(47)

ad
i+1 :=

cd
i+1 :=

i
(cid:88)

j=1

i
(cid:88)

j=1

In other words, the ﬁrst order conditions in (39), together with the terminal state constraint
XN = 0, allow to express u2, . . . , uN explicitly in terms of u1 via (42) and it remains to minimize
the costs in (33) as a linear quadratic function in u1 only, namely

C(u1, . . . , uN ) =

=

η
2

η
2

n=1
N
(cid:88)

N
(cid:88)

u2
n + ν

N
(cid:88)

(Xn−1 − un)2 + (1 − κ)

n=1

N
(cid:88)

n=1

Dn−1un

(anu1 + bnd0 + cnX0)2 + ν

n=1

n=1

N −1
(cid:88)

(ax

n+1u1 + bx

n+1d0 + cx

n+1X0)2

N
(cid:88)

+ (1 − κ)

(ad

nu1 + bd

nd0 + cd

nX0)(anu1 + bnd0 + cnX0)

n=1

(cid:21)
ηa(cid:62)a + (1 − κ)(ad)(cid:62)a + ν(ax)(cid:62)ax)

u2
1

(cid:20) 1
2

=

(cid:20) (cid:16)

+

a(cid:62)(ηb + (1 − κ)bd) + (1 − κ)(ad)(cid:62)b + 2ν(ax)(cid:62)bx(cid:17)

d0

(cid:16)

a(cid:62)(ηc + (1 − κ)cd) + (1 − κ)(ad)(cid:62)c + 2ν(ax)(cid:62)cx(cid:17)

+

(cid:21)

X0

u1

(cid:19)

ηb(cid:62)b + ν(bx)(cid:62)bx + (1 − κ)(bd)(cid:62)b

(cid:18) 1
2
(cid:18) 1
2
(cid:17)
ηb(cid:62)c + 2ν(bx)(cid:62)cx + (1 − κ)((bd)(cid:62)c + (cd)(cid:62)b)

ηc(cid:62)c + ν(cx)(cid:62)cx + (1 − κ)(cd)(cid:62)c − ν

X 2
0

d2
0

(cid:19)

(cid:16)

+

+

+

X0d0

which yields the claims in (20), (21), (22).

Proof of Corollary 2: In the case ν = 0 the constants introduced in (41) reduce to ˜a = κ,
˜b = κ(2 − 3κ + κ2)/(2η − κη) and ˜c = 0. Moreover, direct computations reveal that the coeﬃcients
in (43) simplify to

ai = κ,

bi = ˜b,

ci = 0

(i = 2, . . . , N − 1),

as well as aN = −1 − (N − 2)κ, bN = −(N − 2)˜b, cN = 1. This yields the claim in (24). For the
coeﬃcients in (45) we obtain

ax
i = −1 − (i − 1)κ,

i = −(i − 1)˜b,
bx

cx
i = 1

(i = 2, . . . , N ),

25

and for the coeﬃcients in (47) we have

ad
i = η,

bd
i = 1 − κ,

cd
i = 0

(i = 2, . . . , N ).

This yields the claim in (25). Finally, directly computing ˆb and ˆc as deﬁned in (19) gives the
expressions in (23).

References

[1] Julia Ackermann, Thomas Kruse, and Mikhail Urusov. “C`adl`ag semimartingale strategies for
optimal trade execution in stochastic order book models”. In: Finance and Stochastics 25.4
(2021), pp. 757–810.

[2] Julia Ackermann, Thomas Kruse, and Mikhail Urusov. “Optimal Trade Execution in an
Order Book Model with Stochastic Liquidity Parameters”. In: SIAM Journal on Financial
Mathematics 12.2 (2021), pp. 788–822.

[3] Aur´elien Alfonsi and Jos´e Infante Acevedo. “Optimal Execution and Price Manipulations in
Time-varying Limit Order Books”. In: Applied Mathematical Finance 21.3 (2014), pp. 201–
237.

[4] Aur´elien Alfonsi and Pierre Blanc. “Dynamic optimal execution in a mixed-market-impact

Hawkes price model”. In: Finance and Stochastics 20.1 (2016), pp. 183–218.

[5] Aur´elien Alfonsi, Antje Fruth, and Alexander Schied. “Constrained portfolio liquidation in a
limit order book model”. In: Advances in Mathematics of Finance. Warsaw, Poland: Banach
Center Publi. 83, Polish Acad. Sci. Inst. Math, 2008, pp. 9–25.

[6] Aur´elien Alfonsi, Antje Fruth, and Alexander Schied. “Optimal execution strategies in limit
order books with general shape functions”. In: Quantitative Finance 10.2 (2010), pp. 143–157.

[7] Aur´elien Alfonsi and Alexander Schied. “Capacitary Measures for Completely Monotone
Kernels via Singular Control”. In: SIAM Journal on Control and Optimization 51.2 (2013),
pp. 1758–1780.

[8] Aur´elien Alfonsi and Alexander Schied. “Optimal Trade Execution and Absence of Price
Manipulations in Limit Order Book Models”. In: SIAM Journal on Financial Mathematics
1.1 (2010), pp. 490–522.

[9] Aur´elien Alfonsi, Alexander Schied, and Alla Slynko. “Order Book Resilience, Price Manipu-
lation, and the Positive Portfolio Problem”. In: SIAM Journal on Financial Mathematics 3.1
(2012), pp. 511–533.

[10] Robert Almgren and Neil Chriss. “Optimal Execution of Portfolio Transactions”. In: Journal

of Risk 03 (2001), pp. 5–40.

[11] Robert Almgren, Chee Thum, Emmanuel Hauptmann, and Hong Li. “Direct Estimation of

Equity Market Impact”. In: RISK (July 2005).

[12] Ali Al-Aradi, Adolfo Correia, Danilo Naiﬀ, Gabriel Jardim, and Yuri Saporito. “Solving non-
linear and high-dimensional partial diﬀerential equations via deep learning”. In: arXiv preprint
arXiv:1811.08782 (2018).

[13] Ali Al-Aradi, Adolfo Correia, Danilo de Frietas Naiﬀ, Gabriel Jardim, and Yuri Saporito. “Ap-
plications of the deep Galerkin method to solving partial integro-diﬀerential and Hamilton-
Jacobi-Bellman equations”. In: arXiv preprint arXiv:1912.01455 (2019).

26

[14] Achref Bachouch, Cˆome Hur´e, Nicolas Langren´e, and Huyen Pham. “Deep neural networks
algorithms for stochastic control problems on ﬁnite horizon, Part 2: numerical applications”.
In: arXiv preprint arXiv:1812.05916 (2018).

[15] Emmanuel Bacry, Adrian Iuga, Matthieu Lasnier, and Charles-Albert Lehalle. “Market Im-
pacts and the Life Cycle of Investors Orders”. In: Market Microstructure and Liquidity 01.02
(2015), p. 1550009.

[16] Vlad Bally, Gilles Pag`es, and Jacques Printems. “A quantization tree method for pricing and
hedging multidimensional American options”. In: Mathematical Finance 15.1 (2005), pp. 119–
168.

[17] Peter Bank and Antje Fruth. “Optimal Order Scheduling for Deterministic Liquidity Pat-

terns”. In: SIAM Journal on Financial Mathematics 5.1 (2014), pp. 137–152.

[18] Dirk Becherer, Todor Bilarev, and Peter Frentrup. “Optimal liquidation under stochastic

liquidity”. In: Finance and Stochastics 22.1 (2018), pp. 39–68.

[19] Dimitris Bertsimas and Andrew W. Lo. “Optimal control of execution costs”. In: Journal of

Financial Markets 1.1 (1998), pp. 1–50.

[20] Tomasz R Bielecki, Tao Chen, Igor Cialenco, Areski Cousin, and Monique Jeanblanc. “Adap-
tive robust control under model uncertainty”. In: SIAM Journal on Control and Optimization
57.2 (2019), pp. 925–946.

[21] Jean-Philippe Bouchaud, J. Doyne Farmer, and Fabrizio Lillo. “How Markets Slowly Digest
Changes in Supply and Demand”. In: Handbook of Financial Markets: Dynamics and Evo-
lution. Ed. by Thorsten Hens and Klaus Reiner Schenk-Hoppe. Handbooks in Finance. San
Diego: North-Holland, 2009, pp. 57–160.

[22] Jean-Philippe Bouchaud, Yuval Gefen, Marc Potters, and Matthieu Wyart. “Fluctuations and
response in ﬁnancial markets: the subtle nature of ‘random’ price changes”. In: Quantitative
Finance 4.2 (2004), pp. 176–190.
´Alvaro Cartea and Sebastian Jaimungal. “Incorporating order-ﬂow into optimal execution”.
In: Mathematics and Financial Economics 10.3 (2016), pp. 339–364.

[23]

[24] Ying Chen, Ulrich Horst, and Hoang Hai Tran. “Portfolio liquidation under transient price
impact - theoretical solution and implementation with 100 NASDAQ stocks”. Preprint on
arXiv:1912.06426. 2019.

[25] Gianbiagio Curato, Jim Gatheral, and Fabrizio Lillo. “Optimal execution with non-linear

transient market impact”. In: Quantitative Finance 17.1 (2017), pp. 41–54.

[26] Ngoc-Minh Dang. “Optimal Execution with Transient Impact”. In: Market Microstructure

and Liquidity 03.01 (2017), p. 1750008.

[27] Martin Forde, Leandro S´anchez-Betancourt, and Benjamin Smith. “Optimal trade execution
for Gaussian signals with power-law resilience”. In: Quantitative Finance 0.0 (2021), pp. 1–12.

[28] Antje Fruth, Torsten Sch¨oneborn, and Mikhail Urusov. “Optimal trade execution and price
manipulation in order books with time-varying liquidity”. In: Mathematical Finance 24.4
(2014), pp. 651–695.

[29] Antje Fruth, Torsten Sch¨oneborn, and Mikhail Urusov. “Optimal trade execution in order
books with stochastic liquidity”. In: Mathematical Finance 29.2 (2019), pp. 507–541.

[30] Jim Gatheral. “No-dynamic-arbitrage and market impact”. In: Quantitative Finance 10.7

(2010), pp. 749–759.

27

[31] Jim Gatheral, Alexander Schied, and Alla Slynko. “Transient linear price impact and Fred-

holm integral equations”. In: Mathematical Finance 22.3 (2012), pp. 445–474.

[32] Maximilien Germain, Huyˆen Pham, and Xavier Warin. “Neural networks-based algorithms

for stochastic control and PDEs in ﬁnance”. In: arXiv preprint arXiv:2101.08068 (2021).

[33] Paulwin Graewe and Ulrich Horst. “Optimal Trade Execution with Instantaneous Price Im-
pact and Stochastic Resilience”. In: SIAM Journal on Control and Optimization 55.6 (2017),
pp. 3707–3725.

[34] Robert B Gramacy. Surrogates: Gaussian Process Modeling, Design, and Optimization for

the Applied Sciences. Chapman and Hall/CRC, 2020.

[35] Ben Hambly, Renyuan Xu, and Huining Yang. “Recent Advances in Reinforcement Learning

in Finance”. Preprint on arXiv:2112.04553. 2021.

[36] Jiequn Han and Weinan E. “Deep learning approximation for stochastic control problems”.
In: arXiv preprint arXiv:1611.07422 (2016). NIPS 2016, Deep Reinforcement Learning Work-
shop.

[37] Ulrich Horst and Xiaonyu Xia. “Multi-dimensional optimal trade execution under stochastic

resilience”. In: Finance and Stochastics 23.4 (2019), pp. 889–923.

[38] Gur Huberman and Werner Stanzl. “Price Manipulation and Quasi-Arbitrage”. In: Econo-

metrica 72.4 (2004), pp. 1247–1275.

[39] Cˆome Hur´e, Huyˆen Pham, Achref Bachouch, and Nicolas Langren´e. “Deep neural networks
algorithms for stochastic control problems on ﬁnite horizon, part I: convergence analysis”. In:
arXiv preprint arXiv:1812.04300 (2018).

[40] Amine Ismail and Huyˆen Pham. “Robust Markowitz mean-variance portfolio selection under

ambiguous covariance matrix”. In: Mathematical Finance 29.1 (2019), pp. 174–207.

[41] Sebastian Jaimungal. “Reinforcement learning and stochastic optimisation”. In: Finance and

Stochastics 26.1 (2022), pp. 103–129.

[42] Laura Leal, Mathieu Lauri`ere, and Charles-Albert Lehalle. “Learning a functional control for

high-frequency ﬁnance”. Preprint on arXiv:2006.09611. 2021.

[43] Charles-Albert Lehalle and Eyal Neuman. “Incorporating signals into optimal trading”. In:

Finance and Stochastics 23.2 (2019), pp. 275–311.

[44] Fabrizio Lillo, J. Doyne Farmer, and Rosario N. Mantegna. “Master curve for price-impact

function”. In: Nature 421.6919 (2003), pp. 129–130.

[45] Christopher Lorenz and Alexander Schied. “Drift dependence of optimal trade execution
strategies under transient price impact”. In: Finance and Stochastics 17.4 (2013), pp. 743–
770.

[46] Eyal Neuman and Moritz Voß. “Optimal signal-adaptive trading with temporary and tran-

sient price impact”. To appear in SIAM Journal on Financial Mathematics. 2022.

[47] Anna A. Obizhaeva and Jiang Wang. “Optimal trading strategy and supply/demand dynam-

ics”. In: Journal of Financial Markets 16.1 (2013), pp. 1–32.

[48] Gilles Pag`es, Huyˆen Pham, and Jacques Printems. “An optimal Markovian quantization
algorithm for multi-dimensional stochastic control problems”. In: Stochastics and Dynamics
4.4 (2004), pp. 501–545.

28

[49] Silviu Predoiu, Gennady Shaikhet, and Steven Shreve. “Optimal Execution in a General One-
Sided Limit-Order Book”. In: SIAM Journal on Financial Mathematics 2.1 (2011), pp. 183–
212.

[50] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine

Learning. The MIT Press, 2006.

[51] Alexander Schied, Torsten Sch¨oneborn, and Michael Tehranchi. “Optimal Basket Liquidation
for CARA Investors is Deterministic”. In: Applied Mathematical Finance 17.6 (2010), pp. 471–
489.

29

