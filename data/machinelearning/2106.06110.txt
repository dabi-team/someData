Assessing the Effectiveness of Syntactic Structure
to Learn Code Edit Representations

Syed Arbaaz Qureshi
Microsoft Research India
t-syqure@microsoft.com

Sonu Mehta
Microsoft Research India
Sonu.Mehta@microsoft.com
Ranjita Bhagwan
Microsoft Research India
bhagwan@microsoft.com

Rahul Kumar
Microsoft Research India
rahulku@microsoft.com

1
2
0
2

n
u
J

1
1

]

G
L
.
s
c
[

1
v
0
1
1
6
0
.
6
0
1
2
:
v
i
X
r
a

Abstract—In recent times, it has been shown that one can
use code as data to aid various applications such as automatic
commit message generation, automatic generation of pull request
descriptions and automatic program repair. Take for instance the
problem of commit message generation. Treating source code as
a sequence of tokens, state of the art techniques generate commit
messages using neural machine translation models. However, they
tend to ignore the syntactic structure of programming languages.
Previous work, i.e., code2seq [5] has used structural infor-
mation from Abstract Syntax Tree (AST) to represent source
code and they use it to automatically generate method names. In
this paper, we elaborate upon this state of the art approach and
modify it to represent source code edits. We determine the effect
of using such syntactic structure for the problem of classifying
code edits. Inspired by the code2seq approach, we evaluate
how using structural information from AST, i.e., paths between
AST leaf nodes can help with the task of code edit classiﬁcation
on two datasets of ﬁne-grained syntactic edits.

Our experiments shows that attempts of adding syntactic struc-
ture does not result in any improvements over less sophisticated
methods. The results suggest that techniques such as code2seq,
while promising, have a long way to go before they can be
generically applied to learning code edit representations. We hope
that these results will beneﬁt other researchers and inspire them
to work further on this problem.

Index Terms—Neural network, Classiﬁcation, Abstract Syntax

Tree

I. INTRODUCTION

The recent explosion of open-source and the ready avail-
ability of online version-control systems such as GitHub [13]
have led to an enormous amount of code in the public domain.
This in turn has given rise to the phrase, code as data, which
alludes to using code as input to machine learning models and
creating tools that extract useful information from code. Deep
learning methods, which have been very successful in solving
problems related to natural language processing, translation,
image classiﬁcation and speech recognition, have recently
been applied to code as well.

Take for instance the problem of creating code embeddings.
Recent work has explored how code embeddings, akin to
word embeddings in the ﬁeld of natural language processing
(NLP) [7], [29], [33], can be used to summarize code [4], [16],
generate documentation [23], [30] and detect code clones [35],

[36]. Even though in theory, software can be very complex,
it appears that even a small statistical model can capture
regularity in natural software to a great extent (naturalness
hypothesis) [1]. These ﬁndings have encouraged the use of
embedding-based techniques on code.

However, while the results of these recent efforts have been
encouraging, we recognize that there are signiﬁcant differences
language and code. Along with syntactic
between natural
structure, code statements have long range correlations, much
larger vocabulary than natural language and reduced robust-
ness to minor changes [1]. Previous work attempts to capture
such long range correlations by using syntactic structure:
data and control-ﬂow graphs of code [2], [31], [32], abstract
syntax trees [3], [38] etc. It has largely focused on applying
such techniques on whole code snippets (code snapshots) like
individual methods or classes [5], [6].

In this paper, we ask the question, “Are techniques that
capture syntactic structure useful in understanding code edits,
rather than code snippets?”. Some fundamental differences
between code edits and code snippets should be brought to
light. Code edits do not have a ﬁxed context; they can span
multiple ﬁles and different methods. On the other hand code
snippets have a well-deﬁned context, especially as explored
in previous work, where they span only a single method of
class [5], [6].

Our work is inspired by code2seq, which leverages the
syntactic structure of programming languages to summarize
a given code snippet. It is language agnostic. It represents a
given code snippet as a set of path-contexts over its abstract
syntax tree (AST), where each path is compressed to a ﬁxed
length vector using bi-directional LSTMs. It then uses the
attention mechanism to get a weighted average of the path
vectors to represent the code snippet in a vector space, and
uses it to produce the code summary or caption.

In this paper, we use an approach similar to code2seq to
learn distributed representations of code edits and use it for
ﬁne-grained code edit classiﬁcation. We concentrate on two
speciﬁc tasks: bug-ﬁx classiﬁcation (Section IV-A1) and code
transformation classiﬁcation (Section IV-A2). code2seq is
reported to be the state of the art on the task of method

 
 
 
 
 
 
Fig. 1: edit2vec model architecture

name generation [5], and the authors of code2seq claim
that it is generalizable. This motivates us to test code2seq on
code edit classiﬁcation. We compare our approach to represent
code edits with two other models: a) considering code as
a collection of tokens in a Bag-of-words model and b)
considering code as a sequence of tokens using an LSTM
model.

Our results suggest that using syntactic structure such as
path contexts does not help improve the efﬁcacy of code edit
classiﬁcation. Based on our experience, we observe that:
a) Previous-work using syntactic structure [5], [6] concentrated
on method name prediction, and therefore beneﬁted heavily
from the identiﬁer names which are captured as part of the
terminal nodes in AST. Code edit classiﬁcation, on the other
hand, does not beneﬁt from speciﬁc identiﬁer names.
b) Training models for classifying code edits with syntactic
constructs may require signiﬁcantly more data than is currently
available: code2seq used more than 15 million data-points
for training while we have access to edit datasets of size in the
order of tens of thousands. We believe that it is fundamentally
difﬁcult to collect millions of labelled code edits for reasons
we describe in section VI.

We therefore conclude that using syntactic structure, while
effective on code snippets, has a long way to go before it can
be applied to learning code edit representations.

The main contributions of this paper can be summarized as

follows:

• We propose a new code edit classiﬁcation approach that
uses syntactic structure of code along with the attention
mechanism to learn distributed representation of code
edits. We use this to classify bug-ﬁxes and code trans-
formations.

• We create a new code edit dataset generated by a set of
C# code transformers named Roslynator analyzers [19]
from the top 250 C# GitHub repositories based on their
popularity and release the data at https://ﬁgshare.com/s/
e2e4c6762c696825c6d1.

• We conduct experimental evaluations using two tasks:
bug-ﬁx classiﬁcation on a data-set of Java bug-ﬁxes
(ManySStuBs4J dataset [21]), and code transformation
classiﬁcation using data obtained from the top 250 C#
GitHub repositories based on their popularity. Our results

provide promising empirical evidence that the baseline
approach using LSTMs outperforms the approach that
uses syntactic structure of code.

II. MOTIVATION

Code edits fall into various categories such as bug-ﬁxes,
feature additions, and code refactors. With bug-ﬁxes, there
is ﬁne-grained classiﬁcation such as those based on the bug-
template, as deﬁned in Section IV-A1. Learning representa-
tions of such code edits is useful for many tasks such as bug-
ﬁx classiﬁcation (classifying which bug-ﬁx template is applied
on the code), commit message generation and recommendation
systems for automatic program repair. However, class labelled
data for code edits is relatively scarce, especially for big
edits spanning over multiple lines. Recently, researchers have
created datasets like ManySStuBs4J for bug-ﬁxes in Java
[21]. This has enabled us to concentrate on ﬁne-grained bug-
ﬁx classiﬁcation in this paper. Here is an example of one
such ﬁne-grained bug-ﬁx. Consider a scenario, where we
have two classes, class1 and class2, that inherit from
the same parent class. Both these classes override a method
from the parent class, and have their own implementation of
this method. A developer has mistakenly called this method
from class1, when it should be called from class2. The
developer does not notice this error at compile time, but she
later realizes her mistake and ﬁxes this bug. Previous work
has shown that such bug-ﬁxes, which modify a small set of
lines of code, are seen quite frequently during development
phase [21].

Automatically classiﬁed bug-ﬁxes can help the process of
software development in many ways. For instance, we can
use this information to decide the right set of reviewers to
review a speciﬁc kind of bug-ﬁx. It can be used for test
selection; only tests relevant to the speciﬁc bug-ﬁx need to
be run. Also, it can be used for vulnerability management
tasks, which is one of the most urgent security challenges
faced by software community. As deﬁned in [27], vulnerability
management tasks ensure that the open-source components
included in various products of a company are free from
(known) vulnerabilities. Currently, the National Vulnerability
Database is used to keep track of disclosed vulnerabilities, but

it has consistency and coverage issues. Moreover, this manual
approach cannot scale with the increase in open-source code.
Automatic program repair [8], [10] is another use-case
of learning code edit representations, where they are used
to automatically generate code-patches or bug-ﬁxes to solve
issues similar to those which have occurred in the past [37].
We conduct experiments to evaluate our proposed model on
simple classiﬁcation tasks which have a deterministic solution.
The aim of our work is not to solve the problem of bug-ﬁx
classiﬁcation, but to evaluate if code embeddings learnt using
syntactic structure of code help improve over baseline models
that consider code as natural language, ignoring it’s syntactic
structure.

III. MODEL ARCHITECTURE

We have built three models to evaluate whether syntactic
structure captured using AST helps learn better representations
of code edits. All of these models are classiﬁers, and they
differ in the way we represent the input set of old and new
code snippet. In this section, we describe the details of these
three models and their construction.

1) edit2vec: This model uses the syntactic structure of
code by representing it as a set of paths between different
terminal nodes of AST, similar to the way syntax is captured in
code2seq model. The code2seq model [5] was designed
to generate method names from the method body. It followed
the encoder-decoder architecture for Neural Machine Transla-
tion (NMT). While we draw inspiration from this work, our
model differs signiﬁcantly from code2seq in two ways:

• Characterizing the code edit. The input to the model has
to capture the difference between the code before the edit
and after the edit. The code2seq model did not need to
do this since the input was just a code snippet containing
the method body.

• Classiﬁcation, not generation. Our task is one of clas-
siﬁcation, not generation. Hence we replace the decoder
layer of code2seq with a classiﬁer. In our implemen-
tation, we have used a softmax layer for multi-class
classiﬁcation.

Figure 1 shows the overall design of the edit2vec model.
It should be noted that this model is language agnostic, and
can be used with code changes of any length in a single ﬁle.
We ﬁrst explain how we represent a code edit as an input to
our model.

a) Path-Context Extractor: We characterize a code edit
as the pair {cold, cnew} where cold is the source code before
the edit and cnew is the source code after the edit. Corre-
sponding to {cold, cnew}, we build the Abstract Syntax Tree
(AST) denoted by {told and tnew} respectively. Similar to
code2seq, we use the path-context as a means to capture the
syntactic structure of code. For a given AST, the path-context
is the shortest path from one terminal node (leaf) to another.
We represent a path-context as sequences of terminal and non-
terminal nodes; it starts with one terminal node, which is also
called the left-context, then a set of non-terminal nodes, which

we call as the path, follow. The last node, or the right-context,
is the other terminal node of the path.

The source code can represent the method, the class or
the ﬁle which was edited. An AST for such a source code
can be large and hence it may contain a large number of
path-contexts. Randomly selecting a certain number of path-
contexts (as done in code2seq) might not capture the edit
completely. Since we are dealing with smaller code edits,
typically 1-2 lines of code, we restrict our source code to
the lines of code that are changed. In addition to that, we
ﬁlter out data-points which have more than 40 path-contexts
in told or tnew. We have decided to select 40 path-contexts
as majority of the examples in our dataset have less than 40
path-contexts. In case the AST is very small and has fewer
than 40 path-contexts, we pad the path-contexts with dummy
values, to make the input size uniform 1. Note that, this model
can be used with code changes of any length in a single ﬁle.
However, we cannot use this model for changes spanning over
multiple ﬁles.

1 , . . . , pold

In this way, for each pair of ASTs told, tnew, we get
Pold, Pnew, where Pold = {pold
the set
of 40 path-contexts from the AST told and Pnew =
{pnew
40 } is the set of 40 path-contexts from the AST
1
tnew. For example, there is a code edit where the arguments of
a method are swapped. Table I shows how to get {Pold, Pnew}
from {cold, cnew} for this example.

, . . . , pnew

40 } is

b) Path-Context Encoder (PCE): These path-contexts
are input to the Path-Context Encoder (PCE). This uses a
similar technique as code2seq to encode a path-context
into a 128-dimensional vector called CP CV (Compact Path-
Context Vector). Figure 2 shows the model architecture of the
Path-Context Encoder. Consider a path-context p of Table I:
processURL, NE0, MCE, NE3, baseURL. The PCE encodes
the path-context in the following way:

For terminal nodes (processURL and baseURL) in the
path-context, i.e., the left and right contexts, the encoder ﬁrst
divides them into sub-tokens; the token ‘processURL’ splits
into ‘process’ and ‘url’. The PCE uses a 32-dimensional
embedding vector to represent each sub-token, and then av-
erages the embedding vectors of the sub-tokens to obtain the
ﬁnal vector for this terminal node. This generates two vectors
vprocessU RL and vbaseU RL.

For non-terminal nodes (NE0, MCE, NE3), the PCE ﬁrst
embeds them to three 128-dimensional embedding vectors, and
feeds them to a bi-directional LSTM layer [15] of 160 hidden
units. The output of this LSTM layer, which we call vpath,
thus encodes the path into a 160-dimensional vector.

The PCE concatenates the three vectors vprocessU RL, vpath
and vbaseU RL and feeds this to a fully-connected tanh layer,
which consists 128 hidden units. We refer to the output of this
layer as the Compact Path-Context Vector (CPCV). The CPCV
is a vector representation of the input path-context p.

c) Code Encoder (CE): Since there are not more than
40 path-contexts each for the old and new code {cold, cnew},

1https://www.tensorﬂow.org/guide/keras/masking and padding

Old code
cold = processURL(message, depth, baseURL, url)

New code
cnew = processURL(message, depth, url, baseURL);

told =

tnew =

P old ={ processURL, NE0, MCE, NE3, baseURL
processURL, NE0, MCE, NE2, depth
message, NE1, MCE, NE2, depth ...}

PCE Output = {CP CV old

1

, CP CV old

3

, .... CP CV old

40 }.

CE output = rold [160-D vector]

P new ={ processURL, NE0, MCE, NE4, baseURL
processURL, NE0, MCE, NE2, depth
message, NE1, MCE, NE2, depth ...}

PCE Output = {CP CV new

1

, CP CV new

3

, .... CP CV new

40

}

CE output = rnew [160-D vector]

TABLE I: Example to explain Path Context Encoder. The names of the non-leaf tokens are abbreviated; for example, NE0
stands for NameExpression0.

classiﬁer. The classiﬁer is a neural network with a tanh layer
of 80 hidden units, followed by a softmax layer, which
outputs the class of the code edit.

e) Model Hyperparameters: The entire edit2vec
model is trained end-to-end for 100 epochs, minimizing cat-
egorical cross-entropy loss using the ‘Adam’ [22] optimizer.
In order to reduce over-ﬁtting we use dropout layers in the
bi-directional LSTM layer, one after the tanh layer of the
Path-Context Encoder (dropout rate = 0.2), one after the tanh
layer of the Code-Encoder (dropout rate = 0.4), and one after
the tanh layer of the Classiﬁer (dropout rate = 0.6). The
dropout rates, the number of hidden units in each layer, the
vector dimensions for path (128-D) and context (32-D) token
embeddings, etc. are considered as model hyperparamaters.
The values of these hyperparameters are chosen after training
and evaluating 400 edit2vec models with various combina-
tions of hyperparameter values from a grid of values and then
selecting the one that gives the best accuracy on the validation
set in cross-validation [14].

Fig. 2: Path Context Encoder

the PCE outputs 40 CPCVs each for cold and cnew. Each
set of 40 CPCVs is input to Code Encoder (CE), which uses
the attention mechanism described in the code2seq model
to encode each set of CPCVs into 160-dimensional vector.
We represent these vectors as {rold, rnew} corresponding to
{cold, cnew} . Figure 3 shows the detailed model architecture
for Code Encoder. The input is passed to ReLu layer followed
by softmax layer which outputs the attention weights (a1,
. . . a40) for each CP CV . A weighted summation of
a2,
CPCVs (CP CV1×a1 + CP CV2×a2 + . . . + CP CV40×a40) is
passed through a fully connected tanh layer to output 160-D
vector. Code Encoder is similar to what is used in code2seq
model. Hence, in the interest of space, we do not describe it
in detail.

d) Classiﬁer: The vectors rold and rnew, obtained from
the Code Encoder, are concatenated and passed through a

2) LSTM: To compare with edit2vec, we build an
LSTM-based model (LSTM) [15] that considers code as a
sequence of tokens and ignores the syntactic structure. Given
{cold, cnew}, we tokenize each to a set of tokens. Each code
token is then mapped to a 64-dimensional embedding vector.
For both {cold, cnew} , the sequence of tokens is input to
a standard LSTM layer with 196 hidden units followed by
a dropout layer (dropout rate = 0.8). Figure 4 describes the
architecture of the LSTM model. The model outputs two 196-
dimensional vectors rLST M
corresponding to
old
{cold, cnew} respectively. The vectors {rLST M
, rLST M
new } are
concatenated and then passed to a classiﬁer. The classiﬁer used
here is same as deﬁned in Section III-1d.

and rLST M

new

old

3) Bag-of-words: This is the baseline model based on
classical machine learning techniques. This model ignores both
the syntactic structure as well as the sequence of the code
tokens. It considers the code as a bag of words, regardless of
their sequence in the code snippet.

Given cold and cnew, we tokenize each to a set of tokens.

MethodCallExpressionNameExpression0NameExpression1NameExpression2NameExpression3NameExpression4processURLmessagedepthurlbaseURLMethodCallExpressionNameExpression0NameExpression1NameExpression2NameExpression3NameExpression4processURLmessagedepthbaseURLurlNE0MCENE3processURLCPCVpath embedding layer(128 dimensional vector)bidirectional LSTM (160 hidden units)context embedding layer(32 dimensional vector)fully-connected layer(tanh, 128 hidden units)concatenationprocessurl+context embedding layer(32 dimensional vector)+baseURLbaseurlIn both the cases,

Example 2: file.getSize() → folder.getSize()
the set of diff tokens are {file,
folder}, but Example 1 belongs to class ‘different method
same args’ whereas Example 2 belongs to ‘change caller
in function call’. Using both cold and cnew help generalize
Bag-of-words and distinguish such code edits.

IV. EXPERIMENTAL SETUP
In this section, we describe the two code edit classiﬁcation
tasks along with the two datasets we have used to evaluate the
three models mentioned in Section III.

A. Code Edit Classiﬁcation Task

Given the source code before edit (cold) and source code
after edit (cnew), the code edit classiﬁcation task is to predict
the class of the edit that was applied on cold to generate
cnew. We analyze the following two instances of code edit
classiﬁcation:

1) Bug-ﬁx Classiﬁcation: The ManySStuBs4J data-set [21]
consists of 63, 923 labelled single-line bug-ﬁx changes mined
from 1000 popular open-source Java projects. The bug-ﬁxes
are classiﬁed into one of 16 bug templates (the simple stupid
bug templates - SStuB). Given cold and cnew, this task is to
predict the bug-template that was applied on cold to generate
cnew.

a) Filtering And Pre-processing.: We ﬁrst tokenize the
source code snippets into tokens such as identiﬁers, keywords,
constants, symbols and other elements. We use Python’s
javalang5 module for tokenizing and parsing the code. We
remove all data-points where either the cold or cnew were not
tokenizable by the javalang module. This ﬁlters out 3, 739
data-points.

Next, from the remaining 60, 184 data-points belonging to
16 different templates, we remove data-points belonging to the
three templates - ‘missing throws exception’, ‘delete throws
exception’, ‘change modiﬁer’ as cold and cnew are missing
for data-points of these templates in the dataset. We further
remove data-points from template ‘change unary operator’
because the Path-Context Extractor provided by the authors of
code2seq [5] was not able to capture cold and cnew properly.
Finally, we remove data-points of ‘change caller in function
call’ template because they were already a part of the other
templates.

Though our design and implementation can randomly select
40 path contexts from code, for the sake of this experiment,
we ﬁlter out the data-points where the number of path-contexts
were greater than 40. The idea is to keep the input to the
edit2vec model completely descriptive of the syntactic
change, and therefore, give it every chance to outperform
LSTM and Bag-of-words.

The ﬁltering and pre-processing leaves us with 28, 960 data-
points belonging to 11 classes. Table II provides an overview
of these classes6. This number may seem like a relatively

5https://pypi.org/project/javalang/
6https://github.com/mast-group/mineSStuBs contains the list of all the 16
bug-templates, along with a brief description and the number of data-points
for each bug-template.

Fig. 3: Code Encoder

Fig. 4: LSTM

We ﬁt two different Bag-of-words based vectorizers, the
count-based vectorizer2 and the tf-idf vectorizer3, on the
merged set of tokens. Using the learnt vectorizer, we then
separately vectorize cold and cnew, concatenate the two vectors
and pass it to a classiﬁer, to classify the edit. We test two
different classiﬁers - the linear-kernel and the RBF-kernel
support vector machine (SVM) classiﬁers4. The count-based
vectorizer gives more weightage to the word having high
frequency where as in the tf-idf vectorizer, the weightage
increases with the frequency of the token but is offset by
the number of code snippets in which the token appears, to
adjust for the fact that some tokens appear more frequently
than others (for example: datatypes in code).

Similar to edit2vec and LSTM models, we use both cold
and cnew as input to this model and not just the diff tokens
(tokens that were either added or deleted) so as to capture the
context around the code edit. Consider these two code edit
examples:

Example 1: os.file(path) → os.folder(path)

2https://scikit-

learn.org/stable/modules/generated/sklearn.feature extraction.text.
CountVectorizer.html#sklearn.feature extraction.text.CountVectorizer
3https://scikit-learn.org/stable/modules/generated/sklearn.feature

extraction.text.TﬁdfVectorizer.html

4https://scikit-learn.org/stable/modules/svm.html

CPCVold/new1CPCVold/new2CPCVold/new40aold/new1aold/new2aold/new40fully-connected layer(ReLU, 1 hidden unit)softmax layerfully-connected layer(tanh, 160 hidden units)rold/newCPCVold/newaold/newCPCVold/new . aold/newCode embedding layer (64 dimensional vector)processURL(messageLSTM (196 hidden units)rLSTMdepthbaseURLurl)Bug-template

Description

No of samples

change caller in function
call

Checks whether in a function call expression the caller object for it was
replaced with another one.

change numeral

change operand

change operator

different method
args

less speciﬁc if

more speciﬁc if

Checks whether a numeric literal was replaced with another one

Checks whether one of the operands in a binary operation was wrong.

Checks whether a binary operator was accidentally replaced with another
one of the same type

same

Checks whether the wrong function was called.

Checks whether an extra condition which either this or the original one
needs to hold (|| operand) was added in an if statement’s condition.
Checks whether an extra condition (&& operand) was added in an if
statement’s condition

overload method deleted
args

Checks whether an overloaded version of the function with less arguments
was called

overload method more
args

Checks whether an overloaded version of the function with more arguments
was called

swap arguments

Checks whether a function was called with two of its arguments swapped

swap boolean literal

Checks whether a Boolean literal was replaced

1488

4779

741

1711

9383

2095

1836

1040

3820

536

1531

TABLE II: Descriptions of various SStUB templates, and the number of samples associated with the template.

small set of data-points for deep-learning-based techniques.
However, as we explain in Section VI, code edit classiﬁcation
inherently suffers from a shortage of well-labeled data, a fact
that previous work has also conﬁrmed [21].

We divide the 28, 960 data-points into 26, 322 training
samples and 2, 638 testing samples. We perform stratiﬁed
sampling so that the proportion of each bug template or class
in the training and test set are equal.

2) Code Transformation Classiﬁcation: We created a
dataset of edits generated by a set of C# code transformers
named Rosylnator analyzers [19], which analyze source code,
list out parts which are not in compliance with a rule, and
transform them if there is a code ﬁx. For instance, ana-
lyzer RCS1049 transforms input source code if (var1 ==
false) to if (!var1). Given cold and cnew, this task
is to predict the the Rosylnator analyzer that was applied on
cold to generate cnew.

Our dataset contains 12, 784 code edits generated by ap-
plying 10 analyzers. Table III provides an overview of this
dataset7 over top 250 C# projects from GitHub based on
their popularity. The edits are annotated with the Roslynator
analyzer that has generated them.

We divide the 12, 784 obtained data-points into 11, 617
training samples and 1, 167 test samples. Here too, we perform
stratiﬁed sampling so that the proportion of samples belonging
to any Roslynator analyzer is the same in training and test set.

V. EVALUATION

the
In this section, we compare the performance of
code2seq-based model (edit2vec) with the baseline mod-
els described in Section III for the two code edit classiﬁcation
tasks described in Section IV.

7The analysers are described in https://github.com/JosefPihrt/Roslynator/

tree/master/docs/analyzers

To evaluate the performance of the models, we train the
model with the optimal set of hyperparamaters obtained from
hyperparamater tuning and use the average classiﬁcation ac-
curacy values of 3 runs of 10-fold cross-validation. Table IV
contains evaluation results for both bug-ﬁx classiﬁcation and
the code transformation classiﬁcation tasks.

The format of the table is as follows: the ﬁrst column
shows the classiﬁcation model. The second and third column
lists the average accuracy values for bug-ﬁx classiﬁcation with
and without canonicalization (described later in this section)
the last two columns lists the same for code transformation
classiﬁcation respectively.

For both tasks, LSTM outperforms the other models in terms
of classiﬁcation accuracy (without canonicalization). Clearly,
the LSTM model which considers code as a sequence of
tokens improves the classiﬁcation accuracy signiﬁcantly over
using Bag-of-words. This is because many code edits
are position-sensitive and the sequence of tokens plays a
signiﬁcant role in classiﬁcation. For instance, one example of
‘swap arguments’ class has the source code before edit (cold)
= waitForJobExecutor(3000, 500) and source code
after edit (cnew) = waitForJobExecutor(500, 3000).
The only change in the code before and after the edit is
the order of arguments in the method call. Such an order
or sequence of tokens is well handled by LSTM model but
Bag-of-words fails to capture this kind of change as the
set of tokens for source code both before and after the edit
remains the same.

The edit2vec model, which captures the syntactic
structure of code using path-contexts, does improve the
classiﬁcation accuracy (without canonicalization) over the
Bag-of-words model but it does not outperform the LSTM
model. At ﬁrst, we found these results counter-intuitive as
we expected to see an improvement in accuracy with the

Analyzer tag

Description

No of samples

RCS1001

RCS1032

RCS1049

RCS1085

RCS1123

RCS1124

RCS1146

RCS1163

RCS1168

RCS1220

Add braces (when expression spans over multiple lines)

Remove redundant parentheses

Simplify boolean comparison

Use auto-implemented property

Add parentheses according to operator precedence

Inline local variable

Use conditional access

Rename unused parameter to ‘ ’

Change parameter name to base name when they are not the same

Use pattern matching instead of combination of ’is’ operator and cast operator

443

516

574

2163

1428

1067

3368

2053

816

356

TABLE III: Descriptions of various analyzer tags, and the number of samples associated with the tags.

use of syntactic structure. As the difference between the
accuracy values of both the models is not much, we perform
statistical signiﬁcance tests to conﬁrm if the difference in the
performance is statistically signiﬁcant. For both LSTM and
edit2vec, we ﬁrst perform normality test over 30 accuracy
values, from 3 runs of 10-fold cross validation, using the
D’Agostino-Pearson test [11], testing the null hypothesis that
the accuracy values are normally distributed. We obtain a p-
value of 0.51 and 0.41 for LSTM and edit2vec respectively,
proving that they are indeed normally distributed. Since both
the distributions are normal, we perform Student’s t-test [20]
on the 30 accuracy values, for the LSTM and edit2vec
models, the null hypothesis being that the distributions are
identical. The p-value is 2.4 ∗ 10−11 which clearly shows that
the distributions are not identical, and the difference in the
performance of the two models is statistically signiﬁcant.
To further understand why LSTM does better

than
edit2vec, we examine, through visualizations, how well-
separated and “clean” the outputs from the two models are.
As the data-points belong to 11 different classes, we expect the
code edit representations to form 11 clusters in the interme-
diate latent space. So, we visualize the outputs from the layer
before the softmax classiﬁcation layer for both edit2vec
and LSTM. We reduce these output vectors to 2-dimensional
vectors using t-SNE [28], and plot them, as shown in Figure
6. From the plots, we see that some classes of edits like ‘swap
arguments’ (shown in light blue) and ‘overload method deleted
args’ (shown in pink) are clearly separated from each other
but there is signiﬁcant overlap of the clusters for ‘change
caller in function call’ (red) and ‘different method same args’
(black). We manually looked at some of the examples from
these two classes and found that they are very similar to
each other, which makes their separation more difﬁcult from
in ‘change caller in function
other classes. For instance,
call’ class, var1.var2(param1, param2) is changed
to var3.var2(param1, param2) where as in ‘differ-
ent method same args’,var1.var2(param1, param2) is
changed to var1.var3( param1, param2).

Although both edit2vec and LSTM were not able to
differentiate properly between many clusters, we also observed

that edit2vec was not able to distinguish properly between
some fairly different edits (for instance, classes ‘different
method same args’ (black) and ‘overload method more args’
(grey)), which were well separated by LSTM.

To understand why edit2vec does not perform as well as
LSTM for fairly different edits, we manually analyzed some of
the data-points from the test set which were correctly classiﬁed
by LSTM but incorrectly by edit2vec. One such case is
shown in Figure 5. The type of change (different method same
args) is the same in both the cases. Even the pathold and
pathnew are the same, the only difference is the left and right
context tokens. The LSTM model classiﬁed both correctly.
edit2vec classiﬁes the ﬁrst example correctly but fails to the
classify the second example correctly. We found many similar
examples in our manual analysis.

Based on such manual analyses of test data that LSTM
correctly classiﬁes but edit2vec does not, we hypothesize
that the edit2vec model relies heavily on the context tokens,
and not as much on the path. Previous work [6] too has
made a similar observation where the authors did an ablation
study to understand the contribution of each component of
the path-context for the task of method name prediction and
showed that the context tokens contributed signiﬁcantly higher
than the path of non-terminal nodes. The dependence of the
model on the context tokens is good and, in fact, advantageous
for method name prediction, which was the task targeted by
both code2seq and code2vec, because context tokens and
method names have semantic relationships in a method. In
contrast, semantics of context tokens serve no purpose in code
edit classiﬁcation, as the class of the code edit does not depend
on individual tokens, but on the order of the tokens.

Consequently, to understand whether the token names were
confounding our classiﬁcation tasks, we repeat our experi-
ments by dropping meaningful tokens in the path context and
replacing them with canonical values. The canonicalization
process is brieﬂy explained as follows:

• Rename all identiﬁers with standard variable names like
var1 and var2. For example, getConfig.getID()
is changed to var1.var2().

• Replace all

integer, ﬂoat and string constants with

cold : url.toDecodedString();
cnew : url.toString();
left-contextold : url
left-contextnew : url
right-contextold : toDecodedString
right-contextnew : toString
pathold : NameExpression0, MethodCallEx-
pression , NameExpression2
pathnew : NameExpression0, MethodCallEx-
pression, NameExpression2
tagpredicted : different method same args
tagactual : different method same args

cold : AtmResponse.create();
cnew : AtmResponse.newInstance();
left-contextold : AtmResponse
left-contextnew : AtmResponse
right-contextold : create
right-contextnew : newInstance
pathold: NameExpression0, MethodCallExpres-
sion, NameExpression2
pathnew : NameExpression0, MethodCallEx-
pression, NameExpression2
tagpredicted : change caller in function call
tagactual : different method same args

(a) Example correctly classiﬁed by edit2vec

(b) Example incorrectly classiﬁed by edit2vec

Fig. 5: Examples that are both correctly classiﬁed by LSTM, but only Example 5a is correctly classiﬁed by edit2vec.

standard integer, ﬂoat and string constants For example,
getID(932, 1044) is changed to var1(1,2);
rectangle.perimeter(2.345, 4.234)
var1.var2(0.001, 0.002);
is
to
setName("Alice", "Bob")
to
is
var1("string1", "string2").

changed

changed

We reevaluate the models with canonicalized context tokens
for both the bug-ﬁx classiﬁcation and code transformation
classiﬁcation task. In the case of bug-ﬁx classiﬁcation, we
see that the performance of edit2vec has improved sig-
niﬁcantly. But at the same time, the LSTM model has also
improved. We conﬁrmed that even with such canonical in-
puts, LSTM performs statistically better than the edit2vec
model. Performing the Student’ t-test, with the null hypothe-
sis: accuracy values from LSTM and edit2vec have equal
distributions, resulted in a p-value of 1.53 ∗ 10−26. It again
shows that the difference between the performance of the
two models is statistically signiﬁcant. With such canonicaliza-
tion, the performance of the Bag-of-words model (linear
classiﬁer) reduces, which shows that Bag-of-words relies
heavily on identiﬁer names and not on the syntax of the
code. For code transformation classiﬁcation, though there is
slight improvement in the performance of both LSTM and
edit2vec after canonicalization, the improvement is not
signiﬁcant. This may be because of the larger code snippets in
the code transformation dataset, when compared to the bug-
ﬁx dataset; the number of canonicalized variables in a code
snippet are higher in the former case, defeating the purpose
of canonicalization to some extent. In this case too, LSTM
performs better than edit2vec.

We manually analyzed the mis-classiﬁed canonicalized data.
For the canonicalized dataset, there were 30 examples where
the LSTM model classiﬁed correctly but the edit2vec failed
to classify them correctly. But there were 10 other examples
which edit2vec classiﬁed correctly but LSTM model failed.
By analyzing these examples, we learned that edit2vec
captures some high-level syntactic representation of code

Model

Bug-ﬁx

Accuracy

tf-idf SVM (RBF)

tf-idf SVM (linear)

count SVM (RBF)

32.34%

85.30%

32.34%

count SVM (linear)

86.69%

LSTM

code2seq

94.47%

93.17%

Accuracy
(Canon.)

58.81%

67.37%

72.06%

76.08%

99.21%

98.44%

Code Transformation
Accuracy
(Canon.)

Accuracy

26.31%

85.30%

34.24%

88.39%

92.55%

92.28%

73.82%

69.49%

76.31%

74.78%

92.77%

92.59%

TABLE IV: Classiﬁcation accuracy for the bug-ﬁx classiﬁca-
tion and code transformation classiﬁcation task

which LSTM is not able to capture. Also, in Figure 6a for
LSTM model, we see that the class ‘different method same
args’ (black) is clustered into three sub-clusters, which is
not the case for edit2vec (Figure 6b). On manual analysis
of some examples from each sub-cluster, we observed that
they are grouped based on the number of arguments in a
method. Methods having no arguments form one sub-cluster,
methods with one argument form another sub-cluster and
methods having more than one arguments form the third sub-
cluster, even though they all belong to the same class. This
sub-division within the cluster is not present for edit2vec
model. This also supports that edit2vec is able to capture
some high-level syntactic representation of code which LSTM
fails to capture, but this representation is not sufﬁcient to
classify edit, speciﬁcally simpler and single-line edits. Single-
line edits (specially SStuBs) seem to occur frequently [21]
compared to other types of code edits, and are easy to label,
speciﬁcally in open-source code.

VI. THREATS TO VALIDITY

While our ﬁndings do indicate that syntactic structure does
not help code edit classiﬁcation tasks, we recognize some
threats to validity in this section.

A. Threats to Internal Validity

In this section, we address threats to internal validity.

a) Lack of data.:

It is well-understood that present-
day neural networks require large amounts of data to classify
with high accuracy. The LSTM model has 269, 147 parame-
ters, whereas the edit2vec model has 490, 892 parameters.
edit2vec is more data-hungry, compared to LSTM. It is
indeed possible that with signiﬁcantly more data, edit2vec
’s performance will improve signiﬁcantly over what we report.
However, collecting clean data for code edit classiﬁcation is
fundamentally difﬁcult because it requires signiﬁcant amounts
of manual effort to conﬁrm that labels for training data are
indeed correct. It has been found [12] that commit messages
and descriptions written by developers are very often not
descriptive enough, and in many cases, they are completely
meaningless. Also, developers tend to push multiple changes
together in a commit, which makes it even more difﬁcult to
label smaller ﬁxes within a commit. So fundamentally, we can-
not use data from millions of commits of open source GitHub
repositories. For our analysis, we use the ManySStuBs4J
dataset [21], which contains a particular set of classes of bug-
ﬁxes. These were collected from commits which have more
descriptive commit messages, and their commit messages
contain speciﬁc keywords like ‘error’, ‘bug’, ‘ﬁx’ etc. that
help to decide if a commit is a bug-ﬁx or not. However, this
work shows that the average frequency of bug-ﬁxes is about
1 per 1600-2500 lines of code, whereas the average length of
a code snippet (method), as used by code2seq, is about 7
lines of code. Hence, narrowing in on bug-ﬁxes reduces the
data available by at least two orders of magnitude.

b) Encoding speciﬁcs.: We use the path-context as a
fundamental unit of capturing syntactic structure. It is possible
that a different representation of syntactic structure could
improve the performance of edit2vec. However, our choice
was driven by previous work which has successfully used
the path-context construct [5], [6]. It is also possible that
a radically different model architecture will show different
results. However, our choice of the model is done rigorously,
by rigorously tuning the hyperparameters and choosing the
best of 400 different models, and so we are conﬁdent that
the design of the model will not signiﬁcantly change our
conclusions.

B. Threats to External Validity

In this section, we address threats to external validity. While
we have concentrated on two tasks across two languages, there
may be other code edit classiﬁcation tasks that could beneﬁt
from the use of syntactic structure. Also, our study is on
relatively small code edits. It is possible that syntactic structure
will help in classifying larger code edits. However, there is
a tussle between larger code edits and accurately labelled
data. Larger code edits make it even more difﬁcult to collect
accurately labelled data, since a summarization of the code
edit would have to be more descriptive. Previous work [21]
have found that single-line edits occur relatively often (1 per
1600-2500 lines of code), making them potentially a promising

dataset which can be used for various applications related to
code edits.

VII. RELATED WORK
This paper takes inspiration from three categories of related
work: using NLP techniques on code, using syntactic structure
to learn code embeddings and different ways of learning
distributed representation of code edits. In this section, we
describe the related work in these areas.

A. Using NLP Techniques On Code

Inspired from the naturalness hypotheses and availability
of enormous amount of code in the public domain, the use
of natural language based embedding techniques on code has
increased signiﬁcantly where code is considered as a sequence
of tokens. CODE-NN by Iyer et al. [16] presents an end-to-end
neural attention model using LSTMs to generate summaries
of C# and SQL code. Their model outperforms the baseline
models that use a tf-idf based approach. Li et al. [9] compare
various techniques including CODE-NN for neural code search
and show that attention based weighting scheme on code
embeddings outperforms more sophisticated techniques like
CODE-NN. All of these approaches to learn embeddings of
code tokens ignore the syntax of the code. They deal with
well deﬁned blocks of code (like functions or classes).

B. Code Embeddings Using Syntactic Structure

As mentioned in Section I, code has syntactic structure
and long range correlations which make them different from
natural language. Prior work has explored capturing such long
range correlations by using data and control-ﬂow graphs of
code, abstract syntax trees [2], [3], [31], [32], [38], etc.

Two of the most recent works in this space are code2vec
[6] and code2seq [5]. Both these techniques are used to
represent entire code-snippets (methods) and are tested for
method name prediction. In both these techniques, the code
snippet
is represented as a set of path-contexts extracted
from its AST. An attentional mechanism is used over a
set of randomly sampled path-contexts, while predicting or
generating the method name. There are differences in the
way embeddings are learnt for path-contexts. Path-context
is considered as a single token in code2vec. Whereas,
is considered as a sequence of tokens in code2seq.
it
code2seq has achieved the state of the art performance in
code summarization (speciﬁcally method name prediction) and
this inspired us to use this source code embedding technique
to learn embeddings for code edits.

Gated graph neural networks (GGNNs) [24] are another
way to learn features for graph-structured inputs. Using these
networks, ASTs are extended to graphs, by adding a variety
of code dependencies as edges, to model code semantics [3].
The tree based capsule network [17] is another such tech-
nique which captures both syntactic structure and dependency
information in code, without the need of explicitly adding
dependencies in the trees or splitting a big tree into smaller
ones. All of these techniques are used to represent entire code
snippets, and not code edits.

(a) t-SNE plot of LSTM

(b) t-SNE plot of edit2vec

(cid:4) swap arguments (cid:4) overload method deleted args (cid:4) change operand (cid:4) overload method more args
(cid:4) change numeral (cid:4) different method same args
(cid:4) more speciﬁc if (cid:4) change caller in function call (cid:4) change operator

(cid:4) swap boolean literal

(cid:4) less speciﬁc if

Fig. 6: t-SNE plots of code edits from 11 classes of ManySStuBs4J dataset

C. Learning Distributed Representation of Code Edits

Most of the work related to code edits are targeted for
speciﬁc applications like commit message generation, auto-
matic program repair etc. The ﬁrst works on commit message
generation, by Loyola et al. [26] and Jiang et al. [18], use
attentional encoder-decoder architecture to generate commit
messages from git diffs. Loyola et al. [26] use vanilla archi-
tecture where Jiang et al. [18] used a modiﬁed architecture
with RNNs in the encoder. Liu et al. [25] evaluate the
performance of NMT-based techniques for commit message
generation. They show that the performance of NMT-based
techniques declines signiﬁcantly when automatically generated
trivial commit messages were removed from the data. Liu
et al. [25] propose a simpler technique based on Nearest
Neighbour algorithm (NNGen). In this case, the diff vectors
are presented as a bag-of-words and cosine similarity between
the diff vector of the input and the training data is used to ﬁnd
the nearest commit for the given input.

Closely related work to ours in this space is commit2vec
[27] where Lozoya et al. have compared AST based code
representation learnt by code2vec [6] with other models
based on LSTMs and bag-of-words, for binary classiﬁcation of
security related commits. They use transfer learning by using
pre-trained embeddings from code2vec and another pretext
task of predicting the priority of Jira tickets, both of which
have large training data available. They observe that AST-
based representation gives superior performance over other
representations and pre-training the embeddings from a highly
relevant pretext task further improves the results. They have
tested it on a small dataset of 1950 commits. Also, they have
used code2vec-based embeddings and the task is binary

classiﬁcation, In our case, we use code2seq-based approach
for multi-class classiﬁcation problem on a much larger dataset
from two different languages.

Another related work in this space is DeepBugs [34],
a name based bug-detection technique. They model it as a
binary classiﬁcation problem and use semantic representations
or embeddings of identiﬁer names for classiﬁcation. They also
ignore the syntactic structure of the code while learning the
embeddings.

Allamanis et al. [37] introduced the problem of learning
distributed representation of edits. They propose a neural
network based technique to combine the structure and seman-
tics of code edits. They evaluate their model on the task of
generating new code given old code and edit representation
as input. Based on the results, they conclude that graph based
edit encoder (which uses tree representation of code) often
fails to capture simpler edits and it does not outperform the
sequence based edit-encoder. Our observations from the results
of code edit classiﬁcation have similar conclusions; AST based
representation of code does not help improve over the baseline
approach of considering code as a sequence of tokens, i.e.,
LSTM

VIII. CONCLUSION

In this work, we introduced a code edit classiﬁcation ap-
proach (edit2vec) that uses syntactic structure along with
attention mechanism to learn distributed representation of code
edits. We conducted experimental evaluations of this model
on two tasks: bug-ﬁx classiﬁcation and code transformation
classiﬁcation and compared the performance of the model with
baseline approaches that used LSTM and Bag-of-words.

[21] R.-M. Karampatsis and C. Sutton. How often do single-statement bugs
arXiv preprint arXiv:1905.13334,

occur? the manysstubs4j dataset.
2019.

[22] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization.

arXiv preprint arXiv:1412.6980, 2014.

[23] A. LeClair, S. Jiang, and C. McMillan. A neural model for generating
natural language summaries of program subroutines. In 2019 IEEE/ACM
41st International Conference on Software Engineering (ICSE), pages
795–806. IEEE, 2019.

[24] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence

neural networks, 2015.

[25] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang. Neural-
machine-translation-based commit message generation: How far are we?
In Proceedings of the 33rd ACM/IEEE International Conference on
Automated Software Engineering, ASE 2018, page 373–384, New York,
NY, USA, 2018. Association for Computing Machinery.

[26] P. Loyola, E. Marrese-Taylor, and Y. Matsuo. A neural architecture
for generating natural language descriptions from source code changes.
Proceedings of the 55th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), 2017.

[27] R. C. Lozoya, A. Baumann, A. Sabetta, and M. Bezzi. Commit2vec:
Learning distributed representations of code changes. arXiv preprint
arXiv:1911.07605, 2019.

[28] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. Journal of

machine learning research, 9(Nov):2579–2605, 2008.

[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Dis-
tributed representations of words and phrases and their compositionality.
In Advances in neural information processing systems, pages 3111–
3119, 2013.

[30] D. Movshovitz-Attias and W. Cohen. Natural language models for
predicting programming comments. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2:
Short Papers), pages 35–40, 2013.

[31] R. Nobre, L. G. Martins, and J. M. Cardoso. A graph-based iterative
compiler pass selection and phase ordering approach. ACM SIGPLAN
Notices, 51(5):21–30, 2016.

[32] E. Park, J. Cavazos, and M. A. Alvarez. Using graph-based program
In Proceedings of the Tenth
characterization for predictive modeling.
International Symposium on Code Generation and Optimization, pages
196–206, 2012.

[33] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for
word representation. In Proceedings of the 2014 conference on empirical
methods in natural language processing (EMNLP), pages 1532–1543,
2014.

[34] M. Pradel and K. Sen. Deep learning to ﬁnd bugs. 2017.
[35] M. Tufano, C. Watson, G. Bavota, M. Di Penta, M. White, and
D. Poshyvanyk. Deep learning similarities from different representations
of source code. In 2018 IEEE/ACM 15th International Conference on
Mining Software Repositories (MSR), pages 542–553. IEEE, 2018.
[36] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk. Deep learning
In 2016 31st IEEE/ACM
code fragments for code clone detection.
International Conference on Automated Software Engineering (ASE),
pages 87–98. IEEE, 2016.

[37] P. Yin, G. Neubig, M. Allamanis, M. Brockschmidt, and A. L. Gaunt.

Learning to represent edits, 2018.

[38] J. Zhang, X. Wang, H. Zhang, H. Sun, K. Wang, and X. Liu. A novel
neural source code representation based on abstract syntax tree.
In
2019 IEEE/ACM 41st International Conference on Software Engineering
(ICSE), pages 783–794. IEEE, 2019.

For both the tasks, we observe that the AST-based approach
does not outperform the LSTM model. We observe that even
though edit2vec captures some high-level syntactic repre-
sentation of code, which LSTM is not able to capture, this
complex representation is not required in case of simpler
and smaller edits. We believe that it is difﬁcult to get large
labelled datasets for code edits. While we have evaluated AST-
based models on relatively small datasets of few thousand
examples, further development is needed both in the area
of getting labelled datasets and exploring these models for
different tasks. We hope that our work inspires others to work
in this interesting space.

REFERENCES

[1] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton. A survey of
machine learning for big code and naturalness. ACM Computing Surveys
(CSUR), 51(4):1–37, 2018.

[2] M. Allamanis and M. Brockschmidt. Smartpaste: Learning to adapt

source code. arXiv preprint arXiv:1705.07867, 2017.

[3] M. Allamanis, M. Brockschmidt, and M. Khademi. Learning to represent

programs with graphs, 2017.

[4] M. Allamanis, H. Peng, and C. Sutton. A convolutional attention
In International

network for extreme summarization of source code.
conference on machine learning, pages 2091–2100, 2016.

[5] U. Alon, S. Brody, O. Levy, and E. Yahav.

sequences from structured representations of code.
arXiv:1808.01400, 2018.

code2seq: Generating
arXiv preprint

[6] U. Alon, M. Zilberstein, O. Levy, and E. Yahav. code2vec: Learning
distributed representations of code. Proceedings of the ACM on Pro-
gramming Languages, 3(POPL):1–29, 2019.

[7] A. Bakarov. A survey of word embeddings evaluation methods. arXiv

preprint arXiv:1801.09536, 2018.

[8] S. Bhatia and R. Singh. Automated correction for syntax errors
in programming assignments using recurrent neural networks. arXiv
preprint arXiv:1603.06129, 2016.

[9] J. Cambronero, H. Li, S. Kim, K. Sen, and S. Chandra. When deep
learning met code search. Proceedings of the 2019 27th ACM Joint
Meeting on European Software Engineering Conference and Symposium
on the Foundations of Software Engineering - ESEC/FSE 2019, 2019.
[10] J. C. Campbell, A. Hindle, and J. N. Amaral. Syntax errors just aren’t
natural: improving error reporting with language models. In Proceedings
of the 11th Working Conference on Mining Software Repositories, pages
252–261, 2014.

[11] R. D’Agostino and E. S. Pearson. Tests for departure from normality.
empirical results for the distributions of b2 and square root of b1.
Biometrika, 60(3):613–622, 1973.

[12] R. Dyer, H. A. Nguyen, H. Rajan, and T. N. Nguyen. Boa: A language
and infrastructure for analyzing ultra-large-scale software repositories.
In 2013 35th International Conference on Software Engineering (ICSE),
pages 422–431. IEEE, 2013.

[13] GitHub Inc. https://github.com. [Online; accessed 8-May-2020].
[14] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. Springer series in
statistics. Springer, 2009.

[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural

Comput., 9(8):1735–1780, Nov. 1997.

[16] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer. Summarizing
In Proceedings of the
source code using a neural attention model.
54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 2073–2083, Berlin, Germany, Aug.
2016. Association for Computational Linguistics.

[17] V. Jayasundara, N. D. Q. Bui, L. Jiang, and D. Lo. Treecaps: Tree-
structured capsule networks for program source code processing, 2019.
[18] S. Jiang, A. Armaly, and C. McMillan. Automatically generating commit
2017 32nd
messages from diffs using neural machine translation.
IEEE/ACM International Conference on Automated Software Engineer-
ing (ASE), Oct 2017.

[19] Josef Pihrt. [Online; accessed 8-May-2020].
[20] D. Kalpi´c, N. Hlupi´c, and M. Lovri´c. Student’s t-Tests, pages 1559–

1563. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.

