Periodic Bandits and Wireless Network Selection
Shunhao Oh
Department of Computer Science, National University of Singapore
ohoh@u.nus.edu

Anuja Meetoo Appavoo
Department of Computer Science, National University of Singapore
anuja@comp.nus.edu.sg

Seth Gilbert
Department of Computer Science, National University of Singapore
seth.gilbert@comp.nus.edu.sg

Abstract

Bandit-style algorithms have been studied extensively in stochastic and adversarial settings. Such
algorithms have been shown to be useful in multiplayer settings, e.g. to solve the wireless network
selection problem, which can be formulated as an adversarial bandit problem. A leading bandit
algorithm for the adversarial setting is EXP3. However, network behavior is often repetitive, where
user density and network behavior follow regular patterns. Bandit algorithms, like EXP3, fail to
provide good guarantees for periodic behaviors. A major reason is that these algorithms compete
against ﬁxed-action policies, which is ineﬀective in a periodic setting.

In this paper, we deﬁne a periodic bandit setting, and periodic regret as a better performance
measure for this type of setting. Instead of comparing an algorithm’s performance to ﬁxed-action
policies, we aim to be competitive with policies that play arms under some set of possible periodic
patterns F (for example, all possible periodic functions with periods 1, 2, · · · , P ). We propose
Periodic EXP4, a computationally eﬃcient variant of the EXP4 algorithm for periodic settings.
With K arms, T time steps, and where each periodic pattern in F is of length at most P , we show
that the periodic regret obtained by Periodic EXP4 is at most O(cid:0)pP KT log K + KT log |F |(cid:1). We
also prove a lower bound of Ω(cid:0)q
(cid:1) for the periodic setting, showing that this is
P KT + KT log |F |
log K
optimal within log-factors. As an example, we focus on the wireless network selection problem.
Through simulation, we show that Periodic EXP4 learns the periodic pattern over time, adapts to
changes in a dynamic environment, and far outperforms EXP3.

2012 ACM Subject Classiﬁcation Theory of computation → Online learning algorithms

Keywords and phrases multi-armed bandits, wireless network selection, periodicity in environment

Supplement Material Source code: https://github.com/Ohohcakester/PeriodicEXP4-Source

Acknowledgements This project was funded by Singapore Ministry of Education Grant MOE2018-
T2-1-160 (Beyond Worst-Case Analysis: A Tale of Distributed Algorithms).

9
1
0
2

r
p
A
8
2

]
I

N
.
s
c
[

1
v
5
5
3
2
1
.
4
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
2

Periodic Bandits and Wireless Network Selection

1

Introduction

The multi-armed bandit problem is an online learning problem in which a player has access to
a set of choices (i.e., “arms”) each of which provides some reward (i.e., “gain”). At each time
step, the player chooses an arm and gets some reward. In stochastic variants, rewards are
determined by some probabilistic distribution. In adversarial variants, an adversary speciﬁes
the rewards. Amazingly, even when rewards are adversarially chosen, the player can do fairly
well! For example, the EXP3 algorithm [6] minimizes the player’s “regret”, ensuring that
the player does almost as well as if she had selected the single ﬁxed best arm throughout.
Another fascinating property of bandit algorithms is that they work well in multi-player
settings [27, 16], converging to close variants of a Nash equilibrium.

Recently, it has been shown that bandit-style algorithms can eﬃciently solve the wireless
network selection problem, yielding good performance both in theory and in practice [1, 2, 7].
In this problem, each user has access to a collection of networks (e.g., a few diﬀerent WiFi
networks and a 4G connection); the goal is to pick networks with higher data rates. Selecting
the best network is challenging, especially in dynamic environments where the “best” network
changes over time, as users move and network bandwidth ﬂuctuates. This can be modeled as
an adversarial bandit problem and solved with EXP3 and its variants.

Bandit algorithms have one major weakness in dynamic settings (such as wireless network
settings): they are designed to learn the average payoﬀ of each arm, and to converge to the
arm that provides the best average performance. In the stochastic case, this is exactly what
you want. In the adversarial case, it leads to minimum regret, i.e., the user does almost as
well as if they knew the best network in advance. If, however, the situation is changing over
time, and especially if it is changing in some predictable manner, then learning the average
payoﬀ of each arm is not productive.

Periodic, repetitive patterns are a particularly common type of dynamic behavior. Take,
for example, the problem of network selection. Network behavior is often repetitive, with user
density and network quality following regular patterns: for example, oﬃce WiFi networks
have no users at night, their performance drops when workers arrive in the morning, and the
performance improves again during lunch hour. Other networks are clogged with streaming
video during lunch hour and in the evenings. Periodic patterns are ubiquitous.

Unfortunately, bandit algorithms will fail badly in the case of periodic behavior. As an
example, suppose a player is playing a slot machine with two arms. The ﬁrst arm gives a
reward of 1 when pulled on odd-numbered hours and 0 otherwise, while the second arm does
the reverse, with a reward of 1 on even-numbered hours and 0 otherwise. In this simple case,
a bandit algorithm will never learn this pattern, instead converging to the best single-action
policy; and the best policy can only reap half of the maximum reward. The player will
receive an average payout of only 1/2 per selection, despite a very predictable pattern. And
when this case is extended to cycle among K arms, the best ﬁxed choice of arm gives only
1/K of the total obtainable reward. Thus, algorithms like EXP3 that minimize the regret do
not guarantee good performance on periodic problems.

1.1 Contributions

Our goal in this paper is to develop an eﬃcient adversarial bandit algorithm for periodic
settings, and to demonstrate the eﬀectiveness of this algorithm in the context of the wireless
network selection problem, yielding a new approach to network selection in dynamic, periodic
environments. The ﬁrst step is to establish the right metric by which to evaluate bandit
algorithms. The performance of an adversarial bandit algorithm is heavily characterized by

S. Oh, A. Meetoo Appavoo and S. Gilbert

3

the deﬁnition of “regret,” which forms the baseline that it competes against. And traditionally,
the regret is computed with respect to the best ﬁxed strategy.

For the periodic bandit setting, we deﬁne a better performance measure, ‘periodic regret’,
which compares an algorithm’s performance against the best periodic choice of arms. No
choice of period may match the input data perfectly, but the goal of periodic regret is to
compare against the best choice. Moreover, we provide a generalized notion of periodicity, so
that this notion of periodic regret can capture diﬀerent types of patterned behavior.

Next, we develop an algorithm that minimizes periodic regret, Periodic EXP4, a com-
putationally eﬃcient variant of EXP4 (Exponential-weight algorithm for Exploration and
Exploitation using Expert advice) [6]. We show that the algorithm minimizes periodic
regret in the following sense: with K arms, |F | possible periods, with each possible period
of at most length P , then in an execution of length T the periodic regret is at most
O(cid:0)pP KT log K + KT log |F |(cid:1). We also prove a lower bound of Ω(cid:0)q
(cid:1) on
periodic regret in an adversarial setting, showing that this is optimal within log-factors. An
important aspect of Periodic EXP4 is that it is a polynomial time algorithm: we leverage the
structure provided by the target periodic patterns to reduce the computational complexity.
This is in contrast to EXP4 which requires exponential time and space in this context.

P KT + KT log |F |
log K

The other major contribution of this paper is a new algorithm for network selection that
is especially optimized for environments with periodic, patterned behaviors. We simulate
the network selection problem, comparing Periodic EXP4 to EXP3 and to a “randomized
optimal” omniscient solution. (We have previously seen in [1] that these types of simulations
are reasonably predictive of real-world behavior.)

Our ﬁrst observation is that Periodic EXP4 does in fact eﬃciently learn periodic patterns
and adapts relatively quickly to changes in network data rates (both discrete and continuous).
We also see that Periodic EXP4 does indeed outperform EXP3 in periodic settings, as
expected, potentially yielding signiﬁcant real-world improvements.

Our second question involved the robustness of Periodic EXP4 to noisy patterns. Real-
world periodic patterns are rarely perfectly periodic, suﬀering noise and variance. We
experiment with noisy patterns, and see that Periodic EXP4 continues to work well.

Finally, our third set of experiments looked at the performance of Periodic EXP4 in the
context of user mobility. We simulate several scenarios where users change location over
time, leading to changes in which networks they can access (and hence changes in the load
on those networks). For example, we imagine a typical oﬃce scenario where users arrive at
the oﬃce in the morning, take a break for lunch, return to work, and then head home at the
end of the day. We observe that Periodic EXP4 can also learn this type of periodic behavior,
again, learning to adapt the users’ network selection in a near-optimal fashion. In fact, we
compare two versions of the algorithm: one in which the algorithm is notiﬁed when networks
become unavailable, and one in which it is not—we observe that even in the latter case where
it is completely oblivious to the changes, the user strategy converges to near-optimal choices.
Overall, we conclude that periodic adversarial bandit algorithms may have signiﬁcant
value, that Periodic EXP4 is an eﬃcient algorithm for the problem, and that it yields a
potentially interesting and useful approach to network selection.

2

Related work

In this section, we discuss relevant work done on bandit algorithms, and state-of-art wireless
network selection approaches. Multi-armed bandit techniques have been successfully applied
to wireless network selection [1, 2, 7]. They have also been considered for other resource

4

Periodic Bandits and Wireless Network Selection

selection problems, such as channel selection [13, 27], selection of the right sensors to query
in a sensor network [14], and selection of replica server for content distribution networks [28].
Many variations of bandit problems have been studied, in both stochastic and adversarial
settings. EXP3 is the most well-known algorithm for the standard adversarial bandit problem.
With K arms and T time steps, it establishes a pseudo-regret upper bound of O(
KT log K),
which almost matches the lower bound of Ω(
KT ) [6]. The log K gap in the bounds has
KT ). But, these bound
been recently closed by [5] bringing the upper bound down to O(
the regret against the best single-action policy, limiting their usefulness in a periodic setting.
A related problem is that of bandits with expert advice, deﬁned in the same paper [6].
It deﬁnes a more general notion of regret, by competing against the best policy from a set.
With K arms, T time steps and N experts, the EXP4 algorithm gives a pseudo-regret bound
of O(
KT log N ). However, its possibly high running time and memory cost limit its use in
practice. There are other algorithms for bandits with expert advice, like Context-FTPL. The
latter is more computationally eﬃcient, but has a weaker regret bound [26]. A lower bound

√

√

√

√

q

of Ω(

KT log N

log K ) [23] has been shown, but the log K gap in bounds has not been closed.

An equivalent formulation of our generalized periodic regret (explained later in Section
4.2) has been brieﬂy discussed in [10, Chapter 4.2.1], phrased as a contextual bandit problem
where the algorithm competes against the best context set from a class of context sets. The
possible use of EXP4 is mentioned, but an alternative algorithm with a weaker regret bound
is instead discussed as it has a reasonable polynomial-time performance unlike EXP4.

While much of the existing literature assume a single best arm, there are other eﬀorts to
look beyond this. One approach to the stochastic version of the problem is to allow reward
distributions of the arms to occasionally change [9, 22]. Our work on the other hand is fully
adversarial, and makes no assumptions on the rewards produced by the adversary.

Numerous wireless network selection approaches have been proposed. Some are centralized
[3, 8, 18, 25]; hence, not scalable and limited to managed networks. A number of distributed
approaches have been proposed, with various limitations. Some rely on coordination from
networks [15], while others require cooperation of wireless devices [12]. Others assume
global knowledge [20, 4, 19], or availability of some information [30, 11]. A continuous-time
multi-armed bandit approach in a stochastic setting has been considered in [29]. A similar
setting to ours, though non-periodic and in the stochastic setting, is considered in [7].

3 Wireless Network Selection

Here, we describe the wireless network selection problem, discuss the periodicity of events in
wireless environments, and formulate the network selection problem as a bandit problem.

3.1 Wireless network selection problem.

We consider an environment with multiple wireless devices and heterogeneous wireless
networks, such as the one depicted in Figure 1. The latter illustrates four mobile users with
their (active) mobile devices, and ﬁve wireless networks, namely four WiFi networks and
a cellular network (represented using 3 cellular base stations). The wireless networks have
limited areas of coverage. Hence, each mobile device may have access to a diﬀerent set of
wireless networks depending on their location, e.g. diﬀerent networks are available at home
and at the oﬃce. The bandwidths of wireless networks may also vary with time. Each mobile
device aims to quickly identify and associate with the best network, which may vary over
time, to maximize their data rates.

S. Oh, A. Meetoo Appavoo and S. Gilbert

5

Figure 1 Mobile devices with access to a diﬀerent set of wireless networks as the user moves.

Mobile users tend to have daily routines that follow repetitive patterns—going to the oﬃce
each morning, lunch at noon, returning home in the evening; these activities are performed
at ﬁxed times each weekday. Figure 1 broadly depicts the daily routine of a mobile user,
Alice. Network behavior, which is aﬀected by user density, is also often repetitive and follows
a regular pattern. For example, the available bandwidth of oﬃce WiFi networks is likely to
be higher during lunch hours, where the oﬃce is nearly empty. A good network selection
protocol learns and adapts to periodic patterns in network quality for better performance.

3.2 Wireless network selection as a bandit problem

A device must be aware of the bit rate it can observe from each network to perform an
optimal network selection. While this information is unknown at the time of selection, the
device can estimate the achievable bit rate by exploring the networks. The network selection
problem can be seen as a multi-armed bandit problem in a multi-player setting. A mobile
device is a player, and each network can be considered as an arm. Every so often (e.g. once
per minute), a device selects a network (analogous to pulling an arm) and observes a bit rate
(gain) for that network. The gain from other networks is unknown to the device. Given that
mobile devices operate in a dynamic environment, they must continuously explore and adapt
to changes, by deciding which networks to select in sequence. The goal of each device is to
maximize its cumulative gain over time. Since the quality of a wireless network is aﬀected
by its number of clients, other mobile devices in the environment may be considered to be
adversaries. We hence use the adversarial setting. A leading bandit algorithm in this setting
is EXP3.

4

Periodic Bandit Problem

In this section, we introduce the periodic bandit problem and discuss periodic regret.

We consider a general bandit problem. On each time step, an algorithm is allowed to
pick any one out of K possible arms, and each arm produces a certain amount of reward.
These rewards are unknown to the algorithm, which can only observe the reward of the arm
it picked. We aim to maximize the total reward obtained by the algorithm. We study the
adversarial setting with a possibly adaptive adversary, which decides on the distribution of
rewards at each time step, taking into consideration the outcomes of past random events.

Let K be the number of arms. The set of arms is [K] := {1, 2, · · · , K}. Let xi(t) ∈ [0, 1]
be the reward earned by arm i ∈ [K] at time step t. Let a(t) ∈ [K] be the arm played by
the algorithm at time t. Let T be the total number of time steps. The set of time steps
is [T ] := {1, 2, · · · , T }. Thus, the total reward earned by the algorithm after T iterations
is PT
t=1 xa(t)(t). The commonly used performance measure for bandit algorithms is regret.
Regret compares the total reward obtained by the algorithm against a “best possible” reward

Alice's homeAlice's workplaceIEEE 802.11 WLANCellular base station Mobile usersAlice6

Periodic Bandits and Wireless Network Selection

“OPT” after some number of time steps T . Diﬀerent types of regret compare the algorithm’s
result to diﬀerent notions of the optimal result.

We can deﬁne a form of regret where OPT is allowed to pick any arm in [K] at each time

step. For later reference we will refer to this as full regret, deﬁned as follows:

Rf ull(T ) =

T
X

t=1

max
i∈[K]

h

i
xi(t)

E

− E

h T
X

t=1

i
xa(t)(t)

The above deﬁnition uses what is commonly known as pseudo-regret, rather than expected
regret. For the rest of this paper, we will often refer to pseudo-regret as simply “regret”.
Expectations are taken over the possible randomness of the algorithm and adversary.

In most studies of adversarial bandits, a weaker deﬁnition of regret is used. This is
because full regret uses too powerful an adversary, and it is impossible to achieve better than
linear expected full regret in the worst case (we include a proof in Appendix A.1). Therefore,
it is common to deﬁne a notion of regret where OPT is required to use the same arm for all
T time steps. We refer to this as weak regret, deﬁned as follows:

Rweak(T ) = max
i∈[K]

T
X

t=1

h
xi(t)

i

E

− E

h T
X

t=1

i
xa(t)(t)

Weak regret however, severely limits what OPT can do, and being competitive with an

algorithm that can only pick one arm and stick to it may not be a very strong result.

4.1 Periodic Regret

We can bridge the two with a periodic deﬁnition of regret. Taking the idea that a periodic
choice of arms is likely to perform well in situations with periodic patterns, we can deﬁne
a regret function which measures how competitive an algorithm is with the best periodic
choice of arms. For example, we can say OPT is forced to play the same arm every τ ∈ N
steps. This deﬁnes a regret function as follows,

Rτ (T ) =

τ
X

‘=1

max
i∈[K]

b T −‘
τ c
X

t=0

h

i
xi(tτ + ‘)

E

− E

h T
X

t=1

i

xa(t)(t)

As OPT may optionally still pick the same arm on all time steps, this is a generalization of
weak regret. This makes for a regret value in between weak regret and full regret.

If we were competing against the regret for a speciﬁc, known value of τ , this would
be equivalent to playing τ independent instances of the adversarial bandits problem over
approximately T /τ time steps each. By playing τ separate instances of an algorithm for
τ KT ).
weak regret, and by Theorem 2 in Section 6.1, we have an upper/lower bound of Θ(
However, if we were to consider that the “best possible” period τ may not be known (for
example, if OPT were to consist of the best periodic function for any of the possible periods
τ ∈ {1, · · · , P }), these bounds do not apply as easily.

√

4.2 Generalized Periodic Regret

A generalization of the periodic case is the use of partition functions. Fix a maximum number
of labels P . We deﬁne this upper bound P for use in our analysis later on. A partition
function f : [T ] → [P ] is a function that assigns every time step a label from 1 to P . We
consider two partition functions the same if their choice of label assignments are permutations

S. Oh, A. Meetoo Appavoo and S. Gilbert

7

of each other. The regret under function f would be when OPT is forced to play the same
arm for all timesteps with the same label as assigned by f .

Rf (T ) =

X

‘∈[P ]

max
i∈[K]

X

h
xi(t)

i

E

− E

t∈f −1(‘)

h T
X

t=1

i
xa(t)(t)

(1)

Consider a set of partition functions F ⊆ {f : [T ] → [P ]} for some P ∈ N. F is necessarily
ﬁnite. The regret under the function set f would be when OPT can choose to play using any
of the partition functions in F . This gives the following regret deﬁnition:

RF (T ) = max
f ∈F

X

‘∈[P ]

max
i∈[K]

X

h

i
xi(t)

E

− E

t∈f −1(‘)

h T
X

t=1

i
xa(t)(t)

(2)

This deﬁnition (2) of periodic regret gives us more choice in how we want to deﬁne our
potential periodic patterns to learn, through deciding on the labels on each time step for
each function. We demonstrate this with our choice of partition functions in Section 7.

To model the example described earlier with periods τ ∈ {1, 2, · · · , P }, we can use the
set of partitions F = {f1, f2, · · · , fP }, where fτ (t) := (t mod τ ) + 1 for each t ∈ [T ], τ ∈ [P ].

5

The Periodic EXP4 Algorithm

We discuss the relationship between our generalized periodic setting and the problem of
bandits with expert advice, and hence the applicability of EXP4 [6] to the problem. We use
this to introduce Periodic EXP4, an eﬃcient algorithm for generalized periodic regret.

5.1 Applying Bandits with Expert Advice to Periodic Bandit Problems

Periodic bandit problems can be reduced to the problem of bandits with expert advice. In
the problem of bandits with expert advice, we are given a set Π of N experts. Each expert
predicts an arm on each time step. We ﬁx the number of time steps T . Thus an expert can
be seen as a function π : [T ] → [K]. An algorithm to solve this problem would make use of
each expert’s predictions on each time step, to obtain a reward competitive with the best
expert in the set. This gives us the following regret deﬁnition:

RΠ(T ) = max
π∈Π

T
X

t=1

xπ(t)(t) − E

h T
X

t=1

i

xa(t)(t)

This can be used to model all of the above notions of regret. For full regret, we have
Π := {π : [T ] → [K]}, the set of all possible functions from [T ] to [K]. For weak regret, Π is
the set of all constant functions from [T ] to [K].

In the generalized periodic setting, let F be the set of partition functions f : [T ] → [P ].
For each function f ∈ F , let Θf be the set of all possible mappings θ : f ([T ]) → [K] from
the image set f ([T ]) of f to the set of arms [K] (thus |Θf | = K |f ([T ])|). Each composition
θ ◦ f , f ∈ F , θ ∈ Θf thus represents a possible mapping of the time steps [T ] to arms. Thus,
for the generalized periodic setting, Π = {θ ◦ f | f ∈ F, θ ∈ Θf }.

We note that when Π1 ⊆ Π2, we will have RΠ1 (T ) ≤ RΠ2(T ). Let Πf ull, Πweak and ΠF
be the sets of functions corresponding to full regret, weak regret and generalized periodic
regret under some function set F respectively. Thus, for any nonempty set F of partition
functions, we have RΠweak (T ) ≤ RΠF (T ) ≤ RΠf ull (T ).

8

Periodic Bandits and Wireless Network Selection

√

An existing algorithm for this problem is the EXP4 algorithm [6], which achieves a regret
upper bound of O(
KT log N ), where N := |Π|. We can thus apply EXP4 directly to our
problem. However, a commonly cited drawback of the EXP4 algorithm is that its running
time and memory cost are at least linear in N . This is an issue as N is often very large. For
example, in the generalized periodic setting, the size of N could easily be on the order of
|F |K P , which is exponential in P . However, we show below that in the generalized periodic
setting, we can devise an algorithm that is distributionally equivalent to EXP4 and can be
made to run in time polynomial in |F |, K and P .

The EXP4 algorithm works by assigning a weight wπ (with initial value 1) to each expert
π ∈ Π. The probability pi(t) of playing an arm i ∈ [K] would then be P
π wπ(t),
the ratio of the combined weights of the experts agreeing to play arm i to the total weight of
the experts. Whenever an arm i ∈ [K] is played, each expert who suggested arm i will have
their weight adjusted by some factor exp( γ
K xi(t)/pi(t)). More details on EXP4 are given in
[6]. Note that it discusses a more general form of expert advice where each expert suggests a
probability vector on the arms. However, we only require the case where at each time step,
each expert suggests one arm with probability 1, and all other arms with probability 0.

π(t)=i wπ(t)/P

5.2 Periodic EXP4, Memory and Running Time Costs

Periodic EXP4 (Algorithm 1) is distributionally equivalent to the EXP4 algorithm when run
with the set of experts Π = {θ ◦ f | f ∈ F, θ ∈ Θf }. The key intuition behind this algorithm
is that the generalized periodic setting produces many symmetries in the weight computation
for each expert. Speciﬁcally, we take advantage of how for each partition function f , the set
of experts contains every possible combination of arm assignments to labels in the image set
f ([T ]). This allows us to compute the probabilities that EXP4 would play each arm at each
time step without computing the individual weights of every expert.

For brevity, let Pf := |f ([T ])| be the number of labels used by the function f . Necessarily
f ∈F Pf ), which is at most O(KP |F |). A naive
f ∈F Pf ) per time step, but

Pf ≤ P . The memory requirement is O(K P
implementation of the algorithm gives a running time of O(K 2 P
with some pre-computation, the running time can be lowered as shown in Appendix A.2.

5.3 Correctness of Periodic EXP4

To show correctness, we show that our algorithm produces the same probability distribution
over arms as EXP4 in every time step. Deﬁne πθ,f as the expert which at time t recommends
arm θ ◦ f (t) with probability 1 and all other arms with probability 0. We show this algorithm
is distributionally equivalent to EXP4, where Π = {πθ,f |f ∈ F, θ ∈ Θf }. In EXP4, each
expert πθ,f would have some weight wθ,f (t) at time step t. At time step t, EXP4 plays arm
i with probability pi(t) represented by the following expression:

pi(t) =

P

f ∈F,θ∈Θf ,θ◦f (t)=i wθ,f (t)
P

wθ,f (t)

f ∈F,θ∈Θf

Thus, to show that the two algorithms are distributionally equivalent, as pi(t) := ri(t)/PK
in our algorithm, for each successive time step t, we only need to show the following:

j=1 rj(t)

ri(t) =

X

wθ,f (t)

f ∈F,θ∈Θf ,θ◦f (t)=i

S. Oh, A. Meetoo Appavoo and S. Gilbert

9

Algorithm 1 Periodic EXP4
1: procedure Initialization
for each f ∈ F do
2:

3:

4:

for each ‘ ∈ f ([T ]) do
for each i ∈ [K] do
Initialize b‘,f

(1) = 1

5:
i
6: procedure Algorithm
7:

for each time step t = 1, 2, · · · , T do

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

for each i ∈ [K] do

ri(t) :=

X

(cid:16)

f ∈F

bf (t),f
i

(t)

Y

K
X

(cid:17)
b‘,f
j (t)

‘∈f ([t])\{f (t)}

j=1

for each i ∈ [K] do

pi(t) =

ri(t)
j=1rj(t)

PK

Play arm it ∈ [K] from the probabilities p1(t), p2(t), · · · , pK(t)
Obtain reward xit(t)
for each f ∈ F do

for each ‘ ∈ f ([T ]) do
for each i ∈ [K] do

if i = it and ‘ = f (t) then
(t + 1) = b‘,f

(t) exp( γ

b‘,f
i

i

K xi(t)/pi(t))

else

b‘,f
i

(t + 1) = b‘,f

i

(t)

The details of this derivation is given in Appendix A.3. We can thus formally state a regret
upper bound as follows (Theorem 1). This upper bound comes directly from EXP4’s regret
bound of O(

KT log N ), where the number of experts N =

K |f ([T ])| ≤ |F |K P .

X

√

(cid:73) Theorem 1. With K arms, T time steps, |F | partition functions, with every function having
at most P labels, Periodic EXP4 gives a regret upper bound of O(cid:0)pP KT log K + KT log |F |(cid:1).

f ∈F

6

Lower Bounds

In this section, we provide lower bounds for the case of a single partition and for a set of
partitions. We demonstrate that the upper and lower bounds diﬀer by a factor of log K.

KT log N
log K

(cid:1). This lower bound is derived by dividing the time steps [T ] into log N

The existing regret lower bound for the problem of bandits with expert advice [23] is
Ω(cid:0)q
log K equal
parts. For the generalized periodic setting, as this lower bound uses an instance that can be
modeled with a single partition function, it does not give immediate insight into whether
having multiple diﬀerent periods or partition functions increases the diﬃculty of the problem.

6.1 Lower Bound for a Single Partition

We consider the case with only a single partition function f : [T ] → [P ], which partitions the
time steps into P labels 1, 2, · · · , P . The sizes of the partitions are |f −1(1)|, |f −1(2)|, · · · ,
|f −1(P )| respectively. It seems like intuitively, by seeing this as P separate instances of the

10

Periodic Bandits and Wireless Network Selection

weak regret setting, and by the existing Θ(
we would have an upper/lower bound of Θ(PP
of size approximately T

P each, this bound would be Θ(

‘=1

√

P KT ).

KT ) upper/lower bounds on weak regret [6, 5],
pK|f −1(‘)|). For equally sized partitions

√

However, while the upper bound is clearly met by running P independent instances of
an algorithm for weak regret, the lower bound is less clear. Even when considering it as P
separate instances, there is a possibility of an algorithm “reacting” to losses in other instances
to play diﬀerently in the current instance, obtaining a higher total reward as a result. For
completeness, we include a proof for the lower bound (Theorem 2) in Appendix A.4

(cid:73) Theorem 2. Fix a partition function f : [T ] → [P ] which assigns a label to each time step.
Assume that for each ‘ ∈ f ([T ]), there are at least K/(4 ln 4
3 ) time steps with label ‘. Then
the minimax pseudo-regret (1), over all algorithms a and adversaries R, has a lower bound
as follows, for some positive constant c:

inf
a

sup
R

(cid:16)

max
θ∈Θf

E

h X

i
xθ◦f (t)(t)

− E

h X

i(cid:17)

xa(t)(t)

X

pcK|f −1(‘)|

≥

t∈[T ]

t∈[T ]

‘∈[P ]

If we consider the simple case where OPT may play only periodic functions from any period
τ ∈ {1, 2, · · · , P }, it can do no worse than if it were only allowed to play at period P . We
thus obtain a lower regret bound of

P KT .

√

6.2 Lower Bound for the Generalized Periodic Setting

Let F be the set of partitions, so |F | is the number of partitions. Let P be the maximum
number of labels of any partition in F . For suﬃciently large T and K ≤ P , we obtain a

pseudo-regret(2) lower bound of Ω(

P KT +

KT log |F |

log K ). It is proved in Appendix A.5

If P < K instead, a simple lower bound can be obtained by using only P out of the K
arms, so we obtain a problem with P arms and maximum partition size P . This gives us a

√

q

lower bound of Ω

. We can then merge these two lower bounds into

(cid:16)√

P KT +
(cid:16)√

q

(cid:17)

P T log |F |
log P

q

a single expression Ω

P KT +

min(P, K)T

log |F |
log min(P,K)

(cid:17)

.

6.3 Analysis of Bounds

√

A conclusion we can make from Section 6.2 is that having multiple periods indeed increases
the diﬃculty of the problem - we have obtained a lower bound higher than the known upper
P KT ) had only one partition function of the maximum period P been used.
bound of O(
With K arms, T time steps, |F | partition functions, with every function having at most
P labels, Periodic EXP4 gives an upper bound of O(cid:0)pP KT log K + KT log |F |(cid:1). On the
other hand, we have a lower bound of Ω(cid:0)q
(cid:1) in the case where K ≤ P .
√
This gives a gap of
log K between the two bounds. Interestingly, this log-factor is the
same as the current gap between the upper and lower bounds in the problem of bandits with
expert advice. This is possibly because we use a similar lower bound proof to the problem of
bandits with expert advice [23], as well as a similar algorithm for the upper bound.

P KT + KT log |F |
log K

7

Experimental Evaluation

In this section, we discuss the implementation details of Periodic EXP4 and parameter values
chosen, evaluate the algorithm via simulation, and compare its performance to EXP3 [6].
We show how Periodic EXP4 (a) learns periodic patterns over time under both discrete and

S. Oh, A. Meetoo Appavoo and S. Gilbert

11

continuous changes in network data rates, (b) outperforms EXP3, (c) is robust to noisy
patterns, and (d) adapts to changes due to mobility of users.

We benchmark against “Optimal Random”, a player with prior knowledge of the actual
bandwidths of each network.
In each time slot, it picks a network from a probability
distribution equal to the ratios of the bandwidths. For example, with network bandwidths
4, 10 and 6, the probability of picking the networks will be 0.2, 0.5 and 0.3, respectively.

All the algorithms are implemented in Python, using SimPy [24], while the core algorithm
is written in C++. We use a time-varying learning rate γ = t− 1
10 [17] for both Periodic
EXP4 and EXP3; γ slowly tends to zero to ensure convergence [27] while at the same
time ensures that the algorithm does not take too long to learn (it learns slowly when γ
is very small). Although they are not pre-requirements of Periodic EXP4, for simplicity,
we assume that (a) a network’s bandwidth is equally shared among its clients, and (b)
devices are time-synchronized. To reduce numerical error in our simulations, we substitute
computations of P
x∈Y exp(x) with exp(maxx∈Y x). In nearly all cases, sums of exponentials
in our algorithm are heavily dominated by a single term, making the values of the two
expressions approximately equal. Experimentally, we ﬁnd that this has negligible eﬀects on
the values computed within the algorithm.

We do simulations on synthetic data. We consider setups with 20 mobile devices and 3
wireless networks, unless otherwise speciﬁed. While the number of devices remain constant
throughout the simulation run, the data rates and availability of networks may change. We
assume that a network selection is performed once every minute; hence, 1440 time slots is
one simulated day. All results presented are from 20 simulation runs, of 86,400 time slots
each (i.e., 2 simulated months). The pattern of network behavior and/or user mobility over
the ﬁrst 1440 time slots is repeated 60 times; we refer to each repetition as an ‘iteration’.

We apply Periodic EXP4 in the generalized periodic setting. We deﬁne a partition
function of period τ as one which divides each iteration of 1440 time slots into τ equal
contiguous segments, labeled 1 to τ in chronological order. The same labels are used for each
successive repetition. Unless otherwise speciﬁed, we use the period set {1, · · · , 24}. This
refers to using 24 partition functions, of periods 1 to τ respectively.

7.1 Evaluation Criteria

Good assignments of devices to networks divide the available bandwidth evenly among the
devices. We thus evaluate the performance of the algorithms based on the lowest data rate
observed by any of the devices. We compare this to the optimal allocation of devices, which
maximizes the lowest data rate observed by any device. If a device with the lowest data rate
observes 3Mbps, but the optimal’s lowest is 5Mbps, we say it loses 40% of its achievable gain.
We refer to this percentage loss as the “distance to optimal minimum” in our results.

We do not use average cumulative gain as a performance measure because in our problem

setting, average gain is maximized as long as there is at least one user in each network.

7.2 Performance Comparison of Algorithms

We consider two setups, both at an oﬃce with two WiFi networks and a cellular network.
The data rates of these networks vary over time. The ﬁrst setup involves discrete changes
in network bandwidths at ﬁxed time intervals (Figure 2a). In the second setup, the data
rates vary continuously with time (Figure 2b). Figures 3a and 3b show that in both setups,
the distance to optimal minimum of Periodic EXP4 drops over time while EXP3 shows no
noticeable improvement with time.

12

Periodic Bandits and Wireless Network Selection

(a) Discrete changes in network data rates.

(b) Continuous changes in network data rates.

Figure 2 Changes in network data rates over one iteration (this is repeated 60 times).

(a) Performance under discrete setup.

(b) Performance under continuous setup.

Figure 3 Distance to optimal minimum of Periodic EXP4 and EXP3 over 60 iterations.

(a) EXP3: Combined probabilities for each network over ﬁrst 10 iterations.

(b) Bandwidth ratio

(c) Periodic EXP4: Combined probabilities for each network over ﬁrst 10 iterations.

Figure 4 Area chart showing the time variation of combined probabilities in the continuous setup.
Figure 4b shows the actual ratio of the bandwidths of the three networks within any one iteration.

Figure 4 for the continuous setup explains this improvement. The ﬁgure for the discrete
setup is in Appendix B.1. At each time step, each user has a probability of picking each of
the networks. If we consider the combined probability of picking each network, we can see
that in Periodic EXP4, these probabilities converge towards the ratios of the bandwidths of
the networks (Figures 4c). This is despite the continuous setup having no obvious best period.
On the other hand, EXP3’s probabilities slowly ﬂatten out (Figure 4a). This is consistent
with what we would expect, as EXP3 seeks to be competitive with the best ﬁxed-action
policy, meaning that it only seeks out the best ﬁxed arm to play.

Figure 5 shows that while EXP3 initially learns more quickly, Periodic EXP4 eventually

13607201,0801,44002040TimeslotDatarate(Mbps)CellularOﬃceWiFi1OﬃceWiFi213607201,0801,4400204060TimeslotDatarate(Mbps)CellularOﬃceWiFi1OﬃceWiFi211020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.RandomEXP311020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.RandomEXP301,4402,8804,3205,7607,2008,64010,08011,52012,96014,40000.20.40.60.81TimeslotProbabilityCellularOﬃceWiFi1OﬃceWiFi211,4400204060TimeslotDatarate(Mbps)01,4402,8804,3205,7607,2008,64010,08011,52012,96014,40000.20.40.60.81TimeslotProbabilityCellularOﬃceWiFi1OﬃceWiFi2S. Oh, A. Meetoo Appavoo and S. Gilbert

13

Figure 5 Distances to Optimal minimum in the ﬁrst and last repetitions of the discrete setting

in Figure 2a. Vertical lines indicate points where data rates change.

outperforms EXP3 (which converges to the network with the best average performance),
with a performance similar to Optimal Random. From our experiments, we ﬁnd that while
all algorithms have similar total cumulative gains, we may note that Periodic EXP4 is fairer
than EXP3, with signiﬁcantly lower variance. We present these results in Appendix B.4.

7.3 Other Experiments

In Appendix B, we discuss a few more experiments, the results of which are brieﬂy summarized
as follows:
1. Performance in Noisy Settings: On each time step, we apply a 10% Gaussian noise
to each of the networks’ data rates. We ﬁnd that our algorithms are largely unaﬀected
by noise in the data, giving similar results with and without noise.

2. Comparison of Period Sets: We do a comparison between diﬀerent possible period
sets F . We ﬁnd that the algorithm learns more slowly with larger period sets (e.g.
{1, 2, · · · , 45}, as compared to {1, 2, · · · , 15}), but can converge to better results on more
complex instances (instances where the bandwidth may ﬂuctuate more wildly).

3. Mobility of Users: We consider a setup where users move around and have access to
diﬀerent sets of networks at diﬀerent times. We compare Vanilla Periodic EXP4, which
is oblivious to networks possibly becoming unavailable, against an optimized version,
which selects only from the set of currently available networks. While the optimized
version initially yields a better performance, they eventually perform equally well when
the Vanilla Periodic EXP4 algorithm learns the pattern.

8

Conclusion

In this paper, we develop an eﬃcient variant of EXP4 for the periodic bandit problem, give
nearly matching upper and lower bounds for it, and demonstrate its advantages in learning
periodic behavior in the context of the network selection problem.

An interesting issue raised in contrasting this paper and [9, 22] is whether non-stationary
bandit problems are better modeled stochastically or adversarially. While these papers
address non-stationary rewards primarily in a stochastic setting with some adversarial
aspects, we tackle the periodic bandit problem in a fully adversarial setting. Using the
adversarial setting has the beneﬁt of not placing any constraints on the adversary; we adapt to
the periodic setting only through our deﬁnition of regret. A proper comparison of stochastic
and adversarial methods for network selection is a possible future line of work.

136072010801440020406080Timeslot(Iteration1)DistancetoOpt.Min(%)8496185320856808604086400Timeslot(Iteration60)PeriodicEXP4EXP3OptimalRandom14

Periodic Bandits and Wireless Network Selection

References

1 Anuja Meetoo Appavoo, Seth Gilbert, and Kian-Lee Tan. Shrewd selection speeds surﬁng: Use
smart exp3! In 2018 IEEE 38th International Conference on Distributed Computing Systems
(ICDCS), pages 188–199. IEEE, 2018.

2 Anuja Meetoo Appavoo, Seth Gilbert, and Kian-Lee Tan. Cooperation speeds surﬁng: Use

3

4

5

6

co-bandit! arXiv preprint arXiv:1901.07768, 2019.
E. Aryafar, A. Keshavarz-Haddad, C. Joe-Wong, and M. Chiang. Max-min fair resource
allocation in hetnets: Distributed algorithms and hybrid architecture. In ICDCS, 2017, pages
857–869. IEEE, 2017.
E. Aryafar, A. Keshavarz-Haddad, M.l Wang, and M. Chiang. Rat selection games in hetnets.
In INFOCOM, pages 998–1006. IEEE, 2013.
Jean-Yves Audibert and Sébastien Bubeck. Minimax policies for adversarial and stochastic
bandits. In COLT, pages 217–226, 2009.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed
bandit problem. SIAM Journal on Computing, 32(1):48–77, 2002.

7 O. Avner and S. Mannor. Multi-user lax communications: A multi-armed bandit approach.
In IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer
Communications, pages 1–9, April 2016. doi:10.1109/INFOCOM.2016.7524557.

8 Y. Bejerano, S-J. Han, and L. E. Li. Fairness and load balancing in wireless lans using

association control. In MobiCom, pages 315–329. ACM, 2004.

9 Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem
with non-stationary rewards. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence,
and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27,
pages 199–207. Curran Associates, Inc., 2014. URL: http://papers.nips.cc/paper/
5378-stochastic-multi-armed-bandit-problem-with-non-stationary-rewards.pdf.
Sébastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends® in Machine Learning, 5(1):1–122,
2012.

10

11 M. H. Cheung, F. Hou, J. Huang, and R. Southwell. Congestion-aware distributed network

12

selection for integrated cellular and wi-ﬁ networks. arXiv preprint arXiv:1703.00216, 2017.
S. Deng, A. Sivaraman, and H. Balakrishnan. All your network are belong to us: A transport
framework for mobile network selection. In HotMobile. ACM, 2014.

13 Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. Learning multiuser channel allocations in
cognitive radio networks: A combinatorial multi-armed bandit formulation. In New Frontiers
in Dynamic Spectrum, 2010 IEEE Symposium on, pages 1–9. IEEE, 2010.

14 D. Golovin, M. Faulkner, and A. Krause. Online distributed sensor selection. In Proceedings of
the 9th ACM/IEEE International Conference on Information Processing in Sensor Networks,
pages 220–231. ACM, 2010.

15 B. Kauﬀmann, F. Baccelli, A. Chaintreau, V. Mhatre, K. Papagiannaki, and C. Diot.
Measurement-based self organization of interfering 802.11 wireless access networks. In IN-
FOCOM 2007, pages 1451–1459. IEEE, 2007.

16 R. Kleinberg, G. Piliouras, and E. Tardos. Multiplicative updates outperform generic no-regret

17

learning in congestion games. In ACM STOC, pages 533–542. ACM, 2009.
S. Maghsudi and S. Stanczak. Relay selection with no side information: An adversarial bandit
approach. In WCNC, pages 715–720. IEEE, 2013.

18 A. Mishra, V. Brik, S. Banerjee, A. Srinivasan, and W. A. Arbaugh. A client-driven approach

19

for channel management in wireless lans. In Infocom, 2006.
E Monsef, A. Keshavarz-Haddad, E. Aryafar, J. Saniie, and M. Chiang. Convergence properties
of general network selection games. In INFOCOM, pages 1445–1453. IEEE, 2015.

20 D. Niyato and E. Hossain. Dynamics of network selection in heterogeneous wireless networks:

An evolutionary game approach. TVT, 58(4):2008–2017, 2009.

S. Oh, A. Meetoo Appavoo and S. Gilbert

15

21 Basil Cameron Rennie and Annette Jane Dobson. On stirling numbers of the second kind.

Journal of Combinatorial Theory, 7(2):116–121, 1969.

22 Allesiardo Robin, Raphaël Feraud, and Odalric-Ambrym Maillard. The non-stationary
stochastic multi-armed bandit problem. International Journal of Data Science and Ana-
lytics, 03 2017. doi:10.1007/s41060-017-0050-5.

23 Yevgeny Seldin and Gábor Lugosi. A lower bound for multi-armed bandits with expert advice.

24

In 13th European Workshop on Reinforcement Learning (EWRL), 2016.
SimPy. SimPy - Event discrete simulation for Python, 2016. https://simpy.readthedocs.io/,
accessed 2018-19-12.

25 K. Sui, M. Zhou, D. Liu, M. Ma, D. Pei, Y. Zhao, Z. Li, and T. Moscibroda. Characterizing
and improving wiﬁ latency in large-scale operational networks. In MobiSys, pages 347–360.
ACM, 2016.

26 Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert Schapire. Eﬃcient algorithms for
adversarial contextual learning. In International Conference on Machine Learning, pages
2159–2168, 2016.
C. Tekin and M. Liu. Performance and convergence of multi-user online learning. In GAMEN-
ETS, pages 321–336. Springer, 2011.

27

28 H. A. Tran, S. Hoceini, A. Mellouk, J. Perez, and S. Zeadally. Qoe-based server selection for

content distribution networks. IEEE Transactions on Computers, 63(11):2803–2815, 2014.

29 Q. Wu, Z. Du, P. Yang, Y.-D. Yao, and J. Wang. Traﬃc-aware online network selection in

heterogeneous wireless networks. TVT, 65(1):381–397, 2016.

30 K. Zhu, D. Niyato, and P. Wang. Network selection in heterogeneous wireless networks:

Evolution with incomplete information. In WCNC, pages 1–6. IEEE, 2010.

16

Periodic Bandits and Wireless Network Selection

A

Appendix: Theoretical Results and Proofs

A.1 Lower bound on Worst Case Full Regret

We construct a proof using a deterministic oblivious adversary for full generality. Proofs
using a randomized oblivious adversary or adaptive adversary are simpler. A deterministic
adversary must select the full sequence of rewards prior to the ﬁrst round. This is in contrast
to an adaptive adversary, a more powerful adversary which is allowed to select rewards each
round with full knowledge of the outcomes of random events occurring prior to the round.

Let T be the number of time steps and K be the number of arms. Fix an algorithm a.
We show that there exists a problem instance (a predetermined sequence of rewards for each
arm) such that algorithm a obtains an expected full pseudo-regret of at least T K−1
K .

We construct a problem instance which, for each time step t from 1 to T , has one arm
bt ∈ [K] which gives a reward of 1, while all other arms give a reward of 0. We construct
each bt based on the algorithm (but not on the algorithm’s choices) inductively as follows:
At the start of the algorithm, the algorithm plays arms with probabilities p1, · · · pK

respectively. Deﬁne b1 to be the arm with the lowest probability of being played.

We maintain the following invariant with parameter τ ∈ [T ] - when running the algorithm
a on the constructed problem instance from time steps 1 to τ , the expected total reward
E[Pτ
K . With the deﬁnition of b1 above, we can see
that this invariant holds for τ = 1.

t=1 xa(t)(t)] by the algorithm is at most τ

Now ﬁx any later time step τ ∈ [T ]. The algorithm a’s decision on time step τ can only be
based on past rewards and the sequence of arms played by the algorithm on time steps up to
τ − 1. As our choices of b1 to bτ −1 are ﬁxed, the only randomness comes from the algorithm’s
choices of arms up to this point. Let α = (a1, a2, · · · , aτ ) represent a possible sequence of
arms played by the algorithm for the ﬁrst τ − 1 time steps. This event occurs with probability
Qα, and the algorithm would accumulate a total reward of Rα. Assuming by induction that
the invariant holds on time steps up to τ − 1, we can state that P

α QαRα ≤ τ −1
K .

Now on time step τ , based on its past choices of arms α, the algorithm constructs a
probability vector pα
K representing the probabilities of playing each arm on time
step τ . Now we construct bτ ∈ [K] independently of α. For any ﬁxed bτ , the expected reward
by the algorithm after time step τ is given by:

2 , · · · pα

1 , pα

E[

τ
X

t=1

xa(t)(t)] =

X

α

Qα(Rα + 1 × pα
bτ

) =

X

α

QαRα +

X

α

Qαpα
bτ

As we have the following:

X

α

Qαpα

1 +

X

α

Qαpα

2 + · · · +

X

α

Qαpα

K =

X

α

Qα(pα

1 + pα

2 + · · · + pα

K) =

X

α

Qα × 1 = 1

there must exist a bτ ∈ [K] such that P
conclude that:

α Qαpα
bτ

≤ 1

K . By selecting such a bτ , we can

E[

τ
X

t=1

xa(t)(t)] =

X

α

QαRα +

Qαpα
bτ

≤

τ − 1
K

+

1
K

=

τ
K

X

α

completing the inductive proof. Thus, we can lower bound the full pseudo-regret as follows:

Rf ull(T ) =

T
X

t=1

h
xi(t)

i

− Ea

max
i∈[K]

Ea

h T
X

t=1

i

≥

(cid:16) T
X

(cid:17)
1

−

xa(t)(t)

t=1

T
K

= T

K − 1
K

S. Oh, A. Meetoo Appavoo and S. Gilbert

17

A.2 Optimized Periodic EXP4

In this section, we give an optimized implementation of Periodic EXP4 to show that
the running time bounds can be improved upon with some pre-computation. With this
optimization, we have a running time of O(K P
f ∈F Pf ) for pre-computation, and O(K|F |)
per time step later on. However, we note that such an implementation can potentially
increase the amount of numerical error. The reduction in running time largely comes from
optimizing the following computation via the introduction of variables S‘

f (t) and Bf (t):

ri(t) :=

X

(cid:16)

f ∈F

bf (t),f
i

(t)

Y

K
X

(cid:17)
b‘,f
j (t)

‘∈f ([t])\{f (t)}

j=1

Algorithm 2 Periodic EXP4
1: procedure Initialization
for each f ∈ F do
2:

3:

4:

5:

6:

7:

for each ‘ ∈ f ([T ]) do
for each i ∈ [K] do
Initialize b‘,f

(1) = 1

i

Initialize S‘

f (1) = K

. we maintain S‘

f (t) =

K
X

b‘,f
j (t)

Initialize Bf (1) = K |f ([T ])|

. we maintain Bf (t) =

j=1
Y

‘∈f ([t])

S‘

f (t)

8: procedure Algorithm
9:

for each time step t = 1, 2, · · · , T do

10:

11:

12:

13:

14:

15:

16:

17:

18:

19:

20:

21:

22:

23:

24:

for each i ∈ [K] do
bf (t),f
i

ri(t) :=

X

(t) × Bf (t)/Sf (t)

f

(t)

. running time O(K|F |)

f ∈F

for each i ∈ [K] do

pi(t) =

ri(t)
j=1rj(t)

PK

. running time O(K)

Play arm it ∈ [K] from the probabilities p1(t), p2(t), · · · , pK(t)
Obtain reward xit(t)
for each f ∈ F do

for each ‘ ∈ f ([T ]) do
for each i ∈ [K] do

if i = it and ‘ = f (t) then
(t + 1) = b‘,f

(t) exp( γ

b‘,f
i

i

K xi(t)/pi(t))

else

(t + 1) = b‘,f

b‘,f
i
f (t) + b‘,f
it

(t)
i
(t) − b‘,f
it
f (t)/S‘

f (t) := S‘
S‘
Bf (t) := Bf (t − 1) × S‘

(t − 1)
f (t − 1)

. running time O(|F |)

. implemented as no-op

. running time O(|F |)
. running time O(|F |)

A.3 Correctness of Periodic EXP4

In this section, we complete the proof of correctness of Periodic EXP4 as mentioned in
Section 5.3. As described before, we show that our algorithm produces the same probability

18

Periodic Bandits and Wireless Network Selection

distribution over arms as EXP4 in every time step. In EXP4, πθ,f is the expert which at time
t recommends arm θ ◦ f (t) with probability 1 and all other arms with probability 0. We show
that Periodic EXP4 is distributionally equivalent to EXP4, where Π = {πθ,f |f ∈ F, θ ∈ Θf }.
In EXP4, Each expert πθ,f would have some weight wθ,f (t) at time step t. At time step t,
EXP4 plays arm i with probability pi(t) represented by the following expression:

pi(t) =

P

f ∈F,θ∈Θf ,θ◦f (t)=i wθ,f (t)
P

wθ,f (t)

f ∈F,θ∈Θf

To show that the two algorithms are distributionally equivalent, as pi(t) := ri(t)/PK
in Periodic EXP4, for each successive time step t, we only need to show the following:

j=1 rj(t)

ri(t) =

X

wθ,f (t)

f ∈F,θ∈Θf ,θ◦f (t)=i

ri(t) is deﬁned in Algorithm 1.

We ﬁrst note that for each f ∈ [F ], ‘ ∈ f ([T ]), i ∈ [K], from the way b‘,f
Periodic EXP4 (Algorithm 1), we have the following expression:
exp (cid:0) γ

(t + 1) =

X

Y

i

K bxi(s)(cid:1) = exp (cid:0) γ

K

bxi(s)(cid:1)

b‘,f
i

s∈f −1(‘)∩[t]

s∈f −1(‘)∩[t]

We then note that in EXP4,

(t + 1) is deﬁned in

Divide up [t] by label

As ‘ = f (s)

wθ,f (t + 1) = exp

= exp

= exp

(cid:16) γ
K

(cid:16) γ
K

(cid:16) γ
K

=

=

Y

‘∈f ([t])
Y

‘∈f ([t])

X

(cid:17)
bxθ◦f (s)(s)

s∈[t]
X

X

(cid:17)

bxθ◦f (s)(s)

‘∈f ([t])
X

s∈f −1(‘)∩[t]
X

‘∈f ([t])
(cid:16) γ
K

exp

s∈f −1(‘)∩[t]
X

s∈f −1(‘)∩[t]

(cid:17)

(cid:17)

bxθ(‘)(s)

bxθ(‘)(s)

b‘,f
θ(‘)(t + 1)

where b‘,f

θ(‘)(t) comes from Periodic EXP4 (Algorithm 1). We then note that:

X

wθ,f (t) =

X

(cid:16) X

(cid:17)

wθ,f (t)

f ∈F,θ∈Θf ,θ◦f (t)=i

f ∈F

θ∈Θf ,θ◦f (t)=i

and that, (the last step is as Θf contains every function θ : f ([T ]) → [K])

X

wθ,f (t) =

X

Y

b‘,f
θ(‘)(t)

θ∈Θf ,θ◦f (t)=i

θ∈Θf ,θ◦f (t)=i
X

=

‘∈f ([t])
bf (t),f
θ◦f (t)(t)

Y

b‘,f
θ(‘)(t) Extract current label

(3)

By (3)

θ∈Θf ,θ◦f (t)=i

= bf (t),f
i

(t)

X

‘∈f ([t])\{f (t)}
Y

b‘,f
θ(‘)(t)

θ∈Θf ,θ◦f (t)=i

‘∈f ([t])\{f (t)}

= bf (t),f
i

(t)

Y

K
X

b‘,f
j (t)

‘∈f ([t])\{f (t)}

j=1

S. Oh, A. Meetoo Appavoo and S. Gilbert

19

Thus we have,

X

f ∈F,θ∈Θf ,θ◦f (t)=i

wθ,f (t) =

X

(cid:16)

f ∈F

bf (t),f
i

(t)

Y

K
X

(cid:17)
b‘,f
j (t)

= ri(t)

‘∈f ([t])\{f (t)}

j=1

This shows that the expression for ri(t) we have deﬁned in Periodic EXP4 corresponds to
the sum of weights of all the “experts” πθ,f from EXP4 which agree to play arm i on time
step t. Thus this concludes the proof of distributional equivalence between the algorithms.

A.4 Worst Case Regret Lower Bound on a Single Partition

This is based on the pseudo-regret in the setting with a single partition function (1).

Ω(cid:0)q

We give a proof for Theorem 2. This proof bears many similarities with the proof of a
(cid:1) lower bound for the problem of bandits with expert advice in [23].
We make use of a modiﬁed formulation by [23] of a theorem originally presented in [6].

KT log N
log K

(cid:73) Theorem 3. [23, 6] Assume that the number of time steps T ≥ K/(4 ln 4
exists a randomized oblivious adversary R, such that for algorithms a,

3 ). Then there

(cid:16)

inf
a

max
i∈[K]

E

h X

i
xi(t)

− E

h X

i(cid:17)

xa(t)(t)

√

≥

cKT

t∈[T ]

t∈[T ]

We note that this randomized oblivious adversary R picks arms independently of the
choices made by algorithm a. Details on the construction of this adversary R are given in
2 − 1)/p32 ln(4/3) is a constant independent of any parameter.
[23]. c = (

√

We now proceed with the proof of Theorem 2. For each label ‘ ∈ [P ], we consider a bandit
problem of length T‘ := |f −1(‘)|. If we assume each T‘ ≥ K/(4 ln 4
3 ), then by Theorem 3,
there exists an adversary R‘ such that for any algorithm a‘ running on a bandit problem of
length T‘,

max
i∈[K]

E

h X

i

xi(t)

− E

h X

i
xa‘(t)(t)

≥

p

cKT‘

t∈[T‘]

t∈[T‘]

We now construct an adversary R for a bandit problem of length T on partition function
f . For each time step t, let ‘ := f (t). The adversary R takes R‘’s advice to generate a
randomized reward vector for time step t.

Now consider any algorithm a for a bandit problem of length T on partition function f ,

and run it against the adversary R. Suppose there exists a label ‘ ∈ [P ] such that:

max
i∈[K]

E

h X

i

xi(t)

− E

h X

i
xa(t)(t)

<

p

cKT‘

t∈f −1(‘)

t∈f −1(‘)

We can then consider a “restriction” a‘ of algorithm a that plays on a bandit problem of
length T‘ := |f −1(‘)|. This algorithm a‘ would play exactly what algorithm a would play
on the T‘ time steps of label ‘, while simulating a’s plays against adversary E internally on
all other time steps. Therefore, against the adversary R‘, the algorithm a‘ would achieve a
pseudo-regret under

cKT , which contradicts our choice of adversary R‘.

√

20

Periodic Bandits and Wireless Network Selection

We can thus conclude that:

h X

xθ◦f (t)(t)

i

− E

h X

i

xa(t)(t)

max
θ∈Θf

E

t∈[T ]

=

max
(i1,i2,··· ,iP )∈[K]P

=

≥

X

‘∈[P ]
X

‘∈[P ]

max
i∈[K]

E

p

cKT‘

t∈[T ]
h X

X

(cid:16)

E

i
xi‘ (t)

− E

h X

i(cid:17)

xa(t)(t)

‘∈[P ]

h X

t∈f −1(‘)
i
xi(t)

− E

h X

t∈f −1(‘)

i

xa(t)(t)

t∈f −1(‘)

t∈f −1(‘)

A.5 Lower Bound on Worst Case Generalized Periodic Regret

We show a lower bound on the worst-case pseudo-regret in the generalized periodic setting
(2) based on T , K, |F | and P . In order to make use of Theorem 2 later on in the proof,
we ﬁrst make the base assumption that T is suﬃciently large. Speciﬁcally, we require that
T ≥

× K/(4 ln 4

(cid:16) log |F |

(cid:17)

log K + K − log 0.5

log K

3 ).

Fix any integer M ≥ P . Suppose K ≤ P (more labels than arms). We split the
time steps [T ] into M equally sized sections. Now we let F be the set of all partitions of
[M ] = {1, 2, · · · , M } into K parts. As K ≤ P , we can deﬁne F this way. Each partition in
F assigns one label in [K] = {1, 2, · · · , K} to each of the M sections.

(cid:73) Lemma 4. This set of partitions F covers all possible ways to assign a diﬀerent arm to
each of the M parts.

Proof. Consider any assignment π : [M ] → [K], representing each possible assignment of
arms to the M parts. We have a partition function f ∈ F that partitions [M ] into the
pre-image sets π−1(1), π−1(2), · · · , π−1(K) of π by assigning each a separate label. The
(cid:74)
labels can then be assigned to arms accordingly to represent π.

As OPT can choose an arm for each of the M sections independently, we obtain a regret

q

K T

√

lower bound of Ω(M ×
M KT ) by Theorem 2. Now we express M in terms
of |F |. As |F | is the number of partitions of [M ] into K parts, we have |F | = S(M, K),
where S(M, K) refers to the Stirling numbers of the second kind. Using the upper and lower
bounds for S(M, K) from [21], we can bound |F |, and thus M , as follows:

M ) = Ω(

(cid:19)

KM −K ≤ |F | ≤

(K 2 + K + 2)KM −K−1 − 1

(cid:18)M
K

1
2

=⇒

=⇒

1
2
log |F |
log K

1
2
1
2

KM −K ≤ |F | ≤

(K 2 + K 2 + 2K 2)KM −K−1 = 2KM −K+1

+ (K − 1) −

log 2
log K

≤ M ≤

log |F |
log K

+ K −

log 0.5
log K

This expression validates our use of Theorem 2, as this with our assumption implies
3 ) time steps
log K + K). This

that T /M ≥ K/(4 ln 4
3 ), and thus for any f ∈ F , there are at least K/(4 ln 4
associated with each label. From the bounds on M , we have that M = Θ( log |F |

thus gives us a lower bound of Ω

(cid:17)
log K + K)
Now, we can add a single extra partition function to F , that partitions the time steps [T ]
3 )),

into P equal parts. With the use of Theorem 2 again (note that T /P ≥ T /M ≥ K/4(ln 4

KT ( log |F |

(cid:16)q

.

S. Oh, A. Meetoo Appavoo and S. Gilbert

21

the presence of this partition function in F makes our lower bound no smaller than Ω(
Therefore, we obtain a ﬁnal lower bound of:

√

P KT ).

(cid:16)√

Ω

(cid:16)√

=Ω

s

P KT +

KT (

log |F |
log K

(cid:17)

+ K)

s

P KT +

KT

(cid:17)

log |F |
log K

(as K ≤ P ).

If P < K instead, a simple lower bound can be obtained by using only P out of the K
arms, so we obtain a problem with P arms and maximum partition size P . This gives us a

lower bound of Ω

. We can then merge these two lower bounds into

(cid:16)√

P KT +
(cid:16)√

q

(cid:17)

P T log |F |
log P

q

a single expression Ω

P KT +

min(P, K)T

log |F |
log min(P,K)

(cid:17)
.

B

Appendix: Experiments

B.1 Learning Patterns

Figure 6 illustrates how the average probabilities of the three networks vary with time in the
discrete setup. The conclusions we can draw are the same as with the continuous setup. We
only include this ﬁgure here for completeness.

(a) EXP3: Combined probabilities of each network over ﬁrst 10 iterations.

(b) Bandwidth ratio

(c) Periodic EXP4: Combined probabilities of each network over ﬁrst 10 iterations.

Figure 6 Area chart showing the time variation of combined probabilities in the discrete setup.
Figure 6b shows the actual ratio of the bandwidths of the three networks within any one iteration.

B.2 Noisy Settings

In this experiment, we apply a 10% Gaussian noise to each of the networks’ data rates in
the instances given in Figure 2. These noisy instances are illustrated in 7. Figure 8 shows
that Periodic EXP4 is largely unaﬀected by noise in the data, giving similar results with and
without noise.

01,4402,8804,3205,7607,2008,64010,08011,52012,96014,40000.20.40.60.81TimeslotProbabilityCellularOﬃceWiFi1OﬃceWiFi211,4400204060TimeslotDatarate(Mbps)01,4402,8804,3205,7607,2008,64010,08011,52012,96014,40000.20.40.60.81TimeslotProbabilityCellularOﬃceWiFi1OﬃceWiFi222

Periodic Bandits and Wireless Network Selection

(a) Continuous, 10% gaussian noise.

(b) Discrete, 10% gaussian noise.

Figure 7 Variation of data rates with within one iteration in each setup with noise. The original

instances without noise are given in Figure 2.

(a) Continuous, no noise.

(b) Continuous, 10% gaussian noise.

(c) Discrete, no noise.

(d) Discrete, 10% gaussian noise.

Figure 8 Distance to Optimal minimum plotted over each iteration for each setup.

B.3 Comparison of Diﬀerent Periods

It is useful to run Periodic EXP4 with multiple periods, especially in cases where the best
period is not obvious. The is the case if, for example, the bit rates (or more generally, rewards
of pulling arms) vary continuously with time.

Figures 9 and 10 compares the performance of Periodic EXP4 when run with diﬀer-
ent period sets. The period sets chosen are {1}, {4}, {1, 2, · · · , 15}, {1, 2, · · · , 24} and
{1, 2, · · · , 45}. Running Periodic EXP4 with period set {1} is equivalent to running EXP3,
and running with period set {4} is equivalent to running four separate instance of EXP3,
switching between the four instances every quarter of an iteration. Period set {4} is used for
comparison as the discrete setting we use has a known “best period” of 4. It is important to
note, however, that the “best period” is often not known prior to the experiment in practice.
We also include the Optimal Random player as a point of comparison.

From the results on the discrete setting (Figure 9a), we can see period set {4} immediately
taking the lead, and having signiﬁcantly better performance than other period sets. This is
to be expected, as in this case, the algorithm does not need to ﬁgure out what the “right
period” is. Period set {4} however performs poorly on the continuous settings.

To better illustrate the utility of larger period sets, we run another experiment on a
more complex continuous setting (Figure 10a), with bandwidths that ﬂuctuate more than

13607201,0801,440020406080TimeslotDatarate(Mbps)CellularOﬃceWiFi1OﬃceWiFi213607201,0801,4400204060TimeslotDatarate(Mbps)CellularOﬃceWiFi1OﬃceWiFi211020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.Random11020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.Random11020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.Random11020304050600204060IterationDistancetoOpt.Min(%)PeriodicEXP4Opt.RandomS. Oh, A. Meetoo Appavoo and S. Gilbert

23

(a) Discrete setting from Figure 2a.

(b) Continuous setting from Figure 2b.

Figure 9 Comparison of diﬀerent period sets on the discrete and continuous settings.

(a) “Harder” continuous setting with more bandwidth ﬂuctuations.

(b) Comparison on the “harder” continuous setting.

Figure 10 Comparison of diﬀerent period sets on a continuous setting with more bandwidth

ﬂuctuations. This is run for 100 iterations, a little over three months.

the setting in Figure 2b. From this experiment, we can see that while larger period sets
learn more slowly, they can converge to better results after a suﬃciently long period of time.
However, on simpler instances like in Figure 9b/2b, the advantage from having a larger
period set is less signiﬁcant.

In summary, larger period sets have greater utility when more ﬂuctuations in bandwidths
(or rewards) are expected, as it allows the algorithm to more closely match the target pattern.
However, a larger period set means the algorithm will take a longer time to learn. This is in
line with the usual trade-oﬀs between ﬂexibility and eﬃciency in machine learning problems.

B.4 Performance Comparison of Algorithms.

Figure 11 provides the complete distribution of the cumulative gain each device observes when
using Periodic EXP4, EXP3 or Optimal Random. Table 1 lists the median and standard
deviation for each algorithm. The median and standard deviation values are computed over
20 runs with 20 devices each, for a total of 400 data points. All three have very similar
median cumulative gains. We do note however, that the median cumulative gain is unlikely to
indicate much, as it is likely to be close to the average cumulative gain, which as mentioned
before, is not very useful as a metric as it is maximized whenever there is at least one device

11020304050600204060IterationDistancetoOpt.Min(%)Opt.Random141..151..241..4511020304050600204060IterationDistancetoOpt.Min(%)Opt.Random141..151..241..4513607201,0801,4400204060TimeslotDatarate(Mbps)CellularOﬃceWiFi1OﬃceWiFi21102030405060708090100020406080IterationDistancetoOpt.Min(%)Opt.Random141..151..241..4524

Periodic Bandits and Wireless Network Selection

in each network. On the other hand, we can see that Periodic EXP4 has a lower variance
than EXP3, which suggests that Periodic EXP4 divides the bandwidths more evenly between
the devices.

Table 1 Median and standard deviation of cumulative download (GB) per device, when using

each of the three algorithms, namely Periodic EXP4, EXP3 and Optimal Random.

Median (GB)
Standard deviation (GB)

34.78
0.20

35.08
2.72

34.67
0.04

Periodic EXP4 EXP3 Optimal Random

Figure 11 Cumulative gain (GB) of a single device when using each of the algorithms, taking
into account all devices across all runs — the box represents the interquartile range (middle 50%
cumulative gains), the vertical line in the box denotes the median, the whiskers show the range of
the remaining cumulative gains, excluding outliers shown as dots.

B.5 Mobility of Users

We consider a similar setting to Figure 1, with 20 mobile users (each with a mobile device)
and 9 networks. Devices have access to a diﬀerent set of networks over time depending on
their locations. We divide an iteration into 6 phases, based on the mobility of users, as
listed in Table 2. In phase 1, all the mobile users are at home (same building with common
networks); 10 devices have access to networks 1, 2 and 3 while the remaining have access
to networks 1 and 2. We assume 5 users are always at home (whose devices have access
to networks 1, 2 and 3 — hence, they might need to switch networks when the others go
out). The rest spend 13 hours at home. In phase 2, they travel (one hour) together to oﬃce,
during which they have access to networks 4 and 5. In phase 3, they have access to networks
6, 7 and 8 at the oﬃce. After 3 hours of work, in phase 4, 10 users go out for a 1-hour lunch
during which they have access to networks 8 and 9. After lunch, they spend another 5 hours
at the oﬃce (phase 5), before travelling home for an hour in phase 6.

Table 2 Phases during one iteration, the time slots delimiting each phase (relative to the ﬁrst

time slot of the iteration), and the list of networks available to each device during every phase.

Phase(s)
Time slots delimiting
each phase
Device(s): their list
of available networks

1
1 · · · 780
1381 · · · 1440
1 - 10: 1, 2, 3
11 - 20: 1, 2

2 and 6
781 · · · 840
1081 · · · 1380
1 - 5: 1, 2, 3
6 - 20: 4, 5

3 and 5
841 · · · 1020

4
1021 · · · 1080

1 - 5: 1, 2, 3
6 - 20: 6, 7, 8

1 - 5: 1, 2, 3
6 - 11: 6, 7, 8
11- 20: 8, 9

Vanilla Periodic EXP4 is oblivious to networks possibly becoming unavailable, and we

303540455055OptimalrandomEXP3PeriodicEXP3Perdevicecumulativegain(GB)S. Oh, A. Meetoo Appavoo and S. Gilbert

25

give it a gain of zero whenever it decides to select an inaccessible network, in the hope that
it also learns the pattern of network availability. We compare it against an optimized version
of Periodic EXP4 which selects only from the set of currently available networks. Figure 12
shows that, as expected, the optimized version initially yields a better performance. However,
after the Vanilla Periodic EXP4 algorithm learns the pattern, they both perform equally well.
The intuition for a slightly better performance compared to Optimal Random is that when
using Periodic EXP4, many of the devices eventually converge to selecting a single network
with high probability, and are thus less likely to make a random ‘bad’ selection. On the other
hand, Optimal Random has every device running the exact same (though distributionally
optimal) probability distribution, leading to the occasional random event where too many
devices select the same network simultaneously.

Figure 12 Distance to Optimal minimum in the ﬁrst and last repetitions of the mobility setting.

Vertical lines indicate the time of any change in the environment.

136072010801440020406080100Timeslot(Iteration1)DistancetoOpt.Min(%)8496185320856808604086400Timeslot(Iteration60)VanillaPeriodicEXP4OptimalRandomPeriodicEXP4withoptimization