Scalable Autonomous Vehicle Safety Validation through
Dynamic Programming and Scene Decomposition

Anthony Corso,1 Ritchie Lee,2 and Mykel J. Kochenderfer1

0
2
0
2

n
u
J

6
2

]

O
R
.
s
c
[

2
v
1
0
8
6
0
.
4
0
0
2
:
v
i
X
r
a

Abstract— An open question in autonomous driving is how
best to use simulation to validate the safety of autonomous
vehicles. Existing techniques rely on simulated rollouts, which
can be inefﬁcient for ﬁnding rare failure events, while other
techniques are designed to only discover a single failure.
In this work, we present a new safety validation approach
that attempts to estimate the distribution over failures of an
autonomous policy using approximate dynamic programming.
Knowledge of this distribution allows for the efﬁcient discovery
of many failure examples. To address the problem of scalability,
we decompose complex driving scenarios into subproblems
consisting of only the ego vehicle and one other vehicle. These
subproblems can be solved with approximate dynamic pro-
gramming and their solutions are recombined to approximate
the solution to the full scenario. We apply our approach to
a simple two-vehicle scenario to demonstrate the technique as
well as a more complex ﬁve-vehicle scenario to demonstrate
scalability. In both experiments, we observed an increase in the
number of failures discovered compared to baseline approaches.

I. INTRODUCTION

One common practice for automated vehicle (AV) safety
validation is to maintain a suite of challenging driving
scenarios that the vehicle must successfully navigate after
each update to the driving policy. Although useful,
this
approach will miss any failures that are not already included
in the test suite. Automated testing procedures that treat
the vehicle as a black box must be developed to catch
unknown and unexpected failure modes of the AV which
could dramatically decrease testing time and improve the
safety of autonomous vehicles.

Much of the literature on black box testing focuses on
falsiﬁcation where inputs are generated that cause a system
to violate a safety speciﬁcation [1]. Those inputs serve as a
counter example to the hypothesis that the system is safe.
For autonomous driving, it is not feasible to create an agent
that can avoid all possible accidents [2], so rather than ﬁnd
any failure of an AV, it is preferable to ﬁnd the most likely
failures. Traditional falsiﬁcation techniques do not consider
the probability of the failures they ﬁnd and are therefore
ill-suited to this goal. Adaptive stress testing [3] tries to
ﬁnd the most-likely failure of an autonomous system. This
approach can improve the likelihood of discovered failures
but does not necessarily explore the range of possible failures
of the system. The goal of this work is to develop a safety
validation approach that can reliably ﬁnd all of the most
relevant failures of an autonomous vehicle.

A. Corso and M. J. Kochenderfer are with the Aeronautics and Astronau-
tics Department, Stanford University. e-mail: {acorso,mykel}@stanford.edu
R. Lee is with the Robust Software Engineering group at NASA Ames

Research Center. e-mail: ritchie.lee@nasa.gov

Our approach attempts to estimate the distribution over
failures of an autonomous vehicle operating in a stochastic
environment. If we assume that the vehicle’s policy and
simulator are Markov then we show that the problem sim-
pliﬁes to estimating the probability of failure at each state,
a computation which can be performed using approximate
dynamic programming (DP). Approximate DP is particularly
effective at ﬁnding failures because it can start at a failure
and work backward to see what led to it. Unfortunately,
this approach has difﬁculty scaling to large state spaces. To
improve scalability, we use the structure of driving scenarios
by decomposing the simulation into pairwise interactions
between the ego vehicle and other agents on the road. These
subproblems are tractable for approximate DP, and their
solutions can be recombined to approximate the solution
for the full problem. To account for the approximation error
due to multi-agent interactions, we combine the subproblems
using a learned set of weights.

We apply our approach to two driving scenarios: a simple
two-vehicle scenario to demonstrate the effectiveness of DP,
and a more complex ﬁve-vehicle scenario to demonstrate
the favorable scaling of the approach. In both experiments,
we observed increases in the number of failures discovered
compared to baseline approaches, and the discovered failure
had comparatively high likelihood. The main contributions
of this work are:

• A safety validation approach that estimates the distri-

bution over failures using approximate DP.

• An algorithm for problem decomposition and recon-
struction to scale approximate DP to complex driving
scenarios.

• Demonstration of these techniques on two realistic driv-
ing scenarios and observation of a signiﬁcant increase
in rates of discovered failures.

The remainder of the paper is organized as follows: sec-
tion II gives an overview of related work in the ﬁeld of black-
box validation for autonomous driving, section III describes
our proposed technique in detail, section IV outlines the
two experiments and describes our results, and section V
concludes and discusses future work.

II. RELATED WORK

This section discusses safety validation of autonomous
systems. We ﬁrst give a brief overview of black-box fal-
siﬁcation algorithms and then discuss approaches that were
developed speciﬁcally for autonomous driving.

 
 
 
 
 
 
A. Safety Validation of Black-Box Systems

We would like to know the distribution over failures

Falsiﬁcation of black box systems involves ﬁnding inputs
to the system that lead to violation of the system speciﬁca-
tions. State-of-the-art approaches cast falsiﬁcation as a global
optimization problem over the input space [1] and try to
solve it using surrogate models [4], deep reinforcement learn-
ing [5], genetic algorithms [6], Monte Carlo tree search [7],
or cross-entropy optimization [8]. Adaptive stress testing
(AST) [3], [9]–[11] frames the problem of falsiﬁcation as
a Markov decision process and uses reinforcement learning
to ﬁnd the most-likely failures of a system according to a
prescribed probability model. The ﬁeld of statistical model
checking [12] deals with estimating the probability of failure,
and in doing so will ﬁnd inputs to the system that cause it
to fail.

Several approaches rely on sampling-based methods to
discover failures. Huang et al. [13] use bootstrapping and
importance sampling to obtain a low-variance estimate of
the probability of failure. Another approach uses impor-
tance sampling via the cross-entropy method to increase the
number of failures found in simulation [14], [15]. Uesato
et al. [16] use previous versions of an autonomous agent
to help ﬁnd failures in the ﬁnal version, an approach that
works when when agents have learned behavior. Similar to
the present work, Chryssanthacopoulos, Kochenderfer, and
Williams [17] use DP to estimate the probability of failure.

B. Safety of Autonomous Vehicles

Some work has focused on falsifying components of an
autonomous vehicle such as Adaptive Cruise Control [18]
or perception systems [19], [20]. Other work has focused
on the generation of critical test cases. For example Mullins
et al. [21] identify regions of the input space that separate
distinct types of autonomous agent behavior, and Althoff
and Lutz [22] design adversarial agents to minimize the safe
available driving space of the autonomous vehicle.

III. PROPOSED APPROACH

This section describes our approach to the safety valida-
tion problem. We start with the problem formulation and
deﬁnition of notation. Then, we describe our technique for
estimating the distribution over failures assuming we know
the probability of failure from each state. Lastly, we describe
how to compute that probability in a scalable way.

A. Problem Formulation

Suppose we wish to analyze the safety of a black-box au-
tonomous system (system-under-test, or SUT) that operates
in a stochastic simulated environment. The state of the SUT
and the environment is s ∈ S and the disturbances x ∈ X
are stochastic elements of the environment that inﬂuence
the behavior of the SUT. A state-disturbance trajectory τ =
{s0, x1, s1 . . . , xN , sN } has a likelihood of occurrence p(τ ).
We deﬁne E as the set of all failure states of the SUT and
the notation sN ∈ E means that the trajectory τ ends in a
failure. Let T be the set of all terminal states where E ⊆ T .

f (τ ) =

1 {sN ∈ E} p(τ )
Ep [1{sN ∈ E}]

(1)

where 1 is the indicator function and the denominator
normalizes the distribution. Note that f (τ ) is the minimum-
variance importance sampling distribution for estimating the
probability of failure.

B. Estimating the Distribution Over Failures

The space of all trajectories is exponential in the legnth
of the trajectory, so it will be challenging to represent the
distribution f (τ ) directly. To reduce the dimensionality of
the distribution we assume that the SUT and environment
are Markov. The current disturbance x and next state s(cid:48) will
only depend on the current state s such that

p(x, s(cid:48) | s) =

p(x | s)
(cid:124) (cid:123)(cid:122) (cid:125)
disturbance model

dynamics
(cid:122)
(cid:123)
(cid:125)(cid:124)
p(s(cid:48) | s, x) .

(2)

If we also assume that
the dynamics of the SUT and
the environment are deterministic (i.e. all stochasticity is
controlled through disturbances), then

p(x, s(cid:48) | s) = p(x | s).

(3)

With these assumptions, the distribution over failures only
depends on p(x | s) and is given by

f (τ ) =

1{sN ∈ E}
Ep [1{sN ∈ E}]

N
(cid:89)

t=1

p(xt | st−1).

(4)

The Markov assumption allows us to ﬁnd a distribution
over disturbances, or stochastic policy, π that generates
sample trajectories (rollouts) distributed according to f . Let

=

(cid:80)

π(x | s) =

p(x | s)v(s(cid:48))
v(s)

p(x | s)v(s(cid:48))
a(cid:48) p(x(cid:48) | s)v(s(cid:48)(cid:48))
where v(s) is the probability of failure from state s and s(cid:48)(cid:48) is
the state reached from s after applying disturbance x(cid:48). The
second equality in eq. (5) comes from the observation that
the probability of failure in the current state is a sum of the
probability of failure over possible next states, weighted by
the likelihood of reaching that state.

(5)

Proposition 1: Trajectories generated from rollouts of the

policy π will be distributed according to f .

Proof: Let f ∗(τ ) be the distribution induced by rollouts
of the policy π. We will show that for any τ , f (τ ) = f ∗(τ ).
First, we deﬁne the Bellman equation that describes the
probability of failure of a Markov system as

v(s) =






1
0
(cid:80)

if s ∈ E
if s /∈ E, s ∈ T

(6)

a p(x | s)v(s(cid:48)) otherwise

Then consider an arbitrary trajectory τ that has probability
according to f ∗ given by

f ∗(τ ) =

N
(cid:89)

t=1

π(xt | st−1) =

N
(cid:89)

t=1

p(xt | st−1)v(st)
v(st−1)

(7)

and probability according to f given by eq. (4). There are
two cases to consider.
Case 1: 1{sN /∈ E}

Due to the indicator function in eq. (4), f (τ ) = 0. With
ﬁnal state sN ∈ T and sN /∈ E, we have v(sN ) = 0. The
last term in the product in eq. (7) contains v(sN ), making
f ∗(τ ) = 0.
Case 2: 1{sN ∈ E}

Considering the deﬁnition of f ∗(τ ), we have

N
(cid:89)

f ∗(τ ) =

p(xt | st−1)v(st)
v(st−1)

t=1

(cid:24)(cid:24)(cid:24)(cid:24)(cid:58) 1
v(sN )
v(s0)

N
(cid:89)

t=1

p(xt | st−1)

1
Ep [1{sN ∈ E}]

N
(cid:89)

t=1

p(xt | st−1)

=

=

= f (τ )

(8)

(9)

(10)

(11)

in the second line, all of the v(st) terms were
where,
canceled except v(s0) and v(sN ), and eq. (6) was used
to let v(sN ) = 1. In the third line, we observe that the
probability of failure at the initial state v(s0) is equivalent
to the expectation of failures Ep [1{sN ∈ E}]. Thus, in all
cases, f (τ ) = f ∗(τ ).

Assuming that the distribution over disturbances p(x | s)
is provided (either from domain knowledge or data), then the
problem of computing the distribution over failures amounts
to computing the probability of failure at each state v(s).
Additionally, once v(s) is known, failures can be generated
from any initial condition where a failure can be reached.

C. Computing the Probability of Failure

The feasibility of computing the probability of failure v(s)
depends on the size of the state and disturbance spaces. If
those spaces are discrete and relatively small, then DP can
be used to compute v to any desired level of accuracy. If
the state space is continuous, but is small enough to be
discretized, then local approximation DP can be used to
estimate v(s) [23]. As will be demonstrated by our exper-
iments, this approach is feasible for interactions between
two vehicles on the road. For more vehicles, discretizing the
state space becomes intractable and we must rely on further
approximation.

When scaling to much larger state-spaces, we can leverage
the structure of the problem to improve scalability. We
propose to decompose a complicated driving scenario into
pairwise interactions between the ego vehicle and other
agents on the road, similar to the decomposition approach
used by Bouton et al. [24]. Each subproblem can then be
solved for the probability of failure between the ith vehicle
and the ego vehicle yielding vi(s(i)), where s(i) is the subset
of the state representing only those vehicles. To combine
the probability of failure from each of m subproblems, we
can use the transfer learning approach called attend, adapt
and transfer (A2T) [25]. A2T combines the solutions of m

s

e
t
a
t
S

Base network

Solutions

Attention network

values

weights

v(s)

Fig. 1: The A2T network. Dashed lines represent backprop-
agation for learning parameters.

problems with a solution learned from scratch vbase using a
learned set of state-dependent attention weights w(s). The
estimated probability of failure for a state s, ˜v(s), is then
given by

˜v(s) = w0(s)vbase(s) +

m
(cid:88)

i=1

wi(s)vi(s(i))

(12)

where wi and vbase have parameters that can be learned.

The use of attention weights allows A2T to learn which
solutions are most relevant in which states. If none of the
subproblems are providing a good estimate then the base net-
work will learn a good estimate from scratch. The estimate
from eq. (12) can be represented as the network architecture
shown in ﬁg. 1. The base network has two hidden layers
each with 32 units and relu activations followed by a sigmoid
activation to keep the output between 0 and 1. The solutions
take the state as input and give the probability of failure
estimate for each subproblem. The attention network has one
hidden layer with 32 units and a softmax layer to make
is
sure the weights sum to 1. The base network output
concatenated to the subproblem solution outputs to create a
vector of values that has m + 1 components. The dot product
is taken between the values and the m+1 weights to produce
the ﬁnal estimate of the probability of failure.

Algorithm 1 MC evaluation with function approximation

for Niter iterations

1: function MCPOLICYEVAL(˜vθ, Niter, Nsamp, α)
2:
3:
4:
5:
6:

S, G ← Rollouts(˜vθ, Nsamp)
J = 1
Nsamp
θ ← θ − α∇θJ

(Gj − v(Sj ))2

(cid:80)Nsamp
j=1

return ˜vθ

The network can be trained using rollouts from the
full driving scenario to estimate the probability of failure.
The training procedure we used is Monte Carlo policy
evaluation with function approximation [26] and is shown
in algorithm 1. The algorithm takes as input the network
that estimates the probability of failure ˜vθ with trainable
parameters θ, the number of training iterations Niter, the
number of sampled transitions per iteration Nsamp, and the
learning rate α. On each iteration, a series of rollouts are
performed (line 3). The rollout policy is π from eq. (5)
where v(s) is replaced with the current estimate ˜vθ(s). As the
estimate of the probability of failure is improved, the rollout

policy will produce more failure examples. All of the states
visited during the rollouts are concatenated into a vector S.
The return is computed for each state sj ∈ S as

G(sj) = 1{sN ∈ E}

N
(cid:89)

t=j

p(xt | st−1)/π(xt | st−1)

(13)

where N is the length of the episode that contained state
sj. The estimate G is a Bernoulli sample weighted by
the likelihood ratio of the current sampling policy so the
expected value of of G(s) is the probability of failure from
state s. The cost J is the mean squared error between the
estimated probability of failure ˜vθ(s) and G(s) (line 4). The
parameters of the network are updated using the gradient of
the cost function to improve the estimate (line 5).

Algorithm 2 Intersection navigation algorithm

ttc ← time to cross intersection(veh)
∆sint ← distance to intersection(veh)
if ∆sint < ∆slead

v ← velocity(veh)
vlead, ∆slead ← leading vehicle(veh, scene)
acc ← IDM acceleration(∆slead, v, vlead)
if veh does not have right of way

1: function COMPUTEACCELERATION(veh, scene)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

for agent in scene

return acc

ttenter ← time to enter intersection(agent)
ttexit ← time to exit intersection(agent)
if ttenter < ttc and ttexit + (cid:15) > ttc

acc ← IDM acceleration(∆sint, v, 0)
break

TABLE I: Action space for adversarial vehicles

IV. EXPERIMENTS

Action

Acceleration MC Probability

This section describes two experimental driving scenarios,
a simple scenario with two vehicles, and a more complex sce-
nario with ﬁve vehicles. The simulations were designed with
AutomotiveSimulator.jl, an open-source julia package. Both
simulations rely on the same road geometry and autonomous
driving policy. The SUT is an autonomous vehicle referred to
as the ego vehicle and a failure refers to any instance where
the ego vehicle collides with another vehicle.

The road geometry and initial vehicle conﬁgurations are
pictured in ﬁgs. 2 to 5. The driving scenario an unprotected
left turn of the ego vehicle (in blue) onto a two-lane road.
Other vehicles (referred to adversarial vehicles) are initial-
ized on the through-road and can either continue straight or
turn (the yellow dot represents a turn signal). The right-of-
way rules are 1) vehicles on the through-road have right-
of-way over vehicles turning on to the through-road, and 2)
vehicles turning right have right-of-way over vehicles turning
left.

The state of each vehicle can be described with four
variables: position along the lane, velocity along the lane,
a Boolean indicating if the turn signal is on, an integer
indicating the lane. For approximate DP, the position and
velocity were each discretized into 15 values and each
vehicle can be in one of two lanes so each vehicle had a
total of 900 states.

Each vehicle on the road including the ego vehicle, follows
a modiﬁed version of the intelligent driver model (IDM) [27].
The IDM is a vehicle-following algorithm that tries to drive
at a speciﬁed velocity while avoiding collisions with leading
vehicles. In our experiments, the IDM is parameterized by
a desired velocity of 29 m/s, a minimum spacing of 5 m, a
maximum acceleration of 3 m/s2 and a comfortable braking
deceleration of −2 m/s2, and a simulation timestep of ∆t =
0.18 s. The IDM was modiﬁed with a rule-based algorithm
(algorithm 2) for navigating the T-intersection. Each vehicle
reasons about right-of-way and turning intention of other
vehicles based on the state of their blinker, and uses current
vehicle speeds to calculate if the intersection is safe to cross.
The disturbances in the environment correspond to distur-
bances to the deterministic actions of all adversarial vehicles.

No disturbance
Medium slowdown
Major slowdown
Medium speedup
Major speedup
Toggle blinker
Toggle turn intent

0 m/s2
−1.5 m/s2
−3 m/s2
1.5 m/s2
3 m/s2
N/A
N/A

0.976
1 × 10−2
1 × 10−3
1 × 10−2
1 × 10−3
1 × 10−3
1 × 10−3

The disturbances and their corresponding probabilities are
shown in table I. The ﬁrst produces no disturbance, so the
adversary accelerates by aIDM, the acceleration computed by
the modiﬁed IDM. The next four disturbances perturb the ad-
versary’s acceleration by an amount δa ∈ [−3 m/s2, 3 m/s2]
so that the actual acceleration of the adversary is aIDM + δa.
The next disturbance toggles the adversary’s turn signal
which is observed by other vehicles and used to determine
the adversary’s intention. The ﬁnal disturbance changes the
hidden adversary intention as to whether or not it will turn.
The choice of a disturbance probability model should be
driven by real-world driving data. In absence of that data, we
chose a simple probability model that made disturbances rare
according to their magnitude (see MC Probability in table I).
Medium slowdowns and speedups were give a probability of
1 × 10−2 per timestep while major slowdowns and speedups,
toggling the blinker, and toggling turn intention had a per-
timestep probability of occurrence of 1 × 10−3.

The two metrics we chose to evaluate our approach are
the rate of failures found and the log-likelihood of adversary
disturbances for failure trajectories. The failure rates are
computed from 1000 rollouts and the average log-likelihood
of disturbances is computed from 100 failure examples. The
mean and standard deviations are reported. We compare our
approach against three baselines:

1) Monte Carlo: rollouts with the true probability distri-

bution over disturbances.

2) Uniform importance sampling: rollouts with a uniform

distribution over disturbances.

3) Cross entropy method: rollouts with a distribution over
disturbances that has been optimized using the cross
entropy method [28].

Fig. 2: Normal 2-car scenario at t = (1.26 s, 2.88 s)

Fig. 3: Collision in 2-car scenario at t = (1.26 s, 1.80 s)

A. Two-Vehicle Interaction

The ﬁrst scenario is an interaction between the ego vehicle
and one adversarial vehicle over a range of initial conditions.
Figure 2 shows one mode of expected behavior in the
scenario: the ego vehicle correctly predicts it can cross the
intersection before the other driver arrives so it proceeds with
the left turn. A sample failure is shown in ﬁg. 3. We can see
that the adversary had to accelerate early in the simulation to
cause a collision with the ego vehicle, which did not predict
that an acceleration would occur.

Table II shows the number of failures observed with each
approach. Monte Carlo sampling ﬁnds the fewest failures
with a rate of 8 × 10−3 but the failure trajectories have a
comparatively large log-likelihood. The uniform importance
sampling approach increases the number of failures found by
making rare disturbances more likely, but causes the found
failures to be extremely unlikely due to these rare distur-
bances. The cross entropy method ﬁnds slightly more failures
than the Monte Carlo approach with a larger log likelihood
than uniform importance sampling. The DP approach was
most successful with a failure rate of 1.67 × 10−1 while still
retaining a large value of log likelihood.

B. 5 Vehicle Interaction

The second scenario involves the interaction of the ego
vehicle with four adversarial drivers. A sample of normal
behavior for the scenario is shown in ﬁg. 4 where the cars on
the left and the trailing car on the right go straight, while the
leading car on the right turns onto the vertical road segment.
The ego vehicle gives way to all four vehicles and completes
the left turn after they have passed. A sample failure is shown
in ﬁg. 5. The failure shows that the last car on the left turns it
signal on while continuing straight through the intersection,
tricking the ego vehicle into initiating the left turn too early.
The driving scenario was broken into four subproblems,

TABLE II: 2-Car Scenario Failure Rates

Method

Failure Rate

Log Likelihood

Monte Carlo
Uniform Importance Sampling
Cross Entropy Method
Dynamic Programming (ours)

0.008 ± 0.003 −0.311 ± 0.029
0.050 ± 0.007 −5.291 ± 0.065
0.009 ± 0.003 −0.452 ± 0.038
0.167 ± 0.012 −0.656 ± 0.021

TABLE III: 5-Car Scenario Failure Rates

Method

Failure Rate

Log Likelihood

Monte Carlo
Uniform Importance Sampling
Cross Entropy Method
A2T + DP (ours)

0.0011 ± 0.0003 −0.437 ± 0.069
0.033 ± 0.002
−7.374 ± 0.079
0.0027 ± 0.0005 −0.436 ± 0.041
−0.947 ± 0.145
0.222 ± 0.004

one for each adversarial vehicle. The probability of failure
was computed for each driving scenario using approximate
DP and the solutions were combined using an A2T network
trained on rollouts of the full simulator. One challenge for
this approach is the exponential scaling of the disturbance
space of the full system. If there are 4 subproblems each
with 7 disturbances then the full problem must consider 2401
disturbances per step. To mitigate this problem, we only let
one agent act at each timestep, reducing the possible dis-
turbances to 28. This design choice reduces the complexity
of possible failure modes, but makes the problem tractable
while still ﬁnding failures.

The results are shown in table III. We ﬁrst note that the
failure rate in the scenario is lower than the previous scenario
as indicated by the failure rate of the Monte Carlo approach
(1.1 × 10−3). The uniform importance sampling approach
improves failure rate signiﬁcantly but ﬁnds failures with very
low likelihood due to the increased number of rare distur-
bances. The cross entropy method has twice the failure rate
as the Monte Carlo approach with a similar log-likelihood.
Our approach (DP combined with A2T) has a much larger
failure rate (2.22 × 10−1) while ﬁnding relatively likely
failures, demonstrating that scene decomposition combined
with A2T is an effective strategy for ﬁnding failures of an
autonomous vehicle in a complex driving scenario.

V. CONCLUSIONS

In this work, we have made progress toward the goal of
automated testing of autonomous vehicles. We introduced a
safety validation formulation that uses approximate dynamic
programming to estimate the distribution over failures and
create sequences of disturbances that cause an autonomous
system to fail. The problem of scalability was addressed by
decomposing the driving scenario into pairwise interactions
between the ego vehicles and other agents on the road. These
subproblems were solved and recombined to estimate the
probability of failure of the full system. To correct for errors
in this estimate, we trained an A2T network with Monte
Carlo policy evaluation to weight each subproblem based on
the state. We observed 1 to 2 orders of magnitude increase
in the number of failures found compared to importance
sampling baselines in a two-vehicle driving scenario and a
more complex ﬁve-vehicle driving scenario, demonstrating
the beneﬁt of this approach. Future work will use the
calculated policy to obtain a low-variance estimate of the
probability of failure, test performance on more complicated
driving scenarios with many agents, and attempt to interpret
the attention weights parameters to understand the cause of
failures.

Fig. 4: Normal 5-car scenario at t = (0.9 s, 4.32 s, 7.02 s)

Fig. 5: Collision in 5-car scenario at t = (0.9 s, 2.34 s, 3.42 s)

ACKNOWLEDGMENT

The authors gratefully acknowledge the ﬁnancial support
from the Stanford Center for AI Safety. We also thank the
NASA AOSP SWS Project.

REFERENCES
[1] A. Donz and O. Maler, “Robust satisfaction of temporal logic
over real-valued signals,” in International Conference on Formal
Modeling and Analysis of Timed Systems (FORMATS), 2010.
S. Shalev-Shwartz, S. Shammah, and A. Shashua, “On a for-
mal model of safe and scalable self-driving cars,” ARXIV, no.
1708.06374, 2017.

[2]

[4]

[3] R. Lee, M. J. Kochenderfer, O. J. Mengshoel, G. P. Brat, and
M. P. Owen, “Adaptive stress testing of airborne collision avoidance
systems,” in Digital Avionics Systems Conference (DASC), 2015.
L. Mathesen, S. Yaghoubi, G. Pedrielli, and G. Fainekos, “Falsiﬁ-
cation of cyber-physical systems with robustness uncertainty quan-
tiﬁcation through stochastic optimization with adaptive restart,” in
IEEE Conference on Automation Science and Engineering (CASE),
Aug. 2019.
T. Akazaki, S. Liu, Y. Yamagata, Y. Duan, and J. Hao, “Falsiﬁcation
of cyber-physical systems using deep reinforcement learning,” in
International Symposium on Formal Methods, 2018.

[5]

[7]

[6] Q. Zhao, B. H. Krogh, and P. Hubbard, “Generating test inputs for
embedded control systems,” IEEE Control Systems Magazine, vol.
23, no. 4, pp. 49–57, 2003.
Z. Zhang, G. Ernst, S. Sedwards, P. Arcaini, and I. Hasuo, “Two-
layered falsiﬁcation of hybrid systems guided by Monte Carlo
tree search,” IEEE Transactions on Computer-Aided Design of
Integrated Circuits and Systems (TCAD), vol. 37, no. 11, pp. 2894–
2905, 2018.
S. Sankaranarayanan and G. Fainekos, “Falsiﬁcation of temporal
properties of hybrid systems using the cross-entropy method,” in
ACM international conference on Hybrid Systems: Computation and
Control (HSCC), 2012.

[8]

[9] M. Koren, S. Alsaif, R. Lee, and M. J. Kochenderfer, “Adaptive
stress testing for autonomous vehicles,” in IEEE Intelligent Vehicles
Symposium (IV), 2018.

[10] A. Corso, P. Du, K. Driggs-Campbell, and M. J. Kochenderfer,
“Adaptive stress testing with reward augmentation for autonomous
vehicle validation,” in IEEE International Conference on Intelligent
Transportation Systems (ITSC), 2019.

[11] M. Koren and M. Kochenderfer, “Efﬁcient autonomy validation
in simulation with adaptive stress testing,” in IEEE International
Conference on Intelligent Transportation Systems (ITSC), Oct. 2019.
[12] G. Agha and K. Palmskog, “A survey of statistical model check-
ing,” ACM Transactions on Modeling and Computer Simulation
(TOMACS), vol. 28, no. 1, pp. 1–39, 2018.
Z. Huang, M. Arief, H. Lam, and D. Zhao, “Evaluation uncertainty
in data-driven self-driving testing,” in IEEE International Confer-
ence on Intelligent Transportation Systems (ITSC), 2019.

[13]

[14] Y. Kim and M. J. Kochenderfer, “Improving aircraft collision
risk estimation using the cross-entropy method,” Journal of Air
Transportation, vol. 24, no. 2, pp. 55–62, 2016.

[16]

[15] M. O’Kelly, A. Sinha, H. Namkoong, R. Tedrake, and J. C. Duchi,
“Scalable end-to-end autonomous vehicle testing via rare-event
simulation,” in Advances in Neural Information Processing Systems
(NIPS), 2018.
J. Uesato, A. Kumar, C. Szepesvari, T. Erez, A. Ruderman, K.
Anderson, N. Heess, and P. Kohli, “Rigorous agent evaluation: An
adversarial approach to uncover catastrophic failures,” ARXIV, no.
1812.01647, 2018.
J. P. Chryssanthacopoulos, M. J. Kochenderfer, and R. E. Williams,
“Improved Monte Carlo sampling for conﬂict probability estima-
tion,” in AIAA Non-Deterministic Approaches Conference, 2010.

[17]

[18] M. Koschi, C. Pek, S. Maierhofer, and M. Althoff, “Computationally
efﬁcient safety falsiﬁcation of adaptive cruise control systems,”
in IEEE International Conference on Intelligent Transportation
Systems (ITSC), 2019.

[19] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A.
Chen, K. Fu, and Z. M. Mao, “Adversarial sensor attack on
lidar-based perception in autonomous driving,” in ACM SIGSAC
Conference on Computer and Communications Security, 2019.

[20] A. Balakrishnan, A. Puranic, X. Qin, A. Dokhanchi, J. Deshmukh,
H. Ben Amor, and G. Fainekos, “Specifying and evaluating quality
metrics for vision-based perception systems,” in Design, Automation
and Test in Europe (DATE), May 2019.

[21] G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, and S. K.
Gupta, “Adaptive generation of challenging scenarios for testing
and evaluation of autonomous vehicles,” Journal of Systems and
Software, vol. 137, pp. 197–215, 2018.

[22] M. Althoff and S. Lutz, “Automatic generation of safety-critical
test scenarios for collision avoidance of road vehicles,” in IEEE
Intelligent Vehicles Symposium (IV), 2018.

[23] M. J. Kochenderfer, Decision making under uncertainty: Theory

and application. MIT Press, 2015.

[24] M. Bouton, K. D. Julian, A. Nakhaei, K. Fujimura, and M. J.
Kochenderfer, “Decomposition methods with deep corrections for
reinforcement learning,” International Conference on Autonomous
Agents and Multiagent Systems (AAMAS), vol. 33, no. 3, pp. 330–
352, 2019.
J. Rajendran, A. S. Lakshminarayanan, M. M. Khapra, P. Prasanna,
and B. Ravindran, “Attend, adapt and transfer: Attentive deep
architecture for adaptive transfer from multiple sources in the same
domain,” ARXIV, no. 1510.02879, 2015.

[25]

[26] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduc-

tion. MIT Press, 2018.

[27] M. Treiber, A. Hennecke, and D. Helbing, “Congested trafﬁc states
in empirical observations and microscopic simulations,” Physical
Review E, vol. 62, pp. 1805–1824, 2000.
P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein,
“A tutorial on the cross-entropy method,” Annals of Operations
Research, vol. 134, no. 1, pp. 19–67, 2005.

[28]

