2
2
0
2

n
u
J

8

]

G
L
.
s
c
[

2
v
2
0
9
2
0
.
6
0
2
2
:
v
i
X
r
a

Goal-Space Planning with Subgoal Models

Chunlok Lo, Gabor Mihucz, Adam White∗, Farzane Aminmansour, Martha White∗
Department of Computing Science, Alberta Machine Intelligence Institute (Amii)
University of Alberta
CIFAR Canada AI Chair∗
{chunlok,mihucz,amw8,aminmans,whitem}@ualberta.ca

Abstract

This paper investigates a new approach to model-based reinforcement learning
using background planning: mixing (approximate) dynamic programming updates
and model-free updates, similar to the Dyna architecture. Background planning
with learned models is often worse than model-free alternatives, such as Double
DQN, even though the former uses signiﬁcantly more memory and computation.
The fundamental problem is that learned models can be inaccurate and often
generate invalid states, especially when iterated many steps. In this paper, we avoid
this limitation by constraining background planning to a set of (abstract) subgoals
and learning only local, subgoal-conditioned models. This goal-space planning
(GSP) approach is more computationally efﬁcient, naturally incorporates temporal
abstraction for faster long-horizon planning and avoids learning the transition
dynamics entirely. We show that our GSP algorithm can learn signiﬁcantly faster
than a Double DQN baseline in a variety of situations.

1

Introduction

Planning with learned models in RL is important for sample efﬁciency. Planning provides a mecha-
nism for the agent to simulate data, in the background during interaction, to improve value estimates.
Dyna [41] is a classic example of background planning. On each step, the agent simulates several
transitions according to its model, and updates with those transitions as if they were real experience.
Learning and using such a model is worthwhile in vast or ever-changing environments, where the
agent learns over a long time period and can beneﬁt from re-using knowledge about the environment.

The promise of Dyna is that we can exploit the Markov structure in the RL formalism, to learn and
adapt value estimates efﬁciently, but many open problems remain to make it more widely useful. These
include that (1) one-step models learned in Dyna can be difﬁcult to use for long-horizon planning, (2)
learning probabilities over outcome states can be complex, especially for high-dimensional states and
(3) planning itself can be computationally expensive for large state spaces.

A variety of strategies have been proposed to improve long-horizon planning. Incorporating options
as additional (macro) actions in planning is one approach. An option is a policy coupled with a
termination condition and initiation set [37]. They provide temporally-extended ways of behaving,
allowing the agent to reason about outcomes further into the future. Incorporating options into
planning is a central motivation of this paper, particularly how to do so under function approximation.
Options for planning has largely only been tested in tabular settings [37, 34, 49]. Recent work
has considered mechanism for identifying and learning option policies for planning under function
approximation [40], but as yet did not consider issues with learning the models.

A variety of other approaches have been developed to handle issues with learning and iterating
one-step models. Several papers have shown that using forward model simulations can produce

Preprint. Under review.

 
 
 
 
 
 
simulated states that result in catastrophically misleading values [17, 46, 21]. This problem has been
tackled by using reverse models [27, 17, 46]; primarily using the model for decision-time planning
[46, 32, 5]; and improving training strategies to account for accumulated errors in rollouts [42, 47, 43].
An emerging trend is to avoid approximating the true transition dynamics, and instead learn dynamics
tailored to predicting values on the next step correctly [10, 9, 2]. This trend is also implicit in the
variety of techniques that encode the planning procedure into neural network architectures that can
then be trained end-to-end [44, 33, 26, 50, 11, 31]. We similarly attempt to avoid issues with iterating
models, but do so by considering a different type of model.

Much less work has been done for the third problem in Dyna: the expense of planning. There is,
however, a large literature on approximate dynamic programming—where the model is given—that is
focused on efﬁcient planning (see [29]). Particularly relevant to this work is restricting value iteration
to a small subset of landmark states [22].1 The resulting policy is suboptimal, restricted to going
between these landmark states, but planning is provably much more efﬁcient.

Beyond this planning setting where the model is given, the use of landmark states has also been
explored in goal-conditioned RL, where the agent is given a desired goal state or states. The ﬁrst
work to exploit this idea in reinforcement learning with function approximation, when learning
online, was for learning universal value function approximators (UVFAs) [16]. The UVFA conditions
action-values on both state-action pairs as well as landmark states. A search is done on a learned
graph between landmark states, to identify which landmark to moves towards. A ﬂurry of work
followed, still in the goal-conditioned setting [25, 8, 55, 54, 1, 14, 12, 19, 7].

In this paper, we exploit the idea behind landmark states for efﬁcient background planning in general
online reinforcement learning problems. The key novelty is a framework to use subgoal-conditioned
models: temporally-extended models that condition on subgoals. The models are designed to be
simpler to learn, as they are only learned for states local to subgoals and they avoid generating entire
next state vectors. We use background planning on subgoals, to quickly propagate (suboptimal)
value estimates for subgoals. We propose subgoal-value bootstrapping, that leverages these quickly
computed subgoal values, but mitigates suboptimality by incorporating an update on real experience.
We show in the PinBall environment that our Goal-Space Planning (GSP) algorithm can learn
signiﬁcantly faster than Double DQN, and still reaches nearly the same level of performance. A major
insight of this paper is that many of the pieces to the model-based RL puzzle already exist and just
needed to be assembled into a uniﬁed architecture: the combination of temporal abstraction, subgoal
planning, avoiding learning transitions, and using UVFAs is far greater than the sum of the parts.

2 Problem Formulation

,

,

).

P

A

R

S
:

[0,

S × A × S →

is the state space and

We consider the standard reinforcement learning setting, where an agent learns to make deci-
sions through interaction with an environment, formulated as Markov Decision Process (MDP)
R and the transition
the action space.
(
,
S
P
A
) describes the expected reward and probability of transitioning
probability
to a state, for a given state and action. On each discrete timestep t the agent selects an action At in
state St, the environment transitions to a new state St+1 and emits a scalar reward Rt+1.
[0, 1] that maximizes expected return, the
The agent’s objective is to ﬁnd a policy π : S
×
future discounted reward Gt
[0, 1] depends
on St+1 [38], which allows us to specify termination. If St+1 is a terminal state, then γt+1 = 0; else,
[0, 1]. The policy can be learned using algorithms like Q-learning
γt+1 = γc for some constant γc ∈
[36], which approximate the action-values: the expected return from a given state and action.

.
= Rt+1 + γt+1Gt+1. The state-based discount γt+1 ∈

S × A × S →

∞

→

R

A

:

We can incorporate models and planning to improve sample efﬁciency beyond these basic model-free
algorithms. In this work, we focus on background planning algorithms: those that learn a model
during online interaction and asynchronously update value estimates use dynamic programming
updates.2 The classic example of background planning is Dyna [41], which performs planning steps

1A similar idea to landmark states has been considered in more classical AI approaches, under the term
bi-level planning [53, 15, 6]. These techniques are quite different from Dyna-style planning—updating values
with (stochastic) dynamic programming updates—and so we do not consider them further here.

2Another class of planning algorithms are called model predictive control (MPC). These algorithms learn
a model and use decision-time planning by simulating many rollouts from the current state. Other recent
algorithms using this idea are those doing Monte Carlo tree search (MCTS) online, such as MuZero [31].

2

by selecting previously observed states, generating transitions—outcome rewards and next states—for
every action and performing a Q-learning update with those simulated transitions.

Planning with learned models, however, has several issues. First, even with perfect models, it
can be computationally expensive. Running dynamic programming can require multiple sweeps,
which is infeasible over a large number of states. A small number of updates, on the other hand,
may be insufﬁcient. Computation can be focused by carefully selecting which states to sample
transitions from—called search control—but the question about how to do so effectively remains
largely unanswered with only a handful of works [24, 52, 28].

|

The second difﬁculty arises due to errors in the learned models. In reinforcement learning, the
transition dynamics is represented with an expectation model E[S(cid:48)
s, a] or a probabilistic model
s, a). If the state space or feature space is large, then the expected next state or distribution over
P (s(cid:48)
it can be difﬁcult to estimate, as has been repeatedly shown [43]. Further, these errors can compound
when iterating the model forward or backward [17, 46]. It is common to use an expectation model, but
unless the environment is deterministic or we are only learning the values rather than action-values,
this model can result in invalid states and detrimental updates [48].

|

In this work, we take steps towards the ambitious question: how can we leverage a separate com-
putational procedure (planning with a model) to improve learning in complex environments? More
speciﬁcally, we consider background planning for value-based methods. We address the two difﬁcul-
ties with classic background planning strategies discussed above, by focusing planning on a set of
subgoals (abstract states) and changing the form of the model.

3 Starting Simpler: Goal-Space Planning for Policy Evaluation

To highlight the key idea for efﬁcient planning, we start in a simpler setting: policy evaluation for
learning vπ for a ﬁxed deterministic policy π in a deterministic environment, assuming access to the
true models. The key idea is to propagate values quickly across the space by updating between a
subset of states that we call subgoals, g
to abstract subgoal vectors
that need not correspond to any state.) To do so, we need temporally extended models between
pairs g, g(cid:48) that may be further than one-transition apart. For policy evaluation, these models are the
accumulated rewards rπ,γ :
[0, 1] under π:

R and discounted probabilities Pπ,γ :

. (Later we extend

∈ G ⊂ S

G (cid:54)⊂ S

S × S →

rπ,γ(g, g(cid:48)) def= Eπ[Rt+1 + γg(cid:48),t+1rπ,γ(St+1, g(cid:48))
Pπ,γ(g, g(cid:48)) def= Eπ[1(St+1 = g(cid:48))γt+1 + γg(cid:48),t+1Pπ,γ(St+1, g(cid:48))
|

St = g]
|

S × S →

St = g]

G

∈ G

where γg(cid:48),t+1 = 0 if St+1 = g(cid:48) and otherwise equals γt+1, the environment discount. If we cannot
reach g(cid:48) from g under π, then Pπ,γ(g, g(cid:48)) will simply accumulate many zeros and be zero. We can
treat

as our new state space and plan in this space, to get value estimates v for all g

=

sterminal

v(g) = rπ,γ(g, g(cid:48)) + Pπ,γ(g, g(cid:48))v(g(cid:48)) where g(cid:48) = argmaxg(cid:48)∈
G ∪ {

Pπ,γ(g, g(cid:48))
if there is a terminal state (episodic problems) and otherwise ¯
G

where ¯
G
straightforward to show this converges, because Pπ,γ is a substochastic matrix (see Appendix A).
Once we have these values, we can propagate these to other states, locally, again using the closest g
to s. We can do so by noticing that the above deﬁnitions can be easily extended to rπ,γ(s, g(cid:48)) and
Pπ,γ(s, g(cid:48)), since for a pair (s, g) they are about starting in the state s and reaching g under π.

. It is

=

¯
G

G

}

(1)
Because the rhs of this equation is ﬁxed, we only cycle through these states once to get their values.

v(s) = rγ(s, g) + Pπ,γ(s, g)v(g) where g = argmaxg

Pπ,γ(s, g).

¯
G

∈

All of this might seem like a lot of work for policy evaluation; indeed, it will be more useful to
have this formalism for control. But, even here goal-space planning can be beneﬁcial. Let assume a
chain s1, s2, . . . , sn, where n = 1000 and
only
{
requires sweeping over 10 states, rather than 1000. Further, we have taken a 1000 horizon problem
and converted it into a 10 step one.3 As a result, changes in the environment also propagate faster.

s100, s200, . . . , s1000}

. Planning over g

∈ G

=

G

3In this simpliﬁed example, we can plan efﬁciently by updating the value at the end in sn, and then updating
states backwards from the end. But, without knowing this structure, it is not a general purpose strategy. For
general MDPs, we would need smart ways to do search control: the approach to pick states from one-step
updates. In fact, we can leverage search control strategies to improve the goal-space planning step. Then we get
the beneﬁt of these approaches, as well as the beneﬁt of planning over a much smaller state space.

3

If the reward at s(cid:48) changes, locally the reward model around s(cid:48) can be updated quickly, to change
rπ,γ(g, g(cid:48)) for pairs g, g(cid:48) where s(cid:48) is along the way from g to g(cid:48). This local change quickly updates
the values back to earlier ˜g

.

∈ G

4 Goal-Space Planning with Subgoal-Conditioned Models

Our objective is to allow planning to operate in a subgoal space while the policy operates in the
original MDP. To do so we need to ﬁrst specify how subgoals relate to states and deﬁne the models
required for planning and updating the policy. Then we discuss how to use these for planning and
ﬁnally summarize the overall goal-spacing planning framework, with a diagram in Figure 3.

4.1 Deﬁning Subgoals

, that need not be a subset of the (possibly
Assume we have a ﬁnite subset of subgoal vectors
continuous) space of state vectors. For example, g could correspond to a situation where both the
front and side distance sensors of a robot report low readings—what a person would call being in a
corner. Subgoal vectors could be a one-hot encoding or a vector of features describing the subgoal.

G

To fully specify a subgoal, we need a membership function m that indicates if a state s is a member
of subgoal g: m(s, g) = 1, and zero otherwise. Many states can be mapped to the same subgoal g.
For the above example, if the ﬁrst two elements of the state vector s consist of the front and side
distance sensor, m(s, g) = 1 for any states where s1, s2 are less than some threshold (cid:15). For a concrete
example, we visualize subgoals for the environment in our experiments in Figure 4.

Finally, we only reason about reaching subgoals from a subset of states, called initiation sets for
options [37]. This constraint is key for locality, to learn and reason about a subset of states for a
subgoal. We assume the existence of a (learned) initiation function d(s, g) that is 1 if s is in the
initiation set for g (e.g., sufﬁciently close in terms of reachability) and zero otherwise. We discuss
some approaches to learn this initiation function in Appendix C. But, here, we assume it is part of the
discovery procedure for the subgoals and ﬁrst focus on how to use it.

4.2 Deﬁning Subgoal-Conditioned Models

For planning and acting to operate in two different spaces, we deﬁne four models: two used in
planning over subgoals (subgoal-to-subgoal) and two used to project these subgoal values back into
the underlying state space (state-to-subgoal). Figure 1 visualizes these two spaces.

¯
The state-to-subgoal models are rγ :
G →
πg :
where m(˜s, g) = 1. The reward-model rγ(s, g) is the discounted rewards under option policy πg:

R and Γ :
[0, 1]. An option policy
[0, 1] for subgoal g starts from any s in the initiation set, and terminates in g—in ˜s

¯
G →

S × A →

S ×

S ×

rγ(s, g) = Eπg [Rt+1 + γg(St+1)rγ(St+1, g)

St = s]
|

where the discount is zero upon reaching subgoal g

γg(St+1) def=

(cid:40)

0
γt+1

if m(St+1, g) = 1, namely if subgoal g is achieved by being in St+1
else

The discount-model Γ(s, g) reﬂects the discounted number of steps until reaching subgoal g starting
from s, in expectation under option policy πg

Γ(s, g) = Eπg [m(St+1, g)γt+1 + γg(St+1)Γ(St+1, g)

St = s].
|

These state-to-subgoal will only be queried for (s, g) where d(s, g) > 0: they are local models.

To deﬁne subgoal-to-subgoal models,4 ˜rγ :
state-to-subgoal models. For each subgoal g

R and ˜Γ :

[0, 1], we use the
, we aggregate rγ(s, g(cid:48)) for all s where m(s, g) = 1.

G ×

¯
G →

¯
G →

G ×

∈ G

˜rγ(g, g(cid:48)) def= 1
z(g)

(cid:80)

s:m(s,g)=1 rγ(s, g(cid:48))

and

˜Γ(g, g(cid:48)) def= 1
z(g)

(cid:80)

s:m(s,g)=1 Γ(s, g(cid:48))

(2)

4The ﬁrst input is any g ∈ G, the second is g(cid:48) ∈ ¯G, which includes sterminal. We need to reason about reaching
any subgoal or sterminal. But sterminal is not a real state: we do not reason about starting from it to reach subgoals.

4

for normalizer z(g) def= (cid:80)
s:m(s,g)=1 m(s, g). This deﬁnition assumes a uniform weighting over the
states s where m(s, g) = 1. We could allow a non-uniform weighting, potentially based on visitation
frequency in the environment. For this work, however, we assume that m(s, g) = 1 for a smaller
number of states s with relatively similar rγ(s, g(cid:48)), making a uniform weighting reasonable.
These models are also local models, as we can similarly extract ˜d(g, g(cid:48)) from d(s, g(cid:48)) and only reason
about g(cid:48) nearby or relevant to g. We set ˜d(g, g(cid:48)) = maxs
:m(s,g)>0 d(s, g(cid:48)), indicating that if there
is a state s that is in the initiation set for g(cid:48) and has membership in g, then g(cid:48) is also relevant to g.

∈S

Let us consider an example, in Figure 1. The red states
are members of g (m(A, g) = 1) and the blue members
of g(cid:48) (m(X, g(cid:48)) = 1,m(Y, g(cid:48)) = 1). For all s in the
diagram, d(s, g(cid:48)) > 0 (all are in the initiation set): the
policy πg(cid:48) can be queried from any s to get to g(cid:48). The
green path in the left indicates the trajectory under πg(cid:48)
from A, stochastically reaching either X or Y , with
accumulated reward rγ(A, g(cid:48)) and discount Γ(A, g(cid:48))
(averaged over reaching X and Y ). The subgoal-to-
subgoal models, on the right, indicate g(cid:48) can be reached from g, with ˜rγ(g, g(cid:48)) averaged over both
rγ(A, g(cid:48)) and rγ(B, g(cid:48)) and ˜Γ(g, g(cid:48)) over Γ(A, g(cid:48)) and Γ(B, g(cid:48)), described in Equation (2).

Figure 1: Original and Abstract Space.

4.3 Goal-Space Planning with Subgoal-Conditioned Models

We can now consider how to plan with these models. Planning involves learning ˜v(g): the value for
different subgoals. This can be achieved using an update similar to value iteration, for all g
: ˜d(g,g(cid:48))>0 ˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))˜v(g(cid:48)).

(Background Planning)

∈ G

(3)

˜v(g) = maxg(cid:48)∈

¯
G

The value of reaching g(cid:48) from g is the discounted rewards along the way, ˜rγ(g, g(cid:48)), plus the discounted
value in g(cid:48). If ˜Γ(g, g(cid:48)) is very small, it is difﬁcult to reach g(cid:48) from g—or takes many steps—and so
the value in g(cid:48) is discounted by more. With a relatively small number of subgoals, we can sweep
through them all to quickly compute ˜v(g). With a larger set of subgoals, we can instead do as many
updates possible, in the background on each step, by stochastically sampling g.

, 2) the actions from g

We can interpret this update as a standard value iteration update in a new MDP, where 1) the set
of states is
are state-dependent, corresponding to choosing which
to go to in the set where ˜d(g, g(cid:48)) > 0 and 3) the rewards are ˜rγ and the discounted transition
¯
g(cid:48)
G
probabilities are ˜Γ. Under this correspondence, it is straightforward to show that the above converges
to the optimal values in this new Goal-Space MDP, shown in Proposition 2 in Appendix B.

∈ G

∈

G

This goal-space planning approach does not suffer from typical issues with model-based RL. First,
the model is not iterated, but we still obtain temporal abstraction because the model itself incorporates
it. Second, we do not need to predict entire state vectors—or distributions over them—because we
instead input the outcome g(cid:48) into the function approximator. This may feel like a false success as
it potentially requires restricting ourselves to a smaller number of subgoals. If we want to use a
larger number of subgoals, then we may need a function to generate these subgoal vectors anyway—
bringing us back to the problem of generating vectors. However, this is likely easier as 1) the subgoals
themselves can be much smaller and more abstract, making it more feasibly to procedurally generate
them and 2) it may be more feasible maintain a large set of subgoal vectors, or generate individual
subgoal vectors, than producing relevant subgoal vectors from a given subgoal.

Now let us examine how to use ˜v(g) to update our main policy. The simplest way to decide how to
behave from a state is to cycle through the subgoals, and pick the one with the highest value.

vsub(s) def= maxg

:d(s,g)>0 rγ(s, g) + Γ(s, g)˜v(g)
¯
G

∈

(Projection Step)

(4)

and take action a that corresponds to the action given by πg for this maximizing g. However, this
approach has two issues. First restricting to going through subgoals might result in suboptimal
policies. From a given state s, the set of relevant subgoals g may not be on the optimal path. Second,
the learned models themselves may have inaccuracies, or planning may not have been completed in
the background, resulting in ˜v(g) that are not yet fully accurate.

5

ABXYgg’˜r (g,g0)<latexit sha1_base64="RT8Gm6hEaZhFO30hfO8Vtyd1VtI=">AAAB/3icbVDLSsNAFJ3UV62vqODGTbCIFaQkUtFl0Y3LCvYBTQiTySQdOjMJMxOhxC78FTcuFHHrb7jzb5y2WWjrgQuHc+7l3nuClBKpbPvbKC0tr6yuldcrG5tb2zvm7l5HJplAuI0SmoheACWmhOO2IoriXiowZAHF3WB4M/G7D1hIkvB7NUqxx2DMSUQQVFryzQNXERriXIx9N4aMwVp8Fp+c+mbVrttTWIvEKUgVFGj55pcbJihjmCtEoZR9x06Vl0OhCKJ4XHEziVOIhjDGfU05ZFh6+fT+sXWsldCKEqGLK2uq/p7IIZNyxALdyaAayHlvIv7n9TMVXXk54WmmMEezRVFGLZVYkzCskAiMFB1pApEg+lYLDaCASOnIKjoEZ/7lRdI5rzuN+sVdo9q8LuIog0NwBGrAAZegCW5BC7QBAo/gGbyCN+PJeDHejY9Za8koZvbBHxifPz4BlZc=</latexit>˜ (g,g0)<latexit sha1_base64="DS/p4QEGq7EDkE2JCczhXPEznMA=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5GSxiBSmJVHRZdKHLCvYBTSiTyaQdOjMJMxOhhuKvuHGhiFv/w51/47TNQqsHLhzOuZd77wkSRpV2nC+rsLC4tLxSXC2trW9sbtnbOy0VpxKTJo5ZLDsBUoRRQZqaakY6iSSIB4y0g+HVxG/fE6loLO70KCE+R31BI4qRNlLP3vM0ZSHJvGvEORpX+if9o+OeXXaqzhTwL3FzUgY5Gj370wtjnHIiNGZIqa7rJNrPkNQUMzIueakiCcJD1CddQwXiRPnZ9PoxPDRKCKNYmhIaTtWfExniSo14YDo50gM1703E/7xuqqMLP6MiSTUReLYoShnUMZxEAUMqCdZsZAjCkppbIR4gibA2gZVMCO78y39J67Tq1qpnt7Vy/TKPowj2wQGoABecgzq4AQ3QBBg8gCfwAl6tR+vZerPeZ60FK5/ZBb9gfXwDc12Ukg==</latexit>⇡g0<latexit sha1_base64="WOgL0k4BRYnHD1UJS6YMAVo4UC0=">AAAB73icdVDJSgNBEK2JW4xb1KOXxiB6GmYmGRNvQS8eI5gFkiH0dHqSJj2L3T1CGPITXjwo4tXf8ebf2FkEFX1Q8Hiviqp6fsKZVJb1YeRWVtfWN/Kbha3tnd294v5BS8apILRJYh6Ljo8l5SyiTcUUp51EUBz6nLb98dXMb99TIVkc3apJQr0QDyMWMIKVljq9hPWz4em0XyxZpl1xbdtFlnledh2npknZKl9UHWSb1hwlWKLRL773BjFJQxopwrGUXdtKlJdhoRjhdFropZImmIzxkHY1jXBIpZfN752iE60MUBALXZFCc/X7RIZDKSehrztDrEbytzcT//K6qQpqXsaiJFU0IotFQcqRitHseTRgghLFJ5pgIpi+FZERFpgoHVFBh/D1KfqftBwdlOneVEr1y2UceTiCYzgDG6pQh2toQBMIcHiAJ3g27oxH48V4XbTmjOXMIfyA8fYJVNSQLw==</latexit>Original MDPSubgoal abstractionm(Y,g0)=1<latexit sha1_base64="SCTbVkQ5X+egz7ODMuETUuoO5o8=">AAAB8nicdVDLSsNAFJ3UV62vqks3g0WsICEJqY0LoejGZQX7kDSUyXTaDp1kwsxEKKGf4caFIm79Gnf+jdOHoKIHLhzOuZd77wkTRqWyrA8jt7S8srqWXy9sbG5t7xR395qSpwKTBuaMi3aIJGE0Jg1FFSPtRBAUhYy0wtHV1G/dEyEpj2/VOCFBhAYx7VOMlJb8qHx3Ojg+gRfQ7hZLlmmfec55FVpmxfasqqeJ57iu7UDbtGYogQXq3eJ7p8dxGpFYYYak9G0rUUGGhKKYkUmhk0qSIDxCA+JrGqOIyCCbnTyBR1rpwT4XumIFZ+r3iQxFUo6jUHdGSA3lb28q/uX5qep7QUbjJFUkxvNF/ZRBxeH0f9ijgmDFxpogLKi+FeIhEggrnVJBh/D1KfyfNB3Tds3KjVuqXS7iyIMDcAjKwAZVUAPXoA4aAAMOHsATeDaU8Wi8GK/z1pyxmNkHP2C8fQLz6o/C</latexit>m(X,g0)=1<latexit sha1_base64="tP2bBht0XS1PpZNC+0NSZZYY6QE=">AAAB8nicdVDLSsNAFJ34rPVVdelmsIgVJGRCauNCKLpxWcE+IA1lMp20QycPZiZCCf0MNy4UcevXuPNvnD4EFT1w4XDOvdx7T5ByJpVlfRhLyyura+uFjeLm1vbObmlvvyWTTBDaJAlPRCfAknIW06ZiitNOKiiOAk7bweh66rfvqZAsie/UOKV+hAcxCxnBSkteVOmcDU5O4SVEvVLZMtG5a1/UoGVWkWvVXE1c23GQDZFpzVAGCzR6pfduPyFZRGNFOJbSQ1aq/BwLxQink2I3kzTFZIQH1NM0xhGVfj47eQKPtdKHYSJ0xQrO1O8TOY6kHEeB7oywGsrf3lT8y/MyFbp+zuI0UzQm80VhxqFK4PR/2GeCEsXHmmAimL4VkiEWmCidUlGH8PUp/J+0bBM5ZvXWKdevFnEUwCE4AhWAQA3UwQ1ogCYgIAEP4Ak8G8p4NF6M13nrkrGYOQA/YLx9AvJej8E=</latexit>We instead propose to use vsub(s) within the bootstrap target for the action-values for the main policy.
For a given transition (St, At, Rt+1, St+1), either as the most recent experience or from a replay
buffer, the proposed subgoal-value bootstrapping update to parameterized q(St, At; w) uses TD error

δ def= Rt+1 + γt+1

(cid:16)

(1

−

β) maxa(cid:48)q(St+1, a(cid:48); w)
(cid:125)
(cid:123)(cid:122)
Standard bootstrap target

(cid:124)

+β vsub(St+1)
(cid:124)
(cid:125)
(cid:123)(cid:122)
Subgoal value

(cid:17)

−

q(St, At; w)

(5)

∈

for some β
[0, 1]. For β = 0, we get a standard Q-learning update.
For β = 1, we fully bootstrap off the value provided by vsub(St+1).
This may result in suboptimal values q(St, At; w), but should learn
faster because a reasonable estimate of value has been propagated
back quickly using goal-space planning. On the other hand, β = 0
is not biased by a potentially suboptimal ˜v(g), but does not take
advantage of this fast propagation. An interim β can allow for fast
propagation, but also help overcome suboptimality in the values.

Figure 2: Computing vsub(S(cid:48))
to update the policy at S.

We can show that the above update improves the convergence rate.
This result is intuitive: subgoal-value bootstrapping changes the
discount rate to γt+1(1
β). In the extreme case of β = 1, we are moving our estimate towards
Rt+1 +γt+1vsub(St+1) for vsub not based on q without any bootstrapping: it is effectively a regression
problem. We prove this intuitive result in Appendix B. One other beneﬁt of this approach is that the
initiation sets need not cover the whole space: we can have a state d(s, g) = 0 for all g. If this occurs,
we simply do not use vsub and bootstrap as usual.

−

4.4 Putting it All Together: The Full Goal-Space Planning Algorithm

The remaining piece is to learn the models and put it all to-
gether. Learning the models is straightforward, as we can
leverage the large literature on general value functions [38]
and UVFAs [30]. There are nuances involved in 1) restricting
updating to relevant states according to d(s, g), 2) learning
option policies that reach subgoals, but also maximize rewards
along the way and 3) considering ways to jointly learn d and
Γ. For space we include these details in Appendix C.

The algorithm is visualized in Figure 3 (pseudocode in appx.
C.3). The steps of agent-environment interaction include:
1) take action At in state St, to get St+1, Rt+1 and γt+1;
2) query the model for rγ(St+1, g), Γ(St+1, g), ˜v(g) for all g

where d(St+1, g) > 0;

3) compute projection vsub(St+1) using Eq. (4) and step 2;
4) update the main policy with the transition and vsub(St+1),

Figure 3: Goal-Space Planning.

using Eq. (5).

All background computation is used for model learning using

a replay buffer and for planning to obtain ˜v, so that they can be queried at any time on step 2.

5 Experiments with Goal-Space Planning

We investigate the utility of GSP, for 1) improving sample efﬁciency and 2) re-learning under non-
stationarity. We compare to Double DQN (DDQN) [45], which uses replay and target networks. We
layer GSP on top of this agent: the action-value update is modiﬁed to incorporate subgoal-value
bootstrapping. By selecting β = 0, we perfectly recover DDQN, allowing us to test different β values
to investigate the impact of incorporating subgoal values computed using background planning.

5.1 Experiment Speciﬁcation

We test the agents in the PinBall environment [20], which allows for a variety of easy and harder
instances to test different aspects. The agent has to navigate a small ball to a destination in a maze-like
environment with fully elastic and irregularly shaped obstacles. The state is described by 4 features:

6

max<latexit sha1_base64="rhZcRL7XZrigmGUL29uqSEmmsiU=">AAAB63icbVBNS8NAEJ3Ur1q/qh69LBbBU0mkoseiF48VTFtoQ9lsN+3S3U3Y3Ygl9C948aCIV/+QN/+NmzYHbX0w8Hhvhpl5YcKZNq777ZTW1jc2t8rblZ3dvf2D6uFRW8epItQnMY9VN8Saciapb5jhtJsoikXIaSec3OZ+55EqzWL5YKYJDQQeSRYxgk0u9QV+GlRrbt2dA60SryA1KNAaVL/6w5ikgkpDONa657mJCTKsDCOczir9VNMEkwke0Z6lEguqg2x+6wydWWWIoljZkgbN1d8TGRZaT0VoOwU2Y73s5eJ/Xi810XWQMZmkhkqyWBSlHJkY5Y+jIVOUGD61BBPF7K2IjLHCxNh4KjYEb/nlVdK+qHuN+uV9o9a8KeIowwmcwjl4cAVNuIMW+EBgDM/wCm+OcF6cd+dj0Vpyiplj+APn8wcdp45M</latexit>r (S0,gj)+ (S0,gj)˜v(gj)<latexit sha1_base64="4zdDmzB5pHR0BwHGEYBhBbmM4rc=">AAACGHicbVC7TsMwFHXKq5RXgJHFokIUgUqCimCsYICxCPqQmipyXCc1tZPIdipVUT+DhV9hYQAh1m78De5jKC1HutLxOffK9x4vZlQqy/oxMkvLK6tr2fXcxubW9o65u1eTUSIwqeKIRaLhIUkYDUlVUcVIIxYEcY+Rute9Hfn1HhGSRuGT6sekxVEQUp9ipLTkmufCdQLEOSo8Hp8F7vMJPIXO3azgKMraJO0NCqOXa+atojUGXCT2lOTBFBXXHDrtCCechAozJGXTtmLVSpFQFDMyyDmJJDHCXRSQpqYh4kS20vFhA3iklTb0I6ErVHCszk6kiEvZ557u5Eh15Lw3Ev/zmonyr1spDeNEkRBPPvITBlUERynBNhUEK9bXBGFB9a4Qd5BAWOksczoEe/7kRVK7KNql4uVDKV++mcaRBQfgEBSADa5AGdyDCqgCDF7AG/gAn8ar8W58Gd+T1owxndkHf2AMfwE8/p4O</latexit>r (S0,gi)+ (S0,gi)˜v(gi)<latexit sha1_base64="aFrOQHxx2EgLpiC7I2ZQcQ9cwa8=">AAACGHicbVDLSsNAFJ34rPUVdelmsIgVpSZS0WXRhS4r2gc0IUym03boTBJmJoUS+hlu/BU3LhRx251/46TNorYeuHDmnHuZe48fMSqVZf0YS8srq2vruY385tb2zq65t1+XYSwwqeGQhaLpI0kYDUhNUcVIMxIEcZ+Rht+/S/3GgAhJw+BZDSPictQNaIdipLTkmRfCc7qIc1R8OjnvevQUnkHnflZwFGVtkgxGxfTlmQWrZE0AF4mdkQLIUPXMsdMOccxJoDBDUrZsK1JugoSimJFR3okliRDuoy5paRogTqSbTA4bwWOttGEnFLoCBSfq7ESCuJRD7utOjlRPznup+J/XilXnxk1oEMWKBHj6USdmUIUwTQm2qSBYsaEmCAuqd4W4hwTCSmeZ1yHY8ycvkvplyS6Xrh7LhcptFkcOHIIjUAQ2uAYV8ACqoAYweAFv4AN8Gq/Gu/FlfE9bl4xs5gD8gTH+BThCngs=</latexit>r (S0,gk)+ (S0,gk)˜v(gk)<latexit sha1_base64="S2hNdSEkiDEIbxmwI3Jy9iyFGkg=">AAACGHicbVDLSgMxFM34rPVVdekmWMSKUmekosuiC11WtA/olCGTybShSWZIMoUy9DPc+CtuXCjitjv/xvSxqK0HLpyccy+59/gxo0rb9o+1tLyyurae2chubm3v7Ob29msqSiQmVRyxSDZ8pAijglQ11Yw0YkkQ9xmp+927kV/vEaloJJ51PyYtjtqChhQjbSQvdyE9t404R4Wnk/O21z2FZ9C9nxVcTVlA0t6gMHp5ubxdtMeAi8SZkjyYouLlhm4Q4YQToTFDSjUdO9atFElNMSODrJsoEiPcRW3SNFQgTlQrHR82gMdGCWAYSVNCw7E6O5EirlSf+6aTI91R895I/M9rJjq8aaVUxIkmAk8+ChMGdQRHKcGASoI16xuCsKRmV4g7SCKsTZZZE4Izf/IiqV0WnVLx6rGUL99O48iAQ3AECsAB16AMHkAFVAEGL+ANfIBP69V6t76s70nrkjWdOQB/YA1/AUG6nhE=</latexit>gi<latexit sha1_base64="42XO07Z7Y748l6NPq70vC9ghOto=">AAAB6nicdVDJSgNBEK2JW4xb1KOXxiB4GmZCFr0FvXiMaBZIhtDT6Zk06ekZunuEMOQTvHhQxKtf5M2/sbMIrg8KHu9VUVXPTzhT2nHerdzK6tr6Rn6zsLW9s7tX3D9oqziVhLZIzGPZ9bGinAna0kxz2k0kxZHPaccfX878zh2VisXiVk8S6kU4FCxgBGsj3YQDNiiWHPu8XnZrNfSbuLYzRwmWaA6Kb/1hTNKICk04VqrnOon2Miw1I5xOC/1U0QSTMQ5pz1CBI6q8bH7qFJ0YZYiCWJoSGs3VrxMZjpSaRL7pjLAeqZ/eTPzL66U6OPMyJpJUU0EWi4KUIx2j2d9oyCQlmk8MwUQycysiIywx0Sadggnh81P0P2mXbbdiV68rpcbFMo48HMExnIILdWjAFTShBQRCuIdHeLK49WA9Wy+L1py1nDmEb7BePwDP944s</latexit>gk<latexit sha1_base64="5ONEzQz+z1XQF3JdEKtsvey7PyU=">AAAB6nicdVDLSsNAFL2pr1pfVZduBovgKiS2mnZXdOOyon1AG8pkOkmHTh7MTIQS+gluXCji1i9y5984TSuo6IELh3Pu5d57vIQzqSzrwyisrK6tbxQ3S1vbO7t75f2DjoxTQWibxDwWPQ9LyllE24opTnuJoDj0OO16k6u5372nQrI4ulPThLohDiLmM4KVlm6D4WRYrlhmw6k1qjbSpO5YVScnF07DRrZp5ajAEq1h+X0wikka0kgRjqXs21ai3AwLxQins9IglTTBZIID2tc0wiGVbpafOkMnWhkhPxa6IoVy9ftEhkMpp6GnO0OsxvK3Nxf/8vqp8utuxqIkVTQii0V+ypGK0fxvNGKCEsWnmmAimL4VkTEWmCidTkmH8PUp+p90zky7Zp7f1CrNy2UcRTiCYzgFGxxowjW0oA0EAniAJ3g2uPFovBivi9aCsZw5hB8w3j4B5yCOPA==</latexit>S0<latexit sha1_base64="pajwFsJh5sgT/00nh3jwcoxsoso=">AAAB6XicbVDLTgJBEOzFF+IL9ehlIjF6IrsGo0eiF4/44JEAIbPDLEyYnd3M9JqQDX/gxYPGePWPvPk3DrAHBSvppFLVne4uP5bCoOt+O7mV1bX1jfxmYWt7Z3evuH/QMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbqZ+84lrIyL1iOOYd0M6UCIQjKKV7h9Oe8WSW3ZnIMvEy0gJMtR6xa9OP2JJyBUySY1pe26M3ZRqFEzySaGTGB5TNqID3rZU0ZCbbjq7dEJOrNInQaRtKSQz9fdESkNjxqFvO0OKQ7PoTcX/vHaCwVU3FSpOkCs2XxQkkmBEpm+TvtCcoRxbQpkW9lbChlRThjacgg3BW3x5mTTOy16lfHFXKVWvszjycATHcAYeXEIVbqEGdWAQwDO8wpszcl6cd+dj3ppzsplD+APn8wcRH40Q</latexit>S<latexit sha1_base64="ubwbK8cGZkWbMLWFFThCwgBxy5s=">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHYNRo9ELx4hyiOBDZkdemFkdnYzM2tCCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR7cxvPaHSPJYPZpygH9GB5CFn1Fipft8rltyyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmhdlr1K+rFdK1ZssjjycwCmcgwdXUIU7qEEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+ALC9jN8=</latexit>gj<latexit sha1_base64="/tG7F9mO1gBoiYzD/oTAg+6AFIE=">AAAB6nicdVBNS8NAEJ3Ur1q/qh69LBbBU0hsNe2t6MVjRVsLbSib7SZdu/lgdyOU0J/gxYMiXv1F3vw3btMKKvpg4PHeDDPzvIQzqSzrwygsLa+srhXXSxubW9s75d29joxTQWibxDwWXQ9LyllE24opTruJoDj0OL31xhcz//aeCsni6EZNEuqGOIiYzwhWWroOBneDcsUyG06tUbWRJnXHqjo5OXMaNrJNK0cFFmgNyu/9YUzSkEaKcCxlz7YS5WZYKEY4nZb6qaQJJmMc0J6mEQ6pdLP81Ck60soQ+bHQFSmUq98nMhxKOQk93RliNZK/vZn4l9dLlV93MxYlqaIRmS/yU45UjGZ/oyETlCg+0QQTwfStiIywwETpdEo6hK9P0f+kc2LaNfP0qlZpni/iKMIBHMIx2OBAEy6hBW0gEMADPMGzwY1H48V4nbcWjMXMPvyA8fYJ5ZyOOw==</latexit>PolicyupdateQ via Eq 5sample actionModelplanvia Eq 3projectvia Eq 4Environment ,r,˜ ,˜r<latexit sha1_base64="I1xhHI+Li9cYkRVsgWrw5SViKkQ=">AAACDHicbVDLSgMxFM3UV62vqks3wSK4KGVGKrosutBlBfuAzlDuZDJtaDIzJBmhDP0AN/6KGxeKuPUD3Pk3pu0stPVA4OScc0nu8RPOlLbtb6uwsrq2vlHcLG1t7+zulfcP2ipOJaEtEvNYdn1QlLOItjTTnHYTSUH4nHb80fXU7zxQqVgc3etxQj0Bg4iFjIA2Ur9ccW9ACKhiWcWuZjyg2VyZVPOrnJiUXbNnwMvEyUkF5Wj2y19uEJNU0EgTDkr1HDvRXgZSM8LppOSmiiZARjCgPUMjEFR52WyZCT4xSoDDWJoTaTxTf09kIJQaC98kBeihWvSm4n9eL9XhpZexKEk1jcj8oTDlWMd42gwOmKRE87EhQCQzf8VkCBKINv2VTAnO4srLpH1Wc+q187t6pXGV11FER+gYnSIHXaAGukVN1EIEPaJn9IrerCfrxXq3PubRgpXPHKI/sD5/AAbDmwM=</latexit>S0<latexit sha1_base64="pajwFsJh5sgT/00nh3jwcoxsoso=">AAAB6XicbVDLTgJBEOzFF+IL9ehlIjF6IrsGo0eiF4/44JEAIbPDLEyYnd3M9JqQDX/gxYPGePWPvPk3DrAHBSvppFLVne4uP5bCoOt+O7mV1bX1jfxmYWt7Z3evuH/QMFGiGa+zSEa65VPDpVC8jgIlb8Wa09CXvOmPbqZ+84lrIyL1iOOYd0M6UCIQjKKV7h9Oe8WSW3ZnIMvEy0gJMtR6xa9OP2JJyBUySY1pe26M3ZRqFEzySaGTGB5TNqID3rZU0ZCbbjq7dEJOrNInQaRtKSQz9fdESkNjxqFvO0OKQ7PoTcX/vHaCwVU3FSpOkCs2XxQkkmBEpm+TvtCcoRxbQpkW9lbChlRThjacgg3BW3x5mTTOy16lfHFXKVWvszjycATHcAYeXEIVbqEGdWAQwDO8wpszcl6cd+dj3ppzsplD+APn8wcRH40Q</latexit>{S,A,S0, ,R}<latexit sha1_base64="eDmMXgrLjU15wtAg0RudcqupQ70=">AAACAHicbVC7TsMwFHXKq5RXgIGBxaJCMERVgopgLLAwFkofUhNVjuu0Vu0ksh2kKsrCr7AwgBArn8HG3+C2GaDlSFc6Oude3XuPHzMqlW1/G4Wl5ZXVteJ6aWNza3vH3N1rySgRmDRxxCLR8ZEkjIakqahipBMLgrjPSNsf3Uz89iMRkkbhgxrHxONoENKAYqS01DMP3LRhwSsLNk4s6A4Q58iC927WM8t2xZ4CLhInJ2WQo94zv9x+hBNOQoUZkrLr2LHyUiQUxYxkJTeRJEZ4hAakq2mIOJFeOn0gg8da6cMgErpCBafq74kUcSnH3NedHKmhnPcm4n9eN1HBpZfSME4UCfFsUZAwqCI4SQP2qSBYsbEmCAuqb4V4iATCSmdW0iE48y8vktZZxalWzu+q5dp1HkcRHIIjcAoccAFq4BbUQRNgkIFn8ArejCfjxXg3PmatBSOf2Qd/YHz+ACd0lDg=</latexit>A<latexit sha1_base64="VLG3tZCiZVGlgrCQDyStd2yHkFI=">AAAB6HicbVDLTgJBEOzFF+IL9ehlIjHxRHYNRo+oF4+QyCOBDZkdemFkdnYzM2tCCF/gxYPGePWTvPk3DrAHBSvppFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsGG4EthOFNAoEtoLR3cxvPaHSPJYPZpygH9GB5CFn1FipftMrltyyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQQE5PZ16TPFTIjxpZQpri9lbAhVZQZm03BhuAtv7xKmhdlr1K+rFdK1dssjjycwCmcgwdXUIV7qEEDGCA8wyu8OY/Oi/PufCxac042cwx/4Hz+AJV1jM0=</latexit>update ˜v(g)<latexit sha1_base64="luzoge/Y2lwP6G6qni/H4AOf3GI=">AAAB83icbVBNS8NAEJ3Ur1q/qh69BItQLyWRih6LXjxWsB/QhLLZTtqlm03Y3RRK6N/w4kERr/4Zb/4bt20O2vpg4PHeDDPzgoQzpR3n2ypsbG5t7xR3S3v7B4dH5eOTtopTSbFFYx7LbkAUciawpZnm2E0kkijg2AnG93O/M0GpWCye9DRBPyJDwUJGiTaS52nGB5hNZtXhZb9ccWrOAvY6cXNSgRzNfvnLG8Q0jVBoyolSPddJtJ8RqRnlOCt5qcKE0DEZYs9QQSJUfra4eWZfGGVgh7E0JbS9UH9PZCRSahoFpjMieqRWvbn4n9dLdXjrZ0wkqUZBl4vClNs6tucB2AMmkWo+NYRQycytNh0RSag2MZVMCO7qy+ukfVVz67Xrx3qlcZfHUYQzOIcquHADDXiAJrSAQgLP8ApvVmq9WO/Wx7K1YOUzp/AH1ucP1QeRjg==</latexit> ,r<latexit sha1_base64="zptaDiVhOH4+/Cy6z/nCRUIE1WU=">AAAB73icbVDLSgNBEOyNrxhfUY9eFoPgQcKuRPQY9KDHCOYByRJ6J5NkyMzsOjMrhCU/4cWDIl79HW/+jZNkD5pY0FBUddPdFcacaeN5305uZXVtfSO/Wdja3tndK+4fNHSUKELrJOKRaoWoKWeS1g0znLZiRVGEnDbD0c3Ubz5RpVkkH8w4poHAgWR9RtBYqdW5RSHwTHWLJa/szeAuEz8jJchQ6xa/Or2IJIJKQzhq3fa92AQpKsMIp5NCJ9E0RjLCAW1bKlFQHaSzeyfuiVV6bj9StqRxZ+rviRSF1mMR2k6BZqgXvan4n9dOTP8qSJmME0MlmS/qJ9w1kTt93u0xRYnhY0uQKGZvdckQFRJjIyrYEPzFl5dJ47zsV8oX95VS9TqLIw9HcAyn4MMlVOEOalAHAhye4RXenEfnxXl3PuatOSebOYQ/cD5/AJaSj68=</latexit>vsub(S0)<latexit sha1_base64="RBvFuF+qq4whJ89/zO98ZsL6BDM=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRaxbkoiFV0W3bisaB/QhjCZTtqhk0mYuSnWEPwVNy4Ucet/uPNvnD4W2nrgwuGce7n3Hj/mTIFtfxu5peWV1bX8emFjc2t7x9zda6gokYTWScQj2fKxopwJWgcGnLZiSXHoc9r0B9djvzmkUrFI3MMopm6Ie4IFjGDQkmcepMPMSztAHyBViZ9lpbuTU88s2mV7AmuRODNSRDPUPPOr041IElIBhGOl2o4dg5tiCYxwmhU6iaIxJgPco21NBQ6pctPJ9Zl1rJWuFURSlwBrov6eSHGo1Cj0dWeIoa/mvbH4n9dOILh0UybiBKgg00VBwi2IrHEUVpdJSoCPNMFEMn2rRfpYYgI6sIIOwZl/eZE0zspOpXx+WylWr2Zx5NEhOkIl5KALVEU3qIbqiKBH9Ixe0ZvxZLwY78bHtDVnzGb20R8Ynz/J+5Vu</latexit>Figure 4: (left) The harder PinBall environment used in our ﬁrst experiment. The dark gray shapes
are obstacles the ball bounces off of, the small blue circle the starting position of the ball (with
no velocity), and the red dot the goal (termination). Solid circles indicate the location and radius
of the subgoals (m), with wider initiation set visualized for two subgoals (pink and teal). (right)
Performance in this environment for GSP with a variety of β and DDQN (which is GSP with β = 0),
with the standard error shown. Even just increasing to β = 0.1 allows GSP to leverage the longer-
horizon estimates given by the subgoal values, making it learn much faster than DDQN. Once β is at
1, where it fully bootstraps off of potentially suboptimal subgoal values, GSP still learns quickly but
levels off at a suboptimal value, as expected.

[

∈

∈

∈

−

1, 1], ˙y

[0, 1], y

[0, 1], ˙x

1, 1]. The agent has 5 discrete actions: increase/decrease ˙x,
x
increase/decrease ˙y, and nothing. The agent receives a reward of -5 per step and a reward of 10,000
upon termination at the goal location. PinBall has a continuous state space with complex and sharp
dynamics that make learning and control difﬁcult. We used a harder version of PinBall in our ﬁrst
experiment, shown in Figure 4, and simpler one for the non-stationary experiment, shown in Figure 8,
to allow DDQN a better chance to adapt under non-stationarity.

[
−

∈

The hyperparameters are chosen based on sweeping for DDQN performance. We then ﬁxed these
hyperparameters, and used them for GSP. This approach helps ensure they have similar settings, with
the primary difference due to incorporating subgoal-value bootstrapping. We used neural networks
with ReLU activations and (cid:15) = 0.1; details about hyperparameters are in Appendix F.

The set of subgoals for GSP are chosen to cover the environment in terms of (x, y) locations. For each
subgoal g with location (xg, yg), we set m(s, g) = 1 for s = (x, y, ˙x, ˙y) if the Euclidean distance
between (x, y) and (xg, yg) is below 0.035. Using a region, rather than requiring (x, y) = (xg, yg),
is necessary for a continuous state space. The agent’s velocity is not taken into account for subgoal
termination. The width of the region for the initiation function is 0.4. More details about the layout
of the environment, positions of these subgoals and initiation functions are shown in Figure 4.

5.2 Experiment 1: Comparing DDQN and GSP with Pre-learned Models

We ﬁrst investigate the utility of the models after they have been learned in a pre-training phase.
The models use the same updates as they would when being learned online, and are not perfectly
accurate. Pre-training the model allows us to ask: if the GSP agent had previously learned a model in
the environment—or had ofﬂine data to train its model—can it leverage it to learn faster now? One of
the primary goals of model-based RL is precisely this re-use, and so it is natural to start in a setting
mimicking this use-case. We assume the GSP agent can do many steps of background planning,
so that ˜v is effectively computed in early learning; this is reasonable as we only need to do value
iteration for 9 subgoals, which is fast. We test GSP with β

3, 0.1, 0.5, 1.0].

[10−

We see in Figure 4 that GSP learns much faster than DDQN, and reaches the same level of performance.
This is the result we should expect—GSP gets to leverage a pre-trained model, after all—but it is
an important sanity check that using models in this new way is effective. Of particular note is that
even just increasing β from 0 (which is DDQN) to β = 0.1 provides the learning speed boost without
resulting in suboptimal performance. Likely, in early learning, the suboptimal subgoal values provide
a coarse direction to follow, to more quickly update the action-values, which is then reﬁned with
more learning. We can see that for β = 0.5 and β = 1, we similarly get fast initial learning, but it
3 very close to zero, we see that performance is
plateaus at a more suboptimal point. For β = 10−
more like DDQN. But even for such a small β we get improvements.

∈

7

0500100015002000250001020304050GSP (β = 0.1)GSP (β = 0.5)Steps (x100)GSP (β = 1e-3)DDQNGSP (β = 1.0)Reward
Rate
Averaged
over
30 RunsFigure 5: (left) Visualizing the action-values for DDQN and GSP (β = 0.1) at various points in
training. (right) vsub obtained from using the learned subgoal-values in the projection step.

To further investigate the hypothesis that GSP more quickly changes its value function early in
learning, we visualize the value functions for both GSP and DDQN over time in Figure 5. After 2000
steps, they are not yet that different, because there are only four replay updates on each step and it
takes time to visit the state-space and update values by bootstrapping off of subgoal values. By step
6000, though, GSP already has some of the structure of the problem, whereas DDQN has simply
pushed down many of its values (darker blue).

5.3 Accuracy of the Learned Models

One potential beneﬁt of GSP is that the models themselves may be
easier to learn, because we can leverage standard value function
learning algorithms. We visualize the models learned for the
previous experiment, as well as the resulting vsub, with details
about model learning in Appendix E.

In Figure 6 we see how learned state-to-subgoal models accurately
learn the structure. Each plot shows the learned state-to-subgoal
for one subgoal, visualized only for the initiation set d(s, g) > 0.
We can see larger discount and reward values predicted based on
reachability. However, the models are not perfect. We measured
model error and ﬁnd it is reasonable but not very near zero (see
Appendix E). This result is actually encouraging: inaccuracies in
the model do not prevent useful planning.

Figure 6:
Learned state-to-
subgoal models. White indicates
d(s, g) = 0.

It is informative to visualize vsub. We can see in Figure 5 that the general structure is correct, matching
the optimal path, but that it indeed looks suboptimal compared to the ﬁnal values computed in Figure
5 by DDQN. This inaccuracy is likely due both to some inaccuracy in the models, as well as the fact
that subgoal placement is not optimal. This explains why GSP has lower values particularly in states
near the bottom, likely skewed downwards by vsub.

Finally, we test the impact on learning using less accurate
models. After all, the agent will want to start using its
model as soon as possible, rather than waiting for it to
become more accurate. We ran GSP using models learned
online, using only 50k, 75k and 100k time steps to learn
the models. We then froze the models and allowed GSP
to learn with them. We can see in Figure 7 that learning
with too inaccurate of a model—with 50k—fails, but
already with 75k performance improves considerably and
with 100k we are already nearly at the same level of
optimal performance as the pre-learned models. This
result highlights it should be feasible to learn and use
these models in GSP, all online.

Figure 7: The impact on planning perfor-
mance using frozen models with differing
accuracy (shading shows standard error).

5.4 Experiment 2: Adapting in Nonstationary PinBall

Now we consider another typical use-case for model-based RL: quickly adapting to changes in the
environment. We let the agent learn in PinBall for 50k steps, and then switch the goal to a new
location for another 50k steps. Goal information is never given to the agent, so it has to visit the old

8

20006000100002200040000100000GSPDDQN0500100015002000250004080120160100k75k50kSteps (x100)Reward
Rate
Averaged
over 
30 RunsFigure 8: (left) The Non-stationary PinBall environment. For the ﬁrst half of the experiment, the
agent terminates at goal A while for the second half, the agent terminates at goal B. (right) The
performance of GSP (β = 0.1) and DDQN in the environment. The mean of all 30 runs is shown as
the dashed line. The 25th and 75th percentile run for each algorithm are also highlighted. We see
that GSP with exploration bonus was able to adapt more quickly when the terminal goal switches
compared to the baseline DDQN algorithm where goal values are not used.

goal, realize it is no longer rewarding, and re-explore to ﬁnd the new goal. This non-stationary setting
is harder for DDQN, so we use a simpler conﬁguration for PinBall, shown in Figure 8.

We can leverage the idea of exploration bonuses, introduced in Dyna-Q+ [36]. Exploration bonuses
are proportional to the last time that state-action was visited. This encourages the agent to revisit
parts of the state-space that it has not seen recently, in case that part of the world has changed. For
us, this corresponds to including reward bonus rbonus in the planning and projection steps: ˜v(g) =
maxg(cid:48)∈
:d(s,g)>0rγ(s, g) +
¯
G
Γ(s, g) (cid:0)˜v(g) + rbonus(g)(cid:1). Because we have a small, ﬁnite set of subgoals, it is straightforward to
leverage this idea that was designed for the tabular setting. We use rbonus(g) = 1000 if the count for
g is zero, and 0 otherwise. When the world changes, the agent recognizes that it has changed, and
resets all counts for the subgoals. Similarly, both agents (GSP and DDQN) clear their replay buffers.

: ˜d(g,g(cid:48))>0˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48)) (cid:0)˜v(g(cid:48)) + rbonus(g(cid:48))(cid:1) and vsub(s) = maxg

¯
G

∈

The GSP agent can recognize the world has changed, but not how it has changed. It has to update
its models with experience. The state-to-subgoal models and subgoal-to-subgoal models local to
the previous terminal state location and the new one need to change, but the rest of the models are
actually already accurate. The agent can leverage this existing accuracy.

In Figure 8, we can see both GSP and DDQN drop in performance when the environment changes,
with GSP recovering much more quickly. It is always possible that an inaccurate model might actually
make re-learning slower, reinforcing incorrect values from the model. Here, though, updating these
local models is fast, allowing the subgoal values to also be updated quickly. Though not shown in the
plot, GSP without exploration bonuses performs poorly. Its model causes it to avoid visiting the new
goal region, so preventing the model from updating, because the value in that bottom corner is low.

6 Conclusion

In this paper we introduced a new planning framework, called Goal-Space Planning (GSP). The key
idea is to plan in a much smaller space of subgoals, and use these (high-level) subgoal values to
update state values using subgoal-conditioned models. We show that, in the PinBall environment,
that 1) the subgoal-conditioned models can be accurately learned using standard value estimation
algorithms and 2) GSP can signiﬁcantly improve speed of learning, over Double DQN. The formalism
avoids learning transition dynamics and iterating models, two of the sources of failure in previous
model-based RL algorithms. GSP provides a new approach to incorporate background planning to
improve action-value estimates, with minimalist, local models and computationally efﬁcient planning.

This work introduces a new formalism, and many new technical questions along with it. We have only
tested GSP with pre-learned models and assumed a given set of subgoals. Our initial experiments
learning the models online, from scratch, indicate that GSP can get similar learning speed boosts.
Using a simple recency buffer, however, accumulates transitions only along the optimal trajectory,
sometimes causing the models to become highly inaccurate part-way through learning, causing

9

AB0200400600800050100150200GSP (β = 0.1)DDQNSteps (x100)Reward
RateGSP to fail. An important next step is to incorporate smarter strategies, such as curating the replay
buffer, to learn these models online. The other critical open question is in subgoal discovery. We
somewhat randomly selected subgoals across the PinBall environment, with a successful outcome;
such an approach is unlikely to work in many environments. In general, option discovery and subgoal
discovery remain open questions. One utility of this work is that it could help narrow the scope of the
discovery question, to that of ﬁnding abstract subgoals that help the agent plan more efﬁciently.

References

[1] Arthur Aubret, Laetitia matignon, and Salima Hassas. DisTop: Discovering a Topological

representation to learn diverse and rewarding skills. arXiv:2106.03853 [cs], 2021.

[2] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-Based Rein-
forcement Learning with Value-Targeted Regression. In International Conference on Machine
Learning, 2020.

[3] Andre Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Aygün, Philippe Hamel,
Daniel Toyama, Jonathan hunt, Shibl Mourad, David Silver, and Doina Precup. The Option
Keyboard: Combining Skills in Reinforcement Learning. In Advances in Neural Information
Processing Systems, 2019.

[4] André Barreto, Shaobo Hou, Diana Borsa, David Silver, and Doina Precup. Fast reinforcement
learning with generalized policy updates. Proceedings of the National Academy of Sciences,
117(48), 2020.

[5] Veronica Chelu, Doina Precup, and Hado P van Hasselt. Forethought and hindsight in credit

assignment. In Advances in Neural Information Processing Systems, 2020.

[6] Rohan Chitnis, Tom Silver, Joshua B. Tenenbaum, Tomas Lozano-Perez, and Leslie Pack
Kaelbling. Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning.
arXiv:2105.14074 [cs], 2021.

[7] Rohit K. Dubey, Samuel S. Sohn, Jimmy Abualdenien, Tyler Thrash, Christoph Hoelscher,
André Borrmann, and Mubbasir Kapadia. SNAP:Successor Entropy based Incremental Subgoal
Discovery for Adaptive Navigation. In Motion, Interaction and Games, 2021.

[8] Scott Emmons, Ajay Jain, Misha Laskin, Thanard Kurutach, Pieter Abbeel, and Deepak Pathak.
Sparse graphical memory for robust planning. In Advances in Neural Information Processing
Systems, 2020.

[9] Amir-massoud Farahmand. Iterative Value-Aware Model Learning. In Advances in Neural

Information Processing Systems 31, 2018.

[10] Amir-massoud Farahmand, Andre M S Barreto, and Daniel N Nikovski. Value-Aware Loss
Function for Model-based Reinforcement Learning. In International Conference on Artiﬁcial
Intelligence and Statistics, 2017.

[11] Gregory Farquhar, Tim Rocktäschel, Maximilian Igl, and Shimon Whiteson. TreeQN and
ATreeC: Differentiable Tree-Structured Models for Deep Reinforcement Learning. In Interna-
tional Conference on Learning Representations, 2018.

[12] Robert Gieselmann and Florian T. Pokorny. Planning-Augmented Hierarchical Reinforcement

Learning. IEEE Robotics and Automation Letters, 6(3), 2021.

[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers:
Surpassing human-level performance on imagenet classiﬁcation. In Proceedings of the IEEE
international conference on computer vision, pages 1026–1034, 2015.

[14] Christopher Hoang, Sungryull Sohn, Jongwook Choi, Wilka Carvalho, and Honglak Lee.
Successor Feature Landmarks for Long-Horizon Goal-Conditioned Reinforcement Learning. In
Advances in Neural Information Processing Systems, 2021.

10

[15] Chad Hogg, U. Kuter, and Hector Muñoz-Avila. Learning Methods to Generate Good Plans:
Integrating HTN Learning and Reinforcement Learning. In AAAI Conference on Artiﬁcial
Intelligence, 2010.

[16] Zhiao Huang, Fangchen Liu, and Hao Su. Mapping state space using landmarks for universal

goal reaching. In Advances in Neural Information Processing Systems, 2019.

[17] Taher Jafferjee, Ehsan Imani, Erin Talvitie, Martha White, and Micheal Bowling. Hallucinating
Value: A Pitfall of Dyna-style Planning with Imperfect Environment Models. arXiv:2006.04363
[cs, stat], 2020.

[18] Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina Precup.
What can I do here? A Theory of Affordances in Reinforcement Learning. In International
Conference on Machine Learning, 2020.

[19] Junsu Kim, Younggyo Seo, and Jinwoo Shin. Landmark-Guided Subgoal Generation in
Hierarchical Reinforcement Learning. In Advances in Neural Information Processing Systems,
2021.

[20] G.D. Konidaris and A.G. Barto. Skill discovery in continuous reinforcement learning domains

using skill chaining. In Advances in Neural Information Processing Systems, 2009.

[21] Nathan Lambert, Kristofer Pister, and Roberto Calandra. Investigating Compounding Prediction

Errors in Learned Dynamics Models. arXiv:2203.09637 [cs], 2022.

[22] Timothy A. Mann, Shie Mannor, and Doina Precup. Approximate Value Iteration with Tempo-

rally Extended Actions. Journal of Artiﬁcial Intelligence Research, 53, 2015.

[23] Amy McGovern and Andrew G Barto. Automatic discovery of subgoals in reinforcement
learning using diverse density. In International Conference on Machine Learning, 2001.

[24] Andrew W Moore and Christopher G Atkeson. Prioritized sweeping: Reinforcement learning

with less data and less time. Machine learning, 13(1), 1993.

[25] Soroush Nasiriany, Vitchyr Pong, Steven Lin, and Sergey Levine. Planning with Goal-

Conditioned Policies. In Advances in Neural Information Processing Systems, 2019.

[26] Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. Advances in Neural

Information Processing Systems, 2017.

[27] Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White. Or-
ganizing Experience: A Deeper Look at Replay Mechanisms for Sample-Based Planning in
Continuous State Domains. In International Joint Conference on Artiﬁcial Intelligence, 2018.

[28] Yangchen Pan, Hengshuai Yao, Amir-Massoud Farahmand, and Martha White. Hill climbing
on value estimates for search-control in Dyna. In International Joint Conference on Artiﬁcial
Intelligence, 2019.

[29] Warren B. Powell. What you should know about approximate dynamic programming. Wiley

InterScience, 2009.

[30] Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal Value Function

Approximators. In International Conference on Machine Learning, 2015.

[31] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy
Lillicrap, and David Silver. Mastering Atari, Go, chess and shogi by planning with a learned
model. Nature, 588(7839), 2020.

[32] David Silver, Richard S Sutton, and Martin Müller. Sample-based learning and search with
permanent and transient memories. In International Conference on Machine Learning, 2008.

11

[33] David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel
Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The
Predictron: End-To-End Learning and Planning. In International Conference on Machine
Learning, 2017.

[34] Satinder Singh, Andrew Barto, and Nuttapong Chentanez. Intrinsically Motivated Reinforce-

ment Learning. In Advances in Neural Information Processing Systems, 2004.

[35] Martin Stolle and Doina Precup. Learning Options in Reinforcement Learning. In Abstraction,

Reformulation, and Approximation, 2002.

[36] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press,

2018.

[37] Richard S Sutton, Doina Precup, and Satinder Singh. Between MDPs and semi-MDPs: A
framework for temporal abstraction in reinforcement learning. Artiﬁcial intelligence, 112(1-2),
1999.

[38] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam
White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from
unsupervised sensorimotor interaction. In International Conference on Autonomous Agents and
Multiagent Systems, 2011.

[39] Richard S Sutton, A R Mahmood, and Martha White. An emphatic approach to the problem of

off-policy temporal-difference learning. The Journal of Machine Learning Research, 2016.

[40] Richard S. Sutton, Marlos C. Machado, G. Zacharias Holland, David Szepesvari, Finbarr
Timbers, Brian Tanner, and Adam White. Reward-Respecting Subtasks for Model-Based
Reinforcement Learning. arXiv:2202.03466 [cs], 2022.

[41] R.S. Sutton. Integrated architectures for learning planning and reacting based on approximating

dynamic programming. In International Conference on Machine Learning, 1990.

[42] Erik Talvitie. Model regularization for stable sample roll-outs. In Uncertainty in Artiﬁcial

Intelligence, 2014.

[43] Erik Talvitie. Self-Correcting Models for Model-Based Reinforcement Learning. In AAAI

Conference on Artiﬁcial Intelligence, 2017.

[44] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value Iteration

Networks. In Advances in Neural Information Processing Systems, 2016.

[45] Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double

q-learning. In AAAI conference on artiﬁcial intelligence, 2016.

[46] Hado P van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in
reinforcement learning? In Advances in Neural Information Processing Systems, 2019.

[47] A Venkatraman, M Hebert, and J. Andrew Bagnell. Improving Multi-Step Prediction of Learned

Time Series Models. In AAAI Conference on Artiﬁcial Intelligence, 2015.

[48] Yi Wan, Muhammad Zaheer, Adam White, Martha White, and Richard S. Sutton. Planning

with Expectation Models. In International Joint Conference on Artiﬁcial Intelligence, 2019.

[49] Yi Wan, Abhishek Naik, and Richard S. Sutton. Average-Reward Learning and Planning with

Options. In Advances in Neural Information Processing Systems, 2021.

[50] Theophane Weber, Sebastien Racanière, David P Reichert, Lars Buesing, Arthur Guez,
Danilo Jimenez Rezende, Adrià Puigdomènech Badia, Oriol Vinyals, Nicolas Heess, Yujia Li,
Razvan Pascanu, Peter Battaglia, David Silver, and Daan Wierstra. Imagination-Augmented
Agents for Deep Reinforcement Learning. In Advances in Neural Information Processing
Systems, 2017.

[51] Martha White. Unifying task speciﬁcation in reinforcement learning. In International Confer-

ence on Machine Learning, 2017.

12

[52] David Wingate, Kevin D Seppi, Cs Byu Edu, and Cs Byu Edu. Prioritization Methods for

Accelerating MDP Solvers. Journal of Machine Learning Research, 2005.

[53] Jason Wolfe, Bhaskara Marthi, and Stuart Russell. Combined Task and Motion Planning for
Mobile Manipulation. International Conference on Automated Planning and Scheduling, 2010.

[54] Lunjun Zhang, Ge Yang, and Bradly C. Stadie. World Model as a Graph: Learning Latent

Landmarks for Planning. In International Conference on Machine Learning, 2021.

[55] Tianren Zhang, Shangqi Guo, Tian Tan, Xiaolin Hu, and Feng Chen. Generating adjacency-
constrained subgoals in hierarchical reinforcement learning. In Advances in Neural Information
Processing Systems, 2020.

13

A Proofs for the Deterministic Policy Evaluation Setting

We provide proofs here for Section 3. We assume throughout that the environment discount γt+1 is a
[0, 1) for every step in an episode, until termination when it is zero. The below results
constant γc ∈
can be extended to the case where γc = 1, using the standard strategy for the stochastic shortest path
problem setting.

First, we want to show that given rπ,γ and Pπ,γ, we can guarantee that the update for the values for
G
will converge. Recall that ¯
is the augmented goal space that includes the terminal
G
state. This terminal state is not a subgoal—since it is not a real state—but is key for appropriate
planning.
Lemma 1. Assume that we have a deterministic MDP, deterministic policy π, γc < 1, a discrete set
¯
G| with the dynamic programming update
of subgoals

sterminal

G ∪ {

R|

=

}

, and that we iteratively update vt ∈

G ⊂ S
vt(g) = rπ,γ(g, g(cid:48)) + Pπ,γ(g, g(cid:48))vt

−

1(g(cid:48)) where g(cid:48) = argmax

Pπ,γ(g, g(cid:48))

(6)

¯
G

g(cid:48)∈
¯
R|
G|, with vt(sterminal) ﬁxed at

for all g
zero. Then then vt converges to a ﬁxed point.

, starting from an arbitrary (ﬁnite) initialization v0 ∈

∈ G

Proof. To analyze this as a matrix update, we need to extend Pπ,γ(g, g(cid:48)) to include an additional row
transitioning from sterminal. This row is all zeros, because the value in the terminal state is always
ﬁxed at zero. Note that there are ways to avoid introducing terminal states, using transition-based
discounting [51], but for this work it is actually simpler to explicitly reason about them and reaching
them from subgoals.

To show this we simply need to ensure that Pπ,γ is a substochastic matrix. Recall that

Pπ,γ(g, g(cid:48)) def= Eπ[1(St+1 = g(cid:48))γt+1 + γg(cid:48),t+1Pπ,γ(St+1, g(cid:48))
|

St = g]

where γg(cid:48),t+1 = 0 if St+1 = g(cid:48) and otherwise equals γt+1, the environment discount. If it is
substochastic, then

Pπ,γ(cid:107)2 < 1. Consequently, the Bellman operator
(Bv)(g) = rπ,γ(g, g(cid:48)) + Pπ,γ(g, g(cid:48))˜v(g(cid:48)) where g(cid:48) = argmax

Pπ,γ(g, g(cid:48))

(cid:107)

Bv1 −
(cid:107)

Bv2(cid:107)2 =

is a contraction, because

v2(cid:107)2.
Pπ,γv1 −
Because γc < 1, then either g immediately terminates in g(cid:48), giving 1(St+1 = g(cid:48))γt+1 +
γc. Or, it does not immediately terminate, and 1(St+1 =
γg(cid:48),t+1Pπ,γ(St+1, g(cid:48)) = γt+1 + 0
1. There-
g(cid:48))γt+1 + γg(cid:48),t+1Pπ,γ(St+1, g(cid:48)) = 0 + γcPπ,γ(St+1, g(cid:48))
fore, if γc < 1, then
Pπ,γ(cid:107)2 ≤

γc because Pπ,γ(St+1, g(cid:48))

Pπ,γv2(cid:107)2 ≤ (cid:107)

v2(cid:107)2 <

v1 −

v1 −

γc.

≤

≤

≤

(cid:107)

(cid:107)

(cid:107)

¯
G

g(cid:48)∈
Pπ,γ(cid:107)2(cid:107)

Proposition 1. For a deterministic MDP, deterministic policy π, and a discrete set of subgoals
that are all reached by π in the MDP, given the ˜v(g) obtained from Equation 6, if we set

G ⊂ S

v(s) = rγ(s, g) + Pπ,γ(s, g)˜v(g) where g = argmax

Pπ,γ(s, g)

(7)

for all states s

∈ S

then we get that v = vπ.

g

∈

¯
G

Proof. For a deterministic environment and deterministic policy this result is straightforward. The
term Pπ,γ(s, g) > 0 only if g is on the trajectory from s when the policy π is executed. The term
rγ(s, g) consists of deterministic (discounted) rewards and ˜v(g) is the true value from g, as shown in
Lemma 6 (namely ˜v(g) = vπ(g)). The subgoal g is the closest state on the trajectory from s, and
Pπ,γ(s, g) is γt

c where t is the number of steps from s to g.

B Proofs for the General Control Setting

In this section we assume that γc < 1, to avoid some of the additional issues for handling proper
policies. The same strategies apply to the stochastic shortest path setting with γc = 1, with additional
assumptions.

14

Proposition 2. [Convergence of Value Iteration in Goal-Space] Assuming that ˜Γ is a substochastic
¯
G| initialized to an arbitrary value and ﬁxing vt(sterminal) = 0 for all t, then
matrix, with v0 ∈
iteratively sweeping through all g

with update

R|

˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))vt
−

1(g(cid:48))

(8)

∈ G

vt(g) =

max
: ˜d(g,g(cid:48))>0
¯
g(cid:48)∈
G

convergences to a ﬁxed-point.

Proof. We can use the same approach typically used for value iteration. For any v0 ∈
deﬁne the operator
max
: ˜d(g,g(cid:48))>0
¯
g(cid:48)∈
G

˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))˜v(g(cid:48))

(Bgv)(g) def=

First we can show that Bg is a γc-contraction. Assume we are given any two vectors v1, v2. Notice
that ˜Γ(g, g(cid:48))
γc, because for our problem setting the discount is either equal to γc or equal to zero
at termination. Then we have that for any g

≤

R|

¯
G|, we can

¯
G

∈

(Bgv2)(g)
|

˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))v1(g(cid:48))

˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))v1(g(cid:48))

˜Γ(g, g(cid:48))(v1(g(cid:48))

v2(g(cid:48)))

|

−

v1(g(cid:48))

γc|

v2(g(cid:48))

|

−

(Bgv1)(g)

|

(cid:12)
(cid:12)
(cid:12)

=

g(cid:48)∈

−
max
: ˜d(g,g(cid:48))>0
¯
G
max
: ˜d(g,g(cid:48))>0 |
¯
G

max
: ˜d(g,g(cid:48))>0 |
¯
G
max
: ˜d(g,g(cid:48))>0
¯
G
v2(cid:107)∞
v1 −

≤

=

≤

≤

g(cid:48)∈

g(cid:48)∈

g(cid:48)∈
γc(cid:107)

(cid:12)
˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))v2(g(cid:48))
(cid:12)
(cid:12)

max
: ˜d(g,g(cid:48))>0
¯
G

g(cid:48)∈
(˜rγ(g, g(cid:48)) + ˜Γ(g, g(cid:48))v2(g(cid:48)))

−

−

|

Since this is true for any g, it is true for the max over g, giving
v1 −

Bgv2(cid:107)∞ ≤

Bgv1 −
(cid:107)

γc(cid:107)

v2(cid:107)∞

.

Because the operator Bg is a contraction, since γc < 1, we know by the Banach Fixed-Point Theorem
that the ﬁxed-point exists and is unique.

Now we analyze the update to the main policy, that incorporates the subgoal value estimates into
the bootstrap target. We assume we have a ﬁnite number of state-action pairs n, with parameterized
Rn represented as a vector with one entry per state-action pair. Value iteration
; w)
action-values q(
·
to ﬁnd q∗ corresponds to updating with the Bellman optimality operator

∈

(Bq)(s, a) def= r(s, a) +

(cid:88)

s(cid:48)

P (s(cid:48)

s, a)γ(s(cid:48)) max
|
a(cid:48)∈A

q(s(cid:48), a(cid:48))

(9)

def= q(
; wt), if we assume the parameterized function class can
On each step, for the current qt
·
represent Bqt, then we can reason about the iterations of w1, w2, . . . obtain when minimizing
distance between q(

; wt+1) and Bqt, with
·

q(s, a; wt+1) = (Bq(

; wt))(s, a)
·

Under function approximation, we do not simple update a table of values, but we can get this equality
by minimizing until we have zero Bellman error. Note that q∗ = Bq∗, by deﬁnition.
In this realizability regime, we can reason about the iterates produced by value iteration. The
convergence rate is dictated by γc, as is well known, because
q1 −

γc(cid:107)

q2(cid:107)∞

Bq2(cid:107)∞ ≤

Bq1 −
(cid:107)
rmax, then we can use the fact that 1) the maximal return is no
r(s, a)
| ≤
|
, and 2) for any initialization q0 no larger in magnitude than this maximal
q∗
Bq0 −

2Gmax. Therefore, we get that
γc(cid:107)

Bq0 −
(cid:107)

(cid:107)∞ ≤
q∗

q0 −

(cid:107)∞ ≤

Bq∗

(cid:107)∞

(cid:107)∞

q∗

=

Speciﬁcally, if we assume
def= rmax
greater than Gmax
γc
1
−
return we have that
q0 −
(cid:107)
(cid:107)

15

=

q∗

(cid:107)∞

Bqt

and so after t iterations we have
qt−
(cid:107)
(cid:107)∞ ≤
We can use the exact same strategy to show convergence of value iteration, under our subgoal-value
bootstrapping update. Let rsub(s, a) def= (cid:80)
Gmax, Gmax]
is a given, ﬁxed function. Then the modiﬁed Bellman optimality operator is

s, a)vsub(s(cid:48)), assuming vsub :

qt
γc(cid:107)

γ2
c (cid:107)

q0−

γt
c(cid:107)

(cid:107)∞ ≤

= γt

P (s(cid:48)

S →

Bq∗

1−

2−

1−

(cid:107)∞

(cid:107)∞

. . .

q∗

q∗

q∗

qt

−

≤

(cid:107)

s(cid:48)

−

−

−

[

|

cGmax

(Bβq)(s, a) def= r(s, a) + βrsub(s, a) + (1

β)

−

(cid:88)

s(cid:48)

P (s(cid:48)

s, a)γ(s(cid:48)) max
a(cid:48)∈A

|

q(s(cid:48), a(cid:48))

(10)

Proposition 3 (Convergence rate of tabular value iteration under subgoal bootstrapping). The ﬁxed
point q∗β = Bβq∗β exists and is unique. Further, for q0, and the corresponding w0, initialized such
that
1 for
q0(s, a; w0)
| ≤
t = 1, 2, . . . satisﬁes

Gmax, the value iteration update with subgoal bootstrapping qt = Bβqt

−

|

qt −
(cid:107)

q∗β(cid:107)∞ ≤

(1

−

β)tγt
c

rmax + βGmax
β)γc
1

(1

−

−

β)-contraction. Assume we are given any two vectors
γc, because for our problem setting it is either equal to γc or equal to zero

−

Proof. First we can show that Bβ is a γc(1
q1, q2. Notice that γ(s)
≤
at termination. Then we have that for any (s, a)
(cid:88)
(Bβq2)(s, a)
|

(Bβq1(s, a)

β)

(1

−

−

=

|

|

(1

(1

(1

(1

−

−

−

−

≤

≤

≤

≤

s(cid:48)
(cid:88)

β)γc

β)γc

β)γc

β)γc

s(cid:48)
(cid:88)

s(cid:48)
(cid:88)

s(cid:48)
(cid:88)

s(cid:48)

= (1

β)γc(cid:107)

q1 −

−

P (s(cid:48)

s, a)γ(s(cid:48))[max
|
a(cid:48)∈A

q1(s(cid:48), a(cid:48))

max
a(cid:48)∈A

−

q2(s(cid:48), a(cid:48))]
|

P (s(cid:48)

s, a)
|
|

[max
a(cid:48)∈A

q1(s(cid:48), a(cid:48))

max
a(cid:48)∈A

−

q2(s(cid:48), a(cid:48))]
|

P (s(cid:48)

s, a) max
|
a(cid:48)∈A

|

q1(s(cid:48), a(cid:48))

q2(s(cid:48), a(cid:48))
|

−

q1(s(cid:48), a(cid:48))
|

q2(s(cid:48), a(cid:48))

|

−

s, a) max
|
s(cid:48)∈S
q1 −
s, a)
(cid:107)

,a(cid:48)∈A
q2(cid:107)∞

P (s(cid:48)

P (s(cid:48)

|
q2(cid:107)∞

Since this is true for any (s, a), it is true for the max, giving
β)γc(cid:107)

Bβq2(cid:107)∞ ≤
Because the operator is a contraction, since (1
−
Theorem that the ﬁxed-point exists and is unique.

Bβq1 −

(1

(cid:107)

q1 −

q2(cid:107)∞

.

−
β)γc < 1, we know by the Banach Fixed-Point

Now we can also use contraction property for the convergence rate. Notice ﬁrst that we can consider
˜r(s, a) def= r(s, a) + βrsub(s, a) as the new reward, with maximum value rmax + βGmax. Further, the
new discount is (1

β)γc. Consequently, the maximal return is rmax+βGmax
β)γc

(1

1

.

(1

qt
β)γc(cid:107)

−

1 −

−

q∗

(cid:107)∞

(1

β)tγt
c(cid:107)

q0 −

−

q∗

≤

(cid:107)∞

−

−
. . .

qt −
(cid:107)

q∗β(cid:107)∞

=

−
Bβqt
1 −
−
β)tγt
c

(cid:107)
(1

Bβq∗β(cid:107)∞ ≤
rmax + βGmax
β)γc
1

≤

−

(1
β)γc)t, and for β near 1 gives a much faster convergence rate than
This rate is dominated by ((1
β = 0. We can determine after how many iteration this term overcomes the increase in the upper
bound on the return. In other words, we want to know how big t needs to be to get

−

−

−

−
Rearranging terms, we get that this is true for

−

β)tγt
c

(1

−

rmax + βGmax
1

(1

β)γc ≤

γt
cGmax.

(cid:18)

t > log

rmax + βGmax
(1

Gmax(1

β)γc)

(cid:19)

/ log

(cid:18) 1
1

β

(cid:19)

.

−
For example if rmax = 1, γc = 0.99 and β = 0.5, then we have that t > 1.56. If we have that
rmax = 10, γc = 0.99 and β = 0.5, then we get that t
5. If we have that rmax = 1, γc = 0.99 and
β = 0.1, then we get that t

22.

−

−

≥

≥

16

C Learning the Subgoal Models and Corresponding Option Policies

Now we need a way to learn the models, rγ(s, g) and Γ(s, g). These can both be represented as
General Value Functions (GVFs) [38], and we leverage this form to use standard algorithms in
reinforcement learning to learn them. We start by assuming that we have πg and discuss learning it
after understanding learning these models.

C.1 Model Learning

The data is generated off-policy—according to some behavior b rather than from πg. We can either use
importance sampling or we can learn the action-value variants of these models to avoid importance
sampling. We describe both options here, but in our experiments using the action-value variant since
it avoids importance sampling and the need to have the distribution over actions under behavior b.

Model Update using Importance Sampling We can update rγ(
·
temporal difference (TD) learning update ρtδt∇

St)
rγ(St, g) where ρt = πg(a
St) and
|
b(a
|

, g) with an importance-sampled

δr
t = Rt+1 + γg,t+1rγ(St+1, g)

rγ(St, g)

−

The discount model Γ(s, g) can be learned similarly, because it is also a GVF with cumulant
m(St+1, g)γt+1 and discount γg,t+1. The TD update is ρtδΓ

t where

δΓ
t = m(St+1, g)γt+1 + γg,t+1Γ(St+1, g)

Γ(St, g)

−

All of the above updates can be done using any off-policy GVF algorithm, including those using
clipping of IS ratios and gradient-based methods, and can include replay.

Model Update without Importance Sampling Overloading notation, let us deﬁne the action-value
variants rγ(s, a, g) and Γ(s, a, g). We get similar updates to above, now redeﬁning
δr
t = Rt+1 + γg,t+1rγ(St+1, πg(St+1), g)
rγ(St, At, g). For Γ we have
and using update δt∇

rγ(St, At, g)

−

δΓ
t = m(St+1, g)γt+1 + γg,t+1Γ(St+1, πg(St+1), g)

Γ(St, At, g)

We then deﬁne rγ(s, g) def= rγ(s, πg(s), g) and Γ(s, g) def= Γ(s, πg(s), g) as deterministic functions of
these learned functions.

−

Restricting the Model Update to Relevant States Recall, however, that we need only query these
models where d(s, g) > 0. We can focus our function approximation resources on those states. This
idea has previously been introduced with an interest weighting for GVFs [39], with connections made
between interest and initiation sets [51]. For a large state space with many subgoals, using goal-space
planning signiﬁcantly expands the models that need to be learned, especially if we learn one model
per subgoal. Even if we learn a model that generalizes across subgoal vectors, we are requiring that
model to know a lot: values from all states to all subgoals. It is likely such a models would be hard to
learn, and constraining what we learn about with d(s, g) is likely key for practical performance.

The modiﬁcation to the update is simple: we simply do not update rγ(s, g), Γ(s, g) in states s where
d(s, g) = 0.5 For the action-value variant, we do not update for state-action pairs (s, a) where
d(s, g) = 0 and πg(s)
= a. The model will only ever be queried in (s, a) where d(s, g) = 1 and
πg(s) = a.

Learning the relevance model d We assume in this work that we simply have d(s, g), but we can
at least consider ways that we could learn it. One approach is to attempt to learn Γ for each g, to
determine which are pertinent. Those with Γ(s, g) closer to zero can have d(s, g) = 0. In fact, such
an approach was taken for discovering options [18], where both options and such a relevance function
are learned jointly. For us, they could also be learned jointly, where a larger set of goals start with
d(s, g) = 1, then if Γ(s, g) remains small, then these may be switched to d(s, g) = 0 and they will
stop being learned in the model updates.

5More generally, we might consider using emphatic weightings [39] that allow us to incorporate such interest
weightings d(s, g), without suffering from bootstrapping off of inaccurate values in states where d(s, g) = 0.
Incorporating this algorithm would likely beneﬁt the whole system, but we keep things simpler for now and stick
with a typical TD update.

17

(cid:54)
Learning the subgoal-to-subgoal models Finally, we need to extract the subgoal-to-subgoal
models ˜rγ, ˜Γ from rγ, Γ. The strategy involves updating towards the state-to-subgoal models,
whenever a state corresponds to a subgoal. In other words, for a given s, if m(s, g) = 1, then for a
given g(cid:48) (or iterating through all of them), we can update ˜rγ using

and update ˜Γ using

(rγ(s, g(cid:48))

˜rγ(g, g(cid:48)))

˜rγ(g, g(cid:48))

∇

−

(Γ(s, g(cid:48))

˜Γ(g, g(cid:48)))

˜Γ(g, g(cid:48)).

−

∇

Note that these updates are not guaranteed to uniformly weight the states where m(s, g) = 1. Instead,
the implicit weighting is based on sampling s, such as through which states are visited and in the
replay buffer. We do not attempt to correct this skew, as mentioned in the main body, we presume
that this bias is minimal. An important next step is to better understand if this lack of reweighting
causes convergence issues, and how to modify the algorithm to account for a potentially changing
state visitation.

C.2 A General Algorithm for Learning Option Policies

Finally, we need to learn the option policies πg. In the simplest case, it is enough to learn πg that
makes rγ(s, g) maximal for every relevant s (i.e., d(s, g) > 0). We can learn the action-value variant
rγ(s, a, g), where we overloaded
rγ(s, a, g) using a Q-learning update, and set πg(s) = argmaxa
∈A
the deﬁnition of rγ. We can then extract rγ(s, g) = maxa
rγ(s, a, g), to use in all the above
updates and in planning. In our own Pinball Experiment, this strategy is sufﬁcient for learning πg.
More generally, however, this approach may be ineffective because maximizing environment reward
may be at odds with reaching the subgoal in a reasonable number of steps (or at all). For example, in
environments where the reward is always positive, maximizing environment reward might encourage
the option policy not to terminate.6 However, we do want πg to reach g, while also obtaining the best
return along the way to g. For example, if there is a lava pit along the way to a goal, even if going
through the lava pit is the shortest path, we want the learned option to get to the goal by going around
the lava pit. We therefore want to be reward-respecting, as introduced for reward-respecting subtasks
[40], but also ensure termination.

∈A

We can consider a spectrum of option policies that range from the policy that reaches the goal
as fast as possible to one that focuses on environment reward. We can specify a new reward for
learning the option: ˜Rt+1 = cRt+1 + (1
1). When c = 0, we have a cost-to-goal problem,
−
where the learned option policy should ﬁnd the shortest path to the goal, regardless of reward along
the way. When c = 1, the option policy focuses on environment reward, but may not terminate
in g. We can start by learning the option policy that takes the shortest path with c = 0, and the
corresponding rγ(s, g), Γ(s, g). The constant c can be increased until πg stops going to the goal, or
until the discounted probability Γ(s, g) drops below a speciﬁed threshold.

c)(

−

Even without a well-speciﬁed c, the values under the option policy can still be informative. For
example, it might indicate that it is difﬁcult or dangerous to attempt to reach a goal. For this work,
we propose a simple default, where we ﬁx c = 0.5. Adaptive approaches, such as the idea described
above, are left to future work.

The resulting algorithm to learn πg involves learning a separate value function for these rewards. We
can learn action-values (or a parameterized policy) using the above reward. For example, we can
learn a policy with the Q-learning update to action-values ˜q

(cid:18)

cRt+1 + c

1 + γg,t+1 max

a(cid:48)

−

˜q(St+1, a(cid:48), g)

−

˜q(St, At, g)

(cid:19)

∇

˜q(St, At, g)

Then we can set πg to be the greedy policy, πg(s) = argmaxa

˜q(s, a, g).

∈A

6It is not always the case that positive rewards result in option policies that do not terminate. If there is a
large, positive reward at the subgoal in the environment, Even if all rewards are positive, if γc < 1 and there is
larger positive reward at the subgoal than in other nearby states, then the return is higher when reaching this
subgoal sooner, since that reward is not discounted as many steps. This outcome is less nuanced for negative
reward. If the rewards are always negative, on the other hand, then the option policy will terminate, trying to ﬁnd
the path with the best (but still negative) return.

18

C.3 Pseudocode putting it all together

We summarize the above updates in pseudocode, specifying explicit parameters and how they are
updated. The algorithm is summarized in Algorithm 1, with a diagram in Figure 3. An online update
is used for the action-values for the main policy, without replay. All background computation is used
for model learning using a replay buffer and for planning with those models. The pseudocode assumes
a small set of subgoals, and is for episodic problems. We provide extensions to other settings in
Appendix C.4, including using a Double DQN update for the policy update. We also discuss in-depth
differences to existing related ideas, including landmark states, UVFAs, and Goal-conditioned RL in
Appendix D.

∈A

Note that we overload the deﬁnitions of the subgoal models. We learn action-value variants
rγ(s, a, g; θr), with parameters θr, to avoid importance sampling corrections. We learn the
option-policy using action-values ˜q(s, a; θπ) with parameters θπ, and so query the policy using
πg(s; θπ) def= argmaxa
˜q(s, a, g; θπ). The policy πg is not directly learned, but rather deﬁned
by ˜q. Similarly, we do not directly learn rγ(s, g); instead, it is deﬁned by rγ(s, a, g; θr). Specif-
ically, for model parameters θ = (θr, θΓ, θπ), we set rγ(s, g; θ) def= rγ(s, πg(s; θπ), g; θr) and
Γ(s, g; θ) def= Γ(s, πg(s; θπ), g; θΓ). We query these derived functions in the pseudocode.
Finally, we assume access to a given set of subgoals. But there have been several natural ideas
already proposed for option discovery, that nicely apply in our more constrained setting. One idea
was to use subgoals that are often visited by the agent [35]. Such a simple idea is likely a reasonable
starting point to make a GSP algorithm that learns everything from scratch, including subgoals. Other
approaches have used bottleneck states [23].

Algorithm 1 Goal-Space Planning for Episodic Problems

R|G|, main policy w, model parameters θ = (θr, θΓ, θπ), ˜θ = (˜θr, ˜θΓ)

Assume given subgoals
Initialize table v
Sample initial state s0 from the environment
for t

0, 1, 2, ... do

∈

G

and relevance function d

∈

Take action at using q (e.g., (cid:15)-greedy), observe st+1, rt+1, γt+1
ModelUpdate(st, at, st+1, rt+1, γt+1)
Planning()
MainPolicyUpdate(st, at, st+1, rt+1, γt+1)

Algorithm 2 MainPolicyUpdate(s, a, s(cid:48), r, γ)

maxg

:d(s,g)>0 rγ(s, g; θ) + Γ(s, g; θ)˜v(g)
¯
G

β) maxa(cid:48) q(s(cid:48), a(cid:48); w)

←
∈
r + γβvsub + γ(1
w + αδ

wq(s, a; w)

−

−

q(s, a; w)

vsub
δ
w

←
←

∇

Algorithm 3 Planning()

for n iterations, for each g

do

˜v(g)

maxg(cid:48)∈

←

¯
G

∈ G

:d(g,g(cid:48))>0 ˜rγ(g, g(cid:48); ˜θr) + ˜Γ(g, g(cid:48); ˜θΓ)˜v(g(cid:48))

19

Algorithm 4 ModelUpdate(s, a, s(cid:48), r, γ)

Add new transition (s, a, s(cid:48), r, γ) to buffer B
for g(cid:48)

∇

←
←

q(s, a, g(cid:48); θπ)

q(s, a, g(cid:48); θπ)

˜q(s(cid:48), a(cid:48), g(cid:48); θπ)

1
2 (r
−
θπ + απδπ

¯
, for multiple transitions (s, a, r, s(cid:48), γ) sampled from B do
∈
G
m(s(cid:48), g(cid:48)))
γ(1
γg ←
−
// Update option policy
δπ
1) + γg maxa(cid:48)∈A
θπ
// Update reward model and discount model
πg(s; θπ)
a(cid:48)
δr
r + γgrγ(s(cid:48), a(cid:48), g(cid:48); θr)
δΓ
1(γg = 0)γ + γgΓ(s(cid:48), a(cid:48), g(cid:48); θΓ)
θr
θr + αrδr
∇
θΓ
θΓ + αΓδΓ
∇
// Update goal-to-goal models using state-to-goal models
for each g such that m(s, g) > 0 do

rγ(s, a, g(cid:48); θr)
Γ(s, a, g(cid:48); θΓ)

rγ(s, a, g(cid:48); θr)

←
←
←
←
←

Γ(s, a, g(cid:48); θΓ)

−

−

−

˜θr
˜θΓ

←
←

˜θr + ˜αr(rγ(s, g(cid:48); θ)
˜θΓ + ˜αΓ(Γ(s, g(cid:48); θ)

˜rγ(g, g(cid:48); ˜θr))
˜Γ(g, g(cid:48); ˜θr))

˜rγ(g, g(cid:48); ˜θr)
∇
˜Γ(g, g(cid:48); ˜θΓ)
∇

−
−

C.4 Variants of GSP

It is simple to extend the above pseudocode for the main policy update and the option policy update
to use Double DQN [45] updates with neural networks. The changes from the above pseudocode are
1) the use of a target network to stabilize learning with neural networks, 2) using polyak averaging
to interpolate between the target network and the main network’s weights, 3) changing the one-step
bootstrap target to the DDQN equivalent, 4) adding a replay buffer for learning the main policy, and
5) changing the update from using a single sample to using a batch update. Because the number
of subgoals is discrete, the equations for learning ˜θr and ˜θΓ does not change. We summarize these
changes for learning the main policy in Algorithm 5 and for learning subgoal models in Algorithm 6.

Algorithm 5 MainPolicyDDQNUpdate(s, a, s(cid:48), r, γ)

Add experience (s, a, s(cid:48), r, γ) to replay buffer Dmain
for nmain mini-batches do
Sample batch Bmain =
vsub(s) = maxg
∈
Y (r, s(cid:48), γ) = r + γβvsub + γ(1
1
L =
Bmain
|
|
w
w + α
←
wtarget ←

(s,a,r,s(cid:48),γ)
wL
∇
ρw + (1

(s, a, r, s(cid:48), γ)
}

Bmain(Y (r, s(cid:48), γ)

ρ)wtarget

:d(s,g)>0 rγ(s, g; θ) + Γ(s, g; θ)˜v(g)
¯
G

from Dmain

(cid:80)

−

−

−

{

∈

β)q(s(cid:48), maxa(cid:48) q(s(cid:48), a(cid:48); w), wtarget)
q(s, a; w))2

20

from Dmodel

Algorithm 6 ModelDDQNUpdate(s, a, s(cid:48), r, γ)

Add new transition (s, a, s(cid:48), r, γ) to buffer Dmodel
for g(cid:48)

}

do

γ(1

˜q(s(cid:48), a(cid:48), g(cid:48); θπ)

(s, a, r, s(cid:48), γ)
{
m(s(cid:48), g(cid:48)))

¯
∈
G
for nmodel mini-batches do
Sample batch Bmodel =
γg(γ, s(cid:48))
←
−
// Update option policy
argmaxa(cid:48)∈A
a(cid:48)(s(cid:48))
←
1
δπ(s, a, s(cid:48), r, γ)
2 (r
←
−
1
θπ
θπ + απ
∇θπ
Bmodel
←
∈
θπ
ρmodelθπ + (1
target
target ←
// Update reward model and discount model
δr(s, a, r, s(cid:48)γ)
δΓ(s, a, r, s(cid:48)γ)
θr + αr
θr
θΓ
θΓ + αΓ
θr
θΓ

|
1
Bmodel
|
ρmodelθr + (1
ρmodelθΓ + (1

←
←
1
∇θr
Bmodel
|
∇θΓ

∈
(s,a,r,s(cid:48),γ)
ρmodel)θr
ρmodel)θΓ

←
←
target ←
target ←

ρmodel)θπ

(cid:80)
(cid:80)

|
−
−

(s,a,r,s(cid:48),γ)

(s,a,r,s(cid:48),γ)

|
−

(cid:80)

|

(δr)2
(δΓ)2

Bmodel

Bmodel

∈
target

target

// Update goal-to-goal models using state-to-goal models
. . . same as in prior pseudocode.

1) + γg(γ, s(cid:48))˜q(s(cid:48), a(cid:48)(s(cid:48)), g(cid:48); θπ

target)

q(s, a, g(cid:48); θπ)

−
(δπ(s, a, s(cid:48), r, γ))2

Bmodel

r + γg(γ, s(cid:48))rγ(s(cid:48), a(cid:48), g(cid:48); θr)
1(γg = 0)γ + γg(γ, s(cid:48))Γ(s(cid:48), a(cid:48), g(cid:48); θΓ)

−

rγ(s, a, g(cid:48); θr)

Γ(s, a, g(cid:48); θΓ)

−

C.5 Optimizations for GSP using Fixed Models

It is possible to reduce computation cost of GSP when learning with a ﬁxed model. When the subgoal
models are ﬁxed, vsub for an experience sample does not change over time as all components that are
used to calculate vsub are ﬁxed. This means that the agent can calculate vsub when it ﬁrst receives the
experience sample and save it in the buffer, and use the same calculated vsub whenever this sample is
used for updating the main policy. When doing so, vsub only needs to be calculated once per sample
experienced, instead of with every update. This is beneﬁcial when training neural networks, where
each sample is often used multiple times to update network weights.

An additional optimization possible on top of caching of vsub in the replay buffer is that we can batch
the calculation of vsub for multiple samples together, which can be more efﬁcient than calculating
vsub for a single sample every step. To do this, we create an intermediate buffer that stores up to some
number of samples. When the agent experiences a transition, it adds the sample to this intermediate
buffer rather than the main buffer. When this buffer is full, the agent calculates vsub for all samples in
this buffer at once and adds the samples alongside vsub to the main buffer. This intermediate buffer is
then emptied and added to again every step. We set the maximum size for the intermediate buffer to
1024 in our experiments.

D Connections to UVFAs and Goal-Conditioned RL

There is a large and growing literature on goal-conditioned RL (GCRL). This is a problem setting
s, g) that can be (zero-shot) conditioned on different possible
where the aim is to learn a policy π(a
|
goals. The agent learns for a given set of goals, with the assumption that at the start of each episode
the goal state is explicitly given to the agent. After this training phase, the policy should generalize
to previously unseen goals. Naturally, this idea has particularly been applied to navigation, having
the agent learn to navigate to different states (goals) in the environment. Many GCRL approaches
leverage UVFAs [30].

This setting bears a strong resemblance to what we do in this work, but is notably different. Our
models can be seen as goal-conditioned models—part of the solution—for planning in the general RL
setting. GCRL, on the other hand, is a problem setting. Many approaches do not consider planning,
but instead focus on effectively learning the goal-conditioned value functions or policies.

There is more work, however, using landmark states and planning, for GCRL. In addition to the goal
given for GCRL, the landmark states can be treated as interim subgoals and UVFA models learned

21

for these as well [16]. Planning is done between landmarks, using graph-based search. The policy is
set to reach the nearest goal (using action-values with cost-to-goal rewards of -1 per step) and learned
distance functions between states and goals and between goals. These models are like our reward and
discount models, but tailored to navigation and distances.

The idea of learning models that immediately apply to new subtasks, using successor features, is like
GCRL but goes beyond navigation. The option keyboard involves encoding options (or policies) as
vectors that describe the corresponding (pseudo) reward [3]. This work has been expanded more
recently, using successor features [4]. New policies can then be easily obtained for new reward
functions, by linearly combining the (basis) vectors for the already learned options. No planning is
involved in this work, beyond a one-step decision-time choice amongst options.

E Additional Details on Learning Subgoal Models

This section describes implementation details for learning subgoal models in the PinBall environment
and errors observed in the learned models.

To ensure that we provide sufﬁcient variety of data to learn the model accurately, when learning
the subgoal models, the agent is randomly initialized in the environment at a valid state, ran in the
environment for 20 steps with a random policy, then randomly reset again. To ensure that the agent
gets sufﬁcient experience near goal states, we initialize the agent, with a 0.01 probability, at states
where m(s, g) = 1 for any g with added jitter sampled from U (
0.01, 0.01) for each feature. The
model is trained for 300k steps in this data gathering regime.

−

We restrict model update to relevant states in our experiments. Because the only relevant experience
for learning rγ and Γ are samples where d(s, g) > 0, we maintain a separate buffer for each subgoal
g for learning rγ(s, g) and Γ(s, g) such that all experience within that buffer are relevant. We require
10k samples in the buffer of each subgoal before learning for the corresponding rγ and Γ begins, so
that mini-batches are always drawn from a sufﬁciently diverse set of samples.
Similarly, a sample is only relevant for updating ˜Γ and ˜rγ if m(s, g) > 0 for some g, but this might
not be true for samples stored in the buffers for learning Γ and rγ. To be able to obtain a batch
of samples where all samples are relevant for learning ˜Γ and ˜rγ, the agent uses another buffer that
exclusively stores samples where m(s, g) > 0 to learn ˜Γ and ˜rγ.
We mentioned in Appendix C.1 that we take the simple approach to restricting model updates to states
where d(s, g) = 1. However, this means an update could bootstrap off inaccurate estimates when
learning from a sample (s, a, r, s(cid:48)) if d(s, g) > 0 but d(s(cid:48), g) = 0. In PinBall, this occurs when the
agent starts within the relevance area for a subgoal but taking an action moves the agent outside of it.
We attempt to alleviate this issue in practice by changing the estimation target for those state-action
pairs to be the minimum possible target in the environment. Because we co-learn the option policy
with rγ(s, a, g), we set this minimum value to 1
γ rmin. If the network can learn this target well,
1
−
then the learned option policy will not leave the relevance area.

We also address the issue that for some ﬁxed d, it is possible that not all states where d(s, g) > 0
could reach the subgoal. This can negatively affect the quality of vsub as our algorithm assumes that
goal g is reachable from state s via the option policy if d(s, g) > 0. While this source of error did not
seem to affect GSP in our experiments, it might be important in other environments, so we describe
the modiﬁcation to address this problem here. From these states, the agent should not consider these
subgoals when doing background planning (g(cid:48) is not reachable from g despite ˜d(g, g(cid:48)) = 1) and
projection (g(cid:48) is not reachable from s despite d(s, g(cid:48)) = 1). We check for these states by seeing if
the learned Γ(s, g) is near 0, which indicates that it is either very difﬁcult or impossible to reach g
from s. For states with Γ(s, g) very near 0, we can set d(s, g) = 0 for the purpose of background
planning and projection, but not for learning Γ(s, g) as it might be initialized to a low value. In our
experiments, we set this threshold to 0.

E.1 Error of Learned Subgoal Models

To better understand the accuracy of our learned subgoal models, we performed roll-outs of the learned
option policy at different (x,y) locations (with 0 velocity) across the environment and compared
the true rγ and Γ with the estimated values. Figure 9 shows a heatmap of the absolute error of the

22

Figure 9: A heatmap of the absolute error of Γ and rγ for two different subgoal models learned at
various (x, y). While the absolute error from states near subgoals can be quite low, they increase
substantially as the state gets further away. White indicates that d(s, g) = 0.

model compared to the ground truth, with the mapping of colors on the right. The models learned
tend to be more accurate closer to the goal, and less accurate further away. The absolute error of Γ
can be as low as 0.01 close to the goal, but increase to 0.2 and higher further away. Similarly, the
absolute error for rγ can be as low as below 10 near goals, but can increase over 100 further away.
While the magnitudes of errors are not unreasonable, they are also not very near zero. This results is
encouraging in that inaccuracies in the model do not prevent useful planning.

F Additional Experiment Details

This section provides additional details for the PinBall environment, the various hyperparameters
used for DDQN and GSP, and the hyperparameters sweeps performed. The experiments described
in the main body, along with the hyperparameter sweeps, used approximately 10.7 CPU days on an
Apple M1 chip.

The pinball conﬁguration that we used is based on the "slightly harder conﬁguration" found at
http://irl.cs.brown.edu/pinball/. The Python implementation of PinBall was taken from
https://github.com/amarack/python-rl, which was released under the GPL-3.0 license. We
have modiﬁed the environment to support additional features such as changing terminations, visualiz-
ing subgoals, and various bug ﬁxes.

Network Architecture We used neural networks for learning the main policy, Γ, and rγ. For
experiment 1, we used a neural network with hidden layers [256, 256, 128, 128, 64, 64] for the main
policy and [256, 256, 128, 128, 64, 64, 32, 32] for Γ and rγ. For experiment 2, we used a neural
network with hidden layers [128, 128, 64, 64] for the main policy and [128, 128, 128, 128, 64, 64] for
Γ and rγ. We used ReLU activation function for each layer aside from the output layer. The network’s
bias weights are initialized to 0.001 and other weights were initialized using He uniform initialization
[13]. Each network output a vector of length 5, one for each action.

F.1 Experiment Hyperparameters

For both experiments, we used the Adam optimizer for training both the main policy and the subgoal
models. We used the default hyperparameters for Adam except the step-size (b1 = 0.9, b2 =
8). The main policy was trained with 4 mini-batches per step with batch size of 16,
0.999, (cid:15) = 1e−

23

2000100150500.80.00.40.60.2Absolute ErrorFigure 10: (left) The simple PinBall environment. (right) The performance of Dyna with options
with GSP (β = 0.1) and DDQN in the simple PinBall environment. Dyna with options is depicted by
the grey line. Dyna with options learns slower than GSP with its best beta parameter, but faster than
DDQN, and ultimately achieves the same performance as GSP with its best conﬁguration. Results
are over 30 seeds as before. Dyna with options is implemented with separate stepsize and Polyak
step size hyperparameters for the direct and simulated experience networks, (0.001, 0.1) and (0.005,
0.05), respectively.

while the subgoal models were trained with 1 mini-batch per step with the same batch size. We used
(cid:15)-greedy exploration strategy, with (cid:15) ﬁxed to (cid:15) = 0.1 in our experiments.
For experiment 1, γ = 0.99, απ = αr = αΓ = 5e−
4, and ρmodel = 0.4. For experiment 2, γ = 0.95,
3, and ρmodel = 0.1. We selected the learning rate for Adam and the polyak
απ = αr = αΓ = 1e−
averaging rate ρ for updating the main policy in each experiment using the methodology described in
the section below.

F.2 Hyperparameter Sweep Methodology

For experiment 1, we swept the baseline DDQN algorithm for polyak averaging rate ρ
[0.0125, 0.025, 0.05, 0.1] and learning rate in [1e−
that ρ = 0.025 and learning rate of 5e−
them when running both DDQN and GSP across seeds in the experiment.

∈
4] across 4 seeds. We found
4 had the highest average reward rate in our sweep and used

3, 5e−

4, 3e−

4, 1e−

For experiment 2, we swept DDQN for polyak averaging rate ρ
[1.0, 0.8, 0.4, 0.2, 0.1, 0.05, 0.025]
4] for 8 seeds. We ﬁnd that ρ = 0.05 and learning
and learning rate in [1e−
3 had the highest average reward rate out of all conﬁgurations swept and used these
rate of 1e−
hyperparameters for all DDQN runs in the experiment. For GSP, we used ρ = 0.8 and learning rate
of 1e−

2, 5e−

3, 1e−

3, 5e−

3.

∈

F.3 Exploring a Natural Alternative: Incorporating Options into Dyna

Planning with option models involves using a set of options to include as macro-actions in planning.
The action-values in Dyna with options include both actions and options. If an option πj is selected
when taking a greedy action according to Q, then the ﬁrst action given by πj is executed. The
model in Dyna needs to include option models, which allows the agent to reason about accumulated
rewards under an option, and outcome states after executing an option. Otherwise, the framework is
identical to Dyna. It is a simple, elegant extension on Dyna that allows for planning with temporal
abstraction. One limitation, however, is that as we include new options—more abstraction—our value
function needs to reason over more actions. Our proposed approach allows us to obtain the beneﬁts
of abstraction, without modifying the form of the policy. The model itself is like an option model,
but it is used to directly reason about values for low-level states and actions. Another limitation is
that the model in Dyna is the standard state-to-state model. Though Dyna with options has not been
extended to function approximation — somewhat surprisingly — the natural extension suffers from
similar problems of model errors and the use of expectation models as standard Dyna. We see in
Figure 10 that while Dyna with options learned faster than DDQN, it learned slower than GSP.

24

0100020003000050100150GSP (β = 0.1)Dyna + OptionsDDQNSteps (x100)Reward
Rate
Averaged
over
30 Runs