Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. 

Digital Object Identifier 10.1109/ACCESS.2017.Doi Number 

Energy Efficient Learning with Low 
Resolution Stochastic Domain Wall 
Synapse Based Deep Neural Networks 

Walid A. Misba1, Mark Lozano1, Damien Querlioz2, Senior Member, IEEE, and Jayasimha 
Atulasimha1,3, Senior Member, IEEE 
1Mechanical and Nuclear Engineering Department, Virginia Commonwealth University, Richmond, VA 23284 USA  
2 Université Paris-Saclay, CNRS, Centre de Nanosciences et de Nanotechnologies, Palaiseau, France  
3Electrical and Computer Engineering Department, Virginia Commonwealth University, Richmond, VA 23284 USA 

Corresponding author: (e-mail:). misbawa@vcu.edu, jatulasimha@vcu.edu 

This work was supported in part by the National Science Foundation (NSF) under Grant ECCS 1954589 and Grant CCF 1815033 and 
in part by Virginia Commonwealth Cyber Initiative (CCI) CCI Cybersecurity Research Collaboration Grant.  

ABSTRACT  We  demonstrate  that  extremely  low  resolution  quantized  (nominally  5-state)  synapses  with 
large stochastic variations in Domain Wall (DW) position can be both energy efficient and achieve reasonably 
high testing accuracies compared to Deep Neural Networks (DNNs) of similar sizes using floating precision 
synaptic weights. Specifically, voltage controlled DW devices demonstrate stochastic behavior as modeled 
rigorously  with  micromagnetic  simulations  and  can  only  encode  limited  states;  however,  they  can  be 
extremely  energy  efficient  during  both  training  and  inference.  We  show  that  by  implementing  suitable 
modifications to the learning algorithms, we can address the stochastic behavior as well as mitigate the effect 
of their low-resolution to achieve high testing accuracies. In this study, we propose both in-situ and ex-situ 
training algorithms, based on modification of the algorithm proposed by Hubara et al. [1] which works well 
with quantization of synaptic weights. We train several 5-layer DNNs on MNIST dataset using 2-, 3- and 5-
state DW device as synapse. For in-situ training, a separate high precision memory unit is adopted to preserve 
and accumulate the weight gradients, which are then quantized to program the low precision DW devices. 
Moreover, a sizeable noise tolerance margin is used during the training to address the intrinsic programming 
noise. For ex-situ training, a precursor DNN is first trained based on the characterized DW device model and 
a noise tolerance margin, which is similar to the in-situ training. The highest inference accuracies we obtain 
after the in-situ and ex-situ training are ~ 96.67% and ~96.63% which is very close to the baseline accuracy 
of ~97.1% obtained from a similar topology DNN having floating precision weights with no stochasticity. 
For  ex-situ  inference,  the  energy  dissipation  to  program  the  DW  devices  is  calculated  to  be  2.18  pJ  per 
inference. Remarkably, for in-situ inference the energy dissipation to program the devices is only 13 pJ per 
inference given that the training is performed over the entire MNIST dataset for 10 epochs. Large inter-state 
interval provided by the quantized weights and noise tolerance margin allowed during the in-situ training 
enables it to perform the training with significantly lower number of programming attempts. Our approach is 
specifically attractive for low power intelligent edge devices where the ex-situ learning can be utilized for 
energy efficient non-adaptive tasks and the in-situ learning can provide the opportunity to adapt and learn in 
a dynamically evolving environment. 

INDEX TERMS: Domain wall, synapse, quantized weight, deep neural network, energy efficient, 
neuromorphic, in-memory computing.  

I. INTRODUCTION 
Deep neural networks (DNNs) have proven to be successful in 
image  recognition  and  other  big  data  driven  classification 
tasks.  However,  implementing  a  DNN  with  traditional  von-
Neumann  computing  is  time  consuming  [2]  as  it  requires 

shuttling a large number of synaptic weight data stored in the 
memory  to  the  processing  unit  to  perform  matrix-vector 
multiplication during the forward propagation and backward 
propagation  stages.  Moreover,  shuttling  data  between  the 
computational unit and memory unit is energy intensive [3],

VOLUME XX, 2020 

1 

 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

linear updates and then periodically transfer them to the non-
volatile  PCM  [35].  However,  with  online  training  using  the 
techniques mentioned above during the weight update stage, 
each of the synaptic weights in the cross-bar array are updated. 
This has great implications for the endurance of the devices as 
well  as  energy  consumed  in  training  the  device.  Recently, 
mixed precision framework [36-37] has been proposed where 
large computational load such as the weighted sum operation 
(matrix-vector  multiplication)  along  with  the  conductance 
updates  are  performed  in  low  precision  computational 
memory unit and the weight updates are accumulated in a high 
precision unit. Using this framework, a large variety of DNNs 
have been shown to achieve high classification accuracy with 
significantly smaller number of weight updates [36].   
   In  contrast  to  the  online  training,  for  offline  training  the 
DNN  is  trained  in  software  and  the  actual  devices  are 
programmed based on the learned weights from software. In 
this  case,  hardware  nonidealities  are  characterized  first  and 
then included in the training process. To address stochasticity 
of the devices, Gaussian noise injection for the DNN weights 
has been proposed [38] and  have shown excellent accuracy. 
Random gaussian noise is also added to the ternary weights 
(3-state  weight)  of  a  DNN  [39].  Variation  aware  off-line 
training algorithm is reported in [40-41] where the variation in 
device conductances and device defects are first characterized 
and  then  incorporated  during  the  training  of  the  DNN  in 
software.  In  another  case  involving  a  deep  convolutional 
neural network, the optimal weights for convolution layers and 
fully-connected layers are learned via off-line training before 
the fully connected layers’  weights being updated by online 
training [42]. In this work, we have shown both online and off-
line training strategies to achieve high classification accuracy 
for DNN implemented with highly stochastic (non-gaussian) 
and extremely low resolution (nominally 2-state, 3-state and 
5-state for synaptic weights) analog DW based computational 
memory  devices.  Moreover,  we  show  that  our  proposed 
framework  for  on-line  training  requires  significantly  lower 
number of weight updates. 
   The  rest  of  the  paper  is  organized  as  follows.  In  the 
methods section, we detail the architecture of the DW device 
that can work as a synapse in the DNN and discuss the in-
situ and ex-situ learning algorithms of such DW device based 
DNN. For both of the learning algorithms we adapt quantized 
neural  network  learning  algorithm  [1,29]  with  several 
modifications including the weight deviation tolerance from 
target weight to account for the programming noise intrinsic 
to such stochastic DW devices. For ex-situ training, we also 
incorporate  the  statistical  distribution  of  the  DW  device 
conductance  during  the  training,  which  helps  to  achieve 
higher  test  accuracy.  This  is  followed  by  the  results  and 
discussion section, and then a conclusion. 

these 

In-memory  computing 

which  hinders  the  implementation  of  such  DNNs  in  edge 
devices where energy is at a premium. 
   In-memory computing has been widely explored to reduce 
the  physical  separation  between  computation  and  memory 
is  a  non-von-Neumann 
unit. 
computing paradigm where the computational memory units 
are  arranged  in  a  way  that  certain  computational  tasks  take 
place in the memory itself [4-5]. Matrix vector multiplication, 
the  most  computationally  intensive  part  of  a  DNN  [6],  has 
been  demonstrated  with  in-memory  computing  [7-8].  When 
the computational memory units are connected in a crossbar 
and  programmed  to  provide  conductances  equivalent  to  the 
the  matrix-vector  multiplication 
DNN  weights  [9-10], 
operation  can  be  implemented  in  single  time  step  [4,6]  and 
with minimal data movement. Computational memory such as 
phase change random access memory (PCM) [11,12], resistive 
random-access  memory  (RRAM)  [13,14],  arranged  in  a 
crossbar array have been shown to classify handwritten digits 
[9,15] and recognize human faces [16]. However, these analog 
memory devices have stochastic and non-linear responses and 
provide  limited  resolution  for  synaptic  weights.  To  achieve 
issues  should  be 
higher  classification  accuracy, 
addressed with appropriate training algorithms. 
   Recently  spintronic  memory  devices  are  being  widely 
explored  for  in-memory  DNN  implementation  because  of 
their non-volatility, high endurance, high speed of access, high 
scalability  and  compatibility  with  CMOS 
technology 
[2,17,18-22].  Among  these  spintronic  devices,  DW  based 
computation memory [17,18] is promising and these devices 
can be programmed with a low energy budget [23]. However, 
similar to other analog devices, DW devices have limitations 
such as their stochastic behavior [24-27] and low resolution 
due  to  the  relatively  small  on/off  ratio  of  magnetic  tunnel 
junctions (MTJs)  which are 7:1 at best at room temperature 
[28]. 
   With recent advances in computing, researches have shown 
fast and energy efficient implementation of DNNs with low 
resolution synaptic weight [1,29-32], where the weights value 
can  be  only  binary  (1-bit  or  2-state)  [29].  However,  for  the 
weight  update,  gradients  are  calculated  in  full  precision  to 
achieve high accuracy [1]. This idea of keeping full precision 
gradient  information  for  training  a  network  with  limited 
precision  synaptic  weights  can  be  useful  for  a  DNN  that  is 
built  from  energy  efficient  DWs  or  other  analog  low-
resolution devices.   
   Apart from the low resolution, stochasticity and non-linear 
response  of  the  analog  devices  should  be  addressed  during 
training to achieve higher classification accuracy. To address 
stochasticity  of  the  analog  devices  both  online  (in-situ)  and 
off-line (ex-situ) training of the DNN are proposed. For online 
training,  multiple  devices  per  synapse  have  been  proposed 
with  [33]  or  without  ‘periodic  carry’  [34]  to  address  device 
variability  and  noise.  In  another  work,  a  3T1C  module 
(consists  of  3  CMOS  transistors  and  1  capacitor)  is  used  in 
conjunction to the stochastic PCM device to accumulate small 

2 

VOLUME XX, 2020 

 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

𝑧𝑧⃗

    ,  where 

is the damping constant, 

current flow and 
𝐽𝐽̂ × 𝑧𝑧⃗
𝐽𝐽̂ 
Also,  
layer, 

is  the  unit  vector  defining  the  direction  of 
  is the direction of inversion asymmetry. 
 is the value of current flowing through the heavy metal 
 is the spin Hall angle which is 0.1 for heavy metal Pt, 
𝐽𝐽
 is 
 𝐽𝐽
the  saturation 
the  unit  magnetization  vector, 
𝜓𝜓 
𝑚𝑚��⃗
  is  the 
magnetization,
permeability of free space, 
 is the 
thickness of the nanowire. To equate the Slonczewski torque 
𝑒𝑒
with Spin orbit torque we assume spin polarization to be 

𝛾𝛾
 is the electron charge and 
𝜇𝜇0
𝑑𝑑

 is the gyromagnetic ratio, 
is 

  is  the  reduced  Planck  constant, 

𝑀𝑀𝑠𝑠

 ℏ

accounts 

 and Slonczewski parameter to be

. Here the effective 
𝑃𝑃 =
from 
for 
field, 
1
demagnetization, perpendicular magnetic anisotropy (PMA), 
exchange  interaction  from  Heisenberg  and  Dzyaloshinskii–
Moriya  interaction  (DMI),  stress  induced  anisotropy  and 
thermal noise.  

 can be expressed as follows: 

the 
 𝛬𝛬 = 1

contributions 

𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒

𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒

𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒 = 𝐻𝐻��⃗𝑎𝑎𝑎𝑎𝑎𝑎𝑠𝑠 + 𝐻𝐻��⃗𝑑𝑑𝑒𝑒𝑑𝑑𝑎𝑎𝑑𝑑 + 𝐻𝐻��⃗𝑠𝑠𝑠𝑠𝑠𝑠𝑒𝑒𝑠𝑠𝑠𝑠 + 𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒ℎ

   PMA induced effective field can be expressed as, 

+ 𝐻𝐻��⃗𝑠𝑠ℎ𝑒𝑒𝑠𝑠𝑑𝑑𝑎𝑎𝑒𝑒

(3) 

 : 

(4) 

𝐻𝐻��⃗𝑎𝑎𝑎𝑎𝑎𝑎𝑠𝑠

𝐾𝐾𝑢𝑢

𝐻𝐻��⃗𝑎𝑎𝑎𝑎𝑎𝑎𝑠𝑠 =

2𝐾𝐾𝑢𝑢
𝜇𝜇0𝑀𝑀𝑠𝑠 (𝑢𝑢�⃗. 𝑚𝑚��⃗)𝑢𝑢�⃗
  is  the  first  order  anisotropy  constant  and 

where 
represents the uniaxial anisotropy direction (i.e. perpendicular 
𝑢𝑢�⃗
to plane). Voltage applied across the piezoelectric induces an 
in-plane  stress  [27,46],  which  is  then  transferred  to  the 
racetrack (heavy metal layer is thin ~ 5nm) and modulates the 
PMA by means of stress induced anisotropy resulting from the 
magnetoelastic effect. Voltage induced anisotropy modulation 
    of  the 
is  incorporated  by  varying  the  PMA  coefficient, 
racetrack in our simulation.  
   The effective field due to the interfacial DMI is expressed as 
follows [45]: 

𝐾𝐾𝑢𝑢

II.  Methodology 

×

×

 60 nm 

A.  DW BASED NANO-SYNAPSE AND 
MICROMAGNETIC MODELING FOR DEVICE 
STOCHASTICITY 
We  model  our  synapse  on  a  magnetic  domain  wall  (DW) 
based nanodevice, which is non-volatile in nature. Once  the 
memory  state  (here  the  synaptic  weight)  is  written,  the 
information is retained for a long time. For the nano-synapse 
device, we simulated a thin ferromagnetic racetrack having a 
dimension of 600 nm 
 1 nm with a DW initialized 
and stabilized in a notch at one end. In addition, we assume 
several  engineered  notches  at  regular  intervals  along  the 
racetrack.  The  racetrack  dimension  and  notch  intervals  are 
shown in Fig. 1a. Moreover, we considered edge irregularities 
(rms roughness of ~ 2nm) in the racetrack to mimic the effect 
of  lithographic  imperfections  by  randomly  removing  or 
adding some finite difference cells from the edges [43-44]. We 
assume the racetrack is on top of a heavy metal layer that is 
patterned  on  top  of  piezoelectric  layer  (see  Fig.  1b).  An 
insulator  (MgO  layer)  and  a  reference  ferromagnetic  layer 
(one could also add a synthetic antiferromagnet (SAF) layer to 
cancel dipole coupling from this fixed layer) are stacked on 
top  of  the  racetrack,  these  two  layers  combined  with  the 
racetrack  ferromagnetic  layer  (free  layer)  forms  a  magnetic 
tunnel  junction  (MTJ),  which  facilitates  the  readout  of  the 
device.  With  this  configuration,  a  combination  of  fixed 
amplitude  and  fixed  time  current  pulse  “or  clocking  signal” 
injected  in  the  heavy  metal  layer  and  a  varying  amplitude 
“control”  voltage  pulse  applied  across  the  piezoelectric 
translates the domain  wall into different distances along the 
racetrack.  Different  positions  of  the  DW  lead  to  different 
conductances of the MTJ (see Fig. 1b) thus forming a voltage 
programmable non-volatile synapse. 

B.  MAGNETIZATION DYNAMICS 
The magnetization dynamics in the presence of heavy metal 
layer current,  which exerts Spin Orbit Torque (SOT) on the 
ferromagnetic strip is simulated in MUMAX3 [45] using the 
Landau–Lifshitz–Gilbert-Slonczewski equation: 

2

(1 + 𝜓𝜓

)

𝑑𝑑𝑚𝑚��⃗
𝑑𝑑𝑡𝑡

= −𝛾𝛾𝑚𝑚��⃗ × 𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒

− 𝜓𝜓𝛾𝛾 �𝑚𝑚��⃗ × �𝑚𝑚��⃗ × 𝐻𝐻��⃗𝑒𝑒𝑒𝑒𝑒𝑒��
′
− 𝛽𝛽𝛾𝛾(𝜀𝜀 − 𝜓𝜓𝜀𝜀
+  𝛽𝛽𝛾𝛾(𝜀𝜀

− 𝜓𝜓𝜀𝜀)(𝑚𝑚��⃗ × 𝑚𝑚��⃗𝑃𝑃)

′

)�𝑚𝑚��⃗ × (𝑚𝑚��⃗𝑃𝑃 × 𝑚𝑚��⃗)�

𝛽𝛽 =

ℏ𝐽𝐽𝐽𝐽
𝜇𝜇0𝑒𝑒𝑑𝑑𝑀𝑀𝑠𝑠 ,

𝜀𝜀 =

2

2
𝑃𝑃𝛬𝛬
2

+ 1) + (𝛬𝛬
   We consider secondary spin torque parameter to be 
and neglect the field like torque. Here, spin polarization  
′

− 1)�𝑚𝑚��⃗ ⋅ 𝑚𝑚��⃗𝑝𝑝�

(𝛬𝛬

VOLUME XX, 2020 

𝐻𝐻��⃗𝐷𝐷𝐷𝐷𝐷𝐷 =

2𝐷𝐷
𝜇𝜇0𝑀𝑀𝑠𝑠  �
Here, D is the DMI constant and 

𝜕𝜕𝑚𝑚𝑧𝑧
𝜕𝜕𝜕𝜕

𝜕𝜕𝑚𝑚𝑧𝑧
𝜕𝜕𝜕𝜕

,

, −

(5) 

𝜕𝜕𝑚𝑚𝑒𝑒
𝜕𝜕𝜕𝜕

𝜕𝜕𝑚𝑚𝑦𝑦
𝜕𝜕𝜕𝜕

�

−

, 

 and 

 are the x, 

(1) 

(2) 

y  and  z  component  of  unit  magnetization  vector 
respectively.  Thermal noise induces a random effective field 
𝑚𝑚��⃗

𝑚𝑚𝑦𝑦

𝑚𝑚𝑒𝑒

𝑚𝑚𝑧𝑧

  [47]:  

𝐻𝐻��⃗𝑠𝑠ℎ𝑒𝑒𝑠𝑠𝑑𝑑𝑎𝑎𝑒𝑒

(6) 

Here, 

𝐻𝐻��⃗𝑠𝑠ℎ𝑒𝑒𝑠𝑠𝑑𝑑𝑎𝑎𝑒𝑒 = 𝜂𝜂⃗ �

2𝜓𝜓𝜓𝜓𝜓𝜓
𝜇𝜇0𝑀𝑀𝑠𝑠𝛾𝛾Ω𝛥𝛥
 is a random variable with Gaussian distribution with 
mean zero and unit variance and independent (uncorrelated) in 
each of the 3 cartesian coordinates generated at each time step, 
 is the time 

 is Boltzmann constant, 

 is the cell volume, 

𝜂𝜂⃗

𝜀𝜀

= 𝜓𝜓𝜀𝜀
𝑚𝑚��⃗𝑃𝑃 =

step size. The simulation parameters are listed in Table I.
𝜓𝜓

Ω

𝛥𝛥

3 

 
 
 
 
 
 
 
 
 
 
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

TABLE I  
SIMULATION PARAMETER 

Parameters 

Values 

DMI constant (D) 
Gilbert damping (

) 

Saturation magnetization (

𝜓𝜓
Exchange constant (

𝑀𝑀𝑠𝑠
Saturation magnetostriction (

𝐴𝐴𝑒𝑒𝑒𝑒
Perpendicular Magnetic 
Anisotropy (

) 

) 

0.0006 
0.03 

𝐽𝐽𝑚𝑚

−2

6

−1

 𝐴𝐴𝑚𝑚

10
250 
2 × 10

−11

−1

𝐽𝐽𝑚𝑚

) 

) 

𝜆𝜆𝑠𝑠

𝑝𝑝𝑝𝑝𝑚𝑚
5

−3

7.5 × 10

𝐽𝐽𝑚𝑚

𝐾𝐾𝑢𝑢

𝐾𝐾𝑢𝑢

C.  MAPPING DOMAIN WALL POSITION TO 
CONDUCTIVITY 
The distribution of equilibrium DW positions for five different 
programming  voltages,  represented  by  different  PMA 
, in addition to fixed amplitude and fixed time 
coefficient, 
 applied for 1 ns) in the 
SOT current pulse (
presence of room temperature thermal noise are shown in Fig. 
1c.  The  mean  equilibrium  DW  positions  are  different  for 
,  which  implies  that  different  programming 
different 
voltages  can  be  chosen  for  different  synaptic  states.  For 
example,  one  can  select  five,  three,  or  two  different 
programming  voltages  to  implement  a  5-state,  3-state  or  2-
state  synapse.  Equilibrium  DW  positions  can  be  linearly 
mapped  to  a  conductance  value  by  means  of  the  following 
equations: 

35 × 10

⁄
𝐴𝐴 𝑚𝑚

𝐾𝐾𝑢𝑢

10

2

(7) 

=

+

𝑠𝑠𝑦𝑦𝑎𝑎𝑎𝑎𝑝𝑝𝑠𝑠𝑒𝑒

  < 𝑚𝑚𝑧𝑧 >

𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎
𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 + 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎
𝐺𝐺
2
2
where, 
  is  the  average  magnetization  moment  of 
ferromagnetic racetrack along z-direction. The distribution of 
< 𝑚𝑚𝑧𝑧 >
 is shown  in  Fig. 1d,  which can be derived directly 
 of the 
from DW position. The maximum conductance 
< 𝑚𝑚𝑧𝑧 >
the  average 
DW  nano-synapse  device  occurs  when 
𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒
1. 
magnetization of the ferromagnetic racetrack is 
In this scenario, the DW moves to one end of the racetrack and 
the  magnetization in the racetrack points to the +z-direction 
(reference  ferromagnetic  layer  magnetization  is  assumed  to 
point  upward,  parallel  state).  The  device  conductance  is 
minimum 

 when the average magnetization is 
1,  the  DW  moves  all  the  way  to  the  other  end  and  the 
< 𝑚𝑚𝑧𝑧 >=
magnetization  in  the  racetrack  points  downward  (reference 
−
layer magnetization points upward, anti-parallel state).  

< 𝑚𝑚𝑧𝑧 >=

𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎

FIGURE 1. a. Micromagnetic configuration of a ~2 nm rms rough edge 
racetrack with perpendicular magnetic anisotropy (PMA). Engineered 
notches are placed regularly at 75 nm interval.  A DW is initialized at a 
notch 60 nm from the left of the racetrack. b.  DW based nano-synapse 
device: racetrack ferromagnet/insulator/reference ferromagnet (MTJ) on 
top of a heavy metal layer on a piezoelectric substrate. A fixed current, J 
in a heavy metal layer along with different amplitude voltage, V applied 
across the piezoelectric, which changes the perpendicular anisotropy 
(PMA or 
  constant) of the racetrack and translates the DW (shown in 
red rectangle) into different longitudinal positions along the racetrack. 
c.  Distribution of equilibrium DW positions in the racetrack (shown in 
Fig. 1a) at room temperature T=300K for a fixed SOT current pulse of 

𝑲𝑲𝒖𝒖

 applied for 1 ns and five different PMA coefficients, 

𝟐𝟐

𝟏𝟏𝟏𝟏

(corresponds to five different programming voltages). Different mean 
𝑲𝑲𝒖𝒖
⁄
𝟑𝟑𝟑𝟑 × 𝟏𝟏𝟏𝟏
𝑨𝑨 𝒎𝒎
positions for different 
stochastic synapses can be implemented by choosing 5,3 or 2 different 
programming voltages. d. distribution of average perpendicular 
magnetization, 
Eq. 13) derived directly from DW position. 

𝑲𝑲𝒖𝒖
 (which is equivalent to DNN weights according to 

 implies that 5-state, 3-state or 2-state 

𝑱𝑱 =

< 𝒎𝒎𝒛𝒛 >

III.  LEARNING OF FULLY CONNECTED DNN WITH DW 
NANO-SYNAPSE 

, 

𝜕𝜕1

𝜕𝜕784

×
,…, 

A.  CROSSBAR WITH DW DEVICES 
We assume a crossbar architecture for the DW devices (Fig. 
2b) that implements a fully connected DNN (Fig. 2a). The task 
of  DNN  studied  here  is  classification  of  handwritten  digits 
from  MNIST  test  images  [48].  The  network  is  trained  with 
 28 pixels or a total of 784 
MNIST images each having 28 
pixels  with  intensity  values  ranging  from  0-255.  The  pixel 
}, acts as input of the 
intensities of the image, {
DNN. The output of the DNN is the classification of the digit 
𝜕𝜕2
for the given input image. For input digit 0 the output should 
be  {1,0,0,0….}  and  for  digit  1  the  output  should  be 
{0,1,0,0….} and so on. We have considered 3 hidden layers 
for  the  DNN  and  the  numbers  of  neurons  for  input  layer, 
hidden layers and output layer are chosen to be 784-392-196-
98-10. The reason for the choices is discussed in the results 
section. In a typical DNN shown in Fig. 2a, at each forward 
pass, any layer neuron computes the weighted sum of inputs 
from the previous layer neurons and then passes it through a 
non-linear  activation  function  to  generate  its  output.  If  the 
,  and  the 
inputs  from  layer
weights  of  the  connections  between  layer
  and 
  neuron 
𝑒𝑒
𝜕𝜕𝑎𝑎
−𝑙𝑙
,  then  the 
layer
  neuron  “
  (
"𝑖𝑖"
, is expressed as [49],
output of neuron 

  neurons  are  represented  by 

  is  represented  by 

−𝑙𝑙

−

𝑙𝑙 + 1)

𝑗𝑗"

𝑊𝑊𝑎𝑎𝑖𝑖

4 

𝑗𝑗

VOLUME XX, 2020 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

   Combining  Eq.  10  and  12  and  considering  the  maximum 
absolute value of DNN weight as 

, one can get,  

(8) 

𝑒𝑒+1
𝜕𝜕𝑖𝑖

𝑒𝑒
= 𝑓𝑓 �� 𝑊𝑊𝑎𝑎𝑖𝑖𝜕𝜕𝑎𝑎

�

𝑓𝑓 

represents the activation function of the neuron, for 

where 
which we used the sigmoid function in our simulation. 
   In the hardware implementation of DNN with crossbar (see 
Fig. 2b), the inputs to any layer neurons are transformed into 
a physical voltage before feeding into the rows of the crossbar. 
These  voltages  can  be  generated  by  scaling  the  inputs:  
,…}. DW devices situated at each cross-
point of the crossbar implements the weights of the connection 
𝑉𝑉
between  two  layers  of  neurons.  The  conductance  of  the 
 of 
devices can be scaled linearly to represent the weights 
the DNN [17].  

,…, 

𝑒𝑒
𝜕𝜕2

𝑒𝑒
𝜕𝜕1

𝑒𝑒
𝜕𝜕𝑎𝑎

𝑠𝑠𝑒𝑒𝑎𝑎𝑒𝑒𝑒𝑒

, 

{

𝑊𝑊𝑎𝑎𝑖𝑖

(9) 

𝑠𝑠𝑦𝑦𝑎𝑎𝑎𝑎𝑝𝑝𝑠𝑠𝑒𝑒

𝐺𝐺𝑎𝑎𝑖𝑖

𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 + 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎
2

(𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎)𝑊𝑊𝑎𝑎𝑖𝑖
2 𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒

+

=

𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒

 is the maximum absolute value for the weights of 
Here,  
,  can  be  both  positive  and 
the  DNN.  DNN  weights,
negative; however, the DW devices can only provide positive 
conductance  values.  To  address  the  issue,  one  can  add  a 
, to each of the cross-
parallel conductance, 
points in the crossbar and feed this parallel conductance with 
a voltage that is of opposite polarity to the voltage applied to 
the  DW  device  [17].  With  this  parallel  conductance  the 
effective conductance, 

, at each cross-point would be,  

𝐺𝐺𝑚𝑚𝑚𝑚𝑚𝑚+𝐺𝐺𝑚𝑚𝑚𝑚𝑚𝑚

𝐺𝐺𝑃𝑃 =

 𝑊𝑊𝑎𝑎𝑖𝑖

2

(10) 

𝐺𝐺𝑎𝑎𝑖𝑖
(𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎)𝑊𝑊𝑎𝑎𝑖𝑖
𝐺𝐺𝑎𝑎𝑖𝑖 =
2 𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒
 could be both positive and negative depending 
   The value 
on  the  values  of  the 
  of  the  DNN.  Once  the  device 
conductance,
,  the 
𝑊𝑊𝑎𝑎𝑖𝑖
weighted sum operation, 
  (shown in Eq. 8), can be 
implemented  in  the  crossbar  by  means  of  Ohm’s  and 
𝑒𝑒
∑ 𝑊𝑊𝑎𝑎𝑖𝑖𝜕𝜕𝑎𝑎
Kirchhoff’s laws as follows: 

,  is  programmed  according  to 

𝑊𝑊𝑎𝑎𝑖𝑖

𝐺𝐺𝑎𝑎𝑖𝑖

𝐺𝐺𝑎𝑎𝑖𝑖

𝐼𝐼𝑖𝑖 = � 𝐼𝐼𝑎𝑎𝑖𝑖 = 𝑉𝑉

𝑠𝑠𝑒𝑒𝑎𝑎𝑒𝑒𝑒𝑒 (𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎)

2 𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒 � 𝑊𝑊𝑎𝑎𝑖𝑖𝜕𝜕𝑎𝑎

   The  additional  scaling  factor, 

(11) 

𝑒𝑒

  ,  can  be 

𝑠𝑠𝑒𝑒𝑎𝑎𝑒𝑒𝑒𝑒 (𝐺𝐺𝑚𝑚𝑚𝑚𝑚𝑚−𝐺𝐺𝑚𝑚𝑚𝑚𝑚𝑚)

𝑉𝑉

2 𝑊𝑊𝑚𝑚𝑚𝑚𝑚𝑚

normalized by appropriate design of the peripheral circuitry of 
the crossbar. In addition, the peripheral circuit computes the 
activation  function  and  generates  the  output  of  the  neuron 
(typically  voltage  signals),  which  then  acts  as  inputs  for 
subsequent layer neurons.  
   When a parallel conductance, 
, is added to the DW device 
with an input voltage of opposite polarity then, from Eq. 7, the 
device  conductance  can  be  expressed  in  terms  of  average 
magnetization 

𝐺𝐺𝑃𝑃

. 

< 𝑚𝑚𝑧𝑧 >

𝑠𝑠𝑦𝑦𝑎𝑎𝑎𝑎𝑝𝑝𝑠𝑠𝑒𝑒

𝐺𝐺

= 𝐺𝐺𝑎𝑎𝑖𝑖 =

𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎
2

  < 𝑚𝑚𝑧𝑧 >

(12) 

𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒 = 1

(13) 

𝑊𝑊𝑎𝑎𝑖𝑖 =< 𝑚𝑚𝑧𝑧 >

   From the above equation it is clear that if we train a DNN 
shown in Fig. 2a with weights (both positive and negative) that 
are derived from micro-magnetics (see Fig. 1d) for different 
programming  conditions  we  are  effectively  implementing  a 
hardware DNN with DW devices shown in Fig. 2b given that 
the peripheral circuitry is designed to provide the appropriate 
scaling. 
   After  each  forward  pass,  the  DNN  may  not  be  able  to 
correctly predict the input image. Therefore, an error signal is 
calculated at the output layer, 

, which is expressed as 

𝑖𝑖

𝐿𝐿
𝛿𝛿𝑎𝑎

𝐿𝐿
𝑑𝑑𝑎𝑎

  and 

,  where 

𝐿𝐿
− 𝑑𝑑𝑎𝑎

  are  the  predicted  and  desired 
=
𝐿𝐿
. This error signal is 
outcomes of the output layer’s neuron 
𝐿𝐿
𝐿𝐿
𝜕𝜕𝑎𝑎
𝜕𝜕𝑎𝑎
used  to  train  the  network’s  learnable  parameters,  such  as 
weights,  using  back  propagation  of  errors  and  stochastic 
gradient  descent.  During  the  backward  pass,  any  layer’s 
neurons’ errors are scaled to voltage values by the peripheral 
circuitry  designed,  which  are  then  fed  into  the  cross-bar 
columns (different from the forward pass where the voltages 
are fed into the rows of the crossbar) to generate errors for the 
In  standard 
preceding 
layer  neurons  at 
 neurons 
backpropagation of a DNN, errors of layer
are back propagated to compute error of the previous layer 

rows. 

the 

 neuron 

 as follows:     

−

 𝑙𝑙 + 1 

 𝑙𝑙

𝑖𝑖

𝑒𝑒
𝛿𝛿𝑎𝑎

= � 𝑊𝑊𝑎𝑎𝑖𝑖𝛿𝛿𝑖𝑖

𝑒𝑒+1

   Weighted  sum  operation  shown  above  is  implemented  in 
crossbar by means of Ohm’s and Kirchhoff’s laws: 

−
(14) 

𝐼𝐼𝑎𝑎 = � 𝐼𝐼𝑎𝑎𝑖𝑖 = 𝑉𝑉

𝑠𝑠𝑒𝑒𝑎𝑎𝑒𝑒𝑒𝑒 (𝐺𝐺𝑑𝑑𝑎𝑎𝑒𝑒 − 𝐺𝐺𝑑𝑑𝑎𝑎𝑎𝑎)

𝑒𝑒+1

2 𝑊𝑊𝑑𝑑𝑎𝑎𝑒𝑒 � 𝑊𝑊𝑎𝑎𝑖𝑖𝛿𝛿𝑖𝑖

Similar to the forward pass, in backward pass the peripheral 
circuit  can  be  designed  to  normalize the  scaling  factor.  The 
detailed learning algorithms are discussed in the next section. 

(15) 

B.  BACKPROPAGATION AND LEARNING ALGORITHM 
For  the  training  of  the  DNN,  we  update  the  weights  by 
calculating the gradient of a cost function with respect to the 
weights.  We  considered  mean  square  error, 

1
𝐿𝐿
2 ∑(𝜕𝜕𝑎𝑎

2
)

  as  our  cost  function  where  the  gradient  of  the  cost 
−
function with respect to the output of the output layer neuron 
𝐿𝐿
𝑑𝑑𝑎𝑎
(we also call it error ). Once 
 is expressed as , 
the output layer’s errors are determined, the preceding layer’s 
𝐿𝐿
− 𝑑𝑑𝑎𝑎
𝑖𝑖
errors can be calculated using the backpropagation equation, 
,  which  is  different  from  the  backpropagation 
    reported  in  ref.  [50]  where 

𝐿𝐿
= (𝜕𝜕𝑎𝑎

𝐿𝐿
𝛿𝛿𝑎𝑎

𝐶𝐶 =

𝑒𝑒+1

) 

 is the gradient of the activation function of layer 
 𝑓𝑓′𝑒𝑒+1
neuron. In other words, we do not back propagate the gradients 
𝑓𝑓′𝑒𝑒+1
𝑙𝑙 + 1
of activation function as it does not achieve high testing 

= 𝑊𝑊𝑎𝑎𝑖𝑖𝛿𝛿𝑖𝑖

𝑒𝑒+1

𝑒𝑒
equation, 
= 𝑊𝑊𝑎𝑎𝑖𝑖𝛿𝛿𝑖𝑖
𝛿𝛿𝑎𝑎
𝑒𝑒
𝛿𝛿𝑎𝑎

VOLUME XX, 2020 

5 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
    
 
 
 
    
 
 
 
    
 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

𝑾𝑾𝒊𝒊𝒋𝒋

 . At forward propagation, inputs to neuron 

 are summed and passed through an activation function 

”) and the number of rows in the crossbar are determined by the number of neurons in layer, 

FIGURE 2. a. Architecture of a fully connected deep neural network (DNN). Any neuron 
weight 
backward propagation, errors of layer 
in crossbar with DW devices. The peripheral circuit and the crossbar shown here implements DNN functionalities of only one layer (“
layer (“
of neurons in layer, 
𝒍𝒍 + 𝟏𝟏
such that 
shown at the right-hand side of the crossbar. For each of the DW devices there is a corresponding high precision weight (real weight) that is stored in 
a separate memory unit. These high precision weights are updated after a forward and backward pass before passing it through a quantizer (i.e., 2,3 or 
5-level quantization, depending on the states of the device). The DW device conductance, 
outside of the prescribed range of the target quantized weight, 

. Inputs and errors of neurons are scaled to voltages before feeding them into the crossbar. The flow of the training algorithm is 
𝑾𝑾𝒊𝒊𝒋𝒋
𝒍𝒍 + 𝟏𝟏

 to generate its output, 
𝒍𝒍 + 𝟏𝟏
   b. Implementation of the DNN 
”) and the next 
 and the number of columns by the number 

. At each cross point of the crossbar there is a DW device with conductance 

 neurons are back propagated to calculate the error 

, which is equivalent to the DNN weights 

. Figure idea adopted from [36]. 

) are updated when they fall 

 is connected to neuron 

 (or the corresponding 

𝒍𝒍
 of neuron 

 with synaptic 

 in layer 
𝒇𝒇

𝑮𝑮𝒊𝒊𝒋𝒋 = 𝝁𝝁𝑾𝑾𝒊𝒊𝒋𝒋

 in layer 

 in layer 

𝒍𝒍+𝟏𝟏
𝒙𝒙𝒋𝒋

𝒍𝒍 + 𝟏𝟏

. At 

𝒍𝒍
𝜹𝜹𝒊𝒊

𝑮𝑮𝒊𝒊𝒋𝒋

𝒋𝒋

𝒋𝒋

𝒊𝒊

𝒊𝒊

𝒍𝒍

𝒍𝒍

𝒍𝒍

𝑮𝑮𝒊𝒊𝒋𝒋

𝑾𝑾𝒊𝒊𝒋𝒋

𝑾𝑾𝒒𝒒

accuracy  with quantized  weights. Finally, the derivative of 
the  cost  function  with  respect  to  the  weights  is  calculated, 
  for  the 
which  determines  the  weight  update  signal, 
weights connected between layer 
neuron 

 and layer 
∆𝑾𝑾𝒊𝒊𝒋𝒋

 neuron 

, 

𝒍𝒍

𝒊𝒊

𝒍𝒍 + 𝟏𝟏

training algorithm. For that, the high precision weights are 
quantized  at  each  forward  pass  during  the  training.  For 
weight quantization, we use the following sets of functions 
in the manner of Ref [51]: 

𝒋𝒋

(16) 

𝒄𝒄𝒍𝒍𝒊𝒊𝒄𝒄(𝒙𝒙, 𝒂𝒂, 𝒃𝒃) = 𝐦𝐦𝐦𝐦𝐦𝐦(𝐦𝐦𝐦𝐦𝐦𝐦(𝒙𝒙, 𝒂𝒂) , 𝒃𝒃)

𝜼𝜼

𝒇𝒇′𝒍𝒍+𝟏𝟏

𝒍𝒍
∆𝑾𝑾𝒊𝒊𝒋𝒋 = 𝜼𝜼𝒙𝒙𝒊𝒊

𝒍𝒍+𝟏𝟏
𝜹𝜹𝒋𝒋
Here,  
 denotes the learning rate. For our learning algorithm 
we propose to store the updated weights in a separate high 
precision memory unit. That way, the gradients with respect 
to the weights can be calculated accurately [1]. We note that 
these  high  precision  weights  are  different  from  the  actual 
synaptic  weights (or equivalent conductances) provided by 
the  DW  device  that  are  quantized  and  of  low  precision. 
However, we use these high precision weights to update the 
DW device weights (conductances). As we apply stochastic 
gradient  decent  for  optimization,  these  high  precision 
weights  are  updated  at  each  forward  pass  with  an  input 
image. 

𝜟𝜟 =

𝒃𝒃 − 𝒂𝒂
𝒏𝒏 − 𝟏𝟏

 (17) 

𝒄𝒄𝒍𝒍𝒊𝒊𝒄𝒄(𝒙𝒙, 𝒂𝒂, 𝒃𝒃) − 𝒂𝒂
𝜟𝜟

�� × 𝜟𝜟 + 𝒂𝒂

𝒒𝒒 = �𝒓𝒓𝒓𝒓𝒖𝒖𝒏𝒏𝒓𝒓 �

, 

   Where 

 is the quantized value of the real valued number 
  is  the  level  of 
is  the  quantization  range  and 
𝒒𝒒
quantization.  After  quantization,  a  programming  pulse  is 
𝒙𝒙
[𝒂𝒂; 𝒃𝒃] 
generated to update the DW device weights to the quantized 
value, a target that is similar to the quantized neural network 
learning algorithm [1]. Typically, two types of training are 
possible  for  a  DNN  implemented  with  DW  based  nano- 
synapse device: in-situ and ex-situ.

𝒏𝒏

 As DW devices can only provide limited resolution in their 
synaptic  weights  we  adopt  weight  quantization  in  our 

6 

VOLUME XX, 2020 

 
 
 
 
 
 
 
 
 
 
 
 
In in-situ training the DNN is trained and tested in hardware. 
In  contrast,  in  ex-situ  training,  a  precursor  DNN  is  first 
trained  in  software  and  then  the  learned  weights  are 
transferred  to  the  DW  devices  to  provide  the  equivalent 
learned weights prior to testing.  
1) IN-SITU TRAINING 
Here,  we  describe  in  detail  the  step-by-step  in-situ  training 
algorithm as shown in Fig. 2b. For each DW device in crossbar 
there is a corresponding high precision weight that is stored in 
a separate memory unit. Initially, these high precision weights 
are chosen at random from a gaussian distribution. After each 
forward  and  backward  pass  these  weights  are  updated 
according  to  Eq.  16.  Then,  these  weights  are  clipped  and 
quantized  so  that  they  lie  between  -1  to  1.  After  that,  a 
programming  pulse  is  sent  to  the  DW  device  to  update  its 
synaptic weight value to the quantized value. For example, in 
5-level  quantization  (5-state  for  a  synaptic  device)  the 
quantized  weights  can  be  of  any  value  from  the  set 

𝛼𝛼

𝐾𝐾𝑢𝑢

< 𝑚𝑚𝑧𝑧 >

5
× 10

𝑊𝑊𝑞𝑞
.  Therefore,  at  each 

.  Five  different  programming  voltages 
𝑊𝑊𝑞𝑞 ∈
 = 8, 7.75, 7.5, 
can be applied to the device, which results in 
(−1, −0.5, 0, 0.5 ,1)
) J/m3 to achieve five different quantized 
7.25 and 7.0 (
weights of -1, -0.5,0,0.5 and -1 respectively as seen from Fig. 
 according to Eq. 13). 
1d (DW device weights, 
Because of the significant spread that exists in the DW device 
𝑊𝑊𝑎𝑎𝑖𝑖 =< 𝑚𝑚𝑧𝑧 >
  distribution)  due  to  the  stochastic 
weights  (or  the 
nature  of  the  device  we  introduced  a  noise  tolerance 
hyperparameter called alpha 
 (real valued) during training, as 
after applying a programming pulse (fixed current + control 
voltage) the device weights can be of a value other than the 
desired quantized weight. For instance, if we want to program 
 then we would allow 
a DW device to a quantized weight of 
any  values  for the device  weights that satisfy the condition,  
iteration 
following quantization, we read the states of the DW device 
𝑊𝑊𝑞𝑞 − 𝛼𝛼 ≤ 𝑊𝑊𝑎𝑎𝑖𝑖 ≤ 𝑊𝑊𝑞𝑞 + 𝛼𝛼
(costs read energy but that is typically much lower than write 
energy) and only if it falls outside the noise tolerance margin, 
a  programming  pulse  is  sent  to  the  device  to  write  the 
corresponding  quantized  weight.  However,  due  to  the  large 
inter-state interval in quantized learning, a quantized weight 
does  not  change  at  each  forward  pass  (the  backpropagated 
errors  update  the  weights  slowly  due  to  low  learning  rate). 
Instead,  it  typically  changes  only  after  several  passes. 
Therefore,  the  noise  tolerance  condition  need  not  to  be 
satisfied strictly at each iteration. Furthermore, if a DW device 
weight is programmed outside the tolerance margin, it is not 
rectified in the current iteration, as it has a chance to satisfy the 
window  in  the  next  several  iterations.  This  relaxation  over 
noise  tolerance  condition  speeds  up  the  training  process 
without losing accuracy. Again, the DW device, which already 
satisfies  the  tolerance  margin,  need  not  be  programmed  for 
next  several  iterations  due  to  same  reason  of  the  quantized 
weights not being updated frequently.  Introduction of noise 
 is critical during the training of 
tolerance hyperparameter, 
this stochastic device based DNN. Without 
, the DW device 
needs to be programmed a significantly large number of times 
to achieve a particular quantized weight. On the other hand, a 

𝛼𝛼

𝛼𝛼

Author Name: Preparation of Papers for IEEE Access (February 2017) 

𝜎𝜎

𝛼𝛼

𝛼𝛼

 𝜎𝜎

and 

𝛼𝛼 = 0.15

𝛼𝛼 = 0.25

𝛼𝛼 = 0.25

𝛼𝛼 = 0.15 

 that is up to 

 = 8, 7.5, and 7.0 (

  allows  more  imprecise  weight  update  or 
high  value  of   
higher  variation  of  the  DW  device  weights  from  the  target 
values,  which  will  degrade  the  accuracy.  Thus,  a  proper 
balance needs to found for selecting the value of 
 so that it 
not  only  ensures  high  classification  accuracy  but  also  low 
programming energy. Once the DNN is trained, the  learned 
DW  device’s  weights  (or  conductances)  remains  the  same 
during testing, as these devices are non-volatile. 
   We  have  chosen  two  representative  noise  toleration  limits 
 . Studies have 
for our study that are 
shown  that  during  training  a  Gaussian  noise  of  standard 
deviation, 
 of the maximum magnitude of 
DNN  weights  does  not  degrade  test  accuracies  significantly 
7.5%
when no inference noise is assumed [38]. This motivates us to 
consider  a  noise  tolerance  of 
  that  is  15%  of  the 
maximum  DNN  weights  (most  of  the  weights  in  Gaussian 
distribution lies within 2
~ 15%). However, the DW device 
we studied here does have inference noise due to the device 
stochasticity.  Furthermore,  we  choose  a  maximum  noise 
tolerance  of 
  that  is  25%  of  the  maximum  DNN 
weights so that the state overlaps between two adjacent states 
can  be  restricted  for  5-state  networks  (half  of  the  interstate 
interval for 5-state is 0.25).  
   We note that for 3-level quantization (3-state device), DW 
device can be programmed with control voltages to generate 
) J/m3 that can achieve 
PMA of 
quantized  weights  of  -1,  0,  and  1  respectively.  For  2-level 
quantization (2-state device), the devices can be programmed 
) J/m3 to achieve weights of -1 and 1 
to 
respectively.  During  in-situ  training,  the  device  weights  are 
5
× 10
  distribution  of 
selected  randomly 
the 
from 
    to  program  the  DW  device  to  a  target 
corresponding 
< 𝑚𝑚𝑧𝑧 >
quantized value. Although we have computed 250 instances 
for each of the programming conditions (in Fig. 1c-d) due to 
the limitation in computational resources, we note that there 
are  dominant  pinning  sites  in  the  racetrack  because  of  the 
notches. As a result, the DWs tend to be stuck in or close to 
those pinning sites in most cases rather than the pinning sites 
offered by the rough edges of the racetrack (see supplementary 
Fig. S2). Hence, generating more instances will likely follow 
the probability distribution, which already exists in the current 
distribution. 
2) EX-SITU TRAINING 
In  this  section,  we  discuss  the  steps  of  ex-situ  training 
algorithm.  The  goal  of  ex-situ  training  is  to  achieve  high 
testing  accuracy  in  hardware  although  a  precursor  DNN  is 
trained  in  software.  For  this  training,  we  also  adopt  weight 
quantization  and  allocate  a  separate  memory  in  software 
where we store the high precision weights (similar to in-situ 
training)  in  addition  to  the  DNN  weights.  The  training 
algorithm  shown  in  Fig.  2b  remains  the  same  for  ex-situ 
training.  After  each  iteration  (forward  and  backward  pass), 
high  precision  weights  are  updated  and  then  quantized. 
Ideally,  these  quantized  weights  should  be  used  as  DNN 
weights for the next iteration in case of deterministic quantized 

𝐾𝐾𝑢𝑢
 = 8 and 7.0 (

× 10

𝐾𝐾𝑢𝑢

𝐾𝐾𝑢𝑢

5

VOLUME XX, 2020 

7 

 
𝐾𝐾𝑢𝑢

  =  7.5  (

5
× 10

  distribution  of 

neural network learning [1]. However, as we are dealing with 
a  stochastic  device  for  our  inference  engine,  we  include 
stochastic behavior of synaptic weights during learning. This 
stochasticity is obtained from  a statistical distribution of the 
device (shown in Fig. 1d) rather than from uniform random 
distribution [1] or Gaussian distribution [38, 52]. For example, 
in  5-level  quantization  if  the  quantized  weight  is  0  then  the 
DNN weight can be of any values selected randomly from the 
)  J/m3  which  is 
responsible for generating quantized weight of 0 (see Fig. 1d).  
< 𝑚𝑚𝑧𝑧 >
  is  also  used  during  ex-situ 
   The  noise  tolerance  margin 
training.  This  will  relax  the  stringent  requirement  of 
α
programming  a  stochastic  DW  to  a  predetermined  learned 
weight  value  and  potentially  save  a  large  number  of 
programming  attempts.  More  importantly,  if  the  DNN 
becomes  aware  of  the  statistical  distribution  of  the  device 
during  training  it  can  perform  well  during  inference  as  the 
same device based DNN is used for inference. 
   Once the ex-situ training is accomplished, the DNN weights 
(or the high precision weights) are quantized and transferred 
to the DW devices by suitable programming. Here, the learned 
weights and the programmed weights may not be the same due 
to the programming noise. During the programming, we allow 
 that is used during training. 
the same noise tolerance margin,
  need  to  satisfy,  
Thus,  any  programmed  device  weight, 
. 
The devices can be programmed by repeated programming or 
𝑊𝑊𝑞𝑞 − 𝛼𝛼 ≤ 𝑊𝑊𝑎𝑎𝑖𝑖 ≤ 𝑊𝑊𝑞𝑞 + 𝛼𝛼
𝑊𝑊𝑞𝑞
performing  read-verify-write  operation  in  a  loop,  which  is 
called  “Open  loop  off  device”  method  [53].  As  we  have 
already  trained  our  network  with  stochastic  distribution  of 
weights  by  introducing  finite 
,  the  network  is  expected  to 
perform  well  during  testing  when  we  allow  the  same  noise 
𝛼𝛼
tolerance level for programming the device. 

 for a target-quantized weight of 
𝑊𝑊𝑎𝑎𝑖𝑖

 𝛼𝛼

C.  TESTING THE DNN 
During the testing stage, we computed the predicted class for 
all the image samples from the MNIST test dataset using the 
trained  DNN  and  compared  it  to  the  desired  class.  The 
percentage accuracy is calculated by dividing the total number 
of  accurate  predictions  to  the  total  number  of  test  samples. 
During the testing stage, we consider two scenarios depending 
on  in-situ  or  ex-situ  training.  When  both  the  training  and 
testing  is  performed  on  simulated  hardware,  the  testing 
accuracy  we  record  is  termed  online  testing  accuracy.  In 
contrast, when the training is performed off-line (ex-situ) in 
software and we program the hardware (simulated device in 
this case) prior to testing according to the learned weights then 
the  testing  accuracy  we  record  is  termed  offline  testing 
accuracy. 

IV.  RESULTS AND DISCUSSIONS 

A.    DNN CONFIGURATION SELECTION 
The  focus  of  our  paper  was  to  demonstrate  the  ability  to 
classify  images  using  a  DW  device  based  DNN  and 
benchmark  its  performance  against  a  DNN  with  floating 

Author Name: Preparation of Papers for IEEE Access (February 2017) 

precision (32-bit) weights. The topography of the benchmark 
DNN can be arbitrary as the inference accuracy vary widely 
across  the  spectrum  of  the  parameters  such  as  hidden  layer 
number, layer size ratio (ratio of neurons between a layer and 
the  next  layer),  learning  rate  constant  (see  Fig.  S1  in 
supplementary). Thus, one can select multiple configurations 
for  the  DNN  and  achieve  good  accuracy.  We  select  a 
benchmark  DNN  architecture  consisted  of  a  network  with 
three hidden layers, an initial learning rate of 0.007 and a layer 
. We assume a learning rate decay of 10 % after 
size ratio of 
each epoch and use stochastic gradient decent method as the 
optimizer.  The  selection  criteria  are  detailed 
the 
supplementary section. After training the selected benchmark 
DNN for 10 epochs, the test accuracy we achieve is 97.1 %. 
We note that there are opportunities to improve the accuracy 
further in terms of topography, batch normalization, dropout 
and selection of different optimizers. However, the main goal 
of this study is to show how well a stochastic and low precision 
DW  based  DNN  can  perform  in  comparison  to  a  similar 
architecture floating precision DNN. The selected topography 
mentioned above is used throughout the study  to implement 
the DW device based DNN. 

in 

2

1

B.   ONLINE (IN-SITU) TRAINING 
After determining the DNN topography we investigate the test 
accuracies of the DNNs that are built from 2-state, 3-state and 
5-state DW devices and trained them with the proposed online 
training  algorithm.  For  simplicity,  we  did  not  consider 
additional  hardware  non-idealities  that  could  arise  from 
peripheral  circuits  or  unresponsive  devices  as  these  factors 
would automatically be included as constraints during the on-
line  training  [38]  and  would  not  result  in  a  significant 
degradation  in  performance  compared  to  our  current  work.  
The online training accuracy and online testing accuracy are 
plotted in Fig. 3 with the number of epochs for two different 
  (15%  of  maximum 
noise  tolerance  margins, 
possible  absolute  weight)  and 
  (25%  of  maximum 
possible absolute weight) used during the training stage.  
   The effectiveness of the proposed in-situ training algorithm 
is  evident  from  Fig.  3a  and  Fig.  3b  which  plots  the  in-situ 
training  accuracies  for  different  state  devices  for  low  (

𝛼𝛼 = 0.15

𝛼𝛼 = 0.25

(

  noise 

)  and  high 

tolerance  margin 
𝛼𝛼 =
respectively.  The  results  are  also  compared  with  baseline 
0.15
𝛼𝛼 = 0.25)
accuracy (accuracy of a same topography DNN with floating 
precision  weights  and  no  stochasticity).  The 
training 
accuracies  for  DW  device  based  DNNs  increase  with  the 
number  of  device  states  and  almost  reach  the  baseline 
accuracy of ~ 99.6 % for low noise tolerance of 
=0.15 as can 
be  seen  from  Fig.  3a.  However,  the  training  accuracies  for 
DNNs  with  high  noise  tolerance, 
=0.25  become  slightly 
lower (see fig. 3b) as these networks allow higher deviation 
from the target quantized  weights. Nonetheless, competitive 
training  accuracies  are  achieved  for  both  3-  and  5-  state 
devices with high noise tolerance margin.

𝛼𝛼

𝛼𝛼

8 

VOLUME XX, 2020 

 
 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

a high precision  weight crosses the threshold; the larger  the 
threshold the fewer the updates. Between the 2, 3 and 5 state 
networks  the  5-state  has  the  smallest  threshold,  which 
increases the number of DW device updates as seen in Fig. 4. 
These  DNNs  are  also  compared  with  a  DNN  trained  with 
floating precision  weights (and no  stochasticity). In  floating 
precision  DNN,  all  the  weights  are  updated  at  each  time  a 
training  image  is  passed  to  the  network.  Thus,  although  the 
network is better  trained  with  increasing  number of  epochs, 
the  weight  update  count  remains  almost  constant  as  seen  in 
Fig. 4. In contrast, for DNNs with limited state DW devices 
with the proposed training method, the programming instances 
decrease  significantly  with  the  number  of  epochs.  As 
expected, with low noise tolerance margin the DNNs with DW 
devices become more selective and require higher number of 
weight updates during the course of the training (though this 
is much smaller than the case of floating precision weights). 

FIGURE 4.  Comparison of the total number of programmed weights 
with the number of training epochs for different networks. A 
significantly lower number of weights are updated during the proposed 
online training compared to the full precision weight network of the 
same architecture. 

   In Fig. 5, we show the convergence of DW device weights 
during the training. DNN weights whose noise tolerance are 
higher will converge to a value quicker, on an average, than a 
weight with a lower noise tolerance. In Fig. 5a and 5b, the DW 
device weights fall within 
 of the quantized weight value. 
In  both  cases,  the  DW  device  weight  is  closer  to  the  high 
precision  value  than  the  quantized  weight,  which  tends  to 
provide a higher accuracy for our DW based DNN.

±𝛼𝛼

FIGURE 3.  Online training accuracy and online testing accuracy for 
DNNs with different state DW devices for two different noise tolerance 
margins. These accuracies are compared with a DNN trained and tested 
with full precision weights and no stochasticity (baseline accuracy) a. 
and b. show the online training accuracies with the numbers of epochs 
for  
 = 0.15 and 0.25 respectively. c. and d. show online testing 
accuracies with numbers of epochs for 

 = 0.15 and 0.25 respectively. 

𝜶𝜶

𝜶𝜶

𝛼𝛼

   After each epoch of the in-situ training we test the DNN with 
test images form MNIST dataset and compute the online test 
accuracy. Fig 3c and 3d plots online testing accuracies for low 
and  high  level  of  noise  tolerance  margin  respectively.  The 
baseline  (DNN  with  floating  precision  weights  and  no 
stochasticity) test accuracies are plotted for comparison. For 
low  noise  tolerance  margin  of 
=0.15,  the  test  accuracy  is 
highest  for  5-state  device  and  reaches  ~  96.67%  after  10 
epochs of training. This accuracy is very close to the baseline 
test accuracy of ~ 97.1 %. It is important to note that, the 3-
state device based DNN achieves a test accuracy of ~ 96.6 % 
after 10 epochs of training, which is similar to a 5-state device. 
=0.25, the 
When the noise tolerance margin is increased to 
test accuracies for 5-state and 3-state devices are ~ 96.56% and 
~  96.36%  after  10  epochs  of  training.  Thus,  a  maximum 
decrease of accuracy of ~ 0.74 % from 32-bit precision weight 
is recorded for a 3-state stochastic weight. We note that, the 
test accuracies for 2-state device are ~ 95.14% and ~94.64% 
for low and  high  noise tolerance  margin respectively. Thus, 
the  same  topography  networks  for  2-state  does  not  achieve 
comparable test accuracies. Changing the topography such as 
increasing the number of neurons in hidden layers can increase 
the accuracy of binary DNN [54]. 
   Next, we analyze the total number of programming pulses 
that are applied to the DW devices during the course of the 
online training at various epochs. Because the network updates 
the high-precision weights, a single weight may have its high 
precision  value  updated  many  times  before  crossing  the 
threshold  to  update  the  DW  device  weight.  Because  the 
number of device updates is dependent on the number of times 

𝛼𝛼

VOLUME XX, 2020 

9 

 
 
 
 
 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

accuracy is presented side by side with green bar. When the 
exact  learned  weights  (no  programming  noise  is  considered 
while transferring the learned weights to the device) are used 
to test the DNNs we call it software accuracy.  
   When  offline  training  is  performed  with  both  the  floating 
precision and quantized weights cases, the testing accuracy is 
low,  as  can  be  seen  from  Fig.  6a.  After  floating  precision 
weight training, the learned weights need to be converted to 3- 
or 5-state weights to program the DW devices. Thus, for both 
of  the  3-  and  5-state  hardware  the  test  accuracies  degrade 
compared  to  software  accuracy  of  ~  97.1%.    Converting 
floating  precision  learned  weights  to  5-state  compatible 
weights  (5-level  quantization)  generates  smaller  deviations 
compared to the 3-state weight (3-level quantization). Thus the 
5-state device provides higher test accuracy which is ~ 87% 
compared to the 3-state which is only ~10%. 
Training with quantized weights improves the test accuracies 
to  ~  90%  for  5-state  device  (see  Fig.  6a)  as  the  network 
becomes aware about the limited states of the weights during 
the training period. However, the test accuracy remains low 
(software accuracy is ~ 96.74%). The accuracy loss is mainly 
due  to  the  deviation  of  the  programmed  weights  from  the 
learned  weights.  We  note  that,  with  floating  precision 
training, weight deviations occur in two ways: converting the 
floating precision  weights to quantized  weights and during 
the  programming  of  the  device  where  the  target  quantized 
weights  are  not  achieved  deterministically.  However,  with 
quantized training only the latter deviation occurs during the 
testing stage.  

FIGURE 5.  Weight evolution of high precision weight, quantized weight 
and the DW device weight during the first few training images for two 
different noise tolerance margin a. 
synaptic weight shown here is connected between the two neurons 
located in hidden layer 2 and 3. 

 𝜶𝜶 = 𝟏𝟏. 𝟏𝟏𝟑𝟑

𝜶𝜶 = 𝟏𝟏. 𝟐𝟐𝟑𝟑

 0.15 b. 

. The 

C.   OFFLINE (EX-SITU) TRAINING 
In  this  section,  we  first  analyze  the  effectiveness  of  our 
proposed  ex-situ  training  by  comparing  it  with  other 
techniques.  For  that,  we  train  several  precursor  DNNs  in 
software  using  different  offline  training  algorithms  (Fig.  6) 
and  then test  the DNNs,  which are built  from  DW  synaptic 
devices (3- state and 5-state hardware). Each of the DNNs are 
trained  offline  with  a  total  of  10  epochs  (train  with  entire 
training  dataset  10  times)  and  prior  to  the  testing  the  DW 
devices  are  programmed  according  to  the  weights  that  are 
learned offline. These results are shown in Fig. 6a and 6b when 
we consider a low (
) value of 
noise tolerance margin to program the devices. In both Fig. 6a 
and 6b, for hardware test accuracies, a corresponding software 

) and high (

𝛼𝛼 = 0.15

𝛼𝛼 = 0.25

FIGURE 6.  Testing accuracy comparison of 3-state and 5-state DW device based DNNs for different ex-situ training algorithms for a programming 
noise toleration window of a. 
. The networks are trained off-line with floating precision weights, quantized weights, and stochastic 
quantized weights derived from micromagnetic simulation. Each of the network is trained with a total 10 epochs. Once training is done the, 3-state and 
5-state DW devices are programmed based on the quantized value of trained weights prior to testing. For different training algorithms and for each of 
the test accuracy of DNN built from 3- and 5-state hardware, a corresponding software test accuracy (no programming noise is considered and exact 
trained weights are used for testing the DNN) is plotted side by side with green bar. Error bar seen in the figure is calculated for a total of 10 test trials. 
For both noise tolerance windows, the test accuracy is highest when trained with proposed training algorithm (quantized + stochastic).

𝜶𝜶 = 𝟏𝟏. 𝟐𝟐𝟑𝟑

𝜶𝜶 = 𝟏𝟏. 𝟏𝟏𝟑𝟑

 b. 

10 

VOLUME XX, 2020 

 
 
 
 
 
 
 
In  contrast,  with  our  proposed  training  which  we  call 
quantized + stochastic training, the test accuracy increases and 
reaches up to ~ 96.63% for 5-state device, which is close to 
the 
software  accuracy  of  ~96.67%.  The  accuracy 
improvement can be attributed to even smaller deviation of the 
programmed  weights  from  the  learned  weights.  Unlike 
quantized  training,  in  our  proposed  training  the  weight 
quantization is also accompanied by training the DNN weights 
according  to  the  statistical  distribution  of  the  device.  As  a 
result, during back propagation, the high precision weights are 
updated depending on the weighted sum performed over the 
imprecise  DNN  weights  (which  are  mapped  from  the 
stochastic distribution of the device as in Fig. 1d).   In other 
words, the high precision weights are being tuned based on the 
stochastic  signature  of  the  device.  Thus,  the  statistical 
distribution of the device is embedded in the learning. When 
the same devices are used for testing, the distribution matches 
better and this plays an important role for improving the test 
accuracy. This finding is also supported by other works [38, 
52].  Ref.  [38]  shows  that  the  DNN  trained  with  Gaussian 
distributed  weights  of  a  certain  standard  deviation  performs 
better when weights of same standard deviation are used for 
inference. 
  With  high  programming  noise,  for  both  floating  precision 
and quantized training, the programmed weights deviate more 
from the learned weights because of the higher noise tolerance. 
Thus, the test accuracies for 3- or 5-state hardwires degrade 
significantly compare to the software-based accuracies as seen 
from Fig. 6b.  In contrast, with our proposed training method, 
the DNNs are made aware about the statistical distribution of 
the device thus resulting in significantly higher test accuracy 
compare than other offline training methods. (Note that as the 
device  statistics  are  not  Gaussian  and  instead  heavily 
dominated  by  the  pinning  positions,  training  with  Gaussian 
distributed  weights  does  not  improve  accuracy  and  was  not 
employed).  

We also studied the evolution of offline test accuracies with 
the  number  of  epochs  for  different  state  devices,  which  are 
presented  in  supplemental  Fig.  S3.  The  influence  of  noise 
  on  training  accuracy,  online  testing 
tolerance  margin 
accuracy and offline  testing accuracy  for DNN  with  limited 
state  device  (5-state)  is  shown  in  supplementary  Fig.  S4, 
which shows that off-line testing accuracy is affected most by 
the choice of different

𝛼𝛼

. 

 𝛼𝛼

D.   ENERGY DISSIPATION  
Energy  dissipation  to  program  a  DW-synapse  depends  on 
charging the piezoelectric layer by applying a voltage pulse, 
 loss due to the SOT current in the heavy 

 as well as 

2

2

𝑅𝑅

  J/

1
metal  layer.  The  maximum  change  in  PMA  is 
2 𝐶𝐶𝑉𝑉

𝐼𝐼
.  For  magnetic  racetrack  of  CoFe  the 
∆𝑃𝑃𝑀𝑀𝐴𝐴 =
=250  ppm.  Using  the  above 
saturation  magnetization  is, 
𝑚𝑚
0.5 × 10
values,  the  required  maximum  stress,  σ  is  calculated  to  be,  
𝜆𝜆𝑠𝑠
=133  MPa.    For  CoFe  with  Young’s  Modulus  of  200 

5

3

∆𝑃𝑃𝐷𝐷𝑃𝑃
3
GPa,  the  required  strain  is,   
2𝜆𝜆𝑠𝑠

  ~

.  Previous  study 

VOLUME XX, 2020 

133 𝐷𝐷𝑃𝑃𝑎𝑎

200 𝐺𝐺𝑃𝑃𝑎𝑎

−3

10

Author Name: Preparation of Papers for IEEE Access (February 2017) 

𝜖𝜖𝑠𝑠
𝜖𝜖0𝜖𝜖𝑟𝑟(𝐿𝐿∗𝑏𝑏)
𝑏𝑏

−3

10

[46]  showed  that 
  strain  is  possible  in  Lead  Zirconate 
Titanate (PZT) piezoelectric with an applied electric field of 
E=3  MV/m  when  the  electrode  dimensions  are  in  the  same 
order  of  the  PZT  thickness.  If  we  consider  PZT  layer  to  be 
thick  (same  as  top  electrode  or  racetrack  width  as 
b=60  nm
shown in Fig. 1(b)) then a voltage of, E*b = 0.18 V applied 
between the top electrode pair and the bottom electrode can 
generate the required strain. If the top electrode length L=600 
nm (same as racetrack length 600 nm) and width b=60 nm is 
=3000 then 
considered, and relative permittivity of PZT is 
 ~ 16 fF. 
the effective capacitance is calculated to be 

2

2

3

10

𝛼𝛼

𝛺𝛺

×

×

60

𝑛𝑛𝑚𝑚 

⁄
𝐴𝐴 𝑚𝑚

loss of ~0.5 fJ considering two top 

This suggests a  
electrodes on both sides of the racetrack.  
1
2� 𝐶𝐶𝑉𝑉
   The  heavy  metal  layer  is  considered  to  be  Pt  and  for 
  dimension  Pt  layer  the  resistance  is 
600
5 
 assuming the resistivity of Pt to be 100 
calculated to be 200 
nm. The heat loss in the heavy metal layer is calculated to be 
2.2 fJ for a fixed SOT generating current pulse of magnitude 
𝛺𝛺
  applied  for  1  ns.  Thus,  the  total  energy 
dissipation to program a synapse is calculated to be 2.7 fJ.  
35 × 10
1) IN-SITU TRAINING 
With in-situ training,  highest inference accuracy is achieved 
for  a  5-state  device  when  a  low  noise  margin  is  considered 
during  the  training.  However,  with  higher  noise  tolerance 
margin  similar  test  accuracy  is  obtained  with  fewer  device 
updates as can be seen from Fig. 4. For 5-state device, if we 
consider a noise tolerance margin of 
=0.25, the total number 
of  weight  updates  are  calculated  to  be  ~  48  million  after 
running  the  training  for  10  epochs.  Thus,  the  energy 
dissipation to program the DNN synapses is calculated to be 
~13pJ for one inference event followed by the weight updates 
(10000 test images in MNIST).  
2) EX-SITU TRAINING 
With ex-situ training, highest inference accuracy is achieved 
for 5-state device when the noise margin to program the DW 
devices is considered to be low. Fig. 7 shows the cumulative 
probability  of 
the  DW  device  weights  for  different 
programming condition for a 5-state device. The solid black 
line represents the target quantized weights of 1, 0.5, 0, -0.5 
and  -1  (in  this  case  -0.833)  which  can  be  achieved  by  a 
combination  of  fixed  SOT  current  pulse  and  a  varying 
amplitude voltage pulse which modulates the anisotropy of the 
)  J/m3 
  =  7,  7.25,  7.5,  7.75  and  8.0  (
racetrack  to 
respectively. The adjacent red dotted lines in the figure shows 
5
× 10
the  noise  margin  (
is  allowed  while 
programming  the  DW  device  to  a  specific  quantized  state. 
From Fig. 7 it can be seen that the probability of programming 
the DW device weight to a quantized value of 1 is the lowest 
which is ~ 6 % meaning a number of ~ 20 attempt is required 
to program the device. If we consider the worst-case scenario, 
then  after ex-situ  training prior to the inference  we need 20 
programing  pulses  to  program  each  of  the  DW  devices 
implementing  the  DNN  weights.  Thus,  for  our  network 
topology of 784-392-196-98-10 neurons, the energy

𝛼𝛼 = 0.15

that 

𝐾𝐾𝑢𝑢

) 

11 

 
 
 
 
×

 dissipation to program the DW synapse is 2.8 pJ per inference 
event.  
   The energy dissipation to program the DW devices in in-situ 
training is found to be 5
 the dissipation incurred in ex-situ 
training, which is a moderately low provided that the training 
is  performed  over  the  entire  60000  training  images  for  10 
times. This low dissipation in-situ training is possible due to 
distinct features of proposed training algorithm that benefits 
from weight quantization and noise tolerance margin. Large 
interstate  interval  in  quantized  learning  helps  to  reduce  the 
number  of  weight  updates.  Moreover,  once  the  device  is 
programmed within the noise tolerance margin, further write 
operation is avoided with a simple low cost read operation. We 
note that onsite learning is attractive in power constraint edge 
devices, where the learning itself needs to adapt and respond 
to a continuously evolving environment. Embedded medical 
systems  [55],  real  time  intrusion  detection  [56],  and  dialect 
specific  speech  recognition  systems  can  be  benefitted  from 
such onsite learning.  Ex-site learning  can perform inference 
tasks in edge devices with energy efficient manner (given the 
training is performed over cloud server), however the benefit 
can only apply to non-adaptive tasks.   

Author Name: Preparation of Papers for IEEE Access (February 2017) 

algorithms are presented for DNNs that are implemented with 
2-state, 3-state and 5-state DW devices. For in-situ training, a 
high  precision  memory  unit  is  employed  to  preserve  and 
accumulate  the  weight  gradients,  which  are  quantized  to 
obtain target conductance for updating the low precision DW 
devices. A noise tolerance margin further allows for random 
deviations  of  the  programmed  conductances  from  the  target 
conductance values. For ex-situ training, a precursor DNN is 
first  trained  in  software  by  performing  weight  quantization 
and considering a noise tolerance margin from the quantized 
weight  and  later  tested  with  an  equivalent  DNN  of  DW 
devices programmed with the same noise margin. While the 
energy  dissipation  statistics  for  programming  the  DNN 
synapses shows that ex-situ method is energy efficient, the in-
situ  training  provides  an  opportunity  to  learn  and  adapt  to 
changing environment with only 5
 more dissipation (despite 
the  fact  that  the  in-situ  training  is  performed  over  a  vast 
number of training images for many epochs). This technology 
is specifically attractive for low power intelligent edge devices 
of future IoT where energy requirement is at a premium.  

×

ACKNOWLEDGMENT 

REFERENCES 

[1] 

I.  Hubara,  M.  Courbariaux,  D.  Soudry,  R.  El-Yaniv,  and  Y. 
Bengio,  “Quantized  Neural  Networks:  Training  Neural 
Networks  with  Low  Precision  Weights  and  Activations",  The 
Journal of Machine Learning Research vol. 18, no. 187, pp. 1-
30, Apr. 2017. 

[2]  H.-S. Philip Wong, and S. Salahuddin, “Memory leads the way 
to better computing, Nature Nanotechnology”, vol 10, pp. 191-
194, Mar. 2015.doi: https://doi.org/10.1038/nnano.2015.29  
[3]  A.  Pedram,  S.  Richardson,  M.  Horowitz,  S.  Galal,  and  S. 
Kvatinsky,  “Dark  Memory  and  Accelerator-Rich  System 
Optimization in the Dark Silicon Era”, IEEE Design & Test, vol. 
DOI: 
39-50, 
2, 
34, 
10.1109/MDAT.2016.2573586 

2017. 

Apr. 

pp. 

no. 

 Cumulative probability of normalized DW device weight for 

FIGURE 7. 
5-state device under different programming conditions denoted by 
different 
and the adjacent dotted red lines represent the programming noise 
𝑲𝑲𝒖𝒖
tolerance margin of  

. Black solid line represents the target quantized weights 

.  

𝜶𝜶 = 𝟏𝟏. 𝟏𝟏𝟑𝟑

   Finally,  when  paired  with  purely  CMOS  based  device, 
spintronic devices can provide the non-volatility with higher 
area efficiency and less static power dissipation.  Non-volatile 
PCM  and  RRAM  technology  can  provide  smaller  footprint 
device,  however  the  energy  dissipation  to  program  such  a 
synaptic device is in the ~pJ range [23, 54]. Moreover, these 
devices have low endurance compare to spintronic devices [2].  

V. CONCLUSION 
We have shown that DNNs with extremely low resolution and 
stochastic  DW  device-based  synapses  can  achieve  high 
classification accuracy when trained with appropriate learning 
algorithms.  In  this  study,  both  in-situ  and  ex-situ  training 

[4]  A. Sebastian, M. L. Gallo, R. K.- Aljameh, and E. Eleftheriou, 
“Memory devices and applications for in-memory computing”, 
Nature Nanotechnology, vol. 15, pp. 529–544, Mar. 2020, DOI: 
https://doi.org/10.1038/s41565-020-0655-z 

[5]  D. Ielmini, and H.-S. Philip Wong, “In-memory computing with 
resistive switching devices”, Nature Electronics, vol. 1, pp. 333–
343, Jun. 2018, DOI: https://doi.org/10.1038/s41928-018-0092-
2 

[6]  M. Hu, J. P. Strachan, Z. Li, E. M. Grafals, N. Davila, C. Graves, 
S.  Lam,  N.  Ge,  J.  Yang,  and  R.  S.  Williams,  “Dot-Product 
Engine  for  Neuromorphic  Computing:  Programming  1T1M 
Crossbar to Accelerate Matrix-Vector Multiplication”, in 53nd 
ACM/EDAC/IEEE  Design  Automation  Conference  (DAC), 
Austin, TX, USA, Jun. 2016. 

[7]  C. Li, M. Hu, Y. Li, H. Jiang, N. Ge, E. Montgomery, J. Zhang, 
W. Song, N. Dávila, C. E. Graves, Z. Li, J. P. Strachan, P. Lin, 
Z. Wang, M. Barnell, Q. Wu, R. S. Williams, J. J. Yang, and Q. 
Xia,  “Analogue  signal  and  image  processing  with  large 
memristor  crossbars”,  Nature  Electronics,  vol.  1,  pp.  52–59, 
Dec. 2017, DOI: https://doi.org/10.1038/s41928-017-0002-z 
[8]  G.  W.  Burr,  R.  M.  Shelby,  A.  Sebastian,  S.  Kim,  S.  Kim,  S. 
Sidler, K. Virwani, M. Ishii, P. Narayanan, A. Fumarola, L. L. 
Sanches, I. Boybat, M. L. Gallo, K. Moon, J. Woo, H. Hwang 

12 

 VOLUME XX, 2020 

 
 
 
 
 
 
[9]  and Y. Leblebici, “Neuromorphic computing using non-volatile 
memory”, Advances in Physics: X, vol. 2, no.1, pp. 89-124, Dec. 
2016, DOI: https://doi.org/10.1080/23746149.2016.1259585 
[10]  G.  W.  Burr,  R.  M.  Shelby,  S.  Sidler,  C.  di  Nolfo,  J.  Jang,  I. 
Boybat,  R.  S.  Shenoy,  P.  Narayanan,  K.  Virwani,  E.  U. 
Giacometti,  B.  N.  Kurdi,  and  H.  Hwang,  “Experimental 
Demonstration  and  Tolerancing  of  a  Large-Scale  Neural 
Network (165 000 Synapses) Using Phase-Change Memory as 
the Synaptic Weight Element”, IEEE Trans. Electron Devices, 
vol.  62,  no.  11,  pp.  3498  -  3507,  Nov.  2015.  DOI: 
10.1109/TED.2015.2439635  

[11]  M. Prezioso, F. M.- Bayat, B. D. Hoskins, G. C. Adam, K. K. 
Likharev,  and  D.  B.  Strukov,  “Training  and  operation  of  an 
integrated  neuromorphic  network  based  on  metal-oxide 
memristors”,  Nature,  vol.  521,  pp.  61–64,  May  2015,  DOI: 
https://doi.org/10.1038/nature14441 

[12]  M.  Suri,  O.  Bichler,  D.  Querlioz,  O.  Cueto,  L.  Perniola,  V. 
Sousa, D. Vuillaume, C. Gamrat, and B. DeSalvo, “Phase change 
memory  as  synapse  for  ultra-dense  neuromorphic  systems: 
Application  to  complex  visual  pattern  extraction”,  in  2011 
International  Electron  Devices  Meeting,  pp.  4.4.1-4.4.4,  Dec. 
2011. DOI: 10.1109/IEDM.2011.6131488 

[13]  T. H. Lee, D. Loke, K.-J. Huang, W.-J. Wang, and S. R. Elliott, 
“Tailoring  Transient-Amorphous  States:  Towards  Fast  and 
Power-Efficient  Phase-Change  Memory  and  Neuromorphic 
Computing”, Adv. Mater., vol. 26, no. 44, pp. 7493-749, Nov. 
2014, DOI: https://doi.org/10.1002/adma.201402696 

[14]   S. Yu, Y. Wu, R. Jeyasingh, D. Kuzum, and H.-S. P. Wong, “An 
Electronic  Synapse  Device  Based  on  Metal  Oxide  Resistive 
Switching  Memory  for  Neuromorphic  Computation”,  IEEE 
Trans.  Electron  Devices,  vol.  58,  no.8,  pp.  2729  -  2737,  Aug. 
2011. DOI: 10.1109/TED.2011.2147791   

[15]  J.  Woo,  K.  Moon,  J.  Song,  S.  Lee,  M.  Kwak,  J.  Park,  and  H. 
Hwang,  “Improved  Synaptic  Behavior  Under  Identical  Pulses 
Using  AlOx  /HfO2  Bilayer  RRAM  Array  for  Neuromorphic 
Systems”, IEEE Electron Device Letters, vol. 37, no. 8, pp. 994 
- 997, Aug. 2016, DOI: 10.1109/LED.2016.2582859 

[16]  C.  Li,  D.  Belkin,  Y.  Li,  P.  Yan,  M.  Hu,  N.  Ge,  H.  Jiang,  E. 
Montgomery,  P.  Lin,  Z.  Wang,  W.  Song,  J.  P.  Strachan,  M. 
Barnell, Q. Wu, R. S. Williams, J. J. Yang, and Q. Xia, “Efficient 
and self-adaptive in-situ learning in multilayer memristor neural 
networks”, Nature Communications, vol. 9, pp. 1-8, Jun. 2018, 
Art. no. 2385, DOI: https://doi.org/10.1038/s41467-018-04484-
2 

[17]  P. Yao, H. Wu, B. Gao, S. B. Eryilmaz, X. Huang, W. Zhang, Q. 
Zhang,  N.  Deng,  L.  Shi,  H.-S.  P.  Wong,  and  H.  Qian,  “Face 
Nature 
electronic 
classification 
Communications,  vol.  8,  pp.  1-8,  May  2017,  Art.  no.  15199. 
DOI: 10.1038/ncomms15199 

synapses”, 

using 

[18]  D. Bhowmik, U. Saxena, A. Dankar, A. Verma, D. Kaushik, S. 
Chatterjee,  and  U.  Singh,  “On-chip  learning  for  domain  wall 
synapse  based  Fully  Connected  Neural  Network”,  Journal  of 
Magnetism  and  Magnetic  Materials,  vol.  498,  pp.1-11,  Nov. 
2019, 
DOI: 
https://doi.org/10.1016/j.jmmm.2019.165434. 

1654342, 

Art. 

no. 

[19]  A.  Sengupta,  Y.  Shim, and  K.  Roy,  “Proposal  for  an  All-Spin 
Artificial  Neural  Network:  Emulating  Neural  and  Synaptic 
Functionalities Through Domain Wall Motion in Ferromagnets”, 
IEEE Transactions on Biomedical Circuits and Systems, vol. 10, 
no. 
2016,  DOI: 
10.1109/TBCAS.2016.2525823  

-1160,  Dec. 

1152 

pp. 

6, 

[20]  D.  Zhang,  Y.  Hou,  L.  Zeng,  and  W.  Zhao,  “Hardware 
Acceleration Implementation of Sparse Coding Algorithm With 
Spintronic  Devices”,  IEEE  Transactions  on  Nanotechnology, 
vol. 
2019,  DOI: 
pp. 
10.1109/TNANO.2019.2916149 

531,  May 

518 

10, 

- 

Author Name: Preparation of Papers for IEEE Access (February 2017) 

[21]  A.  F.  Vincent,  J.  Larroque,  N.  Locatelli,  N.  B.  Romdhane,  O. 
Bichler,  C.  Gamrat,  W.  S.  Zhao,  J.-O.  Klein,  S.  G.-Retailleau, 
and D. Querlioz, “Spin-Transfer Torque Magnetic Memory as a 
Stochastic  Memristive  Synapse  for  Neuromorphic  Systems”, 
IEEE Transactions on Biomedical Circuits and Systems, vol. 9, 
2015,  DOI: 
no. 
10.1109/TBCAS.2015.2414423  

174,  Apr. 

166 

pp. 

2, 

- 

[22]  M.  Alamdar,  T.  Leonard,  C.  Cui,  B.  P.  Rimal,  L.  Xue,  O.  G. 
Akinola,  T.  P.  Xiao,  J.  S.  Friedman,  C.  H.  Bennett,  M.  J. 
Marinella, and J. A. C. Incorvia, “Domain wall-magnetic tunnel 
junction  spin–orbit  torque  devices  and  circuits  for  in-memory 
computing”, Appl. Phys. Lett., vol. 118, pp. 1-6, Mar. 2021, Art. 
no. 112401, DOI: https://doi.org/10.1063/5.0038521 

[23]  M.-C. Chen, A. Sengupta, and K. Roy, “Magnetic Skyrmion as 
a  Spintronic  Deep  Learning  Spiking Neuron  Processor”,  IEEE 
Transactions on Magnetics, vol. 54, no. 8, pp. 1-7, Aug. 2018, 
Art. no.1500207, DOI: 10.1109/TMAG.2018.2845890 

[24]  D. Kaushik, U. Singh, U. Sahu, I. Sreedevi, and D. Bhowmik, 
“Comparing  domain  wall  synapse  with  other  non  volatile 
memory devices for on chip learning in analog hardware neural 
network”, AIP Advances, vol. 10, no. 2, pp. 1-7, Feb. 2020, Art. 
no. 025111. DOI: https://doi.org/10.1063/1.5128344 

[25]  V.  Uhlíř,  S.  Pizzini,  N.  Rougemaille,  J.  Novotný,  V.  Cros,  E. 
Jiménez, G. Faini, L. Heyne, F. Sirotti, C. Tieg, A. Bendounan, 
F. Maccherozzi, R. Belkhou, J. Grollier, A. Anane, and J. Vogel, 
“Current-induced motion and pinning of domain walls in spin-
valve nanowires studied by XMCD-PEEM”, Phys. Rev. B, vol. 
81,  no.  22,  pp.  1-10,  Jun.  2010,  Art.  no.  224418.  DOI: 
https://doi.org/10.1103/PhysRevB.81.224418 

[26]  X. Jiang, L. Thomas, R. Moriya, M. Hayashi, B. Bergman, C. 
Rettner, and S. S.P. Parkin, “Enhanced stochasticity of domain 
wall  motion  in  magnetic  racetracks  due  to  dynamic  pinning”, 
Nature Communications, vol. 1, pp. 1-5, Jun. 2010, Art. no: 25. 
DOI: 10.1038/ncomms1024 

[27]  J.  P.  Attan´e,  D.  Ravelosona,  A.  Marty,  Y.  Samson,  and  C. 
Chappert, “Thermally Activated Depinning of a Narrow Domain 
Wall from a Single Defect”, Phys. Rev. Lett., vol. 96, no. 14, pp. 
1-4, 
DOI: 
Art. 
https://doi.org/10.1103/PhysRevLett.96.147204. 

147204. 

2006, 

Apr. 

no. 

[28]  W.  A.  Misba,  T.  Kaisar,  D.  Bhattacharya,  J.  Atulasimha, 
“Voltage-Controlled  Energy-Efficient  Domain  Wall  Synapses 
With  Stochastic  Distribution  of  Quantized  Weights  in  the 
Presence  of  Thermal  Noise  and  Edge  Roughness”,  IEEE 
Transactions on Electron Devices (Early Access), pp. 1-9, Sep. 
2021, doi: 10.1109/TED.2021.3111846 

[29]  S. Ikeda, J. Hayakawa, Y. Ashizawa, Y. M. Lee, K. Miura, H. 
Hasegawa,  M.  Tsunoda,  F.  Matsukura,  and  H.  Ohno,  “Tunnel 
magnetoresistance  of  604%  at  300K  by  suppression  of  Ta 
diffusion in CoFeB∕MgO∕CoFeB pseudo-spin-valves annealed at 
high temperature”, Appl. Phys. Lett. vol. 93, pp.1-3, Aug. 2008, 
Art. no. 082508, DOI: https://doi.org/10.1063/1.2976435 
[30]  M.  Courbariaux,  Y.  Bengio,  and  J.-P  David,  “Binaryconnect: 
Training  deep  neural  networks  with  binary  weights  during 
propagations”, 
the  28th  International 
Conference  on  Neural 
Information  Processing  Systems, 
Montreal, BC, Canada, Dec. 2015, vol.2, pp. 3123–3131. 
[31]  H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, C. Zhang, “ZipML: 
Training Linear Models with End-to-End Low Precision, and a 
Little  Bit  of  Deep  Learning”,  in  Proceedings  of  the  34th 
International Conference on Machine Learning, Sydney, NSW, 
Australia, Aug. 2017, Vol. 70, pp. 4035–4043. 

in  Proceedings  of 

[32]  S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-
net: Training low bitwidth convolutional neural networks with 
low bitwidth gradients”. arXiv:1606.06160, Feb. 2018. 

[33]  D.  Miyashita,  E.  H.  Lee,  B.  Murmann,  “Convolutional  Neural 
Logarithmic  Data  Representation”, 

Networks 
using 
arXiv:1603.01025, Mar. 2016.

VOLUME XX, 2020 

 13 

 
[34]  S. Agarwal, R. B. J. Gedrim, A. H. Hsia, D. R. Hughart, E. J. 
Fuller, A. A. Talin, C. D. James, S. J. Plimpton, M. J. Marinella, 
“Achieving 
in  Analog  Neuromorphic 
Computing Using Periodic Carry”, in 2017 Symposium on VLSI 
Technology, Kyoto, Japan, Jun. 2017. 

Ideal  Accuracies 

[35]  I.  Boybat,  M.  L.  Gallo,  S.  R.  Nandakumar,  T.  Moraitis,  T. 
Parnell, T. Tuma, B. Rajendran, Y. Leblebici, A. Sebastian, and 
E.  Eleftheriou,  “Neuromorphic  computing  with  multi-
memristive synapse”, Nature Communications, vol. 9, pp. 1-12, 
Jun. 2018, Art. no. 2514. DOI: 10.1038/s41467-018-04933-y 

[36]  S. Ambrogio, P. Narayanan, H. Tsai, R. M. Shelby, I. Boybat, C. 
di Nolfo, S. Sidler, M. Giordano, M. Bodini, N. C. P. Farinha, B. 
Killeen,  C.  Cheng,  Y.  Jaoudi,  and  G.  W.  Burr,  “Equivalent-
accuracy  accelerated  neural-network  training  using  analogue 
memory”,  Nature,  vol.  558,  pp.  60–67,  Jun.  2018.  DOI: 
https://doi.org/10.1038/s41586-018-0180-5 

[37]  S.  R.  Nandakumar,  M.  L.  Gallo,  C.  Piveteau,  V.  Joshi,  G. 
Mariani, I. Boybat, G. Karunaratne, R. K- Aljameh, U. Egger, A. 
Petropoulos,  T.  Antonakopoulos,  B.  Rajendran,  A.  Sebastian, 
and E. Eleftheriou, “Mixed-Precision Deep Learning Based on 
Computational Memory”, Frontiers in Neuroscience, vol. 14, pp. 
1-17, May. 2020, Art. no. 406, DOI:10.3389/fnins.2020.00406 

[38]  M. L. Gallo, A. Sebastian, R. Mathis, M. Manica, H. Giefers, T. 
Tuma,  C.  Bekas,  A.  Curioni,  and  E.  Eleftheriou,  “Mixed-
precision in-memory computing”, Nature Electronics, vol. 1, pp. 
246–253, Apr. 2018, DOI: https://doi.org/10.1038/s41928-018-
0054-8 

[39]  V. Joshi, M. Le Gallo, S. Haefeli, I. Boybat, S. R. Nandakumar, 
C.  Piveteau,  M.  Dazzi,  B.  Rajendran,  A.  Sebastian,  and  E. 
Eleftheriou,  “Accurate  deep  neural  network  inference  using 
Nature 
computational 
Communications,  vol.  11,  pp. 1-13, May  2020,  Art.  no:  2473. 
DOI: https://doi.org/10.1038/s41467-020-16108-9 

phase-change 

memory”, 

[40]  G. Boquet, E. Macias, A. Morell, J. Serrano, E. Miranda, and J. 
L.  Vicario,  “Offline  Training  for  Memristor-based  Neural 
Networks”,  in  28th  European  Signal  Processing  Conference 
(EUSIPCO),  Amsterdam,  Netherlands,  Jan.  2021,  DOI: 
10.23919/Eusipco47968.2020.9287574 

[41]   L.  Chen,  J.  Li,  Y.  Chen,  Q.  Deng,  J.  Shen,  X.  Liang,  and  L. 
Jiang, “Accelerator-friendly Neural-network Training: Learning 
Variations  and  Defects  in  RRAM  Crossbar”,  in  Design, 
Automation  &  Test  in  Europe  Conference  &  Exhibition, 
Lausanne, 
DOI: 
10.23919/DATE.2017.7926952  

Switzerland, 

2017, 

Mar. 

[42]  B. Liu, H. Li, Y. Chen, X. Li, Q. Wu, and T. Huang, “Vortex: 
in  52nd 
Variation-aware 
ACM/EDAC/IEEE Design Automation Conference (DAC), San 
Francisco, 
DOI: 
10.1145/2744769.2744930. 

training  for  memristor  X-bar”, 

2015, 

USA, 

Jun. 

CA, 

[43]  P. Yao, H. Wu, B. Gao, J. Tang, Q. Zhang, W. Zhang, J. J. Yang, 
and  H.  Qian,  “Fully  hardware-implemented  memristor 
convolutional  neural  network”,  Nature,  vol.  577,  pp.  641-646, 
Jan. 2020, DOI: https://doi.org/10.1038/s41586-020-1942-4 
[44]  E. Martinez, L.  L.- Diaz,  L. Torres, C. Tristan, and O. Alejos, 
“Thermal  effects  in  domain  wall  motion:  Micromagnetic 
simulations and analytical mode”, Phys. Rev. B, vol. 75, pp. 1-
11,  May. 
DOI: 
Art. 
https://doi.org/10.1103/PhysRevB.75.174409 

174409, 

2007, 

no. 

[45]   S. Dutta, S. A. Siddiqui, J. A. C.- Incorvia, C. A. Ross, and M. 
A.  Baldo,  Micromagnetic  modeling  of  domain  wall  motion  in 
sub-100-nm-wide  wires  with  individual  and  periodic  edge 
defects, AIP ADVANCES, vol. 5, pp. 1-9, Aug. 2015, Art. no. 
127206, DOI: https://doi.org/10.1063/1.4937557 

[46]   A. Vansteenkiste, J. Leliaert, M. Dvornik, M. Helsen, F. Garcia-
Sanchez, and B. V. Waeyenberge, “The design and verification 

Author Name: Preparation of Papers for IEEE Access (February 2017) 

of MuMax3”, AIP Advances, vol. 4, no. 10, pp. 1-22, Oct. 2014, 
Art. no. 107133. DOI: https://doi.org/10.1063/1.4899186 
[47]  J. Cui, J. L. Hockel, P. K. Nordeen, D. M. Pisani, C.-Y. Liang, 
G. P. Carman, and C. S. Lynch, “A method to control magnetism 
in  individual  strain-mediated  magnetoelectric  islands”,  Appl. 
Phys. Lett., vol. 103, no. 23, pp. 1-5, Dec. 2013, Art. no. 232905. 
DOI: https://doi.org/10.1063/1.4838216 

[48]  G.  Bertotti,  I.  D.  Mayergoyz,  and  C.  Serpico,  “Stochastic 
magnetization  dynamics,” 
in  Nonlinear  Magnetization 
Dynamics  in  Nanosystems,  Amsterdam,  The  Netherlands: 
Elsevier, 2009, ch. 10, pp-271-345. 

[49]  Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based 
learning  applied  to document  recognition”,  Proceedings  of  the 
IEEE,  vol.  86,  no.11,  pp-2278  -  2324,  Nov.  1998,  DOI: 
10.1109/5.726791 

[50]  Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning”, Nature, 
DOI: 

436–444,  May 

2015, 

521, 

vol. 
pp. 
https://doi.org/10.1038/nature14539 

[51]  D.  E. Rumelhart,  G.  E.  Hinton,  and R.  J.  Williams,  “Learning 
representations  by  back-propagating  errors”,  Nature,  vol.  323, 
pp. 533–536, Oct. 1986, DOI: https://doi.org/10.1038/323533a0 
[52]  B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. 
Adam,  and  D.    Kalenichenko,  “Quantization  and  Training  of 
Neural  Networks 
Integer-Arithmetic-Only 
Inference”, arXiv:1712.05877, Dec. 2017. 

for  Efficient 

[53]  T. Hirtzlin, M. Bocquet, J.-O. Klein, E. Nowak, E. Vianello, J.-
M.  Portal,  D.  Querlioz,  “Outstanding  Bit  Error  Tolerance  of 
Resistive  RAM-Based  Binarized  Neural  Networks”, 
arXiv:1904.03652, Apr. 2019. 

[54]  B. Liu, H. Li, Y. Chen, X. Li, T. Huang, Q. Wu, and M. Barnell, 
“Reduction and IR-drop compensations techniques for reliable 
in  2014  IEEE/ACM 
neuromorphic  computing  systems”, 
International Conference on Computer-Aided Design (ICCAD), 
Jan. 2015. DOI: 10.1109/ICCAD.2014.7001330 

[55]  T. Hirtzlin, M. Bocquet, B. Penkovsky, J.-O. Klein, E. Nowak, 
E. Vianello, J.-M. Portal, and D. Querlioz, “Digital Biologically 
Plausible  Implementation  of  Binarized  Neural  Networks  With 
Differential  Hafnium  Oxide  Resistive  Memory  Arrays”, 
Frontiers in Neuroscience, Vol. 13, pp.1-14, Jan. 2020, Art. no. 
1383, doi: https://doi.org/10.3389/fnins.2019.01383 

[56]  T. Dalgaty, N. Castellani, C. Turck, K.-E. Harabi, D. Querlioz 
and  E.  Vianello,  “In  situ  learning  using  intrinsic  memristor 
variability  via  Markov  chain  Monte  Carlo  sampling”,  Nature 
Electronics,  Vol.  4 
Jan.  2021,  doi: 
https://doi.org/10.1038/s41928-020-00523-3 

,pp.  151-161, 

[57]  M. S. Alam, B. R. Fernando, Y. Jaoudi, C. Yakopcic, R. Hasan, 
T. M. Taha, G. Subramanyam, “Memristor Based Autoencoder 
for  Unsupervised  Real-Time  Network  Intrusion  and  Anomaly 
Detection”,  in  Proceedings  of  the  International  Conference  on 
Neuromorphic  Systems,  pp.  1-8,  Art.  no.  2,  Jul.  2019,  doi: 
https://doi.org/10.1145/3354265.3354267 

WALID AL MISBA received the B.Sc. degree in 
electrical  and  electronic  engineering  from  the 
Bangladesh  University  of  Engineering  and 
Technology,  Dhaka,  Bangladesh  in  2013.  He 
received  the  MS  in  electrical  engineering  from 
Tuskegee  University,  Alabama,  US.  He 
is 
currently pursuing his Ph.D. degree in Mechanical 
and  Nuclear  Engineering  with  Virginia 
Commonwealth University, Richmond, VA, USA.

14 

VOLUME XX, 2020 

 
 
 
Author Name: Preparation of Papers for IEEE Access (February 2017) 

from 

MARK  LOZANO  received  the  B.S.  degree  in 
mechanical  engineering 
the  Virginia 
Commonwealth  University,  Richmond  VA,  in 
2020.  His  research  interest  includes  designing 
machine 
for  varying 
applications. After fulfilling his ongoing contract 
in Chantilly VA, he plans on going back to school 
to  further  research  in  neuromorphic  computing 
and obtain a M.S. in computer science. 

algorithms 

learning 

DAMIEN  QUERLIOZ  (M’08)  received  the 
predoctoral  education  from  the  Ecole  Normale 
Supérieure,  Paris,  and  the  Ph.D.  degree  from 
Université Paris-Sud, in 2008. After postdoctoral 
appointments at Stanford University and CEA, he 
became a Permanent Researcher with the Centre 
for Nanoscience and Nanotechnology, Université 
Paris-Sud.  He  focuses  on  novel  usages  of 
emerging  non-volatile  memory,  in  particular 
relying on inspirations from biology and machine 
learning.  He  coordinates  the  INTEGNANO  Interdisciplinary  Research 
Group. He is currently a CNRS Research Scientist with Univeristé Paris-
Sud. In 2016, he was a recipient of the European Research Council Starting 
Grant  to  develop  the concept  of  natively  intelligent  memory.  In  2017,  he 
received the CNRS Bronze Medal. 

in  aerospace  engineering  from 

JAYASIMHA  ATULASIMHA 
Jayasimha 
Atulasimha (SM’11) received the M.S. and Ph.D. 
degrees 
the 
University of Maryland, College Park, MD, USA, 
in 2003 and 2006, respectively. He is a Professor 
of  Mechanical  and  Nuclear  Engineering  and 
Electrical  and  Computer  Engineering  with 
Virginia  Commonwealth  University,  Richmond, 
VA,  USA.  His  current  research  interests  include 
nanoscale 
magnetostrictive 
magnetization  dynamics,  and  multiferroic  nanomagnet-based  computing 
architectures. 

materials, 

VOLUME XX, 2020 

15 

 
 
 
Energy efficient learning with low resolution stochastic Domain Wall synapse based 

Supplemental Material to 

Deep Neural Networks 

Walid A. Misba1, Mark Lozano1, Damien Querlioz2 and Jayasimha Atulasimha1,3 

1Mechanical and Nuclear Engineering Department, Virginia Commonwealth University, Richmond, VA 23284 USA 

2 Université Paris-Saclay, CNRS, Centre de Nanosciences et de Nanotechnologies, Palaiseau, France 

3Electrical and Computer Engineering Department, Virginia Commonwealth University, Richmond, VA 23284 USA 

S1. Selection of Deep Neural Network (DNN) Architecture: 

The network topography we try to optimize in terms of training and testing accuracy used floating precision 
(32-bit) weights for the synapses. This optimized network was used to benchmark the performance of the 
DW based DNN of same topography.  

To select the optimal network topography multiple characteristics are considered: training accuracy, testing 
accuracy, network size, and fitness. An algorithm was designed to create, train, and test networks with each 
having a unique combination of parameters such as hidden layer numbers ranging from 0 to 9, the layer 
size ratio (ratio of a layer’s neurons to the previous layer neurons) ranging from 1/32 to 32/32, and a learning 
rate ranging from 0.001 to 0.009. Parameters that remain consistent are the learning rate decay and the 
starting epoch of learning rate decay. All the networks are trained using only one epoch, which results in 
underfitting networks where the training accuracy was lower than the testing accuracy. Limiting the number 
of epochs to one saves computational burden and as the network topographies are compared to one another 
so rather than achieving independent high accuracies the accuracy in reference to an alternate topography 
is prioritized. The two parameters learning rate decay and the starting epoch of decay, are chosen to be 10 
% at each epoch and the first epoch. The networks are trained on the full set of 60,000 training images and 
tested on the full set of 10,000 testing images from the MNIST handwritten digits database. From hidden 
layer number variation results (not shown here) we find that the accuracy increases with the increase of 
hidden layer, however, when the hidden layer number is greater than 3 then the accuracy does not increase 
appreciably. So, we select a total of 3 hidden layers for our DNN configuration. Fig. S1 shows the height 
map  for  training  accuracy,  testing  accuracy  and  average  of  training  and  testing  accuracies  for  different 
architectures of DNN with 3-hidden layers. We can see from Fig. S1 that the accuracies vary widely across 
the spectrum of variables. Thus, one can select several configurations of DNN and achieve good accuracies. 
We choose a layer size ratio of 16/32, so for the selected network the number of neurons for the first hidden 
layer is  
  the number of neurons of the input layer and so on. We choose a learning rate constant of 

1
2

×

0.007 as it would provide sufficient learning capacity with the increase of training epoch (learning rate 
decays by 10 % at each epoch). Thus, the final architecture consists of a network with three hidden layers, 
an initial learning rate of 0.007 and a layer size ratio of 

.   

1
2

 
 
 
Fig. S1:  Height maps showing the accuracies of networks with varying topographies with 3 hidden layers. The highlighted data 
point is the final topography chosen for our DW device based DNN as it is small in number of synapses. From left to right are the 
networks training accuracies, testing accuracies, and the average between the two. 

S2. Equilibrium DW Position Distribution with respect to Dominant Pinning Sites (Notches):  

The following Fig.S2b.-f. shows the equilibrium DW position distribution for the racetrack shown in Fig. 
) J/m3. 
S2a for five different programming conditions represented by 
Although DWs positions are pinned stochastically due to the edge roughness (~ 2nm rms) and the thermal 
noise due to temperature, T=300 K, however, most the DWs are pinned at the location of the pining sites. 
Thus, the distribution is heavily dominated by the pining site locations.  

 = 8, 7.75, 7.5, 7.25 and 7.0 (

× 10

𝐾𝐾𝑢𝑢

5

Fig. S2: a. Racetrack of dimension 600 nm x 60 nm x 1nm with rms edge roughness of ~ 2nm and the corresponding DW at the 
initial position that is 60 nm from the left end. Engineered notches starting from 60 nm are placed at a regular interval of 75 nm in 
the racetrack. b.-f. Distribution of equilibrium DW positions along the racetrack of Fig. S2a for different programming conditions 

 
 
 
represented by different PMA coefficient 
skewed to different notch positions for different programming conditions.   

𝐾𝐾𝑢𝑢
S3. Evolution of off-line Testing Accuracies with training Epoch:  

 . The DWs are primarily pinned at or around the notches, thus the distribution become 

The off-line testing accuracies with the number of epochs are presented in Fig. S3a and Fig. S3b for both 
low and high noise tolerance levels. The baseline testing accuracies with floating precision weights are also 
plotted for comparison. After each epoch of the training, the trained weights are collected and the devices 
in the crossbar are programmed. As we have allowed noise tolerance margin during testing (same noise 
tolerance margin 
 that is used during training) to program the device so for different trials of testing, the 
programmed weights of the network could be different. So, we consider a total 10 different trials for testing 
the DNNs. The error bar shown in the figure is computed for 10 trials of testing after each epoch of the 
=0.15, the offline testing accuracies reaches close to the baseline 
training. For noise tolerance margin of 
testing  accuracies  (same  topology  DNN  with  floating  precision  weights  and  no  stochasticity)  after  10 
epochs of training for 5-state and 3-state devices as seen from Fig. S3a. The highest offline test accuracy 
of ~ 96.63% is achieved for 5-state device which is very close to the baseline test accuracy of ~ 97.1%. 
=0.25, the test accuracies degrade for all the devices as can be seen from Fig. S3b. The 
However, for  
highest offline test accuracy for 5-state, 3-state and 2-state devices are obtained after 10 epochs of training 
which are ~ 95.14%, ~95.93% and ~ 94.24% respectively.  

𝛼𝛼

𝛼𝛼

𝛼𝛼

  .  The 
Fig.  S3.    Offline  testing  accuracy  for  DNNs  of  different  state  DW  devices  for  two  different  noise  tolerance  margins, 
accuracies are compared with a DNN trained and tested with full precision weights and no stochasticity (baseline accuracy). Offline 
testing accuracies with the numbers of epochs for noise tolerance margin of a. 
 = 0.25 used during the programming 
of the devices. Error bar is calculated for a total of 10 trials at each of the data points.  

 = 0.15 and b. 

𝛼𝛼

𝛼𝛼

𝛼𝛼

S4. Influence of Noise Tolerance margin on Accuracy:     

To examine the influence of noise tolerance margin on accuracy, training accuracy, online testing accuracy 
(training is done in-situ) and offline testing accuracy (training is done ex-situ) of 5-state DW based DNNs 
are compared in Fig. S4 for high and low level of noise tolerance margin, 
. From Fig. S4a and S4b we can 
see that, the difference in noise tolerance does not appear to have a significant effect on the training or 
online testing accuracies. This is due to the fact that backpropagation is performed over the imprecise DW 
device weight which is then used to update the  high precision weights for the network. In effect the potential 
values selected at random for the DW device weights are then known by the network, and through training 

𝛼𝛼

 
 
change the DW device weights accordingly based on the quantized value of the high precision weights. 
Once trained in-situ, the learned weights remain the same during testing due to non-volatility. In contrast, 
after ex-situ training the learned weights are transferred to the DNN by programming the DW device with 
a noise tolerance margin that is used during the training. When the devices are programmed prior to testing, 
programming noises are added to the DW device weights. The higher the noise tolerance margin, the higher 
the  amount  of  noise  that  could  be  added  to  the  programmed  device  weights.  Thus,  the  offline  testing 
accuracy degrades with higher noise tolerance margin. For example, if the trained weight is 0.24 then the 
quantized weight would be 0 and after programming  the DW device weight could be -0.24. Thus, with 
=0.25, the maximum deviation of the programmed weight from the learned 
noise tolerance margin of 
weights could be 
=0.15 the maximum deviation could be 
0.3.    

=0.5 whereas for low noise tolerance margin 

𝛼𝛼

2𝛼𝛼

𝛼𝛼

Fig. S4.  a. Training accuracy b. Online testing accuracy and c. Offline testing accuracy for a 5-state DW device based DNN for 
. The training accuracy  and online testing accuracy does not change  appreciably for 
two different noise tolerance  margins of 
different noise tolerance margin. Ex-situ testing accuracy decreases with high noise tolerance margin due to the higher deviation 
of device weights during the programming of the devices. 

𝛼𝛼

 
 
 
