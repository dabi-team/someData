2
2
0
2

b
e
F
1

]

O
L
.
s
c
[

3
v
5
2
8
1
1
.
1
0
0
2
:
v
i
X
r
a

RECURSION, EVOLUTION AND CONSCIOUS SELF

A. D. ARVANITAKIS

Abstract. We introduce and study a learning theory which is
roughly automatic, that is, it does not require but a minimum of
initial programming, and is based on the potential computational
phenomenon of self-reference, (i.e. the potential ability of an algo-
rithm to have its program as an input).

The conclusions agree with scientiﬁc ﬁndings in both biology
and neuroscience and provide a plethora of explanations both (in
conjunction with Darwinism) about evolution, as well as for the
functionality and learning capabilities of human brain, (most im-
portantly), as we perceive them in ourselves.

1. Introduction

Before we go into details about the potential use of self-reference, we

give a short argument which favors it:

We call an executable program c self-editing if its algorithm C has c as
an input and outputs one (or more as we will see) executable programs.
Assume now that we have a tree of self-editing executable programs
upon which some form of selection (either artiﬁcial or natural) applies.
Thus the algorithms of every self-editing executable program in the
tree is in charge of two activities: To answer to the environment and
(as self-editing) to compute their descendants having as an input their
executable program. Assuming now Darwinism, the answers to the
environment should evolve, thus the system can learn how to answer
to the environment. On the other hand, since algorithms produce also
their descendants, due to selection, they should learn how to calculate
them so as to answer better to the environment. This way the self-
editing property of this system serves as a means not only to learn,
but also to learn how to learn, resulting thus in a much more rapid
evolution.

In the classical context of mathematics, self-reference is a key in-
gredient of many famous proofs that arrive at a contradiction. One
can sketch an outline of this method very brieﬂy, as the application
of a function to an argument which is essentially a description of the
function itself. The original inventor of the method is a well known
1

 
 
 
 
 
 
2

A. D. ARVANITAKIS

mathematician, namely G. Cantor in his well known proof about the
existence of diﬀerent cardinalities of the inﬁnite [54]. The method has
since appeared also in a lot of equally famous proofs. Among them the
incompleteness theorem of G¨odel and the Halting problem of Turing. A
detailed study on the subject is provided by R.M. Smullyan in [68].

Especially Turing’s approach in the Halting problem, concerns a
roughly a function that can be calculated

computable function (i.e.
by a personal computer), having as an input its own program.

Henceforth, we are going to use the general term code to refer either

to an executable program, or data, or a unit of both.

Our approach in this paper is slightly diﬀerent from the above men-
tioned classic method both regarding the outcome, (as we ’ll see later
we use this method constructively and not towards the purpose of arriv-
ing at a contradiction), as well as in the method itself. We speciﬁcally
consider computational steps in such a procedure: Given an executable
program c for an algorithm, a computational step of c is determined by
an active instruction of c. To explain things better, assume that the
algorithm alg(c) that c codes for, is given an input i and starting from
this input, it computes successively

i = i1, i2, . . . , in,

giving in as an output. A computational step of alg(c) is any interme-
diate computation ik 7→ ik+1, k < n, in which an instruction of c is
activated. Such an instruction is generally considered simpler from the
algorithm itself, in this case it could be for example to add 1 to the
code calculated so far (i.e. to ik, and thus obtaining ik+1 = ik + 1.).
We are not going to deal here with what is and isn’t simple enough to
count for an instruction, as we are going to use this concept relatively
to the corresponding problem at hand. The situation is well known
in computer programming, in which instructions may vary from basic
ones of the programming language (or the assembly language for that
matter), all the way to complex functions already deﬁned to ﬁt our
purposes.

Since we deal with activated instructions, (which are subcodes of
the original code), it is very convenient to be able to represent the
relation of being a subcode to our notation. Thus, we write c = c[b]
in order to denote that b is a subcode of c. Notice that in this case b
is actually contained in c. For example the code b = [0, 1] is a subcode
of c = [[0, 1], 0], so that we may write c[b] instead of c to designate
that. What we gain this way, is that we may now denote the activated
instruction. For example, if b is the activated instruction of c, then we

RECURSION, EVOLUTION AND CONSCIOUS SELF

3

can use a bar above b to demonstrate that. We write in this case that
c is in the state c[b].

If b is a subcode of c, that is c = c[b], the resulting code if we subtract

b from c, will be denoted as c[∅].

Using these notations, the subject of our study is sequences of exe-

cutable programs

c1, c2, . . . , cn,

where for every computational step ck 7→ ck+1, k < n, ck serves both
as the (state-)program of the step, and also as an input.

In order to distinguish these kind of transitions from the ones found
in classical contexts of mathematics, we will be using the term self-
editing transitions. The executable programs ck, k < n will be called
also self-editing codes. The act of a self-editing transition of a (state-)
program c, will be denoted by self-ed(c). Thus, if c 7→ c′ is a self-editing
transition, and c is in the state c[b], then c′ results as the computation
of the algorithm alg(b), that b codes for, with input c[b], that is,

c′ = alg(b)(c[b]) = self-ed(c[b]).

Since b as an instruction can be essentially arbitrary, we obtain the

main tool of our study1:

Basic Self-Editing Principle. For every algorithm B, there is an
executable program b, (which is in fact a code for B) such that for every
code c[∅],

self-ed(c[b]) = B(c[b]).

In order to grasp the meaning of the above principle, it will be helpful

to freely translate it as follows:

Assume we are determined to make some changes in an executable
program c[b]. As long as these changes can be expressed in the form
of an algorithm (that is B), the same changes can be done by c[b] to
itself, assuming that its computational step is a self-editing one and b
is a code for B.

To see this principle in action, notice that:
Essential capabilities of a self-editing code:
(1) A self-editing code may proliferate itself during a self-editing step.
This results from the above principle, by choosing B to output multiple
codes. (Subsection 4.1, page 12).

1We have used here Church-Turing thesis, which is another way to say that any

algorithm has an executable program

4

A. D. ARVANITAKIS

(2) A self-editing code may potentially organize or change its struc-
ture. Suﬃce it to choose in the above principle an appropriate algo-
rithm B that performs the corresponding changes in the structure of
its input. (See for example subsection 7.1 in page 22)

(3) A self-editing code may retain the memory of itself. To roughly
see this in an one-step calculation, assume that A is any algorithm and
deﬁne B(A) to be an algorithm that outputs in a combined structure
If for example c′ = A(c),
the output of A together with the input.
then B(A) may output the structure [c, c′] that contains both c′ and
c. The result follows by the above Self-editing principle, using B(A) in
the place of B. (Section 6, page 18).

(4) A self-editing code may add an instruction to itself and moreover
it can do it, so that this instruction acts hereditarily. The capability
to add such an instruction follows from the fact that the algorithm
B in the above principe can do so. To do it in such a way that it
acts hereditarily is somehow more complicated and although several
examples that follow in the next sections show that it is possible, the
full statement will be addressed in the subsection 7.3, page 23.

(5) An activated instruction b of a self-editing code may alter param-
eters or subcodes of itself. It suﬃces to choose B to act on parameters
or components of the subcode b of c[b]. (See also the 2nd example in
page 14. We are going to use this feature essentially after page 24.)

Granting the above potential capabilities, let us now try to outline

our learning theory:

First notice that due to the capability (1), a self-editing code, may
produce more than one such codes, the same can happen to each one
of them, and so on. Continuing this procedure we may consider the ac-
tual outcome of a self-editing code to be rather a tree than a sequence
of self-editing codes. Consequently, we may assume some kind of selec-
tion, either artiﬁcial or natural and talk about surviving branches or
sequences. Assume now we observe such a surviving sequence

(1)

c1, c2, . . . , cn.

(Notice at this point, that due to capability (3) of retaining the memory
of itself, we may assume that the algorithm of the state code cn, has as
an input the entire sequence (1), exactly as we are able to observe it).
It is possible then to come to conclusions about what is preferable by
the artiﬁcial or natural environment, by just noticing details about the
structure of each code of the sequence. Assuming as a naive example
that in some part of the structure of each ck, k ≤ n, the number k is
stored, we may conclude that all the descendants of cn should register

RECURSION, EVOLUTION AND CONSCIOUS SELF

5

n + 1 in the corresponding part, even if we don’t really know anything
about the environment. The actual cause of this preference of the
environment, may be two-folded: Either we are actually talking about
an environment that changes smoothly its preference and to comply
with this change we have to add 1 to the aforementioned part of the
structure, or increasing the number that is stored there improves the
ﬁtness of our codes. In either case, the suggested strategy would be to
comply with this, so that if we wanted to enhance the survivability of
our codes, we should insert in cn an hereditary instruction to augment
by 1 this integer in the particular part of the codes of each descendant.
Another naive but essential example, is that we can notice that spe-
ciﬁc parts of the codes c1, . . . , cn remain the same, although this was
not intended from the beginning and therefore was not true for every
branch of the tree. The fact that we are talking about a surviving
branch, may give us in this case the indication, that we should repro-
gram cn adding a hereditary instruction to keep intact these parts in
every possible descendant. The reason is that this seems to be prefer-
able by the environment, which seems to judge any alteration of the
speciﬁc subcodes as non-surviving.

Assuming now that we are working on a computer, we can automate
this procedure by using some algorithm that performs some kind of
pattern recognition, in order to ﬁnd patterns in the surviving sequence
c1, . . . , cn and take decisions how to instruct things further. As we
’ll see immediately after, there is no reason to describe in detail such
an algorithm though. We just adopt the mere assumption that the
procedure generally works as follows:

Assuming that such a pattern can be intentionally constructed by
applying successively the algorithm of an executable code r to ck to
obtain ck+1, k < n, we can search for such an r, by deﬁning an algorithm
∆ that calculates in a row codes r and checks if they ﬁt the sequence,
i.e. it checks if for every k < n it is true that applying the algorithm
of the code r in question to ck has as a result the corresponding value
in ck+1.2 One should have in mind that this relation is not necessary
an equation, since r may output in only a speciﬁc part of the codes.
We say then that the code r ﬁts the transition ck 7→ ck+1 and denote
it as alg(r)(ck) ⊑ ck+1. For example, in the ﬁrst case above, r could
be an executable program that adds 1 to the speciﬁc part of the input
code, whereas in the second case, r could be an executable program
that copies the speciﬁc parts that remain intact.

2Regarding the case of non-convergence of alg(r)(ck), see below.

6

A. D. ARVANITAKIS

In the case that r ﬁts either all the sequence or a recent part of
it, ∆ takes an appropriate decision to apply it for all (or for all but
few) descendants of cn. We are going to use a speciﬁc term for such a
procedure, namely diagonalization.3

Assume now that δ = δn is a code for ∆. By the potential capability
of a self-editing code to retain its self-memory, i.e. (3) above, cn can
also have as an input the all sequence c1, . . . , cn, so that by the basic
self-editing principle, if cn[δn] is the state of cn, then it will produce the
same result as ∆ with the additional and important feature that now
∆ can ﬁnd and perpetuate also patterns of evolution of itself. Indeed,
assuming that δk, k < n, are versions of δ as subcodes of ck = ck[δk]
respectively, any pattern recognizable by ∆ and contained in the seqe-
unce

δ1, δ1, . . . , δn,

will be perpetuated as well. The beneﬁt is of course that ∆ in this case,
not only evolves the rest of the code, but also evolves its own code as
well.

Consequences of this simple approach, are somewhat surprising and

we just name a few here:

–It is well known by Turing’s Halting problem that the relation

alg(r)(ck) ⊑ ck+1
is undecidable.4 Based on its experience and using diagonalization, a
self-editing code can ﬁx an upper time limit about how much it should
wait for an answer. In general (much like us) it can deduce conclusions
about undecidable relations based on a (ﬁnite) experience. (Example
3 and the remark immediately after, in page 42).

–Assuming that a self-editing code has a feedback for the success of
its interaction with the environment, it can learn using diagonalization
to select its successful answers to the external world and diagonalize
upon them also. (In addition to diagonalizing upon all the surviving
sequence). (Section 9, page 34).

–Using diagonalization, a self-editing code may learn by experience
to recognize ﬁrst the most common patterns, even if it hasn’t been
programmed to do so (Example 2 in page 41). Moreover, it can do so
relatively to the task at hand (Learning specialization in page 43).

3The reasons for this name are historical and go back to the original self-

referential proof of Cantor, mentioned earlier.

4i.e. there is no general way to decide if a given executable program with a given

input will stop eventually and give an answer: It may as well run for ever.

RECURSION, EVOLUTION AND CONSCIOUS SELF

7

–It can regulate its internal parameters, including the parameters of
the diagonalization procedure, for example its memory, i.e. the length
of the sequence that uses to perform diagonalization, relatively to the
decision that it should make (Mental experiments 1 and 2 and Remarks
2 and 3 immediately after, page 28).

–Assuming an internal representation of the environment (for which
we are going to talk in section 12.3, page 47), a self-editing code may
using diagonalization to understand and predict smooth environmental
changes, for example velocity and acceleration. (Mental experiments 1
and 2 and Remark 4 immediately after, page 28).

The paper is organized as follows: Section 2 is devoted to basic
notations and deﬁnitions that are used. Sections 3 up to 7 serve as
a detailed approach to the general study of self-editing computations
and their relationship with usual computing. Sections 8 up to 11 are
the essential part of the study and are devoted on diagonalization and
its capabilities relatively to learning. Section 12 is devoted to outline
possible further research on the subject and also give some hints about
it. Finally, section 13 refers to related work on the subject and the
evidence that is provided by it.

Aknowledgements. The research in this paper has been done
in the course of many years and many people (mostly mathemati-
cians) have contributed to it, in various ways. (In chronological order)
Maria Avouri, James Stein, Despoina Zisimopoulou, Dimitris Apat-
sidis, Fotis Mavridis, Antonis Charalambopoulos, Vanda Douka, An-
tonis Karamolegos, Helena Papanikolaou and Miltos Karamanlis are
among them.

2. Structured codes and addresses

We will assume that an algorithm in general has both as an input
and as an output codes. A code is thought to be a string (or sequence)
on a given (ﬁnite) set (or lexicon) L. For example codes on L = {0, 1}
are the strings 001, 11001 and so on. The codes that we are going to use
are supposed to have a structure, something that oﬀers the facility of
either referring to them as activated, or use them as inputs or outputs.
On the other hand, codes can serve also as descriptions of algorithms,
which will be proved very useful in the sequel.
In case we want to
highlight the diﬀerence, we refer to such codes as executable codes or
executable programs, while non-executable codes are referred to as data
codes. Finally a code may contain both executable parts and data
parts.

8

A. D. ARVANITAKIS

For reasons that will become apparent in the sequel, it is convenient
to be able to refer to speciﬁc parts of a code, using specialized for this
purpose algorithms. This procedure can be supported by the structure
itself of a given code. As an example, we may consider the code

01 0010 0011

This code has three parts, the ﬁrst one being the sub-code 01, the
second one being 0010 and ﬁnally the third one being 0011. This way
one can refer to these three sub-codes using the symbols (1), (2) and
(3).

Leaving blank spaces is a very common way to indicate structure and
it just requires to assume that our lexicon L, contains a blank space.
However it is not at all convenient for mathematical analysis where
structure is usually indicated by the use of left and right parentheses,
namely by the symbols ( and ). The advantage of using parentheses is
that one may use them as an easy way to indicate a nested structure
of a code. As an example, we may consider the code

((01, 001), (11, 00), (111, (00, 11, 0))).

One can use also blank spaces to describe the same code:

01 001

11 00

111 00 11 0,

yet this second way although more intuitive, induces ambiguities. Tra-
ditionally, in mathematics, we can refer to the various parts of such a
code, by a sequence of natural numbers indicating the position of the
part being considered. For example, the sequence (1) may indicate the
ﬁrst of the three parts of the above code, namely (01, 001), whereas the
sequence (3, 2, 2), indicates the second part of the second part of the
third part of the code, which is 11. Sequences as the above ones that
determine a part of a code will be called addresses.

In our exposition, we are going to replace parentheses with brackets:
[ and ]. The reason is not to confuse the structure of a code with the
application of a function (usually algorithmic) that is applied on a code.
Moreover, for technical reasons, we prefer to use brackets to indicate
in an unambiguous way a code written using blank spaces. This way,
for example, the code x is identiﬁed with the code [x], something that
will save us from time, space and unnecessary complexity during the
study.

On the other hand, we are going to preserve the above mentioned
traditional way of addressing sub-codes, yet somehow extend it. We
are going to use the term address not only for strict addresses explained
above, but also for simple (algorithmic) functions that compute them.

RECURSION, EVOLUTION AND CONSCIOUS SELF

9

For example by address (last) we mean the last component of a code,
i.e. the n-th part of a code of length n. That is, the (last) address of
[c1, . . . , cn] contains the code cn.

As explained also in the introduction, in order to describe that b is a
subcode of c, we write c as c[b]. The fact that b is contained speciﬁcally
in the address θ of c, is denoted as c[(θ)b], where parentheses around
θ in this case demonstrate that θ is not a part of the code. Similarly,
we use the notation c[(θ)∅] to indicate that θ is empty in c and the
notation c[∅] to describe a code c with an empty address.

3. Algorithmic computations

In order to be able to study self-editing computations, we will have
to describe a general way to compose algorithms, out of simpler ones,
i.e.
instructions. Although we talk about algorithms, we will retain
the term program for such a description.

Such an analysis consists of a ﬁnite sequence B1, . . . , Bn of elements
of a set B, (thought of as the set of instructions), plus an algorithm B
that controls the ﬂow of the execution of B1, . . . , Bn. We assume there-
fore that B1 is the ﬁrst instruction to be executed and subsequently B
controls the next instructions based on the index of the last instruction
executed and the computation up to this step. (This last one, since
the ﬂow of the execution may be conditional). More formally:

A B-algorithmic computation with program (B : B1, . . . , Bn), where

B1, . . . , Bn ∈ B,

and B is an algorithm that controls the ﬂow, is described as follows:

Given a code x (which serves as an input), (B : B1, . . . , Bn) is

thought to produce a sequence of codes

where:

x = x1 7→ x2 7→ · · ·

(1) For every k = 1, 2, . . . , there is an index jk in the range 1, . . . , n,
such that xk+1 = Bjk(xk). In this case, Bjk is called the activated
instruction of the computational step xk 7→ xk+1.

(2) We assume that j1 = 1, i.e. x2 = B1(x1).
(3) For every k = 1, 2, . . . , we have that jk+1 = B(jk, xk+1) i.e. the
instruction that is currently used is an (algorithmic) function
of the previous one and the current computation. (Notice that
xk+1 = Bjk(xk), so that xk+1 is in fact the current computation).
It should be clear that any computer program on any language is
of the form (B : B1, . . . , Bn), where B1, . . . , Bn are instructions of the
language and B is a suitable algorithm that controls the ﬂow of the

10

A. D. ARVANITAKIS

execution. Conditions regarding the ﬂow are thought to be contained
in the computation x1, 7→ x2, 7→ · · · , which is the reason for letting xk
to be part of the input of B. For example, a loop-free program that
executes the instructions B1, B2, . . . , Bn in the same order, on input x
produces the computation

x = x1 7→ x2 7→ · · · 7→ xn 7→ xn+1,

where for all k, xk+1 = Bk(xk) and is formalized by the algorithm
(B : B1, . . . , Bn), where for all k, B(k, xk+1) = k + 1. (In this case, B
just follows the next instruction).

Remark By allowing each instruction of B1, . . . , Bn to be a program
itself, i.e. a similar structure of the form (C : C1, . . . , Cm) (instead of
being just an element in B), we naturally arrive to the notion of a
structured program, that is a program that has nested programs as
instructions, instead of elements in B. Clearly this procedure can be
repeated, yielding in turn programs for C1, . . . , Cm and so on. In this
case, Ci, will be called sub-instructions of the original program. We
will use the same terminology for the instructions of Ci and so on.

Structured programs enable us to use each of B1, . . . , Bn as arbi-
trary algorithms and be therefore able to analyze a computation to the
desired degree.

3.1. Proliferating computations. In what follows, we are going to
abuse the usual notation about sets, by allowing elements in sets to
be repeated. For example {x, x}, x being a code, denotes a set of
two identical codes and is thought to be diﬀerent than {x}. We should
notice that this convention could be avoided, yet doing so would add a
signiﬁcant amount of unnecessary complexity in the present study.

The concept of a proliferating computation follows by letting instruc-
tions in a program having multiple outputs. As a simple example, the
function

R : x 7→ {x, x},
(x being a code) that having x as an input, produces an output of
two identical copies of x, is clearly algorithmic. More generally, if
R1, . . . , Rk is a set of algorithms, not necassarily diﬀerent between
them, let us denote by {R1, . . . , Rk}, the algorithm deﬁned as

{R1, . . . , Rk}(x) = {R1(x), . . . , Rk(x)}.

Such an algorithm that outputs more than one codes will be called pro-
liferating. The codes R1(x), . . . , Rk(x) are called children or immediate
successors of x, and the set that contains them is denoted by im-suc(x).
The instruction {R1, . . . , Rk} is called the activated instruction of the

RECURSION, EVOLUTION AND CONSCIOUS SELF

11

step and similarly, in the (partial) computational step x 7→ Ri(x),
i = 1, . . . , k, Ri is called (as before) the activated instruction corre-
sponding to the (partial) step.

As mentioned above, proliferating programs are programs with one
or more proliferating instructions. Proliferating programs induce a
branching computation, where each code may have more than one im-
mediate successors. Such a structure is called a tree, and we usually
denote it as (xt)t∈T , where the set of indices T has the same tree-
structure. Any element in a tree may have from 0 to any ﬁnite number
of immediate successors or children. The input x = x∅ is called the
root of the tree and the conventions we make about this branching
computation are similar to the ones we made about the simple one.
Namely, given a proliferating program (B : B1, . . . , Bn) and a code x
as an input, the computation is deﬁned as the tree (xt)t∈T such that

(1) The root of the tree x∅ is the input x.
(2) For every t ∈ T, there is an index jt in the range 1, . . . , n, such
that the set of immediate successors of xt, im-suc(xt) is equal
to Bjt(xt).

(3) We assume that j∅ = 1, i.e. im-suc(x∅) = B1(x∅),
(4) If t′ is a child of t, then jt′ = B(jt, xt′), i.e. the instruction that
is currently used is an (algorithmic) function of the previous
one and the current computation.

A branch of a computational tree (xt)t∈T is a sequence

xt1 7→ xt2 . . . ,

such that for any k, xtk+1 is a child of xtk .

4. Self-editing computations

Let c = c[b] be an executable code that contains b as an instruction
in some address. Assume that x1, x2, . . . , xn, . . .
is a computation of
the algorithm that c codes for, with input x1. Assume also that dur-
ing a computational step xk 7→ xk+1 of alg(c), alg(b) is used as the
instruction of the step. We say then that b is the activated code of
the computational step and that the code c[b] is in state c[b]. Codes in
some state will simply referred to as state-codes. The step-algorithm of
a state-code c[b] denoted as step-alg(c[b]) is deﬁned to be the algorithm
that b (i.e the activated code) codes for:

step-alg(a[b]) = alg(b).

12

A. D. ARVANITAKIS

Assuming now that the sequence c1, c2, . . .

is a sequence of state-

codes, a computation

(2)

c1 7→ c2 7→ · · · 7→ ck 7→ · · ·

is called self-editing if for every k, ck plays both the role of the data-
code of the computation and at the same time the role of the state-code
that performs the computation. More formally, the computation (2) is
called self-editing if:

ck+1 = step-alg(ck)(ck)

for every k = 1, 2, . . . .

Every calculating step ck 7→ ck+1 of a self-editing computation will be
called a self-editing step and will be abbreviated as self-ed(ck). I.e.

self-ed(ck) = step-alg(ck)(ck) = ck+1.

If ck 7→ ck+1 is a self-editing step, both the state-code ck and its

step-algorithm step-alg(ck) will be called self-editing.

We can now state the main ingredient of our study which is the

following:

Basic Self-Editing Principle. For every algorithm B, there is a

code b, (which is in fact a code for B) such that for every code c[∅],

self-ed(c[b]) = B(c[b])

Proof. As stated, let b be a code for B and apply the deﬁnitions:

self-ed(c[b]) = step-alg(c[b])(c[b])

= alg(b)(c[b])

=B(c[b])

as needed.

(cid:3)

4.1. Self replication. A ﬁrst simple application of the Basic self-
editing principle results considering as B, the proliferating algorithm
that computes two exact copies of its input. I.e., let B be deﬁned by

B(x) = {x, x},

x being any code.

The basic self-editing principle then asserts that if b is a code for B,
and for any code c[∅], the self-editing computational step of c[b] is a
duplication of itself, i.e. (using the above set-like notation)

self-ed(c[b]) = {c[b], c[b]}.

Clearly, this self-editing computational step is (since b remains ac-
tive) by deﬁnition repeated for each of the two descendants, creating a

RECURSION, EVOLUTION AND CONSCIOUS SELF

13

tree-like structure (xt)t∈T , where T is the dyadic tree, (i.e. every node
has exactly two children) and for all t ∈ T, xt = c[b].

As we will see, more complex self-editing computations, generate

trees with varying ﬁnite number of immediate descendants.

It is clear that a proliferating self-editing computational step need
not be just self reproducing in the sense that instead of producing
copies, it may produce variations of itself, something that is going to
be proved much more interesting regarding evolution. The general case
of such proliferating steps may be stated as follows:

Proliferating principle Let B1, . . . , Bn be algorithms. Then there

is a code b such that for any code c[∅],

self-ed(c[b]) = {B1(c[b]), . . . , Bn(c[b])}.

If moreover b1, . . . , bn are codes for B1, . . . , Bn respectively, then we

may assume that they are also sub-codes of b.

Proof. Let B be the algorithm that having a code c as an input com-
putes:

B(c) = {B1(c), . . . , Bn(c)}.
Let also b be a code for B. By the Basic self-editing principle, we get
that for any code c[∅],

self-ed(c[b]) =B(c[b])

={B1(c[b]), . . . , Bn(c[b])}.

For the second part of the statement,

if b1, . . . , bn are codes for
B1, . . . , Bn, deﬁne the code b = s[b1, . . . , bn] as a code for the following
algorithm:

(1) Let w be the input
(2) For i = 1, . . . , n:

(a) Activate bi on input w and output the result alg(bi)(w).

Then clearly, for any code c[∅],

self-ed(c[b]) = alg(b)(c[b])

= {alg(b1)(c[b]), . . . , alg(bn)(c[b])}

= {B1(c[b]), . . . , Bn(c[b])},

as needed.

(cid:3)

Remark Using the same notation as above, Bi, i = 1, . . . , n is called
the active instruction of the (partial) step c 7→ Bi(c). In case that bi is
both a code for Bi, and a sub-code of b, it will be called as the active
code of the (partial) step c 7→ Bi(c).

14

A. D. ARVANITAKIS

Let us explore some other examples of the application of the prolif-

erating principle, which also demonstrate the versatility of its use:

Examples. 1) We may assume that a self-editing code contains
addresses for input and output regarding to the environment. Such
addresses will be denoted as environmental input and environmental
output respectively, in order to distinguish them from the entire code
which is considered both the input and the output of a self-editing
algorithm. So let [b, 0] be such a code, where environmental output
occurs in address (2) and thus the code outputs 0 in this case. Accord-
ing to the proliferating principle, b can be obtained by deﬁning two
algorithms B1 and B2 as:

B1([b, n]) = [b, n + 1]

and

B2([b, n]) = [b, n + 2].

Thus the self-editing computational step of [b, 0] gives two descen-

dants that output to the environment 1 and 2 respectively:

self-ed([b, 0]) = {B1([b, 0]), B2([b, 0])} = {[b, 1], [b, 2]}.

By deﬁnition, this is repeated for each of the two descendants, so
[b, 1] gives again two descendants that output to the environment 2
and 3 respectively, whereas [b, 2] gives two descendants that output 3
and 4 and so on. The situation can become much more complicated
by deﬁning Bi, i = 1, 2 to act also on b. This can change the way
that the descendants are calculated. The following is an example of
this (permitted by self-editing) situation. We are not going to use it
further for the moment, until section 8.

2) Let θ1 and θ2 be two addresses. Their concatenation is denoted
1 θ2, and is deﬁned as the address θ2 of the code in address θ1. For

as θ⌢
example if θ1 = (1, 2) and θ2 = (1), then θ⌢
Let B(n, θ) be the algorithmic function:

1 θ2 = (1, 2, 1).

B(n, θ) : a[(θ)k] 7→ {a[(θ)k + 1], . . . , a[(θ)k + 1]

},

n times

|

{z

that is, B(n, θ) outputs n identical codes by adding 1 in θ address of
the input code.

Let b[(θ1)n][θ] = b[n][θ] be a code for B(n, θ) and c[(θ2)∅] = c[∅]
2 θ1. Notice then,
, b[n][θ] lies in the θ2 address and in b[n][θ], n
, n lies in the θ0 address.

be any code with empty θ2 address. Deﬁne θ0 = θ⌢
that in the code c
lies in the θ1 address. Therefore in c
(cid:2)

b[n][θ]

b[n][θ]

}

(cid:3)

(cid:2)

(cid:3)

RECURSION, EVOLUTION AND CONSCIOUS SELF

15

Thus, adopting the notation

we get that

,
n × c = c, . . . , c

n times

| {z }

alg(b[n][θ0])

c

b[n][θ0]

= {n × c

b[n + 1][θ0]

},

which shows that

(cid:16)

(cid:2)

(cid:3)(cid:17)

(cid:2)

(cid:3)

self-ed

c

b[n][θ0]

= {n × c

b[n + 1][θ0]

}.

(cid:16)

(cid:2)

(cid:3)(cid:17)

(cid:2)

Concluding, the self-editing computational step of c[b[n][θ0]] gives n
children with varying genetic behavior so as to compute n + 1 children
with varied genetic behavior so as to compute n + 2 children and so on.
It is easy to see that one can alter this example to describe self-
editing codes that compute children with varying genetic behavior
among them. Let us call for the moment n as genetic variable and
let [n/3] be the closest from below natural number to n/3. By deﬁning

(cid:3)

B(n, θ) : c[(θ :)k] 7→ {[n/3] × c[k + 1], [n/3] × c[k], [n/3] × c[k − 1]}

one arrives similarly as above to deﬁne self-editing codes that give
children of increased, the same and decreased genetic variables. Since in
a less theoretical environment available resources limit the production
of children, selection would act in this case, so that the branches with
the more appropriate number of children would survive. Yet selection
could not alter the original behavior described by B(n, θ) above. Even
in surviving branches this strategy would remain the same. In section
8, we are going to introduce a general method called diagonalization
by which a self-editing algorithm may learn to choose the number of
children according to the available resources and alter thus the genetic
behavior described by B(n, θ) more appropriately.

4.2. Self-editing trees. The proliferating principle stated above in-
duces the notion of a proliferating self-editing computation:

Given a state-code c∅, a proliferating self-editing computation is a
tree (ct)t∈T with root c∅, such that the children of every ct are deﬁned
by the application of step-alg(ct) to the code ct, i.e. such that

im-suc(ct) = self-ed(ct) = step-alg(ct)(ct)

for every t ∈ T.

For simplicity reasons, in the case that the step-algorithm of ct produces
a single output, we identify the singleton {self-ed(ct)} with the code
self-ed(ct) and call the corresponding step non-branching. So, a self-
editing computational sequence (where a code may change its value via

16

A. D. ARVANITAKIS

the algorithm that describes) may be thought of as a special case of a
self-editing tree.

5. Programming a self-editing tree

As we saw in the basic self-editing principle, any external algorithm
B may be executed by a self-editing code internally. Assuming that
B = (C : C1, . . . , Cn) is a program, we would like to examine if it is
possible for a self-editing code to use the same calculations as does the
program B. This is particularly useful in the cases that we want to
analyze a particular computation performed by a self-editing code into
simpler ones.

As we ’ll see in the following Programming lemma the answer to this

can be roughly stated as follows:

For every algorithm P, there is a state-code p (which is roughly the
executable program of P ), such that for every code c[∅], the computa-
tion of P with input c[p] is the same as the self-editing computation of
c[p], providing that P does not alter its code i.e. p.

Indeed, if P changes p during the computation, the above state-
ment cannot be expected to hold, since in this case the corresponding
self-editing computation will change its own intended program by mim-
icking P. Although we are going to appeal to this kind of phenomena
later on (and will be proved the main targets for studying), for the
time being we rather wish to avoid them. Therefore one should con-
sider programs that during their computation do not alter the address
of p above. In this direction, we need the following:

We will assume that the result of a program P that outputs the
code s in address θ of the code c[(θ)b], is c[(θ)s] (and not just s), unless
other algorithms output to diﬀerent addresses of the code. (For this
reason, we will write P (c[b]) ⊑ c[s], instead of P (c[b]) = c[s] which
might be misleading). For simplicity reasons, this convention is made
for all possible immediate descendants of the code in the computational
tree. After this convention, a program P will be called θ-stable if for
every computational step c1 7→ c2 instructed by P, the θ address of c1
coincides with that of c2. Clearly if a program P is θ-stable then every
instruction of P that may be used in a computational step ought to be
θ-stable too.

Secondly, P is not expected to activate the correct codes during the
computation, something that is expected to happen during the self-
editing computation of c[p], by the deﬁnition of the states of p during
the computation. Thus one needs to compare the deactivated versions
of the two computations, deﬁned below:

RECURSION, EVOLUTION AND CONSCIOUS SELF

17

Two state-codes are called code-equivalent if they are state-codes
of the same code. Similarly two computations (xt)t∈T and (x′
t)t∈T ′
are called code-equivalent if T = T ′ and for every t ∈ T, xt is code-
equivalent to x′
t.

Granting the above deﬁnitions and notations, we can prove the fol-

lowing (where the code in the stable address is exactly p):

Programming Lemma Let P be a θ-stable (proliferating) program.
Then there is a state-code p, such that for any code c[(θ)∅], the self-
editing computational tree of c[(θ)p] is code-equivalent to the computa-
tional tree of P with input c[(θ)p].

Moreover, if P = (B : B1, . . . , Bn), and b, b1, . . . , bn are codes for the
algorithms B, B1, . . . , Bn respectively, then we may assume also that
b, b1, . . . , bn are sub-codes of p.

Before passing to the proof, for θ an address and i a natural number,
let us denote by θ⌢i the address that results by appending i to θ. For
example, if θ = (1, 3), θ⌢4 = (1, 3, 4).

Proof. For the proof we are going to code both the function of Bi and of
the ﬂow-controlling function B, in a code si = s[b, bi] described below.
Notice, that although P is supposed to be θ stable, the algorithms of si
shouldn’t be, since they have to inactivate and activate the appropriate
instructions. Notice also that for any code that contains [s1, . . . , sn] in
its θ address, the addresses θ⌢i, i = 1, . . . , n, correspond to the codes
si respectively. In the following algorithm we assume that the input
is always a code that contains [s1, . . . , sn] in its θ address and in a
state that one of s1, . . . , sn is activated, something that can be proved
afterwards by induction.

So, let si = s[b, bi] i = 1, . . . , n, be a code for the following algorithm:
(1) Let w be the input.
(We assume here that w is of the form
x[(θ⌢i)[s[b, bi]]].)

(2) Let {x1, . . . , xk} be the output of activating bi with input w. (No-
tice that by assumption bi is a code for Bi which is θ stable)
(3) For every j = 1, . . . , k : (i.e. for every code generated by Bi)

(a) Let i′ be the output of activating b with input (i, xj). (Since
b is a code for B, this should give the number of the next
instruction to be executed).

(b) Let x′

j be the result of deactivating the code in address (θ, i)

of xj. (The particular code should be s[b, bi].)

(c) Let x′′

j be the result of activating the address θ⌢i′ of x′

j. (So

that the correct instruction is activated).

(d) Output x′′
j .

18

A. D. ARVANITAKIS

Set p = [s1, s2, . . . , sn]. Let (xt)t∈T be the computation of P and

(yt)t∈T ′ be the self-editing computation starting with c[p].

Notice ﬁrst that since P is θ-stable, for every t, the code in address

θ of xt must be p.

Assume that for some t ∈ T, it is true that t ∈ T ′, xt and yt are
code-equivalent and that if Bi is the active instruction of P then si is
activated in yt. (Notice that this assumption is true for the roots). So
the immediate successors of yt are the result of alg(si)(yt).

Since Bi and alg(si) have the same number of output codes (which
is k in the algorithm described above), it follows that the immediate
successors of t in T are the same as the immediate successors of t in
T ′. On the other hand, for every xj that Bi outputs, alg(si) outputs
a state-code x′′
j which results from deactivation and subsequently acti-
vation processing of xj. (See the instructions (b), (c) and (d) above).
Therefore xj and x′′
j are state-codes of the same code. Furthermore P
activates the instruction with index i′ = B(i, xj) for xj and in x′′
j is
activated the code in address θ⌢i′ which should be the i′-component
of p, therefore si′. This concludes the inductive step and therefore the
(cid:3)
proof.

Though Programming lemma is necessary to state towards a more
thorough understanding of self-editing, its most interesting applications
as mentioned above, occur in the cases that P is not θ-stable, in which
case alg(p) constructed in the proof, might change p also. In this case
there is no real “program” behind the self-editing procedure that takes
place, yet it gives the opportunity of evolving P as we will see.

6. Memory of self

Let (ct)t∈T be a computation and ﬁx a t ∈ T. The history of ct is

deﬁned to be the sequence

ct1 7→ ct2 7→ · · · 7→ ctn = ct,

where ct1 = c∅ is the root of the tree, and for every i < n, cti+1 is a
child of cti. (Notice that ct itself is contained in this sequence). Below
we denote by hist(ct) the history of ct.

Given a state-code c = c∅, a complete memory self-editing computa-

tion, is a tree (ct)t∈T such that for every t ∈ T,

im-suc(ct) = step-alg(ct)(hist(ct)),

i.e. every state-code ct computes its children by applying its algorithm,
not only on its code, but on the entire sequence of its history.

RECURSION, EVOLUTION AND CONSCIOUS SELF

19

On the other hand, a complete memory instruction, is thought to be
an algorithm B, that computes the children of a code ct having as an
input the history of ct, and a complete memory program is a program
(as we have deﬁned it) that uses complete memory instructions.

It is easy to see that both the basic self-editing principle and the
programming lemma can be stated and proved in the case of complete
memory algorithms. Below,we state the complete memory version of
the basic self-editing principle for future reference:

Basic self-editing principle. (Complete memory version) Let B
be an arbitrary algorithm with code b. Let also c1, . . . , cn be the history
of the code cn in a computational tree. If the code b is activated in cn,
then the self-editing computation of complete memory performed by cn,
is equal to B(c1, . . . , cn).

What is interesting here, is that although at a ﬁrst glance, a self-
editing computation of complete memory seems stronger than a simple
one, this is not true. The idea behind this, is that a complete memory
self-editing computational step can be simulated by a simple one that
stores memory.

Fix φ to be an algorithmic correspondence with an algorithmic in-
verse between sequences of codes and codes. A simple such function is
for example

φ : (c1, . . . , cn) 7→ [c1, . . . , cn].
Let us notice that, although for practical applications, this particular
correspondence is not at all space-saving eﬃcient, it ﬁts very well a
theoretical approach, so we are going to use it in what follows.

Hereafter, we are going to use H(cn) = φ(hist(cn)) to denote brieﬂy
[c1, . . . , cn], where c1, . . . , cn is the history of cn. Also, the address (last)
of φ(hist(cn)) is deﬁned to be where cn (the currently active code) is
lying.

Notice that by replacing a code x with φ(hist(x)), we assume that
all addresses except the address (last) are memory addresses and thus
inactive, even if they do have active components. The reason behind
this approach is the obvious one: We would like to keep the remem-
brance of an active component, and on the other hand, introducing a
new symbol for past activity would result in unnecessary complexity.
Having this convention in mind, we arrive at the following deﬁnition:
A self-editing computation is called memory storing if it is of the

form:

[c1], [c1, c2], . . . , [c1, c2, . . . , cn].
It is useful to notice, that as we have mentioned in section 2, the initial
code [c1] may be thought to be the same as c1. Before passing to state

20

A. D. ARVANITAKIS

and prove that a self-editing computation of complete memory can be
simulated by a simple one that instead stores memory, let us discuss the
idea behind this, which is quite simple. Indeed, what we have to do is
to replace a self-editing computational sequence c1, . . . , cn of complete
memory with the corresponding one that stores memory, namely

c1, [c1, c2], . . . , [c1, c2, . . . , cn]

and make sure that the algorithm that is described by the state code
cn should be executed upon the sequence that is described by its code,
i.e. c1, . . . , cn. This of course would yield easily the result and it could
be accomplished by the simpliﬁed instruction (where appending x to
[c1, . . . , cn] is thought to result in the code [c1, . . . , cn, x]):

If [c1, . . . , cn] is the input, execute cn with input the sequence
c1, . . . , cn, append the result (or the results) to [c1, . . . , cn], and
output the computed code (or codes).

If m is a code for such an instruction and we replace each ck in
the sequence by ck[m], then the result follows easily and immediately.
However, in practice this procedure meets certain technical diﬃculties.
On the other hand, analyzing and solving them, has the advantage
that one discovers a great deal of the surprising versatility of using
self-editing. So let us ﬁrst indicate and discuss them:

1) First of all, the code m in cn[m] should be of a greater priority
than any activated code in cn alone. This has to be so, since the acti-
vated codes in cn should be stopped from being executed at ﬁrst place,
until the sequence c1, . . . , cn which serves as an input to step-alg(cn)
is computed from [c1, . . . , cn]. So we will have to use some notion of
priority to the codes that are being executed, something that we dis-
cuss immediately after. Yet, here comes another problem. To assign
a priority to m that is greater of all priorities used in the (inﬁnite)
sequence c1, c2, . . . . This of course is impossible, yet self-editing as we
will see, permits us to deﬁne m so that alg(m) ﬁxes its own priority in
every step ck[m] 7→ ck+1[m] so that it exceeds that of every activated
code in ck+1 alone.

2) The second technical diﬃculty comes from the fact that we should
be able to ﬁnd an address to accommodate m which is not an address
that is used or being created during the computational sequence. The
easy (yet a bit technical) solution that we use here, is to “add a di-
mension” in the structure of the code by letting cn[m] to stand for
[[cn], m].

Let us mention here that the notion of priority is met both in biology,
where a gene can stop another one from being expressed, and equally

RECURSION, EVOLUTION AND CONSCIOUS SELF

21

well in neuroscience, where a neuron can stop another one from ﬁring.
We will model it here, by assuming that in some relative address of an
instruction, say θp lies a natural number that determines the “strength”
of the instruction relative to other ones.

Since the notation c[m] has been reserved to symbolize that the (ac-
tive) code m is a subcode of c, we use c[(+)m] to denote that a new
address has been appended to c that contains m.

We state and prove at this point the relevant result:

Memory lemma. Let (ct)t∈T be a self-editing computation of com-
plete memory. Then there is a code m such that c∅[(+)m] results in the
simple (without memory) computation (xt)t∈T , where for every t ∈ T,
if c1, . . . , cn = ct is the history of ct in (ct)t∈T , then

xt = [c1[(+)m], . . . , cn[(+)m]].

Proof. Let m be a code for the following algorithm:

(1) Let x be the input. (We assume here that x should be of the
form [[[c1], m], . . . , [[cn], m]] and moreover the priority of m in
each of the codes exceeds that of the corresponding ci.).

(2) Stop any instruction from being executed in address (last, 1, 1).

(Notice that this address should contain cn).

(3) Let n be the length of x.
(4) For i = 1, . . . , n, let zi be the code in address (i, 1, 1) of x. (So

that in fact zi = ci).

(5) Execute the state-code zn with input z1, . . . , zn and let {w1, . . . , ws}
be the result. (So that w1, . . . , ws are in fact the children of cn).
(6) Let y be the code in address (last, 2) of the input. (So that y

should be equal to m itself ).

(7) For j = 1, . . . , s:

(a) Fix the priority of y to be strictly greater than that of any

activated code in wj.

(b) Append [[wj], y] to the input x and output the result.
(Thus according to our initial assumption, the output should be
all the codes of the form

[[[c1], m], . . . , [[cn], m], [[wj], m]],

where wj ranges over the children of cn and moreover the prior-
ity of m in the above code exceeds the one of any activated code
in wj.

Fix now the priority of m to be strictly greater than that of any acti-
vated code in c∅ and let x∅ = [[c∅], m]. Thus the assumption we make

22

A. D. ARVANITAKIS

in the instruction (1) of the deﬁnition of m holds true for x∅ and the
rest comments next to the instructions provide an inductive proof of
(cid:3)
the result.

Memory principle allows us to consider a simple self-editing com-
putation as one of complete memory, assuming that the memory is
stored. Nevertheless, in a self-editing computation, memory storing is
sometimes not needed. This is made obvious in the following examples:

Examples. Let R and S be algorithms. and x1, . . . , xn = x the
history of a code x. Assume that for every k = 1, . . . , n, the computa-
tion up to xk is memory storing, i.e. there are codes c1, . . . , cn, such
that xk = [c1, . . . , ck]. Given an algorithmic function R, notice that the
computations

[c1, . . . , cn] 7→ R(c1) and [c1, . . . , cn] 7→ [c1, . . . , cn, R(c1)],

are both algorithmic, so that by the basic self-editing principle, if their
codes are existent and activated in cn, will produce them. The main
diﬀerence is that while the second one continues to store memory, the
ﬁrst one deletes it. We are going to refer to both of them as cycles,
since they are thought to produce a variation (or even a copy for R
being the identity), of the initial code.

Another interesting example of this kind, is the proliferating self-

editing computation

[c1, . . . , cn] 7→ {R(c1), [c1, . . . , cn]},

which may be thought of as modeling (for R being the identity) the
self-replication of multi-cellular organisms (with one parent).

7. Possible decisions of a self-editing code

7.1. Storing decisions. Let r and c be codes and let us consider the
following algorithmic function B(r) that acts having as input the code
c:

(3)

B(r) : c 7→

[c1, . . . , cn, r]
[c, r]

(

if c = [c1, . . . , cn],
if not

which simply appends r on c. The basic self-editing principle ensures
us that if c is self-editing, b = b[r] is a code of B(r), and c = c[b] is the
corresponding state of c in which b is activated, then the resulting self-
editing step of c is the one described by (3). Notice that the address
that is used by B(r) to store the code r, is a new one, i.e. one that was
not existed previously in c. Such an address will be called an available

RECURSION, EVOLUTION AND CONSCIOUS SELF

23

address, so that we can abbreviate the corresponding self-editing step
of c[b], as resulting from the instruction:

Store r in an available address.
To denote that θ is an available address in a code c, we will write
that c = c[(+θ)∅] or in a more simple form c = c[(+)∅]. (Notice that
in this case θ is not existent in c, yet it may be created).

7.2. Temporary decisions. Let r and c be codes and θ an available
address in c. Let also s = s[r] be a code for the algorithm that is
described by (For the reader’s convenience the input is thought to be
c[(+θ)s] ):

Let x1 be the input.
Let x2 be the result of deleting the address θ from x1.
Output alg(r)(x2).
It can be easily checked that the self-editing step of c[(+θ)s] results
in alg(r)(c). The code r in this case will be called a temporary diﬀer-
entiating code.

It is clear that a temporary diﬀerentiating code may be invoked by

a self-editing algorithm:

If b = b[r] is a code for the calculation:

c[(+θ)∅] 7→ c[(+θ)s[r]] 7→ alg(r)(c),

then by the basic self-editing principle the self-editing step of c[b] would
result in the same calculation. We may abbreviate this step by saying
that c follows the instruction:

Use r as a temporary diﬀerentiating code.

7.3. Permanent decisions. Let us, as before, consider r and c to be
codes and θ an available address in c. We deﬁne s = s[r] to be a code for
the following algorithm (Again for the reader’s convenience the input
is thought to be c[(+θ)s]):
Let x1 be the input.
Let x2 be the result of activating r with input x1.
Activate the code in address θ of x2 and output the result.
One can easily check as before, that the self-editing step of c[(+θ)s]
results in a code c1 that is computed by activating the θ address of
alg(r)(c[(+θ)s]). Thus, assuming that alg(r) is θ-stable, c1 should be
of the form c1[(θ)s]. So by induction, the n-th step should result in
(alg(r))n(c[(+θ)s]) by repeating n times the application of alg(r) to
c[(+θ)s]. The code r in this case, will be called a permanent diﬀeren-
tiating code.

24

A. D. ARVANITAKIS

Again, it is easy to see, that exactly as in the case of temporary dif-
ferentiating codes, the application of a permanent one, may be invoked
by a self-editing algorithm itself. We are going to abbreviate such an
instruction by:

Use r as a permanent diﬀerentiating code.
Remark. Theoretically there is not much diﬀerence between a tem-
porary decision and a permanent one: The ﬁrst can be repeated con-
tinuously, while the second may be interrupted by a code of greater pri-
ority. There is much diﬀerence in practice though, since a continuously
used temporary diﬀerentiating code, costs the repeated complexity of
the decision to use it.

7.4. φ-diﬀerentiating decisions. Given a self-editing code c, an ad-
dress φ in c and a diﬀerentiating code r, a temporary (or permanent)
decision to diﬀerentiate φ by r, is the decision whose temporary (re-
spectively permanent) diﬀerentiating code is the code s = s[φ, r] of the
following algorithmic function:

c = c[(φ)y] 7→ c[(φ) alg(r)(y)],

which replaces the content y of φ, by alg(r)(y). The address of r in c
will be called the address of the diﬀerentiating code of φ.

8. Diagonalization

The combination of the complete memory version of the Basic self-
editing principle and the Memory lemma, allow us to consider a sim-
ple self-editing code that retains its memory as a self-editing code of
complete memory. Henceforth, to avoid unnecessary confusion, we are
going to study self-editing computations of complete memory, having
in mind that our models are ones that instead store it, so that we may
freely apply the complete memory version of the Basic self-editing prin-
ciple. As in the simple case, it would be helpful for the reader, to freely
translate this principle here, as follows:

Assume that by looking at the history c1, c2, . . . , cn of a self-editing
code, we decide to change the programming of cn in a particular way
that can be expressed as an algorithm. Then the same decision can
be potentially made by cn itself, assuming that its computation is self-
editing and the memory of its history has been stored.

The theory of diagonalization applies both to surviving and successful

sequences.

While a surviving sequence can result directly from natural (or ar-
tiﬁcial) selection applied upon a tree of self-editing codes, a successful
sequence results from an internal selection of successful computations.

RECURSION, EVOLUTION AND CONSCIOUS SELF

25

(In section 9, page 34 we examine how such an internal selection may
result as a consequence of diagonalizing over the surviving history of a
self-editing code).

Assume that we have the information of an entire successful or sur-
viving sequence and we ask ourselves what we would do if we were to
help the evolution of such a code and yet we knew nothing either about
the selection that is performed upon it, or about the true functionality
of its code or of parts of it. (Notice that this is about the same knowl-
edge that we assume that our code really has at the beginning). This
is certainly not an easy task, yet the question has an easy, simple and
natural answer: Since the successful or surviving sequence manifests by
itself what is acceptable by the environment, either as a reward or as a
selection, we should look for patterns in the code, aiming to perpetuate
them. Let us formalize this a bit more: Assuming that the sequence

c1 7→ c2 7→ · · · 7→ cn

is either a successful one, or a surviving branch of the computational
tree of a self-editing code, a pattern could be thought of as an algorithm
R, such that for every k < n, R(ck) ⊑ ck+1. Several examples should
clarify this concept. Suppose that we notice that a particular part of
the code remains the same. Assume moreover that the position of this
part is described by an address θ. This observation is equivalent for us
to note that the algorithm R deﬁned by

Copy the θ address of the input to the θ address of the output,

ﬁts the sequence, i.e.

for every k < n, R(ck) ⊑ ck+1. This way
we could advice or program the code to start applying it on purpose.
(Notice that the validness of this rule up to now upon the sequence,
could be either a product of selecting a successful sub-sequence, or a
product of selection). Therefore from now on, R will be applied on
purpose. It is easy to see that one could ﬁnd a lot of examples of this
kind. Here is one more:

Assume we notice that a part of the code contains a parameter in
the form of a natural number, that is continually augmented by, say 1.
If θ is the address of this part, this is equivalent for us to observe that
the algorithm R deﬁned by:

Add 1 to the θ address of the input and
write the result in the θ address of the output,

ﬁts the sequence.
Something useful to note here in order to grasp better the generality
of this recognition of patterns, is that our observation may as well be
conditional, for example

26

A. D. ARVANITAKIS

If it is true that R(ck) then compute ck+1 by S(ck),

where R and S are given algorithms.
This procedure, of our advice which is based on our observation, can

be roughly automated, using a pattern-recognition algorithm:

So, suppose that

c1[δ1], c2[δ2], . . . , cn[δn],

is an either surviving or successful sequence and let ∆ be a pattern-
recognition algorithm. Thus with the above sequence as an input, ∆
should be able to recognize, (and taking the appropriate decision, to
perpetuate), patterns of evolution of the sequence. At the same time,
the same procedure can fulﬁll as well, the requirements of a smoothly
changing environment, as they are probably imprinted in the structure
of the codes of the sequence. (Mental experiments 1 and 2, in page
28, are examples of this). The main problem here, is the complexity of
such an algorithm in order for it to function well. Our proposition, by
means of self-editing, is essentially a simple algorithm that can evolve
also itself: Indeed, assume that δn is a code for ∆, and δ1, . . . , δn the
history of its evolution. Assuming Darwinism, this sequence should be
of increasing eﬀectiveness. Using then the complete memory version
of the Basic self-editing principle and the Memory lemma, the next
self-editing computational step of cn[δn] should be the same as the
application of ∆ to the sequence. This means that ∆ = alg(δn) is
able to recognize not only evolutionary patterns in c1, . . . , cn, but also
in δ1, . . . , δn, i.e.
to itself. As mentioned in the Introduction, this
relatively simple approach has surprising consequences, which we are
about to study.

Notice that our main target at this point, (yet not the only one), is
to show that such an intelligent evolution could start almost without
any knowledge.

8.1. Diagonalizing algorithms. Sequential diagonalization is in fact
a very general deﬁnition of an algorithm that ﬁnds patterns in a se-
quence. We do not aim here to give a detailed description of such a
process.
Instead, the description concerns any such algorithm, even
not successful ones. The reason is that as it has been mentioned and
will be made clearer by the examples, such an algorithm combined with
the self-editing property, has the potential to evolve itself.

By the term decision system, we mean any algorithm ∆ with the

following general description:

∆ is used to make a decision D and it consists of two parts, the

searcher ∆s, and the tester ∆t.

RECURSION, EVOLUTION AND CONSCIOUS SELF

27

The searcher ∆s, is thought to propose codes r1, r2, . . . in a row. We
will refer to these codes as the proposed codes. The set of proposed
codes may be either ﬁnite, or inﬁnite. For the inﬁnite case, we assume
that the searcher is designed so as to construct r1, r2, . . . , by means
of an initial ﬁnite set of instructions and an also ﬁnite set of ways of
composing new codes out of old ones. There are general methods of
such a construction, the more simple and less eﬀective and sophisticated
being to output arbitrary strings on a given alphabet.

The priority of a proposed code is thought to determine its appear-
ance in the sequence r1, r2, . . . and we say that a code is simple if its
priority is high, i.e. if it appears early in the sequence.

The tester ∆t, is thought to use an algorithmic test T (i.e. an algo-
rithm that outputs true or false) in order to choose the simpler one of
the proposed by the searcher code r such that T (r) = true. We will call
T the testing algorithm of ∆t.

Clearly, there can be a lot of variations of the above deﬁnition. For
example, assuming that natural selection functions using a given algo-
rithmic test T, the system of a tree produced by a self-editing code and
the environment, demonstrates a very general such variation. Notice
that in this particular example, the proposed by the searcher sequence
has been replaced by a self-editing tree. Nonetheless, for reasons of
simplicity and for the moment, we will restrict ourselves to the case of
proposed sequences rather than trees.

Another variation of this deﬁnition that should be kept in mind, is
the case of a testing algorithm that outputs an evaluation instead of a
(simpliﬁed for our purposes) true-false answer.

Given now an algorithm S that inputs the history of a self-editing
code c and outputs a selected sub-sequence c1, c2, . . . , cn = c of it, we
may consider the following testing algorithm T :

Let c1, . . . , cn be the output of S. Choose the simpler code
r in the proposed by ∆s sequence r1, r2, . . . of codes, such
that r ﬁts the sequence c1, . . . , cn, i.e.

,

For all i < n,

alg(r)(ci) ⊑ ci+1

and use it as a temporary or permanent diﬀerentiating code.
(For the convergence problem of such a test, we refer to Example 3
of page 42). The term sequential diagonalization is used to describe the
functioning of a decision system ∆ = (∆s, ∆t), such that ∆t uses the
above test T. The sequence c1, . . . , cn will be called the testing sequence
of the diagonalization.

The following principle is essential for our purposes:

28

A. D. ARVANITAKIS

Sequential diagonalization principle (Simple form) Let ∆ be
a sequential diagonalizing algorithm with code δ = δn and c = c[δ] a
self-editing code. Assume that

c1[δ1], c2[δ2], . . . , cn[δn] = c[δ]

is the history of c. Then

self-ed(c[δ]) = ∆(c1[δ1], c2[δ2], . . . , cn[δn]).

Proof. It is an immediate consequence of the complete memory version
(cid:3)
of the Basic self-editing principle and the Memory lemma.

Notice that as it has been mentioned before, a diagonalization pro-
cedure in the frame of self-editing, may detect evolutionary patterns in
its own program and perpetuate them. This could be a pattern in the
sequence δ1, δ2, . . . , δn or in sub-codes of it.

It should be also noticed here, that ignoring the terms surviving and
successful, diagonalization is in agreement with Hebbian theory [37].
(For more details on this subject, see section 13, page 53).

Before proceeding to study it better, let us see it in action, by means
of some mental experiments.
In these experiments, we are going to
ask by a self-editing code to ﬁll the dots in given sequences. Due
to the lack of communication at this stage, instead of providing the
known parts of the sequences, we are going to assume that during the
self editing procedure they are somehow guessed no matter by which
method. Studying the mental experiments that follow, we ’ll refer to
this convention as the basic guessing convention and to such digits that
we ought to provide as not intended to be guessed digits. Another
one convention that we are going to make, is to assume that the set of
candidate diﬀerentiating codes is well formed, that is, simple candidate
diﬀerentiating codes for us, are hypothetically also simple for the self-
editing code. (In Example 2 of page 41, we will see that due to its
particular nature, diagonalization in the frame of self-editing, combined
with adequate experience, may serve to the purpose of learning to
detect useful codes and render them as simple).

Mental experiment 1. In this initial mental experiment we ask
by a self-editing code to ﬁll the dots in 0, 1, 2, .... Of course we are
going to assume as we have already done, an address to output to the
environment, called environmental output and denoted by θe.

Notice that it is necessary to provide the digits 0, 1, 2 in order to
guess the rest, therefore we may assume, due to the basic convention
done above, that in a sequence c0, c1, c2 the codes somehow guess the

RECURSION, EVOLUTION AND CONSCIOUS SELF

29

for i = 0, 1, 2, ci = ci[(θe)i]. Thus c0 should
corresponding digit, i.e.
write 0 in its environmental output address, and similarly c1 should
write 1 and c2 should write 2.

It is clear now, that the code add(1) that adds one on a given inte-
ger is a θe-diﬀerentiating decision that ﬁts the sequence c0, c1, c2. Thus
assuming that it is simple enough, it should be used as a temporary or
permanent θe-diﬀerentiating code. Therefore, if such a procedure as di-
agonalization is activated in c2 the self-editing step c2 7→ c3 should yield
c3 so that c3 = c3[(θe)3], that is c3 should output 3 to the environment.
Clearly, the procedure can be carried on by induction, either by
repeating diagonalization in the case of a temporary decision to use
add(1), as a θe-diﬀerentiating code, or by the decision to use the same
code permanently. Thus in either case, c3 should yield c4 = c4[(θe)4]
and so on.

Mental experiment 2. In this second experiment we aim to study
the behavior of a self-editing code using diagonalization, on the sub-
ject of guessing the dots in a somewhat more complicated structured
sequence, which is as follows:

(0, 1, 2, . . . ), (0, 2, 4, . . . , ), (0, 3, 6, . . . ), . . . .

That is, we ask not only to ﬁll the dots in the ﬁrst three sub-experiments,
but also to guess entirely the fourth sub-experiment (as we certainly
can do).

Guessing of the ﬁrst three sub-experiments can be clearly accom-
plished the same way as mental experiment 1. An exception to this
is the case of the closing right parentheses ), yet this is not a digit
expected to be guessed. (Notice that we couldn’t guess this digit also,
since there is no indication where we should stop the successful guess-
ing of the sequences of the sub-experiments). Thus right parentheses
should follow the rule of the basic convention.

Notice now that right parentheses of the sub-experiments mark their

ending, Thus for example,

Start every new sub-experiment with a left parenthesis fol-
lowed by a 0,
can be regarded of as an alternative way of saying:
If a left parenthesis is written in the address θe, then replace
it in the next step with a right parenthesis and if a right
parenthesis is written in θe then replace it in the next step
with 0.
Clearly if r is a code for such a statement, then r should ﬁt the

memory sequence, since it has been followed up to now.

30

A. D. ARVANITAKIS

Now notice that for i = 0, 1, 2, during the i-th sub-experiment the
address (let us call it simply θ) of the θe-diﬀerentiating code contains
the code add(i) respectively. Thus if θ0 is the (relative) address of i in
add(i), then

In every new sub-experiment add 1 in the address θ⌢θ0,
should ﬁt the memory sequence as well, so that if r is a simple
enough code for it, by diagonalization it should be decided to be used
as a diﬀerentiating code. Therefore a self-editing code should be able
to guess that in the 4-th sub-experiment the number-digits start with
0 and are augmented by 4, thus being able to guess the entire sequence
(0, 4, 8, . . . ).

Remarks: 1. It is clear that we may continue this line of experi-
ments by asking a self-editing code to guess a diﬀerentiating code for
the diﬀerentiating code for environmental output, and so on.
It is
interesting to mention here that the mere decision of creating a dif-
ferentiating address for an existing one can be also a subject to be
established by diagonalization.

2. The environmental address θe is not an essential choice for the ﬂow
of the experiments and could be replaced by any address of the code. In
fact both experiments, may be regarded as a convenient way to estab-
lish that a self-editing code should be able to improve its parameters
based on an initial judgment about success. For a both simpliﬁed and
interesting example of this, assume that θ is the address of the priority
weight w of a possible candidate diﬀerentiating code r. A prejudged as
successful sequence

c[(θ)w], c[(θ)w + 1], c[(θ)w + 2]

indicates that the weight should probably be augmented further and
this is indeed the action that would be taken by diagonalization. It
naturally remains open the problem of homeostasis, i.e. the problem
of at which value an optimal (or locally optimal) is attained. This is
going to be addressed later on, in Example 4, page 43.

Notice that the interest of the above remark, lies exactly onto that,
assuming self-editing computations, diagonalization can practically be
used to improve its own behavior (by regulating its own parameters), as
suggested somewhat theoretically earlier. For example it can be used
to regulate the length of the testing sequence that is appropriate for a
speciﬁc kind of decision.

3. The potential ability of a self-editing code to decide the action to
be taken in relevance to the length of memory that a given proposed
code ﬁts, can be seen also in a diﬀerent manner, as suggested below:

RECURSION, EVOLUTION AND CONSCIOUS SELF

31

Let D(r) be any decision that concerns the proposed by the searcher
code r and d be a code for D. Then a self-editing code may randomly
choose to apply D(r) by activating d[r], according to the basic self-
editing principle. Assume also that for such a decision to be successful,
it is required that r ﬁts at least kd of recent memory length. Thus in
most cases of a surviving sequence (or successful subsequence, as we
’ll see later) c1, . . . , cn, it should be true that the decision D(r) (which
was made randomly) has been taken under the condition that r should
ﬁt in at least kd length of recent memory. (We assume here that the
probability of applying a correct decision D(r) without the condition
of r ﬁtting at least kd steps of recent memory is negligible). Therefore
a code for

Activate d[r] whenever the proposed code r ﬁts at least kd
steps of recent memory
should itself ﬁt the sequence c1, . . . , cn, so that under the premise of

being simple enough, it could be established by diagonalization.

4. If we assume that a self-editing code is equipped with an internal
representation of the environment, the above mental experiments sug-
gest that it is possible to predict by diagonalization (in a similar way
to that we actually do) smooth environmental changes. An obvious
example of this, regards the concept of velocity: Mental experiment 2
in this case, can be used as a basis to establish that acceleration may
also be detectable.

8.2. Other forms of diagonalization.

8.2.1. Statistical form. Let us consider again the case, in which by see-
ing a surviving or successful sequence c1, . . . , cn and without knowing
nothing about its environment, we wish to help it by remarking pat-
terns that are probably necessary to be perpetuated. The ﬁrst kind of
diagonalization procedure we wish to introduce here, has to do with
observing that a simple algorithmic instruction R, may ﬁt a percentage
(and not all) of the transitions ci 7→ ci+1, for i < n. In this case, and in a
need to help the self-editing code cn, we could reprogram or advise it to
use R with the same relative frequency as we see it in the all sequence.
This advice may concern either the calculation of its descendants, in
the case of proliferating transitions, or the probability of using a certain
diﬀerentiating code r (which should be a code for R), in the case of
non-proliferating ones. Say for example that we notice that in the se-
quence c1, . . . , cn, speciﬁc sub-codes are copied in half of the transitions
ci 7→ ci+1, i < n. Assuming that this remark is not seemingly related
to a noticeable condition that is apparent in the rest of the code, we

32

A. D. ARVANITAKIS

may deduce that it is advisable that these speciﬁc sub-codes should be
copied in half of the descendants that are computed, or with probabil-
ity 1/2. This statistical behavioral strategy, gives us by means of the
complete memory version of the basic self-editing principle, combined
with the memory lemma, a statistical version of diagonalization.

Indeed, given a searcher ∆s, we may alter the functioning of the
tester, to check the relative frequency that a given proposed code r,
ﬁts the transitions ci
7→ ci+1, i < n, of a given sequence c1, . . . , cn,
and take the decision to use r with the same relative frequency as a
diﬀerentiating code. If δ is a code of the overall procedure and is acti-
vated in cn = cn[δ], then by the self-editing principle and the memory
lemma, cn can perform the same algorithm and take the same decision.
One should notice here that as in the case of the simple form of diag-
onalization, decisions can be made either to use r as a permanent or
temporary diﬀerentiating code, or as a φ-diﬀerentiating code for some
particular address φ. Again, the length of the recent memory that r ﬁts
with a given relative frequency, plays an important role to establish it
as a permanent or temporary possible behavior. The most crucial ex-
amples of the application of this procedure, are for the ﬁrst part the
establishment of a variety of diﬀerentiating codes that are used to com-
pute descendants in a proliferating step, yet also the probability that
is used for a non-proliferating self-editing code, to calculate its next
state. This last has also to do with the priorities of the proposed by
the searcher code. The case will be examined in more detail, later on.
It is worth to notice here, that the statistical version of diagonaliza-
tion described above, includes the simple form that we have already
seen. Moreover, it is clearly more useful in a practical way. The inter-
esting fact that holds also true, is that the statistical form (as we are
going to see) can be established by the simple form. Roughly speak-
ing, a self-editing code using the simple form, may notice in its history
that it successfully establishes the use of diﬀerentiating codes, with the
same relative frequency that are used in its history. Of course such
an observation to be valid, requires the choice of a surviving sequence
of computations (which can be performed by the environment) or of a
successful sub-sequence, which should have been selected by the self-
editing code itself, as we are going to see below.

8.2.2. Parallel diagonalization. Let us return once again to consider the
case in which we would like to program or advice a self-editing code
c = cn, by trying to ﬁnd patterns in its surviving history c1, . . . , cn.
Let us for the moment assume, that to every code there correspond

RECURSION, EVOLUTION AND CONSCIOUS SELF

33

two addresses θi and θo in which environmental input and environmen-
tal output are registered respectively. Suppose also that the demand
of the environment is a simple enough relation R, (so that it can be
observed by us) between the input and the output of a code and that
it remains the same during our
this relation is time independent, i.e.
experiment. Due to the assumption that c1, . . . , cn is a surviving se-
quence, we deduce that for every k < n, the content of ck in its input θi
is related with the content of θo in ck+1 with this same relation R. I.e.
denoting by c ↾ θ the value of the content of c in address θ we should
have that

R(ck ↾ θi) = ck+1 ↾ θo,
for every k < n. Thus, it is evident that if r is a code for the algorithm
described as:

Output to the environment the code that results by com-
puting R(i), where i is the code of the environmental input,
ﬁts the sequence c1, . . . , cn, so that it can be guessed as a strategy
in case that cn uses diagonalization. The problem we would like to
address here, is that the structure of the sequence c1, . . . , cn does not
indicate the nature of the problem. It should be much more natural to
ask to ﬁnd a code r that ﬁts the set of transitions

{ck 7→ ck+1 : k < n} = {c1 7→ c2, c2 7→ c3, . . . , cn−1 7→ cn},

i.e. to ﬁnd a code r such that for all k < n, alg(r)(ck) ⊑ ck+1. Of course,
in the particular example, the testing algorithm is equivalent to the one
that results from the sequence c1, . . . , cn, although this is not true for
all cases. We will call a diagonalization of the above form, i.e. one that
uses a set rather than a sequence in the place of the testing algorithm,
a parallel diagonalization. Intuitively, whereas sequential diagonaliza-
tion is useful both to detect evolutionary steps of the same kind in an
evolving self-editing code or on the other hand a smoothly changing
environment, where the order of a sequence plays an important role,
parallel diagonalization detects a relation on a set of transitions.

An important example of parallel diagonalization is free-ﬂoating DNA
that is able to communicate genes involved in antibiotic resistance, see
for example [14].

Communication between humans is a similar example.
Notice that in both examples above, there is actually no sequence
to consider. Moreover the code generator is non-existent and has been
replaced by one or more atoms of a certain population (the ones that
transfer the particular message).

Let us now consider the case of a set {s1, . . . , sn} of sub-codes of a
given self-editing code c. It is clear that parallel diagonalization over

34

A. D. ARVANITAKIS

the development of the set {s1, . . . , sn} can create common features of
the sub-codes s1, . . . , sn. That is, permanent decisions that concern all
of the codes in common can be considered as a common extension of all
of the codes. This observation can be used to explain a lot of phenom-
ena of abstraction. To see this, assume moreover that the self-editing
code is equipped with an internal representation of the environment.
In this case, parallel diagonalization over the environmental objects
s1, . . . , sn can reveal their common nature as an abstraction.
If for
example s1, . . . , sn are speciﬁc instances of dogs, created by the ex-
perience of interacting with actual corresponding dogs, then parallel
diagonalization over this set, can potentially create the abstract notion
of a dog. Similarly, in the case that s1, . . . , sn are couples of various
objects, a parallel diagonalization can potentially create the abstract
notion of the number 2, and so on.

9. Surviving sequences and successful sub-sequences.

In the aforementioned mental experiments, let us consider the case
where the answers for the not intended to be guessed digits, are given
by means of proliferating steps in which at least one descendant gives
the correct answer.

During the ﬁrst mental experiment, since we have considered the ﬁrst
three digits 0, 1 and 2 as not intended to be guessed, we may assume
by the basic guessing convention that their correct guessing, results
by mere chance on some of the immediate descendants of the previous
code. That is, we assume that we begin with a code c0 = c0[(θe)0] that
guesses the correct number 0 to be output just by chance, and we ’ll also
assume that there are c1 in the immediate successors of c0 and c2 in the
immediate successors of c1 such that again by chance guess the correct
number to output. That is, c1 = c1[(θe)1], and c2 = c2[(θe)2]. It is
clear now, that diagonalization should instruct c2 to follow the decision
to compute all its immediate descendants by using add(1) as a θe-
diﬀerentiating code, therefore computing all its immediate descendants
so as to give the correct answer. In such a case this should be regarded
as a successful outcome to our mental experiment.

The situation becomes a bit more complicated, assuming that such
environmental demand from the code, as to output the sequence 0, 1, 2, . . .
is only local. (Notice that this actually happens in the second mental
experiment). This is certainly so, since at the moment that this de-
mand stops to incur, the decision to use add(1) as a θe-diﬀerentiating
code for all descendants would immediately result in the surviving of
none of them. (All of them would reply wrongly to the environment).

RECURSION, EVOLUTION AND CONSCIOUS SELF

35

Thus in this case the understanding of the local nature of such a re-
quirement should result in a more conservatory decision of applying
the corresponding diﬀerentiating code, as for example:

Augment the percentage of immediate descendants that
are computed by using add(1) as a θe-diﬀerentiating code.

Notice here that the exact augmentation of such an instruction, being
a parameter of the system, may also be the result of diagonalization as
suggested by remark 2 above.

Such conservatory behavior is certainly in this case, a much better
strategy when it comes to a surviving related problem. Clearly, a local
demand can be distinguished from a non-local one, by the length of the
time past that exists and therefore by the length of the recent part of
the surviving sequence that is forced by selection to be in accordance
with the demand. On the other hand, since this conservative strategy
is a better one, assuming that in a surviving sequence all have been
done well by chance, it would be expected for this strategy to be kept,
exactly in the cases where the chosen diﬀerentiating code ﬁts a short
part of the sequence. Therefore a code for

If the chosen diﬀerentiating code ﬁts a short part of the
recent sequence in memory, then apply it conservatively,
should be a code that ﬁts the entire surviving sequence, so that it

can be established by diagonalization as well.

This conservative behavior which is related to temporal aspects of the
environment, may as well explain the problem of variation of species.
(See section 13 and [51] for more details on the subject).

On the other hand, a diﬀerentiating code that ﬁts the entire sur-
viving sequence, as the preceding one, should be regarded as a better
candidate to be applied permanently and in all (or almost all) descen-
dants. Again, as one can easily see, this should be apparent as well in
the history of a code.

An interesting example of this long-term memory ﬁtting, is the case
where the history of the surviving predecessors of a code c for a long
period indicates that they remain the same, i.e. there is a suﬃciently
large positive integer n such that the most recent n terms of its history
c1, c2, . . . , cn are the same as c. (That is for every i ≤ n, ci = c). In
this case, assuming that cn = c decides its successors using diagonal-
ization, it should be expected that its decision would be to copy itself
to (probably almost) everyone of them.

Clearly this remark holds also true assuming that parts (instead of
all) of the code remain the same for an extended period in its memory.
This function can clearly lead to a stable program of a given self-editing

36

A. D. ARVANITAKIS

code, in the sense that it could learn this way to inherit useful parts
(judged by selection itself) to all of its descendants.

Finally, let us observe here, that the decision of a long-term memory
ﬁtting of a diﬀerentiating code is not necessary to use a really long-term
memory, since a code may delete its memory and at the same time store
the information that a given diﬀerentiating code ﬁts the memory so far,
as a simple evaluation regarding the diﬀerentiating code.

Notice that the previous observations apply equally well to a suc-
cessful sub-sequence of a non-proliferating self-editing computation. In
fact, it is interesting to observe the relation between long-term mem-
ory ﬁtting of a diﬀerentiating code to the diﬃculty that is presented
by human beings to alter a prolonged habit.

9.1. Diagonalization over successful sub-sequences. The prob-
lem we would like to address here, is whether it can be learned by a
self-editing code, to apply diagonalization to successful sub-sequences.
Of course, the candidate method for learning such a process would be
diagonalization itself.

Let us consider δ′ = δ[(+θt)x1, . . . , xm] to be a code for a diago-
nalizing algorithm with x1, . . . , xm being the testing sequence.
I.e.,
alg(δ′) is designed so as to ﬁnd the simpler code r that ﬁts the se-
quence x1, . . . , xm.

Notice now, that if c1, . . . , cn = cn[(θ)δ′] is the history of the code cn

and k ≤ n, then the function

c1, . . . , ck, . . . , cn = cn[(θ⌢θt)x1, . . . , xm] 7→ cn[(θ⌢θt)x1, . . . , xm, ck]

that appends ck in address θ⌢θt of cn where the testing sequence (i.e.
the sequence to be diagonalized) is situated, is clearly algorithmic, so
that by the basic self-editing principle, it can be triggered from within
cn. Therefore a self-editing code may decide about the construction of
the testing sequence over which diagonalization occurs.

Assume now that

c1 7→ c1

1 7→ · · · 7→ cn1
1

is a self-editing computation without proliferating steps. Let us more-
over assume that cn1
1 undertakes a proliferating step by giving as imme-
diate descendants variations of c1 and let c2 be one of them. As before,
let

c2 7→ c1

2 7→ · · · 7→ cn2
2

be the self-editing computation that starts with c2 without prolifer-
ating steps. Continuing in this manner, we may come to a surviving

RECURSION, EVOLUTION AND CONSCIOUS SELF

37

sequence c1, c2, . . . , ck, where for every i ≤ k, ci produces a self-editing
computation

ci 7→ c1

i 7→ · · · 7→ cni
i .

We may call the sequence

(4)

(c1, . . . , cn1

1 ), (c2, . . . , cn2

2 ), . . . , (ck, . . . , cnk
k )

a sequence of cycles or a composite sequence. The important assump-
tions we make are

(i) For every i ≤ k, the computation ci, c1

i , . . . , cni

i has no prolifer-

ating steps.

(ii) For every i < k, the step cni
i

7→ ci+1 is a cycle, i.e. ci+1 is

computed by cni

i as a variation of ci instead of itself.

(iii) For every i < k, the self-editing step that cni

i undertakes is a

proliferating one, thus c1, c2, . . . , ck is a surviving sequence.

Later on, we are going to omit assumption (iii), and replace it with
the demand that c1, c2, . . . , ck is instead successful. The idea is that
composite sequences serve as a kind of a general scheme to be used
for recursion. On the basis of this scheme lies survivability. What we
intend to explore here is that survivability may serve on the basis of
this recursive hierarchy to characterize success.

Composite sequences are very useful towards the purpose of ﬁnding
out the necessary ingredients of a successful computation. Indeed, since
c1, . . . , ck is a surviving sequence, one can deduce that computations
made in between ci and ci+1, i < k, that is, every computation
ci, c1

i , . . . , cni

i

should be successful. So, again, any pattern recognized in these com-
putations should be perpetuated.

In a composite sequence, such as (4), we may distinguish the self-
editing steps in two categories: The certain ones which is thought to
be all the steps cni
i

7→ ci+1, for all i < k.

The uncertain steps which are the steps cm
i

7→ cm+1
i

for all i ≤ k and

m < ni.

The terms certain and uncertain are used here to express exactly
that any cycle in (4) is a product of selection while any uncertain step
is not.

Since now the sequence c1, . . . , ck is a product of selection, we may
assume that any sequential diagonalization that takes place during the
self-editing computation of a certain step uses as a test sequence the
all history of the code.

38

A. D. ARVANITAKIS

The conventions we make about uncertain steps, are that failed un-
certain self-editing steps are selection forgiven, yet failed diagonaliza-
tion during an uncertain step is not. This consideration roughly mirrors
the belief that while false individual answers to the environment, should
be thought of as necessary in a learning procedure, false conclusions
about the environment (which result from a failed sequential diagonal-
ization) may lead to massive failed answers and therefore should be
expected to be condemned by the environment.

Because of this consideration, since the i-th cycle ci, . . . cni
i

is ex-
pected to contain failed answers, which are not supposed to be con-
tained in a sequential diagonalization process, the testing sequence of
such a diagonalization during an uncertain step cm
should not
i
contain the sequence ci, . . . , cm
itself but rather the sub-sequence of it
i
that contains the successful answers.

7→ cm+1
i

Let us now take for granted that there exists a testing algorithmic
procedure T (i.e. one that outputs true or false), such that T (c) =
true, if and only if c should be included in the testing sequence of
a diagonalization during an uncertain step. Given the possibility of
choosing the correct codes to be included in this testing sequence, it is
likely that this should have happened in a surviving sequence of codes
as c1, . . . , ck. Therefore, it should be expected that if r is a code for

Let x be the input.

If T (x) = true, then append x to the testing sequence,

i ) = true, then cj
i )) should be included in the testing sequence of cj+1

should ﬁt (at least statistically) the sequence (4), that is, if for some
i ≤ k, and j < ni, T (cj
i (which is in fact the input of
alg(cj
. As explained
above, this is so, assuming that successful diagonalization should lead
to a code with greater ability to survive. Therefore, the use of r may
be established by diagonalization that occurs in a certain step. Notice
that this is also a concrete example of the ability of diagonalization to
evolve itself in the frame of self-editing.

i

i , . . . , cni

At this point, we can make a step further, by replacing the surviving
sequence c1, . . . , ck in (4), with a successful one. Assume for example
that for i ≤ k, ci, c1
is a computation of the diagonalizing unit,
resulting in a successful answer to the environment which is registered
exactly at the environmental output of cni
i . In this case, diagonaliza-
tion over the entire sequence (4) may lead to a better functioning of
diagonalization itself. Generally speaking, diagonalization to compos-
ite sequences such as (4) is in accordance to whatever we experience as

i

RECURSION, EVOLUTION AND CONSCIOUS SELF

39

a procedure of learning. Indeed, such a procedure should have as neces-
sary components the notions of success and failure as well as repetitive
tasks. Let us see some concrete examples of this procedure in action:

Example 1. Consider the 1st mental experiment, where a self-

editing code has to guess the dots in the sequence

0, 1, 2, . . . .

In this case, we will assume that in non intended to be guessed digits,
the algorithm takes guesses in chance, just as we would do during the
same probation. We will make use of the basic assumption here, in the
form that we ’ll take as granted that the non intended to be guessed
digits, are indeed eventually found. Assuming an input address, let
us call it envronmental input, and a 0-1 feedback in this same address
that encodes for failure or success of the answer respectively (that is,
0 for a failed answer and 1 for a successful one), we may replace the
testing algorithmic procedure T such that T (c) is true if and only if 1
is registered in the environmental input of c. By the discussion above,
we may begin with a code c0 with a stable diagonalizing functioning,
so as to append a code c′ from its history to the sequence to be diago-
nalized in all cases that 1 is registered to the environmental input of c′.
i , . . . , cni
Assuming that ci, c1
i are the attempts to guess the ith digit, by
i [(θe)i], that is cni
i = cni
the basic assumption, we may ensure that cni
i
has managed after some attempts to guess the correct digit (which is
exactly i in this case). Therefore diagonalization now should take place
over the sequence cn0
2 , since these are the codes that take the
feedback 1 in their environmental input. This is indeed the correct se-
quence for diagonalization to infer that the code must proceed adding
1 in the address of environmental output, guessing thus successfully
the rest of the digits.

0 , cn1

1 , cn2

It is easy to see that this same line of thoughts would lead in success

in the second mental experiment as well.

10. Diagonal generalizations.

At this point, we are interested to study the application of diagonal-
ization for the establishment of a code with a variable sub-code. A very
often occurring such example, concerns the potential ability to apply
as a decision code, a code of the form s[r] whenever r meets speciﬁc
requirements. We assume that these requirements consist of a testing
algorithm T (i.e. one that outputs only true or false).

40

A. D. ARVANITAKIS

Consider the following composite sequence as an either surviving or

successful one:

(5)

(c1, c1

1, . . . , cn1

1 ), (c2, c2

2, . . . , cn2

2 ), . . . , (ck, c1

k, . . . , cnk
k )

Let us assume a code t = t[∅] for T and a code s = s[∅], for the
decision to be taken. In this case, we regard the empty addresses in
t and s respectively, to be exactly the addresses to be ﬁlled with the
variable code r. Let us assume further that alg(t) may depend on a sec-
ond argument as well, being either an external condition or an internal
one. (The case that this doesn’t happen is a trivial one, in which either
t[r] is always true or always false). Since the general case we consider
is that this condition is encoded as a sub-code of the self-editing code
itself, either as an environmental input in its memory (in the case of
the condition being external) or not (in the case that the condition
is internal), we may assume that t accepts the self-editing code in a
second address, that is we assume that t is of the form t[∅, ∅]. Under
this terminology, T (r) is true in an instant of (5) that is determined
by an i ≤ k and a j < ni, if and only if alg(t[r, cj

i ]) = true.

Our basic assumption is that the application of the algorithmic in-

struction (let us call it M):

If for some r, alg(t)(r, c) is true, then take the decision alg(s)(r), for

this same r

is a necessary condition for either surviving or succeeding. Therefore,
since (5) is indeed either surviving or succeeding respectively, we deduce
that whenever for some i ≤ k and j < ni, alg(t)(r, cj
i ) is true, then
alg(s)(r) ﬁts (obviously by chance up to now), the computational step
i 7→ cj+1
cj
. Thus, if m is a code for M, we infer that m ﬁts cj
for
all i ≤ k and j < ni. Consequently, by diagonalization, the application
of m may be established to every computational step thereafter.

i 7→ cj+1

i

i

Remarks. 1. The above analysis has been done on the premise that
m is an adequately simple code related to the sequence (5). Generally,
even a not so simple code, may be found to be the simpler one ﬁt-
ting a long and varied experience. This last observation is adequately
backed up by Probability theory. On the other hand, the simpleness
of m relies on the simpleness of t and s. Regarding this point of view,
diagonalization can be used as well in this case to render as simple,
codes that have been proved useful by experience. (For a simple case
of this procedure, see Example 2 below).

2. Obviously, the same reasoning can be used to establish diagonal

generalizations for more than one code-variables, (in addition to r).

RECURSION, EVOLUTION AND CONSCIOUS SELF

41

Example 2. Up to now we have assumed that the priorities of the
set of proposed codes in a diagonalization process is ﬁxed. However, if
δs is the searcher’s code, there are a lot of algorithmic ways to change
the priority of a given code r, the simplest one probably being to add
a corresponding instruction s[r] to δs:

B(r) : δs 7→ δs[(+)s[r]],

where s[r] is a code for

Output r with priority n.

More complex (yet also more interesting) such algorithmic changes
of the priority of a code to a searcher, occur, assuming that codes are
constructed by the searcher by composing elementary instructions and
have to do in this case with altering conditional probabilities of the
composition process inside δs itself. We are not going for the moment
to deal with such complex situations, yet generally if B(r) is such
an algorithm for altering the priority of a code r in a searcher with
code δs, since δs should be a sub-code of a self-editing code that uses
diagonalization, it follows easily from the basic self-editing principle,
that a self-editing code may undertake the decision to alter the priority
of proposing a diﬀerentiating code.

Let now x 7→ x′ be a self-editing step in which x has decided ran-
domly to raise the priority of r as a proposed by the searcher code
in the diagonalization process. Assume moreover that this decision is
successful. If

{(x1, x1

1, . . . , xn1

1 ), (x2, x1

2, . . . , xn2

2 ), . . . , (xk, x1

k, . . . , xnk

k )}

is the set of the computations of the diagonalization unit in the history
of x, it is expected that the relative frequency of these cases that r
has been accepted by the testing sequence as a ﬁtting code, exceeds
the relative priority of r being proposed by the searcher. This should
be correct, since raising the priority of r as a proposed code has been
considered successful.
7→ c′
Therefore if ci

i, i = 1, . . . , m are successful self-editing steps
in the history of a self-editing code c, in which the priorities of the
proposed codes ri, i = 1, . . . , m are respectively raised, then for every
i = 1, . . . , m the step ci
i falls in the category of the previous
observation. Therefore a code for the algorithm

7→ c′

If the relative frequency of the acceptance of any code
r during the diagonalization processes, is greater than its
relative priority to be proposed, then raise this same priority

42

A. D. ARVANITAKIS

should ﬁt an either surviving or successful sub-sequence of the history
of c. Hence, such an instruction can be established. Therefore a self-
editing code can learn by diagonalization to render as simple often
useful proposed codes. Notice that this is also a concrete example of
how diagonalization can be used in the frame of self-editing to improve
itself.

Notice that the stratagem described above as well as the statisti-
cal form of diagonalization is roughly reminiscent of the multiplicative
weights update algorithm (MWUA), an eﬃcient optimization algorithm
that has been discovered many times in computer science, statistics,
and economics [7].

Example 3. The question whether a given code ﬁts a sequence is
in fact a non decidable problem, since it has been proved by A. Turing
in his famous Halting problem [9], that given two codes r and c, there
is no algorithmic way to decide whether alg(r)(c) eventually stops and
gives an answer. (It may run for ever as well). In spite that fact, in our
every day lives, we may conclude that alg(r)(c) is not going to stop,
by simply waiting enough time to do so. Similar decisions are possible
for a self-editing code that uses diagonalization: Let
2 ), . . . , (ck, c1

1, . . . , cn1), (c2, c1

2, . . . , cn2

(c1, c1

k, . . . , cnk
k )

i

i , . . . , cni

be a composite sequence such that c1, . . . , ck is either a surviving or a
successful sub-sequence. Assuming now that for every i = 1, 2, . . . , k,
ci, c1
contains the calculations of the tester to see whether a
given proposed code ﬁts or not the testing sequence, we can deduce
that there should be an upper limit of calculation steps (and therefore
of time) that are performed in every such calculation. This is indeed
so, since above this limit c1, . . . , ck wouldn’t be surviving or successful
respectively. Thus a code for

For every proposed code r and every code c in memory,
wait at most n steps to calculate alg(r)(c),
should ﬁt the sequence and therefore could be established in the

process of diagonalization.

Remark. It is interesting to notice here that assuming an internal
representation of the environment, the same procedure as above can
be used to establish non-decidable truths by experience, (that is by di-
agonalization). Notice also that the procedure to establish such truths
by experience, is very common in our functioning as human beings. Of
course such a method of establishing truths does not contain certainty
and for this reason we also use logical deduction, something that is
much more complicated in nature to examine here. One should bear

RECURSION, EVOLUTION AND CONSCIOUS SELF

43

in mind though, that logical deductions require axioms and axioms are
not subject of proofs, but solely of experience.

Example 4. Homeostasis. It is evident that no parameter in the
evolution of a self-editing code, can be successfully being repeatedly
increased to inﬁnity. Even if there is some possible beneﬁt of doing
so, this should be at some point counteracted by the cost of retaining
a suﬃciently large number for this parameter, in whatever form it is.
Thus, a surviving or successful sequence of self-editing codes, should
have found at least locally optimal values to a lot of parameters. Con-
sidering a sequence

k ),

(c1, c1

1, . . . , cn1

k, . . . , cnk

1 ), (c2, c1

2 ), . . . , (ck, c1

2, . . . , cn2
of such successful outcomes, diagonalization upon the sequence could
provide strategies for achieving such an optimal value. In the case of a
surviving sequence, conservative strategies (for example to generate a
sole descendant of the same value at each step), combined with diago-
nalization, (since in the case of a locally optimal value, this same value
should remain constant in recent memory), provide such a strategy.
Such a conservative strategy can be also used in a successful sequence,
where the role of selection in this case may be played by either internal
or external reward. In fact we are aware of such a strategy, in the cases
where emphasizing gradually a particular characteristic of our person-
ality, can reach a point that this becomes inappropriate. The usual
reaction to such a feeling, is simply to stop emphasizing it more.

11. Learning specialization.

Let us consider the problem of ﬁnding a code r that ﬁts a particular
test T = alg(t), in order to use it in a decision with code d[∅]. (Notice
that the above consideration describes in an abstract form the problem
of applying diagonalization). Assume now that c is a self-editing code
that uses a decision algorithm with code δ[t′] = [δs, δt[t′]] where t′ is
the code of its testing algorithm. Assume moreover that the code d[∅]
of the decision to be made lies in some address of c = c[d[(θ)∅]] so that
θ is the (absolute relatively to c) address of the argument of d[∅] and
let φ be an available address of c. The situation can be summarized by
denoting

c = c[δ[t′], (θ)∅, (+φ)∅].
Using φ as a θ-diﬀerentiating address, one can use the decision code
δ[t] = [δs, δt[t]], (notice that we have replaced the appearance of t′ with
t), as a θ-diﬀerentiating code by simply copying it to φ. The idea here is
that as we saw in Example 2, page 41, the searcher code δs is expected

44

A. D. ARVANITAKIS

to be favorably evolved and thus in this case should be able to be
specialized as a searcher of θ-diﬀerentiating codes. Given the code t,
the above process can be described as the function

c[δ[t′], (θ)∅, (+φ)∅] 7→ c[δ[t′], (θ)∅, (+φ)δ[t]],

which is clearly algorithmic, so that by the basic self-editing principle,
the decision of using it can be made by alg(c) itself. Hereafter we are
going to abbreviate such a decision as

Find a code r such that ...

and we will refer to it as a θ-specializing instruction.

Given now an address θ in an either surviving or successful sequence
c1, . . . , cm of self-editing codes, and assuming that δ[t′] is a code of
a diagonalizing algorithm, it is possible that accepted by the tester
proposed codes that are used as θ-diﬀerentiating in c1, . . . , cm, may
be in disagreement with the initial priorities of the proposed by the
searcher δs codes. This could be proved a very plausible condition that
should trigger a θ-specializing instruction. For the remaining of this
remark, let us refer to this condition as the rate disagreement condition
for θ.

Assuming that the information of the priorities of the proposed codes
can be detectable in the code of the searcher δs and since the rates of
the acceptance of the proposed codes can be detected in the calculation
ci, c1
i of the diagonalizing procedure for every i ≤ m, a rate
disagreement condition for θ, may be checked in the composite sequence

i , . . . , cni

(6)

(c1, c1

1, . . . , cn1

1 ), (c2, c1

2, . . . , cn2

2 ), . . . , (cm, c1

m, . . . , cnm

m ),

so that a θ-specializing instruction can be triggered when necessary
by a self-editing code that uses rate disagreement conditions to trigger
θ-specializing instructions. One should notice that assuming enough
experience, diagonalization can suggest such a behavior:

Indeed, assume that c1, c2, . . . , cm is an either surviving or successful
sequence of a self-editing computation where specializing instructions
have been decided randomly. Since in this particular sequence, these
instructions would be successfully taken and the rate disagreement con-
dition can be checked by a self-editing algorithm, it follows that a code
for

If the rate disagreement condition holds for an address θ,
then decide a θ-specialization,

should ﬁt the sequence, so that a self-editing algorithm may establish

it and thus learn to use it by diagonalization.

RECURSION, EVOLUTION AND CONSCIOUS SELF

45

Remarks. 1. It should be interesting to notice here, that specializa-
tion may as well occur in sub-addresses of the searcher δs of a decision
unit. The interest of this observation lies onto the fact that repeated
such specializations can result in an hierarchy of decision units. The
formation of such an hierarchy may follow the diagonalization principle
as well, something that as we will see in subsection 12.3 of page 47 can
be used of as a way to accomplish abstractions about the environment.
2. In the case of a θ-specializing instruction of a diagonalization unit,
as we have seen, the testing algorithm of the tester, may be a subject
of diagonalization, so that it can be formed by using it. Nevertheless,
it is obvious that the testing algorithm in this case, should be replaced
by the testing algorithm that checks the ﬁtting of the proposed codes
to sequences of successful values of θ.

3. It is evident that the statistical form of diagonalization generalizes
the simple form. On the other hand it may be deduced as a strategy
by the simple form as follows:

Let us assume a self-editing code which in its history has ﬁxed suc-
cessfully the weights of various diﬀerentiating codes that it uses. For
any such ﬁxed weight w of a diﬀerentiating code r, it is expected that
w should match the relative frequency that r ﬁts the sequence in its
history, so that a code for

Find codes r that ﬁt the history sequence with a positive
relative frequency w and use them as diﬀerentiating codes
with the same as w weight,
should ﬁt an either surviving sequence or successful subsequence of
its history. Notice now that this same code performs essentially diag-
onalization in its statistical form.

12. Hints for further research.

12.1. Language. Up to now, we have used combined sequences of the
form

(c1, c1

1, . . . , cn1

1 ), (c2, c1

2, . . . , cn2
in order to assign values of success and failure in steps in internal cycles,
assuming that there is such an assignment to the external ones either
by the environment itself or by recursion.

2 ), . . . , (ck, c1

k, . . . , cnk
k )

However, one can use combined sequences as well, to assign success
and failure in longer time scope, based in the assumption that such
notions (success and failure) have been assigned in shorter time scope.
Naturally, shorter time scope is represented here by the internal cycles
and long time scope by the external ones. This is reminiscent of two
situations which are not completely irrelevant between them.

46

A. D. ARVANITAKIS

The ﬁrst one, is the way we learn in a completely new environment.
This is particularly evident in the case of children whose memory is
of limited time scope and lengthens with the years of age. (See also
[38]). On the other hand, what kind of an algorithm could regulate its
memory according to its knowledge if not a self-editing one. The same
reasoning can provide a plausible cause for the following phenomenon:
It is a common knowledge, as noticed also in [38], that the subjective
perception of elapsed time shortens with age.
In the frame of the
theory that is presented here, a plausible explanation would be that in
the course of time less self evolution occurs, as the later approaches its
upper limit, and on the other hand it occurs a better understanding of
the environment, thus less environmental facts to be related to the self
and between them.

The second one is the way that children learn to speak: They begin
using syllables (notice that as an education practice we tend to reward
them for the correct ones), then try to combine them in words, phrases,
etc. The importance of induction in language learning has been also
emphasized by Hauser, Chomsky and Fitch in [36].

12.2. Structure. As stated in [17], a key driver of the capability of
biological organisms to quickly adapt to new environments, is the wide-
spread modularity of biological networks, that is, their organization as
functional, sparsely connected subunits. Despite its importance and
decades of research, there is no agreement on why modularity evolves
[75, 74, 25, 39, 40, 17].

On the other hand, assemblies of neurons [58, 37, 1, 35, 12, 53, 13,
23, 62], are large populations of neurons believed to imprint memories,
concepts, words, and other cognitive information. Their formation is
not yet understood [12].

In both cases, our proposal is that structure emerges in order to fa-
cilitate diagonalization in particular features of the overall program.
Notice that this view is in accordance to the theory that modularity
arises in biology to enable modifying one subcomponent without af-
fecting others [25] and also with the hypothesis that modularity (in
biology) emerges because of rapidly changing environments that have
common subproblems, but diﬀerent overall problems [39, 40], since the
structure that induces parallel diagonalization is exactly the same. The
general problem of formation of the structure seems to be relative with
the problem of language, since the structure of a self-editing code de-
ﬁnes a language to talk with itself. However, we have already seen
deﬁnitions of modules by a self-editing code, for example the forma-
tion of a diﬀerentiating code. Actually, in Mental experiment 2 of page

RECURSION, EVOLUTION AND CONSCIOUS SELF

47

29, we essentially used the formation of a diﬀerentiating code in order
to diagonalize upon it.

The general guess we can make here about the formation of units of
structure, is that sequential diagonalization can detect codes that are
usually computed in a sequence of computations, so that a self-editing
code can store them for future use, saving thus computing power. The
strategy itself of doing so, can also be detected by diagonalization,
assuming that successful storing of codes has occurred a number of
times before. One should notice here that using this strategy, is exactly
what we do in language when we often use an object or an idea that has
a complex description: We simply deﬁne it by using either a shorter
phrase or even a single word.

12.3. Internal representation of the envirnonment. It has been
hypothesized that the human brain encodes for the environment and
by doing so it can generate predictions and recall memories, see for
example [13, 53, 55, 6]. We would like here to describe a way that such
an encoding could take place, in the case of self-editing computations.
So, assume that c1, . . . , cn is an either surviving or successful se-
quence of self-editing codes and moreover that in every code ck, k ≤ n,
there are addresses for the input ik that ck receives and for the out-
put ok that sends to the environment. Assuming diagonalization, cn
can decide the probably successful output to in (that has received), by
ﬁnding a code r such that for all k ≤ n, alg(r)(ik) = ok. The suggested
output then should be alg(r)(in). It is obvious on the other hand, that
such a strategy as diagonalization, requires a large amount of storing
memory, something that quickly renders it unpractical. This problem
may have a similar to diagonalization answer: Assuming that one has
in his disposition a suﬃciently large amount of codes r1, . . . , rm that
are either rejected or accepted by the above diagonalization process,
one can search for a test code t such that alg(t)(rℓ), ℓ ≤ m is a rejection
if rℓ does not ﬁt the set of pairs ik, ok, k ≤ n and an acceptance if it
does ﬁt the same set of pairs. Naturally, the searcher that proposed the
test-codes like t above should be a subject of evolution (or learning) so
that ideally could reach a point to be successful enough for its choices.
Since obviously it cannot be expected that a code t that describes all
the environment could be possibly calculated at once, the method de-
scribed above should work better combined with the idea of creating
structure, where in subcodes, less complicated testing codes have to be
guessed. This point of view complies with the way we learn about the
world, trying to understand its features one by one.

48

A. D. ARVANITAKIS

In a more sophisticated functionality, one should replace the pair
of rejection and acceptance, which is a 0-1 evaluation, with analogue
evaluations like punishing and rewarding. Thus we come to the idea
of an internal representation of the real world reward and punishment,
in other words, pleasure and dissatisfaction. In this case, in order to
guess the testing code t, that replaces diagonalization, one should have
beforehand the knowledge, not only that c1, . . . , cn is successful, but
also how much successful every transition ck 7→ ck+1, k < n is.

Notice that the above procedure could be carried out to any subcode
of the initial self-editing code to replace diagonalization on the speciﬁc
subcode. Such loops is well know to exist also in the nervous system.
Another useful remark is that the procedure could be carried out by
the self-editing code itself, due to the basic self-editing principle and
that it can be found by diagonalization, assuming of course that some
successful guesses of testing-codes have been performed by chance. This
is so, since successful testing-codes should in general agree with the
testing performed by diagonalization.

Let us recall at this point that due to specialization, diagonalization
units can form an hierarchy if necessary, starting from the searcher δs1
that proposes codes for the relation between the environmental input
and output, continuing this way with diagonalization units that decide
about various subcodes of δs1 and so on. Repeating the above process
of replacing the testing sequence of each of these units with a simpler
algorithmic test which at the basis should represent simple reactions
of the environment to speciﬁc actions of the self-editing algorithm,
one can arrive in an hierarchy of representations of the reality and of
various aspects of it by means of sequences t1, t2, . . . of algorithmic
tests. Since the essential success of forming such an hierarchy is the
simpliﬁcation of the actual reality regarding aspects that are irrelevant
with the decisions of the self-editing code at hand, it is interesting to
investigate the application of diagonalization to the formation of the
sequence t1, t2, . . . . Such kind of diagonalization may lead for example
to the perception of a “line” as an object with only one dimension
(something that certainly does not occur in nature).

12.4. The role of sex. Sex is nearly universal in life [51]: “Species
that do not exchange genes in any form or manner, called “obligate
asexuals,” are extremely rare, inhabiting sparse, recent twigs of the tree
of life, coming from sexual ancestral species that lost their sexuality,
and heading toward eventual extinction without producing daughter
species [50]. Evolutionary theorists have labored for about a century
to ﬁnd explanations for the role of sex in evolution, but all 20th century

RECURSION, EVOLUTION AND CONSCIOUS SELF

49

explanations are valid only under speciﬁc conditions, contradicting the
prevalence of sex in nature [51]. The problem has been called “the
queen of problems” in evolutionary biology [8].”

Our aim at this point, is to propose a plausible reason for this dom-
inance of sexual reproduction, based on the theory that has been de-
veloped here.
In fact, this explanation is quite simple, since sexual
reproduction can serve as a mixture of sequential and parallel diag-
onalization: A code that ﬁts two surviving sequences is much more
safe to be regarded as correct, than a code that ﬁts just one. Notice
that this point of view is able to explain equally well the reason for
which in sex there is attraction of the opposition: This ensures that
the validity of the proposed code can be established under diﬀerent
attitudes. Contrary, relatives are usually excluded from reproduction
for the same reason. We essentially acknowledge this principle in the
usual form that a doubtful perception or idea, becomes instantly more
certain in the case that it is reassured by other(s).

It is worth noticing here that the particular male-female roles in
sexual reproduction is reminiscent of the relation between a searcher
and a tester: Usually there are more than one male that claim a female
and there is always a test (sometimes performed by the female itself) to
decide among challengers. A possible explanation for this could be that
the particular relationship is the product of parallel diagonalization
performed in other structures. Perhaps not surprisingly, one can see
the same relationship in the pair of sperm and egg.

13. Further evidence and related work.

A long-standing open question in biology is how populations are
capable of rapidly adapting to novel environments, a trait called evolv-
ability [61, 42].

There is currently an extended list [77, 31, 3, 78, 61, 73, 28, 15, 7, 41,
42, 45, 17, 70, 2, 76, 47, 32, 51, 50, 72, 63] of biology related research
performed by biologists and computer scientists, that supports that in
the course of evolution occur phenomena like learning, generalizing and
thinking by analogies.

Fischer [32] necessitates the discovery of a mathematical law about

evolution as clean and central as the second law of thermodynamics.

Wright [79] pointed out that the frequency of an allele in a diploid
locus changes in the direction that increases the population’s mean
ﬁtness.

The process itself of parallel diagonalization is apparent in biology:
Entire genomes with their accompanying protein synthetic systems are

50

A. D. ARVANITAKIS

transferred throughout the biosphere primarily as bacteria and protists
which become symbionts as they irreversibly integrate into pre-existing
organisms to form more complex individuals [52].

From [18]: “The ﬂy’s circuit assigns similar neural activity patterns
to similar input stimuli (odors), so that behaviors learned from one
odor can be applied when a similar odor is experienced.”

Also: Two bacterial cells can pair up and build a bridge between

them through which genes are transferred [69].

S. Kauﬀman [41] argues that spontaneous order in biological orga-
nizations may enable, guide and limit selection. Therefore, the spon-
taneous order in such systems implies that selection may not be the
sole source of order in organisms, and that we must invent a new the-
ory of evolution which encompasses the marriage of selection and self-
organization.

Mutations do not seem to be completely random [51]:
“Many important mutations are rearrangements of small stretches
as well as large swaths of DNA: duplications, deletions, insertions, in-
versions, among others [34]. Moreover the chance of a mutation varies
from one region of the genome to another and is aﬀected by both local
and remote DNA [50]. For a long time it has been believed that muta-
tions are the results of accidents such as radiation damage or replication
error. But by now we have a deluge of evidence pointing to involved bi-
ological mechanisms that bring about and aﬀect mutations [50]. Nearly
a quarter of all point mutations in humans happen at a C base which
comes before a G after that C is chemically modiﬁed (methylated) [30];
methylation is known to be the result of complex enzymatic processes.
DNA sequences are prone to “jump” from one place of the genome
to another, carrying other DNA sequences with them [34]. Another
interesting fact is that the same machinery that eﬀects sexual recom-
bination is also involved in mutations, and in fact produces diﬀerent
types of rearrangement mutations, depending on the genetic sequences
that are present [34]. Finally, diﬀerent human populations undergo
diﬀerent kinds of mutations resulting in the same favorable eﬀect, such
as malaria resistance, suggesting that genetic diﬀerences between pop-
ulations cause diﬀerences in mutation origination [50]...Mutations are
random but it may be more productive to think of them as random in
the same way that the ouputs of randomized algorithms are random.
Indeed, mutations are biological processes, and as such they must be
aﬀected by the interactions between genes. This new conception of
heredity is exciting, because it creates an image of evolutuion that is
even more explicitly algorithmic. It also means that genes interacting

RECURSION, EVOLUTION AND CONSCIOUS SELF

51

in one organism can leave hereditary eﬀects on the organims’s oﬀspring
[50].”

Also: The total number of steps that have been done during evolution
process is far less than the steps that cellphone processors do in an hour
[51].

Gene interactions are considered very important to understand the
hidden aspects of evolution [51]. It is clear that they can be explained
by assuming that living organisms are in fact self-editing, since in this
case, a part of their program may assume a critical role in regulating
(editing) another part. In [51], the authors raise the problem of the
Preservation of Variation: Classic data indicates that, for a large va-
riety of plants and animals taken together, the percentage of protein-
coding loci that are polymorphic (in the sense that more than one
protein variant exists that appears in more than 1% of the individuals
in a population), and the percentage of such loci that are heterozy-
gous in an individual, average around 30% and 7% respectively [56].
Such a number is far greater than could be explained by traditional
selection-based theories [49, 56].

The theory of self-editing codes can provide an explanation for such
a variation: Based on its experience via diagonalizing, it should be
clearly possible for a self-editing code to compute its immediate de-
scendants on the basis of an uncertain environmental future. (Notice
that this is exactly the approach that we adopt for the same reason:
One should be able at any moment to demonstrate diﬀerent charac-
teristics, depending on the environmental change). The idea is that a
self-editing code should evolve to prepare its descendants for various
possibilities during proliferation. This is evident by the way we han-
dled the Mental experiments 1 and 2, page 28. Indeed, a searcher may
adapt to compute the most probably appropriate descendants, as we
have naively seen in Example 2, page 41, where diagonalization may
alter the priorities of the proposed codes and as for the necessity of
doing so, it should be apparent by diagonalization in the self-memory
of a surviving sequence of self-editing codes, as a struggle for variation
to be retained in every proliferating step. The argument is again the
same and Darwinian in its nature: Not doing so, should result in the
elimination of the sequence due to poor preparation for various possible
environmental developments in the future.

Traditionally mathematics face the phenomenon of self-reference as
the exception and not the rule. Mathematical functions usually act
on an argument which is usually something that does not contain the
function itself.

52

A. D. ARVANITAKIS

On the other hand, in the case that there exist a suﬃciently large
category of chemical compounds (such as DNA sequences) that are
able to code for computable functions on some input and assuming their
random existence, the most probable random action to take would be to
act on themselves, since this is exactly the input that is spatially closer
to them. Thus (in contrast to mathematics), self-reference should be a
naturally occurring phenomenon, and not the exception to the rule.

A Darwinian theoretical model for learning in the human brain, has

been proposed by various authors [2, 22, 27].

In [76], the authors suggest that a gene regulation network has the
potential to develop ‘recall’ capabilities normally reserved for cognitive
systems.

Structures that may be the result of sequential or parallel diagonal-

ization are apparent in the human brain:

The size of the intersection of two assemblies has been shown in
experiments to represent the extent to which memories co-occur or
concepts are related; the phenomenon is called association of assemblies
[5].

It has been shown that whenever medial temporal lobe neurons re-
spond to more than one concept, these concepts are typically related.
Furthermore, the degree of association between concepts could be suc-
cessfully predicted based on the neurons response patterns [26].

Language is obviously constructed by us in order to communicate.
On the other hand, one can easily observe the similarity in structure
between language and the encoding of concepts of the real world in the
human brain. (See for example [29, 58, 20]). It is therefore very natural
to conclude that the creator of both representations is the same. About
the possible relation between thinking and language see also [60] and
the references therein.

(For the deﬁnition and related notions of recursive functions see for
example [9]). As we have roughly seen in Example 1 of page 14 and
more thoroughly in section 7.3, page 23, self-editing computations are
closed under primitive recursion. It is easy to see that they are also
closed under composition and using test algorithms renders them closed
under the minimization operator. Thus, according to the theory of
recursive functions, a self-editing code requires very few initial functions
to be able to compute theoretically any computable function.
It is
therefore natural, assuming the validity of the theory to human brain,
to see simple learning steps to be accomplished by mere strengthening
of the synapses, something that could correspond either to composition
or to primitive recursion. (However see also [21]).

RECURSION, EVOLUTION AND CONSCIOUS SELF

53

The idea of guessing a diﬀerentiating code out of a (ﬁnite) set of
samples is not new in bibliography: Leslie Valiant [72] has proposed
a theory for learning, namely probably approximately correct learning
which is a framework for mathematical analysis of machine learning.
The learner receives samples and must select a generalization function
(called the hypothesis) from a certain class of possible functions. The
goal is that, with high probability (the “probably” part), the selected
function will have low generalization error (the “approximately correct”
part).

In [11] the authors cover new work that casts human learning as
program induction. They argue that the notion that the mind approx-
imates rational (Bayesian) inference is insuﬃcient due to the fact that
natural learning contexts are typically much more open-ended–there
are often no clear limits on what is possible, and initial proposals often
prove inadequate. Recent work has begun to shed light on this problem
via the idea that many aspects of learning can be better understood
through the mathematics of program induction [16, 46]. People are
demonstrably able to compose hypotheses from parts [33, 60, 66] and
incrementally grow and adapt their models of the world [10]. A number
of recent studies has formalized these abilities as program induction,
using algorithms that mix stochastic recombination of primitives with
memorization and compression to explain data [19, 24, 64], ask infor-
mative questions [65], and support one- and few-shot-inferences [46].
Program induction is also proving to be an important notion for under-
standing development and learning through play [67] and the formation
of geometric understanding about the physical world [4].

As mentioned earlier, it is easy to see that hebbian plasticity [37], can
be thought of as a consequence of diagonalization. Indeed, assuming
that two neurons ﬁre together repeatedly, the permanent decision to
strengthen their synapse, should be in accordance with the self-memory.
There are ﬁndings [13] showing that the brain encodes for the direc-
tion of the head, perhaps also of other parts of the body. This seems
to be interestingly related to self-editing.

The self-reference eﬀect is a phenomenon studied in psychology [44,
71], according to which, a person can better recall information in the
case where this information has been linked to the self.

The next two arguments are closely related to (and explained better

by) the last one:

The well known existence of unconscious parts of ourselves, may be
easily explainable by stabilization about which we have already talked
in page 35, i.e. the permanent decision of a self-editing code, to keep

54

A. D. ARVANITAKIS

unaltered parts of its code. Such a decision can result from diagonal-
ization, assuming that there is no successful attempt in the history of
a self-editing code to change this part of itself.

What about emotions? Other than the basic pair of pleasure and
dissatisfaction (we ’ll talk about them immediately below), one can
notice the inﬂuence that they represent in a self-editing system. For
example, it is easy to observe that fear can make us function in shorter
time cycles, predicting and remembering thus in shorter intervals. This
could easily be explained by the fact that under the regime of fear,
one should review and change well learned behaviors (which should
be established in shorter cycles), in order to adapt them in the new
situation.

Let us assume for the moment that ‘ego’ (the greek and latin word
for ‘I’) can be deﬁned as the part of ourselves that experiences emo-
tions. Notice that among them pleasure and dissatisfaction are the
most central. It is evident, both by the way we use education and by
our experience, that both these emotions can be used to accomplish
learning functionalities. So, one can conclude, based on the above def-
inition, that ‘ego’ (i.e. the self-referential part of ourselves) is exactly
the part that is intended to accomplish the functions of exploration
and learning.

References

[1] Abeles M. Corticonics: Neural Circuits of the Cerebral Cortex. 1991, Cambridge

University Press.

[2] Adams P. Hebb and Darwin. J. Theoret. Biol. 1998; 195: 419-438
[3] Aguilar L, Bennati S, Helbing D. How learning can change the course of evolu-

tion. PLoS ONE 2019 14(9)

[4] Amalric M, Wang L, Pica P, Figueira S, Sigman M, Dehaene S. The language
of geometry: Fast comprehension of geometrical primitives and rules in human
adults and preschoolers. PLoS Comput. Biol., 2017.

[5] Anari N, Daskalakis C, Maass W, Papadimitriou C.H, Saberi A, Vempala S.
Smoothed analysis of discrete tensor decomposition and assemblies of neurons in
Advances in Neural Information Processing Systems31: Annual Confeerence on
Neural Information Processing Systems 2018, NeurlPS 2018, Bengio S, Wallach
H.M, Eds (Curran Associates, 2018), pp 10880-10890.

[6] Bar M. The proactive brain: using analogies and associations to generate pre-

dictions. Trends Cogn Sci. 2007 11, 280-289.

[7] Barton N.H, Novak S, Paix˜ao T. Diverse forms of selection in evolution and com-
puter science. Proc. of National Academy of Sciences U.S.A. 2014;111:10398-
10399

[8] Bell G. The Masterpiece of Nature: The Evolution and Genetics of Sexuality.

University of California Press, Berkeley, CA, 1982.

RECURSION, EVOLUTION AND CONSCIOUS SELF

55

[9] Boolos G. S, Burgess J. P, Jeﬀrey R. C. Computatability and Logic, Cambridge

University Press, 2007.

[10] Bramley N. R, Dayan P, Griﬃths T, Lagnado D. Formalizing Neurath’s Ship:
Approximate Algorithms for Online Causal Learning. Psychological review,
2017.

[11] Bramley N. R, Schulz E, Xu F, Tenenbaum J. Learning as program induction.

Cognitive Science 2018.

[12] Buzs´aki G. Neural syntax: Cell assemblies, synapsembles, and readers. Neuron

2010 68, 362-385

[13] Buzs´aki G. The Brain from Inside Out 2019, Oxford University Press.
[14] Calder´on-Franco D, van Loosdrecht M.C.M, Abeel T, Weissbrodt D.G. Free-
ﬂoating extracellular DNA: Systematic proﬁling of mobile genetic elemants and
antibiotic resistance from wastewater. Water Research 2021 189 116592.
[15] Chastain E, Livnat A, Papadimitriou C, Vazirani U. Algorithms, games, and
evolution. Proc. of National Academy of Sciences U.S.A. 2014;111:10620-10623
[16] Chater N, Oaksford M. Programs as Causal Models: Speculations on Mental

Programs and Mental Representation. Cogn. Sci. 2013.

[17] Clune J, Mouret J-B, Lipson H. The evolutionary origins of modularity. Proc

R Soc B 2013 280: 20122863. http://dx.doi.org/10.1098/rspb.2012.2863

[18] Dasgupta S, Stevens C.F, Navlakha S. A neural algorithm for a fundamental

computing problem. Science 2017 358, 793-796.

[19] Dechter E, Malmaud J, Adams R, Tenenbaum J. Bootstrap Learning via Mod-

ular Concept Discovery. IJCAI, 2013.

[20] Ding N, Melloni L, Zhang H, Tian X, Poeppel D. Cortical Tracking of Hi-
erarchical Linguistic Structures in Connected Speech. Nat. Neurosci. 2016 19,
158-164.

[21] Dranovsky A, Picchini A. M, Moadel T, Sisti A. C, Yamada A, Kimura S,
Leonardo E. D, Hen R. Experience dictates stem cell fate in the adult hip-
pocampus. Neuron 2011 70(5): 908-923.

[22] Edelman G. M. Neural Darwinism. The Theory of Neuronal Group Selection

1987, New York: Basic Books.

[23] Eichenbaum H. Barlow versus Hebb: When is it time to abandon the notion
of feature detectors and adopt the cell assembly as the unit of congnition?
Neurosci. Lett. 2018 680, 88-93.

[24] Ellis K, Dechter E, Tenenbaum J. Dimensionality Reduction via Program In-

duction. AAAI Spring Symposia, 2015.

[25] Espinosa-Soto C, Wagner A. Specialization can drive the evolution of modu-

larity. PLoS Comput. Biol. 2010 6, e1000719.

[26] Falco E, Ison M.J, Fried I, Quian Quiroga R. Long-term coding of personal
and universal associations underlying the memory web in the human brain. Nat.
Commun. 2016 7, 13408.

[27] Fernando C, Szathm´ary E, Husbands P. Selectionist and evolutionary ap-
proaches to brain function: a critical appraisal Front. Comput. Neurosci. 2012;
6:24

[28] Frank S.A. The design of natural and artiﬁcial adaptive systems. in: Rose

M.R. Lauder G.V. Adaptation. Academic Press, 1996: 451-505

56

A. D. ARVANITAKIS

[29] Frankland S.M, Greene J.D. An architecture for encoding sentence meaning
in left mid-superior temporal cortex. 2015 Proc. Natl. Acad. Sci. U.S.A. 112
11732-11737.

[30] Fryxell K.J, Moon W-J. CpG mutation rates in the human genome are highly
dependent on local GC content. Molecular Biology and Evolution. 2005, 22,
650-658.

[31] Gerhart John, Kirschner Marc. The theory of facilitated variation. Proceedings

of the National Academy of Sciences 2007 104(1):8582-8589.

[32] Fisher R.A. The Genetical Theory of Natural Selection. The Clarendon Press,

Oxford, U.K.,1930.

[33] Goodman N.D, Tenenbaum J, Feldman J, Griﬃths T. A Rational Analysis of

Rule-Based Concept Learning. Cogn. Sci., 2008.

[34] Graur D, Li W.-H. Fundamentals of Molecular Evolution. Sinauer Associates,

Sunderland, MA, 2000.

[35] Harris K.D. Neural signatures of cell assembly organization. Nat. Rev. Neu-

rosci. 2005 6 399-407

[36] Hauser M.D, Chomsky N, Fitch W.T. The faculty of language: What is it,

who has it, and how did it evolve? Science 2002 298, 1569-1579

[37] Hebb D.O, The Organization of Behavior: A Neuropsychological Theory 1949,

Wiley, New York, NY.

[38] Hofstadter Douglas. Analogy as the Core of Cognition, in Dedre Gentner,
Keith Hoyoak and Boicho Kokinov (eds.) The Analogical Mind: Perspectives
from Cognitive Science, Cambridge, MA: The MIT Press/Bradford Book, 2001,
pp. 499-538.

[39] Kashtan N, Alon U. Spontaneous evolution of modularity and network motifs.

Proc. Natl. Acad. Sci. USA 2005 102 13773-13778.

[40] Kashtan N, Noor E, Alon U. Varying environments can speed up evolution.

Proc. Natl Acad. Sci. USA 104, 13711-13716.

[41] Kauﬀman S.A. Origins of Order: Self-Organization and Selection in Evolution,

1993, Oxford, U.K., Oxford University Press.

[42] Kirchner M, Gerhart J. Evolvability. Proceedings of National Academy of Sci-

ences U.S.A. 1998; 95: 8420-8427

[43] Kleene Stephen Cole. Introduction to Metamathematics. Wolters-Noordhoﬀ
publishing - Groningen North-Holland publishing company - Amsterdam New
York, 1952.

[44] Klein S.B, Kihlstrom J.F. Elaboration, organization, and the self-reference ef-
fect in memory. J Exp Psychol Gen. 1986 Mar;115(1):26-38. doi: 10.1037//0096-
3445.115.1.26. PMID: 2937872.

[45] Kouvaris K, Clune J, Kounios L, Brede M, Watson R. A. How evolution learns
to generalise: Using the principles of learning theory to understand the evolution
of developmental organisation. PLoS Comput Biol 2017 13(4)

[46] Lake B, Salakhutdinov R, Tenenbaum J. Human-level concept learning through

probabilistic program induction. Science, 2015.

[47] Laland K.N, Uller T, Feldman M.W, Sterelny K, M¨uller G.B, Moczed A,
Jablonka E, Odling-Smee J. The extended evolutionary synthesis:
its struc-
ture assumptions and predictions. Proc. R. Soc. B 2015 282: 20151019.
http://dx.doi.org/10.1098/rspb.2015.1019

RECURSION, EVOLUTION AND CONSCIOUS SELF

57

[48] Levin S.A. Ecosystems and the biosphere as complex adaptive systems. Ecosys-

tems. 1998; 1: 431-436

[49] Lewontin R.C, Hubby J.L. A molecular approach to the study of genic het-
erozygosity in natural poputations; amount of variation and degree of heterozy-
gosity in natural populations of Drosophila pseudoobscura. Genetics 54, 1966,
pp 595–609.

[50] Livnat A. Interaction-based evolution: How natural selection and nonrandom

mutation work together. Biology Direct 8, 1 (2013), 24.

[51] Livnat A, Papadimitriou C. Sex as an Algorithm: The Theory of Evolution
Under the Lens of Computation. Communication of the ACM 2016 59(11) 84-
93.

[52] Margulis L. Origins of species: acquired genomes and individuality. BioSys-

tems. 1993; 31; 121-125

[53] Miller J.E. K., Ayzenshtat I, Carrillo-Reid L, Yuste R. Visual stimuli recruit
intrinsically generated cortical ensembles. Proc. Natl. Acad. Sci. U.S.A. 2014
111, E4053-E4061.

[54] Moschovakis Y.N. Notes on Set Theory. Springer, 2006.
[55] Neisser U. Cognition and reality: principles and implications of cognitive psy-

chology. San Fracisco: Freeman, 1976.

[56] Nevo E, Beiles A, Ben-Shlomo R. The evolutionary signiﬁcance of genetic
diversity: Ecological, demographic and life history correlates. Lecture Notes in
Biomathematics 53, 1984.

[57] Papadimitriou C, Steiglitz K. Combinatiorial Optimization: Algorithms and

Complexity. Dover, 1998.

[58] Papadimitriou C.H, Vempala S. S, Mitropolsky D, Collins M, Maass W. Brain
computation by assemblies of neurons. Proceedings of the National Academy of
Sciences 2020, 117(25) pp 14464-14472; doi:10.1073/pnas.2001893117

[59] Pavlicev M, Cheverud J. M, Wagner G. P, Evolution of adaptive phenotypic
variation patterns by direct selection for evolvability Proc. R. Soc. B Biol. Sci.
2011;278: 1903-1912

[60] Piantadosi S. T, Tenenbaum J. B, Goodman N. D. The logical primitives of
thought: Empirical foundations for compositional cognitive models. Psychol.
Rev. 2016 123, 392-424

[61] Pigliucci M. Is evolvability evolvable?. Nat. Rev. Genet. 2008; 9: pp75-82
[62] Ranhel J. Neural Assembly Computing. IEEE Trans. Neural Netw. Learn.

Syst. 2012 23, 916-927.

[63] Riedl R.J. A systems-analytical approach to macroevolutionary phenomena.

Q. Rev. Biol. 1977; 52: 351-370.

[64] Romano S, Salles A, Amalric M, Dehaene S, Sigman M, Figueira S. Bayesian
selection of grammar productions for the language of thought. bioRxiv, 2017.
[65] Rothe A, Lake B, Gureckis T. Question Asking as Program Generation. NIPS,

2017.

[66] Schulz E, Tenenbaum J, Duvenaud D, Speekenbrink M, Gershman S. Compo-

sitional Inductive Biases in Function Learning. biorxiv, 2016.

[67] Sim A, Xu F. Learning Higher-Order Generalizations Through Free Play: Ev-
idence From 2- and 3-Year-Old Children. Developmental psychology, 2017.
[68] Smullyan R.M. Diagonalization and Self-Reference. Oxford science publica-

tions, 1994.

58

A. D. ARVANITAKIS

[69] Stearns S.C, Hoekstra R.F. Evolution: An Introduction. Oxford University

Press, New York, 2005.

[70] Szathm´ary E. Toward major evolutionary transitions theory 2.0. Proc. Natl.

Acad. Sci. U.S.A. 2015; 112:10104-10111

[71] Symons C.S, Johnson B.T. The self-reference eﬀect

a
meta-analysis. Psychol Bull. 1997 May; 121(3):371-94. doi: 10.1037/0033-
2909.121.3.371. PMID: 9136641.

in memory:

[72] Valiant L. Probably Approximately Correct: Nature’s Algorithms for Learning

and Prospering in a Complex World. Basic Books, 2013.

[73] Wagner, G.P, Altenberg, L. Complex adaptation and the evolution of evolv-

ability. Evolution 1996 50, pp. 967-976.

[74] Wagner G, Mezey J, Calabretta R. Modularity: understanding the development
and evolution of complex natural systems. Natural selection and the origin of
modules. 2001 Cambridge, MA: MIT Press.

[75] Wagner G.P, Pavlicev M, Cheverud J.M. The road to modularity. Nat. Rev.

Genet. 2007 8, 921-931.

[76] Watson R.A, Buckley C.L, Mills R, Davies A. Associative memory in gene reg-
ulation networks. in: Fellermann H. Proceedings of the Artiﬁcial Life Conference
XII. MIT Press, 2010: 194-202

[77] Watson A.R, Szathm´ary E. How Can Evolution Learn? Trends in Ecology &

Evolution, 2015.

[78] West-Eberhard M.J. Phenotypic Plasticity and the Origins of Diversity. Annual

Review of Ecology and Systematics, 20 (1989), pp. 249-278.

[79] Wright S. The distribution of gene frequencies in populations. Proceedings of

the National Academy of Sciences U.S.A. 1937 23(6), 307320.
Email address: aarva@math.ntua.gr

National Technical University of Athens, Department of Mathe-

matics, Athens, Greece

