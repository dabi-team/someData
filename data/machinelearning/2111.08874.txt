GN-Transformer: Fusing Sequence and Graph Representation
for Improved Code Summarization

Junyan Cheng
junyanch@usc.edu
University of Southern California
Los Angele, ca, USA

Iordanis Fostiropoulos
fostirop@usc.edu
University of Southern California
Los Angele, CA, USA

Barry Boehm
boehm@usc.edu
University of Southern California
Los Angele, CA, USA

1
2
0
2

v
o
N
7
1

]

G
L
.
s
c
[

1
v
4
7
8
8
0
.
1
1
1
2
:
v
i
X
r
a

Figure 1: Examples of generated summaries on Java (green block) and Python (orange block) test sets. The comparison is
between the dataset provided code-summary (Reference) and the code-summary generated for the same snippet by a GN-
Transformer (Ours) and a Transformer by Ahmad et al. [1].

ABSTRACT
As opposed to natural languages, source code understanding is
influenced by grammatical relationships between tokens regardless
of their identifier name. Graph representations of source code such
as Abstract Syntax Tree (AST) can capture relationships between
tokens that are not obvious from the source code. We propose
a novel method, GN-Transformer to learn end-to-end on a fused
sequence and graph modality we call Syntax-Code-Graph (SCG).
GN-Transformer expands on Graph Networks (GN) framework us-
ing a self-attention mechanism. SCG is the result of the early fusion
between a source code snippet and the AST representation. We
perform experiments on the structure of SCG, an ablation study
on the model design, and the hyper-parameters to conclude that
the performance advantage is from the fused representation. The
proposed methods achieve state-of-the-art performance in two code
summarization datasets and across three automatic code summa-
rization metrics (BLEU, METEOR, ROUGE-L). We further evaluate
the human perceived quality of our model and previous work with
an expert-user study. Our model outperforms state-of-the-art in
human perceived quality and accuracy.

CCS CONCEPTS
â€¢ Computing methodologies â†’ Natural language generation.

KEYWORDS
code summarization, transformers, graph networks

1 INTRODUCTION
Code summarization is the task of generating a readable summary
that describes the functionality of a snippet. Such a task requires
a high-level comprehension of a source code snippet, and it is an
effective way to evaluate whether a Deep Learning Model is able
to capture information from complex code relationships. Program-
ming languages are context-free formal languages. Abstract Syntax
Tree (AST) is an unambiguous representation that can be derived
from a source code snippet. The structure of a snippet and the
grammar relationships between the tokens can be described by an
AST precisely and without noise. Such representation can provide
an inductive bias for code understanding.

Multiple methods perform code summarization and have used
graph and tree representations using late fusion techniques. For
example, Alon et al. [3] encoded AST paths using random walks,
source code tokens using an LSTM and then fused them by an at-
tention mechanism. Huo et al. [18] used Convolutional Neural Net-
works (CNN) to embed source code statements as node attributes
in a Control Flow Graph (CFG). They apply DeepWalk [27] on the
CFG to learn a representation which they concatenate with a source
code representation embedded in a CNN-based model for the task
of bug localization. LeClair et al. [21] apply Graph Convolutional
Networks (GCNs) directly on an AST and a GRU on the source code
sequence and concatenate their representations in a late fusion
approach. The above methods propose different ways to extract
features from an AST or CFG but with limited cross-modal inter-
action [35]. The graph and code modalities are fused late as the
learned representations of independent models. This is opposed to

 
 
 
 
 
 
Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

Figure 2: The encoder consists of multiple GN-Transformer blocks with a graph ğº as the input. We denote â€˜+â€™ as a residual
connection followed by a normalization layer. The Encoder outputs graph ğºâ€˜ with updated node attributes. For the task of
code-summarization, only the token-nodes (green) are used as input to the decoder while AST-nodes (grey) are discarded.

training end-to-end on a modality that is an outcome of early fu-
sion. Early fusion can improve model performance via cross-modal
interactions [35].

Early fusion methods enable interactions between different modal-
ities and allow a model to extract information of different granu-
larity from each modality [35]. A graph is a flexible representation
that allows for the early fusion of multiple modalities. Recent ad-
vancements in deep learning frameworks for graphs have helped
the improvement of early fusion methods [8, 13, 38].

There are many approaches in learning representations of Graphs.
Spectral-based methods like GCN [20] proposed graph convolu-
tion using the Laplacian of the adjacency matrix and is based on
spectral graph theory. Spatial-based methods like GraphSAGE [14]
aggregate information from neighboring nodes using different ag-
gregation functions like mean, summation, pooling, and LSTM.
However, there are several limitations of current approaches. Over
smoothing problem [23] makes it difficult to improve the model
performance by increasing depth. Work by DGCN [22] proposes to
solve the problem using skip connections. Identification of isomor-
phic graph structures is a difficult problem for graph models. The
ability to identify isomorphism by a model corresponds to the abil-
ity to learn the graph structure. Work by GIN [42] uses multi-layer
perceptrons in a GNN framework to accurately identify isomor-
phism in graphs. When every node in a graph has equally weighted
neighboring nodes it can add noise to the downstream task. GAT
[34] introduced an attention mechanism to learn a weight matrix
for neighboring nodes.

A Transformers architecture [33] combines all the above tech-
niques using a multi-head attention mechanism and a feedforward
network with residual connections. Thus a Transformers archi-
tecture can have a great advantage if formulated in the context of
graphs. Recent work by Battaglia et al. [8] proposed a general graph
deep learning framework, Graph Networks that can be applied in
the context of multiple deep learning architectures. Our work is
motivated by the recent advancements in graph networks and the
advantages of Transformers architecture.

BLEU and METEOR score are the dominant metric in evaluat-
ing deep learning model on the quality of machine generated text.
ROUGE metric is used to evaluate the quality of text generated for

summarization benchmarks. Previous work [9, 24] have shown that
BLUE and ROUGE score do not correlate with human perceived
quality. Metrics such as METEOR [6] have been proposed as an al-
ternative but do not evaluate the accuracy of the summary. Previous
work in code summarization rely on automatic reported metrics
and overlook weaknesses of such score in human perceived quality.
In this paper we propose a novel architecture GN-Transformer
shown in Figure 2 and Syntax Code Graph, a fused AST and code
sequence representation. Our model can learn end-to-end on an
SCG under the Graph Network framework for the task of code
summarization. We perform an ablation study on the model design
as well as on two structure variants of a fused graph and sequence
modality. We further evaluate our model using an expert-user sur-
vey on generated summaries. Based on our insights, we conclude
that the performance advantage is from the GN-Transformer en-
coder that can efficiently learn the representation of an SCG. In
detail:

â€¢ We extend Graph Networks (GN) to a novel GN-Transformer
architecture. A sequence of GN encoder blocks followed by
a sequence-to-sequence Transformer decoder.

â€¢ We propose a novel method for the early fusion of a code
snippet sequence and the corresponding AST representation
called Syntax-Code Graph (SCG).

â€¢ We evaluate our approach for the task of code summariza-
tion on two datasets. We use quantitative experiments and
the largest to-date empirical study on code summarization.
We compare our approach using three quantitative metrics
(BLEU, METEOR, ROUGE-L), we corroborate the results with
a large-scale survey of 330 participants and outperform previ-
ous state-of-the-art in both quantitative metrics and human
perceived quality.

We evaluate our model on a Java [17] and Python [7] dataset.
We compare our results to those of Ahmad et al. [1] under identical
experimental setups. Two qualitative results are presented in Figure
1. Our code, trained models, and pre-processed dataset are available
in https://github.com/chengjunyan1/GN-Transformer-AST.

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

2 RELATED WORK
We identify two main categories of related works. Works that focus
on the fusion between a sequence and graph modalities and works
that focus on training end-to-end on a graph structure.

Augmenting a source code modality with graph representations
like AST and Control-Flow-Graph (CFG) has been used by deep
learning techniques for source code understanding.

Previous methods consider sequences and graphs as two modal-
ities that are processed independently. For a sequence, recurrent
architectures such as RNNs, LSTMs, GRUs are commonly used.
Early fusion methods use domain knowledge to design a fused
modality that enables cross-modal communication. As a result, late
fusion approaches, are simple to implement but potentially lose
their ability to exploit cross-modal information [35]. Late fusion
approaches consider the source code with the corresponding graph
modality as input to two separate models. Alon et al. [4] extract
AST embedding with random walks, concatenate it with the source
code token embedding, and finally applies an attention mechanism,
learning a context vector used for downstream tasks. Hu et al. [16]
propose Structure-Based Traversal (SBT) that flattens the AST into
a sequence. Huo et al. [18] apply DeepWalk with CNN and LSTM
to learn a CFG representation which is then concatenated with the
source code representation. LeClair et al. [21] use GCN to learn
the AST embedding which is then concatenated with source code
embedding.

Wang et al. [37] augment the AST edges using the â€˜typeâ€™ of the
relationship between nodes extracted from the control and data
flow graphs. A gated GNN is applied to train on the augmented AST
representation. Such approaches are not able to effectively capture
cross-modal interactions that can further improve the model perfor-
mance [5]. There are no interactions between different modalities
when encoding them with late fusion approaches [35].

There are some recent works that propose the early fusion of
graph and source code representations for deep learning models.
Ahmad et al. [1] propose the fusion of a graph into a sequence.
They flatten an AST representation using SBT from Hu et al. [16]
and then use the sequence into a relative positional encoding and
a copy mechanism on a Transformer architecture. Their approach
resulted in performance degradation when compared to simply
using the source code tokens as a sequence. Allamanis et al. [2]
propose a â€˜program graphâ€™ which was based on AST. The graph
is constructed by introducing edges designed by expert knowl-
edge into the AST like â€œLastWriteâ€, â€œComputedFromâ€. The edges
define relationships between tokens, the type of edge is applied
using a rule-based approach. Hellendoorn et al. [15] propose to
use relation-aware attention [29] to incorporate graph information
into attention computation. They use a â€œleaves-onlyâ€ graph that is
specific to the context of C language. For their graph structure, they
discard all nodes that do not correspond to a source code token and
introduce them as edges between tokens. The implementation of
the graph structure is not publicly available. In addition, it relies on
a complex language-specific preprocessing pipeline for C language,
which makes it difficult to be extended to other programming lan-
guages. Our method relies only on AST which is easily available
for all programming languages.

There are methods besides code understanding that incorporate
graph or tree information into the sequence. In natural languages,
Nguyen et al. [25] uses the constituency tree from sentences in the
self-attention layer. TreeLSTM [32] fuse tree information with a
sequence using a tree-structured LSTM. Such approaches are limited
only to tree structures. Yao et al. [43] construct heterogeneous
graphs of documents and text. A document contains multiple words
and a word may appear in multiple documents. Many-to-many
relationships are constructed between documents and words in a
text. The constructed graph requires placing test documents with
training document nodes, which is not suitable for the prediction
of new unseen texts.

Recent work try to evaluate the correlation of automatic metric
for code summarization with human perceived quality. Stapleton
et al. [31] compare code generated summaries with the reference
descriptions in a human study of forty-five expert-user. They ask
users to complete coding tasks using the provided summaries and
evaluate their performance. The results show no correlation be-
tween human performance and automatic metrics. The study does
not directly evaluate perceived quality and does not compare ma-
chine generated summaries from multiple models. Gao et al. [12]
propose a sequence-to-sequence attention model specific for the
task of generating a title for a given code snippet for a popular
website, Stack Overflow. The study evaluated on automatic metric
(BLUE, ROUGE) and the results are corroborated with a human
study on perceived quality. However, the study was limited to five
evaluators. Panthaplackel et al. [26] propose a novel task of mod-
ifying documentation for a given code snippet given the commit
difference. They perform a human study in a repository commit
simulation environment and ask users to discard or update source
code documentation with a machine generated one. Modified anno-
tations are generated from the proposed model and the baselines
for a cross-model comparison. However, the number of participants
was limited to nine university students.

Our work is motivated by the advantage of early fusion, the
lack of a flexible fused code representation and a model capable of
learning on the fused modality end-to-end.

3 BACKGROUND
We discuss Graph Networks in Section 3.1. In Section 3.2 we discuss
the theoretical implications for improving cross-modal interactions
using early fusion. In Section 3.3 we discuss relational inductive
biases that provide an advantage in representing a sequence in a
graph instead of the contrary.

3.1 Graph Networks
Graph Networks [8] is a general framework that unifies deep learn-
ing models with a graph representation. Graph Networks is a gener-
alization of previous approaches including Message Passing Neural
Networks [13], Non-Local Neural Networks [38] that can learn a
graphical representation of data under a configurable computation
unit, GN block.

A GN block can flexibly define computations on a graph by con-
figuring subblocks that are a composite of functions. A GN block is
composed of three configurable subblocks. A Node block updates
the node attributes, an Edge block updates the edge attributes

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

(a)

(b)

Figure 3: (a) Orange nodes denote an AST for the statement â€˜a=b+5*câ€™, dashed lines connect each token-node (green) with their
direct parent in AST. (b) Standard SCG structure preserve the AST structure with additional edges between AST-nodes and
tokens.

then aggregates edge information, and a Global block maintains
a global graph attribute by aggregating all nodes and edges infor-
mation on the graph.

By adding, removing, or combining different subblocks, we can
flexibly define arbitrary computation rules on a graph. In our work,
we use a Node block and a modified Edge block, which we call At-
tention block, to define a single GN-Transformer block. We discuss
in further detail our method in Section 4.3.

The GN framework provides flexibility in the structure of the
graph representation. The definition of nodes and edges and their
relationships are context dependant. For example, a node could be
a word in a sentence [30], local feature vector in a CNNâ€™s output
feature map [39], or balls and walls for modeling a rigid body system
[10]. An edge could represent the energy in a physical system or
the relative position for image patches.

At the end of each GN layer, the interaction between nodes is
through the information propagation by the Node block. One GN
layer is the execution of one round of information propagation on
the graph. The blocks within each layer define the rules of how in-
formation propagation on the graph will happen. In our method, we
configure the node block and modify the edge block as an attention
mechanism block, corresponding to 1-hop information propagation.
Thus, an N-block GN-Transformer Encoder corresponds to N-hop
information propagation between graph nodes.

Using Graph Networks frameworks, we can represent data with
arbitrary relational inductive biases which will discuss in Section
3.3. Moreover, a graph modality can provide flexibility as a fusion
unification framework to represent multiple sources of information.

3.2 Cross-Modal Interactions
Multi-modal signals exhibit exploitable correlations which are de-
fined as cross-modal interactions. Cross-model information enables
the model to augment the feature extraction for individual modality
through other modalities implicitly [35].

Early fusion achieves improved interactions between modalities.
Graph Networks can be regarded as a general framework for early

fusion. Due to the representation power of the graph, it is possi-
ble to represent different modalities within a graph. Representing
multiple modalities on a single graph, then using the information
propagation attribute of GN the interaction between modalities
is implicitly learned. For example, to fuse the information of an
image and the caption, we can define one set of nodes representing
extracted image features or even image patches, and another set of
nodes representing token embedding of the caption. Consequently,
the information in the image could directly interact with the infor-
mation contained in the text. The cross-modal interaction could
happen from a low-granularity raw data input such as pixels to
high-granularity abstract features such as features extracted by
convolutional filters.

3.3 Relational Inductive Bias
Relational inductive biases impose constraints on the relationship
and interactions among entities in a learning process. In Graph
Networks, the assumption that two entities have interaction is
expressed by an edge between the corresponding entity nodes. The
absence of an edge expresses the assumption of isolation, which
means there is no direct interaction between the two nodes.

Sequences are unstructured data in which relationships are im-
plicit. Implicit relationships can be represented with each token
as a graph node that is fully connected with all other nodes. Such
representation allows each node to interact with every other node
with no explicit assumptions on isolation. Thus it allows the model
to infer the relationships between them implicitly. Transformers
could be regarded as inferring on such a fully connected attention
graph. Each token in an input sequence corresponds to a node.
The relationship between tokens is thus represented by attention
weights, high attention values correspond to strong interactions,
while low attention means isolation.

However, as shown by the recent work Dosovitskiy et al. [11], a
model without inductive biases may perform worse than the model
with valuable inductive biases when the dataset is relatively small. It
is less efficient for a model to learn to infer relationships without any
explicit assumptions of interactions and isolation [8]. AST provides

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

precise information about interactions and isolation among tokens
in source code since it brings information about the grammatical
relationships between tokens. We can find an explicit mapping
between tokens in a sequence and nodes in the AST through the
scope information provided by a parser for a given programming
language. We further discuss the graph structure of input data
and how we fuse AST with it in Section 4. We can introduce the
relational inductive bias of AST into our model through the fused
graph representation.

There are several benefits in fusing the sequence with the graph
representation as opposed to flattening a graph representation into
a sequence. Firstly, a graph representation can contain information
sources with arbitrary explicit or implicit relational inductive bias
through the graph structure. Secondly, a graph structure can be
augmented using expert knowledge [8], the â€˜program graphâ€™ from
Allamanis et al. [2] could be regarded as an example that introduces
expert knowledge. They use hand-crafted edges into an AST to
improve performance in the variable misuse task. Thirdly, a graph
representation results in better combinatorial generalization [8]
due to the reuse of multiple information sources simultaneously in
a unified representation. As an example consider two code snippets
that have the same functionality but different variable names. The
AST for these snippets will be the same, as such the model could
use the name invariant AST representation to generalize better on
seemingly different code snippets.

4 METHOD
In Section 4.1 we propose a joint graph representation of source
code and AST called standard Syntax-Code Graph (SCG). In Section
4.2 we introduce our model architecture and propose our Graph
Networks based encoder in Section 4.3. Our model follows the
generic Transformer encoder-decoder architecture. The encoder
is an extended architecture of Graph Networks with multiple GN
blocks. The decoder is a vanilla Transformer decoder. The overview
of our architecture is presented in Figure 2.

4.1 Syntax-Code Graph
Syntax-Code Graph (SCG) is the fused graph representation be-
tween an AST and a source code sequence. The graph is built based
on the AST, with source code tokens as additional nodes, and addi-
tional edges connecting them with the AST. We call the nodes on
the AST as AST-node, the source code tokens as token-node. The
node attribute of an AST-node is the identifier name on the AST,
such as â€œNameExprâ€, â€œAssignExprâ€. The node attribute of a token-
node is the source code text associated with that token, such as â€œaâ€,
â€œintâ€, â€œ+â€.

We connect token-nodes with AST-nodes using their direct par-
ent in an AST. Each AST-node has a scope provided by an AST
parser that corresponds to a range of positional marks in the origi-
nal source code snippet. Each positional mark is a line and column
pair. We define the direct parent as the deepest AST-node that
includes the token-node in their scope. Consider the statement
â€˜a=b+5*câ€™ as an example, the AST is shown in Figure 3(a). â€˜Mul-
tiplicativeExprâ€™ has a positional mark line 1, col 5 âˆ¼ line 1, col 7
which corresponds to â€˜5*câ€™, â€˜Numâ€™ and â€˜NameExprâ€˜ have positional
marks of line 1, col 5 âˆ¼ line 1, col 5 and line 1, col 7 âˆ¼ line 1, col

7 respectively. Token-nodes â€˜5â€™ and â€˜câ€™ in this case belong to the
scope of multiple AST-nodes (e.g., â€˜AdditiveExprâ€™, â€˜AssignExprâ€™),
but their direct parents are â€˜Numâ€™ and â€˜NameExprâ€™ respectively.
The dashed lines in Figure 3(a) connect each token-node with their
direct parent AST-node. The graph on Figure 3(b) shows the final
SCG.

We perform two ablation studies on the structure of SCG to see
the effectiveness of connecting token-nodes with only their direct
parents and the effects of isolation between token-nodes by fully
connecting them. Results can be found in Section 5.3.

4.2 Encoder-Decoder Architecture
The encoder consists of a stack of ğ‘ GN-Transformer blocks which
are derived from GN blocks, the main computation unit in Graph
Networks, it takes a graph as input and returns a graph as output
[8]. Each block in our model implements a Multi-Head Attention
layer and a Feed-Forward Neural sublayer equivalent to the Trans-
former encoder sub-layers. Encoder accepts a graph ğº = {ğ‘‰ , ğ¸}
where ğ‘‰ is a node set containing the set of initial node attributes
â„0 âˆˆ R |ğ‘‰ |Ã—ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ where ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ is the input and output dimension
of the encoder. ğ¸ is the edge set used to identify the neighboring
node set ğ‘ğ‘– for each node ğ‘–. The node features in the input graph
are initialized through an embedding layer. AST and token nodes
fetch an embedding vector according to their types and identifier
names respectively. For our implementation, we handle token-node
identifier and AST-node type separately when performing an em-
bedding lookup. This is to avoid representing the same embedding
nodes of different types with a naming conflict. The encoder out-
puts the graph with the updated node features â„ğ‘ . Feature vectors
of token-nodes are fetched as the decoder input. Feature vectors of
AST-nodes are discarded and the token embeddings are padded for
batching (see Figure 2).

4.3 GN-Transformer Blocks
GN-Transformer block is an extension to GN blocks as proposed
by Battaglia et al. [8] with Multi-Head Attention (MHA) and Feed-
Forward Network (FFN) defined in the context of Graph Networks.
As discussed in Section 3.1, in the context of GN, a GN-Transformer
block will execute one round of information propagation and aggre-
gation on the neighboring nodes that update the node attributes in
the graph. The ğ‘¡-th GN-Transformer block accepts node attributes
â„ğ‘¡ as input and output the updated node attributes â„ğ‘¡ +1. The GN-
Transformer block is composed of two subblocks, an Attention block
and Node block. The information propagation is an outcome of the
two subblocks applied sequentially.

An Attention block implements Multi-Head Attention MHA.
It is composed of an attention update function ğœ™ğ‘’ which calcu-
lates the attention weight between each node and their neighboring
nodes, and an attention aggregate function ğ‘ğ‘’â†’ğ‘£ which aggre-
gates the information for each node from all of their neighboring
nodes weighted by the attention weights, denoted as attention
â„ğ‘’ğ‘ğ‘‘ (ğ›¾ )
ğ‘–

The unnormalized attention weight between nodes ğ‘– and ğ‘— from ğ›¾-
. Each At-
ğ›¾ âˆˆ Rğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ğ‘˜

th attention head for the ğ‘¡-th GN-Transformer block is ğ›¼ (ğ›¾ )
ğ‘– ğ‘—
tention block has ğ» attention heads. Where ğ‘Š ğ¾

ğ›¾ ,ğ‘Š ğ‘„

.

are parameter matrices for attention head ğ›¾. The attention weight
from node ğ‘– to node ğ‘— for attention head ğ›¾ is computed by the
attention update function ğœ™ğ‘’ as follows:
ğ‘—ğ‘Š ğ¾

ğ‘–ğ‘Š ğ‘„
â„ğ‘¡

ğ›¾ )T

ğ›¼ (ğ›¾ )
ğ‘– ğ‘— =

ğ›¾ (â„ğ‘¡
âˆšï¸ğ‘‘ğ‘˜

Subsequently, the attention aggregate function ğ‘ğ‘’â†’ğ‘£ aggregates
the information between each node ğ‘– and the set of neighboring
nodes ğ‘ğ‘– based on the attention weight computed above as:

Â¯ğ‘ğ‘¡ +1
ğ‘–

= ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (â„ğ‘’ğ‘ğ‘‘ (1)

ğ‘–

, ..., â„ğ‘’ğ‘ğ‘‘ (ğ» )

ğ‘–

)ğ‘Š ğ‘‚

Where

â„ğ‘’ğ‘ğ‘‘ (ğ›¾ )
ğ‘–

=

âˆ‘ï¸

ğ‘— âˆˆğ‘ğ‘–

ğ‘—ğ‘Š ğ‘‰
â„ğ‘¡

ğ›¾ ğœ (ğ›¼ (ğ›¾ )
ğ‘–

) ğ‘—

and ğœ (Â·) is the softmax function, with ğ‘Š ğ‘‰
parameter matrices.

ğ›¾ âˆˆ Rğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ğ‘£ , ğ‘Š ğ‘‚ as

A Node block is composed of a node update function ğœ™ ğ‘£, that
updates the attribute of each node with the aggregated information
of all of their neighboring nodes Â¯ğ‘ğ‘¡ +1
. The output of a GN block is
ğ‘–
the updated node attributes â„ğ‘¡ +1 updated by ğœ™ ğ‘£ using the equation:
ğ‘– + Â¯ğ‘ğ‘¡ +1
ğ‘–
A Feed-Forward Network FFN with a residual connection is used
such as a single layer MLP with a non-linearity such as ReLu. For our
experiments, we also use dropout and layer normalization identical
to Vaswani et al. [33]. An illustration of our model can be found in
Figure 2.

= ğ¹ ğ¹ ğ‘ (â„ğ‘¡

ğ‘– + Â¯ğ‘ğ‘¡ +1
ğ‘–

) + â„ğ‘¡

â„ğ‘¡ +1
ğ‘–

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

Table 1: Summary of hyper parameters.

Hyper-parameters

Value

Num of layers
Attention heads
ğ‘‘ğ‘˜
ğ‘‘ğ‘£
ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™
Hidden units in FFN
Dropout rate
Optimizer
Initial learning rate
Decay rate
Max epoch num
Early stop epochs
Training Batch set
Testing Beam size
Max src. vocab size
Max tgt. vocab size
Max Java training code len.
Max Python training code len.
Max Java training summary len.
Max Python training summary len.

6
8
64
64
512
2048
0.2
Adam
0.0001
0.99
200
20
30
4
50000
30000
150
400
50
50

5 EXPERIMENT
We evaluate our model on two code summarization datasets. We
perform experiments on the hyperparameters, model structure,
variants of the graph structure and perform a user study. Our exper-
iment settings are presented in Section 5.1. Results are presented
and analyzed in Section 5.2, 5.4 and 5.3.

5.1 Settings
The experiments are conducted on a Java dataset [17] and a Python
dataset [7]. Our preprocessed datasets are composed of the source
code, corresponding AST, and a text summary. We used JavaParser1
to extract the AST and javalang2 for parsing Java source code,
python ast3 to parse and get the AST for Python.

We chose a Transformer as our main comparison baseline which
achieved state of the art in the two datasets. Ahmad et al. [1] propose
a base model which is a vanilla sequence-to-sequence Transformer
and an extended model (full) that uses relative positional embed-
ding and a copy mechanism. We perform experiments where we
reproduce their results on our pre-processed datasets, using only
the source code tokens as input to their model. We additionally
compare our method with Hellendoorn et al. [15]. Since their graph
structure is not publicly available, we reproduced their method on
SCG for comparison. We also compare our method with the results
of other baselines reported in Ahmad et al. [1]. We use metrics
BLUE, ROUGE-L, and METEO that are also reported by all base-
lines. We apply the same hyper-parameters as Ahmad et al. [1]
which are listed in Table 1.
1 https://javaparser.org/
3 https://docs.python.org/3/library/ast.html

2 https://github.com/c2nes/javalang

For data preprocessing, the source code and summary data are
truncated if they exceed the maximum length. We also discard all
AST-nodes if their scope contains a discarded token-node. We set
a vocabulary size limit, and we store only the highest frequent
words. Words that are not in the vocabulary will be replaced by a
special token ğ‘ˆ ğ‘›ğ‘˜ğ‘›ğ‘œğ‘¤ğ‘›_ğ‘¤ğ‘œğ‘Ÿğ‘‘. Our methodology is consistent with
Ahmad et al. [1].

Table 2: Statistics of preprocessed datasets.

Statistics

Java

Python

Examples - Train
Examples - Validation
Examples - Test
Unique Function Tokens
Unique Summary Tokens
Avg. Function Length
Avg. Summary Length
Avg. AST Nodes
Avg. AST Edges

69593
8694
8689
66569
46859
120.29
17.73
50.30
45.89

64939
21605
21670
104839
64898
132.64
9.56
70.10
68.16

The statistics of our pre-processed datasets are shown in Table 2.
Despite the difference in implementation, we kept our methodology
consistent with Ahmad et al. [1]. We used the same ğ¶ğ‘ğ‘šğ‘’ğ‘™ğ¶ğ‘ğ‘ ğ‘’ and
ğ‘ ğ‘›ğ‘ğ‘˜ğ‘’_ğ‘ğ‘ğ‘ ğ‘’ tokenizer from Ahmad et al. [1] to preprocess source
code data in both Java and Python datasets. SCG for subtokens can
be found in Figure 5. We also replace the strings and numbers in the

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

(a)

(b)

(c)

(d)

Figure 4: Survey participants were asked to choose the best description for a given code snippet. Comparison between GN-
Transformer (â€œOursâ€), Hellendoorn et al. [15] (â€œGREATâ€), Ahmad et al. [1] (â€œTransformer (full)â€) and Vaswani et al. [33] (â€œTrans-
formerâ€). Survey results were analyzed by the demographic distribution of participants on (a) self-reported Java expertise (b)
self-reported Python expertise (c) highest level of education completed or currently in progress (d) years of programming
experience.

source code with â€˜âŸ¨ğ‘†ğ‘‡ ğ‘…âŸ©â€™ and â€˜âŸ¨ğ‘ğ‘ˆ ğ‘€âŸ©â€™. For the code summary, we
use the raw corpus for the Java dataset and use the same method
with Wan et al. [36] to process code summaries in the Python
dataset. We use the train/valid/test split from the original corpus
for the Java dataset and we split the Python dataset by 6:2:2. All of
the above are consistent with Ahmad et al. [1].

There are two differences between our preprocessed dataset and

Ahmad et al. [1]:

1. Data cleaning - We discard the samples that cannot be parsed
by the compiler. There are 160 out of 87136 and 4894 out of 113108
samples in Java and Python datasets that are discarded, respectively.
In contrast, there are no samples discarded by Ahmad et al. [1] for
the Java dataset. For the Python dataset, Ahmad et al. [1] follow
the same cleaning process by Wei et al. [41] and discard 20563 out
of the 113108 samples that exceed the length threshold.

2. Python Dataset - We used the same methodology for prepro-
cessing the Python dataset as we did for Java. In contrast, Ahmad
et al. [1] delete special characters in the Python source code while
we preserve them. As a consequence, the average code length of
their preprocessed dataset is 47.98 while ours is 132.64.

Apart from the above two differences, the preprocessing method-
ology is consistent with Ahmad et al. [1] as well as other baselines
reported by them.

5.2 Code Summarization
Table 3 shows the comparisons of our model GN-Transformer with
previous works for code summarization. Our method outperforms
all previous works in all metrics. The most suitable comparison
of our approach with all previous works is that of a vanilla Trans-
former rather than a Transformer (full). Our work is best compared

(a)

(b)

Figure 5: Example of constructing edge connections for
subtokens. Edges between other tokens and AST are omitted.
Figure shows methodology for connecting shortcut edges
for SCG Variant 1 for the subtoken-nodes. (a) Connection
for original token-node. (b) Connection for subtoken-nodes.
All subtoken-nodes copy the same edge from the original
token-node.

with a vanilla Transformer by Ahmad et al. [1] as the main differ-
ence between the two architectures is fusing source code with a
Graph as opposed to using only the code sequence, which is the
scope of this paper. Transformer (full) makes use of additional im-
provements relevant only to sequence-to-sequence models such as
positional embedding and copy mechanism introduced by See et al.
[28], Shaw et al. [29]. Such improvements do not directly apply to
a GN-Transformer Encoder. We discuss potential interpretations
of positional information on a graph in Section 6.2 but leave any
further analysis in the context of GN as future work.

The result shows an improvement of 1.49, 2.09 BLEU, 0.51, 1.10
METEOR, and 1.99, 2.34 ROUGE-L in Java and Python datasets

Table 3: Overall results on Java and Python datasets.

Models

BLEU METEOR ROUGE-L

CODE-NN [19]
Tree2Seq [32]
RL+Hybrid2Seq [36]
API+CODE [17]
DeepCom [16]
Dual Model [40]
GREAT [15]
Transformer [1]
Transformer (full) [1]
Ours

CODE-NN [19]
Tree2Seq [32]
RL+Hybrid2Seq [36]
API+CODE [17]
DeepCom [16]
Dual Model [40]
GREAT [15]
Transformer [1]
Transformer (full) [1]
Ours

Java
27.60
37.88
38.22
41.31
39.75
42.39
44.05
43.99
44.84
45.48
Python
17.36
20.07
19.28
15.36
20.78
21.80
31.19
31.22
32.79
33.31

12.61
22.55
22.75
23.73
23.06
25.77
26.42
26.40
26.89
26.91

9.29
8.96
9.75
8.57
9.98
11.14
18.65
18.56
19.63
19.66

41.10
51.50
51.91
52.25
52.67
53.61
53.01
53.30
54.80
55.29

37.81
35.64
39.34
33.65
37.35
39.45
43.75
44.22
46.51
46.56

respectively. Although the improvement appear to be marginal
and incremental it is consistent with improvement introduced by
previous work. We find that the small improvement in the auto-
matic metrics reported result in significant improvement for human
evaluation metrics as concluded from the experiments in Section
5.5.

5.3 Ablation Study on SCG
We perform an ablation study on two variants of SCG to study the
effectiveness of the SCG structure, results are discussed in Section
5.4. In Variant 1 of SCG, we introduce shortcut edges between each
AST-node and the token-nodes that are related to this AST-node.
The distance between AST-nodes and all nodes within their scope is
shortened to 1. It does not break any isolation among token-nodes
but introduces additional direct interactions between token-nodes
and AST-nodes. We define the related token-nodes of an AST-node
as all token-nodes within the scope of the AST-node.

In Variant 2 of SCG, we make the token-nodes fully connected,
which can be regarded as identical to the information passing in a
Transformer. The nodes are not isolated from each other anymore
and can interact with each other. The difference with a vanilla
Transformer is that we have supplementary AST information.

5.4 Ablation study results on Java dataset
We conduct an ablation study on three aspects of the model design
using the Java dataset. Results are shown in Table 4.

(A) Hyperparameters. Experiments on different combinations of
hyperparameters including the number of layers, embedding size,
width of FFN, and configurations of attention heads.

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

(B) Model structure. We test the effectiveness of the Node block.
We use a 2-hop GN-Transformer block with two MHA sublayers in
each block thus aggregating two hops of information. The results
show that the lack of Node aggregate function harms the perfor-
mance. Collecting two hops of information without an FFN is less
expressive. To determine the effectiveness of the GN-Transformer
block, we replace it with a Graph Attention Networks (GAT) layer
using the same hyperparameters as that of our base model. GAT did
not perform optimally in our problem when modeled text sequence
as a graph. The results show that the GN-Transformer block largely
outperforms GAT for configurations with similar parameters as in
(A). We add an FFN to GAT layers for an ablation study on MHA. Re-
sults show that MHA in combination with FFN brings a significant
improvement. We thus conclude that both the FFN and MHA used
by our GN-Transformer blocks are necessary and greatly improve
performance.

(C) Variants of graph structures. We tested two variants of graph
structures discussed in Section 5.3. Both variants underperform
when compared to an SCG. The performance of Variant 1 is much
closer to standard SCG compared to Variant 2. Variant 1 introduces
redundant edges and leads to redundant interactions between the
AST and tokens. It slightly degrades performance since the struc-
tural information and isolation among tokens introduced by the
AST are preserved. The results demonstrate the effectiveness of our
design of only connecting token-nodes with their direct parent in
AST instead of all AST-nodes within their scope. For Variant 2, the
AST code sequence tokens are fully connected. Isolation among
tokens is lost, which leads to the loss of structural information. The
results show the effectiveness of isolation and structural informa-
tion introduced by SCG. We discuss possible future directions for
improvement in Section 6.1.

Our experiments can provide an explanation as to why Ahmad
et al. [1] did not improve performance when using SBT [16]. In
an SBT the AST-nodes can be regarded as fully connected. The
structural information from the AST graph is not preserved, which
is similar to Variant 2. Moreover, SBT introduces redundant inter-
actions between AST-nodes and token-nodes similar to Variant 1.
However, Variant 2 still outperforms a vanilla Transformer by 0.43
on BLEU, 0.47 on METEOR, and 0.78 on ROUGE-L.

5.5 Expert User Study
We make a survey publicly available through an email list to uni-
versity students and faculty in the Computer Science department
as well as virtual anonymous communities on topics related to
programming. We ask each participant to select the description
that best match the functionality of a code snippet. The survey
question is meant to evaluate two modalities of human perceived
quality (comprehension of summary) as well as technical accuracy
of the summary. For each snippet we provide a user with the code
summary generated by GN-Transformer, Hellendoorn et al. [15],
Transformer and Transformer (full) by Ahmad et al. [1]. We use an
online survey tool4 to track and verify the responses. We ask par-
ticipants for demographic information followed by a randomized

4 https://www.qualtrics.com/

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

Table 4: Additional experimental results. Unlisted values are identical to the base model. Parameter number not including the
embedding layer.

ğ‘ ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™

ğ‘‘ğ‘“ ğ‘“

512

2048

base

6
2
4
8

ğ»

8

1
4
16
32

ğ‘‘ğ‘˜

64

ğ‘‘ğ‘£

64

512
128
32
16

512
128
32
16

256
1024

1024
4096

2-hop GN-Transformer block
GAT
GAT with FFN
Variant 1
Variant 2

(A)

(B)

(C)

BLEU METEOR ROUGE-L

45.48
37.37
43.41
45.76
43.13
44.79
45.52
45.50
40.01
46.19
43.99
45.82
43.90
38.51
40.60
45.25
44.42

26.91
20.66
24.86
27.18
24.77
26.20
27.00
27.11
22.35
28.02
25.40
27.63
25.24
21.14
22.71
26.56
26.87

55.29
49.20
53.62
55.59
53.24
54.59
55.39
55.49
51.16
55.86
54.34
55.71
53.49
48.39
50.05
55.11
54.08

params
(Ã—106)
44.1
14.7
29.4
58.9

22.1
88.2
31.5
69.3
50.4
26.8
39.4

selection of code snippets and collect 790 responses from 330 par-
ticipants. Example of a survey question can be found in Appendix
Figure A1.

Figure 4 shows a bar chart with the frequency at which each
model was selected. Test of proportions show statistical signifi-
cant difference between the three models with p-value < 0.05. The
survey results agree with the reported automatic metrics (BLEU,
METEOR,ROUGE-L) and correlate with the human perceived qual-
ity and accuracy. GN-Transformer outperform all other baselines
and an annotator score of 37% compared to 17% 20% and 26% for
â€œGREATâ€, â€œTransformerâ€ and â€œTransformer (full)â€ respectively. Ran-
domly uniform demographic distribution among participants for
every model show that there was no influence by the level of expe-
rience, expertise or educational background.

We conclude that an incremental improvement in automatic
metrics can have a significant difference in human perceived quality
and accuracy.

6 DISCUSSION
6.1 Discussion on graph structure
An SCG is built on top of an AST structure generated by a parser.
Thus an SCG does not introduce any additional assumptions and
uses only the grammatical knowledge from the compiler. While
SCG is a noise-free representation, the inductive bias is introduced
by the static edge relationships of an AST. This inflexibility can
pose an obstacle in learning more complex relationships from code.
Qualitatively, we observed two factors of SCG that affect the
performance in the code summarization task. The first factor is
the long-range dependencies present in code. It is difficult to pass
information between nodes separated by long paths in SCG. As
discussed in 3.1, each GN block only implements one round of infor-
mation propagation. In Figure 3(b), it would require three rounds for

Table 5: Experimental results on positional encoding.
APE/RPE (token) means only applied APE/RPE on token-
nodes. Parameter number not including the embedding lay-
ers.

Models

BLEU METEOR ROUGE-L

Java

base
APE
APE (token)
RPE
RPE (token)

45.48
44.18
44.42
45.32
45.07

26.91
25.68
25.81
27.08
26.58

Python

base
APE
APE (token)
RPE
RPE (token)

33.33
29.40
31.40
32.49
31.55

19.67
17.08
18.37
19.62
19.09

55.29
54.41
54.59
55.33
55.12

46.57
43.17
45.10
46.30
45.70

information to propagate from the leaf node â€˜aâ€™ to root node â€˜Assign-
Exprâ€™. This is equivalent to at least three GN-Transformer blocks
used sequentially. Thus, it is difficult for information to propagate
between nodes in a large AST which may require a large number
of GN-Transformer blocks. The long-term dependency problem
could be ameliorated by designing additional direct edges between
AST-nodes and token-nodes.

The second factor that affects performance is the isolation be-
tween token-nodes. Our ablation study of Variant 2 shows the
advantage of introducing isolation between the token-nodes when
compared to fully connected nodes. Neither can be the optimal
solution. In a fully connected source code sequence, a token-node
will have as neighbors the preceding and proceeding tokens. In

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

512 and a fixed number of 6 anchor sets with 2 copies each instead
of an adjusted anchor set number in You et al. [44]. We concatenate
RPE and node embeddings fetched from the input embedding layer.
Experiments noted as RPE (token) apply RPE only on toke-nodes
with padding on AST-nodes.

The results show that APE will harm the performance of both
the Java and Python datasets. When applying APE only in token-
nodes, the degradation is minor. We hypothesize that APE is not
useful for AST-nodes. APE in token-nodes represents their absolute
positions in the input sequence. On the contrary, the positions of
AST-nodes in the input sequence should be the scope in relation to
the token-node. The absolute positional information is not useful
in a standard SCG for the AST-nodes.

The results for RPE are also not promising in the interpretation
we made for graph representations. We chose a fixed number of
anchor sets for all graphs. In You et al. [44], they dynamically chose
anchor set numbers for different sizes of graphs. We hypothesize
that this approach is limited by the insufficient anchor set numbers
in large graphs. Consider that the average node number of the
Python dataset is 70.10 compared to 50.30 in the Java dataset. This
can explain the reason RPE improves the performance in the Java
dataset but there is performance degradation for the Python dataset
when compared with a base model.

The results motivate future work to find a suitable positional

representation in the context of Graph Networks.

7 CONCLUSION
In this paper, we analyze the fusion of a sequence and a graph
from a novel perspective of Graph Networks. We propose a GN-
Transformer and Syntax-Code Graph. Our method achieve state-
of-the-art in two code summarization datasets. We perform experi-
ments on hyperparameters, model structure, and graph structure.
We perform the largest to date expert-user study on human per-
ceived quality and accuracy for a code summarization dataset to
corroborate with automatic metrics. Future work include finding a
method to ameliorate the long-term dependency problem of SCG
structure without introducing noise and an effective way of rep-
resenting the positional information of nodes in a graph. Due to
the similarity with Transformer, ideas like masked pretraining can
also be interpreted and implemented in the context of Graph Net-
works. A decoder interpretation for graphs could also improve
performance. In our method, we discard AST-node embeddings
thus losing some structural information. Our method could be used
in any domain where there is a duality of a sequence and graph
representations.

REFERENCES
[1] Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. 2020.
A Transformer-based Approach for Source Code Summarization. In Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).
[2] Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. 2018. Learning
to Represent Programs with Graphs. In International Conference on Learning
Representations. https://openreview.net/forum?id=BJOFETxR-

[3] Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating
Sequences from Structured Representations of Code. In International Conference
on Learning Representations. https://openreview.net/forum?id=H1gKYo09tX
[4] Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. Code2vec: Learn-
ing Distributed Representations of Code. Proc. ACM Program. Lang. 3, POPL,
Article 40 (Jan. 2019), 29 pages. https://doi.org/10.1145/3290353

Figure 6: The average code length of the test set of
Java dataset on thresholds under different sentence BLEU,
ROUGE-L, and METEOR score. The performance of the
model degrades across all metrics when larger code snippets
are used.

an SCG token-nodes can interact only indirectly with each other
through AST-nodes. Consider the expression â€˜5*câ€™ in Figure 3(b) as
an example. There is no direct edge between token-nodes â€˜5â€™ and
â€˜câ€™. For information to propagate between the two nodes, it has to
interact with AST-node â€˜AdditiveExprâ€™.

Longer code contains long-range dependencies and further iso-
lated token-nodes that correspond to our two observations on
the SCG. We analyzed the BLEU, ROUGE-L, and METEOR scores
achieved by the model in relation to the code snippet length on a
test set of the Java dataset, the results can be found in Figure 6. The
results show that introducing interaction between nodes without
discarding isolation is an important future direction for our work.

6.2 Positional encoding for graph
Unlike sequence models like RNN and LSTM, Transformers do not
have an intrinsic mechanism to encode the positional information.
There are positional encoding methods proposed in the vanilla
Transformer. However, the implementation of such methods can
not be expanded directly to a graph. The nodes and edges of a graph
are permutation invariant. It is not trivial representing position
information in the graph. We did two experiments on positional
encodings for a graph. The first experiment uses the Absolute Posi-
tional Embedding directly applied as in [33]. The second experiment
uses a Relative Positional Encoding based on PGNN [44].

Table 5 shows the experimental results. We applied a learnable
Absolute Positional Embedding (APE) layer, we use the summation
of APE and node embeddings fetched from the input embedding
layer as the input to the encoder. We tested APE on all nodes and
APE (token) only on token-nodes.

For the experiments on Relative Positional Encoding (RPE) on
our model, we had to adapt the original definition for sequence
models to that of a graph representation. When RPE [29] is applied
to sequences, it requires the sequence nodes to be fully connected
since the relative position is modeled in relation to an attribute on
a direct edge between two token nodes, which is not the case for
SCG. Instead, we apply a two-layered PGNN [44] that learns relative
positional information on a graph to implement RPE for each node.
We used an APE as the input to PGNN. We set an embedding size of

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

[5] T. BaltruÅ¡aitis, C. Ahuja, and L. Morency. 2019. Multimodal Machine Learning:
A Survey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine
Intelligence 41, 2 (2019), 423â€“443.

[6] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for
MT evaluation with improved correlation with human judgments. In Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for machine
translation and/or summarization. 65â€“72.

[7] Antonio Valerio Miceli Barone and Rico Sennrich. 2017. A parallel corpus of
Python functions and documentation strings for automated code documentation
and code generation. In The 8th International Joint Conference on Natural Language
Processing (IJCNLP 2017), Vol. 2. 314â€“319.

[8] Peter Battaglia, Jessica Blake Chandler Hamrick, Victor Bapst, Alvaro Sanchez,
Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam
Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andy Ballard, Justin
Gilmer, George E. Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Vic-
toria Jayne Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet
Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. 2018. Re-
lational inductive biases, deep learning, and graph networks. arXiv (2018).
https://arxiv.org/pdf/1806.01261.pdf

[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the
role of BLEU in machine translation research. In 11th Conference of the European
Chapter of the Association for Computational Linguistics.

[10] Michael B. Chang, Tomer Ullman, Antonio Torralba, and Joshua B. Tenenbaum.
2017. A Compositional Object-Based Approach to Learning Physical Dynamics.
arXiv:1612.00341 [cs.AI]

[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,
Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
arXiv:2010.11929 [cs.CV]

[12] Zhipeng Gao, Xin Xia, John Grundy, David Lo, and Yuan-Fang Li. 2020. Generat-
ing question titles for stack overflow from mined code snippets. ACM Transactions
on Software Engineering and Methodology (TOSEM) 29, 4 (2020), 1â€“37.

[13] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E.
Dahl. 2017. Neural Message Passing for Quantum Chemistry. In Proceedings of
the 34th International Conference on Machine Learning - Volume 70 (Sydney, NSW,
Australia) (ICMLâ€™17). JMLR.org, 1263â€“1272.

[14] William L. Hamilton, Zhitao Ying, and J. Leskovec. 2017. Inductive Representation

Learning on Large Graphs. In NIPS.

[15] Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and
David Bieber. 2020. Global Relational Models of Source Code. In International
Conference on Learning Representations.
https://openreview.net/forum?id=
B1lnbRNtwr

[16] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin. 2018a. Deep Code Comment Generation. In
2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC).
200â€“20010.

[17] Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. 2018b. Summarizing
Source Code with Transferred API Knowledge. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial Intelligence, IJCAI-18. Inter-
national Joint Conferences on Artificial Intelligence Organization, 2269â€“2275.
https://doi.org/10.24963/ijcai.2018/314

[27] Bryan Perozzi, Rami Al-Rfou, and S. Skiena. 2014. DeepWalk: online learning of

social representations. In KDD â€™14.

[28] Abigail See, Peter Liu, and Christopher Manning. 2017. Get To The Point: Sum-
marization with Pointer-Generator Networks. In Association for Computational
Linguistics. https://arxiv.org/abs/1704.04368

[29] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with
Relative Position Representations. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 2 (Short Papers). Association for Computational
Linguistics, New Orleans, Louisiana, 464â€“468. https://doi.org/10.18653/v1/N18-
2074

[30] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Man-
ning, Andrew Ng, and Christopher Potts. 2013. Recursive Deep Models for
Semantic Compositionality Over a Sentiment Treebank. In Proceedings of the
2013 Conference on Empirical Methods in Natural Language Processing. Asso-
ciation for Computational Linguistics, Seattle, Washington, USA, 1631â€“1642.
https://www.aclweb.org/anthology/D13-1170

[31] Sean Stapleton, Yashmeet Gambhir, Alexander LeClair, Zachary Eberhart, Westley
Weimer, Kevin Leach, and Yu Huang. 2020. A Human Study of Comprehension
and Code Summarization. In Proceedings of the 28th International Conference on
Program Comprehension. 2â€“13.

[32] Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved
Semantic Representations From Tree-Structured Long Short-Term Memory Net-
works. In Proceedings of the 53rd Annual Meeting of the Association for Computa-
tional Linguistics and the 7th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers). Association for Computational Linguistics,
Beijing, China, 1556â€“1566. https://doi.org/10.3115/v1/P15-1150

[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Atten-
tion is All You Need. In Proceedings of the 31st International Conference on Neural
Information Processing Systems (Long Beach, California, USA) (NIPSâ€™17). Curran
Associates Inc., Red Hook, NY, USA, 6000â€“6010.

[34] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2018. Graph Attention Networks. International Con-
ference on Learning Representations (2018). https://openreview.net/forum?id=
rJXMpikCZ accepted as poster.

[35] Petar VeliÄkoviÄ‡. 2019. The resurgence of structure in deep neural networks. Ph.D.

Dissertation. https://doi.org/10.17863/CAM.39380

[36] Y. Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu. 2018. Improving
Automatic Source Code Summarization via Deep Reinforcement Learning. In
2018 33rd IEEE/ACM International Conference on Automated Software Engineering
(ASE). 397â€“407.

[37] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin. 2020. Detecting Code Clones with
Graph Neural Network and Flow-Augmented Abstract Syntax Tree. In 2020 IEEE
27th International Conference on Software Analysis, Evolution and Reengineering
(SANER). 261â€“271.

[38] X. Wang, R. Girshick, A. Gupta, and K. He. 2018. Non-local Neural Networks. In
2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7794â€“7803.
[39] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and
Justin M. Solomon. 2019. Dynamic Graph CNN for Learning on Point Clouds.
arXiv:1801.07829 [cs.CV]

[18] Xuan Huo, M. Li, and Zhi-Hua Zhou. 2020. Control Flow Graph Embedding

[40] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as a Dual

Based on Multi-Instance Decomposition for Bug Localization. In AAAI.

Task of Code Summarization. arXiv preprint arXiv:1910.05923 (2019).

[41] Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019. Code Generation as Dual

Task of Code Summarization. NeurIPS (2019), 6559â€“6569.

[42] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful

are graph neural networks? arXiv preprint arXiv:1810.00826 (2018).

[43] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Graph convolutional net-
works for text classification. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 33. 7370â€“7377.

[44] Jiaxuan You, Rex Ying, and J. Leskovec. 2019. Position-aware Graph Neural

Networks. In ICML.

[19] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.
Summarizing Source Code using a Neural Attention Model. In Proceedings of
the 54th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers). Association for Computational Linguistics, Berlin, Germany,
2073â€“2083. https://doi.org/10.18653/v1/P16-1195

[20] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with

Graph Convolutional Networks. (2017). arXiv:1609.02907 [cs.LG]

[21] Alex LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved Code
Summarization via a Graph Neural Network. In 2020 IEEE/ACM International
Conference on Program Comprehension.

[22] Guohao Li, Matthias MÃ¼ller, Ali Thabet, and Bernard Ghanem. 2019. DeepGCNs:

Can GCNs Go as Deep as CNNs? arXiv:1904.03751 [cs.CV]

[23] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph
convolutional networks for semi-supervised learning. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 32.

[24] Feifan Liu and Yang Liu. 2008. Correlation between rouge and human evaluation
of extractive meeting summaries. In Proceedings of ACL-08: HLT, short papers.
201â€“204.

[25] Xuan-Phi Nguyen, Shafiq Joty, Steven Hoi, and Richard Socher. 2020. Tree-
Structured Attention with Hierarchical Accumulation. In International Conference
on Learning Representations. https://openreview.net/forum?id=HJxK5pEYvr
[26] Sheena Panthaplackel, Pengyu Nie, Milos Gligoric, Junyi Jessy Li, and Raymond J
Mooney. 2020. Learning to update natural language comments based on code
changes. arXiv preprint arXiv:2004.12169 (2020).

protected static void quickSort ( Instances insts , int [] indices , int

def _mergeOptions(inputOptions, overrideOptions ) :

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

attidx , int

left , int right ) {

{

( left < right )
int middle= partition ( insts , indices , attidx , left , right ) ;
quickSort ( insts , indices , attidx , left , middle) ;
quickSort ( insts , indices , attidx , middle + 1, right ) ;

if

}

}

Reference: sorts the instances according to the given attribute/di-
mension. the sorting is done on the master index array and not on
the actual instances object.

Ours: sorts the specified range of the array using the specified

items

Transformer: src the ordinal field array into ascending order
Transformer (full): sorts the specified range of the array using

the given workspace array .

def is_power2(num):

return ( isinstance (num, numbers.Integral ) and (num > 0) and

(not (num & (num âˆ’ 1))))

Reference: test if num is a positive integer power of 2 .
Ours: return true if the power of 2 .
Transformer: returns true if and number is a user-defined

power .

Transformer (full): return whether or not the argument is a

power .

if inputOptions . pickledOptions :

try :

inputOptions =

base64unpickle (inputOptions . pickledOptions )

except Exception as ex:

errMsg = (``provided invalid value `%s' for option

`âˆ’âˆ’pickledâˆ’options' " %
inputOptions . pickledOptions )

errMsg += ((``(`%s ') " % ex)
raise SqlmapSyntaxException(errMsg)

if ex.message else `' )

if inputOptions . configFile :

configFileParser (inputOptions . configFile )

if hasattr (inputOptions , `items ' ) :

inputOptionsItems = inputOptions . items ()

else :

inputOptionsItems = inputOptions . __dict__ . items ()

for (key, value ) in inputOptionsItems:

if

(( key not in conf) or ( value not in (None, False ) )

or overrideOptions ) :

conf[key] = value
for (key, value ) in conf . items () :
( value is not None):
kb. explicitSettings . add(key)
for (key, value ) in defaults . items () :

if

if

( hasattr (conf , key) and (conf[key] is None)):
conf[key] = value

_ = {}
for (key, value ) in os . environ . items () :

if key.upper() . startswith (

SQLMAP_ENVIRONMENT_PREFIX):
_[key[len (SQLMAP_ENVIRONMENT_PREFIX):]

. upper() ] = value

types_ = {}
for group in optDict . keys () :

types_ . update(optDict[group])

for key in conf :

if

(( key.upper() in _) and (key in types_) ) :
value = _[key.upper() ]
if

(types_[key] == OPTION_TYPE.BOOLEAN):
try :

value = bool( value )

except ValueError :
value = False

elif

(types_[key] == OPTION_TYPE.INTEGER):
try :

value = int ( value )

except ValueError :
value = 0

elif

(types_[key] == OPTION_TYPE.FLOAT):
try :

value = float ( value )

except ValueError :
value = 0.0

conf[key] = value
mergedOptions.update(conf)

Reference: merge command line options with configuration file

and default options .

Ours: merges options from a config file .

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

Transformer: merges all of the data used into an option .
Transformer (full): loads configuration attributes and add at-

tributes .

Ours: loads the header data necessary for instantiating .
Transformer: loads a data necessary for credit two types .
Transformer (full): loads key: data from a loader_context .

def LoadFromString(yaml_doc, product_yaml_key,

required_client_values , optional_product_values ) :
(_PY_VERSION_MAJOR == 2):

if

if

(( _PY_VERSION_MINOR == 7) and (_PY_VERSION_MICRO

< 9)):

_logger . warning(_DEPRECATED_VERSION_TEMPLATE,

_PY_VERSION_MAJOR, _PY_VERSION_MINOR,
_PY_VERSION_MICRO)
(_PY_VERSION_MINOR < 7):
_logger . warning(_DEPRECATED_VERSION_TEMPLATE,

elif

_PY_VERSION_MAJOR, _PY_VERSION_MINOR,
_PY_VERSION_MICRO)

data = (yaml. safe_load (yaml_doc) or {})
try :

product_data = data[product_yaml_key]

except KeyError:

raise googleads . errors . GoogleAdsValueError((`The ``%s"
configuration is missing ' % (product_yaml_key,)) )

if

isinstance (product_data , dict ) ) :

(not
raise googleads . errors . GoogleAdsValueError((`The ``%s"

configuration is empty or invalid ' %
(product_yaml_key,)) )

IncludeUtilitiesInUserAgent (

data . get (_UTILITY_REGISTER_YAML_KEY, True))

original_keys = list (product_data . keys () )
client_kwargs = {}
try :

for key in required_client_values :

client_kwargs [key] = product_data[key]
del product_data[key]

except KeyError:

raise googleads . errors . GoogleAdsValueError((`Some of the

required values are missing . Required values are :
%s, actual values are %s' % ( required_client_values ,
original_keys ) ) )

proxy_config_data = data . get (_PROXY_CONFIG_KEY, {})
proxy_config = _ExtractProxyConfig(product_yaml_key,

proxy_config_data)

client_kwargs[`proxy_config ' ] = proxy_config
client_kwargs[` oauth2_client ' ] =

_ExtractOAuth2Client(product_yaml_key, product_data ,
proxy_config)

client_kwargs [ENABLE_COMPRESSION_KEY] =

data.get(ENABLE_COMPRESSION_KEY, False)

if

for value in optional_product_values :
( value in product_data) :
client_kwargs [value] = product_data[value]
del product_data[value]

if product_data :

warnings.warn((`Could not recognize the following keys:
%s. They were ignored. ' % (product_data ,) ) ,
stacklevel =3)

return client_kwargs

Reference: loads the data necessary for instantiating a client

from file storage .

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

Figure A1: Randomly selected expert-user question sample.

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

8 ATTENTION VISUALIZATION FOR

SYNTAX-CODE GRAPH

The attention visualization for this short program is shown here,
we could get more insights about how each nodes paid attention to
their neighboring nodes:

public StartListener (Object resource )

{

resource=resource ;

}

The figures show attention visualization for standard SCG. We
marked the edges within AST-nodes by blue, edges between AST-
nodes and token-nodes by red. Each figure shows the attention
visualization of the nodes in each layer of AST.

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

GN-Transformer: Fusing Sequence and Graph Representation for Improved Code Summarization

Junyan Cheng, Iordanis Fostiropoulos, and Barry Boehm

