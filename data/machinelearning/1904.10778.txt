Some Limit Properties of Markov Chains Induced by Recursive Stochastic
Algorithms∗

Abhishek Gupta, Hao Chen, Jianzong Pi, Gaurav Tendolkar†

Abstract. Recursive stochastic algorithms have gained signiﬁcant attention in the recent past due to data
driven applications. Examples include stochastic gradient descent for solving large-scale optimization
problems and empirical dynamic programming algorithms for solving Markov decision problems.
These recursive stochastic algorithms approximate certain contraction operators and can be viewed
within the framework of iterated random operators. Accordingly, we consider iterated random
operators over a Polish space that simulate iterated contraction operator over that Polish space.
Assume that the iterated random operators are indexed by certain batch sizes such that as batch
sizes grow to inﬁnity, each realization of the random operator converges (in some sense) to the
contraction operator it is simulating. We show that starting from the same initial condition, the
distribution of the random sequence generated by the iterated random operators converges weakly to
the trajectory generated by the contraction operator. We further show that under certain conditions,
the time average of the random sequence converges to the spatial mean of the invariant distribution.
We then apply these results to logistic regression, empirical value iteration, and empirical Q value
iteration for ﬁnite state ﬁnite action MDPs to illustrate the general theory develop here.

Key words. Stochastic Gradient Descent, Empirical Dynamic Programming, Constant Stepsize Q learning,

Iterative Random Maps, Feller Markov Chains

AMS subject classiﬁcations. 93E35, 60J20, 68Q32

1. Introduction. There has been a surge of interest in using randomization to reduce
computational burden in machine learning and reinforcement learning. For instance, in train-
ing neural networks with a large amount of data, stochastic gradient descent is frequently
employed instead of the usual gradient descent. In data-driven Markov decision problems,
empirical dynamic programming and generative models have been employed to determine ap-
proximately optimal policies and value functions. In these algorithms, instead of computing
the expected value of certain functions at each step of the iteration, one computes the em-
pirical expected value that is rather easy to compute if enough data is available. This simple
trick reduces the runtime to determine a reasonably good solution.

It turns out that the outputs of these recursive stochastic algorithms (RSAs) can be viewed
as Markov chains. Indeed, if the parameters of the algorithm do not change with iteration,
then the RSAs can be thought of as an iterated random operator acting onto certain spaces.
Consider, for instance, the case of stochastic gradient descent, where the stepsize remains
constant, data samples picked at every iteration are i.i.d., and the number of data samples

∗Submitted to the editors.
Funding: The authors gratefully acknowledge support from ARPA-E NEXTCAR Program, NSF ECCS Grant

1610615, and NSF CRII Award 1565487.

†Abhishek Gupta, Hao Chen, and Jianzong Pi is with the Electrical and Computer Engineering Department, The
Ohio State University, 2015 Neil Avenue, Columbus, OH 43210, USA. (gupta.706@osu.edu, chen.6945@osu.edu,
pi.35@osu.edu). Gaurav Tendolkar is with Microsoft Corp., 599 N Mathilda Avenue, Sunnyvale, CA 94085, USA.
gatendol@microsoft.com.

1

0
2
0
2

l
u
J

3
2

]

G
L
.
s
c
[

2
v
8
7
7
0
1
.
4
0
9
1
:
v
i
X
r
a

 
 
 
 
 
 
2

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

remain constant. Then, each step of the stochastic gradient descent algorithm can be viewed
as a random operator. To see this, let us consider the problem of minimizing a sum of N
functions, Li : Rn → R, i = 1, . . . , N :

min
x∈Rn

L(x) =

1
N

N
(cid:88)

i=1

Li(x).

In usual gradient descent, one ﬁxes a stepsize β > 0, and runs the iteration

yk+1 = yk − β∇xL(yk) =: T (yk),
where we used T : Rn → Rn to denote the exact gradient descent map. We note here that
under reasonable assumptions on L and β, T becomes a contraction operator under some
norm on Rn (usually (cid:96)2 norm is used).

In contrast, in stochastic gradient descent, the operator applied at every step of the al-
gorithm changes. At time step k of the stochastic gradient descent algorithm, let Nk :=
{i1, . . . , in} be the set of n indices that are sampled independently and uniformly from the set
of all indices {1, . . . , N }. Then, we have

k+1 = ˆzn
ˆzn

k −

β
n

(cid:88)

i∈Nk

∇xLi(ˆzn

k ) =: ˆT n

k (ˆzn

k ).

Since the set of (random) indices Nk is i.i.d., the operator ˆT n
and is “identically distributed”. This implies that the (random) sequence (ˆzn
chain.
gradient descent operator ˆT n
x ∈ Rn.

k is independent of the past maps
k )k∈N is a Markov
It should also be noted that the exact gradient descent operator T and stochastic
k (x) is a consistent estimate of the T (x) for any

k are related – ˆT n

A similar setup is considered in empirical dynamic programming using a generative model
for dynamic decision process. Consider a controlled Markov process in which s is the state
of a system and a is the action of the decision maker. Let p(s(cid:48)|s, a) denote the transition
probability of the next state being s(cid:48) given the current state s and action a and c(s, a) be
the corresponding cost. We use α to denote the discount parameter. In the value iteration
algorithm, one needs to compute E [v(s(cid:48))|s, a], where v is some real-valued function of the
state. This leads to the usual Bellman operator T that acts on the space of value functions
and is given by

T (v)(s, a) = min

a

(cid:20)
c(s, a) + αE (cid:2)v(s(cid:48))|s, a(cid:3)

(cid:21)

.

It is not diﬃcult to see that T is a contraction operator with the contraction coeﬃcient α,
when the space of value functions is endowed with the sup norm. If there is enough data,
one can replace E [v(s(cid:48))|s, a] with its “empirical” average, given by 1
i(s, a)), where
n
{s(cid:48)
i=1 are n samples of the next state given that the current state-action pair is (s, a).
Thus, the random Bellman operator ˆT n
k acts on the space of value functions and is given by

i(s, a)}n

i=1 v(s(cid:48)

(cid:80)n

(cid:34)

ˆT n
k (v)(s, a) = min

a

c(s, a) + α

(cid:35)
i(s, a))

v(s(cid:48)

.

1
n

n
(cid:88)

i=1

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

3

In this case as well, ˆT n
k (v) is a consistent estimate of the T (v) for any value function v. When
the state or action space is continuous, then the above operator also features an additional
function ﬁtting component. The analysis of such algorithms involves understanding the error
introduced due to function ﬁtting, as well as the number of samples used at every iteration.
In fact, in both of the examples above, it is readily observed that the original operator T
is a contraction map. This is no accident – contraction mapping theorem forms the bedrock
of most of the proofs of convergence algorithms used in optimization or MDP problems. Some
examples are noted below:

1. The Bellman operator for a class of stochastic shortest path problems is a contraction

operator under an appropriate weighted sup norm [9, 7].

2. The Bellman operator for a class of average cost MDPs is a contraction operator under

the span seminorm over the quotient space [63, 2].

3. Some variational inequality problems involve contraction operators under the usual 2

norm [29, p. 143].

4. The Bellman operator for continuous state MDP is a contraction operator over the
space of continuous and bounded functions over the state space of the MDP (it requires
a variety of assumptions as elucidated in [40, 41, 28]).

5. The resolvent of a strongly monotone operator is a contraction operator [65]. Many
other contraction operators used in the context of optimization algorithms are dis-
cussed in Section 5 of [65].

In data driven applications, computing the exact (contraction) operator is either computa-
tionally expensive or impossible. Thus, one has to use random mappings, that are drawn
independent of the past operators, and that simulate the eﬀect of contraction mapping. The
two examples explained above are merely instances of this more general methodology. The
primary purpose of the paper is to devise suﬃcient conditions on the relationship between
random maps ˆT n
k and the deterministic contraction operator T so that (a) the convergence
properties of new algorithms can be readily established, and (b) ﬁnd common threads between
the convergence.

1.1. Our Contribution. The primary contribution of this paper is to conceptually unify
the convergence analysis of certain RSAs in optimization, machine learning, and reinforcement
learning using the tools from the Markov chain theory. This is achieved by leveraging several
results available for convergence and stability for Feller Markov chains established in [19], [56,
Sec. 18.5], [17, Sec. 8], and [49]. Our key contributions are summarized below.

1. Consider the deterministic sequence yk = T ◦ . . . ◦ T (y0) (k compositions of the exact
k = ˆT n
operator T ) and the Markov chain ˆzn
0 = y0. It is natural
to assume that using the deterministic operator yields the best convergence guarantee.
We use independent samples of data to approximate T by ˆT n
k , then it introduces error
at every time step. How does this error accumulate? Most authors bound (cid:107)ˆzn
k − x∗(cid:107),
where x∗ is the ﬁxed point of the operator T . Instead, we are interested in the error
(cid:107)ˆzn

k − yk(cid:107), noting that triangle inequality yields

k−1 ◦ . . . ◦ ˆT n

0 ) with ˆzn

0 (ˆzn

(1.1)

(cid:107)ˆzn

k − x∗(cid:107) ≤ (cid:107)ˆzn

k − yk(cid:107) + (cid:107)yk − x∗(cid:107) ≤ (cid:107)ˆzn

k − yk(cid:107) + αk(cid:107)x0 − x∗(cid:107).

We derive a suﬃcient condition on the random operators ˆT n

k and its relationship with

4

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

the exact operator T so that the sequence of distributions over the sequence (ˆzn
k )k∈N
output by the RSA converges in weak* topology to the unit mass over the trajectory
(yk)k∈N output by the exact algorithm as the parameter n → ∞. This further implies
that ˆzn
k is close to yk with high probability. Using the inequality in (1.1) above, we
conclude that the error term (cid:107)ˆzn
k − x∗(cid:107) is less than αk(cid:107)x0 − x∗(cid:107) plus a small error with
high probability.
We further show that the suﬃcient condition is satisﬁed in a suﬃciently general class
of problems encountered in stochastic gradient descent for strongly convex loss func-
tions and synchronous empirical dynamic programming for MDPs with discounted
cost criterion. These examples serve to illustrate how to apply the results to derive
this property of an RSA.

2. Existence of invariant distributions of RSAs is an important property, as it implies
some form of stability of the algorithm. A Markov chain does not admit invariant
distribution if it features a cyclic behavior or diverges to inﬁnity (there could be other
reasons for the non-existence of an invariant distribution, but these two are more
common). In an RSA, we do not usually expect a cyclic behavior due to randomization.
Thus, if an RSA admits an invariant distribution, then it implies that the iterates will
not diverge with high probability. Thus, establishing the existence of an invariant
distribution is an important problem, which we address here.
We show that the Markov chains generated by many RSAs satisfy the weak Feller
1 )|ˆzn
property, that is, if f is a continuous and bounded function, then ˆzn
0 ]
is also a continuous and bounded function. The existence (and in some case, the
uniqueness) of an invariant measure of Feller Markov chains has been presented in
[17]. We apply these results to conclude that under some reasonable assumptions, the
chains generated by stochastic gradient descent and empirical dynamic programming
algorithms admit invariant distributions.
In certain cases, we can show that this
invariant distribution is unique.

0 (cid:55)→ E [f (ˆzn

3. There has been a sustained interest in using time-averages in stochastic gradient de-
scent and deep Q learning. Particularly, references [67, 55, 45, 54, 78] propose that
ﬁxing the stepsize in stochastic gradient descent algorithms and using the average of
the tail of the random sequence leads to a better performance of the trained algorithm.
Within the context of reinforcement learning, [3] and [82] propose averaging the deep
Q function iterates to arrive at a solution with lower variance.
Indeed, we show here that under some conditions on the random operators, the vari-
ance reduction property of time-average (or in these cases, tail average) is largely due
to the fact that the Markov chain output by RSA may be admitting a unique in-
variant distribution. This part leverages the law of large numbers for Markov chains,
presented in [19, 56].

4. We complement the theoretical contributions with numerical simulations of two RSAs
– stochastic gradient descent for logistic regression, empirical value iteration for dis-
counted MDP with a generative model, and synchronous batch Q value iteration for
discounted MDP.

While we present complete proofs of two of our main results stated here, we admit that
our proofs require minor tweaks of existing results in the literature. The need for presenting

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

5

the complete proofs are twofold: Our hypotheses diﬀer in some ways from the hypotheses
presented in the standard texts, particularly in [49], [56, Section 18.5], and [19]. Moreover,
to construct the complete proofs using these texts under our hypotheses require substantial
eﬀort on the part of the reader. To ease this burden, we chose to furnish the complete proofs
using the notation adopted here.

1.2. Previous Work. Convergence proofs of randomized optimization and learning algo-
rithms are usually obtained from speciﬁcally tailored arguments, which are not generalizable
to other settings. For instance, the convergence of stochastic gradient descent, stochastic
variance reduction gradient descent (SVRG), and stochastic average gradient (SAG) descent
follow a completely diﬀerent, and often involved, sequence of arguments [18, 14, 47, 36, 67, 24].
The argument usually starts with identifying some conditions on the functions, such that for
every iteration k, one can upper bound L(ˆzn
k ) − L(x∗) (where we used the notation introduced
above) by a function that decays as k grows. These tailored methods usually also yield the
convergence rates speciﬁc to those algorithms.

It would be conceptually elegant to determine a set of more general conditions which can
be readily applied to these algorithms and many of its variants to establish the asymptotic
convergence to the ﬁxed point of the map. The stochastic approximation theory is one such
elegant theory [16, 43, 50, 15]. There are two types of stochastic approximation algorithms –
one with decreasing (also called tapering or diminishing) stepsize and other one with constant
stepsize. In decreasing stepsize algorithms, the stepsize has to converge to 0 as the number of
iteration goes to inﬁnity (the stepsize is not summable, but is square summable). This leads
to the almost sure convergence guarantee to the ﬁxed point in the limit. For constant stepsize,
the theory says that the iterates will eventually enter a neighborhood of the ﬁxed point and
do a random walk within that neighborhood.

We now present a sample of decreasing stepsize RSAs whose convergence is ascertained
using the stochastic approximation theory. Under reasonable assumptions on the loss function,
stochastic gradient descent and distributed asynchronous gradient descent methods converge
almost surely to the optimal solution [79, 31, 32]. The convergence of reinforcement learning
algorithms usually invoke some version of the stochastic approximation theorem. Reference
[44, 80, 1, 10] studies the convergence of various types of Q learning algorithms developed for
discounted cost or average cost MDPs with ﬁnite state and action spaces. The convergence of
on-policy reinforcement learning algorithm SARSA is established in [74]. More recently, the
stochastic approximation theory has been used to establish the convergence of policy gradient,
temporal diﬀerence, and other related methods in [76, 90, 94, 91, 88, 89]. For more information
on various reinforcement learning algorithms and their convergence proofs, we refer the reader
to books [2, 77, 9] and recent survey papers [92, 93].

Decreasing stepsize RSAs do not yield approximate solutions in a reasonable time frame.
As a result, constant stepsize algorithms are gaining traction as a way to speed up the computa-
tion at the cost of tolerating a small error in the ﬁnal result; see, for example, [30, 58, 4, 27, 26],
where constant stepsizes are used in the context of the stochastic gradient descent-type algo-
rithms and [6, 75, 59, 68] for their usage in the reinforcement learning algorithms.

Constant stepsize stochastic approximation over ﬁnite dimensional state space has been
studied in [16, 43], where the authors derive the asymptotic concentration results. It is well-

6

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

understood that in stochastic gradient descent with constant stepsize, the sequence generated
by the algorithm gets closer to the optimal solution, but then does a random walk around
the optimal solution [26]. The closeness of the random walk to the optimal solution depends
on the number of random samples one uses at each iteration of the algorithm. Similarly, in
Q learning algorithm, constant stepsize Q learning has been studied in [6] (both synchronous
and asynchronous version are studied). Convergence of constant stepsize temporal diﬀerence
methods with linear function approximation is studied in [51, 23, 11, 35, 75]. Empirical value
iteration and their variants, studied in a number of works under varying assumptions [86, 87,
21, 57, 22, 46, 48, 37, 73], are also examples of constant stepsize stochastic approximation
algorithms.

When the stepsize is taken as a constant in a RSA, then the output of the RSA forms a
Markov chain. The goal of this paper is to study the limit properties of such a Markov chain.
There are two ways the limit can be taken. Either the sample size n used at every time step
can grow to inﬁnity or the number of iterations k can escape to inﬁnity. We study both the
limits in this paper. We note here that the generality of the model and minimal assumptions
do not allow us to derive a ﬁnite time guarantee, which has signiﬁcant importance in the
machine learning and the reinforcement learning communities. Further, our proof approach
is not algorithm-dependent. We leave these important problems for a future work.

Our work is largely motivated by the analysis of empirical dynamic programming in [37].
This work viewed empirical dynamic programs within the framework of iterated random op-
erators. It used stochastic dominance based arguments to derive bounds on the asymptotic
probability of error (between the random outputs of the algorithm and the optimal solution)
being large.
Inspired by this work, we extended the arguments to empirical relative value
iteration in [34]. We further relaxed some conditions on random operators assumed in [37]
in our follow up work [33]. The aim of this paper is to further expand the analysis and
present conditions on random operators and its relationship to the exact operator to arrive at
insightful conclusions about the random sequences generated by these RSAs.

1.3. Outline of the Paper. The paper is organized as follows. Section 2 presents a
common mathematical framework to study the problem of convergence and stability of Markov
chains induced by RSAs. We also state the three main questions we address. Section 3
presents some motivating examples where the mathematical framework we develop can be
applied. Through these examples, we also illustrate certain desirable properties that the
random operators enjoy. In Section 4, we show that the distributions over the trajectories
generated by RSA converges to the Dirac mass over the trajectory generated by the exact
algorithm. This constitutes the ﬁrst main result of the paper. In Section 5, we study some
suﬃcient conditions on the operators ˆT n
k such that the resulting Markov chain admits an
invariant distribution. We also study conditions under which the invariant distribution is
unique. Section 6 then introduces the assumptions and establishes the weak law of large
numbers for Markov chains. This constitutes the second main result of the paper. The proofs
of the two main results are presented in Sections 8 and 9. We ﬁnally conclude our discussion
in Section 10.

1.4. Notations. Let (A, ρ) be a Polish space, which is deﬁned as a complete separable
metric space with metric ρ. We use ℘(A) to denote the set of all probability distributions over

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

7

A. We use 1{a} ∈ ℘(A) to denote the Dirac mass over a ∈ A. The notations Cb(A) and Ub(A)
denote, respectively, the set of all continuous and bounded functions and uniformly continuous
and bounded functions over the set A. We use C(A) to denote the set of (possibly unbounded)
continuous functions over the set A. We say that a sequence of measures {µn}n∈N ⊂ ℘(A)
converges to µ in weak* sense iﬀ for every f ∈ Cb(A), (cid:82) f dµn → (cid:82) f dµ as n → ∞. This is
usually also referred to as weak convergence in probability theory literature.

We use 1{·} to denote an indicator function, which takes the value of 1 if {·} is true and
0 otherwise. By a slight abuse of notation, we also use 1F to be the indicator function over a
measurable set F ⊂ A.

2. Problem Formulation. Let (X , ρ) be a Polish space with metric ρ. Consider a contrac-
tion operator T : X → X with contraction coeﬃcient α ∈ (0, 1) and the unique ﬁxed point
denoted by x∗. This means

ρ(T (x1), T (x2)) ≤ αρ(x1, x2) and T (x∗) = x∗.

Starting from any initial point y0 ∈ X , deﬁne the iterates

(2.1)

yk = T (yk−1)

for k ∈ N.

By the Banach contraction mapping theorem, this iteration converges to x∗. In fact, we have

ρ(yk, x∗) ≤ αkρ(y0, x∗).

As discussed previously, in many instances, it is beneﬁcial or required in many iterative algo-
rithms to use randomization to evaluate an approximation of T (x) using a random operator.
We now formulate a framework to analyze the output of this RSA rigorously.

Let (Ω, F, P) be a standard probability space, where Ω is the set of uncertainties, F
is the Borel σ-algebra over Ω and P be the probability distribution function over Ω. Let
ˆT n
k : X × Ω → X be a random operator that is used at the kth iteration and is indexed by
a natural number n. The index n would capture, for instance, the stepsize, the number of
random samples used to approximate the operator T , etc. Although ˆT n
k is a function of ω ∈ Ω,
we will suppress this dependence for ease of exposition. Thus, ˆT n
k (x; ω). We make
the following assumption on the independence of the sequence of operators ˆT n
k .

k (x) := ˆT n

Assumption 2.1. For every x ∈ X , ˆT n

k (x) and ˆT n

k(cid:48)(x) are statistically independent and

identically distributed for k (cid:54)= k(cid:48).

k−1(ˆzn

2.1. Key Questions. Consider the stochastic process that starts from ˆzn

0 = y0 and deﬁne
k−1) for all k ∈ N. Due to Assumption 2.1 and the fact that n does not change
k }k∈N is a time-homogeneous Markov chain. One can view

k = ˆT n
ˆzn
with time, the stochastic process {ˆzn
ˆzn
k as an X -valued Markov chain with the Markov transition kernel given by
Qn(B|x) = P (cid:8)ˆzn

k−1 = x(cid:9)

k ∈ B|ˆzn

for any Borel set B ⊂ X . Note that Qn does not depend on the time index k, since what we
have here is a time-homogeneous Markov process.

8

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

2.1.1. Convergence of Distribution of Trajectories. We are interested in deriving con-
ditions on the random maps ˆT n
k under which the random sequence generated by RSA is close
to the deterministic sequence generated by exact algorithm with high probability. Let us for-
mulate the precise mathematical problem. We let µn ∈ ℘(X N) denote the joint distribution
2 , . . .). Endow X N with the product topology so that it becomes a
of the sequence (ˆzn
Polish space. Then, µn is deﬁned by

0 , ˆzn

1 , ˆzn

(2.2)

µn(B0 × B1 × B2 × . . .) =

(cid:90)

B0×B1×B2×...

1{y0}(dx0)Qn(dx1|x0)Qn(dx2|x1) . . . ,

where B0, B1, B2, . . . are Borel sets in X .

In the similar vein, one can also view the iterates (yk)k∈N deﬁned in (2.1) as a Markov
chain on the same probability space (Ω, F, P), with the distribution over this sequence deﬁned
by

(2.3)

ψ(B0 × B1 × B2 × . . .) =

(cid:90)

B0×B1×B2×...

1{y0}(dx0)1{y1}(dx1)1{y2}(dx2) . . . .

This is a Dirac mass on the sequence (y0, y1, . . .). Our ﬁrst result, stated in Section 4, proves
that under a mild assumption on the random operators ˆT n
k , the sequence of measures µn
converges in the weak* sense to ψ.

A similar setup was considered by Karr [49].

It studies the convergence properties of
a class of Feller Markov chains parametrized by n such that the transition probability Qn
converges in some sense to a transition probability Q as n → ∞. Although our assumptions
are slightly diﬀerent, the proof essentially imitates the one in [49] except for a couple of key
steps. We also discuss numerical implication of this result in Section 4.

2.1.2. Existence of Invariant Measures for Fixed n. For the Markov chain (ˆzn

k )k∈N, one
of the key questions is the existence of an invariant distribution. An invariant probability
k )k∈N is a probability measure πn such that for any Borel
distribution of the Markov chain (ˆzn
set B ⊂ X , we have

πn(B) =

(cid:90)

X

Qn(B|x)πn(dx).

A desirable property of a Markov chain is to have an invariant distribution, since it implies
that the RSA satisﬁes a form of stability. More importantly, it implies that the Markov chain
will not escape to inﬁnity with probability 1. There is a large body of literature that studies
the problem of the existence of invariant measures for Harris recurrent Markov chains that
take values in continuous state spaces [17, 56]. However, the Markov chain generated by the
RSAs seldom satisfy the strong recurrence structure required for Harris recurrent chains.

Instead, these chains satisfy the weak Feller conditions, for which there are limited results
in the literature. Nonetheless, we show that many RSAs satisfy certain desirable properties,
which can be leveraged to not only guarantee the existence of an invariant distribution, but
also establish the uniqueness of the invariant distribution. These properties of the random
operators are discussed in Section 5. This further leads to strong conclusions about the weak
law of large numbers, as we discuss next.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

9

2.1.3. Convergence of Time Average of Iterates. The weak law of large numbers for
independent and identically distributed (i.i.d.) random variables states that the time average
of i.i.d. random variables converges to the mean of the distribution in probability under fairly
mild conditions. In fact, such a version of the weak law of large numbers is also available
for Feller Markov chains. This is established for Feller chains in [19] for chains residing in a
compact Hausdorﬀ space with a unique invariant measure, and in [56, Section 18.5] for the
non-compact case under certain technical conditions, which include existence of an invariant
It turns out that this result can be proved simply under the uniqueness of the
measure.
invariant measure if the starting point ˆzn
0 is chosen according to certain speciﬁc distribution
(in fact, we do not need other technical conditions of [56, Section 18.5]). We prove this result
in Section 6, the proof of which is adapted from the results from [19] and [56].

We illustrate the theoretical results using numerical simulations for batch gradient descent,

empirical value iteration, and synchronous batch Q learning in Section 7.

3. Motivating Examples. We present here two examples where we illustrate how the

random operator framework can be applied.

3.1. Stochastic Gradient Descent in Logistic Regression. Logistic regression has been
widely used in many binary or multi-class classiﬁcation problems. For simplicity, we consider
the logistic regression with binary classiﬁcation. Let U ⊂ Rm be the set of feature vectors.
Let (ui, li)N
i=1 ⊂ U × {0, 1} denote the labeled dataset with N data points and their labels.
Our task is to model conditional probability distribution of label l given the feature vector
u ∈ U. In logistic regression, we model P (cid:8)l = 1| ui(cid:9) as f (ui; x) where x ∈ X := Rm are the
parameters of f to be learned from the data, where f is deﬁned below:

f (u; x) = σ(uTx), where σ(t) =

1
1 + e−t .

Our goal is to compute the parameter x that maximizes the log likelihood (or equivalently,
minimizes the negative log likelihood) given the labeled data. The log likelihood L : X −→ R
of i.i.d data under conditional distribution f is given by

L(x) =

1
N

N
(cid:88)

i=1

Li(x), where Li(x) := −li log f (ui; x) − (1 − li) log(1 − f (ui; x)),

It can be shown that the derivatives of Li are given by

∇xLi(x) = (f (ui; x) − li)ui, ∇2

xxLi(x) =

(cid:16)

(cid:17)
f (ui; x)(1 − f (ui; x))

uiuiT.

Consequently, each Li is a convex function, and thus, L is a convex function over the space X .
If the matrix [u1| . . . |uN ] is full rank and N > m, then it immediately follows that ∇2
xxL(x) is
a full rank matrix with positive eigenvalues. Consequently, L is strongly convex, and therefore
has a unique minimum x∗. This minimum can be computed using the usual gradient descent
algorithm. The algorithm starts at x0, picked arbitrarily, and proceeds in the direction of

10

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

−∇xL(xk) in small steps of size β:

(3.1)

xk+1 = T (xk) := xk − β∇xL(xk) = xk −

β
N

N
(cid:88)

i=1

(f (ui; xk) − li)ui,

where T : X → X is the gradient descent map (dependent on the parameter β).
It can
be further shown that if β is suﬃciently small, then the operator T is a contraction on X ,
endowed with the Euclidean norm. We note here that the above arguments would be true if
{Li}N

i=1 is a collection of strongly convex and smooth loss functions.
In practice, the exact gradient computation of loss function L is computationally expensive
as it requires evaluating N gradients at every time step. Therefore, to ease the computational
burden, a mini-batch Stochastic Gradient Descent (SGD) is employed.
In the mini-batch
SGD, at every step k, the gradient is approximated by a small, randomly sampled, subset (of
size n) of the data set. To introduce this algorithm, let Nk ⊂ {1, . . . , N } be the randomly
sampled subset of size n. The state is updated as

(3.2)

k+1 = ˆT n
ˆzn

k (ˆzn

k ) = ˆzn

k − β

1
n

n
(cid:88)

j=1

∇x(Lj(ˆzn

k )).

Note that ˆT n

k is now a random operator.

Remark 3.1. Any realization of this random operator with small values of n need not be
a contraction since ∇xLj is a rank 1 positive semideﬁnite matrix. One could add a regularizer
to the loss function to make it strongly convex. In particular, if the loss function is chosen as

Li(x) := −li log f (ui; x) − (1 − li) log(1 − f (ui; x)) +

λ
2

(cid:107)x(cid:107)2

2 with λ > 0,

then ˆT n

k is a contraction operator for a suﬃciently small β irrespective of the n used.

(cid:3)

Some obvious properties of these random operators are:

k is continuous in ˆzn
k .

1. ˆT n
2. For every (cid:15) > 0 and x ∈ X , we have

(cid:110)

(cid:107) ˆT n

P

lim
n→∞

k (x) − T (x)(cid:107)2 > (cid:15)

= 0.

(cid:111)

3. Suppose that for any compact set K ⊂ X , (cid:107)∇xLi(x)(cid:107)2 is uniformly bounded, that is,
there exists MK ∈ R such that supx∈K sup1≤i≤N (cid:107)∇xLi(x)(cid:107)2 ≤ MK. Then, for any
(cid:15) > 0,

(3.3)

lim
n→∞

sup
x∈K

P

(cid:110)

(cid:107) ˆT n

k (x) − T (x)(cid:107)2 > (cid:15)

= 0.

(cid:111)

This statement can readily be proved using the Hoeﬀding inequality and the union
bound.

Let us depart from the speciﬁc case of logistic regression, and consider the case where Li
can be any strictly concave function for all i ∈ {1, . . . , N }. Then, ˆT n
k satisﬁes the following
property.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

11

4. If all the eigenvalues of the Hessian of Li(x) satisfy 0 < m ≤ λ(∇2

for all i ∈ {1, . . . , N }, then every realization of the random operator ˆT n
map with contraction coeﬃcient ˆαn
picked β > 0.

xxLi(x)) ≤ M < ∞
k is a contraction
k ≡ max{|1−βM |, |1−βm|} < 1 for an appropriately

We now introduce the empirical dynamic programming algorithm in the context of value

iteration for MDP with discounted cost criteria.

3.2. Empirical Value Iteration for Discounted Cost MDP. Consider a Markov Decision
Problem (MDP) problem described by 4 tuple (S, A, c, p), where S is the ﬁnite state space,
A is the ﬁnite action space, and c : S × A −→ R is the cost function. The state transitions
according to st+1 ∼ p(·|st, at). Let Γ denote the set consisting of all possible deterministic
policies γ : S −→ A. The inﬁnite horizon discounted cost vγ : S −→ R starting from state s
and following policy γ is given by

vγ(s) := E

(cid:34) ∞
(cid:88)

k=0

αkc(sk, ak)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:35)

s0 = s, ak = γ(sk)

,

where α ∈ (0, 1) is the discount factor. The goal is to compute the optimal value v∗(s) =
inf γ∈Γ vγ(s). Let V be the set of all v : S −→ R; this space is isomorphic to the Euclidean
space R|S|.

It can be shown that the optimal inﬁnite horizon discounted cost is a ﬁxed point of a

contraction map T : V → V, where T is the Bellman operator given by

(3.4)

T (v)(s) = inf
a∈A

(cid:110)

c(s, a) + α

(cid:88)

s(cid:48)∈S

v(s(cid:48))p(s(cid:48)|s, a)

(cid:111)
.

Due to the Banach contraction mapping theorem, T : V → V admits a unique ﬁxed point,
which is equal to v∗. The iterative process of ﬁnding this unique ﬁxed point is called the Value
Iteration algorithm:

(3.5)

Initialize v0 randomly and iterate vk+1(s) = T (vk)(s).

In data driven applications, it is often the case that for all possible state-action pairs, multiple
realizations of the next states are available. In this situation, we can replace the expecta-
tion in (3.4) to the empirical average. This algorithm is referred to as empirical dynamic
programming, and is written as

(3.6)

(cid:40)

ˆT n
k (ˆvn

k )(s) = inf
a∈A

c(s, a) + α

(cid:41)

k (s(cid:48)
ˆvn

k,i(s, a))

,

1
n

n
(cid:88)

i=1

where s(cid:48)
k,i(s, a) are n independent and identically distributed samples of the next state given
the current state-action pair (s.a), redrawn at every k independently from the past realizations.
The above intuition can be turned into an algorithm to determine an approximately optimal
value function, and is known as the empirical value iteration algorithm:

(3.7)

Initialize v0 randomly and let ˆvn

k+1(s) = ˆT n

k (ˆvn

k )(s).

Note that ˆT n
k,i(s, a))n
(s(cid:48)

k is a random operator, and its realization is dependent on the samples generated
i=1. The following properties of ˆT n
k are obvious:

12

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

k is a contraction with contraction coeﬃcient α. Therefore, ˆT n

1. ˆT n
2. For every (cid:15) > 0 and v ∈ V, we have

k is continuous.

(cid:110)

(cid:107) ˆT n

P

lim
n→∞

k (v) − T (v)(cid:107)∞ > (cid:15)

= 0.

(cid:111)

3. In fact, we have a stronger property here. For any compact set K ⊂ V, we have for

every (cid:15) > 0

P

sup
v∈K

(cid:110)

(cid:107) ˆT n

k (v) − T (v)(cid:107)∞ > (cid:15)

(cid:111)

(cid:18)

≤ 2|S||A| exp

−

(cid:19)

,

(cid:15)n
|S|k2

where k = maxv∈K (cid:107)v(cid:107)∞. This immediately yields

(3.8)

lim
n→∞

sup
v∈K

P

(cid:110)

(cid:107) ˆT n

k (v) − T (v)(cid:107)∞ > (cid:15)

= 0.

(cid:111)

4. Let V be endowed with the partial order (cid:22) such that v1 (cid:22) v2 implies v1(s) ≤ v2(s) for

all s ∈ S. If c ≥ 0, then ˆT n

(a) If v0 = 0, then v0 (cid:22) ˆT n
(b) If v1 (cid:22) v2, then ˆT n
(c) If vl ↑ v, then ˆT n

k satisﬁes
k (v0).
k (v1) (cid:22) ˆT n
k (vl) → ˆT n

k (v2).

k (v) as l → ∞.

3.3. Observations. Through the two examples above, we observed that the approximate
operator ˆT n
k corresponding to the contraction operator T is context dependent. In the case
of stochastic gradient descent, it is constructed by picking certain loss functions randomly
and then averaging their gradients.
In the case of empirical dynamic programming, the
approximate operator involves computing the empirical average of the future expected value.
Nonetheless, there are some fundamental properties that the empirical operator satisﬁes in
both situations. For instance, the property stated in (3.3) in the context of logistic regression
is (mathematically) the same as the property stated in (3.8) in the context of empirical value
iteration. Similarly, every realization of the random operator ˆT n
k is a contraction map under
certain reasonable assumptions. We will consider more examples in Section 7, where we show
that these properties (or some minor variant of these properties) are enjoyed by other empirical
dynamic programming algorithms as well.

The other important observation is that every realization of random operator ˆT n

k may also
satisfy some other desirable properties. For instance, in the empirical value iteration example,
if we endow V with a partial order (cid:22) and the cost is nonnegative, then every realization of
ˆT n
k satisﬁes certain monotonicity properties. This is very useful in establishing the existence
of unique invariant measure, as we show in Section 5. This property is, unfortunately, not
satisﬁed by the logistic regression problem. This property is also not satisﬁed by the empirical
relative value iteration for the average cost MDP. However, we will show that the realizations
of the random operators in these cases have some other desirable properties that lead to the
existence and uniqueness of the invariant measure.

We now turn our attention to introducing our ﬁrst main result in the next section.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

13

4. Weak* Convergence of the Distribution of Trajectories. We now study the conver-
gence property of the sequence of distributions µn, which is deﬁned in (2.2). Before we study
that, we need to ensure that the random operator ˆT n
k is “close to” the operator T in some
sense. Accordingly, we make the following assumption.

Assumption 4.1. For every compact set K ⊂ X , (cid:15) > 0, and δ > 0, there exists N(cid:15),δ(K) > 0

such that

P

sup
x∈K

(cid:110)

ρ( ˆT n

k (x), T (x)) > (cid:15)

(cid:111)

< δ for all n ≥ N(cid:15),δ(K).

We recall here that this assumption is satisﬁed by the logistic regression and empirical
value iteration for discounted cost MDP (see (3.3) and (3.8) within the discussion at the end
of Subsections 3.1 and 3.2). We are now in a position to introduce our ﬁrst main result.

Theorem 4.2. If Assumptions 2.1 and 4.1 hold, then µn converges in weak* topology to ψ

as n → ∞, where ψ is deﬁned in (2.3).

Proof:
diﬀerent from those in [49]. For completeness, we present a proof in Section 8.

The proof is based on the proof by [49], except that our hypotheses are slightly

Levy-Prohorov’s metric over the space of probability measures over Polish spaces metrizes
the weak* topology [60]. Generally speaking, if distributions of two random variables are close
to each other in Levy-Prohorov’s metric, then it does not imply that the random variables will
be close to each other. As an instance, the Levy-Prohorov’s metric between the measures of two
independent and identically distributed random variables is zero, but the diﬀerence between
the random variables themselves is not zero. If one of the random variables is deterministic
(that is, its distribution is a Dirac mass), then the random variable must be close to the
deterministic variable with high probability. This is established in the lemma below.

y0 = ˆzn
0

ˆzn
1

(cid:15)

y1

ˆzn
3

y2

ˆzn
2

0

1

2

y3

3

ˆzn
4

ˆzn
6

ˆzn
7

y5

y4

4

ˆzn
5

5

y6

6

y7
7

k

Figure 1. Illustration of the behavior of ˆzn

for every (cid:15) > 0, there exists N(cid:15) > 0 such that dP (µn, ψ) < (cid:15) for every n ≥ N(cid:15). For most values of k, ˆzn
within (cid:15) ball around yk. In the illustration above, ˆzn

3 is not within (cid:15) ball around y3.

k with varying values of k. Under the hypotheses of Theorem 4.2,
k stays

Let A be a Polish space with metric ρA. Let dP be the Levy-Prohorov’s metric on the
space of probability measures ℘(A) over A. This metric is deﬁned as follows. For a Borel set

14

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

A ⊂ A, let A(cid:15) be deﬁned as

A(cid:15) = {a ∈ A : ρA(a, b) < (cid:15), b ∈ A}.

Let µ, ν ∈ ℘(A). Then, dP (µ, ν) is deﬁned by

dP (µ, ν) = inf

(cid:110)

(cid:15) > 0 : µ(A) < ν(A(cid:15)) + (cid:15), ν(A) < µ(A(cid:15)) + (cid:15) for all Borel sets A ⊂ A

(cid:111)
.

We are now in a position to introduce our next result. We believe that this result may not be
new, but we could not locate a reference where this result is proved.

Lemma 4.3. Let µ ∈ ℘(A) and 1{a∗} be a unit mass at point a∗ ∈ A. If dP (µ, 1{a∗}) < (cid:15),

then for any random variable W distributed according to the law µ, we have

Proof:

Let B(cid:15) be an open (cid:15) ball around a∗. Let (B(cid:123)

(cid:15) )(cid:15) be deﬁned as

P {ρA(W, a∗) ≥ (cid:15)} < (cid:15).

(B(cid:123)

(cid:15) )(cid:15) = {a ∈ A : ρA(a, b) < (cid:15), b ∈ B(cid:123)

(cid:15) }.

(cid:15) )(cid:15) = A \ a∗, which implies 1{a∗}((B(cid:123)

Note that (B(cid:123)
(cid:15) )(cid:15)) = 0. Let W be a random variable
distributed according to the law µ. Then, from the deﬁnition of Levy-Prohorov’s metric, we
know that

µ(B(cid:123)

(cid:15) ) < 1{a∗}((B(cid:123)

(cid:15) )(cid:15)) + (cid:15) = (cid:15).

The proof then follows from noting that

P {ρA(W, a∗) ≥ (cid:15)} = µ(B(cid:123)
(cid:15) ).

The proof is established.

As a consequence of the lemma above, we conclude that since the distribution µn converges
to the Dirac delta function ψ, it implies that for n suﬃciently large, the random sequence
generated by RSA lies within a small tube around the trajectory induced by the deterministic
contraction operator with high probability. This is illustrated in the Figure 1.

To see this, let ˆzn := (ˆzn

0 , ˆzn

1 , . . .) and y = (y0, y1, . . .). Endow the space X N with the

following metric:

ρX N(ˆzn, y) :=

∞
(cid:88)

k=0

1
2k ρ(ˆzn

k , yk).

It can be readily established that ρX N deﬁned above is a metric on X N. Then, due to Lemma
4.3, we conclude that

P {ρX N(ˆzn, y) < (cid:15)} ≥ 1 − (cid:15).

Next, note that if ˆzn satisﬁes ρX N(ˆzn, y) < (cid:15), then ˆzn
of the k ∈ N.

k is within (cid:15) neighborhood of yk for most

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

15

k )k∈N so that the Markov chain ˆzn

5. Existence of Invariant Measures for Fixed n. In this section, we identify conditions
on the operators ( ˆT n
k admits a unique invariant distribution,
denoted by πn, as k → ∞. The existence of an invariant measure yields insight about “stabil-
ity” of an RSA. In particular, if there is no invariant distribution, then it is likely the case that
the sequence generated by RSA can blow up with positive probability. Thus, by tweaking the
RSA (for instance, by changing the stepsize or increasing the number of samples), one can
ensure that the suﬃcient conditions noted below are satisﬁed, thereby establishing that the
RSA is stable and yields ﬁnite values with probability 1.

In the case where there exists unique invariant measure under certain assumptions on the
initial condition, then it means that any element in the tail of the random sequence generated
by the RSA will have its law as the invariant distribution. This is a crucial step in proving that
the time average of f (ˆzn
k ) for any f ∈ Cb(X ) converges in probability to the spatial average
(cid:82) f dπn of the function with respect to the invariant measure πn. This important result is
established in the next section.

k and ˆT n

properties of ˆT n
we list three assumptions under which we can show that ˆzn

To state the assumptions, we drop the subscript k wherever possible since the statistical
k(cid:48) are independent and identical to each-other as long as k (cid:54)= k(cid:48). Below,
k admits an invariant distribution.
To introduce our ﬁrst assumption, we need to assume a partial order (cid:22) exists on the
normed space X (we will drop the completeness requirement on the space X for this assump-
tion). For instance, X could be the Euclidean space with the natural partial order, wherein
x, y ∈ Rn, x (cid:22) y implies xi ≤ yi for all i ∈ {1, . . . , n}. The limit of an increasing sequence
(xk)k∈N satisfying x1 (cid:22) x2 (cid:22) x3 (cid:22) . . . under this partial order may escape to inﬁnity. Thus, it
is required to bound the space X to ensure that the sequences do not diverge along any of the
coordinates. Another example of a normed space with partial order is the space of measurable
functions from a Euclidean space Rn to [−M, M ], denoted by L∞(Rn, [−M, M ]) (where M is
a ﬁxed positive real number). This space features a natural partial order, wherein f1 (cid:22) f2 iﬀ
f1(s) ≤ f2(s) for every s ∈ Rn.

Assumption 5.1. The following conditions are satisﬁed:

1. X is a bounded normed space (not necessarily Polish) with partial order (cid:22). This ordering
satisﬁes the following property: For any sequence (xk)k∈N satisfying x1 (cid:22) x2 (cid:22) x3 (cid:22) . . .,
there exists a minimal element ¯x ∈ X such that xk (cid:22) ¯x for all k ∈ N. This is denoted as
xk ↑ ¯x.

2. The operator ˆT n satisfy

(a) Monotonicity 1: exists x0 ∈ X such that x0 (cid:22) ˆT n(x0) almost surely.
(b) Monotonicity 2: If x1 (cid:22) x2, then ˆT n(x1) (cid:22) ˆT n(x2) almost surely.
(c) Continuity: If xk ↑ x, then ˆT n(xk) → ˆT n(x) as k → ∞ almost surely.

Assumption 5.1 is satisﬁed in Markov decision processes with non-negative cost functions.
This has been noted in Subsection 3.2 in the context of empirical value iteration for discounted
cost criterion. However, this is also satisﬁed in MDP for total cost criterion with an absorbing
state and having a proper policy (that is, there is a stationary policy under which the MDP
terminates in an absorbing state with probability 1). The proof of the last claim follows from
arguments similar to the one made in Subsection 3.2; see, for example, Chapter 1 of [9], and
standard texts on MDPs with total cost criterion [40, 41, 8].

16

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

For RSAs involving MDPs, the point x0 can be chosen easily. Assuming that the costs
are non-negative, one could take the zero function as the initial value function or Q function
– and the Monotonicity 1 condition is satisﬁed almost surely.

Assumption 5.2. For n ∈ N and m ∈ N, let ˆαm denote the Lipschitz coeﬃcient of ˆT n

1 . The following conditions are satisﬁed:

ˆT n
m−1 ◦ . . . ◦ ˆT n
1. X is a compact Polish space.
2. For n ∈ N, there exists m ∈ N, such that for any (cid:15) > 0, there exists a δ(cid:15) > 0 such that

m ◦

ˆαm ≤ 1, P {ˆαm > 1 − (cid:15)} < δ(cid:15).

The notable point in Assumption 5.2 is that the assumption requires X to be compact.
It is satisﬁed in empirical value iteration for MDP with average cost criterion, as long as we
project the value functions outside a suﬃciently large compact set back to that compact set.
We adopted this approach earlier in [34] to ensure that the value functions obtained through
repeated use of empirical operators do not blow up. During simulations, however, we never
needed to use projection, as the value functions were bounded.

Assumption 5.3. The following conditions are satisﬁed:

1. X is a Polish space.
2. There exists a, b > 0 such that the operator ˆT n satisfy
(cid:16) ˆT n(x∗), x∗(cid:17)
ρ

= P

> (cid:15)

P

(cid:110)

(cid:110)

(cid:111)

ρ

(cid:17)
(cid:16) ˆT n(x∗), T (x∗)

(cid:111)

> (cid:15)

≤

a
(cid:15)b ,

where x∗ is the ﬁxed point of T .

3. Let ˆαn denote the Lipschitz coeﬃcient of ( ˆT n)k∈N. Then,

E [ˆαn] < ∞, E [log(ˆαn)] < 0.

Assumption 5.3 is satisﬁed in stochastic gradient descent of strongly convex and smooth
functions as noted in Subsection 3.1. This is also trivially satisﬁed in the empirical value
iteration for an MDP with a discounted reward criterion, as we have noted in Subsection 3.2.

Remark 5.4. Assumption 5.3(ii) is typically proved using concentration of measures re-
sults, like Hoeﬀding inequality [42, 52, 64] or the theory of empirical processes [61, 81]. Some
references where such inequalities have been used in empirical dynamic programming for con-
tinuous state MDPs are [57, 38, 33].

Our next theorem summarizes the main result of this section. Let µn

k denote the distribu-

tion of ˆzn
k .

Theorem 5.5. Suppose that Assumption 2.1 holds. Additionally, if either one of three as-
sumptions, Assumptions 5.1, 5.2, 5.3, holds, then there exists an invariant measure πn such
k converges to πn in weak* topology. Further, the invariant measure is unique under
that µn
either of the following circumstances:
1. Assumption 5.1 holds, and the RSA is always initialized with x0, where x0 is deﬁned in

Assumption 5.1.

2. Assumption 5.2 holds.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

17

3. Assumption 5.3 holds.

Proof: Under Assumption 5.1, the existence of invariant measures is proved in [17, Theorem
8.1, p. 79-81]. Under Assumption 5.2, the existence of invariant measures is proved in [12]
and [17, Theorem 8.2, p. 82-83]. Under Assumption 5.3, the existence and uniqueness result
is established in [25, Theorem 1.1, p. 87].

Remark 5.6. We can replace the assumption of X being a compact Polish space in As-
0 = x0,

sumption 5.2 by making the following assumption. There exists x0 ∈ X such that if ˆzn
then for any δ > 0, there exists Nδ ∈ N such that for any k ≥ 1, we have
(cid:110)

(cid:111)

(cid:17)

(cid:16)

(5.1)

P

ρ

0 , ˆzn
ˆzn
k

> Nδ

< δ.

If the above condition and Assumption 5.2 (2) holds, then one can show that for any initial
condition ˆzn
0 ∈ X , a unique invariant distribution exists. For a proof, we refer the reader to
[17, p. 179]. However, proving (5.1) is satisﬁed in usual RSAs appears to be diﬃcult in our
(cid:3)
experience.

We now turn our attention to establishing the law of large numbers for time averages of

the outputs from a RSA.

6. Averaging of Iterates and The Weak Law of Large Numbers. RSAs with constant
stepsize and averaging of iterates have received signiﬁcant attention recently. Nemirovski et.
in [58] develops an algorithm for stochastic gradient descent with averaging of the last
al.
few iterates and shows that the algorithm is robust to stepsize selection and yields better
results when properties of the loss functions (such as strong convexity parameter, Lipschitz
coeﬃcient of the derivative, etc.) are unknown. Along similar lines, [5, 4, 27] study stochastic
gradient descent based algorithms with constant stepsize and averaging and derive the ﬁnite
time guarantees on the loss achieved with averaged output. We note here that the constant
stepsize algorithms with averaging are fundamentally diﬀerent from Polyak-Ruppert averaging
used in stochastic approximation [62, 50], where the stepsizes decreases at a rate slower than
1/k.

Motivated by the above references, in this section, we consider the problem of convergence
of the sequence of averages of the random sequence (ˆzn
k ))k∈N for some f ∈
Cb(X )). As stated in Subsection 1.1, averaging of the last few outputs of an RSA is used
to reduce the variance in the ﬁnal output of the algorithm. Speciﬁcally, if we terminate the
k , there is a small chance that this output is far from x∗.
iteration of RSA at k, and output ˆzn
It is generally believed that if we average the last few outputs (assuming none of them are
∞), then it reduces the variance in the output. We establish this result rigorously here, under
the assumption that the Markov chain has a unique invariant distribution.

k )k∈N (or of (f (ˆzn

We will assume throughout that the Markov chain (ˆzn

k )k∈N admits an invariant distribution
(the precise assumption is stated in the sequel). The time-average of the Markov chain is
precisely the law of large numbers for Markov chains. It has been studied within the context
of Markov chains over compact spaces in [19] and over general locally compact spaces in [56,
Sec 18.5]. Let us formulate the problem precisely.

Consider a continuous function f ∈ Cb(X ). Let πn denote the invariant measure of the
k )k∈N. We have already studied the conditions under which such an invariant

Markov chain (ˆzn

18

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

measure would exist in Theorem 5.5. In what follows, we show that under relatively mild
assumptions, we have

(6.1)

1
K

K−1
(cid:88)

k=0

(cid:90)

f (ˆzn

k ) →

f (x)πn(dx) in probability as K → ∞.

Note that in the expression above, the term (cid:82) f (x)πn(dx) is the spatial average of the function
f at the invariant distribution. Thus, what we show is that the time average converges in
probability to the spatial average – which implies that (f (ˆzn
k ))k∈N is a discrete time ergodic
process.

Let us deﬁne the operator F : Cb(X ) → Cb(X ) and its adjoint F ∗ : ℘(X ) → ℘(X ), where

℘(X ) is endowed with the weak* topology, as follows:

F (f )(x) =

(cid:90)

X

f (y)Qn(dy|x) = E

(cid:105)
(cid:104)
f ( ˆT n(x))

, F ∗(µ)(dy) =

(cid:90)

X

Qn(dy|x)µ(dx).

Assumption 6.1. The distribution µ of the initial condition ˆzn

0 is picked from a set M ⊂

℘(X ). Either of the following two conditions holds:
1. There exists a unique invariant measure πn such that for any µ ∈ M, (F ∗)k(µ) converges

in weak* topology to πn.

2. There exists a unique invariant measure πn such that for any µ ∈ M, the averaged operator

satisﬁes

lim
k→∞

1
k

k−1
(cid:88)

(F ∗)k(µ) = πn,

i=0

where (F ∗)k(µ) denotes k compositions of F ∗ applied on µ and the convergence is in weak*
sense.

From the Stolz-Cesaro theorem [20, Theorem 2.7.1, p.59], it is easy to show that if As-
sumption 6.1(1) holds, then Assumption 6.1(2) holds as well. However, the converse may not
be true. To prove our next result, we only need Assumption 6.1(2) to hold. It should be noted
that Assumption 6.1(1) may be rather easy to prove using Theorem 5.5.

Theorem 6.2. If Assumptions 2.1 and 6.1 hold, then (6.1) holds.

Proof:
The proof essentially follows the steps in [19] and [56, Theorem 18.5.1, p. 478],
except that we relax the assumption on compactness of the state space as assumed in [19]
and replace the hypotheses in [56] with Assumption 6.1. For completeness, a detailed proof
is presented in Section 9.

One way the presentation of Assumption 6.1 departs from the traditional Markov chain
It is generally assumed that M = ℘(X ), that is, for every µ ∈
literature is as follows.
℘(X ), (F ∗)k(µ) converges in weak* topology to πn. This is a very strong assumption from
the applicability viewpoint in RSAs. In particular, it is possible to pick the most suitable
initialization for the RSAs, which implies that M can be picked appropriately. For example,
in empirical dynamic programming for MDPs, one can initialize the value function to be 0.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

19

Then, we can utilize Assumption 5.1 (with x0 = 0) to establish the existence of a unique
invariant distribution using Theorem 5.5. Incidentally, for the law of large numbers to hold,
we do not need the stronger condition of M = ℘(X ).

We now have a corollary of the result above, which we capture as a theorem below due to

its importance and applicability to RSAs.

Theorem 6.3. Suppose that X ⊂ Rn is a compact set. Suppose further that either of the

following conditions hold:
1. Assumption 5.1 holds, and the RSA is always initialized with x0, where x0 is deﬁned in

Assumption 5.1.

2. Assumption 5.2 holds.
3. Assumption 5.3 holds.
Consider the average ˆan

k of the Markov chain

ˆan
k =

1
k

k−1
(cid:88)

t=0

ˆzn
t .

If Assumptions 2.1 holds, then for any initialization ˆzn
πn exists and ˆan

k converges almost surely to the mean ¯an of the distribution πn.

0 ∈ X , a unique invariant distribution

Proof:
compact set, we can take f (x) = xi to conclude that (ˆan
[19].

The existence of a unique invariant measure is due to Theorem 5.5. Since X is a
i by

k )i converges almost surely to ¯an

The result in (6.1) requires f to be a bounded function over X . We now consider the case
where f is a potentially unbounded continuous function. To establish essentially the same
result, we need to make the following additional assumption.

Assumption 6.4. There exists a random variable C(ω) such that

f (ˆzn

k ) ≤ C(ω) almost surely,

sup
k≥0

and C(ω) < ∞ for P-almost all ω ∈ Ω.

Theorem 6.5. Suppose that f : X → R is a continuous function (potentially unbounded).

If Assumptions 2.1, 6.1, and 6.4 hold, then (6.1) holds.

Proof: The proof is presented in Subsection 9.1.

It is easy to see in practice if Assumption 6.4 holds or not, particularly when X = Rn. If
the random iterates are uniformly bounded during a single run of the RSA, then Assumption
6.4 holds along that trajectory. If almost all independent runs of the RSA is not expected to
“blow up”, then the time average of the iterates is likely going to have the variance reduction
property–the time average converges in probability to the spatial average.

We now turn our attention to illustrating the application of Theorems 4.2, 6.2, and 6.5 in

various optimization and empirical dynamic programming algorithms.

20

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

7. Numerical Simulations. In this section, we complement the theoretical results proved
above with extensive numerical simulations. We conduct simulations of minibatch stochas-
tic gradient descent for logistic regression on MNIST dataset, empirical value iteration for
discounted and average cost MDPs, and empirical Q-value iteration.

7.1. Stochastic Gradient Descent. Consider the task of classifying a subset of MNIST
handwritten digits, where we consider only the images corresponding to the numbers 0 and 1.
Each data point is a 28 × 28 pixel image with the corresponding label (either 0 or 1). We use
logistic regression and Poisson regression with an (cid:96)2 regularizer for the classiﬁcation task. We
refer the reader to Subsection 3.1 for details of this problem for the logistic regression. The
loss function for the logistic regression with the regularizer is

Li(x) := −li log f (ui; x) − (1 − li) log(1 − f (ui; x)) +

λ
2

(cid:107)x(cid:107)2

2, where λ = 5.

The loss function for Poisson regression with regularizer is

L(x) =

1
N

N
(cid:88)

i=1

Li(x), where Li(x) = exp(uiTx) − li(uiTx) +

λ
2

(cid:107)x(cid:107)2

2, where λ = 1.

The ﬁrst and the second derivatives of this loss function is

∇xLi(x) = exp(uiTx)ui − liui + λx,

∇2

xxLi(x) = exp(uiTx)uiuiT + λI

Figure 2. The Euclidean norm between the outputs of the exact gradient descent and the stochastic gradient
descent with n = 8, 16, 32 for the logistic regression with a regularizer. We observe that as n grows, the
probability that (cid:107)ˆzn
k − yk(cid:107)] (showed using black line) and the
variance of (cid:107)ˆzn

k − yk(cid:107) is large becomes smaller. The mean E [(cid:107)ˆzn

k − yk(cid:107) are computed using 1000 independent runs of the iterations.

We use here the notations introduced in Subsection 3.1. We transform each image into
a vector and append 1 at the beginning of the vector. Thus, the space U = {1} × [0, 1]784.
Thus, the space X = R785. As mentioned previously, the variable n represents the batch size
picked at every SGD iteration step. We pick y0 arbitrarily in X and set ˆzn
0 = y0. Then, we
run the exact gradient descent and the minibatch SGD as follows:

yk+1 = T (yk),

k+1 = ˆT n
ˆzn

k (ˆzn

k ),

ˆan
k =

1
k + 1

k
(cid:88)

i=0

ˆzn
i .

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

21

Figure 3. Plot of (cid:107)ˆan

k − x∗(cid:107) for n = 8, 16, 32 for the logistic regression with a regularizer. The variance of
k − x∗(cid:107) is decreasing as k grows, which indicates that the variance reduction property of the time averages.
k − x∗(cid:107) are computed using 1000 independent runs of the stochastic gradient

(cid:107)ˆan
The mean and the variance of (cid:107)ˆan
descent iterations.

Figure 4. The Euclidean norm between the outputs of the exact gradient descent and the stochastic gradient
descent with n = 64, 256, 1024 for the Poisson regression with a regularizer. We observe that as n grows, the
probability that (cid:107)ˆzn
k − yk(cid:107)] (showed using black line) and the
variance of (cid:107)ˆzn

k − yk(cid:107) is large becomes smaller. The mean E [(cid:107)ˆzn

k − yk(cid:107) are computed using 1000 independent runs of the iterations.

As discussed in Subsection 3.1, it is clear that the exact gradient descent is a contraction map
for stepsize β small enough. Therefore, yk converges to the optimal solution x∗. We can make
the following claim about the operator ˆT n
k :
Theorem 7.1. The random operator ˆT n
k satisﬁes Assumptions 2.1, 4.1. If β is suﬃciently
small, then ˆT n
1 , . . .)
and ψ be the Dirac mass on (y0, y1, . . .). We have {µn}n∈N converges to ψ in weak* topology
as n → ∞ and there exists a unique invariant distribution of the Markov chain (ˆzn
k )k∈N for
any n ∈ N.

k also satisﬁes Assumption 5.3. Let µn denote the distribution of (ˆzn

0 , ˆzn

The ﬁrst conclusion follows from the discussion leading to (3.3) in Subsection 3.1.
Proof:
For β suﬃciently small, every realization of ˆT n
k is a contraction as discussed in Remark 3.1.
We only need to show that Part 2 of Assumption 5.3 is satisﬁed. Since n is ﬁnite, ˆT n
k (x∗) can
take only ﬁnitely many values, and therefore Part 2 of Assumption 5.3 is trivially satisﬁed.

22

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

Figure 5. Plot of (cid:107)ˆan
k − x∗(cid:107) for n = 16, 64, 256 for the Poisson regression with a regularizer. The variance
k − x∗(cid:107) is decreasing as k grows, which indicates that the variance reduction property of the time averages.
k − x∗(cid:107) are computed using 1000 independent runs of the stochastic gradient

of (cid:107)ˆan
The mean and the variance of (cid:107)ˆan
descent iterations.

The existence of invariant distribution then follows from Theorem 5.5.

As a result of the theorem above, we conclude that (cid:107)yk − ˆzn

k (cid:107)2 is generally small for most
k, and its variance converges to 0 as n increases. This is evident in Figures 2 and 4, where we
plot the distance (cid:107)yk − ˆzn
k (cid:107)2 for various values of n (batch sizes) and k ranging from 0 to 5000.
The black curve is the mean E [(cid:107)yk − ˆzn
k (cid:107)2] and the red region shows one standard deviation
of the distance (cid:107)yk − ˆzn
k (cid:107)2 over 1000 sample paths of the minibatch SGD iterations. We picked
the same initial condition ˆzn
0 = y0 = 0 for all the sample paths to generate the ﬁgure.

Figures 3 and 5 plots (cid:107)ˆan

k − x∗(cid:107)2, and we observe that the variance of time averages
reduces as k grows large for any batch size n. This variance reduction of time averages can
be attributed to the contraction property of the random maps ˆT n
k , which in turn is due to the
addition of a strongly convex regularizer to the empirical loss function.

7.2. Empirical Value Iteration for Discounted Cost MDP. We consider here the empiri-
cal value iteration for discounted Markov decision processes (MDP) as described in Subsection
3.2. Consider the value iteration algorithm applied to an MDP in which there are 20 states
and 5 actions. We generate the state transition probability matrix for this MDP randomly at
the beginning of the simulation.

We use here the notation introduced in Subsection 3.2. We initialize v0 arbitrarily and set

ˆzn
0 = v0, and deﬁne the iterates of exact value iteration and empirical value iteration as

vk+1 = T (vk),

k+1 = ˆT n
ˆzn

k (ˆzn

k ),

ˆan
k =

1
k + 1

k
(cid:88)

i=0

ˆzn
i .

We can prove the following result.
Theorem 7.2. The random operator ˆT n

result, we conclude:
1. Let µn denote the distribution of (ˆzn

k satisﬁes Assumptions 2.1, 4.1, and 5.3. As a

0 , ˆzn

1 , . . .) and ψ be the Dirac mass on (y0, y1, . . .). We

have {µn}n∈N converges to ψ in weak* topology as n → ∞.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

23

Figure 6. Plot of (cid:107)vk − ˆzn

the average and variance of (cid:107)vk − ˆzn
k (cid:107) reduces. The mean and the variance of (cid:107)vk − ˆzn
1000 independent runs of the iterations. There are 20 states and 5 actions in this MDP.

k (cid:107) for n = 1, 25, 400 for k = 1, . . . , 1000. It is clear from the plots that as n grows,
k (cid:107) are computed using

Figure 7. Plot of (cid:107)v∗ − ˆan

k (cid:107) for n = 1, 25, 400 for k = 1, . . . , 1000. Notice that for every n, the variance
k (cid:107) reduces as k increases. The plots are constructed using 1000 sample paths. The mean and the

k (cid:107) are computed using 1000 independent runs of the iterations.

in (cid:107)v∗ − ˆzn
variance of (cid:107)v∗ − ˆan

2. There exists a unique invariant distribution πn of the Markov chain (ˆzn

0 , ˆzn
k converges to this invariant distribution πn.

the sequence of distribution of ˆzn

3. The time average ˆan

k converges in probability to the mean of πn as k → ∞.

1 , . . .). Further,

Proof:
satisﬁes Assumptions 4.1 and 5.3.

The proof follows from the discussions in Subsection 3.2, where we show that ˆT n
k

Moreover, since every realization of ˆT n

k is a contraction operator with coeﬃcient α, if
(cid:107)v(cid:107)∞ ≤ (cid:107)c(cid:107)∞
1−α . Thus, there is no loss of generality in restricting X to
be in the compact set {v ∈ R|S| : (cid:107)v(cid:107)∞ ≤ (cid:107)c(cid:107)∞
1−α }. The fact that the time averages converge in
probability to the mean of the invariant distribution is a direct application of Theorem 6.3.

k (v)(cid:107)∞ ≤ (cid:107)c(cid:107)∞

1−α , then (cid:107) ˆT n

24

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

Figure 6 shows the mean (black line) and the variance (red region) of (cid:107)vk − ˆzn

k (cid:107)∞ for
every time step k = 1, . . . , 10000 for n = 1, 25, 400. To compute the mean and the variance,
1000 sample paths were taken. As proved in Theorem 7.2 above, the mean of (cid:107)vk − ˆzn
k (cid:107)∞
becomes smaller as n grows. Figure 7 shows the mean and the variance of (cid:107)v∗ − ˆan
k (cid:107)∞ for
n = 1, 25, 400 for k = 1, . . . , 10000. For every n, since the sequence of distribution of ˆzn
k
converges to a unique invariant distribution, ˆan
k converges in probability to the mean of the
invariant distribution. We observe in the ﬁgure that the variance of (cid:107)v∗ − ˆan
k (cid:107)∞ reduces as k
grows implying that the average ˆan
k is close but not equal to v∗. We further observe that the
variance of (cid:107)v∗ − ˆan
k (cid:107)∞ is vanishingly small for large values of n – a result that we have not
formally proved here, and can be addressed in a future work.

7.3. Synchronous Batch Q-Value Iteration for Discounted Cost MDP. Q-value itera-
tion is another algorithm that, like value iteration, computes the optimal value function in
MDPs. Let Q denote the set of all Q-value functions Q : S × A → R. Similar to Bellman
operator of value iteration, we deﬁne an operator T : Q → Q as

T (q)(s, a) = c(s, a) + α

(cid:88)

s(cid:48)∈S

p(s(cid:48)|s, a) min
a(cid:48)∈A

q(s(cid:48), a(cid:48)).

Similar to Bellman operator, T is a contraction on (Q, (cid:107) · (cid:107)∞). Further, it can be shown
that the ﬁxed point of T is q∗, which is deﬁned as q∗(s, a) = c(s, a) + αE [v∗(s(cid:48))|s, a], where
v∗(·) = mina∈A q(·, a). The Q-value iteration starts with an arbitrary q0 ∈ Q and generates
the sequence according to qk+1 = T (qk), which converges to q∗ as k → ∞.

The exact operator, as in other cases considered in the paper, can be approximated by

the empirical operator ˆT n
k :

ˆT n
k (q)(s, a) = c(s, a) + α

1
n

n
(cid:88)

i=1

min
a(cid:48)∈A

q(s(cid:48)

k,i(s, a), a(cid:48)),

where {s(cid:48)
(s, a).

k,i(s, a)}n

i=1 are n i.i.d. samples of the next state given the current state-action pair

k (ˆzn

k ), where ˆzn

Let us deﬁne ˆzn

k+1 = ˆT n
k be the time averaged version of
k . The properties of the random operator ˆT n
ˆzn
k for empirical Q-value iteration has the same
properties as listed in Theorem 7.2 for the case of empirical value iteration. Thus, we omit
repetition of essentially the same result here. The simulation results are plotted in Figure 8
and 9.

0 = q0. Let ˆan

Figure 8 shows the mean (black line) and the variance (red region around the mean) of
k (cid:107)∞ for various values of n for k ranging from 1 to 10000. This mean and variance is
0 = q0 for

(cid:107)qk − ˆzn
computed using 1000 sample paths of the algorithm starting from the same initial ˆzn
all sample paths. As is evident, the mean reduces as we increase the sample size n.

Figure 9 plots the mean and the variance of (cid:107)q∗ − ˆan

k (cid:107). Once again, we observe that the

variance reduces as k increases since the Markov chain admits a unique invariant measure.

8. Proof of Theorem 4.2. To prove Theorem 4.2, we need to introduce some further
notation. Deﬁne Z+ = {0, 1, 2, . . .} and we use µn ⇒ ψ to denote the convergence in weak

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

25

Figure 8. Plot of the mean and the variance of (cid:107)qk − ˆzn

from the plots that as n grows, the mean of (cid:107)qk − ˆzn
k (cid:107) reduces. The mean and the variance of (cid:107)qk − ˆzn
computed using 1000 independent runs of the iterations. There are 20 states and 5 actions in this MDP.

k (cid:107) for n = 1, 25, 400 for k = 1, . . . , 1000. It is clear
k (cid:107) are

Figure 9. Plot of the mean and the variance of (cid:107)q∗ − ˆan

that for every n, the variance in (cid:107)q∗ − ˆzn
computed using 1000 independent runs of the iterations starting from the same initial condition ˆzn

k (cid:107) reduces as k increases. The mean and the variance of (cid:107)q∗ − ˆan
0 = q0.

k (cid:107) for n = 1, 25, 400 for k = 1, . . . , 1000. Notice
k (cid:107) are

topology. Let Πk : X Z+ → X k+1 denote the projection operator that projects a sequence to
its ﬁrst (k + 1) components, that is,

Πk(x0, x1, x2, . . .) = (x0, . . . , xk),

k ∈ Z+.

For a measure µ ∈ ℘(X Z+), let µ ◦ Π−1
ﬁrst k + 1 components. We note the following fact from probability theory.

k ∈ ℘(X k+1) denote the pullback of the measure to the

(νn)n∈N ⇒ ν∞ if and only if νn ◦ Π−1

Theorem 8.1 ([13], Theorem 2.8). Let (νn)n∈N ⊂ ℘(X Z+) be a sequence of measures. Then,
k
Recall that µn is a measure deﬁned on a stochastic sequence and we would like to show
k ⇒ ψ ◦ Π−1

that µn ⇒ ψ. Due to the theorem above, all we need to establish is that µn ◦ Π−1
for every k. We will establish this result via an induction argument following [49].

for every k ∈ Z+.

k ⇒ ν∞ ◦ Π−1

k

Fix n ∈ N. For k = 0, let µn ◦ Π−1

F ⊂ X . Thus, the statement is true for k = 0.

0 (F ) = 1{y0}(F ) = ψ ◦ Π−1

0 (F ) for all measurable
In the induction step, we prove that if

26

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

µn ◦ Π−1

k−1 ⇒ ψ ◦ Π−1

k−1 and for every closed set F1 ⊂ X k and F2 ⊂ X , we have

lim sup
n→∞

µn ◦ Π−1

k (F1 × F2) ≤ ψ ◦ Π−1

k (F1 × F2).

By the Portmanteau theorem [13, Theorem 2.1], we then conclude that µn ◦ Π−1
for every k ∈ Z+. We divide the proof into three steps.

k ⇒ ψ ◦ Π−1

k

Step 1: We show that for any uniformly continuous function g, the map g ◦ T is also

uniformly continuous since T is Lipschitz operator.

Lemma 8.2. If g ∈ Ub(X ), then g ◦ T ∈ Ub(X ).

Proof:
ρ(x, x(cid:48)) < δ(cid:15), |g(x) − g(x(cid:48))| < (cid:15). Now, for any y, y(cid:48) ∈ X with ρ(y, y(cid:48)) < δ(cid:15), we know that

Fix (cid:15) > 0. Since g ∈ Ub(X ), we can pick a δ(cid:15) > 0 such that for any x, x(cid:48) ∈ X with

ρ(T (y), T (y(cid:48))) < αδ(cid:15) < δ(cid:15).

Taking x = T (y) and x(cid:48) = T (y(cid:48)), we conclude that

|g ◦ T (y) − g ◦ T (y(cid:48))| < (cid:15).

This implies that g ◦ T ∈ Ub(X ).

Step 2: Next, we establish a consequence of Assumption 4.1 and Lemma 8.2. Let x0:l =
for all k = 0, . . . , l, which serves as the

k ⇒ ψ ◦ Π−1

k

(x0, . . . , xl). We assume that µn ◦ Π−1
induction hypothesis.

Lemma 8.3. Suppose that Assumption 4.1 holds. Then, for every (cid:15) > 0, there exists an N(cid:15)

such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

F1

and

g(T (xl))µn ◦ Π−1

l

(dx0:l) −

(cid:90)

F1

g(T (xl))ψ ◦ Π−1

l

(dx0:l)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< (cid:15) for all n ≥ N(cid:15),

(cid:90)

X

(cid:12)
E
(cid:12)
(cid:12)

(cid:104)
g( ˆT n

l (xl)

(cid:105)

(cid:12)
(cid:12)µn ◦ Π−1
(cid:12)
− g(T (xl))

l

(dx0:l) ≤ (cid:15)

for all n ≥ N(cid:15).

Proof:
Lemma 8.2 implies that g ◦ T is uniformly continuous. The ﬁrst result is a direct
consequence of the induction hypothesis. The proof of the second result is presented in
Appendix A.

Step 3: We now complete the proof by establishing the induction step using the result
from Lemma 8.3. We again follow [49]. Let F1 ⊂ X l+1 be any closed set. Then, using Lemma

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

27

8.3, for any g ∈ Ub(X ), there exists N(cid:15) such that
(cid:90)

(cid:90)

(cid:90)

(cid:90)

g(xl+1)µn ◦ Π−1

l+1(x0:l+1) −

g(xl+1)ψ ◦ Π−1

l+1(dx0:l+1)

E

(cid:104)

g( ˆT n

(cid:105)
l (xl))

µn ◦ Π−1

l

(dx0:l) −

g(T (xl))ψ ◦ Π−1

l

(dx0:l)

F1

X
(cid:90)

F1

(cid:104)

(cid:12)
E
(cid:12)
(cid:12)

(cid:105)

g( ˆT n

l (xl)

(cid:12)
(cid:12)µn ◦ Π−1
(cid:12)
− g(T (xl))
(cid:90)

l

(dx0:l)

g(T (xl))µn ◦ Π−1

(dx0:l) −

l

g(T (xl))ψ ◦ Π−1

l

F1

(cid:12)
(cid:12)
(cid:12)
(dx0:l)
(cid:12)
(cid:12)

X

F1
(cid:90)

=

F1

(cid:90)

≤

+

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

X l+1
(cid:90)

F1

(8.1)

< 2(cid:15) for all n ≥ N(cid:15)

Now, for any F2 ⊂ X closed, we can construct a sequence of (gm)m∈N ⊂ Ub(X ) such that
gm+1 ≤ gm for all m ∈ N and gm → 1F2. This leads us to the following inequality for every
m ∈ N:

(cid:90)

(cid:90)

F1

X

1F2(xl)µn ◦ Π−1

l+1(x0:l+1) ≤

(cid:90)

(cid:90)

F1

X

gm(xl)µn ◦ Π−1

l+1(x0:l+1).

Taking the limsup on both sides and using (8.1), we arrive at the following inequality

(8.2)

lim sup
n→∞

µn ◦ Π−1

l+1(F1 × F2) ≤

gm(xl+1)ψ ◦ Π−1

l+1(dx0:l+1).

F1
Now, since the right side holds for every m ∈ N, we take the limit m → ∞ and use the
bounded convergence theorem to conclude

X

(cid:90)

(cid:90)

(8.3)

lim
m→∞

(cid:90)

(cid:90)

F1

X

gm(xl+1)ψ ◦ Π−1

l+1(dx0:l+1) = ψ ◦ Π−1

l+1(F1 × F2).

Collecting the two inequalities in (8.2) and (8.3), we conclude that

lim sup
n→∞

µn ◦ Π−1

l+1(F1 × F2) ≤ ψ ◦ Π−1

l+1(F1 × F2).

The proof of the lemma is complete.

9. Proof of Theorem 6.2. We ﬁrst introduce some notations. For f ∈ Cb(X ), we deﬁne

gk ∈ Cb(X ), k = 0, 1, . . . as

g0(x) = f (x),

gk(x) = F k(f )(x) = E

(cid:104)
f

(cid:16) ˆT n

k ◦ . . . ◦ ˆT n

1 (x)

(cid:17)(cid:105)

.

The following equation follows immediately from the above deﬁnitions:
E (cid:2)gk(ˆzn

(cid:3) = gk+1(ˆzn

(9.1)

m)|ˆzn

m−1).

m−1

Deﬁne ¯F ∗

k = 1
k

(cid:80)k

i=1(F ∗)i. Deﬁne the constant function cf ∈ Cb(X ) as

cf (x) =

(cid:90)

f dπn.

28

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

The average of the functions gk, denoted by ¯gk, is

¯gk(x) =

1
k

k
(cid:88)

i=1

gi−1(x).

For a function f ∈ Cb(X ) and a set C ⊂ X , we use f |C to denote the restriction of the function
on the set C. We now prove three lemmas that lead to the result. For the next result, let us
deﬁne the occupation measure ηk over the set C ⊂ X as

(9.2)

We claim the following.

ηk(C) =

1
k

k−1
(cid:88)

i=0

1{ˆzn

k ∈C}.

Lemma 9.1. If Assumptions 2.1 and 6.1 holds, then for every (cid:15) > 0, there exists a compact

set C(cid:15) ⊂ X such that

P

lim sup
k→∞

(cid:110)
ηk(C(cid:123)

(cid:15) ) ≥ (cid:15)

(cid:111)

< (cid:15).

Proof: Note that since ¯F ∗
Prohorov’s theorem that the sequence { ¯F ∗
compact set such that

k (µ) → πn for any µ ∈ M by Assumption 6.1, we conclude from
k (µ)}k∈N is tight. Thus, for (cid:15) > 0, let C(cid:15) be the

Further, we note that for any set C ⊂ X , we have

k (µ)(C(cid:123)
¯F ∗

(cid:15) ) < (cid:15)2 for all k ∈ N.

E [ηk(C)] = E

(cid:34)

1
k

k−1
(cid:88)

i=0

(cid:35)

1{ˆzn

k ∈C}

= ¯F ∗

k (µ)(C).

Using the above identity and using Markov’s inequality, we conclude that

P

(cid:110)
ηk(C(cid:123)

(cid:15) ) ≥ (cid:15)

(cid:111)

(cid:104)

E

(cid:105)
ηk(C(cid:123)
(cid:15) )
(cid:15)

≤

< (cid:15).

The proof of the theorem is complete.

Lemma 9.2. Let C ⊂ X be a compact set. Then, the sequence of functions (¯gk|C)k∈N is

uniformly bounded and equicontinuous and converges uniformly to cf |C.
Proof: First, we note that (cid:107)gi(cid:107)∞ ≤ (cid:107)f (cid:107)∞ for all i ∈ N, which implies that ¯gk|C is uniformly
bounded.

The proof of equicontinuity follows directly from Assumption 6.1(2) and Ascoli theorem.

Note that as k → ∞, we get

¯gk(x) = (cid:104)f, ¯F ∗

k (δx)(cid:105) → cf (x).

Thus, by Ascoli’s theorem, {¯gk|C}k∈N is an equicontinuous sequence of functions. The result
then follows using Assumption 6.1(2).

Next, we establish the following result.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

29

Lemma 9.3. For every M ∈ N, we have
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

lim
N →∞

g0(ˆzn

N −1
(cid:88)

1
N

(cid:16)

l=0

(cid:17)
l ) − ¯gM (ˆzn
l )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0 P-almost surely.

Proof:

See Appendix B.

The proof can now be completed easily. Fix (cid:15) > 0 and recall the deﬁnition of the set C(cid:15) from
Lemma 9.1. We now note that for every K ∈ N and M ∈ N, we have

1
K

K
(cid:88)

k=1

(cid:90)

f (ˆzn

k ) −

f dπn =

1
K

K
(cid:88)

k=1

g0(ˆzn

k ) −

(cid:90)

f dπn

≤ 1{ˆzn

1 ∈ C(cid:15), . . . , ˆzn

K ∈ C(cid:15)}

< 1{ˆzn

1 ∈ C(cid:15), . . . , ˆzn

K ∈ C(cid:15)}

(cid:32)

(cid:40)

1
K

1
K

K
(cid:88)

k=1
K
(cid:88)

k=1

g0(ˆzn

k ) −

(cid:33)

(cid:90)

f dπn

+ 2(cid:107)f (cid:107)∞ηK(C(cid:123)
(cid:15) ),

(cid:16)

g0(ˆzn

(cid:17)
k ) − ¯gM (ˆzn
k )

+

(cid:32)

1
K

K
(cid:88)

k=1

¯gM (ˆzn

k ) −

(cid:33)(cid:41)

(cid:90)

f dπn

+ 2(cid:107)f (cid:107)∞ηK(C(cid:123)
(cid:15) ).

Since ¯gM (x) → (cid:82) f dπn uniformly on the compact set C(cid:15) due to Lemma 9.2, we can pick M(cid:15)
suﬃciently large such that for all K ∈ N and M ≥ M(cid:15), we get

1{ˆzn

0 ∈ C(cid:15), . . . , ˆzn

K ∈ C(cid:15)}

(cid:32)

1
K

K
(cid:88)

k=1

(cid:90)

¯gM (ˆzn

k ) −

(cid:33)

f dπn

< (cid:15).

For such M(cid:15), as K → ∞, the ﬁrst summand goes to 0 by Lemma 9.3. In the third summand,
we know that ηK(C(cid:123)
(cid:15) ) is less than (cid:15) with probability at least 1 − (cid:15) due to Lemma 9.1 for
suﬃciently large K. Collecting all these results, we conclude that

P

lim sup
K→∞

(cid:40)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K

K
(cid:88)

k=1

f (ˆzn

k ) −

(cid:90)

f dπn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

< (2(cid:107)f (cid:107)∞ + 2)(cid:15)

≥ 1 − (cid:15).

This completes the proof of the theorem.

9.1. Proof of Theorem 6.5. In order to prove the result, let us ﬁrst consider the function
f : X → [0, ∞), which always takes non-negative values. Let fm(x) := min{f (x), m} be the
clipped function, in which case, fm ∈ Cb(X ). Deﬁne Ω0 = {ω : C(ω) < ∞}, and note that
by assumption, P {Ω0} = 1. It is obvious that for any x ∈ X , if we pick m ≥ f (x), then
fm(x) = f (x).

Note that for this case, for any m ∈ N, the following inequality holds:
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k ) − fm(ˆzn
k )

(cid:12)
(cid:12)
(cid:12)
≤
(cid:12)
(cid:12)

f dπn

K−1
(cid:88)

K−1
(cid:88)

K−1
(cid:88)

f (ˆzn

f (ˆzn

k ) −

1
K

1
K

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

(cid:17)

(cid:16)

(cid:90)

fm(ˆzn

k=0

k=0

k ) −

(cid:90)

fmdπn

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+

k=0
(cid:90)

fmdπn −

(cid:90)

f dπn

(cid:12)
(cid:12)
(cid:12)
.
(cid:12)
(cid:12)

(9.3)

30

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

In the next lemma, we show that each of the summand on the right is small with high
probability.

Lemma 9.4. Let f : X → [0, ∞). If Assumptions 2.1, 6.1, and 6.4 hold, then (6.1) holds.

Proof:

For any (cid:15) > 0, there exists (a random natural number) M(cid:15)(ω) such that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K

K−1
(cid:88)

(cid:16)

k=0

f (ˆzn

(cid:17)
k ) − fm(ˆzn
k )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

< (cid:15) for all m ≥ M(cid:15)(ω) and K ∈ N.

Indeed, one can take M(cid:15)(ω) = (cid:100)C(ω)(cid:101), in which case,

(9.4)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K

K−1
(cid:88)

(cid:16)

k=0

f (ˆzn

(cid:17)
k ) − fm(ˆzn
k )

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

= 0 for all m ≥ M(cid:15)(ω) and K ∈ N.

Further, due to the monotone convergence theorem, there exists ¯M such that
(cid:90)

fmdπn −

f dπn

< (cid:15) for all m ≥ ¯M .

(cid:12)
(cid:90)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

Now, pick (cid:15), δ > 0. Let us deﬁne Ωm,K ⊂ Ω as the set such that
(cid:12)
(cid:40)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Ωm,K =

fmdπn

fm(ˆzn

K−1
(cid:88)

k ) −

1
K

ω :

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:90)

k=0

(cid:41)

< (cid:15)

.

For every m, pick Km such that P {Ωm,Km} < δ
6.2. Deﬁne ˇΩ = Ω0

(cid:84) (cid:84)∞

m=1 Ωm,Km. This immediately implies that

2m ; such a Km always exists due to Theorem

P (cid:8) ˇΩ(cid:9) = 1 − P

(cid:40)

Ω(cid:123)
0

(cid:91)

∞
(cid:91)

Ω(cid:123)

m,Km

(cid:41)

> 1 − δ.

m=1

For every ω ∈ ˇΩ, pick m suﬃciently large such that m ≥ M(cid:15)(ω), m ≥ ¯M , and pick any
K = Km. We use (9.3) to conclude that

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

1
K

K−1
(cid:88)

f (ˆzn

k ) −

(cid:90)

f dπn

< 3(cid:15) for all ω ∈ ˇΩ.

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k=0
Since P (cid:8)ω ∈ ˇΩ(cid:9) > 1 − δ, the proof of the lemma is complete.
We can now complete the proof of Theorem 6.5 using the result above. Pick f ∈ C(X ) and
deﬁne f+, f− : X → [0, ∞) such that f+ = max{f, 0} and f− = max{−f, 0}. This immediately
yields f = f+ − f−. Further, we have

1
K

K−1
(cid:88)

k=0
(cid:90)

1
K
(cid:90)

f (ˆzn

k ) =

f dπn =

K−1
(cid:88)

k=0

f+(ˆzn

k ) −

1
K

K−1
(cid:88)

k=0

f−(ˆzn

k ),

(cid:90)

f+dπn −

f−dπn.

Now, it is easy to conclude from Lemma 9.4 that the convergence in probability result in (6.1)
holds for both f+ and f−, due to which the convergence in probability result holds for the
function f itself.

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

31

10. Conclusion. In this paper, we studied the convergence of random sequences generated
from certain RSAs used in machine and reinforcement learning problems. If the randomization
device used within the algorithm is independent at every iteration, and the maps do not change
(for instance, the stepsize is taken as constant), then the random sequence generated can be
viewed using the lens of Markov chains. We leveraged the theory of Feller Markov chains
to deduce many interesting characteristics of the random sequence and their distributions.
Speciﬁcally, under reasonable conditions, we showed that the entire random sequence is close
to the sequence generated by the exact algorithm with high probability for n suﬃciently
large. We further showed that the average of the random sequence converges to the mean
of the invariant distribution if the sequence if there exists a unique invariant measure of the
Markov chain.

We expect that the results presented here can be applied to MDPs over continuous state
and action spaces (referred to as continuous MDPs). Indeed, ﬁnite time guarantees of empirical
value iteration for continuous MDPs with function approximator have been presented in [57,
72, 71, 38, 70, 39, 33] under a variety of assumptions on the MDPs and performance criteria.
Convergence of asynchronous algorithm for continuous MDPs with non-parametric function
approximation is presented in [69]. It will be interesting to investigate if the output of these
algorithms satisfy the suﬃcient conditions for Theorem 4.2 and 6.2. It will also be interesting
to apply the results presented here to variance reduced algorithms [73, 83, 66, 85, 84] that
have been developed recently.

Another problem left for future research is to determine bounds on P {ρ(ˆzn

k , yk) ≥ (cid:15)} for
any (cid:15) > 0 for every n and k. Such bounds would unify our understanding of ﬁnite time
guarantees for RSAs and allow us to improve the existing algorithms. We hope that the
uniﬁed framework developed in this paper will be useful for analyzing many other learning
algorithms in the future, particularly for analyzing MDPs over compact uncountable state
and action spaces.

Appendix A. Proof of Lemma 8.3.
(cid:17)

(cid:16)

n → ∞, we conclude that

µn ◦ Π−1

l

n∈N

compact set such that

Since µn ◦ Π−1
l
is tight. For a ﬁxed (cid:15) > 0, let F2 ⊂ X be the

l → ψ ◦ Π−1

in weak topology as

µn ◦ Π−1

l

(X l × F(cid:15)) > 1 −

(cid:15)
4(cid:107)g(cid:107)∞

for every n ∈ N.

We now need the following result.

Lemma A.1. If Assumption 4.1 holds, then for any g ∈ Ub(X ), compact set K ⊂ X and

(cid:15) > 0, there exists N ∈ N such that

(cid:12)
(cid:12)
(cid:12)

E

(cid:104)

g( ˆT n

(cid:105)
k (x))

(cid:12)
(cid:12)
(cid:12) < (cid:15) for all x ∈ K.
− g(T (x))

Since g is uniformly continuous, for every (cid:15) > 0, there exists a δ(cid:15) > 0 such that
Proof:
for any x, x(cid:48) ∈ X with ρ(x, x(cid:48)) < δ(cid:15), we have |g(x) − g(x(cid:48))| < (cid:15). Since Assumption 4.1 holds,
there exists N(cid:15)(g, K) such that

P

sup
x∈K

(cid:110)

ρ( ˆT n

k (x), T (x)) > δ(cid:15)

(cid:111)

<

(cid:15)
2(cid:107)g(cid:107)∞

for all n ≥ N(cid:15)(g, K).

32

This implies

(cid:12)
E
(cid:12)
(cid:12)

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

|g( ˆT n

k (x)) − g(T (x))|P {dω}

(cid:110)

ρ( ˆT n

k (x), T (x)) < δ(cid:15)

(cid:111)

(cid:104)
g( ˆT n

k (x)) − g(T (x))

(cid:90)

(cid:105) (cid:12)
(cid:12)
(cid:12) ≤
≤ (cid:15)P

+ 2(cid:107)g(cid:107)∞P

< 2(cid:15).

(cid:110)

ρ( ˆT n

k (x), T (x)) ≥ (cid:15)

(cid:111)

The proof of the lemma is complete.

We are now in a position to prove the result. Consider the following expressions

(cid:12)
(cid:12)
(cid:12)

E

(cid:104)

g( ˆT n

(cid:105)
l (xl))

(dx0:l)

l

(cid:12)
(cid:12)µn ◦ Π−1
(cid:12)
− g(T (xl))
(cid:12)
(cid:12)µn ◦ Π−1
(cid:12)
− g(T (xl))
(cid:12)
(cid:12)µn ◦ Π−1
(cid:12)
− g(T (xl))

l

l

(dx0:l)

(dx0:l)

(cid:12)
E
(cid:12)
(cid:12)

(cid:12)
E
(cid:12)
(cid:12)

(cid:104)
g( ˆT n

(cid:105)
l (xl))

(cid:104)
g( ˆT n

(cid:105)
l (xl))

(cid:90)

X l+1
(cid:90)

=

X l×F(cid:15)

(cid:90)

X l×F (cid:123)
(cid:15)
(cid:15)
(cid:15)
2
2

+

+

≤

= (cid:15).

The proof of the lemma is complete.

Appendix B. Proof of Lemma 9.3. Deﬁne gp(ˆzn
Let us deﬁne random variables up,l for l, p = 0, 1, . . . as

−i) := 0 for all i ∈ N and p ∈ {0, 1, . . .}.

up,l = gp(ˆzn

l ) − gp+1(ˆzn

l−1).

Thus, up,0 = gp(ˆzn

0 ). By deﬁnition of up,l, we have for any l, k ≥ 0

g0(ˆzn

l ) − gk(ˆzn

l−k) =

(cid:16)

g0(ˆzn

l ) − g1(ˆzn

l−1)

(cid:17)

(cid:16)

+ . . . +

gk−1(ˆzn

l−k+1) − gk(ˆzn

l−k)

(cid:17)

(B.1)

= u0,l + . . . + uk−1,l =

k−1
(cid:88)

p=0

up,l.

We use the above expression to establish the following identity.

Lemma B.1. For M ≤ N , we have

(B.2)

1
N

N −1
(cid:88)

l=0

(cid:32)

(cid:33)

g0(ˆzn

l ) − ¯gM (ˆzn
l )

=

1
M

M −1
(cid:88)

k−1
(cid:88)

(cid:32)

k=0

p=0

1
N

N −1
(cid:88)

l=0

(cid:33)

up,l

−

1
M

(cid:32)

M −1
(cid:88)

k=0

1
N

N −1
(cid:88)

(cid:33)
.

gk(ˆzn
l )

l=N −k

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

33

Proof:

For any l ∈ {0, 1, . . .}, we have

g0(ˆzn

l ) − ¯gM (ˆzn

l ) = g0(ˆzn

l ) −

1
M

M −1
(cid:88)

k=0

gk(ˆzn

l ) =

1
M

=

=

1
M

1
M

M −1
(cid:88)

(cid:18)(cid:16)

(cid:18)(cid:16)

k=0
M −1
(cid:88)

k=0

g0(ˆzn

l )) − gk(ˆzn

l−k)

g0(ˆzn

l )) − gk(ˆzn

l−k)

M −1
(cid:88)

(cid:16)

g0(ˆzn

l ) − gk(ˆzn
l )

(cid:17)

k=0

(cid:17)

(cid:16)

+

(cid:17)

(cid:16)

+

gk(ˆzn

l−k) − gk(ˆzn
l )

(cid:17)(cid:19)

gk(ˆzn

l−k) − gk(ˆzn
l )

(cid:17)(cid:19)
.

This yields

(B.3)

1
N

N −1
(cid:88)

l=0

(cid:32)

(cid:33)

g0(ˆzn

l ) − ¯gM (ˆzn
l )

=

1
N M

M −1
(cid:88)

N −1
(cid:88)

(cid:18)(cid:16)

k=0

l=0

For any l, k ≥ 0, (B.1) yields

g0(ˆzn

l )) − gk(ˆzn

(cid:17)
l−k)

(cid:16)

+

gk(ˆzn

l−k) − gk(ˆzn
l )

(cid:17)(cid:19)
.

N −1
(cid:88)

(cid:16)

g0(ˆzn

l ) − gk(ˆzn

l−k)

(cid:17)

=

1
N

N −1
(cid:88)

k−1
(cid:88)

up,l =

(cid:32)

k−1
(cid:88)

1
N

(cid:33)

up,l

.

1
N

N −1
(cid:88)

l=0

p=0
Consider the second summand in (B.3). For any k ∈ N, we have

p=0

l=0

l=0

1
N

N −1
(cid:88)

(cid:16)

l=0

gk(ˆzn

(cid:17)
l−k) − gk(ˆzn
l )

= −

1
N

N −1
(cid:88)

l=N −k

gk(ˆzn

l ).

These expressions immediately establish the equality in (B.2).

Lemma B.2. For a ﬁxed p ∈ {0, 1, . . .}, we have

lim
N →∞

1
N

N −1
(cid:88)

l=0

up,l = 0 P-almost surely.

0 , . . . , ˆzn

l−1, up,0, . . . , up,l−1). We show that {up,l}∞

For any p ∈ {0, 1, . . .} and l ∈ N, let Fp,l−1 denote the σ-algebra generated by
l=1 forms a martingale with respect to the

Proof:
(ˆzn
σ-algebra Fp,l−1. By the deﬁnition of up,l, we immediately have for any l ≥ 1,
(cid:3) = E (cid:2)gp(ˆzn

E (cid:2)up,l|ˆzn

l ) − gp+1(ˆzn

l−1)|ˆzn

(cid:3) = 0.

(B.4)

l−1

l−1

This implies

E [up,l|Fp,l−1] = 0, E (cid:2)(up,l)2(cid:3) ≤ 2(cid:107)f (cid:107)2

∞ for p ∈ {0, 1, . . .} and l ∈ N.

The proof then is an immediate consequence of the strong law of large numbers for martingales
in [53, p. 66].

Now note that for any M ∈ N, as N → ∞, the right side of (B.2) converges to 0 almost

surely by Lemma B.2. This yields the result.

34

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

REFERENCES

[1] J. Abounadi, D. P. Bertsekas, and V. Borkar, Stochastic approximation for nonexpansive maps:
Application to Q-learning algorithms, SIAM Journal on Control and Optimization, 41 (2002), pp. 1–
22.

[2] A. L. Almudevar, Approximate iterative algorithms, CRC Press, 2014.
[3] O. Anschel, N. Baram, and N. Shimkin, Averaged-DQN: Variance reduction and stabilization for deep
reinforcement learning, in Proceedings of the 34th International Conference on Machine Learning-
Volume 70, JMLR. org, 2017, pp. 176–185.

[4] F. Bach, Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression,

The Journal of Machine Learning Research, 15 (2014), pp. 595–627.

[5] F. Bach and E. Moulines, Non-strongly-convex smooth stochastic approximation with convergence rate

o (1/n), in Advances in neural information processing systems, 2013, pp. 773–781.

[6] C. L. Beck and R. Srikant, Error bounds for constant step-size Q-learning, Systems & Control Letters,

61 (2012), pp. 1203–1208.

[7] D. P. Bertsekas, Dynamic Programming and Optimal Control, vol. I & II, Athena Scientiﬁc Belmont,

MA, 2011.

[8] D. P. Bertsekas, Abstract dynamic programming, Athena Scientiﬁc Belmont, MA, 2013.
[9] D. P. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming, vol. 5, Athena Scientiﬁc Belmont,

MA, 1996.

[10] D. P. Bertsekas and H. Yu, Q-learning and enhanced policy iteration in discounted dynamic program-

ming, Mathematics of Operations Research, 37 (2012), pp. 66–94.

[11] J. Bhandari, D. Russo, and R. Singal, A ﬁnite time analysis of temporal diﬀerence learning with

linear function approximation, arXiv preprint arXiv:1806.02450, (2018).

[12] R. N. Bhattacharya and O. Lee, Ergodicity and central limit theorems for a class of Markov processes,

in Multivariate Statistics and Probability, Elsevier, 1989, pp. 80–90.
[13] P. Billingsley, Convergence of probability measures, John Wiley & Sons, 2013.
[14] S. Bonnabel, Stochastic gradient descent on Riemannian manifolds, IEEE Transactions on Automatic

Control, 58 (2013), pp. 2217–2229.

[15] V. S. Borkar, Stochastic approximation: A dynamical systems viewpoint, vol. 48, Springer, 2009.
[16] V. S. Borkar and S. P. Meyn, The ODE method for convergence of stochastic approximation and
reinforcement learning, SIAM Journal on Control and Optimization, 38 (2000), pp. 447–469.

[17] A. A. Borovkov, Ergodicity and Stability of Stochastic Processes, John Wiley & Sons, Ltd., 1998.
[18] L. Bottou, Large-scale machine learning with stochastic gradient descent, in Proceedings of COMP-

STAT’2010, Springer, 2010, pp. 177–186.

[19] L. Breiman, The strong law of large numbers for a class of Markov chains, The Annals of Mathematical

Statistics, 31 (1960), pp. 801–803.

[20] A. D. R. Choudary and C. P. Niculescu, Real analysis on intervals, Springer, 2014.
[21] W. L. Cooper, S. G. Henderson, and M. E. Lewis, Convergence of simulation-based policy iteration,

Probability in the Engineering and Informational Sciences, 17 (2003), pp. 213–234.

[22] W. L. Cooper and B. Rangarajan, Performance guarantees for empirical Markov decision processes

with applications to multiperiod inventory models, Operations Research, 60 (2012), pp. 1267–1281.

[23] G. Dalal, B. Sz¨or´enyi, G. Thoppe, and S. Mannor, Finite sample analyses for TD(0) with function

approximation, in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.

[24] A. Defazio, F. Bach, and S. Lacoste-Julien, SAGA: A fast incremental gradient method with support
for non-strongly convex composite objectives, in Advances in neural information processing systems,
2014, pp. 1646–1654.

[25] P. Diaconis and D. Freedman, Iterated random functions, SIAM review, 41 (1999), pp. 45–76.
[26] A. Dieuleveut, A. Durmus, and F. Bach, Bridging the gap between constant step size stochastic

gradient descent and markov chains, arXiv preprint arXiv:1707.06386, (2017).

[27] A. Dieuleveut, N. Flammarion, and F. Bach, Harder, better, faster, stronger convergence rates for

least-squares regression, The Journal of Machine Learning Research, 18 (2017), pp. 3520–3570.

[28] M. Duflo, Random iterative models, vol. 34, Springer Science & Business Media, 2013.
[29] F. Facchinei and J.-S. Pang, Finite-dimensional Variational Inequalities and Complementarity Prob-

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

35

lems, Springer Science & Business Media, 2007.

[30] J.-C. Fort and G. Pages, Asymptotic behavior of a markovian stochastic algorithm with constant step,

SIAM journal on control and optimization, 37 (1999), pp. 1456–1482.

[31] S. B. Gelfand and S. K. Mitter, Recursive stochastic algorithms for global optimization in rd, SIAM

Journal on Control and Optimization, 29 (1991), pp. 999–1018.

[32] S. B. Gelfand and S. K. Mitter, Metropolis-type annealing algorithms for global optimization in rˆd,

SIAM Journal on Control and Optimization, 31 (1993), pp. 111–131.

[33] A. Gupta, R. Jain, and P. Glynn, Probabilistic contraction analysis of iterated random operators,
submitted to SIAM Journal on Control and Optimization, (2018). arXiv preprint arXiv:1804.01195.
[34] A. Gupta, R. Jain, and P. W. Glynn, An empirical algorithm for relative value iteration for average-
cost MDPs, in Proc. of 54th IEEE Conference on Decision and Control (CDC), Dec 2015, pp. 5079–
5084.

[35] H. Gupta, R. Srikant, and L. Ying, Finite-time performance bounds and adaptive learning rate selec-
tion for two time-scale reinforcement learning, in Advances in Neural Information Processing Systems,
2019, pp. 4706–4715.

[36] R. Harikandeh, M. O. Ahmed, A. Virani, M. Schmidt, J. Koneˇcn`y, and S. Sallinen, Stopwast-
ing my gradients: Practical SVRG, in Advances in Neural Information Processing Systems, 2015,
pp. 2251–2259.

[37] W. B. Haskell, R. Jain, and D. Kalathil, Empirical dynamic programming, Mathematics of Opera-

tions Research, 41 (2016), pp. 402–429.

[38] W. B. Haskell, R. Jain, H. Sharma, and P. Yu, A universal empirical dynamic programming algo-
rithm for continuous state MDPs, IEEE Transactions on Automatic Control, 65 (2019), pp. 115–129.
[39] W. B. Haskell, P. Yu, H. Sharma, and R. Jain, Randomized function ﬁtting-based empirical value it-
eration, in 2017 IEEE 56th Annual Conference on Decision and Control (CDC), IEEE, 2017, pp. 2467–
2472.

[40] O. Hern´andez-Lerma and J. B. Lasserre, Discrete-time Markov control processes: basic optimality

criteria, vol. 30, Springer Science & Business Media, 1996.

[41] O. Hern´andez-Lerma and J. B. Lasserre, Further topics on discrete-time Markov control processes,

vol. 42, Springer Science & Business Media, 2012.

[42] W. Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American

Statistical Association, 58 (1963), pp. 13–30.

[43] J. Huang, I. Kontoyiannis, and S. P. Meyn, The ODE method and spectral theory of Markov operators,

in Stochastic Theory and Control, Springer, 2002, pp. 205–221.

[44] T. Jaakkola, M. I. Jordan, and S. P. Singh, Convergence of stochastic iterative dynamic programming

algorithms, in Advances in neural information processing systems, 1994, pp. 703–710.

[45] P. Jain, S. M. Kakade, R. Kidambi, P. Netrapalli, and A. Sidford, Parallelizing stochastic gradient
descent for least squares regression: Mini-batching, averaging, and model misspeciﬁcation, Journal of
Machine Learning Research, 18 (2018), pp. 1–42.

[46] R. Jain and P. Varaiya, Simulation-based optimization of Markov decision processes: An empirical

process theory approach, Automatica, 46 (2010), pp. 1297–1304.

[47] R. Johnson and T. Zhang, Accelerating stochastic gradient descent using predictive variance reduction,

in Advances in neural information processing systems, 2013, pp. 315–323.

[48] D. Kalathil, V. S. Borkar, and R. Jain, Empirical q-value iteration, Submitted to Stochastic Systems,

available online at arXiv preprint arXiv:1412.0180, (2014).

[49] A. F. Karr, Weak convergence of a sequence of Markov chains, Probability Theory and Related Fields,

33 (1975), pp. 41–48.

[50] H. Kushner and G. G. Yin, Stochastic approximation and recursive algorithms and applications, vol. 35,

Springer Science & Business Media, 2003.

[51] C. Lakshminarayanan and C. Szepesvari, Linear stochastic approximation: How far does constant
step-size and iterate averaging go?, in International Conference on Artiﬁcial Intelligence and Statistics,
2018, pp. 1347–1355.

[52] M. Ledoux, The concentration of measure phenomenon, no. 89, American Mathematical Soc., 2001.
[53] M. Lo`eve, Probability Theory II, Springer, New York, NY, 1977.
[54] S. Mandt, M. D. Hoffman, and D. M. Blei, Stochastic gradient descent as approximate bayesian

36

A. GUPTA, G. TENDOLKAR, H. CHEN, J. PI

inference, The Journal of Machine Learning Research, 18 (2017), pp. 4873–4907.

[55] S. Merity, N. S. Keskar, and R. Socher, Regularizing and optimizing lstm language models, arXiv

preprint arXiv:1708.02182, (2017).

[56] S. P. Meyn and R. L. Tweedie, Markov Chains and Stochastic Stability, Cambridge University Press,

2009.

[57] R. Munos and C. Szepesv´ari, Finite-time bounds for ﬁtted value iteration, Journal of Machine Learning

Research, 9 (2008), pp. 815–857.

[58] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro, Robust stochastic approximation approach to

stochastic programming, SIAM Journal on optimization, 19 (2009), pp. 1574–1609.

[59] D. O’Neill, M. Levorato, A. Goldsmith, and U. Mitra, Residential demand response using rein-
forcement learning, in 2010 First IEEE International Conference on Smart Grid Communications,
IEEE, 2010, pp. 409–414.

[60] K. R. Parthasarathy, Probability measures on metric spaces, vol. 352, American Mathematical Soc.,

2005.

[61] D. Pollard, Convergence of stochastic processes, Springer Series in Statistics, Springer-Verlag New York,

1984, https://doi.org/10.1007/978-1-4612-5254-2.

[62] B. T. Polyak and A. B. Juditsky, Acceleration of stochastic approximation by averaging, SIAM Journal

on Control and Optimization, 30 (1992), pp. 838–855.

[63] M. L. Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming, John Wiley

& Sons, 2014.

[64] M. Raginsky and I. Sason, Concentration of measure inequalities in information theory, communica-
tions, and coding, Foundations and Trends in Communications and Information Theory, 10 (2013),
pp. 1–247.

[65] E. K. Ryu and S. Boyd, Primer on monotone operator methods, Appl. Comput. Math, 15 (2016),

pp. 3–43.

[66] H. Sato, H. Kasai, and B. Mishra, Riemannian stochastic variance reduced gradient algorithm with
retraction and vector transport, SIAM Journal on Optimization, 29 (2019), pp. 1444–1472.
[67] M. Schmidt, N. Le Roux, and F. Bach, Minimizing ﬁnite sums with the stochastic average gradient,

Mathematical Programming, 162 (2017), pp. 83–112.

[68] R. Schoknecht and A. Merke, Convergent combinations of reinforcement learning with linear function
approximation, in Advances in Neural Information Processing Systems, 2003, pp. 1611–1618.
[69] D. Shah and Q. Xie, Q-learning with nearest neighbors, in Advances in Neural Information Processing

Systems, 2018, pp. 3111–3121.

[70] H. Sharma, M. Jafarnia-Jahromi, and R. Jain, Approximate relative value learning for average-

reward continuous state MDPs, in Proceedings UAI, 2019.

[71] H. Sharma and R. Jain, An approximately optimal relative value learning algorithm for averaged MDPs
with continuous states and actions, in 2019 57th Annual Allerton Conference on Communication,
Control, and Computing (Allerton), IEEE, 2019, pp. 734–740.

[72] H. Sharma, R. Jain, and A. Gupta, An empirical relative value learning algorithm for non-parametric
MDPs with continuous state space, in 2019 18th European Control Conference (ECC), IEEE, 2019,
pp. 1368–1373.

[73] A. Sidford, M. Wang, X. Wu, L. Yang, and Y. Ye, Near-optimal time and sample complexities
for solving markov decision processes with a generative model, in Advances in Neural Information
Processing Systems, 2018, pp. 5186–5196.

[74] S. Singh, T. Jaakkola, M. L. Littman, and C. Szepesv´ari, Convergence results for single-step

on-policy reinforcement-learning algorithms, Machine learning, 38 (2000), pp. 287–308.

[75] R. Srikant and L. Ying, Finite-time error bounds for linear stochastic approximation and td learning,

arXiv preprint arXiv:1902.00923, (2019).

[76] W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Bas¸ar, and J. Liu, A multi-agent oﬀ-policy actor-critic
algorithm for distributed reinforcement learning, arXiv preprint arXiv:1903.06372, (2019).
[77] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, MIT press, 2018.
[78] N. Tripuraneni, N. Flammarion, F. Bach, and M. I. Jordan, Averaging stochastic gradient descent

on riemannian manifolds, arXiv preprint arXiv:1802.09128, (2018).

[79] J. Tsitsiklis, D. Bertsekas, and M. Athans, Distributed asynchronous deterministic and stochastic

MARKOV CHAINS INDUCED BY RECURSIVE STOCHASTIC ALGORITHMS

37

gradient optimization algorithms, IEEE transactions on automatic control, 31 (1986), pp. 803–812.

[80] J. N. Tsitsiklis, Asynchronous stochastic approximation and Q-learning, Machine learning, 16 (1994),

pp. 185–202.

[81] A. W. Van Der Vaart and J. A. Wellner, Weak Convergence and Empirical Processes With Appli-

cations to Statistics, Springer-Verlag New York, 1996, https://doi.org/10.1007/978-1-4757-2545-2.

[82] H.-T. Wai, Z. Yang, P. Z. Wang, and M. Hong, Multi-agent reinforcement learning via double averag-
ing primal-dual optimization, in Advances in Neural Information Processing Systems, 2018, pp. 9649–
9660.

[83] M. J. Wainwright, Variance-reduced q-learning is minimax optimal, arXiv preprint arXiv:1906.04697,

(2019).

[84] Z. Wang, K. Ji, Y. Zhou, Y. Liang, and V. Tarokh, Spiderboost: A class of faster variance-reduced

algorithms for nonconvex optimization, arXiv preprint arXiv:1810.10690, (2018).

[85] Z. Wang, Y. Zhou, Y. Liang, and G. Lan, Stochastic variance-reduced cubic regularization for non-
convex optimization, in The 22nd International Conference on Artiﬁcial Intelligence and Statistics,
2019, pp. 2731–2740.

[86] W. Whitt, Approximations of dynamic programs, I, Mathematics of Operations Research, 3 (1978),

pp. 231–243.

[87] W. Whitt, Approximations of dynamic programs, II, Mathematics of Operations Research, 4 (1979),

pp. 179–185.

[88] H. Xiong, T. Xu, Y. Liang, and W. Zhang, Non-asymptotic convergence of Adam-type reinforcement

learning algorithms under Markovian sampling, arXiv preprint arXiv:2002.06286, (2020).

[89] T. Xu, Z. Wang, and Y. Liang, Non-asymptotic convergence analysis of two time-scale (natural) actor-

critic algorithms, arXiv preprint arXiv:2005.03557, (2020).

[90] K. Zhang, A. Koppel, H. Zhu, and T. Bas¸ar, Global convergence of policy gradient methods to

(almost) locally optimal policies, arXiv preprint arXiv:1906.08383, (2019).

[91] K. Zhang, E. Miehling, and T. Bas¸ar, Approximate equilibrium computation for discrete-time linear-

quadratic mean-ﬁeld games, arXiv preprint arXiv:2003.13195, (2020).

[92] K. Zhang, Z. Yang, and T. Bas¸ar, Decentralized multi-agent reinforcement learning with networked

agents: Recent advances, arXiv preprint arXiv:1912.03821, (2019).

[93] K. Zhang, Z. Yang, and T. Bas¸ar, Multi-agent reinforcement learning: A selective overview of theories

and algorithms, arXiv preprint arXiv:1911.10635, (2019).

[94] K. Zhang, Z. Yang, and T. Basar, Policy optimization provably converges to Nash equilibria in zero-
sum linear quadratic games, in Advances in Neural Information Processing Systems, 2019, pp. 11602–
11614.

