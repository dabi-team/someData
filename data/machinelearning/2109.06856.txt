Performance of a Markovian neural network versus dynamic
programming on a ﬁshing control problem

Mathieu Lauri`ere1, Gilles Pag`es2, Olivier Pironneau3

1
2
0
2

p
e
S
4
1

]

C
O
.
h
t
a
m

[

1
v
6
5
8
6
0
.
9
0
1
2
:
v
i
X
r
a

Abstract

Fishing quotas are unpleasant but eﬃcient to control the productivity of a ﬁshing site. A
popular model has a stochastic diﬀerential equation for the biomass on which a stochastic
dynamic programming or a Hamilton-Jacobi-Bellman algorithm can be used to ﬁnd the
stochastic control – the ﬁshing quota. We compare the solutions obtained by dynamic
programming against those obtained with a neural network which preserves the Markov
property of the solution. The method is extended to a similar multi species model to
check its robustness in high dimension.

Keywords: Stochastic optimal control, partial diﬀerential equations, neural networks.

1. Introduction

Too much ﬁshing can deplete the biomass to a disastrous level and leave ﬁshermen
out of work, or even, in some part of the world, starving. There are several ways to
control ﬁshing. One is to forbid ﬁshing in regions and to alternate ﬁshing and non-ﬁshing
zones [21]. Another is by imposing quotas. In [2] statistical learning was shown to be
very eﬃcient to calibrate the parameters of the ﬁshing model of [9]. In [24] a stochastic
control problem was derived from the model used in [9] and to speed up the computation of
optimal quotas, a solution by statistical learning was proposed and compared to standard
stochastic control solutions like the Hamilton-Jacobi-Bellman equations (HJB). However
the solution provided by the neural network was not Markovian as it used the future
states, given by the stochastic model, to optimise the present. In this article we propose
to study the performance of a Markovian neural network, in the spirit of [12, 14, 10].

The unpopularity of severe quotas is modeled by a penalty in the cost of an optimisa-
tion problem to compute the best ﬁshing strategy which preserves the ﬁsh biomass, i.e.,
keeps it close to an ideal state Xd. Quotas on ﬁshing should also be as stable in time
as possible because ﬁshermen need to know that the quota will not move too much from
one day to the next. This is modeled by another penalty on the time variations of the
quota.

Let Xt be the ﬁsh biomass at time t, Et the ﬁshing eﬀort, interpreted as the number
of boats at sea. In [9], qXt, with the catchability constant q, is the maximum weight of
ﬁsh that a boat can mechanically catch, meaning that the more ﬁsh there is the more
ﬁshermen will catch them, but proportionally to the capacity of his equipment.

1lauriere@princeton.edu, ORFE dept. Princeton University, USA
2gilles.pages@sorbonne-universite.fr, LPMA, Sorbonne Universit´e, Paris, France
3olivier.pironneau@sorbonne-universite.fr , LJLL, Sorbonne Universit´e, Paris, France.

September 15, 2021

 
 
 
 
 
 
Ideally a ﬁsherman may want to catch a quantity Qt on day t. Imposing a quota QM

means Qt < QM .

2. The ﬁshing site model

Consider a ﬁshing site with d types of ﬁsh in interaction, in the sense that some depend
Rd be the ﬁsh biomasses at time t, E(t) the ﬁshing eﬀort
on others for food. Let X(t)
– interpreted as the number of boats at sea – and the capacity to catch type i is qXi(t),
with the catchability q constant over types for simplicity.

∈

An optimal strategy with quotas for each species i is given to each ﬁsherman, to
Rd, of that species caught on a day t. Hence
impose the maximum weight of ﬁsh, Qi(t)
∈
the total amount of ﬁsh of type i caught on a day is E(t) min(qXi(t), Qi(t)), i = 1, . . . , d.
The logistic equation for X(t) says that the biomasses is a consequence of the natural
d species

1r of X, where κ is the d

growth or decay rate r, the long time limit κ−
interaction matrix, and the depletion due to ﬁshing:

×

dX
dt

(t) = X(t)

Λ[r

·

−

κX(t)

−

min(qX(t), Q(t))E(t)].

(1)

The operator r
example, with d = 2 and κ12 < 0, κ21 > 0, then, in (1),

Λ[r] transforms a vector r

7→

∈

Rd into a diagonal d

d matrix. For

×

X(t)

Λ[r

·

−

κX(t)] =

h

X1(r1 −

κ11X1 +

κ12|
|

X2), X2(r2 − |

κ21|

X1 −

κ22X2)

iT

which means that the ﬁrst species lives on its own but proﬁt from the second species
because it eats it as shown by the equation for the second species which has a death rate
augmented by

X1 > 0.

Let F(t) = min(qX(t), Q(t)). The ﬁshing eﬀort E is driven by the proﬁt p

κ21|
|

the operating cost of a boat c, where pi is the price of ﬁsh of species i:

1
E

dE
dt

= p

F

·

−

c.

F minus

·

(2)

The price is driven by the diﬀerence between the demand D(p) and the resource FE:

Φ

dp
dt

= D(p)

−

FE with the demand D(p)i =

a0i
1 + γpi

(3)

where Φ is the inverse time scale at which the ﬁsh market price adjusts. When Φ << 1,
(3) may be approximated by:

D(p)

−

FE = 0

γpiFiE = a0i −

FiE

⇒

p

·

⇒

FE =

1
γ

tr[a0

−

EF],

where the trace operator is deﬁned by tr[a] := Pd
˜qi = qiγ, ˜Qi = Qiγ, and ˜E = E/γ. Then the whole system (1)–(2) rewrites:

i=1 ai. Let us denote a = Pd

1 a0i/γ2,

dX
dt

= X

Λ[r

·

−

κX

−

min(˜qX, ˜Q) ˜E],

d ˜E
dt

= a

−

(tr[min(˜qX, ˜Q)] + c) ˜E,

(4)

2

Λ[

]X. Since we are not going to use the original
where X
·
variables in the sequel, we drop the tildas and write qi, Qi, and E instead of ˜qi, ˜Qi, and
˜E.

] is matrix-vector product Λ[
·

·

Finally we use another change of variables to get rid of the q variable: we replace t by
t/q, Q by qQ and (r, κ, a, c) by q(r, κ, a, c). Then the above system (4) is identical but
now with q = 1. In the end, with a = tr[a] := Pd
1 ai, the whole system for the evolution
of the ﬁsh biomass X and the ﬁshing eﬀort E is:

dX
dt

= X

Λ[r

κX

min(X, Q)E],

−
2.1. The Stochastic vector control problem

−

·

dE
dt

= a

−

(tr[min(X, Q)] + c)E.

(5)

To prevent ﬁsh extinction, a constraint is set on the total catch min(qXi(t), Qi(t))E(t)),
i = 1, . . . , d, per species. The value of Q(t) is found by solving an optimisation problem
described below. It is expected that Qi(t) < qXi(t) otherwise the policy of quota is irrel-
evant in the sense that the ﬁsherman is given a maximum allowed catch which is greater
than what he could mechanically catch.

Let us add a constraint Q(t)

QM so that min(qX(t), Q(t)) = Q(t). Denote ui(t) =
E(t)Qi(t)/Xi(t) the optimal ﬁshing policy for species i per unit mass. We formulate the
optimal control problem in terms of ui rather than Qi. Then E no longer appears and
the problem becomes:

≤

(

¯J :=

Z T

0

min
u
∈U

X(t)
|

−

2dt :
Xd(t)
|

dX
dt

= X

Λ [r

u

−

−

·

κX] , X(0) = X0

(6)

)

where the control is in the constraint space

=

um ≤

{

U

ui(t)

≤

uM ,

i = 1, . . . , d, t

[0, T ]

,
}

∈

which reﬂects a desire to ensure a minimal ﬁshing um and a maximum one uM so as to
guarantee that ui < qE(t) at all time.

2.2. More constraints on quotas

To avoid small ﬁshing quotas we use penalty and add to the criteria

Z T

d
X

−

0

1

αiui(t).

Moreover, to avoid too many daily changes we penalise the quadratic variation of u over
the time period (0, T ), i.e., to ¯J we add β
, where the quadratic variation is
deﬁned as:

E[u]0,T
t

·

[u]0,T

utk−1|
where P ranges over partitions of the interval (0, T ) =
probability when maxk |

t = lim
P
k

utk −

1| →
−

tk −

tk

k

2

K
X
1 |

0. Here Itˆo calculus [7] tells us that:

∪k(tk

−

1, tk) and the limit is in

E[ui]0,T

t = σ2

Z T

0

E[

Xt · ∇Xui|
|

2]dt.

Hence, an optimal policy in the presence of noise is a solution of
"Z T

(cid:26)

¯J := E

h
X(t)
|

−

0

Xd(t)

2dt
|

−

α

u + β

·

minu

∈U

i

#
dt

:

[u]0,T
t

·

3

dXt = X

Λ [(r

u

−

−

·

κX)dt + σdWt] , X(0) = X0

(cid:27)

.

(7)

For the sake of clarity we assume βi = β for all i.

Remark 1. Let Y = κX. A multiplication of the SDE (7) by κ leads to

κ Λ [(r

dYt = Yt ·
Thus, if all the terms on the diagonal of Λ [(r
σdWt] are equal and
if the initials conditions κX(0) are equal, then the components of Y are independent and
equal because κ and Λ commute. It happens only if κ

⊗
Yt)dt + κ

σdWt] κ−

Yt)dt + κ

σdWt = 1dWt.

−

−

⊗

−

−

u

u

T .

⊗

2.3. Existence of solution

First, the solution of the SDE exists and is positive. For clarity the proof is done for

d = 1:

Note that X

[um, uM ].
Hence for every realization Wt(ω) there is a unique strong solution until a blow-up time
τ which is a stopping time for the ﬁltration

ut)X is locally Lipschitz, uniformly in t since ut ∈

w
t = σ(Ws, s

κX

7→

(r

−

−

t,

F

Ns).

≤

On [0, τ [ by Itˆo calculus,

Xt = X 0 exp

(r

σ2
2

)t

−

Z t

−

0

(κXs + us)ds + σWt

,

!

so:

Hence

Z τ

0

Xt ≤
Xsds = +

X 0eSt(um),

t
∀

∈

(0, τ ], where St(v) := (r

σ2
2

)t

−

−

vt + σWt.

is impossible unless τ = +

,
∞

a.s. Therefore

P

∞

X 0eSt(uM ) exp

(cid:18)

−

κ

Z t

0

(cid:19)

eSs(uM )ds

Xt ≤

≤

X 0eSt(um).

Let y = log x and v = r
L∞(R), ∂yv

κey
L∞(R), then Yt has a PDF ρ

L1(R)
and the following equivalent control problem has a solution:

u(ey, t). From [19], if v

∈
L∞ (L2(R)

−

−

∩

∈

∈

σ2
2 −

∩

loc (R), v/(1 +
W 1,1
L∞(R))

y
|
∈
L2 (H 1(R))

)
|

∩

Z T

h

J(v) :=

min
v
∈V

∂tρ + ∂y(vρ)

Z R

R

−

dy

∂yy[

−

0
σ2
2

(ey

−

Xd)2

−

αu(y, t) + β

2i
∂tv(y, t)
|
|

ρ(y, t)dt :

ρ] = 0, ρ(y, 0) = ρ0(y),

R,

y
∀

∈

t
∀

]0, T [.

∈

where

=

v : vm ≤

{

v

V

≤

vM ,

∂tv

k

kL2(

Q

) ≤

K

.

}

Remark 2. This result also gives a computational method, by discretizing the above and
solving it as an optimisation problem. Of the four methods discussed in this article, this
is the most expensive. It was tested in [18] and shown not to be superior in precision to
other methods.

4

 
3. Time discretization

We consider a uniform grid with M + 1 points in time. Let h = T

M , tm = mh and, for

any f deﬁned on [0, T ], f m an approximation of f (tm). Deﬁne the Euler scheme:

Xm+1 = Xm (1 + Λ [(r

um

−

−

κXm)h + σδWm]) ,

(8)

where δW m
preserved by this scheme, but in practice negative values do not seem to appear.

0,1, i = 1, . . . , d. Note that positivity may not be

i ∼

i = W (m+1)h

√hN

W mh

−

i

A Monte-Carlo method is used with K sample solutions of (8) to compute the cost:

JK(u) :=

M
X

m=1

h
K

K
X

k=1

"

Xm
|

k −

2

Xd|

−

α

·

um

k +

with the convention that uM +1

k

= uM
k .

β
h |

um+1

k −

#

,

um
2
k |

Remark 3. Let X
Let σ
(X + a)):

∈

um(X) be two given diﬀerentiable functions.
R+. The following holds (reading hint: u(X + a) is u at X + a, not u times

um+1(X) and X

7→

7→

um+1(Xm+1) = um+1 (Xm (1 + h(r
um+1 (Xm (1 + h(r

κXm
κXm

−
−

−
−

um) + σδW m))
um))) + u0

m+1σXm+1δW m,

≈
m+1 is evaluated at Xm or at Xm (1 + h(r
where the derivative u0
Hence:

κXm

−

−

um) + σδW m).

E[

um+1
|

um

2
|

Xm]
|

−

≈

(cid:12)
(cid:12)um+1 (Xm (1 + h(r
(cid:12)
Xm+1
+ hσ2
|

κXm
−
m+1
2,
|

· ∇

u0

um)))

−

2

um(cid:12)
(cid:12)
(cid:12)

−

so:

β
h

M
X

E[

m=1
M
β
X
h

m=1

≈

um+1
h
|

um

2
|

Xm]
|

−

(cid:20)(cid:12)
(cid:12)um+1 (Xm (1 + h(r
(cid:12)

h

κXm

−

−

um)))

2

um(cid:12)
(cid:12)
(cid:12)

−

+ hσ2

Xm+1
|

· ∇

um+1

(cid:21)

.

2
|

(9)

Even though the ﬁrst term in (9) is dominated by the second term, we may keep it for
numerical convenience.

4. Stochastic Dynamic Programming (SDP)

Consider the value function

"Z T

(cid:26)

E

V (X, t) = min
∈U
dXt = X

u

t

·

h

X(t)
|

−

2dt
Xd(t)
|

−

α

·

u + β[u]t,T
t

i

#
dt

:

κX)dt + σdWt] , X(t) = X

Λ [(r

u

−

−

5

(cid:27)

.

(10)

Let ζh(X, um, z) be the next iterate of a numerical scheme for the SDE starting at X.
For instance:

ζh(X, u, z) = X(1 + Λ

h
(r

u

−

−

i
κX)h + σz√h

), z being one realisation of a Nd

0,1 r.v.

Bellman’s dynamic programming principle tells us that the optimal control of the problem
is the minimiser, um, in:

(cid:26) Z (m+1)h

E h

u

vm(X) := min
mh
∈U
+ β[u]mh,(m+1)h
t

α
ζh(X, u, z)
|
−
(cid:27)
+ E[vm+1(ζh(X, u, z)]
hE h
u + βE h
2i
n
um+1(ζh(X, u, z))
Xd|
ζh(X, u, z)
|
|
o
+ E[vm+1(ζh(X, u, z)]
, with vM (X) = 0.

Xd|

hα

dτ

−

−

−

u

·

·

2

i

min
u
∈U

≈

Evidently,

u

2i
|

−

(11)

E h
ζh(X, u, z)
|

2i
Xd|

−

=

X
|

−

Xd + hXΛ[r

κX

−

u]

2 + h
σX
|
|

2.
|

−

For every component, to compute E[vm+1(ζh(X, u, z))], we use a quadrature formula
Q
with Q points
zq}
1 based on optimal quantization of the normal
{
distribution Nd
0,1 (see [25] or [22, Chapter 5] and the website www.quantize.maths-fi.
com for download of grids) so that

Q
1 and weights

wq}

{

E[vm+1(ζh(X, u, z))]

Q
X

q=1

≈

wqvm+1(ζh(X, u, zq))

(12)

Finally at every time step and every Xj = jL/J, j = 0, . . . , J (with L >> 1), the result
J
is minimised with respect to u
j=1
is obtained on a grid and a piecewise linear interpolation is constructed to prepare for
the next time step tm
−

by a dichotomic search. In this fashion

um(Xj)
}

∈ U

1.

{

4.1. Numerical results for a single species

For the numerical tests we have chosen: r = 2, κ = 1.2, Xd = 1, T = 2,

α = 0.01,

β = 0.1,

σ = 0.1, L = 3,

J = 40, M = 50, K = 100

7→

and X0 = 0.5 + 0.1 j, j = 0, . . . , 9. Figures 1 and 5 show the control surface (X, t)
7→
v(X, t). Although the control seems to be either
u(X, t) and the value function (X, t)
0.5 or 1 everywhere, there is a small interval near X = Xd = 1 in which it is not bang-
bang.
It is an important region, as seen on the sample solutions which are shown on
Figure 3 and 4 where it is clear that the control is not always 0.5 or 1. For these an
initial condition X0 is chosen, a noise dW is generated and the SDE are integrated by
the Euler scheme with the approximately optimal control u(Xt, t) obtained by the SDP
method described above. Then, by deﬁnition of the cost function, Xt should tend to
be equal to Xd as much as possible without too many jumps for u. The trajectory is
compared with one on which u = uM , i.e., no quota.

Finally on Figure 2 the cost function of the control problem is plotted for 100 samples
and various values of X0. Away from Xd, it increases, which makes sense because X0 = 1
means that we start with the optimal value in terms of ﬁsh biomass.

6

Figure 1: Solution u(X, t) from SDP.

Figure 2: Values of the average cost versus X0 for
100 realizations of dW and with the SDP, for the
3 time meshes 50 n, showing that the results are
independent of n.

Figure 3: Simulation of the ﬁshing model with a
quota function computed by SDP and X0 = 0.7.

Figure 4: Simulation of the ﬁshing model with a
quota function computed by SDP and X0 = 1.3.

5. Hamilton-Jacobi-Bellman solutions (HJB)

Let us return to (11) and note that:

vm(X)

minu

∈U

≈

(cid:26)

X
|

−

2

Xd|

−

α

·

u +

β
h

E h
um+1(ζh(X, u, z))
|

2i
u(X)
|

−

(cid:27)
+E[vm+1(ζh(X, u, z)]

,

(13)

E h

with vM = 0. According to Remark 3
(cid:20)(cid:12)
(cid:12)um+1 (cid:16)
(cid:12)
X + XΛ
h
u
2 |

um+1(ζh(X, u, z))
|
−
(cid:12)
(cid:12)um+1 (X + XΛ [h(r
(cid:12)

2i
≈
|
κX)])

σXu0

−

−

≈

+

E

u

(cid:12)
(cid:12)
(cid:12)

2

m+1

2.
|

h

h(r

κX

−

−

u) + √hσz

i(cid:17)

−

2(cid:21)

(cid:12)
(cid:12)
(cid:12)

u

(14)

Also, by Itˆo formula,

E[vm+1(ζh(X, um, z)] = E[vm+1(X + hXΛ[r

κX

−

−

um] + Xσ√hNd

0,1)]

7

12010.60.81Xtu(X,t)0.60.811.21.4−1−0.500.51234·10−2X0averagecostn=1n=2n=400.511.520.60.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquota00.511.520.60.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquota= vm+1(X + hXΛ[r

κX

−

−

um]) +

h
2 |

σX

2v00
|

m+1(X).

(15)

Consequently, with v00

m+1 replaced by v00

m for numerical convenience, (11) becomes

vm(X)

hα

−

·

(cid:26)

min
≈
um
∈U
um(X) + β

vm+1(X + hXΛ[r

−
um+1(X + hXΛ[r
|

−
κX

−

σX

h
2v00
|
2 |
um(X)
2 + h
|

m+1(X) + h
X
|
β
m+1
2 |

σXu0

2
|

−
(cid:27)

2

Xd|
. (16)

um)]

−

−

κX

um(X))] +

The last line is an approximation motivated by the search for a stable implicit numerical
scheme for vm:

1
h
+

2
|

σX
2
um+1(X + hXΛ[r

vm(X) + |
β
h |

v00

1
h

m(X) =

vm+1(X + hXΛ[r

κX

um]) +

κX

−

−

um])

−

−
um(X)
2 +
|

β
2 |

−
σX2

m+1

u0
|

2

Xd|

−

−

α

·

um(X)

(17)

X
|
2.
|

Furthermore, um is given by minimising the right hand side of (17):

um = um+1(ζh(X, um, 0) +

h
2β

(α + Λ[X]

∇

vm+1(ηh(X, um, 0)) + o(h,

h
β

),

(18)

capped by the bounds um and uM . Boundary conditions are not needed at X = 0
because the PDE is self starting but for large X, cancelation of the quadratic terms
requires v00

m(X)

2/

σ

∼ −

||

2.
||

Remark 4. Notice that (18) makes an important use of the comment in Remark 3.

5.1. Numerical results for a single species by HJB

The ﬁnite element method of degree 1 on intervals was used to approximate spa-
tially (17). The linear systems were solved using the FreeFem++ software [15]. The
range of X was approximated by the segment (0, 3) discretized into 160 intervals; 200
time steps were used.

With the same parameters as in Section 4.1 the results are shown on Figures 7, 6, 9,

10 and 8. These should be compared, respectively, with Figures 1, 5, 3, 4 and 2.

Except for the singularity at ﬁnal time, which is not handled in the same way, the
u(t) is

results are similar. HJB seems to handle better the penalty term with β as t
not bang-bang.

7→

6. Optimisation with Markovian feedback neural network controls

In this section, we look for Markovian feedback controls, i.e., functions u : [0, T ]
u(x, t)

×
R+
uM for every (x, t). We restrict our
attention to such functions which are encoded by neural networks. This strategy has
been used previously in the optimal control literature, see e.g. [12],[14],[10],[3].

R, adapted to Xt, satisfying um ≤

→

≤

8

Figure 5: SDP solution: V (X, t).

Figure 6: HJB solution: value function V (X, t).

Figure 7: Solution u(X, t) from HJB.

Figure 8: Values of the cost functions for the
3 space
50n, to solve the
times meshes 40n
control problem by a HJB algorithm.

×

×

Figure 9: Simulation of the ﬁshing model with a
quota function computed by HJB and X0 = 0.7.

Figure 10: Simulation of the ﬁshing model with a
quota function computed by HJB and X0 = 1.3.

9

01201012XtV(X,t)01201012XtV(X,t)12010.60.81Xtu(X,t)0.60.811.21.4−1−0.500.51234·10−2X0averagecostn=1n=2n=400.511.520.60.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquota00.511.520.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquotaGiven an activation function ψ : R

some helpful notation:

→

R (e.g. a sigmoid or ReLU), let us introduce

(cid:26)

φ : z

Ld1,d2 =

ψ(βi + wi ·
is the set of layer functions with input dimension d1 and output dimension d2.

z)
}

7→ {

∈

∈

∈

β

×

d2
i=1 ∈

Rd2, w

Rd2

Rd1

Rd2

d1

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:27)

(19)

Let d =

d0, . . . , d‘+1}
with ‘ hidden layers and one output layer,

{

; deﬁne the set of feedforward fully connected neural networks

Nd =

(cid:26)

u : Rd0

Rd‘+1, u = φ(‘)

→

φ(‘

1)

−

◦

◦ · · · ◦

φ(0)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

φ(i)

∈

Ldi,di+1, i = 0, . . . , ‘

(cid:27)

(20)

where

◦

denotes the composition of functions. Denote by Θ the set of parameters:

Θ =

(cid:26)

θ := (β(0), w(0), β(1), w(1),

(cid:12)
(cid:12)
, β(‘), w(‘))
(cid:12)
(cid:12)

· · ·

β(i)

∈

Rdi, w(i)

∈

Rdi

×

d1+1

(cid:27)

.

For each θ
may be rewritten as:

∈

Θ, the corresponding network function will be denoted by uθ. Hence (20)

Nd =

uθ : θ
{

Θ
}

∈

For the ﬁshing control problem d0 = d + 1 (d species for the space variable, plus the
time variable) and d‘+1 = d (the dimension of the control variable u). Furthermore,
uM , it is suﬃcient to take the last activation
to ensure the constraint um ≤
≤
function which satisﬁes this constraint.
In the present implementation we have used
um)σ(x) where σ is the sigmoid function. Figure 11 shows a fully connected
um + (uM −
2 layers neural network with 2 inputs, one output and 10 neurons in each layer.

u(x, t)

The control problem is approximated by the minimisation over θ

Θ of:

∈

J(θ) =

h
K

K
X

k=1

M
X
m=1 k

Xm

k −

2

Xdk

−

α

·

uθ(Xm

k , tm) +

β
h |

uθ(Xm

k , tm)

uθ(Xm
−
k

1

, tm

1)
−

2(21)
|

−

where Xm
realization of the noise.

k is the result of one step of the Euler scheme (8) with uθ(Xm
−
k

1

, tm

1) and a
−

Noting that (21) is a sum of terms, we can run a Stochastic Gradient Descent or one
of its variants like ADAM. At each iteration, we sample a mini-batch of initial positions and
realizations of the noise which allow us to compute random realizations of trajectories and
compute a partial sum of (21). It is then used to compute a gradient and back-propagate
it to adjust θ.

The stochastic gradient algorithm ADAM is used to update θ towards a local minimum
; the

of J(θ) needs the gradient of J(θ). A mini-batch is a random subset B
1, . . . , K
⊂ {
gradients with respect to θ are computed as follows. Let JB(θ) := h
P
B jk(θ) where
k
K
∈

}

jk(θ) =

M
X
m=1 k

Xm

k −

2

Xdk

−

α

·

uθ(Xm

k , tm) +

β
h |

uθ(Xm

k , tm)

uθ(Xm
k

1

−

, tm
−

2.
1)
|

−

The gradient of jk(θ) with respect to θ =

djk
dθn

=

X

m

djk
dum

dum
dθn

with

{
djk
dum = P m

k Xm

k −

θn}n is made of:
α

2

β
h

−

(um+1

2um + um

1),
−

(22)

−

10

Figure 11: Sketch of a two layers neural network with 2 inputs, X, t and one output u. Each layer here
has 10 neurons while for this article we used 100 neurons in each layers.

where P m
k

is deﬁned by P M
k

2

−

= 0 and

P m
−
k

1

= P m

k (1 + h(r

2κXm

k −

−

um) + σdW m

k , )

2(Xm

k −

−

Xd), m = M

2, . . . , 1.(23)

−

Softwares for neural networks such as tensorflow provide automatic diﬀerentiation

tools (back propagation) to compute dum
dθn .

Recall that ADAM algorithm consists in choosing B randomly and then

1. Let α = 0.001, m1 = 0.9, m2 = 0.999, ε = 10−

8.

Loop on i:

{

dJB
2. Set gi =
N
n=1
dθi
n }
i
3. Set pi = m1p
1 + (1
−
¯
4. Set qi = m2qi
1 + (1
−
j = mj ·
n = θi

6. Update θi

5. Set mi

mi
−
j

1

1
−
n −

−
−

m1)gi
gi
m2)
|
, j = 1, 2,

ˆpi
α
n
·
√ˆqi + ε

2
|

ˆpi =

pi

mi
1

1

−

, ˆqi =

qi

mi
2

1

−

, n = 1, . . . , N , the number of parameters.

6.1. Numerical results using a neural network for a single species

Notice that we use the tools of AI but there is no learning phase from known solutions,
rather the learning phase is simply to ﬁnd the best parameters to achieve the minimum
of a given cost function.

To check the results we implemented also a single layer neural network with 15000
neurons directly in C++ using the automatic diﬀerentiation in reverse mode of the library
adept [16] and a conjugate gradient algorithm instead of ADAM. Convergence is fast (20
to 40 iterations) but this simple method does not work for multiple layered neural network
because of memory limitation.

For the single species numerical test, the results are the same whether obtained by

the one layer or the two layers networks.

11

Results are shown on Figures 12, 14, 15 and 13 and should be compared, respectively,

with Figures 7, 9, 10 and 8 and/or, respectively, with Figures 1, 3, 4 and 2.

Note that the results are closer to those obtained with HJB than those obtained with

SDP.

Our general impression is that all 3 methods are equal, with a slight reserve for SDP
for which the control oscillates much more than with the other two methods, implying
that it does not handle as well the β-penalisation term.

Figure 12:
computed by the Markovian Neural Network.

Dynamic feedback control,uθ(X, t)

Figure 13:
Single species: Values of the cost
versus X0 for 100 realizations with the Marko-
vian neural network,
for the 3 time meshes
50

n.

×

Figure 14:
Simulation for a single ﬁsh species
computed by the Markovian Neural Network and
X0 = 0.7. Performance with and without quota ut.

Figure 15:
Simulation for a single ﬁsh species
computed by the Markovian Neural Network and
X0 = 1.3. Performance with and without quota ut.

12

012010.60.81Xtuθ(X,t)0.40.60.811.21.41.6−1−0.500.51234·10−2X0averagecostn=1n=2n=400.511.520.60.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquota00.511.520.811.21.4timeu(Xt,t),XtXtwithquotaquotautXtwithoutquota7. Numerical results: 3 species

We now turn our attention to an example with 3 species. Consider the following

interaction matrix between species:

κ =






1.2
0.2
0.1






0.1 0
−
1.2
0
0.1
1

All other parameters are as in Section 4.1 and Xd = (1, 1, 1)T .

With SDP (Stochastic Dynamic Programming) there is a diﬃculty: the minimisation
in (12) is now 3 dimensional and cannot be obtained by dichotomy. So the method was
not tested. We have compared HJB and Neural Network optimisation with 1 and then
with 2 layers. The results diﬀer; according to the last subsection none of the methods
found the true solution.

7.1. Numerical results for 3 species with HJB

The computation is done with 80 time steps. Here too a ﬁnite element discretisation
was used with P 1 elements on tetraedra. The mesh for the cube (0, 3)3 is obtained from
an automatic mesh generator from a 30
30) surface mesh resulting into 29791
30
vertices and 16200 tetraedra. It took 7 min on an M1 Apple laptop. The vector valued
(0.5, 1)3 computed with HJB is shown on
optimal control X, t
∈
Figures 16 to 24. In these, two of the 3 coordinates of X, are ﬁxed at their Xd values.

(0, T )

R3

7→

×

×

×

∈

u

Then the ﬁshing model is integrated with the optimal u and a random realization of
dW for 2 values of X0: X0 = (0.7, 0.7, 0.7)T or (1.3, 1.3, 1.3)T . The results are compared
with a similar simulation without quota, i.e., u
1. See Figures 25, 26, 27. The quality
of the optimisation is seen visually when Xt is closest to Xd and numerically from the
lowest value of the cost function, plotted versus X0 on Figure 28.

≡

Figure 16: HJB: X1, t

u1.

7→

Figure 17: HJB: X2, t

u1.

7→

Figure 18: HJB: X3, t

u1.

7→

13

12010.60.81Xtu1(X1,t)12010.60.8Xtu1(X2,t)12010.60.8Xtu1(X3,t)Figure 19: HJB: X1, t

u2.

7→

Figure 20: HJB: X2, t

u2.

7→

Figure 21: HJB: X3, t

u2.

7→

Figure 22: HJB: X1, t

u3.

7→

Figure 23: HJB:X2, t

u3.

7→

Figure 24: HJB:X3, t

u3.

7→

Figure 25: HJB:Optimal biomass
and quota function when X1(0) =
0.7 and X1(0) = 1.3.

Figure 26: HJB:Optimal biomass
and quota function when X2(0) =
0.7 and X2(0) = 1.3.

Figure 27: HJB: Optimal biomass
and quota function when X3(0) =
0.7 and X3(0) = 1.3.

7.2. Optimisation with a Neural Network with one layer of 15000 Neurons

The parameters are the same. The neural network has one layer with 15000 neurons.
Conjugate gradient with optimal step size is used and the derivatives are computed with
automatic diﬀerentiation in reverse mode.

Convergence of the conjugate gradient algorithm is seen on Figure 29. The norm of
the gradient is reduced from 0.1 to 0.00017. Then the results are displayed as above: ﬁrst
the optimal quota vector function, with 9 surfaces on Figures 30 to 38. The fact that the
u2(1, X2, 1, t) is ﬂat is an obvious indication that the solution found by
surface
the NN with one layer is only suboptimal.

t, X2} 7→

{

Then two types of random trajectories, one starting at X0 = (0.7, 0.7, 0.7)T , the other
at (1.3, 1.3, 1.3)T . Results are shown on Figures 39, 40 and 41. The section ends with a
display of the cost function versus X0 on Figure 42. The results seem less accurate than
with HJB. Figure 34 is surprising! It shows also on Figure 40 where the optimal quota

14

12010.60.81Xtu2(X1,t)12010.60.81Xtu2(X2,t)12010.81Xtu2(X3,t)12010.81Xtu3(X1,t)12010.81XtHJB:u3(X2,t)12010.60.81Xtu3(X3,t)00.511.520.60.811.2timeu(Xt,t),XtX1u1Y1X1u1Y100.511.520.811.2timeu(Xt,t),XtX2u2Y2X2u2Y200.511.520.60.811.2timeu(Xt,t),XtX3u3Y3X3u3Y3Figure 28: Values of the average cost versus
X0 for 20 realizations by HJB for the 3 species
problem.

Figure 29:
convergence of the cost function
during the optimisation process to solve the 3D
problem with a NN with one layer of 150000
neurons.

does not steer X anywhere near Xd.

Figure 30: X1, t

u1.

7→

Figure 31: X2, t

u1.

7→

Figure 32: X3, t

u1.

7→

Figure 33: X1, t

u2.

7→

Figure 34: X2, t

u2.

7→

Figure 35: X3, t

u2.

7→

7.3. With two layers

The same problem is solved with a two layers neural network with 100 neurons on

each layer.

15

0.60.811.21.42·10−23·10−24·10−25·10−2X0averagecost0501001502000.120.140.160.180.2ncostJJn00.511.50.510.60.81tXu1(X1,t)00.511.50.510.40.50.6tXu1(X2,t)00.511.50.510.60.8tXu1(X3,t,t)00.511.50.510.60.81tXu2(X1,1,t)00.511.50.510.40.50.6tXu2(X2,t)00.511.50.510.60.81Xu2(X3,t,t)Figure 36: X1, t

u3.

7→

Figure 37: X2, t

u3.

7→

Figure 38: X3, t

u3.

7→

Figure 39: Optimal biomass and
quota function computed with
the single layer NN when X0 =
0.7 and X0 = 1.3.

Figure 40: Optimal biomass and
quota function computed with
the single layer NN when X0 =
0.7 and X0 = 1.3.

Figure 41: Optimal biomass and
quota function computed with
the single layer NN when X0 =
0.7 and X0 = 1.3.

Figure 42: Values of the average cost versus X0 for 10 realizations of dW and with the Markovian neural
network with 15000 neurons and 10 conjugate gradient iterations.

The results are displayed as above: ﬁrst the optimal quota vector function, with 9

surfaces on Figures 43 to 51.

Then two types of random trajectories, one starting at X0 = (0.7, 0.7, 0.7)T , the other
at (1.3, 1.3, 1.3)T . Results are shown on Figures 52, 53 and 54. The section ends with a

16

00.511.50.510.60.81tXu3(X1,t)00.511.50.510.40.50.6tXu3(X2,t)00.511.50.510.60.81tXu3(X3,t,t)00.511.520.60.811.21.4timeu1(X1t,t),X1tu1tX1tu1tX1t00.511.520.60.811.2timeu(Xt,t),Xtu2tX2tu2tX2t00.511.520.60.811.2timeu(Xt,t),Xtu3tX3tu3tX3t0.60.811.23·10−24·10−25·10−2X0averagecostFigure 43: X1, t

u1.

7→

Figure 44: X2, t

u1.

7→

Figure 45: X3, t

u1.

7→

display of the cost function versus X0 on Figure 55.

Figure 46: X1, t

u2.

7→

Figure 47: X2, t

u2.

7→

Figure 48: X3, t

u2.

7→

Figure 49: X1, t

u3.

7→

Figure 50: X2, t

u3.

7→

Figure 51: X3, t

u3.

7→

Consequently, the neural network with 2 layers give decent results, better than HJB.

The neural network with one layer only gives poor results.

7.4. Assessment of the results

Remark 1 allows the construction of a simple 3D solution from a 1D solution.
Let v, y be solution of

(Z T

0

min
v

E h
y
|

2

yd|

−

−

αv + β

2i
|

v0
|

dt : dyt = yt(r

−

y + v + σdwt) , y(0) = y0

)

17

00.511.500.511.50.60.81tXu1(X1,t)00.511.500.511.50.60.70.8tXu1(X2,t)00.511.500.511.50.80.9tXu1(X3,t,t)00.511.500.511.50.91tXu2(X1,1,t)00.511.500.511.50.60.81tXu2(X2,t)00.511.500.511.50.80.91Xu2(X3,t,t)00.511.500.511.50.9tXu3(X1,t)00.511.500.511.50.60.8tXu3(X2,t)00.511.500.511.50.60.81tXu3(X3,t,t)Figure 52: Optimal biomass and
quota function computed with
the two layers NN when X0 = 0.7
and X0 = 1.3.

Figure 53: Optimal biomass and
quota function computed with
the two layers NN when X0 = 0.7
and X0 = 1.3.

Figure 54: Optimal biomass and
quota function computed with
the two layers NN when X0 = 0.7
and X0 = 1.3.

Figure 55: Values of the average cost versus X0 for 100 realizations of dW and with the Markovian
neural network with two layers of neurons.

R3

Let κ
(y, y, y)T and dWt = (dwt, dwt, dwt)T . Then

∈

×

3, u = (v, v, v)T , r = (r, r, r)T , α = (α, α, α)T , σ = Λ[(σ, σ, σ)T ], Y =

Let X = κ−

1Y. It implies

dYt = Λ [(r

u

−

−

Y)dt + σdWt] Y

dXt = κ−

1Λ [(r

u

−

−

κX)dt + σdWt] κX.

The matrix Λ [(r
−
mutes with κ. Therefore

−

u

κX)dt + σdWt] is diagonal and all terms are equal, so it com-

dXt = Λ [(r

u

−

−

κX)dt + σdWt] X.

and u is solution of

¯J := E

min
u
∈U

"Z T

0

"

κ(X
|

−

κ−

2dt
1Yd)
|

−

α

·

u + β

#

#
dt

du
2
dt |

|

18

00.511.520.60.811.21.4timeu1(X1t,t),X1tu1tX1tu1tX1t00.511.520.60.811.2timeu(Xt,t),Xtu2tX2tu2tX2t00.511.520.60.811.2timeu(Xt,t),Xtu3tX3tu3tX3t0.40.60.811.21.41.6−2·10−202·10−24·10−20.1X0averagecostbecause this is 3 times the cost function of the problem in y, v.

Let X, u be the solution of this problem, then by construction X = κ−

1Y and

u(X1, X2, X3) = (v(Y1), v(Y2), v(Y3))T = (v(y), v(y), v(y))T .

For instance, u1(1, X2, 1) = v((κ(1, X2, 1)T )1).

Now we build on the fact that v is approximatively bang-bang and equal to 0.5 when

y < 1 and 1 otherwise.

Example

κ =






1.2
0.2
0.1



 κ−

1 =

0.1 0
−
1.2
0
0.1
1






0.822
0.137
0.0685

−
−

0.0685 0
0.822
0
0.089 1

−



 κ−

1



1


1
 =

1






1.16
0.89
1.09




 ×

0.77

The scaling 0.77 plays no role.

u(X1, Xd2, Xd3) = v



κ






X1







0.89
1.09

×
×

0.77 = 0.685
0.77 = 0.839





 = v









0.69
1.2X1 −
0.2X1 + 0.82
0.1X1 + 0.91











Hence u1(X1, Xd2, Xd3) is expected to be bang-bang at X1 = 0.56, u2(X1, 1, 1) is expected
to be bang bang at X1 = 0.9, and u3(X1, 1, 1) is expected to be bang bang at X1 = 0.9.

u(Xd1, X2, Xd3) = v



κ



1.16




1.09

×

×

0.77 = 0.89
X2
0.77 = 0.89













 = v







1.07
0.1X2
−
0.18 + 1.2X2
0.98 + 0.1X2











Hence u1(Xd1, X2, Xd3) is expected to be bang bang at X2 = 0.7, and u2(Xd1, X2, Xd3)
is expected to be bang-bang at X2 = 0.68 and u3(Xd1, X2, Xd3) is expected equal to be
bang bang at X2 = 0.2.

u(Xd1, Xd2, X3) = v



κ






1.16
0.89

0.77 = 0.89
0.77 = 0.685









 = v

×
×

X3











1.0
1.0
0.16 + X3











Hence u1(Xd1, Xd2, X3) and u2(Xd1, Xd2, X3), are expected equal to 1 everywhere and
u3(Xd1, Xd2, X3)) is expected to be bang-bang at X3 = 0.84.

7.5. Optimisation with HJB when Xd = κ−

11
The same numerical test was done with HJB but with Xd = (1.16, 0.089, 1.09)T . The

results are shown on Figures 56 to 67. The lowest cost is 0.04, for X0 = 1.

The solution is nearer to the one constructed above but still fairly diﬀerent. The
Neural Networks performances are poor on this test. It is likely that they produced a
local minimum.

19

Figure 56: 3 species: X1, t

7→

u1.

Figure 57: 3 species: X2, t

u1. Figure 58: 3 species: X3, t

u1.

7→

7→

Figure 59: 3 species: X1, t
u2.

7→

Figure 60: 3 species: X2, t
u2.

7→

Figure 61: 3 species: X3, t
u2.

7→

Figure 62: 3 species: X1, t

7→

u3.

Figure 63: 3 species:: X2, t

u3. Figure 64: 3 species: X3, t

u3.

7→

7→

Figure 65: 3 species: Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

Figure 66:
3 species:Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

Figure 67:
3 species:Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

20

00.511.50.510.60.81tXu1(X1,t)00.511.50.510.60.81tXu1(X2,t)00.511.50.510.6tXu1(X3,t)00.511.50.510.60.8tXu2(X1,t)00.511.50.510.60.81tXu2(X2,t)00.511.50.510.61Xu2(X3,t)00.511.50.510.60.8tXu3(X1,t)00.511.50.510.60.81tXu3(X2,t)00.511.50.510.60.81tXu3(X3,t)00.511.520.60.811.21.4timeu1(X1t,t),X1tu1tX1tu1tX1t00.511.520.60.811.2timeu(Xt,t),Xtu2tX2tu2tX2t00.511.520.60.811.2timeu(Xt,t),Xtu3tX3tu3tX3t8. Numerical results: 5 species

The great advantage of Neural Network optimisation is that it scales well with di-
mensions. So to show that it is possible, with the same computer code with very few
modiﬁcations we computed with the one-layer NN with 150000 neurons a case with 5
species. 20 iterations of conjugate gradients were done.

All parameters are as above except the species correlation matrix:





κ =

0.1 0.0
−
0.0
1.2
1.2
0.2
0.1
0.0
0.0
0.1
Only components 1,2,5 are shown see Figures 68 to 76. On Figures 77, 78 and 79
trajectories with optimal quotas show that the NN solution is in general driving X towards
Xd.

0.1
−
0.1
−
0.0
0.0
1.2

0.0
0.0
0.1
−
1.2
0.0

1.2
0.2
0.0
0.0
0.1

















Figure 68: 5 species: X1, t

7→

u1.

Figure 69: 5 species: X2, t

u1. Figure 70: 5 species: XD, t

u1.

7→

7→

Figure 71: 5 species: X1, t
u2.

7→

Figure 72: 5 species: X2, t
u2.

7→

Figure 73: 5 species: X3, t
u2.

7→

The minimum of the cost function is 0.038 for X0 = 1.

9. Conclusion

Control of the biomass of a ﬁshing site has been here a mathematical opportunity
to test the numerical methods at hand. One of the advantage of the model is that it is

21

00.511.50.510.60.81tXu1(X1,t)00.511.50.510.60.8tXu1(X2,t)00.511.50.510.40.50.6tXu1(XD,t)00.511.50.510.60.81tXu2(X1,t)00.511.50.510.60.81tXu2(X2,t)00.511.50.510.40.50.61Xu2(X3,t)Figure 74: 5 species: X1, t

7→

u3.

Figure 75: 5 species: X2, t

u3. Figure 76: 5 species: X3, t

u3.

7→

7→

Figure 77: 5 species: Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

Figure 78: 5 species: Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

Figure 79: 5 species: Optimal
biomass and quota function com-
puted with the single layer NN
when X0 = 0.7 and X0 = 1.3.

meaningful in any dimension, the number of species. Thus it is a testbed for stochastic
control numerical methods.

We have compared Hamilton-Jacobi-Bellman solutions and stochastic dynamic pro-

gramming with two implementation of a neural network based optimisation.

The later is conceptually very simple and the computer libraries of AI and Automatic
Diﬀerentiation can be used. But they need to be validated and it is the object of this
article.

Stochastic dynamic programming is diﬃcult to use numerically beyond dimension 2
and HJB solutions do not scale beyond dimension 4, unless sophisticated discretisation
tools are used like sparse grids.

Neural network based optimisation can be used for large dimension problems but
assessing the precision of the answer seems diﬃcult. Here in dimension 3 the one layer
network did not work well and with the two layers network we could observe discrepancies
with HJB solutions. In dimension 5, the numerical solution seems reasonable but it is
probably suboptimal.

References

[1] P.M. Allen and J.M. McGlade: Modelling complex human systems: A ﬁsheries ex-

ample. European Journal of Operational Research 30 (1987) 147-167.

[2] P. Auger and O. Pironneau, Parameter Identiﬁcation by Statistical Learning of a

22

00.511.50.510.60.81tXu3(X1,t)00.511.50.510.60.8tXuD(X2,t)00.511.50.510.50.5tXuD(XD,t)00.511.520.60.811.21.4timeu1(X1t,t),X1tu1tX1tu1tX1t00.511.520.60.811.2timeu(Xt,t),Xtu2tX2tu2tX2t00.511.520.60.811.21.4timeu(Xt,t),XtuDtXDtuDtXDtStochastic Dynamical System Modelling a Fishery with Price Variation. Comptes-
Rendus de l’Acad´emie des Sciences. May 2020.

[3] A. Bachouch, C.Hur´e, N. Langren´e and Huyˆen Pham; Deep neural networks al-
gorithms for stochastic control problems on ﬁnite horizon: numerical applications;
arXiv:1812.05916v3 [math.OC] 27 Jan 2020.

[4] S. Balakrishnan and V. Biega. Adaptive-critic-based neural networks for aircraft
optimal control. Journal of Guidance, Control and Dynamics, 19(4), 893–898. 1996.

[5] R. Bellman, Dynamic Programming, Princeton, NJ: Princeton University Press,

1957.

[6] D. Bertsekas, Reinforced Learning and Optimal Control. Athena Scientiﬁc, Belmont

Mass. 2019.

[7] A. Bick. Quadratic-Variation-Based dynamic strategies. Management Sciences, vol

41, No 4, p722-732 (1995).

[8] S. Boyd and C. Barratt: Linear Controller Design – Limits of Performance. Prentice-

Hall, 1991.

[9] T. Brochier, P. Auger, D. Thiao, A. Bah, S. Ly, T. Nguyen Huu, P. Brehmer. Can
overexploited ﬁsheries recover by self-organization? Reallocation of the ﬁshing eﬀort
as an emergent form of governance. Marine Policy, 95 (2018) 46-56.

[10] R. Carmona and M. Lauri`ere: Convergence Analysis of Machine Learning Algorithms
for the Numerical Solution of Mean Field Control and Games: II–The Finite Hori-
zon Case. arXiv preprint arXiv:1908.01613, 2019. To Appear in Annals of Applied
Probability.

[11] F. Chollet : Deep learning with Python. Manning publications (2017).

[12] E Gobet and R Munos. Sensitivity Analysis Using Itˆo–Malliavin Calculus and Mar-
tingales, and Application to Stochastic Optimal Control. SIAM Journal on control
and optimisation, 2005

[13] I. Goodfellow, Y. Bengio and A. Courville (2016): Deep Learning, MIT-Bradford.

[14] J. Han and W. E. Deep learning approximation for stochastic control problems. Deep

Reinforcement Learning Workshop, NIPS (2016)

[15] F. Hecht (2012): New development in FreeFem++, J. Numer. Math., 20, pp. 251-

265. (see also www.freefem.org.)

[16] R. G. Hogan :

reverse-mode automatic diﬀerentiation using expres-
sion templates in C++. ACM Trans. Math. Softw., 40, 26:1-26:16 (2014) :
www.met.reading.ac.uk/clouds/adept/

Fast

[17] R. Kamalapurkar and P. Walters and J. Rosenfeld and W. Dixon, Reinforcement

Learning for Optimal Feedback Control. Springer 2018.

23

[18] M. Lauri`ere and O. Pironneau : Dynamic Programming for mean-ﬁeld type control

J. Optim. Theory Appl. 169 (2016), no. 3, 902–924.

[19] C. Le Bris and P.L. Lions, Existence and uniqueness of solutions to Fokker-Planck
type equations with irregular coeﬃcients. Comm. Partial Diﬀerential Equations, 33,
1272-1317 , 2008.

[20] A. Moussaoui and P. Auger: A bioeconomic model of a ﬁshery with saturated catch
and variable price: Stabilizing eﬀect of marine reserves on ﬁshery dynamics. Ecolog-
ical Complexity 45 (2021) 100906.

[21] A. Moussaoui, M. Bensenane, P. Auger, A. Bah, On the optimal size and number
of reserves in a multi-site ﬁshery model, Journal of Biological Systems. Vol. 23, No.
01, pp. 31-47 (2015)

[22] G. Pag`es. Numerical Probability: An Introduction with Applications to Finance.

Springer, Berlin, 2018, 574p.

[23] G. Pag`es, H. Pham and J. Printems: An Optimal Markovian Quantization Algorithm
For Multi-Dimensional Stochastic Control Problems, Stochastics and Dynamics ,
4(4):501–545, 2004.

[24] G. Pag`es and O. Pironneau, Protection of a ﬁshing Site with Optimal Quotas:

Dynamic Programming versus Supervised Learning. Encyclopedia, E. Tr´elat ed. (to
appear)

[25] G. Pag`es, J. Printems. Optimal quadratic quantization for numerics: the Gaussian

case, Monte Carlo Methods and Appl., 9(2):135–165, 2003.

[26] J. Yong and X. Y. Zhou: Stochastic Controls Hamiltonian Systems and HJB Equa-

tions Application of Mathematics series vol 43. Springer 1991.

24

