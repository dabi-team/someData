JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Two-level Graph Neural Network
Xing Ai, Chengyu Sun, Zhihong Zhang∗, and Edwin R Hancock, Fellow, IEEE

2
2
0
2

n
a
J

3

]

G
L
.
s
c
[

1
v
0
9
1
1
0
.
1
0
2
2
:
v
i
X
r
a

Abstract—Graph Neural Networks (GNNs) are recently pro-
posed neural network structures for the processing of graph-
structured data. Due to their employed neighbor aggregation
strategy, existing GNNs focus on capturing node-level information
information. Existing GNNs therefore
and neglect high-level
suffer from representational
limitations caused by the Local
Permutation Invariance (LPI) problem. To overcome these limi-
tations and enrich the features captured by GNNs, we propose
a novel GNN framework, referred to as the Two-level GNN
(TL-GNN). This merges subgraph-level information with node-
level information. Moreover, we provide a mathematical analysis
of the LPI problem which demonstrates that subgraph-level
information is beneﬁcial to overcoming the problems associated
with LPI. A subgraph counting method based on the dynamic
programming algorithm is also proposed, and this has time
complexity is O(n3), n is the number of nodes of a graph.
Experiments show that TL-GNN outperforms existing GNNs and
achieves state-of-the-art performance.

Index Terms—Graph representation, Graph neural networks,

Local permutation invariance, Attention mechanism.

I. INTRODUCTION

ing interest

G RAPH Neural Networks (GNNs) have attracted increas-

in graph-structured data such as social
networks, recommender systems, bioinformatics and combi-
natorial optimization. Scarselli et al. [1] ﬁrst introduced the
concept of the GNN by extending recursive neural networks.
Veliˇckovi´c et al. [2] proposed the Graph Attention Network
(GAT), which leverages masked self-attentional
layers to
address the shortcomings of prior methods based on graph
convolutions. Xu et al. [3] present a theoretical framework to
analyze the representational capability of GNNs, and develop
a simple neural architecture referred to as the Graph Isomor-
phism Network (GIN).

Although the existing neighborhood aggregation strategy
used in GNNs is relatively efﬁcient from the viewpoint of
graph isomorphism classiﬁcation, recent studies [4] [5] [6]
show that such a procedure brings some inherent problems.
Namely, most existing GNNs suffer from local permutation in-
variance (LPI), which leads them to confuse speciﬁc structures.
In fact, invariance is very common in many learning tasks.
Data can produce identical embeddings in a reduced low-
dimensional space after symmetric transformations or rotations
are applied [7], [8]. As for graph-structured data, Garg et
al. [4] have found that existing GNNs have representational

Xing Ai and Chengyu Sun are with School of Informatics, Xiamen

University, Xiamen, Fujian, China.
E-mail: 24320191152507@stu.xmu.edu.cn, 30920201153942@stu.xmu.edu.cn

Corresponding author: Zhihong Zhang is with School of Informatics,

Xiamen University, Xiamen, Fujian, China.
E-mail: zhihong@xmu.edu.cn

Edwin R. Hancock is with University of York, York, UK.

E-mail: edwin.hancock@york.ac.uk

limitations caused by the translation of graph-structured data.
To eliminate such an effect, Sato et al. [5] have exploited
a local port ordering of nodes referred to as the Consistent
Port Numbering GNN (CPNGNN). Moreover, Klicpera et al.
[6] have proposed DimeNet, which is a directional message
passing algorithm introduced in the context of molecular
graphs. However, Garg V et al. [4] prove that all existing
GNN variants have representational limits caused by LPI, and
propose a novel graph-theoretic formalism.

In the meantime, studies show that complex networks can be
succinctly described using graph substructures (also referred
to as subgraphs, graphlets, or motifs). Subgraph methods have
been well-studied and widely used in chemistry [9], biology
[10], and social network graph tasks [11]. For example,
speciﬁc patterns of atoms or modes of interaction can be
discovered by identifying speciﬁc subgraph topologies. Bai et
al. [12] propose a general subgraph-based training framework
referred to as Ripple Walk Training (RWT). This can not
only accelerate the training speed on large graphs but also
solve problems associated with the memory bottleneck. Emily
et al. [13] propose SUBGNN to propagate neural messages
between the subgraph components and randomly sampled
anchor patches. These methods extract node features and
subgraph features separately. Moreover, they characterize only
the number of different subgraphs, which ignore the learning
of their representation.

the node-level

For the sake of the above mentioned problems, we propose
a novel model which merges subgraph-level information into
the node-level representation. First, we merge subgraph-level
information at
to enrich the features. And
secondly, we theoretically verify the model to demonstrate
its performance with real-world datasets. The results show
that our approach is signiﬁcantly more effective than state-
of-the-art baselines. Our main contributions are summarized
as follows:

1. We propose a novel GNN approach,

the Two-level
GNN (TL-GNN), which captures both microscopic (small
scale) and macroscopic (large scale) structural informa-
tion simultaneously and thus enriches the representation
of a graph.

2. We provide a mathematical deﬁnition and analysis of
the effects of LPI on GNNs. Furthermore, we prove that
subgraph-level information offers beneﬁts in overcoming
these limitations.

3. We verify our method on seven different benchmarks and
a synthetic dataset. The results show that TL-GNN is
more powerful than existing GNN’s.

4. A subgraph counting method based on dynamic pro-
gramming is also proposed. The time complexity and
space complexity of this algorithm are O(n3) and O(n3)
respectively, where n is the number of nodes in the graph.

 
 
 
 
 
 
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

The remainder of this paper is organized as follows. Sec-
tion. II provides an overview of the related work. Section. III
introduces the proposed method, including theoretically prov-
ing the capacity of our model to solve the LPI problem.
Section. IV describes our experimental setting and demonstrate
empirically the performance of TL-GNN. Finally, Section. V
concludes the paper and offers directions for future work.

II. RELATED WORK

GNNs have achieved state-of-the-art results on graph clas-
siﬁcation, link prediction and semi-supervised node classiﬁ-
cation.However, recent studies [4] [5] [6] have demonstrated
one of the severe representational limitations of GNNs, namely
Local Permutation Invariance (LPI). In this section, we will
brieﬂy review these interrelated topics.

A. Graph Neural Networks.

Graph Neural Networks (GNNs) have proved to be an
effective machine learning tool for non-Euclidean structure
data for several years. Since the GNN was ﬁrst presented in
[1], a set of more advanced approaches have been proposed,
including but not limited to GraphSAGE [14], Graph Attention
Networks (GAT) [2], Graph Isomorphism Network (GIN) [3],
edGNN [15]. These methods learn local structural information
by recursively aggregating neighbor representations.

In a macroscopic view, the above models follow the same
pattern. For each node v ∈ V within a graph G = (V, E),
GNNS capture k-hop neighbor information hk
N (v), and then
learn a representational vector hk
v after k layers of processing.
On this basis, tasks such as graph classiﬁcation can be accom-
plished. In fact, the critical difference between GNN variants is
how they design and implement the neighbor aggregation func-
tion. Xu et al. [3] summarized some of the most common GNN
approaches and proposed a general framework referred to as
the Graph Isomorphism Network (GIN). The GIN approach
deﬁnes the above steps as three related functions, namely
AGGREGATE (AGG), COMBINE (COM), and READOUT
(READ),






N (v) = AGG({h(k−1)
hk

µ

, ∀µ ∈ N (v)}),

v = COM (hk−1
hk

v

, hk

N (v)),

(1)

hG = READ({hk

v|v ∈ G}),

The initialization is h0

v = Xv, and Xv represents the initial
features of the nodes. The quantity N (v) represents the set of
nodes adjacent to v and hG is the graph representation vector.
Xu et al. [3] indicate that what makes GNN so powerful is
the injective aggregation strategy, which maps different nodes
to different representational units. They also demonstrate that
when the above three functions are all injective functions, for
example a sum, then the GNN can be as powerful as the WL
test [16] on the graph isomorphism problem.

B. Local Permutation Invariance

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, E

Even though the precise form of the aggregation or com-
bination strategy varies across different GNN architectures,
most share the same neighborhood aggregation concept. This
characteristic leads to an underlying graph isomorphism prob-
lem, the so-called local permutation invariance (LPI). A more
common LPI example in the real world is edge rewiring
[17] [18]. For a graph G = (V, E), edge rewiring operation
alters the graph structure and leads to a new graph G
=
) by exchanging a pair of edges, for instance removing
(V
edges eAB, eCD between nodes A, B, C, D and adding edges
eAC, eBD. After changing, G and G
are non-isomorphic
graphs but have identical representation in GNNs: hG = hG(cid:48) .
Garg et al. [4] analyze speciﬁc cases of LPI for the GNNs
aggregation function and provide generalization bounds for
message passing in GNNs. As shown in Fig. 1, the graphs are
obviously non-isomorphic, but their node-level characteristics
are identical. Unfortunately, existing GNNs focus on extracting
node-level information and neglect high-level information, so
they suffer representational limitations. Recent studies have
attempted to overcome these limitations by providing addi-
tional information to the nodes. Consistent Port Numbering
GNN (CPNGNN) [5] assigns port numbers to nodes and
treats their neighbors differently. Klicpera et al. [6] propose
DimNet for molecular graphs. This embeds whole atoms
using a collection of edge embeddings and takes advantage
of directional information by modifying messages based on
their angle. However, Garg et al. [4] demonstrate that there
are some graphs that both CPNGNN and DimeNet can not
distinguish. This means that these approaches fail to overcome
the representational limitations caused by LPI.

C. Subgraph Methods.

Subgraphs can be regarded as the basic structural elements
or building blocks of the larger graph, including paths [19]
and subtrees [20]. Subgraph frequency was studied as a
local feature in social networks by Ugander et al. [21], who
discovered that it can provide unique insights for recognizing
both social organization and graph structure in very large
networks. Ugender et al. [21] propose a novel graph homomor-
phism algorithm based on subgraph frequencies. They deﬁne a
coordinate system that is beneﬁcial both to the representation
and understanding of large sets of dense graphs. Grochow
et al. [22] introduce a symmetry-breaking technique into the
motif (subgraph) discovery process and develop a novel motif
discovery algorithm that can achieve an exponential speed-
up. More recently, signiﬁcant effort has been expended in the
design of subgraph-based GNNS for graph classiﬁcation. The
role of substructures has been explored empirically by Ying et
al. [23] and used to interpret the predictions made by GNNs.
In this paper, we propose an architecture which we refer to
as the Two-level Graph Neural Network (TL-GNN), which is
designed to make the association between the neighbor node
and subgraph structural information. Our model mitigates the
negative impacts of the LPI problem and is veriﬁed to be
efﬁcient on real-world datasets.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

Fig. 1. Two LPI examples, G1 and G2 have identical node-level information but different structures. In other words, they are non-isomorphic graphs. The
nodes of the same colour have the same features. The numbers of nodes and edges number together with the node features are identical. The only difference
(cid:48)
2(cid:48) = (b, µ).
between them is that a pair of edges of G1: e1 = (a, b), e2 = (µ, v), change their nodes and transform the edges of G2: so that e

1 = (a, v), e

(cid:48)

Algorithm 1 Tree counting

1: Input: input original graph G(V, E) and its adjacency

matrix A, hyper parameter tree threshold.

2: Output: output subgraph set S
3: Initialize subgraphset S,

tree set T ree , neighbor set
{N (v)|t = 0, 1, 2...2T , ∀v ∈ V }, and path from v to
µ : {Pv(µ)|∀v, µ ∈ V }

Fig. 2. Subgraphs example

III. PROPOSED METHOD

In order to extract subgraph-level information from a graph,
we ﬁrst count all subgraphs within a certain size range of a
graph and then generate a new graph, namely generated graph,
whose nodes (or supernodes) represent subgraphs in the orig-
inal graph. We develop a novel subgraph counting algorithm
for this purpose. After the generated graph is to hand, we
develop a novel GNN framework, that signiﬁcantly extends the
existing GNN framework. We also develop two key operators
for the novel framework that facilitate the effective merging
of subgraph and node information. This Section will introduce
the proposed subgraph counting algorithm and the new GNN
framework.

A. Constructing the Generated Graph

Consider an undirected graph G = (V, E), where V and
E ⊆ (V × V ) respectively denote the set of nodes and the set
of edges. The element (vi, vj) in E is an unordered pair of
nodes vi and vj, i, j = 1, 2, 3...N , where N is the number of
nodes in the graph, i.e. the size of the network.

1) Subgraph Counting Algorithm : We aim to identify three
different types of subgraph structures, namely trees, paths, and
circuits (cycles). These represent the different basic classes
of subgraph structure, and structures such as triangles or
quadrilaterals can be regarded as different speciﬁc cases as
shown in Fig.2. Finding all subgraphs within a graph is an
NP-hard problem. To implement our method, it is unnecessary
to ﬁnd all subgraphs. For each node in turn, we identify
all subgraphs that are contained within its D-hop neighbors.

for µ ∈ V do

4: for v ∈ V do
5:
6:
7:

if Avµ = 1 then

add µ into N (v)
add v into N (µ)
Pv(µ) ← v
Pµ(v) ← µ

8:
9:
10:
11:

end if
end for
if |N (v)| > tree threshold then
add (N (v) + [v]) into T ree

end if
12:
13: end for
14: add T ree into S
15: return S

We design a dynamic programming algorithm to achieve this
goal. Our algorithm consists of three steps for tree, path, and
circuit location. Firstly, we store the neighbor set of each
node v ∈ V and select those tree-shaped subgraphs which
have more than three nodes (tree threshold = 3). A 3-
node tree or a 2-node tree is essentially a 3-node path or a
2-node path. This step is shown in Algorithm. 1. Secondly,
we ﬁnd all path-shaped and circuit-shaped subgraphs based
on the dynamic programming algorithm. This step is realized
using Algorithm. 2 and Algorithm. 3. If the node µ is one
of the 2d-hop neighbors of the node v and the node a is
one of the i-hop neighbors of the node µ (i ≤ 2d), then the
node a is one of the (2d + i)-hop neighbors of the node v.
After Algorithm. 1 locates the 1-hop neighbors of each node,
Algorithm. 2 and Algorithm. 3 can locate the 2-hop neighbors
of each node by two sequential 1-hop searches. After locating
the 2-hop neighbors of each node, the 3-hop neighbors and

𝐺1𝐺2𝐺1𝐺2𝑎𝑏𝑣𝜇𝑎𝑣𝑏𝜇𝑎𝑏𝑣𝜇𝑎𝑏𝑣𝜇(1)(2)e1e2e1e2𝑒1′𝑒2′𝑒1′𝑒2′(a) path(b) tree(c) circuitJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Algorithm 2 Path and circuit counting

Algorithm 3 Path and subgraph sifting

1: Input: input original graph G(V, E), depth D, subgraph
set S and d-hop adjacency matrix set {Ad ∈ R|V |×|V ||d =
2...2D} and path from v to µ : {Pv(µ)|∀v, µ ∈ V }

1: Input: input original graph G(V, E), adjacency matrix A,
subgraph set S, path from v to µ : {Pv(µ)|∀v, µ ∈ V }
and {Ad|d = 2...2D}

2: Output: output subgraph set S
3: for d = 0, 1, 2...D do
for v ∈ V do
4:
5:
6:

for µ ∈ V do

for a ∈ V do

Circuit, P ath=Algorithm3

end for

end for

7:
8:
9:
10:
11: end for
12: add Circuit, P ath to S
13: return S

end for

4-hop neighbors of each node can be found. Besides, we store
all nodes on the path from the node v to the node µ. If the
node µ’s i-hop neighbor a is not on the path from the node
v to the node µ and the node v has a path to the node a, a
circuit can be found. Otherwise, a path can be obtained.

The main advantage of the proposed algorithm is low
complexity. Early subgraph counting algorithms [24] [25]
[20] required exponential time complexity. The current best-
known algorithm [26] for exact subgraph counting, which
requires O(n ωD
3 ), where ω and D are exponent of fast matrix
multiplication and nodes number of subgraphs. Due to ω can
be neglected, the time complexity of this algorithm is O(nD).
However, the proposed method only requires O(n3). As for
the proposed methods, the time complexity of Algorithm. 1
is O(n2), n = |V |. The time complexity of Algorithm. 2 and
Algorithm. 3 is O(2Dn3). Due to the fact that D is a scalar
no larger than 10, the factor of 2D can be neglected. The time
complexity of the proposed method is therefore O(n3).

As for space complexity, space complexities of Pu(v) and
Ad are O(n3) and O(2Dn2) respectively. As a result the
space complexity of the proposed subgraph counting method
is O(n3).

2) Generating Graphs: After the subgraph counting is
complete, we generate a new graph to represent the subgraph
relationships. Given a network G(V, E), the generated graph
G∗(V ∗, E∗) consists of the set of supernodes representing the
detected subgraphs V ∗ and the set of edges E∗ ⊆ (V ∗ × V ∗)
representing the relationships between them. Two subgraphs
are connected if they share common nodes or links in the orig-
inal network. The features associated with the supernodes are
represented by a two-dimensional vector, whose components
are the node counts and the subgraph type respectively. Fig.
3 provides an example.

2: Output: output circuit set Circuit and path set P ath
3: for i = 1, 2, ..., 2d do
4:
5:
6:

if a /∈ Pµ(v) and Pv(a) (cid:54)= ∅ then

add Pv(µ) + Pµ(a) + Pa(v) to Circuit

vµ = 1 and A(i)

µa = 1 then

if A(2d)

7:
8:

9:

10:
11:
12:
13:
14:

end if
if a /∈ Pµv and Pva = ∅ then

av

va

= 1
= 1

A(2d+i)
A(2d+i)
Pa(v) = Pa(µ) + Pµ(v)
Pv(a) = Pv(µ) + Pµ(a)
add Pv(a) to P ath

end if

end if
15:
16: end for
17: return Circuit, P ath

We also record the appearances of each node in V ∗ by
constructing a transformation matrix T ∈ RN ×M , |V | =
N, |V ∗| = M . The transformation matrix T indicates the cor-
respondences between nodes and subgraphs (i.e. supernodes),
i.e. which supernodes subsume each node. The elements of
the transformation matrix are deﬁned as follows:

Tij =

(cid:40)

1,
0,

if node i in subgraph j
else

(2)

B. The LPI problem and subgraphs

Garg et al. [4] indicate the limitations of GNNs caused by
Local Permutation Invariance (LPI). Speciﬁcally, changing a
pair of edges in a graph leads to structure changing, while
node-level information of the graph is maintained. The existing
aggregation strategy, which extracts node-level information
only is unable to distinguish structure change. Garg et al.
[4] provide several example structures which can not be
distinguished by existing methods, as illustrated in Fig. 1.
Speciﬁcally,
have
information which confuses GNNs to
identical node-level
distinguish them. G and G
can be transformed into each other
by exchanging a pair of edges. It means their only difference
is endpoints of the pair of edges.

two non-isomorphic graphs, G and G

(cid:48)

(cid:48)

However, Garg et al. [4] provide examples only, without
giving mathematical deﬁnitions for the ambiguities encoun-
tered.

Without loss of generality, we ﬁrstly indicate the general
characteristics of graphs that existing GNNs can not distin-
guish due to the LPI ambiguity. Secondly, we demonstrate
that for those graphs that GNNs can not distinguish due to
ambiguities, our generated graphs behave in a different and
useful manner. Finally, we demonstrate mathematically the
effectiveness of our method in Section III-C.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

5

Fig. 3. Example of the graph generation process. The red supernode corresponds to a 6-node circuit in the graph. The green and blue supernodes correspond
to a 4-node path and a 4-node tree in the graph respectively.

These results concerning LPI lead us to propose a new
neighborhood aggregation strategy for GNNs. While the ex-
isting neighborhood aggregation strategy is effective, it some-
times fails to distinguish non-isomorphic graphs. To demon-
strate this we give a deﬁnition of Permutation Non-isomorphic
Graphs(PNG):

Deﬁnition 1. Permutation Non-isomorphic Graphs (PNG)
are non-isomorphic graphs that have the same node set and
the same node features but swap the nodes of two edges.

According to the above equations and the mathematical
of the l-th

inductive. the representations of v ∈ V, v
layer meet the condition:

∈ V

(cid:48)

(cid:48)

v = hl
hl

v(cid:48) ,

(7)

For a GNN with K layers, the representations of G and G

(cid:48)

are identical:

(cid:48)

Assume G = {V, E} and G
2 ∈ E
, e1, e2 ∈ E, e

V, v
following conditions:

1, e

∈ V

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, E

= {V
} are PNG. v ∈
(cid:48)
, then they must satisfy the



(cid:48)

(cid:48)

(cid:48)

(cid:48)




|V | = |V
|E| = |E

∀v ∈ V,
(cid:48)
(cid:48)
}, e
, j

|,
|,
∀v ∈ V,
),
1 = {a

v = h0
h0
v(cid:48)
N (v) = N (v
(cid:48)
e1 = {a, b}, e2 = {i, j}, e

(a)
(b)
(c)
(d)
(e)
(3)
the only difference
between G = {V, E} and G
} is a single pair
of edges. The remaining characteristics of G = {V, E} and
(cid:48)
G

= {V
Due to LPI, the GNNs based neighbor aggregation strategy
can not distinguish permutation non-isomorphic graphs, which
we state in Theorem 2:

The above conditions indicate that
(cid:48)

} are identical.

2 = {i

= {V

(cid:48)
, b

, E

, E

},

(cid:48)

(cid:48)

(cid:48)

(cid:48)

(cid:48)

Theorem 2. GNNs based on Eq(1) can not distinguish

permutation non-isomorphic graphs.

We give a proof of this theorem as follows. For two permu-
tation non-isomorphic graphs G = (V, E) and G
),
their neighbourhood aggregation from the 0-th GNN layer are:

= (V

, E

(cid:48)

(cid:48)

(cid:48)

N (v) = AGG({h(0)
h0

µ |µ ∈ N (v)}), ∀v ∈ V,

(4)

N (v(cid:48) ) = AGG({h(0)
h0

µ |µ ∈ N (v

(cid:48)

)}), ∀v

(cid:48)

∈ V

(cid:48)

.

(5)

Due to the conditions (d) given in Deﬁnition 1:

N (v) = h0
h0

N (v(cid:48) ),

v = h1
h1
v(cid:48) ,

(6)

hG = READ({hk

v|v ∈ G}), hG(cid:48) = READ({hk

v(cid:48) |v

hG = hG(cid:48) .

(cid:48)

(cid:48)
∈ G

}).
(8)

(9)

Obviously, existing GNN provides identical representation
for a pair of PNG: hG = hG(cid:48) . The reason that GNNs
are confused by PNGs is due to their adopted neighbor
aggregation strategy, which captures node-level information
only. Unfortunately, PNGs share identical characteristics at
the node-level but have different global structures. As a result,
GNNs can not distinguish PNGs effectively.

Garg et al. [4] have indicated that the LPI problem limits the
representational power of GNNs. Theorem 2 further indicates
how the LPI problem affects the graph classiﬁcation perfor-
mance of GNNs from the perspective of graph isomorphism.
Another interesting question is how the LPI problem inﬂu-
ences the node classiﬁcation task. Unlike the graph structure
which is implicated in the LPI problem, the node classiﬁcation
task is unaffected by global structural information from the
whole graph. Therefore, to what extent and precisely how
the LPI problem inﬂuences node classiﬁcation needs deeper
mathematical analysis and associated proofs, which are beyond
the scope of this paper we will we hope to investigate in more
detail in further work. In this paper, we simply focus on the
LPI problem for the graph classiﬁcation task.

However, although PNGs have identical node-level charac-
teristics, their subgraph-level characteristics are different. The
following lemmas demonstrate that the structural differences
between PNGs can be learned from their generated graphs.
In other words, the subgraph-level information is helpful in
distinguishing PNGs.

graph1234567910118123456789101134567829101112911generated graphJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

(cid:48)

(cid:48)

(cid:48)

Lemma 3. For two permutation non-isomorphic graphs
), their generated graphs Gg =

(cid:48)

(cid:48)

(cid:48)

G = (V, E) and G
(V g, Eg) and Gg

(cid:48)

= (V
= (V g

, E
(cid:48)
, Eg

(cid:48)

) are structurally distinct.

We provide a proof of Lemma 3. by analyzing the relation-

ship between edges and subgraphs. For the edges:

ei ∈ E,

e

(cid:48)

i ∈ E

(cid:48)

,

(i = 1, 2),

(10)

e1 = (a, b),

e2 = (i, j),

(cid:48)
1 = (a, i),
e

(cid:48)
2 = (b, j),
e

(11)

there are four types of relations between them.
Relation 1:
As shown in Fig. 4 (1), for the subgraphs Si and their nodes

Vi and edges Ei:

i = 1, 2

j = 1, 2, 3, 4.

(23)

Gg = Gg

(cid:48)

if and only if:





h0
S1
h0
S2

= h0
S(cid:48)
1
= h0
S(cid:48)
2

= h0
S3
= h0
S4

= h0
S(cid:48)
3
= h0
S(cid:48)
4

∃e3 = (S1, S4), ∃e4 = (S2, S3),
∃e5 = (S1, S2), ∃e6 = (S3, S4)

(24)

(cid:48)

(cid:48)

when Gg = Gg

Obviously, G = G

. Because swapping
the nodes constituting edges does not change the connection
structure of the subgraphs and their nodes, the structures are
identical.

Else, Gg (cid:54)= Gg

, because of the super-node features or

Si = (Vi, Ei), S

i = (V

i , E

i),

(cid:48)

(cid:48)

(cid:48)

Relation 1 can be written as:

(i = 1, 2).

(12)

connections of super-nodes.

Relation 4:
As shown in Fig. 4 (4), for the subgraphs:

ei ∈ Ei, e

i /∈ E

(cid:48)

(cid:48)

i, i = 1, 2.

(13)

Due to the features of supernodes (subgraphs) include node
counts and the subgraph type respectively (Section. III-A2),
the edges change must lead to node counts or subgraph change
of subgraphs. We thus have:

Ei (cid:54)= E

(cid:48)

i, h0
S1

(cid:54)= h0
S(cid:48)
1

,

(cid:48)

(cid:48)

and so, V g (cid:54)= V g
Relation 2:
As shown in Fig. 4 (2), consider the subgraphs:

, Gg and Gg

are different.

Si = (Vi, Ei), S

i = (V

i , E

i), (i = 1, 2, 3).

(cid:48)

(cid:48)

(cid:48)

Relation 2 can be written as:

e1 ∈ E1, e

i /∈ E

j, e2 /∈ Ej,

(cid:48)

(cid:48)

i = 1, 2

j = 1, 2, 3.

Due to the relation:

e1 ∈ E1, e

1 /∈ E

1.

(cid:48)

(cid:48)

We have:

Vi = V

(cid:48)

i , (i = 1, 2, 3),

(cid:48)

(cid:48)

E1 (cid:54)= E

1, Ej = E

j, (j = 2, 3).

(cid:48)

1, h0
S1

So, S1 (cid:54)= S
Relation 3:
As shown in Fig. 4 (3), for the subgraphs:

. Gg and Gg

(cid:54)= h0
S(cid:48)
1

(cid:48)

are different.

Si = (Vi, Ei), S

i = (V

i , E

i), (i = 1, 2, 3, 4).

(cid:48)

(cid:48)

(cid:48)

Relation 3 can be written as:

ei /∈ Ej, e

i /∈ E

j, Ej = E

j.

(cid:48)

(cid:48)

(cid:48)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

(21)

(22)

S1 = (V1, E1), S

1 = (V

1 , E

1),

(cid:48)

(cid:48)

e1 ∈ S1, e

1 ∈ S

1.

(cid:48)

(cid:48)

(25)

Obviously, S1, S

1 /∈ T ree. If S1 ∈ P ath, S

1 ∈ Cir

(cid:48)

(cid:48)

or

S1 ∈ Cir, S

1 ∈ P ath

(cid:48)

(cid:48)

:

h0
S1

(cid:54)= h0
S(cid:48)
1

.

(26)

There is a special case of Lemma 3 which deserves com-
ment. If S1 is a Path or a Cir, changing a pair of edges would
divide S1 into two subgraphs or lead to a subgraph identical
to S1 but with a different node sequence. We will discuss this
situation in Lemma 4.

Lemma 4. GNNs can distinguish the structural variance

caused by the node sequence.

In summary, because GNN adopts a hierarchical aggrega-
tion strategy, different node sequences will lead to different
information aggregated by GNN in different layers.

(cid:48)

(cid:48)

(cid:48)

As shown in Fig. 5. If u, v are i-hop and j-hop neighbors
are respectively j-hop and
, v
a(cid:48) and

of node a respectively, then u
i-hop neighbors of node a
a (cid:54)= hj
hj
Lemma 5. As for Lemma 3,

. Then as a result hi
a(cid:48) . Therefore, GNNs can distinguish them.

if Gg = (V g, Eg) and
Gg
) are different in terms of their supernode
features or subgraph connections, then a GNN can distinguish
the generated graphs.

a (cid:54)= hi

= (V g

, Eg

(cid:48)

(cid:48)

(cid:48)

Lemma 3 and Lemma4 lead to Lemma 5. Due to the

properties of GNNs, Lemma 5 can be demonstrated easily.

Suppose S

(cid:48)

is the permuted subgraph of S ∈ Vg. Under
and let the equation

∈ V g

(cid:48)

(cid:48)

relations 1,2 and 4, ∃S ∈ V g, S
below be tenable:

S (cid:54)= h0
h0

S(cid:48) .

(27)

Similar to the proof of Theorem. 2, we can demonstrate

hGg (cid:54)= hG(cid:48)

g via the mathematical inductive.

For relation 3, we have:

N (S) (cid:54)= N (S

(cid:48)

),

(28)

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

Fig. 4. Four relations between edges and subgraphs. Si is the subgraph with the index i. Blue and green points are nodes. The same color nodes share the
same feature.

Fig. 5. A special case. Changing a pair of edges, e1 and e2, leads to different
(cid:48)
node sequences. Subgraphs S and S
have identical structure but different
node sequences.

which means:

hN (S) (cid:54)= hN (S(cid:48) ),

(29)

After combining operations, we have:

S = COM (h0
h1

S, hN (S)),

S(cid:48) = COM (h0
h1

S(cid:48) , hN (S(cid:48) )).

(30)
Due to COM is an injective function, different inputs will
be mapped into different points in the feature space. It means
that graph representations after iterations are different:

we refer to as the Two-level Graph Neural Network(TL-GNN).
In this section, we present details of the TL-GNN framework.
Due to the neighbor aggregation strategy, existing GNNs
focus on capturing node-level information and ignore higher
level structural arrangement
information. Unlike existing
GNNs, the TL-GNN can capture both node-level information
and subgraph-level arrangement information simultaneously.
This is achieved by merging subgraph-level information into
supernode-level information.

Speciﬁcally,

the TL-GNN accepts both a graph and its
generated graph (as described in Section III-A) as inputs.
In each layer, the two separate GNN components capture
information concerning the graph (node-level) and generated
graph (subgraph-level) respectively, as illustrated in Fig.6.
The node-level propagation process can be described as:






N (v) = AGG({h(k−1)
hk

µ

, ∀µ ∈ N (v)}),

(32)

v = COM (hk−1
(cid:102)hk

v

, hk

N (v)),

On the other hand, the subgraph-level propagation can be

S (cid:54)= h1
h1

S(cid:48) ,

hGg (cid:54)= hG(cid:48)

g

.

described as:

(31)

Eq. (31) shows that PNGs can be distinguished by the
differences between subgraphs, which proves the Lemma 5.
According to the above theoretical analysis, we propose the
framework of TL-GNN and discuss it in the next subsection.




N (s) = AGG({h(k−1)
hk

γ

, ∀γ ∈ N (s)}),

(33)



s = COM (hk−1
hk

s

, hk

N (s)),

C. The TL-GNN Framework

As noted above the LPI of GNNs lead to serious represen-
tational limitations. Existing GNNs are therefore sometimes
compromised in their performance by PNGs. Fortunately and
as we have shown, the subgraph-level information can be bene-
ﬁcial in distinguishing the PNGs. To overcome the limitations
caused by the LPI problem and enrich the representational
capacity of GNNs, we propose a novel GNN approach which

Here N (v) and N (s) are the neighbor sets of node v and
supernode s respectively. These two information propagation
processes are discrete and do not share parameters.

1) The AGG SUB and MERG functions: With a two-
level representation to hand, we merge the subgraph-level
representation into the node-level representation. We design
two functions AGG SUB and MERG for this process.

For a graph containing N nodes, assume that its generated
graph contains M supernodes. The outputs of the k−th layer

𝒆𝟏𝒆𝟐𝒆𝟏′𝒆𝟐′(1)(2)𝑮𝒈′𝑮𝒈𝒆𝟏𝒆𝟐𝑮𝒈S𝟏S𝟐S𝟏S𝟐S𝟑𝒆𝟏′𝒆𝟐′𝑮𝒈′𝑺𝟏′𝑺𝟐′𝑺𝟏′𝑺𝟐′𝑺𝟑′𝒆𝟏𝒆𝟐(3)𝑮𝒈′𝑮𝒈S𝟏S𝟐S𝟑𝑺𝟏′𝑺𝟐′𝑺𝟑′S𝟒𝑺𝟒′(4)S𝟏𝒆𝟏𝒆𝟐𝒆𝟏′𝒆𝟐′𝑺𝟏′𝒆𝟏′𝒆𝟐′𝑮𝒈′𝑮𝒈ab𝜇𝐒a′𝑣′𝑏′𝜇′𝑣……𝑒1′𝑒2′𝑒1𝑒2𝐒′JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8

Fig. 6. Visual illustration of the TL-GNN framework. The model takes both the original graph and generated graph as input, then aggregates them with
the same strategy. Each TL-GNN layer has two separate GNN blocks for graph and generated graph to capture node-level and subgraph-level information
respectively. At the end of each layer, the subgraph-level representation hs would be converted into h(cid:48)
s via the transformation matrix T , which could be aligned
with node-level representation hn. On this basis, two independent representations can be merged together with the attention mechanism. The summation h(cid:48)
n
would be the input of the next layer.

of TL-GNN are the node-level representation (cid:101)hk
subgraph-level representations hk

S respectively:

N and the

N ∈ RN ×d,
(cid:101)hk

S ∈ RM ×d,
hk

(34)

Here (cid:101)hk

N and hk

S are matrices that contain the node and
supernode (subgraph-level) representations respectively. Each
row of hk
N or hk
S corresponds to the representation of an
individual node or supernode.

We merge (or concatenate) the matrices hk

N and hk
N using the MERG function. Then (cid:102)hk

S into a
N and hk
single matrix (cid:102)hk
S
become the inputs into the next layer of the GNN. Each layer
has an identical matrix structure. After applying the merging
step, the representations of the nodes are merged with the
available subgraph-level information.

Next, we deﬁne two more functions for the TL-GNN,

namely AGG SUB and MERG:

Finally, the representation of the graph G is obtained from

the representations of its nodes.

hG = READ({(cid:102)hk

v|v ∈ G}),

(36)

In order to distinguish PNGs, the AGG SUB and MERG

need to fulﬁll conditions speciﬁed in Lemma 6:

Lemma 6. If AGG SU B and M ERG are both injective
multiset functions, then the representation of G is distinct from
that of G

.

(cid:48)

To demonstrate Lemma 6, We deﬁne C(v), which is a set

of subgraphs that contain node v. According to Lemma 5:

∃S ∈ C(v), S

(cid:48)

∈ C(v(cid:48)),

S (cid:54)= h1
h1

S(cid:48) ,

As

v = h0
h0
v(cid:48),

c(v) (cid:54)= h0
h0

c(v(cid:48) ),

(37)

(38)

H k

s = AGG SU B(hk

S),

hk
N = M ERG( (cid:102)hk

N , H k

S),

v = M ERG(h0
h1

v, h0

c(v)),

(35)

v(cid:48) = M ERG(h0
h1

v(cid:48) , h0

c(v(cid:48) )),

(39)

The AGG SUB operator aggregates all of the subgraph
representations which contain a given node v. The output
of the MERG function is the matrix hk
N , which merges
the node-level and subgraph-level information into a single
representation.

v (cid:54)= h1
h1
v(cid:48),
The above equations are satisﬁed if and only if M ERG is
an injective function. When the above equations are satisﬁed,
we can obtain hG (cid:54)= hG(cid:48), which means G and G(cid:48) can be
distinguished. Lemma 6 is proved.

(40)

Layer 1+GNNlayerGNNlayerLayer 2GNNlayernode-level representationSubgraph-level representationattentionGraph representationGNNlayerOriginal graphGenerated graph𝒉𝒏∈𝑹𝑵×𝒅𝒉𝒔∈𝑹𝑴×𝒅𝒉𝒏(cid:4593)∈𝑹𝑵×𝒅𝑻∈𝑹𝑵×𝑴𝒉𝒔(cid:4593)=𝑻(cid:521)𝒉𝒔Layer KGNNlayerGNNlayer……++……𝒉𝒔(cid:4593)=𝑻(cid:521)𝒉𝒔𝒉𝒔(cid:4593)=𝑻(cid:521)𝒉𝒔Transformation matrixJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

9

There are several choices available for the AGG SUB and
MERG functions, which include summation and concatena-
tion. We chose summation for AGG SUB of TL-GNN.

Due to the fact that the graph size and generated graph
size are different, the subgraph-level representations need to be
transformed into node-level representations of identical size.
The translation matrix mentioned in Section III-A translates

the matrix hk

S into the matrix H k

S ∈ RN ×d:

H k

S = T · hk
S,

(41)

Due to the deﬁnition of T , the i-th row of H k

S is the
summation of representations of those supernodes that contain
node i. Assume i-th line of H k

S is (H k

s )i:

(H k

s )i =

(cid:88)

hk
s ,

s∈C(i)

(42)

where C(i) is the set of supernodes (subgraphs) that contain

node i.

As for the MERG operation, we use an attention mechanism
to deﬁne the MERG function. The attention mechanism for
TL-GNN is essentially a weighted summation. Therefore,the
MERG operation can be rewritten as:

v = αk · (cid:102)hk
hk

v + βk · H k
s ,

(43)

αk =

exp(ˆαk)
exp(ˆαk) + exp( ˆβk)

,

βk =

exp( ˆβk)
exp(ˆαk) + exp( ˆβk)

,

(44)
where ˆαk and ˆβk are randomly initialized scales. α and
β satisfy the condition αk + βk = 1. The parameters αk
and βk can be learned during training. The parameter αk is
large if the node-level representation is more important for the
classiﬁcation of graphs, and vice versa.

Finally,

the effectiveness of TL-GNN in distinguishing

PNGs can be stated mathematically.

Theorem 7. TL-GNN has the ability to distinguish the PNG.
We provide the proof of Theorem 7 using the above Lem-

mas.

For permutation non-isomorphic graphs G = (V, E) and
), According to Lemma 6, we have:
= (V

, E

(cid:48)

(cid:48)

(cid:48)
G

v (cid:54)= h1
h1

v(cid:48) v ∈ V v

(cid:48)

∈ V

(cid:48)

.

(45)

According to mathematical induction, the representations of

v ∈ V, v

∈ V

of l-th layer meet condition:

(cid:48)

(cid:48)

v = hl
hl

v(cid:48) .

(46)

For a GNN with K layers, the representations of G and G

(cid:48)

are different:

hG = READ({hk

v|v ∈ G}),

hG(cid:48) = READ({hk

v(cid:48) |v

hG (cid:54)= hG(cid:48) .

(cid:48)

(cid:48)

∈ G

}),

(47)

(48)

So, TL-GNN can distinguish permutation non-isomorphic

graphs G and G

.

(cid:48)

Theorem 7 indicates that the TL-GNN can distinguish those
ambiguous graphs that confuse existing GNNs. In other words,
TL-GNN is more powerful than GNNs.

IV. EXPERIMENTS

In this section, we perform experimental evaluations of our
TL-GNN method on the graph classiﬁcation task. We compare
the TL-GNN to several state-of-the-art deep learning and graph
kernel methods, and conduct experiments on seven standard
graph classiﬁcation benchmarks together with synthetic data.

A. Datasets

Datasets of this paper include MUTAG [27], PTC [28],
NCI1 [29], PROTEINS [30], COX2 [31], IMDB M [32] and
IMDB B [33]. The IMDB M and IMDB B datasets have no
node features. The remaining datasets have categorical node
features. In order to verify the ability to distinguish PNGs,
we have prepared a synthetic PNG dataset named SPNG. The
details of these datasets are shown in Appendix.

B. Baselines for Comparison

The baselines used for comparison include state-of-the-art

methods which are applied to the graph classiﬁcation task:

(1) The kernel based methods: Weisfeiler-Lehman(WL) [34]
and subgraph Matching Kernel (CSM) [35], Deep Graph
Kernel (DGK) [36].

(2) The state-of-the-art GNNs: Graph convolution network
(GCN) [37], Deep Graph CNN (DGCNN) [38], Graph Iso-
morphism Network (GIN) [3], Random Walk Graph Neural
Network (RW-GNN) [39], Graph Attention Network (GAT)
[2], Motif based Attentional Graph Convolutional Neural
Network (MA-GCNN) [40].

C. Experimental Setup

For our experimental comparison, we set the GNN layers
of GIN so as to have the same structure but with no parameter
sharing. These layers aggregate and combine the original
graph and its generated graph. There are independent attention
parameter pairs for merging operations between the GNN
layers. Each GNN layer has several MLP layers. More details
about experimental setup are shown in Appendix.

D. Results and Discussion

Comparison with existing GNNs on real-world datasets:
The results in Table I indicate that TL-GNN achieves the
best results on 6 out of 7 benchmarks, often with a clear
improvement over alternative GNN methods studied. The
performances for the classical GNNs are quoted from their
indicated reference. We perform 10-fold cross-validation to
compute the GIN, GCN and RW-GNN accuracies on COX2.
The parameters for the deep learning methods are as suggested
by their authors. For fairness, all the methods run on the
same computing device. For cases where accuracy cannot be
obtained, we use the ”-” tag in Table I.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10

TABLE I
CLASSIFICATION ACCURACY (IN % ± STANDARD ERROR)

Datasets
WL
CSM
DGK
GCN
DGCNN
GIN
FDGNN
RW-GNN
GAT
HA-GCNN
TL-GNN sm
TL-GNN ms
TL-GNN mm
TL-GNN(w/o S)
TL-GNN

PROTEINS
IMDB-M
MUTAG
75.0 ± 3.1
50.9 ± 3.8
90.4 ± 5.7
63.3
85.4
-
71.7 ± 0.6
43.9 ± 0.4
87.4 ± 2.7
76.0 ± 3.2
51.9 ± 3.8
85.6 ± 5.8
70.9 ± 2.8
47.8 ± 0.9
85.8 ± 1.7
76.2 ± 2.8
52.3 ± 2.8
89.4 ± 5.6
76.8 ± 2.9
50.0 ± 1.3
88.5 ± 3.8
74.7 ± 3.3
47.8 ± 3.8
89.2 ± 4.3
74.7 ± 2.2
47.8 ± 3.1
89.4 ± 6.1
79.4 ± 1.7
53.8 ± 3.1
93.9 ± 5.2
77.3 ± 2.6
54.3 ± 4.7
90.9 ± 6.4
78.9 ± 2.3
53.4 ± 4.7
91.2 ± 3.9
77.5 ± 2.6
52.2 ± 3.8
90.8 ± 5.4
79.4 ± 3.0
54.4 ± 3.0
92.4 ± 6.3
95.7 ± 3.4 74.4 ± 4.8 83.0 ± 2.1 55.1 ± 3.2 79.7 ± 1.9 88.6 ± 2.7 79.9 ± 4.4

PTC
59.9 ± 4.3
63.8
60.1 ± 2.6
64.2 ± 4.3
58.6 ± 2.5
64.6 ± 7.0
63.4 ± 5.4
61.6 ± 9.5
66.7 ± 5.1
71.8 ± 6.3
68.1 ± 5.0
67.0 ± 7.9
66.3 ± 7.6
70.0 ± 7.9

IMDB-B
73.8 ± 3.9
58.1
65.9 ± 1.0
74.0 ± 3.4
70.0 ± 0.9
75.1 ± 5.1
72.4 ± 3.6
70.8 ± 4.8
70.5 ± 2.3
77.2 ± 3.0
77.5 ± 2.0
77.5 ± 3.5
76.6 ± 3.3
77.8 ± 2.1

COX2
83.2 ± 0.2
80.7 ± 0.3
-
-
-
83.3 ± 5.3
83.4 ± 2.9
81.6 ± 4.7
-
-
86.7 ± 3.5
86.2 ± 4.6
85.6 ± 2.3
87.8 ± 2.7

NCI1
86 ± 1.8
65.5
80.3 ± 0.5
80.2 ± 2.0
74.4 ± 0.5
82.7 ± 1.7
77.8 ± 2.6
−
75.2 ± 3.3
81.8 ± 2.4
81.9 ± 3.3
82.1 ± 4.2
81.0 ± 3.6
82.2 ± 4.9

We found that TL-GNN always achieves the best results
on datasets containing sparse graphs. For PTC and COX2,
TL-GNN achieves 2.6% and 5.2% margins of improvements
over the second-best method. The accuracies of TL-GNN on
MUTAG and IMDB-M are 95.7% and 55.1% respectively.
This represents a slight but consistent improvement over the
alternative methods studied. TL-GNN also achieves the best
performance on PROTEINS. Although TL-GNN gives only a
slight improvement compared to the second-best method. The
average degree of PROTEINS is more than 3. which is dense
compared with the remaining datasets. For this kind of dense
graph, TL-GNN can capture a large number of subgraphs and
enrich the learned information. Even in the cases where TL-
GNN does not achieve the best performance, its accuracy is
close to that of the best performing method. For NCI1, TL-
GNN achieves 0.3% more accuracy than the second best deep
learning method.

GIN and TL-GNN have identical GNN layers. However,
TL-GNN achieves better performances on most of the datasets.
For example, TL-GNN achieves 6.3% and 9.8% improvements
on MUTAG and PTC compared to GIN. Moreover, the stan-
dard errors for TL-GNN on MUTAG and PTC are lower than
those for GIN. NCI1 is the only dataset on which TL-GNN
can not surpass the performance of WL. It is worth noting that
all deep learning methods are also do not outperform WL on
NCI1.

It

is worth noting that both GAT and TL-GNN apply
the attention mechanism. The difference is that GAT assigns
attention weights to the neighbors of nodes. This means that
GAT pays different attention to node-level information. Unlike
GAT, TL-GNN assigns attention weights to different levels of
information (both node-level and subgraph-level). The results
show that TL-GNN outperforms GAT on all datasets, and it
also achieves signiﬁcant improvements on several datasets.
For example, on NCI and IMDB, TL-GNN achieves 6 − 9%

improvement compared to GAT.

Performance comparison on SPNG
An interesting observation that can be made from Table. II
is that the TL-GNN easily achieves 100% training accuracy
while the alternative methods studied do not. None of the
methods can perfectly ﬁt the training data with the exception
of TL-GNN. Due to the fact that both GIN and GCN with
K layers can capture K−hop information, they do offer some
robustness to the LPI problem and can achieve 95.6% and
91.1% training accuracy respectively. However, the remaining
GNN training accuracies are no more than 80%. The results on
SPNG show that the TL-GNN can distinguish PNGs perfectly.
As shown in Fig. 10, the TL-GNN training accuracy after 50
epochs reaches 90%, which represents the fastest convergence
rate. Finally, the TL-GNN converged at a stable 100% training
accuracy after 220 iterations. The training accuracies of RW-
GNN, DGCNN, DAGCN no longer increase after 100 epochs.
The reason for this is that these methods focus on capturing the
local neighborhood information, which leads to less robustness
to the LPI problem.

As for GIN, its convergence rate is slower than the TL-
GNN. According to the curve shown in Fig. 10, as the training
proceeds, GIN saturated at 95.6%. We believe that the TL-
GNN is beneﬁted signiﬁcantly from the subgraph merging
strategy described in Section III-C because both GIN and
TL-GNN have the same GNN layers, but TL-GNN’s overall
performance is better.

Variants of supernode-based subgraph representation
To demonstrate the effectiveness of

supernode-based
subgraph-level representation used by the proposed methods,
we provide a similar TL-GNN architecture, TL-GNN(w/o S)
that accepts separated subgraphs as the input sequentially and
regards each subgraph as a separated graph to capture its fea-
ture, instead of a generated graph containing supernodes. The
results of this experiment are shown in Table. I. It is obvious
that TL-GNN achieves better performance than TL-GNN (w/o

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

11

(a) MUTAG.

(b) PTC.

(c) COX2.

(d) SPNG.

(e) NCI1.

(f) PROTEINS.

(g) IMDB-M.

(h) IMDB-B.

Fig. 7. Variation of the two-level attention parameters on real-world datasets. The number of histograms represents the weight coefﬁcient.

TABLE II
TRAINING ACCURACY ON SYNTEHTIC DATASET(IN % ±
STANDARD ERROR)

Model
DGCNN
DAGCN
GIN
GCN
RW-GCN
TL-GNN

Training Accuracy
64.0
72.0
95.6
91.1
70.37
100

S) because TL-GNN(w/o S) neglects connections between
subgraphs. The reason for this observation is supernode-
based subgraph-level representation provides connections be-
tween subgraphs, which is beneﬁcial to capture subgraph-
level information. However, TL-GNN (w/o S) achieves better
performances than other baselines on most datasets, because
of the subgraph-level information captured by TL-GNN (w/o
S).

Variants of AGG SUB and MERG
In order to verify the theory described in Section. III-C,
we select both non-injective function and injective functions
for the AGG SUB and MERG operators and then compare
their performance. We chose the sum function as the injective
function. For the non-injective function, we chose the max
function. Although there is a large potential choice for the
non-injective function, the max is the most convenient for
use with our method. The speciﬁc TL-GNN variants used are
summarised in Table. III. The performances of the four TL-
GNN variants are shown in Table. I. As shown in Table. I,
TL-GNN achieves a better performance than the remaining
variants of TL-GNN on most of the datasets studied. For
TL-GNN mm, on all the datasets except PROTEINS, TL-
GNN mm is the weakest performing TL-GNN variant. This
observation demonstrates the correctness of Lemma 6. When

AGG SUB and MERG are not injective functions, the TL-
GNN can not effectively capture and retain subgraph-level
information. For TL-GNN ms and TL-GNN sm, the perfor-
mance is better than TL-GNN mm but poorer than TL-GNN.
We believe this is because a non-injective function leads to
information loss.

TABLE III
DETAILED CONFIGURATION OF TL-GNN VARIANCES

TL-GNN variances AGG SUB MERG

TL-GNN
TL-GNN sm
TL-GNN ms
TL-GNN mm

sum
sum
max
max

sum
max
sum
max

Attention weight visualization and analysis
We provide visualization results of each layer on each
dataset in Fig. 7. The blue and orange represent attention
weights of node-level and subgraph-level respectively. Num-
bers of the vertical axis are layers. We applied 3 layers
on MUTAG, PTC, COX2, SPNG and 5 layers on NCI1,
PROTEINS, IMDB-B, IMDB-M, PROTEINS. An observation
is that TL-GNN pays more attention to node-level on all
datasets. However, in most cases, the attention weights of
subgraph-level increased with the increasing of the layer. A
possible reason is that node features become identical with
the layer increasing, namely the oversmoothing problem. So,
the TL-GNN pays more attention to subgraph-level in high
layers. The ratios of node-level attention weight to subgraph-
level attention weight are around 7:3. This observation shows
that node-level information is more important to the graph
information is
classiﬁcation task and little subgraph-level
beneﬁcial to graph classiﬁcation.

Comparison of computational complexity
We report

the average running times per iteration after
training or test of TL-GNN and the baselines Fig. 8. For a
fair comparison, all of the methods are run on a system with

0.76570.73430.60590.23430.26570.3941123MUTAGNode-levelSubgraph-level0.61710.78370.77320.38290.21630.2268123PTCNode-levelSubgraph-level0.87160.82950.80480.12840.17050.1952123COX2Node-levelSubgraph-level0.79010.79320.83750.20990.20680.1625123SPNGNode-levelSubgraph-level0.78570.67160.75510.7190.59160.21430.32840.24490.2810.408412345NCI1Node-levelSubgraph-level0.69120.74410.73610.67650.6840.30880.25590.26390.32350.31612345PROTEINSNode-levelSubgraph-level0.84480.69840.79140.74920.78730.15520.30160.20860.25080.212712345IMDB-MNode-levelSubgraph-level0.85260.65650.76080.67030.47680.14740.34350.23920.32970.523212345IMDB-BNode-levelSubgraph-levelJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

12

Fig. 8. Running time comparison.

(a) Training time.

(b) Test time.

(a) MUTAG.

(b) PTC.

(c) NCI1.

(d) COX2.

(e) IMDB-M.

(f) IMDB-B.

(g) PROTEINS.

Fig. 9. Variation of the parameter D on real-world datasets. The horizontal axis shows number of D.

an Intel Xeon CPU E3-1270 v5 processor. Due to RW-GNN
cannot accept NCI1 as input [39], the running time of RW-
GNN on NCI1 dataset is vacant. Obviously, TL-GNN requires
more time than GIN because of the extra computation of
subgraph-level and attention mechanisms. Although GIN and
TL-GNN have identical GNN layers, TL-GNN costs more
time on both training and test than GIN. The main reason is
that TL requires additional subgraph convolution operations.
However, the training and test time of TL-GNN is less than
several baselines such as DGCNN and RWGNN, because of
the simple but effective architecture of TL-GNN.

Variation of the depth
In order to ﬁnd the optimal value of D, we have carried out
experiments on all of the real-world datasets, and compute
time and accuracies for both training and test are presented.
From Fig.9 the compute time consumed in training and testing
increases with an increasing value of D on most of the datasets
studied. A large value of D leads to a large number of
subgraphs and thus increases the compute time required.

When D reaches a critical value, the compute time con-
sumed in both training and testing ceases to increase. This

then 2D is greater
is because when D is large enough,
than the number of nodes in the graph. This means that all
subgraphs within a graph have been located. For example, on
the MUTAG dataset, the training becomes stable when D is
greater than 5, and this is because the mean node degree of
MUTAG is less than 18. For most of the datasets studied,
the maximum test accuracy is reached when D is 3 or 4.
This indicates that aggregating the subgraph information for
the 8-hop or 16-hop neighbors of a node adds the greatest
beneﬁts to the graph classiﬁcation task. It is clear that too
small a D value leads to an insufﬁcient number of subgraphs,
thus TL-GNN can not capture the subgraph-level information
well. Conversely, too large a value of D leads to subgraphs
that are distant from the node in question being included. The
correlation between these subgraphs and the node in question
is low, and this is not conducive to improved performance.

Comparison of training performance

We compare the effect of training for several GNNs for
which the authors have made their code available. This study
is summarised in Fig. 10. From the table, it is clear that
TL-GNN can better ﬁt all of the datasets studied than the

0.0419 0.0541 0.0147 0.0162 0.0512 0.0161 0.0757 0.0261 0.0302 0.0414 0.0507 0.0277 0.0508 0.0588 0.0464 0.0270 0.0140 0.0206 0.0152 0.0180 0.0205 0.0159 0.0169 0.0196 0.0172 0.0165 0.0172 0.0210 0.0304 0.0756 0.0452 0.0269 0.0160 0.0241 0.0201 0.0402 0.0782 0.0100 0.0181 0.0213 0.0448 0.0169 0.0196 0.0359 0.0557 0.0244 0.0246 0.000.020.040.060.080.10MUTAGPTCCOX2NCI1PROTEINSIMDB-BIMDB-MSPNGSECONDSDGCNNDAGCNGINGCNRW-GNNTL-GNN0.0108 0.0116 0.0120 0.0129 0.0106 0.0067 0.0148 0.0065 0.0065 0.0147 0.0253 0.0165 0.0102 0.0056 0.0142 0.0051 0.0022 0.0040 0.0110 0.0210 0.0134 0.0024 0.0031 0.0036 0.0040 0.0016 0.0073 0.0120 0.0103 0.0040 0.0050 0.0040 0.0020 0.0020 0.0221 0.0134 0.0040 0.0060 0.0020 0.0018 0.0032 0.0106 0.0214 0.0128 0.0052 0.0097 0.0054 0.000.020.040.06MUTAGPTCCOX2NCI1PROTEINSIMDB-BIMDB-MSPNGSECONDSDGCNNDAGCNGINGCNRW-GNNTL-GNN00.20.40.60.811.21.400.20.40.60.8112345678MUTAGtrain timetest timetrain acctest acc00.010.020.030.040.050.060.070.080.090.100.20.40.60.8112345678PTCtrain timetest timetrain acctest acc00.020.040.060.080.10.120.140.160.180.200.20.40.60.8112345678NCI1train timetest timetrain acctest acc00.0050.010.0150.020.0250.0300.20.40.60.8112345678COX2train timetest timetrain acctest acc00.0050.010.0150.020.0250.030.0350.0400.20.40.60.8112345678IMDB-Mtrain timetest timetrain acctest acc00.010.020.030.040.050.060.070.080.090.100.20.40.60.8112345678IMDB-Btrain timetest timetrain acctest acc00.050.10.150.20.250.30.350.40.450.500.20.40.60.8112345678PROTEINStrain timetest timetrain acctest accJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

13

(a) MUTAG.

(b) PTC.

(c) NCI1.

(d) COX2.

(e) PROTEINS.

(f) SPNG.

Fig. 10. Training performances comparison.

(g) IMDB-M.

(h) IMDB-B.

alternative GNNs. For example, TL-GNN is the only method
which achieves 100% training accuracy on MUTAG. The same
observation applies to SPNG. As for the NCI1 and PTC
datasets, GIN and TL-GNN are the best performing methods,
and their training accuracies are roughly identical. On PTC,
the convergence speed of TL-GNN is slower than that of GIN,
but faster than that of the remaining methods. On the COX2
dataset, the convergence rates of TL-GNN and GIN are close,
but the ﬁnal training accuracy of TL-GNN is slightly higher
than that of GIN. On the PROTEINS dataset, the training
accuracy of TL-GNN does not outperform that of DGCNN,
which is equal to that of the other methods studied. On the two
variants of IMDB, TL-GNN and GCN achieve the best training
performance. However, TL-GNN is slightly, but consistently
better than GCN. We also observe that TL-GNN achieves a
higher training accuracy than GIN on many of the datasets
studied. Due to the fact that they have the same GNN layer
structure, these improvements come from the richer sources of
information exploited by TL-GNN. It is worth noting that the
convergence speed of TL-GNN on the SPNG dataset is much
faster than the remaining deep learning methods. The above
observations demonstrate that the subgraph-level information
captured by TL-GNN is not only helpful in distinguishing
PNGs, but also beneﬁcially enriches the graph representation.

V. CONCLUSION

In this paper, we presented a novel deep learning method for
graph-structured data, the so-called Two-level Graph Neural
Network (TL-GNN). Considering the representational limi-
tations on GNNs caused by the LPI problem, we introduce
subgraph-level information into our framework and propose

two novel operators, AGG SUB and MERG to implement
our method. Moreover, we provide distinct mathematical def-
initions for permutation non-isomorphic graphs (PNGs) and
also provide a theoretical analysis of the role of subgraphs in
solving the LPI problem. A novel subgraph counting algorithm
is also proposed, which can locate all subgraphs of a n−node
graph within a D−hop neighborhood of each node; this has
O(Dn3) time complexity and O(Dn3) space complexity.
Experimental results show that TL-GNN achieves state-of-the-
art performance on most real-world datasets and distinguishes
all the data perfectly on the synthetic PNG dataset. As for
further work, we plan to extend the method to heterogeneous
graphs and further enrich the macroscopic information cap-
tured by GNNs. Speciﬁcally, we will treat the different types
of subgraphs as heterogeneous nodes within a heterogeneous
graph and then exploit a discriminative attention mechanism
to differentiate between these nodes. Besides, we will further
discuss and explore the inﬂuence of the LPI problem on the
node classiﬁcation task.

REFERENCES

[1] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfar-
dini, “The graph neural network model,” IEEE transactions on neural
networks, vol. 20, no. 1, pp. 61–80, 2008.

[2] P. Veliˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and
Y. Bengio, “Graph Attention Networks,” International Conference
on Learning Representations, 2018.
[Online]. Available: https:
//openreview.net/forum?id=rJXMpikCZ

[3] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph
neural networks?” International Conference on Learning Representa-
tions (ICLR), 2018.

[4] V. Garg, S. Jegelka, and T. Jaakkola, “Generalization and representa-
tional limits of graph neural networks,” in International Conference on
Machine Learning. PMLR, 2020, pp. 3419–3430.

050100150200250300Epoch0.650.700.750.800.850.900.951.00AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN050100150200250300Epoch0.50.60.70.80.91.0AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN050100150200250300350400Epoch0.50.60.70.80.9AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN0255075100125150175200Epoch0.40.50.60.70.80.9AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN050100150200250300350400Epoch0.600.650.700.750.80AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN050100150200250300350400Epoch0.50.60.70.80.91.0AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN050100150200250300350400Epoch0.350.400.450.500.550.600.65AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNN0255075100125150175200Epoch0.50.60.70.80.9AccuracyGCNGINDAGCNDGCNNRW-GNNTL-GNNJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

14

energies and hydrophobicity,” Journal of medicinal chemistry, vol. 34,
no. 2, pp. 786–797, 1991.

[28] H. Toivonen, A. Srinivasan, R. D. King, S. Kramer, and C. Helma,
“Statistical evaluation of the predictive toxicology challenge 2000–
2001,” Bioinformatics, vol. 19, no. 10, pp. 1183–1193, 2003.

[29] C. Gallicchio and A. Micheli, “Ring reservoir neural networks for
graphs,” in 2020 International Joint Conference on Neural Networks
(IJCNN), 2020, pp. 1–7.

[30] R. Lab, “Protein function prediction via graph kernels,” Oral Radiology,

vol. 6, no. 2, pp. 29–35, 1990.

[31] J. J. Sutherland, L. A. O’brien, and D. F. Weaver, “Spline-ﬁtting with
a genetic algorithm: A method for developing classiﬁcation structure-
activity relationships,” Journal of chemical information and computer
sciences, vol. 43, no. 6, pp. 1906–1915, 2003.

[32] Q. Zhao and Y. Wang, “Learning metrics for persistence-based
summaries and applications for graph classiﬁcation,” CoRR, vol.
abs/1904.12189, 2019. [Online]. Available: http://arxiv.org/abs/1904.
12189

[33] C. Cai and Y. Wang, “A simple yet effective baseline for non-attributed

graph classiﬁcation,” arXiv preprint arXiv:1811.03508, 2018.

[34] N. Shervashidze, P. Schweitzer, E. J. Van Leeuwen, K. Mehlhorn,
and K. M. Borgwardt, “Weisfeiler-lehman graph kernels.” Journal of
Machine Learning Research, vol. 12, no. 9, 2011.

[35] N. Kriege and P. Mutzel, “Subgraph matching kernels for attributed
graphs,” Proceedings of the 29th International Conference on Machine
Learning, ICML 2012, vol. 2, 06 2012.

[36] P. Yanardag and S. Vishwanathan, “Deep graph kernels,” in Proceedings
the 21th ACM SIGKDD international conference on knowledge

of
discovery and data mining, 2015, pp. 1365–1374.

[37] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph
convolutional networks,” International Conference on Learning Repre-
sentations (ICLR), 2017.

[38] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and
J. Leskovec, “Graph convolutional neural networks for web-scale rec-
ommender systems,” in Proceedings of the 24th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery & Data Mining, 2018, pp.
974–983.

[39] G. Nikolentzos and M. Vazirgiannis, “Random walk graph neural
networks,” Advances in Neural Information Processing Systems, vol. 33,
pp. 16 211–16 222, 2020.

[40] H. Peng, J. Li, Q. Gong, Y. Ning, S. Wang, and L. He, “Motif-
matching based subgraph-level attentional convolutional network for
graph classiﬁcation,” in Proceedings of the AAAI Conference on Ar-
tiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 5387–5394.

[5] R. Sato, M. Yamada, and H. Kashima, “Approximation ratios of graph
neural networks for combinatorial problems,” In Neural Information
Processing Systems (NeurIPS), 2019.

[6] J. Klicpera, J. Groß, and S. G¨unnemann, “Directional message passing
for molecular graphs,” International Conference on Learning Represen-
tations (ICLR), 2020.

[7] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” Advances in neural informa-
tion processing systems, vol. 25, pp. 1097–1105, 2012.

[8] M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen, “3d
steerable cnns: Learning rotationally equivariant features in volumetric
data,” arXiv preprint arXiv:1807.02547, 2018.

[9] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. G´omez-
Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, “Convolu-
tional networks on graphs for learning molecular ﬁngerprints,” Advances
in neural information processing systems, pp. 2224–2232, 2015.
[10] M. Koyut¨urk, A. Grama, and W. Szpankowski, “An efﬁcient algorithm
for detecting frequent subgraphs in biological networks,” Bioinformatics,
vol. 20, no. suppl 1, pp. i200–i207, 2004.

[11] C. Jiang, F. Coenen, and M. Zito, “Finding frequent subgraphs in longi-
tudinal social network data using a weighted graph mining approach,” in
International Conference on Advanced Data Mining and Applications.
Springer, 2010, pp. 405–416.

[12] J. Bai, Y. Ren, and J. Zhang, “Ripple walk training: A subgraph-based
training framework for large and deep graph neural network,” arXiv
preprint arXiv:2002.07206, 2020.

[13] E. Alsentzer, S. G. Finlayson, M. M. Li, and M. Zitnik, “Subgraph neural
networks,” in Proceedings of Neural Information Processing Systems,
NeurIPS, 2020.

[14] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation
learning on large graphs,” in Advances in Neural Information Processing
Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds. Curran Associates,
Inc.,
2017, pp. 1024–1034. [Online]. Available: http://papers.nips.cc/paper/
6703-inductive-representation-learning-on-large-graphs.pdf

[15] G. Jaume, A.-p. Nguyen, M. R. Martınez, J.-P. Thiran, and M. Gabrani,
“Edgnn: Asimple and powerful gnn for di-rected labeled graphs,” arXiv
preprint arXiv:1904.08745, 2019.

[16] N. Shervashidze, P. Schweitzer, E. Jan, V. Leeuwen, and K. M. Borg-
wardt, “Weisfeiler-lehman graph kernels,” Journal of Machine Learning
Research, vol. 1, pp. 1–48, 2010.

[17] M. Zhou and J. Liu, “A memetic algorithm for enhancing the robustness
of scale-free networks against malicious attacks,” Physica A: Statistical
Mechanics and its Applications, vol. 410, pp. 131–143, 2014.

[18] L. Rong and J. Liu, “A heuristic algorithm for enhancing the robust-
ness of scale-free networks based on edge classiﬁcation,” Physica A:
Statistical Mechanics and its Applications, vol. 503, pp. 503–515, 2018.
[19] K. M. Borgwardt and H.-P. Kriegel, “Shortest-path kernels on graphs,” in
Fifth IEEE international conference on data mining (ICDM’05).
IEEE,
2005, pp. 8–pp.

[20] N. Shervashidze, S. Vishwanathan, T. Petri, K. Mehlhorn, and K. Borg-
wardt, “Efﬁcient graphlet kernels for large graph comparison,” in Arti-
ﬁcial intelligence and statistics. PMLR, 2009, pp. 488–495.

[21] J. Ugander, L. Backstrom, and J. Kleinberg, “Subgraph frequencies:
Mapping the empirical and extremal geography of large graph collec-
tions,” in Proceedings of the 22nd international conference on World
Wide Web, 2013, pp. 1307–1318.

[22] J. A. Grochow and M. Kellis, “Network motif discovery using subgraph
enumeration and symmetry-breaking,” in Annual International Confer-
ence on Research in Computational Molecular Biology. Springer, 2007,
pp. 92–106.

[23] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “Gnnex-
plainer: Generating explanations for graph neural networks,” Advances
in neural information processing systems, vol. 32, p. 9240, 2019.
[24] T. Kloks, D. Kratsch, and H. M¨uller, “Finding and counting small
induced subgraphs efﬁciently,” Information Processing Letters, vol. 74,
no. 3-4, pp. 115–121, 2000.

[25] S. Wernicke, “Efﬁcient detection of network motifs,” IEEE/ACM trans-
actions on computational biology and bioinformatics, vol. 3, no. 4, pp.
347–359, 2006.

[26] F. Eisenbrand and F. Grandoni, “On the complexity of ﬁxed parameter
clique and dominating set,” Theoretical Computer Science, vol. 326, no.
1-3, pp. 57–67, 2004.

[27] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman,
and C. Hansch, “Structure-activity relationship of mutagenic aromatic
and heteroaromatic nitro compounds. correlation with molecular orbital

