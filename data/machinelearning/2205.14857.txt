2
2
0
2

p
e
S
6

]

B
D
.
s
c
[

2
v
7
5
8
4
1
.
5
0
2
2
:
v
i
X
r
a

Demonstration of LogicLib: An Expressive Multi-Language
Interface over Scalable Datalog System

Mingda Li
limingda@cs.ucla.edu
UCLA
United States

Jin Wang
jinwang@cs.ucla.edu
UCLA
United States

Guorui Xiao
grxiao@ucla.edu
UCLA
United States

Youfu Li
youfuli@cs.ucla.edu
UCLA
United States

Carlo Zaniolo
zaniolo@cs.ucla.edu
UCLA
United States

ABSTRACT
With the ever-increasing volume of data, there is an urgent need to
provide expressive and efficient tools to support Big Data analytics.
The declarative logical language Datalog has proven very effective
at expressing concisely graph, machine learning, and knowledge
discovery applications via recursive queries. In this demonstration,
we develop Logic Library (LLib), a library of recursive algorithms
written in Datalog that can be executed in BigDatalog, a Datalog
engine on top of Apache Spark developed by us. LLib encapsulates
complex logic-based algorithms into high-level APIs, which simplify
the development and provide a unified interface akin to the one
of Spark MLlib. As LLib is fully compatible with DataFrame, it
enables the integrated utilization of its built-in applications and
new Datalog queries with existing Spark functions, such as those
provided by MLlib and Spark SQL. With a variety of examples, we
will (i) show how to write programs with LLib to express a variety
of applications; (ii) illustrate its user experience in Apache Spark
ecosystem; and (iii) present a user-friendly interface to interact
with the LLib framework and monitor the query results.

PVLDB Reference Format:
Mingda Li, Jin Wang, Guorui Xiao, Youfu Li, and Carlo Zaniolo.
Demonstration of LogicLib: An Expressive Multi-Language Interface over
Scalable Datalog System. PVLDB, 15(1): XXX-XXX, 2022.
doi:XX.XX/XXX.XX

1 INTRODUCTION
In the past years, there is a resurgence of Datalog due to its abil-
ity to specify declarative data-intensive applications that execute
efficiently over different systems and architectures [13]. The re-
cent theoretical advances [6, 12, 14] enable the usage of aggregates
in recursions, and this leads to considerable improvements in the
expressive power of Datalog. In response to supporting Datalog
queries over large-scale of datasets, many parallel and distributed
Datalog engines have been developed by researchers from both

This work is licensed under the Creative Commons BY-NC-ND 4.0 International
License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of
this license. For any use beyond those covered by this license, obtain permission by
emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights
licensed to the VLDB Endowment.
Proceedings of the VLDB Endowment, Vol. 15, No. 1 ISSN 2150-8097.
doi:XX.XX/XXX.XX

academia and industry. Following this line of efforts, we have pro-
posed BigDatalog [8], a Datalog engine on top of Apache Spark [11].
Similar with the rich libraries provide by Spark such as Spark SQL,
GraphX, MLlib and SparkR, BigDatalog provides the query inter-
face of Datalog which can benefit from the efficient in-memory
computation for analytical workloads with complicated recursions.
On the basis of above efforts for Datalog system, we further
develop several advanced applications upon it. DatalogML [9] sup-
ports training different machine learning models with gradient
descent written in Datalog queries by enabling mutual and non-
linear recursion in BigDatalog. KDDLog [5] aims at developing
data mining algorithms such as frequent item set mining, clustering
and decision tree in Datalog. Both of them have shown superior
performance than the original machine learning library MLlib in
Apache Spark. Except for Datalog, we also build the RaSQL [4, 10]
language and system to enable expressing aggregating in recursive
SQL queries. However, it requires to develop more easy-to-use APIs
as that of MLlib so as to make them compatible to the Apache Spark
ecosystem and used by broader audience.

In this demonstration, we propose Logic Library (LLib), a library
of applications written in Datalog programs built on top of the
BigDatalog system. To be more specific, LLib not only includes
popular recursive algorithms like the graph search ones [3], but
also machine learning and data mining algorithms introduced in [9]
and [5] that is supported by MLlib now. It provides a unified in-
terface that is similar with MLlib so that it is rather easy for the
audience of Spark community to use it. Users can utilize the built-in
functions that encapsulate Datalog programs to express a variety
of applications by using the DataFrame APIs. LLib also supports
APIs in multiple programming languages such as Scala, Java and
Python. To sum up, the LLib framework enables users to use the
highly efficient Datalog applications in the similar way of those
in Apache Spark MLlib thus significantly improve the usability of
Datalog based systems.

2 THE LOGIC LIB FRAMEWORK
2.1 The BigDatalog System
As introduced above, the LLib framework is built based on BigDat-
alog [8], which is a distributed Datalog engine on top of Apache
Spark. In this section, we first give a brief introduction of the system.

 
 
 
 
 
 
Figure 1: The Overall Architecture

BigDatalog supports relational algebra, aggregation and recur-
sion, as well as a host of declarative optimizations. It uses and
extends Spark SQL [1] operators, and also introduces a novel re-
cursive operator implemented in the Catalyst framework so that
its planning features can be used on the recursive plans of Datalog
programs. BigDatalog can resolve recursion in the compilation step
by recognizing recursive tables when building the operator tree and
evaluate the recursive program using the semi-naive method [7].
By enabling non-linear and mutual recursion, we can support
expressing more complicated applications like machine learning [9]
and data mining [5] using Datalog on the above systems.

2.2 Overview of LLib
The overall architecture of LLib is shown in Figure 1. Similar with
that of MLlib, we encapsulate applications written in Datalog pro-
grams into functions which can be imported and called in programs
written in high-level programming languages. As a result, these can
be seamlessly integrated with other features of the Apache Spark
ecosystem. For example, in an LLib application, there can be steps
consisting of regular DataFrame operations, the data transforma-
tions from MLlib and functions written in the Datalog provided
by LLib. Within a pipeline illustrated in Figure 1, all steps from
preparing Datalog input data to persisting and loading Datalog
execution results for subsequent processing can be sequentially
executed without extra programming efforts.

Similar with that of MLlib, LLib also supports the interfaces for
multiple programming languages, i.e. Java, Scala and Python. Since
Java is interpretable with Scala, it is relatively simple to support
Java interface. The remaining gaps between Scala and Java version
are mainly the conversions of data collections. We bridge these gaps
using collections known by both languages in Scala implementation
and using the implicit converting mechanism in Scala. To support
Python interface, we utilize Py4J 1, a bridge between Java and
Python language, and design a Removal and Recovery mechanism
where the transferred DataFrame in PySpark will be converted
to a common data collection type acceptable by both Java and
Python languages. Then during execution in Java, we could infer
the schema or directly get the schema from users.

1https://www.py4j.org/index.html

2

2.3 Programing with LLib
Next we will describe how to write a program with built-in func-
tions in LLib through the example of Transitive Closure.
2.3.1 Working Session and Acquiring Data. In LLib, the first step is
to construct a working environment for the Datalog queries and
libraries. We follow the paradigm of building Spark Session in a
similar way as that shown Figure 2.

1

2

3

4

5

session = LogiclibSession . builder () . appName("TC")\\
. master(" local [∗] " ) . getOrCreate ()
schema = StructType( List ( StructField ( "Point1" , IntegerType ,
StructField ( "Point2" , IntegerType ,
df = session . read . format( "csv" ) . option( "header" , " false " ) . schema(

true ) ) )

true ) ,

schema).load(" arc . csv" )

Figure 2: Snippet Code for Working Session and Acquiring
Data

Here TC is a built-in function of LLib for Transitive Closure
written in Datalog shown in Figure 3. LogiclibSession synthesizes
the Spark environment with required designs for logical programs.
Within the same session, users can use both Spark libraries like
data loading functions to create a DataFrame df (line 4) and the
functions in our LLib framework.

1

2

3

4

5

6

database ({
arc (From: integer , To:
}) .
tc (From,To)<− arc(From,To).
tc (From,To) <− tc (From,Tmp), arc(Tmp,To).
query tc (From, To) .

integer )

Figure 3: Query 1, Rules for Transitive Closure

Initializing an Executable Object and Schema Mapping. In
2.3.2
LLib, the data processing pipeline for a typical application is wrapped
into an executable object, for which users control initialization and
parameter setting. We initialize a transitive closure object tc with
the TC function. Then, we can set the property in the way shown
in Figure 4.

Among these built-in functions, all the libraries need the func-
tion setDirection, an extension to existing DataFrame made by us,
for schema mapping. One attribute may have different names in
different DataFrames. With schema mapping, we could know the

1

2

3

import edu. ucla . cs . wis. bigdatalog . spark . Logiclib . TC
val tc = new TC()
tc . setDirection (FromCol = "Node1", ToCol = "Node2")

Figure 4: Transitive Closure Initialization

corresponding relationship between input DataFrames and the at-
tributes in LLib. For example, in Query 1 shown in Figure 3 there
are two attributes From and To in the arc table. They can be called in
different ways such as (Node1, Node2) as shown in Figure 4. Then
the mapping between (From, To) and (Node1, Node2) is provided
by the setDirection function.
2.3.3 Execution and Persistence. With the executable object and
imported data, the execution and persistence can be merely a one-
line execution with a pre-defined function (run, genDF or genRDD)
defined by us. These three functions are implemented for each
library in LLib. They expect the input data (Dataframe) and the
environment (Session) as inputs. Their functionality satisfies all
basic requirements needed to operate on data and preserve the
result as a variable or a file. As shown in Figure 5, the function run
executes the logical programs and persist the result directly to the
target address. It can also output the data into a DataFrame or RDD
as shown in second and third line of Figure 5. In this way, users
can also apply other APIs provided by Apache Spark for further
post-processing or execution.

1

2

3

tc . run(df , output = " File " , session )
val dfNew = tc . genDF(df, session )
val rddNew = tc.genRDD(df, session )

Figure 5: Execution of Transitive Closure with LLib

3 APPLICATIONS
In this section, we introduce two typical applications of LLib. Due
to the space limitation, we omit the full Datalog programs here and
just provide the references for them, respectively.

3.1 Recursive Algorithm
We first show how to support a typical recursive algorithm in LLib.
Here we use the Datalog program for Multi-level Market (MLM)
Bonus 2 as an example. In the organization, new members are
recruited by and get products from old members (sponsors). One
member’s bonus is based on his own sales and a proportion of the
sales from the people directly or indirectly recruited by him. The
scale of the proportion is user-defined. There are two relations in
MLM Bonus, including the sponsor and sales. The sponsor relation
describes the recruiting relationship among members, while the
sales relation records the profits for each member. In Datalog syntax,
the base case should be calculating the member’s bonus by the sales
table. And the recursive rule is to calculate the bonus based on the
basic profits and the profits derived from the downstream members.
Under the LLib framework, users could implement this program
by a built-in function MLM. The full program of MLM can be found
in Program 10 of [8]. The program using LLib is detailed in Figure 6.
Here we start with two relations Sales and Sponsor stored in
DataFrame. We first build an executable object of MLM and then set

2https://en.wikipedia.org/wiki/Multi-level_marketing.

1

2

3

4

val appMLM = new MLM()
appMLM.setDirection(MCol = "Member", ProfitCol = "Bonus")
appMLM.setSecDirection(MCol = "Mem1", M2Col = "Mem2")
appMLM.run(Array(Sales, Sponsor), output = " result " , session )

Figure 6: MLM in LLib

the schema mappings for two relations with functions setDirection
and setSecDirection, which is a DataFrame function defined by us
that is similar with setDirection. To operate the data and persist to
resMLM file, we use the run function in the 4-th line.

Since most graph algorithms can be also easily expressed with
Datalog in a similar way [2], the LLib framework can support them
in a similar way.

3.2 Machine Learning and Data Mining

Figure 7: Snippet Code for LLib: Logistic Regression

Then we show how to express machine learning applications
with LLib. The example in Figure 7 expresses the process of training
a Logistic Regression classifier on the training data dataDTrain, and
making prediction on the test data, dataDTest. Here DatalogLR is
the Datalog program for logistic regression which can be found in
Query 3 of [9]. To make use of the Datalog programs for machine
learning, we first construct a working environment, i.e. Logiclib-
Session, for our library of machine learning algorithm (line: 1 to 3).
After importing the required training and predicting functions for
Logistic Regression (line: 14 to 15), we can build executable objects
for training lr (line 16) and predicting lrPredict (line: 22). The lr
object wraps all the logical rules and required relations (e.g. pa-
rameters with default value 0) of the Datalog implementation for
Logistic Regression. When initializing lr, users can exploit the built-
in functions to set the hyper-parameters that control the maximum
number of iterations, the method used for parameter initialization,
and many others. After fitting the model to df, the lrPredict object
could make predictions on the testing instances with the pre-trained
model, lrModel. In both the fitting and predicting processes, the
information of Datalog execution runtime can be obtained by
using session as an input argument, which is same as the practice
of Apache Spark.

3

Figure 9 illustrates the Web-based user interface of the LLib
framework. The upper part is a text box for users to type in the
programs to be executed, while the lower part shows the results
of execution. For novice users who are not familiar with Apache
Spark and Data Frame, we provide some pre-defined examples
which can be reused by simply replacing the name of functions
and the initialization of relations in the program. Advanced users
are welcome to come up with the program by encapsulating new
Datalog programs into user defined functions. We have imported
all supported functions in the second line of the upper text box
shown in Figure 9. Users can feel free to use other functionalities of
DataFrame provided by Apache Spark. We also support three kinds
of programming languages in this demonstration: Python, Scala
and Java. Users can switch the language by clicking the options in
the bottom left.

Our plan of demonstration consists of two steps:
Firstly, we will give a brief introduction to the functionalities of
LLib, in which we will exhibit the key components and features of
our framework. To this end, we will explain the detailed steps of
initializing the session and schema for some simple graph queries,
such as transitive closure and connected components.

Secondly, we will offer a hands-on step in which the public is
invited to directly interact with the system and test its capabilities.
Users can write the programs with algorithms provided by LLib
freely by themselves on different kinds of applications, including
graph algorithm, machine learning and data mining algorithms. We
also encourage audience to try different programming languages
and implement the same program with MLlib so as to gain similar
user experience with the official library in Apache Spark.

REFERENCES
[1] M. Armbrust, R. S. Xin, and C. L. et al. Spark SQL: relational data processing in

spark. In SIGMOD, pages 1383–1394, 2015.

[2] T. Condie, A. Das, M. Interlandi, A. Shkapsky, M. Yang, and C. Zaniolo. Scaling-up
reasoning and advanced analytics on bigdata. TPLP, 18(5-6):806–845, 2018.
[3] A. Das, Y. Li, J. Wang, M. Li, and C. Zaniolo. Bigdata applications from graph
analytics to machine learning by aggregates in recursion. In ICLP, pages 273–279,
2019.

Figure 8: Snippet Code: Implementation with MLlib

For the sake of comparison, we also show how Apache Spark
MLlib will be used to implement the above example. The snippet
code is shown in Figure 8. The pipeline of functionalities is very
similar to that of our APIs; this will make it much easier using the
DataFrame APIs in our library for those who are already familiar
with MLlib. Although there are minor differences in the aspects of
data formatting and usage of some public functions, e.g. transform
and assembler, the expression of MLlib and our LLib are very similar
and both user-friendly.

4 DEMONSTRATION

[4] J. Gu, Y. Watanabe, W. Mazza, A. Shkapsky, M. Yang, L. Ding, and C. Zaniolo.
Rasql: Greater power and performance for big data analytics with recursive-
aggregate-sql on spark. In SIGMOD, pages 467–484, 2019.

[5] Y. Li, J. Wang, M. Li, A. Das, J. Gu, and C. Zaniolo. Kddlog: Performance and
scalability in knowledge discovery by declarative queries with aggregates. In
ICDE, pages 1260–1271, 2021.

[6] M. Mazuran, E. Serra, and C. Zaniolo. Extending the power of datalog recursion.

VLDB J., 22(4):471–493, 2013.

[7] M. Shaw, P. Koutris, B. Howe, and D. Suciu. Optimizing large-scale semi-naïve
datalog evaluation in hadoop. In Datalog in Academia and Industry, pages 165–176,
2012.

[8] A. Shkapsky, M. Yang, M. Interlandi, H. Chiu, T. Condie, and C. Zaniolo. Big data
analytics with datalog queries on spark. In SIGMOD, pages 1135–1149, 2016.
[9] J. Wang, J. Wu, M. Li, J. Gu, A. Das, and C. Zaniolo. Formal semantics and high
performance in declarative machine learning using datalog. VLDB J., 30(5):859–
881, 2021.

[10] J. Wang, G. Xiao, J. Gu, J. Wu, and C. Zaniolo. RASQL: A powerful language and
its system for big data applications. In SIGMOD, pages 2673–2676, 2020.
[11] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M. J. Franklin,
S. Shenker, and I. Stoica. Resilient distributed datasets: A fault-tolerant abstraction
for in-memory cluster computing. In NSDI, pages 15–28, 2012.

[12] C. Zaniolo, A. Das, J. Gu, Y. Li, M. Li, and J. Wang. Monotonic properties of
completed aggregates in recursive queries. CoRR, abs/1910.08888, 2019.
[13] C. Zaniolo, A. Das, J. Gu, Y. Li, M. Li, and J. Wang. Developing big-data application
as queries: an aggregate-based approach. IEEE Data Eng. Bull., 44(2):3–13, 2021.
[14] C. Zaniolo, M. Yang, A. Das, A. Shkapsky, T. Condie, and M. Interlandi. Fixpoint
semantics and optimization of recursive datalog programs with aggregates. TPLP,
17(5-6):1048–1065, 2017.

Figure 9: The User Interface of LLib

4

