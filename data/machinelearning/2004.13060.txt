GIMP-ML : PYTHON PLUGINS FOR USING COMPUTER VISION
MODELS IN GIMP

0
2
0
2

t
c
O
6
2

]

V
C
.
s
c
[

3
v
0
6
0
3
1
.
4
0
0
2
:
v
i
X
r
a

Kritik Soman
Department of Electrical Engineering,
Indian Institute of Technology Kanpur,
India
kritiksoman@ieee.org

October 27, 2020

ABSTRACT

This paper introduces GIMP-ML v1.1, a set of Python plugins for the widely popular GNU Image
Manipulation Program (GIMP). It enables the use of recent advances in computer vision to the conven-
tional image editing pipeline. Applications from deep learning such as monocular depth estimation,
semantic segmentation, mask generative adversarial networks, image super-resolution, de-noising,
de-hazing, matting, enlightening and coloring have been incorporated with GIMP through Python-
based plugins. Additionally, operations on images such as k-means based color clustering have also
been added. GIMP-ML relies on standard Python packages such as numpy, pytorch, open-cv,
scipy. Apart from these, several image manipulation techniques using these plugins have been com-
piled and demonstrated in the YouTube channel (https://youtube.com/user/kritiksoman)
with the objective of demonstrating the use-cases for machine learning based image modiﬁcation.
In addition, GIMP-ML also aims to bring the beneﬁts of using deep learning networks used for
computer vision tasks to routine image processing workﬂows. The code and installation procedure
for conﬁguring these plugins is available at https://github.com/kritiksoman/GIMP-ML.

1

Introduction

Image editing has conventionally been performed manually by users or graphics designers using various image
processing tools or software. A plethora of image editing and transformation functions are provided in such tools,
which are available in open-source, commercial or proprietary license-based modes. Image processing workﬂows have
varying levels of complexity and sometimes even require signiﬁcant effort from the user even for simple modiﬁcations
to images.

GNU Image Manipulation Program (GIMP) is a popular free and open source image editing software that has been
widely used on Linux-based platforms, as well as on other operating systems. It provides several features for image
editing and manipulation and has a simple user interface to work with. It also supports the development of plugins
which can be developed independently and integrated with the local GIMP installation on a computer. Using plugins,
one can realize custom workﬂows or set of operations that can be applied to an image.

Recently, machine learning techniques have completely changed the landscape of image understanding and many
applications which were previously not possible have now become the new baseline. This has signiﬁcantly been
facilitated by recent advances in deep learning and the applications of resultant models to tasks in the computer vision
domain. However, these deep learning models have been made available to users using independent deep learning
frameworks such as Keras, TensorFlow, PyTorch, among others. It may also be noted here that since these networks
have a “large” architecture, their training is done on compute-intensive platforms (using GPUs) and the resultant
models have a high memory footprint. Since the use of these models requires the user to code, graphics designers
and users involved in conventional image editing workﬂows using image processing tools have not often been able
to directly leverage the beneﬁts from the deep learning models. As such, developing a framework that would enable

 
 
 
 
 
 
GIMP-ML - OCTOBER 27, 2020

the use of deep learning models in image editing tasks through commonly available image processing tools would
potentially beneﬁt both the deep learning / computer vision community as well as graphics designers and common users
of such software.

The motivation for this paper is to bridge the gap between cutting edge research in deep learning (computer vision)
and manual image editing, speciﬁcally for the case of GIMP. A pilot implementation of plugins for GIMP, collectively
termed as “GIMP-ML” (GIMP - Machine Learning), have been presented for various tasks such as background blurring,
image coloring, face parsing, generative portrait modiﬁcation, monocular depth based relighting, motion blurring,
image de-noising, de-hazing, matting, enlightening and generating super-resolution images. It is expected that the
image editing process would become highly automated in the upcoming future as the semantic understanding of images
improves, which would be facilitated by advances in artiﬁcial intelligence.

Figure 1: GIMP-ML Plugins Menu

The rest of this paper is organized as follows. Section 2 presents they key dependencies for GIMP-ML. This is followed
by implementation details in Section 3. Various applications of GIMP-ML have been illustrated in Section 5, which also
includes links to demonstration videos on YouTube. Finally, conclusions and future work are presented in Section 6.

2 Dependencies

The Python package dependencies involved in the development of GIMP-ML are as follows:

1. NumPy: The base N-dimensional array package, numpy [1], has been used for converting GIMP layer to a

tensor for use in Pytorch.

2. SciPy: The fundamental library for scientiﬁc computing, scipy [2], has been used for performing basic

computing operations.

3. OpenCV: The opencv-python[3] package provides OpenCV libraries in Python. It has been used for edge

detection.

4. Pre-Trained Models: The pretrainedmodels includes a set of pre-trained models for PyTorch [4], of which

the InceptionResNetV2 has been used for the applications presented in this paper.

2

GIMP-ML - OCTOBER 27, 2020

5. Torch & Torchvision: The torch [4] and torchvision [5] packages have been used to incorporate the deep

learning framework through Pytorch.

3

Implementation Details

The GIMP-ML plugins have been developed in Python 2.7 which is supported in GIMP 2.10. A virtual environment
has been separately created and added to the gimp-python path. This contains all the python packages used by the
plugins. The plugins use CPU by default and switch to GPU for prediction when available. Additionally, there is an
option to force the usage of CPU. The alpha channel in the input layer is dropped when not required as input to the
deep learning network. The plugins take advantage of layers in GIMP for various workﬂows. As a consequence, image
manipulation in the subsequent applications is also non-destructive in nature.

4 Tools

The tools available in GIMP-ML are summarized in Table 1. All deep learning based tools have a button to force the
use of CPU.

faceparse: For segmenting portrait
images, we used
BiSeNet [6] trained on the CelebAMask-HQ dataset 1. It
can segment 19 classes such as skin, nose, eyes, eyebrows,
ears, mouth, lip, hair, hat, eyeglass, earring, necklace, neck,
and cloth.

deblur: We have ported the DeblurGAN-v2 [8] model
trained on the GoPro dataset 2 for deblurring images.

facegen: With the facegen plugin, facial features in por-
trait photo can be segmented, modiﬁed and then newly gen-
erated. Trained on the CelebAMask-HQ dataset [7], this
model 3 relies on facial segmentation map generated in the
previous faceparse sub-section. The mask can be duplicated
into another layer and it can be manipulated using Color
Picker Tool and Paintbrush Tool. The original input image
, original mask and modiﬁed mask can then be fed into
Mask-GAN to generate the desired image.

1https://github.com/zllrunning/face-parsing.PyTorch
2https://seungjunnah.github.io/Datasets/gopro
3https://github.com/switchablenorms/CelebAMask-HQ

3

GIMP-ML - OCTOBER 27, 2020

deepcolor: User guided image colorization 4 as proposed
by Zhang et. al. in [9] is available as the deepcolor tool. The
model is trained on the ImageNet dataset [10]. The color
mask layer should be transparent RGB layer (with alpha
channel) that contains (local points) dots of size 6 pixels
specifying which color should be present at which location.
The tool can be used without this layer if the image and
color mask layers are set to the same layer containing the
image.

monodepth: Disparity maps can be generated from images
using deep learning methods and depth from stereo images.
Recently, robust monocular depth estimation has been pro-
posed in [11]. This model 5 has been ported for GIMP-ML.
The model was trained using multi-objective optimization
on several RGB-depth datasets.

super-resolution: The model in [12] for image super reso-
lution 6 was also implemented. Using this plugin the input
image layer can be upscaled to upto 4x its original size.

semantic-segmentation: We used the Pytorch Deeplabv3
[14] model trained on the Pascal VOC dataset [15] for the
semantic segmentation tool. It supports the 20 classes of
Pascal VOC, namely, person, bird, cat, cow, dog, horse,
sheep, aeroplane, bicycle, boat, bus, car, motorbike, train,
bottle, chair, dining table, potted plant, sofa, and tv/monitor.
These objects can be directly segmented in images.

kmeans: The scipy [2] implementaion of kmeans was used.
The tool requires the image layer, and number of clus-
ters/colors in output. There is also an option to use (x,y)
position coordinates as features for clustering.

4https://github.com/junyanz/interactive-deep-colorization
5https://github.com/intel-isl/MiDaS
6https://github.com/twtygqyy/pytorch-SRResNet

4

GIMP-ML - OCTOBER 27, 2020

dehaze: Image de-hazing based on deep learning 7 as pro-
posed by Li et. al. in All-in-One Dehazing Network [16] is
available as the deep-dehaze tool .

deepmatting: Deep learning based image matting 8 as pro-
posed by Xu et. al. in [13] has also been ported into GIMP-
ML. It requires two layers, namely the image layer and
the trimap layer. The trimap Layer should contain segmen-
tation mask of the object with RGB as [128,128,128] for
boundaries, [255,255,255] for object the and [0,0,0] for
background.

denoise: Image de-noising 9 as propsed by Zhou et.al. in
[17] is available as the deep-denoise tool.

enlighten: EnlightenGAN 10 proposed by Jiang et. al. in
[18] is available as the enlighten tool.

Table 1: Tools available in GIMP-ML

5 Applications

This section describes applications of GIMP-ML, which include background blurring, image re-coloring, face editing,
generative portrait modiﬁcation, monocular depth based relighting, motion blurring and generating super-resolution
images. Demo videos of all the applications are available in the YouTube channel:
https://youtube.com/user/kritiksoman

7https://github.com/MayankSingal/PyTorch-Image-Dehazing
8https://github.com/huochaitiantang/pytorch-deep-image-matting
9https://github.com/yzhouas/PD-Denoising-pytorch
10https://github.com/VITA-Group/EnlightenGAN

5

GIMP-ML - OCTOBER 27, 2020

5.1 Background Blurring

The semantic-segmentation tool can be used to get object boundaries of the 20 predeﬁned classes. We can then use it to
selectively perform operations on regions of the image, such as blurring, hue/saturation change etc. A demonstration
video for background blurring has been shown in https://youtu.be/U1CieWi--gc

5.2

Image Re-coloring

In order to re-colour an image, an RGB image can be converted to grayscale and then multiple colored version can be
generated using different local hints layers. The resulting layers can then be selectively erased to retained colors as
desired. An example has been shown in https://youtu.be/4YpTa-gqEIw.

5.3 Face Editing

The segmentation map from faceparse tool can then be used to selectively manipulate various facial features. Hair color
manipulation has been demonstrated in the video demo using this network. The demo for hair color manipulation using
this plugin can be viewed at https://youtu.be/thS8VqPvuhE

5.4 Generative Portrait Modiﬁcation

Facegen tool can be used to modiﬁy facial features in a portrait image. A drawback of such a Mask-GAN is that
it does not preserve unmodiﬁed facial features. This can, however, be taken care of by manually erasing unwanted
facial feature changes from the generated layer thereby exposing the original image in the layer underneath. This is
a valuable workﬂow since professional image editors spend a large amount of time in making portrait shots perfect
and would retain original image facial features. The demo has generative portrait modiﬁcation has been shown in
https://youtu.be/kXYsWvOB4uk

5.5 Monocular Depth based Relighting

Using the monodepth tool, the disparity map of images can be desaturated, inverted and colorized to created a layer
representing light falling from the sky. In the demo video (https://youtu.be/q9Ny5XqIUKk), a day time image of a
street has been converted to night time using this approach. As another example, a light painting has been created from
a day time image and a tutorial has been shown in the youtube video https://youtu.be/squyQYrllBg.

6 Conclusions and Future Work

This paper presented GIMP-ML, a set of Python plugins that enabled the use of deep learning models in GIMP via
Pytorch for various applications. It has been shown that several manual and time-consuming image processing tasks
can be simpliﬁed by the use of deep learning models, which makes it convenient for the users of image manipulation
software to perform such tasks. GIMP 2.10 currently relies on Python 2.7 which been deprecated as on 1 January 2020.
The next version of GIMP would use Python 3 and GIMP-ML codebase would be updated to support this. Further, deep
learning models suffer from the data bias problem and only work well when the test image is from the same distribution
as the data on which the model was trained. In future, the framework would be enhanced to handle such scenarios.

References

[1] Stéfan van der Walt, S Chris Colbert, and Gael Varoquaux. The numpy array: a structure for efﬁcient numerical

computation. Computing in Science & Engineering, 13(2):22–30, 2011.

[2] Eric Jones, Travis Oliphant, and Pearu Peterson. Scipy: Open source scientiﬁc tools for python. 2001.
[3] Alexander Mordvintsev and K Abid. Opencv-python tutorials documentation. Obtenido de https://media.

readthedocs. org/pdf/opencv-python-tutroals/latest/opencv-python-tutroals. pdf, 2014.

[4] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.

[5] Sébastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In Proceedings of the

18th ACM international conference on Multimedia, pages 1485–1488, 2010.

6

GIMP-ML - OCTOBER 27, 2020

[6] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation

network for real-time semantic segmentation, 2018.

[7] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: towards diverse and interactive facial image

manipulation, 2019.

[8] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang Wang. Deblurgan-v2: Deblurring (orders-of-

magnitude) faster and better, 2019.

[9] Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S Lin, Tianhe Yu, and Alexei A Efros.
Real-time user-guided image colorization with learned deep priors. arXiv preprint arXiv:1705.02999, 2017.
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009.
[11] Katrin Lasinger, René Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation:

Mixing datasets for zero-shot cross-dataset transfer. arXiv preprint arXiv:1907.01341, 2019.

[12] Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew
Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a
generative adversarial network, 2017.

[13] Ning Xu, Brian Price, Scott Cohen, and Thomas Huang. Deep image matting. In Proceedings of the IEEE

Conference on Computer Vision and Pattern Recognition, pages 2970–2979, 2017.

[14] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for

semantic image segmentation, 2017.

[15] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal

visual object classes (voc) challenge. International journal of computer vision, 88(2):303–338, 2010.

[16] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and Dan Feng. Aod-net: All-in-one dehazing network. In

Proceedings of the IEEE international conference on computer vision, pages 4770–4778, 2017.

[17] Yuqian Zhou, Jianbo Jiao, Haibin Huang, Yang Wang, Jue Wang, Honghui Shi, and Thomas Huang. When

awgn-based denoiser meets real noises. arXiv preprint arXiv:1904.03485, 2019.

[18] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and
arXiv preprint

Zhangyang Wang. Enlightengan: Deep light enhancement without paired supervision.
arXiv:1906.06972, 2019.

[19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution,

2016.

7

