1
2
0
2

t
c
O
5

]

C
O
.
h
t
a
m

[

1
v
8
5
8
1
0
.
0
1
1
2
:
v
i
X
r
a

KKT Conditions, First-Order and Second-Order Optimization, and
Distributed Optimization: Tutorial and Survey

Benyamin Ghojogh
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

BGHOJOGH@UWATERLOO.CA

Ali Ghodsi
Department of Statistics and Actuarial Science & David R. Cheriton School of Computer Science,
Data Analytics Laboratory, University of Waterloo, Waterloo, ON, Canada

ALI.GHODSI@UWATERLOO.CA

Fakhri Karray
Department of Electrical and Computer Engineering,
Centre for Pattern Analysis and Machine Intelligence, University of Waterloo, Waterloo, ON, Canada

KARRAY@UWATERLOO.CA

Mark Crowley
Department of Electrical and Computer Engineering,
Machine Learning Laboratory, University of Waterloo, Waterloo, ON, Canada

MCROWLEY@UWATERLOO.CA

Abstract
This is a tutorial and survey paper on Karush-
Kuhn-Tucker (KKT) conditions, ﬁrst-order and
second-order numerical optimization, and dis-
tributed optimization. After a brief review of
history of optimization, we start with some pre-
liminaries on properties of sets, norms, func-
tions, and concepts of optimization. Then, we
introduce the optimization problem, standard op-
timization problems (including linear program-
ming, quadratic programming, and semideﬁnite
programming), and convex problems. We also
introduce some techniques such as eliminating
inequality, equality, and set constraints, adding
slack variables, and epigraph form. We intro-
duce Lagrangian function, dual variables, KKT
conditions (including primal feasibility, dual fea-
sibility, weak and strong duality, complementary
slackness, and stationarity condition), and solv-
ing optimization by method of Lagrange multi-
pliers. Then, we cover ﬁrst-order optimization
including gradient descent, line-search, conver-
gence of gradient methods, momentum, steepest
descent, and backpropagation. Other ﬁrst-order
methods are explained, such as accelerated gra-
dient method, stochastic gradient descent, mini-

batch gradient descent, stochastic average gradi-
ent, stochastic variance reduced gradient, Ada-
Grad, RMSProp, and Adam optimizer, proximal
methods (including proximal mapping, proximal
point algorithm, and proximal gradient method),
and constrained gradient methods (including pro-
jected gradient method, projection onto convex
sets, and Frank-Wolfe method). We also cover
non-smooth and (cid:96)1 optimization methods includ-
ing lasso regularization, convex conjugate, Hu-
ber function, soft-thresholding, coordinate de-
scent, and subgradient methods. Then, we ex-
plain second-order methods including Newton’s
method for unconstrained, equality constrained,
and inequality constrained problems. We explain
the interior-point method, barrier methods, Wolfe
conditions for line-search, fast solving system of
equations (including decomposition methods and
conjugate gradient), and quasi-Newton’s method
(including BFGS, LBFGS, Broyden, DFP, and
SR1 methods). The sequential convex program-
ming for non-convex optimization is also in-
troduced. Finally, we explain distributed opti-
mization including alternating optimization, dual
decomposition methods, augmented Lagrangian,
and alternating direction method of multipliers
(ADMM). We also introduce some techniques
for using ADMM for many constraints and vari-
ables.

 
 
 
 
 
 
Contents

1

Introduction

2 Notations and Preliminaries

2.1 Preliminaries on Sets and Norms .
.
2.2 Preliminaries on Functions .
.
.
2.3 Preliminaries on Optimization .
.
.
2.4 Preliminaries on Derivative .

.

.

.
.
.
.

.
.
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Optimization Problems
3.1 Standard Problems

.

.

.

.

.

.

.

.

.

.

.

.
.

.
.

Second-Order Cone Programming (SOCP)
Semideﬁnite Programming (SDP)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.1 General Optimization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.2 Convex Optimization Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.3 Linear Programming .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.4 Quadratic Programming .
. . . . . . . . . . . . . . . . . . . .
3.1.5 Quadratically Constrained Quadratic Programming (QCQP)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.6
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.7
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.1.8 Optimization Toolboxes
.
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2 Eliminating Constraints and Equivalent Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.1 Eliminating Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 Eliminating Equality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3 Adding Equality Constraints .
.
3.2.4 Eliminating Set Constraints
.
.
3.2.5 Adding Slack Variables .
.
.
.
3.2.6 Epigraph Form .

.
.

.

.

.

.

.

.

4 Karush-Kuhn-Tucker (KKT) Conditions
.

4.1 The Lagrangian Function .

.

.

.

.

.

Sign of Terms in Lagrangian .
Interpretation of Lagrangian .
.
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 Lagrangian and Dual Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.4 Lagrange Dual Function .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
4.2 Primal Feasibility .
4.3 Dual Feasibility .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
4.4 The Dual Problem, Weak and Strong Duality, and Slater’s Condition . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5 Complementary Slackness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
4.6 Stationarity Condition .
4.7 KKT Conditions .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
4.8 Solving Optimization by Method of Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

5 First-Order Optimization: Gradient Methods

5.1 Gradient Descent

.

.

.

.
Step of Update .
.

.
5.1.1
.
.
5.1.2 Line-Search .
5.1.3 Backtracking Line-Search .
.
5.1.4 Convergence Criterion .

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

.

.

.
.
.
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

5

7
7
9
11
12

13
13
13
14
14
14
14
14
14
14
15
15
15
15
15
15
15

15
16
16
16
16
16
16
16
17
18
18
19
19

20
20
20
21
21
21

.
.

.
.
.
.

.
.
.
.

.
.
.
.
.

Steepest Descent

5.4 Stochastic Average Gradient Methods

.
.
.
.
Stochastic Gradient Descent

5.3.1
5.3.2 Mini-batch Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
5.1.5 Convergence Analysis for Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.6 Gradient Descent with Momentum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
5.1.7
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.1.8 Backpropagation .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Accelerated Gradient Method .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Stochastic Gradient Methods .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proximal Mapping and Projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proximal Point Algorithm .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Proximal Gradient Method .
5.6 Gradient Methods for Constrained Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Projected Gradient Method .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Projection Onto Convex Sets (POCS) and Averaged Projections . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Frank-Wolfe Method .

Stochastic Average Gradient
5.4.1
5.4.2
Stochastic Variance Reduced Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4.3 Adapting Learning Rate with AdaGrad, RMSProp, and Adam . . . . . . . . . . . . . . . . . .

5.5 Proximal Methods .

5.5.1
5.5.2
5.5.3

5.6.1
5.6.2
5.6.3

.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Non-smooth and L1 Norm Optimization Methods

.

.
.

.
.

.
.
.

.
.
.

.
.
.

.
.
.

.
.
.

6.1 Lasso Regularization .
.
6.2 Convex Conjugate .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
6.2.1 Convex Conjugate .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.2.2 Huber Function: Smoothing L1 Norm by Convex Conjugate . . . . . . . . . . . . . . . . . . . .
6.3 Soft-thresholding and Proximal Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6.4 Coordinate Descent .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Subgradient .
.
Subgradient Method .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Stochastic Subgradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Projected Subgradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
6.4.1 Coordinate Method .
.
6.4.2 L1 Norm Optimization .
.
.
.

6.5 Subgradient Methods .
.

6.5.1
6.5.2
6.5.3
6.5.4

.
.
.
.
.
.

.
.
.
.
.
.

.
.
.
.
.
.

.
.

.
.

.
.

.
.

.

.

.

7 Second-Order Optimization: Newton’s Method

7.1 Newton’s Method from the Newton-Raphson Root Finding Method . . . . . . . . . . . . . . . . . . . .
7.2 Newton’s Method for Unconstrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.3 Newton’s Method for Equality Constrained Optimization . . . . . . . . . . . . . . . . . . . . . . . . . .
Interior-Point and Barrier Methods: Newton’s Method for Inequality Constrained Optimization . . . . . .
7.4
7.5 Wolfe Conditions and Line-Search in Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6 Fast Solving System of Equations in Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6.1 Decomposition Methods .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7.6.2 Conjugate Gradient Method .
.
7.6.3 Nonlinear Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.7 Quasi-Newton’s Methods .

.

.

.

.

.

.

.

3

22
22
22
23
23
24
24
24
25
25
26
26
27
27
28
28
29
29
30
30

30
30
30
30
31
32
32
32
32
33
33
34
34
34

34
34
35
35
36
36
37
37
38
38
39

.
7.7.1 Hessian Approximation .
7.7.2 Quasi-Newton’s Algorithms .

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8 Non-convex Optimization by Sequential Convex Programming

8.1 Convex Approximation .

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8.1.1 Convex Approximation by Taylor Series Expansion . . . . . . . . . . . . . . . . . . . . . . . .
.
8.1.2 Convex Approximation by Particle Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
8.1.3 Convex Approximation by Quasi-linearization . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.
.
Formulation of Trust Region .
.

8.2.1
8.2.2 Updating Trust Region .

.

.

.

.

.

.

.

.

.

.

.

.

8.2 Trust Region .

9 Distributed Optimization

.

.

.

.

.

.

9.1 Alternating Optimization .
9.2 Dual Ascent and Dual Decomposition Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9.3 Augmented Lagrangian Method (Method of Multipliers)
9.4 Alternating Direction Method of Multipliers (ADMM)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . .
.
.
.
9.5 ADMM Algorithm for General Optimization Problems and Any Number of Variables . . . . . . . . . . .
9.5.1 Distributed Optimization .
.
9.5.2 Making Optimization Problem Distributed . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Simplifying Equations in ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.4.1 ADMM Algorithm .
9.4.2

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

.

.

.

.

.

.

10 Additional Notes

.
10.1 Cutting-Plane Methods .
10.2 Ellipsoid Method .
.
.
10.3 Minimax and Maximin Problems .
.
10.4 Riemannian Optimization .
.
.
10.5 Metaheuristic Optimization .

.
.

.
.

.
.

.
.

.
.

.
.

.

.

11 Conclusion

A Proofs for Section 2

A.1 Proof for Lemma 5 .
A.2 Proof for Lemma 6 .
A.3 Proof for Lemma 7 .
A.4 Proof for Lemma 8 .
A.5 Proof for Lemma 9 .

.
.
.
.
.

B Proofs for Section 5

B.1 Proof for Lemma 13 .
.
B.2 Proof for Theorem 1 .
B.3 Proof for Theorem 2 .

C Proofs for Section 7

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

C.1 Proof for Theorem 8 .

.

.

.

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

39
39

41
41
41
41
42
42
42
42

42
42
43
44
45
45
45
45
45
47

48
48
48
48
48
49

49

49
49
50
50
50
51

51
51
51
51

52
52

1. Introduction
– KKT conditions and numerical optimization: Nu-
merical optimization has application in various ﬁelds of
science. Many of the optimization methods can be ex-
plained in terms of the Karush-Kuhn-Tucker (KKT) condi-
tions (Kjeldsen, 2000), proposed in (Karush, 1939; Kuhn
& Tucker, 1951). The KKT conditions discuss the pri-
mal and dual problems with primal and dual variables, re-
spectively, where the dual minimum is a lower-bound on
the primal optimum. In an unconstrained problem, if set-
ting the gradient of a cost function to zero gives a closed-
form solution, the optimization is done; however, if we
do not have a closed-form solution, we should use nu-
merical optimization which ﬁnds the solution iteratively
and gradually. Besides, if the optimization is constrained,
constrained numerical optimization should be used. The
numerical optimization methods can be divided into ﬁrst-
order and second-order methods.

– History of ﬁrst-order optimization: The ﬁrst-order
methods are based on gradient while the second-order
methods make use of Hessian or approximation of Hes-
sian as well as gradient. The most well-known ﬁrst-
order method is gradient descent, ﬁrst suggested by
Cauchy in 1874 (Lemar´echal, 2012) and Hadamard in 1908
(Hadamard, 1908), whose convergence was later analyzed
in (Curry, 1944). Backpropagation, for training neural
networks, was proposed in (Rumelhart et al., 1986) and
it is gradient descent used with chain rule.
It is found
out in 1980’s that gradient descent is not optimal in con-
vergence rate. Therefore, Accelerated Gradient Method
(AGM) was proposed by Nesterov (Nesterov, 1983; 1988;
2005) which had an optimal convergence rate in gradient
methods. Stochastic methods were also proposed for large
volume optimization when we have a dataset of points.
They randomly sample points or batches of points for use in
gradient methods. Stochastic Gradient Descent (SGD), ﬁrst
proposed in (Robbins & Monro, 1951), was ﬁrst used for
machine learning in (Bottou et al., 1998). Stochastic Av-
erage Gradient (SAG) (Roux et al., 2012), Stochastic Vari-
ance Reduced Gradient (SVRG) (Johnson & Zhang, 2013)
are two other example methods in this category. Some
techniques, such as AdaGrad (Duchi et al., 2011), Root
Mean Square Propagation (RMSProp) (Tieleman & Hin-
ton, 2012), Adaptive Moment Estimation (Adam) (Kingma
& Ba, 2014), have also been proposed for adaptive learning
rate in stochastic optimization.

– History of proximal methods: Another family of op-
timization methods are the proximal methods (Parikh &
Boyd, 2014) which are based on the Moreau-Yosida reg-
ularization (Moreau, 1965; Yosida, 1965). Some prox-
imal methods are the proximal point algorithm (Rock-

5

afellar, 1976) and the proximal gradient method (Nes-
terov, 2013). The proximal mapping can also be used
for constrained gradient methods such as projected gra-
dient method (Iusem, 2003). Another effective ﬁrst-
order method for constrained problems is the Frank-Wolfe
method (Frank & Wolfe, 1956).

– History of non-smooth optimization: Optimization of
non-smooth functions is also very important especially be-
cause of use of (cid:96)1 norm for sparsity in many applica-
tions. Some techniques for (cid:96)1 norm optimization are (cid:96)1
norm approximation by Huber function (Huber, 1992),
soft-thresholding which is the proximal mapping of (cid:96)1
norm, and coordinate descent (Wright, 2015) which can be
used for (cid:96)1 norm optimization (Wu & Lange, 2008). Sub-
gradient methods, including stochastic subgradient method
(Shor, 1998) and projected subgradient method (Alber
et al., 1998), can also be used for non-smooth optimization.

– History of second-order optimization: Second-order
methods use Hessian, or inverse of Hessian, or their ap-
proximations. The family of second-order methods can
be named the Newton’s methods which are based on the
Newton-Raphson method (Stoer & Bulirsch, 2013). Con-
strained second-order methods can be solved using the
interior-point method, ﬁrst proposed in (Dikin, 1967). The
interior-point method is also called the barrier methods
(Boyd & Vandenberghe, 2004; Nesterov, 2018) and Se-
quential Unconstrained Minimization Technique (SUMT)
(Fiacco & McCormick, 1967). Interior-point method is a
very powerful method and is often the main method of solv-
ing optimization problems in optimization toolboxes such
as CVX (Grant et al., 2009).
The second-order methods are usually faster than the ﬁrst-
order methods because of using the Hessian information.
However, computation of Hessian or approximation of
Hessian in second-order methods is time-consuming and
difﬁcult. This might be the reason for why most machine
learning algorithms, such as backpropagation for neural
networks (Rumelhart et al., 1986), use ﬁrst-order methods.
Although, note that some few machine learning algorithms,
such as logistic regression and Sammon mapping (Sam-
mon, 1969), use second-order optimization.
The update of solution in either ﬁrst-order or second-order
methods can be stated as a system of linear equations. For
large-scale optimization, the Newton’s method becomes
slow and intractable. Therefore, decomposition methods
(Golub & Van Loan, 2013), conjugate gradient method
(Hestenes & Stiefel, 1952), and nonlinear conjugate gra-
dient method (Fletcher & Reeves, 1964; Polak & Ribiere,
1969; Hestenes & Stiefel, 1952; Dai & Yuan, 1999) can be
used for approximation of solution to the system of equa-
tions. Truncated Newton’s methods (Nash, 2000), used

for large scale optimization, usually use conjugate gradi-
ent. Another approach for approximating Newton’s method
for large-scale data is the quasi-Newton’s method (No-
cedal & Wright, 2006, Chapter 6) which approximates the
Hessian or inverse Hessian matrix. The well-known algo-
rithms for quasi-Newton’s method are Broyden-Fletcher-
Goldfarb-Shanno (BFGS) (Fletcher, 1987; Dennis Jr &
Schnabel, 1996), limited-memory BFGS (LBFGS) (No-
cedal, 1980; Liu & Nocedal, 1989), Davidon-Fletcher-
Powell (DFP) (Davidon, 1991; Fletcher, 1987), Broyden
method (Broyden, 1965), and Symmetric Rank-one (SR1)
(Conn et al., 1991).

– History of line-search: Both ﬁrst-order and second-
order optimization methods have a step size parameter to
move toward their descent direction. This step size can
be calculated at every iteration using line-search methods.
Well-known line-search methods are the backtracking or
Armijo line-search (Armijo, 1966) and the Wolfe condi-
tions (Wolfe, 1969).

– Standard problems: The terms “programming” and
“program” are sometimes used to mean “optimization”
and ”optimization problem”, respectively,
in the litera-
ture. Convex optimization or convex programming started
to develop since 1940’s (Tikhomirov, 1996). There exist
some standard forms for convex problems which are linear
programming, quadratic programming, quadratically con-
strained quadratic programming, second-order cone pro-
gramming, and Semideﬁnite Programming (SDP). An im-
portant method for solving linear programs was the simplex
method proposed in 1947 (Dantzig, 1983). SDP is also im-
portant because the standard convex problems can be stated
as special cases of SDP and then may be solved using the
interior-point method.

– History of non-convex optimization: There also ex-
ist methods for non-convex optimization. These methods
are either local or global methods. The local methods
are faster but ﬁnd a local solution depending on the ini-
tial solution. The global methods, however, ﬁnd the global
solution but are slower. Examples for local and global
non-convex methods are Sequential Convex Programming
(SCP) (Dinh & Diehl, 2010) and branch and bound (Land
& Doig, 1960), respectively. SCP uses trust region (Conn
et al., 2000) and it solves a sequence of convex approxima-
tions of the problem. It is related to Sequential Quadratic
Programming (SQP) (Boggs & Tolle, 1995) which is used
for constrained nonlinear optimization. The branch and
bound methods use a binary tree structure for optimizing
on a non-convex cost function.

– History of distributed optimization: Distributed op-

6

timization has two beneﬁts. First, it makes the problem
able to run in parallel on several servers. Secondly, it can
be used to solve problems with multiple optimization vari-
ables. Especially, for the second reason, it has been widely
used in machine learning and signal processing. Two most
well-known distributed optimization approaches are alter-
nating optimization (Jain & Kar, 2017; Li et al., 2019)
and Alternating Direction Method of Multipliers (ADMM)
(Gabay & Mercier, 1976; Glowinski & Marrocco, 1976;
Boyd et al., 2011). Alternating optimization alternates
between optimizing over variables one-by-one, iteratively.
ADMM is based on dual decomposition (Dantzig & Wolfe,
1960; Benders, 1962; Everett III, 1963) and augmented
Lagrangian (Hestenes, 1969; Powell, 1969). ADMM has
also been generalized for multiple variables and constraints
(Giesen & Laue, 2016; 2019).

– History of iteratively decreasing the feasible set:
Cutting-plane methods remove a part of feasible point at
every iteration where the removed part does not contain the
minimizer. The feasible set gets smaller and smaller until
it converges to the solution. The most well-known cutting-
plane method is the Analytic Center Cutting-Plane Method
(ACCPM) (Gofﬁn & Vial, 1993; Nesterov, 1995; Atkinson
& Vaidya, 1995). Ellipsoid method (Shor, 1977; Yudin &
Nemirovski, 1976; 1977a;b) has a similar idea but it re-
moves half of an ellipsoid around the current solution at
every iteration. The ellipsoid method was initially applied
to liner programming (Khachiyan, 1979).

– History of other optimization approaches: There ex-
ist some other approaches for optimization. In this paper,
for brevity, we do not explain the theory of these other ap-
proaches and we merely focus on the classical optimiza-
tion. Riemannian optimization (Absil et al., 2009; Boumal,
2020) is the extension of Euclidean optimization to the
cases where the optimization variable lies on a possibly
curvy Riemannian manifold (Hosseini & Sra, 2020b; Hu
et al., 2020) such as the symmetric positive deﬁnite (Sra
& Hosseini, 2015), quotient (Lee, 2013), Grassmann (Ben-
dokat et al., 2020), and Stiefel (Edelman et al., 1998) man-
ifolds.
Metaheuristic optimization (Talbi, 2009), in the ﬁeld of
soft computing, is a a family of methods ﬁnding the op-
timum of a cost function using efﬁcient, and not brute-
force, search. They use both local and global searches for
exploitation and exploration of the cost function, respec-
tively. They can be used in highly non-convex optimiza-
tion with many constraints, where classical optimization is
a little difﬁcult and slow to perform. These methods con-
tain nature-inspired optimization (Yang, 2010), evolution-
ary computing (Simon, 2013), and particle-based optimiza-
tion. Two fundamental metaheuristic methods are genetic

algorithm (Holland et al., 1992) and particle swarm opti-
mization (Kennedy & Eberhart, 1995).

– Important books on optimization: Some important
books on optimization are Boyd’s book (Boyd & Vanden-
berghe, 2004), Nocedal’s book (Nocedal & Wright, 2006),
Nesterov’s books (Nesterov, 1998; 2003; 2018) (The book
(Nesterov, 2003) is a good book on ﬁrst-order methods),
Beck’s book (Beck, 2017), and some other books (Den-
nis Jr & Schnabel, 1996; Avriel, 2003; Chong & Zak, 2004;
Bubeck, 2014; Jain & Kar, 2017), etc.
In this paper, we introduce and explain these optimization
methods and approaches.

Required Background for the Reader
This paper assumes that the reader has general knowledge
of calculus and linear algebra.

2. Notations and Preliminaries
2.1. Preliminaries on Sets and Norms
Deﬁnition 1 (Interior and boundary of set). Consider a set
D in a metric space Rd. The point x ∈ D is an interior
point of the set if:

∃(cid:15) > 0 such that {y | (cid:107)y − x(cid:107)2 ≤ (cid:15)} ⊆ D.

The interior of the set, denoted by int(D), is the set con-
taining all the interior points of the set. The closure of the
set is deﬁned as cl(D) := Rd \ int(Rd \ D). The bound-
ary of set is deﬁned as bd(D) := cl(D) \ int(D). An open
(resp. closed) set does not (resp. does) contain its bound-
ary. The closure of set can be deﬁned as the smallest closed
set containing the set. In other words, the closure of set is
the union of interior and boundary of the set.

Deﬁnition 2 (Convex set and convex hull). A set D is a
convex set if it completely contains the line segment be-
tween any two points in the set D:

∀x, y ∈ D, 0 ≤ t ≤ 1 =⇒ tx + (1 − t)y ∈ D.

The convex hull of a (not necessarily convex) set D is the
smallest convex set containing the set D. If a set is convex,
it is equal to its convex hull.

Deﬁnition 3 (Minimum, maximum, inﬁmum, and supre-
mum). A minimum and maximum of a function f : Rd →
R, f : x (cid:55)→ f (x), with domain D, are deﬁned as:

min
x
max
x

f (x) ≤ f (y), ∀y ∈ D,

f (x) ≥ f (y), ∀y ∈ D,

7

Figure 1. Minimum, maximum, inﬁmum, and supremum of ex-
ample functions.

Figure 2. Examples for stationary points such as local and global
extreme points, strict and non-strict extreme points, and saddle
point.

are the lower-bound and upper-bound of function, respec-
tively:

f (x) := max{z ∈ R | z ≤ f (x), ∀x ∈ D},

f (x) := min{z ∈ R | z ≥ f (x), ∀x ∈ D}.

inf
x
sup
x

Depending on the function, the inﬁmum and supremum of
a function may or may not belong to the range of function.
Fig. 1 shows some examples for minimum, maximum, in-
ﬁmum, and supremum. The minimum and maximum of a
function are also the inﬁmum and supremum of function,
respectively, but the converse is not necessarily true. If the
minimum and maximum of function are minimum and max-
imum in the entire domain of function, they are the global
minimum and global maximum, respectively. See Fig. 2 for
examples of global minimum and maximum.

Lemma 1 (Inner product). Consider two vectors x =
[x1, . . . , xd](cid:62) ∈ Rd and y = [y1, . . . , yd](cid:62) ∈ Rd. Their
inner product, also called dot product, is:

(cid:104)x, y(cid:105) = x(cid:62)y =

d
(cid:88)

i=1

xi yi.

respectively. The minimum and maximum of a function
Inﬁmum and supremum
belong to the range of function.

We also have inner product between matrices X, Y ∈
Rd1×d2 . Let X ij denote the (i, j)-th element of matrix X.

The inner product of X and Y is:

(cid:104)X, Y (cid:105) = tr(X (cid:62)Y ) =

d1(cid:88)

d2(cid:88)

i=1

j=1

X i,j Y i,j,

where tr(.) denotes the trace of matrix.
Deﬁnition 4 (Norm). A function (cid:107) · (cid:107) : Rd → R, (cid:107) · (cid:107) :
x (cid:55)→ (cid:107)x(cid:107) is a norm if it satisﬁes:

1. (cid:107)x(cid:107) ≥ 0, ∀x

2. (cid:107)ax(cid:107) = |a| (cid:107)x(cid:107), ∀x and all scalars a

3. (cid:107)x(cid:107) = 0 if and only if x = 0

4. Triangle inequality: (cid:107)x + y(cid:107) ≤ (cid:107)x(cid:107) + (cid:107)y(cid:107).

Deﬁnition 5 (Important norms). Some important norms for
a vector x = [x1, . . . , xd](cid:62) are as follows. The (cid:96)p norm is:
(cid:107)x(cid:107)p := (cid:0)|x1|p + · · · + |xd|p(cid:1)1/p

,

where p ≥ 1 and |.| denotes the absolute value. Two well-
known (cid:96)p norms are (cid:96)1 norm and (cid:96)2 norm (also called the
Euclidean norm) with p = 1 and p = 2, respectively. The
(cid:96)∞ norm, also called the inﬁnity norm, the maximum norm,
or the Chebyshev norm, is:

8

Figure 3. The unit balls, in R2, for (a) (cid:96)1 norm, (b) (cid:96)2 norm, and
(c) (cid:96)∞ norm.

where X ij denotes the (i, j)-th element of X.
The (cid:96)2,1 norm of matrix X is:

(cid:107)X(cid:107)2,1 :=

d1(cid:88)

(cid:118)
(cid:117)
(cid:117)
(cid:116)

d2(cid:88)

i=1

j=1

X 2

i,j.

The Schatten (cid:96)p norm of matrix X is:

(cid:107)X(cid:107)p :=

(cid:18) min(d1,d2)
(cid:88)

(cid:0)σi(X)(cid:1)p(cid:19)1/p

,

i=1

(cid:107)x(cid:107)∞ := max{|x1| + · · · + |xd|}.

For the matrix X ∈ Rd1×d2, the (cid:96)p norm is:

where σi(X) denotes the i-th singular value of X. A spe-
cial case of the Schatten norm, with p = 1, is called the
nuclear norm or the trace norm (Fan, 1951):

(cid:107)X(cid:107)p := sup
y(cid:54)=0

(cid:107)Xy(cid:107)p
(cid:107)y(cid:107)p

.

A special case for this is the (cid:96)2 norm, also called the spec-
tral norm or the Euclidean norm. The spectral norm is
related to the largest singular value of matrix:

(cid:107)X(cid:107)2 = sup
y(cid:54)=0

(cid:107)Xy(cid:107)2
(cid:107)y(cid:107)2

(cid:113)

=

λmax(X (cid:62)X) = σmax(X),

where λmax(X (cid:62)X) and σmax(X) denote the largest eigen-
value of X (cid:62)X and the largest singular value of X,
respectively. Other specaial cases are the maximum-
absolute-column-sum norm (p = 1) and the maximum-
absolute-row-sum norm (p = ∞):

(cid:107)X(cid:107)1 = sup
y(cid:54)=0

(cid:107)Xy(cid:107)1
(cid:107)y(cid:107)1

= max
1≤j≤d2

d1(cid:88)

i=1

|X i,j|,

(cid:107)X(cid:107)∞ = sup
y(cid:54)=0

(cid:107)Xy(cid:107)∞
(cid:107)y(cid:107)∞

= max
1≤i≤d1

d2(cid:88)

j=1

|X i,j|.

The formulation of the Frobenius norm for a matrix is sim-
ilar to the formulation of (cid:96)2 norm for a vector:

(cid:107)X(cid:107)F :=

(cid:118)
(cid:117)
(cid:117)
(cid:116)

d1(cid:88)

d2(cid:88)

i=1

j=1

X 2

i,j,

(cid:107)X(cid:107)∗ :=

min(d1,d2)
(cid:88)

i=1

σi(X) = tr

(cid:16)(cid:112)

X (cid:62)X

(cid:17)

,

which is summation of the singular values of matrix. Note
that similar to use of (cid:96)1 norm of vector for sparsity, the
nuclear norm is also used to impose sparsity on matrix.

Lemma 2. We have:

(cid:107)x(cid:107)2
(cid:107)X(cid:107)2

2 = x(cid:62)x = (cid:104)x, x(cid:105),
F = tr(X (cid:62)X) = (cid:104)X, X(cid:105),

which are convex and in quadratic forms.

Deﬁnition 6 (Unit ball). The unit ball for a norm (cid:107) · (cid:107) is:

B := {x ∈ Rd | (cid:107)x(cid:107) ≤ 1}.

The unit balls for some of the norms are shown in Fig. 3.

Deﬁnition 7 (Dual norm). Let (cid:107).(cid:107) be a norm on Rd. Its
dual norm is:

(cid:107)x(cid:107)∗ := sup{x(cid:62)y | (cid:107)y(cid:107) ≤ 1}.

(1)

Note that the notation (cid:107)·(cid:107)∗ should not be confused with the
the nuclear norm despite of similarity of notations.

Lemma 3 (H¨older’s (H¨older, 1889) and Cauchy-Schwarz
inequalities (Steele, 2004)). Let p, q ∈ [1, ∞] and:

1
p

+

1
q

= 1.

(2)

These p and q are called the H¨older conjugates of each
other. According to the H¨older’s inequality, for functions
f (.) and g(.), we have (cid:107)f g(cid:107)1 ≤ (cid:107)f (cid:107)p(cid:107)g(cid:107)q. A corollary of
the H¨older’s inequality is that Eq. (2) holds if the norms
(cid:107).(cid:107)p and (cid:107).(cid:107)q are dual of each other. H¨older’s inequality
states that:

|x(cid:62)y| ≤ (cid:107)x(cid:107)p(cid:107)x(cid:107)q,

where p and q satisfy Eq. (2). A special case of the H¨older’s
inequality is the Cauchy-Schwarz inequality, stated as
|x(cid:62)y| ≤ (cid:107)x(cid:107)2(cid:107)x(cid:107)2.

According to Eq. (2), we have:

(cid:107) · (cid:107)p =⇒ (cid:107) · (cid:107)∗ = (cid:107) · (cid:107)p/(p−1),

∀p ∈ [1, ∞].

(3)

For example, the dual norm of (cid:107).(cid:107)2 is (cid:107).(cid:107)2 again and the
dual norm of (cid:107).(cid:107)1 is (cid:107).(cid:107)∞.
Deﬁnition 8 (Cone and dual cone). A set K ⊆ Rd is a cone
if:

1. it contains the origin, i.e., 0 ∈ K,

2. K is a convex set,

3. for each x ∈ K and λ ≥ 0, we have λx ∈ K.

The dual cone of a cone K is:

K∗ := {y | y(cid:62)x ≥ 0, ∀x ∈ K}.

An example cone and its dual are depicted in Fig. 4-a.

Deﬁnition 9 (Proper cone (Boyd & Vandenberghe, 2004)).
A convex cone K ⊆ Rd is a proper cone if:

1. K is closed, i.e., it contains its boundary,

2. K is solid, i.e., its interior is non-empty,

3. K is pointed, i.e., it contains no line. In other words,

it is not a two-sided cone around the origin.

Deﬁnition 10 (Generalized inequality (Boyd & Vanden-
berghe, 2004)). A generalized inequality, deﬁned by a
proper cone K, is:

x (cid:23)K y ⇐⇒ x − y ∈ K.

This means x (cid:23)K y ⇐⇒ x − y ∈ int(K). Note that
x (cid:23)K y can also be stated as x − y (cid:23)K 0. An example for
a generalized inequality is shown in Fig. 4-b.

9

Figure 4. (a) A cone K and its dual cone K∗. Note that the borders
of dual cone are perpendicular to the borders of the cone as shown
in this ﬁgure. (b) An example for the generalized inequality x (cid:23)K
y. As it is shown, the vector (x − y) belongs to the cone K.

Deﬁnition 11 (Important examples for generalized in-
equality). The generalized inequality deﬁned by the non-
negative orthant, K = Rd
+, is the default inequality for
vectors x = [x1, . . . , xd](cid:62), y = [y1, . . . , yd](cid:62):

x (cid:23) y ⇐⇒ x (cid:23)Rd

+

y.

It means component-wise inequality:

x (cid:23) y ⇐⇒ xi ≥ yi,

∀i ∈ {1, . . . , d}.

The generalized inequality deﬁned by the positive deﬁnite
cone, K = Sd
+, is the default inequality for symmetric ma-
trices X, Y ∈ Sd:

X (cid:23) Y ⇐⇒ X (cid:23)Sd

+

Y .

It means (X − Y ) is positive semi-deﬁnite. Note that if the
inequality is strict, i.e. X (cid:31) Y , it means that (X − Y ) is
positive deﬁnite. In conclusion, x (cid:23) 0 means all elements
of vector x are non-negative and X (cid:23) 0 means the matrix
X is positive semi-deﬁnite.

2.2. Preliminaries on Functions
Deﬁnition 12 (Fixed point). A ﬁxed point of a function f (.)
is a point x which is mapped to itself by the function, i.e.,
f (x) = x.

10

(4)
Figure 5. Two deﬁnitions for the convex function: (a) Eq.
meaning that the function value for the point αx + (1 − α)y
is less than or equal to the hyper-line αf (x) + (1 − α)f (y), and
(b) Eq. (5) meaning that the function value for x, i.e. f (x), falls
above the hyper-line f (y) + ∇f (y)(cid:62)(x − y), ∀x, y ∈ D.

Deﬁnition 13 (Convex function). A function f (.) with do-
main D is convex if:

f (cid:0)αx + (1 − α)y(cid:1) ≤ αf (x) + (1 − α)f (y),

(4)

∀x, y ∈ D, where α ∈ [0, 1]. Eq. (4) is depicted in Fig.
5-a.
Moreover, if the function f (.) is differentiable, it is convex
if:

f (x) ≥ f (y) + ∇f (y)(cid:62)(x − y),

(5)

∀x, y ∈ D. Eq. (5) is depicted in Fig. 5-b.
Moreover, if the function f (.) is twice differentiable, it
is convex if its second-order derivative is positive semi-
deﬁnite:

∇2f (x) (cid:23) 0,

(6)

∀x ∈ D.

Each of the Eqs.
(4), (5), and (6) is a deﬁnition for the
convex function. Note that if ≥ is changed to ≤ in Eqs. (4)
and (5) or if (cid:23) is changed to (cid:22) in Eq. (6), the function is
concave.

Deﬁnition 14 (Strongly convex function). A differential
function f (.) with domain D is µ-strongly convex if:

f (x) ≥ f (y) + ∇f (y)(cid:62)(x − y) +

µ
2

(cid:107)x − y(cid:107)2
2,

(7)

∀x, y ∈ D and µ > 0.
Moreover, if the function f (.) is twice differentiable, it is
µ-strongly convex if its second-order derivative is positive
semi-deﬁnite:

Figure 6. Comparison of strongly convex and convex functions.
The strongly convex function has only one strict minimizer while
the convex function can have multiple minimizers with equal
function values.

Deﬁnition 15 (H¨older and Lipschitz smoothness). A func-
tion f (.) with domain D belongs to a H¨older space
H(α, L), with smoothness parameter α and the radius L
for ball (as the space can be seen as a ball), if:

|f (x) − f (y)| ≤ L (cid:107)x − y(cid:107)α
2 ,

∀x, y ∈ D.

(9)

The H¨older space relates to local smoothness. A function in
this space is called H¨older smooth (or H¨older continuous).
A function is Lipschitz smooth (or Lipschitz continuous) if
it is H¨older smooth with α = 1:

|f (x) − f (y)| ≤ L (cid:107)x − y(cid:107)2,

∀x, y ∈ D.

(10)

The parameter L is called the Lipschitz constant. A func-
tion with Lipschitz smoothness (with Lipschitz constant L)
is called L-smooth.

H¨older and Lipschitz smoothness are used in many con-
vergence and correctness proofs for optimization (e.g., see
(Liu et al., 2021)).
The following lemma, which is based on the fundamental
theorem of calculus, is widely used in proofs of optimiza-
tion methods.

Lemma 4 (Fundamental theorem of calculus for multivari-
ate functions). Consider a differentiable function f (.) with
domain D. For any x, y ∈ D, we have:

f (y) = f (x) + ∇f (x)(cid:62)(y − x)

(cid:90) 1

(cid:16)

+

0

∇f (cid:0)x + t(y − x)(cid:1) − ∇f (x)

(cid:17)(cid:62)

(y − x)dt

= f (x) + ∇f (x)(cid:62)(y − x) + o(y − x),

where o(.) is the small-o complexity.

Lemma 5 (Corollary of the fundamental theorem of calcu-
lus). Consider a differentiable function f (.), with domain
D, whose gradient is L-smooth:

(11)

y(cid:62)∇2f (x)y ≥ µ(cid:107)y(cid:107)2
2,

(8)

|∇f (x) − ∇f (y)| ≤ L (cid:107)x − y(cid:107)2,

∀x, y ∈ D.

(12)

∀x, y ∈ D and µ > 0. A strongly convex function has a
unique minimizer. See Fig. 6 for difference of convex and
strongly convex functions.

For any x, y ∈ D, we have:

f (y) ≤ f (x) + ∇f (x)(cid:62)(y − x) +

L
2

(cid:107)y − x(cid:107)2
2.

(13)

Proof. Proof is available in Appendix A.1.

The following lemma is useful for proofs of convergence
of ﬁrst-order methods.

Lemma 6. Consider a convex and differentiable function
f (.), with domain D, whose gradient is L-smooth (see Eq.
(12)). We have:

f (y) − f (x) ≤ ∇f (y)(cid:62)(y − x)

−

1
2L

(cid:107)∇f (y) − ∇f (x)(cid:107)2
2,

(14)

(cid:0)∇f (y) − ∇f (x)(cid:1)(cid:62)

(y − x) ≥

1
L

(cid:107)∇f (y) − ∇f (x)(cid:107)2
2.
(15)

Proof. Proof is available in Appendix A.2.

2.3. Preliminaries on Optimization
Deﬁnition 16 (Local and global minimizers). A point x ∈
D is a local minimizer of function f (.) if and only if:

∃ (cid:15) > 0 : ∀y ∈ D, (cid:107)y − x(cid:107)2 ≤ (cid:15) =⇒ f (x) ≤ f (y),

(16)

meaning that in an (cid:15)-neighborhood of x, the value of func-
tion is minimum at x. A point x ∈ D is a global minimizer
of function f (.) if and only if:

f (x) ≤ f (y),

∀y ∈ D.

(17)

See Fig. 2 for examples of local minimizer and maximizer.

Deﬁnition 17 (Strict minimizers). In Eqs. (16) and (17), if
we have f (x) < f (y) rather than f (x) ≤ f (y), the min-
imizer is a strict local and global minimizer, respectively.
See Fig. 2 for examples of strict/non-strict minimizer and
maximizer.

Lemma 7 (Minimizer in convex function). In a convex
function, any local minimizer is a global minimizer.

Proof. Proof is available in Appendix A.3.

Corollary 1. In a convex function, there exists only one
local minimizer which is the global minimizer. As an imag-
ination, a convex function is like a multi-dimensional bowl
with only one minimizer.

Lemma 8 (Gradient of a convex function at the minimizer
point). When the function f (.) is convex and differentiable,
a point x∗ is a minimizer if and only if ∇f (x∗) = 0.

Proof. Proof is available in Appendix A.4.

11

through a saddle point, the sign of the second derivative
ﬂips to the opposite sign. Minimizer and maximizer points
(locally or globally) minimize and maximize the function,
respectively. A saddle point is neither minimizer nor maxi-
mizer, although the gradient at a saddle point is zero. Both
minimizer and maximizer are also called the extremum
points. As Fig. 2 shows, some of stationary point can be
either a minimizer, a maximizer, or a saddle point of func-
tion.

Lemma 9 (First-order optimality condition (Nesterov,
2018, Theorem 1.2.1)). If x∗ is a local minimizer for a
differentiable function f (.), then:

∇f (x∗) = 0.

(18)

Note that if f (.) is convex, this equation is a necessary and
sufﬁcient condition for a minimizer.

Proof. Proof is available in Appendix A.5.

Note that if setting the derivative to zero, i.e. Eq.
(18),
gives a closed-form solution for x∗, the optimization is
done. Otherwise, one should start with some random ini-
tialized solution and iteratively update it using the gradient.
First-order or second-order methods can be used for itera-
tive optimization (see Sections 5 and 7).

Deﬁnition 19 (Arguments of minimization and maximiza-
tion). In the domain of function, the point which mini-
mizes (resp. maximizes) the function f (.) is the argu-
ment for the minimization (resp. maximization) of function.
The minimizer and maximizer of function are denoted by
arg minx f (x) and arg maxx f (x), respectively.
Remark 1. We can convert convert maximization to mini-
mization and vice versa:

maximize
x
minimize
x

f (x) = − minimize

x

f (x) = − maximize

x

(cid:0)−f (x)(cid:1),
(cid:0)−f (x)(cid:1).

(19)

We can have similar conversions for the arguments of maxi-
mization and minimization but we the sign of optimal value
of function is not important in argument, we do not have
the negative sign before maximization and minimization:

arg max

x
arg min

x

f (x) = arg min

x

f (x) = arg max

x

(cid:0)−f (x)(cid:1),
(cid:0)−f (x)(cid:1).

(20)

If any sequence
Deﬁnition 20 (Convergence rates).
{(cid:15)0, (cid:15)1, . . . , (cid:15)k, (cid:15)k+1, . . . } converges, its convergence rate
has one of the following cases:

Deﬁnition 18 (Stationary, extremum, and saddle points).
In a general (not-necessarily-convex) function f (.), a point
x∗ is a stationary if and only if ∇f (x∗) = 0. By passing

lim
k→∞

(cid:15)k+1
(cid:15)k

=






0
∈ (0, 1)
1

superlinear rate,
linear rate,
sublinear rate.

(21)

2.4. Preliminaries on Derivative
Remark 2 (Dimensionality of derivative). Consider a
function f : Rd1 → Rd2 , f : x (cid:55)→ f (x). Derivative
of function f (x) ∈ Rd2 with respect to (w.r.t.) x ∈ Rd1
has dimensionality (d1 × d2). This is because tweaking
every element of x ∈ Rd1 can change every element of
f (x) ∈ Rd2. The (i, j)-th element of the (d1 × d2)-
dimensional derivative states the amount of change in the
j-th element of f (x) resulted by changing the i-th element
of x.
Note that one can use a transpose of the derivative as the
derivative. This is okay as long as the dimensionality of
other terms in equations of optimization coincide (i.e., they
In that case, the dimensionality of
are all transposed).
derivative is (d2 ×d1) where the (i, j)-th element of deriva-
tive states the amount of change in the i-th element of f (x)
resulted by changing the j-th element of x.
Some examples of derivatives are as follows.

• If the function is f : R → R, f : x (cid:55)→ f (x),
the derivative (∂f (x)/∂x) ∈ R is a scalar because
changing the scalar x can change the scalar f (x).
• If the function is f : Rd → R, f : x (cid:55)→ f (x),
the derivative (∂f (x)/∂x) ∈ Rd is a vector because
changing every element of the vector x can change the
scalar f (x).

• If the function is f : Rd1×d2 → R, f : X (cid:55)→ f (X),
the derivative (∂f (X)/∂X) ∈ Rd1×d2 is a matrix
because changing every element of the matrix X can
change the scalar f (X).

• If the function is f : Rd1 → Rd2, f : x (cid:55)→ f (x), the
derivative (∂f (x)/∂x) ∈ Rd1×d2 is a matrix because
changing every element of the vector x can change
every element of the vector f (x).

• If the function is f : Rd1×d2 → Rd3 , f : X (cid:55)→ f (X),
the derivative (∂f (X)/∂X) is a (d1 × d2 × d3)-
dimensional tensor because changing every element
of the matrix X can change every element of the vec-
tor f (X).

• If the function is f : Rd1×d2 → Rd3×d4, f : X (cid:55)→
f (X), the derivative (∂f (X)/∂X) is a (d1 × d2 ×
d3 × d4)-dimensional tensor because changing every
element of the matrix X can change every element of
the matrix f (X).

In other words, the derivative of a scalar w.r.t. a scalar
is a scalar. The derivative of a scalar w.r.t. a vector is a
vector. The derivative of a scalar w.r.t. a matrix is a matrix.
The derivative of a vector w.r.t. a vector is a matrix. The
derivative of a vector w.r.t. a matrix is a rank-3 tensor. The
derivative of a matrix w.r.t. a matrix is a rank-4 tensor.
Deﬁnition 21 (Gradient, Jacobian, and Hessian). Consider
a function f : Rd → R, f : x (cid:55)→ f (x). In optimizing the

12

function f , the derivative of function w.r.t. its variable x is
called the gradient, denoted by:

∇f (x) :=

∂f (x)
∂x

∈ Rd.

The second derivative of function w.r.t. to its derivative is
called the Hessian matrix, denoted by

B = ∇2f (x) :=

∂2f (x)
∂x2 ∈ Rd×d.

The Hessian matrix is symmetric. If the function is convex,
its Hessian matrix is positive semi-deﬁnite.
If the function is multi-dimensional, i.e., f : Rd1 → Rd2 ,
f : x (cid:55)→ f (x), the gradient becomes a matrix:

J :=

(cid:104) ∂f
∂x1

, . . . ,

(cid:105)(cid:62)

=

∂f
∂xd1







∂f1
∂x1

...

∂f1
∂xd1

. . .
. . .
. . .

∂fd2
∂xd1
...
∂fd2
∂xd1



∈ Rd1×d2,





where x = [x1, . . . , xd1 ](cid:62) and f (x) = [f1, . . . , fd2](cid:62).
This matrix derivative is called the Jacobian matrix.
Corollary 2 (Technique for calculating derivative). Ac-
cording to the size of derivative, we can easily calculate
the derivatives. For ﬁnding the correct derivative for mul-
tiplications of matrices (or vectors), one can temporarily
assume some dimensionality for every matrix and ﬁnd the
correct of matrices in the derivative. Let X ∈ Ra×b, An
example for calculating derivative is:

Ra×b (cid:51)

∂
∂X

(cid:0)tr(AXB)(cid:1) = A(cid:62)B(cid:62) = (BA)(cid:62).

(22)

This is calculated as explained in the following. We as-
sume A ∈ Rc×a and B ∈ Rb×c so that we can have the
matrix multiplication AXB and its size is AXB ∈ Rc×c
because the argument of trace should be a square matrix.
The derivative ∂(tr(AXB))/∂X has size Ra×b because
tr(AXB) is a scalar and X is (a × b)-dimensional. We
know that the derivative should be a kind of multiplication
of A and B because tr(AXB) is linear w.r.t. X. Now, we
should ﬁnd their order in multiplication. Based on the as-
sumed sizes of A and B, we see that A(cid:62)B(cid:62) is the desired
size and these matrices can be multiplied to each other.
Hence, this is the correct derivative.
Lemma 10 (Derivative of matrix w.r.t. matrix). As ex-
plained in Remark 2, the derivative of a matrix w.r.t. an-
other matrix is a tensor. Working with tensors is difﬁcult;
hence, we can use Kronecker product for representing ten-
sor as matrix. This is the Magnus-Neudecker convention
(Magnus & Neudecker, 1985) in which all matrices are
vectorized. For example, if X ∈ Ra×b, A ∈ Rc×a, and
B ∈ Rb×d, we have:

R(cd)×(ab) (cid:51)

∂
∂X

(AXB) = B(cid:62) ⊗ A,

(23)

where ⊗ denotes the Kronecker product.

Remark 3 (Chain rule in matrix derivatives). When hav-
ing composite functions (i.e., function of function), we use
chain rule for derivative. When we have derivative of ma-
trix w.r.t. matrix, this chain rule can get difﬁcult but we
can do it by checking compatibility of dimensions in matrix
multiplications. We should use Lemma 10 and vectoriza-
tion technique in which the matrix is vectorized. Let vec(.)
denote vectorization of a Ra×b matrix to a Rab vector. Also,
a×b(.) be de-vectorization of a Rab vector to a Ra×b
let vec−1
matrix.
For the purpose of tutorial, here we calculate derivative by
chain rule as an example:

f (S) = tr(ASB), S = C (cid:99)M D, (cid:99)M =

M
(cid:107)M (cid:107)2
F

,

where A ∈ Rc×a, S ∈ Ra×b, B ∈ Rb×c, C ∈ Ra×d,
(cid:99)M ∈ Rd×d, D ∈ Rd×b, and M ∈ Rd×d. We have:

Ra×b (cid:51)

Rab×d2

Rd2×d2

(cid:51)

∂f (S)
∂S
∂S
∂ (cid:99)M
∂ (cid:99)M
∂M

(cid:51)

(22)
= (BA)(cid:62).

(23)
= D(cid:62) ⊗ C,

(a)
=

=

1
(cid:107)M (cid:107)4
F
1
(cid:107)M (cid:107)2
F

(cid:0)(cid:107)M (cid:107)2

F I d2 − 2M ⊗ M (cid:1)

(cid:0)I d2 −

2
(cid:107)M (cid:107)2
F

M ⊗ M (cid:1),

where (a) is because of the formula for the derivative of
fraction and I d2 is a (d2 ×d2)-dimensional identity matrix.
ﬁnally, by chain rule, we have:

Rd×d (cid:51)

∂f
M

= vec−1
d×d

(cid:16)(cid:0) ∂ (cid:99)M
∂M

(cid:1)(cid:62)(cid:0) ∂S
∂ (cid:99)M

(cid:1)(cid:62)

vec(cid:0) ∂f (S)
∂S

(cid:1)(cid:17)

.

Note that the chain rule in matrix derivatives usually is
stated right to left in matrix multiplications while transpose
is used for matrices in multiplication.

More formulas for matrix derivatives can be found in the
matrix cookbook (Petersen & Pedersen, 2012) and similar
resources. Here, we discussed only real derivatives. When
working with complex data (with imaginary part), we need
complex derivative. The reader can refer to (Hjorungnes
& Gesbert, 2007) and (Chong, 2021, Chapter 7, Complex
Derivatives) for techniques in complex derivatives.

3. Optimization Problems
3.1. Standard Problems
Here, we review the standard forms for convex optimiza-
tion and we explain why these forms are important. Note
that the term “programming” refers to solving optimization
problems.

13

3.1.1. GENERAL OPTIMIZATION PROBLEM
Consider the function f : Rd → R, f : x (cid:55)→ f (x). Let the
domain of function be D where x ∈ D, x ∈ Rd.
Consider the following unconstrained minimization of a
cost function f (.):

minimize
x

f (x),

(24)

where x is called the optimization variable and the function
f (.) is called the objective function or the cost function.
This is an unconstrained problem where the optimization
variable x needs only be in the domain of function, i.e.,
x ∈ D, while minimizing the function f (.).
The optimization problem can be constrained where the op-
timization variable x should satisfy some equality and/or
inequality constraints, in addition to being in the domain of
function, while minimizing the function f (.). Consider a
constrained optimization problem where we want to mini-
mize the function f (x) while satisfying m1 inequality con-
straints and m2 equality constraint:

minimize
x

f (x)

subject to yi(x) ≤ 0, i ∈ {1, . . . , m1},
hi(x) = 0, i ∈ {1, . . . , m2},

(25)

where f (x) is the objective function, every yi(x) ≤ 0 is an
inequality constraint, and every hi(x) = 0 is an equality
constraint. Note that if some of the inequality constraints
are not in the form yi(x) ≤ 0, we can restate them as:

yi(x) ≥ 0 =⇒ −yi(x) ≤ 0,
yi(x) ≤ c =⇒ yi(x) − c ≤ 0.

Therefore, all inequality constraints can be written in the
form yi(x) ≤ 0. Furthermore, according to Eq. (19), if
the optimization problem (25) is a maximization problem
rather than minimization, we can convert it to maximiza-
tion by multiplying its objective function to −1:

maximize
x

f (x)

subject to constraints

≡

minimize
x

− f (x)

subject to constraints

(26)

Deﬁnition 22 (Feasible point). The point x for the opti-
mization problem (25) is feasible if:

x ∈ D, and
yi(x) ≤ 0,
hi(x) = 0,

∀i ∈ {1, . . . , m1}, and
∀i ∈ {1, . . . , m2}.

(27)

The constrained optimization problem can also be stated
as:

minimize
x

f (x)

(28)

subject to x ∈ S,

where S is the feasible set of constraints.

3.1.2. CONVEX OPTIMIZATION PROBLEM
A convex optimization problem is of the form:

minimize
x

f (x)

subject to yi(x) ≤ 0, i ∈ {1, . . . , m1},

(29)

Ax = b,

where the functions f (.) and yi(.), ∀i are all convex func-
tions and the equality constraints are afﬁne functions. The
feasible set of a convex problem is a convex set.

3.1.3. LINEAR PROGRAMMING
A linear programming problem is of the form:

minimize
x

c(cid:62)x + d

subject to Gx (cid:22) h,
Ax = b,

(30)

where the objective function and equality constraints are
afﬁne functions. The feasible set of a linear program-
ming problem is a a polyhedron set while the cost is pla-
nar (afﬁne). A survey on linear programming methods is
available in the book (Dantzig, 1963). One of the well-
known methods for solving linear programming is the sim-
plex method, initially appeared in 1947 (Dantzig, 1983).
Simplex method moves between the vertices of a simplex,
until convergence, for minimizing the objective function. It
is efﬁcient and its proposal was a breakthrough in the ﬁeld
of optimization.

3.1.4. QUADRATIC PROGRAMMING
A quadratic programming problem is of the form:

minimize
x

(1/2)x(cid:62)P x + q(cid:62)x + r

subject to Gx (cid:22) h,
Ax = b,

(31)

where P (cid:31) 0 (which is the second derivative of objective
function) is a symmetric positive deﬁnite matrix, the ob-
jective function is quadratic, and equality constraints are
afﬁne functions. The feasible set of a quadratic program-
ming problem is a a polyhedron set while the cost is curvy
(quadratic).

3.1.5. QUADRATICALLY CONSTRAINED QUADRATIC

PROGRAMMING (QCQP)
A QCQP problem is of the form:

(1/2)x(cid:62)P x + q(cid:62)x + r

minimize
x
subject to

(1/2)x(cid:62)M ix + s(cid:62)
Ax = b,

i x + zi ≤ 0, i ∈ {1, . . . , m1},

(32)

14

where P , M i (cid:31) 0, ∀i, the objective function and the in-
equality constraints are quadratic, and equality constraints
are afﬁne functions. The feasible set of a QCQP problem
is intersection of m1 ellipsoids and an afﬁne set, while the
cost is curvy (quadratic).

3.1.6. SECOND-ORDER CONE PROGRAMMING (SOCP)
A SOCP problem is of the form:

minimize
x

f (cid:62)x

subject to (cid:107)Aix + bi(cid:107)2 ≤ c(cid:62)
F x = g,

i x + di, i ∈ {1, . . . , m1},

(33)
where the inequality constraints are norm of an afﬁne func-
tion being less than an afﬁne function. The constraint
(cid:107)Aix + bi(cid:107)2 − c(cid:62)
i x − di ≤ 0 is called the second-order
cone whose shape is like an ice-cream cone.

3.1.7. SEMIDEFINITE PROGRAMMING (SDP)
A SDP problem is of the form:

minimize
X

tr(CX)

subject to X (cid:23) 0,

tr(DiX) ≤ ei,
tr(AiX) = bi,

i ∈ {1, . . . , m1},
i ∈ {1, . . . , m2},

(34)

where the optimization variable X belongs to the positive
semideﬁnite cone Sd
+, tr(.) denotes the trace of matrix,
C, Di, Ai ∈ Sd, ∀i, and Sd denotes the cone of (d × d)
symmetric matrices. The trace terms may be written in
summation forms. Note that tr(C(cid:62)X) is the inner product
of two matrices C and X and if the matrix C is symmetric,
this inner product is equal to tr(CX).
Another form for SDP is:

minimize
x

subject to

c(cid:62)x

(cid:16) d
(cid:88)

(cid:17)

xiF i

+ G (cid:22) 0,

(35)

i=1
Ax = b,

where x = [x1, . . . , xd](cid:62), G, F i ∈ Sd, ∀i, and A, b, and c
are constant matrices/vectors.

3.1.8. OPTIMIZATION TOOLBOXES
All the standard optimization forms can be restated as SDP
because their constraints can be written as belonging to
some cones (see Deﬁnitions 10 and 11); hence, they are
special cases of SDP. The interior-point method, or the
barrier method, introduced in Section 7.4, can be used
for solving various optimization problems including SDP
(Nesterov & Nemirovskii, 1994; Boyd & Vandenberghe,
2004). Optimization toolboxes such as CVX (Grant et al.,

2009) often use interior-point method (see Section 7.4) for
solving optimization problems such as SDP. Note that the
interior-point method is iterative and solving SDP usually
is time consuming especially for large matrices. If the op-
timization problem is a convex optimization problem (e.g.
SDP is a convex problem), it has only one local optima
which is the global optima (see Corollary 1).

3.2. Eliminating Constraints and Equivalent Problems
Here, we review some of the useful techniques in convert-
ing optimization problems to their equivalent forms.

3.2.1. ELIMINATING INEQUALITY CONSTRAINTS
As was discussed in Section 7.4, we can eliminate the
inequality constraints by embedding the inequality con-
straints into the objective function using the indicator or
barrier functions.

3.2.2. ELIMINATING EQUALITY CONSTRAINTS
Consider the optimization problem (55). We can eliminate
the equality constraints, Ax = b, as explained in the fol-
lowing. Let A ∈ Rm2×d, m2 < d, N (A) := {x ∈
Rd | Ax = 0} denote the null-space of matrix A. We have:
∀z ∈ N (A), ∃u ∈ Rd−m2, C ∈ Rm2×(d−m2) :
Col(C) = N (A),

z = Cu,

15

by change of variables.

3.2.4. ELIMINATING SET CONSTRAINTS
As was discussed in Section 5.6.1, we can convert problem
(28) to problem (151) by using the indicator function. That
problem can be solved iteratively where at every iteration,
the solution is updated (by ﬁrst- or second-order methods)
without the set constraint and then the updated solution of
iteration is projected onto the set. This procedure is re-
peated until convergence.

3.2.5. ADDING SLACK VARIABLES
Consider the following problem with inequality con-
straints:

minimize
x

f (x)

subject to yi(x) ≤ 0, i ∈ {1, . . . , m1}.

(40)

Using the so-called slack variables, denoted by {ξi ∈
R+}m1
i=1, we can convert this problem to the following
problem:

minimize
x,{ξi}m1
i=1

f (x)

subject to yi(x) + ξi = 0, i ∈ {1, . . . , m1},
i ∈ {1, . . . , m1}.

ξi ≥ 0,

(41)

where Col(.) is the column-space or range of matrix.
Therefore, we can say:

The slack variables should be non-negative because the in-
equality constraints are less than or equal to zero.

∀z ∈ N (A) : A(x − z) = Ax − Az = Ax − 0 = Ax

=⇒ A(x − z) = Ax = b
=⇒ x = A†b + z = A†b + Cu,

(36)

where A† := A(cid:62)(AA(cid:62))−1 is the pseudo-inverse of ma-
trix A. Putting Eq. (36) in problem (55) changes the op-
timization variable and eliminates the equality constraint:

minimize
u

f (Cu + A†b)

subject to yi(Cu + A†b) ≤ 0, i ∈ {1, . . . , m1}.

(37)

If u∗ is the solution to this problem, the solution to problem
(55) is x∗ = Cu∗ + A†b.

3.2.3. ADDING EQUALITY CONSTRAINTS
Conversely, we can convert the problem:

minimize
{xi}m1
i=0

f (Ax0 + b0)

subject to yi(Axi + bi) ≤ 0, i ∈ {1, . . . , m1},

to:

minimize
{ui,xi}m1
i=0

subject to

f (u0)

yi(ui) ≤ 0, i ∈ {1, . . . , m1},
ui = Axi + bi, i ∈ {0, 1, . . . , m1},

(38)

(39)

3.2.6. EPIGRAPH FORM
We can convert the optimization problem (25) to its epi-
graph form:

minimize
x,t

t

subject to f (x) − t ≤ 0,

(42)

yi(x) ≤ 0, i ∈ {1, . . . , m1},
hi(x) = 0, i ∈ {1, . . . , m2},

because we can minimize an upper-bound t on the objec-
tive function rather than minimizing the objective function.
Likewise, for a maximization problem, we can maximize a
lower-bound of the objective function rather than maximiz-
ing the objective function. The upper-/lower-bound does
not necessarily need to be t; it can be any upper-/lower-
bound function for the objective function. This is a good
technique because sometimes optimizing an upper-/lower-
bound function is simpler than the objective function itself.

4. Karush-Kuhn-Tucker (KKT) Conditions
Many of the optimization algorithms are reduced to and
can be explained by the Karush-Kuhn-Tucker (KKT) con-
ditions. Therefore, KKT conditions are fundamental re-
quirements for optimization.
In this section, we explain
these conditions.

4.1. The Lagrangian Function
4.1.1. LAGRANGIAN AND DUAL VARIABLES
Deﬁnition 23 (Lagrangian and dual variables). The La-
grangian function for the optimization problem (25) is L :
Rd × Rm1 × Rm2 → R, with domain D × Rm1 × Rm2 ,
deﬁned as:

L(x, λ, ν) := f (x) +

m1(cid:88)

i=1

λiyi(x) +

m2(cid:88)

i=1

νihi(x)

(43)

= f (x) + λ(cid:62)y(x) + ν(cid:62)h(x),

i=1 and {νi}m2

where {λi}m1
i=1 are the Lagrange multipliers,
also called the dual variables, corresponding to inequal-
ity and equality constraints, respectively. Note that λ :=
[λ1, . . . , λm1](cid:62) ∈ Rm1 , ν := [ν1, . . . , νm2](cid:62) ∈ Rm2 ,
y(x) := [y1(x), . . . , ym1(x)](cid:62) ∈ Rm1, and h(x) :=
[h1(x), . . . , hm2(x)](cid:62) ∈ Rm2 . Eq. (43) is also called the
Lagrange relaxation of the optimization problem (25).

4.1.2. SIGN OF TERMS IN LAGRANGIAN
In some papers, the plus sign behind (cid:80)m2
i=1 νihi(x) is re-
placed with the negative sign. As hi(x) is for equality con-
straint, its sign is not important in the Lagrangian function.
However, the sign of the term (cid:80)m1
i=1 λiyi(x) is important
because the sign of inequality constraint is important. We
will discuss the sign of {λi}m1
i=1 later. Moreover, according
to Eq. (26), if the problem (50) is a maximization prob-
lem rather than minimization, the Lagrangian function is
L(x, λ, ν) = −f (x) + (cid:80)m1
i=1 νihi(x) in-
stead of Eq. (43).

i=1 λiyi(x) + (cid:80)m2

4.1.3. INTERPRETATION OF LAGRANGIAN
We can interpret Lagrangian using penalty. As Eq. (25)
states, we want to minimize the objective function f (x).
We create a cost function consisting of the objective func-
tion. The optimization problem has constraints so its con-
straints should also be satisﬁed while minimizing the ob-
jective function. Therefore, we penalize the cost function
if the constraints are not satisﬁed. For this, we can add the
constraints to the objective function as the regularization
(or penalty) terms and we minimize the regularized cost.
The dual variables λ and ν can be seen as the regulariza-
tion parameters which weight the penalties compared to the
objective function f (x). This regularized cost function is
the Lagrangian function or the Lagrangian relaxation of the
problem (25). Minimization of the regularized cost func-
tion minimizes the function f (x) while trying to satisfy the
constraints.

4.1.4. LAGRANGE DUAL FUNCTION
Deﬁnition 24 (Lagrange dual function). The Lagrange
dual function (also called the dual function) g : Rm1 ×

16

Rm2 → R is deﬁned as:

g(λ, ν) := inf
x∈D

L(x, λ, ν)

(cid:16)

f (x) +

= inf
x∈D

m1(cid:88)

i=1

λiyi(x) +

m2(cid:88)

i=1

(cid:17)

νihi(x)

.

(44)

Note that the dual function g is a concave function. We
will see later, in Section 4.4, that we maximize this concave
function in a so-called dual problem.

4.2. Primal Feasibility
Deﬁnition 25 (The optimal point and the optimum). The
solution of optimization problem (25) is the optimal point
denoted by x∗. The minimum function from this solution,
i.e., f ∗ := f (x∗), is called the optimum function of prob-
lem (25).

The optimal point x∗ is one of the feasible points which
minimizes function f (.) with constraints in problem (25).
Hence, the optimal point is a feasible point and according
to Eq. (27), we have:

yi(x∗) ≤ 0,
hi(x∗) = 0,

∀i ∈ {1, . . . , m1},
∀i ∈ {1, . . . , m2}.

(45)

(46)

These are called the primal feasibility.
The optimal point x∗ minimizes the Lagrangian function
because Lagrangian is the relaxation of optimization prob-
lem to an unconstrained problem (see Section 4.1.3). On
the other hand, according to Eq. (44), the dual function is
the minimum of Lagrangian w.r.t. x. Hence, we can write
the dual function as:

g(λ, ν)

(44)
= inf
x∈D

L(x, λ, ν) = L(x∗, λ, ν).

(47)

4.3. Dual Feasibility
Lemma 11 (Dual function as a lower bound). If λ (cid:23)
0, then the dual function is a lower bound for f ∗, i.e.,
g(λ, ν) ≤ f ∗.

Proof. Let λ (cid:23) 0 which means λi ≥ 0, ∀i. Consider a
feasible (cid:101)x for problem (25). According to Eq. (27), we
have:

L((cid:101)x, λ, ν)

(43)
= f ((cid:101)x) +

m1(cid:88)

i=1

+

λi
(cid:124)(cid:123)(cid:122)(cid:125)
≥0

yi((cid:101)x)
(cid:124) (cid:123)(cid:122) (cid:125)
≤0

m2(cid:88)

i=1

νi hi((cid:101)x)
(cid:124) (cid:123)(cid:122) (cid:125)
=0

≤ f ((cid:101)x).

Therefore, we have:

(48)

f ((cid:101)x)

(48)
≥ L((cid:101)x, λ, ν) ≥ inf
x∈D

L(x, λ, ν)

(44)
= g(λ, ν).

Hence, the dual function is a lower bound for the function
of all feasible points. As the optimal point x∗ is a feasible
point, the dual function is a lower bound for f ∗. Q.E.D.

Corollary 3 (Nonnegativity of dual variables for inequality
constraints). From Lemma 11, we conclude that for having
the dual function as a lower bound for the optimum func-
tion, the dual variable {λi}m1
i=1 for inequality constraints
(less than or equal to zero) should be non-negative, i.e.:

λ (cid:23) 0

or

λi ≥ 0, ∀i ∈ {1, . . . , m1}.

(49)

Note that if the inequality constraints are greater than
∀i because
or equal to zero, we should have λi ≤ 0,
yi(x) ≥ 0 =⇒ −yi(x) ≤ 0.
In this paper, we as-
sume that the inequality constraints are less than or equal
to zero. If some of the inequality constraints are greater
than or equal to zero, we convert them to less than or equal
to zero by multiplying them to −1.

The inequalities in Eq. (49) are called the dual feasibility.

4.4. The Dual Problem, Weak and Strong Duality, and

Slater’s Condition

According to Eq. (11), the dual function is a lower bound
for the optimum function, i.e., g(λ, ν) ≤ f ∗. We want to
ﬁnd the best lower bound so we maximize g(λ, ν) w.r.t. the
dual variables λ, ν. Moreover, Eq. (49) says that the dual
variables for inequalities must be nonnegative. Hence, we
have the following optimization:

maximize
λ,ν

g(λ, ν)

subject to λ (cid:23) 0.

(50)

The problem (50) is called the Lagrange dual optimization
problem for problem (25). The problem (25) is also re-
ferred to as the primal optimization problem. The variable
of problem (25), i.e. x, is called the primal variable while
the variables of problem (50), i.e. λ and ν, are called the
dual variables. Let the solutions of the dual problem be
denoted by λ∗ and ν∗. We denote g∗ := g(λ∗, ν∗) =
supλ,ν g.
Deﬁnition 26 (Weak and strong duality). For all convex
and nonconvex problems, the optimum dual problem is a
lower bound for the optimum function:

g∗ ≤ f ∗

i.e.,

g(λ∗, ν∗) ≤ f (x∗).

(51)

This is called the weak duality. For some optimization
problems, we have strong duality which is when the opti-
mum dual problem is equal to the optimum function:

g∗ = f ∗

i.e.,

g(λ∗, ν∗) = f (x∗).

(52)

The strong duality usually holds for convex optimization
problems.

17

Figure 7. Illustration of weak duality and strong duality.

Figure 8. Progress of iterative optimization:
(a) gradual mini-
mization of the primal function and maximization of dual func-
tion and (b) the primal optimal and dual optimal reach each other
and become equal if strong duality holds.

Corollary 4. Eqs. (51) and (52) show that the optimum
dual function, g∗, always provides a lower-bound for the
optimum primal function, f ∗.

The primal optimization problem, i.e. Eq. (25), is mini-
mization so its cost function is like a bowl as illustrated in
Fig. 7. The dual optimization problem, i.e. Eq. (50), is
maximization so its cost function is like a reversed bowl
as shown in Fig. 7. The domains for primal and dual prob-
lems are the domain of primal variable x and the domain of
dual variables λ and ν, respectively. As the ﬁgure shows,
the optimal x∗ is corresponded to the optimal λ∗ and ν∗.
As shown in the ﬁgure, there is a possible nonnegative gap
between the two bowls. In the best case, this gap is zero. If
the gap is zero, we have strong duality; otherwise, a weak
duality exists.
If optimization is iterative, the solution is updated itera-

tively until convergence. First-order and second-order nu-
merical optimization, which we will introduce later, are it-
erative. In optimization, the series of primal optimal and
dual optimal converge to the optimal solution and the dual
optimal, respectively. The function values converge to the
local minimum and the dual function values converge to the
optimal (maximum) dual function. Let the superscript (k)
denotes the value of variable at iteration k. We have:

{x(0), x(1), x(2), . . . } → x∗,
{ν(0), ν(1), ν(2), . . . } → ν∗,
{λ(0), λ(1), λ(2), . . . } → λ∗,
f (x(0)) ≥ f (x(1)) ≥ f (x(2)) ≥ · · · ≥ f (x∗),
g(λ(0), ν(0)) ≤ g(λ(1), ν(1)) ≤ · · · ≤ g(λ∗, ν∗).

(53)

Hence, the value of function goes down but the value of
dual function goes up. As Fig. 8 depicts, they reach each
other if strong duality holds; otherwise, there will be a gap
between them after convergence. Note that if the optimiza-
tion problem is a convex problem, the eventually found so-
lution is the global solution; otherwise, the solution is local.

Corollary 5. As every iteration of a numerical optimiza-
tion must satisfy either the weak or strong duality, the op-
timum dual function at every iteration always provides a
lower-bound for the optimum primal function at that itera-
tion:

g(λ(k), ν(k)) ≤ f (x(k)),

∀k.

(54)

Lemma 12 (Slater’s condition (Slater, 1950)). For a con-
vex optimization problem in the form:

minimize
x

f (x)

subject to yi(x) ≤ 0, i ∈ {1, . . . , m1},

(55)

Ax = b,

we have strong duality if it is strictly feasible, i.e.:

∃x ∈ int(D) : yi(x) < 0,

∀i ∈ {1, . . . , m1},

Ax = b.

(56)

In other words, for at least one point in the interior of do-
main (not on the boundary of domain), all the inequality
constraints hold strictly. This is called the Slater’s condi-
tion.

4.5. Complementary Slackness
Assume that the problem has strong duality, the primal op-
timal is x∗ and dual optimal variables are λ∗ and ν∗. We

18

have:

f (x∗)

(52)
= g(λ∗, ν∗)

(cid:16)

(44)
= inf
x∈D

f (x) +

m1(cid:88)

i=1

λ∗
i yi(x) +

(cid:17)

ν∗
i hi(x)

m2(cid:88)

i=1

(a)
= f (x∗) +

(b)
= f (x∗) +

m1(cid:88)

i=1

m1(cid:88)

i=1

i yi(x∗) +
λ∗

m2(cid:88)

i=1

i hi(x∗)
ν∗

i yi(x∗)
λ∗

(c)
≤ f (x∗),

(57)

where (a) is because x∗ is the primal optimal solution for
problem (25) and it minimizes the Lagrangian, (b) is be-
cause x∗ is a feasible point and satisﬁes hi(x∗) = 0 in Eq.
(27), and (c) is because λ∗
i ≥ 0 according to Eq. (49) and
the feasible x∗ satisﬁes yi(x∗) ≤ 0 in Eq. (27) so we have:

i yi(x∗) ≤ 0,
λ∗

∀i ∈ {1, . . . , m1}.

(58)

From Eq. (57), we have:

f (x∗) = f (x∗) +

m1(cid:88)

i=1

i yi(x∗) ≤ f (x∗)
λ∗

=⇒

m1(cid:88)

i=1

i yi(x∗) = 0
λ∗

(58)
=⇒ λ∗

i yi(x∗) = 0, ∀i.

Therefore, the multiplication of every optimal dual variable
λ∗
i with yi(.) of optimal primal solution x∗ must be zero.
This is called the complementary slackness:

i yi(x∗) = 0,
λ∗

∀i ∈ {1, . . . , m1}.

(59)

These conditions can be restated as:

i > 0 =⇒ yi(x∗) = 0,
λ∗
yi(x∗) < 0 =⇒ λ∗
i = 0,

(60)

(61)

which means that, for an inequality constraint, if the dual
optimal is nonzero, its inequality function of the primal op-
timal must be zero. If the inequality function of the primal
optimal is nonzero, its dual optimal must be zero.

4.6. Stationarity Condition
As was explained before, the Lagrangian function can be
interpreted as a regularized cost function to be minimized.
Hence, the constrained optimization problem (25) is con-
verted to minimization of the Lagrangian function, Eq.
(43), which is an unconstrained optimization problem:

minimize
x

L(x, λ, ν).

(62)

Note that this problem is the dual function according to Eq.
(44). As this is an unconstrained problem, its optimization

is easy. We can ﬁnd its minimum by setting its derivative
w.r.t. x, denoted by ∇xL, to zero:

∇xL(x, λ, ν) = 0

(43)
=⇒

∇xf (x) +

m1(cid:88)

i=1

λi∇xyi(x) +

νi∇xhi(x) = 0.

(63)

m2(cid:88)

i=1

This equation is called the stationarity condition because
this shows that the gradient of Lagrangian w.r.t. x should
vanish to zero (n.b. a stationary point of a function is a point
where the derivative of function is zero). This derivative
holds for all dual variables and not just for the optimal dual
variables. We can claim that the gradient of Lagrangian
w.r.t. x should vanish to zero because the dual function,
deﬁned in Eq. (44), should exist.

4.7. KKT Conditions
We derived the primal feasibility, dual feasibility, comple-
mentary slackness, and stationarity condition. These four
conditions are called the Karush-Kuhn-Tucker (KKT) con-
ditions (Karush, 1939; Kuhn & Tucker, 1951). The pri-
mal optimal variable x∗ and the dual optimal variables
λ∗ = [λ∗
](cid:62) must satisfy
the KKT conditions. We summarize the KKT conditions in
the following:

](cid:62), ν∗ = [ν∗

1, . . . , λ∗
m1

1 , . . . , ν∗
m2

1. Stationarity condition:

∇xL(x, λ, ν) =∇xf (x) +

+

m2(cid:88)

i=1

2. Primal feasibility:

m1(cid:88)

i=1

λi∇xyi(x)

(64)

νi∇xhi(x) = 0.

yi(x∗) ≤ 0,
hi(x∗) = 0,

∀i ∈ {1, . . . , m1},
∀i ∈ {1, . . . , m2}.

(65)

(66)

3. Dual feasibility:

19

problem (as it is automatically satisﬁed by dual feasibility):

maximize
λ,ν

g(λ, ν),

(69)

which should give us λ∗, ν∗, and g∗ = g(λ∗, ν∗). This is
an unconstrained optimization problem and for solving it,
we should set the derivative of g(λ, ν) w.r.t. λ and ν to
zero:

∇λg(λ, ν) = 0

∇ν g(λ, ν) = 0

(47)
=⇒ ∇λL(x∗, λ, ν) = 0.
(47)
=⇒ ∇ν L(x∗, λ, ν) = 0.

(70)

(71)

Note that setting the derivatives of Lagrangian w.r.t. dual
variables always gives back the corresponding constraints
in the primal optimization problem. Eqs. (64), (70), and
(71) state that the primal and dual residuals must be zero.
Finally, Eqs.
following max-min optimization problem:

(44) and (69) can be summarized into the

g(λ, ν)

sup
λ,ν

(44)
= sup
λ,ν

inf
x

L(x, λ, ν) = L(x∗, λ∗, ν∗).

(72)

The reason for the name KKT is as follows (Kjeldsen,
2000). In 1952, Kuhn and Tucker published an important
paper proposing the conditions (Kuhn & Tucker, 1951).
However, later it was found out that there is a master’s these
by Karush, in 1939, at the University of Chicago, Illinois
(Karush, 1939). That thesis had also proposed the con-
ditions; however, researchers including Kuhn and Tucker
were not aware of that thesis. Therefore, these conditions
were named after all three of them.

4.8. Solving Optimization by Method of Lagrange

Multipliers

We can solve the optimization problem (25) using dual-
ity and KKT conditions. This technique is also called the
method of Lagrange multipliers. For this, we should do the
following steps:

1. We write the Lagrangian as Eq. (43).

λ (cid:23) 0

or

λi ≥ 0, ∀i ∈ {1, . . . , m1}.

(67)

2. We consider the dual function deﬁned in Eq. (44) and

4. Complementary slackness:

i yi(x∗) = 0,
λ∗

∀i ∈ {1, . . . , m1}.

(68)

As listed above, KKT conditions impose constraints on the
optimal dual variables of inequality constraints because the
sign of inequalities are important.
Recall the dual problem (50). The constraint in this prob-
lem is already satisﬁed by the dual feasibility in the KKT
conditions. Hence, we can ignore the constraint of the dual

we solve it:

x† := arg min
x

L(x, λ, ν).

(73)

It is an unconstrained problem and according to Eqs.
(44) and (64), we solve this problem by taking the
derivative of Lagrangian w.r.t. x and setting it to zero,
i.e., ∇xL(x, λ, ν) set= 0. This gives us the dual func-
tion, according to Eq. (43):

g(λ, ν) = L(x†, λ, ν).

(74)

3. We consider the dual problem, deﬁned in Eq.

(50)
which is simpliﬁed to Eq. (69) because of Eq. (67).
This gives us the optimal dual variables λ∗ and ν∗:

λ∗, ν∗ := arg max
λ,ν

g(λ, ν).

(75)

Until reaching the minimum, we want to decrease the cost
function f (.) in every iteration; hence, we desire:

f (x(k) + ∆x) − f (x(k)) < 0.

(81)

According to Eq. (80), one way to achieve Eq. (81) is:

20

It is an unconstrained problem and according to Eqs.
(70) and (71), we solve this problem by taking the
derivative of dual function w.r.t. λ and ν and setting
them to zero, i.e., ∇λg(λ, ν) set= 0 and ∇ν g(λ, ν) set=
0. The optimum dual value is obtained as:
g(λ, ν) = g(λ∗, ν∗).

(76)

g∗ = max
λ,ν

4. We put the optimal dual variables λ∗ and ν∗ in Eq.

(64) to ﬁnd the optimal primal variable:
x∗ := arg min
x

L(x, λ∗, ν∗).

(77)

It is an unconstrained problem and we solve this prob-
lem by taking the derivative of Lagrangian at opti-
mal dual variables w.r.t. x and setting it to zero, i.e.,
∇xL(x, λ∗, ν∗) set= 0. The optimum primal value is
obtained as:

∇f (x(k))(cid:62)∆x +

L
2

(cid:107)∆x(cid:107)2

2 < 0.

Hence, we should minimize ∇f (x(k))(cid:62)∆x + L
w.r.t. ∆x:

2 (cid:107)∆x(cid:107)2

2

minimize
∆x

∇f (x(k))(cid:62)∆x +

L
2

(cid:107)∆x(cid:107)2
2.

(82)

This function is convex w.r.t. ∆x and we can optimize it
by setting its derivative to zero:

(∇f (x(k))(cid:62)∆x +

∂
∂∆x
set= 0 =⇒ ∆x = −

L
2

(cid:107)∆x(cid:107)2

2) = ∇f (x(k)) + L∆x

1
L

∇f (x(k)).

(83)

Using Eq. (83) in Eq. (80) gives:

f ∗ = min
x

L(x, λ∗, ν∗) = L(x∗, λ∗, ν∗).

(78)

f (x(k) + ∆x) − f (x(k)) ≤ −

1
2L

(cid:107)∇f (x(k))(cid:107)2

2 ≤ 0,

5. First-Order Optimization: Gradient

Methods

5.1. Gradient Descent
Gradient descent is one of the fundamental ﬁrst-order
It was ﬁrst suggested by Cauchy in 1874
methods.
(Lemar´echal, 2012) and Hadamard in 1908 (Hadamard,
1908) and its convergence was later analyzed in (Curry,
1944). In the following, we introduce this method.

5.1.1. STEP OF UPDATE
Consider the unconstrained optimization problem (24).
:= arg minx f (x) and f ∗
Here, we denote x∗
:=
minx f (x) = f (x∗).
In numerical optimization for un-
constrained optimization, we start with a random feasible
initial point and iteratively update it by step ∆x:

x(k+1) := x(k) + ∆x,

(79)

until we converge to (or get sufﬁciently close to) the desired
optimal point x∗. Note that the step ∆x is also denoted by
p in the literature, i.e., p := ∆x. Let the function f (.)
be differentiable and its gradient is L-smooth.
If we set
x = x(k) and y = x(k+1) = x(k) + ∆x in Eq. (13), we
have:

f (x(k) + ∆x) ≤ f (x(k)) + ∇f (x(k))(cid:62)∆x +

L
2

(cid:107)∆x(cid:107)2
2

=⇒ f (x(k) + ∆x) − f (x(k))

(81). Eq.

which satisﬁes Eq.
(83) means that it is bet-
ter to move toward a scale of minus gradient for updating
the solution. This inspires the name of algorithm which is
gradient descent.
The problem is that often we either do not know the Lip-
schitz constant L or it is hard to compute. Hence, rather
than Eq. (83), we use:

∆x = −η∇f (x(k)), i.e., x(k+1) := x(k) − η∇f (x(k)),
(84)

where η > 0 is the step size, also called the learning rate
in data science literature. Note that if the optimization
problem is maximization rather than minimization, the step
should be ∆x = η∇f (x(k)) rather than Eq. (84). In that
case, the name of method is gradient ascent.
Using Eq. (84) in Eq. (80) gives:

f (x(k) + ∆x) − f (x(k))

≤ −η(cid:107)∇f (x(k))(cid:107)2

2 +

L
2

η2(cid:107)∇f (x(k))(cid:107)2
2
(85)

= η(

L
2

η − 1)(cid:107)∇f (x(k))(cid:107)2
2

If x(k) is not a stationary point, we have (cid:107)∇f (x(k))(cid:107)2
Noticing η > 0, for satisfying Eq. (81), we must set:

2 > 0.

≤ ∇f (x(k))(cid:62)∆x +

L
2

(cid:107)∆x(cid:107)2
2.

(80)

L
2

η − 1 < 0 =⇒ η <

2
L

.

(86)

(cid:107)∇f (x(k))(cid:107)2
2

Algorithm 1: Gradient descent with line search

On the other hand, we can minimize Eq. (85) by setting its
derivative w.r.t. η to zero:

(−η(cid:107)∇f (x(k))(cid:107)2

∂
∂η
= −(cid:107)∇f (x(k))(cid:107)2

2 +

L
2

η2(cid:107)∇f (x(k))(cid:107)2
2)

2 + Lη(cid:107)∇f (x(k))(cid:107)2
2

= (−1 + Lη)(cid:107)∇f (x(k))(cid:107)2
2

set= 0 =⇒ η =

1
L

.

If we set:

η <

1
L

,

(87)

then Eq. (85) becomes:

f (x(k) + ∆x) − f (x(k))
1
2L

(cid:107)∇f (x(k))(cid:107)2

≤ −

2 +

= −

(cid:107)∇f (x(k))(cid:107)2

2 < 0

1
L
1
2L

=⇒ f (x(k+1)) ≤ f (x(k)) −

1
2L

(cid:107)∇f (x(k))(cid:107)2
2.

(88)

Eq. (87) means that there should be an upper-bound, de-
pendent on the Lipschitz constant, on the step size. Hence,
L is still required. Eq. (88) shows that every iteration of
gradient descent decreases the cost function:

f (x(k+1)) ≤ f (x(k)),

(89)

and the amount of this decrease depends on the norm of
gradient at that iteration. In conclusion, the series of solu-
tions converges to the optimal solution while the function
value decreases iteratively until the local minimum:

{x(0), x(1), x(2), . . . } → x∗,
f (x(0)) ≥ f (x(1)) ≥ f (x(2)) ≥ · · · ≥ f (x∗).

If the optimization problem is a convex problem, the solu-
tion is the global solution; otherwise, the solution is local.

5.1.2. LINE-SEARCH
As was shown in Section 5.1.1, the step size of gradient
descent requires knowledge of the Lipschitz constant for
the smoothness of gradient. Hence, we can ﬁnd the suitable
step size η by a search which is named the line-search. In
line-search of every optimization iteration, we start with
η = 1 and halve it, η ← η/2, if it does not satisfy Eq. (81)
with step ∆x = −η∇f (x(k)):

f (x(k) − η∇f (x(k))) < f (x(k)).

(90)

This halving step size is repeated until this equation is satis-
ﬁed, i.e., until we have a decrease in the objective function.
Note that this decrease will happen when the step size be-
comes small enough to satisfy Eq. (87). The algorithm of
gradient descent with line-search is shown in Algorithm 1.
As this algorithm shows, line-search has its own internal
iterations inside every iteration of gradient descent.

21

1 Initialize x(0)
2 for iteration k = 0, 1, . . . do
3

Initialize η := 1
for iteration τ = 1, 2, . . . do
Check Eq. (90) or (92)
if not satisﬁed then

η ← 1

2 × η

else

x(k+1) := x(k) − η∇f (x(k))
break the loop

Check the convergence criterion
if converged then
return x(k+1)

4

5

6

7

8

9

10

11

12

13

Lemma 13 (Time complexity of line-search). In the worst-
case, line-search takes (log L/ log 2) iterations until Eq.
(90) is satisﬁed.

Proof. Proof is available in Appendix B.1.

5.1.3. BACKTRACKING LINE-SEARCH
A more sophisticated line-search method is the Armijo line-
search (Armijo, 1966), also called the backtracking line-
search. Rather than Eq. (90), it checks if the cost function
is sufﬁciently decreased:

f (x(k) + p) ≤ f (x(k)) + c p(cid:62)f (x(k)),

(91)

where c ∈ (0.0.5] is the parameter of Armijo line-search
and p = ∆x is the search direction for update. The value
of c should be small, e.g., c = 10−4 (Nocedal & Wright,
2006). This condition is called the Armijo condition or
the Armijo-Goldstein condition.
In gradient descent, the
search direction is p = ∆x = −η∇f (x(k)) according to
Eq. (84). Hence, for gradient descent, it checks:

f (x(k) − η∇f (x(k))) ≤ f (x(k)) − η γ(cid:107)∇f (x(k))(cid:107)2
2.
(92)

The algorithm of gradient descent with Armijo line-search
is shown in Algorithm 1. Note that we can have more
sophisticated line-search with Wolfe conditions (Wolfe,
1969). This will be introduced in Section 7.5.

5.1.4. CONVERGENCE CRITERION
For all numerical optimization methods including gradient
descent, there exist several methods for convergence crite-
rion to stop updating the solution and terminate optimiza-
tion. Some of them are:

• Small norm of gradient: (cid:107)∇f (x(k+1))(cid:107)2 ≤ (cid:15) where (cid:15)
is a small positive number. The reason for this crite-

rion is the ﬁrst-order optimality condition (see Lemma
9).

where f ∗ is the minimum of cost function and x∗ is the
minimizer. In other words, after t iterations, we have:

22

• Small change of cost

function:

|f (x(k+1)) −

f (x(k))| ≤ (cid:15).

• Small change of gradient of function: |∇f (x(k+1)) −

∇f (x(k))| ≤ (cid:15).

• Reaching maximum desired number of iterations, de-

noted by maxk: k + 1 < maxk.

5.1.5. CONVERGENCE ANALYSIS FOR GRADIENT

DESCENT

We showed in Eq. (88) that the cost function value is de-
creased by gradient descent iterations. The following theo-
rem provides the convergence rate of gradient descent.

Theorem 1 (Convergence rate and iteration complexity of
gradient descent). Consider a differentiable function f (.),
with domain D, whose gradient is L-smooth (see Eq. (12)).
Starting from the initial point x(0), after t iterations of gra-
dient descent, we have:

min
0≤k≤t

(cid:107)∇f (x(k))(cid:107)2

2 ≤

2L(f (x(0)) − f ∗)
t + 1

,

(93)

where f ∗ is the minimum of cost function. In other words,
after t iterations, we have:

f (x(t)) − f ∗ = O(

1
t

),

(97)

which means the distance of convex function value to its
optimum has sublinear convergence (see Deﬁnition 20) in
gradient descent. The iteration complexity is the same as
Eq. (95).

Proof. Proof is available in Appendix B.3.

Theorem 3 (Convergence rate of gradient descent for
strongly convex functions). Consider a µ-strongly convex
and differentiable function f (.), with domain D, whose
gradient is L-smooth (see Eq. (12)). Starting from the ini-
tial point x(0), after t iterations, the convergence rate and
iteration complexity of gradient descent are:

f (x(t)) − f ∗ ≤ (1 −

µ
L

=⇒ f (x(t)) − f ∗ = O(cid:0)(1 −

)t(cid:0)f (x(0)) − f ∗(cid:1)
µ
L

)t(cid:1),

t = O(log

1
(cid:15)

),

(98)

(99)

respectively, where f ∗ is the minimum of cost function. It
means that gradient descent has linear convergence rate
(see Deﬁnition 20) for strongly convex functions.

∃ x(k) : (cid:107)∇f (x(k))(cid:107)2

2 = O(

1
t

),

Note that some convergence proofs and analyses for gradi-
ent descent can be found in (Gower, 2018).

(94)

which means the squared norm of gradient has sublinear
convergence (see Deﬁnition 20) in gradient descent. More-
over, after:

t ≥

2L(f (x(0)) − f ∗)
(cid:15)

− 1,

(95)

iterations, gradient descent
(cid:107)∇f (x(k))(cid:107)2
dient descent is O(1/(cid:15)).

is guaranteed to satisfy
2 ≤ (cid:15). Hence, the iteration complexity of gra-

Proof. Proof is available in Appendix B.2.

The above theorem provides the convergence rate of gradi-
ent descent for a general function. If the function is convex,
we can simplify this convergence rate further, as stated in
the following.

Theorem 2 (Convergence rate of gradient descent for con-
vex functions). Consider a convex and differentiable func-
tion f (.), with domain D, whose gradient is L-smooth (see
Eq. (12)). Starting from the initial point x(0), after t itera-
tions of gradient descent, we have:

f (x(t+1)) − f ∗ ≤

2L(cid:107)x(0) − x∗(cid:107)2
2
t + 1

,

(96)

5.1.6. GRADIENT DESCENT WITH MOMENTUM
Gradient descent and other ﬁrst-order methods can have
a momentum term. Momentum, proposed in (Rumelhart
et al., 1986), makes the change of solution ∆x a little sim-
ilar to the previous change of solution. Hence, the change
adds a history of previous change to Eq. (84):

(∆x)(k) := α(∆x)(k−1) − η(k)∇f (x(k)),

(100)

where α > 0 is the momentum parameter which weights
the importance of history compared to the descent direc-
tion. We use this (∆x)(k) in Eq.
(79) for updating the
solution. Because of faithfulness to the track of previous
updates, momentum reduces the amount of oscillation of
updates in gradient descent optimization.

5.1.7. STEEPEST DESCENT
Steepest descent is similar to gradient descent but there is
a difference between them. In steepest descent, we move
toward the negative gradient as much as possible to reach
the smallest function value which can be achieved at ev-
ery iteration. Hence, the step size at iteration k of steepest
descent is calculated as (Chong & Zak, 2004):
f (cid:0)x(k) − η∇f (x(k))(cid:1),

η(k) := arg min

(101)

η

The term ∂aj/∂ai is calculated by chain rule as:

23

×

=

∂aj
∂zi

(a)
= xji σ(cid:48)(ai),

∂zi
∂aj
∂ai
∂ai
where (a) is because aj = (cid:80)
i xjizi and zi = σ(ai) and
σ(cid:48)(.) denotes the derivative of activation function. Putting
Eq. (105) in Eq. (104) gives:

(105)

δi = σ(cid:48)(ai)

(δj xji).

(cid:88)

j

Putting this equation in Eq. (103) gives:

∂e
∂xi(cid:96)

= z(cid:96) σ(cid:48)(ai)

(cid:88)

(δj xji).

(106)

j

Backpropagation uses the gradient in Eq. (106) for updat-
ing the weight xi(cid:96), ∀i, (cid:96) by gradient descent:

x(k+1)
i(cid:96)

:= x(k)

i(cid:96) − η(k) ∂e
∂xi(cid:96)

.

This tunes the weights from last layer to the ﬁrst layer for
every iteration of optimization.

5.2. Accelerated Gradient Method
It was shown in the literature that gradient descent is not
optimal in convergence rate and can be improved. It was
at that time that Nesterov proposed Accelerated Gradient
Method (AGM) (Nesterov, 1983) to make the convergence
rate of gradient descent optimal (Nesterov, 2003, Chapter
2.2). AGM is also called the Nesterov’s accelerated gra-
dient method or Fast Gradient Method (FGM). A series of
Nesterov’s papers improved AGM (Nesterov, 1983; 1988;
2005; 2013).
Consider a sequence {γ(k)} which satisﬁes:

k
(cid:89)

(1 − γ(i)) ≥ (γ(k))2,

∀k ≥ 0, γ(k) ∈ [0, 1].

(107)

i=0

An example sequence, satisfying this condition, is γ(0) =
γ(1) = γ(2) = γ(3) = 0, γ(k) = 2/k, ∀k ≥ 4. The AGM
updates the solution iteratively as (Nesterov, 1983):

x(k+1) := y(k) − η(k)∇f (y(k)),
y(k+1) := (1 − γ(k))x(k+1) + γ(k)x(k),

(108)

(109)

until convergence.
Theorem 4 (Convergence rate of AGM for convex func-
tions (Nesterov, 1983, under Eq. 7), (Bubeck, 2014, Theo-
rem 3.19)). Consider a convex and differentiable function
f (.), with domain D, whose gradient is L-smooth (see Eq.
(12)). Starting from the initial point x(0), after t iterations
of AGM, we have:

f (x(t+1)) − f ∗ ≤

2L(cid:107)x(0) − x∗(cid:107)2
2
(t + 1)2

= O(

1
t2 ),

(110)

Figure 9. Neurons in three layers of a neural network.

and then, the solution is updated using Eq. (84) as in gra-
dient descent.
Another interpretation of steepest descent is as follows,
according to (Boyd & Vandenberghe, 2004, Chapter 9.4).
The ﬁrst-order Taylor expansion of function is f (x + v) ≈
f (x) + ∇f (x)(cid:62)v. Hence, the step size in the normalized
steepest descent, at iteration k, is obtained as:

∆x = arg min

v

{∇f (x(k))(cid:62)v | (cid:107)v(cid:107)2 ≤ 1},

(102)

which is used in Eq. (79) for updating the solution.

5.1.8. BACKPROPAGATION
Backpropagation (Rumelhart et al., 1986) is the most well-
It
known optimization method used in neural networks.
is actually gradient descent with chain rule in derivatives
because of having layers of parameters. Consider Fig. 9
which shows three neurons in three layers of a network. Let
xji denote the weight connecting neuron i to neuron j. Let
ai and zi be the output of neuron i before and after applying
its activation function σi(.) : R → R, respectively. In other
words, zi := σi(ai).
According to neural network, we have ai = (cid:80)
(cid:96) xi(cid:96)z(cid:96)
which sums over the neurons in layer (cid:96). By chain rule,
the gradient of error e w.r.t. to the weight between neurons
(cid:96) and i is:

×

=

∂e
∂ai

∂ai
∂xi(cid:96)

(a)
= δi × z(cid:96),

∂e
∂xi(cid:96)
where (a) is because ai = (cid:80)
(cid:96) xi(cid:96)z(cid:96) and we deﬁne δi :=
∂e/∂ai. If layer i is the last layer, δi can be computed by
derivative of error (loss function) w.r.t. the output. How-
ever, if i is one of the hidden layers, δi is computed by chain
rule as:

(103)

δi =

∂e
∂ai

(cid:88)

=

j

(cid:16) ∂e
∂aj

×

∂aj
∂ai

(cid:17)

=

(cid:88)

(cid:16)

δj ×

j

(cid:17)

.

∂aj
∂ai

(104)

where f ∗ is the minimum of cost function and x∗ is the
minimizer. It means the distance of convex function value
to its optimum has sublinear convergence (see Deﬁnition
20) in AGM.

from the optimal solution, the step size can be large; how-
ever, it should be small in the last iterations which is sup-
posed to be close to the optimal solution. Some well-known
adaptations for the step size are:

24

Comparing Eqs. (96) and (110) shows that AGM converges
much faster than gradient descent. A book chapter on AGM
is (Bubeck, 2014, Section 3.7). Various versions of AGM
have been uniﬁed in (Tseng, 2008). Moreover, the connec-
tion of AGM with ordinary differential equations has been
investigated in (Su et al., 2016).

5.3. Stochastic Gradient Methods
5.3.1. STOCHASTIC GRADIENT DESCENT
Assume we have a dataset of n data points, {ai ∈ Rd}n
and their labels {li ∈ R}n
decomposed into summation of n terms {fi(x)}n
well-known examples for the cost function terms are:

i=1
i=1. Let the cost function f (.) be
i=1. Some

• Least squares error: fi(x) = 0.5(a(cid:62)

i x − li)2,

• Absolute error: fi(x) = a(cid:62)

i x − li,

• Hinge loss (for li ∈ {−1, 1}): fi(x) = max(0, 1 −

lia(cid:62)

i x).

• Logistic loss (for li ∈ {−1, 1}): log(

The optimization problem (24) becomes:

1

1+exp(−lia(cid:62)

i x) ).

minimize
x

1
n

n
(cid:88)

i=1

fi(x).

(111)

In this case, the full gradient is the average gradient, i.e:

∇f (x) =

1
n

n
(cid:88)

i=1

∇fi(x),

(112)

so Eq. (83) becomes ∆x = −(1/(Ln)) (cid:80)n
i=1 ∇fi(x(k)).
This is what gradient descent uses in Eq. (79) for updat-
ing the solution at every iteration. However, calculation
of this full gradient is time-consuming and inefﬁcient for
large values of n, especially as it needs to be recalculated
at every iteration. Stochastic Gradient Descent (SGD), also
called stochastic gradient method, approximates gradient
descent stochastically and samples (i.e. bootstraps) one
of the points at every iteration for updating the solution.
Hence, it uses:

x(k+1) := x(k) − η(k)∇fi(x(k)),

(113)

η(k) :=

1
k

,

η(k) :=

1
√
k

,

η(k) := η.

(114)

Theorem 5 (Convergence rates for SGD). Consider a
function f (x) = (cid:80)n
i=1 fi(x) and which is bounded below
and each fi is differentiable. Let the domain of function
f (.) be D and its gradient be L-smooth (see Eq.
(12)).
Assume E[(cid:107)∇fi(xk)(cid:107)2
2 | xk] ≤ β2 where β is a constant.
Depending on the step size, the convergence rate of SGD
is:

O(

O(

1
log t
log t
√
t

)

)

if

if

η(k) =

η(k) =

,

1
k
1
√
k

,

O(

1
t

+ η)

if

η(k) = η,

(115)

(116)

(117)

where t denotes the iteration index. If the functions fi’s are
µ-strongly convex, then the convergence rate of SGD is:

)

O(

1
t
O(cid:0)(1 −

if

η(k) =

1
µk

,

)t + η(cid:1)

if

η(k) = η.

µ
L

(118)

(119)

Eqs. (117) and (119) shows that with a ﬁxed step size η,
SGD converges sublinearly for a non-convex function and
linearly for a strongly convex function (see Deﬁnition 20)
in the initial iterations. However, in the late iterations, it
stagnates to a neighborhood around the optimal point and
never reaches it. Hence, SGD has less accuracy than gradi-
ent descent. The advantage of SGD over gradient descent
is that its every iteration is much faster than every iteration
of gradient descent because of less computations for gra-
dient. This faster pacing of every iteration shows off more
when n is huge. In summary, SGD has fast convergence to
low accurate optimal point.
It is noteworthy that the full gradient is not available in
SGD to use for checking convergence, as discussed in Sec-
tion 5.1.4. One can use other criteria in that section or
merely check the norm of gradient for the sampled point.
Moreover, note that SGD can be used with the line-search
methods, too. SGD can also use a momentum term (see
Section 5.1.6).

ragther than Eq. (84). The idea of stochastic approximation
was ﬁrst proposed in (Robbins & Monro, 1951). It was ﬁrst
used for machine learning in (Bottou et al., 1998).
As Eq. (113) states, SGD often uses an adaptive step size
which changes in every iteration. The step size can be de-
creasing because in initial iterations, where we are far away

5.3.2. MINI-BATCH STOCHASTIC GRADIENT DESCENT
Gradient descent uses the entire n data points and SGD
uses one randomly sampled point at every iteration. For
large datasets, gradient descent is very slow and intractable
in every iteration while SGD will need a signiﬁcant num-
ber of iterations to roughly cover all data. Besides, SGD

has low accuracy in convergence to the optimal point. We
can have a middle case where we use a batch of b randomly
sampled points at every iteration. This method is named the
mini-batch SGD or the hybrid deterministic-stochastic gra-
dient method. This batch-wise approach is wise for large
datasets because of the mentioned problems gradient de-
scent and SGD face in big data optimization (Bottou et al.,
2018).
Usually, before start of optimization, the n data points are
randomly divided into (cid:98)n/b(cid:99) batches of size b. This is
equivalent to simple random sampling for sampling points
into batches without replacement. We denote the dataset
by D (where |D| = n) and the i-th batch by Bi (where
|Bi| = b). The batches are disjoint:

(cid:98)n/b(cid:99)
(cid:91)

Bi = D,

i=1
Bi ∩ Bj = ∅, ∀i, j ∈ {1, . . . , (cid:98)n/b(cid:99)}, i (cid:54)= j.

(120)

(121)

Another less-used approach for making batches is to sam-
ple points for a batch during optimization. This is equiva-
lent to bootstrapping for sampling points into batches with
replacement. In this case, the batches are not disjoint any-
more and Eqs. (120) and (121) do not hold.

Deﬁnition 27 (Epoch). In mini-batch SGD, when all (cid:98)n/b(cid:99)
batches of data are used for optimization once, an epoch is
completed. After completion of an epoch, the next epoch is
started and epochs are repeated until convergence of opti-
mization.

In mini-batch SGD, if the k-th iteration of optimization is
using the k(cid:48)-th batch, the update of solution is done as:

x(k+1) := x(k) − η(k) 1
b

(cid:88)

i∈Bk(cid:48)

∇fi(x(k)).

(122)

The scale factor 1/b is sometimes dropped for simplicity.
Mini-batch SGD is used signiﬁcantly in machine learning,
especially in neural networks (Bottou et al., 1998; Good-
fellow et al., 2016). Because of dividing data into batches,
mini-batch SGD can be solved on parallel servers as a dis-
tributed optimization method.

Theorem 6 (Convergence rates for mini-batch SGD). Con-
sider a function f (x) = (cid:80)n
i=1 fi(x) which is bounded be-
low and each fi is differentiable. Let the domain of function
f (.) be D and its gradient be L-smooth (see Eq. (12)) and
assume η(k) = η is ﬁxed. The batch-wise gradient is an
approximation to the full gradient with some error et for
the t-th iteration:

25

The convergence rate of mini-batch SGD for non-convex
and convex functions are:

O(cid:0) 1
t

+ (cid:107)et(cid:107)2
2

(cid:1),

(124)

where t denotes the iteration index. If the functions fi’s are
µ-strongly convex, then the convergence rate of mini-batch
SGD is:

O(cid:0)(1 −

µ
L

)t + (cid:107)et(cid:107)2
2

(cid:1).

(125)

According to Eq. (123), the expected error of mini-batch
SGD at the k-th iteration is:

E[(cid:107)et(cid:107)2

2] = E

(cid:104)(cid:13)
(cid:13)∇f (x(t)) −

1
b

(cid:88)

i∈Bt(cid:48)

∇fi(x(t))(cid:13)
2
(cid:13)
2

(cid:105)
,

(126)

which is variance of estimation. If we sample the batches
without replacement (i.e., sampling batches by simple ran-
dom sampling before start of optimization) or with replace-
ment (i.e., bootstrapping during optimization), the expected
error is (Ghojogh et al., 2020, Proposition 3):

b
n

)

σ2
b

,

E[(cid:107)et(cid:107)2

2] = (1 −

E[(cid:107)et(cid:107)2

2] =

σ2
b

,

(127)

(128)

respectively, where σ2 is the variance of whole dataset. Ac-
cording to Eqs. (127) and (128), the accuracy of SGD by
sampling without and with replacement increases by b → n
and b → ∞, respectively. However, this increase makes ev-
ery iteration slower so there is trade-off between accuracy
and speed. Also, comparing Eqs. (124) and (125) with Eqs.
(94) and (98), while noticing Eqs. (127) and (128), shows
that the convergence rate of mini-batch gets closer to that
of gradient descent if the batch size increases.

5.4. Stochastic Average Gradient Methods
5.4.1. STOCHASTIC AVERAGE GRADIENT
SGD is faster than gradient descent but its problem is its
lower accuracy compared to gradient descent. Stochastic
Average Gradient (SAG) (Roux et al., 2012) keeps a trade-
off between accuracy and speed. Consider the optimization
problem (111). Let ∇fi(x(k)) be the gradient of fi(.), eval-
uated in point x(k), at iteration k. According to Eqs. (84)
and (112), gradient descent updates the solution as:

x(k+1) := x(k) −

η(k)
n

n
(cid:88)

i=1

∇fi(x(k)).

1
b

(cid:88)

i∈Bt(cid:48)

∇fi(x(t)) = ∇f (x(t)) + et.

(123)

SAG randomly samples one of the points and updates its
gradient among the gradient terms. If the sampled point is

the j-th one, we have:

x(k+1) := x(k) −

(cid:16)

η(k)
n

∇fj(x(k)) − ∇fj(x(k−1))

n
(cid:88)

+

∇fi(x(k−1))

(cid:17)

.

i=1

(129)
In other words, we subtract the j-th gradient from the sum-
mation of all n gradients in previous iteration (k − 1) by
(cid:80)n
i=1 ∇fi(x(k−1)) − ∇fj(x(k−1)); then, we add back the
new j-th gradient in this iteration by adding ∇fj(x(k)).
Theorem 7 (Convergence rates for SAG (Roux et al., 2012,
Proposition 1)). Consider a function f (x) = (cid:80)n
i=1 fi(x)
which is bounded below and each fi is differentiable. Let
the domain of function f (.) be D and its gradient be L-
smooth (see Eq.
(12)). The convergence rate of SAG is
O(1/t) where t denotes the iteration index.

Comparing the convergence rates of SAG, gradient de-
scent, and SGD shows that SAG has the same rate order
as gradient descent; although, it usually needs some more
iterations to converge. Practical experiments have shown
that SAG requires many parameter ﬁne-tuning to perform
perfectly. Some other variants of SAG are optimization of
a ﬁnite sum of smooth convex functions (Schmidt et al.,
2017) and its second-order version named Stochastic Aver-
age Newton (SAN) (Chen et al., 2021).

5.4.2. STOCHASTIC VARIANCE REDUCED GRADIENT
Another effective ﬁrst-order method is the Stochastic Vari-
ance Reduced Gradient (SVRG) (Johnson & Zhang, 2013)
which updates the solution according to Algorithm 2. As
this algorithm shows, the update of solution is similar to
SAG (see Eq. (129)) but for every iteration, it updates the
solution for m times. SVRG is an efﬁcient method and
its convergence rate is similar to that of SAG. It is shown in
(Johnson & Zhang, 2013) that both SAG and SVRG reduce
the variance of solution to optimization. Recently, SVRG
has been used for semideﬁnite programming optimization
(Zeng et al., 2021).

5.4.3. ADAPTING LEARNING RATE WITH ADAGRAD,

RMSPROP, AND ADAM

Consider the optimization problem (111). We can adapt the
learning rate in stochastic gradient methods. In the follow-
ing, we introduce the three most well-known methods for
adapting the learning rate, which are AdaGrad, RMSProp,
and Adam.

1 Initialize (cid:101)x(0)
2 for iteration k = 1, 2, . . . do

26

3

4

5

6

7

8

9

(cid:101)x := (cid:101)x(k−1)
(112)
:= 1
∇f ((cid:101)x)
n
x(0) := (cid:101)x
for iteration τ = 0, 1, . . . , m − 1 do

i=1 ∇fi((cid:101)x)

(cid:80)n

Randomly sample j from {1, . . . , n}.
x(τ +1) := x(τ ) − η(τ )(cid:0)∇fj(x(τ )) −
∇fj((cid:101)x) + ∇f ((cid:101)x)(cid:1).

(cid:101)x(k) := x(m)

Algorithm 2: The SVRG algorithm

ment is:

G(j, j) :=

(cid:118)
(cid:117)
(cid:117)
(cid:116)ε +

k
(cid:88)

τ =0

(cid:0)∇jfiτ (x(τ ))(cid:1)2

,

(131)

where ε ≥ 0 is for stability (making G full rank), iτ is the
randomly sampled point (from {1, . . . , n}) at iteration τ ,
and ∇jfiτ (.) is the partial derivative of fiτ (.) w.r.t. its j-th
element (n.b. fiτ (.) is d-dimensional). Putting Eq. (131)
in Eq. (130) can simplify AdaGrad to:

x(k+1)
j

:= x(k)

j −

(cid:113)

η(k)
(cid:0)∇jfiτ (x(τ ))(cid:1)2

ε + (cid:80)k

τ =0

∇fj(x(k)

j

).

(132)

AdaGrad keeps a history of the sampled points and it takes
derivative for them to use. During the iterations so far, if a
dimension has changed signiﬁcantly, it dampens the learn-
ing rate for that dimension (see the inverse in Eq. (130));
hence, it gives more weight for changing the dimensions
which have not changed noticeably.

– Root Mean Square Propagation (RMSProp): RM-
SProp was ﬁrst proposed in (Tieleman & Hinton, 2012)
which is unpublished. It is an improved version of Rprop
(resilient backpropagation) (Riedmiller & Braun, 1992)
which uses the sign of gradient in optimization. Inspired
by momentum in Eq. (100), it updates a scalar variable v
as (Hinton et al., 2012):

v(k+1) := γv(k) + (1 − γ)(cid:107)∇fi(x(k))(cid:107)2
2,

(133)

– Adaptive Gradient (AdaGrad): AdaGrad method
(Duchi et al., 2011) updates the solution iteratively as:

where γ ∈ [0, 1] is the forgetting factor (e.g. γ = 0.9).
Then, it uses this v to weight the learning rate:

x(k+1) := x(k) − η(k)G−1∇fi(x(k)),

(130)

where G is a (d × d) diagonal matrix whose (j, j)-th ele-

x(k+1) := x(k) −

√

η(k)
ε + v(k+1)

∇fj(x(k)

j

),

(134)

where (cid:15) ≥ 0 is for stability to not have division by zero.
Comparing Eqs. (132) and (134) shows that RMSProp has
a similar form to AdaGrad.

– Adaptive Moment Estimation (Adam): Adam opti-
mizer (Kingma & Ba, 2014) improves over RMSProp by
adding a momentum term (see Section 5.1.6). It updates
the scalar v and the vector m ∈ Rd as:

m(k+1) := γ1m(k) + (1 − γ1)∇fi(x(k)),
v(k+1) := γ2v(k) + (1 − γ2)(cid:107)∇fi(x(k))(cid:107)2
2,

(135)

(136)

where γ1, γ2 ∈ [0, 1]. It normalizes these variables as:

(cid:99)m(k+1) :=

1
1 − γk
1

m(k+1),

(cid:98)v(k+1) :=

1
1 − γk
2

v(k+1).

Then, it updates the solution as:

x(k+1) := x(k) −

√

η(k)

ε + (cid:98)v(k+1) (cid:99)m(k+1),

(137)

which is stochastic gradient descent with momentum while
using RMSProp. Convergences of RMSProp and Adam
methods have been discussed in (Zou et al., 2019). The
Adam optimizer is one of the mostly used optimizers in
neural networks.

5.5. Proximal Methods
5.5.1. PROXIMAL MAPPING AND PROJECTION
Deﬁnition 28 (Proximal mapping/operator
(Parikh &
Boyd, 2014)). The proximal mapping or proximal operator
of a convex function g(.) is:

proxg(x) := arg min
u

(cid:16)

g(u) +

(cid:107)u − x(cid:107)2
2

(cid:17)
.

1
2

(138)

In case the function g(.) is scaled by a scalar λ (e.g., this
often holds in Eq. (148) where λ can scale g(.) as the regu-
larization parameter), the proximal mapping is deﬁned as:

proxλg(x) := arg min
u

(cid:16)

g(u) +

(cid:107)u − x(cid:107)2
2

(cid:17)

.

1
2λ

(139)

The proximal mapping is related to the Moreau-Yosida reg-
ularization deﬁned below.

Deﬁnition 29 (Moreau-Yosida regularization or Moreau
envelope (Moreau, 1965; Yosida, 1965)). The Moreau-
Yosida regularization or the Moreau envelope of function
g(.) is:

Mλg(x) := inf
u

(cid:16)

g(u) +

(cid:107)u − x(cid:107)2
2

(cid:17)

.

1
2

(140)

This Moreau-Yosida regularized function has the same min-
imizer as the function g(.) (Lemar´echal & Sagastiz´abal,
1997).

27

Figure 10. Projection of point x onto a set S.

Lemma 14 (Moreau decomposition (Moreau, 1962)). We
always have the following decomposition, named the
Moreau decomposition:

x = proxg(x) + proxg∗ (x),

x = proxλg(x) + λ prox 1

λ g∗ (

x
λ

),

(141)

(142)

where g(.) is a function in a space and g∗(.) is its corre-
sponding function in the dual space (e.g., if g(.) is a norm,
g∗(.) is its dual norm or if g(.) is projection onto a cone,
g∗(.) is projection onto the dual cone).
Lemma 15 (Projection onto set). Consider an indicator
function I(.) which is zero if its condition is satisﬁed and is
inﬁnite otherwise. The proximal mapping of the indicator
function to a convex set S, i.e. I(x ∈ S), is projection of
the point x onto the set S. Hence, projection of x onto set
S, denoted by ΠS (x), is deﬁned as:

ΠS (x) := proxI(.∈S)(x) = arg min
u∈S

(cid:107)u − x(cid:107)2
2

(cid:17)

.

(cid:16) 1
2

(143)

As Fig. 10 shows, this projection simply means projecting
the point x onto the closest point of set from the point x;
hence, the vector connecting the points x and ΠS (x) is
orthogonal to the set S.

Proof.

proxI(.∈S)(x) = arg min
u

(cid:16)

I(x ∈ S) +

(a)
= arg min
u∈S

(cid:16) 1
2

(cid:107)u − x(cid:107)2
2

1
2
(cid:17)

(cid:17)

(cid:107)u − x(cid:107)2
2

,

where (a) is because I(x ∈ S) becomes inﬁnity if x (cid:54)∈ S.
Q.E.D.

Corollary 6 (Moreau decomposition for norm). If the func-
tion is a scaled norm, g(.) = λ(cid:107).(cid:107), we have from re-
arranging Eq. (142) that:

proxλ(cid:107).(cid:107)(x) = x − λ ΠB(

x
λ

),

(144)

where B is the unit ball of dual norm (see Deﬁnition 6).

Derivation of proximal operator for various g(.) functions
are available in (Beck, 2017, Chapter 6). Here, we review
the proximal mapping of some mostly used functions. If
g(x) = 0, proximal mapping becomes an identity map-
ping:

28

proxλ0(x)

(139)

= arg min

(cid:16) 1
2λ

(cid:17)

(cid:107)u − x(cid:107)2
2

= x.

u
Lemma 16 (Proximal mapping of (cid:96)2 norm (Beck, 2017,
Example 6.19)). The proximal mapping of the (cid:96)2 norm is:

proxλ(cid:107).(cid:107)2

(x) =

(cid:16)

1 −

λ
max((cid:107)x(cid:107)2, λ)

(cid:17)

x

=

(cid:26) (cid:0)1 − λ
(cid:107)x(cid:107)2

0

(cid:1)x if (cid:107)x(cid:107)2 ≥ λ
if (cid:107)x(cid:107)2 < λ.

(145)

Proof. Let g(.) = (cid:107).(cid:107)2 and B be the unit (cid:96)2 ball (see Deﬁ-
nition 6) because (cid:96)2 is the dual norm of (cid:96)2 according to Eq.
(3). We have:

ΠB(x) =

(cid:26) x/(cid:107)x(cid:107)2
x

if (cid:107)x(cid:107)2 ≥ 1
if (cid:107)x(cid:107)2 < 1.

=⇒ proxλ(cid:107).(cid:107)2

(x)
(cid:26) (cid:0)1 − λ
(cid:107)x(cid:107)2

(144)
= x − λ ΠB(
(cid:1)x

x
λ
if (cid:107)x(cid:107)2 ≥ λ
x − λ(x/λ) = 0 if (cid:107)x(cid:107)2 < λ.

)

=

Q.E.D.

Lemma 17 (Proximal mapping of (cid:96)1 norm (Beck, 2017,
Example 6.8)). Let xj denote the j-th element of x =
[x1, . . . , xd](cid:62) ∈ Rd and let [ proxλ(cid:107).(cid:107)1
(x)]j denote the j-th
(x) mapping. The
element of the d-dimensional proxλ(cid:107).(cid:107)1
j-th element of proximal mapping of the (cid:96)1 norm is:

[ proxλ(cid:107).(cid:107)1

(x)]j = max(0, |xj| − λ) sign(xj)

= sλ(xj) :=






xj − λ if xj ≥ λ
if |xj| < λ
0
xj + λ if xj ≤ −λ,

(146)

for all j ∈ {1, . . . , d}. Eq.
(146) is called the soft-
thresholding function, denoted here by sλ(.). It is depicted
in Fig. 11.

Proof. Let g(.) = (cid:107).(cid:107)1 and B be the unit (cid:96)∞ ball (see Def-
inition 6) because (cid:96)∞ is the dual norm of (cid:96)1 according to
Eq. (3). The j-th element of projection is:



[ΠB(x)]j =

1
xj
−1

if xj ≥ 1
if |xj| < 1
if xj ≤ −1



=⇒ [proxλ(cid:107).(cid:107)1

(x)]j

(144)
= xj − λ [ΠB(






=

xj − λ if xj ≥ λ
if |xj| < λ
0
xj + λ if xj ≤ −λ.

x
λ

)]j

Q.E.D.

Figure 11. Soft-thresholding function.

2 in Eq. (139) is strongly

5.5.2. PROXIMAL POINT ALGORITHM
The term g(u) + 1/(2λ)(cid:107)u − x(cid:107)2
convex; hence, the proximal point, proxλg(x), is unique.
Lemma 18. The point x∗ minimizes the function f (.) if
and only if x∗ = proxλf (x∗). In other words, the optimal
point x∗ is the ﬁxed point of the proxλf (.) operator (see
Deﬁnition 12).

Consider the optimization problem (24). Proximal point al-
gorithm, also called proximal minimization, was proposed
in (Rockafellar, 1976).
It ﬁnds the optimal point of this
problem by iteratively updating the solution as:

x(k+1) := proxλf (x(k))

(139)

= arg min

u

(cid:16)

f (u) +

1
2λ

(cid:107)u − x(k)(cid:107)2
2

(cid:17)

,

(147)

until convergence, where λ can be seen as a parameter re-
lated to the step size.
In other words, proximal gradient
method applies gradient descent on the Moreau envelope
Mλf (x) (recall Eq. (140)) rather than on the function f (.)
itself.

5.5.3. PROXIMAL GRADIENT METHOD
– Composite Problems: Consider the following optimiza-
tion problem:

minimize
x

f (x) + g(x),

(148)

where f (x) is a smooth function and g(x) is a convex func-
tion which is not smooth necessarily. According to the fol-
lowing deﬁnition, this is a composite optimization problem.

Deﬁnition 30 (Composite objective function (Nesterov,
2013)). In optimization, if a function is stated as a sum-
mation of two terms, f (x) + g(x), it is called a composite
function and its optimization is a composite optimization
problem.

Composite problems are widely used in machine learning
and regularized problems because f (x) can be the cost
function to be minimized while g(x) is the penalty or reg-
ularization term (Ghojogh & Crowley, 2019).

– Proximal Gradient Method for Composite Optimiza-
tion:
For solving problem (148), we can approximate the func-
tion f (.) by its quadratic approximation around point x be-
cause it is smooth (differentiable):

f (u) ≈ f (x) + ∇f (x)(cid:62)(u − x) +

1
2η

(cid:107)u − x(cid:107)2
2,

where we have replaced ∇2f (x) with scaled identity ma-
trix, (1/η)I. Hence, the solution of problem (148) can be
approximated as:

x = arg min

u

(cid:0)f (u) + g(u)(cid:1)

≈ arg min

u

(cid:0)f (x) + ∇f (x)(cid:62)(u − x) +

1
2η

(cid:107)u − x(cid:107)2
2

+ g(u)(cid:1) = arg min

u

(cid:0) 1
2η

(cid:107)u − (x − η∇f (x))(cid:107)2

2 + g(u)(cid:1).
(149)

The ﬁrst term in Eq. (149) keeps the solution close to the
solution of gradient descent for minimizing the function
f (.) (see Eq. (84)) and the second term in Eq. (149) makes
the function g(.) small.
Proximal gradient method, also called proximal gradient
descent, uses Eq. (149) for solving the composite problem
(148). It was ﬁrst proposed in (Nesterov, 2013) and also in
(Beck & Teboulle, 2009) for g = (cid:107).(cid:107)1. It ﬁnds the optimal
point by iteratively updating the solution as:

x(k+1) (149)
:=
(cid:0) 1
2η(k)

arg min

u

(cid:107)u − (x(k) − η(k)∇f (x(k)))(cid:107)2

2 + g(u)(cid:1)

(139)
= proxη(k)g

(cid:0)x(k) − η(k)∇f (x(k))(cid:1),

(150)

until convergence, where η(k) is the step size which can be
ﬁxed or found by line-search. In Eq. (148), the function
g(.) can be a regularization term such as (cid:96)2 or (cid:96)1 norm. In
these cases, we use Lemmas 16 and 17 for calculating Eq.
(150). The convergence rates of proximal gradient method
is discussed in (Schmidt et al., 2011). A distributed ver-
sion of this method is also proposed in (Chen & Ozdaglar,
2012).

5.6. Gradient Methods for Constrained Problems
5.6.1. PROJECTED GRADIENT METHOD
Projected gradient method (Iusem, 2003), also called gra-
dient projection method and projected gradient descent,
considers g(x) to be the indicator function I(x ∈ S) in
problem (148). In other words, the optimization problem
is a constrained problem as problem (28), which can be re-
stated to:

minimize
x

f (x) + I(x ∈ S),

(151)

because the indicator function becomes inﬁnity if its con-
dition is not satisﬁed. According to Eq. (150), the solution
is updated as:

29

x(k+1) (150)

(cid:0)x(k) − η(k)∇f (x(k))(cid:1)

:= proxη(k)I(.∈S)
(143)
= ΠS

(cid:0)x(k) − η(k)∇f (x(k))(cid:1).

(152)

In other words, projected gradient method performs a step
of gradient descent and then projects the solution onto the
set of constraint. This procedure is repeated until conver-
gence of solution.

Lemma 19 (Projection onto the cone of orthogonal ma-
trices (Parikh & Boyd, 2014, Section 6.7.2)). A function
g : Rd1×d2 → R is orthogonally invariant if g(U XV (cid:62)) =
g(X), for all U ∈ Rd1×d1, X ∈ Rd1×d2, and V ∈
Rd2×d2 where U and V are orthogonal matrices.
Let g be a convex and orthogonally invariant function, and
it works on the singular values of a matrix variable X ∈
Rd1×d2, i.e., g = (cid:98)g ◦ σ where the function σ(X) gives the
vector of singular values of X. In this case, we have:

(cid:16)
proxλ,g(X) = U diag

proxλ,g

(cid:0)σ(X)(cid:1)(cid:17)

V (cid:62),

(153)

where diag(.) makes a diagonal matrix with its input as the
diagonal, and U ∈ Rd1×d1 and V ∈ Rd2×d2 are the ma-
trices of left and right singular vectors of X, respectively.
Consider the constraint for projection onto the cone of or-
thogonal matrices, i.e., X (cid:62)X = I. In this constraint, the
function g deals with the singular values of X. The rea-
son is that, from the Singular Value Decomposition (SVD)
of X, we have: X SVD= U ΣV (cid:62) =⇒ X (cid:62)X =
U ΣV (cid:62)V ΣU (cid:62) (a)
= U Σ2U (cid:62) set= I =⇒ U Σ2U (cid:62)U =
(b)
=⇒ U Σ2 = U =⇒ Σ = I, where (a) and (b)
U
are because U and V are orthogonal matrices. Therefore,
the constraint X (cid:62)X = I (i.e., projecting onto the cone of
orthogonal matrices) can be modeled by Eq. (153) which
is simpliﬁed to setting all singular values of X to one:

proxλ,g(X) = ΠO = U IV (cid:62),

(154)

where I ∈ Rd1×d2 is a rectangular identity matrix and O
denotes the cone of orthogonal matrices. If the constraint
is scaled orthogonality, i.e. X (cid:62)X = λI with λ as the
scale, the projection is setting all singular values to λ by
U (λI)V (cid:62) = λU IV (cid:62).

Although most often projected gradient method is used for
Eq. (152), there are few other variants of projected gradient
methods such as (Drummond & Iusem, 2004):

(cid:0)x(k) − η(k)∇f (x(k))(cid:1),
y(k) := ΠS
x(k+1) := x(k) + γ(k)(y(k) − x(k)),

(155)

(156)

where η(k) and γ(k) are positive step sizes at iteration k. In
this alternating approach, we ﬁnd an additional variable y
by gradient descent followed by projection and then update
x to get close to the found y while staying close to the
previous solution by line-search.

5.6.2. PROJECTION ONTO CONVEX SETS (POCS) AND

AVERAGED PROJECTIONS

Assume we want to project a point onto the intersection of
c closed convex sets, i.e., (cid:84)c
j=1 Sj. We can model this by
an optimization problem with a fake objective function:

minimize
x

x ∈ Rd

subject to x ∈ S1, . . . , x ∈ Sc.

(157)

Projection Onto Convex Sets (POCS) solves this problem,
similar to projected gradient method, by projecting onto the
sets one-by-one (Bauschke & Borwein, 1996):
x(k+1) := ΠS1(ΠS2(. . . ΠSc(x(k)) . . . )),

(158)

and repeating it until convergence. Another similar method
for solving problem (157) is the averaged projections
which updates the solution as:

x(k+1) :=

1
c

(cid:16)

(cid:17)
ΠS1(x(k)) + · · · + ΠSc(x(k))

.

(159)

5.6.3. FRANK-WOLFE METHOD
Frank-Wolfe method, also called conditional gradient
method and reduced gradient algorithm, was ﬁrst proposed
in (Frank & Wolfe, 1956) and it can be used for solving the
constrained problem (28) using gradient of objective func-
tion (Levitin & Polyak, 1966). It updates the solution as:

y(k) := arg min
y∈S

∇f (x(k))(cid:62)y,

x(k+1) := (1 − γ(k)) x(k) + γ(k)y(k),

(160)

(161)

until convergence, where γ(k) is the step size at iteration k.
Eq. (160) ﬁnds the direction to move toward at the iteration
and Eq. (161) updates the solution while staying close to
the previous solution by line-search. A stochastic version
of Frank-Wolfe method is proposed in (Reddi et al., 2016).

6. Non-smooth and L1 Norm Optimization

Methods

6.1. Lasso Regularization
The (cid:96)1 norm can be used for sparsity (Ghojogh & Crow-
ley, 2019). We explain the reason in the following. Spar-
sity is very useful and effective because of betting on spar-
sity principal (Friedman et al., 2001) and the Occam’s razor
(Domingos, 1999). If x = [x1, . . . , xd](cid:62), for having spar-
sity, we should use subset selection for the regularization
of a cost function Ω0(x):

minimize
x

Ω(x) := Ω0(x) + λ ||x||0,

(162)

where:

||x||0 :=

d
(cid:88)

j=1

I(xj (cid:54)= 0) =

(cid:26) 0
1

if xj = 0,
if xj (cid:54)= 0,

30

(163)

is the “(cid:96)0” norm, which is not a norm (so we use “.” for
it) because it does not satisfy the norm properties (Boyd &
Vandenberghe, 2004). The “(cid:96)0” norm counts the number of
non-zero elements so when we penalize it, it means that we
want to have sparser solutions with many zero entries. Ac-
cording to (Donoho, 2006), the convex relaxation of “(cid:96)0”
norm (subset selection) is (cid:96)1 norm. Therefore, we write the
regularized optimization as:

minimize
x

Ω(x) := Ω0(x) + λ ||x||1.

(164)

Note that the (cid:96)1 regularization is also referred to as lasso
(least absolute shrinkage and selection operator) regular-
ization (Tibshirani, 1996; Hastie et al., 2019). Different
methods exist for solving optimization having (cid:96)1 norm,
such as its approximation by Huber function (Huber, 1992),
proximal algorithm and soft thresholding (Parikh & Boyd,
2014), coordinate descent (Wright, 2015; Wu & Lange,
2008), and subgradients. In the following, we explain these
methods.

6.2. Convex Conjugate
6.2.1. CONVEX CONJUGATE
Consider Fig. 12 showing a line which supports the func-
tion f meaning that it is tangent to the function and the
function upper-bounds it. In other words, if the line goes
above where it is, it will intersect the function in more than
a point. Now let the support line be multi-dimensional to
be a support hyperplane. For having this tangent support
hyperplane with slope y ∈ Rd and intercept β ∈ R, we
should have:

y(cid:62)x + β = f (x) =⇒ β = f (x) − y(cid:62)x.

We want the smallest intercept for the support hyperplane:

β∗ = min
x∈Rd

(cid:0)f (x) − y(cid:62)x(cid:1) (19)

= − max
x∈Rd

(cid:0)y(cid:62)x − f (x)(cid:1).

We deﬁne f ∗(y) := −β∗ to have the convex conjugate
deﬁned as below.

Deﬁnition 31 (Convex conjugate of function). The conju-
gate gradient of function f (.) is deﬁned as:

f ∗(y) := sup
x∈Rd

(cid:0)y(cid:62)x − f (x)(cid:1).

(165)

The convex conjugate of a function is always convex, even
if the function itself is not convex, because it is point-wise
maximum of afﬁne functions.

31

According to Eq. (167), we have:

|xj| = sup
y∈R

(cid:0)xjyj − f ∗(yj)(cid:1) (170)

= max
|yj |≤1

xjyj.

This is not unique for xj = 0. Hence, we add a µ-strongly
convex function to the above equation to make the solu-
tion unique at xj = 0 also. This added term is named the
proximity function deﬁned below.

Deﬁnition 32 (Proximity function (Banaschewski &
Maranda, 1961)). A proximity function p(y) for a closed
convex set S ∈ dom(p) is a function which is continuous
and strongly convex. We can change Eq. (167) to:

f (x) ≈ fµ(x) := sup

y∈dom(f ∗)

(cid:0)x(cid:62)y − f ∗(y) − µ p(y)(cid:1),

(171)

Figure 12. Supporting line (or hyper-plane) to the function.

Lemma 20 (Conjugate of convex conjugate).
gate of convex conjugate of a function is:

the conju-

f ∗∗(x) = sup

(cid:0)x(cid:62)y − f ∗(y)(cid:1).

(166)

y∈dom(f ∗)

where µ > 0.

It is always a lower-bound for the function, i.e., f ∗∗(x) ≤
f (x). If the function f (.) is convex, we have f ∗∗(x) =
f (x); hence, for a convex function, we have:

f (x) = sup

(cid:0)x(cid:62)y − f ∗(y)(cid:1).

(167)

y∈dom(f ∗)

Lemma 21 (Gradient in terms of convex conjugate). For
any function f (.), we have:

∇f (x) = arg max

y∈dom(f ∗)

(cid:0)x(cid:62)y − f ∗(y)(cid:1).

(168)

6.2.2. HUBER FUNCTION: SMOOTHING L1 NORM BY

CONVEX CONJUGATE

Lemma 22 (The convex conjugate of (cid:96)1 norm). The convex
conjugate of f (.) = (cid:107).(cid:107)1 is:
(cid:26) 0

if (cid:107)y(cid:107)∞ ≤ 1

f ∗(y) =

(169)

∞ Otherwise.

Proof. We can write (cid:96)1 norm as:

f (x) = (cid:107)x(cid:107)1 = max

(cid:107)z(cid:107)∞≤1

x(cid:62)z.

Using this in Eq. (165) results in Eq. (169). Q.E.D.

(168), we have ∇f (x) =
According to Eq.
arg max(cid:107)y(cid:107)∞≤1 x(cid:62)y. For x = 0, we have ∇f (x) =
arg max(cid:107)y(cid:107)∞≤1 0 which has many solutions. Therefore,
at x = 0, the function (cid:107).(cid:107)1 norm is not differentiable and
not smooth because the gradient at that point is not unique.
We can smooth the (cid:96)1 norm at x = 0 using convex conju-
gate. Let x = [x1, . . . , xd](cid:62). As we have f (x) = (cid:107)x(cid:107)1 =
(cid:80)d
j=1 |xj|, we can use the convex conjugate for every di-
mension f (xj) = |xj|:

f ∗(yj) =

(cid:26) 0

if |yj| ≤ 1
∞ Otherwise.

(170)

Using Eq. (171), we can have:

|xj| ≈ sup
y∈R

(cid:0)xjyj − f ∗(yj) −

(170)

= max
|yj |≤1

(xjyj −

µ
2

y2
j ) =

(cid:1)

y2
j

µ
2
(cid:40) x2
j
2µ
|xj| − µ
2

if |xj| ≤ µ
if |xj| > µ.

This approximation to (cid:96)1 norm, which is differentiable ev-
erywhere, including at xj = 0, is named the Huber function
deﬁned below. Note that the Huber function is the Moreau
envelope of absolute value (see Deﬁnition 29).

Deﬁnition 33 (Huber and pseudo-Huber functions (Huber,
1992)). The Huber function and pseudo-Huber functions
are:

hµ(x) =

(cid:40) x2
2µ
|x| − µ
2

if |x| ≤ µ
if |x| > µ,

(cid:98)hµ(x) =

(cid:115)

(cid:17)2

(cid:16) x
µ

+ 1 − 1,

(172)

(173)

respectively, where µ > 0. The derivative of these functions
is easily calculated. For example, the derivative of Huber
function is:

∇hµ(x) =

(cid:26) x
µ
sign(x)

if |x| ≤ µ
if |x| > µ.

The Huber and pseudo-Huber functions are shown for dif-
ferent µ values in Fig. 13. As this ﬁgure shows, in con-
trast to (cid:96)1 norm or absolute value, these two functions are
smooth so they approximate the (cid:96)1 norm smoothly. This
ﬁgure also shows that the Huber function is always upper-
bounded by absolute value ((cid:96)1 norm); however, this does
not hold for pseudo-Huber function. We can also see that

32

Figure 13. (a) Comparison of (cid:96)1 and (cid:96)2 norms in R1, (b) comparison of (cid:96)1 norm (i.e., absolute value in R1) and the Huber function, and
(c) comparison of (cid:96)1 norm (i.e., absolute value in R1) and the pseudo-Huber function.

the approximation of Huber function is better than the ap-
proximation of pseudo-Huber function; however, its cal-
culation is harder than pseudo-Huber function because it
is a piece-wise function (compare Eqs. (172) and (173)).
Moreover, the ﬁgure shows a smaller positive value µ give
better approximations, although it makes calculation of the
Huber and pseudo-Huber functions harder.

6.3. Soft-thresholding and Proximal Methods
Proximal mapping was introduced in Section 5.5. We can
use proximal mapping of non-smooth functions to solve
non-smooth optimization by proximal methods introduced
in Section 5.5. For example, we can solve an optimization
problem containing (cid:96)1 norm in its objective function using
Eq. (146). That equation, named soft-thresholding, is the
proximal mapping of (cid:96)1 norm. Then, we can use any of the
proximal methods such as proximal point method and prox-
imal gradient method. For solving the regularized prob-
lem (164), which is optimizing a composite function, we
can use the proximal gradient method introduced in Sec-
tion 5.5.

6.4. Coordinate Descent
6.4.1. COORDINATE METHOD
Assume x = [x1, . . . , xd](cid:62). For solving Eq. (24), coor-
dinate method (Wright, 2015) updates the dimensions (co-
ordinates) of solution one-by-one and not all dimensions
together at once:

x(k+1)
1

x(k+1)
2

...
x(k+1)
d

:= arg min
x1

:= arg min
x2

f (x1, x(k)

2 , x(k)

f (x(k+1)
1

, x2, x(k)

3 , . . . , x(k)
d ),
2 , . . . , x(k)
d ),

:= arg min
xd

f (x(k+1)
1

, x(k+1)
2

, x(k+1)
3

, . . . , xd),

(174)

until convergence of all dimensions of solution. Note that
the update of every dimension uses the latest update of pre-
viously updated dimensions. The order of updates for the
dimensions does not matter. The idea of coordinate descent
algorithm is similar to the idea of Gibbs sampling (Geman
& Geman, 1984; Ghojogh et al., 2020) where we work on
the dimensions of the variable one by one.
If we use a step of gradient descent (i.e. Eq.
(84)) for
every of the above updates, the method is named coordi-
nate descent. If we use proximal gradient method (i.e., Eq.
(150)) for every update in coordinate method, the method
is named the proximal coordinate descent. Note that we
can group some of the dimensions (features) together and
alternate between updating the blocks (groups) of features.
That method is named block coordinate descent. The con-
vergence analysis of coordinate descent and block coordi-
nate descent methods can be found in (Luo & Tseng, 1992;
1993) and (Tseng, 2001), respectively. They show that if
the function f (.) is continuous, proper, and closed, the co-
ordinate descent method converges to a stationary point.
There exist some other faster variants of coordinate de-
scent named accelerated coordinate descent (Lee & Sid-
ford, 2013; Fercoq & Richt´arik, 2015).
similar to SGD, the full gradient is not available in coordi-
nate descent to use for checking convergence, as discussed
in Section 5.1.4. One can use other criteria in that section.
Moreover, note that SGD can be used with the line-search
methods, too. Although coordinate descent methods are
very simple and shown to work properly for (cid:96)1 norm op-
timization (Wu & Lange, 2008), they have not sufﬁciently
attracted the attention of researchers in the ﬁeld of opti-
mization (Wright, 2015).

6.4.2. L1 NORM OPTIMIZATION
Coordinate descent method can be used for (cid:96)1 norm (lasso)
optimization (Wu & Lange, 2008) because every coordi-
nate of the (cid:96)1 norm is an absolute value ((cid:107)x(cid:107)1 = (cid:80)d
j=1 |xj|

for x = [x1, . . . , xd](cid:62)) and the derivative of absolute value
a simple sign function (note that we have subgradient for
absolute value at zero, which will be introduced in Section
6.5.1). One of the well-known (cid:96)1 optimization methods
is the lasso regression (Tibshirani, 1996; Friedman et al.,
2001; Hastie et al., 2019):

minimize
β

1
2

(cid:107)y − Xβ(cid:107)2

2 + λ(cid:107)β(cid:107)1,

(175)

where y ∈ Rn are the labels, X = [x1, . . . , xd] ∈ Rn×d
are the observations, β = [β1, . . . , βd](cid:62) ∈ Rd are the re-
gression coefﬁcients, and λ is the regularization parameter.
The lasso regression is sparse which is effective because of
the reasons explained in Section 6.1.
Let c denote the objective function in Eq. (175). The objec-
tive function can be simpliﬁed as 0.5(y(cid:62)y − 2β(cid:62)X (cid:62)y +
β(cid:62)X (cid:62)Xβ) + λ(cid:107)β(cid:107)1. We can write the j-th element of
this objective, denoted by cj, as:

cj =

1
2

(y(cid:62)y − 2x(cid:62)

j yβj
j xjβj + βjx(cid:62)

+ βjx(cid:62)

j X −jβ−j) + λ|βj|,

where X −j := [x1, . . . , xj−1, xj+1, . . . , xd] and β−j :=
[β1, . . . , βj−1, βj+1, . . . , βd](cid:62). For coordinate descent, we
need gradient of objective function w.r.t. every coordinate.
The derivatives of other coordinates of objective w.r.t. βj
are zero so we need cj for derivative w.r.t. βj. Taking
derivative of cj w.r.t. βj and setting it to zero gives:

=

∂c
∂βj
= x(cid:62)

∂cj
∂βj
j xjβj + x(cid:62)

j (X −jβ−j − y) + λ sign(βj) set= 0

=⇒ βj = s λ

(cid:107)xj (cid:107)2
2

(cid:16) x(cid:62)

j (y − X −iβ−i)
j xj

x(cid:62)

(cid:17)






=

x(cid:62)

j (y−X−iβ−i)
(cid:107)xj (cid:107)2
2

− λ

(cid:107)xj (cid:107)2
2

if

0
x(cid:62)

j (y−X−iβ−i)
(cid:107)xj (cid:107)2
2

+ λ

(cid:107)xj (cid:107)2
2

if

if |

x(cid:62)

x(cid:62)

x(cid:62)

j (y−X−iβ−i)
j xj
j (y−X−iβ−i)
j xj
j (y−X−iβ−i)
j xj

x(cid:62)

x(cid:62)

x(cid:62)

≥ λ

(cid:107)xj (cid:107)2
2
| < λ

(cid:107)xj (cid:107)2
2
≤ − λ

(cid:107)xj (cid:107)2
2

(146)).
which is a soft-thresholding function (see Eq.
Therefore, coordinate descent for (cid:96)1 optimization ﬁnds the
soft-thresholding solution, the same as the proximal map-
ping. We can use this soft-thresholding in coordinate de-
scent where we use βj’s in Eq. (174) rather than xj’s.

6.5. Subgradient Methods
6.5.1. SUBGRADIENT
We know that the convex conjugate f ∗(y) is always con-
If the convex conjugate f ∗(y) is strongly convex,
vex.
then we have only one gradient according to Eq.
(168).
However, if the convex conjugate is only convex and not

33

Figure 14. Subgradients: (a) the two extreme subgradients at the
non-smooth point and (b) some examples of the subgradients in
the subdifferential at the non-smooth point.

strongly convex, Eq. (168) might have several solutions so
the gradient may not be unique. For the points in which the
function does not have a unique gradient, we can have a set
of subgradients, deﬁned below.
Deﬁnition 34 (Subgradient). Consider a convex function
f (.) with domain D. The vector g ∈ Rd is a subgradient of
f (.) at x ∈ D if it satisﬁes:

f (y) ≥ f (x) + g(cid:62)(y − x),

∀y ∈ D.

(176)

As Fig. 14 shows, if the function is not smooth at a point, it
has multiple subgradients at that point. This is while there
is only one subgradient (which is the gradient) for a point
at which the function is smooth.
Deﬁnition 35 (subdifferential). The subdifferential of a
convex function f (.), with domain D, at a point x ∈ D
is the set of all subgradients at that point:

∂f (x) := {g | g(cid:62)(y − x)

(176)

≤ f (y) − f (x), ∀y ∈ D}.
(177)

The subdifferential is a closed convex set. Every subgradi-
ent is a member of the subdifferential, i.e., g ∈ ∂f (x). An
example subdifferential is shown in Fig. 14.

An example of subgradient is the subdifferential of absolute
value, f (.) = |.|:

,

∂f (x) =






1
∈ [−1, 1]
−1

if x > 0
if x = 0
if x < 0.

(178)

The subgradient of absolute value is equal to the gradient
for x < 0 and x > 0 while there exists a set of subgradients
at x = 0 because absolute value is not smooth at that point.
We can also compute the subgradient of (cid:96)1 norm because
we have f (x) = (cid:107)x(cid:107)1 = (cid:80)d
i=1 fi(xi) for
x = [x1, . . . , xd](cid:62). We take Eq. (178) as the subdifferen-
tial of the i-th dimension, denoted by ∂fi(xi). Hence, for
f (x) = (cid:107)x(cid:107)1, we have ∂f (x) = ∂f1(x1) × · · · × ∂fd(xd)
where × denotes the Cartesian product of sets.
We can have the ﬁrst-order optimality condition using sub-
gradients by generalizing Lemma 9 as follows.

i=1 |xi| = (cid:80)d

Lemma 23 (First-order optimality condition with subgra-
dient). If x∗ is a local minimizer for a function f (.), then:

0 ∈ ∂f (x∗).

(179)

Note that if f (.) is convex, this equation is a necessary and
sufﬁcient condition for a minimizer.

Proof. According to Eq. (176), we have f (y) ≥ f (x∗) +
g(cid:62)(y − x∗), ∀y. If we have g = 0 ∈ ∂f (x∗), we have
f (y) ≥ f (x∗) + 0(cid:62)(y − x∗) = f (x∗) which means that
x∗ is a minimizer. Q.E.D.

The following lemma can be useful for calculation of sub-
differential of functions.

Lemma 24. Some useful properties for calculation of sub-
differential of functions:

• For a smooth function or at points where the function
is smooth, subdifferential has only one member which
is the gradient: ∂f (x) = {∇f (x)}.
• Linear combination: If f (x) = (cid:80)n
i=1 ai∂fi(x).

ai ≥ 0, then ∂f (x) = (cid:80)n

i=1 aifi(x) with

• Afﬁne transformation: If f (x) = f0(Ax + b), then

∂f (x) = A(cid:62)∂f0(Ax + b).

• Point-wise maximum:

f (x)

Suppose

=
max{f1(x), . . . , fn(x)} where fi’s are differen-
tiable. Let I(x) := {i|fi = f (x)} states which
function has the maximum value for the point x.
At the any point other than the intersection point
of
functions (which is smooth), The subgradient
is g = ∇fi(x) for i ∈ I(x). At the intersection
point of two functions (which is not smooth), e.g.
fi(x) = fi+1(x), we have:

∂f (x) = {g | t∇fi(x) + (1 − t)∇fi+1(x), ∀t ∈ [0, 1]}.

6.5.2. SUBGRADIENT METHOD
Subgradient method, ﬁrst proposed in (Shor, 2012), is used
for solving the unconstrained optimization problem (24)
where the function f (.) is not smooth, i.e., not differen-
tiable, everywhere in its domain. It iteratively updates the
solution as:

x(k+1) := x(k) − η(k)g(k),

(180)

where g(k) is any subgradient of function f (.) in point x at
iteration k, i.e. g(k) ∈ ∂f (x(k)), and η(k) is the step size
at iteration k. Comparing this update with Eq. (84) shows
that gradient descent is a special case of the subgradient
method because for a smooth function, gradient is the only
member of the subdifferential set (see Lemma 24); hence,
the only subgradient is the gradient.

34

6.5.3. STOCHASTIC SUBGRADIENT METHOD
Consider the optimization problem (111) where at least one
of the fi(.) functions is not smooth. Stochastic subgradient
method (Shor, 1998) randomly samples one of the points to
update the solution in every iteration:

x(k+1) := x(k) − η(k)g(k)

i

,

(181)

where g(k)
i ∈ ∂fi(x(k)). Comparing this with Eq. (113)
shows that stochastic gradient descent is a special case of
stochastic gradient descent because for a smooth function,
gradient is the only member of the subdifferential set (see
Lemma 24). Note that there is another method named
stochastic subgradient method which uses a noisy unbiased
subgradient for robustness to noisy data (Boyd & Mutap-
cic, 2008). Here, our focus was on random sampling of the
point and not the noise.
We can have mini-batch stochastic subgradient method
which is a generalization of mini-batch SGD for non-
smooth functions. In this case, Eq. (122) is changed to:

x(k+1) := x(k) − η(k) 1
b

g(k)
i

.

(cid:88)

i∈Bk(cid:48)

(182)

Note that, if the function is not smooth, we can also use
subgradient instead of gradient in other stochastic methods
such as SAG and SVRG, which were introduced before.
For this, we need to use g(k)
rather than ∇f (x(k)) in these
methods.

i

6.5.4. PROJECTED SUBGRADIENT METHOD
Consider the optimization problem (28).
If the function
f (.) is not smooth, we can use he projected subgradient
method (Alber et al., 1998) which generalizes the projected
gradient method introduced in Section 5.6.1. Similar to Eq.
(152), it iteratively updates the solution as:

x(k+1) = ΠS

(cid:0)x(k) − η(k)g(k)(cid:1),

(183)

until convergence of the solution.

7. Second-Order Optimization: Newton’s

Method

7.1. Newton’s Method from the Newton-Raphson Root

Finding Method

We can ﬁnd the root of a function f : x (cid:55)→ f (x) by solv-
ing the equation f (x) set= 0. The root of function can be
found iteratively where we get closer to the root over it-
erations. One of the iterative root-ﬁnding methods is the
Newton-Raphson method (Stoer & Bulirsch, 2013). In ev-
ery iteration, it ﬁnds the next solution as:

x(k+1) := x(k) −

f (x(k))
∇f (x(k))

,

(184)

where ∇f (x(k)) is the derivative of function w.r.t. x. Ac-
cording to Eq. (18), in unconstrained optimization, we can
ﬁnd the extremum (minimum or maximum) of the function
by setting its derivative to zero, i.e., ∇f (x) set= 0. Recall
that Eq. (184) was used for solving f (x) set= 0. Therefore,
for solving Eq. (18), we can replace f (x) with ∇f (x) in
Eq. (184):

x(k+1) := x(k) − η(k) ∇f (x(k))
∇2f (x(k))

,

(185)

where ∇2f (x(k)) is the second derivative of function w.r.t.
x and we have included a step size at iteration k denoted
by η(k) > 0. This step size can be either ﬁxed or adaptive.
If x is multivariate, i.e. x ∈ Rd, Eq. (185) is written as:

x(k+1) := x(k) − η(k)(cid:0)∇2f (x(k))(cid:1)−1

∇f (x(k)),

(186)

where ∇f (x(k)) ∈ Rd is the gradient of function w.r.t. x
and ∇2f (x(k)) ∈ Rd×d is the Hessian matrix w.r.t. x.
Because of the second derivative or the Hessian, this opti-
mization method is a second-order method. The name of
this method is the Newton’s method.

7.2. Newton’s Method for Unconstrained Optimization
Consider the following optimization problem:

minimize
x

f (x).

(187)

where f (.) is a convex function. Iterative optimization can
Iterative optimization up-
be ﬁrst-order or second-order.
dates solution iteratively as in Eq. (79). The update contin-
ues until ∆x becomes very small which is the convergence
of optimization. In the ﬁrst-order optimization, the step of
updating is ∆x := −∇f (x). Near the optimal point x∗,
gradient is very small so the second-order Taylor series ex-
pansion of function becomes:

(x − x∗)
f (x) ≈ f (x∗) + ∇f (x∗)(cid:62)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≈ 0

+

1
2

(x − x∗)(cid:62)∇2f (x∗)(x − x∗)

≈ f (x∗) +

1
2

(x − x∗)(cid:62)∇2f (x∗)(x − x∗).

(188)

This shows that the function is almost quadratic near the
optimal point. Following this intuition, Newton’s method
uses Hessian ∇2f (x) in its updating step:

∆x := −∇2f (x)−1∇f (x).

(189)

In the literature, this equation is sometimes restated to:

∇2f (x) ∆x := −∇f (x).

(190)

35

7.3. Newton’s Method for Equality Constrained

Optimization

The optimization problem may have equality constraints:

minimize
x

f (x)

subject to Ax = b.

(191)

After a step of update by p = ∆x, this optimization be-
comes:

minimize
x

f (x + p)

(192)

subject to A(x + p) = b.

The Lagrangian of this optimization problem is:

L = f (x + p) + ν(cid:62)(A(x + p) − b),

where ν is the dual variable. The second-order Taylor se-
ries expansion of function f (x + p) is:

f (x + p) ≈ f (x) + ∇f (x)(cid:62)p +

1
2

p(cid:62)∇2f (x∗) p.

(193)

Substituting this into the Lagrangian gives:

L = f (x) + ∇f (x)(cid:62)p +

1
2

p(cid:62)∇2f (x∗) p

+ ν(cid:62)(A(x + p) − b).

According to Eqs. (64) and (71) in KKT conditions, the
primal and dual residuals must be zero:

∇xL = ∇f (x) + ∇2f (x)(cid:62)p + p(cid:62) ∇3f (x∗)
(cid:125)

(cid:124)

(cid:123)(cid:122)
≈ 0

p

+ A(cid:62)ν set= 0 =⇒ ∇2f (x)(cid:62)p + A(cid:62)ν = −∇f (x),

∇ν L = A(x + p) − b

(a)
= Ap set= 0,

(194)

(195)

where we have ∇3f (x∗) ≈ 0 because the gradient of func-
tion at the optimal point vanishes according to Eq.
(18)
and (a) is because of the constraint Ax − b = 0 in prob-
lem (191). Eqs. (194) and (195) can be written as a system
of equations:

(cid:20)∇2f (x)(cid:62) A(cid:62)
0

A

(cid:21) (cid:20)p
ν

(cid:21)

(cid:21)

(cid:20)−∇f (x)
0

.

=

(196)

Solving this system of equations gives the desired step p
(i.e., ∆x) for updating the solution at the iteration.

– Starting with Non-feasible Initial Point: Newton’s
method can even start with a non-feasible point which does
not satisfy all the constraints. If the initial point for opti-
mization is not a feasible point, i.e., Ax−b (cid:54)= 0, Eq. (195)
becomes:

∇ν L = A(x + p) − b set= 0 =⇒ Ap = −(Ax − b).

(197)

Hence, for the ﬁrst iteration, we solve the following system
rather than Eq. (196):

(cid:20)∇2f (x)(cid:62) A(cid:62)
0

A

(cid:21) (cid:20)p
ν

(cid:21)

= −

(cid:21)

(cid:20) ∇f (x)
Ax − b

,

(198)

and we use Eq. (198) for the rest of iterations because the
next points will be in the feasibility set (because we force
the solutions to satisfy Ax = b).

7.4. Interior-Point and Barrier Methods: Newton’s

Method for Inequality Constrained Optimization
The optimization problem may have inequality constraints:

minimize
x

f (x)

subject to yi(x) ≤ 0,

i ∈ {1, . . . , m1},

(199)

Ax = b.

We can solve constrained optimization problems using
Barrier methods, also known as interior-point methods
(Nesterov & Nemirovskii, 1994; Potra & Wright, 2000;
Boyd & Vandenberghe, 2004; Wright, 2005). Interior-point
methods were ﬁrst proposed by (Dikin, 1967). The interior-
point method is also referred to as the Unconstrained Min-
imization Technique (UMT) or Sequential UMT (SUMT)
(Fiacco & McCormick, 1967) because it converts the prob-
lem to an unconstrained problem and solves it iteratively.
The barrier methods or the interior-point methods, convert
inequality constrained problems to equality constrained or
unconstrained problems.
Ideally, we can do this conver-
sion using the indicator function I(.) which is zero if its
input condition is satisﬁed and is inﬁnity otherwise (n.b.
the indicator function in optimization literature is not like
the indicator in data science which is one if its input con-
dition is satisﬁed and is zero otherwise). The problem is
converted to:

minimize
x

f (x) +

m1(cid:88)

i=1

I(yi(x) ≤ 0)

subject to Ax = b.

(200)

The indicator function is not differentiable because it is not
smooth:

Hence, we can approximate it with differentiable functions
called the barrier functions (Boyd & Vandenberghe, 2004;
Nesterov, 2018). A barrier function is logarithm, named the
logarithmic barrier or log barrier in short. It approximates
the indicator function by:

I(yi(x) ≤ 0) ≈ −

1
t

log(−yi(x)),

(202)

36

where t > 0 (usually a large number such as t = 106) and
the approximation becomes more accurate by t → ∞. It
changes the problem to:

minimize
x

f (x) −

1
t

m1(cid:88)

i=1

log(−yi(x))

(203)

subject to Ax = b.

This optimization problem is an equality constrained op-
timization problem which we already explained how to
solve. Note that there exist many approximations for the
barrier. One of mostly used methods is the logarithmic bar-
rier.
The iterative solutions of the interior-point method satisfy
Eq. (53) and follow Fig. 8. If the optimization problem
is a convex problem, the solution of interior-point method
is the global solution; otherwise, the solution is local. The
interior-point and barrier methods are used in many opti-
mization toolboxes such as CVX (Grant et al., 2009).

– Accuracy of the log barrier method: In the following
theorem, we discuss the accuracy of the log barrier method.

Theorem 8 (On the sub-optimality of log-barrier method).
Let the optimum of problems (199) and (203) be denoted by
f ∗ and f ∗

r , respectively. We have:

f ∗ −

m1
t

≤ f ∗

r ≤ f ∗,

(204)

meaning that the optimum of problem (203) is no more than
m1/t from the optimum of problem (199).

Proof. Proof is available in Appendix C.1.

Theorem 8 indicates that by t → ∞, the log-barrier method
is more accurate; i.e., the solution of problem (203) is more
accurately close to the solution of problem (199). This is
expected because the approximation in Eq. (202) gets more
accurate by increasing t. Note that by increasing t, opti-
mization gets more accurate but harder to solve and slower
to converge.

7.5. Wolfe Conditions and Line-Search in Newton’s

In Sections 5.1.2 and 5.1.3, we introduced line-search for
gradient descent. We have line-search for second-order
optimization, too. Line-search for second-order optimiza-
tion checks two conditions. These conditions are called
the Wolfe conditions (Wolfe, 1969). For ﬁnding the suit-
able step size η(k), at iteration k of optimization, the Wolfe
conditions are checked. Here, we do not include the step
size η in p = ∆x and the step at iteration k is p(k) =
−∇2f (x(k))−1∇f (x(k)) according to Eq.
(189). The

I(yi(x) ≤ 0) :=

(cid:26) 0

if yi(x) ≤ 0
∞ if yi(x) > 0.

(201)

Method

Wolfe conditions are:

f (x(k) + η(k)p(k)) ≤ f (x(k)) + c1η(k)p(k)(cid:62)f (x(k)),

(205)

− p(k)(cid:62)∇f (x(k) + η(k)p(k)) ≤ −c2p(k)(cid:62)f (x(k)),

(206)

where 0 < c1 < c2 < 1 are the parameters of Wolfe con-
ditions. It is recommended in (Nocedal & Wright, 2006)
to have c1 = 10−4 and c2 = 0.9. The ﬁrst condition is
the Armijo condition (Armijo, 1966) which ensures the step
size η(k) decreases the function value sufﬁciently. The sec-
ond condition is the curvature condition which ensures the
step size η(k) decreases the function slope sufﬁciently. In
quasi-Newton’s method (introduced later in Section 7.7),
the curvature condition makes sure the approximation of
Hessian matrix remains positive deﬁnite. The Armijo and
curvature conditions give an upper-bound and lower-bound
on the step size, respectively. There also exists a strong
curvature condition:

|p(k)(cid:62)∇f (x(k) + η(k)p(k))| ≤ c2|p(k)(cid:62)f (x(k))|, (207)

which can be used instead of the curvature condition. Note
that the Wolfe conditions can also be used for line-search
in ﬁrst-order methods.

7.6. Fast Solving System of Equations in Newton’s

Method

In unconstrained Newton’s method, the update of solution
which is Eq.
(190) is in the form of a system of linear
equations. In constrained Newton’s method (and hence, in
the interior-point method), the update of solution which is
Eq. (196) is also in the form of a system of linear equations.
Therefore, every iteration of all optimization methods is
reduced to a system of equations such as:

M z = q,

(208)

where we need to calculate z. Therefore, the optimization
toolboxes, such as CVX (Grant et al., 2009), solve a sys-
tem of equations to ﬁnd the solution at every iteration. If
the dimensionality of data or the number of constraints is
large, the number of equations and the size of matrices in
the system of equations increase. Solving the large system
of equations is very time-consuming. Hence, some meth-
ods have been developed to accelerate solving the system
of equations. Here, we review some of these methods.

37

– LU decomposition: We use LU decomposition to de-
compose M = P LU . We have:

= q,

M z = P L U z
(cid:124)(cid:123)(cid:122)(cid:125)
=w2
(cid:124) (cid:123)(cid:122) (cid:125)
=w1

where we deﬁne w2 := U z and w1 := Lw2. Hence, we
can solve the system of equations as:

1. LU decomposition: M = P LU

2. permutation: Solve P w1 = q to ﬁnd w1

3. forward subtraction: Solve Lw2 = w1 to ﬁnd w2

4. backward subtraction: Solve U z = w2 to ﬁnd z

– Cholesky decomposition: In most cases of optimization,
the coefﬁcient matrix is positive deﬁnite. for example, in
Eq. (190), the coefﬁcient matrix is the Hessian which is
positive deﬁnite. Therefore, we can use Cholesky decom-
position to decompose M = LL(cid:62). We have:

M z = L L(cid:62)z
(cid:124) (cid:123)(cid:122) (cid:125)
=w1

= q,

where we deﬁne w1 := L(cid:62)z. Hence, we can solve the
system of equations as:

1. Cholesky decomposition: M = LL(cid:62)

2. forward subtraction: Solve Lw1 = q to ﬁnd w1

3. backward subtraction: Solve L(cid:62)z = w1 to ﬁnd z

– Schur complement: Assume the system of equations
can be divided into blocks:

M z =

(cid:20)M 11 M 12
M 21 M 22

(cid:21) (cid:20)z1
z2

(cid:21)

=

(cid:21)

(cid:20)q1
q2

.

From this, we have:

M 11z1 + M 12z2 = q1 =⇒ z1 = M −1
M 21z1 + M 22z2 = q2
=⇒ M 21(M −1
=⇒ (M 22 − M 21M −1

11 (q1 − M 12z2)) + M 22z2 = q2

11 M 12)z2 = q2 − M 21M −1

11 q1.

11 (q1 − M 12z2).

7.6.1. DECOMPOSITION METHODS
We can use various matrix decomposition/factorization
methods (Golub & Van Loan, 2013) for decomposing the
coefﬁcient matrix M and solving the system of equations
(208). We review some of them here.

The term (M 22 − M 21M −1
11 M 12) is the Schur comple-
ment (Zhang, 2006) of block matrix M 11 in matrix M .
We assume that the block matrix M 11 is not singular so its
inverse exists. We use the Schur complement to solve the
system of equations as:

1. Calculate M −1

11 M 12 and M −1
11 q

2. Calculate (cid:102)M := M 22 − M 21(M −1
11 q1)

q2 − M 21(M −1

11 M 12) and (cid:101)q :=

3. Solve (cid:102)M z2 = (cid:101)q (as derived above) to ﬁnd z2
4. Solve M 11z1 = q1 − M 12z2 (as derived above) to

ﬁnd z1

7.6.2. CONJUGATE GRADIENT METHOD
The conjugate gradient method, proposed in (Hestenes &
Stiefel, 1952), iteratively solves Eq. (208) faster than the
regular solution. Its pacing shows off better when the ma-
trices are very large. A good book on conjugate gradient
It is noteworthy that trun-
is (Kelley, 1995, Chapter 2).
cated Newton’s methods (Nash, 2000), which approximate
the Hessian for large-scale optimization, usually use conju-
gate gradient as their approximation method for calculating
the search direction.
Deﬁnition 36 (Conjugate vectors). Two non-zero vectors
x and y are conjugate if x(cid:62)M y = 0 where M (cid:31) 0.
Deﬁnition 37 (Krylov subspace (Krylov, 1931)). The
order-r Krylov subspace, denoted by Kr, is spanned by the
following bases:

38

4

5

1 Initialize: z(0)
2 r(0) := −∇f (z(0)) = q − M z(0), p(0) := r(0)
3 for iteration k = 0, 1, . . . do
η(k) = p(k)(cid:62)r(k)
p(k)(cid:62)M p(k)
z(k+1) := z(k) + η(k)p(k)
r(k+1) := r(k) − η(k)M p(k)
if (cid:107)r(k+1)(cid:107)2 is small then
Break the loop.
β(k+1) := r(k+1)(cid:62)r(k+1)
p(k+1) := r(k+1) + β(k+1)p(k)

r(k)(cid:62)r(k) = (cid:107)r(k+1)(cid:107)2

(cid:107)r(k)(cid:107)2
2

9

7

8

6

2

10
11 Return z(k+1)

Algorithm 3: The conjugate gradient method to
solve Eq. (208).

Initially, the direction is this residual, p(0) = r(0) as in
gradient descent (see Eq. (84)). If we take derivative of
f (z(k+1))

(210)
= f (z(k) + η(k)p(k)) w.r.t. η(k), we have:

∂
∂η(k)

f (z(k) + η(k)p(k)) set= 0

Kr(M , q) = span{q, M q, M 2q, . . . , M r−1q}.

(209)

=⇒ η(k) =

p(k)(cid:62)(q − M x(k))
p(k)(cid:62)M p(k)

(211)
=

p(k)(cid:62)r(k)
p(k)(cid:62)M p(k)

.

(208) should satisfy z = M −1q.
The solution to Eq.
Therefore, the solution to Eq.
(208) lies in the Krylov
subspace. The conjugate gradient method approximates
this solution lying in the Krylov subspace. Every iteration
of conjugate gradient can be seen as projection onto the
Krylov subspace.
According to Eq. (18), the solution to Eq. (208) minimizes
the function:

f (z) =

1
2

z(cid:62)M z − z(cid:62)q,

because Eq.
(208) is the gradient of this function, i.e.,
∇f (z) = M z − q. conjugate gradient iteratively solves
Eq. (208) as:

z(k+1) := z(k) + η(k)p(k).

(210)

It starts by moving toward minus gradient as gradient de-
scent does. Then, it uses the conjugate of gradient. This is
the reason for the name of this method.
At iteration k, the residual (error) for fulﬁlling Eq. (208)
is:

r(k) := q − M z(k) = −∇f (z(k)).

(211)

We also have:

r(k+1) − r(k) (211)

= q − M z(k+1) − q + M z(k)

(210)
= M (−z(k) − η(k)p(k) + z(k)) = −η(k)M p(k).

The conjugate gradient method is shown in Algorithm 3.
As this algorithm shows, the direction of update p is found
by a linear combination of the residual (which was initial-
ized by negative gradient as in gradient descent) and the
previous direction. The weight of previous direction in this
linear combination is β which gets smaller if the residual of
this step is much smaller than the residual of previous iter-
ation. This formula for β = (r(k+1)(cid:62)r(k+1))/(r(k)(cid:62)r(k))
is also used in the Fletcher-Reeves nonlinear conjugate gra-
dient method (Fletcher & Reeves, 1964), introduced in the
next section. The conjugate gradient method returns a z as
an approximation to the solution to Eq. (208).

7.6.3. NONLINEAR CONJUGATE GRADIENT METHOD
As we saw, conjugate gradient is for solving linear equa-
tions. Nonlinear Conjugate Gradient (NCG) generalizes
conjugate gradient to nonlinear functions. Recall that con-
jugate gradient solves Eq. (208):

M z = q =⇒ M (cid:62)M z = M (cid:62)q

=⇒ 2M (cid:62)(M z − q) = 0.

The goal of NCG is to ﬁnd the minimum of a quadratic
function:

f (z) = (cid:107)M z − q(cid:107)2
2,

(212)

using its gradient. The gradient of this function is ∇f (z) =
2M (cid:62)(M z−q) which was found above. The NCG method

1 Initialize: z(0)
2 r(0) := −∇f (z(0)), p(0) := r(0)
3 for iteration k = 0, 1, . . . do
r(k+1) := −∇f (z(k+1))
4
Compute β(k+1) by one of Eqs. (213)
p(k+1) := r(k+1) + β(k+1)p(k)
η(k+1) := arg minη f (z(k+1) + η p(k+1))
z(k+1) := z(k) + η(k+1)p(k+1)

6

5

7

8
9 Return z(k+1)

Algorithm 4: The NCG method to ﬁnd the mini-
mum of Eq. (212).

is shown in Algorithm 4 which is very similar to Algorithm
3. It uses steepest descent for updating the solution (see Eq.
(3)). The direction for update is found by a linear combi-
nation of the residual, initialized by negative gradient as
in gradient descent, and the direction of previous iteration.
Several formulas exist for the weight β for the previous di-
rection in the linear combination:

β(k+1)
1

:=

r(k+1)(cid:62)r(k+1)
r(k)(cid:62)r(k)

=

(cid:107)r(k+1)(cid:107)2
2
(cid:107)r(k)(cid:107)2
2

,

β(k+1)
2

:=

r(k+1)(cid:62)(r(k+1) − r(k))
r(k)(cid:62)r(k)

,

β(k+1)
3

:= −

β(k+1)
4

:= −

r(k+1)(cid:62)(r(k+1) − r(k))
p(k)(cid:62)(r(k+1) − r(k))
r(k)(cid:62)r(k)
p(k)(cid:62)(r(k+1) − r(k))

.

,

(213)

The β1, β2, β3, and β4 are the formulas for Fletcher-Reeves
(Fletcher & Reeves, 1964), Polak-Ribi`ere (Polak & Ri-
biere, 1969), Hestenes-Stiefel (Hestenes & Stiefel, 1952),
and Dai-Yuan (Dai & Yuan, 1999) methods, respectively.
In all these formulas, β gets smaller if the residual of next
iteration gets much smaller than the previous residual. The
NCG method returns a z as an approximation to the mini-
mizer of the nonlinear function in Eq. (212).

7.7. Quasi-Newton’s Methods
7.7.1. HESSIAN APPROXIMATION
Similar to what we did in Eq. (188), we can approximate
the function at the updated solution by its second-order
Taylor series:

f (x(k+1)) = f (x(k) + p(k))

≈ f (x(k)) + ∇f (x(k))(cid:62)p(k) +

1
2

p(k)(cid:62)B(k)p(k).

where p = ∆x is the step size and B(k) = ∇2f (x(k)) is
the Hessian matrix at iteration k. Taking derivative from

39

this equation w.r.t. p gives:

∇f (x(k) + p(k)) ≈ ∇f (x(k)) + B(k)p(k).

(214)

This equation is called the secant equation. Setting this
derivative to zero, for optimization, gives:

B(k)p(k) = −∇f (x(k)) =⇒ p(k) = −H (k)∇f (x(k)),
(215)

where H (k) := (B(k))−1 is the inverse of Hessian ma-
trix. This equation is the previously found Eqs. (189) and
(190). Note that although the letter H seems to be used for
Hessian, literature uses H to denote the (approximation of)
inverse of Hessian. Considering the step size, we can write
the step s(k) as:

Rd (cid:51) s(k) := ∆x = x(k+1) − x(k)

= −η(k)H (k)∇f (x(k)),

(216)

where the step size is found by the Wolfe conditions in line-
search (see Section 7.5).
Computation of the Hessian matrix or its inverse is usu-
ally expensive in Newton’s method. One way to approxi-
mate the Newton’s solution at every iteration is the conju-
gate gradient method, which was introduced before. An-
other approach for approximating the solution is the quasi-
Newton’s methods which approximate the (inverse) Hes-
sian matrix. The quasi-Newton’s methods approximate
Hessian based on the step p(k), the difference of gradients
between iterations:

Rd (cid:51) y(k) := ∇f (x(k+1)) − ∇f (x(k)),

(217)

and the previous approximated Hessian B(k) or its inverse
H (k).
Some papers approximate the Hessian by a diagonal matrix
(Lee & Verleysen, 2007, Appendix C.1), (Andrei, 2019). In
this situation, the inverse of approximated Hessian is sim-
ply the inverse of its diagonal elements. Some methods use
a dense approximation for Hessian matrix. Examples for
these are BFGS, DFP, Broyden, and SR1 methods, which
will be introduced in the following. Some other methods,
such as LBFGS introduced later, approximate the Hessian
matrix only by a scalar.

7.7.2. QUASI-NEWTON’S ALGORITHMS
The most well-known algorithm for quasi-Newton’s
method is Broyden-Fletcher-Goldfarb-Shanno (BFGS)
(Fletcher, 1987; Dennis Jr & Schnabel, 1996). Limited-
memory BFGS (LBFGS) (Nocedal, 1980; Liu & Nocedal,
1989) is a simpliﬁed version of BFGS which utilizes
less memory. Some other quasi-Newton’s methods are
Davidon-Fletcher-Powell (DFP) (Davidon, 1991; Fletcher,
1987), Broyden method (Broyden, 1965), and Symmetric

Rank-one (SR1) (Conn et al., 1991). In the following, we
review the approximations of Hessian matrix and its inverse
by different methods. More explanation on these methods
can be found in (Nocedal & Wright, 2006, Chapter 6).
We deﬁne:

R (cid:51) ρ(k) :=

1
y(k)(cid:62)s(k)

,

Rd×d (cid:51) V (k) := I − ρ(k)y(k)s(k)(cid:62) = I −

(218)

y(k)s(k)(cid:62)
y(k)(cid:62)s(k)

,

(219)

5

6

7

8

9

10

where I is the identity matrix.
– BFGS: The approximations in BFGS method are:

B(k+1) := B(k) + ρ(k)y(k)y(k)(cid:62)

−

B(k)s(k)s(k)(cid:62)B(k)(cid:62)
s(k)(cid:62)B(k)s(k)

,

(220)

H (k+1) := V (k)(cid:62)H (k)V (k) + ρ(k)s(k)s(k)(cid:62).

– DFP: The approximations in DFP method are:

B(k+1) := V (k)B(k)V (k)(cid:62) + ρ(k)y(k)y(k)(cid:62),
H (k+1) := H (k) + ρ(k)s(k)s(k)(cid:62)

−

H (k)y(k)y(k)(cid:62)H (k)(cid:62)
y(k)(cid:62)H (k)y(k)

.

(221)

– Broyden: The approximations in Broyden method are:

B(k+1) := B(k) +

(y(k) − B(k)s(k)) s(k)(cid:62)
s(k)(cid:62)s(k)

,

H (k+1) := H (k) +

(s(k) − H (k)y(k))s(k)(cid:62)H (k)
s(k)(cid:62)H (k)y(k)

.

(222)

– SR1: The approximations in SR1 method are:

B(k+1) := B(k) +

H (k+1) := H (k) +

(y(k) − B(k)s(k))(y(k) − B(k)s(k))(cid:62)
(y(k) − B(k)s(k))(cid:62)s(k)
(s(k) − H (k)y(k))(s(k) − H (k)y(k))(cid:62)
(s(k) − H (k)y(k))(cid:62)y(k)

,

.

(223)
Comparing Eqs. (220) and (221) shows that the BFGS and
DFP methods are dual of each other. Experiments have
shown that BFGS often outperforms DFP (Avriel, 2003).

– LBFGS: The above methods, including BFGS, approxi-
mate the inverse Hessian matrix by a dense (d × d) matrix.
For large d, storing this matrix is very memory-consuming.
Hence, LBFGS (Nocedal, 1980; Liu & Nocedal, 1989) was
proposed which uses much less memory than BFGS. In
LBFGS, the inverse of Hessian is a scalar multiplied to

40

1 Initialize the solution x(0)
2 H (0) :=
3 for k = 0, 1, . . . (until convergence) do
4

1
(cid:107)∇f (x(0))(cid:107)2

I

p(k) ← GetDirection(−∇f (x(k)), k, 1)
η(k) ← Line-search with Wolfe conditions
x(k+1) := x(k) − η(k)p(k)
s(k) := x(k+1) − x(k) = η(k)p(k)
y(k) := ∇f (x(k+1)) − ∇f (x(k))
γ(k+1) := s(k)(cid:62)y(k)
y(k)(cid:62)y(k)
H (k+1) := γ(k+1)I
Store y(k), s(k), and H (k+1)

11
12 return x(k+1)
13
14 // recursive function:
15 Function GetDirection(p, k, n recursion)
16 if k > 0 then
17

// do up to m recursions:
if n recursion > m then

18

19

20

21

22

23

return p

1
ρ(k−1) :=
y(k−1)(cid:62)s(k−1)
˜p := p − ρ(k−1)(s(k−1)(cid:62)p)y(k−1)
(cid:98)p := GetDirection( ˜p, k − 1, n recursion + 1)
return (cid:98)p − ρ(k−1)(y(k−1)(cid:62)
ρ(k−1)(s(k−1)(cid:62)s(k−1))p

(cid:98)p)s(k−1) +

24 else

25

return H (0)p

Algorithm 5: The LBFGS algorithm

identity matrix, i.e., H (0) := γ(k)I; therefore, it approxi-
mates the (d × d) matrix with a scalar. It uses a memory
of m previous variables and recursively calculates the up-
dating direction of solution. In other words, it has recursive
unfoldings which approximate the descent directions in op-
timization. The number of recursions is a small integer, for
example m = 10; hence, not much memory is used. By
m times recursion on Eq. (220), LBFGS approximates the
inverse Hessian as (Liu & Nocedal, 1989, Algorithm 2.1):

H (k+1) := (V (k)(cid:62) . . . V (k−m)(cid:62))H (0)(V (k−m) . . . V (k))
+ ρ(k−m)(V (k)(cid:62) . . . V (k−m+1)(cid:62))s(k−m)s(k−m)(cid:62)
(V (k−m+1) . . . V (k))

+ ρ(k−m+1)(V (k)(cid:62) . . . V (k−m+2)(cid:62))s(k−m+1)s(k−m+1)(cid:62)
(V (k−m+2) . . . V (k)) + · · · + ρ(k)s(k)s(k)(cid:62).

The LBFGS algorithm can be implemented as shown in Al-
gorithm 5 (Hosseini & Sra, 2020a) which is based on (No-
cedal & Wright, 2006, Chapter 6). As this algorithm shows,
every iteration of optimization calls a recursive function for

up to m recursions and uses the stored previous m memory
to calculate the direction p for updating the solution. As
Eq. (215) states, the initial updating direction is the New-
ton’s method direction, which is p = −H (0)∇f (x0).

8. Non-convex Optimization by Sequential

Convex Programming

Consider the optimization problem (25) where the func-
tions f (.), yi(.), and hi(.) are not necessarily convex. The
explained methods can also work for non-convex problems
but they do not guarantee to ﬁnd the global optimum. They
can ﬁnd local minimizers which depend on the random
initial solution. For example, the optimization landscape
of neural network is highly nonlinear but backpropagation
(see Section 5.1.8) works very well for it. The reason for
this is explained in this way: every layer of neural network
pulls data to the feature space such as in kernels (Gho-
jogh et al., 2021). In the high-dimensional feature space,
all local minimizers are almost global minimizers because
the local minimum values are almost equal in that space
(Feizi et al., 2017). Also see (Soltanolkotabi et al., 2018;
Allen-Zhu et al., 2019a;b) to understand why backpropa-
gation optimization works well even in highly non-convex
optimization. Note that another approach for highly non-
convex optimization is metaheuristic optimization which
will be brieﬂy introduced in Section 10.5.
As was explained, the already introduced ﬁrst-order and
second-order optimization methods can work fairly well for
non-convex problems by ﬁnding local minimizers depend-
ing on the initial solution. However, there exist some spe-
ciﬁc methods for non-convex programming, divided into
two categories. The local optimization methods are faster
but do not guarantee to ﬁnd the global minimizer. The
global optimization methods ﬁnd the global minimizer but
are usually slow to ﬁnd the answer (Duchi et al., 2018). Se-
quential Convex Programming (SCP) (Dinh & Diehl, 2010)
is an example for local optimization methods. It is based
on a sequence of convex approximations of the non-convex
problem. It is closely related to Sequential Quadratic Pro-
gramming (SQP) (Boggs & Tolle, 1995) which is used
for constrained nonlinear optimization. Branch and bound
method, ﬁrst proposed in (Land & Doig, 1960), is an ex-
ample for the global optimization methods. It divides the
optimization landscape, i.e. the feasible set, into local parts
by a binary tree and solves optimization in every part. It
checks whether the solution of a part is the global solution
or not. In the following, we explain SCP which is a faster
but local method.

8.1. Convex Approximation
SCP iteratively solves a convex problem where, at every it-
eration, it approximates the non-convex problem (25) with
a convex problem, based on the current solution, and re-

41

stricts the variable to be in a so-called trust region (Conn
et al., 2000). The trust region makes sure that the vari-
able stays in a locally convex region of the optimization
problem. At the iteration k of SCP, we solve the following
convex problem:

minimize
x

(cid:98)f (x)

subject to (cid:98)yi(x) ≤ 0, i ∈ {1, . . . , m1},
(cid:98)hi(x) = 0, i ∈ {1, . . . , m2},
x ∈ T (k),

(224)

where (cid:98)f (.), (cid:98)yi(.), and (cid:98)hi(.), are convex approximations of
functions f (.), yi(.), and hi(.), and T (k) is the trust region
at iteration k. This approximated convex problem is also
solved iteratively itself using one of the previously intro-
duced methods such as the interior-point method. There
exist several approaches for convex approximation of the
functions.
In the following, we introduce some of these
approaches.

8.1.1. CONVEX APPROXIMATION BY TAYLOR SERIES

EXPANSION

The non-convex functions f (.), yi(.), and hi(.) can be ap-
proximated by afﬁne functions (i.e., ﬁrst-order Taylor se-
ries expansion) to become convex. For example, the func-
tion f (.) is approximated as:

(cid:98)f (x) = f (x(k)) + ∇f (x(k))(cid:62)(x − x(k)).

(225)

The functions can also be approximated by quadratic func-
tions (i.e., second-order Taylor series expansion) to become
convex. For example, the function f (.) is approximated as:

(cid:98)f (x) = f (x(k)) + ∇f (x(k))(cid:62)(x − x(k))
1
2

(x − x(k))(cid:62)P (x − x(k)),

+

(226)

+

(∇2f (x(k))) is projection of Hessian onto
where P = ΠSd
the symmetric positive semi-deﬁnite cone. This projection
is performed by setting the negative eigenvalues of Hessian
to zero. The same approaches can be used for approxima-
tion of functions yi(.) and hi(.) using ﬁrst- or second-order
Taylor expansion.

8.1.2. CONVEX APPROXIMATION BY PARTICLE

METHOD

We can approximate the functions f (.), yi(.), and hi(.) in
the domain of trust region using regression. This approach
is named the particle method (Duchi et al., 2018). Let
{xi ∈ T (k)}m
i=1 be m points which lie in the trust region.
We can use least-squares quadratic regression to make the

functions convex in the trust region:

minimize
a∈R,b∈Rd,P ∈Sd

++

m
(cid:88)

i=1

(cid:16) 1
2

(xi − x(k))(cid:62)P (xi − x(k))

+ b(cid:62)(xi − x(k)) + a − f (xi)

(cid:17)2

subject to

P (cid:23) 0.

(227)
Then, the function f (.) is replaced by its convex approxi-
mation (cid:98)f (x) = (1/2)(xi −x(k))(cid:62)P (xi −x(k))+b(cid:62)(xi −
x(k)) + a. The same approach can be used for approxima-
tion of functions yi(.) and hi(.).

8.1.3. CONVEX APPROXIMATION BY
QUASI-LINEARIZATION

Another approach for convex approximation of functions
f (.), yi(.), and hi(.) is quasi-linearization. We should state
the function f (.) in the form f (x) = A(x) x + c(x). For
example, we can use the second-order Taylor series expan-
sion to do this:

f (x) ≈

1
2

x(cid:62)P x + b(cid:62)x + a = (

1
2

P x + b)(cid:62)x + a,

so we use A(x) := ((1/2)P x + b)(cid:62) and c(x) := a which
depend on the Taylor expansion of f (x). Hence, the con-
vex approximation of function f (.) can be:

(cid:98)f (x) = A(x(k)) x + c(x(k)) = (

1
2

P x(k) + b)(cid:62)x + a.

(228)

The same approach can be used for approximation of func-
tions yi(.) and hi(.).

8.2. Trust Region
8.2.1. FORMULATION OF TRUST REGION
The trust region can be a box around the point at that itera-
tion:

T (k) := {x | |xj − x(k)

j

| ≤ ρi, ∀j ∈ {1, . . . , d}}.

(229)

j

where xj and x(k)
are the j-th element of x and x(k), re-
spectively, and ρi is the bound of box for the j-th dimen-
sion. Another option for trust region is an ellipse around
the point to have a quadratic trust region:

T (k) := {x | (x − x(k))(cid:62)P −1(x − x(k)) ≤ ρ},

(230)

where P ∈ Sd
is the radius of ellipse.

++ (is symmetric positive deﬁnite) and ρ > 0

8.2.2. UPDATING TRUST REGION
The trust region gets updated in every iteration of SCP. In
the following, we explain how the trust region can be up-

42

dated. First, we embed the constraints in the objective func-
tion of problem (25):

minimize
x

φ(x) := f (x) + λ

(cid:16) m1(cid:88)

(cid:0) max(yi(x), 0)(cid:1)2

i=1

m1(cid:88)

+

|hi(x)|2(cid:17)

,

i=1

(231)
where λ > 0 is the regularization parameter. This is called
the exact penalty method (Di Pillo, 1994) because it pe-
nalizes violation from the constraints. For large enough
regularization parameter (which gives importance to viola-
tion of constraints), the solution of problem (231) is exactly
equal to the solution of problem (25). That is the reason for
the term “exact” in the name “exact penalty method”. Sim-
ilar to Eq. (231), we deﬁne:

(cid:98)φ(x) := (cid:98)f (x) + λ

(cid:16) m1(cid:88)

(cid:0) max((cid:98)yi(x), 0)(cid:1)2

i=1

+

m1(cid:88)

i=1

|(cid:98)hi(x)|2(cid:17)

,

(232)

for the problem (224). At the iteration k of SCP, let (cid:98)x(k)
be the solution of the convex approximated problem (224)
using any method such as the interior-point method. We
calculate the predicted and exact decreases which are (cid:98)δ :=
φ(x(k)) − (cid:98)φ((cid:98)x) and δ := φ(x(k)) − φ((cid:98)x), respectively.
Two cases may happen:

• We have progress in optimization if α(cid:98)δ ≤ δ where
0 < α < 1 (e.g., α = 0.1). In this case, we accept
the approximate solution, i.e. x(k+1) := (cid:98)x, and we
increase the size of trust region, for the next iteration
of SCP, by ρ(k+1) := βρ(k) where β ≥ 1 (e.g., β =
1.1).

• We do not have progress in optimization if α(cid:98)δ > δ.
In this case, we reject the approximate solution, i.e.
x(k+1) := x(k), and we decrease the size of trust re-
gion, for the next iteration of SCP, by ρ(k+1) := γρ(k)
where 0 < γ < 1 (e.g., γ = 0.5).

In summary, the trust region is expanded if we ﬁnd a good
solution; otherwise, it is made smaller.

9. Distributed Optimization
9.1. Alternating Optimization
When we have several optimization variables, we can alter-
nate between optimizing over each of these variables. This
technique is called alternating optimization in the literature
(Li et al., 2019) (also see (Jain & Kar, 2017, Chapter 4)).
Consider the following multivariate optimization problem:

minimize
{xi}m

i=1

f (x1, . . . , xm),

(233)

where the objective function depends on m variables. Al-
ternating optimization alternates between updating every
variable while assuming other variables are constant, set to
their last updated value. After random feasible initializa-
tion, it updates solutions as (Li et al., 2019):

...
x(k+1)
m

(cid:16)

:= arg min
xm

f (x(k+1)
1

, x(k+1)
2

, . . . , x(k+1)

m−1 , xm)

+

1
2λ

(cid:107)xm − x(k)

m (cid:107)2
2

(cid:17)

.

43

x(k+1)
1

x(k+1)
2

...
x(k+1)
m

:= arg min
x1

:= arg min
x2

f (x1, x(k)

2 , . . . , x(k)

m−1, x(k)

m ),

f (x(k+1)
1

, x2, . . . , x(k)

m−1, x(k)

m ),

:= arg min
xm

f (x(k+1)
1

, x(k+1)
2

, . . . , x(k+1)

m−1 , xm),

until convergence. Any optimization methods, including
ﬁrst-order and second-order methods, can be used for each
of the optimization lines above. In most cases, alternating
optimization is robust to changing the order of updates of
variables.

Remark 4. If the function f (x1, . . . , xm) is decomposable
in terms of variables, i.e., if we have f (x1, . . . , xm) =
(cid:80)m
i=1 fi(xi), the alternating optimization can be simpli-

ﬁed to:

x(k+1)
1

x(k+1)
2
...
x(k+1)
m

:= arg min
x1

f1(x1),

:= arg min
x2

f2(x2),

:= arg min
xm

fm(xm),

because other terms become constant in optimization. The
above updates mean that if the function is completely de-
composable in terms of variables, the updates of variables
are independent and can be done independently. Hence,
in that case, alternating optimization is reduced to m in-
dependent optimization problems, each of which can be
solved by any optimization method such as the ﬁrst-order
and second-order methods.

Proximal alternating optimization uses proximal operator,
Eq. (139), for minimization to keep the updated solution
close to the solution of previous iteration (Li et al., 2019):

x(k+1)
1

:= arg min
x1

(cid:16)

f (x1, x(k)

+

1
2λ
f (x(k+1)
1

x(k+1)
2

:= arg min
x2

(cid:16)

2 , . . . , x(k)
m−1, x(k)
m )
(cid:17)

(cid:107)x1 − x(k)

1 (cid:107)2
2

,

The alternating optimization methods can also be used for
constrained problems:

minimize
{xi}m

i=1

f (x1, . . . , xm)

subject to xi ∈ Si,

∀i ∈ {1, . . . , m}.

(234)

In this case, every line of the optimization is a constrained
problem:

x(k+1)
1

:= arg min
x1

(cid:16)
f (x1, x(k)

2 , . . . , x(k)
(cid:17)

m−1, x(k)

m ),

s.t. x1 ∈ S1

,

x(k+1)
2

:= arg min
x2

(cid:16)

f (x(k+1)
1

, x2, . . . , x(k)

m−1, x(k)

m ),

s.t. x2 ∈ S2

(cid:17)

,

...
x(k+1)
m

:= arg min
xm

(cid:16)
f (x(k+1)
1

, x(k+1)
2

, . . . , x(k+1)

m−1 , xm),

s.t. xm ∈ Sm

(cid:17)

.

Any constrained optimization methods can be used for each
of the optimization lines above. Some examples are pro-
jected gradient method, proximal methods, interior-point
methods, etc.
Finally, it is noteworthy that practical experiments have
shown there is usually no need to use a complete optimiza-
tion until convergence for every step in the alternating op-
timization, either unconstrained or constrained. Often, a
single step of updating, such as a step of gradient descent
or projected gradient method, is enough for the whole al-
gorithm to work.

9.2. Dual Ascent and Dual Decomposition Methods
Consider the following problem:

minimize
x

f (x)

subject to Ax = b.

(235)

We follow the method of multipliers as discussed in Section
4.8. The Lagrangian is:

L(x, ν) = f (x) + ν(cid:62)(Ax − b).

, x2, . . . , x(k)

m−1, x(k)
m )

The dual function is:

+

1
2λ

(cid:107)x2 − x(k)

2 (cid:107)2
2

(cid:17)

,

g(ν) = inf
x

L(x, ν).

(236)

The dual problem maximizes g(ν):

ν∗ = arg max

ν

g(ν),

so the optimal primal variable is:

x∗ = arg min
x

L(x, ν∗).

(237)

(238)

For solving Eq. (237), we should take the derivative of the
dual function w.r.t. the dual variable:

∇ν g(ν)

(236)
= ∇ν (inf
x

L(x, ν))

(238)
= ∇ν (f (x∗) + ν(cid:62)(Ax∗ − b)) = Ax∗ − b.

The dual problem is a maximization problem so we can use
gradient ascent (see Section 5.1) for iteratively updating the
dual variable with this gradient. We can alternate between
updating the optimal primal and dual variables:

x(k+1) := arg min
x

L(x, ν(k)),

ν(k+1) := ν(k) + η(k)(Ax(k+1) − b),

(239)

(240)

where k is the iteration index and η(k) is the step size (also
called the learning rate) at iteration k. Eq. (239) can be
performed by any optimization method. We compute the
gradient of L(x, ν(k)) w.r.t. x. If setting this gradient to
zero does not give x in closed form, we can use gradient
descent (see Section 5.1) to perform Eq. (239). Some pa-
pers approximate Eq. (239) by one step or few steps of gra-
dient descent rather than a complete gradient descent until
convergence. If using one step, we can write Eq. (239) as:

we can have b Lagrangian functions where the total La-
grangian is the summation of these functions:

44

Li(xi, ν) = f (xi) + ν(cid:62)(Axi − b),

L(xi, ν) =

b
(cid:88)

i=1

(cid:0)f (xi) + ν(cid:62)(Axi − b)(cid:1).

We can divide the Eq. (239) into b updates, each for one of
the blocks.

x(k+1)
i

:= arg min
xi

L(x, ν(k)),

∀i ∈ {1, . . . , b}, (242)

ν(k+1) := ν(k) + η(k)(Ax(k+1) − b).

(243)

This is called dual decomposition developed by decompo-
sition techniques such as the Dantzig-Wolfe decomposition
(Dantzig & Wolfe, 1960), Bender’s decomposition (Ben-
ders, 1962), and Lagrangian decomposition (Everett III,
1963). The dual decomposition methods can divide a prob-
lem into sub-problems and solve them in parallel. Hence,
its can be used for big data but they are usually slow to
converge.

9.3. Augmented Lagrangian Method (Method of

Multipliers)

Assume we regularize the objective function in Eq. (235)
by a penalty on not satisfying the constraint:

minimize
x

ρ
2
subject to Ax = b,

f (x) +

(cid:107)Ax − b(cid:107)2
2

(244)

where ρ > 0 is the regularization parameter.

x(k+1) := x(k) − γ∇xL(x, ν(k)),

(241)

Deﬁnition 38 (Augmented Lagrangian (Hestenes, 1969;
Powell, 1969)). The Lagrangian for problem (244) is:

where γ > 0 is the step size. It has been shown empirically
that even one step of gradient descent for Eq. (239) works
properly for the whole alternating algorithm.
We continue the iterations until convergence of the pri-
mal and dual variables to stable values. When we get
closer to convergence, we will have (Axk+1 − b) → 0
so that we will not have update of dual variable according
to Eq. (240). This means that after convergence, we have
(Axk+1 − b) ≈ 0 so that the constraint in Eq. (235) is get-
ting satisﬁed. In other words, the update of dual variable in
Eq. (240) is taking care of satisfying the constraint. This
method is known as the dual ascent method because it uses
gradient ascent for updating the dual variable.
If the objective function can be distributed and decomposed
on b blocks {xi}b

i=1, i.e.:

f (x) = f1(x1) + · · · + f1(xb),

Lρ(x, ν) := f (x) + ν(cid:62)(Ax − b) +

ρ
2

(cid:107)Ax − b(cid:107)2
2.
(245)

This Lagrangian is called the augmented Lagrangian for
problem (235).

We can use this augmented Lagrangian in Eqs. (239) and
(240):

x(k+1) := arg min
x
ν(k+1) := ν(k) + ρ(Ax(k+1) − b),

Lρ(x, ν(k)),

(246)

(247)

where we use ρ for the step size of updating the dual vari-
able. This method is called the augmented Lagrangian
method or the method of multipliers (Hestenes, 1969; Pow-
ell, 1969; Bertsekas, 1982).

9.4. Alternating Direction Method of Multipliers

(ADMM)

ADMM (Gabay & Mercier, 1976; Glowinski & Marrocco,
1976; Boyd et al., 2011) has been used in many recent
machine learning and signal processing papers. The use-
fulness and goal for using ADMM (and other distributed
methods) are two-fold: (1) it makes the problem distributed
and parallelizable on several servers and (2) it makes it pos-
sible to solve an optimization problem with multiple vari-
ables.

9.4.1. ADMM ALGORITHM
Consider the following problem:

minimize
x1,x2

f1(x1) + f2(x2)

subject to Ax1 + Bx2 = c,

(248)

which is an optimization over two variables x1 and x2. The
augmented Lagrangian for this problem is:

ρ
2

Lρ(x1, x2, ν) = f1(x1) + f2(x2)
+ ν(cid:62)(Ax1 + Bx2 − c) +

(cid:107)Ax1 + Bx2 − c(cid:107)2
2.
(249)
We can alternate between updating the primal variables x1
and x2 and the dual variable ν until convergence of these
variables:
x(k+1)
1

Lρ(x1, x(k)

2 , ν(k)),

(250)

:= arg min
x1

x(k+1)
2

:= arg min
x2

Lρ(x(k+1)
1

, x2, ν(k)),

(251)

ν(k+1) := ν(k) + ρ(Ax(k+1)

1

+ Bx(k+1)
2

− c).

(252)

Note that the order of updating primal and dual variables
is important and the dual variable should be updated af-
ter the primal variables but the order of updating primal
variables is not important. This method is called the Alter-
nating Direction Method of Multipliers (ADMM) (Gabay
& Mercier, 1976; Glowinski & Marrocco, 1976). A good
survey on ADMM is (Boyd et al., 2011).
As was explained before, Eqs. (250) and (251) can be per-
formed by any optimization method such as calculating the
gradient of augmented Lagrangian w.r.t. x1 and x2, respec-
tively, and using a few (or even one) iterations of gradient
descent for each of these equations.

9.4.2. SIMPLIFYING EQUATIONS IN ADMM
The last term in the augmented Lagrangian, Eq. (249), can
be restated as:

ν(cid:62)(Ax1 + Bx2 − c) +

ρ
2
= ν(cid:62)(Ax1 + Bx2 − c) +

(cid:107)Ax1 + Bx2 − c(cid:107)2
2

ρ
2

(cid:107)Ax1 + Bx2 − c(cid:107)2
2

+

1
2ρ

(cid:107)ν(cid:107)2

2 −

1
2ρ

(cid:107)ν(cid:107)2
2

45

2

1
ρ2 (cid:107)ν(cid:107)2
(cid:17)
1
−
2ρ

=

(cid:16)

ρ
2

(cid:107)Ax1 + Bx2 − c(cid:107)2

2 +

ν(cid:62)(Ax1 + Bx2 − c)

2
ρ
(cid:13)
(cid:13)Ax1 + Bx2 − c +

1
ρ
(cid:13)Ax1 + Bx2 − c + u(cid:13)
(cid:13)
2
2 −
(cid:13)

ν(cid:13)
2
2 −
(cid:13)
1
2ρ

+

(a)
=

(b)
=

ρ
2
ρ
2

(cid:107)ν(cid:107)2
2

1
2ρ

(cid:107)ν(cid:107)2
2

(cid:107)ν(cid:107)2
2.

where (a) is because of the square of summation of two
terms and (b) is because we deﬁne u := (1/ρ)ν. The last
term −(1/(2ρ))(cid:107)ν(cid:107)2
2 is constant w.r.t. to the primal vari-
ables x1 and x2 so we can drop that term fro Lagrangian
when updating the primal variables. Hence, the Lagrangian
can be restated as:

Lρ(x1, x2, u) = f1(x1) + f2(x2)
(cid:13)Ax1 + Bx2 − c + u(cid:13)
(cid:13)
2
2 + constant.
(cid:13)

+

ρ
2

(253)

For updating x1 and x2, the terms f2(x2) and f (x1) are
constant, respectively, and can be dropped (because here
arg min is important and not the minimum value). Hence,
Eqs. (250), (251), and (252) can be restated as:

f1(x1)

:= arg min
x1
(cid:13)
(cid:13)Ax1 + Bx(k)

+

2 − c + u(k)(cid:13)
2
(cid:13)
2

(cid:17)

,

(254)

x(k+1)
1

x(k+1)
2

(cid:16)

(cid:16)

ρ
2

ρ
2

f2(x2)

:= arg min
x2
(cid:13)
(cid:13)Ax(k+1)

+

1

+ Bx2 − c + u(k)(cid:13)
2
(cid:13)
2

(cid:17)

,

(255)

u(k+1) := u(k) + ρ(Ax(k+1)

1

+ Bx(k+1)
2

− c).

(256)

(254) and (255) can be performed by one
Again, Eqs.
or few steps of gradient descent or any other optimization
method. The convergence of ADMM for non-convex and
non-smooth functions has been analyzed in (Wang et al.,
2019).

9.5. ADMM Algorithm for General Optimization
Problems and Any Number of Variables

9.5.1. DISTRIBUTED OPTIMIZATION
ADMM can be extended to several equality and inequal-
ity constraints for several optimization variables (Giesen &
Laue, 2016; 2019). Consider the following optimization
problem with m optimization variables and an equality and
inequality constraint for every variable:

minimize
{xi}m

i=1

m
(cid:88)

i=1

fi(xi)

subject to yi(xi) ≤ 0, i ∈ {1, . . . , m},
hi(xi) = 0, i ∈ {1, . . . , m}.

(257)

We can convert every inequality constraint to equality con-
straints by this technique (Giesen & Laue, 2016; 2019):

yi(xi) ≤ 0 ≡ y(cid:48)

i(xi) := (cid:0)max(0, yi(xi))(cid:1)2

= 0.

Hence, the problem becomes:

m
(cid:88)

fi(xi)

minimize
{xi}m

i=1

subject to y(cid:48)

i=1
i(xi) = 0, i ∈ {1, . . . , m},
hi(xi) = 0, i ∈ {1, . . . , m}.

Having dual variables λ = [λ1, . . . , λm](cid:62) and ν =
[ν1, . . . , νm](cid:62) and regularization parameter ρ > 0, the aug-
mented Lagrangian for this problem is:

Lρ({xi}m

i=1, ν(cid:48), ν) =

m
(cid:88)

i=1

fi(xi)

m
(cid:88)

λiy(cid:48)

i(xi) +

m
(cid:88)

i=1

νihi(xi)

+

+

i=1

η
2

m
(cid:88)

i=1

(y(cid:48)

i(xi))2 +

ρ
2

m
(cid:88)

i=1

(hi(xi))2

(258)

46

ρ
2

(cid:107)h(x)(cid:107)2
2

be restated as:

λ(cid:62)y(cid:48)(x) + ν(cid:62)h(x) +

(cid:107)y(cid:48)(x)(cid:107)2

2 +

= λ(cid:62)y(cid:48)(x) +

2 +

(cid:107)λ(cid:107)2

2 −

(cid:107)λ(cid:107)2
2

+ ν(cid:62)h(x) +

(cid:107)h(x)(cid:107)2

2 +

(cid:107)ν(cid:107)2

2 −

(cid:107)ν(cid:107)2
2

ρ
2
(cid:107)y(cid:48)(x)(cid:107)2

ρ
2
ρ
2

1
2ρ
1
2ρ
(cid:17)

1
2ρ
1
2ρ
2
ρ
2
ρ

λ(cid:62)y(cid:48)(x)

−

(cid:107)λ(cid:107)2
2

ν(cid:62)h(x)

(cid:17)

−

(cid:107)ν(cid:107)2
2

1
2ρ
1
2ρ

=

+

=

+

(cid:16)

(cid:16)

ρ
2
ρ
2
ρ
2
ρ
2
ρ
2

(cid:107)y(cid:48)(x)(cid:107)2

2 +

(cid:107)h(x)(cid:107)2

2 +

(cid:13)
(cid:13)y(cid:48)(x) +

(cid:13)
(cid:13)h(x) +

2 +

2 +

1
ρ2 (cid:107)λ(cid:107)2
1
ρ2 (cid:107)ν(cid:107)2
1
λ(cid:13)
2
2 −
(cid:13)
2ρ
1
2ρ
ρ
2

1
ρ
1
ν(cid:13)
2
2 −
(cid:13)
ρ

(cid:13)
2
2 +
(cid:13)

(cid:107)λ(cid:107)2
2

(cid:107)ν(cid:107)2
2

(a)
=

(cid:13)
(cid:13)y(cid:48)(x) + uλ

(cid:13)
(cid:13)h(x) + uν

(cid:13)
2
2 − constant,
(cid:13)

where (a) is because we deﬁne uλ := (1/ρ)λ and uν :=
(1/ρ)ν. Hence, the Lagrangian can be restated as:

Lρ({xi}m

i=1, uλ, uν) =

m
(cid:88)

fi(xi)+

(cid:13)
(cid:13)y(cid:48)(x) + uλ
m
(cid:88)

fi(xi) +

(cid:13)
2
2 +
(cid:13)

i=1
ρ
2
(cid:2)(y(cid:48)

+

ρ
2
m
(cid:88)

i=1

=

(cid:13)
(cid:13)h(x) + uν

(cid:13)
2
2 + constant
(cid:13)

i(xi) + uλ,i)2

ρ
2
+ (hi(xi) + uν,i)2(cid:3) + constant,

i=1

=

m
(cid:88)

i=1
ρ
2

+

fi(xi) + λ(cid:62)y(cid:48)(x) + ν(cid:62)h(x)

(cid:107)y(cid:48)(x)(cid:107)2

2 +

ρ
2

(cid:107)h(x)(cid:107)2
2,

m(xm)](cid:62) and Rm (cid:51)
where Rm (cid:51) y(cid:48)(x) := [y(cid:48)
h(x) := [h1(x1), . . . , hm(xm)](cid:62). Updating the primal
and dual variables are performed as (Giesen & Laue, 2016;
2019)::

1(x1), . . . , y(cid:48)

x(k+1)
i

Lρ(xi, λ(k)

:= arg min
xi
λ(k+1) := λ(k) + ρ y(cid:48)(x(k+1)),
ν(k+1) := ν(k) + ρ h(x(k+1)).

i

, ν(k)
i

), ∀i ∈ {1, . . . , m},

Note that as the Lagrangian is completely decomposable by
the i indices, the optimization for every i-th primal or dual
variable does not depend on other indices; in other words,
the terms of other indices become constant for every index.
The last terms in the augmented Lagrangian, Eq. (258), can

where uλ,i = (1/ρ)λi and uν,i = (1/ρ)νi are the i-th el-
ements of uλ and uν, respectively. Hence, updating vari-
ables can be restated as:

x(k+1)
i

u(k+1)
λ,i
u(k+1)
ν,i

(cid:16)

:= arg min
xi
+ (hi(xi) + u(k)

ρ
fi(xi) +
2
ν,i )2(cid:3)(cid:17)
i(x(k+1)
λ,i + ρ y(cid:48)
ν,i + ρ hi(x(k+1)

:= u(k)
:= u(k)

i

i

(cid:2)(y(cid:48)

i(xi) + u(k)

λ,i )2

, ∀i ∈ {1, . . . , m}, (259)

), ∀i ∈ {1, . . . , m}

), ∀i ∈ {1, . . . , m}.

(260)

(261)

– Use of ADMM for Distributed Optimization: ADMM
is one of the most well-known algorithms for distributed
optimization.
If the problem can be divided into several
disjoint blocks (i.e., several primal variables), we can solve
the optimization for each primal variable on a separate core
or server (see Eq. (259) for every i). Hence, in every it-
eration of ADMM, the update of primal variables can be
performed in parallel by distributed servers. At the end of
each iteration, the updated primal variables are gathered
in a central server so that the update of dual variable(s) is
performed (see Eqs. (260) and (261)). Then, the updated

dual variable(s) is sent to the distributed servers so they up-
date their primal variables. This procedure is repeated until
convergence of primal and dual variables.
In this sense,
ADMM is performed similar to the approach of federated
learning (Koneˇcn`y et al., 2015; Li et al., 2020).

We can embed the constraint in the objective function using
an indicator function:

minimize
{x}m

i=1

m
(cid:88)

i=1

(cid:0)fi(xi) + φi(xi)(cid:1),

9.5.2. MAKING OPTIMIZATION PROBLEM

DISTRIBUTED

where φi(xi) := I(xi ∈ Si) is zero if xi ∈ Si and is
inﬁnity otherwise. This problem can be stated as:

47

(263)

u(k+1)
i

We can convert a non-distributed optimization problem to a
distributed optimization problem to solve it using ADMM.
Many recent machine learning and signal processing papers
are using this technique.

– Univariate optimization problem: Consider a regular
non-distributed problem with one optimization variable x:

minimize
x

m
(cid:88)

i=1

fi(x)

subject to yi(x) ≤ 0, i ∈ {1, . . . , m},
hi(x) = 0, i ∈ {1, . . . , m}.

(262)

This problem can be stated as:

minimize
{xi}m

i=1

m
(cid:88)

i=1

fi(xi)

subject to yi(xi) ≤ 0, i ∈ {1, . . . , m},
hi(xi) = 0, i ∈ {1, . . . , m},
xi = z, i ∈ {1, . . . , m},

where we introduce m variables {xi}m
i=1 and use the trick
xi = z, ∀i to make them equal to one variable. Eq. (263)
is similar to Eq. (257) except that it has 2m equality con-
straints rather than m equality constraints. Hence, we can
use ADMM updates similar to Eqs. (259), (260), and (261)
but with slight change because of the additional m con-
straints. We introduce m new dual variables for constraints
xi = z, ∀i and update those dual variables as well as other
variables. The augmented Lagrangian also has some addi-
tional terms for the new constraints. We do not write down
the Lagrangian and ADMM updates because of its similar-
ity to the previous equations. This is a good technique to
make a problem distributed, use ADMM for solving it, and
solving it in parallel servers.

– Multivariate optimization problem: Consider a regular
non-distributed problem with multiple optimization vari-
ables {xi}m

i=1:

minimize
{x}m

i=1

m
(cid:88)

i=1

fi(xi)

subject to xi ∈ Si, i ∈ {1, . . . , m},

(264)

where xi ∈ Si can be any constraint such as belonging to
a set Si, an equality constraint, or an inequality constraint.

minimize
{xi}m

i=1

m
(cid:88)

i=1

(cid:0)fi(xi) + φi(zi)(cid:1)

subject to xi = zi, i ∈ {1, . . . , m},

(265)

where we introduce a variable zi for every xi, use the in-
troduced variable for the second term in the objective func-
tion, and we equate them in the constraint.
As the constraints xi − zi = 0, ∀i are equality constraints,
we can use Eqs. (254), (255), and (256) as ADMM updates
for this problem:

x(k+1)
i

:= arg min
xi

(cid:16)

fi(xi) +

z(k+1)
i

:= arg min
zi

(cid:16)

φi(zi) +

i + u(k)

i

(cid:17)

,

(cid:13)
2
(cid:13)
2

(cid:13)
(cid:13)xi − z(k)

ρ
2
∀i ∈ {1, . . . , m},
ρ
2
∀i ∈ {1, . . . , m},

(cid:13)
(cid:13)x(k+1)

i

− zi + u(k)

i

(266)
(cid:17)
(cid:13)
2
(cid:13)
2

,

(267)

:= u(k)

i + ρ(x(k+1)

i

+ z(k+1)
i

), ∀i ∈ {1, . . . , m}.

Comparing Eqs.
(139) shows
that these ADMM updates can be written as proximal map-
pings:

(266) and (267) with Eq.

x(k+1)
i
z(k+1)
i

u(k+1)
i

:= prox 1

ρ fi

(z(k)

i − u(k)

), ∀i ∈ {1, . . . , m},

:= prox 1

ρ φi

(x(k+1)
i

i
+ u(k)
i

), ∀i ∈ {1, . . . , m},

(268)

:= u(k)

i + ρ(x(k+1)

i

+ z(k+1)
i

), ∀i ∈ {1, . . . , m},

i

i (cid:107)2

i (cid:107)2

2 = (cid:107)zi − x(k+1)

− zi + u(k)
if we notice that (cid:107)x(k+1)
−
u(k)
2. Note that in many papers, such as (Otero et al.,
2018), we only have m = 1. In that case, we only have two
primal variables x and z.
According to Lemma 15, as the function φi(.) is an indica-
tor function, Eq. (268) can be implemented by projection
onto the set Si:

i

z(k+1)
i

:= ΠSi(x(k+1)

i

+ u(k)
i

), ∀i ∈ {1, . . . , m}.

As an example, assume the variables are all matrices so we
have X i, Zi, and U i. if the set Si is the cone of orthogonal
matrices, the constraint X i ∈ Si would be X (cid:62)
i X i = I. In
this case, the update of matrix variable Zi would be done
by setting the singular values of (x(k+1)
) to one (see
Lemma 19).

+u(k)
i

i

10. Additional Notes
There exist some other optimization methods which we
have not covered in this paper, for brevity. Here, we re-
view some of them.

10.1. Cutting-Plane Methods
Cutting plane methods, also called the localization meth-
ods, are a family of methods which start with a large feasi-
ble set containing the solution to be found. Then, iteratively
they reduce the feasible set by cutting off some piece of it
(Boyd & Vandenberghe, 2007). For example, a cutting-
plane method starts with a polyhedron feasible set. It ﬁnds
a plane at every iteration which divides the feasible sets
into two disjoint parts one of which contains the ultimate
solution.
It gets rid of the part without solution and re-
duces the volume of the polyhedron. This is repeated until
the polyhedron feasible set becomes very small and con-
verges to the solution. This is somewhat a generalization of
the bisection method, also called the binary search method,
which was used for root-ﬁnding (Burden & Faires, 1963)
but later it was used for convex optimization. The bisection
method halves the feasible set and removes the part without
the solution, at every iteration (see Algorithm 6). Some of
the important cutting-plane methods are center of gravity
method, Maximum Volume Ellipsoid (MVE) cutting-plane
method, Chebyshev center cutting-plane method, and An-
alytic Center Cutting-Plane Method (ACCPM) (Gofﬁn &
Vial, 1993; Nesterov, 1995; Atkinson & Vaidya, 1995).
Similar to subgradient methods, cutting-plane methods can
be used for optimizing non-smooth functions.

10.2. Ellipsoid Method
Ellipsoid method was developed by several people (Wolfe,
1980; Rebennack, 2009). It was proposed in (Shor, 1977;
Yudin & Nemirovski, 1976; 1977a;b) and it was ini-
tially applied to liner programming in a famous paper
(Khachiyan, 1979). It is similar to cutting-plane methods
in cutting some part of feasible set iteratively. At every it-
eration, it ﬁnds an ellipsoid centered at the current solution:

E(x(k), P ) := {z | (z − x(k))(cid:62)P −1(z − x(k)) ≤ 1},

++ (is symmetric positive deﬁnite).

where P ∈ Sd
It re-
moves half of the ellipsoid which does not contain the so-
lution. Again, another ellipsoid is found at the updated so-
lution. This is repeated until the ellipsoid of iteration is
very small and converges to the solution.

10.3. Minimax and Maximin Problems
Consider a function of two variables, f (x1, x2), and the
following optimization problem:

minimize
x1

(cid:16)

maximize
x2

(cid:17)

f (x1, x2)

.

(269)

48

1 Input: l and u
2 for iteration k = 0, 1, . . . do
3

x(k+1) := l+u
2
if ∇f (x) < 0 then

4

5

6

7

8

9

10

l := x

else

u := x

Check the convergence criterion
if converged then
return x(k+1)

Algorithm 6: The bisection algorithm

In this problem, we want to minimize the function w.r.t.
one of the variables and maximize it w.r.t the other variable.
This optimization problem is called the minimax problem.
We can change the order of this problem to have the so-
called maximin problem:

maximize
x1

(cid:16)

minimize
x2

(cid:17)

f (x1, x2)

.

(270)

Note that under certain conditions, the minimax and max-
imin problems are equivalent if the variables of maximiza-
tion (or minimization) stay the same. In other words, under
some conditions, we have (Du & Pardalos, 2013):

minimize
x1

(maximize
x2

f (x1, x2))

= maximize
x2

(minimize
x1

f (x1, x2)).

In the minimax and maximin problems, the two variables
have conﬂicting or contrastive desires; one of them wants
to maximize the function while the other wants to minimize
it. Hence, they are widely used in the ﬁeld of game theory
as important strategies (Aumann & Maschler, 1972).

10.4. Riemannian Optimization
In this paper, we covered optimization methods in the Eu-
clidean space. The Euclidean optimization methods can be
slightly revised to have optimization on (possibly curvy)
Riemannian manifolds. Riemannian optimization (Absil
et al., 2009; Boumal, 2020) optimizes a cost function while
the variable lies on a Riemannian manifold M. The opti-
mization variable in the Riemannian optimization is usually
matrix rather than vector; hence, Riemannian optimization
is also called optimization on matrix manifolds. The Rie-
mannian optimization problem is formulated as:

minimize
X

f (X)

subject to X ∈ M.

(271)

Most of the Euclidean ﬁrst-order and second-order opti-
mization methods have their Riemannian optimization vari-
ants by some changes in the formulation of methods. In the

Riemannian optimization methods, the solution lies on the
manifold. At every iteration, the descent direction is cal-
culated in the tangent space on the manifold at the current
solution. Then, the updated solution in the tangent space is
retracted (i.e., projected) onto the curvy manifold (Hosseini
& Sra, 2020b; Hu et al., 2020). This procedure is repeated
until convergence to the ﬁnal solution. Some well-known
Riemannian manifolds which are used for optimization are
Symmetric Positive Deﬁnite (SPD) (Sra & Hosseini, 2015),
quotient (Lee, 2013), Grassmann (Bendokat et al., 2020),
and Stiefel (Edelman et al., 1998) manifolds.

10.5. Metaheuristic Optimization
In this paper, we covered classical optimization methods.
There are some other optimization methods, called meta-
heuristic optimization (Talbi, 2009), which are a family of
methods ﬁnding the optimum of a cost function using efﬁ-
cient, and not brute-force, search. They fall in the ﬁeld of
soft computing and can be used in highly non-convex opti-
mization with many constraints, where classical optimiza-
tion is a little difﬁcult and slow to perform. Some very well-
known categories of the Metaheuristic optimization meth-
ods are nature-inspired optimization (Yang, 2010), evolu-
tionary computing (Simon, 2013), and particle-based opti-
mization. The general shared idea of metaheuristic meth-
ods is as follows. We perform local search by some parti-
cles in various parts of the feasible set. Wherever a smaller
cost was found, we tend to do more local search in that area;
although, we should also keep exploring other areas be-
cause that better area might be just a local optimum. Hence,
both local and global searches are used for exploitation and
exploration of the cost function, respectively.
There exist many metaheuristic methods. Two popular
and fundamental ones are Genetic Algorithm (GA) (Hol-
land et al., 1992) and Particle Swarm Optimization (PSO)
(Kennedy & Eberhart, 1995). GA, which is an evolutionary
method inspired by natural selection, uses chromosomes as
particles where the particles tend to cross-over (or marry)
with better particles in terms of smaller cost function. This
results in a better next generation of particles. Mutations
are also done for global search and exploration. Iterations
of making new generations results in very good particles
ﬁnding the optimum.
PSO is a nature-inspired method whose particles can be
seen as a herd ﬁshes or birds. For better understanding,
assume particles are humans digging ground in a vast area
to ﬁnd treasure. They ﬁnd various things which differ in
terms of value. When someone ﬁnds a roughly good thing,
people tend to move toward that person bu also tend to
search more than before, around themselves at the same
time. Hence, they move in a combined direction tending a
little more toward the better found object. The fact that they
still dig around themselves is that the found object may not
be the real treasure (i.e., may be a local optimum) so they

49

also search around for the sake of exploration.

11. Conclusion
This paper was a tutorial and survey paper on KKT condi-
tions, numerical optimization (both ﬁrst-order and second-
order methods), and distributed optimization. We covered
various optimization algorithms in this paper. Reading it
can be useful for different people in different ﬁelds of sci-
ence and engineering. We did not assume much on the
background of reader and explained methods in detail.

Acknowledgement
The authors hugely thank Prof. Stephen Boyd for his great
courses Convex Optimization 1 and 2 of Stanford Univer-
sity available on YouTube (The course Convex Optimiza-
tion 1 mostly focuses on second-order and interior-point
methods and the course Convex Optimization 2 focuses on
more advanced non-convex optimization, non-smooth opti-
mization, ellipsoid method, distributed optimization, prox-
imal algorithms, and some ﬁrst-order methods). They also
thank Prof. Kimon Fountoulakis, Prof. James Geelen, Prof.
Oleg Michailovich, Prof. Massoud Babaie-Zadeh, Prof.
Lieven Vandenberghe, Prof. Mark Schmidt, Prof. Ryan
Tibshirani, Prof. Reshad Hosseini, Prof. Saeed Shariﬁan,
and some other professors whose lectures partly covered
some materials and proofs mentioned in this tutorial paper.
The great books of Prof. Yurii Nesterov (Nesterov, 1998;
2003) were also inﬂuential on some of proofs.

A. Proofs for Section 2
A.1. Proof for Lemma 5

f (y)

(11)
= f (x) + ∇f (x)(cid:62)(y − x)
(cid:90) 1

(cid:16)
∇f (cid:0)x + t(y − x)(cid:1) − ∇f (x)

+

(cid:17)(cid:62)

(y − x)dt

0

(a)
≤ f (x) + ∇f (x)(cid:62)(y − x)

+

(cid:90) 1

0

(cid:107)∇f (cid:0)x + t(y − x)(cid:1) − ∇f (x)(cid:107)2(cid:107)y − x(cid:107)2dt

(b)
≤ f (x) + ∇f (x)(cid:62)(y − x) +

(cid:90) 1

0

Lt(cid:107)y − x(cid:107)2

2dt

= f (x) + ∇f (x)(cid:62)(y − x) + L(cid:107)y − x(cid:107)2
2

(cid:90) 1

0

tdt

= f (x) + ∇f (x)(cid:62)(y − x) +

L
2

(cid:107)y − x(cid:107)2
2,

where (a) is because of the Cauchy-Schwarz inequality and
(b) is because, according to Eq. (12), we have (cid:107)∇f (x +
t(y − x)) − ∇f (x)(cid:107)2 = L(cid:107)x + t(y − x) − x(cid:107)2 = Lt(cid:107)y −
x(cid:107)2. Q.E.D.

50

A.2. Proof for Lemma 6
– Proof for the ﬁrst equation: As f (.) is convex, accord-
ing to Eq. (5),

A.3. Proof for Lemma 7
Consider any y ∈ D. We deﬁne z := αy + (1 − α)x. We
choose a small enough α to have (cid:107)z − x(cid:107)2 ≤ (cid:15). Hence:

f (z) − f (y) ≥ ∇f (y)(cid:62)(z − y)
=⇒ f (y) − f (z) ≤ ∇f (y)(cid:62)(y − z).

(cid:15) ≥ (cid:107)z − x(cid:107)2 = (cid:107)αy + (1 − α)x − x(cid:107)2 = (cid:107)αy − αx(cid:107)2

(272)

= α(cid:107)y − x(cid:107)2 =⇒ α ≤

(cid:15)
(cid:107)y − x(cid:107)2

.

f (y) − f (x) = (cid:0)f (y) − f (z)(cid:1) + (cid:0)f (z) − f (x)(cid:1).

(273)

Also, according to Eq. (13), we have:

f (z) − f (x) ≤ ∇f (x)(cid:62)(z − x) +

L
2

(cid:107)z − x(cid:107)2
2.

(274)

Using Eqs. (272) and (274) in Eq. (273) gives:

f (y) − f (x) ≤ ∇f (y)(cid:62)(y − z) + ∇f (x)(cid:62)(z − x)

+

L
2

(cid:107)z − x(cid:107)2
2.

As α ∈ [0, 1], we should have 0 ≤ α min((cid:15)/(cid:107)y − x(cid:107)2, 1).
As x is a local minimizer, according to Eq. (16), we have
∃ (cid:15) > 0 : ∀z ∈ D, (cid:107)z − x(cid:107)2 ≤ (cid:15) =⇒ f (x) ≤ f (z).
As the function is convex, according to Eq. (4), we have
f (z) = f (cid:0)αy + (1 − α)x(cid:1) ≤ αf (y) + (1 − α)f (x)
(n.b. we have exchanged the variables x and y in Eq. (4)).
Hence, overall, we have:

f (x) ≤ f (z) ≤ αf (y) + (1 − α)f (x)

=⇒ f (x) − (1 − α)f (x) ≤ αf (y)

=⇒ αf (x) ≤ αf (y) =⇒ f (x) ≤ f (y), ∀y ∈ D.

For this, we can minimize this upper-bound (the right-hand
side) by setting its derivative w.r.t. z to zero. It gives:

So, x is the global minimizer. Q.E.D.

z = x −

1
L

(cid:0)∇f (x) − ∇f (y)(cid:1).

Putting this in the upper-bound gives:

f (y) − f (x) ≤ ∇f (y)(cid:62)(y − x)

+

−

+

1
L
1
L
L
2

∇f (y)(cid:62)(∇f (x) − ∇f (y))

∇f (x)(cid:62)(∇f (x) − ∇f (y))

1
L2 (cid:107)∇f (x) − ∇f (y)(cid:107)2

2

(a)
= ∇f (y)(cid:62)(y − x) −

1
L
(cid:107)∇f (x) − ∇f (y)(cid:107)2
2

+

1
2L

(cid:107)∇f (x) − ∇f (y)(cid:107)2
2

= ∇f (y)(cid:62)(y − x) −

1
2L

(cid:107)∇f (x) − ∇f (y)(cid:107)2
2,

were (a) is because (cid:107)∇f (x) − ∇f (y)(cid:107)2
∇f (y))(cid:62)(∇f (x) − ∇f (y)).
– Proof for the second equation:
points x and y in Eq. (14), we have:

2 = (∇f (x) −

If we exchange the

A.4. Proof for Lemma 8
– Proof for side (x∗ is minimizer =⇒ ∇f (x∗) = 0):
According to the deﬁnition of directional derivative, we
have:

∇f (x)(cid:62)(y − x) = lim
t→0

f (x + t(y − x)) − f (x)
t

.

For a minimizer x∗, we have f (x∗) ≤ f (x∗ + t(y − x∗))
because it minimizes f (.) for a neighborhood around it
(n.b. x∗ + t(y − x∗) is a neighborhood of x∗ because t
tends to zero). Hence, we have:

0 ≤ lim
t→0

f (x∗ + t(y − x∗)) − f (x∗)
t
= ∇f (x∗)(cid:62)(y − x∗).

As y can be any point in the domain D, we can choose it to
be y = x∗ − ∇f (x∗) so we have ∇f (x∗) = x∗ − y and
therefore:

0 ≤ ∇f (x∗)(cid:62)(y − x∗) = −∇f (x∗)(cid:62)∇f (x∗)

= −(cid:107)∇f (x∗)(cid:107)2

2 ≤ 0 =⇒ ∇f (x∗) = 0.

f (x) − f (y) ≤ ∇f (x)(cid:62)(x − y)

−

1
2L

(cid:107)∇f (y) − ∇f (x)(cid:107)2
2.

(275)

Adding Eqs. (14) and (275) gives:

0 ≤ (∇f (y) − ∇f (x))(cid:62)(y − x) −

1
L

(cid:107)∇f (y) − ∇f (x)(cid:107)2
2.

Q.E.D.

– Proof for side (∇f (x∗) = 0 =⇒ x∗ is minimizer):
As the function f (.) is convex, according to Eq. (5), we
have:

f (y) ≥ f (x∗) + ∇f (x∗)(cid:62)(y − x∗),

∀y ∈ D. As we have ∇f (x∗) = 0, we can say f (y) ≥
f (x∗), ∀y. So, x∗ is the global minimizer.

51

A.5. Proof for Lemma 9
As x∗ is a local minimizer, according to Eq. (16), we have
∃ (cid:15) > 0 : ∀y ∈ D, (cid:107)y − x∗(cid:107)2 ≤ (cid:15) =⇒ f (x∗) ≤ f (y).
Also, by Eq. (11), we have f (y) = f (x∗)+∇f (x∗)(cid:62)(y −
x∗) + o(y − x∗). From these two, we have:

f (x∗) ≤ f (x∗) + ∇f (x∗)(cid:62)(y − x∗) + o(y − x∗)
=⇒ ∇f (x∗)(cid:62)(y − x∗) ≥ 0.

As y can be any point in the domain D, we can choose it to
be y = x∗ − ∇f (x∗) so we have ∇f (x∗) = x∗ − y and
therefore ∇f (x∗)(cid:62)(y − x∗) = −(cid:107)∇f (x∗)(cid:107)2
2 ≥ 0. Hence,
∇f (x∗) = 0. Q.E.D.

B. Proofs for Section 5
B.1. Proof for Lemma 13
Because we halve the step size every time, after τ internal
iterations of line-search, we have:

η(τ ) := (

1
2

)τ η(τ ) = (

1
2

)τ ,

Overall, Eq. (276) becomes:

(t + 1) min
0≤k≤t

(cid:107)∇f (x(k))(cid:107)2

2 ≤ 2L(cid:0)f (x(0)) − f (x(t+1))(cid:1).
(277)

As f ∗ is the minimum of function, we have:

f (x(t+1)) ≥ f ∗
=⇒ 2L(cid:0)f (x(0)) − f (x(t+1))(cid:1) ≤ 2L(cid:0)f (x(0)) − f ∗)(cid:1).

Hence, Eq. (277) becomes:

(t + 1) min
0≤k≤t

(cid:107)∇f (x(k))(cid:107)2

2 ≤ 2L(cid:0)f (x(0)) − f ∗)(cid:1),

which gives Eq. (93). The right-hand side of Eq. (93) is
of the order O(1/t), resulting in Eq. (94). Moreover, for
convergence, we desire:

min
0≤k≤t

(cid:107)∇f (x(k))(cid:107)2

2 ≤ (cid:15)

(93)
=⇒

2L(f (x(0)) − f ∗)
t + 1

≤ (cid:15),

which gives Eq. (95) by re-arranging for t. Q.E.D.

where η(τ ) = 1 is the initial step size. According to Eq.
(87), we have:

B.3. Proof for Theorem 2

η(τ ) = (

1
2

)τ <

=⇒ τ > log 1
2

1
L
1
L

(a)
=⇒ log 1
2

(

1
2

)τ > log 1

2

1
L

=

log 1
L
log 1
2

= −

log 1
L
log 2

= −

1
log 2

(log 1 − log L) =

log L
log 2

.

Q.E.D.

B.2. Proof for Theorem 1
We re-arrange Eq. (88):

(cid:107)x(k+1) − x∗(cid:107)2
2

= (cid:0)x(k) − x∗ −

(a)

= (cid:13)

(cid:13)x(k) − x∗ −

1
L

∇f (x(k))(cid:13)
2
(cid:13)
2

∇f (x(k))(cid:1)(cid:62)(cid:0)x(k) − x∗ −

1
L

∇f (x(k))(cid:1)

1
L

(x(k) − x∗)(cid:62)∇f (x(k))

+

2 −

= (cid:107)x(k) − x∗(cid:107)2

2
L
1
L2 (cid:107)∇f (x(k))(cid:107)2
2,
where (a) is because of Eqs.
according to Eq. (15), we have:

(278)

(79) and (83). Moreover,

(cid:107)∇f (x(k))(cid:107)2

2 ≤ 2L(cid:0)f (x(k)) − f (x(k+1))(cid:1), ∀k,

(cid:0)∇f (x(k)) − ∇f (x∗)(cid:1)(cid:62)

(x(k) − x∗)

=⇒

t
(cid:88)

k=0

(cid:107)∇f (x(k))(cid:107)2

2 ≤ 2L

t
(cid:88)

k=0

(cid:0)f (x(k)) − f (x(k+1))(cid:1).

≥

1
L

(cid:107)∇f (x(k)) − ∇f (x∗)(cid:107)2
2

(276)

(18)
=⇒ ∇f (x(k))(cid:62)(x(k) − x∗) ≥

1
L

(cid:107)∇f (x(k))(cid:107)2
2.

The right-hand side of Eq. (276) is a telescopic summation:

t
(cid:88)

k=0

(cid:0)f (x(k)) − f (x(k+1))(cid:1) = f (x(0)) − f (x(1))

+ f (x(1)) − f (x(2)) + · · · − f (x(t)) + f (x(t))
− f (x(t+1)) = f (x(0)) − f (x(t+1)).

The left-hand side of Eq. (276) is larger than summation of
its smallest term for (t + 1) times:

(t + 1) min
0≤k≤t

(cid:107)∇f (x(k))(cid:107)2

2 ≤

t
(cid:88)

k=0

(cid:107)∇f (x(k))(cid:107)2
2.

=⇒ −

2
L

∇f (x(k))(cid:62)(x(k) − x∗) ≤ −

2
L2 (cid:107)∇f (x(k))(cid:107)2
2.
(279)

Using Eq. (279) in Eq. (278) gives:

(cid:107)x(k+1) − x∗(cid:107)2

2 ≤ (cid:107)x(k) − x∗(cid:107)2

2 −

+

2 = (cid:107)x(k) − x∗(cid:107)2

1
L2 (cid:107)∇f (x(k))(cid:107)2

1
L2 (cid:107)∇f (x(k))(cid:107)2
2.
This shows that at every iteration of gradient descent, the
distance of point to x∗ is decreasing; hence:

2
L2 (cid:107)∇f (x(k))(cid:107)2
2 −

2

(cid:107)x(k) − x∗(cid:107)2

2 ≤ (cid:107)x(0) − x∗(cid:107)2
2.

(280)

As the function is convex, according to Eq. (5), we have:

f (x∗) ≥ f (x(k)) + ∇f (x(k))(cid:62)(x∗ − x(k))
=⇒ f (x(k)) − f (x∗) ≤ ∇f (x(k))(cid:62)(x(k) − x∗)

(a)
≤ (cid:107)∇f (x(k))(cid:107)2(cid:107)x(k) − x∗(cid:107)2
(280)
≤ (cid:107)∇f (x(k))(cid:107)2(cid:107)x(0) − x∗(cid:107)2

=⇒

−1
2L

(cid:0)f (x(k)) − f (x∗)(cid:1)2
(cid:107)x(0) − x∗(cid:107)2
2

≥

−1
2L

(cid:107)∇f (x(k))(cid:107)2
2,

(281)

where (a) is because of the Cauchy-Schwarz inequality.
Also, from Eq. (88), we have:

f (x(k+1)) ≤ f (x(k)) −

1
2L
=⇒ f (x(k+1)) − f (x∗)
1
2L

≤ f (x(k)) − f (x∗) −

(cid:107)∇f (x(k))(cid:107)2
2

(cid:107)∇f (x(k))(cid:107)2
2

(281)
≤ f (x(k)) − f (x∗) −

1
2L

(cid:0)f (x(k)) − f (x∗)(cid:1)2
(cid:107)x(0) − x∗(cid:107)2
2

.

(282)

We deﬁne δk := f (x(k)) − f (x∗) and µ := 1/(2L(cid:107)x(0) −
x∗(cid:107)2

2). According to Eq. (88), we have:

52

C. Proofs for Section 7
C.1. Proof for Theorem 8
The Lagrangian for problem (203) is:

L(x, ν) = f (x) −

1
t

m1(cid:88)

i=1

log(−yi(x)) + ν(cid:62)(Ax − b).

According to Eq. (73), for parameter t, x†(t) minimizes
the Lagrangian:

∇xL(x∗(t), ν) = ∇xf (x∗(t))

m1(cid:88)

+

−1
t yi(x∗(t))
=⇒ ∇xL(x∗(t), ν) = ∇xf (x∗(t))

i=1

∇xyi(x∗(t)) + A(cid:62)ν set= 0

+

m1(cid:88)

i=1

i (t)∇xyi(x∗(t)) + A(cid:62)ν = 0,
λ∗

(284)

i (t) := −1/(t yi(x∗(t))). Let λ∗(t) :=
where we deﬁne λ∗
[λ∗
(t)](cid:62) and let ν∗(t)) be the optimal dual
1(t), . . . , λ∗
m1
variable ν for parameter t. We take integral from Eq. (284)
w.r.t. x to retrieve the Lagrangian again:

L(x, λ∗(t), ν∗(t)) = f (x∗(t))

+

m1(cid:88)

i=1

i (t)∇xyi(x) + ν∗(t)(cid:62)(Ax − b).
λ∗

(285)

δk+1 ≤ δk =⇒ µ

δk
δk+1

≥ µ.

(283)

The dual function is:

g(λ, ν) = inf
x

L(x, λ∗(t), ν∗(t)).

Eq. (282) can be restated as:

The dual problem is:

δk+1 ≤ δk − µ δ2

k =⇒ µ

δk
δk+1

≤

1
δk+1

−

1
δk

(283)
=⇒ µ ≤

1
δk+1

−

1
δk

=⇒ µ

t
(cid:88)

k=0

(1) ≤

t
(cid:88)

(
k=0

1
δk+1

−

1
δk

).

The last term is a telescopic summation; hence:

µ(t + 1) ≤

t
(cid:88)

(
k=0

1
δk+1

−

1
δk

)

=

(a)
≤

1
δ1

−

1
δ0

+

1
δ2

−

1
δ1

1
δt+1

=⇒ δt+1 ≤

+ · · · +

1
µ(t + 1)

1
δt+1

=

1
δt+1

−

1
δ0

=⇒ f (x(t+1)) − f ∗ ≤

2L(cid:107)x(0) − x∗(cid:107)2
2
t + 1

,

where (a) is because δ0 = f (x(0)) − f (x∗) ≥ 0 because
f ∗ is the minimum in the convex function. Q.E.D.

sup
λ,ν

g(λ, ν) = sup
λ,ν

inf
x

L(x, λ∗(t), ν∗(t))

= L(x∗(t), λ∗(t), ν∗(t))

(285)
= f (x∗(t))

+

m1(cid:88)

i=1

i (t)∇xyi(x∗(t)) + ν∗(t)(cid:62)(Ax∗(t) − b
λ∗
)
(cid:125)

(cid:124)

(cid:123)(cid:122)
= 0
m1
t

(a)
= f (x∗(t)) −

1
t

m1(cid:88)

(1) = f (x∗(t)) −

i=1

= f ∗ −

m1
t

,

is because we had deﬁned λ∗

where (a)
i (t) =
−1/(t yi(x∗(t))) and Ax∗(t) − b = 0 because the point is
feasible. According to Eq. (51) for weak duality, we have
supλ,ν g(λ, ν) ≤ f ∗
r is the optimum of problem
(203); hence, f ∗ − (m1/t) ≤ f ∗
r . On the other hand, we
f ∗
r is the optimum, we have:

r where f ∗

f ∗
r

(203)
= f (x∗) −

1
t

m1(cid:88)

i=1

log(−yi(x∗)) ≤ f (x∗) = f ∗.

Hence, overall, Eq. (204) is obtained. Q.E.D.

References
Absil, P-A, Mahony, Robert, and Sepulchre, Rodolphe.
Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.

Bendokat, Thomas, Zimmermann, Ralf, and Absil, P-
A Grassmann manifold handbook: Basic ge-
arXiv preprint

A.
ometry and computational aspects.
arXiv:2011.13699, 2020.

53

Alber, Ya I, Iusem, Alfredo N., and Solodov, Mikhail V. On
the projected subgradient method for nonsmooth convex
optimization in a Hilbert space. Mathematical Program-
ming, 81(1):23–35, 1998.

Allen-Zhu, Zeyuan, Li, Yuanzhi, and Liang, Yingyu.
Learning and generalization in overparameterized neural
networks, going beyond two layers. Advances in neural
information processing systems, 2019a.

Allen-Zhu, Zeyuan, Li, Yuanzhi, and Song, Zhao.
A convergence theory for deep learning via over-
In International Conference on Ma-
parameterization.
chine Learning, pp. 242–252, 2019b.

Andrei, Neculai. A diagonal quasi-Newton updating
method for unconstrained optimization. Numerical Al-
gorithms, 81(2):575–590, 2019.

Armijo, Larry. Minimization of functions having Lipschitz
continuous ﬁrst partial derivatives. Paciﬁc Journal of
mathematics, 16(1):1–3, 1966.

Atkinson, David S and Vaidya, Pravin M. A cutting plane
algorithm for convex programming that uses analytic
centers. Mathematical Programming, 69(1):1–43, 1995.

Aumann, Robert J and Maschler, Michael. Some thoughts
on the minimax principle. Management Science, 18(5-
part-2):54–63, 1972.

Avriel, Mordecai. Nonlinear programming: analysis and

methods. Courier Corporation, 2003.

Banaschewski, Bernhard and Maranda, Jean-Marie. Prox-
imity functions. Mathematische Nachrichten, 23(1):1–
37, 1961.

Bauschke, Heinz H and Borwein, Jonathan M. On projec-
tion algorithms for solving convex feasibility problems.
SIAM review, 38(3):367–426, 1996.

Beck, Amir. First-order methods in optimization. SIAM,

2017.

Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-
thresholding algorithm for
inverse problems.
SIAM journal on imaging sciences, 2(1):183–202, 2009.

linear

Benders, Jacques F. Partitioning procedures for solving
mixed-variables programming problems. Numerische
mathematik, 4(1):238–252, 1962.

Bertsekas, Dimitri P. The method of multipliers for equal-
ity constrained problems. Constrained optimization and
Lagrange multiplier methods, pp. 96–157, 1982.

Boggs, Paul T and Tolle, Jon W. Sequential quadratic pro-

gramming. Acta numerica, 4:1–51, 1995.

Bottou, L´eon, Curtis, Frank E, and Nocedal, Jorge. Op-
timization methods for large-scale machine learning.
SIAM Review, 60(2):223–311, 2018.

Bottou, L´eon et al. Online learning and stochastic approx-
imations. On-line learning in neural networks, 17(9):
142, 1998.

Boumal, Nicolas.

An introduction to optimization on

smooth manifolds. Available online, 2020.

Boyd, Stephen and Mutapcic, Almir. Stochastic subgra-
dient methods. Technical report, Lecture Notes for
EE364b, Stanford University, 2008.

Boyd, Stephen and Vandenberghe, Lieven. Convex opti-

mization. Cambridge university press, 2004.

Boyd, Stephen and Vandenberghe, Lieven. Localization
and cutting-plane methods. Technical report, Stanford
EE 364b lecture notes, 2007.

Boyd, Stephen, Parikh, Neal, and Chu, Eric. Distributed
optimization and statistical learning via the alternating
direction method of multipliers. Now Publishers Inc,
2011.

Broyden, Charles G. A class of methods for solving non-
linear simultaneous equations. Mathematics of compu-
tation, 19(92):577–593, 1965.

Bubeck, S´ebastien. Convex optimization: Algorithms and
complexity. arXiv preprint arXiv:1405.4980, 2014.

Burden, Richard L. and Faires, J. Douglas. Numerical

Analysis. PWS Publishers, 1963.

Chen, Annie I and Ozdaglar, Asuman. A fast distributed
proximal-gradient method. In 2012 50th Annual Allerton
Conference on Communication, Control, and Computing
(Allerton), pp. 601–608. IEEE, 2012.

Chen, Jiabin, Yuan, Rui, Garrigos, Guillaume, and Gower,
SAN: Stochastic average Newton algo-
arXiv preprint

Robert M.
rithm for minimizing ﬁnite sums.
arXiv:2106.10520, 2021.

Chong, Edwin KP and Zak, Stanislaw H. An introduction

to optimization. John Wiley & Sons, 2004.

Chong, Yidong D. Complex methods for the sciences.
Technical report, Nanyang Technological University,
2021.

Conn, Andrew R, Gould, Nicholas IM, and Toint, Ph L.
Convergence of quasi-newton matrices generated by the
symmetric rank one update. Mathematical program-
ming, 50(1):177–195, 1991.

Conn, Andrew R, Gould, Nicholas IM, and Toint,

Philippe L. Trust region methods. SIAM, 2000.

Curry, Haskell B. The method of steepest descent for
non-linear minimization problems. Quarterly of Applied
Mathematics, 2(3):258–261, 1944.

Dai, Yu-Hong and Yuan, Yaxiang. A nonlinear conju-
gate gradient method with a strong global convergence
property. SIAM Journal on optimization, 10(1):177–182,
1999.

Dantzig, George. Linear programming and extensions.

Princeton university press, 1963.

Dantzig, George B. Reminiscences about the origins of
linear programming. In Mathematical Programming The
State of the Art, pp. 78–86. Springer, 1983.

Dantzig, George B and Wolfe, Philip. Decomposition prin-
ciple for linear programs. Operations research, 8(1):
101–111, 1960.

Davidon, William C. Variable metric method for minimiza-
tion. SIAM Journal on Optimization, 1(1):1–17, 1991.

Dennis Jr, John E and Schnabel, Robert B. Numerical
methods for unconstrained optimization and nonlinear
equations. SIAM, 1996.

Di Pillo, Gianni. Exact penalty methods. In Algorithms for
Continuous Optimization, pp. 209–253. Springer, 1994.

Dikin, I.I.

Iterative solution of problems of linear and
In Doklady Akademii Nauk,
quadratic programming.
volume 174, pp. 747–748. Russian Academy of Sci-
ences, 1967.

Dinh, Quoc Tran and Diehl, Moritz. Local convergence
of sequential convex programming for nonconvex op-
In Recent Advances in Optimization and
timization.
its Applications in Engineering, pp. 93–102. Springer,
2010.

Domingos, Pedro. The role of Occam’s razor in knowledge
discovery. Data mining and knowledge discovery, 3(4):
409–425, 1999.

54

Donoho, David L. For most large underdetermined systems
of linear equations the minimal (cid:96)1-norm solution is also
the sparsest solution. Communications on Pure and Ap-
plied Mathematics: A Journal Issued by the Courant In-
stitute of Mathematical Sciences, 59(6):797–829, 2006.

Drummond, LM Grana and Iusem, Alfredo N. A projected
gradient method for vector optimization problems. Com-
putational Optimization and applications, 28(1):5–29,
2004.

Du, Ding-Zhu and Pardalos, Panos M. Minimax and appli-
cations, volume 4. Springer Science & Business Media,
2013.

Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of machine learning research, 12
(7), 2011.

Duchi, John, Boyd, Stephen, and Mattingley, Jacob. Se-
quential convex programming. Technical report, Notes
for EE364b, Stanford University, 2018.

Edelman, Alan, Arias, Tom´as A, and Smith, Steven T. The
geometry of algorithms with orthogonality constraints.
SIAM journal on Matrix Analysis and Applications, 20
(2):303–353, 1998.

Everett III, Hugh. Generalized Lagrange multiplier method
for solving problems of optimum allocation of resources.
Operations research, 11(3):399–417, 1963.

Fan, Ky. Maximum properties and inequalities for the
eigenvalues of completely continuous operators. Pro-
ceedings of the National Academy of Sciences of the
United States of America, 37(11):760, 1951.

Feizi, Soheil, Javadi, Hamid, Zhang, Jesse, and Tse, David.
Porcupine neural networks:(almost) all local optima are
global. arXiv preprint arXiv:1710.02196, 2017.

Fercoq, Olivier and Richt´arik, Peter. Accelerated, paral-
lel, and proximal coordinate descent. SIAM Journal on
Optimization, 25(4):1997–2023, 2015.

Fiacco, Anthony V and McCormick, Garth P. The se-
quential unconstrained minimization technique (SUMT)
without parameters. Operations Research, 15(5):820–
827, 1967.

Fletcher, Reeves and Reeves, Colin M. Function minimiza-
tion by conjugate gradients. The computer journal, 7(2):
149–154, 1964.

Fletcher, Roger. Practical methods of optimization. John

Wiley & Sons, 1987.

Frank, Marguerite and Wolfe, Philip. An algorithm for
quadratic programming. Naval research logistics quar-
terly, 3(1-2):95–110, 1956.

Friedman, Jerome, Hastie, Trevor, Tibshirani, Robert, et al.
The elements of statistical learning, volume 1. Springer
series in statistics New York, 2001.

Gabay, Daniel and Mercier, Bertrand. A dual algorithm for
the solution of nonlinear variational problems via ﬁnite
element approximation. Computers & mathematics with
applications, 2(1):17–40, 1976.

Geman, Stuart and Geman, Donald. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of im-
IEEE Transactions on pattern analysis and ma-
ages.
chine intelligence, (6):721–741, 1984.

Ghojogh, Benyamin and Crowley, Mark.
ory behind overﬁtting, cross validation,
tion, bagging, and boosting:
arXiv:1905.12787, 2019.

The the-
regulariza-
tutorial. arXiv preprint

55

Goodfellow, Ian, Bengio, Yoshua, and Courville, Aaron.

Deep learning. MIT press, 2016.

Gower, Robert M. Convergence theorems for gradient de-
scent. Lecture notes for Statistical Optimization, 2018.

Grant, Michael, Boyd, Stephen, and Ye, Yinyu. CVX: Mat-
lab software for disciplined convex programming, 2009.

Hadamard, Jacques. M´emoire sur le probl`eme d’analyse re-
latif `a l’´equilibre des plaques ´elastiques encastr´ees, vol-
ume 33. Imprimerie nationale, 1908.

Hastie, Trevor, Tibshirani, Robert, and Wainwright, Mar-
tin. Statistical learning with sparsity: the lasso and gen-
eralizations. Chapman and Hall/CRC, 2019.

Hestenes, Magnus R. Multiplier and gradient methods.
Journal of optimization theory and applications, 4(5):
303–320, 1969.

Hestenes, Magnus Rudolph and Stiefel, Eduard. Methods
of conjugate gradients for solving linear systems, vol-
ume 49. NBS Washington, DC, 1952.

Ghojogh, Benyamin, Nekoei, Hadi, Ghojogh, Aydin, Kar-
ray, Fakhri, and Crowley, Mark. Sampling algorithms,
from survey sampling to Monte Carlo methods: Tutorial
and literature review. arXiv preprint arXiv:2011.00901,
2020.

Hinton, Geoffrey, Srivastava, Nitish, and Swersky, Kevin.
Neural networks for machine learning lecture 6a
overview of mini-batch gradient descent. Technical re-
port, Department of Computer Science, University of
Toronto, 2012.

Ghojogh, Benyamin, Ghodsi, Ali, Karray, Fakhri, and
Crowley, Mark. Reproducing kernel Hilbert space, Mer-
cer’s theorem, eigenfunctions, Nystr¨om method, and use
of kernels in machine learning: Tutorial and survey.
arXiv preprint arXiv:2106.08443, 2021.

Giesen, Joachim and Laue, S¨oren. Distributed convex op-
timization with many convex constraints. arXiv preprint
arXiv:1610.02967, 2016.

Giesen, Joachim and Laue, S¨oren. Combining ADMM and
the augmented Lagrangian method for efﬁciently han-
In International Joint Confer-
dling many constraints.
ence on Artiﬁcial Intelligence, pp. 4525–4531, 2019.

Glowinski, R and Marrocco, A. Finite element approxi-
mation and iterative methods of solution for 2-D non-
linear magnetostatic problems. In Proceeding of Inter-
national Conference on the Computation of Electromag-
netic Fields (COMPUMAG), 1976.

Gofﬁn, Jean-Louis and Vial, Jean-Philippe. On the com-
putation of weighted analytic centers and dual ellipsoids
with the projective algorithm. Mathematical Program-
ming, 60(1):81–92, 1993.

Golub, Gene H and Van Loan, Charles F. Matrix computa-

tions, volume 3. JHU press, 2013.

Hjorungnes, Are and Gesbert, David. Complex-valued ma-
IEEE
trix differentiation: Techniques and key results.
Transactions on Signal Processing, 55(6):2740–2746,
2007.

H¨older, Otto. Ueber einen mittelwerthabsatz. Nachrichten
von der K¨onigl. Gesellschaft der Wissenschaften und
der Georg-Augusts-Universit¨at zu G¨ottingen, 1889:38–
47, 1889.

Holland, John Henry et al. Adaptation in natural and artiﬁ-
cial systems: an introductory analysis with applications
to biology, control, and artiﬁcial intelligence. MIT press,
1992.

Hosseini, Reshad and Sra, Suvrit. An alternative to EM for
Gaussian mixture models: batch and stochastic Rieman-
nian optimization. Mathematical Programming, 181(1):
187–223, 2020a.

Hosseini, Reshad and Sra, Suvrit. Recent advances in
stochastic Riemannian optimization. Handbook of Vari-
ational Methods for Nonlinear Geometric Data, pp.
527–554, 2020b.

Hu, Jiang, Liu, Xin, Wen, Zai-Wen, and Yuan, Ya-Xiang.
A brief introduction to manifold optimization. Journal
of the Operations Research Society of China, 8(2):199–
248, 2020.

56

Huber, Peter J. Robust estimation of a location parameter.
In Breakthroughs in statistics, pp. 492–518. Springer,
1992.

Land, Ailsa H and Doig, Alison G. An automatic method
for solving discrete programming problems. Economet-
rica, 28(3):497–520, 1960.

Iusem, Alfredo N. On the convergence properties of
the projected gradient method for convex optimiza-
tion. Computational & Applied Mathematics, 22:37–52,
2003.

Jain, Prateek and Kar, Purushottam. Non-convex op-
arXiv preprint

timization for machine learning.
arXiv:1712.07897, 2017.

Johnson, Rie and Zhang, Tong. Accelerating stochastic
gradient descent using predictive variance reduction. Ad-
vances in neural information processing systems, 26:
315–323, 2013.

Karush, William. Minima of functions of several vari-
ables with inequalities as side constraints. Master’s the-
sis, Department of Mathematics, University of Chicago,
Chicago, Illinois, 1939.

Kelley, Carl T. Iterative methods for linear and nonlinear

equations. SIAM, 1995.

Kennedy, James and Eberhart, Russell. Particle swarm
optimization. In Proceedings of ICNN’95-international
conference on neural networks, volume 4, pp. 1942–
1948. IEEE, 1995.

Khachiyan, Leonid Genrikhovich. A polynomial algorithm
in linear programming. In Doklady Akademii Nauk, vol-
ume 244, pp. 1093–1096. Russian Academy of Sciences,
1979.

Kingma, Diederik P and Ba,

Jimmy.

method for stochastic optimization.
arXiv:1412.6980, 2014.

Adam: A
arXiv preprint

Kjeldsen, Tinne Hoff. A contextualized historical analysis
of the Kuhn–Tucker theorem in nonlinear programming:
the impact of world war II. Historia mathematica, 27(4):
331–361, 2000.

Koneˇcn`y,
Daniel.
timization beyond the datacenter.
arXiv:1511.03575, 2015.

Jakub, McMahan, Brendan, and Ramage,
Federated optimization: Distributed op-
arXiv preprint

Lee, John A and Verleysen, Michel. Nonlinear dimension-
ality reduction. Springer Science & Business Media,
2007.

Lee, John M. Quotient manifolds.

In Introduction to

Smooth Manifolds, pp. 540–563. Springer, 2013.

Lee, Yin Tat and Sidford, Aaron. Efﬁcient accelerated co-
ordinate descent methods and faster algorithms for solv-
ing linear systems. In 2013 ieee 54th annual symposium
on foundations of computer science, pp. 147–156. IEEE,
2013.

Lemar´echal, Claude. Cauchy and the gradient method. Doc

Math Extra, 251(254):10, 2012.

Lemar´echal, Claude and Sagastiz´abal, Claudia. Practical
aspects of the Moreau–Yosida regularization: Theoreti-
cal preliminaries. SIAM journal on optimization, 7(2):
367–385, 1997.

Levitin, Evgeny S and Polyak, Boris T. Constrained min-
imization methods. USSR Computational mathematics
and mathematical physics, 6(5):1–50, 1966.

Li, Qiuwei, Zhu, Zhihui, and Tang, Gongguo. Alternat-
ing minimizations converge to second-order optimal so-
lutions. In International Conference on Machine Learn-
ing, pp. 3935–3943, 2019.

Li, Tian, Sahu, Anit Kumar, Talwalkar, Ameet, and Smith,
Virginia. Federated learning: Challenges, methods, and
future directions. IEEE Signal Processing Magazine, 37
(3):50–60, 2020.

Liu, Dong C and Nocedal, Jorge. On the limited memory
BFGS method for large scale optimization. Mathemati-
cal programming, 45(1):503–528, 1989.

Liu, Yusha, Wang, Yining, and Singh, Aarti. Smooth bandit
optimization: Generalization to Holder space. In Inter-
national Conference on Artiﬁcial Intelligence and Statis-
tics, pp. 2206–2214, 2021.

Krylov, AN. On the numerical solution of equation by
which are determined in technical problems the frequen-
cies of small vibrations of material systems. News Acad.
Sci. USSR, 7:491–539, 1931.

Luo, Zhi-Quan and Tseng, Paul. On the convergence of
the coordinate descent method for convex differentiable
minimization. Journal of Optimization Theory and Ap-
plications, 72(1):7–35, 1992.

Kuhn, Harold W and Tucker, Albert W. Nonlinear pro-
In Berkeley Symposium on Mathematical
gramming.
Statistics and Probability, pp. 481–492. Berkeley: Uni-
versity of California Press, 1951.

Luo, Zhi-Quan and Tseng, Paul. Error bounds and con-
vergence analysis of feasible descent methods: a general
approach. Annals of Operations Research, 46(1):157–
178, 1993.

57

Magnus, Jan R and Neudecker, Heinz. Matrix differential
calculus with applications to simple, hadamard, and kro-
necker products. Journal of Mathematical Psychology,
29(4):474–492, 1985.

Moreau, Jean Jacques. D´ecomposition orthogonale d’un
espace Hilbertien selon deux cˆones mutuellement po-
laires. Comptes rendus hebdomadaires des s´eances de
l’Acad´emie des sciences, 255:238–240, 1962.

Nocedal, Jorge and Wright, Stephen. Numerical optimiza-
tion. Springer Science & Business Media, 2 edition,
2006.

Otero, Daniel, La Torre, Davide, Michailovich, Oleg V, and
Vrscay, Edward R. Alternate direction method of mul-
tipliers for unconstrained structural similarity-based op-
timization. In International Conference Image Analysis
and Recognition, pp. 20–29. Springer, 2018.

Moreau, Jean Jacques. Proximit´e et dualit´e dans un es-
pace hilbertien. Bulletin de la Soci´et´e math´ematique de
France, 93:273–299, 1965.

Parikh, Neal and Boyd, Stephen. Proximal algorithms.
Foundations and Trends in optimization, 1(3):127–239,
2014.

Nash, Stephen G. A survey of truncated-Newton methods.
Journal of computational and applied mathematics, 124
(1-2):45–59, 2000.

Petersen, Kaare Brandt and Pedersen, Michael Syskind.
The matrix cookbook. Technical University of Denmark,
15, 2012.

Nesterov, Yurii. A method for solving the convex program-
ming problem with convergence rate O(1/kˆ2). In Dokl.
Akad. Nauk SSSR, volume 269, pp. 543–547, 1983.

Nesterov, Yurii. On an approach to the construction of op-
timal methods of minimization of smooth convex func-
tions. Ekonomika i Mateaticheskie Metody, 24(3):509–
517, 1988.

Nesterov, Yurii. Cutting plane algorithms from analytic
centers: efﬁciency estimates. Mathematical Program-
ming, 69(1):149–176, 1995.

Nesterov, Yurii. Introductory lectures on convex program-
ming volume I: Basic course. Lecture notes, 3(4):5,
1998.

Nesterov, Yurii. Introductory lectures on convex optimiza-
tion: A basic course, volume 87. Springer Science &
Business Media, 2003.

Nesterov, Yurii. Smooth minimization of non-smooth func-
tions. Mathematical programming, 103(1):127–152,
2005.

Nesterov, Yurii. Gradient methods for minimizing com-
posite functions. Mathematical Programming, 140(1):
125–161, 2013.

Nesterov, Yurii. Lectures on convex optimization, volume

137. Springer, 2018.

Nesterov, Yurii and Nemirovskii, Arkadii.

Interior-point
polynomial algorithms in convex programming. SIAM,
1994.

Nocedal, Jorge. Updating quasi-Newton matrices with lim-
ited storage. Mathematics of computation, 35(151):773–
782, 1980.

Polak, Elijah and Ribiere, Gerard. Note sur la conver-
gence de m´ethodes de directions conjugu´ees. ESAIM:
Mathematical Modelling and Numerical Analysis-
Mod´elisation Math´ematique et Analyse Num´erique, 3
(R1):35–43, 1969.

Potra, Florian A and Wright, Stephen J.

Interior-point
methods. Journal of computational and applied math-
ematics, 124(1-2):281–302, 2000.

Powell, Michael JD. A method for nonlinear constraints
in minimization problems. Optimization, pp. 283–298,
1969.

Rebennack, Steffen. Ellipsoid method. Encyclopedia of

Optimization, pp. 890–899, 2009.

Reddi, Sashank J, Sra, Suvrit, P´oczos, Barnab´as, and
Smola, Alex. Stochastic Frank-Wolfe methods for non-
In 2016 54th Annual Allerton
convex optimization.
Conference on Communication, Control, and Comput-
ing (Allerton), pp. 1244–1251. IEEE, 2016.

Riedmiller, Martin and Braun, Heinrich. Rprop-a fast adap-
tive learning algorithm. In Proceedings of the Interna-
tional Symposium on Computer and Information Science
VII, 1992.

Robbins, Herbert and Monro, Sutton. A stochastic approx-
imation method. The annals of mathematical statistics,
pp. 400–407, 1951.

Rockafellar, R Tyrrell. Monotone operators and the proxi-
mal point algorithm. SIAM journal on control and opti-
mization, 14(5):877–898, 1976.

Roux, Nicolas Le, Schmidt, Mark, and Bach, Francis. A
stochastic gradient method with an exponential conver-
gence rate for ﬁnite training sets. In Advances in Neural
Information Processing Systems, volume 25, 2012.

58

Rumelhart, David E, Hinton, Geoffrey E, and Williams,
Ronald J. Learning representations by back-propagating
errors. nature, 323(6088):533–536, 1986.

Sammon, John W. A nonlinear mapping for data structure
analysis. IEEE Transactions on computers, 100(5):401–
409, 1969.

Schmidt, Mark, Roux, Nicolas Le, and Bach, Francis. Con-
vergence rates of inexact proximal-gradient methods for
convex optimization. arXiv preprint arXiv:1109.2415,
2011.

Schmidt, Mark, Le Roux, Nicolas, and Bach, Francis. Min-
imizing ﬁnite sums with the stochastic average gradient.
Mathematical Programming, 162(1-2):83–112, 2017.

Shor, Naum Zuselevich. Cut-off method with space exten-
sion in convex programming problems. Cybernetics, 13
(1):94–96, 1977.

Talbi, El-Ghazali. Metaheuristics: from design to imple-

mentation, volume 74. John Wiley & Sons, 2009.

Tibshirani, Robert. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society: Series
B (Methodological), 58(1):267–288, 1996.

Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-
rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural networks for
machine learning, 4(2):26–31, 2012.

Tikhomirov, Vladimir M. The evolution of methods of con-
vex optimization. The American Mathematical Monthly,
103(1):65–71, 1996.

Tseng, Paul. Convergence of a block coordinate descent
method for nondifferentiable minimization. Journal of
optimization theory and applications, 109(3):475–494,
2001.

Shor, Naum Zuselevich. Nondifferentiable optimization
and polynomial problems, volume 24. Springer Science
& Business Media, 1998.

Tseng, Paul. On accelerated proximal gradient methods
for convex-concave optimization. Technical report, MIT
University, 2008.

Shor, Naum Zuselevich. Minimization methods for non-
differentiable functions, volume 3. Springer Science &
Business Media, 2012.

Wang, Yu, Yin, Wotao, and Zeng, Jinshan. Global conver-
gence of ADMM in nonconvex nonsmooth optimization.
Journal of Scientiﬁc Computing, 78(1):29–63, 2019.

Simon, Dan. Evolutionary optimization algorithms. John

Wolfe, Philip. Convergence conditions for ascent methods.

Wiley & Sons, 2013.

SIAM review, 11(2):226–235, 1969.

Slater, Morton. Lagrange multipliers revisited. Technical
report, Cowles Commission Discussion Paper: Mathe-
matics 403, Yale University, 1950.

Wolfe, Philip. Invited note—some references for the ellip-
soid algorithm. Management Science, 26(8):747–749,
1980.

Soltanolkotabi, Mahdi, Javanmard, Adel, and Lee, Ja-
son D. Theoretical insights into the optimization land-
scape of over-parameterized shallow neural networks.
IEEE Transactions on Information Theory, 65(2):742–
769, 2018.

Sra, Suvrit and Hosseini, Reshad. Conic geometric opti-
mization on the manifold of positive deﬁnite matrices.
SIAM Journal on Optimization, 25(1):713–739, 2015.

Steele, J Michael. The Cauchy-Schwarz master class:
an introduction to the art of mathematical inequalities.
Cambridge University Press, 2004.

Stoer, Josef and Bulirsch, Roland. Introduction to numer-
ical analysis, volume 12. Springer Science & Business
Media, 2013.

Wright, Margaret. The interior-point revolution in opti-
mization: history, recent developments, and lasting con-
sequences. Bulletin of the American mathematical soci-
ety, 42(1):39–56, 2005.

Wright, Stephen J. Coordinate descent algorithms. Mathe-

matical Programming, 151(1):3–34, 2015.

Wu, Tong Tong and Lange, Kenneth. Coordinate descent
algorithms for lasso penalized regression. The Annals of
Applied Statistics, 2(1):224–244, 2008.

Yang, Xin-She. Nature-inspired metaheuristic algorithms.

Luniver press, 2010.

Yosida, Kˆosaku. Functional analysis. Springer Berlin Hei-

delberg, 1965.

Su, Weijie, Boyd, Stephen, and Candes, Emmanuel J. A
differential equation for modeling Nesterov’s acceler-
ated gradient method: Theory and insights. The Journal
of Machine Learning Research, 17(1):5312–5354, 2016.

Yudin, DB and Nemirovski, Arkadi S. Informational com-
plexity and efﬁcient methods for the solution of convex
extremal problems. `Ekon Math Metod, English transla-
tion: Matekon, 13(2):22–45, 1976.

Yudin, DB and Nemirovski, AS. Evaluation of the informa-
tional complexity of mathematical programming prob-
lems. `Ekon Math Metod, English translation: Matekon,
13(2):3–24, 1977a.

Yudin, DB and Nemirovski, AS. Optimization methods
adapting to the “signiﬁcant” dimension of the problem.
Autom Telemekhanika, English translation: Automation
and Remote Control, 38(4):513–524, 1977b.

59

Zeng,

Jinshan, Zha, Yixuan, Ma, Ke,

and Yao,
Yuan.
On stochastic variance reduced gradient
method for semideﬁnite optimization. arXiv preprint
arXiv:2101.00236, 2021.

Zhang, Fuzhen. The Schur complement and its applica-
tions, volume 4. Springer Science & Business Media,
2006.

Zou, Fangyu, Shen, Li, Jie, Zequn, Zhang, Weizhong, and
Liu, Wei. A sufﬁcient condition for convergences of
Adam and RMSProp. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recogni-
tion, pp. 11127–11135, 2019.

