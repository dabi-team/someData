0
2
0
2

t
c
O
9

]

C
D
.
s
c
[

1
v
4
1
4
4
0
.
0
1
0
2
:
v
i
X
r
a

A Vertex Cut based Framework for Load Balancing and
Parallelism Optimization in Multi-core Systems

GUIXIANG MA, Intel Labs
YAO XIAO, University of Southern California
THEODORE L. WILLKE, Intel Labs
NESREEN K. AHMED, Intel Labs
SHAHIN NAZARIAN, University of Southern California
PAUL BOGDAN, University of Southern California

High-level applications, such as machine learning, are evolving from simple models based on multilayer
perceptrons for simple image recognition to much deeper and more complex neural networks for self-driving
vehicle control systems. The rapid increase in the consumption of memory and computational resources by
these models demands the use of multi-core parallel systems to scale the execution of the complex emerging
applications that depend on them. However, parallel programs running on high-performance computers often
suffer from data communication bottlenecks, limited memory bandwidth, and synchronization overhead due to
irregular critical sections. In this paper, we propose a framework to reduce the data communication and improve
the scalability and performance of these applications in multi-core systems. We design a vertex cut framework
for partitioning LLVM IR graphs into clusters while taking into consideration the data communication and
workload balance among clusters. First, we construct LLVM graphs by compiling high-level programs into
LLVM IR, instrumenting code to obtain the execution order of basic blocks and the execution time for each
memory operation, and analyze data dependencies in dynamic LLVM traces. Next, we formulate the problem
as Weight Balanced ùëù-way Vertex Cut, and propose a generic and flexible framework, wherein four different
greedy algorithms are proposed for solving this problem. Lastly, we propose a memory-centric run-time
mapping of the linear time complexity to map clusters generated from the vertex cut algorithms onto a
multi-core platform. This mapping takes into consideration cache coherency and data communication to
improve the application performance. We conclude that our best algorithm, WB-Libra, provides performance
improvements of 1.56x and 1.86x over existing state-of-the-art approaches for 8 and 1024 clusters running on
a multi-core platform, respectively.

Additional Key Words and Phrases: Parallel Programming, Weight Balanced ùëù-way Vertex Cut, Memory-Centric
Run-Time Mapping, Graph Partitioning, LLVM Graphs, Edge Cut, Power-law Graphs

1 INTRODUCTION
The massive and growing number of complex applications, such as in machine learning and big data
[Chen and Zhang 2014], call for efficient execution to reduce the run-time overhead. In particular,
sequential programs running in single-core systems fail to provide performance improvement.
Nevertheless, the parallel execution in multi-core systems is not a cure-all in that it may cause
performance degradation due to load imbalance, synchronization overhead, and resource sharing.
The performance of parallel execution is determined by the worst execution time among spawned
threads. Therefore, load imbalance can severely impact the overall performance. On the other hand,
threads compete for the underlying shared hardware resources, which increases synchronization
overhead if not properly handled.

Authors‚Äô addresses: Guixiang Ma, Intel Labs, guixiang.ma@intel.com; Yao Xiao, University of Southern California, xiaoyao@
usc.edu; Theodore L. Willke, Intel Labs, ted.willke@intel.com; Nesreen K. Ahmed, Intel Labs, nesreen.k.ahmed@intel.com;
Shahin Nazarian, University of Southern California, shahin.nazarian@usc.edu; Paul Bogdan, University of Southern
California, pbogdan@usc.edu.

2020. XXXX-XXXX/2020/10-ART $15.00
https://doi.org/

, Vol. 1, No. 1, Article . Publication date: October 2020.

 
 
 
 
 
 
2

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

Therefore, it is crucial to study how to optimize the parallel execution of applications in multi-core
systems. The recent work in this area has focused on fine-grained parallelism and various task-to-
core mapping strategies for minimizing the execution overhead (e.g., run-time, communication cost)
on multi-core systems and optimizing the execution. For example, [Hendrickson and Leland 1995a]
propose new graph partitioning algorithms based on spectral graph theory to partition coarse-
grained dataflow graphs into parallel clusters for mapping large problems onto different nodes
while balancing the computational loads. [Devine et al. 2006] develops hypergraph partitioning
algorithms to better model communication requirements and represent asymmetric problems to
divide computations into clusters. Moreover, some existing research [Murray et al. 2013, 2011; Yu
et al. 2008] designs different systems for general-purpose distributed data-parallel computing.

Despite the large number of works in this area [Hendrickson and Kolda 2000; Hendrickson
and Leland 1995a,b; Verbelen et al. 2013], very few of them have considered instruction-level fine-
grained parallelism, which offers a novel approach to discovering optimal parallelization degree and
minimizing data communication in multi-core platforms. In this paper, we explore the instruction-
level parallelism using graph partitioning techniques on the low level virtual machine (LLVM)
intermediate representation (IR) [Lattner and Adve 2004] graphs and cluster-to-core mapping for
optimizing the parallel execution of applications on multi-core systems. The recent work in [Xiao
et al. 2019, 2017] studies a similar problem and proposed an edge-cut approach that proposes a
community detection inspired optimization framework to partition dynamic dependency graphs
to automatically parallelize the execution of applications while minimizing the inter-core traffic
overhead. Although this work has achieved better performance in the multi-core parallelism
optimization compared to other baseline methods such as sequential execution framework and
thread-based framework, the graph partition approach used in [Xiao et al. 2017] does not consider
some important structural properties of the LLVM IR graphs, for example, the power-law degree
distribution. This may lead to less-than-ideal graph partitions identified by the optimization model.
In this paper, we consider the power-law degree distribution when designing a graph partition
framework for LLVM graphs, and propose vertex-cut strategies that partition graphs for better load
balancing and parallelism in multi-core systems.

Generally, there are three major challenges in designing a vertex cut framework for LLVM IR
graphs: (1) How to formulate the goal of reducing data communication and optimal balanced
workloads among multiple cores into the vertex cut graph partitioning problem, (2) How to
incorporate edge weights into the vertex cut optimization problem, though most of the existing
vertex cut methods are designed for unweighted graphs. However, the LLVM IR graphs are naturally
weighted graphs, where vertices represent instructions, edges represent dynamic data dependencies
among the instructions, and edge weights represent the estimated execution time for memory
operations, which are crucial for measuring the expected workloads for executing instructions,
and (3) How to map the graph partitions (i.e., clusters) generated by the vertex-cut approach to
system‚Äôs processors at run-time.

Contributions. To address these challenges, we propose a generic and flexible vertex cut frame-
work on LLVM IR graphs for optimal load balancing and parallel execution of applications on
multi-core systems as shown in Fig. 1. The proposed framework has an advantage in incorporating
power-law degree distribution into graph partitioning and can achieve extremely balanced parti-
tions. Therefore, it is an ideal framework for balanced workloads based on graph partitioning. More
specifically, we introduce and formalize a new problem, called the weight balanced ùëù-way Vertex
Cut, by incorporating edge weights into the optimization of vertex cut-based graph partitioning
for load balancing. In addition, we propose novel greedy algorithms for solving this problem,

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

3

Fig. 1. Overview of the Proposed Framework. We first pass programs into the structure and resource analyzer
to construct LLVM graphs which capture spatial and temporal data communication. Next, we propose a
vertex-cut based graph partitioning framework for LLVM graphs to obtain balanced clusters with minimized
inter-cluster data communication. Finally, we develop a memory-centric run-time mapping to schedule
clusters onto a multi-core non-uniform memory access (NUMA) platform.

ùê∫
ùëâ
ùê∏
ùëä
ùëÄ (ùëí)
ùê¥(ùë£)
ùë§ùëí
ùõº
ùúÜ

Table 1. Summary of Notation

Input LLVM graph
The set of vertices in a graph ùê∫
The set of edges in a graph ùê∫
The weight matrix for graph ùê∫
The set of clusters that contain edge e
The set of clusters that contain vertex v
The weight of edge e
The power parameter for the power-law graphs
The edge weight imbalance factor

and introduce a memory-centric run-time mapping algorithm for mapping the graph clusters to
multi-core architectures. Our contributions can be summarized as follows:

‚Ä¢ We introduce the vertex-cut graph partition strategy to LLVM IR graphs and propose a vertex
cut-based framework for partitioning LLVM graphs, which reduces data communication and
achieves optimally balanced workloads amongst a set of cores.

‚Ä¢ We prove that the formulated optimization problems possesses submodular properties, which
enables us to design a greedy algorithm for the optimization problem of the Weight Balanced
ùëù-way Vertex Cut with optimality guarantees.

‚Ä¢ We present a memory-centric run-time mapping algorithm for mapping the graph clusters

to multiple cores.

Organization. The rest of the paper is organized as follows. Section 2 describes preliminaries
to help understand the paper, including edge-cut and vertex-cut graph partitioning algorithms.
Section 3 provides detailed procedures for constructing LLVM graphs. Section 4 discusses the
vertex-cut based graph partitioning framework and theoretical analysis. Section 5 presents the
NUMA architecture and memory-centric run-time mapping. Section 6 provides simulation setup
and experimental results. Finally, we discuss the related work in Section 7 and conclude the paper
in Section 8.

2 NOTATION & PRELIMINARIES
In this section, we introduce the notation and provide background for some of the fundamental
concepts used throughout this paper. For a summary of notation, see Table 1.

Power-law Graphs. Let ùê∫ = (ùëâ , ùê∏) denote a graph, where ùëâ is the set of vertices and ùê∏ ‚äÜ ùëâ √óùëâ
is the set of edges in ùê∫. Graph ùê∫ is a power-law graph if its degree distribution follows a power

, Vol. 1, No. 1, Article . Publication date: October 2020.

4

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

(a) Sample Graph

(b) Edge-Cut Strategy 1

(c) Edge-Cut Strategy 2

(d) Vertex-Cut Strategy

Fig. 2. Illustrative Example of Edge Cut vs. Vertex Cut

law [Adamic and Huberman 2002; Gonzalez et al. 2012]:

P(ùëë) ‚àù ùëë ‚àíùõº,

(1)

where P(ùëë) is the probability that a vertex has a degree ùëë and ùõº is a positive constant exponent.
The power-law degree distribution means that most vertices in the graph have few neighbors while
very few vertices have a large number of neighbors. The exponent ùõº controls the "skewness" of the
vertex degree distribution, where a higher ùõº implies a lower ratio of edges to vertices. Many natural
graphs have such power-law degree distributions, such as social networks. The LLVM graphs that
we aim to analyze in this paper are also power-law graphs and will be introduced in Sections 3 and 4.
Some examples of LLVM graphs are shown in Fig. 5. The skewed degree distributions in power-law
graphs challenges graph partitioning, especially for LLVM graphs with a goal of balanced clusters
and minimized data communication in parallel computing.

Edge-Cut. Given a graph ùê∫ = (ùëâ , ùê∏), an edge-cut on ùê∫ is a partition of ùëâ into two subsets ùëÜ
and ùëá by cutting some edges in ùê∏, which results in two clusters with a set of inter-cluster edges
(ùë¢, ùë£) ‚àà ùê∏|ùë¢ ‚àà ùëÜ, ùë£ ‚àà ùëá . Edge-cut based graph partitioning tasks usually have an optimization
model such that after a number of edge cuts, the graph is partitioned into a certain number of
clusters to satisfy the optimization requirements. Examples of edge-cut based graph partitioning
problems include the widely studied max-flow min-cut problem [Dantzig and Fulkerson 2003]
in flow graphs and community detection in social networks [Bedi and Sharma 2016]. In parallel
computing, calculations can be considered as graphs where nodes represent a series of computations
and edges represent data dependencies. Edge-cut based methods have also been studied in this
area for partitioning graphs into interconnected clusters to be mapped onto parallel computers
[Hendrickson and Kolda 2000; Hendrickson and Leland 1995a; Verbelen et al. 2013]. In this paper,
we will also discuss the state-of-the-art edge-cut based methods and apply them as baseline methods
for the optimal parallelism and load balancing in multi-core systems.

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

5

Fig. 3. Workflow of LLVM Graph Construction. Each program is first compiled to static IR instructions via
the LLVM front-end, which is next translated into dynamic IR trace via the LLVM back-end, combined with
instrumentation to obtain information such as memory timing and the sequence of the execution order of
basic blocks. Last, we perform dependency analysis to construct a graph based on the dynamic trace where
nodes denote IR instructions and edges represent dependencies.

Vertex-Cut. Given a graph ùê∫ = (ùëâ , ùê∏), a vertex-cut on ùê∫ is a partition of ùê∏ into subsets by cutting
some vertices in ùê∏. Whenever a vertex is cut, this vertex will be replicated and its replica along with
a subset of adjacent edges are placed into a different cluster. Instead of having inter-cluster edges
like an edge-cut does, vertex-cut partitions only have an inter-cluster connection between each
vertex that has been cut and its replicas. Due to these characteristics, a vertex-cut strategy can offer
more optimal solutions for some graph partitioning tasks compared to an edge-cut strategy. Fig. 2
shows a scenario by illustrating the difference between edge-cut and vertex-cut on a sample graph
for graph partitioning, with the goal of minimizing inter-cluster communication while balancing
the workloads (i.e., the number of edges) among clusters. Since the vertex ùê¥ in the graph has a high
degree while the other vertices have lower degrees, it is challenging for edge-cut approaches to
deal with the edges associated with ùê¥ in order to achieve low inter-cluster communication (i.e.,
cross-cluster edges) and a good balance between clusters. Fig. 2b shows an edge-cut strategy with
low inter-cluster communication but a high imbalance between clusters, while the strategy in
Fig. 2c achieves a good balance but with more inter-cluster communication cost. On the other hand,
the vertex-cut strategy in Fig. 2d perfectly addresses the issues by cutting the vertex ùê¥, where the
original ùê¥ is assigned to the cluster on the left and a replica of ùê¥ is assigned to the cluster on the
right. The connection between ùê¥ and its replica is the only communication cost between the two
clusters and the two clusters are well balanced. These examples demonstrate the advantage of
vertex-cuts over edge-cuts on a graph with skewed node degrees, and this motivates us to propose a
vertex-cut based graph partitioning framework on power-law LLVM graphs to discover the optimal
execution and minimal data communication.

3 LLVM GRAPH CONSTRUCTION
We consider each application as an LLVM graph generated from a dynamic trace. It is a directed and
acyclic graph with edge weights, where nodes represent IR instructions, edges represent dynamic
data dependencies between nodes, and weights represent time for memory operations. The dataflow
representation of LLVM graphs requires the advanced graph partitioning algorithms discussed
later to find balanced clusters in parallel computing. In this section, we discuss the workflow of
LLVM graph construction in three steps: (1) static IR generation via the LLVM front-end from an
input program; (2) dynamic IR generation from static IR combined with instrumentation; (3) LLVM
graph construction via dependency analysis. Before delving into the details, we introduce a graph
definition to help us understand it.

Definition 3.1. Let ùê∫ = (ùëâ , ùê∏,ùëä ) denote an LLVM graph, where each node ùë£ ‚àà ùëâ represents an
LLVM IR instruction, ùëÅ = |ùëâ | is the number of nodes, each edge ùëí = (ùë¢, ùë£) ‚àà ùê∏ represents the data

, Vol. 1, No. 1, Article . Publication date: October 2020.

6

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

Fig. 4. Example of LLVM Graph Construction. This is an example of a graph constructed from a C program
followed by the workflow in Fig. 3. One thing to note is that in instrumented static IR, instructions in blue
keep track of basic blocks whereas instructions in red measure time for memory operations. We only show
partial instrumentation for memory time measurement.

dependency among two nodes, and the corresponding edge weight ùë§ùë¢ùë£ ‚àà ùëä characterizes the data
dependency between node ùë¢ and node ùë£ to guarantee the strict program order.

The LLVM graph in Definition 3.1 captures the spatial and temporal data communication, since
the weight ùë§ùë¢ùë£ measures the amount of time required to transfer data from node ùë¢ to node ùë£ during
memory operations. Therefore, we could measure the cost of data communication, which facilitates
us to propose an optimization model to partition the LLVM graph into clusters while taking into
account data transfer among clusters.

3.1 Static IR Generation
Instruction set architecture (ISA) dependent traces include different characteristics and constraints
for a specific ISA, which cannot satisfy the fast growing hardware specialization and ever-expanding
workloads. Therefore, in parallel computing, in order to have well-balanced workloads with non-
trivial properties and understand the ISA-independent micro-structures, we first compile high-level

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

7

programs into static LLVM IR. LLVM is a compiler engine which makes program analysis lifelong
and transparent by introducing IR as a common model for analysis, transformation, and synthesis
[Lattner and Adve 2004]. IR is an intermediate representation between high-level instructions such
as Python/C and low-level assembly. It ignores the low-level hardware details while preserving the
dataflow structure of programs, as shown in an example in Fig. 4.

3.2 Dynamic Trace Generation
Once the static IR code is generated, we instrument the code to obtain information such as basic
blocks and memory time. First, we use a hash table to keep track of IR instructions within each
basic block. For example, in Fig. 4, instructions from "%1 = alloca i32*, align 8" up to "br label %5"
should be hashed into the index 1 which represents the first basic block. Second, at the beginning
of each basic block, we instrument a printf function to record the execution order of blocks.
Fig. 4 shows the full instrumentation of printf statements in blue. Last, we use the time stamp
counter rdtsc and the printf statements to measure the amount of time for each memory operation
(ùë°ùëñùëöùëí = ùëéùëì ùë°ùëíùëüùëöùëíùëö ‚àí ùëèùëí ùëì ùëúùëüùëíùëöùëíùëö). Instructions in red in Fig. 4 show this instrumentation for the first
two memory operations. Specifically, we insert rdtsc before and after each memory operation and
calculate the difference as the amount of execution time. Once static IR is instrumented, we use the
LLVM back-end to execute it and collect the execution order of basic blocks and the amount of
time for each memory operation. Combined with the hash table, which can be indexed from the
execution order of basic blocks, we obtain dynamic IR trace as shown in Fig. 4.

3.3 LLVM Graph Construction
The dynamic IR trace captures the dataflow nature of high-level programs. In order to understand
the hidden communication structure of the trace and processes that can be potentially be processed
in parallel, we construct the LLVM graph by analyzing the data and memory alias dependencies.
Data dependency analysis identifies source registers and destination registers for each instruction
and checks if source registers of the current instruction match with destination registers of the
prior ones. Alias analysis is used to determine if two pointers used in memory operations have the
same address. For example, the sixth instruction "store i32* %a, i32** %1, align 8" has the source
register %1 which depends on the destination register of the first instruction "%1 = alloca i32*, align
8". The corresponding LLVM graph manifests this dependency by inserting a directed edge from
node 1 to node 6.

4 VERTEX-CUT BASED GRAPH PARTITIONING FRAMEWORK
In this section, we introduce our vertex cut framework for partitioning LLVM graphs to optimize
the parallel execution of applications in multi-core systems.

By investigating the degree distribution of the LLVM graphs that we construct following the
procedures introduced in Section 3 for some applications, we observe that these LLVM graphs
are all power-law graphs, such as the examples shown in Fig. 5 for the Dijkstra algorithm and
the fast Fourier transform (FFT) algorithm. The skewed node degree distribution makes the graph
partitioning a challenging task on these power-law graphs. As discussed in Section 2, vertex-cut has
some advantages over edge-cut on graphs with skewed node degree distributions. Existing works
in graph partitioning for distributed graph computing have also shown that vertex-cut methods
can achieve better performance in terms of data communication and balance among the partitions
than edge-cut methods for power-law graphs [Gonzalez et al. 2012; Xie et al. 2014]. In [Gonzalez
et al. 2012], a vertex-cut approach called PowerGraph is proposed for solving a balanced ùëù-way
vertex-cut problem, where the objective is to minimize the average number of vertex replicas
while keeping the number of edges balanced among different clusters, so as to minimize the data

, Vol. 1, No. 1, Article . Publication date: October 2020.

8

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

(a) Dijkstra

(b) FFT

Fig. 5. Examples of LLVM Graphs

communication among different clusters while balancing their workloads. In [Xie et al. 2014], a
degree-based vertex-cut method called Libra is proposed, which has shown better performance
than PowerGraph for the balanced ùëù-way vertex-cut task. Although these existing vertex-cut
methods have been shown effective in graph partitioning for distributed graph computing, they are
designed for unweighted graphs, where the goal of a balanced cut is to keep the number of edges
balanced on each cluster. However, the LLVM graphs are weighted graphs, where the weights
represent the estimated execution time for memory operations, and the goal of a balanced cut is to
keep the sum of edge weights in different clusters balanced. Therefore, the existing unweighted
vertex cut methods cannot achieve satisfactory performance in the graph partition task on LLVM
graphs. In this paper, we formulate the Weight Balanced ùëù-way Vertex Cut as a new problem and
propose strategies for solving this problem.

4.1 Weight Balanced ùëù-way Vertex Cut
Given an LLVM IR graph ùê∫ = (ùëâ , ùê∏,ùëä ), our goal is to reduce data communication among different
cores (i.e., partitions/clusters) while achieving optimal balanced workloads (i.e., edge weights). We
formalize the objective of the weight balanced ùëù-way vertex-cut by assigning each edge ùëí ‚àà ùê∏ to a
cluster ùëÄ (ùëí) ‚àà {1, ¬∑ ¬∑ ¬∑ , ùëù}. Each vertex then spans the set of clusters ùê¥(ùë£) ‚äÜ {1, ¬∑ ¬∑ ¬∑ , ùëù} that contain
its adjacent edges. We define the objective as:

min
ùê¥

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùê¥(ùë£)|

s.t. max

ùëö

‚àëÔ∏Å

ùëí ‚ààùê∏,ùëÄ (ùëí)=ùëö

ùë§ùëí, < ùúÜ

ùë§ùëéùë£ùëî |ùê∏|
ùëù

(2)

(3)

where ùë§ùëí is the weight of edge ùëí, ùë§ùëéùë£ùëî is the average edge weight of graph ùê∫, and the imbalance
factor ùúÜ ‚â• 1 is a small constant.

As previously discussed, the balanced ùëù-way vertex cut for unweighted graphs has been studied in
some works in the literature [Gonzalez et al. 2012; Xie et al. 2014]. [Gonzalez et al. 2012] introduces
PowerGraph and [Xie et al. 2014] proposes Libra, both of which are state-of-the-art approaches for
the vertex-cut task on unweighted graphs. In PowerGraph [Gonzalez et al. 2012], randomized vertex
cut strategy is first analyzed, based on which a greedy solution is proposed for the edge-placement
process of the vertex-cuts. In [Xie et al. 2014], a degree-based approach, called Libra, is proposed
for vertex-cut graph partition, which is based on PowerGraph but further distinguishes the higher-
degree and lower-degree vertices during an edge placement to achieve better performance. Inspired
by PowerGraph and Libra, in the following sections, we will first perform theoretical analysis on

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

9

the random vertex cut solution for the proposed weighted balanced vertex cut problem and then
provide greedy algorithms for this vertex cut task.

4.2 Theoretical Analysis
4.2.1 Random Weighted Vertex Cut. A simple way to perform vertex-cuts is to randomly assign
edges to clusters. Based on [Gonzalez et al. 2012], we derive the expected normalized replication
factor (Eq.( 2)) in random weighted vertex cut for the weight balanced ùëù-way vertex cut task.

According to linearity of expectation, we have:

E[

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùê¥(ùë£)|] =

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

E[|ùê¥(ùë£)|]

(4)

where E[|ùê¥(ùë£)|] is the expected replication number of a single vertex ùë£.

Assume vertex ùë£ has a degree D[ùë£], then the expected replication of ùë£ can be computed by the
process of assigning the D[ùë£] edges that are adjacent to ùë£. Let ùëãùëñ denote the event that vertex ùë£ has
at least one of its edges on cluster ùëñ, then the expectation E[ùëãùëñ ] is:

E[ùëãùëñ ] = 1 ‚àí P(ùë£ has no edges on cluster ùëñ)

= 1 ‚àí (1 ‚àí

1
ùëù

)D[ùë£ ]

Then, the expected replication factor for vertex ùë£ is:

E[|ùê¥(ùë£)|] =

ùëù
‚àëÔ∏Å

ùëñ=1

E[ùëãùëñ ] = ùëù (1 ‚àí (1 ‚àí

1
ùëù

)D[ùë£ ])

(5)

(6)

In the power-law graph, D[ùë£] can be treated as a Zipf random variable, therefore Eq.(6) can be

further written as:

E[|ùê¥(ùë£)|] = ùëù (1 ‚àí E[(

(ùëù ‚àí 1)
ùëù

)D[ùë£ ]])

(7)

(8)

)D[ùë£ ]])

Then:

E[

1
|ùëâ |

‚àëÔ∏Å

|ùê¥(ùë£)|] =

‚àëÔ∏Å

(1 ‚àí E[(

ùëù
|ùëâ |

(ùëù ‚àí 1)
ùëù

ùë£ ‚ààùëâ
In the power-law graph ùê∫, the probability of a vertex degree being ùëë is P(ùëë) = ùëë ‚àíùõº /h |ùëâ | (ùõº),
ùëë ‚àíùõº is the normalizing constant of the power-law Zipf distribution. Then,

where h |ùëâ | (ùõº) = (cid:205) |ùëâ |‚àí1
ùëë=1

ùë£ ‚ààùëâ

E[(

(ùëù ‚àí 1)
ùëù

)D[ùë£ ]] =

1
h|ùëâ | (ùõº)

|ùëâ |‚àí1
‚àëÔ∏Å

ùëë=1

(1 ‚àí

1
ùëù

)ùëëùëë ‚àíùõº

By plugging Eq.(9) into Eq.(8), we have:

E[

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùê¥(ùë£)|] = ùëù ‚àí

ùëù
h|ùëâ | (ùõº)

|ùëâ |‚àí1
‚àëÔ∏Å

ùëë=1

ùëù ‚àí 1
ùëù

(

)ùëëùëë ‚àíùõº

(9)

(10)

We can improve the randomly weighted vertex cut with greedy strategies which assign next edge
onto the cluster that minimizes the conditional expected replication factor. But before we discuss
greedy based algorithms for the weighted balanced vertex cut, we first prove that the objective
function in Eq. (2) is submodular and a greedy algorithm can provide bounded optimality.

, Vol. 1, No. 1, Article . Publication date: October 2020.

10

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

4.2.2

Submodularity of the Objective Function.

Theorem 4.1. The optimization problem introduced in Section 4.1 is NP-hard.

Proof. K-balanced graph partitioning [Andreev and Racke 2006] divides a graph into ùëò equal
sized clusters while minimizing the capacity of edges cut, which is NP-hard. It reduces to the
‚ñ°
optimization problem by having a unit weight for each edge in a graph to be cut.

Theorem 4.2. The objective function in the Eq. (2) is submodular.

Proof. Given an LLVM IR graph ùê∫ = (ùëâ , ùê∏,ùëä ), define two assignment sets ùëã, ùëå = {ùê¥(ùë£1), ùê¥(ùë£2),
..., ùê¥(ùë£ |ùëâ |)} ‚äÜ Œ© where for any node ùë£, ùê¥(ùë£) ‚äÜ {1, ¬∑ ¬∑ ¬∑ , ùëù} and Œ© is the solution space of the problem.
We define ùëì (ùëã ) as the objective function defined in Eq. (2).

If ùëã ‚à© ùëå = ‚àÖ, then

ùëì (ùëã ‚à© ùëå ) + ùëì (ùëã ‚à™ ùëå ) =

=

‚àëÔ∏Å

ùë£ ‚ààùëâ
‚àëÔ∏Å

1
|ùëâ |
1
|ùëâ |

|ùëã (ùë£) + ùëå (ùë£)| + (cid:8)(cid:8)(cid:8)(cid:42) 0
ùëì (‚àÖ)

|ùëã (ùë£)| +

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùëå (ùë£)|

ùë£ ‚ààùëâ
= ùëì (ùëã ) + ùëì (ùëå )

If ùëã ‚à© ùëå = ùëÜùëê where ùëÜùëê is a set of the common elements, then

ùëì (ùëã ‚à© ùëå ) + ùëì (ùëã ‚à™ ùëå ) =

=

=

‚àëÔ∏Å

ùë£ ‚ààùëâ
‚àëÔ∏Å

ùë£ ‚ààùëâ
‚àëÔ∏Å

1
|ùëâ |
1
|ùëâ |
1
|ùëâ |

|ùëã (ùë£) + ùëå (ùë£) ‚àí ùëÜùëê (ùë£)| +

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùëÜùëê (ùë£)|

{|ùëã (ùë£)| + |ùëå (ùë£)| ‚àí |ùëÜùëê (ùë£)| + |ùëÜùëê (ùë£)|}

|ùëã (ùë£)| +

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùëå (ùë£)|

ùë£ ‚ààùëâ
= ùëì (ùëã ) + ùëì (ùëå )

Therefore, by combining Eq. (11) and Eq. (12), we can infer that the objective function is sub-
‚ñ°

modular because for any two sets ùëã, ùëå ‚äÜ Œ©, ùëì (ùëã ) + ùëì (ùëå ) = ùëì (ùëã ‚à© ùëå ) + ùëì (ùëã ‚à™ ùëå ).

Theorem 4.3. The objective function in the Eq. (2) is monotonic.

Proof. Given an LLVM IR graph ùê∫ = (ùëâ , ùê∏,ùëä ), we define an assignment set ùê¥ = {ùë£1, ùë£2, ..., ùë£ |ùëâ | } ‚äÜ

Œ© and an arbitrary assignment ùë£ùëò ‚äà ùê¥.

|ùê¥‚Ä≤(ùë£)|

{|ùê¥(ùë£)| + |ùë£ùëò (ùë£)|}

ùëì (ùê¥ ‚à™ ùë£ùëò ) =

=

=

‚àëÔ∏Å

ùë£ ‚ààùëâ
‚àëÔ∏Å

ùë£ ‚ààùëâ
‚àëÔ∏Å

1
|ùëâ |
1
|ùëâ |
1
|ùëâ |

|ùê¥(ùë£)| +

1
|ùëâ |

‚àëÔ∏Å

ùë£ ‚ààùëâ

|ùë£ùëò (ùë£)|

ùë£ ‚ààùëâ
= ùëì (ùê¥) + ùëì (ùë£ùëò )

Therefore, ùëì (ùê¥ ‚à™ ùë£ùëò ) ‚àí ùëì (ùê¥) ‚©æ 0.

, Vol. 1, No. 1, Article . Publication date: October 2020.

(11)

(12)

(13)

‚ñ°

Short Title

11

Theorem 4.4. Given a monotonic submodular function ùëì , the greedy maximization algorithm1

returns

ùëì (ùê¥ùëîùëüùëíùëíùëëùë¶) ‚©æ (1 ‚àí

1
ùëí

) max
|ùê¥ |<ùêæ

ùëì (ùê¥)

(14)

where ùêæ is the maximum number of possible assignments. Therefore, even though the optimiza-
tion problem is NP-hard, algorithm 1 is designed to find an assignment which provides a (1 ‚àí 1/ùëí)
approximation of the optimal value of ùê¥.

The proof of Theorem 4.4 follows from [Krause and Golovin 2014].

4.3 Greedy Algorithms for Weight Balanced Vertex Cut

To solve the vertex cut optimization problem defined in Eq. (2) via a greedy approach, we consider
the task of placing the (ùëñ + 1)-th edge after having placed the previous ùëñ edges. We define the
objective based on the conditional expectation, as shown below.

arg min
ùëò

E(cid:104) ‚àëÔ∏Å
ùë£ ‚ààùëâ

|ùê¥(ùë£)|

(cid:12)
(cid:12)
(cid:12)

ùê¥ùëñ, ùê¥(ùëíùëñ+1) = ùëò

(cid:105)

(15)

where ùê¥ùëñ is the assignment for the previous ùëñ edges.

In the following paragraphs, we propose four different greedy solutions for the edge placement of
the weight balanced vertex-cut. We call them Weighted PowerGraph, Weight Balanced PowerGraph,
Weighted Libra, and Weight Balanced Libra.

Weighted PowerGraph. The PowerGraph approach is proposed in [Gonzalez et al. 2012] for
unweighted vertex cuts, which assigns edges to clusters while balancing the number of edges
assigned to each cluster. Inspired by the greedy edge placement in PowerGraph and based on the
objective of the weighted vertex cut defined in Eq. (2), we define the edge placement rules for our
Weighted PowerGraph greedy algorithm as follows. For an edge (ùë¢, ùë£),

‚Ä¢ Case 1: If ùê¥(ùë¢) ‚à© ùê¥(ùë£) ‚â† ‚àÖ, then assign the edge to the least loaded cluster in ùê¥(ùë¢) ‚à© ùê¥(ùë£),
where the workload of each cluster refers to the total weights of all the edges assigned to the
cluster.

‚Ä¢ Case 2: If ùê¥(ùë¢) ‚à© ùê¥(ùë£) = ‚àÖ and ùê¥(ùë¢) ‚â† ‚àÖ, ùê¥(ùë£) ‚â† ‚àÖ, then assign edge (ùë¢, ùë£) to the least loaded

cluster in ùê¥(ùëô), where ùëô is the vertex from ùë¢, ùë£ that has more unassigned edges.

‚Ä¢ Case 3: If one of ùê¥(ùë¢) and ùê¥(ùë£) is not empty, then assign the edge (ùë¢, ùë£) to the least loaded

cluster in the non-empty set (i.e., ùê¥(ùë¢) ‚à™ ùê¥(ùë£)).

‚Ä¢ Case 4: If ùê¥(ùë¢) = ‚àÖ and ùê¥(ùë£) = ‚àÖ, then assign (ùë¢, ùë£) to the least loaded one of the ùëù clusters.

Weighted Libra. Due to the power-law degree distribution in LLVM graphs, the edge weights
associated with high-degree vertices tend to accumulate in a single cluster if these vertices are not
cut and spanned over multiple clusters, which can lead to workload imbalance. Moreover, cutting
the higher-degree vertices tends to save more communication cost between clusters compared to
cutting lower-degree vertices. The Libra unweighted vertex cut approach in [Xie et al. 2014] first
proposes a degree-based hashing strategy to address such an issue for cutting power-law graphs,
where the higher-degree vertex associated with an edge will be cut with priority if a vertex has
to be cut in order to place this edge. Inspired by the degree-based strategy in Libra, we exploit
the degree property of vertices during edge placement. Based on Weighted PowerGraph and this
degree-based rule, we propose a greedy algorithm called Weighted Libra, which has the following
edge placement rules: For an edge (ùë¢, ùë£),

1We can easily convert minimization to maximization in this problem by adding a negative sign to the function.

, Vol. 1, No. 1, Article . Publication date: October 2020.

12

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

Algorithm 1 Weight Balanced Libra: A Greedy Algorithm for Vertex Cut Graph Partitioning

1: Input: Edge set ùê∏; edge weight matrix ùëä ; vertex set ùëâ ; a set of clusters ùê∂ = {1, 2, ¬∑ ¬∑ ¬∑ , ùëù}; ùúÜ.
2: Output: The assignment ùëÄ (ùëí) ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , ùëù} of each edge ùëí.
3: Count the degree ùëëùëñ for each vertex ùë£ùëñ , ‚àÄùëñ ‚àà {1, 2, ¬∑ ¬∑ ¬∑ , |ùê∏|}
4: Compute the cluster weight sum bound ùëè = ùúÜ (cid:205)ùëí‚ààùê∏ ùë§ùëí
5: for each ùëí = (ùë£ùëñ, ùë£ ùëó ) ‚àà ùê∏ do
6:
7:
8:
9:

else if ùê¥(ùë£ùëñ ) ‚â† ‚àÖ ‚àß ùê¥(ùë£ ùëó ) = ‚àÖ then

if ùê¥(ùë£ùëñ ) = ‚àÖ and ùê¥(ùë£ ùëó ) = ‚àÖ then

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê∂)

ùëù

else if ùê¥(ùë£ùëñ ) = ‚àÖ ‚àß ùê¥(ùë£ ùëó ) ‚â† ‚àÖ then

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë£ùëñ ))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then
ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê∂)

end if

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë£ ùëó ))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then
ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê∂)

end if

else if ùê¥(ùë£ùëñ ) ‚à© ùê¥(ùë£ ùëó ) ‚â† ‚àÖ then

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë£ùëñ ) ‚à© ùê¥(ùë£ ùëó ))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë£ùëñ ) ‚à™ ùê¥(ùë£ ùëó ))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then
ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê∂)

end if

end if

else

ùë† = ùëéùëüùëî minùëô {ùëëùëô |ùëô ‚àà {ùëñ, ùëó })
ùë° = {ùë£ùëñ, ùë£ ùëó } ‚àí {ùë†}
ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë†))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then

ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê¥(ùë°))
if ùëôùëúùëéùëë (ùëö) ‚â• ùëè then
ùëö = ùëôùëíùëéùë†ùë°ùëôùëúùëéùëëùëíùëë (ùê∂)

10:
11:
12:
13:
14:
15:

16:
17:
18:
19:
20:
21:

22:
23:
24:
25:
26:
27:

28:
29:
30:
31:
32:
33:

end if

end if

34:
35:
36:
37: ùëÄ (ùëí) ‚Üê ùëö; ùê¥(ùë£ùëñ ) ‚Üê ùëö; ùê¥(ùë£ ùëó ) ‚Üê ùëö
38: end for

end if

‚Ä¢ Case 1: If ùê¥(ùë¢) ‚à© ùê¥(ùë£) ‚â† ‚àÖ, then assign the edge to the least loaded cluster in ùê¥(ùë¢) ‚à© ùê¥(ùë£),
where the workload of each cluster refers to the total weights of all the edges assigned to the
cluster.

‚Ä¢ Case 2: If ùê¥(ùë¢) ‚à© ùê¥(ùë£) = ‚àÖ and ùê¥(ùë¢) ‚â† ‚àÖ, ùê¥(ùë£) ‚â† ‚àÖ, then assign edge (ùë¢, ùë£) to the least loaded

cluster in ùê¥(ùëô), where ùëô is either ùë¢ or ùë£ whichever has the lower degree.

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

13

‚Ä¢ Case 3: If one of ùê¥(ùë¢) and ùê¥(ùë£) is not empty, then assign the edge (ùë¢, ùë£) to the least loaded

machine in the non-empty set (i.e., ùê¥(ùë¢) ‚à™ ùê¥(ùë£)).

‚Ä¢ Case 4: If ùê¥(ùë¢) = ‚àÖ and ùê¥(ùë£) = ‚àÖ, then assign (ùë¢, ùë£) to the least loaded one of the ùëù clusters.
According to the edge placement rules of Weighted PowerGraph and Weighted Libra, the load
balancing among clusters is considered by assigning edges to the least loaded cluster under each
case. However, this strategy cannot guarantee the overall balance of the workload (i.e., total
edge weights) among different clusters or permit control of the emphasis to put on the balance
constraint. To address this issue and further improve load balancing, we incorporate an explicit
constraint on the balance of edge weights among clusters into the greedy edge placement rules
of the Weighted PowerGraph and Weighted Libra, and have two new greedy algorithms: Weight
Balanced PowerGraph and Weight Balanced Libra. Specifically, we incorporate the constraint
on the edge weight balance, which is formulated in Eq. (3), into the greedy edge placement rules of
the Weighted PowerGraph and Weighted Libra. For cases 1-3 in both algorithms, before placing
an edge to the target cluster, we first check if the current sum of edge weights in a target cluster
is within the bound given by ùúÜ ùë§ùëéùë£ùëî |ùê∏ |
, where ùúÜ ‚â• 1 is a constant. If it is, then we place the edge
into this cluster. Otherwise, we search another cluster from the remaining set that satisfies this
condition as the target cluster for the placement. By setting different values to ùúÜ, we can allow
different amounts of emphasis on the workload balance. To illustrate the overall workflow of these
greedy algorithms, we summarize the Weighted Balanced Libra greedy algorithm in Algorithm 1
as an example.

ùëù

4.4 Discussions

Time Complexity. According to the workflow of the Weighted Balanced Libra algorithm as
shown in Algorithm 1, given a graph ùê∫ = (ùëâ , ùê∏,ùëä ), for each edge ùëí in ùê∏, the algorithm retrieves
the cluster with the least load (i.e., total edge weights) either from the entire cluster set ùê∂ or from a
subset of ùê∂. For the former case (line 7, 11, 16, 23, or 33 in Algorithm 1), it takes ùëÇ (|ùê∂ |) time, and
for the latter case, it takes ùëÇ (|ùê∂1|) time (|ùê∂1| ‚â§ |ùê∂ |) if the balance constraint is satisfied (line 9, 14,
19, 29), and otherwise it takes ùëÇ (|ùê∂1| + |ùê∂2|), |ùê∂2| ‚â§ |ùê∂ | (line 21, 31), or ùëÇ (|ùê∂1| + |ùê∂ |) (line 11, 16),
or ùëÇ (|ùê∂1| + |ùê∂2| + |ùê∂ |) (line 23, 33). Note that line 27 in the algorithm takes ùëÇ (1). So in the worst
case, the algorithm takes ùëÇ (3|ùê∂ |) + ùëÇ (1) = ùëÇ (|ùê∂ |) for placing one edge. Therefore, the overall time
complexity of Weighted Balanced Libra algorithm is ùëÇ (|ùê∏| √ó |ùê∂ |). Based on the edge placement
rules introduced in Section 4.3, this time complexity applies to the three other algorithms as well,
although the Weighted Libra and Weighted PowerGraph may take relatively less time in practice
compared to Weight Balanced PowerGraph and Weight Balanced Libra, since they do not have
the weight balanced constraint. Therefore, they have the time complexity of ùëÇ (|ùê∂ |) or ùëÇ (|ùê∂1|)
discussed above for placing one edge.

Edge Weight Imbalance. Besides the replication factor discussed in Section 4.2.1 as a goal of
the optimization model for the weight balanced vertex cut problem, the edge weight balance among
different clusters is another key metric for evaluating the performance, which determines the load
balance. As we discussed above in Section 4.3, the degree-based hashing strategy introduced by
Libra tends to have more balanced cut results as the higher-degree vertices have a higher priority
to be cut than the lower-degree vertices. This statement has also been proved theoretically by
[Xie et al. 2014], which shows that Libra can achieve a lower edge imbalance than PowerGraph.
By incorporating this degree-based vertex cut rule, our proposed Weighted Libra algorithm is
expected to achieve a better load balancing (i.e., a lower edge weight imbalance) than the Weighted
PowerGraph algorithm. Furthermore, the proposed Weight Balanced PowerGraph and Weight
Balanced Libra allows for a further improvement in the load balancing via incorporating a constraint

, Vol. 1, No. 1, Article . Publication date: October 2020.

14

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

Fig. 6. UMA and NUMA Architectures. UMA is a shared memory architecture with uniform memory access
whereas NUMA enables fast memory access for a processor to its local physical memory and slow memory
access to the rest of memory banks.
for the edge weight imbalance by the given bound ùúÜ ùë§ùëéùë£ùëî |ùê∏ |
(ùúÜ ‚â• 1). If we set ùúÜ = 1, these two
algorithms can guarantee near-ideal balanced vertex cut results, with an edge weight imbalance
1 + ùúñ, where ùúñ is a small non-negative constant.

ùëù

5 ARCHITECTURE AND MAPPING
In this section, we discuss the non-uniform memory access (NUMA) architecture used in the
evaluation and propose a memory-centric mapping scheme for mapping the clusters obtained via
the vertex-cuts onto processors in the NUMA architecture.

5.1 NUMA Architecture
Uniform memory access (UMA) is a shared memory architecture for processors running in parallel
as shown in Figure 6. It develops a unified vision of the shared physical memory, meaning that
access time to a particular memory address is independent regardless of which processor requests
data from different memory banks. On the contrary, NUMA allows memory access time dependent
on the relative processor location. That is, a processor has fast memory access time to its local
memory and slow access to the rest of memory. This non-uniformity enables potential fewer
memory accesses with fast access time. Limiting the number of memory accesses and fast memory
accesses provides the key to high performance computing. Therefore, we decide to apply the NUMA
architecture and propose a memory-centric run-time mapping to utilize the benefits of NUMA and
reduce the amount of data communicated among processors.

5.2 Memory-centric Run-time Mapping
At run-time, processes/clusters generated in Section 4 from each application are mapped onto
processors in a NUMA architecture in order to be executed. Depending on the mapping (e.g., A
process has to fetch data from the farthest memory bank.), data communication is a performance
bottleneck. The goal is to improve the amortized time when slow accesses occur only once in a
while and fast local accesses happen frequently.

Therefore, without fully observing the structure of clusters with corresponding physical memo-
ries, performance would degrade due to these reasons: (1) Waiting for cache update: The multi-core
platforms require the cache coherence protocol to have consistent data over private caches. A
process later mapped to a different core may increase the time spent for the cache coherence
protocol to fetch a cache line from the previous core. (2) Block memory operations between IOs and
memory: Block memory operations in IOs constitute a large overhead in the program execution
because a large amount of data are referenced and transferred between caches and main memory
banks. (3) Core utilization: In an extreme case, processes may be mapped only onto one core to

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

15

Fig. 7. Memory-Centric Run-Time Mapping, which considers memory hierarchy and data communication.

exploit cache temporal and spatial locality. However, the rest cores remain idle for a long time.
Therefore, core utilization is another factor for efficient parallelism in multi-core systems.

In order to improve performance, the run-time mapping should exploit and optimize the paral-
lelism in clusters while considering data communication between clusters and resource utilization.
Figure 7 shows three important factors to help design a memory-centric run-time mapping.

1. Clusters which reference the same data structures can be mapped onto one core to prevent

the time spent on cache coherence protocols and reduce the number of block operations.

2. Clusters which communicate with each other can be mapped to adjacent processors to improve
the amortized time by reducing the number of times on fetching data from the farthest processor
on a NUMA architecture.

3. Clusters which are independent of each other can be mapped to different regions of a multi-core

platform (architecture decomposition) to reduce the number of sharing paths of messages.

Therefore, the memory-centric run-time mapping algorithm as shown in Algorithm 2 takes as
inputs the clusters, their interactions, and data communication and schedules a mapping from
clusters to processors with the objective of improving application performance. The uttermost
important criterion for a run-time mapping is the small time complexity. Therefore, we propose a
greedy algorithm to achieve high performance with the time complexity of ùëÇ (ùëÉ) where ùëÉ is the
number of schedulable clusters. In the algorithm, we first check whether a cluster is ready to sched-
ule, and keep track of all clusters with which this cluster communicates. Next, based on the factor
3, the architecture decomposition is performed to make sure that independent clusters are mapped
onto faraway processors to distribute workloads and traffic evenly on hardware communication
substrate in a multi-core platform. Then, we calculate the execution time for two clusters with the
shared memory to be mapped onto the same processor and different ones, respectively. A mapping
of the current cluster is decided based on factors 1 and 3. Mapping clusters with the shared memory
onto the same processor could reduce the large overhead for block operations, but at the same time
the parallelism may suffer if too many clusters are mapped to a single processor. Therefore, we set
an upper threshold of the number of clusters to be mapped to a processor. In the evaluation, the
threshold is 4.

6 EVALUATIONS
In this section, we discuss the simulator configurations and present experimental results to investi-
gate the soundness of the proposed methodology.

6.1 Simulation Configurations
We use gem5 [Binkert et al. 2011] to simulate a varying number of out-of-order cores with the NUMA
architecture. Table 2 shows detailed simulation parameters. Table 3 lists the considered applications

, Vol. 1, No. 1, Article . Publication date: October 2020.

16

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

else

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëù = C // Factor 1

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëñùëùùëê = C // Factor 2

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëñùëùùëê = ‚àÖ // Factor 3

end if
RQ.ùëùùë¢ùë†‚Ñé(ùëêùëôùë¢ùë†ùë°ùëíùëü )

if ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùë†ùë°ùëéùë°ùë¢ùë† = ùëÜùê∂ùêªùê∏ùê∑ùëà ùêøùê¥ùêµùêøùê∏ then

if C = ClusterFromMem(ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëö) != ‚àÖ then

end if
if C = ClusterComm(ùëêùëôùë¢ùë†ùë°ùëíùëü ) != ‚àÖ then

Algorithm 2 Memory-Centric Run-Time Mapping
1: INPUT: Clusters and data communication
2: OUTPUT: A mapping from clusters to the architecture
3: Runqueue RQ = ‚àÖ
4: for ùëêùëôùë¢ùë†ùë°ùëíùëü in ùëêùëôùëñùë†ùë° do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
end if
16: end for
17: Regions = ArchitectureDecomposition()
18: LastCluster = NULL
19: repeat
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35: until RQ is empty

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëêùëúùëüùëí = ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëù // Factor 1
LastCluster->ùëêùëúùëüùëí->ùëõùë¢ùëö++

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëêùëúùëüùëí = DiffRegion(ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëñùëùùëê->ùëêùëúùëüùëí)

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëêùëúùëüùëí = Nearby(ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëñùëùùëê->ùëêùëúùëüùëí)

ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëêùëúùëüùëí = DiffRegion(ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëêùëúùëüùëí)

end if
LastCluster = ùëêùëôùë¢ùë†ùë°ùëíùëü

else if ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëñùëùùëê != ‚àÖ then

end if

else

else

ùëêùëôùë¢ùë†ùë°ùëíùëü = RQ.ùëùùëúùëù()
if ClusterFromMem(ùëêùëôùë¢ùë†ùë°ùëíùëü ->ùëö)->ùëêùëúùëüùëí == LastCluster->ùëêùëúùëüùëí then

// Decide on mapping clusters onto the same processor
if LastCluster->ùëêùëúùëüùëí->ùëõùë¢ùëö <= threshold then

Table 2. System Configuration

Cores
Clock frequency
L1 private cache

Out-of-order cores, 16 MSHRs
2.4 GHz
64KB, 4-way associative
32-byte blocks
256KB, distributed
4 GB, 8 GB/s bandwidth
Mesh

Virtual channel flit-based

L2 shared cache
Memory
Topology
Routing algorithm XY routing
Flow control

CPU

Network

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

Table 3. Summary and Description of Benchmarks

Benchmark Description

Input Size

Dijkstra
FFT
K-means
Mandel
MD
NN
Neuron
CNN
Strassen8
Strassen16

Find the shortest path
Compute fast Fourier transform
Partition data into k clusters
Calculate Mandelbrot set
Simulate molecular dynamics
Neural networks
A list of neurons with the ReLU function
Convolutional neural networks
Strassen‚Äôs matrix multiplication
Strassen‚Äôs matrix multiplication

50 nodes
A vector of size 1024
128 2D tuples
4092 points
512 particles
Three hidden fully connected layers
64 Neurons
Conv-Pool-Conv-Pool-FC
Matrices of size 64
Matrices of size 256

17

Source

MiBench
OmpSCR
Self-collected
OmpSCR
OmpSCR
Self-collected
Self-collected
Self-collected
Self-collected
Self-collected

from various benchmarks with different graph characteristics, including applications from the
OpenMP Source Code Repository (OmpSCR)[Dorta et al. 2005], Mibench [Guthaus et al. 2001],
and implementations of some other benchmark algorithms. We generate LLVM IR graphs from
these applications following the procedure introduced in Section 3. For baseline comparisons, we
consider four baseline methods for graph partitioning: (1) the work in [Xiao et al. 2017] abbreviated
as CompNet; (2) METIS [LaSalle et al. 2015], which is an edge-cut method that implements various
multilevel algorithms by iteratively coarsening a graph, computing a partition, and projecting
the partition back to the original graph; (3) the unweighted vertex-cut method PowerGraph (PG)
[Gonzalez et al. 2012]; and (4) the unweighted vertex-cut method Libra [Xie et al. 2014]. We compare
these baselines with the proposed four greedy vertex-cut algorithms: Weighted PowerGraph (W-PG),
Weight Balanced PowerGraph (WB-PG), Weighted Libra (W-Libra), Weight Balanced Libra (WB-Libra).

6.2 Experimental Results
In this section, we evaluate the proposed methods and baselines on the LLVM graphs transformed
from the benchmarks listed in Table 3 for the proposed graph partitioning and compare their
performance in the graph partition quality (in terms of replication factor and edge weight imbalance).
Next, we execute clusters generated from each method to measure the overall execution time and
data communication. We also analyze the sensitivity of the parameter ùúÜ involved in the constraint
of load balancing to the execution time.

6.2.1 Replication Factor. In Section 4.2.1, we have derived the theoretical expected replication
factor of the weighted vertex cut with random edge placement, which is in fact a theoretical
upper-bound for the replication factor of the proposed greedy algorithms. We now empirically
evaluate the performance of the proposed four greedy vertex cut algorithms in replication factor,
and compare the results with the theoretical upper-bound we compute by Eq. (10). Fig. 8 shows the
results on four graphs. As we can see, the four greedy algorithms achieve comparable performance
in the replication factor. All of their replication factors are within the theoretical upper-bound with
a considerable gap, which indicates the superior advantage of the greedy vertex-cut algorithms
over the random vertex cut strategy.

6.2.2 Edge Weight Imbalance. As discussed in Section 4.4, edge weight imbalance is a key metric
for evaluating the performance of vertex-cuts in load balancing among clusters. The edge weight
imbalance is defined by (maxùëö (cid:205)ùëí ‚ààùê∏,ùëÄ (ùëí)=ùëö ùë§ùëí )/(
), which measures how much the most-
loaded cluster deviates from the expected average load between clusters. A good load-balancing
vertex-cut method should achieve an edge weight imbalance close to 1, which indicates the absolute

ùë§ùëéùë£ùëî |ùê∏ |
ùëù

, Vol. 1, No. 1, Article . Publication date: October 2020.

18

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

balance. We evaluate the edge weight imbalance of all the six vertex cut methods, where we set
ùúÜ = 1 in the sum of weights in a cluster (line 4 of Algorithm 1) for WB-PG and WB-Libra, in order
to obtain their optimal balance of edge weights for comparisons with the other methods. Table 5
shows the results from edge weight imbalance of the six methods on all ten graphs. We observe
from the table that, WB-Libra achieves the best results in most of the graphs, except for Dijkstra,
Mandel, and Md, where WB-PG performs the best. Both the two unweighted vertex cut methods, i.e.
PowerGraph and Libra, achieve worse results compared to the four weighted vertex cut methods.
This is mainly due to the fact that, the unweighted vertex cut was designed to balance the number
of edges among clusters for unweighted graphs and therefore they can not properly handle the load
balancing for weighted graphs. By comparing between WB-PG and W-PG, and between WB-Libra
and W-Libra, we can see that the edge weight balance constraint we incorporate into the weighted
balanced algorithms is effective for further improving the edge weight balance among clusters and
is able to push the balance to the near-ideal situation.

(a) FFT

(b) Mandel

(c) Md

(d) CNN

Fig. 8. Replication Factor of the Proposed Four Greedy Algorithms with Comparison to the Computed
Theoretical Upper-bound by Eq. (10)

6.2.3 Execution Time. Fig. 9 shows the execution time of each application for different graph
partitioning algorithms with various cluster numbers. Specifically, tables 6 and 7 show the execution
time for 8 and 1024 clusters, respectively. As we can see, the vertex-cut methods overall achieve
a better performance the edge-cut baselines CompNet and METIS. This verifies our expectation
that the vertex-cut based graph partitioning methods would work better than edge-cut methods
for the power-law graphs. Among the six vertex-cut methods, the proposed four methods (i.e.,

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

19

Table 4. Statistics of Graph Datasets

Graph Dataset No. Nodes No. Edges power-law ùõº Avg. Path Length
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

291,112
143,183
119,112
260,042
2,361,213
161,428
73,431
758,712
46,756
254,392

248,959
109,295
98,592
235,051
1,799,353
124,496
57,883
573,694
36,831
197,827

136.4
194.56
479.4
42.67
524.61
171.52
179.25
824.37
21.22
123.94

2.29
2.21
2.24
2.43
2.17
2.16
2.20
2.13
2.21
2.20

Table 5. Edge Imbalance of the Vertex Cut Methods on Graphs

Datasets
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

PG
1.00227
1.00586
1.00177
1.00730
1.00015
1.00187
1.00260
1.00040
1.01074
1.00338

W-PG WB-PG Libra W-Libra WB-Libra
1.00092
1.00831
1.00469
1.00233
1.00007
1.00235
1.00487
1.00035
1.01307
1.00352

1.00007
1.00075
1.00042
1.00008
1.00003
1.00028
1.00081
1.00010
1.00177
1.00029

1.00106
1.00400
1.00180
1.00171
1.00008
1.00106
1.00236
1.00027
1.00787
1.00170

1.02136
1.05030
1.04566
1.00749
1.00791
1.03441
1.05738
1.00956
1.05036
1.04206

1.00010
1.00057
1.00035
1.00014
1.00003
1.00019
1.00045
1.00008
1.00123
1.00028

Table 6. Overall Execution Time (/s) for 8 Clusters From Different Algorithms in a Multi-core Platform

Datasets
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

CompNet METIS PG
317.27
279.6
244.87
341.35
2568.72
351.23
214.75
1568.59
142.41
326.75

332.48
288.22
261.38
373.28
2723.71
376.93
242.68
1736.37
155.39
351.26

346.15
209.27
279.53
265.15
2822.47
354.32
213.95
1454.88
131.24
323.5

W-PG WB-PG Libra W-Libra WB-Libra
263.75
253.71
206.25
289.74
2313.9
311.74
187.23
1425.63
121.37
304.21

260.91
230.02
195.53
257.12
1821.68
256.41
163.63
1221.53
104.99
274.63

253.86
248.42
201.54
277.31
2178.41
297.59
174.54
1358.61
112.62
285.44

262.51
239.33
188.25
256.49
1824.95
278.24
157.69
1315.78
111.23
264.58

242.26
291.53
178.58
245.82
1642.18
253.89
131.4
1175.8
96.24
248.25

W-PG, WB-PG, W-Libra and WB-Libra) outperform the two unweighted vertex-cut methods. This is
reasonable since the unweighted vertex-cuts are not able to handle the load balancing for weighted
graphs, as we discussed in Section 4.4, and the load imbalance among clusters will lead to a longer
overall execution time for the applications, as the overall execution time depends on the time for
executing the cluster with the largest workload. Among the four proposed methods, WB-Libra
achieve the best performance in most cases consistently for all different numbers of clusters. This
demonstrates the benefit of using the degree-based vertex hashing strategy and the load balancing
constraint in the vertex-cuts. These results in execution time indicate that the proposed vertex-cut

, Vol. 1, No. 1, Article . Publication date: October 2020.

20

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

(a) 8 clusters

(b) 16 clusters

(c) 32 clusters

(d) 64 clusters

(e) 128 clusters

(f) 256 clusters

(g) 512 clusters

(h) 1024 clusters

Fig. 9. Application Performance From Different Graph Partitioning Algorithms on a Multi-core System

based graph partitioning framework is effective in load balancing and parallelism optimization for
multi-core systems and it has superior performance than the state-of-the-art baselines.

6.2.4 Data Communication. Fig. 10 shows data communication of each application for different
graph partitioning algorithms with various cluster numbers. Specifically, Tables 8 and 9 show the
communication for 8 and 1024 clusters, respectively. As we can see, all the vertex cut methods

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

21

Table 7. Overall Execution Time (/s) for 1024 Clusters From Different Algorithms in a Multi-core Platform

Datasets
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

CompNet METIS PG
33.5
31.08
25.08
24.92
18.26
16.77
15.8
23.48
233.02
228.43
34.92
33.44
23.73
21.3
148.48
157.5
14.57
12.51
29.23
31.81

46.48
32.4
37.23
41.37
245.61
52.36
48.32
170.92
15.03
38.14

W-PG WB-PG Libra W-Libra WB-Libra
27.79
22.64
16.37
19.81
204.53
29.35
20.93
145.87
13.55
29.62

29.27
23.96
12.54
14.52
174.23
25.48
21.62
110.67
12.39
27.36

29.58
23.2
15.3
17.47
192.23
29.84
19.19
132.22
12.17
27.22

26.43
20.31
13.92
14.43
169.18
27.9
17.97
119.43
11.11
25.25

23.4
18.59
11.26
12.6
155.71
23.22
16.11
105.29
10.31
23.42

Table 8. Data Communication for 8 Clusters From Different Graph Partitioning Algorithms

Datasets
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

CompNet METIS PG W-PG WB-PG Libra W-Libra WB-Libra
54%
100%
65%
100%
52%
100%
41%
100%
39%
100%
55%
100%
53%
100%
56%
100%
50%
100%
46%
100%

60% 46%
71% 53%
59% 41%
48% 31%
47% 33%
63% 43%
59% 42%
64% 50%
55% 43%
54% 42%

142%
156%
135%
158%
160%
169%
137%
192%
171%
193%

50%
56%
46%
36%
36%
48%
47%
53%
46%
45%

54%
62%
52%
42%
40%
53%
52%
55%
48%
47%

57%
65%
55%
45%
44%
57%
56%
58%
52%
50%

Table 9. Data Communication for 1024 Clusters From Different Graph Partitioning Algorithms

Datasets
Dijkstra
FFT
K-means
Mandel
Md
NN
Neuron
CNN
Strassen8
Strassen16

CompNet METIS PG W-PG WB-PG Libra W-Libra WB-Libra
47%
100%
50%
100%
42%
100%
28%
100%
35%
100%
44%
100%
30%
100%
55%
100%
44%
100%
48%
100%

53% 34%
55% 42%
47% 31%
33% 18%
41% 25%
52% 35%
38% 27%
63% 44%
52% 35%
53% 38%

142%
163%
132%
117%
156%
176%
148%
183%
145%
166%

39%
43%
32%
19%
29%
39%
31%
49%
38%
41%

46%
46%
39%
23%
31%
45%
33%
55%
42%
45%

47%
49%
43%
27%
35%
47%
35%
57%
46%
48%

have comparable good performance in reducing data communication and outperform the edge-cut
methods (METIS and CompNet) by a huge amount. For example, according to Table 8, the WB-
PG reduces the data communication for 8-cores by an average of 48.9% compared to CompNet
over the 10 graphs, and WB-Libra reduces it by an average of 46.1%. METIS fails to reduce data
communication compared to others. However, it is interesting to note that METIS causes less than
120% for the Mandelbrot application whereas the data communication is more than 130% for the rest
of applications. It is because Mandelbrot is a embarrassingly parallel application where little effort
is required to separate it into a number of parallel clusters. However, traditional edge cut algorithms

, Vol. 1, No. 1, Article . Publication date: October 2020.

22

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

(a) 8 clusters

(b) 16 clusters

(c) 32 clusters

(d) 64 clusters

(e) 128 clusters

(f) 256 clusters

(g) 512 clusters

(h) 1024 clusters

Fig. 10. Data Communication Cost From Different Graph Partitioning Algorithms in a Multi-core Platform

such as CompNet and METIS still lead to a seriously huge amount of data communication between
clusters, while vertex cut methods is able to maintain a much lower communication cost. This
is mainly because that the data communication in edge-cut partitions comes from all the inter-
cluster edges, while there is no such communication cost in vertex-cut partitions since there is no
inter-cluster edges and the only communication for the vertex-cut partitions is the communication

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

23

(a) Mandel

(b) Md

(c) NN

(d) Neuron

(e) Strassen16

Fig. 11. Execution Time With Different ùúÜ Values for WB-Libra and WB-PG Algorithms. Dotted lines indicate
the execution time for W-Libra and W-PG to which WB-Libra and WB-PG reduce, respectively, when ùúÜ
becomes large enough.

between the replicas of the cut vertices across different clusters. Another thing to notice is that the
general trend of data communication from 8 clusters up to 1024 clusters is it first goes down and up
again at 128 clusters. The trend of data communication going down is mainly due to the efficient
parallelism while minimizing data communication. However, as the number of clusters increases
beyond 128 clusters, synchronization starts to take over the impact of data communication because
processes are synchronized to allow only one process enter the critical section to modify the shared
data structures in main memory. Nevertheless, the least data communication overhead in these
cases is still from the proposed vertex-cut algorithms.

Sensitivity Analysis: Execution Time v.s. Parameter ùúÜ. Fig. 11 shows the execution time with
6.2.5
different ùúÜ values in Eq. (3) on five applications, i.e., Mandel, Md, NN, Neuron, and Strassen16. ùúÜ
controls the balance of the clusters. In the WB-PG and WB-Libra algorithms, we explicitly use ùúÜ to
set a balance bound (line #4 in Algorithm 1), and ùúÜ = 1 indicates the ideal balanced case. When
ùúÜ is large enough, WB-Libra and WB-PG reduce to W-Libra and W-PG, respectively. To analyze
the sensitivity of the parameter ùúÜ to the execution time, we evaluate WB-Libra and WB-PG with
different ùúÜ values in the range of 1 to 1.00012 with a step size of 0.00001 for Md due to its large
size and 1 to 1.0012 with a step size of 0.0001 for the rest graphs. The dotted blue line refers to
the performance of W-PG and the dotted red line refers to the performance of W-Libra, which
can be treated as upper bound for WB-PG and WB-Libra, respectively. There are times when the
execution time of applications exceeds the upper bound indicated by the dotted lines because of
frequent synchronization such as fetching data from memory and flushing dirty data into memory.
In general, the trend is going up, indicating that increasing ùúÜ causes the performance degradation.
It is recommended to set ùúÜ = 1 in WB-Libra and WB-PG to improve the execution time.

, Vol. 1, No. 1, Article . Publication date: October 2020.

24

Guixiang Ma, Yao Xiao, Theodore L. Willke, Nesreen K. Ahmed, Shahin Nazarian, and Paul Bogdan

7 RELATED WORK
Parallel computing enables the continued growth of complex applications [Asanovic et al. 2006,
2009]. Most existing work [Murray et al. 2013, 2011; Yu et al. 2008] exploits the coarse-grained
parallelism of the dataflow graphs where it is common to represent computations as nodes and data
dependencies among them as edges. The work in [Murray et al. 2013] proposes a new computa-
tional model, timely dataflow, and captures opportunities for parallelism across different algorithms.
Timely dataflow combines dataflow graphs with timestamps to allow vertices send and receive
logically timestamped messages along directed edges for data parallel computation in a distributed
cluster. [Yu et al. 2008] proposes DryadLINQ, a system for general-purpose data-parallel com-
putation. The system architecture incorporates the dataflow graph representation of jobs with a
centralized job manager to schedule jobs on clustered computers. [Murray et al. 2011] introduces
CIEL, a universal execution engine for distributed dataflow programs. It coordinates the distributed
execution of a set of data-parallel tasks, and dynamically builds the DAG as tasks execute. Others
develop different edge-cut graph partitioning algorithms in parallel computing such as spectral
graph theory [Hendrickson and Leland 1995a], hypergraph models [Devine et al. 2006; Hendrickson
and Kolda 2000], and a multi-level algorithm [Hendrickson and Leland 1995b]. Few [Xiao et al.
2018, 2017] exploit the fine-grained instruction parallelism in high-level programs and propose
community detection inspired optimization models to benefit from the underlying hardware such
as multi-core platforms and processing-in-memory architectures.

Related works in vertex cut are mainly from the distributed graph computing field, where vertex-
cuts are used to partition large power-law graphs for optimizing the distributed execution of real
applications such as PageRank. The PowerGraph [Gonzalez et al. 2012] and Libra [Xie et al. 2014]
discussed in previous sections are two state-of-the-art works in this field. Some other relevant
works include [Jain et al. 2013], [Gonzalez et al. 2014], and [Chen et al. 2019].

8 CONCLUSION
In this paper, we explore IR instruction-level parallelism via graph partitioning on LLVM IR graphs
and cluster-to-core mapping for optimizing the parallel execution of applications on multi-core
systems. we propose a vertex cut-based framework on LLVM IR graphs for load balancing and
parallel optimization of application execution on multi-core systems. Specifically, we formulate
a new problem called Weight Balanced ùëù-way Vertex Cut by incorporating the weights into the
optimization of vertex-cut graph partitioning, and we provide greedy algorithms for solving this
problem. We also introduce a memory-centric run-time mapping algorithm for mapping the graph
clusters to the multi-core architecture. Our simulation results demonstrate the superior performance
of the proposed framework for load balancing and multi-core execution speed-up compared to the
state-of-the-art baselines.

REFERENCES
Lada A Adamic and Bernardo A Huberman. 2002. Zipf‚Äôs law and the Internet. Glottometrics 3, 1 (2002), 143‚Äì150.
Konstantin Andreev and Harald Racke. 2006. Balanced graph partitioning. Theory of Computing Systems 39, 6 (2006),

929‚Äì939.

Krste Asanovic, Ras Bodik, Bryan Christopher Catanzaro, Joseph James Gebis, Parry Husbands, Kurt Keutzer, David A
Patterson, William Lester Plishker, John Shalf, Samuel Webb Williams, et al. 2006. The landscape of parallel computing
research: A view from berkeley. (2006).

Krste Asanovic, Rastislav Bodik, James Demmel, Tony Keaveny, Kurt Keutzer, John Kubiatowicz, Nelson Morgan, David
Patterson, Koushik Sen, John Wawrzynek, et al. 2009. A view of the parallel computing landscape. Commun. ACM 52, 10
(2009), 56‚Äì67.

Punam Bedi and Chhavi Sharma. 2016. Community detection in social networks. Wiley Interdisciplinary Reviews: Data

Mining and Knowledge Discovery 6, 3 (2016), 115‚Äì135.

, Vol. 1, No. 1, Article . Publication date: October 2020.

Short Title

25

Nathan Binkert, Bradford Beckmann, Gabriel Black, Steven K Reinhardt, Ali Saidi, Arkaprava Basu, Joel Hestness, Derek R
Hower, Tushar Krishna, Somayeh Sardashti, et al. 2011. The gem5 simulator. ACM SIGARCH computer architecture news
39, 2 (2011), 1‚Äì7.

CL Philip Chen and Chun-Yang Zhang. 2014. Data-intensive applications, challenges, techniques and technologies: A survey

on Big Data. Information sciences 275 (2014), 314‚Äì347.

Rong Chen, Jiaxin Shi, Yanzhe Chen, Binyu Zang, Haibing Guan, and Haibo Chen. 2019. Powerlyra: Differentiated graph
computation and partitioning on skewed graphs. ACM Transactions on Parallel Computing (TOPC) 5, 3 (2019), 1‚Äì39.
G Dantzig and Delbert Ray Fulkerson. 2003. On the max flow min cut theorem of networks. Linear inequalities and related

systems 38 (2003), 225‚Äì231.

Karen D Devine, Erik G Boman, Robert T Heaphy, Rob H Bisseling, and Umit V Catalyurek. 2006. Parallel hypergraph
partitioning for scientific computing. In Proceedings 20th IEEE International Parallel & Distributed Processing Symposium.
IEEE, 10‚Äìpp.

Antonio J Dorta, Casiano Rodriguez, and Francisco de Sande. 2005. The OpenMP source code repository. In 13th Euromicro

Conference on Parallel, Distributed and Network-Based Processing. IEEE, 244‚Äì250.

Joseph E Gonzalez, Yucheng Low, Haijie Gu, Danny Bickson, and Carlos Guestrin. 2012. Powergraph: Distributed graph-
parallel computation on natural graphs. In Presented as part of the 10th {USENIX} Symposium on Operating Systems
Design and Implementation ({OSDI} 12). 17‚Äì30.

Joseph E Gonzalez, Reynold S Xin, Ankur Dave, Daniel Crankshaw, Michael J Franklin, and Ion Stoica. 2014. Graphx:
Graph processing in a distributed dataflow framework. In 11th {USENIX} Symposium on Operating Systems Design and
Implementation ({OSDI} 14). 599‚Äì613.

Matthew R Guthaus, Jeffrey S Ringenberg, Dan Ernst, Todd M Austin, Trevor Mudge, and Richard B Brown. 2001. MiBench:
A free, commercially representative embedded benchmark suite. In Proceedings of the fourth annual IEEE international
workshop on workload characterization. WWC-4 (Cat. No. 01EX538). IEEE, 3‚Äì14.

Bruce Hendrickson and Tamara G Kolda. 2000. Graph partitioning models for parallel computing. Parallel computing 26, 12

(2000), 1519‚Äì1534.

Bruce Hendrickson and Robert Leland. 1995a. An improved spectral graph partitioning algorithm for mapping parallel

computations. SIAM Journal on Scientific Computing 16, 2 (1995), 452‚Äì469.

Bruce Hendrickson and Robert W Leland. 1995b. A Multi-Level Algorithm For Partitioning Graphs. SC 95, 28 (1995), 1‚Äì14.
Nilesh Jain, Guangdeng Liao, and Theodore L Willke. 2013. Graphbuilder: scalable graph etl framework. In First International

Workshop on Graph Data Management Experiences and Systems. 1‚Äì6.

Andreas Krause and Daniel Golovin. 2014. Submodular Function Maximization. , 71‚Äì104 pages.
Dominique LaSalle, Md Mostofa Ali Patwary, Nadathur Satish, Narayanan Sundaram, Pradeep Dubey, and George Karypis.
2015. Improving graph partitioning for modern graphs and architectures. In Proceedings of the 5th Workshop on Irregular
Applications: Architectures and Algorithms. 1‚Äì4.

Chris Lattner and Vikram Adve. 2004. LLVM: A compilation framework for lifelong program analysis & transformation. In

International Symposium on Code Generation and Optimization, 2004. CGO 2004. IEEE, 75‚Äì86.

Derek G Murray, Frank McSherry, Rebecca Isaacs, Michael Isard, Paul Barham, and Mart√≠n Abadi. 2013. Naiad: a timely

dataflow system. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. 439‚Äì455.

Derek G Murray, Malte Schwarzkopf, Christopher Smowton, Steven Smith, Anil Madhavapeddy, and Steven Hand. 2011.

CIEL: a universal execution engine for distributed data-flow computing.

Tim Verbelen, Tim Stevens, Filip De Turck, and Bart Dhoedt. 2013. Graph partitioning algorithms for optimizing software

deployment in mobile cloud computing. Future Generation Computer Systems 29, 2 (2013), 451‚Äì459.

Yao Xiao, Shahin Nazarian, and Paul Bogdan. 2018. Prometheus: Processing-in-memory heterogeneous architecture design
from a multi-layer network theoretic strategy. In 2018 Design, Automation & Test in Europe Conference & Exhibition
(DATE). IEEE, 1387‚Äì1392.

Yao Xiao, Shahin Nazarian, and Paul Bogdan. 2019. Self-optimizing and self-programming computing systems: A combined
compiler, complex networks, and machine learning approach. IEEE Transactions on Very Large Scale Integration (VLSI)
Systems 27, 6 (2019), 1416‚Äì1427.

Yao Xiao, Yuankun Xue, Shahin Nazarian, and Paul Bogdan. 2017. A load balancing inspired optimization framework for
exascale multicore systems: A complex networks approach. In 2017 IEEE/ACM International Conference on Computer-Aided
Design (ICCAD). IEEE, 217‚Äì224.

Cong Xie, Ling Yan, Wu-Jun Li, and Zhihua Zhang. 2014. Distributed power-law graph computing: Theoretical and empirical

analysis. In Advances in neural information processing systems. 1673‚Äì1681.

Yuan Yu, Michael Isard, Dennis Fetterly, Mihai Budiu, √ölfar Erlingsson, Pradeep Kumar Gunda, and Jon Currey. 2008.
DryadLINQ: A System for General-Purpose Distributed Data-Parallel Computing Using a High-Level Language. In
Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation (San Diego, California)
(OSDI‚Äô08). USENIX Association, USA, 1‚Äì14.

, Vol. 1, No. 1, Article . Publication date: October 2020.

