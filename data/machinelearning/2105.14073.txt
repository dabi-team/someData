Task-Guided Inverse Reinforcement Learning Under Partial Information

Franck Djeumou1, Murat Cubuktepe1, Craig Lennon2, and Ufuk Topcu1
1 The University of Texas at Austin
2 Army Research Laboratory
fdjeumou@utexas.edu, mcubuktepe@utexas.edu, craig.t.lennon.civ@mail.mil, utopcu@utexas.edu

1
2
0
2

c
e
D
6
1

]

G
L
.
s
c
[

2
v
3
7
0
4
1
.
5
0
1
2
:
v
i
X
r
a

Abstract

We study the problem of inverse reinforcement learning
(IRL), where the learning agent recovers a reward function
using expert demonstrations. Most of the existing IRL tech-
niques make the often unrealistic assumption that the agent
has access to full information about the environment. We re-
move this assumption by developing an algorithm for IRL in
partially observable Markov decision processes (POMDPs).
The algorithm addresses several limitations of existing tech-
niques that do not take the information asymmetry between
the expert and the learner into account. First, it adopts causal
entropy as the measure of the likelihood of the expert demon-
strations as opposed to entropy in most existing IRL tech-
niques, and avoids a common source of algorithmic com-
plexity. Second, it incorporates task speciﬁcations expressed
in temporal logic into IRL. Such speciﬁcations may be in-
terpreted as side information available to the learner a priori
in addition to the demonstrations and may reduce the infor-
mation asymmetry. Nevertheless, the resulting formulation is
still nonconvex due to the intrinsic nonconvexity of the so-
called forward problem, i.e., computing an optimal policy
given a reward function, in POMDPs. We address this non-
convexity through sequential convex programming and in-
troduce several extensions to solve the forward problem in
a scalable manner. This scalability allows computing poli-
cies that incorporate memory at the expense of added com-
putational cost yet also outperform memoryless policies. We
demonstrate that, even with severely limited data, the algo-
rithm learns reward functions and policies that satisfy the task
and induce a similar behavior to the expert by leveraging the
side information and incorporating memory into the policy.

Introduction
Inverse reinforcement learning (IRL) is a technique that
recovers a reward function using expert demonstrations
and learns a policy inducing a similar behavior to the ex-
pert’s. IRL techniques have found a wide range of appli-
cations (Abbeel, Coates, and Ng 2010; Kitani et al. 2012;
Hadﬁeld-Menell et al. 2016; Dragan and Srinivasa 2013;
Finn, Levine, and Abbeel 2016). The majority of the work
has focused on Markov decision processes (MDPs), assum-
ing that the learning agent can fully observe the state of
the environment and the expert’s demonstrations (Abbeel,

Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Coates, and Ng 2010; Ziebart et al. 2008; Zhou, Bloem, and
Bambos 2017; Ziebart, Bagnell, and Dey 2010; Hadﬁeld-
Menell et al. 2016; Finn, Levine, and Abbeel 2016). Often,
in reality, the learning agent will not have such full observa-
tion. For example, a robot will never know everything about
its environment (Ong et al. 2009; Bai, Hsu, and Lee 2014;
Zhang et al. 2017) and may not observe the internal states
of a human with whom it works (Akash et al. 2019; Liu and
Datta 2012). Such information limitations violate the intrin-
sic assumptions made in existing IRL techniques.

We study IRL in partially observable Markov decision
processes (POMDPs), a widely used model for decision-
making under imperfect information. The partial observabil-
ity brings three key challenges in IRL. The ﬁrst two chal-
lenges are related to the so-called information asymmetry
between the expert and the learner. First, the expert typically
has access to full information about the environment, while
the learner has only a partial view of the expert’s demon-
strations. Second, even in the hypothetical case in which the
actual reward function is known to the learner, the learner’s
optimal policy under limited information may not yield the
same behavior as the expert due to information asymmetry.
The third challenge is due to the computational complex-
ity of policy synthesis in POMDPs. Many standard IRL tech-
niques rely on a subroutine that solves the so-called forward
problem, i.e., computing an optimal policy for a given re-
ward. Solving the forward problem for POMDPs is signiﬁ-
cantly more challenging than MDPs, both theoretically and
practically. Optimal policies for POMDPs may require inﬁ-
nite memory of observations (Madani, Hanks, and Condon
1999), whereas memoryless policies are enough for MDPs.
An additional limitation in existing IRL techniques is
due to the limited expressivity and often impracticability
of state-based reward functions in representing complex
tasks (Littman et al. 2017). For example, it will be tremen-
dously difﬁcult to deﬁne a merely state-based reward func-
tion to describe requirements such as “do not steer off the
road while reaching the target location and coming back
to home” or “monitor multiple locations with a certain or-
der.” On the other hand, such requirements can be concisely
and precisely speciﬁed in temporal logic (Baier and Katoen
2008; Pnueli 1977). Recent work has demonstrated the util-
ity of incorporating temporal logic speciﬁcations into IRL
in MDPs (Memarian et al. 2020; Wen, Papusha, and Topcu

 
 
 
 
 
 
2017). In this work, we address these challenges and limita-
tions in IRL techniques by studying the problem:

Task-guided IRL: Given a POMDP, a task speciﬁca-
tion ϕ expressed in temporal logic, and a set of expert
demonstrations, learn a policy along with the underly-
ing reward function that maximizes the causal entropy
of the induced stochastic process, induces a behavior
similar to the expert’s, and ensures satisfaction of ϕ.

We highlight two parts of the problem statement. Using
causal entropy as an optimization criterion results in a least-
committal policy that induces a behavior obtaining the same
accumulated reward as the expert’s demonstrations while
making no additional assumptions about the demonstrations.
Given the task requirements, the task speciﬁcations guide
the learning process by describing the feasible behaviors and
allowing to learn performant policies with respect to the task
requirements. Such speciﬁcations can also be interpreted as
side information available to the learner a priori in addition
to the demonstrations and partially alleviates the information
asymmetry between the expert and the learner.

Most existing work on IRL relies on entropy as a mea-
sure of the likelihood of the demonstrations, yet, when ap-
plied to stochastic MDPs, has to deal with nonconvex op-
timization problems (Ziebart et al. 2008; Ziebart, Bagnell,
and Dey 2010). On the other hand, IRL techniques that adopt
causal entropy as the measure of likelihood enjoy formula-
tions based on convex optimization (Zhou, Bloem, and Bam-
bos 2017; Ziebart, Bagnell, and Dey 2010). We show similar
algorithmic beneﬁts in maximum-causal-entropy IRL carry
over from MDPs to POMDPs.

A major difference between MDPs and POMDPs in
maximum-causal-entropy IRL is, though, due to the intrinsic
nonconvexity of policy synthesis in POMDPs, which yields
a formulation of the task-guided IRL problem as a noncon-
vex optimization. It is known that this nonconvexity severely
limits the scalability for synthesis in POMDPs. We develop
an algorithm that solves the resulting nonconvex problem
in a scalable manner by adapting sequential convex pro-
gramming (SCP) (Yuan 2015; Mao et al. 2018). The algo-
rithm is iterative. In each iteration, it linearizes the underly-
ing nonconvex problem around the solution from the previ-
ous iteration. The algorithm introduces several extensions,
among which a veriﬁcation step not present in existing SCP
schemes. We show that it computes a sound and locally op-
timal solution to the task-guided IRL problem.

In several examples, we show that the algorithm scales
to POMDPs with tens of thousands of states as opposed to
tens of states in the existing work, e.g., belief-based tech-
niques. In POMDPs, ﬁnite-memory policies that are func-
tions of the history of the observations outperform memory-
less policies (Yu and Bertsekas 2008). Computing a ﬁnite-
memory policy for a POMDP is equivalent to computing
a memoryless policy on a larger product POMDP (Junges
et al. 2018). On the other hand, existing IRL techniques on
POMDPs cannot effectively utilize memory, as they do not
scale to large POMDPs. We leverage the scalability of our
algorithm to compute more performant policies that incor-
porate memory using ﬁnite-state controllers (Meuleau et al.

1999; Amato, Bernstein, and Zilberstein 2010).

We demonstrate the applicability of the approach through
several examples. We show that, without task speciﬁcations,
the developed algorithm can compute more performant poli-
cies than a straight adaptation of the original GAIL (Ho
and Ermon 2016) to POMDPs. Then, we demonstrate that
by incorporating task speciﬁcations into the IRL procedure,
the learned reward function and policy accurately describe
the behavior of the expert while outperforming the policy
obtained without the task speciﬁcations. Additionally, we
show that incorporating memory into the learning agent’s
policy leads to more performant policies. We also show that
with more limited data, the performance gap becomes more
prominent between the learned policies with and without us-
ing task speciﬁcations. Finally, we demonstrate the scalabil-
ity of our approach for solving the forward problem through
extensive comparisons with several state-of-the-art POMDP
solvers and show that on larger POMDPs, the algorithm can
compute more performant policies in signiﬁcantly less time.

Related work. The closest work to ours is by Choi and
Kim (2011), where they extend classical maximum-margin-
based IRL techniques for MDPs to POMDPs. However, even
on MDPs, maximum-margin-based approaches cannot re-
solve the ambiguity caused by suboptimal demonstrations,
and they work well when there is a single reward function
that is clearly better than alternatives (Osa et al. 2018). In
contrast, we adopt causal entropy that has been shown (Osa
et al. 2018; Ziebart, Bagnell, and Dey 2010) to alleviate
these limitations on MDPs. Besides, Choi and Kim (2011)
rely on efﬁcient off-the-shelf solvers to the forward problem.
Instead, this paper also develops an algorithm that outper-
forms off-the-shelf solvers and can scale to POMDPs that
are orders of magnitude larger compared to the examples in
Choi and Kim (2011). Further, Choi and Kim (2011) do not
incorporate task speciﬁcations in their formulations.

Prior work tackled the ill-posed IRL problem using max-
imum margin formulations (Ratliff, Bagnell, and Zinkevich
2006; Abbeel and Ng 2004; Ng, Russell et al. 2000), or prob-
abilistic models to compute the likelihood of expert demon-
strations (Ramachandran and Amir 2007; Ziebart et al. 2008;
Ziebart, Bagnell, and Dey 2010; Zhou, Bloem, and Bambos
2017; Finn, Levine, and Abbeel 2016; Ho and Ermon 2016).
Besides, the idea of using side information to guide and aug-
ment IRL has been explored in recent work (Papusha, Wen,
and Topcu 2018; Wen, Papusha, and Topcu 2017; Memarian
et al. 2020). However, these IRL techniques are only appli-
cable to MDPs as opposed to POMDPs.

IRL under some restricted notion of partial information
has been studied in prior work. Boularias, Kr¨omer, and Pe-
ters (2012) consider the setting where the features of the re-
ward function are partially speciﬁed. Kitani et al. (2012);
Bogert and Doshi (2014) consider IRL problems from par-
tially observable demonstrations and use the hidden Markov
decision process framework as a solution. Therefore, all
these approaches consider a particular case of POMDPs. We
also note that none of these methods incorporate side infor-
mation into IRL and do not provide guarantees on the per-
formance of the policy with respect to a task speciﬁcation.

Background
Notation. We denote the set of nonnegative real numbers
by R+, the set of all probability distributions over a ﬁnite or
countably inﬁnite set X by Distr(X ), the set of all (inﬁnite
or empty) sequences x0, x1, . . . , x∞ with xi ∈ X by (X )∗
for some set X , and the expectation of a function g of jointly
distributed random variables X and Y by EX,Y [g(X, Y )].

POMDPs. A partially observable Markov decision pro-
cess (POMDP) is a tuple M = (S, A, Z, P, O, R, µ0, γ),
with ﬁnite sets S, A and Z denoting the set of states, ac-
tions, and observations, respectively, a transition function
P : S × A (cid:55)→ Distr(S), an observation function O : S (cid:55)→
Distr(Z), a reward function R : S × A (cid:55)→ R+, an initial
state of distribution µ0 ∈ Distr(S), and a discount factor
γ ∈ (0, 1). We denote P(s(cid:48)|s, α) as the probability of tran-
sitioning to state s(cid:48) after an action α is selected in state s,
and O(z|s) is the probability of observing z ∈ Z in state s.

Policies. An observation-based policy σ : (Z×A)∗×Z (cid:55)→
Distr(A) for a POMDP M maps a sequence of observations
and actions to a distribution over actions. A M-ﬁnite-state
controller (M-FSC) consists of a ﬁnite set of memory states
of size M and two functions. The action mapping η(n, z)
takes a FSC memory state n and an observation z ∈ Z,
and returns a distribution over the POMDP actions. The
memory update δ(n, z, α) returns a distribution over mem-
ory states and is a function of the action α selected by η.
An FSC induces an observation-based policy by following
a joint execution of these two functions upon a trace of the
POMDP. Memoryless FSCs, denoted by σ : Z → Distr(A),
are observation-based policies, where σz,α is the probability
of taking the action α given solely observation z.

Remark 1 (REDUCTION TO MEMORYLESS POLICIES). In
the remainder of the paper, for ease of notation, we synthe-
size optimal M-FSCs for POMDPs (so-called forward prob-
lem) by computing memoryless policies σ on theoretically-
justiﬁed larger POMDPs obtained from the so-called prod-
uct of the memory update δ and the original POMDPs. In-
deed, Junges et al. (2018) provide product POMDPs, whose
sizes grow polynomially with the size of the domain of δ.

Causal Entropy in POMDPs. For a POMDP M, a policy
σ induces the stochastic processes Sσ
∞),
0:∞ := (Z σ
0 , . . . , Aσ
Aσ
∞). At
each time index t, the random variables Sσ
t , and Z σ
t
take values st ∈ S, αt ∈ A, and zt ∈ Z, respectively.

0 , . . . , Z σ
t , Aσ

0:∞ := (Aσ

∞), and Z σ

0:∞ := (Sσ

0 , . . . , Sσ

t )].

t |Sσ

[− log P(Aσ

t=0 γtEAσ

σ := (cid:80)∞

We consider the inﬁnite time horizon setting and deﬁne
the discounted causal entropy as (Zhou, Bloem, and Bambos
2017): H γ
t ,Sσ
t
Remark 2. The entropy of POMDPs (or MDPs) involves
the future policy decisions (Ziebart et al. 2008), i.e., Sσ
t+1:T ,
at a time index t, as opposed to the causal entropy in
POMDPs (or MDPs). Thus, Ziebart et al. (2008) show that
the problem of computing a policy that maximizes the en-
tropy is nonconvex, even in MDPs. Inverse reinforcement
learning techniques that maximize the entropy of the policy
rely on approximations or assume that the transition func-
tion of the MDP is deterministic. On the other hand, comput-

ing a policy that maximizes the causal entropy can be formu-
lated as a convex optimization problem in MDPs (Ziebart,
Bagnell, and Dey 2010; Zhou, Bloem, and Bambos 2017).

LTL Speciﬁcations. We use general linear temporal logic
(LTL) to express complex task speciﬁcations on the POMDP
M. Given a set AP of atomic propositions, i.e., Boolean
variables with truth values for a given state s or observation
z, LTL formulae are constructed inductively as following:
ϕ := true | a | ¬ϕ | ϕ1 ∧ϕ2 | Xϕ | ϕ1Uϕ2, where a ∈ AP,
ϕ, ϕ1, and ϕ2 are LTL formulae, ¬ and ∧ are the logic nega-
tion and conjunction, and X and U are the next and until
temporal operators. Besides, temporal operators such as al-
ways (G) and eventually (F) are derived as Fϕ := trueUϕ
and Gϕ := ¬F¬ϕ. A detailed description of the syntax and
semantics of LTL is beyond the scope of this paper and can
be found in Pnueli (1977); Baier and Katoen (2008).

M(ϕ) denotes the probability of satisfying the LTL for-

mula ϕ when following the policy σ on the POMDP M.

Prσ

Formal Problem Statement
In this section, we formulate the problem of task-guided in-
verse reinforcement learning (IRL) in POMDPs. Given a
POMDP M with an unknown reward function R, we seek to
learn a reward function R along with an underlying policy σ
that induces a behavior similar to the expert demonstrations.
We deﬁne an expert trajectory on the POMDP M as
the perceived observation and executed action sequence
τ = {(z0, α0), (z1, α1), . . . , (zT , αT )}, where zi ∈ Z and
αi ∈ A for all i ∈ {0, . . . , T }, and T denotes the length of
the trajectory. Similarly to Choi and Kim (2011), we assume
given or we can construct from τ (Bayesian inference), the
belief trajectory bτ = {b0 := µ0, . . . , bT }, where bi(s) is the
probability of being at state s at time index i. In the follow-
ing, we assume that we are given a set of belief trajectories
D = {bτ1 , . . . , bτN } from trajectories τ1, . . . , τN , where N
denotes the total number of underlying trajectories.

We build on the traditional encoding of the reward func-
tion as R(s, α) := (cid:80)d
k=1 θkφk(s, α) = θTφ(s, α), where
φ : S × A (cid:55)→ Rd is a known vector of basis functions with
components referred to as feature functions, θ ∈ Rd is an un-
known weight vector characterizing the importance of each
feature, and d is the number of features.

t ,Aσ
t
(cid:80)

t=0 γtESσ
(cid:80)
bτ ∈D

t , Aσ
[φ(Sσ
bi∈bτ γi (cid:80)

Speciﬁcally, we seek for a weight θ deﬁning R and a
policy σ such that its discounted feature expectation Rφ
σ
matches an empirical discounted feature expectation ¯Rφ of
σ = ¯Rφ,
the expert demonstration D. That is, we have that Rφ
σ := (cid:80)∞
where Rφ
t )|σ] and the empir-
ical mean ¯Rφ = 1
s∈S bi(s)φ(s, αi).
N
However, there may be inﬁnitely many reward functions
and policies that can satisfy this feature matching condition.
Therefore, to resolve the policy ambiguity, we seek for a
policy σ that also maximizes the discounted causal entropy
H γ
Problem 1. Given a reward-free POMDP M, a demonstra-
tion set D, and a feature φ, compute a policy σ and weight θ
such that (a) (cid:80)∞
t )|σ] = ¯Rφ; (b) The
causal entropy H γ

σ . We now deﬁne the problem of interest.

t=0 γtESσ
t ,Aσ
t
σ is maximized by σ.

t , Aσ

[φ(Sσ

Additionally, we seek to incorporate, if available, a priori
high-level side information on the task demonstrated by the
expert in the design of the reward R and the policy σ.
Problem 2. Given a linear temporal logic formula ϕ, com-
pute a policy σ and weight θ such that the constraints (a)
and (b) in Problem 1 are satisﬁed, and Prσ
M(ϕ) ≥ λ for a
given parameter λ ≥ 0.

We note that, even though the parameter λ that speciﬁes
the threshold for satisfaction of ϕ is assumed to be given, the
approach can easily be adapted to compute the optimal λ.

Nonconvex Formulation for IRL in POMDPs
In this section, we formulate the IRL problem and the con-
straints induced by (a) and (b) in Problem 1 and Problem 2
as a nonconvex optimization problem. Then, we utilize a La-
grangian relaxation of the nonconvex problem as a part of
our solution approach. We recall the reader (see Remark 1)
that we compute M-FSC for POMDPs by computing mem-
oryless policies σ on larger product POMDPs.

Substituting Visitation Counts. We eliminate the (inﬁ-
nite) time dependency in H γ
σ and the feature matching con-
straint by a substitution of variables involving the policy-
σ : S (cid:55)→ R+ and
induced discounted state visitation count µγ
σ : S × A (cid:55)→ R+. For a pol-
state-action visitation count νγ
icy σ, state s, and action α, the discounted visitation counts
:= ESt[(cid:80)∞
t=1 γt1{St=s}|σ] and
are deﬁned by µγ
σ (s, α) := EAt,St[(cid:80)∞
t=1 γt1{St=s,At=α}|σ], where 1{·}
νγ
σ (s, α) = πs,αµγ
is the indicator function. Further, νγ
σ(s),
where πs,α = P[At = a|St = s] is a state-based policy.

σ(s)

We ﬁrst provide a concave expression for the discounted

causal entropy H γ
H γ

(cid:88)∞

σ :=

t=0

σ as a function of µγ
γtESσ
(cid:88)

[− log(πst,αt)]

t ,Aσ
t

(cid:88)∞

σ and νγ
σ :

=

=

t=0

(s,α)∈S×A

(cid:88)

(s,α)∈S×A

−(log πs,α)πs,αµγ

σ(s)

−(log πs,α)πs,αγtP[Sσ

t = s]

=

(1)

(cid:88)

− log

(s,α)∈S×A

νγ
σ (s, α),

νγ
σ (s, α)
µγ
σ(s)
where the ﬁrst equality is due to the deﬁnition of the dis-
counted causal entropy H γ
σ , the second equality obtained by
expanding the expectation. The third and fourth equalities
follow by the deﬁnition of the state visitation count µγ
σ, and
the state-action visitation count νγ
σ . Next, we obtain a linear
expression in νγ
σ for the discounted feature expectation Rσ
φ
as:

Rφ

σ =

=

∞
(cid:88)

(cid:88)

φ(s, α)γtP[Sσ

t = s, Aσ

t = α]

t=0

(s,α)∈S×A
(cid:88)

φ(s, α)νγ

σ (s, α),

(s,α)∈S×A

where the second equality is obtained by the deﬁnition of
the visitation count νγ
σ . The following nonconvex constraint
in µγ

σ(s) and σz,α ensures observation-based policies:

σ (s, α) = µγ
νγ

σ(s)

(cid:88)

z∈Z

O(z|s)σz,α.

(3)

Algorithm 1: Compute the weight vector θ and policy σ so-
lution of the Lagrangian relaxation of the IRL problem.
Input: Feature expectation ¯Rφ from D, initial weight θ0,
step size η : N (cid:55)→ R+, and (if available) a priori side
information ϕ and λ ∈ [0, 1] imposing Prσ

M(ϕ) ≥ λ .

1: σ0 ← uniform policy
(cid:46) Initialize uniform policy
2: for k = 1, 2, . . . , do (cid:46) Compute θ via gradient descent
(cid:46) Solve the
3:

σk ← SCPForward(θk, σk−1, ϕ, λ)
forward problem (5)–(7) with optional ϕ and λ

θk+1 ← θk − η(k)(cid:0)Rφ

σk − ¯Rφ(cid:1)

(cid:46) Gradient step

4:
5: end for
6: return σk, θk

Finally, the variables for the discounted visitation counts
must satisfy the so-called Bellman ﬂow constraint to ensure
that the policy is well-deﬁned. For each state s ∈ S,
(cid:88)
σ (s(cid:48), α).

σ(s) = µ0(s) + γ

P(s|s(cid:48), α)νγ

(cid:88)

µγ

(4)

s(cid:48)∈S

α∈A

Lagrangian Relaxation of Feature Matching Constraint.
Computing a policy σ that satisﬁes the feature matching con-
σ = ¯Rφ might be infeasible due to ¯Rφ being an
straint Rφ
empirical estimate from the ﬁnite set of demonstrations D.
Additionally, the feature matching constraint might also be
infeasible due to the information asymmetry between the ex-
pert and the learner, e.g., the expert has full observation.

We build on a Lagrangian relaxation to incorporate the
feature matching constraints into the objective of the for-
ward problem, similar as other IRL algorithms in the liter-
ature. Speciﬁcally, we introduce θ ∈ Rd as the dual vari-
ables of the relaxed problem. The desired weight vector θ
and policy σ of Problem 1 and Problem 2 are the solutions
σ − ¯Rφ). Algorithm 1
of minθ f (θ) := maxσ H γ
updates the reward weights by using gradient descent. To
this end, the algorithm computes the gradient ∇f (θk) =
σ − ¯Rφ).
σk − ¯Rφ, where σk = arg maxσ H γ
Rφ
In the following, we refer to the problem of computing such
σk given θk as the forward problem, and we develop the al-
gorithm SCPForward, presented in next section, to solve
it in an efﬁcient and scalable manner while incorporating
high-level task speciﬁcations to guide the learning.

σ + (θk)T(Rφ

σ + θT(Rφ

Nonconvex Formulation of
the Forward Problem.
Given a weight vector θk, we take advantage of the obtained
substitution by the expected visitation counts to formulate
the forward problem associated to Problem 1 as the noncon-
vex optimization problem:

maximize
σ,νγ

σ ,σ

µγ

(2)

(cid:88)

− log

νγ
σ (s, α)
µγ
σ(s)

νγ
σ (s, α)

(s,α)∈S×A
(cid:88)

+

(s,α)∈S×A

(θk)Tφ(s, α)νγ

σ (s, α),

(5)

subject to (3) − (4),
∀(s, α) ∈ S × A, µγ

∀(s, α) ∈ S × A, µγ

σ(s) ≥ 0, νγ
σ(s) =

(cid:88)

σ (s, α) ≥ 0,
νγ
σ (s, α),

α∈A

(6)

(7)

where the source of nonconvexity is from (3), and we re-
move the constant −(θk)T ¯Rφ from the cost function.

Sequential Convex Programming Formulation
We develop SCPForward, adapting a sequential convex
programming (SCP) scheme to efﬁciently solve the non-
convex forward problem (5)–(7). SCPForward involves a
veriﬁcation step to compute sound policies and visitation
counts, which is not present in the existing SCP schemes.
Additionally, we describe in the next section how to take ad-
vantage of high-level task speciﬁcation (Problem 2) through
slight modiﬁcations of the obtained optimization problem
solved by SCPForward.

Linearizing Nonconvex Problem
SCPForward iteratively linearizes the nonconvex con-
straints in (3) around a previous solution. However, the lin-
earization may result in an infeasible or unbounded linear
subproblem (Mao et al. 2018). We ﬁrst add slack variables to
the linearized constraints to ensure feasibility. The linearized
problem may not accurately approximate the nonconvex
problem if the solutions to this problem deviate signiﬁcantly
from the previous solution. Thus, we utilize trust region con-
straints (Mao et al. 2018) to ensure that the linearization is
accurate to the nonconvex problem. At each iteration, we in-
troduce a veriﬁcation step to ensure that the computed pol-
icy and visitation counts are not just approximations but ac-
tually satisfy the nonconvex policy constraint (3), improves
the realized cost function over past iterations, and satisfy the
temporal logic speciﬁcations, if available.

Linearizing Nonconvex Constraints and Adding Slack
Variables. We linearize the nonconvex constraint (3),
which is quadratic in µγ
σ(s) and σz,α, around the previ-
ously computed solution denoted by ˆσ, µγ
ˆσ . How-
ever, the linearized constraints may be infeasible. We alle-
viate this drawback by adding slack variables ks,α ∈ R for
(s, α) ∈ S × A, which results in the afﬁne constraint:

ˆσ, and νγ

σ (s, α) + ks,α = µγ
νγ
(cid:0)µγ

ˆσ(s)
σ(s) − µγ

z∈Z

(cid:88)

ˆσ(s)(cid:1) (cid:88)

O(z|s)σz,α +

(8)

O(z|s)ˆσz,α.

z∈Z

Trust Region Constraints. The linearization may be in-
accurate if the solution deviates signiﬁcantly from the pre-
vious solution. We add following trust region constraints to
alleviate this drawback:

∀(z, α) ∈ Z × A,

ˆσz,α/ρ ≤ σz,α ≤ ˆσz,αρ,

(9)

where ρ is the size of the trust region to restrict the set of
allowed policies in the linearized problem. We augment the
cost function in (5) with the term −β (cid:80)
(s,α)∈S×A ks,α to
ensure that we minimize the violation of the linearized con-
straints, where β is a large positive constant.

Linearized Problem. Finally, by differentiating x (cid:55)→
x log x and y (cid:55)→ x log(x/y), we obtain the coefﬁcients re-
quired to linearize the convex causal entropy cost function

in (5). Thus, we obtain the following linear program (LP):

maximize
σ,νγ

σ ,σ

µγ

(cid:88)

(s,α)∈S×A

(cid:32)

−

βks,α −

(cid:17)

µγ

σ(s)

(cid:16) νγ
ˆσ (s, α)
µγ
ˆσ(s)
(cid:33)

νγ
ˆσ (s, α)
µγ
ˆσ(s)

(cid:17)

+ 1

νγ
σ (s, α)

(cid:16)

log

(cid:88)

+

+

(s,α)∈S×A

(θk)Tφ(s, α)νγ

σ (s, α) (10)

subject to (4), (6) − (9).

˜σ , ˜µγ

Veriﬁcation Step. After each iteration, the linearization
might be inaccurate, i.e, the resulting policy ˜σ and poten-
tially inaccurate visitation counts ˜νγ
˜σ might not be fea-
sible to the nonconvex policy constraint (3). As a conse-
quence of the potential infeasibility, the currently attained
(linearized) optimal cost might signiﬁcantly differ from the
realized cost by the feasible visiation counts for the ˜σ. Ad-
ditionally, existing SCP schemes linearizes the nonconvex
problem around the previously inaccurate solutions for ˜νγ
˜σ ,
and ˜µγ
˜σ, further propagating the inaccuracy. The proposed
veriﬁcation step solves these issues. Given the computed
policy ˜σ, SCPForward computes the unique and sound so-
lution for the visitation count µγ
˜σ by solving the correspond-
ing Bellman ﬂow constraints:

µγ
˜σ(s) =µ0(s)+
γ

(cid:88)

(cid:88)

P(s|s(cid:48), α)µγ

˜σ(s(cid:48))

(cid:88)

O(z|s)˜σz,α,

(11)

α∈A

s(cid:48)∈S
for all s ∈ S, and where µγ
linear program. Then, SCPForward computes νγ
˜σ(s(cid:48)) (cid:80)
µγ
current iteration is deﬁned by

˜σ ≥ 0 is the only variable of the
˜σ (s, α) =
z∈Z O(z|s)˜σz,α and the realized cost cost at the

z∈Z

C(˜σ, θk) =

(cid:88)

(s,α)∈S×A

(cid:88)

(s,α)∈S×A

+

− log

νγ
˜σ (s, α)
µγ
˜σ

νγ
˜σ (s, α)

(θk)Tφ(s, α)νγ

˜σ (s, α),

(12)

where we assume 0 log 0 = 0. Finally, if the realized cost
C(˜σ, θk) does not improve over the previous cost C(ˆσ, θk),
the veriﬁcation step rejects the obtained policy ˜σ, contracts
the trust region and SCPForward iterates with the previ-
ous solutions ˆσ, µγ
ˆσ . Otherwise, the linearization
is sufﬁciently accurate, the trust region is expanded, and
SCPForward iterates with ˜σ, µγ
˜σ . By incorporat-
ing this veriﬁcation step, we ensure that SCPForward al-
ways linearizes the nonconvex optimization problem around
a solution that satisﬁes the nonconvex constraint (3).

ˆσ, and νγ

˜σ and νγ

Incorporating High-Level Task Speciﬁcations
Given high-level side information on the agent tasks as the
LTL formula ϕ, we ﬁrst compute the product of the POMDP
and the ω-automaton representing ϕ to ﬁnd the set T ⊆ S of
states, called target or reach states, satisfying ϕ with proba-
bility 1 by using standard graph-based algorithms as a part of
preprocessing step. We refer the reader to Baier and Katoen
(2008) for a detailed introduction on how LTL speciﬁcations
can be reduced to reachability speciﬁcations given by T .

Algorithm 2: SCPForward: Linear programming-based algorithm to solve the forward problem (5)–(7), i.e., compute a policy
σk that maximizes the causal entropy, considers the feature matching, and satisﬁes the speciﬁcations, if available.
Input: Current weight estimate θk, current best policy ˆσ, side information ϕ and λ, trust region ρ > 1, penalization coefﬁcients

β, βsp ≥ 0, constant ρ0 to expand or contract trust region, and a threshold ρlim for trust region contraction.

ˆσ = µγ
ˆσ via linear constraint (11) and νγ
1: Find µγ
2: Find µsp
ˆσ = µsp
ˆσ via linear constraint (13) with νsp
3: Compute the realized cost C(ˆσ, θk) ← (12) + Csp
4: while ρ > ρlim do
5:

Find optimal ˜σ to the augmented LP (10) via ˆσ, µγ

ˆσ (s(cid:48)) (cid:80)
ˆσ , given ˆσ
ˆσ, νγ

ˆσ(s(cid:48)) (cid:80)

(6), (7), (8), (13), and (spec) induced by µsp

˜σ ,µsp
Compute the realized µγ
{ˆσ ← ˜σ; ρ ← ρρ0} if C(˜σ, θk) ≥ C(ˆσ, θk) else {ρ ← ρ/ρ0}

˜σ , νsp

˜σ, νγ

ˆσ , µsp
σ , and by adding −β (cid:80)
˜σ , and C(˜σ, θk) via ˜σ as in lines 1–3

ˆσ , νsp

σ , νsp

ˆσ

(s,α)∈S×A ksp

z∈Z O(z|s)ˆσz,α, given ˆσ

z∈Z O(z|s)ˆσz,α, given ˆσ

(cid:46) Realized visitation counts
(cid:46) If ϕ is available
(cid:46) Add speciﬁcations’ violation
(cid:46) Trust region threshold
(cid:46) We augment the LP with constraints
s,α − βspΓsp to the cost (10).

(cid:46) Veriﬁcation step

6:
7:
8: end while
9: return σk := ˆσ

As a consequence, the probability of satisfying ϕ is the
sum of the probability of reaching the target states s ∈ T ,
which are given by the undiscounted state visitation count
σ . That is, Prσ
µsp
σ (s). Unless γ = 1,
σ , νsp
σ (cid:54)= µγ
µsp
σ , and
the adequate constraints in the linearized problem (10).

σ. Thus, we introduce new variables µsp

M(ϕ) = (cid:80)

s∈T µsp

Incorporating Undiscounted Visitation Variables to Lin-
earized Problem. We append new constraints, similar
to (6), (7), and (8), into the linearized problem (10), where
ˆσ, νγ
the variables µγ
σ , νsp
σ ,
s,α, µsp
ksp
ˆσ , respectively. Further, we add the constraint

ˆσ are replaced by µsp

σ , ks,α, µγ

ˆσ , νsp

σ, νγ

µsp

σ (s) = µ0(s) +

(cid:88)

(cid:88)

s(cid:48)∈S\T

α∈A

P(s|s(cid:48), α)νsp

σ (s(cid:48), α),

(13)

which is a modiﬁcation of the Bellman ﬂow constraints such
that µsp
σ (s) for all s ∈ T only counts transitions from non-
target states. Finally, we penalize the introduced slack vari-
ables for feasibility of the linearization by augmenting the
cost function with the term −β (cid:80)

(s,α)∈S×A ksp
s,α.

s∈T µsp

Relaxing Speciﬁcation Constraints. We add the con-
straint (spec) := (cid:80)
σ (s) + Γsp ≥ λ to the linearized
problem, where Γsp ≥ 0 is a slack variable ensuring the
linearized problem is always feasible. We augment the cost
function with −βspΓsp to penalize violating ϕ, where βsp is
a hyperparameter positive constant .

Updating Veriﬁcation Step. We modify the previously-
introduced realized cost C(˜σ, θk) to penalize if the obtained
policy does not satisfy the speciﬁcation ϕ. This cost also
accounts for the linearization inaccuracy of the new pol-
icy constraint due to σ, µsp
σ . At each iteration,
SCPForward computes the accurate µsp
˜σ of current pol-
icy ˜σ through solving a feasibility LP with constraints given
by the modiﬁed Bellman ﬂow constraints (13). Then, it aug-
s∈T µsp
ments Csp
˜σ (s)−λ)βsp} to the realized
cost to take the speciﬁcation constraints into account.

˜σ = min{0, ((cid:80)

σ , and νsp

Numerical Experiments
We evaluate the proposed IRL algorithm on several POMDP
instances, from Junges, Jansen, and Seshia (2020). We ﬁrst

compare our IRL algorithm with a straightforward variant
of GAIL (Ho and Ermon 2016) adapted for POMDPs. Then,
we provide some results on the data-efﬁciency of the ap-
proach when taking advantage of side information. Finally,
we demonstrate the scalability of the routine SCPForward
for solving the forward problem through comparisons with
state-of-the-art solvers such as SolvePOMDP (Walraven
and Spaan 2017), SARSOP (Kurniawati, Hsu, and Lee
2008), PRISM-POMDP (Norman, Parker, and Zou 2017).
We consider throughout this section the hyperparameters
β = 1e3, βsp = 10, ρ = 1.01, ρ0 = 1.5, ρlim = 1e−4, and
γ = 0.999. Besides, we provide in the supplementary ma-
terials additional details on the key results of this paper and
the experiments, e.g., preprocessing steps such as the prod-
uct POMDP with M-FSC or computing reachability speciﬁ-
cations from LTL speciﬁcations.

2

1

6

9

12

4

3

7

10

13

5

8

11

14

Figure 1: Some examples from the benchmark set. From left
to right, we have the Maze and Avoid, respectively.

Benchmark Set. The POMDP instances are as follows.
Evade is a turn-based game where the agent must reach
a destination without being intercepted by a faster player.
In Avoid,
the agent must avoid being detected by two
other moving players following certain preset, yet unknown
routes. In Intercept, the agent must intercept another player
who is trying to exit a gridworld. In Rocks, the agents must
sample at least one good rock over the several rocks without
any failures. Finally, in Maze, the agent must exit a maze as
fast as possible while avoiding trap states.

Variants of Learned Policies and Experts. We refer
to four types of policies. The type of policy depends on
whether it uses side information from a temporal speciﬁca-

No information asymmetry

Under information asymmetry

GAIL

Finite-memory policy

25

50

75

100

Without side
information

With side
information

Rφ
σ

0

Rφ
σ

60
40
20
0
−20

60
40
20
0
−20

Memoryless policy

25

50

75

100

Rφ
σ

0

Rφ
σ

60
40
20
0
−20

60
40
20
0
−20

0

25

50

75

100

0

25

50

75

100

Time Steps

Time Steps

Figure 2: Representative results on the Maze example showing the reward of the policies under the true reward function (Rφ
σ)
versus the time steps. Compare the two rows: The policies in the top row that do not utilize side information suffer a performance
drop under information asymmetry. On the other hand, in the bottom row, the performance of policies incorporating side
information into learning does not decrease under information asymmetry. Compare the two columns: The performance of the
ﬁnite-memory policies in the left column is signiﬁcantly better than memoryless policies. Except for the memoryless policies
without side information, our algorithm outperforms GAIL. The expert reward on the MDP is 48.22, while 47.83 on POMDP.

tion ϕ or not, and whether it uses a memory size M = 1 or
M = 10. We also consider two types of experts. The ﬁrst
expert has full information about the environment and com-
putes an optimal policy in the underlying MDP. The second
expert has partial observation and computes a locally opti-
mal policy in the POMDP with a memory size of M = 15.
Recall that the agent always has partial information. There-
fore, the ﬁrst type of expert corresponds to having informa-
tion asymmetry between the learning agent and expert. Be-
sides, we consider as a baseline a variant of GAIL where we
learn the policy on the MDP without side information, and
extend it to POMDPs via an ofﬂine computation of the be-
lief in the states. Doing so provides a signiﬁcant advantage
to GAIL since we learn on the MDP. We do not compare
with Choi and Kim (2011) as explained in the related work.
We discuss the effect of side information and memory in
the policies. While we detail only on the Maze example,
where the agent must exit a maze as fast as possible, we ob-
serve similar patterns for other examples. We give detailed
results for the other examples in the supplementary material.

Maze Example
The POMDP M is speciﬁed by S = {s1, . . . , s14} corre-
sponding to the cell labels in Figure 1. An agent in the maze
only observes whether or not there is a wall (in blue) in a
neighboring cell. That is, the set of observations is O =
{o1, . . . , o6, o7}. For example, o1 corresponds to observing
west and north walls (s1), o2 to north and south walls (s2,
s4), and o5 to east and west walls (s6, s7, s8, s9, s10, s11).
The observations o6 and o7 denote the target state (s13) and
bad states(s12, s14). The transition model is stochastic with
a probability of slipping p = 0.1. Further, the states s13 and
s14 lead to the end of the simulation (trapping states).

In the IRL experiments, we consider three feature func-
tions. We penalize taking more steps with φtime(s, α) = −1
for all s, α. We provide a positive reward when reaching
s13 with φtarget(s, α) = 1 if s = s13 and φtarget(s, α) =
0 otherwise. We penalize bad states s12 and s14 with
φbad(s, α) = −1 if s = s12 or s = s14, and φbad(s, α) = 0
otherwise. Finally, we have the LTL formula ϕ = G ¬ bad
as the task speciﬁcation, where bad is an atomic proposition
that is true if the current state s = s12 or s = s14. We con-
strain the learned policy to satisfy Prσ

M(G ¬ bad) ≥ 0.9.

Side Information Alleviates the Information Asymmetry.
Figure 2 shows that if there is an information asymmetry be-
tween the learning agent and the expert, the policies that do
not utilize side information suffer a signiﬁcant performance
drop. The policies that do not incorporate side information
into learning obtain a lower performance by 57% under in-
formation asymmetry, as shown in the top row of Figure 2.
On the other hand, as seen in the bottom row of Figure 2,
the performance of the policies that use side information is
almost unaffected by the information asymmetry.

Memory Leads to More Performant Policies. The re-
sults in Figure 2 demonstrate that incorporating memory into
the policies improves the performance, i.e., the attained re-
ward, in all examples, both in solving the forward problem
and learning policies from expert demonstrations. Incorpo-
rating memory partially alleviates the effects of information
asymmetry, as the performance of the ﬁnite-memory policy
decreases by 18% under information asymmetry as opposed
to 57% for the memoryless policy.

We see that in Table 2, incorporating memory into policy
on the Maze and Rocks benchmarks, allows SCPForward
to compute policies that are almost optimal, evidenced by

Problem

|S|

|S × O|

Maze
Maze(10-FSC)
Rock
Rock(5-FSC)
Intercept
Intercept
Evade
Evade
Avoid
Avoid

17
161
550
2746
1321
1321
2081
36361
2241
19797

162
2891
4643
41759
5021
7041
16761
341121
8833
62133

|O|

SCPForward
Rφ
Time (s)
σ
39.24
11
46.32
101
19.68
67
19.82
331
19.83
1025
1025
19.81
96.79
1089
18383 94.97
9.86
1956
3164
9.72

0.1
2.04
12.2
97.84
10.28
13.18
26.25
3600
14.63
3503

SARSOP

SolvePOMDP

Rφ
σ
47.83
NA
19.83
NA
19.83
19.81
95.28
−
9.86
−

Time (s) Rφ
σ
47.83
NA
−
NA
−
−
−
−
−
−

0.24
NA
0.05
NA
13.71
81.19
3600
−
210.47
−

Time (s)

0.33
NA
−
NA
−
−
−
−
−
−

Table 1: Results for the benchmarks. On larger benchmarks (e.g., Evade and Avoid), the method we developed can compute
a locally optimal policy. We set the time-out to 3600 seconds. An empty cell (denoted by −) represents the solver failed to
compute any policy before the time-out, while NA refers to not applicable due to the approach being based on belief updates.

obtaining almost the same reward as the solver SARSOP.

Side Information Improves Data Efﬁciency and Perfor-
mance. Figure 3 shows that even on a low data regime,
learning with task speciﬁcations achieves signiﬁcantly bet-
ter performance than without the task speciﬁcations.

Without LTL

With LTL

Opt. Rew. POMDP

d
r
a
w
e
r

l
a
t
o
T

40
30
20

46
44
42
40

5

15
Number of trajectories

10

5

15
Number of trajectories

10

Figure 3: We show the data efﬁciency of the proposed ap-
proach through the total reward obtained by the learned poli-
cies as a function of the number of expert demonstrations
(No information asymmetry). The ﬁgure on the left shows
the performance of learning memoryless policies, while the
ﬁgure on the right shows the performance of a 5-FSC.

Besides, in a more complicated environment such as
Avoid, Figure 9 shows that task speciﬁcations are crucial
to hope even to learn the task. We refer the reader to the
supplementary material for the details of the experiment.

With side information

Without side information

φσ
R

40
20
0
−20

0

75

150
Time Steps

225

300

Figure 4: Results on the Avoid example show that side in-
formation can help to crucially improve the performance.

SCPForward Yields Better Scalability
We highlight three observations regarding the scalability
of SCPForward. First, the results in Table 2 show that
only SARSOP is competitive with SCPForward on larger
POMDPs. SolvePOMDP runs out of time in all but the
smallest benchmarks, and PrismPOMDP runs out of mem-
ory in all benchmarks. Most of these approaches are based
on updating a belief over the states, which for a large state
space can become extremely computationally expensive.

Second, in the benchmarks with smaller state spaces, e.g.,
Maze and Rock, SARSOP can compute policies that yield
better performance in less time. This is due to the efﬁciency
of belief-based approaches on small-size problems. On the
other hand, SARSOP does not scale to larger POMDPs with
a larger number of states and observations. For example, by
increasing the number of transitions in Intercept benchmark
from 5021 to 7041, the computation time for SARSOP in-
creases by 516%. On the other hand, the increase of the com-
putation time of SCPForward is only 28%.

Third, on the largest benchmarks, including tens of thou-
sands of states and observations, SARSOP fails to compute
any policy before time-out, while SCPForward found a
solution. Finally, we also note that SCPForward can also
compute a policy that maximizes the causal entropy and sat-
isﬁes an LTL speciﬁcation, unlike SARSOP.

Conclusion
We develop an algorithm for inverse reinforcement learning
under partial observation. We empirically demonstrate that
by incorporating task speciﬁcations into the learning pro-
cess, we can alleviate the information asymmetry between
the expert and the agent and learn policies that yield sim-
ilar performance to the expert. Further, we show that our
routine SCPForward to solve the forward problem outper-
forms state-of-the-art POMDP solvers on instances with a
large number of states and observations.

Work Limitations. This work assumes that the transi-
tion and observation functions of the POMDP are known
to the algorithm. Future work will investigate removing this
assumption and developing model-free-based approaches.
We will also integrate the framework with more expressive
neural-network-based reward functions.

References
Abbeel, P.; Coates, A.; and Ng, A. Y. 2010. Autonomous He-
licopter Aerobatics Through Apprenticeship Learning. Int.
Journal of Robotics Research, 29(13): 1608–1639.
Abbeel, P.; and Ng, A. Y. 2004. Apprenticeship Learning via
Inverse Reinforcement Learning. In Proc. of the 21st ICML.
Akash, K.; Polson, K.; Reid, T.; and Jain, N. 2019. Improv-
ing Human-Machine Collaboration Through Transparency-
based Feedback–Part I: Human Trust and Workload Model.
IFAC-PapersOnLine, 51(34): 315–321.
Amato, C.; Bernstein, D. S.; and Zilberstein, S. 2010. Opti-
mizing Fixed-Size Stochastic Controllers for POMDPs and
Decentralized POMDPs. AAMAS, 21(3): 293–320.
Bai, H.; Hsu, D.; and Lee, W. S. 2014. Integrated Percep-
tion and Planning in the Continuous Space: A POMDP Ap-
proach. Int. Journal of Robotics Research, 33: 1288–1302.
Baier, C.; and Katoen, J.-P. 2008. Principles of Model
Checking. The MIT Press.
Bogert, K.; and Doshi, P. 2014. Multi-Robot Inverse Re-
inforcement Learning under Occlusion with Interactions. In
Proc. of the Int. Conf. on Auto. Ag. and Multi. Sys., 173–180.
Boularias, A.; Kr¨omer, O.; and Peters, J. 2012. Structured
In Joint Eur. Conf. on Machine
Apprenticeship Learning.
Learning and Knowledge Discovery in Databases, 227–242.
Cassandra, A.; Littman, M. L.; and Zhang, N. L. 1997. In-
cremental Pruning: A Simple, Fast, Exact Method for Par-
In In Pro-
tially Observable Markov Decision Processes.
ceedings of the Thirteenth Conference on Uncertainty in Ar-
tiﬁcial Intelligence, 54–61. Morgan Kaufmann Publishers.
Choi, J.; and Kim, K.-E. 2011.
Inverse Reinforcement
Learning in Partially Observable Environments. Journal of
Machine Learning Research, 12: 691–730.
Dragan, A. D.; and Srinivasa, S. S. 2013. A Policy-Blending
Formalism for Shared Control. The International Journal of
Robotics Research, 32(7): 790–805.
Finn, C.; Levine, S.; and Abbeel, P. 2016. Guided Cost
Learning: Deep Inverse Optimal Control via Policy Opti-
mization. In Int. conf. on machine learning, 49–58. PMLR.
Gurobi Optimization, L. 2021. Gurobi Optimizer Reference
Manual.
Hadﬁeld-Menell, D.; Russell, S. J.; Abbeel, P.; and Dragan,
A. D. 2016. Cooperative Inverse Reinforcement Learning.
In NIPS.
Hensel, C.; Junges, S.; Katoen, J.-P.; Quatmann, T.; and
Volk, M. 2020. The Probabilistic Model Checker Storm.
arXiv preprint arXiv:2002.07080.
Ho, J.; and Ermon, S. 2016. Generative adversarial imitation
learning. In NIPS, 29: 4565–4573.
Junges, S.; Jansen, N.; and Seshia, S. A. 2020. Enforcing
Almost-Sure Reachability in POMDPs. arXiv preprint.
Junges, S.; Jansen, N.; Wimmer, R.; Quatmann, T.; Winterer,
L.; Katoen, J.-P.; and Becker, B. 2018. Finite-State Con-
trollers of POMDPs using Parameter Synthesis. In UAI.

Kitani, K. M.; Ziebart, B. D.; Bagnell, J. A.; and Hebert,
M. 2012. Activity Forecasting. In European Conference on
Computer Vision, 201–214. Springer.
Kurniawati, H.; Hsu, D.; and Lee, W. S. 2008. Sarsop: Efﬁ-
cient Point-Based POMDP Planning by Approximating Op-
timally Reachable Belief Spaces. In RSS. Citeseer.
Littman, M. L.; Topcu, U.; Fu, J.; Isbell, C.; Wen, M.; and
MacGlashan, J. 2017. Environment-Independent Task Spec-
iﬁcations via GLTL. arXiv preprint arXiv:1704.04341.
Liu, X.; and Datta, A. 2012. Modeling Context Aware Dy-
namic Trust Using Hidden Markov Model. In AAAI, 1.
Madani, O.; Hanks, S.; and Condon, A. 1999. On the Un-
decidability of Probabilistic Planning and Inﬁnite-Horizon
Partially Observable Markov Decision Problems. In AAAI.
Mao, Y.; Szmuk, M.; Xu, X.; and Ac¸ikmese, B. 2018. Suc-
cessive Convexiﬁcation: A Superlinearly Convergent Algo-
rithm for Non-convex Optimal Control Problems. arXiv.
Memarian, F.; Xu, Z.; Wu, B.; Wen, M.; and Topcu, U. 2020.
Active Task-Inference-Guided Deep Inverse Reinforcement
Learning. In 2020 59th IEEE CDC, 1932–1938.
Meuleau, N.; Kim, K.-E.; Kaelbling, L. P.; and Cassandra,
A. R. 1999. Solving POMDPs by searching the space of
ﬁnite policies. In UAI, 417–426.
Ng, A. Y.; Russell, S. J.; et al. 2000. Algorithms for Inverse
Reinforcement Learning. In Proceedings of the 17th Inter-
national Conference on Machine Learning, volume 1, 2.
Norman, G.; Parker, D.; and Zou, X. 2017. Veriﬁcation and
Control of Partially Observable Probabilistic Systems. Real-
Time Systems, 53(3): 354–402.
Ong, S. C.; Png, S. W.; Hsu, D.; and Lee, W. S. 2009.
POMDPs for Robotic Tasks with Mixed Observability. In
Robotics: Science and Systems, volume 5, 4.
Osa, T.; Pajarinen, J.; Neumann, G.; Bagnell, J.; Abbeel, P.;
and Peters, J. 2018. An Algorithmic Perspective on Imitation
Learning. Foundations and Trends in Robotics, 7: 1–179.
Papusha, I.; Wen, M.; and Topcu, U. 2018. Inverse Optimal
Control with Regular Language Speciﬁcations. In 2018 An-
nual American Control Conference (ACC), 770–777. IEEE.
Pnueli, A. 1977. The Temporal Logic of Programs. In Proc.
18th Annu. Symp. Found. Computer Sci., 46–57.
Ramachandran, D.; and Amir, E. 2007. Bayesian Inverse
Reinforcement Learning. In IJCAI, volume 7, 2586–2591.
Ratliff, N. D.; Bagnell, J. A.; and Zinkevich, M. A. 2006.
Maximum Margin Planning. In Proceedings of the 23rd In-
ternational Conference on Machine Learning, 729–736.
Walraven, E.; and Spaan, M. 2017. Accelerated Vector Prun-
ing for Optimal POMDP Solvers. In AAAI, volume 31.
Wen, M.; Papusha, I.; and Topcu, U. 2017. Learning from
Demonstrations with High-Level Side Information. In Proc.
of the 26th Int. Joint Conference on Artiﬁcial Intelligence.
Yu, H.; and Bertsekas, D. P. 2008. On Near Optimality of the
Set of Finite-State Controllers for Average Cost POMDP.
Mathematics of Operations Research, 33(1): 1–11.
Yuan, Y.-x. 2015. Recent Advances in Trust Region Algo-
rithms. Mathematical Programming, 151(1): 249–281.

Zhang, S.; Sinapov, J.; Wei, S.; and Stone, P. 2017. Robot
Behavioral Exploration and Multimodal Perception using
POMDPs. In AAAI Spring Symposium.
Zhou, Z.; Bloem, M.; and Bambos, N. 2017. Inﬁnite Time
Horizon Maximum Causal Entropy Inverse Reinforcement
Learning. IEEE Trans. Autom. Control, 63(9): 2787–2802.
Ziebart, B. D.; Bagnell, J. A.; and Dey, A. K. 2010. Mod-
eling Interaction via the Principle of Maximum Causal En-
tropy.
Ziebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K.
2008. Maximum Entropy Inverse Reinforcement Learning.
In AAAI, volume 8, 1433–1438.

Task-Guided Inverse Reinforcement Learning Under Partial Information
− Supplementary Material −

Derivations of the Constraints and Incorporating Nonlinear Reward Functions into the Learning
Algorithm

In this section, we ﬁrst recall the obtained expression of the causal entropy H γ
σ and νγ
σ .
We then prove the concavity of the causal entropy, which enables convex-optimization-based formulation of the task-guided
inverse reinforcement learning (IRL) problem. Then, we provide additional details on the derivation of the afﬁne constraint
implied by the Bellman ﬂow constraint. Finally, we show how the algorithm developed in this paper can be adapted to learn a
reward function with a nonlinear parameterization, e.g., a neural network, of the observations.

σ as a function of the visitation counts µγ

Concave Causal Entropy. We ﬁrst recall the deﬁnitions of the state and state-action visitation counts. For a policy σ, state
t=1 γt1{St=s}] and the discounted state-
s, and action α, the discounted state visitation counts are deﬁned by µγ
t=1 γt1{St=s,At=α}], where 1{·} is the indicator function and t is
action visitation counts are deﬁned by νγ
the time step. From the deﬁnitions of the state and state-action visitation counts µγ
σ , it is straightforward to deduce that
σ (s, α) = σs,αµγ
νγ

σ(s), where σs,α = P[At = a|St = s]. We use the visitation counts to prove in Section 4 that

σ (s, α) (cid:44) EAt,St[(cid:80)∞

σ(s) (cid:44) ESt[(cid:80)∞

σ and νγ

(s,α)∈S×A
σ(s). We claim in the paper that H γ
(s,α)∈S×A − log νγ
σ (s,α)
σ , µγ
σ(s) νγ
µγ
σ and ¯νγ
σ. Then, we have the following result:

σ) = (cid:80)
σ , ¯µγ

σ is a concave fucntion
σ (s, α) is concave.

H γ

σ =

(cid:88)

(s,α)∈S×A

−(log πs,α)πs,αµγ

σ(s) =

(cid:88)

− log

νγ
σ (s, α)
µγ
σ(s)

νγ
σ (s, α),

σ (s, α)/µγ
where the last inequality is obtained by using that πs,α = νγ
of the visitation counts. Thus, we want to show that the function f (νγ
σ , µγ
To this end, consider any λ ∈ (0, 1) and the two sets of variables νγ
σ + (1 − λ)¯νγ
(cid:88)

σ + (1 − λ)¯µγ
σ , λ¯µγ
σ)
σ (s, α) + (1 − λ)¯νγ
λνγ
σ(s) + (1 − λ)¯µγ
λµγ

f (λνγ

− log

σ (s, α)
σ(s, α)

=

(λνγ

σ (s, α) + (1 − λ)¯νγ

σ (s, α))

−λνγ

σ (s, α) log

λνγ
λµγ

σ (s, α)
σ(s, α)

− (1 − λ)¯νγ

σ (s, α) log

(1 − λ)¯νγ
(1 − λ)¯µγ

σ (s, α)
σ(s, α)

−λνγ

σ (s, α) log

νγ
σ (s, α)
µγ
σ(s, α)

− (1 − λ)¯νγ

σ (s, α) log

¯νγ
σ (s, α)
¯µγ
σ(s, α)

(s,α)∈S×A

(cid:88)

(s,α)∈S×A

(cid:88)

≥

=

(s,α)∈S×A
σ , µγ

= λf (νγ

σ) + (1 − λ)f (¯νγ
where the ﬁrst inequality is obtained by applying to the well-known log-sum inequality, i.e.,
x1
y1
for nonnegative numbers x1, x2, y1, y2. Speciﬁcally, we apply the substitution x1 = λνγ
y2 = (1 − λ)¯µγ

x1 + x2
y1 + y2

≥ (x1 + x2) log

+ x2 log

σ , ¯µγ

x1 log

x2
y2

σ),

,

σ. Note that the inequality
f (λνγ
σ) is concave in νγ

σ , µγ

σ + (1 − λ)¯νγ

σ , λ¯µγ
σ , and µγ
σ.

implies that f (νγ

σ + (1 − λ)¯µγ

σ) ≥ λf (νγ

σ , µγ

σ) + (1 − λ)f (¯νγ

σ , ¯µγ
σ)

σ , y1 = λµγ

σ, x2 = (1 − λ)¯νγ

σ , and

Bellman Flow Constraint. For the visitation count variables to correspond to a valid policy generating actions in the POMDP
M , νγ

σ must satisfy the bellman ﬂow constraint given by

σ and µγ

σ(s) = ESσ
µγ

t

(cid:104) ∞
(cid:88)

t=0

(cid:105)

γt1{Sσ

t =s}

= µ0(s) + γESσ

t

(cid:104) ∞
(cid:88)

t=0

γt1{Sσ

t+1=s}

(cid:105)

= µ0(s) + γ

∞
(cid:88)

(cid:88)

(cid:88)

γtP(s|s(cid:48), α)P[Sσ

t = s(cid:48), Aσ

t = α]

= µ0(s) + γ

t=0
(cid:88)

s(cid:48)∈S
(cid:88)

α∈A
P(s|s(cid:48), α)νγ

σ (s(cid:48), α).

s(cid:48)∈S

α∈A

Nonlinear Reward Parameterization. Consider the parameterization of the reward function as R(s, α) (cid:44) hθ(s, α), where
θ ∈ Rd is an unknown parameter vector and h is a nonlinear function of θ. Then, at each iteration of Algorithm 2 in line 3,
the gradient step can be computed ∇hθ(θk) or the corresponding the subgradient in the case hθ is not differentiable. We recall
σk − ˆRφ. Furthermore, the reader can straight-
that for a linear parameterization, the gradient is obtained by ∇f (θk) = Rφ
forwardly observe that SCPForward remains practically unchanged as, given θk, we only need to replace the linear term
(cid:80)
(s,α)∈S×A(θk)Thθk (s, α)νγ
(s,α)∈S×A(θk)Tφ(s, α)νγ
σ (s, α), which is also linear in the
state-action visitation count νγ
σ . Thus, the only challenge in handling nonlinear parameterization is computing the correspond-
ing gradient, which can be done by standard platforms such as TensorFlow and Torch. Note that nonlinear parameterization
enables to learn more expressive reward functions that are typically represented by neural networks.

σ (s, α) in the cost function by (cid:80)

Experimental Tasks
In this section, we ﬁrst provide a detailed description of the POMDP models used in the benchmark. The simulations on the
benchmark examples empirically demonstrate that side information alleviates the information asymmetry, and more memory
leads to more performant policies. Then, we provide additional numerical simulations supporting the claim that SCPForward
is sound and yields better scalability than off-the-shelf solvers for the forward problem, i.e., computing an optimal policy on a
POMDP for a given reward function.

Computation Resources and External Assets
All the experiments of this paper were performed on a computer with an Intel Core i9-9900 CPU 3.1GHz ×16 processors and
31.2 Gb of RAM. All the implementations are written and tested in Python 3.8, and we attach the code with the supplementary
material.

Required Tools. Our implementation requires the Python interface Stormpy of Storm (Hensel et al. 2020) and Gurobipy
of Gurobi 9.1 (Gurobi Optimization 2021). On one hand, we use Storm, a tool for model checking, to parse POMDP ﬁle
speciﬁcations, to compute the product POMDP with the ﬁnite state controller in order to reduce the synthesis problem to the
synthesis of memoryless policies, and to compute the set T of target states satisfying a speciﬁcation ϕ via graph preprocessing.
On the other hand, we use Gurobi to solve both the linearized problem in (7) and the feasible solution of the Bellman ﬂow
constraint needed for the veriﬁcation step.

In order to show the scalability of the developed algorithm SCPForward,
Off-The-Shelf Solvers for Forward Problem.
we compare it to state-of-the-art POMDP solvers SolvePOMDP (Walraven and Spaan 2017), SARSOP (Kurniawati, Hsu,
and Lee 2008), and PRISM-POMDP (Norman, Parker, and Zou 2017). The solver SolvePOMDP implements both exact and
approximate value iterations via incremental pruning (Cassandra, Littman, and Zhang 1997) combined with state-of-the-art
vector pruning methods (Walraven and Spaan 2017). Finally, PrismPOMDP discretizes the belief state and adopts a ﬁnite
memory strategy to ﬁnd an approximate solution of the forward problem. For all the solvers above, we use the default settings
except from the timeout enforced to be 3600 seconds. These solvers are not provided with our implementation. However, we
provide the POMDP models that each of the solvers can straightforwardly use. Further details are provided in the readme ﬁles
of our implementation.

Benchmark Set
We evaluate the proposed learning algorithm on several POMDP instances adapted from (Junges, Jansen, and Seshia 2020).
We attached the modiﬁed instances in our code with the automatically generated models for each off-the-shelf solver that the
reader can straightforwardly use to reproduce Table 2. The reader can refer to Table 2 for the number of states, observations,
and transitions of each environment of the benchmark set. In all the examples, we gather 10 trajectories from an expert that can
fully observe its current state in the environment and an expert having partial observation of the environment. Our algorithm
learns reward functions from these trajectories under different memory policies and high-level side information.

In the environment Rocks, an agent navigates in a gridworld to sample at least one valuable rock (if a
Rocks Instance.
valuable rock is in the grid) over the two possibly dangerous rocks, without any failures. When at least one valuable rock has
been collected, or the agent realizes that all the rocks are dangerous, it needs to get to an exit point to terminate the mission. The
partial observability is due to the agent can only observe if its current location is an exit point or a dangerous rock. Furthermore,
the agent has noisy sensors enabling sampling neighbor cells.

We consider three feature functions. The ﬁrst feature provides a positive reward when reaching the exit point with at least
one valuable rock or no rocks when all of them are dangerous. The second feature provides a negative reward when the agent
is at the location of a dangerous rock. Finally, the third feature penalizes each action taken with a negative reward to promote
reaching the exit point as soon as possible.

No information asymmetry

Under information asymmetry

Finite-memory policy

75

150

225

300

Rφ
σ

Rφ
0
σ

100

50

0

100

50

0

Without side
information

With side
information

Memoryless policy

75

150

225

300

Rφ
σ

Rφ
0
σ

100

50

0

100

50

0

0

75

150

225

300

0

75

150

225

300

Figure 5: Representative results on the Rock example showing the reward of the policies under the true reward function (Rφ
σ)
versus the time steps.

Time Steps

Time Steps

We compare scenarios with no side information and the a priori knowledge on the task such as the agent eventually reaches
an exit point with a probability greater than 0.995. Figure 5 supports our claim that side information indeed alleviates the
information asymmetry between the expert and the agent. Additionally, we also observe that incorporating memory leads to
more performant policies in terms of the mean accumulated reward.

In the environment Obstacle[n], an agent must ﬁnd an exit in a gridworld without colliding with any of
Obstacle Instance.
the ﬁve static obstacles in the grid. The agent only observes whether the current position is an obstacle or an exit state. The
parameter n speciﬁes the dimension of the grid.

Similar to the Rocks example, the agent receives a positive reward if it successfully exits the gridworld and a negative reward

for every taken action or colliding with an obstacle.

As for the side information, we specify in temporal logic that while learning the reward, the agent should not collide any

obstacles until it reaches an exit point with a probability greater than 0.9.

No information asymmetry

Under information asymmetry

Finite-memory policy

25

50

75

100

Without side
information

With side
information

Rφ
σ

Rφ
0
σ

400

200

0

−200

400

200

0

−200

Memoryless policy

25

50

75

100

Rφ
σ

Rφ
0
σ

400

200

0

−200

400

200

0

−200

0

25

50

75

100

0

25

50

75

100

Figure 6: Representative results on the Obstacle example showing the reward of the policies under the true reward function
(Rφ

σ) versus the time steps.

Time Steps

Time Steps

Similar to the Maze and Rock examples, Figure 6 supports our claim that side information alleviates the information asym-

metry and memory leads to more performant policies.

Evade Instance. Evade[n, r, slip] is a turn-based game where the agent must reach a destination without being intercepted
by a faster player. The player cannot access the top row of the grid. Further, the agent can only observe the player if it is within
a ﬁxed radius from its current location and upon calling the action scan. The parameters n, r, and slip specify the dimension
of the grid, the view radius, and the slippery probability, respectively.

The feature functions are deﬁned such that the agent receives a positive reward if at the destination, a high negative reward if

it is intercept by the player, and a small negative reward for each action taken, including the scan action.

With side information

Without side information

20

10

0

d
r
a
w
e
r

d
e
t
a
l
u
m
u
c
c
a
n
a
e

M

−10

0

25

50

Time Steps

75

100

Figure 7: Representative results on the Evade example showing the reward of the policies under the true reward function (Rφ
σ)
versus the time steps.

As for the side information, we specify in temporal logic that while learning the reward, the agent must reach an exit point

with probability greater than 0.98.

Figure 7 shows that learning with side information provides higher reward than without side information. Besides, there is
less randomness in the policy with side information compared to the policy without side information. Speciﬁcally, the standard
deviation of the policy with side information is signiﬁcantly smaller than the policy without side information.

We did not discuss the impact of different memory size policies in this example since the performance of the memoryless
policy is already near-optimal, as the policy obtains the same reward as SARSOP (see Table 2 for a reference. Speciﬁcally, we
observe that the optimal policy on the underlying MDP yields comparable policies to the optimal memoryless policy on the
POMDP. As a consequence, we observe that the information asymmetry between the expert and the agent does not hold here
either, and the learned policies obtain a similar performance.

Intercept[n, r, slip] is a variant of Evade where the agent must intercept another player who is trying
Intercept Instance.
to exit the gridworld. The agent can move in 8 directions and can only observe the player if it is within a ﬁxed radius from the
agent’s current position when the action scan is performed. Besides, the agent has a camera that enables it to observe all cells
from west to east from the center of the gridworld. In contrast, the player can only move in 4 directions. The parameters n, r,
and slip specify the dimension of the grid, the view radius, and the slippery probability, respectively.

We consider three feature functions to parameterize the unknown reward. The ﬁrst feature provides a positive reward to the
agent upon intercepting the player. The second feature penalizes the agent if the player exits the gridworld. The third feature
imposes a penalty cost for each action taken.

With side information

Without side information

20

10

0

d
r
a
w
e
r

d
e
t
a
l
u
m
u
c
c
a

n
a
e

M

−10

0

25

50

Time Steps

75

100

Figure 8: Representative results on the Intercept example showing the reward of the policies under the true reward function
(Rφ

σ) versus the time steps.

We encode the high-level side information as the temporal logic task speciﬁcation Eventually intercept the player with
probability greater than 0.98, i.e., the agent should eventually reach an observation where its location coincides with the
player’s location.

Figure 8 demonstrates that side information does not improve the performance of the policy. This result is because memo-
ryless policies are optimal in this example, and a combination of the given reward features can perfectly encode the temporal
logic speciﬁcations, similar to the Evade example.

Avoid Instance. Avoid[n, r, slip] is a variant of Evade, where the agent must reach an exit point while avoiding being
detected by two other moving players following certain predeﬁned yet unknown routes. The agent can only observe the players
if they are within a ﬁxed radius from the agent’s current position when the action scan is performed. Besides, with the players’
speed being uncertain, their position in the routes can not be inferred by the agent. The parameters n, r, and slip specify the
dimension of the grid, the view radius, and the slippery probability, respectively.

We consider four feature functions to parameterize the unknown reward. The ﬁrst feature provides a positive reward to the
agent upon reaching the exit point. The second feature penalizes the agent if it collides with a player. The third feature penalizes
the agent if it is detected by a player. The fourth feature imposes a penalty cost for each action taken.

With side information

Without side information

d
r
a
w
e
r

d
e
t
a
l
u
m
u
c
c
a

n
a
e

M

40

20

0

−20

0

75

150
Time Steps

225

300

Figure 9: Representative results on the Avoid example showing the reward of the policies under the true reward function (Rφ
σ)
versus the time steps.

We encode the high-level side information as the temporal logic task speciﬁcation avoid being detected until reaching the

exit point with probability greater than 0.98.

Figure 9 shows that the algorithm is unable to learn without side information while side information induces a learned policy
that is optimal. Speciﬁcally, the learned policy without side information seems to only focus on avoiding being detected and
collision as the corresponding learned features were close to zero. Thus, the agent using that policy does not move until sensing
a player is closed by. As a consequence it gets always a negative reward from taking actions at each time step.

Summary of the Results
Side Information Alleviates the Information Asymmetry. As mentioned in the submitted manuscript, side information can
indeed alleviate the information asymmetry. Speciﬁcally, we observe that if there is an information asymmetry in the forward
problem, i.e., the obtained reward from an optimal policy on the underlying POMDP is lower than from an optimal policy on
the underlying fully observable MDP, incorporating side information in temporal logic speciﬁcations alleviates the information
asymmetry between the expert and the agent. For example, we can see the effects of such information asymmetry in the Maze,
Rocks, Obstacle, and Avoid examples. In these examples, having partial observability reduces the obtained reward in the
forward problem. The policies that do not incorporate side information into the learning procedure also obtain a lower reward
under information asymmetry.

Memory Leads to More Performance Policies. Similarly to the side information, we also observe that if incorporating
memory improves the performance of the learned policies, if it also improves the obtained reward in the forward problem, as
seen in the Maze, Rocks, and Obstacle instances. In Table 2, we can also see that incorporating memory helps to compute a
better optimal policy in these examples, unlike computing a memoryless policy.

SCPForward Yields Better Scalability.
We support our claim that SCPForward yields better scalability through additional simulations on the benchmark examples by
varying the size of the considered examples. Speciﬁcally, we demonstrate that on large-size problems, only SARSOP attempts to
solve the problems, and when it succeeds, its computation time is at least two orders of magnitude slower than SCPForward.

Problem

|S|

|S × O|

Maze
Maze (3-FSC)
Maze (10-FSC)
Obstacle[10]
Obstacle[10](5-FSC)
Obstacle[25]
Rock
Rock (3-FSC)
Rock (5-FSC)
Intercept[5, 2, 0]
Intercept[5, 2, 0.1]
Evade[5, 2, 0]
Evade[5, 2, 0.1]
Evade[10, 2, 0]
Avoid[4, 2, 0]
Avoid[4, 2, 0.1]
Avoid[7, 2, 0]

17
49
161
102
679
627
550
1648
2746
1321
1321
2081
2081
36361
2241
2241
19797

162
777
2891
1126
7545
7306
4643
23203
41759
5021
7041
13561
16761
341121
5697
8833
62133

|O|

SCPForward
Rφ
Time (s)
σ
39.24
11
31
44.98
46.32
101
5
19.71
19.77
31
19.59
5
19.68
67
199
19.8
19.82
331
19.83
1025
1025
19.81
97.3
1089
1089
96.79
18383 94.97
9.86
1956
9.86
1956
9.72
3164

0.1
0.6
2.04
8.79
38
14.22
12.2
15.25
97.84
10.28
13.18
26.25
26.25
3600
34.74
14.63
3503

SARSOP

Rφ
σ
47.83
47.83
47.83
19.8
19.8
19.8
19.83
19.83
19.83
19.83
19.81
97.3
95.28
−
9.86
9.86
−

Time (s)

0.24
0.24
0.24
0.02
0.02
0.1
0.05
0.05
0.05
13.71
81.19
3600
3600
−
9.19
210.47
−

Time (s)

SolvePOMDP
Rφ
σ
47.83
47.83
47.83
5.05
5.05
5.05
−
−
−
−
−
−
−
−
−
−
−

0.33
0.33
0.33
3600
3600
3600
−
−
−
−
−
−
−
−
−
−
−

Table 2: Results for the benchmarks. On larger benchmarks (e.g., Evade and Avoid), only SCPForward can compute a locally
optimal policy. We set the time-out to 3600 seconds. An empty cell (denoted by −) represents the solver failed to compute any
policy before the time-out.

We see that in Table 2, incorporating memory into policy on the Maze, Rock, and Obstacle benchmarks, allows
SCPForward to compute policies that are as optimal as SARSOP, which uses belief update rules. We recall that the com-
parisons with PrismPOMDP were not reported in this table due to either having negative values in the reward function, which
is not allowed by PrismPOMDP, or the solver runs out of memory before computing any policy.

