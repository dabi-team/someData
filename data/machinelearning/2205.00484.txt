2
2
0
2

y
a
M
1

]
L
C
.
s
c
[

1
v
4
8
4
0
0
.
5
0
2
2
:
v
i
X
r
a

Dynamic Programming in Rank Space: Scaling Structured Inference with
Low-Rank HMMs and PCFGs

Songlin Yang∗, Wei Liu*, Kewei Tu†
School of Information Science and Technology, ShanghaiTech University
Shanghai Engineering Research Center of Intelligent Vision and Imaging
{yangsl,liuwei4,tukw}@shanghaitech.edu.cn

Abstract

Hidden Markov Models (HMMs) and Proba-
bilistic Context-Free Grammars (PCFGs) are
widely used structured models, both of which
can be represented as factor graph grammars
(FGGs), a powerful formalism capable of de-
scribing a wide range of models. Recent re-
search found it beneﬁcial to use large state
spaces for HMMs and PCFGs. However, in-
ference with large state spaces is computation-
ally demanding, especially for PCFGs. To
tackle this challenge, we leverage tensor rank
decomposition (aka. CPD) to decrease infer-
ence computational complexities for a sub-
set of FGGs subsuming HMMs and PCFGs.
We apply CPD on the factors of an FGG
and then construct a new FGG deﬁned in the
Inference with the new FGG
rank space.
produces the same result but has a lower
time complexity when the rank size is smaller
than the state size. We conduct experiments
on HMM language modeling and unsuper-
vised PCFG parsing, showing better perfor-
mance than previous work. Our code is pub-
licly available at https://github.com/
VPeterV/RankSpace-Models.

1

Introduction

Hidden Markov Models (HMMs) and Probabilistic
Context-Free Grammars (PCFGs) are widely used
structured models in natural language processing.
They can both be represented as factor graph gram-
mars (FGGs) (Chiang and Riley, 2020), which are
a powerful tool to describe a wide range of mod-
els, allowing exact and tractable inference in most
situations of interest.

Over-parameterization has been shown beneﬁ-
cial in facilitating optimization of deep networks
(Arora et al., 2018; Xu et al., 2018; Du et al.,
2019). Buhai et al. (2020) found that over-
parameterization is also helpful in learning latent

∗ Equal contributions: Songlin Yang formulated the idea
and Wei Liu conducted the experiments. † Corresponding
author.

variable models by increasing the number of hidden
states. Buhai et al. (2020) argued that it is impor-
tant to study over-parameterization in structured
settings because structured latent variable models
are more suitable to model real-word phenomena
which exhibit complex dependencies. HMMs and
PCFGs are typical structured latent variable mod-
els, and recently researchers have found it beneﬁ-
cial to use large state spaces for HMMs and PCFGs
(Dedieu et al., 2019; Chiu and Rush, 2020; Yang
et al., 2021b; Chiu et al., 2021). However, struc-
tured inference with large state spaces is computa-
tionally demanding, especially for PCFGs, push-
ing researchers to develop methods to decrease the
computational complexities. Chiu and Rush (2020)
propose a neural VL-HMM with 215 states for lan-
guage modeling, narrowing down the performance
gap between HMMs and LSTMs. They follow
Dedieu et al. (2019) to impose a strong sparsity
constraint (i.e., each hidden state can only generate
a small subset of terminal symbols) to decrease
the time complexity of the forward algorithm, thus
requiring pre-clustering of terminal symbols. Yang
et al. (2021b) use a large state space for neural
PCFG induction and achieve superior unsupervised
constituency parsing performance. They follow
Cohen et al. (2013) to use tensor rank decomposi-
tion (aka. canonical-polyadic decomposition (CPD)
(Rabanser et al., 2017)) to decrease the computa-
tional complexity of the inside algorithm, but only
scale the state size from tens to hundreds because
the resulting complexity is still high. Chiu et al.
(2021) use tensor matricization and low-rank ma-
trix decomposition to accelerate structured infer-
ence on chain and tree structure models. However,
their method has an even higher complexity than
Yang et al. (2021b) on PCFGs. Recently, Fu and
Lapata (2021) propose a family of randomized dy-
namic programming algorithms to scale structured
models to tens of thousands of states, which is
orthogonal to the aforementioned low-rank-based

 
 
 
 
 
 
approaches as the former performs approximate in-
ference whereas the latter perform exact inference.
In this work, we propose a new low-rank-based
approach to scale structured inference, which can
be described by FGG notations intuitively. We ﬁrst
provide an intuitive and unifying perspective to-
ward the work of Yang et al. (2021b) and Chiu et al.
(2021), showing that their low-rank decomposition-
based models can be viewed as decomposing large
factors in an FGG—e.g., the binary rule probabil-
ity tensor in PCFGs— into several smaller factors
connected by new “rank” nodes. Then we target at
a subset of FGGs—which we refer to as B-FGGs—
subsuming all models considered by Chiu et al.
(2021), whereby the inference algorithms can be
formulated via B-graphs (Gallo et al., 1993; Klein
and Manning, 2001). We propose a novel frame-
work to support a family of inference algorithms in
the rank space for B-FGGs. Within the framework,
we apply CPD on the factors of a B-FGG and then
construct a new B-FGG deﬁned in the rank space
by marginalizing all the state nodes. Inference with
the new B-FGG has the same result and a lower
time complexity if the rank size is smaller than the
state size.

We conduct experiments in unsupervised PCFG
parsing and HMM language modeling. For PCFG
induction, we manage to use 20 times more hidden
states than Yang et al. (2021b), obtaining much bet-
ter unsupervised parsing performance. For HMM
language modeling, we achieve lower perplexity
and lower inference complexity than Chiu et al.
(2021).

2 Background

2.1 Factor graph grammar

Factor graphs are ﬁxed-sized and thus incapable
of modeling substructures that repeat a variable
number of times. Chiang and Riley (2020) pro-
pose factor graph grammars (FGGs) to overcome
this limitation, which are expressive enough to sub-
sume HMMs and PCFGs. The main purpose of
introducing FGGs in this work is to facilitate more
intuitive presentation of our method, and to enable
generalization beyond HMMs and PCFGs.

2.1.1 Basics
We display necessary notations and concepts of
FGGs (Chiang and Riley, 2020, Def. 1,2,5,6,8).

• V and E are ﬁnite set of nodes and hyper-

edges.

• att : E → V (cid:63) maps each hyperedge to zero
or more (not necessarily distinct) endpoint
nodes.

• labV : V → LV assigns labels to nodes.
• labE : E → LE assigns labels to edges.

Deﬁnition 2. A factor graph is a hypergraph with
mappings Ω and F where

• Ω maps node labels to sets of possible values.

Ω(v) (cid:44) Ω(labV (v)).

• F maps edge labels to functions. F (e) (cid:44)
F (labE(e)) is of type Ω(v1) × · · · × Ω(vk)
where att(e) = v1 · · · vk.

In the terminology of factor graphs, a node v with
its domain Ω(v) is a variable, and an hyperedge e
with F (e) is a factor. We typically use T, N, O to
denote hidden state, nonterminal state and observa-
tion variables for HMMs and PCFGs.

Deﬁnition 3. A hypergraph fragment is a tuple
(V, E, att, labV , labE, ext) where

• (V, E, att, labV , labE) is a hypergraph.
• ext ∈ V (cid:63) is a set of zero or more external
nodes and each of which can be seen as a
connecting point of this hypergraph fragment
with another fragment.

Deﬁnition 4. A hyperedge replacement graph
grammar (HRG) (Drewes et al., 1997) is a tuple
(N, T, P, S) where

• N, T ⊂ LE is ﬁnite set of nonterminal and

terminal symbols. N ∩ T = ∅.

• P is a ﬁnite set of rules (X → R) where
X ∈ N and R is a hypergraph fragment with
edge labels in N ∪ T 1.
• S ∈ N is the start symbol.

Deﬁnition 5. A HRG with mapping Ω, F (Def.
2) is referred to as an FGG. In particular, F is
deﬁned on terminal edge labels T only.

Notations.

• N : variable node. N : external node.
•

Xe : hyperedge e with label X ∈ N .

indicates zero or more endpoint nodes.

Deﬁnition 1. A hypergraph
(cid:0)V, E, att, labV , labE(cid:1) where

is

a

tuple

1Note that, for the lhs of P , Chiang and Riley (2020) also
draw their endpoint nodes using external node notations. We
follow this practice.

T1 = bos

π1−→

S

T1

X2, (0)

T1

X, (i − 1)

π2−→ T1

p(T2 | T1)

p(O3 | T2)

T2

X4, (i)

O3

O3 = wi−1

T1

X, (n)

π3−→ T1

p(T2 | T1)

T2 = eos

T2

(a)

N1

S(cid:48)

π4−→ N1

X, (l − 1, l)

π5−→

X2, (0, n)

N1

O2

p(N1 → O2)

O2 = wl−1

N1

N1

X, (i, j)

π6−→

p(N1 → N2N3)

N2

N3

X4, (i, k)

X5, (k, j)

(b)

Figure 1: FGG representations of (a) HMMs and (b) PCFGs. Examples come from Chiang and Riley (2020).

• F (e)

: factor F (e).

Fig. 1 illustrates HGG representations of HMM

and PCFG.

Generative story. An FGG starts with S , re-
Xe and uses rule X → R
peatedly selects
Xe ex-

from P to replace e with R, until no
ists.

2.1.2 Conjunction
The conjunction operation (Chiang and Riley, 2020,
Sec. 4) allows modularizing an FGG into two parts,
one deﬁning the model and the other deﬁning a
query. In this paper, we only consider querying the
observed sentence w0, · · · , wn−1, which is exem-
pliﬁed by the red part of Fig. 1. We sometimes
omit the red part without further elaboration.

Inference

2.1.3
Denote ξ as an assignment of all variables, ΞD as
the set of all assignments of factor graph D, and
D(G) as the set of all derivations of an FGG G,
i.e., all factor graphs generated by G. an FGG G
assigns a score wG(D, ξ) to each D ∈ D(G) along
with each ξ ∈ ΞD. A factor graph D ∈ D(G)
assigns a score wD(ξ) to each ξ ∈ ΞD:

wD(ξ) =

(cid:89)

e∈D

F (e)(ξ(v1), . . . , ξ(vk))

(1)

with att(e) = v1 · · · vk. Notably, wD(ξ) (cid:44)
wG(D, ξ). The inference problem is to compute
the sum-product of G:

ξ∈ΞD

To obtain ZG, the key difﬁculty is in the marginal-
ization over all derivations, since (cid:80)
wD(ξ)
can be obtained by running standard variable elim-
ination (VE) on factor graph D. To tackle this,
Chiang and Riley (2020, Thm. 15) propose an ex-
2, deﬁne P X
tended VE. For each X ∈ N, ξ ∈ ΞX
as all rules in P with left-hand side X, and then
deﬁne:

ψX (ξ) =

(cid:88)

τR(ξ).

(3)

(X→R)∈P X

each

rhs R

(V, EN ∪
for
ET , att, labV , labE, ext), where EN , ET consist
of nonterminal/terminal-labeled edges only, and
τR(ξ) is given by:

=

τR(ξ) =

(cid:88)

(cid:89)

F (e) (cid:0)ξ(cid:48)(att(e))(cid:1)

ξ(cid:48)∈ΞR
ξ(cid:48)(ext)=ξ

e∈ET

(cid:89)

e∈EN

ψlabE (e)

(cid:0)ξ(cid:48)(att(e))(cid:1)

(4)

This deﬁnes a recursive formula for computing
ψS, i.e., ZG. Next, we will show how Eq. 3-4
recover the well-known inside algorithm.

Example: the inside algorithm. Consider π6 in
Fig.1(b). All possible fragments R (rhs of π6) dif-
fers in the value of k, i.e., the splitting point, so we
use Rk to distinguish them. Then Eq. 3 becomes:

ψXi,k (ξ) =

(cid:88)

i<k<j

τRk (ξ)

(5)

(cid:88)

(cid:88)

ZG =

wG(D, ξ)

(2)

D∈D(G)

ξ∈ΞD

2ΞX is deﬁned as the set of assignments to the endpoints
of an edge e labeled X, so ΞX = Ω ((cid:96)1) × · · · × Ω ((cid:96)k) where
att(e) = v1 · · · vk, labV (vi) = (cid:96)i.

Putting values into Eq. 4:

τRk (ξ) =

(cid:88)

n2,n3

p(ξ, n2, n3)ψXi,k (n2)ψXk,j (n3)

(6)
where p denotes FGG rule probability p(N1 →
N2N3). It is easy to see that ψXi,k is exactly the
inside score of span [i, k), and Eq. 5-6 recovers the
recursive formula of the inside algorithm.

Remark. Eq. 4 can be viewed as unidirectional
(from e ∈ EN to external nodes) belief propaga-
tion (BP) in the factor graph fragment R, where the
incoming message is ψlabE (e) for e ∈ EN , and the
outcome of Eq. 4 can be viewed as the message
passed to the external nodes. The time complexity
of message updates grows exponentially with the
number of variables in the factors. Therefore, to de-
crease inference complexity, one may decompose
large factors into smaller factors connected by new
nodes, as shown in the next subsection.

v1

F (e) →

v2

v..

vk

v1

R

F (e1)

F (e2)

F (ek)

v2

v..

vk

Figure 2: Using CPD to decompose a factor can be seen
as adding a new node.

N1

U

R

V
N2

W
N3

N1

R

U

V(cid:48)

N2

N3

X4, (i, k)

X5, (k, j)

X4, (i, k)

X5, (k, j)

(a)

(b)

Figure 3: Representations of the rhs of π6 (Fig. 1) af-
ter decomposition. (a): TD-PCFG (Cohen et al., 2013;
Yang et al., 2021b). (b): LPCFG (Chiu et al., 2021).

2.2 Tensor rank decomposition on factors

3 Low-rank structured inference

Consider a factor F (e) (Def. 2), it can be repre-
sented as an order-k tensor in RN1×···×Nk where
Ni (cid:44) |Ω(vi)|. We can use tensor rank decom-
position (aka. CPD) to decompose F (e) into a
weighted sum of outer products of vectors:

F (e) =

r
(cid:88)

q=1

λqwq

e1 ⊗ wq

e2 ⊗ · · · ⊗ wq
ek

where r is the rank size; wq
ek ∈ RNk ; ⊗ is outer
product; λq is weight, which can be absorbed into
{wq

ek } and we omit it throughout the paper.
Dupty and Lee (2020, Sec. 4.1) show that BP
can be written in the following matrix form when
applying CPD on factors:

In this section, we recover the accelerated inside
algorithms of TD-PCFG (Cohen et al., 2013; Yang
et al., 2021b) and LPCFG (Chiu et al., 2021) in
an intuitive and unifying manner using the FGG
notations. The accelerated forward algorithm of
LHMM (Chiu et al., 2021) can be derived similarly.
Denote T ∈ Rm×m×m as the tensor represen-
tation of p(N1 → N2N3) , and αi,j ∈ Rm
as the inside score of span [i, j). Cohen et al.
(2013) and Yang et al. (2021b) use CPD to decom-
pose T, i.e., let T = (cid:80)r
q=1 uq ⊗ vq ⊗ wq where
uq, vq, wq ∈ Rm. Denote U, V, W ∈ Rr×m as
the resulting matrices of stacking all uq, vq, wq,
Cohen et al. (2013) derived the recursive form:

(cid:0)(cid:12)j∈N (e)\iWej nje

mei = WT
ei
nie = (cid:12)c∈N (i)\emci

(cid:1)

(7)

(8)

αi,j =

j−1
(cid:88)

k=i+1

UT ((Vαi,k) (cid:12) (Wαk,j))

(9)

ej , · · · , wr

where mei ∈ RNi
is factor-to-node message;
nie ∈ RNi is node-to-factor message; N (·) indi-
ej ]T ∈
cates neighborhood ; Wej = [w1
Rr×m; (cid:12) is element-wise product. We remark that
this amounts to replacing the large factor F (e) with
smaller factors {F (ei)} connected by a new node
R that represents rank, where each F (ei) can be
represented as Wei. Fig. 2 illustrates this intuition.
We refer to R as rank nodes and others as state
nodes thereafter.

= UT

j−1
(cid:88)

k=i+1

((Vαi,k) (cid:12) (Wαk,j))

(10)

Eq. 9 can be derived automatically by combining
Eq. 7 (or Fig. 3 (a)) and Eq. 5-6. Cohen et al.
(2013) note that UT can be extracted to the front
of the summation (Eq. 10), and Vαi,k, Wαk,j can
be cached and reused, leading to further complex-
ity reduction. The resulting inside algorithm time
complexity is O(n3r + n2mr).

Recently, Chiu et al. (2021) use low-rank matrix
decomposition to accelerate PCFG inference. They
ﬁrst perform tensor matricization to ﬂatten T to
T(cid:48) ∈ Rm×m2
, and then let T(cid:48) = UT V where
U ∈ Rr×m, V ∈ Rr×m2
. By un-ﬂattening V to
V(cid:48) ∈ Rr×m×m, their accelerated inside algorithm
has the following recursive form:

αi,j =

j−1
(cid:88)

k=i+1

UT (cid:0)V(cid:48) · αk,j · αi,k

(cid:1)

(11)

= UT

j−1
(cid:88)

k=i+1

(cid:0)V(cid:48) · αk,j · αi,k

(cid:1)

(12)

Eq. 11 can be derived by combining Fig. 3 (b) and
Eq. 5-6. The resulting inside time complexity is
O(n3m2r + n2mr), which is higher than that of
TD-PCFG.

When learning a PCFG and a HMM, there is no
need to ﬁrst learn T and then perform decomposi-
tion on T. Instead, one can learn the decomposed
matrices (e.g., U, V) to learn T implicitly. During
inference, one can follow Eq. 10 or 12 without the
need to reconstruct T.

Validity of probability. The remaining problem
is to ensure that T is a valid probability tensor (i.e.,
being nonnegative and properly normalized) when
learning it implicitly. Yang et al. (2021b) essen-
tially transform Fig. 3(a) into a Bayesian network,
adding directed arrows N1 → R, R → N2, R →
N3. This is equivalent to requiring that V, W are
nonnegative and column-wise normalized and U
is nonnegative and row-wise normalized, as de-
scribed in Yang et al. (2021b, Thm. 1). One can
apply the Softmax re-parameterization to enforce
such requirement, which is more convenient in end-
to-end learning. Chiu et al. (2021) replace the local
normalization of Yang et al. (2021b) with global
normalization, and we refer readers to their paper
for more details. We adopt the strategy of Yang
et al. (2021b) in this work.

4 Rank-space modeling and inference

4.1 Rank-space inference with B-FGGs

Interestingly, when applying CPD on factors and if
the rank size is smaller than the state size, we can
even obtain better inference time complexities for
a subset of FGGs which we refer to as B-FGGs.

We call a hyperedge a B-edge if its head contains
exactly one node. B-graphs (Gallo et al., 1993) are
a subset of directed hypergraphs whose hyperedges

are all B-edges. Many dynamic programming algo-
rithms can be formulated through B-graphs (Klein
and Manning, 2001; Huang, 2008; Azuma et al.,
2017; Chiu et al., 2021; Fu and Lapata, 2021), in-
cluding the inference algorithms of many struc-
tured models, e.g., HMMs, Hidden Semi-Markov
Models (HSMMs), and PCFGs. We follow the
concept of B-graphs to deﬁne B-FGGs.

Deﬁnition 6. A hypergraph fragment is a B-
hypergraph fragment iff. there is exactly one ex-
ternal node and there is no nonterminal-labeled
hyperedge connecting to it. An FGG is a B-FGG
iff. all rhs of its rules are B-hypergraph fragments.

It is easy to see that the aforementioned models
are subsumed by B-FGGs. We can design a fam-
ily of accelerated inference algorithms for B-FGGs
based on the following strategy. (1) If there are mul-
tiple factors within a hypergraph fragment, merge
them into a single factor. Then apply CPD on the
single factor, thereby introducing rank nodes. (2)
Find repeated substructures that take rank nodes
as external nodes. Marginalize all state nodes to
derive new rules. (3) Design new inference algo-
rithms that can be carried out in the rank space
based on the general-purpose FGG inference al-
gorithm and the derived new rules. We give two
examples, the rank-space inside algorithm and the
rank-space forward algorithm, in the following two
subsections to help readers understand this strategy.

4.2 The rank-space inside algorithm

Consider an B-FGG G shown in Fig.1(b) and re-
place the rhs of π6 with Fig. 3(a), i.e., we use
CPD to decompose binary rule probability tensor.
Besides U, V, W ∈ Rr×m deﬁned in Sec. 3, we
deﬁne the start rule probability vector as s ∈ Rm×1,
and the unary rule probability matrix as E ∈ Ro×m
where o is the vocabulary size.

Fig. 4(a) is an example (partial) factor graph D
generated by G. We highlight substructures of inter-
est with dashed rectangles. Each substructure con-
sists of a node N and two factors connecting to it.
N is an external node connecting two hypergraph
fragments which contain the two factors respec-
tively. For each substructure, we can marginalize
the state node N out, merging the two factors into a
single one. After marginalizing all state nodes, we
obtain a (partial) factor graph D(cid:48) shown in the right
of Fig. 4(a) where H = VUT , I = WUT , J =
VET , K = WET , L = (Us)T . We denote this
transformation as M(D) = D(cid:48). It is worth men-

L

R1

H

R2

I

R3

K

J

O1 O2

H

R4

...

I

R5

...

N1

R1

N2

R2

N3

R3

N4

R4

...

N5

N6

N7

O1 O2

R5

...

(a)

R1

R1

H

R2

I

R3

H

R2

K

O3

X4, (i, k)

X5, (k, j)

X4, (i, j − 1) O3 = wj−1

π1

π3

R1

π2

X, (i, j)

π4

R1

R1

J

O2

I

R3

J

O2

K

O3

L

R1

X2, (0, n)

π5

π6

S

O1

O2 = wi

X5, (i + 1, j)

O2 = wi

O3 = wi+1

O1 = w1

(b)

Figure 4: (a): illustration of marginalizing state nodes N. (b): rule set of the new FGG. π1 can be applied when
k (cid:54)= i + 1 and k + 1 (cid:54)= j; π2 and π3 can be applied when i (cid:54)= j − 1; π4 can be applied when j = i + 2.

tioning that H, I, J, K, L are computed only once
and then reused multiple times during inference,
which is the key to reduce the time complexity.

Then we deﬁne a new B-FGG G(cid:48) with rules
shown in Fig 4(b).
It is easy to verify that for
each D ∈ D(G), we have M(D) ∈ D(G(cid:48)), and
vice versa. Moreover, we have:

(cid:88)

ξ∈ΞD

wG(D, ξ) =

(cid:88)

ξ∈ΞM(D)

wG(cid:48)(M(D), ξ)

because marginalizing hidden variables does not af-
fect the result of sum-product inference. Therefore,
ZG = ZG(cid:48) (Eq. 2).

We can easily derive the inference (inside) algo-
rithm of G(cid:48) by following Eq. 3-4 and Fig. 4(b) 3.
Let αi,j ∈ Rr denote the rank-space inside score
for span [i, j). When j > i + 2:

(cid:122)

αi,j =

from π1 of Fig. 4(b)
(cid:125)(cid:124)

(cid:123)
(Hαi,k (cid:12) Iαk,j)

(cid:88)

i+1<k<j−1
+ J:,wi (cid:12) Iαi+1,j
(cid:125)
(cid:123)(cid:122)
from π2

(cid:124)

+ Hαi,j−1 (cid:12) K:,wj−1
(cid:125)
(cid:123)(cid:122)
from π3

(cid:124)

and when j = i + 2, αi,j = J:,wi (cid:12) K:,wi+1 (from
π4). wj is the index of the j-th word of the input
sentence in the vocabulary; A:,j indicates the j-th
column of A.

We note that, similar to Cohen et al. (2013), we
can cache Hαi,k, Iαk,j and reuse them to further

i,j ∈ Rr as
accelerate inference 4. Denote αL
the inside scores of span [i, j) serving as a left/right
child of a larger span. Then we have:

i,j, αR

αL

i,i+1 = K:,i
αL

i,j = Hαi,j

αi,j =

(cid:88)

i<k<j

(αL

αR

i,i+1 = J:,i
αR
i,j = Iαi,j
i,k (cid:12) αR

k,j)

and ﬁnally, ZG(cid:48) = Lα0,n. We minimize − log ZG(cid:48)
using mini-batch gradient descent for unsupervised
learning. The resulting inference complexity is
O(n3r + n2r2)5, which is lower than O(n3r +
n2mr) of TD-PCFG when r < m, enabling the
use of a large state space for PCFGs in the low-rank
setting.

The key difference between the rank-space in-
ference and the original state-space inference is
that they follow different variable elimination or-
ders. The former marginalizes all state nodes be-
fore performing inference and marginalizes rank
nodes from bottom up during inference; whereas
the later marginalizes both state and rank nodes
alternately from bottom up during inference.

Parsing. Low-rank inference does not support
the Viterbi semiring6, inhibiting the use of CYK

4In fact, this is a typical application of the unfold-refold
transformation (Eisner and Blatz, 2007; Vieira et al., 2021).

5This does not take into account the one-time cost of com-

puting H, I, J, K before inference.

3π6 is used for generating sentences of length 1, we do not
consider this in the following derivation of the inside algorithm
to reduce clutter.

6The Viterbi semiring is also known as the max-product
semiring. Chiu et al. (2021, Appd. C) and Yang et al. (2021b,
Sec. 6) have discussed this issue.

p(T2 | T1)

T1

T2

X4

T1

U

V

T3

X4

T1 = bos

T1

T4

T7

T10

T3 = eos

p(O3 | T2)

O3

R2

O4

W

(a)

R2

R5

R8

O9

O3

O6

(b)

R2

R5

R8

O9

O3

O6

(c)

Figure 5: (a): merge the two factors into a single one, and apply CPD on the resulting factor. (b): factor graph of a
HMM for sentences of length 3. (c): the resulting factor graph after marginalizing the state nodes.

decoding. Therefore, we follow Yang et al. (2021b)
to use Minimum Bayes-Risk decoding (Good-
man, 1996). Speciﬁcally, we estimate the span
marginals using auto-differentiation (Eisner, 2016;
Rush, 2020), which has the same complexity as the
inside algorithm. Then we use the CYK algorithm
to ﬁnd the ﬁnal parse with the maximum number
of expected spans in O(n3) time, similar to Smith
and Eisner (2006).

Implementation. The implementation of the in-
side algorithm greatly inﬂuences the actual run-
ning speed. First, O(n2) out of O(n3) can be
computed in parallel using parallel parsing tech-
niques (Yi et al., 2011; Canny et al., 2013; Zhang
et al., 2020; Rush, 2020). In this work, we adapt
the efﬁcient implementation of Zhang et al. (2020)
for fast inside computation. Second, we adopt
the log-einsum-exp trick (Peharz et al., 2020)
to avoid expensive log-sum-exp operations on
high-dimensional vectors, which reduces both GPU
memory usage and total running time.

4.3 The rank-space forward algorithm

Consider an B-FGG G shown in Fig. 1 (a). We
replace the rhs of π2 by the hypergraph fragment
in the right of Fig. 5(a), i.e., we merge the factor
p(T2 | T1) and p(O3 | T2) into a single factor,
which can be represented as T ∈ Rm×m×o and
can be decomposed into three matrices U, V ∈
Rr×m, W ∈ Rr×o via CPD, where m/o/r is the
state/vocabulary/rank size. Fig. 5(b) gives an exam-
ple factor graph of HMMs with sentences of length
3. Similar to previous subsection, we marginal-
ize state nodes T to construct a new B-FGG G(cid:48).
The rule set of G(cid:48) can be obtained by replacing all
variable nodes T with R and modifying all fac-
tors accordingly, as one can easily infer from Fig.
5(c). Inference with G(cid:48) simply coincides with the
forward algorithm, which has a O(nr2) time com-
plexity and is lower than O(nmr) of LHMM (Chiu
et al., 2021) when r < m.

4.4 Neural parameterization

We use neural networks to produce probabilities
for all factors, which has been shown to beneﬁt
learning and unsupervised induction of syntactic
structures (Jiang et al., 2016; He et al., 2018; Kim
et al., 2019; Han et al., 2019; Jin et al., 2019; Zhu
et al., 2020; Yang et al., 2020, 2021b; Zhao and
Titov, 2020; Zhang et al., 2021; Chiu and Rush,
2020; Chiu et al., 2021; Kim, 2021). We use the
neural parameterization of Yang et al. (2021b) with
slight modiﬁcations. We show the details in Appd.
A and Appd. B.

5 Experiments

5.1 Unsupervised parsing with PCFGs

Setting. We evaluate our model on Penn Tree-
bank (PTB) (Marcus et al., 1994). Our implemen-
tation is based on the open-sourced code of Yang
et al. (2021b)7 and we use the same setting as theirs.
For all experiments, we set the ratio of nonterminal
number to the preterminal number to 1:2 8 which is
the common practise. We set the rank size to 1000.
We show other details in Appd. C and D.

Main result. Table 1 shows the result on PTB.
Among previous unsupervised PCFG models, TN-
PCFG (Yang et al., 2021b) uses the largest number
of states (500 perterminals and 250 nonterminals).
Our model is able to use much more states thanks to
our new inside algorithm with lower time complex-
ity, surpassing all previous PCFG-based models by
a large margin and achieving a new state-of-the-art
in unsupervised constituency parsing in terms of
sentence-level F1 score on PTB.

7github.com/sustcsonglin/TN-PCFG
8Although we did not explicitly distinguish between non-
terminal and preterminal symbols previously, in our implemen-
tation, we follow Kim et al. (2019) to make such distinction,
in which terminal words can only be generated by preterminal
symbols, and binary rules can only be invoked by nonterminal
symbols.

Model

N-PCFG (Kim et al., 2019)
C-PCFG (Kim et al., 2019)
NL-PCFG (Zhu et al., 2020)
TN-PCFG (Yang et al., 2021b)
NBL-PCFG (Yang et al., 2021a)

Ours with 9000 PTs and 4500 NTs

For reference

S-F1

50.8
55.2
55.3
57.7
60.4

64.1

Constituency test (Cao et al., 2020)
S-DIORA (Drozdov et al., 2020)
StructFormer (Shen et al., 2021)
DIORA+span constraint (Xu et al., 2021)

62.8
57.6
54.0
61.2

Table 1: Results on PTB. S-F1: sentence-level F1. PTs:
preterminals. NTs: nonterminals.

Model

VL-HMM (215 states, Brown)
VL-HMM (214 states, Brown)†
VL-HMM (214 states, Uniform)†
LHMM (214 states)

Ours (214 states)
Ours (215 states)

Val

Test

125.0
136
146
141.4

135.6
137.0

116.0
-
-
131.8

127.0
126.4

For reference

HMM+RNN (Buys et al., 2018)
AWD-LSTM (Merity et al., 2018)

142.3
60.0

-
57.3

Table 2: Resulting perplexity on PTB validate set and
test set. VL-HMM: (Chiu and Rush, 2020). LHMM:
(Chiu et al., 2021). † denotes results reported by abla-
tion study of Chiu and Rush (2020).

)

%

(

e
r
o
c
s

1
F

y
t
i
x
e
l
p
r
e
P

70

65

60

55

200

190

180

170

160

4

6
2
Number of preterminal symbols (K)

8

10

(a)

4

6
2
Number of preterminal symbols (K)

8

10

(b)

Figure 6: The change of F1 scores and perplexities with
the change of number of perterminal symbols.

Ablation study. Fig. 6 shows the change of the
sentence-level F1 scores and perplexity with the
change of the number of preterminals. As we can
see, when increasing the state, the perplexity tends
to decrease while the F1 score tends to increase, val-
idating the effectiveness of using large state spaces
for neural PCFG induction.

5.2 HMM language modeling

Setting. We conduct the language modeling ex-
periment also on PTB. Our implementation is based

#States

Val

Test

212
213
214
215

149.8
143.8
149.5
141.1

139.1
133.4
137.4
131.1

Table 3: Perplexity with varying numbers of states. Fol-
lowing Chiu et al. (2021), we ﬁx the rank to 2048 for
faster ablation studies.

on the open-sourced code of Chiu et al. (2021)9.
We set the rank size to 4096. See Appd. C and D
for more details.

Main result. Table 2 shows the perplexity on the
PTB validation and test sets. As discussed ear-
lier, VL-HMM (Chiu and Rush, 2020) imposes
strong sparsity constraint to decrease the time com-
plexity of the forward algorithm and requires pre-
clustering of terminal symbols. Speciﬁcally, VL-
HMM uses Brown clustering (Brown et al., 1992),
introducing external information to improve perfor-
mance. Replacing Brown clustering with uniform
clustering leads to a 10 point increase in perplexity
on the PTB validation set. LHMM (Chiu et al.,
2021) and our model only impose low-rank con-
straint without using any external information and
are thus more comparable. Our method outper-
forms LHMM by 4.8 point when using the same
state number (i.e., 214), and it can use more states
thanks to our lower inference time complexity.

Ablation study. As we can see in Table 3, the
perplexity tends to decrease when increasing the
state number, validating the effectiveness of using
more states for neural HMM language modeling.

9github.com/justinchiu/low-rank-models

Discussion.
It is interesting to note that our
HMM model is roughly equivalent to another
HMM with interchanged rank and state sizes as
can be seen in Fig.5(c). To verify this equivalence,
we run LHMM in the original state space with
2048 states and rank 215. The resulting perplexity
is 133.49 on average on the PTB test set, which is
worse than that of ours (126.4). We leave further
experimentation and analyses of this discrepancy
for future work.

decompose second-order factors in semantic depen-
dency parsing to accelerate second-order parsing
with mean-ﬁeld inference. Besides CPD, Ducamp
et al. (2020) use tensor train decomposition for fast
and scalable message passing in Bayesian networks.
Bonnevie and Schmidt (2021) leverage matrix prod-
uct states (i.e., tensor trains) for scalable discrete
probabilistic inference. Miller et al. (2021) lever-
age tensor networks for fast sequential probabilistic
inference.

6 Related work

Tensor and matrix decomposition have been used to
decrease time and space complexities of probabilis-
tic inference algorithms. Siddiqi et al. (2010) pro-
pose a reduced-rank HMM whereby the forward al-
gorithm can be carried out in the rank space, which
is similar to our model, but our method is more gen-
eral. Cohen and Collins (2012); Cohen et al. (2013)
use CPD for fast (latent-variable) PCFG parsing,
but they do not leverage CPD for fast learning and
they need to actually perform CPD on existing
probability tensors. Rabusseau et al. (2016) use
low-rank approximation method to learn weighted
tree automata, which subsumes PCFGs and latent-
variable PCFGs. Our method can subsume more
models. Yang et al. (2021b,a) propose CPD-based
neural parameterizations for (lexicalized) PCFGs.
Yang et al. (2021b) aim at scaling PCFG inference.
We achieve better time complexity than theirs and
hence can use much more hidden states. Yang
et al. (2021a) aims to decrease the complexity of
lexicalized PCFG parsing, which can also be de-
scribed within our framework. Chiu et al. (2021)
use low-rank matrix decomposition, which can be
viewed as CPD on order-2 tensors, to accelerate
inference on chain and tree structure models includ-
ing HMMs and PCFGs. However, their method is
only efﬁcient when the parameter tensors are of
order 2, e.g., in HMMs and HSMMs. Our method
leverages full CPD, thus enabling efﬁcient infer-
ence with higher-order factors, e.g., in PCFGs. Our
method can be applied to all models considered
by Chiu et al. (2021), performing inference in the
rank-space with lower complexities.

Besides HMMs and PCFGs, Wrigley et al.
(2017) propose an efﬁcient
sampling-based
junction-tree algorithm using CPD to decompose
high-order factors. Dupty and Lee (2020) also use
CPD to decompose high-order factors for fast be-
lief propagation. Yang and Tu (2022) use CPD to

7 Conclusion and future work

In this work, we leveraged tensor rank decompo-
sition (CPD) for low-rank scaling of structured in-
ference. We showed that CPD amounts to decom-
posing a large factor into several smaller factors
connected by a new rank node, and gave a unifying
perspective towards previous low-rank structured
models (Yang et al., 2021b; Chiu et al., 2021). We
also presented a novel framework to design a fam-
ily of rank-space inference algorithms for B-FGGs,
a subset of FGGs which subsume most structured
models of interest to the NLP community. We have
shown the application of our method in scaling
PCFG and HMM inference, and experiments on
unsupervised parsing and language modeling val-
idate the effectiveness of using large state spaces
facilitated by our method.

We believe our framework can be applied to
many other models which have high inference time
complexity and are subsumed by B-FGGs, includ-
ing lexicalized PCFGs, quasi-synchronous context-
free grammars (QCFGs), etc. A direct application
of our method is to decrease the inference complex-
ity of the neural QCFG (Kim, 2021).

Acknowledgments

This work was supported by the National Natural
Science Foundation of China (61976139).

References

Sanjeev Arora, Nadav Cohen, and Elad Hazan. 2018.
On the optimization of deep networks: Implicit ac-
celeration by overparameterization. In Proceedings
of the 35th International Conference on Machine
Learning, ICML 2018, Stockholmsmässan, Stock-
holm, Sweden, July 10-15, 2018, volume 80 of Pro-
ceedings of Machine Learning Research, pages 244–
253. PMLR.

Ai Azuma, Masashi Shimbo, and Yuji Matsumoto.
An algebraic formalization of forward

2017.

forward-backward

and
abs/1702.06941.

algorithms.

CoRR,

Rasmus Bonnevie and Mikkel N. Schmidt. 2021. Ma-
trix product states for inference in discrete proba-
bilistic models. Journal of Machine Learning Re-
search, 22(187):1–48.

Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural
language.
Computational Linguistics, 18(4):467–480.

Rares-Darius Buhai, Yoni Halpern, Yoon Kim, An-
drej Risteski, and David A. Sontag. 2020. Em-
pirical study of the beneﬁts of overparameteriza-
tion in learning latent variable models. In Proceed-
ings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event, volume 119 of Proceedings of Machine
Learning Research, pages 1211–1219. PMLR.

Jan Buys, Yonatan Bisk, and Yejin Choi. 2018. Bridg-
ing hmms and rnns through architectural transforma-
tions.

John Canny, David Hall, and Dan Klein. 2013. A multi-
In Pro-
teraﬂop constituency parser using GPUs.
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1898–
1907, Seattle, Washington, USA. Association for
Computational Linguistics.

Steven Cao, Nikita Kitaev, and Dan Klein. 2020. Unsu-
pervised parsing via constituency tests. In Proceed-
ings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
4798–4808, Online. Association for Computational
Linguistics.

David Chiang and Darcey Riley. 2020. Factor graph
grammars. In Advances in Neural Information Pro-
cessing Systems 33: Annual Conference on Neu-
ral Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual.

Justin Chiu, Yuntian Deng, and Alexander Rush. 2021.
Low-rank constraints for fast inference in structured
models. Advances in Neural Information Process-
ing Systems, 34.

Justin Chiu and Alexander Rush. 2020. Scaling hid-
den Markov language models. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1341–1349,
Online. Association for Computational Linguistics.

Shay B. Cohen and Michael Collins. 2012. Tensor
decomposition for fast parsing with latent-variable
pcfgs. In Advances in Neural Information Process-
ing Systems 25: 26th Annual Conference on Neural
Information Processing Systems 2012. Proceedings
of a meeting held December 3-6, 2012, Lake Tahoe,
Nevada, United States, pages 2528–2536.

Shay B. Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate PCFG parsing using tensor de-
In Proceedings of the 2013 Confer-
composition.
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 487–496, Atlanta, Geor-
gia. Association for Computational Linguistics.

Antoine Dedieu, Nishad Gothoskar, Scott Swingle,
Wolfgang Lehrach, Miguel Lázaro-Gredilla, and
Dileep George. 2019. Learning higher-order se-
CoRR,
quential structure with cloned hmms.
abs/1905.00507.

Frank Drewes, Hans-Jörg Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement, graph grammars.
In Grzegorz Rozenberg, editor, Handbook of Graph
Grammars and Computing by Graph Transforma-
tions, Volume 1: Foundations, pages 95–162. World
Scientiﬁc.

Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen,
Tim O’Gorman, Mohit Iyyer, and Andrew McCal-
lum. 2020. Unsupervised parsing with S-DIORA:
Single tree encoding for deep inside-outside recur-
sive autoencoders. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 4832–4845, Online. As-
sociation for Computational Linguistics.

Simon S. Du, Xiyu Zhai, Barnabás Póczos, and Aarti
Singh. 2019. Gradient descent provably optimizes
In 7th Inter-
over-parameterized neural networks.
national Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.
OpenReview.net.

Gaspard Ducamp, Philippe Bonnard, Anthony Nouy,
and Pierre-Henri Wuillemin. 2020. An efﬁcient low-
rank tensors representation for algorithms in com-
In Interna-
plex probabilistic graphical models.
tional Conference on Probabilistic Graphical Mod-
els, PGM 2020, 23-25 September 2020, Aalborg,
Hotel Comwell Rebild Bakker, Skørping, Denmark,
volume 138 of Proceedings of Machine Learning Re-
search, pages 173–184. PMLR.

Mohammed Haroon Dupty and Wee Sun Lee. 2020.
Neuralizing efﬁcient higher-order belief propaga-
tion. CoRR, abs/2010.09283.

Jason Eisner. 2016.

Inside-outside and forward-
backward algorithms are just backprop (tutorial pa-
per). In Proceedings of the Workshop on Structured
Prediction for NLP, pages 1–17, Austin, TX. Asso-
ciation for Computational Linguistics.

Jason Eisner and John Blatz. 2007. Program transfor-
mations for optimization of parsing algorithms and
In Proceedings of
other weighted logic programs.
FG 2006: The 11th Conference on Formal Gram-
mar, pages 45–85. CSLI Publications.

Yao Fu and Mirella Lapata. 2021.

tured inference with randomization.
abs/2112.03638.

Scaling struc-
CoRR,

Giorgio Gallo, Giustino Longo, and Stefano Pallottino.
1993. Directed hypergraphs and applications. Dis-
cret. Appl. Math., 42(2):177–201.

Joshua Goodman. 1996. Parsing algorithms and met-
rics. In 34th Annual Meeting of the Association for
Computational Linguistics, pages 177–183, Santa
Cruz, California, USA. Association for Computa-
tional Linguistics.

Wenjuan Han, Yong Jiang, and Kewei Tu. 2019. En-
hancing unsupervised generative dependency parser
with contextual information. In Proceedings of the
57th Annual Meeting of the Association for Com-
putational Linguistics, pages 5315–5325, Florence,
Italy. Association for Computational Linguistics.

Junxian He, Graham Neubig,

and Taylor Berg-
Kirkpatrick. 2018. Unsupervised learning of syn-
tactic structure with invertible neural projections.
In Proceedings of
the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1292–1302, Brussels, Belgium. Association
for Computational Linguistics.

Liang Huang. 2008. Advanced dynamic programming
in semiring and hypergraph frameworks. In Coling
2008: Advanced Dynamic Programming in Compu-
tational Linguistics: Theory, Algorithms and Appli-
cations - Tutorial notes, pages 1–18, Manchester,
UK. Coling 2008 Organizing Committee.

Yong Jiang, Wenjuan Han, and Kewei Tu. 2016. Un-
supervised neural dependency parsing. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing, pages 763–771,
Austin, Texas. Association for Computational Lin-
guistics.

Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane
Schwartz, and William Schuler. 2019. Unsuper-
vised learning of PCFGs with normalizing ﬂow.
In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, pages
2442–2452, Florence, Italy. Association for Compu-
tational Linguistics.

Yoon Kim. 2021. Sequence-to-sequence learning with
latent neural grammars. Advances in Neural Infor-
mation Processing Systems, 34.

Yoon Kim, Chris Dyer, and Alexander Rush. 2019.
Compound probabilistic context-free grammars for
grammar induction. In Proceedings of the 57th An-
nual Meeting of the Association for Computational
Linguistics, pages 2369–2385, Florence, Italy. Asso-
ciation for Computational Linguistics.

Dan Klein and Christopher D. Manning. 2001. Pars-
In Proceedings of the Sev-
ing and hypergraphs.
enth International Workshop on Parsing Technolo-
gies (IWPT-2001), 17-19 October 2001, Beijing,
China. Tsinghua University Press.

Mitchell Marcus,

Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate ar-
gument structure. In Human Language Technology:
Proceedings of a Workshop held at Plainsboro, New
Jersey, March 8-11, 1994.

Stephen Merity, Nitish Shirish Keskar, and Richard
Socher. 2018. Regularizing and optimizing LSTM
In 6th International Conference
language models.
on Learning Representations, ICLR 2018, Vancou-
ver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net.

Jacob Miller, Guillaume Rabusseau, and John Terilla.
2021. Tensor networks for probabilistic sequence
modeling. In The 24th International Conference on
Artiﬁcial Intelligence and Statistics, AISTATS 2021,
April 13-15, 2021, Virtual Event, volume 130 of
Proceedings of Machine Learning Research, pages
3079–3087. PMLR.

Robert Peharz, Steven Lang, Antonio Vergari, Karl
Stelzner, Alejandro Molina, Martin Trapp, Guy Van
den Broeck, Kristian Kersting, and Zoubin Ghahra-
mani. 2020. Einsum networks: Fast and scalable
In Pro-
learning of tractable probabilistic circuits.
ceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020,
Virtual Event, volume 119 of Proceedings of Ma-
chine Learning Research, pages 7563–7574. PMLR.

Stephan Rabanser, Oleksandr Shchur, and Stephan
Günnemann. 2017.
Introduction to tensor decom-
positions and their applications in machine learning.
CoRR, abs/1711.10781.

Guillaume Rabusseau, Borja Balle, and Shay B. Cohen.
2016. Low-rank approximation of weighted tree au-
In Proceedings of the 19th International
tomata.
Conference on Artiﬁcial Intelligence and Statistics,
AISTATS 2016, Cadiz, Spain, May 9-11, 2016, vol-
ume 51 of JMLR Workshop and Conference Pro-
ceedings, pages 839–847. JMLR.org.

Alexander Rush. 2020. Torch-struct: Deep structured
In Proceedings of the 58th An-
prediction library.
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 335–
342, Online. Association for Computational Linguis-
tics.

Yikang Shen, Yi Tay, Che Zheng, Dara Bahri, Donald
Metzler, and Aaron Courville. 2021. StructFormer:
Joint unsupervised induction of dependency and con-
stituency structure from masked language modeling.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers), pages
7196–7209, Online. Association for Computational
Linguistics.

Sajid M. Siddiqi, Byron Boots, and Geoffrey J. Gor-
don. 2010. Reduced-rank hidden markov models.

In Proceedings of the Thirteenth International Con-
ference on Artiﬁcial Intelligence and Statistics, AIS-
TATS 2010, Chia Laguna Resort, Sardinia, Italy,
May 13-15, 2010, volume 9 of JMLR Proceedings,
pages 741–748. JMLR.org.

David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 787–794, Sydney, Australia.
Association for Computational Linguistics.

Tim Vieira, Ryan Cotterell, and Jason Eisner. 2021.
Searching for more efﬁcient dynamic programs. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021, pages 3812–3830, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.

Andrew Wrigley, Wee Sun Lee, and Nan Ye. 2017.
In Proceedings of the
Tensor belief propagation.
34th International Conference on Machine Learning,
ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, volume 70 of Proceedings of Machine Learn-
ing Research, pages 3771–3779. PMLR.

Ji Xu, Daniel J. Hsu, and Arian Maleki. 2018. Bene-
ﬁts of over-parameterization with EM. In Advances
in Neural Information Processing Systems 31: An-
nual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, 3-8 December 2018,
Montréal, Canada, pages 10685–10695.

Zhiyang Xu, Andrew Drozdov, Jay Yoon Lee, Tim
O’Gorman, Subendhu Rongali, Dylan Finkbeiner,
Shilpa Suresh, Mohit Iyyer, and Andrew McCallum.
2021.
Improved latent tree induction with distant
supervision via span constraints. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, pages 4818–4831, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.

Songlin Yang, Yong Jiang, Wenjuan Han, and Kewei
Tu. 2020. Second-order unsupervised neural depen-
In Proceedings of the 28th Inter-
dency parsing.
national Conference on Computational Linguistics,
pages 3911–3924, Barcelona, Spain (Online). Inter-
national Committee on Computational Linguistics.

Songlin Yang and Kewei Tu. 2022. Modeling la-
bel correlations for second-order semantic depen-
dency parsing with mean-ﬁeld inference. CoRR,
abs/2204.03619.

Songlin Yang, Yanpeng Zhao, and Kewei Tu. 2021a.
Neural bi-lexicalized PCFG induction. In Proceed-
ings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 2688–2699,
Online. Association for Computational Linguistics.

Songlin Yang, Yanpeng Zhao, and Kewei Tu. 2021b.
PCFGs can do better: Inducing probabilistic context-
free grammars with many symbols. In Proceedings

of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 1487–1498,
Online. Association for Computational Linguistics.

Youngmin Yi, Chao-Yue Lai, Slav Petrov, and Kurt
Keutzer. 2011. Efﬁcient parallel CKY parsing on
In Proceedings of the 12th International
GPUs.
Conference on Parsing Technologies, pages 175–
185, Dublin, Ireland. Association for Computational
Linguistics.

Songyang Zhang, Linfeng Song, Lifeng Jin, Kun Xu,
Dong Yu, and Jiebo Luo. 2021. Video-aided unsu-
pervised grammar induction. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1513–1524, On-
line. Association for Computational Linguistics.

Yu Zhang, Houquan Zhou, and Zhenghua Li. 2020.
Fast and accurate neural CRF constituency parsing.
In Proceedings of the Twenty-Ninth International
Joint Conference on Artiﬁcial Intelligence, IJCAI
2020, pages 4046–4053. ijcai.org.

Yanpeng Zhao and Ivan Titov. 2020.

Visually
grounded compound PCFGs. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4369–4379,
Online. Association for Computational Linguistics.

Hao Zhu, Yonatan Bisk, and Graham Neubig. 2020.
The return of lexical dependencies: Neural lexical-
ized PCFGs. Transactions of the Association for
Computational Linguistics, 8:647–661.

A Neural parameterization of PCFGs

In this section, we give the full parameterization
of PCFGs. We follow Yang et al. (2021b) with
slight modiﬁcations for generations of U, V, W ∈
Rm×r in 4.2. We use the same MLPs with two
residual layers as Yang et al. (2021b):

(cid:80)

(cid:80)

(cid:80)

s =

E =

U =

Eh2(wt)

H f1(wn)

S h1(wA)

E(cid:48)h2(wt))

S h1(wA(cid:48)))

exp(uT
A(cid:48)∈N exp(uT
exp(uT
E(cid:48)∈Σ exp(uT
exp(uT
n(cid:48)∈N exp(uT
exp(uT
H (cid:48)∈H exp(uT
exp(uT
H (cid:48)∈H exp(uT
hi(x) = gi,1(gi,2( ˜Wix))
gi,j(y) = ReLU ( ˜Vi,jReLU ( ˜Ui,jy)) + y

H f1(wn(cid:48)))

H (cid:48)f3(wl))

H (cid:48)f2(wl))

H f3(wl)

H f2(wl)

W =

V =

(cid:80)

(cid:80)

D Experimental details

For PCFGs, we use Xavier normal initialization
to initialize the weights in hi and fi. We opti-
mize our model using Adam optimizer with β1 =
0.75, β2 = 0.999, and the learning rate 0.002, set-
ting the dimension of all embeddings to 256.

For HMMs, we initialize all parameters by
Xavier normal initialization except for ws and ww.
We use AdamW optimizer with β1 = 0.99, β2 =
0.999, and the learning rate 0.001, and a max grad
norm of 5. We use dropout rate of 0.1 to dropout
ws and U, V in HMMs. We train for 30 epochs
with a max batch size of 256 tokens, and reduce
the learning by multiplying 1
2 if the validation per-
plexity fails to improve after 2 evaluations. Evalua-
tions are performed one time per epoch. We follow
Chiu et al. (2021) to shufﬂe sentences and lever-
age bucket iterator, where batch of sentences are
drawn from buckets containing sentences of similar
lengths to minizing padding.

We run all experiments on NVIDIA TITAN RTX
and NVIDIA RTX 2080ti and all experimental re-
sults are averaged from four runs.

where Σ is the vocabulary set, H is the
set of rank, N is a ﬁnite set of nonterminals,
Wl = [Wn; Wt], wl, wn, wt ∈ Wl, Wn, Wt.
The main differences of neural parameterization
between ours and previous work are that we
make the projection parameter uH shared among
U, V, and U.

B Neural parameterization of HMMs

In this section, we give the full parameterization of
HMMs, which is similar to PCFGs’ parameteriza-
tion. Deﬁne s as start probability for HMMs. And
the deﬁnitions of U, V, W are same as deﬁnitions
in 4.3:

(cid:80)

(cid:80)

s =

U =

H (cid:48)wu)

P h1(ws))

P h1(ws(cid:48)))

exp(uT
s(cid:48)∈S exp(uT
exp(uT
H wu)
H (cid:48)∈H exp(uT
exp(uT
H wv)
v(cid:48)∈S exp(uT
exp(uT
w(cid:48)∈Σ exp(uT
hi(x) = gi,1(gi,2( ˜Wix))
gi,j(y) = ReLU ( ˜Vi,jReLU ( ˜Ui,jy)) + y

H wv(cid:48))
W h2(ww)

W h2(ww(cid:48)))

W =

V =

(cid:80)

(cid:80)

where S is a ﬁnite set of states, H is the set of

rank, Σ is vocabulary set.

C Data details

Penn Treebank (PTB) (Marcus et al., 1994)10 con-
sists of 929k training words, 73k validation words,
and 82k test words, with a vocabulary of size 10k.
For PCFGs, we follow Yang et al. (2021b) and
use their code to preprocess dataset. This process-
ing discards punctuation and lowercases all tokens
with 10k most frequent words as the vocabulary.
The splits of the dataset are: 2-21 for training, 22
for validation and 23 for test.

For HMMs, we follow Chiu et al. (2021) and
use their code to preprocess dataset. We lowercase
all words and substitutes OOV words with UNKs.
EOS tokens have been inserted after each sentence.

10The licence of PTB dataset is LDC User Agreement for
Non-Members, which can be seen on https://catalog.
ldc.upenn.edu/LDC99T42

