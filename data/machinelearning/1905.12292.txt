9
1
0
2

y
a
M
9
2

]
L
P
.
s
c
[

1
v
2
9
2
2
1
.
5
0
9
1
:
v
i
X
r
a

Categorization of Program Regions for Agile
Compilation using Machine Learning and Hardware
Support

Sanket Tavarageri
Intel Labs
sanket.tavarageri@intel.com

ABSTRACT
A compiler processes the code written in a high level lan-
guage and produces machine executable code. The compiler
writers often face the challenge of keeping the compilation
times reasonable. That is because aggressive optimization
passes which potentially will give rise to high performance
are often expensive in terms of running time and memory
footprint. Consequently the compiler designers arrive at a
compromise where they either simplify the optimization
algorithm which may decrease the performance of the pro-
duced code, or they will restrict the optimization to the subset
of the overall input program in which case large parts of the
input application will go un-optimized.

The problem we address in this paper is that of keeping
the compilation times reasonable, and at the same time op-
timizing the input program to the fullest extent possible.
Consequently, the performance of the produced code will
match the performance when all the aggressive optimization
passes are applied over the entire input program.

2 OVERVIEW OF THE SOLUTION
We first train a machine learning (ML) model to classify pro-
gram segments into hard and easy regions. The ML model is
then synthesized into hardware. Figure 1 depicts the entire
workflow. Real world code, and synthetically generated pro-
gram segments are used to train the ML model. Features are
extracted from programs, and form part of the input to the
machine learning algorithm. The code is compiled through
the compiler, and run on Intel architectures. The resulting
performance statistics, and features gleaned from static anal-
ysis of code are used together to build the ML model. The
model is subsequently hardwired on a configurable architec-
ture.

The compiler will use the deployed ML model to perform
differentiated compilation. Figure 2 shows the process. The
input program that is to be compiled is demarcated into hard
and easy modules by the trained ML model. The compiler will
use this information to accelerate the compilation process by
applying fewer optimizations on the easy parts, and extensive
optimizations on the hard parts.

1 INTRODUCTION
We build a machine learning model to classify parts of in-
put programs into “easy”, and “hard” modules. The machine
learning model will be encoded on a configurable architec-
ture. The compiler will use the hardwired machine learning
model to classify an input program into easy and hard mod-
ules and will accordingly apply optimizations âĂŞ no/very
little optimizations for easy ones, and aggressive optimiza-
tions for hard ones.

The differential compilation technique described here could
be used to achieve dual goals: reasonable compilation times
which is crucial for programmer productivity and high per-
formance on Intel Architectures. Additionally, the machine
learning model for classification of program regions and the
attendant hardware support for it, will be useful for library
developers, and compiler writers as well as they can readily
focus on hard-to-optimize code in emerging applications
and/or on emerging architectures.

1

Figure 1: Machine Learning training workflow

Training program segmentsCompilerIntel ArchitecturesMachine Learning algorithmHardware synthesis 
 
 
 
 
 
machine learning algorithm in making determination on the
effect of optimizations.

We extract the following loop centric features. For each
loop nest that occurs in the function, we construct the fol-
lowing features. When there are multiple loop nests in a
function, we construct a single set of features by adding the
corresponding features of different loop nests.

• loop_niters: It is a vector encoding the number of it-
erations of loops that appear in the function being
analyzed. The tuple is ordered in terms of the textual
order of loops in the program text.

• loop_num_logical_ops: The number of logical and rela-

tional operations that appear in the loop nest.

• loop_num_arith_ops: The definition of this is similar
to that of logical_ops except that this feature describes
the number of arithmetic operations.

• loop_num_branches: The number of branches that are

present inside the loop nest.

• loop_num_arrays: The number of arrays used in the

loop nest.

• loop_num_scalars: The number of scalars used in the
loop nest excluding the loop variables themselves.

We reduce the features of different loop nests using weighted

addition. The weight of a loop nest is its depth – the number
of loops in the loop nest. We normalize the loop_niters feature
which is a vector variable (and, it is the only vector feature,
the rest are scalar features) so that we can use an element
wise addition for vector addition. We determine the maxi-
mum loop depth – max_depth among all loop nests within
the training data and extend the loop_niters vectors to the
max_depth length by padding zeros.

For non-loop code, we define the following features which

have similar meaning as those defined for loop nests.

• num_logical_ops
• num_arith_ops
• num_branches
• num_arrays
• num_scalars

Example: Consider the function shown in Fig. 3 which
forms the core of FloydâĂŞWarshall algorithm for finding
the shortest paths between all pairs of vertices in a graph.

The features developed for the above function will be as

follows.

• loop_niters: [S, S, S].

Figure 2: Machine Learning model deployment

3 MACHINE LEARNING MODEL FOR
CATEGORIZATION OF PROGRAM
REGIONS

To train a machine learning model for categorization of pro-
gram regions, the following tasks are performed. 1) Collec-
tion of training data: Both real world code and syntheti-
cally generated programs that are syntactically and semanti-
cally correct are used for training. A system such as Csmith
[6] will be used to generate synthetic code. We choose the
function as the unit of code that we will categorize as either
“easy” or “hard” in terms of difficulty in optimizing it.

Each function is compiled with two optimization flags –
1) one that represents basic optimizations (e.g., -O1 in many
compilers) 2) flag that denotes very aggressive optimizations
(e.g., -O3 in several compilers). This process creates two
versions of each function: Fbasic and Faддr . We execute both
the versions and categorize the function as “easy” to optimize
if the execution time of Fbasic - Tbasic - is very close to that
of Faддr - Taддr . More formally, a function is labelled easy
if the performance of Fbasic is within a certain factor δ (say
80%) of Faддr :

(cid:40)

Label of F =

Taддr
Tbasic

easy
if
hard otherwise

> δ

2) Feature engineering: A static analysis of the input
program is performed and the features are extracted. It is the
loops within functions that consume most execution time,
and the loops need to be optimized optimally in order to
achieve good performance. We capture the characteristics
of loops and the rest of the function separately to aid the

If the loop iterations are symbolic constants as in this
case, they are mapped to the special attribute ‘S’.

• loop_num_logical_ops: 1.

The comparator operator ‘<’ is the only relational op-
erator that appears in the loop nest (excluding the
operators used in the for loops).

2

CompilerInput programHardware synthesized ML modelIntel Architecturesvoid f l o y d _ w a r s h a l l ( i n t n ,

f l o a t p a t h [N ] [ N ] )

{

j , k ;

i ,
( k = 0 ; k < N ; k ++ )

i n t
f o r
{

f o r ( i = 0 ;

i < N ;

i ++ )

f o r

( j = 0 ;

j < N ;

{
j ++ )

{

p a t h [ i ] [ j ] = p a t h [ i ] [ j ] < p a t h [ i ] [ k ] + p a t h [ k ] [ j ] ? p a t h [ i ] [ j ]
: p a t h [ i ] [ k ] + p a t h [ k ] [ j ] ;

}

}

}

}

Figure 3: The Floyd-Warshall algorithm code

Figure 4: Example random forest for classifying program regions

• loop_num_arith_ops: 1.

‘+’ is the only arithmetic operation that is executed in
any branch of the inner most loop.

• loop_num_branches: 1.

This corresponds to the ternary if operator in the loop.

• loop_num_arrays: 1.

The array – path is the array used.

• loop_num_scalars: 0.

The values of all non-loop features are 0 as there is no

non-loop code in the floyd_warshall function.

3) ML model selection and training: We use the Ran-
dom forests [3] as the machine learning model for classifi-
cation of program regions. A random forest comprises of
multiple decision trees, each of which will predict whether
a given function will be “easy” or “hard” to optimize. The
majority vote of the trees is considered the overall prediction
of the random forest. The random forest is superior to single

3

trees as it avoids the problem of overfitting the data – each
constituent tree is trained with the subset of the training
data, and consequently the random forest avoids overfitting.
An example random forest comprising of two decision
trees is shown in Figure 4. The nodes in a decision tree are
conditionals that are functions on the features. The features
of the function to be classified are extracted and are then used
to run them through the component decision trees. Each tree
will label the function as either easy or hard and the majority
vote is considered the true label of the function.

4) Discussion: We described an exemplar approach to
realizing the creation of a machine learning model to catego-
rize program regions. We note that there are a great many
variations possible in terms of selection of features, and se-
lection of the machine learning model itself. For example,
from a feature selection point of view, one could use con-
textual flow graphs [2] that combine data- and control- flow

loop_niters[0] > 100loop_num_arith_ops < 5loop_num_branches < 5num_arrays < 3loop_num_scalars > 4num_logical_ops > 4truetruetruetruetruetruefalsefalsefalsefalsefalsefalseloop_num_branches > 5loop_num_arith_ops > 5loop_num_array > 5num_logical_ops > 4truetruetruetruefalsefalsefalsefalsegraphs of programs. With regards to selection of machine
learning models, deep neural networks could be used. But
the approach described in this document represents a viable
implementation of the idea of using machine learning models
for the purpose of classifying program regions.

4 RELATED WORK
In the JIT compilation approach [1], a program is interpreted.
However âĂĲhotâĂİ loops (loops where significant running
time is spent, or the ones that are frequently executed) are
identified at runtime, and those loops are compiled to native
code. Virtual Machines such as Java Virtual Machine (JVM)
use variants of this strategy to achieve high performance for
interpreted languages.

Leather et al [4] develop a machine learning approach to
program optimization, specifically for choosing loop unroll
factors. Many other research efforts have also focused on
using different machine learning methods to optimize pro-
grams in various ways. Wang et al’s article [5] provides a
comprehensive survey of such attempts.

The previous solutions (such as JIT compilation) attempt
to identify hot loops at runtime, and compile them. This
approach can accelerate compilation process by compiling
only those loops that are important. However, the overall
achieved performance is not optimal because 1) the hot loops
are identified only when the execution is well underway, and
not apriori 2) the non-hot loops are not compiled at all, but
are interpreted.

The machine learning based compiler optimizations focus
on improving the performance of the generated code, but
in fact increase the compilation time significantly. Since the
compilation can take inordinately long, the solutions are
often not practical, and are rarely used in practice.

5 CONCLUSION
In this paper, we proposed a machine learning based solu-
tion to accelerate the process of compilation of programs.
Compiler writers often face the challenge of designing so-
phisticated compiler passes to increase the performance of
generated code while keeping the compilation times rea-
sonable. In this paper, the machine learning approach to
categorization of program regions as easy or hard addresses
that problem by automatically figuring out which program
segments benefit from longer compiler passes, and which
do not. The upshot of the differentiated compiler treatment
of program regions will be that expensive compiler passes
are applied only to the regions that very likely benefit from
them and skipping the passes for regions that do not. Conse-
quently, the performance of the generated code will be high
and the compilation times will be practical.

REFERENCES
[1] [n. d.]. Tracing just-in-time compilation. https://en.wikipedia.org/

wiki/Tracing_just-in-time_compilation

[2] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018.
Neural Code Comprehension: A Learnable Representation of Code
Semantics. In Advances in Neural Information Processing Systems. 3589–
3601.

[3] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001),

5–32.

[4] Hugh Leather, Edwin Bonilla, and Michael O’Boyle. 2009. Automatic
feature generation for machine learning based optimizing compilation.
In Proceedings of the 7th annual IEEE/ACM International Symposium on
Code Generation and Optimization. IEEE Computer Society, 81–91.
[5] Zheng Wang and Michael O’Boyle. 2018. Machine Learning in Compiler

Optimization. Proc. IEEE (2018).

[6] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Find-
ing and Understanding Bugs in C Compilers. In Proceedings of the
32Nd ACM SIGPLAN Conference on Programming Language Design
and Implementation (PLDI ’11). ACM, New York, NY, USA, 283–294.
https://doi.org/10.1145/1993498.1993532

4

